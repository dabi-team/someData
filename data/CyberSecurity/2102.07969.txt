1
2
0
2

v
o
N
5
1

]

R
C
.
s
c
[

2
v
9
6
9
7
0
.
2
0
1
2
:
v
i
X
r
a

Machine Learning Based Cyber Attacks Targeting on Controlled Information:
A Survey

YUANTIAN MIAO, School of Software and Electrical Engineering, Swinburne University of Technology
CHAO CHEN, School of Software and Electrical Engineering, Swinburne University of Technology
LEI PAN, School of Information Technology, Deakin University
QING-LONG HAN, School of Software and Electrical Engineering, Swinburne University of Technology
JUN ZHANG, (corresponding author) School of Software and Electrical Engineering, Swinburne University of

Technology

YANG XIANG, School of Software and Electrical Engineering, Swinburne University of Technology

Stealing attack against controlled information, along with the increasing number of information leakage incidents, has become an

emerging cyber security threat in recent years. Due to the booming development and deployment of advanced analytics solutions,

novel stealing attacks utilize machine learning (ML) algorithms to achieve high success rate and cause a lot of damage. Detecting and

defending against such attacks is challenging and urgent so that governments, organizations, and individuals should attach great

importance to the ML-based stealing attacks. This survey presents the recent advances in this new type of attack and corresponding

countermeasures. The ML-based stealing attack is reviewed in perspectives of three categories of targeted controlled information,

including controlled user activities, controlled ML model-related information, and controlled authentication information. Recent

publications are summarized to generalize an overarching attack methodology and to derive the limitations and future directions

of ML-based stealing attacks. Furthermore, countermeasures are proposed towards developing effective protections from three

aspects—detection, disruption, and isolation.

Additional Key Words and Phrases: Cyber attacks, machine learning, information leakage, cyber security, controlled information.

ACM Reference Format:

Yuantian Miao, Chao Chen, Lei Pan, Qing-Long Han, Jun Zhang, and Yang Xiang. 2021. Machine Learning Based Cyber Attacks

Targeting on Controlled Information: A Survey. 1, 1 (November 2021), 43 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

Authors’ addresses: Yuantian Miao, School of Software and Electrical Engineering, Swinburne University of Technology, Swinburne University of
Technology, Hawthorn, Melbourne, VIC, 3122, ymiao@swin.edu.au; Chao Chen, School of Software and Electrical Engineering, Swinburne Univer-
sity of Technology, Swinburne University of Technology, Hawthorn, Melbourne, VIC, 3122, chaochen@swin.edu.au; Lei Pan, School of Information
Technology, Deakin University, Deakin University, Geelong, Melbourne, VIC, 3220, l.pan@deakin.edu.au; Qing-Long Han, School of Software and
Electrical Engineering, Swinburne University of Technology, Swinburne University of Technology, Hawthorn, Melbourne, VIC, 3122, qhan@swin.edu.au;
Jun Zhang, (corresponding author) School of Software and Electrical Engineering, Swinburne University of Technology, Swinburne University of
Technology, Hawthorn, Melbourne, VIC, 3122, junzhang@swin.edu.au; Yang Xiang, School of Software and Electrical Engineering, Swinburne University
of Technology, Swinburne University of Technology, Hawthorn, Melbourne, VIC, 3122, yxiang@swin.edu.au.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2021 Association for Computing Machinery.
Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2

1 INTRODUCTION

Miao et al.

Driven by the needs to protect the enormous value within data and the evolution of the emerging data mining techniques,

information leakage becomes a growing concern for governments, organizations and individuals [3]. Compromising the

confidentiality of protected information is an information leakage incident and a prominent threat of cyber security [2],

for instance, the leakage of sensitive information results in both financial and reputational damages to the organizations

[19]. Thus, information leakage incidents are indeed an urgent threat that deserves the public attention.

This survey introduces the stealing attack in the cyber security area. According to [32], the information leakage

can be defined as the violation of confidentiality of methods/mechanisms/framework which stores information or has

access to information. In other words, the introduced attack aims at stealing the controlled information. According to

the cyber attack definition in [60], the term “controlled” has an implicit meaning as “protected”. Comparing to the attack

compromising of a computing environment/infrastructure or data integrity, it is more difficult to detect the controlled

information stealing attack in advance. Cyber attacks that is defined as “disrupt, disable, destroy, or maliciously control

a computing environment/infrastructure and destroy the integrity of the data” [60] are out of the scope of this survey,

for example, a DDoS attack leaking customer data [14] is excluded. According to the literature collected between 2014

and 2019, there are three common vulnerabilities subject to the controlled information stealing attacks:

(1) User activity information is a primary target, especially the one stored on the mobile devices. For example, [26]
extracted user’s foreground app running in Android in order to exploit it for the phishing attack, while the user

activity information was protected by a nonpublic system level permission [97].

(2) ML models and their training data are also exploited, particularly those which are hosted on the Machine-Learning-
as-a-Service (MLaaS) systems. For instance, an ML model is confidential due to the pay-per-query development in

a cloud-based ML service [125] as well as the security mechanisms contained in spam/fraud detection applications

[9, 51, 81, 120].

(3) Authentication information is the third category such as keystroke information, secret keys, and passwords.

As a fast-growing technique in the recent years, ML techniques are applied widely in various cyber security areas,

such as cyber attack prediction [123], insider threat detection [77], network traffic classification [78, 146–148], spam

detection [17], and software vulnerability detection [73]. MLaaS [109] assists users with limited computing resources

or limited ML knowledge to utilize ML models. [151] leveraged the ML framework to enhance the accuracy of a user

activities inference attack. Because of the effectiveness and efficiency brought by ML techniques to the stealing attack,

the loss resulted from the ML-based stealing attack is significant. In this survey, the ML-based stealing attack is defined
as follows: An attacker utilizes an ML algorithm to build up a computational model to disclose the controlled information,
while the raw dataset is collected in the legitimate ways. This definition is explained in two attack modes according to
the targets: In the first attack mode, attackers aiming to perform an accurate and efficient stealing attack build up an

ML model as a tool to derive the targeted controlled information; in the second attack mode, the ML model itself is

the target. Building up the ML model means reconstructing the targeted controlled information — the model within

an MLaaS platform, which is also known as model reconstruction attack [34]. Two types of the ML-based stealing

attacks are summarized in this survey. But other attacks, which leak controlled information without applying ML

techniques, have been surveyed in [31] and [7]. Furthermore, malware was introduced to leak password files in [44], and

an eavesdropping attack was proposed in [145] to increase the information leakage rate without using ML algorithms.

This survey investigates the ML-based stealing attack.

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

3

Fig. 1. Introduced Stealing Controlled Information Attack Categories. (Info: information)

There are a few surveys related to information leakage/data breach illustrating the threat and/or prevention. The

prevention of leaking confidential data was studied in [3] through security procedures (i.e. information security policies)

and regular security mechanisms (e.g. intrusion detection system). The conducted leakage is mainly caused by the

incomplete predefined rules of these security mechanisms. This survey analyzes the information leakage problem from

the viewpoint of insufficient predefined rules (i.e. permissions) and exploitable weaknesses of the target infrastructure.

In addition, the threat of data breach towards cloud computing was reviewed in [7] with primary causes to data breaches

as malicious codes, hacked system, electronic back-ups, malicious insiders, and lost devices. A similar conclusion

was drawn in [31] in the context of ad hoc social network. The attacks of leaking privacy was summarized in [31]

including eavesdropping-based attack, man-in-the-middle attack, DoS attack, and so on. This survey investigates the

ML-based attack using ML techniques to steal information in a legitimate way instead of breaking or hacking the

security mechanisms.

A concept for data leakage prevention and information leakage prevention is proposed in [45], that is, the previous

attacks were categorized by the causes of leakage prior to the implementation of the prevention. Furthermore, similar

analysis results of the attack categories were found in [7]. The protection methods will be reviewed based on the analysis

of identified information leakage, but the explored attacks are categorized by the types of the controlled information. In

[77], malicious insiders could steal confidential information if the data is shared within the entire user hierarchy. This

survey conducts the ML-based stealing attack from both outsiders’ and insiders’ (i.e. as a participant in the GAN attack)

viewpoints.

The core papers reviewed in this survey were primarily selected from the top four conferences in cyber security

research field from 2014 to 2019, which are, IEEE S&P, ACM CCS, NDSS, and USENIX Security Symposium. The four

keywords used in our search are “leak”, “information”, “train” and “attack”. Then we further filtered out the papers

without the keyword “machine learning”. We refined the paper collection based on the citation counts. That is, by

checking the papers published in other high quality conferences and journals, we selected the papers with high citations

as the core papers.

This survey introduces a new rising threat of stealing controlled information, and catches up with the trends of this

kind of stealing attack and its countermeasures. Our contributions can be itemized as follows:

• We introduce the ML-based stealing attack, which aims at stealing the controlled/protected information and
leads to huge economic loss. Herein, ML algorithms are applied in the attack to increase the success rate in

various aspects. The classification of the ML-based stealing attacks is built based on the targeted controlled

information preferentially. Based on this classification, the vulnerabilities in various systems and corresponding

attacks are sorted out and revealed.

Manuscript submitted to ACM

4

Miao et al.

• We survey the advances of the ML-based stealing attacks between 2014 and 2019. A methodology applied for
the ML-based stealing attack against the controlled information is generalized to five phases — reconnaissance,

data collection, feature engineering, attacking the objective, and evaluation. The methodology highlights the

similarity of these attacks from strategies and technical perspectives. The public datasets used for the attack

analysis are also summarized and referenced correspondingly.

• We discuss the challenges of attacks stealing controlled information and forecast their future directions accord-
ingly. Since controlled information security is a subject of competition between attackers and defenders, the

countermeasures are summarized and discussed based on the analysis of the ML-based stealing attacks.

By improving our knowledge of this emerging attack, the ultimate purpose of this survey is to safeguard the information

thoroughly. In the information era, the leakage of information, especially those have already been controlled, will result

in tremendous damage to both corporations and individuals [15, 19, 36, 37]. This survey reveals that current protections

cannot fully suppress the existing ML-based stealing attacks. As discussed in Section 4, in the near future, protecting

controlled information can be improved from detecting the access states of related data, disrupting the related data

with considerable utility, and isolating the related data from being accessed.

The rest of this survey is organized as follows: The stealing attack methodology is summarized in Section 2. In

Section 3, the literature review of the stealing attack using ML algorithms in the past five years is presented, where

stealing attacks are reviewed in three categories classified by the types of targeted controlled information. In Section 4,

challenges of the attack are discussed and followed by corresponding future directions. Section 5 concludes the survey.

2 ATTACK METHODOLOGY: MLBSA

This section proposes an attack methodology for stealing controlled information attacks utilizing ML techniques as

shown in Fig. 2. And the methodology is named as the Machine Learning Based Stealing Attack (MLBSA) methodology.

We revised the cyber kill chain [59, 123] for modeling the MLBSA methodology. A typical kill chain consists of seven

stages including reconnaissance, weaponization, delivery, exploitation, installation, command and control, and actions
on objectives [61, 138]. Reconnaissance aims to identify the target by assessing the environment. As a result, the
prior knowledge of attacks can guide data collection. Regarding the ML-based stealing attack, weaponization means
data collection. Extracting the useful information via feature engineering is essential. Using supervised learning, the
ML-based model is built as a weapon taking actions on objectives. Moreover, the ML-based stealing attack may keep
improving its performance and accumulate the knowledge gained from its retrieved results. Other stages of kill chain,
including delivering the weapon to the victim, exploiting the vulnerabilities, installing the malware, and using command
channels for remote control [61], are considered as a preparation phase before attacking the objectives. In this paper,
the preparation phase is named feature engineering. Having consolidated a few steps of the kill chain, the MLBSA

methodology consists of five phases, which are organized in a circular form implying a continuous and incremental

process. The five phases of the MLBSA methodology are 1) reconnaissance, 2) data collection, 3) feature engineering, 4)

attacking the objective, and 5) evaluation. The following subsections will illustrate each phase in details.

2.1 Reconnaissance

Reconnaissance refers to a preliminary inspection of the stealing attack. The two aims of this inspection include defining

adversaries’ targets and analyzing the accessible data in order to facilitate the forthcoming attacks.

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

5

Fig. 2. ML-based stealing attack methodology (abbreviated as MLBSA methodology).

The target of adversaries in the published literature is usually the confidential information controlled by systems and

online services. According to Kissel [60] and Dukes [27], the term “information” is defined as “the facts and ideas which

can be represented as various forms of data, within which the knowledge in any medium or form are communicated

between system entities”. For example, an ML model (e.g. prediction model) represents the knowledge of the whole

training dataset and can act as a service to return results of any search queries [125, 131]. Thereby, the controlled

information can be interpreted as the information stored, processed, and communicated in the controlled area for which

the organization or individuals have confidence that their protections are sufficient to secure confidentiality. It is more

difficult to detect the attack against confidentiality than the attacks against integrity and availability. These information

stealing attacks are often referred to as the “unknown unknowns”.

In this survey, the targeted controlled information can be classified into three categories: user activities information,

ML related information, and authentication information. Herein, user activities of the mobile system, such as which app

is running in the foreground, are considered as sensitive information. Such sensitive information should be protected

against security threats like phishing [26]. ML models can be provided on the Internet as a service to analyze the big data

and build predictive models [125, 131], such as Google Prediction API and Amazon SageMaker. In this scenario, both the

model and training data are considered to be confidential subjects. However, when some ML services allow white-box

access from users, only training data is considered as confidential information. For example, passwords and secret keys

unlocking mobile devices and authenticating online services should always be stored securely [42, 90, 128, 132]. Using

ML to infer the password from user’s keystrokes breaks the information confidentiality [79, 122].

Since the information that adversaries aimed to steal is in control, the accessible data is the breakthrough point.

In order to analyze its value, the attacker mimics a legitimate user to learn the characteristics and capabilities of the

targeted systems, especially those related to the controlled information. During the reconnaissance of accessible data, the

attacker needs to exhaustively search all possible entry points of the targeted system, reachable data paths, and readable

data [119]. When the attacker aims at user’s activities, the triggered hardware devices and their corresponding logged

information will be investigated [26, 119, 151]. For example, the attacker always searches and explores the readable

system files, such as interrupt timing data [26, 119] and network resources [151]. To perform a successful stealing

attack against a model [125, 131] or training samples [34, 49, 116], the functionalities of ML services (e.g. Amazon

ML) are analyzed by querying specific inputs. The attacker analyzes the relationship between the inputs and the

outputs including output labels, their corresponding probabilities (also known as confidence values), and the top ranked

information [34, 116, 125]. The relationship reveals some internal information about the target model and/or training

samples. For authentication information stealing attacks, stealing keystroke information needs to utilize some sensor

Manuscript submitted to ACM

6

Miao et al.

activity information activated by the attacker, while the intermediate data can be regarded as accessible data [79, 122].

The fine-grained information about security domains, e.g. secret keys, can be inferred by analyzing the accessible cache

data [42, 152]. The accessible data related to the target information are defined in the reconnaissance phase.

2.2 Data Collection

Having conducted a detailed reconnaissance process, the attacker refines the scope of their targeted controlled infor-

mation alongside with the awareness of the related accessible data. Then the attacker designs specific queries against

the target system/service to collect useful accessible data. What differentiates the information stealing attack and

other forms of cyber attacks is that the datasets are collected in a malicious manner instead of being collected via the

malicious way. In accordance with the intelligence gained during the reconnaissance phase, data collection can be

either active collection or passive collection.

Active collection refers to that the attacker actively interacts with the targeted system for data collection. Specifically,

an attacker designs some initial queries to interact with the system and subsequently collects the data. The goal of

the attacker guides the design of malicious interactions, referring to the analysis results from the reconnaissance

phases. All data closely related to the controlled information can be gathered as a dataset for the stealing attack. For
example, if an attacker intends to identify which app is launched in a user’s mobile, some system files like 𝑝𝑟𝑜𝑐 𝑓 𝑠
recording app launching activities should be collected, according to [119]. In addition, various apps will be launched

for several times by the attacker for app identification. By running 100 apps, a dataset of kernel information about

the foreground UI refreshing process was gathered in [26] to identify different apps. The active collection for stealing

keystroke information and secret keys are similar to that for stealing user activities information. Interacting with the

operating system with different keystrokes inputs, sensor information like acceleration data and video records was

collected in [79, 122] to infer keystroke movements. Moreover, cache data was collected in [42, 152] to analyze the

relationship between memory access activities and different secret keys.

Additionally, the active collection for stealing ML-related information aims to design effective and efficient queries

against the target model. The design of active collection varies when the target model allows black-box access or

white-box access (see more details in Section 3). With the black-box access, the inputs and corresponding outputs are

collected to reveal the model’s internal information. The number of inputs should be sufficient to measure the model’s

functionality [125, 131] or clarify its decision boundary [34, 98, 101]. A set of inputs was synthesized in [101] to train a

local model to substitute the target model. The range of inputs should also be wide enough to include samples inside

and outside a model’s training set [110, 116]. With the white-box access, not only inputs and outputs but also some

internal information are collected to infer the training data information. For instance, the model was updated in [49, 91]

by training a local model with data having different features, and the changes in the global model’s parameters are

utilized to infer the targeted feature values.

The other kind of collection is passive collection, which is defined as gathering all data related to the targeted

controlled information without engaging with the targeted system/service directly. The passive collection mainly

used to steal the password information in this survey where the targeted system/service is always a login system or

permission granted. In such a case, engaging with the target system/service directly could only provide attackers the

information on whether the guessing password is correct or not. This information can be used to validate an ML-based

attack but is unable to contribute to an attack model as training or testing data. Users’ passwords were cracked in

[132] by generating many passwords in high probabilities based on people’s behaviors of password creation, while

passwords were generated in [128] based on the semantic structure of passwords. Specifically, attackers collect the

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

7

relevant information such as network data, personal identifiable information (PII), previous leaked passwords, the

service site’s information and so on [90, 128, 132]. The information can be gathered by searching online and accessing

some open sources data like leaked passwords (as shown in Table 11). The attack targeting at password data primarily

uses the passive collection to gather information.

All of the stealing attacks involved in this survey use the supervised learning algorithms. The ground truth need to be

established in the data collection phase. Among the investigated ML-based stealing attacks, collected data was labeled

with the target information or something related. For instance, the kernel data about the foreground UI refreshing

process caused by app launching activities is labeled with corresponding apps [26]. In [110, 116], data was labeled with

member or non-member of the target training set. Similarly, for stealing the authentication information, the collected

sensor data was labeled in [79, 122] with corresponding input keystrokes. The ground truth of datasets for ML-based

stealing attacks are closely related to attackers’ target information.

2.3 Feature Engineering

After the datasets are prepared, feature engineering is the subsequent essential phase to generate representative vectors

of the data to empower the ML model. The two key points in feature engineering for ML-based attacks consist of dataset

cleaning and extracting features.

An obstacle of feature engineering is cleaning the noises and irrelevant information in the raw data. In general,

deduplication and interpolation can be used to reduce the noise from accessible resource [26]. To reduce the noise, a Fast

Fourier Transform (FFT) filter and an Inverse FFT (IFFT) filter are applied [79]. Other popular methods extract refined

information and replace redundant information, such as Dynamic Time Warping (DTW) and Levenshtein-distance (LD)

algorithms for similarities calculation in time series data [26, 119, 151], Symbolic Aggregate approXimation (SAX) for

dimensionality reduction [105, 151], normalization and discretization for effectiveness [151], and Bag-of-Patterns (BoP)

representation and Short Time Fourier Transform (STFT) for feature refinement [50, 74, 151].

In order to extract features, it is necessary to analyze and clarify the relationship between the dataset and the targeted

controlled information. The relationship determines what kinds of features the attacker should extract. For instance,

the inputs and their corresponding confidential values reveal the behaviour of the model stored in cloud service (like

Google service). Adversaries choose each query’s confidential value as a key feature. Therefore, this relationship can

be leveraged to steal an ML model and customer’s training samples using reverse-engineering techniques [125] and

the shadow training samples’ generation [116]. Using reverse-engineering techniques, the target model’s parameters

were revealed in [125] by finding the threshold where confidential value changes with various inputs. Shadow training

samples are intended to be statistically similar to the target training set and are synthesized according to the inputs

with high confidence values.

When targeting at user activities, some feature extraction approaches are applied in a kernel dataset for the stealing

attack. In [26, 119], the diverse foreground apps were characterized by the changes in electrostatic field found in

interrupt timing log files stored on Android. The statistics of interrupt timing data are calculated as features [26].

Feature extraction techniques depend on the type of the useful information. For example, several extraction techniques,

including interrupt increment computation, gram segmentation, difference calculation, and the histogram construction,

are specialized for the sequential data, like interrupt time series [26, 136, 151]. For the authentication information

stealing attack, the ways of defining features are similar to those methods mentioned above [79, 90, 132]. One typical

method is transforming the characteristics of information as features, such as logical values of the state of sensor [117],

temporal information accessing memory activities [42], different kinds of PIIs from Internet resources [128, 132], and

Manuscript submitted to ACM

8

Miao et al.

acceleration segments within a period of time collected from smartwatches’ accelerometer [79]. In addition, manually

defining the features based on the attackers’ domain knowledge is another popular method [71, 119, 128, 151].

2.4 Attacking the Objective

(a) The First Attack Mode

(b) The Second Attack Mode

Fig. 3. Two ML-based attack modes: the first mode uses this ML-based model as a weapon to steal controlled information, while this
model itself is the target for the second mode. Based on the result from reconnaissance, attackers design the input queries. Querying
the target system/service, attackers collect required accessible data from the inputs and their query outputs. To set up ground truth,
the data are labeled according to the target information. After feature engineering, training dataset is built with labels to train an
supervised ML model. For the first mode, testing samples without labels test the model whose outputs are the target information. For
the second mode, the training dataset is used to reconstruct a model which is the attacker’s target.

In this survey, we only consider the ML-based stealing attack as defined in Section 1 targeting at user activity

information, ML model related information, and authentication information. We summarize the ML-based stealing

attack into two attack modes as illustrated in Fig. 3. That is, the initial five actions are identical in both attack modes.

These five actions correspond to the first three phases within the MLBSA methodology. Specifically, the attacker

firstly reconnoiters the environment storing targeted controlled information. The environment provides an interface

taking users’ queries and responding to the queries. The attacker designs the input queries and inquiries the target

system/service. As stated in the data collection phase, the inputs and their query results are collected as the required

accessible dataset, which reveals the target information. Based on the target information, the ground truth of the dataset

is set up in this phase. With proper feature engineering methods, the training dataset is prepared to attack the objective.

But the subsequent actions to steal the controlled information using machine learning differ between two attack modes.

For the first attack mode as shown in Fig. 3a, the training dataset is used to train an ML model to steal the

controlled information. The testing dataset has the same features as the training dataset. Regarding the testing dataset

is collected from a victim’s system/service, the testing samples are not labeled while querying the attack model. Since

the attack model is built to infer the controlled information from these accessible data, the output of the model is the

targeted controlled information. This attack mode is applied in the ML-based stealing attack against the user activity

information, the authentication information, and training set information. The literature applies ML algorithms to
train the classification model such as Logistic Model Tree (LMT) [117], 𝑘-Nearest Neighbors (𝑘-NN) [26, 119, 151],
Support Vector Machine (SVM) [136, 151], Naive Bayes (NB) [117, 128], Random Forest (RF) [79, 110, 122], Neural

Network (NN) [38, 71], Convolutional Neural Network (CNN) [1] and logistic regression [34, 91]. Apart from these

classification models, the probabilistic forecasting model is popular to predict the probability of the real password

with a guessing password pattern. There are a few probabilistic algorithms applied, such as Probabilistic Context-Free

Grammars (PCFG), Markov model, and Bayesian theory [42, 90, 128].

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

9

Table 1. Confusion Matrix for Evaluation. As stated in Section 2.2, the outputs of the attack model are the target information. Class 𝐴
and Class 𝐵 refer to the subclass of one controlled information, like foreground app 𝐴 and 𝐵 in stealing user activity information
attack. Additionally, Class 𝐴 and Class 𝐵 can also refer to one controlled information and not this information respectively. For
example, if Class 𝐴 is a member of the target training set, then Class 𝐵 is not member of that set.

(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)

Predicted

Actual

Class 𝐴
Class 𝐵

Class 𝐴

Class 𝐵

True Positive (TP)
False Negative (FN)

False Positive (FP)
True Negative (TN)

For the second attack mode illustrated in Fig. 3b, the training dataset is used to train an ML model while the model

itself is the target of the attack. This attack mode is mostly applied in the ML-based stealing attack against the ML model

related information. In a black-box setup, stealing the ML model attack aims at calculating the detailed expression of

the model’s objective function. Reconstructing the original model is essentially a reconstruction attack [34]. Using

the equation-solving and path-finding methods [125, 131], the inputs and their query outputs for solving the specific

objective function expression is interpreted as the training set. Therefore, this attack can be simplify regarded as an

ML-based attack. Additionally, based on the attackers’ inputs and the query outputs, the training set is synthesized and

used to build a substitute model for reconstruction [98, 101]. Several ML algorithms were applied in the literature, such

as decision tree [101, 125], SVM [125, 131], NN [98, 125, 131], Recurrent Neural Network (RNN) [90], ridge regression

(RR), logistic regression, and linear regression [101, 125, 131].

Moreover, some popular and publicly available tools can be used to train the ML model for the attack, for example,

WEKA and monkeyrunner [26, 122]. In summary, despite the model itself is the attacker’s objective, the adversary can

predict the results which reveal the controlled information in the training data using ML techniques.

2.5 Evaluation

During the evaluation phase, attackers measure how likely they can successfully steal the controlled information.

Evaluation metrics differ between two attack modes. As we investigate the ML-based attack under the first attack

mode, the attack evaluation measures the performance of the attack model using confusion matrix. The higher the

performance of the model is, the more powerful the weapon the attacker builds. While under the second attack mode,

the attack evaluation measures the differences between the attack model and the target model. The attack will be

considered more successful when the attack model is more similar to the target model. Evaluation metrics for two

attack modes are summarized separately.

For the first attack mode, the attack model is the attacker’s weapon. Its performance is measured by effectiveness

and efficiency. Specifically, metrics like execution time and battery consumption are used for efficiency evaluation.

Most metrics commonly used to measure the effectiveness include accuracy, precision, recall, FPR, FNR, and F-measure,

which are derived from the confusion matrix in Table 1. The evaluation metrics are listed as below.

• Accuracy: It is also known as success rate and inference accuracy [26, 79, 122]. Accuracy means the number
of correctly inferred samples to the total number of predicted samples. Accuracy is a generic metric evaluating
the attack model’s effectiveness. 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =

𝑇 𝑃 +𝑇 𝑁
𝑇 𝑃 +𝑇 𝑁 +𝐹 𝑃 +𝐹 𝑁

• Precision: It is regarded as one of the standard metrics for attack accuracy [116]. Precision illustrates the
percentage of samples correctly predicted as controlled class 𝐴 among all samples classified as 𝐴. Precision
reveals the correctness of the model’s performance on a specific class [71, 117, 119], especially when features’
values are binary [34]. 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇 𝑃

𝑇 𝑃 +𝐹 𝑃

Manuscript submitted to ACM

10

Miao et al.

• Recall: It is regarded as another standard metric for attack accuracy [116]. Recall is also called sensitivity or
True Positive Rate (TPR) [117]. It is the probability of the amount of class A correctly predicted as class 𝐴.
Similar to precision, recall also reveals the model’s correctness on a specific class. These two metrics are almost
always applied together [34, 38, 91, 110, 116, 117, 119]. 𝑅𝑒𝑐𝑎𝑙𝑙 = 𝑇 𝑃

𝑇 𝑃 +𝐹 𝑁

• F-measure: This metric or F1-score is the harmonic mean of recall and precision. F-measure provides a

comprehensive analysis of precision and recall [117]. 𝐹 − 𝑚𝑒𝑎𝑠𝑢𝑟𝑒 = 2×𝑅𝑒𝑐𝑎𝑙𝑙×𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛
𝑅𝑒𝑐𝑎𝑙𝑙+𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛

• False positive rate (FPR): This metric denotes the proportion of class 𝐵 samples mistakenly categorized as

class 𝐴 sampled. FPR assesses the model’s misclassified samples. 𝐹 𝑃𝑅 =

𝐹 𝑃
𝑇 𝑁 +𝐹 𝑃

• False negative rate (FNR): This metric stands for the ratio between class 𝐴 samples mistakenly categorized as
class 𝐵 samples. Similar to FPR, FNR assesses the model’s misclassified samples from another aspect. FPR and
FNR are almost always applied together to measure the model’s error rate [117]. 𝐹 𝑁 𝑅 = 𝐹 𝑁

𝑇 𝑃 +𝐹 𝑁

• Execution time: The execution time is used in training the model which indicates the efficiency of the attack

model [26, 151, 152].

• Battery consumption: It is also known as power consumption [151]. Battery consumption refers to the target
mobile’s battery while the target system is a mobile system [26, 119, 151], which indicates the efficiency of the

attack model.

For the second attack mode, ML-based attacks of stealing the ML model are assessed with other metrics. This kind of

attack is the ML model reconstruction attack. Inherently, the reconstruction attack requires a set of comparison metrics.
The target of this kind of attack is an ML model ˆ𝑓 which closely matches the original ML model 𝑓 . Generally, the stolen
model ˆ𝑓 will be constructed locally. Its prediction results will be compared to the results of the original model with the
same inputs. The applied evaluation metrics are defined and listed below:

• Test error is the average error based the same test set (𝐷) testing at learned model and targeted model [125]. A

low test error means ˆ𝑓 matches 𝑓 well. 𝐸𝑟𝑟𝑜𝑟𝑡𝑒𝑠𝑡 (𝑓 , ˆ𝑓 ) =

(cid:205)𝑥 ∈𝐷 𝑑𝑖 𝑓 𝑓 (𝑓 (𝑥), ˆ𝑓 (𝑥))
|𝐷 |

• Uniform error is an estimation of the portion of full feature space that the learned model is different from the
(cid:205)𝑥 ∈𝑈 𝑑𝑖 𝑓 𝑓 (𝑓 (𝑥), ˆ𝑓 (𝑥))
|𝑈 |

targeted one, when the testing set (𝑈 ) are selected uniformly [125]. 𝐸𝑟𝑟𝑜𝑟𝑢𝑛𝑖 𝑓 𝑜𝑟𝑚 (𝑓 , ˆ𝑓 ) =

• Extraction accuracy indicates the performance of model extraction attack based on the test error and the

uniform error [125]. 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦𝑒𝑥𝑡𝑟𝑎𝑐𝑡𝑖𝑜𝑛 = 1 − 𝐸𝑟𝑟𝑜𝑟𝑡𝑒𝑠𝑡 (𝑓 , ˆ𝑓 ) = 1 − 𝐸𝑟𝑟𝑜𝑟𝑢𝑛𝑖 𝑓 𝑜𝑟𝑚 (𝑓 , ˆ𝑓 )

• Relative estimation error (EE) measures the effectiveness of model extraction attack using its learned hyper-

parameters ( ˆ𝜆) contrasting to the original hyperparameters (𝜆) [131]. 𝐸𝑟𝑟𝑜𝑟𝐸𝐸 = | ˆ𝜆−𝜆 |
𝜆

• Relative mean square error (MSE) measures how well the model extraction attack reconstructs the regression
models via comparing the mean square error after learning hyperparameters using cross-validation techniques
[131]. 𝐸𝑟𝑟𝑜𝑟𝑀𝑆𝐸 =

|𝑀𝑆𝐸 ˆ𝜆−𝑀𝑆𝐸𝜆 |
𝑀𝑆𝐸𝜆

• Relative accuracy error (AccE) measures how well the model extraction attack reconstructs the classification
models via comparing accuracy error after learning hyperparameters using cross-validation techniques [131].
𝐸𝑟𝑟𝑜𝑟𝐴𝑐𝑐𝐸 =

|𝐴𝑐𝑐𝐸 ˆ𝜆−𝐴𝑐𝑐𝐸𝜆 |
𝐴𝑐𝑐𝐸𝜆

The adversary applies evaluation metrics to determine whether the performance of attack is satisfactory or not.

If the value of any metrics does not meet the expectations, adversaries can restart the stealing attack by redefining

the targeted controlled information. The stealing attack can be executed incrementally until the attacker gains the

satisfactory results.

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

11

3 ML-BASED STEALING ATTACKS AND PROTECTIONS

Table 2. Outline of Reviewed Papers (info: information)

Reference

Year

[26]

[119]

[151]

[136]

[50]

[117]

[125]

[101]

[131]

[98]

[34]

[49]

[116]

[110]

[38]

[91]

[95]

[100]

[70]

[79]

[122]

2016

2018

2018

2015

2016

2017

2016

2017

2018

2018

2015

2017

2017

2019

2018

2019

2018

2017

2017

2015

2016

Targeted Info
Unlock pattern;
Foreground app
Visited websites;
Foreground app
Visited websites;
Foreground app; Map
Visited websites;
Input keystrokes
Manufacturing
activities
User activities info
Parameters of
an ML model
Internal info of
an ML model
Hyperparameters
of an ML model
Hyperparameters
of an ML model
Training data for
an ML model
Training data for
an ML model
Training data for
an ML model
Training data for
an ML model
The property of
training set
The property of
training set
Training data for
an ML model
Training data for
an ML model
Training data for
an ML model
Input PINs;
User input texts
Input PINs;
User input texts

Accessible Data

Hardware interrupt data

Interrupt data; Network
&Memory process record
Memory data; Network
source; File system data
Kernel data-structure
fields
Acoustic sensor data;
Magnetic sensor data
Sensor data
Input features &
Query outputs
Input features &
Query outputs
Input features &
Query outputs
Input features &
Query outputs
Input features & Query
outputs & model structure
Input features & Query
outputs & model structure
Input features &
Query outputs
Input features &
Query outputs
Input features & Query
outputs & model structure
Input features & Query
outputs & model structure
Input features &
Query outputs
Input features &
Query outputs

Goals
Unlock pattern & foreground app inference attacks via analyzing
interrupt time collected from interrupt log file.
Search and attack the kernel records leaking user’s specific events
(i.e. app starts, website launch, keyboard gesture).

Several side-channel inference attack on iOS mobile device.

Protect by injecting noise into the value of kernel data
structure values to secure 𝑝𝑟𝑜𝑐 𝑓 𝑠.
An attack capture acoustic & magnetic sensor data to steal a
manufacturing process specification or a design.
Contextual model detect malicious behavior of sensors like leaking.
Model extraction attacks leverage confidence info with
predictions against MLaaS APIs in black-box setting.
Build a local model to substitute the target model and use
it craft adversarial examples in black-box setting.
Hyperparameters stealing attack via observing minima objective
function against MLaaS in black-box setting.
Build a metamodel to predict hyperparameters with a given
classifier in black-box setting to generate adversarial examples.
Model inversion attacks used confidence info leaking
training samples with predictions against MLaaS in two settings.
Online Attack using GAN against collaborative deep learning
model leaking user’s training sample.
Membership inference attacks use shadow training technique to
leak the specific record’s membership of original training set.
Enlarge the scope of membership inference attacks by releasing
some key assumptions.
Infer global properties of the training data unintended to be shared
in white-box setting.
Membership inference attacks against collaborative deep learning
model leaking others’ unintended feature.
Protect against black-box membership inference attack using
an adversarial training algorithm.
Protect training set of model from leakage with teacher and student
models using PATE.

N/A

Protect training dataset in stored from leakage before training.

Acoustic sensor data;
Accelerometer data

Audio sensor data

[42]

2018

Cryptographic keys

TLB Cache data

[152]

2016

Secret keys

CPU Cache data

[132]

[128]

[90]

2016

2014

2016

Password info

Password info

PII & leaked password
& site info
Corpus & Site leaked list

Password info

Corpus library

Attack infers users’ inputs on keyboards via accelerometer data
within user’s smartwatch.
Attack infers a user’s typed inputs from surreptitious video
recordings of a tablet’s backside motion.
TLBleed attack TLBs to leak secret keys about victim’s memory
activities via reversing engineer and ML strategies.
Mitigate access-driven side-channel attacks with CacheBar
managing memory pages cacheability.
Attack with seven mathematical guessing models for seven
password guessing scenario using different personal info.
Password guessing attack by analyzing its semantic patterns.
Mitigate against password guessing attack by modeling
password guessability in password creation stage.

This section reviews all the core papers in accordance with the MLBSA methodology presented in the previous

section. The review will be undertaken hierarchically according to the structure illustrated in Fig. 1. This section consists

of three subsections. The secondary class of Section 3.1 is based on different kinds of accessible data, while Section 3.2

and Section 3.3 are grouped by different kinds of controlled targeted information. For each attack category, the attack

methods and the corresponding countermeasures are discussed.

To understand the information leakage threat and the stealing attack comprehensively, an outline of relevant

high-quality papers from 2014 to 2019 is provided and shown in Table 2, which lists each paper’s targeted controlled

information, the accessible data and the goal. In the end, these papers are summarized in Table 12 from four perspectives

including the attack, protection, related ML techniques, and evaluation. For each subsection, the key points of the attack

Manuscript submitted to ACM

12

Miao et al.

are listed, such as “dataset for an experiment”, “dataset description”, “feature engineering (/targeted ML model)”, and

“ML-based attack methods”. Tables 3 to 11 summarize all subclasses of the review. In this section, all tables highlight

the essential elements of the each ML-based attack. The detailed information of the dataset and source code for these
attacks are listed on Github 1.

3.1 Controlled User Activities Information

It is essential for security specialists to protect user activities information. Not only because the private activities are

valuable to adversaries, but also the adversary can exploit some specific activities (i.e. foreground app) to perform

malicious attacks such as the phishing attack [26]. In general, the attackers pursue two types of data — kernel data

and sensor data, as shown in Fig. 4. As stated in Section 2, the works of literature trained their attack models with

supervised learning. We organize the reviewed papers according to the MLBSA methodology. The countermeasures

against this kind of ML-based stealing attack are discussed at the end of Section 3.1. According to the utilized kernel

data and sensor data, controlled user activities information were stolen through timing analysis and frequency analysis.

Fig. 4. The ML-based stealing attack against user activities information. As stated in Section 2, reconnoiter and query in the
reconnaissance phase aim to gain the data that is accessible and valuable to the attack. The required data for this attack can be
categorized as kernel data and sensor data. In the data collection phase, these datasets are collected and labeled with input actions as
ground truth. The label’s value can be either continuous (i.e. a series of lines for one unlock pattern [26]) or discrete (i.e. various apps
[119]). Upon completion of extracting features, the training set will be used to train the attack model, and the testing set is prepared to
test and evaluate the model with its outputs. Herein, regression models predict the output as a continuous value (i.e. swipe lines),
whilst classification models predict a discrete value (i.e. a foreground app).

3.1.1 Stealing controlled user activities from kernel data. The dataset collected from the kernel about system

process information is too noisy and coarse-grained to disclose any intelligible and valuable information. Hence, neither

specific requirements are required nor strong protections are deployed in accessing these data. However, through

analyzing plenty of such data, the adversary could deduce some confidential information about the victim’s activities.

With the aid of ML algorithms, the effectiveness and efficiency of this attack can be improved significantly. As a result,

such an information leakage problem becomes more severe than ever, and the corresponding protection is in great
need. The stealing attack utilizes the interrupt sources, such as 𝑝𝑟𝑜𝑐 𝑓 𝑠 and the OS-level information, such as memory,
network, and file system information.

Stealing User Activities with Timing Analysis: The security implications of the kernel information were evaluated in
[26, 119] through integrating some specific hardware components into Android smartphones. During the reconnaissance

phase, user activities information records the user’s interactions with hardware devices, before responded by the kernel

layer. The targeted user activities in [26] were unlock patterns and foreground apps. Moreover, users’ browsing behavior

1https://github.com/skyInGitHub/Machine-Learning-Based-Cyber-Attacks-Targeting-on-Controlled-Information-A-Survey

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

13

Table 3. Stealing Controlled User Activities using Kernel Data

Reference

Dataset for Experiment

Description

[26]

[119]

[151]

[136]

Interrupt data for unlock
pattern and for apps

Time series for apps,
website,keyboard guests
1200 x 6 time series of
data about app;
1000 website traces
Consecutively reading
data; Resident size field data

Collect from 𝑝𝑟𝑜𝑐 𝑓 𝑠

Collect from 𝑝𝑟𝑜𝑐 𝑓 𝑠

120 apps(App Store+iOS )
+10 trace x 6 time series;
10 traces for each website
Collect from 𝑝𝑟𝑜𝑐 𝑓 𝑠

Feature Engineering
Deduplication; Interpolation;
Interrupt Increment Computation;
Gram Segmentation; DTW
Automatically extract with
𝑡𝑠 𝑓 𝑟𝑒𝑠ℎ; DTW

Manually defined;
SAX, BoP representation

ML-based Attack Method
HMM with Viterbi
algorithm; 𝑘-NN classifier
with DTW
Viterbi algorithm with DTW;
SVM classifier with DTW
SVM classifier;
𝑘-NN classifier
with DTW

N/A; Construct a histogram binned
into seven equal-weight bins

SVM classifiers

was targeted by the attacker in [119]. One kind of kernel data was accessible to legitimate users, which logged the time

series of the hardware interrupt information can reveal previous activities. Specifically, the reported interrupts imply

the real-time running status of some specific hardware (i.e. touchscreen controller). However, accessing the similar

process-specific information had been continuously restricted since Android 6, and the interrupt statistics became

unavailable in Android 8 [119]. Different Android versions contain different kinds of process information which is
accessible to legitimate users without permissions under 𝑝𝑟𝑜𝑐 filesystem (𝑝𝑟𝑜𝑐 𝑓 𝑠). Thus, an app was developed in [119]
to search all accessible process information under 𝑝𝑟𝑜𝑐 𝑓 𝑠. The time series of these accessible data could distinguish the
event of interests including unlocked screen, the foreground app, and the visited website. Reconnaissance showed the
value of time series of data in 𝑝𝑟𝑜𝑐 𝑓 𝑠.

During the data collection, the interrupt time logs were collected by pressing and releasing the touchscreens in [26].

Specifically, for the versions prior to Android 8, a variety of interrupt time series recording the changes of electrostatic

field from touchscreen were gathered as one dataset for stealing the user’s unlock pattern. Another dataset was built

for stealing foreground apps’ information by recording the time series of starting the app from accessible sources like

interrupts from the Display Sub-System [26] and the virtual memory statistics [119]. Moreover, the time series of some

network process information fingerprinted online users. These fingerprints were gathered as the dataset for stealing

user’s web browsing information. Different sets of time series were prepared with respect to the information of different

user activities.

In terms of feature engineering, the attacker can analyze the process information on 𝑝𝑟𝑜𝑐 𝑓 𝑠 to study the characteristics
of the user’s unlock pattern, foreground app status, and the user’s browsing activity. The datasets were firstly processed

by deduplication, interpolation and increment computation. The distinct features of three datasets were constructed via
several methods such as segmentation, similarity calculation and DTW. An automatic method named 𝑡𝑠 𝑓 𝑟𝑒𝑠ℎ [20] was
utilized for feature extraction. Subsequently, for the stealing attack targeting unlock patterns, a Hidden Markov Model

(HMM) was used to model the attack to infer the unlock patterns through the Viterbi algorithm [33]. The evaluation

results showed that its success rate outperformed the random guessing significantly. Targeting at foreground apps,
the processed data was used to train a 𝑘-NN classifier. For the evaluation, the results showed that the classifier had
high accuracy, which achieved 87% on average in [26] and 96% in [119]. To reveal the user’s browsing activities, SVM

classifier was used to mount the attack. The results showed that both precision and recall values were above 80% in

[119]. Among these three attack scenarios, the consumption of battery and time were acceptable (less than 1% and

shorter than 6 minutes). The ML-based stealing attack showed its effectiveness with less consumption in time and

battery.

Stealing User Activities with iOS Side-channel Attack: The OS-level side-channel attacks were investigated in [151] on
iOS-based mobile devices (Apple), which stole user activities information. In iOS systems, one popular side-channel attack

Manuscript submitted to ACM

14

Miao et al.

vector of Linux system about the process information — 𝑝𝑟𝑜𝑐 𝑓 𝑠 — is inaccessible, which hinders the aforementioned
attacks from leaking the sensitive information. Attackers have actively looked for new resources to exploit in the

Operating System level.

In the reconnaissance phase, several attack vectors, which are feasible to Apple, were applied to perform cross-app

information leakage [151]. Specifically, three iOS vectors enabled apps accessing the global usage statics without requir-

ing any special permissions to bypass the timing channel: the memory information, the network-related information,

and the file system information. The attacker aimed to steal the user activities’ information (such as foreground apps,

visited websites and map searches) and in-app activities (such as online transactions). To collect data for an ML-based

attack, attackers manually collected several data traces for interesting events like foreground apps, website footprints

and map searches. To improve the performance of such an inference attack, the information collected from multiple

attack vectors was combined and fed into the ML models. Particularly, time series data from the targeted vectors were

exploited frequently. As for feature engineering and the stealing attacks, ML frameworks were utilized to exfiltrate the

user’s information from accessible vectors [151]. The changes of the time series reflected in the difference between two

consecutive data traces. The feature processing methods were applied to transform the sequences into the Symbolic

Aggregate approXimation (SAX) strings [105] and to construct the Bag-of-Patterns (BoP) of the sequences. In [151],

two ML-based attacks with a large amount of data were presented — classifying the user activities and detecting the

sensitive in-app activities. An SVM classifier was trained and tested for the former attack. The Viterbi algorithm [33]

with DTW was utilized for the latter attack. In terms of the evaluation of the first attack stealing three users’ activities,

the foreground app classification accuracy achieved 85.5%, Safari website classification accuracy reached 84.5%, and the

accuracy accomplished 79% in inferring map searches. The proposed attacks could be trained on the attacker’s device

and tested on other devices such as the victim’s devices. Meanwhile, the power consumption was acceptable with only

5% extra power used in an hour, while the attacks’ execution time was tolerable as well (within 19 minutes). In the

context of stealing user activities information, ML-based attacks exploited the OS-level data with time series analysis.
Protection using Privacy Mechanism: An attack exploiting the kernel process information [26] via decreasing the data’s
resolution was defended by [136]. A differential privacy (DP) mechanism was utilized to prevent the attackers from

gaining any useful storage information. Additionally, the noise was injected into the kernel data-structure values in order
to protect the contents within the 𝑝𝑟𝑜𝑐 𝑓 𝑠 with quantifiable security. Specifically, a generalized differential-privacy was
applied to quantify the distance between two series of 𝑝𝑟𝑜𝑐 𝑓 𝑠 information, which is essential to infer the information
about user activities. To retain the utility of 𝑝𝑟𝑜𝑐 𝑓 𝑠, the invariants of these noised fields were reestablished on the
noised output, in order to assist the applications that depend on them. This method is named as 𝑑𝑝𝑝𝑟𝑜𝑐 𝑓 𝑠, a modified
𝑝𝑟𝑜𝑐 𝑓 𝑠, preventing the attacker from utilizing the value reported via 𝑝𝑟𝑜𝑐 𝑓 𝑠 interfaces. As a result, 𝑑𝑝𝑝𝑟𝑜𝑐 𝑓 𝑠 showed a
reliable security guarantee resistant to the stealing attack with noise injection and generalization.

Two particular attacks were proposed to evaluate the protection approach in [136], including defending against the

keystroke timing attack and mitigating the website inference. The former attack collected the data from the kernel

layer to obtain the status of keystroke actions. When attacking the objective, an SVM classifier was trained to recognize
what keystrokes occurred [136]. The latter attack collected the data from the browser’ memory footprints in 𝑝𝑟𝑜𝑐 𝑓 𝑠
regarding the top-10 websites. By constructing a histogram of the counts of visited websites, records with bin counts

are extracted as features and the websites as labels. This dataset was used to train and test an SVM classifier. As for the
evaluation of applying the privacy-preserved kernel records in 𝑝𝑟𝑜𝑐 𝑓 𝑠, both of its security and utility were assessed.
From the security aspect, by infusing noise to the kernel data-structure values, the success rate of the keystroke timing

attack was reduced significantly (around 44%). Similarly, in the website inference scenario, the accuracy of this attack

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

15

declined significantly. From the utility aspect, several 𝑝𝑟𝑜𝑐 𝑓 𝑠 queries were used to test the top 𝑘 processes in 𝑑𝑝𝑝𝑟𝑜𝑐 𝑓 𝑠
which were influenced by noised kernel data-structure values. The relative errors were the modest (within 5%) for both
scenarios at a low level, while the rank accuracy on top 𝑘 processes for monitoring and diagnosis tests were at a high
level (all over 80% for top-10). However, the utility would decay if the number of queries grew and a large number of

noises were added. The effectiveness of noise injection method was unclear against attacks exploiting other stored

information. In summary, a defense proposed by [136] added noise to accessible data against the information inference

attacks leveraging the kernel data breach.

3.1.2 Stealing controlled user activities using sensor data. The stealing attack using sensor data should be studied

seriously by the defenders, not only from the application of effective ML mechanisms, but also from the popularity of

sensing enabled applications. Currently, there are an increasing number of apps using various sensor information on

smart devices to improve the efficiency and user experience [66, 67, 86, 104, 143]. The sensor information can reveal the

controlled information indirectly as demonstrated in this stealing attack, such as acoustic and magnetic data.

Table 4. Stealing Controlled User Activities using Sensor Data

Reference

Dataset for Experiment

[50]

Audio signature dataset

[117]

Sensor dataset

Description
Recorded with a phone put
within 4 inches of the printer
Sensor data collected benign
and malicious activities

Feature Engineering ML-based Attack Method
STFT,
noise normalization

A regression model

N/A

Markov Chain, NB, LMT,
(alternative algorithms e.g. PART)

Stealing Machine’s Activities with Sensor-based Attack: A side-channel attack was proposed by [50] on manufacturing
equipment exploiting sensor data collected by mobile phones, which revealed its design and the manufacturing process.

That is, the attacker managed to reconstruct the targeted equipment. As a result of reconnaissance, the security threat

of the manufacturing sector was indicated. In detail, the adversary placed an attack-enabled phone near the targeted

equipment like a 3D printer. The accessible acoustic and magnetic information reflected the product’s manufacturing

activities indirectly. During data collection phase, the acoustic and magnetic sensors embedded in the phone would

record audio and gain magnetometer data from the manufacturing equipment. The magnetometer data was transferred

into a type of acoustic information. Then these acoustic signal information was combined as the training dataset. Hence,

acoustic and magnetic data can be leveraged by the attack.

After the dataset was gathered, the ML-based attack in [50] was completed by feature engineering, attacking with

model training, and evaluation. The features were extracted from the audio signal’s frequency with the help of STFT

and the noise normalization [50]. With features constructed, the product’s manufacturing process could be inferred

by an ML model, especially for 3D printers. A regression algorithm was used to train the ML model for this attack.

In the experiments, the adversaries tested the effectiveness of the reconstruction of a star, a gun and an airplane by a

3D printer. All products were reconstructed except the airplane, which was more like a “fish mouth” [50]. As for the

difference of angles between original product and the reconstructed one, the differences of all angles were within one

degree in average, which was acceptable. A defense of this kind of attack was proposed by [50]. The protection method

obfuscated the acoustic leakage by adding the noise (i.e. play recordings) during production. Sensor-based attacks built

up model by analyzing the frequency of the manufacturing equipment, but noise injection can mitigate this attack to

some extent.

Context-aware Sensor-based Detector: A framework named 6thSense in [117] detects the malicious behavior of sensors
leaking the information about user activities. Three sensor-based threats were highlighted as — using sensors to send

Manuscript submitted to ACM

16

Miao et al.

a message to activate a malicious app, using sensors to leak information to any third party, and using the sensor to

steal data to deduce a particular device mode (like sleeping). The last two threats related to the attack intended to steal

user activities information via sensor data. The ML-based method was used to prevent these threats by detecting the

abnormal recordings of sensor information. The dataset was collected from nine sensors while 50 users conducted nine

activities with malicious and benign apps. The training samples were collected to present not only the user activities

leakage behaviors but also the usual changes within the sensors. Three ML algorithms listed in Table 4 were adopted to

create contextual models in the 6thSense framework to differentiate the benign or malicious sensor behaviors. According

to an empirical evaluation, 6thSense achieved high classification performance with over 96% accuracy. ML models were

trained to detect any sensor’s malicious behavior caused by sensor-based attacks.

3.1.3 Summary. ML-based attacks in Section 3.1 steal user activities information from operating systems. According

to the data sources, there are two kinds of attacks — using kernel data and using sensor data. Kernel data reveals some

system-level behaviors of the target system, while sensor data reflects the system’s reactions on its specific functionality

used by users [26]. The kernel data is analyzed by the adversary from a time dimension, while the sensor data is

exploited with frequency analysis.

Countermeasures: Regarding the protection mechanism, differential privacy is an important method for the attacks
stealing user activities information. In [136, 150], an example applied the noise to an accessible data source (like Android

kernel log files). Another kind of solution is to restrict access to accessible data [151]. It is also effective to build a model

to detect potential stealing threats like in [117]. The in-depth research in protecting against user activities information

can explore the differential privacy appliance or a management system design for kernel files and sensor data. Noise

injection and access restriction are two effective protections, and the detection can alert the stealing attack.

3.2 Controlled ML Model Related Information

ML model related information consists of the model description, training data information, testing data information,

and testing results. In this subsection, the ML model and users’ uploaded training data are the targets, which are stored

in the cloud. By querying the model via MLaaS APIs, the prediction/classification results are displayed. The model

description and training data information are controlled, otherwise, it is easy for an attacker to interpret the victim’s

query result. As most of ML services charge users per query [41, 92, 112], this kind of attack may cause huge financial

losses. Additionally, several ML models including neural networks are suffered from adversarial examples. Adding

small but intentionally worst-case perturbations to inputs, adversarial examples result in the model predicting incorrect

answers [39]. By revealing the knowledge of either the model’s internal information or its training data, the stealing

attack can facilitate the generation of adversarial examples [98, 101]. The generalized attack in this category is illustrated

in Fig. 5. Leveraging the query inputs and outputs, the model description can be stolen by using a model extraction

attack or a hyperparameter stealing attack, and the training samples can be stolen by using the model inversion attack,

GAN attack, membership inference attack, and property inference attack.

3.2.1 Stealing controlled ML model description. It is important to protect the confidentiality of ML models online.

If the ML model’s knowledge description was stolen, the profit of the MLaaS platform may diminish because of its

pay-per-query deployment [125]. If spam or fraud detection are based on ML models [9, 51, 81, 120], understanding the

model means that adversaries can evade detection [81]. A specific ML model is defined by two important elements

including ML algorithm’s parameters and hyperparameters. Parameters are learned from the training data by minimizing

the corresponding loss function. Additionally, hyperparameters aid to find the balance within objective function between

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

17

Fig. 5. The ML-based stealing attack against ML model related information. In this category, ML-based attacks aim at stealing the
training samples or the ML model. Stealing the controlled training sample attacks use an ML model to determine whether the input
sample is contained in the target training set.

its loss function and its regularization terms, which cannot be learned directly from the estimators. Since the model

is controlled, its parameters and hyperparameters should be deemed confidential by nature. Stealing these model

descriptions, the main approaches are equation-solving, patch-finding and linear least square methods.

Table 5. Stealing Controlled ML Model Description

Reference

[125]

[101]

[131]

[98]

Dataset for Evaluation
Circles, Moons, Blobs,
5-Class [125];
Steak Survey [126],
GSS Survey [118],
Adult (Income/race) [126],
Iris [126],
Digits [107],
Breast Cancer [126],
Mushrooms [126],
Diabetes [126]

Description
Synthetic, 5,000 with 2 features,
Synthetic,1000 with 20 features,
331 records with 40 features,
16,127 records with 101 features,
48,842 records with 108/105 features,
150 records with 4 features,
1,797 records with 64 features,
683 records with 10 features,
8,124 records with 112 features,
768 records with 8 features

MNIST [69],
GTSRB [121]

Diabetes [126],
GeoOrig [126],
UJIIndoor [126];
Iris [126],
Madelon [126],
Bank [126]
MNIST [69]

70,000 handwritten digit images,
49,000 traffic signs images

442 records with 10 features,
1,059 records with 68 features,
19,937 records with 529 features;
100 records with 4 features;
4,400 records with 500 features;
45,210 records with 16 features
70,000 handwritten digit images

Targeted ML Model

Attack Methods

Logistic Regression;
Decision Tree;
SVM;
Three-layer NN

Equation-solving
attack; Path-finding
attack

DNN; SVM; 𝑘-NN;
Decision Tree;
Logistic Regression

Jacobian-based Dataset
Augmentation

Regression algorithms;
Logistic regression
algorithms; SVM; NN

Equation solving

NNs

Metamodel methods

Stealing Parameters Attack: Model extraction attacks targeting ML models of the MLaaS systems were described in
[125]. The goal of the model extraction attacks was constructing the adversary’s own ML model which closely mimics

the original model on the MLaaS platform. That is, the constructed ML model can duplicate the functionality of the

original one. During the reconnaissance, MLaaS allows clients to access the predictive model in the black-box setting

through API calls. That is, the adversary can only obtain the query results. Most MLaaS provides information-rich

query results consisting of high-precision confidence values and the predicted class labels. Adversaries can exploit this

information to perform the model extraction attack. The first step was collecting confidence values with query inputs.

Feature extraction needs to map the query inputs into a feature space of the original training set. Feature extraction

methods were applied for categorical and numerical features as listed in Table 5. Equation-solving and patch-finding

attacks were used to calculate the objective function of the targeted model. Three popular ML models were targeted

listed in Table 5, while two online services namely BigML [10] and Amazon ML [112] were compromised as case studies.

The key processes of model extraction attacks include query input design, confidence values collection, and attack with

equation-solving and patch-finding.

Manuscript submitted to ACM

18

Miao et al.

Stealing the model’s parameters, the equation-solving attack and patch-finding attack are illustrated in detail.

Regarding the attack mentioned in [125], the equation-solving attacks can extract confidence values from all logistic

models including logistic regression and NNs, whereas the patch-finding attacks work on decision trees model. The

equation solving was based on the large class probabilities for unknown parameters and then calculated the model.

Specifically, the objective function of the targeted ML model was the equation which adversaries aimed to solve. With

several input queries and their predicted probabilities, the parameters of the objective function were calculated. The

patch-finding attack exploited the “ML API particularities” to query specific inputs to traversal the decision trees [125].

A path-finding algorithm and a top-down approach helped locate the target model algorithm to reveal paths of the

tree. In this way, the detailed structure of the targeted decision tree classifier was reconstructed. The performance

of such an attack was evaluated based on the extraction accuracy. The online model extraction attack targeted the

decision tree model which was set up by the users on BigML [10]. The accuracy was over 86% irrespective of the

completeness of queries. In another case study targeting the ML model on Amazon services, the attacker reconstructed

a logistic regression classification model. The results showed that the cost of this attack was acceptable in terms of time

consumption (less than 149s) and the price charged ($0.0001 per prediction). The model was successfully learned by

calculating the parameters.

Apart from reconstructing the exact model parameters, another model extraction attack reveals the model’s internal

information by building a substitute model in [101]. Herein, the substitute model shares similar decision boundaries

with the target model. During the reconnaissance, adversaries can only obtain labels predicted by the target model

with given inputs. To train this substitute model, a substitute dataset is collected using a synthetic data generation

technique named Jacobian-based Dataset Augmentation with a small initial set [101]. Specifically, the ground truth of a

synthetic data has the label predicted by the target model, but the architecture is selected based on the understanding

of the classification task. The best synthetic training set is determined by the substitute model’s accuracy and the

similarity of decision boundaries. To approximate the target model’s boundaries, the Jacobian matrix is used to identify

the directions of the changes in the target model’s output. Hence, the model can be reconstructed as a substitute model.
Stealing Hyperparameters Attack: Stealing hyperparameters in the objective function of the targeted MLaaS model
may result in gaining financial benefits [131]. The investigated MLaaS models can be regarded as black-box providing

query results only, like Amazon ML [112] and Microsoft Azure Machine Learning [92]. By analyzing the model’s

training process, the attacker successfully learned the model’s parameters when the objective function reached its

minimum value. That is, the gradient of objection function at the model parameters should be the vector whose entries

are all close to zeros. Hence, the hyperparameters were learned covertly with a system of linear equations, when the

gradient was set to vector of zero.

A threat model was proposed by [131] where the attacker acted as a legitimate user of MLaaS platform. Some

popular ML algorithms used by the platform were analyzed in Table 5. That is, the attacker knew the ML algorithm

in advance. Given the learned model’s parameters, the attacker set the gradient vector of the objective function of

the non-kernel/kernel algorithm. By solving this equation with the linear least square method, the attacker found the

hyperparameters. In some black-boxed MLaaS models, the attacker applied the model parameter stealing attacks relied

on equation-solving in [125] to learn the parameters as an initial step. Thus, even though the parameters were hidden,

the attacker could steal the hyperparameters. Therefore, the target model was reconstructed successfully.

To evaluate the effectiveness of this stealing hyparameters attack [131], several real-world datasets listed in Table 5

were used. Additionally, a set of hyperparameters whose span covered a large range were predefined. Apart from these,

a scikit-learn package was applied to implement different ML models and worked out the values of each hyperparameter.

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

19

For empirical evaluation, relative mean square error (MSE), relative accuracy error (AccE), and relative estimation error

(EE) were applied. The results showed a high accuracy of attack performance with all estimation errors were below 10%.

The good performance indicated that the attacker successfully stole the target model.

Regarding a target model as a black-box, its hyperparameters can also be learned by building another metamodel

which takes various classifiers’ input and output pairs as training data [98]. Firstly, by observing outputs of the

target model with given inputs, a diverse set of white-box models need to be trained by varying values of various

hyperparameters (i.e. activation function, the existence of dropout or maxpooling layers, and etc). More importantly,

these white-box models are expected to be similar to the target model. The training set of the metamodel can be

collected by querying inputs over these white-box models, while the ground truth label should be the hyperparameter’s

value used by the corresponding white-box model. Afterwards, by querying the target model, its hyperparameter

can be predicted given its output to the metamodel. Stealing hyperparamters attack is complementary to the stealing

parameters attack.

3.2.2 Stealing controlled ML model’s training data. Another type of controlled information about MLaaS product

is the training data. Training data is not only useful to construct the model using ML algorithms provided by an MLaaS

platform, but also sensitive as the records can be private information [34, 35]. For example, a user’s health diagnostic

model is trained by personal healthcare data [116]. Hence, the confidentiality of the model’s training data should be

protected. The ML-based stealing attacks include the model inversion attack, GAN attack, membership inference attack,

and property inference attack. Moreover, two protections are demonstrated: One uses adversarial regularization against

membership inference attack, while another utilizes count featurization for protecting the models’ training data.

Table 6. Stealing Controlled ML Model’s Training Data. A method was proposed in [95] to prevent training set leakage against the
membership inference attack [110] which provides a simple attack without using shadow models. Because the methods in [70, 100]
were proposed for only protecting training data, the feature engineering and methods for the ML-based attack are omitted.

Reference

[34]

[49]

[116]

[110]

[38]

[91]

[95]

Dataset for Experiment
FiveThirtyEight survey,
GSS marital happiness survey
MNIST [69],
AT&T [111]
CIFAR10 [65],
CIFAR100 [65],
Purchases [52],
Foursquare [140],
Texas hospital stays [47],
MNIST [25],
Adult (income) [126]
Include 6 sets in [116],
News [53],
Face [68]
Adult (income) [126],
MNIST [69],
CelebFaces Attributes [80],
Hardware Performance Counters
Face [68],
FaceScrub [96],
PIPA [149],
Yelp-health, Yelp-author [141],
FourSquare [140], CSI corpus [129]
CIFAR100 [65],
Purchase100 [52],
Texas100 [47]

Description
553 records with 332 features,
16,127 records with 101 features
70,000 handwritten digit images,
400 personal face images
6,000 images in 10 classes,
60,000 images in 100 classes,
10,000 records with 600 features,
1,600 records with 446 features,
10,000 records with 6170 features,
10,000 handwritten digit images,
10,000 records with 14 attribute
Same as above cell,
20,000 newsgroup documents in 20 classes,
13,000 faces from 1,680 individuals
299,285 records with 41 features,
70,000 handwritten digit images,
more than 200K celebrity images,
36,000 records with 22 features
13,233 faces from 5,749 individuals,
76,541 faces from 530 individuals,
60,000 photos of 2,000 individuals,
17,938 reviews, 16,207 reviews,
15,548 users in 10 locations, 1,412 reviews
60,000 images in 100 classes,
197,324 records with 600 features,
67,330 records with 6,170 features

Feature Engineering

N/A

Features learned
with DNN

ML-based Attack Method
Decision Tree,
Regression model
Convolutional Neural
Network (CNN) with GAN

Regarded shadow model
resulted as features and
label records as in/out

NN

Regarded shadow model
resulted as features and
label records as in/out

Random Forest,
Logistic Regression,
Multilayer perceptron

Neuron sorting ,
Set-based representation

NN

N/A

Logistic regression,
gradient boosting,
Random Forests

Regarded shadow model
resulted as features and
label records as in/out

NN

Model Inversion Attack & Defense: The model inversion attack was developed by [34] via conducting the commercial
MLaaS APIs and leveraging confidence information with predictions. Though another model inversion attack proposed

in [35] leaked the sensitive information from ML’s training set, the attack could not work well under other settings

Manuscript submitted to ACM

20

Miao et al.

e.g. the training set has a large number of unknown features. However, the attack proposed in [34] aimed to be

applicable across both white-box setting and black-box setting. For the white-box setting, an adversarial client had

a prior knowledge about the description of the model as the APIs allowed. For the black-box setting, the adversary

was only allowed to make prediction queries on ML APIs with some feature vectors. Considered as the useful data

for the attack, the confidence values were extracted from ML APIs by making prediction queries. The attacks were

implemented in two case studies — inferring features of the training dataset, and recovering the training sample of

images. The model inversion attack targets the ML model’s training data under both settings.

The first attack was inferring sensitive features of the inputs from a decision tree classifier. BigML [10] was used to

reveal the decision tree’s training and querying routines. With query inputs with different features and the corresponding

confidential values, the attacker in [34] accessed marginal priors for each feature of the training dataset. For the black-

box setting, the attacker utilized the inversion algorithm [35] to recover the target’s sensitive feature with weighted

probability estimation. A confusion matrix was used to assess their attacks. For the white-box setting, the white-box

with counts (WBWC) estimator was used to guess the feature values. Evaluated with GSS dataset, the results showed

that white-box inversion attack on decision tree classifier achieved 100% precision, while the black-box attack achieved

38.8% precision. Additionally, the attack in the white-box setting received 32% less recall than that in the black-box

setting. Comparing to black-box attacks, white-box inversion attacks show a significant advance on feature leakage,

especially in precision.

The second attack was recovering the images from an NN model from a facial recognition service accessed by

APIs. Learning the training samples was required to steal the recognition model firstly. Two specific model inversion

attacks were proposed in [34], including reconstructing victim’s image with a given label, and determining whether the

blurred image existed in training set. Specifically, the MI-Face method and Process-DAE algorithm were used to perform

the attacks. Herein, the query inputs and confidential values were used to refine the image. The best reconstruction

performance from evaluation was 75% overall accuracy and 87% identification rate. Moreover, the attacker employed an

algorithm named maximum a posterior to estimate the effectiveness. The evaluation results showed that the proposed

attacks enhanced the inversion attack efficacy significantly comparing to the previous attack [35]. The training images

were recovered accurately.

Stealing the Training Data of Deep Model with GAN: An attack against the privacy-preserving collaborative deep
learning was designed to leak the participants’ training data which might be confidential [49]. A distributed, federated,

or decentralized deep learning algorithm can process each users’ training set by sharing the subset of parameters

obfuscated with differential privacy [1, 115]. However, the training dataset leakage problem had not been solved by

using the collaborative deep learning model [115]. An adversary can deceive the model with incorrect training sample

to deduce other participants to leak more local data. Then by leveraging the learning process nature, the adversary

can train a Generative Adversarial Network (GAN) for stealing others’ training samples. The GAN attack targets the

collaborative deep learning.

Specifically, the GAN simulated the original model in collaborative learning process to leak the targeted training

records [49]. During the reconnaissance phase, an adversary pretended as one of the honest participants in collaborative

deep learning, so that the adversary could influence the learning process and induce the victims to release more

information about the targeted class. To collect the valuable data, the adversary did not need to compromise the central

parameter server instead of inferring the meaningful information of that class based on the victim’s changed parameters.

In addition, as a participant was building up the targeted model, part of the training samples were known to the

adversary. The fake training dataset could be sampled randomly from other datasets. The true training dataset and a

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

21

fake training dataset were collected to train the discriminator of the GAN using the CNN algorithm. The outputs of this

discriminator and another fake training dataset were used to train the generator of the GAN using CNN. Since the

feature of the training data was known by default, the adversary sampled the targeted training data with the targeted

label and random feature values. This fake sample was fed into the generator model. The adversary modified the feature

values of this fake sample until the predicted label was the targeted label. The final modification of this fake sample was

regarded as the target training sample. In the experiments, the GAN attack against collaborative learning was evaluated

with MNIST [69] and AT&T datasets [111] as inputs. Comparing with model inversion attack, the discriminator within

GAN attack reached 97% accuracy and recovered the MNIST image trained in the collaborated CNN clearly. In a word,

the GAN attack trained the discriminator and generator to steal the training data.

Membership Inference Attack: Learning a specific data record was targeted by [116], which was the membership of
the training set of the targeted MLaaS model. Since the commercial ML model only allowed black-box access provided

by Google and Amazon, not only the training data but also the training data’s underlying distribution were controlled.

Though the training set and corresponding model were unknown, the output based on a given input revealed the

model’s behavior. By analyzing such behaviors, adversaries found that the ML model behaved differently on the input

that they trained compared to the input which was new to the model. Therefore, according to this observation, an attack

model was trained, which could recognize such differences and determine whether the input data was the member

of targeted training set or not. The attack is intended to recognize the model’s behavior testing with target training

sample.

The attack model was constructed by leveraging a shadow training technique [116]. Specifically, multiple “shadow

models” were built to simulate the targeted model’s behavior, which informed the ground truth of membership of

their inputs. All “shadow models” applied the same service (i.e. Amazon ML) as the targeted model. In addition, the

training data that the adversary used can be generated by the model-based synthesis and statistic-based synthesis

methods. The generated dataset shared the similar distribution to the object model’s training set, while the testing set

was disjoint from training set. Querying these “shadow models” with the training sets and testing sets, the prediction
results were added a label of 𝑖𝑛 or 𝑜𝑢𝑡. These records could be collected as the attack model’s training set. Then the
adversary utilized the built binary classifier to learn a specific data record by determining whether it was 𝑖𝑛 or 𝑜𝑢𝑡
of the training set for MLaaS model. Such an offline attack was difficult to be detected, while MLaaS system would

consider the adversary as a legitimate user since the adversary was just querying online. Shadow models were trained

to produce the inputs for the membership inference attack.

For this membership inference attack evaluation [116], several public datasets were used and listed in Table 6. Three

targeted models were constructed by Google Prediction API, Amazon ML, and CNN, respectively. The evaluation metrics

used by the adversaries were accuracy, precision, and recall. According to the evaluation results, Google Prediction API

was suffered from the biggest training data leakage due to this attack. The accuracy of the attack model was above the

baseline 50% (random guessing result) in all experiments, while the precision were all over 60%, and recall was close to

100%. The membership inference attack learned the training sample effectively.

In 2019, membership inference attack was further studied in [110] to make it broadly applicable at a low cost.

Specifically, three assumptions mentioned in [116] are relaxed including using multiple shadow models, synthesizing

the dataset from the similar distribution of the target model’s training set, and the knowledge of the target model’s

learning algorithm. The results show that the performance of these attack will not be affected with only one shadow

model trained with a dataset from other distributions. The results of using different classification algorithms on one

shadow model are not promising. However, by combining a set of ML models trained with various algorithms as

Manuscript submitted to ACM

22

Miao et al.

one shadow model, the performance of membership inference attack can be tolerable (above 85% in precision and

recall). Herein, the attack is based on an assumption that one model of the model set is trained with the learning

algorithm used by the target model. Furthermore, by selecting a threshold of the posterior results to determine the

input data’s membership, even shadow model is not needed for the membership inference attack. Therefore, the scope

of membership inference attack is enlarged.

Property Inference Attack: Different from learning a specific training record, the property inference attack targets
at the properties of training data that the model producer unintended to share. The target model was defined in [38]

as a white-box Fully Connected Neural Networks (FCNNs) with the aim to infer some global properties such as a

higher proportion of women. To launch this attack and take a model as input, a meta-classifier was built to predict

whether the global property exists in this model’s training set or not. Above all, several shadow models were trained on

a similar dataset using similar training algorithms to mimic the target FCNNs. During the feature engineering phase,

the meta-training set was formed in [38] by applying set-based representation instead of using a flattened vector of all

parameters [5]. Specifically, a set-based representation is learned using the DeepSets architecture [144] in four steps —

1) flattening each nodes’ parameters from all hidden layers, 2) obtaining a node representation with node processing

function based on the target property, 3) summing a layer representation with layer summation, and 4) concatenating

these layer representations as a classifier representation. The accuracy of this attack reached 85% or over on binary

income prediction, smile prediction or gender classification task. This property inference attack against white-box

FCNNs is effective to steal training set information.

In collaborative learning, leaking unintended features about participants’ training data is another kind of property

inference attack [91]. Instead of global properties, the unintended feature they targeted is held for a certain subset of

training set or even independent of the model’s task. For example, the attacker infers black face property of training data

while learning a gender classifier in a federated manner. In the reconnaissance process, the adversary as a participant

can download the current joint model for each iteration of the collaborative learning. The aggregated gradient updates

from all participants are computed, thereafter, the adversary can learn the aggregated updates other than his own

updates [89]. Since the gradients of one layer are calculated based on this layer’s features and the previous layer’s error,

such aggregated updates can reflect the feature values of other participants’ private training set. After several iterations,

these updates are labeled with the targeted property and fed to build a batch property classifier. Given model updates

as inputs, this classifier can predict corresponding unintended features effectively with most precision values larger

than 80%. Therefore, the collaborative learning is vulnerable to property inference attack as well.

Protection using Adversarial Regularization: A protection for black-box MLaaS models against the membership
inference attack was introduced in [95]. As described in [116], membership inference attack could learn whether a

data sample was a member of targeted model’s training set, even if the adversary only observed the queried output

of a cloud service. Regularizing the ML model with L2-norm regularizers was a popular mitigation method [34, 116],

which was not considered to offer a rigorous defense. On the other hand, researchers concluded that differential privacy

mechanism prevented this information leakage by sacrificing the model’s usability which was expendable. To guarantee

the confidentiality and privacy of training set rigorously, there is a need for a privacy mechanism more powerful than

regularization and differential privacy.

A defender’s objective was analyzed firstly by formalizing membership inference attack in [95]. Precisely, the input

of an inference model consisted of a testing data for the targeted classifier, its prediction vector, and a label about

membership. An adversary aimed to maximize his inference gain, which was effected by the targeted training dataset

and a disjoint dataset for reference attack training. Therefore, the defender intended to minimize the adversary’s

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

23

inference gain while minimizing the loss of targeted classifier’s performance. That is, the defender enhanced the security

of ML model by training it in an adversarial process. The inference gain as an intermediate result was regarded as the

classifier’s regularizer to revise the ML model with several training epochs. An adversarial regularization was used in

training the classifier.

To evaluate the defense mechanism, three common datasets were used in membership inference attack [116]. The

classifier’s loss was calculated when the attacker’s inference gain reached the highest score [95]. The results showed that

the classification loss reduced from 29.7% to 7.5% with defense comparing to that without defense for the Texas model,

which could be insignificant. For the membership inference attack, the accuracy performance targeted at protected ML

model is around 50%, which was close to the random guessing. In a word, the protection model using the adversarial

regularization enhanced the confidentiality and privacy of its training data.

The protection proposed in [95] was powerful against the membership inference attack. However, its effectiveness in

protecting training data leaked by other attacks remains unknown. Additionally, it did not discuss whether adversarial

regularization can protect the white-box MLaaS models from membership inference attack or not. Moreover, this

defense method failed against the online attack which steals the training data of deep model using a GAN model [49].
Protection using PATE: To protect the training set of an ML model generally, Private Aggregation of Teacher Ensembles
(PATE) was proposed by [100]. Specifically, PATE prevents training set information leakage from model inversion attack,

GAN attack, membership inference attack, and property inference attack. Two kinds of models are trained in this general

ML strategy including “teacher” and “student” models. Teacher models are not published and trained on sensitive data

directly. Splitting sensitive dataset into several partitions, several teacher models are trained using learning algorithms

independently. These teacher models are deployed as an ensemble making predictions in a black-box manner. Given an

input to these teachers, aggregating their predictions as a single prediction depends on each teacher’s vote. To avoid that

teachers do not have an obvious preference in aggregation, Laplacian noise is added to vote counts. Obtaining a set of

public data without ground truth, the student will label them by querying the teacher models. Then the student model

can be built in a privacy-preserving manner by transferring the knowledge from teachers. Moreover, its variant PATE-G

uses the GAN framework to train the student model with a limited number of labels from teachers. In conclusion, the

PATE framework provides a strong privacy guarantee to the model’s training set.

Protection using Count Featurization: A limited-exposure data management system named Pyramid enhanced the
protection for organizations’ training data storage [70]. It mitigated the data breaches problem by limiting widely

accessible training data, and constructing a selective data protection architecture. For emerging ML workloads, the

selective data protection problem was formalized as a training set minimization problem. Minimizing the training set

can limit the stolen data.

During the prior data management [124], only in-use data were retained in the accessible storage for the ML training

periodically. However, the whole dataset would be exposed continuously from updated accessible storage [70]. For this

concern, distinguishing and extracting the necessary data for effective training was the key process. The workflow of

Pyramid kept accessible raw data within a small rolling window. The core method named “count featurization” was

used to minimize the training set. Specifically, the counts summarized the historical aggregated information from the

collected data. Then Pyramid trained the ML model with the raw data featurized with counts in a rolling window. The

counts were rolled over and infused with differential privacy noise to preserve the training set [29]. In addition, the

balance between training set minimization and model performances (accuracy and scalability) should also be considered.

Three specific techniques were applied to retrofit the count featurization for data protection. The infusion with the

weighted noise added less noise to noise-sensitive features of the training set. Another technique, called unbiased

Manuscript submitted to ACM

24

Miao et al.

private count-median sketch, solved the negative bias problem arising from the noise infusion, while the automatic

count selection found out useful features automatically and counted them together. For training data protection, count

featurization was used to remain necessary data within data storage. Pyramid prevented the attacker from learning the

extracted information from the training set.

Table 7. Categories of Stealing ML related information attacks from three perspectives (info: information). As for attack targets, two
types of information may be stolen — model internal information and training set information. From attack surfaces, attacks may
occur during either model’s training phase or inference phase. Considering the attacker’s capability, the ML model usually allows
either the black-box access or the white-box access. The first category is used for this subsection’s organization.

Attack Type

Model extraction attack [125]
Model extraction attack [101]
Hyperparameter stealing attack [131]
Hyperparameter stealing attack [98]
Black-box inversion attack [34]
White-box inversion attack [34]
GAN attack [49]
Membership inference attack [116]
Membership inference attack [110]
Property inference attack [38]
Property inference attack [91]

Attack Targets

Attack Surfaces

Model Info
YES
YES
YES
YES
no
no
no
no
no
no
no

Training Set Info
no
no
no
no
YES
YES
YES
YES
YES
YES
YES

Training Phase
no
no
no
no
no
no
YES
no
no
no
YES

Inference Phase
YES
YES
YES
YES
YES
YES
no
YES
YES
YES
no

Attacker’s Capabilities
Black-box Access White-box Access

YES
YES
YES
YES
YES
no
no
YES
YES
no
no

no
no
no
no
no
YES
YES
no
no
YES
YES

Table 8. Attack’s prior knowledge under black-box access and white-box access. The black-box access allows the users to query the
model and obtain prediction outputs which include the predicted label and confidence value. The white-box access allows the users
to access any information of its model which includes predicted label, predicted confidence, parameters, and hyperparameters.

Model’s Information
Predicted Label
Predicted Confidence
Parameters
Hyperparameters

Black-box Access White-box Access

YES
YES
NO
NO

YES
YES
YES
YES

3.2.3 Summary. In Section 3.2, ML-based stealing attacks against model related information target at either model

descriptions or model’s training data. In addition to this category, as shown in Table 7, the other two ways focus on

attacks at training/inference phase and with black-/white-box access [102]. Model extraction attacks [101, 125] and

hyperparameter stealing attacks [98, 131] leak the model’s internal information happened at inference phase. Attackers

steal model’s training data mostly at inference phase, except the GAN attack [49] and the property inference attack

[91] which happen at training phase of collaborative learning. When attacking during training phase, attackers with

white-box access to the model can exploit its internal information. As shown in Table 8, the white-box access allows

attackers to have more prior knowledge than black-box, which results in high performance of the stealing attack [34].

On the other hand, black-box attacks can be more applicable in the real world. Except [110], most of the attackers in

this category under black-box access know the learning algorithm of the target model [34, 71, 98, 101, 125, 131].

Countermeasures: Concerning the ML pipeline, the protection methods will be applied in data preprocessing phase,
training phase, and inference phase respectively. Differential privacy noise used in the first phase can build a privacy-

preserving training set [70]. Differential privacy is the most common countermeasures to defend against the stealing

attack, however, it alone cannot prevent the GAN attack [49]. Differential privacy, regularization, dropout, and rounding

techniques are popular protections at the training and inference phases. At the training phase, differential privacy on

parameters cannot resist the GAN attack [49], while rounding parameters is ineffective against hyperparameter stealing

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

25

Fig. 6. The ML-based stealing attack against authentication information — keystroke information and secret keys. After reconnoitering
and querying, attackers targeting at keystroke information and secret keys interact with the target system to collect data, which
refers to the active collection. The attack involved active collection shares a similar workflow as Fig. 4 depicted.

attack [131]. Regularization may be effective depending on the targeted algorithm towards hyperparameter stealing

attack [131].

3.3 Controlled Authentication Information

The authentication information is one of the most important factors in security while accessing the information from

services or mobile applications. Thus, users’ authentication information is always stored with protected mechanisms.

In Section 3.3, the controlled authentication information mainly contains keystroke data, secret keys and password

data. As shown in Fig. 6 and Fig. 7, classification models or probabilistic models are trained to steal the controlled

authentication information. Additionally, the protections of controlled authentication information stealing attacks are

summarized in this subsection.

3.3.1 Stealing controlled keystroke data for authentication. The dataset collected from a device’s sensor infor-

mation can be used to infer the controlled keystroke information as depicted in Fig. 6. The keystroke data contains

the information about user authentication data, especially for keystroke authentications [4, 40, 64, 135]. Leveraging

the acceleration, acoustic and video information, we review the attacks stealing these keystroke information and the

countermeasures.

Table 9. Stealing Controlled Keystroke Data for Authentication

Paper

Dataset for Experiment

[79]

Acceleration data set

[122]

Video recordings set

Description
Consecutive vectors
with 26 labels
Image resolution and
frame rate

Feature Engineering
FFT & IFFT filter, Movement capturing,
Optimization with change direction
Extract from selected AOIs’ motion
signals for motion patterns

ML-based Attack Method
Random Forest;
𝑘-NN; SVM; NN

multi-class SVM

Keystroke Inference Attack: Several types of sensor information can be utilized to steal keystroke authentication
information while targeting at keyboard inputs. In the process of reconnaissance, [79] found that sensor data from the

accelerometer and microphone in a smartwatch was related to user keystrokes. Since the smartwatch was worn on the

user’s wrist, the accelerometer data reflected the user’s hand movement. Therefore, the user’s inputs on keyboards can

be inferred. The authors presented the corresponding practical attack based on this finding. Adversaries collected the

accelerometer and microphone data when inputting the keystrokes in a keyboard. By leveraging the acceleration and

acoustic information, adversaries were able to distinguish the content of the typed messages. In addition, two kinds

Manuscript submitted to ACM

26

Miao et al.

of keyboards were targeted: a numeric keypad of POS terminal and a QWERTY keyboard. The datasets about sensor

information were collected for the inference attack.

During the feature engineering phase, adversaries manually defined the 𝑥-axis and 𝑦-axis as two movement features
of acceleration data, and the frequency features were extracted from acoustic data. Then FFT was employed to filter

the linear noise and high-frequency noise. Applying the ML strategies into the attacks, keystroke inference models
were set up to reduce the impact caused by the noise within sensor data [79]. Specifically, the modified 𝑘-NN algorithm
cooperated with an optimization scoring algorithm was applied to enhance the accuracy of their inference attack.

Thereafter, the typed information within these two keyboards were leaked, including users’ banking PINs and English

texts. The attack inferred the keystroke information containing authentication information.

Regarding the evaluation, the results showed that the keystroke inference attack on the numeric keypad had

65% accuracy in leaking banking PINs among the top 3 candidates [79]. Unlike the previous work in decoding PINs

[13, 93, 137], any devices containing the POS terminal could be compromised by this attack. For the attack targeted

at QWERTY keypads, comparing to previous work [6, 8], a notable improvement had to be achieved to find the

word correctly, where the accuracy improved by 50% with strong allowance to acoustic noise. In the end, several

mitigation solutions against the keystroke inference attack were provided [79]: restricting the access to accelerometer

data; limiting the acoustic emanation; and adding the permissions in accessing the sensors which should be managed

dynamically according to the context. The attack inferred the keystroke information accurately and was mitigated with

the restrictions.

Video-Assisted Keystroke Inference Attack: Apart from accelerometer and microphone, the video records are another
kind of sensor information for attackers to infer the keystroke authentication information. An attack named VISIBLE,

provided by [122], leaked the user’s typed inputs leveraging the stealthy video recording the backside of a tablet. The

attack scenario assumed that the targeted tablet was placed on a tablet holder, and another two types of soft keyboards

were used for inputs including alphabetical and PIN keyboards. The dataset for the ML-based attack contained the

video of the backside motion of a tablet during the text typing process. In the process of feature engineering, the

area of interests (AOIs) were selected and decomposed. The tablet motion was analyzed with amplitude quantified.

Then, the features were extracted from temporal and spatial domains. As an ML-based attack, a multi-class SVM was

applied to classify various motion patterns to infer the input keystrokes. VISIBLE exploited the relationship between a

dictionary and linguistic to refine such inference results. The experiments showed that the accuracy of VISIBLE in

leaking input single keys, words and sentences, outperformed the random guess significantly. Particularly, the average

accuracy scores of the aforementioned inference attacks were above 80% for the alphabetical keyboard and 68% for

the PIN keyboard. The countermeasures of this attack include providing no useful information for the video camera,

randomizing the keyboards layout, and adding noise when accessing the video camera. The attack leveraged the video

information can also infer keystroke authentication information very accurately.

3.3.2 Stealing controlled secret keys for authentication. Secret keys are used to encrypt and decrypt sensitive

messages [18, 63, 139]. Reconstructing the cryptographic keys means that one host is authenticated to read the message

in some cases [46, 56, 106]. However, an adversary has the ability to deduce the sensitive information like cryptographic

keys, by understanding the changes in the state of shared caches [42]. In this part, the attack stealing controlled secret

key information is surveyed via analyzing the state of targeted cache set.

Stealing secret keys with TLB Cache Data: Due to the abuse of hardware translation look-aside buffers (TLBs), secret
key information can be revealed by the adversary via analyzing the TLB information [42]. The targeted fine-grained

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

27

Table 10. Stealing Controlled Secret Keys for Authentication (Information: info)

Reference

Dataset for Experiment

Description

[42]

300 observed TLB latencies

Collect from TLB signals

Feature Engineering
Encode info using a
normalized latencies vector

[152]

500,000 Prime-Probe trials

Number of absent cache
lines + cache lines available

N/A

ML-based Attack Method

SVM classifier

NB classifier

information of user memory activities (i.e. cryptographic keys) was safeguarded in the controlled channels like cache

side channels [43, 75]. During the reconnaissance, the legitimate user accesses the shared TLBs, which reflects victims’

fine-grained cache memories. In detail, the victim’s TLB records could be accessed by other users using CPU affinity

system calls or leveraging the same virtual machine. Adversaries reverse-engineered unknown addressing functions

which mapped virtual addresses to different TLB sets, in order to clarify the CPU activities from the TLB functions. To

design the data collection, the adversary monitored the states of shared TLB sets indicating the functions missed or

performed by the victims. Without privileged access to properties of TLB information (i.e. TLB shootdown interrupts),

adversaries timed the accesses to the TLB set and measured the memory access latency, which indicated the state of

a TLB set. Instructing the targeted activities with a set of functions statements, the adversary accessed the TLB data

shared by the victim and collected the corresponding temporal information as a training dataset. The label, in this case,

was the state of the function written in the statement. Datasets about the TLB state information were prepared for the

stealing attack.

For the feature engineering, features were extracted from TLB temporal signals by encoding information with

a vector of normalized latencies. Additionally, ML algorithms were adopted to distinguish the targeted TLB set by

analyzing memory activity. Specifically, with high-resolution temporal features extracted to present the activity, an

SVM classifier was built to distinguish the access to the targeted set and other arbitrary sets. In the experiment of [42],
the training set contained 2,928 TLB latency data in three different sets. The end-to-end TLBleed attack on libgcrypt
captured the changes of the target TLB set, extracted the feature signatures and reconstructed the private keys. During

the evaluation phase, TLBleed reconstructed the private key at an average success rate of 97%. Particularly, a 256-bit

EdDSA secret key was leaked with TLBleed successfully with the success rate of 98%, while 92% in reconstructing RSA

keys. Potential mitigations against the TLBleed attack was discussed in [42] including executing sensitive process in

isolation on a core, partitioning TLB sets among distrusting processes, and extending hardware transactional memory

features. Hence, secret cryptography keys can be reconstructed by distinguishing the targeted TLB set.

Protection Against Leakage from CPU Cache Data: Since an attacker can deduce the private keys from the changes of
memory activities, a memory management system should be implemented to prevent these unexpected leakage, like

the CacheBar system proposed by [152]. In addition to the TLB cache data, this kind of attack can leverage last-level

caches (LLCs) [54, 76] to steal the fine-grained information from victims like secret keys. Preventing the attacker

analyzing some valuable information from LLCs, CacheBar dynamically managed the shared memory pages with

a copy-on-access mechanism. For example, each domain kept its own copy of one accessed physical page. Another

method used in CacheBar was limiting the cacheability of memory pages. For example, only a limited number of lines

per cache set could be accessed by attackers. CacheBar was evaluated empirically with a particular attack scenario

named the Prime-Probe attack [54, 76, 99]. CacheBar protected the secret key by restricting the access to the CPU cache

data.

A Prime-Probe attack with ML strategies was implemented in [152]. This attack was conducted when the attacker

and the victim ran the programs sharing the same CPU cache sets. In the process of reconnaissance phase, the changes

Manuscript submitted to ACM

28

Miao et al.

Fig. 7. The ML-based stealing attack against authentication information — password data. To infer the password, attackers reconnoiter
and collect the online information with the passive collection. For the attack with passive collection, attackers do not need to interact
the target service with designed inputs. They collect the required data labeled with semantic categories according to human behaviors
of password creation [132] or passwords’ generic structure [128]. During the feature engineering phase, different segments from
the required data are extracted. A semantic classifier is trained using probabilistic algorithms. After testing this classifier, various
passwords can be constructed as outputs with the semantic generalization.

of LLCs were useful for the attack. Targeting at these shared CPU cache sets, the attacker firstly primed the cache sets

using its memory. When the victim executed the program loaded in the memory, the conflicts would occur resulting in

the absent of some cache set memory. If the attacker probed the cache by calculating the time to load memory into the

sharing cache sets, he was able to infer the number of cache lines that were absent in each cache set. To collect training

and testing datasets, the attacker run the Prime-Probe attacks several times and recorded the information about the

absent cache lines numbers within each cache set. Six classes of these dataset were defined according to the number of

absent cache lines [152]. For instances, the record with zero absent cache lines was labeled as NONE. An NB classifier

was used to determine the classes of absent cache lines, which revealed the changes of different types of cache sets

caused by the user. To infer private keys, this ML-based Prime-Probe attack recognized the state of targeted CPU cache

set.

For the effect of the protection system, CacheBar significantly mitigated the Prime-Probe attack [152]. CacheBar

was designed to maintain the queue of cacheable memory pages dynamically. Fundamentally, it limited the number of

lines per sharing cache set probed by the attacker, since each physical memory page contained numerous memory

blocks mapping to different cache sets. In order to limit the number of cache lines, CacheBar limited the number of

domain-cacheable pages whose contents could be loaded in the same cache set. CacheBar also applied one reserved bit

within the domain mapping to these non-domain-cacheable pages. Thereafter, the access to any of these pages would

be processed by the page fault handler. As a result, by applying CacheBar, the overall accuracy of Prime-Probe attack

was decreased significantly from 67.3% to 33%. Limiting the access to chacheable CPU caches moderated the threat of

the ML-based Prime-Probe attack.

3.3.3 Stealing controlled password data for authentication. Passwords are considered as one of the most impor-

tant sensitive information of the user, and its leakage can raise a serious security concern. Most of the useful information

to the stealing attack is collected passively from the network services as illustrated in Fig. 7. The password guessing

attack was studied by analyzing the password patterns with ML techniques.

Online Password Guessing Attack: Online password guessing problem and a framework named TarGuess to model
targeted online guessing scenarios systematically were introduced by [132]. Since attackers perform an online password

guessing attack based on the victim’s personal information, systematically summarizing the all possible attack scenarios

helps analysts understand the security threats. The architecture of TarGuess was demonstrated with three phases

including the preparing phase to determine the targeted victim and build up its password profile, the training phase

to generate the guessing model, and the guessing phase to perform the guessing attack. During the reconnaissance

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

29

Table 11. Stealing Controlled Password Data for Authentication

Reference

[132]

[128]

[90]

Dataset for Experiment
Dodonew, CSDN,
126, Rockyou,
000webhost, Yahoo,
12306,
Rootkit;
Hotel, 51job
RockYou
PGS training set [127],
1class8, 1class16 [58],
3class12 [114], 4class8 [88],
webhost [12]

Description
16,258,891 (6,428,277) leaked passwords,
6,392,568 (32,581,870) leaked passwords,
15,251,073 (442,834) leaked passwords,
6,392,568 leaked passwords + 129,303 PII,
69,418 leaked passwords + 69,324 PII;
20,051,426 PII, 2,327,571 PII
32,581,870 leaked passwords
33 million passwords,
3,062 (2,054) leaked passwords,
990 (990) leaked passwords,
30,000 leaked passwords

Feature Engineering ML-based Attack Method

N/A

Segmented with NLP

N/A

PCFG-based
algorithm [134],
Markov-based
algorithm [85],
LD algorithm

PCFG-based algorithm
PCFG-based
algorithm [62],
Markov models [85],
NN

phase, according to the diversity of people’s password choices, three kinds of information were beneficial in online

guessing attacks: PII like name and birthday, site information like service type, and leaked password information like

sister passwords and popular passwords. In particular, PII could be divided into two types, including Type-1 PII used to

build part of the passwords (i.e. birthday), and Type-2 PII reflected user behavior in setting passwords (e.g. language

[88]). Some leaked passwords were reused by the user. During the data collection phase, the datasets were combined by

multiple types of PIIs and leaked passwords. To be more specific, the four TarGuess were listed as TarGuess-I based

on Type-1 PII, TarGuess-II based on leaked passwords, TarGuess III based on leaked passwords and Type-1 PII, and

TarGuess-IV based on leaked passwords and PII. Datasets for password guessing attacks were prepared.

After dataset was collected with its initial features, attackers adopted probabilistic guessing algorithms including

PCFG, Markov n-gram and Bayesian theory to train the four TarGuess models to infer passwords [132]. The accuracy

of these four guessing algorithms was evaluated when the guessing time was limited online. Comparing with [72],
the TarGuess-I algorithm outperformed with 37.11% to 73.33% more passwords successfully inferred within 10 − 103
guesses. It also outperformed the three trawling online guessing algorithms [11, 85, 134] significantly by cracking at

least 412% to 740% more passwords. Comparing to [22] with 8.98% success rate, TarGuess-II achieved 20.19% within 100

guesses. As for TarGuess-III, no prior research could be compared and it achieved 23.48% success rate within 100 guesses.

Concerning TarGuess-IV, the improvements of accuracy were between 4.38% to 18.19% comparing to TarGuess-III.

Through modeling guessing attack scenarios, a serious security concern was revealed in [132] about online password

leakage with effective guessing algorithms.

Password Guessing with Semantic Pattern Analysis: An attempt was made in [132] to formalize several passwords guess
lists for one targeted user. And a similar attempt was made in [128] to find a general password pattern. A framework

presented by [128] built up semantic patterns of passwords for users in order to understand their password security. The

security impacts of user’s preferences in password creation were identified. For a better reconnaissance, the passwords

were analyzed by breaking into two conceptually consistent parts containing semantic and syntactic patterns. Since

a password consists of the combination of word and/or gap segments, the attacker intended to understand these

patterns by inferring the password’s meanings and syntactic functions. By comprehending how well the semantic

pattern characterizing the password, plenty of password guesses could be learned for attacks. When those attacks were

successful with some guesses, the true passwords were learned. The attack formalized the password with semantic and

syntactic patterns.

The password datasets could be collected from password data leakage like the RockYou’s password list. Firstly, the

NLP methods was used for passwords segmentation and semantic classification. The segmentation was the fundamental

step to process the passwords in various forms. The source corpora was a collection of raw words as the segmentation

candidates, whereas the reference corpora contained part-of-speech (POS). Specifically, the POS was tagged with

Manuscript submitted to ACM

30

Miao et al.

Natural Language Toolkit (NLTK) [108] based on Contemporary Corpus of American English. With N-gram probabilities

representing the frequency of use, the tagged POS was used to select the most suitable segmentation for passwords. After

processing the password dataset, the NLP algorithm was used to classify the segments of input passwords and result in

a semantic category. Secondly, a semantic guess generator could be built with the PCFG algorithm. Since syntactic

functions of the password were structural relationships among semantic classes, the PCFG algorithm was employed to

model the password’s syntactic and semantic patterns. In detail, this model learned the password grammar from the

dataset, generated the guessing sentence of a language [87] with different constructs, and encoded the probabilities of

these constructs as output. To learn any true passwords, the semantic guess generator sorted the outputs according to

the probability of the password cracking attack. The generator generated a guessing list based on the semantic and

syntactic patterns.

To assess the advantage of the semantic guess generator, the success rate of this generator was compared to the result

of previous offline guessing attack approach, i.e. Weir approach [134]. To crack a password within 3 billion guessing

times, 67% more passwords were cracked by the semantic approach than the Weir approach in terms of LinkedIn leakage

[128]. Exploiting the leakage from MySpace, this approach outperformed the Weir approach by inferring 32% more

passwords.

Protection with Modeling Password Guessability: As a main form of authentication, human-chosen textual password
should be resistant to guessing attacks. Evaluating the strength of passwords is one way to avoid creating the password

which is vulnerable to the guessing attack. The strength of password can be checked via modeling adversarial password

guessing [24, 58, 133]. According to [90], it was more successful to apply NNs rather than other password-guessing

methods like Markov models [85, 132] and PCFG [58, 128, 134]. It was intended to provide a client-side password

checker to defend the guessing attack by modeling the passwords guessability using NNs [90].

The proactive password checker was implemented to detect weak passwords. The RNN algorithm was applied to

guess the text-based password [90]. Firstly, the training datasets were collected from Password Guessability Service

(PGS) training set [127]. Instead of processing the password in word-level, the RNN algorithm was used to learn the

password text in character-level. Lists of tokens were generated from the training set according to the most 2,000

frequently used tokenized words. During the training phase, the RNN model was trained with the help of transference

learning [142], which overcame the sparsity of the training set. Given the preceding character of one password, the

guessing model was about to generate the next character till a special password-ending symbol [23, 85]. The checker

would enumerated all possible passwords when their probabilities exceeded a given threshold. The targeted password’s

guess number was the number of guesses taken by the attacker to verify the password when all possibilities were

in the descending order of likelihood [90]. The lower the guess number was, the higher possibility of the targeted

passwords can be guessed. Once the guess number of one created password was lower than a threshold, the checker

would alert user to choose another stronger password. All in all, the checker protected the password by alerting its

high guessability.

3.3.4 Summary. According to different forms of authentications, ML-based stealing attacks target at users’ keystroke

authentication, secret keys and passwords. As shown in Fig. 6 and Fig. 7, attackers steal users’ passwords by cracking the

useful information online. For the other two objectives, they exploit the information based on users’ activities recorded

by an Operating System (i.e. TLB/CPU cache data). Additionally, password guessing attacks use the probabilistic method

to construct a password with the least number of guesses. The attack on the remaining two targets can be transferred

as classification tasks by generating keystroke patterns and cache set states.

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

31

Countermeasures: From the security perspective, two types of countermeasures are introduced as the access restriction
and the attack detection. The secret keys, for example, can be protected by managing the accessible related cache data

[152]. The analysis of password guessability [90] can secure the user’s account by setting a strong password. The weak

passwords are evaded by detection. The future direction can target the effectiveness of guessing model prediction which

is limited by the sparsity of training samples [90]. The defense for the keystroke inference has not been well-developed.

The future work may explore the secured access of related sensor data.

Table 12. Summary of reviewed papers from attack, protection, related ML techniques they utilized, and the evaluation metrics.

Reference

[26]

Attack
Unlock pattern & foreground
app inference attack

Protection
Restrict access to kernel resources;
Decrease the resolution of interrupt data

[119]

Leaking specific events attack

Restrict access to kernel resources;
App Guardian [94, 150]

Related ML Techniques
HMM with Viterbi algorithm;
𝑘-NN classifier with DTW
𝑘-NN classifier with DTW;
Multi-class SVM with DTW

[136]

[151]

Keystroke timing attack;
website inference attack

Stealing user activities;
Stealing in-app activities

[50]

Stealing product’s design

[117]

Information leakage via a
sensor; Stealing information
via a sensor

[125]

Model extraction attack

[101]

Model extraction attack

[131]

[98]

Hyperparameters stealing attack

Hyperparameters stealing attack

[34]

Model inversion attack

Design 𝑑∗-private mechanism

Multi-class SVM classifier

Eliminate the attack vectors; Rate limiting;
Runtime detection [150]; Coarse-grained
return values; Privacy-preserving statistics
report [136]; Remove the timing channel
Obfuscate the acoustic emissions

The contextual model detects malicious
behavior of sensors

Rounding confidences [34];
Differential privacy (DP) [28, 55, 71, 130];
Ensemble methods [120]
Gradient masking [39] and defensive
distillation [103] for a robust model
Cross entropy and square hinge loss
instead of regular hinge loss
N/A
Incorporate inversion metrics in training;
Degrade the quality/precision of the
model’s gradient information.

SVM classifier;
𝑘-NN classifier with DTW

A regression model

Markov Chain; NB;
Alternative set of ML
algorithms (e.g. PART)

Logistic regression;
Decision tree; SVM;
Three-layer NN
DNN; SVM; 𝑘-NN; Decision
Tree; Logistic regression
Regression algorithms; NN;
Logistic regression; SVM
Metamodel methods

Decision Tree;
Regression model

Evaluation
Success rate; Time &
battery consumption
Accuracy;
Precision; Recall;
Battery consumption
Accuracy;
Relative AccE

Accuracy;
Execution time;
Power consumption

Accuracy
Accuracy; FNR;
F-measure; FPR;
Recall; Precision;
Power consumption
Test error;
Uniform error;
Extraction Faccuracy

Success rate

Relative EE; Relative
MSE; Relative AccE
Accuracy
Accuracy;
Precision;
Recall

[49]

The GAN attack stealing
users’ training data

N/A

CNN with GAN

Accuracy

[116]

Membership inference attack

Restrict class in the prediction vector;
Coarsen precision; Increase entropy of the
prediction vector [48]; Regularization

NN

[110]

Membership inference attack

Dropout; Model Stacking

[38]

[91]

[95]

Property inference attack

Property inference attack

Membership inference attack

[100]

N/A

[70]

N/A

[79]

Keystroke inference attack

Typed input inference attack

TLBleed attack infers secret keys
ML-based prime-probe attack
infers secret keys

Multiply the weights and bias of each
neuron; Add noise; Encode arbitrary data
Share fewer gradients; Reduce input
dimension; Dropout; user-level DP
Protect with adversarial regularization
PATE: transfer knowledge from an
ensemble model to a student model

Protect stored training data with count-
based featurization

Restrict access to accelerometer data; Limit
acoustic emanation; Dynamic permission
management based on context
Design a featureless cover; Randomize
the keyboards’ layouts; Add noise
Protect in hardware [43, 75]
CacheBar manage memory pages
cacheability

Password guessing attack

N/A

Password guessing attack

Password guessing attack

N/A
Mitigate the threat by modeling password
guessability

[122]

[42]

[152]

[132]

[128]

[151]

Logistic regression; Random
Forest; Multilayer perceptron

NN

Logistic regression; Gradient
boosting; Random Forests
NN
Semi-supervised learning;
GAN
NN; Gradient boosted tree;
Logistic regression;
Linear regression

Random Forest;
𝑘-NN; SVM; NN

multi-class SVM

SVM classifier

NB classifier

PCFG algorithm; Markov
model; Bayesian theory
PCFG-based algorithm
PCFG-based algorithm;
Markov models; NN

Accuracy;
Precision;
Recall
Precision;
Recall; AUC
Accuracy;
Precision; Recall
Precision;
Recall; AUC
Accuracy

Accuracy

Average logistic
squared loss

Success rate

Accuracy

Success rate
Accuracy;
Execution time

Success rate

Success rate

Accuracy

Manuscript submitted to ACM

32

Miao et al.

Fig. 8. The Challenges of ML-based Stealing Attack and Its Defenses

4 CHALLENGES AND FUTURE WORKS

In Section 3, the recent publications about the ML-based stealing attacks against the controlled information and the

corresponding defense methods are reviewed. Some attacks can steal the information, but they make strong assumptions

of the attacker’s prior knowledge. For instance, the attacker is assumed to know the ML algorithm as a necessary

condition prior to stealing the model/training samples. However, this prior knowledge is not always publicly known in

the real world cases. Additionally, the attack methods are not mature technologies and have great room for improvement.

Table 2 outlines the target and accessible data for each paper. And Table 12 summarizes the core research papers in the

perspectives of attack, protection, related ML techniques, and evaluation. The following subsections will discuss the

future directions of the ML-based stealing attack and feasible countermeasures as shown in Fig. 8.

4.1 Attack

During the battle between attackers and defenders, it is crucial for defenders to anticipate the strategies of the attackers’

future actions. To discuss the future attacks, the challenges of the ML-based stealing attack are analyzed. The analysis

results and possible solutions can be summarized and regarded as the future direction of the ML-based stealing attack.

In Section 4.1, challenges and future directions are discussed from five phases of the MLBSA methodology listed as

reconnaissance, data collection, feature engineering, attacking the objective, and evaluation.

4.1.1 Reconnaissance. As illustrated in Section 2.1, the reconnaissance phase consists of two main tasks — the target

definition and valuable accessible data analysis. The denotation of the target determines which kind of accessible

resources is valuable. The further attack mechanism is designed according to the analysis of accessible data during

the reconnaissance phase. It is essential to ensure that the information accessible to legitimate users contains valuable

information for stealing attacks to succeed.

A challenge of an attack during the reconnaissance phase is the lack of the effective information from the accessible

data. As stated in Table 2, the first category attack — stealing the user activities information — primarily relies on the

accessible data source including the kernel data and the sensor data. The attacker captures the information without

special permissions and utilizes different user activities’ representatives as explained in Section 2. Setting the appropriate

permission requirements can protect the accessible data from being exploited by the attacker. For example, Android

version 8 restricts the access to kernel resources including interrupt timing log files [119]. Because of an insufficient

amount of information collected under the black-box setting, the model/training data obtained by the stealing attacks

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

33

is insufficient to reconstruct an ML model as good as the original model [34, 125, 131]. As for the third category of

the attack, the majority of the stealing methods are proposed based on a large amount of PII, effective sensor data or

coarse-grained cache data. However, these information, especially PII, are sensitive enough to raise privacy concerns

and may be protected in the future [16, 113]. In summary, current attack vectors restricted to access can block part of

ML-based stealing attacks.

Dealing with the lack of information, the future work of the attacker is to find new exploitable data sources as

replacements. Some plausible solutions are proposed in [117, 119]. [119] defines all interested targets in a list and

automatically triggers the activities of interest, followed by searching exploitable logs filed in the newest version

of the targeted system (Android version 7 and 8). And it is proven worthless to simply monitor the changes in the

accessible data such as the sensor data [119], because the detection of the changes gives clues for stealing information.

Consequently, future directions can be inspired as searching exploitable source in iOS and monitoring the possible

changes of sensor data in order to perform a potential stealing attack. For the attack stealing authentication information,

a solution is to use COCA corpus instead of PII. Using the COCA corpus, a successful password guessing attack was

performed by [128]. Additionally, analyzing the password structure with anthropological analysis [128] may reduce the

attacker’s reliance on PII. The attackers were predicted to search new sources or explore new characteristics for further

attacks.

4.1.2 Data Collection. Determining the valuable accessible data is only a part of an ML-based stealing attack. To take

advantages of the ML mechanism, the valuable dataset collected in this phase should guarantee its representation,

reliability, and comprehensiveness. If either one of three is unsatisfactory, then the results of the stealing attack will be

inaccurate.

The first challenge is collecting valuable data with the representative information of the targeted information

including all systems/devices. Especially when the valuable data is kernel data or sensor data, some forms of data

recording may vary greatly from systems and devices. Regarding this problem, the data was collected by [26] and [50]

from eight different mobile devices and different machines. Hence, a future research direction is collecting data from

heterogeneous sources and aggregating the representative data. Various forms of representative information affect the

attack’s probability of success.

The second challenge appears while collecting a reliable dataset. The quality of the training dataset is critical to the

attack performance. Most of the explored stealing attacks utilized the model’s query output results — confidential values

associated with attacker’s query inputs. The preciseness of this value affects the success of the attack. Specifically, the

confidential information was leveraged by [125], [131] and [34] to imitate the objective ML model through techniques

such as equation solving, path searching, and inversion attack as summarized in Table 5 and Table 6. Furthermore, the

performance of an important attack named membership inference attack [116] depends on the training dataset of the

shadow models. The training dataset can be generated based on the target model’s confidential information, which

is distributed similarly to the targeted training set. Under these circumstances, the aforementioned attacks will not

succeed if the target model’s API outputs the class only or the polluted confidential information. This inconvenience was

scrutinized by [125], and a method was proposed to extract the model with only class labels are informed. Accordingly,

these findings were further explored in the context of several other ML algorithms together with less monetary

advantage when using ML APIs as an honest user. The poor quality of collected dataset hinders the success of the

ML-based stealing attack.

Manuscript submitted to ACM

34

Miao et al.

The third challenge of comprehensive dataset collection involves determining the size/distribution of the training

dataset and the testing dataset. The size of the training inputs often dictates whether the attacker can easily gain all

possible classes of the targeted controlled information, especially when the predictive model outputs only one class per

query. In [116], a comprehensive training dataset was collected by generating the dataset which has a similar distribution

to the targets. The size of testing dataset indicates indirectly the amount of controlled information that attackers can

learn. For instance, the testing set size of a membership inference model depended on how many training members

might be included and would be distinguished [116]. A future work may investigate the impact of the size/distribution

of training and testing datasets to the success of ML-based stealing attacks. Partial or imbalanced distribution reduces

the success rates of stealing attacks.

4.1.3

Feature Engineering. Feature engineering in the MLBSA methodology intends to refine the collected data for the

effective and efficient training process. It is critical to the performance of ML-based attack by eliminating the noise

from the collected data. However, among the current research, the techniques used in feature engineering remain

underdeveloped.

As shown in Table 3, Table 4, Table 9, and Table 11, many existing solutions select features manually. Manual feature

selection, relying on the attacker’s domain-specific knowledge and human intelligence, usually produces a small number

of features. That is, manual feature selection is inefficient because of the nature of human involvements, and it may

ignore the useful features with low discriminative power. To improve the attack’s effectiveness, the automation of

feature selection has great research potentials. For example, CNN was used in [49] to learn a representation of features

based on the correlation among data for optimal classification. A future trend of the attack is searching for or developing

other automatic methods to overpower the manual feature selection [21]. In [57], a regression-based feature learning

algorithm was developed to select and generate features without domain-specific knowledge required. Automating

feature selection with such generic algorithm would promote the efficiency and effectiveness of the ML-based attack.

4.1.4 Attacking the Objective. In the phase of attacking the objective with ML techniques, the main tasks include

training and testing the ML model to steal the controlled information. There are a few challenges of stealing attacks

with respect to training and testing ML models including unknown model algorithms, unknown hyperparameters of

ML model, and the limited amount of testing time.

For ML-based attack stealing the controlled model/training data, the first challenge is that most of the research

considered the model algorithm as a prior knowledge. However, the model algorithms for many MLaaS are unknown

to the end-user. Most of the attacks would not succeed without specifying the correct model algorithm. In [131], this

concern was discussed, afterwards, a conclusion was drawn that attacks without understanding the model algorithm

can be impossible in some circumstances. It is worth investigating the possibility of success for an attack in the context

of the unknown model algorithm. In 2019, membership inference attack was considered in [110] against a black-box

model by choosing a threshold to reveal model’s training set information. However, it remains unknown whether this

method is applicable to other attacks under the black-box access like the parameter stealing attack [125].

The second challenge involves the unknown hyperparameters of the ML model learned from stealing attack comparing

to the targeted model. The more precise the model learned, the more accurate the model’s functionality; the more precise

the model learned, the more detailed the training records that will be revealed. The stealing attack predominantly

stole the model by reckoning the parameters of matching objective functions. However, hyperparameter as a critical

element has been ignored, the values of which influence the accuracy of stealing attacks. In [131], a solution was

proposed to prevent an attacker calculating the hyperparameters of some ML algorithms which consist of a set of

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

35

linear regression algorithms and three-layer neural networks. The future direction toward solving this difficulty can
enable a hyperparameter stealing attack against other popular ML algorithms such as 𝑘-NN and RNN. With unknown
hyperparameters, only the parameters can be calculated while extracting the ML model.

The third category of stealing attacks is password guessing attacks. This type of attack generally assumes an unlimited

login testing attempts for each account. One exception is in [132] where each login password attack was performed less

than 100 times. To crack the password effectively, researchers applied ML algorithms to analyze the password generator

based on personal information, website-related information and/or previous public leaked passwords. The future work

for this stealing attack can be a successful attack mechanism, which is designed for the targeted authentication system

with less than 100 times for login testing. With limited login testing attempts, guessing attack may be failed with the

first a few guesses.

4.1.5 Evaluation. To effectively infer the controlled information, most of the investigated research applied ML mech-

anism mentioned in Section 3. The prediction of the unknown testing samples is a challenge for ML-based stealing

attacks, as the supervised learning algorithm dominates the attack methods. That is, if the true label of a testing sample

has not been learned by the model during the training phase, this sample will be recognized as an incorrect class. The

testing samples, which are unknown to the training dataset, affect the evaluation results and subsequently reduce the

stealing attack’s accuracy. To improve the performance of such attacks, the attacker needs to achieve breakthroughs

towards predicting the unknown data.

For stealing the user activities attack, when an attacker wants to know the foreground app running in a user’s mobile,

some distinctive features of the accessible data set which represents the status of running apps will be extracted and

learned by ML algorithms [26, 136, 151]. After the attack model is trained, the accessible data recording a new foreground

app running in the mobile is the testing sample, this new app is unknown to the attack model [26, 71, 136, 151]. For

stealing authentication information attack, these attacks are difficult to be effective when users change their passwords

frequently or adopt new layouts for the keyboard [122]. This is owing to the uncertainty of the users’ password

generation behaviors and the variety of users’ input keyboards. Evaluating the prediction of unknown class is a

challenging task for stealing attack against the user activities and the authentication information.

4.2 Defense

Targeting diverse controlled information, the countermeasures in protecting the information from ML-based stealing

attacks are summarized. In general, the countermeasures can be summarized into three groups: 1) the detection is

indicated as detecting related critical indications; 2) the disruption intends to break the accessible data at a tolerable

cost of service’s utility; and 3) isolation aims to limit the access to some valuable data sources. As shown in Fig. 8, the

countermeasures mainly applied in the first two phases. Specifically, isolation restricts the attacker’s access and makes

the attack fail at the first phase; and disruption may confuse the attacker in the second phase and hinder the attacker

to build a successful attack model. The detection techniques can detect the attacker’s actions and then protect the

information from being stolen. These issues are explained as follows.

4.2.1 Detection. To detect potential stealing attacks in advance, the relevant crucial indications are required by

analyzing the functionality related to the controlled information. Defenders should notice the attackers’ actions as soon

as the attackers start the reconnaissance or the data collection processes. Based on the attacker’s future directions, the

detection is proposed accordingly in order to prevent the attack at an early stage and minimize the loss of stealing the

controlled information.

Manuscript submitted to ACM

36

Miao et al.

In the presence of any malicious activities in the stage of reconnaissance and data collection, the change or the usage

of relevant crucial information should be analyzed and checked. For example, when the attacker steals the information

based on the accessible sensor data calling from some APIs, the calling rate and the API usage can be deemed as two

critical indications for detection [151]. Since attackers may intend to exploit unknown critical indications, a defender

can trade off between the access frequency of all related information and the service’s utility. Thereafter, the defender

detects the unusual access frequency against the stealing attack.

Another detection method is assessing the ability of the service securing the controlled information. This protection

can promise that the ML-based attack for stealing information is less powerful than the current attacks. The memory

page’s cacheability was managed by [152] to protect secret keys within user’s memory activities, while the password’s

guessability was checked by [90]. Thereafter, users were alarmed with the weak password. If a defender can assess the

ability to hide the ML model and training set from unauthorized peeps, the controlled information are protected to

some extent. The detector alerts the user if the assessed ability is below a predefined threshold.

4.2.2 Disruption. Disruption can protect the controlled information via obstructing the information used in each phase

of the MLBSA methodology. Disrupting the accessible data currently involves two methods as adding noise to data

sources and degrading the quality/precision of service’s outputs. For more advanced countermeasures, further research

needs to better understand the attacker’s future directions.

By disrupting the accessible data, attackers cannot find out valuable accessible sources in the reconnaissance phase,

get reliable dataset in the collection phase, or use feature engineering effectively. Therefore, disruption minimizes the

success rate of the ML-based stealing attack. The major technique for adding noise is the differential privacy (DP) as

applied in [28, 70, 71, 136, 151]. Furthermore, DP can be categorized as three categories of disruption methods including

global DP [30], local DP [30] and distributed DP [82–84]. The global DP can be applied on model’s global parameters,

and local DP and distributed DP are mainly used on collaborative schemes similar to a GAN attack [49] at the model’s

parameter-level and record-level. According to [49], the record-level DP could be more robust against the GAN attack

than the parameter-level DP. From another perspective, the record-level DP does not affect the model stealing attacks

[125, 131], but all kinds of DP can protect training set information at a certain level. As for the latter method, the specific

techniques include rounding the values of outputs (i.e. coarse-grained predictions and/or confidence) [34, 116, 125, 151],

and regularizing or obfuscating the accessible data sources [26, 70, 95, 117, 122].

The advanced disruption methods against the attacker’s further attacks are also considered. Assuming that attackers

may search and collect a class of information from a few devices in the future, advanced disruption should premeditate

adding different noise for the similar information shared between various devices. To prevent the advanced feature

engineering techniques and ML-based analysis that attackers might apply, complicated and skillful methods for

disruption can be applied to defend the controlled information such as an adversarial training algorithm acting as a

strong regularizer [95].

4.2.3

Isolation. Isolation can assist the system by eliminating the information stealing threat, which hinders the

attacker from progressing through the reconnaissance phase. No matter how attackers improve their strategies and

techniques, isolation can protect the controlled information by restricting access to the data of interest. Specifically, it is

effective to control the accessible data via restricting the access or managing the dynamic permission [26, 79, 116, 119].

When the stealing attacks advance, defenders can apply ML techniques to automatically control as many as possible

accesses related to the targeted controlled information. However, this protection is highlighted to be applied cautiously

by concerning the utility of the service. On the one hand, specialists can remove the some information channels

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

37

which may reveal valuable information to the adversary [42, 151]. On the other hand, if attackers find new exploitable

accessible sources in the future, then it is challenging to isolate all the relevant data while ensuring the service’s utility.

Isolation effectively protects the information by restricting access to the data.

5 CONCLUSION

In this survey, the ML-based stealing attack against the controlled information and the defense mechanisms are

reviewed. The generalized MLBSA methodology compatible with the published work is outlined. Specifically, the
MLBSA methodology uncovers how adversaries steal the controlled information in five phases, i.e. reconnaissance,
data collection, feature engineering, attacking the objective, and evaluation. Based on different types of the controlled

information, the literature was reviewed in three categories consisting of the controlled user activities information,

the controlled ML model related information, and the controlled authentication information. The attacker is assumed

to use the system without any administrative privilege. This assumption implies that user activities information was

stolen by leveraging the kernel data and the sensor data both of which are beyond the protection of the application.

The attack against the controlled ML model-related information is demonstrated with stealing the model description

and/or stealing the training data. Similarly, keystroke data, secret keys, and password data are the examples of stealing

the controlled authentication information.

Besides the stealing attack, the corresponding protections are summarized for each category. After reviewing the

recent research, it is essential to indicate the challenges clearly in the majority of existing work according to the five

attacking phrases.

The future directions matching various limitations are presented. Comparing to the explicit breaking/destroying

attack, the controlled information leaked by such stealing attacks is much more difficult to be detected, so that the

estimated loss should be extended accordingly. This survey, therefore, can help researchers familiarize these stealing

attacks, their future trends, and the potential defense methods.

REFERENCES

[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential
privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Vienna, Austria, 308–318.
[2] Mohammad Ahmadian and Dan Cristian Marinescu. 2019. Information leakage in cloud data warehouses. IEEE Transactions on Sustainable

Computing (2019), 1–12.

[3] Sultan Alneyadi, Elankayer Sithirasenan, and Vallipuram Muthukkumarasamy. 2016. A survey on data leakage prevention systems. Journal of

Network and Computer Applications 62, Feb (2016), 137–152.

[4] Orcan Alpar. 2017. Frequency spectrograms for biometric keystroke authentication using neural network based classifier. Knowledge-Based Systems

116, Jan (2017), 163–171.

[5] Giuseppe Ateniese, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni Felici. 2015. Hacking Smart Machines

with Smarter Ones: How to Extract Meaningful Data from Machine Learning Classifiers. Int. J. Secur. Netw. 10, 3 (2015), 137–150.

[6] Michael Backes, Markus Dürmuth, and Dominique Unruh. 2008. Compromising reflections-or-how to read LCD monitors around the corner. In

Proceedings of the 2008 IEEE Symposium on Security and Privacy (SP). IEEE, Oakland, CA, USA, 158–169.

[7] R Barona and EA Mary Anita. 2017. A survey on data breach challenges in cloud computing security: Issues and threats. In Proceedings of the 2017

International Conference on Circuit, Power and Computing Technologies (ICCPCT). IEEE, Kollam, India, 1–8.

[8] Yigael Berger, Avishai Wool, and Arie Yeredor. 2006. Dictionary attacks using keyboard acoustic emanations. In Proceedings of the 13th ACM

SIGSAC Conference on Computer and Communications Security (CCS). ACM, Alexandria, Virginia, USA, 245–254.

[9] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion attacks
against machine learning at test time. In Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases.
Springer, Prague, Czech Republic, 387–402.

[10] BigML. 2019. Machine Learning made beautifully simple for everyone. https://bigml.com/
[11] Joseph Bonneau. 2012. The science of guessing: Analyzing an anonymized corpus of 70 million passwords. In Proceedings of the 2012 IEEE Symposium

on Security and Privacy (SP). IEEE, San Francisco, CA, USA, 538–552.

Manuscript submitted to ACM

38

Miao et al.

[12] Thomas Brewster. 2015. 13 Million Passwords Appear To Have Leaked From This Free Web Host. https://www.forbes.com/sites/thomasbrewster/

2015/10/28/000webhost-database-leak/#5b2a9ad06098

[13] Liang Cai and Hao Chen. 2011. TouchLogger: Inferring keystrokes on touch screen from smartphone motion. In Proceedings of the 6th USENIX

Workshop on Hot Topics in Security (HotSec’11). USENIX Association, San Francisco, CA, USA, 9–15.

[14] Anthony Califano, Ersin Dincelli, and Sanjay Goel. 2015. Using features of cloud computing to defend smart grid against DDoS attacks. In

Proceedings of the 10th Annual Symposium on Information Assurance (Asia 15). NYS, Albany, New York, 44–50.

[15] InfoWatch Analytics Center. 2018. Global Data Leakage Report, 2017. https://infowatch.com/report2017#
[16] Farah Chanchary, Yomna Abdelaziz, and Sonia Chiasson. 2018. Privacy concerns amidst OBA and the need for alternative models. IEEE Internet

Computing 22, Apr (2018), 52–61.

[17] Chao Chen, Yu Wang, Jun Zhang, Yang Xiang, Wanlei Zhou, and Geyong Min. 2017. Statistical features-based real-time detection of drifted twitter

spam. IEEE Transactions on Information Forensics and Security 12, 4 (2017), 914–925.

[18] Rongmao Chen, Yi Mu, Guomin Yang, Fuchun Guo, and Xiaofen Wang. 2016. Dual-server public-key encryption with keyword search for secure

cloud storage. IEEE Transactions on Information Forensics and Security 11, 4 (2016), 789–798.

[19] Long Cheng, Fang Liu, and Danfeng Yao. 2017. Enterprise data breach: causes, challenges, prevention, and future directions. Wiley Interdisciplinary

Reviews: Data Mining and Knowledge Discovery 7, 5 (2017), e1211.

[20] Maximilian Christ, Andreas W Kempa-Liehr, and Michael Feindt. 2016. Distributed and parallel time series feature extraction for industrial big

data applications. arXiv:cs.LG/1610.07717

[21] Rory Coulter, Qing-Long Han, Lei Pan, Jun Zhang, and Yang Xiang. accepted, to appear in 2020. Data driven cyber security in perspective —

intelligent traffic analysis. IEEE Transactions on Cybernetics (accepted, to appear in 2020).

[22] Anupam Das, Joseph Bonneau, Matthew Caesar, Nikita Borisov, and XiaoFeng Wang. 2014. The tangled web of password reuse. In Proceedings of

the 21st Annual Network and Distributed System Security Symposium (NDSS). IEEE, San Diego, CA, USA, 1–15.

[23] Matteo Dell’Amico and Maurizio Filippone. 2015. Monte Carlo strength evaluation: Fast and reliable password checking. In Proceedings of the 22nd

ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Denver, Colorado, USA, 158–169.

[24] Matteo Dell’Amico, Pietro Michiardi, and Yves Roudier. 2010. Password strength: An empirical analysis. In Proceedings of the Annual IEEE

International Conference on Computer Communications (INFOCOM). IEEE, San Diego, CA, USA, 1–9.

[25] Li Deng. 2012. The MNIST database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Processing Magazine

29, 6 (2012), 141–142.

[26] Wenrui Diao, Xiangyu Liu, Zhou Li, and Kehuan Zhang. 2016. No pardon for the interruption: New inference attacks on android through interrupt

timing analysis. In Proceedings of the 2016 IEEE Symposium on Security and Privacy (SP). IEEE, San Jose, CA, USA, 414–432.

[27] CW Dukes. 2015. Committee on national security systems (CNSS) glossary. Technical Report. Committee on National Security Systems Instructions

(CNSSI).

[28] Cynthia Dwork. 2008. Differential privacy: A survey of results. In Proceedings of the International Conference on Theory and Applications of Models

of Computation. Springer, Xi’an, China, 1–19.

[29] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating noise to sensitivity in private data analysis. In Proceedings of

the Theory of Cryptography Conference. Springer, New York, NY, USA, 265–284.

[30] Cynthia Dwork and Aaron Roth. 2014. The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer

Science 9, 3–4 (2014), 211–407.

[31] Mohamed Amine Ferrag, Leandros Maglaras, and Ahmed Ahmim. 2017. Privacy-preserving schemes for ad hoc social networks: A survey. IEEE

Communications Surveys & Tutorials 19, 4 (2017), 3015–3045.

[32] Carlos Flavián and Miguel Guinalíu. 2006. Consumer trust, perceived security and privacy policy: three basic elements of loyalty to a web site.

Industrial Management & Data Systems 106, 5 (2006), 601–620.

[33] G David Forney. 1973. The viterbi algorithm. Proc. IEEE 61, 3 (1973), 268–278.
[34] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion attacks that exploit confidence information and basic countermeasures.

In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Denver, Colorado, USA, 1322–1333.
[35] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. Privacy in Pharmacogenetics: An End-to-End
Case Study of Personalized Warfarin Dosing. In Proceedings of the 23rd USENIX Security Symposium (USENIX Security 14). USENIX Association, San
Diego, CA, USA, 17–32.

[36] Ponemon from IBM. 2018. 2018 Cost of a Data Breach Study: Global Overview. https://www.ibm.com/security/data-breach
[37] Sam Smith from Juniper Research. 2015. CYBERCRIME WILL COST BUSINESSES OVER $2 TRILLION BY 2019. https://www.juniperresearch.

com/press/press-releases/cybercrime-cost-businesses-over-2trillion

[38] Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. 2018. Property inference attacks on fully connected neural networks using
permutation invariant representations. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM,
Toronto, ON, Canada, 619–633.

[39] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and harnessing adversarial examples. In Proceedings of the 3rd

International Conference on Learning Representations (ICLR 2015). OpenReview.net, San Diego, CA, USA, 1–11.

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

39

[40] Adam Goodkind, David Guy Brizan, and Andrew Rosenberg. 2017. Utilizing overt and latent linguistic structure to improve keystroke-based

authentication. Image and Vision Computing 58, Feb (2017), 230–238.

[41] Google. 2019. Predictive Analytics - Cloud Machine Learning Engine. https://cloud.google.com/ml-engine/
[42] Ben Gras, Kaveh Razavi, Herbert Bos, and Cristiano Giuffrida. 2018. Translation leak-aside buffer: Defeating cache side-channel protections with
{TLB} attacks. In Proceedings of the 27th USENIX Security Symposium (USENIX Security 18). USENIX Association, Baltimore, MD, USA, 955–972.
[43] Daniel Gruss, Julian Lettner, Felix Schuster, Olya Ohrimenko, Istvan Haller, and Manuel Costa. 2017. Strong and efficient cache side-channel
protection using hardware transactional memory. In Proccedings of the 26th USENIX Security Symposium (USENIX Security 17). USENIX Association,
Vancouver, BC, Canada, 217–233.

[44] Mordechai Guri and Yuval Elovici. 2018. Bridgeware: The air-gap malware. Commun. ACM 61, 4 (2018), 74–82.
[45] Barbara Hauer. 2015. Data and information leakage prevention within the scope of information security. IEEE Access 3, Dec (2015), 2554–2565.
[46] Debiao He, Sherali Zeadally, Neeraj Kumar, and Jong-Hyouk Lee. 2017. Anonymous authentication for wireless body area networks with provable

security. IEEE Systems Journal 11, 4 (2017), 2590–2601.

[47] Texas Health and Human Service. 2018. Hospital Discharge Data Public Use Data File. https://www.dshs.texas.gov/THCIC/Hospitals/Download.shtm
[48] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv:cs.LG/1503.02531
[49] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. 2017. Deep models under the GAN: information leakage from collaborative deep
learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Dallas, Texas, USA, 603–618.
[50] Avesta Hojjati, Anku Adhikari, Katarina Struckmann, Edward Chou, Thi Ngoc Tho Nguyen, Kushagra Madan, Marianne S Winslett, Carl A Gunter,
and William P King. 2016. Leave your phone at the door: Side channels that reveal factory floor secrets. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security (CCS). ACM, Vienna, Austria, 883–894.

[51] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and JD Tygar. 2011. Adversarial machine learning. In Proceedings of the 4th

ACM Workshop on Security and Artificial Intelligence. ACM, Chicago, Illinois, USA, 43–58.

[52] Kaggle Inc. 2014. Acquire Valued Shoppers Challenge. https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data
[53] Kaggle Inc. 2017. 20 Newsgroups. https://www.kaggle.com/crawford/20-newsgroups
[54] Gorka Irazoqui, Thomas Eisenbarth, and Berk Sunar. 2015. S $ A: A shared cache attack that works across cores and defies VM sandboxing–and its

application to AES. In Proceedings of the 2015 IEEE Symposium on Security and Privacy (SP). IEEE, San Jose, CA, USA, 591–604.

[55] Geetha Jagannathan, Krishnan Pillaipakkamnatt, and Rebecca N Wright. 2009. A practical differentially private random decision tree classifier. In

Proceedings of the IEEE International Conference on Data Mining Workshops (ICDMW’09). IEEE, Miami, Florida, USA, 114–121.

[56] Qi Jiang, Sherali Zeadally, Jianfeng Ma, and Debiao He. 2017. Lightweight three-factor authentication and key agreement protocol for internet-

integrated wireless sensor networks. IEEE Access 5, Mar (2017), 3376–3392.

[57] Ambika Kaul, Saket Maheshwary, and Vikram Pudi. 2017. Autolearn—Automated feature generation and selection. In Proceedings of the 2017 IEEE

International Conference on Data Mining (ICDM). IEEE, New Orleans, LA, USA, 217–226.

[58] Patrick Gage Kelley, Saranga Komanduri, Michelle L Mazurek, Richard Shay, Timothy Vidas, Lujo Bauer, Nicolas Christin, Lorrie Faith Cranor, and
Julio Lopez. 2012. Guess again (and again and again): Measuring password strength by simulating password-cracking algorithms. In Proceedings of
the 2012 IEEE Symposium on Security and Privacy (SP). IEEE, San Francisco, CA, USA, 523–537.

[59] Muhammad Salman Khan, Sana Siddiqui, and Ken Ferens. 2018. A cognitive and concurrent cyber kill chain model. Springer, Cham. 585–602 pages.
[60] Richard Kissel. 2013. Glossary of key information security terms. National Institute of Standards and Technology (NIST) - Computer Security

Resource Center, Gaithersburg, MD, US.

[61] Dennis Kiwia, Ali Dehghantanha, Kim-Kwang Raymond Choo, and Jim Slaughter. 2018. A cyber kill chain based taxonomy of banking Trojans for

evolutionary computational intelligence. Journal of computational science 27 (2018), 394–409.

[62] Saranga Komanduri. 2016. Modeling the adversary to evaluate password strength with limited samples. Ph.D. Dissertation. School of Computer

Science, Carnegie Mellon University.

[63] Venkata Koppula, Omkant Pandey, Yannis Rouselakis, and Brent Waters. 2016. Deterministic public-key encryption under continual leakage. In

Proceedings of the International Conference on Applied Cryptography and Network Security. Springer, Guildford, UK, 304–323.

[64] Sowndarya Krishnamoorthy, Luis Rueda, Sherif Saad, and Haytham Elmiligi. 2018. Identification of User Behavioral Biometrics for Authentication
Using Keystroke Dynamics and Machine Learning. In Proceedings of the 2018 2nd International Conference on Biometric Engineering and Applications.
ACM, Amsterdam, The Netherlands, 50–57.

[65] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features from tiny images. Technical Report. Citeseer.
[66] Nicholas D Lane, Emiliano Miluzzo, Hong Lu, Daniel Peebles, Tanzeem Choudhury, and Andrew T Campbell. 2010. A survey of mobile phone

sensing. IEEE Communications magazine 48, 9 (2010), 140–150.

[67] Nicholas D Lane, Ye Xu, Hong Lu, Shaohan Hu, Tanzeem Choudhury, Andrew T Campbell, and Feng Zhao. 2011. Enabling large-scale human
activity inference on smartphones using community similarity networks (csn). In Proceedings of the 13th International Conference on Ubiquitous
Computing. ACM, Beijing, China, 355–364.

[68] Erik Learned-Miller, Gary B Huang, Aruni RoyChowdhury, Haoxiang Li, and Gang Hua. 2016. Labeled faces in the wild: A survey. In Advances in

Face Detection and Facial Image Analysis. Springer, New York, NY, 189–248.

[69] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. 2011. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/

Manuscript submitted to ACM

40

Miao et al.

[70] Mathias Lecuyer, Riley Spahn, Roxana Geambasu, Tzu-Kuo Huang, and Siddhartha Sen. 2017. Pyramid: Enhancing selectivity in big data protection

with count featurization. In Proccedings of the 2017 IEEE Symposium on Security and Privacy (SP). IEEE, San Jose, CA, USA, 78–95.

[71] Ninghui Li, Wahbeh Qardaji, Dong Su, Yi Wu, and Weining Yang. 2013. Membership privacy: a unifying framework for privacy definitions. In

Proceedings of the 2013 ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Berlin, Germany, 889–900.

[72] Yue Li, Haining Wang, and Kun Sun. 2016. A study of personal information in human-chosen passwords and its security implications. In Proceedings

of the 35th Annual IEEE International Conference on Computer Communications (INFOCOM). IEEE, San Francisco, CA, USA, 1–9.

[73] Guanjun Lin, Jun Zhang, Wei Luo, Lei Pan, Yang Xiang, Olivier De Vel, and Paul Montague. 2018. Cross-Project Transfer Representation Learning

for Vulnerable Function Discovery. IEEE Transactions on Industrial Informatics 14, 7 (2018), 3289–3297.

[74] Jessica Lin and Yuan Li. 2009. Finding structural similarity in time series data using bag-of-patterns representation. In Proceedings of the International

Conference on Scientific and Statistical Database Management. Springer, New Orleans, LA, USA, 461–477.

[75] Fangfei Liu, Qian Ge, Yuval Yarom, Frank Mckeen, Carlos Rozas, Gernot Heiser, and Ruby B Lee. 2016. Catalyst: Defeating last-level cache side
channel attacks in cloud computing. In Proceedings of the 2016 IEEE International Symposium on High Performance Computer Architecture (HPCA).
IEEE, Barcelona, Spain, 406–418.

[76] Fangfei Liu, Yuval Yarom, Qian Ge, Gernot Heiser, and Ruby B Lee. 2015. Last-level cache side-channel attacks are practical. In Proceedings of the

2015 IEEE Symposium on Security and Privacy (SP). IEEE, San Jose, CA, USA, 605–622.

[77] Liu Liu, Olivier De Vel, Qing-Long Han, Jun Zhang, and Yang Xiang. 2018. Detecting and Preventing Cyber Insider Threats: A Survey. IEEE

Communications Surveys & Tutorials 20, 2 (2018), 1397–1417.

[78] Shigang Liu, Jun Zhang, Yang Xiang, and Wanlei Zhou. 2017. Fuzzy-based information decomposition for incomplete and imbalanced data learning.

IEEE Transactions on Fuzzy Systems 25, 6 (2017), 1476–1490.

[79] Xiangyu Liu, Zhe Zhou, Wenrui Diao, Zhou Li, and Kehuan Zhang. 2015. When good becomes evil: Keystroke inference with smartwatch. In
Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Denver, Colorado, USA, 1273–1285.
[80] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face Attributes in the Wild. In Proceedings of the International

Conference on Computer Vision (ICCV). IEEE, New York, NY, 3730–3738.

[81] Daniel Lowd and Christopher Meek. 2005. Adversarial learning. In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge

Discovery in Data Mining. ACM, Chicago, Illinois, USA, 641–647.

[82] Lingjuan Lyu, Yee Wei Law, Jiong Jin, and Marimuthu Palaniswami. 2017. Privacy-preserving aggregation of smart metering via transformation

and encryption. In 2017 IEEE Trustcom/BigDataSE/ICESS. IEEE, 472–479.

[83] Lingjuan Lyu, Yee Wei Law, and Kee Siong Ng. 2019. Distributed Privacy-Preserving Prediction. arXiv preprint arXiv:1910.11478 (2019).
[84] Lingjuan Lyu, Karthik Nandakumar, Benjamin Rubinstein, Jiong Jin, Justin Bedo, and Marimuthu Palaniswami. 2018. PPFA: Privacy Preserving

Fog-enabled Aggregation in Smart Grid. IEEE Transactions on Industrial Informatics 14, 8 (2018), 3733–3744.

[85] Jerry Ma, Weining Yang, Min Luo, and Ninghui Li. 2014. A study of probabilistic password models. In Proceedings of the 2014 IEEE Symposium on

Security and Privacy (SP). IEEE, San Jose, CA, USA, 689–704.

[86] Elsa Macias, Alvaro Suarez, and Jaime Lloret. 2013. Mobile sensing systems. Sensors 13, 12 (2013), 17292–17321.
[87] Christopher D Manning, Christopher D Manning, and Hinrich Schütze. 1999. Foundations of statistical natural language processing. MIT press,

London, UK.

[88] Michelle L Mazurek, Saranga Komanduri, Timothy Vidas, Lujo Bauer, Nicolas Christin, Lorrie Faith Cranor, Patrick Gage Kelley, Richard Shay, and
Blase Ur. 2013. Measuring password guessability for an entire university. In Proceedings of the 2013 ACM SIGSAC Conference on Computer and
Communications Security (CCS). ACM, Berlin, Germany, 173–186.

[89] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. 2017. Communication-efficient learning of deep networks from
decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS). PMLR, Fort Lauderdale, FL,
USA, 1273–1282.

[90] William Melicher, Blase Ur, Sean M Segreti, Saranga Komanduri, Lujo Bauer, Nicolas Christin, and Lorrie Faith Cranor. 2016. Fast, lean, and
accurate: Modeling password guessability using neural networks. In Proceedings of the 25th USENIX Security Symposium (USENIX Security 16).
USENIX Association, Washington, D.C., USA, 175–191.

[91] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. 2019. Exploiting unintended feature leakage in collaborative learning.

In Proceedings of the 2019 IEEE Symposium on Security and Privacy (SP). IEEE, San Fransisco, CA, US, 1–16.

[92] Microsoft. 2019. Azure Machine Learning Studio. https://azure.microsoft.com/en-au/services/machine-learning-studio/
[93] Emiliano Miluzzo, Alexander Varshavsky, Suhrid Balakrishnan, and Romit Roy Choudhury. 2012. Tapprints: your finger taps have fingerprints. In

Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services. ACM, Ambleside, UK, 323–336.

[94] Indiana University Nan from System Security Lab. 2015. App Guardian: An App Level Protection Against RIG Attacks. https://sit.sice.indiana.edu/

en/2015/09/11/app-guardian-oarland/

[95] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Machine Learning with Membership Privacy using Adversarial Regularization. In Proceedings

of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Toronto, Canada, 634–646.

[96] Hong-Wei Ng and Stefan Winkler. 2014. A data-driven approach to cleaning large face datasets. In Proceedings of the 2014 IEEE International

Conference on Image Processing (ICIP). IEEE, New York, NY, 343–347.

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

41

[97] Wale Ogunwale. 2016. Lockdown AM.getRunningAppProcesses API with permission.REAL_GET_TASKS. https://gitlab.tubit.tu-berlin.de/justus.

beyer/streamagame_platform_frameworks_base/commit/9dbaa54f6834e013a63f18bd51ace554de811d80

[98] Seong Joon Oh, Max Augustin, Bernt Schiele, and Mario Fritz. 2018. Towards Reverse-Engineering Black-Box Neural Networks. In Proceedings of

the 6th International Conference on Learning Representations (ICLR 2018). OpenReview.net, Vancouver, BC, Canada, 1–20.

[99] Dag Arne Osvik, Adi Shamir, and Eran Tromer. 2006. Cache attacks and countermeasures: the case of AES. In Proceedings of the Cryptographers’

Track at the RSA Conference. Springer, San Jose, CA, USA, 1–20.

[100] Nicolas Papernot, Martín Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. 2017. Semi-supervised knowledge transfer for deep learning
from private training data. In Proceedings of the 5th International Conference on Learning Representations (ICLR 2017). OpenReview.net, Toulon,
France, 1–16.

[101] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against
machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security (AsiaCCS). ACM, Abu Dhabi,
United Arab Emirates, 506–519.

[102] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael P Wellman. 2018. SoK: Security and privacy in machine learning. In Proceedings

of the 2018 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, London, UK, 399–414.

[103] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense to adversarial perturbations against

deep neural networks. In Proceedings of the 2016 IEEE Symposium on Security and Privacy (SP). IEEE, New York, NY, 582–597.

[104] Bong-Won Park and Kun Chang Lee. 2011. The effect of users’ characteristics and experiential factors on the compulsive usage of the smartphone.
In Proceedings of the International Conference on Ubiquitous Computing and Multimedia Applications. Springer, Daejeon, Korea, 438–446.
[105] Pranav Patel, Eamonn Keogh, Jessica Lin, and Stefano Lonardi. 2002. Mining motifs in massive time series databases. In Proceedings of the 2002

IEEE International Conference on Data Mining (ICDM). IEEE, Maebashi City, Japan, 370–377.

[106] L Yu Paul, Gunjan Verma, and Brian M Sadler. 2015. Wireless physical layer authentication via fingerprint embedding. IEEE Communications

Magazine 53, 6 (2015), 48–53.

[107] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,
Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research 12, Oct (2011), 2825–2830.
[108] Himanshu Raj, Ripal Nathuji, Abhishek Singh, and Paul England. 2009. Resource management for isolation enhanced cloud services. In Proceedings

of the 2009 ACM workshop on Cloud Computing Security. ACM, Chicago, Illinois, USA, 77–84.

[109] Mauro Ribeiro, Katarina Grolinger, and Miriam AM Capretz. 2015. Mlaas: Machine learning as a service. In Proceedings of the 2015 IEEE 14th

International Conference on Machine Learning and Applications (ICMLA). IEEE, Miami, FL, USA, 896–902.

[110] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes. 2019. ML-Leaks: Model and Data Independent
Membership Inference Attacks and Defenses on Machine Learning Models. In Proceedings of the 26th Annual Network and Distributed System
Security Symposium (NDSS). IEEE, San Diego, California, USA, 1–15.

[111] Ferdinando S Samaria and Andy C Harter. 1994. Parameterisation of a stochastic model for human face identification. In Proceedings of the Second

IEEE Workshop on Applications of Computer Vision. IEEE, Sarasota, FL, USA, 138–142.

[112] AMAZON ML SERVICES. 2019. Amazon aws Machine Learning. https://aws.amazon.com/machine-learning/
[113] Snehkumar Shahani, Jibi Abraham, and R Venkateswaran. 2017. Distributed Data Aggregation with Privacy Preservation at Endpoint. In Proceedings

of the IEEE International Conference on Management of Data. IEEE, Chennai, India, 1–9.

[114] Richard Shay, Saranga Komanduri, Adam L Durity, Phillip Seyoung Huh, Michelle L Mazurek, Sean M Segreti, Blase Ur, Lujo Bauer, Nicolas Christin,
and Lorrie Faith Cranor. 2014. Can long passwords be secure and usable?. In Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems. ACM, Toronto, ON, Canada, 2927–2936.

[115] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and

Communications Security (CCS). ACM, Denver, Colorado, USA, 1310–1321.

[116] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In

Proceedings of the 2017 IEEE Symposium on Security and Privacy (SP). IEEE, San Jose, CA, USA, 3–18.

[117] Amit Kumar Sikder, Hidayet Aksu, and A Selcuk Uluagac. 2017. 6thsense: A context-aware sensor-based attack detector for smart devices. In

Proceedings of the 26th USENIX Security Symposium (USENIX Security 17). USENIX Association, Vancouver, BC, Canada, 397–414.

[118] Tom W Smith, Peter Marsden, Michael Hout, and Jibum Kim. 2012. The General social surveys. Technical Report. National Opinion Research Center

at the University of Chicago.

[119] Raphael Spreitzer, Felix Kirchengast, Daniel Gruss, and Stefan Mangard. 2018. ProcHarvester: Fully automated analysis of procfs side-channel
leaks on Android. In Proceedings of the 2018 on Asia Conference on Computer and Communications Security (AsiaCCS). ACM, Incheon, Republic of
Korea, 749–763.

[120] Nedim Srndic and Pavel Laskov. 2014. Practical evasion of a learning-based classifier: A case study. In Proceedings of the 2014 IEEE Symposium on

Security and Privacy (SP). IEEE, San Jose, CA, USA, 197–211.

[121] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs. computer: Benchmarking machine learning algorithms for

traffic sign recognition. Neural networks 32 (2012), 323–332.

[122] Jingchao Sun, Xiaocong Jin, Yimin Chen, Jinxue Zhang, Yanchao Zhang, and Rui Zhang. 2016. VISIBLE: Video-assisted keystroke inference from
tablet backside motion. In Proceedings of the 23rd Annual Network and Distributed System Security Symposium (NDSS). IEEE, San Diego, CA, USA,

Manuscript submitted to ACM

42

1–15.

Miao et al.

[123] Nan Sun, Jun Zhang, Paul Rimba, Shang Gao, Yang Xiang, and Leo Yu Zhang. 2019. Data-driven cybersecurity incident prediction: A survey. IEEE

Communications Surveys & Tutorials 21, 2 (2019), 1744–1772.

[124] Yang Tang, Phillip Ames, Sravan Bhamidipati, Ashish Bijlani, Roxana Geambasu, and Nikhil Sarda. 2012. CleanOS: Limiting Mobile Data Exposure
with Idle Eviction. In Proceedings of the USENIX Symposium on Operating Systems Design and Implementation (OSDI). USENIX, Hollywood, CA,
USA, 77–91.

[125] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. 2016. Stealing Machine Learning Models via Prediction APIs. In

Proceedings of the 25th USENIX Security Symposium (USENIX Security 16). USENIX Association, Washington, D.C., USA, 601–618.

[126] UCIdataset. 2018. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets.html
[127] Blase Ur, Sean M Segreti, Lujo Bauer, Nicolas Christin, Lorrie Faith Cranor, Saranga Komanduri, Darya Kurilova, Michelle L Mazurek, William
Melicher, and Richard Shay. 2015. Measuring Real-World Accuracies and Biases in Modeling Password Guessability.. In Proceedings of the 24th
USENIX Security Symposium (USENIX Security 15). USENIX Association, Washington, D.C., USA, 463–481.

[128] Rafael Veras, Christopher Collins, and Julie Thorpe. 2014. On Semantic Patterns of Passwords and their Security Impact. In Proceedings of the 21st

Annual Network and Distributed System Security Symposium (NDSS). IEEE, San Diego, CA, USA, 1–16.

[129] Ben Verhoeven and Walter Daelemans. 2014. CLiPS Stylometry Investigation (CSI) corpus: a Dutch corpus for the detection of age, gender,
personality, sentiment and deception in text. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC).
European Languages Resources Association (ELRA), Reykjavik, Iceland, 3081–3085.

[130] Staal A Vinterbo. 2012. Differentially private projected histograms: Construction and use for prediction. In Proceedings of the Joint European

Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Bristol, UK, 19–34.

[131] B. Wang and N. Z. Gong. 2018. Stealing Hyperparameters in Machine Learning. In Proceedings of the 2018 IEEE Symposium on Security and Privacy

(SP). IEEE, San Francisco, CA, USA, 36–52.

[132] Ding Wang, Zijian Zhang, Ping Wang, Jeff Yan, and Xinyi Huang. 2016. Targeted online password guessing: An underestimated threat. In Proceedings

of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Vienna, Austria, 1242–1254.

[133] Matt Weir, Sudhir Aggarwal, Michael Collins, and Henry Stern. 2010. Testing metrics for password creation policies by attacking large sets of
revealed passwords. In Proceedings of the 17th ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Chicago, Illinois,
USA, 162–175.

[134] Matt Weir, Sudhir Aggarwal, Breno De Medeiros, and Bill Glodek. 2009. Password cracking using probabilistic context-free grammars. In Proceedings

of the 2009 IEEE Symposium on Security and Privacy (SP). IEEE, Berkeley, CA, USA, 391–405.

[135] Pei-Yuan Wu, Chi-Chen Fang, Jien Morris Chang, and Sun-Yuan Kung. 2017. Cost-effective kernel ridge regression implementation for keystroke-

based active authentication system. IEEE transactions on cybernetics 47, 11 (2017), 3916–3927.

[136] Qiuyu Xiao, Michael K Reiter, and Yinqian Zhang. 2015. Mitigating storage side channels using statistical privacy mechanisms. In Proceedings of

the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Denver, Colorado, USA, 1582–1594.

[137] Zhi Xu, Kun Bai, and Sencun Zhu. 2012. Taplogger: Inferring user inputs on smartphone touchscreens using on-board motion sensors. In Proceedings

of the 5th ACM Conference on Security and Privacy in Wireless and Mobile Networks. ACM, Tucson, AZ, USA, 113–124.

[138] Tarun Yadav and Arvind Mallari Rao. 2015. Technical aspects of cyber kill chain. In Proceedings of the International Symposium on Security in

Computing and Communication. Springer, New York, NY, 438–452.

[139] Zheng Yan and Mingjun Wang. 2017. Protect pervasive social networking based on two-dimensional trust levels. IEEE Systems Journal 11, 1 (2017),

207–218.

[140] Dingqi Yang, Daqing Zhang, and Bingqing Qu. 2016. Participatory cultural mapping based on collective behavior data in location-based social

networks. ACM Transactions on Intelligent Systems and Technology (TIST) 7, 3 (2016), 30:1–30:23.

[141] Yelp. 2014. Yelp Open Dataset. https://www.yelp.com/dataset
[142] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks?. In Proceedings of the

Advances in Neural Information Processing Systems. NIPS, Montreal, Quebec, Canada, 3320–3328.

[143] Yan Yu, Jianhua Wang, and Guohui Zhou. 2010. The exploration in the education of professionals in applied internet of things engineering. In

Proceedings of the 4th International Conference on Distance Learning and Education (ICDLE). IEEE, San Juan, PR, USA, 74–77.

[144] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. 2017. Deep sets. In

Proceedings of the Advances in Neural Information Processing Systems. Curran Associates, Inc., Long Beach, CA, USA, 3391–3401.

[145] Yong Zeng and Rui Zhang. 2016. Active eavesdropping via spoofing relay attack. In Proceedings of the 2016 IEEE International Conference on

Acoustics, Speech and Signal Processing (ICASSP). IEEE, Shanghai, China, 2159–2163.

[146] Jun Zhang, Chao Chen, Yang Xiang, Wanlei Zhou, and Yong Xiang. 2013. Internet traffic classification by aggregating correlated naive bayes

predictions. IEEE Transactions on Information Forensics and Security 8, 1 (2013), 5–15.

[147] Jun Zhang, Xiao Chen, Yang Xiang, Wanlei Zhou, and Jie Wu. 2015. Robust network traffic classification. IEEE/ACM Transactions on Networking

(TON) 23, 4 (2015), 1257–1270.

[148] Jun Zhang, Yang Xiang, Yu Wang, Wanlei Zhou, Yong Xiang, and Yong Guan. 2013. Network traffic classification using correlation information.

IEEE Transactions on Parallel and Distributed Systems 24, 1 (2013), 104–117.

Manuscript submitted to ACM

Machine Learning Based Cyber Attacks Targeting on Controlled Information: A Survey

43

[149] Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fergus, and Lubomir Bourdev. 2015. Beyond frontal faces: Improving person recognition using
multiple cues. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, New York, NY, 4804–4813.
[150] Nan Zhang, Kan Yuan, Muhammad Naveed, Xiaoyong Zhou, and XiaoFeng Wang. 2015. Leave me alone: App-level protection against runtime

information gathering on android. In Proceedings of the 2015 IEEE Symposium on Security and Privacy (SP). IEEE, San Jose, CA, USA, 915–930.

[151] Xiaokuan Zhang, Xueqiang Wang, Xiaolong Bai, Yinqian Zhang, and XiaoFeng Wang. 2018. OS-level side channels without procfs: Exploring
cross-app information leakage on iOS. In Proceedings of the 25th Annual Network and Distributed System Security Symposium (NDSS). IEEE, San
Diego, CA, USA, 1–15.

[152] Ziqiao Zhou, Michael K Reiter, and Yinqian Zhang. 2016. A software approach to defeating side channels in last-level caches. In Proceedings of the

2016 ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, Vienna, Austria, 871–882.

Manuscript submitted to ACM

