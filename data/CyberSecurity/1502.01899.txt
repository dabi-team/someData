5
1
0
2

r
a

M
0
1

]

R
C
.
s
c
[

2
v
9
9
8
1
0
.
2
0
5
1
:
v
i
X
r
a

A framework for trustworthiness assessment
based on ﬁdelity in cyber and physical domains

Vincenzo De Florio† and Giuseppe Primiero‡
†: MOSAIC, University of Antwerp & iMinds, 2020 Antwerpen, Belgium
‡: Department of Computer Science, Middlesex University, London, UK

September 17, 2018

Abstract

We introduce a method for the assessment of trust for n-open systems based on a measurement of
ﬁdelity and present a prototypic implementation of a complaint architecture. We construct a MAPE
loop which monitors the compliance between corresponding ﬁgures of interest in cyber- and physical
domains; derive measures of the system’s trustworthiness; and use them to plan and execute actions
aiming at guaranteeing system safety and resilience. We conclude with a view on our future work.

1 Introduction

Fidelity of an open system can be interpreted as the compliance between corresponding ﬁgures of interest
in two separate but communicating domains, see [9]. In cyber-physical systems, perfect ﬁdelity means
that actions in the physical domain have a well-deﬁned and stable counterpart in the cyber domain, and
vice-versa. This is an ideal state, as no concrete cyber-physical system may guarantee at all times a
perfect correspondence between its domains of action. In practice, ﬁdelity is aﬀected by circumstances
that let the system drift from the optimal case. Our stance is that, by observing the characteristics of
said drifting, we may introduce a ﬁne-grained characterisation of the quality of system trustworthiness.
To this aim, we introduce a practical method for the assessment of trust based on the measurement of
ﬁdelity in computational systems, including cyber-physical ones. As a way to measure ﬁdelity drifting we
propose to adopt and extend the approach described in [7, 10]. We propose to make use of the accrued
information to assess the characteristics of drifting in ﬁdelity; we derive from it dynamic properties of
both user and machine; evaluate it in terms of system’s trustworthiness and use it to execute safety-
assurance actions. This generates a trust-induced MAPE-loop.

The paper is structured as follows. In Sect. 2 we describe the conceptual model of system ﬁdelity; in
Sect. 3 the model for ﬁdelity evaluation is implemented in an architecture for a cyber-physical system; in
Sect. 4 we build the link to trustworthiness evaluation; ﬁnally, in Sect. 5 we conclude by drawing some
observation and describing further lines of research.

2 Conceptual understanding of System Fidelity

In the present section we introduce a conceptual model of system ﬁdelity, derived from the one presented
in [9]. Our starting point is the concept of n-open systems (nOPS), characterised by the following
properties:

• nOPS interact with one or more of the environments they are deployed in.

• nOPS base their action on the ability to sense n classes of raw facts taking place in their deployment

environments.

• nOPS are able to construct and maintain n classes of internal representations of the raw facts,

called qualia.

1

 
 
 
 
 
 
• Qualia are, to some extent, faithful, meaning that they timely reﬂect the dynamic variation of the

corresponding class of raw facts.

We discuss here ﬁdelity as a characterisation of the above-mentioned faithfulness. More formally,
given any S ∈ nOPS, we consider n classes of raw facts, [r]i, 1 ≤ i ≤ n, and n classes of binary operations,
[(cid:117)]i, such that each of the couples

([r]i, [(cid:117)]i)
(1)
constitutes an algebraic structure; for instance, when [(cid:117)]i is a singleton, then ([r]i, [(cid:117)]i) is a group.
Likewise, for any 1 ≤ i ≤ n, we call [q]i the class of qualia corresponding to [r]i and [⊕]i the class of
binary operations corresponding to [(cid:117)]i. Moreover, as we did for (1), we assume that ([q]i, [⊕]i) is an
algebraic structure. Then, for each 1 ≤ i ≤ n, we consider the following function:

Φi : [r]i → [q]i,

(2)

mapping the qualia corresponding to any raw fact in [r]i. We refer to the Φi functions as the reﬂective
maps of some n-open system S. Reﬂective maps are assumed to be bijective functions, with Φ−1
being
the inverted reﬂective maps of S associating the raw fact corresponding to each input quale. We shall say
that Φi expresses perfect ﬁdelity between ([r]i, [(cid:117)]i) and ([q]i, [⊕]i) if and only if Φi preserves its algebraic
structures (i.e., it is an isomorphism). More formally, for any couple of raw facts (r1, r2) ∈ [r]i × [r]i and
for all + ∈ [(cid:117)]i and all · ∈ [⊕]i: Φi(r1 + r2) = Φi(r1) · Φi(r2).

i

Perfect ﬁdelity may be better understood through an example. Let us assume that S is a cyber-
physical system responsible for the operation of a mission critical hard-real-time service. An operator
is responsible for the issuing of requests for service, which is done through a user interface (UI). A
set of raw facts and prescribed behaviours pertaining to the physical environment are represented as
“cyber-qualia” and “cyber-behaviours” stored in computer memories. Likewise, a set of “UI-qualia”
and “UI-behaviours” are respectively rendered and operable through the UI. Perfect ﬁdelity states that
the correspondence between the physical, the cyber, and the UI domains is such that the prescribed
behaviours as well as the referred raw facts and qualia are consistent on either of the involved domains.
Thus certain operations and objects represented and rendered via the UI perfectly correspond to opera-
tions and objects encoded in S’s computer components, which in turn perfectly correspond to physical
actions having eﬀects on physical entities. Obviously, perfect ﬁdelity only represents a reference point
and can not be sustained and guaranteed at all times in real life. A slightly diﬀerent and more practical
deﬁnition of ﬁdelity is given by a function φi, 1 ≤ i ≤ n:

φi : [r]i → [q]i,

such that ∀ + ∈ [(cid:117)]i, ∀ · ∈ [⊕]i : φi(r1 + r2) = φi(r1) · φi(r2) · ∆i(t).

(3)

As for the Φi function, φi returns the qualia associated with the input raw facts. Contrarily to the Φi
function, the φi does not preserve their algebraic structures unless the value of the error component ∆i(t)
is zero. The use of lower-case “φ” is meant to suggest that φi represents a less-than-perfect version of Φi.
The ∆i(t) quantiﬁes a drifting in time (represented by variable t) of the ability to create a trustworthy
“internal” representation of an experienced raw fact.

In [9], ﬁdelity is classiﬁed in function of the type of drifting. Classes may include, e.g., the following

cases:

• Hard-bound ﬁdelity drifting, exempliﬁed by hard-real-time nOPS.

• Statistically-bound ﬁdelity drifting—as typical of, e.g., soft real-time systems.

• Unbound ﬁdelity drifting characterised by a “trend”.

• Unbound ﬁdelity drifting with a random trend.

Accordingly, very disparate cases can be presented to exemplify imperfect ﬁdelity, e.g. the accidents
experienced by the linear accelerator Therac-25 [21, 8] and the system failure caused by the last Scud
ﬁred during the Gulf War [24]. In the former case, one of the reasons that led to some of the accidents was
that the UI-qualia and the cyber-qualia did not match when the operator was typing at a very fast pace.

2

As a result of such imperfect ﬁdelity, the quantities represented on the screen of the Therac-25 did not
correspond to the data stored in its memories—and, regrettably, to the amount of radiations supplied to
the patients by the linear accelerator. In the Patriot case, resulting in 28 US Army reservists being killed
and 97 injured by a Scud missile on 25 February 1991. The missile-defence system was an nOPS that
interacted with its environment through a number of context ﬁgures that included velocity and time. As
discussed in [18], the cyber-quale corresponding to physical time was represented as the number of tenths
of seconds from a reference epoch and stored in a 24-bit integer variable. Imprecision in the conversion
of said variable into a real number translated in an unbound drifting of ﬁdelity over time. The more the
Patriot missile-defence system operated without a reboot, the larger was the ∆i pertaining to time and,
as a consequence, the greater the discrepancy between the expected and the real position and velocity of
the incoming Scud missile. An obvious workaround to the above unbound drifting is that of rebooting
the system regularly so as to rejuvenate [17] the qualia management system and bring back the ∆i to
“safe” values. Although both problem and workaround were known at the time of the accident, no upper
bound was known beyond which the resilience of the system would be aﬀected. Common belief was that
the unresilience threshold would never be reached in practice. Regrettably, reality proved the trust on
that belief to be misplaced. The Patriot missile that had to intercept the Scud never took oﬀ. The cases
of the Therac-25 and of the Patriot system reveal a common denominator: behaviours such as those of a
human operator or those produced by a numerical algorithm are all translated into a same, homogeneous
form: that of a stream of numerical data representing samples of the ∆i(t) dynamic systems. A major
methodological assumption in the present work is that the above data could be compared with other data
representing reference conditions. In the Therac-25 case, such data may correspond to, e.g., reference
user stereotypes of expected operator behaviours, represented for instance as the numerical weights in a
Hidden Markov Model [15]. Likewise, in the Patriot case, those reference data may correspond to, e.g.,
a threshold representing safe ranges for accumulated or cumulative numerical errors produced through
the iterations of numerical methods. In both the exempliﬁed cases, assessing ﬁdelity is thus translated
into the problem of evaluating a “distance” between observed and reference data.

In what follows, we propose an architecture and a prototypical system to evaluate systematically a
system’s ﬁdelity drifting. These are modelled on a toy example, easily modiﬁed and extended to a real
case scenario, whose presentation we reserve to an extended version of this work.

3 Janus’ Architecture

In view of the above discussion, our approach to systematic evaluation of ﬁdelity requires at least the
following components:

• A sensory service, interfacing the deployment environments so as to register a number of “raw
facts”, namely variations in a number of environmental properties. Raw facts could refer, for
instance, to variations in luminosity, temperature, or the amount of network bandwidth available
between two endpoints.

• A uniform qualia service, providing consistent, timely and reliable access to cyber-qualia (computer-

based representations of the raw facts).

• An application layer, providing a convenient means for ﬁdelity assessment and reactive control.

In what follows we introduce the above components and a prototypical system compliant to the just
sketched models, see Fig. 1.

3.1 Sensory and Qualia Services

Our sensory and qualia service is based on so-called reﬂective and refractive variables (RRvars) [12, 11],
a tool for the development of nOPS in the C programming language. The idea behind RRvars is quite
simple: a number of predeﬁned variables provide access to the qualia of corresponding raw facts. Those
variables are “volatile”, meaning that their content is asynchronously and continuously updated by
an associated thread. Thus for instance RRvar int cpu does not retain a constant value; rather, it

3

Figure 1: Sequence diagram of the Janus’ components.

is continuously updated by the Cpu() thread. Such thread cyclically retrieves the percentage of CPU
currently in use and stores it in the memory cells associated to cpu as an integer number ranging between
0 (CPU fully available) and 100 (no CPU available). In the current implementation, which runs on Linux
and Windows/Cygwin systems, Cpu() retrieves its raw facts by calling the top utility. This is referred
to as process1b in Fig. 1.

A second and slightly more complex example is given by RRvar int mplayer, a variable updated
by thread Mplayer(), referred to as process2b in Fig. 1. The latter component communicates with an
instrumented MPlayer movie player [2] through a simple UDP client/server protocol. By reading the
content of mplayer one is informed of the state of the MPlayer—see Fig. 3. Currently, the following
integer values are used:

#define UDPMSG_STOP
#define UDPMSG_SLOW
#define UDPMSG_PAUSED
#define UDPMSG_START
#define UDPMSG_SIGNAL

1 // mplayer has finished playing a video
2 // mplayer is encountering problems while playing a video
3 // mplayer has been paused
4 // mplayer has been launched
5 // mplayer caught an exception and is about to exit abnormally.

A third case is given by RRvar int ui, updated by thread Ui(). This is a special case in that
this RRvar represents a UI-qualia (see Sect. 2) reporting raw facts speciﬁc of an instance of a user
interface. This is referred to as process3b–process3e in Fig. 1. Said user interface and the Ui() thread
communicate transparently of the operator via the same mechanism presented for RRvar mplayer.
The values returned in RRvar int ui represent usability raw facts derived by comparing the behaviours
exercised by the current user with “reference behaviours” representing the expected behavioural patterns
of a trustworthy operator. The method to derive these raw facts is described in [7, 15]. This method
may be used to detect gradual behavioural driftings (due to, e.g., fatigue, stress, or the assumption of
psychotropic substances) and sudden behavioural driftings (caused, e.g., by an account takeover or other
cyber-criminal attacks).

3.2 Control Layer: Janus

Janus is the name of our exemplary RRvar client component.1 The structure of Janus is the one typical
of RRvar clients [11] and exempliﬁed in Fig. 2. As can be seen from the picture, the RRvar metaphor
makes it possible to quickly deﬁne nOPS components based on the three classes of qualia presented
above, see Fig. 4.

1As for the system described in [13], the name of our component comes from mythical Janus Bifrons, the god of
transitions, who had two faces and thus could observe and reason by considering two diﬀerent “views” at the same time.
Of the proposed etymologies of Janus, particularly intriguing here is the one proposed by Paul the Deacon [14]: hiantem,
hiare, “to be open”. Due to this fact one would be tempted to refer to Janus Bifrons here as to a 2-open system.

4

Figure 2: Typical structure of an RRvar client. Here three RRvars (cpu, mplayer, and ui) are declared
and continuously displayed.

Figure 3: An instance of the MPlayer connects with Janus and reports its state.

Figure 4: The RRvar client connects with both MPlayer and an exemplary user interface.

5

Figure 5: Log of the interaction between user and UI. Between 50 and 60s a rapid burst of keystrokes is
interpreted as an anomalous situation.

4 Fidelity as Trustworthiness

In this ﬁnal section, we link the model of open systems oﬀered in Sect. 2, and their architecture im-
plemented in Sect. 3, to a model of trustworthiness evaluation. We use ﬁdelity to assess the working
conditions of an open system and to establish a metric of trustworthiness. Our ﬁnal objective is to
provide a qualitative extension of the standard MAPE-loop based on trustworthiness assessment. Trust
for security, management and reputation systems is gaining a lot of attention in the literature and it
is typically accounted for as a ﬁrst-order relation between (possibly autonomous) agents or system’s
[4, 6, 5, 16, 25]. Our account considers trust as a second order property character-
modules, see e.g.
ising cumulatively system’s ﬁdelity, where the latter is obtained as the dynamic variation of properties
of the system’s modules. This approach to second-order trust has been already used for information
transmission evaluations ([23]), access-based control ([22]), and software management systems ([19]). In
the present analysis, trustworthiness is used to plan reaction to malfunctioning and restoring of func-
tionalities in cyber-physical systems. As the cases of Therac-25 and the Patriot-missile defence system
show, unacknowledged drifting can be crucial in maximising unjustiﬁed trust to dangerous levels. On
the other hand, a metric evaluating minimal functionality thresholds can minimise unjustiﬁed mistrust,
reducing conﬁdence that the system will choke and eventually fail (antitrust). Early cases of low true-
alarm rate or high false-alarm rate in automation do not need to set constants for future behaviour of
the user. Similarly, criteria to compare intended and current behaviour are essential to allow mechanical
assessment of user’s inability, incompetence or threats. Fidelity and drifting can provide the systematic
methodology and quantitative model to continually evaluate and experiment the system’s vulnerability
in the context of its operations, thus making the dichotomy conditional vs. unconditional trust (faith)
working. Trust can be thought of as a measure of conﬁdence about minimal drifting from ﬁdelity for
all the components, and hence that the overall behaviour of the system is suﬃciently resilient. In the
following, we reconstruct the process of ﬁdelity monitoring, trustworthiness evaluation and operation
execution in terms of a MAPE-loop designed for trust, see [20]. In the architecture presented in Sect. 3,
the Janus assesses the behaviour of the system. This is parametrised in view of reﬂective variables
for CPU consumption and a component’s operations (on the machine side of the system), and for the
user interface (on the user’s side). Mapping of these variables values as raw facts and qualia provides a
measure of system’s ﬁdelity:

• ΦCP U : [r]CP U → [q]CP U , obtained by the mapping of input values from the top process to

pre-selected parameters assigned to CPU-consumption behaviour;

• Φcomponent : [r]component → [q]component, obtained by the mapping of input values from the exe-

cutable’s observable behaviour to pre-selected parameters assigned to its operations;

• ΦU I : [r]U I → [q]U I , obtained by the mapping of input values from the user’s observable behaviour

6

to pre-selected parameters assigned to a standard or expected user’s behaviour.

The system’s component monitoring the classes [r]i of raw facts is called the sensor layer ; similarly, we
use qualia layer to refer to the component monitoring the classes [q]i of qualia. The combination of the
sensory and representative layers constitutes the Monitoring component within our MAPE-loop. Fidelity
is then approximated as the inversely proportional function of the drifting from appropriate mappings
Φi. We shall refer to the value of user-based mappings as user-deﬁned ﬁdelity (ΦU ); correspondingly, we
shall call machine-deﬁned ﬁdelity (ΦM ) the value based on mappings related to the machine behaviour.
For the Janus introduced in Sect. 3,

ΦU = 1/∆(t)U I
ΦM = 1/f (∆(t)CP U , ∆(t)exec)

(4)

(5)

where f is some function, weighted according to domain-speciﬁc and user deﬁned parameters. We refer
to the set of values Φ(t) = {ΦU , ΦM } as the content of our Analysis component, with the global value
Φ parametrised by time. As an example, consider the class of mappings ΦU I , with a value of the sensor
layer indicating e.g. quick typing and a value of the qualia layer returning a distress indication: in this
case the ﬁdelity layer reports an high value. As an example across distinct mappings, assume that the
reﬂective variable for MPlayer indicates that the application is running slower, while the one for CPU
monitoring indicates low usage value: this is expressed by a low ﬁdelity value across the two classes in
ΦM . Analysing ﬁdelity values across the distinct monitoring layers allows a cumulative evaluation to
be obtained. This value is monitored by the apperception layer. This layer is used to evaluate system
trustworthiness as a global value of user-deﬁned and machine-deﬁned ﬁdelity values. The next level
is represented by the Janus feeding the content of the apperception layer into the control layer. This
corresponds to the Planning component of our system. At this stage, system trustworthiness is matched
to a resilience scale that identiﬁes and automatically triggers actions aimed at preserving system safety or
enabling ameliorating conditions. The latter part of the system is the Execution component, monitoring
an action layer. Despite the fact that a resilience scale should be highly domain speciﬁc, a possible
general model can be given in terms of four essential stages:

1. Trustworthy System identiﬁes high levels of Φ(t), inducing optimal, sustainable working conditions;

2. Unstable System identiﬁes high-to-medium ΦU and low ΦM levels, inducing reconﬁgurable working

conditions;

3. Unsafe System identiﬁes high-to-medium ΦM and low ΦU levels, inducing alarm-rising working

conditions;

4. Untrustworthy System identiﬁes low-levels of Φ(t), inducing inadvisable or below-safety working

conditions.

The above analysis is summarised in the MAPE-loop in Fig. 6.

5 Conclusions

We have presented a prototypical model architecture for a trust-based MAPE-loop for cyber-physical
systems. The trust evaluation is grounded on the assessment of ﬁdelity drifting with respect to values
representing ideal reference conditions for both user and machine. Although prototypical, we believe
that our architecture proves the feasibility of our approach to trustworthiness assessment and paves the
way towards future implementations. Our goal is to develop systems that are trustworthy in integrating
quality-of-service and quality-of-experience, by optimising the relations between system-level, “micro-
scopic” aspects and user-level, “macroscopic” ones. A further goal is to extend our architecture into
that of a MAPE-K loop and apply machine learning methods such that systems based on our approach
may systematically improve the match with the environments they interact with. Costs analysis for
the trust-based MAPE-loop remains to be explored. Its use at early stages of design can reduce risks;

7

Figure 6: A MAPE-loop for System trustworthiness based on ﬁdelity.

our examples shows, nonetheless, that it is possible to deploy the methodology on existing software, by
selecting relevant variables. Further work will focus on formal modelling of trustworthiness assessment
in a probabilistic epistemic setting.

References

[1] Anonymous. Patriot missile defense: Software problem led to system failure at dhahran, saudi
arabia. Technical Report GAO/IMTEC-92-26, U.S. Government Accountability Oﬃce, US, General
Accounting Oﬃce, Washington, D.C., 20548, 1992.

[2] Anonymous. Mplayer — the movie player, 2015.

Retrieved on February 3, 2015 from

www.mplayerhq.hu/design7/info.html.

[3] Anonymous. Top - display linux processes, 2015. Manual page obtained through the command

“man top” on Linux systems.

[4] Marco Carbone, Mogens Nielsen, and Vladimiro Sassone. A formal model for trust in dynamic
networks. In 1st International Conference on Software Engineering and Formal Methods (SEFM
2003), 22-27 September 2003, Brisbane, Australia, page 54. IEEE Computer Society, 2003.

[5] Jian Chang, Krishna K. Venkatasubramanian, Andrew G. West, Sampath Kannan, Boon Thau
Loo, Oleg Sokolsky, and Insup Lee. AS-TRUST: A trust quantiﬁcation scheme for autonomous
systems in BGP. In Jonathan M. McCune, Boris Balacheﬀ, Adrian Perrig, Ahmad-Reza Sadeghi,
Angela Sasse, and Yolanta Beres, editors, Trust and Trustworthy Computing - 4th International
Conference, TRUST 2011, Pittsburgh, PA, USA, June 22-24, 2011. Proceedings, volume 6740 of
Lecture Notes in Computer Science, pages 262–276. Springer, 2011.

[6] Stephen Clarke, Bruce Christianson, and Hannan Xiao. Trust*: Using local guarantees to extend the
reach of trust. In Bruce Christianson, James A. Malcolm, Vashek Matyas, and Michael Roe, editors,
Security Protocols XVII, 17th International Workshop, Cambridge, UK, April 1-3, 2009. Revised
Selected Papers, volume 7028 of Lecture Notes in Computer Science, pages 171–178. Springer, 2009.

[7] V. De Florio and C. Blondia. Safety enhancement through situation-aware user interfaces. In System
Safety, incorporating the Cyber Security Conference 2012, 7th IET International Conference on,
pages 1–6, Oct 2012.

[8] Vincenzo De Florio. Software assumptions failure tolerance: Role, strategies, and visions. In Antonio
Casimiro, Rog´erio de Lemos, and Cristina Gacek, editors, Architecting Dependable Systems VII,

8

volume 6420 of Lecture Notes in Computer Science, pages 249–272. Springer Berlin / Heidelberg,
2010. 10.1007/978-3-642-17245-8 11.

[9] Vincenzo De Florio. Antifragility = elasticity + resilience + machine learning. models and algo-
rithms for open system ﬁdelity. Procedia Computer Science, 32:834–841, 2014. 1st ANTIFRAGILE
workshop (ANTIFRAGILE-2015), the 5th International Conference on Ambient Systems, Networks
and Technologies (ANT-2014).

[10] Vincenzo De Florio and Chris Blondia. Reﬂective and refractive variables: A model for eﬀective and
maintainable adaptive-and-dependable software. In Proc. of the 33rd EUROMICRO Conference on
Software Engineering and Advanced Applications (SEAA 2007), L¨ubeck, Germany, August 2007.

[11] Vincenzo De Florio and Chris Blondia. Reﬂective and refractive variables: A model for eﬀective and
maintainable adaptive-and-dependable software. In Proceedings of the 33rd Euromicro Conference
on Software Engineering and Advanced Applications (SEEA 2007), Software Process and Product
Improvement track (SPPI), L¨ubeck, Germany, August 2007. IEEE Computer Society.

[12] Vincenzo De Florio and Chris Blondia. On the requirements of new software development. Inter-

national Journal of Business Intelligence and Data Mining, 3(3), 2008.

[13] Vincenzo De Florio, Geert Deconinck, Mario Truyens, Wim Rosseel, and Rudy Lauwereins. A
hypermedia distributed application for monitoring and fault-injection in embedded fault-tolerant
parallel programs. In Proc. of the 6th Euromicro Workshop on Parallel and Distributed Processing
(Euro-PDP’98), pages 349–355, Madrid, Spain, January 1998. IEEE Comp. Soc. Press.

[14] Paulus Winfridus Diaconus. Excerpta ex libris Pompeii Festi de signiﬁcatione verborum. W. M.

Lindsay, 1930.

[15] Jens Van Duyse. A toolkit for the concurrent analysis and adaptation of graphical user inter-
faces. Master’s thesis, Department of Mathematics and Computer Science, University of Antwerp,
Middelheimlaan 1, 2020 Antwerpen, Belgium, 2013. Promotor: Vincenzo De Florio.

[16] Tyrone Grandison and Morris Sloman. A survey of trust in internet applications. Commun. Surveys

Tuts., 3(4):2–16, October 2000.

[17] Michael Grottke, Matias Jr. Rivalino, and Kishor S. Trivedi. The fundamentals of software aging. In
Proceedings of the 1st International Workshop on Software Aging and Rejuvenation, 19 International
Symposium on Software Reliability Engineering, 2008.

[18] Michael Grottke and Kishor S. Trivedi. Fighting bugs: Remove, retry, replicate, and rejuvenate.

IEEE Computer, 40(2):107–109, 2007.

[19] Giuseppe Primiero Jaap Boender and Franco Raimondi. Minimizing transitive trust threats in
software management systems. Technical report, Foundations of Computing Group, Middlesex
University, 2015.

[20] Bart Jacob, Richard Lanyon-Hogg, Devaprasad K Nadgir, and Amr F Yassin. A Practical Guide to

the IBM Autonomic Computing Toolkit. IBM Redbooks, 2004.

[21] Nancy Leveson and Clark S. Turner. An investigation of the Therac-25 accidents. IEEE Computer,

26(7):18–41, 1993.

[22] Giuseppe Primiero and Franco Raimondi. A typed natural deduction calculus to reason about
secure trust.
In Ali Miri, Urs Hengartner, Nen-Fu Huang, Audun Jøsang, and Joaqu´ın Garc´ıa-
Alfaro, editors, 2014 Twelfth Annual International Conference on Privacy, Security and Trust,
Toronto, ON, Canada, July 23-24, 2014, pages 379–382. IEEE, 2014.

[23] Giuseppe Primiero and Mariarosaria Taddeo. A modal type theory for formalizing trusted commu-

nications. J. Applied Logic, 10(1):92–114, 2012.

9

[24] Eric Schmitt. After the war; army is blaming patriot’s computer for failure to stop the dhahran

scud. New York Times, May 1991.

[25] Zheng Yan and Christian Prehofer. Autonomic trust management for a component-based software

system. IEEE Transactions on Dependable and Secure Computing, 8(6):810–823, 2011.

10

