2
2
0
2

g
u
A
7
2

]

G
L
.
s
c
[

1
v
1
4
9
2
1
.
8
0
2
2
:
v
i
X
r
a

Virtual Control Group: Measuring Hidden
Performance Metrics

Moshe Tocker

Meta

June 2022

1 ABSTRACT

Performance metrics measuring in Financial Integrity systems are crucial for
maintaining an eﬃcient and cost eﬀective operation. An important
performance metric is False Positive Rate. This metric cannot be directly
monitored since we don’t know for sure if a user is bad once blocked. We
present a statistical method based on survey theory and causal inference
methods to estimate the false positive rate of the system or a single blocking
policy. We also suggest a new approach of outcome matching that in some
cases including empirical data outperformed other commonly used methods.
The approaches described in this paper can be applied in other Integrity
domains such as Cyber Security.

Keywords— causal inference, ﬁnancial risk, outcome score matching, performance
metrics, control group, false positive rate

2

INTRODUCTION

Measuring performance metrics in Financial Integrity Systems is crucial for decision
making. Integrity systems need to hold a high level of precision and block bad users
to reduce bad activity and comply with regulation and commercial requirements. On
the other hand, risk systems that block many good users provide a terrible user
experience which aﬀects growth and adoption of the services.
To monitor False Positives one can use a control group or periodically send
transactions for manual review. Control Groups cost is high as they introduce bad
activity into the system. In addition, for Integrity systems in many cases this
approach is not possible due to product or regulatory constraints. Manual labeling is
another approach to estimate false positive rate. This method however requires
manpower and is extremely noisy (fraud labeling is a high-dimensional problem).
Another shortcoming of the approaches described above is measurement time delay.
In the ﬁnancial domain the most common indications of fraud are chargebacks. A

1

 
 
 
 
 
 
chargeback is a return of money to a payer of a transaction, especially a credit card
transaction after a successful dispute. This process takes time and therefore the
indication of fraud takes time to arrive. This eﬀect introduces a delay in the metrics
measurement.
In this paper we present an alternative approach based on survey theory and causal
inference methods that is purely statistical and has many advantages in terms of
operational costs and time to measurement delay. We were able to predict the fraud
prevalence among the blocked transactions with acceptable errors in several use cases.
We believe that this approach can be useful in many integrity domains such as
Financial Services, Cyber security (Malware detection, Spam detection, . . . ),
Content Integrity and more.

3 PRELIMINARIES

Assume we have two functions of covariates vector x: Outcome Y(x) and Treatment
assignment model W(x). In the ﬁnancial integrity setting we control the treatment
assignment as we deﬁne the risk policies based on system risk signals (in contrast to
the normal causal inference settings in which some treatment covariates are
unknown). Units are sent to treatment based on the binary function W(x). W(x)
can model a single fraud policy or the entire risk policies as a whole. The outcome
function Y(x) is known only for units i of which W(xi)=0. We seek to estimate the
mean value of the outcome of the treated group E[Y |W = 1]. This problem can be
viewed as estimating population average of the non respondents from in a survey
which has been dealt with by many authors such as [8] and [5]. In the setting of
survey theory surveys are sent to a sampled population. Some of the people in the
sample might not respond to the survey (a.k.a non-respondents) and thus the survey
outcome would be unknown for them. Estimating the survey results just by the
outcome of the respondents might lead to a biased result. We suggest that
estimating false positive rate in ﬁnancial risk is an equivalent problem in which the
outcome is fraud or not-fraud and the non-respondents are the blocked transactions
for which the outcome is unknown.

3.1 Estimators

The ﬁelds of Survey theory and causal inference produced statistical methods to try
and replicate randomized experiments when only observational data is available. We
describe in this section some of the methods of which we have considered. The
method tries to estimate W(x), Y(x) or both based on the observed covariates and
then draw conclusions over the mean of an unobserved outcome of the treated group.

Many estimation methods are based on the Propensity Score which is the probability
of a unit to be treated and deﬁned by [8].

π(x) = P (W = 1|X = x)

(1)

Propensity Score matching is a well-known approach used widely in practice. Each
unit’s propensity score is estimated. Then units from the treated group are matched
to units from the untreated based on their nearest neighbor in terms of the
propensity score. The matching can be 1:1 also called pair matching or it can be

2

k-nearest neighbor matching in which every item in the treated group is matched to
k-nearest neighbors with replacement in the untreated group. Let Iπ be the set of
units in the untreated group which are the k-nearest neighbors by the propensity
score of some unit in the treated group. Let yi be the true outcome of unit i. The set
Iπ is then used to estimate the mean outcome of the treated group:

ˆµP SM =

1
|Iπ|

(cid:88)

yi

i∈Iπ

(2)

Let ˆm(x) be a Logistic Regression model ﬁtted on the outcome of the untreated
group. Outcome score matching is done by matching units from the treated group
with units from the untreated group with nearest values of ˆm(x). Let Iy be the set
of units in the untreated group which are the k-nearest neighbors by their predicted
outcome of some unit in the treated group. The mean outcome is then estimating
the mean the same way as in Equation 2.

ˆµOSM =

1
|Iy|

(cid:88)

yi

i∈Iy

(3)

We used matching on a single continuous covariate i,e the outcome score since
according to [1] for the case when only a single continuous covariate is used to
match, the eﬃciency loss can be made arbitrarily close to zero by allowing a
suﬃciently large number of matches.
We have also considered inverse propensity weighting estimator over the population
of nonrespondents [5]

ˆµIP W −N R =

(cid:80)
(cid:80)

i wiπ−1(1 − π)yi
i wiπ−1(1 − π)

(4)

A diﬀerent group of estimators are regression estimators. These estimators model the
outcome conditioned on the covariates using regression and use this model to
estimate the missing outcomes. Let the outcome model denoted by ˆm(x) and Ut the
set of units in the treated group then the mean predicted outcome estimator is

ˆµM P O =

1
|Ut|

(cid:88)

i∈Ut

ˆm(xi)

(5)

Doubly robust methods take into account both the model predictions for yi with
inverse-probability weights. These methods are highly eﬃcient when the y-model is
true yet remain asymptotically unbiased when the y-model is misspeciﬁed [6]. We
have considered such regression model for which the Logistic Regression was trained
with non-respondents weights π−1(1 − π) and refer to to it as ˆmw(x) so that the
weighted mean predicted outcome estimator is :

ˆµW M P O =

1
|Ut|

(cid:88)

i∈Ut

ˆmw(xi)

(6)

3.2

conﬁdence interval Estimation

In contrast to Machine Learning settings causal inference requires reporting of
conﬁdence intervals on top of the prediction. This is highly important so that the
analyst can make a proper decision and know when prediction is of low quality. We

3

use bootstrapping to estimate the standard error of all our estimators as
recommended by many authors including [2]. In bootstrapping data variability
estimation the dataset is sampled many times with replacement to produce a sample
with the same size. For each sample the estimator is being calculated. The standard
deviation of the estimations is calculated and reported as the estimation of the
standard error of the estimator. From this standard error (SE) the 95% conﬁdence
interval can be simply derived by

CI95% = [ˆµ − 1.96 ∗ SE, ˆµ + 1.96 ∗ SE]

(7)

4 SIMULATION STUDIES

4.1 Treated Group Mean Estimation

A simulation study was done to study the performance of diﬀerent estimators for the
potential mean outcome of the treated group. Design (1) is based on similar setting
described by [7] and design (2) is similar to one described in [2]

Treatment Design 1: W (X) = 1[a1X1 + a2X2 + a3X3 > w]
Outcome Design 1 : Y (X) = 1[b1X1 + b2X2 + b3X3 + (cid:15) > y]
Outcome Design 2 : Y (X) = 1[b1X 2
1 + b2X2 + b3X3 + (cid:15) > y]

Xi ∼ N (0, 1), ai ∼ U(0, 1), bi ∼ U(0, 1), (cid:15) ∼ N (0, σ)
w ∼ U (−1, 1), y ∼ U (−1, 1)

We use σ to control the outcome model noise and thus the error rate of the
estimation. For each design we sample 1000 data sets and estimate the mean
outcome using all the estimators described in section 3.1.
The results are summarized in Table 1, Table 2 and Table 3. All results in this paper
are presented in percentage format (actual value times 100) for reader convenience.
The method of Mean predicted Outcome gave best results in our simulation studies
in terms or Root mean square error and Mean Absolute error. We’ve noticed that an
increase in the number of samples from 10K to 100K resulted in an improvement for
the misspeciﬁed model but in quantities much lower than
matching was the second runner up with low errors suggesting the beneﬁts of
matching over more than one unit. Methods not taking into account the outcome
such as Inverse probability weighting and propensity score matching gave very bad
results which makes them an unreasonable choice for the problem settings.
[6] found that the y-model and -model based estimators are sensitive to
misspeciﬁcation models as not observing the direct confounders and rather observing
some non-linear function of them. We have observed the same results for the
methods tested. We’ve also noticed that introducing non-linearity by itself is enough
to degrade the mean population estimation even if the covariates are observed.

n. The 10-NN outcome

√

4.2 Conﬁdence Interval

We’ve evaluated the conﬁdence interval validity produced by bootstrapping. We ran
the simulation again with 100 samples of design 1. Each sample was of size 10K.
Standard errors were estimated using 100 bootstrapped samples for each iteration.
We have calculated the distribution ratio between the error and the estimated

4

Figure 1: Mean Estimation Vs True Mean for 6 diﬀerent methods for Design 1
with 10K samples

Figure 2: Mean Estimation Vs True Mean for 6 diﬀerent methods for Design 2
with 10K samples

Figure 3: Mean Estimation Vs True Mean for 6 diﬀerent methods for Design 2
with 100K samples

5

Table 1: Performance of the estimators with 10,000 samples over 1000 iterations
for design 1 - correctly speciﬁed y-model and π-model

Method
Mean Predicted Outcome
10-NN Outcome Matching
IPW Mean Predicted Outcome
1-NN Outcome Matching
IPW
1-NN Propensity Score Matching

BIAS RMSE MAE
1.726
3.017
0.003
2.059
4.09
0.077
2.268
3.879
0.111
2.559
6.267
0.23
10.714
13.512
0.353
44.327
48.309
2.088

Table 2: Performance of the estimators with 10,000 samples over 1000 iterations
for design 2 - misspeciﬁed y-model and correctly speciﬁed π-model

Method
Mean Predicted Outcome
10-NN Outcome Matching
1-NN Outcome Matching
IPW Mean Predicted Outcome
IPW
1-NN Propensity Score Matching

BIAS RMSE MAE
3.626
5.473
-0.06
3.837
6.025
-0.204
4.21
7.128
0.06
4.321
6.567
0.163
11.218
14.462
0.192
45.304
48.969
0.646

Table 3: Performance of the estimators with 100,000 samples over 1000 itera-
tions for design 2 - misspeciﬁed y-model and correctly speciﬁed π-model

Method
Mean Predicted Outcome
10-NN Outcome Matching
1-NN Outcome Matching
IPW Mean Predicted Outcome
IPW
1-NN Propensity Score Matching

BIAS RMSE MAE
2.903
4.949
0.199
2.95
5.192
0.359
3.109
5.826
0.202
3.21
5.477
0.206
10.612
13.699
0.242
46.488
49.951
-1.961

6

Table 4: Design 1 Coverage Rate of diﬀerent estimation methods using boot-
strapping
Method
Mean Predicted Outcome
1-NN Outcome Score Matching
10-NN Outcome Score Matching
IPW Mean Predicted Outcome
1-NN Propensity Score Matching
IPW

Coverage Rate
94
96
92
91
91
24

Table 5: Design 2 Coverage Rate of diﬀerent estimation methods using boot-
strapping
Method
1-NN Propensity Score Matching
1-NN Outcome Score Matching
10-NN Outcome Score Matching
IPW Mean Predicted Outcome
Mean Predicted Outcome
IPW

Coverage Rate
94
76
76
75
72
23

standard error and also the 95% coverage rate presented in Tables 4, 5. For the
correctly speciﬁed model all conﬁdence intervals were estimated relatively good
besides for the IPW conﬁdence interval which underestimated the error with only
about 24% coverage. For the design 2 with misspeciﬁed outcome model our
estimation of the conﬁdence interval degraded with about 75% coverage. The only
good estimation of the coverage rate was produced by the propensity score matching
estimator.

5 EMPIRICAL RESULTS

5.1 Treated Group Mean Estimation

To further investigate the estimator we’ve conducted an analysis over the public
dataset “Credit Card Fraud Detection” from the Kaggle website. The dataset
contains transactions made by credit cards in September 2013 by European
cardholders. This dataset presents transactions that occurred in two days, with 492
frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive
class (frauds) account for 0.172% of all transactions. It contains only numeric input
variables which are the result of a PCA transformation. Due to conﬁdentiality issues,
the original features were not provided. Features V1, V2, . . . V28 are the principal
components obtained with PCA, the only features which have not been transformed
with PCA are ’Time’ and ’Amount’. Feature ’Time’ contains the seconds elapsed
between each transaction and the ﬁrst transaction in the dataset. The feature
’Amount’ is the transaction Amount, Feature ’Class’ is the response variable and it
takes value 1 in case of fraud and 0 otherwise.

7

Figure 4: Design 1 Histograms of ratio between estimation error and the stan-
dard error

Figure 5: Design 2 Histograms of ratio between estimation error and the stan-
dard error

8

Table 6: Performance of the estimators over 1000 iterations with Kaggle fraud
credit card dataset and treated group size of 100

Method
10-NN Outcome Matching
1-NN Outcome Matching
Mean Predicted Outcome
IPW Mean Predicted Outcome
IPW
1-NN Propensity Score Matching

BIAS RMSE MAE
3.26
3.82
-1.77
4.01
4.77
-1.90
5.10
4.31
-4.56
7.63
7.25
-5.81
8.83
6.97
7.37
9.81
8.02
8.79

Figure 6: Mean Estimation Vs True Mean for 6 diﬀerent methods for Real Fraud
Dataset from Kaggle website

We performed feature selection using the Anova test and selected the top 10 features.
To simulate a policy and treated group we sample at each iteration 4 features and
create a mock policy using a Gauss Naive Bayes model. The data is split 50:50 to a
train set and test set. The model is trained on the train set. The treated group size
is ﬁxed to 100 units. The treated group is assigned to the units with top model
scores in the test set. Following that we run all the estimators and evaluate the
results which are summarized in Table 6.
For the empirical results 10-NN score matching outperformed all other methods in
terms of RMSE and MAE. IPW remains a bad choice for our domain but
surprisingly 1-NN propensity score matching performance increased compared to our
initial simulation studies.
The empirical studies along with the simulation studies give strong evidence that
these methods can be used in practice to get estimates of fraud prevalence and catch
fraud trends.

5.2 Conﬁdence Interval

We evaluate the performance of bootstrapping in estimating the conﬁdence interval
of our estimators. We sample the Kaggle dataset 100 times for each estimation
method. Each sample contains 900 non-fraud units and 100 fraud units. We train a
Gaussian Naive Bayse model on 10% of the data with 4 random features. We then

9

Table 7: Kaggle Fraud Dataset Coverage Rate of diﬀerent estimation methods
using bootstrapping

Method
IPW
1-NN Propensity Score Matching
1-NN Outcome Score Matching
10-NN Outcome Score Matching
IPW Mean Predicted Outcome
Mean Predicted Outcome

Coverage Rate
95
98
100
100
100
100

Figure 7: Fraud Dataset Histograms of ratio between estimation error and the
standard error

predict the model score over the entire set. We add random uniform noise U(0, 1) to
the Bayse Model score to get a higher variance in the positive ratio within the
treated group. We select a random treated group size n with distribution U(50, 80)
and then assign the top n scores to the treated group. The results are summarized in
Table 7 and Figure 7.
It seems that bootstrapping for the use case of Kaggle fraud dataset with Naive
Bayse policy overestimated the conﬁdence interval and yielded conservative
estimation.

6 PRACTICAL CONSIDERATIONS

1. Null Ratio - special consideration should be given to the null ratio both feature
level and item level. Since in production many times feature breakage comes in
bursts and items with high null rate (> 10%) will get the wrong match. We
recommend using very reliable features and monitoring for breakage.

2. Feature Selection - Features should be selected such that they explain both the

outcome and the treatment. This can be veriﬁed using common unitary
feature selection methods such as mutual information. Model should include as
many covariates as possible to ensure correctness of the outcome model.

10

7 DISCUSSION

In this work we’ve presented a method for estimating unobserved performance
metrics with focus on Financial Services. We’ve shown how matching on the
outcome score can predict metrics such as False Positive Rate with acceptable errors
and serve as an additional tool. Moreover it was found to outperform other methods
in a real fraud dataset.
We believe these methods, most of them known for many years in the statistics
community, can be extremely useful in many Integrity domains for which control
groups are not available and manual labeling eﬀort reduction is desirable.

Acknowledgement

The author would like to thank Shaked Bar for valuable comments.

11

References

[1] Alberto Abadie and Guido W Imbens. Large sample properties of matching
estimators for average treatment eﬀects. econometrica, 74(1):235–267, 2006.

[2] Peter C Austin and Dylan S Small. The use of bootstrapping when using

propensity-score matching without replacement: a simulation study. Statistics
in medicine, 33(24):4306–4319, 2014.

[3] Jianqing Fan and Qiwei Yao. Eﬃcient estimation of conditional variance
functions in stochastic regression. Biometrika, 85(3):645–660, 1998.

[4] Michele Jonsson Funk, Daniel Westreich, Chris Wiesen, Til St¨urmer, M Alan

Brookhart, and Marie Davidian. Doubly robust estimation of causal eﬀects.
American journal of epidemiology, 173(7):761–767, 2011.

[5] Keisuke Hirano and Guido W Imbens. Estimation of causal eﬀects using

propensity score weighting: An application to data on right heart
catheterization. Health Services and Outcomes research methodology,
2(3):259–278, 2001.

[6] Joseph DY Kang and Joseph L Schafer. Demystifying double robustness: A
comparison of alternative strategies for estimating a population mean from
incomplete data. Statistical science, 22(4):523–539, 2007.

[7] Ronnie Pingel. Estimating the variance of a propensity score matching estimator

for the average treatment eﬀect. Observational Studies, 4(1):71–96, 2018.

[8] James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of

regression coeﬃcients when some regressors are not always observed. Journal of
the American statistical Association, 89(427):846–866, 1994.

[9] Paul R Rosenbaum and Donald B Rubin. The central role of the propensity

score in observational studies for causal eﬀects. Biometrika, 70(1):41–55, 1983.

[10] Elizabeth A Stuart. Matching methods for causal inference: A review and a

look forward. Statistical science: a review journal of the Institute of
Mathematical Statistics, 25(1):1, 2010.

[11] Machine Learning Group ULB. Credit card fraud detection [dataset].

https://www.kaggle.com/mlg-ulb/creditcardfraud, 2018. Data was retrieved
from Kaggle.

[12] Stefan Wager and Susan Athey. Estimation and inference of heterogeneous
treatment eﬀects using random forests. Journal of the American Statistical
Association, 113(523):1228–1242, 2018.

[13] Yuxiang Xie, Meng Xu, Evan Chow, and Xiaolin Shi. How to measure your app:

A couple of pitfalls and remedies in measuring app performance in online
controlled experiments. In Proceedings of the 14th ACM International
Conference on Web Search and Data Mining, pages 949–957, 2021.

12

