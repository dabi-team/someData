SSIDS: Semi-Supervised Intrusion Detection System
by Extending the Logical Analysis of Data

Tanmoy Kanti Das, Sugata Gangopadhyay, Jianying Zhou

1

0
2
0
2

l
u
J

1
2

]

R
C
.
s
c
[

1
v
8
0
6
0
1
.
7
0
0
2
:
v
i
X
r
a

Abstract—Prevention of cyber attacks on the critical
network resources has become an important issue as
the traditional Intrusion Detection Systems (IDSs) are
no longer eﬀective due to the high volume of network
traﬃc and the deceptive patterns of network usage
employed by the attackers. Lack of suﬃcient amount of
labeled observations for the training of IDSs makes the
semi-supervised IDSs a preferred choice. We propose
a semi-supervised IDS by extending a data analysis
technique known as Logical Analysis of Data, or LAD
in short, which was proposed as a supervised learning
approach. LAD uses partially deﬁned Boolean functions
(pdBf) and their extensions to ﬁnd the positive and
the negative patterns from the past observations for
classiﬁcation of future observations. We extend the
LAD to make it semi-supervised to design an IDS.
The proposed SSIDS consists of two phases: oﬄine and
online. The oﬄine phase builds the classiﬁer by iden-
tifying the behavior patterns of normal and abnormal
network usage. Later, these patterns are transformed
into rules for classiﬁcation and the rules are used during
the online phase for the detection of abnormal network
behaviors. The performance of the proposed SSIDS is
far better than the existing semi-supervised IDSs and
comparable with the supervised IDSs as evident from
the experimental results.

Keywords:Intrusion Detection System, Logical Analy-

sis of Data, Semi-Supervised Classiﬁcation.

I. Introduction

The Internet has evolved into a platform to deliver
services from a platform to disseminate information. Con-
sequently, misuse and policy violations by the attackers
are routine aﬀairs nowadays. Denning [11] introduced the
concept of detecting the cyber threats by constant mon-
itoring of network audit trails using Intrusion Detection
System (IDS) to discover abnormal patterns or signatures
of network or system usage. Recent advancements in IDS
are related to the use of machine learning and soft com-
puting techniques that have reduced the high false positive
rates which were observed in the earlier generations of
IDSs [15] [19]. The statistical models in the data mining
techniques provide excellent intrusion detection capability
to the designers of the existing IDSs which have increased
their popularity. However, the inherent complications of

T. K. Das is with Department of Computer Applications, National
Institute of Technology Raipur, INDIA. Email:tkdas.mca@nitrr.ac.in
S. Gangopadhyay is with Department of CSE, Indian Institute of

Technology Roorkee, INDIA 247667, Email:sugatfma@iitr.ac.in

J. Zhou is with iTrust, Center for Research in Cyber Security,
Singapore University of Technology and Design, Singapore. Email:
jianying zhou@sutd.edu.sg

IDSs such as competence, accuracy, and usability pa-
rameters make them unsuitable for deployment in a live
system having high traﬃc volume. Further, the learning
process of IDSs requires a large amount of training data
which may not be always available, and it also requires a
lot of computing power and time. Studies have revealed
that it is diﬃcult to handle high-speed network traﬃc by
the existing IDSs due to their complex decision-making
process. Attackers can take advantage of this shortcoming
to hide their exploits and can overload an IDS using
extraneous information while they are executing an attack.
Therefore, building an eﬃcient intrusion detection is vital
for the security of the network system to prevent an attack
in the shortest possible time.

A traditional IDS may discover network threats by
matching current network behavior patterns with that of
known attacks. The underlying assumption is that the
behavior pattern in each attack is inherently diﬀerent
compared to the normal activity. Thus, only with the
knowledge of normal behavior patterns, it may be possible
to detect a new attack. However, the automatic generation
of these patterns (or rules) is a challenging task, and most
of the existing techniques require human intervention dur-
ing pattern generation. Moreover, the lack of exhaustive
prior knowledge (or labeled data) regarding the attacks
makes this problem more challenging. It is advantageous
for any IDS to consider unlabeled examples along with the
available (may be small in number) labeled examples of the
target class. This strategy helps in improving the accuracy
of the IDSs against the new attacks. An IDS which can
use both labeled and unlabeled examples is known as a
semi-supervised IDS. Another important aspect of any
intrusion detection system is the time required to detect
abnormal activity. Detection in real time or near real time
is preferred as it can prevent substantial damage to the
resources. Thus, the primary objective of this work is to
develop a semi-supervised intrusion detection system for
near real-time detection of cyber threats.

Numerous security breaches of computer networks have
encouraged researchers and practitioners to design several
Intrusion Detection Systems. For a comprehensive review,
we refer to [16]. Researchers have adopted various ap-
proaches to design IDSs, and a majority of them modeled
the design problem as a classiﬁcation problem. In [3], a
feature selection method is used with a standard classiﬁer
like SVM as the conventional classiﬁers perform poorly
due to the presence of redundant or irrelevant features.
Authors of [20] also adopted a similar approach. Most
i.e.,
of these designs share one common disadvantage,

 
 
 
 
 
 
they follow a supervised learning approach. Recently, a
new semi-supervised IDS has been proposed in [4], and
it outperforms the existing semi-supervised IDSs, but it
suﬀers from the low accuracy of detection.

It is essential to understand the behavior patterns of
the known attacks, as well as the behaviors of normal
activity to discover and prevent the attacks. Generation
of patterns or signatures to model the normal as well as
the abnormal activities is a tedious process, and it can
be automatized using the application of LAD. Peter L.
Hammer introduced the concept of logical analysis of data
(or LAD) in the year 1986 [12] and subsequently developed
it as a technique to ﬁnd the useful rules and patterns
from the past observations to classify new observations [6],
[9]. Patterns (or rules) can provide a very eﬃcient way to
solve various problems in diﬀerent application areas, e.g.,
classiﬁcation, development of rule-based decision support
system, feature selection, medical diagnosis, network traf-
ﬁc analysis, etc. The initial versions of LAD [1], [9], [12]
were designed to work with the binary data having either
of the two labels, i.e., positive or negative. Thus, the data
or observations were part of a two-class system. A speciﬁc
goal of LAD is to learn the logical patterns which set apart
observations of a class from the rest of the classes.

LAD has been used to analyze problems involving med-
ical data. A typical dataset consists of two disjoint sets
Ω+, Ω− which represent a set of observations consisting of
positive and negative examples, respectively. Here, each
observation is a vector consisting of diﬀerent attribute
values. In the domain of medical data analysis, each vector
represents the medical record of a patient, and the patients
in Ω+ have a speciﬁc medical condition. On the other
hand, Ω− represents the medical records of the patients
who do not have that condition. Subsequently, if a new
vector / patient is given, one has to decide whether the
new vector belongs to Ω+ or Ω−, i.e., one has to determine
whether the patient has the particular medical condition
or not. Thus,
in this example, the medical diagnosis
problem can be interpreted as a two-class classiﬁcation
problem. The central theme of LAD is the selection of such
patterns (or rules) which can collectively classify all the
known observations. LAD stands out in comparison with
other classiﬁcation methods since a pattern can explain
the classiﬁcation outcome to human experts using formal
reasoning.

Conventional LAD requires labeled examples for the
pattern or rule generation. However, there exist several ap-
plication domains (e.g., intrusion detection system, fraud
detection, document clustering, etc.) where the existence
of labeled examples are rare or insuﬃcient. To harness the
strength of LAD in these application domains, one needs
to extend LAD for unsupervised and semi-supervised pat-
tern generation [7]. Here, we introduce a preprocessing
methodology using which we can extend the LAD in such
a manner that it can use unlabeled observations along
with the labeled observations for pattern generation. Con-
sequently, it acts like a semi-supervised learning approach.
The central theme is to use the classical LAD to generate

2

initial positive and negative patterns from the available
labeled observations. Once the patterns are available, we
measure the closeness of the unlabeled observations with
the initial positive or negative patterns using balance
score. The observations with high positive balance score
are labeled as the positive observations and the obser-
vations having high negative balance score are labeled
as the negative examples. Once labels are generated, the
standard LAD can be used as it is. We have used this ap-
proach successfully in the design of a new semi-supervised
and lightweight Intrusion Detection System (IDS) which
outperforms the existing methods in terms of accuracy and
requirement of computational power.

Creation of signatures or patterns to model the nor-
mal as well as the abnormal network activities can be
accomplished using the semi-supervised LAD (or S-LAD
in short), and in this eﬀort, we have used S-LAD to design
a semi-supervised IDS. Here, S-LAD is used to generate
the patterns which can diﬀerentiate the normal activities
from the malicious activities, and these patterns are later
converted to rules for the classiﬁcation of unknown net-
work behavior(s). The proposed SSIDS has two phases,
the oﬄine phase is used to design a rule-based classiﬁer.
This phase uses historical observations, both labeled and
unlabeled, to ﬁnd the patterns or rules of classiﬁcation,
and require a signiﬁcant amount of processing power. Once
the classiﬁcation rules are generated, the online phase
uses those rules to classify any new observation. The
online phase requires much less processing power than the
oﬄine phase, and it can detect threats in near real-time.
The accuracy of proposed semi-supervised IDS is much
better than any state-of-the-art semi-supervised IDS and
comparable with the supervised IDSs.

The main contributions of the proposed paper are: (1) a
new implementation of LAD having extensively modiﬁed
pattern generation algorithm; (2) a new strategy to extend
LAD that is suitable for the design of semi-supervised
classiﬁers; (3) a LAD-based design of a lightweight semi-
supervised intrusion detection system that outperforms
any existing semi-supervised IDSs.

The rest of the paper is organized as follows. Next
section gives a brief description of our modiﬁed imple-
mentation of LAD and Section III describes the proposed
method to extend LAD to the semi-supervised LAD.
Details of the proposed SSIDS is available in the Sec-
tion IV. Performance evaluation and comparative results
are available in the Section V and we conclude the paper
in the Section VI.

II. Proposed Implementation of LAD

LAD is a data analysis technique which is inspired by
the combinatorial optimization methods. As pointed out
earlier, the initial version of LAD was designed to work
with the binary data only. Let us ﬁrst brieﬂy describe
the basic steps of LAD when it is applied to the binary
data. An observation having n attributes may be repre-
sented as a binary vector of length n + 1 as the last bit
(a.k.a. the class label) indicates whether it is a member

of Ω+ or Ω−. Thus, the set of binary observations Ω
(= Ω+ ∪ Ω− ⊆ {0, 1}n) can be represented by a partially
deﬁned Boolean function (pdBf in short) φ, indicating
a mapping of Ω → {0, 1}. The goal of LAD is to ﬁnd
an extension f of the pdBf φ which can classify all the
unknown vectors in the sample space. However, this goal
is clearly unachievable and we try to ﬁnd an approximate
extension f 0 of f . f 0 should approximate f as closely as
possible based on the several optimality criteria. Normally,
the extension is represented in a disjunctive normal form
(DNF). In brief, the LAD involves following steps [1].

1) Binarization of Observations. We have used a slightly

modiﬁed implementation of binarization here.

2) Elimination of Redundancy (or Support Sets Generation).
3) Pattern Generation. Our extensively modiﬁed pattern
generation algorithm makes the ’Theory Formation’ step
redundant.

4) Theory Formation. We have omitted this step.
5) Classiﬁer Design and Validation.
There are many application domains from the ﬁnance
to the medical where the naturally occurring data are not
binary [1], [5]. Thus, to apply LAD in those domains,
a method to convert any data to binary is discussed
in the subsection II-A. Moreover, we have modiﬁed the
original pattern generation algorithm in such a manner
that the coverages of every pair of patterns have a very
low intersection. Thus, the step “theory formation” is no
longer required. Recently, a technique to produce inter-
nally orthogonal patterns (i.e., the coverages of every pair
of patterns have empty intersection) is also reported in [8].

A. Binarization of Observations

A threshold (a.k.a. cut-point) based method was pro-
posed to convert the numerical data to binary. Any nu-
merical attribute x is associated with two types of Boolean
variables, i.e. the level variables and the interval variables.
Level variables are related to the cut-points and indicate
whether the original attribute value is greater than or less
than the given cut-point β. For each cut-point β, we create
a Boolean variable b(x, β) such that

b(x, β) =

(cid:26)1,

if x ≥ β.

0, otherwise.

(1)

Similarly, interval variables are created for each pair of
cut-points β1 and β2 and represented by Boolean variable
b(x, β1, β2) such that

b(x, β1, β2) =

(cid:26)1,

if β1 ≤ x < β2.

0, otherwise.

(2)

We are yet to discuss how the cut-points are determined.
The cut-points should be chosen carefully such that the
resultant pdBf should have an extension in the class of all
Boolean functions CALL [5]. Let us consider the numerical
attribute x having k + 1 distinct values present in the
observations and the attribute values are ordered such
that x0 > x1 > . . . > xk. We introduce a cut-point
between xi and xi+1 if they belong to diﬀerent classes.
The resulting pdBf is referred to as the master pdBf if
we create cut-point for each pair of values. Note that, the

3

resultant master pdBf has extension in CALL if and only
if Ω+ ∩ Ω− = ∅.

The process for selection of cut-points is explained below
using an example from [10]. The original dataset presented
in the Table I is converted to the Table II by adding the
class labels (or truth values of pdBf). Those observations
that are the members of Ω+ have 1 as the class label and
rest of the observations have 0 as the class labels. Now,
if we want to convert the numeric attribute A to binary,
we form another dataset as represented in the Table III.
Next, we sort the dataset over the attribute A to get a new
dataset D that is presented in the Table IV. After that,
we apply the following steps to get the cut points.

1) Preprocessing of D: This step is a slight modiﬁcation
of the usual technique used in [5], [6], and other
related papers. If two or more consecutive observa-
tions have the same attribute value vi but diﬀerent
class labels, remove all those observations except one
observation. Now, we change the existing class label
of vi to a new and unique class label which does not
appear in D and include that in the set of class labels
of D. Refer to Table V.

2) Now, if two consecutive observations Ai and Ai+1
have diﬀerent class labels, introduce a new cut-point
βA
j as

βA
j =

(Ai + Ai+1)

1 = 3.05, βA

1
2
If we follow the above mentioned steps, the obtained cut-
points are βA
3 = 1.65. Thus, we
will have six Boolean variables consisting of three level
variables and three interval variables corresponding to
these cut-points. A “nominal” or descriptive attribute x
can be converted into binary very easily by relating each
possible value vi of x with a Boolean variable b(x, vi) such
that

2 = 2.45, βA

(

if x = vi.
1,
0, otherwise.

(3)

b(x, vi) =

B. Support sets generation

S ∩ Ω−

S ∩ Ω−

S and Ω−

Binary dataset obtained through the binarization or any
other process may contain redundant attributes. A set
S of binary attributes is termed as a support set if the
projections Ω+
S of Ω+ and Ω−, respectively, are
such that Ω+
S = ∅. A support set is termed minimal
if elimination any of its constituent attributes leads to
Ω+
S 6= ∅. Finding the minimal support set of a binary
dataset, like Table XI (see Appendix), is equivalent of
solving a set covering problem. A detailed discussion on
the support set, minimal support set and a few algorithms
to solve the set covering problem can be found in [2], [9],
[12]. Here, we have used the “Mutual-Information-Greedy”
algorithm proposed in [2] to solve the set covering problem
in our implementation. Note that, our implementation
produces the set S in a manner such that the constituent
binary attributes are ordered according to their discrim-
inating power and it helps us to achieve the simplicity
objective which is mentioned in the description of LAD.

Attributes

A

B

C

A

B

C

Ω+:positive
examples

Ω−:negative
examples

3.5
2.6
1.0
3.5
2.3

3.8
1.6
2.1
1.6
2.1

2.8
5.2
3.8
3.8
1.0

3.5
2.6
1.0
3.5
2.3

3.8
1.6
2.1
1.6
2.1

2.8
5.2
3.8
3.8
1.0

Class
Labels
1
1
1
0
0

A

3.5
2.6
1.0
3.5
2.3

Class
Labels
1
1
1
0
0

A

3.5
3.5
2.6
2.3
1.0

Class
Labels
1
0
1
0
1

A

3.5
2.6
2.3
1.0

Class
Labels
2
1
0
1

TABLE I

TABLE II

TABLE III

TABLE IV

TABLE V

4

Following binary feature variables are selected if we apply
the said algorithm: S = {b15, b8, b1, b2}.

C. Modiﬁed pattern generation method

Let us ﬁrst recall a few common Boolean terminologies
that we may require to describe the pattern generation
process. A Boolean variable or its negation is known as
literals and conjunction of such literals is called a term.
The number of literals present in a term T is known as
its degree. The characteristic term of a point p ∈ {0, 1}n
is the unique term of degree n, such that T (p) = 1. The
term T is said to cover the point p if T (p) = 1. A term T
is called a positive pattern of a given dataset (Ω+
S ) if

S , Ω−

1) T (p) = 0 for every point p ∈ Ω−
S .
2) T (p) = 1 for at least one point p ∈ Ω+
S .

Similarly, one can deﬁne the negative patterns. Here,
T (ΩS) is deﬁned as T (ΩS) = S
T (p). Both the positive
p∈ΩS

and the negative patterns play a signiﬁcant role in any
LAD based classiﬁer. A positive pattern is deﬁned as a
subcube of the unit cube that intersects Ω+
S but is disjoint
from Ω−
S . A negative pattern is deﬁned as a subcube of
the unit cube that intersects Ω−
S but is disjoint from Ω+
S .
Consequently, we have a symmetric pattern generation
procedure. In this paper, we have used an extensively
modiﬁed and optimized version of the pattern generation
technique that has been proposed by Boros et al. [6].

We have made two major changes in Algorithm 1 for
pattern generation over the algorithm proposed in [6].
Steps 18 and 21 are diﬀerent from the original algorithm
and Step 21 increases the probability that a point or
observation is only covered by a single pattern instead
of multiple patterns. We expect that the majority of the
observations will be covered by a unique pattern. Thus, we
no longer require the ‘theory formation’ step to select the
most suitable pattern to cover an observation. In Step 18,
we have ensured that a pattern is selected if and only if it
covers at least k many positive observations. This ensures
that a selected pattern occurs frequently in the dataset.
One major drawback of this approach is that if k > 1, then
it may so happen that all the observations present in the
dataset may not be covered by the selected set of patterns.
However, a properly chosen value of k ensures that more
than 95% of the observations are covered. Note that, the
negative prime patterns can also be generated in a similar
fashion. If we apply the algorithm 1 over the projection
S = {b15, b8, b1, b2} of the binary dataset presented in the
Table XI (see Appendix), following positive patterns are

Ω+

Algorithm 1 Positive prime pattern enumeration algorithm.
S ⊂ {0, 1}n - Sets of positive and

S , Ω−
Input:
negative observations in binary.
ˆd - Maximum degree of generated patterns.
k - Minimum number of observations covered by a
generated pattern.
Output: χ

- Set of prime patterns.

1: χ = ∅.
2: G0 = {∅}.
3: for d = 1, . . . , ˆd do
if d < ˆd then
4:
Gd = ∅.
5:
6:
7:
8:
9:

end if
for τ ∈ Gd−1 do

- G ˆd is not required.

p = maximum index of the literal in τ .
for s = p + 1, . . . , n do
for lnew ∈ {ls, ¯ls} do

τ 0 = τ klnew.
for i = 1 to d − 1 do

τ 00 = remove ith literal from τ 0.
if τ 00 /∈ Gd−1 then
go to Step 26.

end if
end for
if k ≤ P
k many positive observations.
S ) then

if 1 /∈ τ 0(Ω−

y∈Ω+
S

negative observation.

τ 0(y) then - τ 0 covers at least

- τ 0 covers no

χ = χ ∪ {τ 0}.

Remove the points (or observations)

covered by τ 0 from Ω+

S .

else if d < ˆd then
Gd = Gd ∪ {τ 0}.

end if

10:
11:
12:
13:
14:

15:
16:
17:
18:

19:

20:
21:

22:
23:
24:
25:

end if
end for

end for

26:
27:
28:
29: end for

end for

generated: (i) b2b8, (ii) b2
corresponding negative patterns are (i) ¯b2

¯b1, (iii) ¯b2b15 using k = 1 and the

¯b15, (ii) b2b15.

D. Design of Classiﬁer

The patterns which are generated using Algorithm 1,
are transformed into rules and later these rules are used
to build a classiﬁer. The rule generation process is trivial
and it’s explained using an example. Let us take the

ﬁrst positive pattern b2b8. The meaning of b2 is whether
(A ≥ 2.45) is true or false as evident from the Table XI.
Similarly, the meaning of ¯b2 is whether ¬(A ≥ 2.45) is
true or false. Consequently, the rule generated from the
pattern b2b8 is (A ≥ 2.45) ∧ (B ≥ 1.85) =⇒ L = 1. The
corresponding pseudo-code is as follows.
if (A ≥ 2.45) ∧ (B ≥ 1.85) then

Class label L = 1

end if
We can combine more than one positive rule into an
‘if else-if else’ structure to design a classiﬁer. Similarly,
one can build a classiﬁer using the negative patterns
also. Hybrid classiﬁers can use both the positive and the
negative rules to design a classiﬁer. A simple classiﬁer
using the positive patterns is presented below.

Simple Classiﬁer.

Input: Observation consisting of attribute A, B, C.
Output: Class label L.

Class label L = 1.

Class label L = 1.

1: if (A ≥ 2.45) ∧ (B ≥ 1.85) then
2:
3: else if (A ≥ 2.45) ∧ ¬(A ≥ 3.05) then
4:
5: else if ¬(A ≥ 2.45) ∧ (3.3 ≤ C < 4.5) then
6:
7: else
8:
9: end if

Class label L = 1.

Class label L = 0.

In general, a new observation x is classiﬁed as positive
if at least a positive pattern covers it and no negative
pattern covers it. Similar deﬁnition is possible for negative
observations. However, in the ‘Simple Classiﬁer’, we have
relaxed this criterion and we consider x as negative if it
is not covered by any positive patterns. Another classiﬁ-
cation strategy that has worked well in our experiment is
based on balance score[1]. The balance score is the linear
combination of positive (Pi) and negative (Ni) patterns
and deﬁned as :

∆(x) =

1
q

q
X

l=1

Pl(x) −

1
r

r
X

i=1

Ni(x)

(4)

The classiﬁcation η(x) of the new observations x is given
by

η(x) =






1,
0,
(cid:15),

if ∆(x) > 0.
if ∆(x) < 0.
if ∆(x) = 0. Here, (cid:15) indicates unclassiﬁed.

(5)

III. Extension of LAD

Majority of the applications of the LAD which are avail-
able in the existing literature [1], work with the labeled
data during the classiﬁer design phase. There are many
applications where a plethora of data are available which
are unlabeled or partially labeled. These applications re-
quire semi-supervised or unsupervised pattern generation
approach. One such application is intrusion detection sys-
tem where the lightweight classiﬁcation methods designed

5

using the LAD are desirable. However, the dearth of
labeled observations makes it diﬃcult for the development
of a LAD based solution. In this eﬀort, we propose a pre-
processing method which can label the available unlabeled
data. However, the proposed method requires that some
labeled data are available during the design of classiﬁers.
Thus, the method is akin to a semi-supervised learning
approach [?], [22].

The process of class label generation is very simple and
it uses a standard LAD based classiﬁer [6] having balance
score [1] as a discriminant function to classify an unla-
beled observation. First, we design a balance score based
classiﬁer using the set of available labeled observations
DL. Later, we classify each observation in the unlabeled
dataset using the balance score based classiﬁer. However,
we replace the classiﬁer described in the Equation 5 by
the Equation 6. Thus, we keep those observations unla-
beled which are having very low balance score and those
observations are also omitted form farther processing.
Basically, we are ensuring that if a given observation has
a strong aﬃnity towards the positive or negative patterns,
then only the observation is classiﬁed/labeled during the
labeling process.

η0(x) =






1,
0,
(cid:15),

if ∆(x) > τ1.
if ∆(x) < τ0.
if τ0 ≤ ∆(x) ≤ τ1.

(6)

We have evaluated the performance of the said strategy
using the KDDTrain 20 percent dataset which is part of
the NSL-KDD dataset. The KDDTrain 20percent dataset
consists of 25, 192 observations and we have partitioned
the dataset into two parts. The ﬁrst part DL consists
of 5000 randomly selected observations, and the second
part DUL consists of the rest of the observations. We
have removed the labels from the observations of DUL.
Afterward, DL is used to design a classiﬁer based on the
Equation 6. This classiﬁer latter used for the classiﬁcation
of DUL and the output of the labeling process is a dataset
0 which consists of all the labeled examples from the
DL
DUL. The results are summarized in the Table VI. It is
obvious that any error in the labeling process will have
a cascading eﬀect on the performance of Algorithm 2.
On the other hand, the unlabeled samples (marked as
(cid:15)) would have no such consequence on the performance
of the proposed SSIDS. Thus, while reporting the ac-
curacy of the labeling process, we have considered the
labeled samples only. It is clear from the Table VI that
the number of observations that are currently labeled is
17601 + 5000 = 22601 and these many observations would
be used for farther processing. One important aspect that
remains to be discussed is the values of τ0, and τ1. We have
used τ0 = −0.021 and τ1 = 0.24 in our experiments. We
have arrived at these values after analyzing the outcome of
the labeling process on the training dataset DL. Following
the introduction of this pre-processing step, the steps of a
semi-supervised LAD (or S-LAD) are as follows.

1) Class label (or truth value) generation.
2) Binarization.

#DUL

20192

Labeled
# Correctly #Wrongly Accuracy
98.48%

17333

268

TABLE VI
Results related to the labeling of DUL.

#Unlabeled((cid:15))

Algorithm 2 Steps of Oﬄine Phase of IDS

6

2591

Input: Historical dataset consisting of labeled and unla-
beled data.
Output: Rule based classiﬁer for the online phase.

1: Read the historical dataset DL and DUL.
2: Using DL, build a standard LAD Classiﬁer based on the

balance score (i.e., Equation 6).

3) Elimination of redundancy (or Support sets generation).
4) Pattern generation.
5) Classiﬁer design and validation.

IV. Design of a Semi-Supervised IDS using S-LAD

Organizations and governments are increasingly using
the Internet to deliver services, and the attackers are
trying to gain unfair advantages from it by misusing the
network resources. Denning [11] introduced the concept of
detecting the cyber threats by constant monitoring of the
network audit trails using the intrusion detection systems.
The intrusion can be deﬁned as the set of actions that seek
to undermine the availability, integrity or conﬁdentiality of
a network resource [11], [13], [21]. Traditional IDSs that
are used to minimize such risks can be categorized into
two: (i) anomaly based, (ii) misuse based (a.k.a. signature
based). The anomaly based IDSs build a model of normal
activity, and any deviation from the model is considered as
an intrusion. On the contrary, misuse based models gen-
erate signatures from the past attacks to analyze existing
network activity. It was observed that the misuse based
models are vulnerable to “zero day” attacks [17]. Our
proposed technique is unique in the sense that it can be
used as either a misuse based or an anomaly based model.
Hybridization is also possible in our proposed technique.

A. Proposed Intrusion Detection System

The proposed SSIDS is presented in the Figure 1. It
consists of two major phases, i.e., the oﬄine phase and the
online phase. The oﬄine phase uses an S-LAD to design
a classiﬁer which online phase uses for real-time detection
of any abnormal activity using the data that describe the
network traﬃc. It is obvious that the oﬄine phase should
run at least once before the online phase is used to detect
any abnormal activity. The oﬄine phase may be set up to
run at a regular interval of time to upgrade the classiﬁer
with the new patterns or rules. Let us now summarize the
steps of the oﬄine phase in Algorithm 2. Note that, the
Step 2 of Algorithm 2 implicitly uses the Steps 4 to 6
to build the classiﬁer. The online phase is very simple as
it uses the classiﬁer generated in the oﬄine phase for the
classiﬁcation of new observations. One can use the positive
rules only to build a classiﬁer in Step 7 of Algorithm 2,
then the IDS can be termed as anomaly-based. On the
other hand, if it uses only the negative rules, the design is
similar to a signature-based IDS.

V. Performance Evaluations

Most widely used datasets for validation of IDSs are
NSL-KDD [18] and KDDCUP’99 [14]. NSL-KDD is a

3: Using the classiﬁer from the previous step, label the dataset

DUL to generate DL

0 and Ω =DL ∪ DL

0.

4: Binarize Ω using the process described in Subsection II-A.
5: Generate support set S from the binary dataset.
6: Generate positive and negative patterns (i.e., rules) using

Algorithm 1.

7: Design a classiﬁer from the generated patterns following
the example of ’Simple Classiﬁer’ from Subsection II-D.

Fig. 1. Block Diagram of the proposed SSIDS

modiﬁed version of the KDDCUP’99 dataset and we have
used the NSL-KDD dataset in all our experiments. Both
the datasets consist of 41 features along with a class label
for each observation. These features are categorized into
four diﬀerent classes and they are (i) basic features, (ii)
content features, (iii) time-based traﬃc features, (iv) host-
based traﬃc features. Here, the basic features are extracted
from the TCP/IP connections without scanning the pack-
ets and there are nine such features in the NSL-KDD
dataset. On the other hand, features which are extracted
after inspecting the payloads of a TCP/IP connection
are known as the content features and there are 13 such
features present in the dataset. A detailed description
of the features is available in the Table VII. There are
diﬀerent types of attacks present in the dataset but we
have clubbed them to one and consider them as “attack”
only. Thus, there are two types of class labels that we have
considered in our experiments and they are “normal” and
“attack”. We have used the KDDTrain 20percent dataset
which is a part of the NSL-KDD dataset to build the
classiﬁer in the oﬄine phase. The KDDTest+ and the
KDDTest-21 have been used during the online phase for
validation testing. The details of the experimental setup
are presented in Subsection V-A.

A. Experimental setup

The next step after the labels are generated is bina-
rization. Detailed attention is needed to track the number
of binary variables produced during this process. In the
case of numeric or continuous features, the number of
binary variables generated is directly dependent on the
number of cut-points. Thus, if a feature is producing a
large number of cut-points, it will increase the number
if the
of binary variables exponentially. For example,

On Line PhaseOff line PhaseHistoricalNetworkDataLabelingBinarizeSupportSetGenerationPattern GenerationRule basedClassifier DesignClassifierOutput Classe
r
u
t
a
e
F

e
p
y
T

c
i
s
a
B

s
t
n
e
t
n
o
C

o
N

type

Input Feature
duration
protocol
service
ﬂag
src bytes
dst bytes
land

e
p
y
T
a
t
a
D
C
S
S
S
C
C
S
C
C
urgent
C
hot
C
num failed logins
S
logged in
C
num compromised
C
root shell
C
su attempted
C
num root
C
num ﬁle creations
C
num shells
num access ﬁles
C
num outbound cmds C
S
is hot
S
is guest

.
l
o
C
1
2
3
4
5
6
7
8 wrong fragment
9
10
11
12
13
14
15
16
17
18
19
20
21
22

login

login

e
r
u
t
a
e
F

e
p
y
T

)
d
e
s
a
B
e
m
T
(

i

c
ﬃ
a
r
T

)
d
e
s
a
B
t
s
o
H

(

c
ﬃ
a
r
T

o
N

Input Feature

e
p
y
T
a
t
a
D
C
C
srv count
C
serror rate
C
srv error rate
C
rerror rate
C
srv rerror rate
C
same srv rate
C
diﬀ srv rate
C
srv diﬀ host rate
C
dst host count
C
dst host srv count
C
dst host same srv rate
dst host diﬀ srv rate
C
dst host same src port rate C
C
dst host srv diﬀ host rate
C
dst host serror rate
C
dst host srv serror rate
C
dst host rerror rate
C
dst host srv rerror rate

.
l
o
C
23 Count
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41

C means Continuous
S means Symbolic

7

positives during testing. An empirical analysis helps us to
ﬁx the threshold at k = 100. At this threshold, more than
95% of the observations present in the training dataset are
covered by the generated patterns having degree up to 4.

Classiﬁer 1 Details of SSIDS

Input: Observation obs having 41 features.

1:
2: Output: Class label L.
3:
4:
- L = 1 indicates normal behavior.
5: else if obs(5) ≥ 28.50 ∧ ¬(obs(37) ≥ 0.005 ∧ obs(37) < 0.915) ∧

if ¬(obs(36) ≥ 0.0050) ∧ (obs(37) ≥ 0.0050 ∧ obs(37) < 0.1150) then

L = 1

strcmp(obs(r, 3),0 f tp data0) then

L = 1

6:
7: else if obs(5) ≥ 28.5000 ∧ (obs(37) ≥ 0.0050 ∧ obs(37) < 0.9150) ∧ obs(34) ≥

0.1950 then
L = 1

8:
9: else if obs(5) ≥ 28.500000 ∧ ¬(obs(5) ≥ 333.500000) ∧ obs(37) ≥ 0.005000 ∧

obs(37) < 0.085000 then

10:
L = 1
11: else if obs(5)

strcmp(obs(r, 3),0 f tp data0) then

≥

28.500000 ∧ ¬(obs(34)

≥

0.645000) ∧

TABLE VII
Input features of the NSL-KDD dataset.

12:
13: else if obs(6) ≥ 0.500000 ∧ obs(37) ≥ 0.005000 ∧ obs(37) < 0.915000 ∧

L = 1

¬(obs(40) ≥ 0.005000) then

2

number of cut-points is 100, the total number of interval
(cid:1) = 4950 and after considering the level
variables is (cid:0)100
variables, the total number of binary variables created
will be 4950 + 100 = 5050. Consequently, the memory
requirement will increase afterward to an unmanageable
level. On the other hand, a large number of cut-points
indicate that the feature may not have much inﬂuence on
the classiﬁcation of observations. Our strategy is to ignore
such features completely. Another set of features which
are having a fairly large number of cut-points are ignored
partially. Given a feature x, if the number of cut-points
is greater than or equal to 175, we completely ignore the
feature x and if the number of cut-points is greater than
or equal to 75 but less than 175, we ignore that feature
partially by only generating the level variables. We have
arrived at these thresholds after empirical analysis using
the training data. List of features that have been fully or
partially ignored are presented in the Table VIII.

Col. Num.
1
5
23
24
32
33
34
35
36
38
40

Input feature
duration
src bytes
Count
srv count
dist host count
dist host srv count
dst host same srv rate
dst host diﬀ srv rate
dst host same src port
dst host serror
rerror
dst host

rate
rate

rate

#Cut-points
102
116
374
302
254
255
100
93
100
98
100

Ignored ?
Partially
Partially
Fully
Fully
Fully
Fully
Partially
Partially
Partially
Partially
Partially

TABLE VIII
Binarization: Ignored features of the NSL-KDD dataset.

Another important aspect that we have incorporated
into our design is the support of a pattern. Support of
a positive (negative) pattern is k if it covers k positive
(negative) observations and it should not cover any nega-
tive (positive) observation. Thus, the value of k in Step 18
of Algorithm 1 holds immense importance. In a previous
implementation [6] the value of k = 1 have been used, but
it is observed during experiments that such a low support
is generating a lot of patterns/rules having little practical
signiﬁcance. Moreover, these patterns cause a lot of false

L = 1

14:
15: else if obs(6) ≥ 0.500000 ∧ ¬(obs(5) ≥ 333.500000) ∧ obs(5) ≥ 181.500000 then
16:
17: else if obs(5) ≥ 333.500000 ∧ (obs(36) ≥ 0.015000) ∧ obs(6) ≥ 0.500000 ∧

L = 1

obs(6) < 8303.000000 then

18:
19: else if obs(5) ≥ 333.500000∧obs(34) ≥ 0.645000∧obs(6) ≥ 0.500000∧obs(6) <

L = 1

8303.000000 then

20:
21: else if ¬(obs(40) ≥ 0.005000) ∧ obs(34) ≥ 0.645000 ∧ ¬(obs(36) ≥ 0.005000)

L = 1

then

22:
23: else if ¬(obs(40) ≥ 0.005000) ∧ obs(6) ≥ 0.500000 ∧ obs(6) < 8303.000000 ∧

L = 1

¬(obs(34) ≥ 0.045000) then

24:
25: else if obs(5) ≥ 28.50 ∧ obs(6) ≥ 0.500 ∧ obs(36) ≥ 0.015000 ∧ ¬(obs(10) ≥

L = 1

0.500000 ∧ obs(10) < 29.000000) then

26:
27: else if obs(5) ≥ 28.50 ∧ obs(6) ≥ 0.500 ∧ ¬(obs(40) ≥ 0.005000) ∧ ¬(obs(10) ≥

L = 1

0.500000 ∧ obs(10) < 29.000000) then

L = 1

28:
29: else
30:
31: end if

L = 0

- L = 0 indicates attack.

B. Experimental Results

We have described all the steps required to design a
classiﬁer in the oﬄine phase. Let us now summarize the
outcome of the individual steps.
1. Labeling: We have used 5000 labeled observations for
labeling 20192 unlabeled observations as described in Sec-
tion III. This step produces 22601 labeled observations
which have been used in the following steps to design the
classiﬁer.
2. Binarize: During this step, total 10306 binary variables
are produced and a binary dataset along with its class
labels having size 22601 × 10307 is generated.
3. Support Set Generation: We have selected 21 binary
features according to their discriminating power.
4. Pattern Generation: During pattern generation, we
found 13 positive and 7 negative patterns.
5. Classiﬁer Design: We have developed a rule-based IDS
using the 13 positive patterns that are generated in the
last step. Thus, the SSIDS contains 13 rules. The details
of the SSIDS is available in the Classiﬁer 1. The NSL-KDD
dataset contains two test datasets: (i) KDDTest+ having
22, 544 observations, and (ii) KDDTest21 having 11, 850
observations. These two datasets are used to measure the
accuracy of the proposed SSIDS and the results related
to the accuracy of the IDS is presented in Table IX.
These results compare favorably with the state of the
art classiﬁers proposed in [4], and the comparative results
are presented in Table X. It is evident that the proposed
SSIDS outperforms the existing IDSs by a wide margin.

Dataset
KDDTest+
KDDTest21

Accuracy Precision

90.91%
83.92%

0.9458
0.9417

Sensitivity F1-Score Time in sec.
0.9179
0.8971

0.000156
0.000173

0.8915
0.8564

TABLE IX
Results related to KDDTest+ and KDDTest21.

Classiﬁers$

J48*
Naive Bayes*
NB Tree*
Random forests*
Random Tree*
Multi-layer perceptron*
SVM*

Experiment-1 of
Experiment-2 of

[4]
[4]

LAD@
Proposed SSIDS

Accuracy using Dataset(%)
KDDTest21
KDDTest+
63.97
81.05
55.77
76.56
66.16
82.02
63.25
80.67
58.51
81.59
57.34
77.41
42.29
69.52
67.06
82.41
68.82
84.12
79.09
87.42
83.92
90.91

* Results as reported in [4].
@ Classiﬁer designed using dataset DL only by

omitting the ‘labeling’ process.

$ All the classiﬁers use the same training
dataset, i.e., KDDTrain 20percent.

TABLE X
Performance Comparison between different Classifiers,
IDSs and the proposed SSIDS.

VI. Conclusion

The intrusion detection system (IDS) is a critical tool
used to detect cyber attacks, and semi-supervised IDSs
are gaining popularity as it can enrich its knowledge-
base from the unlabeled observations also. Discovering and
understanding the usage patterns from the past observa-
tions play a signiﬁcant role in the detection of network
intrusions by the IDSs. Normally, usage patterns establish
a causal relationship among the observations and their
class labels and the LAD is useful for such problems where
we need to automatically generate useful patterns that
can predict the class labels of future observations. Thus,
LAD is ideally suited to solve the design problems of
IDSs. However, the dearth of labeled observations makes it
diﬃcult to use the LAD in the design of IDSs, particularly
semi-supervised IDSs, as we need to consider the unlabeled
examples along with the labeled examples during the
design of IDSs. In this eﬀort, we have proposed a simple
methodology to extend the classical LAD to consider unla-
beled observations along with the labeled observations. We
have employed the proposed technique successfully to de-
sign a new semi-supervised “Intrusion Detection System”
which outperforms the existing semi-supervised IDSs by
a wide margin both in terms of accuracy and detection
time.

References

[1] G. Alexe, S. Alexe, T. O. Bonates, and A. Kogan, “Logical
analysis of data – the vision of peter l. hammer,” Annals of
Mathematics and Artiﬁcial Intelligence, vol. 49, no. 1, pp. 265–
312, Apr 2007.

[2] H. Almuallim and T. G. Dietterich, “Learning boolean concepts
in the presence of many irrelevant features,” Artiﬁcial Intelli-
gence, vol. 69, pp. 279–305, 1994.

[3] M. A. Ambusaidi, X. He, P. Nanda, and Z. Tan, “Building an
intrusion detection system using a ﬁlter-based feature selection
algorithm,” IEEE Transactions on Computers, vol. 65, no. 10,
pp. 2986–2998, 2016.

8

[4] R. A. R. Ashfaq, X.-Z. Wang, J. Z. Huang, H. Abbas, and Y.-
L. He, “Fuzziness based semi-supervised learning approach for
intrusion detection system,” Information Sciences, vol. 378, pp.
484 – 497, 2017.

[5] E. Boros, P. L. Hammer, T. Ibaraki, and A. Kogan, “Logi-
cal analysis of numerical data,” Mathematical Programming,
vol. 79, no. 1, pp. 163–190, Oct 1997.

[6] E. Boros, P. L. Hammer, T. Ibaraki, A. Kogan, E. Mayoraz, and
I. Muchnik, “An implementation of logical analysis of data,”
IEEE Trans. on Knowl. and Data Eng., vol. 12, no. 2, pp. 292–
306, Mar. 2000.

[7] R. Bruni and G. Bianchi, “Eﬀective classiﬁcation using a
small training set based on discretization and statistical analy-
sis,” IEEE Transactions on Knowledge and Data Engineering,
vol. 27, no. 9, pp. 2349–2361, Sep. 2015.

[8] R. Bruni, G. Bianchi, C. Dolente, and C. Leporelli, “Logical
analysis of data as a tool for the analysis of probabilistic discrete
choice behavior,” Computers & Operations Research, 2018.
[9] Y. Crama, P. Hammer, and T. Ibaraki, “Cause-eﬀect relation-
ships and partially deﬁned boolean functions,” Ann. Oper. Res.,
vol. 16, no. 1-4, pp. 299–325, Jan. 1988.

[10] T. K. Das, S. Ghosh, E. Koley, and J. Zhou, “Design of a
fdia resilient protection scheme for power networks by securing
minimal sensor set,” in Proceedings of 2019 International Work-
shop on Artiﬁcial Intelligence and Industrial Internet-of-Things
Security, LNCS-11605, 2019.

[11] D. E. Denning, “An intrusion-detection model,” IEEE Transac-
tions on Software Engineering, vol. SE-13, no. 2, pp. 222–232,
Feb 1987.

[12] P. Hammer, “Partially deﬁned boolean functions and cause-
eﬀect relationships,” in International Conference on Multi-
attribute Decision Making Via OR-based Expert Systems. Uni-
versity of Passau, Passau, Germany, April 1986.

[13] E. Hern´andez-Pereira, J. A. Su´arez-Romero, O. Fontenla-
Romero, and A. Alonso-Betanzos, “Conversion Methods for
Symbolic Features: A comparison applied to an Intrusion Detec-
tion Problem,” Expert Systems with Applications, vol. 36, no. 7,
pp. 10 612–10 617, 2009.

[14] KDD, “Kdd cup 1999 data,” http://kdd.ics.uci.edu/databases/
[Online; accessed 05-July-

kddcup99/kddcup99.html, 1999,
2018].

[15] G. Kim, S. Lee, and S. Kim, “A novel hybrid intrusion detection
method integrating anomaly detection with misuse detection,”
Expert Systems with Applications, vol. 41, no. 4, Part 2, pp. 1690
– 1700, 2014.

[16] H.-J. Liao, C.-H. R. Lin, Y.-C. Lin, and K.-Y. Tung, “Intrusion
detection system: A comprehensive review,” Journal of Network
and Computer Applications, vol. 36, no. 1, pp. 16 – 24, 2013.

[17] S. Mukkamala, A. H. Sung, and A. Abraham, “Intrusion de-
tection using an ensemble of intelligent paradigms,” J. Netw.
Comput. Appl., vol. 28, no. 2, pp. 167–182, Apr. 2005.

[18] M. Tavallaee, E. Bagheri, W. Lu, and A. A. Ghorbani, “A
detailed analysis of the kdd cup 99 data set,” in Proceedings
of the Second IEEE International Conference on Computa-
tional Intelligence for Security and Defense Applications, ser.
CISDA’09. Piscataway, NJ, USA: IEEE Press, 2009, pp. 53–
58.

[19] C.-F. Tsai, Y.-F. Hsu, C.-Y. Lin, and W.-Y. Lin, “Intrusion
detection by machine learning: A review,” Expert Systems with
Applications, vol. 36, no. 10, pp. 11 994 – 12 000, 2009.

[20] H. Wang, J. Gu, and S. Wang, “An eﬀective intrusion de-
tection framework based on svm with feature augmentation,”
Knowledge-Based Systems, vol. 136, pp. 130 – 139, 2017.
[21] Q. Yan and F. R. Yu, “Distributed denial of service attacks
in software-deﬁned networking with cloud computing,” IEEE
Communications Magazine, vol. 53, no. 4, pp. 52–59, April 2015.
[22] X. Zhu and A. B. Goldberg, “Introduction to semi-supervised
learning,” Synthesis Lectures on Artiﬁcial Intelligence and Ma-
chine Learning, vol. 3, no. 1, pp. 1–130, 2009.

Appendix

9

5
0
.
3
<
A
≤
5
6
.
1

b4
0
1
0
0
1

5
0
.
3
<
A
≤
5
4
.
2

b5
0
1
0
0
0

5
4
.
2
<
A
≤
5
6
.
1

b6
0
0
0
0
1

5
9
.
2
<
B
≤
5
8
.
1

b9
0
0
1
0
1

5
9
.
2
≥
B

b7
1
0
0
0
0

5
8
.
1
≥
B
b8
1
0
1
0
1

5
0
.
3
≥
A
b1
1
0
0
1
0

5
4
.
2
≥
A
b2
1
1
0
1
0

5
6
.
1
≥
A
b3
1
1
0
1
1

5
.
4
≥
C
b10
0
1
0
0
0

3
.
3
≥
C
b11
0
1
1
1
0

9
.
1
≥
C
b12
1
1
1
1
0

5
.
4
<
C
≤
9
.
1

b13
1
0
1
1
0

3
.
3
<
C
≤
9
.
1

b14
1
0
0
0
0

5
.
4
<
C
≤
3
.
3

b15
0
0
1
1
0

s
s
a
l
C
L
1
1
1
0
0

TABLE XI
Binary dataset generated from the Table II having 15 binary variables from b1 to b15.

