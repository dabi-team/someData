Towards Explainable Meta-Learning for DDoS Detection

Qianru Zhoua, Rongzhen Lia, Lei Xua,∗, Arumugam Nallanathanb, Jian Yanga, Anmin Fua

aSchool of Compute Science and Engineering, Nanjing University of Science and Technology, Nanjing, China.
bSchool of Electronic Engineering and Computer Science, Queen Mary University of London, London E1 4NS, U.K.

2
2
0
2

g
u
A
6
1

]
I

A
.
s
c
[

3
v
5
5
2
2
0
.
4
0
2
2
:
v
i
X
r
a

Abstract

The Internet is the most complex machine humankind has ever built, and how to immune it from intrusions is even
more complex. With the ever increasing of new intrusions, intrusion detection tasks are increasingly rely on Artiﬁcial
Intelligence. Interpretability and transparency of the machine learning model is the foundation of trust in AI-driven
intrusion detection. Current interpretable Artiﬁcial Intelligence technologies in intrusion detection are heuristic, which
is neither accurate nor suﬃcient. This paper proposed a rigorous interpretable Artiﬁcial Intelligence driven intrusion
detection approach, based on artiﬁcial immune system. Details of rigorous interpretation calculation process for the
decision tree model are presented. A map, combine and merge (M&M) method is proposed to discretize continuous
features into boolean expression and simplify into prime implicants. Prime implicant explanations for DDoS LOIC
and HOIC attack traﬃc ﬂows are given in detail as rules for the DDoS intrusion detection system. Experiments are
carried out use real-life traﬃc to evaluate the system, it is evident that as the interpretable method is based on formal
logic calculation process, the explanation provides rigorous and suﬃcient reasons for the decision of DDoS traﬃc
ﬂow classiﬁcation.

Keywords: Interpretable machine learning, explainable Artiﬁcial Intelligence, prime implicant, intrusion detection
system, DDoS attack.

1. Introduction

Artiﬁcial Intelligent based decision making have been used broadly in various domains across industries, govern-
ment, and everyday life, such as facial recognition, loan assessment, bail assessment, healthcare, self-driving vehicle,
and cybersecurity etc [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]. However, in domains where security is of
utmost importance, such as cybersecurity, trust is the fundamental basis and guarantee of the validity and prosperity
of AI-based decision making. People’s trust on the decisions made is based on the interpretability and transparency of
the machine learning models make them [16]. Unfortunately, most of the popular machine learning models, such as
deep learning, neural networks, and even the tree-based models are uninterpretable (although the tree-based models
are believed to be interpretable for they can provide the decision paths that lead to the decisions, many have point
out that these explanations are “shallow” and contain potentially too many redundant features and rules, and thus
actually unable to provide rigorous suﬃcient reasons, also known as prime implicant explanations, or minimal suﬃ-
cient reasons [17]). Consequences of the decision made by uninterpretable machine learning models are occasionally
catastrophic, for example the fatal car crushes by Google’s autonomous car [18] and Tesla’s autopilot system [19]; An
automatic bail risk assessment algorithm is believed to be biased and keep many people in jail longer than they should
without explicit reasons, and another machine learning based DNA trace analysis software accuses people with crimes
they did not commit 1; Millions of African-American could not get due medical care by a biased machine learning
assessment algorithm 2; In Scotland, a football game is ruined because the AI camera mistakes the judge’s bald head

∗Corresponding author. zhouqianru@njust.edu.cn
1See https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html
2See https://www.wsj.com/articles/researchers

-find-racial-bias-in-hospital-algorithm-11571941096

Preprint submitted to Elsevier

August 17, 2022

 
 
 
 
 
 
as the ball and keep focusing on it rather than the goal scene3. The key reasons lay in that all machine learning models
suﬀer from overﬁtting [20]. Overﬁtting could be seriously exacerbated by noisy data, and real-life data is, and almost
always, noisy. These, among many other reasons (like GDPR requirements [21] and judicial requirements [22]), have
driven the surge of research interest on the interpretation of machine learning models, analyzing the reasons for posi-
tive or negative decisions, interrogating them by human domain experts, and adjusting them if necessary. That gives
rise to the surge of research interest in Explainable Artiﬁcial Intelligence (XAI) or Interpretable Machine Learning
(IML)4[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15].

In this paper, we make a modest step towards a rigorous XAI driven intrusion detection system for DDoS attacks.
A map, combine and merge (M&M) methodology is proposed to transform any decision tree model with continuous
features into boolean expression with formal logic calculation. By calculating the prime implicants of the boolean
expression, rigorous explanation of the model is extracted and interpreted with human natural language. A decision
tree model achieved almost 100% accuracy in training with CIC-AWS-2018 DDoS datasets is used as a demo in this
paper, rigorous M&M transformation is applied and exhaustive human-readable explanations are presented in detail.
The paper is organized as follows, Section 2 provide a overall literature review for XAI methodologies and its
applications in intrusion detection; Section 3 provides details of proposed rigorous XAI methodologies, including
continuous features discretization and prime implicants calculation, at the end of the section, proposed M&M system
is presented with its key components and architecture; Section 4 present the details of prime implicant explanation
calculated from the target model, together with the evaluation results tested on real-life traﬃc ﬂow instances. Section
5 summarize the work and discuss about a few future challenges that need to be done.

2. Explainable Artiﬁcial Intelligent driven intrusion detections

2.1. Explainable Artiﬁcial Intelligence

Started almost three decades ago, the research on Explainable AI or interpretable AI methodologies have been
numerous, the methodologies proposed can be roughly classiﬁed into two types: rigorous methods (or logical rea-
soning methods) and heuristic methods. Most of the proposed explanation approaches are heuristic approaches, such
as Shaply additive explanations (SHAP), Local Interpretable Model-Agnostic Explanations (LIME), and ANCHOR.
The major issues of heuristic approaches are that they are model-agnostic and cannot guarantee accuracy, thus, they
are not really trustworthy. Besides, the explanations provided are not necessary minimal [23]. Logic based rigorous
approaches, on the other hand, are based on formal method and thus are provably accurate. Two main methodologies
are adopted in logic based rigorous XAI/IAI approaches, one is knowledge compilation, the other is abductive reason-
ing [23]. Adnan Darwiche and his team use knowledge compilation to compile binary models into Boolean circuits,
and seek rigorous, logical complete, prime implicant explanations [14].

As model complexity is a concept often mentioned in the ﬁeld of artiﬁcial intelligence, which emphasizes the
complexity of the model in structure, thus heuristic methods can be classiﬁed into two categories by limiting the
complexity of the model: one is ante-hoc, and the other is post-hoc. Ante-hoc is mainly for the model with lower
complexity, and post-hoc is mainly for the model with higher complexity [24, 25]. Examples of “interpretable”
methods are tree-based models, linear regression, logistic regression, and Naive Bayes Model. Although the tree-
based models are believed to be “interpretable” for they can provide the decision paths (or rules) that leads to the
decisions, many researchers point out that the explanations are “shallow” and contain potentially too many redundant
features and rules, and thus actually unable to provide irredundant suﬃcient reasons, which also known as prime
implicant explanations, or minimal suﬃcient reasons [17].

Heuristic interpretation methods mainly include Partial Dependence Plot (PDP) [26], Accumulated Local Eﬀect
Plot (ALEP) [27], feature interaction, approximation model, local approximation model, shapely value, SHAP [11].
PDP and ALEP both describe how features aﬀect the prediction of machine learning models, and can show whether

3see https://www.ndtv.com/offbeat/ai-camera-ruins-

football-game-by-mistaking-referees-bald-head-
for-ball-2319171

4there is subtle diﬀerence between explainable and interpretable AI, but this is not within the focus of this paper, so we will use XAI to represent

both methodologies throughout the paper.

2

Name

LIME

ANCHOR

Marino
et.al.’s work

LEMNA

MMD-critic

G-REX

DeNNeS

TFs

Johannson
et.al.’s work

BEEF

SHAP

Tree
explainer

DeLP3E

knowledge
compilation

Abductive
based
explanation

Table 1: Summary of XAI methodologies.

Description
generate an explanation by approximating
the underlying model by an interpretable one
(such as a linear model with only
a few non-zero coeﬃcients), learned on perturbations of the original instance
(e.g., removing words or hiding parts of the image)
Improved based on LIME,
provide rule-based, model-agnostic explanations on local behaviors of the models.
use adversarial machine learning to ﬁnd the minimum
modiﬁcations (of the input features) required to correctly
classify a given set of misclassiﬁed samples,
by ﬁnding an adversarial sample that is classiﬁed as positive while minimizing
the distance between the real sample and the modiﬁed sample.
LEMNA generates a small set of interpretable features to
explain the model by approximating a local area
of the complex deep learning decision boundary.
Use maximum mean discrepancy (MMD),
a measure of the diﬀerence between distributions,
to eﬃciently learns prototypes and criticism for Bayesian models.
use a genetic rule extract method G-REX to interpret
the knowledge represented by the architecture and the weights
of machine learning model, generating in forms of regression trees, fuzzy rules,
Boolean rules, and decision trees.
An embedded, deep learning-based cybersecurity
expert system extracting reﬁned rules from
a trained multilayer DNN, could either function on a complete or
incomplete dataset.
use user-tailored Tripolar Argumentation Frameworks (TFs)
to give explanations that can help elicit users’ feedback leading to
positive eﬀects on the quality of future recommendations.
use user provided rules, perform argument-based
reasoning mechanism in prolog, and compute the Explanation
graphs to explain the decisions made.
clustering algorithm over the entire dataset,
in order to generate a local explanation
(in both supporting and counterfactual rules).
to calculate an additive feature
importance score for each particular prediction with a set
of desirable properties
(local accuracy, missingness and consistency) that its antecedents lacked.
compute Shapley value explanations to
directly capture feature interactions and
then get local explanations. By combining
many local explanations, get an understanding of the global model structure.
an extension of the PreDeLP probability
reasoning language in which sentences can be
annotated with probabilistic events.
Use attribution query to ﬁgure out
attributing responsibility to entities given a cyber event,
and achieve interpretation.
compile model into boolean circuits
in the forms of CNF or DNF, and interpret the model
by solving the prime implicant of the circuit.
represent a model into a formalized constraints
and provide cardinality-minimal explanations by
applying abductive reasoning on the model to
answer entailment queries.

Target Models

Ref.

DNN

Any

DNN

DNN

Bayesian

Any

DNN

A new model

decision system

[1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

decision system

[10]

Any

[11]

Trees based

[12]

Bayesian

[13]

Decision Tree
Bayesian
Binary Neural Network

Any

[14]

[15]

Heuristic

rigorous

3

the relationship between target and feature is linear, monotonic, or more complex. Among them, ALEP has faster
calculation speed than PDP, and calculation deviation is smaller. The global proxy model is an interpretable model,
which can approximate the prediction of the black box model after training. Black box models can be explained by
explaining the proxy model. The purpose of the interpretable global proxy model is to approximate the prediction
of the underlying model as accurate as possible, and it can be interpreted at the same time. The concept of proxy
model can be found under diﬀerent names: Approximation Model, Meta Model, Response Surface Model, Emulator,
etc. The local proxy model is itself an interpretable model, used to explain the prediction of a single instance of the
black box machine learning model. Local Interpretable Model-agnostic Explanations (LIME) [1], Local Explanation
Method-using Nonlinear Approximation (LEMNA) [4] and other variations have been proposed as local approxima-
tion methods. By capturing the local features of the model and achieving good results in the interpretation of text and
images, local proxy model methods have achieved good results in the interpretation of text and images by capturing
the local features of the model. These agent models have the beneﬁt of yielding simple explanations, but they focus on
training a local agent model to explain a single prediction, and could not train a global agent model. The SHAP [11]
based on the Shapley value determines its importance by calculating the individual’s contribution, and it was tried by
the Bank of England to explain the mortgage default model [11]. Google also combines Tensorﬂow with SHAP to
further improve interpretability [11]. Rule argumentation based methods use user deﬁned rules as an argument for
reasoning system and express the explanation to the decision made.

A brief summarization of XAI methodologies are presented in Table 1, most of the current XAI approaches
are based on heuristic methods, although they work well on the selected datasets, they cannot guarrenteen their
performance on the other datasets.

2.2. Explainable Artiﬁcial Intelligence in intrusion detection

Various approaches trying to explain machine learning models for cybersecurity have been proposed [28, 29, 30,
31, 32, 3, 33]. Luca Vigano and his group proposed a new paradigm in security research called Explainable Security
(XSec) in [28]. They propose the “Six Ws” of XSec (Who? What? Where? When? Why? and How?) as the standard
perspectives of XAI in cybersecurity domain.

Marco Melis trys to explain malicious black-box android malware detections on any-type of models [31]. This
work leverages a gradient-based approach to identify the most inﬂuential local features. It also enables use of nonlinear
models to potentially increase accuracy without sacriﬁcing interpretability of decisions on the DREBIN Dataset.
Drebin as such explains its decisions by reporting, for any given application, the most inﬂuential features, i.e., those
features present in a given application and assigned the highest absolute weights by the classiﬁer.

[3] tries to use adversarial machine learning to ﬁnd the minimum modiﬁcations (of the input features) required
to correctly classify a given set of misclassiﬁed samples, to be speciﬁc, it tries to ﬁnd an adversarial sample that is
classiﬁed as positive with the minimum distance between the real sample and the modiﬁed sample.

Other works on explainable android malware detection do not speciﬁcally use explainable machine learning mod-
els but do make use of other feature analyses to reduce uncertainty of information. For example, [33] uses static
analysis and probability statistics-based feature extraction analysis to detect and analyze malicious Android apps.

[32] try to interpret the rules for malicious node identiﬁcation by directly use paths in decision tree model trained
by KDD dataset. However, many researchers believe the direct interpretation provided by decision tree model is
“shallow” and redundancy, and thus is not necessarily the minimal prime implicant interpretation, in other words, it is
not the radical reason.

A brief summary of current XAI applications on cybersecurity is presented in Table 2, with details of the methodol-
ogy, datasets used, and target models. In the author’s humble knowledge, almost all the state-of-the-art methodologies
in intrusion detection are heuristic or simply direct “shallow” interpretations provided by tree-based models, both
are neither accurate nor suﬃcient, thus cannot be considered really “interpretable” [23, 34]. Logic based rigorous
approaches, on the other hand, are based on formal method and thus are provably accurate [14].

2.3. DDoS Attacks

DDoS is an attack that the attacker seeks to exhaust network resource by disrupting services of multiple hosts
in the network. It is usually done by ﬂooding the target hosts with superﬂuous requests attempting to overload the
network.

4

DDoS attack traﬃc ﬂows show diﬀerent patterns according to the tools that generate them. There are many tools

available for DDoS attack generation, the top ones are:

Low Orbit Ion Cannon (LOIC): designed to ﬂood target systems with junk TCP, UDP and HTTP GET requests.
However, a single LOIC user is unable to generate enough requests to signiﬁcantly impact a target. For an attack to
succeed, thousands of users must coordinate and simultaneously direct traﬃc to the same network.

High Orbit Ion Cannon (HOIC): Designed to improve several LOIC ﬂaws, HOIC is able to attack as many as
256 URLs at the same time. Unlike LOIC, which is able to launch TCP, UDP and HTTP GET ﬂoods, HOIC conducts
attacks based solely on HTTP GET and POST requests.

SolarWinds5: provides a security event manager that is eﬀective mitigation and prevention software to stop the
DDoS Attack. It will monitor the event logs from a wide range of sources for detecting and preventing DDoS activities.
The security event manager will identify interactions with potential command and control servers by taking advantage
of community-sourced lists of known bad actors. For this, it consolidates, normalizes, and reviews logs from various
sources like IDS/IPs, ﬁrewalls, servers, etc.

HTTP Unbearable Load King (HULK)6: It is a DoS attack tool for the web server. It is created for research
purposes. It can bypass the cache engine, generate unique and obscure traﬃc, but it may fail in hiding the identity.
Traﬃc coming through HULK can be blocked.

Tor’s Hammer7: It is created for testing purposes for slow post attack.
Slowloris8: Slowloris tool is used to make a DDoS attack. It is used to make the server down. It sends authorized
HTTP traﬃc to the server while doesn?t aﬀect other services and ports on the target network. This attack tries to keep
the maximum connection engaged with those that are open by sending a partial request. It tries to hold the connections
as long as possible. As the server keeps the false connection open, this will overﬂow the connection pool and will
deny the request to the true connections. However, as it makes the attack at a slow rate, traﬃc can be easily detected
and blocked.

XOIC9: a DDoS attacking tool that can ﬁre attack on small websites. It is easy to use, but also easy to detect and

block.

DDoS Simulator (DDOSIM)10: works on Linux system, it is designed for simulating the real DDoS attack. It

can attack on the website as well as on the network using valid or invalid requests.

R-U-Dead-Yet (RUDY)11: makes the attack using a long form ﬁeld submission through POST method. As it

works at a very slow rate, it can be easily detected and blocked.

DDoS HOIC attack, DDoS LOIC UDP attack, and DDoS LOIC HTTP attack datasets are collected and investigated

in this paper.

3. Rigorous XAI Driven Intrusion Detection System

3.1. Rigorous XAI

While heuristic XAI methods compute approximations of real explanations, rigorous explanations are guaranteed
to be accurate and suﬃcient. Rigorous explanation methods compile machine learning models into Boolean circuits
that can make the same decisions with the models.

In rigorous XAI theory [14, 15, 23], a classiﬁer is a Boolean function which can be represented by a propositional
formula ∆. An implicant τ of a propositional formula ∆ is a term that satisﬁes ∆, namely τ |= ∆. A prime implicant is
an implicant that is not subsumed by any other implicants, that is there is no implicate τ(cid:48) that contains a strict subset
of the literals of τ. Prime implicant have been used to give rigorous explanations in XAI. Explanations given using
prime implicant are also called suﬃcient reasons, which are deﬁned formally below by [14].

5https://www.solarwinds.com/security-event-manager/use-cases/ddos-attack?CMP=BIZ-RVW-SWTH-DDoSAttackTools-SEM-UC-Q120
6https://packetstormsecurity.com/files/112856/HULK-Http-Unbearable-Load-King.html
7https://sourceforge.net/projects/torshammer/
8https://github.com/gkbrk/slowloris
9https://sourceforge.net/directory/?q=xoic
10https://sourceforge.net/projects/ddosim/
11https://sourceforge.net/projects/r-u-dead-yet/

5

Table 2: Summary of XAI Methodologies in Intrusion Detection.

Name

Marino
et.al.’s work

Marco Melis
et.al.’s work

DeNNeS

Description
use adversarial machine learning to ﬁnd the minimum
modiﬁcations (of the input features) required to correctly
classify a given set of misclassiﬁed samples,
by ﬁnding an adversarial sample that is classiﬁed
as positive while minimizing the distance
between the real sample and the modiﬁed sample.
leveraging a gradient-based approach to
identify the most inﬂuential local features,
apply on any black-box machine-learning model
An embedded, deep learning-based cybersecurity
expert system extracting reﬁned rules
from a trained multilayer DNN, could either
function on a complete or incomplete dataset.

J. N. Paredes
et.al.’s work

An vision of combining knowledge reasoning-driven
and data-driven approaches for XAI in cybersecurity

LEMNA

LEMNA generates a small set of interpretable features to
explain the model by approximating a local area
of the complex deep learning decision boundary.

Target Models

Dataset

DNN

NSL-KDD

Any

DNN

N/A

DNN

DREBIN

UCI’s phishing websites dataset
Android malware dataset

National Vulnerability Database
MITRE CVE
MITRE CWE
MITRE ATT&CK
Binary dataset
generated in
BYTEWEIGHT[35]

Ref.

[3]

[31]

[7]

[34]

[4]

Deﬁnition 1 (Suﬃcient Reason [14]) A suﬃcient reason for decision ∆α is a property of instance α that is also a

prime implicant of ∆α (∆α is ∆ if the decision is positive and (cid:113)∆ otherwise).

A suﬃcient reason (or prime implicant explanation) is also the minimum explanation. The major diﬀerence
between suﬃcient reason and prime implicant is that suﬃcient reason disclose the reasons of a certain instance while
prime implicant illustrate the essential characteristics of the model [14]. Suﬃcient reason explains the root cause of
the decision for an instance, in terms of the prime implicants involved. The decision will stay unchanged no matter
how the other characters change, and none of its strict subsets can justify the decision. Please be noted that a decision
may have multiple suﬃcient reasons, sometimes many [14].

The Quine–McCluskey algorithm (QMC) (also known as the method of prime implicants or tabulation method) is
used in this paper to get the prime implicants from a Boolean expression. It is a classic boolean expression minimiza-
tion method developed by Willard V. Quine in 1952 and extended by Edward J. McCluskey in 1956 [36]. Many of the
state-of-the-art methods computing prime implicant from a boolean expression are variations of the Quine–McCluskey
algorithm.

3.2. Architecture of M&M

The architecture of the proposed rigorous XAI driven intrusion detection system M&M is shown in Fig. 1.
As discussed in our previous work [37], the ﬂow-based statistical data generated from CICFlowMeter are used
instead of direct packet header information. Features generated from CICFlowMeter, which have continuous values,
are mapped into discrete variables as discussed in Section 3.3. Then boolean expression of the machine learning
model will be generated and further simpliﬁed into prime implicants. Depend on the speciﬁc boolean expression, the
simpliﬁcation process, which is a SAT quesion, may have NP-hard complexity. The prime implicant generated are
the suﬃcient reasons learned by the machine learning model from the ﬂow-based statistical traﬃc data. They should
be interrogated by human expert, judging with the experts’ knowledge and experience. The audited rules should be
taken into consideration when designing new informed machine learning models, to achieve more accurate intrusion
detection.

3.3. Formal Description of Map and Merge (M&M)

Rigorous logical reasoning methods work directly on boolean expressions. Machine learning models with boolean
features can be immediately represented as boolean circuits [14]. Machine learning models with discrete features can
be transformed into boolean expressions by representing the fact that “a feature equals to a certain value” with an atom
variable [14]. However, most of the classiﬁers used in intrusion detection system have continuous features. Darwiche
et.al. proposed a mapping method to map continuous features into discrete ones [14], as presented below in Fig. 2.
Based on that mapping method, we proposed a M&M algorithm for discretized feature for meta-learning models, by

6

Figure 1: The proposed architecture of M&M.

(a) A Example of Decision
Tree

(b) Discretized Features

Figure 2: A simple decision tree with continuous values.

7

add combine and simplify process, hence map, combine, and merge (M&M) method. The detail of M&M discretize
method is presented below.

3.3.1. Map

Before describe the process of M&M in detail, some formal deﬁnitions are given below.
Deﬁnition Decision Path: A decision path is a conjunction of conditions from root node to the leaf. For a decision

tree model with n leafs, there are n decision paths.

Deﬁnition Rule: A rule r in a decision tree model is deﬁned in the form of “if F1 > F1down and F1 < F1up and
. . . and Fn > Fndown and Fn < Fnup then Labeli”, where Fi is the ith feature on the decision path and Fiup is the upper
bound of the feature value in the decision path, and Fidown is the lower bound of the feature value.

Based on the deﬁnitions, the decision tree model can be deﬁned as a set of rules R = {ri}, 0 (cid:54) i (cid:54) n, where n is

the number of decision paths in the model.

Algorithm 1 Rule of Map

LET Feature Space: F ←− ∅
LET rule Set: R ←− {ri}, 0 (cid:54) i (cid:54) n
FOR r IN R:

FOR features fi IN rule r:

F ← fi
fi.values ←− ∅
FOR every time fi appear IN rule r:

fi.values ← fi.value

SORT fi.values
FOR intervals (fidown , fiup] in fi.values:

ASSIGN discrete features Dfi = fi ∈ (fidown, fiup]

RETURN Discrete Features Df = {Dfi}

Take the decision tree in Fig. 4(a) for example, the solid lines represent if the node is true, while the dashed lines
represent false. Based on each decision node, features with continuous values are discretized into several variables,
each represent an interval divided by decision nodes. As shown in Fig. 4(b), feature X in Fig. 4(a) are discretized into
x1, and x2, representing the intervals (−∞, 2) and [2, ∞) respectively, thus we have

x1 ∨ x2 |= U && x1 ∧ x2 |= (cid:11)
y1 ∨ y2 |= U && y1 ∧ y2 |= (cid:11)
z1 ∨ z2 |= U && z1 ∧ z2 |= (cid:11)

Thus, the decision rule of the decision tree in Fig. 4(a) can be represented by boolean expression

∆ = (x1 ∧ y1) ∨ (x2 ∧ z1) ∨ (x2 ∧ z2)

According to Eq. 3, we have

Together with De Morgan’s law, Eq. 4 can be further simpliﬁed to

z1 = ¯z2

∆ = (x1 ∧ y1) ∨ x2

(1)

(2)

(3)

(4)

(5)

In which x1 ∧ y1 and x2 are prime implicants of the decision tree in Fig. 4(a), and thus the rigorous explanation
of the decision tree are “So long as x1 ∧ y1 or x2, the decision will be 1”. According to the discretization rule, the
rigorous explanation can further be “So long as X < 2 and Y < 3, or X ≥ 5 in the instance, the decision is guarranteed
to be 1.”

8

3.3.2. Combine

As intrusion datasets are often collected separately and distributively over the networking system, meta-learning
is often required for each network element to build their own model and collect their own data and train their classiﬁer
as accurate as possible, and then all the models can be combined into a uniﬁed model ready for interpretation. The
meta-learning model combining algorithm is shown in Algorithm 2 in detail.

Algorithm 2 Rule of Combine

LET Ti ←− One model to combine
LET T j ←− Another model to combine
LET Fi ←− features in Ti:
LET F j ←− features in T j:
LET Fcombine ←− ∅
FOR feature fi in Fi:
IF fi NOT IN F j:
Fcombine ←− fi

ELSE:

Fcombine ←− merged sets fi ∪ f j

RETURN Fcombine

3.3.3. Merge

The discrete features get from the map process may (and often) contain (potentially a large number of) redundancy.

As the number of boolean expression is

N = 2n

where n is the number of discrete features, directly transform the discrete features into boolean circuits may experience
a huge waste of computing and storage expense, due to Combinatorial Explosion. Thus the merge process proposed
in Algorithm 3 is used in M&M.

Algorithm 3 Rule of Merge

For xi = (v1, v2] and xi+1 = (v2, v3], (v1 ≤ v2 ≤ v3)
AND si = {set o f f eatures in rule ri}
IF ¬∃((xi ∈ si) ∧ (xi+1 (cid:60) si)) ∨ ((xi (cid:60) si) ∧ (xi+1 ∈ si))
THEN DELETE xi+1 AND xi = xi ∪ xi+1 = (v1, v3]

For example, let ∆ = ((x1 ∨ x2) ∧ y1) ∨ ((x1 ∨ x2) ∧ z2) be the boolean expression of a model after map process,
according to Algorithm 3, (x1 ∨ x2) ﬁts the requirement of merge rule, and thus the model can be simpliﬁed into
∆ = (x1new ∧ y1) ∨ (x1new ∧ z2), where x1new = x1old ∨ x2old.

4. Evaluation of M&M

Evaluation experiments are carried out and presented in this section.

4.1. Model Selection

In our previous work, we have evaluated 8 kinds of common machine learning models on eleven diﬀerent kinds
of real-life intrusion traﬃc data [37]. From the evaluation result in [37], it is evident that decision tree has the best
performance in both accuracy and time expense when detecting known intrusion. Thus we use decision tree model
trained in the previous experiment as the target model, and compute the rigorous explanation of the model.

The decision tree model varies during the ﬁtting process. A model is considered to be stable when the features of
model (e.g., number of leaves in the tree, maximum depth, and node count) are tending towards stability. For example,
if the number of leaves (or maximum depth, node count, etc.) in the tree keeps growing with the ﬁtting process, it

9

implies that the model is still learning new rules from the training data, and when the features ﬂuctuates around a
certain value, it implies the model has learnt all the rules from training dataset and is considered to be stable. The
training process is carried out and the model features together with the detection results are recorded and represented
in Fig. 3. The features of the decision tree models used are number of leaves in the tree, maximum depth, and node
count. The indicators for detection accuracy are area under the curve (AUC), precision of benign traﬃc, precision of
evil traﬃc.

Figure 3: The stability of decision tree model.

From Fig. 3 it is evident that the number of leaves, maximum depth, and node count of decision tree models
trained per round ﬂuctuate within a narrow range, do not show any pronounce trend (of increase or decline). It is
reasonable to believe that the model is stable and all that could be learn from the training dataset have been learn. A
modest model (by modest I mean in terms of number of leaves, maximum depth, and node count) is selected as the
target model for rigorous explanation computing. The correlation between features of the selected models are shown
in Fig. 4.

4.2. Feature Discretization with M&M

M&M discretization method in Section 3.3 is used to transform continuous features into discretized one. The
Features before and after discretization are presented in Table 3. After discretization, 16 continuous features are
transformed to 47 discrete variables. Thus, based on their feature, each instance can be mapped into a 47 bits binary
expression

a1a2b1b2c1c2d1d2d3d4d5e1e2 f1 f2 f3 f4 f5 f6 f7 f8g1g2g3

g4g5g6h1h2h3i1i2i3 j1 j2k1k2l1l2m1m2n1n2o1o2 p1 p2

Algorithm 4 Continuous Features of a sample ﬂow.

S ub f low Fwd Byts = 553,
Flow IAT Max = 73403,
Fwd S eg S ize Avg = 61.4, Flow Pkts s = 113.2,
Init Fwd Win Byts = 8192, Dst Port = 22,
Fwd IAT Min = 14, Flow Duration = 88751,
Fwd IAT S td = 49295.7, Flow IAT S td = 42321,
Flow IAT Mean = 42321, Idle Max = 0.0,
Idle Min = 0, Ack Flag Cnt = 0.0
Fwd Act Data Pkts = 7

For example, a ﬂow instance with the features shown in Algorithm 4, will be mapped into

a1 ¯a2b1 ¯b2 ¯c1c2d1 ¯d2 ¯d3 ¯d4 ¯d5e1 ¯e2 ¯f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 f8 ¯g1 ¯g2 ¯g3

10

(a) Features correlation of DDos LOIC UDP & HTTP

(b) Feature correlation of DDoS HOIC

Figure 4: A simple decision tree with continuous values.

¯g4g5 ¯g6h1 ¯h2 ¯h3i1 ¯i2 ¯i3 ¯j1 j2k1 ¯k2l1 ¯l2m1 ¯m2n1 ¯n2o1 ¯o2 p1 ¯p2

which can be represented in numeric:

10100110000100000000100001010010001101010101010

4.3. Prime Implicant Explanations

After the M&M feature discretization, the model can be represented into boolean expressions, and prime Impli-
cants are calculated with Quine-McCluskey algorithm. The prime implicants explanations for DDoS LOIC HTTP,
DDoS LOIC UDP, and DDoS HOIC in minterms are shown in Table 4, 5, 6, and 8, in which a “−” means a “don’t
care”. The behavior of the DDoS traﬃc detection of the decision tree model can be rigorously interpreted as

∆ = τ1 ∨ τ2 ∨ · · · ∨ τ68

Prime implicants explanation examples for ﬁve real-life benign traﬃc ﬂow instances explanation is presented
below. The original features, discretized features, and boolean expression of each instance are presented in detail.
The reason for why it is classiﬁed as benign is provided and marked in red in its boolean expression, by the prime
implicant that is used to make the decision, which is suﬃcient and rigorous to explain the decision.

Use Case 1: In ﬂow instance 1, its boolean expression is

after feature discretization and mapping, and it matches with prime implicant τ41:

0011000010010100100100101000101100110

001 − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − −10

which means “if a ﬂow’s boolean expression start with 001, and end with 10, no matter what values is for the
other bits, it is a benign ﬂow.”. With the feature discretization mapping method, prime implicant τ41 is originally
“ ¯a1 ¯a2a3n1 ¯n2” which can also be interpreted as “if a ﬂow’s Fwd Pkt Len Min is larger than 110.5, and Fwd Seg Size Avg
is smaller than 486, then it is benign.” Thus, the model decides this ﬂow is benign. The whole decision process is
formal and rigorous, and the reasons is suﬃcient, as proved in Section 3.1.

11

Table 3: Discretized Feature after M&M process.

Continuous

A: Subﬂow Fwd Byts

B: Flow IAT Max

C: Fwd Seg Size Avg

D: Flow IAT Min

E: Flow Pkts s

F: Init Fwd Win Byts

G: Dst Port

H: Fwd IAT Min

I: Flow Duration

J: Fwd IAT Std

K: Flow IAT Std

L: Flow IAT Mean

M: Idle Max

N: Idle Min

O: ACK Flag Cnt

P: Fwd Act Data Pkts

After Map
a1: (−∞, 21]
a2: (21, +∞)
b1: (−∞, 342916]
b2: (342916, +∞)
c1: (−∞, 5.7]
c2: (5.7, +∞)
d1: (−∞, 1.5]
d2: (1.5, 4997.5]
d3: (4997.5, 5440.5]
d4: (4997.5, 5440.5]
d5: (5836, 11980.5]
d6: (11980.5, 56360494]
d7: (56360494, +∞]
e1: (−∞, 367.35]
e2: (367.35, +∞)
f1: (−∞, 252.5]
f2: (252.5, 1794]
f3: (1794, 1948]
f4: (1948, 1999.5]
f5: (1999.5, 2079.5]
f6: (2079.5, 2520.5]
f7: (2520.5, 5120.5]
f8: (5120.5, 30969]
f9: (30969, +∞)
g1: (−∞, 51.5]
g2: (51.5, 52.5]
g3: (52.5, 80.5]
g4: (80.5, 261.5]
g5: (261.5, 25929]
g6: (25929, +∞)
h1: (−∞, 16.5]
h2: (16.5, 92921]
h3: (92921, +∞)
i1: (−∞, 52151692]
i2: (52151692, 53010668]
i3: (53010668, +∞)
j1: (−∞, 16847.75]
j2: (16847.75, +∞)
k1: (−∞, 978470.19]
k2: (978470.19, +∞)
l1: (−∞, 16000000]
l2: (16000000, +∞)
m1: (−∞, 52194244]
m2: (52194244, +∞)
n1: (−∞, 28200000]
n2: (28200000, 53100000]
n3: (53100000, +∞)
o1: (−∞, 0.5]
o2: (0.5, +∞)
p1: (−∞, 201.5]
p2: (201.5, +∞)

After M&M
a1: (−∞, 21]
a2: (21, +∞)
b1: (−∞, 342916]
b2: (342916, +∞)
c1: (−∞, 5.7]
c2: (5.7, +∞)

d1−3: (−∞, 5440.5]

d4: (4997.5, 5440.5]
d5: (5836, 11980.5]
d6: (11980.5, 56360494]
d7: (56360494, +∞]
e1: (−∞, 367.35]
e2: (367.35, +∞)
f1−2: (−∞, 1794]

f3: (1794, 1948]
f4: (1948, 1999.5]
f5: (1999.5, 2079.5]
f6: (2079.5, 2520.5]
f7: (2520.5, 5120.5]
f8: (5120.5, 30969]
f9: (30969, +∞)
g1: (−∞, 52.5]
g2: (51.5, 52.5]
g3: (52.5, 80.5]
g4: (80.5, 261.5]
g5: (261.5, 25929]
g6: (25929, +∞)
h1: (−∞, 16.5]
h2: (16.5, 92921]
h3: (92921, +∞)
i1: (−∞, 52151692]
i2: (52151692, 53010668]
i3: (53010668, +∞)
j1: (−∞, 16847.75]
j2: (16847.75, +∞)
k1: (−∞, 978470.19]
k2: (978470.19, +∞)
l1: (−∞, 16000000]
l2: (16000000, +∞)
m1: (−∞, 52194244]
m2: (52194244, +∞)
n1: (−∞, 28200000]
n2−3: (28200000, +∞)
o1: (−∞, 0.5]
o2: (0.5, +∞)
p1: (−∞, 201.5]
p2: (201.5, +∞)

Use Case 2: In ﬂow instance 5, whose feature is mapped into boolean expression

0100100100010100100100101010001100110

is classiﬁed into “Benign” because it matches with prime implicant τ39:

0100100 − − − − − − − − − − − − − − − − − − − − − − − − − −01 − −

which is

¯a1a2 ¯a3 ¯b1b2 ¯b3 ¯b4 ¯m1m2

says “if a ﬂow has Fwd Pkt Len Min between 49.5 and 110.5, Fwd Pkt s between 0.01 and 3793, and Fwd IAT Tot
larger than 486, then it is benign.” It is worth noting that one instance can match more than one (sometimes even
many) prime implicants (although we did not have such experience in this experiment), which means that there are
multiple explanations for the decision made on that instance, and each one of these reasons is suﬃcient and rigorous.
As the explanation method is based on formal logic, the rules explained is guaranteed to have the same behavior with
the model, thus the accuracy of M&M is the same as the decision tree model, which is compared with other models

12

Table 4: Prime Implicants explanations for DDoS LOIC HTTP attack (1/2).

#

τ1

τ2

τ3

τ4

τ5

τ6

τ7

τ8

τ9

τ10

τ11

τ12

τ13

τ14

τ15

τ16

τ17

τ18

τ19

τ20

τ21

τ22

τ23

τ24

τ25

τ26

τ27

τ28

τ22

τ23

τ24

τ25

τ26

τ27

τ28

τ29

τ30

τ31

τ32

τ33

τ34

τ35

τ36

τ37

τ38

τ39

Minterm
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1d2 ¯d3 ¯d4 ¯d5 ¯e1e2
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯f1 ¯f2 ¯f3 ¯f4 f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯f1 ¯f2 ¯f3 ¯f4 ¯f5 f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8g1 ¯g2 ¯g3 ¯g4 ¯g5 ¯g6 ¯h1 ¯h2h3
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8g1 ¯g2 ¯g3 ¯g4 ¯g5 ¯g6 ¯h1 ¯h2h3
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1g2 ¯g3 ¯g4 ¯g5 ¯g6 ¯h1 ¯h2h3
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1g2 ¯g3 ¯g4 ¯g5 ¯g6 ¯h1 ¯h2h3
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1 ¯g2g3 ¯g4 ¯g5 ¯g6 ¯h1 ¯h2h3
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1 ¯g2g3 ¯g4 ¯g5 ¯g6 ¯h1 ¯h2h3
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1 ¯g2 ¯g3g4 ¯g5 ¯g6 ¯h1 ¯h2h3
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1 ¯g2 ¯g3g4 ¯g5 ¯g6 ¯h1 ¯h2h3
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3d4 ¯d5 ¯f1 f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯f1 f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3d4 ¯d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2c1 ¯c2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯f1 ¯f2 ¯f3 ¯f4 f5 ¯f6 ¯f7 ¯f8
a1 ¯a2b1 ¯b2 ¯c1c2g1 ¯g2 ¯g3 ¯g4 ¯g5 ¯g6
a1 ¯a2b1 ¯b2 ¯c1c2 ¯g1g2 ¯g3 ¯g4 ¯g5 ¯g6
a1 ¯a2b1 ¯b2 ¯c1c2 ¯g1 ¯g2g3 ¯g4 ¯g5 ¯g6
a1 ¯a2b1 ¯b2 ¯c1c2 ¯g1 ¯g2 ¯g3g4 ¯g5 ¯g6
a1 ¯a2b1 ¯b2 ¯c1c2 ¯g1 ¯g2 ¯g3 ¯g4g5 ¯g6
a1 ¯a2 ¯g1 ¯g2g3 ¯g4 ¯g5 ¯g6i1 ¯i2 ¯i3 j1 ¯j2 ¯j3k1 ¯k2
a1 ¯a2 ¯b1b2g1 ¯g2 ¯g3 ¯g4 ¯g5 ¯g6h1 ¯h2 ¯h3 ¯j1 j2
a1 ¯a2 ¯b1b2 ¯g1g2 ¯g3 ¯g4 ¯g5 ¯g6h1 ¯h2 ¯h3 ¯j1 j2
a1 ¯a2 ¯b1b2 ¯g1 ¯g2g3 ¯g4 ¯g5 ¯g6h1 ¯h2 ¯h3 ¯j1 j2
a1 ¯a2 ¯b1b2d1 ¯d2 ¯d3 ¯d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2 ¯d1d2 ¯d3 ¯d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2d3 ¯d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2 ¯d1d2 ¯d3 ¯d4 ¯d5 ¯f1 f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2d3 ¯d4 ¯d5 ¯f1 f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3d4 ¯d5 ¯f1 f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2 ¯d1d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2d3 ¯d4 ¯d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3d4 ¯d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3 ¯m1m2
a1 ¯a2 ¯b1b2d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3
a1 ¯a2 ¯b1b2 ¯d1d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3
a1 ¯a2 ¯b1b2 ¯d1 ¯d2d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1i2 ¯i3

Boolean Expression

1010100100001 − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000100 − −00100000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000100 − −00010000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000100 − −00001000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000100 − −00000100 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000010 − −10000000100000001 − − − − − − − − − − − − − − − −−

10101000001 − −10000000100000001 − − − − − − − − − − − − − − − −−

10101000010 − −10000000010000001 − − − − − − − − − − − − − − − −−

10101000001 − −10000000010000001 − − − − − − − − − − − − − − − −−

10101000010 − −10000000001000001 − − − − − − − − − − − − − − − −−

10101000001 − −10000000001000001 − − − − − − − − − − − − − − − −−

10101000010 − −10000000000100001 − − − − − − − − − − − − − − − −−

10101000001 − −10000000000100001 − − − − − − − − − − − − − − − −−

10101000010 − −01000000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000001 − −01000000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000010 − −00100000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000001 − −00100000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000010 − −00010000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000001 − −00010000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000010 − −00001000 − − − − − − − − − − − − − − − − − − − − − − − − − −

10101000001 − −00001000 − − − − − − − − − − − − − − − − − − − − − − − − − −

101001 − − − − − − − − − − − − − − − 100000 − − − − − − − − − − − − − − − − − − − −

101001 − − − − − − − − − − − − − − − 010000 − − − − − − − − − − − − − − − − − − − −

101001 − − − − − − − − − − − − − − − 001000 − − − − − − − − − − − − − − − − − − − −

101001 − − − − − − − − − − − − − − − 000100 − − − − − − − − − − − − − − − − − − − −

101001 − − − − − − − − − − − − − − − 000010 − − − − − − − − − − − − − − − − − − − −

1001 − − − − − − − − − − − − − − − − − 001000 − − − 1001010 − − − − − − − − − −

1001 − − − − − − − − − − − − − − − − − 10000010010001 − − − − − − − − − − − −

1001 − − − − − − − − − − − − − − − − − 01000010010001 − − − − − − − − − − − −

1001 − − − − − − − − − − − − − − − − − 00100010010001 − − − − − − − − − − − −

1001 − −10000 − −10000000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −01000 − −10000000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −00100 − −10000000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −00010 − −10000000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −10000 − −01000000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −01000 − −01000000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −00100 − −01000000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −00010 − −01000000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −10000 − −00100000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −01000 − −00100000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −00100 − −00100000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −00010 − −00100000 − − − − − − − − − 010 − − − − − −01 − − − − − −

1001 − −10000 − −00010000 − − − − − − − − − 010 − − − − − − − − − − − − − −

1001 − −01000 − −00010000 − − − − − − − − − 010 − − − − − − − − − − − − − −

1001 − −00100 − −00010000 − − − − − − − − − 010 − − − − − − − − − − − − − −

1001 − −00010 − −00010000 − − − − − − − − − 010 − − − − − − − − − − − − − −

13

Table 5: Prime Implicants explanations for DDoS LOIC HTTP attack (2/2).

#

τ40

τ41

τ42

τ43

τ44

τ45

τ46

τ47

τ48

τ49

τ50

τ51

τ52

Minterm
a1 ¯a2 ¯b1b2d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1 ¯i2i3
a1 ¯a2 ¯b1b2 ¯d1d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1 ¯i2i3
a1 ¯a2 ¯b1b2 ¯d1 ¯d2d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1 ¯i2i3
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯i1 ¯i2i3
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3 ¯d4d5g1 ¯g2 ¯g3 ¯g4 ¯g5 ¯g6 ¯i1i2 ¯i3 ¯n1n2 ¯o1o2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯g1g2 ¯g3 ¯g4 ¯g5 ¯g6 ¯i1i2 ¯i3 ¯n1n2 ¯o1o2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯g1 ¯g2g3 ¯g4 ¯g5 ¯g6 ¯i1i2 ¯i3 ¯n1n2 ¯o1o2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯g1 ¯g2 ¯g3g4 ¯g5 ¯g6 ¯i1i2 ¯i3 ¯n1n2 ¯o1o2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3 ¯d4d5g1 ¯g2 ¯g3 ¯g4 ¯g5 ¯g6 ¯i1 ¯i2i3 ¯n1n2 ¯o1o2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯g1g2 ¯g3 ¯g4 ¯g5 ¯g6 ¯i1 ¯i2i3 ¯n1n2 ¯o1o2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯g1 ¯g2g3 ¯g4 ¯g5 ¯g6 ¯i1 ¯i2i3 ¯n1n2 ¯o1o2
a1 ¯a2 ¯b1b2 ¯d1 ¯d2 ¯d3 ¯d4d5 ¯g1 ¯g2 ¯g3g4 ¯g5 ¯g6 ¯i1 ¯i2i3 ¯n1n2 ¯o1o2

Boolean Expression

1001 − −10000 − −00010000 − − − − − − − − − 001 − − − − − − − − − − − − − −

1001 − −01000 − −00010000 − − − − − − − − − 001 − − − − − − − − − − − − − −

1001 − −00100 − −00010000 − − − − − − − − − 001 − − − − − − − − − − − − − −

1001 − −00010 − −00010000 − − − − − − − − − 001 − − − − − − − − − − − − − −

1001 − −00001 − − − − − − − − − −100000 − − − 010 − − − − − − − −0101 − −

1001 − −00001 − − − − − − − − − −010000 − − − 010 − − − − − − − −0101 − −

1001 − −00001 − − − − − − − − − −001000 − − − 010 − − − − − − − −0101 − −

1001 − −00001 − − − − − − − − − −000100 − − − 010 − − − − − − − −0101 − −

1001 − −00001 − − − − − − − − − −100000 − − − 001 − − − − − − − −0101 − −

1001 − −00001 − − − − − − − − − −010000 − − − 001 − − − − − − − −0101 − −

1001 − −00001 − − − − − − − − − −001000 − − − 001 − − − − − − − −0101 − −

1001 − −00001 − − − − − − − − − −000100 − − − 001 − − − − − − − −0101 − −

¯a1a2 ¯p1 p2

01 − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − 01

Table 6: Prime Implicants explanations for DDoS LOIC UDP attack.

#

τ53

τ54

τ55

τ56

τ57

τ58

τ59

τ60

τ61

τ62

τ63

τ64

τ65

τ66

τ60

τ61

τ62

τ63

τ64

τ65

τ66

Minterm
d1 ¯d2 ¯d3 ¯d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8g1 ¯g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8g1 ¯g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8g1 ¯g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8g1 ¯g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 f5 ¯f6 ¯f7 ¯f8g1 ¯g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 ¯f5 f6 ¯f7 ¯f8g1 ¯g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 f7 ¯f8g1 ¯g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 f5 ¯f6 ¯f7 ¯f8 ¯g1g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 ¯f5 f6 ¯f7 ¯f8 ¯g1g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 f7 ¯f8 ¯g1g2 ¯g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1 ¯g2g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1 ¯g2g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 f3 ¯f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1 ¯g2g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 f4 ¯f5 ¯f6 ¯f7 ¯f8 ¯g1 ¯g2g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 f5 ¯f6 ¯f7 ¯f8 ¯g1 ¯g2g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 ¯f5 f6 ¯f7 ¯f8 ¯g1 ¯g2g3
d1 ¯d2 ¯d3 ¯d4 ¯d5 ¯f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 f7 ¯f8 ¯g1 ¯g2g3

Boolean Expression

− − − − − − 10000 − −10000000100000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −01000000100000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00100000100000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00010000100000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00001000100000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00000100100000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00000010100000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −10000000010000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −01000000010000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00100000010000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00010000010000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00001000010000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00000100010000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00000010010000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −10000000001000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −01000000001000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00100000001000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00010000001000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00001000001000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00000100001000 − − − − − − − − − − − − − − − − − − − −

− − − − − − 10000 − −00000010001000 − − − − − − − − − − − − − − − − − − − −

Table 7: Prime Implicants explanations for DDoS HOIC attack.

#

τ67

τ68

Minterm
¯f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 f8 ¯g1g2 ¯g3
¯f1 ¯f2 ¯f3 ¯f4 ¯f5 ¯f6 ¯f7 f8 ¯g1 ¯g2g3

Boolean Expression

− − − − − − − − − − − − −00000001010000 − − − − − − − − − − − − − − − − − − − −

− − − − − − − − − − − − −00000001001000 − − − − − − − − − − − − − − − − − − − −

14

in Tables. 8 and 9. As shown in Tables 8 and 9, the true-positive rate for all DDoS intrusions discussed in this paper
are 100%, and the false-positive rate are all 0%.

Table 8: Results of DDoS-HOIC Attack on M&M and other Models.

True-Positive
False-Positive

recall

f1-score

Benigh
DDoS-HOIC
Benigh
DDoS-HOIC

Random Forest
100%
0%
1.0
1.0
1.0
1.0

Naive Bayes
100%
0%
1.0
1.0
1.0
1.0

M&M
100%
0%
1.0
1.0
1.0
1.0

Neural Network (MLP)
100%
0%
1.0
1.0
1.0
1.0

Quadratic Discriminante
100%
0%
1.0
1.0
1.0
1.0

KNeighbors
100%
0%
1.0
1.0
1.0
1.0

Table 9: Results of DDOS-LOIC-UDP Attack on M&M and other Models.

True-Positive
False-Positive

recall

f1-score

Benigh
DDOS-LOIC-UDP
Benigh
DDOS-LOIC-UDP

Random Forest
100%
0%
1.0
1.0
1.0
1.0

Naive Bayes
100%
0%
1.0
0.66
1.0
0.80

M&M
100%
0%
1.0
1.0
1.0
1.0

Neural Network (MLP)
100%
0%
1.0
0.93
1.0
0.97

Quadratic Discriminante
100%
0%
1.0
0.66
1.0
0.80

KNeighbors
100%
0%
1.0
0.69
1.0
0.81

5. Conclusion and future research challenges

The implementation of an machine learning driven intrusion detection system depends entirely on the ability to
explicitly and suﬃciently interpret the machine learning models. However, current machine learning interpretation
methods in intrusion detection are heuristic, which could not garrantee the accracy or suﬃciency of the rules explained.
In this paper, we have proposed a rigorous rules extraction method for identifying DDoS traﬃc ﬂow from a decision
tree model with 100% accuracy. By discretizing the continuous model into boolean expression and calculating the
prime implicants out of it, the proposed map, combine, and merge method is able to provide suﬃcient and rigorous
explanations for DDoS detection. As the M&M method is based on formal logic calculation, the rules extracted have
exactly the same behavior with the model.

Although a rigorous XAI driven AIS have been proposed for the ﬁrst time in our humble knowledge, several

limitations remain and are challenging.

– First, the rule extraction process is built based on the hypothesis that features used are independent with each
other. However, it may not hold in real life. How to revise the features to make sure they are independent require
expert knowledge and experience and therefore more work.

– Second, the rigorous XAI technology depend on prime implicants, and the prime implicant calculation method-
ology, although being under heated discussion and more tools keep emerging, is not guaranteed to ﬁnish within
polynomial time and space complexity. The time and space expense highly depend on the boolean expression to
solve.

– Third, the extraction of rules from a well-performed machine learning model is not the end of story. For the
model keeps changing each time it is ﬁtted with new data, and the rules extracted from it are not the same every time. It
is a controversial to claim which one is the right one, for the machine learning algorithms can only recognize patterns
from data, but cannot tell which one is the correct one that will hold in the future (due to Hume’s law). Human experts
should be involved in to use their intelligent and experience to work out the ﬁnal rules for benign traﬃc detection.

– Finally, although in this paper, I explained decision tree model for it performs better than the other models in
detecting known intrusions according to previous work, it is possible that rigorous explanation for other models are
required in other scenarios. As the features in intrusion detection domain are usually continuous, the discretization,
boolean expression generation, and prime implicants calculation for other machine learning models (such as deep
learning, SVM, etc.) are much more challenging than decision tree, and much work still remain to be done.

To summarize, although a promising step towards rigorous XAI driven DDoS intrusion detection system has been
made, still much is to be done to make it practical and scalable, especially human experts’ eﬀort are required and
progress on SAT solver will also means signiﬁcantly towards this goal.

15

Table 10: Examples of Prime Implicant Explanations for Benign Flow Instance.

A:

146

B:

0.0

C:

D:

E:

30043443.7

90130331

9627.8

F:

4

G:

32

H:

I:

J:

K:

L:

M:

30051116

146

17500

30032640

30032640

90130331

N:

146

¯a1 ¯a2a3b1 ¯b2 ¯b3 ¯b4 ¯c1c2 ¯c3 ¯d1d2 ¯d3e1 ¯e2 ¯e3 f1 ¯f2 ¯f3g1 ¯g2 ¯h1h2 ¯h3i1 ¯i2 ¯j1 ¯j2 j3 ¯k1k2l1 ¯l2 ¯m1m2n1 ¯n2
0011000010010100100100101000101100110

001 − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − −10

¯a1 ¯a2a3n1 ¯n2

0.0

1

0.0

119999476

121

130.9

999705
a1 ¯a2 ¯a3 ¯b1b2 ¯b3 ¯b4c1 ¯c2 ¯c3 ¯d1 ¯d2d3e1 ¯e2 ¯e3 ¯f1 f2 ¯f3g1 ¯g2 ¯h1h2 ¯h3i1 ¯i2 j1 ¯j2 ¯j3 ¯k1k2l1 ¯l2 ¯m1m2n1 ¯n2
1000100100001100010100101010001100110

1000244

0.0

0.0

0

0.0

119999476

0.0

0

0.2

45007702.5

90808764

15355813.3

17

352

45015090

517

443

0

45000315

90803655

67.7

1000100 − − − 001 − − − 010 − − − − − − − − − − − − − − − − − −
a1 ¯a2 ¯a3 ¯b1b2 ¯b3 ¯b4 ¯d1 ¯d2d3 ¯f1 f2 ¯f3

¯a1 ¯a2a3 ¯b1b2 ¯b3 ¯b4 ¯c1c2 ¯c3 ¯d1d2 ¯d3 ¯e1e2 ¯e3 f1 ¯f2 ¯f3g1 ¯g2 ¯h1 ¯h2h3i1 ¯i2 ¯j1 j2 ¯j3k1 ¯k2l1 ¯l2 ¯m1m2n1 ¯n2
001010010010010100100011001010100110

001 − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − −10

¯a1 ¯a2a3n1 ¯n2

40

0.0

38448540.3

115345621.0

358398.1

4

32

38687191

40

1947

38036412

38036412

115345621

40

a1 ¯a2 ¯a3b1 ¯b2 ¯b3 ¯b4 ¯c1c2 ¯c3 ¯d1 ¯d2d3e1 ¯e2 ¯e3 f1 ¯f2 ¯f3g1 ¯g2 ¯h1h2 ¯h3i1 ¯i2 ¯j1 ¯j2 j3 ¯k1 ¯k2l1 ¯l2 ¯m1m2n1 ¯n2
1001000010010100100100101000110100110

1001000010 − − − − − − − − − − − − − − − − − − − − − − − − − −−
a1 ¯a2 ¯a3b1 ¯b2 ¯b3 ¯b4 ¯c1c2 ¯c3

50

2

0.0

1514340

7697.6

751727
¯a1a2 ¯a3 ¯b1b2 ¯b3 ¯b4c1 ¯c2 ¯c3 ¯d1d2 ¯d3e1 ¯e2 ¯e3 f1 ¯f2 ¯f3g1 ¯g2 ¯h1h2 ¯h3i1 ¯i2 j1 ¯j2 ¯j3 ¯k1k2l1 ¯l2 ¯m1m2n1 ¯n2
0100100100010100100100101010001100110

762613

137

3.0

50

24

0100100 − − − − − − − − − − − − − − − − − − − − − − − − − −01 − −
¯a1a2 ¯a3 ¯b1b2 ¯b3 ¯b4 ¯m1m2

0.0

1514340

50

1

PI τ41 :

2

PI τ36 :

3

PI τ41 :

4

PI τ1 :

5

PI τ39 :

6. Acknowledgments

The authors gratefully acknowledge the ﬁnancial supports from the National Natural Science Foundation of China

(No. 61973161, 61991404), Jiangsu Science and technology planning project (No. be2021610).

References

[1] M. T. Ribeiro, S. Singh, C. Guestrin, “why should i trust you?” explaining the predictions of any classiﬁer, in: 22nd ACM SIGKDD, 2016,

pp. 1135–1144.

[2] M. T. Ribeiro, S. Singh, C. Guestrin, Anchors: High-precision model-agnostic explanations, in: AAAI, Vol. 32, 2018.
[3] D. L. Marino, C. S. Wickramasinghe, M. Manic, An adversarial approach for explainable ai in intrusion detection systems, in: IEEE IECON,

IEEE, 2018, pp. 3237–3243.

[4] W. Guo, D. Mu, J. Xu, P. Su, G. Wang, X. Xing, Lemna: Explaining deep learning based security applications, in: ACM SIGSAC, 2018, pp.

364–379.

[5] B. Kim, R. Khanna, O. O. Koyejo, Examples are not enough, learn to criticize! criticism for interpretability, Advances in neural information

processing systems 29.

[6] U. Johansson, R. K¨onig, L. Niklasson, The truth is in there-rule extraction from opaque models using genetic programming., in: FLAIRS,

Miami Beach, FL, 2004, pp. 658–663.

[7] S. Mahdavifar, A. A. Ghorbani, Dennes: deep embedded neural network expert system for detecting cyber attacks, Neural Computing and

Applications 32 (18) (2020) 14753–14780.

[8] A. Rago, O. Cocarascu, F. Toni, Argumentation-based recommendations: Fantastic explanations and how to ﬁnd them, in: Proceedings of the

Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence, 2018, pp. 1949–1955.

[9] M. E. B. Brarda, L. H. Tamargo, A. J. Garc´ıa, Using argumentation to obtain and explain results in a decision support system, IEEE Intelligent

Systems 36 (2) (2020) 36–42.

[10] S. Grover, C. Pulice, G. I. Simari, V. Subrahmanian, Beef: Balanced english explanations of forecasts, IEEE Transactions on Computational

Social Systems 6 (2) (2019) 350–364.

[11] S. M. Lundberg, S. I. Lee, A uniﬁed approach to interpreting model predictions, in: The 31st NIPS, 2017, pp. 4768–4777.
[12] S. M. Lundberg, G. Erion, H. Chen, A. DeGrave, J. M. Prutkin, B. Nair, R. Katz, J. Himmelfarb, N. Bansal, S.-I. Lee, From local explanations

to global understanding with explainable ai for trees, Nature machine intelligence 2 (1) (2020) 56–67.

16

[13] P. Shakarian, G. I. Simari, G. Moores, D. Paulo, S. Parsons, M. A. Falappa, A. Aleali, Belief revision in structured probabilistic argumentation,

Annals of Mathematics and Artiﬁcial Intelligence 78 (3) (2016) 259–301.

[14] A. Darwiche, A. Hirth, On the reasons behind decisions, arXiv preprint arXiv:2002.09284.
[15] A. Ignatiev, N. Narodytska, J. Marques-Silva, Abduction-based explanations for machine learning models, in: AAAI, Vol. 33, 2019, pp.

1511–1519.

[16] D. Gunning, D. Aha, Darpa’s explainable artiﬁcial intelligence (xai) program, AI Magazine 40 (2) (2019) 44–58.
[17] G. Audemard, S. Bellart, L. Bounia, F. Koriche, J. Lagniez, P. Marquis, On the explanatory power of decision trees, CoRR abs/2108.05266.
[18] V. Mathur, Google autonomous car experiences another crash, Government Technology 17.
[19] V. A. Banks, K. L. Plant, N. A. Stanton, Driver error or designer error: Using the perceptual cycle model to explore the circumstances

surrounding the fatal tesla crash on 7th may 2016, Safety science 108 (2018) 278–285.

[20] P. Domingos, The master algorithm: How the quest for the ultimate learning machine will remake our world, Basic Books, 2015.
[21] L. Edwards, M. Veale, Slave to the algorithm: Why a right to an explanation is probably not the remedy you are looking for, Duke L. & Tech.

Rev. 16 (2017) 18.

[22] A. Deeks, The judicial demand for explainable artiﬁcial intelligence, Columbia Law Review 119 (7) (2019) 1829–1850.
[23] A. Ignatiev, Towards trustable explainable ai., in: IJCAI, 2020, pp. 5154–5158.
[24] A. B. Arrieta, N. D´ıaz-Rodr´ıguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, S. Garc´ıa, S. Gil-L´opez, D. Molina, R. Benjamins, et al.,
Explainable artiﬁcial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai, Information Fusion 58
(2020) 82–115.

[25] Z. C. Lipton, The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery., Queue

16 (3) (2018) 31–57.

[26] Q. Zhao, T. Hastie, Causal interpretations of black-box models, Journal of Business & Economic Statistics 39 (1) (2021) 272–281.
[27] D. W. Apley, J. Zhu, Visualizing the eﬀects of predictor variables in black box supervised learning models, Journal of the Royal Statistical

Society: Series B (Statistical Methodology) 82 (4) (2020) 1059–1086.

[28] L. Vigan`o, D. Magazzeni, Explainable security, IEEE EuroS&PW (2020) 293–300.
[29] J. Vadillo, R. Santana, J. A. Lozano, When and how to fool explainable models (and humans) with adversarial examples, arXiv preprint

2107.01943.

[30] R. K. Muna, H. T. Maliha, M. Hasan, Demystifying machine learning models for iot attack detection with explainable ai, Ph.D. thesis, Brac

University (2021).

[31] M. Melis, D. Maiorca, B. Biggio, G. Giacinto, F. Roli, Explaining black-box android malware detection, in: 26th EUSIPCO, 2018, pp.

524–528.

[32] B. Mahbooba, M. Timilsina, R. Sahal, M. Serrano, Explainable artiﬁcial intelligence (xai) to enhance trust management in intrusion detection

systems using decision tree model, Complexity 2021.

[33] K. Grosse, P. Manoharan, N. Papernot, M. Backes, P. McDaniel, On the (statistical) detection of adversarial examples, arXiv1702.06280.
[34] J. N. Paredes, J. C. L. Teze, G. I. Simari, M. V. Martinez, On the importance of domain-speciﬁc explanations in ai-based cybersecurity

systems (technical report), arXiv preprint arXiv:2108.02006.

[35] T. Bao, J. Burket, M. Woo, R. Turner, D. Brumley, Byteweight: Learning to recognize functions in binary code, in: 23rd USENIX Security

Symposium, 2014, pp. 845–860.

[36] W. V. Quine, The problem of simplifying truth functions, The American mathematical monthly 59 (8) (1952) 521–531.
[37] Q. Zhou, D. Pezaros, Evaluation of machine learning classiﬁers for zero-day intrusion detection–an analysis on cic-aws-2018 dataset, arXiv

preprint arXiv:1905.03685.

17

