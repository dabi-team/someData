1
2
0
2

n
a
J

5
1

]

R
C
.
s
c
[

3
v
9
1
9
7
0
.
4
0
0
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

1

A Framework for Enhancing Deep Neural
Networks Against Adversarial Malware

Deqiang Li, Qianmu Li, Yanfang Ye, and Shouhuai Xu

Abstractâ€”Machine learning-based malware detection is known to be vulnerable to adversarial evasion attacks. The state-of-the-art is
that there are no effective defenses against these attacks. As a response to the adversarial malware classiï¬cation challenge organized
by the MIT Lincoln Lab and associated with the AAAI-19 Workshop on Artiï¬cial Intelligence for Cyber Security (AICSâ€™2019), we
propose six guiding principles to enhance the robustness of deep neural networks. Some of these principles have been scattered in the
literature, but the others are introduced in this paper for the ï¬rst time. Under the guidance of these six principles, we propose a defense
framework to enhance the robustness of deep neural networks against adversarial malware evasion attacks. By conducting
experiments with the Drebin Android malware dataset, we show that the framework can achieve a 98.49% accuracy (on average)
against grey-box attacks, where the attacker knows some information about the defense and the defender knows some information
about the attack, and an 89.14% accuracy (on average) against the more capable white-box attacks, where the attacker knows
everything about the defense and the defender knows some information about the attack. The framework wins the AICSâ€™2019
challenge by achieving a 76.02% accuracy, where neither the attacker (i.e., the challenge organizer) knows the framework or defense
nor we (the defender) know the attacks. This gap highlights the importance of knowing about the attack.

Index Termsâ€”Adversarial Machine Learning, Deep Neural Networks, Malware Classiï¬cation, Adversarial Malware Detection.

(cid:70)

1 INTRODUCTION

communitiesâ€™

M ALWARE remains a big threat to cyber security despite

tremendous countermeasure efforts.
For example, Symantec [2] reports seeing 357,019,453 new
malware variants in the year 2016, 669,974,865 in the year
2017, and 246,002,762 in the year 2018. Worse yet, there is
an increasing number of malware variants that attempted
to undermine anti-virus tools and indeed evaded many
malware detection systems [3].

In order to cope with the increasingly severe situation,
we have to resort to machine learning for automating the
detection of malware in the wild [4]. However, machine
learning based techniques are vulnerable to adversarial
evasion attacks, by which an adaptive attacker perturbs or
manipulates malware examples into adversarial examples
that would be detected as benign rather than malicious
(see, for example, [5], [6], [7], [8], [9], [10], [11]). The
state-of-the-art
the
problem of effective defense is largely open. For example,
adversarial training is known to be able to harden classiï¬ers
against adversarial examples, but requires knowing about
the attack in terms of (for example) its manipulation set
[7]. This is indeed the context in which the AICSâ€™2019
Malware Classiï¬cation Challenge is proposed. In a broader

there are many attacks, but

is that

A preliminary version of the paper was presented at AICSâ€™2019, which does
not publish any formal proceedings [1].

â€¢ D. Li is with School of Computer Science and Engineering, Nanjing

University of Science and Technology.

â€¢ Q. Li is with School of Computer Science and Engineering, Nanjing
Intelligent

University of Science and Technology and School of
Manufacturing, Wuyi University

â€¢ Y. Ye is with Department of Computer and Data Sciences, Case Western

â€¢

Reserve University.
S. Xu is with Department of Computer Science, University of Colorado
Colorado Springs. This work was done when he was at University of Texas
at San Antonio, USA. E-mail: sxu@uccs.edu

context, adversarial malware examples are a particular kind
of attacks against adversarial machine learning. Although
adversarial machine learning has received much attention
in application domains such as image processing (see,
e.g., [12], [13], [14]), the problem of adversarial malware
examples are much less investigated [5], [7], [9], [15].

The AICSâ€™2019 challenge mentioned above is essentially
about whether we can defend against adversarial examples in
the wild. The challenge is characterized as follows. First,
we (i.e., any team participating in the Challenge as the
defender) are given a training set in the form of anonymized
feature representation by the Challenge organizer (i.e., we
do not even know what the feature names are), as well
as the corresponding ground-truth labels. We are informed
by the Challenge organizer that the training data contains
no adversarial examples. Second, we are given a set of
test data (again,
in anonymized feature representation)
and are told that the test data contains both adversarial
examples and non-adversarial examples. We do not know
what attacks are used by the Challenge organizers. We do
not know which examples in the test set are perturbed
by adversaries. This means that we neither know which
are adversarial examples, nor the attacks that are used
to generate them, nor the manipulation set. Third, our
task is to accurately classify the test data, including both
adversarial and non-adversarial examples. The setting of the
Challenge is realistic because in the real world defenders do
not know attackerâ€™s speciï¬cations such as attack methods,
manipulation sets, and speciï¬c adversarial examples. The
importance of the problem in defending against adversarial
malware examples and the realistic setting of the Challenge
motivate the present study.

2327-4697 Â© 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.

 
 
 
 
 
 
IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

2

1.1 Our Contributions

In this paper, we make the following contributions. First, we
propose, to the best of our knowledge, the ï¬rst systematic
defense framework to enhance the robustness of Deep
Neural Network (DNN)-based malware classiï¬ers against
adversarial evasion attacks. The framework is designed
under the guidance of a set of principles, some of which
are known but scattered in the literature (e.g., using an
ensemble of classiï¬ers and minmax adversarial training), but
others are introduced in this paper for the ï¬rst time, such
as the following. We propose (i) using white-box attack,
where the attacker knows everything about the defense,
to bound the capability of grey-box attacks with respect to
the (cid:96)p (p â‰¥ 1) norm, where the attacker knows something
about the defense; (ii) using adversarial regularization (i.e.,
adversarial training with small perturbations) when the
manipulation set is not available to the defender; (iii)
leveraging semantics-preserving representations (realized
by Denoising Auto-Encoder or DAE for shorthand).

Second, we empirically validate the effectiveness of
the framework against 11 grey-box attacks and 9 white-
box attacks (i.e., 20 attacks in total). The 11 grey-box
attacks include the Random attack, two Mimicry attacks
[16], the Fast Gradient Sign Method (FGSM) attack [13],
Grosse attack [5], Bit Gradient Ascent (BGA) attack [7],
Bit Gradient Ascent (BCA) attack [7], and four variants
of the Projected Gradient Descent (PGD) attacks. The 9
white-box attacks leverage the victim models directly and
the attack algorithms are the same as the latter 9 ones
mentioned above. Among these attacks, the four variants
of the PGD attacks are used to be investigated in other
application settings and are adapted to the adversarial
malware detection domain for the ï¬rst time. The variant
PGD attacks permit feature addition and feature removal,
incurring larger manipulation sets than the Grosse, BGA,
and BCA attacks. In these experiments, adversarial malware
examples are generated by manipulating regular malware
examples while preserving their malicious functionalities.
Our empirical ï¬ndings include: (i) standard DNNs without
incorporating defense can be ruined by both grey-box and
white-box attacks; (ii) adversarial regularization without
considering attacks in the training phase has limited success
in terms of improving the robustness of DNNs against
adversarial examples; (iii) adversarial training with the
Adam optimizer can signiï¬cantly enhance the robustness
of DNNs against multiple grey-box evasion attacks, but
not the more capable white-box Grosse, BCA and PGD-
(cid:96)1 attacks; (iv) DAE provides a degree of extra robustness
when used together with adversarial training, which is
ineffective in defending against the white-box Grosse, BCA
and PGD-(cid:96)1 attacks; (v) adding ensembles further improves
the robustness of DNNs, at the price of sacriï¬cing a degree
of the effectiveness of adversarial training against the white-
box PGD-(cid:96)2, PGD-(cid:96)âˆž and PGD-Adam attacks.

Third, we apply the framework to the AICSâ€™2019
adversarial malware classiï¬cation challenge organized by
the MIT Lincoln Lab. According to the Challenge organizers,
there were â€œover 300 participants attempted to download
and classify the malware data setâ€ [17] and we win the
Challenge by achieving a 73.60% Harmonic mean score

(which is the metric the organizer chose to use before
making the data available); i.e., we achieve the highest score
among all of the participating teams.

Fourth, after announcing that we win the Challenge, the
organizer makes the ground-truth labels publicly available
at http://www-personal.umich.edu/âˆ¼arunesh/AICS2019/
challenge.html.
In order to understand why we only
achieve a 73.60% Harmonic mean score, we leverage the
ground-truth labels to conduct a further study. We show
that (i) oversampling beneï¬ts adversarial regularization in
defending against evasion attacks in term of the Macro-F1
score and (ii) adversarial regularization tends to overï¬t the
perturbed examples while this phenomenon does not occur
to the non-adversarial (i.e., original) data.

Fifth, we show that the framework is effective in resisting
grey-box attacks via the widely-used Drebin Android
malware dataset (with a 98.49% accuracy on average), where
the attacker knows some information about the defense
and the defender knows some information about the attack.
When applied to the AICSâ€™2019 challenge dataset but only
considering the adversarial examples (for the sake of fair
comparison with the experiment on the Drebin dataset), the
framework only achieves a 76.02% accuracy on average,
where it is still true that neither the attacker knows the
defense nor the defender knows the attacks. This highlights
that the defender should always strive to know as much
information as possible about
the attacks. In order to
avoid any confusion, we reiterate that the aforementioned
experiment result (i.e., 73.60% in the Harmonic mean score)
considers both adversarial and non-adversarial examples
(as required by the challenge organizer); whereas the 76.02%
accuracy disregards of the non-adversarial examples (for
fair comparison with the experiment with the Drebin
dataset). Another difference is that in the new experiment
achieving a 76.02% accuracy we use an ensemble of 5
building-block models, whereas in the experiment achieving
73.60% Harmonic mean score we use 10 building-block
models.

Last but not

the least, we made our the code of
our models publicly available at https://github.com/
deqangss/aics2019 challenge adv mal defense.

1.2 Related Work

Since the present paper
focuses on defense against
adversarial malware examples, we review related prior
input
studies
prepossessing, adversarial training/regularization, and DAE-
based representation learning.

categories:

in four

learning,

ensemble

Ensemble learning can reduce the generalization error
by diversifying the building-block models. Biggio et al.
[19] show how the bagging and random subspace
[18],
techniques can enhance the robustness of linear models
against evasion attacks. Smutz and Stavrou [20] propose
using the conï¬dence score produced by random forest
classiï¬ers to detect adversarial malware. Stokes et al. [21]
investigate the resilience of ensemble DNNs against evasion
attacks.
In this paper, we diversify the building-block
models via randomly initialized parameters and the random
subspace algorithm.

Input prepossessing transforms the input to a different
representation, aiming to reduce the degree of perturbations

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

3

applied to the original input. For example, Random Feature
Nulliï¬cation (RFN) randomly nulliï¬es features both in the
training and test phases [22]; HashTran [23] reduces small
perturbations using a locality-sensitive hashing; DroidEye
[24] quantizes binary representation via count featurization.
In our framework, inspired by the idea of feature squeezing
[25], we use binarization to reduce the perturbations.

Adversarial training augments the training data with
adversarial examples. Various kinds of heuristic training
strategies have been proposed (see, e.g., [5], [12], [13], [26],
[27]. However, these strategies typically deal with speciï¬c
evasion methods and are not effective against others.
Furthermore, researchers propose considering adversarial
training with the optimal attack, which in a sense
corresponds to the worst-case scenario and therefore could
lead to classiï¬ers that are robust against the non-optimal
attacks [7], [28]. In our framework, we seek the optimal
attack via a gradient descent method, while meeting the
requirement of discrete inputs via a nearest neighbor search.
training
method that aims to train a model with slightly perturbed
examples, which may or may not be functionality-
preserving.
the
generalization of DNN models [13], [27], [29], [30], [31]. This
approach may be useful because in the context of malware
detection, the defender may not know the manipulation set
of the attacker.

Adversarial regularization is an adversarial

Intuitively, small perturbations beneï¬t

DAE facilitates robust representation learning [32],
[33]. Li et al. [23] propose detecting adversarial malware
examples using DAE. In our framework, we use DAE
to learn the robust representation that is insensitive to
perturbations.

1.3 Paper Outline

The rest of the paper is organized as follows. Section 2
presents the adversarial malware evasion attacks, including
four attacks that are adapted to the domain of adversarial
malware detection for the ï¬rst time. Section 3 describes
our defense framework. Section 4 validates our defense
framework with a real-world dataset. Section 5 presents
the results when applying the framework to the AICSâ€™2019
Challenge without knowing anything about
the attack.
Section 6 presents our further study after winning the
AICSâ€™2019 Challenge and being given the ground-truth
labels of the test data. Section 7 concludes the present paper.

arg max returns the index of the maximum element in a o-
dimension vector. Let L : Ro Ã— Y â†’ R be a loss function.
The parameters of F, denoted by Î¸, are optimized via

E(x,y)âˆˆX Ã—Y [L(F(x), y)] .

min
Î¸

(1)

Speciï¬cally, the cross-entropy is leveraged L(F(x), y) =
âˆ’1(cid:62)
y log(F(x)), where 1y is the one-hot encoding vector for
the label y. For simplifying notations, we use F (rather than
FÎ¸) to denote a neural network. Table 1 summarizes the
main notations used in the paper.

TABLE 1: Main notations used in the paper

Notation
z âˆˆ Z
(x, y) âˆˆ X Ã— Y

Î´x âˆˆ Mx

x(cid:48) âˆˆ S(x, Mx)

o
dim
f : X â†’ Y
F : X â†’ Ro
Î¸
L : Ro Ã— Y â†’ R

Meaning
z is a software example; Z is the example space
x is feature representation of z; X is the feature
space; y is the label of x; Y is the label space
Î´x is a perturbation vector of x; Mx is the
manipulation set of x
x(cid:48) is perturbed from x; S is the set of perturbed
representations derived from x and Mx; S âŠ† X
o is the number of classes
dim is the number of dimension of x
f is the classiï¬er
F denotes a neural network
Î¸ denotes parameters of neural network F
L is cross-entropy loss function

2.2 Basic Idea
With regard to the feature space X , given the representation-
label pair (x, y), the adversarial evasion attack attempts to
perturb x into x(cid:48), such that

f (x(cid:48)) (cid:54)= y s.t. x(cid:48) âˆˆ S(x, Mx)

(2)

where S(x, Mx) is the set of perturbed representations
derived from the non-adversarial feature representation x
and a manipulation set Mx (i.e. the set of manipulations
that can preserve the malicious functionality of malware
examples). The perturbation vector is denoted by Î´x = x(cid:48) âˆ’ x
with Î´x âˆˆ Mx. Since the manipulation is conducted in
the feature space, the attacker needs to map x(cid:48) back into
the example space Z in order to obtain an executable
adversarial malware example z(cid:48) âˆˆ Z. This is a requirement
that distinguishes adversarial malware detection from
adversarial machine learning in other application domains,
which induces the problem of generating adversarial
examples in the discrete space. It is worth mentioning that
an attacker tends to modify malware examples by exploiting
one or multiple feasible manipulations [5], [10], [16].

2 ADVERSARIAL MALWARE EVASION ATTACKS

2.3 Threat Model

2.1 Notations

Given a non-adversarial malware example z âˆˆ Z, its feature
representation x âˆˆ X can be obtained via some feature
extraction methods, where Z denotes the example space (i.e.,
the set of all possible software examples) and X denotes the
feature space. A classiï¬er f : X â†’ Y takes x as input and
outputs its label y âˆˆ Y, where Y is the label space.

We focus on a classiï¬er f that is learned as a neural
network model F : X â†’ Ro, whose output (softmax)
is treated as the probability mass function over o = |Y|
classes [1], [5], [7], [34]. That is f = arg maxjâˆˆY F, where

The threat model against malware classiï¬ers and detectors
can be speciï¬ed by what the attacker knows, what the attacker
can do, and how the attacker wages the attack.

2.3.1 What the attacker knows

There are three kinds of models from this perspective. A
black-box attacker knows nothing about classiï¬er f except
what is implied by f â€™s responses to the attackerâ€™s queries.
A white-box attacker knows all kinds of information about
f ,
including its learning algorithms, model parameters,
defenses strategies, etc. A grey-box attacker knows an

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

4

amount of information about f that resides in between
the preceding two extremes. For example, the attacker may
know the feature set.

2.3.2 What the attacker can do

In evasion attack,
the attacker only can manipulate
malware examples in the test set using various kinds
of manipulations, while obeying some constraints. One
constraint is to preserve the malicious functionality of
malware. A simplifying assumption is to consider insertion
only (e.g., ï¬‚ipping a feature value from â€˜0â€™ to â€˜1â€™ [5], [7],
[9], [22], [35], [36], [37], [38], while noting that attackers can
manipulate a malware example by inserting and deleting
operations [39], [40]. Since a larger manipulation set gives
the attacker more freedom, we permit
the attacker to
conduct both feature addition and feature removal. The other
constraint is to maintain the relation between features.
Using the AICSâ€™2019 malware classiï¬cation challenge as an
example, we note that n-gram (uni-gram, bi-gram, and tri-
gram) features reï¬‚ect sequences of Windows system API
calls. This means that when the attacker inserts an API call
into a malware example, several features related to this API
call will need to be changed according to the deï¬nition of
n-gram features.

2.3.3 How the attacker wages the attack

Researchers generate adversarial malware examples using
various machine learning-based techniques such as genetic
learning, and generative net-
algorithms, reinforcement
works [26], [40], [41], [42]. In order to generate adversarial
malware examples effectively and efï¬ciently, attackers often
exploit the gradient-based methods [13], [35], [36], [43], [44].
We here brieï¬‚y describe multiple types of attacks, some of
which were introduced in the context of malware detection
but the others were introduced in the context of image
classiï¬cation and then adapted to the context of malware
detection.

Random Attack. We introduce this method as a baseline
attack in the adversarial malware detection domain. In this
attack, the attacker randomly modiï¬es a feature at each
iteration until a predeï¬ned step is reached or no more
features can be manipulated.

Mimicry Attack. This attack was introduced in [16], [35],
[36], [45] for studying adversarial malware detection. In
this attack, the attacker perturbs or manipulates a malware
example such that the resulting adversarial version mimics
a chosen benign example as much as possible. In order to
reduce the degree of perturbations, the attacker may select
the benign example to be close to the malware example that
is to be modiï¬ed.

FGSM Attack. This attack was introduced in the context of
image classiï¬cation [13] and then adapted to the malware
detection [7], [24]. FGSM perturbs a feature vector x in the
direction of the (cid:96)âˆž norm of the gradients of the loss function
with respect to the input, namely:

x(cid:48) = ProjS (x + Îµ Â· sign((cid:79)xL(F(x), y))) ,
where Îµ > 0 is a scalar known as step size, (cid:79)x indicates
the derivative of the loss function L(F(x), y) with respect

to x, and ProjS (Â·) projects an input into S that denotes the
shorthand of S(x, Mx).

Grosse Attack. This attack was introduced by Grosse et al.
[5] in the context of malware detection. The attack considers
sensitive features, namely the features have large positive
gradients as far as the softmax output is concerned. The
attack is to manipulate the absence of a feature (e.g., not
making a certain API call) into the presence of the feature
(i.e., making the API call), while preserving their malicious
functionalities. These sensitive features can be identiï¬ed by
leveraging the gradients of the softmax output of a malware
example with respect to the input.

BGA Attack and BCA Attack. In the context of malware
detection, Al-Dujaili et al.
[7] proposed two separate
methods, dubbed BGA and BCA, aiming to solve:

max
x(cid:48)âˆˆS(x,Mx)

L(F(x(cid:48)), y).

(3)

In addition, the authors considered malware examples in
the binary space and restricted Mx to API calls addition.
Both attack methods iterate multiple steps. In each iteration,
BGA increases the feature value from â€˜0â€™ to â€˜1â€™
if the
corresponding partial derivative of the loss function with
respect
to the
to the input
gradientâ€™s (cid:96)2 norm divided by
dim, where dim is the input
dimension. In contrast, BCA ï¬‚ips â€˜0â€™ to â€˜1â€™ for a component
at the iteration corresponding to the maximum gradient of
the loss function with respect to the input.

is greater than or equal

âˆš

PGD Attack. We adapt the PGD method to the context
of malware detection, by accommodating discrete input
spaces. In contrast to the Grosse, BGA, and BCA attacks,
the adapted PGD attacks permit both feature addition and
feature removal. Speciï¬cally, PGD ï¬nds perturbations via an
iterative procedure

(cid:0)Î´i

x), y)(cid:1) ,

Î´i+1
x = Proj Ë†Mx

x + Î± Â· (cid:79)Î´xL(F(x + Î´i

(4)
where Î± > 0 is the step size, (cid:79)Î´x is the derivative
of the loss function L(F(x + Î´i
x), y) with respect to Î´x,
and Proj Ë†Mx projects perturbations into a predetermined
space Ë†Mx. We set Ë†Mx = [u, u] for u = minimum(Mx)
and u = maximum(Mx), where minimum returns the
component-wise minimum vector (i.e., each component
of
the
the vector corresponding to the minimum of
corresponding component values of the vectors in Mx) and
maximum returns the component-wise maximum vector.

When solving Eq.(4), we encounter two issues that need
to be addressed: (i) small derivatives of g = (cid:79)Î´xL and (ii)
mapping perturbations into discrete space Mx. To see issue
(i), we note that by writing F as F(x) = softmax(Z(x)),
we have âˆ‚L/âˆ‚Î´x = (F âˆ’ 1y) Â· âˆ‚Z/âˆ‚Î´x, meaning that the
derivatives approach zero when F predicts x as y with high
conï¬dence. To cope with this, researchers [28], [46] have
proposed to â€œnormalizeâ€ the derivatives using (cid:96)p-norm and
leveraging the steepest direction as follows:

â€¢

For p = 1, the direction is sign(gi)1i, where i is the
index corresponding to the maximum absolute value
of g = (g1, . . . , gdim) with dim being the number of
input dimension, 1i has the same dimension as g and

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

5

has value 1 at the ith component and value 0 at the
other components, and sign returns 1 when the input
> 0, âˆ’1 when the input < 0, and 0 when the input
= 0.
For p = 2, the direction is g/ (cid:107) g (cid:107)2.
For p = âˆž, the direction is sign of gradients, i.e.,
sign(g).

â€¢

â€¢

We call these variant PGD attacks PGD-(cid:96)1, PGD-(cid:96)2 and
PGD-(cid:96)âˆž, respectively. Note that PGD-(cid:96)1 degrades to the
BCA attack when only feature addition is permitted. In
addition to these (cid:96)p-norm based attacks, we observe that
the attacker can use the Adam optimizer to accelerate the
process of gradient descent (the â€œnormalizedâ€ gradients are
approximate to Â±1) [47], leading to a new variant of the
PGD attack, which we call PGD-Adam.

Algorithm 1: PGD attack in the feature space.

Input: The feature representation-label pair (x, y),

manipulation set Mx, number of iterations T ,
step size Î±

Output: Perturbed example x(cid:48)
1 Initialize perturbation vector Î´0
2 Derive the continuous space Ë†Mx and the perturbed

x = 0;

representation set S;
3 for i = 0 to T âˆ’ 1 do
4

Obtain the derivatives (cid:79)Î´xL and normalize them
using (cid:96)p-norm where p = 1, 2, âˆž or the Adam
method;
Calculate Î´i+1

via the Eq.(4);

x

5
6 end
7 Obtain x(cid:48) by mapping Ëœx(cid:48) = x + Î´T
8 return x(cid:48).

x via Eq.(5);

To address the issue (ii), we introduce a mapping method
to consider two settings as follows. In order to follow the
direction of â€œnormalizedâ€ gradients, let the perturbation Î´x
be continuous during the optimization process. We map
the perturbed representation obtained at the last iteration,
denoted by Ëœx(cid:48) = (Ëœx(cid:48)
dim), into S by selecting its
closest neighbor x(cid:48) = (x(cid:48)

1, . . . , Ëœx(cid:48)

dim) such that

1, . . . , x(cid:48)

x(cid:48) = arg min

x(cid:48)âˆˆS

(cid:107) x(cid:48) âˆ’ Ëœx(cid:48) (cid:107)1= arg min

x(cid:48)âˆˆS

dim
(cid:88)

i=1

|x(cid:48)

i âˆ’ Ëœx(cid:48)
i|.

(5)

Geometrically speaking, Eq.(5) says that
dimension, x(cid:48)
1 summarizes the PGD attacks in the feature space.

i is the feasible scalar closest to Ëœx(cid:48)

the ith
for
i. Algorithm

3.1.1 Principle 1: Knowing the enemy

This principle says that
the defender should strive to
extract useful information about the attacks as much as
possible as the information will offer insights on designing
countermeasures. Threat models are a standard approach to
modeling attacks. Moreover, it is possible to design some
indicators of adversarial examples. On the other hand, the
attack method and manipulation set may not be known
to the defender. This means that whenever possible, the
defender has to simulate them.

3.1.2 Principle 2: Bridging grey-box vs. white-box gap

In grey-box attacks, the attacker knows some information
about the feature set and therefore can train a surrogate
classiï¬er Ë†f : X â†’ Y from a training set (where the
realization of Ë†f is a neural network Ë†F), leveraging the
transferability from Ë†f to f to generate adversarial examples.
Consider an input x for which a grey-box attacker generates
perturbations using

Ë†Î´x =

arg max
||Î´x||â‰¤Î· âˆ§ Î´xâˆˆMx

L(Ë†F(x + Î´x), y),

where Î· is an upper bound and possibly large. Based on
the degree of perturbations, we consider two cases: (i) Î· is
small and (ii) Î· is large. We further assume that the optimal
perturbation vector Î´x of F exists.

In case (i) or when Î· is small, the change to the loss of f

incurred by Ë†Î´x is

|âˆ†L| =

â‰ˆ

(cid:12)
(cid:12)
(cid:12)L(F(x + Ë†Î´x), y) âˆ’ L(F(x), y)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:79)L(F(x), y)(cid:62)Ë†Î´x
(cid:79)L(F(x), y)(cid:62)Î´
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) â‰¤ max

||Î´||â‰¤Î·

(cid:12)
(cid:12)
(cid:12)

= Î·||(cid:79)L||âˆ—,

where the approximation is derived using the ï¬rst-order
Taylor expansion at point x, (cid:79) is the operator for computing
partial derivatives of the loss function with respect to the
input of neural network F, and â€œ|| Â· ||âˆ—â€ is the dual norm of
|| Â· ||.

In case (ii) or when Î· is large, we derive

|âˆ†L| =

=

=

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)L(F(x + Ë†Î´x), y) âˆ’ L(F(x), y)
(cid:12)
(cid:12)
(cid:90) Ë†Î´x
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:79)L(F(x + tË†Î´x), y)(cid:62)Ë†Î´xdt

(cid:79)L(F(x + Î´), y)dÎ´

0
(cid:90) 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0
â‰¤ Î· sup
||Î´||â‰¤Î·

(cid:107)(cid:79)L(F(x + Î´), y)(cid:107)âˆ— .

3 ADVERSARIAL MALWARE DEFENSE

3.1 Guiding Principles

The preceding observation shows that corresponding to the
same perturbation upper bound Î·, the loss incurred by grey-
box attacks is upper bounded by the loss incurred by white-
box attacks. This suggests us to focus on the robustness of
classiï¬er f against the optimal white-box attack.

These principles are geared to neural network classiï¬ers,
which are chosen as our focus because deep learning
techniques are increasingly employed in the domain of
malware detection/classiï¬cation, but their vulnerability to
adversarial evasion attack has yet to be tackled [48].

3.1.3 Principle 3: Not putting all eggs in one basket

This is suggested by the observation that no single classiï¬er
may be effective against all kinds of attacks. An ensemble
can be built by many methods (e.g., bagging, boosting,
or stacking) [49]. For example, random subspace [50] is

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

6

seemingly particularly suitable for formulating malware
classiï¬er ensembles owing to the high dimensional feature
vector of malware, which indicates a high vulnerability of
classiï¬ers to adversarial malware examples [16], [51].

Formally, an ensemble fen : X â†’ Y contains a set
i=1, namely {Fi : X â†’ Ro} for
of neural networks {Fi}l
1 â‰¤ i â‰¤ l. Given a test example x, we treat the base model
equally, as suggested by the study [18], [52], and the voting
method is

Fen(x) =

1
l

l
(cid:88)

i=1

Fi(x).

We obtain the predicted label by fen = arg maxjâˆˆY Fen.

3.1.4 Principle 4: Using transformation against perturbation

In typical applications, the defender does not know what
kinds of evasion attacks are waged by the attacker. These
attacks can produce a spectrum of perturbations, from
manipulating a few features (e.g., the PGD-(cid:96)1 attack) to
manipulating a large number of features (e.g., the FGSM
attack). Moreover, we may give higher weights to the
transformation techniques that can simultaneously reduce
the degrees of multiply types of perturbations such as (cid:96)âˆž
norm, (cid:96)1 norm, or (cid:96)2 norm. This suggests us to use the
binarization technique [25], [53]: When the feature value of
the ith feature, denoted by xi, is smaller than a threshold
Î˜i, we binarize xi to 0; otherwise, we binarize xi to 1.

3.1.5 Principle 5: Using vaccine

We harden a model
adversarial training:

incorporating the known minmax

min
Î¸

E(x,y)âˆˆX Ã—Y

(cid:20)

L(F(x), y) + max
x(cid:48)âˆˆS

(cid:21)
L(F(x(cid:48)), y)

.

(6)

[7]

Al-Dujaili et al.
instantiate this method by using
attacks with feature addition solely (e.g., BGA). In order to
accommodate more manipulations, we solve the problem
of inner maximization using the Adam optimizer (see
Section 2.3.3). Given the issue of local minima, we run the
inner maximizer several times, each with a random initial
point near the training data, and then select the point that
maximizes the loss function of L.

It is worth mentioning that in the AICSâ€™2019 Challenge,
the defender does not know the manipulation set Mx and
thus cannot derive S. In this case, we propose training
malware classiï¬ers by applying small perturbations to
the feature representations of malware examples (without
necessarily preserving their malicious functionalities). This
would beneï¬t model generalization [13], [29]. Let a norm
|| Â· || measure the perturbation Î´x with Î· bounded.
We have max||Î´x||â‰¤Î· L(F(x + Î´x), y) â‰ˆ L(F(x), y) +
Î·||(cid:79)L(F(x), y)||âˆ—, leading to |L(F(x(cid:48)), y) âˆ’ L(F(x), y)| â‰¤
Î·||(cid:79)L(F(x), y)||âˆ—. Therefore, adversarial
regularization
assures
small perturbations do not change the
that
prediction signiï¬cantly.

3.1.6 Principle 6: Preserving semantics

This suggests us to strive to learn neural network models
that are sensitive to malware semantics, but not
the
perturbations because adversarial examples must retain the
malicious functionality of original malware. Speciï¬cally, we

propose using denoising auto-encoder to learn semantics-
preserving representations, rendering neural network less
sensitive to perturbations. A DAE ae = d â—¦ e uniï¬es two
components: an encoder e : X â†’ H that maps an input
M (x) to a latent representation r âˆˆ H and a decoder
d : H â†’ X that reconstructs x from r, where the H
is the latent representation space and M refers to some
operations applied to x (e.g., adding Gaussian noises to
x). Vincent et al. [32] showed that the lower bound of the
mutual information between x and r is maximized when the
reconstruction error is minimized. In the case of Gaussian
noise (cid:15) âˆ¼ N (0, Ïƒ2) and reconstruction loss
E(cid:15)âˆ¼N (0,Ïƒ2) (cid:107)ae(x + (cid:15)) âˆ’ x)(cid:107)2
2 ,
Alain and Bengio [54] showed that the optimal aeâˆ—(x) is

(7)

aeâˆ—(x) =

E(cid:15) [p(x âˆ’ (cid:15))(x âˆ’ (cid:15))]
E(cid:15) [p(x âˆ’ (cid:15))]

,

(8)

where p(Â·) is the probability density function. Eq.(8) says
that representations of a well-trained DAE are insensitive to
x because of the weighted average from the neighborhood
of x, which is reminiscent of the attention mechanism [55].
This means that DAE can handle certain types of small
perturbations. To learn a DAE model, we leverage two
kinds of noise: (i) Salt-and-pepper noise (cid:15): A small fraction
of elements of original example x are randomly selected,
and then set their values as their respective minimum or
maximum. (ii) Adversarial perturbation Î´x: A perturbation
Î´x is added to x such that classiï¬er f or base classiï¬er fi
misclassiï¬es x(cid:48) = x + Î´x. Given a training example x over
the feature space X , the risk of a denoising auto-encoder is

ExâˆˆX [Lae(x, ae(x + (cid:15))) + Lae(x, ae(x(cid:48)))] ,

(9)

min
ËœÎ¸,Î¾

where Lae : X Ã— X (cid:55)â†’ R calculates the mean-square error,
the learnable parameters ËœÎ¸ and Î¾ respectively belongs to the
encoder and decoder.

3.2 Turning Principles into A Framework

The principles discussed above guide us to propose
a framework for adversarial malware classiï¬cation and
detection, which is highlighted in Figure 1 and elaborated
below. Speciï¬cally, we ï¬rst examine whether the attacks
have some useful information that could to be incorporated
via a proper preprocessing (according to Principle 1).
We propose using an ensemble fen of classiï¬ers {fi}l
i=1
(according to Principle 3), which are trained from random
subspace of the original feature space. Each classiï¬er fi is
hardened by three countermeasures: input transformation
via binarization (according to Principle 4); adversarial
training/regularization models on the attacks using Adam
optimizer (dot arrows in Figure 1, according to Principle 2
and 5); semantics-preservation is achieved via an encoder
and a decoder (according to Principle 6). In order to
attain adversarial training and at the same time semantics-
preservation, we learn classiï¬er fi via block coordinate
descent to optimize different components of the model.

Algorithm 2 integrates all pieces for training individual
classiï¬ers. The training procedure consists of the following
steps. (i) Given a training set (X, Y ), we randomly select a

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

7

Fig. 1: Overview of the adversarial malware defense framework. In the training phase, an ensemble of l neural network
classiï¬ers are trained, with each classiï¬er hardened by three countermeasures (i.e., input transformation, semantics-
preserving, and adversarial training on the transformed data).

Algorithm 2: Training classiï¬er fi

Input: The training set (X, Y ), number of repeat

times K, epoch Nepoch and mini-batch size N .

1 Select a ratio Î› of sub-features from the feature set;
2 Transform input X to X via binarization;
3 for epoch = 1 to Nepoch do
4

i=1 from the (X, Y );

Sample a mini-batch {xi, yi}N
for k = 1 to K do

i=1;

Apply slight salt-and-pepper noises to
{xi}N
Derive the perturbed representation {x(cid:48)k
via Algorithm 1 using Adam method;

i }N

i=1

end
Select x(cid:48)
maximize the cross-entropy loss;

i from {x(cid:48)k

i }K

k=1 for xi (i = 1, Â· Â· Â· , N ) to

Calculate the reconstruction loss via Eq.(9);
Backpropagate the loss and update the denoising
autoencoder parameters;

Calculate the adversarial training loss via Eq.(6);
Backpropagate the loss and update classiï¬er
parameters;

5

6

7

8

9

10

11

12

13

14 end

i for xi âˆˆ {xi}N

ratio Î› of sub-features to the feature set, and then transform
X into X via the binarization discussed above. (ii) We
sample a mini-batch {xi, yi}N
i=1 from (X, Y ), and calculate
adversarial examples x(cid:48)
i=1 according to Lines
5-9 in Algorithm 2. (iii) We pass the {x(cid:48)
i=1 through the
denoising auto-encoder to compute the reconstruction loss
with respect to the target {xi}N
i=1 via Eq.(9), and update
the parameters of the denoising auto-encoder. (iv) We pass
i=1 and {xi}N
both the {xi + Î´xi}N
i=1 through the neural
networks to compute the classiï¬cation error with respect

i}N

to the ground-truth label {yi}N
i=1 via Eq.(6), and update the
parameters of the classiï¬er via backpropagation. Note that
Steps (ii)-(iv) are performed in a loop. The output of the
training algorithm is a neural network classiï¬er.

4 VALIDATING FRAMEWORK VIA DREBIN DATASET

We validate the effectiveness of the framework using the
Drebin dataset of Android malware [56], while considering
11 grey-box attacks and 9 white-box attacks. This dataset
also applied by former studies in the domain of adversarial
malware detection [5], [16], [23], [57].

4.1 Data Pre-Processing

Dataset. The Drebin dataset [56] contains 5,615 malicious
Android packages (APKs), and also provides features of
123,453 benign examples, together with their SHA256 values
but not the examples themselves. All samples were labeled
using the VirusTotal service [58] before the year 2015. An
example was treated as malicious if there are at least two
scanners say it is malicious, and is treated as benign if no
scanners detect it [56]. Because the VirusTotal may update
the detection result along with the time [59], we consider
relabeling the APKs. We download benign applications
corresponding to the given SHA256 values from the APK
markets (e.g., Google Play, AppChina, etc.), and collect
54,829 APKs in total. We send all of these examples (i.e.,
malicious and benign alike) to the VirusTotal service again.
Surprisingly, 12,496 benign APKs are detected as malicious
(rather than benign) by at least one scanners, and most of
them are detected as Adware or Trojan; this suggests that
the original Drebin training set has been contaminated by
the poisoning attack. This may be caused by some of the
following reasons: (i) Virustotal updates the scanners over
the time; (ii) Virustotal updates the report of a ï¬le when
a user requires to rescan the ï¬le; (iii) after an update,

ð± ð± Votingð‘¦ â€¦Sampleð± Classifier ð‘“1 Classifier ð‘“1 Target labelsClassifier ð‘“ð‘™ Ensembleð‘“ð‘’ð‘› ð‘“ð‘™ Classifier TrainingTestingForwardGradientTraining dataloss functionTransformationTransformeddataâ€¦EncoderSemantics-preservationDecoderloss functionTransformationTransformeddataâ€¦EncoderSemantics-preservationDecoderpreprocessingâ€¦IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

8

the previous report cannot be obtained anymore. We thus
remove these 12,496 benign examples from the original
benign dataset, leaving 42,333 benign APKs. The resulting
dataset contains 5,615 malicious APKs and 42,333 benign
APKs, namely 47,948 examples in total. We randomly
split the dataset into three disjoint sets for training (60%),
validation (20%), and test (20%), respectively.

Feature Extraction. APK is an archive ï¬le containing
AndroidManifest.xml, classes.dex, and others (e.g., res, assets).
The manifest ï¬le describes an APKâ€™s information, such as
the name, version, announcement, library ï¬les used by the
application. The source code is compiled to build the .dex
ï¬le which is understandable by the Java Virtual Machine.
The Drebin dataset has eight subsets of features, including
four subsets of features extracted from AndroidManifest.xml
(denoted by S1, S2, S3, S4, respectively) and four subsets
of
features extracted from the disassembled dexcode
(denoted by S5, S6, S7, S8, respectively). More speciï¬cally,
(i) S1 contains features corresponding to the access of
an APK to the hardware of a smartphone (e.g., camera,
touchscreen, or GPS module); (ii) S2 contains features
corresponding to the permissions requested by the APK
in question; (iii) S3 contains features corresponding to
the application components (e.g., activities, service, receivers,
etc.); (iv) S4 contains features corresponding to the APKâ€™s
communications with the operating system; (v) S5 contains
features corresponding to the critical system API calls,
which cannot run without appropriate permissions or the
root privilege; (vi) S6 contains features corresponding to the
used permissions; (vii) S7 contains features corresponding
to the API calls that can access sensitive data or resources
on a smartphone; (viii) S8 contains features corresponding
to IP addresses, hostnames and URLs found in the
disassembled code.

In order

to extract applicationsâ€™

features, we use
Androgurad 3.3.5, a reverse engineering toolkit for APK
analysis [60]. Note that 141 APKs cannot be analyzed.
Moreover, a feature selection is conducted to remove
those low-frequency features for the sake of computational
efï¬ciency. As a result, we keep 10,000 features with high
frequencies. The APK is mapped on the feature space as a
binary feature vector, where â€˜1â€™ (â€˜0â€™) represents the presence
(absence) of a feature in the APK.

4.2 Training Classiï¬ers

Classiï¬ers. In order to validate the defense framework, we
use and compare ï¬ve classiï¬ers: (i) the basic DNN with no
effort made to defend adversarial examples; (ii) hardened
DNN incorporating adversarial
training with known
manipulation set (dubbed Adversarial Training), which
manifests Principle 2 (grey-box attacks can be bounded by
the worst-case white-box attack) and Principle 5 (min-max
adversarial training); (iii) hardened DNN incorporating
adversarial regularization because the defender may know
the manipulation set, which is true in
nothing about
the case of AICSâ€™2019 Challenge (dubbed Adversarial
Regularization); (iv) Denoising Auto-Encoder (DAE) based-
classiï¬er, which manifests Principle 6 (semantics-preserving
representations); (v) classiï¬er hardened by both Adversarial
Training and DAE (dubbed AT+DAE); (vi) ensemble of

AT+DAE classiï¬ers in the random subspace (manifesting
Principle 3, dubbed Ensemble AT+DAE). For Principle 1
(i.e., knowing your enemy), we will simulate attacks in
the next subsection. Since we use binary feature vector,
Principle 2 (binarization) is not applicable.
Hyper-parameters Setting. We use DNNs with two fully-
connected hidden layers (each layer having 160 neurons)
with ReLU activation function. All classiï¬ers are optimized
by using Adam with epochs 150, mini-batch size 128,
and learning rate 0.001. For Adversarial Training,
the
inner maximization is optimized by using Adam with
learning rate 0.02 and iteration steps T = 100 to search
adversarial examples as many as possible. For Adversarial
Regularization, we set the learning rate as 0.01 for Adam
and conduct preliminary experiments to tune a proper
iteration step T . Finally, we set T = 60. We use an ensemble
of 5 base classiï¬ers. Our preliminary experiments suggest
us to learn base classiï¬ers from the entire training set and
the entire feature set. Unless with special mentioning, all
classiï¬ers that require to solve the inner maximization are
trained without random starting points so as to ease the
analysis (i.e., K = 0).

4.3 Attack Experiments and Classiï¬cation Results

is,

the attacker knows the dataset,

We present threat models speciï¬ed by whether the attacker
wages grey-box or white-box attacks, and constraints on the
attackerâ€™s manipulation set.
Grey-box vs. White-box Attacks. We consider two attack
scenarios. (i) Grey-box attacks: In this setting, we simulate
the attack scenario of the AICSâ€™2019 Challenge organizers.
That
feature set,
but not the defenderâ€™s learning algorithm. The attacker
generates adversarial examples from a surrogate classiï¬er.
We consider a surrogate model of two fully-connected
hidden layers (200 neurons each layer) and learn the model
on the training set. (ii) White-box attacks: In this setting,
the attacker knows everything about the malware detector.
Therefore, the adversarial examples are directly generated
from the corresponding malware detector.
Manipulations Constraints. Given an APK, we consider
both incremental and decremental manipulations. The
incremental manipulation allows the attacker to insert
some objects (e.g., activity) into an APK example to avoid
detection. The decremental manipulation allows the attacker
to hide some objects (e.g., activity) to avoid detection. In any
case, the adversarial example should preserve the malicious
functionality of the malware from which the adversarial
example is generated.

When the attacker uses incremental manipulations, the
attacker can insert some manifest features (e.g., request
extra permissions and hardware, state additional services,
Intent-ï¬lter, etc.). However, some elements are hard to insert,
such as content-provider, because the absence of URI will
corrupt an APK example. With respect to the .dex ï¬le, a
dead function or class (which is never called) containing
speciï¬ed system APIs can be injected without destroying
the APK example. The similar means can be performed for
the string injection (e.g., IP address), as well.

public void hideAPI() throws Exception{
// hide â€™printlnâ€™

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

9

String e_str = "ExMLXEZUDw";
// get â€™printlnâ€™
String p_str = decryptStr(e_str);
Class c = java.lang.System.class;
Field f = c.getField("out");
Method m = f.getType().
getMethod(p_str,String.class);
m.invoke(f.get(null), "hello world!");
return void
}

Listing 1: Java code to hide the API method â€œprintlnâ€.

When the attacker uses decremental manipulations,
the APKâ€™s information in the xml ï¬les can be changed
(e.g., package name). However, it is impossible to remove
activity entirely because an activity may represent a class
implemented in the .dex code. Nevertheless, we can rename
an activity and change its relevant information (e.g., activity
label), while noting that the related components in the .dex
should be modiï¬ed accordingly. The other components (e.g.,
service, provider and receiver) also can be modiï¬ed in the
similar fashion, and the resource ï¬les (e.g., images, icons)
can be manipulated as well. In terms of dexcode, the method
names and class names that are deï¬ned by developers could
be modiï¬ed, too. Note that the corresponding statement,
instantiation, reference, and announcements should be
changed accordingly. Moreover, user-speciï¬ed strings can
be obfuscated using encryption and the cipher-text will
be decrypted at running time. Further, the attacker can
hide public and static system APIs using Java reï¬‚ection and
encryption together. This is shown by the example in List 1.
All of the modiï¬cations mentioned above only obfuscate an
APK without changing its functionalities.

One challenge is that the attacker needs to perform
ï¬ne-grained manipulations on compiled ï¬les automatically
at scale, while preserving the functionalities of malware
examples. This important because a small change in a
malware example can render the ï¬le unexecutable. Since
Android APIs have upgraded multiple times in the past 5
years, the attacker has to inject compatible APIs into an APK
when manipulating a malware example. The preservation
of malicious functionalities may be estimated by using a
dynamic malware analysis tool, (e.g., Sandbox).

to

Feature

Mapping Manipulations
Space. The
aforementioned manipulations modify static Android
features, such as API calls and components in the manifest
ï¬le. Two kinds of perturbations can be applied to the Drebin
feature space. (i) Feature addition. The attacker can increase
the feature values (e.g., ï¬‚ipping â€˜0â€™ to â€˜1â€™) of appropriate
objects, such as components (e.g., activity), system APIs,
and IP address. (ii) Feature removal. The attacker can ï¬‚ip â€˜1â€™
to â€˜0â€™ by removing or hiding objects (e.g., activity, APIs.)
Table 2 summarizes our manipulations in the Drebin feature
space. We observe that neither feature addition nor feature
removal can be applied to S6 because these features depend
on S2 and S5, meaning that modiï¬cations on S2 or S5 may
cause changes to features in S6.
Evasion Attacks Setting. We randomly select 800 malware
examples from the test set to wage evasion attacks by
using the attack algorithms described in Section 2.3. In the
settings of Random, Grosse, BGA, BCA, and (cid:96)1-PGD attacks,

TABLE 2: Overview of manipulations on feature space,
where (cid:88)((cid:55)) indicates that the feature addition or removal
operation can (cannot) be performed on features in the
corresponding subset.

manifest

dexcode

Feature sets

S1 Hardware
S2 Requested permissions
S3 Application components
S4 Intents

Addition
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Removal
(cid:55)
(cid:55)
(cid:88)
(cid:55)

S5 Restricted API calls
S6 Used permission
S7 Suspicious API calls
S8 Network addresses

(cid:88)
(cid:55)
(cid:88)
(cid:88)

(cid:88)
(cid:55)
(cid:88)
(cid:88)

we iterate these algorithms until reaching a predeï¬ned
maximum number of steps, while noting that Grosse, BGA,
and BGA attacks leverage feature addition only. For waging
the Mimicry attack, in order to increase its effectiveness, we
use Nb benign examples to guide the perturbation of a single
malware example, leading to Nb perturbed examples; then,
we select a resulting example such that it causes the mis-
classiï¬cation with the smallest perturbation. Therefore, we
denote this attack as MimicryÃ—Nb. For other attacks, we set
Îµ = 1.0 for the FGSM attack. In (cid:96)âˆž norm and Adam based
PGD attacks, the step size is Î± = 0.01 with iterative times
100. The (cid:96)2 norm PGD attack is performed for 100 iterations
with step size 1.0.

Experimental Validation of Functionality. In order to test
whether or not perturbations in the feature space render to
executable ï¬les in the example space, we use Cuckoodroid
[61] to install and run APKs in an Android emulator. Owing
to efï¬ciency considerations, we randomly select 10 malware
APKs and generate their perturbed APKs using the PGD-
Adam attack against the Basic DNN model. Among the
10 original (i.e., unperturbed) APKs, all of them can be
loaded but 2 cannot run in the Android emulator. Among
the 10 perturbed examples, all of them can be loaded but
5 of them cannot run (and 2 of these 5 correspond to
the 2 original APKs that cannot run). This means that
more research is needed in order to systematically assure
that perturbation can indeed preserve the functionalities of
malware examples, which is unique to adversarial malware
detection [9], [11].

4.4 Experimental Results

TABLE 3: Effectiveness of the defense framework when
there are no adversarial attacks.

Defense

FNR (%)

FPR (%) Accuracy (%)

Basic DNN
Adversarial Training
Adversarial Regularization
DAE
AT+DAE
Ensemble AT+DAE

3.684
3.246
4.737
3.246
3.246
2.456

0.320
1.777
0.190
0.450
1.694
2.464

99.28
98.05
99.27
99.22
98.12
97.54

The Case of No Adversarial Attacks. Table 3
summarizes the classiï¬cation results on the test set, which
are measured with the standard metrics of False Negative

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

10

TABLE 4: Effectiveness of the defense framework against grey-box adversarial malware evasion attacks.

Attack

No Attack
Random Attack
MimicryÃ—1
MimicryÃ—10
FGSM [13]
Grosse [5]
BGA [7]
BCA [7]
PGD-(cid:96)1
PGD-(cid:96)2
PGD-(cid:96)âˆž
PGD-Adam

Basic DNN Adversarial Training (AT) Adversarial Regularization DAE AT+DAE

Ensemble AT+DAE

Accuracy (%)

96.63
100.0
53.88
35.25
4.00
1.13
0.25
0.25
0.25
58.63
0.25
52.50

97.00
100.0
86.13
85.63
97.50
97.00
100.0
100.0
100.0
100.0
100.0
100.0

95.63
100.0
52.75
34.88
95.88
11.75
71.13
49.50
43.88
99.75
100.0
100.0

96.88
100.0
56.88
52.63
96.88
65.13
100.0
58.00
53.88
100.0
100.0
100.0

96.50
100.0
91.50
85.13
96.75
97.63
100.0
100.0
100.0
100.0
100.0
100.0

97.75
100.0
96.13
89.88
98.00
99.38
100.0
100.0
100.0
100.0
100.0
100.0

TABLE 5: Effectiveness of the defense framework against white-box adversarial malware evasion attacks.

Attack

MimicryÃ—10
FGSM [13]
Grosse [5]
BGA [7]
BCA [7]
PGD-(cid:96)1
PGD-(cid:96)2
PGD-(cid:96)âˆž
PGD-Adam

Basic DNN Adversarial Training (AT) Adversarial Regularization DAE AT+DAE

Ensemble AT+DAE

Accuracy (%)

11.63
0.00
0.00
0.00
0.00
0.00
3.00
0.00
1.13

68.25
97.00
60.75
97.00
61.13
69.50
93.63
90.38
95.13

14.88
95.00
16.63
91.50
16.63
21.88
82.13
89.75
89.63

40.88
96.88
35.50
74.00
35.38
51.00
89.75
35.38
88.25

69.13
96.50
81.13
96.50
81.50
81.25
91.13
85.50
92.88

79.75
97.75
91.75
97.50
91.75
88.50
91.63
73.63
90.00

Rate (FNR), False Positive Rate (FPR), and classiï¬cation
Accuracy (i.e., the percentage of the test examples that are
classiï¬ed correctly) [62]. We observe that when compared
with the Basic DNN, Adversarial Training achieves a lower
FNR (0.438% lower) but a higher FPR (1.457% higher).
A similar tendency is exhibited by DAE, AT+DAE and
Ensemble AT+DAE. This can be explained as follows: by
injecting adversarial malware examples into the training set,
the learning process makes the model search for malware
examples in a bigger space, explaining the drop in FNR and
increase in FPR and therefore a slightly drop (â‰¤ 1.74%)
in the classiï¬cation accuracy. Adversarial Regularization
achieves a comparable classiï¬cation accuracy as Basic DNN,
but the highest FNR among the classiï¬ers we considered.
This is caused by the fact that DNN is regularized using
small perturbations applied to both benign and malicious
samples. In summary, we draw:

Insight 1. In the absence of adversarial attacks, Adversarial
Training and DAE can detect more malware examples than the
Basic DNN (because of their smaller FNR), at the price of a small
side-effect in the FPR and therefore the classiï¬cation accuracy;
Adversarial regularization achieves comparable accuracy as the
Baisc DNN while increasing the FNR.

The Case of Grey-box Attacks. Table 4 reports the
classiï¬cation results of the defense framework against grey-
box attacks. We make the following observations. First,
Basic DNN cannot defend against evasion attacks and
is completely ruined by attacks that
include Mimicry,
FGSM, Grosse, BGA, BCA, PGD-(cid:96)1, and PGD-(cid:96)âˆž. Second,
Adversarial Training signiï¬cantly enhances the robustness
of DNN, achieving the accuracy of 86.13% and 85.63%

against the MimicryÃ—1 and MimicryÃ—10 attack respectively
and a 100% accuracy against the other 6 attacks (i.e.,
BGA, BCA and 4 variants of PGD). Third, Adversarial
Regularization, without seeing any adversarial examples,
can defend against FGSM, PGD-(cid:96)âˆž, PGD-(cid:96)2 and PGD-
Adam attacks, but are not effective against attacks such
as Grosse, BCA, and PGD-(cid:96)1. A similar phenomenon is
observed for DAE. Nevertheless, when using Adversarial
Training and DAE together, namely AT+DAE, the defense
achieves the highest robustness against evasion attacks than
using Adversarial Training and DAE individually, except
for the MimicryÃ—10 attack and FGSM attack (encountering
a âˆ¼1% decrease). Fourth, the Ensemble AT+DAE consists
of ï¬ve AT+DAE classiï¬ers and achieves the highest
robustness among the considered defenses against
the
attacks investigated. In summary, we draw:

Insight 2. Under grey-box attack scenario, Adversarial Training
is an effective defense against evasion attacks; DAE offers some
defense capability that may not be offered by Adversarial Training;
using an ensemble of ï¬ve AT+DAE classiï¬ers is more effective
than using a single AT+DAE classiï¬er against evasion attacks;
Without knowing the attackerâ€™s manipulation set, Adversarial
Regularization enhances the robustness of Basic DNN but cannot
defend attacks such as Grosse.

The Case of White-box Attacks. Table 5 presents the
classiï¬cation results against white-box attacks. We make the
following observations. (i) All attacks can almost completely
evade Basic DNN, but the Mimicry attack is, relatively
speaking, less effective because this attack leverages less
information about the classiï¬ers than what the other attacks
do. (ii) Adversarial Training is effective against the FGSM

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

11

attack, BGA attack and PGD-Adam attack, but not effective
against the Grosse attack, BCA attack, and PGD-(cid:96)1 attack
because these attacks manipulate a few features when
generating adversarial examples and these manipulations
are unlikely perceived by Adversarial Training (owing to the
fact that Adversarial Training penalizes adversarial spaces
searched by Adam optimizer). (iii) As expected, Adversarial
Regularization is less effective than Adversarial Training.
Adversarial Regularization achieves a 91.50% accuracy
against the white-box BGA attack, in contrast to the 71.13%
accuracy against the grey-box BGA attack. This is counter-
intuitive but can be attributed to the fact that Adversarial
Regularization may render some gradient-based methods,
such as BGA, useless, which is a phenomenon known
as gradient-masking [63], [64], [65]. (iv) AT+DAE achieves
considerable robustness against those attacks, with at least
an 81.13% accuracy except for the MimicryÃ—10 attack,
which defeats the AT+DAE defense because Mimicry can
make adversarial malware examples similar to benign ones
[16]. (v) The ensemble of AT+DAE defense achieves the
highest accuracy against the MimicryÃ—10, the Grosse attack
and the BCA attack than the other defenses, with about
10% higher accuracy when compared with the AT+DAE
defense. However, the ensemble of AT+DAE achieves lower
accuracy than AT+DAE against the PGD-(cid:96)2 attack, the PGD-
(cid:96)âˆž attack, and the PGD-Adam attack. This may be caused
by the fact that the base model AT+DAE cannot effectively
mitigate these attacks. In summary, we draw:

Insight 3. Adversarial Training cannot effectively defend against
white-box attacks that were not considered in the training phase;
DAE can be useful when adversarial training is not effective;
employing ensembles can further improve the robustness against
certain white-box attacks. That is, no defenses can defeat all white-
box attacks effectively.

5 APPLICATION TO AICSâ€™2019 CHALLENGE
WHEN KNOWING NOTHING ABOUT ATTACKS

The challenge is in the context of adversarial malware
classiï¬cation, namely devising evasion-resistant, machine
learning based malware classiï¬ers. The dataset, including
both the training set and the test set, consists of feature
vectors extracted from Windows malware examples, each
of which belongs to one of the following ï¬ve classes:
Virus, Worm, Trojan, Packed malware, and AdWare. For
each example, the features are collected by the challenge
organizer via dynamic analysis, including the Windows
API calls and further processed unigram, bigram, and
trigram API calls. The feature names (e.g., API calls)
and the class labels are â€œobfuscatedâ€ by the challenge
organizer as integers, while noting the obfuscation preserves
the mapping between the features and the integers
representation of them. For example, three API calls are
represented by three unique integers, say 101, 102, and 103;
then, a trigram API call â€œ101;102;103â€ means a sequence of
API calls 101, 102, and 103. In total there are 106,428 features.
The test set consists of adversarial examples and non-
adversarial examples (i.e., unperturbed malware examples).
Adversarial examples are generated by a variety of
perturbation methods, which are not known to the

participating teams. However, the ground-truth labels of
the test examples are not given to the participating teams.
This means that the participating teams cannot calculate
the accuracy of their detectors by themselves. Instead, they
need to submit their classiï¬cation results (i.e., labels on the
examples in the test set) to the challenge organizer, who will
calculate the classiï¬cation score of each participating team.
The Challenge organizer decided to use the Macro F1 score
as the classiï¬cation accuracy metric. The Macro F1 score is
the unweighted mean of the F1 score [66] for each class of
objects in question (i.e., type of malware in this case). The
ï¬nal score is the Harmonic mean upon the two Macro F1
scores, namely the one for the adversarial examples in the
test data and the other for the non-adversarial examples in
the test data. Given these two numbers, say a1 and a2, their
harmonic mean 2a1a2
a1+a2

.

5.1 Basic Analysis of the AICSâ€™2019 Challenge

Is the Training Set Imbalanced? The training set consists of
12,536 instances, and the test set consists of 3,133 instances.
The training set contains 8,678 instances in class â€˜0â€™, 1,883
instances in class â€˜1â€™, 771 instances in class â€˜2â€™, 692 instances
in class â€˜3â€™, and 512 instances in class â€˜4â€™. We can calculate
the maximum ratio between the number of instances in
different classes is 16.95, indicating that the training set is
highly imbalanced. In order to cope with the imbalance in
the training set, we use the Oversampling method to replicate
randomly selected feature vectors from a class with a small
number of feature vectors. The replication process ends until
the number of feature vectors is comparable to that of the
largest class (i.e., the class with the largest number of feature
vectors), where â€œcomparableâ€ is measured by a predeï¬ned
ratio. In order to see the effect of this ratio, we use a 5-fold
cross validation on the training set to investigate the impact
of this ratio. The classiï¬er consists of neural networks with
two fully-connected layers (each layer having 160 neurons
with the ReLU activation function), which are optimized
via Adam with epochs 50, mini-batch size 128, learning
rate 0.001. The model is selected when achieving the best
Macro F1 score on the validation set. Table 6 shows that
the Macro F1 score decreases as the oversampling ratio of
minority classes increases. In order to make each mini-batch
of training data contain examples from all classes, which
would be critical in multiclass classiï¬cation, our experience
suggests us to select the 30% ratio.

TABLE 6: Accuracy (%) and Macro F1 score (%) are reported
with a 95% conï¬dence interval with respect to the ratio
parameter (%), where â€˜â€”â€™ means learning a classiï¬er using
the original training set.

Ratio (%) Accuracy (%) Macro F1 (%)

â€”
30
40
50
60

93.20Â±1.04
92.86Â±0.75
92.38Â±1.00
92.21Â±0.60
92.48Â±1.12

85.52Â±1.12
85.47Â±1.04
84.87Â±1.07
84.87Â±1.00
84.62Â±1.01

Are There Simple Indicators of Adversarial Examples?
In the ï¬rst test set published by the challenge organizer,
we see negative values for some features. These negative

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

12

values would indicate that they are adversarial examples.
In the revised test set provided by the challenge organizer,
there are no negative feature values, meaning that there are
no simple ways to tell whether an example is adversarial
or not. In spite of this, we can speculate on the count of
perturbed features by comparing the number of nonzero
entries corresponding to feature vectors in the training set
and feature vectors in the test set. Figure 2 shows the
normalized frequency of the number of nonzero entries of
feature vectors in the training set vs. test set. We observe that
their normalized frequencies are similar except that some
test examples have more nonzero entries. Their mean values
are close and are much smaller than the input dimension
(106, 428), suggesting that the average degree of perturbed
features may be small.

Fig. 2: Histogram of
the
number of nonzero entries of feature vectors in the training
set vs. test set. The dashed line represents the mean value.

the normalized frequency of

5.2 Classiï¬cation Results: Challenge Winner

We train 10 deep neural network classiï¬ers to formulate
an ensemble model,
including 4 classiï¬ers using the
binarization, adversarial regularization, and semantics-
preservation techniques discussed in the framework, and
the other 6 classiï¬ers using the binarization and adversarial
regularization techniques because examples are perturbed
without preserving their malicious functionality in the
training. Since we do not have access to the malware
examples, we cannot tell whether a feature perturbation
preserves the malware functionality or not. The inner
maximization performed by using gradient descent with
respect to the transformed input iterates T = 55 times
via the Adam optimizer [47] with learning rate 0.01. We
leverage the random start points and K = 5. The ratio for
ensemble of random subspace method is set as Î› = 0.5.
Each base classiï¬er has two fully-connected hidden layers
(each layer having neurons 160), uses the ELU activation
function, and is optimized by Adam. The ensemble achieves
a Macro F1 score of 88.30% upon non-attack dataset, a 63.0%
Macro F1 score under attacks, and a Harmonic mean on both
Macro F1 scores of 73.60%. This is the highest Harmonic
Mean score among the participating teams. Although this
score is not ideal, this may be inherent to the fact that we as
the defender do not know any information about the attack.
This leads to:

Insight 4. The information â€œbarrierâ€ that the defender does not
know the attackerâ€™s manipulation set is a fundamental one because
the attacker may use adversarial malware examples that are far
away from what the defender would use to train its defense model.

6 APPLICATION TO AICSâ€™2019 CHALLENGE
AFTER KNOWING GROUND TRUTH

After the Challenge organizer announced that we won the
Challenge, the ground-truth labels of the test set are released
so that we can conduct further study. We stress that we still
do not know the attacks that were used by the Challenge
organizer.

6.1 Training Classiï¬ers

Classiï¬er. We consider and compare ï¬ve classiï¬ers: (i) Basic
DNN without incorporating any defense; (ii) hardened
DNN incorporating the binarization technique [25] (dubbed
Binarization); (iii) hardened DNN incorporating adversarial
regularization
Regularization);
(dubbed Adversarial
(iv) hardened DNN incorporating Binarization and
Adversarial Regularization (dubbed Binarization+AR);
(v) an ensemble of Binarization+AR classiï¬ers (dubbed
Ensemble Binarization+AR).

Hyper-parameter Settings. All of the DNNs we use have
two fully-connected hidden layers (each layer having
160 neurons), use the ReLU activation function, and are
optimized by Adam with epochs 30, mini-batch size 128,
and learning rate 0.001. For Adversarial Regularization, we
perform the inner maximization via Adam (with learning
rate 0.01). Our preliminary experiments suggest us to set
iterations T = 60. The starting point is chosen from K = 5
initialized points with salt-and-pepper noises, which have
a noise ratio (cid:15)r chose uniformly at random from 0 to
(cid:15)r
max = 10%. This means at most 10% of the features can be
changed by salt-and-pepper noises in each training round.
For the ensemble, we train 5 Binarization+AR classiï¬ers,
each of which is learned from an 80% data randomly
selected from the training set, with a Î› = 0.5 fraction
of features. We augment the training set for the last three
classiï¬ers as described in Section 5.1.

6.2 Classiï¬cation Results

Table 7 presents the results with and without adversarial
attacks. We make three observations.
(i) Adversarial
Regularization signiï¬cantly improves the Macro F1 score
against the attacks when compared with the Basic DNN
(a 23.93% higher Macro F1 score). The Macro F1 score of
Adversarial Regularization in the absence of adversarial
attacks drops slightly when compared with the Basic DNN
(â‰ˆ 1%). (ii) By comparing Binarization (row 2) and the Basic
DNN, Binarization can improve the robustness of DNN
against adversarial attacks a little bit (a 0.47% increase in the
Macro F1 score). (iii) Ensemble Binarization+AR achieves a
higher classiï¬cation accuracy than Binarization+AR, in the
presence or absence of adversarial attacks.

Sensitivity.
In
max is crucial and is

Hyper-parameters
Adversarial
Regularization, (cid:15)r
set manually.
Intuitively, a greater (cid:15)r
max lets the defense perceive a
larger space, but inhibiting the convergence of training.
In addition, we want to know whether the oversampling
is useful or not for Adversarial Regularization. We thus
conduct a group of experiments to justify these settings.
Table 8 shows the experimental results. We observe that

030060090012001500Number of nonzero entry0.0000.0010.0020.0030.0040.005Normalized frequencyMean: 503.92Training set030060090012001500Number of nonzero entryMean: 556.72Testing setIEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

13

TABLE 7: Classiï¬ers Accuracy (%) and Macro F1 score (%) with no attacks vs. using grey-box adversarial evasion attacks
respectively, and the Harmonic mean (%) of the two Macro F1 scores.

Classiï¬ers

No attacks (%)

Attacks (%)

Accuracy Macro F1 Accuracy Macro F1

Harmonic mean (%)

Basic DNN
Binarization
Adversarial Regularization (AR)
Binarization+AR
Ensemble Binarization+AR

96.24
95.80
95.66
95.62
95.93

88.91
87.99
87.98
87.87
88.58

63.46
63.79
72.02
75.22
76.02

35.00
35.47
58.93
59.87
62.95

50.23
50.56
70.58
71.22
73.60

TABLE 8: Accuracy (%) and Macro F1 score (%) of
Adversarial Regularization in the absence vs. presence of
adversarial evasion attacks, with respect to the maximum
salt-and-pepper noise ratio (cid:15)r
max, where âˆ— means that a
classiï¬er is learned using oversampling.

Noise Ratio (%)

No attacks (%)

Attacks (%)

Accuracy Macro F1 Accuracy Macro F1

(cid:15)r
max = 0
(cid:15)r
max = 0.1
(cid:15)r
max = 1
(cid:15)r
max = 10
(cid:15)r
max = 20
max = 10âˆ—
(cid:15)r

96.11
95.93
96.24
96.19
96.06
95.66

88.43
88.10
89.14
88.46
88.14
87.98

69.68
74.00
73.11
77.11
75.11
72.02

49.87
50.52
55.98
56.22
51.23
58.93

the Macro F1 score in the presence of adversarial evasion
attacks increase with the increase of (cid:15)r
max from 0% to
10%. Meanwhile, Accuracy and Macro F1 score do not
decrease in the absence of adversarial evasion attacks, and
actually slightly increase at (cid:15)r
max = 1%. Furthermore, when
the oversampling technique is leveraged at (cid:15)r
max = 10%
(the last row), both Accuracy and Macro F1 score in the
absence of adversarial evasion attacks decrease slightly
(< 1%). Nevertheless, the Macro F1 score in the presence
of adversarial evasion attacks increases from 56.22% to
58.93%. This leads us to draw:

Insight 5. Oversampling is not necessary when there are no
adversarial evasion attacks, but improves the effectiveness of
Adversarial Regularization against adversarial evasion attacks in
terms of macro F1 score.

6.3 Retrospective Analysis of the AICSâ€™2019 Challenge

Fig. 3: Cross entropy loss of the classiï¬er hardened by
Adversarial Regularization over the training set, the test set
with no adversarial evasion attacks, and the test set with
adversarial evasion attacks.

Figure 3 demonstrates that adversarial regularization
over-ï¬ts the perturbations searched by the inner maximizer
unexpectedly. We observe that
the cross-entropy loss
induced by the perturbations increases signiï¬cantly after
about 10 epochs. Meanwhile, the cross-entropy loss on the
test set with no adversarial evasion attacks changes slightly,
until the number of epochs approaches 100. This means that
the DNN will memorize the perturbations produced in the
training phase, leading to poor generalization. Therefore,
new defense strategies are needed in order to achieve a
much higher accuracy against the Challenge instances. This
suggests:

Insight 6. Adversarial regularization triggers the over-ï¬tting
issue; Without knowing the manipulation set, unsupervised
learning may play an important role because unsupervised
defenses are devised without using label information about the
perturbed examples.

7 CONCLUSION
We have presented six principles for enhancing the
robustness of neural network classiï¬ers against adversarial
evasion attacks in the setting of malware classiï¬cation.
These principles guided us to design a framework, which
is validated via a real-world dataset and the AICSâ€™2019
Challenge. We drew a number of insights that are useful
for real-world defenders.

[68],

We hope this paper will

inspire more research into
this important problem. Future research problems are
abundant, such as the following. First, the adversarial
training in our study is applied to feature representations
satisfying box-constraints (in a discrete space). How should
we accommodate other kinds of feature extractions such
as graph-based or sequential-like ones [67],
[69],
[70]? One possible approach is to instantiate the minmax
adversarial training using a generic method, which does
not need to know the special knowledge of the hardened
model. Second,
it is imperative to generate adversarial
malware examples in an end-to-end fashion, assuring
that a perturbed malware example indeed preserves the
functionality of the original, unperturbed malware example.
Third, it is an open problem to adapt the provable or
certiï¬ed defense [71] into the context of adversarial malware
detection because it is not clear how one should deï¬ne
convex manipulation sets for perturbing malware examples.
Unlike the image data where (cid:96)p-norm may quantify visual
semantics, (cid:96)p-norm cannot characterize the functionalities
of malware examples. Fourth, what are the other principles
that can be leveraged to defend against adversarial malware
examples?

100101102Number of Epoch0.00.20.40.60.81.01.2Cross EntropyTrainingTest w/ no attacksTest w/ attacksIEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

14

REFERENCES

[1] D. Li, Q. Li, Y. Ye, and S. Xu, â€œEnhancing robustness of deep
neural networks against adversarial malware samples: Principles,
framework, and aicsâ€™2019 challenge,â€ CoRR, vol. abs/1812.08108,
2018. [Online]. Available: http://arxiv.org/abs/1812.08108
Symantec.
https://www.symantec.com/security-center/threat-report

(2018) Symantec @ONLINE.

[Online]. Available:

[2]

[3] CISCO.

(2018) Cisio @ONLINE.

[Online]. Available: https:

//www.cisco.com

[4] Y. Ye, T. Li, D. A. Adjeroh, and S. S. Iyengar, â€œA survey on malware
detection using data mining techniques,â€ ACM Comput. Surv.,
vol. 50, no. 3, pp. 41:1â€“41:40, 2017.

[5] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and
P. McDaniel, â€œAdversarial examples for malware detection,â€ in
European Symposium on Research in Computer Security.
Springer,
2017, pp. 62â€“79.

[6] L. Chen, Y. Ye, and T. Bourlai, â€œAdversarial machine learning
in malware detection: Arms race between evasion attack and
defense,â€ in EISICâ€™2017, 2017, pp. 99â€“106.

[7] A. Al-Dujaili, A. Huang, E. Hemberg, and U.-M. Oâ€™Reilly,
â€œAdversarial deep learning for robust detection of binary encoded
malware,â€ in 2018 IEEE Security and Privacy Workshops (SPW).
IEEE, 2018, pp. 76â€“82.
S. Hou, Y. Ye, Y. Song, and M. Abdulhayoglu, â€œMake evasion
harder: An intelligent android malware detection system,â€ in
Proceedings of the Twenty-Seventh IJCAI, 2018, pp. 5279â€“5283.

[8]

[9] F. Pierazzi, F. Pendlebury,

J. Cortellazzi, and L. Cavallaro,
â€œIntriguing properties of adversarial ml attacks in the problem
space,â€ in 2020 IEEE Symposium on Security and Privacy (SP).
IEEE,
2020, pp. 1332â€“1349.

[10] D. Li and Q. Li, â€œAdversarial deep ensemble: Evasion attacks and
defenses for malware detection,â€ IEEE Transactions on Information
Forensics and Security, vol. 15, pp. 3886â€“3900, 2020.

[11] Y. Kucuk and G. Yan, â€œDeceiving portable executable malware
classiï¬ers into targeted misclassiï¬cation with practical adversarial
examples,â€ in Proceedings of the Tenth ACM Conference on Data and
Application Security and Privacy, 2020, pp. 341â€“352.

[12] C. Szegedy, W. Zaremba,

J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus, â€œIntriguing properties of neural
networks,â€ arXiv preprint arXiv:1312.6199, 2013.

I. Sutskever,

[13] I.

J. Goodfellow,

J. Shlens, and C. Szegedy, â€œExplaining
and harnessing adversarial examples (2014),â€ arXiv preprint
arXiv:1412.6572.

[14] A. C. Serban and E. Poll, â€œAdversarial examples-a complete
preprint

phenomenon,â€

arXiv

the

of

characterisation
arXiv:1810.01185, 2018.

[15] D. Li, Q. Li, Y. Ye, and S. Xu, â€œSok: Arms race in adversarial

malware detection,â€ arXiv preprint arXiv:2005.11671, 2020.

[16] A. Demontis, M. Melis, B. Biggio, D. Maiorca, D. Arp, K. Rieck,
I. Corona, G. Giacinto, and F. Roli, â€œYes, machine learning can be
more secure! a case study on android malware detection,â€ IEEE
Transactions on Dependable and Secure Computing, vol. 16, no. 4, pp.
711â€“724, July 2019.

[17] M. Nazir.
@ONLINE.
pub releases/2019-01/uota-uwg011819.php

(2019) Utsa wins global cyber security challenge
[Online]. Available: https://www.eurekalert.org/

[18] B. Biggio, G. Fumera, and F. Roli, â€œMultiple classiï¬er systems for
robust classiï¬er design in adversarial environments,â€ International
Journal of Machine Learning and Cybernetics, vol. 1, no. 1-4, pp. 27â€“
41, 2010.

[19] â€”â€”, â€œMultiple classiï¬er systems under attack,â€ in International
Workshop on Multiple Classiï¬er Systems. Springer, 2010, pp. 74â€“83.
[20] C. Smutz and A. Stavrou, â€œWhen a tree falls: Using diversity in
ensemble classiï¬ers to identify evasion in malware detectors.â€ in
NDSS, 2016.

[21] J. W. Stokes, D. Wang, M. Marinescu, M. Marino, and B. Bussone,
â€œAttack and defense of dynamic analysis-based, adversarial
neural malware detection models,â€ in MILCOM 2018 - 2018 IEEE
Military Communications Conference (MILCOM), 2018, pp. 1â€“8.
[22] Q. Wang, W. Guo, K. Zhang, and et al., â€œAdversary resistant deep
neural networks with an application to malware detection,â€ in
Proceedings of the 23rd KDD. ACM, 2017, pp. 1145â€“1153.

[23] D. Li, R. Baral, T. Li, H. Wang, Q. Li, and S. Xu, â€œHashtran-
dnn: A framework for enhancing robustness of deep neural
networks against adversarial malware samples,â€ arXiv preprint
arXiv:1809.06498, 2018.

[24] L. Chen, S. Hou, Y. Ye, and S. Xu, â€œDroideye: Fortifying security
of learning-based classiï¬er against adversarial android malware
attacks,â€ in FOSINT-SIâ€™2018, 2018, pp. 253â€“262.

[25] W. Xu, D. Evans, and Y. Qi, â€œFeature squeezing: Detecting
arXiv

in deep neural

networks,â€

adversarial
preprint:1704.01155, 2017.

examples

[26] L. Xu, Z. Zhan, S. Xu, and K. Ye, â€œAn evasion and counter-
evasion study in malicious websites detection,â€ in CNS, 2014 IEEE
Conference on.

IEEE, 2014, pp. 265â€“273.

[27] A. Kurakin, I. Goodfellow, and S. Bengio, â€œAdversarial examples
in the physical world,â€ arXiv preprint arXiv:1607.02533, 2016.
[28] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
â€œTowards deep learning models resistant to adversarial attacks,â€
arXiv preprint arXiv:1706.06083, 2017.

[29] H. Drucker

â€œImproving generalization
performance using double backpropagation,â€ IEEE Transactions on
Neural Networks, vol. 3, no. 6, pp. 991â€“997, 1992.

and Y. Le Cun,

[30] C. Lyu, K. Huang, and H.-N. Liang, â€œA uniï¬ed gradient
regularization family for adversarial examples,â€ in Proceedings of
the 2015 IEEE International Conference on Data Mining (ICDM),
ser. ICDM â€™15. Washington, DC, USA: IEEE Computer Society,
2015, pp. 301â€“309.
[Online]. Available: http://dx.doi.org/10.
1109/ICDM.2015.84

[31] T. Miyato, A. M. Dai, and I. Goodfellow, â€œAdversarial training
methods for semi-supervised text classiï¬cation,â€ arXiv preprint
arXiv:1605.07725, 2016.

[32] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,
â€œStacked denoising autoencoders: Learning useful representations
in a deep network with a local denoising criterion,â€ Journal of
machine learning research, vol. 11, no. Dec, pp. 3371â€“3408, 2010.
[33] D. Meng and H. Chen, â€œMagnet: a two-pronged defense against

adversarial examples,â€ pp. 135â€“147, 2017.

[34] M.

Nazir.

challenge
problem. [Online]. Available: http://www-personal.umich.edu/
âˆ¼arunesh/AICS2019/challenge.html

workshop

(2019)

2019

Aics

[35] I. C. B. Biggio and D. M. et al., â€œEvasion attacks against machine
learning at test time,â€ in Machine Learning and Knowledge Discovery
in Databases: European Conference. Springer, 01 2013, pp. 387â€“402.
[36] P. L. Nedim rndic, â€œPractical evasion of a learning-based classiï¬er:
A case study,â€ in Security and Privacy (SP), 2014 IEEE Symposium
on.

IEEE, 2014, pp. 197â€“211.

[37] I. Rosenberg, A. Shabtai, L. Rokach, and Y. Elovici, â€œGeneric black-
box end-to-end attack against rnns and other calls based malware
classiï¬ers,â€ arXiv preprint, 2017.

[38] L. Chen, S. Hou, and Y. Ye, â€œSecuredroid: Enhancing security
of machine learning-based detection against adversarial android
malware attacks,â€ in ACSAC. USA: ACM, 2017, pp. 362â€“372.
[39] H. Dang, Y. Huang, and E.-C. Chang, â€œEvading classiï¬ers by

morphing in the dark,â€ in CCS. ACM, 2017, pp. 119â€“133.

[40] H. S. Anderson, A. Kharkar, B. Filar, and P. Roth, â€œEvading

machine learning malware detection,â€ Black Hat, 2017.

[41] W. Xu, Y. Qi, and D. Evans, â€œAutomatically evading classiï¬ers: A
case study on pdf malware classiï¬ers,â€ in NDSS, January 2016.
[42] W. Hu and Y. Tan, â€œGenerating adversarial malware examples for

black-box attacks based on gan,â€ 02 2017.

[43] N. Carlini and D. Wagner, â€œTowards evaluating the robustness of
neural networks,â€ in 2017 38th IEEE Symposium on Security and
Privacy (SP).

IEEE, 2017, pp. 39â€“57.

[44] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik,
and A. Swami, â€œThe limitations of deep learning in adversarial
settings,â€ in Security and Privacy (EuroS&P), 2016 IEEE European
Symposium on.

IEEE, 2016, pp. 372â€“387.

[45] O. Suciu, R. Marginean, Y. Kaya, H. D. III, and T. Dumitras,
â€œWhen does machine learning FAIL? generalized transferability
for evasion and poisoning attacks,â€ in 27th USENIX Security
Symposium (USENIX Security 18).
Baltimore, MD: USENIX
Association, Aug. 2018, pp. 1299â€“1316.

[46] F. Tram`er and D. Boneh, â€œAdversarial training and robustness
for multiple perturbations,â€ CoRR, vol. abs/1904.13000, 2019.
[Online]. Available: http://arxiv.org/abs/1904.13000

[47] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic

optimization,â€ CoRR, vol. abs/1412.6980, 2014.

[48] E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, and
C. Nicholas, â€œMalware detection by eating a whole exe,â€ arXiv
preprint arXiv:1710.09435, 2017.

[49] Z.-H. Zhou, Ensemble methods: foundations and algorithms. CRC

press, 2012.

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

15

[50] T. K. Ho, â€œThe random subspace method for constructing decision
forests,â€ IEEE Transactions on PAMI, vol. 20, no. 8, pp. 832â€“844,
1998.

[51] A. Demontis, M. Melis, M. Pintor, M. Jagielski, B. Biggio, A. Oprea,
C. Nita-Rotaru, and F. Roli, â€œWhy do adversarial attacks transfer?
explaining transferability of evasion and poisoning attacks,â€ in
28th {USENIX} Security Symposium ({USENIX} Security 19), 2019,
pp. 321â€“338.

[52] E. Grefenstette, R. Stanforth, B. Oâ€™Donoghue,

J. Uesato,
G. Swirszcz, and P. Kohli, â€œStrength in numbers: Trading-off
robustness and computation via adversarially-trained ensembles,â€
CoRR, vol. abs/1811.09300, 2018.
[Online]. Available: http:
//arxiv.org/abs/1811.09300

[53] L. Schott, J. Rauber, M. Bethge, and W. Brendel, â€œTowards the
ï¬rst adversarially robust neural network model on mnist,â€ arXiv
preprint arXiv:1805.09190, 2018.

[54] G. Alain and Y. Bengio, â€œWhat regularized auto-encoders learn
from the data-generating distribution,â€ The Journal of Machine
Learning Research, vol. 15, no. 1, pp. 3563â€“3593, 2014.

[55] T. Luong, H. Pham, and C. D. Manning, â€œEffective approaches to
attention-based neural machine translation,â€ in Proceedings of the
2015 Conference on Empirical Methods in Natural Language Processing,
2015, pp. 1412â€“1421.

[56] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, K. Rieck,
and C. Siemens, â€œDrebin: Effective and explainable detection of
android malware in your pocket.â€ in Ndss, vol. 14, 2014, pp. 23â€“
26.

[57] A. Demontis, M. Melis, M. Pintor, M. Jagielski, B. Biggio, A. Oprea,
C. Nita-Rotaru, and F. Roli, â€œWhy do adversarial attacks transfer?
explaining transferability of evasion and poisoning attacks,â€ in
28th USENIX Security Symposium (USENIX Security 19).
Santa
Clara, CA: USENIX Association, Aug. 2019, pp. 321â€“338.

[58] (2018, May) Virustotal.

[Online]. Available: https://www.

virustotal.com

[59] M. SebastiÂ´an, R. Rivera, P. Kotzias, and J. Caballero, â€œAvclass:
A tool for massive malware labeling,â€ in Research in Attacks,
Intrusions, and Defenses. Cham: Springer International Publishing,
2016, pp. 230â€“253.

[60] A. Desnos. (2019) Androguard @ONLINE. [Online]. Available:

https://github.com/androguard/androguard

[61] I. Revivo and O. Caspi, â€œCuckoodroid,â€ in Black Hat USA, Las

Vegas, NV, Jul. 2017.

[62] M. Pendleton, R. Garcia-Lebron, J.-H. Cho, and S. Xu, â€œA survey
on systems security metrics,â€ ACM Comput. Surv., vol. 49, no. 4,
pp. 1â€“35, Dec. 2016.

[63] A. Athalye, N. Carlini, and D. A. Wagner, â€œObfuscated gradients
give a false sense of security: Circumventing defenses to
adversarial examples,â€ CoRR, vol. abs/1802.00420, 2018.

[64] F. Tram`er, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh,
and P. McDaniel, â€œEnsemble adversarial training: Attacks and
defenses,â€ arXiv preprint arXiv:1705.07204, 2017.

[65] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik,
and A. Swami, â€œPractical black-box attacks against deep learning
systems using adversarial examples,â€ arXiv preprint, 2016.

[66] Y. Sasaki et al., â€œThe truth of the f-measure,â€ Teach Tutor mater,

vol. 1, no. 5, pp. 1â€“5, 2007.

[67] S. Cui, B. Xia, T. Li, M. Wu, D. Li, Q. Li, and H. Zhang,
â€œSimwalk: Learning network latent representations with social
relation similarity,â€ vol. 2018-January, 2017, pp. 1 â€“ 6. [Online].
Available: http://dx.doi.org/10.1109/ISKE.2017.8258804

[68] Y. Fan, S. Hou, Y. Zhang, Y. Ye, and M. Abdulhayoglu, â€œGotcha -
sly malware!: Scorpion A metagraph2vec based malware detection
system,â€ in Proceedings of KDDâ€™2018, 2018, pp. 253â€“262.

[69] S. Cui, T. Li, S.-C. Chen, M.-L. Shyu, Q. Li, and H. Zhang,
â€œDisl: Deep isomorphic substructure learning for network
representations,â€ Knowledge-Based Systems, vol. 189, p. 105086,
2020.

[70] S. Cui, Q. Li, and S.-C. Chen, â€œAn adversarial learning approach
for discovering social relations in human-centered information
networks,â€ EURASIP Journal on Wireless Communications and
Networking, vol. 2020, no. 1, pp. 1â€“19, 2020.

[71] M. Balunovic and M. Vechev, â€œAdversarial training and provable
defenses: Bridging the gap,â€ in International Conference on Learning
Representations, 2020.

