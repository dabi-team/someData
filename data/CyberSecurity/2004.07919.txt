1
2
0
2

n
a
J

5
1

]

R
C
.
s
c
[

3
v
9
1
9
7
0
.
4
0
0
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

1

A Framework for Enhancing Deep Neural
Networks Against Adversarial Malware

Deqiang Li, Qianmu Li, Yanfang Ye, and Shouhuai Xu

Abstract—Machine learning-based malware detection is known to be vulnerable to adversarial evasion attacks. The state-of-the-art is
that there are no effective defenses against these attacks. As a response to the adversarial malware classiﬁcation challenge organized
by the MIT Lincoln Lab and associated with the AAAI-19 Workshop on Artiﬁcial Intelligence for Cyber Security (AICS’2019), we
propose six guiding principles to enhance the robustness of deep neural networks. Some of these principles have been scattered in the
literature, but the others are introduced in this paper for the ﬁrst time. Under the guidance of these six principles, we propose a defense
framework to enhance the robustness of deep neural networks against adversarial malware evasion attacks. By conducting
experiments with the Drebin Android malware dataset, we show that the framework can achieve a 98.49% accuracy (on average)
against grey-box attacks, where the attacker knows some information about the defense and the defender knows some information
about the attack, and an 89.14% accuracy (on average) against the more capable white-box attacks, where the attacker knows
everything about the defense and the defender knows some information about the attack. The framework wins the AICS’2019
challenge by achieving a 76.02% accuracy, where neither the attacker (i.e., the challenge organizer) knows the framework or defense
nor we (the defender) know the attacks. This gap highlights the importance of knowing about the attack.

Index Terms—Adversarial Machine Learning, Deep Neural Networks, Malware Classiﬁcation, Adversarial Malware Detection.

(cid:70)

1 INTRODUCTION

communities’

M ALWARE remains a big threat to cyber security despite

tremendous countermeasure efforts.
For example, Symantec [2] reports seeing 357,019,453 new
malware variants in the year 2016, 669,974,865 in the year
2017, and 246,002,762 in the year 2018. Worse yet, there is
an increasing number of malware variants that attempted
to undermine anti-virus tools and indeed evaded many
malware detection systems [3].

In order to cope with the increasingly severe situation,
we have to resort to machine learning for automating the
detection of malware in the wild [4]. However, machine
learning based techniques are vulnerable to adversarial
evasion attacks, by which an adaptive attacker perturbs or
manipulates malware examples into adversarial examples
that would be detected as benign rather than malicious
(see, for example, [5], [6], [7], [8], [9], [10], [11]). The
state-of-the-art
the
problem of effective defense is largely open. For example,
adversarial training is known to be able to harden classiﬁers
against adversarial examples, but requires knowing about
the attack in terms of (for example) its manipulation set
[7]. This is indeed the context in which the AICS’2019
Malware Classiﬁcation Challenge is proposed. In a broader

there are many attacks, but

is that

A preliminary version of the paper was presented at AICS’2019, which does
not publish any formal proceedings [1].

• D. Li is with School of Computer Science and Engineering, Nanjing

University of Science and Technology.

• Q. Li is with School of Computer Science and Engineering, Nanjing
Intelligent

University of Science and Technology and School of
Manufacturing, Wuyi University

• Y. Ye is with Department of Computer and Data Sciences, Case Western

•

Reserve University.
S. Xu is with Department of Computer Science, University of Colorado
Colorado Springs. This work was done when he was at University of Texas
at San Antonio, USA. E-mail: sxu@uccs.edu

context, adversarial malware examples are a particular kind
of attacks against adversarial machine learning. Although
adversarial machine learning has received much attention
in application domains such as image processing (see,
e.g., [12], [13], [14]), the problem of adversarial malware
examples are much less investigated [5], [7], [9], [15].

The AICS’2019 challenge mentioned above is essentially
about whether we can defend against adversarial examples in
the wild. The challenge is characterized as follows. First,
we (i.e., any team participating in the Challenge as the
defender) are given a training set in the form of anonymized
feature representation by the Challenge organizer (i.e., we
do not even know what the feature names are), as well
as the corresponding ground-truth labels. We are informed
by the Challenge organizer that the training data contains
no adversarial examples. Second, we are given a set of
test data (again,
in anonymized feature representation)
and are told that the test data contains both adversarial
examples and non-adversarial examples. We do not know
what attacks are used by the Challenge organizers. We do
not know which examples in the test set are perturbed
by adversaries. This means that we neither know which
are adversarial examples, nor the attacks that are used
to generate them, nor the manipulation set. Third, our
task is to accurately classify the test data, including both
adversarial and non-adversarial examples. The setting of the
Challenge is realistic because in the real world defenders do
not know attacker’s speciﬁcations such as attack methods,
manipulation sets, and speciﬁc adversarial examples. The
importance of the problem in defending against adversarial
malware examples and the realistic setting of the Challenge
motivate the present study.

2327-4697 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.

 
 
 
 
 
 
IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

2

1.1 Our Contributions

In this paper, we make the following contributions. First, we
propose, to the best of our knowledge, the ﬁrst systematic
defense framework to enhance the robustness of Deep
Neural Network (DNN)-based malware classiﬁers against
adversarial evasion attacks. The framework is designed
under the guidance of a set of principles, some of which
are known but scattered in the literature (e.g., using an
ensemble of classiﬁers and minmax adversarial training), but
others are introduced in this paper for the ﬁrst time, such
as the following. We propose (i) using white-box attack,
where the attacker knows everything about the defense,
to bound the capability of grey-box attacks with respect to
the (cid:96)p (p ≥ 1) norm, where the attacker knows something
about the defense; (ii) using adversarial regularization (i.e.,
adversarial training with small perturbations) when the
manipulation set is not available to the defender; (iii)
leveraging semantics-preserving representations (realized
by Denoising Auto-Encoder or DAE for shorthand).

Second, we empirically validate the effectiveness of
the framework against 11 grey-box attacks and 9 white-
box attacks (i.e., 20 attacks in total). The 11 grey-box
attacks include the Random attack, two Mimicry attacks
[16], the Fast Gradient Sign Method (FGSM) attack [13],
Grosse attack [5], Bit Gradient Ascent (BGA) attack [7],
Bit Gradient Ascent (BCA) attack [7], and four variants
of the Projected Gradient Descent (PGD) attacks. The 9
white-box attacks leverage the victim models directly and
the attack algorithms are the same as the latter 9 ones
mentioned above. Among these attacks, the four variants
of the PGD attacks are used to be investigated in other
application settings and are adapted to the adversarial
malware detection domain for the ﬁrst time. The variant
PGD attacks permit feature addition and feature removal,
incurring larger manipulation sets than the Grosse, BGA,
and BCA attacks. In these experiments, adversarial malware
examples are generated by manipulating regular malware
examples while preserving their malicious functionalities.
Our empirical ﬁndings include: (i) standard DNNs without
incorporating defense can be ruined by both grey-box and
white-box attacks; (ii) adversarial regularization without
considering attacks in the training phase has limited success
in terms of improving the robustness of DNNs against
adversarial examples; (iii) adversarial training with the
Adam optimizer can signiﬁcantly enhance the robustness
of DNNs against multiple grey-box evasion attacks, but
not the more capable white-box Grosse, BCA and PGD-
(cid:96)1 attacks; (iv) DAE provides a degree of extra robustness
when used together with adversarial training, which is
ineffective in defending against the white-box Grosse, BCA
and PGD-(cid:96)1 attacks; (v) adding ensembles further improves
the robustness of DNNs, at the price of sacriﬁcing a degree
of the effectiveness of adversarial training against the white-
box PGD-(cid:96)2, PGD-(cid:96)∞ and PGD-Adam attacks.

Third, we apply the framework to the AICS’2019
adversarial malware classiﬁcation challenge organized by
the MIT Lincoln Lab. According to the Challenge organizers,
there were “over 300 participants attempted to download
and classify the malware data set” [17] and we win the
Challenge by achieving a 73.60% Harmonic mean score

(which is the metric the organizer chose to use before
making the data available); i.e., we achieve the highest score
among all of the participating teams.

Fourth, after announcing that we win the Challenge, the
organizer makes the ground-truth labels publicly available
at http://www-personal.umich.edu/∼arunesh/AICS2019/
challenge.html.
In order to understand why we only
achieve a 73.60% Harmonic mean score, we leverage the
ground-truth labels to conduct a further study. We show
that (i) oversampling beneﬁts adversarial regularization in
defending against evasion attacks in term of the Macro-F1
score and (ii) adversarial regularization tends to overﬁt the
perturbed examples while this phenomenon does not occur
to the non-adversarial (i.e., original) data.

Fifth, we show that the framework is effective in resisting
grey-box attacks via the widely-used Drebin Android
malware dataset (with a 98.49% accuracy on average), where
the attacker knows some information about the defense
and the defender knows some information about the attack.
When applied to the AICS’2019 challenge dataset but only
considering the adversarial examples (for the sake of fair
comparison with the experiment on the Drebin dataset), the
framework only achieves a 76.02% accuracy on average,
where it is still true that neither the attacker knows the
defense nor the defender knows the attacks. This highlights
that the defender should always strive to know as much
information as possible about
the attacks. In order to
avoid any confusion, we reiterate that the aforementioned
experiment result (i.e., 73.60% in the Harmonic mean score)
considers both adversarial and non-adversarial examples
(as required by the challenge organizer); whereas the 76.02%
accuracy disregards of the non-adversarial examples (for
fair comparison with the experiment with the Drebin
dataset). Another difference is that in the new experiment
achieving a 76.02% accuracy we use an ensemble of 5
building-block models, whereas in the experiment achieving
73.60% Harmonic mean score we use 10 building-block
models.

Last but not

the least, we made our the code of
our models publicly available at https://github.com/
deqangss/aics2019 challenge adv mal defense.

1.2 Related Work

Since the present paper
focuses on defense against
adversarial malware examples, we review related prior
input
studies
prepossessing, adversarial training/regularization, and DAE-
based representation learning.

categories:

in four

learning,

ensemble

Ensemble learning can reduce the generalization error
by diversifying the building-block models. Biggio et al.
[19] show how the bagging and random subspace
[18],
techniques can enhance the robustness of linear models
against evasion attacks. Smutz and Stavrou [20] propose
using the conﬁdence score produced by random forest
classiﬁers to detect adversarial malware. Stokes et al. [21]
investigate the resilience of ensemble DNNs against evasion
attacks.
In this paper, we diversify the building-block
models via randomly initialized parameters and the random
subspace algorithm.

Input prepossessing transforms the input to a different
representation, aiming to reduce the degree of perturbations

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

3

applied to the original input. For example, Random Feature
Nulliﬁcation (RFN) randomly nulliﬁes features both in the
training and test phases [22]; HashTran [23] reduces small
perturbations using a locality-sensitive hashing; DroidEye
[24] quantizes binary representation via count featurization.
In our framework, inspired by the idea of feature squeezing
[25], we use binarization to reduce the perturbations.

Adversarial training augments the training data with
adversarial examples. Various kinds of heuristic training
strategies have been proposed (see, e.g., [5], [12], [13], [26],
[27]. However, these strategies typically deal with speciﬁc
evasion methods and are not effective against others.
Furthermore, researchers propose considering adversarial
training with the optimal attack, which in a sense
corresponds to the worst-case scenario and therefore could
lead to classiﬁers that are robust against the non-optimal
attacks [7], [28]. In our framework, we seek the optimal
attack via a gradient descent method, while meeting the
requirement of discrete inputs via a nearest neighbor search.
training
method that aims to train a model with slightly perturbed
examples, which may or may not be functionality-
preserving.
the
generalization of DNN models [13], [27], [29], [30], [31]. This
approach may be useful because in the context of malware
detection, the defender may not know the manipulation set
of the attacker.

Adversarial regularization is an adversarial

Intuitively, small perturbations beneﬁt

DAE facilitates robust representation learning [32],
[33]. Li et al. [23] propose detecting adversarial malware
examples using DAE. In our framework, we use DAE
to learn the robust representation that is insensitive to
perturbations.

1.3 Paper Outline

The rest of the paper is organized as follows. Section 2
presents the adversarial malware evasion attacks, including
four attacks that are adapted to the domain of adversarial
malware detection for the ﬁrst time. Section 3 describes
our defense framework. Section 4 validates our defense
framework with a real-world dataset. Section 5 presents
the results when applying the framework to the AICS’2019
Challenge without knowing anything about
the attack.
Section 6 presents our further study after winning the
AICS’2019 Challenge and being given the ground-truth
labels of the test data. Section 7 concludes the present paper.

arg max returns the index of the maximum element in a o-
dimension vector. Let L : Ro × Y → R be a loss function.
The parameters of F, denoted by θ, are optimized via

E(x,y)∈X ×Y [L(F(x), y)] .

min
θ

(1)

Speciﬁcally, the cross-entropy is leveraged L(F(x), y) =
−1(cid:62)
y log(F(x)), where 1y is the one-hot encoding vector for
the label y. For simplifying notations, we use F (rather than
Fθ) to denote a neural network. Table 1 summarizes the
main notations used in the paper.

TABLE 1: Main notations used in the paper

Notation
z ∈ Z
(x, y) ∈ X × Y

δx ∈ Mx

x(cid:48) ∈ S(x, Mx)

o
dim
f : X → Y
F : X → Ro
θ
L : Ro × Y → R

Meaning
z is a software example; Z is the example space
x is feature representation of z; X is the feature
space; y is the label of x; Y is the label space
δx is a perturbation vector of x; Mx is the
manipulation set of x
x(cid:48) is perturbed from x; S is the set of perturbed
representations derived from x and Mx; S ⊆ X
o is the number of classes
dim is the number of dimension of x
f is the classiﬁer
F denotes a neural network
θ denotes parameters of neural network F
L is cross-entropy loss function

2.2 Basic Idea
With regard to the feature space X , given the representation-
label pair (x, y), the adversarial evasion attack attempts to
perturb x into x(cid:48), such that

f (x(cid:48)) (cid:54)= y s.t. x(cid:48) ∈ S(x, Mx)

(2)

where S(x, Mx) is the set of perturbed representations
derived from the non-adversarial feature representation x
and a manipulation set Mx (i.e. the set of manipulations
that can preserve the malicious functionality of malware
examples). The perturbation vector is denoted by δx = x(cid:48) − x
with δx ∈ Mx. Since the manipulation is conducted in
the feature space, the attacker needs to map x(cid:48) back into
the example space Z in order to obtain an executable
adversarial malware example z(cid:48) ∈ Z. This is a requirement
that distinguishes adversarial malware detection from
adversarial machine learning in other application domains,
which induces the problem of generating adversarial
examples in the discrete space. It is worth mentioning that
an attacker tends to modify malware examples by exploiting
one or multiple feasible manipulations [5], [10], [16].

2 ADVERSARIAL MALWARE EVASION ATTACKS

2.3 Threat Model

2.1 Notations

Given a non-adversarial malware example z ∈ Z, its feature
representation x ∈ X can be obtained via some feature
extraction methods, where Z denotes the example space (i.e.,
the set of all possible software examples) and X denotes the
feature space. A classiﬁer f : X → Y takes x as input and
outputs its label y ∈ Y, where Y is the label space.

We focus on a classiﬁer f that is learned as a neural
network model F : X → Ro, whose output (softmax)
is treated as the probability mass function over o = |Y|
classes [1], [5], [7], [34]. That is f = arg maxj∈Y F, where

The threat model against malware classiﬁers and detectors
can be speciﬁed by what the attacker knows, what the attacker
can do, and how the attacker wages the attack.

2.3.1 What the attacker knows

There are three kinds of models from this perspective. A
black-box attacker knows nothing about classiﬁer f except
what is implied by f ’s responses to the attacker’s queries.
A white-box attacker knows all kinds of information about
f ,
including its learning algorithms, model parameters,
defenses strategies, etc. A grey-box attacker knows an

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

4

amount of information about f that resides in between
the preceding two extremes. For example, the attacker may
know the feature set.

2.3.2 What the attacker can do

In evasion attack,
the attacker only can manipulate
malware examples in the test set using various kinds
of manipulations, while obeying some constraints. One
constraint is to preserve the malicious functionality of
malware. A simplifying assumption is to consider insertion
only (e.g., ﬂipping a feature value from ‘0’ to ‘1’ [5], [7],
[9], [22], [35], [36], [37], [38], while noting that attackers can
manipulate a malware example by inserting and deleting
operations [39], [40]. Since a larger manipulation set gives
the attacker more freedom, we permit
the attacker to
conduct both feature addition and feature removal. The other
constraint is to maintain the relation between features.
Using the AICS’2019 malware classiﬁcation challenge as an
example, we note that n-gram (uni-gram, bi-gram, and tri-
gram) features reﬂect sequences of Windows system API
calls. This means that when the attacker inserts an API call
into a malware example, several features related to this API
call will need to be changed according to the deﬁnition of
n-gram features.

2.3.3 How the attacker wages the attack

Researchers generate adversarial malware examples using
various machine learning-based techniques such as genetic
learning, and generative net-
algorithms, reinforcement
works [26], [40], [41], [42]. In order to generate adversarial
malware examples effectively and efﬁciently, attackers often
exploit the gradient-based methods [13], [35], [36], [43], [44].
We here brieﬂy describe multiple types of attacks, some of
which were introduced in the context of malware detection
but the others were introduced in the context of image
classiﬁcation and then adapted to the context of malware
detection.

Random Attack. We introduce this method as a baseline
attack in the adversarial malware detection domain. In this
attack, the attacker randomly modiﬁes a feature at each
iteration until a predeﬁned step is reached or no more
features can be manipulated.

Mimicry Attack. This attack was introduced in [16], [35],
[36], [45] for studying adversarial malware detection. In
this attack, the attacker perturbs or manipulates a malware
example such that the resulting adversarial version mimics
a chosen benign example as much as possible. In order to
reduce the degree of perturbations, the attacker may select
the benign example to be close to the malware example that
is to be modiﬁed.

FGSM Attack. This attack was introduced in the context of
image classiﬁcation [13] and then adapted to the malware
detection [7], [24]. FGSM perturbs a feature vector x in the
direction of the (cid:96)∞ norm of the gradients of the loss function
with respect to the input, namely:

x(cid:48) = ProjS (x + ε · sign((cid:79)xL(F(x), y))) ,
where ε > 0 is a scalar known as step size, (cid:79)x indicates
the derivative of the loss function L(F(x), y) with respect

to x, and ProjS (·) projects an input into S that denotes the
shorthand of S(x, Mx).

Grosse Attack. This attack was introduced by Grosse et al.
[5] in the context of malware detection. The attack considers
sensitive features, namely the features have large positive
gradients as far as the softmax output is concerned. The
attack is to manipulate the absence of a feature (e.g., not
making a certain API call) into the presence of the feature
(i.e., making the API call), while preserving their malicious
functionalities. These sensitive features can be identiﬁed by
leveraging the gradients of the softmax output of a malware
example with respect to the input.

BGA Attack and BCA Attack. In the context of malware
detection, Al-Dujaili et al.
[7] proposed two separate
methods, dubbed BGA and BCA, aiming to solve:

max
x(cid:48)∈S(x,Mx)

L(F(x(cid:48)), y).

(3)

In addition, the authors considered malware examples in
the binary space and restricted Mx to API calls addition.
Both attack methods iterate multiple steps. In each iteration,
BGA increases the feature value from ‘0’ to ‘1’
if the
corresponding partial derivative of the loss function with
respect
to the
to the input
gradient’s (cid:96)2 norm divided by
dim, where dim is the input
dimension. In contrast, BCA ﬂips ‘0’ to ‘1’ for a component
at the iteration corresponding to the maximum gradient of
the loss function with respect to the input.

is greater than or equal

√

PGD Attack. We adapt the PGD method to the context
of malware detection, by accommodating discrete input
spaces. In contrast to the Grosse, BGA, and BCA attacks,
the adapted PGD attacks permit both feature addition and
feature removal. Speciﬁcally, PGD ﬁnds perturbations via an
iterative procedure

(cid:0)δi

x), y)(cid:1) ,

δi+1
x = Proj ˆMx

x + α · (cid:79)δxL(F(x + δi

(4)
where α > 0 is the step size, (cid:79)δx is the derivative
of the loss function L(F(x + δi
x), y) with respect to δx,
and Proj ˆMx projects perturbations into a predetermined
space ˆMx. We set ˆMx = [u, u] for u = minimum(Mx)
and u = maximum(Mx), where minimum returns the
component-wise minimum vector (i.e., each component
of
the
the vector corresponding to the minimum of
corresponding component values of the vectors in Mx) and
maximum returns the component-wise maximum vector.

When solving Eq.(4), we encounter two issues that need
to be addressed: (i) small derivatives of g = (cid:79)δxL and (ii)
mapping perturbations into discrete space Mx. To see issue
(i), we note that by writing F as F(x) = softmax(Z(x)),
we have ∂L/∂δx = (F − 1y) · ∂Z/∂δx, meaning that the
derivatives approach zero when F predicts x as y with high
conﬁdence. To cope with this, researchers [28], [46] have
proposed to “normalize” the derivatives using (cid:96)p-norm and
leveraging the steepest direction as follows:

•

For p = 1, the direction is sign(gi)1i, where i is the
index corresponding to the maximum absolute value
of g = (g1, . . . , gdim) with dim being the number of
input dimension, 1i has the same dimension as g and

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

5

has value 1 at the ith component and value 0 at the
other components, and sign returns 1 when the input
> 0, −1 when the input < 0, and 0 when the input
= 0.
For p = 2, the direction is g/ (cid:107) g (cid:107)2.
For p = ∞, the direction is sign of gradients, i.e.,
sign(g).

•

•

We call these variant PGD attacks PGD-(cid:96)1, PGD-(cid:96)2 and
PGD-(cid:96)∞, respectively. Note that PGD-(cid:96)1 degrades to the
BCA attack when only feature addition is permitted. In
addition to these (cid:96)p-norm based attacks, we observe that
the attacker can use the Adam optimizer to accelerate the
process of gradient descent (the “normalized” gradients are
approximate to ±1) [47], leading to a new variant of the
PGD attack, which we call PGD-Adam.

Algorithm 1: PGD attack in the feature space.

Input: The feature representation-label pair (x, y),

manipulation set Mx, number of iterations T ,
step size α

Output: Perturbed example x(cid:48)
1 Initialize perturbation vector δ0
2 Derive the continuous space ˆMx and the perturbed

x = 0;

representation set S;
3 for i = 0 to T − 1 do
4

Obtain the derivatives (cid:79)δxL and normalize them
using (cid:96)p-norm where p = 1, 2, ∞ or the Adam
method;
Calculate δi+1

via the Eq.(4);

x

5
6 end
7 Obtain x(cid:48) by mapping ˜x(cid:48) = x + δT
8 return x(cid:48).

x via Eq.(5);

To address the issue (ii), we introduce a mapping method
to consider two settings as follows. In order to follow the
direction of “normalized” gradients, let the perturbation δx
be continuous during the optimization process. We map
the perturbed representation obtained at the last iteration,
denoted by ˜x(cid:48) = (˜x(cid:48)
dim), into S by selecting its
closest neighbor x(cid:48) = (x(cid:48)

1, . . . , ˜x(cid:48)

dim) such that

1, . . . , x(cid:48)

x(cid:48) = arg min

x(cid:48)∈S

(cid:107) x(cid:48) − ˜x(cid:48) (cid:107)1= arg min

x(cid:48)∈S

dim
(cid:88)

i=1

|x(cid:48)

i − ˜x(cid:48)
i|.

(5)

Geometrically speaking, Eq.(5) says that
dimension, x(cid:48)
1 summarizes the PGD attacks in the feature space.

i is the feasible scalar closest to ˜x(cid:48)

the ith
for
i. Algorithm

3.1.1 Principle 1: Knowing the enemy

This principle says that
the defender should strive to
extract useful information about the attacks as much as
possible as the information will offer insights on designing
countermeasures. Threat models are a standard approach to
modeling attacks. Moreover, it is possible to design some
indicators of adversarial examples. On the other hand, the
attack method and manipulation set may not be known
to the defender. This means that whenever possible, the
defender has to simulate them.

3.1.2 Principle 2: Bridging grey-box vs. white-box gap

In grey-box attacks, the attacker knows some information
about the feature set and therefore can train a surrogate
classiﬁer ˆf : X → Y from a training set (where the
realization of ˆf is a neural network ˆF), leveraging the
transferability from ˆf to f to generate adversarial examples.
Consider an input x for which a grey-box attacker generates
perturbations using

ˆδx =

arg max
||δx||≤η ∧ δx∈Mx

L(ˆF(x + δx), y),

where η is an upper bound and possibly large. Based on
the degree of perturbations, we consider two cases: (i) η is
small and (ii) η is large. We further assume that the optimal
perturbation vector δx of F exists.

In case (i) or when η is small, the change to the loss of f

incurred by ˆδx is

|∆L| =

≈

(cid:12)
(cid:12)
(cid:12)L(F(x + ˆδx), y) − L(F(x), y)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:79)L(F(x), y)(cid:62)ˆδx
(cid:79)L(F(x), y)(cid:62)δ
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ max

||δ||≤η

(cid:12)
(cid:12)
(cid:12)

= η||(cid:79)L||∗,

where the approximation is derived using the ﬁrst-order
Taylor expansion at point x, (cid:79) is the operator for computing
partial derivatives of the loss function with respect to the
input of neural network F, and “|| · ||∗” is the dual norm of
|| · ||.

In case (ii) or when η is large, we derive

|∆L| =

=

=

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)L(F(x + ˆδx), y) − L(F(x), y)
(cid:12)
(cid:12)
(cid:90) ˆδx
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:79)L(F(x + tˆδx), y)(cid:62)ˆδxdt

(cid:79)L(F(x + δ), y)dδ

0
(cid:90) 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0
≤ η sup
||δ||≤η

(cid:107)(cid:79)L(F(x + δ), y)(cid:107)∗ .

3 ADVERSARIAL MALWARE DEFENSE

3.1 Guiding Principles

The preceding observation shows that corresponding to the
same perturbation upper bound η, the loss incurred by grey-
box attacks is upper bounded by the loss incurred by white-
box attacks. This suggests us to focus on the robustness of
classiﬁer f against the optimal white-box attack.

These principles are geared to neural network classiﬁers,
which are chosen as our focus because deep learning
techniques are increasingly employed in the domain of
malware detection/classiﬁcation, but their vulnerability to
adversarial evasion attack has yet to be tackled [48].

3.1.3 Principle 3: Not putting all eggs in one basket

This is suggested by the observation that no single classiﬁer
may be effective against all kinds of attacks. An ensemble
can be built by many methods (e.g., bagging, boosting,
or stacking) [49]. For example, random subspace [50] is

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

6

seemingly particularly suitable for formulating malware
classiﬁer ensembles owing to the high dimensional feature
vector of malware, which indicates a high vulnerability of
classiﬁers to adversarial malware examples [16], [51].

Formally, an ensemble fen : X → Y contains a set
i=1, namely {Fi : X → Ro} for
of neural networks {Fi}l
1 ≤ i ≤ l. Given a test example x, we treat the base model
equally, as suggested by the study [18], [52], and the voting
method is

Fen(x) =

1
l

l
(cid:88)

i=1

Fi(x).

We obtain the predicted label by fen = arg maxj∈Y Fen.

3.1.4 Principle 4: Using transformation against perturbation

In typical applications, the defender does not know what
kinds of evasion attacks are waged by the attacker. These
attacks can produce a spectrum of perturbations, from
manipulating a few features (e.g., the PGD-(cid:96)1 attack) to
manipulating a large number of features (e.g., the FGSM
attack). Moreover, we may give higher weights to the
transformation techniques that can simultaneously reduce
the degrees of multiply types of perturbations such as (cid:96)∞
norm, (cid:96)1 norm, or (cid:96)2 norm. This suggests us to use the
binarization technique [25], [53]: When the feature value of
the ith feature, denoted by xi, is smaller than a threshold
Θi, we binarize xi to 0; otherwise, we binarize xi to 1.

3.1.5 Principle 5: Using vaccine

We harden a model
adversarial training:

incorporating the known minmax

min
θ

E(x,y)∈X ×Y

(cid:20)

L(F(x), y) + max
x(cid:48)∈S

(cid:21)
L(F(x(cid:48)), y)

.

(6)

[7]

Al-Dujaili et al.
instantiate this method by using
attacks with feature addition solely (e.g., BGA). In order to
accommodate more manipulations, we solve the problem
of inner maximization using the Adam optimizer (see
Section 2.3.3). Given the issue of local minima, we run the
inner maximizer several times, each with a random initial
point near the training data, and then select the point that
maximizes the loss function of L.

It is worth mentioning that in the AICS’2019 Challenge,
the defender does not know the manipulation set Mx and
thus cannot derive S. In this case, we propose training
malware classiﬁers by applying small perturbations to
the feature representations of malware examples (without
necessarily preserving their malicious functionalities). This
would beneﬁt model generalization [13], [29]. Let a norm
|| · || measure the perturbation δx with η bounded.
We have max||δx||≤η L(F(x + δx), y) ≈ L(F(x), y) +
η||(cid:79)L(F(x), y)||∗, leading to |L(F(x(cid:48)), y) − L(F(x), y)| ≤
η||(cid:79)L(F(x), y)||∗. Therefore, adversarial
regularization
assures
small perturbations do not change the
that
prediction signiﬁcantly.

3.1.6 Principle 6: Preserving semantics

This suggests us to strive to learn neural network models
that are sensitive to malware semantics, but not
the
perturbations because adversarial examples must retain the
malicious functionality of original malware. Speciﬁcally, we

propose using denoising auto-encoder to learn semantics-
preserving representations, rendering neural network less
sensitive to perturbations. A DAE ae = d ◦ e uniﬁes two
components: an encoder e : X → H that maps an input
M (x) to a latent representation r ∈ H and a decoder
d : H → X that reconstructs x from r, where the H
is the latent representation space and M refers to some
operations applied to x (e.g., adding Gaussian noises to
x). Vincent et al. [32] showed that the lower bound of the
mutual information between x and r is maximized when the
reconstruction error is minimized. In the case of Gaussian
noise (cid:15) ∼ N (0, σ2) and reconstruction loss
E(cid:15)∼N (0,σ2) (cid:107)ae(x + (cid:15)) − x)(cid:107)2
2 ,
Alain and Bengio [54] showed that the optimal ae∗(x) is

(7)

ae∗(x) =

E(cid:15) [p(x − (cid:15))(x − (cid:15))]
E(cid:15) [p(x − (cid:15))]

,

(8)

where p(·) is the probability density function. Eq.(8) says
that representations of a well-trained DAE are insensitive to
x because of the weighted average from the neighborhood
of x, which is reminiscent of the attention mechanism [55].
This means that DAE can handle certain types of small
perturbations. To learn a DAE model, we leverage two
kinds of noise: (i) Salt-and-pepper noise (cid:15): A small fraction
of elements of original example x are randomly selected,
and then set their values as their respective minimum or
maximum. (ii) Adversarial perturbation δx: A perturbation
δx is added to x such that classiﬁer f or base classiﬁer fi
misclassiﬁes x(cid:48) = x + δx. Given a training example x over
the feature space X , the risk of a denoising auto-encoder is

Ex∈X [Lae(x, ae(x + (cid:15))) + Lae(x, ae(x(cid:48)))] ,

(9)

min
˜θ,ξ

where Lae : X × X (cid:55)→ R calculates the mean-square error,
the learnable parameters ˜θ and ξ respectively belongs to the
encoder and decoder.

3.2 Turning Principles into A Framework

The principles discussed above guide us to propose
a framework for adversarial malware classiﬁcation and
detection, which is highlighted in Figure 1 and elaborated
below. Speciﬁcally, we ﬁrst examine whether the attacks
have some useful information that could to be incorporated
via a proper preprocessing (according to Principle 1).
We propose using an ensemble fen of classiﬁers {fi}l
i=1
(according to Principle 3), which are trained from random
subspace of the original feature space. Each classiﬁer fi is
hardened by three countermeasures: input transformation
via binarization (according to Principle 4); adversarial
training/regularization models on the attacks using Adam
optimizer (dot arrows in Figure 1, according to Principle 2
and 5); semantics-preservation is achieved via an encoder
and a decoder (according to Principle 6). In order to
attain adversarial training and at the same time semantics-
preservation, we learn classiﬁer fi via block coordinate
descent to optimize different components of the model.

Algorithm 2 integrates all pieces for training individual
classiﬁers. The training procedure consists of the following
steps. (i) Given a training set (X, Y ), we randomly select a

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

7

Fig. 1: Overview of the adversarial malware defense framework. In the training phase, an ensemble of l neural network
classiﬁers are trained, with each classiﬁer hardened by three countermeasures (i.e., input transformation, semantics-
preserving, and adversarial training on the transformed data).

Algorithm 2: Training classiﬁer fi

Input: The training set (X, Y ), number of repeat

times K, epoch Nepoch and mini-batch size N .

1 Select a ratio Λ of sub-features from the feature set;
2 Transform input X to X via binarization;
3 for epoch = 1 to Nepoch do
4

i=1 from the (X, Y );

Sample a mini-batch {xi, yi}N
for k = 1 to K do

i=1;

Apply slight salt-and-pepper noises to
{xi}N
Derive the perturbed representation {x(cid:48)k
via Algorithm 1 using Adam method;

i }N

i=1

end
Select x(cid:48)
maximize the cross-entropy loss;

i from {x(cid:48)k

i }K

k=1 for xi (i = 1, · · · , N ) to

Calculate the reconstruction loss via Eq.(9);
Backpropagate the loss and update the denoising
autoencoder parameters;

Calculate the adversarial training loss via Eq.(6);
Backpropagate the loss and update classiﬁer
parameters;

5

6

7

8

9

10

11

12

13

14 end

i for xi ∈ {xi}N

ratio Λ of sub-features to the feature set, and then transform
X into X via the binarization discussed above. (ii) We
sample a mini-batch {xi, yi}N
i=1 from (X, Y ), and calculate
adversarial examples x(cid:48)
i=1 according to Lines
5-9 in Algorithm 2. (iii) We pass the {x(cid:48)
i=1 through the
denoising auto-encoder to compute the reconstruction loss
with respect to the target {xi}N
i=1 via Eq.(9), and update
the parameters of the denoising auto-encoder. (iv) We pass
i=1 and {xi}N
both the {xi + δxi}N
i=1 through the neural
networks to compute the classiﬁcation error with respect

i}N

to the ground-truth label {yi}N
i=1 via Eq.(6), and update the
parameters of the classiﬁer via backpropagation. Note that
Steps (ii)-(iv) are performed in a loop. The output of the
training algorithm is a neural network classiﬁer.

4 VALIDATING FRAMEWORK VIA DREBIN DATASET

We validate the effectiveness of the framework using the
Drebin dataset of Android malware [56], while considering
11 grey-box attacks and 9 white-box attacks. This dataset
also applied by former studies in the domain of adversarial
malware detection [5], [16], [23], [57].

4.1 Data Pre-Processing

Dataset. The Drebin dataset [56] contains 5,615 malicious
Android packages (APKs), and also provides features of
123,453 benign examples, together with their SHA256 values
but not the examples themselves. All samples were labeled
using the VirusTotal service [58] before the year 2015. An
example was treated as malicious if there are at least two
scanners say it is malicious, and is treated as benign if no
scanners detect it [56]. Because the VirusTotal may update
the detection result along with the time [59], we consider
relabeling the APKs. We download benign applications
corresponding to the given SHA256 values from the APK
markets (e.g., Google Play, AppChina, etc.), and collect
54,829 APKs in total. We send all of these examples (i.e.,
malicious and benign alike) to the VirusTotal service again.
Surprisingly, 12,496 benign APKs are detected as malicious
(rather than benign) by at least one scanners, and most of
them are detected as Adware or Trojan; this suggests that
the original Drebin training set has been contaminated by
the poisoning attack. This may be caused by some of the
following reasons: (i) Virustotal updates the scanners over
the time; (ii) Virustotal updates the report of a ﬁle when
a user requires to rescan the ﬁle; (iii) after an update,

𝐱 𝐱 Voting𝑦 …Sample𝐱 Classifier 𝑓1 Classifier 𝑓1 Target labelsClassifier 𝑓𝑙 Ensemble𝑓𝑒𝑛 𝑓𝑙 Classifier TrainingTestingForwardGradientTraining dataloss functionTransformationTransformeddata…EncoderSemantics-preservationDecoderloss functionTransformationTransformeddata…EncoderSemantics-preservationDecoderpreprocessing…IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

8

the previous report cannot be obtained anymore. We thus
remove these 12,496 benign examples from the original
benign dataset, leaving 42,333 benign APKs. The resulting
dataset contains 5,615 malicious APKs and 42,333 benign
APKs, namely 47,948 examples in total. We randomly
split the dataset into three disjoint sets for training (60%),
validation (20%), and test (20%), respectively.

Feature Extraction. APK is an archive ﬁle containing
AndroidManifest.xml, classes.dex, and others (e.g., res, assets).
The manifest ﬁle describes an APK’s information, such as
the name, version, announcement, library ﬁles used by the
application. The source code is compiled to build the .dex
ﬁle which is understandable by the Java Virtual Machine.
The Drebin dataset has eight subsets of features, including
four subsets of features extracted from AndroidManifest.xml
(denoted by S1, S2, S3, S4, respectively) and four subsets
of
features extracted from the disassembled dexcode
(denoted by S5, S6, S7, S8, respectively). More speciﬁcally,
(i) S1 contains features corresponding to the access of
an APK to the hardware of a smartphone (e.g., camera,
touchscreen, or GPS module); (ii) S2 contains features
corresponding to the permissions requested by the APK
in question; (iii) S3 contains features corresponding to
the application components (e.g., activities, service, receivers,
etc.); (iv) S4 contains features corresponding to the APK’s
communications with the operating system; (v) S5 contains
features corresponding to the critical system API calls,
which cannot run without appropriate permissions or the
root privilege; (vi) S6 contains features corresponding to the
used permissions; (vii) S7 contains features corresponding
to the API calls that can access sensitive data or resources
on a smartphone; (viii) S8 contains features corresponding
to IP addresses, hostnames and URLs found in the
disassembled code.

In order

to extract applications’

features, we use
Androgurad 3.3.5, a reverse engineering toolkit for APK
analysis [60]. Note that 141 APKs cannot be analyzed.
Moreover, a feature selection is conducted to remove
those low-frequency features for the sake of computational
efﬁciency. As a result, we keep 10,000 features with high
frequencies. The APK is mapped on the feature space as a
binary feature vector, where ‘1’ (‘0’) represents the presence
(absence) of a feature in the APK.

4.2 Training Classiﬁers

Classiﬁers. In order to validate the defense framework, we
use and compare ﬁve classiﬁers: (i) the basic DNN with no
effort made to defend adversarial examples; (ii) hardened
DNN incorporating adversarial
training with known
manipulation set (dubbed Adversarial Training), which
manifests Principle 2 (grey-box attacks can be bounded by
the worst-case white-box attack) and Principle 5 (min-max
adversarial training); (iii) hardened DNN incorporating
adversarial regularization because the defender may know
the manipulation set, which is true in
nothing about
the case of AICS’2019 Challenge (dubbed Adversarial
Regularization); (iv) Denoising Auto-Encoder (DAE) based-
classiﬁer, which manifests Principle 6 (semantics-preserving
representations); (v) classiﬁer hardened by both Adversarial
Training and DAE (dubbed AT+DAE); (vi) ensemble of

AT+DAE classiﬁers in the random subspace (manifesting
Principle 3, dubbed Ensemble AT+DAE). For Principle 1
(i.e., knowing your enemy), we will simulate attacks in
the next subsection. Since we use binary feature vector,
Principle 2 (binarization) is not applicable.
Hyper-parameters Setting. We use DNNs with two fully-
connected hidden layers (each layer having 160 neurons)
with ReLU activation function. All classiﬁers are optimized
by using Adam with epochs 150, mini-batch size 128,
and learning rate 0.001. For Adversarial Training,
the
inner maximization is optimized by using Adam with
learning rate 0.02 and iteration steps T = 100 to search
adversarial examples as many as possible. For Adversarial
Regularization, we set the learning rate as 0.01 for Adam
and conduct preliminary experiments to tune a proper
iteration step T . Finally, we set T = 60. We use an ensemble
of 5 base classiﬁers. Our preliminary experiments suggest
us to learn base classiﬁers from the entire training set and
the entire feature set. Unless with special mentioning, all
classiﬁers that require to solve the inner maximization are
trained without random starting points so as to ease the
analysis (i.e., K = 0).

4.3 Attack Experiments and Classiﬁcation Results

is,

the attacker knows the dataset,

We present threat models speciﬁed by whether the attacker
wages grey-box or white-box attacks, and constraints on the
attacker’s manipulation set.
Grey-box vs. White-box Attacks. We consider two attack
scenarios. (i) Grey-box attacks: In this setting, we simulate
the attack scenario of the AICS’2019 Challenge organizers.
That
feature set,
but not the defender’s learning algorithm. The attacker
generates adversarial examples from a surrogate classiﬁer.
We consider a surrogate model of two fully-connected
hidden layers (200 neurons each layer) and learn the model
on the training set. (ii) White-box attacks: In this setting,
the attacker knows everything about the malware detector.
Therefore, the adversarial examples are directly generated
from the corresponding malware detector.
Manipulations Constraints. Given an APK, we consider
both incremental and decremental manipulations. The
incremental manipulation allows the attacker to insert
some objects (e.g., activity) into an APK example to avoid
detection. The decremental manipulation allows the attacker
to hide some objects (e.g., activity) to avoid detection. In any
case, the adversarial example should preserve the malicious
functionality of the malware from which the adversarial
example is generated.

When the attacker uses incremental manipulations, the
attacker can insert some manifest features (e.g., request
extra permissions and hardware, state additional services,
Intent-ﬁlter, etc.). However, some elements are hard to insert,
such as content-provider, because the absence of URI will
corrupt an APK example. With respect to the .dex ﬁle, a
dead function or class (which is never called) containing
speciﬁed system APIs can be injected without destroying
the APK example. The similar means can be performed for
the string injection (e.g., IP address), as well.

public void hideAPI() throws Exception{
// hide ’println’

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

9

String e_str = "ExMLXEZUDw";
// get ’println’
String p_str = decryptStr(e_str);
Class c = java.lang.System.class;
Field f = c.getField("out");
Method m = f.getType().
getMethod(p_str,String.class);
m.invoke(f.get(null), "hello world!");
return void
}

Listing 1: Java code to hide the API method “println”.

When the attacker uses decremental manipulations,
the APK’s information in the xml ﬁles can be changed
(e.g., package name). However, it is impossible to remove
activity entirely because an activity may represent a class
implemented in the .dex code. Nevertheless, we can rename
an activity and change its relevant information (e.g., activity
label), while noting that the related components in the .dex
should be modiﬁed accordingly. The other components (e.g.,
service, provider and receiver) also can be modiﬁed in the
similar fashion, and the resource ﬁles (e.g., images, icons)
can be manipulated as well. In terms of dexcode, the method
names and class names that are deﬁned by developers could
be modiﬁed, too. Note that the corresponding statement,
instantiation, reference, and announcements should be
changed accordingly. Moreover, user-speciﬁed strings can
be obfuscated using encryption and the cipher-text will
be decrypted at running time. Further, the attacker can
hide public and static system APIs using Java reﬂection and
encryption together. This is shown by the example in List 1.
All of the modiﬁcations mentioned above only obfuscate an
APK without changing its functionalities.

One challenge is that the attacker needs to perform
ﬁne-grained manipulations on compiled ﬁles automatically
at scale, while preserving the functionalities of malware
examples. This important because a small change in a
malware example can render the ﬁle unexecutable. Since
Android APIs have upgraded multiple times in the past 5
years, the attacker has to inject compatible APIs into an APK
when manipulating a malware example. The preservation
of malicious functionalities may be estimated by using a
dynamic malware analysis tool, (e.g., Sandbox).

to

Feature

Mapping Manipulations
Space. The
aforementioned manipulations modify static Android
features, such as API calls and components in the manifest
ﬁle. Two kinds of perturbations can be applied to the Drebin
feature space. (i) Feature addition. The attacker can increase
the feature values (e.g., ﬂipping ‘0’ to ‘1’) of appropriate
objects, such as components (e.g., activity), system APIs,
and IP address. (ii) Feature removal. The attacker can ﬂip ‘1’
to ‘0’ by removing or hiding objects (e.g., activity, APIs.)
Table 2 summarizes our manipulations in the Drebin feature
space. We observe that neither feature addition nor feature
removal can be applied to S6 because these features depend
on S2 and S5, meaning that modiﬁcations on S2 or S5 may
cause changes to features in S6.
Evasion Attacks Setting. We randomly select 800 malware
examples from the test set to wage evasion attacks by
using the attack algorithms described in Section 2.3. In the
settings of Random, Grosse, BGA, BCA, and (cid:96)1-PGD attacks,

TABLE 2: Overview of manipulations on feature space,
where (cid:88)((cid:55)) indicates that the feature addition or removal
operation can (cannot) be performed on features in the
corresponding subset.

manifest

dexcode

Feature sets

S1 Hardware
S2 Requested permissions
S3 Application components
S4 Intents

Addition
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Removal
(cid:55)
(cid:55)
(cid:88)
(cid:55)

S5 Restricted API calls
S6 Used permission
S7 Suspicious API calls
S8 Network addresses

(cid:88)
(cid:55)
(cid:88)
(cid:88)

(cid:88)
(cid:55)
(cid:88)
(cid:88)

we iterate these algorithms until reaching a predeﬁned
maximum number of steps, while noting that Grosse, BGA,
and BGA attacks leverage feature addition only. For waging
the Mimicry attack, in order to increase its effectiveness, we
use Nb benign examples to guide the perturbation of a single
malware example, leading to Nb perturbed examples; then,
we select a resulting example such that it causes the mis-
classiﬁcation with the smallest perturbation. Therefore, we
denote this attack as Mimicry×Nb. For other attacks, we set
ε = 1.0 for the FGSM attack. In (cid:96)∞ norm and Adam based
PGD attacks, the step size is α = 0.01 with iterative times
100. The (cid:96)2 norm PGD attack is performed for 100 iterations
with step size 1.0.

Experimental Validation of Functionality. In order to test
whether or not perturbations in the feature space render to
executable ﬁles in the example space, we use Cuckoodroid
[61] to install and run APKs in an Android emulator. Owing
to efﬁciency considerations, we randomly select 10 malware
APKs and generate their perturbed APKs using the PGD-
Adam attack against the Basic DNN model. Among the
10 original (i.e., unperturbed) APKs, all of them can be
loaded but 2 cannot run in the Android emulator. Among
the 10 perturbed examples, all of them can be loaded but
5 of them cannot run (and 2 of these 5 correspond to
the 2 original APKs that cannot run). This means that
more research is needed in order to systematically assure
that perturbation can indeed preserve the functionalities of
malware examples, which is unique to adversarial malware
detection [9], [11].

4.4 Experimental Results

TABLE 3: Effectiveness of the defense framework when
there are no adversarial attacks.

Defense

FNR (%)

FPR (%) Accuracy (%)

Basic DNN
Adversarial Training
Adversarial Regularization
DAE
AT+DAE
Ensemble AT+DAE

3.684
3.246
4.737
3.246
3.246
2.456

0.320
1.777
0.190
0.450
1.694
2.464

99.28
98.05
99.27
99.22
98.12
97.54

The Case of No Adversarial Attacks. Table 3
summarizes the classiﬁcation results on the test set, which
are measured with the standard metrics of False Negative

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

10

TABLE 4: Effectiveness of the defense framework against grey-box adversarial malware evasion attacks.

Attack

No Attack
Random Attack
Mimicry×1
Mimicry×10
FGSM [13]
Grosse [5]
BGA [7]
BCA [7]
PGD-(cid:96)1
PGD-(cid:96)2
PGD-(cid:96)∞
PGD-Adam

Basic DNN Adversarial Training (AT) Adversarial Regularization DAE AT+DAE

Ensemble AT+DAE

Accuracy (%)

96.63
100.0
53.88
35.25
4.00
1.13
0.25
0.25
0.25
58.63
0.25
52.50

97.00
100.0
86.13
85.63
97.50
97.00
100.0
100.0
100.0
100.0
100.0
100.0

95.63
100.0
52.75
34.88
95.88
11.75
71.13
49.50
43.88
99.75
100.0
100.0

96.88
100.0
56.88
52.63
96.88
65.13
100.0
58.00
53.88
100.0
100.0
100.0

96.50
100.0
91.50
85.13
96.75
97.63
100.0
100.0
100.0
100.0
100.0
100.0

97.75
100.0
96.13
89.88
98.00
99.38
100.0
100.0
100.0
100.0
100.0
100.0

TABLE 5: Effectiveness of the defense framework against white-box adversarial malware evasion attacks.

Attack

Mimicry×10
FGSM [13]
Grosse [5]
BGA [7]
BCA [7]
PGD-(cid:96)1
PGD-(cid:96)2
PGD-(cid:96)∞
PGD-Adam

Basic DNN Adversarial Training (AT) Adversarial Regularization DAE AT+DAE

Ensemble AT+DAE

Accuracy (%)

11.63
0.00
0.00
0.00
0.00
0.00
3.00
0.00
1.13

68.25
97.00
60.75
97.00
61.13
69.50
93.63
90.38
95.13

14.88
95.00
16.63
91.50
16.63
21.88
82.13
89.75
89.63

40.88
96.88
35.50
74.00
35.38
51.00
89.75
35.38
88.25

69.13
96.50
81.13
96.50
81.50
81.25
91.13
85.50
92.88

79.75
97.75
91.75
97.50
91.75
88.50
91.63
73.63
90.00

Rate (FNR), False Positive Rate (FPR), and classiﬁcation
Accuracy (i.e., the percentage of the test examples that are
classiﬁed correctly) [62]. We observe that when compared
with the Basic DNN, Adversarial Training achieves a lower
FNR (0.438% lower) but a higher FPR (1.457% higher).
A similar tendency is exhibited by DAE, AT+DAE and
Ensemble AT+DAE. This can be explained as follows: by
injecting adversarial malware examples into the training set,
the learning process makes the model search for malware
examples in a bigger space, explaining the drop in FNR and
increase in FPR and therefore a slightly drop (≤ 1.74%)
in the classiﬁcation accuracy. Adversarial Regularization
achieves a comparable classiﬁcation accuracy as Basic DNN,
but the highest FNR among the classiﬁers we considered.
This is caused by the fact that DNN is regularized using
small perturbations applied to both benign and malicious
samples. In summary, we draw:

Insight 1. In the absence of adversarial attacks, Adversarial
Training and DAE can detect more malware examples than the
Basic DNN (because of their smaller FNR), at the price of a small
side-effect in the FPR and therefore the classiﬁcation accuracy;
Adversarial regularization achieves comparable accuracy as the
Baisc DNN while increasing the FNR.

The Case of Grey-box Attacks. Table 4 reports the
classiﬁcation results of the defense framework against grey-
box attacks. We make the following observations. First,
Basic DNN cannot defend against evasion attacks and
is completely ruined by attacks that
include Mimicry,
FGSM, Grosse, BGA, BCA, PGD-(cid:96)1, and PGD-(cid:96)∞. Second,
Adversarial Training signiﬁcantly enhances the robustness
of DNN, achieving the accuracy of 86.13% and 85.63%

against the Mimicry×1 and Mimicry×10 attack respectively
and a 100% accuracy against the other 6 attacks (i.e.,
BGA, BCA and 4 variants of PGD). Third, Adversarial
Regularization, without seeing any adversarial examples,
can defend against FGSM, PGD-(cid:96)∞, PGD-(cid:96)2 and PGD-
Adam attacks, but are not effective against attacks such
as Grosse, BCA, and PGD-(cid:96)1. A similar phenomenon is
observed for DAE. Nevertheless, when using Adversarial
Training and DAE together, namely AT+DAE, the defense
achieves the highest robustness against evasion attacks than
using Adversarial Training and DAE individually, except
for the Mimicry×10 attack and FGSM attack (encountering
a ∼1% decrease). Fourth, the Ensemble AT+DAE consists
of ﬁve AT+DAE classiﬁers and achieves the highest
robustness among the considered defenses against
the
attacks investigated. In summary, we draw:

Insight 2. Under grey-box attack scenario, Adversarial Training
is an effective defense against evasion attacks; DAE offers some
defense capability that may not be offered by Adversarial Training;
using an ensemble of ﬁve AT+DAE classiﬁers is more effective
than using a single AT+DAE classiﬁer against evasion attacks;
Without knowing the attacker’s manipulation set, Adversarial
Regularization enhances the robustness of Basic DNN but cannot
defend attacks such as Grosse.

The Case of White-box Attacks. Table 5 presents the
classiﬁcation results against white-box attacks. We make the
following observations. (i) All attacks can almost completely
evade Basic DNN, but the Mimicry attack is, relatively
speaking, less effective because this attack leverages less
information about the classiﬁers than what the other attacks
do. (ii) Adversarial Training is effective against the FGSM

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

11

attack, BGA attack and PGD-Adam attack, but not effective
against the Grosse attack, BCA attack, and PGD-(cid:96)1 attack
because these attacks manipulate a few features when
generating adversarial examples and these manipulations
are unlikely perceived by Adversarial Training (owing to the
fact that Adversarial Training penalizes adversarial spaces
searched by Adam optimizer). (iii) As expected, Adversarial
Regularization is less effective than Adversarial Training.
Adversarial Regularization achieves a 91.50% accuracy
against the white-box BGA attack, in contrast to the 71.13%
accuracy against the grey-box BGA attack. This is counter-
intuitive but can be attributed to the fact that Adversarial
Regularization may render some gradient-based methods,
such as BGA, useless, which is a phenomenon known
as gradient-masking [63], [64], [65]. (iv) AT+DAE achieves
considerable robustness against those attacks, with at least
an 81.13% accuracy except for the Mimicry×10 attack,
which defeats the AT+DAE defense because Mimicry can
make adversarial malware examples similar to benign ones
[16]. (v) The ensemble of AT+DAE defense achieves the
highest accuracy against the Mimicry×10, the Grosse attack
and the BCA attack than the other defenses, with about
10% higher accuracy when compared with the AT+DAE
defense. However, the ensemble of AT+DAE achieves lower
accuracy than AT+DAE against the PGD-(cid:96)2 attack, the PGD-
(cid:96)∞ attack, and the PGD-Adam attack. This may be caused
by the fact that the base model AT+DAE cannot effectively
mitigate these attacks. In summary, we draw:

Insight 3. Adversarial Training cannot effectively defend against
white-box attacks that were not considered in the training phase;
DAE can be useful when adversarial training is not effective;
employing ensembles can further improve the robustness against
certain white-box attacks. That is, no defenses can defeat all white-
box attacks effectively.

5 APPLICATION TO AICS’2019 CHALLENGE
WHEN KNOWING NOTHING ABOUT ATTACKS

The challenge is in the context of adversarial malware
classiﬁcation, namely devising evasion-resistant, machine
learning based malware classiﬁers. The dataset, including
both the training set and the test set, consists of feature
vectors extracted from Windows malware examples, each
of which belongs to one of the following ﬁve classes:
Virus, Worm, Trojan, Packed malware, and AdWare. For
each example, the features are collected by the challenge
organizer via dynamic analysis, including the Windows
API calls and further processed unigram, bigram, and
trigram API calls. The feature names (e.g., API calls)
and the class labels are “obfuscated” by the challenge
organizer as integers, while noting the obfuscation preserves
the mapping between the features and the integers
representation of them. For example, three API calls are
represented by three unique integers, say 101, 102, and 103;
then, a trigram API call “101;102;103” means a sequence of
API calls 101, 102, and 103. In total there are 106,428 features.
The test set consists of adversarial examples and non-
adversarial examples (i.e., unperturbed malware examples).
Adversarial examples are generated by a variety of
perturbation methods, which are not known to the

participating teams. However, the ground-truth labels of
the test examples are not given to the participating teams.
This means that the participating teams cannot calculate
the accuracy of their detectors by themselves. Instead, they
need to submit their classiﬁcation results (i.e., labels on the
examples in the test set) to the challenge organizer, who will
calculate the classiﬁcation score of each participating team.
The Challenge organizer decided to use the Macro F1 score
as the classiﬁcation accuracy metric. The Macro F1 score is
the unweighted mean of the F1 score [66] for each class of
objects in question (i.e., type of malware in this case). The
ﬁnal score is the Harmonic mean upon the two Macro F1
scores, namely the one for the adversarial examples in the
test data and the other for the non-adversarial examples in
the test data. Given these two numbers, say a1 and a2, their
harmonic mean 2a1a2
a1+a2

.

5.1 Basic Analysis of the AICS’2019 Challenge

Is the Training Set Imbalanced? The training set consists of
12,536 instances, and the test set consists of 3,133 instances.
The training set contains 8,678 instances in class ‘0’, 1,883
instances in class ‘1’, 771 instances in class ‘2’, 692 instances
in class ‘3’, and 512 instances in class ‘4’. We can calculate
the maximum ratio between the number of instances in
different classes is 16.95, indicating that the training set is
highly imbalanced. In order to cope with the imbalance in
the training set, we use the Oversampling method to replicate
randomly selected feature vectors from a class with a small
number of feature vectors. The replication process ends until
the number of feature vectors is comparable to that of the
largest class (i.e., the class with the largest number of feature
vectors), where “comparable” is measured by a predeﬁned
ratio. In order to see the effect of this ratio, we use a 5-fold
cross validation on the training set to investigate the impact
of this ratio. The classiﬁer consists of neural networks with
two fully-connected layers (each layer having 160 neurons
with the ReLU activation function), which are optimized
via Adam with epochs 50, mini-batch size 128, learning
rate 0.001. The model is selected when achieving the best
Macro F1 score on the validation set. Table 6 shows that
the Macro F1 score decreases as the oversampling ratio of
minority classes increases. In order to make each mini-batch
of training data contain examples from all classes, which
would be critical in multiclass classiﬁcation, our experience
suggests us to select the 30% ratio.

TABLE 6: Accuracy (%) and Macro F1 score (%) are reported
with a 95% conﬁdence interval with respect to the ratio
parameter (%), where ‘—’ means learning a classiﬁer using
the original training set.

Ratio (%) Accuracy (%) Macro F1 (%)

—
30
40
50
60

93.20±1.04
92.86±0.75
92.38±1.00
92.21±0.60
92.48±1.12

85.52±1.12
85.47±1.04
84.87±1.07
84.87±1.00
84.62±1.01

Are There Simple Indicators of Adversarial Examples?
In the ﬁrst test set published by the challenge organizer,
we see negative values for some features. These negative

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

12

values would indicate that they are adversarial examples.
In the revised test set provided by the challenge organizer,
there are no negative feature values, meaning that there are
no simple ways to tell whether an example is adversarial
or not. In spite of this, we can speculate on the count of
perturbed features by comparing the number of nonzero
entries corresponding to feature vectors in the training set
and feature vectors in the test set. Figure 2 shows the
normalized frequency of the number of nonzero entries of
feature vectors in the training set vs. test set. We observe that
their normalized frequencies are similar except that some
test examples have more nonzero entries. Their mean values
are close and are much smaller than the input dimension
(106, 428), suggesting that the average degree of perturbed
features may be small.

Fig. 2: Histogram of
the
number of nonzero entries of feature vectors in the training
set vs. test set. The dashed line represents the mean value.

the normalized frequency of

5.2 Classiﬁcation Results: Challenge Winner

We train 10 deep neural network classiﬁers to formulate
an ensemble model,
including 4 classiﬁers using the
binarization, adversarial regularization, and semantics-
preservation techniques discussed in the framework, and
the other 6 classiﬁers using the binarization and adversarial
regularization techniques because examples are perturbed
without preserving their malicious functionality in the
training. Since we do not have access to the malware
examples, we cannot tell whether a feature perturbation
preserves the malware functionality or not. The inner
maximization performed by using gradient descent with
respect to the transformed input iterates T = 55 times
via the Adam optimizer [47] with learning rate 0.01. We
leverage the random start points and K = 5. The ratio for
ensemble of random subspace method is set as Λ = 0.5.
Each base classiﬁer has two fully-connected hidden layers
(each layer having neurons 160), uses the ELU activation
function, and is optimized by Adam. The ensemble achieves
a Macro F1 score of 88.30% upon non-attack dataset, a 63.0%
Macro F1 score under attacks, and a Harmonic mean on both
Macro F1 scores of 73.60%. This is the highest Harmonic
Mean score among the participating teams. Although this
score is not ideal, this may be inherent to the fact that we as
the defender do not know any information about the attack.
This leads to:

Insight 4. The information “barrier” that the defender does not
know the attacker’s manipulation set is a fundamental one because
the attacker may use adversarial malware examples that are far
away from what the defender would use to train its defense model.

6 APPLICATION TO AICS’2019 CHALLENGE
AFTER KNOWING GROUND TRUTH

After the Challenge organizer announced that we won the
Challenge, the ground-truth labels of the test set are released
so that we can conduct further study. We stress that we still
do not know the attacks that were used by the Challenge
organizer.

6.1 Training Classiﬁers

Classiﬁer. We consider and compare ﬁve classiﬁers: (i) Basic
DNN without incorporating any defense; (ii) hardened
DNN incorporating the binarization technique [25] (dubbed
Binarization); (iii) hardened DNN incorporating adversarial
regularization
Regularization);
(dubbed Adversarial
(iv) hardened DNN incorporating Binarization and
Adversarial Regularization (dubbed Binarization+AR);
(v) an ensemble of Binarization+AR classiﬁers (dubbed
Ensemble Binarization+AR).

Hyper-parameter Settings. All of the DNNs we use have
two fully-connected hidden layers (each layer having
160 neurons), use the ReLU activation function, and are
optimized by Adam with epochs 30, mini-batch size 128,
and learning rate 0.001. For Adversarial Regularization, we
perform the inner maximization via Adam (with learning
rate 0.01). Our preliminary experiments suggest us to set
iterations T = 60. The starting point is chosen from K = 5
initialized points with salt-and-pepper noises, which have
a noise ratio (cid:15)r chose uniformly at random from 0 to
(cid:15)r
max = 10%. This means at most 10% of the features can be
changed by salt-and-pepper noises in each training round.
For the ensemble, we train 5 Binarization+AR classiﬁers,
each of which is learned from an 80% data randomly
selected from the training set, with a Λ = 0.5 fraction
of features. We augment the training set for the last three
classiﬁers as described in Section 5.1.

6.2 Classiﬁcation Results

Table 7 presents the results with and without adversarial
attacks. We make three observations.
(i) Adversarial
Regularization signiﬁcantly improves the Macro F1 score
against the attacks when compared with the Basic DNN
(a 23.93% higher Macro F1 score). The Macro F1 score of
Adversarial Regularization in the absence of adversarial
attacks drops slightly when compared with the Basic DNN
(≈ 1%). (ii) By comparing Binarization (row 2) and the Basic
DNN, Binarization can improve the robustness of DNN
against adversarial attacks a little bit (a 0.47% increase in the
Macro F1 score). (iii) Ensemble Binarization+AR achieves a
higher classiﬁcation accuracy than Binarization+AR, in the
presence or absence of adversarial attacks.

Sensitivity.
In
max is crucial and is

Hyper-parameters
Adversarial
Regularization, (cid:15)r
set manually.
Intuitively, a greater (cid:15)r
max lets the defense perceive a
larger space, but inhibiting the convergence of training.
In addition, we want to know whether the oversampling
is useful or not for Adversarial Regularization. We thus
conduct a group of experiments to justify these settings.
Table 8 shows the experimental results. We observe that

030060090012001500Number of nonzero entry0.0000.0010.0020.0030.0040.005Normalized frequencyMean: 503.92Training set030060090012001500Number of nonzero entryMean: 556.72Testing setIEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

13

TABLE 7: Classiﬁers Accuracy (%) and Macro F1 score (%) with no attacks vs. using grey-box adversarial evasion attacks
respectively, and the Harmonic mean (%) of the two Macro F1 scores.

Classiﬁers

No attacks (%)

Attacks (%)

Accuracy Macro F1 Accuracy Macro F1

Harmonic mean (%)

Basic DNN
Binarization
Adversarial Regularization (AR)
Binarization+AR
Ensemble Binarization+AR

96.24
95.80
95.66
95.62
95.93

88.91
87.99
87.98
87.87
88.58

63.46
63.79
72.02
75.22
76.02

35.00
35.47
58.93
59.87
62.95

50.23
50.56
70.58
71.22
73.60

TABLE 8: Accuracy (%) and Macro F1 score (%) of
Adversarial Regularization in the absence vs. presence of
adversarial evasion attacks, with respect to the maximum
salt-and-pepper noise ratio (cid:15)r
max, where ∗ means that a
classiﬁer is learned using oversampling.

Noise Ratio (%)

No attacks (%)

Attacks (%)

Accuracy Macro F1 Accuracy Macro F1

(cid:15)r
max = 0
(cid:15)r
max = 0.1
(cid:15)r
max = 1
(cid:15)r
max = 10
(cid:15)r
max = 20
max = 10∗
(cid:15)r

96.11
95.93
96.24
96.19
96.06
95.66

88.43
88.10
89.14
88.46
88.14
87.98

69.68
74.00
73.11
77.11
75.11
72.02

49.87
50.52
55.98
56.22
51.23
58.93

the Macro F1 score in the presence of adversarial evasion
attacks increase with the increase of (cid:15)r
max from 0% to
10%. Meanwhile, Accuracy and Macro F1 score do not
decrease in the absence of adversarial evasion attacks, and
actually slightly increase at (cid:15)r
max = 1%. Furthermore, when
the oversampling technique is leveraged at (cid:15)r
max = 10%
(the last row), both Accuracy and Macro F1 score in the
absence of adversarial evasion attacks decrease slightly
(< 1%). Nevertheless, the Macro F1 score in the presence
of adversarial evasion attacks increases from 56.22% to
58.93%. This leads us to draw:

Insight 5. Oversampling is not necessary when there are no
adversarial evasion attacks, but improves the effectiveness of
Adversarial Regularization against adversarial evasion attacks in
terms of macro F1 score.

6.3 Retrospective Analysis of the AICS’2019 Challenge

Fig. 3: Cross entropy loss of the classiﬁer hardened by
Adversarial Regularization over the training set, the test set
with no adversarial evasion attacks, and the test set with
adversarial evasion attacks.

Figure 3 demonstrates that adversarial regularization
over-ﬁts the perturbations searched by the inner maximizer
unexpectedly. We observe that
the cross-entropy loss
induced by the perturbations increases signiﬁcantly after
about 10 epochs. Meanwhile, the cross-entropy loss on the
test set with no adversarial evasion attacks changes slightly,
until the number of epochs approaches 100. This means that
the DNN will memorize the perturbations produced in the
training phase, leading to poor generalization. Therefore,
new defense strategies are needed in order to achieve a
much higher accuracy against the Challenge instances. This
suggests:

Insight 6. Adversarial regularization triggers the over-ﬁtting
issue; Without knowing the manipulation set, unsupervised
learning may play an important role because unsupervised
defenses are devised without using label information about the
perturbed examples.

7 CONCLUSION
We have presented six principles for enhancing the
robustness of neural network classiﬁers against adversarial
evasion attacks in the setting of malware classiﬁcation.
These principles guided us to design a framework, which
is validated via a real-world dataset and the AICS’2019
Challenge. We drew a number of insights that are useful
for real-world defenders.

[68],

We hope this paper will

inspire more research into
this important problem. Future research problems are
abundant, such as the following. First, the adversarial
training in our study is applied to feature representations
satisfying box-constraints (in a discrete space). How should
we accommodate other kinds of feature extractions such
as graph-based or sequential-like ones [67],
[69],
[70]? One possible approach is to instantiate the minmax
adversarial training using a generic method, which does
not need to know the special knowledge of the hardened
model. Second,
it is imperative to generate adversarial
malware examples in an end-to-end fashion, assuring
that a perturbed malware example indeed preserves the
functionality of the original, unperturbed malware example.
Third, it is an open problem to adapt the provable or
certiﬁed defense [71] into the context of adversarial malware
detection because it is not clear how one should deﬁne
convex manipulation sets for perturbing malware examples.
Unlike the image data where (cid:96)p-norm may quantify visual
semantics, (cid:96)p-norm cannot characterize the functionalities
of malware examples. Fourth, what are the other principles
that can be leveraged to defend against adversarial malware
examples?

100101102Number of Epoch0.00.20.40.60.81.01.2Cross EntropyTrainingTest w/ no attacksTest w/ attacksIEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

14

REFERENCES

[1] D. Li, Q. Li, Y. Ye, and S. Xu, “Enhancing robustness of deep
neural networks against adversarial malware samples: Principles,
framework, and aics’2019 challenge,” CoRR, vol. abs/1812.08108,
2018. [Online]. Available: http://arxiv.org/abs/1812.08108
Symantec.
https://www.symantec.com/security-center/threat-report

(2018) Symantec @ONLINE.

[Online]. Available:

[2]

[3] CISCO.

(2018) Cisio @ONLINE.

[Online]. Available: https:

//www.cisco.com

[4] Y. Ye, T. Li, D. A. Adjeroh, and S. S. Iyengar, “A survey on malware
detection using data mining techniques,” ACM Comput. Surv.,
vol. 50, no. 3, pp. 41:1–41:40, 2017.

[5] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and
P. McDaniel, “Adversarial examples for malware detection,” in
European Symposium on Research in Computer Security.
Springer,
2017, pp. 62–79.

[6] L. Chen, Y. Ye, and T. Bourlai, “Adversarial machine learning
in malware detection: Arms race between evasion attack and
defense,” in EISIC’2017, 2017, pp. 99–106.

[7] A. Al-Dujaili, A. Huang, E. Hemberg, and U.-M. O’Reilly,
“Adversarial deep learning for robust detection of binary encoded
malware,” in 2018 IEEE Security and Privacy Workshops (SPW).
IEEE, 2018, pp. 76–82.
S. Hou, Y. Ye, Y. Song, and M. Abdulhayoglu, “Make evasion
harder: An intelligent android malware detection system,” in
Proceedings of the Twenty-Seventh IJCAI, 2018, pp. 5279–5283.

[8]

[9] F. Pierazzi, F. Pendlebury,

J. Cortellazzi, and L. Cavallaro,
“Intriguing properties of adversarial ml attacks in the problem
space,” in 2020 IEEE Symposium on Security and Privacy (SP).
IEEE,
2020, pp. 1332–1349.

[10] D. Li and Q. Li, “Adversarial deep ensemble: Evasion attacks and
defenses for malware detection,” IEEE Transactions on Information
Forensics and Security, vol. 15, pp. 3886–3900, 2020.

[11] Y. Kucuk and G. Yan, “Deceiving portable executable malware
classiﬁers into targeted misclassiﬁcation with practical adversarial
examples,” in Proceedings of the Tenth ACM Conference on Data and
Application Security and Privacy, 2020, pp. 341–352.

[12] C. Szegedy, W. Zaremba,

J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus, “Intriguing properties of neural
networks,” arXiv preprint arXiv:1312.6199, 2013.

I. Sutskever,

[13] I.

J. Goodfellow,

J. Shlens, and C. Szegedy, “Explaining
and harnessing adversarial examples (2014),” arXiv preprint
arXiv:1412.6572.

[14] A. C. Serban and E. Poll, “Adversarial examples-a complete
preprint

phenomenon,”

arXiv

the

of

characterisation
arXiv:1810.01185, 2018.

[15] D. Li, Q. Li, Y. Ye, and S. Xu, “Sok: Arms race in adversarial

malware detection,” arXiv preprint arXiv:2005.11671, 2020.

[16] A. Demontis, M. Melis, B. Biggio, D. Maiorca, D. Arp, K. Rieck,
I. Corona, G. Giacinto, and F. Roli, “Yes, machine learning can be
more secure! a case study on android malware detection,” IEEE
Transactions on Dependable and Secure Computing, vol. 16, no. 4, pp.
711–724, July 2019.

[17] M. Nazir.
@ONLINE.
pub releases/2019-01/uota-uwg011819.php

(2019) Utsa wins global cyber security challenge
[Online]. Available: https://www.eurekalert.org/

[18] B. Biggio, G. Fumera, and F. Roli, “Multiple classiﬁer systems for
robust classiﬁer design in adversarial environments,” International
Journal of Machine Learning and Cybernetics, vol. 1, no. 1-4, pp. 27–
41, 2010.

[19] ——, “Multiple classiﬁer systems under attack,” in International
Workshop on Multiple Classiﬁer Systems. Springer, 2010, pp. 74–83.
[20] C. Smutz and A. Stavrou, “When a tree falls: Using diversity in
ensemble classiﬁers to identify evasion in malware detectors.” in
NDSS, 2016.

[21] J. W. Stokes, D. Wang, M. Marinescu, M. Marino, and B. Bussone,
“Attack and defense of dynamic analysis-based, adversarial
neural malware detection models,” in MILCOM 2018 - 2018 IEEE
Military Communications Conference (MILCOM), 2018, pp. 1–8.
[22] Q. Wang, W. Guo, K. Zhang, and et al., “Adversary resistant deep
neural networks with an application to malware detection,” in
Proceedings of the 23rd KDD. ACM, 2017, pp. 1145–1153.

[23] D. Li, R. Baral, T. Li, H. Wang, Q. Li, and S. Xu, “Hashtran-
dnn: A framework for enhancing robustness of deep neural
networks against adversarial malware samples,” arXiv preprint
arXiv:1809.06498, 2018.

[24] L. Chen, S. Hou, Y. Ye, and S. Xu, “Droideye: Fortifying security
of learning-based classiﬁer against adversarial android malware
attacks,” in FOSINT-SI’2018, 2018, pp. 253–262.

[25] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting
arXiv

in deep neural

networks,”

adversarial
preprint:1704.01155, 2017.

examples

[26] L. Xu, Z. Zhan, S. Xu, and K. Ye, “An evasion and counter-
evasion study in malicious websites detection,” in CNS, 2014 IEEE
Conference on.

IEEE, 2014, pp. 265–273.

[27] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples
in the physical world,” arXiv preprint arXiv:1607.02533, 2016.
[28] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
“Towards deep learning models resistant to adversarial attacks,”
arXiv preprint arXiv:1706.06083, 2017.

[29] H. Drucker

“Improving generalization
performance using double backpropagation,” IEEE Transactions on
Neural Networks, vol. 3, no. 6, pp. 991–997, 1992.

and Y. Le Cun,

[30] C. Lyu, K. Huang, and H.-N. Liang, “A uniﬁed gradient
regularization family for adversarial examples,” in Proceedings of
the 2015 IEEE International Conference on Data Mining (ICDM),
ser. ICDM ’15. Washington, DC, USA: IEEE Computer Society,
2015, pp. 301–309.
[Online]. Available: http://dx.doi.org/10.
1109/ICDM.2015.84

[31] T. Miyato, A. M. Dai, and I. Goodfellow, “Adversarial training
methods for semi-supervised text classiﬁcation,” arXiv preprint
arXiv:1605.07725, 2016.

[32] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,
“Stacked denoising autoencoders: Learning useful representations
in a deep network with a local denoising criterion,” Journal of
machine learning research, vol. 11, no. Dec, pp. 3371–3408, 2010.
[33] D. Meng and H. Chen, “Magnet: a two-pronged defense against

adversarial examples,” pp. 135–147, 2017.

[34] M.

Nazir.

challenge
problem. [Online]. Available: http://www-personal.umich.edu/
∼arunesh/AICS2019/challenge.html

workshop

(2019)

2019

Aics

[35] I. C. B. Biggio and D. M. et al., “Evasion attacks against machine
learning at test time,” in Machine Learning and Knowledge Discovery
in Databases: European Conference. Springer, 01 2013, pp. 387–402.
[36] P. L. Nedim rndic, “Practical evasion of a learning-based classiﬁer:
A case study,” in Security and Privacy (SP), 2014 IEEE Symposium
on.

IEEE, 2014, pp. 197–211.

[37] I. Rosenberg, A. Shabtai, L. Rokach, and Y. Elovici, “Generic black-
box end-to-end attack against rnns and other calls based malware
classiﬁers,” arXiv preprint, 2017.

[38] L. Chen, S. Hou, and Y. Ye, “Securedroid: Enhancing security
of machine learning-based detection against adversarial android
malware attacks,” in ACSAC. USA: ACM, 2017, pp. 362–372.
[39] H. Dang, Y. Huang, and E.-C. Chang, “Evading classiﬁers by

morphing in the dark,” in CCS. ACM, 2017, pp. 119–133.

[40] H. S. Anderson, A. Kharkar, B. Filar, and P. Roth, “Evading

machine learning malware detection,” Black Hat, 2017.

[41] W. Xu, Y. Qi, and D. Evans, “Automatically evading classiﬁers: A
case study on pdf malware classiﬁers,” in NDSS, January 2016.
[42] W. Hu and Y. Tan, “Generating adversarial malware examples for

black-box attacks based on gan,” 02 2017.

[43] N. Carlini and D. Wagner, “Towards evaluating the robustness of
neural networks,” in 2017 38th IEEE Symposium on Security and
Privacy (SP).

IEEE, 2017, pp. 39–57.

[44] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik,
and A. Swami, “The limitations of deep learning in adversarial
settings,” in Security and Privacy (EuroS&P), 2016 IEEE European
Symposium on.

IEEE, 2016, pp. 372–387.

[45] O. Suciu, R. Marginean, Y. Kaya, H. D. III, and T. Dumitras,
“When does machine learning FAIL? generalized transferability
for evasion and poisoning attacks,” in 27th USENIX Security
Symposium (USENIX Security 18).
Baltimore, MD: USENIX
Association, Aug. 2018, pp. 1299–1316.

[46] F. Tram`er and D. Boneh, “Adversarial training and robustness
for multiple perturbations,” CoRR, vol. abs/1904.13000, 2019.
[Online]. Available: http://arxiv.org/abs/1904.13000

[47] D. P. Kingma and J. Ba, “Adam: A method for stochastic

optimization,” CoRR, vol. abs/1412.6980, 2014.

[48] E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, and
C. Nicholas, “Malware detection by eating a whole exe,” arXiv
preprint arXiv:1710.09435, 2017.

[49] Z.-H. Zhou, Ensemble methods: foundations and algorithms. CRC

press, 2012.

IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING

15

[50] T. K. Ho, “The random subspace method for constructing decision
forests,” IEEE Transactions on PAMI, vol. 20, no. 8, pp. 832–844,
1998.

[51] A. Demontis, M. Melis, M. Pintor, M. Jagielski, B. Biggio, A. Oprea,
C. Nita-Rotaru, and F. Roli, “Why do adversarial attacks transfer?
explaining transferability of evasion and poisoning attacks,” in
28th {USENIX} Security Symposium ({USENIX} Security 19), 2019,
pp. 321–338.

[52] E. Grefenstette, R. Stanforth, B. O’Donoghue,

J. Uesato,
G. Swirszcz, and P. Kohli, “Strength in numbers: Trading-off
robustness and computation via adversarially-trained ensembles,”
CoRR, vol. abs/1811.09300, 2018.
[Online]. Available: http:
//arxiv.org/abs/1811.09300

[53] L. Schott, J. Rauber, M. Bethge, and W. Brendel, “Towards the
ﬁrst adversarially robust neural network model on mnist,” arXiv
preprint arXiv:1805.09190, 2018.

[54] G. Alain and Y. Bengio, “What regularized auto-encoders learn
from the data-generating distribution,” The Journal of Machine
Learning Research, vol. 15, no. 1, pp. 3563–3593, 2014.

[55] T. Luong, H. Pham, and C. D. Manning, “Effective approaches to
attention-based neural machine translation,” in Proceedings of the
2015 Conference on Empirical Methods in Natural Language Processing,
2015, pp. 1412–1421.

[56] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, K. Rieck,
and C. Siemens, “Drebin: Effective and explainable detection of
android malware in your pocket.” in Ndss, vol. 14, 2014, pp. 23–
26.

[57] A. Demontis, M. Melis, M. Pintor, M. Jagielski, B. Biggio, A. Oprea,
C. Nita-Rotaru, and F. Roli, “Why do adversarial attacks transfer?
explaining transferability of evasion and poisoning attacks,” in
28th USENIX Security Symposium (USENIX Security 19).
Santa
Clara, CA: USENIX Association, Aug. 2019, pp. 321–338.

[58] (2018, May) Virustotal.

[Online]. Available: https://www.

virustotal.com

[59] M. Sebasti´an, R. Rivera, P. Kotzias, and J. Caballero, “Avclass:
A tool for massive malware labeling,” in Research in Attacks,
Intrusions, and Defenses. Cham: Springer International Publishing,
2016, pp. 230–253.

[60] A. Desnos. (2019) Androguard @ONLINE. [Online]. Available:

https://github.com/androguard/androguard

[61] I. Revivo and O. Caspi, “Cuckoodroid,” in Black Hat USA, Las

Vegas, NV, Jul. 2017.

[62] M. Pendleton, R. Garcia-Lebron, J.-H. Cho, and S. Xu, “A survey
on systems security metrics,” ACM Comput. Surv., vol. 49, no. 4,
pp. 1–35, Dec. 2016.

[63] A. Athalye, N. Carlini, and D. A. Wagner, “Obfuscated gradients
give a false sense of security: Circumventing defenses to
adversarial examples,” CoRR, vol. abs/1802.00420, 2018.

[64] F. Tram`er, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh,
and P. McDaniel, “Ensemble adversarial training: Attacks and
defenses,” arXiv preprint arXiv:1705.07204, 2017.

[65] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik,
and A. Swami, “Practical black-box attacks against deep learning
systems using adversarial examples,” arXiv preprint, 2016.

[66] Y. Sasaki et al., “The truth of the f-measure,” Teach Tutor mater,

vol. 1, no. 5, pp. 1–5, 2007.

[67] S. Cui, B. Xia, T. Li, M. Wu, D. Li, Q. Li, and H. Zhang,
“Simwalk: Learning network latent representations with social
relation similarity,” vol. 2018-January, 2017, pp. 1 – 6. [Online].
Available: http://dx.doi.org/10.1109/ISKE.2017.8258804

[68] Y. Fan, S. Hou, Y. Zhang, Y. Ye, and M. Abdulhayoglu, “Gotcha -
sly malware!: Scorpion A metagraph2vec based malware detection
system,” in Proceedings of KDD’2018, 2018, pp. 253–262.

[69] S. Cui, T. Li, S.-C. Chen, M.-L. Shyu, Q. Li, and H. Zhang,
“Disl: Deep isomorphic substructure learning for network
representations,” Knowledge-Based Systems, vol. 189, p. 105086,
2020.

[70] S. Cui, Q. Li, and S.-C. Chen, “An adversarial learning approach
for discovering social relations in human-centered information
networks,” EURASIP Journal on Wireless Communications and
Networking, vol. 2020, no. 1, pp. 1–19, 2020.

[71] M. Balunovic and M. Vechev, “Adversarial training and provable
defenses: Bridging the gap,” in International Conference on Learning
Representations, 2020.

