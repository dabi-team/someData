c(cid:13)2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.

An Adversarial Approach for Explainable AI in
Intrusion Detection Systems

Daniel L. Marino, Chathurika S. Wickramasinghe, Milos Manic
Department of Computer Science
Virginia Commonwealth University
Richmond, USA
marinodl@vcu.edu, misko@ieee.org

8
1
0
2

v
o
N
8
2

]

G
L
.
s
c
[

1
v
5
0
7
1
1
.
1
1
8
1
:
v
i
X
r
a

Abstract—Despite the growing popularity of modern machine
learning techniques (e.g. Deep Neural Networks) in cyber-security
applications, most of these models are perceived as a black-box
for the user. Adversarial machine learning offers an approach
to increase our understanding of these models. In this paper
we present an approach to generate explanations for incorrect
classiﬁcations made by data-driven Intrusion Detection Systems
(IDSs) An adversarial approach is used to ﬁnd the minimum
modiﬁcations (of the input features) required to correctly classify
a given set of misclassiﬁed samples. The magnitude of such
modiﬁcations is used to visualize the most relevant features
that explain the reason for the misclassiﬁcation. The presented
methodology generated satisfactory explanations that describe
the reasoning behind the mis-classiﬁcations, with descriptions
that match expert knowledge. The advantages of the presented
methodology are: 1) applicable to any classiﬁer with deﬁned
gradients. 2) does not require any modiﬁcation of the classiﬁer
model. 3) can be extended to perform further diagnosis (e.g.
vulnerability assessment) and gain further understanding of
the system. Experimental evaluation was conducted on the
NSL-KDD99 benchmark dataset using Linear and Multilayer
perceptron classiﬁers. The results are shown using intuitive
visualizations in order to improve the interpretability of the
results.

Index Terms—Adversarial Machine Learning, Adversarial

samples, Explainable AI, cyber-security,

I. INTRODUCTION

The increasing incorporation of Cyber-based methodologies
for monitoring and control of physical systems has made
critical infrastructure vulnerable to various cyber-attacks such
as: interception, removal or replacement of information, pen-
etration of unauthorized users and viruses [1] [2] [3] [4].
Intrusion detection systems (IDSs) are an essential tool to
detect such malicious attacks [5].

Extendibility and adaptability are essential requirements for
an IDS [6]. Every day, new strains of Cyber-attacks are created
with the objective of deceiving these systems. As a result,
machine learning and data-driven intrusion detection systems
are increasingly being used in cyber-security applications [6].
Machine learning techniques such as deep neural networks
have been successfully used in IDSs [6] [7]. However, these
techniques often work as a black-box model for the user. [8]
[7].

With machine learning being increasingly applied in the
operation of critical systems, understanding the reason behind

Accepted version of the paper appearing in the proceedings of the 44th
Annual Conference of the IEEE Industrial Electronics Society, IECON 2018.

Fig. 1: Explainable AI [9] is concerned with designing of interfaces
that help users to understand the decisions made by a trained machine
learning model.

to the point

the decisions made by a model has become a common
requirement
that governments are starting to
include it
into legislation [10] [11]. Recent developments
in the machine learning community have been focused in
the development of methods which are more interpretable
for the users. Explainable AI (Figure 1) [8] makes use of
visualizations and natural language descriptions to explain the
reasoning behind the decisions made by the machine learning
model.

It is crucial that the inner workings of data-driven models
are transparent for the engineers designing IDSs. Decisions
presented by explainable models can be easily interpreted by
a human, simplifying the process of knowledge discovery.
Explainable approaches help on diagnosing, debugging, and
understanding the decisions made by the model, ultimately
increasing the trust on the data-driven IDS.

In the case of data-driven IDS, when the model is presented
with new attacks where data is not available, the model might
mis-classify an attack as normal, leading to a breach in the
system. Understanding the reasons behind the misclassiﬁcation
of particular samples is the ﬁrst step for debugging and
diagnosing the system. Providing clear explanations for the
cause of misclassiﬁcation is essential to decide which steps to
follow in order to prevent future attacks.

In this paper, we present an explainable AI interface for
diagnosing data-driven IDSs. We present a methodology to
explain incorrect classiﬁcations made by the model following
an adversarial approach.

Although adversarial machine learning is usually used to

 
 
 
 
 
 
Fig. 2: Overview of the presented explainable interface. The approach provides explanations for misclassiﬁed samples. An adversarial
approach is followed to modify the misclassiﬁed samples until the model assigns the correct class. This approach provides a mechanism to
understand the decision boundaries of the classiﬁer.

deceive the classiﬁer, in this paper we use it to generate
explanations by ﬁnding the minimum modiﬁcations required
in order to correctly classify the misclassiﬁed samples. The
difference between the original and modiﬁed samples provide
information of the relevant features responsible for the mis-
classiﬁcation. We show the explanations provide satisfactory
insights behind the reasoning made by the data-driven models,
being congruent with the expert knowledge of the task.

The rest of the paper is organized as follows: Section II
presents an overview of adversarial machine learning; Section
III describes the presented methodology for explainable IDS
systems. Section IV describes the experimental results carried
out using the NSL-KDD99 intrusion detection benchmark
dataset. Section V concludes the paper.

II. ADVERSARIAL MACHINE LEARNING

Adversarial machine learning has been extensively used in
cyber-security research to ﬁnd vulnerabilities in data-driven
models [12] [13] [14] [15] [16] [17]. Recently, there has
been an increasing interest on adversarial samples given the
susceptibility of Deep Learning models to these type of attacks
[18] [19].

Adversarial samples are samples crafted in order to change
the output of a model by making small modiﬁcations into a
reference (usually real) sample [18]. These samples are used
to detect blind spots on ML algorithms.

Adversarial samples are crafted from an attacker perspec-
tive to evade detection, confuse the classiﬁer [18], degrade
performance [20] and/or gain information about the model or
the dataset used to train the model [17]. Adversarial samples
are also useful from a defender point of view given that they
can be used to perform vulnerability assessment [16], study
the robustness against noise, improve generalization and debug
the machine learning model [20].

In general, the problem of crafting an adversarial sample is

stated as follows [17]:

max
ˆx
s.t.

L(ˆx) − Ω(ˆx)

ˆx ∈ φ(x)

(1)

where:

• L(x) measures the impact of the adversarial sample in

the model,

• Ω(x) measures the capability of the defender to detect

the adversarial sample.

• ˆx ∈ φ(x) ensures the crafted sample ˆx is inside the
domain of valid inputs. It also represents the capabilities
of the adversary.

The objective of Eq. (1) can be interpreted as crafting
an attack point ˆx that maximizes the impact of the attack,
while minimizing the chances of the attack to be detected.
The deﬁnition of L(x) will depend on the intent of the
attack and the available information about the model under
attack. For example: 1) L(x) can be a function that measures
the difference between a target class ˆy and the output from
the model; 2) Ω(x) measures the discrepancy between the
reference x0 and the modiﬁed sample ˆx

In this paper, instead of deceiving the classiﬁer, we use
Equation 1 to ﬁnd the minimum number of modiﬁcations
needed to correctly classify a misclassiﬁed sample. The
methodology is described in detail in section III.

III. EXPLAINING MISCLASSIFICATIONS USING
ADVERSARIAL MACHINE LEARNING

In this paper we are interested in generating explanations
for incorrect estimations made by a trained classiﬁer. Figure 2
presents an overview of the presented explainable interface.
The presented methodology modiﬁes a set of misclassiﬁed
samples until they are correctly classiﬁed. The modiﬁcations
are made following an adversarial approach: ﬁnding the min-
imum modiﬁcations required to change the output of the
model. The difference between the modiﬁed samples and the
original real samples is used to explain the output from the

classiﬁer, illustrating the most relevant features that lead to the
misclassiﬁcation.

In the following sections we explain in detail each compo-

nent of the presented explainable interface.

A. Data-driven classiﬁer

The classiﬁer p (y = k|x, w) estimates the probability of
a given sample x to belong to class k. The classiﬁer is
parameterized by a set of parameters w that are learned
from data. Learning is performed using standard supervised
learning. Given a training dataset D = (cid:8)(x(i), y(i))(cid:9)M
of
M samples, the parameters w of the model are obtained by
minimizing the cross-entropy:

i

w∗ = arg min

w

M
(cid:88)

i

(cid:16)

H

y(i), p

(cid:16)

y|x(i), w

(cid:17)(cid:17)

and minimum values found in the training dataset. This
constraint ensures that the adversarial example is inside the
domain of the data distribution.

The class ˆy of the adversarial sample is speciﬁed by the
user. Note that y (cid:54)= ˆy, i.e. the class of the real sample is
different from the class of the adversarial sample.

The optimization problem in Eq. 2 provides a clear objective
to solve. However, the problem as stated in Eq. 2 is not
straightforward to solve using available deep-learning opti-
mization frameworks. In order to simplify the implementation,
we modify the way the constraints are satisﬁed by moving the
constraint in Eq. 3 into the objective function:

min
ˆx

H(ˆy, p (y|ˆx, w))αI (ˆx,ˆy)

(4)

+ (ˆx − x0)T Q (ˆx − x0)

where y(i) is a one-hot encoding representation of the class:

s.t xmin (cid:22) ˆx (cid:22) xmax

yi =

(cid:40)
1
0

if x(i) ∈ class i
otherwise

Depending on the complexity of the model p (y|x, w) and
the dataset D, the model may misclassify some of the samples.
We are interested on provide explanations for these incorrect
estimations.

B. Modifying misclassiﬁed samples

In this paper, instead of deceiving the classiﬁer, we make
use of adversarial machine learning to understand why some of
the samples are being mis-classiﬁed. The idea of this approach
is that adversarial machine learning can help us to understand
the decision boundaries of the learned model.

The objective is to ﬁnd the minimum modiﬁcations needed
in order to change the output of the classiﬁer for a real sample
x0. This is achieved by ﬁnding an adversarial sample ˆx that
is classiﬁed as ˆy while minimizing the distance between the
real sample (x0) and the modiﬁed sample ˆx:

min
ˆx
s.t

(ˆx − x0)T Q (ˆx − x0)

argmaxk p (y = k|ˆx, w) = ˆy
xmin (cid:22) ˆx (cid:22) xmax

(2)

(3)

where Q is a symmetric positive deﬁnite matrix, that allows
the user to specify a weight in the quadratic difference metric.
The program in Equation 2 can serve multiple purposes
depending on how x0 and ˆy are speciﬁed. For the purpose of
explaining incorrect classiﬁcations, in this paper, x0 represents
the real misclassiﬁed samples that serve as a reference for the
modiﬁed samples ˆx. Furthermore, the value of ˆy is set to
the correct class of x0. In this way, the program (2) ﬁnds
a modiﬁed sample ˆx that is as close as possible to the real
misclassiﬁed sample x0, while being correctly classiﬁed by
the model as ˆy.

where:
• (x0, y) is a reference sample from the dataset
• ˆx is the modiﬁed version of x0 that makes the estimation

of the class change from y to the target ˆy

• H(ˆy, p (y|ˆx, w)) is the cross-entropy between the esti-
mated adversarial sample class p (y|ˆx, w) and the target
class ˆy

• I (ˆx,ˆy) is an indicator function that speciﬁes whether the

adversarial sample ˆx is being classiﬁed as ˆy

(cid:40)

I (ˆx,ˆy) =

0 if argmaxk p (y = k|ˆx, w) = ˆy
1

otherwise

This function provides a mechanism to stop the modiﬁ-
cations once the sample ˆx is classiﬁed as ˆy. We assume
this function is not continuous, hence, the gradients with
respect the inputs are not required.

• α is a scale factor that can be used to weight

the
contribution of the cross-entropy H to the objective loss.
The problem stated in Eq. 4 can be seen as an instance
of the adversarial problem stated in Eq. 1, where the cross-
entropy H represents the effectiveness (L) of the modiﬁcations
while the quadratic difference represents the discrepancy (Ω)
between x0 and ˆx.

C. Explaining incorrect estimations

We used the adversarial approach stated in Eq. 4, to generate
an explanation for incorrect classiﬁcation. Using a set of
x0 misclassiﬁed samples as reference, we use Eq. 4 to ﬁnd
the minimum modiﬁcations needed to correctly classify the
samples. We use as target ˆy the real class of the samples.

The explanations are generated by visualizing the difference
(x0 − ˆx) between the misclassiﬁed samples x0 and the
modiﬁed samples ˆx. This difference shows the deviation of
the real features x0 from what the model considers as the
target class ˆy.

We constrain the sample ˆx to be inside the bounds
(xmin, xmax). These bounds are extracted from the maximum

The explanations can be generated for individual samples
x0 or for a set of misclassiﬁed samples. We used the average

(a) misclassiﬁed samples x0

(b) misclassiﬁed x0 and modiﬁed samples ˆx

Fig. 3: t-SNE visualization of misclassiﬁed samples and corresponding modiﬁed samples for the MLP classiﬁer. The legend shows the class
to which the misclassiﬁed samples belong to. The axes correspond to an embedded 2D representation of the samples. The ﬁgure shows there
is no clear distinction between the misclassiﬁed real samples x0 and the modiﬁed samples ˆx, demonstrating that small modiﬁcations can
be used to change the output of the classiﬁer.

deviation (x0 − ˆx) to present the explanations for a set of
misclassiﬁed samples.

gradient ∇xH of the cross-entropy loss with respect to the
inputs.

IV. EXPERIMENTS AND RESULTS

A. Dataset

For experimental evaluation, we used the NSL-KDD intru-
sion detection dataset [21]. The NSL-KDD dataset is a revised
version of the KDD99 dataset [22], a widely used benchmark
dataset used for intrusion detection algorithms. The NSL-
KDD dataset removes redundant and duplicate records found
in the KDD99 dataset, alleviating some of the problems of the
KDD99 dataset mentioned in [21].

The NSL-KDD consists of a series of aggregated records
extracted from a packet analyzer log. Besides normal com-
munications, the dataset contains records of attacks that fall
in four main categories: DOS, R2L, U2R and proving. For
our experiments, we only considered normal, DOS and probe
classes.

The dataset consists of 124926 training samples and 16557
testing samples. We used the same split provided by the
authors of the dataset. To alleviate the effects of the unbalanced
distribution of the samples over the classes, we trained the
models extracting mini-batches with an equal ratio of samples
from each class. Samples were extracted with replacement. A
detailed description of the dataset features can be found in
[23].

The dataset samples were normalized in order to make the
quadratic distance metric in Equation 4 invariant to the features
scales. We used the mean and standard deviation of the training
dataset for normalization:

x(i) ←

x(i) − MEAN ({x|x ∈ D})
STD ({x|x ∈ D})

For the experimental section, we used the following data-
driven models: 1) a linear classiﬁer, and 2) a Multi Layer Per-
ceptron (MLP) classiﬁer with ReLU activation function. We
used weight decay (L2 norm) and early-stopping regularization
for training both models.

Table I shows the accuracy achieved with the linear and the
MLP classiﬁers. We can see that the MLP classiﬁer provides
higher accuracy in the training and testing datasets.

TABLE I: Accuracy of Classiﬁers

classiﬁer

train

test

Linear
MLP

0.957
0.995

0.936
0.955

C. Modiﬁed misclassiﬁed samples

We used t-SNE [24] to visualize the misclassiﬁed samples
(x0) and the modiﬁed/corrected samples (ˆx) found using
Equation 4. t-SNE is a dimensionality reduction technique
commonly used to visualize high-dimensional datasets.

Figure 3 shows the visualization of the misclassiﬁed sam-
ples x0 and the corresponding modiﬁed samples ˆx using t-
SNE. This ﬁgure shows the effectiveness of the presented
methodology to ﬁnd the minimum modiﬁcations needed to
correct the output of the classiﬁer. No visual difference in
the visualization can be observed between the real and the
modiﬁed samples. The modiﬁed samples ˆx are close enough
to the real samples x0 that the modiﬁed samples occlude the
real samples in Figure 3b.

B. Classiﬁers

D. Explaining incorrect estimations

One of the advantages of the methodology presented in this
paper is that it works for any classiﬁer which has a deﬁned

Figure 4 shows the generated explanation for Normal sam-
ples being incorrectly classiﬁed as DOS. Figure 4a shows the

(a) Normal samples misclassiﬁed as DOS using Linear model

(a) DOS samples misclassiﬁed as Normal using Linear model

(b) Normal samples misclassiﬁed as DOS using MLP model

Fig. 4: Explanation for Normal samples being misclassiﬁed as DOS
using the difference between real samples x0 and modiﬁed samples
ˆx.

(b) DOS samples misclassiﬁed as Normal using MLP model

Fig. 6: Explanation for DOS samples being misclassiﬁed as Normal
using the difference between real samples x0 and modiﬁed samples
ˆx.

(logged in, is guest login)

• high percentage of connections originated from the same

source port (dst host same src port rate)

• low percentage of connections directed to different ser-

vices (diff srv rate)

• low number of connections were directed to the same

destination port (dst host srv count)

Figures 4a and 4b that a high number of connections with
low duration and low login success rate is responsible for
misclassifying Normal samples as DOS. These attributes are
clearly suspicious and match what a human expert would
consider as typical behavior of a DOS attack, providing a
satisfactory explanation for the incorrect classiﬁcation.

A more detailed view of the difference between the duration
of real (x0) and modiﬁed (ˆx) samples is presented in Figure
5. This ﬁgure shows that all misclassiﬁed Normal samples
had connections with a duration of zero seconds, which the
classiﬁer considers suspicious for a Normal behavior.

The graphs also provide a natural way to extract knowledge
and understand the concepts learned by the model. For exam-
ple, ﬁgures 4a and 4b show the classiﬁer considers suspicious
when there is a low percentage of connections directed to
different services (low diff srv rate).

A parallel analysis can be performed to other misclassiﬁed
samples. Figure 6 provides explanations for DOS attacks
misclassiﬁed as Normal connections for Linear and MLP
models. Overall, Figures 6a and 6b, show that the samples are

Fig. 5: Comparison of duration feature between x0 (Normal samples
misclassiﬁed as DOS) and ˆx (modiﬁed samples). The model consid-
ers connections with zero duration suspicious. The modiﬁed samples
have an increased duration in order to correctly classify the samples
as Normal.

explanations for the Linear model while Figure 4b shows the
explanations for the MLP model.

We observed that the explanations for both models provide
similar qualitative explanations. Figures 4a and 4b can be
naturally interpreted as follows:
Normal samples were mis-classiﬁed as DOS because:

• high number of connections to the same host (count) and

to the same destination address (dst host count)

• low connection duration (duration)
• low number of operations performed as root

in the

connection (num root)

• low percentage of samples have successfully logged in

(a) Deviation for continuous features

(b) Comparison of categorical features distributions

Fig. 7: Explanation for Normal samples mis-classiﬁed as DOS while taking into account categorical features (MLP model).

misclassiﬁed as Normal because they have: (a) lower error rate
during the connection (b) higher login success. These features
are usually expected to belong to normal connections, which
successfully explains the reason for the misclassiﬁcation.

The explanations shown in Figures 4 and 6 do not take cat-
egorical features into account. In order to include categorical
features into the analysis, we perform a round operation to
the inputs of the indicator I (ˆx,ˆy). 1 This ensures the objective
function in Equation 4 takes into consideration the effects of
the rounding operation.

Figure 7 shows the explanations generated when consid-
ering categorical features. Figure 7a shows the deviation of
continuous features, providing the same information as the
explanations from Figures 4 and 6. Figure 7b shows the
comparison between the histograms of misclassiﬁed samples
and modiﬁed samples. Figure 7b shows that protocol type was
not modiﬁed, suggesting that this feature is not relevant in
order to explain the misclassiﬁcation. On the other hand, the
service feature was modiﬁed in almost all samples. The ﬁgure
shows that most of the Normal samples misclassiﬁed as DOS
used a private service. Changing the service value helps the
classiﬁer to correctly estimate the class of the samples. The
explanation shows that the model considers communication
with private service as suspicious.

V. CONCLUSION

In this paper, we presented an approach for generating
explanations for the incorrect classiﬁcation of a set of samples.
The methodology was tested using an Intrusion Detection
benchmark dataset.

The methodology uses an adversarial approach to ﬁnd the
minimum modiﬁcations needed in order to correctly classify
the misclassiﬁed samples. The modiﬁcations are used to
ﬁnd and visualize the relevant features responsible for the
misclassiﬁcation. Experiments were performed using Linear

1Given that we do not use the gradient of I (ˆx,ˆy) during the optimization
process, we are allowed to include discontinuous operations like the rounding
function

and Multilayer perceptron classiﬁers. The explanations were
presented using intuitive plots that can be easily interpreted
by the user.

The proposed methodology provided insightful and satis-
factory explanations for the misclassiﬁcation of samples, with
results that match expert knowledge. The relevant features
found by the presented approach showed that misclassiﬁcation
often occurred on samples with conﬂicting characteristics
between classes. For example, normal connections with low
duration and low login success are misclassiﬁed as attacks,
while attack connections with low error rate and higher login
success are misclassiﬁed as normal.

VI. DISCUSSION

An advantage of the presented approach is that it can be
used for any differentiable model and any classiﬁcation task.
No modiﬁcations of the model are required. The presented
approach only requires the gradients of the model cross-
entropy w.r.t. the inputs. Non-continuous functions can also
be incorporated into the approach, for example rounding
operations for integer and categorical features.

The presented adversarial approach can be extended to
perform other analysis of the model. For example, instead
of ﬁnding modiﬁcations for correct the predicted class, the
modiﬁcations can be used to deceive the classiﬁer, which
can be used for vulnerability assessment. Future research will
be conducted to incorporate the explanations to improve the
accuracy of the model without having to include new data into
the training procedure.

REFERENCES

[1] D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, and K. J. Kim, “A
survey of deep learning-based network anomaly detection,” Cluster
Computing, Sep 2017.
[Online]. Available: https://doi.org/10.1007/
s10586-017-1117-8

[2] B. S. Sridhar, A. Hahn, and M. Govindarasu, “Cyber Physical System

Security for the Electric Power Grid,” vol. 100, no. 1, 2012.

[3] R. Raj, I. Lee, and J. Stankovic, “Cyber-Physical Systems : The Next

Computing Revolution,” pp. 731–736, 2010.

[4] A. C. Alvaro, “Challenges for Securing Cyber Physical Systems.”

[5] W. L. W. Lee, S. J. Stolfo, and K. W. Mok, “A data mining
framework for building intrusion detection models,” Proceedings
the 1999 IEEE Symposium on Security and Privacy Cat
of
No99CB36344, vol. 00, pp. 120–132, 1999.
[Online]. Available:
http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=766909
[6] A. Buczak and E. Guven, “A survey of data mining and machine learning
methods for cyber security intrusion detection,” IEEE Communications
Surveys & Tutorials, vol. PP, no. 99, p. 1, 2015.

[7] K. Amarasinghe and M. Manic, “Toward explainable deep neural net-
work based anomaly detection,” in 2018 11th International Conference
on Human System Interactions (HSI), July 2018.

[8] W. Samek, T. Wiegand, and K. M¨uller, “Explainable artiﬁcial
intelligence: Understanding, visualizing and interpreting deep learning
models,” CoRR, vol. abs/1708.08296, 2017.
[Online]. Available:
http://arxiv.org/abs/1708.08296
[9] D. Gunning, “Explainable artiﬁcial

intelligence (xai),” Defense Ad-

vanced Research Projects Agency (DARPA), nd Web, 2017.

[10] Parliament and C. of the European Union, “General data protection

regulation,” 2016.

[11] B. Goodman and S. R. Flaxman, “European union regulations on
algorithmic decision-making and a,” AI Magazine, vol. 38, no. 3, pp.
50–57, 2017.

[12] D. Lowd and C. Meek, “Adversarial

learning,” Proceeding of
the eleventh ACM SIGKDD international conference on Knowledge
discovery in data mining - KDD ’05, p. 641, 2005. [Online]. Available:
http://portal.acm.org/citation.cfm?doid=1081870.1081950

[13] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar, “The security
of machine learning,” Machine Learning, vol. 81, no. 2, pp. 121–148,
2010.

[14] D. Wagner and P. Soto, “Mimicry Attacks on Host-Based Intrusion
Detection Systems,” Proceedings of
the 9th ACM conference on
Computer and communications security, pp. 255–264, 2002. [Online].
Available: http://portal.acm.org/citation.cfm?doid=586110.586145
[15] J. Li, W. Monroe, T. Shi, A. Ritter, and D. Jurafsky, “Adversarial
learning for neural dialogue generation,” CoRR, vol. abs/1701.06547,
2017. [Online]. Available: http://arxiv.org/abs/1701.06547

[16] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar,
“Can machine learning be secure?” in Proceedings of the 2006 ACM
Symposium on Information, computer and communications security.
ACM, 2006, pp. 16–25.

[17] C. Frederickson, M. Moore, G. Dawson, and R. Polikar, “Attack
strength vs. detectability dilemma in adversarial machine learning,”
arXiv preprint arXiv:1802.07295, 2018.

[18] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples (2014),” arXiv preprint arXiv:1412.6572.

[19] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the

physical world,” arXiv preprint arXiv:1607.02533, 2016.

[20] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,” in
Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security. ACM, 2017, pp. 506–519.

[21] M. Tavallaee, E. Bagheri, W. Lu, and A. A. Ghorbani, “A detailed
analysis of the kdd cup 99 data set,” in Computational Intelligence for
Security and Defense Applications, 2009. CISDA 2009. IEEE Symposium
on.

IEEE, 2009, pp. 1–6.

[22] (1999) KDD cup 1999 dataset. [Online]. Available: http://kdd.ics.uci.

edu/databases/kddcup99/kddcup99.html

[23] “Gurekddcup database description.” [Online]. Available: http://www.

aldapa.eus/res/gureKddcup/README.pdf

[24] L. van der Maaten and G. Hinton, “Visualizing data using t-SNE,”
Journal of machine learning research, vol. 9, no. Nov, pp. 2579–2605,
2008.

