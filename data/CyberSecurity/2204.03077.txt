Control barrier function based attack-recovery with provable
guarantees

Kunal Garg

Ricardo G. Sanfelice

Alvaro A. Cardenas

2
2
0
2

r
p
A
6

]

Y
S
.
s
c
[

1
v
7
7
0
3
0
.
4
0
2
2
:
v
i
X
r
a

Abstract— This paper studies provable security guarantees
for cyber-physical systems (CPS) under actuator attacks. In
particular, we consider CPS safety and propose a new attack-
detection mechanism based on a zeroing control barrier func-
tion (ZCBF) condition. In addition we design an adaptive recov-
ery mechanism based on how close the system is from violating
safety. We show that the attack-detection mechanism is sound,
i.e., there are no false negatives for adversarial attacks. Finally,
we use a Quadratic Programming (QP) approach for online
recovery (and nominal) control synthesis. We demonstrate the
effectiveness of the proposed method in a simulation case study
involving a quadrotor with an attack on its motors.

I. INTRODUCTION

Cyber-physical systems (CPS) such as autonomous and
semi-autonomous air, ground and space vehicles must main-
tain their safe operation and achieve mission objectives
under various adversarial environments,
including cyber-
attacks. Security measures can be classiﬁed into two types of
mechanisms [1], i) proactive, which considers design choices
deployed in the CPS before attacks, and ii) reactive, which
takes effect after an attack is detected. A proactive method,
which considers design choices deployed in the CPS before
attacks, can result in a conservative design. On the other
hand, reactive methods, which take effect after an attack is
detected, heavily rely on fast and accurate attack-detection
mechanisms. An optimal approach to achieving resilience
against cyber attacks must utilize the beneﬁts of the two
approaches while minimizing their limitations.

There is a plethora of work on attack detection for CPS,
see e.g. [2]–[5]. However, as discussed in [6], a knowledge-
able attacker can design stealthy attacks that can disrupt
the nominal system behavior slowly in order to evade these
detection mechanisms. Such methods can lead to system
failure by pushing the system out of its safe operating limits.
Thus, a new attack-detection mechanism must be devised
based on the closeness of the system to violating safety.
Safety, i.e., the system does not go out of a safe zone, is an
essential requirement, violation of which can result in loss
of money, or human life, particularly when a system is under

Research was sponsored by the Army Research Ofﬁce and was
accomplished under Grant Number W911NF-20-1-0253. The views and
conclusions contained in this document are those of the authors and should
not be interpreted as representing the ofﬁcial policies, either expressed
or implied, of the Army Research Ofﬁce or the U.S. Government. The
is authorized to reproduce and distribute reprints for
U.S. Government
Government purposes notwithstanding any copyright notation herein.

K. Garg and R.G. Sanfelice are with the Department of Elec-
trical and Computer Engineering, and A.A. Cardenas is with the De-
partment of Computer Science and Engineering, University of Califor-
nia, Santa Cruz, CA, 95064, USA e-mail(s): {kgarg2, ricardo,
alacarde}@ucsc.edu.

attack [7]. In most practical problems involving CPS, safety
can be realized as guaranteeing forward-invariance of a safe
set. One of the most common approaches to guaranteeing
that system trajectories stay in a safe set or that the safe set
is forward invariant is based on a control barrier function
(CBF), as it allows a real-time implementable quadratic
programming (QP)-based control synthesis framework [8].

In our prior work [9], we use a proactive scheme consisting
of only designing a safe feedback law using CBF. One
disadvantage of that approach is that the control is con-
servative because we assumed the system could constantly
be under attack. In contrast, this paper designs a reactive
security mechanism that activates the conservative control
only after an attack is detected. In particular, we consider
actuator manipulation, where an attacker can assign arbitrary
values to the input signals for a subset of the actuators.
Furthermore, we allow multiple attacks on the system and
provide conditions for guaranteed safety under repeated
attacks on system actuators.

We consider the safety property with respect to an unsafe
set and propose an attack-detection mechanism based on
the CBF condition for safety. We use an adaptive parameter
based on how close the system is from violating the safety
requirement, and use this adaptive parameter in the attack
detection to reduce conservatism. Based on the detection, we
utilize a switching-based recovery from a nominal feedback
law (to be used when there is no attack) to a safe feedback
law when the system is under an adversarial attack. The
contributions of the paper are summarized below:

1) We present a novel attack detection mechanism using
CBF conditions for safety. In the absence of knowledge
of actual system input under an attack, we utilize
an approximation scheme and show that the attack-
detection mechanism is sound, i.e., it does not generate
any false negatives. While there is work on CBF-based
safety of CPS under faults and attacks [10], [11], to the
best of the authors’ knowledge, this is the ﬁrst work
utilizing CBF conditions for attack detection;

2) Based on the zeroing-CBF condition [8], we propose an
adaptation scheme to minimize the false-positive rate of
the attack-detection mechanism;

3) Finally, we use a switching law for input assignment and
a QP formulation for online feedback synthesis for both
nominal and safe feedback. We illustrate the efﬁcacy of
the proposed method in a case study involving an attack
on a motor of a quadrotor and show how the proposed
framework can recover the quadrotor from an attack.

 
 
 
 
 
 
Notation: Throughout the paper, R denotes the set of real
numbers and R+ denotes the set of non-negative real num-
bers. We use |x| to denote the Euclidean norm of a vector
x ∈ Rn. We use ∂S to denote the boundary of a closed set
S ⊂ Rn and int(S) to denote its interior. The Lie derivative
of a continuously differentiable function h : Rn → R along
a vector ﬁeld f : Rn → Rm at a point x ∈ Rn is denoted
as Lf h(x) := ∂h
∂x (x)f (x). The right and left limits of the
function z : R+ → Rn are given by z(t−) = limτ րt z(τ )
and z(t+) = limτ ցt z(τ ) respectively.

II. PROBLEM FORMULATION

A. System model

Consider a nonlinear control system S given as

˙x = F (x, u) + d(t, x),
x ∈ D, u ∈ U,

S :

(

(1)

where F : D × U → Rn is a known function continuous on
D × U, with D ⊂ Rn and U ⊂ Rm, d : R+ × Rn → Rn is
unknown and represents the unmodeled dynamics, x ∈ D is
the system state, and u ∈ U is the control input.

B. Attacker model

Similar to [9], in this paper, we consider attacks on the
control input of the system. In particular, we consider an
attack where a subset of the components of the control input
is compromised. Under such an attack, the system input takes
the form:

(2)

u = (uv, us),
where uv ∈ Uv ⊂ Rmv represents the vulnerable compo-
nents of the control input that might be compromised or
attacked, and us ∈ Us ⊂ Rms the secure part that cannot be
attacked, with mv + ms = m and U := Uv × Us. Under this
class of attack, we assume that we know which components
of the control input are vulnerable.

Under this attack model, the input to the system takes the

form:

,

(

(3)

u(t, x) =

if t /∈ Ta;
if t ∈ Ta;

(λv(x), λs(x))
(ua(t), ks(x))
where ua : R+ → Uv is the attack signal on the input uv,
ks : R+ ×Rn → Rms is a safe feedback law for the input us,
to be designed and used when the system is under attack, and
the pair λv : Rn → Uv, λs : Rn → Us deﬁne the nominal
feedback law λ = (λv, λs), to be designed and used when
there is no attack. The set Ta ⊂ R+ is the set of time-
intervals when an attack is launched on the system input. In
particular, for each i ≥ 1, let [ti
1 denote
the interval of time when the attack is launched for the i−th
time where t1

1 ≥ 0, so that Ta :=

2) with ti

2). Deﬁne

2 ≥ ti

1, ti
[ti

1, ti

as the maximum length of the attack and the minimum
length of the interval without an attack on the system input,
respectively. In this work, we assume that the set Ta is
unknown, and only the maximum period of attack, T , and
minimum period without an attack, Tna, are known.

Now, we present the control design problem studied in the
paper. Consider a non-empty, compact set S ⊂ Rn, referred
to as a safe set, to be rendered forward invariant. We make
the following assumption on the unmodeled dynamics d in
(1):

Assumption 1. There exists a known δ > 0 such that
|d(t, x)| ≤ δ for all t ≥ 0 and x ∈ D.

Problem 1. Given the system in (1) with unmodeled dynam-
ics d that satisﬁes Assumption 1, a set S and the attack model
in (2), design an attack-detection mechanism to raise a ﬂag
that the system is under attack and a safe input assignment
policy such that, for a set of initial conditions X0 ⊂ S and
attack signals ua : R+ → Uv, the closed-loop trajectories
x : R+ → Rn of (1) resulting from applying the designed
input policy satisfy x(t) ∈ S for all t ≥ 0 and for all
x(0) ∈ X0.

Note that for the safety requirement as imposed in Problem
1, an attack is adversarial only if it can push the system
trajectories out of the set S, as deﬁned below.
Deﬁnition 1. An attack signal ua : R+ → Uv is adversarial
if there exist x(0) ∈ S and a ﬁnite t ≥ 0 such that for any
κ : R+ × Rn → Us, each system trajectory x : R+ → Rn of
(1) resulting from applying u = (ua, κ) satisﬁes x(t) /∈ S.

Per above deﬁnition, it is possible that there is an attack
on the system but the system does not violate the safety
requirement. We use this observation to focus our detection
mechanism only on the attacks that can potentially push the
system out of the safe set.

C. Preliminaries

We ﬁrst review the notion of forward invariance of the set
S and the corresponding barrier function conditions. In the
rest of the paper, to keep the presentation simple, we assume
that the maximal solutions of (1) exists and are unique.
Deﬁnition 2. A set S ⊂ Rn is termed as forward invariant
for system (1) if every solution x : R+ → Rn of (1) satisﬁes
x(t) ∈ S for all t ≥ 0 and for all initial conditions x(0) ∈ S.

Next, we review a sufﬁcient condition for guaranteeing
forward invariance of a set without an attack. For the sake
of simplicity, we assume that every solution of (1) exists and
is unique for all t ≥ 0 whether or not there is an attack on the
system.1 Following the notion of robust CBF in [13], we can
state the following result guaranteeing forward invariance of
the set S for the system (1).

T := max
i≥1

{ti

Tna := min
i≥2

{ti

i≥0
S
2 − ti
1},
1 − t(i−1)

2

(4)

(5)

},

1In this work, we assume that under attack, the solution of the system is
unique in forward time. It is possible to study the case when this assumption
does not hold using the notion of strong invariance (see [12]).

Lemma 1 ([13]). Given a continuously differentiable func-
tion B : Rn → R, the set S = {x | B(x) ≤ 0} is
forward invariant for (1) under d satisfying Assumption 1
if the following condition holds:

inf
u∈U

LF B(x, u) ≤ −lBδ ∀x ∈ ∂S,

(6)

where lB is the Lipschitz constant of the function B.

III. ATTACK DETECTION

A. CBF based detection

In this section, we present a method of detecting whether
the system (1) is under attack using the barrier function
condition (6). In particular, we check whether inequality
(6) holds on the boundary of the safe set to raise a ﬂag
for an attack. In contrast to using the value of the barrier
function B, we use the value of its time derivative due to the
following reason. The time derivative of the function B on
the boundary of the safe set indicates whether the system will
violate the safety constraint. Moreover, the time derivative of
the function B includes the system dynamics. Hence, it is
a better indicator of whether the given system will violate
the given safety constraint than the function B itself, which
does not capture the system information. Given B and F ,
deﬁne H : Rn × Rm → R:

H(x, u) := LF B(x, u) + lBδ,

(7)

where δ is the bound on the disturbance d per Assumption
1 and lB is the Lipschitz constant of the function B. Note
that condition (6) can be written in terms of the function H
as

inf
u∈U

H(x, u) ≤ 0 ∀x ∈ ∂S.

(8)

Note that if an attack signal ua is adversarial, then it holds
that there exists a ﬁnite time t ≥ 0 such that x(t) ∈ ∂S and
H(x(t), (ua(t), vs)) > 0 for any vs ∈ Us where x : R+ →
Rn is the solution of (1) resulting from the input (3). Using
this, a detection mechanism can be devised to ﬂag that the
system input is under attack. In particular, if the input u to
the system is known at time t when x(t) ∈ ∂S, an attack
detection mechanism can be designed by checking the value
of H(x(t), u(t, x(t))). However, in the presence of an attack
with a delay td in detection, it is not possible to know the
actual input u to the system. Thus, it is not possible to use
the evaluation of H to ﬂag an attack.

On the other hand, note that the function B only depends
on the state x, and thus, we use an approximation method
to estimate the value of H at any given time t using the
consecutive measurements of the function B at time (t − τ )
and t, for some τ > 0. Deﬁne eB : R+ → R

eB(t) :=

˙B(x(t)) −
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B(x(t)) − B(x(t − τ ))
τ

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

as the error between the actual time derivative of the function
B and its ﬁrst order approximation where x : R+ → Rn is
the solution of (1). Assume that the function B is twice
continuously differentiable, and the functions F in (1) is

continuously differentiable. Under these conditions, using the
Taylor’s theorem for the second-order approximation of the
function B, there exists 0 ≤ ¯t ≤ τ such that

B(x(t − τ )) = B(x(t)) − ˙B(x(t))τ + ¨B(x(¯t))

τ 2
2

,

where ¨B is the second time derivative of the function B.
Assume that k ¨B(x)k ≤ η for some η > 0 for all x ∈ Sc. For
the sake of brevity, denote ˆ˙B(x(t), τ ) = B(x(t))−B(x(t−τ ))
so that we have

τ

eB(t) = k ˙B(x(t)) − ˆ˙B(x(t), τ )k =

k ¨B(x(¯t))k

τ
2
ητ
2
2 . Using the bound on eB, we

≤

.

Thus, it holds that eB(t) ≤ ητ
obtain that for each t ≥ 0 and τ ≥ 0, the following holds
ˆ˙B(x(t), τ ) −

≤ H(x(t), u(t)) ≤ ˆ˙B(x(t), τ ) +

, (9)

ητ
2

ητ
2

where x : R+ → Rn is the solution of (1) resulting from
applying the input u : R+ → U. Using (9), it holds that for
each t ≥ 0 and τ ≥ 0,
ητ
2

≤ 0 =⇒ H(x(t), u(t)) ≤ 0.

ˆ˙B(x(t), τ ) +

With the above construction, we propose the following attack
detection mechanism:

1) Given τ > 0 and ¯t ≥ 0 such that x(¯t) ∈ ∂S, evaluate

ˆ˙B(x(¯t), τ ).

2) If ˆ˙B(x(¯t), τ ) > − ητ

under attack.

2 , raise a ﬂag that the system is

More concisely, we deﬁne the time when a ﬂag for an

attack is raised as

ˆtd = inf

t

ˆ˙B(x(t), τ ) > −

ητ
2

, x(t) ∈ ∂S

,

(10)

n

(cid:12)
(cid:12)
(cid:12)

where η is the bound on the second time-derivative ¨B and
τ > 0. We have the following result stating that the attack
detection mechanism in (10) detects the attack before the
system trajectories leave the safe set.

o

Lemma 2. Given a twice continuously differentiable function
B, system (1) with d satisfying Assumption 1, a continuously
differentiable function F , and an adversarial attack starting
at t = ti

1 be deﬁned as

1, let T ≥ ti

T = inf{t ≥ ti
1 | H(x(t), u(t, x(t))) > 0, x(t) ∈ ∂S}, (11)
where x : R+ → Rn is the solution of (1) resulting from
applying the input u : R+ → U and η is the bound on the
second time-derivative ¨B. Then, for each τ ≥ 0, it holds that
ˆtd ≤ T , where ˆtd is given in (10).
Proof. If ˆtd > T , it holds that there exists t ∈ (T, ˆtd) such
that H(x(t), u(t)) > 0 and ˆ˙B(x(t), τ ) ≤ − ητ
2 . Using this
along with the second inequality in (9) at time instant t, we
obtain that

0 < H(x(t), u(t)) ≤ ˆ˙B(x(t), τ ) +

ητ
2

≤ 0,

which is a contradiction and hence, ˆtd ≤ T .

Lemma 2 implies that the attack-detection mechanism in
(10) raises an alert on or before the system trajectories reach
the boundary of the set ∂S under an attack. In other words,
while the detection-mechanism (10) can have false positives
(i.e., raise an alert when there is no attack), it will never have
a false negative (i.e., it will not miss any attack).

B. Adaptive scheme for ZCBF-based attack detection

One of the limitations of using the inequality H(x, u) ≤ 0
at the boundary of the safe set S for detecting an attack is
that it is not robust due to the following two reasons: (i) any
small measurement uncertainty or disturbance can lead to
violation of safety, and (ii) any non-zero delay in response
will lead to violation of safety. One method to make the
detection method robust is to check the inequality at the
boundary of the set Sc = {x | B(x) ≤ −c} for some c > 0.
Deﬁne cM ∈ R as

cM := − min
x∈S

B(x),

(12)

so that the set Sc := {x | B(x) ≤ −c} is nonempty for all
c ∈ [0, cM ).2 Now, since it is possible to allow the function
H to take positive values in the interior of the safe set S,
we use an inequality H(x, u) ≤ γ for some γ > 0 instead
of H(x, u) ≤ 0, to detect attacks. Note that a constant γ >
0 might lead to false-positives if γ is too small, or false-
negatives if γ is too large. To this end, we make the following
assumption when the system is not under an attack.
Assumption 2. There exist ¯c ∈ (0, cM ), ¯δ ∈ R and a
continuous input ¯u : Rn → U such that the following
inequality holds:

H(x, ¯u(x)) ≤ −¯δB(x) ∀x ∈ S \ int(S¯c).

(13)

Similar assumptions have been made in the literature on
safety using ZCBFs (see e.g. [8]). Note that under Assump-
tion 2, using the comparison lemma, it can be shown that

H(x(t), ¯u(x(t))) := ˙B(x(t)) ≤ −¯δB(x(t))

=⇒ B(x(t)) ≤ B(x(¯t))e−¯δ(t−¯t) ∀t ≥ ¯t,
where ¯t = inf{t | x(t) ∈ ∂S¯c} and x : R+ → Rn is
the solution of (1) resulting from applying the continuous
input ¯u. Using this, we design an adaptive scheme for the
parameter γ. Let γ : R+ → R+ be an adaptive parameter
whose adaptation law is given as

γ(t) = ¯δ¯ce−δ(t−¯t),
for t ≥ ¯t, where ¯δ, ¯c are as deﬁned in Assumption 2.
Note that under Assumption 2, there exists a feedback law
¯u : Rn → U such that H(x(t), ¯u(x(t))) ≤ γ(t), ∀t ≥ ¯t.
Using this observation, we propose a new attack-detection

(14)

2Compactness of the set S guarantees existence of cM ∈ R+.

for mechanism that raises a ﬂag for the i−th time at t = ˆti
d
where

ˆti
d = inf

t ≥ max

n

¯t, ˆt(i−1)
d
n

o (cid:12)
(cid:12)
(cid:12)

ˆ˙B(x(t), τ ) > γ(t) −

x(t) ∈ S \ int(S¯c)

,

ητ
2
, (15)

o

where η is the bound on the second time-derivative ¨B, γ is
as deﬁned in (14), ˆt0
d = −T and τ > 0.

Remark 1. Under an attack, the proposed detection mech-
anism allows the system to get closer to the boundary of the
safe set as long as the rate at which the system approaches
˙B) is bounded according to
the boundary (dictated by
Assumption 2. Also, it is worth noting focuses on detecting
only adversarial attacks, and not every attack. That is, if
there is an attack on the system that cannot push it out of
the safe set, the proposed detection mechanism will not detect
it.

IV. QP-BASED RECOVERY CONTROLLER

In this section, we present a switching based control
assignment to recover from an adversarial attack based on
the detection mechanism from the previous section. To this
end, we make the following assumption.

Assumption 3. There exists ¯c ∈ (0, cM ) such that
following hold:

the

inf
us∈Us

sup
ua∈Ua

H(x, (ua, us) ≤ 0 x ∈ S \ int(S¯c).

(16)

The above assumption implies that the set Sc can be

rendered forward invariant under any attack ua ∈ Ua.

Remark 2. When the system undergoes a single period
of attack (i.e., t2
1 = ∞), Assumption 3 can be relaxed by
requiring the left-hand side of (16) to be upper-bounded
by a positive constant that is inversely proportional to the
maximum length of the attack T . In case of multiple attacks,
it is possible to use the same relaxation by using the notion
of periodic safety of the set S¯c with respect to the set S with
time Tna (see [14]), where Tna := mini≥2{ti
} is
the minimum length of time when there is no attack. Periodic
safety requires that the system trajectories reach the set S¯c
within a time Tna starting from anywhere in the set S,
thereby allowing the result for the case when there is a single
period of attack to hold for multiple attacks.

1 − t(i−1)

2

Based on the detection scheme in the previous section,
we propose a switching-based control assignment for attack
recovery. Consider a time-interval [t(i−1)
1) so that the
system input is not under an attack during t ∈ [t(i−1)
, ti
1)
2
and is under an attack during t ∈ [ti
2). Deﬁne Td :=
d + T ) as the set of time intervals when an attack is
d is the time when the attack is ﬂagged the
d = −T . Due to Ta being unknown,

ﬂagged, where ˆtj
S
j−th time, j ≥ 0 with ˆt0

d, ˆtj
[ˆtj

1, ti

, ti

2

the system input is deﬁned as

u(t, x) = (uv(t, x), us(t, x)),

uv(t, x) =

us(t, x) =

λv(x)
ua(t)

(

λs(x)
ks(x)

(

if
if

if
if

t /∈ Ta,
t ∈ Ta,

t /∈ Td,
t ∈ Td.

(17a)

(17b)

(17c)

Remark 3. In this work, we do not determine the time instant
ti
2, i.e., when the attack stops. Instead, we take a conservative
approach, and assume that the attack duration is for the
maximum possible length T . Estimation of the time when
the attack stops can reduce this conservatism, and is left for
future work.

We have the following result showing the existence of
nominal and safe feedback laws for (17) that can recover the
system from an attack.

Theorem 1. Given system (1) with F ∈ C1, B ∈ C2 and
the attack model (2), suppose that Assumption 1 holds, and
Assumptions 2-3 hold for some ¯c ∈ (0, cM ). Then, there
exist feedback laws λ : Rn → U and ks : Rn → Us such
that under the effect of the input u in (17) with ˆtj
d is deﬁned
in (10), the system trajectories of (1) resulting from applying
(17) satisfy x(t) ∈ S for all t ≥ 0 and for all x(0) ∈ X0 =
int(S).

Proof. Let x(0) ∈ int(S) and consider the four cases: t ∈
Ta \ Td, t ∈ Ta ∩ Td, t ∈ Td \ Ta and t /∈ (Ta

Td).

Case 1: t ∈ Ta \ Td. Since t /∈ Td, from the deﬁnition
of ˆtd in (15), it holds that either x(t) ∈ int(S¯c) or x(t) ∈
S\int(S¯c) and H(x, u) ≤ 0. Thus, it holds that x(t) ∈ int(S)
for all t ∈ Ta \ Td.

S

Case 2: t ∈ Ta ∩ Td. Per Assumption 3, it holds that there
exists a feedback ks such that the set Sˆc is forward invariant
for (1) with u(t, x) = (ua(t), ks(x)) for any ua : R+ → Uv.
Thus, it holds that x(t) ∈ int(S \ int(S¯c) ⊂ int(S) for all
t ∈ Ta ∩ Td.

Case 3: t ∈ Td \ Ta. In this case, the above arguments hold
for ua = λv(x) and thus, x(t) ∈ int(S \ int(S¯c) ⊂ int(S) for
all t ∈ Td \ Ta.

Case 4: t /∈ (Ta

Td). In this case, per Assumption 2,
there exists a feedback λ : Rn → U such that the set S is
S
forward invariant for (1) under u = λ(x).

Thus, it holds that x(t) ∈ S for all t ≥ 0 and x(0) ∈

int(S).

In essence, Theorem 1 provides sufﬁcient conditions for
the existence of a control algorithm such that Problem 1
can be solved. We also note that the main bottleneck in the
proposed method is ﬁnding parameters for the satisfaction
of Assumptions 2-3. While Assumptions 2 and 3 serve
different purposes (as illustrated in the proof of Theorem
1), it is easy to see that satisfaction Assumption 3 for some
¯c ∈ (0, cM ) implies Assumption 2 holds for the same ¯c.
Thus, it is sufﬁcient to verify that Assumption 3 holds.

One practical method of ﬁnding a subset of the safe set S,
where Assumption 3 holds, is the computationally efﬁcient
sampling-based method proposed in [9]. In the interest of
space, we leave the details for future work.

Next, we present a control syntheses method to design
both the nominal feedback λ and the safe recovery feedback-
law ks for (17). In order to use a tractable optimization
problem for control synthesis, we assume that the system
(1) is control afﬁne and is of the form

˙x = f (x) + g(x)u + d(t, x),

(18)

where f : Rn → Rn and g : Rn → Rn×m are continuous
functions. Assume that the input constraint set U is given as
U = {u | Au ≤ b}.

First, we present a quadratic program (QP) formulation
to synthesize the nominal feedback law λ. Consider the
following QP for each x ∈ S:

1
2

min
(v,η)
s.t.

η2

|v|2+

1
2
Av ≤ b,

Lf B(x) + LgB(x)v ≤ − ηB(x) − lBδ,

(19a)

(19b)
(19c)

where q > 0 is a constant, lB is the Lipschitz constants of
the function B. Next, we use a similar QP to compute the
safe feedback-law ks. To this end, let g = [gs gv] with gs :
Rn → Rn×ms, gv : Rn → Rn×mv and assume that the input
constraint set for us is given as Us = {us | Asus ≤ bs}.
Now, consider the following QP for each x ∈ S \ int(S¯c):
1
2

(20a)

ζ 2

min
(vs,ζ)
s.t.

|vs|2+

1
2
Asvs ≤ bs,

(20b)

Lf B(x) + Lgs B(x)vs ≤ − ζB(x) − lBδ

− sup
uv ∈Uv

Lgv B(x)uv,

(20c)

Let the solution of the QP (19) be denoted as (v∗, η∗) and
that of (20) as (v∗
s , ζ∗). In order to guarantee continuity of
these solutions with respect to x, we need to impose the strict
complementary slackness condition (see [15]). In brief, if the
i−the constraint of (19) (or (20)), with i ∈ {1, 2}, is written
as Gi(x, z) ≤ 0, and the corresponding Lagrange multiplier
is λi ∈ R+, then strict complementary slackness requires
that λ∗
i denote the optimal
solution and the corresponding optimal Lagrange multiplier,
respectively. We are now ready to state the following result.

i G(x, z∗) < 0, where z∗, λ∗

Theorem 2. Given the functions F, d, B and the attack
model (2), suppose Assumptions 1-3 hold with ¯δ > 0 and
¯c ∈ (0, cM ). Assume that the strict complementary slackness
holds for the QPs (19) and (20) for all x ∈ S and x ∈
S \ int(Sc), respectively. Then, the QPs (19) and (20) are
feasible for all x ∈ S and x ∈ S \ int(Sc), respectively,
v∗, v∗
s are continuous on int(S) and x ∈ int(S \ int(Sc)),
and the control input deﬁned in (17) with λ(x) = v∗(x) and
s (x) and td = ˆtd, where ˆtd is deﬁned in (10),
ks(x) = v∗
solves Problem 1 for all x(0) ∈ int(S).

Proof. Per Assumption 2, the set S is a viability domain for
the system (18). Per Assumption 3, any sublevel set of B in
S \ int(S¯c) is a viability domain for the system (18) under

attack. Thus, feasibility of the QPs (19) and (20) follows
from [15, Lemma 6]. Per [15, Theorem 1], the respective
solutions of the QPs (19) and (20) are continuous on int(S)
and int(S \ int(S¯c)), respectively. Finally, since the set S is
compact, it follows from [15, Lemma 7] that the closed-loop
trajectories are uniquely deﬁned for all t ≥ 0. Uniqueness
of the closed-loop trajectories, Assumption 1 and feasibility
of the QPs (19) and (20) for all x ∈ S and x ∈ S \ int(S¯c)
implies that all the conditions of Theorem 1 are satisﬁed with
λ deﬁned as the solution of (19) (i.e., λ(x) = v∗(x)) and ks
as the solution of (20) (i.e., ks(x) = v∗
s (x)). It follows that
the set int(S) is forward invariant for the system (18).

Thus, the QPs (19) and (20) can be used to synthesize a
nominal and a safe input for a system under attack. Next,
we present a numerical case study involving an attack on
one of the motors of a quadrotor and demonstrate how the
proposed defense mechanism can save the quadrotor from
crashing and keep it hovering at the desired altitude.

V. NUMERICAL SIMULATION

We consider a simulation case study involving a quadrotor
with an attack on one of its motors.3 The quadrotor dynamics
is given as (see [16], [17]):

c(φ)c(ψ)s(θ) + s(φ)s(ψ)

uf − kt ˙x

(21a)

(cid:16)(cid:0)

(cid:1)
c(φ)s(ψ)s(θ) − s(φ)c(ψ)

uf − kt ˙y

(cid:16)(cid:0)
c(θ)c(φ)uf − mg − kt ˙z

(cid:1)

(cid:17)

(cid:17)

(cid:16)

˙φ = p + qs(φ)t(θ) + rc(φ)t(θ)
˙θ = qc(φ) − rs(φ)

(cid:17)

qs(φ) + rc(φ)

(cid:0)

(cid:1)
− krp − qr(Izz − Iyy) + τp

− krq − pr(Ixx − Izz) + τq

− krr − pq(Iyy − Izz) + τr

(cid:17)

(cid:1)
,

(cid:1)

where m, Ixx, Iyy, Izz, kr, kt > 0 are system parameters,
g = 9.8 is the gravitational acceleration, c(·), s(·), t(·)
denote cos(·), sin(·), tan(·), respectively, (x, y, z) denote the
position of the quadrotor, (φ, θ, ψ) its Euler angles and
u = (uf , τp, τq, τr) the input vector consisting of thrust uf
and moments τp, τq, τr. The relation between the vector u
and the individual motor thrusts is given as

¨x =

¨y =

¨z =

1
m
1
m
1
m

˙ψ =

˙p =

˙q =

˙r =

1
c(θ)
1
Ixx
1
Iyy
1
Izz

(cid:16)

(cid:0)

(cid:0)

Fig. 1. The closed-loop path traced by the quadrotor with the proposed
detection mechanism (in blue) and without the detection mechanism (in
red). The vulnerable motor is shown in red.

system parameters for simulations as: Ixx = Iyy = 0.177
kg-m2, Izz = 0.344 kg-m2, m = 4.493 kg, l = 0.1 m,
d = 0.0024 m, kt = 1 and kr = 1.5 (see [17]). Furthermore,
we consider the bound on each motor given as |fi| ≤ 27.7
N for i ∈ {1, 2, 3, 4}. We use τ = 10−3 for approximation
of ˙B. Without loss of generality, we assume that motor #4 is
vulnerable. Note that under an attack, the input-thrust relation
reads:

uf
τp
τq
τr









= 









1
1
1
0
0 −l
−l
l
0
d −d d







,

f1
f2
f3









(23)

It is not possible to keep all the inputs (uf , τp, τq, τr) close
to its desired value simultaneously under an attack on motor
#4. Thus, we focus on designing a control law to maintain the
desired altitude of the quadrotor (through uf ) and minimize
its oscillations (through (τp, τq)). It implies that τr will not
be matched with its desired value to control the yaw angle
ψ, resulting in an uncontrolled yaw angle increase.

We choose the control objective to make the quadrotor

10

5

Take-off

0

0

Hover

Recovery

Crash

Crash

10

20

30

40

50

-0.5

(21b)

(21c)

(21d)

(21e)

(21f)

(21g)

(21h)

(21i)

uf
τp
τq
τr









= 









1
1
1
l
0 −l
−l
0
0
d −d d −d

1
0
l

f1
f2
f3
f4



















,

(22)

0

0

1

2

where fi is the thrust generated by the i−th motor for i ∈
{1, 2, 3, 4}, d, l > 0 are system parameters. We choose the

3A

video

of

the

simulation

is

available

at

https://tinyurl.com/ye28ksx3.

The z−coordinate of the closed-loop system with and without
Fig. 2.
the detection mechanism. In the absence of the detection mechanism,
the quadrotor crashes (i.e., z = 0 m). In the presence of the detection
mechanism, the altitude remains close to the desired altitude z = 5m
(shown by black line). The conservative approach in [9], resulting in crash
even without an attack, is shown in green (see the inset plot).

1

0

0

10

20

30

40

0.2
0
-0.2
-0.4

0.2

0

-0.2

-0.4

0

10

20

30

40

Fig. 3. The attack (respectively, the detection) activity where 1 denotes that
attack is active (respectively, ﬂagged) and 0, that the attack is non-active
(respectively, not ﬂagged).

Fig. 5. Euler angles (φ, θ) of the closed-loop system. The safety constraints
|φ| ≤ φM and |θ| ≤ θM are satisﬁed at all times.

hover at location (0, 0, 5), starting from (0, 0, 0.2). Based
on the above observation and the fact
that ψ does not
contribute in changing the altitude of the quadrotor, the safety
constraints are to keep the angles (φ, θ) in a given bounded
range, i.e., |φ| ≤ φM , |θ| ≤ θM , for some φM , θM > 0, and
to keep the quadrotor above the ground, i.e., z > 0. Thus,
the safe set is deﬁned as S =
(φ, θ, z) | |φ| ≤ φM , |θ| ≤

. We choose φM = θM = 0.3 and ǫ = 0.02.
θM , z ≤ −ǫ
The maximum length of the attack is randomly chosen as
T = 0.934 seconds and the period of no attack is chosen as
Tna = 2.238 seconds.

o

n

The barrier

functions used for enforcing safety are
B1(z) = −z + 0.02, B2(φ) = |φ|2 − φ2
M and B3(θ) =
M . The parameters ¯δ, ¯c for detection are ¯δ = 0.1, ¯c =
|θ|2 − θ2
1
4 (0.3)2. Figure 1 shows the closed-loop path traced by the
quadrotor. Figure 2 plots the position coordinates (x, y, z).
The safety constraint z ≤ 0 is satisﬁed at all times, and the
quadrotor is able to hover at an altitude z = 5 m. Figure 3
shows the attack and the detection signal. It can be seen
that detection has a non-zero delay during some attacks,

1

0
0

-4

0.4

-0.2
0.03

-0.03

7

9

11

12

Fig. 4. The detection mechanism in action (vertical cyan line marks the
beginning of the ﬂagging and the vertical pink line, its ending). The attack
is ﬂagged per (15) when B(x(t)) + ¯c = 0 (shown in black line) and
ˆ˙B(t) − γ(t) + ητ
2 = 0 (shown in green line). The ﬁrst ﬂag is raised at
t = 10 second for B = B1(z). The mechanism keeps the system in the
ﬂagged mode for the maximum length of the attack, even if the attack is
stopped.

and zero delay during some attacks. It can also be seen
that some of the attacks are not detected, as they do not
fall into the category of adversarial attack per Deﬁnition 1.
Figure 4 illustrates the detection mechanism in action. The
attack is ﬂagged according to (15) and remains ﬂagged for
the duration T . The bound |fi| ≤ 27.7 N is satisﬁed for
each motor at all times. The vulnerable motor is highlighted
in green. Figure 5 plots the Euler angles (φ, θ). It can be seen
that the safety constraint |φ| ≤ 0.3 and |θ| ≤ 0.3 is satisﬁed
at all times. Finally, Figure 6 plots the thrust for each motor
under nominal conditions as well as under attack.

Thus, the proposed scheme can successfully detect an
attack on a quadrotor motor before the quadrotor crashes.
Furthermore, the designed safe input can keep the quadrotor
in the safe zone even under attack, thus demonstrating a suc-
cessful recovery after detection. The conservative approach
in [9], which assumes that the rotor #4 is constantly under
attack, fails to keep the quadrotor from crashing even when
there is no attack (see Figure 2). In contrast, the proposed
approach is non-conservative and reacts to an adversarial
attack, thereby not interfering with the system’s nominal
functionality.

20
10
0
20
10
0
20
10
0
20
10
0

0

10

20

30

40

Thrust fi of each motor. The thrust of motor 4 under attack is
Fig. 6.
shown in red. The switch in the rest of the motors is clearly seen when an
attack is ﬂagged.

[3] H. Choi, W.-C. Lee, Y. Aafer, F. Fei, Z. Tu, X. Zhang, D. Xu, and
X. Deng, “Detecting attacks against robotic vehicles: A control invari-
ant approach,” in Proceedings of the 2018 ACM SIGSAC Conference
on Computer and Communications Security, ser. CCS ’18. New York,
NY, USA: ACM, 2018, pp. 801–816.

[4] C. Feng, V. R. Palleti, A. Mathur, and D. Chana, “A systematic
framework to generate invariants for anomaly detection in industrial
control systems,” in 2019 Network and Distributed System Security
Symposium (NDSS).

[5] V. Renganathan, N. Hashemi, J. Ruths, and T. H. Summers, “Distribu-
tionally robust tuning of anomaly detectors in cyber-physical systems
with stealthy attacks,” in 2020 American Control Conference (ACC).
IEEE, 2020, pp. 1247–1252.

[6] D. I. Urbina, J. A. Giraldo, A. A. Cardenas, N. O. Tippenhauer,
J. Valente, M. Faisal, J. Ruths, R. Candell, and H. Sandberg, “Limiting
the impact of stealthy attacks on industrial control systems,” in
Proceedings of the 2016 ACM SIGSAC conference on computer and
communications security, 2016, pp. 1092–1105.

[7] M. N. Al-Mhiqani, R. Ahmad, W. Yassin, A. Hassan, Z. Z. Abidin,
N. S. Ali, and K. H. Abdulkareem, “Cyber-security incidents: a review
cases in cyber-physical systems,” Int. J. Adv. Comput. Sci. Appl, no. 1,
pp. 499–508, 2018.

[8] A. D. Ames, X. Xu, J. W. Grizzle, and P. Tabuada, “Control barrier
function based quadratic programs for safety critical systems,” IEEE
Transactions on Automatic Control, vol. 62, no. 8, pp. 3861–3876,
2017.

[9] K. Garg, R. G. Sanfelice, and A. A. Cardenas, “Sampling based com-
putation of viability domain to prevent safety violations by attackers,”
arXiv preprint arXiv:2110.08632, 2021.

[10] A. Clark, Z. Li, and H. Zhang, “Control barrier functions for safe cps
under sensor faults and attacks,” in 2020 59th IEEE Conference on
Decision and Control (CDC).

IEEE, 2020, pp. 796–803.

[11] B. Ramasubramanian, L. Niu, A. Clark, L. Bushnell, and R. Pooven-
dran, “Linear temporal logic satisfaction in adversarial environments
using secure control barrier certiﬁcates,” in International Conference
on Decision and Game Theory for Security.
Springer, 2019, pp.
385–403.

[12] F. H. Clarke, Y. S. Ledyaev, R. J. Stern, and P. R. Wolenski, Nonsmooth
Springer Science & Business Media,

analysis and control theory.
2008, vol. 178.

[13] K. Garg and D. Panagou, “Robust control barrier and control lyapunov
functions with ﬁxed-time convergence guarantees,” in 2021 American
Control Conference (ACC), 2021, pp. 2292–2297.

[14] K. Garg, R. K. Cosner, U. Rosolia, A. D. Ames, and D. Panagou,
“Multi-rate control design under input constraints via ﬁxed-time bar-
rier functions,” IEEE Control Systems Letters, pp. 1–1, 2021.
[15] K. Garg, E. Arabi, and D. Panagou, “Fixed-time control under
spatiotemporal and input constraints: A quadratic program based
approach,” Automatica, to appear, 2022.

[16] A. Lanzon, A. Freddi, and S. Longhi, “Flight control of a quadrotor
vehicle subsequent to a rotor failure,” Journal of Guidance, Control,
and Dynamics, vol. 37, no. 2, pp. 580–591, 2014.
[17] A. Akhtar, S. L. Waslander, and C. Nielsen, “Fault

tolerant path
following for a quadrotor,” in 52nd IEEE Conference on Decision
and Control.

IEEE, 2013, pp. 847–852.

VI. DISCUSSION

The simulation results illustrate that the approach in [9] is
too conservative for the considered example, and that the
chosen initial condition does not satisfy the requirements
of the framework in [9]. The addition of the detection
mechanism removes this conservatism and results in im-
proved system performance, even if there is no attack.
It is also important to note the difference between fault-
tolerant control (FTC) (see e.g. [16], [17] in the context
of quadrotor control). The control scheme under the FTC
paradigm assumes that a subset of actuators have failed and
are not operating nominally. Furthermore, the focus of FTC-
based schemes is to control the system with the available
non-faulty actuators. In contrast, the focus of the proposed
scheme is to not only control the system with the non-
vulnerable actuators but also to design them in a way that for
all possible attacked signals, the system is still safe. Thus,
the proposed method is robust against any random actuator
signal.

VII. CONCLUSIONS

We presented a novel attack-detection scheme based on the
control Barrier function. In addition, we introduced an online
QP-based formulation to design a recovery controller that
prevents the system from violating the safety speciﬁcation.
Our formulation is adaptive, in the sense that the further away
the system is from violating safety our recovery controller
focuses on performance rather than safety; however, if the
system keeps approaching the safety limit, our adaptive
mechanism switches to a recovery controller to counteract
the potential attack. We demonstrated the efﬁcacy of the
proposed method on a simulation example involving an
attack on a quadrotor motor.

This work opens up a line of research on non-conservative
control design for CPS security with provable guarantees.
Provable safety guarantees when the system sensors are
under attack is still an open problem. Future work involves
studying more general attacks on CPS, such as attacks on
system sensors and simultaneous attacks on system sensors
and actuators. As noted in Remark 1, our future investigation
also includes studying methods of estimating the time when
the attack has stopped.

VIII. ACKNOWLEDGMENT

The authors would like to thank Dr. Adeel Akhtar for
providing the MATLAB ﬁles for the 3D visualization of the
quadrotor in the simulation video.

REFERENCES

[1] A. Cardenas, “Cyber-physical

systems security knowledge area
issue.” The Cyber Security Body Of Knowledge. [Online]. Available:
https://www.cybok.org/media/downloads/Cyber-Physical Systems Security issue 1.0.pdf

[2] Y. Chen, C. M. Poskitt, and J. Sun, “Learning from mutants: Using
code mutation to learn and monitor invariants of a cyber-physical
system,” in 2018 IEEE Symposium on Security and Privacy (SP).
IEEE, 2018, pp. 648–660.

