2
2
0
2

r
p
A
3

]

R
C
.
s
c
[

1
v
5
8
9
0
0
.
4
0
2
2
:
v
i
X
r
a

Towards Web Phishing Detection Limitations and Mitigation

Alsharif Abuadbba
sharif.abuadbba@data61.csiro.au
CSIRO’s Data61 & Cybersecurity
CRC, Australia

Shuo Wang
Shuo.Wang@data61.csiro.au
CSIRO’s Data61 & Cybersecurity
CRC, Australia

Mahathir Almashor
Mahathir.Almashor@data61.csiro.au
CSIRO’s Data61 & Cybersecurity
CRC, Australia

Muhammed Ejaz Ahmed
Ejaz.Ahmed@data61.csiro.au
CSIRO’s Data61, Australia

Raj Gaire
Raj.Gaire@data61.csiro.au
CSIRO’s Data61 & Cybersecurity
CRC, Australia

Surya Nepal
Surya.Nepal@data61.csiro.au
CSIRO’s Data61 & Cybersecurity
CRC, Australia

Seyit Camtepe
Seyit.Camtepe@data61.csiro.au
CSIRO’s Data61, Australia

ABSTRACT
Web phishing remains a serious cyber threat responsible for the
overwhelming majority of data breaches. Machine Learning (ML)-
based anti-phishing detectors are seen as an effective countermea-
sure, and are increasingly adopted by web-browsers and software
products. However, with an average of 10K phishing links reported
per hour by platforms such as PhishTank and VirusTotal (VT), the
deficiencies of such ML-based solutions are laid bare.

We first explore how phishing sites are bypassing ML-based
detection with a deep dive into 13K phishing pages targeting major
brands such as Facebook and Paypal. Results show successful eva-
sion is caused by: (1) use of benign services such as sites.google
or vu.co to obscure phishing URLs; (2) high similarity between the
HTML structures of phishing and benign pages; (3) hiding the ulti-
mate phishing content within Javascript and running such scripts
only on the client; (4) looking beyond typical credentials and credit
cards for new content such as IDs and documents; (5) hiding phish-
ing content until after human interaction. We attribute the root
cause to the dependency of ML-based models on the vertical feature
space (e.g., URL and webpage content). That is, these solutions rely
only on what phishers present within the page itself.

Thus, we propose Anti-SubtlePhish, a more resilient model based
on logistic regression. The key augmentation is the inclusion of
a horizontal feature space, which examines correlation variables
between the final render of suspicious pages against what trusted
services have recorded (e.g., PageRank and WHOIS age). To harden
against (1) and (2), we correlate information between WHOIS,
Google index ranks, and page analytics. To combat (3), (4) and (5),
we correlate features after rendering the page. Experiments with a
mixture of 100K phishing and benign sites show promising accu-
racy for both reported and undetected phishing pages (96.1% and
98.8% respectively). Moreover, we obtained 100% accuracy against
0-day phishing pages that were manually crafted, comparing well
to the 0% recorded by VT vendors over the first four days.

KEYWORDS
Web Phishing, Logistic Regression, Detection, zero-day phishing.

1 INTRODUCTION
Web phishing is a persistent cyber threat and responsible for over
90% of data breaches [1, 2]. It tries to deceive a page visitor by
pretending as a service the visitor knows in order to steal personal
information such as login credentials. In addition, the proliferation
of people online transactions nowadays such as bills, shopping
and entertainment render the normal user exposure to phishing
attacks inevitable. Phishing webpages costs American business half
a billion dollars a year [3]. Many anti-phishing solutions have been
proposed to detect phishing attacks. Examples include blacklists [4],
heuristic [5], similarity [6, 7], third-party services [8] and machine
learning (ML)-based [9] approaches. However, the nature of these
blacklists explains it’s shortcomings in not being able to detect 0-day
phishing attacks [4]. Similarity and heuristic-based models might
be able to detect 0-day phishing attacks, but have scalability and
accuracy limitations [10]. Relying only on third-party services (e.g.,
search engines) may have a few limitations as identified recently
by Rao et al. [11] such as high false negative in the case of new
benign sites.

On the other hand, ML-based models are the only stream that
shows the ability to detect 0-day phishing attacks while being scal-
able and accurate. Hence, they have been widely explored [10, 12–
16] and integrated into industrial browsers (e.g., Chrome and Edge
[17]). ML-based models depend on the assumption that phishing
and legitimate pages contain statistically distinct patterns in the
domains, HTML content or visual appearance [18]. Consequently,
ML-based techniques learn from an extracted set of features (e.g.,
URL characteristics, DOM structure, etc) from the content of phish-
ing and benign websites, and later try to predict similar patterns.
Hereby, the accuracy of these models relies heavily on the features
selected and how they are impervious to future phishing attacks.
While the use of feature engineering techniques by ML-based
anti-phishing tools have exhibited promises to defeat web phish-
ing and have been adopted by many web browsers and software
products, the number of reported phishing links every hour, 10,000
per hour on average, on the platforms like PhishTank [19] and
VirusTotal [20] reveals a very different story. That is, these ML-
based solutions are not as effective as they should be, which leaves

 
 
 
 
 
 
Abuadbba, et al.

significant questions (i.e., Why is that? How to improve?) yet to be
addressed. Therefore, this work is dedicated to first investigating
the answers to the following research questions (RQ):

RQ1: Why ML-based approaches fail to capture recent phish-
ing attacks?

To answer RQ1, we first investigate the potential limitations by
a deep dive case study across 13,000 phishing pages reported and
manually verified to PhishTank in June 2020 that target famous
victim brands including Facebook, PayPal, Google, Microsoft, eBay,
and Amazon. Our results suggest five phishing trends for why ML-
based on features extracted from only the webpage and its URL
might be limited:

RQ2: How can we improve the recent ML-based approaches?

We here focus on identifying potential features that not only rely
on vertical feature space which phishers present in their URL/ initial
page content but also horizontal feature space by correlating with
collected information from multi-trusted services (e.g., WHOIS,
PageRank and Selenium rendering). We find that it is possible to
build a more resilient ML-based model from these horizontal feature
space with promising results against zero-day phishing attacks. This
is due to the difficulty for the phishers to temper with not only the
webpage itself (vertical feature space) but also many other trusted
services records (horizontal feature space).

Correspondingly, we have made the following contributions:

(1) Use of benign web services to camouflage the phishing pages
such as sites.google, ddns.net, co.vu. For example, the
attacker creates Facebook phishing page and deploy under
benign service as (phishFB.ddns.net) which in turns bypass
URL based phishing detectors such as [21–25] owing to
ddns.net is a benign service.

(2) High similarity in HTML structure between benign and phish-
ing webpages due to the usage of benign services. For instance,
the attacker creates eBay phishing webpage and deploy un-
der sites.google. This means the webpage skeleton struc-
ture of (phisheBay.sites.google) and (benign.sites.google) are
barely distinguishable which in turns contributes to the de-
tection failure of ML-based anti-phishing that rely on HTML
distinguishable characteristics such as [18, 26, 27].

(3) Hiding the ultimate HTML content behind Javascript to block
feature extraction including the title/form and run only at the
client browser. Javascript is a benign web programming lan-
guage that allows the webpage owner to change the content
at the user browser. The attackers exploit that to initially
include benign content not related to phishing target (e.g.,
Facebook) then change the content only after bypassing the
phishing detectors at the user browser when interacting with
the webpage.

(4) Looking for not only credentials and credit cards but also new
custom content such as IDs and documents. Many ML-based
anti-phishing models have been developed to specifically
aim to identify credentials related features such as in [1,
10, 14, 28] which limit their efficacy when they faced with
phishing webpages that target custom content such as IDs
and documents.

(5) Blocking the actual page content until human interacts with
the page. For example, the attacker includes reCAPTCHA
which is a benign tool that enables web hosts to distinguish
between human and automated access to websites. In turns,
ML-based anti-phishing are faced with an empty incoming
webpage which limits their ability to extract vital features.
The webpage phishing content will only appear after the
victim answers the reCAPTCHA challenge.

We derive the root cause as the dependency, to some extent, on
vertical feature-space such as webpage URL and HTML structure.
Vertical refers to extracting in-depth features only from what attack-
ers introduce within their webpages. Motivated by these findings,
we address the second RQ:

• We identify five new web phishing trends that consistently
used to bypass existing ML-based web phishing detectors due
to relying vertically on features extracted from the webpage
URL, HTML content and visual appearance.

• To mitigate these phishing attacks, we propose a novel fea-
ture extraction framework that not only relies vertically on
the initial page URL/content but also horizontally by a cor-
relation with collected information from multi-trusted ser-
vices. Our collected features aim at capturing four identified
components for robust classification: Page Reputation, Goal,
Consistency and Analytics.

• We develop a logistic regression-based model called Anti-
SubtlePhish that relies on the identified vital features to
detect potential webpage phishing.

• We extensively evaluate Anti-SubtlePhish against a large
dataset of 100,000 benign/phishing webpages collected from
PhishTank [19], OpenPhish [29] and Alexa [30]. We also eval-
uate against crafted zero-day phishing attacks simulating the
above trends. Our obtained results demonstrate promising
accuracy between 96.1% and 98.8% over the testing dataset
and accuracy of 100% over 0-day phishing attacks crafted
dataset.

2 BACKGROUND
This section provides the necessary information to understand our
work.

2.1 Website and Webpage Structure
A website is a collection of webpages that offer related content and
can be identified by a common domain e.g., facebook.com. Each web-
page can be accessed via a unique URL such as www.facebook.com/login.
As depicted in Figure 1, a webpage has a tree hierarchy called Doc-
ument Object Model (DOM) which defines the logical structure of
the page to be easily rendered and manipulated by the browsers.
The tree has many types of nodes, named DOM nodes, including
element (e.g., <a> hyperlink element), text (e.g., viewed link text),
attributes (e.g., the actual link), and comment codes that are used
only for illustration.

Besides, scripts such as Javascript code are also a key part of
the DOM tree which allows the webpage owner to manipulate its
content at the user browser by adding, modifying, or deleting the
DOM tree structure. Javascript is a benign programming language
used widely. However, the attackers may also exploit it to disguise

Towards Web Phishing Detection Limitations and Mitigation

their actual phishing until a later stage to run only after reaching
the victim user as detailed in Section 3.

during learning. The hypothesis can be presented as
ℎ𝜃 (𝑥) = 𝜃0 + 𝜃1𝑥1 + 𝜃2𝑥2 + ... + 𝜃𝑛𝑥𝑛

(1)

• Cost Function: after calculating the hypothesis, the cost
function is used to measure how far our estimation is from
the ground truth 𝑦 and can be calculated for 𝑚 samples as

𝐽 (𝜃 ) =

1
2𝑚

𝑚
∑︁

𝑖=1

ℎ𝜃 (𝑥 (𝑖) ) − 𝑦 (𝑖)

(2)

• Gradient Decent: is a general function for minimizing the
cost function to be as close as possible to the ground truth
𝑦. We use calculated cost J(𝜃 ), a learning rate 𝛼 and partial
derivative 𝜕 of the initial 𝜃𝑖 to calculate new parameter 𝜃 𝑗 as
𝜕
𝜕𝜃𝑖

𝜃 𝑗 = 𝜃 𝑗 − 𝛼

𝐽 (𝜃𝑖 )

(3)

Figure 1: Webpage Document Object Model Structure

TRENDS EXAMPLES

These three stages will be repeated until the model produces
the best convergence.

3 KEY INSIGHTS: NEW WEBPHISHING

2.2 Web Phishing
Web Phishing is a type of social engineering attack [31] often used
to steal user sensitive information, including login credentials and
credit card details. It happens when a phisher firstly creates a fake
webpage, masquerading as a trusted brand, and then distributes
its URL to lure victim users to click on the link which in turn
redirects them to that phishing webpage to extract users sensitive
information [1]. Phishers also create fake websites to be as similar
as possible to known entities’ websites to the user in terms of visual
appearance or/and URLs.

The life cycle of web phishing, as extensively studied recently
by Adam et al. [32], goes through various steps in a lifespan av-
erage of days or even hours. First, phishers start by configuring a
fake website of a targeted victim brand before deploying publicly
with a phishing URL. Then, the phisher launches a campaign to
distribute the phishing URL through emails, instant messages, or
text messages. Some victim users start visiting the fake website and
few provide their sensitive information.

2.3 ML-based Anti-Phishing
ML-based anti-phishing methods typically extract a set of features
𝑥1, 𝑥2, .., 𝑥𝑛 of webpage’s URL, DOM trees, logos, icons, visual ap-
pearance, and elements of legitimate/phishing websites. These fea-
tures are fed into learning algorithms along with the ground truth
label 𝑦 (e.g., legit/phish) which in turn produces a classification
model to distinguish between legitimate and phishing websites.
Based on many existing ML-based anti-phishing solutions such
as [10, 12–16], we can generalise the underline ML classifiers pro-
cess into the following three stages:

• Hypothesis: a model representation that tries to estimate
or map input features 𝑥𝑠 to output 𝑦 using model parameters
called 𝜃 . 𝜃 starts with initial values while being updated

The focus is to answer the first research question:

RQ1: Why ML-based approaches fail to capture recent phishing
attacks?

Phishing pages often aim to lure users by imitating commonly
used legitimate sites that they already know such as Facebook or
PayPal. Most of the existing ML-based web phishing detection meth-
ods start by extracting features from unrelated pages categorised
as phishing and benign. Namely, the phishing pages might be col-
lected in bulk from PhishTank and unrelated to benign pages that
are collected in bulk from another source like Amazon Top-sites
links from Alexa. While this process may produce reasonable detec-
tion accuracy, but nevertheless it is not taking the direct approach
of inspecting benign sites against related phishing pages that tar-
get the same brand. Thus, it may fail to reveal some insights that
assist us to understand potential trends and limitations. Therefore,
this motivates us to start by deep-dive case study, we call it the
apple-vs-apple approach to identify potential limitations. In this
approach, we analyse the famous victim brands (e.g., Facebook) vs
their phishing counterparts (Reported phishing page of Facebook).
Our Insights Dataset. We build our own dataset of 13000 phish-
ing pages reported and manually verified to PhishTank [19] in June
2020 that target top 6 famous victim brands including Facebook, Pay-
Pal, Google, Microsoft, eBay, and Amazon. Those brands are selected
as they have collectively over 4 billion users and understanding
the phishing trends targeting them would have high significance.
We then perform a deep dive case study across those 13000 phish-
ing samples. In the following, we present five identified evasion
strategies to limit the efficacy of existing ML-based detection mech-
anisms.

3.1 Using Benign Web Services to Camouflage

Phishing URLs

Many previous research studies have focused on the efficacy of
phishing sites by analysing deceptive URLs (containing the name

WebpageDocumentRoot element<html>Element<head>Element<title>Element<body>Element<a>Text“link text”Attributehrefof linkElement<h1>URLhttp(s)://www.example.xyzDocumentObjectModel (DOM)Abuadbba, et al.

on the HTML DOM structure as a source of ML-based features is
less effective owing to the difficulty of identifying any patterns
between phishing and benign sites.

Table 1: The percentages of phishing webpages with high
DOM structure similarity to sites on benign Services.

𝐹𝑎𝑐𝑒𝑏𝑜𝑜𝑘 𝑃𝑎𝑦𝑃𝑎𝑙 𝐺𝑜𝑜𝑔𝑙𝑒 𝑀𝑖𝑐𝑟𝑜𝑠𝑜 𝑓 𝑡
12.3% 23.9%
24.4%

18.3%

𝑒𝐵𝑎𝑦 𝐴𝑚𝑎𝑧𝑜𝑛
16.4% 21.7%

3.3 Hiding the Ultimate HTML DOM Content

behind Javascript

Usually, ML-based phishing detection systems scrape the HTML
DOM structure of the incoming URLs and extract features to classify
webpages. During our analysis, we find a new trend that attackers
recently used where they hide the final phishing page HTML struc-
ture behind a Javascript call that only execute at the user browser.
Web browsers have rendering engine that takes HTML code and
interprets it into what you see visually. It also call Javascript engine
to execute any Javascript commands within the HTML and retrieve
the relevant items from the remote servers like Images, which then
manipulate the HTML elements. The Javascript part is the more
dangerous because at the initial stage it looks as few commands
but after rendering it could transform the content completely.

Figure 5 shows an example from the phishing webpages we ob-
tained from PhishTank on the 24th of June 2020. When we directly
scrape the HTML DOM structure, we get the output (a) where
the attacker uses shorten URL service, general title "Home" and
a Javascript call to "Cloudfront.net" which is benign content (e.g.,
video, other files) delivery network offered by Amazon Web Ser-
vices. However, after we render the webpage, using chrome render-
ing engine, the Javascript call is executed and we get the output
(b) where the URL is redirected to benign service to deploy front-
end applications called "vercel.app", the title turned into "Facebook
Video", and two input text-box appeared to collect email/password.
We detect 8%-20% of the phishing webpages using some form of
Javascript camouflage to their actual intent as shown in Table 2.
This trend indicates that the attackers try to bypass the existing
ML-based detection system that relies on features extracted verti-
cally from only scraping the static webpage’s URL and HTML DOM
structure to build the classifier. Namely, the attackers conceal the
malicious content including the title/forms and only rending them in
late-stage at the client browser which usually comes way after they
bypass the classifiers.

Table 2: Phishing webpages using Javascript.

𝐹𝑎𝑐𝑒𝑏𝑜𝑜𝑘 𝑃𝑎𝑦𝑃𝑎𝑙 𝐺𝑜𝑜𝑔𝑙𝑒 𝑀𝑖𝑐𝑟𝑜𝑠𝑜 𝑓 𝑡
17.3%

19.8%

13.9%

9.4%

𝑒𝐵𝑎𝑦 𝐴𝑚𝑎𝑧𝑜𝑛
8.4%

16.7%

3.4 Looking for not only Credentials but Broad

Identity Documents

Figure 2: Benign Services used to deploy phishing webpages.

of the victim brand) v.s. random URLs. However, we are not aware
of an existing study that looks beyond that to examine the pro-
portion of where these sites are hosted. Our analysis suggests that
there is a new phishing trend where attackers try to camouflage
phishing URLs by deploying websites on benign services such as
sites.google, 000webhostapp, ddns.net, co.vu, etc. We iden-
tify 16 benign services as shown in Figure 2. Recent concurrent
work by De Silva et al. [33] also confirmed a similar trend in more
general malicious URL datasets collected from VirusTotal and not
specific to phishing.

Figure 4 exemplifies our findings which shows a reasonable
proportion of the phishing websites that target the top 6 victim
brands are deployed on various benign web services. Namely, 2047
phishing websites target Facebook and 1058(51.6%) of them are de-
ployed on benign services. 5922 phishing websites targeting PayPal
and 1909(32.2%) are deployed on benign sites. 885, 2346, 1288 and
391 target Google, Microsoft, eBay, and Amazon respectively. Out
of these phishing sites, 32%-40% are deployed on benign services.
Hence, while many ML-based anti-phishing models such as [21–25]
that relies on detecting phishing by analysing the URLs may still
effective to some extent, but nevertheless this trend limits their
efficacy as the URL is from a benign service. In other words, the
victim user is lured by a benign URL into a phishing webpage.

3.2 High Similarity in HTML DOM Structure
Due to some of the limitations of the ML-based models relying
only on the URL, many existing works [18, 26, 27] go beyond that
using the HTML Document Object Model (DOM)1 structure to
identify abnormal patterns as features to detect phishing. They do
that by collecting two corpora of phishing and benign webpages.
Although this stream of models may have some efficacy, but we
identify a new trend that may limit their accuracy. That is, several
phishing websites that target the top 6 brands and deployed on be-
nign services (e.g., phisheBay.ddns.net) and have high HTML DOM
structure similarity to other pages deployed on benign services (e.g.,
benign.ddns.net).

Figure 3 illustrates that by visualising two examples obtained
from ddns.net and 000webhosting.com. By using HTML2GDL2
library, we exemplify the visual HTML DOM structure of benign
and phishing websites deployed on two benign services. Table 1
shows the percentages of phishing webpages that target the 6 top
brands and have their DOM structure with high similarity to benign
services. This indicates that attackers increasingly employing this
trend to ensure their phishing webpages have indistinguishable
HTML DOM structure from benign services. Hence, the focus only

1When a web page is loaded, the browser creates an HTML DOM of the page
2https://www.burlaca.com/2009/01/html2gdl/

Our analysis suggests that phishers are not only looking merely
for credentials such as usernames and passwords, they also seek

Docs.google, Storage.googleSites.googleFirebase.googleDrive.googleCloud.googleOnedrive.liveSharepoint.comForms.officeCloudfront.netBlogspot.comCo.vuDdns.net000webhost.comDropbox.comRockysite.netGoogle servicesMicrosoft servicesOther servicesTowards Web Phishing Detection Limitations and Mitigation

Figure 3: Examples of the HTML DOM structure between phishing and benign pages on two benign services ddns.net and
000webhosting.com. The circles represents high level HTML tags such as <HTML> or <BODY>. The benign services used have
an HTML template which demonstrates high similarity.

Figure 4: Results of phishing sites that target the top 6 vic-
tim brands and reported to PhishTank in June 2020 against
how many of these sites deployed on benign services to cam-
ouflage their URLs.

Figure 5: Example of how phishers hide the final HTML
DOM structure behind Javascript. (a) Prior rendering by
the browser has nothing abnormal or related to Facebook.
(b) Post rendering at the client browser where the phisher
presents the actual goal to phish Facebook users

identity documents and photographs that they may monetize later.
To be deceptive in luring the user, the attackers show a high level of
professionalism similar to benign services by providing illustrations

Figure 6: Example of a new trend where phishers not only
collect credentials but also target identity documents. They
do that with industry-standard professionalism including il-
lustrations and steps to guide the user through the process.

and steps to guide the user through as shown in Figure 6. While
this trend is less frequent than the previous three but still worth
highlighting as it may limit the efficacy of existing ML-based models
that developed intensively focusing on detecting phishing pages
that look for credentials such as in [1, 10, 14, 28].

3.5 Pretending as not Valid or Looking for

Human Verification

We observe that 5%-15% of phishing pages pretending to be not valid
by presenting one of the known web interaction HTTP protocol
tags such as “Page Not Found 404”, “404 Error Page”, “OOPS! Page
Not Exist”, “access Forbidden”, etc. This is only presented within the
body of the page but the actual HTTP protocol tag is "success 200".
In other words, the webpage is still active but only camouflaging
the content due to phishing kit cloaking strategy which might
be an effort to only target users from certain time zones or to
avoid crawlers. Another rarely appeared but worth highlighting
observation is that few pages presenting the reCAPTCHA system
with no other content, that tries to distinguish between human and
automated access to websites. This is an effort from the phisher to
phase out any web crawlers or ML-based systems that detect based
on the page content and wait for human action to present the lure
phishing content (e.g., login forms).

http://foregrounder6.ddns.net/go.phphttp://yourname.ddns.nethttps://parlando-analyzer.000webhostapp.com/https://yoursite.000webhost.com/PhishingPhishingBenignBenign(a) ddns.net(b) 000webhosting.com20475922885234612883911058190935682841412601000200030004000500060007000FacebookPayPalGoogleMicrosofteBayAmazonTotal Number of websitesTotal Phishing SitesNumber of them Deployed on Benign ServicesShorten URL: https://shorturl.at/gNPR0<!DOCTYPE html><title>Home</title>………………<scripttype="text/javascript" src="https://d1zviajkun9gxg.cloudfront.net/content/index.js"></script>Real url: https://18-d30ximnwn.vercel.app/<!DOCTYPE html><title>Facebook Video</title>…….<inputrequired="" type="text" class="input-text" name="id" placeholder="Emailor phone number"> <inputrequired="" type="password" name="pass" class="input-text" placeholder="Password"> <buttonclass="btnblue">Continue</button>Phishingwebpage(a) Before renderingbythebrowser(b) After rendering by browserMasked to protect the privacyAbuadbba, et al.

Figure 7: High-level design of Anti-SubtlePhish

Summary: To answer RQ1, we have identified 5 web phish-
ing trends that may limit the efficacy of ML-based detection
models which rely only on what phishers present in their web
URL address or page content.

limitations against newly registered legitimate sites and updated
URLs as studied by Rao et al. [34]. Therefore, by correlating the
information from these trusted services together, we demonstrate
the ability of better detection as will be shown shortly.

4 ANTI-SUBTLEPHISH SYSTEM DESIGN
In this section, we provide the Anti-SubtlePhish detection system
exploiting the above-identified insights to answer the RQ. 2:

RQ2: How can we improve the recent ML-based approaches?

We first define the threat model that we focus on in this paper.
We then provide an overview of the Anti-SubtlePhish detection
system that can effectively distinguish phishing webpages from
benign webpages.

4.1 Threat Model
We consider phishing pages targeting famous victim brands’ web-
sites as they have significant number of users. We assume that
the attacker would be motivated to target websites that are widely
known and trusted by using subtle phishing techniques such as
camouflaging the URL, having high similarity in website HTML
content with benign services (e.g., sites.google) by deploying into
these benign services, hiding the page goal behind a Javascript and
running only at the user browser, or finally blocking the content un-
til human interaction. We assume that the attacker could still craft
the phishing page to be fully or partially similar to any page from
the targeted websites. We finally assume that the attacker sends
those phishing webpages links through luring emails to victims.

4.2 Overview of Anti-SubtlePhish
The overview of Anti-SubtlePhish is illustrated in Figure 7. It in-
volves three stages: Information retrieval, correlation and feature
extraction, and phishing detection. The aim of these stages is to
overcome the challenges of relying on what phishers present ini-
tially to camouflage the page URL and HTML DOM structure as
shown in Section 3. The intuition is that we believe that multiple
trusted domain can provide the information that help to ascertain
the legitimacy of the webpage once correlated with the information
within the webpage itself. Therefore, we retrieve information from
multi-trusted services (e.g., WHOIS, Google Index, Selenium, GSB
Reporting) which we then correlate to extract reliable features that
provide effective detection. While some of these services have been
used solely to detect phishing, but nevertheless each has some

Information retrieval. This is stage 1 as shown in Fig-
4.2.1
ure 7 which aims at collecting information from multi-trusted ser-
vices. In specific, we use the initial page URL to retrieve information
from four services.

(a) WHOIS: is a query and response protocol that is widely
used for querying trusted databases that store some metadata about
domain names. We use Python wrapper (https://pypi.org/project/
whois/) for Linux “WHOIS” command that is able to extract data
for all the popular top-level domains such as (com, org, net, biz, info,
pl, jp, uk, nz, . . . ).

(b) Google Index: is a search engine ranking results provided
after rigorous examination from Google crawlers. We utilize Python
wrapper (https://pypi.org/project/google-api-python-client/ to hook
up with Google Custom search (https://developers.google.com/
custom-search/v1/overview). We check the presence of domain
names and retrieve the top ten matches with their metadata.

(c) Selenium Rendering: is browser-based regression automa-
tion that enables simulation of rendering webpages as Chrome or
Firefox. The reason behind that is to combat phishing trends where
the goal of the page is hidden behind Javascripts and only run at
the user browser. We use Python wrapper (https://pypi.org/project/
seleniumwrapper/) to retrieve both the URL and the actual HTML
DOM content before and after rendering.

(d) GSB Reporting: after rendering the page using Selenium,
we use Google safe browsing (GSB) reporting API to retrieve any
available information about the domain name. We use the publicly
available API provided by Google (https://developers.google.com/
safe-browsing/).

4.2.2 Correlation & Feature Extraction. The stage 2 aims at
correlating and extracting potential features that would be reliable
for the ML training and detection phase. We use the Pearson Cor-
relation Coefficient [35] to measure how closely two sequences of
extracted features and produce correlated coefficient which could
be a robust feature for classification. From the collected features
in stage 1, we identify 13 features from both vertical feature-space
and correlated with horizontal feature-space under 4 categories –
proposed to capture comprehensive views about the webpage. The
four categories are the website Reputation, Goal, Consistency and
Analytic. Next, We explain these features and categories in detail.

Page URLGoogleRankingWhoisSeleniumRenderingGSBReportingInformation RetrievalConsistencyReputationAnalyticGoalCorrelation & Feature ExtractionTraining dataPhishing DetectionClassifierLearningClassifierPredictionPhishBenign123Towards Web Phishing Detection Limitations and Mitigation

(i) Reputation: Under this category, we examine the web-page
reputation in terms of validity, active duration(age), ranking, and
reported by others as suspicious. For validity, we look for webpages
that pretend to be not valid. To do so, we first render the webpage
to execute all its scripts by using chrome rendering engine and
Selenium. We then identify validity indicators using many selected
keywords from exploring the phishing trends such as "no longer
available, not found, unpublished, does not exist, 404, access forbid-
den, etc.". For the active duration(age), we identify that the public
WHOIS service would provide Creation Date as a tag field in their
response. However, we experienced the challenge that registrars
may use other tags for this field. We, therefore, identify many of
these alternatives used by different registrars such as "Registration
Time, Registered Date, Commencement Date, Changed Date, Regis-
tered On, Created On, etc.". We also identify that relying only on
the active duration(age) of the website with the assumption that
phishing websites are always new would not be a reliable feature.
Figure 8a shows the retrieved creation date of 1000 top sites vs
1000 phishing sites targeting them. It is clear that they have a wide
overlap which confirms our finding in the insight Section 3.1 as
phishers are using benign services to deploy phishing sites. To
overcome this challenge, we correlate the active duration(age) with
the obtained ranking of the page as shown in Figure 8b bottom-left
which clearly could be a reliable feature for detection.

(ii) Goal: Under this category, we concentrate on the webpage
goal by examining two things: Does the webpage looks for input?
Does the page redirect you? To do so, we correlate the initial URL
with the actual URL after rendering the webpage using chromium
engine and Selenium. We use Levenshtein distance3 as a string
metric for measuring the difference between two sequences. The
Levenshtein distance 𝜈 between two strings 𝑥, 𝑦 (of length |𝑥 | and
|𝑦| respectively) is shown in Eq. 4.

𝜈 (𝑥, 𝑦) =

1 + 𝑚𝑖𝑛






|𝑥 |
|𝑦|
𝜈 (𝑡 (𝑥), 𝑡 (𝑦))
𝜈 (𝑡 (𝑥), 𝑦)

𝜈 (𝑥, 𝑡 (𝑦))
𝜈 (𝑡 (𝑥), 𝑡 (𝑦))



𝑖 𝑓 |𝑥 | = 0,
𝑖 𝑓 |𝑦| = 0,
𝑖 𝑓 𝑥 [0] = 𝑏 [0]

𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒,

(4)
where the 𝑡 of string 𝑥 is a sequence of all but the first character of
𝑥. Note the elements in the minimum 𝑚𝑖𝑛 corresponds to rotations
through both strings 𝑥 and 𝑦. Figure 9 clearly demonstrates that
while the distance between the initial URL vs the actual URL after
the rendering is close to zero for benign sites, it is much larger,
e.g., > 30 for phishing websites. We also decide if the webpage is
looking for input after rendering the page by identifying forms,
credit card fields, passwords, emails, IDs, photographs, etc.

(a) Histogram of the creation age retrieved
from only WHOIS of a top 1000 benign
sites vs 1000 phishing sites targeting them.
The obtained values show wide overlap
which means relying only on the WHOIS
database might not be sufficient.

(b) Correlating the website age with ranking demon-
strates better reliable feature for ML model to classify
benign vs phishing.

Figure 8: Example of correlating website creation age from
WHOIS (a) with ranking from Google index as shown in (b)
bottom-left would be preferable for ML models to detect.

(iii) Consistency: Under this category, we aim at the consis-
tency of the initial URL, the actual URL, and the webpage HTML
content after rendering. We also examine if the actual URL has been
deployed on free hosting or benign services such as sites.google,
ddns.net, vu.co, branch.io, etc

(iv) Analytic: Under this category, we focus on performing
analytics on actual URL-related features after rendering to identify
common phishing trends. Among those, we selected a few that

3https://pypi.org/project/python-Levenshtein/

Figure 9: Histogram of the distance between the initial URL
vs the actual URL after Selenium rendering. We see that the
URLs distance of benign sites is close to zero, but it is much
larger >30 for phishing websites.

19851990199520002005201020152020Webpage Age020406080100120Frequencyphish agebenign age1990200020102020Age198020002020Age0.00.20.40.60.81.0Rank0.00.51.0RankLabelphishbenign0100200300400500Distance between the original URL and redirected URL050100150200250Frequencybenign URLphish URLwere already been identified by researchers and still applicable,
such as IP address within the URL [10], suspicious symbols e.g.,
’@, -’ [36], number of sub-domains [37], and domain length [38].
We also observe a new trend where even though the benign URL
might be longer than phishing, but the main part of the domain of
phishing websites is much longer than the benign in the case of
setting up new domains.

Phishing Detection. The stage 3 aims at training the
4.2.3
ML model over the extracted features and measuring the efficacy.
We select the Logistic Regression (LR) which is a common ML
technique to build a model that can discriminate between samples
from two classes. The LR can be explained with Logistic function,
also known as Relu function that take any real input 𝑥, and outputs
a probability value between 0 and 1 which is defined in Eq. 5.

ℎ𝜃 (𝑥) =

1
1 + 𝑒−𝜃𝑇 𝑥

(5)

where 𝜃 is model parameters, 𝑥 is the input features and 𝑇 indi-
cates transpose process. The learning process is similar to what we
explained in Section 2.3 and the model fit using the above Logistic
function can be visualised as shown in Figure 10.

Figure 10: Example of Logistic regression function “relu”
that classifies input 𝑋 that has 𝑛 features into two district
values as 0 or 1.

Summary: To answer RQ2, we have developed a Logistic
regression-based model that can learn from correlated fea-
tures extracted from the information collected from multi-
trusted services. We have demonstrated the efficacy of some
of these correlations. Next, we focus on evaluating the detec-
tion accuracy of the developed model.

5 EVALUATION
This section introduces the performance evaluation results of Anti-
SubtlePhish including its efficacy against zero-day web phishing
attacks in a comparison with industry-standard ML-based phish-
ing detectors such as CyberCrime, Forcepoint, Fortinet, Kaspersky,
Netcraft, and Microsoft SmartScreen, and VirusTotal other ven-
dors4.

4https://www.virustotal.com/gui/

Abuadbba, et al.

5.1 Dataset Collection and Experiments

Settings

To evaluate the performance of Anti-SubtlePhish, we collected be-
nign and phishing pages from publicly available resources which is
different from our insights dataset discussed in Section 3 to avoid
bias. For phishing websites, we collected 50,000 URLs from Phish-
Tank [19] and OpenPhish [29]5 over the period Jun to September
2020. For benign website, we collected 50,000 URLs from Alexa
Topsites Amazon service [30]6. We pass these URLs to the informa-
tion retrieval stage 1 as shown in Figure 7 to collect the relevant
information from the aforementioned trusted entities. To evaluate
a zero-day phishing attack scenario, we also setup 9 phishing sites
using the identified insights. In summary, we eliminate any dupli-
cates and split the dataset into four groups to carry out thorough
experiments as follows:

• Targeted dataset (Dtarget): Dtarget includes the top 1000
benign sites globally vs 1000 phishing sites that target top
victim brands discussed in Section 3.

• Small dataset (Dsmall): Dsmall includes the top 5000 benign
sites vs 5000 random phishing sites to examine the efficacy
of the obtained features.

• Large dataset (Dlarge): Dlarge includes 44,000 benign sites

vs 44,000 phishing sites.

• Zero-day dataset (Dzero): Dzero includes 9 hand-crafted
phishing sites that we set up to examine the potential of
Anti-SubtlePhish in comparison to other algorithms against
unseen phishing webpages using the identified insights.

5.2 Evaluation Metrics
The detection accuracy of Anti-SubtlePhish is evaluated with five
metrics, accuracy, precision, recall, false acceptance rate (FAR), and
false rejection rate (FRR), which are popularly used to evaluate the
performance of classifiers.

• Accuracy (Acc.) is the percentage of correctly classified

webpages by a detection method.

• Precision (Pre.) is the percentage of webpages classified as
phishing by a detection method, which are actual phishing
webpages.

• Recall (Rec.) is the percentage of phishing webpages that

were accurately classified by a detection method.

• FAR is the percentage of phishing webpages that are classi-

fied as benign webpages by a detection method.

• FRR is the percentage of benign webpages that are classified

as phishing webpages by a detection method.

In general, while FRR is an indication of detection systems’ reli-
ability, FAR shows the security performance. Ideally, both FRR and
FAR should be 0%. Often, a detection system tries to minimize its
FAR while maintaining an acceptable FRR as a trade-off, especially
under security-critical applications.

5.3 Results of Web Phishing Detection
Table 3 shows the obtained results from our logistic regression-
based Anti-SubtlePhish model vs various other ML models such

5https://openphish.com/
6https://www.alexa.com/topsites

Towards Web Phishing Detection Limitations and Mitigation

as Decision Tree, KNN, Naive Bayes, Random Forest, and SVM. In
this experiment, we use Dtarget dataset. It is clear that our Anti-
SubtlePhish could achieve the highest accuracy of 98.8%, precision
of 99.1%, and recall of 97.8% in comparison to other ML-based
models. It also produces the lowest FAR of 0.9% and FRR of 2.2%.
These results demonstrate that correlating collected features from
multi-trusted entities could detect a substantial number of the subtle
phishing tricks we identified against the top targeted sites.

Table 3: Dtarget Dataset Results.

𝐿𝑜𝑔𝑖𝑠𝑡𝑖𝑐 𝑅𝑒𝑔.
𝐷𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇 𝑟𝑒𝑒
𝐾𝑁 𝑁
𝑁 𝑎𝑖𝑣𝑒 𝐵𝑎𝑦𝑒𝑠
𝑅𝑎𝑛𝑑𝑜𝑚 𝐹𝑜𝑟𝑒𝑠𝑡
𝑆𝑉 𝑀

𝐴𝑐𝑐.
𝑅𝑒𝑐.
𝑃𝑟𝑒𝑐.
98.8% 99.1% 97.8%
95.3% 97.8% 93.7%
93.7% 97.1% 90.9%
95.3% 99.5% 91.4%
95.4% 97.1% 94.9%
95.9% 98.3% 91.5%

𝐹𝐴𝑅
0.9%
2.2%
2.9%
0.5%
2.9%
1.7%

𝐹𝑅𝑅
2.2%
6.3%
9.9%
8.6%
5.9%
8.5%

To generalise our findings to the non-targeted dataset, in the
next experiment we use Dsmall dataset that includes more diverse
and general phishing pages. In specific, it includes 5000 benign sites
and 5000 random phishing sites. Table 4 shows the obtained results.
While there is a slight drop in the accuracy compared to Dtarget
dataset, but nevertheless the Anti-SubtlePhish based logistic regres-
sion model still achieves high accuracy of 96.8%, Precision of 97.7%,
Recall of 96.6%. It also maintains a low FAR of 2.3% and FRR of
3.4%. This illustrates that correlating collected information from
multi-trusted entities could also detect a diverse range of phishing
sites.

Table 4: Dsmall Dataset Results.

𝐿𝑜𝑔𝑖𝑠𝑡𝑖𝑐 𝑅𝑒𝑔.
𝐷𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇𝑟𝑒𝑒
𝐾𝑁 𝑁
𝑁 𝑎𝑖𝑣𝑒 𝐵𝑎𝑦𝑒𝑠
𝑅𝑎𝑛𝑑𝑜𝑚 𝐹𝑜𝑟𝑒𝑠𝑡
𝑆𝑉 𝑀

𝐴𝑐𝑐.
𝑅𝑒𝑐.
𝑃𝑟𝑒𝑐.
96.8% 97.7% 96.6%
95.8% 94.6% 95.9%
94.9% 93.8% 93.6%
90.1% 95.1% 78.5%
96.1% 94.6% 96.2%
95.7% 92.3% 96.5%

𝐹𝐴𝑅
2.3%
5.4%
6.2%
4.9%
5.4%
7.7%

𝐹𝑅𝑅
3.4%
4.1%
6.9%
21.5%
3.8%
3.5%

To explore the Anti-SubtlePhish efficacy on a large scale general
phishing dataset, in the next experiment we use Dlarge that in-
cludes 44,000 benign sites and 44,000 phishing sites. Table 5 presents
the obtained results which demonstrate that our Anti-SubtlePhish
based logistic regression model achieves high accuracy of 96.1%, Pre-
cision of 96.7%, Recall of 96.3%. It also preserves a low FAR of 2.6%
and FRR of 3.6% similar to the previous Dsmall experiments. It is
also worth mentioning that in this large scale experiment, we have
not removed any outliers from the phishing listed on PhishTank
as many existing works do to be as close as possible to real-world
scenarios. Hereby, our results indicate that correlating information
from multi-trusted parties is a reliable way of detecting phishing
websites.

Table 5: Dlarge Dataset Results.

𝐿𝑜𝑔𝑖𝑠𝑡𝑖𝑐 𝑅𝑒𝑔.
𝐷𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇𝑟𝑒𝑒
𝐾𝑁 𝑁
𝑁 𝑎𝑖𝑣𝑒 𝐵𝑎𝑦𝑒𝑠
𝑅𝑎𝑛𝑑𝑜𝑚 𝐹𝑜𝑟𝑒𝑠𝑡
𝑆𝑉 𝑀

𝐴𝑐𝑐.
𝑅𝑒𝑐.
𝑃𝑟𝑒𝑐.
96.1% 96.7% 96.3%
95.4% 95.6% 95.2%
91.6% 94.3% 88.4%
89.6% 92.3% 86.3%
94.3% 93.8% 93.1%
93.1% 93.3% 92.6%

𝐹𝐴𝑅
2.6%
3.4%
5.7%
7.7%
6.2%
6.7%

𝐹𝑅𝑅
3.6%
4.8%
11.6%
13.7%
6.9%
7.4%

5.4 Zero-day Phishing Detection
One essential expectation from ML-based models is to detect new
phishing pages called zero-day attacks. While our Anti-SubtlePhish
based logistic regression model demonstrates reasonable detec-
tion efficacy, but nevertheless we previously tested only with al-
ready reported phishing pages to PhishTank. Therefore, to examine
the efficacy of our Anti-SubtlePhish based model against zero-day
web phishing attacks, we setup 9 phishing pages shown in Fig-
ure 11. The websites are Facebook, eBay, Netflix, Zoom, Shopify,
ANZ, Telstra, Kmart, meetup. We ensure selecting a wide range of
services including social media, e-shopping, banks, video confer-
encing, telecommunications, and streaming. We follow standard
ethical considerations including obtaining ethical clearance, reach-
ing out to these organisations to let them know where possible
and not capturing any credentials nor distributing any links. The
steps of our experiments are as follows. (1) Setup zero-day phish-
ing attack webpages of those brands with the identified subtle
phish trends highlighted in the Key Insights Section 3. (2) Deploy
these 9 webpages under benign services with randomly chosen do-
main names (e.g., https://rU47H?urZBU8zBSd.co.vu) to avoid being
inadvertently visited by real users. (3) Examine those 9 domain
names against our Anti-SubtlePhish as well as various industry-
standard ML-based phishing detectors via VirusTotal APIs such as
CyberCrime, Forcepoint, Fortinet, Kaspersky, Netcraft, Microsoft
SmartScreen, along with other VirusTotal vendors.

Figure 12 shows the obtained results over 11 days period. While
our Anti-SubtlePhish detected all 9 out of 9 deployed pages on the
first day, other industrial ML-based services did not flag these pages
until the fifth day. Microsoft SmartScreen was able to flag 8 out
9 after 11 days. This illustrates the detection efficacy of our Anti-
SubtlePhish based model by relying on correlating the collected
information from multiple trusted services.

6 DISCUSSION AND FUTURE WORK

Considerations for adaptive attacks: Anti-SubtlePhish is built
upon correlating collected information from not only the webpage
itself but also multiple trusted parties such as WHOIS, Google
Ranking and Selenium Rendering. We also noted from our key
insights analysis that many existing adaptive attacks are developed
to bypass a single source of information. For example, ML-based
models [21–25] that relies on using URLs as a single source for
detection have been bypassed by deploying phishing webpages
under benign hosting services. In other words, the content of the
phishing webpage is still as it is but the URL is a benign link. A few
other ML-based models [18, 26, 27] that rely on the DOM structure

Abuadbba, et al.

Figure 11: Snapshots of the nine(9) phishing pages that we created for the zero-day attack experiments. Each white-page shows
2 images: (top) the original webpage, (bottom) the fake webpage we crafted.

TLDs might not be accessible through general WHOIS which might
be challenging especially in real-time deployment scenarios. One
mitigation approach is to reach out to other regional Domain Name
Service (DNS) servers to get information about these domains. (2)
Some of the collected information from the trusted third party ser-
vices might not be organised nor easy to extract what we want
autonomously. For example, WHOIS information usually provides
the age of the domain, but this piece of information might be pre-
sented under many tags such as "Registration Time, Registered Date,
Commencement Date, Changed Date, Registered On, Created On, etc.".
During our experiments, we had to go through a large number of
samples to identify those tags. However, we expect there might
be more variations of these tags which might be challenging on a
real-time basis. One mitigation strategy might be to employ a more
sophisticated unsupervised information extraction mechanism over
unstructured data to be able to extract the dates from unseen tags. (3)
Most of the third-parties trusted entities may charge for the usage
of collecting their information such as Google Ranking. They might
also have imposed a daily limit (e.g., 10,000, 20,000, etc) on these
services. Therefore, one would need to consider these challenges
and obtain a custom access package to deploy Anti-SubtlePhish in
real-time.

7 RELATED WORK
There are many anti-phishing solutions that have been proposed
in the literature. Examples include blacklists [4], heuristic [5], simi-
larity [6, 7] and machine learning (ML)-based [9] approaches. Each
category has some challenges. For instance, the nature of these
blacklists explains it’s shortcomings by not able to detect 0-day
attacks [4]. In addition, similarity and heuristic-based models may
be able to detect 0-day attacks but have scalability and accuracy
limitations [10]. Lately, ML-based models are the only stream that
shows the ability to detect 0-day attacks while being scalable and
accurate. We next summarise the anti-phishing detection models
based on the source of information they rely on to learn and detect.

Image Similarity: models under this category try to learn the
similarity to existing legitimate websites. This similarity can be (1)
Visual-based, (2) Source-based, or (3) Address-based. In (1) Visual-
based, the model infers the similarity by learning from visual screen-
shots. As examples, Fu et al. [6] used Earth Mover’s Distance (EMD)
to compute similarity, then Zhang et al. [39] used EMD along with
textual features. Cheng et al. [40], Dunlop et al. [41] used logo
retrieval to determine a website identity that might be tricked by
omitting the logo. Chen et al. [42] approximated human perception

Figure 12: Comparison between our Anti-SubtlePhish and
various industrial ML-based detectors. Anti-SubtlePhish
could detect all 9 zero-day crafted pages from the first day,
whereas other vendors started to detect from the fifth day.

as a single source to identify phishing have been bypassed by using
similar DOM structures to benign sites. Therefore, while it seems
slightly easy for a phisher to develop an adaptive attack to bypass
a single source of information, but nevertheless it would be more
challenging to bypass not only the page information alone but also
correlated information collected from multiple trusted entities.

Why detection accuracy dropped in the large scale experi-
ment? We develop Anti-SubtlePhish from the insights obtained
from Dtarget that contains 1000 confirmed phishing webpages
targeting the top famous brands. We achieved an accuracy of 98.8%.
On the other hand, we achieve an accuracy of 96.1% in the large
scale experiments. We use Dlarge that contains 44,000 reported
suspicious webpages to PhishTank where some of them might not
be confirmed phishing yet. We also observe that the intention of a
few of these webpages in Dlarge might not be phishing to collect
information but rather have popup screens and inappropriate adult
content. We believe that some of these webpages contributed to
a slightly lower accuracy compared to the Dtarget dataset. Also,
the comparable obtained results from our both datasets Dsmall of
10K samples and accuracy at 96.8% vs Dlarge of 100K samples and
accuracy at 96.1% indicates that our model could produce stable
accuracy.

Limitations and Challenges: Our Anti-SubtlePhish has the fol-
lowing limitations and challenges. (1) while WHOIS information
is available for most of the top-level domains (TLD) such as (com,
org, net, biz, info, pl, jp, uk, nz, . . . ), but nevertheless some of the

01234567891234567891011Detection NumberDaysZero-day ExperimentsAnti-SubtlePhishCyberCrimeForcepoinKasperskyNetcraftMicrosoft SmartScreenVirusTotal other vendorsTowards Web Phishing Detection Limitations and Mitigation

with Gestalt theory to decide the visual similarity. Recently, Sahar
et al. [43] introduced a triplet convolutional neural network to learn
the similarity not only from the login page but rather the entire web-
site. This stream usually suffers from protecting only limited top
famous brands in the magnitude of 100 to 150 pages. In (2) Source-
based, the similarity between phishing page to trusted page can be
inferred by comparing the HTML content. Huang et al. [44] extract
the source content representation to compare against trusted identi-
ties. Liu et al. [45] segmented a webpage source content into blocks
based on HTML visual cues to learn the page similarity to trusted
pages. This stream is vulnerable to code obfuscation as illustrated
in [46]. In (3) Address-based, the similarity can be inferred by con-
verting the page URL address to an image and training the model
to later detect high similar suspicious URLs as demonstrated by
Woodbridge et al. [47]. This stream can be easily tricked by using
random URL addresses.

URLs: models under this category tries to learn the representation
of the legitimate URLs vs the phishing URLS to detect phishing [23,
48]. Justin et al. [21] used statistical methods to discover the tell-
tale lexical and host-based properties of malicious website URLs to
predict the phishing sites. Rakesh et al. [22] employed a two-sample
Kolmogorov-Smirnov test along with other features extracted from
the URLs and trained ML model to predict benign vs phishing URLs.
Hung et al. [24] proposed a neural network model that learns from
the character-level and word-level of the URLs built as a dictionary.
Eint et al. [25] proposed a URL-based phishing detection model that
learns from the entropy of non-alphanumeric characters, which
relies on the hypothesis that phishing URLs differs from legitimate
ones in their disorder structures. Despite the efficacy of models
under this category, but they are prone to the adaptive attack of
deploying the phishing webpage under benign service URLs, as
shown in Section 3.1.

DOM structure: models under this category try to go beyond the
webpage URL and visual appearance into its HTML DOM structure
to extract features that may help to distinguish phishing webpages
from benign ones. Basnet et al. [49] detect by extracting features
about alarm windows, hidden and restricted information within the
DOM structure as well as redirection patterns. Chidimma et al. [18]
proposed a recurrent neural network-based model that learns from
the entire DOM representation of benign and phishing webpages to
be able to detect new unseen phishing webpages. Yukun et al. [26]
introduced a stacking model that combines 20 features from both
the DOM structure along with the URL to build an ensemble model
for better detection. Rami et al. [37] learned from the number of
internal and external links within the DOM structure. Xiang et
al. [14] detect only based on the information collection and forms.
Alkhozae et al. [50] rely on the hypothesis that the phishing web-
pages are relatively shorter than the legitimate pages and used that
for detection. Despite the efficacy of some of these techniques, var-
ious evasion mechanisms we presented in Section 3 that highlights
the limitations of relying only on DOM structure to detect phishing.
To name a few: (a) deploying phishing pages on benign services
without changing the default DOM structure and (b) hiding the
ultimate HTML DOM structure behind Javascript which is a widely
adopted trend among benign websites as well.

Third-party Service: models under this category relies only on
third-party services such as search engine ranking and Whois for
detection. Mohammad et al. [51], Rao et al. [52] used age of the
domain, registration date or expiry date to classify. Relying on
these features only could be bypassed by deploying phishing web-
pages on benign services as we demonstrated in Figure 8b. Chiew
et al. [53, 54] used logo as a query to Google image search to deter-
mine phishing and then improved that by using favicon in search
engine for detection. Varshney et al. [55] extracted titles along with
domains and query search engines to classify. Tan et al. [56] used
URL tokens and 𝑁 gram model for extracting the webpage identity
keywords which are fed to search engines for detection. Jain et
al. [8] introduced a two-level search engine based technique that
relies on one domain/title as well as hyperlink based features for
classification. Relying only on search engines may have a few limi-
tations as identified recently by Rao and Pais [11] such as high false
negative in the case of deploying the phishing page into benign
sites. On the other hand, they may lead to high false-positive in the
case of newly registered legitimate sites.

To overcome the highlighted shortcoming of the above cate-
gories, there is a compelled need to develop models that do not rely
on a single source of information but rather correlating collected
information from multiple trusted parties along with the rendering
the webpage itself for better detection.

8 CONCLUSION
In this paper, we explored the potential reasons why many recent
phishing sites bypass ML-based detection. This is achieved by con-
ducting a deep-dive case study across 13,000 phishing pages that
target 6 top victim brands. We identified five potential reasons for
the successful evasion. We derive the root cause as the dependency,
to some extent, on vertical feature-space which limits the efficacy of
the classifiers to be resilience against those types of evasion attacks.
To alleviate these attacks, we develop a logistic regression-based
model that relies not only on a single source of information comes
from what phishers present in the webpage but also horizontal
feature-space by a correlation with third-party trusted entities such
as WHOIS, Google Index Ranking, Selenium rendering and page
analytic. We devise a framework of four elements for these features
to evaluate page reputation, goal, consistency, and analytic. Our
obtained results from 100,000 phishing/benign websites demon-
strate promising accuracy of between 96.1% and 98.8%. as well as
an accuracy of 100% over 0-day attack crafted dataset.

REFERENCES
[1] Peng Peng, Chao Xu, Luke Quinn, Hang Hu, Bimal Viswanath, and Gang Wang.
What happens after you leak your password: Understanding credential sharing
on phishing sites. In Proceedings of the 2019 ACM AsiaCCS, pages 181–192, 2019.
[2] Amir Kashapov, Tingmin Wu, Alsharif Abuadbba, and Carsten Rudolph. Email
summarization to assist users in phishing identification. AsiaCCS 2022, 2022.
[3] L Mathews. Phishing scams cost american businesses half a billion dollars a year,

2017.

[4] Adam Oest, Yeganeh Safaei, Adam Doupé, Gail-Joon Ahn, Brad Wardman, and
Kevin Tyers. Phishfarm: A scalable framework for measuring the effectiveness of
evasion techniques against browser phishing blacklists. In 2019 IEEE Symposium
on Security and Privacy (SP), pages 1344–1361. IEEE, 2019.

[5] Neil Chou Robert Ledesma Yuka Teraguchi and John C Mitchell. Client-side
defense against web-based identity theft. Computer Science Department, Stanford
University. Available: http://crypto.stanford. edu/SpoofGuard/webspoof.pdf, 2004.
[6] Anthony Y Fu, Liu Wenyin, and Xiaotie Deng. Detecting phishing web pages
with visual similarity assessment based on earth mover’s distance (emd). IEEE

transactions on dependable and secure computing, 3(4):301–311, 2006.

Security Symposium (USENIX Security 21), pages 3721–3738, 2021.

Abuadbba, et al.

[7] Angelo PE Rosiello, Engin Kirda, Fabrizio Ferrandi, et al. A layout-similarity-
based approach for detecting phishing pages. In 2007 Third International Con-
ference on Security and Privacy in Communications Networks and the Workshops-
SecureComm 2007, pages 454–463. IEEE, 2007.

[8] Ankit Kumar Jain and Brij B Gupta. Two-level authentication approach to protect
from phishing attacks in real time. Journal of Ambient Intelligence and Humanized
Computing, 9(6):1783–1796, 2018.

[9] Ying Pan and Xuhua Ding. Anomaly based web phishing page detection. In
2006 22nd Annual Computer Security Applications Conference (ACSAC’06), pages
381–392. IEEE, 2006.

[10] Yue Zhang, Jason I Hong, and Lorrie F Cranor. Cantina: a content-based approach
to detecting phishing web sites. In Proceedings of the 16th international conference
on WWW, pages 639–648, 2007.

[11] Routhu Srinivasa Rao and Alwyn Roshan Pais. Jail-phish: An improved search
engine based phishing detection system. Computers and Security, 83:246–267,
2019.

[12] Colin Whittaker, Brian Ryner, and Marria Nazif. Large-scale automatic classifica-

tion of phishing pages. 2010.

[13] Kurt Thomas, Chris Grier, Justin Ma, Vern Paxson, and Dawn Song. Design
and evaluation of a real-time url spam filtering service. In 2011 IEEE S&P, pages
447–462. IEEE, 2011.

[14] Guang Xiang, Jason Hong, Carolyn P Rose, and Lorrie Cranor. Cantina+ a feature-
rich machine learning framework for detecting phishing web sites. ACM Trans
on Info and System Security, 14(2):1–28, 2011.

[15] Igino Corona, Battista Biggio, Matteo Contini, Luca Piras, Roberto Corda, Mauro
Mereu, Guido Mureddu, Davide Ariu, and Fabio Roli. Deltaphish: Detecting
phishing webpages in compromised websites. In in ESORICS, pages 370–388.
Springer, 2017.

[16] Samuel Marchal, Giovanni Armano, Tommi Gröndahl, Kalle Saari, Nidhi Singh,
and N Asokan. Off-the-hook: An efficient and usable client-side phishing pre-
vention application. IEEE Transactions on Computers, 66(10):1717–1733, 2017.

[17] Microsoft. Microsoft defender smartscreen, 2020.
[18] Chidimma Opara, Bo Wei, and Yingke Chen. Htmlphish: Enabling accurate
phishing web page detection by applying deep learning techniques on html
analysis. arXiv preprint arXiv:1909.01135, 2019.

[19] PhishTank. Phishtank: an open source anti-phishing site, 2021.
[20] VirusTotal. Analyze suspicious files, domains, ips and urls to detect malware and

other breaches, 2021.

[21] Justin Ma, Lawrence K Saul, Stefan Savage, and Geoffrey M Voelker. Beyond
blacklists: learning to detect malicious web sites from suspicious urls. In Proceed-
ings of the 15th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 1245–1254, 2009.

[22] Rakesh Verma and Keith Dyer. On the character of phishing urls: Accurate and
robust statistical learning classifiers. In Proceedings of the 5th ACM Conference on
Data and Application Security and Privacy, pages 111–122, 2015.

[23] Doyen Sahoo, Chenghao Liu, and Steven CH Hoi. Malicious url detection using

machine learning: A survey. arXiv preprint arXiv:1701.07179, 2017.

[24] Hung Le, Quang Pham, Doyen Sahoo, and Steven CH Hoi. Urlnet: Learning a
url representation with deep learning for malicious url detection. arXiv preprint
arXiv:1802.03162, 2018.

[25] Eint Sandi Aung and Hayato Yamana. Url-based phishing detection using the
entropy of non-alphanumeric characters. In Proceedings of the 21st International
Conference on Information Integration and Web-based Applications and Services,
pages 385–392, 2019.

[26] Yukun Li, Zhenguo Yang, Xu Chen, Huaping Yuan, and Wenyin Liu. A stacking
model using url and html features for phishing webpage detection. Future
Generation Computer Systems, 94:27–39, 2019.

[27] Yusi Lei, Sen Chen, Lingling Fan, Fu Song, and Yang Liu. Advanced evasion
attacks and mitigations on practical ml-based phishing website classifiers. arXiv
preprint arXiv:2004.06954, 2020.

[28] Ke Tian, Steve TK Jan, Hang Hu, Danfeng Yao, and Gang Wang. Needle in a
haystack: Tracking down elite phishing domains in the wild. In Proceedings of
the IMC 2018, pages 429–442, 2018.

[29] OpenPhish. Openphish: free service providing feed of global phishing urls that

were detected by fraudsense’s phishing detection technology, 2021.

[30] Alexa. Alexa top sites: providing access to lists of websites ordered by amazon

alexa traffic rank, 2021.

[31] Sen Chen, Lingling Fan, Chunyang Chen, Minhui Xue, Yang Liu, and Lihua Xu.
Gui-squatting attack: Automated generation of android phishing apps.
IEEE
Transactions on Dependable and Secure Computing, 2019.

[32] Adam Oest, Penghui Zhang, Brad Wardman, Eric Nunes, Jakub Burgis, Ali Zand,
Kurt Thomas, Adam Doupé, and Gail-Joon Ahn. Sunrise to sunset: Analyzing
the end-to-end life cycle and effectiveness of phishing attacks at scale. In 29th
{USENIX} Security Symposium ({USENIX} Security 20), 2020.

[33] Ravindu De Silva, Mohamed Nabeel, Charith Elvitigala, Issa Khalil, Ting Yu, and
Chamath Keppitiyagama. Compromised or {Attacker-Owned}: A large scale
classification and study of hosting domains of malicious {URLs}. In 30th USENIX

[34] Routhu Srinivasa Rao, Tatti Vaishnavi, and Alwyn Roshan Pais. Phishdump:
A multi-model ensemble based technique for the detection of phishing sites in
mobile devices. Pervasive and Mobile Computing, 60:101084, 2019.

[35] Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson corre-
lation coefficient. In Noise reduction in speech processing, pages 1–4. Springer,
2009.

[36] Xiaoqing Gu, Hongyuan Wang, and Tongguang Ni. An efficient approach to
detecting phishing web. Journal of Computational Information Systems, 9(14):5553–
5560, 2013.

[37] Rami M Mohammad, Fadi Thabtah, and Lee McCluskey. Intelligent rule-based

phishing websites classification. IET Information Security, 8(3):153–160, 2014.

[38] D Kevin McGrath and Minaxi Gupta. Behind phishing: An examination of phisher

modi operandi. Usenix, 8:4, 2008.

[39] Haijun Zhang, Gang Liu, Tommy WS Chow, and Wenyin Liu. Textual and visual
content-based anti-phishing: a bayesian approach. IEEE transactions on neural
networks, 22(10):1532–1546, 2011.

[40] Ee Hung Chang, Kang Leng Chiew, Wei King Tiong, et al. Phishing detection
In 2013 international conference on IT

via identification of website identity.
convergence and security (ICITCS), pages 1–4. IEEE, 2013.

[41] Matthew Dunlop, Stephen Groat, and David Shelly. Goldphish: Using images for
content-based phishing analysis. In 2010 Fifth international conference on internet
monitoring and protection, pages 123–128. IEEE, 2010.

[42] Teh-Chung Chen, Scott Dick, and James Miller. Detecting visually similar web
pages: Application to phishing detection. ACM Transactions on Internet Technology
(TOIT), 10(2):1–38, 2010.

[43] Sahar Abdelnabi, Katharina Krombholz, and Mario Fritz. Visualphishnet: Zero-
day phishing website detection by visual similarity. In Proceedings of the 2020 ACM
SIGSAC Conference on Computer and Communications Security, pages 1681–1698,
2020.

[44] Chun-Ying Huang, Shang-Pin Ma, Wei-Lin Yeh, Chia-Yi Lin, and Chien-Tsung
Liu. Mitigate web phishing using site signatures. In TENCON 2010-2010 IEEE
Region 10 Conference, pages 803–808. IEEE, 2010.

[45] Wenyin Liu, Xiaotie Deng, Guanglin Huang, and Anthony Y Fu. An antiphishing
strategy based on visual similarity assessment. IEEE Internet Computing, 10(2):58–
65, 2006.

[46] Ieng-Fat Lam, Wei-Cheng Xiao, Szu-Chi Wang, and Kuan-Ta Chen. Counteracting
phishing page polymorphism: An image layout analysis approach. In International
Conference on Information Security and Assurance, pages 270–279. Springer, 2009.
[47] Jonathan Woodbridge, Hyrum S Anderson, Anjum Ahuja, and Daniel Grant.
In 2018 IEEE

Detecting homoglyph attacks with a siamese neural network.
Security and Privacy Workshops (SPW), pages 22–28. IEEE, 2018.

[48] Mahathir Almashor, Ejaz Ahmed, Benjamin Pick, Sharif Abuadbba, Raj Gaire,
Seyit Camtepe, and Surya Nepal. Characterizing malicious url campaigns. arXiv
preprint arXiv:2108.12726, 2021.

[49] Ram B Basnet, Andrew H Sung, and Quingzhong Liu. Rule-based phishing
attack detection. In Proceedings of the International Conference on Security and
Management (SAM), page 1. Citeseer, 2011.

[50] Mona Ghotaish Alkhozae and Omar Abdullah Batarfi. Phishing websites detection
International

based on phishing characteristics in the webpage source code.
Journal of Information and Communication Technology Research, 1(6), 2011.
[51] Rami M Mohammad, Fadi Thabtah, and Lee McCluskey. An assessment of
features related to phishing websites using an automated technique. In 2012
International Conference for Internet Technology and Secured Transactions, pages
492–497. IEEE, 2012.

[52] Routhu Srinivasa Rao and Alwyn Roshan Pais. An enhanced blacklist method
to detect phishing websites. In International Conference on Information Systems
Security, pages 323–333. Springer, 2017.

[53] Kang Leng Chiew, Ee Hung Chang, Wei King Tiong, et al. Utilisation of website

logo for phishing detection. Computers and Security, 54:16–26, 2015.

[54] Kang Leng Chiew, Jeffrey Soon-Fatt Choo, San Nah Sze, and Kelvin SC Yong.
Leverage website favicon to detect phishing websites. Security and Communica-
tion Networks, 2018, 2018.

[55] Gaurav Varshney, Manoj Misra, and Pradeep K Atrey. Improving the accuracy of
search engine based anti-phishing solutions using lightweight features. In 2016
11th International Conference for Internet Technology and Secured Transactions
(ICITST), pages 365–370. IEEE, 2016.

[56] Choon Lin Tan, Kang Leng Chiew, KokSheik Wong, et al. Phishwho: Phishing
webpage detection via identity keywords extraction and target domain name
finder. Decision Support Systems, 88:18–27, 2016.

