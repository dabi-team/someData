Event-Triggered State Observers for

Sparse Sensor Noise/Attacks

Yasser Shoukry and Paulo Tabuada

1

4
1
0
2

p
e
S
9
1

]

C
O
.
h
t
a
m

[

3
v
1
1
5
3
.
9
0
3
1
:
v
i
X
r
a

Abstract

This paper describes two algorithms for state reconstruction from sensor measurements that are

corrupted with sparse, but otherwise arbitrary, “noise”. These results are motivated by the need to secure

cyber-physical systems against a malicious adversary that can arbitrarily corrupt sensor measurements.

The ﬁrst algorithm reconstructs the state from a batch of sensor measurements while the second algorithm

is able to incorporate new measurements as they become available, in the spirit of a Luenberger observer.

A distinguishing point of these algorithms is the use of event-triggered techniques to improve the

computational performance of the proposed algorithms.

I. INTRODUCTION

The security of Cyber-Physical Systems (CPSs) has recently become a topic of scientiﬁc

inquiry in no small part due to the discovery of the Stuxnet malware, the most famous example

of an attack on process control systems [1]. Although one might be tempted to associate CPS

security with large-scale and critical infrastructure, such as the power-grid and water distribution

systems, previous work by the authors and co-workers has shown that even smaller systems,

such as cars, can be attacked. It was shown in [2] how to attack the velocity measurements of

anti-lock braking systems so as to force drivers to loose control of their vehicles.

In this paper we propose two state observers for discrete-time linear systems in the presence

of sparsely corrupted measurements. Sparse “noise” is a natural model to describe the effect

Y. Shoukry and P. Tabuada are with the UCLA Electrical Engineering Department, yshoukry@ucla.edu,

tabuada@ee.ucla.edu

This work was partially sponsored by the NSF award 1136174 and by DARPA under agreement number FA8750-12-2-0247.

The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright

notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily

representing the ofﬁcial policies or endorsements, either expressed or implied, of NSF, DARPA or the U.S. Government.

September 22, 2014

DRAFT

 
 
 
 
 
 
2

of a malicious attacker that has the ability to alter the measurements of a subset of sensors in

a feedback control loop. While measurements originating from un-attacked sensors are “noise”

free, measurements from attacked sensors can be arbitrary: we make no assumption regarding

its magnitude, statistical description, or temporal evolution. Hence, the noise vector is sparse;

its elements are either zero or arbitrary real numbers.

Several results on state reconstruction under sensor attacks have recently appeared in the

literature. We classify the existing work in two classes based on how the physical plant is

modeled: 1) steady-state operation (no dynamics) and 2) linear time-invariant dynamics. In both

classes the attacker is assumed to corrupt a few sensor measurements and thus its actions are

adequately modeled as sparse noise.

The results reported in [3], [4], [5], [6], [7] fall in the ﬁrst class – steady state operation –

and address security problems in the context of smart power grid systems. Due to the steady

state assumption, all of these results fail to exploit the constraints imposed by the continuous

dynamics as done in this paper.

Representative work in the second class – linear time-invariant dynamics – includes [8],

[9], [10], [11]. The work reported in [8] addresses the detection of attacks through monitors

inspired by the fault detection and isolation literature. Such methods are better suited for small

systems since the number of monitors grows combinatorially with the number of sensors. The

work reported in [9], [10], [11] draws inspiration from error correction over the reals [12]

and compressive sensing [13] and formulates the secure state reconstruction problem as a

Lr\L1, r > 1 optimization problem.

The problem of reconstructing the state under sensor attacks is closely related to fault-tolerant

state reconstruction. The robust Kalman ﬁlter, described in [14], is the approach to fault-tolerant

state reconstruction closer to the results in this paper, at the technical level. In robust Kalman

ﬁltering the state estimate updates are obtained by solving a convex L1 optimization problem

that is robust to outliers. With the advances in the computational power of current processors, the

robust Kalman ﬁlter can be efﬁciently computed in real-time. However, no theoretical guarantees

are known regarding the performance of this ﬁlter in the presence of malicious attacks.

In this paper, we extend the work in [9], [10], [11] by focusing on efﬁcient algorithms in

the sense of being implementable on computationally limited platforms. Rather than relying on

classical algorithms for Lr\L1, r > 1 optimization, as was done in [9], [10], [11], we develop

September 22, 2014

DRAFT

3

in this paper customized gradient-descent algorithms which have lightweight implementations.

The computational efﬁciency claims are supported by numerical simulations showing an order

of magnitude decrease in the computation time.

The proposed algorithms reconstruct both the state as well as the sparse noise/attack signal.

Hence, they can be seen as an extension of compressive sensing techniques to the case where

part of the signal to be reconstructed is sparse and the other part is governed by linear dynamics.

A similar problem is studied in [15] where the recovery of a sparse streaming signal with linear

dynamics is discussed. Although the work in [15] also exploits the linear dynamics, it is not

applicable to the state reconstruction problem where the sparse signal models an attack for which

no dynamics is known.

Technically, we make the following contributions:

• The reconstruction or decoding of compressively sensed signals is characterized by prop-

erties such as the restricted isometry or the restricted eigenvalues [13]. We show that the

relevant notion in our case is a strong notion of observability.

• We extend one of the algorithms previously proposed for the reconstruction or decoding of

compressively sensed signals [16] to the case where part of the signal is sparse while the

rest is governed by linear dynamics.

• We propose a recursive implementation of the method discussed in the previous bullet so

that new measurement information can be used as it becomes available, in the spirit of a

Luenberger observer.

The rest of this paper is organized as follows. Section II formally introduces the problem

under consideration. The notions of s-observability and s-restricted eigenvalues are introduced

in Section III. The main results of this paper which are the Event-Triggered Projected Gradient-

Descent algorithm and the Event-Triggered Projected Luenberger Observer, along with their

convergence properties, are presented in Sections IV and V, respectively. Simulation results for

the proposed algorithms are shown in Section VI. Finally, Section VII concludes this paper.

II. THE SECURE STATE RECONSTRUCTION PROBLEM

A. Notation

The symbols N0, R, and R+ denote the set of natural, real, and positive real numbers, re-
spectively. Given two vectors x ∈ Rn1 and y ∈ Rn2, we denote by (x, y) ∈ Rn1+n2 the vector

September 22, 2014

DRAFT

4

(cid:104)

xT

(cid:105)T

yT

. We also use the notation zx and zy to denote the natural projection of the vector

z = (x, y) on its ﬁrst and second component, respectively.

If S is a set, we denote by |S| the cardinality of S. The support of a vector x ∈ Rn, denoted
by supp(x), is the set of indices of the non-zero elements of x. We call a vector x ∈ Rn s-sparse,
if x has at most s nonzero elements, i.e., if |supp(x)| ≤ s. A vector z = (x1, x2, . . . , xp) ∈ Rnp
is called cyclic s-sparse, if each xi ∈ Rn is s-sparse for all i ∈ {1, 2, . . . , p} and supp(x1) =

supp(x2) = . . . = supp(xp). With some abuse of notation, we use s-sparse to denote cyclic

s-sparse.

For a vector x ∈ Rn, we denote by (cid:107)x(cid:107)2 the 2-norm of x and by (cid:107)M (cid:107)2 the induced 2-norm of
a matrix M ∈ Rm×n. We also denote by Mi ∈ R1×n the ith row of M . For a set Γ ⊆ {1, . . . , m},
we denote by MΓ ∈ R|Γ|×n the matrix obtained from M by removing all the rows except those
indexed by Γ. We also denote by MΓ ∈ R(m−|Γ|)×n the matrix obtained from M by removing
the rows indexed by the set Γ, for example, if m = 4, and Γ = {1, 2}, then:








MΓ =



 and MΓ =



 .

M1

M2

M3

M4

For any ﬁnite set S = {s1, s2, . . . , sk} ⊂ N we denote by rS and r + S, r ∈ N, the sets

rS = {rs1, rs2, . . . , rsk} and r + S = {r + s1, r + s2, . . . r + sk}, respectively. Finally we denote

the set of eigenvalues, the minimum eigenvalue, and the maximum eigenvalue of a symmetric

matrix M by λ{M }, λmin{M }, and λmax{M } respectively.

B. Dynamics and Attack Model

Consider the following linear discrete-time control system where x(t) ∈ Rn is the system state

at time t ∈ N0, u(t) ∈ Rm is the system input, and y(t) ∈ Rp is the observed measurement:

x(t + 1) = Ax(t) + Bu(t),

y(t) = Cx(t) + a(t).

(II.1)

(II.2)

The matrices A, B, and C have appropriate dimensions and a(t) ∈ Rp is a s-sparse vector

modeling how an attacker changed the sensor measurements at time t. If sensor i ∈ {1, . . . , p}

is attacked then the ith element in the vector a(t) is non-zero otherwise the ith sensor is not

attacked. Hence, s describes the number of attacked sensors. Note that we make no assumptions

September 22, 2014

DRAFT

5

on the vector a(t) other than being s-sparse. In particular, we do not assume bounds, statistical

properties, nor restrictions on the time evolution of the elements in a(t). The value of s is also

not assumed to be known although we assume the knowledge of an upper bound. The set of

sensors the attacker has access to is assumed to remain constant over time (and has cardinality

at most s). However, the attacker has complete freedom in deciding which sensor or sensors in

this set are attacked and when, including the possibility of attacking all of them at all times.

Our objective is to simultaneously construct a delayed version of the state, x(t−τ +1), and the

cyclic s-sparse attack vector E(t) = (a(t − τ + 1), a(t − τ + 2), . . . , a(t)) from the measurements

y(t − τ + 1), y(t − τ + 2), . . . , y(t).

It is worth explaining the cyclic sparse nature of E(t) in more detail. Consider the attack vector
E(t) = (a(t − τ + 1), a(t − τ + 2), . . . , a(t)) and reformat this data as the matrix (cid:101)E(t) ∈ Rp×τ
where column i is given by a(t − i + 1). Since we assume that the set of sensors under attack

does not change over time, the sparsity pattern appears in (cid:101)E(t). The rows corresponding to the

un-attacked sensors will only have zeros, while the rows corresponding to the attacked sensors

will have arbitrary (zero or non-zero) elements.

Example II.1. Consider a system with 4 sensors, τ = 4, and an attack on the second and third

sensors. The attack matrix (cid:101)E(t) will be of the form:



0 0 0



0

(cid:101)E(t) =








2 5 6 10

4 8 0 12

.








0 0 0

0

The cyclic sparsity structure appears once the attack matrix (cid:101)E(t) is reshaped as the vector E(t):

E(t) =

(cid:104)
0 2 4 0 0 5 8 0 . . .

(cid:105)T

.

Since cyclic s-sparse vectors pervade this paper, it is convenient to denote them by a special

symbol.

Deﬁnition II.2 (Cyclic s-sparse set Ss). The subset of Rpτ consisting of the vectors that are
cyclic r-sparse for all r ∈ {0, 1, . . . , s}, s ≤ p, is denoted by Ss.

September 22, 2014

DRAFT

Using this cyclic sparsity notion, we pose two problems which will lead to the two proposed

6

algorithms.

C. Static Batch Optimization

By collecting τ ∈ N observations with τ ≤ n, we can write the output equation as:

(cid:101)Y (t) = Ox(t − τ + 1) + E(t) + F U (t)

(cid:104)

=

O I

(cid:105)





x(t − τ + 1)

E(t)



 + F U (t)

= Qz(t) + F U (t),













=





x(t − τ + 1)

E(t)



 , E(t) =



a(t − τ + 1)








a(t − τ + 2)
...
a(t)










,

where:

z(t) =



x(t − τ + 1)

a(t − τ + 1)

a(t − τ + 2)
...
a(t)













C

O =








CA
...
CAτ −1










, Q =

(cid:104)
O I

(cid:105)

,

F =










0

0

0

. . .


0

0


...



CAτ −2B CAτ −3B . . . CB 0

CB
...

. . .
. . .

0

0

(II.3)

, (cid:101)Y (t) =



y(t − τ + 1)








y(t − τ + 2)
...
y(t)










, U (t) =










u(t − τ + 1)

u(t − τ + 2)
...
u(t)










.

Since all the inputs in the vector U (t) are known, we can further simplify the output equation

to:

where Y (t) = (cid:101)Y (t) − F U (t).

Y (t) = Qz(t),

September 22, 2014

DRAFT

Problem II.3. (Static Batch Optimization) For the linear control system deﬁned by (II.1) and
(II.2) construct the estimate ˆz = (ˆx, ˆE), where ˆx ∈ Rn is the state estimate and ˆE ∈ Ss is the
attack vector estimate, obtained as the solution of the following optimization problem:

arg min

ˆz∈Rn×Ss

1
2

(cid:107)Y − Qˆz(cid:107)2
2 .

7

We dropped the time t argument since this optimization problem is to be solved at every
time instance. We note that we seek a solution in the non-convex set Rn × Ss and no closed-

form solution is known for this problem. Note also that this optimization problem asks for the

reconstruction of a delayed version of the state x(t − τ + 1). However, we can always reconstruct

of the current state x(t) from x(t − τ + 1) by recursively rolling the dynamics forward in time.

Alternatively, when A is invertible, we can directly recover x(t) by re-writing the measurement

equation as a function of x(t).

As a ﬁnal remark we note that it follows from the Cayley-Hamilton theorem that there is no

loss of generality in taking the number of collected measurements τ to be no greater than the

number of states n.

D. Luenberger-like Observer

Problem II.3 asks for the reconstruction of z(t) using a batch approach based on the measured

data in the vector Y . On computationally restricted platforms we may be faced with the difﬁculty

of having to process new measurements before being able to compute a solution to Problem

II.3. It would then be preferable to reconstruct z(t) using an algorithm that can incorporate new

measurements as they become available. This motivates the following problem.

Problem II.4. (Luenberger-like Observer) For the linear control system deﬁned by (II.1) and

(II.2) construct a dynamical system:

such that:

ˆz(t + 1) = f (ˆz(t), U (t), Y (t)),

(z∗(t) − ˆz(t)) = 0,

lim
t→∞

where z∗(t) = (x∗(t − τ + 1), E∗(t)) ∈ Rn × Ss, x∗(t) is the solution of (II.1) under the inputs
U (t), and Y (t) is the sequence of the last τ observed outputs corrupted by E∗(t).

September 22, 2014

DRAFT

III. s-SPARSE OBSERVABILITY AND THE RESTRICTED EIGENVALUE

8

Recall that Problem II.3 asks for the minimizer of 1

2. Since the matrix Q has a
non trivial kernel, there exist many pairs ˆz = (ˆx, ˆE) which solve this problem. In this section

2(cid:107)Y − Qˆz(cid:107)2

we look closely at the fundamental question of uniqueness of solutions.

A. s-Sparse Observability

We start by introducing the following notion of observability.

Deﬁnition III.1. (s-Sparse Observable System) The linear control system deﬁned by (II.1) and

(II.2) is said to be s-sparse observable if for every set Γ ⊆ {1, . . . , p} with |Γ| = s, the pair

(A, CΓ) is observable.

In other words, a system is s-sparse observable if it remains observable after eliminating any

choice of s sensors. This strong notion of observability underlies most of the results in this paper

and can also be expressed using the notion of strong observability for linear systems described

in [17], [18]. To make this connection explicit, consider a linear system (A; B = 0; C; D = Ia)

where the attack signal a(t) is regarded as an input and Ia is the diagonal matrix deﬁned so

that the ith element on the diagonal is zero whenever ai(t) is zero and is one otherwise. Since

in our formulation we assume no knowledge of the support of the attack signal a(t), we need

to consider all the different supports for the attack vector. It is then not difﬁcult to see that a

linear system is s-sparse observable if and only if the systems (A; B = 0; C; D = Ia) are strong

observable for all the matrices Ia obtained by considering attack vectors with all the possible

supports.

We use the notion of s-sparse observability to characterize the uniqueness of solutions to

Problem II.3.

Theorem III.2. (Existence and uniqueness of solutions to Problem II.3) Problem II.3 has a
2 has a unique minimum on the set Rn × Ss, if and

unique solution, i.e., the function 1

2(cid:107)Y − Qˆz(cid:107)2

only if the linear dynamical system deﬁned by (II.1) and (II.2) is 2s-sparse observable.

Proof: We ﬁrst note that (cid:107)Y − Qz(cid:107)2

2 is always non-negative and it becomes zero whenever

x is the true state and E is the true attack vector. Hence, (cid:107)Y − Ox − E(cid:107)2

2 has a unique minimum

if and only if the equality Y = Qz = Ox + E only holds for the true state and the true attack

September 22, 2014

DRAFT

9

vector. However, this is equivalent to injectivity of the map f : Rn × Ss → Rpτ deﬁned by

f (x, E) = Ox + E.

To prove the stated result we assume, for the sake of contradiction, that f is not injective and
(A, C) is 2s-sparse observable. Since f is not injective there exist (x, E), (x(cid:48), E(cid:48)) ∈ Rn × Ss
such that f (x, E) = f (x(cid:48), E(cid:48)). Moreover, it follows from the deﬁnition of f that x (cid:54)= x(cid:48). We

now note that:

f (x, E) = f (x(cid:48), E(cid:48)) ⇔ Ox + E = Ox(cid:48) + E(cid:48) ⇔ O(x − x(cid:48)) = E(cid:48) − E.

Since the support of a and a(cid:48) is at most s, the support of E(cid:48)−E is at most 2s. Let Γ = supp(E(cid:48) − E)
be the set consisting of the indices where E(cid:48) −E is supported. Then, O(x−x(cid:48)) = E(cid:48) −E implies
OΓ(x − x(cid:48)) = 0 which in turn implies that the pair (A, CΓ) is not observable since x (cid:54)= x(cid:48), a
contradiction.

Conversely, if the pair (A, CΓ) is not observable there is a non-zero vector v ∈ ker OΓ. Fix
x ∈ Rn and let x(cid:48) = x + v. Since OΓ(x − x(cid:48)) = 0, the support of O(x − x(cid:48)) is at most |Γ| = 2s.
Split Γ in two sets Γ1 and Γ2 so that Γ = Γ1 ∪ Γ2, |Γ1| ≤ s and |Γ2| ≤ s. We can now deﬁne E
by making its ith entry Ei to be equal to (O(x − x(cid:48)))i if i ∈ Γ1 and zero otherwise. Similarly,
i to be equal to (O(x − x(cid:48)))i if i ∈ Γ2 and zero otherwise.
we deﬁne E(cid:48) by making its ith entry E(cid:48)
As deﬁned, the support of E and E(cid:48) is at most s and the equality f (x, E) = f (x(cid:48), E(cid:48)) holds

thereby showing that f is not injective.

Although the notion of s-sparse observability is of combinatorial nature, since we have to check

observability of all possible pairs (A, CΓ), it does clearly illustrate a fundamental limitation: it is
impossible to correctly reconstruct the state whenever p/2 or more sensors are attacked. Indeed,

suppose that we have an even number of sensors p and s = p/2 sensors are attacked. Theorem

III.2 requires the system to still be observable after removing 2s = p rows from the matrix C

which leads to CΓ being the linear transformation mapping every state to zero. This fundamental

limitation is consistent with and had been previously reported in [9], [10], [18].

In [9], [10], the possibility of reconstructing the state under sensor attacks is characterized

by Proposition 2 and under certain assumptions on the A matrix by Proposition 4. The charac-

terization based on s-sparse observability complements the characterizations in [9], [10] in the
following sense. Proposition 2 in [9], [10] requires a test to be performed for every state x ∈ Rn

and does not lead to an effective algorithm. In contrast, s-sparse observability only requires a

September 22, 2014

DRAFT

ﬁnite, albeit large, number of computations. Proposition 4 in [9], [10] requires an even smaller

number of tests than s-sparse observability but only applies under additional assumptions on

the A matrix. Moreover, s-sparse observability connects the state reconstruction problem under

sensor attacks to the well known systems theoretic notion of observability.

10

B. s-Restricted Eigenvalue

While s-sparse observability provides a qualitative characterization of the existence and unique-

ness of solutions to Problem II.3, in this section we discuss a more quantitative version termed

the s-restricted eigenvalue property [19]. This property is directly related to the possibility of

solving Problem II.3 and Problem II.4 using gradient descent inspired methods.

Deﬁnition III.3. (s-Restricted Eigenvalue of a Linear Control System) For a given set (cid:101)Γs ⊆ {1, . . . , p}

with |(cid:101)Γs| = p − s, let Γs be deﬁned by:

Γs = (cid:101)Γs ∪ p(cid:101)Γs ∪ 2p(cid:101)Γs ∪ . . . ∪ (τ − 1)p(cid:101)Γs

(III.1)

and deﬁne (with some abuse of notation) the matrix QΓs ∈ Rpτ ×(n+sτ ) as the matrix obtained
from Q =

by removing from I the columns indexed by Γs, i.e.:

O I

(cid:104)

(cid:105)

QΓs =

(cid:104)
O (IΓs)T

(cid:105)

.

(III.2)

The s-restricted eigenvalue of the control system deﬁned by (II.1) and (II.2) is the the smallest

eigenvalue of all the matrices QT
Γs

QΓs obtained by considering all the different sets (cid:101)Γs.

The s-restricted eigenvalue of a control system can be related to the s-observability as follows.

Proposition III.4. (Non-zero Restricted Eigenvalue) Let the linear control system, deﬁned
by (II.1) and (II.2), be 2s-sparse observable. There exists a δ2s ∈ R+ such that for every
z = (x, E) ∈ Rn × S2s the 2s-restricted eigenvalue is no smaller than δ2s.

In other words, although Q has a non-trivial kernel, QT Q has a non-zero minimum eigenvalue

when operating on a vector z = (x, E) with cyclic 2s-sparse zE, i.e., the following inequality
holds if z = (x, E) ∈ Rn × S2s:

September 22, 2014

0 < δ2szT z ≤ zT QT Qz.

(III.3)

DRAFT

Proof: Inequality (III.3) follows directly from Theorem III.2 which states that Qz (cid:54)= 0
for any z = (x, E) (cid:54)= 0 with zE cyclic 2s-sparse. This is equivalent to zT QT Qz > 0. Deﬁne
Γ2s = supp(zE) and Γ = n + Γ2s. Now note that zT QT Qz = zT
Γ

zΓ and hence:

QΓ2s

QT

Γ2s

11

λmin

(cid:110)

QT

Γ2s

(cid:111)

QΓ2s

zT
Γ zΓ ≤ zT

Γ QT
Γ2s

QΓ2s

zΓ,

from which we conclude that inequality (III.3) holds with:

where Γ2s is deﬁned in (III.1).

δ2s = min
Γ2s

λmin

(cid:110)

QT

Γ2s

(cid:111)

,

QΓ2s

Similarly to 2s-sparse observability, the computation of δ2s is combinatorial in nature since

one needs to calculate the eigenvalues of QT

Γ2s

QΓ2s for all the different sets Γ2s.

IV. EVENT TRIGGERED PROJECTED GRADIENT DESCENT

The problem of reconstructing a sparse signal from measurements is well studied in the

compressive sensing literature [20], [21]. In this section we present an algorithm for state

reconstruction that can be seen as an extension of the iterative hard thresholding algorithm,

reported in [16], to the case where part of the signal to be reconstructed is sparse and part

is governed by linear dynamics. We also draw inspiration from [22] where it is shown how

to interpret optimization algorithms as dynamical systems with feedback. Once this link is

established, Lyapunov analysis techniques become available to design as well as to analyze

the performance of optimization algorithms.

A. The Algorithm

The Event Triggered Projected Gradient Descent (ETPG) algorithm updates the estimate
ˆz ∈ Rn × Rpτ of z ∈ Rn × Rpτ by following the gradient direction of the Lyapunov candidate
function V (ˆz) = 1

2 (cid:107)Y − Qˆz(cid:107)2

2, i.e.

ˆz+ := ˆz + ηQT (Y − Qˆz)

for some step size η. Since ˆz will not, in general, satisfy the desired sparsity constraints, gradient

steps are alternated with projection steps using the projection operator:

Π : Rn × Rpτ → Rn × Ss

September 22, 2014

DRAFT

12

that takes ˆz ∈ Rn × Rpτ to the closest point in Rn × Ss. In order to ensure that V decreases

along the execution of the algorithm, multiple gradient steps are performed for each projection

step. By monitoring the evolution of V , the algorithm can determine when the decrease in V

caused by the gradient steps compensates the increase caused by Π. This is akin to triggering

an event in event-triggered control [23].

Deﬁne the error vector as e = z∗ − ˆz (where z∗ denotes the solution of Problem II.3) and

note that eE = z∗
E − ˆzE is, at most, 2s-sparse whenever ˆzE is at most s-sparse. From Theorem
III.2 we know that the intersection of the cyclic 2s-sparse set S2s with the kernel of Q is only
one point, e = 0, which corresponds to ˆz = z∗. Hence, by driving V (ˆz) to zero while forcing
ˆzE to be cyclic s-sparse, ˆz is guaranteed to converge to z∗. Formalizing these ideas leads to

Algorithm 1.

Algorithm 1 Event-Triggered Projected Gradient Descent
1: Initialize k := 1, m := 0, ˆz(0)
ˆz(k−1)
2: while V
Π
ˆz(k,0) := ˆz(k−1)

Π := 0;

> 0 do

(cid:17)

(cid:16)

3:

;

Π

(cid:16)

(cid:17)

ˆz(k−1)
Π

;

(cid:16)

(cid:17)

while Vtemp ≥ V

reset m := 0, Vtemp := V
ˆz(k−1)
Π
ˆz(k,m+1) := ˆz(k,m) + ηQT (cid:0)Y − Qˆz(k,m)(cid:1);
Vtemp := V (cid:0)Π(ˆz(k,m+1))(cid:1);
m := m + 1;

do

4:

5:

6:

7:

8:

9:

10:

11:

end while
ˆz(k)
Π := Π(ˆz(k,m));
k := k + 1;

12: end while
13: return ˆz(k)
Π

A typical execution of the ETPG algorithm is illustrated in Figure 1 where the evolution of the

Lyapunov candidate function V is shown. We can see that at k = 0, 1 the ETPG algorithm applied

one gradient step before applying the new projection step, while at k = 2 and k = 8 two gradient

steps were executed to compensate for the increase in V caused by the projection step. This

September 22, 2014

DRAFT

13

30

20

10

n
o
i
t
c
n
u
f

v
o
n
u
p
a
y
L

0
(0,0) (1,0) (2,0)

(3,0) (4,0) (5,0) (6,0) (7,0) (8,0)

(9,0)

Loop index (k, m)
V (cid:0)ˆz(k)

V (cid:0)ˆz(k,m)(cid:1)

(cid:1)

Π

Fig. 1. An example of the evolution of the Lyapunov candidate function V while running the ETPG algorithm. The subsequence

highlighted in blue (dashed) is a decreasing subsequence.

adaptive nature of the ETPG algorithm is a consequence of the event-triggering mechanism. The

algorithm ensures that the subsequence depicted in blue in Figure 1 is a converging subsequence

and thus stability in the Lyapunov sense is attained.

The ETPG algorithm uses two counters, one for each loop. Accordingly, we will use the
notation ˆz(k,m) where k counts the number of iterations of the outer loop, while m counts the

number of internal gradient descent steps within each iteration of the inner loop.

B. The Projection Operator

Before discussing the convergence of the proposed algorithm, it is important to show how to

compute the projection operator Π.

Deﬁnition IV.1. Given a vector z = (x, E) ∈ Rn × Rpτ , we denote by Π(z) the element of
Rn × Ss closest to z in the 2-norm sense, i.e.,

(cid:107)Π(z) − z(cid:107)2 ≤ (cid:107)z(cid:48) − z(cid:107)2 ,

(IV.1)

for any z(cid:48) ∈ Rn × Ss.

We ﬁrst note that Π(z) = Π(x, E) = (x, Π(cid:48)(E)). To explain how Π(cid:48) is computed, recall from
Example II.1 the matrix (cid:101)E obtained by formatting E ∈ Rpτ so that the ith column of (cid:101)E is given
(cid:13)
(cid:13)E(cid:13)
2
by a(t − i + 1). Now, deﬁne E ∈ Rp by Ei =
(cid:13)
2 and that E is
(cid:13) (cid:101)Ei
(cid:13)
cyclic s-sparse if and only if E is s-sparse, it immediately follows that Π(cid:48)(z) can be computed

. By noting that (cid:107)E(cid:107)2

2 = (cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

by setting to zero the elements of E corresponding to the smallest p − s entries of E.

September 22, 2014

DRAFT

Example IV.2. Let us consider the following example with p = 3, τ = 2 and s = 1:

E =

(cid:104)
1 2 3 4 5 6 7 8 9

(cid:105)T

∈ Rpτ

14

Hence,








(cid:101)E =

which leads to:








1 4 7

2 5 8

3 6 9

∈ Rp×τ ,

E =








66

93

126










∈ Rp,

Π(cid:48)(E) =

∈ Rp,








0

0






126

Π(cid:48)(E) =

(cid:104)
0 0 3 0 0 6 0 0 9

(cid:105)T

.

C. Convergence of the ETPG Algorithm

In this subsection we discuss the convergence and performance of the ETPG algorithm. The

main result of this section is stated in the following theorem.

Theorem IV.3. (Convergence of the ETPG algorithm) Let the linear control system deﬁned

by (II.1) and (II.2) be 2s-sparse observable with 2s-restricted eigenvalue δ2s and let z∗ =
(x∗, E∗) ∈ Rn × Ss denote the solution of Problem II.3. The ETPG algorithm converges to z∗ if

the following holds:

1) 0 < η < 2λ−1

max{QT Q},

2) δ2s > 4

9λmax{QT Q}.

Remark IV.4. Theorem IV.3 shows that the ETPG algorithm correctly reconstructs the state

whenever the attacker has access to no more than s sensors and the system is 2s-sparse

observable. In practice, one does not exactly know the number s of attacked sensors. However, if

an upper bound s is known, the ETPG is guaranteed to work as long as the system is 2s-sparse

observable. For this reason the ETPG algorithm is run with the largest s for which 2s-spare

observability holds.

Remark IV.5. The ETPG algorithm can be seen as a generalization of the “Normalized Iterative

Hard Thresholding” (NIHT) algorithm presented in [24]. The ETPG algorithm uses multiple

gradient descent steps supervised by an event-triggering mechanism whereas the NIHT algorithm

only uses one gradient step. These two ingredients, multiple gradient steps and event-triggering,

September 22, 2014

DRAFT

15

result in weaker conditions for convergence: δ2s > 4
δ2s > 8

9λmax{QT Q} for the NIHT algorithm (Theorem 4 in [24]).

9λmax{QT Q} for the ETPG algorithm vs

In the remainder of this subsection, we focus on proving this theorem by resorting to the

Lyapunov candidate function:

W (ˆz) = (cid:107)z∗ − ˆz(cid:107)2 = (cid:107)e(cid:107)2 .

Note that, unlike V , the Lyapunov candidate function W has a unique minimum at ˆz = z∗.
However, one should note that evaluation of W requires the knowledge of z∗ which we do not

have a priori. Accordingly, W is only adequate to discuss stability but can not be used to design

the ETPG algorithm.

The proof is divided into small steps. First we discuss the effect of the two operations used

inside the ETPG algorithm: projection and gradient descent. This is done in Propositions IV.6,

IV.7, and IV.8. Next we provide sufﬁcient conditions under which the inner loop is guaranteed

to terminate. These are given in Proposition IV.10. Finally, we use the triggering condition to

establish that W is a Lyapunov function.

1) Effect of the Projection Operator:

Proposition IV.6. The following inequality holds for any z ∈ Rn × Rpτ :

W ◦ Π(z) ≤ 2W (z).

Proof: The result is proved by direct computation as follows:

W ◦ Π(z) = (cid:107)z∗ − Π(z)(cid:107)2 ≤ (cid:107)z∗ − z(cid:107)2 + (cid:107)Π(z) − z(cid:107)2 ≤ 2 (cid:107)z∗ − z(cid:107)2 = 2W (z),

where the ﬁrst inequality follows from the triangular inequality and the second inequality follows

from (IV.1).

2) Effect of Gradient Steps: In this subsection we study the effect of the gradient descent step

along with sufﬁcient conditions for termination of the inner loop of the ETPG algorithm. The

ﬁrst proposition is a technical result used in the proof of the second proposition that characterizes

the decease in W caused by gradient descent steps.

September 22, 2014

DRAFT

Proposition IV.7. Let the linear control system deﬁned by (II.1) and (II.2) be 2s-sparse observ-
able with 2s-restricted eigenvalue δ2s. The following holds for any z ∈ Rn × S2s:

16

(cid:13)(I − Q+Q)z(cid:13)
(cid:13)

(cid:13)2 ≤ (1 − δ2sλ−1

max{QT Q})

1

2 (cid:107)z(cid:107)2

where Q+ is the Moore-Penrose pseudo inverse of Q.

Proof: The result follows from direct computations as follows:
(cid:13)(I − Q+Q)z(cid:13)
(cid:13)
2
2 = zT (I − Q+Q)2z
(cid:13)

(a)
= zT (I − Q+Q)z

(b)
= zT (I − QT (QQT )−1Q)z

(c)
≤ zT (I − λ−1

max{QT Q}QT Q)z

(d)
≤ (1 − δ2sλ−1

max{QT Q})zT z

where equality: (a) follows by noticing that the projection operator I − Q+Q is idempotent; (b)
follows from the deﬁnition of Q+ = QT (QQT )−1; (c) follows from the fact that QQT = I +OOT
is a positive deﬁnite matrix and hence the inverse (QQT )−1 exists and can be bounded as
max{QT Q}I; and (d) follow from the 2s-sparse observability assumption along
(QQT )−1 ≥ λ−1

with Proposition III.4.

Proposition IV.8. Let the linear control system deﬁned by (II.1) and (II.2) be 2s-sparse observ-

able with 2s-restricted eigenvalue δ2s. If the following conditions hold:

1) the estimate ˆz(k,0)
E
2) the step size η satisﬁes η < 2λ−1

is s-sparse, i.e., ˆz(k,0) =
max{QT Q},

(cid:16)

ˆz(k,0)
x

, ˆz(k,0)
E

∈ Rn × Ss,

(cid:17)

then for any ε ∈ R+, the following inequality holds for any m ≥
(cid:17)

(cid:16)(cid:0)1 − δ2sλ−1

max{QT Q}(cid:1) 1

2 + ε

W (cid:0)ˆz(k,m)(cid:1) ≤

log ε
log(1−ηδ2s)
W (cid:0)ˆz(k,0)(cid:1) ,

(cid:108)

(cid:109)

(IV.2)

where ˆz(k,m) ∈ Rn × Rpτ is recursively deﬁned by:

ˆz(k,m+1) := ˆz(k,m) + ηQT (cid:0)Y − Qˆz(k,m)(cid:1) .

Proof: The Lyapunov candidate function W (cid:0)ˆz(k,m)(cid:1), after taking one gradient descent step,

can be written as follows:
W (cid:0)ˆz(k,m)(cid:1) = (cid:13)

(cid:13)z∗ − ˆz(k,m−1) − ηQT (cid:0)Y − Qˆz(k,m−1)(cid:1)(cid:13)
(cid:13)2

(cid:13)2 = (cid:13)

(cid:13)z∗ − ˆz(k,m)(cid:13)
(cid:13)z∗ − ˆz(k,m−1) − ηQT (cid:0)Qz∗ − Qˆz(k,m−1)(cid:1)(cid:13)
(cid:13)2
(cid:13)e(k,m−1) − ηQT Qe(k,m−1)(cid:13)

= (cid:13)

= (cid:13)

(cid:13)2 .

September 22, 2014

DRAFT

Recursively extending the previous analysis to m steps we obtain:

W (cid:0)ˆz(k,m)(cid:1) = (cid:13)

(cid:13)(I − ηQT Q)me(k,0)(cid:13)

(cid:13)2 .

Now deﬁne the projection error (cid:15)(m) as:

(cid:15)(m) = (I − Q+Q) − (I − ηQT Q)m

17

(IV.3)

(IV.4)

where Q+ is the Moore-Penrose pseudo inverse of Q. It follows from Corollary 3 in [25] that:

Given any ε ∈ R+, we conclude that in no more than:

(cid:107)(cid:15)(m)(cid:107)2 ≤ (1 − ηδ2s)m .

(cid:24)

(cid:25)

log ε
log (1 − ηδ2s)

steps, the projection error (cid:107)(cid:15)(m)(cid:107)2 satisﬁes (cid:107)(cid:15)(m)(cid:107)2 ≤ ε. Hence:
(cid:13)(I − Q+Q)e(k,0) − (cid:15)(m)e(k,0)(cid:13)
(cid:13)2

W (cid:0)ˆz(k,m)(cid:1) (a)

= (cid:13)

(b)

≤ (cid:13)

(cid:13)(I − Q+Q)e(k,0)(cid:13)
(cid:16)(cid:0)1 − δ2sλ−1
(cid:16)(cid:0)1 − δ2sλ−1

(cid:13)2 + (cid:13)
max{QT Q}(cid:1) 1
max{QT Q}(cid:1) 1

(c)
≤

≤

(cid:13)(cid:15)(m)e(k,0)(cid:13)
(cid:13)2

(cid:17) (cid:13)
(cid:13)e(k,0)(cid:13)
(cid:13)2

2 + (cid:107)(cid:15)(m)(cid:107)2
(cid:17)

2 + ε

W (cid:0)ˆz(k,0)(cid:1)

where the equality: (a) follows by substituting (IV.4) in (IV.3); (b) follows from the triangular

inequality; and (c) follows from Proposition IV.7 and the ﬁrst assumption which guarantees that
ˆz(k,0)
E

is s-sparse and hence z∗

is at most 2s-sparse.

E − ˆz(k,0)

E

Remark IV.9. It follows from the previous analysis that we can replace the inner loop in

Algorithm 1 with a one step projection of ˆz on the kernel of Q, that is, Algorithm 1 can be

simpliﬁed to the alternation between the following two steps:

ˆz(k+1) := ˆz(k)

Π + Q+ (cid:16)
:= Π (cid:0)ˆz(k+1)(cid:1) .

ˆz(k+1)
Π

Y − Qˆz(k)
Π

(cid:17)

,

However, since computing the pseudo inverse matrix Q+ can, in general, suffer from numerical

issues, we argue that using the gradient descent algorithm (or any other recursive implementation

that computes Q+, e.g., Newton method, conjugate gradients, ...) is preferable.

September 22, 2014

DRAFT

3) Termination of the ETPG’s Inner Loop: In the following result, we establish sufﬁcient

conditions for termination of the inner loop.

Proposition IV.10. Let the linear control system deﬁned by (II.1) and (II.2) be 2s-sparse

observable with 2s-restricted eigenvalue δ2s. If the following holds:

18

1) 0 < η < 2λ−1

max{QT Q},

2) δ2s > 4

9λmax{QT Q}
then, after no more than:







inner loop iterations, the inner loop condition V

(cid:17)

(cid:16)

ˆz(k)
Π

< V

ˆz(k−1)
Π

log 3
2

(cid:0)δ2sλ−1

max

(cid:8)QT Q(cid:9)(cid:1) 1

2 − 1

log (1 − ηδ2s)






(cid:16)

(IV.5)

(cid:17)

is satisﬁed.

Proof: Before we start, we recall that V (cid:0)ˆz(k)

Π

(cid:1) = V (cid:0)ˆz(k,0)(cid:1) = 1

2

It follows from the deﬁnition of e(k,0) = z∗ − Π(z(k−1,m)) that e(k,0)

(cid:13)Qe(k,0)(cid:13)
(cid:13)
(cid:13)Y − Qˆz(k,0)(cid:13)
(cid:13)
2
2
2 = 1
2.
(cid:13)
(cid:13)
is at most 2s-sparse.

2

E

Accordingly, inequality (III.3) holds as follows:

W 2 (cid:16)

ˆz(k)
Π

(cid:17)

≤ V

(cid:17)

(cid:16)

ˆz(k)
Π

≤

δ2s
2

λmax{QT Q}
2

W 2 (cid:16)

ˆz(k)
Π

(cid:17)

(IV.6)

Hence, if we can prove that applying

(cid:38)

log 3

2 (δ2sλ−1

max{QT Q})

log(1−ηδ2s)

(cid:39)

1
2 −1

gradient descent steps followed

by a projection step implies that:
W 2 (cid:16)

ˆz(k)
Π

(cid:17)

< δ2sλ−1

max{QT Q}W 2 (cid:16)

ˆz(k−1)
Π

(cid:17)

(IV.7)

holds, we can combine (IV.7) with (IV.6) to obtain:

(cid:17)

(cid:16)

ˆz(k)
Π

≤

V

λmax{QT Q}
2

W 2 (cid:16)

ˆz(k)
Π

(cid:17)

<

δ2s
2

W 2 (cid:16)

ˆz(k−1)
Π

(cid:17)

≤ V

(cid:16)

ˆz(k−1)
Π

(cid:17)

and conclude that the inner loop condition is satisﬁed. Therefore, to ﬁnalize the proof we need

to show that inequality (IV.7) holds. This follows directly from:

(cid:17)

(cid:16)
ˆz(k)
Π

W

= W (cid:0)ˆz(k,0)(cid:1) = (cid:13)

(cid:13)z∗ − Π(ˆz(k−1,m))(cid:13)
(cid:13)2
(cid:17)

max{QT Q}(cid:1) 1

2 + ε

W

(a)

≤ 2W (cid:0)ˆz(k−1,m)(cid:1)

(cid:16)

ˆz(k−1)
Π

(cid:17)

(b)
≤ 2

(cid:16)(cid:0)1 − δ2sλ−1

where the inequality (a) follows from Proposition IV.6 while inequality (b) follows from Proposi-
(cid:17)

tion IV.8. To conclude the result, we need to show that the factor 2

(cid:16)(cid:0)1 − δ2sλ−1

max{QT Q}(cid:1) 1

2 + ε

September 22, 2014

DRAFT

is strictly less than (δ2sλ−1

max{QT Q}) 1

2 . But this follows directly from assumption 2):

19

δ2sλ−1
max

(cid:8)QT Q(cid:9) >

4
9

(cid:8)QT Q(cid:9)(cid:1) 1

2 > 2

⇒ 3 (cid:0)δ2sλ−1
(cid:16)

max

⇒ 2

1 − (cid:0)δ2sλ−1

max

(cid:8)QT Q(cid:9)(cid:1) 1

2

(cid:16)

⇒ 2

1 − (cid:0)δ2sλ−1

max

(cid:8)QT Q(cid:9)(cid:1) 1

2

(cid:17)

(cid:17)

< (cid:0)δ2sλ−1

max

(cid:8)QT Q(cid:9)(cid:1) 1

2

+ 2ε < (cid:0)δ2sλ−1

max

(cid:8)QT Q(cid:9)(cid:1) 1

2

for any ε satisfying:

2ε < (cid:0)δ2sλ−1

max

(cid:8)QT Q(cid:9)(cid:1) 1

2 − 2

(cid:16)

1 − (cid:0)δ2sλ−1

max

(cid:8)QT Q(cid:9)(cid:1) 1

2

(cid:17)

= 3 (cid:0)δ2sλ−1

max

(cid:8)QT Q(cid:9)(cid:1) 1

2 − 2.

D. Stability of ETPG Algorithm

Using the previous results, we can now show convergence of the ETPG algorithm.

Proof of Theorem IV.3: Convergence of the algorithm, in the Lyapunov sense, follows

directly from the termination of the inner loop shown in Proposition IV.10. Consider the following

sequence:

Y (k,m) =

(cid:114) 2
δ2s

V (ˆz(k,0)) =

(cid:114) 2
δ2s

1
2

(cid:107)Qe(k,0)(cid:107)2
2

We ﬁrst show that this sequence forms an upper bound for W (cid:0)ˆz(k,m)(cid:1). For m = 0, it follows
is at most 2s-sparse.

from the deﬁnition of e(k,0) = z∗ − ˆz(k,0) = z∗ − Π(ˆz(k−1,m)) that e(k,0)

E

Accordingly inequality (IV.6) holds as follows:

δ2s
2

W 2 (cid:0)ˆz(k,0)(cid:1) ≤ V (cid:0)ˆz(k,0)(cid:1) ⇒ W (cid:0)ˆz(k,0)(cid:1) ≤ Y (k,0)

(IV.8)

Note that upper bound (IV.8) is satisﬁed only when ˆE is cyclic s-sparse. Thus it is only applicable
after applying the projection operator, i.e., when m = 0. We now extend this bound for m > 0

as follows:

W (cid:0)ˆz(k,m)(cid:1) < W (cid:0)ˆz(k,0)(cid:1) = Y (k,0) = Y (k,m),

(IV.9)

where the inequality follows from Proposition IV.8 along with assumption 2).
(cid:16)
ˆz(k+1)
Π

Since 0 ≤ W (cid:0)ˆz(k,m)(cid:1) ≤ Y (k,m), and by the triggering condition V

(cid:17)

< V

(cid:17)

(cid:16)
ˆz(k)
Π

we

have:

lim
k→∞

Y (k,m) = lim
k→∞

(cid:114) 2
δ2s

V (ˆz(k,0)) = 0

September 22, 2014

DRAFT

and we conclude:

W (cid:0)ˆz(k,0)(cid:1) = 0.

lim
k→∞

20

V. AN EVENT-TRIGGERED PROJECTED LUENBERGER OBSERVER

In this section we describe a solution to Problem II.4 obtained by rendering the ETPG

algorithm recursive.

A. The Algorithm

We start again with the linear dynamical system deﬁned by (II.1) and (II.2). The dynamics of
z(t) = (x(t − τ + 1), E(t)) ∈ Rn × Rpτ can be written, using the equality a(t) = y(t) − Cx(t),

as follows:


x(t − τ + 1)








a(t − τ + 1)
...
a(t)










=












A

0
...

0 0 . . . 0





x(t − τ )



0 I

. . . 0















a(t − τ )
...
a(t − 1)








+

−CAτ 0 0 . . . 0

B

0
...

0

0

. . .

. . .

0

0
...








−CAτ −3B −CAτ −2B . . . −CB





u(t − τ )















u(t − τ + 1)
...
u(t − 1)










+












0


0


...



I

y(t),










y(t − τ )

y(t − τ + 1)
...
y(t − 1)










=












C








CA
...
CAτ

x(t − τ ) +



a(t − τ )








a(t − τ + 1)
...
a(t − 1)










+ F U (t − 1)

or in the more compact form:

z(t) = Az(t − 1) + B ¯u(t − 1)

Y (t − 1) = Qz(t − 1),

September 22, 2014

DRAFT

where:

A =













A

0
...
0

0 . . . 0

I

. . . 0
. . .
0 . . . I

−CAτ 0 . . . 0













, B

=













B

0
...
0

0

0

0

. . .

. . .

. . .

0

0
...
0

0

0
...
0

−CAτ −3B −CAτ −2B . . . −CB I

21













,

Y (t − 1) = (cid:101)Y (t − 1) − F U (t − 1), ¯u(t − 1) =

deﬁned as in Section II.



U (t − 1)





y(t)

, and z(t), (cid:101)Y (t), U (t), Q, F are as

The Event-Triggered Projected Luenberger (ETPL) Observer consist of the iteration of the

following two steps:

a) Time Update: Starting from an estimate ˆzΠ(t−1) = (ˆx(t−1), ˆEΠ(t−1)) with ˆEΠ(t−1)

s-sparse, we use the dynamics to update the previous estimate:

ˆz(t) = AˆzΠ(t − 1) + Bu(t − 1).

This may result in an increase in the value of the Lyapunov candidate function V .

b) Event-Triggered Projected Luenberger (Measurement) Update: In this step, we alternate

between applying the Luenberger update step:

ˆz(m+1)(t) = ˆz(m)(t) + L (cid:0)Y (t) − Qˆz(m)(t)(cid:1) ,

multiple times followed by the projection operator Π once:

ˆzΠ(t) = Π (cid:0)ˆz(m)(t)(cid:1) .

It follows from the proof of Theorem IV.3 that alternating between multiple Luenberger updates
(which is the generalization of the gradient descent step when L = QT Σ for some positive

deﬁnite matrix Σ) and projection steps forces a decrease of the Lyapunov candidate function
V (ˆz(t)) = 1

2. In order to compensate for the increase introduced by the time
update step, we need to ensure that V decreases along the execution of the algorithm. Hence,

2 (cid:107)Y (t) − Qˆz(t)(cid:107)2

multiple Luenberger updates and projection steps are performed for each time-update step. Using

the same triggering technique, by monitoring the evolution of V , the algorithm can determine

when the decrease in V caused by the Luenberger update/projection steps compensates the

increase caused by the time-update. This sequence of steps results in Algorithm 2.

September 22, 2014

DRAFT

22

Algorithm 2 Event Triggered Projected Luenberger Observer

a) Time Update:

ˆz(t) = Aˆz(t − 1) + Bu(t − 1);

b) Event-Triggered Projected Luenberger (Measurement) Update:

while V (ˆzΠ(t)) ≥ V (ˆzΠ(t − 1)) do

reset m := 0, ˆz(0)(t) = ˆzΠ(t) = Π(ˆz(t));

Vtemp := V (ˆzΠ(t));

while Vtemp ≥ V (ˆzΠ(t)) do

ˆz(m+1)(t) := ˆz(m)(t) + L (cid:0)Y (t) − Qˆz(m)(t)(cid:1);
Vtemp := V (Π (cid:0)ˆz(m+1)(t)(cid:1));
m := m + 1;

end while

ˆz(t) := ˆz(m)(t);

end while

To make the connection with the standard Luenberger observer clearer, assume that only one

Luenberger/projection update is required per time update. In this case, the ETPL observer can

be written as:

ˆz(t) = AˆzΠ(t − 1) + Bu(t − 1)

+ L(cid:48) (Y (t − 1) − QˆzΠ(t − 1))

ˆzΠ(t) = Π(ˆz(t))

which has the form of a standard Luenberger observer with gain L(cid:48) = A L along with the

projection Π step.

Remark V.1. It follows from Remark IV.9 that we can replace the inner loop in Algorithm 2

with one update step if we ﬁx the Luenberger gain L to be L = Q+.

B. Convergence of the ETPL Observer

In this subsection we discuss the convergence and performance of the ETPL observer. The

main result of this subsection is stated in the following theorem.

September 22, 2014

DRAFT

23

Theorem V.2. (Convergence of the ETPL observer) Let the linear control system deﬁned by

(II.1) and (II.2) be 2s-observable system with 2s-restricted eigenvalue δ2s. If the following

condition holds:

δ2s >

4
9

λmax{QT Q}

then the dynamical system deﬁned by the ETPL observer is a solution to Problem II.4 whenever

L = QT Σ for any positive deﬁnite weighting matrix Σ satisfying λmax{Σ} < λ−1

max{QT Q}.

The proof of Theorem V.2 is based on the Lyapunov candidate function:

W (t) = (cid:107)z(t) − ˆz(t)(cid:107)2 = (cid:107)e(t)(cid:107)2 ,

and follows the exact same argument used in the proof of Theorem IV.3 with the exception

of the need to account for an increase in V caused by the time update step. Such increase is

compensated by applying the Luenberger update loop multiple times. This increase is described

in the next result.

C. Effect of Time Update

Proposition V.3. Consider the linear control system deﬁned by (II.1) and (II.2). The following

inequality holds:

W (ˆz(t)) ≤ (cid:13)

(cid:13)A(cid:13)

(cid:13)2 W (ˆz(t − 1)),

whenever ˆz(t) ∈ Rn × Rpτ and ˆz(t − 1) ∈ Rn × Rpτ are related by:

ˆz(t) = Aˆz(t − 1) + Bu(t − 1).

Proof: The Lyapunov candidate function W (ˆz(t)) after applying the time update step, can

be written as follows:

W (ˆz(t)) = (cid:107)z∗(t) − ˆz(t)(cid:107)2 = (cid:13)

(cid:13)Az∗(t − 1) + Bu(t − 1) − Aˆz(t − 1) − Bu(t − 1)(cid:13)
(cid:13)2

= (cid:13)

(cid:13)A(z∗(t − 1) − ˆz(t − 1))(cid:13)

(cid:13)2 ≤ (cid:13)

(cid:13)A(cid:13)

(cid:13)2 W (ˆz(t − 1)).

September 22, 2014

DRAFT

24

Proof of Theorem V.2: First, we note that the Luenberger update loop in Algorithm 2 is

(cid:17)

(cid:16)

ˆz(k)
Π

identical to the outer loop of Algorithm 1 with the loop guard V

< 0 (Line 2 in Algorithm

1) replaced with V (ˆzΠ(t)) < V (ˆzΠ(t − 1)). Hence, it follows from Theorem IV.3 that “Event-

Triggered Luenberger Update” loop terminates. From the termination of the Luenberger update

loop, we conclude that the increase caused by the time update (Proposition V.3) can always be

compensated. Accordingly, using the same argument that was used in the proof of Theorem IV.3,

we conclude that limt→∞ W (ˆzΠ(t)) = 0 and hence the estimate converges to the desired value
z∗.

A. Multiple Attacking Sequences

VI. SIMULATION RESULTS

In this example we consider an Unmanned Ground Vehicle (UGV) under different types of

sensor attacks. We assume that the UGV moves along straight lines and completely stops before

rotating. Under these assumptions, we can describe the dynamics of the UGV by:





 =







 =







˙x

˙v





˙θ

˙ω





0

1

0 −B
M





0

1

0 −Br
J





x





 +





 F,

0

1
M




v




θ

ω

 +





 T,

0

1
J

where x, v, θ, ω are the states of the UGV corresponding to position, linear velocity, angular

position and angular velocity, respectively. The parameters M, J, B, Br denote the mechanical

mass, inertia, translational friction coefﬁcient and the rotational friction coefﬁcient. The inputs

to the UGV are the force F and the torque T . The UGV is equipped with a GPS sensor which

measures the UGV position, two motor encoders which measure the translational velocity and an

inertial measurement unit which measures both rotational velocity and rotational position. We as-

sume that encoders perform the necessary processing to directly provide a velocity measurement

September 22, 2014

DRAFT

and hence the resulting output equation can be written as:






y =











1 0 0 0

0 1 0 0

0 1 0 0

0 0 1 0

0 0 0 1



+

















x

v

θ

ω





















25













ψ1

ψ2

ψ3

ψ4

ψ5

,

where ψi is the measurement noise of the ithe sensor which is assumed to be gaussian with zero

norm and ﬁnite covariance.

By applying Proposition III.4 for different values of s, we conclude that the UGV can be

resilient only to one attack on any of the two encoders. Attacking any other sensor precludes

the system from being 2s-sparse observable.

Figure 2 shows the performance of the proposed algorithms under different attacks on the

UGV motor encoders. The attacker alternates between corrupting the left and the right encoder

measurements as shown in Figure 2(d) and Figure 2(c). Three different types of attacks are

considered. First, the attacker corrupts the sensor signal with random noise. The next attack

consists of a step function followed by a ramp. Finally a replay-attack is mounted by replaying

the previously measured UGV velocity.

The UGV vehicle goal is to move 5m along a straight line, stop and perform a 90o rotation

and repeat this pattern 3 times until it traces a square and returns to its original position and

orientation. In Figures 2(a) and 2(b) we show the result of using the ETPG algorithm and the

ETPL observer to reconstruct the state under sensor attacks. The reconstructed state is used by

a linear feedback tracking controller forcing the UGV to track the desired square trajectory.

The reconstructed position and velocity are shown in Figures 2(a) and 2(b). These ﬁgures

show that both algorithms are able to successfully reconstruct the state and hence the UGV

is able to reach its goal despite the attacks. Moreover, we observe that the ETPL observer is

less sensitive to noise compared to the ETPG. This follows from the fact the ETPL observer

“averages out” the noise by using all the available sensor data as it becomes available.

Recall that the attack model in Section II requires the set of attacked sensors to remain constant

over time. However, Figure 2 shows the proposed algorithms correctly constructing the state even

though this assumption is violated. This is due to the fact that the period during which only

September 22, 2014

DRAFT

one sensor is attacked is sufﬁciently long compared with the time it takes for the algorithms to

26

converge.

B. Computational Performance

We compare the efﬁciency of the proposed ETPG and ETPL algorithms against the L1/Lr

decoder introduced in [10]. To perform this comparison, we randomly generated 100 systems

with n = 20 and p = 25 and simulated them against an increasing number of attacked sensors

ranging from 0 to 12. For each test case we generated a random support set for the attack vector,

random attack signal and random initial conditions. Averaged results for the different numbers

of attacked sensors are shown in Figure 3. Although we claim no statistical signiﬁcance, the

results in Figure 3 are characteristic of the many simulations performed by the authors. The

L1/Lr decoder is implemented using CVX while the ETPG and ETPL algorithms are direct

implementations of Algorithms 1 and 2 in Matlab. The tests were performed on a desktop

equipped with an Intel Core i7 processor operating at 3.4 GHz and 8 GBytes of memory.

Note that the ETPL observer, as required by any solution to Problem II.4, is an asymptotic

observer, i.e., the reconstructed state converges to the true state asymptotically. This should be

contrasted with the ETPG algorithm where a single execution of Algorithm 1 is sufﬁcient to

construct an estimate which is sufﬁciently close1 to the system state. Hence, to compare the

ETPG and ETPL algorithms we deﬁne the execution time as the time needed by Algorithms 1

and 2 to terminate and the convergence time as the time needed by each algorithm from the start
of the execution until the estimate becomes (cid:15)-close to the system state (with (cid:15) is set to (cid:15) = 10−6

in this example). Note that the execution time affects the choice of the sampling period when

the algorithm is deployed while the convergence time reﬂects the performance of each of the

algorithms.

In Figure 3(a) we can appreciate how both the ETPG and the ETPL algorithms outperform

the L1/Lr decoder by an order of magnitude in execution time. We also observe that the ETPG

algorithm requires more execution time compared to the ETPL observer. This follows from the

existence of the outer-loop in the ETPG algorithm which requires the Lyapunov function to

1Although the outer loop of Algorithm 1 requires V
(cid:16)
ˆz(k−1)
Π

≤ (cid:15) with (cid:15) = 10−6.

(cid:17)

V

(cid:16)
ˆz(k−1)
Π

(cid:17)

≤ 0 for termination, our implementation used instead

September 22, 2014

DRAFT

27

(a) Reconstructed position versus ground truth.

(b) Reconstructed velocity versus ground truth.

(c) Reconstructed attack on left encoder versus ground truth.

(d) Reconstructed attack on right encoder versus ground truth.

Fig. 2. Performance of the UGV controller in the cases where no attack takes place versus the case where the attack signal is
applied to the UGV encoders. The objective is to move 5 m, stop and perform a 90o rotation and repeat this pattern to follow

a square path. The controller uses ETPG algorithm and ETPL Observer to reconstruct the UGV states. In both cases we show

the linear position (top), linear velocity (middle), and the reconstruction of the attack signal (bottom).

September 22, 2014

DRAFT

0510152025303405101520Time[s]Position[m]ETPGAlgorithmGroundtruth0510152025303505101520Time[s]Position[m]ETPLObserverGroundtruth05101520253034012Time[s]Velocity[m/s]ETPGAlgorithmGroundtruth05101520253035012Time[s]Velocity[m/s]ETPLObserverGroundtruth051015202530340246Time[s]AttacksignalETPGAlgorithmAttacksignal051015202530350246Time[s]AttacksignalETPLObserverAttacksignal051015202530340246Time[s]AttacksignalETPGAlgorithmAttacksignal0510152025303502468Time[s]AttacksignalETPLObserverAttacksignal28

]
s
[

e
m

i
t

n
o
i
t
u
c
e
x
E

0.4

0.2

0

Lr/L2 Decoder
ETPG Algorithm
ETPL Observer

2

4

6

8

10

12

Number of attacked sensors s

]
s
[

e
m

i
t

e
c
n
e
g
r
e
v
n
o
C

0.4

0.2

0

Lr/L2 Decoder
ETPG Algorithm
ETPL Observer

2

4

6

8

10

12

Number of attacked sensors s

(a) Execution time.

(b) Convergence time.

Fig. 3. Timing analysis of the L1/L2 decoder versus the ETPG and ETPL algorithms for different numbers of attacked sensors.

Subﬁgure (a) shows the time required to execute each of the three algorithms. Subﬁgure (b) shows the time required for the
estimate, computed by each of the algorithms, to become (cid:15)-close to the system state for (cid:15) = 10−6.

reach zero before termination. In Figure 3(b) we show the convergence time for each of the

three algorithms. It follows from the nature of the L1/Lr decoder and the ETPG algorithm that

the execution time and the convergence time are both equal. This is not the case for the ETPL

observer which requires a longer convergence time compared to the ETPG algorithm. These two

ﬁgures illustrate the tradeoff between execution timing and performance.

VII. CONCLUSIONS

In this paper we considered the problem of designing computationally efﬁcient algorithms for

state reconstruction under sparse adversarial attacks/noise. We characterized the solvability of

this problem by using the notion of sparse observability and proposed two algorithms for state

reconstruction. To improve the timing performance of the proposed algorithms, we adopted an

event-triggered approach that determines on-line how many gradient steps should be executed

per projection on the set of constraints. These algorithms can be further improved along multiple

directions such as dynamically adjusting the step size or using more reﬁned gradient algorithms,

such as conjugated gradient.

[1] R. Langner, “Stuxnet: Dissecting a cyberwarfare weapon,” IEEE Security and Privacy Magazine, vol. 9, no. 3, pp. 49–51,

2011.

REFERENCES

September 22, 2014

DRAFT

29

[2] Y. Shoukry, P. D. Martin, P. Tabuada, and M. B. Srivastava, “Non-invasive spooﬁng attacks for anti-lock braking systems,”

in Workshop on Cryptographic Hardware and Embedded Systems, ser. G. Bertoni and J.-S. Coron (Eds.): CHES 2013,

LNCS 8086.

International Association for Cryptologic Research, 2013, pp. 55–72.

[3] K. Sou, H. Sandberg, and K. Johansson, “On the exact solution to a smart grid cyber-security analysis problem,” IEEE

Transactions on Smart Grid, vol. 4, no. 2, pp. 856–865, 2013.

[4] Y. Liu, P. Ning, and M. K. Reiter, “False data injection attacks against state estimation in electric power grids,” in

Proceedings of the 16th ACM conference on Computer and communications security, ser. CCS ’09. New York, NY, USA:

ACM, 2009, pp. 21–32.

[5] G. D`an and H. Sandberg, “Stealth attacks and protection schemes for state estimators in power systems,” in First IEEE

International Conference on Smart Grid Communications (SmartGridComm), 2010, pp. 214–219.

[6] O. Kosut, L. Jia, R. Thomas, and L. Tong, “Malicious data attacks on the smart grid,” IEEE Transactions on Smart Grid,

vol. 2, no. 4, pp. 645–658, 2011.

[7] T. Kim and H. Poor, “Strategic protection against data injection attacks on power grids,” IEEE Transactions on Smart

Grid, vol. 2, no. 2, pp. 326–333, 2011.

[8] F. Pasqualetti, F. Dorﬂer, and F. Bullo, “Attack detection and identiﬁcation in cyber-physical systems,” IEEE Transactions

on Automatic Control, vol. 58, no. 11, pp. 2715–2729, Nov 2013.

[9] H. Fawzi, P. Tabuada, and S. Diggavi, “Secure estimation and control for cyber-physical systems under adversarial attacks,”

IEEE Transactions on Automatic Control, vol. 59, no. 6, pp. 1454–1467, June 2014.

[10] ——, “Secure state-estimation for dynamical systems under active adversaries,” in 49th Annual Allerton Conference on

Communication, Control, and Computing (Allerton), sept. 2011, pp. 337–344.

[11] ——, “Security for control systems under sensor and actuator attacks,” in IEEE 5st Annual Conference on Decision and

Control (CDC), Nov. 2012, pp. 3412–3417.

[12] E. Candes and T. Tao, “Decoding by linear programming,” IEEE Transactions on Information Theory, vol. 51, no. 12, pp.

4203–4215, 2005.

[13] M. A. Davenport, M. F. Duarte, Y. C. Eldar, and G. Kutyniok, “Introduction to compressed sensing,” in Compressed

Sensing: Theory and Applications, Y. C. Eldar and G. Kutyniok, Eds. Cambridge University Press, 2011.

[14] J. Mattingley and S. Boyd, “Real-time convex optimization in signal processing,” IEEE Signal Processing Magazine,

vol. 27, no. 3, pp. 50–61, May 2010.

[15] M. S. Asif and J. K. Romberg, “Sparse recovery of streaming signals using l1-homotopy,” CoRR, vol. abs/1306.3331,

2013.

[16] T. Blumensath, “Accelerated iterative hard thresholding,” Signal Processing, vol. 92, no. 3, pp. 752–756, 2012.

[17] W. Kratz, “Characterization of strong observability and construction of an observer,” Linear Algebra and its Applications,

vol. 221, no. 0, pp. 31–40, 1995.

[18] S. Sundaram and C. Hadjicostis, “Distributed function calculation via linear iterative strategies in the presence of malicious

agents,” IEEE Transactions on Automatic Control, vol. 56, no. 7, pp. 1495–1508, 2011.

[19] G. Raskutti, M. J. Wainwright, and B. Yu, “Restricted eigenvalue properties for correlated gaussian designs,” J. Mach.

Learn. Res., vol. 99, pp. 2241–2259, August 2010.

[20] E. Candes and M. Wakin, “An introduction to compressive sampling,” IEEE Signal Processing Magazine, vol. 25, no. 2,

pp. 21–30, 2008.

September 22, 2014

DRAFT

30

[21] E. Candes, J. Romberg, and T. Tao, “Robust uncertainty principles: exact signal reconstruction from highly incomplete

frequency information,” IEEE Transactions on Information Theory, vol. 52, no. 2, pp. 489–509, 2006.

[22] B. Amit and K. Eugenius, Control Perspectives on Numerical Algorithms and Matrix Problems.

Society for Industrial

and Applied Mathematics, 2006.

[23] P. Tabuada, “Event-triggered real-time scheduling of stabilizing control tasks,” IEEE Transactions on Automatic Control,

vol. 52, no. 9, pp. 1680–1685, Sept.

[24] T. Blumensath and M. Davies, “Normalized iterative hard thresholding: Guaranteed stability and performance,” IEEE

Journal of Selected Topics in Signal Processing, vol. 4, no. 2, pp. 298–309, April 2010.

[25] W. Petryshyn, “On generalized inverses and on the uniform convergence of (I − βK)n with application to iterative

methods,” Journal of Mathematical Analysis and Applications, vol. 18, no. 3, pp. 417–439, 1967.

September 22, 2014

DRAFT

