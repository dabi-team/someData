StreaMon: a data-plane programming abstraction for
Software-deﬁned Stream Monitoring∗

Giuseppe Bianchi, Marco Bonola, Giulio Picierro, Salvatore Pontarelli, Marco Monaci
University of Rome "Tor Vergata"
name.surname@uniroma2.it

3
1
0
2

v
o
N
1
1

]
I

N
.
s
c
[

1
v
2
4
4
2
.
1
1
3
1
:
v
i
X
r
a

ABSTRACT
The fast evolving nature of modern cyber threats and net-
work monitoring needs calls for new, “software-deﬁned”,
approaches to simplify and quicken programming and de-
ployment of online (stream-based) trafﬁc analysis functions.
StreaMon is a carefully designed data-plane abstraction de-
vised to scalably decouple the “programming logic” of a
trafﬁc analysis application (tracked states, features, anomaly
conditions, etc.) from elementary primitives (counting and
metering, matching, events generation, etc), efﬁciently pre-
implemented in the probes, and used as common instruction
set for supporting the desired logic. Multi-stage multi-step
real-time tracking and detection algorithms are supported via
the ability to deploy custom states, relevant state transitions,
and associated monitoring actions and triggering conditions.
Such a separation entails platform-independent, portable, on-
line trafﬁc analysis tasks written in a high level language,
without requiring developers to access the monitoring device
internals and program their custom monitoring logic via low
level compiled languages (e.g., C, assembly, VHDL). We
validate our design by developing a prototype and a set of
simple (but functionally demanding) use-case applications
and by testing them over real trafﬁc traces.

1.

INTRODUCTION

The sheer volume of networked information, in con-
junction with the complexity and polymorphous na-
ture of modern cyberthreats, calls for scalable, accurate,
and, at the same time, ﬂexible and programmable mon-
itoring systems [13, 11]. The challenge is to promptly
react to fastly mutating needs by deploying custom traf-
ﬁc analyses, capable of tracking event chains and multi-
stage attacks, and eﬃciently handle the many hetero-
geneous features, events, and conditions which char-
acterize an operational failure, a network application
mis-behavior, an anomaly or an incoming attack. Such
needed level of ﬂexibility and programmability should
address scalability by design, through systematic ex-
ploitation of stream-based analysis techniques. And,
even more challenging, traﬃc analyses and mitigation

∗
ferred without notice, after which this version may no longer be accessible.

This work will be submitted for possible publication. Copyright may be trans-

primitives should be ideally brought inside the monitor-
ing probes themselves at data plane, so as to avoid ex-
porting traﬃc data to central analysis points, an hardly
adequate way to cope with the traﬃc scale and the strict
(ideally real time) mitigation delay requirements.

To face this rapidly evolving scenario, in this paper we
propose StreaMon, a data-plane programming abstrac-
tion for stream-based monitoring tasks directly running
over network probes. StreaMon is devised as a prag-
matic tradeoﬀ between full programmability and ven-
dors’ need to keep their platforms closed. StreaMon’s
strategy closely resembles that pioneered by Openﬂow
[25] in the abstraction of networking functionalities, thus
paving the road towards software-deﬁned networking1.
However, the analogy with Openﬂow limits to the strate-
gic level; in its technical design, StreaMon signiﬁcantly
departs from Openﬂow for the very simple reason that
(as discussed in Section 2) the data-plane programma-
bility of monitoring tasks exhibits very diﬀerent require-
ments with respect to the data-plane programmability
of networking functionalities, and thus mandate for dif-
ferent programming abstractions.
The actual contribution of this paper is threefold.

(1) We identify (and design an execution platform
for) an extremely simple abstraction which appears ca-
pable of supporting a wide range of monitoring appli-
cation requirements. The proposed API decouples the
monitoring application “logic”, externally provided by
a third party programmer via (easy to code) eXtended
Finite State Machines (XFSM), from the actual “prim-
itives”, namely conﬁgurable sketch-based measurement
modules, d-left hash tables, state management primi-
tives, and export/mitigation actions, hard-coded in the
device. While our handling of sketch-based measure-
ment primitives may recall [34], we radically diﬀerenti-
ate from any other work we are aware of, in our pro-

1 And in fact, even if the focus of this paper is (as a ﬁrst step)
only on the data plane interface itself, we believe that Strea-
Mon could eventually candidate to play the role of a pos-
sible southbound data-plane interface for Software-Deﬁned
Monitoring frameworks, devised to translate high level mon-
itoring tasks into a sequence of low-level traﬃc analysis pro-
grams distributed into network-wide StreaMon probes.

1

 
 
 
 
 
 
Figure 1: StreaMon data plane identiﬁcation/measurement/decision abstraction, and its mapping to implementation-speciﬁc workﬂow tasks

posal to inject monitoring logic in the form of XFSM
devised to locally orchestrate and run-time adapt mea-
surements to the tracked state for each monitored entity
(ﬂows, hosts, DNS names, etc), with no need to resort
to an external controlling device.

(2) We implement two StreaMon platform prototypes,
a full SW and a FPGA/SW integrated implementation.
We functionally validate them with ﬁve use case exam-
ples (P2P traﬃc classiﬁcation, Conﬁcker botnet detec-
tion, packet entropy HW analysis, DDos detection, Port
Knocking), not meant as stand-alone contributions, but
rather selected to showcase the StreaMon’s adaptability
to diﬀerent application requirements.

(3) We assess the performance of the proposed ap-
proach: even if the current prototype implementation is
not primarily designed with performance requirements
in mind, we show that it can already sustain traﬃc in
the multi-gbps range even with several instantiated met-
rics (for example 2.315 Gbps of real world replayed traf-
ﬁc with 16 metrics, see section 5.2); moreover, we show
that scalability can be easily enhanced by oﬄoading the
SW implementation with HW accelerated metrics; in
essence, it seems fair to say that scalability appears to
be an architectural property of our proposed API, rather
than a side eﬀect of an eﬃcient implementation.

2. STREAMON ABSTRACTION

Our strategy in devising an abstraction for deploying
stream-based monitoring tasks over a (general-purpose)
network monitoring probe is similar in spirit to that
brought about by the designers of the Openﬂow [25]
match/action abstraction, for programming network-
ing functionalities over a switching fabric. Indeed, we
also aim at identifying a compromise between full pro-
gramming ﬂexibility, so as to adapt to the very diverse
needs of monitoring application developers and permit
a broad range of innovation, and consistency with the
vendors’ need for closed platforms. However, the re-
quirements of monitoring applications appear largely
diﬀerent from that of a networking functionality, and
this naturally drives towards a diﬀerent pragmatic ab-
straction with respect to a match/action table.

At least three important diﬀerences do emerge. First,
the “entity” being monitored is not consistently associ-
ated with the same ﬁeld (or set of ﬁelds) in the packet
header. For instance, if the target is to detect whether

an IP address (monitored entity) is a bot, and the cho-
sen mechanism is to analyze if the percentage of DNS
NXDomain replies (feature) is greater than a given thresh-
old (condition), the ﬂow key to use for accounting is the
source IP address when the arriving packet is a DNS
query (event), but becomes the destination IP address
when the packet is a DNS response (a diﬀerent event).
Second, the type of analysis (and possibly the mon-
itoring entity target) entailed by a monitoring applica-
tion may change over time, dynamically adapting to the
knowledge gathered so far. For instance, if an IP ad-
dress exhibits a critical percentage of DNS NXDomain
replies, hence a bot suspect, we may further track its
TCP SYNACK/SYN ratio and determine whether hor-
izontal network scans occur, so as to reinforce our sus-
picion. And we might then follow up by deriving even
more in-depth features, e.g., based on deep packet in-
spection. But at the same time, we would like to avoid
tracking all features for all the possible ﬂows, as this
would unnecessarily drain computational resources.

Finally, activities associated to a monitoring task are
not all associated to a matching functionality: only
measurement and accounting tasks are. Rather, less
frequent, but crucial, activities (such as forging and ex-
porting alerts, changing states, setting a mitigation ﬁl-
tering rule, and so on) are based on decisions taken on
what we learned so far, and which are hence triggered
by conditions applied to the gathered features.

Our proposed StreaMon abstraction,

illustrated in
ﬁgure 1, appears capable to cope with such require-
ments (as more extensively shown with the use cases
presented in section 4). It comprises of three “stages”,
programmable by the monitoring application developer
via external means (i.e. not accessing the internal probe
platform implementation).

(1) The Identiﬁcation stage permits the program-
mer to specify what is the monitored entity (more pre-
cisely, deriving its primary key, i.e., a combination of
packet ﬁelds that identify the entity) associated to an
event triggered by the actual packet under arrival, as
well as retrieve an eventually associated state2.

2 Our implementation uses d-left hashes for O(1) complex-
ity in managing states. Also, memory consumption can be
greatly reduced by opportunistically storing only non-default
(e.g. anomalous) states. See the illustrative experiment in
Fig. 2 for a rough idea of the attainable saving in the Con-
ﬁcker detection example, use case 4.2.

2

Event match Event ID Event ID Primary Key Flow State Triggered Action IDENTIFICATION (of tracked entity) MEASUREMENT (associated to entity and its state) DECISION (condition evaluation) (consequent action & state transition) Primary Key Event ID Primary Key  Flow Status  Features   Packet or Timeout Entitymatch State Lookup Sketch-based  O(1) meas. bank Feature extraction Condition verification State transition   External Data/alert Export when applicable             Figure 3: StreaMon processing engine architecture

tems, namely Measurement subsystem and Logic sub-
system, detailed in sections 3.2 and 3.3, respectively.

Event layer - Such layer is in charge of parsing each
raw captured packet, and match an event among those
user-programmed via the StreaMon API. The matched
event identiﬁes a user-programmed primary key which
permits to retrieve an eventually stored state. The event
layer is further in charge of supplementary technical
tasks (see section 3.3), such as handling special time-
out events, deriving further secondary keys, etc.

Metric layer - StreaMon operates on a per-packet
basis and does not store any (raw) traﬃc in a local
database. The application programmer can instanti-
ate a number of metrics derived by a basic common
implemented as computation/memory eﬃ-
structure,
cient multi-hash data structures (i.e., Bloom-type sketches),
updated at every packet arrival.

Feature layer - this layer permits to compute user-
deﬁned arithmetic functions over (one or more) metric
outputs. Whereas metrics carry out the bulky task of
accounting basic statistics in a scalable and computa-
tion/memory eﬃcient manner, the features compute de-
rived statistics tailored to the speciﬁc application needs,
at no (noticeable) extra computational/memory cost.

Decision layer - this ﬁnal processing stage imple-
ments the actual application logic. This layer keeps
a list of conditions expressed as mathematical/logical
functions of the feature vector provided by the pre-
vious layer and any other possible secondary status.
Each condition will trigger a set of speciﬁed and pre-
implemented actions and a state transition.

StreaMon’s programming language

Application programmers describe their desired moni-
toring operations through an high-level XML-like lan-
guage, which permits to specify custom (dynamic) states,
conﬁgure measurement metrics, formalize when (e.g.in
which state and for which event) and how (i.e. by
performing which operations over available metrics and
state information) to extract features, and under which
conditions trigger relevant actions (e.g. send an alert or
data to a central controller). We remark that a moni-
toring application formally speciﬁed using our XML de-

Figure 2: Experimental analysis of number of NXDomain DNS re-
sponses received (a feature commonly used in Botnet de-
tection, e.g., [22]). Data obtained using a trace of 155
minutes with 955 diﬀerent hosts belonging to 36 diﬀer-
ent /24 subnetworks. (a): Number F1 of DNS NXDomain
responses received per host (430 IPs performing at least
1 DNS query, 401 of them don’t receive any NXDomain
response); (b): number of tcp packet per seconds for all
the ﬂows (pkt1) and only for the ﬂows for which F1 >= 1
(pkt2), 15 minutes sample; analysis time for whole pkt1
trace: 1179 seconds, versus 68 seconds for pkt2.

(2) The Measurement stage permits the program-
mer to conﬁgure which information should be accounted.
It integrates hard-coded and eﬃciently implemented hash-
based measurement primitives (metric modules), fed by
conﬁgurable packet/protocol ﬁelds, with externally pro-
grammed features, expressed as arbitrary arithmetic op-
erations on the metric modules’ output.

(3) The Decision stage is the most novel aspect
of our abstraction.
It permits to take decisions pro-
grammed in the form of eXtended Finite State Machines
(XFSM), i.e. check conditions associated to the current
state and tested over the currently computed features,
and trigger associated actions and/or state transitions.
The obvious compromise in our proposed approach
is that (as per the match/action Openﬂow primitives)
new metrics or actions can only be added by directly
implementing them over the monitoring devices, thus
extending the device capabilities. However, even re-
stricting to our actual StreaMon prototype, the subset
of metrics we implemented appear reusable and suﬃ-
cient to support features proposed in a meaningful set
of literature applications - see Table 1 in Section 3.2.

3. STREAMON PROCESSING ENGINE

3.1 System components

The previously introduced abstraction can be con-
cretely implemented by a stream processing engine whose
architecture is depicted in Figure 3. It consists of four
modular layers descriptively organized into two subsys-

3

0	  500	  1000	  1500	  2000	  2500	  0	  300	  600	  900	  TCP	  [packets	  per	  second]	  9me	  [sec]	  (b)	  pkt1	  pkt2	  1	  10	  100	  1000	  0	  20	  40	  60	  80	  100	  120	  140	  160	  Number	  of	  hosts	  Number	  of	  NXDomain	  responses	  (a)	  M1	  Metric Layer M2	  M3	  Feature Layer F1	  =	  M1+M2	  F2	  =	  M3/M2	  Decision Layer if	  (f1>200)	  then	  ACTION	  	  if	  (f2<.05)	  	  then	  ACTION	  Event Layer Timeouts	  Status	  Table	  Capture	  Engine	  	  	  incoming packet 	  	  	  	  	  	  state transition timeout update timeout expiration Logic subsystem Measurement  subsystem Figure 5: Metric module structure

tional blocks exporting a simple interface to update and
retrieve metric values associated to a given key. Even
though in principle such modules can be implemented
with any compact data structure compatible to our pro-
posed stream mode approach (and indeed the current
architecture support pluggable user deﬁned metric mod-
ules), several metrics of practical interest can be derived
from a basic structure depicted in Figure 5 and extend-
ing the construction proposed in [5]. It permits to count
(or time-average, see below) distinct values (called vari-
ations in [5]) associated to a same key; for example: the
number of distinct IP destinations contacted by a same
IP source, or the number of distinct DNS names asso-
ciated to a same IP address. The MH module is imple-
mented using Bloom ﬁlter extensions [9, 5, 6]. Process-
ing occurs in three stages: (i) extract from the packet3
two binary strings, Detector FlowKey (DFK) and Mon-
itor FlowKey (MFK), that will be used in to query the
subsequent ﬁlters; (ii) detect whether the DFK has al-
ready appeared in a past time window (Variation De-
tector ﬁlter, VD), and if this is the case, iii) account to
the MFK key some packet-related quantity (e.g. num-
ber of bytes, or simply add 1 if packet count is targeted)
to the third stage’s Variation Monitor (VM) ﬁlter. The
reader interested in the rationale behind such construc-
tion and in further design details may refer to [5]. Fi-
nally note that, as a special case, the MH module can
be conﬁgured to either i) provide an ordinary count,
by disabling the VD stage, or ii) perform a match for
checking whether some information associated to the
incoming packet is ﬁrst seen (in a given time window),
by disabling the VM stage.

Programming the Measurement subsystem -
StreaMon programmers can deploy two types of “count-
ing” data structures: (i) an ordinary Count Sketch that
sums a value associated to the MFK (CBF) or (ii) a
Time decaying Bloom Filter (TBF) [6] that performs a
smoothed (exponentially decaying) average of a value
associated to the MFK. At deployment time, an MH
module is conﬁgured by: (i) enabling/disabling the VD
or the VM; (ii) specifying the counting data structure
in the VM; (iii) setting the MH parameters, i.e., num-

3 Even if, for simplicity of explanation, we account such
an extraction to the MH module, for obvious performance
reasons we perform packet parsing and the consequent ex-
traction of the ﬂowkeys DFK and MFK once for all, for all
metrics, in the Event Layer. Indeed, diﬀerent metrics may
make usage of a same packet ﬁeld.

Figure 4: Excerpt of XML based StreaMon code showing: (i) met-
ric element allocation, (ii) feature compositions, (iii) an
event logic description. In particular this picture shows a
timeout event handler, described in terms of metric oper-
ations, feature extractions, conditions, actions and state
transition

scription does not require to be compiled by application
developers, but is run-time installed, thus signiﬁcantly
simplifying on-ﬁeld deployment. Figure 4 shows an ex-
cerpt of a StreaMon application code.

HW acceleration

StreaMon allows the seamless integration of HW ac-
celerated metrics, e.g. mandated by stringent perfor-
mance requirements. This remains transparent to the
application programmer, which can thus port the same
application from a SW based probe to a dedicated HW
platform with no changes in the application program.
Seamless HW integration is technically accomplished
by performing all metrics, parsing, and event match-
ing HW-accelerated computations in a front-end (in our
speciﬁc case, an FPGA), and by bringing the relevant
results up to the user plane through a HW/SW inter-
face, by appending the meta-data generated by the HW
tasks to the packet.

3.2 Measurement Subsystem

The StreaMon Measurement subsystem provides a
fast, computation/memory eﬃcient, set of n highly con-
ﬁgurable built-in modules that allows a programmer to
deploy and compute a wide range of traﬃc features in
stream mode.

Multi-Hash Metric module (MH) - From a high
level point of view, StreaMon metric modules are func-

4

Detector BF Variation Detector Variation Monitor Monitor BF (TBF, CBF, SC, DLEFT) Flowkey Extractor Flowkeys  (DFK, MFK) parser config 	  	  	  	  params packet + metadata Learning BF 	  	  	  	  DFK 	  	  MFK 	  	  Paper Description
[7]

[32]

[17]

[33]

[26]

[12]

[16]

Passive DNS anal. x mali-
cious domain detection
Traﬃc charact. via Com-
plex Network modeling
DNS analysis for fastﬂux
domain detection

Stateful VOIP attack de-
tection
DDOS detection through
ICMP and L4 analysis
SVM-based method for
P2P traﬃc classiﬁcation

Mail spammer detection
via Network level analysis

Brief description of reference features supported by StreaMon
(i) Short life and Access Ratio per domain; (ii) Multi-homing/multi-address per
domain; (iii) AVG, STD, total number, CDF of the TTL for a given domain
CDF, STD, Max/Min of: (i) total number of endpoints and correspondent host per
endpoint; (ii) bytes exchanged between two endpoints
Arithmetic functions of: (i) number of unique A records returned in all DNS lookups,
(ii) number of A records in a DNS response, (iii) number of name server (NS) records
in one single lookup, (iv) number of unique ASNs for each A record
Stateful rules: (i) RTP traﬃc after SIP BYE? (ii) source IP address changed in time
window? (iii) RTP sequence numbers evolve correctly?
Sent/received TCP packet ratio; Sent/received for diﬀerent ICMP messages; UDP
bitrate; No. diﬀerent UDP connections per destination; Connection packet count
Per distinct host and port: (i) ratio between TCP and UDP traﬃc; (ii) AVG traﬃc
speed; (iii) ratio between TCP and UDP AVG packet length; Per distinct IP: (i)
ARP request count, (ii) ratio between sent/received TCP, UDP packet, (iii) ratio
between {TCP, UDP} and total traﬃc; Per distinct port: traﬃc duration
(i) AVG/STD msg len in past 24h; (ii) AVG distance to 20 nearest IP neighbors of
the sender; (iii) AVG/STD of geodesic distance between sender and recipient; (iv)
Message body len; (v) AVG, STD and total no. diﬀerent recipients

Table 1

ber of hash functions, total memory allocated, swapping
threshold [5] or memory time window in the VD, TBF’s
smoothing parameter [6]; (iv) chain multiple MH mod-
ule (so as to update a metric only if a former one was up-
dated). At run time, the programmer may dynamically
change ﬂowkeys and updating quantities.
In particu-
lar, for each possible event and (if needed) ﬂow status
(i.e., XFSM entry), a list of Metric Operations (MOP)
is deﬁned, where a MOP is a set/get primitive that de-
ﬁnes the ﬂowkeys (as a packet ﬁelds combinations) and
quantities to be monitored/accounted. Finally, a set of
StreaMon metrics are combined into Features by simply
deﬁning mathematical functions Fi = fi(M ), where M
is a Metric vector.

What practical features can be supported? -
We believe that the above metrics fulﬁll the needs of a
non negligible fraction of real world monitoring appli-
cations. Indeed, [6] shows a concrete example of a real
world application reimplemented by just using modules
derived from our general MH module described above.
Indeed, limiting to the StreaMon’s measurement sub-
system (we will signiﬁcantly extend the framework in
the next section), table 1 shows features considered in
a number of works taken from a somewhat heteroge-
neous set (diﬀerent targets, operating at diﬀerent net-
work layers, diﬀerent classiﬁcation approaches) which,
according to our analysis, are readily deployed through
suitable conﬁguration of the above metrics/features. In-
deed, most applications either require to track/match
(1) features that are directly retrievable from a single
packet and do not have memory of past values (e.g.:
short life of a DNS domain, message body length, all
the features in [17] except the ﬁrst one); and/or (2) re-
quire counting or averaging over a various set of param-
eters (eventually uniquely accounted - e.g. variations),

which are readily instantiated with an MH module4;
and/or (3) require logical/mathematical combinations
of diﬀerent statistics, which is the goal of our Feature
Layer. Traﬃc features not covered by the families listed
above are those which require a stateful correlation of
diﬀerent ﬂows status. Such metrics (like the ones ex-
ploited in [33]) are supported by StreaMon but require
the stateful framework described in Section 3.3.

3.3 Logic subsystem

StreaMon’s Logic Subsystem is the result of the inter-
work between the Event Layer and the Decision Layer.
It provides the application developer with the ability to
customize the tracking logic associated to a monitored
entity subject to speciﬁc user-deﬁned conditions, so as
to provide a verdict and/or perform analyses beyond
those provided by the Monitoring Subsystem.

Event Layer - The Event Layer generates the events
triggering the StreaMon processing chain, identiﬁes the
monitored entity (primary key), and retrieves the spe-
ciﬁc event context (in particular the ﬂow status). This
layer is further composed of three functional compo-
nent:
(i) the capture engine responsible for “captur-
ing” the triggering event, either a “packet arrival” or a
“timeout expiration”;(ii) the timeout manager in charge
of keeping track of all registered timeouts and manage
their expiration; (iii) the status table, a (dleft hash) ta-
ble storing the status associated to the processed ﬂows
- the primary key status - and the so-called secondary

4 For instance, the number of bytes exchanged between two
hosts can be obtained by setting as VMK is the concate-
nation of source and destination IP address and as updat-
ing quantity the length ﬁeld of the packet; similarly, unique
counts such as the total number of distinct TTL for a given
DNS domain is obtained by setting as VDK the concatena-
tion of Domain Name and TTL in the DNS response and as
VMK is the Domain Name.

5

support table, a table storing states not associated to
the primary key but required by the application state
machine to take some decision.

The triggering event is associated to a primary key,
i.e. the monitored entity (ﬂow, host, etc) under inves-
tigation for which the monitoring application will pro-
vide a ﬁnal verdict, like “infected” or “legitimate”. For
intercepted packets, the primary key is a combination
of packet ﬁelds parsed by the (extensible) protocol dis-
sector implemented in this layer. For locally generated
timeout expiration events, the primary key is retrieved
from the timeout context. The primary key is used to
retrieve the current status of the ﬂow: no match in the
state table is considered to be a default state.

If the primary key is not directly retrievable from the
packet, and instead the ﬂow status is related to some
other ﬂow previously processed, a secondary support
table storing the references to the entries in the status
table is used. Such secondary support table is called re-
lated status table. For example, this can be the case of
an application keeping track of SIP URIs, (i.e.: the pri-
mary key is the SIP user ID in SIP INVITE messages)
and consider as suspicious all UDP packets related to a
data connection with a suspicious SIP user. In case of
UDP packets, neither of the packet ﬁelds can be used to
retrieve the ﬂow status. Instead, a secondary support
table is used to keep a reference between the socket 5-
tuple and the status entry of the associated SIP initiator
user. If the application does not take into account the
ﬂow status, the primary extraction is skipped.

Decision Layer - The Decision Layer is the last
processing unit of the StreaMon architecture and re-
ceives the event context carrying an indication of the
current ﬂow status and a set of traﬃc features con-
tainer. This layer keeps a list of Decision Entries (DEs)
deﬁned as the 3-tuple (enabling Condition (C), Out-
put actions (O), State Transition (ST)). For each trig-
gering event, and according to the current ﬂow status,
the decision layer veriﬁes the enabling conditions and
executes the actions and the state transition associ-
ated to the matched condition. Since secondary sup-
port tables can be updated, StreaMon support vari-
able conditions, i.e. in which the comparison operands
may change during time. The ﬁrst matched condition
will trigger the execution of a action set (like DROP,
ALERT, SET TIMEOUT etc..).

Programming the Logic Subsystem - Program-
mers describe an XFSM specifying (i) states and trig-
gering events; (ii) for each state, which metrics and fea-
tures are updated and which auxiliary information is
collected/processed (secondary table); (iii) which con-
ditions trigger a state transition and the associated ac-
tions.

To identify the triggering event, StreaMon keeps a
list of user-deﬁned “event descriptors”, expressed as a

Operators

Actions

Logic Subsystem built-in primitives
SUM, DIFF, DIV, MULT, MOD
EQ, NEQ, LT, GT, SQRT, LOG, POW
AND, OR, NOT, XOR
SET_TIMEOUT, UPDATE_TIMEOUT
SAVE_TIMEOUT_CTX, DROP, ALLOW, MARK
NEXT_STATUS, UPDATE_TABLE, PRINT, EXPORT

Table 2

3-tuple (type, descriptor, primary key). For example,
in case of intercepted packets, an event can be deﬁned
as: (packet; (udp.dport == 53); ip.src).

Moreover, programmers are given primitives to deﬁne
for each event and state: (i) a set of metric operations
(MOP) as described in section 3.2; (ii) a set of con-
ditions expressed as an arithmetic function of features
and secondary support table values. For each condi-
tion, programmers deﬁne a set of built-in actions and a
state transition. Table 2 summarizes the logic subsys-
tem supported condition operators and actions.

4. SIMPLE USE CASE EXAMPLES

We use the following simple examples to highlight
the ﬂexibility of StreaMon in supporting heterogeneous
features commonly found in real-world monitoring ap-
plications. The input data traces are obtained by prop-
erly merging a packet trace gathered from a regional
Internet provider with either (i) real malicious traﬃc
extracted from traces captured in our campus network
(use case 4.1 and 4.2) or (ii) synthetic traces properly
generated in our laboratories (use case 4.3).

4.1 P2P trafﬁc classiﬁcation

This example shows how straightforward is the im-
plementation of three transport layer traﬃc features de-
scribed in [20], for detecting peer to peer protocols that
use UDP and TCP connections. The (stateless) appli-
cation considers the following packet events (which will
ignore well known UDP and TCP ports.):
E1 : if (ip.proto == U DP )&&(udp.port (cid:54)= 25, 53, 110, ...)
E2 : if (ip.proto == T CP )&&(tcp.port (cid:54)= 25, 53, 110, ...)
This application extracts the following traﬃc features
F1 = M1&M2; F2 = |M3 − M4| where:
{M1, M2}: VD enabled - return 1 for each IP src/dst
pair which previously opened a {UDP, TCP} socket, 0
otherwise;
{M3, M4}: VD enabled and VM type CBF - count
the number of diﬀerent {TCP source ports, hosts} con-
nected to the same destination IP address.

Metrics are read/updated on the basis of the matched

event, as follows:

E1
set(M1, ip.src|ip.dst)
get(M2, ip.src|ip.dst)
get(M3, ip.dst)
get(M4, ip.dst)

E2
get(M1, ip.src|ip.dst);
set(M2, ip.src|ip.dst)
set(M3, tcp.sport|ip.dst, ip.dst)
set(M4, ip.src|ip.dst, ip.dst)
get(Mi, ip.src), i = 3, 4

6

Figure 6: Application XFSMs: (a) Conﬁcker use case; (b) DDOS use case

The application detects a p2p client if the follow-
ing condition holds after a transitory period: (F1 ==
1&&(F2 < 10)).

4.2 Conﬁcker botnet detection

Conﬁcker is one of the largest Botnets found in re-
cent years [29]. A multi-step detection algorithm can
attempt to track the two following (separated) phase:
(1) a bot tries to contact the C&C Server, and (2) a
single bot tries to contact and infect other hosts.

To contact the C&C Server, infected hosts perform
a huge number of DNS queries (with a high NXDo-
main error probability) to resolve randomly generated
domains.
In the infection phase, every host tries to
open a TCP connection to the ports 445 of random
IPs. Our Conﬁcker detector will use the following met-
rics (VD disabled and VM of type TBF ): number of
total DNS queries per host (M1), number of DNS NX-
Domain per host (M2) and number of TCP SYN and
SYNACK to/from port 445 (respectively M3 and M4).
These metrics are combined into the following features:
F1 = M2/M1, F2 = M4/M3.

For a DNS NXDomain response, the condition F1 >
0.25 is checked. If the condition is true, the state of the
actual ﬂow changes to alert and an event timeout is set.
In the alert state the application updates M3 and M4
and, when this timeout expires, these metrics are used
to compute F2 and to verify the related condition: if the
condition is true, then the host is considered infected
and goes to the next state, otherwise it returns to the
default state.

The application XFSM is graphically described (with
simpliﬁed syntax) in Figure 6.a. Figure 7 shows the
trend of features used in this conﬁguration, for an host
infected by Conﬁcker (A) and for a clean host (B). In
the ﬁrst case, the value of F1 is relatively high since
the beginning of the monitoring (due to the presence

Figure 7: Conﬁcker features temporal evolution

of reverse DNS queries, easily ﬁltered by the applica-
tion); the value increases when the host starts to per-
form Conﬁcker queries. The value of F2 instead is very
low, clearly denoting a port scan. Also for the host B
the presence of rDNS queries increases the value of F1
and this involve a change of state, and the application
starts to analyze TCP feature. However the F2 value
(nearly 100% of SYNACKs are received) reveals that
this is clearly not a network scan. Testing this conﬁgu-
ration in a 90-hours trace with 53 diﬀerent hosts in idle
state, we obtained 100% of detection (8 infected hosts
detected) without false positive or false negative.

4.3 DDOS detection and mitigation

In this section we sketch a simple algorithm which
can be used as an initial base for detecting and miti-
gating DDOS attacks. The algorithm is driven by the
number of SYN packets received by possible DDOS tar-
gets. The XFSM of this conﬁguration is depicted in 6.b,
and is governed by the following two events:
E1 : if (ip.proto == T CP )&&(tcp.f lags == SY N )
E2 : timeoutexpired

Metric M1 (VD=oﬀ, VM=TBF) tracks the number
of TCP SYN addressed to a same target in 60 seconds
(with 240s TBF’s memory). All external servers for

7

TRANSITION_EVENT metric_operations() [conditions] <transition_action()> (a) (b) INFECTED RECV_DNS_RESP update(M2, ip.dst) get(M1, ip.src) [if (F1 > .25) <set_timeout(10s)>  RECV_DNS_REQ update(M1, ip.src) get(M2, ip.src) DEFAULT RECV_SYNACK update(M4, ip.dst) RECV_SYN update(M3, ip.src) EXPIRED_TIMEOUT [if F2 > .5] <alert()> ALERT EXPIRED_TIMEOUT [if F2 <= .5] RECV_TCP_SYN update(M1, ip.dst) update(M2, ip.src|ip.dst) RECV_TCP_SYN update(M1, ip.dst) update(M2, ip.src|ip.dst) [if F1 < 100] EXPIRED_TIMEOUT [if F1 >= 1.2*F1_old] <set_timeout(10)> <update_table(ip.dst, F1)>  MONITORED RECV_ANY_FROM_IFACE1 [If F2 > 5] <DROP> RECV_TCP_SYN update(M1, ip.dst) EXPIRED_TIMEOUT [if F1 < 0.8*F1_old] <set_timeout(10)> <update_table(ip.dst, F1)> ATTACK EXPIRED_TIMEOUT [if F1 > 1.2*F1_old] <quarantine(ip.dst)> <set_timeout (10)> <update_table(ip.dst, F1)> NORMAL RECV_TCP_SYN update(M1, ip.dst) update(M2, ip.src|ip.dst) [if F1 >= 100] <set_timeout(10)> <update_table(ip.dst, F1)> EXPIRED_TIMEOUT [if F1 < 1.2*F1_old] <update_table(ip.dst, F1)> ALERT 0.0001	  0.001	  0.01	  0.1	  1	  0	  400	  800	  1200	  1600	  2000	  2400	  2800	  3200	  3600	  Features	  value	  Time	  [sec]	  F1	  A	  F2	  A	  F1	  B	  F2	  B	  Figure 8: DDOS temporal representation:

(i) DDOS target host
status (red curve), (ii) F1 value for the DDOS target
(blu curve) and (iii) F2 for two diﬀerent legitimate traﬃc
sources connecting to the DDOS target server

which F1 = M1 is under a given threshold (100 in this
example) are in default state (because they are obvi-
ously not under attack and thus do not need an ex-
plicit status). When F1 exceeds this threshold, the tar-
get goes in monitored state, a timeout is set and the
current value of F1 value is stored into a secondary
support table with key ip.dst. As soon as this time-
out expires, the diﬀerence between the current feature
value and the one stored is computed. If the condition
if (F1(t) > 1.2 ∗ F1(t − 1)), holds for two consecutive
times, i.e. the rate of TCP SYN has increased twice for
more than 20%, the ﬂow goes into an attack state.

Our use case example mimics this mitigation strat-
egy by considering legitimate all the source hosts that
have already shown meaningful activity and contacted
the server before the actual emergence of the attack,
and thus likely dropping most of the TCP SYN hav-
ing a spoofed source IP address.
In our use case ex-
ample, a second metric M2 tracks the TCP SYN rate,
smoothed over a chosen time window, at which which
each host contacts each destination IP addresses (M2 is
conﬁgured with VD disabled and VM of type TBF with
smoothing window 240s, and it is updated with MFK
(IPsrc, IPdst).

Figure 8 shows the temporal evolution of the state of
a server, in an experiment where we performed a DDoS
attack, with spoofed TCP SYN packets, after about
140s. The ﬁgure also reports the measured TCP SYN
rate, as well as the traﬃc generated by two hosts per-
forming regular queries towards the server: one starting
at time 0, and the other starting right in the middle of
the attack. When the actual attack is detected, the
mitigation strategy starts ﬁltering traﬃc. Thanks to
the second tracked metric, M2, the user starting ac-
tivities before the DDoS attack is not ﬁltered; on the
contrary, connections from sources not previously de-
tected via the M2 metric are blocked, as shown by the
green curve. New connections can be accepted as soon
the server leaves the attack state (see the F2 growth of

Figure 9: (a) percentage of printable character as function of the
percentage of packets (b) percentage of bit set to 1 as
function of the percentage of packets

the second host).

4.4 HW accelerated detection of non standard

encrypted trafﬁc

This example shows how HW metrics can be inte-
grated in StreaMon. Since one of the task performed
during deep packet inspection is the collection of statis-
tics on byte frequencies, oﬀsets for common byte-values,
packet information entropy, we illustrate an use case
that is a simpliﬁed version of the approach described in
[14], in which encrypted ﬂows are detected by combin-
ing two traﬃc features: (i) the bit information entropy
of a packet; (ii) the percentage of printable characters,
i.e. ASCII characters in the range [32 . . . 127]. Both
features are computationally demanding for a software
implementation, whereas they are best delegated to an
FPGA implementation (just 250 logic cells for our im-
plementation).

This use case implements a simple stateless Strea-
Mon application with the following characteristics: (i)
it considers as event the reception of a UDP or TCP
packet with length greater than 100 bytes; (ii) the pri-
mary key is the socket 5-tuple of the packet; (iii) it
makes use of two FPGA precomputed metrics: pop-
count (number of bit set to 1) M1 and printable chars
percentage M2 (which is directly mapped into a Strea-
Mon feature F1 = M1); (v) the packet entropy Feature
F2 is obtained as − 1
i=0 ni · (log(ni) − log(N ), where
N
N = len, n0 = N − M1, n1 = M1. An encrypted ﬂow is
detected if: (F2 < 0.75)&(−3σ < F 1 − HU (l) < 3σ)

(cid:80)1

where HU (l) is the entropy of a packet with uniform
distribution of 1 in the payload and length l and σ is
the standard deviation. Figure 9 shows M1 and M2
to the simple case of HTTP (unencrypted ﬂow) and
SSL (encrypted ﬂow) traﬃc, and as in [14] justify the
condition expressed above.

4.5 Port knocking

8

0,1	  1	  10	  100	  1000	  allowed pre dos allowed post dos SYN rate 0	  50	  100	  150	  200	  250	  300	  350	  400	  Time [s] server state a*ack	  alert	  monitored	  default	  Figure 10: Port Knocking use case XFSM

Figure 11: StreaMon prototype implementation architecture

This use case shows how a stateful ﬁrewall application
can be easily implemented with StreaMon. In particu-
lar, this use case implements a port knocking mecha-
nism to enable SSH access. The application’s XFSM is
depicted in ﬁgure 10. SSH access will be granted only
to those clients guessing the correct port sequence 5000,
4000, 7000 (a short sequence only for presentation con-
venience).

The application considers four events (TCP SYN re-
ceived respectively on port 5000, 4000, 7000, other ports)
and four states (default, 5000contacted, 4000contacted,
allowed). A state transition is triggered only if the client
(identiﬁed by the IP source address) contacts the ex-
pected port in the sequence. If after a state transition
the next expected port is not contacted in ﬁve seconds,
the ﬂow status is rolled back to default.

In addition, to avoid random port scanning for guess-
ing the correct sequence, a metric M1 (VD=ON, VM=TBF
with smoothing window 5s) counting the SYN rate is
used. If a port scan is detected (if (F1 = M1) > 40) the
ﬂow status is updated to ”attack”, the host is blocked for
2 minutes and an alert is generated. M1 is updated with
DF K = ip.src|ip.dst|tcp.dport and M F K = ip.src
when a SYN to an unexpected port is contacted.

5. PERFORMANCE EVALUATION

5.1

Implementation Overview

We experimented StreaMon on two platforms. A
SW-only deployment leverages oﬀ-the-shelf NIC drivers,
integrating the eﬃcient PFQ packet capturing kernel
module [8]. To test HW acceleration, we also deployed
StreaMon using a 10 gbps Combo FPGA card as packet
source. In what follows, unless otherwise speciﬁed (specif-
ically, section 5.2.3 which deals with HW accelerated
metrics implemented over the Combo FPGA card) we
focus on the SW-only conﬁguration.

StreaMon start-up sequence is summarized as follows.
The application program is given as input to a pre-
processor script as a XML formatted textual ﬁle. The
program is parsed, StreaMon process is conﬁgured and
executed while in parallel the interpreted feature and

9

condition expressions are transformed them into C++
code and build a ”on-demand DLL”5;

StreaMon implementation takes advantages from mul-
ticore platforms by implementing parallel processing chains
as shown in the implementation architecture depicted in
ﬁgure 11 (for the SW-only setup). The PFQ driver is
used as packet ﬁlter and conﬁgured with a number of
software queues equal to the number of cores. Traﬃc
ﬂows are dispatched from the physical queues of the
NIC by the PFQ steering function. For each PFQ, a
separate StreaMon chain is executed by a single thread
with CPU aﬃnity ﬁxed to one of the available core and
share the metric banks, status tables and timeout tables
(the concurrent access to the shared data is protected
by spinlocks).

The throughput measurement has been performed on
a Intel Xeon X5650 (2.67 GHz, 6 cores) Linux server
with, 16 GB ram and Intel 82599eb 10Gbit optical in-
terfaces.

5.2 Performance Analysis

5.2.1 Measurement subsystem performance

Figure 12 shows the processing throughput expressed
as percentage of the maximum throughput (6,468 Gbps)
obtained with one PFQ source block without the over-
head of StreaMon, expressed as function of the number
of metrics (1, 4, 8, 16, 24, 32) in case of (1, 2, 3, 6)
CPU core parallel processing. The input test data is
a small portion of a long trace captured within a local
internet provider infrastructure and its parameters are
summarized in the following table:

5 Note that this is an optimization step which is devised
to on-the-ﬂy compile (transparent to the programmer), the
application code, so as to avoid a ”feature/condition inter-
preter”, which would have lowered the overall performance
and would not have permitted line rate feature computation
and condition veriﬁcation. Since no recompilation of the
StreaMon code is ever needed, but user deﬁned features are
integrated as a DLL, dynamic deployment of user programs
is made possible.

default 4000contacted allowed TCP_SYN5000 <set_timeout(5)> TCP_SYN4000 <set_timeout(5)> TCP_SYN7000 <allow(from %ip.src to tcp.dport 22)> TIMEOUT_EXIPERED TCP_SYN update(M1) [if F1 > 30] <set_timeout(120)> <alert(%ip.src scanning)> TIMEOUT_EXIPERED TIMEOUT_EXIPERED TIMEOUT_EXIPERED 5000contacted attack 	  	  NIC driver PFQ driver Kernel incoming packets  . . .  Q1 Q2 QN Event source Event source Event source Shared Status Tables Metric/feature computation Metric/feature computation Metric/Feature computation Shared Metric Banks Chain thread N: affinity set tu CPU N Decision Decision Decision Shared Timeout Tables Chain thread 2: affinity set tu CPU 2 Chain thread 1: affinity set tu CPU 1 User-space StreaMon SW implementation Table 3 shows the details of the test traces used for
three among the use cases presented in Section 4 and
the their performance evaluation, where pkt. no is the
total number of packets, AVG plen is the average packet
length, M is the total number of metrics, FK is that the
transmission bitrate is ﬁxed by the injector probe and
depends on the average packet length.

As already underlined, the use case applications ex-
perience a better throughput with respect to the bitrate
obtained with the same number of metrics in the worst
case measurement showed in 12. Note that due to the
diﬀerent performances of the COMBO card processor
and the limitation to single core processing6, the en-
crypted ﬂow detection use case performance is omitted
to avoid confusion, as it would present results not di-
rectly comparable with the remaining ones.

5.2.3 FPGA accelerated primitives

As already discussed, our current StreaMon proto-
type supports seamless integration of HW precomputed
primitives, and in particular we have implemented packet
entropy computation (see section 4.4) and event parsing
on a INVEA-TECH Combo FPGA card [1]. The inter-
face between the Combo card and StreaMon core soft-
ware is realized via a footer appended to the ethernet
frame. The Combo card capture the packet traveling
in the network under inspection, performs the required
operation and transfer the resulting metrics and signals
as a list of Type Len Value (TLV) to the PC hosting the
capture card. This list of TLV is parsed by the Event
Layer and will be available to the remaining layers.

Our next step is to implement a full deep packet in-
spection module. To conﬁrm its necessity, we have so far
implemented a simpliﬁed content matching packet pay-
load inspection primitive in both SW and HW. Figure
13 compares the two implementations for a toy exam-
ple of just 32 content strings, showing the severe perfor-
mance degradation of the SW implementation even in
such small scale example. At the same time, the ﬁgure
shows that, by oﬄoading content matching primitives
to the HW front-end, our prototype achieves almost op-
timal throughput performance.

6. RELATED WORK

In the literature, several monitoring platforms have
targeted monitoring applications’ programmability. A
Monitoring API for programmable HW network adapters
is proposed in [31]. On top of such probe, network ad-
ministrators may implement custom C++ monitoring
applications. One of the developed applications is App-
mon [4]. It uses deep packet inspection to classify ob-
served ﬂows and attribute these ﬂows to an application.

6This is due the fact that the COMBO card capture driver
does not allow to open multiple instances of the same com-
munication channel toward the FPGA NIC

Figure 12: System throughput evaluation

pkt no. AVG pkt len Host no. TX rate
6.47 Gbps
6320928

632.82 bytes

31827

where pkt len and TX are the average packet length

and replayed transmission bitrate respectively.

As expected, the throughput decreases with the num-
ber of metrics and grows with the number of cores (even
though the growth is lower than expected due to the
simple thread concurrency management adopted in this
prototype). It is important to underline that such graph
shows the worst metric computational scenario in which:
(i) all metrics are sequentially updated and retrieved
for each packet (single event and stateless logic) and
are conﬁgured with both the VD (a BF pair) and the
MV (DLEFT) enabled; (ii) all ﬂow keys are diﬀerent
(n metrics, n * 2 ﬂow keys). Nevertheless, the results
are promising, as for example in case of 16 metrics, for
which we observe the following average bitrate (Gbps):

1 core
0.566

2 cores
0.983

3 cores
1.367

6 cores
2.315

Without discussing here the advantages of a state-
ful pre-ﬁltering (which will be shown in section 5.2.2),
we want to underline that, in the typical case in which
not all the metrics are updated for each packet and re-
quire distinct ﬂow keys, we can experience a better rela-
tion between the processing throughput and the number
of metrics. For this reason, we compared the average
throughput obtained by executing the traﬃc classiﬁ-
cation algorithm described in [32] (the most compu-
tational demanding algorithm cited in 1, in terms of
number of distinct metrics and ﬂow keys, and portion
of processed traﬃc). The following table shows the av-
erage bitrate (in Gbps) obtained by [32] in case of TCP
traﬃc classiﬁcation ([32]: 6 metrics, 3 ﬂowkeys) and
the average bitrate of the worst case conﬁguration (6
metrics and 12 ﬂow keys).

[32]
worst case

1 core
2.792
1.243

2 cores
4.316
2.812

3 cores
5.199
2.008

5.2.2 Use case performance evaluation

10

0%	  10%	  20%	  30%	  40%	  50%	  60%	  70%	  80%	  90%	  100%	  0	  4	  8	  12	  16	  20	  24	  28	  32	  Throughput [%] Metric Number case
4.1
4.2
4.3

pkts.
7676958
11852112
9387240

AVG plen [bytes] Ms, FKs. TX [Gbps] RX [# core: Gbps]

404.97
329.77
215.38

8, 4
4, 2
2, 2

3.658
2.462
2.067

1: 1.956, 2: 2.954, 3: 3.654
1: 2.462
1: 2.003, 2: 2.067

Table 3: Use case trace parameters and throughput

users to deﬁne new traﬃc metrics by simply performing
queries to the DSMS. The DSMS is conﬁgured through
an XML ﬁle that is processed to obtain a C++ applica-
tion code. Gigascope [10] is another stream database for
network monitoring that provide an architecture pro-
grammable via SQL-like queries.

Opensketch [34] is a recent work proposing an eﬃ-
cient generic data plane based on programmable metric
sketches. If on the one hand we share with Opensketch
the same measurement approach, on the other hand its
data plane abstraction delegates any decision stage and
logic adaptation the control plane and, with reference
to our proposed abstraction, does not go beyond the
functionalities of our proposed Measurement Subsys-
tem. On the same line, ProgME [35] is a programmable
measurement framework which revolves around the ex-
tended and more scalable notion of dynamic ﬂowset
composition, for which it provides a novel functional
language.

Even though equivalent dynamic tracking strategies
might be deployed over Openﬂow based monitoring tools,
by exploiting multiple tables, metadata and by dele-
gating ”monitoring intelligence” to external controllers,
this approach would require to fully develop the speciﬁc
application logic and to forward all packets to an exter-
nal controller, (like in Openﬂow based monitoring tool
Fresco [28]), which will increase complexity and aﬀect
performance.

Finally, while our work is, to the best of our knowl-
edge, the ﬁrst which exploits eXtended Finite State
Machines (XFSM) for programming custom monitor-
ing logic, we acknowledge that the idea of using XFSM
as programming language for networking purposes was
proposed in a completely diﬀerent ﬁeld (wireless MAC
protocols programmability) by [30].

7. CONCLUSION

The StreaMon programmable monitoring framework
described in this paper aims at making the deployment
of monitoring applications as fast an easy as conﬁguring
a set of pre-established metrics and devising a state ma-
chine which orchestrates their operation while following
the evolution of attacks and anomalies.
Indeed, the
major contribution of the paper is the design of a prag-
matic application programming interface for developing
stream-based monitoring tasks, which does not require
programmers to access the monitoring device internals.
Despite their simplicity, we believe that the wide range

Figure 13: FPGA accelerated and SW string matching comparison

Flow states are stored in an hash table and retrieved
when an old ﬂow is observed again. This way to handle
states bears some resemblance with that proposed in
this work, which however makes usage of (much) more
descriptive eXtended Finite State Machines. CoMo [19]
is another well known network monitoring platform. We
share with CoMo the (for us, side) idea of extensible
plug-in metric modules, but besides this we are quite
orthogonal to such work, as we rather focus on how to
combine metrics with features and states using higher
level programming techniques (versus CoMo’s low level
queries).

Bro [27] provides a monitoring framework relying on
event-based programming language for real time statis-
tics and notiﬁcation. Despite the attempt to deﬁne a
versatile high level language, Bro is not designed to ex-
pose a clear and simple abstraction for monitoring ap-
plication development and leave full programmability to
its users (which we believe results in a more descriptive
and yet more complex programming language).

The Real-Time Communications Monitoring (RTC-
Mon) framework [15] permits development of monitor-
ing applications, but again the development language
is a low level one (C++), and (unlike us) any feature
extraction and state handling must be dealt with inside
the custom application logic developed by the program-
mer. CoralReef [21], FLAME [3] and Blockmon [18]
are other frameworks which grant full programmability
by permitting the monitoring application developers to
“hook” their custom C/C++/Perl traﬃc analysis func-
tion to the platform. On a diﬀerent line, a number
of monitoring frameworks are based on suitable exten-
sions of Data Stream Management Systems (DSMS).
PaQueT [23], and more recently BackStreamDB [24],
are programmable monitoring frameworks developed as
an extension of the Borealis DSMS [2]. Ease of pro-
gramming and high ﬂexibility is provided by permitting

11

of features accounted in the proposed use cases suggest
that StreaMon’s ﬂexibility can be exploited to develop
and deploy several real world applications.

8. REFERENCES
[1] COMBO product brief. http://www.invea-
tech.com/data/combo/combo pb en.pdf.

[2] D. J. Abadi, Y. Ahmad, M. Balazinska, M. Cherniack,
J. hyon Hwang, W. Lindner, A. S. Maskey, E. Rasin,
E. Ryvkina, N. Tatbul, Y. Xing, and S. Zdonik. The
design of the borealis stream processing engine. In In
CIDR, pages 277–289, 2005.

[3] K. G. Anagnostakis, M. Greenwald, S. Ioannidis, and

S. Miltchev. Open packet monitoring on ﬂame: Safety,
performance, and applications. In 4th Int. Conf. on
Active Networks, IWAN ’02, 2002.

[4] D. Antoniades, M. Polychronakis, S. Antonatos, E. P.
Markatos, S. Ubik, and A. Oslebo. Appmon: An
application for accurate per application network traﬃc
characterisation. In IST Broadband Europe, 2006.
[5] G. Bianchi, E. Boschi, S. Teoﬁli, and B. Trammell.
Measurement data reduction through variation rate
metering. In INFOCOM, pages 2187–2195, 2010.

[6] G. Bianchi, N. d’Heureuse, and S. Niccolini.
On-demand time-decaying bloom ﬁlters for
telemarketer detection. ACM SIGCOMM Comput.
Commun. Rev., 41(5):5–12, 2011.

[7] L. Bilge, E. Kirda, C. Kruegel, and M. Balduzzi.
EXPOSURE : Finding malicious domains using
passive DNS analysis. In NDSS 2011, 2011.

[8] N. Bonelli, A. Di Pietro, S. Giordano, and G. Procissi.
On multi-gigabit packet capturing with multi-core
commodity hardware. In Passive and Active
Measurement (PAM), pages 64–73, 2012.
[9] A. Broder and M. Mitzenmacher. Network

applications of bloom ﬁlters: A survey. In Internet
Mathematics, pages 636–646, 2002.

[10] C. Cranor, T. Johnson, and O. Spataschek. Gigascope:

a stream database for network applications. In
SIGMOD, pages 647–651, 2003.

[11] A. Dainotti, A. Pescape, and K. Claﬀy. Issues and
future directions in traﬃc classiﬁcation. IEEE
Network, 26(1):35–40, 2012.

[12] H. Deng, A. M. Yang, and Y. D. Liu. P2p traﬃc

classiﬁcation method based on svm. Comp. Eng. and
Applications, 44(14):122, 2008.

[13] L. Deri, F. Fusco, and J. Gasparakis. Towards
monitoring programmability in future internet:
Challenges and solutions. In Trustworthy Internet,
pages 249–259. Springer, 2011.

[14] P. Dorﬁnger, G. Panholzer, B. Trammell, and T. Pepe.
Entropy-based traﬃc ﬁltering to support real-time
skype detection. In Proc. 6th Int. Wireless Commun.
and Mobile Computing Conf. (IWCMC ’10), 2010.
[15] F. Fusco, F. Huici, L. Deri, S. Niccolini, and T. Ewald.

Enabling high-speed and extensible real-time
communications monitoring. In 11th IFIP/IEEE
Integrated Network Management Symp., IM’09, 2009.

[16] S. Hao, N. A. Syed, N. Feamster, A. G. Gray, and
S. Krasser. Detecting spammers with snare:
Spatio-temporal network-level automatic reputation
engine. In USENIX Sec. Symp., pages 101–118, 2009.

[17] T. Holz, C. Gorecki, K. Rieck, and F. C. Freiling.

Measuring and detecting fast-ﬂux service networks. In
NDSS, 2008.

[18] F. Huici, A. di Pietro, B. Trammell, J. M.

12

Gomez Hidalgo, D. Martinez Ruiz, and N. d’Heureuse.
Blockmon: a high-performance composable network
traﬃc measurement system. In Proc. ACM
SIGCOMM 2012, Demo, pages 79–80, 2012.
[19] G. Iannaccone, C. Diot, D. McAuley, A. Moore,

I. Pratt, and L. Rizzo. The como white paper. 2004.

[20] T. Karagiannis, A. Broido, M. Faloutsos, and

K. Claﬀy. Transport layer identiﬁcation of p2p traﬃc.
In 4th ACM Internet Measurement Conference (IMC
’04), pages 121–134, 2004.

[21] K. Keys, D. Moore, R. Koga, E. Lagache, M. Tesch,
and K. Claﬀy. The architecture of coralreef: an
internet traﬃc monitoring software suite. In Passive
and Active Measurements (PAM), 2001.
[22] F. Leder and T. Werner. Know Your Enemy:

Containing Conﬁcker, To Tame a Malware. Technical
report, The Honeynet Project, Apr. 2009.

[23] N. Ligocki and C. H. C. Lyra. A ﬂexible network

monitoring tool based on a data stream management
system. In ISCC 2008, pages 800–805, 2008.

[24] C. Lyra, C. Hara, and J. Duarte, EliasP.

Backstreamdb: A distributed system for backbone
traﬃc monitoring providing arbitrary measurements in
real-time. In Passive and Active Measurement (PAM),
pages 42–52, 2012.

[25] N. McKeown, T. Anderson, H. Balakrishnan,

G. Parulkar, L. Peterson, J. Rexford, S. Shenker, and
J. Turner. Openﬂow: enabling innovation in campus
networks. SIGCOMM Comput. Commun. Rev.,
38(2):69–74, Mar. 2008.

[26] J. Mirkovic, G. Prier, and P. L. Reiher. Attacking
ddos at the source. In ICNP, pages 312–321, 2002.

[27] V. Paxson. Bro: a system for detecting network
intruders in real-time. Computer networks,
31(23):2435–2463, 1999.

[28] S. Shin, P. Porras, V. Yegneswaran, M. Fong, G. Gu,
and M. Tyson. Fresco: Modular composable security
services for software-deﬁned networks. In ISOC NDSS,
2013.

[29] B. Stone-gross, M. Cova, L. Cavallaro, B. Gilbert,
M. Szydlowski, R. Kemmerer, C. Kruegel, and
G. Vigna. Your botnet is my botnet: Analysis of a
botnet takeover, 2009.

[30] I. Tinnirello, G. Bianchi, P. Gallo, D. Garlisi,

F. Giuliano, and F. Gringoli. Wireless mac processors:
Programming mac protocols on commodity hardware.
In INFOCOM, pages 1269–1277, 2012.

[31] P. Trimintzios, M. Polychronakis, A. Papadogiannakis,

M. Foukarakis, E. P. Markatos, and A. Oslebo.
Dimapi: An application programming interface for
distributed network monitoring. In 10th IEEE/IFIP
NOMS, 2006.

[32] X. Wu, K. Yu, and X. Wang. On the growth of
internet application ﬂows: A complex network
perspective. In INFOCOM, 2011.

[33] Y.-S. Wu, S. Bagchi, S. Garg, N. Singh, and T. K.

Tsai. Scidive: A stateful and cross protocol intrusion
detection architecture for voice-over-ip environments.
In DSN, pages 433–442, 2004.

[34] M. Yu, L. Jose, and R. Miao. Software deﬁned traﬃc

measurement with opensketch. In 10th USENIX NSDI
2013, pages 29–42.

[35] L. Yuan, C.-N. Chuah, and P. Mohapatra. Progme:
towards programmable network measurement.
IEEE/ACM Trans. Netw., 19(1):115–128, 2011.

