0
2
0
2

r
p
A
4
1

]

R
C
.
s
c
[

1
v
2
7
6
6
0
.
4
0
0
2
:
v
i
X
r
a

Fidelity of Statistical Reporting in 10 Years of
Cyber Security User Studies (Technical Report)(cid:63)

Thomas Groß

Newcastle University, Newcastle upon Tyne, UK

Abstract. Background. Studies in socio-technical aspects of security
often rely on user studies and statistical inferences on investigated rela-
tions to make their case. They, thereby, enable practitioners and scien-
tists alike to judge on the validity and reliability of the research under-
taken.
Aim. To ascertain this capacity, we investigated the reporting ﬁdelity of
security user studies.
Method. Based on a systematic literature review of 114 user studies
in cyber security from selected venues in the 10 years 2006–2016, we
evaluated ﬁdelity of the reporting of 1775 statistical inferences using
the R package statcheck. We conducted a systematic classiﬁcation of in-
complete reporting, reporting inconsistencies and decision errors, leading
to multinomial logistic regression (MLR) on the impact of publication
venue/year as well as a comparison to a compatible ﬁeld of psychology.
Results. We found that half the cyber security user studies considered
reported incomplete results, in stark diﬀerence to comparable results
in a ﬁeld of psychology. Our MLR on analysis outcomes yielded a slight
increase of likelihood of incomplete tests over time, while SOUPS yielded
a few percent greater likelihood to report statistics correctly than other
venues.
Conclusions. In this study, we oﬀer the ﬁrst fully quantitative analysis
of the state-of-play of socio-technical studies in security. While we high-
light the impact and prevalence of incomplete reporting, we also oﬀer
ﬁne-grained diagnostics and recommendations on how to respond to the
situation.

Keywords: User studies · SLR · Cyber security · Statistical reporting

1

Introduction

Statistical inference is the predominant method to ascertain that eﬀects observed
in socio-technical aspects of security are no mere random ﬂukes, but considered
to be “the real McCoy.”

(cid:63) Preregistered at the Open Science Framework: osf.io/549qn/. The short version of
this paper is appearing in the Proceedings of the 9th Workshop on Socio-Technical
Aspects in Security (STAST 2019), pp. 1–24, LNCS, Springer Verlag (2020).

 
 
 
 
 
 
In general, statistical inference sets out to evaluate a statistical hypothesis
stated a priori. It employs observations made in studies to establish the likeli-
hood as extreme as or more extreme than the observations made, assuming the
statistical hypothesis not to be true. This likelihood is colloquially referred to as
a p-value. Alternatively to Null Hypothesis Signiﬁcance Testing (NHST)—and
often used complementarily—studies may estimate the magnitude of eﬀects in
reality and conﬁdence intervals thereon [6]. Due to the inherent variability of
the behavior of human subjects these methods invariably come into focus in
quantitative studies of human-factor or social dimensions.

The onus of proof is generally on the authors of a study. There are numerous
factors inﬂuencing whether a study’s results can be trusted—a) sound research
questions and hypotheses, b) vetted and reliable constructs and instruments,
c) documentation favoring reproducibility, d) sound experiment design, yield-
ing internal and external validity, e) randomization and blinding, f) systematic
structured and standardized reporting—in the end, it is the outcomes of the
statistical inference that often render a ﬁnal verdict.

These outcomes do not only indicate whether an eﬀect is likely present in re-
ality or not. They also yield what magnitude the eﬀect is estimated at. Thereby,
they are the raw ingredient for (i) establishing whether an eﬀect is practically
relevant, (ii) evaluating its potential for reuse, and (iii) including it further quan-
titative research synthesis.

Ultimately, one would hope that individual studies are prepared to advance
the knowledge of a ﬁeld with a high degree of certainty. One would hope that
a ﬁeld is prepared to reinforce robust and reliable research; that it engages in
self-correction if studies are missing the mark.

While there have been a number of publications in socio-technical aspects
of security oﬀering guidance to the community to that end [21,16,25,4,2] as
well as proposals in other communities [1,17,15], the evidence of the state-of-
play of the ﬁeld has been largely anecdotal [25] or in human-coded analysis [3].
While this ﬁeld is arguably quite young, we argue that it would beneﬁt greatly
from attention to statistical reporting, from attaining fault tolerance through
reporting ﬁdelity and from preparing for research synthesis. (cf. Section 2.1)

In this study, we aim at systematically evaluating the ﬁdelity of statistical
reporting in socio-technical aspects of security. We analyze (i) whether statistical
inferences are fault-tolerant, in the sense of their internal consistency being pub-
licly veriﬁable, and (ii) whether the reported p-values are correct. Through the
semi-automated empirical analysis of 114 publications in the ﬁeld from 2006–
2016, we oﬀer a wealth of information including meta-aspectssuch as the use
of Amazon Mechanical Turk (AMT), multiple-comparison corrections, and es-
timation methods. We compare statistical reporting ﬁdelity of this ﬁeld with a
related ﬁeld of psychology as well as analyze the trajectory of the ﬁeld, that is,
the trends found over time. We substantiate the these results with qualitative
coding of errors observed to elucidate what to watch out for.

Contributions. We are the ﬁrst to subject our own ﬁeld to a systematic empirical
analysis of statistical reporting ﬁdelity. In that, we oﬀer a well-founded intro-

spection in the ﬁeld of socio-technical aspects of security that can serve program
committees and authors alike to inform their research practice.

2 Background

2.1 Importance and Impact of Statistical Reporting

Null Hypothesis Signiﬁcance Testing (NHST) establishes statistical inference by
stating a priori statistical hypotheses, which are then tested based on observa-
tions made in studies. Such statistical inference results in a p-value, which gives
the conditional probability of ﬁnding data as extreme as or more extreme than
the observations made, assuming the null hypothesis being true. Many ﬁelds
combine NHST with point and interval estimation, that is, establishing an esti-
mate of the magnitude of the eﬀect in the population and the conﬁdence interval
thereon.

Reporting Fidelity and Fault Tolerance. Diﬀerent reporting practices yield
diﬀerent degrees of information and ﬁdelity. It goes without saying that a simple
comparison with the signiﬁcance level α, e.g., by stating that p < .05, yields the
least information and the least ﬁdelity. Reporting the actual p-value observed
oﬀers more information as well as a means to quantify the likelihood of the
eﬀect.1

To gain further reporting ﬁdelity and fault tolerance, one would not only
report the exact p-value, but also the chosen test parameters (e.g., independent-
samples or one-tailed), the test statistic itself (e.g., the t-value) and the degrees of
freedom (df ) of the test. We, then, obtain a consistent triplet (test statistic, df ,
p-value) along with the test parameters. Table 1 exempliﬁes degrees of ﬁdelity.
The upshot of a diligent reporting procedure including full triplets is that it
enables cross-checks on their internal consistency and, thereby, a degree of fault
tolerance. Vice versa, if only the p-value or a comparison with a signiﬁcance level
is reported, the capacity to validate inferences is impaired.

Impact on Research Synthesis. Published studies usually do not stand on
their own. To learn what relations are actually true in reality and to what degree,
we commonly need to synthesize the results of multiple studies investigating the
same relations. More mature ﬁelds (such as evidence-based medicine or psychol-
ogy) engage in systematic reviews and meta analyses to that end.

For these down-stream analyses to be viable, the original studies need to
contain suﬃcient data for subsequent meta-analyses. If the original studies omit
the actual test statistics and degrees of freedom, the synthesis in meta analyses
is hamstringed or rendered impossible altogether.

1 We note that the p-value itself does not state the likelihood that a positively reported
result is actually true in reality (inverse fallacy) and refer interested readers to
literature on the Positive Predicted Value (PPV) [13].

Table 1. Degrees of ﬁdelity in statistical
two-
tailed independent-samples t-test on a relation with a large eﬀect size (ES).
Note:

= can be estimated

reporting for

= impossible

= supported

same

the

(cid:35)

(cid:72)(cid:35)

Incomplete Triplet

(cid:32)
Complete Triplet

Sig.

p-Value ES Inferable

ES Explicit

Example

p < .05

p = .019

t(24) = 2.52 , t(24) = 2.52, p = .019,

p = .019

Hedges’ g = 0.96,
CI [0.14, 1.76]

p Quantiﬁable
Cross-Checkable
ES Quantiﬁable
Synthesizable

(cid:35)
(cid:35)
(cid:35)
(cid:35)

(cid:32)
(cid:35)
(cid:35)
(cid:35)

(cid:32)
(cid:32)
(cid:72)(cid:35)
(cid:72)(cid:35)

(cid:32)
(cid:32)
(cid:32)
(cid:32)

2.2 Reporting and Methodology Guidelines

Reporting ﬁdelity is usually one of the goals of reporting standards. Given that
the ﬁeld of socio-technical research in cyber security is a young and does not
have its own established reporting standards, it is worthwhile to consider ones
of other ﬁelds. Psychology seems a sound candidate to consider as a guiding
example in this study. Other ﬁelds, such as behavioral economics, are equally
viable.

The publication guidelines of the American Psychology Association (APA) [1]
require that inferences are reported with their full test statistics and degrees of
freedom. Exact p-values are preferred. The APA guidelines require to report
appropriate eﬀect sizes and their conﬁdence intervals. Beyond the publication
manual itself, we draw attention to the summary of a related working group [22].
Fidler [9] oﬀers a quick summary of important revisions with an eye on passing
them on.

Of course, there are also methodological guidelines that go far beyond report-
ing statistical tests. For instance, the CONSORT guidelines [17] cover report-
ing for randomized trials; the PRISMA statement [18] covers systematic review
and meta analyses. Furthermore, recently LeBel et al. [15] proposed a uniﬁed
framework to quantify and support credibility of scientiﬁc ﬁndings, drawing on
the four areas: (i) method and data transparency, (ii) analytic reproducibility,
(iii) analytic robustness, and (iv) eﬀect replicability .

Even though socio-technical aspects of security is a young ﬁeld, there have
been initiatives to advance research methodology, considered in chronological or-
der: (i) In 2007, Peisert and Bishop [21] oﬀered a short position paper scientiﬁc
design of security experiments. (ii) Maxion [16] focused on making experiments
dependable, focusing on the hallmarks of good experiments with an eye on va-
lidity. (iii) In 2013, Schechter [25] considered common pitfalls seen in SOUPS
submissions and made recommendation on avoiding them, incl. statistical report-
ing and multiple-comparison corrections. (iv) Coopamootoo and Groß proposed
an introduction for evidence-based methods [4], incl. sound statistical inference

and structured reporting. (v) The same authors published an experiment design
and reporting toolset [2], considering nine areas with reporting considerations,
incl. test statistics and eﬀect sizes.

Notably, the SLR used as sample in this study covers the same years (2006-
2016) that have seen these diﬀerent guidelines being proposed by members of
the community.

2.3 Analysis of Statistical Reporting

We analyze statistical reporting of publications with the R package statcheck [8].
The statcheck tool extracts Strings of the form ts(df ) = x, p op y, where ts is the
test statistic, df the degrees of freedom, and op a inﬁx relation, such as, <. It
recognizes t, F , r, χ2, and z as test statistics and recomputes the corresponding
p-values from them. It, hence, enables a consistency check of reported triplets of
test statistic, degrees of freedom and p-values.

In this analysis, statcheck recognizes one-tailed tests to some extent from
searching keywords and computing if a test were valid if considered one-tailed.
It adheres to the rounding guidelines of the American Psychology Association
(APA) [1]. Nuijten et al. [19] concede that statcheck does not recognize p-values
adjusted for multiple-comparison corrections.

While the creators of statcheck have argued for its validity and reliabil-
ity [19,20], the tool faced scrutiny and controversy [26] over its false positive
and false negative rates. Schmidt [26], for example, criticized that statcheck’s
inability to recognize corrected p-values, such as from Greenhouse-Geisser cor-
rections. Lakens [14] found reported errors typically to be minor. However, we
ﬁnd that multiple-comparison, variability or degrees-of-freedom corrections are
rarely applied in the ﬁeld of cyber security.

While we perceive statcheck as a useful tool to evaluate statistical reporting,

we perceive it as crucial to cross-check results manually.

For this study, we prepare to mitigate possible statcheck mis-classiﬁcations

by manually checking and coding its outcomes.

2.4 Related Works

In 2016/17 Coopamootoo and Groß [3] conducted a Systematic Literature Re-
view (SLR) on cyber security user studies published in the years between 2006–
2016. This research was ﬁrst presented at a 2017 community meeting of the
UK Research Institute in the Science of Cyber Security (RISCS). Their study
contained three parts: (i) the SLR itself, yielding a sample of 146 cyber secu-
rity papers, (ii) a qualitative coding of nine “completeness indicators,” based
on an a priori codebook [5]. (iii) a quantitative analysis on a sub-sample using
parametric tests on diﬀerences between means (e.g., t-tests).

While this study uses the same set of papers as a sample to enable a com-
parison of results, this study takes an entirely diﬀerent approach to the analysis:
Firstly, instead manually coding completeness indicators, which is invariably

largely based on human judgment whether statistical reporting is appropriate,
we focus on an automated approach extract p-values and respective test statis-
tics. To a very large extent equally automated, we ascertain whether the the
statistical reporting is internally consistent.

Secondly, while also conduct systematic coding, it is in tandem with the
automated analysis and includes a level of detail of individual statistical tests.
We, thereby, obtain a more ﬁne-grained understanding of “things going wrong”
as well as the magnitude of the deviation from a correct result. We also introduce
a feedback-loop in the coding that enables the error-correction of faults of the
tool.

Thirdly, whereas Coopamootoo and Groß [3] focused on the quantitative
analysis of a small sub-sample of tests and their observed (post-hoc) eﬀect sizes,
we focus our quantitative lens on inconsistencies and decision errors of all sta-
tistical tests conducted in the sample.

3 Aims

3.1 Outcome Deﬁnition.

We deﬁne the classes of statcheck outcomes for test statistics and papers.

Deﬁnition 1 (SC Outcome Categories).
Individual Tests: SCOutcome has the following cases for individual tests:
1. CorrectNHST: The NHST is reported with its test statistic triplet. The given
triplet is correct, where “correct” is deﬁned as matching triplet of test statis-
tic, degrees of freedom and corresponding re-computed p-value.

2. Inconsistency: The reported triplet (test statistic, df , p-value) is inconsistent.
3. DecisionError: The reported triplet (test statistic, df , p-value) is grossly in-
consistent, that is, the re-computed p-value leads to a diﬀerent decision on
rejecting the null hypothesis.

4. Incomplete: A p-values is reported without suﬃcient data for an evaluation

of the triplet (test statistic, df , p-value).

Entire Papers: SCOutcome has the following cases for aggregated over papers:
1. CorrectNHST: There exist one or more NHSTs reported with correct test
statistic triplets. The given complete triplets are correct throughout, where
“correct” is deﬁned as matching triplet of test statistic, degrees of freedom
and corresponding re-computed p-value. A paper can be classiﬁed as Cor-
rectNHST even if there exist incomplete test statistics.

2. Inconsistency: There exists an inconsistent triplet (test statistic, df , p-value).
3. DecisionError: There exists a gross inconsistency in any reported triplet (test
statistic, df , p-value), in which a re-computed p-value leads to a diﬀerent
decision on rejecting the null hypothesis.

4. Incomplete: For all p-values reported, it holds that there is insuﬃcient data
for a correct triplet (test statistic, df , p-value). For a paper classiﬁed as
Incomplete, there is not a single p-value with complete test statistic found.

We call Complete the complement of Incomplete.

This scale is conservative against false positives in that it deems it acceptable
if some p-values are reported without their test statistics and only classiﬁes an
entire paper as Incomplete if all p-values are reported without test statistics.

3.2 Descriptives.

We will analyze and visualize the prevalence of statistical misreporting along the
following lines.

RQ 1 (Prevalence) How many papers report on Null Hypothesis Signiﬁcance
Testing (NHST) and fall into one of the deﬁned SC outcome categories according
to Def. 1 1. CorrectNHST, 2. Inconsistency, 3. DecisionError, 4. Incomplete. Which
papers use 1. MTurk, 2. multiple-comparison corrections (MCC), 3. eﬀect sizes.

3.3 Comparison to Other Fields.

We intend to compare the statcheck results in this ﬁeld with analyses that
have been conducted in other ﬁelds that seem related. We are most interested
in ﬁelds at the intersection of human behavior and technology, such as HCI.
Granted that statcheck surveys have not been that widely conducted yet, we
consider the Journal of Media Psychology (JMP) [7] as a primary candidate.
This choice is made because of similarities

(i) media psychology is concerned with human subjects and socio-technical

aspects,

(ii) media psychology includes topics that might also have been published in
user studies in cyber security, such as adversarial behavior (e.g., violence)
vis-`a-vis of HCI, cyber bullying, behavior on social media,

(iii) media psychology is a relatively young ﬁeld, JMP having been founded in

1989 and gained its current name 2008.

The distinct diﬀerence we are interested in is that JMP is subject to reporting
standards (APA). We note that the selection of JMP as comparison sample may
be controversial and that—at the same time—comparisons to further ﬁelds are
easily done, yet out of the scope for this study.

RQ 2 (Comparison) To what extent do the statcheck SCOutcomes diﬀer be-
tween our sample in this ﬁeld and a comparable ﬁeld in psychology?
HC,0: The distribution of the SCOutcomes in cyber security user studies is the
same as the distribution in the comparison ﬁeld. HC,1: There is a systematic
diﬀerence of SCOutcome in cyber security user studies to the comparison ﬁeld.

3.4 Statistical Model on Venue&Year.

We establish a statistical model on in a correlational study on the question:

RQ 3 (Inﬂuence of Venue and Year) Considering outcome categories SCOut-
come from Def. 1 as response variable, what is the inﬂuence of predictors publi-
cation Venue and Year?

1. HV,0: There is no inﬂuence of the publication Venue on the occurrence of the
statcheck outcome SCOutcome. HV,1: There is a systematic inﬂuence of the
publication Venue on the occurrence of the statcheck outcome SCOutcome.
2. HY,0: There is no inﬂuence of the publication Year on the occurrence of the
statcheck outcome SCOutcome. HY,1: There is a systematic inﬂuence of the
publication Year on the occurrence of the statcheck outcome SCOutcome.

As an exploratory inquiry, we employ the statcheck analysis to the submis-

sions of STAST 2019, testing its usefulness in supporting PC members.

4 Method

The study has been pre-registered at the Open Science Framework (OSF)2, which
also contains Online Supplementary Materials, such as a summary of the SLR
speciﬁcation and the sample itself. All analyses, graphs and tables are computed
directly from the data with the R package knitr, where the statcheck output was
cached in csv ﬁles.

All statistical tests are computed at a signiﬁcance level of α = .05. The Fisher
Exact Tests (FETs) for cases with low expected cell frequency are computed with
simulated p-values with 105 replicates.

4.1 Ethics

This study followed the guidelines of the ethical boards of its institution. While
we make the entire list of analyzed papers available for reproducibility, we de-
cided not to single out individual papers. We are aware that the the descriptive
statistics presented allow making a link to the respective papers; we accept that
residual privacy risk. Full disclosure: one of the sample’s papers belongs to the
author of this study; statcheck ﬂagged it.

4.2 Sample

The target population of this study was cyber security user studies. The sam-
pling frame for this study is derived from a 2016/17 Systematic Literature Re-
view (SLR) conducted by Coopamootoo and Groß [3] whose results were ﬁrst
published at a 2017 Community Meeting of the Research Institute in the Science
of Cyber Security (RISCS). This source SLR’s search, inclusion and exclusion
criteria are reported in Appendix B.

We have chosen this sample to gain comparability to earlier qualitative and
quantitative analyses on it [3]. This sample restricts the venues considered to
retain statistical power for a regression analysis. We stress that the automated
the analysis methodology can be easily applied to other samples.

2 osf.io/549qn/

Fig. 1. Flow chart of the study’s procedure with two interlinked analyses.

4.3 Procedure

Our procedure, as depicted in Figure 1, constituted a mixed-methods approach
that fusing two interlinked analysis processes: (i) Statistical Validity Analysis
and (ii) Grounded Coding of paper properties and errors detected. Our analysis
script received as input the PDFs of studies included from the source SLR.

Statistical Validity Analysis. we computed two iterations of statcheck, one only
considering statistical statements in standard format and one including all p-
values found. The statcheck results were subjected to a manual cross-check,
possibly resulting the the reshaping of papers that statcheck could not parse
out of the box. Subsequently, we merged the results of both analyses and ag-
gregated their events (counting number of correct tests, inconsistencies, decision
errors and p-values without parseable test statistics). We, thereby, established
the dependent variable SCOutput per statistical test and per paper.

Grounded Coding. We coded paper properties in NVivo. We evaluated the
statcheck results in a second lane of grounded coding, classifying errors of statcheck
as well as errors committed by authors of the papers.

As a part of this analysis, we “reshape” papers that could not be parsed by
statcheck for reasons outside of the research aims of this study. For instance, if a
paper embedded statistical tests as image rather than text, we would transcribe
the images to text and re-run statcheck on the “reshaped” input.

Once these results are coded, we amend the statcheck outcomes recorded in
SCOutcome to ensure that this variable reﬂects an accurate representation of the
sample.

Finally, based on the accumulated meta-data on the publications, we then

evaluated the statcheck results with respect to publication year and venue.

4.4 Grounded Coding

Grounded coding refers to the the code being grounded in properties found in
the data, instead of being based on an a priori codebook.

PAPERMLRPlotsCodeYear, VenueCodeSample Size, MturkGrounded Codingstatcheckall p-valuesStatistical Validity Analysisstatcheckvalid tripletsMultinomialLogisticRegressionConsolidatestatcheckAnalysisAggregateError ClassesReshapeUnparseablePaperCodeQualitativeError Typesp-Values & Test Statistics Found?YesNoRe-Analyze Reshaped PapersExcludeExclude Papers w/o p-ValuePaper Properties. We conducted a systematic coding in NVivo with the pur-
pose to establish overall properties of all papers. We were extracting especially:
(i) sample size, (ii) use of recruiting platforms (e.g., MTurk), (iii) use of multiple-
comparison corrections, and (iv) use of dependent-samples tests.

Analysis Outcomes. After having run statcheck on the sample, we ﬁrst conducted
a grounded coding of statistical tests marked as inconsistency or decision error.
We re-computed the p-values from the test statistics ourselves and interpreted
the results in the context of the reporting of the paper. We took into account the
formulation around the test as well as overall speciﬁcation of hypotheses, test
parameters (e.g., one-tailed) and multiple-comparison corrections. We include
the resulting emergent codebook presented in Table 2.

Secondly, we analyzed the outcomes statcheck marked as neither inconsis-
tency nor decision error. For those results, we compared the raw text with
statcheck’s parsed version as well as recomputed p-value. We ignored small round-
ing diﬀerences as statcheck as authors rounding test statistics for reporting will
naturally cause small diﬀerences. In cases of a mismatch between raw text and
interpretation (e.g., in degrees of freedom accounted for), we re-computed the
statistics manually.

Finally, we coded whether a mistake by statcheck would be considered a
FalsePositive or FalseNegative. After this evaluation, we adjusted the SCOutcome
to ensure that the subsequent analysis is based on a correct representation of
the sample.

Table 2. Codebook of the grounded coding of error types.

Errors of statcheck

Errors of authors

Code

Deﬁnition

Code

Deﬁnition

scParsedOK parsed the PDF correctly
statcheck result validated
scCorrect
scMisclassiﬁed misclassiﬁed test
scMissedMC missed multiple-comparison

Likely mis-typed

Typo
RoundingError incorrect rounding rules
OneTailedUS unspeciﬁed one-tailed test
Miscalculation miscalculated the statistics,

corrections speciﬁed paper

wrong p-value for statistic

4.5 Evaluation of statcheck

Appendix A contains the details of the corresponding qualitative coding.

Reshaping of Unparseable Papers. There were eight of papers for which statcheck
could neither extract p-values nor test statistics due to encoding issues (e.g., em-
bedding statistics as images). For all of those, we recorded them as unparseable,
yet transformed them into parseable text ﬁles for further analysis.

Table 3. Confusion matrix for statcheck evaluating tests.

Predicted

Reference

Positive
Negative

Positive
29
0

Negative
5
218

Accuracy: .98, 95% CI [.95, .99], Acc > NIR(.88), < .001***,
Sensitivity = 1.00, Speciﬁcity = .98, PPV = .85, F1 = .92

Errors Committed by statcheck. Of the total 252 parsed tests, 34 contained an
error, 10 of which a decision error. We compared those outcomes against the
grounded coding of results and our re-computation of the statistics.

We found that (i) statcheck parsed papers that were correctly reported with-
out fail, (ii) it misclassiﬁed two tests, (iii) it detected one-tailed tests largely
correctly, (iv) it treated dependent-samples tests correctly, (v) it did not recog-
nize the speciﬁed multiple-comparison corrections in three cases. This leaves us
with 5 false positives and no false negatives, marked in Sub-Figure 8a.

Detection Performance of statcheck. For the analysis of complete test triplets,
we analyzed the confusion matrix of statcheck results vs. our coding (Table 3).
The Positive Predictive Value (PPV) of 85% indicates a decent likelihood of a
positive statcheck report being true.

4.6 Multinomial Logistic Regression

We conducted multinomial logistic regressions with the R package nnet, relying
on Fox’s work [10] for visualization. The models were null, year-only, venue-
only and year and venue combined. The dependent variables was SCOutput. The
independent variables were Year (interval) and Venue (factor).

5 Results

5.1 Sample

We have reﬁned the inputed sample of 146 publications by excluding publications
that do neither contain empirical data nor signiﬁcance tests (p-value), retaining
114 publications for further analysis. We illustrate the sample reﬁnement in
Table 4. We include the ﬁnal sample in Appendix C and outline its distribution
by publication venue and year in Table 9. We note that the sample is skewed
towards SOUPS and more recent publications. We note that the sample was
drawn only from 10 speciﬁc venues in an eﬀort to retain power in a logistic
regression with venue as a categorical factor.

Table 4. Sample Reﬁnement and Final Composition

Phase

Excluded Retained Sample

Source SLR [3] (Google Scholar)

Inclusion/Exclusion

This study

Studies with Empirical Data
Studies with NHST/p-Value

—
1011

24
8

1157
146

122
114 → Final Sample

Table 5. Sample composition by venue and year.

3
1
0
2

0
1
0
2

7
0
0
2

9
0
0
2

5
1
0
2

8
0
0
2

6
0
0
2

6
1
0
2

4
1
0
2

2
1
0
2

1
m
1
u
0
S
2
SOUPS 6 3 4 6 8 4 10 8 13 9 6 77
4
8
7
6
3
2
2
4
1
Sum 7 4 4 7 8 6 17 9 24 13 15 114

USEC 0 0 0 0 0 0 0 0 4 0 0
CCS 0 0 0 0 0 0 0 0 4 1 3
USENIX 0 0 0 1 0 0 4 1 1 0 0
PETS 1 0 0 0 0 1 1 0 0 1 2
TISSEC 0 0 0 0 0 1 0 0 0 0 2
LASER 0 0 0 0 0 0 1 0 0 0 1
S&P 0 0 0 0 0 0 0 0 1 1 0
TDSC 0 1 0 0 0 0 1 0 1 0 1
WEIS 0 0 0 0 0 0 0 0 0 1 0

5.2 Exploration of the Distribution

We oﬀer explorative descriptive analyses of the SLR cohort, before we move on
to the pre-registered analyses.

Distribution of Qualitative Properties. We visualize the presence of qualitative
properties of papers over time in Figure 2. We observe (i) MTurk being used
from 2010 (2a), (ii) Multiple-Comparison Corrections seeing adoption from 2009
(2b), (iii) Eﬀect sizes being on and oﬀ over the years (2c).

Distribution of p-Values. We analyze the distribution of p-values per paper.
Therein we distinguish incomplete and complete triplets including test statis-
tic and degrees of freedom. In Figure 3, we depict this p-value distribution;
3a is ordered by number of the tests reported on, distinguishing between com-
plete/incomplete triplets while annotating the presence of multiple comparison
corrections (MCC); 3b is organized by publication year. The included linear
regression lines indicate little to no change over time.

5.3 Prevalence of Statistical Misreporting

For RQ1, we compare statistical misreporting by venue and year, considering
individual tests as well as entire papers (cf. contingency tables in Appendix D).

(a) MTurk use

(b) MCC

(c) Eﬀect size reporting

Fig. 2. Properties of SLR papers by year. MCC = Multiple-Comparison Corrections

Misreported Tests. For individual tests, there is a statistically signiﬁcant associ-
ation between the statcheck outcomes and the publication venue, FET p = .033,
as well as the publication year, FET p < .001. This oﬀers ﬁrst evidence to reject
the null hypotheses HV,0 and HY,0.

Table 10 contains the corresponding contingency table.

Table 6. Contingency table of individual test statcheck outcomes by venue, FET p =
.033.

I

S
P
U
O
S

C
E
S
U
CorrectNHST 170 1
19 1
Inconsistency
9 0
DecisionError

X
R
N
S
E
I
S
E
S
E
C
A
S
W
U
C
L
4 11 6 5 0 12 0
9
0 0 0 1 0 0 0
3
0 0 0 1 0 0 0
0
Incomplete 1028 33 122 100 72 71 19 11 60 7

C
E
S
S
I
T

C
S
D
T

S
T
E
P

P
&
S

Papers with Misreporting. Sub-Figure 6a on p. 16 shows a hierarchical waﬄe
plot of the statcheck outcomes. For aggregated outcomes per paper displayed in
Figure 4, the associations per venue and year are not statistically signiﬁcant,
FET p = .964 and FET p = .458 respectively. A likely reason for this result is
visible in the histograms of Figure 5: errors are at times clustered, in that, some
papers contain multiple errors.

5.4 Comparison with JMP

With respect to RQ2, the statcheck outcomes of the included SLR and Jour-
nal of Media Psychology (JMP) are statistically signiﬁcantly diﬀerent, χ2(3) =

0%25%50%75%100%20062007200820092010201120122013201420152016YearProportionMTurk SampleNoYes0%25%50%75%100%20062007200820092010201120122013201420152016YearProportionMultiple−Comparison CorrectionsAbsentPresent0%25%50%75%100%20062007200820092010201120122013201420152016YearProportionEffect Size ReportingAbsentInferableExplicit(a) Number of reported p-Values per paper

(b) Distribution by Year

Fig. 3. Distribution of statistical reporting of papers, that is, how many p-values per
paper are reported Incomplete or Complete. MCC = Multiple-Comparison Corrections.

050100150MCCAbsentPresentReportingIncompleteComplete05010015020062007200820092010201120122013201420152016ReportingIncompleteComplete(a) Venues

(b) Years

Fig. 4. Proportions of per-paper aggregated statcheck outcomes by venue and year. The
results by year are shown as area plot to highlight development over time.

(a) Frequency of inconsistencies

(b) Frequency of decision errors

Fig. 5. Number of errors per paper.

88.803, p < .001. Hence, we reject the null hypothesis HC,0 and conclude that
there is a systematic diﬀerence between ﬁelds. We ﬁnd an eﬀect of Cram´er’s
V = 0.646, 95% CI [0.503, 0.773].

If we restrict the analysis to the papers containing Complete tests and,
thereby, exclude papers marked Incomplete, we ﬁnd that the diﬀerence between
ﬁelds is not statistically signiﬁcant any longer, χ2(2) = 0.197, p = .906, Cram´er’s
V = 0.037, 95% CI [0, 0.139].

5.5 Reporting Test Outcomes by Venue and Year

While we analyzed tests and aggregated paper SCOutcome by venue and year,
we found that these multinomial logistic regressions were not stable. Even if
the models were statistically signiﬁcant, this missing stability was evidenced
in extreme odds-ratios, which was likely rooted in the sparsity of the dataset.

0%25%50%75%100%SOUPSUSECCCSUSENIXPETSTISSECLASERS&PTDSCWEISVenueProportionSCOutcomeCorrectNHSTInconsistencyDecisionErrorIncomplete0%25%50%75%100%20062007200820092010201120122013201420152016YearProportionSCOutcomeCorrectNHSTInconsistencyDecisionErrorIncomplete01234567891011123456InconsistenciesFrequency0123456789101112012345Decision ErrorsFrequency(a) This Study (SLR)

(b) JMP

Fig. 6. Hierarchical Waﬄe plots comparing user studies (SLR) in cyber security and
the Journal of Media Psychology (JMP) (One square represents one paper).

(We report all MLR conducted in Appendix G for reference). To overcome the
sparsity, we chose to collapse the venue factor into SOUPS and OTHER levels,
called venue’ (and the corresponding null hypothesis HV(cid:48),0).

A multinomial logistic regression on individual tests with SCOutcome ∼
venue’+year with Incomplete as reference level is statistically signiﬁcant, LR,
χ2(6) = 15.417, p = .017. Because the model explains McFadden R2= .01 of the
variance, we expect little predictive power.

The corresponding predictors are statistically signiﬁcant as well. Hence, we
reject the null hypotheses HV(cid:48),0 and HY,0. Figure 7 contains an overview of the
scatter plot vs. the predicted probabilities from the MLR.

While we ﬁnd that there is an eﬀect of year in increasing likelihood of In-
complete outcomes, this only accounts for an increase of 0.2% per year, barely
perceptible in the graph. Everything else being equal, a transition from venue
SOUPS to OTHER yields an increase of likelihood of the Incomplete outcomes,
by a factor of roughly 2. However, these changes are dwarfed by the overall
intercept of tests being correct (in comparison to Incomplete).

In absolute terms, the expected likelihood of tests being Incomplete is 80%,
with OTHER venues having a few percent greater Incomplete likelihood. SOUPS
exhibits an expected likelihood of 13% of being CorrectNHST, while OTHER
venues yield a few percent lower likelihood.

5.6 Qualitative Analysis

We oﬀer a summary of the analysis here, a detailed account is included in Ap-
pendix A.

Composition of Incomplete p-Values. Sub-Figure 8b contains an overview of the
classes of incompletely reported p-values. Less than half the cases of incomplete

(a) Scatter plot

(b) MLR probabilities

Fig. 7. Per-teststatcheck outcomes by venue and year. Note: The multinomial logistic
regression (MLR) is statistically signiﬁcant, LR Test, χ2(6) = 15.417, p = .017.

Table 7. Confusion matrix for researchers determining signiﬁcance.

Predicted

Reference

Signiﬁcant
NS

Signiﬁcant
191
1

NS
12
47

Accuracy: .95, 95% CI [.91, .97], Acc > NIR(.76), < .001***,
Sensitivity = .99, Speciﬁcity = .80, PPV = .94, F1 = .97

triplets contain an actual p-values (half of them, in turn, signiﬁcant or not signif-
icant). 31% of the incomplete cases compared to lower signiﬁcance bound than
α = .05. 9% of the tests are simply declared non-signiﬁcant, another 7% reported
as signiﬁcant wrt. p < .05.

Distribution of p-values. Figure 9 shows the diﬀerence between reported and
computed p-values. When comparing reported and re-computed p-values, we
found that in 22 out of 34 cases, the reported p-value was more signiﬁcant than
the computed one (65%).

5.7 Signiﬁcance Detection Performance

We analyzed the decision making of authors on statistical signiﬁcance of reported
results vis-`a-vis of recomputed p-values (Table 7). We observe a somewhat low
speciﬁcity of 80%. Note that this analysis only refers to a reported signiﬁcance
decision is valid with respect to a corresponding correct p-value, and not whether
a positive reported result is true.

CorrectNHSTInconsistencyDecisionErrorIncomplete20062007200820092010201120122013201420152016CorrectNHSTInconsistencyDecisionErrorIncompleteYearSCOutcomeVenueSOUPSOTHERCorrectNHSTInconsistencyDecisionErrorIncomplete2006200820102012201420160%25%50%75%100%0%25%50%75%100%0%25%50%75%100%0%25%50%75%100%YearProbabilityVenueSOUPSOTHER(a) Inconsistency/DecisionError classes.

(b) Incomplete classes.

Fig. 8. Classiﬁcation of reported statcheck outcomes.

5.8 Supporting the STAST 2019 PC in Checking Statistics

Aligned with Recommendation 2 in Section 7, we oﬀered a statcheck analysis
to the STAST PC members to support the workshop’s discussion phase. Of 28
submitted papers, 9 papers (32%) included a statistical inference.

Let us consider these 9 papers in detail as an exploratory analysis. One paper
contained a major error in terms of statistics being invalid, two papers used the
wrong statistical method for the experiment design at hand (e.g., independent-
samples statistics in a dependent-samples design). Two of those three papers
were also ﬂagged by statcheck. These errors themselves, however, were detected
by program committee members, not by the statcheck analysis.

On third of the papers reported statistics in an APA compliant format. 6
papers (66%) reported exact p-values, 4 papers (44%) reported eﬀect sizes as re-
quired by the STAST submission guidelines. Of the 9 papers, 7 needed multiple-
comparison corrections, which only two provided in their initial submission.

In terms of statcheck evaluation with the methodology of this study, we
found 5 papers (56%) to be Incomplete, one paper Inconsistent, three papers
(33%) CorrectNHST. This distribution is not signiﬁcantly diﬀerent from the
SLR sample shown in Figure 6a, χ2(3) = 0.829, p = .843, Cram´er’s V = 0.082,
95% CI [0, 0.188].

0%25%50%75%100%ProportionError ClassesTypoRounding ErrorCopy & PasteOne−Tailed Not ExplicitMiscalculationFalse Positive0%25%50%75%100%ProportionIncomplete Classes"ns""p > .05""p < .05""p < .01" | "p < .001" | "p < .0001"actual p, p < .05actual p, p >= .05Fig. 9. Histogram of diﬀerence reported p-values minus statcheck-computed p-values.

6 Discussion

Incomplete reporting holds back the ﬁeld. Nearly two thirds of the pa-
pers with p-values did not report a single complete test triplet (cf. Fig. 6a). This
impairs the ability to cross-check internal consistency of tests and, thereby, un-
dermines fault-tolerance. Hence, such papers have limited credibility and ﬁdelity
of statistical information.

The incomplete reporting observed in this study is in stark contrast to the
analysis of the Journal of Media Psychology (JMP), in which not a single paper
was Incomplete. Hence, we conclude that mandated reporting standards are an
eﬀective tool.

It is further troubling that the likelihood of incomplete reporting did not

seem to decrease over time (cf. Fig. 7b).

In terms of research reuse and synthesis, the situation is aggravated, because
eﬀect sizes are vastly under-reported in this ﬁeld. Only a small minority reports
them explicitly; one third of the papers allows to infer them (cf. Fig 2c).

There are three consequences to this phenomenon: (i) It is exceedingly dif-
ﬁcult for practitioners to ascertain the magnitude of eﬀects and, thereby, their
practical signiﬁcance. (ii) It is near-impossible to compare research results in
meta-analyses and to synthesize well-founded summary eﬀects. (iii) Hence, dis-
putes over diﬀerences between original studies and replications are hard to settle
satisfactorily. .

While some errors are minor, we caution against clustered errors and
miscalculations. Of the 44 papers with complete test statistic triplets ana-
lyzed, 60% were deemed correct; more than one quarter had at least one incon-
sistency; 14% had at least one decision error. Of all tests with complete triplets
analyzed 14% were erroneous. Here, the socio-technical security sample showed
similar error rates as the psychology sample.

Especially the 26 papers with complete test triplets and correct reporting—
one quarter of the sample—stand testament to eﬀorts of authors and program
committees “get it right.”

The errors observed by statcheck were often minor typos and rounding errors
that could have been easily avoided, however nearly 40% seemed to be serious

Reported ~ Computed+/− 0.01(203)Reported < Computed(24)Computed < Reported(25)02468−1.0−0.50.00.5DifferenceFrequencymiscalculations. We found that these errors were at times clustered: there are a
few papers with a number of errors.

Of course, we would need to assume that the 68 papers without complete
test triplets have at least the same error rates as the ones with complete triplets,
yielding another dark ﬁgure.

There is a dark ﬁgure of decision errors lurking in the underuse of
multiple-comparison corrections. This study leaves the detailed analysis of
power and multiple-comparison corrections (MCCs) to future work. Still, we do
not want to withhold insights already apparent from Fig. 3a: There is a Damocles
sword hanging over many papers: Multiple-Comparison Corrections (MCCs).

We have seen in Fig. 2b that even though MCCs came in use from year 2009,
only about one third of the papers employed them. From Fig. 3a, we observe
that there are papers with a considerable number of reported p-values without
MCCs. Hence, there may well be a sizable dark ﬁgure of papers with decision
errors in store once adequate MCCs are employed.

These observations inform Recommendation 3 in that observing studies with
many comparisons but without corrections can be an indication of the number
of comparisons, multiple-comparison corrections as well as the power needed to
sustain them only being considered as an afterthought.

Automated checking of statistical reporting is viable. The statcheck
detection rates were very good and comparable to the rates reported by Nui-
jten et al. [19]. We note, however, that statcheck did not operate completely
autonomously, but was complemented with human coding to overcome pars-
ing issues. We ﬁnd the approach viable for the use in socio-technical aspects of
security.

We believe that we encountered problems reported by Schmidt [26,27] to a
lesser extent as this ﬁeld is largely operating with simple statistical tests and
few corrections of p-values.

6.1 Limitations

Generalizability. The study is based on an existing SLR sample that largely
consists of SOUPS publications and only contains few cases for other venues.
Dealing with a sparse matrix, the likelihoods computed for non-SOUPS venues
as well as overall logistic regressions suﬀer from more uncertainty.

Also, the use of the SLR sample instead of a statistical sampling method

with a complete sampling frame limits generalizability.

Syntactic Validity Checks. While we have made good experiences with
statcheck and only found few false positives and negatives, we observe that
statcheck results can suﬀer from hidden errors. While we complemented the auto-
mated analysis with a human review and coding of reported errors, we observe

that statcheck could have missed or misinterpreted individual tests. However,
based our inspection of the 114 analyzed papers, we expect that the number of
statcheck errors is small compared to the 1775 tests analyzed. In the end, an
automated tool cannot replace the trained eye of a knowledgable reviewer. How-
ever, this study is about the overall distribution of errors, which will be hardly
skewed by rare false positives or negatives.

Deviations from the Pre-Registration.
1. We did not attempt the exploratory of author&institution as the sample
seemed too small and dimensionality reduction may introduce artifacts.
2. We pre-registered an ordinal logistic regression as primary analysis, however
found that SCOutcome is not a valid ordinal variable and retained the also
pre-registered multinomial logistic regression as tool of choice.

3. We merged the non-SOUPS venue levels to overcome the sparsity of the

dataset,

4. We did not pursue a logistic regression on Coopamootoo and Groß’s com-
pleteness indicators [3], as the nine additional regressions would yield a
higher Type-I error rate.

7 Recommendations

The recommendations made here need to be seen as part of a greater paradigm
shift. Instead of focusing on single publications, one may consider that a study
does not stand on its own. Truly advancing the knowledge of a ﬁeld calls for
creating robust studies that prepare the ground for systematic replications, reuse
and research synthesis.

1. Establish sound reporting standards. Sound and generally accepted re-
porting standards could greatly improve the credibility of the ﬁeld. This could
either mean developing systematic reporting standards for socio-technical as-
pects of security or adopting existing standards.

Developing systematic reporting standards would involve a stable coalition
of program committee chairs and members as well as journal editors forming a
working group to that eﬀect. Such a working group would likely take into account
requirements for this ﬁeld as well as examples of mature reporting standards from
other ﬁelds.

Given that considerable thought has gone into APA standards [1] and Psy-
chology Journal standards [22] and that these standards apply to human di-
mensions, they are a viable and suﬃciently mature candidate, at least when
it comes to statistical reporting. Our analysis showed that the majority of pa-
pers reporting complete test statistics triplets were actually compliant to APA
requirements.

While not perfect, their recommendations on statistical reporting could have
considerable beneﬁts for reporting ﬁdelity, research reusability and synthesis.

One option in this context would be to only adopt a subset of recommendations
directly beneﬁting reporting ﬁdelity.

In any case, one would consider sound reporting for test statistics themselves,
eﬀect sizes and their conﬁdence intervals, as well as essential information on the
sample, design and procedure. Again, this ﬁeld can well take into account more
comprehensive initiatives from other ﬁelds [15].

2. Support PCs in checking statistics. From our experience researching this
study, we can attest that checking statistics can be a tedious aﬀair. Even with all
their failings, tools like statcheck can support program committee members in
detecting incorrect results. Such an approach certainly requires human mediation
to avoid false positives, yet can oﬀer insights at low cost.

As reported in Section 5.8, we tested this recommendation on the STAST
2019 program committee. While statcheck correctly identiﬁed reporting issues
and did not produce a false positive, major errors were discovered by program
committee members in the analysis of experiment designs vis-`a-vis their sta-
tistical inferences. This yields an indication that an automated tool, such as
statcheck, will only support but never replace the expert judgment of the re-
viewers.

There are organizational methods, such as pre-registrations or registered re-

ports, that can support a PC further in ascertaining the integrity of results.

3. Embrace a priori power and multiple-comparison corrections. We
make this recommendation with a grain of salt, as we have not reported on a
dedicated study on power, yet. However, even this study on reporting ﬁdelity
shows that this consideration would beneﬁt the community.

Low power and missing adequate MCCs can well undermine the results of a
good study and increase the likelihood of a positive result being a false positive.
We encourage researchers to plan in advance for the power required, accounting
for the MCCs necessary for the planned tests.

8 Conclusion

This study is the ﬁrst systematic analysis of a large sample of security user
studies with respect to their statistical reporting ﬁdelity. For the ﬁrst time, we
oﬀer a comprehensive, quantitative, and empirical analysis of the state-of-play
of the ﬁeld of socio-technical aspects of security. We oﬀer a wealth of diﬀerent
perspectives on the sample, enabling us to obtain a ﬁne-grained analysis as well
as broad recommendations for authors and program committees alike.

We stress that the research and reviewing process for security user studies
constitutes a socio-technical system in itself that impacts the decision making
in security and privacy. Because scientists and practitioners alike seek to re-
use research results, the ﬁdelity or uncertainty of those results—especially their
statistical inferences—plays a major role in the credibility of the ﬁeld and the

conﬁdence of its audience. Hence, self-reﬂection of the ﬁeld will ultimately impact
the decision making by users in security and privacy, as well.

As future work, we consider expanding the sample, including further venues,
such as CHI, as well as oﬀering a dedicated analysis of statistical power and
Positive Predictive Value (PPV) present in the ﬁeld.

Acknowledgment

We would like to thank Malte Elson for the discussions on statcheck, on the
corresponding analyses in psychology, and on general research methodology. We
thank the anonymous reviewers of STAST 2019 for their discussion and insightful
comments, as well as the volume co-editor Theo Tryfonas for oﬀering additional
pages to include the requested changes.

This study was in parts funded by the UK Research Institute in the Science
of Cyber Security (RISCS) under a National Cyber Security Centre (NCSC)
grant on “Pathways to Enhancing Evidence-Based Research Methods for Cyber
Security” (Pathway I led by Thomas Groß). The author was in parts funded by
the ERC Starting Grant CASCAde (GA no716980).

References

1. American Psychological Association (ed.): Publication Manual of the American
Psychological Association (6th revised ed.). American Psychological Association
(2009)

2. Coopamootoo, K., Groß, T.: Cyber security & privacy experiments: A design &
reporting toolset. In: IFIP International Summer School on Privacy and Identity
Management (2017)

3. Coopamootoo, K., Groß, T.: Systematic evaluation for evidence-based methods in

cyber security. Technical Report TR-1528, Newcastle University (2017)

4. Coopamootoo, K.P., Groß, T.: Evidence-based methods for privacy and identity
management. In: IFIP International Summer School on Privacy and Identity Man-
agement. pp. 105–121. Springer (2016)

5. Coopamootoo, K.P., Groß, T.: A codebook for experimental research: The nifty
nine indicators v1.0. Tech. Rep. TR-1514, Newcastle University (November 2017)
6. Cumming, G.: Understanding the new statistics: Eﬀect sizes, conﬁdence intervals,

and meta-analysis. Routledge (2013)

7. Elson, M., Przybylski, A.K.: The science of technology and human behav-
ior – standards old and new. Journal of Media Psychology 29(1),
1–7
(2017). https://doi.org/10.1027/1864-1105/a000212, https://doi.org/10.1027/
1864-1105/a000212

8. Epskamp, S., Nuijten, M.B.: statcheck: Extract statistics from articles and recom-
pute p values (v1.3.0). https://CRAN.R-project.org/package=statcheck (May
2018)

9. Fidler, F., et al.: The American Psychological Association publication manual sixth
edition: Implications for statistics education. Data and context in statistics educa-
tion: Towards an evidence based society (2010)

10. Fox, J., Andersen, R.: Eﬀect displays for multinomial and proportional-odds logit

models. Sociological Methodology 36(1), 225–255 (2006)

11. Fox, J., Weisberg, S.: An R companion to applied regression. Sage Publications

(2018)

12. Fox, J., Weisberg, S., et al.: car: Companion to applied regression (v.2.1.5). https:

//CRAN.R-project.org/package=car (Jul 2017)

13. Ioannidis, J.P.: Why most published research ﬁndings are false. PLoS Med 2(8),

e124 (2005)

14. Lakens, D.: Checking your stats, and some errors we make. http://daniellakens.
blogspot.com/2015/10/checking-your-stats-and-some-errors-we.html (Oct
2015)

15. LeBel, E.P., McCarthy, R.J., Earp, B.D., Elson, M., Vanpaemel, W.: A uniﬁed
framework to quantify the credibility of scientiﬁc ﬁndings. Advances in Methods
and Practices in Psychological Science 1(3), 389–402 (2018)

16. Maxion, R.: Making experiments dependable. In: Dependable and Historic Com-

puting, pp. 344–357. Springer (2011)

17. Moher, D., Hopewell, S., Schulz, K.F., Montori, V., Gøtzsche, P.C., Devereaux, P.,
Elbourne, D., Egger, M., Altman, D.G.: CONSORT 2010 explanation and elabo-
ration: updated guidelines for reporting parallel group randomised trials. Journal
of clinical epidemiology 63(8), e1–e37 (2010)

18. Moher, D., Liberati, A., Tetzlaﬀ, J., Altman, D.G.: Preferred reporting items for
systematic reviews and meta-analyses: the PRISMA statement. Annals of internal
medicine 151(4), 264–269 (2009)

19. Nuijten, M.B., van Assen, M.A., Hartgerink, C.H., Epskamp, S., Wicherts, J.: The
validity of the tool “statcheck” in discovering statistical reporting inconsistencies.
https://psyarxiv.com/tcxaj/ (2017)

20. Nuijten, M.B., Hartgerink, C.H., van Assen, M.A., Epskamp, S., Wicherts, J.M.:
The prevalence of statistical reporting errors in psychology (1985–2013). Behavior
research methods 48(4), 1205–1226 (2016)

21. Peisert, S., Bishop, M.: How to design computer security experiments. In: Fifth
World Conference on Information Security Education. pp. 141–148. Springer (2007)
22. Publications, APA and on Journal, Communications Board Working Group: Re-
porting standards for research in psychology: Why do we need them? what might
they be? The American Psychologist 63(9), 839 (2008)

23. Ripley, B., Venables, W.: nnet: Feed-forward neural networks and multinomial
log-linear models. https://CRAN.R-project.org/package=nnet (Feb 2016)
24. Rudis, B., Gandy, D.: waﬄe: Create waﬄe chart visualizations in R. https://

CRAN.R-project.org/package=waffle (Jan 2017)

25. Schechter, S.: Common pitfalls in writing about security and privacy human sub-
jects experiments, and how to avoid them. https://www.microsoft.com/en-us/
research/wp-content/uploads/2016/02/commonpitfalls.pdf (2013)

26. Schmidt, T.: Sources of false positives and false negatives in the STATCHECK algo-
rithm: Reply to nuijten et al.(2016). https://arxiv.org/abs/1610.01010 (2016)
27. Schmidt, T.: Statcheck does not work: All the numbers. reply to nuijten et

al.(2017). https://psyarxiv.com/hr6qy/ (2017)

A Details on Qualitative Analysis

A.1 Errors Committed by statcheck.

Parsing Accuracy.
In all 34 error cases, statcheck parsed the PDF ﬁle cor-
rectly, and its raw test representation corresponded to the PDF. In all but two
tests, statcheck recognized the test correctly. In said two cases, it mistook a
non-standard-reported Shapiro-Wilk test as χ2 test, creating two false positives.
There was one case in which the statcheck computed p-value for an independent-
samples t-test diﬀered slightly from our own calculation, yet only marginally so,
presumably because of a unreported Welch correction.

One-Tailed Tests. In seven cases, statcheck recognized one-tailed tests correctly.
For three of those tests, the authors framed the hypotheses as one-tailed. In
three other tests, the authors used one-tailed test results without declaring their
use. There was one additional case in which the authors seemed to have used a
one-tailed test, yet the rounding was so far oﬀ the one-tailed result that statcheck
did not accept it as “valid if one-tailed” any longer. There was one test marked
as “one-tail” which statcheck did not recognize as one-tailed, yet that test also
suﬀered from rounding errors.

Dependent-Samples Tests. There were 7 papers using dependent-samples meth-
ods (such as matched-pair tests or mixed-methods regressions). We found that
statcheck treated the corresponding dependent-samples statistics correctly.

Multiple Comparison Corrections. In three cases, statcheck did not recognize p-
values that were correctly Bonferroni-corrected, counting as three false positives.
It is an open point, however, how many paper should have employed multiple-
comparison corrections, but have not done so, an analysis statcheck does not
perform.

A.2 Errors Committed by Authors

Typos. We considered 6 to be typos or transcription errors (18%). Another 1
error seemed to be a copy-paste error (3%)

Rounding Errors. Of all 34 reported errors, we found 8 to be rounding errors
(24%).

Miscalculations. We found 13 cases to be erronious calculations (38%).

A.3 Composition of Incomplete p-Values

Of 1523 incomplete cases, 134 were declared “non-signiﬁcant” without giving
the actual p-value (9%). Further, 6 were shown as p > .05. (0%).

Of the incomplete cases, 102 were reported statistically signiﬁcant at a .05

signiﬁcance level (7%).

Of the incomplete cases, 477 were reported statistically signiﬁcant at a lower

signiﬁcance level of .01, .001, or .0001 (31%).

Of 1523 incomplete p-values, 680 gave an exact p-value (45%). Of those ex-
atly reported p-values, half (367) were claimed statistically signiﬁcant at a sig-
niﬁcance level of α = .05 (54%). Of those exatly reported p-values, 19 claimed
an impossible p-value of p = 0 (3%).

Online Supplementary Materials

We made the materials of the study (speciﬁcation of the inputted SLR, included
sample, contingency tables) also publicly available at its Open Science Frame-
work Repository3.

B Underlying Systematic Literature Review

This meta-analytic study is based on a Systematic Literature Review (SLR),
which was conducted in 2016/17 for the UK Research Institute in the Science
of Cyber Security (RISCS). We adapt this description of the SLR’s search from
its technical report [3].

B.1 Search Strategy of the SLR Sample

The SLR included security and privacy papers published between 2006 and 2016
(inclusive).

The search was restricted to the following security and privacy venues:

– journals: IEEE Transactions on Dependable & Secure Computing (TDSC),

ACM Transactions on Information and System Security (TISSEC),

– ﬂagship security conferences: IEEE S&P, ACM CCS, ESORICS, and PETS

or

– specialized conferences and workshops: LASER, SOUPS, USEC and WEIS.
The search was conducted on Google Scholar. Each query extracts articles
mentioning “user study” and at least one of the words “experiment,” “evidence”
or “evidence based.” The described query was executed for each of the 10 pub-
lication venues. In the advanced search option of Google Scholar, each of the
following ﬁelds were set:

– with all words = user study
– at least one of the words = experiment evidence “evidence based”
– where my words occur = anywhere in the article
– return articles published in = [publication venue]
– return articles dated between = 2006–2016

The search yielded 1157 publications.

B.2 SLR Inclusion/Exclusion Criteria

We adapt the inclusion/exclusion criteria of the 2017 SLR [3] for this pre-
registration. The SLR focused on human factors studies including a human sam-
ple. The following Inclusion Criteria were applied to its overall pool of 1157
publications:

– Studies including a user study with human participants.

3 osf.io/549qn/

– Studies concerned with evidence-based methods or eligible for hypothesis

testing and statistical inference.

– Studies that lend themselves to quantitative evaluation, quoting statements

of statistical signiﬁcance, p-values or eﬀect sizes.

– Studies with true experiments, quasi-experiments or observational analysis.
Of the papers included, the ones fulﬁlling the following Exclusion Criteria were
excluded:

– Papers that were not subject to research peer-review, key note statements,

posters and workshop proposals.

– Position papers or informal arguments.
– Papers not including a study with human participants,
– Theoretical papers.
– Studies with qualitative methodology.

This inclusion/exclusion process yielded a ﬁnal sample of 146 publications.

C SLR Sample

Table 8: Sample of inputted SLR [3] and this study with marked exclu-
sions (Ex.).

Tag

Tilte

AcqGro2006

Imagined Communities Awareness Infor-
mation Sharing and Privacy on Facebook
AdAcBr2013 Sleights of Privacy Framing disclosures and
the limits of transparency
AfBrGr2012 Detecting Hoaxes Frauds and Deception in

Writing Style Online

Venue Year Ex.

PETS

2006

SOUPS 2013

S&P

2012

AfCaSt2014 Doppelg”anger Finder Taking Stylometry

S&P

2014

to the Underground

AgShJa2013 Do not embarass Re-examining user con-

SOUPS 2013

cerns for online tracking and advertising

AhmIss2007 A New Biometric Technology Based on

TDSC 2007

Mouse Dynamics

AkhPor2013 Alice in warningland a large-scale ﬁeld
study of browser security warning eﬀective-
ness

AlbMai2015 Evaluating the Eﬀectiveness of Using Hints
for Autobiographical Authentication A
ﬁeld Study

AlFaWr2015 The Impact of Cues and User Interaction
on the Memorability of System Assigned
Recognition-Based Graphical Passwords

AlPoRe2014 Your Reputation Precedes You History
Reputation and the Chrome Malware
Warning

USENIX 2013

SOUPS 2015

SOUPS 2015

SOUPS 2014

Continued on next page

Table 8 – Sample. Continued from previous page

Tag

Tilte

AngOrt2015 WTH Experiences Reactions and Expecta-
tions Related to Online Privacy Panic Sit-
uations

Venue Year Ex.

SOUPS 2015

AtBoHe2015 Leading Johnny to Water Designing for

SOUPS 2015

Usability and Trust

BaMaLi2014 The Privacy and Security Behaviors of
Smartphone App Developers
BeGiKr2015 User Acceptance Factors for Anonymous

USEC 2014

WEIS

2015

Credentials

BeLoSi2007 Establishing Darknet Connections An

SOUPS 2007

BelShe2016

BenRei2013

evaluation of Usability and Security
Crowdsourcing for Context Regarding Pri-
vacy in Beacon Encounters via Contextual
Integrity
Should users be
informed On risk-
perception between Android and iPhone
users

BeWaLi2010 The Impact of Social Navigation on Pri-
vacy Policy Conﬁguration
BiCoIn2015 What the App is That Deception and
Countermeasures in the Android User In-
terface

PETS

2016

SOUPS 2013

SOUPS 2010

S&P

2015

BonSch2014 Towards reliable storage of 56-bit secrets

USENIX 2014

in human memory

BoSaRe2012 Neuroscience Meets Cryptography Design-
ing Crypto Primitives Secure Against Rub-
ber Hose Attacks

USENIX 2012

BrCrDo2013 Your Attention Please

SOUPS 2013

- Designing
to make genuine

security-decision UIs
risks harder to ignore

BrCrKo2014 Harder to Ignore - Revisiting Pop-up Fa-

SOUPS 2014

BruVil2007

BrGrSt2011

tigue and Approaches to Prevent it
Indirect content privacy surveys - measur-
ing privacy without asking about it
Improving Security Decisions with Poly-
morphic and Audited Dialogs
BrViDj2008 Evaluating the Usability of Usage Controls
in Electronic Collaboration
BuBeFa2010 How good are Humans

at Solving

SOUPS 2011

SOUPS 2007

SOUPS 2008

S&P

2010 ∅

CAPTCHAs - A Large Scale Evaluation

BuBePa2011 The

failure

of Noise-Based Non-

S&P

2011

Continuous Audio Captchas
BuWoVo2014 Introducing Precautionary Behavior by
Temporal Diversion of Voter Attention
from Casting to Verifying their Vote

USEC 2014

CaMiVa2016 Hidden Voice Commands

USENIX 2016 ∅
Continued on next page

Table 8 – Sample. Continued from previous page

Tag

Tilte

CaoIve2006

Intentional Access Management - Making
Access Control Usage for End-Users

Venue Year Ex.

SOUPS 2006

ChBiOr2007 A second look at the usability of click-
based graphical passwords
ChBoKa2014 On the Eﬀectiveness of Obfuscation Tech-

SOUPS 2007

PETS

2014

niques in Online Social Networks

ChChBa2015 You shouldnt collect my secrets - Thwart-
ing sensitive keystroke leakage in mobile
IME apps

USENIX 2015

ChMuAs2015 On the impact of touch id on iphone pass-

SOUPS 2015

codes

ChObSt2009 Sanitizations slippery slope- the design and

SOUPS 2009 ∅

study of a text revision assistant

ChPoSe2012 Measuring user conﬁdence in smartphone

SOUPS 2012

security and privacy

ChStFo2012 Persuasive

cued click-points

- Design
a
implementation and evaluation of
knowledge-based authentication mecha-
nism

TDSC 2012

CzDeYa2010 Parenting from the pocket - Value tensions
and technical directions for secure and pri-
vate parent-teen mobile safety

DaKrDa2014 Increasing security sensitivity with social
proof - A large-scale experimental conﬁr-
mation

SOUPS 2010

CCS

2014

DaPuRa2012 Impact of spam exposure on user engage-

USENIX 2012

ment

DewKul2006 Aligning usability and security - a usability

SOUPS 2006

study of Polaris

DuHeAs2010 A closer look at recognition-based graphi-

SOUPS 2010

cal passwords on mobile devices

DuNiOl2008 Securing passfaces for description
EgJaPo2014 Are you ready to lock
FaFeSh2015 Anatomization and Protection of Mobile

SOUPS 2008
2014
USENIX 2015

CCS

Apps Location Privacy Threats

FaHaAc2013 On the ecological validity of a password

SOUPS 2013

study

FaHaMu2012 Helping Johnny 2.0 to encrypt his Face-

SOUPS 2012

book conversations

FoChOo2008 Improving text passwords through persua-

SOUPS 2008

sion

GaCaCo2012 Risk communication design - video vs. text PETS
GaCaMa2011 Designing risk communication for older

2012
SOUPS 2011

adults

GaChLi2014 Eﬀective risk communication for android

TDSC 2014

apps

Continued on next page

Table 8 – Sample. Continued from previous page

Tag

Tilte

Venue Year Ex.

GawFel2006 Password management strategies for online

SOUPS 2006

accounts

GiEgCr2006 Power Streip Prophylactics and Privacy

SOUPS 2006

Oh My

GrCoAl2016 Eﬀect of cognitive depletion on password

LASER 2016

GroBar2014

choice
Social status and the demand for security
and privacy

PETS

2014

HaChDh2008 Use your illusion- secure authentication us-

SOUPS 2008 ∅

able anywhere

HaChHa2009 New directions in multisensory authentica-

SOUPS 2009

tion

HaCrKl2014 Targeted threat index - Characterizing and
quantifying politically-motivated targeted
malware

HaDeSm2015 Where Have You Been - Using Location-
Based Security Questions for Fallback Au-
thentication

HaRiSt2012 Goldilocks and the two mobile devices - go-
ing beyond all-or-nothing access to a de-
vices applications

HaScWr2014 Applying psychometrics to measure user
comfort when constructing a strong pass-
word
Its a hard lock life - A ﬁeld study of smart-
phone un-locking behavior and risk percep-
tion

HaZeFi2014

USENIX 2014

SOUPS 2015

SOUPS 2012

SOUPS 2014

SOUPS 2014

HuMoWa2012 Clickjacking - attacks and defenses
HuOhKi2015 Surpass - System-initiated user-replaceable

USENIX 2012
2015

CCS

passwords

JaRaBe2014 To authorize or not authorize - helping
users review access policies in organiza-
tions
Tracking website data-collection and pri-
vacy practices with the iWatch web crawler

JeSaJe2007

SOUPS 2014

SOUPS 2007

JoEgBe2012 Facebook and privacy - its complicated
JusAsp2009 Personal choice and challenge questions - a

SOUPS 2012
SOUPS 2009

security and usability assessment

KaBrDa2014 Privacy Attitudes of Mechanical Turk
Workers and the US Public
KaFlRo2010 Two heads are better than one - secu-
rity and usability of device associations in
group scenarios

SOUPS 2014

SOUPS 2010

KaMaSo2015 Sound-proof - Usable two-factor authenti-

USENIX 2015

cation based on ambient sound

Continued on next page

Table 8 – Sample. Continued from previous page

Tag

Tilte

KaTyWa2009 Conditioned-Safe Ceremonies and a User
Study of an Application to Web Authenti-
cation

Venue Year Ex.

SOUPS 2009

KayTer2010 Textured agreements - re-envisioning elec-

SOUPS 2010

tronic consent
KeBrCr2009 A nutrition label for privacy
KeCaLi2012

Self-identiﬁed experts lost on the interwebs
- The importance of treating all results as
learning experiences

KhHeVo2015 Usability and security perceptions of im-
plicit authentication - Convenient secure
sometimes annoying

SOUPS 2009
LASER 2012

SOUPS 2015

KilMax2012 Free vs. transcribed text for keystroke-

LASER 2012

dynamics evaluations

KluZan2009 Balancing usability and security in a video

SOUPS 2009 ∅

CAPTCHA

KorBoh2014 Too Much Choice - End-User Privacy De-
cisions in the Context of Choice Prolifera-
tion

KoShCr2014 Telepathwords - Preventing weak pass-
words by reading users minds
KoSoTs2009 Serial hook-ups - a comparative usability

study of secure device pairing methods

SOUPS 2014

SOUPS 2014

SOUPS 2009

KrHuHo2016 Use the Force- Evaluating Force-Sensitive

SOUPS 2016

Authentication for Mobile Devices

KuCrAc2009 School of phish - a real-world evaluation of

SOUPS 2009 ∅

anti-phishing training

KuRoCr2006 Human selection of mnemonic phrase-

SOUPS 2006

based passwords

LeMoPe2016 Privacy Challenges in the Quantiﬁed Self

PETS

2016

Movement - An EU Perspective

LiAnSc2016 Follow my recommendations - A personal-
ized privacy assistant for mobile app per-
missions

SOUPS 2016

LiAsCa2008 Risk communication in security using men-

USEC 2008

tal models
LiBrYe2011 Demographic Proﬁling
Gameplay

from MMOG

PETS

2011

LiLiSa2014 Modeling users’ mobile app privacy prefer-
ences - Restoring usability in a sea of per-
mission settings
Smartening the crowds- computational
techniques for improving human veriﬁca-
tion to ﬁght phishing scams
LlPoAt2015 Face-oﬀ - Preventing Privacy Leakage

LiXiPe2011

From Photos in Social Networks

SOUPS 2014

SOUPS 2011

CCS

2015

Continued on next page

Table 8 – Sample. Continued from previous page

Tag

Tilte

Venue Year Ex.

MaDeKe2011 Using data type based security alert di-

SOUPS 2011

alogs to raise online security awareness

MaLeAd2012 The PViz comprehension tool for social

SOUPS 2012

MalPre2013

network privacy settings
Sign-up or give-up- Exploring user drop-
out in web service registration

MoGaSa2014 Dynamic cognitive game captcha usability
and detection of streaming-based farming
MohaBe2010 Do windows users follow the principle of
least privilege - investigating user account
control practices

SOUPS 2013

USEC 2014

SOUPS 2010

MoLiVi2014 Understanding and specifying social access

SOUPS 2014

control lists

NoBlCa2014 Why Johnny Cant Blow the Whistle -
Identifying and Reducing Usability Issues
in Anonymity Systems

USEC 2014

PanCut2010 Usably secure low-cost authentication for

SOUPS 2010

mobile banking

PaNoKa2012 Reasons rewards regrets - privacy consider-
ations in location sharing as an interactive
practice

PanPra2014 Crowdsourcing attacks on biometric sys-

tems

PeKoBu2014 Cloak and swagger - Understanding data
sensitivity through the
lens of user
anonymity

SOUPS 2012

SOUPS 2014 ∅

S&P

2014

PoHaEg2012 Android permissions - User attention com-

SOUPS 2012

prehension and behavior

PoIlMa2014 Faces in the distorting mirror- Revisiting

CCS

2014

photo-based social authentication

PuGros2015 Towards a Model on the Factors Inﬂuenc-
ing Social App Users Valuation of Interde-
pendent Privacy

RaBoJa2014 To befriend or not - a model of friend re-
quest acceptance on facebook
RaDeGr2016 Privacy Wedges- Area-Based Audience Se-

Rader2014

lection for Social Network Posts
Awareness of Behavioral Tracking and In-
formation Privacy Concern in Facebook
and Google

PETS

2015

SOUPS 2014

SOUPS 2016

SOUPS 2014

RaHaBe2009 Revealing hidden context- improving men-

SOUPS 2009

tal models of personal ﬁrewall users

RajCam2016 Inﬂuence of Privacy Attitude and Privacy

SOUPS 2016

Cue Framing on Android App Choices

RaWaBr2012 Stories as informal lessons about security SOUPS 2012

Continued on next page

Table 8 – Sample. Continued from previous page

Tag

Tilte

Venue Year Ex.

ReKrMa2016 How I Learned to be Secure- a Census-
Representative Survey of Security Advice
Sources and Behavior

CCS2016

RiBoMo2016 Measuring the inﬂuence of perceived cy-

TDSC 2016

RiQiSt2012

bercrime risk on online service avoidance
Progressive authentication- deciding when
to authenticate on mobile phones

RoCuJo2014 Behavioral Experiments Exploring Victims
Response to Cyber-based Financial Fraud
and Identity Theft Scenario Simulations

USENIX 2012 ∅

SOUPS 2014

RuKiBu2013 Confused Johnny- when automatic encryp-

SOUPS 2013

tion leads to confusion and mistakes

RuOnYo2016 User Attitudes Toward the Inspection of

SOUPS 2016

Encrypted Traﬃc

SchBon2015 Learning assigned secrets for unlocking

SOUPS 2015

SchRee2009

mobile devices
1 plus 1 equal you- measuring the com-
prehensibility of metaphors for conﬁguring
backup authentication

ScMcPa2011 Empowering end users to conﬁne their own
applications - The results of a usability
study comparing SELinux AppArmor and
FBAC-LSM

ScWaKo2013 Exploring the design space of graphical
passwords on smartphones
ShBeRo2016 Behavioral Study of Users When Interact-
ing with Active Honeytokens
ShKeKo2012 Correct horse battery staple- Exploring the

usability of system-assigned passphrases

SOUPS 2009

TISSEC 2011

SOUPS 2013

TISSEC 2016

SOUPS 2012

ShKoDu2016 Designing Password Policies for Strength

TISSEC 2016

and Usability
ShKoKe2010 Encountering

password
requirements- user attitudes and be-
haviors

stronger

SOUPS 2010

ShKrVi2015 Portrait of a Privacy Invasion
ShKuSe2014 Beware your hands reveal your secrets
ShMaKo2007 Anti-phishing phil- the design and evalua-
tion of a game that teaches people not to
fall for phish
SmeGoo2009 How users use access control
StHuBr2012 Are privacy concerns a turn-oﬀ- engage-

ment and privacy in social networks

PETS
CCS

2015 ∅
2014 ∅

SOUPS 2007

SOUPS 2009 ∅
SOUPS 2012

StoBid2013 Memory retrieval and graphical passwords SOUPS 2013
USENIX 2009
SuEgAl2009 Crying Wolf - An Empirical Study of SSL

Warning Eﬀectiveness

Continued on next page

Table 8 – Sample. Continued from previous page

Tag

Tilte

TaOzHo2006 A comparison of perceived and real
shoulder-surﬁng risks between alphanu-
meric and graphical passwords

ThLiCh2016 What Questions Remain - An Examination
of How Developers Understand an Interac-
tive Static Analysis Tool

UrKeKo2012 How does your password measure up - the
eﬀect of strength meters on password cre-
ation
Balancing privacy concerns and impression
management strategies on Facebook

VItak2015

WaGeCh2016 On the Security and Usability of Segment-
based Visual Cryptographic Authentica-
tion Protocols

WaRaBe2016 Understanding Password Choices - How
Frequently Entered Passwords are Re-used
Across Websites

WrPaBi2012 Do you see your password- applying recog-
nition to textual passwords
WuMiLi2006 Web wallet- preventing phishing attacks by

XuReCh2012 Security

revealing user intentions
and usability

moving-object CAPTCHAs-
codewords in motion

challenges

of
decoding

Venue Year Ex.

SOUPS 2006

SOUPS 2016 ∅

USENIX 2012

SOUPS 2015

CCS

2016

SOUPS 2016

SOUPS 2012

SOUPS 2006

USENIX 2012

YaLiCh2016 An Empirical
Sentence-based
Strategies

Study
of Mnemonic
Password Generation

CCS

2016

YeHeOp2014 An epidemiological study of malware en-
counters in a large enterprise
ZhPaWa2016 An Eﬃcient User Veriﬁcation System Us-
ing Angle-Based Mouse Movement Bio-
metrics

ZhWaJi2014 Privacy Concerns in Online Recommender
Systems- Inﬂuences of Control and User
Data Input

CCS

2014

TISSEC 2016 ∅

SOUPS 2014

D Contingency Tables

We include a number of contingency tables on the distribution of papers and
test results per venue and year. Table 9 shows the distribution of the sample,
that is, included papers by venue and year.

Tables 12 and 13 contain the statcheck outcomes aggegated per paper, by

venue and year, respectively.

Tables 10 and 11 show the corresponding statcheck results for individual

tests, by venue and year, respectively.

Table 9. Sample composition by venue and year.

9
0
0
2

3
1
0
2

6
1
0
2

8
0
0
2

4
1
0
2

0
1
0
2

5
1
0
2

2
1
0
2

7
0
0
2

6
0
0
2

1
m
1
u
0
S
2
SOUPS 6 3 4 6 8 4 10 8 13 9 6 77
4
8
7
6
3
2
2
4
1
Sum 7 4 4 7 8 6 17 9 24 13 15 114

USEC 0 0 0 0 0 0 0 0 4 0 0
CCS 0 0 0 0 0 0 0 0 4 1 3
USENIX 0 0 0 1 0 0 4 1 1 0 0
PETS 1 0 0 0 0 1 1 0 0 1 2
TISSEC 0 0 0 0 0 1 0 0 0 0 2
LASER 0 0 0 0 0 0 1 0 0 0 1
S&P 0 0 0 0 0 0 0 0 1 1 0
TDSC 0 1 0 0 0 0 1 0 1 0 1
WEIS 0 0 0 0 0 0 0 0 0 1 0

Table 10. Contingency table of individual test statcheck outcomes by venue, FET
p = .033.

I

S
P
U
O
S

C
E
S
U
CorrectNHST 170 1
19 1
Inconsistency
9 0
DecisionError

X
R
N
S
E
I
S
E
S
E
C
A
S
W
U
C
L
9
4 11 6 5 0 12 0
3
0 0 0 1 0 0 0
0 0 0 1 0 0 0
0
Incomplete 1028 33 122 100 72 71 19 11 60 7

C
E
S
S
I
T

C
S
D
T

S
T
E
P

P
&
S

E Statistics Tools

We used R (version 3.4.1), with statcheck [8] (version 1.3.0).

We calculated p-values for t, F , χ2, and Z test statistics with the R functions

pt(), pf(), pchisq(), and pz(), respectively4.

We computed the waﬄe plots with the R package waﬄe [24] (version 0.7.0).
We computed the multinomial logistic regression with the R package nnet
(version 7.3-12) using lmtest (version 0.9-35) for the likelihood-ratio tests. We
used John Fox’s package car [12,11] for regression diagnostics (version 2.1-5). To
display the multinomial logistic regressions over time we used the R function
polytomous eﬀects, which was originally developed by John Fox for the eﬀect
display of multinomial odds [10].

F Root Causes for Unparseable Papers

One paper could not be parsed by statcheck because its p-values were incon-
sistently reported (partially as capital P, partially as capital “Pr”, “Pr < t”
etc.).

4 We calculate pz() as pz = 2 ∗ pnorm(−abs(z )).

Table 11. Contingency table of individual test statcheck outcomes by year, FET p <
.001.

0
1
0
2

8
0
0
2

1
1
0
2

9
0
0
2

6
0
0
2

7
2
0
1
0
0
2
2
CorrectNHST 13 24 14 18 26 13 22
2
Inconsistency 2 1 0
1
DecisionError 0 5 0

3
6
1
1
0
0
2
2
9 37 28 14
5
4
1
1
Incomplete 53 57 28 105 96 59 347 123 270 170 215

1 2 0
1 1 0

5
1
0
2

4
1
0
2

3
0

4
0

Table 12. Contingency table of aggregated paper statcheck outcomes by venue, FET
p = .964.

S
P
U
O
S

I

X
N
E
S
U

C
E
S
S
I
T

R
E
S
A
L

C
S
D
T

S
C
C

S
T
E
P

C
S
I
E
E
S
W
U
CorrectNHST 19 0 1 2 3 1 0 0 1 0
Inconsistency 10 1 1 0 0 0 0 0 0 0
DecisionError 5 0 0 0 0 0 1 0 0 0
Incomplete 43 3 6 5 3 2 1 2 3 1

P
&
S

One paper gave a p-value as the Greek letter rho (ρ), which was embedded

in the PDF as image.

One paper only reported statistics, but unparseable as the test statistics were
not transcribed to text. The tables of χ2-results as well as in-text statistics were
embedded as bitmaps. We resorted to consulting the publisher’s page for an
HTML that could be translated to text.

Three papers wrote out equations as text, e.g., “Chi-Sq = . . . ” or “p-value

= . . . ”

One paper only reported regression tables with signiﬁcance codes, but no

p-values.

One paper had a single statistical statement, which could not be parsed.

G All Multinomial Logistic Regressions Conducted

The primary regression tables are available in Figures 14, 15, and 16.

Reporting of Test Statistics. We analyzed statcheck outcome for all statistical
tests found (N = 1775) by venue and year. We conducted a multinomial logistic
regression on SCOutcome per test.

Having conducted a likelihood-ratio test between the venue+year model and
the null model, the overall model is statistically signiﬁcant, χ2(30) = 90.713, p <
.001. The model explains McFadden R2= .05 of the variance.

The corresponding predictors are statistically signiﬁcant as well. Hence, we

reject the null hypotheses HV,0 and HY,0.

Table 13. Contingency table of aggregated paper statcheck outcomes by year, FET
p = .458.

4
1
0
2

6
0
0
2

7
0
0
2

9
0
0
2

0
1
0
2

1
1
0
2

3
1
0
2

2
1
0
2

8
6
0
1
0
0
2
2
CorrectNHST 2 1 1 2 2 4 5 1 5 3 1
Inconsistency 2 0 0 1 1 0 0 0 3 3 2
DecisionError 0 1 0 1 1 0 1 1 0 0 1
Incomplete 3 2 3 3 4 2 11 7 16 7 11

5
1
0
2

Table 14. MLR coeﬃcients for CorrectNHST.

b

SE z-value

p-Value

OR

LL

UL

(Intercept) 4.805 0.000 31592.632 < .001*** 122.086 122.049 122.122
0.997
0.828

Year -0.003 0.000 -79.882 < .001***
.001**

VenueOTHER -0.526 0.172 -3.056

0.997
0.591

0.997
0.422

Figure 10 on p. 40 contains an overview of the scatter plot vs. the predicted

probabilities from the MLR in the top pane (10a).

Reporting Excluding Incomplete Test Statistics. We note that the cases with
incomplete test statistics dominate the analysis of the MLR on tests. We, there-
fore, conduct a second MLR solely on tests with complete test statistics triplets.
A likelihood-ratio test venue+year vs. null shows that the model is marginally
statistically signiﬁcant, χ2(16) = 24.491, p = .079, McFadden R2= .10.

The predicted probabilities are shown in the middle pane (10b) of Figure 10

on p. 40 along with the corresponding scatter plot, for information.

Reporting per Paper. We conducted an analysis of the statcheck outcome per
paper by year. For that, we have aggregated the statcheck results for each paper
and then conducted a multinomial logistic regression on the aggregate N = 114.
Testing for the overall signiﬁcance by a likelihood-ratio test between the
designated model and the null model, we ﬁnd the overall model non-signiﬁcant,
χ2(3) = 3.331, p = .343, McFadden R2= .01.

The scatterplot for this analysis and the predicted probabilities are shown in

the bottom pane (10c) of Figure 10 on p. 40, for information.

Table 15. MLR coeﬃcients for Inconsistency.

b

SE z-value

p-Value OR LL UL

(Intercept) -0.349 0.000 -176649.306 < .001*** 0.705 0.705 0.705
< .001*** 0.998 0.998 0.998
< .001*** 0.531 0.529 0.534

Year -0.002 0.000 -17.656

VenueOTHER -0.633 0.002 -284.961

Table 16. MLR coeﬃcients for DecisionError.

b

SE z-value

p-Value OR LL UL

(Intercept) 0.633 0.000 1477755.342 < .001*** 1.883 1.883 1.883
< .001*** 0.997 0.997 0.998
VenueOTHER -1.464 0.000 -3144.058 < .001*** 0.231 0.231 0.231

Year -0.003 0.000 -16.893

(a) All tests analyzed, incl. with incomplete test statistics, MLR signiﬁcant, χ2(30) =
90.713, p < .001

(b) Tests analyzed, excluding Incomplete test statistics, MLR not signiﬁcant, χ2(16) =
24.491, p = .079

(c) Results aggregated for papers, MLR not signiﬁcant, χ2(3) = 3.331, p = .343

Fig. 10. Comparison of scatter plots and corresponding multinomial logistic regression
(MLR, 95% conﬁdence bands) of statcheck results via polytomous eﬀects [10]. We con-
sider (10a) reported tests, (10b) complete test statistics only, and (10c) aggregates for
papers; (10a) and (10b) by venue and year; (10c) is by year only.

CorrectNHSTInconsistencyDecisionErrorIncomplete20062007200820092010201120122013201420152016YearSCOutcomeVenueSOUPSUSECCCSUSENIXPETSTISSECLASERS&PTDSCWEISCorrectNHSTInconsistencyDecisionErrorIncomplete2006200820102012201420160%25%50%75%100%0%25%50%75%100%0%25%50%75%100%0%25%50%75%100%YearProbabilityVenueSOUPSUSECCCSUSENIXPETSTISSECLASERS&PTDSCWEISCorrectNHSTInconsistencyDecisionError20062007200820092010201120122013201420152016YearSCOutcomeVenueSOUPSUSECCCSUSENIXPETSTISSECLASERTDSCCorrectNHSTInconsistencyDecisionError2006200820102012201420160%25%50%75%100%0%25%50%75%100%0%25%50%75%100%YearProbabilityVenueSOUPSUSECCCSUSENIXPETSTISSECLASERCorrectNHSTInconsistencyDecisionErrorIncomplete20062007200820092010201120122013201420152016YearSCOutcomeVenueSOUPSUSECCCSUSENIXPETSTISSECLASERS&PTDSCWEISCorrectNHSTInconsistencyDecisionErrorIncomplete2006200820102012201420160%25%50%75%100%0%25%50%75%100%0%25%50%75%100%0%25%50%75%100%YearProbability