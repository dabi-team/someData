0
2
0
2

b
e
F
7
1

]
T
G
.
s
c
[

1
v
5
2
0
7
0
.
2
0
0
2
:
v
i
X
r
a

Secure-by-synthesis network with active deception
and temporal logic speciﬁcations

Jie Fu∗, Abhishek N. Kulkarni ∗, Huan Luo†, Nandi O. Leslie ‡, and Charles A. Kamhoua ‡
∗ †Dept. of Electrical and Computer Engineering,
Robotics Engineering Program,
Worcester Polytechnic Institute, MA, US
∗jfu2, ankulkarni@wpi.edu, †hluo12@126.com
‡ U.S. Army Research Laboratory, MD, US
charles.a.kamhoua.civ, nandi.o.leslie.ctr@mail.mil

Abstract—This paper is concerned with the synthesis of strate-
gies in network systems with active cyber deception. Active
deception in a network employs decoy systems and other defenses
to conduct defensive planning against the intrusion of malicious
attackers who have been conﬁrmed by sensing systems. In this
setting, the defender’s objective is to ensure the satisfaction
of security properties speciﬁed in temporal logic formulas. We
formulate the problem of deceptive planning with decoy systems
and other defenses as a two-player games with asymmetrical
information and Boolean payoffs in temporal logic. We use level-
2 hypergame with temporal logic objectives to capture the in-
complete/incorrect knowledge of the attacker about the network
system as a payoff misperception. The true payoff function is
private information of the defender. Then, we extend the solution
concepts of ω-regular games to analyze the attacker’s rational
strategy given her incomplete information. By generalizing the
solution of level-2 hypergame in the normal form to extensive
form, we extend the solutions of games with safe temporal logic
objectives to decide whether the defender can ensure security
properties to be satisﬁed with probability one, given any possible
strategy that is perceived to be rational by the attacker. Further,
we use the solution of games with co-safe (reachability) temporal
logic objectives to determine whether the defender can engage
the attacker, by directing the attacker to a high-ﬁdelity honeypot.
The effectiveness of the proposed synthesis methods is illustrated
with synthetic network systems with honeypots.

I. INTRODUCTION

In networked systems, many vulnerabilities may remain in
the network even after being discovered, due to the delay
in applying software patches and the costs associated with
removing them. In the presence of such vulnerabilities, it is
critical to design network defense strategies that ensure the
security of the network system with respect to complex, high-
level security properties. In this paper, we consider the prob-
lem of automatically synthesizing defense strategies capable of
active deception that satisfy the given security properties with
probability one. We represent the security properties using
temporal logic, which is a rich class of formal languages
to specify correct behaviors in a dynamic system [18]. For
instance, the property, “the privilege of an intruder on a given

This material is based upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Agreement No. HR00111990015.
† Huan Luo is a visiting student with Dr. Jie Fu at the Worcester Polytechnic

Institute from Sept to Nov, 2019.

is always lower than the root privilege,” is a safety
host
property that asserts that the property must be true at all times.
The property, “it is the always the case that eventually all
critical hosts will be visited,” is a liveness property, which
states that something good will always eventually happen.

In the past, formal veriﬁcation, also known as model check-
ing, has been employed to verify the security properties of
network systems, expressed in temporal logic [14], [19], [23].
These approaches construct a transition system that captures
all possible exploitation by (malicious or legitimate) users in
a given network. Then, a veriﬁcation/model checking tool is
used to generate an attack graph as a compact representation
to
of all possible executions that an attacker can exploit
violate safety and other critical properties of the system.
By construction, the attack graph captures multi-step attacks
that may exploit not only an individual vulnerability but also
multiple vulnerabilities and their causal dependencies. Using
an attack graph, the system administrator can perform analysis
of the risks either ofﬂine or at run-time.

However, veriﬁcation and risk analysis with attack graphs
have limitations: they do not take into account the possible
defense mechanisms that can be used online by a security
system during an active attack. For example, once an attacker
is detected, the system administrators may change the network
topology online (using software-deﬁned networking) [17] or
activate decoy systems and ﬁles. Given increasingly advanced
cyber attacks and defense mechanisms, it is desirable to syn-
thesize, from system speciﬁcation, the defense strategies that
can be deployed online against active and progressive attacks
to ensure a provably secured network. Motivated by this need,
we present a game-theoretic approach to synthesize reactive
defense strategies with active deception. Active deception
employs decoy systems and ﬁles in synthesizing proactive
security strategies, assuming a malicious attacker has been
detected by the sensing system [27].

Game theory has been developed to model and analyze
defense deception in network security (see a recent survey in
[21]). Common models of games have been used in security
include Stackelberg games, Nash games, Bayesian games, in
which reward or loss functions are introduced to model the
payoffs of the attacker and the defender. Given the reward

1

 
 
 
 
 
 
(resp. loss) functions, the player’s strategies are computed by
maximizing (resp. minimizing) the objective function [8], [11],
[13]. Carrol and Grosu [5] used a signaling game to study
honeypot deception. The defender can disguise honeypots as
real systems and real systems as honeypots. The attacker can
waste additional resources to exploits honeypots or to deter-
mine whether a system is a true honeypot or not. The solution
of perfect Bayesian equilibrium provides the defend/attack
strategies. Huang and Zhu [2] used dynamic Bayesian games
to solve for defense strategies with active deception. They
considered both one-sided incomplete information, where the
defender has incomplete information about the type of the at-
tacker (legitimate user or adversary), and two-sided incomplete
information, where the attacker also is uncertain about the
type of the defender (high-security awareness or low-security
awareness). Based on the analysis of Nash equilibrium, the
method enables the prediction of the attacker’s strategy and
proactive defense strategy to mitigate losses.

Comparing to the existing quantitative game-theoretic ap-
proach, game-theoretic modeling, and qualitative analysis with
Boolean security properties have not been developed for
deception and network security. The major difference between
quantitative and qualitative analysis in games lies in the deﬁni-
tions of the payoff function. Instead of minimizing loss/max-
imizing rewards studied in prior work, the goal of qualitative
reasoning is to synthesis a security policy that ensures, with
probability one (i.e., almost-surely), given security properties
are satisﬁed in the network during the dynamic interactions
between the defender and the attacker.

To synthesize provably secured networks with honeypot
deception, we develop a game-theoretic model called “ω-
regular hypergame”, played between two players: the defender
and the attacker. A hypergame is a game of games, where
the players play their individual perceptual game, constructed
using the information available to each player. Similar to
[2], we assume that the attacker has incomplete information
about the game and is not aware of the deployment of decoy
systems. However, both the defender and attacker are aware
of each other’s actions and temporal logic objectives (also
known as ω-regular objectives). Therefore, the defender and
the attacker play different games, which together deﬁne the
ω-regular hypergame.

To solve for active deception strategies in the ω-regular hy-
pergame, we ﬁrst construct the game graphs of the individual
games being played by the attacker and defender. In this con-
text, a game is deﬁned using three components: (a) a transition
system, called arena, which captures all possible interactions
over multiple stages of the game; (b) a labeling function that
relates an outcome–a sequence of states in the game graph–to
properties speciﬁed in logic; and (c) the temporal logic speci-
ﬁcations as the players’ Boolean objectives. The construction
of the transition system is closely related to the attack graph as
it captures the causal dependency between the vulnerabilities
in the network. The only difference is that in the attack graph
analysis, the transitions are introduced by the attacker’s moves
only, whereas in the game transition system the transitions

may be triggered by both attacker’s and the defender’s actions.
Cyber deception is introduced through payoff manipulation:
when the attacker does not know which hosts are decoys, the
attacker might misperceive herself to be winning if the security
properties of the defender have been violated, when in fact,
they are not. We deﬁne a hypergame transition system for the
defender to synthesize 1) the rational, winning strategy of the
attacker given the attacker’s (mis)perception of the game; and
then 2) the deceptive security strategies for the defender that
exploits the attacker’s perceived winning strategy.

The paper is structured as follows. In Sec. II, we present
the deﬁnition of ω-regular games, and show how such a game
can be constructed from a network system. In Sec. III, we
formulate a modeling framework called “cyber-deception ω-
regular game” and show how to capture the use of decoys
as a payoff manipulation mechanism in such a game. In
the solution for the cyber-deception
Sec. IV, we present
ω-regular game with asymmetrical
information. Using the
solution of the game, a defender’s strategy, if one exists, can be
synthesized to ensure that the security properties are satisﬁed
with probability one. This strategy uses both active deception
and reactive defense mechanisms. Further, we analyze whether
a strategy exists to ensure that the defender can achieve a
preferred outcome, for example, forcing the attacker to visit a
honeypot eventually. Finally, we use examples to illustrate the
methods and effectiveness of the synthesized security strategy.
In Sec. VI we conclude and discuss potential future directions.

II. PRELIMINARIES AND PROBLEM FORMULATION

Notation: Given a set X, the set of all possible distributions
over X is denoted D(X). For a ﬁnite set X, the powerset
(set of subsets) of X is denoted 2X . For any distribution
d ∈ D(X), the support of d, denoted Supp(d), is the set of
elements in X that has a nonzero probability to be selected by
the distribution, i.e., Supp(d) = {x ∈ X | d(x) > 0}. Let Σ be
an alphabet, a sequence of symbols w = σ0σ1 . . . σn, where
σi ∈ Σ, is called a ﬁnite word and Σ∗ is the set of ﬁnite
words that can be generated with alphabet Σ. We denote Σω
the set of words obtained by concatenating the elements in
Σ inﬁnitely many times. Notations used in this paper can be
found in the nomenclature.

A. The game arena of cyber-deception game

Attack graph is a formalism for automated network security
analysis. Given a network system, its mathematical model
can be constructed as a ﬁnite-state transition system which
includes a set of states describing various network conditions,
the set of available actions that can be performed by the
defender or the attacker to change the network conditions,
their own states, and transition function that captures the pre-
and post-conditions of actions given states. A pre-condition is
a logical formula that needs to be satisﬁed for an action to
be taken. A post-condition is a logical formula describes the
effect of an action in the network system. Various approaches
to attack graph generation have been proposed (see a recent
survey [1]).

2

The attacker takes actions to exploit

the network, such
as remote control exploit, escalate privileges, and stop/start
services on a host under attack. When generating the attack
graph, we also introduce a set of defender’s actions, enabled
by defensive software such as ﬁrewalls, multi-factor user
authentication, software-deﬁned networking to remove some
vulnerabilities in real-time. After incorporating both defender’s
and attacker’s moves, we obtain a two-player game arena in
the form of a deterministic transition system in Def. 1. In this
game, we refer the defender to be player 1, P1 (pronoun ‘he’)
and the attacker to be player 2, P2 (pronoun ‘she’).

Deﬁnition 1. A turn-based game arena consists of a tuple

G = (cid:104)S, A, T, AP, L(cid:105),

where:

• S = S1 ∪ S2 is a ﬁnite set of states partitioned into P1’s

states S1 and P2’s states S2;

• A1 (resp., A2) is the set of actions for P1 (resp., P2);
• T : (S1×A1)∪(S2×A2) → S is a deterministic transition
function that maps a state-action pair to a next state.

• AP is the set of atomic propositions.
• L : S → 2AP is the labeling function that maps each
state s ∈ S to a set L(s) ⊆ AP of atomic propositions
evaluated true at that state.

The set of atomic propositions and labeling function to-
gether enable us to specify the security properties using logical
formulas. A path ρ = s0s1 . . .
in the game arena is a
(ﬁnite/inﬁnite) sequence of states such that for any i ≥ 0,
there exists a ∈ A1 ∪ A2, δ(si, a) = si+1. A path ρ = s0s1 . . .
can be mapped to a word in 2AP , w = L(s0)L(s1) . . ., which
is evaluated against the pre-deﬁned security properties in logic.
In this work, we consider deterministic, turn-based game
arena, i.e., at any step, either the attacker or the defender takes
an action and the outcome of that action is deterministic. This
turn-based interaction can be understood as if the defense is
reactive against attacker’s exploits. The turn-based interaction
has been adapted in cyber-security research [6]. The general-
ization to concurrent stochastic game arena is a part of our
future work.

The model of game arena is generic enough to capture many
attack graphs generated by different approaches. For example,
consider the lateral movement attack, the state s can include
information about the source host of the attacker, the user
credential obtained by the attacker, and the current network
condition–the active hosts and services. Depending on the pre-
and post- conditions of each vulnerability, the attacker can
select a vulnerability to exploit. For example, a vulnerability
named “IIS overﬂow” requires the attacker to have a creden-
tial of user or a root, and the host running IIS webserver [26].
After exploiting the vulnerability, the attacker moves from the
source host to a target host, changes her credential and the
network condition. For example, after the attacker exploits the
vulnerability of “IIS overﬂow”, the service will be stopped on
the target host and the attacker gains the credential as a root
user. Then the defender may choose to stop services on a host

or to change the network connectivity, or to replace a current
host with a decoy as defense actions. These defense actions
again change the network status–resulting in a transition in the
arena.

B. The payoffs in the game

We consider that

the defender’s objective is to satisfy
security properties of the system, speciﬁed in temporal logic.
The attacker’s objective is to violate security properties of the
system. We assume the security properties are common knowl-
edge between the attacker and the defender, corresponding to
the worst case assumption of the attacker. Next, we give the
formal syntax and semantics of Linear Temporal Logic (LTL)
and then several examples related to network security analysis.
Let AP be a set of atomic propositions. Linear Temporal

Logic (LTL) has the following syntax,

ϕ := (cid:62) | ⊥ | p | ϕ | ¬ϕ | ϕ1 ∧ ϕ2 | (cid:13)ϕ | ϕ1 U ϕ2,

where

• (cid:62), ⊥ are universally true and false, respectively.
• p ∈ AP is an atomic proposition.
• (cid:13) is a temporal operator called the “next” operator (see

semantics below).

• U is a temporal operator called the “until” operator (see

semantics below).

Let Σ := 2AP be the ﬁnite alphabet. Given a word w ∈ Σω,
let w[i] be the i-th element in the word and w[i . . .] be the
subsequence of w starting from the i-th element. For example,
w = abc, w[0] = a and w[1 . . .] = bc. Formally, we have the
following deﬁnition of the semantics:

• w |= p if p ∈ w[0];
• w |= ¬p if p /∈ w[0];
• w |= ϕ1 ∧ ϕ2 if w |= ϕ1 and w |= ϕ2.
• w |= (cid:13)ϕ if w[1 . . .] |= ϕ.
• w |= ϕ U ψ if ∃i ≥ 0, w[i . . .] |= ψ and ∀0 ≤ j < i,

w[j . . .] |= ϕ.

From these two temporal operators ((cid:13), U ), we deﬁne two
additional temporal operators: ♦ eventually and (cid:3) always.
Formally, ♦ ϕ means true U ϕ. (cid:3) ϕ means ¬♦ ¬ϕ. For details
about the syntax and semantics of LTL, the readers are referred
to [22].

Next, we present some examples of LTL formulas for
describing security properties in a network. Consider a set of
atomic propositions AP = {p1, p2, p3, p4}, where

• p1: service A is enabled on host 1.
• p2: the attacker has the root privilege on a host.
• p3: the attacker is on host 1.
• p4: the attacker is on host 2.

Using the set of atomic propositions, the following security
properties or attacker’s objectives can be described:

• (cid:3) p1: “Service A will always be enabled on host 1”.
• (cid:3) ♦ p1: “It is always the case that service A will be

eventually enabled on host 1.”

• ♦ (p3 ∧ p2): “Eventually the attacker reaches host 1 with

a root privilege”.

3

• (p1 ∧ p3 ∧ p2) =⇒ (cid:13)¬p1: “If the attacker is at host 1
with a root access and service A is enabled, then at the
next step she will stop the service A on host 1.”

• ♦ (p3 ∧ ♦ p4): “Eventually the attacker visits host 1 and

Deﬁnition 2 (ω-regular game). An ω-regular game with
safe/co-safe objective is G = (G, (ϕ1, ϕ2)) that includes a
game arena G, and player i’s objectives expressed by safe/co-
safe LTL formulas.

then visits host 2.”

Using the semantics of LTL, we can evaluate whether a word
w ∈ Σω satisfy a given formula. For example, ∅∅{p3, p2}
satisﬁes the formula ♦ (p3 ∧ p2).
In this work, we restrict

to a subclass of LTL called
syntactically safe and co-safe LTL [16], which are closely
related to bad and good preﬁxes of languages: Given an LTL
formula ϕ, a bad preﬁx for ϕ is a ﬁnite word that can not be
extended in any way to satisfy ϕ; a good preﬁx for ϕ is a ﬁnite
word that can extended in any way to satisfy ϕ. Safe LTL is
a set of LTL formulas for which any inﬁnite word that does
not satisfy the formula has a ﬁnite bad preﬁx. co-safe LTL is
the set of LTL formulas for which any satisfying inﬁnite word
has a ﬁnite good preﬁx.

The advantage of restricting to safe and co-safe LTL is that
both types of formulas can be represented by Deterministic
Finite-State Automaton (DFA)s with different acceptance con-
ditions. A DFA is a tuple A = (Q, Σ, δ, I, (F, type)) which
includes a ﬁnite set Q of states, a ﬁnite set Σ = 2AP of sym-
bols, a deterministic transition function δ : Q × Σ → Q, and
a unique initial state I. The acceptance condition is speciﬁed
in a tuple (F, type) where F ⊆ Q and type ∈ {safe, cosafe}.
The intuition is that states in a DFA for an LTL formula are in
fact ﬁnite-memory states to keep track of partial satisfaction
of the said formula [9].

Given a word w = σ0σ1 . . . ∈ Σω, its corresponding run
in the DFA is a sequence of automata states q0, q1, . . . such
that q0 = I and qi+1 = δ(qi, σi) for i ≥ 0. Different types of
DFAs deﬁne different accepting conditions:

• when type = safe. A word w is accepted if its corre-
sponding run only visits states in F . That is, for all i ≥ 0,
qi ∈ F .

• when type = cosafe. A word w is accepted if its
corresponding run visits a state in F . That is, there exists
i ≥ 0, qi ∈ F .

A safe LTL formula ϕ translates to a DFA with type = safe. A
co-safe LTL formula ϕ translates to a DFA with type = cosafe.
We will present several examples of LTL formulas and their
DFAs in Section III and Section V.

Remark 1. The restriction to these two types of acceptance
conditions does not allow us to specify recurrent properties,
for example, “always eventually a service is running on
host i.” However, we can specify temporally extended goals
and a range of safety properties in a network systems. The
extension to more complex speciﬁcations is also possible but
requires different synthesis algorithms that deal with recurrent
properties.

An ω-regular game G is zero-sum if ϕ1 = ¬ϕ2–that is,
the formula that P1 wants to satisfy is the negation of P2’s
formula.

In an ω-regular game, a deterministic strategy is a function
: S∗ → Ai that maps a history to an action. A set-
πi
deterministic strategy is a function πi : S∗ → 2Ai that maps
a history to a subset of actions among which player i can
select nondeterministically. A mixed/randomized strategy is
: S∗ → D(Ai) that maps a history into a
a function πi
distribution over actions. If the strategy depends on the current
state only, then we call the strategy memoryless. A strategy is
almost-sure winning for player i if and only if by committing
to this strategy, no matter which strategy the opponent commits
to, the outcome of their interaction satisﬁes the objective ϕi
of player i, with probability one.

The following result is rephrased from [4].

[4] A zero-sum turn-based ω-regular game is
Theorem 1.
determined, i.e. for a given history, only one player (with a
ﬁnite memory strategy) can win the game.

III. MODELING: A HYPERGAME FOR CYBER-DECEPTION

When decoy systems are employed, the game between the
attacker and defender is a game with asymmetric, incomplete
information. In such a game, at least one player has privileged
information over other players. In active cyber-deception with
decoys, the defender has correct information about honeypot
locations. We employ hypergame, also known as game of
games, to capture the defender/attacker interaction with asym-
metric information.

Deﬁnition 3.
pair

[3], [28] A level-1 two-player hypergame is a

HG1 = (cid:104)G1, G2(cid:105),

where G1, G2 are games perceived by players P1 and P2,
respectively. A level-2 two-player hypergame is a pair

HG2 = (cid:104)HG1, G2(cid:105)

where P1 perceives the interaction as a level-1 hypergame and
P2 perceives the interaction as game G2.

In general, if P1 computes his strategy using m-th level
hypergame and P2 computes her strategy using an n-th level
hypergame with n < m, then the resulting hypergame is said
to be a level-m hypergame given as

HGm = (cid:104)HGm−1

1

, HGn

2 (cid:105).

We refer to the game perceived by player i as the perceptual

game of player i.

Putting together the game arena and the payoffs of players,

we can formally deﬁne ω-regular games.

In active cyber-deception,

the defender uses decoys to
induce one-sided misperception of attacker. We introduce

4

the following function, called mask, to model the resulting
misperception of the attacker:

objective ¬ϕ2, which is the negation of P2’s speciﬁcation
and known to P2.

Deﬁnition 4 (Mask). Given the set of symbols Σ, P2’s
perception of Σ is given by a parameterized mask function
mask : Σ × Θ → Σ where for each σ ∈ Σ and a given
parameter θ ∈ Θ, P2 perceives mask(σ, θ). We say that
two symbols σ1, σ2 ∈ Σ are observation-equivalent for P2,
[σ]θ = {σ(cid:48) ∈ Σ |
if mask(σ1, θ) = mask(σ2, θ). Let
mask(σ, θ) = mask(σ(cid:48), θ)}.

Note that the set of observation-equivalent states partitions
the set Σ = (cid:83)
σ∈Σ[σ]θ due to transitivity in this equivalence
relation. We use a simple example to illustrate this deﬁnition.

Example 1. Suppose there is an atomic proposition p5: “host
1 is a decoy.” and P2 cannot observe the valuation of the
proposition, then P2 is unable to distinguish a regular host
from a decoy host. The parameter θ can be a set of hosts in
the network where decoys are placed.

The inclusion of parameter θ will allow us to deﬁne different
misperceptions. Given P1’s labeling function and P2’s labeling
function L2 : S → Σ, the misperception of P2 given the label-
ing function can be represented as L2(s) = mask(L1(s), θ),
for each s ∈ S. Slightly abusing the notation, we denote
the function L2 = mask(L1, θ). In this paper, we consider
a ﬁxed mask function, which means the parameter θ is ﬁxed.
Thus, we omit θ from the mask function. It is noted that the
optimal selection of θ is the problem of mechanism design of
the hypergame (c.f. [24]), which is beyond the scope of this
paper.

Deﬁnition 5. Given the mask function mask, the interaction
between players is a level-2 hypergame in which P1 has
complete information about the labeling function L1 = L :
S → 2AP in the arena G and P2 has a misperceived labeling
function in G. In addition, P1 knows P2’s misperceived
labeling function. The level-2 hypergame is a tuple

HG2 = (cid:104)HG1, G2(cid:105),

where HG1 = (cid:104)G1, G2(cid:105) is a level-1 hypergame with

G1 = (cid:104)G1 = (cid:104)S, A, T, AP, L1(cid:105), (ϕ1, ϕ2)(cid:105)

and

G2 = (cid:104)G2 = (cid:104)S, A, T, AP, L2 = mask(L1, θ) (cid:54)= L(cid:105), (ϕ1, ϕ2)(cid:105),

where ϕi is player i’s objective in temporal logic, for i = 1, 2.

In this work, we consider the following payoffs for players.
• The attacker’s objective is given by a co-safe LTL formula
ϕ2. For example, ϕ2 can be “eventually visit host 1.”
• The defender’s objective is given by a preference ¬ϕ2 ∧
ψ (cid:31) ¬ϕ2 where ψ is a co-safe LTL formula and (cid:31) is the
operator for “is strictly preferred to.” That is, P1 prefers
to prevent P2 from achieving her objective and to satisfy
a hidden objective. If not feasible, then P1 is to satisfy the

For example, P1’s objective ϕ1 could be “always stop
attacker from reaching host 1”, while the additional objective
ψ could be “eventually force the attacker to visit a decoy”.

In level-2 hypergame, P1’s strategy is inﬂuenced by his
perception of P2’s perceptual equilibrium. Since P2 plays a
level-0 hypergame, her perceptual equilibrium is the solution
of G2. Player 1 should leverage the weakness in player 2’s
strategy to achieve better outcomes concerning his objective.
To this end, we aim to solve the following qualitative planning
problem with cyber-deception using decoys.

Problem 1. Given a level-2 hypergame in Def. 5, synthesize a
strategy for P1, if exists, such that no matter which equilibrium
P2 adopts in her perceptual game, P1 can ensure to satisfy
his most preferred logical objective with probability one, i.e.,
almost surely.

IV. SYNTHESIS OF DECEPTIVE STRATEGIES

In this section, we present

the synthesis algorithm for
deceptive strategies. First, we show that when temporal logic
speciﬁcations are considered, the defender requires ﬁnite mem-
ory to monitor the history (a state sequence) with respect
to the partial satisfaction of given defender’s and attacker’s
objectives. This construction of ﬁnite-memory states and tran-
sitions between these states is given in Sec. IV-A. Second,
we construct a hypergame transition system in Sec. IV-B
with which the planning problem for the defender reduces
to solving games with safe and co-safe objectives. Third, we
compute the set of rational strategies of the attacker given
her perceptual game in Sec. IV-C. Lastly in Sec. IV-D, we
show how to synthesize the deceptive winning strategy for the
defender, assuming that the attacker commits to an arbitrary
strategy perceived to be rational by herself. The complexity
analysis is given in Sec. IV-E.

A. Monitoring the history with ﬁnite memory of P1

To design a deceptive strategy, P1 needs to monitor the
history of states with respect to both players’ objectives–that
is, maintaining the evolution of some ﬁnite-memory states.
Thus, we ﬁrst introduce a product using the DFAs for ψ and
ϕ2. The states of this product constitutes this set of “ﬁnite-
memory states”. Later, we use an example to illustrate this
construction.

two

6. Given

complete1 DFAs, A1

=
Deﬁnition
(cid:104)Q1, Σ, δ1, I1, (F1, cosafe)(cid:105)–the defender’s hidden co-safe
LTL objective and A2 = (cid:104)Q2, Σ, δ2, I2, (F2, cosafe))(cid:105)–
the attacker’s co-safe LTL objective ϕ2 and the mask
function mask : Σ → Σ, the product automaton given P2’s
misperception A1 ⊗ A2 is a DFA:

A = (cid:104)Q, Σ, δ, I, (F1, cosafe), (F2, cosafe)(cid:105),

1A DFA A = (cid:104)Q, Σ, δ, I, (F, cosafe)(cid:105) is complete if for any symbol σ ∈
Σ, for any state q ∈ Q, δ(q, σ) is deﬁned. An incomplete DFA can always
be made complete by adding a non-accepting sink state and redirecting all
undeﬁned transitions to the sink.

5

where:

• Q = Q1 × Q2 is the state space.
• Σ is the alphabet.
• δ

: Q × Σ → Q is deﬁned as
2) ∈ Q, δ((q1, q2), σ) = (q(cid:48)

1, q(cid:48)

follows: Let
2) if and only

1, q(cid:48)

(q1, q2), (q(cid:48)
if
– δ1(q1, σ) = q(cid:48)
– there exists σ(cid:48)

1 and

such that δ2(q2, σ(cid:48)) = q(cid:48)
mask(σ) = mask(σ(cid:48))–that
is, σ(cid:48)
equivalent to σ from P2’s viewpoint.

2 and
is observation-

• I = (I1, I2).
• (F1, cosafe) = F1 × Q2.
• (F2, cosafe) = Q1 × F2.

The product computes the transition function using the
union of DFAs A1 and A2, while considering the observation-
equivalent classes of symbols under the mask function. It
maintains two acceptance conditions for accepting the lan-
guages for A1 and A2, respectively. In fact, given the ﬁrst
acceptance condition, A = (cid:104)Q, Σ, δ, I, (F1, cosafe)(cid:105) accepts
the same language as DFA A1. Given the second acceptance
condition, A = (cid:104)Q, Σ, δ, I, (F2, cosafe)(cid:105) accepts a set L of
words such that w = σ0σ1 . . . σn ∈ L if there exists a
1 . . . σ(cid:48)
word w(cid:48) = σ(cid:48)
i) for
i = 0, . . . , n, that is accepted by DFA A2.

n, where mask(σi) = mask(σ(cid:48)

0σ(cid:48)

It can be proven that the transition is deterministic.

Lemma 1. If δ((q1, q2), σ) = (q(cid:48)
exists only one σ(cid:48) such that δ(q2, σ(cid:48)) = q(cid:48)
mask(σ(cid:48)).

1, q(cid:48)

2) is deﬁned, then there
2 and mask(σ) =

The proof is included in Appendix.
It is noted that a state in a DFA captures a subset of sub-
formulas that have been satisﬁed given the input to reach the
state from the initial state [9]. Intuitively, given an input word
w, δ((I1, I2), w) = (q1, q2) captures 1) P1’s knowledge about
the true logical properties satisﬁed by reading the input; 2) P1’s
knowledge about what logical properties that P2 thinks have
been satisﬁed by reading the input. Next, we use examples to
illustrate the product deﬁnition.

Example 2. Consider the following example of players’
objectives:

• the attacker’s co-safe objective is given by ϕ2 := ♦ t for
eventually reaching a set of goal states, labeled t, in the
game graph. A goal state can be that the attacker reaches
a critical host.

• the defender’s objective known to the attacker is ¬ϕ2,
which is satisﬁed when the attacker is conﬁned from
visiting any goal state.

• the defender’s hidden, co-safe objective is ψ := ♦ d,
which is satisﬁed when the attacker reaches a honeypot
labeled d. In this co-safe objective, the defender is to lead
the attacker into a honeypot. The attacker’s activities at
high-ﬁdelity honeypots can be analyzed for understanding
the intent and motives of the attacker.

Given the task speciﬁcation, we generate DFAs A1, A2 in
Fig. 1. The set AP = {t, d} where t := “the current host is a

6

target”; and d := “the current host is a decoy”. The alphabet
Σ = 2AP = {∅, {d}, {t}, {d, t}}. The mask function is such
that mask({d}) = ∅ and mask({d, t}) = {t} In words, P2
does not know which host is a decoy. The symbol (cid:62) stands
for universally true. Transition q (cid:62)−→ q(cid:48) means q (cid:62)−→ q(cid:48) for any
σ ∈ Σ. The product automaton is given in Fig. 2. For example,
{t,d}
−−−→ (1, 1) is deﬁned because
the transition from (0, 0)
δ1(0, {t, d}) = δ1(0, {d}) = 1 and mask({t, d}}) = {t} and
{d}
δ2(0, {t}) = 1. The reader can verify that (0, 0)
−−→ (1, 0) is
deﬁned because mask({d}) = ∅.

The defender’s co-safe objective F1 = {(1, 0), (1, 1)} and
the attacker’s co-safe objective F2 = {(1, 1), (0, 1)}. If the
state in F1 is visited, then the defender knows that the attacker
visited a decoy. The state {1, 1} lies in the intersection of F1
and F2. If this state is visited, then the defender knows that
the attacker visited a decoy and the attacker wrongly thinks
that she has reached a critical host.

∅

0

start

{d}, {t, d}

1

(cid:62)

(a) A1: the defender’s co-safe, hidden
objective.

∅

0

start

{t}

1

(cid:62)

(b) A2: the attacker’s co-safe objec-
tive

Fig. 1: Examples of DFAs.

{d}, ∅

1, 0

{d}

∅

{t}, ∅

start

0, 0

{t}

0, 1

{t}, {t, d}

{t, d}

{d}, {t, d}

(cid:62)

1, 1

Fig. 2: Example of the product automaton A.

B. Reasoning in hypergames on graphs

To synthesize deceptive strategy for P1, we construct the
following transition system, called hypergame transition sys-
tem, for P1 to keep track of the history of interaction, the
partial satisfaction of P1’s safe and co-safe objectives given
the history, as well as what P1 knows about P2’s perceived
partial satisfaction of her co-safe objective.

7.

the

Given

product

(cid:104)Q, Σ, δ, I, (F1, cosafe), (F2, safe)(cid:105),

Deﬁnition
automaton
A
=
DFA
A2 = (Q2, Σ2, δ2, I2, (F2, cosafe)) for P2’s co-safe objective,
and the game arena G = (cid:104)S, A, T, AP, L(cid:105), let L1 : S → Σ be
the labeling function of P1 and L2 : S → Σ be the labeling
function perceived by P2. A hypergame transition system is
a tuple

HTS = (cid:104)(S×Q×Q2), A1∪A2, ∆, v0, F1,cosafe, F1,safe, F2(cid:105)

where

• V = S × Q × Q2 is a set of states. A state v = (s, q, q2)
includes the state s of the game arena and a state q
from the product automaton A and a state q2 from the
automaton A2. The set of states V is partitioned into
V1 = S1 × Q × Q2 and V2 = S2 × Q × Q2.

• A1 ∪ A2 is a set of actions.
• ∆ : V × (A1 ∪ A2) → V is the transition function. For a

given state v = (s, q, q2),

∆((s, q, q2), a) = (s(cid:48), q(cid:48), q(cid:48)

2),

where s(cid:48) = T (s, a), q(cid:48) = δ(q, L1(s(cid:48))) is the transition in
the automaton A, q(cid:48)
2 = δ2(q2, L2(s(cid:48))) is a transition in
P2’s DFA A2. That is, after reaching the new state s(cid:48),
both P1 and P2 update their DFA states to keep track of
progress with respect to their speciﬁcations.

• v0 = (s0, δ(I, L1(s0)), δ2(I2, L2(s0))) is the initial state.
• F1,cosafe = (S × F1 × Q2) is a set of states such that if
any state in this set is reached, then the defender achieves
his hidden, co-safe objective.

• F1,safe = (S ×(Q\F2)×Q2) is a set of states such that if
the game state is always within F1,safe then the defender
achieves his safety objective.

• F2 = (S × Q × F2) is the set of states such that if any
state in this set is reached, then the attacker achieves her
co-safe objective.

To understand this hypergame transition system. Let’s con-

sider a ﬁnite sequence of states in the game arena:

ρ = s0s1s2 . . . sn.

From P2’s perception, the labeling sequence is:

L2(ρ) = L2(s0)L2(s1)L2(s2) . . . L2(sn).

This word L2(ρ) is evaluated against the formula ϕ2 using
the semantics of LTL and then state q2 = δ2(I2, L2(ρ)) is
reached. The perceived progress of P2 is tracked by P1.

From P1’s perspective, the labeling sequence is

Consider Example 2, if there exists a state s ∈ S labeled to be
L1(s) = ∅ and L2(s) = {t}–that is, the attacker misperceives
a non-critical host as her target, then a path ρ ∈ Sω, with
L1(si) = ∅ for all i ≥ 0, and L2(sk) = {t} for some k ≥ 0
can be considered to satisfy both the defender’s and attacker’s
objectives.

Example 3. We use a simple game arena with ﬁve states,
shown in Fig. 3a to illustrate the construction of hypergame
transition system. In this game arena, at a square state, P2
selects an action in the set {b1, b2, b3}; at a circle state, P1
selects an action in the set {a1, a2}. The labeling functions
for two players are given as follows:

L1(0) = L1(1) = L1(2) = ∅; L1(3) = {t}; L1(4) = {d};

L2(0) = L2(1) = L2(2) = ∅; L2(3) = L2(4) = {t}.

Note that state 4 is labeled t by P2 but d by P1. With this
mismatch in the labeling functions, if state 4 is reached, then
the attacker exploits a high-ﬁdelity honeypot, known to P1,
but falsely believes that she has exploited a target host.

start

0

b1

1

a1

b3

a2

b2

b1

2

3

4

(a) An example of game arena.

start

v0

b1

v1

v3

a1

b3

a2

b2

b1

v2

v4

(b) A hypergame transition system.

v0
v1
v2
v3
v4

(0,(0,0),0)
(1,(0,0),0)
(2,(0,0),0)
(3,(0,1),1)
(4,(1,0),1)

(c) States in the HTS.

Fig. 3: An example of game arena and the constructed hyper-
game transition system. The states in the hypergame transition
systems are renamed in Table (c) for clarity.

L1(ρ) = L1(s0)L1(s1)L1(s2) . . . L1(sn).

P1 evaluates the word L1(ρ) against two formulas: ¬ϕ2 and ψ
and compute (q1, q∗
2 can be
different from q2 because that P2’s misperception introduces
differences in the labeling functions L1 (cid:54)= L2.

2) = δ((I1, I2), L1(ρ)). The state q∗

Note that due to misperception, players can be both winning
for a given outcome in their respective perceptual games.

The hypergame transition system is shown in Fig. 3b and
constructed from game arena in Fig. 3a, DFAs A in Fig. 2, and
A2 in Fig. 1b. After pruning unreachable states, the hypergame
transition system happened to share the same graph topology
as the game arena. Here are two examples to illustrate the
b1−→ v3 is generated because
construction: A transition from v1
L1(3)={t}
of the transitions 1 b1−→ 3, (0, 0)
−−−−−−→ (0, 1) in A, and

7

L2(3)={t}
−−−−−−→ 1 in A2. A transition v2

0
because of transitions 2 b1−→ 4, (0, 0)
and 0

L2(4)={t}
−−−−−−→ 1 in A2.

b1−→ v4 is generated
L1(4)={d}
−−−−−−−→ (1, 0) in A

In the hypergame transition system, F2 = {v3, v4} –the
set of states that if reached, then P2 believes that she has
reached the target; F1,safe = {v0, v1, v2, v4} (shaded in blue
and green)–the set of safe states that P1 wants the game to
stay in; F1,cosafe = {v4}(shaded in blue)–the set of states that
P1 preferred to reach as a hidden, co-safe objective.

C. P2’s perceptual game and winning strategy

In the level-2 hypergame, P1 will compute P2’s rational
strategy, which is her perceived almost-sure winning strategy.
Then, based on the predicted behavior of P2, P1 can solve his
own almost-sure winning strategies for the safety objective
¬ϕ2 and the more preferred objective ¬ϕ2 ∧ ψ, respectively.

8.

=
Deﬁnition
Given
(Q2, Σ, δ2, I2, (F2, cosafe))
arena
for
G = (cid:104)S, A, T, AP, L(cid:105) and P2’s labeling function L2 : S → Σ,
the perceptual game of player 2 is a tuple

DFA
the

A2
game

P2,

the

G2 = (cid:104)(S × Q2), A1 ∪ A2, ∆2, (s0, q2,0), S × F2(cid:105)

(1)

where

• S × Q2 is a set of states.
• A1 ∪ A2 is a set of actions.
• ∆2 : (S × Q2) × (A1 ∪ A2) → (S × Q2) is the transition

function. For a given state (s, q2),

∆2((s, q2), a) = (s(cid:48), q(cid:48)

2),
2 = δ2(q2, L2(s(cid:48))) is a transition in

where s(cid:48) = T (s, a), q(cid:48)
P2’s DFA A2.

• (s0, q2,0) with q2,0 = δ2(I2, L2(s0)) is the initial state.
• S × F2 is the set of states such that if any state in this
set is reached, then the attacker perceives that she has
achieved her co-safe objective.

A path of the game graph G2 is an inﬁnite sequence
z0z1zk . . . of states in S × Q2 such that ∆2(zk, a) = zk+1
for some a ∈ A for all k ≥ 0. We denote the set of paths of
G2 by P ath(G2).

Due to the determinacy (Thm. 1), we can compute the
solution of this zero-sum game and partition the game states
into two sets:

• P2’s perceived winning states for P1: Win2

1 ⊆ S × Q2,

• P2’s perceived winning states for herself: Win2

2 = (S ×

and

Q2) \ Win2
1.

The superscript 2 means that this solution is for P2’s percep-
tual game. The winning region Win2
2 can be computed from
the attacker computation, described in Alg. 1 in Appendix with
input game G2 with X1 := S1 × Q2, X2 := S2 × Q2, A := A1
and B := A2, T := ∆2.

Given the winning region,

there exists more than one
winning strategies that P2 can select. We classify the set of
winning strategies for P2 into two sets:

8

1) Greedy strategies for P2: In a turn-based co-safe game,
for a state in player 2’s winning region Win2
there ex-
2,
ists a memoryless, deterministic, sure-winning strategy πsw
:
i
Win2
2 → 2Ai such that by following the strategy, player 2
can achieve his/her objective with a minimal number of steps
under the worst case strategy used by her opponent.

We call this strategy greedy for short. This strategy of P2
is extracted from the solution of G2 as follows: Using Alg. 1,
we obtain a sequence of sets Zk ⊆ S × Q2, for k = 1, . . . , N ,
and deﬁne the level sets: level0 = S × F2, and

levelk = Zk \ Zk−1, for k = 1, . . . , N.

(2)

Intuitively, for any state in levelk, P2 has a strategy to ensure
to visit a state in level0 in a maximal k steps under the worst
case strategy of P1.

For each k = 1, . . . , N , for each z ∈ levelk, let

πsw
2 (z) = {a ∈ A2 | ∆2(z, a) ∈ Zk−1}.

In words, by following the greedy strategy, P2 ensures that the
maximal number of steps to reach S×F2 is strictly decreasing.
2) A non-greedy opponent with unbounded memory: When
P2 is allowed to use ﬁnite-memory, stochastic strategies, there
can be inﬁnitely many winning strategies for P2 given her co-
safe LTL objective.

To see why it is the case, we start with classifying P2’s
actions into two sets: Given a history ρ ∈ (S × Q2)∗ ending
in state (s, q2),

• an action a is perceived to be safe by P2 if
∆2((s, q2), a) ∈ Win2
taking action a will
2. That
ensure P2 to stay within her perceptual winning region.
• an action a is perceived to be sure winning for P2 if
2 ((s, q2))–that is, an action chosen by the greedy

is,

a ∈ πsw
winning strategy.

Lemma 2. For a ﬁnite-memory, randomized strategy of P2
π2 : Win2
2 → D(A2), P2 can win by following π2 in her
perceptual game if the strategy π2 satisﬁes: For every state
(s, q2) ∈ Win2
2, let X = {ρ ∈ P ath(G2) | Last(ρ) = (s, q2)}
be a set of paths ending in (s, q2), it holds that

1) for any a ∈ ∪ρ∈X Supp(π2(ρ)), ∆2((s, q2), a) ∈ Win2
2.

That is, only safe actions are taken.

2) ∪ρ∈X Supp(π2(ρ)) ∩ πsw

2 ((s, q2)) (cid:54)= ∅. That is, eventually
some sure-winning action will be taken with a nonzero
probability.

Further, this strategy is almost-sure winning for P2.

Proof. The ﬁrst condition must be satisﬁed for any winning
strategy for P2 to stay within the her winning region. The
second condition is to ensure that a state in S×F2 is eventually
visited. To see this, let’s use induction: By following strategy
π2, for an arbitrary P1’s strategy π1, let {X0, X1, X2, . . .} be
the stochastic process of states visited at steps 0, 1, 2, . . . given
(π1, π2) in the game G2. By deﬁnition of π2 and the properties
of the game’s solution, we have the following conditions
satisﬁed: 1) If it is a state of P2, then the probability of
reaching levelk−1 in a ﬁnite number of steps from a state

in levelk is strictly positive: P (Xt+m ∈ levelk | Xt−1 ∈
levelk−1 ∧ Xt−1 ∈ S2 × Q2) > 0 for some integer m ≥ 0;
2) If it is P1’s state at level k, then for any action of P1, the
next state must be in level(cid:96) for (cid:96) ≤ k − 1: P (Xt ∈ level(cid:96) |
Xt−1 ∈ levelk−1 ∧ Xt−1 ∈ S1 × Q2) = 1; 3) And the game
state is always in Win2
2, P (Xt ∈ Win2
2) = 1.
Let En be the event that “The level of state is reduced by one
in n steps”. Given (cid:80)∞
n=1 P (En) ≥ (cid:80)∞
n=1 p(1 − p)n−1 = 1
where p is the minimal probability of P (En), the probability
of reducing the level by one along the path in inﬁnitely number
of steps is one. In addition, with probability one, the level of
Xt is ﬁnite for all t ≥ 0. Thus, eventually, the level of Xt
reduces to zero as t approaches inﬁnity.

2 | Xt−1 ∈ Win2

In words, the almost-sure winning strategy for P2 only
allows a (perceived) safe action to be taken with a nonzero
probability. It also enforces that a “greedy” action used by
the sure-winning strategy must be selected with a nonzero
probability eventually. Since there can be inﬁnitely many such
almost-sure winning strategies, we take an approximation of
the set of almost-sure winning strategy as a memoryless set-
based strategy as follows.

ˆπasw
2

((s, q2)) = {a | ∆2((s, q2), a) ∈ Win2

2}.

(3)

That is, we assume that at any state (s, q2), P2 can select any
action from a set of safe actions to staying within her perceived
winning region. Note that the progressing action πsw
2 ((s, q2))
is also safe, that is, πsw

2 ((s, q2)) ∈ ˆπasw

((s, q2)).

2

b1

(1, 0)

(3, 1)

a1

start

(0, 0)

b2

b3

a2

b1

(2, 0)

(4, 1)

Fig. 4: The perceptual game of P2.

almost-sure winning

Example 4. We construct P2’s perceptual game graph in
Fig. 4 using DFA A2 in Fig. 1b and the game arena in
Fig. 3a. Using Alg. 1, we obtain Z0 = {(3, 1), (4, 1)},
Z1 = {(1, 0), (2, 0)} ∪ Z0, and Z2 = {(0, 0)} ∪ Z1. Thus,
level0 = Z0, level1 = {(1, 0), (2, 0)}, and level2 = {0, 0}.
The greedy winning strategy πsw
2 ((1, 0)) = {b1, b2} and
πsw
2 ((2, 0)) = {b1}.
One

that
π2((1, 0), bi) = (cid:15)i,
for any bi ∈ {b1, b2, b3} and
π2((2, 0), b1) = 1. The parameters (cid:15)i, i = 1, 2, 3 can be
picked arbitrary under the constraints (cid:15)2 > 0 or (cid:15)3 > 0,
and (cid:80)3
i=1 (cid:15)i = 1. This strategy of P2 ensures that even
if the loop (0, 0) a1−→ (1, 0) b3−→ (0, 0) occurs, it can only
occur ﬁnitely many times. Eventually, b1 or b2 will be
selected by P2 to reach S × F2. The approximation of the
almost-sure winning strategies is ˆπasw
((1, 0)) = {b1, b2, b3}
and ˆπasw

((2, 0)) = {b1}.

for P2

strategy

is

2

2

D. Synthesizing P1’s deceptive winning strategy

Next, we use the hypergame transition system, a P2’s

strategy, to compute a subgame for P1.

Deﬁnition 9 (πi-induced subgame graph). Given the graph of
a game G = (cid:104)V1 ∪ V2, A1 ∪ A2, ∆, v0(cid:105) and a strategy of player
i, πi : Vi → 2Ai , a πi-induced subgame graph, denoted G/πi,
is the game graph (cid:104)V1 × V2, A1 ∪ A2, ∆/πi, v0(cid:105) where

∆/πi(v, a) = ∆(v, a);
∆/πi(v, a) = ∆(v, a);
∆/πi(v, a) ↑;

∀a ∈ Ai \ πi(v).

∀a ∈ Ak, k (cid:54)= i;
∀a ∈ πi(v);

where ↑ means that the function is undeﬁned for the given
input.

The subgame graph restricts player i’s actions to these
allowed by strategy πi. It does not restrict player k’s actions.
Given a subgame graph G/πi and another player k’s strategy
πk : Vk → 2Ak , we can compute another subgame graph
induced by πk from G/πi and denote the subgame as G/πi,πk .
Now, assuming that P2 follows the perceived winning
strategy π2 : S × Q2 → 2A2 , we can construct an induced
subgame graph from the hypergame transition system HTS
using P2’s strategy deﬁned by: π(cid:48)
2(v) := π2(s, q2) for each
v = (s, q1, q2) ∈ V2. Slightly abusing the notation, we still
use π2 to refer to P2’s strategy deﬁned over domain V2.

By replacing π2 to be either 1) the greedy policy πsw
2 or
2) the approximation of almost-sure winning strategies ˆπasw
,
2
we can solve P1’s winning strategy with respect to different
objectives, ¬ϕ2 or ¬ϕ2 ∧ ψ, leveraging the information about
P2’s misperception.

We present a two-step procedure to solve P1’s deceptive

sure-winning strategy.

1) Step 1: Solve the π2-induced subgame for P1 with respect

to the safety objective:

HTS/π2 = (cid:104)V, A1 ∪ A2, ∆/π2 , v0, (F1,safe, safe)

2 with input X1 = V1, X2 = V2 and
using Alg.
T = ∆/π2 , and let B = F1,safe. The outcome is
a tuple (Win1,safe, π1,safe)–that is, a set of states from
which P1 ensures that the safety objective ¬ϕ2 can be
satisﬁed, by following the winning, set-based strategy
π1,safe : Win1,safe → 2A1.

2) Step 2: Compute the π1,safe-induced sub-game from

HTS/π2, denoted as

HTS/(π1,safe,π2)

= (cid:104)V ∩ Win1,safe, A1 ∪ A2, ∆/(π1,safe,π2),

v0, (F1,cosafe ∩ Win1,safe, cosafe)(cid:105).

Then, we solve the subgame for P1’s co-safe objective
using Alg. 1 with input X1 = V1 ∩ Win1,safe,, X2 = V2 ∩
Win1,safe, and T = ∆/(π1,safe,π2), and let F = F1,cosafe ∩
Win1,safe. The outcome is a tuple (Win1,cosafe, π1,cosafe)–
that is, a set of states from which P1 ensures that both
the safety and co-safe, hidden objectives can be satisﬁed,

9

by following the winning, set-based strategy π1,cosafe :
Win1,cosafe → 2A1 . Note that safety objective is satisﬁed
because P1 only can select actions allowed by his safe
strategy π1,safe.

The next Lemma shows that for any safe or co-safe objective
of P1, if a strategy is winning for P1 against the approximation
of almost-sure winning strategies of P2 (see (3) for the
deﬁnition), then the same strategy is winning for P1 against
any almost-sure winning strategy of P2.

Lemma 3. Consider a game G = (V1 ∪ V2, A1 ∪ A2, ∆, v0)
and two strategies of Player 2, π2 : V ∗ → D(A2) is ﬁnite-
memory and randomized; and ˆπ2 : V → 2A2 is a stationary,
set-based, and deterministic. If these two strategies satisﬁes
that for any ρ = v0v1 . . . vk ∈ V ∗, π2(ρ, a) > 0 if and only if
a ∈ ˆπ2(vk). That is, Supp(π2(ρ)) = ˆπ2(vk). Then, given an
initial state v0 ∈ V and any subset F ⊆ V , if P1 has a winning
strategy π1 in the ˆπ2-induced game for objective (F, safe) (or
(F, cosafe)), then P1 wins even if P2 follows strategy π2.

Proof. We consider two cases:

Case 1: P1’s objective is given by (F, safe)–that is, P1 is to
ensure the game states to stay in the set F . By deﬁnition, the
winning strategy of P1 ensures that the game stays within a
set Win1 ⊆ F of safe states, no matter which action P2 selects
using ˆπ2. For any history ρ = v0v1 . . . vk ∈ V ∗, for any action
a that π2(ρ) will select with a nonzero probability, then the
resulting state is still within Win1 ⊆ F , because a ∈ ˆπ2(vk).
Thus, P1 ensures to satisfy the safety objective with strategy
π1 even against P2’s strategy π2.

Case 2: P1’s objective is given by (F, cosafe)–that is, P1
is to reach set F . In this case, the winning region of P1 is
partitioned into level sets (see Alg. 1 and the deﬁnition of
level sets (2)). Given a history ρ ∈ V ∗, if vk is P1’s turn and
vk ∈ leveli, then by construction, there exists an action of P1
to reach leveli−1. Otherwise, vk is P2’s turn and vk ∈ leveli,
by construction, for any action of P2 in ˆπ2(vk), the next state
is in leveli−1. While P2 follows π2, the probability of reaching
leveli−1 in one step is
(cid:88)

π2(ρ, a)1(∆(vk, a) ∈ leveli−1)

=

=

a∈A2

(cid:88)

a∈Supp(π2(ρ))
(cid:88)

a∈Supp(π2(ρ))

π2(ρ, a)1(∆(vk, a) ∈ leveli−1)

π2(ρ, a) = 1,

where the ﬁrst equality is because only actions in the sup-
port of π2(ρ) can be chose with nonzero probabilities; and
the second equality is because Supp(π2(ρ)) ⊆ ˆπ2(sk) and
1(∆(vk, a) ∈ leveli−1) = 1 for any a ∈ ˆπ2(vk). Thus, the
level will be strictly decreasing every time an action is taken
by P1 or P2 and the level of any state in the winning region
Win1 is ﬁnite. When the level reaches zero, P1 visits F .

Remark 2. In our analysis, the defender is playing against
all possible strategies that can be used by the attacker. The

deceptive winning strategy computed for the defender can be
conservative for any ﬁxed attack strategy used by the attacker.
For example, if the attack takes a set of almost-sure winning
actions uniformly at random (all actions are equally winning
as she perceives), then it may leave the opportunity for the
defender to ensure, with a positive probability, the attacker will
be lured into a honeypot, and at the same time, the defender
can ensure, with probability one, the safety objective of the
system is satisﬁed. This chance-winning strategy requires
further analysis of positive winning in games and could be
explored in the future.

Next, we use the toy example to demonstrate the computa-
tion of player 1’s almost-sure winning strategy with deception.

Example 5. Let’s revise the simple example in Ex. 3 to
illustrate the computation of deceptive winning regions for
P1. We added one transition 2 b2−→ 1. The resulting hypergame
transition system is shown in Fig. 5a. Note that the perceptual
game of P2 is changed in similar way by adding a transition
from (1, 0) b2−→ (1, 0). We omit the ﬁgure here.

b1

v1

v3

a1

start

v0

b2

b2

b3

a2

b1

v2

v4

start

v0

a1

a2

b1

v1

v3

b2

b1

v2

v4

(a) The example of HTS after revising
a transition.

(b) The πsw

2 -induced HTS.

2 (1, 0) = {b1, b2}, πsw

(1, 0) = {b1, b2, b3} and ˆπasw

With this change, it can be veriﬁed that the winning region
of P2 in her peceptual game is not changed and the sure-
winning strategy is still πsw
2 (2, 0) =
{b1}. The approximation of almost-sure winning strategy is
now ˆπasw

(2, 0) = {b1, b2}.
2 -induced subgame is shown
in Fig. 5b. By applying Algorithm 1, we obtain P1’s deceptive
winning region and winning strategy Win1,safe = {v0, v2, v4}
and π1(v0) = a2. This strategy also ensures that P1 can satisfy
the hidden, co-safe objective and lead P2 to visit the decoy.

Case 1: P2 is greedy. The πsw

2

2

2

Case 2: P2 is randomized. The ˆπasw

-induced subgame is
exactly the one in Fig. 5a. By applying Algorithm 1, we
obtain P1’s deceptive winning region and winning strategy
Win1,safe = ∅. Thus, P1 cannot achieve either his safety or
hidden co-safe objectives when P2 is not greedy and uses a
ﬁnite-memory, randomized strategy.

E. Complexity analysis

Algorithm 1 and Algorithm 2 run in O(|X|+|T |) where |X|
is the number of states and |T | is the number of transitions in
the game. Thus, the time complexity of solving P1’s deceptive
winning region for safety and the preferred objectives is
O(|V | + |∆|). To compute a policy πi-induced subgame,
the time complexity is Θ(|Dom(πi)|) where Dom(πi) is the
domain of policy πi.

10

V. EXPERIMENTAL RESULT

TABLE I: The pre- and post-conditions of vulnerabilities.

We demonstrate the effectiveness of the proposed synthesis
methods in a synthetic network. The topology of the network
is manually generated. The set of vulnerabilities is deﬁned
with the pre- and post-conditions of concrete vulnerability
instances in [26]. The generation of the attacker graph is
based on multiple prerequisite attack graph [12] with some
modiﬁcation. We assume that the attacker cannot carry out
attacks from multiple hosts at the same time. We perform two
experiments on two networks (small and large) and different
sets of attacker and defender objectives.

A. Experiment 1: A small network with simple attacker/de-
fender objectives

vul id.

pre- and post-

0

1

2

Pre : c ≥ 1, service 0 running on target host,
Post : c = 2, stop service 0 on the target, reach target host.
Pre: c ≥ 1, service 1 running on the target host,
Post : reach target host.
Pre: c ≥ 1, service 2 running on the target host
Post : c = 2, reach target host.

TABLE II: The network status and the defender’s options

host id.
0
1
2
3

services
{1}
{0, 1}
{0, 1, 2}
{0, 1, 2}

non-critical services
∅
{1}
{1}
{0, 1, 2}

3

a state of the game arena is a tuple

Given the network topology, the services and vulnerabilities,

0

1

2

Fig. 5: The connectivity graph of hosts in a small network.

Formally, the network consists of a list Hosts of hosts,
with a connectivity graph shown in Fig. 5. Each host runs a
subset Servs = {0, 1, 2} of services. A user in the network can
have one of the three login credentials credentials = {0, 1, 2}
standing for “no access” (0), “user” (1), and “root” (2).
There are a set of vulnerabilities in the network, each of
which is deﬁned by a pre-condition and a post-condition. The
pre-condition is a Boolean formula that speciﬁes the set of
logical properties to be satisﬁed for an attacker to exploit the
vulnerability instance. The post-condition is a Boolean formula
that speciﬁes the logical properties that can be achieved after
the attacker has exploited that vulnerability. For example, a
vulnerability named “IIS overﬂow” requires the attacker to
have a credential of user or a root, and the host running IIS
webserver. After the attacker exploits this vulnerability, the
service will be stopped and the attacker gains the credential
as a root user. The set of vulnerabilities are given in Table I
and are generated based on the vulnerabilities described in
[25]. Note that the game-theoretic analysis is not limited to
these particular instances of vulnerabilities in [25] but rather
used these as a proof of concept.

The defender can temporally suspend noncritical services
from servers. To incorporate this defense mechanism, we
assign each host a set of noncritical services that can be
suspended from the host. In Table II, we list the set of services
running on each host, and a set of noncritical services that can
be suspended by the defender. Other defenses can be consid-
ered. For example, if the network topology can be reconﬁgured
online, then the state in the game arena should keep track
of the current topology conﬁguration of the network. In our
experiment, we consider simple defense actions. The method
extends to more concrete defense mechanisms.

11

(h, c, t, NW),

where h ∈ Hosts is the current host of the attacker, c ∈ Creds
is the current credential of the attacker on that host h, t is a
turn variable indicating whether the attacker (t = 1) takes an
action or the defender does (t = 0). The last component NW
speciﬁes the network condition as a list of (hi, Servi) pairs
that gives a set Servi of services running on host hi, for each
hi ∈ Hosts. We denote the set of possible network conditions
in the network as NWs.

The attacker, at a given attacker’s state, can exploit any
existing vulnerability on the current host. The defender, at a
defender’s state, can choose to suspend a noncritical service
on any host in the network. Given the defenses and attacks, a
sampled path in the arena is illustrated as follows.

(0, 1, 1, {0 : {1}, 1 : {0, 1}, 2 : {0, 1, 2}, 3 : {0, 1, 2}})

(1,0)
−−−→ (1, 2, 0, {0 : {1}, 1 : {1}, 2 : {0, 1, 2}, 3 : {0, 1, 2}})

(2,1)
−−−→ (1, 2, 1, {0 : {1}, 1 : {1}, 2 : {0, 2}, 3 : {0, 1, 2}}),

This speciﬁes that the attacker is in source host h = 0
with user access c = 1 and has her turn t = 1 to take an
action. She chooses to attack host 1 with the vulnerability
0 (action (1, 0)). The vulnerability 0’s pre-condition requires
that the attacker has a user/root access on the source host
and service 0 is running on the target host. After taking the
action, the attacker reaches the target host h(cid:48) = 1, stopped the
service and change her access to root c(cid:48) = 2 (see Table I).
Now it is the defender’s turn t(cid:48) = 0. The service 0 is stopped
in host 1, resulting the only change in the NW as shown in
boldface. Then, the defender takes an action (2, 1), indicating
the action to stop service 1 on host 2. This action results in a
change in the network status, as shown in boldface. The arena
is generated to consider all possible actions that can be taken
by the attacker or defender. If at a given state there is no action
available, i.e., no vulnerability to exploit or action to defense,

the player selects a “null” action, switching the turn to the
other player. The full arena is shown in Fig. 8 in Appendix.
At the beginning of this game, the attacker is at host 0 with
user access. Each host runs all available services. The state
colored in red in Fig. 8 is the initial state in the arena.

In the following experiments, we consider players’ ob-
jectives in Example 2. The following labeling functions are
deﬁned: For the defender, we have

L1(h, c, t, NW) =






{t}
{d}
∅

if h = 3 and c ≥ 1,
if h = 2 and c ≥ 1,
otherwise.

In words, the decoy is set to host 2. The attacker violates
the safety objective if she reaches host 3 with user or root
credentials. For the attacker, we have

L2(h, c, t, NW) =

(cid:26) {t}
∅

if h = 3, 2 and c ≥ 1,
otherwise.

That is, the attacker is to reach either host 2 or host 3, which
are perceived to be critical hosts by the attacker.

For this experiment, we compare the set of states from
which P1 has a winning strategy to achieve his two objectives:
safety ¬ϕ2 and preferred ¬ϕ2 ∧ ψ. Let Win1 (resp. Win(cid:31)
1 )
be the winning region of P1 given objective ¬ϕ2 (resp.
¬ϕ2 ∧ ψ). As shown in Table III, the total number of states
in the hypergame transition system is 259, when P2 has no
misperception and plays the zero-sum game against P1’s two
objectives, there does not exist a winning strategy for P1 given
the initial state of the game. When P2 perceives the decoy to
be a critical host (see the labeling function L2) and uses the
sure-winning greedy strategy, P1’s winning regions for both
objectives are larger and include the initial state, that is, P1
has a winning strategy to ensure the safety objective and force
P2 to visit the decoy host. However, under this misperception,
when P2 selects a randomized, ﬁnite memory strategy, there
does not exist a winning strategy for P1 given the initial state
of the game. This means a greedy adversary is easier to deceive
than a non-greedy adversary. When P1 uses the policy ˆπasw
to approximate P2’s randomized strategies, his strategies are
conservative for both objectives.

2

In Appendix, Figure 9 depicts the winning regions computed
using the proposed method and P2’s sure-winning greedy
policy, with three partitions of the states in the hypergame
transition system: 1) A set of states colored in blue or orange
are the states where P2 perceives herself to be winning given
the co-safe objective and P1 can deceptively win against P2’s
co-safe objective given P2’s greedy strategy. 2) A set of states
colored in red are the states where P2 perceives herself to be
winning and P1 cannot deceptively win against P2. 3) A set
of states colored in yellow are the states where P2 perceives
herself to be losing given the co-safe objective and P1 can
deceptively win the safety objective. The union of blue and
yellow states is Win1. 4) A set of four states colored in orange
and grey are those in P1’s deceptive winning region Win1
under any sure winning strategy of P2 but not in Win1 under
any randomized, ﬁnite memory, almost-sure winning strategy

TABLE III: Comparison P1’s winning regions with/without
deception.

|V |

259

No Misperception
Win(cid:31)
Win1
1
116, lose
187, lose

Greedy

Randomized

Win1
191, win

Win(cid:31)
1
134, win

Win1
187, lose

Win(cid:31)
1
130, lose

for P2. 5) The initial state is marked with an incoming arrow
and colored in grey. It is noted that the set of yellow states is
isolated from P2’s perceived winning region. This is because
we restricted P1’s actions to only the ones allowed by his
winning strategy. By following these strategies, P2 is conﬁned
to a set of states within Win1.

B. Experiment: A larger network with more complex attack-
er/defender objectives

In this experiment, we consider a larger network with 7
hosts, with the connectivity graph shown in Fig. 6. P2’s co-safe
objective is given by an LTL formula ϕ2 := ♦ A∧♦ B–that is,
visiting hosts labeled A and B eventually. The DFA A2 that
represents this speciﬁcation is given in Fig. 7. The defender’s
co-safe objective A1 is the same as in Fig. 1a, which is to
force the attacker to visit a decoy. In Table IV, we list the
set of services running on each host, and a set of noncritical
services that can be suspended by the defender.

TABLE IV: The set of available services and non-critical
services on each host.

host id.
0
1
2
3
4
5
6

services
{1, 2}
{0, 1, 2}
{0, 1, 2}
{0, 1, 2}
{0, 1, 2}
{1, 2}
{0, 1, 2}

non-critical services
{2}
{1}
{1}
{1}
{0, 1, 2}
{2}
{1}.

0

1

3

6

4

2

5

Fig. 6: The connectivity graph of hosts in a network of seven
hosts.

The following labeling functions are deﬁned for P1 and P2:

L1(h, c, t, NW) =






{A}
{B}
{d}
∅

if h = 2 and c ≥ 1,
if h = 6 and c ≥ 1,
if h = 4 and c ≥ 1,
otherwise.

L2(h, c, t, NW) =






{A}
{B}
∅

if h = 2, 4 and c ≥ 1,
if h = 6 and c ≥ 1,
otherwise.

12

∅

0

start

{B}

{A}

3

(cid:62)

∅, {A}

{A}

{B}

1

2

∅, {B}

Fig. 7: The DFA A2 for P2’s co-safe objective.

TABLE V: Comparison P1’s winning regions with/without
deception.

|V |

No Misperception

Greedy

Randomized

Win1
17013, win

Win(cid:31)
1
10782, lose

Win1
17170, win

Win(cid:31)
1
10964, win

Win1
17013, win

Win(cid:31)
1
10802, win

28519

In words, the decoy is set to host 4. The attacker misper-
ceives the decoy host 4 as one critical target, with the same
label as host 2.

Similar to the previous experiment, we compare the set
of states from which P1 has a winning strategy to achieve
his two objectives: safety ¬ϕ2 and preferred ¬ϕ2 ∧ ψ. The
result is shown in Table V. It turns out that with deception,
we see that the deceptive winning region for P1 given either
objective is larger than the winning regions of games in which
P2 has no misperception. At the beginning of this game,
the attacker is at host 0 with user access. Each host runs
all available services. P1 can prevent P2 from achieving her
co-safe objective from the initial state with probability one.
However, P1 cannot ensure to force P2 to running into the
decoy without deception. When P2 misperceives the label of
states, no matter P2 uses a sure-winning or an almost-sure
winning randomized strategy, P1’s deceptive winning regions
for both safety and the preferred objectives include the initial
state of the game. That is, P1 can prevent P2 from achieving
her co-safe objective and force P2 to visit the decoy host 4.
The code is implemented in python with a MacBook Air
with 1.6 GHz dual-core 8th-generation Intel Core i5 Processor
and 8GB memory. In the ﬁrst experiment, the game arena
(attack graph) is generated in 2.34 sec and the hypergame
transition system is generated in 8.16 sec. The computation of
winning regions and winning strategies for different P1 and
P2 objectives took 0.02 to 0.04 sec. In the second experiment,
the game arena (attack graph) is generated in 70.2 sec and
the hypergame transition system is generated in 854.83 sec.
The computation of winning regions and winning strategies
for different P1 and P2 objectives took 3 to 10 sec.

VI. CONCLUSION

In this paper, we have developed the theory and solutions of
ω-regular hypergame and algorithms for synthesizing provably
correct defense policies with active cyberdeception. Building
on attack graph analysis in formal veriﬁcation, we have shown
that by modeling the active deception as an ω-regular game
in which the attacker misperceives the labeling function, the

solution of such games can be used effectively to design decep-
tive security strategies under complex security requirements
in temporal logic. We introduced a set-based strategy that
approximates attacker’s all possible rational decisions in her
perceptual game. The defender’s deceptive strategy against this
set-based strategy, if exists, can ensure the security properties
to be satisﬁed with probability one. We have experiments
with synthetic network systems to verify the correctness of
the policy and the advantage of deception. Our result is the
to integrate formal synthesis for ω-regular games in
ﬁrst
designing secure-by-synthesis network systems using attack
graph modeling.

The modeling and solution approach can be further gen-
eralized to consider partial observations during interaction,
concurrent interactions, and multiple target attacks from the
attacker. The mechanism design problem in such a game
is to be investigated for resource allocation in the network.
Further, we will investigate methods to improve the scalability
of the solution. One approach for tractable synthesis for such
games is to use hierarchical aggregation in attack graphs [20].
From formal synthesis,
is also possible to improve the
scalability of algorithm with compositional synthesis [10], [15]
and abstraction methods [7] of ω-regular games.

it

REFERENCES

[1] Rachid Ait Maalem Lahcen, Ram Mohapatra, and Manish Kumar.
Cybersecurity: A survey of vulnerability analysis and attack graphs,
volume 253. Springer Singapore, 2018.

[2] Ehab Al-Shaer, Jinpeng Wei, Kevin W. Hamlen, and Cliff Wang. Dy-
namic Bayesian Games for Adversarial and Defensive Cyber Deception.
In Ehab Al-Shaer, Jinpeng Wei, Kevin W. Hamlen, and Cliff Wang,
editors, Autonomous Cyber Deception: Reasoning, Adaptive Planning,
and Evaluation of HoneyThings, pages 75–97. Springer International
Publishing, Cham, 2019.

[3] Peter G Bennett. Hypergames: developing a model of conﬂict. Futures,

12(6):489–507, 1980.

[4] J Richard B¨uchi and Lawrence H Landweber.

conditions by ﬁnite-state strategies.
Mathematical Society, 138:295–311, 1969.

Transactions of

Solving sequential
the American

[5] Thomas E. Carroll and Daniel Grosu. A Game Theoretic Investigation
In 2009 Proceedings of 18th
of Deception in Network Security.
International Conference on Computer Communications and Networks,
pages 1–6, August 2009.

[6] Tanmoy Chakraborty, Sushil Jajodia, Noseong Park, Andrea Pugliese,
Edoardo Serra, and VS Subrahmanian. Hybrid adversarial defense:
Merging honeypots and traditional security methods. Journal of Com-
puter Security, 26(5):615–645, 2018.

[7] Edmund Clarke, Orna Grumberg, Somesh Jha, Yuan Lu, and Helmut
Veith. Counterexample-guided abstraction reﬁnement. In International
Conference on Computer Aided Veriﬁcation, pages 154–169. Springer,
2000.

[8] Fred Cohen. The Use of Deception Techniques: Honeypots and Decoys.

In Handbook of Information Security 3.1. 2006.

[9] Javier Esparza, Jan Kˇret´ınsk`y, and Salomon Sickert. From LTL to
deterministic automata. Formal Methods in System Design, 49(3):219–
271, 2016.

[10] Emmanuel Filiot, Naiyong Jin, and Jean Franc¸ois Raskin. Antichains
and compositional algorithms for LTL synthesis. Formal Methods in
System Design, 39(3):261–296, 2011.

[11] Karel Hor´ak, Branislav Boˇsansk´y, Christopher Kiekintveld, and Charles
Kamhoua. Compact Representation of Value Function in Partially
In Proceedings of the Twenty-Eighth
Observable Stochastic Games.
International Joint Conference on Artiﬁcial Intelligence, pages 350–356.
International Joint Conferences on Artiﬁcial Intelligence Organization,
2019.

13

[12] Kyle Ingols, Richard Lippmann, and Keith Piwowarski. Practical attack
graph generation for network defense. In The 22nd Annual Computer
Security Applications Conference, pages 121–130. IEEE, 2006.
[13] Sushil Jajodia, V. S. Subrahmanian, Vipin Swarup, and Cliff Wang.
Cyber deception: Building the scientiﬁc foundation. Springer, 2016.
[14] S. Jha, O. Sheyner, and J. Wing. Two formal analyses of attack graphs.
Proceedings of the Computer Security Foundations Workshop, 2002-
Jan:49–63, 2002.

[15] Abhishek Ninad Kulkarni and Jie Fu. A Compositional Approach to
In American

Reactive Games under Temporal Logic Speciﬁcations.
Control Conference, pages 2356–2362. IEEE, 2018.

[16] Orna Kupferman and Moshe Y Vardi. Model checking of safety

properties. Formal Methods in System Design, 19(3):291–314, 2001.

[17] Jiaqiang Liu, Yong Li, Huandong Wang, Depeng Jin, Li Su, Lieguang
Zeng, and Thanos Vasilakos. Leveraging software-deﬁned networking
Information Sciences, 327:288–299,
for security policy enforcement.
2016.

[18] Zohar Manna and Amir Pnueli. The temporal logic of reactive and
concurrent systems: Speciﬁcation. Springer Science & Business Media,
2012.

[19] Peng Ning, Yun Cui, Douglas S Reeves, and Dingbang Xu. Techniques
and tools for analyzing intrusion alerts. ACM Transactions on Informa-
tion and System Security, 7(2):274–318, 2004.

[20] Steven Noel. Managing Attack Graph Complexity through Visual
In In VizSEC/DMSEC ’04: Proceedings of
Hierarchical Aggregation.
the 2004 ACM Workshop on Visualization And, pages 109–118. ACM
Press, 2004.

[21] Jeffrey Pawlick, Edward Colbert, and Quanyan Zhu. A Game-theoretic
Taxonomy and Survey of Defensive Deception for Cybersecurity and
Privacy. ACM Comput. Surv., 52(4):82:1–82:28, August 2019.

[22] A. Pnueli and R. Rosner. On the synthesis of a reactive module.
In Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, POPL ’89, pages 179–190, New
York, NY, USA, 1989. ACM.

[23] Alex Ramos, Marcella Lazar, Raimir Holanda Filho, and Joel J.P.C.
Rodrigues. Model-Based Quantitative Network Security Metrics: A
Survey. IEEE Communications Surveys and Tutorials, 19(4):2704–2734,
2017.

[24] Tuomas W. Sandholm. Distributed rational decision making, 1999.
[25] O. Sheyner, J. Haines, S. Jha, R. Lippmann, and J.M. Wing. Automated
In Proceedings 2002 IEEE
generation and analysis of attack graphs.
Symposium on Security and Privacy, pages 273–284. IEEE Computer
Society, 2002.

[26] Oleg Mikhail Sheyner. Scenario graphs and attack graphs. PhD thesis,

Carnegie Mellon University, 2004.

[27] AJ Underbrink. Effective cyber deception. In Cyber Deception, pages

115–147. Springer, 2016.

[28] Russell Richardson III Vane. Using Hypergames to Select Plans in
Competitive Environments. PhD thesis, George Mason University, 2000.

a) Proof of Lemma 1: By way of contradiction. If there
exists another σ(cid:48)(cid:48) (cid:54)= σ(cid:48) such that δ(q2, σ(cid:48)(cid:48)) = q(cid:48)(cid:48)
2 (cid:54)= q(cid:48)
2
and mask(σ) = mask(σ(cid:48)(cid:48)). Given mask(σ(cid:48)) = mask(σ(cid:48)(cid:48)) =
mask(σ), P2 cannot distinguish σ(cid:48) from σ(cid:48)(cid:48)–that is, σ(cid:48) = σ(cid:48)(cid:48) in
A2. Thus, q(cid:48)
2 contradicts the fact that A2 is deterministic
in the product automaton.

2 and q(cid:48)(cid:48)

2 (cid:54)= q(cid:48)(cid:48)

b) Algorithms for solving co-safe/reachability and safety
games: Consider a two-player turn-based game arena G =
(S = S1 ∪ S2, A = A1 ∪ A2, T ) where for i = 1, 2, Si is a set
of states where player i takes an action, Ai is a set of actions
for player i, and T : S × A → S is the transition function.

We ﬁrst deﬁne two functions:
Pre∃

i (X) = {s ∈ Si | ∃a ∈ Ai, such that T (s, a) ∈ X};

which is a set of states from which player i has an action to
ensure reaching the set X in one step.

Pre∀

i (X) = {s ∈ Si | ∀a ∈ Ai, such that T (s, a) ∈ X};

which is a set of states from which all actions of player i will
lead to a state within X.

Algorithm 1 is Zielonka’s attractor algorithm, for solving
player 1’s winning region given a reachability objective.
Algorithm 2 is the solution for turn-based games with safety
objective for player 1.

Algorithm 1 Almost-Sure Winning for player 1 given the
winning condition (F, cosafe) (reaching F ).

Inputs: (G = (X = X1 ∪ X2, A1 ∪ A2, T ), F ⊆ S).

1 (Zk) ∪ Pre∀

2 (Zk)

1: Z0 = F
2: while True do
3:
4:

Zk+1 = Zk ∪ Pre∃
if Zk+1 = Zk then
Win1 ← Zk;
End Loop

end if
k ← k + 1

5:
6:
7:
8:
9: end while
10: for j = 1, . . . , k do
11:
12:

for x ∈ Zj ∩ X1 do

π1(x) = a if T (x, a) ∈ Zj−1.

(cid:46) a sure-winning

action from state x.

end for

13:
14: end for
15: return Win1, π1.

strategy for player 1.

(cid:46) Return the winning region and

Algorithm 2 Almost-Sure Winning for player 1 given the
winning condition (B, safe) (staying in B)

Inputs: (G = (X = X1 ∪ X2, A1 ∪ A2, T ), B ⊆ S).

1: Z0 = B.
2: while True do
3:

Y = S \ Zi;
Zi+1 ← Zi \

end if
i ← i + 1;

if Zi+1 = Zi then
Win1 ← Zi;
End Loop

5:
6:
7:
8:
9:
10:
11: end while
12: for x ∈ Win1 ∩ X1 do
13:
14: end for
15: return Win1, π1.

π1(x) = {a | T (x, a) ∈ Win1};

(cid:46) Return the winning region and

strategy for player 1.

Figure 8 is the game arena for experiment 1. The set of
states where the attacker makes a move is shown in grey
box. The rest are the defender’s states. Since the transition
is deterministic, we omit the labels on the transitions.

14

APPENDIX

4:

(cid:16)

(cid:46) Y is a set of unsafe states.
1 (Y ) ∪ Pre∃
2 (Y ))

Y ∪ (Pre∀
(cid:46) remove unsafe states from the set Zi.

(cid:17)

;

Fig. 8: The game arena of Experiment 1.

Figure 9 is the hypergame transition system given P1’s
sure winning strategy and P2’s perceived winning strategy in
experiment 1.

15

(0, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1, 2}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 1, 2}})(1, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {1, 2}, 3: {0, 1, 2}})(2, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1, 2}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 1, 2}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {1, 2}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 2}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 1}})(1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}})(1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}})(1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}})(1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1}})(2, 2, 1, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 1, 2}})(2, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1, 2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1, 2}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0,1,2}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {1, 2}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 2}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 1}})(0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {0, 1, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}})(3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1, 2}})(3, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}})(3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}})(0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {1, 2}, 3: {1, 2}})(2, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}})(3, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}})(3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}})(0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {1, 2},3: {0, 2}})(2, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}})(3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {2}})(3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}})(0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {1, 2}, 3: {0, 1}})(2, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1}})(3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1}})(3, 1, 0, {0: {1}, 1: {0, 1}, 2: {0,1,2}, 3: {0, 1}})(2, 2, 0, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 1, 2}})(0, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1, 2}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 1, 2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1, 2}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {1, 2}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 2}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 1}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {2}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {1}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0}})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {0, 1, 2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}})(3, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1,2}})(3, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}})(3, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}})(0, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1,2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {1, 2}, 3: {1, 2}})(2, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}})(3, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}})(3, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}})(0, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 2}})(2, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}})(3, 2, 1, {0:{1}, 1: {0}, 2: {0, 1, 2}, 3: {2}})(3, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}})(0, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1}})(2, 2, 1, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 1}})(2, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3:{0, 1}})(3, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1}})(3, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1, 2}})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1, 2}})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 2}})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 1}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {1, 2}})(1,2,0,{0:{1},1:{1},2:{0,2},3:{0,2}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 1}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {2}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {1}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0}})(1,2,0,{0:{1},1:set(),2:{0,2},3:{0,1,2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {0, 1, 2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}})(3, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1, 2}})(3, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1, 2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {1, 2}, 3: {1, 2}})(2, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}})(1, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {1, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1, 2}})(0, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}})(3, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 2}})(2, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}})(1, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {0, 2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0,2}})(0, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}})(3, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {2}})(3, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 1}})(2, 2, 0, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 1}})(2, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}})(1, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {0, 1}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1}})(0, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}})(3, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {2}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {1}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: set()})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {1, 2}})(1,2,1,{0:{1},1:set(),2:{0,2},3:{0,2}})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1}})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {2}})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1}})(1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {2}})(1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1}})(2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {1, 2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1, 2}})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1, 2}})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0}})(1, 1, 1, {0: {1}, 1: {0,1}, 2: {0, 2}, 3: {0}})(2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {0, 2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2},3: {0, 2}})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {0, 1}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1}})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {2}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {1}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0}})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: set()})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {1, 2}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 2}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {2}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1}})(0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2},3: {2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {2}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {2}})(3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {2}})(0, 1, 0, {0: {1},1: {0, 1}, 2: {0, 2}, 3: {1}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {1}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1}})(3, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1}})(2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {1, 2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1, 2}})(1, 2, 0, {0:{1}, 1: set(), 2: {0, 1, 2}, 3: {0}})(0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {0}})(2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0}})(3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: set()})(2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {0, 2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {0, 1}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1}})(1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: set()})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {2}})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {1}})(1, 2, 1, {0: {1}, 1: set(), 2:{0, 2}, 3: {0}})(1,2,1,{0:{1},1:set(),2:{0,1,2},3:set()})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {2}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}})(3, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1}})(2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {1}})(2, 2,1,{0: {1}, 1: {0}, 2: {0, 2}, 3: {1}})(3, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1}})(0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0}})(2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {0}})(2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0}})(3, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: set()})(1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: set()})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {2}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {1}})(1, 2, 0, {0: {1}, 1: set(), 2: {0,2}, 3: {0}})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: set()})(2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}})(3, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}})(2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {1}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1}})(2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {0}})(2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0}})(3, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: set()})(1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: set()})(1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: set()})16
Fig. 9: The winning regions of two players in Experiment 1.

((0, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q0’, ’q0’), ’q0’)((2, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((1, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q0’, ’q0’), ’q0’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((2, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q0’), ’q0’)((1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((2, 2, 1, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((0, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q1’), ’q1’)((0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q0’), ’q0’)((3, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((3, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((2, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((3, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1}}), (’q0’, ’q1’), ’q1’)((2, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {1}}), (’q0’, ’q1’), ’q1’)((3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}}), (’q0’, ’q1’), ’q1’)((0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((2, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 1, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((3, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q1’), ’q1’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q0’), ’q0’)((3, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q1’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((3, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((3, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((0, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((3, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((2, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((0, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((3, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}}), (’q0’, ’q1’), ’q1’)((2, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((3, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1}}), (’q0’, ’q1’), ’q1’)((3, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}}), (’q0’, ’q1’), ’q1’)((0, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((2, 1, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((3, 2, 1, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {0, 1, 2}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q1’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q1’), ’q1’)((3, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((3, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((1, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((2, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((0, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((1, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((2, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 1}}), (’q0’, ’q1’), ’q1’)((0, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {1}}), (’q0’, ’q1’), ’q1’)((3, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((1, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((0, 1, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0}, 2: {0, 1, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: set()}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1}}), (’q0’, ’q1’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 2}}), (’q0’, ’q1’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1}}), (’q0’, ’q1’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((1, 1, 1, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0}}), (’q0’, ’q1’), ’q1’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 1, 2}, 3: set()}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {1, 2}}), (’q0’, ’q1’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 1}}), (’q0’, ’q1’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0, 2}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((3, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {1, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {1}}), (’q0’, ’q1’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((0, 1, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0, 1}, 2: {2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0, 1}, 2: {0, 2}, 3: set()}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {0, 1}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: {0}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {0, 2}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: {1}, 2: {0, 2}, 3: set()}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: set()}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {1}}), (’q0’, ’q1’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: {0}}), (’q0’, ’q1’), ’q1’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((3, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1}}), (’q0’, ’q1’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((3, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: set()}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: set()}), (’q0’, ’q1’), ’q1’)((0, 1, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((2, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((2, 2, 1, {0: {1}, 1: {0}, 2: {2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((3, 2, 1, {0: {1}, 1: {0}, 2: {0, 2}, 3: set()}), (’q0’, ’q1’), ’q1’)((1, 2, 0, {0: {1}, 1: {1}, 2: {0, 2}, 3: set()}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: set()}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {1}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {2}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0}}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {1}}), (’q0’, ’q1’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: {0}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {1}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {2}}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: set()}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 1, 2}, 3: set()}), (’q0’, ’q1’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((2, 2, 0, {0: {1}, 1: {0}, 2: {2}, 3: {0}}), (’q1’, ’q0’), ’q1’)((3, 2, 0, {0: {1}, 1: {0}, 2: {0, 2}, 3: set()}), (’q0’, ’q1’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: set()}), (’q1’, ’q0’), ’q1’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: set()}), (’q0’, ’q0’), ’q0’)((1, 2, 1, {0: {1}, 1: set(), 2: {0, 2}, 3: set()}), (’q0’, ’q1’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: set()}), (’q1’, ’q0’), ’q1’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: set()}), (’q0’, ’q0’), ’q0’)((1, 2, 0, {0: {1}, 1: set(), 2: {0, 2}, 3: set()}), (’q0’, ’q1’), ’q1’)