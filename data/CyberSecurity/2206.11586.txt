1

MAGIC: A Method for Assessing Cyber Incidents
Occurrence

Massimo Battaglioni, Giulia Rafaiani, Franco Chiaraluce, Marco Baldi
Polytechnic University of Marche,
Department of Information Engineering,
Ancona, Italy
e-mail: {m.battaglioni, g.rafaiani, f.chiaraluce, m.baldi}@univpm.it

2
2
0
2

n
u
J

3
2

]

R
C
.
s
c
[

1
v
6
8
5
1
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—The assessment of cyber risk plays a crucial role for
cybersecurity management, and has become a compulsory task
for certain types of companies and organizations. This makes
the demand for reliable cyber risk assessment tools continu-
ously increasing, especially concerning quantitative tools based
on statistical approaches. Probabilistic cyber risk assessment
methods, however, follow the general paradigm of probabilistic
risk assessment, which requires the magnitude and the likelihood
of incidents as inputs. Unfortunately, for cyber incidents, the
likelihood of occurrence is hard to estimate based on historical
and publicly available data; so, expert evaluations are commonly
used, which however leave space to subjectivity. In this paper, we
propose a novel probabilistic model, called MAGIC (Method for
AssessinG cyber Incidents oCcurrence), to compute the likelihood
of occurrence of a cyber incident, based on the evaluation of the
cyber posture of the target organization. This allows deriving
tailor-made inputs for probabilistic risk assessment methods, like
HTMA (How To Measure Anything in cybersecurity risk), FAIR
(Factor Analysis of Information Risk) and others, thus consid-
erably reducing the margin of subjectivity in the assessment of
cyber risk. We corroborate our approach through a qualitative
and a quantitative comparison with several classical methods.

I. INTRODUCTION

The massive exploitation of data and information systems
in companies and organizations is motivating an increasing
attention to cybersecurity and its management. One of the main
pillars upon which cybersecurity management relies is cyber
risk assessment, for which a plethora of standards and models
exist. Risk does not have a unique deﬁnition, but according to
the NIST (National Institute of Standards and Technology) [2],
it is a measure of the extent to which an entity is threatened by
a potential circumstance or event, and is typically a function
of: (i) the adverse impacts that would arise if the circumstance
or event occurs; and (ii) the likelihood of occurrence. A unique
method for cyber risk assessment does not exist, but inter-
national standards provide general guidelines which should
be followed when designing a cyber risk assessment method.
For example, the ISO (International Standard Organization)
unpacks risk assessment into risk identiﬁcation, analysis and
evaluation [3], [4]. In the risk identiﬁcation phase, the critical
services are identiﬁed and the threats and vulnerabilities they

The material

the AEIT 2021
in this paper was presented in part at
International Annual Conference [1]. This research was supported in part by
the “Cyber Risk Assessment Models and Algorithms (CybeRAMA)” project
(Ref. 2019.0421), funded by the Cariverona Foundation within the “Research
and Development 2018” call (https://cyberama.dii.univpm.it/).

could cause are determined. Risk analysis, instead, is needed
to determine the likelihood of occurrence and the impact of
these threats [4], [5]. Finally, in the risk evaluation phase the
obtained results are compared with some pre-established risk
acceptance criteria [3], [4]. In this paper we focus on the
second one of the aforementioned phases, that is, risk analysis
and likelihood estimation.

Existing cyber risk assessment approaches can be grouped
into quantitative and qualitative methods. In the former ones,
the risk analysis phase is carried out through numerical eval-
uations, mostly based on probability theory, as in classical
PRA (probabilistic risk assessment) [6]. Widely speaking,
these approaches have some potential advantages, like their
robustness, reproducibility, and comparability of results. While
it seems generally reasonable to rely on past events for risk
analysis, in the speciﬁc case of cyber incidents these data may
be unavailable or incomplete and, thus, probabilistic methods
must resort to the help of experts, which makes them barely
practical in real-case scenarios [2].

Qualitative methods, in turn, exploit a nonnumerical ap-
proach and, therefore, they are often simpler to implement
and interpret. However, their results are scarcely reproducible
and comparable and, above all, they have an intrinsic nature
of subjectivity.

A. Contribution

To the best of our knowledge, a probabilistic method
enabling the computation of the likelihood of occurrence of
cyber incidents, based on the posture of the organization rather
than on subjective expert estimates, is currently missing. To
ﬁll this gap, we propose a method called MAGIC (Method for
AssessinG cyber Incidents oCcurrence), which is a probabilis-
tic model to quantitatively estimate the likelihood of a cyber
incident for a speciﬁc organization, starting from qualitative
assessment approaches based on questionnaires. Owing to its
own nature, MAGIC should be seen as a tool to be used in
conjunction with existing probabilistic cyber risk assessment
methods, to make their results more reproducible and less
subjective.

For this purpose, we consider four indicators representing
the awareness of the employees, the maturity, the complexity,
and the attractiveness of the target organization. These aspects
are commonly qualitatively assessed through questionnaires,

 
 
 
 
 
 
so we estimate the values of the corresponding indicators start-
ing from questionnaires concerning the target organization,
provided by one or more assessors. Then, the four indexes
are combined, using a probabilistic quantitative approach, in
order to ﬁnd the likelihood of occurrence of a given type of
cyber incident. In particular, we consider two scenarios: in
the ﬁrst simpliﬁed scenario, we obtain as output the estimated
frequency of occurrence of cyber incidents in a given time
period (and the associated likelihood); in the second scenario,
we obtain the probability that the organization will face exactly
one successful cyber incident in a given time period. Indeed,
in the latter case we assume that, after one cyber incident, the
organization will change its posture and thus a new assessment
needs to be performed.

MAGIC provides inputs that can be used in conjunction
with classical PRA methods such as HTMA (How To Measure
Anything in cybersecurity risk) [7], FAIR (Factor Analysis of
Information Risk) [8], as well as many others. In the former
method, it is assumed that the frequency of adverse events
follows a log-normal distribution, whose mean and variance
is not directly related to the organization posture. Similarly,
in the FAIR method, for a given adverse event, an expert is
needed to estimate the minimum, maximum and most likely
values for its frequency of occurrence. In the case of cyber
events, these processes often become subjective, due to the
lack of reliable historical data concerning each speciﬁc type
of cyber incident. Our target is to reduce such a subjectivity
as much as possible, by following and extending the approach
in [1], [9].

In short, according to the proposed method, the main effort
required from the target organization is to provide information
regarding the state of their technological systems and pro-
tection measures: expert evaluations are no longer required
and the relations among all the components of the considered
infrastructure do not need to be ﬁgured out. We consider ques-
tionnaires based on international cybersecurity standards and
frameworks, which are comprehensive and widely recognized,
but MAGIC is general and can be applied with other types of
questionnaires.

A basic version of our model has been introduced in [1].
Even though the approach proposed in this paper has some
similarities with that in [1], since the ﬁnal goal of both of
them is to estimate the likelihood of occurrence of cyber
incidents based on the organization posture, there are some
profound differences between them. The most important ones
are described next:

• the approach in [1] does not take the awareness of the
employees into account as a determinant factor for risk
assessment, which is instead considered in the model we
propose;

• we provide the set of controls to be used for complexity

assessment;

• the approach in [1] stands as an independent method for
probabilistic assessment, whereas MAGIC is a transversal
tool to provide tailor-made inputs for existing probabilis-
tic cyber risk assessment methods; owing to this, we are
also able to report a larger number of numerical results;

2

• in this paper, the ﬁnal likelihood is computed differ-
ently from that in [1], where the probability that the
organization faces a certain number of cyber attacks is
not considered in the computation. Through the novel
approach, instead, the likelihood of a cyber incident can
be easily converted into a frequency, comparable to that
obtained through expert evaluations in existing methods;
in this paper we also consider
the practical scenario in which the target organization
does not immediately realize that a cyber incident has
occurred.

• differently from [1],

B. Paper organization

The paper is organized as follows. In Section II we provide
a high-level description of many related works. In Section III
we recall the basic functioning of some probabilistic cyber
risk assessment methods. In Section IV the components of the
cyber incident occurrence model we propose are discussed.
In Section V we show how our approach can be combined
with known probabilistic methods in order to overcome their
main limitations when dealing with cyber risks. In Section
VI we present some numerical results, aimed at validating
the effectiveness of the proposed approach. In Section VII we
provide both a quantitative and a qualitative comparison of our
method with other approaches. Finally, Section VIII concludes
the paper.

II. RELATED WORKS

A plethora of approaches for cyber risk assessment can be
found in existing literature. Besides those proposed by national
and international organizations (like the mentioned ISO/IEC
(International Standard Organization/International Electrotech-
nical Commission) 27005:2018 [4] and NIST SP (Special Pub-
lication) 800-30 [2]), others have been introduced by public
and private organizations, like EBIOS (Expression des Besoins
et Identiﬁcation des Objectifs de S´ecurit´e) [10], CRAMM
(Central Computer and Telecommunications Agency Risk
Analysis and Management Method) [11], OCTAVE (Opera-
tionally Critical Threat, Asset, and Vulnerability Evaluation)
[12], MEHARI (MEthod for Harmonized Analysis of RIsk)
[13], MAGERIT (Risk Analysis and Management Methodol-
ogy for Information Systems) [14], IRAM2 (Information Risk
Assessment version 2) [15], IT-Grundschutz [16], and CORAS
[17]. An extensive and critical literature review can be found
in [18], [19]. A lot of attention has been devoted to solving the
problem of estimating the likelihood of occurrence of a threat
and the corresponding impact. For example, several methods
have been proposed using different techniques like Bayesian
networks [20], attack path graphs [21], fuzzy logic [22],
probabilistic model checking [23], vulnerability assessments
[24], Monte Carlo simulations [7], [8], and others.

Next we provide a brief description of some of the afore-
mentioned methods, highlighting the differences with MAGIC
as well as the possible common aspects. A deeper comparison
is then carried out in Section VII, after the description of our
method.

• ISO/IEC 27005:2018 [4]: is an international framework
for managing information risks. It describes all processes
for risk management, including risk assessment. In this
case, the estimation of the likelihood can be performed
in a qualitative, quantitative, or hybrid way. However,
the ISO/IEC 27005 standard only provides guidelines for
doing it, without describing any speciﬁc practical method.
• NIST SP 800-30 [2]: is a guide for conducting risk
assessment. In order to determine the likelihood of oc-
currence of a security incident, this method identiﬁes all
the potential vulnerabilities and the probability of their
exploitation. The likelihood is then described using a
qualitative or semi-quantitative scale. Such an approach is
opposed to the numerical one we propose, since it leaves
space to subjectivity.

• EBIOS [10]: is a scenario-based approach for risk man-
agement that relies on the establishment of a strong link
among different stakeholders. It uses a modular approach
for identifying risk causes. However, all the phases con-
sidered in this method, including risk assessment, are the
result of security debates among the team and, therefore,
they are subjective.

• CRAMM [11]:

is a qualitative method for risk as-
sessment. It measures risks as the product of asset,
threat, and vulnerability values. It uses trained experts.
Threats and vulnerabilities are not exhaustively assessed,
but the assessor can choose among different predeﬁned
threat/asset and threat/impact combinations. Relying on
structured questionnaires, and/or on the expertise of the
assessor, the method determines the likelihood of threats
and vulnerabilities; however, differently from MAGIC, it
does not directly link those likelihoods with the likelihood
of occurrence of a cyber incident.

• OCTAVE [12]: is an asset-driven method for assessing
information security risks. While the original approach
is designed for large organizations, the OCTAVE-S [25]
and the more recent OCTAVE-Allegro [26] versions can
be also applied to small and medium enterprises. This
method ﬁrstly identiﬁes all the assets, and then focuses
on the critical ones. For each of them, it determines the
related threats, and qualitatively labels their likelihood of
occurrence as “Low”, “Medium” or “High”.

• MEHARI [13]: is a risk management model with the
aim of helping the implementation of ISO/IEC 27005. It
performs the risk assessment phase through an audit. This
phase includes the identiﬁcations of assets, threats and
vulnerabilities. The likelihood of occurrence of a threat
is qualitatively described using a four levels scale.

• MAGERIT [14]: is an asset-oriented risk management
model. Using this model, security professionals evaluate
the assets and all the possible threats. Then, they describe
the likelihoods of occurrence of those threats using a
numerical scale with a limited number of levels. The
likelihood is usually evaluated relying on the annual rate
of occurrence of any speciﬁc threat for each speciﬁc asset,
which is substantially estimated through historical data.
• IRAM2 [15]: is a qualitative threat-driven method for
information risk assessment and treatment. The likeli-

3

hood of success of a threat is estimated using lookup
tables having as input
two numerical values describ-
ing the strength of the threat and that of the security
controls implemented by the organization. Then, using
this method, the residual likelihood is evaluated (also,
through a lookup table) as a combination of likelihood of
success and likelihood that an attacker will try to cause
an incident based on the considered threat.

• IT-Grundschutz [16]: is a qualitative method for identi-
fying and assessing security incidents. Using this method,
some qualiﬁed staff has to identify all possible threats.
Then, for each of them this method evaluates the fre-
quency of occurrence using a qualitative scale. This
frequency is ﬁnally combined with the impact through
a risk matrix.

• CORAS [17]: is a method for security risk analysis.
All the risk assessment phases are performed through
structured brainstorming, where people with different
backgrounds and competences collaborate to identify
assets, vulnerabilities, threats, and the related likelihoods
of occurrence. The likelihood assessment can be done
both in a qualitative or a quantitative way; however, the
evaluation is strictly related to the subjectivity of the
assessors.

• LiSRA [21]: is a risk assessment method taking a bidi-
mensional input; on one dimension domain-speciﬁc infor-
mation is required from an expert, while the other dimen-
sion is ﬁlled by the user, according to the practices of the
considered organization. Then, the risk is conventionally
obtained as the combination of the probability to have
successful attacks and their impact. The probability of
success is computed based on attack trees but, differently
from our method, the latter need to be entirely computed
by external experts. Therefore, MAGIC might be seen,
with some adaptations, as an alternative method allowing
to bypass the need of experts, rather than a completely
different approach.

• Risk analysis based on fuzzy decision theory [22]: the
ﬁrst step of this approach is to identify an expert; then,
a taxonomy of events and scenarios has to be deﬁned
(second step). Finally, the expert builds a matrix with
potential accidents on the rows and possible scenarios
on the columns: each entry of the matrix has to be ﬁlled
with a probability that the accident takes place in a certain
scenario. Also in this case, differently from our method,
external expert estimates are used. Starting from this
matrix, the fuzzy decision theory is then applied, which
returns the expected value of the considered option. Also
in this case, MAGIC is not in contrast with this method,
but can be seen as a variation of it. In fact, in MAGIC
the a-priori probabilities can be numerically derived from
the posture of the organization, rather than estimated by
an expert.

• CVSS-based risk assesment: the CVSS (Common Vul-
nerability Scoring System) [27] gives scores to threats
exploiting vulnerabilities on the basis of three categories.
One can combine this approach with attack graphs [24]
in order to derive the vulnerabilities starting from known

threat sources, or to bayesian decision networks [20],
in which the attacks are modeled starting from corre-
lated alerts. The probabilities that the considered threats
exploit certain vulnerabilities are computed from the
scores associated to each of them, according to CVSS. In
particular, CVSS 2.0 provides for the use of three types of
metrics: base, temporal and environmental. Base metrics
represent the intrinsic characteristics of threats exploit-
ing vulnerabilities that are constant over time and user
environments; temporal metrics represent the features of
threats exploiting vulnerabilities that change over time
but not over user environments; environmental metrics are
based on the characteristics of threats exploiting vulnera-
bilities that are unique to a particular user’s environment.
These methods may be directly comparable to MAGIC, in
that the probabilities derive from numerical assessments,
even though a subjective component is still required.
In particular, in the basic version of CVSS-based risk
assessment methods, the probability of occurrence of the
i-th item in the Common Vulnerabilities and Exposures
(CVE) list can be computed as a product of some CVSS
metrics, i.e.,

Li = AV × AC × Au × E × RC,

where AV is the access vector metric, AC is the access
complexity metric, Au is the authentication metric, E is
the exploitability metric and RC is the report conﬁdence
metric. The exact numerical values of all these metrics
need to be chosen in a range of preﬁxed values, which
makes this numerical approach also subjective.

There are several reasons that make many of the aforemen-
tioned approaches difﬁcult to apply in real case scenarios. In
fact, as explained above, they make risk assessment result
in a long process requiring the availability of a signiﬁcant
amount of data. Moreover, the application of these methods
often requires the help of an expert assessor external to the
organization and able to provide quantitative measures of the
cyber risk, which makes them cumbersome and exposed to
subjectivity. MAGIC, instead, allows computing quantitative
parameters starting from simple questionnaires, which makes
the risk assessment process straightforward and less exposed
to subjectivity.

III. PROBABILISTIC CYBER RISK ASSESSMENT:
PRELIMINARIES

In this section we describe two state-of-the-art probabilistic
cyber risk assessment methods based on Monte Carlo simula-
tions, i.e., HTMA [7] and FAIR [8].

A. Probabilistic risk assessment using HTMA

In a nutshell, the HTMA method for cyber risk assessment

[7] is based on the following steps:

• deﬁnition of the potential cyber threats;
• estimation of the likelihood of occurrence and impact of

each event;

• Monte Carlo simulation for generating the scenarios;
• results interpretation.

4

While the likelihood of occurrence of each threat is given
by a single value, the impact is associated to a 90% conﬁdence
interval, identiﬁed by a lower and an upper bound. All these
three numerical values are to be determined by external
experts.

The Monte Carlo simulation, in each scenario, works as

follows:
1. for any threat i, a real number r is generated by sampling
uniformly at random the range between 0 and 1, boundaries
included, denoted as [0, 1]. If r < Li, where Li is the
likelihood of the i-th threat, it is assumed that the event
has not happened, and vice versa;

2. the impact of events which did not occur is 0, whereas
the impact of occurred events is obtained by randomly
sampling a log-normal distribution of the impacts, obtained
according to the values of the given 90% conﬁdence
interval;

3. all the impacts of occurred events are summed, in order to

obtain an estimate of the total annual risk.
The results obtained with the Monte Carlo simulation are
used to construct the LEC (Loss Exceedance Curve), which
corresponds to the graphical representation of the complemen-
tary cumulative distribution function of the annualized loss
expectancy.

B. Probabilistic risk assessment using FAIR

The FAIR methodology [8] can be described through the

following four steps:

• deﬁnition of the scenario under exam and its decomposi-

tion into sub-scenarios;

• estimation of the parameters for any sub-scenario;
• generation of the frameworks through Monte Carlo sim-

ulations;

• results interpretation.
More generally, FAIR deﬁnes an ontology for the risk,
which is summarized in Fig. 1. The ontology describes how the
assessment of risk can be obtained; in particular, it considers
all the risk factors that contribute to the evaluation of the risk
and all the relationships between them. Risk factors can be
measured and estimated; then, it is possible to calculate risk
using mathematical expressions of the relationships among
factors. In essence, the risk is computed as a combination of
LEF (Loss Event Frequency) and Loss Magnitude. In order to
facilitate the process of risk evaluation, these two factors can
be individually decomposed in other factors that, in their turn,
could be further decomposed, as well. This way, the user can
assess the risk considering a speciﬁc layer of the ontology,
according to which factors he is able to estimate. The LEF
depends on several factors; the most relevant ones for our
model are redeﬁned next: the TEF (Threat Event Frequency)
is deﬁned as the frequency with which, in a given time period,
the attacker tries to breach the organization; the Vulnerability
is the probability of success of any of these breaches. The Loss
Magnitude, instead, is the sum of the losses caused by the a
certain primary threat event and of the losses caused by its
side-effects (such as, for example, the reaction of secondary

Risk

Loss Event
Frequency

Loss Magnitude

5

Threat Event
Frequency

Vulnerability

Primary Loss

Secondary Risk

Contact Frequency

Probability
of Action

Threat Capability

Difﬁculty

Secondary Loss
Event Frequency

Secondary Loss
Magnitude

Fig. 1. Ontology of the FAIR risk.

stakeholders), which are included in the concept of secondary
risk.

be directly related to the training programs the organization
supplies to its workers.

Conventionally, the outputs of the FAIR approach are scat-
terplots with LEF and Loss Magnitude on the x-axis and y-
axis, respectively, and/or tables summarizing various results of
the Monte Carlo simulation.

IV. CYBER INCIDENT MODEL

In this section we introduce the quantitative approach we
propose for estimating the likelihood of occurrence of a cyber
incident, called MAGIC. First of all, we provide some basic
deﬁnitions:

• Cyber threat [28]: any circumstance or event with the
potential to adversely impact organizational operations
(including mission, functions, image, or reputation), or-
ganizational assets, or individuals through an information
system via unauthorized access, destruction, disclosure,
modiﬁcation of information, and/or denial of service. We
will simply refer to cyber threats as “threats” in the
following.

• Cyber attack: any realized attempt to partially or totally
disclose, expose and/or compromise data by a malicious
entity. We will simply refer to cyber attacks as “attacks”
in the following.

• Cyber incident: any cyber attack which had success.
The proposed model, with all the key parameters, which
we will discuss next, is schematized in Fig. 2. We need to
distinguish between two types of threats: those coming from
external threat agents and those happening as a consequence
of human misbehavior, i.e., non-malicious threats. In Fig. 2,
the dashed arrows are referred to the former scenario, the
dotted arrows describe the latter, whereas the solid arrows are
valid for both scenarios. The model components in black are
described in detail in the following subsections, while those in
blue are not among the objects of our theoretical analysis, but
they are taken into account in Section VI, where numerical
results are provided.

A. Awareness of the employees

In order to practically evaluate the awareness, it is possible
to consider a list of best practices devoted to this issue, as that
in the NIST SP 800-53 [28], or to rely on speciﬁc subsets of
controls proposed in cybersecurity frameworks; for example,
Control 14 in the CIS (Center for Internet Security) Controls
[29] deals with security awareness and training programs. The
evaluation of the awareness parameter can be carried out in
different ways. For example, the assessor can determine if ev-
ery identiﬁed control is fully implemented by the organization
or not, simply associating to it a Yes or No answer. Another
approach can be using a scale in order to determine at which
degree each considered control is implemented and, therefore,
assigning better ratings as the level of implementation com-
pleteness of the control increases. With both these approaches,
an N/A option needs to be included; this should be used when
one or more controls are considered to be not applicable in
the context under examination. Then, according to the results
of the assessment, a numerical score should be assigned to
every control. For example, using the ﬁrst of the approaches
stated above, a score equal to 1 may be assigned to all the
controls with Yes as an answer, while 0 may be assigned to
the controls answered with No. Instead, when using the second
approach, one may consider as many scores as the number of
the possible implementation levels. In other words, if the scale
used for the implementation evaluation has 5 possible levels,
the score that can be assigned to each control can be, for
example, a number between 0 and 4. But, obviously, other
choices are possible. In the following, we denote by smax the
maximum score value (so, smax = 1 or smax = 4 in the
mentioned examples). Moreover, a weight may be assigned to
every control. The weights are given mainly according to the
aspects that the organization wants to stress out, so they are
not mandatory. In fact, an organization may want to focus its
analysis on some speciﬁc training strategies, while it may not
be interested in other ones; in this case, it can assign higher (or
lower) weights to the controls that it considers more crucial
(or less relevant). Finally, a weighted average is calculated in
order to obtain a ﬁnal value for the awareness. Note that the
N/A controls should not be counted in the average.

The awareness of the employees can be deﬁned as their
level of consciousness about the cybersecurity risks. It can

In this paper, we consider the awareness index as a number
between 0 and 10. Let E be the total number of considered

Awareness
of the employees

Attractiveness
of the organization

Number
of attacks

Maturity
of attackers

6

Risk

Likelihood
of occurrence of a
cyber incident

Impact
derived from the occurrence
of a cyber incident

Probability
of success
of an attack

Maturity
of the organization

Complexity
of the organization

Fig. 2. Relations among the components of MAGIC.

controls, si ∈ [0, smax] and ai ≥ 0 the score and the weight
associated to the i-th control, respectively; the awareness index
is computed as

Awareness Index = AI =

E
i=1 si × ai
E
i=1 ai

P

×

10
smax

.

(1)

P

Notice that, as shown in Fig. 2, the awareness directly
inﬂuences the maturity of the organization, formally deﬁned
in Section IV-B. This holds for both malicious and non-
malicious threats. Moreover, when speciﬁcally dealing with
non-malicious threats, the awareness is inversely related to
the number of threats. In other words, if employees are well-
trained about cyber risks, we can assume that the organization
will suffer fewer potential threats, since the employees are less
prone to causing cyber incidents.

B. Maturity of the organization

The maturity of an organization can be deﬁned as the level
of implementation of all practices and procedures that the
organization executes in order to reduce the risk of receiving
cyber attacks which may cause security breaches, data leakage,
denial of service, and so on. As mentioned in Section IV-A,
awareness concurs to determine maturity (being one of its
most relevant components) but other issues need to be taken
into account, too. The evaluation of this further part of the
maturity can be done by assessing the organization compliance
to the security controls proposed by one or more cybersecurity
frameworks. The frameworks chosen as the reference ones will
inﬂuence the area of application and, therefore, the kind of risk
the organization is going to evaluate. An organization, in fact,
may be interested in assessing the risk of suffering breaches,
or the risk of losing, as a consequence of an attack, data
conﬁdentiality and/or integrity and/or availability, or may be
interested in assessing the risk related to a combination of them
both. The security frameworks that can be used as reference
are, for example, the one proposed by CIS in [29] for assessing
cybersecurity risk, or the set of controls introduced by the

ENISA (European Union Agency for Cybersecurity) in [30]
for assessing data protection risk, or the NIST Cybersecurity
Framework [31] for assessing both cybersecurity and data
protection risks. Therefore, ﬁrst of all, the organization should
choose the kind of risk it intends to assess and, according to the
choice, select one or multiple appropriate security frameworks
as reference. The next step is evaluating if and at which level
all the security controls listed in the reference framework are
implemented within the organization. Using the controls of
cybersecurity frameworks to assess the maturity will result
in an accurate picture of the actual cyber posture of the
organization. In fact, since the security frameworks are usually
considered as a set of best practices, their use will lead to a
robust and complete mapping of the area of interest. Moreover,
since the security frameworks are continuously updated, the
maturity can be dynamically assessed, simply repeating the
evaluation as soon as new versions of the frameworks are
released.

Because of the need to combine the mentioned different
aspects, the maturity index evaluation is performed through
a two-step procedure. The ﬁrst step is equivalent to that
described in the previous section for the evaluation of the
awareness index. Clearly, the controls of the chosen framework
about cybersecurity awareness and training programs must be
excluded from the list used for this part of the evaluation.
Thus, after answering to all the remaining controls, a score is
associated to any answer. The N/A option should be included as
well. Also in this case, it is possible to assign a weight to each
control, according to its relevance for the kind of assessment
the organization wants to perform. Let T be the total number of
considered controls, s′
i ≥ 0 respectively the
score and the weight associated to the i-th control; a weighted
average is computed as follows
i=1 s′

max] and a′

i ∈ [0, s′

i × a′

T

i

(2)

M =

T

i=1 a′

i

P

×

10
s′
max

.

Then, in the second step of the procedure, the maturity index

P

is eventually obtained as a weighted average of the awareness
index and M. In this case, AI and M are weighted by E/(E +
T ) and T /(E + T ), respectively. We ﬁnally obtain

Maturity Index =

AI × E + M × T
E + T

.

(3)

We observe that, according to this deﬁnition, the maturity
index is a real number ranging between 0 and 10.

Note that

the maturity of the organization will directly
inﬂuence the probability of success of an attack. In fact, a
higher maturity means that the organization has addressed
more attention to cybersecurity practices and, therefore, the
probability for an attack attempt to be successful will decrease,
and vice versa.

C. Complexity of the organization

The complexity of an organization can be deﬁned as the
measurement of the intricacy of its technological infrastructure
and of how the processes, the activities, and the services are
managed. The concept that the risk does not only depend
on the maturity of the organization, but also on its com-
plexity is introduced in [29], where three IGs (Implemen-
tation Groups) are deﬁned. Following that approach, every
organization should identify itself, mainly according to its
dimension, in one of the proposed IGs and, therefore, should
implement a speciﬁc subset of security controls. This is due
to the fact that smaller organizations are usually expected to
be less exposed to threats, when compared to larger organiza-
tions. However, the dimension should not be considered as
the only discriminating parameter. For this reason, starting
from the controls proposed in [32] for the evaluation of
the inherent risk, we have identiﬁed a set of punctual and
speciﬁc controls useful to assess the inherent complexity of
an organization1. Coherent with the above considerations, the
controls we address do not consider only the dimension of
the organization, but they try to identify all possible critical
points of hardware, software, networks, and facilities. More
precisely, we have grouped all the considered controls into
ﬁve categories: Networks and Infrastructure, IP (Internet Pro-
tocol) network technologies, Applications, Services and IT
(Information Technology) department. Therefore, beside the
number of employees, the assessment includes the number and
the characteristics of the components (physical and software
systems) and their interconnections, the number of services
and their characteristics, and the entropy of the IT system
management. In order to facilitate the assessment, for each
control we have identiﬁed ﬁve possible guided answers. The
answers are associated to Very Low, Low, Medium, High, and
Very High complexity.

In order to practically assess the complexity, the process
is equivalent to the maturity assessment. After evaluating all
the controls, a score is associated to every evaluation. The
N/A option should be included as well. Also in this case,
it is possible to assign a weight to each control, according
to its relevance for the kind of assessment the organization

1The set of controls considered in this paper for the complexity assessment

can be found at https://github.com/secomms/cyber-risk-assessment.

7

i,j ∈ [0, s′′

j,max] and a′′

wants to perform. Then, a weighted average is calculated in
order to obtain a complexity index for each one of the ﬁve
categories considered above. Similarly to the previous indexes,
we consider the complexity index as a number between 0 and
10. Let Cj be the number of controls included in the category
j, s′′
i,j ≥ 0 respectively the score and
the weight associated to the i-th control of the j-th category;
the complexity index of the category j is computed as
Cj
i,j × a′′
i=1 s′′
Cj
i=1 a′′
Finally, a global complexity index for the organization is
computed as the weighted average of the ﬁve complexity
indexes computed before; in this case, the weight associated
to each category is simply computed as the number of controls
included in that category divided by the total number of
controls, i.e., bj = Cj/(

5
j=1 Cj). Then, we have

s′′
j,max

CI(j) =

(4)

P

P

10

×

i,j

i,j

.

Complexity Index =

P

5

(CI(j) × bj).

(5)

j=1
X

We observe that the same approach with the two weighted
averages could be used for the maturity assessment when the
security controls chosen for the evaluation are divided into
categories.

Note that the complexity of the organization will inversely
inﬂuence the probability of success of an attack. In fact, a
higher complexity means that the technological infrastructure
of the organization is more intricate and, therefore, it is more
prone to being successfully targeted. In other words,
the
probability for an attack attempt to be successful will increase
for increasing complexities, and vice versa, according to the
law that will be described in Section IV-F.

D. Attractiveness of the organization

The attractiveness of an organization can be deﬁned as the
level of interest the organization causes in potential attackers.
The attractiveness depends on several factors, as the type of
business, the kind and the amount of data the organization
manages, etc. The insertion of this parameter in our model is
due to the assumption that cyber criminals will likely attack
organizations from which they can obtain larger proﬁts. The
dimension of the organization does not affect its attractiveness.
In fact, for example, small organizations may operate in critical
environments and/or may process a large amount of sensi-
tive data, making them more attractive to cyber criminals if
compared to larger organizations with a limited technological
value.

In order to practically estimate the attractiveness, we have
examined the data proposed in cybersecurity reports like [33].
In particular, we have considered the number of attacks any
type of organization has suffered in one year, with respect to
the total number of attacks analyzed in the report. According
to the given data, we have classiﬁed the type of organization
as reported in Table I.

Therefore, during the assessment, the organization should
simply identify itself in one of the proposed types of business,
and the attractiveness will be assigned consequently.

TABLE I
CLASSIFICATION OF THE ORGANIZATIONS AS A FUNCTION OF THE
PERCENTAGE π OF ATTACKS RECEIVED WITH RESPECT TO THE TOTAL
NUMBER.

Percentage of attacks
π < 1.25%
1.25% ≤ π < 2.5%
2.5% ≤ π < 5%
5% ≤ π < 10%
π ≥ 10%

Type of organization
Very lowly attractive
Lowly attractive
Averagely attractive
Highly attractive
Very highly attractive

Note that the attractiveness will inﬂuence both the number
of attacks and the maturity of attackers. In fact, we assume
that a highly attractive organization will likely suffer more
and more structured attacks if compared to a lowly attractive
organization.

E. Maturity of attackers

Given the deﬁnition of cyber attack proposed in Section
IV, let us devise the attacker model. As mentioned above,
we should distinguish between attacks coming from malicious
attackers, and threats deriving from the lack of awareness
of the employees. For the probabilistic model we devise,
however, there is no need to mathematically distinguish be-
tween these cases. Thus, for the sake of simplicity, in the
rest of the paper we will also refer to non-malicious threats as
“attacks”, keeping in mind that they come from non-malicious
employees, who assume the role of unaware attackers in the
model.

In general, the organization might be targeted by multiple
attackers, but we assume that they cannot conduct more than
one attack in the same time slot ∆t. The value ∆t can be
chosen sufﬁciently small, so that attacks performed in close
periods of time can be distinguished. Moreover, under this
assumption, the identity of the attacker does not play a relevant
role and, therefore, we will generically refer to “the attacker”.
Another assumption that we make, in order to keep the analysis
feasible, is that different attack attempts are not correlated. In
other words, each attack attempt does not depend on previous
attack attempts, and its outcome does not inﬂuence future
attack attempts. Clearly, this hypothesis may not always be
veriﬁed and, in the scenario where the assumptions are too
optimistic, our model provides a lower bound to the likelihood
of the adverse events, rather than an estimate.

As shown in Fig. 2, the attractiveness is related to the ma-
turity of the attackers: attractive organizations will more likely
face more structured attacks, and vice versa. The maturity of
the attackers, in its turn, inﬂuences the probability of success
of an attack, described in the next section.

F. Probability of success of an attack

Obviously (and luckily) attacks are not always successful
and, therefore, more than one attempt may be needed before
breaking through the organization defenses. The single attack
attempt
is thus associated to a probability of success. In
particular, the computation of the probability of success of a
single attack takes into account the four key indexes described

8

above. Considering the maturity of the organization as a
variable x, we need a function that decreases when x increases,
since we expect more mature organizations to be more resistant
to cyber attacks, and vice versa. Furthermore, we do not
expect this trend to be linear, since slight improvements of
the maturity of a very immature organization may not be
sufﬁcient to signiﬁcantly decrease the probability of success
of an attack and, similarly, very mature organizations should
not need to further improve signiﬁcantly their posture, since
the probability of an attacker breaching them may already be
low enough. A function that may ﬁt well this scenario is the
logistic function [34]

f (x) =

K
1 + e−B(x−t) ,

(6)

where K is the saturation level, i.e., the upper horizontal
asymptote that limits the curve’s maximum value; t is the mid-
point, i.e., the value of x corresponding to half the saturation
level and B is the growth rate, i.e., the steepness of the curve.
In order to obtain the aforementioned trend, B must be chosen
negative. According to (6), f (x) has a lower asymptote equal
to 0.

It is possible to extend (6), by considering a lower asymptote
different from 0 and a non-symmetric shape, thus obtaining the
so-called generalized logistic function [35]

f (x) = A +

K − A
(1 + Q × e−B(x−x0))1/ν ,

(7)

K−A

where A is now the lower asymptote, Q is a variable, related
to f (0), that inﬂuences the inﬂection point, f (x0) = A +
(1+Q)1/ν and ν > 0 determines the asymmetry of the curve.
Notice that, by choosing Q = ν = 1, x0 corresponds to
the point at which the curve is at its midpoint and has the
maximum slope.

By using the generalized logistic function, with Q = ν = 1,
to express the probability of success of a single attack Ps(x),
(sometimes Ps, for notation simplicity, in the following) we
have

Ps(x) = A +

K − A
1 + e−B(x−x0) .

(8)

By increasing x0, Ps(x) shifts toward right. So, we associate
the value of the complexity of the organization to x0, to take
into account that, for the same value of x, more complex orga-
nizations are expected to face attacks with a larger probability
of success.

According to the discussion in Section IV-B, the maturity x
takes values in the range from 0 to 10. Moreover, we assume
that the probability of success cannot reach the extreme and
thus unrealistic values of 1 and 0. So, we set the maximum
value U at x = 0 and the minimum value L at x = 10.
Consequently, K and A depend on x0 and can be easily
obtained by solving the system with f (0) = U and f (10) = L.
An example of curves for different values of x0 is shown in
Fig. 3.

Indeed, as mentioned above, a more signiﬁcant value of the
probability of success can be obtained by taking into account
the maturity of the attackers, the latter playing the role of

x0 = 1
x0 = 5
x0 = 9

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
P

0

0

1

2

3

4

6

7

8

9

10

5
x

PMF (probability mass function) associated to the binomial
distribution is therefore

9

t
n

(cid:19)

t
n
(cid:19) 

Pr(N = n) =

(cid:18)

Pr(A = 1)nPr(A = 0)(t−n)

n

navg

(t−n)

navg

1 −

.

(10)

=

(cid:18)

t !

t !
In (10) we have denoted by navg the average number of
attack attempts suffered by the organization in the considered
time period, so that, reminding the properties of the binomial
distribution, Pr(A = 1) = navg
t = 1 − Pr(A = 0). A possible
choice is to pick navg as the number of attack attempts suffered
by the organization in a previous time period having the same
length of the considered one.

Fig. 3. Probability of success of a single attack (Ps) for different values of
x0. B = −1, U = 0.97, L = 0.03.

Then, wishing to calculate the probability that the organi-
zation suffers at most (or at least) n attack attempts, we have

weight coefﬁcient w. Explicitly this means that the ﬁnal value
of the probability of success of a single attack results in

p∗ = ps(x) = wPs(x).

(9)

A possible choice for the values of w is given in Table
II, showing how more attractive organizations suffer attacks
which are more likely to be successful because of the higher
maturity of the attackers. In this case, we assume that a linear
law models well the expected behaviour. We observe that, in
case of non-malicious threats, w is always equal to 1, since
attractiveness does not play any role.

TABLE II
POSSIBLE VALUES FOR w AS A FUNCTION OF THE ATTRACTIVENESS OF
THE ORGANIZATION.

Attractiveness
w

Very Low
0.6

Low Medium High
0.9
0.8
0.7

Very High
1

G. Number of attacks

Let us consider a time period containing t time slots, each of
duration ∆t. An attacker (malicious or non-malicious, accord-
ing to the discussion in Section IV-F) may perform an attack
attempt or not in any time slot, with a certain probability.
Therefore, at most t attack attempts will be suffered by the
organization in the considered time period. Let us deﬁne two
discrete random variables:

• A: represents the outcome of the attacker choice in a
time slot. Either he performs an attack attempt (A = 1),
or he does not (A = 0). Therefore, A is a binary random
variable and follows a Bernoulli distribution.

• N : represents the number of attack attempts suffered
by the organization in the considered time period. Its
realization n respects the condition 0 ≤ n ≤ t.

We assume that N follows a binomial distribution2. The

2An alternative, which might be considered when navg is signiﬁcantly
smaller than t, is the Poisson distribution, whose probability mass function is
Pr(N = n) = λn

n! e−λ, where λ = navg.

n

Pr(N ≤ n) =

Pr(N = n)

Xk=0
t

Pr(N ≥ n) =

Pr(N = n),

Xk=n

respectively.

H. Likelihood of occurrence of cyber incidents

In the previous section, we have analyzed the probability
that the organization suffers a certain number of attack at-
tempts. Even though this represents an interesting information
for the stakeholders, it is more relevant to estimate the likeli-
hood of these attacks being successful. In fact, in the deﬁnition
of risk [2] the likelihood of a successful attack, i.e., a cyber
incident, is multiplied by its impact, and the impact of non-
successful attacks is obviously 0. We therefore introduce a
further random variable S, which models the number of cyber
incidents faced by the organization in the considered period
of time. We consider two scenarios:

• the likelihood of facing s cyber incidents, after at most t
attempts by the attacker. In this simplistic approach, it is
assumed that the organization does not promptly realize
when it is successfully attacked, and, therefore, does not
take the appropriate countermeasure to change posture;
• the likelihood of facing exactly one cyber incident, after
at most t attack attempts by the attacker. In this more
realistic approach, it is assumed that the organization
immediately notices the breach, and tries to improve its
posture.

1) Organizations which do not change posture:

In this
scenario, having assumed that the organization does not react
immediately after experiencing one or more cyber incidents,
we can model
the attacks as Bernoulli experiments, with
success probability given by (9). We can actually assume that
also the outcome of the single attack attempt is a continuous
random variable P with realization p. In particular, taking
into account the uncertainty inherent in the problem (where
the maturity index, for example, results from the opinion of
some experts or from the assessment of questionnaires) we

 
assume that P follows a PERT (Project Evaluation and Review
Techniques) distribution, with pm = ps(min{x + q, 10}),
pM = ps(max{x − q, 0}), for some arbitrary value of q, and
p∗ = ps(x) respectively the minimum, maximum, and most
likely values. In the following, we have considered q = 1.
Therefore, the probability that s out of n ≤ t trials are
successful is given by

Pr(S = s|N = n) =

pM

ps(1 − p)(n−s)fP(p)dp,

n
s

where

fP (p) =

pm (cid:18)
Z

(cid:19)
(p − pm)α−1(pM − p)β−1
B(α, β)(pM − pm)α+β−1 ,
∗−pm
pM −pm

being α = 1 + 4 p
B(α, β) =
the likelihood, we ﬁnally obtain

, while
0 xα−1(1 − x)β−1dx is the Beta function. For
R

∗
and β = 1 + 4 pM −p
pM −pm

1

L(NC)(s) = Pr(S = s)

t

=

Pr(S = s|N = n)Pr(N = n).

(11)

n=1
X

2) Organizations which change posture: In this scenario,
the organization runs into a single cyber
we assume that
incident, detects it immediately and decides to promptly take
countermeasures, changing its posture. Therefore, a new cyber
risk assessment should be performed after one successful at-
tack, since the initial conditions, and the value of the maturity
index above all, should have been changed. In this case, we
consider a cumulative geometric distribution, i.e.,

Pr(S = 1|N = n) =

pM

n

p(1 − p)(k−1)fP (p)dp.

pm

Z

Xk=1

Then, the likelihood of occurrence of a single cyber incident
can be obtained by considering all possible values of n, i.e.,

L(C) = Pr(S = 1) =

t

n=1
X

Pr(S = 1|N = n)Pr(N = n).

(12)

V. CYBER INCIDENT MODEL IN HTMA AND FAIR

In this section we show how the values of the likelihood
(or frequency) computed in Sections IV-H1 and IV-H2 can be
plugged into HTMA and FAIR, minimizing the subjectivity
of the assessor and, rather, allowing the computation of risk
according to the posture of the considered organization.

A. Use of MAGIC with HTMA

According to the description in Section III-A, HTMA does
not take into account the possibility that a threat happens
more than once in a year and, therefore, we must consider
the likelihood that the organization faces the threat just once.
This means that the scenario described in Section IV-H2 must
be applied. As for the monetary impact of these threats, they
are not object of investigation in this paper and, therefore, we
rely on available results.

In particular, MAGIC can be used to estimate the likelihood
of occurrence of the threats considered in the Monte Carlo

10

simulation (point 1. in Section III-A). In fact, instead of relying
on the expertise of the assessor, it is possible to employ the
likelihood obtained through our cyber incident model (12) as
input to the HTMA method. However, we need to associate
a likelihood to every single cyber threat in the considered list
(and not a single, general likelihood value) and, therefore,
the model must be adapted. Explicitly the procedure works
as follows. We start by deﬁning a list of threats. This list is
then combined with the list of considered controls through a
table containing weight coefﬁcients ωi,j as shown in Table III,
where τj and γi denote the j-th threat and the i-th control,
respectively.

TABLE III
WEIGHT COEFFICIENTS FOR LIKELIHOOD ASSESSMENT IN HTMA.

τ1
ω1,1
ω2,1
..
.

γ1
γ2
..
.
γm ωm,1

τ2
ω1,2
ω2,2
..
.
ωm,1

· · ·
· · ·
· · ·
. . .
· · ·

τn
ω1,n
ω2,n
..
.
ωm,n

First of all, one should answer the question “if the control
γj is not implemented, does the risk relative to the threat τi
increase?”. If not, then ωi,j is 0, otherwise a non-zero value
can be assigned to ωi,j, according to some predetermined rule.
This way, for each threat we can select a subset of controls that
includes only those controls that have a non-zero weight ω for
that speciﬁc threat. Notice that the subsets are not necessarily
disjoint. We can compute a maturity index for each of these
subsets and obtain the likelihood of occurrence of a cyber
incident caused by the threat τj, noted by Lj, following the
procedure presented in the previous sections.

B. Use of MAGIC with FAIR

Referring to the FAIR ontology in Fig. 1, the blocks that are
directly inﬂuenced by our model are the TEF, the Vulnerability
and the LEF (both for the primary and the secondary threat
event). Clearly, these blocks will also indirectly inﬂuence the
upper layers of the ontology which, however, also depend on
blocks which are not object of our analysis. We remind that the
TEF is deﬁned as the frequency with which, in a given time
period, the attacker tries to breach the organization. In Section
IV-G, we have devised a model that returns the probability
that the organization faces a given number of attacks. As
discussed below, the link between number of attacks and
frequency of the attacks is immediate, once the reference
time period has been deﬁned. The Vulnerability, deﬁned as
the probability of success of any of these breaches, assumes
the same meaning of the probability of success of the attacks,
deﬁned in Section IV-F and denoted as p∗. Finally, the LEF is
obtained by combining the other two parameters; it is deﬁned
as the frequency with which the attacker succeeds, causing
a monetary loss to the organization. Also this parameter
has been covered by our analysis, especially that in Section
IV-H1, where the likelihood that the organization is breached
a given number of times is computed through (11). Also in
this case, the connection between likelihood and frequency is
straightforward.

In the original FAIR approach, it is assumed that the LEF
follows a PERT distribution, whose parameters must be com-
puted based on experts’ estimates. Using MAGIC, we maintain
the Monte Carlo approach, as already done for the HTMA
approach, but we sample the LEF from the distribution of S
(i.e., Pr(S = s)) reported in (11), which is computed taking
into account both the TEF and the Vulnerability. The random
sampling can be performed very easily, since it is referred to
a discrete distribution: we can associate a probability to each
allowed value of s and sample according to these probabilities.
Notice that, once s is obtained by random sampling, the
LEF in the considered framework can be simply computed
as LEF = s
t .

VI. NUMERICAL RESULTS

In this section we provide some numerical results, which

aim to validate the effectiveness of MAGIC.

A. Case study with HTMA

In order to take into account a realistic scenario, we have
considered the list of threats given in [9, Table 1] (originally
taken from [36]), and here reported, for the sake of clarity,
in Table IV. Furthermore, as a case study, we have analyzed
a dummy organization in the “Healthcare” sector. We have
simulated a posture assessment, ﬁnding, for each threat, a
maturity index as in Table IV. As for the attractiveness,
according to the discussion in Section IV-D, we have found
that organizations in the healthcare sector are very highly
attractive. Indeed, this is not surprising, and well-known in
the literature. Finally, we have considered a range of values
for the complexity index, from 4.5 to 7.5 with step 0.5. We
have also considered different values of navg, ranging from
2 to 5; we have assumed that, in each simulation, navg is
the same for all threats (for the sake of comparison between
different threats). As in [1], we choose B = −2, U = 0.97 and
L = 0.03 as parameters of the generalized logistic function.
Looking at Table IV we notice that we are talking about
an attractive organization, which however has some serious
maturity shortcomings, which may lead to weaknesses against
cyber threats.

The parameters of the PERT distribution, along with the
likelihood of occurrence of each threat in a year (t = 365
and ∆t = 1, assuming that the attacker performs at most one
attack a day), computed by (12), are also shown in Table IV
for navg = 4 and complexity index 5. As expected, due to
the relatively low values of the maturity indexes associated to
each threat, the organization has a relatively high probability
threats
of suffering a cyber incident
corresponding to lower maturity indexes are less likely to be
suffered than threats corresponding to higher maturity indexes.
Regarding the monetary impacts, we have considered those
in [9, Table 2] and also reported in Table V, for the sake of
completeness. In order to keep a uniform notation with the rest
of the paper, we have applied a currency exchange from dollars
(used in [9]) to Euro. Notice that, assuming that all threats
happen and the associated cyber incidents cause the largest
possible loss (given by the upper value of the corresponding

through them. Still,

11

C
E
L

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Complexity index = 4.5
Complexity index = 5
Complexity index = 5.5
Complexity index = 6
Complexity index = 6.5
Complexity index = 7
Complexity index = 7.5

2

4

6

8
Loss [million Euro]

10

12

Fig. 4. LEC for the considered (very highly attractive) organization, for
different values of the complexity index and navg = 4, using the HTMA
method.

C
E
L

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

2

4

6

8
Loss [million Euro]

10

12

Fig. 5. LEC for the considered (very highly attractive) organization, for
different values of navg and complexity index equal to 5, using the HTMA
method.

interval), we obtain an upper bound for the total loss which, in
the proposed example, is a value between 11.5 and 12 million
Euro (precisely, 11.8361 million Euro).

As usual, we express the ﬁnal output of the model as a
LEC, summarizing the probability that the loss faced by the
organization will be greater than or equal to a certain value,
obtained with a Monte Carlo simulation with 10, 000 trials.

We show the LECs, obtained using the aforementioned
values of the complexity and considering navg = 4, in Fig.
4. Notice that, coherent with our analysis, more complex
organizations are more likely to suffer cyber attacks and,
therefore, they will be subject to larger monetary losses.

Finally, we have shown in Fig. 5 the LECs for complexity
index equal to 5 and different values of navg. Also in this
case, as expected, the losses increase for increasing values of
navg, which we have chosen as the number of attack attempts
suffered in the previous year, relatively to each threat.

TABLE IV
PARAMETERS OF THE PROPOSED MODEL, AS INPUTS OF HTMA FOR THE CONSIDERED SCENARIO, WHEN THE COMPLEXITY INDEX IS 5 AND navg = 4.

12

ID
1
2
3
4
5
6
7
8
9

Threat
Malware
Web-based attacks
Denial of services
Malicious insiders
Phishing and social engineering
Malicious code
Stolen devices
Ransomware
Botnets

Maturity index
4.3
5.6
3.6
1.9
3.6
6.0
4.8
5.1
4.3

pm
0.28
0.11
0.43
0.79
0.43
0.08
0.19
0.16
0.28

pM
0.72
0.43
0.83
0.95
0.83
0.34
0.62
0.55
0.72

LC
0.86
0.61
0.92
0.97
0.92
0.52
0.78
0.72
0.86

p∗
0.50
0.23
0.66
0.90
0.66
0.17
0.38
0.32
0.50

104

6

5.5

5

4.5

4

3.5

3

2.5

TABLE V
IMPACT RANGE OF THE CONSIDERED THREATS, IN MILLION EURO.

ID
1
2
3
4
5
6
7
8
9

Threat
Malware
Web-based attacks
Denial of services
Malicious insiders
Phishing and social engineering
Malicious code
Stolen devices
Ransomware
Botnets

Impact range
[2.1360, 2.3941]
[1.8156, 2.0381]
[1.4151, 1.5842]
[1.2816, 1.4329]
[1.1748, 1.3172]
[1.1659, 1.2994]
[0.77875, 0.87576]
[0.48060, 0.53845]
[0.31684, 0.35600]

B. Case study with FAIR

We have considered the same scenario described in [8,
Chapter 8]. For the sake of simplicity, we have assumed
that the Loss Magnitude is only caused by primary losses
and then ignored secondary risk. Clearly, this assumption can
be removed without changing the rationale of the model.
Concerning the Loss Magnitude, we have not changed the
minimum, maximum, and most likely values (and neither we
have changed the conﬁdence level) for the considered cate-
gories in [8, Chapter 8]: productivity, response, replacement,
ﬁnes & judgements, reputation and competitive advantage.
The only categories having non-zero impact values are re-
sponse and replacement and, for the sake of completeness, we
have reported them in Table VI. Also in this case, we have
considered a dummy organization in the “Healthcare” sector.
Simulating a different posture assessment from the previous
section, we have found a complexity index equal to 5.2 and a
maturity index equal to 6.9. Being in the healthcare sector,
the organization is considered again very highly attractive.
The values of B, U and L are once again set as −2, 0.97
and 0.03, respectively. The analysis is carried out on daily
intervals within a year, i.e., t = 365 and ∆t = 1, and we have
considered navg = 84. Notice that this value is signiﬁcantly
larger than those adopted for the examples in Section VI-A,
just in view of enlarging the ensemble of considered scenarios.
Running the Monte Carlo simulation, we obtain the Loss
Magnitudes in Fig. 6. Each point corresponds to a different
simulation. The red circle in the ﬁgure represents the mean
value (for both axes). Deﬁning the k-th percentile as the
score below which the k% of the scores fall in the given
distribution, we have found that the 10-th percentile and the
90-th percentile, for this example, are approximately 18, 000
C and 58, 000 C, respectively. The results of the Monte Carlo

10-2

LEF

10-1

Fig. 6. Loss magnitude for the considered organization, which is very highly
attractive, when the complexity index is 5.2 and the maturity index is 6.9,
obtained through the FAIR method.

simulation are also summarized in Table VII where, besides
the minimum, the maximum and the mean, also the mode
(that is the most frequent value occurred in the Monte Carlo
simulation, differently from the most likely value, that is the
result of the statistical inference for the same quantity) is
reported.

Clearly, ﬁxed t, a smaller value of ∆t increases the number
of possible attacks per year (which is now 365), thus increasing
the number of possible values of the LEF, in its turn.

In order to evaluate the impact of different values of the
maturity index, complexity index and navg, we have ﬁxed
two of these parameters and let the other change, obtaining
Figs. 7, 8 and 9, respectively. The results are coherent with
our analysis: on the one hand, for increasing values of the
complexity and of navg, the total loss exposure increases; on
the other hand, the total loss exposure decreases when the
organization has increasing values of the maturity.

VII. COMPARISON AND DISCUSSION

In this section we perform both a quantitative and a quali-

tative comparison of MAGIC with other methods.

A. Quantitative comparison

Let us consider the same organization in the “Healthcare”
sector as in Section VI-A, for which a posture assessment has

TABLE VI
IMPACT RANGE OF THE CONSIDERED EVENT, IN C.

13

Category
Response
Replacement

Minimum Maximum Most Likely
8, 250
30, 000

22, 000
50, 000

2, 750
20, 000

Conﬁdence
20
20

TABLE VII
SUMMARY OF THE RESULTS OF THE MONTE CARLO SIMULATION, WHEN THE COMPLEXITY INDEX IS 5.2 AND THE MATURITY INDEX IS 6.9.

Primary Loss Events per year
Primary Loss Magnitude (C)
Total Loss Exposure (C)

Minimum
1
28, 709
35, 369

Mean
9.5
39, 045
369, 260

Mode
8
32, 948
295, 520

Maximum
27
52, 822
1, 141, 600

3.5

3

2.5

2

1.5

1

0.5

0

0

106

Minimum
Mean
Mode
Maximum

1

2

3

4
Complexity

5

6

7

8

106

3

2.5

2

1.5

1

0.5

0
40

Minimum
Mean
Mode
Maximum

60

80

100

120

140

160

180

200

220

240

Fig. 7. Minimum, mode, mean and maximum of the total loss exposure, for
different values of the complexity index.

Fig. 9. Minimum, mode, mean and maximum of the total loss exposure, for
different values of navg.

Minimum
Mean
Mode
Maximum

106

6

5

4

3

2

1

0

0

0.5

1

1.5

2

2.5
Maturity

3

3.5

4

4.5

5

Fig. 8. Minimum, mode, mean and maximum of the total loss exposure, for
different values of the maturity index.

returned a complexity index equal to 5. The maturity index
associated to each of the considered threats is given in Table
IV (third column). Owing to its sector, the organization is
considered very highly attractive.

We have computed the likelihood of occurrence of cyber
incidents caused by the same threats using the basic version

of CVSS-based risk assessment method, relying on the funda-
mentals described in [27]. The low level metrics we use for
the comparison are those given in [27], which we also report
in Table VIII for the sake of completeness. The assignment of
the numerical values to these CVSS low level metrics, which
is the possibly subjective phase of this approach, has been
performed by means of a brainstorming session, based on
the experience of the authors and on available evidences. It
must be stressed that some assignments are not subjective:
for example, the Access Vector of stolen devices or malicious
insiders is objectively “Local”. However, this is neither true
for all metrics, nor for all threats.

Lastly, a team of certiﬁed external experts, taking as inputs
the status of the organization used for the posture assessment,
in order to ﬁnd the maturity, complexity and attractiveness in-
dexes, has provided estimates for the likelihood of occurrence
of cyber incidents deriving from the nine considered threats.
This expert-based approach is used in many modern cyber-
risk assessment methods, such as HTMA, FAIR, MAGERIT,
CRAMM, and many others.

The results obtained through MAGIC and the aforemen-
tioned approaches are compared in Table IX, where we denote
the likelihoods resulting from the newly proposed method, the
CVSS-based method and the expert estimates as LC, LCVSS
and LEXPERT, respectively.

14

TABLE VIII
LOW LEVEL METRICS FOR LIKELIHOOD ASSESSMENT USING CVSS.

Metric

CVSS

Access Vector (AV)

Base

Authentication (Au)

Base

Access Complexity (AC)

Base

Exploitability (E)

Temporal

Report Conﬁdence (RC)

Temporal

Metric evaluation
Local
Adjacent
Network
Multiple
Single
None
High
Medium
Low
Unproven
Proof of concept
Functional
High
Not deﬁned
Unconﬁrmed
Uncorroborated
Conﬁrmed
Not deﬁned

Numerical value
0.4
0.6
1
0.5
0.55
1
0.5
0.75
1
0.85
0.9
0.95
1
1
0.9
0.9
1
1

TABLE IX
LIKELIHOOD OF OCCURRENCE OF THE CONSIDERED THREATS, OBTAINED USING DIFFERENT METHODS.

ID
1
2
3
4
5
6
7
8
9

Threat
Malware
Web-based attacks
Denial of services
Malicious insiders
Phishing and social engineering
Malicious code
Stolen devices
Ransomware
Botnets

LC
0.86
0.61
0.92
0.97
0.92
0.52
0.78
0.72
0.86

LCVSS
0.90
0.64
0.95
0.15
0.90
0.81
0.09
0.99
0.86

LEXPERT
0.80
0.75
0.90
0.55
0.95
0.70
0.45
0.85
0.80

We notice that,

in several cases,

the three considered
methods provide comparable results. However, in some cases,
the numerical methods (MAGIC and the CVSS-based) give
contrasting results. We argue this is due to the incapability of
the CVSS-based method to catch that different organizations
may react differently to the same threats, if the state of their
organizational infrastructure is dissimilar. In fact, the CVSS-
based method considers only marginally and indirectly the
degree of compliance of the organization to the best practices
for threat prevention, whereas MAGIC takes it into account
in the preliminary posture assessment. In contrast, on the one
hand, the external experts evaluation seems very balanced in
most situations but, on the other hand, as already asserted, it
is subjective, in that a different team of experts might return
signiﬁcantly different results. Moreover, the involvement of
external experts is costly, it is time consuming, and requires
a signiﬁcant amount of data to be processed, which may not
be available.

B. Qualitative comparison

In this section we provide a qualitative comparison of
several cyber risk assessment and/or management methods.
The considered approaches, the evaluated metrics and the
results of the assessment are shown in Table X.

In short, we notice that among the most relevant existing
risk assessment/management methods, most of them require
the subjective estimates of calibrated or external experts.

like our method, but

Clearly, the extensive use of external experts (described in
Section VII-A) restricts the range of the organizations that
can afford it to those in the medium and large scale, making
the associated methods barely applicable to small organiza-
tions. Some methods, such as MEHARI or MAGERIT, rely
they implement
on internal experts,
only qualitative approaches. Therefore, they do not provide
a numerical estimate of the likelihood of occurrence of cyber
incidents, leading to results which might need an involved
interpretation and may not be easily reproducible. Even in
the CVSS-based methods, where the likelihood is numerically
estimated based on internal experts analyses, the process is
still subjective, since there is no automatic and universal
way to assign scores to the threats exploiting the considered
vulnerabilities. Moreover, none of the considered methods
directly relate the likelihood of occurrence of cyber incidents
to the posture of the organization. MAGIC tries to overcome
all these shortcomings, by providing a numerical approach for
the likelihood assessment, relying on the simple initial task
of some internal experts compiling questionnaires about the
status of the target organization.

VIII. CONCLUSION

Most existing cyber risk assessment methods are affected
by subjectivity or require a high level of expertise from the
organizations, which makes them barely practical in real-case
scenarios. We have tackled this issue by proposing a novel

TABLE X
QUALITATIVE COMPARISON BETWEEN THE CONSIDERED METHODS. LEGEND: RM=RISK MANAGEMENT; RA=RISK ASSESSMENT; LA=LIKELIHOOD
ASSESSMENT.

15

Method
ISO/IEC 27005:2018
NIST SP 800-30
EBIOS
CRAMM
OCTAVE
MEHARI
MAGERIT
IRAM2
IT-Grundschutz
CORAS
HTMA
FAIR
LiSRA
Fuzzy-logic based
CVSS-based
MAGIC
* Can be extended to other types of organizations.

Likelihood Assessment
Guidelines only
Guidelines only
Qualitative
Qualitative
Qualitative
Qualitative
Numerical
Qualitative
Qualitative
Qualitative or numerical
Numerical
Numerical
Numerical
Numerical
Numerical
Numerical

Scope
RM
RA
RM
RA
RM
RM
RM
RM
RM
RA
RA
RA
RA
LA
LA
LA

probabilistic method, called MAGIC that, based on the orga-
nization posture, estimates the likelihood (or the frequency)
of occurrence of a cyber incident or, more generally, of a list
of cyber incidents. Through numerical simulations we have
shown how MAGIC can be plugged into statistical cyber risk
assessment methods, like HTMA and FAIR, to eventually
assess the risk as the likelihood of occurrence of cyber
incidents combined with their impact. We have performed
both qualitative and quantitative comparisons with alternative
approaches, showing that our method is reliable and general,
in the sense that it can provide inputs to other risk assessment
and/or risk management methods. Moreover, MAGIC tries
to catch all the advantages of existing likelihood assessment
methods. In conclusion, due to its simplicity and rather low
computational demand, we argue that MAGIC can be applied
to any type of organization (small, medium, or large), without
any loss of generality.

ACKNOWLEDGMENT

The authors are very grateful to Giovanni Libertini and to
the team of cybersecurity analysts of Ancharia S.r.l. for their
precious insights.

Assessors
External experts
External experts
Team of different stakeholders
External experts
Internal experts
Internal experts
Calibrated experts
Internal experts
Internal and external experts
Internal and external experts
Calibrated experts
Calibrated experts
External experts
External experts
Internal experts
Internal experts

Target Organizations
All
Governmental*
All
Large scale*
All
All
Public Administration*
All
All
Medium and large scale
Medium and large scale
Medium and large scale
All
Medium and large scale
All
All

[7] D. W. Hubbard and R. Seiersen, How to Measure Anything in Cyberse-

curity Risk.

John Wiley & Sons Inc, 2016.

[8] J. Freund and J. Jones, Measuring and Managing Information Risk: A

FAIR Approach. Butterworth-Heinemann, 2014.

[9] P. Santini, G. Gottardi, M. Baldi, and F. Chiaraluce, “A data-driven
approach to cyber risk assessment,” Security and Communication Net-
works, vol. 2019, Article ID 6716918, 2019.

[10] National Cybersecurity Agency of France (ANSSI), “EBIOS Risk Man-
ager,” https://www.ssi.gouv.fr/en/guide/ebios-risk-manager-the-method.
[11] Z. Yazar, “A qualitative risk analysis and management tool - CRAMM,”

SANS InfoSec Reading Room White Paper, vol. 11, pp. 12–32, 2002.
J.

[12] C. Alberts, A. Dorofee,

Stevens,

and

to

“Introduction
the
https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=51546 ,
Carnegie-Mellon Univ Pittsburgh Pa Software Engineering Inst, Tech.
Rep., 2003.

OCTAVE

C. Woody,
Approach,”

[13] CLUSIF (Club de la S´ecurit´e de l’Information Franc¸ais), “MEHARI
(MEthod for Harmonized Analysis of RIsk).” [Online]. Available:
https://www.enisa.europa.eu/topics/threat-risk-management/risk-management/current-risk/risk-management-inventory/rm-ra-methods/m mehari.html

[14] Ministerio

de Hacienda

y Administraciones Publicas

Ministry of Finance
version
–
Risk Analysis
https://administracionelectronica.gob.es/pae Home/dam/jcr:80b16a91-75b1-432d-ab23-844a12aab5fc/MAGERIT v 3 book 1 method PDF NIPO 630-14-162-0.pdf.

and Public Administrations),
for
and Management,Book

Information
I

3.0 Methodology

-

(Spanish
“MAGERIT
Systems
The Method,”

[15] Information

Security
Assessment Methodology
https://www.securityforum.org/solutions-and-insights/information-risk-assessmentmethodology-iram2/

Risk
[Online]. Available:

“Information

(IRAM2).”

Forum,

2

f¨ur

[16] Bundesamt
(BSI),
based
https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/Grundschutz/International/bsi-standard-2002 en pdf.html

Informationstechnik
analysis
Available:

Sicherheit
Standard
IT-Grundschutz.”

in
100-3

“BSI
on

[Online].

Risk

der

-

REFERENCES

[1] G. Rafaiani, M. Battaglioni, M. Baldi, F. Chiaraluce, G. Libertini,
L. Spalazzi, and G. Cancellieri, “A functional approach to cyber risk
assessment,” in Proc. AEIT 2021 International Annual Conference,
Second Virtual Edition, Oct. 2021.

[2] National Institute of Standards and Technology (NIST), “Special pub-
lication 800-30 revision 1 - information security: Guide for conducting
risk assessments,” Sep. 2012.

[3] International Organization for Standardization (ISO), “ISO 31000:2018

- risk management - guidelines,” Feb. 2018.

[4] ——, “ISO/IEC 27005:2018 information technology - security tech-

niques - information security risk management,” Jul. 2019.

[5] P. Shamala, R. Ahmad, and M. Yusoff, “A conceptual framework of info
structure for information security risk assessment (ISRA),” Journal of
Information Security and Applications, vol. 18, no. 1, pp. 45–52, 2013,
sETOP’2012 and FPS’2012 Special Issue.

[6] M. Stamatelatos, “Probabilistic risk assessment: What is it and why is it
worth performing it?” NASA Ofﬁce of Safety and Mission Assurance,
Tech. Rep., 2000.

[17] SourceForge, “The CORAS Method,” http://coras.sourceforge.net/.
[18] O. Giuca, T. Popescu, A. Popescu, G. Prostean, and D. Popescu,
“A survey of cybersecurity risk management frameworks,” in Soft
Computing Applications. SOFA 2018. Advances in Intelligent Systems
and Computing, V. Balas, L. Jain, M. Balas, and S. Shahbazova, Eds.
Springer, Cham, 2021, vol. 1221.

[19] D. Gritzalis, G. Iseppi, A. Mylonas, and V. Stavrou, “Exiting the risk
assessment maze: A meta-survey,” ACM Comput. Surv., vol. 51, no. 1,
pp. 1–30, Jan. 2019.

[20] M. Khosravi-Farmad and A. Ghaemi-Bafghi, “Bayesian decision
network-based security risk management framework,” Journal of Net-
work and Systems Management, vol. 28, no. 4, pp. 1794–1819, Oct.
2020.

[21] C. Schmitz and S. Pape, “LiSRA: Lightweight security risk assessment
for decision support in information security,” Computers & Security, vol.
90, 101656, Mar. 2020.

[22] A. P. Henriques de Gusm˜ao, L. Camara e Silva, M. Mendonc¸a Silva,
T. Poleto, and A. P. Cabral Seixas Costa, “Information security risk
analysis model using fuzzy decision theory,” International Journal of
Information Management, vol. 36, no. 1, pp. 25–34, Feb. 2016.

16

[23] A. Handa, S. Mukhopadhyay, S. Mallick, N. Kumar, S. Shukla, R. Minz,
S. Nagarmat, and R. Rakesh, “Cyber risk assessment of networked
cyber assets using probabilistic model checking,” in Proc. 2019 IEEE
Conference on Information and Communication Technology, Allahabad,
India, Dec. 2019.

[24] M. Aksu, M. Dilek, E. Tatli, K. Bicakci, H. Dirik, M. Demirezen, and
T. Aykir, “A quantitative CVSS-based cyber security risk assessment
methodology for IT systems,” in Proc. 2017 International Carnahan
Conference on Security Technology (ICCST), Madrid, Spain, Oct. 2017.
[25] C. Alberts, A. Dorofee, J. Stevens, and C. Woody, “OCTAVE-S im-
plementation guide, Version 1.0,” Manuel ´electronique. Pittsburg, PA,:
Software Engineering Institute, Carbegie Mellon university, 2005.
[26] R. Caralli, J. Stevens, L. Young, and W. Wilson, “Introducing OCTAVE
Allegro: Improving the Information Security Risk Assessment Process,”
http://resources.sei.cmu.edu/library/asset-view.cfm?AssetID=8419, Soft-
ware Engineering Institute, Carnegie Mellon University, Pittsburgh, PA,
Tech. Rep. CMU SEI-2007-TR-012, 2007.

the

common

[27] P. Mell, K. Scarfone, and S. Romanosky, “A complete guide
2.0,”

to
https://tsapps.nist.gov/publication/get pdf.cfm?pub id=51198, 2007.
[28] National Institute of Standards and Technology (NIST), “Special pub-
lication 800-53a revision 5 - assessing security and privacy controls in
information systems and organizations,” Jan. 2022.

system version

vulnerability

scoring

[29] Center of Internet Security (CIS), “CIS Controls v.8,” May 2021.
[30] European Union Agency For Network and Information Security
(ENISA), “Handbook on security of personal data processing,” Dec.
2017.

[31] National Institute of Standards and Technology (NIST), “Framework for
improving critical infrastructure cybersecurity - version 1.1,” Apr. 2018.
[32] Federal Financial Institutions Examination Council (FFIEC), “Cyberse-

curity assessment tool,” May 2017.

[33] Associazione Italiana per la Sicurezza Informatica, “Rapporto Clusit

2021 sulla sicurezza ICT in Italia,” Oct. 2021, in Italian.

[34] D. Fekedulegn, M. M. Siurtain, and J. Colbert, “Parameter estimation
of nonlinear growth models in forestry,” Silva Fennica, vol. 33, no. 4,
pp. 327–336, Dec. 1999.

[35] P. Verboon and G. J. Ygram Peters, “Applying the generalized logis-
tic model in single case designs: Modeling treatment-induced shifts,”
Behavior Modiﬁcation, vol. 44, no. 1, pp. 27–48, Jan. 2020.

[36] Ponemon Institute LLC-Accenture, “Cost of cyber crime study, Techni-

cal Report,” 2017.

