Asymptotic Security by Model-based Incident Handlers
for Markov Decision Processes

Hampei Sasahara and Henrik Sandberg

1
2
0
2

r
a

M
4
2

]

Y
S
.
s
s
e
e
[

1
v
1
2
1
3
1
.
3
0
1
2
:
v
i
X
r
a

Abstract— This study investigates general model-based in-
cident handler’s asymptotic behaviors in time against cyber
attacks to control systems. The attacker’s and the defender’s
dynamic decision making is modeled as an equilibrium of a dy-
namic signaling game. It is shown that the defender’s belief on
existence of an attacker converges over time for any attacker’s
strategy provided that the stochastic dynamics of the control
system is known to the defender. This fact implies that the
rational behavior of the attacker converges to a harmless action
as long as the defender possesses an effective counteraction.
The obtained result supports the powerful protection capability
achieved by model-based defense mechanisms.

I. INTRODUCTION

Secure control system design is an urgent matter as
illustrated by several fatal incidents in critical infrastructures
that occurred in the last decade [1]–[4]. Risk assessment
is necessary as one of the fundamental steps to build se-
cure control systems. Speciﬁcally, it is required to evaluate
multiple factors, such as possibility of vulnerability, impacts
of potential threats, and implementation cost for appropriate
countermeasures, in a quantitative manner [5]. In particular,
counteractions carried out during adverse events are referred
to as incident handling [6]. For incident handling, which
is divided into multiple steps including attack detection,
inﬂuence reduction, and vulnerability elimination, we need
to perform highly complicated decision making. To handle
the complexity, automatic incident handlers that utilize the
dynamical model of the system to be defended have been
proposed [7], [8].

This study investigates behaviors of general model-based
incident handlers with perfect model knowledge for risk as-
sessment of control systems. Mostly, a model-based incident
handler passively monitors the control system’s behavior,
and proactively carries out a proper reaction by estimating
a reasonable attack scenario if the system’s behavior is
inconsistent with its model. However, even if the perfect
model knowledge is available, it is impossible to choose the
appropriate reaction instantaneously owing to randomness of
the environment, such as disturbance and noise. Thus, the
possibility of transient deception caused by the randomness
is unavoidable to the defender. For the sake of generality,
we conﬁne our attention to model-based incident handler’s
asymptotic behaviors in time.

This work was supported by Swedish Research Council.
H. Sasahara and H. Sandberg are with Division of Decision and
Control Systems, School of Electrical Engineering and Computer Sci-
ence, KTH Royal Institute of Technology, Stockholm SE-100 44, Sweden
{hampei,hsan}@kth.se

Our main interest

is to examine whether model-based
incident handlers can be deceived not only transiently but
also permanently. In other words, we derive a condition under
which the defender achieves appropriate reaction in a ﬁnite
time step. As a speciﬁc scenario, we suppose a powerful at-
tacker who possesses perfect knowledge of the system model,
the defender’s decision making rule, and the input-output
data. For description of the system’s behavior, we model
the attacker’s and defender’s decision making as reasonable
strategies of a dynamic signaling game. Using the model,
we analyze the action proﬁles taken with the reasonable
strategies and the induced trajectories of defender’s belief
on the existence of the attacker.

Technically, we show that model-based incident handlers
can guarantee asymptotic security as long as the defender
possesses an effective counteraction. First, it is shown that
the defender’s belief on the existence of the attacker con-
verges without oscillation in a stochastic sense for any
attacker’s strategy. Moreover, except for the case where the
defender forms a ﬁrm belief, the control system’s behavior
must be consistent with the one under nominal operation
for convergence of the belief. This observation implies that
attacks cannot be injected after a sufﬁcient period of time
has elapsed, without being detected. In this sense, the control
system is guaranteed to be secure in an asymptotic manner.
A number of studies have addressed security of control
systems (see, e.g., [9] and references therein). In particular,
for risk assessment, speciﬁc incident handling schemes, such
as attack detection [7], [10], resilient state estimation [11],
and attack containment [12], have been treated and analyzed.
Mostly, the systems have been assumed to belong to a limited
class, typically linear time-invariant systems. On the other
hand, this work considers general systems and discusses a
universal property of model-based incident handlers. This
generality is achieved by focusing only on asymptotic be-
haviors. With respect to information system security, sig-
naling games are often used for representing strategic and
adversarial decision making [13]–[16]. However, those works
consider only one-step games, i.e., transient decision making
and its inﬂuence is analyzed. In contrast, this paper addresses
asymptotic analysis of the signaling game. Finally, this study
is a generalized version of the preliminary work [17].

This paper is organized as follows. In Sec. II, the system’s
behavior with a model-based incident handler under the
supposed attack scenario is modeled as a dynamic signaling
game. In Sec. III, it is shown that the defender’s belief of
the existence of an attacker converges overt time. This fact
derives an asymptotic security of control systems when the

 
 
 
 
 
 
attacker prefers to conceal her existence. Sec. IV veriﬁes
the result
through a numerical example and discusses a
protection scheme based on the obtained result. Finally,
Sec. V draws the conclusion.

Notation

Let N, Z+, and R be the sets of natural numbers, non-
negative integers, and real numbers, respectively. The k-ary
Cartesian power of the set X is denoted by X k. The ﬁltered
probability space considered in this paper is denoted by
(Ω, F, Pr; {Fk}k∈N). The σ-algebra generated by a random
variable X is denoted by σ(X). The expected value of a
real-valued random variable X is denoted by E[X]. The
conditional expected value of X given a σ-algebra G is
denoted by E[X|G]. For a sequence of events {Ek}∞
k=1 ⊂
N =1 ∪∞
F, the supremum set ∩∞
k=N Ek, namely, the event
where Ek occurs inﬁnitely often, is denoted by {Ek i.o.}.
Appendix II contains the proofs.

II. MODELING USING DYNAMIC SIGNALING GAMES

A. Motivating Example

This subsection provides a motivating example. We here
treat water distribution networks (WDNs), which supply
drinking water of suitable quality to customers. Because
of their indispensability to our life, WDNs are attractive
targets for adversaries [18]. In particular, we consider the
water tank system illustrated by Fig. 1, where a tank is
connected to a reservoir within a WDN. The amount of
the water in the tank varies due to usage for drinking and
ﬂow between the external network. Thus the tank system
is required to be properly controlled through actuation of
the pump and the valve to keep the water amount within a
desired range [19]. A programmable logic controller (PLC)
transmits on/off control signals to the pump and the valve
monitoring the state, namely, the water level of the tank.
The dynamics are modeled as a Markov decision process,
where the state space and the action space are given by
quantized water levels and ﬁnite control actions. Interaction
to the external network is modeled as a randomness in the
process.

We here suppose an attack scenario considered in [20].
The adversary succeeds to hijack the PLC and can directly
manipulate its control logic. Such an intrusion can be carried
out by stealthy and evasive maneuvers in advanced persistent
threats [21]. The objective of the attack is to damage the
system by causing water overﬂow through inappropriate con-
trol signals without being detected. To deal with this attack,
we suppose that a model-based incident handler, which can
monitor only the state, is installed with the water tank. The
model-based incident handler chooses a proper reaction by
detecting if the system is under attack through observation
of the state. If the system’s behavior is highly suspicious, for
example, the incident handler suggests an aggressive reaction
such as log analysis or dispatch of operators.

The key notion to analyze the system’s resilience is belief,
namely, conﬁdence on the existence of an attacker. If the
attacker executes an attack, then the system’s behavior is

Fig. 1. Motivating example: a water tank system connected to a reservoir
within a water distribution network. The programmable logic controller
(PLC) transmits on/off control signals to the pump and the valve monitoring
the state, namely, the water level of the tank. In the supposed scenario, an
adversarial software possibly intrudes into the PLC and then the infected
PLC tries to cause overﬂow by sending inappropriate control signals without
being detected. A model-based incident handler, which can monitor only the
state, is also installed to deal with the attack.

(a) Behavior of the belief on existence of an attacker. The belief is
increased under attacks while it is expected to be decreased without
attacks.

(b) Behavior of the belief when the model-based incident handler is per-
manently deceived. The belief may oscillate through intermittent attacks.

Fig. 2. Possible behaviors of the belief on existence of an attacker.

different from the one under the nominal operation and
accordingly the belief should be increased. Conversely, if the
attacker stays calm by choosing proper control signals, it is
expected that the belief is decreased as depicted by Fig. 2a.
Our main interest in this study is to investigate whether
the model-based incident handler is permanently deceived,
i.e., the possibility of sophisticated attacks that may cause
oscillation of the belief as illustrated by Fig. 2b.

B. System Description

Let us now introduce the general system description.
Consider a control system, possibly under attack, as depicted
in Fig. 3. There is an agent, called a sender, who can alter
the behavior of the system through an action ak ∈ A for
k ∈ N. The sender can be an attacker when an adversary has
intruded in the control system. The output of the system at
the kth step is denoted by xk ∈ X . Based on the measured
output, the other party, called a receiver, chooses an action

Fig. 3. Control system and defense architecture.

rk ∈ R at each time step. We henceforth refer to rk as
a reaction for emphasizing that rk denotes a counteraction
against potentially malicious attacks. The dynamics of the
system is described by the map

Σk : Ω × Ak × Rk → X

for k ∈ N where the effect of the initial condition is
disregarded for simplicity on the premise that the initial
state is publicly known. Note that, although the dynamics
in the motivating example is independent of the reaction, the
control system is assumed to be dependent for generality.

For simplicity, we assume that the sets of signals and
actions, namely, X , A, R, are ﬁnite sets. Moreover, Σk
is assumed to be a time-homogeneous Markov decision
process. The transition probability from x to x(cid:48) with a and
r is denoted by p(x(cid:48)|x, a, r). The following assumption is
made to guarantee variation of the control system’s behavior
for different actions.

Assumption 1 For any x ∈ X and r ∈ R, there exists x(cid:48) ∈
X such that p(x(cid:48)|x, a, r) (cid:54)= p(x(cid:48)|x, a(cid:48), r) for different actions
a (cid:54)= a(cid:48).

Assumption 1 eliminates the possibility of stealthy attacks
such as covert attack [22] and zero-dynamics attack [23],
[24].

Next, we formulate the decision making as a dynamic
signaling game. Let θ ∈ Θ denote the type of the sender.
For simplicity, the type is assumed to be binary, i.e., Θ =
{θb, θm}, where θb and θm correspond to benign and ma-
licious senders, respectively. The types θb and θm describe
the situations where there does not and do exist an adversary
in the control system, respectively. This binary assumption
implies that we focus on a single threat scenario. For
handling multiple scenarios, it sufﬁces to consider multiple-
valued types. Let {ss
k}k∈N denote the sender’s
and receiver’s strategy proﬁles, respectively. The strategies
at the kth step are given by

k}k∈N and {sr

k → A,

k : Θ × I s
ss
k and ir

k ∈ I r

k : I r
sr

k → R

k are information sets at the kth

where is
k ∈ I s
step given by

is
k = (x1:k−1, a1:k−1),

ir
k = (x1:k−1, r1:k−1).

These information sets imply measurability of the state and
perfect recall of the agents’ decisions. We hereinafter denote
the strategy proﬁle of each player by ss := {ss
k}k∈N, sr :=
k}k∈N and the pair of them by s := (ss, sr). The sender’s
{sr

and receiver’s admissible strategy sets are denoted by S s and
S r, respectively.

In preparation for the game-theoretic formulation in the

sequel,
X θ,s
Aθ,s

k (ω) := Σk(ω, Aθ,s
k(θ, I s,θ,s
k (ω) := ss

k

1:k(ω), Rθ,s

1:k(ω)),

(ω)), Rθ,s

k (ω) := sr

k(I r,θ,s

k

(ω))

(1)

where

I s,θ,s
k
I r,θ,s
k

(ω) := (X θ,s
(ω) := (X θ,s

1:k−1(ω), ss
1:k−1(ω), sr

1:k−1(θ, I s,θ,s
1:k−1(I r,θ,s

1:k−1(ω))).

1:k−1(ω))),

We denote the conditional probability mass function of X θ,s
k+1
given x1:k by pθ,s

k+1(xk+1|x1:k).

This setup describes the situation where the incident han-
dler does not know whether the control system is attacked,
or not. This game is thus categorized into the class of
incomplete information games, and in particular, signaling
games because the type of a player is unknown to the
opponent.

C. Signaling Game Setup

To deﬁne reasonable strategies, we introduce belief sys-
tems, for the receiver, on the sender’s type. A belief system is
a tuple of the functions πk : Θ ×X k → [0, 1]. As the belief is
close to one, the receiver believes that the sender is malicious
with high conﬁdence. When the following conditions are
satisﬁed, the belief system π := {πk}k∈Z+ is said to be
consistent with the strategy proﬁle s:

• The initial belief satisﬁes π0(ˆθ) ≥ 0 for any ˆθ ∈ Θ and

(cid:80)

ˆθ∈Θ π0(ˆθ) = 1.

• For any k ∈ Z+ and x1:k+1 ∈ X k+1 that satisfy the
k+1(xk+1|x1:k)πk(φ; x1:k) (cid:54)= 0, the

condition (cid:80)
transition follows Bayes’ rule determined by s:

φ∈Θ pφ,s

πk+1(ˆθ; x1:k+1) = f s

k+1(ˆθ, x1:k+1)πk(ˆθ; x1:k)

where

ˆθ,s
k+1(xk+1|x1:k)

k+1(ˆθ, x1:k+1) :=
f s

p
φ∈Θ pφ,s
k+1(xk+1|x1:k)πk(φ; x1:k)
The initial belief π0 is assumed to be known to both players.
Note that ˆθ represents not the true type but an estimated type
by the receiver in the notation.

(cid:80)

.

As a decision rule for rational players’ strategies, we
consider uniform equilibria. Let the sender’s instantaneous
utility be given by U s : Θ × X × A × R → R. The sender’s
expected average utility up to the T th step is given by

¯U s
T (θ, s) := E

(cid:34)

1
T

T
(cid:88)

k=1

U s(θ, X θ,s

k , Aθ,s

(cid:35)
k , Rθ,s
k )

.

Similarly, with the receiver’s instantaneous utility given by
U r : X × A × R → R, the receiver’s expected average utility
up to the T th step is given by

¯U r
T (s, π) :=

T
(cid:88)

(cid:88)

E



1
T

k=1

ˆθ∈Θ

U r(ˆθ, X

ˆθ,s
k , A

ˆθ,s
k , R

ˆθ,s
k )πk(ˆθ; X

ˆθ,s
1:k)



.

Under this notation, the strategy proﬁle s = (ss, sr) is said to
be a Bayesian-Nash equilibrium if ( ¯U s
T (s, π)) con-
verges to ( ¯U s(θ, s), ¯U r(s, π)) as T → ∞ with a consistent
belief system π and



ss ∈ arg max

T (θ, s), ¯U r

¯U s(θ, (ˆss, sr)) ∀θ ∈ Θ,
¯U r((ss, ˆsr), π)

sr ∈ arg max

ˆss∈S s



ˆsr∈S r

is satisﬁed. Note that the results in this paper can be extended
to the case when the utility is not taken to be uniform, such
as discounted utilities.

The subsequent section analyzes properties of reasonable
strategies on the premise that an equilibrium exists although
its existence is a fundamental issue to be investigated. It is
known that mixed strategies admit existence of equilibria
in most cases. For example, repeated games with com-
plete information always have a Nash equilibrium in mixed
strategies [25, Chap. 8]. Although we consider only pure
strategies in this paper, an extension to mixed strategies is
straightforward.

III. ANALYSIS
In this section, we analyze asymptotic behaviors of beliefs
and actions for detection-averse strategies. It is shown that
the control system is guaranteed to be secure in an asymp-
totic manner as long as the defender possesses an effective
counteraction. Throughout this section, we ﬁx a strategy
proﬁle given as an equilibrium and omit s in the notation
for simplicity.

A. Belief’s Asymptotic Behavior

First, we investigate asymptotic behaviors of beliefs. Sup-
pose that a true type θ, a strategy proﬁle s and a belief system
π are given as an equilibrium. Then the value of the belief
about ˆθ at the kth step for each outcome ω ∈ Ω is given by

π

ˆθ,θ
k (ω) := πk(ˆθ; X θ

1:k(ω)).
We denote the belief on the true type by π ˆθ=θ
in the following discussion.

k

(ω) := πθ,θ

k (ω)

First of all, the following key lemma holds.

Lemma 1 For any type θ, strategy proﬁle s, and consistent
belief system π,
is a
submartingale with respect to the ﬁlteration σ(X θ

the belief on the true type π ˆθ=θ
k
1:k).

Lemma 1 implies that the belief on the true type is non-
decreasing in a stochastic sense. As a direct conclusion of
this lemma, obtained by the Doob’s convergence theorem,
the following theorem holds.

Theorem 1 For any type θ, strategy proﬁle s, and consistent
belief system π, the belief on the true type π ˆθ=θ
converges
almost surely as k → ∞.

k

Theorem 1 implies that the belief does not oscillate even
under an intermittent attack as in Fig. 2b. We denote the
limit by π ˆθ=θ

∞ : Ω → [0, 1] where
ˆθ=θ
∞

ˆθ=θ
k → π

π

a.s.

as k → ∞.

Remark: A heuristic justiﬁcation of Theorem 1 from an
information-theoretic perspective can be given as follows.
Suppose that the true type is θm and the state sequence x1:k
is observed. Then the belief is given by

π

ˆθ=θm
k

=

=

π0(θm)
(pθb(x1:k)/pθm (x1:k))π0(θb) + π0(θm)

π0(θm)
exp(kSk)π0(θb) + π0(θm)

(2)

where pθ(x1:k) is the joint probability mass function of x1:k
and

Sk :=

1
k

k
(cid:88)

i=1

log

pθb(xi|x1:i−1)
pθm (xi|x1:i−1)

.

Assuming that pθ
k(xk|x1:k−1) approaches a stationary distri-
bution pθ and the strong law of large numbers (SLLN) can
be applied, for sufﬁciently large k we have

Sk (cid:39) Ex∼pθm

(cid:2)log pθb(x)/pθm (x)(cid:3) = −DKL(pθm ||pθb)

ˆθ=θm
k

where DKL denotes the Kullback-Leibler divergence. Since
DKL is nonnegative for any pair of distributions, Sk con-
verges to a nonnegative number, which results in convergence
of π
. Bayesian estimator’s convergence to the true
parameter, referred to as Bayesian consistency, has been
investigated mainly in the context of statistics [26]. In this
sense, Theorem 1 can be regarded as another representation
of Bayesian consistency in the context of security. However,
note again that this discussion is not a rigorous proof but a
heuristic explanation since the state is essentially non-i.i.d.
(independent and identically distributed) and applicability of
SLLN cannot be ensured.

B. Deﬁnition of Detection-averse Utilities

To clarify our interest, we deﬁne the notion of detection-

averse utilities.

Deﬁnition 1 (Detection-averse Utilities) A pair (U s, U r) is
said to be detection-averse utilities when

π

ˆθ=θm
∞ < 1

a.s.

(3)

for any Bayesian-Nash equilibrium s and consistent belief
system π.

Deﬁnition 1 characterizes utilities with which the malicious
sender avoids having the defender form a ﬁrm belief on
the existence of an attacker. In other words, the reasonable
strategy becomes detection-averse when the defender pos-
sesses an effective counteraction. If the utilities of interest
are not detection-averse,
there is no trade-off from the
attacker’s perspective. For protecting such systems, design of
appropriate counteractions should be performed as a premise
of the presented framework.

Examples of effective counteractions include fallback con-
trol [27] and separation-based reconﬁguration [12]. Suppose
that the control system to be protected is networked and con-
nected to the Internet through, for example, human machine

interface. Those proposed methods detect an unauthorized
access and exclude the attacker by disconnecting the attacked
components. When the attacker prefers to lurk without being
detected and keep the unauthorized access, this situation can
be modeled with detection-averse utilities.

C. Asymptotic Security

As a preparation of our main claim, we investigate the
asymptotic behavior of state transition. By dividing the cases
with respect to the limit of the belief, we obtain the following
lemma.

Lemma 2 For any type θ, strategy proﬁle s, and consistent
belief system π, we have

Pr(Eθ

f→1 ∪ Eθ

π→0) = 1

where Eθ
converges to one and Eθ
converges to zero, i.e.,

f→1 is the event where the coefﬁcient of Bayes’ rule
π→0 is the event where the belief

f→1 := {ω ∈ Ω : f ˆθ=θ
Eθ
π→0 := {ω ∈ Ω : π ˆθ=θ
Eθ

(ω) → 1},
k
∞ (ω) = 0}

with

ˆθ=θ
k

f

(ω) := fk(θ, X θ

1:k(ω)),

which is the coefﬁcient in Bayes’ rule for the true type.

Lemma 2 implies that there are only two cases: one is that
the belief update gradually stops and the other is that the
belief on the true type converges to zero.

the attacker’s type. Therefore, we can interpret Lemma 3 as
the fact that the state has to lose information on the type
asymptotically.

From Lemma 3 and Assumption 1, the actions themselves
must be identical. This fact yields the main result of this
study: asymptotic security is achieved by model-based inci-
dent handlers.

Theorem 2 Let Assumptions 1 and 2 hold. Every Bayesian-
Nash equilibrium with detection-averse utilities satisﬁes

d(A

ˆθb,θm
k

ˆθm,θm
, A
k

) → 0

a.s.

ˆθb,θm
k

(ω) := ss

with A
(ω) :=
k(θm, I s,θm (ω)) where d : A × A → [0, ∞) is a distance
ss
given by

k(θb, I s,θm (ω)) and A

ˆθm,θm
k

d(a, a(cid:48)) =

(cid:26) 0
1

if a = a(cid:48),
otherwise,

which induces the discrete topology.

Theorem 2 implies that the malicious sender’s action con-
verges to the benign one. Equivalently, an attacker necessar-
ily behaves as a benign sender after a sufﬁciently large step.
Therefore, the control system is guaranteed to be secure in
an asymptotic manner, i.e., model-based incident handlers
are never deceived permanently. This result indicates the
powerful defense capability achieved by model knowledge.

IV. NUMERICAL EXAMPLE AND DISCUSSION

We here need a technical assumption to eliminate the latter

A. Numerical Example

case.

Assumption 2 For any type θ, strategy proﬁle s, and con-
sistent belief system π, Pr(Eθ

π→0) = 0 holds.

Assumption 2 guarantees that the belief on the true type
does not converge to zero. A control system that satisﬁes
Assumption 2 is provided in Appendix I.

Under Assumption 2, Lemma 2 implies that the coefﬁcient
of Bayes’ rule converges to one almost surely. This claim is
equivalent to that the state eventually loses information on
the type.

Lemma 3 Let Assumption 2 hold. Every Bayesian-Nash
equilibrium with detection-averse utilities satisﬁes

|p

ˆθb,θm
k

ˆθm,θm
− p
k

| → 0 a.s.

where

ˆθb,θm
p
k
ˆθm,θm
p
k

(ω) := pθb
(ω) := pθm

1:k−1(ω)),
1:k−1(ω)).

k (ω)|X θm
k (ω)|X θm

k (X θm
k (X θm
For interpretation of Lemma 3, consider the ideal case where
ˆθm,θm
ˆθb,θm
holds at some time step k. This condition
= p
p
k
k
means that
the transition of the state’s probability mass
function is identical regardless of the estimated type. In
other words, the state does not possess information about

We conﬁrm the theoretical results through numerical sim-
ulation. We assume the state space and the action space
i.e., X = {xn, xa} and A = {ab, am}.
to be binary,
The states xn and xa represent the normal and abnormal
states, respectively, and ab and am represent benign and
malicious actions, respectively. The benign and malicious
actions correspond to proper and improper control signals,
respectively. The reaction set is given by R = {rb, rm}.
As in the motivating example, we assume that the transition
probability is independent of the reaction. The state transition
diagram is depicted by Fig. 4, where the transition probability
from xn to xa with ab is denoted by pb
an, and the other
transition probabilities are denoted in a similar manner. The
inequalities in Fig. 4 mean that the malicious action leads to
a higher probability of the abnormal state than the benign
action. The speciﬁc values of the transition probabilities
are given in Table I, where each value corresponds to the
probability from the state in the row to the state in the
column. The utilities are given in Table II, which implies
that the benign sender always prefers the normal state, the
receiver always prefers the reaction corresponding to the true
type, the malicious sender prefers non-aggressive reaction,
and also the abnormal state for non-aggressive reaction. The
initial state is xn. The initial belief is given by π0(θm) = 0.1.
Since it is difﬁcult to compute an exact equilibrium for
the inﬁnite time horizon problem, we consider a sequence

Fig. 4. Example of state transition diagram with binary state and action
spaces.

TABLE I
TRANSITION PROBABILITIES. LEFT: PROBABILITIES WITH THE BENIGN
ACTION. RIGHT: PROBABILITIES WITH THE MALICIOUS ACTION.

ab
xn
xa

xn
0.9
0.8

xa
0.1
0.2

am
xn
xa

xn
0.8
0.7

xa
0.2
0.3

of equilibria for a ﬁnite time horizon problem. Deﬁne the
ﬁnite time horizon average utilities by

¯U s
k,T := E

(cid:34)

1
T

k+T −1
(cid:88)

i=k

U s(θ, X θ,s

i

, Aθ,s
i

, Rθ,s
i

)

.

(cid:35)

and

¯U r
k,T (s, π) :=

k+T −1
(cid:88)

E



1
T

(cid:88)

U r(ˆθ, X

ˆθ,s
i

ˆθ,s
, A
i

ˆθ,s
, R
i

)πi(ˆθ; X

ˆθ,s
1:i )



.

i=k

ˆθ∈Θ
With those utilities, the obtained ss
k are used for the
kth strategy, in a manner similar to receding horizon control.
The horizon length is given by T = 2.

k and sr

Under this setting, a sample path of the state, the action,
and the belief on θm for θ = θm is depicted in Fig. 5. The
vertical lines in the graph of belief means the action at the
time instant is am. For 0 ≤ k ≤ 12, the belief is sufﬁciently
small, and thus am is the rational action. For 13 ≤ k < 21,
the belief is large, and hence ab is taken when the state is
xn. At k = 21, the state is xa, and the belief begins to
decrease. At k = 26, the state is xn. Then the belief exceeds
the threshold, and ab regardless of the state is the rational
action. This result coincides with Theorem 2. Note that, the
reaction is always rb when the belief is less than 0.5, and
hence the sender’s instantaneous utility depends only on the
state and the receiver’s utility is one in this example.

To investigate a long-term behavior, consider a situation
where detection is more difﬁcult. Speciﬁcally, the transition
probability is given in Table III, which means the deviation
of the transition probability by am is small. A sample path
of the state, the action, and the belief on θm for θ = θm is
depicted in Fig. 6. Although the convergence speed is later
than Fig. 5, the asymptotic security claim in Theorem 2 can
be conﬁrmed.

B. Discussion: Protection by Passive Blufﬁng

Roughly speaking, the result in Section III claims that
the defender always wins in an asymptotic manner when
the stochastic model of the control system is completely

TABLE II
UTILITIES

U s

0(θb)
xn
xa

rb
1
0

rm
1
0

U s

0(θm)
xn
xa

rb
1
2

rm
0
0

U r
0
θb
θm

rb
1
0

rm
0
1

Fig. 5. Sample paths of the state, the action, and the belief when θ = θm.

known and the vulnerability is known and modeled. The
latter requirement is quantitatively described by the condition
π0(θm) > 0. Although the derived result claims a quite
powerful defense capability, it is also true that it is almost
impossible to be aware of all possible vulnerabilities in
advance and to prepare appropriate counteraction for all
scenarios.

As a practically interesting defense scheme, it may be
possible to use the obtained property for passive blufﬁng.
Suppose that the attacker does not know whether her attack
scenario is supposed (π0(θm) > 0) or not (π0(θm) = 0).
Also, imposing a certain property into the control system, we
assume that state observation does not provide information
about the reaction. For instance, the control system in the
numerical example, where the behavior is independent of the
reaction, satisﬁes this property. Under those assumptions, if
the defender can conceal the actually conducted reactions,
the true belief is completely unknown to the attacker. In this
case, even if the attack is actually a zero-day attack through
an unknown vulnerability (π0(θm) = 0), there is a possibility
to be able to protect the control system. Speciﬁcally, if the
attacker is risk-averse, i.e., she cares about the case π0(θm) >
0, then she would possibly stop the attack after a while in a
rational manner although the attack is unnoticed. Analysis
of such passive blufﬁng utilizing the powerful detection
capability achieved by model-based incident handling is a
possible future direction.

V. CONCLUSION

This study has investigated behaviors of model-based
incident handlers using the framework of dynamic signaling
games. It has been shown that the control system can be
guaranteed to be secure in an asymptotic manner when the
defender possesses an effective counteraction. Future work

stateaction0102030405000.5beliefTABLE III
TRANSITION PROBABILITIES. LEFT: PROBABILITIES WITH THE BENIGN

ACTION. RIGHT: PROBABILITIES WITH THE MALICIOUS ACTION.

pb
xn
xa

xn
0.9
0.8

xa
0.1
0.2

pm
xn
xa

xn
0.85
0.79

xa
0.15
0.21

Fig. 6. Sample paths of the state, the action, and the belief when θ = θm
and the transition probability is given by Table III.

includes generalization of the results and a formal analysis
of passive blufﬁng discussed in Sec. IV-B.

APPENDIX I
EXAMPLE ENSURING ASSUMPTION 2

This appendix provides a simple example of a system
that ensures the condition of Assumption 2. Consider a
binary state space and assume all transition probabilities
are uniformly set to 1/2 at the equilibrium when θ = θm.
Assume also that the transition probabilities from one state
to the other are p (cid:54)= 1/2 when θ = θb. Deﬁne E2k as
the event that the number of reaching one state is equal to
the number of reaching the other state at the time step 2k.
From the random walk theory, E2k occurs inﬁnitely often
almost surely. If ω ∈ E2k, then the belief at the 2kth step
ˆθ,θ
2k (ω) = π0(θm)/(α(1 − π0(θm)) + π0(θm))
is given by π
with α = pk(1 − p)k4k. Because 0 < α < 1, we have
π ˆθ=θ
2k (ω) > π0(θm). Since E2k occurs inﬁnitely often almost
surely, the condition of Assumption 2 holds. It is expected
that a similar justiﬁcation can be applied to a broader class
of systems.

APPENDIX II
PROOFS

for ˆθ = θ. Thus it sufﬁces to show (4) for any k ∈ N and
x1:k ∈ X k.

First, we reduce the index of the summation in (4). When
πk(ˆθ; x1:k) = 0, the inequality (4) always holds. Thus it is
assumed that πk(ˆθ; x1:k) > 0 in the following. Deﬁne

X 0

k :=






xk+1 ∈ X :

(cid:88)

p

ˆθ∈Θ

ˆθ,s
k+1(xk+1|x1:k)πk(ˆθ; x1:k) = 0






.

Because πk(ˆθ; x1:k) is positive, if xk+1 belongs to X 0
ˆθ,s
k+1(xk+1|x1:k) = 0 holds. Hence (4) is equivalent to
p

k then

(cid:88)

xk+1∈X +
k

k+1(xk+1|x1:k)πk+1(ˆθ; x1:k+1) ≥ πk(ˆθ; x1:k)
pθ,s

(5)

where X +

k := X \ X 0
k .
For notational simplicity, we deﬁne
π(ˆθ) := πk(ˆθ; x1:k), pθ(x) := pθ,s(x|x1:k), X + := X +
k

for ﬁxed k and x1:k. Under this notation, since π is consistent
with s, the inequality (5) is equivalent to

(cid:88)

pθ(x)

x∈X +

pˆθ(x)π(ˆθ)
φ∈Θ pφ(x)π(φ)

(cid:80)

≥ π(ˆθ).

Because ˆθ = θ and π(θ) > 0, this inequality is equivalent to

(cid:88)

pθ(x)

x∈X +
(cid:124)

(cid:80)

pθ(x)
φ∈Θ pφ(x)π(φ)
(cid:123)(cid:122)
(cid:125)
=:G(θ)

≥ 1.

(6)

By rewriting the left-hand side and applying Jensen’s in-
equality, we have

G(θ) =

(cid:88)

x∈X +

pθ(x)
π(θ) + pθ(cid:48)(x)/pθ(x)π(θ(cid:48))

1

≥

≥

x∈X + pθ(x)pθ(cid:48)(x)/pθ(x)π(θ(cid:48))

π(θ) + (cid:80)
1
π(θ) + π(θ(cid:48))

= 1,

which leads to the claim.

Proof of Theorem 1: Because the belief is uniformly
ˆθ=θ,s
bounded, we have supk∈N E
π
< ∞. From Lemma 1
k
and Doob’s convergence theorem [28, Theorem 4.1], the
claim holds.

(cid:104)

(cid:105)

Proof of Lemma 1: Since it is clear that the belief is
adapted to the ﬁlteration and integrable, it sufﬁces to show

Proof of Lemma 2: Assume π ˆθ=θ,s

∞ (ω) = α ∈ (0, 1].

Then we have

E

(cid:104)

(cid:105)
ˆθ=θ,s
k+1 |σ(X θ,s
1:k)

π

≥ π

ˆθ=θ,s
k

a.s.

lim
k→∞

f θ,s
k (ω) = lim
k→∞

π

ˆθ=θ,s
k+1 (ω)/π

ˆθ=θ,s
k

(ω) = α/α = 1,

for the claim. Fix ω ∈ Ω and denote X θ,s
the inequality is equivalent to

1:k(ω) by x1:k. Then

(cid:88)

xk+1∈X

k+1(xk+1|x1:k)πk+1(ˆθ; x1:k+1) ≥ πk(ˆθ; x1:k) (4)
pθ,s

which leads to the claim.

f θm,s
k
and the denominator of f θm,s

Proof of Lemma 3: From Lemma 2 and Assumption 2,
converges to one almost surely. Denote the numerator
D,k . Since

by f θm,s
N,k

and f θm,s

k

stateaction050010001500200000.5beliefD,k |f θm,s

k − 1| ≤ |f θm,s

D,k ≤ 1, we have 0 ≤ f θm,s

k − 1|.
D,k is bounded,
− 1| → 0 almost surely. This leads to

0 < f θm,s
Because |f θm,s
we have f θm,s
ˆθb,θm,s
that |p
) → 0 almost surely.
k
Since s is a Bayesian-Nash equilibrium with detection-averse
utilities, the claim holds.

k −1| → 0 almost surely and f θm,s
D,k |f θm,s
k
− p

ˆθm,θm,s
k

ˆθ=θm,
k

|(1 − π

Proof of Theorem 2: First, note that

the claim is

equivalent to

ˆθb,θm,s
k

Pr({A
(cid:124)

ˆθm,θm,s
k

(cid:54)= A
(cid:123)(cid:122)
=:E

) = 0
i.o.}
(cid:125)

from the ﬁniteness of A.

Because the Markov decision process is ﬁnite, Lemma 3

is equivalent to

Pr({p
(cid:124)

ˆθb,θm,s
k

(cid:54)= p
(cid:123)(cid:122)
=:Fk

ˆθm,θm,s
k

(cid:125)

i.o.}) = 0.

This is equivalent to Pr(F ) = 0 where

(cid:40)

F :=

ω ∈ Ω :

∞
(cid:88)

k=1

Pr(Fk|σ(X θm,s

1:k−1))(ω) = ∞

(cid:41)

from the generalized second Borel-Cantelli lemma [29, The-
orem 4.3.4]. Now assume Pr(E) (cid:54)= 0. From Pr(E ∩ F ) = 0
and Pr(E ∩ F ) = Pr(F |E)Pr(E) = 0, we have Pr(F |E) =
0. We here show Pr(F |E) (cid:54)= 0 and prove the claim by
contradiction.

Take ω ∈ E. Then there exists a subsequence {ki}i∈N,
holds

(cid:54)= A

ˆθm,θm,s
ki(ω)

ˆθb,θm,s
ki(ω)

which depends on ω, such that A
for any i ∈ N. For this subsequence,
∞
(cid:88)

∞
(cid:88)

Pr(Fk|σ(X θm,s

1:k−1))(ω) ≥

Pr(Fki(ω)|σ(X θm,s

1:ki−1(ω)))(ω)

i=1
k=1
holds. From the ﬁniteness of the Markov decision process
and Assumption 1, we have

inf
i∈N

Pr(Fki(ω)|σ(X θm,s

1:ki−1(ω)))(ω) > 0.

Thus (cid:80)∞
Therefore Pr(F |E) = 1, which leads to a contradiction.

1:k−1))(ω) = ∞ holds for ω ∈ E.

k=1 Pr(Fk|σ(X θm,s

REFERENCES

[1] N. Falliere, L. O. Murchu, and E. Chien, “W32. Stuxnet Dossier,”

Symantec, Tech. Rep., 2011.

[2] Cybersecurity & Infrastructure Security Agency, “Stuxnet malware
mitigation,” Tech. Rep. ICSA-10-238-01B, 2014, [Online]. Available:
https://www.us-cert.gov/ics/advisories/ICSA-10-238-01B.

[3] ——,

“Cyber-attack against Ukrainian critical
Tech. Rep.
IR-ALERT-H-16-056-01, 2018,
https://www.us-cert.gov/ics/alerts/IR-ALERT-H-16-056-01.

infrastructure,”
[Online]. Available:

[4] ——, “HatMan - safety system targeted malware,” Tech. Rep.
https://www.us-

MAR-17-352-01,
cert.gov/ics/MAR-17-352-01-HatMan-Safety-System-Targeted-
Malware-Update-B.

[Online]. Available:

2017,

[5] National Institute of Standards and Technology, “Guide for conducting

risk assessments,” Tech. Rep. SP 800-30 Rev. 1, 2012.

[6] P. Cichonski, T. Millar, T. Grance, and K. Scarfone, “Computer
security incident handling guide,” National Institute of Standards and
Technology, Tech. Rep. SP 800-61 Rev. 2, 2012, [Online]. Available:
https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-
61r2.pdf.

[7] J. Giraldo et al., “A survey of physics-based attack detection in cyber-

physical systems,” ACM Comput. Surv., vol. 51, no. 4, 2018.

[8] L. Xiao, X. Wan, X. Lu, Y. Zhang, and D. Wu, “IoT security
techniques based on machine learning,” IEEE Signal Process. Mag.,
vol. 35, no. 5, pp. 41–49, 2018.

[9] S. M. Dibaji, M. Pirani, D. B. Flamholz, A. M. Annaswamy, K. H.
Johansson, and A. Chakrabortty, “A systems and control perspective
of CPS security,” Annual Reviews in Control, vol. 47, pp. 394–411,
2019.

[10] A. J. Gallo, M. S. Turan, F. Boem, T. Parisini, and G. Ferrari-Trecate,
“A distributed cyber-Attack detection scheme with application to DC
microgrids,” IEEE Trans. Autom. Control, vol. 65, no. 9, pp. 3800–
3815, 2020.

[11] M. Pajic, I. Lee, and G. J. Pappas, “Attack-resilient state estimation
for noisy dynamical systems,” IEEE Trans. Control Netw. Syst., vol. 4,
no. 1, pp. 82–92, 2017.
Sasahara, T.

Sandberg,
J.
“Disconnection-aware attack detection and isolation with separation-
based
[Online]. Available:
https://arxiv.org/abs/2009.11205.

reconﬁguration,”

and H.

Ishizaki,

[12] H.

detector

Imura,

2020,

[13] T. E. Carroll and D. Grosu, “A game theoretic investigation of de-
ception in network security,” Security and Communication Networks,
vol. 4, no. 10, pp. 1162–1172, 2011.

[14] F. Farokhi, A. M. H. Teixeira, and C. Langbort, “Estimation with
strategic sensors,” IEEE Trans. Autom. Control, vol. 62, no. 2, pp.
724–739, 2017.

[15] J. Pawlick, E. Colbert, and Q. Zhu, “Modeling and analysis of leaky
deception using signaling games with evidence,” IEEE Trans. Inf.
Forensics Security, vol. 14, no. 7, pp. 1871–1886, July 2019.
[16] Q. Zhu and Z. Xu, Secure Estimation of CPS with a Digital Twin.

Springer, 2020, pp. 115–138.

[17] H. Sasahara, S. Sarıtas¸, and H. Sandberg, “Asymptotic security of
control systems by covert reaction: Repeated signaling game with
undisclosed belief,” in Proc. 59th IEEE Conference on Decision and
Control, 2020.

[18] A. Rasekh, A. Hassanzadeh, S. Mulchandani, S. Modi, and M. K.
Banks, “Smart water networks and cyber security,” Journal of Water
Resources Planning and Management, vol. 142, no. 7, 2016.

[19] E. Creaco, A. Campisano, N. Fontana, G. Marini, P. R. Page, and
T. Walski, “Real time control of water distribution newtorks: A state-
of-the-art review,” Water Research, vol. 161, pp. 517–530, 2019.
[20] R. Taormina, S. Galelli, N. O. Tippenhauer, E. Salomons, and A. Os-
tfeld, “Characterizing cyber-physical attacks on water distribution
systems,” Journal of Water Resources Planning and Management, vol.
143, no. 5, 2017.

[21] P. Chen, L. Desmet, and C. Huygens, “A study on advanced persistent
threats,” in Proc. International Conference on Communications and
Multimedia Security, 2014, pp. 63–72.

[22] R. S. Smith, “Covert misappropriation of networked control systems:
Presenting a feedback structure,” IEEE Control Systems Magazine,
vol. 35, no. 1, pp. 82–92, Feb. 2015.

[23] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson, “Revealing
stealthy attacks in control systems,” in Proc. 50th Annual Allerton
Conference on Communication, Control, and Computing, 2012, pp.
1806–1813.

[24] F. Pasqualetti, F. D¨orﬂer, and F. Bullo, “Control-theoretic methods for
cyberphysical security: Geometric principles for optimal cross-layer
resilient control systems,” IEEE Control Systems Magazine, vol. 35,
no. 1, pp. 110–127, 2015.

[25] R. Laraki, J. Renault, and S. Sorin, Mathematical Foundations of Game

Theory, ser. Universitext. Springer, 2019.

[26] P. Diaconis and D. Freedman, “On the consistency of bayes estima-

tion,” Annals of Statistics, vol. 14, no. 1, pp. 1–26, 1986.

[27] T. Sasaki, K. Sawada, S. Shin, and S. Hosokawa, “Model based
fallback control for networked control system via switched Lyapunov
function,” in Proc. 41st Annual Conference of the IEEE Industrial
Electronics Society, 2015, pp. 2000–2005.

[28] E. C¸ inlar, Probability and Statistics, ser. Graduate Texts in Mathemat-

ics. Springer, 2011.

[29] R. Durrett, Probability: Theory and Examples, ser. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University
Press, 2019.

