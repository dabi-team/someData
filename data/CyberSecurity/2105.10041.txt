DATA CURATION AND QUALITY ASSURANCE FOR MACHINE
LEARNING-BASED CYBER INTRUSION DETECTION

1
2
0
2

y
a
M
0
2

]

R
C
.
s
c
[

1
v
1
4
0
0
1
.
5
0
1
2
:
v
i
X
r
a

Haihua Chen1, Ngan Tran1, Anand Sagar Thumati1, Jay Bhuyan2, and Junhua Ding∗1

1Department of Information Science, University of North Texas, Denton, TX, 76203, USA
2Department of Computer Science, Tuskegee University, Tuskegee, AL, 36088, USA

Abstract

Intrusion detection is an essential task in the cyber threat environment. Machine learning and deep
learning techniques have been applied for intrusion detection. However, most of the existing research
focuses on the model work but ignores the fact that poor data quality has a direct impact on the
performance of a machine learning system. More attention should be paid to the data work when
building a machine learning-based intrusion detection system. This article ﬁrst summarizes existing
machine learning-based intrusion detection systems and the datasets used for building these systems.
Then the data preparation workﬂow and quality requirements for intrusion detection are discussed. To
ﬁgure out how data and models affect the machine learning performance, we conducted experiments
on 11 HIDS datasets using seven machine learning models and three deep learning models. The
experimental results show that BERT and GPT were the best algorithms for HIDS on all of the
datasets. However, the performance on different datasets vary, indicating the differences between the
data quality of these datasets. We then evaluate the data quality of the 11 datasets based on quality
dimensions proposed in this paper to determine the best characteristics that a HIDS dataset should
possess in order to yield the best possible result. This research initiates a data quality perspective
for researchers and practitioners to improve the performance of machine learning-based intrusion
detection.

Keywords Data curation · Data quality · Machine learning · Intrusion detection · Host-based intrusion detection
systems

1

Introduction

The increasing usage of digital devices in a cyber-physical system (CPS) has enhanced the efﬁciency of operating
systems, but has also led to vulnerability from cyber-attacks. Cyber assaults on process control and the monitoring
of these intelligent systems could lead to a signiﬁcant control failure [1] and huge economic losses. This makes
cyber-security a major concern due to the high level of attacks on networks and systems for CPS [2]. Therefore, building
an intrusion detection system (IDS) to predict and respond to assaults on a CPS, has become an essential task among
the software engineering community. However, it is very challenging because of the range of novelty involved in
cyber-attacks [1]. Recently, machine learning and deep learning have been applied for intrusion detection at both the
operation system level (a host-based intrusion detection system, called HIDS) and the network level (a network-based
intrusion detection system, called NIDS). As indicated by Google Research [3], both the models/algorithms and the
quality of the data greatly impact the performance of machine learning systems. The computing rule of “garbage in,
garbage out” is still applicable to machine learning [4] and the lack of high quality training data becomes a barrier for
building high performance machine learning systems [5]. Therefore, we should not only optimize the models but also
systematically evaluate and ensure the data quality to improve performance for intrusion detection.

However, current studies on machine learning-based intrusion detection only focus on model construction and opti-
mization. For example, Sahu et al. compared the performance of various linear and non-linear classiﬁers for NIDS
using the KDD dataset [6], while Al-Maksousy compared deep neural networks (DNN) and various traditional machine
learning (ML) models for NIDS on the same dataset, ﬁnding that DNN outperformed ML models in terms of accuracy,

∗Corresponding author. E-mail address: junhua.ding@unt.edu.

 
 
 
 
 
 
A PREPRINT - MAY 24, 2021

running time, and false positive rates [7]. Hu et al. proposed an incremental HMM training framework that incorporates
a simple data pre-processing method for identifying and removing similar sub-sequences of system calls for HIDS [8].
This training strategy has been widely applied since it can save on the training cost (especially on large data) without
noticeable degradation of intrusion detection performance [8]. Convolutional neural network (CNN) and recurrent
neural network (RNN) have also been used in HIDS [9, 2]. Recently, Liu and Lang conducted a comprehensive survey
on machine learning and deep learning methods for IDS [10]. Nevertheless, very few research studies have paid
attention to data requirements, data quality issues, and data quality assurance for IDS.

The input of machine learning-based intrusion detection is a collection of data instances, each of which is represented
by only one feature or a group of features. The features can be binary, categorical, or continuous. Data instances can be
related to each other, such as sequence data, spatial data, and graph data. Each instance can be labeled as normal class
or anomaly class, or is not labeled. The data requirements for different machine learning techniques vary. A training
dataset should be labeled with normal and anomaly classes for supervised learning. The model is trained on the labeled
data, then any unseen data is input to the trained model to determine to which class it belongs [11]. Semi-supervised
learning assumes labeled instances only to include either normal class or anomaly class, then the trained model is used
to identify anomalies in the test data [11]. Noisy labels, insufﬁcient labeled data, and class imbalance are the most
common data quality issues for supervised and semi-supervised learning-based IDS. Surprisingly, unsupervised learning
does not require labeled data [11]. However, unsupervised learning techniques usually assume that normal instances are
far more frequent than anomalies in the test data. Under this assumption, semi-supervised learning can be incorporated
with unsupervised learning by taking a sample of the unlabeled dataset as training data to improve the robustness of the
model. Other data quality issues, such as inconsistency, duplication, incompleteness, incomprehension, no variety, or
imprecise timestamps, might also exist in the data input of all the machine learning models.

Instead of optimizing the machine learning-based IDS from the model/algorithm perspective, this paper targets the
data-centric IDS and discusses how to systematically evaluate and ensure the data quality to improve performance for
intrusion detection. We will answer a few important questions regarding the data quality to the performance of machine
learning-based IDS:

• How should an appropriate dataset be prepared for intrusion detection in a speciﬁc scenario? What are the data
requirements? We investigate and compare the existing datasets used for different intrusion detection tasks to
answer these questions.

• How should data quality be evaluated? How is it possible to judge whether the data quality or the machine
learning model has a major effect on the intrusion detection performance? How does data quality affect the
intrusion detection performance? We conduct a case study on a HIDS, which aims to detect user anomalous
behaviors in an operating system based on system call sequences. Ten different machine learning and deep
learning techniques are implemented for the intrusion detection. A comprehensive analysis on the experiments
is then performed to answer the second question.

To the best of our knowledge, this is the ﬁrst study which explores intrusion detection from a data-centric rather than a
model-centric perspective. It will beneﬁt IDS researchers and practitioners with new insights on improving intrusion
detection performance by enhancing the data quality. The rest of the paper is structured as follows: Section 2 presents
the literature review related to machine learning-based IDS and data quality. Section 3 discusses the data preparation
and quality requirements for intrusion detection. Section 4 discusses the experiments, experimental results, analysis,
and data quality assessment. Section 5 concludes the paper and discusses future work. The code and datasets used in
this research are available on GitHub 2.

2 Related work

2.1 Machine learning-based intrusion detection systems

Intrusion detection aims to detect malicious activities or intrusions (break-ins, penetrations, and other forms of computer
abuse) in a computer-related system (operating system or network system) [11]. An intrusion acts differently than the
normal behavior of the system and, hence, the techniques used for anomaly detection can also be used for intrusion
detection.

Machine learning techniques used for intrusion detection can be divided into supervised learning and unsupervised
learning. Whether the labeling of data is sufﬁcient or not becomes the key criteria for selecting a machine learning
technique. However, the detection performance of unsupervised learning methods is usually inferior to those of

2https://github.com/anandsagarthumati9848/HIDS

2

A PREPRINT - MAY 24, 2021

supervised learning methods [10]. Meanwhile, due to issues with how the data for intrusion detection typically ﬂows (in
a streaming fashion) and the data imbalance caused by low false alarm rates, the usage of machine learning techniques
in intrusion detection is more challenging than other anomaly detection applications. Table 1 summarizes the machine
learning algorithms used for HIDS and NIDS in the last ﬁve years.

Table 1: Machine learning techniques for IDS. Full names and abbreviations of the models are introduced in the
Appendix A.

Year
2017
2017
2018

2018
2018
2018

2019

2019
2020

2020

2020

2020

Reference
[12]
[13]
[14]

[15]
[16]
[17]

[18]

[19]
[20]

[21]

[22]

[23]

Technique used Model
Supervised
Supervised
Supervised

Supervised
Supervised
Supervised

KNN, SR
LR, RF
DNN, imbalanced network trafﬁc,
RF, VAE
DNN
DNN
LR, NB, KNN, DT, AdaBoost, RF,
CNN, CNN-LSTM, LSTM, GRU,
SimpleRNN, DNN

Datasets
KDD-Cup99
UNSW
CIDDS-001

HIDS/NIDS
NIDS
NIDS
NIDS

NIDS
NIDS
NIDS

NSL-KDD
NSL-KDD
CICIDS-
2017, UNSW-
NB15,
ICS
cyberattack

Unsupervised

Metric learning + clustering + SVM Kyoto 2006,

NIDS

Supervised
Supervised

Unsupervised

Unsupervised

NB, AODE, RBFN, MLP, J48 DT
Pruned exact linear time, quantile
regression forests
Autoencoder, IF, KNN, K-Means,
SCH, SVM
IF, HBOS, CBLOF, K-Means

Supervised

DNN

Supervised

Supervised
Supervised
Supervised

KNN, RF, SVM-rbf, DNN, ResNet-
50, one-vs-all classiﬁer, multiclass
classiﬁer
NB, DT, RF, ANN
SVM, MLP, NB, DT
CNN

Semi-Supervised

SC4ID

Supervised
Supervised

Supervised

Supervised

GRU, LSTM, CNN+GRU
LR, SVM, DT, RF, ANN

NN, DT, linear discriminate analysis
with the bagging algorithm
NB, LR, KNN, SVM, IntruDTree

NSL-KDD
UNSW-NB15 NIDS
NIDS
NetFlow data

NIDS

NIDS

NIDS

NSL-KDD,
ISCX
BRO DNS,
BRO CONN
KDD-Cup99,
NSL-KDD,
UNSW-NB15
NSL-KDD

NIDS

2020

[24]

KDD-Cup99
ADFA-LD
NGIDS-DS,
ADFA-LD
ADFA-LD,
UNM dataset
ADFA-LD
DS2OS trafﬁc
traces
NSL-KDD

NIDS
HIDS
HIDS

HIDS

HIDS
HIDS

HIDS

cy-

HIDS

Kaggle
bersecurity
datasets

2020
2017
2017

2018

2019
2019

2019

2020

[25]
[26]
[9]

[27]

[2]
[20]

[28]

[29]

As can be seen from the Table 1, most existing studies focus on NIDS. More datasets have been created for NIDS and
different machine learning algorithms have been explored. However, compared to NIDS, HIDS is more of a challenge
due to [30]: (1) More labeled data is required to reduce the false positive alarm rate. (2) It is difﬁcult to design an
efﬁcient HIDS which can prevent outgoing denial-of-service attacks. (3) In a shared system environment, the HIDS
needs to work as an independent module since the shared parameters may cause the attack. Nowadays, HIDS are
becoming more important and play a major role in most of the intrusion detection systems [30]. Even though some

3

A PREPRINT - MAY 24, 2021

studies have been conducted on HIDS [26, 9, 27, 2, 20, 29], more attention should be paid to HIDS. The applications of
SOTA techniques, such as combining powerful language models with deep learning, might be a promising direction.

2.2 Datasets for intrusion detection

As shown in Table 1, many datasets have been created for intrusion detection [31, 32]. Datasets for NIDS mainly
include information from the packet itself and aim at detecting the malicious activity in network trafﬁc using the content
of individual packets, while datasets for HIDS usually include information about events or system calls/logs on a
particular system with the purpose of detecting vulnerability exploits against a target application or computer system
[2]. We conducted an investigation into the popular datasets used for NIDS and HIDS, as shown in Table 2.

Table 2: Overview of public datasets for IDS. The detailed description of the datasets are presented in our GitHub
repository.

Dataset

Volume

KDD-Cup99 [33]

Kyoto 2006 [34]

4.9 millions for training,
2 million for testing
3,054,682 for
1,563,923 for testing

training,

DARPA-2009
[35]

NSL-KDD [36]

ISCX [37]

UNSW-NB15
[38]

NGIDS-DS [39]

for

training,

673,931 records for train-
ing and 74,880 records
for testing
125,973
22,544 for testing
30,814
and
normal
15,375 attack traces for
training, 13,154 normal
and 6,580 attack traces
for testing
175,341 records for train-
ing, 82,332 records for
testing
631,85,992 records for
training and 34,987,493
records for testing

CICIDS2017 [40]

75,561 records for train-
ing and 25,187 records
for testing

DARPA 98/99
[41]

4,898,431 records
for
training and 2,984,154
records for testing

UNM dataset [42]

se-
system-call
627
quences
training
for
and 3,136 system-call
sequences for testing

Format

Labeled Balanced Year

packet,
logs
packet,
logs

yes

yes

packets

yes

packet,
logs
packets

yes

yes

packets

yes

packet,
logs

yes

no

yes

no

no

no

no

no

1999

2006

2009

2009

2012

2015

2016

packets

yes

no

2017

packet,
logs

yes

no

1998

logs

yes

no

1998

Information
NIDS

41 features, 20 types of
attacks
14 features derived from
the KDD-Cup99 and 10
additional features
16 network features and
26 packet features; 7000
pcap ﬁles
41 features, 22 types of
attacks
1.5 million network traf-
ﬁc packets, with 20 fea-
tures and covered seven
days of network activity

49 features in pcap ﬁle
format and 9 categories
of attacks
88,791,734 records for
benign and 1,262,426
records for malicious ac-
tivities.
7 features for
ground-truth cs; 9 fea-
tures for the 99 csv ﬁles
of host logs; 18 features
for NGIDS.pcap
3,119,345 instances and
83 features containing 15
class labels

HIDS

normal

97,277
and
311,744 intrusion traces
with 41 features and
classes labeled as either
normal or any of the 22
types of attacks
4,298 normal traces and
1,001 intrusion traces and
467 features

4

KDD99 [33]

494,021 for training and
311,029 for testing

NSL-KDD [36]

ADFA-LD [43]

ADFA-WD [43]

DARPA-2009
[35]

ADFA-IDS [44]

NGIDS-DS [39]

DS2OS
traces [45]

trafﬁc

Kaggle cybersecu-
rity datasets [29]

traces

normal

distinct
from KDD99:
and

1,152,281
records
860,725
291,556 attack traces
833 traces and 308,077
system calls for train-
ing, 4,373 traces and
2,122,085 system calls
for testing
and
355
system
13,504,419
calls for training, 1,827
traces and 117,918,735
system calls for testing
673,931 records for train-
ing and 74,880 records
for testing
308,077 system calls and
833 traces for training,
212,2085 system calls
and 4,372 traces for test-
ing
313,926 records with
7 attributes in ground-
truth csv ﬁle;1,262,426
attack and 88,791,734
normal with 9 attributes;
1,094,231 capture pack-
ets with 18 unique IPs in
NGIDS.pcap ﬁle
61.52 MB

25,000 instances

A PREPRINT - MAY 24, 2021

1,033,372 normal and
4,176,086 attack traces;
41 features
41 features per record

packets

yes

packets

yes

no

no

1999

2000

746 trace
quences
system
sequences

attack se-
and 317,388
attack
call

5,542 trace attack se-
quences and 74,202,804
system call attack se-
quences

16 network features and
26 packet features; 7000
pcap ﬁles
15 attack types
and
36,636 malicious system
call traces

yes

no

2014

yes

no

2014

packets

yes

logs

yes

no

no

2009

2013

cyber normal and abnor-
mal trafﬁc scenarios for
different enterprises

packet,
host
logs

yes

no

2017

traces captured in the IoT
environment DS2OS for
different services
3 qualitative features and
38 quantitative features

–

–

yes

yes

yes

yes

2018

2020

Generally, the following properties are required when creating an IDS dataset: (1) Normal user behavior. The quality
of an IDS is primarily determined by its attack detection rate and false alarm rate. Therefore, the presence of normal
user behavior is indispensable for evaluating an IDS [31]. (2) Attack trafﬁc. The attack types in different scenarios
varies, so it is necessary to clarify the attacks in the IDS dataset. (3) Format. An IDS dataset can be in different formats
such as packet-based, ﬂow-based, host-based log ﬁles, etc. (4) Anonymity. Some of the information is anonymized
due to privacy concerns, and this property indicates which attributes will be affected. (5) Duration. The recording
time (e.g., daytime vs. night or weekday vs. weekend) of the dataset is indicated since a behavior might be regarded
as an attack only when it occurs in a speciﬁc duration. (6) Labeled. Labeled datasets are necessary for training
supervised learning and semi-supervised learning models and for evaluating supervised learning, semi-supervised
learning, and unsupervised learning models. (7) Other information, such as attack scenarios, network structure, IP
addresses, recording environment, download URL, are also useful. Quality issues can easily appear in the above
information. Those data quality issues, if not checked and eliminated appropriately, will greatly affect the intrusion
detection performance. However, few studies have discussed the qualities of IDS datasets [37, 39] for machine learning,
although data quality issues, such as duplication and imbalance, have been reported in the KDD dataset [36].

5

A PREPRINT - MAY 24, 2021

2.3 Data quality evaluation and assurance for machine learning

Poor data quality has a direct impact on the performance of the machine learning system that is built on the data. For
example, a face recognition-based gender classiﬁcation system that was implemented with machine learning algorithms
produced a 0.8% error rate when recognizing the faces of lighter-skinned males, but went as high as a 34.7% error rate
when recognizing the faces of darker-skinned females [46]. The problem was due to a signiﬁcant imbalance of the
training datasets in skin colors [46]. Recently, a study showed that ten of the most commonly-used computer vision,
natural language, and audio datasets had serious data quality issues [47]. The computing rule of “garbage in, garbage
out” is also applicable to machine learning-based anomaly detection. However, there is not much research on the
analysis of low quality training data and its impact on machine learning-based anomaly detection.

One of the fundamental issues might be the noisy labels. To investigate the impact of the quality of the labeling
process on the performance of the machine learning-based network intrusion detection, Lauría and Tayi compared two
classiﬁcation algorithms (DT and NB) which were trained on poor quality data [48]. The experiments showed that
data with totally clean labels may not be required to train a classiﬁer that performs at an acceptable level as a detector
of network intrusions [48]. Class imbalance is another common issue in IDS. As pointed out by Sahu et al., both of
the datasets CIDDS and KDD are imbalanced in classes, and the distribution of KDD is even less uniform: the two
most dominant classes both have more than 40,000 instances while the least dominant 16 classes have less than 1,000
instances [6]. Their experiments demonstrated that data balancing cannot improve its performance if the training dataset
is less uniform and data balancing will beneﬁt the neural network if improving the predictive accuracy of less dominant
classes is desired [6]. Oversampling and undersampling have been used to deal with the class imbalance in IDS [49].
Missing information, duplicate data, attack diversity, dataset difﬁculty, and feature sparsity can be other factors that
reduce the performance of machine learning-based IDS [50, 6]. Different dimensions have been proposed to measure
the data quality for machine learning systems. Fan argued that data consistency, data de-duplication, information
completeness, data currency, and data accuracy are central to data quality [51]. These dimensions are related to the
data itself. The dimensions related to users include accessibility, integrability, interpretability, rectiﬁability, relevance,
and timeliness [52]. Recently, Chen, Chen, and Ding deﬁned “data quality” as a measurement of data for ﬁtting the
purpose of building a machine learning system [5]. Dimensions, such as comprehensiveness, correctness, and variety,
are critical to evaluate the data quality for machine learning systems [5]. Gradient boosted decision tree, data ﬁltering,
SVM, and transfer learning have all been investigated for data quality assurance and improvement [53, 5, 54].

3 Data preparation and quality requirement for intrusion detection

As discussed above, data quality is crucial for machine learning-based IDS. However, each stage of the data preparation
can be plagued with problems of data quality, such as scattered presence of the data source, insufﬁcient data during
data collecting, label noise during the data annotation, and overlapping issues during the training-testing data splitting.
Therefore, it is necessary to identify the dataset attributes and potential, then develop a guideline for quality assurance
during data preparation.

3.1 Data preparation workﬂow

Data preparation for machine learning-based IDS mainly includes four steps: (1) Selecting a data source or multiple
data sources. (2) Collecting the data from the selected data source. (3) Labeling the data for training and testing. (4)
Preprocessing the data as the model input. The workﬂow is shown in Figure 1. The data sources for HIDS and NIDS
are different: data for HIDS can be collected from audit records, log ﬁles, the application program interface (API), rule
patterns, and system calls, while data for NIDS is usually collected from the simple network management protocol
(SNMP), network packets (TCP/UDP/ICMP), management information base (MIB), and router NetFlow records. Data
can be collected from one data source or by integrating multiple data sources. The data source is the foundation of
accessing high quality data. Once the data source is conﬁrmed, additional information should be collected, such as
metadata, format, duration, etc., for further analysis. The data collecting procedure should be well designed to ensure
the data quality. For example, when collecting sequential events, they should be correctly organized by their order. Data
labeling is an essential step for supervised machine learning-based IDS. Most existing machine learning algorithms
make the assumption that the training data feeding the algorithms is accurate (has no errors). However, errors in label
data entry, lack of precision in expert judgment, and imbalanced data distribution in different categories during the
process of labeling the training examples can impact the predictive accuracy of the classiﬁcation algorithms [48]. Data
preprocessing aims at removing the outliers, cleaning the data, extracting the useful features, and splitting the data for
training and test. This process, if handled inappropriately, will cause data quality issues, such as data sparsity and bias,
and overlapping between training and test, which can also reduce the performance of the machine learning algorithms.

6

A PREPRINT - MAY 24, 2021

Figure 1: Data preparation workﬂow for machine learning-based IDS

3.2 Dataset properties and attributes

As pointed out by [31], certain properties of a dataset should be collected and evaluated based on a speciﬁc scenario.
We believe that to build a machine learning-based IDS, the following information should be collected during the data
curation.

General information. General information of a dataset might include year of creation, public availability, metadata,
format, and data volume. Both network trafﬁc and system calls face the issue of concept drift over time, and new
attacks might appear in any scenario. A machine learning model built in 1990 might not be useful to predict the data in
2021. Therefore, the year of creation (age) of an intrusion detection dataset is critical for deciding the scope of an IDS.
Intrusion detection datasets should be publicly available to serve as a basis for comparing different intrusion detection
methods and for quality evaluation by third parties [31]. Metadata, such as network structure, hosts, IP addresses,
conﬁguration of the clients, and attack scenarios, can provide users content-related explanations of the results. IDS
datasets can be roughly divided into three formats: (1) packet, (2) ﬂow, and (3) logs. The processing for different data
formats varies. The format is directly associated with the volume of data. Data volume can be described by the number
of packets, ﬂows, points, and instances, which is crucial when selecting machine learning models.

Duration. Duration refers to the timestamps of when that data was collected. For example, the MIT dataset includes
live normal data for lpr for two weeks using 77 hosts, while the ISCX dataset consists of the seven days of network
activity (normal and malicious) from the Information Security Center of Excellence (ISCX) at the University of
New Brunswick. As mentioned in [55], in order to enable the evaluation of detection algorithms that consider the
cyclostationary evolution of trafﬁc (i.e., differences in trafﬁc between daytime/nighttime or weekdays/weekends), a
long duration trace is needed.

Context. Context is related to the recording environment. It delineates the network environment and conditions in
which the datasets are captured. As for NIDS, the kind of trafﬁc, type of network, and complete network indicate the
context information. HIDS is generally a software component and is located on the system being monitored, so is
typically monitoring a single system. The context information allows deeper understanding of processes and activities.

Normal traces and types of attacks for intrusion traces. This information is related to data labeling, which is the
foundation of machine learning-based IDS. The training/validate/testing data should be correctly labeled as normal or
not. In attack records, the type of attack is also needed [55]. Maciá-Fernández et al. applied the following strategy to
label an instance: a) an attack label for the ﬂows that they positively know correspond to an attack, b) a normal label for

7

A PREPRINT - MAY 24, 2021

those that are generated synthetically with normal patterns, and c) a background label for those which no one knows
exactly if they are attacks or not [55]. Ring et al. summarized the speciﬁc types of attacks used in different datasets
[31].

Features. The input of machine learning-based IDS is a collection of data instances, each of which is represented
by only one feature or a group of features. As shown in Table 2, most of the datasets include the feature information,
which can be qualitative features or quantitative features, network features or packet features, n-gram features, and
other features. Features are the key components for developing an effective IDS.

3.3 Dataset quality principles and dimensions

Different principles, metrics, and dimensions have been proposed to measure data quality [56, 57, 51, 58]. However,
few of them are discussed in the context of building machine learning systems [5]. As for machine learning-based IDS,
central to data quality are reputation, relevance, comprehensiveness, timeliness, variety, accuracy, consistency, and
de-duplication. We will discuss these dimensions in the following:

Reputation.
“Reputation” is related to reliability, believability, and trustworthiness, which lays the foundation of the
data quality. Reputation is evaluated using an information-theoretic concept, the Kullback-Leibler distance. Data for
IDS can be collected from different sources, including host logs, network trafﬁc, and application data. For a single data
source, we can use a collective measure of trustworthiness (in the sense of reliability) based on the referrals or ratings
from members in a community to evaluate the reputation [59]. PageRank is another method for reputation measurement
[60]. If we generate the dataset by data fusion of multiple data sources, an “opinion” (a metric of the degree of belief)
can be generated to represent the uncertainty in the aggregated result.

Relevance.
“Relevance” indicates why the data is collected. Data should be collected and evaluated by “ﬁt for
purpose” [5]. For example, HIDS data should be collected from the host system and audit sources, such as operating
system, window server logs, ﬁrewall logs, application system audits, or database logs. While NIDS data should be
extracted from a network through packet capture, NetFlow, and other network data sources. Moreover, if the IDS targets
a speciﬁc type of attack, such as DDoS attacks, then the data also needs to be relevant to this attack.

Comprehensiveness. Existing machine learning-based intrusion detection systems frequently suffer from the issues
of bias and lacking of robustness, which are mainly caused by the incomprehensiveness of the dataset. For example, the
dataset does not contain balanced data for different normal or attack behavior, and the data cannot represent various
features. Comprehensiveness requires a dataset to contain all representative samples from the population [5]. For
example, the NSL-KDD dataset includes a total of 39 attacks where each one of them is classiﬁed into one of the
following four categories: DoS, R2L, U2R, and probe. Suppose that all the attacks should have an instance in the
training set. However, 17 of these attacks are introduced only in the testing set. This dataset cannot then be considered
as comprehensive. The importance of the comprehensiveness of data to machine learning, especially deep learning, is
well understood since a deep learning model normally includes millions of parameters that needs a large amount of data
to train it. If a comprehensive dataset with as many different types of attacks included, similar to the ImageNet for
computer vision, can be developed for IDS, it would enhance the performance of machine learning-based IDS.

Timeliness.
“Timeliness” (also called “currency”) refers to the extent to which the age of the data [56] is appropriate
for the IDS task. Timeliness is an important factor to affect the performance of machine learning models since new
types of attacks are emerging constantly, and some existing datasets, such as DARPA and KDD99, are too old to reﬂect
these new attacks. Although the ADFA dataset contains many new attacks, it cannot be considered as comprehensive.
For that reason, testing of machine learning models for IDS using DARPA, KDD99, and ADFA datasets does not offer
a real evaluation and could result in inaccurate claims for their effectiveness [32]. Ideally, datasets should include most
of the common attacks and correspond to current network environments [10].

Variety.
“Variety” concerns the coverage of the instances on the selected features. For example, the KDD-Cup99
dataset has 41 features, and it is supposed to be a normal distribution in the selected features with known mean and
standard deviation in the real world. Otherwise, it will induce the data sparsity issue. Moreover, to improve the
robustness in machine learning models, the instances in the validate data and test data should have enough variety to test
the training model. Variety is considered as a subset of comprehensiveness in the scenario of constructing a machine
learning system for intrusion detection.

Accuracy. According to the deﬁnition from Wang and Strong [56], “accuracy” means “the extent to which data
are correct, reliable and certiﬁed.” Generally, accuracy can be distinguished by two aspects: syntactic and semantic

8

A PREPRINT - MAY 24, 2021

[10]. However, for machine learning systems, the labeling accuracy should also be taken into consideration. Syntactic
accuracy aims to check whether a value is any one of the values of, or how close it is to, the elements of the corresponding
deﬁned feature, while semantic accuracy requires an instance to be semantically represented appropriately. Labeling
accuracy means that an instance should be correctly labeled as normal or as any type of attack.

Consistency.
“Data consistency” refers to the validity and integrity of data representing real-world entities [51].
It aims to detect errors, such as inconsistencies and conﬂicts in the data, typically identiﬁed as violations of data
dependencies [51]. For example, the system call for HIDS should be represented to ensure the sequential order, and the
value of an attribute’s domain (feature) should be constrained to the range of admissible values.

De-duplication. Data de-duplication is the problem of identifying the same instances. It is a longstanding issue that
has been studied for decades. Duplication was usually caused by “data from a large number of (heterogeneous) data
sources was not fused appropriately.” Recently, Panigrahi and Borah reported that the CICIDS2017 dataset contains
many redundant records which seems to be irrelevant for training any IDS [61]. However, duplication is not necessarily
a data quality issue for machine learning-based IDS. For example, if one of the records (a data instance and its label)
appears multiple times in training data, it reﬂects the probability of this attack behavior. This will not be considered as
a duplication issue. However, having a large overlap between the training and the test data can potentially introduce
bias in the model and contribute to high inaccuracy, as pointed out by Chen et al. [5]. Transfer learning [5], the
distance-based approach [51], the rule-based approach [51], and probabilistic [51] can be used for data de-duplication.

In Section 4, we will conduct a case study of the datasets used in the experimental study using the data quality
dimensions discussed above. Based on the case study, we will discover how data quality affects the performance of
machine learning-based IDS, thereby understanding the strategies to assure data quality.

4 Experiment on machine learning-based intrusion detection

4.1 Data collection

4.1.1 Data cleaning and prepossessing

Since the goal of this case study is to develop a host-based intrusion detection system (HIDS), we conduct machine
learning experiments on the UNM, MIT, and ADFA-LD datasets. A detailed introduction of these datasets can be
found in our GitHub repository. Regardless of the slight difference in ADFA and UNM data formats, we use similar
data cleaning and augmentation techniques to create a dataset for each class. Processing data for pre-trained language
models vectors, such as BERT [62] and GPT-2 [63], is similar to normal machine learning algorithms. We use tokenizer
to parse data into system call sequences with a length of six. Since the UNM dataset contains system calls from
concurrently running processes, we group them by PID to ensure their sequential order. On the other hand, as the
ADFA-LD dataset is already organized by different processes, and there is no PID provided, we do not need to group
system calls together. Once the data is in order, we tokenize them into a sequence of six. By tokenizing into a sequence
of 6-grams, we increase the amount of data for training as well as testing purposes. In addition, the number of features
will decrease when a trace is tokenized into smaller chunks, and this will increase the efﬁciency of training as well as
testing performances [42].

We clean the data by removing rows or sequences that appear in both normal and intrusion data. This step draws
distinctive characteristics between the two classes and effectively boosts the machine learning performance. A row with
normal sequence is labeled 0, whereas the one with intrusion sequence is labeled 1. We use normal data and intrusion
data from each dataset to create a sample pool. If it is imbalanced, we use the bootstrapping method to create a balanced
sample of normal sequences and intrusion sequences. Then, we split the sample into training and testing sets in a 70-30
ratio. By training with only signature sequences from both classes, we increase the model accuracy and recall (true
positive rate, TPR) as well as decrease its false positive rate (FPR).

4.1.2 Descriptive analysis of the datasets

In order to compare the difference between the two classes, we graph an overlaid histogram of normal data and intrusion
data from each dataset in Figure 2. We use all of the UNM (except Sendmail), MIT Live Lpr, and ADFA-LD datasets.
Each dataset has two histogram versions. However, we only include the histograms of the processed and unprocessed
UNM Synthetic Sendmail dataset. The histograms of the rest of the datasets along with their descriptive analyses
are included in our GitHub repository. The ﬁrst histogram version displays the dispersion of original traces that have
not been cleaned nor processed yet, so it can show the actual difference between normal and intrusion sequences.
After cleaning duplicated sequences that exist in both classes, the overlaid histograms of most datasets have changed

9

A PREPRINT - MAY 24, 2021

signiﬁcantly. Therefore, the second version exhibits a more distinguished difference between the two classes than the
original data. The goal is to differentiate normal sequences from intrusion sequences as much as possible so that the
algorithms can learn to distinguish them from one another. Therefore, the more distinctive the two classes are, the better
the candidate algorithms can perform.

Figure 2a shows that there is a slight difference between normal data and intrusion data from the Synthetic Sendmail
dataset. However, most of them overlap each other. After cleaning duplicated sequences, the dispersion of normal
system calls and intrusion system calls have changed in Figure 2b. Normal system calls have expanded from two wide
ranges (68 to 83 and 101 to 118) to multiple speciﬁc ranges. Particularly, sequences that contain system call numbers
ranging between 13 and 15, 64 and 77, 100 and 102, 128 and 134, and 150 and 177 are most likely normal. This
increases the homogeneity of normal data and reduces false negatives. Additionally, after being processed, intrusion
system calls have expanded to more speciﬁc ranges. For example, sequences that contain system call numbers from 17
to 27, 77 to 84, and 102 to 128 are most likely intrusion. This narrows down possible intrusion likelihoods and reduces
false positives in overlapped system calls.

(a) The Original UNM Synthetic Sendmail Dataset

(b) The Processed UNM Synthetic Sendmail Dataset

Figure 2: Overlaid Histograms of the Original and the Processed System Calls in Normal and Intrusion Data of
the UNM Synthetic Sendmail Dataset.

4.2 Machine learning algorithms

K-means. K-means is a common algorithm for anomaly detection, which groups similar characteristics into a cluster.
We choose the number of clusters to be two because there are two categories in this dataset: normal (0) and intrusion
(1). The rest of the parameters of K-means are set as default from the scikit-learn library. K-means is trained with 9,800
sequences and tested with 4,200 sequences (roughly 7,000 normal and 7,000 intrusion sequences are split into training
and testing sets; the rest of the models use the same data for training and testing) from both classes in each dataset.

Logistic Regression. Logistic regression (LR) is another potential candidate for detecting anomaly using maximum
likelihood. Since this is a supervised model, we use sequences from the training set with labels to train the model. The

10

A PREPRINT - MAY 24, 2021

model is evaluated on the testing set, where the true labels are compared against the predicted ones to measure the
model performance. All parameters of this model are set as default in the scikit-learn library.

Support Vector Machine. Support vector machine (SVM) is a supervised machine learning algorithm which detects
anomaly by separating normal behavior from the other by using different kernel types (linear, polynomial, and RBF).
We choose a polynomial kernel with 3 degrees and 1.0 regularization to represent a SVM model.

Neural Network. Neural network (NN) is efﬁcient at learning underlying complex relationships because of its
composite architecture. The model is comprised of three layers: an input layer of six features, where each of them
represents a system call from a sequence of six; a ReLU layer with six hidden nodes containing extracted information
from each system call; and a sigmoid output layer with two output nodes, where each one represents a probability of a
sequence belonging to either class. Whichever class has a higher chance in the output node will be the predicted label
of the given input sequence.

Decision Tree. Decision tree (DT) relies on a set of rules, derived from the training process, to partition data into
groups that are as homogeneous as possible. The goal is to generate a DT to classify normal sequences from intrusion
sequences at the lowest possible error rate. That way, it can be generalized on the testing set as well as future unseen
data. For every split, the model considers, at most, a square root number of features and decides where to split using
GINI criterion with a minimum of ten samples per split and ﬁve samples per leaf. Each leaf node contains sequences,
where most of them belong to the same class.

Random Forest. Random forest (RF) distinguishes normal sequences from intrusion sequences using the ensemble
learning method. RF creates multiple DTs to classify the same observation. The ﬁnal decision is based on the wisdom
of the crowd that is the majority predicted class of that particular observation. Similarly, we set the same parameters as
the DT model above. The only difference is that the model allows bootstrap samples to build multiple trees.

K Nearest Neighbor. K nearest neighbor (KNN) classiﬁes an observation by counting the majority classes of the
nearest neighbors or sequences. This model enhances its performance by minimizing inter-variability within a group
while maximizing inter-variability between different groups. For simplicity, we choose the number of neighbors to be
three. Still, any odd number of k is recommended to avoid a tie situation. Additionally, the weights parameter is set to
be uniform, where all observations in each neighborhood are weighted equally regardless of their distances.

Naïve Bayes. Naïve Bayes (NB) is known for classifying data based on conditional probabilities without making any
assumptions. Given lots of labeled sequences, we hope that this model can efﬁciently distinguish intrusion sequences
from normal sequences. A predicted label is determined based on the highest probability of a class. The parameters of
this model are set as default from the scikit-learn library.

BERT. Bidirectional encoder representations from transformers (BERT) was developed by the Google AI language,
using transformer architecture with attention mechanisms for learning context. BERT is a bidirectional unsupervised
language model, which takes the texts before and after the token into contextual account. There are two versions of
BERT models based on structure size: BERT Base with 110 million parameters and BERT Large with 345 million
parameters [62]. In this research, the processed string in Section 4.1.1 is passed to a tokenizer and each string of system
call sequence is tokenized by the BERT tokenizer (bert-base-uncased). Each sequence is padded with [CLS] and [SEP]
at the beginning and the end of the sequence. The output of the BERT model is then passed to a linear network with 768
neurons and the output of the linear layer is passed to a single neuron with SoftMax activation. The model is trained
and tuned, using normal and intrusion sequences from each dataset, with 16 epochs and 16 batch size.

GPT-2. Generative pretrained transformers (GPT) was developed by Microsoft’s Open AI, using the decoder part of
the transformer architecture [63]. There are four types of GPT-2 depending on the model structure size, which can be
small – 117M parameters, medium – 345M parameters, large – 762M parameters, and extra large – 1542M parameters.
GPT-2 is auto-regressive which reuses its own output as an input sequence. We use small GPT-2 to train on processed
data from both classes of each dataset using 16 epochs and 16 batch size. Unlike BERT, GPT-2 does not require an
additional output layer; instead, it outputs the sequence’s likelihood in each class.

11

A PREPRINT - MAY 24, 2021

4.3 Experimental results

4.3.1 Overall results

Table 3 shows the performance of different machine learning algorithms on 11 datasets from UNM, MIT, and ADFA-LD
regarding performance indicator accuracy, recall, precision, macro-f1 score, false positive rate (meaning the percentage
of malicious behaviors that are labeled as benign behaviors, FPR), and AUC score [32]. Our goal is to ﬁnd the best
candidate model with high accuracy and high recall but low FPR. High performance measures are in bold so that we
can easily identify the best candidate model. We make the following observations:

• Almost all of the algorithms achieved a better performance on the Synthetic Lpr, Live Lpr, Xlock, Live Named,
Inetd, and Stide datasets than the others, indicating that the data quality of the former group of datasets might
be higher than the latter group.

• Decision tree, Random forest, KNN, BERT, and GPT-2 are the best candidate algorithms since they achieved

higher accuracy, recall, and precision, yet at a lower false positive rate on all datasets.

• BERT and GPT-2 are the best algorithms for HIDS on all of the datasets since they outperformed other

algorithms on almost all the metrics, and all the recalls are above 0.90 while the FPRs are below 0.06.

• Class imbalance is not a major issue for the HIDS datasets used in this paper since we use the bootstrapping
technique to generate a balanced sample of both classes. This sample is then split into training and testing sets
with a 70-30 ratio to train and test the models.

Table 3: Model performance regarding accuracy, recall, precision, macro-F1, FPR, and AUC score on different HIDS
datasets.

Dataset

Synthetic
Sendmail

Synthetic
Ftp

Synthetic
Lpr

Live Lpr

Model
K-means
Logistic Regression
SVM
Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM
Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM
Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM
Neural Network

Accuracy Recall
0.63
0.63
0.73
0.66
0.98
0.99
1.00
0.63
1.00
1.00
0.20
0.74
0.79
0.80
0.99
0.99
0.99
0.77
1.00
0.99
0.55
0.97
0.99
0.97
0.99
1.00
1.00
0.96
1.00
1.00
0.86
0.98
1.00
0.98

0.61
0.57
0.57
0.63
1.00
1.00
1.00
0.57
1.00
1.00
0.07
0.77
0.67
0.89
1.00
1.00
1.00
0.82
1.00
0.99
0.10
0.99
0.99
0.99
1.00
0.99
1.00
1.00
1.00
1.00
0.76
1.00
1.00
1.00

12

Precision Macro-F1
0.64
0.62
0.85
0.68
0.97
0.98
1.00
0.61
1.00
0.99
0.10
0.74
0.90
0.76
0.99
0.99
0.99
0.75
1.00
1.00
0.93
0.95
0.99
0.95
0.98
1.00
1.00
0.93
0.99
1.00
0.96
0.97
1.00
0.95

0.63
0.61
0.73
0.67
0.98
0.99
1.00
0.61
1.00
1.00
0.20
0.74
0.79
0.80
0.99
0.99
0.99
0.77
1.00
0.99
0.54
0.97
0.99
0.97
0.99
1.00
1.00
0.96
1.00
1.00
0.86
0.98
1.00
0.98

FPR AUC score
0.63
0.36
0.63
0.35
0.73
0.10
0.66
0.29
0.98
0.03
0.99
0.02
1.00
0.00
0.63
0.36
1.00
0.00
1.00
0.01
0.20
0.66
0.74
0.28
0.08
0.79
0.80
0.30
0.99
0.02
0.99
0.01
0.99
0.01
0.77
0.28
1.00
0.00
0.99
0.00
0.01
0.54
0.97
0.05
0.99
0.01
0.97
0.05
0.99
0.02
1.00
0.00
1.00
0.00
0.96
0.07
1.00
0.01
1.00
0.00
0.03
0.86
0.98
0.03
1.00
0.00
0.98
0.05

A PREPRINT - MAY 24, 2021

Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM
Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM
Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM
Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM
Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM
Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM

1.00
1.00
1.00
0.96
1.00
1.00
0.70
0.74
0.76
0.75
1.00
1.00
1.00
0.73
1.00
1.00
0.37
0.77
0.84
0.79
0.99
0.99
0.99
0.68
1.00
0.99
0.21
0.83
0.92
0.84
1.00
1.00
1.00
0.87
1.00
1.00
0.52
0.68
0.92
0.71
1.00
1.00
1.00
0.63
1.00
1.00
0.68
0.70
0.96
0.86
1.00
1.00
1.00
0.71
1.00
1.00
0.54
0.80
0.91

MIT Live
Lpr

Xlock

Live
Named

Login and
Ps

Inetd

Stide

1.00
1.00
1.00
0.96
1.00
1.00
0.70
0.74
0.75
0.75
1.00
1.00
1.00
0.73
1.00
1.00
0.37
0.77
0.84
0.79
0.99
0.99
0.99
0.68
1.00
0.99
0.21
0.83
0.92
0.84
1.00
1.00
1.00
0.87
1.00
1.00
0.51
0.68
0.92
0.71
1.00
1.00
1.00
0.63
1.00
1.00
0.68
0.70
0.96
0.86
1.00
1.00
1.00
0.71
1.00
1.00
0.54
0.80
0.91

1.00
1.00
1.00
0.93
1.00
1.00
0.79
0.78
0.79
0.77
0.99
1.00
1.00
0.78
1.00
1.00
0.36
0.84
0.77
0.82
0.97
0.98
0.99
0.68
1.00
0.99
0.29
0.90
0.99
0.95
1.00
1.00
1.00
0.92
1.00
1.00
0.53
0.77
0.92
0.78
1.00
1.00
1.00
0.67
1.00
1.00
0.64
0.69
0.97
0.80
1.00
1.00
1.00
0.68
1.00
1.00
0.53
0.82
0.85

1.00
1.00
1.00
1.00
1.00
1.00
0.54
0.67
0.69
0.70
1.00
1.00
1.00
0.66
1.00
1.00
0.31
0.67
0.97
0.74
1.00
1.00
1.00
0.69
1.00
1.00
0.38
0.75
0.87
0.72
1.00
1.00
1.00
0.81
1.00
1.00
0.64
0.55
0.94
0.60
1.00
1.00
1.00
0.55
1.00
1.00
0.82
0.75
0.95
0.95
1.00
1.00
1.00
0.81
1.00
1.00
0.66
0.76
0.99

13

0.00
0.00
0.00
0.07
0.00
0.00
0.15
0.18
0.17
0.20
0.01
0.00
0.00
0.19
0.00
0.00
0.56
0.13
0.29
0.17
0.03
0.02
0.01
0.32
0.00
0.01
0.96
0.08
0.01
0.04
0.00
0.00
0.00
0.07
0.00
0.00
0.61
0.18
0.09
0.18
0.00
0.00
0.00
0.29
0.00
0.00
0.46
0.33
0.03
0.24
0.00
0.00
0.00
0.39
0.00
0.00
0.59
0.17
0.18

1.00
1.00
1.00
0.96
1.00
1.00
0.70
0.74
0.75
0.75
1.00
1.00
1.00
0.73
1.00
1.00
0.37
0.77
0.84
0.79
0.99
0.99
0.99
0.68
1.00
0.99
0.21
0.83
0.92
0.84
1.00
1.00
1.00
0.87
1.00
1.00
0.51
0.68
0.92
0.71
1.00
1.00
1.00
0.63
1.00
1.00
0.68
0.70
0.96
0.86
1.00
1.00
1.00
0.71
1.00
1.00
0.54
0.80
0.91

A PREPRINT - MAY 24, 2021

Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network
K-means
Logistic Regression
SVM
Neural Network
Decision Tree
Random Forest
KNN
Naïve Bayes
BERT
GPT-2 Network

0.80
1.00
1.00
1.00
0.76
1.00
1.00
0.61
0.61
0.65
0.60
0.85
0.88
0.80
0.61
0.94
0.93

0.77
1.00
1.00
1.00
0.80
1.00
1.00
0.62
0.62
0.53
0.70
0.84
0.87
0.70
0.61
0.92
0.92

0.82
1.00
1.00
1.00
0.74
1.00
1.00
0.60
0.59
0.69
0.58
0.85
0.89
0.88
0.60
0.95
0.94

0.80
1.00
1.00
1.00
0.76
1.00
1.00
0.61
0.61
0.65
0.60
0.85
0.88
0.80
0.61
0.94
0.93

0.17
0.00
0.00
0.00
0.29
0.00
0.00
0.39
0.41
0.23
0.49
0.14
0.11
0.09
0.38
0.05
0.06

0.80
1.00
1.00
1.00
0.76
1.00
1.00
0.61
0.61
0.65
0.60
0.85
0.88
0.80
0.61
0.94
0.93

ADFA-
LD

4.3.2 ROC Curve

Figure 3 shows the testing ROC curves of all candidate algorithms on the Synthetic Sendmail dataset. The ROC curves
of the other datasets are included in our GitHub repository. Overall, RF, DT, KNN, BERT, and GPT-2 generate the
highest results and outperform the others on all datasets. Their average AUC scores are 0.986 (RF), 0.982 (DT), and
0.980 (KNN), with 0.995 (BERT) and 0.993 (GPT-2) demonstrating near perfect performances. Moreover, their FPR
are below 0.04 on all datasets except ADFA-LD. These ﬁve algorithms can truly learn and effectively distinguish
between normal sequences and intrusion sequences. Contrarily, most of the other candidates only perform well on
certain datasets, such as the Synthetic Ftp, Synthetic Lpr, Live Lpr, Xlock, Live Named, Inetd, and Stide datasets.
Besides the ﬁve best candidates, there is no other algorithm that performs well when trained and tested on the Synthetic
Sendmail, MIT Live Lpr, and ADFA-LD datasets.

Figure 3: ROC Curves on the Synthetic Sendmail dataset regarding different machine learning algorithms.

When training and testing the machine learning algorithms with the Synthetic Sendmail dataset, RF, DT, KNN, BERT,
and GPT-2 surpass the other algorithms with AUC above 0.98. On the other hand, the other algorithms can only achieve
AUC up to 0.74. The difference in performance has separated the candidate algorithms into two groups. One contains
more effective algorithms, such as BERT, GPT-2, RF, DT, and KNN, and the other one contains average algorithms,
such as SVM, K-means, LR. Next, we will use the log ratio of recall over FPR to determine the best and worst intrusion
detection algorithms in Section 4.3.3.

14

A PREPRINT - MAY 24, 2021

4.3.3 Ratio of Recall over False Positive Rate

To ﬁnd out the best algorithm among DT, RF, KNN, BERT, and GPT-2, we take the average of log ratios between
recall (TPR) and FPR from Table 3 and demonstrate it in Figure 4. The highest bar shows the best performing model.
Therefore, BERT is the best model with the highest average of log ratio between recall and false positive, which is
2.75. This indicates that BERT yields the highest true positive (recall) at a very low FPR, which is our primary goal in
an intrusion detection system. BERT achieves a 0.00 FPR on nine out of 11 datasets (except the Synthetic Lpr and
ADFA-LD ones). The model’s FPR on the Xlock data is 0.01 and on the ADFA-LD is 0.05. BERT also achieves the
highest recalls on all datasets. Additionally, GPT-2 is the second best model with the second highest average of log
ratio, which is 2.74. GPT-2 achieves the highest recalls on ten out of 11, and its FPR is always below 0.06. Furthermore,
KNN is the third best model with 2.72 average of log ratio. This is because KNN has higher FPR than DT and RF on
the Synthetic Ftp, Xlock, and ADFA-LD datasets. On the other hand, K-means is the lowest performing model with the
average log ratio of 0.19. NB is the second lowest performing model, whose average log ratio is 0.56. This indicates
that these models cannot distinguish between normal sequences and intrusion sequences. Since a low FPR is more
important than recall and accuracy, we conclude that BERT is the best candidate in detecting intrusions.

Figure 4: Log ratio of recall over false positive rate using different machine learning models

4.4 Results discussion and data quality assessment

To compare the overall performance on different datasets, we have created a clustered bar chart of average recalls
(TPR) and average FPR - Figure 5. This ﬁgure is plotted using the performance metrics of processed data and original
data, and it is sorted decreasingly by the average FPR. The blue bars are the average recalls of the original data, while
the orange bars are the average recalls of the processed data. The grey bars are the average FPR of the original data,
whereas the yellow bars are the average of the processed data. By visualizing the difference in performances among the
datasets, we can identify the best dataset as well as the worst dataset. After being processed, the Live Lpr dataset yields
the highest TPR and the lowest FPR. Likewise, the Synthetic Lpr yields the second highest TPR and the second lowest
FPR. Therefore, for HIDS, the Live Lpr is the best dataset and the Synthetic Lpr is the second best.

Additionally, Figure 5 is also helpful in delineating the difference in performances before and after a dataset is processed.
Based on this ﬁgure, the best performing processed datasets with the lowest average FPR are the Live Lpr, Synthetic
Lpr, MIT Lpr, and Live Named in decreasing order. This indicates that most candidate algorithms yield very low FPR
on these datasets. This is also conﬁrmed by the performance metrics from Table 3. The magnitude of the grey bars
indicates that these datasets do not yield such low FPR before being processed. On average, the original Live Lpr
dataset has lower quality and, therefore, yields a FPR 20 times higher than the processed one. Similarly, the original
Synthetic Lpr yields a FPR 27.5 times higher than the processed one, the original Live Named yields a FPR 5 times
higher, and the original MIT Lpr yields a FPR 1.8 times higher. Furthermore, these datasets yield higher recall after
being processed. On average, after being processed, the Live Lpr yields recall 1.22 times higher and the Synthetic Lpr
yields recall almost 1.5 times higher. The processed MIT Lpr only increases its average recall by 0.006 because there is
no duplication in this dataset. Therefore, the data cleaning process has no effect on this data; hence, its performance
was not increased. Although the processed Live Named has lower recall than the original data, its FPR is signiﬁcantly

15

decreased from 0.644 to 0.116. Therefore, it is safe to say that effective data cleaning has lowered the FPR and increased
the TPR and, therefore, improved the models’ performance on these datasets. On the other hand, the ADFA-LD dataset
has the lowest average recall and the highest FPR. Therefore, it is the worst dataset for HIDS.

A PREPRINT - MAY 24, 2021

Figure 5: A Clustered Bar Chart of Average Recalls and Average FPR from Unprocessed and Processed Data

Based on these observations, we also conduct data quality assessment to determine the best characteristics that a HIDS
dataset should possess in order to yield the best possible result. Using data quality principles mentioned in section 3.3,
we have created a data quality evaluation table (Table 4) to show the detailed characteristics of each dataset.

As can be seen from Table 4, Live Lpr, Live Named, and Synthetic Lpr are the three best datasets with the highest
performance. Overall, these datasets are very similar to the other datasets in terms of data quality principles and
dimensions. However, what set them apart is some of their data characteristics. Live Lpr, Live Named, and Synthetic
Lpr have a similarity in data reputation, where data were collected over at least a month. This provides sufﬁcient data
from both classes for training and testing purposes.

Another common ground these datasets have is data comprehensiveness, where original and processed data are
imbalanced. The disproportion between normal and intrusion classes is typical and inevitable in a HIDS dataset as
intrusion rarely happens. Nevertheless, the outstanding performances on these three datasets have proven that class
imbalance is not an issue in HIDS performance since we use the bootstrapping technique as a countermeasure.

Furthermore, these datasets contain consistent data, where each trace is collected sequentially and then grouped by its
PID during data processing. As processes were running concurrently during data collection, this step ensures that each
system call trace belongs to only one PID without any interruption from other PIDs. ADFA-LD contains data from
different processes without PID, so we could not process them similarly to the UNM and MIT datasets. As traces of
system calls are not grouped by their corresponding PID, signature sequences are not guaranteed to be from the same
PID. This is a probable reason why ADFA-LD yields the lowest overall performance (0.61).

Besides, both classes from these datasets are collected and stored separately. This guarantees correct data labeling
(data accuracy) and, hence, increases the models’ performances. Lastly, having a medium to large overlap percentage
between normal sequences and intrusion sequences could have been an obstacle in yielding optimal performances.
However, effective data cleaning has helped us avoid this problem. By removing duplicated sequences existing in both
classes, we only train the models with unique signature sequences from each class. As a result, this increases recalls,
reduces false positives and false negatives, and, therefore, boosts the model performances on these datasets.

Our analysis shows that the most important qualities in a HIDS dataset are data reputation, data accuracy, and data
consistency. Data reputation ensures data sufﬁciency and trustworthiness in the data source. To achieve this quality,
data should be collected over at least a month of activities by a credible institute. Data accuracy guarantees correct
data labeling and reliable detection results. This can be achieved by following the same data collection technique from

16

A PREPRINT - MAY 24, 2021

UNM, MIT, and ADFA that is collecting data from different classes separately. Data consistency ensures signature
sequences are pure and corresponding to the same PID. This quality can be achieved by recording each system call
along with their corresponding PID - similar to the UNM data format.

Table 4: Data quality evaluation of the datasets used in the experiment regarding the data quality dimensions discussed
in Section 3.3. A: Reputation; B: Relevance; C: Comprehensiveness; D: Timeliness; E: Variety; F: Accuracy; G:
Consistency; H: Duplication. Performance represents the average log ratio of TPR over FPR per dataset.

Dataset

Synthetic
Sendmail

Synthetic
Ftp

Synthetic
Lpr

Live Lpr

MIT Live
Lpr

Data quality evaluation
A: Collected from the sendmail program using strace on Sun SPARC stations running
unpatched SunOS 4.1.1 and 4.1.4.
B: To monitor normal usage and detect sunsendmailcp intrusion, decode intrusion, and
forwarding loops error.
C: Both original data (1.8 million normal vs. 6,755 intrusion system calls) and pro-
cessed data (7,759 normal vs. 451 intrusion sequences) are imbalanced.
D: Collected in 1996.
E: The signature of both classes covers system call numbers from 1 to 168.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: There is 99.5% of overlap between normal sequences and intrusion sequences.
A: Collected from the Washington University ftpd server using strace on a Linux
machine.
B: To monitor normal usage and detect misconﬁguration vulnerability.
C: Both original data (180,315 normal vs. 1,363 intrusion system calls) and processed
data (28,415 normal vs. 376 intrusion sequences) are extremely imbalanced.
D: Collected in 1998.
E: The signature of both classes covers system call numbers from 1 to 164.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: There is 84.15% of overlap between normal sequences and intrusion sequences.
A: Collected from the lpr program over 15 months using strace on Sun SPARC stations
running unpatched SunOS 4.1.4.
B: To monitor normal usage data and detect lprcp intrusion signature.
C: Original data is extremely imbalanced (2,400 normal vs. 164,232 intrusion system
calls). Processed data is imbalanced (975 normal vs. 2,232 intrusion sequences).
D: Collected in 1991.
E: The signature of both classes covers system call numbers from 2 to 168.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: There is 98.08% of overlap between normal sequences and intrusion sequences.
A: Live data were collected over 3 months from a SunOS 4.1.4 machine at UNM.
B: To monitor normal usage data and lprcp intrusion signature from the same MIT Lpr
scripted attack.
C: Both original data (187,102 normal vs. 164,232 intrusion system calls) and pro-
cessed data (108,700 normal vs. 4,000 intrusion sequences) are balanced.
D: Collected in 1996.
E: The signature of both classes covers system call numbers from 1 to 168.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: There is 67.92% of overlap between normal sequences and intrusion sequences.
A: Live data were collected over 2 weeks from 77 hosts on SunOS 4.1.4 machines at
the MIT lab.
B: To monitor normal usage data and lprcp intrusion signatures from scripted attacks.
C: Original data is balanced (174,260 normal vs. 165,248 intrusion system calls).
D: Collected in 1997.
E: The signature of both classes covers system call numbers from 0 to 169.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: No duplication.

17

Performance
1.40

1.40

1.94

2.34

1.68

A PREPRINT - MAY 24, 2021

Xlock

Live
Named

Login and
ps

Inetd

Stide

A: Both live and synthetic data from Xlock were collected on a Linux machine over 2
days.
B: To monitor normal usage of xlock command and detect a buffer overﬂow exploit
signature.
C: Original data is extremely imbalanced (339,177 normal vs. 949 intrusion system
calls). Therefore, we only use 25,000 normal system calls. Processed data is imbalanced
(19,487 normal vs. 635 intrusion sequences).
D: Collected in 1997.
E: The signature of both classes covers system call numbers from 1 to 164.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: There is 23.09% of overlap between normal sequences and intrusion sequences.
A: Live normal data was collected over a month from the Named program on a UNM
Linux 2.0.35 kernel.
B: To monitor normal usage data and detect a buffer overﬂow exploit.
C: Original data is extremely imbalanced (9.2 million normal vs. 1,800 intrusion system
calls). Therefore, we only use 2,000 normal system calls to create a more balanced
dataset. Processed data is imbalanced (99 normal vs. 273 intrusion sequences).
D: Collected in 1998.
E: The signature of both classes covers system call numbers from 1 to 141.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: There is 90.18% of overlap between normal sequences and intrusion sequences.
A: Both live and synthetic data were collected on a 2.0.35 Linux kernel over a month.
The Login version is from Red Hat util-linux-2.5.38. The Ps version is from Red Hat
procps v.1.01.
B: To monitor normal usage data and detect Trojan intrusion signature.
C: Original data is balanced (15,050 normal vs. 11,825 intrusion system calls). Pro-
cessed data is imbalanced (176 normal vs. 714 intrusion sequences).
D: Collected in 1998.
E: The signature of both classes covers system call numbers from 1 to 142.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: There is 96.69% of overlap between normal sequences and intrusion sequences.
A: Live data was collected from the Inetd program on a Linux 2.0.35 kernel at UNM.
B: To monitor normal usage data and detect the signature of a DoS attack which ties
up all network connection resources.
C: Original data is imbalanced (541 normal vs. 8,371 intrusion system calls). Processed
data is balanced (536 normal vs. 487 intrusion sequences).
D: Collected in 1999.
E: The signature of both classes covers system call numbers from 1 to 137.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: There is 94.53% of overlap between two classes, and all normal data appear in the
intrusion data. Therefore, we only remove the overlapped intrusion sequences.
A: Live Stide data was collected from a modiﬁed Linux 2.0.35 kernel at UNM.
B: To monitor normal usage data and detect the signature of a DoS attack which affects
requesting memory from other programs.
C: Original data is extremely unbalanced (15.6 million normal vs. 206,000 intrusion
system calls). Therefore, we only used 1.1 million normal system calls. Processed data
is imbalanced (17,182 normal vs. 1,562 intrusion sequences).
D: Collected in 1999.
E: The signature of both classes covers system call numbers from 1 to 136.
F: Data is correctly labeled. Normal and intrusion data are stored in separate folders.
G: Data is consistent. Features are extracted sequentially by PIDs.
H: There is 98.57% of overlap between normal sequences and intrusion sequences.

1.22

1.98

1.73

1.70

1.75

18

A PREPRINT - MAY 24, 2021

ADFA-
LD

0.61

A: Live data was collected from an UbuntuOS version 11.04.
B: To monitor normal usage data and detect different types of attack signatures. There
are six attack types included in the testing set: brute force attack over open FTP ports
and SSH ports, unauthorized root user creation, target host compromise through Java
and Linux meterpreter payloads, privilege escalation over webshell.
C: Both original data (308,077 normal vs. 317,388 intrusion system calls) and pro-
cessed data are balanced (161,400 normal vs. 194,000 intrusion sequences).
D: Collected in 2013.
E: The signature of both classes covers system call numbers from 1 to 325 in Linux
kernel 2.6.38.
F: Data is correctly labeled. The training and validating sets only contain normal data,
and the testing set only contains intrusion data.
G: Data is consistent. Features are extracted by speciﬁc processes.
H: There is 43.16% of overlap between normal sequences and intrusion sequences.

5 Conclusion and future work

Both the quality of a dataset and the capability of a model could contribute to the performance of a machine learning
system. However, researchers usually under-value data work vis-a-vis model development [3]. In this article, we ﬁrst
discussed the data preparation workﬂow and data quality attributes for intrusion detection. Taking a HIDS as a case
study, we then conducted experiments on 11 datasets using seven machine learning models and three deep learning
models. Based on the experimental results, we propose the following conclusions: (1) Deep learning models, such
as BERT and GPT-2, outperform the traditional machine learning models for intrusion detection since the former can
encode the contextual information for sequential data (Figure 4). (2) Almost all of the algorithms achieved a better
performance on the Synthetic Lpr, Live Lpr, Xlock, Live Named, Inetd, and Stide datasets than the others, indicating
that the data quality of these datasets might be higher than the other datasets (Table 3). (3) Improving the data quality
can enhance the performance of the machine learning performance in most of the situations (Figure 5). (4) The class
imbalance issue in intrusion detection could be solved by using the bootstrapping technique to generate a balanced
dataset in different categories. (5) Reputation, accuracy, and consistency are the data quality dimensions which yield
high quality datasets for HIDS in this research.

To assure data quality, we need to carefully check the data quality ﬁrst. Tools such as the data validation system [64],
BoostClean [65], and ActiveClean [66] can be used to detect and ﬁx some potential data issues. However, these tools
fail to connect data quality with machine learning performance. Therefore, it is necessary to conduct quantitative studies
to verify the correlations between data quality and machine learning performance. The second step is to improve the
data quality. Different approaches can be used to improve data quality regarding different data quality dimensions. For
example, to improve the correctness of a dataset, we may need to remove the label noises. To improve the variety of
the dataset, we increase the unique data items in a dataset and make sure the distribution of the dataset follows the
distribution of the population. To alleviate the data imbalance issue, we can use the bootstrapping technique to generate
a more balanced dataset. Techniques such as transfer learning [5] and knowledge graph [67] have also been proved
useful for data quality improvement.

In the future, we will use external resources, such as knowledge graphs, to enhance the semantic representation of the
input data to improve the model performance for intrusion detection [67]. More importantly, transfer learning over
knowledge graph is also a promising strategy to incorporate domain knowledge for performance improvement for
intrusion detection.

Acknowledgements

This work was supported in part by the National Science Foundation under Grant 1852249, in part by the National
Security Agency under Grant H98230-20-1-0417. The authors would like to thank Mr. Abdullah Gadi for conducting
the investigation of existing public datasets for IDS and creating tables 2. The authors are grateful to Ms. Marie
Bloechle for editing the paper.

19

A PREPRINT - MAY 24, 2021

References

[1] Abiodun Ayodeji, Yong-kuo Liu, Nan Chao, and Li-qun Yang. A new perspective towards the development of
robust data-driven intrusion detection for industrial control systems. Nuclear Engineering and Technology, 2020.

[2] Ashima Chawla, Brian Lee, Sheila Fallon, and Paul Jacob. Host based intrusion detection system with combined
cnn/rnn model. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
pages 149–158. Springer, 2018.

[3] Nithya Sambasivan, Shivani Kapania, Hannah Highﬁll, Diana Akrong, Praveen Paritosh, and Lora M Aroyo.
“everyone wants to do the model work, not the data work”: Data cascades in high-stakes ai. In Proceedings of the
2021 CHI Conference on Human Factors in Computing Systems, CHI ’21, New York, NY, USA, 2021.

[4] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender

classiﬁcation. In Conference on fairness, accountability and transparency, pages 77–91, 2018.

[5] Haihua Chen, Jiangping Chen, and Junhua Ding. Data evaluation and enhancement for quality improvement of

machine learning. IEEE Transactions on Reliability, pages 1–17, 2021.

[6] Abhijeet Sahu, Zeyu Mao, Katherine Davis, and Ana E Goulart. Data processing and model selection for machine
learning-based network intrusion detection. In 2020 IEEE International Workshop Technical Committee on
Communications Quality and Reliability (CQR), pages 1–6. IEEE, 2020.

[7] Hassan Hadi Al-Maksousy, Michele C Weigle, and Cong Wang. Nids: Neural network based intrusion detection
system. In 2018 IEEE International Symposium on Technologies for Homeland Security (HST), pages 1–6. IEEE,
2018.

[8] Jiankun Hu, Xinghuo Yu, Dong Qiu, and Hsiao-Hwa Chen. A simple and efﬁcient hidden markov model scheme

for host-based anomaly intrusion detection. IEEE network, 23(1):42–47, 2009.

[9] Nam Nhat Tran, Ruhul Sarker, and Jiankun Hu. An approach for host-based intrusion detection system design
using convolutional neural network. In International Conference on Mobile Networks and Management, pages
116–126. Springer, 2017.

[10] Hongyu Liu and Bo Lang. Machine learning and deep learning methods for intrusion detection systems: A survey.

applied sciences, 9(20):4396, 2019.

[11] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys

(CSUR), 41(3):1–58, 2009.

[12] Shengchu Zhao, Wei Li, Tanveer Zia, and Albert Y Zomaya. A dimension reduction model and classiﬁer for
anomaly-based intrusion detection in internet of things. In 2017 IEEE 15th Intl Conf on Dependable, Autonomic
and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data
Intelligence and Computing and Cyber Science and Technology Congress (DASC/PiCom/DataCom/CyberSciTech),
pages 836–843. IEEE, 2017.

[13] Tara Salman, Deval Bhamare, Aiman Erbad, Raj Jain, and Mohammed Samaka. Machine learning for anomaly
detection and categorization in multi-cloud environments. In 2017 IEEE 4th International Conference on Cyber
Security and Cloud Computing (CSCloud), pages 97–103. IEEE, 2017.

[14] Razan Abdulhammed, Miad Faezipour, Abdelshakour Abuzneid, and Arafat AbuMallouh. Deep and machine
learning approaches for anomaly-based intrusion detection of imbalanced network trafﬁc. IEEE sensors letters,
3(1):1–4, 2018.

[15] Sidharth Behera, Ayush Pradhan, and Ratnakar Dash. Deep neural network architecture for anomaly based
intrusion detection system. In 2018 5th International Conference on Signal Processing and Integrated Networks
(SPIN), pages 270–274. IEEE, 2018.

[16] Sheraz Naseer, Yasir Saleem, Shehzad Khalid, Muhammad Khawar Bashir, Jihun Han, Muhammad Munwar
Iqbal, and Kijun Han. Enhanced network anomaly detection based on deep neural networks. IEEE access,
6:48231–48246, 2018.

[17] Nebrase Elmrabit, Feixiang Zhou, Fengyin Li, and Huiyu Zhou. Evaluation of machine learning algorithms for
anomaly detection. In 2020 International Conference on Cyber Security and Protection of Digital Services (Cyber
Security), pages 1–8. IEEE, 2020.

[18] Roya Aliakbarisani, Abdorasoul Ghasemi, and Shyhtsun Felix Wu. A data-driven metric learning-based scheme

for unsupervised network anomaly detection. Computers & Electrical Engineering, 73:71–83, 2019.

20

A PREPRINT - MAY 24, 2021

[19] Mukrimah Nawir, Amiza Amir, Naimah Yaakob, and Ong Bi Lynn. Effective and efﬁcient network anomaly
detection system using machine learning algorithm. Bulletin of Electrical Engineering and Informatics, 8(1):46–51,
2019.

[20] Marina Evangelou and Niall M Adams. An anomaly detection framework for cyber-security data. Computers &

Security, 97:101941, 2020.

[21] Jorge Meira, Rui Andrade, Isabel Praça, João Carneiro, Verónica Bolón-Canedo, Amparo Alonso-Betanzos, and
Goreti Marreiros. Performance evaluation of unsupervised techniques in cyber-attack anomaly detection. Journal
of Ambient Intelligence and Humanized Computing, 11(11):4477–4489, 2020.

[22] Ayush Hariharan, Ankit Gupta, and Trisha Pal. Camlpad: Cybersecurity autonomous machine learning platform
for anomaly detection. In Future of Information and Communication Conference, pages 705–720. Springer, 2020.

[23] Sarika Choudhary and Nishtha Kesswani. Analysis of kdd-cup’99, nsl-kdd and unsw-nb15 datasets using deep

learning in iot. Procedia Computer Science, 167:1561–1573, 2020.

[24] Maonan Wang, Kangfeng Zheng, Yanqing Yang, and Xiujuan Wang. An explainable machine learning framework

for intrusion detection systems. IEEE Access, 8:73127–73141, 2020.

[25] Hamed Alqahtani, Iqbal H Sarker, Asra Kalim, Syed Md Minhaz Hossain, Sheikh Ikhlaq, and Sohrab Hossain.
Cyber intrusion detection using machine learning classiﬁcation techniques. In International Conference on
Computing Science, Communication and Security, pages 121–131. Springer, 2020.

[26] Basant Subba, Santosh Biswas, and Sushata Karmakar. Host based intrusion detection system using frequency
analysis of n-gram terms. In TENCON 2017-2017 IEEE Region 10 Conference, pages 2006–2011. IEEE, 2017.

[27] Pierre-François Marteau. Sequence covering for efﬁcient host-based intrusion detection. IEEE Transactions on

Information Forensics and Security, 14(4):994–1006, 2018.

[28] Elham Besharati, Marjan Naderan, and Ehsan Namjoo. Lr-hids: logistic regression host-based intrusion detection
system for cloud environments. Journal of Ambient Intelligence and Humanized Computing, 10(9):3669–3692,
2019.

[29] Iqbal H Sarker, Yoosef B Abushark, Fawaz Alsolami, and Asif Irshad Khan. Intrudtree: a machine learning based

cyber security intrusion detection model. Symmetry, 12(5):754, 2020.

[30] Shijoe Jose, D Malathi, Bharath Reddy, and Dorathi Jayaseeli. A survey on anomaly based host intrusion detection

system. In Journal of Physics: Conference Series, volume 1000, page 012049. IOP Publishing, 2018.

[31] Markus Ring, Sarah Wunderlich, Deniz Scheuring, Dieter Landes, and Andreas Hotho. A survey of network-based

intrusion detection data sets. Computers & Security, 86:147–167, 2019.

[32] Ansam Khraisat, Iqbal Gondal, Peter Vamplew, and Joarder Kamruzzaman. Survey of intrusion detection systems:

techniques, datasets and challenges. Cybersecurity, 2(1):1–22, 2019.

[33] Salvatore J Stolfo, Wei Fan, Wenke Lee, Andreas Prodromidis, and Philip K Chan. Cost-based modeling for
fraud and intrusion detection: Results from the jam project. In Proceedings DARPA Information Survivability
Conference and Exposition. DISCEX’00, volume 2, pages 130–144. IEEE, 2000.

[34] Jungsuk Song, Hiroki Takakura, and Yasuo Okabe. Description of kyoto university benchmark data. http:
//www.takakura.com/Kyoto_data/BenchmarkData-Description-v5.pdf, 2006. [online; accessed 12-
April-2021].

[35] DARPA2009. Darpa 2009 intrusion detection dataset. http://www.darpa2009.netsec.colostate.edu/,

2009. [online; accessed 12-April-2021].

[36] Mahbod Tavallaee, Ebrahim Bagheri, Wei Lu, and Ali A Ghorbani. A detailed analysis of the kdd cup 99 data set.
In 2009 IEEE symposium on computational intelligence for security and defense applications, pages 1–6. IEEE,
2009.

[37] Ali Shiravi, Hadi Shiravi, Mahbod Tavallaee, and Ali A Ghorbani. Toward developing a systematic approach to

generate benchmark datasets for intrusion detection. computers & security, 31(3):357–374, 2012.

[38] Nour Moustafa and Jill Slay. Unsw-nb15: a comprehensive data set for network intrusion detection systems
(unsw-nb15 network data set). In 2015 military communications and information systems conference (MilCIS),
pages 1–6. IEEE, 2015.

[39] Waqas Haider, Jiankun Hu, Jill Slay, Benjamin P Turnbull, and Yi Xie. Generating realistic intrusion detection
system dataset based on fuzzy qualitative modeling. Journal of Network and Computer Applications, 87:185–192,
2017.

21

A PREPRINT - MAY 24, 2021

[40] Iman Sharafaldin, Arash Habibi Lashkari, and Ali A Ghorbani. Toward generating a new intrusion detection

dataset and intrusion trafﬁc characterization. In ICISSp, pages 108–116, 2018.

[41] MIT Lincoln Laboratory. 1998 darpa intrusion detection evaluation dataset. https://www.ll.mit.edu/r-d/
datasets/1998-darpa-intrusion-detection-evaluation-dataset, 1998. [online; accessed 12-April-
2021].

[42] Steven A Hofmeyr, Stephanie Forrest, and Anil Somayaji. Intrusion detection using sequences of system calls.

Journal of computer security, 6(3):151–180, 1998.

[43] Gideon Creech and Jiankun Hu. A semantic approach to host-based intrusion detection systems using contiguou-

sand discontiguous system call patterns. IEEE Transactions on Computers, 63(4):807–819, 2013.

[44] Dainius ˇCeponis and Nikolaj Goranin. Towards a robust method of dataset generation of malicious activity
for anomaly-based hids training and presentation of awsctd dataset. Baltic Journal of Modern Computing,
6(3):217–234, 2018.

[45] Marc-Oliver Pahl and François-Xavier Aubet. All eyes on you: Distributed multi-dimensional iot microservice
anomaly detection. In 2018 14th International Conference on Network and Service Management (CNSM), pages
72–80. IEEE, 2018.

[46] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender
classiﬁcation. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81,
pages 77–91, New York, NY, USA, 2018.

[47] Curtis G Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machine

learning benchmarks. arXiv preprint arXiv:2103.14749, 2021.

[48] Eitel J. M. Lauría and G. Tayi. Statistical machine learning for network intrusion detection: a data quality

perspective. International Journal of Services Sciences, 1:179–195, 2008.

[49] Abhishek Divekar, Meet Parekh, Vaibhav Savla, R. Mishra, and M. Shirole. Benchmarking datasets for anomaly-
based network intrusion detection: Kdd cup 99 alternatives. 2018 IEEE 3rd International Conference on
Computing, Communication and Security (ICCCS), pages 1–8, 2018.

[50] Rakesh M. Verma, Victor Zeng, and Houtan Faridi. Data quality for security challenges: Case studies of phishing,
malware and intrusion detection datasets. Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security, 2019.

[51] Wenfei Fan. Data quality: From theory to practice. Acm Sigmod Record, 44(3):7–18, 2015.
[52] Alan F. Karr, Ashish P. Sanil, and David L. Banks. Data quality: A statistical perspective. Statistical Methodology,

3(2):137–173, 2006.

[53] Chieh-Han Wu and Yang Song. Robust and distributed web-scale near-dup document conﬂation in microsoft
academic service. In 2015 IEEE International Conference on Big Data (Big Data), pages 2606–2611. IEEE, 2015.

[54] José A Sáez, Bartosz Krawczyk, and Michał Wo´zniak. On the inﬂuence of class noise in medical data classiﬁcation:

Treatment using noise ﬁltering methods. Applied Artiﬁcial Intelligence, 30(6):590–609, 2016.

[55] Gabriel Maciá-Fernández, José Camacho, Roberto Magán-Carrión, Pedro García-Teodoro, and Roberto Therón.
Ugr ‘16: A new dataset for the evaluation of cyclostationarity-based network idss. Computers & Security,
73:411–424, 2018.

[56] Richard Y Wang and Diane M Strong. Beyond accuracy: What data quality means to data consumers. Journal of

management information systems, 12(4):5–33, 1996.

[57] Carlo Batini, Cinzia Cappiello, Chiara Francalanci, and Andrea Maurino. Methodologies for data quality

assessment and improvement. ACM computing surveys (CSUR), 41(3):1–52, 2009.

[58] Haihua Chen, Gaohui Cao, Jiangping Chen, and Junhua Ding. A practical framework for evaluating the quality of
knowledge graph. In China Conference on Knowledge Graph and Semantic Computing, pages 111–122. Springer,
2019.

[59] Audun Jøsang, Roslan Ismail, and Colin Boyd. A survey of trust and reputation systems for online service

provision. Decision support systems, 43(2):618–644, 2007.

[60] Amira Bradai and Hossam Aﬁﬁ. Game theoretic framework for reputation-based distributed intrusion detection.

In 2013 International Conference on Social Computing, pages 558–563. IEEE, 2013.

[61] Ranjit Panigrahi and Samarjeet Borah. A detailed analysis of cicids2017 dataset for designing intrusion detection

systems. International Journal of Engineering & Technology, 7(3.24):479–482, 2018.

22

A PREPRINT - MAY 24, 2021

[62] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pages 4171–4186, 2019.

[63] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are

unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[64] Neoklis Polyzotis, Martin Zinkevich, Sudip Roy, Eric Breck, and Steven Whang. Data validation for machine

learning. Proceedings of Machine Learning and Systems, 1:334–347, 2019.

[65] Sanjay Krishnan, Michael J Franklin, Ken Goldberg, and Eugene Wu. Boostclean: Automated error detection and

repair for machine learning. arXiv preprint arXiv:1711.01299, 2017.

[66] Sanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J Franklin, and Ken Goldberg. Activeclean: Interactive

data cleaning for statistical modeling. Proceedings of the VLDB Endowment, 9(12):948–959, 2016.

[67] Shreyansh Bhatt, Amit Sheth, Valerie Shalin, and Jinjin Zhao. Knowledge graph semantic enhancement of input

data for improving ai. IEEE Internet Computing, 24(2):66–72, 2020.

A Appendix A: Abbreviations of the machine learning models

Logistic regression (LR), support vector machine (SVM), decision tree (DT), random forest (RF), neural network
(NN), artiﬁcial neural network (ANN), isolation forest (IF), k nearest neighbor (KNN), scaled convex hull (SCH),
histogram-based outlier score (HBOS), cluster-based local outlier factor (CBLOF), naïve Bayes (NB), averaged one
dependence estimator (AODE), radial basis function network (RBFN), multi-layer perceptron (MLP), deep neural
networks (DNN), convolutional neural network (CNN), long short-term memory (LSTM), recurrent neural network
(RNN), gated recurrent unit (GRU), softmax regression (SR), hidden Markov model (HMM), variational autoencoder
(VAE), sequence covering for intrusion detection (SC4ID), intrusion detection tree (IntruDTree).

23

