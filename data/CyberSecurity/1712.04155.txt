8
1
0
2

y
a
M
0
1

]
I

A
.
s
c
[

2
v
5
5
1
4
0
.
2
1
7
1
:
v
i
X
r
a

Towards ‘Verifying’ a Water Treatment System(cid:63)

Jingyi Wang13, Sun Jun1, Yifan Jia14, Shengchao Qin23, Zhiwu Xu3

1Singapore University of Technology and Design
2School of Computing, Media and the Arts,Teesside University
3 College of Computer Science and Software Engineering, Shenzhen University
4TUV-SUD Asia Paciﬁc Pte Ltd, Singapore

Abstract. Modeling and verifying real-world cyber-physical systems is chal-
lenging, which is especially so for complex systems where manually modeling
is infeasible. In this work, we report our experience on combining model learn-
ing and abstraction reﬁnement to analyze a challenging system, i.e., a real-world
Secure Water Treatment system (SWaT). Given a set of safety requirements, the
objective is to either show that the system is safe with a high probability (so that
a system shutdown is rarely triggered due to safety violation) or not. As the sys-
tem is too complicated to be manually modeled, we apply latest automatic model
learning techniques to construct a set of Markov chains through abstraction and
reﬁnement, based on two long system execution logs (one for training and the
other for testing). For each probabilistic safety property, we either report it does
not hold with a certain level of probabilistic conﬁdence, or report that it holds
by showing the evidence in the form of an abstract Markov chain. The Markov
chains can subsequently be implemented as runtime monitors in SWaT.

1

Introduction

Cyber-physical systems (CPS) are ever more relevant to people’s daily life. Examples
include power supply which is controlled by smart grid systems, water supply which is
processed from raw water by a water treatment system, and health monitoring systems.
CPS often have strict safety and reliability requirements. However, it is often challeng-
ing to formally analyze CPS since they exhibit a tight integration of software control
and physical processes. Modeling CPS alone is a major obstacle which hinders many
system analysis techniques like model checking and model-based testing.

The Secure Water Treatment testbed (SWaT) built at Singapore University of Tech-
nology and Design [28] is a scale-down version of an industry water treatment plant in
Singapore. The testbed is built to facilitate research on cyber security for CPS, which
has the potential to be adopted to Singapore’s water treatment systems. SWaT consists
of a modern six-stage process. The process begins by taking in raw water, adding nec-
essary chemicals to it, ﬁltering it via an Ultraﬁltration (UF) system, de-chlorinating
it using UV lamps, and then feeding it to a Reverse Osmosis (RO) system. A back-
wash stage cleans the membranes in UF using the water produced by RO. The cyber
portion of SWaT consists of a layered communications network, Programmable Logic
Controllers (PLCs), Human Machine Interfaces (HMIs), Supervisory Control and Data

(cid:63) Corresponding authors: Sun Jun, Shengchao Qin.

 
 
 
 
 
 
Acquisition (SCADA) workstation, and a Historian. Data from sensors is available to
the SCADA system and recorded by the Historian for subsequent analysis. There are 6
PLCs in the system, each of which monitors one stage using a set of sensors embedded
in the relevant physical plants and controls the physical plants according to predeﬁned
control logics. SWaT has a strict set of safety requirements (e.g., the PH value of the
water coming out of SWaT must be within certain speciﬁc range). In order to guarantee
that the safety requirements are not violated, SWaT is equipped with safety monitoring
devices which trigger a pre-deﬁned shutdown sequence. Our objective is thus to show
that the probability of a safety violation is low and thus SWaT is reliable enough to
provide its service.

One approach to achieve our objective is to develop a model of SWaT and then
apply techniques like model checking. Such a model would have a discrete part which
models the PLC control logic and a continuous part which models the physical plants
(e.g., in the form of differential equations). Such an approach is challenging since
SWaT has multiple chemical processes. For example, the whole process is composed of
pre-treatment, ultraﬁltration and backwash, de-chlorination, reverse osmosis and out-
put of the processed water. The pre-treatment process alone includes chemical dos-
ing, hydrochloric dosing, pre-chlorination and salt dosing. Due to the complexity in
chemical reactions, manual modeling is infeasible. Furthermore, even if we are able
to model the system using modeling notations like hybrid automata [11], the existing
tools/methods [23,9,22] for analyzing such complicated hybrid models are limited.

An alternative approach which does not require manual modeling is statistical model
checking (SMC) [35,16,7]. The main idea is to observe sample system executions and
apply standard techniques like hypothesis testing to estimate the probability that a given
property is satisﬁed. SMC however is not ideal for two reasons. First, SMC treats the
system as a black box and does not provide insight or knowledge of the system on why
a given property is satisﬁed. Second, SMC requires sampling the system many times,
whereas starting/restarting real-world CPS like SWaT many times is not viable.

Recently, there have been multiple proposals on applying model learning techniques
to automatically ‘learn’ system models from system executions and then analyze the
learned model using techniques like model checking. A variety of learning algorithms
have been proposed (e.g., [25,24,4,22]), some of which require only a few system exe-
cutions. These approaches offer an alternative way of obtaining models, when having a
model of such complex systems is a must. For instance, in [19,6,33,32], it is proposed
to learn a probabilistic model ﬁrst and then apply Probabilistic Model Checking (PMC)
to calculate the probability of satisfying a property based on the learned model.

It is however far from trivial to apply model learning directly on SWaT. Existing
model learning approaches have only been applied to a few small benchmark systems.
It is not clear whether they are applicable or scalable to real-world systems like SWaT.
In particular, there are many sensors in SWaT, many of which generate values of type
ﬂoat or double. As a result, the sensor readings induce an ‘inﬁnite’ alphabet which
immediately renders many model learning approaches infeasible. In fact, existing model
learning approaches have rarely discussed the problem of data abstraction. To the best
of our knowledge, the only exception is the LAR method [32], which proposes a method
of combining model learning and abstraction/reﬁnement. However, LAR requires many

system executions as input, which is infeasible in SWaT. In this work, we adapt the
LAR method so that we require only two long sequences of system execution logs (one
for training and the other for testing) as input. We successfully ‘veriﬁed’ most of the
properties for SWaT this way. For each property, we either report that the property is
violated with a certain conﬁdence, or report that the property is satisﬁed, in which case
we output a model in the form of an abstract Markov chain as evidence, which could
be further validated by more system runs or expert review. Note that in practice these
models could be implemented as runtime monitors in SWaT.

The remainders of the paper are organized as follows. Sec. 2 presents background
on SWaT, our objectives as well as some preliminaries. Sec. 3 details our learning ap-
proach. We present the results in Sec. 4 and conclude with related work in Sec. 5.

2 Background

In this section, we present the target SWaT system and state our motivation and goals.

System Overview The system under analysis is the Secure Water Treatment (SWaT)
built at the iTrust Center in Singapore University of Technology and Design [20]. It is
a testbed system which scales down but fully realized the functions of a modern water
treatment system in cities like Singapore. It enables researchers to better understand the
principles of cyber-physical Systems (CPS) and further develop and experiment with
smart algorithms to mitigate potential threats and guarantee its safety and reliability.

SWaT takes raw water as input and executes a series of treatment and output recy-
cled water eventually. The whole process contains 6 stages as shown in Figure 1. The
raw water is taken to the raw water tank (P1) and then pumped to the chemical tanks.
After a series of chemical dosing and a static mixer (P2), the water is ﬁltered by an
Ultra-ﬁltration (UF) system (P3) and UV lamps (P4). It is then fed to a Reverse Osmo-
sis (RO) system (P5) and a backwash process cleans the membranes in UF using the
water produced by RO (P6). For each stage, a set of sensors are employed to monitor
the system state. Meanwhile, a set of actuators controlled by the programming logic
controller (PLC) are built in to manipulate the state of the physical process. The read-
ings of sensors are collected and sent periodically to the PLC, while the PLC returns a
set of actuators values according to the control logics and the current sensor values. For
instance, the sensor LIT 101 is used to monitor the water level of the Raw Water Tank.
The PLC reads its value and decides whether to set a new value to the actuators. For
example if LIT 101 is beyond a threshold, the PLC may deactivate the valve M V 101
to stop adding water into the tank.

SWaT has many built-in safety mechanisms enforced in PLC. Each stage is con-
trolled by local dual PLCs with approximately hundreds of lines of code. In case one
PLC fails, the other PLC takes over. The PLC inspects the received and cached sensor
values and decides the control strategy to take. Notice that the sensor values are ac-
cessible across all PLCs. For example, the PLC of tank 1 may decide whether to start
pump P 101 according to the value of LIT 301, i.e., the water level of tank 3. In case
the controller triggers potential safety violations of the system according to the current
values of the sensors, the controller may shut down the system to ensure the safety. The

Fig. 1: Six stages of water treatment in SWaT [20].

system then needs to wait for further inspection from technicians or experts. Shutting
down and restarting SWaT however is highly non-trivial, which takes signiﬁcant costs
in terms of both time and resource, especially in the real-world scenario. Thus, instead
of asking whether a safety violation is possible, the question becomes: how often a
system shutdown is triggered due to potential safety violations?

In total, SWaT has 25 sensors (for monitoring the status) and 26 actuators (for ma-
nipulating the plants). Each sensor is designed to operate in a certain safe range. If a sen-
sor value is out of the range, the system may take actions to adjust the state of the actua-
tors so that the sensor values would go back to normal. Table 1 shows all the sensors in
the 6 plants, their operation ranges. The sensors has 3 categories distinguished by their
preﬁxes. For instance, AIT xxx stands for Analyzer Indicator/Transmitter; DP IT xxx
stands for Differential Pressure Indicator/Transmitter; F IT xxx stands for Flow Indi-
cator Transmitter; LIT xxx stands for Level Indicator/Transmitter.

SWaT is also equipped with a historian which records detailed system execution
log, including all sensor readings and actuator status. Table 2 shows a truncated system
log with part of sensors. Each row is the sensor readings at a time point and each row is
collected every millisecond. Notice that different sensors may have different collection
period. The table is ﬁlled such that a sensor keeps its old value if no new value is
collected, e.g., AIT 202 in Table 2. A dataset of SWaT has been published by the iTrust
lab in Singapore University of Technology and Design [27,10]. The dataset contains the
execution log of 11 consecutive days (i.e., 7 days of normal operations and another 4
days of the system being under various kind of attacks [27,10]).

Table 1: Safety properties.

Plant Sensor

Description

Operating range points

P1

FIT101
LIT101 Level Transmitter (Ultrasonic)

Flow Transmitter (EMF)

P2 AIT201
AIT202
AIT203
FIT201

Analyser (Conductivity)
Analyser (pH)
Analyser (ORP)
Flow Transmitter (EMF)

P3 DPIT301
FIT301
LIT301 Level Transmitter (Ultrasonic)

DP Transmitter
Flow Transmitter (EMF)

P4 AIT401
AIT402
FIT401
LIT401 Level Transmitter (Ultrasonic)

Analyser (Hardness)
Analyser
Flow Transmitter (EMF)

Analyser (pH)
Analyser (ORP)
Analyser (Cond)
Analyser (Cond)
Flow Transmitter

P5 AIT501
AIT502
AIT503
AIT504
FIT501
FIT502 Flow Transmitter (Paddlewheel)
FIT503
FIT504
PIT501
PIT502
PIT503

Flow Transmitter (EMF)
Flow Transmitter (EMF)
Pressure Transmitter
Pressure Transmitter
Pressure Transmitter

2.5 − 2.6m3/h
500 - 1100mm

30 - 260µS/cm
6-9
200 - 500mV
2.4 - 2.5m3/h

0.1 - 0.3 Bar
2.2 - 2.4m3/
800 - 1000mm

5-30ppm
150 - 300mV
1.5 - 2m3/h
800 - 1000mm

6-8
100-250mV
200- 300µS/cm
5-10µS/cm
1-2m3/h
1.1 - 1.3m3/h
0.7 - 0.9m3/h
0.25 - 0.35m3/h
2-3 Bar
0-0.2 Bar
1-2 Bar

Objectives As discussed above, each sensor reading is associated with a safe range,
which constitutes a set of safety properties (i.e., reachability). We remark that we fo-
cus on safety properties concerning the stationary behavior of the system in this work
rather than those properties concerning the system initializing or shutting down phase.
In general, a stationary safety property (refer to [6] for details) takes the form S≤r(ϕ)
(where r is the safety threshold and ϕ is an LTL formula). In our particular setting, the
property we are interested in is that the probability that a sensor is out of range (either
too high or too low) in the long term is below a threshold. Our objective is to ‘verify’
whether a given set of stationary properties are satisﬁed or not.

Manual modeling of SWaT is infeasible, with 6 water tanks interacting with each
other, plenty of chemical reactions inside the tanks and dozens of valves controlling the
ﬂow of water. A group of experts from Singapore’s Public Utility Board have attempted
to model SWaT manually but failed after months of effort because the system is too
complicated. We remark that without a system model, precisely verifying the system is
impossible. As discussed above, while statistical model checking (SMC) is another op-

Table 2: A concrete system log with the last column being the abstract system log after
predicate abstraction with predicate LIT 101 > 1100.

F IT 101 LIT 101 M V 101 P 101 P 102 AIT 201 AIT 202 AIT 203 F IT 201 LIT 101 >1100

2.470294 261.5804 2
2.457163 261.1879 2
2.439548 260.9131 2
2.428338 260.285 2
2.424815 259.8925 2
2.425456 260.0495 2
2.472857 260.2065 2

2
2
2
2
2
2
2

1
1
1
1
1
1
1

244.3284 8.19008 306.101 2.471278
244.3284 8.19008 306.101 2.468587
244.3284 8.19008 306.101 2.467305
244.3284 8.19008 306.101 2.466536
244.4245 8.19008 306.101 2.466536
244.5847 8.19008 306.101 2.465127
244.5847 8.19008 306.101 2.464742

0
0
0
0
0
0
0

tion to provide a statistical measure on the probability that a safety property is satisﬁed,
it is also infeasible in our setting.

Thus, in this work, we aim to verify the system by means of model learning. That is,
given a safety property, either we would like to show that the property is violated with
certain level of conﬁdence or the property is satisﬁed with certain evidence. Ideally,
the evidence is in the form of a small abstract model, at the right level-of-abstraction,
which could be easily shown to satisfy the property. The advantage of presenting the
model as the evidence is that the model could be further validated using additional data
or through expert review. Furthermore, the models can serve other purposes. Firstly,
the models could be implemented as runtime monitors to detect potential safety viola-
tions at runtime. Secondly, we could also prevent future safety violations by predictive
analysis based on the model and take early actions.

3 Our approach

We surveyed existing model learning algorithms (for the purpose of system veriﬁcation
through model checking) and found most existing model learning approaches [19,6,33]
are inapplicable in our setting. The reason is that the real-typed (ﬂoat or double) vari-
ables in SWaT lead to an inﬁnite alphabet. The only method which seems feasible is
the recently proposed model learning approach called LAR (short for learning, abstrac-
tion and reﬁnement) documented in [32], which allows us to abstract sensor readings in
SWaT and automatically learn models at a proper level of abstraction based on a coun-
terexample guided abstraction reﬁnement (CEGAR) framework. However, LAR was
designed to take many independent execution logs as input whereas we have only few
long system logs of SWaT. We thus adapt LAR to sLAR which learns system models
from a single long system log instead. In the following, we brieﬂy explain how sLAR
works. Interested readers are referred to [32] for the detailed explanation of LAR.

Our overall approach is shown in Fig. 2. Given a training log and a safety property,
we ﬁrst construct an abstract log through predicate abstraction and use a learner to
learn a model based on the abstract log. Then, the safety property is veriﬁed against
the learned model. If the veriﬁcation returns true, we report true and output the learned
model as evidence. Otherwise, we test the property using a validator on the testing log. If
the validator ﬁnds that the property is violated, we report safety violation together with

Fig. 2: Overall approach.

the level of conﬁdence we achieve. Otherwise, we use a reﬁner to reﬁne the abstraction
and start over from the learner. Although sLAR is based on LAR, our goal of this case
study is to verify stationary properties of SWaT and construct a stationary probabilistic
model from one single long system log, which is different from LAR. Consequently,
the procedures to verify the property and validate the result of the veriﬁer are different.
In the following, we present each part of our approach in details.

3.1 The model

From an abstract point of view, SWaT is a system composed of n variables (including
sensors, actuators as well as those variables in the PLC control program) which capture
the system status. A system observation σ is the valuation of all variables at a time point
t. A system log L = σt0σt1 · · · σtk is a sequence of system observations collected from
time point t0 to tk. Given a system log L, we write L(t) = σt to denote the system
observation at time t and Lp(t) to denote the system observations before t, i.e., from
t0 to t. In this case study, we use L and Lt to denote the training log and testing log
respectively. We also use T1 and T2 to denote their lasting time respectively.

Several machine learning algorithms exist to learn a stationary system model from
a single piece of system log [6,24,33]. However, applying these algorithms directly is
infeasible because of the real-typed (ﬂoat or double) variables in SWaT, since system
observations at different time points are almost always different and thus the input al-
phabet for the learning algorithms is ‘inﬁnite’. To overcome this problem, our ﬁrst step
is to abstract the system log through predicate abstraction [29]. Essentially, a predicate
is a Boolean expression over a set of variables. Given a system log and a set of predi-
cates, predicate abstraction turns the concrete variable values to a bit vector where each
bit represents whether the corresponding predicate is true or false. For example, given a
predicate LIT 101 > 1100, the concrete system log on the left of Table 2 becomes the
abstract system log on the right.

The models we learn from the log are in the form of discrete-time Markov Chain
(DTMC), which is a widely used formalism for modeling stochastic behaviors of com-
plex systems. Given a ﬁnite set of states S, a probability distribution over S is a function

µ : S → [0, 1] such that (cid:80)
over S. Formally,

s∈S µ(s) = 1. Let Distr (S) be the set of all distributions

Deﬁnition 1. A DTMC M is a tuple (cid:104)S, ıinit, P r(cid:105), where S is a countable, nonempty
set of states; ıinit : S → [0, 1] is the initial distribution s.t. (cid:80)
s∈S ıinit(s) = 1; and
P r : S → Distr (S) is a transition function such that P r(s, s(cid:48)) is the probability of
transiting from state s to state s(cid:48).

We denote a path starting with s0 by πs0 = (cid:104)s0, s1, s2, · · · , sn(cid:105), which is a sequence
of states in M, where P r(si, si+1) > 0 for every 0 ≤ i < n. Furthermore, we write
Path s
f in(M) to denote the set of ﬁnite paths of M starting with s. We say that sj ∈ πs0
if sj occurs in πs0. In our setting, we use a special form of DTMC, called stationary
DTMC (written as sDTMC) to model the system behaviors in the long term. Compared
to a DTMC, each state in an sDTMC represents a steady state of the system and thus
there is no prior initial distribution over the states.

Deﬁnition 2. An sDTMC is irreducible if for every pair of states si, sj ∈ S , there
exists a path πsi such that sj ∈ πsi.

Intuitively, an sDTMC is irreducible if there is path between every pair of states. For
an irreducible sDTMC, there exists a unique stationary probability distribution which
describes the average time a Markov chain spends in each state in the long run.

Deﬁnition 3. Let µj denote the long run proportion of time that the chain spends in
m=1 I{Xm = sj|X0 = si} with probability 1., for all
state sj: µj = limn→∞
states si. If for each sj ∈ S, µj exists and is independent of the initial state si, and
(cid:80)
sj ∈S µj = 1, then the probability distribution µ = (µ0, µ1, · · · ) is called the limiting

(cid:80)n

1
n

or stationary or steady-state distribution of the Markov chain.

In this work, we ‘learn’ a stationary and irreducible sDTMC to model the long term
behavior of SWaT. By computing the steady-state distribution of the learned sDTMC,
we can obtain the probability that the system is in the states of interests in the long run.

3.2 Learning algorithm

After predicate abstraction, the training log becomes a sequence of bit vectors, which
is applicable for learning. We then apply an existing learning algorithm in [24] to learn
a stationary system model. The initial learned model is in the form of a Probabilistic
Sufﬁx Automata (PSA) as shown in Figure 3, where a system state in the model is
identiﬁed by a ﬁnite history of previous system observations. A PSA is an sDTMC
by deﬁnition. Each state in a PSA is labeled by a ﬁnite memory of the system. The
transition function between the states are deﬁned based on the state labels such that
there is a transition s × σ → t iff l(t) is a sufﬁx of l(s) · σ, where l(s) is the string label
of s. A walk on the underlying graph of a PSA will always end in a state labeled by a
sufﬁx of the sequence. Given a system log Lp(t) at t, a unique state in the PSA can be
identiﬁed by matching the state label with the sufﬁxes of Lp(t). For example, · · · 010 is
in state labeled by 0 and if we observe 1 next, the system will go to state labeled by 01.

(cid:104)(cid:105)

(0.5, 0.5)

0:0.25

00

0

(0.5, 0.5)

1

(0.5, 0.5)

0:0.75

1:0.75

(0.25, 0.75)

00

10

(0.75, 0.25)

10

0:0.5

1:0.25

1:0.5

1

Fig. 3: An example stationary model. The left is the PST representation, where each
state is associated with a label and a distribution of the next observation. The right is
the corresponding PSA model where leaves are taken as states.

Algorithm 1: Learn P ST

1: Initialize T to be a single root node representing (cid:104)(cid:105);
2: Let S = {σ|f re(σ, α) > (cid:15)} be the candidate sufﬁx set;
3: while S is not empty do
4:
5:

Take any π from S; Let π(cid:48) be the longest sufﬁx of π in T ;
(B) If fre(π, α) · (cid:80)

Pr (π(cid:48),σ) ≥ (cid:15)
add π and all its sufﬁxes which are not in T to T ;

σ∈Σ Pr (π, σ) · log Pr (π,σ)

(C) If fre(π, α) > (cid:15), add (cid:104)e(cid:105) · π to S for every e ∈ Σ if fre((cid:104)e(cid:105) · π, α) > 0;

6:
7: end while

To learn a PSA, we ﬁrst construct an intermediate tree representation called Proba-
bilistic Sufﬁx Tree (PST), namely tree(L) = (N, root, E) where N is the set of sufﬁxes
of L; root = (cid:104)(cid:105); and there is an edge (π1, π2) ∈ E if and only if π2 = (cid:104)e(cid:105) · π1. Based
on different sufﬁxes of the execution, different probabilistic distributions of the next
observation will be formed. The central question is how deep should we grow the PST.
A deeper tree means that a longer memory is used to predict the distribution of the next
observation. The detailed algorithm is shown in Algorithm 1. The tree keeps growing
as long as adding children to a current leaf leads to a signiﬁcant change (measured by
K-L divergence) in the probability distribution of next observation (line 5). After we
obtain the PST, we transform it into a PSA by taking the leaves as states and deﬁne
transitions by sufﬁx matching. We brieﬂy introduce the transformation here and readers
are referred to Appendix B of [24] for more details. For a state s and next symbol σ, the
next state s(cid:48) must be a sufﬁx of sσ. However, this is not guaranteed to be a leaf in the
learned T . Thus, the ﬁrst step is to extend T to T (cid:48) such that for every leaf s, the longest
preﬁx of s is either a leaf or an internal node in T (cid:48). The transition functions are deﬁned
as follows. For each node s in T ∩ T (cid:48) and σ ∈ Σ, let P r(cid:48)(s, σ) = P r(s, σ). For each
new nodes s(cid:48) in T (cid:48) − T , let P r(cid:48)(s(cid:48), σ) = P r(s, σ), where s is deepest ancestor of s(cid:48) in
T . An example PST and its corresponding PSA after transformation is given in Fig. 3.
Readers are referred to [24] for details.

3.3 Veriﬁcation

Once we learn an sDTMC model, we then check whether the learned model satis-
ﬁes the given safety property. To do so, we ﬁrst compute the steady-state distribution
of the learned model. There are several methods we could use for the calculation in-
cluding power methods, solving equations or ﬁnding eigenvector [2]. The steady-state
distribution tells the probability that a state occurs in the long run. Once we obtain
the steady-state distribution of the learned model, we could then calculate the proba-
bility that the system violates the safety property in the long run by summing up the
steady-state probability of all unsafe states. Assume µ is the steady-state distribution,
Su is the set of unsafe states in the learned model and Pu is the probability that the
system is in unsafe states in the long run. We calculate the probability of unsafe states
as Pu = (cid:80)
µ{si}. We then check whether the learned model satisﬁes the safety
property by comparing whether Pu is beyond the safety threshold r. Take the PSA
model in Figure 3 as example. The steady-state distribution over states [1, 00, 10] is
[0.4, 0.31, 0.29]. States 1 is the unsafe state. The steady-state probability that the sys-
tem is in unsafe states is thus 0.4.

si∈Su

There are two kinds of results. One is that Pu is below the threshold r, which means
the learned model under current abstraction level satisﬁes the safety requirement. Then,
we draw the conclusion that the system is ‘safe’ and present the learned model as ev-
idence. The soundness of the result can be derived if the learned abstract model simu-
lates the actual underlying model [12]. However, since the model is obtained through
learning from limited data, it is not guaranteed that the result is sound. Nevertheless, the
model can be further investigated by validating it against future system logs or reviewed
by experts, which we leave to future works. The other result is that the learned model
does not satisfy the safety requirement, i.e., the probability of the system being in an
unsafe state in the steady-state is larger than the threshold. In such a case, we move
to the next step to validate whether the safety violation is introduced by inappropriate
abstraction [32] or not.

3.4 Abstraction reﬁnement

In case we learn a model which shows that the probability of the system being in unsafe
states in long term is beyond the safety threshold, we move on to validate whether
the system is indeed unsafe or the violation is spurious due to over-abstraction. For
spuriousness checking, we make use of a testing log which is obtained independently
and compute the probability of the system being in unsafe states, which is denoted
by P t
u. The testing log has the same format with the training log. We estimate P t
u by
calculating the frequency that the system is in some unsafe states in the testing log. If P t
u
is larger than the threshold r, we report the safety violation together with a conﬁdence
by calculating the error bound [26]. Otherwise, we conclude that the violation is caused
by too coarse abstraction and move to the next step to reﬁne the abstraction.

Let N be the total number of states, and n be the number of unsafe states in the
testing log. Let Y = X1 + X2 + · · · + XN , where Xi is a Bernoulli random variable on
whether a state is unsafe. The conﬁdence of the safety violation report is then calculated
as α = 1 − P{Y = n|Pu < r}. For example, for property LIT 101 > 1000, if we

Algorithm 2: Algorithm CountST (MP , Lt)

1: Augment each transition (si, sj) in MP with a number #(si, sj) recording how many

times we observe such a transition in Lt and initialize them to 0;

2: Let t0 be the ﬁrst time that suf f ix(Lt(t0)) matches a label of a state in MP and a time

pointer t = t0;
3: while t < T2 do
4:
5:
6:
7: end while

Refer to MP for the current state st;
Take Lt(t + 1) from Lt and refer to MP to get the next state st+1;
Add #(st, st+1) by 1, add t by 1;

observe 1009 times (n) that LIT 101 is larger than 1000 and the total length of the
testing log is 100000 (N), then the estimated P t

u is 1009/100000 = 0.01009.

If we conclude that the current abstraction is too coarse, we continue to reﬁne the
abstraction by generating a new predicate following the approach in [32]. The predicate
is then added to the set of predicates to obtain a new abstract system log based on the
new abstraction. The algorithm then starts over to learn a new model based on the new
abstract log. Next, we introduce how to generate a new predicate in our setting.

Finding spurious transitions A spurious transition in the learned model is a transition
whose probability is inﬂated due to the abstraction. Further, a transition (si, sj) is spu-
rious if the probability of observing si transiting to sj in the actual system PM(si, sj)
is actually smaller than PMP (si, sj) in the learned model [32]. Without the actual sys-
tem model, we estimate the actual transition probability based on the testing log. Given
the learned model MP and the testing log Lt, we count the number of times si is ob-
served in Lt (denoted by #si) and the number of times the transition from si to sj in
is observed Lt (denoted by #(si, sj)) using Alg. 2. The actual transition probability
P (si, sj) is estimated by (cid:98)PM(si, sj) = #(si, sj)/#si. Afterwards, we identify the
transitions satisfying PMP (si, sj) − (cid:98)PM(si, sj) > 0 as spurious transitions and order
them according to the probability deviation.

Predicate generation After we obtain a spurious transition (si, sj), our next step is
to generate a new predicate to eliminate the spuriousness. The generated predicate is
supposed to separate the concrete states of si which transit to sj (positive instances)
from those which do not (negative instances). We collect the dataset for classiﬁcation in
a similar way to Alg. 2 by iterating the testing log. If si is observed, we make a decision
on whether it is a positive or negative instance by telling whether its next state is sj.
With the labeled dataset, we then apply a supervised classiﬁcation technique in machine
learning, i.e., Support Vector Machines (SVM [5,1]) to generate a new predicate. Then,
we add the predicate for abstraction and start a new round.

3.5 Overall algorithm

The overall algorithm is shown as Alg. 3. The inputs of the algorithm are a system log L
for training, a system log Lt for testing, a property in the form of S≤r(ϕ). During each

Algorithm 3: Algorithm sLAR(L,Lt, S≤r(ϕ))
1 let P be the predicates in ϕ;
2 while true do
3

construct abstract trace LP based on training log L and P ;
apply Alg. 1 to learn a stationary model MP based on LP ;
check MP against ϕ;
if MP |= ϕ then

report ϕ is veriﬁed, the model MP ;
return;

use the testing log Lt to validate the property violation;
if validated then

report ϕ is violated with conﬁdence;
return;

identify the most spurious transitions (cid:104)s, s(cid:48)(cid:105) in MP ;
collect labeled dataset D+(s, MP , Lt) and D−(s, MP , Lt);
apply SVM to identify a predicate p separating the two sets;
add p into P ;

4

5

6

7

8

9

10

11

12

13

14

15

16

iteration of the loop from line 2 to 16, we start with constructing the abstract trace based
on L and a set of predicates P . The initial set of predicates for abstraction is the set of
predicates in the property. Next, an abstract sDTMC MP is learned using Algorithm 1.
We then verify MP against the property. If the property is veriﬁed, the system is veriﬁed
and MP is presented as the evidence. Otherwise, we validate the veriﬁcation result
using a testing log Lt at line 9. If the test passes, we report a safety violation together
with the conﬁdence. Otherwise, at line 13, we identify the most spurious transition and
obtain a new predicate at line 15. After adding the new predicate into P , we restart
the process from line 2. If SVM fails to ﬁnd a classiﬁer for all the spurious transitions,
Alg. 3 terminates and reports the veriﬁcation is unsuccessful. Otherwise, it either reports
true with a supporting model as evidence or a safety violation with conﬁdence.

4 Case study results

In the following, we present our ﬁndings on applying the method documented in Sec-
tion 3 to SWaT. Given the 11 day system log [10], we take the 7 day log under normal
system execution and further split it into two parts for training (4 days) and testing (3
days) respectively. The main reason we split them into training and testing log is to
avoid over-ﬁtting problem without the testing data. Note that the historian makes one
record every second. The training log and testing log contains 288000 and 208800 sys-
tem observations respectively. The properties we veriﬁed are whether the steady-state
probability that a sensor runs out of its operating range is beyond or below a threshold.
Let Ptrain, Plearn and Ptest be the probability that a sensor is out of operating range in
the training log, learned models and the testing log respectively. In our study, we set the
threshold r in each property as 20 percent larger than the probability observed in the

actual system for a long time, during which the system functioned reliably. The idea is
to check whether we can establish some underlying evidence to show that the system
would satisfy the property indeed.

The experiment results of all sensors are summarized in Table 3. The detailed im-
plementation and models are available in [30]. The ﬁrst column is the plant number.
Column 2 and 3 are the sensors and their properties to verify which are decided by their
operating ranges. The following 4 columns show the probability that a sensor value is
out of operating range in the training log, the safety threshold, the probability in the
learned model and the probability in the testing log respectively. Column ‘result’ is the
veriﬁcation result of the given safety properties. ‘SUC’ means the property is success-
fully veriﬁed. ‘FAL’ means the property is not veriﬁed. ‘VIO’ means the property is
violated. Column ‘model size’ is the number of states in the learned model. Column (cid:15)
is the parameter we use in the learning parameter. The last column is the running time.

Summary of results
In total, we managed to evaluate 47 safety properties of 24 sensors.
Notice that the sensor from P6 is missing in the dataset. Among them, 19 properties are
never observed to be violated in the training log. We thus could not learn any models
regarding these properties and conclude that the system is safe from the limited data we
learn from. This is reasonable as according to the dataset, the probability violating the
property is 0. For the rest 28 properties, we successfully veriﬁed 24 properties together
with a learned abstract Markov chain each and reported 4 properties as safety violation
with a conﬁdence.

We have the following observations from the results. For those properties we suc-
cessfully veriﬁed, we managed to learn stationary abstract Markov chains which closely
approximate the steady-state probability of safety violation (evaluated based on the
probability computed based on the testing log). It means that in these cases, sLAR is
able to learn a model that is precise enough to capture how the sensor values change.
Examples are F IT 101 > 2.6, LIT 301 > 1000, LIT 301 < 800 and LIT 401 > 1000.
Besides, it can be observed that the learned abstract models are reasonably small, i.e.,
usually with less than 100 states and many with only a few states. This is welcomed
since a smaller model is easier to comprehend and thus more meaningful for expert
review or to be used as a runtime monitor. An underlying reason (why a small model is
able to explain why a property is satisﬁed) is perhaps the system is built such that the
system modiﬁes its behavior way before a safety violation is possible. Besides, we iden-
tify two groups of states which are of special interest. One of them are F IT 401 < 1.5,
F IT 502 < 1.1, F IT 503 < 0.7 and F IT 504 < 0.25. The 4 properties have the
same probability 0.0117 of safety violation in the training log and 0 in the testing log.
We learn the same models for all of them and Plearn equals 0 which is the same as
the testing log. We could observe that these sensors have tight connections with each
other. Moreover, these sensors are good examples that our learned models generalize
from the training data and are able to capture the long run behaviors of the system with
Plearn equals Ptest, which is 0. The same goes for the other group of properties, i.e.,
F IT 501 < 1, P IT 501 < 20 and P IT 503 < 10.

For those properties we reported as safety violations, i.e., AIT 401 > 100, P IT 501 >

30, P IT 502 > 0.2 and P IT 503 > 20, a closer look reveals that these sensors all have
high probability of violation (either 0.7156 or 0.989) in the training log. Our learned

models report that the probability of violation in the long term is 1, which equals the
probability in the testing log in all cases. This shows that our learned models are precise
even though the properties are not actually satisﬁed.

Discussions
1) We give a 20% margin for the safety threshold in the above experi-
ments. In practice, the actual safety threshold could be derived from the system relia-
bility requirement. In our experiments, we observe that we could increase the threshold
to obtain a more abstract model and decrease the threshold to obtain a more detailed
model. For instance, we would be more likely to verify a property with a loose thresh-
old. 2) The parameter (cid:15) in Algorithm 1 effectively controls the size of learned model. A
small (cid:15) used in the model learning algorithm leads to a learned model with more states
by growing a deeper tree. However, it is sometimes non-trivial to select a good (cid:15) [33].
In our experiment, we use 0.01 as the basic parameter. If we can not learn a model (the
tree does not grow), we may choose a more strict (cid:15). Examples are LIT 401 > 1000 and
AIT 504 > 10. This suggests one way to improve existing model learning algorithms.
3) Each sensor has a different collection period and most of them are changing very
slowly, thus the data is not all meaningful to us and we only take a data point from the
dataset every minute to reduce the learning cost. 4) One possible reason for the safety
violation cases is that the system has not exhibited stationary behaviors within 7 days
as the probability of safety violations is 1 in the testing data for all these cases.

Limitation and future work Model learning will correctly learn an underlying model
in the limit [18,24]. However, since our models are learned from a limited amount of
data from a practical point of view, they are not guaranteed to converge to the actual
underlying models. One of our future work is how to further validate and update the
learned models from more system logs. In general, it is a challenging and interesting
direction to derive a conﬁdence for the learned model (as a machine learning problem)
or the veriﬁcation results based on the learned models (as a model checking problem)
given speciﬁc training data. Or alternatively, how can we derive a requirement on the
training data to achieve a certain conﬁdence. Some preliminary results on the number
of samples required to achieve an error bound are discussed in [13].

5 Conclusion and related work

In this work, we conducted a case study to automatically model and verify a real-world
water treatment system testbed. Given a set of safety properties of the system, we com-
bine model learning and abstraction reﬁnement to learn a model which 1) describes
how the system would evolve in the long run and 2) veriﬁes or falsiﬁes the properties.
The learned models could also be used for further investigation or other system analysis
tasks such as probabilistic model checking, simulation or runtime monitoring.

This work is inspired by the recent trend on adopting machine learning to automati-
cally learn models for model checking. Various kinds of model learning algorithms have
been investigated including continuous-time Markov Chain [25], DTMC [19,6,33,31,34]
and Markov Decision Process [18,3]. In particular, this case study is closely related to
the learning approach called LAR documented in [32], which combines model learning
and abstraction reﬁnement to automatically ﬁnd a proper level of abstraction to treat the

problem of real-typed variables. Our algorithm is a variant of LAR, which adapts it to
the setting of stationary probabilistic models [6].

This case study aims to formally and automatically analyze a real-world CPS by
modeling and verifying the physical environment probabilistically. There are several
related approaches for this goal. One popular way is to model the CPS as hybrid au-
tomata [11]. In [23], a theorem prover for hybrid systems is developed. dReach is an-
other tool to verify the δ-complete reachability analysis of hybrid system [9]. Never-
theless, they both require users to manually write a hybrid model using differential
dynamic logic, which is highly non-trivial. In [22], the authors propose to learn hy-
brid models from a sample of observations. In addition, HyChecker borrows the idea
of concolic testing to hybrid system based on a probabilistic abstraction of the hybrid
model and achieves faster detection of counterexamples [15]. sLAR is different as it
is fully automatic without relying on a user-provided model. SMC is another line of
work which does not require a model beforehand [7]. However, it requires sampling the
system many times. This is unrealistic for our setting since shutting down and restarting
SWaT yield signiﬁcant cost. Besides, SMC does not provide insight on how the system
works but only provides the veriﬁcation result. Our learned models however can be used
for other system analysis tasks.

Several case studies are related to our case study in some way. In [17], the authors
applied integrated simulation of the physical part and the cyber part to an intelligent
water distribution system. In [8], the authors use model learning to infer models of
different software components for TCP implementation and apply model checking to
explore the interaction of different components. In [14], a case study on self-driving car
is conducted for the analysis of parallel scheduling for CPS. In [21], automata learning
is applied in different levels of a smart grid system to improve the power management.
As far as we know, our work is the ﬁrst on applying probabilistic model learning for
verifying a real-world CPS probabilistically.

Acknowledgement

The work was supported in part by Singapore NRF Award No. NRF2014NCR-NCR001-
40, NSFC projects 61772347, 61502308, STFSC project JCYJ20170302153712968.

Plant Sensor Property Ptrain

r

Plearn Ptest Result Model Size

(cid:15)

Time

Table 3: Experiment results.

P1

FIT101 >2.6
<2.5

0.2371 0.2845 0.2371 0.233 SUC
0.5092 0.611 0.5092 0.5245 SUC
LIT101 >800 0.1279 0.1535 0.1271 0.1141 SUC
<500 0.1485 0.1782 0.147 0.0977 SUC

P2 AIT201 >260 0.6044 0.7253 0.647

AIT202

<250
>9
<6

0
0
0

–
–
–

–
–
–

AIT203 >500 0.0362 0.043 0.0363
<420 0.7654 0.9185 0.7654

1
–
–
–
0
1
–

SUC
–
–
–
SUC
SUC
–

FIT201 >2.5
<2.4

0

–
0.2577 0.3092 0.2567 0.2529 SUC

–

0

–

P3 DPIT301 >30
<10
FIT301 >2.4
<2.2

–
0.2006 0.2407 0.1991 0.1799 SUC
–
0.2217 0.266 0.2209 0.1756 SUC
LIT301 >1000 0.134 0.1608 0.135 0.1299 SUC
<800 0.0877 0.1052 0.0876 0.0609 SUC

–

–

–

–

–

0

P4 AIT401 >100 0.7156 0.8587
<5
0.2844 0.3413
AIT402 >250
<150
>2
<1.5

VIO
SUC
–
–
–
SUC
LIT401 >1000 0.0035 0.0042 0.0037 0.0034 SUC
<800 0.1227 0.1472 0.123 0.079 SUC

0.0117 0.014

1
1
–
–
–
0

1
0
–
–
–
0

FIT401

–
–
–

0
0
0

P5 AIT501

AIT503

>8
<6
AIT502 >250
<100
300
<200
AIT504 >10
<5
>2
<1
FIT502 >1.3
<1.1
FIT503 >0.9
<0.7
FIT504 >0.35

FIT501

–
–
–
–
–
–
1
–
–

–
–
–
–
–
–
1
–
–
0

0
0
0
0
0
0
0.9983
0
0

–
–
–
–
–
–
–
–
–
–
–
–
SUC
0.9983
–
–
–
–
0.011 0.0132
SUC
0
0.0356 0.0427 0.0361 0.3241 SUC
SUC
0
0.0117 0.014
–
–
SUC
0
–
–
SUC
0
VIO
1
SUC
0
VIO
1
VIO
1
SUC
0

0
–
0
–
0
1
0
1
1
0

0.0117 0.014

1
1

1

–

–

0

0
<0.25 0.0117 0.014
0.989
0.011 0.0132
0.989
0.989
0.011 0.0132

PIT501 >30
<20
PIT502 >0.2
PIT503 >20
<10

26
31
130
54

2
–
–
–
2
2
–
59

–
119
–
42
60
69

2
2
–
–
–
2
208
70

–
–
–
–
–
–
2
–
–
3
9
2
–
2
–
2
3
3
3
3
3

0.01 300
0.01 298
0.01
0.01

4
2

0.01
–
–
–
0.01
0.01
–
0.01

–
0.01
–
0.01
0.01
0.01

31
–
–
–
27
32
–
4

–
4
–
4
4
2

0.002 35
33
0.01
–
–
–
–
–
–
37
0.01
0.002 455
2
0.01

–
–
–
–
–
–

–
–
–
–
–
–
0.001 37
–
–
38
15
38
–
38
–
38
38
38
37
37
38

–
–
0.01
0.01
0.01
–
0.01
–
0.01
0.01
0.01
0.01
0.01
0.01

References

1. Thomas Abeel, Yves Van de Peer, and Yvan Saeys. Java-ml: A machine learning library.

The Journal of Machine Learning Research, 10:931–934, 2009.

2. Richard F Bass. Stochastic processes, volume 33. Cambridge University Press, 2011.
3. Tom´aˇs Br´azdil, Krishnendu Chatterjee, Martin Chmel´ık, Vojtˇech Forejt, Jan Kˇret´ınsk`y, Marta
Kwiatkowska, David Parker, and Mateusz Ujma. Veriﬁcation of markov decision processes
In Automated Technology for Veriﬁcation and Analysis, pages
using learning algorithms.
98–114. Springer, 2014.

4. Rafael C Carrasco and Jos´e Oncina. Learning stochastic regular grammars by means of a
state merging method. In International Colloquium on Grammatical Inference, pages 139–
152. Springer, 1994.

5. Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM

Transactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011.

6. Yingke Chen, Hua Mao, Manfred Jaeger, Thomas Dyhre Nielsen, Kim Guldstrand Larsen,
In NASA

and Brian Nielsen. Learning markov models for stationary system behaviors.
Formal Methods, pages 216–230. Springer, 2012.

7. Edmund M Clarke and Paolo Zuliani. Statistical model checking for cyber-physical systems.

In Automated Technology for Veriﬁcation and Analysis, pages 1–12. Springer, 2011.

8. Paul Fiter˘au-Bros¸tean, Ramon Janssen, and Frits Vaandrager. Combining model learning and
model checking to analyze tcp implementations. In International Conference on Computer
Aided Veriﬁcation, pages 454–471. Springer, 2016.

9. Sicun Gao, Soonho Kong, Wei Chen, and Edmund Clarke. Delta-complete analysis for

bounded reachability of hybrid systems. arXiv preprint arXiv:1404.7171, 2014.

10. Jonathan Goh, Sridhar Adepu, Khurum Nazir Junejo, and Aditya Mathur. A dataset to sup-
port research in the design of secure water treatment systems. In International Conference
on Critical Information Infrastructures Security, pages 88–99. Springer, 2016.

11. Thomas A Henzinger. The theory of hybrid automata. In Veriﬁcation of Digital and Hybrid

Systems, pages 265–292. Springer, 2000.

12. Holger Hermanns, Bj¨orn Wachter, and Lijun Zhang. Probabilistic cegar. In Computer Aided

Veriﬁcation, pages 162–175. Springer, 2008.

13. Cyrille Jegourel, Jun Sun, and Jin Song Dong. Sequential schemes for frequentist estimation
In International Conference on Quantitative

of properties in statistical model checking.
Evaluation of Systems, pages 333–350. Springer, 2017.

14. Junsung Kim, Hyoseung Kim, Karthik Lakshmanan, and Ragunathan Raj Rajkumar. Parallel
scheduling for cyber-physical systems: Analysis and case study on a self-driving car.
In
Proceedings of the ACM/IEEE 4th International Conference on Cyber-Physical Systems,
pages 31–40. ACM, 2013.

15. Pingfan Kong, Yi Li, Xiaohong Chen, Jun Sun, Meng Sun, and Jingyi Wang. Towards con-
colic testing for hybrid systems. In FM 2016: Formal Methods: 21st International Sympo-
sium, Limassol, Cyprus, November 9-11, 2016, Proceedings 21, pages 460–478. Springer,
2016.

16. Axel Legay, Benoˆıt Delahaye, and Saddek Bensalem. Statistical model checking: An

overview. RV, 10:122–135, 2010.

17. Jing Lin, Sahra Sedigh, and Ann Miller. Towards integrated simulation of cyber-physical
systems: a case study on intelligent water distribution. In Dependable, Autonomic and Se-
cure Computing, 2009. DASC’09. Eighth IEEE International Conference on, pages 690–695.
IEEE, 2009.

18. Hua Mao, Yingke Chen, Manfred Jaeger, Thomas D Nielsen, Kim G Larsen, and Brian
arXiv preprint
Learning markov decision processes for model checking.

Nielsen.
arXiv:1212.3873, 2012.

19. Hua Mao, Yingke Chen, Manfred Jaeger, Thomas D Nielsen, Kim G Larsen, and Brian
Nielsen. Learning deterministic probabilistic automata from a model checking perspective.
Machine Learning, 105(2):255–299, 2016.

20. Aditya P Mathur and Nils Ole Tippenhauer. Swat: A water treatment testbed for research and
training on ics security. In Cyber-physical Systems for Smart Water Networks (CySWater),
2016 International Workshop on, pages 31–36. IEEE, 2016.

21. Sudip Misra, P Venkata Krishna, Vankadara Saritha, and Mohammad S Obaidat. Learning
automata as a utility for power management in smart grids. IEEE Communications Maga-
zine, 51(1):98–104, 2013.

22. Oliver Niggemann, Benno Stein, Asmir Vodencarevic, Alexander Maier, and Hans Kleine
In AAAI, volume 2, pages

B¨uning. Learning behavior models for hybrid timed systems.
1083–1090, 2012.

23. Andr´e Platzer. Logical analysis of hybrid systems: proving theorems for complex dynamics.

Springer Science & Business Media, 2010.

24. Dana Ron, Yoram Singer, and Naftali Tishby. The power of amnesia: Learning probabilistic

automata with variable memory length. Machine learning, 25(2-3):117–149, 1996.

25. Koushik Sen, Mahesh Viswanathan, and Gul Agha. Learning continuous time markov chains
from sample executions. In Quantitative Evaluation of Systems, 2004. QEST 2004. Proceed-
ings. First International Conference on the, pages 146–155. IEEE, 2004.

26. Koushik Sen, Mahesh Viswanathan, and Gul Agha. Statistical model checking of black-box

probabilistic systems. In CAV, volume 3114, pages 202–215. Springer, 2004.
27. SUTD. Swat dataset website. https://itrust.sutd.edu.sg/dataset/.
28. SUTD. Swat website. http://itrust.sutd.edu.sg/research/testbeds/

secure-water-treatment-swat/.

29. Bjorn Wachter, Lijun Zhang, and Holger Hermanns. Probabilistic model checking modulo
In Fourth International Conference on the Quantitative Evaluation of Systems,

theories.
pages 129–140. IEEE, 2007.

30. Jingyi Wang. Ziqian website. https://github.com/wang-jingyi/Ziqian.
31. Jingyi Wang, Xiaohong Chen, Jun Sun, and Shengchao Qin.

Improving probability esti-
mation through active probabilistic model learning. In International Conference on Formal
Engineering Methods, pages 379–395. Springer, 2017.

32. Jingyi Wang, Jun Sun, and Shengchao Qin. Verifying complex systems probabilistically

through learning, abstraction and reﬁnement. CoRR, abs/1610.06371, 2016.

33. Jingyi Wang, Jun Sun, Qixia Yuan, and Jun Pang. Should we learn probabilistic models for
model checking? A new approach and an empirical study. In International Conference on
Fundamental Approaches to Software Engineering, pages 3–21. Springer, 2017.

34. Jingyi Wang, Jun Sun, Qixia Yuan, and Jun Pang. Learning probabilistic models for model
checking: an evolutionary approach and an empirical study. International Journal on Soft-
ware Tools for Technology Transfer, pages 1–16, 2018.

35. H.L.S Younes. Veriﬁcation and Planning for Stochastic Processes with Asynchronous Events.

PhD thesis, Carnegie Mellon, 2005.

