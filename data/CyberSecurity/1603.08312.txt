6
1
0
2

r
a

M
8
2

]

R
C
.
s
c
[

1
v
2
1
3
8
0
.
3
0
6
1
:
v
i
X
r
a

Optimizing Active Cyber Defense

Wenlian Lu1,2, Shouhuai Xu3, and Xinlei Yi1

1 School of Mathematical Sciences, Fudan University
Shanghai, P. R. China, 200433
Emails: {wenlian,11210180008}@fudan.edu.cn
2 Department of Computer Science, University of Warwick
Coventry CV4 7AL, UK
3 Department of Computer Science, University of Texas at San Antonio
San Antonio, Texas 78249, USA
Email: shxu@cs.utsa.edu

Abstract. Active cyber defense is one important defensive method for
combating cyber attacks. Unlike traditional defensive methods such as
ﬁrewall-based ﬁltering and anti-malware tools, active cyber defense is
based on spreading “white” or “benign” worms to combat against the
attackers’ malwares (i.e., malicious worms) that also spread over the
network. In this paper, we initiate the study of optimal active cyber
defense in the setting of strategic attackers and/or strategic defenders.
Speciﬁcally, we investigate inﬁnite-time horizon optimal control and fast
optimal control for strategic defenders (who want to minimize their cost)
against non-strategic attackers (who do not consider the issue of cost).
We also investigate the Nash equilibria for strategic defenders and attack-
ers. We discuss the cyber security meanings/implications of the theoretic
results. Our study brings interesting open problems for future research.
Keywords: cyber security model, active cyber defense, optimization,
epidemic model

1

Introduction

The importance of cyber security is well recognized now. However, our under-
standing of cyber security is still at its infant stage. In general, the attackers are
constantly escalating their attack power and sophistication, while the defenders
largely lag behind. To be speciﬁc, we mention the following asymmetry between
cyber attack and cyber defense: The eﬀect of malware-like attacks is automati-
cally ampliﬁed by the network connectivity, while the defense eﬀect is not. This
phenomenon had been implied by many previous results (e.g., [28, 9, 6, 26, 34]),
but was not explicitly pointed out until very recently [35]. The asymmetry is fun-
damentally caused by that the defense is reactive, including intrusion detection
systems, ﬁrewalls and anti-malware tools. The asymmetry can be eliminated by
the idea of active cyber defense [35], where the defender also aims to take ad-
vantage of the network connectivity. The concept of active cyber defense is not
completely new because researchers have proposed for years the idea of using the
defender’s “white” or “benign” worms to combat against the attackers’ malwares

 
 
 
 
 
 
[5, 1, 29, 23, 16, 18, 13, 30]. In a sense, active cyber defense already happened in
practice; for example, the Welchia worm attempted to “kill” the Blaster worm in
compromised computers [23, 20]. It appears that full-ﬂedged active cyber defense
is perhaps inevitable in the near future according to some recent reports [18, 24,
31]. It is therefore more imperative than ever to systematically characterize the
eﬀectiveness of active cyber defense. This motivates the present study.

1.1 Our Contributions

This paper is inspired by the recent mathematical model of active cyber de-
fense dynamics [35], which characterizes the eﬀect of various model parameters
(including the underlying complex network structures) in the setting where nei-
ther the attacker nor the defender is strategic (i.e., both the attacker and the
defender do not consider the issue of cost). Here we study a new perspective of
active cyber defense, namely the strategic interaction between the attacker and
the defender. On one hand, our study moves a step beyond [35] because we in-
corporate control-theoretic and game-theoretic models to accommodate strategic
interactions. On the other hand, our study assumes away the underlying complex
network structures that are explicitly investigated in [35]. This means that our
study is essentially based on the homogeneous (or well-mixed) assumption that
each compromised computer can attack the same portion of computers. Tackling
the problem of strategic attack-defense interactions with explicit complex net-
work structures is left for future research. Therefore, we deem the present paper
as a signiﬁcant ﬁrst step toward ultimately understanding the eﬀectiveness of
strategic active cyber defense. Speciﬁcally, we make the following contributions.
First, we investigate two ﬂavors of optimal control for strategic defenders
against non-strategic attackers: inﬁnite-time horizon optimal control and fast
optimal control. In the setting of inﬁnite-time horizon optimal control for the
defender, we characterize the conditions under which the defender should adjust
its active cyber defense power in a certain quantitative fashion. For example, we
identify a condition under which the defender should give up using active cyber
defense alone, and instead should resort to other defense methods as well (e.g.,
proactive defense). In the setting of fast optimal control, where the defender
wants to occupy a certain portion of the network as soon as possible and at the
minimal cost, there is a signiﬁcant diﬀerence between the case that the active
defense cost is linear and the case that the active defense cost is quadratic.

Second, we identify the Nash equilibrium strategies when both the defender
and the attacker are strategic. The ﬁndings are interesting. For example, when
the defender (or attacker) is reluctant to use/expose its advanced active cyber
defense tools (or zero-day exploits), it will give up escalating its active defense
(or attack) power; otherwise, there are three scenarios: (i) If the defender (or
attacker) initially occupies only a certain small portion of the network, it will
give up escalating its active defense (or attack). (ii) If the defender (or attacker)
initially occupies a certain signiﬁcant portion of the network, it will escalate its
active defense (or attack) as much as possible. (iii) If the defender (or attacker)

initially occupies a certain large portion of the network, it will not escalate its
active defense (or attack) — a sort of diminishing returns.

The rest of the paper is structured as follows. Section 2 brieﬂy reviews the
related prior work. Section 3 describes the basic active cyber defense model under
the homogeneous assumption. Section 4 investigates optimal control for strategic
defenders against non-strategic attackers. Section 5 studies Nash equilibria for
strategic defenders and attackers. Section 6 concludes the paper with some open
problems. Lengthy proofs are deferred to the Appendix. The main notations
used in the paper are listed below:

αB, αR defender B’s defense power αB and attacker R’s attack power αR
iB(t), iR(t) portions of the nodes occupied respectively by the defender and the

attacker at time t, where iB(t) + iR(t) = 1
πB, πB(t) πB is control variable and πB(t) is control function

ˆπB solution in the inﬁnite-time horizon optimal control case
B solutions in the case of

fast optimal control with linear and

B, π∗∗
π∗

quadratic cost functions, respectively

z discount rate

kB normalization ratio between the defender’s detection cost and re-

covery cost

λ normalization ratio between the unit of time and the defender’s

active defense cost

kR normalization ratio between the attacker’s maintenance cost and

penetration cost

2 Related Work

Our investigation is built on recent studies in mathematical computer malware
models. These models originated in the mathematical biological epidemic models
introduced in the 1920’s [19, 12], which were ﬁrst adapted to study the spread-
ing of computer virus in the 1990’s [10, 11]. All these models made the homo-
geneous assumption that each individual (e.g., computer) in the population has
equally infection eﬀect on the other individuals in the population, and the as-
sumption that the infected individuals recover because of reactive defense (e.g.,
anti-malware tools). In the past decade, there were many studies that aim to
eliminate the aforementioned homogeneous assumption, by explicitly incorporat-
ing the heterogeneous network structures [28, 9, 6, 26, 34, 32]. The mathematical
tools used for these studies are Dynamical Systems in nature. These studies
demonstrated that the attack eﬀect of malware spreading against reactive de-
fense is automatically ampliﬁed by the largest eigenvalue of the adjacency ma-
trix, which represents the underlying complex network structure. This is the
attack-defense asymmetry phenomenon mentioned above.

The attack-defense asymmetry phenomenon motivated the study of mathe-
matical models of active cyber defense [35], which is a relatively new sub-ﬁeld
in cyber security [18, 24, 31] as previous explorations were mainly geared toward

legal and policy issues [5, 1, 29, 23, 16, 18, 13, 30]. One real-life incident of the ﬂa-
vor of active cyber defense is that the Welchia worm attempted to “kick out”
another kind of worms (e.g., the Blaster worm) [23, 20]. In the ﬁrst mathemat-
ical characterization of active cyber defense [35], neither the attacker nor the
defender is strategic (i.e., they do not consider the issue of cost), albeit the
model accommodates the underlying complex network structure. In the present
paper, we move a step toward ultimately understanding optimal active cyber
defense, where the attacker and/or the defender are/is strategic (i.e., they want
to minimize their cost). Finally, we note that automatic patching [27] is not ac-
tive cyber defense because automatic patching aims to prevent attacks, whereas
active cyber defense aims to identify and possibly clean up infected computers.
There have been many studies (e.g., [33, 21, 8, 4, 14, 22, 15, 25]) on applying
Control Theory and Game Theory to understand various issues related to com-
puter malware spreading. Our study is somewhat inspired by the botnet-defense
model investigated in [4]. All the studies mentioned above only considered reac-
tive defense; whereas we investigate how to optimize active cyber defense. For
general information about the applications of Control Theory and Game Theory
to cyber security, we refer to [2, 17] and the references therein.

3 The Basic Active Cyber Defense Model

Consider a population of nodes, which can abstract computers in a cyber system.
At any point in time, a node is either occupied by defender B (i.e., the node is
secure), or occupied by attacker R (i.e., the node is compromised). Denote by
iB(t) the portion of nodes that are occupied by the defender at time t, and by
iR(t) the portion of nodes that are occupied by the attacker at time t, where
iB(t)+iR(t) = 1 for any t ≥ 0. In the interaction between cyber attack and active
cyber defense, the defender and the attacker can “grab” nodes from each other
in the same fashion. Let αB abstract defender B’s power in grabbing attacker-
occupied nodes using active cyber defense, and αR abstract attacker R’s power in
compromising defender-occupied nodes using malware-like cyber attacks. Under
the homogeneous assumption that (i) each secure node has the same power in
“grabbing” the attacker-occupied nodes and (ii) each compromised node has
the same power in compromising the defender-occupied nodes, we obtain the
following Dynamical System model:

diB (t)

dt = αBiB(t)iR(t) − αRiR(t)iB(t)

(

diR(t)

dt = αRiR(t)iB(t) − αBiB(t)iR(t),

where iB(t) + iR(t) = 1, iB(t) ≥ 0, and iR(t) ≥ 0 for all t ≥ 0. Due to the
symmetry, we only need to consider

diB(t)
dt

= αBiB(t)(1 − iB(t)) − αRiB(t)(1 − iB(t)).

(1)

If neither the attacker nor the defender is strategic (i.e., they do not consider

the issue of cost), the dynamics of system (1) can be characterized as follows.

– If the attacker is more powerful than the defender, namely αR > αB, the
attacker will occupy the entire network in the fashion of the Logistic equation
(i.e., when iR is small, iR increases slowly; when iR is around a threshold
value, iR increases exponentially; when iR is large, iR increases slowly).
– If the defender is more powerful than the attacker, namely αB > αR, the
defender will occupy the network in the same fashion as in the above case.
– If the attacker and the defender are equally powerful, namely αR = αB, the
system state is in equilibrium. In other words, iB(t) = iB(0) and iR(t) =
iR(0) = 1 − iB(0) for any t > 0.

The above model accommodates non-strategic attackers and non-strategic de-
fenders, and is the starting point for our study of optimal active cyber defense.

4 Optimal Control for Strategic Defender Against

Non-Strategic Attacker

4.1 Inﬁnite-time Horizon Optimal Control

In this setting, the non-strategic attacker R maintains a ﬁxed degree of attack
power αR, while the defender B is strategic. That is, the strategic defender aims
to minimize its cost (speciﬁed below) by adjusting its defense power αB via

αB = b + πB(a − b),

while obeying the dynamics of (1), where πB ∈ [0, 1] is the control variable and
αB ∈ [b, a] is the defender’s defense power with 1 ≥ a > b ≥ 0. The cost to the
defender consists of two parts.

– The recovery cost for recovering the compromised nodes to secure states
(e.g., re-installing the operating systems and updating the backup data ﬁles,
interference with the computers’ routine functions). We represent this cost by
fB(iB(t)) for some real-valued function fB(·). We assume f ′
B(·) < 0 because
the more nodes the defender occupies, the lower the cost for the defender to
recover the compromised nodes.

– The detection cost for detecting (or recognizing) compromised nodes via
active cyber defense, which partly depends on the attack’s evasiveness. We
represent this cost by kB ·πB(·), where kB is the normalization ratio between
the detection cost and the recovery cost, and πB(·) is the control function
that speciﬁes the adjustable degree of active cyber defense power. This is
plausible because using more powerful active defense mechanisms (e.g., more
sophisticated/advanced “white” worms) causes a higher cost but allows the
defender to ﬁght against the attacks more eﬀectively.

The above deﬁnition of cost accommodates at least the following family of ac-
tive cyber defense: The defender uses “white” worms to detect the compromised
nodes, then possibly manually recovers the compromised nodes. This is perhaps
the most probable scenario because for example, the attacker’s malware may

have corrupted or deleted some data ﬁles in the compromised computers. Note
that the detection cost highlights the diﬀerence between (i) active-cyber-defense
based detection, where the defender’s detection tools (i.e., “white” worms) do
not reside on the compromised computers, and (ii) reactive-cyber-defense based
detection such as the current generation of anti-virus software, where the detec-
tion tools do not spread over the network.

Assuming that the attacker maintains a ﬁxed degree of attack power αR,
the defender’s optimization goal is to minimize the total cost with a constant
discount rate z over an inﬁnite-time horizon, namely

inf
0≤πB (·)≤1

JB(πB(·)) =

(cid:26)

∞

0
Z

e−zt(fB(iB(t)) + kB · πB(t))dt

,

(2)

(cid:27)

where f ′
B(·) < 0, πB(·) ∈ [0, 1], and the attacker’s ﬁxed degree of attack power αR
is treated as a constant. Now the optimization problem reduces to identifying
the optimal defense strategy ˆπB. To solve the minimization problem, we use
Pontryagin’s Minimum Principle to ﬁnd the Hamiltonian associated to (2):

HB(iB, πB, p)

= fB(iB) + kBπB + p[αBiB(1 − iB) − αRiB(1 − iB)]
= (kB + piB(1 − iB)(a − b))πB + fB(iB) + pbiB(1 − iB) − pαRiB(1 − iB),(3)

where p is the adjoint equation

= − ∂HB
∂iB

˙p
p1(∞) = 0.

(cid:26)

+ zp = −f ′

B(iB) + p[z − (αB − αR)(1 − 2iB)]

(4)

The optimal strategy ˆπB is obtained by minimizing the Hamiltonian HB(iB, πB, p).
Since HB(iB, πB, p) is linear in πB, the optimal control strategy ˆπB takes the
following bang-bang control form:

if ∂HB
1
∂πB
uB (0 < uB < 1, to be determined) if ∂HB
∂πB
if ∂HB
∂πB

0

< 0

= 0

> 0

ˆπB = 


(5)



where ∂HB
∂πB
period of time, we have

= kB + piB(1 − iB)(a − b). In the singular form ∂HB
∂πB

p =

−kB
iB(1 − iB)(a − b)

.

= 0 and for a

(6)

Further diﬀerentiating ∂HB
∂πB

with respect to t, we have

d
dt

∂HB
∂πB (cid:19)

(cid:18)

= ˙piB(1 − iB)(a − b) + p(1 − 2iB)˙iB(a − b)

= iB(1 − iB)(a − b)

− f ′

B(iB) + p[z − (αB − αR)(1 − 2iB)]

(cid:26)
+p(1 − 2iB)(a − b)

αBiB(1 − iB) − αRiB(1 − iB)

(cid:27)

(cid:26)
= −iB(1 − iB)(a − b)f ′
B(iB) − kBz

(cid:27)

Deﬁne FB(iB) = −iB(1 − iB)(a − b)f ′
roots of FB(·) = 0.

B(iB) − kBz. Then we need to study the

)
b
−
a
(
)

i

−
1
(

i

=
y

B

B

0.25

0.2

0.15

0.1

0.05

0
0

y=k

z
B

i
1
0.2

0.4

i
B

0.6

i
2

0.8

1

Fig. 1. Illustration of the roots of FB(iB) = 0 with fB(iB) = 1−iB, a−b = 1 and kBz =
1/8, where the x-axis represents iB and the y-axis represents y(iB) = iB(1 − iB )(a − b).
The arrows indicate the directions the outcome under optimal control will head for.

Before presenting the results, we discuss the ideas behind them. In this paper,
we focus on case fB(iB) = 1 − iB, which can be easily extended to any linear
recovery-cost function. If kBz < 1

4 (a − b), then FB(iB) = 0 has two roots:

i1 =

1 −

1 − 4 kB z
a−b

q

2

and i2 =

1 +

1 − 4 kB z
a−b

q

2

with 0 < i1 < i2 < 1. As illustrated in Figure 1, this implies

FB(iB) < 0
FB(iB) > 0
FB(iB) < 0




if iB < i1
if i1 < iB < i2
if iB > i2.

Then, the optimal strategy ˆπB of the singular form can be obtained by solving
˙iB |iB =i1 or iB =i2 = 0.



Theorem 1. Suppose the non-strategic attacker maintains a ﬁxed degree of at-
tack power αR, fB(iB) = 1 − iB and kBz < 1
4 (a − b). Let i1 < i2 be the roots of
FB(iB) = 0. Let uB = αR−b

a−b . The optimal control strategy for defender B is:

0
uB
1
uB
0

if iB < i1
if iB = i1
if i1 < iB < i2
if iB = i2
if iB > i2

.

(7)

ˆπB =






Proof of Theorem 1 is deferred to Appendix A. In practice, i1 and i2 can
be obtained numerically. Theorem 1 (also as illustrated in Figure 1) shows that
the outcome of the inﬁnite-time horizon optimal control, namely limt→∞ iB(t),
depends on the initial system state iB(0) as follows:

– If 1 > iB(0) > i2, the defender should use the least powerful/costly active
defense mechanisms (i.e., αB = b) because ˆπB = 0. Moreover, the outcome
of the optimal defense is that the defender will occupy i2 portion of the
network, namely limt→∞ iB(t) = i2. This suggests a sort of diminishing
returns in active cyber defense: It is more cost-eﬀective to pursue “good
enough” security (i.e., limt→∞ iB(t) = i2 < 1) than to pursue “perfect”
security (i.e., limt→∞ iB(t) = 1) even if it is possible.

– If 0 = iB(0) < i1, the defender should use the least powerful/costly active
defense mechanisms (i.e., αB = b) because ˆπB = 0. Moreover, the outcome
of the optimal defense is that the defender should give up (using active cyber
defense as the only defense methods), as the attacker will occupy the entire
network, namely limt→∞ iB(t) = 0. In other words, the defender should
resort to other defense methods as well (e.g., proactive defense).

– If iB(0) ∈ (i1, i2), the defender should use the most powerful/costly active
defense mechanisms (i.e., αB = a) because ˆπB = 1. Moreover, the outcome
of the optimal defense is that the defender will occupy i2 portion of the
network, namely limt→∞ iB(t) = i2. This also suggests a sort of diminishing
returns mentioned above.

– If iB(0) = i1 or iB(0) = i2, the defender should adjust its deployment
of active cyber defense mechanisms according to uB = αR−b
a−b , which means
αB = αR. Moreover, the outcome of the optimal defense is that iB(t) = iB(0)
for all t > 0.

Now we consider the degenerated scenarios of kBz ≥ 1/4(a − b). The proof
is similar to, but much simpler than, the proof of Theorem 1, and thus omitted.

Theorem 2. Suppose the non-strategic attacker maintains a ﬁxed degree of at-
tack power αR and fB(iB) = 1 − iB.

– If kBz = 1/4(a − b), then FB(iB) = 0 has only one root, i1 = i2 = 1

2 . The

optimal control strategy is

0
uB = αR−b
a−b
0

if iB < i1
if iB = i1
if iB > i1.

ˆπB =






(8)

– If kBz > 1/4(a − b), then FB(iB) = 0 has no root. The optimal control

strategy is ˆπB = 0.

The cyber security implications of Theorem 2 are the following. In the case
4 (a − b), the outcome under the optimal control depends on the initial

kBz = 1
system state as follows:

– If 1 > iB(0) > i1, the defender should use the least powerful/costly active
cyber defense mechanisms because ˆπB = 0. The outcome is that the defender
will occupy i1 portion of the network, namely limt→∞ iB(t) = i1.

– If 0 = iB(0) < i1, the defender should use the least powerful/costly active
cyber defense mechanisms because ˆπB = 0. The outcome is that the defender
will give up using active cyber defense alone, as the attacker will occupy the
entire the network, namely limt→∞ iB(t) = 0. In other words, the defender
should resort to other defense methods as well (e.g., proactive defense).
– If iB(0) = i1, the defender will adjust its degree of active cyber defense power
a−b , which means αB = αR. The outcome is that

according to ˆπB = uB = αR−b
iB(t) = iB(0) for all t > 0.

In the case kBz > 1/4(a−b), the defender should use the least powerful/costly ac-
tive cyber defense mechanisms because ˆπB = 0. The outcome is that limt→∞ iB(t) =
0, meaning that the defender should give up using active cyber defense alone and
resort to other defense methods as well (e.g., proactive defense).

By considering Theorems 1 and 2 together, we draw some deeper insights.
Speciﬁcally, for a given z, diﬀerent kB’s suggest diﬀerent optimal active defense
strategies. More speciﬁcally, if kB > 1
4z (a − b), meaning that the cost of optimal
control is dominating, then defender B should use the least powerful/costly
active cyber defense mechanisms because ˆπB(t) = 0 for all t and the outcome is
limt→∞ iB = 0. In other words, the defender should give up using active cyber
defense alone, and resort to other kinds of defense methods as well (e.g., proactive
defense). If kB < 1
4z (a − b), meaning that the cost of control is not dominating,
the defender should enforce optimal control according to the initial state iB(0).
In particular, if kB = 0, meaning that the special case that the cost of control
is not counted, defender B should use the most powerful/costly active defense
mechanisms as ˆπB(t) = 1 for all t, and the outcome is that limt→∞ iB = 1,
namely that the defender will occupy the entire network.

4.2 Fast Optimal Control for Strategic Defenders against

Non-Strategic Attackers

Now we consider fast optimal control for strategic defenders against non-strategic
attackers, as motivated by the following question: Suppose the attacker maintains
a ﬁxed degree of attack power αR and the defender initially occupies iB(0) =
i0 < ie portions of the nodes, how can the defender use optimal control to
occupy the desired ie portions of the nodes as soon as possible? More precisely,
the optimization is to minimize the sum of active defense cost and time (after

appropriate normalization), which can be described by the following functional:

JF (πB(·)) = T + λ

h(πB(t))dt

T

0
Z

where h(·) is the cost function with respect to the control function πB(·). We
consider two scenarios of cost functions: linear and quadratic. In both scenarios,
we need to identify defender B’s optimal strategy with respect to the dynamics
of (1) and a given objective ie > i0 for some hitting time T that is to be identiﬁed.
Scenario I: Fast optimal control with linear cost functions. In this sce-
nario, we have h(πB) = πB. The optimization task is to minimize the active
defense cost plus the time T :

inf
0≤πB(·)≤1 (

JF (πB(·)) = T + λ

πB(t)dt

T

0
Z

)

(9)

diB (t)

dt = αBiB(t)(1 − iB(t)) − αRiB(t)(1 − iB(t))

subject to




iB(0) = i0
iB(T ) = ie



where λ > 0 is the normalization ratio between the unit of time and the active
T
0 πB(t)dt, and i0 < ie. That is, λ, i0 and ie are given, but T is
defense cost
T
free. Note that the active defense cost
0 πB(t)dt includes both detection and
R
recovery cost, where πB(t) is the control function.

R

Theorem 3. The solution to the fast optimal control problem (9) is

(π∗

B, T ∗) = (1, T1),

(10)

where T1 = 1

a−αR

ln

ie
1−ie

1−i0
i0

.

(cid:16)

(cid:17)

Proof of Theorem 3 is deferred to Appendix B. The cyber security implication
of Theorem 3 is the following. In order to achieve fast optimal control, the
defender should use the most powerful/costly active cyber defense mechanisms,
namely πB(t) = 1 for t < T ∗, until the system state becomes iB(T ∗) = ie at time
T ∗. After time T ∗, if the defender continues enforcing πB(t) = 1 for t > T ∗, then
limt→∞ iB(t) = 1, meaning that the defender will occupy the entire network.
Scenario II: Fast optimal control with quadratic cost functions. In this
scenario, we have h(πB) = π2
B. The optimization task is to minimize the following
sum of active defense cost and time, which diﬀers from the linear cost (9) in that
the cost function πB is replaced with cost function π2
B:

inf
0≤πB(·)≤1 (

JF (πB(·)) = T + λ

T

0
Z

π2
B(t)dt

)

(11)

dt = αBiB(t)(1 − iB(t)) − αRiB(t)(1 − iB(t))

diB (t)

iB(0) = i0
iB(T ) = ie

subject to






T

where λ > 0 is the ratio between the unit of time and the active defense cost
0 π2
B(t)dt (including both recovery cost and detection cost), and i0 < ie. That
is, λ, i0 and ie are given, but T is free.
R
Theorem 4. The solution to the fast optimal control problem (11) is

(π∗∗

B , T ∗∗) =

(u∗, T2), if λ ≥ a−b
(1, T3), otherwise

a+b−2αR

(

and a − b > 2(αR − b),

(12)

where

u∗ =

T2 =

T3 =

αR − b
a − b

+

2

b − αR
a − b

r(cid:16)
1
b + (a − b)u∗ − αR

(cid:17)
ln

,

+

1
λ
ie
1 − ie

1
a − αR

ln

ie
1 − ie

(cid:18)

(cid:18)
1 − i0

.

i0 (cid:19)

1 − i0

i0 (cid:19)

,

Proof of Theorem 4 is deferred to Appendix C. It cyber security implication is:
Unlike in the setting of linear cost function (Theorem 3), the defender should not
necessarily enforce the most powerful/costly active cyber defense mechanisms as
π∗∗
B is not always equal to 1. If the defender continues enforcing πB(t) = 1 for
t > T ∗∗ after the system reaches state iB(T ∗∗) = ie at time T ∗∗, the defender
will occupy the entire network, namely limt→∞ iB(t) = 1.

5 Nash Equilibria for Strategic Attacker and Defender

Now we ask the question: What if the attacker is also strategic? Analogous to the
way of modeling strategic defenders, we assume αR ∈ [b, a]. (It is straightforward
to extend the current setting αB, αR ∈ [b, a] to the setting αB ∈ [bB, aB] and
αR ∈ [bR, aR].) A strategic attacker can adjust its attack power

αR = b + πR(a − b),

via control variable πR(·) ∈ [0, 1]. That is, the attacker can launch more sophis-
ticated attacks (i.e., greater πR leading to greater αR), which however incurs
higher cost (e.g., the investment for obtaining more powerful attack tools).

Since both the defender and the attacker are strategic, we naturally consider

a game-theoretic model. Speciﬁcally, the defender B’s optimization task is

φB(iB) =

inf
0≤πB (·)≤1

0
(cid:26)
Z
and the attacker R’s optimization task is

JB(πB(·), πR(·)) =

∞

e−zt(fB(iB(t)) + kB · πB(iB(t)))dt

,

(cid:27)

φR(iB) =

inf
0≤πR(·)≤1

JR(πB(·), πR(·)) =

(cid:26)

∞

0
Z

e−zt(fR(iB(t)) + kR · πR(iB(t)))dt

,

(cid:27)

where πB(·), πR(·) ∈ [0, 1], f ′
B(·) < 0 (as in the inﬁnite-time horizon optimal
control case investigated above), f ′
R(·) > 0 because fR(iB(t)) represents the
maintenance cost to the attacker, kR is the normalization ratio between the at-
tacker’s maintenance cost and penetration cost (which depends on the capability
of the attack tools), and kR · πR(·) is the penetration cost. Note that f ′
R(·) > 0
is relevant because the attacker may need to conduct some costly (or risky)
activities after “grabbing” a node from the defender (e.g., downloading attack
payloads from some remote server, while this downloading operation may in-
crease the chance that the compromised node is detected by active defense).
Since f ′
R(·) > 0 implies dfR/diR < 0, the attacker’s optimization task for πR is
in parallel to the optimization for πB. The Hamiltonians associated to defender
B’s and attacker R’s optimization problems are:

HB(iB, πB(iB), πR(iB), p1)

= fB(iB) + kBπB + p1[αBiB(1 − iB) − αRiB(1 − iB)]
= (kB + p1iB(1 − iB)(a − b))πB + fB(iB) + p1biB(1 − iB) − p1αRiB(1 − iB);

HR(iB, πB(iB), πR(iB), p2)

= fR(iB) + kRπR + p2[αBiB(1 − iB) − αRiB(1 − iB)]
= (kR − p2iB(1 − iB)(a − b))πR + fR(iB) + p2αBiB(1 − iB) − p2biB(1 − iB).

The adjoint equation is

= − ∂HB
∂iB

˙p1
p1(∞) = 0
˙p2
p2(∞) = 0.

= − ∂HR
∂iB

+ zp1 = −f ′

B(iB) + p1[z − (αB − αR)(1 − 2iB)]

+ zp2 = −f ′

R(iB) + p2[z − (αB − αR)(1 − 2iB)]





R(iB) − kRz.

Theorem 5. Suppose fB(iB) = 1 − iB, fR(iB) = iB. Then, the Nash equilibria
under various scenarios are listed in Table 1, where FB(iB) = −iB(1 − iB)(a −
b)f ′

B(iB) − kBz and FR(iB) = iB(1 − iB)(a − b)f ′
Proof of Theorem 5 is similar to the proof of Theorem 1 and omitted due
to space limitation. Its cyber security implication is: The outcome of playing
the Nash equilibrium strategies also depends on the initial system state and
the relationship between kB and kR. As illustrated in Figure 2, if kB < kR with
kRz < 1
4 (a−b), meaning that the attacker is more concerned with its control cost
(e.g., reluctant to use/expose its advanced attack tools such as zero-day exploits)
than the defender, then FB(iB) = 0 has two roots i1, i2 and FR(iB) = 0 has two
roots i3, i4. Then, we have i1 < i3 < i4 < i2 (the only possibility under the given
conditions). Therefore, the outcomes under the Nash equilibrium strategies are
summarized as follows:

– If iB(0) < i1, then iB(t) = iB(0) and iR(t) = iR(0) for all t > 0 because

ˆπB = ˆπR = 0 are the Nash equilibrium strategies.

– If i3 > iB(0) > i1, then ˆπB = 1 and ˆπR = 0 until iB = i3, which implies that
iB(t) strictly increases until iB = i3. When iB(t) = i3 at some point in time
t = t1, ˆπB = ˆπR = 1 implies iB(t) = i3 for t > t1.

Table 1. Nash equilibrium strategies for defender and attacker in various cases.

kB

kR

Roots of FB (iB ) = 0 Roots of FR(iB ) = 0

kB z < 1

4 (a − b) kRz < 1

4 (a − b)

kB z < 1

4 (a − b) kRz = 1

4 (a − b)

0 < i1 < i2 < 1

0 < i3 < i4 < 1

0 < i1 < i2 < 1

i3 = i4 = 1
2

ˆπB = 



ˆπR = 






ˆπB =

Nash equilibria
0 if iB (0) ≤ i1
1 if i1 < iB (0) < i2
0 if iB (0) ≥ i2
0 if iB (0) < i3
1 if i3 ≤ iB (0) ≤ i4
0 if iB (0) > i4
0 if iB (0) ≤ i1
1 if i1 < iB (0) < i2
0 if iB (0) ≥ i2


ˆπR = 


0 if iB (0) < i3
1 if iB (0) = i3
0 if iB (0) > i3

kB z < 1

4 (a − b) kRz > 1

4 (a − b)

0 < i1 < i2 < 1

No real-valued roots

ˆπB = 




kB z = 1

4 (a − b) kRz < 1

4 (a − b)

kB z = 1

4 (a − b) kRz = 1

4 (a − b)

kB z = 1

4 (a − b) kRz > 1

4 (a − b)

0 < i1 = i2 = 1
2

0 < i3 < i4 < 1

0 < i1 = i2 = 1
2

i3 = i4 = 1
2

0 < i1 = i2 = 1
2

No real-valued roots

kB z > 1

4 (a − b) kRz < 1

4 (a − b) No real-valued roots

0 < i3 < i4 < 1

ˆπR =

kB z > 1
kB z > 1

4 (a − b) kRz = 1
4 (a − b) kRz > 1

4 (a − b) No real-valued roots
4 (a − b) No real-valued roots No real-valued roots

i3 = i4 = 1
2


0 if iB (0) ≤ i1
1 if i1 < iB (0) < i2
0 if iB (0) ≥ i2

ˆπB = 


ˆπR = 0
0 if iB (0) < i1
1 if iB (0) = i1
0 if iB (0) > i2

ˆπR =





0 if iB (0) ≤ i3
1 if i3 < iB (0) < i4
0 if iB (0) ≥ i4


ˆπB = 



ˆπR = 


0 if iB (0) < i1
πR if iB (0) = i1
0 if iB (0) > i2
0 if iB (0) < i3
πB if iB (0) = i3
0 if iB (0) > i3


ˆπB = 0, ˆπR = 0
ˆπB = 0




0 if iB (0) ≤ i3
1 if i3 < iB (0) < i4
0 if iB (0) ≥ i4

ˆπB = 0, ˆπR = 0
ˆπB = 0, ˆπR = 0

)
b
−
a
(
)

i

−
1
(

i

=
y

B

B

0.25

0.2

0.15

0.1

0.05

0
0

y=k

z
R

y=k

z
B

i
3

i
1
0.2

0.4

i
B

0.6

i
4

i
2

0.8

1

Fig. 2. Illustration of the roots of FB(iB) = 0 with fB(iB) = 1 − iB, and the roots of
FR(iB) = 0 with fR(iB) = iB, where a − b = 1, kBz = 1/8 and kRz = 1/6. The x-axis
represents iB and the y-axis represents y(iB) = iB(1 − iB)(a − b). Arrows indicate
the directions the outcome under the Nash equilibrium heads for. Black-colored bars
indicate that the trajectory under the Nash equilibrium stays static.

– If i4 > iB(0) > i3, then iB(t) = iB(0) and iR(t) = iR(0) for all t > 0 because

ˆπB = ˆπR = 1.

– If i2 > iB(0) > i4, then ˆπB = 1 and ˆπR = 0 until iB = i2, which implies that
iB(t) strictly increases until iB = i2. When iB(t) = i2 at some point in time
t = t2, ˆπB = ˆπR = 1 implies iB(t) = i3 for t > t2.

– If iB(0) > i2, then iB(t) = iB(0) and iR(t) = iR(0) for all t > 0 because

ˆπB = ˆπR = 1.

If kR > 1

4 (a − b) > kB, meaning that the attacker is extremely concerned
with its control cost (e.g., not willing to easily use/expose its advanced attack
tools such as zero-day exploits) but the defender is not, then it always holds
that ˆπR = 0 because FR(iB) = 0 has no root but FB(iB) = 0 has two roots
i1 < i2. From Table 1, we see that limt→∞ iB(t) = 1 always holds, namely that
the attacker gives up using its advanced attack tools.
4 (a− b) and kR > 1

4 (a− b), meaning that both the defender and
the attacker are extremely concerned with their control costs (i.e., neither the
defender wants to easily use/expose its advanced active defense tools, nor the
attacker wants to use/expose its advanced attack tools such as zero-day exploits),
then it always holds that ˆπB = ˆπR = 0 because FB(iB) = 0 and FR(iB) = 0
have no real-valued roots. As a result, iB(t) = iB(0) for any t > 0.

If both kB > 1

The scenarios that one or both FB(iB) = 0 and FR(iB) = 0 have one root can
be regarded as degenerated cases of the above. Moreover, the cases of kB > kR
(i.e., the defender is more concerned about its control cost, such as not willing
to easily use/expose its advanced active defense tools), the outcomes under the
Nash equilibria can be derived analogously.

6 Conclusion

We have investigated how to optimize active cyber defense, by presenting opti-
mal control solutions for strategic defenders against non-strategic attackers, and
identifying Nash equilibrium strategies for strategic defenders and attackers. We
have discussed the cyber security implications of the theoretic results.

This paper brings interesting problems for future research. First, it is inter-
esting to extend the models to accommodate nonlinear fB(·) and fR(·). Second,
the models are geared toward active cyber defense. A comprehensive defense
solution, as hinted in our analysis, should require the optimal integration of re-
active, active, and proactive cyber defense. Therefore, we need to extend the
models to accommodate reactive defense and proactive cyber defense. Moreover,
it is interesting to investigate how to extend the models to accommodate moving
target defense, which has not be systematically evaluated yet [7]. Third, how to
extend the models to accommodate the underlying network structures?
Acknowledgement. Wenlian Lu was jointly supported by the Marie Curie
International Incoming Fellowship from the European Commission (no. FP7-
PEOPLE-2011-IIF-302421), the National Natural Sciences Foundation of China
(no. 61273309), the Shanghai Guidance of Science and Technology (SGST) (no.

09DZ2272900) and the Laboratory of Mathematics for Nonlinear Science, Fudan
University. Shouhuai Xu was supported in part by ARO Grant #W911NF-12-1-
0286 and AFOSR Grant FA9550-09-1-0165. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material are those of the author(s)
and do not necessarily reﬂect the views of any of the funding agencies.

References

1. D. Aitel.

Nematodes – beneﬁcial worms.

http://www.immunityinc.com/

downloads/nematodes.pdf, Sept. 2005.

2. T. Alpcan and T. Ba¸sar. Network Security: A Decision and Game Theoretic Ap-

proach. Cambridge University Press, 2011.

3. M. Bardi and I. Capuzzo-Dolcetta. Optimal control and viscosity solutions of

Hamilton-Jacobi-Bellman equations. Birkhauser, 2008.

4. Alain Bensoussan, Murat Kantarcioglu, and SingRu Hoe. A game-theoretical
In Proc.

approach for ﬁnding optimal strategies in a botnet defense model.
GameSec’10, pages 135–148, 2010.

5. F. Castaneda, E. Sezer, and J. Xu. Worm vs. worm: preliminary study of an active

counter-attack mechanism. In Proc. ACM WORM’04, pages 83–93, 2004.

6. D. Chakrabarti, Y. Wang, C. Wang, J. Leskovec, and C. Faloutsos. Epidemic
thresholds in real networks. ACM Trans. Inf. Syst. Secur., 10(4):1–26, 2008.
7. M. Collins. A cost-based mechanism for evaluating the eﬀectiveness of moving

target defenses. In Proc. GameSec’12, pages 221–233, 2012.

8. Neal Fultz and Jens Grossklags. Blue versus Red: Towards a Model of Distributed
Security Attacks. In Proc. Financial cryptography and data security (FC’99), pages
167–183. 2009.

9. A. Ganesh, L. Massoulie, and D. Towsley. The eﬀect of network topology on the

spread of epidemics. In Proc. of IEEE Infocom 2005, 2005.

10. J. Kephart and S. White. Directed-graph epidemiological models of computer
viruses. In Proc. IEEE Symposium on Security and Privacy, pages 343–361, 1991.
11. J. Kephart and S. White. Measuring and modeling computer virus prevalence. In

Proc. IEEE Symposium on Security and Privacy, pages 2–15, 1993.

12. W. Kermack and A. McKendrick. A contribution to the mathematical theory of

epidemics. Proc. of Roy. Soc. Lond. A, 115:700–721, 1927.

13. J. Kesan and C. Hayes. Mitigative counterstriking: Self-defense and deterrence
in cyberspace. Harvard Journal of Law and Technology (forthcoming, available at
SSRN: http: // ssrn. com/ abstract= 1805163 ).

14. M. Khouzani, S. Sarkar, and E. Altman. A dynamic game solution to malware

attack. In Proc. IEEE INFOCOM, pages 2138–2146, 2011.

15. M. Khouzani, S. Sarkar, and E. Altman. Saddle-point strategies in malware attack.

IEEE Journal on Selected Areas in Communications, 30(1):31–43, 2012.

16. H. Lin. Lifting the veil on cyber oﬀense. IEEE Security & Privacy, 7(4):15–21,

2009.

17. M. Manshaei, Q. Zhu, T. Alpcan, T. Basar, and J. Hubaux. Game theory meets

network security and privacy. ACM Computing Survey, to appear.

18. W. Matthews. U.s. said to need stronger, active cyber defenses. http://www.

defensenews.com/story.php?i=4824730, 1 Oct 2010.

19. A. McKendrick. Applications of mathematics to medical problems. Proc. of Edin.

Math. Soceity, 14:98–130, 1926.

20. R.

Naraine.

’friendly’

welchia

worm

wreaking

havoc.

http://www.internetnews.com/ent-news/article.php/3065761/
Friendly-Welchia-Worm-Wreaking-Havoc.htm, August 19, 2003.

21. J. Omic, A. Orda, and P. Van Mieghem. Protecting against network infections: A

game theoretic perspective. In Infocom’09, pages 1485–1493, 2009.

22. R. P´ıbil, V. Lis´y, C. Kiekintveld, B. Bosansk´y, and M. Pechoucek. Game theoretic
model of strategic honeypot selection in computer networks. In Proc. GameSec’12,
pages 201–220, 2012.

23. B. Schneier. Benevolent worms.

http://www.schneier.com/blog/archives/

2008/02/benevolent_worm_1.html, February 19, 2008.

24. L. Shaughnessy. The internet: Frontline of the next war? http://www.cnn.com/

2011/11/07/us/darpa/, November 7, 2011.

25. George Theodorakopoulos, Jean-Yves Le Boudec, and John S. Baras. Selﬁsh re-

sponse to epidemic propagation. IEEE Trans. Aut. Contr., 58(2):363–376, 2013.

26. Piet Van Mieghem, Jasmina Omic, and Robert Kooij. Virus spread in networks.

IEEE/ACM Trans. Netw., 17(1):1–14, February 2009.

27. M. Vojnovic and A. Ganesh. On the race of worms, alerts, and patches. IEEE/ACM

Trans. Netw., 16:1066–1079, October 2008.

28. Y. Wang, D. Chakrabarti, C. Wang, and C. Faloutsos. Epidemic spreading in real

networks: An eigenvalue viewpoint. In Proc. IEEE SRDS’03, pages 25–34, 2003.

29. N. Weaver and D. Ellis. White worms don’t work. ;login: The USENIX Magazine,

31(6):33–38, 2006.

Active cyber-defense strategy best de-
30. Homeland Security News Wire.
http://www.homelandsecuritynewswire.com/
terrent against cyber-attacks.
active-cyber-defense-strategy-best-deterrent-against-cyber-attacks, 28
June 2011.

31. J. Wolf. Update 2-u.s. says will boost its cyber arsenal. http://www.reuters.com/
article/2011/11/07/cyber-usa-offensive-idUSN1E7A61YQ20111107, November
7, 2011.

32. S. Xu, W. Lu, and L. Xu. Push- and pull-based epidemic spreading in arbitrary
networks: Thresholds and deeper insights. ACM Transactions on Autonomous and
Adaptive Systems (ACM TAAS), 7(3):32:1–32:26, 2012.

33. S. Xu, W. Lu, L. Xu, and Z. Zhan. Adaptive epidemic dynamics in networks:
Thresholds and control. ACM Transactions on Autonomous and Adaptive Systems
(ACM TAAS), to appear.

34. S. Xu, W. Lu, and Z. Zhan. A stochastic model of multivirus dynamics. IEEE

Trans. Dependable Sec. Comput., 9(1):30–45, 2012.

35. Shouhuai Xu, Wenlian Lu, and Hualun Li. A stochastic model of active cyber

defense dynamics. Internet Mathematics, to appear.

A Proof of Theorem 1

Proof. By the Dynamic Programming (DP) argument [3], we know that defender
B’s value function of the optimal solution can be deﬁned as:

φ(iB) =

inf
0≤πB (·)≤1

JB(πB(·)) =

(cid:26)

∞

0
Z

e−zt(fB(iB(t)) + kB · πB(t))dt

.

(13)

(cid:27)

This leads to the following Bellman equation:

zφ(iB) = inf

0≤πB(·)≤1

= inf

0≤πB(·)≤1

= inf

0≤πB(·)≤1

fB(iB) + kBπB(t) + φ′(iB)[αBiB(1 − iB) − αRiB(1 − iB)]

(cid:26)
HB(iB, πB(t), φ′(iB))

HB(iB, πB(t), p), where p = φ′(iB).

(cid:27)

(14)

From (5), we know that the optimal strategy ˆπB takes the form:

ˆπB = 1kB +piB (1−iB )(a−b)<0 + uB1kB +piB (1−iB )(a−b)=0,

(15)

where 1 is the indicator function. The inﬁmum of Hamiltonian (3) is:

inf
0≤πB (·)≤1

HB(iB, πB, p)

= fB(iB) + [kB + piB(1 − iB)(a − b)]1kB +piB (1−iB )(a−b)<0 + p(b − αR)iB(1 − iB).

Hence, we have

zφ(iB)

= fB(iB) + [kB + piB(1 − iB)(a − b)]1kB +piB (1−iB )(a−b)<0 + p(b − αR)iB(1 − iB)
= fB(iB) + [kB + φ′(iB)iB(1 − iB)(a − b)]1kB +φ′(iB )iB (1−iB )(a−b)<0 +

φ′(iB)(b − αR)iB(1 − iB).

(16)

Let y(iB) = kB + φ′(iB)iB(1 − iB)(a − b). In what follows, we are to verify that
(7) satisﬁes (15) with φ(iB) deﬁned by (16), which means that (7) minimizes the
Hamiltonian in the term of (13). This completes the proof.

In order to verify that (7) satisﬁes (15) with φ(iB) deﬁned by (16), we dif-

ferentiate (16) with respect to iB to obtain

[(b − αR) + (a − b)1y<0]iB(1 − iB)y′ − zy − FB(iB) = 0,

(17)

which can be rewritten as

y′ −

z
[(b − αR) + (a − b)1y<0]iB(1 − iB)

y −

FB(iB)
[(b − αR) + (a − b)1y<0]iB(1 − iB)
(18)

= 0.

If kB + φ′(iB)iB(1 − iB)(a − b) < 0 namely y(iB) < 0, (18) should be

d
dx

y(x)e−
(cid:20)

x
0
R

z

(a−αR )ξ(1−ξ) dξ

−

FB(x)
(a − αR)x(1 − x)

(cid:21)

e−

x
0
R

z

(a−αR )ξ(1−ξ) dξ = 0. (19)

If kB + φ′(iB)iB(1 − iB)(a − b) > 0 namely y(iB) > 0, (18) should be:

y(x)eR

1
x

z

(b−αR )ξ(1−ξ) dξ

d
dx

(cid:20)

−

FB(x)
(b − αR)x(1 − x)

(cid:21)

1
eR
x

z

(b−αR )ξ(1−ξ) dξ = 0.

(20)

Therefore, we only need to prove that the optimal defense strategy (7) satisﬁes
(17), namely (19) or (20). The proof is split into cases, depending on x residing
in interval (i2, 1) or (0, i1) or (i1, i2), or x = i1, or x = i2.
Case 1: i2 < x < 1. By (20), we have

y(x)eR

1
x

z

(b−αR )ξ(1−ξ) dξ −

x

i2

Z

FB(ζ)
(b − αR)ζ(1 − ζ)

1
ζ

eR

z

(b−αR )ξ(1−ξ) dξdζ = 0.

Hence, we have

y(x) =

x

i2

Z

FB(ζ)
(b − αR)ζ(1 − ζ)

x
ζ

eR

z

(b−αR )ξ(1−ξ) dξdζ.

(21)

Since iB > i2, we have y(x) > 0. Therefore, we have ˆπB = 0 for iB ∈ (i2, 1).
Case 2: 0 < x < i1. By (20), we have

y(x)e−

x
0
R

z

(b−αR )ξ(1−ξ) dξ −

x

i1

Z

FB(ζ)
(b − αR)ζ(1 − ζ)

e−

ζ
0
R

z

(b−αR )ξ(1−ξ) dξdζ = 0.

Hence,

x

y(x) =

0
Z
=kB −

FB(ζ)
(b − αR)ζ(1 − ζ)
a − b
b − αR

x

f ′
B(ζ)eR

0
Z

x
eR
ζ

z

(b−αR )ξ(1−ξ) dξdζ + kBeR

x
0

z

(b−αR )ξ(1−ξ) dξ

x
ζ

z

(b−αR )ξ(1−ξ) dξdζ

(22)

Since iB < i1, we have y(x) > 0. Therefore we have ˆπB = 0 for iB ∈ (0, i1).
Case 3: i1 < x < i2. By (19), we have

1
y(x)eR
x

z

(a−αR )ξ(1−ξ) dξ −

x

i2

Z

FB(ζ)
(a − αR)ζ(1 − ζ)

1
eR
ζ

z

(a−αR )ξ(1−ξ) dξdζ = 0.

Hence

y(x) =

x

i2

Z

FB(ζ)
(a − αR)ζ(1 − ζ)

x
eR
ζ

z

(a−αR )ξ(1−ξ) dξdζ.

(23)

Since iB ∈ (i1, i2), we have y(x) < 0. This implies ˆπB = 1.
Cases 4 & 5: x = i1 or x = i2. By (21,22,23), we have y(x) = 0. If x = i1 or
x = i2, we can derive φ′(iB) =
i∗(1−i∗)(a−b) from the deﬁnition of y(·). According
αR−b
a−b . Diﬀerentiating with respect to i∗,
to (16), we have: zφ(i∗) = fB(i∗) + kB
we have −i∗(1 − i∗)(a − b)f ′
B(i∗) − kBz = FB(i∗) = 0. Consider the singular
form iB(t) = i∗ for a period of time. We obtain that ˙iB |iB =i∗ = 0 and thus
ˆπB = uB = αR−b
⊓⊔

a−b , where i∗ = i1 or i∗ = i2.

−kB

B Proof of Theorem 3

Proof. To solve the minimization problem, we formulate the current value Hamil-
tonian associated with (9):

HF (iB, πB, q) = λπB + q(αB − αR)iB(1 − iB)

= [λ + q(a − b)iB(1 − iB)]πB + q(b − αR)iB(1 − iB).

The adjoint equation is ˙q = − ∂HF
∂iB
condition

= −q(αB − αR)(1 − 2iB), with the boundary

B(T ∗), π∗
where T ∗ denotes the optimal hitting time that iB(T ∗) = ie, π∗
optimal feedback control, and i∗

B(T ∗), q(T ∗)) + 1 = 0,

HF (i∗

(24)

B(·) denotes the

B(·) denotes the corresponding trajectory.
B is obtained by minimizing Hamiltonian HF (iB, πB, q).
B takes the following

Since HF (iB, πB, q) is linear in πB, the optimal control π∗
bang-bang form:

The optimal control π∗

0

= 0

< 0

1
B (0 < u∗
u∗

if ∂HF
∂πB
B < 1, to be determined) if ∂HF
∂πF
if ∂HF
∂πB

π∗
B = 


where ∂HF
∂πB
≥ 0, then 0 = b−αR
− λ) + 1, which implies ∂HF
If ∂HF
∂πB
∂πB
a−b ( ∂HF
positive constant. (ii). If ∂HF
∂πB
∂πB
b−αR
implies ∂HF
a−b λ − 1
is a negative constant. It can be seen that
∂πB
only under the above (ii), the constraint iB(T ) = ie can be obtained for some T .
Then, the solution to the optimal fast control should be πB(t) = 1 for all time.
So (π∗

= λ + q(a − b)iB(1 − iB). From (24), there are two possibilities: (i).
= a−b
αR−b + λ is a
− λ) + 1, which

B, T ∗) = (1, T1), where T1 satisﬁes

< 0, then 0 = ∂HF
∂πB

a−b ( ∂HF
∂πB

= a−b
a−αR

+ b−αR

> 0

i

h

iB(T1) =

i0
1−i0
1 + i0
1−i0

e(a−αR)T1

e(a−αR)T1

= ie,

that is, T1 = 1

a−αR

ln

ie
1−ie

1−i0
i0

. This completes the proof.

⊓⊔

(cid:16)

(cid:17)

C Proof of Theorem 4

Proof. To solve the optimization problem, we formulate the current value Hamil-
tonian associated with (11):

HF (iB, πB, q) = λπ2
= λπ2

B + q(αB − αR)iB(1 − iB)
B + q(a − b)iB(1 − iB)πB + q(b − αR)iB(1 − iB).

The adjoint equation is ˙q = − ∂HF
∂iB
condition is

= −q(αB − αR)(1 − 2iB), and the boundary

HF (iB(T ∗∗), π∗∗

(25)
where T ∗∗ denotes the optimal ﬁnal time, π∗∗
B (·) denotes the optimal feedback
control, iB(·) denotes the corresponding trajectory, and iB(T ∗∗) = ie. Let D =
q(a − b)iB(T ∗∗)(1 − iB(T ∗∗)). From (25) we have

B (T ∗∗), q(T ∗∗)) + 1 = 0,

HF (iB(T ∗∗), π∗∗

B (T ∗∗), q(T ∗∗)) + 1 = λ(π∗∗

B )2 + Dπ∗∗

B +

b − αR
a − b

D + 1 = 0. (26)

The optimal control, π∗∗

B , is obtained by minimizing the Hamiltonian HF (iB, πB, q).

Because the Hamiltonian HF (iB, πB, q) is quadratic in πB, the optimal control,
π∗∗
B , takes the following form:

2λ (0 < u∗

B < 1, to be determined) if 0 ≤ − D

2λ ≤ 1

0

if − D

2λ < 0.

if − D

2λ > 1

1
− D

π∗∗
B = 



From (26), we know there are three possibilities. (i). If − D
b−αR
a−b D + 1, namely that D = a−b
0 = b−αR
D < −2λ if and only if a − b ≤ 2(αR − b), or if and only if λ < a−b
a − b > 2(αR − b). (iii). If 0 ≤ − D

2λ < 0, then 0 =
2λ > 1, then
(λ + 1) is also a constant. Note that
and

αR−b is a positive constant. (ii) If − D

a−b D + 1, namely that D = − a−b
a−αR

a+b−2αR

2λ ≤ 1, then

0 = λ

−

namely that

(cid:18)

D
2λ

2

(cid:19)

− 2λ

−

(cid:18)

D
2λ

2

−

(cid:19)

b − αR
a − b

2λ

−

(cid:18)

D
2λ

(cid:19)

+ 1,

D = 2

λ −

b − αR
b − αR
a − b
a − b
2λ ∈ (0, 1) if and only if λ ≥ a−b

4
r

+ 4λ

(cid:16)

(cid:17)

λ

2

a+b−2αR

is a constant. Note that − D
2(αR − b).

and a − b >

In term of minimizing the Hamiltonian HF under the above case (i), we
B = 0 for all time, which is impossible to obtain iB(T ) = ie; under the
B = 1 for all time; under the above case (iii), we have

have π∗∗
above case (ii), we have π∗∗
B = D
π∗∗

−2λ for all time. To sum up, we have
(u∗, T2) if λ ≥ a−b
(1, T3) otherwise

B , T ∗∗) =

(π∗∗

a+b−2αR

(cid:26)

and a − b > 2(αR − b)

(27)

where u∗ = D

−2λ = αR−b

a−b +

b−αR
a−b

+ 1

λ , and T2 and T3 satisfy

2

(cid:17)

e(b+(a−b)u∗−αR)T2

r(cid:16)

e(b+(a−b)u∗−αR)T2

i0
1−i0
1 + i0
1−i0

respectively. This completes the proof.

iB(T2) =

= ie,

iB(T3) =

i0
1−i0
1 + i0
1−i0

e(a−αR)T3

e(a−αR)T3

= ie,

⊓⊔

