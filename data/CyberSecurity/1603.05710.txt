Information Flow for Security in Control Systems

Sean Weerakkody

Bruno Sinopoli

Soummya Kar

Anupam Datta

6
1
0
2

r
a

M
7
1

]

Y
S
.
s
c
[

1
v
0
1
7
5
0
.
3
0
6
1
:
v
i
X
r
a

Abstract— This paper considers the development of infor-
mation ﬂow analyses to support resilient design and active
detection of adversaries in cyber physical systems (CPS).
The area of CPS security, though well studied, suffers from
fragmentation. In this paper, we consider control systems as an
abstraction of CPS. Here, we extend the notion of information
ﬂow analysis, a well established set of methods developed in
software security, to obtain a uniﬁed framework that captures
and extends system theoretic results in control system security.
In particular, we propose the Kullback Liebler (KL) divergence
as a causal measure of information ﬂow, which quantiﬁes the
effect of adversarial inputs on sensor outputs. We show that the
proposed measure characterizes the resilience of control systems
to speciﬁc attack strategies by relating the KL divergence
to optimal detection techniques. We then relate information
ﬂows to stealthy attack scenarios where an adversary can
bypass detection. Finally, this article examines active detection
mechanisms where a defender intelligently manipulates control
inputs or the system itself in order to elicit information ﬂows
from an attacker’s malicious behavior. In all previous cases, we
demonstrate an ability to investigate and extend existing results
by utilizing the proposed information ﬂow analyses.

I. INTRODUCTION

The security of cyber physical systems (CPS), which
integrate sensing, communication, and control in physical
spaces, has become a signiﬁcant challenge in society [1], [2].
Because CPS now pervade our critical infrastructures includ-
ing transportation, manufacturing, health care, and energy,
and are often implemented using off the shelf components,
they offer both motivation and opportunity for potential at-
tackers. There exist precedence for attacks on CPS including
Stuxnet [3] and the Maroochy Shire incident [4] .

The ability to detect and characterize such attacks is
paramount to the well being of CPS. In particular, to deliver
appropriate countermeasures for attacks on physical systems,
the operator must be able to passively detect attacks in a
timely manner as they occur. Moreover, the defender must
understand the set of stealthy attacks to motivate resilient
design and active detection. Here, passive detection refers
to the defender’s use of available information to ascertain
if the system is operating normally or under attack. Passive
detection techniques against attacks in CPS have been well
studied. For instance, traditional methods of fault detection
[5], [6] have been considered. However, such schemes are
usually designed to deal with benign failures. Consequently,
recent work has aimed to consider the detection of stealthy
adversaries who perform integrity attacks on sensor measure-
ments and control inputs [7], [8], [9].

Despite this previous work,

the detection of arbitrary
attacks on CPS by adversaries with diverse information and
capabilities has not been well categorized. In this article, we

propose using information ﬂows as a means to quantify the
detectability of generic adversarial attack models. Informa-
tion ﬂows analysis is an establised set of tools in software
security [10], which attempt to determine if the processes
of one agent alter the processes of another agent. We intend
to use information ﬂow to develop a uniﬁed treatment of
security in CPS, speciﬁcally focusing on dynamical control
aspects in this paper while leaving general cyber-physical
treatments to future work.

In this article, we propose the KL divergence as a
quantitative measure for information ﬂow to determine the
extent to which an attacker’s inputs in a control system
affect the system outputs. To complement this measure, we
introduce notions of conditional ǫ-weak information ﬂows
and conditional ǫ-strong information ﬂows. Here, conditional
weak information ﬂows characterize stealthy attack strategies
conditioned on the system model and the defender’s control
policy. Moreover, conditional strong information ﬂows deﬁne
active defense strategies which enable detection of adver-
sarial behavior, conditioned on the attacker’s policy. The
resulting framework and analysis allows us to recover, in a
uniﬁed manner, a collection of prior results in cyber-physical
control systems, obtained using different techniques, in a
number of papers. Moreover, in certain cases our framework
allows us to present reﬁnements on existing results to reveal
additional insights. We summarize these instances below.

First, in section V, we directly leverage the results of [11]
to demonstrate that the KL divergence allows us to char-
acterize optimal passive detectability by speciﬁcally relating
this measure to the optimal decay rate of the probability of
false alarm. Moreover, we show through residue analysis and
information theoretic bounds that the KL divergence can, in
many instances, be efﬁciently evaluated.

Next, in section VI, we consider the study of conditional
weak information ﬂows where we additionally assume a
defender chooses an arbitrary control policy. We show that an
adversary can develop attacks which generate 0 information
ﬂow if and only if the system is left invertible. This allows
us to recover results applied by [8] to analyze undetectable
attack scenarios. In addition, we show, under certain con-
straints on adversarial policy, that the information ﬂow is
a quadratic function of the bias injected on measurement
residues. This allow us to recover results in [12] and [13]
on false data injections which used the residue bias as a
constraint when studying impacts of stealthy adversaries. We
are able to reﬁne these results by presenting optimal detection
guarantees for adversaries that satisfy these constraints.

Finally, information ﬂow analysis allows us to consider
results in active detection where the defender changes system

 
 
 
 
 
 
parameters [14], [15], [16] or the control policy itself [17],
[18], [19], [20], [21] to detect an attack. We consider the spe-
ciﬁc case of replay attacks. Here, we are able to recover re-
sults which show that certain systems and control policies are
vulnerable to replay attacks [17]. However, unlike [17] which
uses speciﬁc continuity arguments, we use our framework to
demonstrate that replay attacks generate a conditional weak
information ﬂow. We then recover results which state that
introducing physical watermarking to the defender’s policy
[18] enables detection of replay adversaries. We do this by
directly proving such a policy yields a conditional strong
information ﬂow for replay attacks. We are able to extend
previous results [18] by using the calculated information ﬂow
to directly evaluate the detectability of a replay attack in a
system with physical watermarking.

To close, we note that [11] also leverages results relating
the KL divergence to optimal passive detectability in order
to deﬁne the notion of an ǫ-stealthy attack. This is subse-
quently used to analyze maximum estimation degradation by
a stealthy adversary in a scalar system. Our paper proposes
using the KL divergence not only as a tool to analyze speciﬁc
attacks, but as a unifying measure to characterize attacks and
defenses in control system security. We also argue that our
proposed framework is more general. Speciﬁcally, the notion
of conditional information ﬂow allows us to both characterize
how an adversarial policy can be tuned to avoid detection by
speciﬁc defenders and consider how the defender can adjust
the system or his control policy to actively detect an attacker.
We will revisit [11] in a more technical context later.

The rest of the paper is summarized as follows. In section
II, we describe the system model. In section III, we introduce
a general model of an adversary in a CPS. Next, in section
IV we deﬁne an information ﬂow in a CPS through the
KL divergence and relate it to existing notions in software
security. After, in section V, we motivation information ﬂow
as a computable measure of optimal passive detectability. In
section VI, we discuss stealthy attack scenarios. Then, in
section VII, we consider information ﬂow in the context of
active detection. We conclude the paper in section VIII.

II. SYSTEM MODEL

We consider a control system with discrete linear time

invariant model given below.

xk+1 = Axk + Buk + wk, yk = Cxk + vk.

(1)

Here xk ∈ Rn is the state, uk ∈ Rp is the set of control
inputs and yk ∈ Rm is the set of sensor outputs. We let
x0 be the initial state. Furthermore, wk ∼ N (0, Q) and
vk ∼ N (0, R) are independent and identically distributed
(IID) process and IID measurement noise respectively. We
consider a ﬁnite horizon up to time T .

The previous linear model of a system is leveraged to
derive the ensuing results related to control system security.
However, we stress that the paradigm of information ﬂows,
to be introduced, can consider general nonlinear and time
varying dynamical systems.

We let Ik be the information available to the defender
time k after making a measurement. From the de-
at
fender’s perspective, the initial state is unknown. However,
the defender knows that f (x0|I−1) = N (ˆx0|−1, P0|−1).
The defender at time −1 is aware of the system model
M = {A, B, C, Q, R, ˆx0|−1, P0|−1}. In total the defender’s
information at time k is given by

Ik = {y0:k, u0:k−1, M}.

(2)

y0:k refers to the ﬁnite sequence {y0, · · · , yk}. Therefore,
the defender is a central entity having cumulative knowledge
of the dynamics of the system and the history of outputs
and inputs. We now deﬁne an admissible defender control
strategy as follows.

Deﬁnition 1: An admissible defender control strategy
is a sequence of deterministic measureable functions
{U0, U1, · · · , UT −1} where Uk : Ik → Rp for all k ∈
{0, 1, · · · , T − 1} and uk = Uk(Ik).

As a result, the defender computes a deterministic function
of the current information to generate an input. Finally,
we assume that the defender implements some passive bad
data detector to determine whether the system is operating
normally, denoted by a null hypothesis H0, or if there exist
an abnormality (or possible attack), denoted by a state of
H1. We deﬁne an admissible detector as follows.

Deﬁnition 2: An admissible defender detector strategy
is a sequence of deterministic measureable functions
{Ψ0, Ψ1, · · · , ΨT −1} where Ψk : Ik → {H0, H1} for all
k ∈ {0, 1, · · · , T }.

Thus at each time k, the defender intelligently constructs a
function Ψk which maps the defender’s available information
to a decision about the state of the system, whether it is
operating normally or has faulty and/or malicious behavior.

III. ATTACK MODEL

We now introduce an adversarial environment where an
attacker, depending on his capabilities, as well as knowledge
of the system can manipulate control inputs or sensor mea-
surements to degrade control and estimation performance.
Here, we formulate an adversary’s effect on a system by
including additive attacker inputs ua

k as follows.

k and da

xk+1 = Axk + Buk + Baua
k + vk.

yk = Cxk + Dada

k + wk,

(3)
(4)

Ba characterizes the adversarial inputs, which could be a
subset of actuators the attacker usurps from the defender, or
his own inputs. Without loss of generality, we assume Ba is
full column rank. We assume the adversary can modify m′
sensors, S = {γ1, · · · , γm′} ⊆ {1, · · · , m}. Therefore, we
deﬁne Da ∈ Rm×m
entrywise as Da
k ∈ Rp
and da

are unknown
to the defender. Thus a defender can only measure an
adversary’s effect on a system through sensor readings.

u,v = 1u=γj ,v=j.
k ∈ Rm

It is assumed that ua

′

′

′

We represent the adversary’s knowledge of the system
time k as I a
k . Here, we assume at a minimum that
0:k−1, da
k . Thus, the adversary is aware of his

0:k} ⊂ I a

at
{ua

own history. Moreover, the adversary may have the ability
to read a subset of control inputs uk or sensor outputs yk
from the defender. For instance, if the attacker can modify
channels, he may also be able to intercept signals sent along
these channels, thereby utilizing a man in the middle attack.
The portion of inputs and outputs the attacker and defender
can read are public and are denoted upu
k . Finally, the
adversary may have some imperfect prior knowledge of
ˆM, the controller ˆC, and the detector ˆD. The
the plant
adversary’s information is
0:k−1, da
k = {ua
I a

0:k, ˆM, ˆC, ˆD}.

0:k−1, ypu

0:k, upu

k , ypu

(5)

An admissible attack strategy leverages the attacker’s

information I a

k to generate attack inputs for the system.

0 , Da

T −1, Da

T −1, Da

0 , · · · , U a

Deﬁnition 3: An admissible attack strategy on the plant
is a sequence of deterministic measureable functions
k → Rp′ for
{U a
k : I a
T } where U a
all k ∈ {0, 1, · · · , T − 1} and ua
k (I a
k = U a
k ). Additionally,
k−1 × ypu
k → Rm′ for all k ∈ {0, 1, · · · , T } and
k : I a
Da
k−1, ypu
k(I a
k = Da
da
k ).
We note that while current state of the art adversarial
models for control systems consider attackers who do not
change their attack strategy, our model considers an attacker
with the freedom to leverage all his information to construct
an attack input.

IV. INFORMATION FLOWS IN PHYSICAL SYSTEMS

In software security, an information ﬂow exists from a
private input
to a public output if including the private
input changes the behavior of the public output. We wish to
extend this notion for adversarial inputs and sensor outputs
of control systems. In this section we propose a means to
quantify information ﬂow to characterize the detectability of
adversarial strategies.

We quantify the information ﬂow through the KL diver-
gence between the distribution of the output under attack
and the distribution of the output under normal operation
[22]. For deﬁniteness, we assume that all discrete time
stochastic processes of interest considered hereafter induce
(joint) distributions on the path space that are absolutely
continuous with respect to Lebesgue measure. Thus, they
possess densities in the usual sense. The KL divergence
between a distribution with probability density function p(x)
and a distribution with probability density function q(x) over
a sample space X is given by

DKL(p(x)||q(x)) =

log

p(x)dx.

(6)

ZX
The above deﬁnition can be generalized to probability mea-
sures [23]. The KL divergence has the following properties
[22].

(cid:18)

(cid:19)

p(x)
q(x)

1) DKL(p(x)||q(x)) ≥ 0 .
2) DKL(p(x)||q(x)) = 0 if and only if p(x) = q(x)

almost everywhere.

3) DKL(p(x)||q(x)) 6= DKL(q(x)||p(x)).

We now use the KL divergence to deﬁne information ﬂows
in a physical system. To begin, denote the conditional

0:T −1, Da
1
T + 1

distribution of the output based on apriori information as
follows.
DM,U0:k−1,U a

= f (y0:k|I−1, U0:k−1, U a

0:k−1,Da

0:k−1, Da

0:k).

y0:k

0:k

Deﬁnition 4: The information ﬂow from the attacker’s

inputs (U a

0:T ) to the defender’s outputs y0:T is

0:T

y0:T

y0:T

0:T −1,Da

DKL(DM,U0:T −1,U a

||DM,U0:T −1,0,0

IFT =

).
The proposed deﬁnition of information ﬂows has many
desirable properties, which make it compatible with existing
measures of information ﬂow in cyber security. First, the
KL divergence allows us to recover the property of nonin-
terference [24] in deterministic systems and probabilististic
noninterference [25] in stochastic systems. There exists inter-
ference from a high level user to a low level user if changing
high level inputs changes low level outputs.

In our model, the low level inputs are the defender’s
actions, the high level inputs are the attacker’s actions, and
the low level outputs are the defender’s outputs y0:k. In a
deterministic system, if an adversary’s actions change the
output y0:k, the KL divergence is inﬁnite, reﬂecting the fact
that there is interference. However, if the output y0:k is
the same when the system is operating normally and under
attack, indicating noninterference, the KL divergence is 0.
There exists probabilistic interference from a high level user
to a low level user if changing high level inputs measurably
alters the distribution of low level outputs. IFT = 0 if and
only if there exists probabilistic noninterference.

Finally, when there exists probabilistic interference, we
would like to have a means to measure information ﬂow.
In software security, this is done through research in quan-
titative information ﬂow. A majority of previous work in
software security [26] have proposed associative measures
of information ﬂow such as mutual information. Associative
measures of information ﬂow, which quantify correlation,
attempt
to evaluate how much information is leaked by
an input to the output and thus provide utility in privacy
applications.

The KL divergence however is a causal measure which
directly determines how varying an attacker’s inputs changes
the distribution of public outputs. The extent to which an
attacker’s input changes the system output will mark the
defender’s ability to distinguish outputs under attack from
outputs under normal operation and thus detect the presence
of an adversary. While recent work in software security has
begun to investigate causal measures of information ﬂow for
violation detection, to our knowledge, the ensuing results
will be the ﬁrst work applied to physical systems.

To close the section we attempt to categorize adversarial
policies which generate information ﬂows bounded above by
ǫ when the defender implements control policies in a set U
or has a system with model in M.

Deﬁnition 5: A permissible attack (U a

0:T ) gener-
ates a (M, U) conditional ǫ- weak information ﬂow if for
all U0:T −1 ∈ U and for all M ∈ M, IFT ≤ ǫ.

0:T −1, Da

Several special cases which satisfy this deﬁnition have arisen
in the literature. For instance, a replay attack, generates an
information ﬂow bounded above by ǫ only for certain classes
of models M and strategies U. Another special case is below.

Deﬁnition 6: An adversary generates a M conditional ǫ-
weak information ﬂow if for a speciﬁc model M, IFT ≤ ǫ,
regardless of the defender’s policy UT −1.
This special case, where we remove any constraints on the
defender’s policy, is equivalent to ǫ-stealthiness in [11] and
contains false data injections and zero dynamic attacks which
we consider in section VI. We now consider defender policies
and system design which elicit information ﬂows.

Deﬁnition 7: A change in the system M or a permissible
control policy U0:T −1 generates a Ua conditional ǫ- strong
0:T −1, Da
information ﬂow if for (U a
The preceding deﬁnition characterizes active detection where
an adversary changes system parameters or his control policy
to create an information ﬂow. We will examine this topic
further in section VII.

0:T ) ∈ Ua, IFT ≥ ǫ.

We can instead consider the normalized residue zk, obtained
from a Kalman ﬁlter [27].

ˆxk+1|k = Aˆxk|k + Buk, ˆxk|k = (I − KkC)ˆxk|k−1 + Kkyk,
(7)

Pk+1|k = APk|k−1AT + Q − AKkCPk|k−1AT ,
Kk = Pk|k−1CT (CPk|k−1CT + R)−1,

zk = (CPk|k−1CT + R)− 1

2 (yk − C ˆxk|k−1).

(8)

The Kalman ﬁlter computes optimal state estimates ˆxk|k−1
and ˆxk of xk. The normalized residue zk is a normalized
measure of the difference between the defender’s outputs and
the expected outputs derived from the state estimate. We now
have the following result [28].

Lemma 9: The set of residues f (z0:k|I−1) = N (0, I)
when the system is operating normally. Given ﬁxed strategy
U0:k−1 and ˆx0|−1, z0:k is an invertible function of y0:k.
Because the residues and outputs are related by an invertible
mapping, we can show their KL divergences are equal [23].

Theorem 10: The KL divergence between sensor outputs

V. PASSIVE DETECTION

and between residues are equivalent.

In this section we motivate the KL divergence as a
tool to quantify the passive detectability of an adversary
and evaluate the special case of M conditional ǫ- weak
information ﬂows. Speciﬁcally, we show that this measure is
directly related to the optimal decay rate for the probability
of false alarm. We now have the following result from [11].
Theorem 8: Let 0 < δ < 1. Deﬁne αk the probability of

false alarm and βk the probability of detection as follows

αk , Pr (Ψk(Ik) = H0|H0) , βk , Pr (Ψk(Ik) = H1|H1) .

IFk ≥ ǫ. Then there exists a detector Ψk

Suppose lim sup

k→∞

such that
βk ≥ 1 − δ, ∀k,

lim sup
k→∞

− 1

k+1 log(αk) ≥ ǫ.

Alternatively, suppose additionally that the sequences gener-
ated by y0:k operating normally and under attack are ergodic.
Suppose lim
k→∞

IFk ≤ ǫ. Then for all detectors Ψk

βk ≥ 1 − δ, ∀k =⇒ lim sup
k→∞

−

1
k + 1

log(αk) ≤ ǫ.

Based on Theorem 8, the information ﬂow is essentially
equivalent to the optimal decay rate in the probability of false
alarm and an adversary who generates an M conditional ǫ-
weak information ﬂow will have false alarm rate bounded
above by ǫ. As a result, information ﬂow allows us to gener-
ically evaluate and compare the detectability of different
attack policies. However unlike other potential measures such
as βk, the KL divergence can be efﬁciently characterized.
it may be difﬁcult

to compute the KL
divergence of the outputs y0:T −1 directly. For instance, if
a control policy includes nonlinear feedback, the Gaussian
property of the output is destroyed, which likely removes
the ability to obtain closed form distributions of the output.

We note that

DKL(DM,U0:T −1,U a
= DKL(DM,U0:T −1,U a

y0:T

z0:T

0:T −1,Da

0:T

0:T −1,Da

0:T

||DM,U0:T −1,0,0

)

y0:T
||DM,U0:T −1,0,0

z0:T

)

Due to theorem 10, we can analyze the residues operating
normally and under attack instead of the system output when
computing the information ﬂow. Residues under normal
operation have a known zero-mean Gaussian distribution. If
the distribution of the residue under attack remains Gaussian,
a closed form solution exists for the KL divergence. The
KL divergence between two Gaussian distributions N1 =
N1(µ1, Σ1) and N0 = N0(µ0, Σ0) with µ1 ∈ Rl is [22]

DKL(N1||N0) = −

+

1
2

tr(Σ−1

l
2
(µ1 − µ0)T Σ−1

0 Σ1) +

+

1
2

1
2

log det

Σ0Σ−1
1

0 (µ1 − µ0).

(cid:0)

(cid:1)
(9)

If the attacker’s policy is independent of the defender’s
outputs, it is known that the distribution of residues under
attack remain Gaussian. In general however, it may still
be difﬁcult to compute the KL divergence of z0:k since it
is a growing sequence. Fortunately, we can leverage the
independence of the residues to obtain the following bound.
Theorem 11: The information ﬂow generated by an adver-
sary can be lower bounded by the sum of the residue-based
KL divergences generated at each time step.
DKL(DM,U0:k−1,U a

||DM,U0:k−1,0,0

0:k−1,Da

0:k

)

T

zk

zk

T + 1

IFT ≥

k=0
X

Proof:

By Theorem 10 and Bayes rule we know
DKL(DM,U0:k−1,U a

0:k−1,Da

0:k

T

zk|z0:k−1

||DM,U0:k−1,0,0

zk

IFT =

k=0
X

T + 1

.

)

.

Thus, we observe

IFT − IF LB

T =

T

M,U0:k−1,U a
I
zk,z0:k−1

0:k−1,Da

0:k

k + 1

.

k=0
X

where IF LB
the mutual information [22] which is nonnegative.

is the obtained lower bound and Izk,z0:k−1 is

T

Instead of computing the KL divergence of vectors z0:k ∈
Rmk, which in general requires us to store and compute
the determinant of a matrix in Rmk×mk, we can instead
obtain a recursive lower bound by computing the sum of
T divergences for vectors zk ∈ Rm. Moreover, note that the
gap between the lower bound and IFT is the scaled sum of
mutual informations between zk and z0:k−1 so that if attack
residues are independent, the gap is 0.

VI. STEALTHY ADVERSARIAL BEHAVIOR

We next describe attacks which generate M conditional
ǫ-weak information ﬂows, where regardless of the defender’s
policy the attacker remains stealthy. Understanding these
scenarios motivate resilient design of M and also allow us
to capture and extend research on left invertibility and false
data injection attacks. The ﬁrst scenario we consider is when
ǫ = 0 where there exists probabilistic noninterference.

Let ya

0:T denote outputs realized from the distribution
and y0:T denote outputs
. If U0:T −1 =

under attack DM,U0:T −1,U a
realized from the normal system DM,U0:T −1,0,0
0, then, due to the linearity of our model M,

0:T −1,Da

y0:T

y0:T

0:T

0:T = y0:T + ∆y0:T (da
ya

0:T , ua
∆xk+1 = A∆xk + Baua
k, ∆x0 = 0,
∆yk = C∆xk + Dada
k.

0:T −1),

(10)

(11)
(12)

0:T , ua

0:T , ua
Proof: Suppose ∆y0:T (da

We now obtain the following result.
Theorem 12: A nonzero attack strategy (U a

0:T )
generates a M conditional 0-weak information ﬂow if and
only if ∆y0:T (da

0:T −1) = 0 with probability 1.

0:T −1, Da

0:T , ua

0:T , ua

0:T −1) = 0 with proba-
bility 1 − ǫ where ǫ > 0. Then for U0:T −1 = 0, we have with
probability 1 − ǫ, ya
0:T 6= y0:T . Thus, the KL divergence is
greater than 0. Now instead suppose ∆y0:T (da
0:T −1) =
0 with probability 1. From (3) and (4), we observe that (10)
holds if ∆y0:T (da
0:T −1) = 0. This is based on the fact
that the defender’s control strategy will not change if the
output does not change. Thus, if ∆y0:T (da
0:T −1) = 0
with probability 1, then ya
0:T = y0:T with probability 1.
Therefore, the KL divergence and information ﬂow is 0.
We have shown that there exists a 0-information ﬂow attack
if and only if there exists nontrivial (U a
0:T ) which
satisfy (11), (12) for 0 ≤ k ≤ T . For long enough time
horizon this is in fact equivalent to left invertibility.
ˆBa =

ˆDa =
Ba
Theorem 13: Let
0m×p′ Da
. Suppose T ≥ n − p′ + 1. A nonzero adver-
sarial policy (U a
0:T ) can generate a M conditional
(cid:2)
0-weak information ﬂow if and only if (A, ˆBa, C, ˆDa) is not
left invertible.

0:T −1, Da

0:T −1, Da

0:T , ua

0n×m′

(cid:2)

(cid:3)

(cid:3)

,

Proof: The result follows directly from Theorem 12

and Corollary 1 of [29].

Left invertibility in control systems has been well studied in
previous work in CPS security as a subset of zero dynamic
attacks [8]. Our general framework of information ﬂows
is able to recover this property and consequently, we can
directly apply previous results related to left invertibility in
our study of 0-weak information ﬂows. For instance, we can
consider conditions on M which allow for the existence of
0 information ﬂow attacks to motivate resilient design of the
system (A, B, C) and channel security (Ba, Da).

Theorem 14: [8] Let T ≥ n − p′ + 1. An attack policy
can create a M conditional 0-weak information ﬂow if and
< n + p′ + m′,
only if rank

∀ λ ∈ C

¯P (M)

(cid:0)

(cid:1)
where ¯P (M) =

λI − A ˆBa
ˆDa

C

.

(cid:20)
We now wish to consider the case of M conditional ǫ-
weak information ﬂows for ǫ > 0. However, we assume that
the adversary injects additive inputs which are independent
of the defender’s system outputs. Thus, we assume

(cid:21)

ua
k = U a
k = Da
da

k (ua
k(ua

0:k−1, da
0:k−1, da

0:k, ˆM, ˆC, ˆD),
0:k−1, ˆM, ˆC, ˆD).

(13)

Such attacks are known as false data injection attacks. We
now have the following result.

Theorem 15: Consider an admissible adversarial policy

which satisﬁes (13). Then,

IFT =

1
2(T + 1)

∆zT

0:T ∆z0:T ,

(14)

where ∆zk satisﬁes ∆e0|−1 = 0 and

∆ek+1|k = (A − AKkC)∆ek|k−1 + Baua
∆zk = (CPk|k−1CT + R)− 1
Proof: See Appendix I.

2

C∆ek|k−1 + Dada
k

k − AKkDada
k,
(15)

.

(cid:0)

(cid:1)

Thus, the information ﬂow is proportional to the norm of
∆zk squared where ∆zk represents the bias the adversary
injects on the normalized residue. The norm of the residue
bias has been previously used as a measure of the stealthiness
in false data injection attacks. For instance, [12] and [13], in
their investigation of false data injection attacks, restrict

k∆zkk2 ≤ B ∀k.

(16)

with the motivation that the increase in βk will be bounded
by some B′ in this scenario. For B ≤ 2ǫ, such an attacker
generates a M conditional ǫ-weak information ﬂow. Conse-
quently we have the following result.

− log(αk)

k+1 = ǫ.

Theorem 16: Suppose a false data injection attack satisﬁes
k∆zkk2 ≤ 2ǫ ∀k. Then, for δ > 0 there exists a detector
such that βk ≥ 1 − δ and lim sup
k→∞
Again, the results obtained in [12], evaluating models M
and attacks Da
0:k which stealthily destabilize a system, and
[13], estimating the bias an adversary can stealthily inject
on the system state in M, can all be reframed as attacks
which generate M conditional ǫ-weak information ﬂow. This
reﬁnement of existing results allows us to now quantify
detectability in addition to system impact.

VII. ACTIVE DETECTION OF ADVERSARIAL BEHAVIOR

ρ(A) < 1. Then, almost surely lim
T →∞

IFT ≥ ǫ, where

In this section, we will revisit and extend results related
to the active detection of replay attacks using the proposed
measure of information ﬂow. Recall that in active detection,
the defender changes the system or his policy to elicit
an information ﬂow. Speciﬁcally, we will use information
ﬂows to determine when replay attacks are stealthy. We will
then extend previous work by using information ﬂows to
characterize optimal detection with watermarking.

In a replay attack, the adversary observes a sequence of
measurements from y−N to y−N +T −1. Then, without loss of
generality, at time 0, the attacker replays these measurements.
Here, we will assume −N is large so that the adversary
has an adequate buffer and that the replayed outputs are
independent of the current outputs. Moreover we assume
the system at time −N is in steady state. We ﬁrst argue
that a replay attack generates a (M, U) conditional ǫ-weak
information ﬂow for a large class of systems M and common
control policies U. For instance, consider a defender that
uses state feedback with gain L so Uk(Ik) = Lˆxk|k.

Let A = (A + BL)(I − KC) and P = CP CT + R. It

has been shown that [18]

zk = zk−N − P − 1

2 CAk(ˆx0|−1 − ˆx−N |−N −1).

(17)

If M and U0:k−1 generate stable A the second term con-
verges to 0. Therefore, we have the following result regarding
the information ﬂow with proof in appendix II.

Theorem 17: Suppose that our control system (1) with
state feedback control is under replay attack, where ρ(A) <
1. Then,

IFT = 0.

lim
T →∞

If A(M, U0:k−1) is stable, the adversary’s actions are
asymptotically undetectable since the information ﬂow is
0. This result was previously obtained in [17] by instead
showing that continuous functions of the defender’s informa-
tion are indistinguishable under normal and replay scenarios.
Information ﬂows allow us to recover this result via a general
CPS security framework.

In this example, the defender’s control strategy U0:T −1
of state feedback, leaves the system vulnerable to a replay
attack. The defender ideally should be able to perform
active detection and determine a control strategy which
simultaneously addresses system objectives while creating
an information ﬂow from a replay adversary.

Watermarking techniques allow the defender to increase
the information ﬂow from the attacker input to defender
output and as a result create an Ua conditional ǫ-strong
information ﬂow, where Ua contains the replay attack policy.
In watermarking, noisy control inputs are used with uk =
Uk(Ik) = Lˆxk|k + ∆uk where ∆uk ∼ N (0, Q). Note that
while the watermark is random, it can be predetermined
ofﬂine so that Uk(Ik) remains a deterministic function. We
now show watermarking creates a strong information ﬂow.
Theorem 18: Suppose the system (1) with state feedback
control and watermarking is under replay attack, where

tr

ǫ =

P −1CΣCT
2

(cid:0)

(cid:1)

, Σ = AΣAT + BQBT .

Proof: See Appendix III.

(cid:0)

P −1CΣCT

From the theorem above,

the defender can make the
information ﬂow from an adversarial input arbitrarily large
by increasing tr
which is a linear function
of the watermark covariance Q. In fact, previous work
(cid:1)
on watermarking [18] does aim to design watermarks by
maximizing tr
subject to constraints on control
performance in the system. Thus, our results motivate the
choice of this objective function. The use of information
ﬂows also allow us to extend previous results to analyze
optimal detection of replay attacks under watermarking sce-
narios.

P −1CΣCT

(cid:1)

(cid:0)

Corollary 19: Assume system (1) with state feedback
control and watermarking is under replay attack, where
ρ(A) < 1. Then for δ > 0 there exists a detector such that
βk ≥ 1 − δ, ∀ k and

lim sup
k→∞

−

1
k

log(αk) ≥

tr

P −1CΣCT
2

(cid:0)

(cid:1)

.

(18)

Proof: The result follows from Theorems 18 and 8.
We simulate a vehicle moving along a single axis [12]
under replay attack. Here, we assume that
the defender
obtains the gain L using a linear quadratic Gaussian (LQG)
controller which attempts minimize a cost J given by

J = lim
T →∞

1
T + 1

E

"

k xk + uT
xT

k uk

.

#

T

k=0
X

The LQG cost increases linearly with Q. We select the
covariance Q of the watermark so that ∆J, the increased
cost due to watermarking, is 40% of the optimal J. Here,
we simulate the system 1000 times over a horizon of 200
steps. We plot the average information ﬂow in Fig 1, both
with watermarking and without watermarking. As expected
from Theorem 17,
the
information ﬂow generated by a replay attack converges to
0. If physical watermarking is implemented, the information
ﬂow generated by an adversary has a lower bound ǫ which
grows linearly with Q. We implement a Neyman Pearson
detector [22] and plot the average probability of false alarm
and detection as a function of k in Fig 2.

in the absence of watermarking,

VIII. CONCLUSION

In this article, we introduced a physical measure of infor-
mation ﬂow to characterize detection in CPS and provide a
uniﬁed approach to dealing with security in both the cyber
and physical domains. We proposed the KL divergence as
a measure of information ﬂow. We motivate its use through
results in optimal passive detection and computational ease
of evaluation. We examined attacks which are stealthy for
ﬁxed models, and all
input strategies, recovering results
related to left invertibility and false data injection attacks.
Finally, we investigated replay attacks and used information

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

l

w
o
F
n
o
i
t
a
m
r
o
f
n
I

0

0

Information Flow with Watermarking
Lower Bound ǫ
Information Flow without Watermarking

50

100
Time k

150

200

Let zs
0:k−1 = 0 and Da
U a

k be the residue under normal operation, where

0:k = 0. Then,
k+1|k = (A − AKkC)es
es
k = (CPk|k−1CT + R)− 1
zs

2

k|k−1 + wk − AKkvk,

Ces

k|k−1 + vk

.

(cid:17)
It can be seen from the linearity of the system that

(cid:16)

zk = zs

k + ∆zk,

and that (15) holds. Moreover, from an inductive argument,
we see that ∆zk is a deterministic variable since U a
0:k−1 and
0:k are known functions of deterministic variables ˆM, ˆC, ˆD.
Da
As a result, DM,U0:k−1,U a
= N (∆z0:k, I). Finally,
from (9) and Theorem 15, we have

0:k−1,Da

z0:k

0:k

Fig. 1.
Information Flow generated by a replay attack. The information
ﬂow as a function of k in the presence of watermarking is included along
with its lower bound ǫ, and the information ﬂow generated when physical
watermarking is not present

DKL (N (µ1, Σ1), N (µ2, Σ1)) =

The result immediately follows.

kΣ

− 1
2
1

1
2

(µ1 − µ2)k2.

y
t
i
l
i

b
a
b
o
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Probability of false alarm
Probability of detection

APPENDIX II
PROOF OF THEOREM 17

Proof: We observe from (17) that

z0:k ∼ N (µr, Σr),

µr(jm : jm + m − 1) = E[zj] = −P − 1

2 CAk ˆx0|−1,

(19)

(20)

Σr(jm : jm + m − 1, lm : lm + m − 1) = Cov(zj, zT

= P − 1

2 CAj W(Al)T CT P − 1

2 + δ(l − m)I,

l ),
(21)

50

100
Time k

150

200

where W is the steady state covariance of ˆxk|k−1 and δ refers
to the discrete delta dirac function. From (9), Theorem 10,
and Sylvester’s determinant theorem we have
DKL(DM,U0:k−1,U a

||DM,U0:k−1,0,0

0:k−1,Da

) =

0:k

y0:k

y0:k

c1 + c2 + c3
2

Fig. 2. Probability of detection and probability of false alarm vs time for
a Neyman Pearson Detector

where

c1 = tr

k

P − 1

2 CAj W(Aj)T CT P − 1

2

ﬂows to quantify optimal detection performance with physi-
cal watermarking. We close by noting that information ﬂow
tools are amenable to true CPS analysis. In particular, we can
consider a richer set of problems emcompassing both cyber
and physical domains by leveraging the proposed results in
physical security and existing parallels in cyber and software
security. Approaching these general problems will mark the
next stage of obtaining a uniﬁed paradigm for addressing
CPS security.




k

j=0
X

c2 =

j=0
X

0|−1(Aj )T CT P −1CAj ˆx0|−1,
ˆxT

,





k

I +

1

2 (Aj)T CT P −1CAj W

1
2

W

c3 = − log det



j=0
X
Let X1 and X2 be given by



.





∞

X1 =

AjW(Aj )T = AX1AT + W,

APPENDIX I
PROOF OF THEOREM 15

j=0
X

∞

Proof: Let ek|k−1 = xk − ˆxk|k−1. From (3),(4), and

(7) we obtain

ek+1|k = (A − AKkC)ek|k−1 + Baua

k + wk − AKkvk
− AKkDada
k,

X2 =

(Aj )T CT P −1CAj = AT X2A + CT P −1C.

j=0
X

From Lyapunov’s equation and since A is stable, the matrices
X1 and X2 exist and are bounded. Since c1, c2, and |c3| are
monotonic in k, we have for all k

zk = (CPk|k−1CT + R)− 1

2

Cek|k−1 + vk + Dada
k

.

c1 ≤ tr

(cid:0)

(cid:1)

2 CX1CT P − 1

2

P − 1
(cid:16)

, c2 ≤ ˆxT

0|−1X2 ˆxT

0|−1,

(cid:17)

 
|c3| ≤ log det

I + W

1

2 X2W

1
2

.

Consequently, for all k there exists M ∗ satisfying

(cid:16)

(cid:17)

DKL(DM,U0:k−1,U a

y0:k

0:k−1,Da

0:k

||DM,U0:k−1,0,0

y0:k

) ≤ M ∗,

Dividing by k + 1, the result follows.

APPENDIX III
PROOF OF THEOREM 24

Proof: When under a replay attack, we have [18]

zk = zk−N − P − 1

2 CAk(ˆx0|−1 − ˆx−N |−N −1)

(22)

k−1

− P − 1

2 C

Ak−1−j B (∆uj − ∆uj−N ) ,

j=0
X

where N is some unknown, but large delay between the
replayed sequence and the true sequence. Thus, under attack
zk ∼ N (µk, Σk + I) with

µk = P − 1

2 CAk ˆx0|−1 + P − 1

2 C

Ak−1−j B∆uj,

k−1

j=0
X

Σk = P − 1

2 C[AkW Ak T +

k−1

AjBQBT Aj T ]CT P − 1
2 .

j=0
X
Thus, the KL divergence between zk under attack and under
normal operation is given by

DKL(DM,U0:k−1,U a

zk

0:k−1,Da

0:k

||DM,U0:k−1,0,0

zk

) =

k + c3
k + c2
c1
k
2

(23)

where

k = µT
c1

k µk,

c2
k = − log det (I + Σk) ,

c3
k = tr(Σk).

From [20], it is known that

k + c3
c2
Furthermore, by the law of large numbers, we know

k ≥ 0.

(24)

lim
T →∞

1
T + 1

k=0
X
Using (23), (24) and (25)

T

c1
k

a.s.
→ tr

P −1CΣCT

.

(25)

(cid:0)

(cid:1)

lim
T →∞

T

k=0
X

DKL(DM,U0:k−1,U a

zk

0:k−1,Da

0:k

T + 1

||DM,U0:k−1,0,0

zk

)

≥ ǫ.

(26)

By Theorem 11, the result immediately follows.

REFERENCES

[1] A. A. C´ardenas, S. Amin, and S. Sastry, “Research challenges for the
security of control systems,” in HOTSEC’08: Proceedings of the 3rd
conference on Hot topics in security. Berkeley, CA, USA: USENIX
Association, 2008, pp. 1–6.

[2] T. Cardenas, A. A.and Roosta and S. Sastry, “Rethinking security
properties, threat models, and the design space in sensor networks:
A case study in scada systems,” Ad Hoc Networks, vol. 7, no. 8, pp.
1434–1447, 2009.

analysis
[3] R. Langner,
of what
Langner
Communications, Tech. Rep., November 2013. [Online]. Available:
www.langner.com/en/wp-content/uploads/2013/11/To-kill-a-centrifuge.pdf

centrifuge: A technical
achieve,”
to

“To
stuxnet’s

a
creators

tried

kill

[4] J. Slay and M. Miller, “Lessons learned from the maroochy water
Springer US, 2008,

breach,” in Critical Infrastructure Protection.
pp. 73–82.

[5] H. L. Jones, “Failure detection in linear systems,” Ph.D. dissertation,

M.I.T., Cambridge, Massachusetts, 1973.

[6] A. S. Willsky, “A survey of design methods for failure detection in
dynamic systems,” Automatica, vol. 12, pp. 601–611, Nov 1976.
[7] Y. Mo, J. Hespanha, and B. Sinopoli, “Robust detection in the presence
of integrity attacks,” in American Control Conference (ACC), 2012,
June 2012, pp. 3541–3546.

[8] F. Pasqualetti, F. Dorﬂer, and F. Bullo, “Attack detection and identiﬁca-
tion in cyber-physical systems,” Automatic Control, IEEE Transactions
on, vol. 58, no. 11, pp. 2715–2729, Nov 2013.

[9] S. Sundaram, M. Pajic, C. Hadjicostis, R. Mangharam, and G. J.
Pappas, “The wireless control network: monitoring for malicious
behavior,” in IEEE Conference on Decision and Contro, Atlanta, GA,
Dec 2010.

[10] D. E. Denning and P. J. Denning, “Certiﬁcation of programs for
secure information ﬂow,” Commun. ACM, vol. 20, no. 7, pp. 504–513,
1977. [Online]. Available: http://doi.acm.org/10.1145/359636.359712
[11] C.-Z. Bai, F. Pasqualetti, and V. Gupta, “Security in stochastic con-
trol systems: Fundamental limitations and performance bounds,” in
American Control Conference (ACC), 2015, June 2015.

[12] Y. Mo and B. Sinopoli, “False data injection attacks in control
systems,” in First Workshop on Secure Control Systems, Stockholm,
Sweden, April 2010.

[13] ——, “Integrity attacks on cyber-physical systems,” in Proceedings
of the 1st international conference on High Conﬁdence Networked
Systems. ACM, 2012, pp. 47–54.

[14] A. Teixeira, I. Shames, H. Sandberg, and K. Johansson, “Revealing
stealthy attacks in control systems,” in Communication, Control, and
Computing (Allerton), 2012 50th Annual Allerton Conference on, Oct
2012, pp. 1806–1813.

[15] F. Miao, Q. Zhu, M. Pajic, and G. Pappas, “Coding sensor outputs
for injection attacks detection,” in Decision and Control (CDC), 2014
IEEE 53rd Annual Conference on, Dec 2014, pp. 5776–5781.
[16] S. Weerakkody and S. B., “Detecting integrity attacks on control
systems using a moving target approach,” in Submitted to Decision and
Control (CDC), 2015 IEEE 54th Annual Conference on, Dec 2015.

[17] Y. Mo and B. Sinopoli, “Secure control against replay attacks,” in
Communication, Control, and Computing, 2009. Allerton 2009. 47th
Annual Allerton Conference on, Sept 2009, pp. 911–918.

[18] Y. Mo, R. Chabukswar, and B. Sinopoli, “Detecting integrity attacks
on scada systems,” Control Systems Technology, IEEE Transactions
on, vol. 22, no. 4, pp. 1396–1407, July 2014.

[19] S. Weerakkody, Y. Mo, and B. Sinopoli, “Detecting integrity attacks on
control systems using robust physical watermarking,” in Decision and
Control (CDC), 2014 IEEE 53rd Annual Conference on, Dec 2014,
pp. 3757–3764.

[20] Y. Mo, S. Weerakkody, and B. Sinopoli, “Physical authentication
of control systems: Designing watermarked control inputs to detect
counterfeit sensor outputs,” Control Systems, IEEE, vol. 35, no. 1, pp.
93–109, Feb 2015.

[21] F. Miao, M. Pajic, and G. Pappas, “Stochastic game approach for
replay attack detection,” in Decision and Control (CDC), 2013 IEEE
52nd Annual Conference on, Dec 2013, pp. 1854–1859.

[22] T. M. Cover and J. A. Thomas, Elements of Information Theory
(Wiley Series in Telecommunications and Signal Processing). Wiley-
Interscience, 2006.

[23] S. Kullback, Information theory and statistics. Courier Corporation,

1968.

[24] J. A. Goguen and J. Meseguer, “Security policies and security models,”
in IEEE Symposium on Security and Privacy, 1982, pp. 11–20.
[25] D. M. Volpano and G. Smith, “Probabilistic noninterference in a
concurrent language,” Journal of Computer Security, vol. 7, no. 1,
1999.

[26] G. Smith, “On the foundations of quantitative information ﬂow,” in
Foundations of Software Science and Computational Structures, 12th
International Conference, FOSSACS 2009, Held as Part of the Joint
European Conferences on Theory and Practice of Software, ETAPS
2009, York, UK, March 22-29, 2009. Proceedings, 2009, pp. 288–302.
[27] R. E. Kalman, “A new approach to linear ﬁltering and prediction
problems,” Journal of Fluids Engineering, vol. 82, no. 1, pp. 35–45,
1960.

[28] R. K. Mehra and J. Peschon, “An innovations approach to fault
detection and diagnosis in dynamic systems,” Automatica, vol. 7, no. 5,
pp. 637–640, 1971.

[29] A. S. Willsky, “On the invertibility of linear systems,” IEEE Transac-
tions on Automatic Control, vol. 19, no. 3, pp. 272–274, 1974.

