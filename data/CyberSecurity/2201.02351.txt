IEEE JOURNAL, VOL. XX, NO. XX, XXXX 202X

1

Asymptotic Security using Bayesian Defense
Mechanism with Application to Cyber Deception

Hampei Sasahara, Member, IEEE, Henrik Sandberg, Senior Member, IEEE

2
2
0
2

b
e
F
8

]

R
C
.
s
c
[

2
v
1
5
3
2
0
.
1
0
2
2
:
v
i
X
r
a

Abstract— This paper addresses the question whether
model knowledge can guide a defender to appropriate
decisions, or not, when an attacker intrudes into control
systems. The model-based defense scheme considered in
this study, namely Bayesian defense mechanism, chooses
reasonable reactions through observation of the system’s
behavior using models of the system’s stochastic dynam-
ics, the vulnerability to be exploited, and the attacker’s
objective. On the other hand, rational attackers take de-
ceptive strategies for misleading the defender into mak-
ing inappropriate decisions. In this paper, their dynamic
decision making is formulated as a stochastic signaling
game. It is shown that the belief of the true scenario has
a limit in a stochastic sense at an equilibrium based on
martingale analysis. This fact implies that there are only
two possible cases: the defender asymptotically detects
the attack with a ﬁrm belief, or the attacker takes actions
such that the system’s behavior becomes nominal after
a ﬁnite time step. Consequently, if different scenarios re-
sult in different stochastic behaviors, the Bayesian de-
fense mechanism guarantees the system to be secure in
an asymptotic manner provided that effective countermea-
sures are implemented. As an application of the ﬁnding,
a defensive deception utilizing asymmetric recognition of
vulnerabilities exploited by the attacker is analyzed. It is
shown that the attacker possibly stops the attack even if
the defender is unaware of the exploited vulnerabilities as
long as the defender’s unawareness is concealed by the
defensive deception.

Index Terms— Bayesian methods, game theory, intrusion

detection, security, stochastic systems.

I. INTRODUCTION

S OCIETAL monetary loss from cyber crime is estimated to

be about a thousand billion USD per year presently, and
even worse, a rising trend can be observed [1]. Another trend
is that not only information systems but also control systems,
which are typically governed by physical laws, are exposed
to cyber threats as demonstrated by recent
incidents [2]–
[5]. Deception is a key notion to predict the consequence of
incidents. Rational attackers take deceptive strategies, i.e., the
attacker tries to conceal her existence and even mislead the
defender into taking inappropriate decisions. An example of

H. Sasahara is with the Department of Systems and Control Engi-
neering, Tokyo Institute of Technology, Tokyo, 152-8552 Japan e-mail:
sasahara@sc.e.titech.ac.jp.

H. Sandberg is with the Division of Decision and Control Systems,
KTH Royal Institute of Technology, Stockholm, SE-100 44 Sweden e-
mail: hsan@kth.se.

This work was supported by Swedish Research Council grant 2016-

00861.

Manuscript received Xxx xx, 20xx; revised Xxx xx, 20xx.

deception is replay attacks, which hijacks sensors of the plant,
eavesdrops the nominal data transmitted when the system is
operated under normal conditions, and replays the observed
nominal data during the execution of another damaging attack.
A replay attack was executed in the Stuxnet incident, and
it was an essential factor leading to serious damage in the
targeted plant [6]. The incident suggests that prevention of
deception is a fundamental requirement for secure system
design.

Supposing the situation where an attacker might intrude
into a control system with which a defense mechanism is
this paper addresses the following question:
implemented,
Can model knowledge guide the defender to appropriate
decisions against attacker’s deceptive strategies? Speciﬁcally,
we consider the case where the stochastic model of the control
system, the vulnerability to be exploited, and the objective of
the attacker are known. The setting naturally leads to Bayesian
defense mechanisms, which monitor the system’s behavior
and form a belief on the existence of the attacker using
the model. If the system’s behavior is inconsistent with the
nominal one, the belief increases owing to Bayes’ rule. When
the belief is strong enough, the Bayesian defense mechanism
proactively carries out a proper reaction. On the other hand,
we also suppose a powerful attacker who knows the model
and the defense scheme to be implemented. The attacker aims
at achieving her objective while avoiding being detected by
deceiving the defender.

For mathematical analysis, we formulate the decision mak-
ing as a dynamic game with incomplete information. More
speciﬁcally, we refer to the game as a stochastic signaling
game, because it is a stochastic game [7] in the sense that
the system’s dynamics is given as a Markov decision process
(MDP) governed by two players and it is also a signaling
game [8] in the sense that one player’s type is unknown to
the opponent. In this game, the attacker strategically chooses
harmful actions while avoiding being detected, while the
defender, namely, the Bayesian defense mechanism, chooses
appropriate counteractions according to her belief.

Based on the game-theoretic formulation, we ﬁnd that model
knowledge can always lead the defender to appropriate deci-
sions in an asymptotic sense as long as the system’s dynamics
admits no stealthy attacks. More speciﬁcally, there are only
two possible cases: one is that the defender asymptotically
forms a ﬁrm belief on the existence of an attacker and the
other is that the attacker continues to take harmless actions
after a ﬁnite time such that the system results to nominal
behavior. This ﬁnding leads to the conclusion that the Bayesian

 
 
 
 
 
 
2

IEEE JOURNAL, VOL. XX, NO. XX, XXXX 202X

defense mechanism guarantees the system to be secure in an
asymptotic manner provided that an effective countermeasure
is implemented. The result indicates the powerful defense
capability achieved by model knowledge.

The analysis means that the defender always wins in an
asymptotic manner when the stochastic model of the system
is available and the vulnerability exploited for the intrusion
is known and modeled. However, in practice, it is hard to be
aware of all possible vulnerabilities in advance. As an appli-
cation of the ﬁnding above, we consider defensive deception
using blufﬁng that utilizes asymmetric recognitions between
the attacker and the defender. Speciﬁcally, we suppose that,
the defender is unaware of the exploited vulnerability but the
attacker is unaware of the defender’s unawareness. If the state
of the system does not possess any information about the
defender’s recognition on the vulnerability, the attacker cannot
identify whether the defender is aware of the vulnerability, or
not. The result obtained in the former part suggests that the
attacker may possibly stop the execution in the middle of the
attack if the defender’s reactions affect only the attacker’s util-
ity without inﬂuence to the system’s behavior. The difﬁculty
of the analysis is that standard incomplete information games,
which assume common prior, cannot describe the situation
of interest. The common prior implicitly assumes that the
attacker is aware of the defender’s unawareness. To overcome
the difﬁculty, we employ the Mertens-Zamir model, which
can represent incomplete information games without common
prior assumption, using the notion of belief hierarchy [9],
[10]. Based on the setting, we show, in a formal manner, that
the defensive deception effectively works when the attacker
strongly believes that the defender is aware of the vulnerability.

Related Work

Model-based security analysis helps the system designer
to prioritize security investments [11]. Attack graphs [12]
and attack trees [13] are basic models of vulnerabilities,
attacks, and consequences. Incorporating defensive actions
into the graphical representation induces defense trees [14].
For dynamic models, attack countermeasure trees, partially
observable MDP, and Bayesian network model have been
used [15]–[17]. Those probabilistic models naturally lead to
Bayesian defense mechanisms, such as Bayesian intrusion
detection [18], [19], Bayesian intrusion response [20], and
Bayesian security risk management [21]. Meanwhile,
the
model of the dynamical system to be protected is also used for
control system security [22], [23]. For example, identifying
existence of stealthy attacks and removing the vulnerability
require the dynamical model [24], [25], and attack detection
performance can be enhanced by model knowledge [26].
Our Bayesian defense mechanisms can be interpreted as a
general description of those approaches. This work reveals a
fundamental property of such commonly used model-based
defense schemes.

Game theory is a standard approach to modeling the deci-
sion making in cyber security, where there inevitably arises a
need to address strategic interactions between the attacker and
the defender [27], [28]. In particular, games with incomplete

information play a crucial role in deceptive situations [29]–
[32]. The modeling in this study follows the signaling game
framework in [33], [34]. Our main concern is especially on
asymptotic phenomena in the dynamic deception and effec-
tiveness of model knowledge.

Our ﬁnding is based on analysis of an asymptotic behavior
of Bayesian inference. The convergence property of Bayesian
inference on the true parameter is referred to as Bayesian
consistency, which has been investigated mainly in the context
of statistics [35], [36]. However, those existing results are basi-
cally applicable only to independent and identically distributed
(i.i.d.) samples because the discussion mostly relies on the
strong law of large numbers (SLLN). Although there is an
extension to Markov chains [37], the observable variable in our
work is not even Markov. Indeed, sophisticated attackers can
choose strategies such that the states at all steps are correlated
with the entire previous trajectory. Thus, existing results for
Bayesian consistency cannot be applied to our problem in a
straightforward manner.

Preliminary versions of this work have been presented
in [38], [39], but
they made the claim of Theorem 2 as
an assumption instead of proving it. Moreover, they did not
include rigorous proofs of the claims in Section III and
analysis of the blufﬁng proposed in Section IV.

Organization and Preliminaries

In Section II, we present a motivating example of water
supply networks, and subsequently, formulate the decision
making as a stochastic signaling game. Section III analyzes
the consequence of the formulated game and shows that
Bayesian defense mechanisms can achieve asymptotic security
of the system to be protected. In Section IV, we analyze a
defensive deception that utilizes asymmetric recognition as
an application of the ﬁnding of Section III. The game of
interest is reformulated using the Mertens-Zamir model. It
is shown that the attacker possibly stops the execution even
if the defender is unaware of the exploited vulnerabilities as
long as the defender’s belief is concealed. Section V veriﬁes
the theoretical results through numerical simulation. Finally,
Section VI concludes and summarizes the paper.

Let N, Z+, and R be the sets of natural numbers, non-
negative integers, and real numbers, respectively. The k-ary
Cartesian power of the set X is denoted by X k. The tuple
(x0, . . . , xk) is denoted by x0:k. The cardinality of a set X is
denoted by |X |. For a set X , the Kronecker delta denoted by
δ : X × X → {0, 1} is deﬁned by δ(x, y) = 1 if x = y and
δ(x, y) = 0 otherwise. The σ-algebra generated by a random
variable X is denoted by σ(X). For a sequence of events Ek
for k ∈ N, the supremum set ∩∞
k=N Ek, namely, the event
where Ek occurs inﬁnitely often, is denoted by {Ek i.o.}.
Jensen’s inequality, which is often applied in this paper, is
given as follows: For a real convex function ϕ and a ﬁnite set
X , the inequality

N =1∪∞

(cid:80)

x∈X p(x)ϕ(a(x)) ≥ ϕ (cid:0)(cid:80)

x∈X p(x)a(x)(cid:1)

(1)

holds where a : X → R and p : X → [0, 1] that satisﬁes
the equation (cid:80)
x∈X p(x) = 1. The inequality is reversed if ϕ

SASAHARA et al.: ASYMPTOTIC SECURITY USING BAYESIAN DEFENSE MECHANISM WITH APPLICATION TO CYBER DECEPTION

3

Fig. 1. Motivating example: water tank system connected to a reservoir
within a water distribution network. The programmable logic controller
(PLC) transmits on/off control signals to the pump and the valve monitor-
ing the state, the water level of the tank. In the scenario, an adversarial
software possibly intrudes into the PLC and then the infected PLC tries
to cause overﬂow by sending inappropriate control signals without being
detected. A Bayesian defense mechanism, which utilizes the data of the
monitored state and forms her belief on existence of the attacker based
on the system model, is also equipped to deal with the attack.

is concave. The generalized Borel-Cantelli’s second lemma is
given as follows [40, Theorem 4.3.4]: Let Fk for k ∈ Z+ be
a ﬁltration of a probability space (Ω, F, P) with F0 := {∅, Ω}
and let Ek for k ∈ Z+ be a sequence of events with Ek ∈
Fk+1. Then

{Ek i.o.} = {ω ∈ Ω : (cid:80)∞

k=0

P(Ek|Fk)(ω) = ∞} .

(2)

The appendix contains the proofs of the claims in the paper.

II. MODELING USING STOCHASTIC SIGNALING GAMES

A. Motivating Example

As a motivating example, we consider water distribution
networks (WDNs), which supply drinking water of suitable
quality to customers. Because of their indispensability to our
life, WDNs are an attractive target for adversaries and expose
their architecture to cyber-physical attacks [41]. In particular,
we treat the water tank system illustrated by Fig. 1, where a
tank is connected to a reservoir within a WDN. The amount
of the water in the tank varies due to usage for drinking
and ﬂow between the external network. Thus the tank system
is needed to be properly controlled through actuation of the
pump and the valve to keep the water amount within a desired
range [42]. A programmable logic controller (PLC) transmits
on/off control signals to the pump and the valve monitoring
the state, namely, the water level of the tank. The dynamics
is modeled as a MDP, where the state space and the action
space are given by quantized water levels and ﬁnite control
actions. Interaction to the external network is modeled as the
randomness in the process.

We here suppose an attack scenario considered in [43].
The adversary succeeds to hijack the PLC and can directly
manipulate its control logic. Such an intrusion can be carried
out by stealthy and evasive maneuvers in advanced persistent
threats [44]. The objective of the attack is to damage the sys-
tem by causing water overﬂow through inappropriate control
signals without being detected. To deal with this attack, we
suppose that a Bayesian defense mechanism, which utilizes the

(a)

(b)

Fig. 2. Possible behaviors of the belief on existence of an attacker. (a):
Behavior of the belief on existence of an attacker. The belief is increased
during attacks while it is expected to be decreased without attacks.
(b): Behavior of the belief when the Bayesian defense mechanism
is permanently deceived. The belief oscillates and the attack can be
executed intermittently and indeﬁnitely.

data of the monitored state and forms her belief on existence
of the attacker based on the system model, is equipped with
the water tank. The Bayesian defense mechanism chooses a
proper reaction by identifying if the system is under attack
through an observation of the state. If the system’s behavior
is highly suspicious, for example,
the defense mechanism
takes an aggressive reaction such as log analysis, dispatch of
operators, or emergency shutdown.

The defender’s belief on existence of an attacker plays a
key role to analyze the consequence of the threat. When the
attacker executes an attack, the system’s behavior becomes
different from the one of the normal operation and accordingly
the belief is increased. Conversely, if the attacker stops the at-
tack by choosing proper control signals, the belief is expected
to be decreased as depicted by Fig. 2a. Our main interest
in this study is to investigate defense capability achieved by
the Bayesian defense mechanism against permanent deception,
namely, intermittent attacks that may cause oscillation of the
belief as illustrated by Fig. 2b.

B. Modeling using Stochastic Signaling Game

We introduce the general description based on dynamic
games with incomplete information. In particular, we refer to
the game as a stochastic signaling game where the system’s
dynamics is given as a MDP and the type of a player is
unknown to the opponent.

The system to be protected with a Bayesian defense mech-
anism is depicted in Fig. 3. The system is modeled by a ﬁnite
MDP governed by two players as in standard stochastic games.
Formally, the MDP considered in this paper is given by the
tuple M := (X , A, R, P ) where X is a ﬁnite state space, A
and R are ﬁnite action spaces, P : X × X × A × R → [0, 1]
is a transition probability. The state at the kth step is denoted
by xk ∈ X . There is an agent who can alter the behavior of
the system through an action ak ∈ A for k ∈ Z+. We refer to

4

IEEE JOURNAL, VOL. XX, NO. XX, XXXX 202X

do not specify S here, it can be taken to be any set of history-
dependent strategies. While we consider a general strategy set
in Sec. III, we impose a constraint into S in Sec. IV.

Fig. 3. Block diagram of the system to be protected using the Bayesian
defense mechanism. The system is governed by actions and reactions,
which are decided by the sender and the receiver, respectively. The
sender type θb means that the system is normally operated. The other
type θm means that there exists an attacker who executes malicious
actions. The receiver is the Bayesian defense mechanism that forms
her belief on the existence of an attacker utilizing the measured data
and chooses reactions based on the belief.

the agent as sender as in standard signaling games. Based
on the measured output, the Bayesian defense mechanism,
called a receiver, chooses an action rk ∈ R at each time
step. We henceforth refer to rk as a reaction for emphasizing
that rk denotes a counteraction against potentially malicious
attacks. The system dynamics is given by P , with which the
transition probability from x to x(cid:48) with a and r is denoted by
P (x(cid:48)|x, a, r). To eliminate the possibility of trivial stealthy
attacks, we assume that the system’s behavior varies in a
stochastic sense when different actions are taken.

Assumption 1 For any x ∈ X and r ∈ R, there exists x(cid:48) ∈ X
such that

P (x(cid:48)|x, a, r) (cid:54)= P (x(cid:48)|x, a(cid:48), r)

for different actions a (cid:54)= a(cid:48).

Next, we determine the class of the decision rules. Let
θ ∈ Θ denote the type of the sender. For simplicity, the type
is assumed to be binary, i.e., Θ = {θb, θm}, where θb and
θm correspond to benign and malicious senders, respectively.
The types θb and θm describe the situations where there does
not and does exist an adversary, respectively. The true type
θ is known to the sender, but unknown to the receiver. Let
ss
:= (sr
k)k∈Z+ denote the sender’s
and receiver’s strategy, respectively. It is assumed that the
receiver’s available information about the sender type is only
the state, i.e., she cannot observe her instantaneous utility,
deﬁned below, and the sender’s action. Similarly, it is assumed
that the sender can observe only the state and her action. The
strategies at the kth step with the available information are
given by

k)k∈Z+ and sr

:= (ss

k : Θ × Hs
ss
k and hr

k → A,
k ∈ Hr

k ∈ Hs

k : Hr
sr

k → R

k are histories at the kth step

where hs
given by

Once a strategy proﬁle is ﬁxed, the stochastic property of
the system is induced. Construct the canonical measurable
space (Ω, F) of the MDP where Ω := Π∞
k=0(X × A × R)
and F is its product σ-algebra [45, Chapter 2]. We denote
ω = ((x0, a0, r0), (x1, a1, r1), . . .) ∈ Ω. The random variables
Xk, Ak, and Rk are deﬁned on the measurable space (Ω, F)
by the projections of ω such that Xk(ω) := xk, Ak(ω) := ak,
Rk(ω) := rk. Denote the random variable of the entire history
by Hk := ((X0, A0, R0), . . . , (Xk−1, Ak−1, Rk−1), Xk) and
its realization by hk ∈ Hk. The random variables of the
available information histories of the sender and the receiver
are denoted by H s
k := ((X0, A0), . . . , (Xk−1, Ak−1), Xk)
and H r
k := ((X0, R0), . . . , (Xk−1, Rk−1), Xk), respectively,
whose realizations are given by (3). For a true type θ ∈ Θ
and an initial state x0 ∈ X , the probability measure on (Ω, F)
induced by s is denoted by Ps

, which satisﬁes

θ,x0






Ps
Ps
Ps
Ps

θ,x0

θ,x0

θ,x0

(X0 = x0) = 1,
(Ak = ak|H s
(Rk = rk|H r
(Xk+1 = xk+1|Hk = hk, Ak = ak, Rk = rk)

k) = δ(ak, ss
k) = δ(rk, sr

k(θ, hs
k(hr
k)),

k = hs
k = hr

k)),

θ,x0
= P (xk+1|xk, ak, rk)

by Ps

for any k ∈ Z+. Throughout this paper, we ﬁx the initial state
for simplicity and denote Ps
θ where the subscript x0
θ,x0
is omitted. To simplify the notation, we denote the conditional
probability mass function by
θ(xk+1|x0:k) := Ps
ps
The expectation with respect to Ps

θ(Xk+1 = xk+1|X0 = x0, . . . , Xk = xk).

θ is denoted by Es
θ.

We introduce receiver’s belief systems on the sender type.
A belief system is a tuple of the functions πk : Θ × X k+1 →
[0, 1] for k ∈ Z+
1. As the belief is close to one, the receiver
believes that the sender is malicious with a strong conﬁdence.
The initial belief π0(θ|x0) is simply denoted by π0(θ) or πθ
0.
In Sec. III the initial belief is assumed to be known to both
players, i.e., we make the common prior assumption. On the
other hand, in Sec. IV, we consider the case where the initial
belief is unknown to the sender.

A Bayesian defense mechanism forms her belief according
to Bayes’ rule. When the following conditions are satisﬁed,
the belief system π := (πk)k∈Z+ is said to be consistent with
the strategy proﬁle s:

• The initial belief satisﬁes (cid:80)
• For any k ∈ Z+ and x0:k+1 ∈ X k+2, the belief system

θ∈Θ π0(θ) = 1.

satisﬁes

πk+1(θ|x0:k+1) = f s

k+1(θ, x0:k+1)πk(θ|x0:k)

hs
k = (x0:k, a0:k−1),

hr
k = (x0:k, r0:k−1).

(3)

where

Although this paper treats pure strategies, our analysis can be
extended to mixed or behavioral strategies in a straightforward
manner. The strategy proﬁle is denoted by s := (ss, sr). The
sender’s and receiver’s admissible strategy sets are denoted
by S s and S r, respectively. The set of admissible strategy
proﬁles is denoted by S := S s × S r. Note that, although we

f s
k+1(θ, x0:k+1) :=

(cid:80)

φ∈Θ ps

ps
θ(xk+1|x0:k)
φ(xk+1|x0:k)πk(φ|x0:k)

(4)

1The belief function at the kth step should depend on the receiver’s available
k rather than the state trajectory x0:k ∈ X k+1. However,
k in the presented formulation, we

information hr
since x0:k is a sufﬁcient statistic for hr
deﬁne the belief to be dependent only of the state to simplify notation.

k ∈ Hr

SASAHARA et al.: ASYMPTOTIC SECURITY USING BAYESIAN DEFENSE MECHANISM WITH APPLICATION TO CYBER DECEPTION

5

as long as (cid:80)

φ∈Θ ps

φ(xk+1|x0:k)πk(φ|x0:k) (cid:54)= 0.

As the solution concept used in the analysis, we consider
uniform equilibria. Let U s : Θ × X × A × R → R be the
sender’s instantaneous utility. For a given strategy proﬁle s ∈
S and type θ ∈ Θ, the sender’s expected average utility up to
the T th step is given by
(cid:34)

¯U s
T (s, θ) := Es
θ

1
T + 1

T
(cid:88)

k=0

(cid:35)
U s(θ, Xk, Ak, Rk)

.

Similarly, with the receiver’s instantaneous utility given by
U r : Θ × X × A × R → R, the receiver’s expected average
utility up to the T th step is given by

Fig. 4.
Distributions of the belief sequence when there exists an
attacker. Lemma 1 and Theorem 1 claim that its expectation is non-
decreasing over time and the belief has a limit. When the adversary
stops the attack, the belief is invariant.

¯U r

T (s, π)
(cid:88)

:=

Es
θ

θ∈Θ

(cid:34)

1
T + 1

T
(cid:88)

k=0

U r(θ, Xk, Ak, Rk)πk(θ|X0:k)

(cid:35)
.

Note that, because the receiver does not know the true type,
she takes the expectation over the possible types based on her
belief system. We denote the limits by
( ¯U s
( ¯U s(s, θ), ¯U r(s, π)) := lim
T →∞

T (s, θ), ¯U r

T (s, π)),

assuming they exist. Under this notation, the strategy proﬁle
s = (ss, sr) is said to be a Bayesian-Nash equilibrium (BNE)
if ( ¯U s(s, θ), ¯U r(s, π)) satisﬁes

(cid:26) ss ∈ BRs(sr, θ),
sr ∈ BRr(ss, π)

∀θ ∈ Θ,

with a consistent belief system π, where BRs and BRr are
best responses deﬁned by

BRs(sr, θ) := arg max

BRr(ss, π) := arg max

˜ss∈S s

¯U s((˜ss, sr), θ),
¯U r((ss, ˜sr), π).

˜sr∈S r

Note that, our analysis can be extended to the case of general
objective functions rather than expected average utilities as
long as the utilities result in detection-averse strategies, which
are formally stated in Deﬁnition 1 below.

We deﬁne the game formulated above by

G1 := (M, S, U, Θ, π0),

(5)

where the initial belief is common information. This game
belongs to the class of incomplete, imperfect, and asymmetric
information stochastic games. Owing to the existence of the
type θ, which is unknown to the receiver, the information
is incomplete. Because the actions taken by each player are
unobservable to the opponent, the information is imperfect and
asymmetric. Although investigating existence and computing
equilibria of the game are challenging, we discusses properties
of equilibria on the premise that they exist and are given
because our interest is in the consequence of the threat.

III. ANALYSIS: ASYMPTOTIC SECURITY

In this section, we analyze asymptotic behaviors of beliefs
and actions when the utilities are detection-averse. It is shown
that the system is guaranteed to be secure in an asymptotic
manner as long as the defender possesses an effective coun-
teraction.

A. Belief’s Asymptotic Behavior

The random variable of the belief at the kth step πθ

k : Ω →

[0, 1] is given by

πθ
k(ω) := πk(θ|X0:k(ω)).

Recall that πθ
k represents the receiver’s conﬁdence on existence
of an attacker. If the belief is low in spite of existence
of malicious signals, this means that the Bayesian defense
mechanism is deceived. Because we are interested in whether
the Bayesian defense mechanism is permanently deceived, or
not, we examine asymptotic behavior of the belief.

We ﬁrst investigate increment of the belief sequence. The

following lemma is the key to our analysis.

Lemma 1 Consider the game G1. The belief on the true type
πθ
k is a submartingale with respect to the probability measure
Ps
θ and the ﬁltration σ(X0:k) for any type θ, strategy proﬁle
s, and consistent belief system π.

Lemma 1 implies that the belief on the true type is non-
decreasing in a stochastic sense. As a direct conclusion of this
lemma, the following theorem holds.

Theorem 1 Consider the game G1. There exists an integrable
random variable πθ
∞ : Ω → [0, 1] such that

lim
k→∞

k = πθ
πθ
∞

Ps

θ−a.s.

for any type θ, strategy proﬁle s, and consistent belief system
π.

Theorem 1 implies that the belief has a limit even if an
intermittent attack is executed. Fig. 4 depicts the distributions
of the belief sequence when there exists an attacker. Owing
to the model knowledge, if the adversary stops the attack at
some time step then the belief is invariant, which is illustrated
as the transition of the belief at k = 1 in Fig. 4. Moreover,
the expectation of the belief is non-decreasing over time as
claimed by Lemma 1. Thus, there exists a limit πθm
∞ as shown
at the right of Fig. 4.

We next investigate the limit. An undesirable limit is πθ
∞ =
0, which means that the defender is completely deceived. We
show that this does not happen as long as the initial belief is
nonzero. The following lemma holds.

6

IEEE JOURNAL, VOL. XX, NO. XX, XXXX 202X

Lemma 2 Consider the game G1. If πθ
0 > 0 for any type θ,
k) with any basis converges Ps
then log(πθ
θ-almost surely to an
integrable random variable as k → ∞ for any type θ, strategy
proﬁle s, and consistent belief system π.

B. Asymptotic Security

It has turned out that the belief has a positive limit. To
clarify our interest, we deﬁne the notion of detection-averse
utilities.

Lemma 2 leads to the following theorem.

Theorem 2 Consider the game G1. If πθ

0 > 0 then

∞ > 0 Ps
πθ

θ−a.s.

for any type θ, strategy proﬁle s, and consistent belief system
π.

Theorem 2 implies that the complete deception described by
πθ
∞ = 0 does not occur.
Remark: Theorems 1 and 2 can heuristically be justiﬁed
from an information-theoretic perspective as follows. Suppose
that the state sequence x0:k is observed at the kth step. Then
the belief is given by

π0(θ)

πk(θ|x0:k) =

(cid:80)

φ(cid:54)=θ

ps
φ(x0:k)
θ(x0:k) π0(φ) + π0(θ)
ps
π0(θ)
k (x0:k))π0(φ) + π0(θ)

φ(cid:54)=θ exp(kSφ

=

(cid:80)

(6)

θ(x0:k) and ps

where ps
functions of x0:k with respect to Ps

φ(x0:k) are the joint probability mass
φ, respectively, and

θ and Ps

Sφ
k (x0:k) :=

1
k

k
(cid:88)

i=1

log

ps
φ(xi|x0:i−1)
ps
θ(xi|x0:i−1)

.

Assuming that ps
bution ps

θ(xk|x0:k−1) approaches a stationary distri-

θ(x) on X and SLLN can be applied, we have

lim
k→∞

Sφ
k = Ex∼ps

θ

(cid:2)log ps

φ(x)/ps

φ(x)(cid:3) = −DKL(ps

θ||ps
φ)

where DKL denotes the Kullback-Leibler divergence. Since
DKL is nonnegative for any pair of distributions, Sφ
k converges
to a nonpositive number, which results in convergence of πθ
k. If
ps
θ (cid:54)= ps
k becomes negative,
and hence limk→∞ exp(kSφ

φ for any φ ∈ Θ \{θ}, the limit of Sφ

k (x0:k)) = 0, which leads to

lim
k→∞

πk(θ|x0:k) =

(cid:88)

φ(cid:54)=θ

lim
k→∞

= 1.

π0(θ)
k (x0:k))π0(φ) + π0(θ)

exp(kSφ

Thus, the belief on the true type converges to one. Such a
convergence property of Bayesian estimator on the true param-
eter, referred to as Bayesian consistency, has been investigated
mainly in the context of statistics [35], [36]. In this sense,
Theorems 1 and 2 can be regarded as another representation of
Bayesian consistency. However, note again that this discussion
is not a rigorous proof but a heuristic explanation because
the state is essentially non-i.i.d. and even non-ergodic in our
game-theoretic formulation. Therefore, this discussion cannot
be applied to our problem.

Deﬁnition 1 (Detection-averse Utilities) A pair (U s, U r) in
the game G1 is said to be detection-averse utilities when
∞ < 1 Ps
πθm
θm

−a.s.

(7)

for any BNE s and consistent belief system π.

Deﬁnition 1 characterizes utilities with which the malicious
sender avoids having the defender form a ﬁrm belief on the
existence of an attacker. Naturally, the reasonable strategy
should be detection-averse as long as the defender possesses
an effective counteraction. If the utilities of interest are
not detection-averse, this means that the defense mechanism
cannot cope with the attack. For protecting such systems,
appropriate counteractions should be implemented beforehand.
Suppose that there is an effective countermeasure, and hence
the utilities are detection-averse. A simple malicious sender’s
strategy that satisﬁes (7) is to imitate the benign sender’s
strategy after a ﬁnite time step. We give a formal deﬁnition of
such strategies.

Deﬁnition 2 (Asymptotically Benign Strategy) A strategy
proﬁle s in the game G1 is said to be asymptotically benign
when

(cid:16)

lim
k→∞

δ

(cid:17)

Aθm

k , Aθb

k

= 1 Ps
θm

−a.s.

where Aθ
deﬁned by

k is the action taken by the sender with the type θ

Aθ

k := ss

k(θ, H s

k).

The objective of this subsection is to show that Bayesian
defense mechanisms can restrict all reasonable strategies to be
asymptotically benign as long as an effective countermeasure
is implemented.

As a preparation for proving our main claim, we investigate
the asymptotic behavior of state transition. From Theorems 1
and 2, we can expect that the state eventually loses information
on the type, which is justiﬁed by the following lemma.

Lemma 3 Consider the game G1 with detection-averse utili-
ties. If πθm
(cid:12)
(cid:12)ps
θm

(Xk+1|X0:k) − ps
θb

(Xk+1|X0:k)(cid:12)

(cid:12) = 0 Ps
θm

0 > 0, then

−a.s.

lim
k→∞

for any BNE s and consistent belief system π.

Under Assumption 1, which eliminates possibility of
stealthy attacks, Lemma 3 implies that the actions themselves
must be identical. This fact derives the following theorem, one
of the main results in this paper.

Theorem 3 Consider the game G1 with detection-averse utili-
ties. Let Assumption 1 hold and assume πθm
0 > 0. Then, every
BNE of G1 is asymptotically benign.

SASAHARA et al.: ASYMPTOTIC SECURITY USING BAYESIAN DEFENSE MECHANISM WITH APPLICATION TO CYBER DECEPTION

7

Theorem 3 implies that the malicious sender’s action con-
verges to the benign action. Equivalently, an attacker nec-
essarily behaves as a benign sender after a ﬁnite time step.
Therefore, the system is guaranteed to be secure in an asymp-
totic manner, i.e., Bayesian defense mechanisms can prevent
deception in an asymptotic sense. This result indicates the
powerful defense capability achieved by model knowledge.

IV. APPLICATION: ANALYSIS OF DEFENSIVE DECEPTION
UTILIZING ASYMMETRIC RECOGNITION

A. Idea of Defensive Deception using Blufﬁng

The result in Section III claims that the defender, namely,
the Bayesian defense mechanism, always wins in an asymp-
totic manner when the stochastic model of the system is
available and the vulnerability to be exploited for intrusion
is known and modeled. The latter condition is quantitatively
described by the condition π0(θm) > 0. Although the derived
result proves a quite powerful defense capability, it is also
true that it is almost impossible to be aware of all possible
vulnerabilities in advance. Moreover, it is also challenging to
implement effective countermeasures for all scenarios and to
compute the equilibrium of the dynamic game.

In this section, as an application of the ﬁnding in the
previous section, we consider defensive deception using bluff-
ing that utilizes asymmetric recognitions between the at-
tacker and the defender. Suppose that an attacker exploits
a vulnerability of which the defender is unaware but
the
attacker is unaware of the defender’s unawareness. Then their
recognition becomes asymmetric in the sense that the attacker
does not correctly recognize the defender’s recognition of the
vulnerability. This situation naturally arises in practice because
defender’s recognition is private information. By utilizing the
asymmetric recognition, the defender can possibly deceive
the attacker such that the attacker believes that the defender
might be aware of the vulnerability and carrying out effective
counteractions. Speciﬁcally, we consider the blufﬁng strategies
with which system’s state does not possess information about
the defender’s belief. For instance, if the defender chooses
the reactions that affect only the players’ utilities without
inﬂuence to the system, the state is independent of the reaction.
By concealing the defender’s unawareness,
the defender’s
recognition, which is quantiﬁed by her belief, is completely
unknown to the attacker over time.

The defensive deception is possibly able to force the attacker
to stop the execution in the middle of the attack even if the
defender is actually unaware of the exploited vulnerability.
For instance, consider the example in Sec. II-A and suppose
that emergency shutdown of the system can be carried out
by the defender. Suppose also that the attacker wants to keep
administrative privileges of the PLC. In this case, the attacker
may rationally terminate her evasive maneuvers after a ﬁnite
time step due to the risk of sudden shutdown. The objective of
this section is to show that the hypothesis is true in a formal
manner.

B. Reformulation using Type Structure

The situation of interest in this section is that the defender
is unaware of the vulnerability to be exploited but the attacker

Fig. 5.
Attacker’s belief on the defender’s belief with symmetric
recognition, which is the case of the game G1. The attacker is aware
of the fact that the defender is unaware of the vulnerability. Moreover,
the defender is aware of the attacker’s awareness.

Fig. 6. Attacker’s belief on the defender’s belief with asymmetric recog-
nition. Because the defender’s true belief is unknown to the attacker,
the attacker forms a belief on both cases that the defender is aware or
unaware of the vulnerability. Moreover, the defender forms a belief on
the attacker’s belief. This process induces the notion of belief hierarchy.

is not necessarily aware of this defender’s unawareness. To
address the uncertainty on defender’s recognition, the attacker
forms her belief on the defender’s belief. Fig. 5 illustrates
the attacker’s belief on the defender’s belief with the common
prior assumption, i.e., the initial defender’s belief is known to
the attacker, which has been made in the previous section.
In this case, the attacker has a ﬁrm belief that the sender
is unaware of the vulnerability. On the other hand, Fig. 6
illustrates the attacker’s belief without
the common prior
assumption. Then the attacker’s belief is no longer ﬁrm as
depicted by the ﬁgure. In addition, because of the lack of the
common prior assumption, the defender also forms another
belief on the attacker’s belief on the defender’s belief on the
existence of an attacker. This procedure repeats indeﬁnitely
and induces inﬁnitely many beliefs.

The notion of belief hierarchy has been proposed to handle
the inﬁnitely many beliefs [9], [10], [46]. A belief hierarchy
is formed as follows. Let ∆(·) denote the set of probability
measures over a set. The ﬁrst-order initial belief is given as
π1
0 ∈ ∆(Θ), which describes the defender’s initial belief on
existence of the attacker. The second-order initial belief is
given as π2
0 ∈ ∆(∆(Θ)), which describes the attacker’s initial
belief on the defender’s ﬁrst-order belief. In a similar manner,
the belief at any level is given, and the tuple of the beliefs at
all levels is referred to as a belief hierarchy.

To handle belief hierarchies, the Mertens-Zamir model has
been introduced [9], [10], [46]. The model considers type
structure, in which a belief hierarchy is embedded. A type
structure consists of players, sets of types, and initial beliefs.
In particular, a type structure for our situation of interest can
be given by

T = ((s, r), (Θ s, Θ r), (πs

0, πr

0))

(8)

8

IEEE JOURNAL, VOL. XX, NO. XX, XXXX 202X

TABLE I
INITIAL BELIEFS ON OPPONENT TYPE

0(·|θr
πr
u)
0(·|θr
πr
a)

θs
b
1
α

θs
m
0
1 − α

πs
0(·|θs
b)
0(·|θs
πs
m)

θr
u
1
β

θr
a
0
1 − β

where (s, r) represents the sender and the receiver, Θ s and
Θ r represent the sets of player types, and πs
0 : Θ r × Θ s →
0 : Θ s × Θ r → [0, 1] represent the initial beliefs.
[0, 1] and πr
0(θr|θs) denotes the sender’s initial belief of the
The value πs
receiver type θr when the sender type is θs, and πr
0(θs|θr)
denotes the corresponding receiver’s initial belief. The ﬁrst-
order initial belief is given by π1
0(θs|θr) for the true
receiver type θr ∈ Θ r, and the second-order initial belief is
0(θr|θs) for the true sender type
given by π2
θs ∈ Θ s. By repeating it, the belief at any level of the belief
hierarchy can be derived from the type structure. Importantly,
for any reasonable belief hierarchy there exists a type structure
that can generate the belief hierarchy of interest. For a formal
discussion, see [9], [10], [46].

0(πr(·|θr)|θs) = πs

0(θs) = πr

We model the situation of interest by using the binary type

sets:

Θ s = {θs

b, θs

m}, Θ r = {θr

u, θr

a}.

b and θs

While θs
m represent benign and malicious senders,
respectively, θr
a represent receivers being unaware and
aware of the vulnerability, respectively. The receiver’s initial
beliefs are set to

u and θr

0(θs
πr

b|θr

u) = 1,

and

0(θs
πr

m|θr

u) = 0

0(θs
πr

b|θr

a) = α,

0(θs
πr

m|θr

a) = 1 − α

with α ∈ [0, 1). The initial beliefs mean that, the receiver θr
u
is unaware of the vulnerability and ﬁrmly believes that the
system is normally operated, while the receiver θr
a is aware
of the vulnerability and suspects existence of an attacker with
probability 1 − α. The sender’s initial beliefs are assumed to
be given by

0(θr
πs

u|θs

b) = 1,

0(θr
πs

a|θs

b) = 0,

TABLE II
CONTRASTING INGREDIENTS OF THE GAMES WITH SYMMETRIC AND
ASYMMETRIC RECOGNITIONS

receiver’s strategy
sender’s belief
receiver’s belief
sender’s utility
receiver’s utility

symmetric recognition
k(hr
sr
k)
N/A
π(θ)
¯U s(s, θ)
¯U r(s, π)

asymmetric recognition
sr
k(θr, hr
k)
πs(θr|θs)
πr(θs|θr)
¯U s(s, θs, πs)
¯U r(s, θr, πr)

sender’s expected average utility up to the T th step is given
by
¯U s

U s(θs, Xk, Ak, Rk)πs

(cid:35)
k(θr|X0:k, θs)

.

T (s, θs, πs) :=
(cid:88)

(cid:34)

Es

θs,θr

θr∈Θ r

1
T + 1

T
(cid:88)

k=0

(9)
The receiver’s expected average utility up to the T th step is
given by
¯U r

U r(θs, Xk, Ak, Rk)πr

k(θs|X0:k, θr)

.

(cid:35)

T (s, θr, πr) :=
(cid:88)

(cid:34)

Es

θs,θr

θs∈Θ r

1
T + 1

T
(cid:88)

k=0

A strategy s is said to be a BNE when the limit of the utilities
( ¯U s(s, θs, πs), ¯U r(s, θr, πr)) satisﬁes
(cid:26) ss ∈ BRs(sr, θs, πs),
sr ∈ BRr(ss, θr, πr),

∀θs ∈ Θ s,
∀θr ∈ Θ r

with consistent belief systems πs and πr where

BRs(sr, θs, πs) := arg max

BRr(ss, θr, πr) := arg max

˜ss∈S s

¯U s((˜ss, sr), θs, πs),
¯U r((ss, ˜sr), θr, πr).

˜sr∈S r

We deﬁne the game formulated above by

G2 := (M, S, U, (Θ s, Θ r), (πs

0, πr

0)),

(10)

where the defender’s initial belief is not common information
in contrast to G1.

In the following discussion, we analyze G2 through G1. To
clarify their relationship, we describe the game G1 using the
modiﬁed formulation. Deﬁne another game

and

0(θr
πs

u|θs

m) = β,

0(θr
πs

a|θs

m) = 1 − β

where

ˆG2 := (M, S, U, (Θ s, Θ r), (ˆπs

0, πr

0)),

with β ∈ [0, 1]. The malicious sender does not know the
true receiver type, i.e., whether the sender is aware of the
vulnerability or not. The given initial beliefs are summarized
in Table I.

In accordance with the introduction of the type structure,
the deﬁnition of strategies, belief systems, and equilibria are
needed to be slightly modiﬁed. The contrasting ingredients
of the game with symmetric recognition and the one with
asymmetric recognition are listed in Table II, where those with
asymmetric recognition can analogically be deﬁned. Denote
the probability measure induced by s ∈ S with types θs ∈ Θ s
and θr ∈ Θ r by Ps
θs,θr . The

θs,θr and the expectation by Es

0(θr
ˆπs

a|θs

m) = 1.

The initial belief means that the adversary believes that the
defender is aware of the vulnerability. The situation of ˆG2 is the
same as that of G1 if the defender is aware of the vulnerability.
Thus, these games lead to the same consequence when the true
types are θs

a. The following lemma holds.

m and θr

Lemma 4 Consider the games G1 and ˆG2. For a strategy
2) in ˆG2, let s1 = (ss
proﬁle ˆs2 = (ˆss
1) be a strategy
proﬁle in G1 such that

2, ˆsr

1, sr

1 := ˆss
ss
2,

1 := ˆsr
sr

2|θr=θr

a

(11)

SASAHARA et al.: ASYMPTOTIC SECURITY USING BAYESIAN DEFENSE MECHANISM WITH APPLICATION TO CYBER DECEPTION

9

2|θr=θr

a is the restriction of ˆsr

where ˆsr
a. Then the
probability measures induced by s1 and ˆs2 are equal when
θs = θs

2 with θr = θr

m and θr = θr

a, i.e.,
Ps1
θs
m
2 ∈ BRs(ˆsr
2, θs

.

= Pˆs2
m,θr
θs
a
m, ˆπs) then ss

Moreover, if ˆss

1 ∈ BRs(sr

1, θm).

We extend the notions of detection-averse utilities and
asymptotically benign strategies to G2. We say utilities in G2 to
be detection-averse when the adversary avoids being detected
if she believes that the defender is aware of the vulnerability.

Deﬁnition 3 (Detection-averse Utilities in G2) A pair of
utilities (U s, U r) in the game G2 is said to be detection-averse
utilities when

lim
k→∞

k(θs
πr

m|θr

a) < 1 Ps
m,θr
θs
a

−a.s.

(12)

for any BNE s of ˆG2 and consistent belief system πr.

Note that, Deﬁnition 3 does not require the adversary to
avoid being detected in the game G2. Even with detection-
averse utilities, the adversary may choose a strategy such
that (12) does not hold unless she has a ﬁrm belief that the
defender is aware of the vulnerability.

Next, we deﬁne desirable strategies that should be achieved
by Bayesian defense mechanisms. We say a strategy in G2 to
be asymptotically benign when it becomes benign regardless
of the defender’s awareness.

Deﬁnition 4 (Asymptotically Benign Strategies in G2) A
strategy proﬁle s in the game G2 is said to be asymptotically
benign when

(cid:16)

δ

lim
k→∞
for any θr ∈ Θ r.

Aθm

k , Aθb

k

(cid:17)

= 1 Ps
m,θr −a.s.
θs

(13)

Note that Deﬁnition 4 requires the strategy to be asymptot-
ically benign for any θr ∈ Θ r. In other words, the strategy
is needed to be asymptotically benign even if the defender is
unaware of the vulnerability.

C. Passively Blufﬁng Strategies

We expect that there exists a chance of preventing attacks
that exploit unnoticed vulnerabilities if the state does not pos-
sess information about the defender’s recognition. To formally
verify this expectation, we deﬁne passively blufﬁng strategies.

Deﬁnition 5 (Passively Blufﬁng Strategies) A strategy pro-
ﬁle s in G2 is said to be a passively blufﬁng strategy proﬁle
when any sender’s belief consistent with s satisﬁes

0(θr|θs) Ps

k(θr|X0:k, θs) = πs
πs

(14)
for any θs ∈ Θ s, θr ∈ Θ r, and k ∈ Z+. A strategy proﬁle set
S in G2 is said to be a passively blufﬁng strategy set when its
all elements are passively blufﬁng.

θs,θr−a.s.

Deﬁnition 5 requires the sender’s belief to be invariant
over time. If the strategy is passively blufﬁng, the adversary

cannot identify whether the defender is aware of the exploited
vulnerability or not even in an asymptotic sense.

Passively blufﬁng strategies can relax the condition for
asymptotically benign strategies. The following lemma holds.

Lemma 5 Consider the game G2. If a passively blufﬁng
strategy proﬁle s satisﬁes

(cid:16)

lim
k→∞

δ

Aθm

k , Aθb

k

(cid:17)

= 1 Ps
m,θr
θs
a

−a.s.

(15)

then s is asymptotically benign.

The difference between (13) and (15) is the required receiver
type. Lemma 5 implies that, if a passively benign strategy pro-
ﬁle is asymptotically benign when the receiver is aware of the
vulnerability, then the strategy is needed to be asymptotically
benign even when the receiver is unaware of the vulnerability.
Remark: Although Deﬁnition 5 depends not only on the
receiver’s strategy but also the sender’s strategy for generality,
the blufﬁng should be realized only by the defender in practice.
A simple defender’s approach to achieving the blufﬁng is to
choose reactions that do not inﬂuence the system’s behavior.
Let Rpb ⊂ R be the set of reactions such that the system’s
dynamics is independent of the reaction, i.e., the transition
probability satisﬁes

P (x(cid:48)|x, a, r) = P (x(cid:48)|x, a, r(cid:48))

(16)

for any x(cid:48) ∈ X , x ∈ X , a ∈ A, r ∈ Rpb, r(cid:48) ∈ Rpb. If the
receiver’s strategy takes only reactions in Rpb, every strategy
proﬁle is passively blufﬁng. Indeed, because the transition
probability is independent of r ∈ Rpb, the probability dis-
tribution of the state is independent of θr. Thus, from Bayes’
rule, we have

k(θr|x0:k, θs) =
πs

0(θr|θs)

θs,φr(x0:k)πs
0(θr|θs)

0(φr|θs)

θs(x0:k)πs

0(φr|θs)

=

(cid:80)

(cid:80)

ps
θs,θr (x0:k)πs
φr∈Θ r ps
ps
θs(x0:k)πs
φr∈Θ r ps
0(θr|θs)
πs
(cid:80)
φr∈Θ r πs
0(θr|θs)
= πs

0(φr|θs)

=

when ps
θs,θr (x0:k) (cid:54)= 0. An example of such reactions is just
analyzing the network log and raising an alarm inside the
operation room without control of the system itself. Note that
the reaction still affects the players’ decision making through
their utility functions even if (16) holds.

D. Analysis

Our expectation can be described in a quantitative form
based on the deﬁnition of passively blufﬁng strategies, which
lead to a simple representation of the sender’s utility. If s is
passively blufﬁng, the sender’s belief is invariant over time.
Thus the malicious sender’s utility with ﬁnite horizon in (9)

10

IEEE JOURNAL, VOL. XX, NO. XX, XXXX 202X

is given by
¯U s
T (s, θs
(cid:88)

=

m, πs)

Es
m,θr
θs

θr∈Θ r
(cid:88)

θr∈Θ r

=

Es
m,θr
θs

(cid:34)

(cid:34)

1
T + 1

1
T + 1

T
(cid:88)

k=0
T
(cid:88)

k=0

U s(θs

m, Xk, Ak, Rk)πs

0(θr|θs

(cid:35)
m)

U s(θs

m, Xk, Ak, Rk)

πs
0(θr|θs

m).

(cid:35)

Fig. 7. State transition diagram of the numerical example.

Hence, the sender’s utility with inﬁnite horizon is given by
(cid:88)

¯U s

θr (s, θs

m)πs

0(θr|θs

m)

(17)

¯U s(s, θs

m, πs) =

where

θr∈Θ r

¯U s

θr(s, θs

m) := lim
T →∞

Es
m,θr
θs

(cid:34)

1
T + 1

T
(cid:88)

k=0

(cid:35)
m, Xk, Ak, Rk)

U s(θs

.

and ¯U s
θr
u

Note that ¯U s
denote the sender’s utilities of the
θr
a
two cases where the defender is aware and unaware of the
vulnerability, respectively. Thus (17) implies that the sender’s
utility is simply given as a sum weighted by her initial beliefs
when the strategy is passively blufﬁng. Therefore, we can
expect that the sender possibly stops the execution in the
0(θr
middle of the attack if πs
m) is sufﬁciently large. We show
existence of such sender’s initial belief. Note that πs
m) =
1 is the trivial case, and thus we consider sender’s initial beliefs
that are strictly less than one.

0(θr

a|θs

a|θs

First, we rephrase the result in Sec. III. Let Snab denote
the set of non-asymptotically-benign strategies in G2. Our aim
here is to show that the set of BNE of G2 does not overlap with
Snab when the attacker strongly believes that the defender is
aware of the vulnerability. It sufﬁces to show that there is no
overlap between the set of BNE of G2 and S ∗
nab := Snab ∩ S ∗
where

S ∗ := { (ss, sr) ∈ S : ss ∈ BRr(sr, θs

b, πs),
sr ∈ BRr(ss, θr, πr), ∀θr ∈ Θ r},

where the benign sender and the receiver with any type take
their best response strategies. Note that, Snab and S ∗ of the
games G2 and ˆG2 are identical because the sets are independent
of the malicious sender’s belief. The following lemma is
another description of the claim of Theorem 3 with respect
to ¯U s
θr
a

and S ∗

nab.

Lemma 6 Consider the game ˆG2 with detection-averse util-
ities. Let Assumption 1 hold. For any strategy proﬁle s =
nab, there exists ˜ss ∈ S s such that
(ss, sr) in S ∗

Dθr

a

(s, ˜ss) > 0

holds where

Dθr (s, ˜ss) := ¯U s

θr((˜ss, sr), θs

m) − ¯U s

θr (s, θs

m).

Lemma 6 implies the existence of a function

g : S ∗

nab → S s

s.t. Dθr

a

(s, g(s)) > 0

for any s ∈ Snab. Thus we have γ ≥ 0 where

γ := inf
s∈S ∗

nab

Dθr

a

(s, g(s)).

(18)

(19)

(20)

(21)

We here make an assumption that Dθr
lower bounded by a positive value.

a

(s, g(s)) is uniformly

Assumption 2 For the game ˆG2, there exists g in (20) such
that the inﬁmum (21) is positive, i.e., γ > 0.

Assumption 2 eliminates the case where the difference
between the sender’s utilities achievable by asymptotically
benign strategies and non-asymptotically-benign strategies is
inﬁnitesimally small.

The following theorem,

the main result of this section,

holds.

Theorem 4 Consider the game G2 with detection-averse util-
ities and a passively blufﬁng strategy set. Let Assump-
tions 1 and 2 hold. Then, there exists a sender’s initial belief
a|θs
πs
0(θr
m) < 1 such that every BNE of G2 is asymptotically
benign.

Theorem 4 implies that the system can possibly be protected
by passively blufﬁng strategies if the attacker strongly believes
that the defender is aware of the vulnerability. The result
suggests importance of concealing defender’s recognition and
effectiveness of defensive deception.

V. SIMULATION

In this section, we conﬁrm the theoretical results through

numerical simulation.

A. Fundamental Setup

We assume the state space and the action space to be binary,
i.e., X = {xn, xa} and A = {ab, am}. The states xn and xa
represent the normal and abnormal states, respectively, and ab
and am represent benign and malicious actions, respectively.
The benign and malicious actions correspond to nominal and
malicious control signals, respectively. The reaction set
is
given by R = {rb, rm}. The state transition diagram is
depicted by Fig. 7. The initial state is set to xn.

is

The utilities are given as follows. The benign sender’s utility
(cid:26) 1
0

if x = xn,
otherwise

U s(θb, x, a, r) =

for any a ∈ A and r ∈ R, which means that the benign sender
prefers the nominal state regardless of other variables. The
malicious sender’s utility is given by Table III. The benign
action ab is a risk-free action, which always induces zero
utility, while the malicious action am is a risky action. If the
reaction is rb, the malicious sender obtains positive utility,

SASAHARA et al.: ASYMPTOTIC SECURITY USING BAYESIAN DEFENSE MECHANISM WITH APPLICATION TO CYBER DECEPTION

11

TABLE III
MALICIOUS SENDER’S UTILITY

U s(θm, x, ab, r)
xn
xa

rb
0
0

rm
0
0

U s(θm, x, am, r)
xn
xa

rb
1
2

rm
-3
-3

TABLE V
TRANSITION PROBABILITIES FROM THE ABNORMAL STATE TO
ABNORMAL STATE.

P (xa|xa, a, r)
ab
am

rb
0.5
0.6

rm
0.3
0.4

TABLE IV
RECEIVER’S UTILITY

U r(θb, x, r)
xn
xa

rb
5
1

rm
0
0

U r(θm, x, r)
xn
xa

rb
0
0

rm
5
1

where the abnormal state xa is more preferred than xn. On
the other hand, if the reaction is rm, the malicious sender
incurs loss. The receiver’s utility is set to be independent on
a ∈ A and given by Table IV, where a is omitted. The receiver
obtains utility only when she takes an appropriate reaction
depending on the sender type. When an appropriate reaction
is chosen, the normal state is more preferred than the abnormal
state.

Since it is difﬁcult to compute an exact equilibrium for the
inﬁnite time horizon problem, we treat a sequence of equilibria
for a ﬁnite time horizon problem as a tractable approximation.
Deﬁne the ﬁnite time horizon average utilities from the kth
time step by

¯U s
k,T (s, θ) := Es
θ

(cid:34)

1
T + 1

k+T −1
(cid:88)

i=k

(cid:35)

U s(θ, Xk, Ak, Rk)

.

and

¯U r

k,T (s, π) :=
(cid:34)
1
(cid:88)
T + 1

Es
θ

θ∈Θ

k+T −1
(cid:88)

(cid:35)
U r(θ, Xk, Ak, Rk)πk(θ|X0:k)

i=k

k+T −1, sr
for the game G1. Letting (ss
k, sr
k+T −1) be the
resulting equilibrium, we use ss
k as the kth strategies as
with receding horizon control. For the game G2, the strategies
in the simulation are given in a similar manner. The horizon
length is set to T = 2.

k, . . . , ss
k and sr

B. Simulation: Asymptotic Security

In the ﬁrst scenario, we consider the case where the vul-
nerability is known, and thus this situation corresponds to the
game G1 in (5). The initial belief is given by πr
0(θm) = 0.01,
which is known to the sender. The true sender type is given
by θs = θs

m.

The transition probability is given as follows. Set

the

transition probability from xn to be given by

P (xa|xn, a, r) =

(cid:26) 0.2 if a = ab,
if a = am

0.3

for any r ∈ R, which means that the probability from the
normal state to the abnormal state is increased by the malicious
action and it is independent of the reaction. The transition
probability from xa to xa is given by Table V, which means
that the probability from the abnormal state to the abnormal
state is increased by the malicious action and it is decreased
by the reaction rm.

Sample paths of the belief on the malicious sender, state,
Fig. 8.
action, and reactions with θ = θm. The belief converges to a nonzero
value over time as claimed by Theorems 1 and 2. The action converges
to the benign action as claimed by Theorem 3. The results evidence
asymptotic security achieved by the Bayesian defense mechanism.

Under the setting, sample paths of the belief on the mali-
cious sender, the state, the action, and the reaction with θ = θm
are depicted in Fig. 8. The belief converges to a nonzero
value over time as claimed by Theorems 1 and 2. The action
converges to the benign action as claimed by Theorem 3. The
graphs evidence asymptotic security achieved by the Bayesian
defense mechanism. In more detail, it can be observed that
the malicious sender takes the malicious action am while the
receiver takes the reaction rb until about the time step k = 50.
This is because the receiver’s belief on the malicious sender
is low during the beginning of the game. On the other hand,
between the time steps k = 50 and k = 100, ab and rm
sporadically appear because the belief is increased. Finally,
after the time step k = 100, the belief exceeds a threshold,
which results in the ﬁxed actions a = ab and r = rm.

C. Simulation: Defensive Deception using Blufﬁng

In the second scenario, we consider the case where the
defender is unaware of the vulnerability and the attacker is
unaware of the defender’s unawareness. Then this situation
corresponds to the game G2 in (10). The initial beliefs are
given by

0(θs
πr

a) = 0.3,

0(θr
m|θr
πs
The true types are given by θs = θs
πr
0(θs
of the attack while the game is proceeding.

u. Note that
u) = 0 and hence the defender is completely unaware

a|θs
m) = 0.8.
m and θr = θr

m|θr

We ﬁrst consider the case where the strategy is not passively
blufﬁng. The same transition probability as that used in

05010015020025030000.51time12

IEEE JOURNAL, VOL. XX, NO. XX, XXXX 202X

TABLE VI
TRANSITION PROBABILITIES FROM THE ABNORMAL STATE TO THE
ABNORMAL STATE WITH PASSIVELY BLUFFING STRATEGIES.

P (xa|xa, a, r)
ab
am

rb
0.5
0.6

rm
0.5
0.6

Fig. 9. Sample paths of the receiver’s belief on the malicious sender
when the receiver is aware, the sender’s belief on the receiver being
aware, state, action, actual reactions, and reactions taken when the
receiver is aware, where the strategy is not passively blufﬁng. The
sender’s belief converges to zero, i.e., the attacker notices that the
defender is unaware of the vulnerability. As a result, the malicious action
is continuously taken after a sufﬁciently large time step.

the previous simulation, where it depends on the receiver’s
reaction. As a result, the state possesses information about the
receiver type.

Fig. 9 depicts sample paths of the receiver’s belief on
the malicious sender if the receiver were aware of the vul-
nerability, the sender’s belief on the receiver being aware,
the actual state, the actual action, the actual reactions taken
by the receiver being unaware, and reactions that would be
taken by the receiver being aware. It can be observed that
the sender’s belief converges to zero, i.e., the sender notices
that the receiver is unaware of the vulnerability. As a result,
malicious actions are constantly taken after a sufﬁciently large
time step. The result indicates that the defense mechanism fails
to defend the system in this case.

We next consider the blufﬁng case. Let

the transition
probability from the abnormal state to the abnormal state
be given by Table VI, and the other probabilities be the
same as the previous one. Then the transition probability is
independent of the reaction, and hence any strategy becomes
passively blufﬁng. Note that, this modiﬁcation increases the
probability of the abnormal state, and hence it would not
improve the performance at all
if the recognitions of the
players are symmetric.

Fig. 10 depicts sample paths of those depicted in Fig. 9
under the blufﬁng setting. The sender’s belief is invariant
over time because the state does not possess information

Fig. 10. Sample paths of the receiver’s belief on the malicious sender
when the receiver is aware, the sender’s belief on the receiver being
aware, state, action, actual reactions, and reactions taken when the
receiver is aware, where the strategy is passively blufﬁng. The sender’s
belief is invariant over time because the state does not possess infor-
mation about the receiver type. Thus, the attacker keeps to be cautious
about being detected. As a result, the benign action is continuously
taken after a sufﬁciently large time step in contrast to Fig. 9.

about the receiver type. Thus, the malicious sender remains
cautious about being detected. As a result, the benign action
is continuously taken after a sufﬁciently large time step in
contrast to Fig. 9. The result indicates that asymptotic security
is achieved by the blufﬁng even if the defender is unaware
of the vulnerability. The simulation suggests importance of
concealing the defender’s belief even if it degrades control
performance.

VI. CONCLUSION

This study has analyzed defense capability achieved by
Bayesian defense mechanisms. It has been shown that the
system to be protected can be guaranteed to be secure by
Bayesian defense mechanisms provided that effective coun-
termeasures are implemented. This fact implies that model
knowledge can prevent
the defender from being deceived
in an asymptotic sense. As a defensive deception utilizing
the derived asymptotic security, blufﬁng utilizing asymmetric
recognition has been considered. It has also been shown that

05010015020025030000.5105010015020025030000.51time05010015020025030000.5105010015020025030000.51timeSASAHARA et al.: ASYMPTOTIC SECURITY USING BAYESIAN DEFENSE MECHANISM WITH APPLICATION TO CYBER DECEPTION

13

the attacker possibly stops the execution in the middle of
the attack in a rational manner when she strongly believes
the defender to be aware of the vulnerability, even if the
vulnerability is unnoticed.

Important future work includes an extension to inﬁnite state
spaces because the state space in a control system typically
is a subset of the Euclidean space. Moreover, although it
is assumed that the state is observable in this framework, a
generalization to partially observable setting is a more practical
setting. Finally, analysis of the transient behavior with the
defense scheme is an open problem.

APPENDIX
In the proofs, we omit the symbol s in the notation for

simplicity when no confusion arises.

Proof of Lemma 1: It is clear that πθ
ﬁltration σ(X0:k). It is also clear that πθ
respect to Pθ since it is bounded. Thus, it sufﬁces to show
k+1|σ(X0:k)(cid:3) ≥ πθ

k is adapted to the
k is integrable with

Pθ−a.s.

(cid:2)πθ

Eθ

k

for the claim. For a ﬁxed outcome ω ∈ Ω with which
X0:k(ω) = x0:k, the inequality is equivalent to

(cid:88)

xk+1∈X

pθ(xk+1|x0:k)πk+1(θ|x0:k+1) ≥ πk(θ|x0:k).

(22)

Thus it sufﬁces to show (22) for any k ∈ N and x0:k ∈ X k+1.
First, we reduce the index of the summation in (22). When
πk(θ|x0:k) = 0, the inequality (22) always holds. Thus we
assume πk(θ|x0:k) > 0 in the following. Deﬁne

X 0

k :=






xk+1 ∈ X :

pφ(xk+1|x0:k)πk(φ|x0:k) = 0






(cid:88)

φ∈Θ

By applying Jensen’s inequality in (1) with the functions
p(x) := pθ(x), a(x) := (cid:80)
pφ(x)
pθ(x) π(φ), and ϕ(ξ) := 1/ξ,
we have

φ∈Θ

G(θ) ≥ ϕ

= ϕ

= ϕ

(cid:16)(cid:80)
(cid:16)(cid:80)
(cid:16)(cid:80)
(cid:16)(cid:80)

(cid:17)
pφ(x)
pθ(x) π(φ)
(cid:17)

x∈X +

x∈X + pθ(x) (cid:80)
(cid:80)

φ∈Θ
φ∈Θ pφ(x)π(φ)
x∈X + pφ(x)(cid:1) π(φ)
φ∈Θ
(cid:17)
φ∈Θ π(φ)

(cid:0)(cid:80)

(cid:17)

= ϕ
= ϕ(1)
= 1,

which leads to the claim.

Proof of Theorem 1: Because the belief is uniformly
(cid:3) < ∞. From
bounded over time, we have supk∈N Eθ
Lemma 1 and Doob’s convergence theorem [47, Theo-
rem 4.4.1], the claim holds.

(cid:2)πθ

k

0 > 0, we have πθ

k > 0 Pθ-
Proof of Lemma 2: Since πθ
almost surely for any k ∈ N. Thus log(πθ
k) is well-deﬁned. We
ﬁrst show that log(πθ
k) is a submartingale with respect to the
probability measure Pθ and the ﬁltration σ(X0:k). It is clear
that log(πθ
k) is adapted to the ﬁltration σ(X0:k). Because the
k) is ﬁnite, log(πθ
number of elements in the support of log(πθ
k)
is integrable for any k ∈ N. Thus it sufﬁces to show that
k+1)|σ(X0:k)(cid:3) ≥ log(πθ

k) Pθ−a.s.

(cid:2)log(πθ

Eθ

As in the proof of Lemma 1, this inequality is equivalent to

(cid:88)

xk+1∈X +
k

pθ(xk+1|x0:k) log(πk+1(θ|x0:k+1)) ≥ log(πk(θ|x0:k))

.

for any x0:k ∈ X k+1. With the notation (24), this is equivalent
to

Because πk(φ|x0:k) is positive for any φ ∈ Θ, if xk+1 belongs
to X 0
k then pθ(xk+1|x0:k) = 0. Hence (22) is equivalent to
(cid:88)

pθ(xk+1|x0:k)πk+1(θ|x0:k+1) ≥ πk(θ|x0:k)

(23)

xk+1∈X +
k

(cid:32)

pθ(x) log

(cid:33)

pθ(x)π(θ)
φ∈Θ pφ(x)π(φ)

(cid:80)

(cid:88)

x∈X +

≥ log(π(θ)).

(26)

Because the left-hand side can be rewritten by

where X +

k := X \ X 0
k .

To simplify notation, we deﬁne

π(θ) := πk(θ|x0:k), pφ(x) := pφ(x|x0:k), X + := X +
k

(24)

for ﬁxed k and x0:k. Since π is consistent with s,
inequality (23) is equivalent to

the

(cid:88)

x∈X +

pθ(x)

pθ(x)π(θ)
φ∈Θ pφ(x)π(φ)

(cid:80)

≥ π(θ).

Because π(θ) > 0, this inequality is equivalent to

pθ(x)

(cid:88)

x∈X +
(cid:124)

(cid:80)

pθ(x)
φ∈Θ pφ(x)π(φ)
(cid:123)(cid:122)
(cid:125)
=:G(θ)

≥ 1.

(25)

By rewriting G(θ), we have

G(θ) =

(cid:88)

pθ(x)

x∈X +

1
pφ(x)
pθ(x) π(φ)

.

(cid:80)

φ∈Θ

(cid:88)

x∈X +
(cid:88)

x∈X +
(cid:88)

x∈X +

=

=

pθ(x) log

(cid:40)

(cid:32)

(cid:80)

(cid:32)

pθ(x)

log

(cid:32)

pθ(x) log

(cid:80)

(cid:33)

pθ(x)π(θ)
φ∈Θ pφ(x)π(φ)
pθ(x)
φ∈Θ pφ(x)π(φ)
(cid:33)

pθ(x)
φ∈Θ pφ(x)π(φ)

(cid:80)

(cid:33)

(cid:41)

+ log(π(θ))

+ log(π(θ)),

the inequality (26) is equivalent to

(cid:32)

pθ(x) log

(cid:88)

x∈X +

pθ(x)
φ∈Θ pφ(x)π(φ)

(cid:80)

(cid:33)

≥ 0,

which is also equivalent to

(cid:88)

x∈X +
(cid:124)

pθ(x) log

(cid:18) (cid:80)

φ∈Θ pφ(x)π(φ)
pθ(x)

(cid:123)(cid:122)
H(θ)

≤ 0.

(cid:19)

(cid:125)

14

IEEE JOURNAL, VOL. XX, NO. XX, XXXX 202X

By applying Jensen’s inequality for a concave function with
, and ϕ(ξ) := log(ξ),
p(x) := pθ(x), a(x) :=
we have

φ∈Θ pφ(x)π(φ)
pθ(x)

(cid:80)

Since s is a BNE with detection-averse utilities,

lim
k→∞

(1 − πθm

k ) (cid:54)= 0 Pθm −a.s.

H(θ) ≤ log

(cid:16)(cid:80)
(cid:16)(cid:80)

x∈X + pθ(x)
(cid:80)

x∈X +

(cid:80)

φ∈Θ pφ(x)π(φ)
pθ(x)

(cid:17)

Therefore, (29) leads to the claim.

Proof of Theorem 3: From the deﬁnition of the condi-

tional probability mass function, we have

(cid:17)

φ∈Θ pφ(x)π(φ)

= log
= log(1)
= 0,

which implies that log(πθ

k) is a submartingale.

From Doob’s convergence theorem [47, Theorem 4.4.1], it
sufﬁces to show that the expectation of the nonnegative part of
log(πθ
k) is
nonpositive for any k ∈ N, and hence the uniform boundedness
holds.

k) is uniformly bounded. Because πθ

k ∈ (0, 1], log(πθ

Proof of Theorem 2: We prove the claim by contradiction.
Deﬁne E as the inverse image of {0} for πθ
∞. Assume that
Pθ(E) > 0. For any ω ∈ E, πθ
k(ω) → 0 as k → ∞. Hence,
from the continuity of logarithm functions, it turns out that
log(πθ
k(ω)) → −∞ as k → ∞. This means that log(πθ
k(ω))
diverges for ω ∈ E. However, Lemma 2 states that log(πθ
k)
converges Pθ-almost surely. This is a contradiction.

Proof of Lemma 3: We ﬁrst show that the coefﬁcient of

Bayes’ rule converges to one, i.e.,

pθ(Xk+1|X0:k) = P (Xk+1|Xk, ss

k(θ, H s

k), sr

k(H r

k))

k and H r

where H s
k are the sender’s and receiver’s histories
uniquely determined by X0:k with s, respectively. Thus the
claim of Lemma 3 can be rewritten by

k , Rk) − P (Xk+1|Xk, Aθb

|P (Xk+1|Xk, Aθm
Pθm -almost surely as k → ∞, where Rk := sr
ﬁniteness of the MDP, the condition (30) is equivalent to

k , Rk)| → 0 (30)
k(H r

k). From

Pθm ({Ek i.o.}) = 0

(31)

where

Ek :=

(cid:110)

P (Xk+1|Xk, Aθm

k , Rk) (cid:54)= P (Xk+1|Xk, Aθb

k , Rk)

(cid:111)

.

By applying the generalized Borel-Cantelli’s second lemma
in (2) with Fk := σ(X0:k), we have
Pθm (E) = 0

(32)

lim
k→∞

fk(θm, X0:k) = 1 Pθm −a.s.

(27)

where

where fk is deﬁned in (4). Because the belief system is
∞ is nonzero Pθm-almost surely from
consistent with s and πθm
Theorem 2, we have

lim
k→∞

fk+1(θm, X0:k) = lim
k→∞
∞ /πθm
= πθm
∞
= 1
Pθm -almost surely. Thus (27) holds.

(cid:16)
k+1/πθm
πθm

k

(cid:17)

Deﬁne

fN,k+1(ω) := pθm (Xk+1(ω)|X0:k(ω)),
fD,k+1(ω) := (cid:80)

φ∈Θ pφ(Xk+1(ω)|X0:k(ω))πφ

k (ω),

which denote the numerator and the denominator of the
coefﬁcient fk+1(θm, X0:k+1(ω)), respectively. Since πθm
0 > 0,
we have 0 < fD,k+1 ≤ 1 for any ω ∈ Ω, k ∈ Z+. Thus
0 ≤ fD,k+1|fk+1 − 1| ≤ |fk+1 − 1| Pθm −a.s.

(28)

for any k ∈ Z+. Because

lim
k→∞

|fk+1 − 1| = 0 Pθm −a.s.

from (27), the squeeze theorem for (28) yields

E := {ω ∈ Ω : (cid:80)∞
k=0

Pθm (Ek|σ(X0:k)) (ω) = ∞} .

We derive a simpler description of the event E. For any
ω ∈ Ω, the set of nonnegative integers Z+ can be divided into
two disjoint subsets ˆZ+(ω) and Z+ \ ˆZ+(ω) such that

(cid:26) Aθm
Aθm

k (ω) (cid:54)= Aθb
k (ω) = Aθb

k (ω),
k (ω),

∀k ∈ ˆZ+(ω),
∀k ∈ Z+ \ ˆZ+(ω).

For a ﬁxed ω ∈ Ω, Pθm (Ek|σ(X0:k))(ω) = 0 for k ∈ Z+ \
ˆZ+(ω). Thus we have
∞
(cid:88)

(cid:88)

Pθm (Ek|σ(X0:k)) (ω) =

Pθm (Ek|σ(X0:k)) (ω).

k=0

k∈ˆZ+(ω)

Moreover, for any k ∈ Z+ and ω ∈ Ω, we have
Pθm (Ek|σ(X0:k)) (ω) =

(cid:88)

P (xk+1|xk, aθm

k , rk)

where

xk+1∈Xk+1(ω)

Xk+1(ω) := {x ∈ X : P (x|xk, aθm

k , rk) (cid:54)= P (x|xk, aθb

k , rk)}

k := Aθ
with x0:k := X0:k(ω), aθ
the condition in the deﬁnition of E can be rewritten by

k(ω), and rk := Rk(ω). Thus,

lim
k→∞

fD,k+1|f θm,s

k+1 − 1| = 0 Pθm −a.s.

(29)

(cid:88)

(cid:88)

P (xk+1|xk, aθm

k , rk) = ∞.

(33)

Now we have

k+1 − 1|

fD,k+1|f θm,s
= |fN,k+1 − fD,k+1|
= |pθm (Xk+1|X0:k) − (cid:80)
= |pθm (Xk+1|X0:k)(1 − πθm
= |pθm (Xk+1|X0:k) − pθb(Xk+1|X0:k)|(1 − πθm

φ∈Θ pθ(Xk+1|X0:k)πθ
k|
k ) − pθm (Xk+1|X0:k)πθb
k |

k ).

k∈ˆZ+(ω)

xk+1∈Xk+1(ω)

Now we deﬁne

F := {Aθm
k

(cid:54)= Aθb

k i.o.}.

We show the claim to be proven, namely,

Pθm (F ) = 0

(34)

(35)

SASAHARA et al.: ASYMPTOTIC SECURITY USING BAYESIAN DEFENSE MECHANISM WITH APPLICATION TO CYBER DECEPTION

15

by contradiction. Assume Pθm (F ) > 0. Then Pθm (E|F ) is
well-deﬁned. From (32), we have

we suppose s ∈ S ∗
0(θr
πs

m) < 1 such that

a|θs

nab. It sufﬁces to show that there exists

Pθm (E ∩ F ) = Pθm (E|F )Pθm (F ) = 0.

inf
s∈S ∗

nab

D(s, g(s)) > 0

(38)

Because Pθm (F ) is assumed to be nonzero,
implies

this equation

where

Pθm (E|F ) = 0.

(36)

D(s, ˜ss) := ¯U s((˜ss, sr), θs

m, πs) − ¯U s(s, θs

m, πs)

We now calculate Pθm (E|F ) from its deﬁnition. Let

γ(ω) := inf

k∈ ˆZ+(ω)

(cid:88)

P (xk+1|xk, aθm

k , rk).

xk+1∈Xk+1(ω)

For any ω ∈ Ω, the set Xk+1(ω) is nonempty for k ∈ ˆZ+(ω)
from Assumption 1. This fact and the ﬁniteness of the MDP
lead to that γ(ω) > 0. The inﬁmum leads to the inequality

(cid:88)

(cid:88)

k∈ˆZ+(ω)

xk+1∈Xk+1(ω)

P (xk+1|xk, aθm

k , rk) ≥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆZ+(ω)
(cid:12)
(cid:12) γ(ω).

(37)
then ˆZ+(ω) has inﬁnite elements and hence
If ω ∈ F ,
|ˆZ+(ω)|γ(ω) = ∞. Thus, for any ω ∈ F , from the inequal-
ity (37), the condition (33) holds. Therefore, Pθm (E|F ) = 1,
which contradicts (36). Hence (35) holds.

2 ∈ BRs(ˆsr

u. Assume ˆss
u|θs

Proof of Lemma 4: The former claim is obvious since the
probability measures are independent of the strategies with θs
b
2, θs
and θr
m) =
m) = 0 for k ∈ Z+, the malicious sender’s
1 and ˆπs
expected average utility up to the T th step in ˆG2 is given by
(cid:35)
m, Xk, Ak, Rk)

m, ˆπs). Because ˆπs

m, ˆπs) = Eˆs2
m,θr
θs
a

T (ˆs2, θs

U s(θs

k(θr

k(θr

T
(cid:88)

a|θs

¯U s

(cid:34)

,

1
T + 1

k=0

which is the malicious sender’s utility up to the T th step in
G1. Thus, BRs(ˆsr
1, θm) in
G1. Hence, ss

m, ˆπs) in ˆG2 is equal to BRs(sr

2, θs
1 ∈ BRs(sr

1, θm).

Proof of Lemma 5: If s is a passively blufﬁng strategy,
then the distribution of H s
k is independent of the receiver type.
(cid:16)
Aθm
Thus the distribution of δ
is also independent of
the receiver type. Hence, if (15) holds, the same condition
holds for θr

k , Aθb

(cid:17)

u as well.

k

Proof of Lemma 6: Take s = (ss, sr) ∈ S ∗

G1 corresponding to ˆG2. Let s1 = (ss
in G1 given by (11). From Lemma 4, we have

1, sr

nab. Consider
1) be a strategy proﬁle

(cid:16)

Ps1
θs
m

δ(Aθm

k , Aθb

k ) = 0

(cid:17)

(cid:16)

= Pˆs2
θs
m,θr
a

δ(Aθm

k , Aθb

k ) = 0

(cid:17)

.

From the contraposition of Lemma 5, this equation implies that
s1 is not asymptotically benign in the sense of the game G1
since s ∈ Snab. Thus, s1 is not a BNE of G1 from Theorem 3.
This means that ss
1 contains a strategy that is not a best
1 (cid:54)∈ BRs(sr
response. Because s ∈ S ∗, this means ss
1, θm).
From the contraposition of Lemma 4, ss (cid:54)∈ BRs(sr, θs
m, ˆπs),
which is equivalent to (18).

a|θs

Proof of Theorem 4: We prove the existence of
πs
0(θr
m) < 1 such that the contraposition of the condition
holds, i.e., if s is not asymptotically benign then s is not
a BNE. Let s ∈ Snab. If s (cid:54)∈ S ∗, s is not a BNE. Thus

and g is given in (20).
From (17), we have

D(s, ˜ss) =

(cid:88)

θr∈Θ r

Dθr (s, ˜ss)πs

0(θr|θs

m).

From the deﬁnition of γ in (21), we have

u

u|θs

(s, g(s))πs

= γ + (Dθr

D(s, g(s)) ≥ Dθr

0(θr
(s, g(s)) − γ)πs

a|θs
0(θr
u|θs
m)
(s, g(s)) − γ ≥ 0 for any
nab. Then (39) implies that D(s, g(s)) ≥ γ for any
m) < 1. From Assumption 2, this inequality leads to

Consider the case where Dθr
s ∈ S ∗
a|θs
0(θr
πs

m) + γπs
0(θr

(39)

m)

u

u

inf
s∈S ∗

nab

D(s, g(s)) ≥ γ > 0,

which implies (38).

Next, consider the case where Dθr

nab. By taking an initial belief πs

u

(s, g(s)) − γ < 0 for
0(θr
m) > 0

u|θs

some s ∈ S ∗
such that

0(θr
πs

u|θs

m) < inf
s∈T

−γ

Dθr

u

(s, g(s)) − γ

(40)

where

T := {s ∈ S ∗

nab : Dθr

u

(s, g(s)) − γ < 0},

we have (38) from (39). From the deﬁnition of Dθr
Dθr
strategy. Thus we have

(s, g(s)) is bounded in T because ¯U s

u in (19),
θr is bounded for any

u

inf
s∈T

−γ

Dθr

u

(s, g(s)) − γ

> 0.

Thus a nonzero initial belief that satisﬁes (40) exists.

REFERENCES

[1] McAfee, “The hidden costs of cybercrime,” Tech. Rep., 2020, [Online].
https://www.mcafee.com/enterprise/en-us/assets/reports/rp-

Available:
hidden-costs-of-cybercrime.pdf.

[2] Cybersecurity & Infrastructure Security Agency, “Stuxnet malware
mitigation,” Tech. Rep. ICSA-10-238-01B, 2014, [Online]. Available:
https://www.us-cert.gov/ics/advisories/ICSA-10-238-01B.

[3] ——, “HatMan - safety system targeted malware,” Tech. Rep. MAR-17-
352-01, 2017, [Online]. Available: https://www.us-cert.gov/ics/MAR-17-
352-01-HatMan-Safety-System-Targeted-Malware-Update-B.

[4] ——, “Cyber-attack against Ukrainian critical infrastructure,” Tech. Rep.
[Online]. Available: https://www.us-

IR-ALERT-H-16-056-01, 2018,
cert.gov/ics/alerts/IR-ALERT-H-16-056-01.

[5] ——, “DarkSide ransomware: Best practices for preventing business
disruption from ransomware attacks,” Tech. Rep. AA21-131A, 2021,
[Online]. Available: https://us-cert.cisa.gov/ncas/alerts/aa21-131a.
[6] N. Falliere, L. O. Murchu, and E. Chien, “W32. Stuxnet Dossier,”

Symantec, Tech. Rep., 2011.

[7] A. Neyman and S. Sorin, Eds., Stochastic Games and Applications.

Springer, 2003.

[8] I.-K. Cho and D. M. Kreps, “Signaling Games and Stable Equilibria,”
The Quarterly Journal of Economics, vol. 102, no. 2, pp. 179–221, 1987.

16

IEEE JOURNAL, VOL. XX, NO. XX, XXXX 202X

[9] J. Mertens and S. Zamir, “Formulation of Bayesian analysis for games
with incomplete information,” International Journal of Game Theory,
vol. 14, pp. 1–29, 1985.

[10] E. Dekel and M. Siniscalchi, “Epistemic game theory,” in Handbook of

Game Theory. Elsevier, 2015, ch. 12, pp. 619–702.

[11] M. S. Lund, B. Solhaug, and K. Stolen, Model-Driven Risk Analysis.

Springer, 2011.

[12] C. Phillips and L. P. Swiler, “A graph-based system for network-
vulnerability analysis,” in Proc. 1998 Workshop on New Security
Paradigms, 1998, p. 71–79.

[13] B. Schneier, “Attack trees,” Dr. Dobb’s Journal, vol. 24, no. 12, pp.

21–29, 1999.

[14] S. Bistarelli, F. Fioravanti, and P. Peretti, “Defense trees for economic
evaluation of security investments,” in Proc. First International Confer-
ence on Availability, Reliability and Security, 2006.

[15] A. Roy, D. S. Kim, and K. S. Trivedi, “Attack countermeasure trees
(ACT): towards unifying the constructs of attack and defense trees,”
Security and Communication Networks, vol. 5, no. 8, pp. 929–943, 2012.
[16] E. Miehling, M. Rasouli, and D. Teneketzis, “A POMDP approach to
the dynamic defense of large-scale cyber networks,” IEEE Trans. Inf.
Forensics Security, vol. 13, no. 10, pp. 2490–2505, 2018.

[17] S. Chockalingam, W. Pieters, A. Teixeira, and P. Gelder, “Bayesian
network models in cyber security: A systematic review,” in Secure IT
Systems, ser. Lecture Notes in Computer Science. Springer, 2017.
[18] C. Kruegel, D. Mutz, W. Robertson, and F. Valeur, “Bayesian event
classiﬁcation for intrusion detection,” in Proc. 19th Annual Computer
Security Applications Conference, 2003, pp. 14–23.

[19] W. Alhakami, A. ALharbi, S. Bourouis, R. Alroobaea, and N. Bouguila,
“Network anomaly intrusion detection using a nonparametric Bayesian
approach and feature selection,” IEEE Access, vol. 7, pp. 52 181–52 190,
2019.

[20] S. A. Zonouz, H. Khurana, W. H. Sanders, and T. M. Yardley, “RRE:
A game-theoretic intrusion response and recovery engine,” IEEE Trans.
Parallel Distrib. Syst., vol. 25, no. 2, pp. 395–406, 2014.

[21] N. Poolsappasit, R. Dewri, and I. Ray, “Dynamic security risk manage-
ment using Bayesian attack graphs,” IEEE Trans. Dependable Secure
Comput., vol. 9, no. 1, pp. 61–74, 2012.

[22] H. Sandberg, S. Amin, and K. H. Johansson, “Cyberphysical security in
networked control systems: An introduction to the issue,” IEEE Control
Systems Magazine, vol. 35, no. 1, pp. 20–23, 2015.

[23] S. M. Dibaji, M. Pirani, D. B. Flamholz, A. M. Annaswamy, K. H.
Johansson, and A. Chakrabortty, “A systems and control perspective of
CPS security,” Annual Reviews in Control, vol. 47, pp. 394–411, 2019.
[24] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson, “Revealing
stealthy attacks in control systems,” in Proc. 50th Annual Allerton
Conference on Communication, Control, and Computing, 2012, pp.
1806–1813.

[25] J. Miloˇsevi´c, A. Teixeira, T. Tanaka, K. H. Johansson, and H. Sandberg,
“Security measure allocation for industrial control systems: Exploiting
systematic search techniques and submodularity,” International Journal
of Robust and Nonlinear Control, vol. 30, no. 11, pp. 4278–4302, 2020.
[26] J. Giraldo et al., “A survey of physics-based attack detection in cyber-

physical systems,” ACM Comput. Surv., vol. 51, no. 4, 2018.

[27] M. Tambe, Security and Game Theory: Algorithms, Deployed Systems,

Lessons Learned. Cambridge University Press, 2012.

[28] T. Alpcan and T. Bas¸ar, Network Security: A Decision and Game-

Theoretic Approach. Cambridge University Press, 2010.

[29] J. Pawlick, E. Colbert, and Q. Zhu, “A game-theoretic taxonomy and
survey of defensive deception for cybersecurity and privacy,” ACM
Computing Surveys, vol. 52, no. 4, 2019.

[30] M. O. Sayin and T. Bas¸ar, “Deception-as-defense framework for cyber-
physical systems,” in Safety, Security and Privacy for Cyber-Physical
Systems, R. M. Ferrari and A. M. H. Teixeira, Eds. Springer Interna-
tional Publishing, 2021, pp. 287–317.

[31] H. Sasahara and H. Sandberg, “Epistemic signaling games for cyber
deception with asymmetric recognition,” IEEE Contr. Syst. Lett., vol. 6,
pp. 854–859, 2022.

[32] J. Pawlick and Q. Zhu, Game Theory for Cyber Deception: From Theory
to Applications, ser. Static & Dynamic Game Theory: Foundations &
Applications. Springer, 2021.

[33] J. Pawlick, E. Colbert, and Q. Zhu, “Modeling and analysis of leaky
deception using signaling games with evidence,” IEEE Trans. Inf.
Forensics Security, vol. 14, no. 7, pp. 1871–1886, July 2019.

[34] Q. Zhu and Z. Xu, “Secure estimation of CPS with a digital twin,” in
Cross-Layer Design for Secure and Resilient Cyber-Physical Systems.
Springer, 2020, pp. 115–138.

[35] P. Diaconis and D. Freedman, “On the consistency of Bayes estimation,”

The Annals of Statistics, vol. 14, no. 1, pp. 1–26, 1986.

[36] S. Walker, “New approaches to Bayesian consistency,” The Annals of

Statistics, vol. 32, no. 5, pp. 2028–2043, 2004.

[37] P. Eichelsbacher and A. Ganesh, “Bayesian inference for Markov
chains,” Journal of Applied Probability, vol. 39, no. 1, pp. 91–99, 2002.
[38] H. Sasahara, S. Sarıtas¸, and H. Sandberg, “Asymptotic security of control
systems by covert reaction: Repeated signaling game with undisclosed
belief,” in Proc. 59th IEEE Conference on Decision and Control, 2020.
[39] H. Sasahara and H. Sandberg, “Asymptotic security by model-based
incident handlers for Markov decision processes,” in Proc. 60th IEEE
Conference on Decision and Control, 2021.

[40] R. Durrett, Probability: Theory and Examples, ser. Cambridge Series in
Statistical and Probabilistic Mathematics. Cambridge University Press,
2019.

[41] A. Rasekh, A. Hassanzadeh, S. Mulchandani, S. Modi, and M. K. Banks,
“Smart water networks and cyber security,” Journal of Water Resources
Planning and Management, vol. 142, no. 7, 2016.

[42] E. Creaco, A. Campisano, N. Fontana, G. Marini, P. R. Page, and
T. Walski, “Real time control of water distribution newtorks: A state-
of-the-art review,” Water Research, vol. 161, pp. 517–530, 2019.
[43] R. Taormina, S. Galelli, N. O. Tippenhauer, E. Salomons, and A. Ostfeld,
“Characterizing cyber-physical attacks on water distribution systems,”
Journal of Water Resources Planning and Management, vol. 143, no. 5,
2017.

[44] P. Chen, L. Desmet, and C. Huygens, “A study on advanced persistent
threats,” in Proc. International Conference on Communications and
Multimedia Security, 2014, pp. 63–72.

[45] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dy-

namic Programming.

John Wiley & Sons, Inc., 1994.

[46] S. Zamir, “Bayesian games: Games with incomplete information,” in
Encyclopedia of Complexity and Systems Science. Springer, 2009, pp.
426–441.

[47] E. C¸ inlar, Probability and Statistics, ser. Graduate Texts in Mathematics.

Springer, 2011.

PLACE
PHOTO
HERE

Hampei Sasahara (M’15) received the Ph.D.
degree in engineering from Tokyo Institute of
Technology in 2019. He is currently an Assis-
tant Professor with Tokyo Institute of Technology,
Tokyo, Japan. From 2019 to 2021, he was a
Postdoctoral Scholar with KTH Royal Institute
of Technology, Stockholm, Sweden. His main
interests include secure control system design
and control of large-scale systems.

PLACE
PHOTO
HERE

Henrik Sandberg (M’04) received the M.Sc.
degree in engineering physics and the Ph.D. de-
gree in automatic control from Lund University,
Lund, Sweden, in 1999 and 2004, respectively.
He is a Professor with the Division of Deci-
sion and Control Systems, KTH Royal Institute
of Technology, Stockholm, Sweden. From 2005
to 2007, he was a Postdoctoral Scholar with
the California Institute of Technology, Pasadena,
CA, USA. In 2013, he was a Visiting Scholar
with the Laboratory for Information and Decision
Systems, Massachusetts Institute of Technology, Cambridge, MA, USA.
He has also held visiting appointments with the Australian National
University, Canberra, ACT, USA, and the University of Melbourne,
Parkville, VIC, Australia. His current research interests include security
of cyberphysical systems, power systems, model reduction, and funda-
mental limitations in control. Dr. Sandberg received the Best Student
Paper Award from the IEEE Conference on Decision and Control
in
2004, an Ingvar Carlsson Award from the Swedish Foundation for
Strategic Research in 2007, and Consolidator Grant from the Swedish
Research Council in 2016. He has served on the editorial board of IEEE
TRANSACTIONS ON AUTOMATIC CONTROL and the IFAC Journal
Automatica.

