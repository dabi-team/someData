Autonomous Attack Mitigation for Industrial Control Systems

1
2
0
2

v
o
N
3

]

R
C
.
s
c
[

1
v
5
4
4
2
0
.
1
1
1
2
:
v
i
X
r
a

John Mern
Stanford University

Kyle Hatch
Stanford University

Ryan Silva
Johns Hopkins University Applied Physics Lab

Cameron Hickert
Johns Hopkins University Applied Physics Lab

Tamim Sookoor
Johns Hopkins University Applied Physics Lab

Mykel J. Kochenderfer
Stanford University

Abstract

Defending computer networks from cyber attack requires
timely responses to alerts and threat intelligence. Decisions
about how to respond involve coordinating actions across
multiple nodes based on imperfect indicators of compromise
while minimizing disruptions to network operations. Cur-
rently, playbooks are used to automate portions of a response
process, but often leave complex decision-making to a human
analyst. In this work, we present a deep reinforcement learn-
ing approach to autonomous response and recovery in large
industrial control networks. We propose an attention-based
neural architecture that is ﬂexible to the size of the network un-
der protection. To train and evaluate the autonomous defender
agent, we present an industrial control network simulation
environment suitable for reinforcement learning. Experiments
show that the learned agent can effectively mitigate advanced
attacks that progress with few observable signals over several
months before execution. The proposed deep reinforcement
learning approach outperforms a fully automated playbook
method in simulation, taking less disruptive actions while
also defending more nodes on the network. The learned pol-
icy is also more robust to changes in attacker behavior than
playbook approaches.

1 Introduction

Cyber attacks have been increasingly targeting computer
networks that are integrated with industrial control systems
(ICS) [1]. Attack techniques focusing on stealth and redundant
access make them difﬁcult to detect and stop [2]. Intrusion
detection systems monitor networks and alert security orches-
trators to potential intrusions, though high false-alarm rates
cause many alerts to receive no security response. Advanced
persistent threat (APT) attackers take advantage of this by
spreading across target networks undetected over long periods
to prepare an eventual attack [3]. In addition to the theft of
sensitive data common in APT attacks on standard networks,
attacks on industrial control systems can additionally result

in disruption of the controlled system, physical destruction of
equipment, and even loss of life [4–6].

It is not feasible for human security teams to respond to ev-
ery potential intrusion alert and to reliably mitigate all threats.
This work seeks to demonstrate the feasibility of developing
an automated cyber security orchestrator (ACSO) to assist
human analysts by automatically investigating and mitigating
potential attacks on ICS networks. We ﬁrst present a model of
this task as a discrete-time sequential decision making prob-
lem. The proposed model captures many of the challenges of
the underlying decision problem while remaining agnostic to
speciﬁc network conﬁguration and communication dynamics.
To support this, we implemented a cyber attack simulator that
can efﬁciently generate large quantities of trial data.

Many sequential decision making methods cannot solve
this problem without explicit models of APT attack dynamics
and alert behaviors that are not generally known. Reinforce-
ment learning (RL) can solve complex tasks without explicit
models [7, 8]. There are several aspects of the computer net-
work defense problem that make learning with existing re-
inforcement learning approaches difﬁcult. The number of
potential observations that an agent may make at a given
time-step grows with the number of nodes on a network. Sim-
ilarly, the number of mitigation actions the agent can take
scales directly with the number of nodes. Deep reinforcement
learning is known to struggle to learn on problems with large
observation spaces [9] and action spaces [10].

APT attacks are designed to be difﬁcult to detect, causing
limited observability of the true compromise state of nodes on
the network. Solving partially observable problems requires
learning over sequences of observations to infer the hidden
states [11], making them signiﬁcantly more difﬁcult to solve
than fully observable problems. Effective defense against
APTs requires timely response to compromises, though in-
trusion campaigns can last several months. Modeling the
decision process requires ﬁne resolution time-steps, leading
to very long time horizons. Long time-horizons and sparse
rewards can drastically increase the sample complexity of
learning, both through exploration difﬁculty and temporally

1

 
 
 
 
 
 
Figure 1: APT Attack Progression. This ﬁgure shows the typical progression of an APT attack at the level of tactics in the MITRE
ATT&CK framework. The process starts on the left with initial intrusion, proceeding over several months to eventual execution.

delayed credit assignment [12, 13].

This paper proposes a neural network architecture and train-
ing algorithm that scales to the large problem space. We
present an attention-based architecture that effectively learns
over many cyber network nodes without growth in the number
of required parameters. To overcome exploration difﬁculty,
we introduce a potential-function shaping reward to aid train-
ing without biasing the converged policy behavior. To simu-
late the alert generation with an intrusion detection system
(IDS), we develop a dynamic Bayes network ﬁlter [14] that
aggregates network observations in an approximately optimal
way.

We tested the proposed solution methods against several
baseline automation policies using the same simulation pa-
rameters that were used for training. To test the robustness
of each approach to changes in attacker behavior, we evalu-
ated performance with perturbations to APT attack policies
and action effects. aqThe results show that the reinforcement
learning agent can successfully defend against attack with
fewer disruptive mitigation actions than the baseline methods.
Results also indicate that the agent is more robust to changes
in attack trajectory than baselines.

2 Background and Related Work

As their name implies, APTs are getting more sophisti-
cated, and artiﬁcial intelligence (AI) offers growing potential
for automating and providing intelligence to malware [15].
APTs are increasingly targeting critical infrastructure systems.
These developments necessitate rapid advancements in cyber
defense capabilities. In particular, active cyber defense (ACD)
capabilities [16] need to mature beyond their reliance on do-
main expert developed courses of action (COA) in order to
rapidly, and dynamically adapt to changing adversary tactics,
techniques, and procedures (TTPs) [17]. For improved cyber
defense, AI has been proposed in a number of areas [18]:

• Penetration Testing: Proactively identify vulnerabilities

and patch them

• Detection and Identiﬁcation: Use anomaly detection and
pattern recognition to detect and classify malicious ac-
tivity within a network

• Recovery and Response: Mitigate adversary actions and
return the system to an operational state autonomously
or in collaboration with a human operator

Every computer system has inherent vulnerabilities that
an adversary can exploit. In order to identify and eliminate
these weaknesses, system owners utilize teams of penetration
testers to conduct exercises where they attempt to inﬁltrate
the system, move laterally within the network, search for
exploits and execute malicious payloads, as shown in the
MITRE ATT&CK framework in ﬁg. 1. These exercises re-
sult in reports describing identiﬁed vulnerabilities which the
system owner can then attempt to patch. In order to auto-
mate and speed up the process of identifying and patching
vulnerabilities, DARPA conducted a Cyber Grand Challenge
where teams developed AI-based systems to both probe for
weaknesses in each other networks and patch vulnerabilities
in their own networks [19]. For cyber defense, teams could
either patch binaries or update IDS rules. Xandra, which ﬁn-
ished in second place, used fuzzing and symbolic execution
to generate offensive proofs of vulnerability and mainly fo-
cused on binary patching for defense with a single, ﬁxed
network ﬁlter rule set that it selectively deployed [20]. May-
hem, which won the competition did not use the IDS system
and only relied on binary patching [21]. These approaches did
not extensively use machine learning and relied instead on
pre-deﬁned rules and procedures from software vulnerability
analysis. Zennaro et al. propose an approach to us model-free
reinforcement learning to solve capture the ﬂag challenges,
thus demonstrating an approach for automated penetration
testing using machine learning [22]. While automated pen
testing and patching capabilities are necessary in order to
improve system robustness and resilience, their contributions
to cyber defense are complementary to the automated attack
mitigation described in this work.

The NIST cybersecurity framework deﬁnes ﬁve func-
tions necessary for critical infrastructure system cybersecu-
rity: Identify, Protect, Detect, Respond, and Recover [23]. A
number of intrusion detection solutions, including Bro [24],
Snort [25], and Security Onion [26] provide functionality such
as identifying and detecting malicious activity. Companies
such as Claroty [27] and Tenable [28] even produce IDSs fo-
cused on industrial control systems. Many of these solutions

2

Initial IntrusionThe attacker gains control over a low-privilege node, typically through social engineering Lateral MovementThe attacker gains control over additional nodes to ensure persistence and to gain visibility DiscoveryThe attacker searches for data on the network and the targeted physical processExecutionThe attacker delivers malicious code, resulting in data loss, process disruption, or equipment destruction.Privilege EscalationThe attacker mines credentials and compromise nodes necessary to execute its attackFigure 2: Simulated Network Architecture. The problem simulates level 2 and level 1 of the PERA model. Each level contains an
operations VLAN and a nominally empty quarantine VLAN. Level 2 contains twenty-ﬁve workstation nodes and three servers.
Level 1 has ﬁve local human-machine interface nodes and ﬁfty networked PLCs. All network message trafﬁc is simulated through
virtual switches, routers, and ﬁrewalls. The computing node for the ACSO is not explicitly modeled.

rely on pre-deﬁned rules against which machine activity or
network trafﬁc is compared in order to ﬂag anomalies. Some
recent solutions incorporate machine learning capabilities to
develop models of normal network trafﬁc which can then be
used to identify anomalous behaviour. ACSO relies on inputs
from IDSs such as these as well as other sensors deployed
within computer networks in order to identify ongoing at-
tacks. Due to the existing solutions and extensive research
being conducted to address the ﬁrst three phases of the NIST
framework, ACSO is focused on enhancing decision making
and responding to attacks in order to enable a compromised
system to recover faster than would be possible with existing
recovery and response capabilities.

Existing approaches to recovery and response rely on
manned Security Operations Centers (SOCs) where human
operators take actions in response to alerts generated by Se-
curity Information and Event Management Systems (SIEMs).
Security Orchestration, Automation, and Response (SOAR)
systems such as XSOAR and Splunk SOAR assist the cyber
defenders by walking them through pre-deﬁned COAs in re-
sponse to alerts and indicators from network sensors. As Dhir
et al. state, AI controlled attacks could simply overwhelm this
current generation of cyber defense by adapting faster than
pre-deﬁned playbooks can respond [29]. ACSO attempts to
enhance the automation enabled by the current generation of
SOAR technologies with reinforcement learning approaches.

As adversaries get more sophisticated and machine learn-
ing capabilities become more widely accessible, its likely
that malware will be learning enabled. Such an advancement
would result in an asymmetric environment where existing
defensive approaches are reacting to malicious autonomous
agents at human speed. In order to get ahead of this poten-
tially catastrophic situation, it is necessary to develop capa-

bilities such as the Autonomous Cyber Security Orchestrator
(ACSO).

3 Advanced Persistent Threat Scenario

APTs tend to be well-funded and attack speciﬁc targets for
signiﬁcant periods of time [30]. APT attack goals vary from
ﬁnancial, to data theft, to physical infrastructure command
and control. In this work, we focus on attacks aimed to dis-
rupt or destroy industrial control systems. APTs have been
responsible for a number of recent, high-proﬁle attacks that
damaged or disabled infrastructure ICS. Attacks on the power
grid in Ukraine and electrical utilities in the eastern United
States have been linked to APT groups [31, 32]. Addition-
ally, a water treatment plant in Oldsmar, Florida was accessed
through remote management software by a malicious actor
attempting to alter chemical levels in the water [33].

ICS networks present unique defense difﬁculties. The need
to control equipment distributed across large geographical
areas make internet-connected devices a mainstay of infras-
tructure systems. To attempt to mitigate this vulnerability,
many networks are organized into ﬁrewall-separated lev-
els, with low-privilege nodes on less restricted and internet-
accessible levels, and process-critical nodes on more isolated
subnets [34].

APTs overcome the layered ICS network structure by ﬁrst
compromising a low-privilege node in the target network [35].
Using this node, the APT will then conduct internal network
reconnaissance and take control over more privileged nodes
until it gains the authority needed to achieve its primary ob-
jective, for example to destroy equipment. During this recon-
naissance and lateral movement phase, intrusion detection
systems tend to struggle to detect the attackers’ activity [3].

3

VLAN 2.1 (Ops)VLAN 3.1 (Ops)Level 2/3 - EngineeringLevel 2/1 - PlantHistorianOPCDCEngineering Stations (25x)PLCs (50x)Local HMIFigure 3: Attacker Trajectory. The simulated APT must meet various conditions to enact high-level tactics, moving towards the
goal of disrupting or destroying PLCs. Rectangles indicate machine states that represent speciﬁc subroutines, diamonds represent
exit criteria necessary for transitioning to the next state, and the rounded boxes indicate the start and end states.

This can extend over a period of months, while the attack itself
is often executed in a matter of hours; thus, it is particularly
important to disrupt the offensive during the staging phase.

For this work the APT behavior is modeled at the level
of tactics in the MITRE ATT&CK framework [36]. Speciﬁc
techniques, vulnerabilities, and exploits are not explicitly mod-
eled. The underlying assumption is that an APT will choose
the right technique to achieve its tactical objective and the
variance in time, odds of success, and alert probability can
be represented by a stochastic distribution. We model APT
attacks that aim to capture programmable logic controllers
(PLCs) at the plant level of the Purdue Enterprise Reference
Architecture (PERA) [37].

Figure 2 shows the network diagram used in this study,
which is organized into levels 1 and 2 of the PERA architec-
ture. Level 2 contains exclusively workstations (twenty-ﬁve)
and servers (three), and level 1 contains ﬁve workstations and
all ﬁfty PLCs. We focus on attack scenarios that start with
control of a beachhead node in level 2, from which the at-
tacker’s goal is to disrupt or destroy the PLCs in level 1. This
scenario models the layered ICS network structure and APT
attack approach described above, and thus represents a range
of malicious activity that has impacted critical infrastructure,
such as those on Ukraine’s electrical grid.

3.1 Simulation Environment

To develop and test our approach, we implemented an ICS
network attack simulator (INASIM). INASIM runs high-level
simulations of APT attacks on ICS networks in super-real
time. Simulated networks are organized into levels as in the
PERA architecture. The network is deﬁned by three types of
objects: nodes, networking devices, and PLCs. Nodes are com-
puters that APTs may compromise to spread to other nodes
and to launch attacks on PLCs. Nodes may be workstation
hosts or servers, and are connected to one-another through
networking devices. The number of nodes, devices, PLCs, and
the speciﬁc network connectivity, are all conﬁgurable in the
simulator.

The agent interacts with the environment through an API
which deﬁnes the actions the agent can take and how these
actions impact the state of the network. The API for this simu-
lator is designed to provide a generic interface for interacting
with networks of different sizes and connectivity. Using the
API, a defender agent may choose to take investigation or
mitigation actions on any node at each decision step. The re-
sult of an action taken on a node depends on that node’s state.
Node states deﬁne how a node has been modiﬁed by the APT,
either to enable greater APT permissions, to prevent detection,
or to ensure persistence. Table 1 lists the various compromise

4

Lateral Movement (L3)Discover, compromise, and escalate hostsProcess DiscoveryCompromise and analyze  data historian server# compromised hosts ≥ thresholdHistorian analysis startedNetwork DiscoveryDiscover VLAN subnets and switchesHMI CaptureCompromise initial L2 HMI NodeLateral Movement (L2)Discover, compromise, and escalate HMIsAll VLANs discoveredHMI compromised# HMIs compromised ≥ threshold PLC DiscoveryLocate PLC and required data for attack# PLCs discovered ≥ threshold Execute AttackDisrupt PLC processesFirmware CompromiseFlash firmware on targeted PLCs# PLCs firmware compromised  ≥ threshold OPC CompromiseCompromise OPC ServerOPC compromised and cleanedAll VLANs discoveredExecute AttackDisrupt PLC processes# PLCs discovered ≥ threshold conditions that each node may experience. A node may have
several compromise conditions at a given time.

Defender investigation actions do not change the states
of nodes on the network, but instead stochastically provide
alerts of the status of compromised nodes that are investigated.
Mitigation actions change the states of nodes to impede the
progress of APT attacks. Each action also has a cost deﬁning
relatively how much it disrupts nominal network operation.
See the appendix for tables of available defender actions,
along with each action’s duration and cost, that were chosen to
model the industrial control network security decision making
problem.

We model an Intrusion Detection System (IDS) that gener-
ates alerts based on activity in the network. Alerts provide the
IP address of the node or networking device that generated the
alert as well as a level of severity from 1 (lowest) to 3 (high-
est). Severity levels are based on the state of the node that
generated the alert. Each action attempted by the APT may
generate an alert from the IDS. Actions occurring only on a
single node generate alerts with a probability given by the
alert rate associated with an action. For actions that generate
messages on the network, this rate is multiplied by the de-
vice factor for each networking device it passes through. The
baseline IDS also passively generates alerts on compromised
nodes with probability 0.1. Finally, the IDS generates false
alerts at each level each hour of simulated time with 5 ∗ 10−2
probability, 5 ∗ 10−3 probability, and 2.5 ∗ 10−3 probability
for level 1, 2, and 3 severity respectively. See the appendix
for full details on alert distributions.

3.2 Attacker Model

Attacker behavior is generated from a stochastic ﬁnite state
machine. Finite state machines are graphical models of intelli-
gent behavior in which sub-routines are executed from a given
machine state until certain exit conditions are met. The ma-
chine state then transitions to a new state and sub-routine until
a ﬁnal goal state is reached. The proposed ﬁnite state machine
deﬁnes the high level actions available to the attacker based
on the current state, similar to previous work modelling attack
scenarios with graphs [38]. The graph in Figure 3 shows the
trajectories available to the APT at the tactic level. See the
appendix for full details on actions available to the attacker.
The APT can either choose to access the PLCs by com-
promising the OPC server or the Level 1 HMI nodes. The
APT can also choose whether to disrupt the ICS process or to
destroy the plant equipment. Destroying equipment requires
ﬂashing PLC ﬁrmware, which is not required for PLC pro-
cess disruption. Because disruption does not require PLC
ﬁrmware compromise, it is easier to achieve than PLC de-
struction, though the effects are less costly to the ICS than
equipment destruction. Compromising the OPC server is eas-
ier as it only requires a single Level 2 node be compromised
prior to attack. However, sending commands from Level 2

to Level 1 creates more alerts than sending commands from
Level 1 HMIs. Each APT attack is additionally deﬁned by
several quantitative parameters. The APT parameters are

• Lateral Threshold: The number of Level 2 nodes to be
compromised before escalating attack to the next phase.

• PLC Threshold: The number of PLCs to discover before

executing attack.

• Labor budget: Each task takes some amount of time (in
hours) and has an associated labor rate (in labor hours
per clock hour). Each APT has a labor budget deﬁning
the maximum executable labor hours per wall clock hour.

The default APT policy has a lateral threshold of 3, and
a PLC threshold of 15 for destroying attacks and 25 for dis-
rupting attacks. The default policy assumes two full-time
attackers at keyboard for a labor-rate of two.

4 Solution Method

In this work, we formulate network security as a sequential de-
cision making problem. Sequential decision problems model
the environment with states s that evolve according to po-
tentially stochastic dynamics. An agent takes actions a that
condition state transition distributions T (s(cid:48) | s, a) and generate
rewards r(s, a, s(cid:48)). In many problems, the state of the world
is not known. In these partially observable domains, agents
receive noisy observations according to o ∼ Z(o | s, a).

A sequential decision problem is solved by an ac-
tion sequence that maximizes the expected value V (s) =
E(cid:2)
∑t γt r(st , at )(cid:3) for all states in the trajectory. Reinforcement
learning methods learn a policy π : ot−τ:t (cid:55)→ at that maps a his-
tory of observations to actions through repeated trial-and-error
with the environment. In each trial, actions are taken accord-
ing to the current policy until a terminal condition is reached.
At the end of the trial, the policy is updated to improve the
expected performance. Reinforcement learning methods that
use neural networks to represent the learned policy are known
as deep reinforcement learning.

In this work, we solve a the simulated network security
problem using deep reinforcement learning (DRL). We study
reinforcement learning because it is a model-free method
that does not require a formal, mathematical model of net-
work dynamics to solve. Deep reinforcement learning with
neural networks learns behavior that can generalize to situ-
ations not experienced in training. Reinforcement learning
can train policies that are robust to mismatch between simula-
tion and real-world dynamics and to adversarial approaches.
Neural network policies can be continually updated from
data observed during deployment. Unlike many formal game-
theoretic methods, multi-object reinforcement learning can
scale to very large networks [39].

We propose a learning method based on Deep Q-Network
(DQN) learning. DQN is a popular method in which

5

Condition

Effect

Scanned
Initial Compromise
Reboot Persistence
Admin Access
Credential Persistence
Malware Cleaned

Allows APT to gain command and control
Allows APT to take actions on and from node
Prevents control loss from reboot
Enables additional actions
Prevents control loss on password change
Reduces probability of alert generation

Required Condition

None
Scanned
Initial compromise
Initial compromise
Admin access
Admin access

Table 1: Node States. This table deﬁnes the possible compromise states a node may experience as a result of APT actions.

the learned neural network predicts the expected value
Q(ot−τ:t , a) = E(cid:2)r(st , a) + γV (st+1)(cid:3) of taking each action in
the action space for a given input history [40]. The Q-network
is used as a policy by taking the action with the highest pre-
dicted value a∗ = argmaxaQ(ot−τ:t , a). The values of the net-
work parameters are tuned with batches of experiences over
episodes of the task.

The network security problem presents many challenges
to existing reinforcement learning methods, including high
sample complexity, difﬁcult exploration, and potentially ad-
versarial dynamics. Deep RL training typically requires large
amounts of data [41]. The amount of trials required to solve a
task tends to grow with the size of the input and output space
of the problem. In tasks with very large input spaces, output
spaces, or very long time horizons, the odds of ﬁnding a suc-
cessful trajectory through random exploration are low [42].
Adversarial dynamics emerge when an opposing agent in the
environment observes the learned agent and adapts their own
behavior to make the problem more difﬁcult. In this work, we
present methods to address the ﬁrst two concerns and leave
adversarial training to future work.

The focus of this work is on making security decisions
given some limited awareness of the network environment.
This work does not propose methods to learn effective ﬁlter-
ing of network observations, as this is a separate active area
of research [18]. To avoid confounding the two problems, we
implemented a dynamic Bayes network (DBN) ﬁlter as a sur-
rogate for integrated network awareness systems. The DBN
learns approximate network dynamics and alert probabilities
from data and provides a distribution over each node state at
every time step.

4.1 Problem Deﬁnition

The objective of the learning task is to prevent an attack with
as few disruptive actions as possible. To encode this objec-
tive into a suitable optimization target, we deﬁne a per-step
reward function. The reward function r(s, a, s(cid:48)) deﬁnes the
"goodness" of taking action a from state s and then transi-
tioning to state s(cid:48). The policy objective is to maximize the
discounted sum of rewards over a time horizon.

Reward each step is based on the fraction of PLCs that

are operating nominally and the inconvenience caused by the
ACSO actions taken. The reward function is deﬁned as

r(s, a) = rPLC(s, a) + λrIT(s, a) + rterm(s, a)
(cid:17)

(cid:16)

1 − 0.05ndisrupted − 0.1ndestroyed
(cid:17)

rPLC(s, a) =

rIT (s, a) =

cost(a)

(cid:16)
1 − ∑
a∈At

rterm(s, a) =

1
1 − γ

1{stime ≥ tmax}

(1)

(2)

(3)

(4)

where λ is a weighting parameter and ndisrupted and ndestroyed
give the total number of disrupted and destroyed PLCs, re-
spectively. At is the set of all actions completing at time t.

The ﬁrst term rPLC rewards the agent for preventing the
PLCs from being compromised. The second term imposes
a penalty for ACSO actions, where each action is assigned
a cost based on its perceived burden to network operations.
The ﬁrst and second terms encode opposing objectives. To
prioritize PLC defense, the λ weighting term can be set to less
than 1. In this work, λ = 0.1 for all experiments. The ﬁnal
term rewards the agent for reaching the episode time limit
tmax. The 1/(1 − γ) magnitude of the terminal reward ensures
the optimal state value does not drift with episode time.

Actions that are more effective at securing compromised
nodes are given a higher cost. For example the low-disruption
action of rebooting a workstation has a cost of 0.01, while
the more disruptive action of re-imaging a server has a cost
of 0.05. Action costs were set to provide a range of high,
medium, and low disruption actions.

Each time step of the simulation corresponds to one hour
of real-world time. We run each trial episode for a maximum
of 5,000 time steps or approximately six months. This was
chosen to ensure the attacker would have enough time to
complete an intrusion campaign. The time discount factor was
set to γ = 0.9995. Having a discount rate near 1 allows the
agent to learn to account for action effects far into the future.
Given this discount rate, the maximum discounted return for
a given episode is 2200, though achieving this would require
defending the network without taking any actions.

6

4.2 Training Algorithm

4.3 Dynamic Bayes Network

We train our neural network to estimate the value of tak-
ing each available action a given a history of previous ob-
servations on the network h. The proposed solution uses
an augmented DQN algorithm to learn a policy deﬁned by
our attention-based neural network. We implemented sev-
eral extensions to the baseline algorithm, based on studies
of Rainbow DQN [43]. The included extensions are double-
DQN [44], prioritized experience replay [45], and n-step TD
loss [46].

The policy is trained to minimize the error in its estimation,
deﬁned by a temporal difference (TD) loss. The TD loss
uses a bootstrap estimation method to reduce training sample
variance and a second copy of the policy network, called the
target network, to reduce sample over-estimation. The training
loss for a given step is

(cid:16) t+n
(cid:13)
(cid:13)
∑
(cid:13)
τ=t+1

γτ−t rτ + γnQφ

(cid:0)ht+n, at+n

(cid:1)(cid:17)

(cid:13)
(cid:13)
− Qπ(ht , a)
(cid:13)

(5)

where ht is the history of observations at time t, Qπ is the
action value estimate of the policy network and Qφ is the
action value estimate of the target network. The action at+n is
given as argmaxa(cid:48)Qπ(ht+n, a(cid:48)). The (cid:107) · (cid:107) represents the Huber-
loss norm. This loss is calculated over batches of importance-
weighted samples from an experience replay buffer and used
to estimate the gradient for each network update.

In addition to the task reward presented in eq. (1), a shap-
ing reward was deﬁned based on the non-biased potential
formulation [47]. This reward was designed to incentivize the
agent to secure compromised nodes. The shaping function
was deﬁned as

rshape(s, a, s(cid:48)) = γ(AδW + BδS)

(6)

where δW and δS are changes in the number of workstations
and servers compromised by the APT from state s to s(cid:48), re-
spectively, and A and B are weight factors. The weighted sum
of eq. (1) and eq. (6) were used for training. Only eq. (1)
was used for evaluation. This additional shaping reward was
critical to enable the agent to learn a meaningful policy, as
the baseline reward function provided too sparse of a signal
over the long episode lengths.

Training hyper-parameters were tuned by a grid search
training on a smaller ICS network with ten level 2 worksta-
tion nodes, three level 1 workstations nodes, and thirty PLCs.
We searched over the shaping reward weight, the observation-
history interval, the target network update frequency, and the ε-
greedy exploration decay schedule. The parameter set leading
to the highest average performing policy over several seeds
after 500 episodes was selected. We used the PyTorch frame-
work for neural network implementation and training [48].
Numerical values for the training parameters as well as addi-
tional practical details on the training process can be found in
the appendix.

The network simulation generates alerts of potential compro-
mise on nodes at each time step. The perception problem
of inferring the compromise state from sequences of these
observations is not the focus of this work. To ﬁlter observa-
tions we implemented a dynamic Bayes network (DBN). The
DBN takes observations ot from the network and produces a
distribution over the compromise state of each node. We refer
to the distribution over possible states as a belief. A graphical
depiction of the DBN is shown in ﬁg. 4.

Figure 4: Dynamic Bayes Network. This ﬁgure shows the
DBN ﬁlter for node si. Circular nodes denote random vari-
ables each with an associated conditional probability distri-
bution, and square nodes represent actions. The probability
distribution associated with a random variable is conditioned
on the values of that variable’s parent nodes, indicated by
the arrow direction. Values for yellow nodes µt−1 and st−1
are calculated from previous beliefs. The cyan nodes at and
ot represent values known during calculation. The probabil-
ity distribution associated with the variable st is the updated
belief.

The belief of a node i is represented as a vector bi, where
each element gives the probability of that node having been
compromised in a particular way. The DBN calculates beliefs
by recursively applying Bayes rule. Given a belief on a node
state st−1
i and
i
observing ot

, the new belief after having taken action at

i is calculated as

ηP(ot

i | st

i, at

i) ∑
st−1
i ∈S

P(st

i | st−1
i

, µt−1, at

i)bt−1
i

(st−1
i

)

(7)

where η is a normalizing constant and µt−1 is a summary
statistic on the complete network state.

7

stist-1iµt-1atiotiFigure 5: Neural Network. Inputs for each network node are passed into the dynamic Bayes network ﬁlter. The DBN updates the
prior belief of each node’s compromise with the new action and observation. These vectors are stacked and passed through a
self-attention sub-graph, to provide global context to each latent vector. These vectors are then passed through fully connected
sub-graphs to output action value estimates for each node.

The update in eq. (7) approximates the optimal Bayesian
update. The true optimal update conditions the transition
probability of a node on the state of every other node in the
network. Calculating this would require enumerating over
every possible combination of node states and is intractable
for networks with more than a small number of nodes. Instead,
we condition on summary values, such as the total number
of compromised nodes. These values are represented by the
vector µ.

The conditional probability distributions are not generally
known. We learned these distributions from data. We ran
1,000 episodes with the APT and a defender policy that takes
random actions each step. For each episode we recorded the
states, actions, and observations at each step. Using these val-
ues, we calculated probability tables for each distribution. We
validated the DBN performance by measuring the maximum
Kullback-Leibler (KL) divergence [49] of the DBN belief and
the true state over many episodes.

4.4 Neural Network Policy

The ACSO problem has input and output spaces with dimen-
sions that scale with the number of nodes in the ICS net-
work. Specialized neural architectures can take advantage of
structure inherent in input data to improve training efﬁciency.
For example, convolutional nets leverage spatial invariance
in images and recurrent networks encode correlation in se-
quences [50, 51]. We designed the neural network shown
in ﬁg. 5 using attention mechanisms [52] to accommodate

the large input space without signiﬁcant degradation of ex-
planatory capacity or intractable growth in size. Attention
mechanisms have been shown to improve learning efﬁciency
on tasks with exchangeable inputs [53, 54].

Each node observation is passed through the DBN and a
belief vector is returned. The belief vectors are then stacked
and input to a global attention sub-graph. This sub-graph
allows the network to learn which features of neighboring
nodes are relevant to the value function of actions on a given
node. In this problem, we assume that we can directly observe
whether or not a PLC process has been successfully disrupted
or equipment destroyed. A vector encoding the PLC states
is concatenated with the node belief vectors. These contex-
tualized node vectors are then passed to feed-forward output
sub-graphs. All sub-graphs of a given node type share the
same parameter set, so a growth in the number of nodes does
not cause a growth in the total number of network parameters.

5 Experiments

We tested the ability of the trained ACSO to defend against
simulated attacks. We ran trials with the same simulation pa-
rameters used during policy training. To test policy robustness,
we also tested performance under several deviations to APT
behavior. We also implemented and evaluated several base-
line policies for comparison. For each test, we ran 100 attack
scenarios and measured performance using three metrics:

• PLCs ofﬂine: the total number of PLCs disrupted or

8

Global Attention Node State30xHost MLP Host Action Values3xServer MLPServer Action ValuesHost ObservationsServer ObservationsDBN FilterPrior BeliefNew BeliefPLC StatesPolicy

Discounted Return

Final PLCs Ofﬂine Average IT Cost Average Nodes Compromised

ACSO
DBN Expert
Playbook
Semi Random

2149.9 ± 0.2
1970.5 ± 26.6
2142.6 ± 0.1
2071.9 ± 0.1

0.0 ± 0.0
5.6 ± 0.9
0.0 ± 0.0
0.0 ± 0.0

0.15 ± 0.0
0.40 ± 0.0
0.21 ± 0.0
0.60 ± 0.0

0.56 ± 0.0
0.62 ± 0.0
0.63 ± 0.1
0.88 ± 0.6

Table 2: Nominal Evaluation Results. This table presents the performance of the ACSO agent and the three baseline policies
tested with nominal simulation parameters. The discounted task return, total number of PLCs ofﬂine, average disruptive IT cost
per step, and the average number of nodes compromised by the APT per hour are reported. For each metric, the mean and one
standard error bounds over 100 episodes are given.

destroyed during the attack. Minimizing this is the main
objective of a defender agent.

• Average IT cost: the total disruption to nominal IT net-

work behavior caused by the ACSO actions.

Policies that take actions more aggressively will incur a
higher average action cost.

• Average nodes compromised per hour: how many total
nodes in levels 1 and 2 that the APT has command and
control on in the network. This metric gives a holistic
measure of the total network compromise during the
episode.

We perturbed the APT policy in two ways. First, we
changed how effective the APT was at evading detection.
To do this, we modiﬁed the APT cleanup effectiveness pa-
rameter that made its node cleanup actions more effective
at preventing alerts. Higher cleanup effectiveness denotes a
lower probability of detection. We tested policy performance
with cleanup effectiveness rates both higher and lower than
the nominal rate used in training.

Our second APT perturbation modiﬁed the qualitative pa-
rameters to create a more aggressive attacker. The number of
lateral 3 hosts controlled before beginning process discovery
was changed from three to one. The number of PLCs compro-
mised required to execute the attack was changed from ﬁfteen
to ﬁve when destroying equipment and from twenty-ﬁve to
ten when disrupting. The result of these changes is an attacker
model, referred to as APT2, that moves more quickly through
the tactics graph shown in ﬁg. 3, while being less resilient to
setbacks, as the APT has less redundant access.

5.1 Baseline Policies

We implemented three different baseline policies to compare
to the ACSO. A random policy simulates independent security
analysts and users taking actions on the network. The random
policy takes actions by sampling action type from a static
categorical distribution and a node uniformly from the nodes
of the appropriate type in the network. A playbook policy
executes pre-deﬁned COAs when speciﬁc alert conditions

are met. Finally, an expert policy takes actions stochastically
conditioned on the compromise predictions of the dynamic
Bayes net.

The security automation playbook baseline deﬁnes ﬁxed
responses to alerts. For the scope of defender actions and IDS
alerts that we consider, it is infeasible to create playbooks
that address the entire decision making space. We only design
playbooks that are triggered by a single alert, of which there
are 3 types: An alert triggered by an APT action, a passive
alert generated on a host node, and a passive alert generated
on a server node. See the appendix for ﬁgures of the play-
book courses of action triggered by a passive host alert. This
playbook baseline is more automated and therefore provides
faster response than most playbooks used in practice, as most
security playbooks today defer to human analysts at crucial
decision points.

The expert policy samples actions from a distribution con-
ditioned on the output of the DBN ﬁlter. The DBN estimates
the compromise state of each node in the network from the
observed alerts, and the most appropriate action is chosen
based on the believed state of a node. For example, if a node
is believed to be compromised, with no reboot persistence,
then a reboot action will be taken, and if a node is believed
to be compromised with credential persistence, a re-image
action will be taken.

6 Results

The results from the experiments with the baseline environ-
ment are shown in table 2. The learned ACSO policy perfor-
mance meets or exceeds the performance of every baseline
policy in all four metrics. Both the playbook and the ACSO
policies greatly outperformed the random baseline, suggesting
that coordinated, automated response signiﬁcantly improves
network security over random, decentralized behavior. All
policies, except for the DBN expert were able to successfully
prevent any PLCs from being attacked, however, only the
ACSO and playbook did so with average network disruption
under 0.25. The ACSO was the most efﬁcient defender, with
an average IT disruption rate nearly 30% lower than the dis-

9

(a) Final PLCs Ofﬂine

(b) Average Layer 2/1 Nodes Compromised

Figure 6: APT Cleanup Effectiveness Experiments. These ﬁgures show the performance of the defender policies under perturba-
tions to the APT cleanup effectiveness. APT cleanup effectiveness modiﬁes the probability of a scan detecting the APT after the
APT removed malware from the node. An effectiveness of 0.5 denotes that detections are 0.5 as likely after cleanup. The ACSO
was trained with an APT cleanup effectiveness of 0.5.

ruption rate of the playbook. The ACSO agent was also the
most effective at securing compromised network nodes, with
just 0.56 nodes compromised per-hour, which is 12% lower
than the compromise rate of the playbook and 37% lower than
the random rate.

The performance of the policies under perturbations of the
APT evasion effectiveness are shown in ﬁg. 6. As can be seen,
as the cleanup effectiveness increases from the nominal 0.5
level, both the ACSO and playbook eventually begin to fail.
The playbook fails sooner and more sharply than the ACSO.
Additionally, the ACSO policy maintains a lower action cost
and lower average nodes compromised for most cleanup rates.
The DBN policy is not sensitive to the change in perception
characteristics in the environment, and uses a more aggressive
approach, leading to a signiﬁcantly higher action cost than
the other policies. These results show that while ACSO was
not trained in an environment with different detection proba-
bilities after APT cleanup, it is robust to moderate changes.

The results from experiments done with a less cautious at-
tack policy demonstrate that ACSO is more robust to changes
in APT behavior than other policies we evaluated. See ﬁg. 10
in the appendix for full results. The ACSO performance in
this setting is similar to its performance against the nomi-
nal attacker policy. The ACSO is able to fully prevent PLC
compromise while using the least disruptive actions of any
of the tested policies with an average IT cost of 0.149, a 31%
reduction when compared to the playbook policy, which had
the next best average IT cost of 0.216. The more aggressive at-
tacker was able to compromise PLCs in a number of episodes
when tested against the playbook policy, causing an average
of 0.45 PLCs to be ofﬂine at the end of an episode. Both

the DBN-based policy and random policy are slightly more
effective against APT2 compared to the APT1, and the poli-
cies’ average action costs suggest that they maintain a fairly
aggressive stance towards defending the network.

These results showing greater robustness of the ACSO are
partially due to ability of the neural network abstractions to
generalize learned behavior to new experiences.

7 Conclusions

In this work, we demonstrate the feasibility of using deep rein-
forcement learning to develop agents to autonomously secure
a computer network. We presented a high-level simulation
environment able to efﬁciently generate the large amounts
of trial data needed for RL training. We proposed a solution
method to overcome several of the challenges that the ACSO
learning problem poses to conventional deep RL. The dy-
namic Bayes network ﬁlter approximated an optimal observa-
tion ﬁlter, allowing this study to focus on the decision making
problem without simultaneously learning a perception system.
The attention-based neural network allowed the learned agent
to scale to large, varying-sized computer networks without
an intractable growth in parameters. We proposed a shaping
reward which allowed the agent to learn over the long episode
lengths without biasing the ﬁnal solutions.

We tested the ACSO performance against baseline policies
in several experiments. Results show that the ACSO is able to
outperform baseline policies when tested in the same environ-
ment in which it was trained. The results also show that the
ACSO is more robust to changes in the APT behavior than a
comparable rules-based playbook policy, despite not having

10

been trained with examples of that behavior.

This work is an initial study of deep reinforcement learning
in holistic computer network defense. Future work should
prioritize improving the testing and training infrastructure,
for example by improving the ﬁdelity and ﬂexibility of sim-
ulations. Data-efﬁcient methods to validate learned policies
performance in high-ﬁdelity emulations should be developed.
Methods for pre-training models using simulations, and ﬁne-
tuning for deployment to speciﬁc ICS networks should be
explored. This work only considered a narrow set of attacker
behaviors modeling current, known attack trajectories. As
attackers continue to evolve and incorporate learning and
planning in their attacks, focus should be placed on adver-
sarial learning methods that can discover and obviate new
attacks before they are observed in the real-world.

Acknowledgment

This material is based upon work supported by the Johns
Hopkins University Applied Physics Laboratory, as well as the
Institute for Assured Autonomy. The work was also supported
by the Stanford University Institute for Human-Centered AI
(HAI) and Google Cloud.

Availability

The code for the network simulation environment and APT
policies are available as a Julia repository. The code required
to train and test the neural network agent are also available.
Scripts to run all experiments presented are provided. Links
to code will be made public after anonymous review.

References

[1] R. Das and M. Z. Gündüz, “Analysis of cyber-attacks in
IoT-based critical infrastructures,” International Journal
of Information Security Science, vol. 8, no. 4, pp. 122–
133, 2020.

[2] T. Alladi, V. Chamola, and S. Zeadally, “Industrial con-
trol systems: Cyberattack trends and countermeasures,”
Computer Communications, vol. 155, pp. 1–8, 2020.

[3] M. Li, W. Huang, Y. Wang, W. Fan, and J. Li, “The
study of APT attack stage model,” in IEEE/ACIS In-
ternational Conference on Computer and Information
Science (ICIS), vol. 15, 2016, pp. 1–5.

[4] R. Langner, “Stuxnet: Dissecting a cyberwarfare
weapon,” IEEE Security and Privacy, vol. 9, no. 3, pp.
49–51, 2011.

[5] G. Liang, S. R. Weller, J. Zhao, F. Luo, and Z. Y. Dong,
“The 2015 Ukraine blackout: Implications for false data

injection attacks,” IEEE Transactions on Power Systems,
vol. 32, no. 4, pp. 3317–3318, 2016.

[6] A. Di Pinto, Y. Dragoni, and A. Carcano, “TRITON:
The ﬁrst ICS cyber attack on safety instrument systems,”
in Black Hat USA, 2018, pp. 1–26.

[7] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Math-
ieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell,
T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss,
I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou,
M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen,
V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine,
Ç. Gülçehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yo-
gatama, D. Wünsch, K. McKinney, O. Smith, T. Schaul,
T. P. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps,
and D. Silver, “Grandmaster level in StarCraft II using
multi-agent reinforcement learning,” Nature, vol. 575,
no. 7782, pp. 350–354, 2019.

[8] C.-J. Hoel, K. Driggs-Campbell, K. Wolff, L. Laine,
and M. J. Kochenderfer, “Combining planning and deep
reinforcement learning in tactical decision making for
autonomous driving,” IEEE Transactions on Intelligent
Vehicles, vol. 5, no. 2, pp. 294–305, 2019.

[9] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep
reinforcement learning for multiagent systems: A review
of challenges, solutions, and applications,” IEEE Trans-
actions on Cybernetics, vol. 50, no. 9, pp. 3826–3839,
2020.

[10] G. Dulac-Arnold, R. Evans, P. Sunehag, and B. Coppin,
“Reinforcement learning in large discrete action spaces,”
Computing Research Repository, 2015.

[11] M. J. Hausknecht and P. Stone, “Deep recurrent Q-
learning for partially observable MDPs,” in AAAI Con-
ference on Artiﬁcial Intelligence (AAAI), 2015, pp. 29–
37.

[12] M. Andrychowicz, D. Crow, A. Ray, J. Schneider,
R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel,
and W. Zaremba, “Hindsight experience replay,” in
Advances in Neural Information Processing Systems
(NeurIPS), 2017, pp. 5048–5058.

[13] J. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Un-
terthiner, J. Brandstetter, and S. Hochreiter, “RUDDER:
return decomposition for delayed rewards,” in Advances
in Neural Information Processing Systems (NeurIPS),
2019, pp. 13 544–13 555.

[14] S.-U. Normand and D. Tritchler, “Parameter updating in
a Bayes network,” Journal of the American Statistical
Association, 1992.

11

[15] N. Kaloudi and J. Li, “The ai-based cyber threat land-
scape: A survey,” ACM Computing Surveys (CSUR),
vol. 53, no. 1, pp. 1–34, 2020.

[16] M. J. Herring and K. D. Willett, “Active cyber defense:
a vision for real-time cyber defense,” Journal of Infor-
mation Warfare, vol. 13, no. 2, pp. 46–55, 2014.

[17] A. Burke, “Robust artiﬁcial intelligence for active cyber
defence,” Alan Turing Institute, Tech. Rep, 2020.

[18] G. Apruzzese, M. Colajanni, L. Ferretti, A. Guido, and
M. Marchetti, “On the effectiveness of machine and
deep learning for cyber security,” in IEEE Conference
on Cyber Conﬂict (CyCon), 2018, pp. 371–390.

[19] J. Song and J. Alves-Foss, “The DARPA cyber grand
challenge: A competitor’s perspective,” IEEE Security
& Privacy, vol. 13, no. 6, pp. 72–76, 2015.

[20] A. Nguyen-Tuong, D. Melski, J. W. Davidson, M. Co,
W. Hawkins, J. D. Hiser, D. Morris, D. Nguyen, and
E. Rizzi, “Xandra: An autonomous cyber battle system
for the cyber grand challenge,” IEEE Security & Privacy,
vol. 16, no. 2, pp. 42–51, 2018.

[21] T. Avgerinos, D. Brumley, J. Davis, R. Goulden, T. Nigh-
swander, A. Rebert, and N. Williamson, “The Mayhem
cyber reasoning system,” IEEE Security & Privacy,
vol. 16, no. 2, pp. 52–60, 2018.

[22] F. M. Zennaro and L. Erdodi, “Modeling penetration
testing with reinforcement learning using capture-the-
ﬂag challenges and tabular Q-learning,” arXiv preprint
arXiv:2005.12632, 2020.

[23] “Framework for improving critical infrastructure cyber-
security,” National Institute of Standards and Technol-
ogy (NIST), Tech. Rep., Apr 2018. [Online]. Available:
https://doi.org/10.6028/NIST.CSWP.04162018

[24] V. Paxson, “Bro: A system for detecting network intrud-
ers in real-time,” Computer networks, vol. 31, no. 23-24,
pp. 2435–2463, 1999.

[25] M. Roesch et al., “Snort: Lightweight intrusion detec-
tion for networks.” in Lisa, vol. 99, no. 1, 1999, pp. 229–
238.

[26] D. Burks, “Security onion,” Securityonion. blogspot.

com, 2012.

[27] “Claroty solution brief. tech. rep.” 2016. [Online]. Avail-
able: https://s3.amazonaws.com/clarotypublic/Claroty_
Solution_Brief.pdf

[28] “Disrupt ot threats with tenable.ot,” 2021. [Online].
Available: https://www.tenable.com/products/tenable-ot

[29] N. Dhir, H. Hoeltgebaum, N. Adams, M. Briers,
A. Burke, and P. Jones, “Prospective artiﬁcial intel-
ligence approaches for active cyber defence,” arXiv
preprint arXiv:2104.09981, 2021.

[30] J. R. Gosler and L. Von Thaer, “Resilient military sys-
tems and the advanced cyber threat,” Defense Science
Board Technical Report, p. 30–31, 2013.

[31] R. M. Lee, M. J. Assante, and T. Conway, “Analysis of
the cyber attack on the Ukrainian power grid,” Electric-
ity Information Sharing and Analysis Center (E-ISAC),
vol. 388, 2016.

[32] C. Eaton

and D. Volz,

berattack
line].
cyberattack-forces-closure-of-largest-u-s-reﬁned-fuel-pipeline

cy-
[On-
https://www.wsj.com/articles/

“U.S.
closure,” May

forces
Available:

pipeline
2021.

supply

[33] J. Peiser, “A hacker broke into a ﬂorida town’s
poison
it with
water
lye, police
[Online]. Avail-
able: https://www.washingtonpost.com/nation/2021/02/
09/oldsmar-water-supply-hack-ﬂorida/

to
said,” Feb 2021.

tried

and

[34] K. Stouffer, J. Falco, and K. Scarfone, “Guide to in-
dustrial control systems (ICS) security,” NIST special
publication, vol. 800, no. 82, pp. 16–16, 2011.

[35] S. Meckl, G. Tecuci, D. Marcu, M. Boicu, and A. B.
Zaman, “Collaborative cognitive assistants for advanced
persistent threat detection,” in AAAI Conference on Arti-
ﬁcial Intelligence (AAAI), 2017, pp. 171–178.

[36] B. E. Strom, A. Applebaum, D. P. Miller, K. C. Nick-
els, A. G. Pennington, and C. B. Thomas, “MITRE
ATT&CK®: Design and philosophy,” Mitre Technical
Report, 2016.

[37] T. J. Williams, “The Purdue enterprise reference archi-
tecture,” in Information Infrastructure Systems for Man-
ufacturing, vol. B-14, 1993, pp. 43–64.

[38] S. Panda and Y. Vorobeychik, “Near-optimal interdiction
of factored MDPs,” in Conference on Uncertainty in
Artiﬁcial Intelligence, 2017.

[39] R. Elderman, L. J. Pater, A. S. Thie, M. M. Drugan, and
M. A. Wiering, “Adversarial reinforcement learning in
a cyber security simulation.” in ICAART (2), 2017, pp.
559–566.

[40] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves,
I. Antonoglou, D. Wierstra, and M. A. Riedmiller, “Play-
ing atari with deep reinforcement learning,” Computing
Research Repository, 2013.

12

[41] C. Wei and T. Ma, “Data-dependent sample complexity
of deep neural networks via Lipschitz augmentation,”
in Advances in Neural Information Processing Systems
(NeurIPS), 2019, pp. 9722–9733.

[42] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and
J. Clune, “First return, then explore,” Nature, vol. 590,
no. 7847, pp. 580–586, 2021.

[43] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Os-
trovski, W. Dabney, D. Horgan, B. Piot, M. G. Azar,
and D. Silver, “Rainbow: Combining improvements in
deep reinforcement learning,” in AAAI Conference on
Artiﬁcial Intelligence (AAAI), 2018, pp. 3215–3222.

[44] H. van Hasselt, “Double Q-learning,” in Advances
in Neural Information Processing Systems (NeurIPS),
2010, pp. 2613–2621.

[45] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prior-
itized experience replay,” in International Conference
on Learning Representations (ICLR), 2016.

[46] R. S. Sutton and A. G. Barto, Reinforcement Learning:
An Introduction, 2nd ed. The MIT Press, 2018.

[47] A. Y. Ng, D. Harada, and S. J. Russell, “Policy invari-
ance under reward transformations: Theory and applica-
tion to reward shaping,” in International Conference on
Machine Learning (ICML), 1999, pp. 278–287.

[48] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Rai-
son, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,
J. Bai, and S. Chintala, “Pytorch: An imperative style,
high-performance deep learning library,” in Advances
in Neural Information Processing Systems (NeurIPS),
2019, pp. 8024–8035.

[49] S. Kullback and R. Leibler, “On Information and Sufﬁ-
ciency,” The Annals of Mathematical Statistics, vol. 22,
no. 1, pp. 79 – 86, 1951.

[50] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Ima-
geNet classiﬁcation with deep convolutional neural net-
works,” in Advances in Neural Information Processing
Systems (NeurIPS), 2012, pp. 1106–1114.

[51] K. Cho, B. van Merrienboer, Ç. Gülçehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio, “Learning
phrase representations using RNN encoder-decoder
for statistical machine translation,” in Conference on
Empirical Methods in Natural Language Processing
(EMNLP), 2014, pp. 1724–1734.

[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,

“Attention is all you need,” in Advances in Neural Infor-
mation Processing Systems (NeurIPS), 2017, pp. 5998–
6008.

[53] J. Mern, D. Sadigh, and M. J. Kochenderfer, “Object ex-
changability in reinforcement learning,” in International
Conference on Autonomous Agents and Multiagent Sys-
tems (AAMAS), 2019, pp. 2126–2128.

[54] ——, “Exchangeable input representations for reinforce-
ment learning,” in American Control Conference (ACC),
2020, pp. 3971–3976.

Network Simulator

The ICS network attack simulator (INASIM) is organized
into functional modules, as shown in ﬁg. 7. The code is orga-
nized around the Network Simulation module, which deﬁnes
the network structure and the dynamics of how APT and
ACSO actions affect the network state. The APT Module
deﬁnes the actions available to the APT and provides a base-
line attacker policy. The IDS Module deﬁnes how alerts are
generated based on the network state and agent actions. The
Reward Module allows a reward function to be deﬁned for the
decision making problem.

Figure 7: ICS Network Simulation Organization: The ﬁg-
ure shows the modules composing the ICS network attack
simulation. Network status and dynamics are deﬁned by the
Network Simulation Module. The APT Agent Module de-
ﬁnes what actions the APT can take and how they affect the
network dynamics. The IDS Module deﬁnes how alerts are
generated from the network state. The Orchestrator Module
deﬁnes what actions are available to an external ACSO agent,
and the reward module allows deﬁnition of a reward function.

The ACSO agent interacts with the environment based on
the API deﬁned in the Orchestrator Module. The orchestrator
module deﬁnes which actions the ACSO can take and how
these actions impact the state of the network. An internal API
deﬁnes the interfaces between modules, allowing develop-
ers to implement new modules to simulate different attack
scenarios.

An external API allows simulations to be run with any user
deﬁned ACSO agent compatible with either the OpenAI Gym
or POMDPs.jl interface.

13

INASIM EnvironmentAPT Agent ModuleNetwork Simulation ModuleIDS ModuleOrchestrator ModuleReward ModuleACSONetwork Simulation Module

IDS Module

The network simulation module is the main element of the
Gollum environment. The network is deﬁned by three types of
objects: nodes, networking devices, and PLCs. As described
earlier, nodes are computing elements that APTs may com-
promise to spread to other nodes and to launch attacks on
PLCs. Nodes may be workstation hosts or servers. Nodes are
connected to other nodes through networking devices. Each
node is connected to a switch and each switch is connected to
single a router. Each router is connected to switches or other
routers through ﬁrewalls.

Workstation nodes are homogeneous and there are three
different types of server nodes. The open platform commu-
nications (OPC) server provides direct access to scan and
control the PLCs. The data historian server records the per-
formance of the ICS process under control. In this simulation,
attackers must compromise the data historian to gain knowl-
edge of the process before executing an attack. The domain
controller node allows network credentials to be accessed and
manipulated. In the current simulation, however, this func-
tionality is disabled and the domain controller is functionally
equivalent to a workstation.

All nodes are organized around switches into VLANs. In re-
ality, VLANs do not require physical switches, though for sim-
plicity each VLAN is assumed to be connected to a discrete
switch, though workstation nodes can be instantly moved be-
tween VLANs. Nodes connected to the same switch may more
easily discover one another than nodes on separate VLANs.
Each level has a dedicated router and external ﬁrewall. Level
1 PLCs are assumed to be connected to a level 1 switch as
shown in ﬁg. 2.

The simulation state advances in event-driven steps. Each
time the environment is stepped, actions generate events in a
queue, and the next event is returned from the queue, along
with the time at which the event occurs. Event steps are mea-
sured in integer units.

The state of each node on the network deﬁnes its com-
promise state and location. A node’s location deﬁnes which
switch it is connected to in the network and is represented by
an IPv4 address. A node compromise state deﬁnes how the
node has been modiﬁed by the APT, either to enable greater
APT access or to prevent the ACSO from detecting or secur-
ing the intrusion.

The full set of actions that may be taken by an APT at
a given step is deﬁned by the state of the nodes under its
control and their neighbors. The APT has full knowledge of
the compromise state of all nodes under its control. If a node
the APT has previously scanned has been moved, it is not
aware and must re-scan the node to discover it.

14

As described above, the IDS Module speciﬁes how alerts are
generated from network events and APT actions.

As described above, the IDS Module speciﬁes how alerts
are generated from network events and APT actions. For
APT actions the alert probability and level are deﬁned by the
APT action type. Alerts from APT actions may be generated
from a single probability draw, a draw per-hour the action is
taking place, or based on the message trafﬁc generated. In
the last case, an APT action that originates from one node
and is taken on another (e.g. compromising a new node from
a previously compromised node) generates message trafﬁc
across the network. In this case, each device that the message
passes through may generate an alert. Given a base probability
of alert p, messages passing through a switch generate an
alert with probability of p, messages passing through a router
generate an alert with probability 2p, and messages passing
through a ﬁrewall generate an alert with probability 5p.

Orchestrator Module

The orchestrator module deﬁnes the actions the ACSO may
take and their effect on the network state. The ACSO may
choose to take investigation and/or mitigation actions each
time step, each with the effects described above (namely, in-
vestigation actions may provide alerts of compromised nodes’
status while mitigation actions change the status of nodes to
impede attacks).

Table 3 shows the investigation actions available to the
defender. Detection probability is given for instances when
malware is present. Investigations are assumed to not generate
false alarms. Simple Scan and Human Analysis actions are
taken by taking a single draw from a Bernoulli distribution
with probability equal to the listed detection probability. If
this draw is true, then an alert is generated at time equal to
the current time current plus the action duration. Advanced
scan actions continue until either an alert is created or the
max duration is reached.

In addition to investigation actions, the ACSO can take
actions to secure compromised nodes. These actions are de-
scribed in table 4. These actions will clear all compromise
conditions for the targeted node unless the listed countermea-
sure condition is present. In this case, the action will have no
effect. Additionally, the ACSO may quarantine workstation
nodes. The Quarantine action moves the targeted node from
its current VLAN to the corresponding quarantine VLAN in
the same level. The quarantine action will return a previously
quarantined node.

Action parameters such as duration and cost were solicited
from a small group of cyber security researchers. The primary
goal of the design of these parameters was to model the deci-
sion making space and trade-offs a cyber operator must make.
In real networks, effective mitigation actions are usually more

Action Description

Simple
Scan
Advanced
Scan
Human
Analy-
sis

Simple background
malware scan
Disruptive malware
scan
Task human analyst
to node

Detect
Probability

Duration Cost

0.03/0.012

0.05/0.028

0.5/0.25 8

0.01

0.03

0.05

Table 3: ACSO (Defender) Investigations. This table lists
the investigation actions the ACSO may take on any node in
the network. The completion time is deterministic and the
alerts are generated stochastically. The detection probability
is given with and without Malware Cleaned condition.

costly, while less effective actions usually are less disruptive.
Our goal was to build a ﬂexible simulation environment with
parameters that capture the aspects critical to the sequential
decision making problem. These parameters could be adapted
to suit speciﬁc settings as needed.

APT Agent Module

The APT Agent Module deﬁnes the actions that the APT can
take and the observations the APT receives. Any attacker
policy can be used with the provided API. The module also
provides a baseline attacker policy deﬁned as a stochastic
ﬁnite state machine (FSM). The APT actions are given in ta-
ble 5. These actions are organized by which machine state
uses them in the baseline policy, however, they may be called
at any point by an attacker policy. Each time step, the APT can
attempt one or more of the given actions, where each action
attempt succeeds with the probability shown. The amount
of time to complete each action is sampled from a Binomial
distribution taking n trials each with probability p. Changes to
network state caused by the action are enacted at the current
time plus the duration. Each action attempt may generate an
alert from the IDS. Actions occurring only on a single node
generate alerts with a probability given by the alert rate. For
actions that generate messages on the network, this rate is
multiplied by the device factor for each networking device it
passes through.

APT Baseline Policy

The baseline APT agent is deﬁned by a ﬁnite state machine
policy. Each machine state deﬁnes a stochastic sub-policy
determining what actions to take in the environment and a
set of transition criteria that set when to transition to a new
machine state. The FSM is parameterized by two discrete,

Action

Description

Reboot

Reset Pass-
word

Re-image
Node

Reset PLC

Replace
PLC

cycle

Power
node
Cleaned
cached creden-
tials
Clean disk and
reinstall OS

Reset PLC con-
ditions
Replace
destroyed PLC

Counter-
measures

Reboot
Persistence
Credential
Persistence

Cost
(host/server)

0.01/0.03

0.03/0.05

–

–

–

0.05/0.1

0.02

0.04

Table 4: ACSO (Defender) Mitigations. This table lists the
actions the ACSO can take to secure nodes compromised by
the APT. If the countermeasures listed are not present on the
targeted node, the node will be returned to nominal. The ﬁrst
set of actions listed can be taken on computing nodes and
the second set can be taken on PLCs. The disruption cost
for each action is given. For the computing node actions, the
disruption costs for actions taken on workstations/servers are
shown.

qualitative parameters and three quantitative parameters. The
qualitative parameters are

• Attack Objective: Deﬁnes whether the objective of the
APT is to disrupt the ICS process or to destroy the plant
equipment.

• Attack Vector: Deﬁnes whether the APT accesses the
PLCs by compromising the OPC server or the Level 1
HMI nodes.

The implications of the separate parameter selections are
described in the main body of the work.

Figure 8: APT Attack Phases. This ﬁgure shows the progres-
sion of APT machine states through the course of an attack.
All attacks start in lateral movement phase and progress ac-
cording to the APT objective and access vector.

There are four possible conﬁgurations of the FSM qualita-
tive parameters which determine the progression of machine
states through an attack. Figure 8 shows the possible machine
state progressions. Each of the APTs follows a unique policy

15

Lateral MovementNetwork DiscoveryOPC CompromiseHMI CompromisePLC DiscoveryDisruptDestroyAction

Description

Success Prob

Time Dist. Alert Rate

Lateral Movement

Scan targeted VLAN for nodes
Gain initial control over a node
Set reboot persistence

Scan
Compromise
Reboot Persist
Escalate Privilege Gain administrator access
Credential Persist
Cleanup

Set credential change persistence
Remove malware ﬁles

Discover VLAN
Discover Server
Analyze Historian Analyze compromised historian

Scan for occupied VLANs
Scan for a server on VLAN

Vertical Movement

Discover PLC
Flash Firmware
Disrupt PLC
Destroy PLC

Scan VLAN for PLCs
Corrupt PLC ﬁrmware
Disrupt PLC process
Destroy PLC equipment

Attack

1.0
0.9
1.0
1.0
1.0
1.0

1.0
1.0
1.0

1.0
1.0
1.0
1.0

(60, 0.9)
(60, 0.8)
(4, 0.9)
(22, 0.9)
(4, 0.9)
(4, 0.9)

(60, 0.9)
(60, 0.9)
(600, 0.9)

(24, 0.875)
(1, 1.0)
(8, 0.9)
(1, 1.0)

0.01
0.05
0.05
0.05
0.05
0.05

0.05
0.01
0.00

0.03
0.5
0.9
1.0

Table 5: APT Attacker Actions. This table shows the actions available to the attacker agent during the simulation. The action
name and a description of the simulated effect are given. Each action attempt succeeds with the probability shown. The amount
of time to complete each action is sampled from a Binomial distribution with the n and p parameters shown. The alert rate gives
the base rate of an alert being generated by the action. For actions that generate messages on the network, this rate is multiplied
by the device factor for each networking device it passes through.

based on which conﬁguration it is in. Every APT starts in a
lateral movement state and ends in a disrupt or destroy attack
execution state. The APT remains in each state until an exit
criteria is met. If during execution, an earlier phase criteria is
no longer satisﬁed, the policy will revert to that earlier phase
before continuing.

Neural Network Details

The conﬁguration of our proposed neural network is given
in table 6. Sub-graphs for each of the three different node-
types have the same dimensions. A single learned vector

Sub-graph

No. Layers Hidden Size Output Size

Attention CNN
Global Attention
Output MLP

4
2
2

64
128
128

32
128
–

Table 6: Proposed Network Conﬁguration. This table gives
the parameters for each component sub-graph of the proposed
attention neural network. The sub-graphs for each node type
have the same dimensions.

representation is passed into a global attention graph to corre-
spond to the output vector for the no action value. The MLP
output heads all have tanh activation on the ﬁnal layer output.

The baseline convolutional network architecture parame-
ters are shown in table 7. The baseline architecture has three
1D convolution layers, striding in the temporal dimension.
The hidden dimensions were chosen to be as small as pos-
sible without shrinking the dimension of the latent vectors
too quickly. The ﬁnal output vector was factored into a set
of vectors giving the action value for each node on the ICS
network plus the value of no action.

Training Details

Reinforcement learning for both our proposed neural network
and the baseline convolutional network was conducted using
hyperparameters found via grid-search. The grid search was
conducted on a smaller version of the problem with ten level
2 workstation nodes, three level 1 HMI nodes and thirty PLCs.
The parameters and corresponding values searched over were

• Shaping reward weight: 0, 1, 1

γ = 1000

• Observation-history interval: 64, 256

• Target network update frequency: 1000, 5000, 10000

• ε-greedy decay rate: 0.999, 0.9999

where the selected parameters are shown in bold. The perfor-
mance of each parameter setting was evaluated based on the

16

Layer

Input dim.

Filter size

Stride Output dim. Activation

Conv 1
Conv 2
Conv 3
MLP 1
Output

872
256
128
256
256

4
4
4
–
–

4
4
4
–
–

256
128
64
256
329

LeReLU
LeReLU
LeReLU
LeReLU
tanh

Table 7: Baseline Network Conﬁguration. The table gives the parameters for each layer of the baseline convolutional network.
The convolution strides in the temporal dimension.

average discounted return after 500 episodes of training. We
calculated the n-step TD loss with n = 8 and batches of 64.
The parameters for pretraining were selected by coordinate
ascent, ﬁrst testing the target margin δ with margin weighting
λ set to 0.1. The ﬁnal values used were δ = 0.05 and λ = 0.1.
The Adam optimizer was used with an initial learning rate set
to 10−4 for all neural network learning.

The networks were trained using virtual machines (VM)
on a cloud computing service. Each VM had 64 GB of RAM
and 16 virtual CPUs from the Intel Sandy Bridge, Ivy Bridge,
Haswell, Broadwell, and Skylake lines. Every VM used an
Nvidia Tesla T4 GPU with CUDA-enabled PyTorch v1.8.
Training 1.25 million steps of the proposed neural network
took approximately 100 hours. All packages used were pro-
vided under open use licenses with limited restrictions, such
as MIT or BSD. Our source code is provided with an open-
source MIT license. Details can be found in the source code
license.md ﬁles.

Playbook Policy

responses to alerts, and is meant to be representative of current
automation standards that used predeﬁned logic to respond
to triggers. When an alert is generated from the simulation,
it triggers an automated course of action depending on the
level of the severity associated with the alert. The courses
of action are comprised of scans alternating with mitigation
actions based on the result of the scan, terminating when no
more alerts are generated for the node.

Additional Results

Figure 10 contains additional experimental results indicating
the performance of the various defender policies when faced
with APT1 and APT2. As described in the results section,
these indicate the robustness of the ACSO policy, particularly
in comparison to the playbook policy baseline.

Figure 9: Playbook policy. The decision logic triggered by a
node alert in the playbook policy.

The security automation playbook baseline deﬁnes ﬁxed

17

(a) Final PLCs Ofﬂine

(b) Average IT Cost

(c) Average Layer 2/1 Nodes Compromised

Figure 10: APT Policy Experiment Results. Mean of the metric over 100 trial episodes along with one standard error bounds
for each policy in environments with different APTs. APT1 is the attacker used while training ACSO, while APT2 is a more
aggressive attack policy.

18

