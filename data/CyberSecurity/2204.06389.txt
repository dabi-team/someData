CRUSH: Contextually Regularized and User anchored Self-supervised
Hate speech Detection

Souvic Chakraborty∗†, Parag Dutta1∗#, Sumegh Roychowdhury∗†, Animesh Mukherjee†
#Indian Institute of Science (IISc), Bangalore, KA, IN - 560012
paragdutta@iisc.ac.in
†Indian Institute of Technology (IIT), Kharagpur, WB, IN - 721302
{chakra.souvic,sumeghtech,animeshm}@gmail.com

2
2
0
2

y
a
M
4

]
L
C
.
s
c
[

2
v
9
8
3
6
0
.
4
0
2
2
:
v
i
X
r
a

Abstract

The last decade has witnessed a surge in the
interaction of people through social network-
ing platforms. While there are several positive
aspects of these social platforms, their prolif-
eration has led them to become the breeding
ground for cyber-bullying and hate speech. Re-
cent advances in NLP have often been used
to mitigate the spread of such hateful con-
tent. Since the task of hate speech detection
is usually applicable in the context of social
networks, we introduce CRUSH, a framework
for hate speech detection using User Anchored
self-supervision and contextual regularization.
Our proposed approach secures ≈ 1-12% im-
provement in test set metrics over best per-
forming previous approaches on two types of
tasks and multiple popular English language
social networking datasets.
Note: This paper contains materials that may
be offensive or upsetting to some people.

1

Introduction

Today, the world is more connected than ever in
the history of mankind. This can primarily be at-
tributed to: (i) the technological advancements that
have made affordable internet connections and de-
vices available to people, and (ii) the social net-
working platforms that have hosted and connected
these people. As a result, even people divided
by geography can seamlessly interact in real-time
without stepping outside their homes. In fact, social
networks are an integral part of today’s society.

We, however, are more concerned about the pit-
falls of this global widespread use of social net-
works. The unprecedented and rapid explosion
in social networking and social media use has left
many people — particularly the youth, women, and
those from ethnic and religious minority groups —
vulnerable to the negative aspects of the online

∗Equal Contribution.
1Work done while at IIT Kharagpur.
Accepted in Findings of NAACL-HLT 2022.

Figure 1: [Best viewed in color] Hateful content tend to
cluster together in common threads and usually come
from few hateful users in social media. We stress
on this informed assumption to learn better represen-
tations using self supervision and contextual regular-
ization. In the sub-graph shown in the pic, the textual
content is in the form of posts, comments on the posts
(optional), and replies to the comments (optional). nu
is the total number of users in the network (dynamic),
and each of the text sequences can be attributed to one
of the users.

world like sexual harassment, fake news and hate
speech (Jakubowicz, 2017). The number of toxic
users dumping their radically biased views and
polarising content onto these networks have bur-
geoned to such a level that they are causing political
discord and communal disharmony. Therefore such
posts must be intervened upon and ﬁltered before
they have the intended effect on the mass. With a
huge number of posts on many popular social net-
working websites every second, manual ﬁltering
for hate speech does not scale. Hence, automating
hate speech detection has been the primary focus of
many researchers in recent years, both in academia
and industry alike.

Figure 1 depicts and describes a sub-graph of a

 
 
 
 
 
 
typical social network. It is essential to leverage
this structure within social networks for infusing
network context while identifying hate speech. We
investigate and notice that the social graph context
can be disentangled into two components: (i) Post
context: the context in the neighborhood around
the text sequence, i.e., the sub-graph consisting of
posts, comments, and replies (see Figure 1) and
(ii) User context: the context from all the exist-
ing text sequences in the network that originated
from the same user (see, for instance, the connec-
tions emanating from users 1, 2, nu etc.
in the
Figure 1). Relying on the echo-chamber effect, we
accordingly propose a framework that uses self su-
pervision from unlabelled data harnessed from the
social networks (Gab & Reddit in our case), so that
we can use contextual information (user & post
context) to generate better representations from in-
put sentences for hate speech related tasks. The
main contributions of this paper are:

(i) First, we propose UA (User Anchored self-
supervision), a self-supervised contrastive
pre-training objective. Essentially we try to
incorporate the mapping from text sequences
to users into the language model. In addition,
we provide a Robust UA strategy that incor-
porates the hate speech downstream task in-
formation into our proposed UA pre-training
approach.

(ii) Next, we propose CR (Contextual Regular-
ization), a regularization strategy based on
the ﬁndings of Mathew et al. (2020). Here
we introduce a loss based on the informed
assumption that the neighboring comments
and replies (in the social graph) of a hateful
comment is more likely to be hateful than
the comments/replies in the vicinity of a non-
hateful comment. It helps us to regularize the
supervised learning paradigm by learning bet-
ter representations of text sequences in social
network context.

(iii) We experiment with two types of hate speech
tasks – classiﬁcation and scoring – across
three datasets. We show that our approach
secures ≈ 1-4% improvement in F1-scores
(for classiﬁcation tasks) and ≈ 12% improve-
ment in mean absolute error (for scoring task)
when compared to best competing baselines.

To the best of our knowledge, we are the ﬁrst to use
text sequence based self-supervision in hate speech
using user characteristics. One of the key technical

contribution that this paper makes is to show that it
is more advantageous to use context to regularize
a classiﬁcation model than directly infusing the
context into the model. Also, none of our proposed
approaches require any additional annotation effort
or introduce any extra parameter into the training
pipeline, and are therefore scalable.

2 Related work

Hate speech is heavily reliant on linguistic com-
plexity. Waseem (2016) showed that classiﬁcation
consensus is rare for certain text sequences even
among human annotators. Automatic detection of
hate speech is further strongly tied to the develop-
ments in machine learning based methods.

Until recently, feature engineering was one of
the popularly used techniques. Gitari et al. (2015)
designed several sentiment features and Del Vigna
et al. (2017) used the sentimental value of words
as the main feature to measure the hate constituted
within a text sequence. Empirical evidence was
provided by Malmasi and Zampieri (2017) indicat-
ing that n-gram features and sentiment features can
be successfully applied to detect hate speech. Ro-
driguez et al. (2019) constructed a dataset of hate
speech from Facebook, and consequently proposed
a rich set of sentiment features, including negative
sentiment words and negative sentiment symbols,
for detecting hateful text sequences. As witnessed
by the above works, it was widely believed that
sentiment played an important role in detecting
underlying hate.

More recently, deep learning based methods
(Badjatiya et al., 2017) have garnered considerable
success in detecting hate speech since they can
extract the latent semantic features of text and pro-
vide the most direct cues for detecting hate speech.
Zhang et al. (2018) developed a CNN+GRU based
model to learn higher-level features. (Kshirsagar
et al., 2018) passed pre-trained word embeddings
through a fully connected layer, which achieved
better performance than contemporary approaches
despite its simplicity. Tekiro˘glu et al. (2020) con-
structed a large-scale dataset for hate speech and
its responses and used the pre-trained language
models (LM) for detection. These methods demon-
strated the considerable advantages of interpreting
words in the context of given sentences over static
sentiment based methods.

Self-supervision and auxiliary supervision have
also been explored for hate speech detection. Hate-

BERT (Caselli et al., 2021) used the Masked Lan-
guage Modelling (MLM) objective to learn con-
textual hate semantics within text sequences from
individual Reddit posts. HateXplain (Mathew et al.,
2021) used human annotators to obtain rationale
about the text containing hate speech and then ap-
plied the same for improving their language model.
Researchers have previously tried context infusion
in the inputs (Menini et al., 2021; Vidgen et al.,
2021; Pavlopoulos et al., 2020). Del Tredici et al.
(2019) showed that user context helps in down-
stream tasks. On the other hand, Menini et al.
(2021) and Vidgen et al. (2021) show that contex-
tual classiﬁcation is harder given contextual anno-
tations. A similar theme recurs when Pavlopoulos
et al. (2020) shows that context infusion does not
easily increase the performance of classiﬁers in
context of toxicity detection in text. Alternatively
instead of using context directly as input we use it
as a regularizer to improve the classiﬁer by learn-
ing better contextual representations while training.
During inference, we use only the post text, thus
adding no computation overhead.

We ﬁnd that using self-supervision for hate
speech detection systems leveraging the associated
context within a social network is heavily under-
explored. s demonstrated by Mathew et al. (2020),
learning in a broader socio-personal context of the
users and contemporary social situations is also
very important. The authors showed the clustering
tendency of hateful text sequences on speciﬁc hate-
ful user timelines and after speciﬁc events (tempo-
ral clustering). However, to the best of our knowl-
edge, no prior work in hate speech detection has
explored self-supervision in these directions using
the context information.

3 Proposed approaches

Assumptions about invariance(s) are required in all
self-supervised learning objectives. We make the
following two assumptions articulated below.

(i) In the masked language modeling (MLM) ob-
jective, the invariant is that the conditional
probability distribution of the vocabulary to-
kens (or words) for each masked token can be
reasonably estimated given the context (in the
form of tokens) around the masked token.
(ii) In the User Anchored self-supervision (UA)
objective, we assume that the users’ writing
style and bias (speciﬁcally cultural and in-
group bias) are invariant (Hughes et al., 2012).

Hence, the inverse mapping of a post to the
corresponding user should be estimable sub-
ject to the language understanding capability.
We denote the model being trained for the down-
stream tasks as M. M has two modules: (a) an
encoder for encoding the input sentences, and (b)
a classiﬁer or regressor for mapping the hidden
representations generated by the encoder into one
of K classes and a single value respectively. For
instance, the encoder in M can be modeled by a
transformer (Vaswani et al., 2017). See Figure 2
for block diagram of our proposed approaches.

3.1 Phase I: Incorporating hateful content
into the language model through
continual pre-training (CP)

We start with pre-trained language models and con-
tinue pre-training them by minimizing the standard
MLM objective using text sequences (posts) accu-
mulated from a variety of social network datasets.
The procedure for domain adaptation in LMs can
be found in Gururangan et al. (2020) and a pro-
cedure tailored for hateful words can be found in
HateBERT. We use the text sequences available on
Pushshift2 in addition to RAL-E from HateBERT
for pre-training our Hate-infused Language Model
(HateLM).

3.2 Phase II: User Anchored self-supervision

(UA)

Some users are more biased than others and hence
are more prone to post toxic content (Mathew
et al., 2020). We employ a User Anchored self-
supervision (UA) objective in the next pre-training
phase to compare the users’ writing style. Hence,
given an LM is capable of language understanding,
it should also be able to distinguish between users’
writing styles from their posts with high probabil-
ity, when the pool of posts from various users is
not very large.

Here we use a self-supervised method based on
contrastive pre-training with negative samples to
efﬁciently incorporate UA into an LM. This nega-
tive sampling makes the approach highly scalable3.
For the text sequence S corresponding to the ith
sampled post and user u0, we try to decrease some
distance metric between representation of S and an-
other sentence by u0 among the nu available users

2https://ﬁles.pushshift.io/gab
3Owing to the extremely large number of users in a plat-
form and the dynamic nature of the graph, it is infeasible to
simply add a user classiﬁcation network after the encoder

Figure 2: [Best viewed in color] An illustration of the various phases of training and inference for the classiﬁcation
task. The regression task will have a similar structure except for a regressor head instead of a classiﬁer head in
(c) and (d). The blue arrows indicate the forward pass, and the orange arrows indicate the backward pass. (a)
corresponds to Phase I, i.e. continual pre-training using self-supervised MLM objective on hateful sequences to
incorporate contextual hate understanding. (b) corresponds to Phase II, i.e.
the User Anchored self-supervised
learning objective, it depicts the scalable contrastive objective model that does not add any additional parameters.
(c) corresponds to Phase III, i.e. our contextual regularization procedure where we add post and user context. And
ﬁnally (d) shows the inference phase which does not require any additional context.

while increasing the distance metric with sentences
authored by other users in a contrastive fashion.
Next, we sample k << nu number of users uni-
formly at random, without repetition, from among
the list of all users in the social network. Adding
u0 along with the k sampled users in a set, we get:
Ui = {ui0, ui1, ..., uik|∀j1,j2∈{0,..,k}uij1 (cid:54)= uij2}
(1)

The set of posts Pi made by users in Ui.
Pi = {pi0, pi1, ..., pik|pi0 (cid:54)= S}
pi0 cannot be same as S because our positive sam-
ple needs to be different from the original post. The
remaining ∀j∈[k]{pij} become negative samples.

(2)

For Phase II pre-training and updating the param-
eters θE of the encoder in M, we start by sampling
an anchor sequence ai ≡ S from our dataset. We
pass ai through encoder in M to obtain zi.

zi = FθE (ai)
Next, we generate Pi by sampling as described
above. We pass all of the pij’s ∈ P through our
encoder as well, to get embeddings:

(3)

Zi = {zi0, zi1, ..., zik}
We use a self-supervised contrastive loss inspired
from Khosla et al. (2020) and modify it for our
purposes:

(4)

LUA = E

i∼U ([N ])

(cid:34)

(cid:32)

−log

exp(zi · zi0)
j∈[k] exp(zi · zij)

(cid:80)

(cid:33)(cid:35)

(5)

Here, ‘·’ represents the inner product and U(.) rep-
resents discrete uniform distribution. Since there
are no additional models, only θE parameters need
to be updated during backpropagation.

Robust UA: During training UA objective ex-
hibits the problem of over-ﬁtting. After a certain
number of epochs, the information in the language
model accumulated during the CP phase gets over-
shadowed by information obtained from the UA
phase. In order to handle the problem, the model
needs to be ﬁne-tuned after every epoch. Since this
is not feasible, we propose a method for a more
robust training method aligned to our downstream
task of hate speech classiﬁcation. Since we already
have the annotations in the target dataset, we add an
auxiliary task to align the model parameter updates
towards the downstream task rather than diverge as
a consequence of over-ﬁtting.

For an anchor sequence ai ≡ S sampled from
the training set, we get the original class label of
the example and get a positive example p by sam-
pling a new post from the training set belonging
to the same class. Similarly, we sample two posts
from each of the remaining classes in order to get a
set of l = 2(nc − 1) negative examples, where nc
is the number of classes in the downstream classiﬁ-
cation task. Hence similar to Pi in Equation 2, our
auxiliary post set corresponding to the ith sampled
sequence is as follows:

P i = {pi0, pi1, ..., pil|pi0 (cid:54)= S}
Subsequently, we pass them through the encoder

(6)

in M and generate auxiliary embedding set:

and the most likely class assignment would be

Zi = {zi0, zi1, ..., zil}
Similar to the supervised contrastive loss described
in Equation 5, we add the following auxiliary loss:
(cid:33)(cid:35)
(cid:32)

(7)

(cid:34)

Laux = E

i∼U ([N ])

−log

exp(zi · zi0)
j∈[l] exp(zi · zij)

(cid:80)

(8)
Here, it is the same i that was being sampled in
Equation 5. Finally we take a convex combination
of both the losses to get:

LRobustUA = λLUA + (1 − λ)Laux

(9)

where, 0 < λ < 1 is a hyper-parameter.

Note: We do not have classes for a regression
task. Therefore, we group the labels into K clusters
by using K-means clustering where K is a hyper-
parameter. Then we use the associated cluster la-
bels as a proxy for the class labels.

3.3 Phase III: Contextual Regularization

(CR)

Our primary assumption in this approach is that:
A post is inﬂuenced by its context. We demon-
strate how to exploit the intuition “hate begets hate”
(Mathew et al., 2020) to our advantage.

3.3.1 CR in classiﬁcation

HateLM and UA, i.e., the methods described in
section 3.1 and 3.2 (with the exception of Robust
UA training) were self-supervised; hence, no in-
formation about the annotations available in the
training set was used. After getting the pre-trained
model in Phase II, we ﬁne-tune using the annota-
tions available for the dataset. We consider a pair
of text sequence and its corresponding label (cid:104)S, y(cid:105)
sampled from the training set uniformly at random,
where y ∈ [K] denotes the true class label of the
sampled sequence S. Usually, we would get the
vector embedding z by passing S through the en-
coder. We would then have used the classiﬁer in
M to get the vector v of K dimensions as follows:

v = FC(z; θC)
where θC parameterizes the classiﬁer in M. Ac-
cording to the model, the probability of the sample
belonging to jth class among K classes would be
as follows:

(10)

P[class(S) = j] =

exp(vj)
k∈[K] exp(vk)

(cid:80)

(11)

ˆy = arg max

P[class(S) = j]

(12)

j∈[K]

The cross-entropy loss would be calculated as:

LCE = E

i∼U ([N ])

[−log P[class(S) = y]]

(13)

We propose an additional method for regulariza-
tion of the model using the contextual information
during ﬁne-tuning. For the given S we sample
at most na posts from the same thread (all com-
ments/replies concerning the parent post) where
the comment is posted (post context) and at most
nb more posts from the timeline of the user who
posted S (user context). Both post and user context
is sampled without replacement. So, we generate a
context set C of S for the ith text sequence sampled
from the dataset:

Ci = {ci1, ..., cina, ci(na+1), ..., ci(na+nb)} (14)
We then generate the vector v using S as mentioned
in Equation 10. Next we generate Vi from Ci:
Vi = {vi1, ..., vim|∀j∈[m]vij = M(cij)}

(15)
where m = na + nb and, M(.) ≡ FθC (FθE (.)).
Our contextual loss from Vi is as follows:
exp(vijt)
k∈[K] exp(vijk)

LCCE,i =

−1
m

(cid:88)

log

(cid:80)

(cid:32)

(cid:33)

j∈[m]

(16)
where t is the true class of the original labelled post.
Therefore the auxiliary contextual cross-entropy
loss is calculated as:

LCCE = E

[LCCE,i]

(17)

i∼U ([N ])
Our ﬁnal contextual regularization loss function
is a linear combination of the losses mentioned in
Equation 13 and 17 as described below:
Contextual = λLCE + (1 − λ)LCCE
Back-propagating this loss we update the parame-
ters θC and θE , corresponding to the classiﬁer and
the encoder in M respectively, as one might have
done normally.

Lclassiﬁcation

(18)

3.3.2 CR in regression
The regression task is almost similar to classiﬁca-
tion as mentioned in section 3.3.1 with the excep-
tion that the y ∈ R in the tuple (cid:104)S, y(cid:105) sampled
from the training set, and the output of the model
M being a real value r rather than a vector v of K
dimensions. Since we use a regressor, Equation 10
changes to

r = FR(z; θR)

(19)

Dataset

Annotated Total users

Labels

HateXplain
LTI-GAB
Ruddit

samples
10,000
30,500
6,000

3,300
4,900
4,200

Hate speech, Offensive, Normal
Toxic and Non-Toxic
Regression (-1 to +1)

Max context
sampled (per user)
100
50
20

Table 1: Dataset statistics for the datasets we used for performing the experiments along with the additional
unsupervised data we collected. All datasets are in the English language.

where θR parameterizes the regressor in M.

The cross-entropy loss in Equation 13 becomes

squared loss as follows:
LMSE = E

i∼U ([N ])

(cid:2)(yi − ri)2(cid:3)

(20)

where yi is the label associated with the ith ex-
ample sampled from the training dataset, ri is the
predicted value by model M.

The techniques for selection of the context dis-
cussed in section 3.3.1 remain unaltered since our
post and user context is unsupervised. However,
the set of vectors Vi becomes a set of real values
Ri generated as follows:

models during each of the above-mentioned sep-
arate training phases (HateBERT during Phase
I; BERT and HateXplain during the remaining
phases). Consistent performance improvement ir-
respective of the language model would again indi-
cate the advantage of our approach.
(iii) We use three datasets from two different social
networks and two types of tasks (classiﬁcation and
regression). This establishes the capability of our
methods to solve a range of heterogeneous tasks
across contrasting datasets.

We next describe the datasets and the implemen-

tation details of our approaches.

Ri = {ri1, ..., rim|∀j∈[m]rij = M(cij)}

(21)

4.1 Datasets

And our auxiliary contextual mean squared loss is
calculated as the follows:

LCM SE = E

i∼U ([N ])





−1
m

(cid:88)

j∈[m]



(yi − rij)2

 (22)

Our ﬁnal contextual regularization loss function is
thus a linear combination of the losses mentioned
in Equation 20 and 22 as described below:
Contextual = λLMSE + (1 − λ)LCMSE

Lregression

(23)

4 Experiments

We experiment with the downstream task of hate
speech detection. We establish the effectiveness
of our proposed approaches by demonstrating that
they are: (i) mutually independent of each other,
(ii) independent of the base language model used,
and (iii) applicable across various social network
datasets. We validate these claims by comparing
our approaches with the following baselines:
(i) To establish mutual independence among the
training phases, we train a base language model
(BERT from Devlin et al. (2019)) with each of the
training phases separately. An improvement over
the performance of the base model in each phase
would indicate the advantage of our approach over
naïve training.
(ii) To establish independence from the base lan-
guage model, we train multiple baseline language

Social networks: In particular we experiment with
two popular social networks (a) gab.ai (GAB),
and (b) reddit.com (Reddit). Our choice of the
dataset was guided by the availability of the context
graph and additional data collection time. Baum-
gartne (2018) had scraped GAB and made the net-
work freely available for academic use. On the
other hand, Reddit has an API4 available to get
the public domain data. Therefore, these websites
were favorable for our experiments. Since Red-
dit involved additional data collection (a time con-
suming process), we chose a popular dataset that
contains less than 10,000 datapoints.

Annotated hate speech data: We use the follow-
ing english hate speech datasets for our experi-
ments (See Table 1 for more information on dataset
statistics) – (i) HateXplain-GAB dataset (Mathew
et al., 2021) (contains data from GAB), (ii) LTI-
GAB dataset (Qian et al., 2019) (contains data from
GAB) and, (iii) Ruddit (Hada et al., 2021) (contains
data from Reddit).

We use only the GAB subset of the annotated
data from both the datasets (i) and (ii), because the
social network context graph for GAB is publicly
available. The GAB subset of dataset (i) has around
10K annotated data samples, which are already di-
vided into 80-10-10 train-val-test split. Dataset (i)

4https://www.reddit.com/dev/api/

Phase

Model

Existing approaches
as baselines

Continual pre-train-
ing (CP phase)

TF-IDF
BERT
HateXplain†5

HateBERT
HateLM

User Anchored self-
supervision (UA phase) HateXplain + UA

BERT + UA

Contextual Regu-
larization (CR phase)

UA and CR phase
together

HateLM + UA

BERT + CR
HateXplain + CR
HateLM + CR

BERT + UA + CR
HateXplain + UA + CR
CRUSH(cid:62)

HX-GAB5 (3 class)
Macro-F1
Acc

0.6337
0.6763
0.6905

0.6843
0.6882

0.6950
0.7058
0.7087

0.6819
0.6935
0.6924

0.7017
0.7099
0.7133

0.5604
0.6376
0.6511

0.6458
0.6493

0.6632
0.6680
0.6711

0.6452
0.6555
0.6558

0.6673
0.6702
0.6749

LTI-GAB (2 class)
Acc

F1 (toxic) MSE

Ruddit (regression)

0.8993
0.9141
0.9177

0.9166
0.9174

0.9192
0.9211
0.9215

0.9176
0.9199
0.9200

0.9211
0.9236
0.9234

0.8824
0.9019
0.9062

0.9040
0.9058

0.9089
0.9105
0.9117

0.9051
0.9088
0.9096

0.9104
0.9122
0.9149

0.1128
0.1041
0.1036

0.1029
0.1018

0.0994
0.0989
0.0958

0.1019
0.1011
0.0995

0.0968
0.0955
0.0921

MAE

0.2651
0.2521
0.2520

0.2516
0.2474

0.2367
0.2329
0.2229

0.2438
0.2410
0.2342

0.2314
0.2291
0.2188

Table 2: Experimental outcomes of our approaches. Our model CRUSH ≡ HateLM + UA + CR. For reporting
results (except CR and UA+CR phases) the encoder models were attached with a classiﬁer (refer section 4.2) and
ﬁne-tuned (using the loss mentioned in Equation 13). Across the columns: we show the results with three datasets.
In HateXplain (HX)’s and Learning to Intervene (LTI)’s results, higher metrics are better. In Ruddit results, lower
metrics are better. Across the rows: we show the various models grouped by the phase of training along with
corresponding baselines. In each group, we have indicated the best-performing models’ results corresponding to
the evaluation metric in bold. The overall best performing models’ results have additionally been italicized. The
models denoted by † (the best competing baseline) and (cid:62) (CRUSH) are signiﬁcantly different (M-W U test
with p < 0.05) across all datasets.

sure of hate within a sentence ranging between
−1 and 1, with higher numbers corresponding to a
higher extent of hate. There are about 6K examples
in this dataset. We use a train test split similar to
that we used in the dataset (ii).

Our pre-processing procedure for all the textual
data, both labeled and unlabelled is exactly the
same as that of HateXplain. (Mathew et al., 2021).
Unlabelled data for self-supervision: We get the
threads corresponding to the annotated Reddit dat-
apoints using the Reddit API and use that as our
unlabelled corpora for self-supervision in case of
the regression task. For GAB, we use the whole
network datadump available on Pushshift (Baum-
gartne, 2018) as mentioned in Section 3.2.

4.2

Implementation details

For our encoder, we use the pre-trained
bert-base-uncased that is available on hug-
gingface 6. By continual pre-training on this model,
we obtain our HateLM using the text sequences
from GAB and RAL-E as described in section 3.1.
Afterward, we can obtain a 768 dimensional pooled

5HateXplain refers to both a dataset and the corresponding
model provided in the paper. However, since we only use the
GAB subset of the HateXplain dataset, we require to ﬁne-tune
the HateXplain model on this subset, for it to be comparable
to other models, and therefore it is considered a baseline.

6https://huggingface.co/bert-base-uncased

Figure 3: [Best viewed in color] F1-scores of correctly
classiﬁed posts grouped by three user contexts and
three post contexts. The improvement along the user
contexts and post contexts demonstrate the validity of
the UA and CR phases of CRUSH training respectively.

has three class annotations (hate speech, offensive,
and normal). Dataset (ii) contains intervention sen-
tences along with which sentences to intervene and
a binary label - hate and non-hate. We use a 90-
10 train-test split with the random seed 2021, and
among the training set we use 10% randomly sam-
pled data for validation. Dataset (iii) was collected
from Reddit and is labeled for regression task. The
dataset contains ratings corresponding to the mea-

Figure 4: [Best viewed in color] Comparison of model error in low data regime across the three datasets. Our
model (black solid line) consistently outperforms existing approaches that do not use the social network context.
For plots showing comparisons with non-contextualized LMs (e.g., TF-IDF) as well, refer to Figure 5 in Appendix.

Word
black

BERT
everywhere,
free, not

muslim accepted, opti-

jews

men

women

onal, allowed
persecuted, ex-
cluded, present
equal, free,
citizens
excluded,
only, bias

HateLM
racist, evil,
stupid
terrorist,
evil, shit
idiots, bad,
stupid
evil, people,
bad
dumb, evil,
stupid

Table 3: Top-3 next word predictions by each LM when
the corresponding word is given as prompt. The hateful
words as the output of the next word prediction demon-
strate the effectiveness of the hate-infusion into LM.

output from our encoder which can then be passed
onto the classiﬁer/regressor. For both our classi-
ﬁer and regressor model, we select a neural net-
work consisting of a 2 Fully-connected layers. The
hidden layer for the same is chosen to have 128
dimensions along with a ReLU activation. The
input dimension is dependent on the output of the
encoder, and in our case is set to 768. The output di-
mension is 1, 2 or 3 corresponding to the regressor,
binary and ternary classiﬁer. We used Adam opti-
mizer with batch size 48, max seq length 128 and
learning rates 3e-6 and 2e-5 for our encoder and
classiﬁer/regressor respectively. Our approaches
do not require additional context during inference
(see Figure 2(d)).

4.3 Ablation studies
We frame the ablation studies to answer a series of
interesting questions. These are:
Q1: How much does self-supervised learning ob-
jective during pre-training help in the downstream
task (if at all)?

Q2: Similarly, how much does contextual regular-
ization during ﬁne-tuning help?
Q3: Do these two approaches work only in con-
junction with each other?

In addition to the above, we perform the follow-
ing ablation experiments to validate the usefulness
of various components of our model:
Q4: How much of the performance can be at-
tributed to the detection of isolated hateful words
without any contextual information in place? To
answer this question, we use the TF-IDF (Sammut
and Webb, 2010) vector embedding of each text
sequence. since TF-IDF computes representations
solely based on the frequency of words without any
contextual information.
Q5: Have the hateful words been contextually in-
corporated into the hate-infused language model?
Here, we prompt our hate-infused language model
and study the next word it predicts.
Q6: How do these approaches perform in a few-
shot setting?

5 Results

The Table 2 compares the performance of our meth-
ods with related models and baselines. We report
the performance metrics according to the test sets
of the corresponding datasets.
[1]
It is evident from Table 2 that both CP and UA
phases (self-supervised phases) lead to improve-
ments over the baselines. This answers Q1. To
answer how useful regularization was we can look
at the CR phase results which are again better than
the baselines thus answering Q2.

Further, we can also see that UA and CR phase
results individually beat the baseline. Furthermore,
combined UA+CR phase outperforms all models.
Hence, the conjunction hypothesis is valid thus

Post (along with ground-truth label)
if your humor is based on racism homophobia sexism and rape
you’re not f**king funny go home (non-hate)
<user> ah man i f**king hate you so much (hateful)
the french government position is that france is made stronger by
the immigrants it is bribing to leave porter (non-hate)
i am presuming he means the standard left wing idiots lots of
hypocritical women amongst them think maria ladenburger obvi-
ously you are against these muhammedans (hateful)
a covington catholic a native american indian and a black hebrew
israelite walk into a bar (non-hate)
if money was grown on trees women would be dating monkeys
oh wait never mind (hateful)

BERT HateLM CRUSH
(cid:51)

(cid:51)

(cid:55)

(cid:55)
(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)
(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)
(cid:51)

(cid:51)

(cid:55)

(cid:55)

Table 4: Qualitative results using text sequences from HateXplain dataset. (cid:51) indicates sentences were classiﬁed
properly by the corresponding models while (cid:55) indicates incorrect classiﬁcation. MLM based training of BERT
and HateLM does not seem to capture the complexities of hate speech properly, thus CRUSH outperforms them.

answering Q3.

[2] Q4 is answered by the considerable improve-
ment over the TF-IDF baseline as can be seen in
Table 2. In addition, Figure 3 shows the advan-
tages gained by using CRUSH over HateLM. It
demonstrates both the user-level and the post-level
contextual information from the network incorpo-
rated into our CRUSH model are individually help-
ful over models that do not have social network
context information.

[3] Table 3 answers Q5 by showing some of
the prompt completion results for BERT vs our
Hate-infused LM. It can be noticed that although
HateLM deﬁnitely has been incorporated with
racial and ethnic hate, it surprisingly does not dis-
criminate between genders.

[4] Figure 4 shows the few-shot training results
with small percentages of data sampled from our
training datasets (2% to 20% data points) uniformly
at random. Intuitively our model is already able
to outperform all other baselines consistently even
without context because of the self-supervised train-
ing procedures we propose during the CP and UA
phases which captures the hate & users’ style+bias.
This is evident from the results of CP & UA phases
in Table 3. So later adding context in few-shot
setting simply further enhances our model thus an-
swering Q6 (see Appendix for full results).

Discussion of some qualitative examples: Table
4 presents a few text sequences from the Hat-
eXplain dataset and their corresponding results
(classiﬁcation/missclassiﬁcation) for the models
– BERT, HateLM, and CRUSH. It can be noticed

that the ﬁrst two sentences are properly classiﬁed
by HateLM and CRUSH but not by vanilla BERT.
That leads us to believe that BERT predictions are
heavily dependent on the token occurrences rather
than the context in the sentence where the tokens
have occurred. As for the next two sentences, it is
evident that the sentence representations learnt by
CRUSH (due to UA + CR phases of training, see
Section 3.2, 3.3) are superior to those learnt by the
HateLM, hence it classiﬁes these text sequences
better than simple MLM based training. Finally,
the last two sentences indicate that CRUSH (along
with the other approaches) still lacks the ability to
identify humor, sarcasm, and implicit hate speech,
which are known to be difﬁcult problems.

6 Conclusion

In this paper, we provide approaches to infuse so-
cial network context using the self-supervised user-
attribution pre-training objective combined with
the contextual regularization objective on top of
traditional MLM for hate speech tasks. We empiri-
cally demonstrate the advantage of our methods by
improving over existing competitive baselines in
hate speech detection and scoring tasks across three
different datasets. We also show that our method
performs superior in the low data regime as well
when compared to existing approaches. We also do
ablations to understand the beneﬁts of each objec-
tive separately. Future work include exploiting the
relations among users, using different base mod-
els capable of incorporating longer contexts, and
trying to address hard problems like sarcasm and
implicit hate speech detection in social networks.

7 Ethical considerations

All the datasets that we use are publicly available.
We report only aggregated results in the paper. For
context mining, we have used data either available
in the public domain or available through ofﬁcial
APIs of the corresponding social media. Neither
have we, nor do we intend to share any personally
identiﬁable information with this paper. We also
make our codebase publicly available here - https:
//github.com/parag1604/CRUSH.

Our model CRUSH helps advance the state-
of-the-art in hate speech detection by incorporat-
ing continual pre-training, capturing user writing
biases and leveraging both user & post context
(which is publicly available as well). This in turn
should be able limit the amount of hateful threads
in social networks/media by better detection of
hate speech, thus promoting a more friendly and
welcoming environment for people from all race,
religion, ethnicity, gender etc.

Unfortunately, these models are not completely
free from all potential negative impact. One such
example being that our models have hate knowl-
edge infused within them during the CP phase
(refer Section 3.1) of our training pipeline. As
shown in Table 3, these language models could
be used potentially used for generating hateful
words/sentences given an initial prompt.

However, the hateful words generated by the
HateLM can be also identiﬁed by the platform if
the platform uses a hate speech detection algorithm
like ours or uses a set of hate lexicons to directly
ﬁlter out such keywords generated (Gitari et al.,
2015). Moreover, our ﬁnal model - CRUSH - is
not exactly suitable for toxic language generation
as it is further pre-trained on the user style dis-
crimination task and ﬁne-tuned on the hate speech
classiﬁcation task making its classiﬁcation capabil-
ities (potential use by the social media platforms to
detect hate speech) stronger than the hate speech
generation capabilities (the potential of the model
being abused with malicious intent). So, the plat-
forms using our better-informed model will easily
detect any hate speech generated by the model.

Moreover,

large-scale technologies for bet-
ter generation of hate speech using GPT-2 and
other generative models (Wullach et al., 2021;
Hartvigsen et al., 2022) (with the intention to train
hate speech classiﬁers better) already exist and as a
model trained only for classiﬁcation, our model is
likely to be far weaker than these models for gen-

eration of harmful content. While these language
models can be used both to beneﬁt or disrupt our
lifestyle just like any other technology (Douglas,
2014), we urge the researchers to exercise ultimate
caution while using them, including ours. For the
same reason, we do not make the trained model
parameters publicly available (except for the code
to promote reproducibility).

Also, one of the factors increasing bias in the
classiﬁcation model is the data it is trained upon.
Hence, any potential bias in the datasets that are
manually annotated by human beings (who are not
free from bias) can result the model being biased
for/against some speciﬁc target groups.

To incorporate better inductive bias into the
model, we have trained the model to initially map
text sequences posted by the same user to vectors
that are close in the embedding space. This helps
the model to identify the dialects of the various
users and help the model perform better. How-
ever, this inductive bias may cluster the linguistic
characteristics of some groups which in turn might
potentially increase the chances of that particular
language style being classiﬁed as hate speech if
the annotated data contains only hateful instances
from that particular dialect. This may make the
model biased if those particular dialects are used
predominantly by some protected category (Blacks,
Jews, Women, Mexican etc.).

The most

simple and effective solution
here would be to annotate data for each di-
alect/vernacular group, potentially stratifying the
dataset into clusters using the pre-trained language
model. If there are balanced examples from each of
these dialect/vernacular or at least each protected
category (Blacks, Jews, Women, Mexican etc.) for
both the hate and non-hate categories, such biases
can be well mitigated.

Further, our model like any other hate speech
model is not suitable for in-the-wild deployment
without explicit human scrutiny. Language use is
often very speciﬁc to each platform. So, the dis-
tribution of words and data-points may not match
the training data distribution of the model. Thus,
it is extremely important to ﬁrst test the model
on the platform, check for potential biases against
the protected categories and individual linguistic
groups/dialects and deploy it as a preliminary ﬁlter
assisting the human experts detecting hate speech.

8 Acknowledgements

We thank Facebook (Meta Platforms, Inc.)
for
sponsoring the stay of PD at IIT Kharagpur through
the Ethics in AI Research grant and Tata Consul-
tancy Services (TCS) for funding SC with the TCS
PhD fellowship.

References

Pinkesh Badjatiya, Shashank Gupta, Manish Gupta,
and Vasudeva Varma. 2017. Deep learning for hate
In Proceedings of the
speech detection in tweets.
26th International Conference on World Wide Web
Companion, WWW ’17 Companion, page 759–760,
Republic and Canton of Geneva, CHE. International
World Wide Web Conferences Steering Committee.

Jason Baumgartne. 2018.

gab.ai corpora on
pushshift.io.
https://files.pushshift.
io/gab/. Added: 2018-11-30, Announcement:
https://twitter.com/jasonbaumgartne/
status/1068610394992926720.

Tommaso Caselli, Valerio Basile, Jelena Mitrovi´c, and
Michael Granitzer. 2021. HateBERT: Retraining
BERT for abusive language detection in English. In
Proceedings of the 5th Workshop on Online Abuse
and Harms (WOAH 2021), pages 17–25, Online. As-
sociation for Computational Linguistics.

Marco Del Tredici, Diego Marcheggiani, Sabine
Schulte im Walde, and Raquel Fernández. 2019.
You shall know a user by the company it keeps: Dy-
namic representations for social media users in NLP.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 4707–
4717, Hong Kong, China. Association for Computa-
tional Linguistics.

Fabio Del Vigna, Andrea Cimino, Felice Dell’Orletta,
Marinella Petrocchi, and Maurizio Tesconi. 2017.
Hate me, hate me not: Hate speech detection on face-
book. In Proceedings of the First Italian Conference
on Cybersecurity, ITASEC 2017.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Thomas Douglas. 2014. The dual-use problem, scien-
tiﬁc isolationism and the division of moral labour.
Monash bioethics review, 32:86–105.

Njagi Dennis Gitari, Zhang Zuping, Zuping Zhang,
Hanyurwimfura Damien, and Jun Long. 2015. A
lexicon-based approach for hate speech detection.
In International Journal of Multimedia and Ubiqui-
tous Engineering, volume 10, pages 215–230.

Suchin Gururangan, Ana Marasovi´c,

Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Don’t stop pretraining:
Adapt language models to domains and tasks.
In
the
the 58th Annual Meeting of
Proceedings of
Association for Computational Linguistics, pages
8342–8360, Online. Association for Computational
Linguistics.

Rishav Hada, Sohi Sudhir, Pushkar Mishra, Helen
Yannakoudakis, Saif M. Mohammad, and Ekaterina
Shutova. 2021. Ruddit: Norms of offensiveness for
In Proceedings of the
English Reddit comments.
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 2700–2717, Online. As-
sociation for Computational Linguistics.

Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,
Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.
Toxigen: A large-scale machine-generated dataset
for adversarial and implicit hate speech detection.
arXiv preprint arXiv:2203.09509.

James M. Hughes, Nicholas J. Foti, David C. Krakauer,
and Daniel N. Rockmore. 2012. Quantitative pat-
terns of stylistic inﬂuence in the evolution of litera-
ture. Proceedings of the National Academy of Sci-
ences, 109(20):7682–7686.

Andrew Jakubowicz. 2017.

Alt_right white lite:
Trolling, hate speech and cyber racism on social me-
dia. Cosmopolitan Civil Societies: An Interdisci-
plinary Journal, 9:41.

Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron
Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. 2020. Su-
pervised contrastive learning. In Advances in Neural
Information Processing Systems, volume 33, pages
18661–18673. Curran Associates, Inc.

Rohan Kshirsagar, Tyrus Cukuvac, Kathy McKeown,
and Susan McGregor. 2018. Predictive embeddings
for hate speech detection on twitter. In Abusive Lan-
guage Online Workshop, EMNLP 2018, pages 26–
32.

Shervin Malmasi and Marcos Zampieri. 2017. Detect-
In Proceedings
ing hate speech in social media.
of the International Conference Recent Advances in
Natural Language Processing, RANLP 2017, pages
467–472, Varna, Bulgaria. INCOMA Ltd.

Binny Mathew, Anurag Illendula, Punyajoy Saha,
and Animesh
Soumya Sarkar, Pawan Goyal,
Mukherjee. 2020. Hate begets hate: A temporal
study of hate speech. Proc. ACM Hum.-Comput. In-
teract., 4(CSCW2).

NLP and Computational Social Science, pages 138–
142, Austin, Texas. Association for Computational
Linguistics.

Tomer Wullach, Amir Adler, and Einat Minkov. 2021.
Fight ﬁre with ﬁre: Fine-tuning hate detectors using
large samples of generated hate speech. In Findings
of the Association for Computational Linguistics:
EMNLP 2021, pages 4699–4705, Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.

Ziqi Zhang, D. Robinson, and Jonathan Tepper. 2018.
Detecting hate speech on twitter using a convolution-
gru based deep neural network. In Extended Seman-
tic Web Conference, ESWC 2018.

Binny Mathew, Punyajoy Saha, Seid Muhie Yi-
mam, Chris Biemann, Pawan Goyal, and Animesh
Mukherjee. 2021. Hatexplain: A benchmark dataset
for explainable hate speech detection. Proceedings
of the AAAI Conference on Artiﬁcial Intelligence,
35(17):14867–14875.

Stefano Menini, Alessio Palmero Aprosio, and Sara
Tonelli. 2021. Abuse is contextual, what about nlp?
the role of context in abusive language annotation
and detection. ArXiv, abs/2103.14916.

John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon,
Nithum Thain, and Ion Androutsopoulos. 2020.
Toxicity detection: Does context really matter? In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4296–
4305, Online. Association for Computational Lin-
guistics.

Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Beld-
ing, and William Yang Wang. 2019. A bench-
mark dataset for learning to intervene in online hate
speech. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
4755–4764, Hong Kong, China. Association for
Computational Linguistics.

Axel Rodriguez, Carlos Argueta, and Yi-Ling Chen.
2019. Automatic detection of hate speech on face-
book using sentiment and emotion analysis. In Inter-
national Conference on Artiﬁcial Intelligence in In-
formation and Communication, ICAIIC 2019, pages
169–174.

Claude Sammut and Geoffrey I. Webb, editors. 2010.
TF–IDF, pages 986–987. Springer US, Boston, MA.

Serra Sinem Tekiro˘glu, Yi-Ling Chung, and Marco
Guerini. 2020.
Generating counter narratives
against online hate speech: Data and strategies. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1177–
1190, Online. Association for Computational Lin-
guistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, volume 30. Curran Associates, Inc.

Bertie Vidgen, Dong Nguyen, Helen Margetts, Patricia
Rossini, and Rebekah Tromble. 2021. Introducing
CAD: the contextual abuse dataset. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 2289–2303,
Online. Association for Computational Linguistics.

Zeerak Waseem. 2016. Are you a racist or am I seeing
things? annotator inﬂuence on hate speech detection
on Twitter. In Proceedings of the First Workshop on

A Appendix

Figure 5: Full comparison of model errors in low data regime. This includes a comparison of our CRUSH model
with baseline models built on non-contextualized and contextualized embeddings.

