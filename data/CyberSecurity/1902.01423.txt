A Moving Target Defense for Securing
Cyber-Physical Systems

Paul Grifﬁoen, Student Member, IEEE, Sean Weerakkody, Member, IEEE,
and Bruno Sinopoli, Senior Member, IEEE

1

0
2
0
2

l
u
J

7

]

Y
S
.
s
c
[

3
v
3
2
4
1
0
.
2
0
9
1
:
v
i
X
r
a

Abstract—This article considers the design and analysis of
multiple moving target defenses for recognizing and isolating
attacks on cyber-physical systems. We consider attackers who
perform integrity attacks on some set of sensors and actuators
in a control system. In such cases,
it has been shown that
a model aware adversary can carefully design attack vectors
to bypass bad data detection and identiﬁcation ﬁlters while
causing damage to the control system. To counter such an
attacker, we propose the moving target defense which introduces
stochastic, time-varying parameters in the control system. The
underlying random dynamics of the system limit an attacker’s
knowledge of
the model and inhibits his or her ability to
construct stealthy attack sequences. Moreover, the time-varying
nature of the dynamics thwarts adaptive adversaries. We explore
three main designs. First, we consider a hybrid system where
parameters within the existing plant are switched among multiple
modes. We demonstrate how such an approach can enable both
the detection and identiﬁcation of malicious nodes. Next, we
investigate the addition of an extended system with dynamics
that are coupled to the original plant but do not affect system
performance. Here, an attack on the original system will affect
the authenticating subsystem and in turn be revealed by a set
of sensors measuring the extended plant. Lastly, we propose the
use of sensor nonlinearities to enhance the effectiveness of the
moving target defense. The nonlinear dynamics act to conceal
normal operational behavior from an attacker who has tampered
with the system state, further hindering an attacker’s ability to
glean information about the time-varying dynamics. In all cases
mechanisms for analysis and design are proposed. Finally, we
analyze attack detectability for each moving target defense by
investigating expected lower bounds on the detection statistic.
Our contributions are also tested via simulation.

is an essential goal

I. INTRODUCTION
Securing cyber-physical systems (CPSs), the amalgamation
of sensing, processing, control, and communication in physical
spaces,
in today’s society. CPSs are
ubiquitous in modern critical infrastructure such as transporta-
tion systems, energy delivery, health care, and sewage/water
management. Consequently, these systems are attractive targets
for adversaries and are essential to protect. Unfortunately,
CPSs are vulnerable to adversarial attacks [1] due to the
large number of attack surfaces found in these large scale,
heterogeneous, and highly connected systems. Additionally,
existing defenses from cyber security alone are insufﬁcient for
protecting CPSs. Traditional techniques such as authenticated
encryption, message authentication codes, and signatures that
typically enable the detection of integrity attacks can be

P. Grifﬁoen and S. Weerakkody are with the Department of Elec-
trical and Computer Engineering, Carnegie Mellon University, Pitts-
burgh, PA, USA 15213. B. Sinopoli
is with the Department of Elec-
trical and Systems Engineering, Washington University in St. Louis, St.
Louis, MO, USA 63130. Email: pgriffi1@andrew.cmu.edu,
sweerakk@alumni.cmu.edu, bsinopoli@wustl.edu

This work is partially supported by Department of Energy grant de-

oe0000779 and by National Science Foundation grant 164652.

computationally complex and are ineffective against a class of
attacks known as physical attacks. Moreover, updating legacy
systems can prove to be impractical.

The vulnerabilities in CPSs have culminated in several
effective attacks from highly resourceful and knowledgeable
attackers. In the year 2000, a malicious insider was able to
utilize detailed system knowledge to attack a waste manage-
ment system in Queensland, Australia [2], resulting in the
leakage of millions of liters of sewage. With Stuxnet [3],
a nation state adversary was able to compromise a uranium
enrichment facility in Iran, leading to the destruction of a
thousand centrifuges. More recently in 2015, hackers were
able to remotely compromise a supervisory control and data
acquisition (SCADA) system in Ukraine [4], allowing them to
cause widespread blackouts.

Motivated by the threat of such sophisticated attackers, we
aim to design resilient CPSs. As a ﬁrst step we focus on the
problem of detecting and in some cases isolating attacks from
malicious attackers. The problem of recognizing attacks is
not trivial, especially when considering highly knowledgeable
and resourceful attackers. For instance adversaries can utilize
model knowledge to engage in deceptive and powerful stealthy
attacks, including false data injection attacks [5], [6], covert
attacks [7], zero dynamics attacks [8], [9], and replay attacks
[10]. Here the adversary is able to leverage access to system
channels and/or model knowledge to construct attacks which
bypass traditional bad data detectors such that the outputs
received by a SCADA operator are statistically consistent with
expected output behavior.

To counter such an attacker, a defender must engage in
active detection [11] by designing a system that adds additional
redundancy or introduces a physical secret. For instance, phys-
ical watermarking was introduced in [10] to counter replay
adversaries. Here, the defender changes his or her control
input to introduce random authenticating perturbations to the
system. Several extensions have been pursued, for instance
[12]–[15]. Alternatively, the defender can pursue one time
changes to the system, including changes to the parameters
[16] or structural changes, for instance involving sensing and
communication [17]. In addition, encryption or lower cost
mechanisms such as coding [18] can be effective tools for au-
thentication. Nonetheless, the above schemes can be rendered
ineffective by strong attackers. Watermarking can fail against
additive attacks pursued by model aware attackers. Increasing
robustness through one time changes can fail against highly
resourceful attackers. Finally sensor coding can be ineffective
against attackers with physical access to sensors and a certain
class of zero dynamics attacks.

To address these challenges, we consider the moving target
defense, which was ﬁrst introduced in [19] with extensions in
[20]–[22]. Here, the defender introduces time-varying param-

 
 
 
 
 
 
eters into the control system, resulting in periodic changes
to the system matrices. The unknown parameters limit the
attacker’s understanding of the system model. Moreover, the
time-varying dynamics ideally act as a moving target, changing
fast enough to hinder a potential adaptive adversary from
performing system identiﬁcation.

The article considers three main moving target designs. In
the ﬁrst design, we evaluate a hybrid moving target where
the system is switched among a number of discrete modes.
We provide a set of design recommendations for the hybrid
moving target which enable a defender to both detect and
identify sensor attacks in control systems. Secondly, we design
an extended moving target where we introduce an auxiliary
system with time-varying dynamics coupled to the original
plant. An attacker who perturbs the original system will also
affect the additional dynamics due to this coupling. More-
over, the time-varying behavior of the system prevents the
defender from concealing his or her attack through fake sensor
measurements. We provide efﬁciently solvable optimization
problems to design the parameters that generate the time-
varying matrices in this extended moving target.

Thirdly, we note that even in the presence of time-varying
dynamics, the attacker still has some opportunity to learn
useful information about the model which can be applied
to an attack. To limit this information, we introduce random
nonlinearities in the sensor measurements which are ampliﬁed
when the system state is perturbed and consequently conceal
information about the system from the adversary when the
plant is under attack. We provide a limit analysis to demon-
strate the effectiveness of this approach as well as optimization
problems to design the coefﬁcient matrix associated with the
nonlinearity. Lastly, we provide mechanisms to analyze attack
detectability by investigating expected lower bounds on the
detection statistic for each moving target defense.

An overarching goal of this article is to present the moving
target as a general technique, which can be realized to counter
a variety of attackers or molded to ﬁt a variety of architectures.
In order to illustrate this, previous results on the hybrid moving
target defense [21] and the extended moving target defense
[19], [22] are repeated in this article. The main contributions
relative to our previous work in [19], [21], and [22] are as
follows:

1) An extension of the work in [22] to account for a time-
varying covariance that generates the distribution of the
auxiliary actuators. (section IV-A)

2) The introduction, analysis, and design of random non-
linearities in the sensor measurements that conceal in-
formation about the time-varying dynamics from the
adversary when the plant is under attack. (section V,
VII-A)

3) The presentation and validation of a simpler and more
accurate method for computing expected lower bounds
on the detection statistic than that given in [19]. (section
VI, VII-B)

4) The organized presentation of three different moving
target techniques differentiated by their differing archi-
tectures and the attack models they wish to thwart.
The rest of the article is summarized below. In section II, we
introduce the system and attack models along with the moving
target defense. In section III, we consider the design of a
hybrid moving target defense, placing a special focus on attack
identiﬁcation. Next, in section IV, we investigate the design of

2

an extended moving target defense for attack detection. Later,
in section V, we pursue the design of a nonlinear moving target
defense to limit an attacker’s ability to identify the system
model. In section VI, we propose statistical bounds to analyze
the performance of the moving target defense. Lastly, section
VII includes simulation results and section VIII concludes the
article.

II. MODELING THE MOVING TARGET

A. System Model

To begin, we introduce the model for the system under
consideration. We model our CPS as a linear time-invariant
system as follows

yk = Cxk + vk.

xk+1 = Axk + Buk + wk,

(1)
Here xk ∈ Rn represents the system state at time k, uk ∈ Rp is
a vector of control inputs, and yk ∈ Rm represents a collection
of m scalar sensor outputs. In addition, to capture uncertainty
we consider independent and identically distributed (IID)
Gaussian process noise wk ∼ N (0, Q) and IID Gaussian
sensor noise vk ∼ N (0, R). We assume that (A, B) and
(A, Q 1

2 ) are stabilizable, (A, C) is detectable, and R (cid:31) 0.
In this article, we consider an adversary who can perform
integrity attacks. For the hybrid moving target, we assume
that the attacker is able to corrupt all of the outputs. For
the extended moving target and nonlinear moving target, we
assume that the attacker can corrupt all of the inputs and
outputs. This for instance can be done over a network through
a man in the middle attack where an attacker intercepts true
packets and replaces them with false packets. Alternatively,
physical attacks can disrupt the integrity of a system. For
instance, the attacker can change the settings of programmable
logic controllers (PLCs) or the environment surrounding sys-
tem sensors. Mathematically, we model an integrity attack as
follows
xk+1 = Axk + Buk + Baua

yk = Cxk + Dada

k + vk. (2)

k + wk,

Without loss of generality, an attack is assumed to begin at
time k = 0. Here, Baua
k represents attacks on the control
inputs and Dada
k represents attacks on sensor outputs. If all
actuators can be corrupted, Ba = B and if all sensors can be
modiﬁed, Da = I. Motivated by the resources of malicious
insiders and nation state adversaries, we will in the case of the
extended moving target and nonlinear moving target consider
this worst case scenario. Additionally, we will also consider
the possibility that an attacker has detailed system knowledge.
A fundamental understanding of the plant when combined
with signiﬁcant disclosure and disruption resources can lead
to powerful attacks [9]. For instance, an attacker can attempt
to subtract his or her inﬂuence. Here an adversary chooses
an arbitrary sequence of control inputs {ua
k} in order to drive
the system along the controllable subspace (A, Ba). To avoid
detection, the attacker leverages model knowledge to construct
stealthy outputs. Speciﬁcally,

Dada

k = −Cxa
k,

k+1 = Axa
xa

k + Baua
k,

xa
0 = 0.

(3)

It can be shown that the probability distribution of the outputs
under such an attack is identical to the distribution under
normal operation. Consequently no standard bad data detector
can recognize this adversarial behavior, and as a result this
behavior is perfectly stealthy. We remark that a signiﬁcant

resource for an attacker here is model knowledge, which
allows the adversary to carefully construct fake sensor outputs.
In the ensuing subsections we propose three main designs
which allow us to limit an attacker’s knowledge of the system
model. We call
this collection of tools the moving target
defense.

B. Hybrid Moving Target Defense

In the hybrid moving target we change parameters of the
system, particularly the system matrices, in a time-varying
the adversary’s knowledge of the system
fashion to limit
model. The time-varying sequence of system matrices is
known to the defender but kept hidden from the adversary,
which limits the effectiveness of an adaptive attacker. The
dynamics of the hybrid moving target are given below

xk+1 = Akxk + Bkuk + wk,

yk = Ckxk + vk.

(4)

We assume that our plant
is a switching hybrid system.
Here, (Ak, Bk, Ck) belong to a ﬁnite set of modes Γ =
{(A(1), B(1), C(1)), · · · , (A(l), B(l), C(l))}. While Γ may
be known to an attacker,
the exact realization of system
matrices will be unknown. This forces an attacker to leverage
imperfect system information when constructing an attack,
which in turn can reveal his or her malicious behavior. We
assume the adversary is able to modify all of the sensor mea-
surements. The information available to the defender and the
attacker at time step k, denoted by I D
k , respectively,
is given by

k and I A

I D
k (cid:44) {A0:k, B0:k, C0:k, u0:k, ya
k (cid:44) {Γ, u0:k, yA
I A

0:k, Dada

0:k, f (wk, vk)},

0:k, f (wk, vk)},

where ya
ceived by the system operator, yA
surements that the attacker intercepts, and Dada
the bias the attacker adds to the sensor measurements.

k represents the attacked sensor measurements re-
k denotes the sensor mea-
0:k represents

Remark 1. The sequence of time-varying matrices can be
determined for instance by a cryptographically secure pseudo
random number generator (PRNG). Here, the sequence of
system matrices will be entirely determined by the seed of the
random number generator. As we aim to prevent an attacker
from learning about the sequence of system matrices, the
random seed must be hidden from the attacker. Alternatively,
the defender must know the random seed to perform tasks of
detection and estimation. As such the random seed serves as
a root of trust and is analogous to a cryptographic key.

In section III, we will investigate the applications of the
hybrid moving target defense for the purposes of identifying
sensor attacks in control systems, speciﬁcally considering how
to design Γ and the sequence of time-varying matrices. We
note that introducing parameter changes to the system can
result in tradeoffs between security and control performance.
This issue is addressed in the next section when we consider
the extended moving target defense.

C. Extended Moving Target Defense

In the extended moving target, an authenticating subsystem
is added on top of the nominal control system. Speciﬁcally, we
introduce additional states ˜xk ∈ R˜n measured by additional
sensors ˜yk ∈ R ˜m which have dynamics that are coupled to

3

(cid:21)

(cid:21)

(cid:21)

=

the dynamics of the original state xk. The dynamics of the
extended moving target are given below
(cid:20) ˜Bk
B
(cid:124) (cid:123)(cid:122) (cid:125)
Bk
(cid:21)
(cid:20)˜vk
,
vk
(cid:124) (cid:123)(cid:122) (cid:125)
¯vk

(cid:20)˜xk+1
xk+1
(cid:124) (cid:123)(cid:122) (cid:125)
¯xk+1
(cid:20)˜yk
yk
(cid:124) (cid:123)(cid:122) (cid:125)
¯yk

(cid:20) ˜A ¯Ak
0 A
(cid:124)
(cid:123)(cid:122)
Ak
(cid:20) ˜C ¯Ck
C
0

(cid:20)˜xk
xk
(cid:124) (cid:123)(cid:122) (cid:125)
¯xk
(cid:20)˜xk
xk
(cid:124) (cid:123)(cid:122) (cid:125)
¯xk

(cid:20) ˜wk
wk
(cid:124) (cid:123)(cid:122) (cid:125)
¯wk

uk +

(cid:21)
,

(6)

(5)

(cid:123)(cid:122)
Ck

=

+

+

(cid:21)

(cid:21)

(cid:21)

(cid:21)

(cid:124)

(cid:125)

(cid:125)

with process noise ¯wk ∼ N (0, Q) and sensor noise ¯vk ∼
N (0, R) such that Q (cid:44) BlkDiag( ˜Q, Q) (cid:23) 0 and R (cid:44)
(cid:2) ˜R ˜R12 ;
12 R(cid:3) (cid:31) 0. We assume that the time-varying
˜RT
matrices ¯Ak, ˜Bk, and ¯Ck are selected from an IID distribution
(to be designed later in section IV). Without loss of generality,
the control inputs are multiplexed to the actuators of both the
nominal and extended systems. The extended moving target is
designed so that if an adversary attempts to bias the original
state xk, he or she also modiﬁes the auxiliary state ˜xk. This
in turn will cause changes to the measurements ˜yk. Ideally
an attacker who can modify ˜yk will be unable to do so in a
convincing manner due to his or her lack of knowledge about
the time-varying dynamics. The time-varying behavior will
also impede the task of system identiﬁcation. We assume the
adversary is able to modify all of the control inputs and sensor
measurements. The information available to the defender and
the attacker at time step k is given by

k (cid:44) {A, B, C, ˜A, ¯A0:k, ˜B0:k, ˜C, ¯C0:k, u0:k, ¯ya
I D
k (cid:44) {A, B, C, ˜A, ˜C, f ( ¯A, ˜B, ¯C), u0:k, ua
0:k, ¯yA
I A

0:k, f ( ¯wk, ¯vk)},
0:k, ¯da

0:k, f ( ¯wk, ¯vk)},

k represents the attacked sensor measurements re-
k denotes the bias the attacker
k represents the sensor measure-
k denotes the bias the

where ¯ya
ceived by the system operator, ua
adds to the control inputs, ¯yA
ments that the attacker intercepts, and ¯da
attacker adds to the sensor measurements.
Remark 2. While this system involves matrices ¯Ak, ˜Bk, and
¯Ck selected from an IID distribution, the extended moving
target defense can still be effective in other scenarios. For
instance, the system parameters can evolve at multiple time
scales. In this case, the longer the target remains in place, the
easier it is for the adversary to identify the system.

A signiﬁcant advantage of the extended moving target
defense relative to the hybrid moving target defense is po-
tential system performance. In particular, if we do not care
about controlling the additional states ˜xk, the controller of
the original system can remain unchanged and no online
performance is sacriﬁced. Because the dynamics of the original
plant remain in place, there is no tradeoff between security and
control. We will consider the design of the parameters that
generate the system matrices in the extended moving target
for attack detection in section IV.

Remark 3. As before, the sequence of time-varying matrices
can be determined by a PRNG. The extended system itself
can be introduced by leveraging existing dynamics in the
system. For instance one can consider the dynamics of waste
products such as heat in a chemical reaction or the friction
in a mechanical generator. The dynamics can be made time-
varying by changing conditions at the plant. Alternatively, one
can introduce external hardware such as RLC circuits with

variable resistors and capacitors to generate the time-varying
auxiliary system.

D. Nonlinear Moving Target Defense

The utility of the moving target lies in the challenges it
poses for an adversary aiming to perform system identiﬁcation.
However, we acknowledge that the sensor measurements as
constructed do reveal some information about
the system
dynamics. In order to further limit the information available
to an attacker, we can intentionally introduce nonlinearities.
Many systems are inherently nonlinear, allowing us to leverage
the dynamics of the system to introduce these nonlinearities.
Notably, consider the sensor measurements for the nonlinear
moving target below
(cid:20)˜yk
yk
(cid:124) (cid:123)(cid:122) (cid:125)
¯yk

(cid:20)Gkh(xk)
0

(cid:20)˜xk
xk
(cid:124) (cid:123)(cid:122) (cid:125)
¯xk

(cid:20) ˜C ¯Ck
C
0

(cid:20)˜vk
vk
(cid:124) (cid:123)(cid:122) (cid:125)
¯vk

(7)

(cid:123)(cid:122)
Ck

+

+

=

(cid:21)

(cid:21)

(cid:21)

(cid:21)

(cid:21)

(cid:124)

(cid:125)

.

It is assumed that the extended state dynamics are unchanged
(5). However, a nonlinearity Gkh(xk) is introduced into the
auxiliary sensor measurements where Gk is a random matrix
chosen from an IID distribution. Here the nonlinearity takes a
form such that Gk determines the direction of the nonlinearity
and h(xk) determines the magnitude of the nonlinearity. The
nonlinearity is designed so that it is approximately 0 when the
system state lies within a normal region of operation. When
the state has been perturbed away from its normal region of
operation, the nonlinearity becomes large and unpredictable.
We assume the adversary is able to modify all of the control
inputs and sensor measurements. The information available to
the defender and the attacker at time step k is given by

k (cid:44) {A, B, C, ˜A, ¯A0:k, ˜B0:k, ˜C, ¯C0:k, G0:k, nonlinear function h,
I D

u0:k, ¯ya

0:k, f ( ¯wk, ¯vk)},

k (cid:44) {A, B, C, ˜A, ˜C, f ( ¯A, ˜B, ¯C), f (G), nonlinear function h,
I A
0:k, f ( ¯wk, ¯vk)}.

u0:k, ua

0:k, ¯yA

0:k, ¯da

An attacker who aims to remain stealthy must be able to
produce counterfeit measurements which do not contain this
large nonlinearity. Nonetheless, this is impractical because the
attacker does not know the time-varying matrix Gk which de-
termines the nonlinearity. Moreover, the large highly nonlinear
attack measurements will signiﬁcantly impede an attacker’s
ability to learn the time-varying matrices ( ¯Ak, ˜Bk, ¯Ck) from
the measurements ˜yk. The design of matrix Gk and an analysis
of the nonlinear moving target is presented in section V.

E. Estimation and Detection

A Kalman ﬁlter can be used to compute the minimum mean
squared error state estimate ˆ¯xk|k given the set of previous
measurements up to ¯yk. The Kalman ﬁlter is a linear estimator
given by

ˆ¯xk+1|k= Ak ˆ¯xk|k + Bkuk,

ˆ¯xk|k= (I − KkCk)ˆ¯xk|k−1 + Kk ¯yk,
Kk= Pk|k−1CT

k (CkPk|k−1CT

k + R)−1,
k + Q,

(8)

(9)

(10)

Pk+1|k= Ak(I − KkCk)Pk|k−1AT

(11)
where ˆ¯xk+1|k is the a priori state estimate, ˆ¯xk|k is the a
posteriori state estimate, Pk+1|k is the a priori error covariance
matrix, and Kk is the Kalman gain. To detect attacks on

4

the CPS, a residue-based detector that leverages the a priori
state estimate ˆ¯xk|k−1 is utilized. The residue ¯zk represents the
difference between the observed and expected value of the
measurements and is given by

¯zk = ¯yk − Ck ˆ¯xk|k−1.

(12)

By incorporating this residue, a χ2 detector given by

gk(¯zk−T +1:k) =

k
(cid:88)

i=k−T +1

i (CiPi|i−1CT
¯zT

i + R)−1 ¯zi

H1≷
H0

ηk,

(13)

with detection statistic gk follows a χ2 distribution under
normal operation. The χ2 detector, which has T (m + ˜m)
degrees of freedom, attempts to exploit this fact by testing
to see if the residues follow the correct distribution. Here ηk
represents the threshold of the bad data detector, H0 is the null
hypothesis which represents normal system operation, H1 is
the alternative hypothesis which denotes that the system is un-
der attack, and T represents the detector window that considers
past measurements. Measurements that are in close agreement
with expected values generate small detection statistics and
thus raise no alarm. Large deviations between measured and
expected behavior will lead to a large detection statistic, thus
causing an alarm.

Remark 4. Estimation and detection for the hybrid moving
target is described by replacing ˆ¯xk+1|k, ˆ¯xk|k, ¯yk, ¯zk, Ak, Bk,
Ck, Kk, Pk+1|k, Q, and R in eqs. (8) to (13) with ˆxk+1|k,
ˆxk|k, yk, zk, Ak, Bk, Ck, Kk, Pk+1|k, Q, and R, respectively,
where ˆxk+1|k and ˆxk|k are the a priori and a posteriori
state estimates for the nominal system, zk is the residue for
the nominal system, Kk is the Kalman gain for the nominal
system, and Pk+1|k is the a priori error covariance matrix for
the nominal system.

While the estimation and detection techniques described
above can be applied to the hybrid moving target and the
extended moving target, a slight modiﬁcation must occur when
performing estimation and detection for the nonlinear moving
target. Because the sensor measurements are nonlinear, an
extended Kalman ﬁlter is used and is given by

ˆ¯xk+1|k= Ak ˆ¯xk|k + Bkuk,

ˆ¯xk|k= (I − KkCk)ˆ¯xk|k−1 + Kk ¯yk − KkGkh(ˆxk|k−1),
Kk= Pk|k−1ΦT

k (ΦkPk|k−1ΦT
Pk+1|k= Ak(I − KkΦk)Pk|k−1AT

k + R)−1,
k + Q,

(14)

(15)

(16)

(17)

Gk(cid:44)

(cid:21)

(cid:20)Gk
0

, Φk (cid:44) Ck +

(cid:34)

0 Gk
0

∂h(xk)
∂xk
0

(cid:12)
(cid:12)ˆxk|k−1

(cid:35)

.

The residue is then ¯zk = ¯yk − Ck ˆ¯xk|k−1 − (cid:2)Gkh(ˆxk|k−1); 0(cid:3),
and the detector is the same as the χ2 detector in (13) except
that Ci is replaced by Φi.

III. HYBRID MOVING TARGET DEFENSE
We ﬁrst consider the hybrid moving target defense, where
we perform active detection by changing the parameters of
the plant itself in a discrete fashion. This technique will aid
not only in the detection of malicious adversaries but will
also prevent unidentiﬁable attacks by limiting the adversary’s
knowledge of the system. To begin, we let y(x0, Dada
k, k)
be the output yk due to the initial state x0 and the sequence
of attacks {Dada
k represent the

k}, and we let ys

0, · · · , Dada

sth entry of yk where we have dropped the superscript a
(denoting the measurements received by the system operator)
for notational simplicity.

time t if

Deﬁnition 1. A nonzero attack on sensor s is unambiguously
0 ∈ Rn satisfying
identiﬁable at
ys
k = ys(x∗
0, 0, k) for 0 ≤ k ≤ t. An attack on sensor s is
unambiguously identiﬁable if it is unambiguously identiﬁable
for all t.

there is no x∗

The notion of unambiguous identiﬁability characterizes
when the defender can be certain that sensor s is faulty or
under attack. This scenario occurs only if there exists no
initial state which produces the output sequence at ys. We
seek to design a system that forces the attacker to generate
unambiguously identiﬁable attacks on all
targeted sensors,
allowing the defender to identify these misbehaving sensors.
We consider the hybrid moving target dynamics as given
in (4) from the adversary’s perspective, where the adver-
sary performs an attack on an ordered set of sensors L =
k ∈ R|L| such that yk =
{s1, · · · , s|L|} using additive inputs da
Ckxk +Dada
k +vk. Without loss of generality, we assume that
an attack starts at time k = 0. Here, Da ∈ Rm×|L| is deﬁned
uv(L) = Iu=si,v=i where I is the indicator function and
as Da
(u, v) are the indices of an element of Da. Implicitly, we
assume that the set of sensors which the adversary targets
is constant due to (ideally) the inherent difﬁculty of hijacking
sensors. In an integrity attack, the adversary seeks to adversely
affect the physical system by preventing proper feedback.

Consequently, it is important for the defender to identify
trusted sensor nodes. Estimation and control algorithms can
then be tuned to ignore attacked nodes. We assume that the
defender knows the system dynamics Ak, Bk, Ck as well as
the input and output histories given by u0:k−1 and y0:k but
is unaware of the set L and the initial state x0. In addition,
we assume that the adversary is limited to sensor attacks.
That is, unlike the attack models considered in the extended
moving target defense and nonlinear moving target defense,
no integrity attacks will be performed on actuators. Hence the
problem of identifying malicious nodes is independent of the
control input, allowing us to disregard the control input and
let Bk be constant. In the deterministic case, the dynamics are
then given by

xk+1 = Akxk,

yk = Ckxk + Dada
k.

(18)

Remark 5. In the deterministic case, we explore attacks where
the defender has no knowledge of the initial state. While this
is certainly not realistic, the attack vectors developed in this
scenario can still remain stealthy in a practical stochastic
setting if the adversary carefully ensures that his or her initial
attack inputs remain hidden by the noise of the system.

We now characterize attacks which are not unambiguously
identiﬁable. For notational simplicity let the sth row of Ck
and Da be denoted as C s

k and Ds, respectively.

Theorem 1. An attack on sensor s in (18) is not unambigu-
ously identiﬁable at time t if and only if there exists an x∗
0 such
that Dsda
j=0 Ak−1−j)x∗
0 for all time 0 ≤ k ≤ t and
k((cid:81)k−1
C s
0 (cid:54)= 0 for some time 0 ≤ k ≤ t.
Proof. The proof is given in [21].

k((cid:81)k−1
k = C s
j=0 Ak−1−j)x∗

Changing the system matrices as a function of time allows
the system to act like a moving target. Even if an attacker is

5

aware of the existing conﬁgurations Γ of the system, he or she
will likely be forced to generate unambiguously identiﬁable
attacks since he or she is not aware of the sequence of system
matrices.

A. System Design for Deterministic Identiﬁcation

We now consider criteria that can allow a defender to design
an effective set Γ. We assume that the adversary knows Γ,
the sequence of attack inputs Dada
0:k, and the probability
distribution of the sequence of system matrices Ak and Ck
but does not know the input sequence u0:k−1 or the output
sequence y0:k. Given this knowledge, an adversary can guess
the sequence of system matrices and if correct can generate
attacks that are not unambiguously identiﬁable.

We would like to consider systems where Ak and Ck
remain constant for multiple time steps due to the system’s
inertia. For now, we assume the pair (Ak, Ck) ⊂ Γ is
constant. An adversary can use his or her knowledge of Γ
to guess a pair (Ak, Ck) ∈ Γ and generate unidentiﬁable
attack inputs. We next determine when an attacker is able to
guess an incorrect pair and avoid generating an unambiguously
identiﬁable attack.

(A(1), C(1))
attack
inserting
(cid:44)

=

the

along

Suppose

using
image

t,2 where Os
t,j

nonzero
by

(A, C)
generates

adversary
sensor
s

a
(A(2), C(2))
of Os

Theorem 2.
an
and
input
on
attacks
(cid:2)(C s(j))T
Let Λj
tinct
(cid:26)
λj
1,1, · · · , v
v
i
a maximal
eigenvectors associated with eigenvalue λj
Jordan blocks satisfying

(C s(j)A(j))T · · · (C s(j)A(j)t−1)T (cid:3)T .
of
set
=
A(j).
eigenvalues
(cid:27)

associated
λj
i
(cid:96)(λj
i ),1
linearly

λj
1,rij (1), · · · , v
i

1, · · · , λj
qj

independent

the
with

i ),rij ((cid:96)(λj

λj
i
(cid:96)(λj

, · · · , v

{λj

set

be

i ))

of

}

(generalized)
i with (cid:96)(λj
i )

dis-
Let

be

A(j)v

λj
τ,1, A(j)v
i

λj
τ,1 = λj
i v
i
rij(t), and deﬁne V λj

λj
τ,k+1 = λj
i v
i

Let rM

ij = max

t

i

s,k ∈ CrM

ij ×rij (k) as

λj
τ,k+1 + v
i

λj
τ,k.
i

(19)



C s(j)v

λj
i
k,1

V λj

i
s,k

(cid:44)







0

0
0

λj
i
k,rij (k)

· · · C s(j)v
...
. . .
C s(j)v
· · ·
· · ·

0

λj
i
k,1









.

There exists an attack on sensor s which is not unambiguously
identiﬁable for all time if and only if there exists λ1
∈ Λ1
i1
and λ2
i2

∈ Λ2 which satisfy λ1
i1

= λ2
i2

and

Null

(cid:16)

λ1
i1
V
s

(cid:17)

λ2
i2
V
s

(cid:16)

λ2
i2
V
s

(cid:17)

,

> Null


λj
ij
s,1

V

(cid:17)

(cid:16)

λ1
i1
V
s

· · · V

j ×(cid:80)
rM

j
ij

)

(cid:96)(λ

t=1

+ Null


λj
ij
s,(cid:96)(λj
ij

)





,

rij j (t)

λj
ij
s (cid:44)

V





0

1 = rM

i22 − rM

with rM
i11 > rM
rM
Otherwise the attack can be detected in time t ≤ 2n − 1.

i11 if rM
j = 0 otherwise.

i22, and rM

i11 < rM

i11 − rM

2 = rM

i22, rM

i22 if

Proof. The proof is given in [23].

Roughly speaking, given enough observations, the output
at sensor s for a time-invariant system will be dominated by
the observable mode(s) that have the largest eigenvalue. Thus,
if the eigenvalues between two system matrices are distinct,
we are able to distinguish the resulting outputs. Theorem 2
gives the defender an efﬁcient way to determine if an attacker
can guess Γ incorrectly and still remain unidentiﬁed when the
system matrices are kept constant for at least 2n time steps.
It also prescribes a means to perform perfect identiﬁcation.
Design Recommendations

1) For all pairs i (cid:54)= j ∈ {1, · · · , l}, Λi ∩ Λj = ∅.
2) The system matrices (Ak, Ck) are periodically changed

after every κ ≥ 2n time steps.

3) Let {lk} be a sequence where lk ∈ {1, · · · , l}. Let (cid:37)k
denote the indices of a subsequence. Pr((A(cid:37)k , C(cid:37)k ) =
(A(lk), C(lk)), ∀k) = 0.

4) The pair (A(i), C(i)) is observable ∀i ∈ {1, · · · , l}.
5) For all i ∈ {1, · · · , l}, 0 /∈ Λi.

Corollary 1. Assume a defender follows the design recom-
mendations. Suppose sensor s is attacked and there is no t∗
such that Dsda
k = 0 for all k ≥ t∗. Then the sensor attack
will be unambiguously identiﬁable with probability 1.

Proof. The proof is given in [21].

As a result, an attacker who persistently biases a sensor
will be perfectly identiﬁed. Note that recommendation 3 can
be achieved with an IID assumption or an aperiodic and
irreducible Markov chain. The last 2 recommendations are not
needed for this result but are justiﬁed in the next subsection
when we consider stochastic systems.

Remark 6. Keeping the system matrices constant
for a
long enough period of time appears counter-intuitive for the
hybrid moving target. However, the given adversary is not
performing system identiﬁcation and is instead guessing the
system matrices. As such, keeping the dynamics constant does
not provide useful information for an attacker. Additionally,
keeping the matrices constant long enough gives the defender
the information he or she needs to distinguish between the
different hybrid states. Similar to the problem of observability,
the problem of identiﬁcation involves a rank deﬁcient matrix
until enough measurements have been gathered.

B. False Data Injection Detection

We now examine the effectiveness of the hybrid moving
target defense for detection in the case of a stochastic system
where the dynamics are given by

xk+1 = Akxk + wk,

yk = Ckxk + Dada

k + vk.

(20)

The information and goals of the adversary and the defender
remain unchanged except that both the adversary and the de-
fender are aware of the noise statistics and the defender knows
the distribution of the initial state x0 ∼ N (ˆx0|−1, P0|−1). To
characterize detection performance, we consider the additive
bias ∆zk the adversary injects on the normalized residues due
to his or her sensor attacks. The normalized residue is the
normalized difference between the observed measurement and
its expected value, which is slightly different than the unnor-
malized residue deﬁned in section II that is used throughout
the rest of the article. The bias ∆ek (cid:44) xk − ˆxk|k on the

a posteriori state estimation error and the bias ∆zk on the
normalized residues are given by

6

∆ek= (Ak−1 − KkCkAk−1)∆ek−1 − KkDada
k,
k + R)− 1
∆zk= (CkPk|k−1C T

2 (CkAk−1∆ek−1 + Dada

k) ,

(21)

(22)

with ∆e−1 = 0. A residue detector such as the χ2 detector
will recognize large residues and mark them as belonging to
an attack. We now show that an adversary is restricted in the
bias he or she can inject on the state estimation error without
signiﬁcantly biasing the residues and incurring detection.

Theorem 3. Suppose a defender uses a hybrid moving target
defense leveraging the design recommendations listed above.
Then lim supk→∞ (cid:107)∆ek(cid:107) = ∞ =⇒ lim supk→∞ (cid:107)∆zk(cid:107) =
∞ with probability 1.

Proof. The proof is given in [21].

Thus the attacker is able to destabilize the estimation error
only by destabilizing the residues. As such, there is a point
where an attacker is unable to introduce additional bias to the
estimation error without revealing his or her presence due to
his or her effect on the measurement residues.

C. Resilient Estimation and Identiﬁcation

While the hybrid moving target defense guarantees we can
detect unbounded false data injection attacks, we want to
also identify speciﬁc malicious sensors as in the deterministic
case. To do so, we present a resilient estimator that fuses
state estimates generated by individual sensors since previous
results [24], [25] suggest such an estimator has better fault
tolerance. This is desirable since we are attempting to force a
normally stealthy adversary to generate faults. We will show
that an attacker can destabilize this estimator only if the culprit
sensors can be identiﬁed. In particular, we will show that the
estimation error will become unbounded only if the bias on a
sensor residue is also unbounded.

To begin, we assume that for each sensor s, NS(Os

n,1) =
· · · = NS(Os
n,l), where NS(A) denotes the null space of A.
Such a condition is realistic since it implies that changing
the system dynamics does not affect what portion of the state
the sensor itself can observe. Using a Kalman decomposi-
tion for each sensor s, there exists a state transformation
Ts (cid:44) (cid:2)T uo
= xk
and (cid:2)T uo
the columns
n,1) while the columns of T o
of T uo
s
s
should be chosen so the resulting Ts is invertible. Using
the same transform Ts for each mode in Γ, there exists a
Γs = {(Cs(1), As(1)), · · · , (Cs(l), As(l))} corresponding to
Γ such that

(cid:3) such that (cid:2)T uo
T o
s
(cid:3) (cid:104)
T o
ψuoT
s
k,s
are a basis for NS(Os

= wk. Here,

ζ uoT

ψT
k,s

s
(cid:105)T

ζ T
k,s

T o
s

(cid:3) (cid:104)

(cid:105)T

k,s

s

s

ζk+1,s = Ak,sζk,s + ψk,s,

k = Ck,sζk,s + vs
ys
k,

(23)

where each pair (Ak,s, Ck,s) is observable and belongs to Γs.

A Kalman ﬁlter with bounded covariance (see proof of
0:k.

Theorem 3) can be constructed to estimate ζk,s given ys
From the deﬁnition of the Kalman ﬁlter, we have

ˆζk+1|k,s= Ak,s ˆζk|k,s,

k|k−1C T

ˆζk|k,s= (I − Kk,sCk,s)ˆζk|k−1,s + Kk,sys
k,
k,s + Rs,s)−1,
Kk,s= P s,s
k+1|k= Ak,s1 P s1,s2
P s1,s2
k|k = (I − Kk,s1 Ck,s1 )P s1,s2
P s1,s2
+Kk,s1 Rs1,s2 K T

k,s(Ck,sP s,s
k|k AT

k|k−1C T
k,s2 + Qs1,s2 ,

k|k−1(I − Kk,s2 Ck,s2 )T

k,s2 ,

zk,s= (Ck,sP s,s

k|k−1C T

k,s + Rs,s)− 1

2 (ys

k − Ck,s ˆζk|k−1,s),

(24)

(25)

(26)

(27)

(28)

(29)

k,s2

k|k,s2

k|k−1,s2

(cid:44) E[ek|k−1,s1eT

where ˆζk|k−1,s and ˆζk|k,s are the a priori and a posteriori
state estimates of ζk,s, P s1,s2
] and
k|k−1
P s1,s2
(cid:44) E[ek|k,s1 eT
] are the a priori and a posteriori
k|k
error covariance matrices with ek|k−1,s (cid:44) ζk,s − ˆζk|k−1,s and
ek|k,s (cid:44) ζk,s − ˆζk|k,s, Kk,s is the Kalman gain, Qs1,s2
(cid:44)
E[ψk,s1ψT
], Ri,j is the (i, j)th entry of R, and zk,s is the
normalized residue. Note that (27) and (28) hold for s1 = s2.
We would like to use the individual state estimates ˆζk|k,s
associated with each sensor s to obtain an overall state estimate
ˆζk|k,s + αk,s,
of xk. To do this, ﬁrst deﬁne xo
where αk,s is an IID sequence of Gaussian random variables
with αk,s ∼ N (0, σI) for some small σ > 0. Moreover
{αk,s1 } and {αk,s2 } are independent sequences. αk,s is a
mathematical artifact introduced so the subsequent estimator
has a simpliﬁed closed form and can be easily removed
or mitigated by letting σ tend to 0. From here we obtain
ˆyk = W xk + αk, where ˆyk (cid:44) (cid:104)
, xk (cid:44)
(cid:104)
ζ uoT

k,1 · · · xoT
xoT

k,s as xo
k,s

· · · ζ uoT

(cid:44) T o
s

k,m xT
k

(cid:105)T

(cid:105)T

k,m

k,1

,





αk (cid:44)

−T o

1 ek|k,1 + αk,1
...
mek|k,m + αk,m


 , W (cid:44)





−T uo
1
...
0

· · ·
. . .
· · · −T uo
m

0
...



 .

I
...
I

−T o

It can be seen that αk is normally distributed so that αk ∼
N (0, Υ), where Υ (cid:31) 0 consists of m × m blocks with the
(i, j)th block given by T o
k|kT oT
j + δijσI. Here δij is the
Kronecker delta. The minimum variance unbiased estimate
(MVUB) [26] of xk given ˆyk is given by

i P i,j

ˆxk = (W T Υ−1W )−1W T Υ−1ˆyk.

(30)

The last n entries of ˆxk, denoted as ˆx∗
k, constitute an MVUB
estimate of xk given the set of sensor estimates ˆyk. We
next show that the proposed estimator of xk has bounded
covariance.

Theorem 4. Consider the estimator of xk deﬁned by eqs. (24)
to (30). The estimator has bounded covariance.

Proof. The proof is given in [21].

We lastly demonstrate that the proposed estimator is sen-
sitive to biases ∆zk,s in individual normalized residues,
speciﬁcally showing that an inﬁnite bias introduced into the
estimator implies that the residues are also inﬁnite. Deﬁning
e∗
k due to
k
the adversary’s inputs, we have the following result.

k represent the bias on e∗

k and letting ∆e∗

(cid:44) xk − ˆx∗

7

Theorem 5. Consider the estimator of xk deﬁned by eqs. (24)
to (30). Then, with probability 1,
k(cid:107) =
∞ =⇒ lim supk→∞ (cid:107)∆zk,i(cid:107) = ∞ for some i ∈ {1, · · · , m}.
Proof. The proof is given in [21].

lim supk→∞ (cid:107)∆e∗

While the proposed estimator does not guarantee each
malicious sensor will be identiﬁed, it does guarantee that the
defender will be able to identify and remove sensors whose
attacks cause unbounded bias in the estimation error simply
by analyzing each sensor’s measurements individually. This is
due to the fact that the bias on the residues of such sensors
will grow unbounded, which can be easily detected by a χ2
detector. As a result, we propose the following detector to
identify malicious behavior for each individual sensor s

gk,s(zk−T +1:k,s) =

k
(cid:88)

j=k−T +1

z2
j,s

Hs
1≷
Hs
0

τ i
k.

(31)

Here gk,s is the detection statistic for sensor s, τ i
k represents
1 and Hs
the threshold of the detector, i ∈ {1, · · · , m}, and Hs
0
are the hypotheses that sensor s is malfunctioning or is work-
ing normally, respectively. A sensor s which repeatedly fails
detection can be removed from consideration when obtaining
a state estimate and the proposed fusion-based estimation
scheme can be adjusted accordingly.

Remark 7. Consider a standard LTI system whose dynamics
are known to an attacker. From [27], there exists an estimator
which can recover the system state with up to q sensor
attacks if and only if the system is 2q sparse observable
(that is observable even if any 2q sensors are removed). The
hybrid moving target allows us to perform perfect (stable)
state estimation in the deterministic (stochastic) scenario if
the system is merely q sparse observable. In the deterministic
case, we simply identify the q attacked sensors, remove them,
and use the healthy sensors to recover the initial state. In
the stochastic case, we can identify q sensors that cause any
destabilizing estimation errors, remove them, and then use
the proposed fusion-based state estimator to obtain a stable
estimate.

IV. EXTENDED MOVING TARGET DEFENSE

Instead of varying the system matrices directly, the extended
moving target defense introduces an auxiliary system whose
sensor measurements reveal any biases an adversary exerts on
the nominal control system. As such, we seek to design the
auxiliary system in such a way as to maximize the probability
of detection when the system is under attack. Speciﬁcally, we
would like to design the parameters that generate ¯Ak, ˜Bk,
and ¯Ck to maximize detection performance. Because a joint
maximization over ¯Ak, ˜Bk, and ¯Ck becomes infeasible for
¯Ak and ¯Ck, we recognize that detection performance is a
direct function of accurate state estimation. Consequently, we
design the parameters that generate ¯Ak and ¯Ck to maximize
estimation performance while designing the parameters that
generate ˜Bk to maximize detection performance.
Remark 8. For notational simplicity, we will assume that ¯Ak,
˜Bk, and ¯Ck are not sparse. However, the analysis presented
in this section can easily be extended to designs where ¯Ak,
˜Bk, and ¯Ck are sparse matrices.

We consider a general set of additive integrity attacks as
modeled in (2) for the nominal system where Ba = B (all
actuators can be corrupted) and Da = I (all sensors can be
modiﬁed). An adversary with these capabilities and knowledge
of the nominal system dynamics can arbitrarily and stealthily
perturb the nominal system using a covert attack [7]. This set
of additive integrity attacks can be written as

k+1 = Ak ¯xA
¯xA

k + Bk(uk + ua

k) + ¯wk,

k = Ck ¯xA
¯ya

k + ¯da

k + ¯vk, (32)

where ¯xA
represents the attacked states, ua
k denotes the
k
attacker’s additive bias on the control inputs, ¯da
k represents the
attacker’s additive bias on the sensor measurements, and ¯ya
k de-
notes the biased sensor measurements received by the system
operator. Here the auxiliary actuators ˜Bk and coupling matri-
ces ¯Ak and ¯Ck are generated from the following distributions:
) ∀i, ¯Ak(row i) ∼ N (µ ¯A, Σ ¯A) ∀i,
˜Bk(row i) ∼ N (µ ˜B, Σ ˜Bk
and ¯Ck(row i) ∼ N (µ ¯C, Σ ¯C) ∀i with independence between
rows over time. We consider a strong adversary who is able to
read and modify all of the inputs and outputs so that the design
of the parameters generating ¯Ak, ˜Bk, and ¯Ck is optimal for
even the strongest additive integrity attacks. Given this attack
model, we now describe how to design the covariances Σ ˜Bk
,
Σ ¯A, and Σ ¯C of the distributions associated with the auxiliary
actuators and the coupling matrices to maximize detection and
estimation performance, respectively.

A. Auxiliary Actuators Design

To design the covariance Σ ˜Bk

to maximize detection per-
formance, we use the Kullback-Liebler (KL) divergence as
a metric for detection performance that, roughly speaking,
quantiﬁes the distance between the distribution of the residue
under attack and the distribution of the residue under normal
operation. We note that any additive integrity attack will result
in an additive bias on the residue which can be written as
a linear combination of the control input biases ua
j:k−1 and
sensor measurement biases ¯da
j+1:k exerted by the attacker.
Here j denotes the time when the attacker ﬁrst exerts a bias
on the control inputs and j + 1 represents the time when the
attacker ﬁrst attempts to hide his or her attack by exerting a
bias on the sensor measurements. As shown in [22], the bias
on the residue ∆¯zi can be written as

8

on the residue. Because the covariance of f0 and f1 are the
same, the KL divergence is symmetric and can be written as

DKL (f1(¯zk−T +1:k)||f0(¯zk−T +1:k)) =

= E¯zk−T +1:k

(cid:32)

(cid:34)

1
2

k
(cid:88)

i=k−T +1

−¯zT

i Σ−1

i ¯zi+

+ (¯zi − M(j,i)φj:k)T Σ−1

i

(¯zi − M(j,i)φj:k)

=

1
2

φT
j:k

(cid:32) k

(cid:88)

i=k−T +1

M T

(j,i)Σ−1

i M(j,i)

(cid:33)

φj:k,

(cid:35)

H0

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(36)

where the second equality follows from the fact
residue has zero mean under normal operation.

that

the

(j,i)Σ−1

Maximizing the KL divergence becomes difﬁcult because
the attacker biases φj:k are unknown to the defender. However,
we note that M T
i M(j,i) is positive semideﬁnite, allowing
us to maximize the expected value of the KL divergence
for all possible additive integrity attacks. This is carried
out by maximizing a nonnegative constant (cid:15) such that the
expected value of the KL divergence is greater than a positive
semideﬁnite lower bound N ((cid:15)) that is a function of (cid:15). Since
there are real-world constraints on the variance magnitude of
the auxiliary actuators, we constrain the covariance Σ ˜Bk
with
a positive semideﬁnite upper bound NB. This maximization
problem is presented below

arg max
(cid:15),Σ ˜Bk

(cid:15)

s.t. Σ ˜Bk

(cid:22) NB,

1
2

E ˜Bj:k−1

(cid:34)

k
(cid:88)

i=k−T +1

M T

(j,i)Σ−1

i M(j,i)

(cid:35)

(cid:23) N ((cid:15)).

(37)

As shown in [22], we can construct the positive semidef-
inite lower bound N ((cid:15)) to match the block structure of
(cid:80)k
(j,i)Σ−1
i M(j,i). The off-diagonal blocks of this
structure are not functions of Σ ˜Bk
, allowing the constraint in
(37) to be simpliﬁed to the following series of constraints

i=k−T +1 M T

(cid:2)M x
(cid:124)

(j,i) −M y
(cid:123)(cid:122)
M(j,i)

(j,i)

(cid:3)

(cid:125)

(cid:104)
uaT

j

(cid:124)

· · · uaT

k−1

j+1 · · · ¯daT
¯daT

k

(cid:123)(cid:122)
φj:k

(cid:105)T

,

(cid:125)

(33)

1
2

k
(cid:88)

i=k−T +1

Ψtt

(j,i) (cid:23) (cid:15)Nt,

t = 1, · · · , i − j.

(38)

where φj:k represents a vector containing all of the attacker’s
biases and M x

(j,i) are given by

(j,i) and M y

Here Nt (cid:23) 0, and Ψtt

(j,i) is shown in [22] to be

M x
M y

(j,i)

(j,i)(cid:44) [CiD(j,i)Bj · · · CiD(i−1,i)Bi−1

0i · · · 0k−1] ,

(cid:44) [Ξ(j+1,i) · · · Ξ(i−1,i)

I

0i+1 · · · 0k] ,

(34)

(35)

with D(j,i) (cid:44) (cid:81)i−1
CiD(j,i)AjKj, 0t ∈ R(m+ ˜m)×p for M x
R(m+ ˜m)×(m+ ˜m) for M y

t=j+1 Ai+j−t(I − Ki+j−tCi+j−t), Ξ(j,i) (cid:44)
(j,i), and I, 0t ∈

(j,i).

Under normal operation, it can be shown that the residue
follows a normal distribution f0(¯zi) = N (0, Σi) with zero
mean and covariance Σi (cid:44) CiPi|i−1CT
i + R. If the defender
has no prior information about the attacker’s biases φj:k, the
residue under attack will also follow a normal distribution
f1(¯zi) = N (M(j,i)φj:k, Σi) with a mean equal to the bias

Ψtt

(j,i) = Tr( ˜DT
+ Sum( ˜DT
+ BT ¯DT
+ [µ ˜B · · ·
+ BT ¯DT

(j+t−1,i)CT
(j+t−1,i)CT

i Ci ˜D(j+t−1,i))Σ ˜Bj+t−1
i Ci ˜D(j+t−1,i))µ ˜BµT
˜B

i Σ−1
i Σ−1
i Ci ¯D(j+t−1,i)B
(j+t−1,i)CT
i Σ−1
i Ci ˜D(j+t−1,i) [µ ˜B · · ·

i Ci ¯D(j+t−1,i)B

i Σ−1
µ ˜B] ˜DT
i Σ−1

(j+t−1,i)CT

(j+t−1,i)CT

µ ˜B]T ,

(39)

where (cid:2) ˜D(j+t−1,i)
represents the sum of all the elements of A.

¯D(j+t−1,i)

(cid:3) (cid:44) D(j+t−1,i) and Sum(A)

Combining (38) and (39) allows the second constraint in
(37) to be written as a set of positive semideﬁnite constraints.
We choose j = k − T so that the KL divergence is maximized

over the time window T of the chi-squared detector. This
allows the optimization problem in (37) to be written as

maximizing this amount of information should increase detec-
tion performance.

9

arg max
(cid:15),Σ ˜Bk

(cid:15)

s.t. Σ ˜Bk

(cid:22) NB,

k−t
(cid:88)

Tr( ˜DT

(k−T +t,i)CT

i Σ−1

i Ci ˜D(k−T +t,i))Σ ˜Bk−T +t

1
2

(k−T +t,i)CT

i=k−T +1
+ Sum( ˜DT
+ [µ ˜B · · · µ ˜B] ˜DT
+ BT ¯DT
(k−T +t,i)CT
+ BT ¯DT
(k−T +t,i)CT

i Σ−1
(k−T +t,i)CT
i Σ−1
i Σ−1

i Ci ˜D(k−T +t,i))µ ˜BµT
˜B

i Σ−1

i Ci ¯D(k−T +t,i)B

i Ci ˜D(k−T +t,i) [µ ˜B · · · µ ˜B]T
i Ci ¯D(k−T +t,i)B (cid:23) (cid:15)Nt+1,

(40)

Remark 10. If the system is operating normally, this design
will maximize the amount of information the system operator
the unaltered states xk. Consequently,
receives about
this
design will
increase estimation performance regardless of
whether or not the system is under attack.

We consider the amount of information all the biased aux-
0:k contain about all the attacked
0:k. As shown in [22], we can represent all the biased
0:k as

iliary sensor measurements ˜ya
states xA
auxiliary sensor measurements ˜ya

with t = 0, · · · , T − 1. If Σ ˜Bk
is time-invariant, this becomes
a semideﬁnite program with unique solutions for (cid:15) and Σ ˜Bk
.
However, to make this optimization problem solvable when
Σ ˜Bk
is time-varying, we reformulate the optimization by
considering a global optimization problem of (40) over all time
steps k. We add a subscript to (cid:15) in the second constraint of (40)
so that it is now (cid:15)k−T +t, distinguishing it from (cid:15)’s at other
time steps. Furthermore, we modify the objective function to
be (cid:80)∞
k=0 (cid:15)k. This formulation allows the global optimization
problem to be divided into optimization problems for each
time step k, resulting in

arg max
(cid:15)k,Σ ˜Bk

(cid:15)k

s.t. Σ ˜Bk

(cid:22) NB,

HD (cid:44)

1
2

k+T −2t
(cid:88)

Tr( ˜DT

(k,i)CT

i Σ−1

i Ci ˜D(k,i))Σ ˜Bk

(k,i)CT

i=k+1−t
+ Sum( ˜DT
+ [µ ˜B · · · µ ˜B] ˜DT
+ BT ¯DT
i Σ−1
+ BT ¯DT
i Σ−1

(k,i)CT
(k,i)CT

i Σ−1

i Ci ¯D(k,i)B

i Ci ˜D(k,i))µ ˜BµT
˜B
(k,i)CT
i Σ−1
i Ci ˜D(k,i) [µ ˜B · · · µ ˜B]T
i Ci ¯D(k,i)B (cid:23) (cid:15)kNt+1,

(41)

which is a semideﬁnite program with t = 0, · · · , T − 1 and
unique solutions for (cid:15)k and Σ ˜Bk
. As a result, this optimization
problem provides a method for designing Σ ˜Bk
so that the
expected value of the KL divergence is maximized for the
set of all possible additive integrity attacks described by (32).

Remark 9. One could alternatively consider formulating an
optimization problem around the KL divergence as a function
of both the mean µ ˜B and the covariance Σ ˜Bk
. However,
such formulations will be non-convex. Addressing the joint
design of µ ˜B and Σ ˜Bk
using non-convex techniques along
with alternative formulations of (37) are left for future work.
This remark also applies to the joint design of µ ¯A, µ ¯C, Σ ¯A,
and Σ ¯C in (44) and µG and ΣG in (66).

B. Coupling Matrices Design

We now focus our attention on the design of the covariances
Σ ¯A and Σ ¯C that generate the coupling matrices ¯Ak and
¯Ck. To maximize estimation performance, we seek to design
Σ ¯A and Σ ¯C to maximize the amount of information the
defender receives about the attacked states xA
k through the
biased auxiliary sensor measurements ˜ya
k. Accurate estimation
of the attacked state will enable a system operator to better
distinguish between true and falsiﬁed measurements. Since
the accuracy of the state estimate depends on the amount of
information the sensor measurements carry about the state,

0:k = (HA + HC )xA
˜ya

0:k + HBuA

0:k + ˜da

0:k + HW ˜w0:k−1 + ˜v0:k, (42)

(cid:105)T

(cid:44) (cid:104)

(cid:105)T

(cid:44) (cid:104)

0

0

uAT
0
(cid:105)T

where ˜ya

0:k
(cid:44) (cid:104)

˜yaT
· · · uAT

, xA
xAT
0:k
(cid:44) uk + ua

· · · ˜yaT
k
(cid:105)T
, uA
k
, ˜w0:k−1 (cid:44) (cid:2)˜xT

· · · xAT
k, ˜da
uA
0:k
0:k
(cid:104)
(cid:3)T , ˜v0:k (cid:44)
˜daT
˜wT
0
(cid:3)T , HA (cid:44) HDBlkDiag( ¯A0, · · · , ¯Ak), HB (cid:44)
(cid:2)˜vT
0 · · · ˜vT
k
HDBlkDiag( ˜B0, · · · , ˜Bk), and HC (cid:44) BlkDiag( ¯C0, · · · , ¯Ck)
with

0 · · · ˜wT

· · · ˜daT

k−1

(cid:44)

,

k

k

k

0







0
˜C ˜A0
...
˜C ˜Ak−1

· · ·
· · ·
. . .
· · ·

0
0
...
˜C ˜A0







0
0
...
0

, HW (cid:44)





˜C ˜A0
...
˜C ˜Ak

· · ·
. . .
· · ·



 .

0
...
˜C ˜A0

To quantify the amount of information the defender receives
about the attacked states through the biased auxiliary sensor
measurements, we use the Fisher information matrix I. The
Fisher information matrix is a metric that quantiﬁes the amount
of information a set of measurements contains about a set of
unknown parameters. As seen in [22], the Fisher information
matrix is shown to be

I = (HA + HC )T (HW Σ ˜QH T

W + Σ ˜R)−1(HA + HC ),

(43)

where ˜x0 ∼ N (0, ˜P0|−1), ˜Pk+1|k is the a priori er-
(cid:44)
ror covariance matrix for
BlkDiag( ˜P0|−1, ˜Q, · · · , ˜Q), and Σ ˜R

the auxiliary system, Σ ˜Q

(cid:44) BlkDiag( ˜R, · · · , ˜R).

We note that I is positive semideﬁnite, allowing us to
maximize the expected value of the Fisher information matrix
for all possible additive integrity attacks. This is carried out by
maximizing a nonnegative constant γ such that the expected
value of the Fisher information matrix is greater than a positive
semideﬁnite lower bound Θ(γ) that is a function of γ. Since
there are real-world constraints on the variance magnitude
of the state coupling and auxiliary sensors, we constrain
the covariances Σ ¯A and Σ ¯C with positive semideﬁnite upper
bounds ΘA and ΘC, respectively. This maximization problem
is presented below

arg max
γ,Σ ¯A,Σ ¯C

γ s.t. Σ ¯A (cid:22) ΘA, Σ ¯C (cid:22) ΘC , E ¯A0;k−1, ¯C0:k

[I] (cid:23) Θ(γ).

(44)
As shown in [22], we can construct the positive semideﬁnite
lower bound Θ(γ) to match the block structure of I. The off-
diagonal blocks of this structure are not functions of Σ ¯A or
Σ ¯C, allowing the constraint in (44) to be simpliﬁed to the
following series of constraints

Ωi

A + Ωi

C + Ωi

AC + ΩiT

AC (cid:23) γΘi,

i = 0, · · · , k,

(45)

i

AC

(cid:2) ¯Ci

(cid:2) ¯AT

(cid:3) FiiE ¯Ci

A = Tr(Jii)Σ ¯A + Sum(Jii)µ ¯AµT
¯A,
(cid:44)
C = Tr(Sii)Σ ¯C + Sum(Sii)µ ¯CµT
¯C,
(cid:3). Jii ∈ R˜n×˜n, Sii ∈ R ˜m× ˜m, and
W +
W +

where Θi (cid:23) 0, Ωi
Ωi
E ¯Ai
Fii ∈ R˜n× ˜m represent the (i, i)th blocks of H T
Σ ˜R)−1HD, (HW Σ ˜QH T
W + Σ ˜R)−1, and H T
Σ ˜R)−1, respectively.

D(HW Σ ˜QH T
D(HW Σ ˜QH T

and Ωi

Utilizing (45) and the results above allows the third con-
straint in (44) to be written as a series of positive semideﬁnite
constraints. We choose k = T − 1 so that the Fisher infor-
mation matrix is maximized over the time window T of the
chi-squared detector. This allows the optimization problem in
(44) to be written as

arg max
γ,Σ ¯A,Σ ¯C

γ s.t. Σ ¯A (cid:22) ΘA, Σ ¯C (cid:22) ΘC ,

Tr(Jii)Σ ¯A + Tr(Sii)Σ ¯C + Sum(Jii)µ ¯AµT
¯A
+ Sum(Sii)µ ¯C µT
+ [µ ¯C · · · µ ¯C ] F T

¯C + [µ ¯A · · · µ ¯A] Fii [µ ¯C · · · µ ¯C ]T
ii [µ ¯A · · · µ ¯A]T (cid:23) γΘi,

(46)

which is a semideﬁnite program with i = 0, · · · , T − 1
and unique solutions for γ, Σ ¯A, and Σ ¯C. As a result, this
optimization problem provides a method for designing Σ ¯A
and Σ ¯C so that the expected value of the Fisher information
matrix is maximized for the set of all possible additive integrity
attacks described by (32).

Remark 11. Unlike the hybrid moving target defense, the
extended moving target defense as presented is not designed
directly to perform attack identiﬁcation or resilient state
estimation. The nominal sensors only measure the original
LTI dynamics and as such offer no inherent advantages over
standard techniques for identiﬁcation and estimation. Never-
theless, if the sensors for the extended subsystem are secure,
an avenue exists for resilient state estimation. Speciﬁcally,
consider a system with (only) sensor attacks on the nominal
sensors. If the system consisting of the nominal dynamics with
the extended sensors is observable, a resilient state estimate
can be obtained that can be used to determine which of the
nominal sensors are reporting false or misleading information,
resulting in attack identiﬁcation.

V. NONLINEAR MOVING TARGET DEFENSE

While the parameters that generate the time-varying dynam-
ics of the extended moving target are designed to maximize
detection and estimation performance, it is still possible for
an intelligent adversary to perform some system identiﬁcation.
This is due to the fact that the auxiliary sensor measurements
contain some information about the time-varying matrices ¯Ak,
˜Bk, and ¯Ck. As a result, we seek to leverage the nonlinearity
Gkh(xk) in the nonlinear moving target
to minimize the
amount of information an adversary may receive about the
time-varying matrices ¯Ak, ˜Bk, and ¯Ck. The auxiliary sensors
in this system measure a nonlinear function of the state, where
the nonlinear function h(xk) is an element-wise mapping from
Rn → Rn and Gk ∈ R ˜m×n is generated from the distribution
Gk(column i) ∼ N (µG, ΣG) ∀i with independence between
columns over time.

A. Limiting System Identiﬁcation

We again consider a general set of integrity attacks as
modeled in (32) for the nonlinear moving target defense. From

the perspective of the attacker, this can be written as

k+1= Ak ¯xA
¯xA

k + Bk (uk + ua
k)
(cid:124)
(cid:125)

+ ¯wk,

(cid:123)(cid:122)
uA
k

=

(cid:21)

(cid:20)˜yA
k
yA
k
(cid:124) (cid:123)(cid:122) (cid:125)
¯yA
k

(cid:21)

(cid:20) ˜C ¯Ck
C
0

(cid:124)

(cid:123)(cid:122)
Ck

(cid:125)

(cid:21)

(cid:20)˜xA
k
xA
k
(cid:124) (cid:123)(cid:122) (cid:125)
¯xA
k

(cid:20)

+

Gkh(xA
k )
0

(cid:21)

+

,

(cid:21)

(cid:20)˜vk
vk
(cid:124) (cid:123)(cid:122) (cid:125)
¯vk

10

(47)

(48)

where ¯yA
k represents the sensor measurements that the attacker
intercepts. Given these dynamics, the auxiliary intercepted
sensor measurements are given by

k = ˜C ˜Ak ˜x0 + ˜C
˜yA

k−1
(cid:88)

j=0

˜Ak−1−j( ¯AjxA

j + ˜BjuA

j + ˜wj)

(49)

+ ¯CkxA

k + Gkh(xA

k ) + ˜vk.
Considering the amount of information all the auxiliary inter-
cepted sensor measurements ˜yA
0:k contain about all the time-
varying matrices ¯A0:k, ˜B0:k, and ¯C0:k, we can represent all
the auxiliary intercepted sensor measurements ˜yA

0:k as
0:k)

(50)

(cid:105)T

0:k = HX vec( ¯AT
˜yA

0:k) + HU vec( ˜BT
+ HF vec(G0:k) + HW ˜w0:k−1 + ˜v0:k,

0:k) + HEvec( ¯C T

(cid:104)

(cid:44)

0

k

,

˜yA
0:k

˜yAT

· · · ˜yAT

0:k) (cid:44) (cid:2)vec( ¯AT
0:k) (cid:44) (cid:2)vec( ˜BT
0:k) (cid:44) (cid:2)vec( ¯C T

0 )T · · · vec( ¯AT
0 )T · · · vec( ˜BT
0 )T · · · vec( ¯C T

where
k )T (cid:3)T ,
vec( ¯AT
k )T (cid:3)T ,
vec( ˜BT
k )T (cid:3)T ,
vec( ¯C T
vec(G0:k) (cid:44) (cid:2)vec(G0)T · · · vec(Gk)T (cid:3)T
and
are given by HX (cid:44) HDBlkDiag(I˜n ⊗ xAT
, · · · , I˜n ⊗ xAT
0
, · · · , I˜n ⊗ uAT
(cid:44)
HU
(cid:44)
HE
HF (cid:44) BlkDiag (cid:0)h(xA

I ˜m ⊗ xAT
0
0 )T ⊗ I ˜m, · · · , h(xA
We consider a strong adversary who has full knowl-
0:k. With this adversary,
edge of
the distribution of all
the auxiliary intercepted sensor
measurements given the time-varying parameters θ0:k (cid:44)
0:k)T (cid:3)T follows a normal dis-
(cid:2)vec( ¯AT
vec( ¯C T
0:k)T
tribution ˜yA
0:k|θ0:k , Σ˜yA
0:k|θ0:k ) with mean and
covariance given by

and
the matrices
k ),
k ),
and

HDBlkDiag(I˜n ⊗ uAT
0
BlkDiag

, · · · , I ˜m ⊗ xAT
k
(cid:1).
k )T ⊗ I ˜m

the attacked states xA

0:k|θ0:k ∼ N (µ˜yA

vec( ˜BT

0:k)T

(cid:16)

(cid:17)

,

µ˜yA

0:k|θ0:k

= HX vec( ¯AT

0:k)+HU vec( ˜BT

0:k)+HEvec( ¯C T

Σ˜yA

0:k|θ0:k

= HF Σ(cid:126)gH T

F + HW Σ ˜QH T

W + Σ ˜R,

µ(cid:126)g
where
BlkDiag(ΣG, · · · , ΣG).

(cid:44)

(cid:2)µT

G · · · µT
G

(cid:3)T

and

Σ(cid:126)g

(cid:44)

To quantify the amount of information the strong adversary
the time-varying parameters θ0:k from the
receives about
auxiliary intercepted sensor measurements ˜yA
0:k, we use the
Bayesian Fisher information matrix IN L which accounts for
prior information the strong adversary has access to about the
time-varying parameters. Since the joint distribution of ˜yA
0:k
and θ0:k is Gaussian, each element IN L(i, j) of the Fisher
information matrix takes the following form [28]

IN L(i,j)=Eθ0:k

(cid:34) ∂µT
˜yA
0:k|θ0:k
∂θ0:k(i)

Σ−1
˜yA
0:k|θ0:k

∂µ˜yA
0:k|θ0:k
∂θ0:k(j)

+

1
2

Σ−1
θ0:k

(i,j)

+

1
2

Tr

(cid:32)

Σ−1
˜yA
0:k|θ0:k

∂Σ˜yA

0:k|θ0:k
∂θ0:k(i)

Σ−1
˜yA
0:k|θ0:k

∂Σ˜yA
0:k|θ0:k
∂θ0:k(j)

(cid:33)(cid:35)
,

(53)

0:k)+HF µ(cid:126)g,
(51)
(52)

(cid:44)

BlkDiag(Σ ¯A, · · · , Σ ¯A, Σ ˜B, · · · , Σ ˜B,
where Σθ0:k
Σ ¯C, · · · , Σ ¯C), and the partial derivatives of the mean µ˜yA
0:k|θ0:k
0:k|θ0:k equal 0 and H(column i),
and the covariance Σ˜yA
respectively, with H (cid:44) (cid:2)HX HU HE
(cid:3). Applying these
results to each element of the Fisher information matrix
implies that IN L can be written as

IN L = H T (HF Σ(cid:126)gH T
(cid:32) n
(cid:88)

(cid:32)

BlkDiag

= H T

F + HW Σ ˜QH T

W + Σ ˜R)−1H +

1
2

Σ−1
θ0:k

(cid:33)

h(xA

0 (i))2ΣG, · · · ,

h(xA

k (i))2ΣG

(54)

n
(cid:88)

i=1

i=1

(cid:33)−1

+ HW Σ ˜QH T

W + Σ ˜R

H +

1
2

Σ−1
θ0:k

.

To understand how the nonlinear term Gkh(xA
k ) inﬂuences the
Fisher information matrix, we consider the dynamics given in
(47) and (48) without the nonlinear term Gkh(xA
k ) and see
that the Fisher information matrix IL can be written as

IL = H T (HW Σ ˜QH T

W + Σ ˜R)−1H +

1
2

Σ−1
θ0:k

.

(55)

Given these representations of IN L and IL in (54) and (55),
we can use the Woodbury identity to show that the difference
between the Fisher information matrices associated with the
systems containing and not containing the nonlinearity is
positive deﬁnite as seen below

IL−IN L = H T Σ−1

F Σ−1

N HF )−1H T

F Σ−1

N H

= H T Σ−1
N

(cid:32)

(cid:126)g + H T
N HF (Σ−1
(cid:18)
1
i=1 h(xA
(cid:19)

BlkDiag

(cid:80)n

· · · ,

1
i=1 h(xA

(cid:80)n

k (i))2

Σ−1
G

+ Σ−1
N

(cid:33)−1

Σ−1

N H (cid:23) 0,

0 (i))2

Σ−1

G , · · ·

(56)

k ) → ∞, IN L → 1

where ΣN (cid:44) HW Σ ˜QH T
W + Σ ˜R. Because this difference is
positive deﬁnite, adding the nonlinear term Gkh(xA
k ) to the
system dynamics decreases the amount of information the
strong adversary receives about the time-varying parameters
θ0:k from the auxiliary intercepted sensor measurements ˜yA
0:k.
To minimize this amount of information, we would like
to design ΣG and the function h to maximize IL − IN L,
the difference between the Fisher information matrices as-
sociated with the systems containing and not containing the
nonlinearity. From (54) and (56), we see that as (cid:107)ΣG(cid:107) → ∞
2 Σ−1
or as h(xA
and IL − IN L →
θ0:k
W + Σ ˜R)−1H. As the covariance ΣG or non-
H T (HW Σ ˜QH T
linear function h(xA
k ) approaches inﬁnity, the information the
adversary receives about the time-varying parameters θ0:k is
reduced to his or her a priori information about θ0:k. As a
result, an increase in the covariance ΣG or an increase in
the magnitude of the nonlinear function h(xA
k ) results in the
adversary receiving less information about the time-varying
parameters θ0:k. In addition, as ΣG → 0 or as h(xA
k ) → 0,
IN L → IL and IL − IN L → 0. Consequently, a decrease in
the covariance ΣG or a decrease in the magnitude of h(xA
k )
results in the adversary receiving more information about the
time-varying parameters θ0:k.

This general analysis provides intuition about the effects of
the magnitude of the nonlinearity on the information received
by the adversary. Because the function h(xk) determines the
magnitude of the nonlinearity while the coefﬁcient matrix Gk
determines the direction of the nonlinearity, in Section VII we

11

design the function h(xk) to limit the adversary’s information
while in the next subsection we design the coefﬁcient matrix
Gk to maximize the defender’s estimation performance.

B. Nonlinearity Design

We now consider the same additive integrity attacks on the
nonlinear moving target as described in (47) and (48) from
the perspective of the defender. We want to provide joint
guidelines for designing the parameters of the distributions
that generate the time-varying matrices ¯Ak, ¯Ck, and Gk. Here
we do not design the parameters that generate ˜Bk because the
extended moving target design of Σ ˜Bk
should be sufﬁcient
to maximize detection performance as long as the magnitude
of the nonlinear function h is approximately zero when the
state lies within a normal region of operation. In providing a
joint design for the time-varying matrices ¯Ak, ¯Ck, and Gk,
we seek to design Σ ¯A, Σ ¯C, and ΣG to maximize the amount
of information the defender receives about the attacked states
to improve the defender’s estimation performance, in turn
improving detection performance. The state dynamics are the
same as those given in (47), while the sensor measurements
are given by
(cid:20)˜ya
k
ya
k
(cid:124) (cid:123)(cid:122) (cid:125)
¯ya
k

(cid:21)
Gkh(xA
k )
0

(cid:20) ˜C ¯Ck
C
0

(cid:20)˜vk
vk
(cid:124) (cid:123)(cid:122) (cid:125)
¯vk

(cid:20) ˜da
k
da
k
(cid:124) (cid:123)(cid:122) (cid:125)
¯da
k

(cid:20)˜xA
k
xA
k
(cid:124) (cid:123)(cid:122) (cid:125)
¯xA
k

(57)

(cid:21)
,

(cid:123)(cid:122)
Ck

=

+

+

+

(cid:21)

(cid:21)

(cid:21)

(cid:20)

(cid:21)

(cid:124)

(cid:125)

where ¯ya
k represents the biased sensor measurements that the
defender receives. Given these dynamics, the biased auxiliary
sensor measurements are given by

k = ˜C ˜Ak ˜x0 + ˜C
˜ya

k−1
(cid:88)

j=0

˜Ak−1−j (cid:16) ¯AjxA

j + ˜BjuA

j + ˜wj

(cid:17)

(58)

+ ¯CkxA

k + Gkh(xA

k ) + ˜vk + ˜da
k.
Considering the amount of information all the biased auxiliary
sensor measurements ˜ya
0:k carry about all the attacked states
xA
0:k, we can represent all the biased auxiliary sensor measure-
ments ˜ya

0:k as
0:k = (HA + HC )xA
˜ya

0:k + HGh(xA
0:k + HW ˜w0:k−1 + ˜v0:k,

+ ˜da

0:k) + HBuA
0:k

(59)

0 )T · · · h(xA

where HG (cid:44) BlkDiag(G0, · · · , Gk) and h(xA
0:k) (cid:44)
k )T (cid:3)T . With this representation, we see that
(cid:2)h(xA
the distribution of all the biased auxiliary sensor measurements
given all the attacked states follows a normal distribution
˜ya
0:k|xA
) with mean and covari-
ance given by

0:k ∼ N (µ˜ya

0:k|xA
0:k

0:k|xA
0:k

, Σ˜ya

µ˜ya

0:k|xA
0:k

= (HA + HC )xA

0:k + HGh(xA

0:k) + HBuA

0:k + ˜da

0:k, (60)

Σ˜ya

0:k|xA
0:k

W + Σ ˜R.

= HW Σ ˜QH T
To quantify the amount of information the defender receives
about the attacked states through the biased auxiliary sensor
measurements, we use the Fisher information matrix ¯I. Since
˜ya
0:k|xA
0:k follows a multivariate Gaussian distribution, each
element of the Fisher information matrix ¯I(i, j) takes the
following form [28]

(61)

¯I(i, j) =

+

1
2

Tr

∂µT
˜ya
0:k|xA
0:k
∂xA
0:k(i)

(cid:32)

Σ−1
0:k|xA
˜ya
0:k

Σ−1
0:k|xA
˜ya
0:k

∂µ˜ya
∂xA

0:k|xA
0:k
0:k(j)

+

∂Σ˜ya
∂xA

0:k|xA
0:k
0:k(i)

Σ−1
0:k|xA
˜ya
0:k

∂Σ˜ya
∂xA

0:k|xA
0:k
0:k(j)

(62)

(cid:33)

,

where the partial derivative of the covariance Σ˜ya
0 and the partial derivative of the mean is given by

0:k|xA
0:k

equals

∂µ˜ya
∂xA

0:k|xA
0:k
0:k(i)

= HA(clmn i) + HC (clmn i) + HG

∂h(xA
∂xA

0:k)
0:k(i)

.

(63)

Applying these results to each element of the Fisher informa-
tion matrix implies that ¯I can be written as

(cid:18)

¯I =

HA + HC + HG

(cid:19)T

0:k)

∂h(xA
∂xA
0:k

(cid:18)

+ Σ ˜R)−1

HA + HC + HG

W +

(HW Σ ˜QH T
(cid:19)

0:k)

∂h(xA
∂xA
0:k

.

(64)

To maximize the amount of information the defender re-
ceives about the attacked states, we want to maximize the
expected value of the Fisher information matrix which takes
the following form

E ¯A0:k−1, ¯C0:k,G0:k

(cid:104)

(cid:2)¯I(cid:3)=E ¯A0:k−1, ¯C0:k
(cid:104)
ΩG+ΩAG+ΩT

+E ¯A0:k−1, ¯C0:k,G0:k

ΩA+ΩC+ΩAC+ΩT

AG+ΩCG+ΩT

CG

(cid:105)

AC
(cid:105)
,

(65)

0:k

0:k

0:k

0:k

0:k)

0:k)

H T

0:k)T

N HG

N HG

Because

A Σ−1

C Σ−1

(cid:2)ΩAG + ΩT

∂h(xA
∂xA
N HG

AG + ΩCG + ΩT

E ¯A0:k−1, ¯C0:k,G0:k

and ΩAC are deﬁned as given in
, ΩAG (cid:44)
∂h(xA
0:k)
∂xA

where ΩA, ΩC,
(45) and ΩG (cid:44) ∂h(xA
GΣ−1
∂xA
∂h(xA
, and ΩCG (cid:44) H T
H T
. We
∂xA
note that the ﬁrst term in (65) contains Σ ¯A and Σ ¯C while the
second term only contains ΣG. Furthermore, the ﬁrst term in
(65) is simply the expected value of the Fisher information
matrix given in (43). Consequently, we can maximize the
amount of information the defender receives about the attacked
states by jointly designing Σ ¯A and Σ ¯C as given in (46) while
designing ΣG to maximize the second term in (65).

entries of EG0:k [ΩG]

(cid:3)
CG
does not contain ΣG, we only consider EG0:k [ΩG] when
maximizing the second term in (65). Furthermore, only
the diagonal
functions of
ΣG, so we consider Tr (EG0:k [ΩG]) as the metric to be
maximized where Tr (EG0:k [ΩG]) = xAT
EG0:k [FG] xA
N L
N L
(cid:105)T
and FG (cid:44)
0:k(1))
0:k((k+1)n))
with xA
0:k(1)
0:k((k+1)n)
Diag(HG(clmn 1)T Σ−1
+
N HG(clmn 1), · · · , HG(clmn (k
1)n)T Σ−1
N L is unknown to
the defender, it becomes difﬁcult to maximize Tr (EG0:k [ΩG]).
However, we note that EG0:k [FG] is positive semideﬁnite,
allowing us to maximize Tr (EG0:k [ΩG]) for all possible xA
N L
by maximizing a nonnegative constant β such that EG0:k [FG]
is greater than a positive semideﬁnite lower bound βI. This
maximization problem is presented below where M is a
positive semideﬁnite upper bound that represents real-world
constraints on the variance magnitude of
the nonlinear
auxiliary sensors

N HG(clmn (k + 1)n)). Because xA

(cid:44) (cid:104) ∂h(xA
∂xA

· · · ∂h(xA
∂xA

are

N L

arg max
β,ΣG

β s.t. ΣG (cid:22) M, EG0:k [FG] (cid:23) βI.

(66)

Since FG is a diagonal matrix, the second constraint in (66)

can be simpliﬁed to the following series of constraints

EG0:k [FG(i, i)] ≥ β,

i = 1, · · · , (k + 1)n.

(67)
Noting that EG0:k [FG(i, i)] = Tr (cid:0)(ΣG + µGµT
(cid:1) where
j (cid:44) (cid:98)(i−1)/n(cid:99) and choosing k = T −1 so that Tr (EG0:k [ΩG])
is maximized over the time window T of the chi-squared
detector, the optimization problem in (66) can be written as

G)Sjj

β s.t. ΣG (cid:22) M, Tr

(cid:16)
(ΣG + µGµT

G)Sii

(cid:17)

≥ β,

(68)

arg max
β,ΣG

12

with i = 0, · · · , T − 1 and unique solutions for β and ΣG.
Consequently, this optimization problem provides a method for
designing ΣG to maximize the expected value of the Fisher
information matrix for all possible additive integrity attacks
described by (47) and (57).

VI. BOUNDS ON ATTACKER’S PERFORMANCE
We now turn our attention to calculating lower bounds on
the detection statistic associated with optimal attacks on each
of the moving target systems. These bounds characterize the
worst case detection performance while under attack. We ﬁrst
investigate lower bounds on the attacker’s state estimation and
use these bounds to understand how well an adversary can fool
the bad data detector.

A. Attack Strategy

We consider an attack strategy where the adversary aims to
track the system operator’s state estimate ˆ¯xk|k−1. By tracking
the system operator’s state estimate, the adversary attempts to
generate stealthy outputs. We assume the adversary has full
knowledge of the nominal static system model, is able to read
and modify all the control inputs and all the sensor outputs,
and knows the probability density function (pdf) of the random
matrices and the noise. Without loss of generality, we assume
that the attack begins at k = 0. For the hybrid moving target,
the attacker’s observations and strategy are formulated as

(cid:21)

(cid:20)

xA
k+1
ˆxk+1|k

=

(cid:20)Ak

0

(cid:21) (cid:20)

0 Ak(I − KkCk)
(cid:20)Bk Bk
Bk

0 AkKk

0

+

(cid:21) (cid:34)uk
ua
k
ya
k

(cid:21)

xA
k
ˆxk|k−1
(cid:35)

(cid:20)wk
0

+

(69)

(cid:21)

,

yA
k = [Ck

(cid:21)

(cid:20)

0]

xA
k
ˆxk|k−1

+vk, da

k = E

(cid:104)

Ck ˆxk|k−1

(cid:105)

(cid:12)
(cid:12)I A
(cid:12)

k

−yA

k , (70)

where xA
represents the attacked states for the nominal
k
system, yA
k denotes the sensor measurements for the nominal
system that the attacker intercepts, ya
k represents the biased
sensor measurements for the nominal system received by the
system operator, and ua
k = 0. For the extended moving target,
the attack strategy is the same as (69) and (70) except that
xA
k , ˆxk|k−1, yA
k, wk, vk, Ak, Bk, Ck, and Kk are
replaced by ¯xA
k, ¯wk, ¯vk, Ak, Bk, Ck, and
Kk, respectively. For the nonlinear moving target, this attack
strategy is
(cid:20) ¯xA
(cid:21)
k+1
ˆ¯xk+1|k

k, da
k , ya
k , ˆ¯xk|k−1, ¯yA

(cid:21) (cid:20) ¯xA

(cid:20) ¯wk
0

k, ¯da

k , ¯ya

(cid:20)Ak

(71)

=

+

(cid:21)

(cid:21)

0

k
ˆ¯xk|k−1




AkKk

(cid:20)Bk Bk
Bk

+

0

0 Ak(I − KkCk)
(cid:35)
(cid:21) (cid:34)uk
ua
k
¯ya
k
(cid:20)
Gkh(xA
k )
0

0 AkKk

+

(cid:21)

−

(cid:20) ¯xA
k
ˆ¯xk|k−1
(cid:20)

(cid:21)

¯da
k = E

Ck ˆ¯xk|k−1 +

+ ¯vk,
(cid:20)Gkh(ˆxk|k−1)
0

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

I A
k

− ¯yA
k .

0

(cid:20)Gkh(ˆxk|k−1)
0



(cid:21)
 ,

(72)

¯yA
k = [Ck

0]

Here I A

k refers to the information available to the attacker
as presented in section II for the hybrid moving target,
extended moving target, and nonlinear moving target. These
attack strategies are motivated by the following result which
states that for the extended moving target, the chosen sensor
measurement bias ¯da
k minimizes the expected value of the χ2

detection statistic. This illustrates the potential effectiveness
of the attack when countered by a χ2 detector. This result
can easily be extended to account for the sensor biases and
the measurement residues of the hybrid moving target and the
nonlinear moving target.

Theorem 6. Consider a strong adversary who knows
(cid:3),
{Cj, Pj|j−1} for all j ∈ Z. Deﬁning ˆ¯xe
(cid:105)
(cid:104)

(cid:44) E (cid:2)ˆ¯xk|k−1

(cid:12)
(cid:12)I A
k

k|k−1

¯da∗
k (cid:44) arg min

E

gk(¯zk−T +1:k)

= Ck ˆ¯xe

k|k−1 − ¯yA
k .

(73)

(cid:12)
(cid:12)I A
(cid:12)

k

¯da
k

Proof. Observe that

(cid:104)
gk(¯zk−T +1:k)

E

(cid:105)

(cid:12)
(cid:12)I A
(cid:12)

k

=

(cid:90)

k
(cid:88)

ϑk

i=k−T +1

i Σ−1
¯zT

i ¯zif (ϑk|I A

k )dϑk, (74)

+

1
2

Tr

k+1, ω(j)

{x(j)
using the following approximation

j=1. As shown in [29], I D

k }L

k+1 can be computed

13

I D
k+1 ≈

L
(cid:88)

j=1

ω(j)
k J S

k+1(x(j)

k+1),

(79)

where J S
with element (m, n) given by

k+1(x(j)

k+1) is the standard Fisher information matrix

J S

k+1(m, n) =

Σ(x(j)

∂µ(x(j)
k+1)T
∂x(j)
k+1(m)
k+1)−1 ∂Σ(x(j)
k+1)
∂x(j)
k+1(m)

k+1)−1 ∂µ(x(j)
k+1)
∂x(j)
k+1(n)
k+1)−1 ∂Σ(x(j)
k+1)
∂x(j)
k+1(n)

Σ(x(j)

(cid:32)

Σ(x(j)

(80)

(cid:33)

,

k+1|x(j)

where p(¯yA
k+1)). Using a sim-
ple Gaussian approximation for the prediction distribution
p(xk+1|¯yA

k+1) ∼ N (µ(x(j)
0:k) ≈ N (µk, ¯Σk) with

k+1), Σ(x(j)

(75)

¯Σk =

L
(cid:88)

j=1

ω(j)
k (x(j)

k+1 − µk)(x(j)

k+1 − µk)T , µk =

L
(cid:88)

j=1

ω(j)
k x(j)

k+1, (81)

i + ¯da

where ¯zi = ¯yA
Taking the gradient with respect to ¯da
expression equal to 0, we obtain

i −Ci ˆ¯xi|i−1 and ϑk (cid:44) ˆ¯x[k−T +1|k−T ]:[k|k−1].
k and setting the resulting

(cid:90)

2Σ−1

k (¯yA

k + ¯da

k − Ck ˆ¯xk|k−1)f (ϑk|I A

k )dϑk = 0.

ϑk
Solving for ¯da

k yields

¯da
k = −¯yA

k + Ck

(cid:90)

ϑk

ˆ¯xk|k−1f (ϑk|I A

k )dϑk,

(76)

and the result holds.

B. Bounds on Attacker’s State Estimation

Given the attack strategies considered in the last section,
we now want to characterize a lower bound Zk on the mean
square error matrix of the attacker’s estimate of ˆ¯xk|k−1. Since
k|k−1 represents the attacker’s estimate of ˆ¯xk|k−1, this lower
ˆ¯xe
bound Zk is given by

(cid:104)
(ˆ¯xe

E

k|k−1 − ˆ¯xk|k−1)(ˆ¯xe

k|k−1 − ˆ¯xk|k−1)T (cid:12)
(cid:12)¯yA
(cid:12)

0:k

(cid:105)

≥ Zk.

(77)

To approximate Zk, we leverage conditional posterior Cramer-
Rao lower bounds for Bayesian sequences. Unlike the tradi-
tional Cramer-Rao lower bound which is limited to unbiased
estimators, the Bayesian Cramer-Rao lower bound considers
both biased and unbiased estimators. Here we propose using
the direct conditional posterior Cramer-Rao lower bound as set
forth in [29] to approximate Zk. The authors here make use
of the Bayesian Cramer-Rao lower bound or Van Trees bound
derived in [30] which states that the mean squared error matrix
is bounded by the inverse of the Fisher information matrix Ik
as follows

≥ I −1
k ,

(78)

(cid:104)
(xe

E

k − xk)(xe

(cid:105)

k − xk)T (cid:12)
(cid:12)¯yA
(cid:12)
(cid:44) (cid:104)
(cid:105)T

0:k

(cid:105)T

(cid:44) (cid:104)

k

k

k|k−1

k|k−1

ˆ¯xeT

¯xAT

¯xeT
where xe
, xk
ˆ¯xT
, and
k
¯xe
k is the attacker’s estimate of ¯xA
k . Zk can be obtained by
simply taking the lower right (n + ˜n) × (n + ˜n) block of
I −1
k . As demonstrated in [29], Ik+1 can be decomposed into
two parts as Ik+1 = I D
k+1 represents the
information gained from the new measurements averaged over
the a priori distribution and I P
k+1 represents the information
contained in the a priori distribution.

k+1, where I D

k+1 + I P

To compute Ik+1, a particle ﬁlter

is used to repre-
the distribution of xk+1 with the weighted particles

sent

I P
k+1 can be approximated by the inverse of the covariance
matrix as demonstrated in [29] so that I P

k+1 ≈ ¯Σ−1
k .

By choosing the importance density of the particle ﬁlter to
k ), the weight update equation derived
k |x(j)
k = ω(j)
k ).

be the prior p(xk+1|x(j)
in [31] simpliﬁes to ω(j)

k−1p(¯yA

A sequential importance sampling algorithm such as that
presented in [31] can be used to implement the particle ﬁlter,
and resampling can be introduced to keep the particle ﬁlter
from degenerating. Other algorithms presented in [31] such
as the auxiliary sampling importance resampling ﬁlter and the
regularized particle ﬁlter can be used to protect the particle
ﬁlter from sample impoverishment, which is severe in the case
of small process noise.

Remark 12. Computing a lower bound for the hybrid moving
target is described by replacing ˆ¯xk|k−1, ˆ¯xe
k, ¯yA
k , ¯xe
k ,
¯da
k, Ck, Pk|k−1, and ¯zk in eqs. (77) to (83) with ˆxk|k−1, ˆxe
k|k−1,
xA
k , xe
k, Ck, Pk|k−1, and zk, respectively, where
ˆxe
k is the attacker’s estimate
k|k−1
of xA
k .

k, yA
k , da
(cid:44) E (cid:2)ˆxk|k−1

k|k−1, ¯xA

(cid:3) and xe

(cid:12)
(cid:12)I A
k

C. Bounds on Detection

The algorithm described above provides a method for com-
puting an approximate lower bound on the mean square error
matrix of the attacker’s estimate of ˆ¯xk|k−1 for a given set of
inputs u0:k, ua
0:k, allowing
us to obtain a lower bound on the expected value of the
χ2 detection statistic. The following result characterizes how
small an attacker is able to make the detection statistic given
the information available to him or her.

0:k and observation history ¯yA

0:k, ¯da

Theorem 7. Consider a strong adversary who knows
{Cj, Pj|j−1} for all j ∈ Z. Suppose a lower bound Zi on the
error matrix of ˆ¯xi|i−1 is obtained for i = {k − T + 1, · · · , k}
as presented in (77). Then we have

(cid:104)

E

min
¯da
k

gk(¯zk−T +1:k)

(cid:105)

(cid:12)
(cid:12)I A
(cid:12)

k

≥

k
(cid:88)

i=k−T +1

Tr(CT

i Σ−1

i CiZi).

(82)

Proof. We have the following.

(cid:104)

E

min
¯da
k





=E

gk(¯zk−T +1:k)

(cid:105)

(cid:12)
(cid:12)IA
(cid:12)

k

k
(cid:88)

(¯yA

i + ¯da∗

i −Ci ˆ¯xi|i−1)T Σ−1

i

(¯yA

i + ¯da∗

i −Ci ˆ¯xi|i−1)

i=k−T +1


k
(cid:88)


E



Tr

i=k−T +1

k
(cid:88)

(cid:16)

(cid:104)

E

Tr

=

≥

i=k−T +1

k
(cid:88)

i=k−T +1

Tr(CT

i Σ−1

i CiZi).

(Ci(ˆ¯xe

i|i−1−ˆ¯xi|i−1))(Ci(ˆ¯xe

i|i−1−ˆ¯xi|i−1))T Σ−1

i

(ˆ¯xe

i|i−1−ˆ¯xi|i−1)(ˆ¯xe

i|i−1−ˆ¯xi|i−1)T (cid:12)

(cid:12)IA
(cid:12)

k


=

(cid:12)
(cid:12)
(cid:12)
IA
(cid:12)
k
(cid:12)
(cid:12)









(cid:12)
(cid:12)
(cid:12)
IA
(cid:12)
k
(cid:12)
(cid:12)

(cid:105)

i Σ−1
CT

i Ci

(cid:17)

(83)

The ﬁrst three equalities follow from Theorem 6 and the
properties of the trace and expectation. The ﬁnal inequality
follows from (77).

Remark 13. In general, the adversary’s ability to estimate
{ˆ¯xk|k−1} is dependent on the inputs {ua
k}. For instance,
the more the adversary biases the state away from its expected
region of operation, the more challenging it is to perform
estimation. Thus if the system operator wants to analyze how
well an adversary can generate stealthy outputs, he or she
k, ¯da
must consider a particular sequence of attack inputs ua
k.

k}, { ¯da

VII. SIMULATION
We validate each moving target design by considering the
quadruple tank process [32], a multivariable laboratory process
that consists of four interconnected water tanks. The goal is
to control the water level of the ﬁrst two tanks using two
pumps. The system has four states (water level for each tank),
two inputs (voltages applied to the pumps), and two outputs
(voltages from level measurement devices for the ﬁrst two
tanks). We use an LQG controller with weights following
suggestions in [33]. To ensure an appropriate noise magnitude,
Q, ˜Q, R, and R are created by generating a matrix from
a uniform distribution, multiplying it by its transpose, and
dividing by 100. A window size of 10 is used for the χ2
detector, ˜A and ˜C are composed of 50% nonzero entries pulled
from a standard normal distribution, and ˜A is stable.

The extended system is comprised of 4 auxiliary states and 2
auxiliary sensors. The auxiliary states can represent the water
level of the tub into which each of the tanks dispense water,
the rate of change in the tub’s water level, and the supply and
dispense rates of water ﬂowing into and out of the tub. The
auxiliary sensors can measure the tub’s water level and the
rate at which the tub is supplied with water. The time-varying
nature of ¯Ak, ˜Bk, and ¯Ck can be achieved by varying the
length and width of the tub over time through the auxiliary
actuators. The matrices ˜A and ˜C mathematically describe the
auxiliary system dynamics. Experiments are averaged over
1000 trials, and simulation results for the hybrid moving target
defense and extended moving target defense can be found in
[23] and [22], respectively.

A. Nonlinear Moving Target Defense

For the nonlinear moving target, we consider nonlinear
functions that take the form of an element-wise power func-
k, c ∈ Z+. We ﬁrst investigate how the power
tion, h(xk) = xc

14

of this nonlinear function affects the amount of information
an adversary receives about the time-varying matrices ¯Ak, ˜Bk,
and ¯Ck through the auxiliary intercepted sensor measurements.
We consider an adversary who starting at time 200 sec. adds
a constant input of 0.2 volts to the optimal LQG input and
avoids detection by trying to subtract his or her own inﬂuence
from the sensor measurements as described in (3). We assume
that the attacker does not know the realizations of ¯Ak, ˜Bk, ¯Ck,
or Gk but performs his or her attack by sampling the matrices
from ˜Bk(row i) ∼ N (µ ˜B, Σ ˜B), ¯Ak(row i) ∼ N (µ ¯A, Σ ¯A),
¯Ck(row i) ∼ N (µ ¯C, Σ ¯C), and Gk(column i) ∼ N (µG, ΣG)
where we have chosen µ ˜B = (cid:126)0, µ ¯A = µ ¯C = (cid:126)1, and µG = (cid:126)0.
We plot the absolute mean tank height deviation in Figure
1a for h(xk) = x2
k where we see the effect of the attacker’s
constant bias on the control inputs. In Figure 1b, we plot the
spectral norm of the attacker’s Fisher information matrix for
a few different nonlinear power functions in addition to the
case when there is no nonlinear function. For these ﬁgures,
we use the optimal covariances Σ∗
G which
˜B
are generated according to the optimization problems in (40),
(46), and (68). Here the means of ¯Ak and ¯Ck are used to design
a time-invariant Σ ˜B, and the positive semideﬁnite bounds are
set to NB = (cid:126)1(cid:126)1T + 0.5I, Nt = tI, ΘA = (cid:126)1(cid:126)1T + 0.5I, ΘC =
(cid:126)1(cid:126)1T + 0.5I, Θi = I, and M = (cid:126)1(cid:126)1T + 0.5I.

¯C, and Σ∗

¯A, Σ∗

, Σ∗

(a) Tank Height Deviations

(b) Norm of Fisher Information Matrix

Fig. 1. For an attacker who adds a constant bias to the control inputs and
subtracts his or her inﬂuence from the sensor measurements, a) absolute mean
tank height deviations and b) spectral norm of the attacker’s Fisher information
matrix for various nonlinear power functions

As seen in Figure 1b, the presence of the nonlinearity in
the auxiliary sensor measurements results in a decrease of
information that the adversary receives about the time-varying
matrices ¯Ak, ˜Bk, and ¯Ck. Furthermore, we see that nonlinear
functions with larger powers (which generally have greater
magnitudes) cause the adversary to receive less information
about the time-varying matrices, consequently making it more
difﬁcult for an adversary to generate stealthy outputs.

We now consider the joint design of the covariances Σ ¯A,
Σ ¯C, and ΣG for the coupling matrices and nonlinear coefﬁ-
cient matrix of the quadruple tank process. The optimal time-
invariant covariance Σ∗
obtained previously is used to gener-
˜B
ate ˜Bk, and we consider the same adversary as previously.

In Figure 2a, we plot the detection statistic for optimal and
unintelligent designs of Σ ¯A, Σ ¯C, and ΣG with h(xk) = x2
k.
The optimal covariances Σ∗
¯A, Σ∗
G are generated
according to the optimization problems in (46) and (68) while
the unintelligent covariances ΣIID
2 I take
IID structures that satisfy all the constraints of (46) according

¯C, and Σ∗

1 I and ΣIID

¯A = ξ∗

¯C = ξ∗

02004006008001000Time (sec)00.511.52Height Deviation (cm)Tank Height DeviationsTank 1Tank 2Tank 3Tank 402004006008001000Time (sec)050010001500200025003000350040002-Norm of Fisher Information MatrixAttacker's Fisher Information Matrixto

arg max
ξ1,ξ2

ξ1 + ξ2

s.t. ξ1I (cid:22) ΘA, ξ2I (cid:22) ΘC ,

Tr(Jii)ξ1I + Tr(Sii)ξ2I + Sum(Jii)µ ¯AµT
¯A
+ Sum(Sii)µ ¯C µT
+ [µ ¯C · · · µ ¯C ] F T

¯C + [µ ¯A · · · µ ¯A] Fii [µ ¯C · · · µ ¯C ]T
ii [µ ¯A · · · µ ¯A]T (cid:23) γ∗Θi,

(84)

where i = 0, · · · , T − 1, ξ1 and ξ2 are nonnegative constants,
and γ∗ is the optimal nonnegative constant obtained from (46).
In both (46) and (84), the positive semideﬁnite bounds are the
same as those used previously. The unintelligent covariance
G = ϕ∗I takes an IID structure satisfying the ﬁrst constraint
ΣIID
of (68) according to

arg max
ϕ

ϕ s.t. ϕI (cid:22) M,

(85)

where ϕ is a nonnegative constant and the positive semideﬁnite
bound is the same as that used previously. Originally (85) was
constructed so that ΣIID
G would satisfy all the constraints in
(68), but this problem proved to be infeasible, implying that
the unintelligent covariance is unable to achieve the chosen
lower bound on the Fisher information matrix.

As seen in Figure 2a, designing Σ ¯A, Σ ¯C, and ΣG according
to (46) and (68) results in a detection statistic that is signiﬁ-
cantly greater than that of a non-optimal design for Σ ¯A, Σ ¯C,
and ΣG. This supports the idea that increasing the amount
of information the defender receives about the attacked states
xA
k through the biased auxiliary sensor measurements ˜ya
k will
result in an increase in detection performance. Even for small
biases on the optimal control input (0.2 volts), the nonlinear
moving target defense with optimal designs of Σ ¯A, Σ ¯C, and
ΣG results in a detection statistic that is far greater than the
detection statistic under normal operation.

(a) Detection Statistic

(b) Norm of Fisher Information Matrix

Fig. 2. For an attacker who adds a constant bias to the control inputs and
subtracts his or her inﬂuence from the sensor measurements, a) detection
statistic for optimal and unintelligent designs of Σ ¯A, Σ ¯C , and ΣG and b)
spectral norm of the Fisher information matrix for optimal and unintelligent
designs of Σ ¯A, Σ ¯C , and ΣG

Figure 2b shows the spectral norm of the Fisher information
matrix for optimal and unintelligent designs of Σ ¯A, Σ ¯C, and
ΣG with h(xk) = x2
k. As seen, designing Σ ¯A, Σ ¯C, and ΣG
according to (46) and (68) results in much more information
being gained from the biased auxiliary sensor measurements
˜ya
k about the attacked states xA
k than if a non-optimal design
were used. Maximizing this amount of information will help
produce a more accurate state estimate regardless of whether
or not the system is under attack.

15

target defense. We consider an adversary who starting at time
200 sec. adds a constant input of 0.3 volts to the optimal LQG
input and avoids detection by trying to subtract his or her own
inﬂuence from the sensor measurements as described in (3).
We assume that the attacker does not know the realizations
of ¯Ak, ˜Bk, or ¯Ck but performs his or her attack by sampling
the matrices from ˜Bk(row i) ∼ N (µ ˜B, Σ ˜B), ¯Ak(row i) ∼
N (µ ¯A, Σ ¯A), and ¯Ck(row i) ∼ N (µ ¯C, Σ ¯C) where we have
chosen µ ˜B = (cid:126)0 and µ ¯A = µ ¯C = (cid:126)1.

We plot the χ2 detection statistic and its associated lower
bound in Figure 3 where we use the optimal covariances Σ∗
,
˜B
Σ∗
¯C generated according to the optimization problems
in (40) and (46). Here the means of ¯Ak and ¯Ck are used
to design a time-invariant Σ ˜B, and the positive semideﬁnite
bounds are set to NB = (cid:126)1(cid:126)1T + 0.5I, Nt = tI, ΘA = (cid:126)1(cid:126)1T +
0.5I, ΘC = (cid:126)1(cid:126)1T + 0.5I, and Θi = I.

¯A, and Σ∗

Fig. 3. χ2 detection statistic and lower bound on the expected value of the
detection statistic for an attacker who adds a constant bias to the control inputs
and subtracts his or her inﬂuence from the sensor measurements

As seen in Figure 3, the magnitude of the lower bound on
the expected value of the detection statistic is much greater
than detection thresholds associated with very small false
implying that on average, any attack on this
alarm rates,
extended moving target system will be detected. These results
demonstrate that the moving target defense is effective even
in worst case attack scenarios.

VIII. CONCLUSION
This article presented the moving target defense for de-
tecting and identifying attacks in CPSs. The moving target
seeks to limit an adversary’s knowledge of the model by
introducing stochastic time-varying parameters in the control
system. We considered the hybrid moving target, the extended
moving target, and the nonlinear moving target, analyzing each
system and providing guidelines for the design of the system
parameters. We demonstrated how the hybrid moving target
enables both detection and identiﬁcation of malicious nodes,
presented designs for the extended moving target that maxi-
mize detection and estimation performance, and showed how
the nonlinear moving target minimizes any information an ad-
versary receives about the time-varying parameters. Lastly, we
investigated lower bounds on the detection statistic, showing
that the moving target defense is able to detect even the most
stealthy attacks. Future work consists of applying the moving
target defense to speciﬁc use cases, investigating where and
how the time-varying parameters might be introduced to take
advantage of the existing system dynamics.

B. Bounds on Attacker’s Performance

REFERENCES

To investigate lower bounds on the detection statistic when
the system is under attack, we consider the extended moving

[1] A. A. Cardenas, S. Amin, and S. Sastry, “Secure control: Towards
survivable cyber-physical systems,” in 28th International Conference on

02004006008001000Time (sec)00.511.522.533.54Detection Statistic104Zero Dynamics Attack02004006008001000Time (sec)1000150020002500300035002-Norm of Fisher Information MatrixFisher Information Matrix02004006008001000Time (sec)0246810Detection Statistic104Zero Dynamics Attack2 StatisticLower BoundDistributed Computing Systems Workshops, 2008. ICDCS’08.
2008, pp. 495–500.

IEEE,

[2] J. Slay and M. Miller, “Lessons learned from the maroochy water
breach,” in International Conference on Critical Infrastructure Protec-
tion. Springer, 2007, pp. 73–82.

[3] T. M. Chen, “Stuxnet, the real start of cyber warfare?[editor’s note],”

IEEE Network, vol. 24, no. 6, pp. 2–3, 2010.

[4] D. U. Case, “Analysis of the cyber attack on the ukrainian power grid,”
Electricity Information Sharing and Analysis Center (E-ISAC), 2016.
[5] Y. Liu, P. Ning, and M. K. Reiter, “False data injection attacks against
state estimation in electric power grids,” ACM Transactions on Informa-
tion and System Security (TISSEC), vol. 14, no. 1, p. 13, 2011.

[6] Y. Mo, E. Garone, A. Casavola, and B. Sinopoli, “False data injection
attacks against state estimation in wireless sensor networks,” in 49th
IEEE Conference on Decision and Control (CDC).
IEEE, 2010, pp.
5967–5972.

[7] R. S. Smith, “Covert misappropriation of networked control systems:
Presenting a feedback structure,” IEEE Control Systems, vol. 35, no. 1,
pp. 82–92, 2015.

[8] F. Pasqualetti, F. D¨orﬂer, and F. Bullo, “Attack detection and identi-
ﬁcation in cyber-physical systems,” IEEE Transactions on Automatic
Control, vol. 58, no. 11, pp. 2715–2729, 2013.

[9] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson, “A se-
cure control framework for resource-limited adversaries,” Automatica,
vol. 51, pp. 135–148, 2015.

[10] Y. Mo and B. Sinopoli, “Secure control against replay attacks,” in
47th Annual Allerton Conference on Communication, Control, and
Computing.

IEEE, 2009, pp. 911–918.

[11] S. Weerakkody, O. Ozel, P. Grifﬁoen, and B. Sinopoli, “Active detection
for exposing intelligent attacks in control systems,” in 2017 IEEE
Conference on Control Technology and Applications (CCTA).
IEEE,
2017, pp. 1306–1312.

[12] Y. Mo, R. Chabukswar, and B. Sinopoli, “Detecting integrity attacks
on scada systems,” IEEE Transactions on Control Systems Technology,
vol. 22, no. 4, pp. 1396–1407, 2014.

[13] Y. Mo, S. Weerakkody, and B. Sinopoli, “Physical authentication of con-
trol systems: Designing watermarked control inputs to detect counterfeit
sensor outputs,” IEEE Control Systems, vol. 35, no. 1, pp. 93–109, 2015.
[14] B. Satchidanandan and P. R. Kumar, “Dynamic watermarking: Active
defense of networked cyber–physical systems,” Proceedings of the IEEE,
vol. 105, no. 2, pp. 219–240, 2017.

[15] S. Weerakkody, O. Ozel, and B. Sinopoli, “A bernoulli-gaussian physical
watermark for detecting integrity attacks in control systems,” in 55th An-
nual Allerton Conference on Communication, Control, and Computing.
IEEE, 2017, pp. 966–973.

[16] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson, “Revealing
stealthy attacks in control systems,” in 50th Annual Allerton Confer-
ence on Communication, Control, and Computing.
IEEE conference
proceedings, 2012, pp. 1806–1813.

[17] S. Weerakkody, X. Liu, and B. Sinopoli, “Robust structural analysis
and design of distributed control systems to prevent zero dynamics
attacks,” in 56th IEEE Annual Conference on Decision and Control
(CDC).

IEEE, 2017, pp. 1356–1361.

[18] F. Miao, Q. Zhu, M. Pajic, and G. J. Pappas, “Coding sensor outputs
for injection attacks detection,” in 53rd Annual Conference on Decision
and Control (CDC).
IEEE, 2014, pp. 5776–5781.

[19] S. Weerakkody and B. Sinopoli, “Detecting integrity attacks on control
systems using a moving target approach,” in 54th IEEE Conference on
Decision and Control (CDC), Dec 2015, pp. 5820–5826.

[20] C. Schellenberger and P. Zhang, “Detection of covert attacks on cyber-
physical systems by extending the system dynamics with an auxiliary
system,” in 56th Annual Conference on Decision and Control (CDC),
Dec 2017, pp. 1374–1379.

[21] S. Weerakkody and B. Sinopoli, “A moving target approach for identi-
fying malicious sensors in control systems,” in 54th Annual Allerton
Conference on Communication, Control, and Computing (Allerton).
IEEE, 2016, pp. 1149–1156.

[22] P. Grifﬁoen, S. Weerakkody, and B. Sinopoli, “An optimal design of a
moving target defense for attack detection in control systems,” in 2019
American Control Conference (ACC), 2019, pp. 4527–4534.
“A moving target

approach
for identifying malicious sensors in control systems,” CoRR, vol.
abs/1609.09043, 2016. [Online]. Available: http://arxiv.org/abs/1609.
09043

[23] S. Weerakkody and B. Sinopoli,

[24] S.-L. Sun and Z.-L. Deng, “Multi-sensor optimal information fusion
kalman ﬁlter,” Automatica, vol. 40, no. 6, pp. 1017–1023, Jun. 2004.
[Online]. Available: http://dx.doi.org/10.1016/j.automatica.2004.01.014
[25] Q. Gan and C. J. Harris, “Comparison of two measurement fusion meth-
ods for kalman-ﬁlter-based multisensor data fusion,” IEEE Transactions
on Aerospace and Electronic Systems, vol. 37, no. 1, pp. 273–279, Jan
2001.

16

[26] L. L. Scharf and C. Demeure, Statistical signal processing: detection,
estimation, and time series analysis. Addison-Wesley Reading, MA,
1991, vol. 63.

[27] H. Fawzi, P. Tabuada, and S. Diggavi, “Secure estimation and control for
cyber-physical systems under adversarial attacks,” IEEE Transactions on
Automatic Control, vol. 59, no. 6, pp. 1454–1467, June 2014.

[28] K. S. Miller, Complex stochastic processes: an introduction to theory
and application. Addison Wesley Publishing Company, 1974.
[29] Y. Zheng, O. Ozdemir, R. Niu, and P. K. Varshney, “New conditional
posterior cramr-rao lower bounds for nonlinear sequential bayesian
estimation,” IEEE Transactions on Signal Processing, vol. 60, no. 10,
pp. 5549–5556, Oct 2012.

[30] H. L. V. Trees, Detection, Estimation, and Modulation Theory. New

York: Wiley, 1968, vol. 1.

[31] M. S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, “A tutorial
on particle ﬁlters for online nonlinear/non-gaussian bayesian tracking,”
IEEE Transactions on Signal Processing, vol. 50, no. 2, pp. 174–188,
Feb 2002.

[32] K. H. Johansson, “The quadruple-tank process: a multivariable labora-
tory process with an adjustable zero,” IEEE Transactions on Control
Systems Technology, vol. 8, no. 3, pp. 456–465, May 2000.

[33] M. Grebeck, A Comparison of Controllers for the Quadruple Tank
System, ser. Technical Reports TFRT-7576. Department of Automatic
Control, Lund Institute of Technology (LTH), 1998.

Paul Grifﬁoen received the B.S. degree in Engineer-
ing, Electrical/Computer concentration, from Calvin
College, Grand Rapids, MI, USA in 2016 and the
M.S. degree in Electrical and Computer Engineering
from Carnegie Mellon University, Pittsburgh, PA,
USA in 2018. He is currently pursuing the Ph.D.
degree in Electrical and Computer Engineering at
Carnegie Mellon University. His research interests
include the modeling, analysis, and design of active
detection techniques and resilient mechanisms for
secure cyber-physical systems.

Sean Weerakkody received the B.S. degree in
Electrical Engineering and Mathematics from the
University of Maryland, College Park, USA, in 2012
and the Ph.D. degree in Electrical and Computer
Engineering from Carnegie Mellon University, Pitts-
burgh PA, USA,
in 2018. He was awarded the
National Defense Science and Engineering Graduate
fellowship in 2014 and the Siebel Scholarship in En-
ergy Science in 2018. His research interests include
secure design and active detection in cyber-physical
systems and estimation in sensor networks.

Bruno Sinopoli received the Dr. Eng. degree from
the University of Padova in 1998 and his M.S. and
Ph.D. in Electrical Engineering from the University
in 2003 and 2005 re-
of California at Berkeley,
spectively. After a postdoctoral position at Stanford
University, Dr. Sinopoli was the faculty at Carnegie
Mellon University from 2007 to 2019, where he was
full professor in the Department of Electrical and
Computer Engineering with courtesy appointments
in Mechanical Engineering and in the Robotics In-
stitute and co-director of the Smart Infrastructure
Institute, a research center aimed at advancing innovation in the modeling
analysis and design of smart infrastructure. In 2019 Dr. Sinopoli
joined
Washington University in Saint Louis, where he is the chair of the Electrical
and Systems Engineering department. Dr. Sinopoli was awarded the 2006 Eli
Jury Award for outstanding research achievement in the areas of systems,
communications, control and signal processing at U.C. Berkeley, the 2010
George Tallman Ladd Research Award from Carnegie Mellon University and
the NSF Career award in 2010. His research interests include the modeling,
analysis and design of Secure by Design Cyber-Physical Systems with
applications to Energy Systems, Interdependent Infrastructures and Internet
of Things.

