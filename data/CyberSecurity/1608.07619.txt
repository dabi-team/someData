Interacting with Massive Behavioral Data

Shih-Chieh Su
Qualcomm Inc.
5775 Morehouse Drive
San Diego, CA 92121 USA
shihchie@qualcomm.com

6
1
0
2

g
u
A
6
2

]

G
L
.
s
c
[

1
v
9
1
6
7
0
.
8
0
6
1
:
v
i
X
r
a

ABSTRACT
In this short paper, we propose the split-diﬀuse (SD) algo-
rithm that takes the output of an existing word embedding
algorithm, and distributes the data points uniformly across
the visualization space. The result improves the perceivabil-
ity and the interactability by the human.

We apply the SD algorithm to analyze the user behavior
through access logs within the cyber security domain. The
result, named the topic grids, is a set of grids on various
topics generated from the logs. On the same set of grids,
diﬀerent behavioral metrics can be shown on diﬀerent tar-
gets over diﬀerent periods of time, to provide visualization
and interaction to the human experts.

Analysis, investigation, and other types of interaction can
be performed on the topic grids more eﬃciently than on the
output of existing dimension reduction methods. In addition
to the cyber security domain, the topic grids can be further
applied to other domains like e-commerce, credit card trans-
action, customer service to analyze the behavior in a large
scale.

CCS Concepts
•Human-centered computing → Visual analytics; In-
formation visualization;

Keywords
data visualization, human interaction, dimension reduction,
risk management

1.

INTRODUCTION

When there are multiple measures of the each sample,
the data is described in the a high dimensional space H
by these measures. To make these high dimensional data
points visible to human, a word embedding (or dimension
reduction) technique is employed to map the data points to
a lower dimensional space L. Usually L is a two-dimensional
(2D) or three-dimensional (3D) space. The word embedding

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).

KDD 2016 Workshop on Interactive Data Exploration and Analytics

(IDEA’16) August 14th, 2016, San Francisco, CA, USA.
© 2016 Copyright held by the owner/author(s).

technique of choice attempts to preserve some relationship
among the data points in H after mapping them to L.

For example, the multi-dimensional scaling (MDS) [5] M
tries to preserve the distance between data points, during
the mapping from H to L. The stochastic neighbor em-
bedding (SNE) [6] type of algorithms further emphasize the
local relationship ahead of the global relationship. There are
other dimension reduction techniques putting emphasis on
diﬀerent favored metrics over relationship. On a speciﬁc sit-
uation, one particular dimension reduction technique could
be more suitable or more eﬃcient than others.

The output from existing dimension reduction algorithm
is a set of data points that are non-uniformly scattered
around the visualization space, which has some drawbacks:

1. Some data points may overlap with others. Overlap

makes the information less perceivable.

2. The data points are denser in some area. The hetero-
geneity makes human interaction with the data points
more diﬃcult.

2. METHODS

In order to better utilize the visualization space, we pro-
posed to distribute the data points evenly over the visual-
ization space. The cloud of data points is deformed in the
same space deﬁned by the dimension reduction algorithm of
choice. This deformation is denoted as S. In the meanwhile,
it is desirable to preserve the point-wise relationship main-
tained by the dimension reduction algorithm. Our strategy
in approaching this goal is prioritized as follows:

1. Points are equally spaced after the mapping S.

2. Point-wise topology is preserved. S attempts to keep
point pj on the same side of point pi as before the
mapping.

3. Point-wise geometry is loosely followed. When pi is far

from pj, S(pi) is far from S(pj).

The algorithm we propose is called the split-diﬀuse (SD)
algorithm (Algorithm 1), which follows the strategies above.
In our implementation, the SD algorithm ﬁrst picks the x-
axis as the dimension to split. As in Figure 2 (a), it splits
the data points into two groups: the ones smaller than or
equal to the median, and the ones larger than the median.
Each group goes through this split step again over the y-
dimension, as in Figure 2 (b). We recursively split the points
in x- and y-dimension iteratively, until there is only one
point in current recursion.

 
 
 
 
 
 
(a) 2D t-SNE output

(b) 2D MDS output

(c) 3D t-SNE output

(d) 3D MDS output

(e) SD output from (a)

(f) SD output from (b)

(g) SD output from (c)

(h) SD output from (d)

Figure 1: The split-diﬀuse (SD) algorithm takes the output of any dimension reduction technique and dis-
tributes the data points evenly while maintaining the topology among them. Example inputs of 64 data
points from (a) 2D t-SNE, (b) 2D MDS, (c) 3D t-SNE and (d) 3D MDS are distributed evenly in the same
space as shown in (e)-(h) respectively.

Algorithm 1 Split-diﬀuse algorithm (square of power of 2)
Input: data points {p} of length 2h × 2h, depth d = 0,
allocation string c = ''
split-diﬀuse ({p}, d, c)
k ← length of {p}
if k = 1, then

resolve S(p) from c
return p

end if
a ← mod(depth, 2)
m ← median of {p} in the dimension a
return

([split-diﬀuse ({p : p ≤ m|dim=a}, d+1, c+'L')],
[split-diﬀuse ({p : p > m|dim=a}, d+1, c+'R')])

(a) ﬁrst-level split

(b) second-level splits

Figure 2: The split-diﬀuse algorithm over the 4 × 4
layout.

We keep track of the splitting path in string c. At the
end of the recursion, the placement each single point p is
resolved. The indexes of the SD-mapped points, S(p), are
all integers, and forms a 2h × 2h array. This means that the
mapped data points are equally spaced in a 2h × 2h square.
To achieve this uniformity in the space L, the data points
are essentially diﬀused from the denser area to the coarser
area by the SD algorithm — hence the name split-diﬀuse.

Some sample outputs from existing dimension reduction
techniques are shown in Figure 1, as well as the correspond-
ing SD outputs. Although we only present the results from
t-SNE and MDS, the SD algorithm can be applied to out-
puts of other techniques such as the principal component
analysis (PCA) [3], isomap [4], spectral embedding [1], and
totally random trees embedding [2].

3.

INTERACTING WITH THE DATA

The motivation to better utilize the visual space comes

from the need to interact with massive amount of data. Con-
sider the case that there are millions of items being shipped
to a city every month. How can we easily observe the dif-
ference in the monthly shipping patterns? When vectoring
each item as a data point, putting all the data points on a
chart makes the chart hard to read. Instead, using clustering
algorithms to group the points and showing the representa-
tives is a better way to present the shipping pattern. Still,
with the existing dimension reduction techniques (Figure 1
(a)-(d)), it is diﬃcult to visually compare the diﬀerence and
interact with the representative points for more detail.

In our use case, we apply the SD algorithm to help ana-
lyzing behavioral content in the cyber security domain. The
goal of the system is to detect behavioral anomaly based
on the access logs. Topics are generated on the content of
the logs in a word vector space of 19K+ dimensions. MDS
is applied to reduce the dimension. As shown in Figure 1
(b) and (f), topics are represented by the most relevant key-
words, encrypted. Topics close to each other may share the

(a) current activities of
an entity

(b) historical activities
of this entity

(c) risk against the his-
torical activities of this
entity

(d) historical activities
of the peers

(e) risk against the his-
torical activities of the
peers

Figure 3: The topic grids. The self risk in (c) is derived from comparing the current activities (a) and
the historical activities (b) of a speciﬁc entity. The peer risk in (e) is derived from comparing the current
activities (a) and the peers’ activities (d) of a speciﬁc entity.

same representative keyword. The SD algorithm follows to
generate the topic grids and visualize diﬀerent metrics about
the behavior of a user (Figure 3).

When not directly displaying the detail keywords about a
topic, the topic grids requires less space. At the same time,
the human expert still can easily keep track of the topics
based on their indexes over all dimensions and compare the
diﬀerence between diﬀerent sets of topic grids. Human inter-
action, which is the ultimate goal of the uniform placement
of the data points, can be done more easily on the topic grids
than on the raw dimension reduction output as in Figure 1
(a)-(d). For example, the mouse over event on a grid pops
up the topical summary, and the click event to overlay the
detailed topical activities.

It is also useful to monitor the behavior change over time.
In such cases, we reserve a dimension in L as the time axis.
For a 2D space L, a 1D version of SD algorithm is applied to
maintain the point-wise topology. The cumulative activities
have a shape of curtain. Meanwhile, we can pile up the
2D topic grids on the time axis over the 3D L, as shown in
Figure 4. With normal or usual behavior, it is expected to
see the consistent hot grids at the same locations over time.

4. FUTURE WORK

In addition to the cyber security domain, the topic grids
can be applied to other domains having free-form text logs
to analyze the behavior described by the logs. Some pos-
sible use cases include e-commerce, credit card transaction,
customer service, or others with large volume of behavioral
data to be analyzed.

It is also possible to apply the topic grids to the struc-
tured data, on which an arbitrary clustering algorithm can
generate cluster centers. The data points are then organized
into these cluster centers, the same way we use the topic to
represent the log entries related to it.

5. REFERENCES

[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for
dimensionality reduction and data representation.
Neural computation, 15(6):1373–1396, 2003.

[2] P. Geurts, D. Ernst, and L. Wehenkel. Extremely

randomized trees. Machine Learning, 63(1):3–42, 2006.
[3] K. Pearson. On lines and planes of closest ﬁt to systems
of points in space. The London, Edinburgh, and Dublin

Philosophical Magazine and Journal of Science,
2(11):559–572, 1901.

[4] J. B. Tenenbaum, V. D. Silva, and J. C. Langford. A

global geometric framework for nonlinear
dimensionality reduction. Science,
290(5500):2319–2323, 2000.

[5] W. S. Torgerson. Multidimensional scaling: I. theory
and method. Psychometrika, 17(4):401–419, 1952.
[6] L. Van der Maaten and G. E. Hinton. Visualizing data
using t-SNE. Journal of Machine Learning Research,
9(2579-2605):85, 2008.

(a) Topic curtain

(b) Topic shower

Figure 4: Other formats of the topic grids

