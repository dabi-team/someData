2
2
0
2

n
u
J

3
2

]

R
C
.
s
c
[

1
v
4
2
4
0
0
.
7
0
2
2
:
v
i
X
r
a

LBDMIDS: LSTM Based Deep Learning Model for
Intrusion Detection Systems for IoT Networks

Kumar Saurabh1, Saksham Sood1, P. Aditya Kumar1, Uphar Singh1, Ranjana Vyas1, O.P. Vyas1, Rahamatullah
Khondoker2
1Indian Institute of Information Technology, Allahabad, India
2Department of Business Informatics, THM University of Applied Sciences, Friedberg, Germany
pwc2017001@iiita.ac.in, iit2019164@iiita.ac.in, iit2019165@iiita.ac.in, pse2017003@iiita.ac.in,
ranjana@iiita.ac.in, opvyas@iiita.ac.in, rahamatullah.khondoker@mnd.thm.de

Abstract—In the recent years, we have witnessed a huge growth
in the number of Internet of Things (IoT) and edge devices being
used in our everyday activities. This demands the security of
these devices from cyber attacks to be improved to protect its
users. For years, Machine Learning (ML) techniques have been
used to develop Network Intrusion Detection Systems (NIDS)
with the aim of increasing their reliability/robustness. Among the
earlier ML techniques DT performed well. In the recent years,
Deep Learning (DL) techniques have been used in an attempt
to build more reliable systems. In this paper, a Deep Learning
enabled Long Short Term Memory (LSTM) Autoencoder and a
13-feature Deep Neural Network (DNN) models were developed
which performed a lot better in terms of accuracy on UNSW-
NB15 and Bot-IoT datsets. Hence we proposed LBDMIDS, where
we developed NIDS models based on variants of LSTMs namely,
stacked LSTM and bidirectional LSTM and validated their
performance on the UNSW NB15 and BoTIoT datasets. This
paper concludes that these variants in LBDMIDS outperform
classic ML techniques and perform similarly to the DNN models
that have been suggested in the past.

Index Terms—IoT Security, Intrusion, IDS, LSTM, Deep

Learning

I. INTRODUCTION

Internet of Things (IoT) is a collection of devices which
gather and share data over the Internet. Data is gathered using
sensors, which are embedded in these devices. IoT devices
are used in many areas, ranging from household devices like
smart watches, smart bulbs, smart air conditioners, temperature
sensors to more complex devices like smart vehicles and smart
electrical grids. They are also extensively used in manufac-
turing, transportation, infrastructure, military equipment and
healthcare. It was estimated that there are over 35 billions
IoT devices would be in use upto 2021 [1]. This high number
has contributed to increasing number of cyber attacks on IoT
networks, which demands the security of these devices from
cyber attacks to be increased as the current security measures
have proven to be inadequate [2].

Network Intrusion Detection Systems (NIDS) are used to
detect cyber attacks and malicious activities like Denial of
Service (DoS), Distributed DoS (DDoS), Worms, Backdoor,
etc. Network trafﬁc is monitored and potential threats are
identiﬁed. Signature-based NIDS is good at detecting known
attacks but fail at detecting attacks which have not been seen
before. Anomaly-based NIDS is good at detecting new attacks

978-1-6654-8453-4/22/$31.00 ©2022 IEEE

as they focus more on attack patterns but give high false
positives.

Therefore, NIDS integrated with ML and DL techniques
have been developed to better identify new threats [3]. Earlier,
ML and DL methods have been applied unanimously to
develop systems capable of detecting intrusion in the networks
for conventional and extensive communication. However, ev-
ery method came with limitations of its own and gradually
the need for a better system increased as the classical models
failed in terms of handling the heterogeneity of data. Also,
the dataset used here should have multiple types of attack
vectors (like DoS, DDoS, Worms) to tackle multi-classiﬁcation
of attack categories. Traditional models are unable to detect
zero-day attacks as the dataset is unable to hold the entries
which are new to the attack analysis.

So, a smart model is needed which can detect any anomaly
which rises from any deviation from normal behavior of the
associated network. So, this hypotheses could also detect zero-
day attack as the model will not solely depend on the pre-
built classes of attacks. However, this will again give rise to a
problem which will bias towards the general class. This will
lead to raised false positive rates. To get an intermediate model
which can act more sustainable, DL needs to be implemented.
Classic ML techniques have been used in this ﬁeld for 20+
years while keeping the KDD99 dataset in consideration. But,
with the onset of technological boom, the attack categories
are increasing spontaneously. So, the potential of any classic
ML model is far smaller than the reach of the intrusion. So,
methods like SVM (Support Vector Machine), DT (Decision
Tree), KNN (K-Nearest Neighbor), etc. have been ruled out
long before when considering for industrial applications. Also,
the hybrid method using these models has worked well for a
long time, but eventually falls short in front of recent DL
models. The basis for working of an ML model is basically
supervised and unsupervised learning techniques other than
the reinforcement learning part. ML extensively depends upon
how rich the dataset is available to us. Also, the inability to
scale the data accordingly also poses a large limitation to the
extensive use of any ML model.

To handle such issues, DL methods like ANN (Artiﬁcial
Neural Networks) came into picture. Here the model is built
on neurons which are coordinated by parameters and hyper-

 
 
 
 
 
 
parameters. To scale the input and use it on extensive scale,
the number of layers are kept accordingly to get maximum
efﬁciency. DL methods have proved far better than the ML
models in terms of accuracy, precision, etc. with the ability
to handle large amounts of data. ANN, CNN (Convolutional
Neural Networks), RNN (Recurrent Neural Networks), FDN
(Feed Forward Deep Networks) are many examples of DL
architectures. DL techniques in general have outperformed ML
techniques.

As research progressed, it was seen that basic DL archi-
tectures lacked the ability to detect unknown attacks and
even if they did, the false positives and false negatives were
very high. To tackle this problem, LSTM (Long Short-Term
Memory) was introduced. This methodology is a special form
of RNN. The most distinguishing feature of LSTM is to keep
the information/parameter for later use in the system. Thus,
they can handle the data which is time series and could be
variable with respect to time.

The primary motivation of using LSTM lies in its approach
where is is not restricted to the limitations of conventional
DL (neural networks) methods. Here, the input sequences and
output sequences between layers are variable which could ac-
cordingly work efﬁciently to detect known as well as unknown
attacks.

The paper is organized as follows: Section II contains
related research which demonstrates the earlier work done,
Section III describes the datasets used in our experiments,
Section IV includes our proposed methodology, Section V
contains the results and performance analysis, and Section VI
gives the conclusion and future works.

II. LITERATURE REVIEW

In paper [4], IDS was developed to learn the behavior
of normal network trafﬁc. Dataset used was UNSW-NB 15
for communication of external networks. The methods being
experimented here were Artiﬁcial Immune System (AIS),
Filtered-based SVM (FSVM), Euclidean distance Map (EDM),
and Geometric Area Analysis (GAA) which performed 85%,
92%, 90% and 93% respectively. In [5], BoT-IoT and UNSW-
NB15 datasets are considered in the experiment. After the data
standardization is done, the MLP (Multi-Layered Perceptron)
model is used followed by adapting the hyperparamters. The
comparison was based on the BoT-IoT dataset while the
classiﬁer was built on UNSW-NB15 dataset. ARM (Associ-
ation Rule Mining) and Na¨ıve Bayes only had 85% and 72%
accuracy respectively. The normal perceptron model was basic
and gave 63% accuracy but when converted to the prima facie
13- feature DNN model, it gave 99% accuracy. Among the ML
techniques, Decision tree gave the highest accuracy (93%).
The process of training the models consumes a lot of time
and memory [5]. In paper [6], the authors explore ML based
approach. On carrying out an experiment it was seen that
the ML methods along with ﬂow identiﬁers were effective
in detecting botnet attacks. Four ML algorithms were used
namely, ARM, Artiﬁcial Neural Network (ANN), Na¨ıve Bayes
and Decision Tree. Decision Tree gave the best results with
the highest accuracy of 93.23% and the lowest False Alarm
Rate of 6.77% whereas ANN was the least accurate with an

2

accuracy of 63.97% and False Alarm Rates of 36.03%. In [7],
the authors have addressed the issue of lack of a data set which
can appropriately show the modern network trafﬁc and attacks.
So this paper looks into the creation of a UNSW-NB15 dataset.
It has 49 features developed using Argus and Bro-IDS tools.
In paper [8], the authors proposed a hybrid IDS for IoT net-
works by combining CNNs (Convolutional Neural Network)
and LSTM. They used UNSW NB-15 dataset and compared
the performances of the hybrid IDS and RNN for binary
classiﬁcation, and the accuracy achieves was 95.7% and 98.7%
respectively. In [9], the authors compared the performances of
DL methods like RNN, GRU and Text-CNN with some of the
traditional ML methods on the KD99 and ADFA-LD datasets.
The DL methods were able to outperform the traditional ML
techniques. In paper [10], the authors implemented Linear
Discriminant Analysis (LDA), Classiﬁcation and Regression
Tree (CART) and Random Forest (RF) on KD99 dataset. The
accuracy achieved were 98.1%, 98% and 99.65% respectively.
the authors implemented a Feed-Forward Neural
In [11],
Network for binary and multi-class classiﬁcation on the Bot-
IoT dataset for normal and three different attack categories
(DDoS/DoS, Reconnaissance, Information Theft). They were
able to achieve 98%, 99.4%, 98.4% and 88.9% accuracy on
the normal and the three attack categories respectively. In
paper [12], the authors proposed an IDS based on blockchain
and deep learning. The DNN model was able to achieve 98%
accuracy for binary classiﬁcation and 97% accuracy for multi-
class classiﬁcation on the NSL-KDD dataset.

III. DATASET EXPLANATION
In order to train our models and to determine their reliability
in testing phase, Data selection is necessary. In the past
datasets like KDD98, KDDCUP99 and NSLKDD were used
as comprehensive datasets for Network Intrusion Detection
System(NIDS). In the recent times the researches have shown
that the these datasets don’t reﬂect the modern network trafﬁc
(normal and attack vectors). Hence, we selected a few datasets
that were made publicly available by researchers in the last few
years. These datasets contained labelled network data that were
generated in labs with the help of virtual network setup. The
datasets are a hybrid of Normal network trafﬁc and synthetic
botnet attack trafﬁc. The datasets selected are:

A. UNSW-NB15 Dataset

Widely in use since 2015 as a dataset for evaluating NIDS, it
was created by Cyber Range Lab of UNSW-Canberra [14-17].
The Dataset contains 49 (excluding class labels) features and a
total 2,540,044 records that were split across 4 CSV ﬁles. The
dataset contains a total of 10 class labels out of which there
are 9 types of Botnet attack vectors and 1 Normal class. The
different types of attack trafﬁc are as follows: analysis, fuzzers,
backdoor, generic, Denial of Service (DoS), reconnaissance,
shellcode, exploits and worms [13]. Out of the 49 features 13
important ones were selected in the paper [4], namely source
ip address, source port, destination ip address, destination
port, duration, source bytes, destination bytes, source TTL,
destination TTL, Source load, Destination load, Source packets
and Destination packets.

3

Fig. 1. Proposed LBDMIDS architecture

B. Bot-Iot Dataset

Created by the Cyber Range Lab of UNSW-Canberra in
2019, it includes network and botnet trafﬁc incorporated into
a network environment. The Dataset contains 47 features
including class label, attack category and attack subcategory
[18]. A total of 72 million records of Normal and attack trafﬁc
constitute the Dataset. A pre-selected training and testing
dataset which includes 10-best features was created which
have been used for training and validation purposes. The train-
ing and testing dataset have a combined of 3.6 million records.
The Dataset includes attack vectors of types DDoS, DoS, OS
and service scan, keylogging and Data exﬁltration attacks. On
the basis of protocol used DDoS and Dos categories are further
divided into different types.

IV. PROPOSED METHODOLOGY

A. Working Principles

Let us understand the working principles behind our project.
We are using two variants of LSTM in our research, Stacked
let us see Feed Forward Neural
and Bidirectional. First,
Networks and Recurrent Neural Networks.

1) Feed Forward Neural Networks: This model consists
of input layer and output layer with several hidden layers in
between. This model is generally designed to contain three or
more hidden layers. The layers consist of biases and weights.
The input data is injected into the ﬁrst layer and consequently
the output is generated which becomes the input for the next
layer. The number of nodes are adjusted so that they match
the number of features in input data. The weights and biases
used in the layers are initialised randomly but later optimised
through a method known as back-propagation.

Important note here is that the model is “one-way” and there
are no backward connections here. The data computation ﬂows
in one direction where any input generates an output where
there is no connection such that the output could be injected
back to the model. The absence of backward connections give
rise to limitations such as the ”error-sum” problem where
the error generated in one layer keeps on increasing in the
consequent layers. As there is no option of taking the feedback
from the output, the input and output become independent
which makes the tuning of hyper-parameters by the model an
un-achievable task.

2) Recurrent Neural Networks: RNN is a type of network
normally distinguished by “memory”. The layers present in
the model usually take account of the previous inputs while
considering the input of a certain layer. While traditional
neural networks work on the principle that input and output
are generated independently, RNN works on the method of
feeding the output back into the system which generally works
in favor of reducing the error and balancing the model.

Another variation which makes RNN different from FFNN
(Feed Forward Neural Networks) is the weights and parame-
ters associated with the layers of network. Each node in FFNN
uses different weights which are adjusted through the pro-
cesses of gradient descent and back-propagation. In RNN, the
parameters are shared between the layers of network, although
the parameters are still adjusted by the same processes used
in FFNN (gradient descent and back-propagation).

While taking the “feedbacks” from the succeeding layers,
RNN is able to adjust its parameters by analysing the errors
encountered by the model. This works well in maintaining the
error percentage to a threshold and also ﬁts the model better.
Overcoming the problem faced by FFNN, RNN faces two
other limitations. These are known as vanishing gradient and
exploding gradient problem. These problems generally depend
upon the function of error in the model. Here, gradient is
deﬁned as the slope of the error function. If the slope becomes
too low, then it will continue to decrease as the threshold keeps
decreasing. This will give rise to non-learning of the model.
The parameters in the model will become negligible and the
memory of model will saturate. On the other hand, when slope
becomes too high, the model will become unstable and the data
fed to the model will generate ﬂuctuating outputs which will
in turn render the model parameters too large to be considered.
3) Long Short-Term Memory: LSTMs are a special variant
of RNN. These are particularly used in learning long-term
dependencies. RNNs fail in learning information when the
dataset is large and the entries have much similarity. So, LSTM
was designed to handle information for longer feeds of input
and work for larger epochs.

As the layers of RNN have a very simple structure, for
example a single Tanh layer [19], remembering information
for a long period of time becomes a struggling task for them.
On the other hand, in a general LSTM model, there are four

interacting layers which work in a special way to remember
the important information for a long duration of time.

The problem of vanishing gradient faced by RNN is also
solved by LSTM as they continue to learn new information
while keeping the previous ones. Hence the signiﬁcance of
parameters is maintained and the model becomes stable.

LSTMs also have different variations. Although the dif-
ference is pretty small between those,
they could be of
great signiﬁcance depending upon the input data fed to the
network. Common types of LSTM are Classic, Stacked and
Bidirectional. Stacked LSTM has several LSTM layers and
can only access past samples. Bi-directional LSTM has two
layers and has access to both past and future samples.

B. Network Intrusion Detection System(NIDS) Architecture

We proposed NIDS based on two variants of LSTM -
Stacked LSTM and Bi-directional LSTM to help ﬁlter out
normal and attack vectors in IoT network trafﬁc. The workﬂow
of the NIDS architecture can be divided into three phases,
namely the data Preprocessing, Training of the Model and
Model Validation as shown in Fig.1.

C. Data Preprocessing

In Data Preprocessing stage, the raw data extracted from
IoT networks in form of PCAP/CSV ﬁles is processed in a
suitable form to be fed to the LSTM Model. The data is
ﬁltered to remove any redundancies and get rid of Null values.
Afterwards the most important features are selected to be fed
into the proposed model, which is then followed by z-score
Normalization of feature which ensures similar distributions
for each feature.

1) Feature Extraction: We managed to reduce the Dimen-
sionality of the raw data by selecting the most important
features which made the data suitable for processing. Very
often, large Datasets have a lot of redundant and correlated
data that can be ﬁltered out without
losing important or
relevant information. In case of USNW NB15, we selected
the following most important 13 features [1]:

1) Source ip address - IP address of the attacker computer
2) Source port - Port number of the attacker computer
3) Destination ip address - IP address of the victim com-

puter

4) Destination port - Port number of the victim computer
5) Duration - Record total duration of transaction
6) Source bytes - Number of bytes sent from source to the

destination

7) Destination bytes - Number of bytes sent from destina-

tion to the source

8) Source TTL - Source to destination time to live value
9) Destination TTL - Destination to source time to live

value

10) Source load - Transmission rate in bits per second
11) Destination load - Reception rate in bits per second
12) Source packets - Number of packets sent from source to

the destination

13) Destination packets - Number of packets sent from

destination to the source

4

For Bot-IoT dataset, the 10 features that were pre-selected

in the Reduced training and testing set were:

1) rate - Total packets per second in transaction
2) srate - Source-to-destination packets per second
3) drate - Destination-to-source packets per second
4) min - Minimum duration of aggregated records
5) max - Maximum duration of aggregated records
6) mean - Average duration of aggregated records
7) std dev - Standard deviation of aggregated records
8) state number - Numerical representation of feature state
9) ﬂgs number - Numerical representation of feature ﬂags
10) seq - Argus sequence number
2) z-scale Normalization: The process of normalization is
done to transform the data in a way they are distributed
similarly. It helps the model to treat each feature with similar
importance as it provides similar weights to each feature.
Assuming the feature subspace has N rows and M columns
i.e., X (feature subspace) = RN ×M , the z-scale normalization
can be implemented as follows:

µm =

σm =

(cid:80)N −1

i=0 xim
N
(cid:80)N −1

i=0 (xim − µm)2
N

z-scale normalized feature vector can be obtained as

zm =

Xm − µm
σm

(1)

(2)

(3)

Here, µm is the mean of the entries of the m’th column and
σm is the standard deviation of the entries of the m’th column.

Algorithm 1 Z-SCALE NORMALIZATION

for each col m in X(0,1,2...M-1)

µm ← compute(1)
σm ← compute(2)
zm ← compute(3)

end for.

D. Training and Validation Process

Data Processing stage transforms the data into a more
suitable form to be processed by the model which will lead to
more accurate predictions. It is followed by changing the shape
of the data to be processed by the LSTM layers. A suitable
Timesteps parameter is selected and the dimensionality of the
dataset is changed(samples, timesteps, features). Timesteps
is the number of past samples on which the LSTM model
looks back at. The training and validation period of the model
consists of feeding the time-series sequential data to the LSTM
layers.

1) Stacked LSTM:

In case of stacked LSTM, there are
multiple layers of LSTM stacked on top of one another. The
LSTM layers help in uncovering the patterns and dependence
of features to their class labels because they can learn at higher
levels of abstractions. The input LSTM layer is followed by a
batch of hidden LSTM layers that process sequenced input and

5

combine learning patterns from the previous layers, to produce
learning representations at higher levels of abstraction. The
Dense layer is the ﬁnal layer with the number of nodes equal
to the number of categories in the output label. In the decision
phase, the soft-max activation function[19] is used by the
dense layer to select the most probable of output classes, and
the prediction error is calculated with the help of ’Sparse Cat-
egorical Crossentropy’ which is then backpropogated to adjust
the weights of the neural network. The model hyperparameters
are shown in table 1.

• True Positive (TP): Number of correctly predicted attack

samples.

• False Positive (FP): Number of falsely predicted attack

samples.

• True Negative (TN): Number of correctly predicted

normal samples.

• False Negative (FN): Number of falsely predicted normal

samples.

• Accuracy: Ratio of correctly predicted samples to total

samples.

TABLE I
Model hyper-parameters for Stacked LSTM

Dataset

of

No.
Layers

UNSW-NB15

Bot-IoT

4

2

LSTM
Cells/Layer
40 128 128
64
32 32

No.
of
Epochs

Learning
Rate

50

5

0.002

0.002

2) Bi-Directional LSTM: In case of Bi-Directional LSTMs,
the recurrent network layer is replicated and it works along
side the ﬁrst layer. The ﬁrst layer processes the input sequence,
while the reversed copy of the input sequence is fed to the
second layer. Learning from the past instances and the future
instances provides more context to the network and results
in better learning. The input layer is a Bi-Directional LSTM
layer which feeds forward a non-sequential output to a Dense
Layer. The Dense layer is the ﬁnal layer with the number
of nodes equal to the number of categories in the output
label. In the decision phase the soft-max activation function
is used by the dense layer to select the most probable of
output classes, and the prediction error is calculated with
the help of ’Sparse Categorical Crossentropy’ which is then
backpropogated to adjust the weights of the neural network.
The model hyperparameters are shown in table 2.

TABLE II
Model hyper-parameters for Bi-LSTM

Dataset

UNSW-NB15
Bot-IoT

No.s of
Layers
1
1

LSTM
Cells/Layer
64
12

No.s of
Epochs
50
5

Learning
Rate
0.0015
0.001

3) Model Structure and Parameters: The model is trained
over a number of epochs and training and validation loss
decreases gradually with time. The learning process is stopped
when the number of epochs cross the maximum limit or the
model starts overﬁtting on the training dataset.

V. RESULTS AND PERFORMANCE ANALYSIS

A. Experimental Setup

We have used the Google Colab’s GPU. The speciﬁcations
were Intel(R) Xeon(R) CPU with 2 cores@2.20 GHz, 12.7
GB of RAM and 78 Gb of Hard Disk space. The version of
python installed was 3.7.12 and Tensorﬂow was 2.7.0.

B. Evaluation Metrics

There is no single metric which can accurately tell how good
a particular model is. Hence, we have used several metrics to
evaluate the DL models:

ACC =

T P + T N
T P + T N + F P + F N

(4)

• Precision: Ratio of correctly predicted attack samples to

total predicted attack samples.

P R =

T P
T P + F P

(5)

• Recall: Ratio of correctly predicted attack samples to

total number of attack samples

RE =

T P
T P + F N

• F1 Score: Weighted mean of precision and recall.

F 1Score =

2 ∗ Recall ∗ P recision
Recall + P recision

(6)

(7)

• Weighted avg: The data points which have higher fre-

quency contribute more than others.

TABLE III
Stacked LSTM

Attack
Normal
Exploits
Reconnaissance
DoS
Generic
Shellcode
Fuzzers
Worms
Backdoor
Analysis
Weighted avg

Precision
0.98
0.56
0.79
0.87
1.00
0.62
0.52
1.00
1.00
1.00
0.97

Recall
1.00
0.86
0.51
0.01
0.98
0.41
0.29
0.00
0.00
0.00
0.96

F1
0.99
0.67
0.62
0.01
0.99
0.49
0.37
0.00
0.00
0.00
0.96

C. Performance Analysis on UNSW NB-15 Dataset:

The dataset was splitted into 75% for training and 25%
for validation purpose. The training phase consisted of 50
epochs that lasted over 5 hours. In case of Stacked LSTM,
the time taken for the validation phase was 128 seconds with
the processing speed of 0.2 ms/sample and for Bidirectional
LSTM, the validation phase took 90 seconds with the pro-
cessing speed of 0.14 ms/sample. The accuracy achieved by
Stacked LSTM was 96.60% and the accuracy achieved by Bi-
directional LSTM was 96.41%

TABLE IV
Bi-directional LSTM

Attack
Normal
Exploits
Reconnaissance
DoS
Generic
Shellcode
Fuzzers
Worms
Backdoor
Analysis
Weighted avg

Precision
0.99
0.54
0.53
0.47
1.00
0.68
0.56
0.53
0.67
1.00
0.96

Recall
0.99
0.79
0.59
0.03
0.98
0.59
0.35
0.16
0.00
0.00
0.96

F1
0.99
0.64
0.56
0.06
0.99
0.64
0.43
0.25
0.01
0.00
0.96

D. Performance Analysis on Bot-IoT Dataset:

In case of Bot IoT,

the training dataset had 2,934,817
samples and testing dataset had 733,705 samples. The time
taken by the training phase was 20 minutes which consisted
of 5 epochs for both the stacked and bi-directional LSTM
models. In case of Stacked LSTM, the validation phase lasted
for 48 seconds with the processing speed of 0.06 ms/sample.
For Bidirectional LSTM, the model took 156 seconds with the
processing speed of 0.195 ms/sample. .

6

Fig. 4. Training vs. validation accuracy (BI-LSTM)

The accuracy achieved by Stacked LSTM was 99.99% and
the accuracy achieved by Bi-directional LSTM was 99.99%.

Fig. 5. Training vs.Validation loss (BI-LSTM)

TABLE V
Stacked LSTM & Bi-directional LSTM

Fig. 2. Training vs.Validation accuracy (STACKED LSTM)

Attack
Normal
DDoS
DoS
Reconnaissance
Theft
Weighted avg

Precision Recall F1
0.98
1.00
1.00
1.00
1.00
1.00

0.83
1.00
1.00
0.93
0.36
1.00

0.89
1.00
1.00
0.96
0.53
1.00

Precision Recall F1
1.00
1.00
1.00
1.00
1.00
1.00

1.00
1.00
0.79
0.93
0.36
1.00

1.00
1.00
0.88
0.96
0.53
1.00

E. Comparison with other techniques:

TABLE VI
Comparison of Results on BoT-IoT and UNSW NB-15 Datasets

BoT-IoT

UNSW NB-15

Method
ARM [5]
Decision Tree [5]
DNN [5]
Naive Bayes [5]
Perceptron [5]
-
-
LBDMIDS

Accuracy (%)
85.6
93.2
99.9
72.7
63.9
-
-
99.9

Method
FSVM [4]
GAA [4]
DNN [5]
ANN [6]
ARM [6]
RNN [8]
Hybrid [8]
LBDMIDS

Accuracy (%)
92
93
99.2
63.97
86.45
95.7
98.7
96.6

Fig. 3. Training vs. Validation loss (STACKED LSTM)

7

[3] A. Javaid, Q. Niyaz, W. Sun, M. Alam, A deep learning approach for
network intrusion detection system, in: Proceedings of the 9th EAI In-
ternational Conference on Bioinspired Information and Communications
Technologies (formerly BIONETICS), 2016, pp. 21–26.

[4] J. Ashraf, A. D. Bakhshi, N. Moustafa, H. Khurshid, A. Javed and A. Be-
heshti, ”Novel Deep Learning-Enabled LSTM Autoencoder Architecture
for Discovering Anomalous Events From Intelligent Transportation Sys-
tems,” in IEEE Transactions on Intelligent Transportation Systems, vol.
22, no. 7, pp. 4507-4518, July 2021, doi: 10.1109/TITS.2020.3017882.
[5] Koroniotis, Nickolaos & Moustafa, Nour & Sitnikova, Elena. (2020). A
new network forensic framework based on deep learning for Internet
of Things networks: A particle deep framework. Future Generation
Computer Systems. 110. 10.1016/j.future.2020.03.042.

[6] Koroniotis, Nickolaos & Moustafa, Nour & Sitnikova, Elena & Slay,
Jill. (2017). Towards Developing Network forensic mechanism for Botnet
Activities in the IoT based on Machine Learning Techniques.

[7] N. Moustafa and J. Slay, ”UNSW-NB15: a comprehensive data set for
network intrusion detection systems (UNSW-NB15 network data set),”
2015 Military Communications and Information Systems Conference
(MilCIS), 2015, pp. 1-6, doi: 10.1109/MilCIS.2015.7348942.

[8] Smys, S., Abul Basar, and Haoxiang Wang. ”Hybrid intrusion detection
system for internet of things (IoT).” Journal of ISMAC 2.04 (2020): 190-
199.

[9] Zhong, Ming, Yajin Zhou, and Gang Chen. ”Sequential model based
intrusion detection system for IoT servers using deep learning methods.”
Sensors 21.4 (2021): 1113.

[10] Saranya, T., et al. ”Performance analysis of machine learning algorithms
in intrusion detection system: A review.” Procedia Computer Science 171
(2020): 1251-1260.

[11] M. Ge, X. Fu, N. Syed, Z. Baig, G. Teo and A. Robles-Kelly, ”Deep
Learning-Based Intrusion Detection for IoT Networks,” 2019 IEEE
24th Paciﬁc Rim International Symposium on Dependable Computing
(PRDC), 2019, pp. 256-25609, doi: 10.1109/PRDC47002.2019.00056.
[12] Liang, Chao, et al. ”Intrusion detection system for Internet of Things
based on a machine learning approach.” 2019 International Conference
on Vision Towards Emerging Trends in Communication and Networking
(ViTECoN). IEEE, 2019.

[13] N. Moustafa and J. Slay, ”UNSW-NB15: a comprehensive data set for
network intrusion detection systems (UNSW-NB15 network data set),”
2015 Military Communications and Information Systems Conference
(MilCIS), 2015, pp. 1-6, doi: 10.1109/MilCIS.2015.7348942.

[14] Moustafa, Nour, and Jill Slay. ”The evaluation of Network Anomaly
Detection Systems: Statistical analysis of the UNSW-NB15 dataset and
the comparison with the KDD99 dataset.” Information Security Journal:
A Global Perspective (2016): 1-14.

[15] Moustafa, Nour, et al. ”Novel geometric area analysis technique for
anomaly detection using trapezoidal area estimation on large-scale net-
works.” IEEE Transactions on Big Data (2017).

[16] Moustafa, Nour, et al. ”Big data analytics for intrusion detection system:
statistical decision-making using ﬁnite dirichlet mixture models.” Data
Analytics and Decision Support for Cybersecurity. Springer, Cham, 2017.
127-156.

[17] Sarhan, Mohanad, Siamak Layeghy, Nour Moustafa, and Marius Port-
mann. NetFlow Datasets for Machine Learning-Based Network Intrusion
Detection Systems. In Big Data Technologies and Applications: 10th
EAI International Conference, BDTA 2020, and 13th EAI International
Conference on Wireless Internet, WiCON 2020, Virtual Event, December
11, 2020, Proceedings (p. 117). Springer Nature.

[18] Koroniotis, Nickolaos & Moustafa, Nour & Sitnikova, Elena & Turnbull,
Benjamin. (2018). Towards the Development of Realistic Botnet Dataset
in the Internet of Things for Network Forensic Analytics: Bot-IoT Dataset.
[19] Sagar Sharma, Sep 6, 2017, Activation Functions in Neural Net-
works. URL https://towardsdatascience.com/activation-functions-neural-
networks-1cbd9f8d91d6

[20] K. Saurabh, Vikash, L. Mishra and S. Varma, ”An Efﬁcient IoT Model
for On-Demand Particulate Matter Control System in Coal Mining Cities,”
2020 IEEE 17th India Council International Conference (INDICON),
2020, pp. 1-7, doi: 10.1109/INDICON49873.2020.9342085

Fig. 6. NIDS accuracy comparision of Bi-LSTM and Stacked LSTM on both
datasets

The results in paper [4], [6] and [8] are for binary classi-
ﬁcation (Attack and Benign) while the results in paper [5]
and proposed methodology (LBDMIDS) is for Multi-Class
classiﬁcation.

VI. CONCLUSION AND FUTURE WORKS

To protect the IoT networks from attackers and their attacks,
it is necessary to detect the intrusions precisely. In this paper, a
DL method based model called LBDMIDS has been proposed
which shows promising performance in intrusion detection.

In this paper, the focus was on detection of malicious events
with improved accuracy in IoT networks where the dataset is
large and time series. LBDMIDS works on LSTM architecture
to detect intrusion in a network. To validate LBDMIDS, the
experiment was performed on two well known datasets, i.e.,
BoT-IoT and UNSW-NB15. We scaled and normalized the
dataset accordingly and fed it
to LBDMIDS. The output
and results produced by LBDMIDS are good in terms of
prediction accuracy and F1-score. To generalize our model
in LBDMIDS, Stacked and Bidirectional LSTM were used
on UNSW-NB15 dataset, the accuracy achieved by Stacked
LSTM model was 96.60% and the accuracy achieved by Bi-
Directional LSTM model was 96.41%. Similarly, on BoT-IoT
dataset, the accuracy achieved by Stacked and Bi-Directional
LSTM model was 99.99%.

Owing to the system limitations and epoch duration, the
accuracy could be signiﬁcantly improved considering more
efﬁcient and robust machines. Our experiment was not per-
formed on industrial scale or real world IoT applications [20].
With more efﬁcient GPUs and more hybrid DL models, the
prediction accuracy could be improved.

REFERENCES

[1] Jack Steward, The Ultimate List of Internet of Things Statistics for 2022.

URL https://ﬁndstack.com/internet-of-things-statistics/

[2] M. Nawir, A. Amir, N. Yaakob, O.B. Lynn, Internet of things(iot):
Taxonomy of security attacks, in: 2016 3rd International Conference on
Electronic Design (ICED), IEEE, 2016, pp. 321–326.

