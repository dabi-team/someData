Data Driven Exploratory Attacks on Black Box Classiﬁers in Adversarial
Domains

Tegjyot Singh Sethia,∗, Mehmed Kantardzica

aData Mining Lab, University of Louisville, Louisville, USA

7
1
0
2

r
a

M
3
2

]
L
M

.
t
a
t
s
[

1
v
9
0
9
7
0
.
3
0
7
1
:
v
i
X
r
a

Abstract

While modern day web applications aim to create impact at the civilization level, they have become vulnerable to
adversarial activity, where the next cyber-attack can take any shape and can originate from anywhere. The increasing
scale and sophistication of attacks, has prompted the need for a data driven solution, with machine learning forming
the core of many cybersecurity systems. Machine learning was not designed with security in mind, and the essential
assumption of stationarity, requiring that the training and testing data follow similar distributions, is violated in an
adversarial domain. In this paper, an adversary’s view point of a classiﬁcation based system, is presented. Based
on a formal adversarial model, the Seed-Explore-Exploit framework is presented, for simulating the generation of
data driven and reverse engineering attacks on classiﬁers. Experimental evaluation, on 10 real world datasets and
using the Google Cloud Prediction Platform, demonstrates the innate vulnerability of classiﬁers and the ease with
which evasion can be carried out, without any explicit information about the classiﬁer type, the training data or the
application domain. The proposed framework, algorithms and empirical evaluation, serve as a white hat analysis of
the vulnerabilities, and aim to foster the development of secure machine learning frameworks.

Keywords: Adversarial machine learning, reverse engineering, black box attacks, classiﬁcation, data diversity,
cybersecurity.

1. Introduction

The growing scale and reach of modern day web ap-
plications has increased its reliance on machine learning
techniques, for providing security. Conventional secu-
rity mechanisms of ﬁrewalls and rule-based black and
white lists, cannot eﬀectively thwart evolving attacks at
a large scale (Saha and Sanyal, 2014). As such, the
use of data driven machine learning techniques in cy-
bersecurity applications, has found widespread accep-
tance and success (Group et al., 2013). Whether it be
for outlier detection for network intrusion analysis (Za-
mani and Movahedi, 2013), biometric authentication us-
ing supervised classiﬁcation (DSouza, 2014), or for un-
supervised clustering of fraudulent clicks (Walgampaya
and Kantardzic, 2011), the use of machine learning in
cybersecurity domains is ubiquitous. However, during
this era of increased reliance on machine learning mod-
els, the vulnerabilities of the learning process itself have

∗Corresponding author.
Email addresses: tegjyotsingh.sethi@louisville.edu

(Tegjyot Singh Sethi ), mehmedkantardzic@louisville.edu
(Mehmed Kantardzic)

mostly been overlooked. Machine learning operates un-
der the assumption of stationarity, i.e. the training and
the testing distributions are assumed to be identically
and independently distributed (IID) ( ˇZliobait˙e, 2010).
This assumption is often violated in an adversarial set-
ting, as adversaries gain nothing by generating samples
which are blocked by a defender’s system (Guerra et al.,
2010). The dynamic and contentious nature of this do-
main, demands a thorough analysis of the dependability
and security of machine learning systems, when used in
cybersecurity applications.

In an adversarial environment, the accuracy of clas-
siﬁcation has little signiﬁcance,
if an attacker can
easily evade detection by intelligently perturbing the
input samples (Kantchelian et al., 2013). Any de-
ployed classiﬁer is susceptible to probing based attacks,
where an adversary uses the same channel as the client
users, to gain information about the system, and then
subsequently uses that information to evade detection
(Abramson, 2015; Biggio et al., 2014a). This is seen in
Fig. 1 a), where the defender starts by learning from the
training data and then deploys the classiﬁer C, to pro-

Preprint submitted to Elsevier

March 24, 2017

 
 
 
 
 
 
vide services to client users. Once deployed, the model
C is vulnerable to adversaries, who try to learn the be-
havior of the defender’s classiﬁer by submitting probes
as input samples, masquerading as client users. In do-
ing so, the defender’s classiﬁer is seen only as a black
box, capable of providing tacit Accept/Reject feedback
on the submitted samples. An adversary, backed by the
knowledge and understanding of machine learning, can
use this feedback to reverse engineer the model C (as
C(cid:48)). It can then avoid detection on future attack samples,
by accordingly perturbing the input samples.
It was
shown recently that, deep neural networks are vulnera-
ble to adversarial perturbations (Papernot et al., 2016b).
A similar phenomenon was shown to aﬀect a wide va-
riety of classiﬁers in (Papernot et al., 2016a), where it
was demonstrated that adversarial samples are transfer-
able across diﬀerent classiﬁer families. Cloud based
machine learning services (such as Amazon AWS Ma-
chine Learning1 and Google Cloud Platform2), which
provide APIs for accessing predictive analytics as a ser-
vice, are also vulnerable to similar black box attacks
(Tram`er et al., 2016).

An example of the aforementioned adversarial envi-
ronment is illustrated in Fig. 1b), where a behavioral
mouse dynamics based CAPTCHA (Completely Auto-
mated Public Turing test to tell Computers and Humans
Apart) system is considered. Popular examples of these
systems are Google’s reCAPTCHA3 and the classiﬁer
based system developed in (DSouza, 2014). These sys-
tems use mouse movement data to distinguish humans
from bots and provide a convenient way to do so, rely-
ing on a simple point and click feedback, instead of re-
quiring the user to infer garbled text snippets (DSouza,
2014). The illustrative 2D model of Fig. 1 b), shows a
linear classiﬁer trained on the two features of - Mouse
movement speed and Click time. An adversary, aim-
ing to evade detection by this classiﬁer, starts by guess-
ing the click time as a key feature (intuitive in this set-
ting), and then proceeds to makes probes to the black
box model C, to learn its behavior. Probes are made by
going through the spectrum of average reaction times
for humans4, guided by the Accept(green)/Reject(red)
feedback from the CAPTCHA server. The information
learned by reconnaissance on the black box system, can
then be used to modify the attack payload so as to subse-
quently evade detection. While, this example was sim-
plistic, its purpose is to illustrate the adversarial envi-

1https://aws.amazon.com/machine-learning/
2cloud.google.com/machine-learning
3www.google.com/recaptcha
4www.humanbenchmark.com/tests/reactiontime

2

(a) An adversary making probes to the black box model C, can learn
it as C’, using active learning.

(b) Example task of attacking behavioral CAPTCHA. Black box
model C, based on Mouse Speed and Click Time features, is used
to detect benign users from bots. Adversary can reverse engineer C
as C(cid:48), by guessing click time feature and making probes based on the
human response time chart, using the same input channels as regular
users.

Figure 1: Classiﬁers in adversarial environment, a) shows the general
adversarial nature of the problem and b) shows an example consider-
ing a behavioral CAPTCHA system.

ronment in which classiﬁers operate. Practical deployed
classiﬁers tend to be more complex, non linear and mul-
tidimensional. However, the same reasoning and ap-
proach can be used to evade complex systems. An ex-
ample of this is the good words and bad words attacks
on spam detection systems (Lowd and Meek, 2005b).
By launching two spam emails each diﬀering in only
one word ’Sale’, it can be ascertained that this word
is important to the classiﬁcation, if the email containing
that word is ﬂagged as spam. Knowing this information,
the adversary can modify the word to be ’Sa1e’, which
looks visually the same but avoids detection. These eva-
sion attacks are non-intrusive in nature and diﬃcult to
eliminate by traditional encryption/security techniques,
because they use the same access channels as regular
input samples and they see the same black box view of
the system. From the classiﬁcation perspective, these at-
tacks occur at test time and are aimed at increasing the

false negative rate of the model, i.e. increase the num-
ber of Malicious samples classiﬁed as Legitimate by C
(Barreno et al., 2006; Biggio et al., 2013).

Data driven attacks on deployed classiﬁcation sys-
tems, presents a symmetric ﬂip side to the task of learn-
ing from data. Instead of learning from labeled data to
generate a model, the task of an attacker is to learn about
the model, to generate evasive data samples (Abram-
son, 2015). With this motivation, we propose the Seed-
Explore-Exploit(SEE) framework in this paper, to ana-
lyze the attack generation process as a learning problem,
from a purely data driven perspective and without in-
corporating any domain speciﬁc knowledge. Research
work on detecting concept drift in data streams(Sethi
et al., 2016a,b), motivated the need for a formal anal-
ysis of the vulnerabilities of machine learning, with an
initial evaluation proposed in our work in (Sethi et al.,
2017). In this paper, we extend the earlier version with:
i) incorporating and evaluating eﬀects of Diversity of
attacks on the defender’s strategy, ii) introducing adver-
sarial metrics of attack quality and the eﬀects of vary-
ing the parameters of attack algorithms, iii) extensive
detailed experimentation of the framework using a va-
riety of defender models and the Google Cloud Predic-
tion Service, and iv) experimentation simulating eﬀects
of diversity on blacklisting based countermeasures. The
main contributions of this paper are:

• A domain independent data driven framework
is presented,
to simulate attacks using an
Exploration-Exploitation strategy. This generic
framework and the algorithms presented, can be
used to analyze simple probing attacks to more so-
phisticated reverse engineering attacks.

• Formal adversarial model and adversary’s metrics
are proposed, as a background for sound scientiﬁc
development and testing for secure learning frame-
works.

• Empirical analysis on 10 real world datasets,
demonstrates that feature space information is suf-
ﬁcient to launch attacks against classiﬁers, irre-
spective of the type of classiﬁer and the application
domain. Additional experimentation on Google’s
Cloud Prediction API, demonstrates vulnerability
of remote black box prediction services.

• The analysis of diversity and its eﬀect on black-
listing based countermeasures, demonstrates that
such security measures (as proposed in (Kantche-
lian et al., 2013)) are ineﬀective when faced with
reverse engineering based attacks of high diversity.

3

The rest of the paper is organized as follows: Sec-
tion 2, presents background and related work on the
security of machine learning. Section 3, presents the
formal model of an adversary and the proposed Seed-
Explore-Exploit(SEE) framework, for attack genera-
tion. Based on the framework, the Anchor Points at-
tack and the Reverse Engineering attack algorithms are
presented in Section 3.2 and 3.3, respectively. Experi-
mental evaluation and detailed analysis is presented in
Section 4. Additional discussion about diversity of at-
tacks and its importance is presented in Section 5. Con-
clusion and avenues for further research are presented
in Section 6.

2. Related Work on the Security of Machine Learn-

ing

One of the earliest works on machine learning secu-
rity was presented in (Barreno et al., 2006), where a tax-
onomy of attacks was deﬁned, based on the principles
of information security. The taxonomy categorized at-
tacks on machine learning systems along the three axis
of: Speciﬁcity, Inﬂuence and the type of Security Vi-
olation, as shown in Table 1. Based on the inﬂuence
and the portion of the data mining process that these at-
tacks aﬀect, they are classiﬁed as being either Causative
or Exploratory (Biggio et al., 2014b). Causative attacks
aﬀect the training data while Exploratory attacks aﬀect
the model at test time. Speciﬁcity of the attacks, refers
to whether they aﬀect a set of targeted samples, in order
to avoid detection by perturbing them, or if the attacks
are aimed towards indiscriminately aﬀecting the system,
with no speciﬁc pre-selected instances. Based on the
type of security violation, the attacks can aim to violate
integrity, by gaining unsanctioned access to the system,
or can be used to launch a denial of service availability
attack.

Causative attacks aim to mislead training, by poi-
soning the training set, so that future attacks are eas-
ily evaded (Li and Yeung, 2014; Biggio et al., 2014a).
Causative attacks, although severe in eﬀect, can be pre-
vented by careful curation of the training data (Li and
Chan, 2014) and by keeping the training data secure- us-
ing database security measures, authentication and en-
cryption. Exploratory attacks are more commonplace,
less moderated and can be launched remotely without
raising suspicion. These attacks aﬀect the test time data,
and are aimed at reducing the system’s predictive per-
formance (Biggio et al., 2014b). This is done by crafting
the attack samples, to evade detection by the defender’s
model (Biggio et al., 2013; Lowd and Meek, 2005a).

Table 1: Categorization of attacks against machine learning systems

Inﬂuence

Speciﬁcity

Security
violation

Causative- Attacks inﬂuence training data,
to mislead learning
Exploratory- Attacks aﬀect test time data,
to evade detection
Targeted- Attack aﬀect onyl particular instances
Indiscriminate- Attacks irrespective of instances
Integrity- Results in increased false negatives
Availability- Denial of service attacks,
due to increased errors

Once a model is trained and deployed in a cybersecu-
rity application, it is vulnerable to exploratory attacks.
These attacks are non intrusive and are aimed at gaining
information about the system, which is then exploited
to craft evasive samples, to circumvent the system’s se-
curity. Since, these attacks use the same channel as the
client users, they are harder to detect and prevent.

Targeted-Exploratory attacks aim to modify a speciﬁc
set of malicious input samples, minimally, to disguise
them as legitimate. Indiscriminate attacks are more gen-
eral in their goals, as they aim to produce any sam-
ple which will result in the defender’s model to have
a misclassiﬁcation. Most work on exploratory attacks
are concentrated on the targeted case, considering it as
a constrained form of indiscriminate attacks, with the
goal of starting with a malicious sample and making
minimal modiﬁcations to it, to avoid detection (Biggio
et al., 2014a, 2013; Xu et al., 2016; Pastrana Portillo,
2014; Lowd and Meek, 2005b). This idea was formal-
ized in (Lowd and Meek, 2005a), where the Minimal
Adversarial Cost (MAC) metric, of a genre of classi-
ﬁers, was introduced to denote the ease with which clas-
siﬁers of a particular type can be evaded. The hardness
of evasion was given in terms of the number of probes
needed to obtain a low cost evasive sample. A classiﬁer
was considered easy to evade if making a few optimal
modiﬁcations to a set of samples resulted in a high ac-
curacy of evasion. Work in (Nelson and et al., 2010)
shows that linear and convex inducing classiﬁer are all
vulnerable to probing based attacks, and (Nelson et al.,
2012) presents eﬃcient probing strategies to carry out
these attacks.

Particular strategies developed for performing ex-
ploratory attacks vary based on the amount of infor-
mation available to the adversary, with a broad clas-
siﬁcation presented in (Alabdulmohsin et al., 2014)
as: a) Evasion attacks and b) Reverse Engineering at-
tacks. Evasion attacks are used when limited informa-
tion about the system is available, such as a few le-

gitimate samples only. These legitimate samples are
exploited by masking techniques such as- mimicking
(Smutz and Stavrou, 2016) and spooﬁng (Akhtar et al.,
2011), which masquerade malicious content within the
legitimate samples. The mimicry attack was presented
in (Smutz and Stavrou, 2016), where the Mimicus tool5
was developed, to implement evasion attacks on pdf
documents, by hiding malicious code within benign
documents. The good words attacks on spam emails
uses a similar technique (Lowd and Meek, 2005b). A
spam email is inserted with benign looking words, to
evade detection. Similarly, spooﬁng attacks are com-
mon in biometrics(Akhtar et al., 2011) and for phishing
websites(Huh and Kim, 2011), where visual similarity
can be achieved with totally diﬀerent content. A gen-
eral purpose, domain independent technique for evasion
was presented in (Xu et al., 2016). Here, using genetic
programming, variants of a set of malicious samples
were generated as per a monotonically increasing ﬁt-
ness function, which denoted success of evasion. This
is an attractive technique due to its generic approach,
but limited probing budgets and lack of a graded ﬁtness
In the
function, are some of its practical limitations.
presence of a large probing budget, or speciﬁc informa-
tion about the defender’s classiﬁer model, the gradient
descent evasion attack of (Biggio et al., 2013), can be
used. This attacks relies on knowing the exact classi-
ﬁer function used by the defender, or the ability to re-
verse engineer it using a suﬃcient number of probes.
Once information about the classiﬁer is known, the at-
tack uses a gradient descent strategy to ﬁnd an optimal
low cost evasion sample for the classiﬁer. Search strate-
gies were developed for a wide range of classiﬁers with
diﬀerentiable decision functions, including neural net-
works, non-linear Support vector machines, one class
classiﬁers and for classiﬁers operating in discrete fea-

5www.github.com/srndic/mimicus/blob/master/

mimicus/attacks/mimicry.py

4

attack. Analyzing performance of models under such
attack scenarios is essential to understanding its vulner-
abilities in a more general real world situation, where
all types of attacks are possible. Also, while most recent
methodologies develop attacks as an experimental tool
to test their safety mechanisms, there is very few works
(Biggio et al., 2013; Xu et al., 2014; Wagner and Soto,
2002; Pastrana Portillo, 2014), which have attempted to
study the attack generation process itself. Our proposed
work analyzes Indiscriminate-Exploratory-Integrity vi-
olating attacks, under a data driven framework, with
diverse adversarial goals and while considering only a
black box model for the defender’s classiﬁer. We ana-
lyze the attacks from an adversary’s point of view, con-
sidering the adversarial samples generation process, so
as to understand the vulnerabilities of classiﬁers and to
motivate the development of secure machine learning
architectures.

3. Proposed Methodology for Simulating Data

Driven Attacks on Classiﬁers

Data driven exploratory attacks on classiﬁers, aﬀect
the test time data seen by a deployed classiﬁer. An ad-
versary intending to evade classiﬁcation, will begin by
learning information about the system, over time, and
then will launch an attack campaign to meet its goals.
The adversary can only interact with the system as a
black box model, receiving tacit Accept/Reject feedback
for the submitted samples. However, an adversary can
only make a limited number of probes, before it gets de-
tected or runs out of resources. Additionally, we assume
a minimal shared knowledge environment between the
adversary and the defender (Rndic and Laskov, 2014).
Only the feature space information is shared between
the two parties, as both operate on the same data space.
All other information - such as the defender’s classiﬁer
type, the model parameters and the training data, is kept
hidden by the defender. Both the adversary and the de-
fender are assumed to be capable machine learning ex-
perts, who are equipped with the tools and understand-
ing of using a data driven approach to best suit their
goals. Based on this intuitive understanding, the formal
model of an adversary based on it’s knowledge, goals
and resources (Biggio et al., 2014a), is presented below:

• Knowledge- The adversary is aware of the num-
ber, type and range of features, used by the classi-
ﬁcation model. This could be approximated from
publications, publicly available case studies in re-
lated applications, or by educated guessing (Rndic
and Laskov, 2014). For example, in case of spam

Figure 2: Gradient descent evasion attack over 500 iterations. Left-
Initial image of digit 3, Center- Image which ﬁrst gets classiﬁed as 7,
Right- Image after 500 iteration.(Biggio et al., 2013)

ture spaces (Biggio et al., 2013). An illustration of the
gradient descent attacks for masquerading a sample is
shown in Fig. 2, where the image 3 is modiﬁed to be
classiﬁed as 7 over 500 iterations of gradient descent.

Reverse engineering the defender’s model provides
avenues for sophisticated exploratory attacks, as it ex-
poses features important to the classiﬁer, to be used for
mimicry attacks or large scale indiscriminate attacks.
Perfect reverse engineering is not needed, as an adver-
sary is interested only in identifying the portion of the
data space which is classiﬁed as Legitimate. Reverse
engineering was ﬁrst employed in (Lowd and Meek,
2005a), where a sign witness test was used to see if a
particular feature had a positive or negative impact on
the decision. Reverse engineering of a decision tree
classiﬁer, as a symmetric model for defender and adver-
sary, was presented in (Xu et al., 2014). (Xu et al., 2016)
used genetic programming as a general purpose reverse
engineering tool, under the assumption of known train-
ing data distribution and feature construction algorithm.
The genetic programming output, because of its intu-
itive tree structure, was then used to guide evasion at-
tacks on intrusion detection systems. The idea of re-
verse engineering was linked to that of active learning
via query generation in (Alabdulmohsin et al., 2014),
where the robustness of SVM to reverse engineering is
tested using active learning techniques of random sam-
pling, uncertainty sampling and selective sampling.

In the above mentioned works of targeted-exploratory
attacks, it is assumed that if an evasion is expensive (far
from the original malicious sample), the adversary will
give up. The above techniques are not designed for a
determined adversary, who is willing to launch indis-
criminate attacks. An adversary who wants to launch an
indiscriminate attack will not bother with the near opti-
mal evasion problem (Nelson and et al., 2010). These
type of attacks have been largely ignored, with the only
mention we found was in (Zhou et al., 2012), where it is
termed - the free range attack, as an adversary is free to
move about in the data space. In such attacks, the adver-
sary will ﬁrst analyze the vulnerabilities of the model,
looking for prediction blind spots, before attempting an

5

classiﬁcation, the feature domain could be the dic-
tionary of English words, which is publicly avail-
able and well known. This represents a symmetric
learning problem with both parties operating on the
same feature space. No other information about the
defender’s model is known by the adversary.

• Goals- The adversary intends to cause false nega-
tives for the defender’s classiﬁer, on the submitted
attack samples. Additionally, the adversary also
wants the attacks to be robust, such that it can avoid
being detected and stopped by simple blacklisting
techniques (Bilge and Dumitras, 2012). From a
data driven perspective, the attacker aims to avoid
detection by generating an attack set with high
diversity and variability. While, repeating a sin-
gle conﬁrmed attack point, over and over, leads
to ensured false negatives, such attacks are easily
stopped by blacklisting that single point. We con-
sider serious attackers only, who aim to force the
retraining of the defender’s classiﬁcation system.

• Resources- The adversary has access to the sys-
tem only as a client user. It can submit probes and
receive binary feedback on it, upto a limited prob-
ing budget, without being detected. The adversary
does not have control over the training data or the
model trained by the defender.

The presented model of the adversary represents a
general setting, where an attacker can take the form of
an end user and then attack the system over time. Mod-
ern day web applications, which aim to reach as many
users as possible, all operate under this environment and
are susceptible to data driven attacks. Based on the ad-
versary’s model, the attack can be formalized here. A
classiﬁer C, trained on a set of training data DT rain, is
responsible for classifying incoming samples into Le-
gitimate or Malicious classes. An adversary aims to
generate an attack campaign of samples D(cid:48)
Attack, such
that C(D(cid:48)
Attack) has a high false negative rate. The adver-
sary has at its disposal, a budget BExplore of probing data
D(cid:48)
Explore, which it can use to learn C and understand it
as C(cid:48)(D(cid:48)
Explore). The number of attack samples (NAttack)
should be much larger than BExplore, to justify expendi-
ture on the adversary’s part. This speciﬁed notation will
be used through the rest of the paper.

The Seed-Explore-Exploit (SEE) framework is pre-
sented in Section 3.1, which provides an overview of
the attack paradigm. Two speciﬁc attack strategies de-
veloped under the SEE framework, the Anchor Points
attacks (AP) and the Reverse Engineering attacks (RE),

are presented in Section 3.2 and Section 3.3, respec-
tively.

3.1. The Seed-Explore-Exploit (SEE) Framework

The SEE framework employs a data driven ap-
proach for generating adversarial samples. The idea
of Exploration-Exploitation is common in search based
optimization techniques, where the goal is to learn the
data space and then emphasize only on the promising
directions (Chen et al., 2009). An adversary can also
utilize a similar strategy, to best utilize the exploration
budget (BExplore), such that the resulting attack samples
(D(cid:48)
Attack) have high accuracy and high diversity. The spe-
ciﬁc steps of the framework are explained below:

• Seed- An attack starts with a seed phase, where it
acquires a legitimate sample (and a malicious sam-
ple), to form the seed set D(cid:48)
S eed. This seed sample
can be acquired by random sampling in the fea-
ture space, by guessing a few feature values, or
from an external data source of a comparable ap-
plication (Papernot et al., 2016a). For the case of
a spam classiﬁcation task, picking an email from
one’s own personal inbox would be a functional
legitimate seed sample.

• Explore- Exploration is a reconnaissance task,
starting with D(cid:48)
S eed, where the goal is to obtain
maximum diverse information, to understand the
coverage and extent of the space of legitimately
classiﬁed samples.
In this phase, the adversary
submits probes and receives feedback from the de-
fender’s black box. The defender can be probed
upto a budget BExplore, without being thwarted or
detected. To avoid detection, it is natural that the
adversary needs to spread out the attacks over time
and data space, in which case the BExplore is the
time/resources available to the adversary. The ex-
ploration phase results in a set of labeled samples
D(cid:48)
Explore, and the goal of the adversary is to best
choose this set based on it’s strategy.

• Exploit- The information gathered in the explo-
ration phase is used here to generate a set of attack
Attack. The eﬃcacy of the attack is based
samples D(cid:48)
on the accuracy and the diversity of these samples.

The SEE framework provides a generic way of deﬁn-
ing attacks on classiﬁers. Speciﬁc instantiations of the
three phases can be developed, to suit one needs and
simulation goals.

6

3.2. The Anchor Points Attack (AP)

The Anchor Points attack is suited for adversaries
with a limited probing budget BExplore, who have a goal
of generating evasive samples for immediate beneﬁts.
An example of this would be - zero day exploits, where
an adversary wants to exploit a new found vulnerabil-
ity, before it is ﬁxed (Bilge and Dumitras, 2012). These
attacks start by obtaining a set of samples classiﬁed as
Legitimate by C, called the Anchor Points, which serve
as ground truth for generating further attack samples.
From a data driven perspective, this attack strategy is
deﬁned under the SEE framework as given below.

• Seed- The attack begins with a single legitimate
S eed).

sample as the Seed (D(cid:48)

• Explore- After the initial seed has been obtained
(provided or randomly sampled), the exploration
phase proceeds to generate the set of Anchor
Points, which will enable the understanding of the
space of samples classiﬁed as Legitimate. The ex-
ploration phase is described in Algorithm 1, and
is a radius based incremental neighborhood search
technique, around the seed samples, guided by the
feedback from the black box model C. Diver-
sity of search is maintained by dynamically adjust-
ing the search radius (Ri), based on the amount of
ground truth obtained so far (Line 5). This ensures
that radius of exploration increases in cases where
the number of legitimate samples obtained is high,
and vice versa, thereby balancing diversity of sam-
ples with their accuracy. Samples are explored by
perturbing an already explored legitimate sample
(Seed sample in case of ﬁrst iteration), within the
exploration radius (Line 7). The ﬁnal exploration
dataset of Anchor Points - D(cid:48)
Explore, is comprised of
all explored samples xi, for which C(xi) indicated
the Legitimate class label. The exploration phase
is illustrated on a synthetic 2D dataset in Fig. 3,
where the neighborhood radius Ri indicates the ex-
ploration neighborhood of a sample.

• Exploit- The anchor points obtained as D(cid:48)

Explore,
forms the basis for launching the dedicated attack
campaign on the classiﬁer C. The exploitation
phase (Algorithm 2) combines two techniques to
ensure high accuracy and diversity of attack sam-
ples: a) Simple perturbation- The anchor point
samples are perturbed, similar to the exploration
phase, using a radius of exploitation- RExploit (Line
4) and, b) Convex combination- The perturbed
samples are combined using convex combination

Algorithm 1: AP- Exploration Phase
Input

: Seed Data D(cid:48)
S eed, Defender black box C.
Parameters: Exploration budget BExplore,
Exploration neighborhood- [Rmin, Rmax]

Output: Exploration data set D(cid:48)

Explore

S eed

Explore ← D(cid:48)

1 D(cid:48)
2 count
3 for i = 1 .. BExplore do
4

legitimate=0

xi ← Select random sample from D(cid:48)
Ri = (Rmax − Rmin) ∗ (count legitimate/i) + Rmin
(cid:46) Dynamic neighborhood search
(cid:46) perturbed sample

ˆxi ← Perturb(xi , Ri)
if C.predict( ˆxi) is Legitimate then

Explore

Explore∪ ˆxi

D(cid:48)
count legitimate ++

5

6

7

8

9

10

11 Procedure Perturb(sample, RNeigh)
12

return sample+=random(mean=0, std=RNeigh)

of samples, two at a time (Line 7). This is in-
spired by the Synthetic Minority Oversampling
Technique (SMOTE), which is a popular oversam-
pling technique for imbalanced datasets (Chawla
et al., 2002). The attack set D(cid:48)
Attack, shown in red in
Fig. 3, is the ﬁnal attack on the classiﬁer C.

The performance of the AP attack is largely depen-
dent on the probes collected in the initial seed and ex-
ploration phase. As such, maintaining diversity is key,
as larger coverage ensures more ﬂexibility in attack gen-
eration. By the nature of these attacks, they can be
thwarted by blacklists capable of approximate match-
ing (Prakash et al., 2010). Nevertheless, they are suited
for adhoc swift blitzkriegs, before the defender has time
to respond.

3.3. The Reverse Engineering Attack (RE)

In case of sophisticated attackers, with a large
BExplore, direct reverse engineering of the classiﬁcation
boundary is more advantageous. It provides a better un-
derstanding of the classiﬁcation landscape, which can
then be used to launch large scale evasion or availability
attacks (Kantchelian et al., 2013). Reverse engineering
could also be an end goal in itself, as it provides in-
formation about feature importance to the classiﬁcation
task (Lowd and Meek, 2005a). A reverse engineering
attack, if done eﬀectively, can avoid detection and make
retraining harder on the part of the defender. However,
unlike the AP attacks, these attacks are aﬀected by the

7

Figure 3: Illustration of AP attacks on 2D synthetic data.(Left - Right): The defender’s model from it’s training data. The Exploration phase
depicting the seed(blue) and the anchor points samples(purple). The Exploitation attack phase samples (red) generated based on the anchor points.

Algorithm 2: AP- Exploitation Phase
Input

: Exploration data set D(cid:48)
attacks NAttack, Radius of Exploitation
RExploit
Output: Attacks set D(cid:48)

Explore, Number of

Attack

Attack ←[]

1 D(cid:48)
2 for i = 1 .. NAttack do
3

xA, xB ← Select random samples from D(cid:48)
ˆxA, ˆxB ← Perturb(xA, RExploit),
Perturb(xB, RExploit)

Explore

λ = random(0, 1)
attack samplei ← ˆxA ∗ λ + (1 − λ) ∗ ˆxB

(cid:46) Random perturbation
(cid:46) number in [0,1]

(cid:46) Convex combination

4

5

6

7

8

D(cid:48)

9

Attack ∪ attack samplei
10 Procedure Perturb(sample, RExploit)
11

return sample+=random(mean=0, std=RExploit)

type of model used by the black box C, the dimensional-
ity of the data and the number of probes available. Nev-
ertheless, the goal of an adversary is not to exactly ﬁt
the decision surface, but to infer it suﬃciently, so as to
be able to generate attacks of high accuracy and diver-
sity. As such, a linear approximation to the defender’s
model and a partial reverse engineering attempt should
be suﬃcient for the purposes of launching a reduced ac-
curacy attack. This reduction in accuracy can be com-
pensated for by launching a massive attack campaign,
exploiting the information provided by the reverse engi-
neered model C(cid:48).

Eﬀective reverse engineering relies on the availabil-
ity of informative samples. As such, it is necessary

8

to use the probing budget BExplore eﬀectively. Random
sampling can lead to wasted probes, with no additional
information added, making it ineﬀective for the pur-
poses of this attack. The query synthesis technique of
(Wang et al., 2015), generates samples close to the clas-
siﬁcation boundary and spreads the samples along the
boundary, to provide a better learning opportunity. The
approach of (Wang et al., 2015) was developed for the
purpose of active labeling of unlabeled data. We mod-
ify the approach to be used for reverse engineering as
part of the SEE framework, where the attacker learns a
surrogate classiﬁer C(cid:48), based on probing the defender’s
black box C. The SEE implementation of the RE attack
is given below:

• Seed- The seed set consists of one legitimate and

one malicious class sample.

• Explore- The exploration phase (Algorithm 3)
uses the Gram-Schmidt process (Wang et al., 2015)
to generate orthonormal samples, near the mid-
point of any two randomly selected seed points of
opposite classes (Line 8). This has the eﬀect of
generating points close to the separating decision
boundary of the two classes, and also of spreading
the samples along this boundary’s surface, as de-
picted in the exploration phase of Fig. 4. The mag-
nitude of the orthonormal vector is set based on
λi, which is selected as a random value in [0,λmax],
to impart diversity to the obtained set of samples
(Line 10-11). At the end of the exploration phase,
the resulting set of labeled samples (D(cid:48)
Explore), is
used to train a linear classiﬁer of choice, to form
the surrogate reverse engineered model C(cid:48) (Line
19). Fig. 4 shows the reverse engineered model

Algorithm 3: RE Exploration - Using Gram-
Schmidt process.

Input

S eed, Defender black box

: Seed Data D(cid:48)
model C. Parameters: Exploration budget
BExplore, Magnitude of dispersion λmax

Explore, Surrogate

Explore L

Output: Exploration data Set D(cid:48)
classiﬁer C(cid:48)
= Legitimate samples of D(cid:48)
= Malicious samples of D(cid:48)

1 D(cid:48)
2 D(cid:48)
3 for i = 1 .. BExplore do
4

Explore M

S eed

S eed

xL ← Select random samples from D(cid:48)
xL ← Select random samples from D(cid:48)
x0= xL − xM
Generate random vector xR
xR = xR − <xR,x0>

<x0,x0> ∗ x0

Explore L

Explore M

(cid:46) Gram-Schmidt process - xR orthogonal to
x0
λi = random(0, λmax)
xR= λi
norm(xR) *xR
(cid:46) set magnitude of orthogonal midperpendicular
xS =xR + (xL + xM)/2
(cid:46) Set xR to midpoint
if C.predict(xS ) is Legitimate then

D(cid:48)

Explore L ∪ xS

else

5

6

7

8

9

10

11

12

13

14

15

16

17

D(cid:48)

Explore M ∪ xS
= D(cid:48)

Explore L ∪ D(cid:48)

18 D(cid:48)
19 Train C(cid:48) using D(cid:48)

Explore

Explore

Explore M

20

(cid:46) Training can be based on linear classiﬁer of

choice

(red), as learned from the original black box clas-
siﬁer C (green).

• Exploit- The surrogate model C(cid:48), can be used to
generate attacks with high accuracy and diversity.
Ideally, a set of random points can be generated
and veriﬁed against the reverse engineered model
C(cid:48), before adding them to the attack set D(cid:48)
Attack.
However, a practical and eﬃcient way would be
to use the exploration set samples D(cid:48)
Explore of Al-
gorithm 3, as a seed set to generate a set of an-
chor points as in Algorithm 1, with the exception
that we probe C(cid:48) instead of the original model C.
Since C(cid:48) is a locally trained model, probing it does
not impact BExplore. Thus allowing an adversary
to make a large number of probes, at theoretically
zero cost. The anchor points obtained can then be
used to generate the attack samples using Algo-

9

rithm 2. A larger attack radius RExploit can be used
with this attack strategy, as additional validation is
available via the model C(cid:48).

The RE attack strategy is suited for a patient adver-
sary, who spends time/eﬀort to probe the system and
learn it, so as to have an eﬀective attack with high di-
versity. Such attacks, are often hard to detect and stop
by simple blacklisting techniques. However, the success
of this attack relies on the goodness of the reverse en-
gineered model, and could be aﬀected by the nature of
learning employed by the black box.

4. Experimental Evaluation

This section presents experimental evaluation of the
AP and the RE approaches, on classiﬁers trained with
7 real world datasets. Additionally, evaluation on 3
datasets from the cybersecurity domain is presented, to
demonstrate the vulnerabilities of machine learning sys-
tems in adversarial milieus, to exploratory attacks. The
experiments are presented from an adversary’s point of
view, who wishes to have eﬀective attacks with high
accuracy and diversity. Section 4.1 presents the met-
rics and the experimental protocol used, to encourage
reproduce-ability of results. Experimental results and
discussions is presented in Section 4.2

4.1. Experimental Methods and Setup
4.1.1. Adversary Metrics for Attack Quality

An adversary aiming to create maximum impact,
needs to make the set of attack samples D(cid:48)
Attack - Ac-
curate and Diverse. Accuracy ensures that the attack
samples will cause an increase in the false negative rate
of the defender’s model C. While, diversity ensures
that the attack set has enough variability, so that they
can go unnoticed for a long time. These intuitive ideas
are quantiﬁed using 4 proposed quality metrics, to mea-
sure adversary eﬀectiveness. The Eﬀective Attack Rate
(EAR) measures the accuracy of attacks, and the 3 met-
rics: Deviation of attacks (σEA), K-Nearest Neighbor
Distance (KNN-dist) and the Minimum Spanning Tree
distance (MST-dist), collectively represent the diversity
of attacks. The metrics are based on the following deﬁ-
nition of eﬀective attacks (EA):

EA

=

(cid:110)

x : C(x) = Legitimate ∧ x ∈ D(cid:48)

(cid:111)

Attacks

(1)
Based on Eqn. 1, an attack sample is eﬀective if it is
classiﬁed as Legitimate by the black box C. The adver-
sarial quality metrics over the set EA are deﬁned below:

Figure 4: Illustration of RE attacks on 2D synthetic data.(Left - Right): The defender’s model based on training data. The Exploration phase
depicting reverse engineering(red) using the Gram-Schmidt orthonormalization process. The Exploitation attack phase samples generated after
validation from the surrogate classiﬁer (red samples).

a ) Eﬀective Attack Rate (EAR): This is the accuracy
of attacks, measured as the ratio of attack samples
which successfully evade the defenders classiﬁer,
given by Eqn. 2. A value of 1 denotes perfect eva-
sion.

EAR

=

|EA|
(cid:12)(cid:12)(cid:12)D(cid:48)

Attacks

(cid:12)(cid:12)(cid:12)

(2)

b ) Deviation of eﬀective attacks (σEA): This is a
measure of diversity, which computes the spread
of data around its mean, given by Eqn. 3.

σEA

=

(cid:115)

1
|EA − 1|

(cid:88)

xi(cid:15)EA

(xi − µEA)2

(3)

where, µEA indicates the Euclidean mean of sam-
ples in the eﬀective attack set EA. A large value
of σEA indicates that the data has high data space
coverage.

c ) K-Nearest Neighbor distance of eﬀective attacks
(KNN − distEA): This measure of diversity, com-
putes local density information of the data sam-
ples (motivated by (He and Carbonell, 2007)). It
is computed by ﬁnding the average distance of the
K-nearest neighbors of a sample, for all samples
and then averaging this value, as given by Eqn. 4.

KNN − distEA

=

(cid:80)

x(cid:15)EA

(cid:80)K

i=1
K.

dist(x, NNi(x))
|EA|

(4)

Where, the dist(.)
function computes Euclidean
distance between two vectors, and NNi(x) gives the
ith nearest neighbor of a sample x. A higher value
of KNN-dist, indicates that data samples are rel-
atively far from each other and that every sample

10

is in a locally sparse region of space, indicating
higher spread. A value of K=5 is chosen for ex-
perimentation.

d ) Minimum Spanning Tree distance of eﬀective at-
tacks (MS T − distEA): This is also a measure of di-
versity, which is computed by ﬁnding the length of
the minimum spanning tree over the set of EA sam-
ples, as per Eqn. 5 (Lacevic and Amaldi, 2011).
This is a measure which promotes ectropy or col-
location of points, in an attempt to obtain a more
global uniform and diverse spread of samples. This
is especially useful in recognizing multiple locally
dense clusters which are far from each other.

MS T − distEA

=

length(MS T (EA))
|EA| − 1

(5)

The MST measure computes cluster separation
only once, as opposed to pairwise distance metrics
which calculate distance between one point and ev-
ery other point. Thus the MST provides a better
sense of global diversity, by allowing sub groups of
data to have less diversity. A high value of MST-
distance will indicate high diversity.

The three diversity metrics are aﬀected by diﬀerent
data distributions and together they provide a holistic
representation of the variability of the attack data. Stan-
dard deviation captures the overall spread of the data
and is severely aﬀected by outliers. A larger spread
of data results in higher deviation, as seen in Fig. 5
a) where the deviation σ=0.228 is higher than in b),
where the deviation is σ=0.058. Although, deviation
is eﬀective in capturing the global spread of data, it
fails at capturing the local data characteristics. As seen
in Fig. 5 a) and c), which have very close deviation

(a) σ=0.228; KNN-dist=0.071;
MST-dist=0.056

(b) σ=0.058; KNN-dist=0.018;
MST-dist=0.015

(c) σ=0.218; KNN-dist=0.017;
MST-dist=0.037

(d) σ=0.226; KNN-dist=0.017;
MST-dist=0.063

Figure 5: Values of σ, KNN-dist and MST-dist for diﬀerent 2D synthetic data distributions over 100 test points.

values (∆σ=0.002), but totally diﬀerent distribution of
data. These diﬀerences are caught by the KNN − dist
metric, which is higher for scattered data (Fig. 5 a),
KNN-dist=0.071), as compared to closely packed data
(Fig. 5 c), KNN-dist=0.017). However, the KNN-dist
metric does not account for disjoint clusters spread out
in space, as its a local measure and is myopic in scope.
This distinction is caught eﬀectively by the MST-dist
metric, which shows a signiﬁcant diﬀerence in the diver-
sity values for Fig. 5 c) and d) (∆MS T − dist=0.026),
even though the KNN-dist metric shows no diﬀerence
between the two. The MST metric is suitable for at-
tacks such as the Anchor Points attacks, where the at-
tacks are concentrated around a few ground truth points,
but the ground truth points themselves are spread out in
space. The three metrics together represent the variabil-
ity of the samples, in high dimensional spaces, where
a visual examination of the data is not possible. The
subscript EA is omitted in the representation of the di-
versity metrics, through the rest of the paper, with the
implicit understanding that these metrics are computed
over the eﬀective attacks set only.

4.1.2. Description of Datasets Used

Experimental evaluation is performed on 10 real
world datasets, the details of which are presented in Ta-
ble 2. The ﬁrst 7 datasets were chosen from the UCI ma-
chine learning (Lichman, 2013) repository and are pop-
ularly used for classiﬁcation tasks, in literature. These
datasets do not traditionally embody any security risks,
but were chosen to evaluate the vulnerability of classi-
ﬁers in the diﬀerent data domains and distributions. The
Spambase6(Lichman, 2013), KDD997(Lichman, 2013)
and the CAPTHCA (DSouza, 2014) datasets are binary

6https://archive.ics.uci.edu/ml/datasets/Spambase
7http://kdd.ics.uci.edu/databases/kddcup99/

kddcup99.html

11

Table 2: Description of datasets used for experimentation of SEE
framework

Dataset
Digits08
Credit
Cancer
Qsar
Sonar
Theorem
Diabetes
Spambase
KDD99
CAPTCHA 1885

#Instances
1500
1000
699
1055
208
3060
768
4600
494021

#Dimensions
16
61
10
41
60
51
8
57
41
26

classiﬁcation tasks, which represent 3 diﬀerent cyberse-
curity domains that use machine learning as a core tech-
nique. The Spambase dataset, contains data about spam
emails (such as fraud schemes, ads, etc) and legitimate
personal and work emails. The KDD99 dataset is a net-
work intrusion detection dataset, to classify normal con-
nections from diﬀerent classes of attack connections.
The CAPTCHA dataset was developed in (DSouza,
2014), for the task of blocking bots from human users,
based on their mouse movement patterns, while solving
a visual image based behavioral CAPTCHA puzzle.

All datasets were pre-processed by ﬁrst reducing
them to a binary class problem. The Digits dataset
was reduced to have samples of the digit 0 and 8 only,
KDD99 was reduced to represent only two classes - at-
tacks and normal. The dataset was then converted to
contain only numerical values by transforming categori-
cal and nominal features to binary variables. The result-
ing number of features is shown in Table 2. The data
was then normalized to the range of [0,1].
Instances
were shuﬄed to remove any bias due to inherent con-
cept drift. In all datasets, the class label 1 is taken to
be the Malicious class and 0 is taken as the Legitimate

class, as convention.

4.2.1. Experiments with Linear Defender Model

4.1.3. Experimental Protocol and Setup

All experiments begin with a seed sample, which is
obtained by random sampling in the feature space. The
Anchor Points(AP) attack requires only one legitimate
seed sample while the Reverse Engineering(RE) attack
requires one legitimate and one malicious sample. The
seed phase concludes when the minimum required seed
samples are obtained. The exploration probing bud-
get BExplore is taken as 1000 samples and the num-
ber of attack samples required NAttack is taken as 2000.
For the AP attack, the neighborhood radius [Rmin, Rmax]
is set at [0.1,0.5] and the exploitation radius is set at
RExploit=0.1. In case of the RE attack, a larger exploita-
tion radius is taken as RExploit=0.5, due to additional val-
idation from the surrogate learned classiﬁer C(cid:48). Eﬀects
of varying this radius values are also presented in the
analysis. The magnitude of dispersion λmax is taken as
0.25, and it was found that changing this had little im-
pact on the ﬁnal results. The adversary’s reverse engi-
neered model is taken as a linear kernel SVM with a
high regularization constant (c=10). This ensures that
the model is robust and does not overﬁt to the explored
samples, which are limited and inadequate to general-
ize over the entire space. All experimentation was per-
formed using Python 2.78 and the scikit-learn machine
learning library (Pedregosa and et al, 2011). The results
presented are averaged over 30 runs for every experi-
ment.

4.2. Experimental Results and Analysis

Experimental analysis is presented here, by consider-
ing diﬀerent models for the defender’s black box, and
measuring its impact on the adversary’s eﬀectiveness.
Section 4.2.1 presents the results of a symmetric case,
where both the adversary and the defender have simi-
lar model types (linear in this case). Results of a non
symmetric setting are presented in Section 4.2.2, where
we consider 4 diﬀerent model types for the defender,
while the adversary, agnostic of these changes, still em-
ploys a linear model. Experiments on a truly remote
black box model is presented in Section 4.2.3, where
we present experiments performed on Google’s Cloud
Prediction API. Eﬀects of parameters on the adversary’s
performance is presented in Section 4.2.4.

8www.python.org

12

Experiments in this section consider a linear model
for the defender’s classiﬁer C. A linear kernel SVM
(regularization parameter, c=1) is considered. This in-
formation is not available to the adversary, who is ca-
pable of accessing this model only via probing upto a
budget BExplore=1000.

The results of the Seed and Exploration phase are
presented in Table 3. The initial accuracy of the de-
fender, as perceived by cross-validation on its training
dataset before deployment, is seen in Column 2 of Ta-
ble 3. A high accuracy (> 70%) is seen across all the
datasets. The seed phase uses random sampling in the
feature space to ﬁnd seed samples. No more than 50
samples, on average, were needed for ﬁnding seeds to
start the attack process. The number of Anchor Points
obtained is seen to be > 50% of BExplore, indicating the
ability to launch an AP attack on all 10 high dimensional
domains. For the RE attack, the reverse engineering ac-
curacy of model C(cid:48) is computed by evaluating it on the
original dataset, as an adhoc metric of C(cid:48)’s understand-
ing of the original data space and the extent of reverse
engineering.

After the exploration phase, 2000 attack samples are
generated in the exploitation phase. The Eﬀective At-
tack Rate (EAR) and diversity metrics are presented in
Table 4 for both the AP and the RE attacks. It is seen
that an EAR of 97.7% in the case of AP and > 91.2%
for the RE attacks, is obtained on average. This is seen
even though the defender’s model is perceived to have
a high accuracy as per Table 3. Accuracy of classi-
ﬁers is of little signiﬁcance if the model can be easily
evaded. The high eﬀective attack rate for all 10 cases,
highlight the vulnerability of classiﬁcation models and
the misleading nature of accuracy, in an adversarial en-
vironment, irrespective of the data application domain.
The high EAR of the RE attacks, indicate that partial
reverse engineering and the linear approximation of the
defender’s model surface is suﬃcient to launch an eﬀec-
tive attack against it. This can be seen for the KDD99
dataset, which has a reverse engineering accuracy of
55% while its EAR for the RE attack was 93%. This
is because, generating a high accuracy on the training
dataset is not the goal of the RE approach. It is more
concerned with generating a large number of diverse at-
tack samples which would be classiﬁed as legitimate.
This is possible even with partial reverse engineering.
While a high RE accuracy indicates a high EAR (con-
sider Cancer dataset), it is not a required condition for
the RE attack, making it of practical use in high dimen-
sional spaces.

Table 3: Results of Seed and Exploration phases, with linear defender model

Dataset

Digits08
Credit
Cancer
Qsar
Sonar
Theorem
Diabetes
Spambase
KDD99
CAPTCHA

Defender’s
Initial Accuracy
98%
79%
97%
87%
88%
72%
78%
91%
99%
100%

Random probes
to ﬁnd seed
4.6±2.63
3.13±1.89
42.91±29.36
49.5±28.81
24.03±18.92
4.07±2.52
2.93±1.23
20.64±12.93
6.07±4.23
7.27±5.35

Explored Anchor
Points/BExplore
0.63±0.01
0.71±0.01
0.99±0.01
0.99±0.01
0.98±0.01
0.67±0.02
0.50±0.02
0.50±0.02
0.91±0.01
0.92±0.01

Accuracy of
RE model C(cid:48)
92%
71%
95%
42%
61%
57%
71%
59%
55%
91%

The diversity of the RE attacks is higher than the AP
attacks, on all three metrics, indicating - a larger spread
of attacks, lower collocation of points and a uniform dis-
tribution in the attack space. This high diversity is ob-
tained for RE, while still maintaining a reasonable high
attack rate. The AP attacks, produces lower diversity
but has high attack accuracy than the RE attacks. This
is because the number of explored anchor points was
> 50% (Table 3), allowing a large scale AP attack to be
feasible. The AP attack is therefore an attractive quick
attack strategy in high dimensional spaces, irrespective
of the attack domain, application type and the model
used. The eﬀectiveness of the RE attack depends on the
ability of the surrogate model C(cid:48) to represent the space
of Legitimately classiﬁed samples by C. The reverse en-
gineering task is dependent on the availability of enough
probing budget and the complexity of the boundary rep-
resented by C. This is the cause for the higher variability
in the EAR values for RE attacks in Table 4, as opposed
to the AP attacks, where attacks are more tightly packed
with the obtained anchor points, leading to lower vari-
ability.

The RE approach’s EAR is low for the Credit, the
In case of the
Theorem and the Spambase datasets.
Credit and Theorem dataset, the defender’s accuracy is
low, indicating a nonlinear separation/ inseparability of
samples. The RE accuracy approaches close to the de-
fender’s accuracy, but since the original model C has
low accuracy, the reverse engineered model can only
be so good. For the Spambase dataset, the majority of
the features follow a heavy tailed distribution as shown
for Feature #5 in Fig. 6. In such distributions, random
sampling in the range [0,1] on each features is not the
best choice.
Integrating domain information which is
commonly known, as in the case of text datasets having
heavy tails, can be beneﬁcial. However, following a do-

Figure 6: Distribution of Feature #5 for Spambase dataset, showing a
heavy tail. (Red - Malicious, Blue - Legitimate)

main agnostic approach here, a 71% attack rate is still
achieved, indicating the viability of such attacks.

4.2.2. Experiments with Non-Linear Defender Model

The SEE framework considers a black box model for
C. As such, it is developed as a generic data driven at-
tack strategy, irrespective of the defender’s model type,
training data or the model parameters. To demon-
strate the eﬃcacy of these attacks under a variety of
defender environments, experiments with diﬀerent non
linear black box models for C are presented here. Par-
ticularly, the following defender models were evalu-
ated: K-Nearest Neighbors classiﬁer with k=3 (kNN)
(Cover and Hart, 1967), SVM with an radial basis func-
tion kernel with gamma of 0.1 (SVM-RBF) (Xiaoyan,
2003), C4.5 Decision Tree (DT)(Quinlan, 1993), and a
Random Forest of 50 models (RF)(Breiman, 2001), as
shown in Table 5. The attacker’s model is kept the same
as before and the experiments are repeated for each of
the defender’s model. Average values of EAR over 30
runs are reported in Table 5.

The AP approach is minimally aﬀected by the choice
of defender’s model, with Table 5 showing a high EAR
for all defender models. The drop in case of Spam-
base, is attributed to the heavy tailed distributions as

13

Table 4: Results of accuracy and diversity of AP and RE attacks, with linear defender model

Dataset

Digits08

Credit

Cancer

Qsar

Sonar

Theorem

Diabetes

Spambase

KDD99

CAPTCHA

Method
AP
RE
AP
RE
AP
RE
AP
RE
AP
RE
AP
RE
AP
RE
AP
RE
AP
RE
AP
RE

EAR
0.96±0.01
0.93±0.06
0.98±0.01
0.80±0.15
0.99±0.01
0.99±0.01
1
0.99+0.01
0.99±0.01
0.98±0.01
0.97±0.01
0.87±0.08
0.98±0.01
0.95±0.04
0.93±0.01
0.71±0.2
0.99±0.01
0.93±0.04
0.99±0.01
0.97±0.02

σ
0.23±0.002
0.273±0.009
0.218±0.001
0.265±0.001
0.215±0.001
0.263±0.001
0.216±0.001
0.264±0.001
0.215±0.001
0.265±0.001
0.219±0.002
0.267±0.002
0.217±0.003
0.262±0.001
0.233±0.003
0.273±0.004
0.215±0.001
0.263±0.001
0.215±0.001
0.264±0.001

KNN-dist MST-dist
0.41±0.01
0.48±0.01
0.65±0.04
0.76±0.01
1.01±0.02
1.19±0.02
1.72±0.31
2.22±0.02
0.33±0.01
0.38±0.01
0.45±0.01
0.5±0.01
0.94±0.01
1.1±0.01
1.64±0.01
1.71±0.01
1.16±0.01
1.37±0.01
2.1±0.015
2.22±0.01
0.89±0.02
1.05±0.02
1.64±0.15
1.96±0.02
0.23±0.01
0.27±0.01
0.31±0.01
0.36±0.01
0.79±0.02
0.96±0.02
1.39±0.4
2.04±0.06
0.91±0.01
1.06±0.01
1.53±0.06
1.71±0.01
0.68±0.01
0.80±0.01
1.12±0.03
1.22±0.01

explained in Fig. 6. In case of the decision trees, the
model trained for Spambase, focuses only on a few key
features to perform the classiﬁcation. Random probing
attacks, space out the attack samples across dimensions,
without considering their feature importance to classiﬁ-
cation. This leads to skipping over the key features in
the attack generation, making the attacks less eﬀective.
However, this could be compensated by performing par-
tial reverse engineering and using a smaller exploitation
radius.

The RE results are signiﬁcantly dependent on the de-
fender’s choice of model. In case of nonlinear data sep-
aration, as in the Credit and the Theorem datasets, the
linear approximation is a bad choice and this is reﬂected
In all other cases, the low at-
in the low attack rate.
tack rate is attributed to the over simpliﬁcation of the
understanding of the models, which in case of the de-
cision tree and random forest tend to be complicated in
high dimensional spaces. However, in a majority of the
cases it is seen that a 50% attack rate is still possible
with the same linear SVM model used by the adver-
sary. This makes the SEE framework generally appli-
cable to attack classiﬁcation systems, without explicit
assumptions about model types, application domain or
the parameters of classiﬁcation. The eﬃcacy of these
approaches, highlights the vulnerability of classiﬁers to
purely data driven attacks, requiring only feature space

information.

4.2.3. Experiments with Google Cloud Prediction Ser-

vice

To demonstrate the applicability of the RE and the AP
techniques on real world remote black box classiﬁers,
we performed experiments using the Google Cloud Pre-
diction API9. This API provides machine learning-as-
a-service, by allowing users to upload datasets to train
models, and then use the trained model to perform pre-
diction on new incoming samples. Google’s Prediction
API, provides a black box prediction system, as they
have not disclosed the model type or the technique used
for learning, to the best of our knowledge. As such, this
provides for an ideal test of the SEE’s attack models,
where the defender is remote, accessed from a client and
has no information about the defender’s models (Paper-
not et al., 2016a). We use the API’s Python client library
to access the cloud service, and the results on the three
cybersecurity datasets are shown in Table 6.

The results of the experiment demonstrate that the
AP and the RE attacks are eﬀective in attacking the de-
fender’s classiﬁer, by generating a high EAR over all
datasets. The diversity of the RE approach is seen to

9https://cloud.google.com/prediction/

14

Table 5: Eﬀective Attack Rate (EAR) of AP and RE attacks, with non linear defender’s model (Low EAR values are italicized.)

Dataset
Digits08
Credit
Cancer
Qsar
Sonar
Theorem
Diabetes
Spambase
KDD99
Captcha

kNN

AP
0.89
0.96
0.99
1
0.99
0.97
0.99
0.93
0.99
0.99

RE
0.96
0.78
0.99
0.99
0.98
0.813
0.935
0.99
0.93
0.92

SVM-RBF
RE
AP
0.89
0.97
0.53
0.94
0.99
0.99
0.99
0.99
1
1
0.5
0.95
0.9
0.99
0.48
0.84
0.99
1
0.92
0.99

DT

RF

AP
0.87
0.79
0.97
0.96
0.97
0.95
0.83
0.08
0.89
0.97

RE
0.63
0.42
0.89
0.76
0.62
0.79
0.63
0.11
0.54
0.83

AP
0.85
0.79
0.99
0.99
0.99
0.62
0.88
0.99
0.92
0.93

RE
0.48
0.33
0.98
0.99
0.95
0.78
0.61
0.98
0.27
0.89

be higher for the RE attacks on all three metrics of σ,
KNN − dist and MS T − dist, indicating the variabil-
ity of attacks achieved using the RE approach, in a real
world setting. Furthermore, the RE accuracy in case
of the Spambase dataset (48.1%) highlights that, lin-
ear approximation and partial reverse engineering are
suﬃcient to launch an eﬀective RE attack (EAR=1).
These experiments use the same exploration budget
(BExplore=1000) as the previous sections, to generate at-
tacks of high accuracy and high diversity.
In a truly
blind-folded setting, where we have no prior informa-
tion about the defender’s classiﬁer, a budget of 1000 (≈
$0.5)10 samples indicates the relative ease with which
classiﬁers can be evaded and the need for a more com-
prehensive defense strategy, beyond a static machine
learning model.

4.2.4. Eﬀects of Varying BExplore and RExploit

In evaluating the AP and RE approaches, the RExploit
was kept ﬁxed at 0.1 for AP and 0.5 for RE. This was
intuitively motivated, as conﬁdence in attacks would re-
duce as distance from anchor points increases, as they
are the only ground truth information available to the at-
tackers in the AP strategy. Eﬀect of increasing RExploit,
on the accuracy and diversity of AP attacks, is shown in
Fig. 7. The Credit, Theorem and the Spambase datasets
were chosen for these evaluation, as they have low EAR
for the RE approach (Table 4) and could therefore bene-
ﬁt from parameter tuning. Eﬀect of increasing RExploit to
increase diversity of AP attacks, and increasing BExplore
to increase EAR of RE attacks, as viable alternatives to
improve performance over these three datasets is ana-
lyzed and presented.

10https://cloud.google.com/prediction/pricing

15

The eﬀective attack rate (EAR) reduces with an in-
crease in the exploitation radius, as seen in Fig. 7a),
because attack samples move away from the anchor
points. There is an associated increase in the diver-
sity using both KNN-dist and MST-dist measures, as
shown in c) and e). Comparison of diversity and EAR
at RExploit=0.5 for the RE and AP approach shows, that
for increasing diversity it is much better to switch to the
RE approach instead of increasing RExploit arbitrarily, as
the eﬀectiveness of attacks starts dropping rapidly with
increased radius. The drop in MST-dist in Fig. 7 e) is
due to the reduction of the size of the Eﬀective Attack
set (EA).

As increasing diversity for AP approach leads to a
drop in EAR, we investigate if we can increase the EAR
of the RE approach while maintaining its high diversity.
Increasing the exploration budget increases the EAR,
due to availability of labeled training data for the re-
verse engineered model, leading to better learning of the
data space. The increase in EAR ultimately plateaus,
as per the Probably Approximate Learning(PAC) prin-
ciples (Haussler, 1990), indicating that it is not neces-
sary to arbitrarily keep increasing this budget. The knee
point is seen in Fig. 7b) (around 1500 for all datasets).
After the knee point, the EAR of all three datasets is
85%, and adding more probing budget has little impact
on the EAR or the diversity(Fig. 7 d, f). It is necessary to
have suﬃcient probing budget to reach this knee point,
to allow eﬀective reverse engineering in complex data
spaces. This extra eﬀort provides long term beneﬁts as
it leads to increased diversity of attacks. RE is suitable
for patient adversaries who want to apply data science
in breaking the system. In case of a low budget the AP
approach is more suitable, but with the RE strategy the
assumption is that the adversary wants to spend time to
learn the system before attempting an attack

Table 6: Results of AP and RE attacks using Google Cloud Prediction API as the defender’s black box

Training
Accuracy

EAR
σ
KNN-dist
MST-dist
Accuracy of
RE model C(cid:48)

Spambase

KDD99

CAPTCHA

93%

99%

100%

Attack Metrics
RE
1
0.264
2.148
2.078

AP
1
0.218
1.105
0.944

AP
1
0.216
1.324
1.127

RE
1
0.265
1.714
1.645

AP
0.99
0.218
0.813
0.695

RE
0.97
0.265
1.228
1.131

48.1%

97.2%

100%

(a) Eﬀect of RExploit on EAR

(b) Eﬀect of RExploit on KNN-dist

(c) Eﬀect of RExploit on MST-dist

(d) Eﬀect of BExplore on EAR

(e) Eﬀect of BExplore on KNN-dist

(f) Eﬀect of BExplore on MST-dist

Figure 7: Eﬀect of changing RExploit, for the AP attacks (Top), and BExplore, for the RE attacks (Bottom), on the Eﬀective Attack Rate (EAR) and
Diversity (KNN-dist, MST-dist)

5. Why diversity is an important consideration in

designing attacks?

Throughout the design and evaluation of the SEE
framework, diversity of attacks has been considered as
an important goal for the adversary. This was intu-
itively motivated, as diversity ensures that the attacks
have enough variability, so as to make its detection and
prevention diﬃcult. In this section, we quantify the ef-
fects of diversity on the ability to thwart defenses, espe-
cially those based on blacklisting of samples. Blacklists
are ubiquitous in security applications, as an approach
to ﬂag and block known malicious samples (Kantche-
lian et al., 2013). Modern blacklists are implemented
using approximate matching techniques, such as Local-
ity Sensitive Hashing, which can detect perturbations

to existing ﬂagged samples (Prakash et al., 2010). The
goal of an attacker is to avoid detection by these black-
lists, as they can make a large number of attack samples
unusable with a quick ﬁltering step. With high diver-
sity, it is unlikely that blacklisting a few samples will
cause the attack campaign to stop. In case of a diverse
attack, the defender will have to resort to choosing be-
tween maintaining a huge blacklist of samples, or to re-
model the machine learning system, both of which are
expensive tasks and require time.

To empirically evaluate the eﬀect of diversity on
blacklisting, a synthetic blacklisting experiment is pre-
sented, which simulates the eﬀect of approximate
matching ﬁlters. The blacklist is maintained as a list
BL, of previously seen attack samples, with an associ-
ated approximation factor: (cid:15). An attack is detected if

16

the exploration budget is kept ﬁxed at 1000, the ex-
ploitation samples NAttack=2000 and an additional 2000
samples(NAttack New) are generated to test the blacklist-
ing eﬀects.

From Fig. 8a), it is seen that the AP approach has a
higher percentage of stopped attacks than the RE ap-
proach, across all datasets. This is a result of the higher
diversity of the RE attacks as seen from the MST-dist
metric in Fig. 8 b). The high diversity of the RE at-
tack causes blacklisting to be totally ineﬀective (0 at-
tacks stopped) for 8 out of the 10 datasets.
In these
cases, increasing the (cid:15), as a countermeasure, to stop
attacks is not a viable option, due to additional false
alarms caused. A higher diversity in RE would force
the reevaluation of the security system, leading to re-
design, feature engineering and collection of additional
labeled samples. All these are time taking and expen-
sive eﬀorts, making the RE approach eﬀective as an at-
tack strategy. As such, if an attacker is sophisticated
and has enough probing budget BExplore, it can launch
a diverse attack campaign, which is harder to stop by
adhoc security measures. The AP attack provides for
high accuracy and precise attacks, as was seen in Ta-
ble 4. However, a signiﬁcant portion of this attack cam-
paign (45%, on average) is stopped by using a black-
list capable of approximate matching. This experiment
aims to highlight the eﬀect of diversity and does not
claim to be a concrete defense mechanism. Neverthe-
less, it intends to be a motivation for further analysis
into the eﬃcacy of incorporating such techniques for
designing secure machine learning frameworks. Black-
lists capable of heuristic matching, could empower clas-
siﬁers (Kantchelian et al., 2013) with the ability to rec-
ognize perturbed attack samples and as such continue
to be eﬀective against anchor points attacks.
In case
of reverse engineering attacks, a blacklisting based ap-
proach was seen to be ineﬀective. Taking preemptive
counter-measures before deploying the classiﬁer, to suc-
cessfully detect and mislead attacks, could be a promis-
ing direction for dealing with these attacks (Hong and
Kim, 2016).

6. Conclusion and Future Work

In this paper, an adversary’s view of machine learn-
ing based cybersecurity systems is presented. The pro-
posed Seed-Explore-Exploit framework, provides a data
driven approach to simulate probing based exploratory
attacks on classiﬁers, at test time. Experimental evalua-
tion on 10 real world datasets shows that, even models
having high perceived accuracy (>90%), can be eﬀec-
tively circumvented with a high evasion rate (>95%).

(a) Percentage of attacks stopped by blacklist

(b) MST-dist of AP and RE attacks

Figure 8: Relationship between diversity and ability to thwart attacks
by blacklisting samples.

a new sample falls within (cid:15) distance of any sample in
the blacklist BL. The entire blacklisting process is simu-
lated as follows: i) the attackers use the SEE framework
to generate NAttacks attack samples which are submitted
to the defender model C, ii) the defender is assumed to
gain information over time about these NAttacks samples
and then proceeds to blacklist them by storing them in
BL, iii) The attacker, still unaware of the blacklisting,
continues to use its existing explored information (AP
or RE model) to generate additionally more NAttacks New
attack samples. The eﬀectiveness of the blacklisting
process is computed as the number of eﬀective attacks
in NAttacks New, which are detected by BL. The percent-
age of attacks stopped, indicates eﬀectiveness of black-
lists and consequently the eﬀect of diversity. A small
rate would indicate that blacklisting is not eﬀective in
stopping such attacks.

√

Results of the blacklisting experiment, with an ap-
proximation factor of (cid:15) = 0.1, on the UCI datasets is
shown in Fig. 8. In order to balance eﬀects of approxi-
mation across datasets, the approximation factor is mul-
d, where d is the number of dimensions of
tiplied by
the dataset. The (cid:15) is chosen so as to balance the eﬃ-
cacy of the blacklists to its false positive rate. Increas-
ing the approximation factor leads to an increase in the
number of false positives, which causes legitimate sam-
ples to be misclassiﬁed as attacks. Limiting false posi-
tives is essential, as a blacklist which causes too many
false alarms would be impractical. In all experiments,

17

These attacks assumed a black box model for the de-
fender’s classiﬁer and were carried out agnostic of the
type of classiﬁer, its parameters and the training data
used. Evaluation results considering 4 diﬀerent non-
linear classiﬁer’s and considering the Google Cloud
Platform based prediction service, for the defender’s
black box, demonstrates the innate vulnerability of ma-
chine learning in adversarial environments, and the mis-
leading nature of accuracy in providing a false sense of
security. Also, the ability to reverse engineer the de-
fender’s classiﬁer, and subsequently launch diverse at-
tacks was demonstrated. Attacks with high diversity
were shown to be more potent, as they can avoid de-
tection and thwarting, by countermeasures employing
blacklists and approximate matching.

The purpose of this work is to make the model de-
signers aware of the nature of attacks that invade classi-
ﬁcation systems, from a purely data driven perspective.
Wearing the white hat, we draw attention to the vul-
nerabilities introduced by using classiﬁers in cyberse-
curity systems. As Sun Tzu says in The Art of War- ’To
know your Enemy, you must become your Enemy’(Tzu,
1963). We hope that this work serves as background
and motivation for the development and testing of novel
machine learning based security frameworks and met-
rics. Use of moving target defense strategies (Hong and
Kim, 2016) and dynamic adversarial drift handling tech-
niques (Kantchelian et al., 2013; Sethi et al., 2016a), are
promising directions warranting further research. Fu-
ture work will concentrate on developing and analyzing
preemptive strategies, in the training phase of the classi-
ﬁers, to facilitate reliable attack detection and relearning
for eﬀective recovery.

References

References

Abramson, M., 2015. Toward adversarial online learning and the sci-
ence of deceptive machines. In: 2015 AAAI Fall Symposium Se-
ries.

Akhtar, Z., Biggio, B., Fumera, G., Marcialis, G. L., 2011. Robust-
ness of multi-modal biometric systems under realistic spoof attacks
against all traits. In: BIOMS 2011. IEEE, pp. 1–6.

Alabdulmohsin, I. M., Gao, X., Zhang, X., 2014. Adding robustness
to support vector machines against adversarial reverse engineering.
In: Proceedings of 23rd ACM CIKM. ACM, pp. 231–240.

Barreno, M., Nelson, B., Sears, R., Joseph, A. D., Tygar, J. D., 2006.
Can machine learning be secure? In: Proceedings of the 2006
ACM Symposium on Information, computer and communications
security. ACM, pp. 16–25.

Biggio, B., Corona, I., Maiorca, D., Nelson, B., ˇSrndi´c, N., Laskov, P.,
Giacinto, G., Roli, F., 2013. Evasion attacks against machine learn-
ing at test time. In: Machine Learning and Knowledge Discovery
in Databases. Springer, pp. 387–402.

18

Biggio, B., Fumera, G., Roli, F., 2014a. Pattern recognition systems
under attack: Design issues and research challenges. International
Journal of Pattern Recognition and Artiﬁcial Intelligence 28 (07),
1460002.

Biggio, B., Fumera, G., Roli, F., 2014b. Security evaluation of pattern
classiﬁers under attack. IEEE transactions on knowledge and data
engineering 26 (4), 984–996.

Bilge, L., Dumitras, T., 2012. Before we knew it: an empirical
study of zero-day attacks in the real world. In: Proceedings of the
2012 ACM conference on Computer and communications security.
ACM, pp. 833–844.

Breiman, L., 2001. Random forests. Machine learning 45 (1), 5–32.
Chawla, N. V., Bowyer, K. W., Hall, L. O., Kegelmeyer, W. P., 2002.
Smote: synthetic minority over-sampling technique. Journal of ar-
tiﬁcial intelligence research 16, 321–357.

Chen, J., Xin, B., Peng, Z., Dou, L., Zhang, J., 2009. Opti-
mal contraction theorem for exploration–exploitation tradeoﬀ in
search and optimization. IEEE Transactions on Systems, Man, and
Cybernetics-Part A: Systems and Humans 39 (3), 680–691.

Cover, T. M., Hart, P. E., 1967. Nearest neighbor pattern classiﬁcation.

Information Theory, IEEE Transactions on 13 (1), 21–27.

DSouza, D. F., 2014. Avatar captcha: telling computers and humans
apart via face classiﬁcation and mouse dynamics. Electronic The-
ses and Dissertations-1715.

Group, B. D. W., et al., 2013. Big data analytics for security intelli-

gence. Cloud Security Alliance.

Guerra, P. H. C., , et al., 2010. Exploring the spam arms race to char-
acterize spam evolution. In: Proceedings of the 7th Collaboration,
Electronic messaging, Anti-Abuse and Spam Conference (CEAS).
Citeseer.

Haussler, D., 1990. Probably approximately correct learning. Univer-
sity of California, Santa Cruz, Computer Research Laboratory.
He, J., Carbonell, J. G., 2007. Nearest-neighbor-based active learning
for rare category detection. In: Advances in neural information
processing systems. pp. 633–640.

Hong, J. B., Kim, D. S., 2016. Assessing the eﬀectiveness of mov-
ing target defenses using security models. IEEE Transactions on
Dependable and Secure Computing 13 (2), 163–177.

Huh, J. H., Kim, H., 2011. Phishing detection with popular search
engines: Simple and eﬀective. In: Foundations and Practice of Se-
curity. Springer, pp. 194–207.

Kantchelian, A., Afroz, S., Huang, L., Islam, A. C., Miller, B.,
Tschantz, M. C., Greenstadt, R., Joseph, A. D., Tygar, J., 2013.
Approaches to adversarial drift. In: Proceedings of the 2013 ACM
workshop on Artiﬁcial intelligence and security. ACM, pp. 99–
110.

Lacevic, B., Amaldi, E., 2011. Ectropy of diversity measures for pop-
ulations in euclidean space. Information Sciences 181 (11), 2316–
2339.

Li, H., Chan, P. P., 2014. An improved reject on negative impact de-
fense. In: International Conference on Machine Learning and Cy-
bernetics. Springer, pp. 452–459.

Li, Y., Yeung, D. S., 2014. A causative attack against semi-supervised
learning. In: Machine Learning and Cybernetics. Springer, pp.
196–203.

Lichman, M., 2013. UCI machine learning repository.

URL http://archive.ics.uci.edu/ml

Lowd, D., Meek, C., 2005a. Adversarial learning. In: Proceedings of

the 11th ACM SIGKDD. ACM, pp. 641–647.

Lowd, D., Meek, C., 2005b. Good word attacks on statistical spam

ﬁlters. In: CEAS.

Nelson, B.,

, et al., 2012. Query strategies for evading convex-
inducing classiﬁers. The Journal of Machine Learning Research
13 (1), 1293–1332.

Nelson, B., et al., 2010. Near-optimal evasion of convex-inducing

ACM SIGKDD international conference on Knowledge discovery
and data mining. ACM, pp. 1059–1067.

ˇZliobait˙e, I., 2010. Learning under concept drift: an overview. arXiv

preprint arXiv:1010.4784.

classiﬁers. arXiv preprint arXiv:1003.2751.

Papernot, N., McDaniel, P., Goodfellow, I., 2016a. Transferability in
machine learning: from phenomena to black-box attacks using ad-
versarial samples. arXiv preprint arXiv:1605.07277.

Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B.,
Swami, A., 2016b. The limitations of deep learning in adversar-
ial settings. In: 2016 IEEE European Symposium on Security and
Privacy (EuroS&P). IEEE, pp. 372–387.

Pastrana Portillo, S., 2014. Attacks against intrusion detection net-
works: evasion, reverse engineering and optimal countermeasures.
Doctoral Theses, Universidad Carlos III de Madrid.

Pedregosa, F., et al, 2011. Scikit-learn: Machine learning in Python

12, 2825–2830.

Prakash, P., Kumar, M., Kompella, R. R., Gupta, M., 2010. Phishnet:
predictive blacklisting to detect phishing attacks. In: INFOCOM,
2010 Proceedings IEEE. IEEE, pp. 1–5.

Quinlan, J. R., 1993. C4. 5: Programming for machine learning. Mor-

gan Kauﬀmann.

Rndic, N., Laskov, P., 2014. Practical evasion of a learning-based clas-
siﬁer: A case study. In: Security and Privacy (SP), 2014 IEEE
Symposium on. IEEE, pp. 197–211.

Saha, A., Sanyal, S., 2014. Application layer intrusion detection with
combination of explicit-rule-based and machine learning algo-
rithms and deployment in cyber-defence program. arXiv preprint
arXiv:1411.3089.

Sethi, T. S., Kantardzic, M., Arabmakki, E., 2016a. Monitoring clas-
siﬁcation blindspots to detect drifts from unlabeled data. In: 17th
IEEE International Conference on Information Reuse and Integra-
tion (IRI). IEEE.

Sethi, T. S., Kantardzic, M., Hu, H., 2016b. A grid density based
framework for classifying streaming data in the presence of con-
cept drift. Journal of Intelligent Information Systems 46 (1), 179–
211.

Sethi, T. S., Kantardzic, M., Ryu, J. W., 2017. Security theater: On the
vulnerability of classiﬁers to exploratory attacks. In: (Under Con-
sideration) 12th Paciﬁc Asia Workshop on Intelligence and Secu-
rity Informatics. Springer.

Smutz, C., Stavrou, A., 2016. When a tree falls: Using diversity in
ensemble classiﬁers to identify evasion in malware detectors. In:
NDSS Symposium.

Tram`er, F., Zhang, F., Juels, A., Reiter, M. K., Ristenpart, T.,
2016. Stealing machine learning models via prediction apis. arXiv
preprint arXiv:1609.02943.

Tzu, S., 1963. The art of war. edited by samuel b. griﬃth.
Wagner, D., Soto, P., 2002. Mimicry attacks on host-based intrusion
detection systems. In: Proceedings of the 9th ACM Conference on
Computer and Communications Security. ACM, pp. 255–264.
Walgampaya, C., Kantardzic, M., 2011. Cracking the smart clickbot.
In: 13th IEEE International Symposium on Web Systems Evolu-
tion (WSE),. IEEE, pp. 125–134.

Wang, L., Hu, X., Yuan, B., Lu, J. g., 2015. Active learning via
query synthesis and nearest neighbour search. Neurocomputing
147, 426–434.

Xiaoyan, W. P. Z., 2003. Model selection of svm with rbf kernel and
its application. Computer Engineering and Applications 24, 021.
Xu, L., Zhan, Z., Xu, S., Ye, K., 2014. An evasion and counter-evasion
study in malicious websites detection. In: Communications and
Network Security (CNS), 2014 IEEE Conference on. IEEE, pp.
265–273.

Xu, W., Qi, Y., Evans, D., 2016. Automatically evading classiﬁers. In:
Proceedings of the Network and Distributed Systems Symposium.
Zamani, M., Movahedi, M., 2013. Machine learning techniques for

intrusion detection. arXiv preprint arXiv:1312.2177.

Zhou, Y., Kantarcioglu, M., Thuraisingham, B., Xi, B., 2012. Adver-
sarial support vector machine learning. In: Proceedings of the 18th

19

