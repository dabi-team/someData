1
2
0
2

p
e
S
2
2

]

R
C
.
s
c
[

1
v
9
3
8
0
1
.
9
0
1
2
:
v
i
X
r
a

Why Most Results of Socio-Technical Security
User Studies Are False(cid:63)

Thomas Groß

School of Computing
Newcastle University, UK
thomas.gross@newcastle.ac.uk

Abstract. Background. In recent years, cyber security user studies
have been scrutinized for their reporting completeness, statistical report-
ing ﬁdelity, statistical reliability and biases. It remains an open question
what strength of evidence positive reports of such studies actually yield.
We focus on the extent to which positive reports indicate relation true
in reality, that is, a probabilistic assessment.
Aim. This study aims at establishing the overall strength of evidence in
cyber security user studies, with the dimensions (a) Positive Predictive
Value (PPV) and its complement False Positive Risk (FPR), (b) Like-
lihood Ratio (LR), and (c) Reverse-Bayesian Prior (RBP) for a ﬁxed
tolerated False Positive Risk.
Method. Based on 431 coded statistical inferences in 146 cyber security
user studies from a published SLR covering the years 2006–2016, we ﬁrst
compute a simulation of the a posteriori false positive risk based on as-
sumed prior and bias thresholds. Second, we establish the observed likeli-
hood ratios for positive reports. Third, we compute the reverse Bayesian
argument on the observed positive reports by computing the prior re-
quired for a ﬁxed a posteriori false positive rate.
Results. We obtain a comprehensive analysis of the strength of evidence
including an account of appropriate multiple comparison corrections. The
simulations show that even in face of well-controlled conditions and high
prior likelihoods, only few studies achieve good a posteriori probabilities.
Conclusions. Our work shows that the strength of evidence of the ﬁeld
is weak and that most positive reports are likely false. From this, we
learn what to watch out for in studies to advance the knowledge of the
ﬁeld.

Keywords: User studies · SLR · Cyber security · Strength of Evidence

1

Introduction

Empirical user studies heave an important place in studying socio-technical secu-
rity. They investigate user attitude and behaviors in face of security technologies
as well as the impact of diﬀerent interventions. They aﬀect a wide range of topics
in the ﬁeld.

(cid:63) Open Science Framework: https://osf.io/7gv6h/.

 
 
 
 
 
 
Aiming at advancing the quality of evidence in the ﬁeld, cyber security user
studies have been appraised for a number of factors in recent years: (i) their re-
porting completeness [3], (ii) their statistical reporting ﬁdelity [8,9], and (iii) their
statistical reliability [10,11].

Most recently, Groß [10] estimated eﬀect sizes from reported statistical tests,
simulated statistical power estimates from eﬀect size thresholds, and showed a
range of overall biases of the ﬁeld. Speciﬁcally, that prior study investigated the
estimated statistical power of studies in question and observed that few studies
achieved recommended power thresholds. Thereby, the study observed a power
failure in the ﬁeld. These observations, however, do not quantify the strength of
evidence the studies of the ﬁeld actually yield. Hence, the research gap we seek
to close is estimating the magnitude of the strength of evidence found in the
ﬁeld.

We understand as strength of evidence the probability of a claimed relation
being true in reality. We will consider multiple metrics to evaluate that prob-
ability under diﬀerent circumstances. First, we consider the Positive Predictive
Value (PPV), that is, the a posteriori probability of a relation being true after
the study was conducted. The estimate of the PPV and its complement, the False
Positive Risk (FPR), are dependent on knowing or assuming the prior probabil-
ity of before the study. Second, unlike the PPV, the likelihood ratio quantiﬁes
the strength of evidence independent of the knowledge of a prior. Finally, we
investigate the reverse Bayesian prior, that is, the prior probability one would
have needed to achieve a desired ﬁxed false positive risk. These three perspec-
tives, though all drawn from Bayes’ law, oﬀer diﬀerent lenses to appraise the
strength of evidence of relevant user studies.

Clearly, the investigation of positive predicted value and related quantities
is not entirely new. Most famously, Ioannidis [13] made a convincing case that
most published results are false, in general. Others have added to this argu-
ment, considering false positive risks or replications in a range of ﬁelds [16,19]
or promoted Bayesian views on statistical testing [12,1,2]. In fact, some of the
inspiration for this work is drawn from Ioannidis bold proclamation [13] and
Colquhoun’s thoughts on strength of evidence [2]. In this study, however, we are
the ﬁrst to evaluate the strength of evidence in socio-technical security studies
based on an empirical grounding.

In addition, we are interested to what extent the ﬁeld pays attention to the
strength of evidence as a factor to make the decision to cite studies. We thereby
ask to what extent the number of citations of studies in the ﬁeld is correlated to
the strength of evidence they provide.

Overall, the strength of evidence evaluation provided in this study oﬀers an

empirical scaﬀolding to make decisions on further studies in the ﬁeld.

Our Contributions. We are the ﬁrst to oﬀer a systematic evaluation of the
strength of evidence in socio-technical security user studies. Based on a size-
able empirical sample from a systematic literature review and coded statistical
tests. We provide an assessment of false positive risk based on conﬁgurable pa-
rameters bias, prior, and eﬀect size. Further, we evaluate the speciﬁc strength

of evidence of positive reports in investigated user studies, yielding distributions
of likelihood ratios and reverse bayesian prior. Overall, these contributions yield
a comprehensive review of the strength of evidence, creating opportunities to
make empirically informed decisions to advance the ﬁeld.

2 Background

2.1 Null Hypothesis Signiﬁcance Testing

Null Hypothesis Signiﬁcance Testing (NHST) [7] is a statistical method com-
monly used to evaluate whether a null hypothesis H0 can be rejected and an
alternative hypothesis H1 be considered plausible in its place. Recent reviews
of the method include, for instance, the work by Lehmann and Romano [14].
Null hypothesis signiﬁcance testing has often been criticized, in its own right
as well as for how scientists have fallen for a range of fallacies [17]. Problems
with the null hypothesis signiﬁcance testing have led to a stronger endorsement
of estimation theory, that is, relying more on eﬀect sizes and their conﬁdence
intervals [6].

In broad strokes, the method computes a p-value, that is, the probability
of how likely it us to make observations as extreme or more extreme than the
observations made D, assuming the null hypothesis H0 to be true. Hence, the
p-value is a conditional probability:

p := P [D | H0] .

Clearly, the p-value does not tell us how likely the alternative hypothesis is
after having made the observations of the study. However, misinterpretations of
the p-value often lead to confusion.

2.2 Bayes’ Law

Naturally, we are interested in establishing how likely the hypotheses of a study
are, a posteriori of its observations. This can be achieved by consulting Bayes’
Law. We shall write in a form conducive to our subsequent argument, as pro-
moted by Colquhoun [2]:

P [H1 | D]
P [H0 | D]
(cid:123)(cid:122)
(cid:125)
(cid:124)
a posteriori odds

=

P [D | H1]
P [D | H0]
(cid:124)
(cid:125)
(cid:123)(cid:122)
likelihood ratio

×

.

H1
H0
(cid:124)(cid:123)(cid:122)(cid:125)
prior odds

As we can see, the p-value P [D | H0] is the denominator of the likelihood ratio.
The corresponding numerator P [D | H1] indicates the probability of observa-
tions as extreme or more extreme than the observations made, assuming the al-
ternative hypothesis H1 being true, the statistical power of the test. In general,
we subscribe to the Bayesian interpretation of Bayes’ law, in which a probability
quantiﬁes the belief in a hypothesis.

Probability Interpretations. We distinguish two interpretations for proba-
bilities around the p-value, which were discussed by Colquhoun [2]. First, we
have the p-less-than interpretation, which considers observations as extreme or
more extreme as the ones obtained. Under this interpretation the likelihood ra-
tio considered above is computed by the statistical power divided by the p-value
itself.

Second, in the p-equals interpretation, we evaluate the likelihoods exactly
at the probability of the observations made. Hence, we consider the probability
exactly at the test statistic obtained under the alternative hypothesis divided
by the probability at the test statistic under the null hypothesis.

These two interpretations yield diﬀerent results for the likelihood ratio and
related evaluations, where typically, by p-value, the p-equals interpretation will
yield a greater false positive risk than the p-less-than interpretation.

Positive Predictive Value (PPV) and False Positive Risk (FPR). The
ﬁrst quantity we are interested in is the positive predictive value (PPV) P [H1 | D]
and its partner, the false positive risk (FPR) P [H0 | D].

PPPV := P [H1 | D] =

P [H1] P [D | H1]
P [H1] P [D | H1] + (1 − P [H1])P [D | H0]

PFPR := P [H0 | D] =

(1 − P [H1])P [D | H0]
P [H1] P [D | H1] + (1 − P [H1])P [D | H0]

Integrating Bias into Estimations. Ioannidis [13] deﬁned bias as “the com-
bination of various design, data, analysis, and presentation factors that tend to
produce research ﬁndings when they should not be produced.” He quantiﬁed it
as u, the proportion of tests that would not have been ﬁndings. We use Ioannidis’
estimation for PPV under the inﬂuences of bias, where R constitutes the prior
odds. We express the formula in the probability terminology introduced above.

PPPV,u :=

P [D | H1] R + u(1 − P [D | H1])R
R + P [D | H0] − P [D | H1] R + u − uP [D | H0] + u(1 − P [D | H1])R

Likelihood Ratio (LR). The likelihood ratio measures the strength of evidence
independent from priors and is given by

LR :=

P [D | H1]
P [D | H0]

.

Reverse Bayesian Argument. The reverse Bayesian argument aims at com-
puting the prior necessary to achieve a desired ﬁxed false positive risk P∗
FPR. This

method was originally proposed by Matthews [15] and endorsed by Colquhoun [2].
We compute the reverse Bayesian prior PRBP = P∗ [H1] as follows:

PRBP := P∗ [H1] =

3 Related Works

P [D | H0] (1 − P∗

P [D | H0] (1 − P∗

FPR)
FPR) + P [D | H1] P∗

FPR

3.1 Strength of Evidence in Other Fields

That most published ﬁndings are likely false was prominently discussed by Ioan-
nidis [13] in general terms and applicable to any ﬁeld. That study focused on
the estimation of the positive predictive value and oﬀered estimation formula
and thresholds for the inclusion of study biases. We adopt Ioannidis estimation
methods in this study, as well.

Other studies considered false positive reporting probability and the impact

of replications, where we cite two examples [19,16] as context.

This study is also related to approaches to estimate likelihood ratios and re-
verse Bayesian prior. Colquoun [1,2] oﬀered such estimations in the discussion of
the p-value null hypothesis signiﬁcance testing and reproducibility. He promoted
the use of the reverse Bayesian prior, where the use of the reverse Bayesian
argument was originally proposed by Matthews [15].

3.2 Appraisals of Cyber Security User Studies

Cyber security user studies have received a range of appraisals in recent years.
A ﬁrst step was made by Coopamootoo and Groß [3], who conducted a system-
atic literature review of cyber security user studies from relevant venues in the
years 2006–2016. That study included a coding of nine reporting completeness
indicators [4], giving a qualitative overview of scientiﬁc reporting. The authors
expanded upon said completeness indicators in a design and reporting toolkit [5].
Subsequently, Groß [8,9] built on the same SLR sample to establish the ﬁ-
delity of statistical reporting. This study re-computed p-values from published
test statistics and parameters to ﬁnd quantitative and decision errors.

Groß [10,11] turned to estimating eﬀect sizes and their conﬁdence intervals of
statistics tests in the papers obtained from the 2017 Coopamootoo-Groß-SLR.
That work further established simulations of statistical power vis-`a-vis speciﬁed
eﬀect size thresholds, highlighting a power failure. This power failure was de-
duced by comparison to typical expert recommendations of power thresholds,
but did not quantify the actual strength of evidence. In addition, the study
showed the presence of statistical biases, such as the publication bias or the
winner’s curse.

This paper, however, takes a diﬀerent tack. Though based on the same SLR
sample as previous work and considering the same statistical tests as extracted by
Groß [10], this work focuses on strength of evidence and estimates false positive
risk, likelihood ratio and reverse Bayesian prior for the statistical tests reported
in papers of the SLR sample.

4 Aims

4.1 Strength of Evidence

RQ 1 (Strength of Evidence) What is the distribution of the strength of ev-
idence in the ﬁeld of cyber security user studies?

This study aims at investigating the strength of evidence measured in an empir-
ical evaluation as (a) Positive Predictive Value (PPV) and its complement False
Positive Risk (FPR) (based on assumed priors), (b) Likelihood Ratio (LR), and
(c) Reverse-Bayesian Prior (RBP) for a ﬁxed false positive probability.

4.2 Attention to Strength of Evidence

RQ 2 (Attention to Strength of Evidence) To what extent is the attention
studies receive (in citations) related to their strength of evidence?

We investigate this question by evaluating the correlation between the strength
of evidence and the number of citations, measured as Average Citations Per
Annum (ACPA). We do so by testing the following hypotheses.
HC,0: There is no correlation between the strength of evidence (in terms of

Reverse Bayesian Prior) and the measured ACPA.

HC,1: The strength of evidence (in terms of Reverse Bayesian Prior) and the

measured ACPA are correlated.

5 Method

This study is pre-registered in the Open Science Framework1. The statistical
estimations are computed with R. Statistical tests are computed at a signiﬁcance
level α = .05.

5.1 Sample

We obtained the sample for this study from prior work. Its original foundation is
the 2016/17 Systematic Literature Review (SLR) by Coopamootoo and Groß [3].
The characteristics of the SLR are also documented by Groß [8].

In addition, this work is based on the eﬀect size extraction achieved by
Groß [10]. It contains t-, χ2-, r-, one-way F -tests, and Z-tests. The eﬀect sizes
were extracted based on an automated evaluation by statcheck [18] as well as
manual coding.

1 https://osf.io/7gv6h/?view_only=222af0e071a94b2482bb8ccb3e1eaa4c

5.2 Procedure

The study proceeded in the following fashion.
1. We have taken as input a table of coded p–values, standardized eﬀect sizes,

sample sizes, citations, and year of publication.

2. We set as parameters

(a) eﬀect size thresholds valued at (i) small: d = 0.3, (ii) medium: d = 0.5,

(iii) large: d = 0.8;

(b) biases valued at (i) theoretical minimum: u = .0, (ii) well-run RCT:

u = .2, (iii) weak RCT: u = .3, (iv) biased study: u = .8;

(c) priors valued at (i) “conﬁrmatory,” one-to-one ratio of relations being
true, prior = .5, (ii) “intermediate,” one-to-four ratio of relations being
true, prior = .2, (iii) “exploratory,” one-to-nine ratio of relations being
true, prior = .1;

3. We established a statistical power simulation based on the parametrized eﬀect

size thresholds, incl. a variant with multiple-comparison corrections.

4. Based on the actual p-values in the studies as well as their multiple-comparison
adjusted variants, we computed the positive-predictive value (PPV), the false
positive risk (FPR), and the reverse Bayesian prior (RBP).

5. We computed the likelihood ratio in the p-less-than interpretation from power

and p-values.

6. To assess the relation between attention studies are receiving and their strength
of evidence, we computed a hierarchical linear model on strength of evidence
by Average Citations per Annum (ACPA), using the study ID as random-
eﬀect variable.

7. Finally, we established the graphs by study and test ID.

6 Results

6.1 Sample

The reﬁned sample shown in Table 1 includes studies with extractable eﬀect
sizes as determined by Groß [10].

Table 1. Sample reﬁnement on SLR papers (as reported in [10])

Phase

Source SLR [3]

Excluded Retained

Search results (Google Scholar)
Inclusion/Exclusion

—
1011

1157
146

Reﬁnement in this study

Empirical studies
With sample sizes
With extractable tests

2
21
69

144
123
54

The sample diplayed in Table 2 includes statistical tests and their eﬀect sizes
that could be extracted from the papers in the sample. The ﬁnal sample includes
431 statistical tests and their eﬀect sizes.

Table 2. Sample reﬁnement on extracted eﬀect sizes (adapted from [10])

Phase

Excluded Retained

Total eﬀects extracted

0

statcheck automated extraction
Test statistic manual coding
Means & SD manual coding

650

252
89
309

Reﬁnement in Groß [10]
Violated reporting and assumptions

219

431

6.2 False Positive Risk

We examine the distribution of false positive risk (or False Positive Reporting
Probability, FPRP), that is, the a posteriori probability that the alternative
hypotheses of statistical tests are false.

Fixed Bias and Prior We ﬁrst consider the simulation for a weak random-
controlled trial (bias b = .3) and an intermediate prior (one in four investigated
relations being true, prior = 0.2) presented in Figure 1, the statistics corrected
for family-wise multiple comparisons. This plot corresponds to the centre one of
subsequent Figure 2. Let us unpack this plot step by step as an orientation.

The ﬁgure depicts the false positive risk as a scatter plot depending of the
underlying sample size of the corresponding study. As we would expect, the false
positive risk generally decreases with an increase of the sample size due to the
increasing statistical power—up to a point.

(cid:32)

(cid:32)

: d = 0.2, medium

The three colors represent assumed eﬀect size thresholds in the population—
small
: d = 0.8. Naturally, the
: d = 0.5, and large
greater the eﬀect size in the population, the less the false positive risk. The
lines drawn constitute a Loess smoothing of the corresponding scatter plot. We
observe that—assuming a setting of a weak random-controlled trials and priors
of 0.2—the false positive risk is at least 60%, irrespective of the size of the eﬀect
sizes thresholds assumed in the population or the additionally excerted power.
On the right-hand margin of the graph, we included the density of the false
positive risks for the SLR sample at hand. Here, we see that with respect to
smaller eﬀect size thresholds, there are clusters of greater false positive risk.

(cid:32)

Considering the expectation on number of false positive results, we obtain
that of the 142 MCC-corrected positive reports out of 444 tests under investiga-

Fig. 1. False Positive Risk for a weak Random Controlled Trial (RCT). Note: Pa-
rameters are ﬁxed to bias u = 0.3, prior = 0.2. The statistics are multiple-comparison
corrected. Eﬀect size thresholds are small (d = .2), medium (d = .5), and large (d = .8).

25%50%75%100%025050075010001250Sample SizeFalse Positive Risktion only an expected 48 statistically signiﬁcant results are true in reality, 34%.
Conversely, 94 of the positive reports are likely false positives (66%).

Variable Bias and Prior. Having evaluated a single parameter set of the
simulation, we are now in the position to consider the eﬀects of the parameters
bias and prior being varied.

Figure 2 displays false positive risk graphs for three cases for bias and
prior, respectively. The graphs are based statistics with family-wise multiple-
comparison corrections.

First, we observe that the amount of bias present in the study depresses the
capacity to reduce the false positive risk with additional power. For biased stud-
ies (bias u = .8), power in terms of increased sample size, is inconsequential to
eliminate false positive risk. Second, diﬀerent degrees of conﬁrmatoriness (vary-
ing priors) oﬀset the false positive risk, the more conﬁrmatory a study is, the
greater a prior a test operates against the less the false positive risk. Overall, we
ﬁnd that only conﬁrmatory studies (prior = 0.5) that are either run as well-run
RCT (u = .2) or weak RCT (u = .3) yield a false positive risk less than 50%.
Appendix A discusses the capacity to gain knowledge with a heatmap.

6.3 Strength Evidence of Positive Reports

In the following, we focus on positive reports, that is, relations studies reported
as statistically signiﬁcant.

Likelihood Ratio. In Figure 3a, we consider the strength of evidence of pos-
itive reports quantiﬁed as likelihood ratio, that is, the ratio of the report being
true in reality by the report being false in reality. This likelihood ratio is inde-
pendent of the presumed prior. The likelihood ratio is depicted in a scatter plot
by the sample size of the corresponding tests, the left Figure 3a containing the
original positive reports, the right Figure 3b showing the same reports under
appropriate family-wise multiple-comparison corrections. Studies are marked by
diﬀerent colors.

We ﬁnd in the MCC-corrected Figure 3b that most statistically signiﬁcant
results are clustered below a likelihood ratio of LR = 25. The sample has a
median likelihood ratio of 5.21. That is, for half the positive reports it is ap-
proximatly less than ﬁve times as likely for the report being true to it being
false. Remarkably, a number of studies yield positive reports with a consider-
ably greater likelinood ratio, that is, a considerable strength of evidence to the
positive report made.

Reverse Bayesian Prior In Figure 4, we evaluate the Reverse Bayesian Prior,
that is, prior that one would have needed a priori to reach a ﬁxed a posteriori
false positive risk of 5%. In general, one would consider a required prior of greater
than 50% as unreasonable, especially in a ﬁeld that contains copious amounts

Prior Probability (Conﬁrmatoriness)

Exploratory (prior = 0.1) Intermediate (prior = 0.2) Conﬁrmatory (prior = 0.5)

)
2
.

=
u
(
T
C
R
n
u
r
-
l
l
e

W

)
3
.

=
u
(
T
C
R
k
a
e

W

)
8
.

=
u
(

y
d
u
t
S

d
e
s
a
i
B

s
a
i
B
y
d
u
t
S

Fig. 2. False Positive Risk (FPR) versus bias and prior. Note: Statistics are based on
family-wise multiple comparison corrections. Eﬀect size thresholds are small (d = .2),
medium (d = .5), and large (d = .8).

25%50%75%100%025050075010001250Sample SizeFalse Positive Risk25%50%75%100%025050075010001250Sample SizeFalse Positive Risk25%50%75%100%025050075010001250Sample SizeFalse Positive Risk25%50%75%100%025050075010001250Sample SizeFalse Positive Risk25%50%75%100%025050075010001250Sample SizeFalse Positive Risk25%50%75%100%025050075010001250Sample SizeFalse Positive Risk25%50%75%100%025050075010001250Sample SizeFalse Positive Risk25%50%75%100%025050075010001250Sample SizeFalse Positive Risk25%50%75%100%025050075010001250Sample SizeFalse Positive Risk(a) Without MCC

(b) With MCC

Fig. 3. Distribution of the likelihood ratio by sample size. Note: The eﬀect size thresh-
old is ﬁxed at medium (d = 0.5). We limited the displayed Likelihood Ratio to
LR < 1000 for visual clarity.

of exploratory studies. For this evaluation, we ﬁx the eﬀect size threshold to
medium (d = 0.5) and the bias to .3. The ﬁgure contains the statistics without
and with adjustment for Multiple Comparison Corrections (MCC).

Without multiple-comparison corrections, Figure 4a shows (9%) of the 204
positive reports yield a reverse Bayesian prior greater than 50%. Considerng
the upper half of Figure 4b, we observe that approximately half (48%) of the
142 positive reports left after an adjustment for multiple-comparison corrections
show a reverse Bayesian prior greater than or equal to 50%. The closer a positive
report is positioned to a prior of zero, the stronger is the strength of evidence
speaking for the report, and the more likely the report will advance knowledge.

6.4 Relation to Citations

We conducted a hierarchical linear model with the paper ID as random-eﬀect
factor to investigate the correlation between the reverse Bayesian prior and a
metric of the citations, the average citations per annum. The model is based on
n = 204 statistically signiﬁcant reports (after correction for multiple comparison
corrections) from 25 papers. The model did not converge, that is, we could not
conﬁrm a correlation between these variables. Hence, we failed to reject the null
hypothesis HC,0. Appendix B contains a scatter plot of the variables.

median(LR) = 65.38025050075010000100200300400Sample SizeLikelihood Ratiomedian(LR) = 5.21025050075010000100200300400Sample SizeLikelihood Ratio(a) Without MCC

(b) With MCC

Fig. 4. Reverse Bayesian prior without and with multiple-comparison corrections.

7 Discussion

7.1 Why most results of socio-technical user studies are false

Let us start with the eponymous question of this paper. While this is generally
well understood for any ﬁeld of science [13], we quantiﬁed and contextualized
the impact of the adage based on a systematically drawn empirical sample of
cyber security user studies. Based on the sample at hand, we can estimate that
for moderate biases (u = .3) of weak random controlled trials—arguably already
an optimistic assumption—moderate priors (.2) and medium eﬀect sizes in the
population (d = .5) as done in Figure 1 of Section 6.2, an expected two thirds of
reported statistically signiﬁcant results are likely false positives (66%).

This should give us pause. Well-run random-controlled trials are rare in the
ﬁeld, we often see studies that incur a range of biases in their sampling, weakly
randomized or unblinded experiment design, or failure to meet statistical as-
sumptions. Exploratory studies seem to be relatively frequent, often investigat-
ing multiple relations, few of which are true in reality. Hence, in many cases we
would expect false positives even more numerous.

7.2 What to take away from the simulations

The simulation parametrized by bias and prior in Figure 2 yields a number of
important take-aways. Typically, when the question arises what to do to gain
more certainty in statistical tests, the answer is having adequate power. Groß [10]
argued the point of power failure of the ﬁeld in STAST’2020. While the require-
ment of adequate power—and, thereby, suﬃcient sample sizes—is a valid point,
the simulation teaches us that statistical power only takes us that far. Eventually,
the impact of power on the false positive risk is lower-bounded.

0%25%50%75%100%0200400600800Sample SizeReverse Bayesian Prior0%25%50%75%100%0200400600800Sample SizeReverse Bayesian PriorClearly, we cannot change the impact of the prior probability, the a priori
likelihood of an investigated relation being true. However, it is important to
raise the awareness of its eﬀect. Highly exploratory studies, which investigate a
large number of relations of which only a few are likely true in reality incur a
considerably greater false positive risk than highly conﬁrmatory studies. While
it is a natural sentiment not to take exploratory studies at face value, it is
worthwhile to take into account quantitatively the impact of lower priors.

Finally, we ﬁnd that incurred bias is a crucial factor to consider. As observed
in Figure 2, increasing bias depresses the capability of studies irrespective of their
conﬁrmatoriness or power to yield new knowledge. Hence, minimizing biases, for
instance by conducting systematically sampled, well-run, double-blind random
controlled trials, is a premier way to bringing down the false positive risk. Clearly,
these results also stress the importance of well-run replications.

We encourage readers to use the simulation in Figure 2 as a way of orientating

themselves for the appraisal of an individual study or a ﬁeld. Ask yourself:
1. What is the likely ratio of true relations to investigated relations (yielding

an estimate of the prior)?

2. How well are the studies under considerations run? How well do they mini-

mize biases?

3. What is the likely magnitude of the eﬀects investigated in the population?
4. What is the eﬀective sample size used for statistical tests.
Based on the former two questions select the facet in Figure 2 best reﬂecting
the appraisal. Based on the latter two questions select the appropriate eﬀect size
threshold and intersection with the sample size. This approach oﬀers a rough
approximation of the false positive risk to be expected. Of course, Ioannidis’
simulation equations [13] can be used directly to compute the same results.

7.3 The impact of strength of evidence

We observed in Figure 3b that positive reports under appropriate multiple-
comparison corrections were largely clustered around low likelihood ratios, with
a median LR = 5.21 against a medium eﬀect size. That means that in the
median—irrespective of the prior—a true positive result is ﬁve times as likely
than a false positive result.

We saw as well that a few studies achieve considerably greater likelihood
ratios. We believe that the community would beneﬁt from valuing studies that
achieve a great strength of evidence. While independence of the unknown prior
makes the likelihood ratio appealing as a metric, it does not frame the results
in absolute terms.

Here, the reverse bayesian prior comes into play, that is, what prior proba-
bility is required to achieve a ﬁxed false positive risk. In Figure 4b, we presented
the reverse bayesian prior of positive reports by the sample sized used. Nearly
half of the tests needed a prior greater than 50% to yield a false positive risk
of 5%. Hence we would have been implausibly certain of the relation a priori of
the study.

7.4 Attention to strength of evidence

Our evaluation of the correlation between average citations per annum and
strength of evidence were rooted in the aim to estimate whether authors take
strength of evidence into account when citing papers. As we could not show a
correlation being present, we have not found evidence of an association. Hence,
we would venture the opinion that the strength of evidence is not a strong factor
in deciding to cite another study.

7.5 Limitations

Generalizability. The study is founded on an existing sample of a systematic
literature review (SLR) of the years 2006–2016. While that sample yields chal-
lenges in terms of having been obtained on Google Scholar been restricted to
speciﬁc venues. Hence, its generalizability to the entire ﬁeld of socio-technical
security user studies is limited. However, we believe that the distribution we
observe in strength of evidence is not untypical of the ﬁeld at large.

Probability Interpretation. The computations in this work as based on the
p-less-than interpretation of test probabilities. That is, the likelihood ratio, for
instance, is computed as the ratio of statistical power (1 − β) by p-value itself.
Colquhoun [2] made a convincing case that the p-equals interpretation is more
appropriate for evaluating the strength of evidence of a single test. The p-equals
interpretation puts into relation the ordinate of the probability distributions of
the null and alternate hypotheses.

At the same time, in the studies of the SLR sample we are often missing the
data too compute the p-equals interpretation reliably. Hence, even if the prereg-
istration for this study asked for the p-equals interpretation as preferred metric,
we established the simulations on the p-less-than interpretation. For a large num-
ber of tests evaluated, this still gives us a conservative estimate: (i) Based on
Colquhoun’s comparative simulations of both interpretation [2]ß, we ﬁnd that
the p-equals interpretation yields a greater false positive risk than the p-less-than
interpretation. Hence, if anything, we are underestimating the false positive prob-
ability of statistical tests. (ii) Electing the p-less-than interpretation, we obtain
a larger sample of tests with likelihood-ratio estimations and, thereby, oﬀer a
more reliable sample to estimate the properties of the ﬁeld.

8 Conclusions

We showed, based on an systematically drawn empirical sample, that most pub-
lished ﬁndings in socio-technical security user studies are false. While this con-
cern has been stated generally for studies of any ﬁeld [13], we are the ﬁrst to
quantify the strength of evidence and expected number of false positive reports
based on an empirical foundation in socio-technical security user studies.

While our simulations depend on external parameters (i) bias, (ii) prior,
and (iii) eﬀect size threshold in the population, which are inherently diﬃcult
to estimate accurately, we oﬀer the readers a multi-faceted view of parameter
combinations and their consequences. For instance, for the false positive risk,
we can estimate make our own assumption on bias and prior present and then
consider the consequences of setup.

We also oﬀer investigations of positive reports, that is, results stated as sta-
tistically signiﬁcant, showing the distribution of likelihood ratios and reverse
Bayesian prior. These simulations establish an appraisal of the strength of evi-
dence while being independent from an unknown prior.

Our results raise caution about believing positive reports out of hand and
sensitize towards appraising the strength of evidence found in studies under
consideration. Our work makes the case to drive biases down as a factor of
experiment design and execution too often neglected in this ﬁeld. Finally, we
believe that we can see that strength of evidence is receiving little attention as
a factor in the decision to cite publications, raising the awareness of including
the strength-of-evidence consideration into the reporting recommendations for
authors and reviewing recommendations for gatekeepers.

Acknowledgment

Early aspects of this study were in parts funded by the UK Research Institute in
the Science of Cyber Security (RISCS) under a National Cyber Security Centre
(NCSC) grant on “Pathways to Enhancing Evidence-Based Research Methods
for Cyber Security.” Thomas Groß was funded by the ERC Starting Grant CAS-
CAde (GA no716980).

References

1. Colquhoun, D.: An investigation of the false discovery rate and the misinterpreta-

tion of p-values. Royal Society open science 1(3), 140216 (2014)

2. Colquhoun, D.: The reproducibility of research and the misinterpretation of p-

values. Royal society open science 4(12), 171085 (2017)

3. Coopamootoo, K., Groß, T.: Systematic evaluation for evidence-based methods in

cyber security. Technical Report TR-1528, Newcastle University (2017)

4. Coopamootoo, K.P., Groß, T.: A codebook for experimental research: The nifty
nine indicators v1.0. Tech. Rep. TR-1514, Newcastle University (November 2017)
5. Coopamootoo, K.P., Groß, T.: Cyber security and privacy experiments: A design
and reporting toolkit. In: IFIP International Summer School on Privacy and Iden-
tity Management. pp. 243–262. Springer (2017)

6. Cumming, G.: Understanding the new statistics: Eﬀect sizes, conﬁdence intervals,

and meta-analysis. Routledge (2013)

7. Fisher, R.A.: Statistical methods for research workers. Genesis Publishing Pvt Ltd

(1925)

8. Groß, T.: Fidelity of statistical reporting in 10 years of cyber security user studies.
In: Proceedings of the 9th International Workshop on Socio-Technical Aspects in
Security (STAST’2019). LNCS, vol. 11739, pp. 1–24. Springer Verlag (2019)

9. Groß, T.: Fidelity of statistical reporting in 10 years of cyber security user studies
[extended version]. arXiv Report arXiv:2004.06672, Newcastle University (2020)

10. Groß, T.: Statistical reliability of 10 years of cybersecurity user studies. In: Pro-
ceedings of the 10th International Workshop on Socio-Technical Aspects in Security
(STAST’2020). LNCS, vol. 12812, pp. 157–176. Springer Verlag (2020)

11. Groß, T.: Statistical reliability of 10 years of cybersecurity user studies [extended

version]. arXiv Report arXiv:2010.02117, Newcastle University (2020)

12. Howson, C., Urbach, P.: Scientiﬁc reasoning: the Bayesian approach. Open Court

Publishing (2006)

13. Ioannidis, J.P.: Why most published research ﬁndings are false. PLoS Med 2(8),

e124 (2005)

14. Lehmann, E.L., Romano, J.P.: Testing statistical hypotheses. Springer Texts in

Statistics (2005)

15. Matthews, R.A.: Why should clinicians care about bayesian methods? Journal of

Statistical Planning and Inference 94(1), 43–58 (2001)

16. Moonesinghe, R., Khoury, M.J., Janssens, A.C.J.: Most published research ﬁndings
are false—but a little replication goes a long way. PLoS Med 4(2), e28 (2007)
17. Nickerson, R.S.: Null hypothesis signiﬁcance testing: a review of an old and con-

tinuing controversy. Psychological methods 5(2), 241 (2000)

18. Nuijten, M.B., van Assen, M.A., Hartgerink, C.H., Epskamp, S., Wicherts, J.: The
validity of the tool “statcheck” in discovering statistical reporting inconsistencies.
https://psyarxiv.com/tcxaj/ (2017)

19. Wacholder, S., Chanock, S., Garcia-Closas, M., Rothman, N., et al.: Assessing the
probability that a positive report is false: an approach for molecular epidemiology
studies. Journal of the National Cancer Institute 96(6), 434–442 (2004)

A Capacity to Gain Knowledge

In Figure 5, we take a diﬀerent perspective from Figure 2. Here we consider the
simulation of a posteriori probability of the alternative hypotheses, as a heatmap
faceted by bias and eﬀect size threshold. This ﬁgure is organized by study and
conveys the maximal Positive Predictive Value (PPV) the study can achieve
with any test conducted. It is ordered by maximal PPV.

Figure 5 shows as yellow and light green a probability that positive reports
are more likely than 50%. The ﬁgure can be read as follows: Assuming a certain
eﬀect size in the population (say medium, d = 0.5), assuming that studies in
question were conducted as weakly-run RCT, and assuming a great prior close
to .5, we would select the centre facet of the plot. Here we would see that only
one ﬁfth (22%) of the studies reached a PPV equal or greater to 50% and only
conditioned on a prior close to .5.

B Association Between RBP and ACPA

Figure 6 depicts the association between the reverse Bayesian prior and the
citation metric. It is apparent in th graph that there is no visble correlation
between the variables under investigation.

Fig. 5. Heatmap of Positive Predictive Value (PPV) by eﬀect size threshold and bias.

smallmediumlargeWell−done RCT (u = .2)Weak RCT (u = .3)Biased Study (u = .8)0.00.10.20.30.40.50.00.10.20.30.40.50.00.10.20.30.40.5Prior0.20.40.6Maximal PPVFig. 6. Reverse Bayesian Prior by Average Citations per Annum. Note: The dot colors
denote diﬀerent studies.

0%25%50%75%100%010203040Average Citations per AnnumReverse Bayesian Prior