Applying High-Performance Bioinformatics Tools for Outlier Detection in Log Data

Markus Wurzenberger, Florian Skopik, Roman Fiedler
Austrian Institute of Technology, Center for Digital Safety and Security
Donau-City-Strasse 1, 1220 Vienna, Austria
ﬁrstname.lastname@ait.ac.at

Wolfgang Kastner
Vienna University of Technology
Treitlstrasse 3, 1040 Vienna, Austria
k@auto.tuwien.ac.at

Abstract—Most of today’s security solutions, such as security
information and event management (SIEM) and signature
based IDS, require the operator to evaluate potential attack
vectors and update detection signatures and rules in a timely
manner. However, today’s sophisticated and tailored advanced
persistent threats (APT), malware, ransomware and rootkits,
can be so complex and diverse, and often use zero day exploits,
that a pure signature-based blacklisting approach would not be
sufﬁcient to detect them. Therefore, we could observe a major
paradigm shift towards anomaly-based detection mechanisms,
which try to establish a system behavior baseline – either
based on netﬂow data or system logging data – and report
any deviations from this baseline. While these approaches
look promising, they usually suffer from scalability issues.
As the amount of log data generated during IT operations
is exponentially growing, high-performance analysis methods
are required that can handle this huge amount of data in real-
time. In this paper, we demonstrate how high-performance
bioinformatics tools can be applied to tackle this issue. We
investigate their application to log data for outlier detection to
timely reveal anomalous system behavior that points to cyber
attacks. Finally, we assess the detection capability and run-time
performance of the proposed approach.

1. Introduction

Many of today’s ICT security solutions promise an auto-
matic detection (and even mitigation) of malicious behavior.
They apply complex detection schemes and heuristics – and
massive data exchange across systems, organizations and
even countries: Threat Intelligence is the new hype. But
still, effective monitoring of a technical infrastructure is the
most essential phase of security incident handling within
organizations today.

To establish situational awareness, it is indispensable
for organizations to have a thorough understanding about
what is going on in their network infrastructures. Therefore,
clustering techniques are very effective tools for periodically
reviewing rare events (outliers) and checking frequent events
by comparing cluster sizes over time (e.g., trends in the
number of requests to certain resources). Furthermore, a
methodology and supporting tools to review log data and
to ﬁnd anomalous events in log data are needed. Existing

tools are basically suitable to cover all these requirements,
but they still suffer from some essential shortcomings. Most
of them, such as SLCT [1], implement word-based matching
of log entries, but for example do not identify synonyms
which only differ in one character, such as ‘php-admin’ and
‘phpadmin’; or consider similar URLs as completely differ-
ent words, although they have the same meaning. Hence, the
implementation of character-based matching with compara-
ble speed such as word-based matching is necessary. Fur-
thermore, existing tools are often notcapable of processing
large log ﬁles to extract cluster candidates over months and
to perform gradual logging, which can be applied to generate
a base corpora to identify how and where log clusters are
changing over the time.

In the domain of bioinformatics, various methods have
been developed to analyze and study the similarity of bio-
logic sequences (DNA, RNA, amino acids), group similar
sequences and extract common properties [2]. The algo-
rithms that implement these features need to fulﬁll some
ﬁerce requirements – similarly important to process log data:

• Adequate digital representation: Biologic sequences
must be represented as data streams in an appropriate
format, i.e., no information must be lost, but the format
should be as simple as possible.

• Dealing with natural variations: The dependency be-
tween a segment of a sequence and a certain biologic
function (implemented by this segment) is sometimes
not strict (or obvious). This means natural variations
need to be accounted for and a certain degree of
fuzziness in the input data accepted.

• Dealing with artiﬁcial inaccuracies: The process of
recording long and complex biologic sequences causes
inevitable inaccuracies and small errors. The negative
inﬂuence of those (artiﬁcially introduced) variations in
the following analysis phase should, however, be kept
to an absolute minimum.

• Dealing with massive data volumes: Since biologic
data sequences are (even to represent simple functions)
very complex, algorithms need to deal with these large
amounts of data usually by (i) being scheduled in
parallel and (ii) accepting certain inaccuracies caused
by this non-sequential processing.

In general, all these requirements also apply to mod-

ern log data processing as (i) data needs to be processed
extremely fast (this means depending on the application
approximately in real time); (ii) data analysis needs to be
scheduled in parallel in order to scale; and (iii) the process
needs to accept certain inaccuracies and errors that occur
due to conversion errors from varying character encodings,
and slight differences in conﬁgurations and output across
software versions. Furthermore, these tools aim at process-
ing character sequences without taking into account their
semantic meanings.

As a consequence, if mentioned tools are not applied
to biologic sequences but to re-coded (converted) digital
sequences, such as log data (or even malware code), all of
the unique properties of these algorithms can be exploited
directly, without the need to design and implement complex
tools again.

In this paper, we deﬁne a method for re-coding log
data into the alphabet used for representing canonical amino
acid sequences. This allows us to apply high-performance
bioinformatics tools to cluster log data. Based on the clus-
tering we perform outlier detection analysis to discover
anomalous and erratic behavior. Furthermore, we investigate
the applicability and feasibility of our approach in a real
setting by simulating a scenario of an attack and evaluate the
proposed approach. Finally, we provide an outlook for fur-
ther applications of the novel model beyond straightforward
outlier detection, such as time series analysis to discover
anomalous trends.

The remainder of the paper is structured as follows. Sect.
2 outlines important background and related work. Then,
Sect. 3 describes the overall model for discovering outliers
in log data. Section 4 elaborates on the re-coding model,
into a representation
which transforms log line content
which can be understood by bioinformatics tools. After that,
we describe how log lines are compared and clustered in
Sect. 5 and the detection of outliers in Sect. 6. Section 7
demonstrates the application of our approach and evaluates
the feasibility in a realistic setting. Finally, Sect. 8 concludes
the paper.

2. Background and Related Work

In the domain of cyber security, logging and log data
management are of high importance and improve visibility
and security intelligence for computer networks. Thus, log
data is a source for security- and computer network analysis
tools such as anomaly detection [3] and intrusion detection
systems [4] that identify anomalous system behavior. In
this paper, we focus on the application of the proposed
model
in the domain of cyber security and concentrate
on outlier detection for anomaly and intrusion detection.
Various outlier detection methods are discussed in [5].

The main technique we apply in our model for detecting
outliers and for creating a computer network’s situation
picture is clustering. Many different clustering approaches
and algorithms are surveyed in [6]. For clustering log lines
density and distance based approaches can be applied. Sim-
ple Logﬁle Clustering Tool (SLCT) [1] is an example for

a density based clustering algorithm especially developed
for clustering log data. But the proposed model focuses on
distance based algorithms.

One of the ﬁrst metrics to compare two sequences of any
kind of symbols, hence also log lines, was the Hamming
distance [7], which bases on the number of mismatches
and therefore can only be applied to sequences of the same
length. A further development of thi metric is the Leven-
shtein or edit distance [8], which also recognizes insertions
and deletions and therefore enables comparison of sequences
of different size.

Most bioinformatics tools for clustering amino acid or
DNA sequences apply a modiﬁed version of the previously
mentioned Levenshtein distance. In the case of amino aecid
sequences, the number of occurring unique symbols reduces
from 256 (in UTF-8 code) to 20 (canonical amino acids).
Furthermore, these algorithms make use of the knowledge
that
there exist empirical statistics that one amino acid
naturally can evolve to another one over time. The most
popular scoring matrices representing these relations are the
PAM (point accepted mutation) matrix and the BLOSUM
(blocks substitution matrix) matrix [9].

There exist a couple of sequence alignment algorithms
that compare two amino acid sequences. Therefore, a
distinction is made between global alignment, where all
symbols of two sequences are compared, such as the
Needleman-Wunsch algorithm [10], the Hirschberg algo-
rithm [11] and the Gotoh algorithm [12] and local alignment,
where just a subsequence is compared, such as the Smith-
Waterman algorithm [13]. There exist also fast heuristic
algorithms such as FASTA [14] and BLAST [15] to produce
alignments.

Various algorithms for clustering amino acids exploit
sequence alignment. Some examples are CD-HIT [16],
CLUSTAL [17] and UCLUST [18]. CD-HIT also applies
a powerful short word ﬁlter, which signiﬁcantly improves
the performance of the clustering algorithm.

3. Model for Applying Bioinformatics Cluster-
ing Tools on Log Data

The following section deﬁnes the theoretical model for
applying high-performance bioinformatics tools for cluster-
ing computer log data, which we already motivated in [19].
The proposed modular model comprises several steps from
re-coding log data to the alphabet used for describing amino
acid sequences to interpretation and analysis of the output
for cyber security application:

(i) collect log data,
(ii) homogenize log data,
(iii) re-code and format log data,
(iv) compare pairs of log lines according to their similarity,
(v) cluster log lines,
(vi) retranslate data,
(vii) detect outliers and analyse time series.

Figure 1 visualizes the proposed model. As the ﬁgure
shows, the model can be roughly divided into three blocks

A

B

C

A

B

C

a1
b1
a2
c1

a1
b1
a2
c1

>bio1
>bio2
>bio3
>bio4

Collect log data (i)

Homogenize (ii)

Re-co de (iii)

>bio1
>bio2
>bio3
>bio4

d(bio1,bio2)
d(bio1,bio3)
d(bio2,bio3)

Compare(iv)

Cluster (v)

>bio1
>bio2

text1
text2

Re-translate (vi)

outlier1
outlier2

Outlier detection and time 
series analysis (vii)

I

II

III

Figure 1. Visualization of the model for applying high-performance bioin-
formatics tools for the application in the domain of cyber security.

which are sequentially repeated. Block I covers the process
of re-coding log data into a format, which can be exploited
by bioinformatics tools. First, in step (i) log data from dif-
ferent sources of the monitored network is collected. When
analyzing log data from different sources usually the data
shows some differences in the format. For example, main
properties such as time stamps are represented in different
formats. Therefore, step (ii) is required to homogenize the
data. A common time stamp format is important to order log
lines chronologically when combining data from different
sources. In step (iii), the homogenized data is re-coded
from UTF-8 (256 symbols) to the alphabet describing the
canonical amino acids (20 symbols).

In block II, bioinformatics tools are applied to the re-
coded data. During step (iv) the re-coded log lines are
compared and a distance d between all pairs of lines is
calculated. Therefore, a sequence alignment algorithm for
amino acid sequences is applied to the data. Based on
the calculated distance in step (v), the log lines then are
clustered.

Block III implements the security analysis component
of the proposed model. First in step (vi), a reverse look up
function is used to re-translate the log lines from the alpha-

bet describing canonical amino acids into UTF-8 encoded
log data, which is readable for human users. Finally in step
(vii), an outlier detection and time series/trend analysis is
performed to detect on the one hand rare events and on
the other hand changes in the common system behavior.
Both can be caused by cyber attacks or invaders, as well
as misconﬁguration and erratic system behavior. In the
remaining paper we focus on outlier detection.

4. Re-coding Model

Log data from ICT systems is usually modeled in
human-readable textual form. Therefore, before tools from
the domain of bioinformatics can be applied to it, step (iii)
has to be carried out, i.e., re-coding the log data using the
alphabet used for representing amino acids and converting it
into a format, which can be exploited by the applied tools.
A basic unit of logging information, e.g., one line for
line-based logging, or one XML-element, is called a textual
log atom Ltext which consists of a series of symbols s
– typically letters and numbers (Eq. 1).The used alphabet
to represent log data consists (in most cases) of UTF-8
encoded characters (256 different symbols) of 8 bit size.
In the following AUT F −8 refers to this alphabet.

Ltext = hs1s2s3 . . . sni where si ∈ AUT F −8

(1)

But data represented in this format is unsuitable as input
to bioinformatics tools. Those tools require input (biologic
sequences) encoded with symbols of the alphabet Abio
(Eq. 2) deﬁned for amino acid or DNA sequences. This
alphabet consists of 20 symbols only, which represent the
20 canonical amino acids.

Abio = {A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y} (2)

A re-coding function takes an input stream encoded as
UTF-8 data and transforms it into a representation Lbio (Eq.
3) that is processable by bioinformatics tools.

Lbio = hs1s2s3 . . . smi where sj ∈ Abio

(3)

In the simplest case, this transformation is a straight
forward bijective mapping, where one AUT F −8 symbol
is represented by two symbols from Abio. However, for
data where certain larger blocks frequently appear, those
whole blocks (e.g., server names or IP addresses) could be
replaced with a single symbol. This would effectively allow
compression of data. Even further information loss could
be – depending on the application use case – acceptable.
For instance, frequently appearing symbol blocks could be
replaced through applying a more intelligent, but just one
way mapping, e.g., not a whole IP address but just the
last byte or the address’ cross sum could be translated to
Abio. Another example are paths (from Web server logs),
where each component of a path could be translated through
hashing into single symbols of Abio. Furthermore, symbols
can be grouped by type, so that for example all separators

such as ’/’, ’;’ or spaces can be replaced by one speciﬁc
element of Abio.

Even more complex re-coding schemes are possible, e.g.,
after identifying dynamic and static parts of log lines with
simple log line clustering tools (such as SLCT [1]), more
symbols could be spent on the variable parts of log lines
(those with higher information entropy) and less symbols
(or no symbols at all) on the rather static parts.

One simple - but effective - method for re-coding log
data into Abio is described in details in the following. In
order to re-code Ltext
into Lbio, a simple and straight
forward solution is to convert each si ∈ Ltext into two1
corresponding sj ∈ Lbio symbol by symbol (without any
loss of information). For this purpose, each symbol in Ltext
(i.e., the single letters of the words in a log line) is converted
to its numerical representation in UTF-8. The result of this
operation is Lutf (Eq. 4).

Lutf = ha1, a2, a3 . . . ani where ai ∈ {0, . . . , 255}

(4)

In a second step, each numerical value ai ∈ Lutf is
converted into two symbols of the alphabet Abio. Since the
size of this alphabet is always 20, a straight forward solution
(and in order to use the whole possible input range) is to
divide each ai ∈ Lutf by 20, and additionally keep the
rest of this division. Eventually, both results s1 (the result
of the integer division) and s2 (the rest of the division)
are mapped via a simple conversion table (see Tab. 1) to
Abio. Concatenating all these symbols in a single stream
effectively produces Lbio – the input
to alignment and
clustering tools from the domain of bioinformatics.

Table 1. BIO ALPHABET SYMBOL MAPPING.

Number:
Symbol:

Number:
Symbol:

0
A

10
M

1
C

11
N

2
D

12
P

3
E

13
Q

4
F

14
R

5
G

15
S

6
H

16
T

7
I

17
V

8
K

18
W

9
L

19
Y

The re-coding process is further described in Alg. 1.
There, the symbol ⊕ extends the collection on the left side
with the symbol on the right side. The function utf2num
looks up the decimal symbol number in a standard UTF-8
table (e.g., the letter ‘A’ corresponds to the number 65). The
function num2bio looks up the letter representation of the
numbers 0 to 19 (according to Tab. 1).

A simple option to reduce/compress the mount of data
needed to represent one log line by 50% is, instead of
representing each si ∈ Ltext by two sj ∈ Lbio (cf. Lf ull
bio
in Tab 2), to omit the leading character s1 (cf. Alg. 1).
This one has less entropy compared to the trailing s2 (cf.
Alg. 1), because if all 256 letters of AUT F −8 are occurring
in the considered data only the ﬁrst 12 letters of Abio are
used to represent s1. Since usually less than 100 symbols of
AUT F −8 occur in a realistic dataset and these symbols are

1Since the size of the alphabet of Ltext is larger (256 elements) than
that of Lbio (20 elements), one si ∈ Ltext has a much higher entropy
than one sj ∈ Lbio.

Lutf ← Lutf ⊕ utf2num(si)

Algorithm 1 Re-Coding Ltext into Lbio
1: Lbio ← ∅
2: Lutf ← ∅
3: for all si ∈ Ltext do
4:
5: end for
6: for all ai ∈ Lutf do
7:
s1 ← ai/20
8:
s2 ← ai%20
9:
Lbio ← Lbio ⊕ num2bio(s1) ⊕ num2bio(s2)
10: end for

in general numerically represented in the same region, for
example ai ∈ {51, . . . , 150}, which reduces the possible
options for s1 to at most 5. Hence, only a quarter of all
possible options is used to describe s1. As a consequence,
s1 stores less information than s2. Furthermore, while one s1
occurs in the description of 20 symbols, one s2 maximally
represents 5 symbols of AUT F −8. Finally, when omitting
s1 still the combination of sj ∈ Abio in Lbio raises the
entropy of every s2 obtained from Alg. 1. Hence, the result
is that the length of Lbio can effectively be cut to a half
by accepting a ”small” ambiguity (cf. Lbio in Tab 2). In the
remaining paper, we always apply this method for recording
Ltext into Lbio.

Table 2. STEP-BY-STEP RECODING EXAMPLE.

Ltext:
Lutf :
Lf ull
bio :
Lbio:

1
49
DL
L

9
57
DV
V

.
2
50
46
DM DH
H
M

1
49
DL
L

6
54
DR
R

8
56
DT
T

.
46
DH
H

...
...
...
...

To complete step (iii), the data has to be transformed into
the correct format. Most of the bioinformatic tools require
data in the FASTA format, which has been introduced by
Lipman and Pearson [14]. An example for this format is
given in Listing 1. As described later, the header required
by the FASTA format can be used to store information for
the re-translation implemented by step (vi).

> 0x
LVMHLRTHLVLHPPGPGPNNKIECPIMKLPWKKWMMWKQPEKKKKQPRNLFPI...
> 1x
LVMHLRTHLVLHPPGPGPNNKIECPIMKLPWKKWMMWMQPEKKKKQPRNLFPI...

Listing 1. Example for two sequences is FASTA format.

5. Comparing and Clustering of Log Data with
Bioinformatics Tools

A promising extension of bioinforamtics tools, such as
CD-HIT [16], CLUSTAL [17] and UCLUST [18] for log
analysis in order to increase the accuracy of results (iden-
tifying regions of similarity etc.) is the application of bio-
clustering on re-coded log data. Since bio-clustering applies
methods to group similar sequences which are fundamen-
tally different from the commonly applied text mining ap-
proaches, using bio-clustering can considerably improve the

quality of results. The reason for this improvement is mani-
fold. First, many common correlation algorithms require de-
cent knowledge about the syntax and semantics of the input
data. But, however this is not realistic for logging data from
different systems. Second, many text mining and clustering
algorithms lack the required degree of uncertainty when pro-
cessing log data. For instance, if two words differ by just one
letter, they are usually considered as completely different in
the clustering process, because for text mining, synonym
tables are more appropriate. This is however not true for
log data, where text junks, such as ‘php-admin’ and ‘ph-
padmin’ should be considered similar, if not almost equal.
Alignment algorithms from the domain of bioinformatics
assist by using a different metric to measure word similarity
which eventually improves the effectiveness. Third, most
text mining algorithms do not handle special characters
adequately, as they have different meanings in regular text
and log messages. For instance ‘../../etc/passwd’ is hard to
process, as ‘.’ and ‘/’ are considered natural delimiters in
written texts, but not in logs. Additionally, certain log se-
quences, e.g., paths ‘../../etc/passwd’ and ‘../../../etc/passwd’,
have considerably different meaning, however look similar
for text mining algorithms. Through applying deletions and
insertions in the bio-representations, these properties are ad-
equately handled by bioinformatics tools, and thus, distances
calculated accordingly. In the following, we describe how
alignment algorithms from the domain of bioinformatics
work for comparing two into Abio re-coded log lines and
how bio-clustering tools can be used for grouping log data.

5.1. Pairwise Log Line Comparison

Sequence alignment algorithms, which are applied in
step (iv), cf. Sect. 3 to compare two log lines form the
base for most bio-clustering tools. Some examples for such
algorithms are given in Sect. 2. Alignment algorithms use
a scoring function d to calculate the distance between two
sequences. When comparing two sequences LA
bio and LB
bio
element by element, there can occur three possible cases:
j was replaced by symbol sB
j ,

1) mismatch: symbol sA
2) deletion: symbol sA
3) insertion: symbol sB

j was removed in LB
bio,
j was inserted in LB
bio.

option

alignment

score

(i)

(ii)

(iii)

GAC
GC-
GAC--
---GC
GAC
G-C

1 − 1 − 1 = −1

−1 − 1 − 1 − 1 − 1 = −5

1 − 1 + 1 = 1

Table 3. SCORES FOR EXAMPLE 5.1

scoring system, which does not take into account that sA
j
could evolve to sB

j by a speciﬁc probability:

1
−1,

if sA
is sA

j = sB
j ,
j 6= sB
j ,

d(sA

j , sB

j ) =

(
j , −) = −1 deletion,
j ) = −1 insertion.

d(sA
d(−, sB

(5)

When comparing two amino acid sequences, there are
usually various options to build the alignment. In our model,
since the sequences are considered as homologous,
the
alignment with the highest score is chosen, because a higher
score suggests a higher similarity. Example 5.1 shows how
the optimal alignment is chosen.

bio = GC. We assume that LA

Example 5.1. Given two amino acid sequences LA
bio = GAC
and LB
bio are homolo-
gous. As scoring function serves d deﬁned in Eq. (5). Table 3
summarizes the possible alignments. Here option (iii) would
be the optimal alignment since it has the highest score.

bio and LB

In the proposed model, the similarity, between the two
amino acid sequences can be calculated as the ratio between
the number of identical symbols in the alignment and the
length of the alignment as shown in Eq. (6). Equation (6) is
a normalized version of the inverted Lvenshtein distance
[8], i.e., the identical symbols are calculated instead of
the number of changes. In the case of Example 5.1, the
similarity for option (iii) would be approximately 66, 66%.

similarity =

identicalSymbolsAlign(LA
bio, LB
lengthOf Align(LA

bio, LB
bio)

bio)

(6)

bio and LB

The alignment between two amino acid sequences is
always built under the assumption that LA
bio have
common ancestors, i.e., they are homologous. This means in
the end the alignment which refers to the highest similarity
is chosen [20]. How similar two amino acid sequences are
is speciﬁed by a similarity score. The predeﬁned score for
a match is usually constant. In most cases, the score for a
mismatch depends on the probability that sA
j can evolve to
sB
j over time. These probabilities are based on empirical
statistics and represented in a 20 × 20 lower triangular
matrix, which is called scoring matrix. The score for a gap
caused by deletions or insertions is also predeﬁned and can
depend on the size of the gap, or if a gap is opened or just
extended. The simplest deﬁnition for a scoring function d
relies on unit costs. In the following, we apply this simple

bio and LB

bio and LB

Listing 2 shows a full example of the comparison of the
two bio-encoded sequences LA
bio from Listing 1,
generated with the BLAST tool [15]. The output of this
tool is the alignment of LA
bio (see Query and
Subject depicted by Listing 2). The result is Algn, where
gaps are inserted between the residues so that identical
or similar characters are aligned in successive columns. In
case there is a bijective mapping back to the original data
Ltext, the original LA
text can be depicted aligned
using an inverse function (refer to Listing 2). Eventually, the
differences between the original input lines are marked with
either ‘X’, which means different symbols on the respective
positions in LA
text; or ‘-’ which means that there
is a gap and input stream Query could not be aligned to
Subject for the symbols on this position.

text and LB

text and LB

LA
LB

text : 192.168.191.4 - - [30/Sep/2014:00:22:05 +0000] "GET /login_page.php HTTP/1.1" 200 3307 "-" "Zabbix monitoring"
text : 192.168.191.4 - - [30/Sep/2014:00:22:25 +0000] "GET / HTTP/1.1" 200 5300 "-" "Zabbix monitoring"

LA
LB

bio

bio

:

:

LVMHLRTHLVLHPPGPGPNNKIECPIMKLPWKKWMMWKQPEKKKKQPRNLFPIKNEGMSPVECHPFPPPFFAILHLRPMKKPNNKSPRGRPRMVWWGAPLNMGTNRGMER

LVMHLRTHLVLHPPGPGPNNKIECPIMKLPWKKWMMWMQPEKKKKQPRNLFPIPPFFAILHLRPMKKPQNKKPRGRPRMVWWGAPLNMGTNRGMER

Query: LVMHLRTHLVLHPPGPGPNNKIECPIMKLPWKKWMMWKQPEKKKKQPRNLFPIKNEGMSPVECHPFPPPFFAILHLRPMKKPNNKSPRGRPRMVWWGAPLNMGTNRGMER
Algn: LVMHLRTHLVLHPPGPGPNNKIECPIMKLPWKKWMMW QPEKKKKQPRNLFPI
PPFFAILHLRPMKKP NK PRGRPRMVWWGAPLNMGTNRGMER
Sbjct: LVMHLRTHLVLHPPGPGPNNKIECPIMKLPWKKWMMWMQPEKKKKQPRNLFPI--------------PPFFAILHLRPMKKPQNKKPRGRPRMVWWGAPLNMGTNRGMER

Query: 192.168.191.4 - - [30/Sep/2014:00:22:05 +0000] "GET /login_page.php HTTP/1.1" 200 3307 "-" "Zabbix monitoring"
Algn: 192.168.191.4 - - [30/Sep/2014:00:22:X5 +0000] "GET /
HTTP/1.1" 200 X30X "-" "Zabbix monitoring"
Sbjct: 192.168.191.4 - - [30/Sep/2014:00:22:25 +0000] "GET /-------------- HTTP/1.1" 200 5300 "-" "Zabbix monitoring"

Diff:

X

--------------

X

X

Listing 2. Full example from real data: The ﬁrst block shows the input in textual form; the second block the bio-encoded sequences; the third block the
aligned output in bio-representation; the fourth block the aligned version in text representation and the ﬁfth block outlines the differences (‘X’ means
different symbols in the input streams and ‘-’ means gaps).

5.2. Log Line Clustering

Step (v), cf. Sect. 3, clustering log data is based on the
previously deﬁned alignment of two bio-encoded log lines.
By re-coding a whole log data set and subsequent pair-
wise comparison of bio-encoded log lines through sequence
alignment as shown before, distances can be determined
by calculating the similarity of two sequences (cf. Eq.
(6)). Clustering tools then try to cluster the bio-encoded
sequences in a way so that the distances between any two
cluster members ci ∈ C, cj ∈ C is lower than the distance to
the next cluster center. This analysis can be performed with
various existing bio-clustering tools, such as the prominent
CD-HIT [16]. CD-HIT ﬁrst applies an efﬁcient and fast short
word ﬁlter. If a sequence is considered as similar to the
representative sequence of a cluster, the alignment and the
exact similarity is calculated. Based on this, the algorithm
decides if the sequence corresponds to the cluster or not.

For further analysis of the clustering output, the se-
quences have to be re-translated into understandable text
– step (vi). Therefore,
the FASTA format provides the
possibility to store the position of a log line in the original
log ﬁle in the header (cf. Listing 1 > 0x and > 1x). Using
this information, it is possible to look up the corresponding
log line for each bio-encoded sequence in the input log ﬁle.

6. Outlier Detection

The following section brieﬂy deals with step (vii) – out-
lier detection for detecting anomalies. Outlier detection aims
at identifying so-called point anomalies [3]. These outliers
are clusters with just a few elements and/or usually a large
distance to other clusters, which deﬁne the normal state of an
network environment. In case of log data, outlier clusters in-
clude rare or atypically structured events (log entries). Those
outliers are log entries, that require further investigations.
Eventually, the previously deﬁned model allows to apply
high-performance bioinformatics tools on log data to cluster
log lines. During the re-translation from Abio to AUT F −8
the clusters can be sorted by size to detect clusters of small
size, which represent the outliers. Since it is also possible to
generate a representative alignment for every cluster, i.e., to
generate a multiple sequence alignment accounting for all

log lines assigned to one cluster, the function deﬁned in Eq.
(6) can be used to calculate the distance between all obtained
clusters. Hence, it is possible to discover the clusters, with
the largest distance to the group of clusters describing the
typical system behavior of a network environment.

7. Evaluation

The following section deals with the evaluation of the
proposed approach for outlier detection in computer net-
works. The evaluation shows on the one hand the detection
capability of our model and on the other hand evaluates
the run-time performance of the approach. The section is
structured as follows: First, we describe the set-up of the
evaluation environment and the conﬁguration of the different
components of the model. Then, we introduce the use case
on which the evaluation of the detection capability bases
and the test data we used for the evaluation. Finally, the
evaluation results are discussed.

7.1. Evaluation environment set-up and model con-
ﬁguration

As test environment, we used a workstation with an Intel
Xeon CPU E5-1620 v2 at 3.70GHz 8 cores and 16 GB
memory, running Ubuntu 16.04 LTS operating system.

The implementation of the model consists of three main
parts. First, we use a python script
to re-code the log
data from UTF-8 code to the alphabet of canonical amino
acids. Therefore, we apply the method described in Alg. 1.
During the evaluation, we compare two different methods
for re-coding log data. Once we translate the log lines to
Lf ull
(translation without loss of information) and once we
bio
comprise the data by re-coding to Lbio (translation, which
compresses the amount of data by just storing the second
character with higher entropy, and therefore leads to a loss
of information), as shown in Tab. 2.

Second, CD-HIT [16] is used for clustering the re-
coded log data. Since we have not evaluated an optimal
conﬁguration for the scoring matrix so far and the predeﬁned
matrix applied by CD-HIT is using properties of amino acid

sequences, we modiﬁed the downloaded C++ scripts2 and
deﬁned the scoring function as shown in Eq. (7). In this
formulation, the gap symbolizes an insertion or deletion.

6
−5,

d(sA

j , sB

j ) =

(
dopen gap = −11
dextend gap = −1

if sA
is sA

j = sB
j ,
j 6= sB
j

(7)

Furthermore, we conﬁgured the algorithm, so that every log
line is added to the cluster, where the representing element
is the most similar one to the processed log line and not
to the ﬁrst cluster it matches. Moreover, the length of the
shorter log line sl must have at least x% length of the longer
compared log line ll (cf. Eq. (8)), where x is the chosen
similarity threshold, which speciﬁes how similar two lines
have to be to match the same cluster. Also the length of
the calculated alignment must have at least the length of
the shorter log line sl (cf. Eq. (9)) and at least x% of the
longer log line ll (cf. Eq. (10)). This ensures a sequence
alignment as long as possible.

length(sl) >= x · length(ll)
length(sl) + length(gaps) = length(alignment)
length(alignment) = x · length(ll)

(8)
(9)
(10)

In the third part of the evaluation, we apply a python
script for retranslating the amino acid sequences into read-
able UTF-8 coded text data. Therefore, as described in Sect.
5, the ID which is assigned to every amino acid sequence
during the re-coding process is used to look up the log lines
in the original log ﬁle.

7.2. Testdata generation

Our test environment consisted of virtual servers running
on Apache Web server and the MANTIS Bug Tracker
System3 on top, a MySQL database, a ﬁrewall and a reverse
proxy. The log messages of these systems are aggregated
using syslog. To evaluate the presented approach, we used
log data from this system. For generating the data, we
applied a slightly modiﬁed version of the approach presented
in [21]. With this method it is possible to generate log ﬁles
of any size/time interval for a given system by simulating
user input in virtual machines. In our case, we created
four user machines that exhibit a typical behavior on a bug
tracker system, for example, logging in and out, submitting
and editing bug reports. This allowed us to control the
complexity of the scenarios, inject attacks at known points
of time. With this method, highly realistic conditions can
be achieved. Since the deployed environment is also used
in similar settings by real companies for managing bugs in
their software, the produced log data is representative.

For evaluating the proposed approach, we generated 4
different log ﬁles. In order to simulate different levels of

2http://weizhongli-lab.org/cd-hit
3https://www.mantisbt.org/

Data Set

Simulated
Users

Recorded
Time (h)

Data
Length
(lines)

Set

Used Conﬁgura-
tion

U1C1
U4C1
U1C2
U4C2

1
4
1
4

10
10
10
10

484.239
1.887.824
413.106
1.600.217

Conﬁg I
Conﬁg I
Conﬁg II
Conﬁg II

Table 4. PROPERTIES OF THE EXPLOITED SEMI-SYNTHETIC LOG FILES.

complexity, we implemented two conﬁgurations - conﬁgu-
ration I (low complexity: the virtual users only click on the
same three pages, in the same order) and conﬁguration II
(high complexity, see [21]). For generating the log ﬁles, the
user activity was logged for 10 hours. Table 4 shows that the
data set length, i.e., number of log lines, is mostly effected
by the number of simulated users. In both cases (running
one virtual user and running four concurrent virtual users),
changing from conﬁguration I to conﬁguration II generated
around 15% less log lines. This happens because in con-
ﬁguration II there are more options for the virtual users to
choose their actions from and there are more actions which
raise a longer waiting time until a virtual user performs his
next action.

7.3. Outlier detection

7.3.1. Detection Capability. Since the proposed approach
has to be considered as work in progress and not all parts are
fully implemented yet, we evaluated the detection capability
in the context of a simple but catchy scenario. Therefore,
we implemented an insider attacker who is an employee of
an organization using the MANTIS bug tracker platform.
The employee has valid credentials to log into the platform.
Usually he accesses the database through an application
hosted on the Web server, but because of a misconﬁguration
he found out, which port allows direct access to the database.
Additionally, he uses a private device to get access to the
database to steal data for unauthorized use. In our scenario,
the employee wants to access one speciﬁc database entry,
which he would not be authorized to access, when con-
necting to the database through the Web server. Therefore,
when he connects to the database, a different IP address
and a different MAC address, which only occurs once when
accessing the database, are logged and can be detected as
outlier. For simulating the scenario, we modiﬁed the log
lines which are part of one logged data base access from
the original log ﬁles and added it at a random location. For
our proposed approach, the order of the log lines makes no
difference, because they are sorted by their length, starting
with the longest log line, before clustering. The log line,
which includes the important information about the MAC
address and the IP address is shown in List. 3. The modiﬁed
MAC and IP address are chosen randomly.

It also would be possible to detect this kind of attack
with a common whitelist approach (i.e., explicitly specify
the known good IP addresses and MAC addresses). However
this simple, but catchy scenario allows us to show the
sensitivity of our proposed approach and prove its detection

Jul 16 08:47:32 v3ls1316.d03.arc.local kernel: [757325.314310]

iptables:ACCEPT-INFO IN=eth0 OUT= MAC=00:50:56:9c:25:67:**:**:**:**:**:**
:08:00 SRC=***.***.***.*** DST=169.254.0.2 LEN=60 TOS=0x00 PREC=0x20 TTL
=59 ID=36376 DF PROTO=TCP SPT=38947 DPT=80 SEQ=901703914 ACK=0 WINDOW
=29200 RES=0x00 SYN URGP=0 OPT (020405B40402080A1D6066F20000000001030307)

Listing 3. Log line in which the MAC and IP address are logged during a
data base access; the ‘*’ symbols mark the parts of the log line which are
modiﬁed.

capability. Furthermore, the information gathered from this
elementary test scenario serves as basis for more complex
cases and more complex application possibilities such as
time series analysis.

To evaluate the detection capability of the model, we
added the modiﬁed log lines to all the four log ﬁles men-
tioned in Tab. 4. In this evaluation, we deﬁned clusters
consisting of only one log line as outliers. To show the
detection capability of the proposed model, we calculated
two statistics. First, we calculated the absolute number of
false positives F P . We deﬁned every cluster consisting of
only one log line and not including the modiﬁed version of
the log line shown in List. 3 as F P . Second, we calculated
the ratio F P R between the number of F P and the log ﬁle
length (cf. Eq. (11)).

F P R =

F P
length(Log File)

(11)

For the evaluation, we ran the proposed algorithm on
the test data varying the similarity threshold, applied for
comparing the log lines, between 85% and 99%, raising it
by 1% every run. Table 7 shows the similarity threshold
for both methods at which the outlier we searched for was
detected ﬁrst. The outlier was also detected for all higher
similarity thresholds. Table 5 summarizes some of the results
for re-coding the log data into Lf ull
bio , which is a translation
without loss of information, and Tab. 6 presents the results
for re-coding the log data into Lbio, which is a translation
that compresses the amount of data, but leads to a loss of
information (cf. Tab. 2).

Table 7 indicates the lowest similarity threshold for both
re-coding methods and the four test datasets at which the
outlier we searched for was detected. Table 7 demonstrates
that a lower threshold can be chosen, when re-coding the log
data to Lbio, to detect the outlier. Furthermore, the lowest
threshold at which the outlier is detected is independent from
the number of users and the chosen complexity of the logged
network environment. It only depends on the applied re-
coding model. Moreover, Tab. 7 reveals that re-coding to
Lbio is more sensitive for detecting outliers since the outlier
is detected at a lower threshold. This can be explained by the
fact that when re-coding to Lf ull
bio every symbol is re-coded
into two symbols of the canonical amino acids alphabet. For
the ﬁrst symbol, only at most 13 out of 20 letters are used
(cf. Sect. 4). Hence, one of the ﬁrst symbols occurs more
often, than one of the second symbols.

In contrast to the lowest threshold at which the outlier
is detected, Tab. 5 and Tab. 6 show that the number of F P
and also the F P R depend on the complexity of the logged

Total Time [s]

Re-coding Time [s]

U1C1V1
U4C2V1
U1C1V2
U4C2V2

 1800

 1600

 1400

 1200

 1000

 800

 600

 400

 200

U1C1V1
U4C2V1
U1C1V2
U4C2V2

 250

 200

 150

 100

 50

e
m
T

i

n
u
R

 0

 0

6
 1x10

6
 2x10

6
 3x10

Number of Log Lines

(a) total time.

Clustering Time [s]

 0

 0

6
 1x10

6
 2x10

6
 3x10

Number of Log Lines

(b) re-coding time.

Re-translation Time [s]

U1C1V1
U4C2V1
U1C1V2
U4C2V2

 1600

 1400

 1200

 1000

 800

 600

 400

 200

 12

 10

U1C1V1
U4C2V1
U1C1V2
U4C2V2

e
m
T

i

n
u
R

 8

 6

 4

 2

e
m
T

i

n
u
R

e
m
T

i

n
u
R

 0

 0

6
 1x10

6
 2x10

6
 3x10

 0

 0

6
 1x10

6
 2x10

6
 3x10

Number of Log Lines

Number of Log Lines

(c) clustering time

(d) re-translation time.

Figure 2. Run time and scalability for the single steps of the proposed
approach.

network environment. The F P and F P R is higher for
the more complex conﬁguration. According to the results,
the number of logged users only has a very low inﬂuence
on the number of F P and the F P R. That was to be
expected, because every user can carry out the same actions.
Furthermore, the tables show that the number of F P and
the F P R for re-coding to Lf ull
(cf. Tab 5) are a bit lower
bio
than for re-coding to Lbio (cf. 6). This can be explained in
the same way, as in the previous paragraph due to the fact
that the lowest threshold at which the outlier is detected is
lower when recoding to Lbio. Again this results from the
fact that re-coding to Lbio allows a more sensitive outlier
detection than re-coding to Lf ull
bio . Furthermore in both cases,
the F P and F P R start increasing much faster at a speciﬁc
threshold. Again, because of the higher sensitivity this can
be recognized earlier, when re-coding to Lbio (at 93% sim-
ilarity) than when re-coding to Lf ull
(at 96%). Thus, when
bio
using a higher similarity threshold, which generates more
F P , the outliers can be clustered again, applying a lower
threshold. This makes it easier to understand the detected
outliers and increase situational awareness, since they are
grouped. Using realistic similarity thresholds, especially the
thresholds summarized in Tab. 7, the number of F P and the
F P R are very low and the detected outliers can be easily
investigated manually by a system administrator.

7.3.2. Model Scalability. To evaluate the scalability of our
approach for detecting outliers, we generated log ﬁles of
different lengths, i.e., line numbers, for the simplest conﬁg-
uration U 1C1 and the most complex conﬁguration U 4C2,
cf. Tab. 4. The length of the log ﬁles ranges from 100, 000
lines to 3, 000, 000 lines. For generating the log ﬁles, we

 
 
 
 
Table 5. F P AND F P R RESULTS, WHEN RECODING TO Lf ull
bio

Threshold
0.86
0.88
0.91
0.92
0.95
0.96
0.97

F PU 1C1
14
17
24
27
45
2223
16972

F P RU 1C1
2,89E-05
3,51E-05
4,96E-05
5,58E-05
9,29E-05
4,59E-03
3,50E-02

F PU 1C2
202
286
696
758
1659
5397
25763

F P RU 1C2
4,89E-04
6,92E-04
1,68E-03
1,83E-03
4,02E-03
1,31E-02
6,24E-02

F PU 4C1
4
6
13
16
40
2973
59289

F P RU 4C1
2,12E-06
3,18E-06
6,89E-06
8,48E-06
2,12E-05
1,57E-03
3,14E-02

F PU 4C2
160
378
1718
2052
4955
11978
87107

F P RU 4C2
1,00E-04
2,36E-04
1,07E-03
1,28E-03
3,10E-03
7,49E-03
5,44E-02

Table 6. F P AND F P R RESULTS, WHEN RECODING TO Lbio

Threshold
0.86
0.87
0.88
0.90
0.92
0.93
0.94

F PU 1C1
15
18
22
33
48
523
8852

F P RU 1C1
3,10E-05
3,72E-05
4,54E-05
6,81E-05
9,91E-05
1,08E-03
1,83E-02

F PU 1C2
362
523
701
825
1160
2388
13964

F P RU 1C2
8,76E-04
1,27E-03
1,70E-03
2,00E-03
2,81E-03
5,78E-03
3,38E-02

F PU 4C1
15
16
19
24
41
521
21308

F P RU 4C1
7,95E-06
8,48E-06
1,01E-05
1,27E-05
2,17E-05
2,76E-04
1,13E-02

F PU 4C2
483
901
1878
2119
3363
6030
35144

F P RU 4C2
3,02E-04
5,63E-04
1,17E-03
1,32E-03
2,10E-03
3,77E-03
2,20E-02

Table 7. THRESHOLDS AT WHICH THE OUTLIER IS DETECTED.

Conf.
U1C1
U1C2
U4C1
U4C2

Lf ull
bio
0,91
0,91
0,92
0,92

Lbio
0,88
0,87
0,86
0,88

applied the approach proposed in [22]. This algorithm allows
to generate highly realistic semi-synthetic log ﬁles based
on a small piece of real log data. We compared the run-
time for re-coding to Lf ull
bio (V1) and re-coding to Lbio (V2).
As similarity threshold, we used the values obtained from
the analysis of the lowest value at which the outlier was
detected (cf. Tab. 7). Besides the total run time we calculated
the time for re-coding, clustering and re-translating. The
plots in Fig. 2 demonstrate that the runtime of the model
is increasing linearly. The total runtime is higher for the
more complex conﬁguration and also for the translation to
Lf ull
bio . Depending on the complexity of the data, in our
test environment (c.f. Sect. 7.1) the algorithm is able to
process between 1800 and 5000 log lines per second. The
re-coding time only depends on the re-coding method and
is longer for recoding to Lf ull
bio , since more symbols are
generated. The clustering time depends on the complexity
of the analyzed system and on the re-coding method, i.e.,
on the length of the analyzed sequences. The re-translation
time only depends on the length of the log ﬁle. The most
time consuming part is the clustering.

7.4. Outlook on further application possibilities

The outlined approach is in an early stage and not all
parts of the proposed model are fully implemented yet.
There exist also other application possibilities of the pro-
posed approach than evaluated in Sect. 7.3. To enable real-
time log data processing and outlier detection, we foresee
the application of the following method: First the clustering
model is trained with log data of optional length, which
represents the normal system behavior of the monitored

network environment. The training log data should at least
cover a cycle, which includes also activities such as update
and back up processes, which are usually done in speciﬁc
time periods. Afterwards new log lines obtained from the
system are sorted to the clusters. If a log line does not
match to any cluster it is considered as outlier and raises an
alarm. Furthermore, if the system administrator decides that
a detected outlier does not represent anomalous behavior, the
log line can be added to the cluster model as representative
element of a new cluster. Since the training phase to gen-
erate new clusters just runs occasionally (and potentially in
parallel to the regular detection of outliers), its run-time does
not negatively inﬂuence the actual detection. In this phase,
also clusters with just one member do not represent outliers,
but are ﬁlled up in the later detection phase with log line
instances. This means, in the training phase a higher simi-
larity threshold can be set. This would especially decrease
the number of F P and raise the detection capability.

The proposed model can also be applied for time series
analysis to detect attacks and invaders. For this purpose,
clustering models are created for different time periods.
Then, the properties of the obtained clustering models can be
compared (e.g., between two consecutive hours or days). If
one compares two clustering models and one cluster occurs
only in one of the two models, these cluster can be seen
as outliers (e.g., a new device was plugged to the network,
which should not be there). Furthermore also a change in the
size of a cluster is an indication for anomalous behavior. For
example if an attacker ex-ﬁltrates data, the number of log
lines referring to the database server will increase (especially
in relation to the number of log lines in other clusters). If
this is done with a machine, which belongs to the company
and therefore uses a legitimate MAC and IP address the log
lines would not be recognized as outliers. But in the time
series analysis it would be clearly visible that the sizes of
speciﬁc clusters, which are related to the database server, are
increasing, and thus a major change in the system utilization
behavior detected.

8. Conclusion and Future Work

References

This paper describes a novel model, which allows to
apply high-performance bioinformatics tools in the context
of anomaly detection on log data produced in computer
networks. Since most of the bioinformatics tools operate on
canonical amino acid sequences, we introduced two different
methods to re-code log data coded in UTF-8 code, consisting
of 255 symbols, to the alphabet of canonical amino acids,
consisting of only 20 symbols. The ﬁrst method describes
a translation without loss of information, while the second
method describes a translation, which compresses the data
and therefore some information gets lost, but it allows faster
anomaly detection. We further demonstrated how the re-
coded log data can be clustered applying bioinformatics
algorithms and tools for generating sequence alignments. We
furthermore explained how the output can be re-translated
into a human-readable format using an ID number. Finally,
we described and evaluated the outlier detection.

In opposite to most other approaches, which work
with word matching algorithms, our model implements a
character-based sequence comparison. This allows a much
more sensitive anomaly detection (e.g., similar URLs with
slight deviations are recognized as related). Since the bioin-
formatics tools are developed for many years they are op-
timized for high-performance and high data throughput to
allow processing huge amounts of data in very short times.
Furthermore, our approach does not have to know about
syntax and semantics of the log data (i.e., no speciﬁc parsers
are required to detect anomalous system behavior). Hence, it
can be applied in any computer network, which logs events
in text formats. This especially enables the application in
legacy systems (e.g., in the Industrial Control Systems (ICS)
domain, which are often not well documented, as well as in
less mature systems with a small market share).

In the future, we plan to focus on further development
of the re-coding model described in Sect. 4 and the scoring
system deﬁned in Sect. 5.1. Therefore, we intend to modify
the re-coding function, so that often re-occurring parts of log
ﬁles (e.g., static texts) are translated into less symbols and
therefore accept a higher loss of information for these parts;
and less frequent parts, which include the more interesting
variable parts (e.g., IP addresses, user names, port numbers)
of log lines should be translated without loss of information.
Furthermore, we plan to investigate if the scoring system can
be adjusted, so that, similar to the analysis of amino acid
sequences, the score is higher for highly related letters and
lower in other cases. Moreover, we want to implement and
evaluate the application methods of our approach described
in Sect. 7.4 for real-time outlier detection and time series
analysis.

Acknowledgments

This work was partly funded by the FFG project syn-
ERGY (855457) and carried out in course of a PhD thesis
at the Vienna University of Technology funded by the FFG
project BAESE (852301).

[1] R. Vaarandi, “A data clustering algorithm for mining patterns from

event logs,” in IPOM, Oct 2003, pp. 119–126.

[2]

J. T. L. Wang, M. J. Zaki, H. Toivonen, and D. Shasha, Data Mining
in Bioinformatics. Springer Science & Business Media, Mar. 2006.

[3] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A
survey,” ACM computing surveys (CSUR), vol. 41, no. 3, p. 15, 2009.

[4]

S. Axelsson, “Intrusion detection systems: A survey and taxonomy,”
Technical report Chalmers University of Technology, Goteborg, Swe-
den, Tech. Rep., 2000.

[5] V. J. Hodge and J. Austin, “A survey of outlier detection methodolo-

gies,” Artiﬁcial Intelligence Review, vol. 22, no. 2, 2004.

[6]

P. Berkhin, “A survey of clustering data mining techniques,” in
Grouping multidimensional data. Springer, 2006, pp. 25–71.

[7] R. W. Hamming, “Error detecting and error correcting codes,” Bell

Syst. tech. journal, vol. 29, no. 2, pp. 147–160, 1950.

[8] V. I. Levenshtein, “Binary codes capable of correcting deletions,

insertions, and reversals,” in Soviet physics doklady, vol. 10, 1966.

[9]

S. Henikoff and J. G. Henikoff, “Amino acid substitution matrices
from protein blocks,” Proceedings of the National Academy of Sci-
ences, vol. 89, no. 22, 1992.

[10] S. B. Needleman and C. D. Wunsch, “A general method applicable to
the search for similarities in the amino acid sequence of two proteins,”
Journal of molecular biology, vol. 48, no. 3, pp. 443–453, 1970.

[11] D. S. Hirschberg, “A linear space algorithm for computing maximal
common subsequences,” Communications of the ACM, vol. 18, 1975.

[12] O. Gotoh, “An improved algorithm for matching biological se-
quences,” Journal of molecular biology, vol. 162, no. 3, 1982.

[13] T. F. Smith and M. S. Waterman, “Identiﬁcation of common molecular
subsequences,” Journal of molecular biology, vol. 147, no. 1, 1981.

[14] W. R. Pearson and D. J. Lipman, “Improved tools for biological
sequence comparison,” National Academy of Sciences, vol. 85, 1988.

[15] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, and D. J. Lipman,
“Basic local alignment search tool,” Journal of molecular biology,
vol. 215, no. 3, pp. 403–410, 1990.

[16] W. Li, L. Jaroszewski, and A. Godzik, “Tolerating some redundancy
signiﬁcantly speeds up clustering of large protein databases,” Bioin-
formatics, vol. 18, no. 1, pp. 77–82, 2002.

[17] D. G. Higgins and P. M. Sharp, “Clustal: a package for performing
multiple sequence alignment on a microcomputer,” Gene, vol. 73,
no. 1, pp. 237–244, 1988.

[18] R. C. Edgar, “Search and clustering orders of magnitude faster than
blast,” Bioinformatics, vol. 26, no. 19, pp. 2460–2461, 2010.

[19] M. Wurzenberger, F. Skopik, R. Fiedler, and W. Kastner, “Discovering
insider threats from log data with high-performance bioinformatics
tools,” in Proceedings of the 2016 International Workshop on Man-
aging Insider Security Threats. ACM, 2016, pp. 109–112.

[20] D. W. Mount, Bioinformatics: Sequence and Genome Analysis.

CSHL Press, 2004.

[21] F. Skopik, G. Settanni, R. Fiedler, and I. Friedberg, “Semi-synthetic
data set generation for security software evaluation,” in Privacy,
Security and Trust (PST).

IEEE, 2014, pp. 156–163.

[22] M. Wurzenberger, F. Skopik, G. Settanni, and W. Scherrer, “Complex
log ﬁle synthesis for rapid sandbox-benchmarking of security- and
computer network analysis tools,” Inf. Syst., vol. 60, Aug. 2016.

