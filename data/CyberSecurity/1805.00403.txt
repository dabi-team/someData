iSTRICT: An Interdependent Strategic Trust Mechanism for
the Cloud-Enabled Internet of Controlled Things

Jeffrey Pawlick, Juntao Chen, and Quanyan Zhu

8
1
0
2

v
o
N
5
1

]

R
C
.
s
c
[

2
v
3
0
4
0
0
.
5
0
8
1
:
v
i
X
r
a

Abstractâ€”The cloud-enabled Internet of controlled things
(IoCT) envisions a network of sensors, controllers, and actua-
tors connected through a local cloud in order to intelligently
control physical devices. Because cloud services are vulnerable
to advanced persistent threats (APTs), each device in the IoCT
must strategically decide whether to trust cloud services that
may be compromised. In this paper, we present iSTRICT, an
interdependent strategic trust mechanism for the cloud-enabled
IoCT. iSTRICT is composed of three interdependent layers. In
the cloud layer, iSTRICT uses FlipIt games to conceptualize
APTs. In the communication layer, it captures the interaction be-
tween devices and the cloud using signaling games. In the physical
layer, iSTRICT uses optimal control to quantify the utilities in
the higher level games. Best response dynamics link the three
layers in an overall â€œgame-of-games,â€ for which the outcome is
captured by a concept called Gestalt Nash equilibrium (GNE). We
prove the existence of a GNE under a set of natural assumptions
and develop an adaptive algorithm to iteratively compute the
equilibrium. Finally, we apply iSTRICT to trust management
for autonomous vehicles that rely on measurements from remote
sources. We show that strategic trust in the communication layer
achieves a worst-case probability of compromise for any attack
and defense costs in the cyber layer.

Index Termsâ€”Internet of controlled things, cyber-physical sys-
tems, strategic trust, cybersecurity, advanced persistent threats,
autonomous vehicles, game-of-games

I. INTRODUCTION

The Internet of Things (IoT) will impact a diverse set of
consumer, public sector, and industrial systems. Smart homes
and buildings, autonomous vehicles and transportation [1], and
the interaction between wearable ï¬tness devices and social net-
works [2] provide a few examples of application areas which
will be particularly impacted by the IoT. One deï¬nition of
the IoT is a â€œdynamic global network infrastructure with self-
conï¬guring capabilities based on standard and interoperable
communication protocols where physical and virtual â€˜thingsâ€™
have identities, physical attributes, and virtual personalitiesâ€
[3]. This deï¬nition envisions a decentralized, heterogeneous
network with plug-and-play capabilities. The related concept
of cyber-physical systems (CPS) refers to â€œsmart networked
systems with embedded sensors, processors, and actuatorsâ€
[5] provides a detailed introduction to CPS and reports
[4].
on its development status. The term CPS emphasizes the
â€œsystemsâ€ nature of these networks. In both IoT and CPS,

The authors are with the Department of Electrical and Computer Engi-
neering, Tandon School of Engineering, New York University, Brooklyn, NY,
11201 USA. E-mail: {jpawlick,jc6412,qz494}@nyu.edu.

This work is partially supported by an NSF IGERT grant through the
Center for Interdisciplinary Studies in Security and Privacy (CRISSP) at
New York University, by the grants CNS-1544782, EFRI-1441140, and SES-
1541164 from National Science Foundation (NSF) and de-ne0008571 from
the Department of Energy.

Fig. 1: iSTRICT addresses security and trust issues for a cloud-enabled IoCT.
The cloud-enabled IoCT consists of connected sensors and devices, with a
cloud as the interface. Adversaries are capable of compromising cloud services
and modifying the control signals that they transmit to the devices. The trust
issue lies between the cloud (sender) and IoCT (receiver). Each IoCT device
should determine which signals to trust from cloud services strategically.

â€œthe joint behavior of the â€˜cyberâ€™ and physical elements
of the system is criticalâ€”computing, control, sensing, and
networking can be integrated into every componentâ€ [4]. The
importance of sensing, actuation, and control to devices in the
IoT has given rise to the term â€œInternet of controlled things,â€
or IoCT. Hereafter, we refer to the IoCT as a way to address
challenges of both CPS and IoT.

The IoCT requires an interface between heterogeneous com-
ponents. Local clouds (or fogs or cloudlets) offer promising
solutions. In these networks, a cloud provides services for
data aggregation, data storage, and computation. In addition,
the cloud provides a market for the services of software
developers and computational intelligence experts [6]. Figure
1 depicts a cloud-enabled IoCT. In this network, sensors
push environment data to the cloud, where it is aggregated
and sent to devices (or â€œthingsâ€), which use the data for
feedback control. These devices modify the environment, and
the cycle continues. Note that the control design of the IoCT
is distributed, since each device can determine which cloud
services to use for feedback control.

A. Advanced Persistent Threats in the Cloud-Enabled IoCT

Unfortunately, cyberattacks on the cloud are increasing as
more businesses utilize cloud services [7]. To provide reliable
support for IoCT applications, sensitive data provided by the
cloud services needs to be well protected [8]. In this paper
we focus on the attack model of advanced persistent threats
(APTs): â€œcyber attacks executed by sophisticated and well-
resourced adversaries targeting speciï¬c information in high-
proï¬le companies and governments, usually in a long term
campaign involving different stepsâ€ [9]. In the initial stage of
an APT, an attacker penetrates the network through techniques

C1  Thing 1  (Thermostat)     Thing 2  (Wearable computing)      Thing 3  (Washing machine)     Environment Smart Cars Smart Home Smart City Smart things Controller network Sensor network Cloud C2 C3 S2 S3 S4 Attacker Defender S1 Feedback Output Feedback loop  
 
 
 
 
 
such as social engineering, malicious hardware injection, theft
of cryptographic keys, or zero-day exploits [10]. For example,
the Naikon APT, which targeted governments around the South
China Sea in 2010-2015, used a bait document that appeared
to be a Microsoft Word ï¬le but which was actually a malicious
executable that installed spware [11]. The cloud is particularly
vulnerable to initial penetration through application-layer at-
tacks, because many applications are required for developers
and clients to interface with the cloud. Our iSTRICT can be
applied to many cyberattack scenarios. For example, cross-
site scripting (XSS) and SQL injection are two types of
application-layer attacks. In SQL injection, attackers insert
malicious SQL code into ï¬elds which do not properly process
string literal escape characters. The malicious code targets
the server, where it could be used to modify data or bypass
authentication systems. By contrast, XSS targets the execution
of code in the browser on the client side. All of these attacks
give attackers an initial entry point into a system, from which
they can begin to gain more complete, insider control. This
control of the cloud can be used to transmit malicious signals
to CPS and cause physical damage.

B. Strategic Trust

Given the threat of insider attacks on the cloud, each IoCT
device must decide which signals to trust from cloud services.
Trust refers to positive beliefs about the perceived reliability
of, dependability of, and conï¬dence in another entity [12].
These entities may be agents in an IoCT with misaligned
incentives. Many speciï¬c processes in the IoCT require trust,
such as data collection, aggregation and processing, privacy
protection, and user-device trust in human-in-the-loop inter-
actions [13]. While many factors inï¬‚uence trust, including
subjective beliefs, we focus on objective properties of trust.
These include 1) reputation, 2) promises, and 3) interaction
context. Many trust management systems are based on tracking
reputation over multiple interactions. Unfortunately, agents in
the IoCT may interact only once, making reputation difï¬cult to
accrue [14]. This property of IoCT also limits the effectiveness
of promises such as contracts or policies. Promises may not be
enforceable for entities that interact only once. Therefore we
focus on strategic trust that is predictive rather than reactive.
We use game-theoretic utility functions to capture the moti-
vations for entities to be trustworthy. These utility functions
change based on the particular context of the interaction. In
this sense, our model of strategic trust is incentive-compatible,
i.e., consistent with each agent acting in its own self-interest.

C. Game-Theoretic iSTRICT Model

We propose a framework called iSTRICT, which is com-
posed of three interacting layers: a cloud layer, a commu-
nication layer, and a physical layer. In the ï¬rst layer, the
cloud-services are threatened by attackers capable of APTs
and defended by network administrators (or â€œdefendersâ€). The
interaction at each cloud-service is modeled using the FlipIt
game recently proposed by Bowers et al. [10] and van Dijk et
al. [15]. iSTRICT uses one FlipIt game per cloud-service.
In the communication layer, the cloud-servicesâ€”which may

be controlled by the attacker or defender according to the
outcome of the FlipIt gameâ€”transmit information to a
the cloud-services.
device which decides whether to trust
This interaction is captured using a signaling game. At the
physical layer, the utility parameters for the signaling game are
determined using optimal control. The cloud, communication,
layers are interdependent. This motivates an
and physical
overall equilibrium concept called Gestalt Nash equilibrium
(GNE). GNE requires each game to be solved optimally given
the results of the other games. Because this is a similar idea
to best-response in Nash equilibrium, we call the multi-game
framework a game-of-games.

D. Contributions

In summary, we present the following contributions:
1) Trust Model: We develop a multi-layer framework
(iSTRICT) and associated equilibrium concept (GNE)
to capture interdependent strategic trust in the cloud-
enabled IoCT. iSTRICT combines analysis at the cloud,
communication, and physical layers.

2) GNE Analysis: We prove the existence of GNE, and
we show that strategic trust in the communication layer
guarantees a worst-case probability of compromise re-
gardless of attack costs in the cyber layer.

3) Adaptive Algorithm: We present an adaptive algorithm
using best-response dynamics to compute a GNE.
4) Autonomous Vehicle Application: We simulate the
control of a pair of autonomous vehicles using iSTRICT,
and show improvement over the performance under
naive policies.

The rest of the paper proceeds as follows. In Section II,
we give a broad outline of the iSTRICT model. Section III
presents the details of the FlipIt game, signaling game,
physical layer control system, and equilibrium concept. Then,
in Section IV, we study the equilibrium analytically using an
adaptive algorithm. Finally, we apply the framework to the
control of autonomous vehicles in Section V.

E. Related Work

Designing trustworthy cloud service systems has been inves-
tigated extensively in the literature. Various methods, including
a feedback evaluation component, Bayesian game, and domain
partition have been proposed [16]â€“[18]. Trust models to pre-
dict the cloud trust values (or reputation) can be mainly divided
into objective and subjective classes. The ï¬rst are based on
the quality of service parameters, and the second are based on
feedback from cloud service users [16], [19].

In the IoCT, however, agents may not have sufï¬cient number
of interactions, which makes reputation challenging to obtain
[14]. In addition, trust value-based cloud trust management
systems can be compromised by reputation attacks through
fake feedback which can severely degrade the system perfor-
mance [16], [20]. Therefore, in this work, we aim to design a
strategic trust mechanism which is predictive rather than reac-
tive through an integrative game-theoretic framework. Rather
than using trust value [20], [21], IoCT devices in our iSTRICT

model make decisions based on the strategies of players at
the cloud layer as well as based on the physical system
performance. This multi-layer design provides resilience to
reputation attacks.

Cyber-physical systems security becomes a critical concern
due to the prevailing threats from both cyber and physical
components in the system [22]â€“[24]. To facilitate a secure
system design, game theory has been widely adopted to model
and capture the strategic interactions between the attackers and
defenders [25]â€“[27]. Our iSTRICT framework builds on two
existing game models. One is the signaling game which has
been used in intrusion detection systems [28] and network
defense [29]. The other one is the FlipIt game [10], [15]
which has been applied to security of a single cloud service
[25], [30] as well as AND/OR combinations of cloud services
[31]. In contrast to previous works, in this paper we propose a
three-layer interdependent model to enable devices to decide
whether to trust cloud services that may be compromised.
Speciï¬cally, trust management decisions are coupled by the
dynamics of cloud-enabled devices, because data provided by
the cloud services is used for feedback control. Devices must
balance the need for as many data sources as possible (in
order to increase the quality of the feedback control) with
the imperative to reject data sources that are compromised by
attackers.

In terms of the technical framework, iSTRICT builds on
existing achievements in IoCT architecture design [6], [32]â€“
[34], which describe the roles of different layers of the IoCT
at which data is collected, processed, and accessed by devices
[33]. Each layer of the IoCT consists of different enabling
technologies such as wireless sensor networks and data man-
agement systems [34]. Our perspective, however, is distinct
from this literature because we emphasize an integrated math-
ematical framework. iSTRICT leverages game theory to obtain
optimal defense strategies for IoCT components, and it uses
control theory to quantify the performance of devices.

II.

ISTRICT OVERVIEW

We consider a cyber-physical attack in which an adversary
penetrates a cloud service in order to transmit malicious
signals to a physical device and cause improper operation.
This type of cross-layer attack is increasingly relevant in IoCT
settings. Perhaps the most famous cross-layer attack is the
Stuxnet attack that damaged Iranâ€™s nuclear program. But even
more recently, an attacker allegedly penetrated the supervisory
control and data acquisition (SCADA) system that controls the
Bowman Dam, located less than 20 miles north of Manhattan.
The attacker gained control of a sluice gate which manages
water level1 [35]. Cyber-physical systems ranging from the
smart grid to public transportation need to be protected from
similar attacks.

The iSTRICT framework offers a defense-in-depth approach
to IoCT security. In this section, we introduce each of the
three layers of iSTRICT very brieï¬‚y, in order to focus on the
interaction between the layers. We describe an equilibrium

1The sluice gate happened to be disconnected for manual repair at the time,

however, so the attacker could not actually change water levels.

TABLE I: Nomenclature

Notation
S = {1, 2, . . . , N }
Ai, Di, i âˆˆ S, R

A]iâˆˆS
D]iâˆˆS
A]iâˆˆS
D]iâˆˆS
A, vi

vA = [vi
vD = [vi
pA = [pi
pD = [pi
A = T Fi (vi
piâˆ—
D)
D) âˆˆ T S (pA)
A, vâˆ—
(vâˆ—
A, f i
f i
D
uFi
A, f i
A (f i
D)
uFi
A, f i
D (f i
D)
Î¸ = [Î¸i]iâˆˆS
Î¸i âˆˆ Î˜ = {Î¸A, Î¸D}
m = [mi]iâˆˆS
mi âˆˆ M = {mL, mH }
a = [ai]iâˆˆS
ai âˆˆ A = {aT , aN }
A (m, a), uSi
uSi
D (m, a)
uS
R(Î¸, m, a)
Ïƒi
A(m) âˆˆ Î£A
Ïƒi
D(m) âˆˆ Î£D
ÏƒR(a | m) âˆˆ Î£N
R
Âµ(Î¸ | m) = [Âµi(Î¸ | m)]iâˆˆS
Â¯uSi
A ; Ïƒâˆ’i
A (ÏƒR; Ïƒi
D )
Â¯uSi
D, Ïƒâˆ’i
D (ÏƒR; Ïƒâˆ’i
D )

A, Ïƒâˆ’i
A ; Ïƒi

x[k], Ë†x[k], u[k],
A[k], âˆ†i
âˆ†i
D[k],
ÎÎ¸[k]
y[k], Ëœy[k],
Î¾, Î¶
Î½[k], (cid:15)
DÏƒR (Î½[k])
A/vi
Vi, PRi
A = ËœT Fi (vi
piâˆ—
AD)
AD âˆˆ ËœT S (pA)
vâˆ—
AD âˆˆ ËœT Sâ—¦F (vAD)
vâˆ—
AD âˆˆ ËœT Sâ—¦F (vâ€ 
vâ€ 
AD)

AD = vi
vi

D, i âˆˆ S

Meaning
Cloud services (CSs)
Attackers, defenders, device
Values of CSs for A
Values of CSs for D
Probabilities that A controls CSs
Probabilities that D controls CSs
FlipIt mapping for CS i
Signaling game mapping
Frequencies Ai and Di
Aiâ€™s utility in FlipIt game i
Diâ€™s utility in FlipIt game i
Types of CSs
Type spaces of CSs
Messages from CSs
Low or high risk message
Actions for CSs
Trust or not trust action
Signaling game utility of Ai and Di
Signaling game utility for R
Signaling game mixed strategies of Ai
Signaling game mixed strategies of Di
Signaling game mixed strategy for R
Beliefs of R about CSs
Signaling game utility for Ai
Signaling game utility for Di
State, estimated state, control
bias terms of Ai and Di
cloud type matrix
Measurements without and with biases
Covariance matrices of noises
Innovation, innovation thresholds
Innovation gate
Ratios of CSsâ€™ value for Ai and Di
AD and pi
Spaces of vi
A in a GNE
Redeï¬ned FlipIt mapping for CS i
Redeï¬ned signaling game mapping
Composition of ËœT Fi , i âˆˆ S and ËœT S
Fixed-point requirement for a GNE

concept for the simultaneous steady-state of all three layers.
Later, Section III describes each layer in detail. Table I lists
the notation for the paper.

A. Cloud Layer

Consider a cloud-enabled IoCT composed of sensors that
push data to a cloud, which aggregates the data and sends
it to devices. For example, in a cloud-enabled smart home,
sensors could include lighting sensors, temperature sensors,
and blood pressure or heart rate sensors that may be placed on
the skin or embedded within the body. Data from these sensors
is processed by a set of cloud services S = {1, . . . , N } , which
make data available for control.

For each cloud service i âˆˆ S, let Ai denote an attacker
who attempts to penetrate the service using zero-day exploits,
social engineering, or other techniques described in Section I.
Similarly, let Di denote a defender or network administrator
attempting to maintain the security of the cloud service. Ai
and Di attempt to claim or reclaim control of the each cloud
service at periodic intervals. We model the interactions at all
of the services using FlipIt games, one for each of the N
services.

to the amount of time that

In the FlipIt game [10], [15], an attacker and a defender
gain utility proportional
they
control a resource (here a cloud service), and pay attack costs
proportional to the number of times that they attempt to claim
or reclaim the resource. We consider a version of the game
in which the attacker and defender are restricted to attacking
at ï¬xed frequencies. The equilibrium of the game is a Nash
equilibrium.
Let vi

D âˆˆ R denote the values of each cloud
service i âˆˆ S to Ai and Di, respectively. These quantities
represent the inputs of the FlipIt game. The outputs of
the FlipIt game are the proportions of time for which Ai
and Di control the cloud service. Denote these proportions by
pi
A âˆˆ [0, 1] and pi
A, respectively. To summarize
each of the FlipIt games, deï¬ne a set of mappings T Fi :
R Ã— R â†’ [0, 1] , i âˆˆ S, such that
A = T Fi (cid:0)vi
piâˆ—

A âˆˆ R and vi

D = 1 âˆ’ pi

A, vi
D

(1)

(cid:1)

maps the values of cloud service i for Ai and Di to the pro-
portion of time piâˆ—
A for which the service will be compromised
in equilibrium. We will study this mapping further in Section
III-A.

B. Communication Layer

In the communication layer, the cloud services i âˆˆ S, which
each may be controlled by Ai or Di, send data to a device R,
which decides whether to trust the signals. This interaction is
modeled by a signaling game. The signaling game sender is
the cloud service. The two types of the sender are attacker or
defender. The signaling game receiver is the device R. While
we used N FlipIt games to describe the cloud layer, we
use only one signaling game to describe the communication
layer, because R must decide which services to trust all at
once.

The prior probabilities in the communication layer are the
A, i âˆˆ S from
equilibrium proportions pi
A and pi
the equilibrium of the cloud layer. Denote the vectors of
(cid:3)
the prior probabilities for each sensor by pA = (cid:2)pi
iâˆˆS ,
(cid:3)
pD = (cid:2)pi
iâˆˆS . These prior probabilities are the inputs of
D
the signaling game.

D = 1 âˆ’ pi

A

The outputs of the signaling game are the equilibrium
utilities received by the senders. Denote these utilities by vi
A
D, i âˆˆ S. Importantly, these are the same quantities
and vi
that describe the incentives of A and D to control each cloud
service in the FlipIt game, because the party which controls
each service is awarded the opportunity to be the sender in
the signaling game. Deï¬ne vectors to represent each of these
iâˆˆS , vD = (cid:2)vi
utilities by vA = (cid:2)vi
(cid:3)
(cid:3)
iâˆˆS .
A
[0, 1]N â†’ P(R2N ) be a mapping that
Finally, let T S :
summarizes the signaling game, where P(X) is the power set
of X. According to this mapping, the set of vectors of signaling
game equilibrium utility ratios vâˆ—
D that result from the
vector of prior probabilities pA is given by
D) = T S (pA) .

A and vâˆ—

A, vâˆ—

(vâˆ—

(2)

D

This mapping summarizes the signaling game. We study the
mapping in detail in Section III-B.

Fig. 2: In iSTRICT, FlipIt games model attacks on the set of cloud services.
T Fi , i âˆˆ S, map the value of each service to the proportion of time that it
will be compromised in equilibrium. The communication layer is modeled by
a signaling game. T S maps probabilities of compromise to the value of each
cloud service. The cloud layer and communication layer are interdependent.
The physical layer performance quantiï¬es the utilities for the signaling game.

C. Physical Layer

Many IoCT devices such as pacemakers, cleaning robots,
appliances, and electric vehicles are dynamic systems that op-
erate using feedback mechanisms. The physical-layer control
of these devices requires remote sensing of the environment
and the data stored or processed in the cloud. The security
at the cloud and the communication layers of the system are
intertwined with the performance of the controlled devices at
the physical layer. Therefore the trustworthiness of the data has
a direct impact on the control performance of the devices. This
control performance determines the utility of the device R as
well as the utility of each of the attackers Ai and defenders Di.
The control performance is quantiï¬ed using a cost criterion for
observer-based optimal feedback control. The observer uses
data from the cloud services that R elects to trust, and ignores
the cloud services that R decides not to trust. We study the
physical layer control in Section III-C.

D. Coupling of the Cloud and Communication Layers

Clearly, the cloud and communication layers are coupled
through Eq. (1) and Eq. (2). The cloud layer security serves
as an input to the communication layer. The resulting utilities
of the signaling game at the communication layer further
becomes an input
the cloud layer.
to the FlipIt game at
In addition,
layer performance quantiï¬es the
utilities for the signaling games. Fig. 2 depicts this concept.
In order to predict the behavior of the whole cloud-enabled
IoCT, iSTRICT considers an equilibrium concept which we
call Gestalt Nash equilibrium (GNE). Informally, a triple
(cid:16)
A, vâ€ 
A, vâ€ 
pâ€ 
is a GNE if it simultaneously satisï¬es Eq. (1)
D
and Eq. (2).

the physical

(cid:17)

GNE is useful for three reasons. First, cloud-enabled IoCT
networks are dynamic. The modular structure of GNE requires
the FlipIt games and the signaling game to be at equilib-
rium given the parameters that they receive from the other type

Gestalt Nash Equilibrium Conceptï€¨ï€©ï€¨ï€©**,SvvTpïƒï€¨ï€©222*2,FpTvvï€½Physical LayerCommunication LayerCloud-Layerï€¨ï€©111*1,FpTvvï€½ï€¨ï€©*,NNNNFpTvvï€½SJuï€¨ï€©,vvpSuof game. This imposes the requirement of perfection, in the
sense that each game must be optimal given the other game.
In GNE, perfection applies in both directions, because there is
no clear chronological order or directional ï¬‚ow of information
between the two games. Actions in each sub-game must be
chosen by prior-commitment relative to the results of the other
sub-game.

Second, GNE draws upon established results from FlipIt
games and signaling games instead of attempting to ana-
lyze one large game. IoCT networks promise plug-and-play
capabilities, in which devices and users are easily able to
enter and leave the network. This also motivates plug-and-play
availability of solution concepts. The solution to one sub-game
should not need to be totally recomputed if an actor enters or
leaves another subgame. GNE follows this approach.

Finally, GNE serves as an example of a solution approach
which could be called game-of-games. The equilibrium so-
lutions to the FlipIt games and signaling game must be
rational â€œbest responsesâ€ to the solution of the other type of
game.

III. DETAILED ISTRICT MODEL

In this section, we deï¬ne more precisely the three layers of

the iSTRICT framework.

A. Cloud Layer: FlipIt Game

We use a FlipIt game to model the interactions between

the attacker and the defender over each cloud service.

1) FlipIt Actions: For each service, Ai and Di choose
A and f i
f i
D, the frequencies with which they claim or reclaim
control of the service. These frequencies are chosen by prior
commitment. Neither player knows the other playerâ€™s action
when she makes her choice. Figure 3 depicts the FlipIt
game. The green boxes above the horizontal axis represent
control of the service by Di and the red boxes below the axis
represent control of the service by Ai.

it

From f i

A and f i
D,

is easy to compute the expected
proportions of the time that A and D control service i [10],
[15]. Let R+ denote the set of non-negative real numbers.
Deï¬ne the function Ï : R+ Ã— R+ â†’ [0, 1] , such that
(cid:1) gives the proportion of the time that Ai
A = Ï (cid:0)f i
pi
will control the cloud service if he attacks with frequency
f i
A and Di renews control of the service (through changing
cryptographic keys or passwords, or through installing new
hardware) with frequency f i

A, f i
D

D. We have

Ï (cid:0)f i

A, f i
D

(cid:1) =

ï£±
ï£´ï£´ï£²

ï£´ï£´ï£³

0,
f i
,
A
2f i
D
1 âˆ’ f i
D
2f i
A

,

if f i
if f i

if f i

A = 0,
D â‰¥ f i
A > f i

A > 0,
D â‰¥ 0.

(3)

A > f i

Notice that when f i

is greater than the renewal frequency of Di,

of Ai
proportion of time that service i is insecure is Ï (cid:0)f i
A, f i
D
(cid:1) â‰¤ 1
and when f i
2 .

D â‰¥ 0, i.e., the attacking frequency
the
(cid:1) > 1
2 ,

A > 0, we obtain Ï (cid:0)f i

D â‰¥ f i

A, f i
D

that vi

A : R+ Ã—R+ â†’ R and Â¯uFi

2) FlipIt Utility Functions: Recall

A and vi
D
denote the value of controlling service i âˆˆ S for Ai and Di,
respectively. These quantities deï¬ne the heights of the red and
green boxes in Fig. 3. Denote the costs of renewing control of
the cloud service for the two players by Î±i
A and Î±i
D. Finally,
let Â¯uFi
D : R+ Ã—R+ â†’ R be expected
utility functions for each FlipIt game. The utilities of each
player are given in Eq. (4) and Eq. (5) by the values vi
D and
A of controlling the service multiplied by the proportions pi
vi
D
and pi
A with which the service is controlled, minus the costs
D and Î±i
Î±i
A of attempting to renew control of the service.
(cid:0)f i
Â¯uFi
D

(cid:1)(cid:1) âˆ’ Î±i

(cid:1) = vi

A, f i
D

Df i
D.

(4)

D

A, f i
D
(cid:0)f i
A, f i
D

Â¯uFi
A

(cid:0)1 âˆ’ Ï (cid:0)f i
AÏ (cid:0)f i

A, f i
D

(cid:1) = vi

(cid:1) âˆ’ Î±i

Af i
A.

(5)

A, f i

Therefore, based on the attackerâ€™s action f i
determines f i
of controlling the cloud service i, 1âˆ’Ï(f i
the cost of choosing f i
D.

A, the defender
D strategically to maximize the proportional time
D), and minimize

Note that in the game, the attacker knows vi

A and Î±i
A,
and the defender knows vi
A, f i
D and Î±i
D)
is public information, and hence both players know the fre-
quencies of control of the cloud through (3). Therefore, the
communication between two players at the cloud layer is not
necessary when determining their strategies.

D. Furthermore, Ï(f i

3) FlipIt Equilibrium Concept: The equilibrium concept
for the FlipIt game is Nash equilibrium, since it
is a
complete information game in which strategies are chosen by
prior commitment.

Deï¬nition 1. (Nash Equilibrium) A Nash equilibrium of the
FlipIt game played for control of service i âˆˆ {1, . . . , N }
is a strategy proï¬le (cid:0)f iâˆ—

A , f iâˆ—
D

(cid:1) such that
(cid:0)f iâˆ—

Â¯uFi
D

A , f iâˆ—
D

(cid:1) ,

f iâˆ—
D âˆˆ arg max
DâˆˆR+
f i

f iâˆ—
A âˆˆ arg max
AâˆˆR+
f i

Â¯uFi
A

(cid:0)f iâˆ—

A , f iâˆ—
D

(cid:1) ,

(6)

(7)

where Â¯uFi

D and Â¯uFi

A are computed by Eq. (4) and Eq. (5).
let

D and f iâˆ—
A ,

From the equilibrium frequencies f iâˆ—

the
equilibrium proportion of time that Ai controls cloud service
i be given by piâˆ—
A according to Eq. (3). The Nash equilibrium
solution can then be used to determine the mapping in Eq. (1)
from the cloud service values vi
D to the equilibrium
A , where T Fi : R Ã— R â†’ [0, 1].
attacker control proportion piâˆ—
The T Fi mappings, i âˆˆ S, constitute the top layer of Fig. 2.

A and vi

B. Communication Layer: Signaling Game

Because the cloud services are vulnerable, devices which
depend on data from the services should rationally decide
whether to trust
them. This is captured using a signaling
game. In this model, the device R updates a belief about the
state of each cloud service and decides whether to trust it.
Figure 4 depicts the actions that correspond to one service
of the signaling game. Compared to the trust value-based
cloud trust management system where the reputation attack
can signiï¬cantly inï¬‚uence the trust decision [16], [20], in

evidence-based signaling game approaches could be used to
update belief in a manner robust to reputation attacks [29],
[36], [37].

Based on these beliefs, R chooses which cloud services to
trust. For each service i, R chooses ai âˆˆ A = {aT , aN } where
aT denotes trusting the service (i.e., using it for observer-based
optimal feedback control) and aN denotes not trusting the ser-
vice. Assume that R, aware of the system dynamics, chooses
actions for each service simultaneously, i.e., a = (cid:2)ai(cid:3)

iâˆˆS .

Next, deï¬ne ÏƒR : AN â†’ [0, 1] such that ÏƒR (a | m) âˆˆ Î£N
R
gives the mixed strategy probability with which R plays the
vector of actions a given the vector of risk levels m.

4) Signaling Game Utility Functions: Let Râ€™s utility func-
R : Î˜N Ã— M N Ã— AN â†’ R, such that
tion be denoted by uS
uS
R (Î¸, m, a) gives the utility that R receives when Î¸ is the
vector of cloud service types, m is the vector of risk levels,
and R chooses the vector of actions a.
For i âˆˆ S, deï¬ne the functions uSi
A : M N Ã— AN â†’ R and
A (m, a) and uSi
D : M N Ã— AN â†’ R, such that uSi
uSi
D (m, a)
give the utility that Ai and Di receive for service i when the
risk levels are given by the vector m, and R plays the vector
of actions a.

Next, consider expected utilities based on the strategies of
R â†’ R denote the expected utility
each player. Let Â¯uS
function for R, such that Â¯uS
R (ÏƒR | m, Âµ (â€¢ | m)) gives Râ€™s
expected utility when he plays mixed strategy ÏƒR given that
he observes risk levels m and has belief Âµ. We have

R : Î£N

Â¯uS
R (ÏƒR | m, Âµ) =

(cid:88)

(cid:88)

Î¸âˆˆÎ˜m

aâˆˆAm

uS
R (Î¸, m, a) Âµ (Î¸ | m) ÏƒR (a | m) .

(8)

,

A =

and Ïƒâˆ’i

R Ã— Î£N

A Ã— Î£N âˆ’1

(cid:111)
Ïƒj
D | j âˆˆ S\{i}

Ai and Di, deï¬ne Ïƒâˆ’i
(cid:110)

In order to compute the expected utility functions for
(cid:111)
(cid:110)
Ïƒj
A | j âˆˆ S\{i}
D =
the sets of the strategies of all of the
senders except the sender on cloud service i. Then deï¬ne Â¯uSi
A :
(cid:1)
D â†’ R such that Â¯uSi
A ; Ïƒâˆ’i
Î£N
D
gives the expected utility to Ai when he plays mixed strategy
Ïƒi
A, and the attackers and defenders on the other services
play Ïƒâˆ’i
D . Deï¬ne the expected utility to Di by
(cid:1) in a similar manner.
(cid:0)ÏƒR; Ïƒâˆ’i
Â¯uSi
D, Ïƒâˆ’i
D
Let X i âˆˆ {A, D} denote the player that controls service i
and X âˆˆ {A, D}N denote the set of players that control each
service. Then the expected utilities are computed by

A and Ïƒâˆ’i
A ; Ïƒi

(cid:0)ÏƒR; Ïƒi

A, Ïƒâˆ’i

A

D

Â¯uSi
A

(cid:0)ÏƒR; Ïƒi

A, Ïƒâˆ’i

A ; Ïƒâˆ’i

D

(cid:1) =

(cid:88)

(cid:88)

(cid:88)

. . .

(cid:88)

mâˆˆM N

aâˆˆAN
(cid:88)

(cid:88)

. . .

X 1âˆˆ{A,D}
uSi
A (m, a) ÏƒR (a | m) Ïƒi

X iâˆ’1âˆˆ{A,D}

A

X i+1âˆˆ{A,D}

(cid:0)mi(cid:1) (cid:89)
jâˆˆS\{i}

Ïƒj
X j

X N âˆˆ{A,D}
(cid:0)mj(cid:1) pj

X j ,

(9)

Fig. 3: In each FlipIt game, Ai and Di periodically claim control of cloud
service i. The values of the service for each player are given by vi
D,
which depend on the equilibrium of the signaling game.

A and vi

Fig. 4: The vector of types Î¸ âˆˆ Î˜N deï¬nes whether each cloud service i âˆˆ S
is controlled by an attacker or defender. Each prior probability pi
A comes
from the corresponding FlipIt game. The player who controls each service
chooses mi. R observes all mi and chooses ai, i âˆˆ S simultaneously. Here,
we show one service, although all of the services are coupled.

iSTRICT, Râ€™s decision is based on the strategies of each
Ai and Di at the cloud layer as well as the physical layer
performance, and hence it does not depend on the feedback
of cloud services from users which could be malicious due to
attacks. We next present the detailed model of signaling game.

.

Î¸2

iâˆˆS

. . .

Î¸m (cid:3)T

1) Signaling Game Types: The types of each cloud service
i âˆˆ S are Î¸i âˆˆ Î˜ = {Î¸A, Î¸D} , where Î¸i = Î¸A indicates that
the service is compromised by Ai, and Î¸ = Î¸D indicates that
the service is controlled by Di. Denote the vector of all the
service types by Î¸ = (cid:2)Î¸i(cid:3)

(cid:44) (cid:2) Î¸1
2) Signaling Game Messages: Denote the risk level of the
data from each service i by mi âˆˆ M = {mL, mH } , where
mL and mH indicate low-risk and high-risk messages, respec-
tively. (We deï¬ne this risk level in Section III-C.) Further,
deï¬ne the vector of all of the risk levels by m = (cid:2)mi(cid:3)

iâˆˆS .
Next, deï¬ne mixed strategies for Ai and Di. Let Ïƒi

A :
D : M â†’ [0, 1] be functions such that
(cid:1) âˆˆ Î£D give the proportions
A and
D, respectively, from each cloud service i that they control.
D, depending on who

M â†’ [0, 1] and Ïƒi
(cid:0)mi
Ïƒi
A
with which Ai and Di send messages with risk levels mi
mi
Note that R only observes mi
controls the service i. Let

(cid:1) âˆˆ Î£A and Ïƒi

A or mi

(cid:0)mi

A

D

D

mi =

(cid:40)

mi
mi

A,
D,

if
if

Î¸i = Î¸A
Î¸i = Î¸D

,

iâˆˆS .

denote risk level of the message that R actually observes.
Finally, deï¬ne the vector of observed risk levels by m =
(cid:2)mi(cid:3)

3) Signaling Game Beliefs and Actions: Based on the risk
levels m that R observes, it updates its vector of prior beliefs
pA. Deï¬ne Âµi : Î˜ â†’ [0, 1] , such that Âµi (cid:0)Î¸ | mi(cid:1) gives the
belief of R that service i âˆˆ S is of type Î¸ given that R observes
risk level mi. Also write the vector of beliefs as Âµ (Î¸ | m) =
(cid:2)Âµi (cid:0)Î¸i | mi(cid:1)(cid:3)
iâˆˆS . As a direction for future work, we note that

timeğ’Ÿğ‘–moveğ’œğ‘–moveğœƒğ’Ÿğœƒğ’œğ‘£ğ’Ÿğ‘–ğ‘£ğ’œğ‘–1/ğ‘“ğ’Ÿğ‘–1/ğ‘“ğ’œğ‘–ğœƒğ’œğœƒğ’Ÿğ‘ğ’œğ‘–ğ‘šğ»ğ‘šğ¿ğ‘šğ»ğ‘šğ¿ğ‘ğ‘‡ğ‘ğ‘ğ‘ğ‘‡ğ‘ğ‘ğ‘ğ‘‡ğ‘ğ‘ğ‘ğ‘‡ğ‘ğ‘Â¯uSi
D

(cid:0)ÏƒR; Ïƒâˆ’i

A ; Ïƒi

D, Ïƒâˆ’i
D

(cid:1) =

(cid:88)

(cid:88)

(cid:88)

. . .

(cid:88)

mâˆˆM N

aâˆˆAN
(cid:88)

(cid:88)

. . .

X 1âˆˆ{A,D}
uSi
D (m, a) ÏƒR (a | m) Ïƒi

D

X iâˆ’1âˆˆ{A,D}

X i+1âˆˆ{A,D}
Ïƒj
X j

(cid:0)mi(cid:1) (cid:89)
jâˆˆS\{i}

X N âˆˆ{A,D}
(cid:0)mj(cid:1) pj

X j .

(10)

5) Perfect Bayesian Nash Equilibrium Conditions: Finally,
we can state the requirements for a perfect Bayesian Nash
equilibrium (PBNE) for the signaling game [38].

let Â¯uS

A, Ïƒâˆ’i

(cid:0)ÏƒR; Ïƒi
D, Ïƒâˆ’i

(PBNE) For the device,

let Â¯uSi
A
(cid:0)ÏƒR; Ïƒâˆ’i
A ; Ïƒi

R (ÏƒR | m, Âµ)
Deï¬nition 2.
be formulated according to Eq.
(8). For each service
(cid:1) be given by Eq. (9)
i âˆˆ S,
A ; Ïƒâˆ’i
(cid:1) be given by Eq. (10). Finally,
and Â¯uSi
D
let vector pA give the prior probabilities of each ser-
vice being compromised. Then, a perfect Bayesian Nash
equilibrium of
the signaling game is a strategy pro-
(cid:1) and a vector of beliefs
ï¬le (cid:0)Ïƒâˆ—
A ; Ïƒ1âˆ—
Âµ (Î¸ | m) such that the following hold:

D , . . . ÏƒN âˆ—
D

A , . . . ÏƒN âˆ—

R; Ïƒ1âˆ—

D

D

âˆ€i âˆˆ S, Ïƒiâˆ—

A (â€¢) âˆˆ arg max
AâˆˆÎ£A

Ïƒi

âˆ€i âˆˆ S, Ïƒiâˆ—

D (â€¢) âˆˆ arg max
DâˆˆÎ£D

Ïƒi

Â¯uSi
A

Â¯uSi
D

(cid:0)Ïƒâˆ—

R; Ïƒi

A, Ïƒâˆ’iâˆ—

A ; Ïƒâˆ’iâˆ—

D

(cid:0)Ïƒâˆ—

R; Ïƒâˆ’iâˆ—

A ; Ïƒi

D, Ïƒâˆ’iâˆ—
D

(cid:1) ,

(11)

(cid:1) ,

(12)

âˆ€m âˆˆ M, Ïƒâˆ—

R âˆˆ arg max
ÏƒRâˆˆÎ£m
R

Â¯uS
R (ÏƒR | m, Âµ (â€¢ | m)) ,

(13)

and âˆ€i âˆˆ S,

Âµi (cid:0)Î¸A | mi(cid:1) =

(cid:0)mi(cid:1) pi

A

Ïƒiâˆ—
A
A + Ïƒiâˆ—

A (mi) pi
Ïƒiâˆ—
(cid:0)mi(cid:1) pi
(cid:0)mi(cid:1) pi

D (mi) (cid:0)1 âˆ’ pi
D (cid:54)= 0, and Âµi (cid:0)Î¸A | mi(cid:1) âˆˆ [0, 1] , if
D = 0. Additionally, Âµi (cid:0)Î¸D | mi(cid:1) =

A

(cid:1) ,

(14)

(cid:0)mi(cid:1) pi

A+Ïƒiâˆ—
if Ïƒiâˆ—
D
A
A + Ïƒiâˆ—
Ïƒiâˆ—
D
A
1 âˆ’ Âµi (cid:0)Î¸A | mi(cid:1) in both cases.

(cid:0)mi(cid:1) pi

Note that we have denoted the equilibrium utilities for Ai

and Di, i âˆˆ S by

A = Â¯uSi
vi
A

D = Â¯uSi
vi
D

(cid:0)Ïƒâˆ—

R; Ïƒiâˆ—

A , Ïƒâˆ’iâˆ—

A ; Ïƒâˆ’iâˆ—

D

(cid:0)Ïƒâˆ—

R; Ïƒâˆ’iâˆ—

A ; Ïƒiâˆ—

D , Ïƒâˆ’iâˆ—
D

(cid:1) ,

(cid:1) ,

(15)

(16)

and the vectors of those values by vA = (cid:2)vi
iâˆˆS , vD =
(cid:2)vi
(cid:3)
iâˆˆS . We now have the complete description of the sig-
naling game mapping Eq. (2), where T S : [0, 1]N â†’ P(R2N ).
This mapping constitutes the middle layer of Fig. 2.

A

D

(cid:3)

1) Device Dynamics: Each device in the IoCT is governed
by dynamics. We can capture the dynamics of the things by
the linear system model

x[k + 1] = Ax[k] + Bu[k] + w[k],

(17)

where A âˆˆ RnÃ—n, B âˆˆ RnÃ—q, x[k] âˆˆ Rn is the system
state, u[k] âˆˆ Rq is the control input, w[k] denotes the system
white noise, and x[0] = x0 âˆˆ Rn is given. Let y[k] âˆˆ RN
represent data from cloud services which suffers from white,
additive Gaussian sensor noise given by the vector v[k]. We
have y[k] = Cx[k] + v[k], where C âˆˆ RN Ã—n is the output
matrix. Let the system and sensor noise processes have known
covariance matrices E {w[k]w(cid:48)[k]} = Î¾, E {v[k]v(cid:48)[k]} = Î¶,
where Î¾ and Î¶ are symmetric, positive, semi-deï¬nite matrices,
and w(cid:48)[k] and v(cid:48)[k] denote the transposes of the noise vectors.
In addition, for each cloud service i âˆˆ S, the attacker Ai and
defender Di in the signaling game choose whether to add bias
D[k] âˆˆ R denote
terms to the measurement yi[k]. Let âˆ†i
these bias terms. The actual noise levels that R observes
depends on who controls the service in the FlipIt game.
Recall that the vector of types of each service is given by
Î¸ = (cid:2)Î¸i(cid:3)
iâˆˆS . Let 1{â€¢} represent the indicator function, which
takes the value of 1 if its argument is true and 0 otherwise.
Then, deï¬ne the matrix

A[k], âˆ†i

ÎÎ¸ = diag (cid:8)(cid:2) 1{Î¸1=Î¸A}

. . . 1{Î¸N =Î¸A}

(cid:3)(cid:9) .

Including the bias term, the measurements are given by

Ëœy[k] = Cx[k]+v[k]+ÎÎ¸[k]âˆ†A[k]+(I âˆ’ ÎÎ¸[k]) âˆ†D[k], (18)

where I is the N -dimensional identity matrix.

2) Observer-Based Optimal Feedback Control: Let F, Q,
and R be positive-deï¬nite matrices of dimensions nÃ—n, nÃ—n,
and q Ã— q, respectively. The device chooses the control u that
minimizes the operational cost given by

(cid:40)

J = E

x(cid:48)[T ]F x[T ] +

T âˆ’1
(cid:88)

k=0

x(cid:48)[k]Qx[k] + u(cid:48)[k]Ru[k]

,

(cid:41)

(19)

subject to the dynamics of Eq. (17).

To attempt to minimize Eq. (19), the device uses observer-
based optimal feedback control. Deï¬ne P [k] by the forward
Riccati difference equation

P [k + 1] = A

(cid:16)

P [k] âˆ’ P [k]C (cid:48)

(cid:17)
(CP [k]C (cid:48) + Î¾)âˆ’1 CP [k]

A(cid:48) + Î¶,

with P [0] = E{(x[0] âˆ’ Ë†x[0])(x[0] âˆ’ Ë†x[0])(cid:48)}, and let L[k] =
P [k]C (cid:48)(CP [k]C (cid:48) + Î¾)âˆ’1. Then the observer is a Kalman ï¬lter
given by [39]

C. Physical Layer: Optimal Control

Ë†x[k + 1] = AË†x[k] + Bu[k] + L[k] (Ëœy[k] âˆ’ C Ë†x[k]) .

The utility function uS

R (Î¸, m, a) is determined by the per-
formance of the device controller as shown in Fig. 2. A block
illustration of the control system is shown in Fig. 5. Note that
the physical system in the diagram refers to the IoCT devices.

3) Innovation: In this context, the term Ëœy[k] âˆ’ C Ë†x[k] is
the innovation. Label the innovation by Î½[k] = Ëœy[k] âˆ’ C Ë†x[k].
This term is used to update the estimate Ë†x[k] of the state. We
consider the components of the innovation as the signaling-
game messages that the device decides whether to trust. Let

Fig. 5: A block diagram shows the various components of the control system in the iSTRICT. The physical system refers to IoCT device whose states are
collected by sensors. Each Ai and Di in the cloud layer can add bias terms to the measured senor data before sending it to the receiver. R decides whether
to trust or not trust each of the cloud services, and then designs an optimal control for the physical system. Since the optimal control is designed over a
ï¬nite-horizon cost criterion, the loop terminates after T time steps.

us label each component of the innovation as low-risk or high-
risk. For each i âˆˆ S, we classify the innovation as

mi =

(cid:40)

mL,
mH ,

if (cid:12)
if (cid:12)

(cid:12)Î½i[k](cid:12)
(cid:12)Î½i[k](cid:12)

(cid:12) â‰¤ (cid:15)i
(cid:12) > (cid:15)i ,

where (cid:15) âˆˆ RN
++ is a vector of thresholds. Since R is
strategic, it chooses whether to incorporate the innovations
using the signaling game strategy ÏƒR(a | m), given the vector
of messages m.

Deï¬ne a strategic innovation ï¬lter by DÏƒR : RN â†’ RN
such that, given innovation Î½, the components of gated inno-
vation Â¯Î½ = DÏƒR (Î½) are given by

Â¯Î½i =

(cid:40)

Î½i,
0,

if ai = aT
otherwise

,

for i âˆˆ S. Now we incorporate the function DÏƒR into the
estimator by

Ë†x[k + 1] = AË†x[k] + Bu[k] + L[k]DÏƒR (Î½[k]) .

4) Feedback Controller: The optimal controller is given by

the feedback law u[k] = âˆ’K[k]Ë†x[k], with gain

K[k] = (B(cid:48)[k]S[K + 1]B + R)âˆ’1 B(cid:48)S[k + 1]A,

where S[k] is obtained by the backward Riccati difference
equation

S[k] = A(cid:48)(cid:16)

S[k + 1] âˆ’ S[k + 1]B

(B(cid:48)S[k + 1]B + R)âˆ’1 B(cid:48)S[k + 1]

(cid:17)

A + Q,

with S[T ] = F.

5) Control Criterion to Utility Mapping: The control cost
J determines the signaling game utility of the device R.
This utility should be monotonically decreasing in J. We
consider a mapping J (cid:55)â†’ uS
R (Î¸, m, a) =
(Â¯vR âˆ’ vR)eâˆ’Î²RJ + Â¯vR, where Â¯vR and vR denote maximum
and minimum values of the utility, and Î²R represents the
sensitivity of the utility to the control cost.

R deï¬ned by uS

D. Deï¬nition of Gestalt Nash Equilibrium

We now deï¬ne the equilibrium concept for the overall
game, which is called Gestalt Nash equilibrium (GNE). To
differentiate with the equilibria in FlipIt game and signaling
game, we use notations with a superscript â€  to emphasize the
solution at GNE.

D

(Gestalt Nash

A represents

equilibrium) The

3.
(cid:17)
, where pâ€ 

Deï¬nition
triple
(cid:16)
A, vâ€ 
A, vâ€ 
pâ€ 
the probability of
compromise of each of the cloud services, and vâ€ 
A and vâ€ 
D
represent the vectors of equilibrium utilities for Ai and Di,
i âˆˆ S, constitutes a Gestalt Nash equilibrium of the overall
game if both Eq. (20) and Eq. (21) are satisï¬ed:

âˆ€i âˆˆ {1, . . . , m} , piâ€ 

A = T Fi

(cid:16)

A, viâ€ 
viâ€ 

D

(cid:17)

,

(20)

ï£«

ï£®

ï£¬
ï£¬
ï£¬
ï£­

ï£¯
ï£¯
ï£¯
ï£°

v1â€ 
A
v2â€ 
A
...
vN â€ 
A

ï£¹

ï£º
ï£º
ï£º
ï£»

ï£®

ï£¯
ï£¯
ï£¯
ï£°

,

ï£¹

ï£¶

ï£º
ï£º
ï£º
ï£»

ï£·
ï£·
ï£·
ï£¸

v1â€ 
D
v2â€ 
D
...
vN â€ 
D

âˆˆ T S

ï£¹

ï£¶

ï£º
ï£º
ï£º
ï£»

ï£·
ï£·
ï£·
ï£¸

.

ï£«

ï£®

ï£¬
ï£¬
ï£¬
ï£­

ï£¯
ï£¯
ï£¯
ï£°

p1â€ 
A
p2â€ 
A
...
pN â€ 
A

(21)

According to Deï¬nition 3, the overall game is at equilibrium
when, simultaneously, each of the FlipIt games is at
equilibrium and the one signaling game is at equilibrium.

IV. EQUILIBRIUM ANALYSIS

In this section, we give conditions under which a GNE
exists. We start with a set of natural assumptions. Then we
narrow the search for feasible equilibria. We show that the
signaling game only supports pooling equilibria, and that only
low-risk pooling equilibria survive selection criteria. Finally,
we create a mapping that composes the signaling and FlipIt
game models. We show that this mapping has a closed graph,
and we use Kakutaniâ€™s ï¬xed-point
theorem to prove the
existence of a GNE. In order to avoid obstructing the ï¬‚ow
of the paper, we brieï¬‚y summarize the proofs of each lemma,
and we refer readers to the GNE derivations for a single cloud
service in [25] and [40].

Physical SystemSensorsEstimationOptimal ControlSystem StateSensor MeasurementsNext Step ControlEstimated System State[1][][][]xkAxkBukwkï€«ï€½ï€«ï€«[][][]ykCxkvkï€½ï€«[]xk[][][][]Ë†([])[][]ADkykkkIkkCxkï±ï±ïµï€½ï€«ï˜ï„ï€«ï€­ï˜ï„ï€­Cloud-Enabled IoCT InterfaceInnovationGated Innovation[1]Ë†[1][1]ukKkxkï€«ï€½ï€­ï€«ï€«Ë†Ë†[1][][][]([])RxkAxkBukLkDkï³ïµï€«ï€½ï€«ï€«Ë†[1]xkï€«[]kïµ[]kïµ[1]ukï€«[]ykReceiverâ€™s Strategy[]([])RkDkï³ïµïµï€½[]iAkï„Attackerâ€™s BehaviorDefenderâ€™s Behavior[]iDkï„Optimal Control of the Physical LayerTABLE II: Assumptions

Assumption (âˆ€i âˆˆ S)

0 = uSi
0 < uSi

A (mL, aN ) = uSi
A (mL, aT ) < uSi

A (mH , aN ) = uSi
D (mH , aT ) < uSi

D (mL, aN ) = uSi
D (mL, aT ) < uSi

D (mH , aN ) .
A (mH , aT ) .

âˆ€Î¸âˆ’i, mâˆ’i, aâˆ’i, uS
âˆ€Î¸âˆ’i, mâˆ’i, aâˆ’i, uS

âˆ€Î¸, mâˆ’i, aâˆ’i, uS

R (Î¸, m, Â¯a) > uR (Î¸, m, Ëœa) , where Î¸i = Î¸A, mi = mH , Â¯aâˆ’i = Ëœaâˆ’i = aâˆ’i, Â¯ai = aN , and Ëœai = aT .
R (Î¸, m, Ëœa) , where Î¸i = Î¸D, mi = mL, Â¯aâˆ’i = Ëœaâˆ’i = aâˆ’i, Â¯ai = aN , and Ëœai = aT .
R (Î¸, m, Â¯a) < uS
R (Î¸, Ëœm, a) , where ai = aT , Â¯mâˆ’i = Ëœmâˆ’i = mâˆ’i, Â¯mi = mL, and Ëœmi = mH .
R (Î¸, Â¯m, a) > uS

#

A1
A2
A3
A4
A5

A. Assumptions

A (m, a) â‰¡ uSi

D (m, a) â‰¡ uSi

For simplicity, let the utility functions of each signaling
game sender i be dependent only on the messages and actions
on cloud service i. That is, âˆ€i âˆˆ S, uSi
A (mi, ai)
and uSi
D (mi, ai). This can be removed, but
it makes analysis more straightforward. Table II gives ï¬ve
additional assumptions. Assumption A1 assumes that each Ai
and Di, i âˆˆ S, get zero utility when their messages are not
trusted. A2 assumes an ordering among the utility functions
for the senders in the signaling game. It implies that a) Ai
and Di get positive utility when their messages are trusted;
b) for trusted messages, A prefers mH to mL; and c) for
trusted messages, D prefers mL to mH . These assumptions
are justiï¬ed if the goal of the attacker is to cause damage (with
a high-risk message), while the defender is able to operate
under normal conditions (with a low-risk message).

Assumptions A3-A4 give natural requirements on the utility
function of the device. First, the worst case utility for R is
trusting a high-risk message from an attacker. Assume that, on
every channel i âˆˆ S, regardless of the messages and actions
on the other channels, R prefers to play ai = aN if mi = mH
and Î¸i = Î¸A. This is given by A3. Second, the best case utility
for R is trusting a low-risk message from a defender. Assume
that, on every channel i âˆˆ S, regardless of the messages and
actions on the other channels, R prefers to play ai = aT
if mi = mL and Î¸i = Î¸D. This is given by A4. Finally,
under normal operating conditions, R prefers trusted low-risk
messages compared to trusted high risk messages from both
an attacker and a defender. This is given by A5.

B. GNE Existence Proof

We prove the existence of a GNE using Lemmas 1-5 and

Theorem 1.

1) Narrowing the Search for GNE: Lemma 1 eliminates

some candidates for GNE.

A, vi

D > 0.

D) satisï¬es: âˆ€i âˆˆ S, vi

(GNE Existence Regimes [40]) Every GNE

Lemma 1.
A, vâ€ 
A, vâ€ 
(pâ€ 
The basic idea behind the proof of Lemma 1 is that vi

A = 0
or vi
D = 0 cause either Ai or Di to give up on capturing or
recapturing the cloud. The cloud becomes either completely
secure or completely insecure, neither of which can result in a
GNE. Lemma 1 has a signiï¬cant intuitive interpretation given
by Remark 1.
Remark 1. In any GNE, for all i âˆˆ S, R plays ai = aT
with non-zero probability. In other words, R never completely
ignores any cloud service.

2) Elimination of Separating Equilibria:

In signaling
games, equilibria in which different types of senders trans-
mit the same message are called pooling equilibria, while
equilibria in which different types of senders transmit distinct
messages are called separating equilibria [38]. The distinct
messages in separating equilibria completely reveal the type
of the sender to the receiver. Lemma 2 is typical of signaling
games between players with opposed incentives.

Equilibria

Separating

2.
all
A , . . . ÏƒN âˆ—

(No
pure-strategy
D , . . . ÏƒN âˆ—
D

[40])
Lemma
Consider
equilibria
(cid:1) in which each Ai and
(cid:0)Ïƒâˆ—
R; Ïƒ1âˆ—
Di, i âˆˆ S, receive positive expected utility. All such equilibria
D (m) for all m âˆˆ M and i âˆˆ S. That is,
satisfy Ïƒiâˆ—
the senders on each cloud service i use pooling strategies.

A (m) = Ïƒiâˆ—

signaling-game

A ; Ïƒ1âˆ—

Lemma 2 holds because it is never incentive-compatible for
an attacker Ai to reveal his type, in which case R would not
trust Ai. Hence, Ai always imitates Di by pooling.

3) Signaling Game Equilibrium Selection Criteria: Four
pooling equilibria are possible in the signaling game: Ai and
Di transmit mi = mL and R plays ai = aT (which we
label EQ-L1), Ai and Di transmit mi = mL and R plays
ai = aN (EQ-L2), Ai and Di transmit mi = mH and R
plays ai = aT (EQ-H1), and Ai and Di transmit mi = mH
and R plays ai = aN (which we label EQ-H2). In fact, the
signaling game always admits multiple equilibria. Lemma 3
performs equilibrium selection.

Lemma 3. (Selected Equilibria) The intuitive criterion [41]
and the criterion of ï¬rst mover advantage imply that equilibria
EQ-L1 and EQ-L2 will be selected.

Proof: The ï¬rst mover advantage states that, if both Ai
and Di prefer one equilibrium over the others, they will choose
the preferred equilibrium. Thus, Ai and Di will always choose
EQ-L1 or EQ-H1 if either of those is admitted. When neither
is admitted, we select EQ-L22. When both are admitted, we
use the intuitive criterion to select among them. Assumption
A2 states that Ai prefers EQ-H1, while Di prefers EQ-L1.
Thus, if a sender deviates from EQ-H1 to EQ-L1, R can
infer that the sender is a defender, and trust the message.
Therefore, the intuitive criterion rejects EQ-H1 and selects
EQ-L1. Finally, Assumption A5 can be used to show that EQ-
H1 is never supported without EQ-L1. Hence, only EQ-L1
and EQ-L2 survive the selection criteria.

At the boundary between the parameter regime that supports
EQ-L1 and the parameter regime that supports EQ-L2, R can
choose any mixed strategy, in which he plays both ai = aT

2This is without loss of generality, since A1 implies that the sender utilities

are the same for EQ-L2 and EQ-H2.

and ai = aN with some probability. Indeed, for any cloud
service i âˆˆ S, hold pj
A, j (cid:54)= i and j âˆˆ S, constant, and let pi(cid:5)
A
denote the boundary between the EQ-L1 and EQ-L2 regions.
Then Remark 2 gives an important property of pi(cid:5)
A .
Remark 2. By Lemma 1, all GNE satisfy pi
A â‰¤ pi(cid:5)
A . Therefore,
pi(cid:5)
A is a worst-case probability of compromise.
Remark 2 is a result of the combination of the signaling
and FlipIt games. Intuitively, it states that strategic trust
in the communication layer is able to limit the probability of
compromise of a cloud service, regardless of the attack and
defense costs in the cyber layer.

4) FlipIt Game Properties: For the FlipIt games on
each cloud service i âˆˆ S, denote the ratio of attacker and
D. For i âˆˆ S,
defender expected utilities by vi
deï¬ne the set Vi by
(cid:110)

AD = vi

A/vi

(cid:111)

v âˆˆ R+ : 0 â‰¤ v â‰¤ uSi

A (mL, aT )/uSi

D (mL, aT )

Vi =

.

Also deï¬ne the set PRi, i âˆˆ S, by PRi =
(cid:110)

(cid:16)

p âˆˆ [0, 1] : 0 < p < T Fi

A (mL, aT ), uSi
uSi

D (mL, aT )

(cid:17)(cid:111)

.

Next, for i âˆˆ S, deï¬ne modiï¬ed FlipIt game mappings
ËœT Fi : Vi â†’ PRi, where

AD

(cid:1) â‡â‡’ piâˆ—

A âˆˆ T Fi (cid:0)vi

A = ËœT Fi (cid:0)vi
piâˆ—
Then Lemma 4 holds.
Lemma 4. (Continuity of ËœT Fi [25]) For i âˆˆ S, ËœT Fi (vi
continuous in vi

A, vi
D

(cid:1) .

AD âˆˆ Vi.

(22)

AD) is

The dashed curve in Fig. 6 gives an example of ËœT Fi for
i = 1. The independent variable is on the vertical axis, and
the dependent variable is on the horizontal axis.
5) Signaling Game Properties: Let vAD = [vi
iâˆˆS Vi, and PR = (cid:81)

AD]iâˆˆS, V =
iâˆˆS PRi. Deï¬ne a modiï¬ed signaling

(cid:81)
game mapping by ËœT S : PR â†’ P(V) such that
AD âˆˆ ËœT S (pA) â‡â‡’ (vâˆ—
vâˆ—

D) âˆˆ T S (pA) ,

A, vâˆ—

(23)

where T S selects the equilibria given by Lemma 3. Then we
have Lemma 5.
Lemma 5. (Properties of ËœT S) Construct a graph

G =

(cid:110)

(pA, vâˆ—

AD) âˆˆ PR Ã— V : vâˆ—

AD âˆˆ ËœT S (pA)

(cid:111)

,

The graph G is closed. Additionally, for every pA âˆˆ PR, the
set of outputs of ËœT S(pA) is non-empty and convex.

Proof: The graph G is closed because it contains all of
its limit points. The set of outputs is non-empty because a
signaling game equilibrium exists for all pA. It is convex
because expected utilities for mixed-strategy equilibria are
convex combinations of pure strategy utilities and because
assumption A2 implies that convexity also holds for the ratio
of the utilities.

The step functions (plotted with solid lines) in Figure 6 plot

example mappings from p1
on the vertical axis for vAD âˆˆ ËœT S(pA), holding pi
{2, 3, . . . , N } ï¬xed. It is clear that the graphs are closed.

A on the horizontal axis to v1

AD
A, i âˆˆ

Fig. 6: The (solid) step-functions depict modiï¬ed signaling game mappings
ËœT S for ï¬ve different sets of parameters. The (dashed) curve depicts a modiï¬ed
FlipIt game mapping ËœT F1 . The intersection is a GNE. The ï¬gure shows
only one dimension out of N dimensions.

6) Fixed-Point Theorem: By combining Eq. (20-21) with
Eq. (22) and Eq. (23), we see that the vector of equilibrium
utility ratios vâ€ 
D) must
satisfy

AD]iâˆˆS in any GNE (pâ€ 

AD = [viâ€ 

A, vâ€ 

A, vâ€ 

ï£¹

ï£º
ï£º
ï£º
ï£»

ï£®

ï£¯
ï£¯
ï£¯
ï£°

v1â€ 
AD
v2â€ 
AD
...
vN â€ 
AD

âˆˆ ËœT S

ï£«

ï£®

ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ËœT F1
ËœT F2

ËœT F2

(cid:17)

(cid:17)

(cid:17)

ï£¹

ï£¶

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

.

(cid:16)

(cid:16)

v1â€ 
AD
v2â€ 
AD
...
(cid:16)
vN â€ 
AD

Denote this composed mapping by ËœT Sâ—¦F : V â†’ P(V)
such that the GNE requirement can be written by vâ€ 
AD âˆˆ
ËœT Sâ—¦F (vâ€ 
AD). Figure 6 gives a one-dimensional intuition be-
hind Theorem 2. The example signaling game step functions
ËœT S have closed graphs, and the outputs of the functions are
non-empty and convex. The FlipIt curve ËœT F1 is continu-
ous. The two mappings are guaranteed to intersect, and the
intersection is a GNE.

According to Lemma 5, the graph G of the signaling game
mapping is closed, and the set of outputs of ËœT S is non-empty
and convex. Since each modiï¬ed FlipIt game mapping ËœT Fi,
i âˆˆ S is a continuous function, each ËœT Fi produces a closed
graph and has non-empty and (trivially) convex outputs. Thus,
the graph of the composed mapping, ËœT Sâ—¦F , is also closed, and
has non-empty and convex outputs. Because of this, we can
apply Kakutaniâ€™s ï¬xed-point theorem, famous for its use in
proving Nash equilibrium.

Theorem 1. (Kakutani Fixed-Point Theorem [42]) - Let Î¦ be
a non-empty, compact, and convex subset of some Euclidean
space Rn. Let Z : Î¦ â†’ P(Î¦) be a set-valued function on
Î¦ with a closed graph and the property that, for all Ï† âˆˆ Î¦,
Z(Ï†) is non-empty and convex. Then Z has a ï¬xed point.

The mapping ËœT Sâ—¦F is a set-valued function on V, which is
a non-empty, compact, and convex subset of RN . ËœT Sâ—¦F also
has a closed graph, and the set of its outputs is non-empty and
convex. Therefore, ËœT Sâ—¦F has a ï¬xed-point, which is precisely
the deï¬nition of a GNE. Hence, we have Theorem 2.

-4-3.5-3-2.5-2-1.5-1-0.5000.511.522.533.544.55Theorem 2. (GNE Existence) Let the utility functions in the
signaling game satisfy Assumptions A1-A5. Then a GNE exists.

Proof: The proof has been constructed from Lemmas 1-5

and Theorem 1.

Algorithm 1 Adaptive defense algorithm for iSTRICT

1) Initialize parameters Î±i
FlipIt game, and Ïƒi
signaling game
Signaling game:

A, Î±i
D, pi
A and Ïƒi

D, âˆ€i âˆˆ S, in each
A, pi
D, âˆ€i âˆˆ S, ÏƒR in the

2) Solve optimization problems in Eq. (11) and Eq. (12),

respectively, and obtain Ïƒiâˆ—

D , âˆ€i âˆˆ S
3) Update belief Âµi(Î¸A | mi) based on Eq.
Âµi(Î¸D | mi) = 1 âˆ’ Âµi(Î¸A | mi), âˆ€i âˆˆ S

A and Ïƒiâˆ—

(14), and

4) Solve receiverâ€™s problem in Eq. (13) and obtain Ïƒâˆ—
R
5) If Ïƒiâˆ—

R do not change, go to step 6; otherwise,

A , Ïƒiâˆ—

D , Ïƒâˆ—

go back to step 2
A and vi

6) Obtain vi

D, âˆ€i âˆˆ S, from Eq. (15) and Eq. (16),

respectively
FlipIt game:

7) Solve defendersâ€™ and attackersâ€™ problems in Eq. (6) and

A and f iâˆ—

D , âˆ€i âˆˆ S
D ) to the probability pair

Eq. (7) jointly, and obtain f iâˆ—

8) Map the frequency pair (f iâˆ—

A , f iâˆ—
D ) through Eq. (3), âˆ€i âˆˆ S
D , piâˆ—

(piâˆ—
A , piâˆ—
9) If (piâˆ—

A ), âˆ€i âˆˆ S, do not change, go to step 10;

otherwise, go back to step 2
10) Return pâ€ 
A := Ïƒiâˆ—
A := pâˆ—
Ïƒâ€ 
R := Ïƒâˆ—
R

A, Ïƒiâ€ 

A , Ïƒiâ€ 

D := Ïƒiâˆ—

D , âˆ€i âˆˆ S, and

C. Adaptive Algorithm

Numerical simulations suggest

that Assumptions A1-A5
often hold. If this is not
the case, however, Algorithm 1
can be used to compute the GNE. The main idea of the
adaptive algorithm is to update the strategic decision-making
of different entities in iSTRICT iteratively.

A ; Ïƒ1âˆ—

A , . . . ÏƒN âˆ—

D , . . . ÏƒN âˆ—
D

Given the probability vector pA, Lines 2-5 of Algorithm
1 compute a PBNE for the signaling game which consists
of the strategy proï¬le (cid:0)Ïƒâˆ—
(cid:1) and
R; Ïƒ1âˆ—
belief vector Âµ (Î¸ | m) . The algorithm computes the PBNE
iteratively using best response. The vectors of equilibrium
utilities (vâˆ—
D) are given by Eq. (15) and Eq. (16). Using
(vâˆ—
A, vâˆ—
D) , Line 7 of Algorithm 1 updates the equilibrium
strategies of the FlipIt games and arrives at a new prior
D ), âˆ€i âˆˆ S, through the mapping in
probability pair (piâˆ—
Eq. (3). This initializes the next round of the signaling game
with the new (piâˆ—
D ). The algorithm terminates when the
probabilities remain unchanged between rounds.

A , piâˆ—

A , piâˆ—

A, vâˆ—

To illustrate Algorithm 1, we next present an example in-
cluding N = 4 cloud services. The detailed physical meaning
of each service will be presented in Section V. Speciï¬cally,
the costs of renewing control of cloud services are Î±1
A = $2k,
A = $12k, and Î±1
Î±2
A = $0.8k, Î±3
D = $0.2k,
Î±1
D = $0.1k, Î±1
D = $0.03k, for the attackers
and defenders, respectively. The initial proportions of time
of each attacker and defender controlling the cloud services

A = $10k, Î±4
D = $0.05k, Î±1

D = 0.6, p3

A = 0.6, p4

A = 0.2, p2

A = 0.4, p3

are p1
A = 0.15, and
p1
D = 0.4, p4
D = 0.8, p2
D = 0.85, respectively.
For the signaling game at the communication layer, the initial
probabilities that attacker sends low-risk message at each
cloud service are equal to 0.2, 0.3, 0.1, and 0.4, respectively.
Similarly, the defenderâ€™s initial probabilities of sending low-
risk message are equal to 0.9, 0.8, 0.95, and 0.97, respectively.
Figure 7 presents the results of Algorithm 1 on this example
system. The result in Fig. 7a shows that the cloud services
1 and 2 can be compromised by the attacker. Figure. 7a
shows the deviceâ€™s belief on the received information. At the
GNE, the attacker also sends low-risk message to deceive the
receiver and gain utility when controlling the cloud service.
Four representative devicesâ€™ actions are shown in Fig. 7b,
low-risk message in
where the devices strategically reject
some cases due to the couplings between layers in iSTRICT.
Because of the large attack and defense cost ratios and the
crucial impact on physical system performance of services 3
and 4, p3
D = 1, insuring a secure information provision.
In addition, the defense strategies at the cloud layer and the
communication layer are adjusted adaptively according to the
attackersâ€™ behaviors. Within each layer, all players are required
to best respond to the strategies of the other players. This
cross-layer approach enables a defense-in-depth mechanism
for the devices in iSTRICT.

D = p4

V. APPLICATION TO AUTONOMOUS VEHICLE CONTROL

In this section, we apply iSTRICT to a cloud-enabled
autonomous vehicle network in which the framework of ve-
hicular cloud computing is similar to the one in [43]. Two au-
tonomous vehicles use an observer to estimate their positions
and velocities based on measurements from six sources, four
of which may be compromised. They also implement optimal
feedback control based on the estimated state.

A. Autonomous Vehicle Security

Autonomous vehicle technology will make a powerful im-
pact on several industries. In the automotive industry, tra-
ditional car companies as well as technology ï¬rms such
as Google [44] are racing to develop autonomous vehicle
technology. Maritime shipping is also an attractive application
of autonomous vehicles. Autonomous ships are expected to be
safer, higher-capacity, and more resistant to piracy attacks [45].
Finally, unmanned aerial vehicles (UAVs) have the potential
to reshape ï¬elds such as mining, disaster relief, and precision
agriculture [46].

Nevertheless, autonomous vehicles pose clear safety risks.
In ground transportation, in March of 2018, an Uber self-
driving automobile struck and killed a pedestrian [47]. On
the sea, multiple crashes of ships in the Unites States Navy
during 2017 [48] have prompted concerns about too much
reliance on automation. In the air, cloud-enabled UAVs could
be subject to data integrity or availability attacks [49]. In
general, autonomous vehicles rely on many remote sources
(e.g., other vehicles, GPS signals,
location-based services)
for information. In the most basic case, these sources are
subject to errors that must be handled robustly. In addition,

(a) Device belief

Fig. 8: We use bicycle steering models from [50] to conceptually capture
the vehicle dynamics. The vehicle states are given by x1[k] : ï¬rst vehicle
position, x2[k] : ï¬rst vehicle angle; x3[k] : offset between vehicles; x4[k] :
second vehicle angle. Controls u1[k] and u2[k] represent the steering angles
of the ï¬rst and second vehicles, respectively.

(b) Device action

(c) FlipIt game

Fig. 7: Adaptive Algorithm with four cloud services. (a) and (b) depict each
deviceâ€™s belief and action, respectively. (c) shows the result of cloud security.
The algorithm converges to a GNE in four steps, where one step represents
a round of updates including the FlipIt game and signaling game.

the sources could also be selï¬sh and strategic. For instance,
an autonomous ship could transmit its own coordinates dishon-
estly in order to clear its own shipping path of other vessels.
In the worst case, the sources could be malicious. An attacker
could use a spoofed GPS signal in order to destroy a UAV or
to use the UAV to attack another target. In all of these cases,
autonomous vehicles must decide whether to trust the remote
sources of information.

Fig. 9: Local sensors include a localization camera on vehicle 1 and a range
ï¬nding device on vehicle 2. Remote sensors include magnetic compass sensors
and GPS receivers on both vehicles.

B. Physical-Layer Implementation

We consider an interaction between nine agents. Two au-
tonomous vehicles implement observer-based optimal feed-
back control according to the iSTRICT framework. Each
vehicle has two states: position and angle. Thus, the combined
system has the state vector x[k] âˆˆ R4 described in Fig. 8. The
states evolve over ï¬nite horizon k âˆˆ {0, 1, . . . , T }.

These states are observed through both remote and local
measurements. Figure 9 describes these measurements. The
local measurements y5[k] and y6[k] originate from sensors
on the autonomous vehicle, so these are secure. Hence, the
autonomous vehicle always trusts y5[k] and y6[k]. In ad-
dition, while the magnetic compass sensors are subject to
electromagnetic attack, this involves high attack costs Î±3
A and
A. The defense algorithm yields that R always trusts y3[k]
Î±4
and y4[k] at the GNE. The remote measurements Ëœy1[k] and
Ëœy2[k] are received from cloud services that may be controlled
by defenders D1 and D2, or that may be compromised by
attackers A1 and A2.

A[k] or âˆ†2

In the signaling game, attackers A1 and A2 may add bias
A[k] if Î¸1 = Î¸A or Î¸2 = Î¸A, respectively.
terms âˆ†1
Therefore, the autonomous vehicles must strategically decide
whether to trust these measurements. Each Ëœyi[k], i âˆˆ {1, 2},
is classiï¬ed as a low-risk (mi = mL) or high-risk (mi =
mH ) message according to an innovation ï¬lter. R decides
whether to trust each message according to the action vector
a = [ a1 a2 ](cid:48), where a1, a2 âˆˆ {aN , aT }. We seek an
equilibrium of the signaling game that satisï¬es Deï¬nition 2.

0510Step00.51ProbabilityDevice'sBelief7(3jm)71(3AjmL)71(3AjmH)71(3DjmL)71(3DjmH)0510Step00.51ProbabilityDevice'sBelief7(3jm)72(3AjmL)72(3AjmH)72(3DjmL)72(3DjmH)0510Step00.51ProbabilityDevice'sBelief7(3jm)73(3AjmL)73(3AjmH)73(3DjmL)73(3DjmH)0510Step00.51ProbabilityDevice'sBelief7(3jm)74(3AjmL)74(3AjmH)74(3DjmL)74(3DjmH)0510Step00.20.40.60.81ProbabilityCloudSignalRisks(mL;mL;mL;mL)(aT;aT;aT;aT)(aT;aN;aT;aT)(aN;aT;aT;aT)(aN;aN;aT;aT)0510Step00.20.40.60.81ProbabilityCloudSignalRisks(mL;mH;mH;mH)(aT;aT;aN;aN)(aT;aN;aN;aN)(aN;aT;aN;aN)(aN;aN;aN;aN)0510Step00.20.40.60.81ProbabilityCloudSignalRisks(mH;mL;mH;mL)(aT;aT;aN;aT)(aT;aN;aN;aT)(aN;aT;aN;aT)(aN;aN;aN;aT)0510Step00.20.40.60.81ProbabilityCloudSignalRisks(mH;mH;mL;mH)(aT;aT;aT;aN)(aT;aN;aT;aN)(aN;aT;aT;aN)(aN;aN;aT;aN)0246810Step00.10.20.30.40.50.60.70.80.91ProbabilityProbabilityofControllingtheCloudServicesp1Ap2Ap3Ap4Ap1Dp2Dp3Dp4Dğ‘¥1[ğ‘˜]ğ‘¥4[ğ‘˜]ğ‘¥2[ğ‘˜]ğ‘¥3[ğ‘˜]ğ‘¢1[ğ‘˜]ğ‘¢2[ğ‘˜]ğ‘¦1[ğ‘˜]ğ‘¦2[ğ‘˜]ğ‘¦3[ğ‘˜]ğ‘¦4[ğ‘˜]ğ‘¦5[ğ‘˜]ğ‘¦6[ğ‘˜]C. Signaling Game Results

Figure 10 depicts the results of three different signaling-
game strategy proï¬les for the attackers, defenders, and device,
using the software MATLAB [51]. The observer and controller
are linear, so the computation is rapid. Each iteration of the
computational elements of the control loop depicted in Fig. 5
takes less than 0.0002s on a Lenovo ThinkPad L560 laptop
with 2.30 GHz processor and 8.0 GB of installed RAM.

In all of the scenarios, we set the position of the ï¬rst vehicle
to a track a reference trajectory of x1[k] = 4, the offset
between the second vehicle and the ï¬rst vehicle to track a
reference trajectory of x3[k] = 8, and both angles to target
x2[k] = x4[k] = 0. Column 1 depicts a scenario in which
A1 and A2 send mH and R plays a1 = a2 = aT . The
spikes in the innovation represent the bias terms added by
the attacker when he controls the cloud. The spikes in Fig.
10(a) are large because these the attacker adds bias terms
corresponding to high risk messages. These bias terms cause
large deviations in the position and angle from their desired
values (Fig. 10(d)). For instance, at time 10, the two vehicles
come within approximately 4 units of each other.

Column 2 depicts the best response of R to this strategy.
The vehicle uses an innovation ï¬lter (here, at (cid:15)1 = (cid:15)2 = 10)
which categorizes the biased innovations as mH . The best
response is to choose
(cid:20) mL
mL
(cid:20) mH
mL

(cid:18)(cid:20) aT
aN
(cid:18)(cid:20) aN
aN

(cid:18)(cid:20) aT
aT
(cid:18)(cid:20) aN
aT

(cid:20) mL
mH
(cid:20) mH
mH

= ÏƒR

= ÏƒR

= 1,

= 1,

ÏƒR

ÏƒR

(cid:21)(cid:19)

(cid:21)(cid:19)

(cid:21)(cid:19)

(cid:21)(cid:19)

(cid:21)

(cid:21)

(cid:21)

(cid:21)

|

|

|

|

i.e., to trust only low-risk messages. The circled data points in
Fig. 10(b) denote high-risk innovations from the attacker that
are rejected. Figure 10(e) shows that this produces very good
results in which the positions of the ï¬rst and second vehicle
converge to their desired values of 4 and âˆ’4, respectively, and
the angles converge to 0.

A[k] and âˆ†2

But iSTRICT assumes that the attackers are also strategic.
A1 and A2 realize that high-risk messages will be rejected,
so they add smaller bias terms âˆ†1
A[k] which
are classiï¬ed as mL. This is depicted by Fig. 10(c). It is
not optimal for the autonomous vehicle to reject all low-risk
messages, because most such messages come from a cloud
controlled by the defender. Therefore, the device must play
a1 = a2 = aT . Nevertheless, Fig. 10(f) shows that these
low-risk messages create signiï¬cantly less disturbance than
the disturbances from high-risk messages in Fig. 10(d). In
summary, the signaling-game equilibrium is for A1, A2, D1,
and D2 to transmit low-risk messages and for R to trust
low-risk messages while rejecting high-risk messages off the
equlibrium path.

D. Results of the FlipIt Games

Meanwhile, A1 and D1 play a FlipIt game for control
of Cloud Service 1, and A2 and D2 play a FlipIt game
for control of Cloud Service 2. Based on the equilibrium of
the signaling game, all players realize that the winners of the
FlipIt games will be able to send trusted low-risk messages,

but not trusted high-risk messages. Based on Assumption A2,
low-risk messages are more beneï¬cial to the defenders than
to the attackers. Hence, the incentives to control the cloud are
larger for defenders than for attackers. This results in a low
p1
A and p2
A from the FlipIt game. If the equilibrium from
the previous subsection holds for these prior probabilities, then
the overall ï¬ve-player interaction is at a GNE as described in
Deï¬nition 3 and Theorem 2.

Table III is useful for benchmarking the performance of
iSTRICT. The table lists the empirical value of the control cri-
terion given by Eq. (19). The ï¬rst three columns quantify the
performance depicted in Fig. 10. Column 1 is the benchmark
case, in which A1 and A2 add high-risk noise, and the noise
is mitigated somewhat by a Kalman ï¬lter, but the bias is not
handled optimally. Column 2 shows the improvement provided
by iSTRICT against a nonstrategic attacker, and Column
3 shows the improvement provided by iSTRICT against a
strategic attacker. The improvement is largest in Column 2,
but it is signiï¬cant against a strategic attacker as well.

E. GNE for Different Parameters

Now consider a parameter change in which A2 develops
new malware to compromise the GPS position signal Ëœy2[k] at
a much lower cost Î±2
A. (See Subsection III-A). In equilibrium,
this increases p2
A from 0.03 to 0.10. A higher number of
perturbed innovations are visible in Fig. 11(a). This leads to
the poor state trajectories of Fig. 11(d). The control cost from
Eq. (19) increases, and the two vehicles nearly collide at time
8. The large changes in angles show that the vehicles turn
rapidly in different directions.

(cid:21)

(cid:21)(cid:19)

In this case, Râ€™s best response is
(cid:20) mL
mL
(cid:20) mH
mL

(cid:18)(cid:20) aT
aN
(cid:18)(cid:20) aN
aN

(cid:18)(cid:20) aT
aN
(cid:18)(cid:20) aN
aN

= ÏƒR

= ÏƒR

(cid:21)(cid:19)

(cid:21)

|

|

ÏƒR

ÏƒR

(cid:21)

(cid:21)

(cid:20) mL
mH
(cid:20) mH
mH

|

|

(cid:21)(cid:19)

(cid:21)(cid:19)

= 1,

= 1,

i.e., to not trust even the low-risk messages from the remote
GPS signal. The circles on Î½2[k] for all k in Fig. 11(b)
represent not trusting. The performance improvement can be
seen in Fig. 11(e).

Interestingly, though, Remark 1 states that this cannot be
an equilibrium. In the FlipIt game, A2 would have no
incentive to capture Cloud Service 2, since R never trusts that
cloud service. This would lead to p2
A = 0. Moving forward, R
would trust Cloud Service 2 in the next signaling game, and
A2 would renew his attacks. iSTRICT predicts that this pattern
of compromising, not trusting, trusting, and compromising
would repeat in a limit cycle, and not converge to equilibrium.
A mixed-strategy equilibrium, however, does exist. R
chooses a mixed strategy in which he trusts low-risk messages
on Cloud Service 2 with some probability. This probability
incentivizes A2 to attack the cloud with a frequency between
those that best respond to either of Râ€™s pure strategies. At the
GNE, the attack frequency of A2 produces 0 < p2
A < 0.10 in
A = p2(cid:5)
the FlipIt game. In fact, this is the worst case p2
A
from Remark 2. In essence, Râ€™s mixed-strategy serves as a

(a) Innovation

(b) Innovation

(c) Innovation

(d) State trajectories

(e) State trajectories

(f) State trajectories

Fig. 10: Column 1: A1 and A2 send mH and R plays [ aT
A2 send mL and R plays [ aT

aT ](cid:48). Row 1: Innovation, Row 2: State trajectories.

aT ](cid:48), Column 2: A1 and A2 send mH and R plays [ aN aN ](cid:48), Column 3: A1 and

TABLE III: Control Costs and Benchmarks for the Simulations depicted in Fig. 11-12

Trial 1
Trial 2
Trial 3
Trial 4
Trial 5
Trial 6
Trial 7
Trial 8
Trial 9
Trial 10
Average

Ungated mH
274,690
425,520
119,970
196,100
229,870
139,880
129,980
97,460
125,490
175,670
191,463

Gated mH
42,088
42,517
42,444
42,910
42,733
42,412
42,642
42,468
42,633
42,466
42,531

Trusted mL
116,940
123,610
125,480
89,980
66,440
69,510
116,560
96,520
50,740
78,700
93,448

Trusted, Frequent mL
185,000
211,700
213,500
239,400
94,400
2,581,500
254,000
1,020,000
250,900
4,182,600
923,300

Untrusted, Frequent mL Mixed Trust with mL

128,060
121,910
144,090
138,350
135,160
119,700
138,160
130,260
138,960
135,780
133,043

146,490
143,720
130,460
135,930
139,680
125,270
122,790
146,370
151,470
126,550
136,873

last-resort countermeasure to the parameter change due the
new malware obtained by A2.

against a nonstrategic attacker. In both cases, the cost criterion
decreases by a factor of at least six.

Figure 11(c) depicts the innovation with a mixed strategy in
which R sometimes trusts Cloud Service 2. Figure 11(f) shows
the impact on state trajectories. At this mixed-strategy equi-
librium, A1, A2, D1, and D2 choose optimal attack/recapture
frequencies in the cloud-layer and send optimal messages in
the communications layer, and R optimally chooses which
messages to trust in the communication layer based on an
innovation ï¬lter and observer-based optimal control in the
physical layer. No players have incentives to deviate from their
strategies at the GNE.

Columns 4-6 of Table III quantify the improvements pro-
vided by iSTRICT in these cases. Column 4 is the benchmark
case, in which an innovation gate forces A2 to add low-risk
noise, but his frequent attacks still cause large damages. Col-
umn 5 gives the performance of iSTRICT against a strategic
attacker, and Column 6 gives the performance of iSTRICT

VI. CONCLUSION AND FUTURE WORK

iSTRICT attains robustness through a combination of mul-
tiple, interdependent layers of defense. At the lowest physical
layer, a Kalman ï¬lter handles sensor noise. The Kalman ï¬lter,
however, is not designed for the large bias terms that can
be injected into sensor measurements by attackers. We use
an innovation gate in order to reject these large bias terms.
But even measurements within the innovation gate should
be rejected if there is a sufï¬ciently high risk that a cloud
service is compromised. We determine this threshold risk
level strategically, using a signaling game. Now, it may not
be possible to estimate these risk levels using past data.
Instead, iSTRICT estimates the risk proactively using FlipIt
games. The equilibria of the FlipIt games depend on the
incentives of the attackers and defenders to capture or reclaim

0246810-20-15-10-5051015200246810-20-15-10-5051015200246810-20-15-10-505101520024681012-8-6-4-20246810024681012-8-6-4-20246810024681012-8-6-4-20246810(a) Innovation

(b) Innovation

(c) Innovation

(d) State trajectories

(e) State trajectories

(f) FlipIt game

Fig. 11: Column 1: Innovation and state trajectories for p2
and R plays [ aT

aN ](cid:48), Column 3: Innovation and state trajectories in which R mixes strategies between [ aT

aT ](cid:48), Column 2: Innovation and state trajectories for p2
aN ](cid:48).

aT ](cid:48) and [ aT

A = 0.10

A = 0.10 and R plays [ aT

the cloud. These incentives result from the outcome of the
signaling game, which means that
the equilibrium of the
overall interaction consists of a ï¬xed point between mappings
that characterize the FlipIt games and the signaling game.
This equilibrium is a GNE.

We have proved the existence of GNE under a set of
natural assumptions, and provided an algorithm to iteratively
compute the GNE. We have shown that a device can use
iSTRICT to guarantee a worst-case compromise probability,
even without fully rejecting measurements from any of the
cloud services. Through an application to autonomous vehicle
networks, we have shown the performance gains achieved by
iSTRICT over naive strategies. Because of the modularity of
the GNE concept, the solutions to each layer do not need to be
completely recomputed when devices enter or leave the IoCT.
Future work can extend the framework to a fourth layer
composed of a cloud radio access network and a ï¬fth layer of
resource management for economic and policy issues of the
IoCT. Another promising extension is to incorporate intelligent
control designs that further mitigate the performance loss due
to cyber threats by addressing them at the physical layer.
These future directions would further contribute to policies for
strategic trust management in the dynamic and heterogeneous
IoCT.

REFERENCES

[1] M. Swan, â€œSensor mania! the internet of things, wearable computing,
objective metrics, and the quantiï¬ed self 2.0,â€ Journal of Sensor and
Actuator Networks, vol. 1, no. 3, pp. 217â€“253, 2012.

[2] â€œInternet of Things: Privacy and Security in a Connected World,â€ Federal

Trade Commission, Tech. Rep., January 2015.

[3] â€œVisions and challenges for realising the internet of things,â€ CERP-IoT

Cluster, European Commission, Tech. Rep., 2010.

[4] â€œCyber physical syst. vision statement,â€ Networking and Inform. Techol.

Res. and Develop. Program, Tech. Rep., 2015.

[5] Y. Liu, Y. Peng, B. Wang, S. Yao, and Z. Liu, â€œReview on cyber-physical
systems,â€ IEEE/CAA J Automatica Sinica, vol. 4, no. 1, pp. 27â€“40, 2017.
[6] J. Jin, J. Gubbi, S. Marusic, and M. Palaniswami, â€œAn information
framework for creating a smart city through internet of things,â€ IEEE
Internet of Things J, vol. 1, no. 2, pp. 112â€“121, 2014.
[7] â€œCloud security report,â€ Alert Logic, Tech. Rep., 2015.
[8] E. Fernandes, J. Paupore, A. Rahmati, D. Simionato, M. Conti, and
A. Prakash, â€œFlowfence: Practical data protection for emerging iot
application frameworks,â€ in 25th USENIX Security Symp,, pp. 531â€“548.
[9] P. Chen, L. Desmet, and C. Huygens, â€œA study on advanced persistent
threats,â€ in IFIP Intl. Conf. on Commun. and Multimedia Security.
Springer, 2014, pp. 63â€“72.

[10] K. D. Bowers, M. Van Dijk, R. Grifï¬n, A. Juels, A. Oprea, R. L.
Rivest, and N. Triandopoulos, â€œDefending against the unknown enemy:
Applying FlipIt to system security,â€ in Decision and Game Theory for
Security. Springer, 2012, pp. 248â€“263.

[11] K. Baumgartner and M. Golovkin, â€œThe naikon apt: Tracking down geo-
political intell. across apac one nation at a time. [Online]. Available:
https://securelist.com/analysis/publications/69953/the-naikon-apt/.â€
[12] B. Fogg and H. Tseng, â€œThe elements of computer credibility,â€ in Proc.
SIGCHI conf. on Human Factors in Computing Syst. ACM, 1999, pp.
80â€“87.

[13] Z. Yan, P. Zhang, and A. V. Vasilakos, â€œA survey on trust management
for internet of things,â€ J Net. and Comput. Applicats., vol. 42, pp. 120â€“
134, 2014.

[14] U. F. Minhas, J. Zhang, T. Tran, and R. Cohen, â€œA multifaceted approach
to modeling agent trust for effective communication in the application of
mobile ad hoc vehicle networks,â€ IEEE Trans. Syst., Man, and Cybern.,
Part C (Applications and Reviews), vol. 41, no. 3, pp. 407â€“420, May
2011.

[15] M. van Dijk, A. Juels, A. Oprea, and R. L. Rivest, â€œFlipit: The game of
â€œstealthy takeoverâ€,â€ J Cryptology, vol. 26, no. 4, pp. 655â€“713, 2013.
[16] S. Siadat, A. M. Rahmani, and H. Navid, â€œIdentifying fake feedback in
cloud trust management systems using feedback evaluation component

0246810-20-15-10-5051015200246810-20-15-10-5051015200246810-20-15-10-505101520024681012-8-6-4-20246810024681012-10-5051015024681012-10-5051015and bayesian game model,â€ J. Supercomputing, vol. 73, no. 6, pp. 2682â€“
2704, 2017.

[17] P. Zhang, Y. Kong, and M. Zhou, â€œA domain partition-based trust model
for unreliable clouds,â€ IEEE Trans. on Inform. Forensics and Security,
vol. 13, no. 9, pp. 2167â€“2178, 2018.

[18] C. Zhu, H. Nicanfar, V. C. Leung, and L. T. Yang, â€œAn authenticated trust
and reputation calculation and management system for cloud and sensor
networks integration,â€ IEEE Trans. Inform. Forensics and Security,
vol. 10, no. 1, pp. 118â€“131, 2015.

[19] W. Fan, S. Yang, and J. Pei, â€œA novel two-stage model for cloud service
trustworthiness evaluation,â€ Expert Systs., vol. 31, no. 2, pp. 136â€“153,
2014.

[20] T. H. Noor, Q. Z. Sheng, and A. Alfazi, â€œReputation attacks detection
for effective trust assessment among cloud services,â€ in IEEE Interna-
tional Conference on Trust, Security and Privacy in Computing and
Communications (TrustCom), 2013, pp. 469â€“476.

[21] I. U. Haq, I. Brandic, and E. Schikuta, â€œSla validation in layered
cloud infrastructures,â€ in International Workshop on Grid Economics
and Business Models. Springer, 2010, pp. 153â€“164.

[22] Y. Xie, L. Liu, R. Li, J. Hu, Y. Han, and X. Peng, â€œSecurity-aware signal
packing algorithm for can-based automotive cyber-physical systems,â€
IEEE/CAA J. Automatica Sinica, vol. 2, no. 4, pp. 422â€“430, 2015.
[23] Y. Mo, T. H.-J. Kim, K. Brancik, D. Dickinson, H. Lee, A. Perrig, and
B. Sinopoli, â€œCyberâ€“physical security of a smart grid infrastructure,â€
Proceedings of the IEEE, vol. 100, no. 1, pp. 195â€“209, 2012.

[24] J. Chen and Q. Zhu, â€œInterdependent strategic cyber defense and robust
switching control design for wind energy systems,â€ in IEEE Power &
Energy Society General Meeting, 2017, pp. 1â€“5.

[25] J. Pawlick, S. Farhang, and Q. Zhu, â€œFlip the cloud: cyber-physical
signaling games in the presence of advanced persistent
threats,â€ in
International Conference on Decision and Game Theory for Security,
2015, pp. 289â€“308.

[26] M. H. Manshaei, Q. Zhu, T. Alpcan, T. BacsÂ¸ar, and J.-P. Hubaux, â€œGame
theory meets network security and privacy,â€ ACM Computing Surveys
(CSUR), vol. 45, no. 3, p. 25, 2013.

[27] Q. Zhu and T. Basar, â€œGame-theoretic methods for robustness, security,
and resilience of cyberphysical control systems: games-in-games prin-
ciple for optimal cross-layer resilient control systems,â€ IEEE Control
Syst., vol. 35, no. 1, pp. 46â€“65, 2015.

[28] T. Alpcan and T. Basar, â€œA game theoretic approach to decision and
analysis in network intrusion detection,â€ in IEEE Conf. Decision and
Control, vol. 3, 2003, pp. 2595â€“2600.

[29] J. Pawlick and Q. Zhu, â€œDeception by Design: Evidence-Based
Signaling Games for Network Defense,â€ Delft, The Netherlands, 2015.
[Online]. Available: http://arxiv.org/abs/1503.05458

[30] J. Chen and Q. Zhu, â€œOptimal contract design under asymmetric infor-
mation for cloud-enabled internet of controlled things,â€ in International
Conference on Decision and Game Theory for Security. Springer, 2016,
pp. 329â€“348.

[31] A. Laszka, G. Horvath, M. Felegyhazi, and L. ButtyÂ´an, â€œFlipThem:
Modeling targeted attacks with ï¬‚ipit for multiple resources,â€ in Decision
and Game Theory for Security. Springer, 2014, pp. 175â€“194.

[32] J. Chen and Q. Zhu, â€œSecurity as a service for cloud-enabled internet
of controlled things under advanced persistent threats: a contract design
approach,â€ IEEE Trans. Inform. Forensics and Security, vol. 12, no. 11,
pp. 2736â€“2750, 2017.

[33] Y. Sun, H. Song, A. J. Jara, and R. Bie, â€œInternet of things and big data
analytics for smart and connected communities,â€ IEEE Access, vol. 4,
pp. 766â€“773, 2016.

[34] A. Cenedese, A. Zanella, L. Vangelista, and M. Zorzi, â€œPadova smart
city: An urban internet of things experimentation,â€ in IEEE 15th Int.
Symp. on a World of Wireless, Mobile and Multimedia Nets. (WoWMoM).
IEEE, 2014, pp. 1â€“6.

seven Iranians

Justice, â€œManhattan U.S. attorney announces charges
[35] U. D. of
for conducting coordinated campaign of
against
Islamic
cyber attacks against U.S. ï¬nancial sector on behalf of
Revolutionary Guard Corps-sponsored entities.
[online]. available:
https://www.justice.gov/usao-sdny/pr/manhattan-us-attorney-announces-
charges-against-seven-iranians-conducting-coordinated.â€

[36] J. Pawlick and Q. Zhu, â€œQuantitative models of imperfect deception in
network security using signaling games with evidence,â€ IEEE Commun.
and Net. Security, 2017.

[37] J. Pawlick, E. Colbert, and Q. Zhu, â€œModeling and analysis of leaky
deception using signaling games with evidence,â€ in Workshop on the
Econ. of Inform. Security, Seattle, U.S.A., 2018.

[38] D. Fudenberg and J. Tirole, â€œGame theory,â€ Cambridge, Massachusetts,

vol. 393, 1991.

[39] G. F. Franklin, J. D. Powell, and M. L. Workman, Digital control of
dynamic systems. Addison-wesley Menlo Park, CA, 1998, vol. 3.
[40] J. Pawlick and Q. Zhu, â€œStrategic trust in cloud-enabled cyber-physical
systems with an application to glucose control,â€ IEEE Trans. Inform.
Forensics and Security, vol. 12, no. 12, pp. 2906â€“2919, 2017.

[41] I.-K. Cho and D. M. Kreps, â€œSignaling games and stable equilibria,â€

Quarterly J of Econ., pp. 179â€“221, 1987.

[42] S. Kakutani, â€œA generalization of Brouwerâ€™s ï¬xed point theorem,â€ Duke

Math. J, vol. 8, no. 3, pp. 457â€“459, 1941.

[43] N. Cordeschi, D. Amendola, M. Shojafar, and E. Baccarelli, â€œDistributed
and adaptive resource management in cloud-assisted cognitive radio
vehicular networks with hard reliability guarantees,â€ Vehicular Com-
munications, vol. 2, no. 1, pp. 1â€“12, 2015.

[44] E. Guizzo, â€œHow Googleâ€™s self-driving car works,â€ IEEE Spectrum

Online, October, vol. 18, 2011.

[45] O. Levander, â€œForget autonomous carsâ€”autonomous ships are almost

here,â€ IEEE Spectrum, February 2017.

[46] C. Zhang and J. M. Kovacs, â€œThe application of small unmanned
aerial systems for precision agriculture: a review,â€ Precision Agriculture,
vol. 13, no. 6, pp. 693â€“712, 2012.

[47] D. Wakabayashi, â€œUberâ€™s self-driving cars were struggling before ari-

zona crash,â€ The New York Times, March 2018.

[48] S. Shane, â€œSleeping sailors on U.S.S. Fitzgerald awoke to a calamity at

sea,â€ The New York Times, June 2017.

[49] Z. Xu and Q. Zhu, â€œSecure and resilient control design for cloud enabled
networked control systems,â€ in Proc. ACM Workshop on Cyber-Physical
Systems-Security and/or PrivaCy. ACM, 2015, pp. 31â€“42.

[50] K. J. AstrÂ¨om and R. M. Murray, Feedback Syst.: an Introduction for

Scientists and Engineers. Princeton U Press, 2010.

[51] MATLAB, R2017b. Natick, Massachusetts: The MathWorks Inc., 2017.

Jeffrey Pawlick received the B.S. degree from Rens-
selaer Polytechnic Institute in Troy, NY in 2013, and
the Ph.D. degree from New York University (NYU)
in 2018, both in Electrical Engineering. At NYU
he studied in the Laboratory for Agile and Resilient
Complex Systems within the Tandon School of Engi-
neering. His research interests include game theory,
privacy, security, trust management, and deception
in cyber-physical systems and the Internet of things.

Juntao Chen (Sâ€™15) received the B.Eng. degree in
Electrical Engineering and Automation from Central
South University, Changsha, China, in 2014. He is
currently pursuing the Ph.D. degree in the Labora-
tory for Agile and Resilient Complex Systems, Tan-
don School of Engineering, New York University,
NY, USA. His research interests include mechanism
design, game theory, and cyber-physical systems
security.

Quanyan Zhu (Mâ€™14) received the Ph.D. degree
from the University of Illinois at Urbana-Champaign
(UIUC) in 2013. After a short stint at Princeton
University, he joined the Department of Electrical
and Computer Engineering at New York Univer-
sity (NYU) as an assistant professor in 2014. He
spearheaded and chaired INFOCOM Workshop on
Communications and Control on Smart Energy Sys-
tems (CCSES), Midwest Workshop on Control and
Game Theory (WCGT), and 7th Game and Decision
Theory for Cyber Security (GameSec). His research
interests include game theory, smart grid, network security and privacy,
resilient critical infrastructures, cyber-physical systems and cyber deception.

