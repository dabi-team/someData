0
2
0
2

n
u
J

2

]

R
C
.
s
c
[

1
v
9
4
4
1
0
.
6
0
0
2
:
v
i
X
r
a

Less is More: Robust and Novel Features for
Malicious Domain Detection

Chen Hajaj1, Nitay Hason2, Nissim Harel3, and Amit Dvir4

1 Ariel Cyber Innovation Center, Data Science and Artiﬁcial Intelligence Research
Center, IEM Department, Ariel University, Israel
chenha@ariel.ac.il
https://www.ariel.ac.il/wp/chen-hajaj/
2 Ariel Cyber Innovation Center, CS Department, Ariel University, Israel
nitay.has@gmail.com
3 CS Department, Holon Institute of Technology, Israel
nissimh@hit.ac.il
4 Ariel Cyber Innovation Center, CS Department, Ariel University, Israel
amitdv@ariel.ac.il
https://www.ariel.ac.il/wp/amitd/

Abstract. Malicious domains are increasingly common and pose a se-
vere cybersecurity threat. Speciﬁcally, many types of current cyber at-
tacks use URLs for attack communications (e.g., C&C, phishing, and
spear-phishing). Despite the continuous progress in detecting these at-
tacks, many alarming problems remain open, such as the weak spots of
the defense mechanisms. Since machine learning has become one of the
most prominent methods of malware detection, A robust feature selec-
tion mechanism is proposed that results in malicious domain detection
models that are resistant to evasion attacks. This mechanism exhibits
high performance based on empirical data. This paper makes two main
contributions: First, it provides an analysis of robust feature selection
based on widely used features in the literature. Note that even though
the feature set dimensional space is reduced by half (from nine to four
features), the performance of the classiﬁer is still improved (an increase
in the model’s F1-score from 92.92% to 95.81%). Second, it introduces
novel features that are robust to the adversary’s manipulation. Based
on extensive evaluation of the diﬀerent feature sets and commonly used
classiﬁcation models this paper show that models which are based on
robust features are resistant to malicious perturbations, and at the same
time useful for classifying non-manipulated data.

Keywords: URL, Malicious, ML

1

Introduction

In the past two decades, cybersecurity attacks have become a major issue for
governments and civilians [57]. Many of these attacks are based on malicious web
domains or URLs (See Figure 1 for the structure of a URL). These domains are

 
 
 
 
 
 
used for phishing [16,29,31,47,55] (e.g. spear phishing), Command and Control
(C&C) [53] and a vast set of virus and malware [19] attacks.

Fig. 1: The URL structure

The Domain Name System (DNS) maps human-readable domain names to
their associated IP addresses (e.g., google.com to 172.217.16.174). However, DNS
services are abused in diﬀerent ways in order to conduct various attacks. In such
attacks, the adversary can utilize a set of domains and IP addresses to orchestrate
sophisticated attacks [56,12]. Therefore, the ability to identify a malicious do-
main in advance is a massive game-changer [11,12,14,15,18,20,22,25,26,27,37,45,49,56,60,61,64].

A common way of identifying malicious/compromised domains is to collect
information about the domain names (alphanumeric characters) and network in-
formation (such as DNS and passive DNS data1). This information is then used
for extracting a set of features, according to which machine learning (ML) algo-
rithms, are trained based on a desirably massive amount of data [12,13,14,15,18,20,25,26,27,37,45,46,49,60,64].

A mathematical approach can also be used in a variety of ways [22,64], such
as measuring the distance between a known malicious domain name and the
analyzed domain (benign or malicious) [64]. Still, while ML-based solutions are
widely used, many of them are not robust; an attacker can easily bypass these
models with minimal feature perturbations (e.g., change the length of the domain
or modify network parameters such as Time To Live, TTL) [43,3].

In this context, one of the main questions is how to identify malicious/compromised

domains in the presence of an intelligent adversary that can manipulate domain
properties.

For these reasons, a feature selection mechanism which is robust to adversar-
ial manipulations is used to tackle the problem of identifying malicious domains.
Thus, even if the attacker has a black-box access to the model, tampering with
the domain properties or network parameters will have a negligible eﬀect on the
model’s accuracy. In order to achieve this goal, a broad set of both malicious and
benign URLs were collected and surveyed for commonly used features. These fea-
tures were then manipulated to show that some of them, although widely used,
are only slightly robust or not robust at all. Thus, novel features were engineered
to enhance the robustness of the models. While these novel features support the
detection of malicious domains from new angles (e.g. autonomous system num-
bers), the resulting accuracy of models that were solely based on these features
is not necessarily higher than the original features. Therefore, a hybrid set of
features was generated, combining a subset of the well-known features, with the

1 Most works dealing with malicious domain detection are based on DNS features, and

only some take the passive DNS features into account as well.

novel features. Finally, the diﬀerent sets of features were evaluated using well-
known machine learning and deep learning algorithms, which led to a robust and
eﬃcient malicious domain detection system.

The rest of the paper is organized as follows: Section 2 presents the evaluation
metric used, and Section 3 summarizes related works. Section 4 describes the
methodology and the novel features. Section 5 presents the empirical analysis
and evaluation. Finally, Section 6 concludes and summarizes this work.

2 Evaluation Metrics

Machine Learning (ML) is a subﬁeld of Computer Science aimed at getting
computers to act and improve over time autonomously by feeding them data in
the form of observations and real-world interactions. In contrast to traditional
programming, when one provides input and algorithm and receives an output
when using ML, one provides a list of inputs and their associated outputs, in
order to extract the algorithm that maps the two.

ML algorithms are often categorized as either supervised or unsupervised. In
supervised learning, each example is a pair consisting of an input vector (also
called data point) and the desired output value (class/label). Unsupervised learn-
ing learns from data that has not been labeled, classiﬁed, or categorized. Instead
of responding to feedback, unsupervised learning identiﬁes commonalities in the
data and reacts based on the presence or absence of such commonalities in each
new piece of data.

In order to evaluate how a supervised model is adapted to a problem, the
dataset needs to be split into two, the training set and the testing set. The
training set is used to train the model, and the testing set is used to evaluate
how well the model "learned" (i.e. by comparing the model predictions with the
known labels). Usually, the train/test distribution is around 75%/25% (depend-
ing on the problem and the amount of data). Standard evaluation criteria are as
follows: Recall, Precision, Accuracy, F1-score, and Loss. All of these criteria can
easily be extracted from the evaluation’s confusion matrix.

Confusion matrix (Table 1) is commonly used to describe the performance
of a classiﬁcation model. WLOG, we deﬁne positive instances as malicious and
negative as benign. Recall (Eq. 1) is deﬁned as the number of correctly classiﬁed
malicious examples out of all the malicious ones. Similarly, Precision (Eq. 2) is
the number of correctly classiﬁed malicious examples out of all examples clas-
siﬁed as malicious (both correctly and wrongly classiﬁed). Accuracy (Eq. 3) is
used as a statistical measure of how well a classiﬁcation test correctly identiﬁes
or excludes a condition. That is, the accuracy is the proportion of true results
(both true positives and true negatives) among the total number of cases exam-
ined. Finally, F1-score (Eq. 4) is a measure of a test’s accuracy. It considers both
the precision and the recall of the test to compute the score. The F1-score is the
harmonic average of the precision and recall, where an F1-score reaches its best
value at 1 (perfect precision and recall) and worst at 0. These criteria are used

as the main evaluation metric. Since a classiﬁcation model is being examined
here, the Logarithmic loss (Eq. 5) was chosen as the loss function.

In this research, the problem of identifying malicious web domains can be
classiﬁed as supervised learning, as the correct label (i.e. malicious or benign)
can be extracted using a blacklist-based method, as we describe in the next
chapter.

Prediction Outcome
Positive
True
Positive
False
Positive
P

Negative
False
Negative
True
Negative
N

Total

TP+FN

FP+TN

Actual
Value

Positive

Negative

Total

Table 1: Confusion Matrix

Recall =

T P
T P + F N

P recision =

T P
T P + F P

=

T P
P

Accuracy =

T P + T N
T P + F P + T N + F N

=

T
P + N

F1 − score = 2 ·

P recision · Recall
P recision + Recall

Loss = −

1
N

N
(cid:88)

(yi · log(pi) + (1 − yi) · log(1 − pi))

i=1

3 Related Work

(1)

(2)

(3)

(4)

(5)

The issue of identifying malicious domains is a fundamental problem in cyber-
security. This section discusses recent results in identifying malicious domains,
focusing on three signiﬁcant methodologies: Mathematical Theory approaches,
ML-based techniques, and Big Data approaches.

The use of graph theory to identify malicious domains was more pervasive in
the past [22,28,35,41,64]. Yadav et al. [64] presented a method for recognizing
malicious domain names based on fast ﬂux. Fast ﬂux is a DNS technique used
by botnets to hide phishing and malware delivery sites behind an ever-changing
network of compromised hosts acting as proxies. Their methodology analyzed
the DNS queries and responses to detect if and when domain names are being
generated by a Domain Generation Algorithm (DGA). Their solution was based
on computing the distribution of alphanumeric characters for groups of domains
and by statistical metrics with the KL (Kullback Leibler) distance, Edit distance
and Jaccard measure to identify these domains. Their results for a fast-ﬂux
attack using the Jaccard Index achieved impressive results, with 100% detection
and 0% false positives. However, for smaller numbers of generated domains for
each TLD, their false positive results were much higher, at 15% when 50 domains
were generated for the TLD using the KL-divergence over unigrams, and 8%
when 200 domains were generated for each TLD using Edit distance.

Dolberg et al. [22] described a system called Multi-dimensional Aggregation
Monitoring (MAM) that detects anomalies in DNS data by measuring and com-
paring a âĂĲsteadinessâĂİ metric over time for domain names and IP addresses
using a tree-based mechanism. The steadiness metric is based on a similar do-
main to IP resolution patterns when comparing DNS data over a sequence of
consecutive time frames. The domain name to IP mappings were based on an
aggregation scheme and measured steadiness. In terms of detecting malicious
domains, the results showed that an average steadiness value of 0.45 could be
used as a reasonable threshold value, with a 73% true positive rate and only 0.3%
false positives. The steadiness values might not be considered a good indicator
when fewer malicious activities were present (e.g. <10%).

However, the most common approach to identifying malicious domains is us-
ing machine learning (ML) [12,13,18,21,37,46,52,56,60]. Using a set of extracted
features, researchers can train ML algorithms to label URLs as malicious or be-
nign. Shi et al. [56] proposed a machine learning methodology to detect malicious
domain names using the Extreme Learning Machine (ELM) [27] which is closest
to the one employed here. ELM is a new neural network with high accuracy
and fast learning speed. The authors divided their features into four categories:
construction-based, IP-based, TTL-based, and WHOIS-based. Their evaluation
resulted in a high detection rate, an accuracy exceeding 95%, and a fast learning
speed. However, as shown below, a signiﬁcant fraction of the features used in
this work emerged as non-robust and ineﬀective in the presence of an intelligent
adversary.

Sun et al. [60] presented a system called HinDom, that generate a heteroge-
neous graph (in contrast to homogeneous graphs created by [64,49]) in order to
robustly identify malicious attacks (e.g. spams, phishing, malware and botnets).
Even though HinDom collected DNS and pDNS data, it also has the ability
to collect information from various clients inside networks (e.g. CERNET2 and
TUNET) and by that the perspective of it is diﬀerent from the perspective of
this study (i.e. client perspective). Nevertheless, HinDom has achieved remark-

able results using transductive classiﬁer it managed to achieve high accuracy
and F1-score with 99% and 97.5% respectively.

Bilge et al. [15] created a system called Exposure, designed to detect malicious
domain names. Their system uses passive DNS data collected over some period
of time to extract features related to known malicious and benign domains. Pas-
sive DNS Replication[61,12,13,15,46,37,49] refers to the reconstruction of DNS
zone data by recording and aggregating live DNS queries and responses. Passive
DNS data could be collected without requiring the cooperation of zone adminis-
trators. The Exposure is designed to detect malware- and spam-related domains.
It can also detect malicious fast-ﬂux and DGA-related domains based on their
unique features. The system computes the following four sets of features from
anonymized DNS records: (a) Time-based features related to the periods and
frequencies that a speciﬁc domain name was queried in; (b) DNS-answer-based
features calculated based on the number of distinctive resolved IP addresses and
domain names, the countries that the IP addresses reside in, and the ratio of
the resolved IP addresses that can be matched with valid domain names and
other services; (c) TTL-based features that are calculated based on statistical
analysis of the TTL over a given time series; (d) Domain name-based features
are extracted by computing the ratio of the numerical characters to the domain
name string, and the ratio of the size of the longest meaningful substring in
the domain name. Using a Decision Tree model, Exposure reported a total of
100,261 distinct domains as being malicious, which resolved to 19,742 unique
IP addresses. The combination of features that were used to identify malicious
domains led to the successful identiﬁcation of several domains that were related
to botnets, ﬂux networks, and DGAs, with low false positive and high detection
rates. It may not be possible to generalize the detection rate results reported
by the authors (98%) since they were highly dependent on comparisons with
biased datasets. Despite the positive results, once an identiﬁcation scheme is
published, it is always possible for an attacker to evade detection by mimicking
the behaviors of benign domains.

Rahbarinia et al. [49] presented a system called Segugio, an anomaly detec-
tion system based on passive DNS traﬃc to identify malware-controlled domain
names based on their relationship to known malicious domains. The system de-
tects malware-controlled domains by creating a machine domain bipartite graph
that represents the underlying relations between new domains and known be-
nign/malicious domains. The system operates by calculating the following fea-
tures: (a) Machine Behavior, based on the ratio of âĂĲknown maliciousâĂİ and
âĂĲunknownâĂİ domains that query a given domain d over the total number of
machines that query d. The larger the total number of queries and the fraction
of malicious related queries, the higher the probability that d is a malware con-
trolled domain; (b) Domain Activity, where given a time period, domain activity
is computed by counting the total number of days in which a domain was actively
queried; (c) IP Abuse, where given a set of IP addresses that the domain resolves
to, this feature represents the fraction of those IP addresses that were previously
targeted by known malware controlled domains. Using a Random Forest model,

Segugio was shown to produce high true positive and very low false positive rates
(94% and 0.1% respectively). It was also able to detect malicious domains earlier
than commercial blacklisting websites. However, Segugio is a system that can
only detect malware related domains based on their relationship to previously
known domains and therefore cannot detect new (unrelated to previous mali-
cious domains) malicious domains. More information about malicious domain
ﬁltering and malicious URL detection can be found in [21,52].

Adversarial Machine Learning is a subﬁeld of Machine Learning in which the
training and testing set do not share the same distribution, for example, given
perturbations on a malicious instance so that it will be falsely classiﬁed. These
manipulated instances are commonly called adversarial examples (AE)[24]. AE
are samples an attacker changes, based on some knowledge of the model classiﬁ-
cation function. These examples are slightly diﬀerent from correctly classiﬁed ex-
amples. Therefore, the model fails to classify them correctly. AE are widely used
in the ﬁelds of spam ﬁltering[38], network intrusion detection systems (IDS)[23],
Anti-Virus signature tests[39] and bio-metric recognition[51].

Attackers commonly follow one of two models to generate adversarial exam-
ples: 1) white-box attacker[34,48,59], which has full knowledge of the classiﬁer
and the train/test data; 2) black-box attacker[34,42,54], which has access to the
model’s output for each given input.

Various methods emerged to tackle AE-based attacks and make ML models
robust. The most promising are those based on game-theoretic approaches[17,58,65],
robust optimization[34,48,63], and adversarial retraining[32,40,3]. These approaches
mainly concern feature-space models of attacks where feature space models as-
sume that the attacker changes values of features directly. Note that these attacks
may be an abstraction of reality as random modiﬁcations to feature values may
not be realizable or avoid the manipulated instance functionality.

Big Data is an evolving term that describes any voluminous amount of struc-
tured, semi-structured and unstructured data that can be mined for information.
Big data is often characterized by 3Vs: the extreme Volume of data, the wide
Variety of data types and the Velocity at which the data must be processed.
To implement Big Data, high volumes of low-density, unstructured data need to
be processed. This can be data of unknown value, such as Twitter data feeds,
click streams on a web page or a mobile app, or sensor enabled equipment. For
some organizations, this might be tens of terabytes of data. For others, it may
be hundreds of petabytes. Velocity is the fast rate at which data are received
and (perhaps) acted on. Normally, the highest velocity of data streams directly
into memory rather than being written to disk.

Torabi et al. [61] surveyed state of the art systems that utilize passive DNS
traﬃc for the purpose of detecting malicious behaviors on the Internet. They
highlighted the main strengths and weaknesses of these systems in an in depth
analysis of the detection approach, collected data, and detection outcomes. They
showed that almost all systems have implemented supervised machine learning.
In addition, while all these systems require several hours or even days before
detecting threats, they can achieve enhanced performance by implementing a

system prototype that utilizes big data analytic frameworks to detect threats
in near real-time. This overview contributed in four ways to the literature. (1)
They surveyed implemented systems that used passive DNS analysis to detect
DNS abuse/misuse; (2) they performed an in-depth analysis of the systems and
highlighted their strengths and limitations; (3) they implemented a system pro-
totype for near real-time threat detection using a big data analytic framework
and passive DNS traﬃc; (4) they presented real-life cases of DNS misuse/abuse
to demonstrate the feasibility of a near real time threat detection system pro-
totype. However, the cases that were presented were too speciﬁc. In order to
understand the real abilities of their system, the system must be analyzed with
a much larger test dataset.

4 Methodology

The following criteria are used: Section 4.1 outlines the characteristics and meth-
ods of collection of the dataset. Section 4.2, deﬁnes each of the well known
features evaluated. Section 4.3 covers the evaluation of their robustness, and
Section 4.4 presents novel features and evaluates their robustness.

4.1 Data Collection

The main ingredient of ML models is the data on which the models are trained.
As discussed above, data collection should be as heterogeneous as possible to
model reality. The data collected for this work include both malicious and be-
nign URLs: the benign URLs are based on the Alexa top 1 million [1], and the
malicious domains were crawled from multiple sources [4,7,5] due to the fact
they are quite rare.

According to [10], 25% of all URLs in 2017 were malicious, suspicious or
moderately risky. Therefore, to make a realistic dataset, all the evaluations in-
clude all 1,356 malicious active unique URLs, and consequently 5,345 benign
active unique URLs as well. For each instance, the URL and domain informa-
tion properties were crawled from Whois, and DNS records. Whois is a widely
used Internet record listing that identiﬁes who owns a domain, how to get in
contact with them, the creation date, update dates, and expiration date of the
domain. Whois records have proven to be extremely useful and have developed
into an essential resource for maintaining the integrity of the domain name reg-
istration and website ownership. Note that according to a study by ICANN 2 [6],
many malicious attackers abuse the Whois system. Hence, only the information
that could not be manipulated was used.

Finally, based on these resources (Whois and DNS records), the following fea-
tures were generated: the length of the domain, the number of consecutive char-
acters, and the entropy of the domain from the URLs’ datasets. Next, the lifetime
of the domain and the active time of domain were calculated from the Whois

2 Internet Corporation for Assigned Names and Numbers

data. Based on the DNS response dataset (a total of 263,223 DNS records), the
number of IP addresses, distinct geolocations of the IP addresses, average Time
to Live (TTL) value, and the Standard deviation of the TTL were extracted. For
extracting the novel features (Section 4.4) Virus Total (VT) [9] and Urlscan [8]
were used, where Urlscan was used to extract parameters such as the IP address
of the page element of the URL.

4.2 Feature Engineering

Based on previous works surveyed, a set of features which are commonly used
for malicious domain classiﬁcation [12,13,15,46,49,50,52,56,62] were extracted.
Speciﬁcally,the following nine features were used as the baseline:

– Length of domain:

Length of domain = length(Domain(i))

(6)

The length of domain is calculated by the domain name followed by the TLD
(gTLD or ccTLD). Hence, the minimum length of a domain is four since the
domain name needs to be at least one character (most domain names have
at least three characters) and the TLD (gTLD or ccTLD) is composed of at
least three characters (including the dot character) as well. For example, for
the URL http://www.ariel-cyber.co.il, the length of the domain is 17 (the
number of characters for the domain name - "ariel-cyber.co.il").

– Number of consecutive characters:

Number of consecutive characters =

max{consecutive repeated characters in Domain(i)}

(7)

The maximum number of consecutive repeated characters in the domain.
This includes the domain name and the TLD (gTLD or ccTLD). For exam-
ple for the domain "aabbbcccc.com" the maximum number of consecutive
repeated characters value is 4.

– Entropy of the domain:

Entropy of the domain =

−

ni(cid:88)

j=1

count(ci
j)
length(Domain(i))

· log

count(ci
j)
length(Domain(i))

(8)

The calculation of the entropy (i.e. Feature 3) for a given domain Domain(i)
1, ci
consists of ni distinct characters {ci
}. For example, for the do-
main "google.com" the entropy is
−(5 · ( 1
10 ) + 3(· 3
10 · log 2
The domain has 5 characters that appear once (’l’, ’e’, ’.’, ’c’, ’m’), one char-
acter that appears twice (’g’) and one character that appears three times
(’o’).

10 ) + 2 · ( 2

10 )) = 1.25

10 · log 1

10 · log 3

2, . . . , ci
ni

– Number of IP addresses:

Number of IP addresses = (cid:107)distinct IP addresses(cid:107)

(9)

The number of distinct IP addresses in the domain’s DNS record. For ex-
ample for the list ["1.1.1.1", "1.1.1.1","2.2.2.2"] the number of distinct IP
addresses is 2.

– Distinct Geo-locations of the IP addresses:

Distinct Geo-locations of the IP addresses =

For each IP address in the DNS record, the countries for each IP were listed
and the number of countries was counted. For example for the list of IP
addresses ["1.1.1.1", "1.1.1.1","2.2.2.2"] the list of countries is ["Australia",
"Australia", "France"] and the number of distinct countries is 2.

(cid:107)distinct countries(cid:107)

(10)

– Mean TTL value:

Mean TTL value =

µ{TTL in DNS records of Domain(i)}

(11)

For all the DNS records of the domain in the DNS dataset, the TTL val-
ues were averaged. For example, if 30 checks of some domain’s DNS records
were conducted, and in 20 of these checks the TTL value was "60" and in
10 checks the TTL value was "1200", the mean is 20·60+10·1200

= 440.

30

– Standard deviation of the TTL:

Standard deviation of TTL =

σ{TTL in DNS records of Domain(i)}

(12)

For all the DNS records of the domain in the DNS dataset, the standard
deviation of the TTL values were calculated. For the ”Mean TTL value” ex-
ample above, the standard deviation of the TTL values is 537.401.

– Lifetime of domain:

Lifetime of domain =

DateExpiration − DateCreated

(13)

The interval between a domain’s expiration date and creation date in years.
For example for the domain "ariel-cyber.co.il", according to Whois informa-
tion, the dates are: Created on 2015-05-14, Expires in 2020-05-14, Updated
on 2015-05-14. Therefore the lifetime of the domain is the number of years
from 2015-05-14 to 2020-05-14; i.e. 5.

– Active time of domain:

Active time of domain =

DateU pdated − DateCreated

(14)

Similar to the lifetime of a domain, the active time of a domain is calculated
as the interval between a domain’s update date and creation date in years.
Using the same example as for the ”Lifetime of domain”, the active time
of the domain "ariel-cyber.co.il" is the number of years from 2015-05-14 to
2018-06-04; i.e., 3.

4.3 Robust Feature Selection

Next the robustness of the set of features described above was evaluated to ﬁlter
those that could signiﬁcantly harm the classiﬁcation process given basic manipu-
lations. In the following analysis, the robustness of these features’ is analysed (i.e.
the complexity of manipulating the feature’s values to result in a false classiﬁca-
tion). Table 2 lists the common features along with the mean value and standard
deviation for malicious and benign URLs based on the dataset. Strikingly, the
table shows that some of the features have similar mean values for both benign
and malicious instances. For example, whereas “Distinct geolocations of the IP
addresses” is quite similar for both types of instances (i.e. not eﬀective in mali-
cious domain classiﬁcation), it is widely used [56,15,13]. Furthermore, whereas
“Standard deviation of the TTL” has distinct values for benign and malicious
domains, it is shown that an intelligent adversary can easily manipulate this
feature, leading to a benign classiﬁcation of malicious domains.

Feature

Benign mean (std) Malicious mean (std)

Length of domain
Number of consecutive characters
Entropy of the domain
Number of IP addresses
Distinct geolocations of the IP addresses
Mean TTL value
Standard deviation of the TTL
Lifetime of domain
Active time of domain

14.38 (4.06)
1.29 (0.46)
4.85 (1.18)
2.09 (1.25)
1.00 (0.17)
7,578.13 (17,781.47)
2,971.65 (8,777.26)
10.98 (7.46)
8.40 (6.79)

15.54 (4.09)
1.46 (0.5)
5.16 (1.34)
1.94 (0.94)
1.02 (0.31)
8,039.92 (15,466.29)
2,531.38 (7,456.62)
6.75 (5.77)
4.64 (5.66)

Table 2: Classic features - statistical properties

In order to understand the malicious abilities of an adversary, the base fea-
tures were manipulated over a wide range of possible values, one feature at a

time.3 For each feature only the range of possible values were taken into ac-
count. This analysis considers an intelligent adversary with black-box access to
the model (i.e. a set of features or output for a given input). The robustness anal-
ysis is based on an ANN model that classiﬁes the manipulated samples, where
the train set is the empirically crawled data, and the test set includes the manip-
ulated malicious samples. Figure 2 depicts the possible adversary manipulations
over any of the features. The evaluation metric, the prediction percentage, was
deﬁned as the average detection rate after modiﬁcation.

Fig. 2: Base feature manipulation graphs (* robust or semi-robust features)

The well-known features were divided into three groups: robust features,
robust features that seemed non-robust (deﬁned as semi-robust), and non-robust
features. Next, it can be seen how an attacker can manipulate the classiﬁer for
each feature and deﬁne its robustness:

1. ”Length of domain”: an adversary can easily purchase a short or long
domain to result in a benign classiﬁcation for a malicious domain; hence this
feature was classiﬁed as non-robust.

2. ”Number of consecutive characters”: surprisingly, as depicted in Fig-
ure 2, manipulating the ”Number of consecutive characters” feature can sig-

3 Each feature was evaluated over all possible values for that feature.

niﬁcantly lower the prediction percentage (e.g., move from three consecutive
characters to one or two). Nevertheless, as depicted in Table 2, on average,
there were 1.46 consecutive characters in malicious domains. Therefore, ma-
nipulating this feature is not enough to break the model, and it is considered
to be a robust feature.

3. ”Entropy of the domain”: in order to manipulate the ”Entropy of the do-
main” feature as benign domain entropy, the adversary can create a domain
name with entropy < 4. Take, for example, the domain âĂĲddcd.ccâĂİ
which is available for purchase. The entropy for this domain is 3.54. This
value falls precisely in the entropy area of the benign domains deﬁned by
the trained model. This example breaks the model and causes a malicious
domain to look like a benign URL. Hence, this feature was classiﬁed as non-
robust.

4. ”Number of IP addresses”: note that an adversary can add many A
records to the DNS zone ﬁle of its domain to imitate a benign domain.
Thus, to manipulate the number of IP addresses, an intelligent adversary
only needs to have several diﬀerent IP addresses and add them to the zone
ﬁle. This fact classiﬁes this feature as non-robust.

5. ”Distinct Geolocations of the IP addresses”: in order to be able to
break the model with the ”Distinct Geolocations of the IP addresses” feature,
the adversary needs to use several IP addresses from diﬀerent geolocations.
If the adversary can determine how many diﬀerent countries are suﬃcient
to mimic the number of distinct countries of benign domains, he will be
able to append this number of IP addresses (a diﬀerent IP address from
each geo-location) to the DNS zone ﬁle. Thus, this feature was also classiﬁed
as non-robust. ”Mean TTL value” and ”Standard deviation of the
TTL”: there is a clear correlation between the ”Mean TTL value” and the
”Standard deviation of the TTL” features since the value manipulated by
the adversary is the TTL itself. Thus, it makes no diﬀerence if the adversary
cannot manipulate the ”Mean TTL value” feature if the model uses both.
In order to robustify the more, it is better to use the ”Mean TTL value”
feature without the ”Standard deviation of the TTL” one. Solely in terms of
the ”Mean TTL value” feature, Figure 2 shows that manipulation will not
result in a false classiﬁcation since the prediction percentage does not drop
dramatically, even when this feature is drastically manipulated. Therefore
this feature is considered to be robust.
An adversary can set the DNS TTL values to [0,120000] (according to the
RFC 2181 [2] the TTL value range is from 0 to 231 − 1). Figure 2 shows
that even manipulating the value of this feature to 60000 will break the
model and cause a malicious domain to be wrongly classiﬁed as a benign
URL. Therefore the ”Standard deviation of the TTL” cannot be considered
a robust feature.

6. ”Lifetime of domain”: As for the lifetime of domains, based on [56] we
know that a benign domain’s lifetime is typically much longer than a ma-
licious domain’s lifetime. In order to break the model by manipulating the
”Lifetime of domain” feature, the adversary must buy an old domain that is

available on the market. Even though it is possible to buy an appropriate
domain, it will take time to ﬁnd one, and it will be expensive. Hence we
considered this to be a semi-robust feature.

7. ”Active time of domain”: Similar to the previous feature, in order to break
”Active time of domain”, an adversary must ﬁnd a domain with a particular
active time (Figure 2), which is much more tricky. It is hard, expensive,
and possibly unfeasible. Therefore this was considered to be a semi-robust
feature.

Based on the analysis above, the robust features from Table 2 were selected,
and the non-robust ones were dropped. Using that subset the model was trained
and an accuracy of 95.71% with an F1-score of 88.78% was achieved can be
improved. Therefore, we extended our analysis and searched for new features
that would meet the robustness requirements to build a robust model with a
higher F1-score, ending up with a model that results in an accuracy of 99.36%
and F1-score of 98.42%.

4.4 Novel Features

This section presents four novel features which are both robust, and improve the
model’s eﬃciency.

As stated above, mimicking benign URLs in order to bypass is a mammoth
problem. The aim of the research was to validate that manipulating the features
in order to result in misclassiﬁcation of malicious instances will require a dis-
proportionate eﬀort that will deter the attacker from doing so. The four novel
features were designed according to this paradigm based on two communication
information properties, passive DNS changes, and the remaining time of SSL cer-
tiﬁcate. For each IP, Urlscan extract the geo-location, which in turn is appended
to a communication country list. Similarly, the communication ASNs is a list
of ASNs, that was extracted using Urlscan, each IP address, and appended the
ASNs list. Using the URL dataset and the Urlscan service, benign-malicious ratio
tables for communication countries and for communication ASNs (Figures 3,4)
were created. The ratio tables were calculated for each element E (country - for
the communication countries ratio table, or ASN - for the communication ASNs
ratio table). Each table represents the probability that a URL associated with a
country (ASN) is malicious. In order to extract those probabilities, the number
of malicious URLs associated with E was divided by the total URLs associated
with E. Initially, due to the heterogeneity of the dataset (i.e. there exist some
elements that appear only a few times), the ratio tables were seen to be biased.
To overcome this challenge, an initial threshold was set as an insertion criteria
which is later detailed in Algorithm 1 (CRA).

The following is a detailed summary of the novel features:

– Communication Countries Rank (CCR):

CCR =

CRA(the communication countries list of U RL(i))

(15)

Fig. 3: Communication Countries Ratio

Fig. 4: Communication ASNs Ratio

This feature looks at the communication countries with respect to the com-
munication IPs, and uses the countries ratio table to rank a speciﬁc URL.

– Communication ASNs Rank (CAR):

CAR =

CRA(the communication ASNs list of U RL(i))

(16)

Similarly, this feature analyzes the communication ASNs with respect to the
communication IPs, and uses the ASNs ratio table to rank a speciﬁc URL.
While there is some correlation between the ASNs and the countries, the
second feature examines each AS (usually ISPs or large companies) within
each country to gain a wider perspective.

– Number of passive DNS changes:

P DN S =

count(list of passive DNS records of Domain(i))

(17)

When inspecting the passive DNS records, benign domains emerged as hav-
ing much larger DNS changes that the sensors (of the company that collects
the DNS records) could identify, unlike malicious domains (i.e. 26.4 vs. 8.01,
as reported in Table 3).
For the âĂİNumber of passive DNS changesâĂİ the number of DNS records
changes were counted, which is somewhat similar to other features described
in [61,12]. Still, these features require much more elaborated information
which is not publicly available. On the other hand, this feature can be ex-
tracted from passive DNS records obtained from VirusTotal, which are scarce
(in terms of record types).

– Remaining time of SSL certiﬁcate:

SSL = Certif icateV alid·

· (Certif icateExpiration − Certif icateU pdated)

(18)

When installing an SSL certiﬁcate, there is a validation process conducted
by a Certiﬁcate Authority (CA). Depending on the type of certiﬁcate, the
CA veriﬁes the organization’s identity before issuing the certiﬁcate. When
analyzing our data it was noted that most of the malicious domains do not
use valid SSL certiﬁcates and those that do only use one for a short period.
Therefore, this feature was engineered which represents the time the SSL
certiﬁcate remains valid.
For the âĂİRemaining time of SSL certiﬁcateâĂİ, in contrast to a binary
feature version used by [50], this feature extends the scope and represents
both the existence of an SSL certiﬁcate and the remaining time until the
SSL certiﬁcate expires.

ItemsList = communication countries list of the URL

ItemsList = ASNs list of the URL

Algorithm 1 Communication Rank
Input: URL, Threshold, Type
Output: Rank (CCR or CAR)
1: if Type = Countries then
2:
3: else
4:
5: end if
6: Rank = 0
7: for Item in ItemsList do
8:
9:
10:
11:
12:
13:
14:
15: end for

T otal_norm = N ormalize(Item)
Ratio = BenignRatio(Item)

end if
Rank+ = (log0.5(Ratio + (cid:15))/T otal_norm)

Ratio = 0.75 {Init value}
T otal_norm = 1 {Init value}
if T otalOccurrences(Item) >= T hreshold then

The CRA (Algorithm 1) gets a URL as an input and returns its country
communication rate or the ASN communication rate (based on the type in the
input of the algorithm).

For each item (i.e., country or ASN), ﬁrst the algorithm initialized the value
of the ratio variable to 0.75 (according to [10], 25% of all URLs in 2017 were
malicious, suspicious or moderately risky) and the normalized total occurrences
(Total_norm) of an item to be 1. Next, in Step 9, if the total number of oc-
currences of an item was ≥ to the threshold, the algorithm replaced the ratio
and normalized occurrences to the correct values according to the ratio tables
given in Figures 3 and 4. Finally, the algorithm sums the rank with a log base
0.5 of the ratio (+ some epsilon) and divide this value by the normalized total
occurrences.

Feature

Benign mean (std) Malicious mean

Communication Countries Rank (CCR) 31.31 (91.16)

Communication ASNs Rank (CAR)
Number of passive DNS changes
Remaining time of SSL certiﬁcate

935.59 (12,258.99)
26.40 (111.99)
1.547E7 (2.304E7)

(std)
59.40 (215.15)
12,979.38 (46,384.86)
8.01 (16.63)
4.365E6 (1.545E7)

Table 3: Novel features - statistical properties

Figure 5 depicts the prediction percentage as a function of changing the novel
features values for each feature in Table 3, Similar to the analysis in Section 4.3.
This evaluation proves that manipulating the values of the novel features does

Fig. 5: Novel robust feature manipulation graphs

not break the robust model (i.e., the prediction percentage remains steady).
While one may be concerned by the negative correlation between ”Remaining
time of SSL certiﬁcate” feature and the prediction percentage, note that the
average value for malicious domains is three times higher than the benign ones.
While theoretically the adversary can lower this value, the implications of such
action are acquiring (or use some free solution) an SSL certiﬁcate. Since there is
a validation process, this process will cause the adversary to lose its anonymity
and be identiﬁed.

5 Empirical Analysis and Evaluation

This section describes the testbed used for the evaluation of models based on the
types of features (both robust and not). General settings are provided for each
of the models (e.g. the division of the data into training and test set), as are
the parameters used to conﬁgure each of the models, followed by the eﬃciency
of each model. 4

5.1 Experimental Design

Apart from intelligently choosing the model parameters, one should verify that
the data used for the learning phase accurately represents the real-world distri-
bution of domain malware. Hence, the dataset was constructed such that 75%
were benign domains, and the remaining 25% were malicious domains (~5,000
benign URLs and ~1,350 malicious domains respectively) [10].

There are many ways to deﬁne the eﬃciency of a model. To account for
most of them, a broad set of metrics was extracted including accuracy, recall,

4 Our

code

is

publicly

available

at

https://github.com/nitayhas/

robust-malicious-url-detection

F1-score, and training time. Note that for each model, the dataset was split
into train and test sets where 75% of the data (both benign and malicious) was
randomly assigned to the train test, and the remaining domains are assigned to
the test set.

The evaluation step measured the eﬃciency of the diﬀerent models while
varying the robustness of the features included in the model. Speciﬁcally, four
diﬀerent models (i.e. Logistic Regression, SVM, ELM, and ANN) were trained
using the following feature sets:

– Base (B ) - The set of commonly used features in previous works (see Table 2

for more details).

– Base Robust (BR) - the subset of robust base features (marked with a * in

Figure 2).

– ”TCP” (TCP ) - The four novel features: Time of SSL certiﬁcate, Communi-

cation ranks (CCR and CAR) and PassiveDNS changes (see Table 3).

– Base Robust + ”TCP”(BRTCP ) - the union of BR and TCP, the robust

subset of all features.

– Base + ”TCP” (BTCP ) - the union of B and TCP.

Recall that feature sets (i.e. TCP, BRTCP, and BTCP) which are based on
the novel features (i.e. CCR and CAR) , require communication ratio tables.
Hence, to keep the models unbiased (i.e. omit the data that was used for gener-
ating the ratio tables from the learning process), it was necessary allocate some
of the data to creation of the ratio tables. Due to the variety of countries and
ASNs, it was decided to dedicate the majority of the data to this process. For the
evaluation, the dataset was split into two parts: when using the novel features
75% of the data was used to create the ratio tables and the remaining 25% of
the data was used to extract the features, train and test for the models, as can
be seen in Figure 6a. In the case of using the well-known features, 100% of the
data was used for the feature extraction as can be seen in Figure 6b. Evaluations
for the opposite split ratio (i.e. 25/75) were also conducted, this time allocat-
ing most of the data to the learning phase. The ﬁndings showed that the trend
between these two ratios was similar (as presented in Section 5.2). Both ratios
returned high results but, for most of the models, the 75/25 ratio returned a
higher F1-Score and higher Recall measures (e.g. LR, SVM, and ANN for each
feature set). For the 25/75 distribution, the F1-Score and the Recall measures
were only higher for the ELM (around 1%-2% higher). Therefore, while both
results are reported in the end of this section, WLOG, the main focus of the
discussion is on the 75/25 distribution.

5.2 Models and Parameters

Four commonly used classiﬁcation models are analyzed: Logistic Regression
(LR), Support Vector Machines (SVM), Extreme Learning Machine (ELM), and
Artiﬁcial Neural Networks (ANN). All the models were trained and evaluated
on a Dell XPS 8920 computer, Windows 10 64Bit OS with 3.60GHz Intel Core
i7-7700 CPU, 16GB of RAM, and NVIDIA GeForce GTX 1060 6GB.

(a) Data distribution while using our novel
features

(b) Data distribution while using only the
base features

Fig. 6: Data Distribution

In the following paragraphs, for each model, the hyperparameters used for
the evaluation are ﬁrst described, followed by the empirical, experimental results
(which sums up several test results using diﬀerent random train-test sets), and
a short discussion of the ﬁndings and their implications.

Logistic Regression As a baseline for the evaluation process, and before us-
ing the nonlinear models, the LR classiﬁcation model was used. The LR model
with the ﬁve feature sets was trained and the hyperparameters were tuned to
maximize the model’s performance.

Hyperparameters: Polynomial degree: 3;K-Fold Cross-Validation k=10;

Solver: L-BFGS [33].

Table 4 shows that the diﬀerent feature sets resulted in similar accuracy
rates. However, the accuracy rate measures how well the model predicts (i.e.
TP+TN) with respect to all the predictions (i.e. TP+TN+FP+FN). Thus given
the unbalanced dataset (75% of the dataset are benign and 25% are malicious
domains), ~90% accuracy is not necessarily a suﬃcient result in terms of malware
detection. For example, the TCP feature set has high accuracy but in contrast a
very poor F1-Score, due to the high Precision rate and poor Recall rate (which
represents the ratio of malicious instances detected). As the recall is low for all
features sets, this work demonstrates that accuracy rate is not a good measure in
this domain; therefore, it was decided to focus on the F1-score measure, which is
the harmonic mean of the precision and the recall measures. Next, it was decided
to use the SVM model with an RBF kernel as a nonlinear model.

Support Vector Machine (SVM) Hyperparameters: Polynomial degree:
3; K-Fold Cross-Validation k=10; γ=2; Kernel: RBF [44].

Compared to the results of the LR model (Table 4), the results of the SVM
model (Table 5) show a signiﬁcant improvement in the recall and F1-score mea-
sures; e.g. for Base, the recall and the F1-score measures were both above 90%.
One could be concerned by the fact that the model trained on the Base feature
set resulted in a higher recall (and F1-score) compare to the one trained on
the Robust Base feature set. However, it should be noted that the Robust Base

feature set is robust to adversarial manipulation and uses less than half of the
features provided in the training phase with the Base feature set. This discus-
sion also applies to the BRTCP and BTCP feature sets. Another advantage of
including the novel features is the fact that models converge much faster.

The results are based on analyzing a non-manipulated dataset. As stated
above, the Base feature set includes some non-robust features. Hence, an in-
telligent adversary can manipulate the values of these features, resulting in a
wrong classiﬁcation of malicious instances (up to the extreme of a 0% recall).
However, an intelligent adversary should invest much more eﬀort for a model
that was trained using the Robust Base or TCP features, since each of them
was speciﬁcally chosen to avoid such manipulations. In order to ﬁnd models that
were also eﬃcient on the non-manipulated dataset, the two sophisticated models
were examined in the analysis, the ELM model as presented in [56] and the ANN
model.

Feature set Accuracy Recall F1-Score

Base
Robust Base
TCP
BRTCP
BTCP

89.99% 38.82% 53.21%
88.33% 38.87% 49.42%
86.20% 8.30% 14.99%
88.82% 52.46% 65.57%
92.86% 64.14% 72.48%

Table 4: Model performance - Logistic Regression (in %)

Feature set Accuracy Recall F1-Score

Base
Robust Base
TCP
BRTCP
BTCP

96.49% 91.20% 91.36%
90.14% 56.51% 69.93%
83.10% 60.21% 54.21%
96.78% 91.37% 92.02%
97.95% 90.73% 92.83%

Table 5: Model performance - SVM (in %)

ELM Hyperparameters: One input layer, one hidden layer, and one output
layer. Activation function: ﬁrst layer - ReLU [36]; hidden layer - Sigmoid. K-Fold
Cross-Validation k=10 [56]. Overall, the ELM model resulted in high accuracy
and higher Recall rates compared to Table 4, for any feature set. When compared
to the SVM models, the Base model resulted in lower recall (though a higher
F1-score was achieved with the ELM model). On the other hand, the Robust

Base resulted in a higher recall in the ELM model compared to the SVM one.
Even though the Robust Base feature set had a low dimensional space, the three
rates (i.e. Accuracy, Recall, and F1-score) were higher than those of the Base
feature set. Moving to the sets that include the novel features increased these
metrics, while improving the robustness of the model at the same time.

Feature set Accuracy Recall F1-Score

Base
Robust Base
TCP
BRTCP
BTCP

98.17% 88.81% 92.92%
98.83% 92.24% 95.81%
98.88% 94.64% 96.84%
98.86% 95.82% 97.07%
98.19% 93.09% 95.34%

Table 6: Model performance - ELM (in %)

ANN Hyperparameters: One input layer, three hidden layers, and one out-
put layer. Activation function: ﬁrst layer - ReLU; ﬁrst hidden layer - RELU;
Second hidden layer - LeakyReLU; Third hidden layer - Sigmoid. Batch size
-150, learning rate of 0.01; Solver: Adam [30] with β1 = 0.9 and β2 = 0.999.
K-Fold Cross-Validation k=10. Similarl to the ELM results, the ANN results
show high performance on all feature sets. For the “basic” feature sets (i.e. Base
and Robust Base) the ELM models resulted in higher recall and F1-score. Still,
the main focus was in the BTCP feature set and more speciﬁcally in the BRTCP
variant, and for those feature sets, the ANN models resulted in higher recall and
F1-score.

Feature set Accuracy Recall F1-Score

Base
Robust Base
TCP
BRTCP
BTCP

97.20% 88.03% 90.23%
95.71% 83.63% 88.78%
98.03% 96.83% 95.24%
99.36% 98.77% 98.42%
99.82% 99.47% 99.56%

Table 7: Model performance - ANN (in %)

Discussion This analysis conclude with Figure 7 and Tables 8, 9 and 10 that
summarize the evaluation. In particular, Figure 7 shows the overall results and
presents the F1-scores of the feature sets for all the models.

Fig. 7: The F1-Score by feature sets and models

Feature set

(cid:97)(cid:97)(cid:97)(cid:97)(cid:97)(cid:97)(cid:97)(cid:97)(cid:97)(cid:97)(cid:97)

Model

Logistic
Regression

SVM

ELM

ANN

Base

Base Robust

Accuracy: 0.90
Precision: 0.84
Recall: 0.39
F1-score: 0.53
Loss: 3.45
AUC: 0.85

Accuracy: 0.96
Precision: 0.91
Recall: 0.91
F1-score: 0.91
Loss: 1.20
AUC: 0.96

Accuracy: 0.98
Precision: 0.98
Recall: 0.88
F1-score: 0.92
Loss: 0.63
AUC: 0.99

Accuracy: 0.97
Precision: 0.92
Recall: 0.88
F1-score: 0.90
Loss: 0.44
AUC: 0.98

Accuracy: 0.88
Precision: 0.68
Recall: 0.39
F1-score: 0.49
Loss: 4.03
AUC: 0.81

Accuracy: 0.90
Precision: 0.91
Recall: 0.56
F1-score: 0.69
Loss: 3.40
AUC: 0.92

Accuracy: 0.98
Precision: 0.99
Recall: 0.92
F1-score: 0.95
Loss: 0.40
AUC: 0.99

Accuracy: 0.95
Precision: 0.94
Recall: 0.83
F1-score: 0.88
Loss: 0.68
AUC: 0.97

Table 8: Raw data results for the models that trained with the base features
(100% of the dataset records)

Tables 8-10 depicts all the evaluation matrices (i.e. accuracy, precision, recall,
F1-score, loss, and AUC). While Table 9 depicts the models’ performance where
75% of the data was used to construct the ratio table, Table 10 depicts the
performance where the minority of data (i.e. 25%) was used to construct the
ratio tables. Looking at the resultant recall and F1-score, it seems that models
analyzed in Table 9, in which most of the data is dedicated to the ratio tables
(i.e. to the TCP features), resulted in higher performance, even though these
models had one third of the data available to the models evaluated in Table 10.
At the same time, it is important to note that the best model resulted by using
the Base Robust + TCP features, to train the ELM model and dedicating 25% of
the data to the feature engineering phase, and the reminder 75% to the learning
process.

All the results provided above are based on clean data (i.e. with no adversar-
ial manipulation). Naturally, given an adversarial model where the attacker can
manipulate the values of features, models which are based on the Robust Base or
TCP feature sets will dominate models that are trained using the Base dataset.
Thus, by showing that the Robust Base feature set does not dramatically de-
crease the performance of the classiﬁer using clean data, and that adding the
novel feature improves the model’s performance as well as its robustness, the
conclusion is that malicious domain classiﬁers should use this feature set for
robust malicious domain detection.

6 Conclusion

Numerous attempts have been made to tackle the problem of identifying ma-
licious domains. However, many of them fail to successfully classify malware
in realistic environments where an adversary can manipulate the URLs and/or
other extracted features. Speciﬁcally, this research tackled the case where an at-
tacker has access to the model (i.e. a set of features or output for a given input),
and tampers with the domain properties. This tampering has a catastrophic
eﬀect on the modelâĂŹs eﬃciency. As a countermeasure, an intelligent feature
selection procedure was used which is robust to adversarial manipulation as well
as inclusion of novel robust features. Feature robustness and model eﬀectiveness
were evaluated based on well known machine and deep learning models over a
sizeable realistic dataset.

The evaluation showed that models that are trained using the robust features
are more precise in terms of manipulated data while maintaining good results on
clean data as well. Clearly, further research is needed to create models that can
also classify malicious domains into malicious attack types. Another promising
direction would be clustering a set of malicious domains into one cyber campaign.

References

1. Alexa, https://www.alexa.com
2. Clariﬁcations to the dns speciﬁcation, https://tools.ietf.org/html/rfc2181

Model/Feature set

TCP

Base Robust + TCP Base + TCP

Logistic Regression

SVM

ELM

ANN

Accuracy: 0.86
Precision: 0.77
Recall: 0.08
F1-score: 0.15
Loss: 4.76
AUC: 0.79

Accuracy: 0.83
Precision: 0.60
Recall: 0.49
F1-score: 0.54
Loss: 5.83
AUC: 0.80

Accuracy: 0.98
Precision: 0.99
Recall: 0.94
F1-score: 0.96
Loss: 0.38
AUC: 0.99

Accuracy: 0.98
Precision: 0.93
Recall: 0.96
F1-score: 0.95
Loss: 0.31
AUC: 0.99

Accuracy: 0.88
Precision: 0.87
Recall: 0.52
F1-score: 0.65
Loss: 3.86
AUC: 0.93

Accuracy: 0.96
Precision: 0.92
Recall: 0.91
F1-score: 0.92
Loss: 1.11
AUC: 0.98

Accuracy: 0.98
Precision: 0.98
Recall: 0.95
F1-score: 0.97
Loss: 0.39
AUC: 0.99

Accuracy: 0.99
Precision: 0.98
Recall: 0.98
F1-score: 0.98
Loss: 0.10
AUC: 0.99

Accuracy: 0.92
Precision: 0.83
Recall: 0.64
F1-score: 0.72
Loss: 2.46
AUC: 0.94

Accuracy: 0.97
Precision: 0.95
Recall: 0.90
F1-score: 0.92
Loss: 0.70
AUC: 0.98

Accuracy: 0.98
Precision: 0.97
Recall: 0.93
F1-score: 0.95
Loss: 0.62
AUC: 0.98

Accuracy: 0.99
Precision: 0.99
Recall: 0.99
F1-score: 0.99
Loss: 0.02
AUC: 0.99

Table 9: Raw data results for the models that trained on the 75/25 dataset
distribution, 42,578/10,979 URLs (ratio tables/feature extraction)

Model/Feature set

TCP

Base Robust + TCP Base + TCP

Logistic Regression

SVM

ELM

ANN

Accuracy: 0.82
Precision: 0.80
Recall: 0.07
F1-score: 0.13
Loss: 5.94
AUC: 0.70

Accuracy: 0.82
Precision: 0.89
Recall: 0.07
F1-score: 0.13
Loss: 5.88
AUC: 0.76

Accuracy: 0.99
Precision: 0.99
Recall: 0.96
F1-score: 0.97
Loss: 0.25
AUC: 0.99

Accuracy: 0.94
Precision: 0.99
Recall: 0.72
F1-score: 0.83
Loss: 0.81
AUC: 0.97

Accuracy: 0.87
Precision: 0.85
Recall: 0.39
F1-score: 0.53
Loss: 4.24
AUC: 0.89

Accuracy: 0.91
Precision: 0.81
Recall: 0.65
F1-score: 0.72
Loss: 3.09
AUC: 0.94

Accuracy: 0.99
Precision: 0.99
Recall: 0.98
F1-score: 0.98
Loss: 0.16
AUC: 0.99

Accuracy: 0.98
Precision: 0.99
Recall: 0.90
F1-score: 0.94
Loss: 0.31
AUC: 0.99

Accuracy: 0.90
Precision: 0.82
Recall: 0.57
F1-score: 0.68
Loss: 3.42
AUC: 0.93

Accuracy: 0.96
Precision: 0.91
Recall: 0.87
F1-score: 0.89
Loss: 1.33
AUC: 0.97

Accuracy: 0.98
Precision: 0.97
Recall: 0.93
F1-score: 0.95
Loss: 0.54
AUC: 0.99

Accuracy: 0.98
Precision: 0.97
Recall: 0.95
F1-score: 0.96
Loss: 0.18
AUC: 0.99

Table 10: Raw data results for the models that trained on the 25/75 dataset
distribution 10,979/42,578 URLs (ratio tables/feature extraction)

3. improving robustness of ml classiﬁers against realizable evasion attacks using con-

served features

4. Phishtank, https://www.phishtank.com
5. Scumware, https://www.scumware.org
6. A study of whois privacy and proxy service abuse, https://gnso.icann.org/
sites/default/files/filefield_41831/pp-abuse-study-20sep13-en.pdf

7. Url abuse, https://urlhaus.abuse.ch
8. urlscan.io, https://www.urlscan.io
9. Virustotal, https://www.virustotal.com
10. Webroot,

https://www-cdn.webroot.com/9315/2354/6488/

2018-Webroot-Threat-Report_US-ONLINE.pdf

11. Ahmed, M., Khan, A., Saleem, O., Haris, M.: A fault tolerant approach for ma-
licious url ﬁltering. In: International Symposium on Networks, Computers and
Communications. pp. 1–6 (2018)

12. Antonakakis, M., Perdisci, R., Dagon, D., Lee, W., Feamster, N.: Building a dy-

namic reputation system for dns. In: USENIX. pp. 273–290 (2010)

13. Antonakakis, M., Perdisci, R., Lee, W., Vasiloglou, N., Dagon, D.: Detecting mal-
ware domains at the upper dns hierarchy. In: USENIX. vol. 11, pp. 1–16 (2011)
14. Berger, H., Dvir, A.Z., Geva, M.: A wrinkle in time: A case study in DNS poisoning.

CoRR (2019), http://arxiv.org/abs/1906.10928

15. Bilge, L., Sen, S., Balzarotti, D., Kirda, E., Kruegel, C.: Exposure: A passive dns
analysis service to detect and report malicious domains. Trans. Inf. Syst. Secur.
16(4), 1–28 (2014)

16. Blum, A., Wardman, B., Solorio, T., Warner, G.: Lexical feature based phishing
url detection using online learning. In: Workshop on Artiﬁcial Intelligence and
Security. pp. 54–60 (2010)

17. Brückner, M., Scheﬀer, T.: Stackelberg games for adversarial prediction problems.
In: International Conference on Knowledge Discovery and Data Mining. pp. 547–
555 (2011)

18. Caglayan, A., Toothaker, M., Drapeau, D., Burke, D., Eaton, G.: Real-time detec-
tion of fast ﬂux service networks. In: Conference For Homeland Security, Cyberse-
curity Applications & Technology. pp. 285–292 (2009)

19. Canali, D., Cova, M., Vigna, G., Kruegel, C.: Prophiler: a fast ﬁlter for the large-
scale detection of malicious web pages. In: International Conference on World Wide
Web. pp. 197–206 (2011)

20. Choi, H., Zhu, B.B., Lee, H.: Detecting malicious web links and identifying their

attack types. WebApps 11(11), 218 (2011)

21. Das, A., Data, G., Platform, A., Jain, E., Dey, S.: Machine learning features for

malicious url ﬁltering–the survey (2019)

22. Dolberg, L., François, J., Engel, T.: Eﬃcient multidimensional aggregation for large

scale monitoring. In: LISA. pp. 163–180 (2012)

23. Fogla, P., Sharif, M.I., Perdisci, R., Kolesnikov, O.M., Lee, W.: Polymorphic blend-

ing attacks. In: USENIX. pp. 241–256 (2006)

24. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial

examples. arXiv preprint:1412.6572 (2014)

25. Harel, N., Dvir, A., Dubin, R., Barkan, R., Shalala, R., Hadar, O.: Misal-a minimal
quality representation switch logic for adaptive streaming. Multimedia Tools and
Applications (2019)

26. Hu, Z., Chiong, R., Pranata, I., Susilo, W., Bao, Y.: Identifying malicious web
domains using machine learning techniques with online credibility and performance
data. In: Congress on Evolutionary Computation (CEC). pp. 5186–5194 (2016)

27. Huang, G.B., Zhu, Q.Y., Siew, C.K.: Extreme learning machine: theory and appli-

cations. Neurocomputing 70(1-3), 489–501 (2006)

28. Jung, J., Sit, E.: An empirical study of spam traﬃc and the use of dns black lists.

In: SIGCOMM Conference on Internet Measurement. pp. 370–375 (2004)

29. Khonji, M., Iraqi, Y., Jones, A.: Phishing detection: a literature survey. IEEE

Communications Surveys & Tutorials 15(4), 2091–2121 (2013)

30. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980 (2014)

31. Le, A., Markopoulou, A., Faloutsos, M.: Phishdef: Url names say it all. In: INFO-

COM. pp. 191–195 (2011)

32. Li, B., Vorobeychik, Y.: Evasion-robust classiﬁcation on binary domains. Transac-

tions on Knowledge Discovery from Data 12(4), 50 (2018)

33. Liu, D.C., Nocedal, J.: On the limited memory BFGS method for large scale opti-

mization. Mathematical Programming (1989)

34. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning

models resistant to adversarial attacks. arXiv preprint:1706.06083 (2017)

35. Mishsky, I., Gal-Oz, N., Gudes, E.: A topology based ﬂow model for computing
domain reputation. In: IFIP Annual Conference on Data and Applications Security
and Privacy. pp. 277–292 (2015)

36. Nair, V., Hinton, G.E.: Rectiﬁed linear units improve restricted boltzmann ma-
chines. In: Proceedings of the 27th International Conference on Machine Learning.
pp. 807–814 (2010)

37. Nelms, T., Perdisci, R., Ahamad, M.: Execscent: Mining for new c&c domains in
live networks with adaptive control protocol templates. In: USENIX. pp. 589–604
(2013)

38. Nelson, B., Barreno, M., Chi, F.J., Joseph, A.D., Rubinstein, B.I., Saini, U., Sutton,
C.A., Tygar, J.D., Xia, K.: Exploiting machine learning to subvert your spam ﬁlter.
LEET 8, 1–9 (2008)

39. Newsome, J., Karp, B., Song, D.: Paragraph: Thwarting signature learning by
training maliciously. In: International Workshop on Recent Advances in Intrusion
Detection. pp. 81–105 (2006)

40. Nissim, N., Moskovitch, R., BarAd, O., Rokach, L., Elovici, Y.: Aldroid: eﬃcient
update of android anti-virus software using designated active learning methods.
Knowledge and Information Systems 49(3), 795–833 (2016)

41. Othman, H., Gudes, E., Gal-Oz, N.: Advanced ﬂow models for computing the
reputation of internet domains. In: IFIP International Conference on Trust Man-
agement. pp. 119–134 (2017)

42. Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z.B., Swami, A.: Practi-
cal black-box attacks against machine learning. In: Asia Conference on Computer
and Communications Security. pp. 506–519 (2017)

43. Papernot, N., McDaniel, P., Wu, X., Jha, S.: Distillation as a defense to adversarial
perturbations against deep neural networks. In: IEEE Symposium on Security and
Privacy, (2016)

44. Park, J., Sandberg, I.W.: Universal approximation using radial-basis-function net-

works. Neural Computation 3(2), 246–257 (1991)

45. Peng, T., Harris, I., Sawa, Y.: Detecting phishing attacks using natural language
processing and machine learning. In: International Conference on Semantic Com-
puting. pp. 300–301. IEEE (2018)

46. Perdisci, R., Corona, I., Giacinto, G.: Early detection of malicious ﬂux networks
via large-scale passive dns traﬃc analysis. IEEE Transactions on Dependable and
Secure Computing 9(5), 714–726 (2012)

47. Prakash, P., Kumar, M., Kompella, R.R., Gupta, M.: Phishnet: predictive black-

listing to detect phishing attacks. In: INFOCOM. pp. 1–5 (2010)

48. Raghunathan, A., Steinhardt, J., Liang, P.: Certiﬁed defenses against adversarial

examples. arXiv preprint:1801.09344 (2018)

49. Rahbarinia, B., Perdisci, R., Antonakakis, M.: Eﬃcient and accurate behavior-
based tracking of malware-control domains in large isp networks. ACM Transac-
tions on Privacy and Security 19(2), 4 (2016)

50. Ranganayakulu, D., Chellappan, C.: Detecting malicious urls in e-mail–an imple-

mentation. AASRI 4, 125–131 (2013)

51. Rodrigues, R.N., Ling, L.L., Govindaraju, V.: Robustness of multimodal biometric
fusion methods against spoof attacks. Journal of Visual Languages & Computing
20(3), 169–179 (2009)

52. Sahoo, D., Liu, C., Hoi, S.C.: Malicious url detection using machine learning: A

survey. arXiv preprint:1701.07179 (2017)

53. Sandell, N., Varaiya, P., Athans, M., Safonov, M.: Survey of decentralized control
methods for large scale systems. IEEE Transactions on Automatic Control 23(2),
108–128 (1978)

54. Shahpasand, M., Hamey, L., Vatsalan, D., Xue, M.: Adversarial attacks on mobile
malware detection. In: International Workshop on Artiﬁcial Intelligence for Mobile.
pp. 17–20 (2019)

55. Sheng, S., Wardman, B., Warner, G., Cranor, L.F., Hong, J., Zhang, C.: An empir-
ical analysis of phishing blacklists. In: Conference on Email and Anti-Spam (2009)
56. Shi, Y., Chen, G., Li, J.: Malicious domain name detection based on extreme

machine learning. Neural Processing Letters pp. 1–11 (2017)

57. Shu, X., Tian, K., Ciambrone, A., Yao, D.: Breaking the target: An analysis of

target data breach and lessons learned. arXiv preprint:1701.04940 (2017)

58. Singh, A., Lakhotia, A.: Game-theoretic design of an information exchange model
for detecting packed malware. In: International Conference on Malicious and Un-
wanted Software. pp. 1–7 (2011)

59. Song, Y., Kim, T., Nowozin, S., Ermon, S., Kushman, N.: Pixeldefend: Leveraging
generative models to understand and defend against adversarial examples. arXiv
preprint:1710.10766 (2017)

60. Sun, X., Tong, M., Yang, J., Xinran, L., Heng, L.: Hindom: A robust malicious do-
main detection system based on heterogeneous information network with transduc-
tive classiﬁcation. In: International Symposium on Research in Attacks, Intrusions
and Defenses. pp. 399–412 (2019)

61. Torabi, S., Boukhtouta, A., Assi, C., Debbabi, M.: Detecting Internet Abuse by
Analyzing Passive DNS Traﬃc: A Survey of Implemented Systems. Communica-
tions Surveys & Tutorials (2018)

62. Xiang, G., Hong, J., Rose, C.P., Cranor, L.: Cantina+: A feature-rich machine
learning framework for detecting phishing web sites. Transactions on Information
and System Security 14(2), 21 (2011)

63. Xu, H., Caramanis, C., Mannor, S.: Robustness and regularization of support vec-

tor machines. Journal of Machine Learning Research 10, 1485–1510 (2009)

64. Yadav, S., Reddy, A.K.K., Reddy, A.L.N., Ranjan, S.: Detecting algorithmically
generated domain-ﬂux attacks with dns traﬃc analysis. Transactions on Network-
ing 20, 1663–1677 (2012)

65. Zolotukhin, M., Hämäläinen, T.: Support vector machine integrated with game-
theoretic approach and genetic algorithm for the detection and classiﬁcation of
malware. In: Globecom Workshops. pp. 211–216 (2013)

