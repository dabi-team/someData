Evading Malware Classiﬁers
via Monte Carlo Mutant Feature Discovery

John Boutsikas * 1 Maksim E. Eren * 1 Charles Varga 1 Edward Raff 2 1 Cynthia Matuszek 1 Charles Nicholas 1

Abstract

The use of Machine Learning has become a signif-
icant part of malware detection efforts due to the
inﬂux of new malware, an ever changing threat
landscape, and the ability of Machine Learning
methods to discover meaningful distinctions be-
tween malicious and benign software. Antivirus
vendors have also begun to widely utilize malware
classiﬁers based on dynamic and static malware
analysis features. Therefore, a malware author
might make evasive binary modiﬁcations against
Machine Learning models as part of the malware
development life cycle to execute an attack suc-
cessfully. This makes the studying of possible
classiﬁer evasion strategies an essential part of
cyber defense against malice. To this extent, we
stage a grey box setup to analyze a scenario where
the malware author does not know the target clas-
siﬁer algorithm, and does not have access to de-
cisions made by the classiﬁer, but knows the fea-
tures used in training. In this experiment, a ma-
licious actor trains a surrogate model using the
EMBER-2018 dataset to discover binary muta-
tions that cause an instance to be misclassiﬁed
via a Monte Carlo tree search. Then, mutated
malware is sent to the victim model that takes the
place of an antivirus API to test whether it can
evade detection.

1
2
0
2

n
u
J

5
1

]

R
C
.
s
c
[

1
v
0
6
8
7
0
.
6
0
1
2
:
v
i
X
r
a

1. Introduction

The number of malware in the wild has increased sig-
niﬁcantly in recent years. 1.1 billion total malware was
recorded in 2020 alone. That is an over 139% increase from
2015 (The Independent IT Security Institute, 2020). The

*Equal contribution 1Department of Computer Science and
Electrical Engineering, University of Maryland Baltimore County,
1000 Hilltop Circle, Baltimore, MD, 21250. 2Booz Allen Hamil-
ton. Correspondence to: John Boutsikas <iboutsi1@umbc.edu>,
Maksim E. Eren <meren1@umbc.edu>.

Presented at the Malware Technical Exchange Meeting, Online,
2021. Copyright 2021 by the author(s).

inﬂux in the quantity of malware makes Machine Learning
(ML) approaches such as statistical modeling, behavior anal-
ysis, and deep learning an ideal choice for malware analysis
by cyber defenders. When used as a helper in the cyber do-
main, ML has shown to be one of the most effective ways to
reduce risk, drive precise detection, reduce cost, and reduce
response and recovery time (Bissell et al., 2020). Because
of its advantages, ML and Artiﬁcial Intelligence (AI) are uti-
lized by 38% of organizations (Bissell & Ponemon, 2019),
and 83% of these spend more than 20% of their cybersecu-
rity budget on such technologies (Bissell et al., 2020). At
the same time, antivirus (AV) vendors have begun to widely
utilize ML based malware detection techniques (Microsoft
365 Defender Threat Intelligence Team, 2020; Quintero,
2019b;a; Fleshman et al., 2018).

Recent work has shown that top AV that utilize some form of
ML technology can be bypassed with simple modiﬁcations
on the malware such as by adding a new section, appending
a single byte, removing the debug and certiﬁcate values,
or renaming a section (Song et al., 2020; Anderson et al.,
2018). In our work, we borrow the feature changes that are
shown to be effective in prior research, and explore a new
mutant malware discovery methodology that is based on
Monte Carlo Tree Search (MCTS).

We approach classiﬁer evasion as a game playing exercise
between the adversary and the ML model where a win-
ning hand is a successful mutation that makes the malware
undetectable. Similar to chess, there are numerous state
permutations – mutations in our case – that can yield a win-
ning play at any stage of our ”game”. MCTS can ﬁnd the
winning hand by simulating a subset of all possible mu-
tations, and discover an optimal path using an empirical
scoring method. This allows empirical evaluation of muta-
tions while minimizing error, and examines paths without
actually computing all the possible permutations of changes.
At the end, MCTS prioritizes minimizing the number of
trackable changes that lead to evasion, hence avoiding ex-
cessive changes to the binary.

In this experiment, we stage a grey box adversarial scenario
where the target classiﬁer algorithm, decisions made by
the classiﬁer, and the data used in training are unknown,
but the features used in training are known. The attacker

 
 
 
 
 
 
Evading Malware Classiﬁers via Monte Carlo Mutant Feature Discovery

trains a local Decision Tree (DT) with the test portion of
the EMBER-2018 dataset (Anderson & Roth, 2018) and
discovers evasive feature modiﬁcations via MCTS while us-
ing the surrogate model (DT) for conﬁrmation. This allows
attackers to circumvent querying AV APIs to verify their
modiﬁcations. Such a scenario is feasible for an adversary
when an API for veriﬁcation is unavailable because it is part
of an internal defense system at an organization, or if the
malicious actor wants to avoid attention. The performance
of the mutations is then evaluated against the victim Mul-
tilayer Perceptron (MLP), which is trained on the training
portion of EMBER-2018, that takes the place of the target
antivirus engine.

Our results show that MCTS ﬁnds successful mutations on
the surrogate model for 52.18% of the malware, out of which
8.78% evades detection by the victim model, performing
slightly better than our random mutation baseline with a
5.26% evasion rate. Our contributions in this paper include:

• Demonstrating an evasive malware feature discovery

technique with Monte Carlo tree search.

• Analyzing the detection avoidance success rate when
the adversary does not have access to the ground truth
via an AV API.

2. Relevant Work

Adversarial attacks against ML models and defenses for
these attacks are widely studied research areas (Jia et al.,
2020; Papernot et al., 2017; Tram`er et al., 2017). Conse-
quently, an open source toolbox for adversarial training has
become popular in recent years 1. These attacks include
modiﬁcations on the training data, poisoning the feature
vector, or extracting the ML model parameters. When it
comes to malware data science, however, there are special
considerations to be taken.

Song et al. argues that some attacks are not always realistic
or have an effect on the end user in the malware domain
(Song et al., 2020). Attacks performed on feature vectors
are not realistic as the poisoned feature vector may not pos-
sess the corresponding binary representation. Conversely,
the attacks on byte-based models like MalConv (Raff et al.,
2021; 2018) insert adversarial content into an executable
that maintains validity while attempting to hide the evasive
payload (Suciu et al., 2019; Kolosnjaji et al., 2018; Kreuk
et al., 2018). An additional constraint (to the defender’s
advantage) is that attacks are one-way, as benign authors do
not attempt to alter their ﬁles to become falsely detected as
malicious – only malicious authors alter their ﬁles to avoid
detection (Fleshman et al., 2019; Incer et al., 2018). There-

1https://github.com/Trusted-AI/

adversarial-robustness-toolbox

fore, in our analysis we avoid performing attacks towards
feature vectors and focus on modifying the malware itself.

Similarly, changing malware requires additional rumina-
tions as the direct modiﬁcations of the compiled malware
may make the binary non-functional (Fleshman et al., 2018;
Song et al., 2020). Malware that is generated in such a
manner can evade the detector, but at the same time it may
no longer demonstrate the malicious behavior. A keylogger
could, for example, obtain permanence and display some
of its malware characteristics, but may no longer actively
log keystrokes, as that portion of the code has been altered
during the modiﬁcations to avoid identiﬁcation. We rec-
ognize this problem by developing heuristics that limit the
possible changes in the Monte Carlo Tree Search. With
that, we introduce a strong assumption that after the modi-
ﬁcation(s), the malware is still functional. In a real world
example, an adversary can alter this rule set if the changes
break the malware. Nevertheless, malware functionality can
be veriﬁed on a virtual sandbox environment, and we leave
this signiﬁcant step to future work.

Additionally, Song et al. points out that some of the attacks
demonstrated by other research assume inside knowledge
with a white box setup, which is not realistic for an adver-
sary to possess in real world scenarios (Song et al., 2020).
For example, an attacker may not have access to the mal-
ware classiﬁer’s architecture when it is part of the internal
company infrastructure, unless the attacker obtains such
knowledge via other means in the reconnaissance stage of
the attack. Furthermore, in such a scenario, the attacker
does not have access to the decisions the malware classiﬁer
makes. With this in mind, we stage a grey box attack where
it is assumed that the attacker lacks access to the malware
classiﬁer but knows the features used in training; hence, the
attack must be conducted without the ground truth in clas-
siﬁcation. Consequently, in our setup an adversary trains a
surrogate model using a different set of data to verify the
success of mutations.

With all the important considerations above, Song et al. de-
velop a malware modiﬁcation and mutant malware function-
ality veriﬁcation system, and show that the static classiﬁers
of real AV programs can be evaded 24.3% to 41.9% of the
time with changes as simple as a single byte modiﬁcation
(Song et al., 2020). Finally, they discuss the fact that even
though a sample can evade the classiﬁer, it is not known
what factors contribute to this evasion. In order to alleviate
this concern, they provide a framework that can generate
malware mutations while preserving the original malicious
behavior of the sample, and also providing a way to trace
back those modiﬁcations in order to explain which features
were responsible for the evasion. Similarly, our mutant fea-
ture searching method can re-track the changes that caused
the original mutation with a minimal number of steps.

Evading Malware Classiﬁers via Monte Carlo Mutant Feature Discovery

Table 1. Statistics for 299,867 benign and 299,851 malicious binaries in EMBER-2018 training set

Malware

Benign Software

Mean

Std

Min

Max

Mean

Std

Min

Max

Strings Entropy
Number of Strings
File Size
Number of Exports
Number of Imports
Timestamp (POSIX)
Size of Code
Number of Sections

5.967
6.12 × 103
1.24 × 106
9.019
98.993
1358407000
2.75 × 106
5.088

0.615
1.67 × 104
2.39 × 106
166.313
140.209
416879700
7.10 × 107
3.229

0
0
512
0
0
0
0
1

6.584
1.63 × 106
2.71 × 108
5.26 × 104
3074
4294967000
4.29 × 109
97

5.595
8.26 × 103
1.71 × 106
51.877
113.456
1332756000
5.92 × 105
4.854

0.659
3.37 × 104
6.93 × 106
625.764
286.144
574590400
4.17 × 106
2.639

0
0
2.34 × 102
0
0
0
0
0

6.585
2.48 × 106
5.36 × 108
52628
21344
4294967000
1.67 × 109
198

3. Dataset

3.1. Pre-processing

We base our analysis on the EMBER-2018 dataset (Ander-
son & Roth, 2018). EMBER contains features extracted
from 1.1M Windows PE ﬁles in a human friendly, JSON
format, along with their label indicating whether the sample
is malicious or benign. The main reason for our choice is
the same that drove the creation of EMBER. A large col-
lection of binaries is bound by many difﬁculties including
logistical ones (i.e. storage, safe distribution) and legal ones
(i.e. copyrighted binaries). EMBER resolves that by not
sharing the binaries themselves – or even their name for that
matter – but by sharing a parsed representation of various
metadata that can be extracted from the PE header. We use
this metadata to train our surrogate and victim models.

The default training and test set split in the EMBER dataset
has a signiﬁcant factor in our grey-box setup. Our victim
malware classiﬁer is trained on the EMBER-2018 train-
ing portion which consists of 299,867 benign and 299,851
malware instances for a total of 599,718 samples. On the
other hand, our surrogate (adversary’s local model) will only
utilize the EMBER-2018 test set that contains 199,800 sam-
ples, of which exactly half are malicious. This split supports
our grey-box setup as in the real world the attacker would
not have access to the malware samples used in training of
the target malware classiﬁer, and would most likely have
access to fewer samples than the entity supporting the target
classiﬁer. In Table 1 we compare the statistics for malware
and benign-ware in the EMBER training set.

At ﬁrst glance, we see that malware has a higher mean
string entropy compared to benign software on average, as
a result of malware often being obfuscated to avoid detec-
tion. Obfuscation randomizes the information in malware
and increases the entropy value. Meanwhile, the average
number of library exports and imports is greater for benign
software. This supports the idea that malware is developed
for more speciﬁc tasks requiring a smaller number of im-
ports and exports. We then proceed to the pre-processing of
our dataset, utilizing the tools available in the Scikit-learn
library (Pedregosa et al., 2011).

During training, both the surrogate model and the victim
classiﬁer utilize numerical and categorical static malware
analysis based features. The numerical features used in
training are String Entropy, Number of Strings, File Size,
Timestamp, Size of Code, Number of Sections, Number of
Exports, and Number of Imports. These features are scaled
to unit variance during pre-processing, and missing values
are replaced with the median.

Categorical attributes include the Has Debug ﬂag which
indicates if the binary has the debug value set, the Has Sig-
nature which is used to indicate the existence of the binary
certiﬁcate, and the Entry string that names the section name
the binary uses to start the code execution. Categorical
features are vectorized via one-hot encoding and missing
values are replaced with the most frequent element for each
feature. Unknown categorical attributes in the validation set
are ignored. We also used imported libraries and imported
functions during training. Unlike the previous categorical
features, imported libraries and imported functions are vec-
torized by hashing their values while limiting the number of
dimensions to 210 for libraries and 213 for the functions. Fi-
nally, all the features that are mentioned above are combined
via multimodal feature fusion to create a single features vec-
tor X representing each n malware sample.

4. Adversarial Model

Adversarial scenarios differ in terms of the knowledge that
an attacker possesses about the target. In a white-box adver-
sarial setting, it is assumed that the attacker has signiﬁcant
knowledge about the target model, including the algorithm
and the features-set used in training. On the other hand,
black-box attacks are more realistic as the only known in-
formation is the output label of an otherwise unknown al-
gorithm. For the malware classiﬁers; however, the output
label may also be unavailable when the model is part of an
internal defense system at an organization. Therefore, an
attacker needs to verify the validity of modiﬁcations locally
before the attack is deployed.

Evading Malware Classiﬁers via Monte Carlo Mutant Feature Discovery

Our adversarial setup follows a grey-box scenario, a struc-
ture that is in between the white-box and the black-box
setting, where the malware features and their corresponding
pre-processing steps are the same for the surrogate and the
target model. All the other information about the victim
model is assumed to be unknown, including the prediction
labels. To this extent, we train two different models where
one takes the place of the victim malware classiﬁer an AV
would have used, and a separate surrogate classiﬁer that the
malicious actor trains to identify the evasive feature muta-
tions. In this section we describe both of these models, and
provide their performance with receiver operation charac-
teristics (ROC) and area under the curve (AUC), and with a
precision-recall (PR) curve and the F1 scores.

4.1. Grey-box Surrogate Learner

Because the malicious actor does not have access to the
decisions made by the target classiﬁer, a local Decision Tree
is trained, using Scikit-learn (Pedregosa et al., 2011), on a
random subset of 60% of the EMBER test set. Predictions
made by the Decision Tree are used to identify the features
which makes the mutation valid. A valid mutation is when
a malware sample is predicted to be benign by the surrogate
after a mutation or combination of mutations.

We set the Decision Tree (surrogate) maximum depth hyper-
parameter to 12. Our model obtains an ROC-AUC of 0.956
and an F1 score of 0.916 when classifying the remaining
40% of the EMBER-2018 test set.

4.2. Victim Malware Classiﬁer API

We train a Multi-layer Perceptron (MLP) that takes the
place of the AV. We train our model on the EMBER training
set with a 20% split for the validation set which is used
in hyper-parameter tuning. Our model consists of seven
hidden fully connected layers with 512, 256, 128, 256, 128,
256, and 128 hidden nodes, respectively. In the hidden
layers of the MLP we use the Rectiﬁed Linear Unit (ReLU)
activation function (Nwankpa et al., 2018; Nair & Hinton,
2010). The output layer with a single node transfers the

input xi with the Sigmoid activation function
the model optimizer, we use Adaptive Moment Estimation
(Adam) (Kingma & Ba, 2019). Adam’s exponential decay
hyper-parameters β1 and β2 are left at default β1 = 0.9 and
β2 = 0.999 as suggested in (Kingma & Ba, 2019).

. For

1
1 + e−xi

With this setup, our model obtains the ROC-AUC score of
0.964 and F1 score of 0.934 when classifying the 200,000
benign and malicious instances in the held out EMBER-
2018 test.

5. Mutant Malware Generation

As discussed earlier, our goal is to generate mutations of the
malware in the EMBER-2018 dataset, such that the mutated
malware will not be detected by the API classiﬁer. However,
it is very important that those samples remain malicious
after the modiﬁcation. To that end, we provide a way to
limit the mutations that can be applied to any given sample –
mutated or otherwise. We will next explain these limitations
and their purpose. There is a difference in the complexity
of the mutations, as some imply that a number of different,
simpler mutations also happen, and can be thought of as
aggregate mutations.

5.1. Target Features

Here we list the set of possible mutations available to
our Monte Carlo implementation, and their corresponding
heuristic that limits the changes:

1. Add String: Any sample can have a total of 15 strings
added to it, the reason being that there is a limit to the
free, unused space in an already compiled binary, and
as such there is a limit to how many strings can be
added by using that space.

2. Add String with Size: This mutation shares the same
15 strings limit as the previous mutation; however,
only 5 of these strings can also modify the size of the
binary. The purpose of this simulation is to account
for strings that can be added by small extensions to
existing sections of the binary. Our implementation
changes the size of the binary by 30 bytes.

3. Change String Entropy: This mutation attempts to
modify the string entropy of the binary, towards a ”be-
nign” value. For our benign value we use the average
entropy of all benign samples. There can be a total of 7
entropy modiﬁcations per sample. It represents adding
speciﬁc strings with the goal of modifying the entropy
towards a target value, versus just adding any string
like the previous 2 mutations did.

4. Change String Entropy with Size: This mutation is
similar to the previous one, and shares the same 7 appli-
cations cap. However, only 3 of those 7 mutations can
also affect the size of the binary. The major difference
with this mutation, is that since the attacker can specify
the size, it is easier to construct strings that provide a
more signiﬁcant change to the overall entropy. Both
entropy mutations imply the addition of a string.

5. Remove String: By removing a string, we are trying
to simulate the act of the attacker ﬁnding a series of
random bytes that are interpreted as strings, but they
are in fact not strings, and changing those bytes so

Evading Malware Classiﬁers via Monte Carlo Mutant Feature Discovery

they are no longer detected as strings by the feature
extraction. We limit these removals to 4 per sample, as
those type of strings should be less frequent.

6. Add Section: With this mutation we add a section with
benign contents to the sample. There is no limit to how
many times this can be applied, as after some testing
we found that our algorithm would avoid adding a large
quantity of sections, and instead use a shorter route to
evasion. Each section adds 512 bytes to the sample’s
size.

7. Add Bytes: In this mutation we simulate appending
benign bytes to any of the existing malware sections.
This mutation adds 128 bytes to the size of that section
and the overall size of the ﬁle. The mutation can be
applied as many times as desired.

8. Add Code Bytes: Appends 64 benign bytes to the
code section speciﬁcally. This mutation be applied as
many times as desired, but again our algorithm seems
to avoid extensive application of these.

9. Import Function: This mutation will add a function to
the import table of the malware, as well as the matching
DLL if it is missing. In order to decide what those
functions would be, we made a list of the most common
functions in the benign samples that do not appear in
malware samples. This gave us 14 candidate functions,
so each application of this mutation will select one of
those 14 at random, provided it is not already present
in the sample.

10. Change Timestamp: This mutation will modify the
timestamp of the sample towards a target timestamp, by
the given step size. For our target timestamp we used
the average timestamp of all benign samples, with a
step of 1000 milliseconds. The mutation can be applied
as many times as desired, as it can adjust the direction
of the step to move towards the target timestamp.

11. Remove Debug: This mutation will set the debug ﬂag
to false. The mutation can only be applied if the sam-
ple’s debug ﬂag is set to true.

12. Change Signature: This mutation sets the certiﬁcate
ﬂag to true. The mutation can only be applied if the
sample does not have a certiﬁcate already.

We use this list of feature modiﬁcations as they seemed to
provide reasonable results and prior work showed that suc-
cessful evasion is possible with some of them (Song et al.,
2020). Our framework allows for easy modiﬁcation of the
mutation values – by the researcher – to match any given
dataset, including restricting the heuristics if the modiﬁca-
tions break the malware. It also allows for easy extension
of these mutations to accommodate for the samples at hand.

5.2. Monte Carlo Mutant Feature Search

We decided to approach the mutation ﬁnding activity as a
game playing problem where a winning hand is a successful
mutation that makes the sample undetectable. This ﬁts well
with our problem because at any stage of our ”game”, there
are many different possible combinations of mutations that
can be the winning hand. There are multiple algorithms that
can be used in this context, including, but not limited to,
the mini-max alpha beta pruning and the expectimax tree
search algorithms. The main drawback of these algorithms
is that they rely on the existence of two opponents playing
the game. Thus, our analysis direction changed towards a
game playing algorithm that could be adapted to a ”single
player” context as our only player would be the program
that is trying to beat the surrogate model (i.e. the surrogate
model cannot make any ”plays”). As such, we decided to
proceed with the Monte Carlo Tree Search (MCTS) algo-
rithm. This algorithm still shares all the tree-related pitfalls
– like state explosion and extreme redundancy – with the
previously mentioned algorithms. We address those issues
in our implementation.

MCTS has gained popularity based on its application for
the game Go as described by (Coulom, 2009), as well as its
latest implementation in Google’s AlphaZero (Silver et al.,
2018). The algorithm consists of 4 conceptual stages. All
stages happen once per iteration.

1. Tree Traversal: In this phase, the algorithm traverses
the existing nodes of the tree. A node is a combination
of the mutations that have happened to the sample up to
that point. The goal is to reach a node that is currently
a leaf, and expand it if it is not a terminal node. We
consider a node terminal when a benign classiﬁcation
is reached. We name this behavior the Tree Policy.

2. Expansion: During expansion the algorithm will add a
number of children nodes to the node found during tree
traversal. Each child will represent a mutation allowed
by our rules. The expansion of a given node happens
only once; the second time a non-terminal, leaf node is
visited. The ﬁrst time we visit a leaf node, we simulate
it on its own; therefore, we cannot also expand it at the
same time. We name this the Expansion Policy.

3. Simulation: The application of Monte Carlo methods
happens in this phase. Starting from the node that
is currently evaluated, the algorithm generates subse-
quent states at random until it reaches a terminal node
or we reach our simulation depth. At the end of this
phase, a score is calculated for the node that is under
evaluation by accounting for the simulated number of
mutations, then the simulation states are discarded. Fi-
nally, the score is then propagated back up the tree. We
name this behavior the Simulation Policy.

Evading Malware Classiﬁers via Monte Carlo Mutant Feature Discovery

4. Back propagation: In this phase, we update the score
of the node we just evaluated as well as its visit count,
and then propagate this change up the tree until we
reach the root node. This way the tree policy can select
a more suitable path to explore in the next iteration.

Algorithm 1 Tree Policy
Input: node root
Initialize node = root
while node.is expanded do

node = arg maxucb1 node.children

end while

MCTS on its own does not sufﬁce. Even successful imple-
mentations – AlphaGo Zero for instance – do not rely on
the algorithm alone, and make modiﬁcations. Similarly, we
implement the following modiﬁcations to the various phases
of MCTS, to better suit our problem.

We borrow inspiration from the Upper Conﬁdence Bounds
algorithm as applied to trees (UCT) for the Tree Traversal
phase (Kocsis & Szepesv´ari, 2006). UCT allows select-
ing the next best child to explore using an empirical score,
through the use of bandit methods, instead of relying on
obtaining the actual score. Computing the real score of the
child is expensive, and quite often impossible for our case
as ﬁnding a successful mutation from any given node is
not always achievable. However, the use of the empirical
score introduces a potential divergence from the true value.
Thankfully, the algorithm is shown in the original paper to
be consistent within the estimation error caused by sam-
pling. We apply a modiﬁcation on that algorithm, and use
the UCB1 function to evaluate the next child to traverse:

ucb1 =

scorechild
visitschild

+ c ·

(cid:115)

ln(visitsparent )
visitschild

where the ﬁrst element in the sum is the empirical score
of the child node we want to evaluate, c is the exploration
coefﬁcient, and the squared root factor gives a higher score
to the children that we have visited the least. In this context,
a 0 visit means always exploring the child node. Finally, a
high exploration coefﬁcient makes our traversal more akin to
breadth-ﬁrst search, while a lower value makes the traversal
similar to a depth-ﬁrst search. We put these modiﬁcations in
place with the hope that our search will not limit itself into
exploring a few speciﬁc mutations, but will instead explore
a larger portion of the mutation space.

By using the above evaluation score the Tree Policy selects
the child with the highest evaluation to traverse next with:

– mentioned in Section 5.1. In the original MCTS, a child
is added for each potential action from the current node
when a node is expanded. Speciﬁcally, we limit the avail-
able children nodes to the ones with applicable mutations
according to our invariants. This reduces the potential state
explosion, and limits generating mutations that are likely to
break the malware. Finally, we need to address the redun-
dancy present in a tree structure. If 0, 1, 4 are 3 different
mutations, the classiﬁcation result for [0, 4, 1] and [4, 1, 0]
should be the same since we examine the mutation set as a
whole instead of each mutation individually. By sorting the
proposed path and hashing it, we can cross-reference the
hash of the sorted new mutation with all the other, sorted
paths which we have already examined. If we have a match
for this proposed path for the malware sample n, the muta-
tion is not added during the expansion phase as a child node.
This further reduces the potential state expansion as well as
our runtime, as we no longer spend time on the previously
seen combinations.

Algorithm 2 Expansion Policy
Input: Current node node
Input: Existing mutations mutations
Input: Seen paths seen paths
for mutation ∈ mutations do
if mutation.is allowed then

Proposed path
p = node.mutation path ∪ mutation
Hashed path
h = hash(sort(p))
if h /∈ seen paths then

Make a new child based on mutation
node.children = node.children ∪ new child

end if

end if
end for

next child = arg max

children

ucb1

This allows our search to change which part of the tree it
explores in each iteration, and give high prioritization to the
nodes that have not been explored before using the UCB1
based evaluation.

Similarly, we apply some modiﬁcations in the expansion
phase. We begin with our mutations rules – or invariants

Our simulation phase is not that different from the original
algorithm. However, we do still enforce our invariants when
expanding the randomly selected nodes. If we skip this step,
the algorithm could generate a successful mutation that is
not allowed by our invariants. Therefore, this part of the
tree would get a high score, but the search would never
be able to construct the path as it is not considered valid
during the expansion phase. During the simulation phase,
child selection happens through random sampling instead

Evading Malware Classiﬁers via Monte Carlo Mutant Feature Discovery

of UCB1 evaluation.

Algorithm 5 Path recovery

Algorithm 3 Monte-Carlo Tree Search

Input: sample s, iterations it
Initialize iterations = 0.
Initialize root from sample
node = root
repeat

while node.is expanded do
node = TreePolicy(node)

end while
if node.visit count (cid:54)= 0 then
ExpansionPolicy(node)
node = TreePolicy(node)

end if
path = SimulationPolicy(node)
score = EvaluatePath(path)
BackPropagate(score)

until iterations is it

Finally, before we move onto back propagation, we need a
value to assign as the score of the newly evaluated node. We
elected to use the length of the simulated path to a benign
mutation. So for example, if our simulation policy produced
the path [7, 8, 4, 4, 9, 10] from some node n, then the score
of n will be length(path) = 6. However, since we care
about the shortest mutation paths we use the negative of that
score. This allows the shortest paths to produce the higher
scores. If no benign classiﬁcation was found, we set the
score to −∞:

(cid:26)−length(min(path)),

s =

−∞,

if benign
otherwise

(1)

We should note that this score is produced only once, as each
node is simulated only once. It can be, however, updated
since in the subsequent iterations the search can explore the
children of n. The back propagation phase of these children
will modify the score of n.

Algorithm 4 Back propagation

Input: Node’s ancestors ancestors
Input: Score s
for node ∈ ancestors do

if node.score (cid:54)= −∞ AND s (cid:54)= −∞ then

node.score += s

else

node.score = s

end if
node.visits += 1

end for

We provide pseudocode for the search in Algorithm 3, where
the various policies are implementations of what we have
discussed so far, and EvaluatePath is shown in Equation 1.

Input: node root
Output: Mutation Path path
Initialize node = root.
Initialize path = empty list
while node.is expanded AND NOT node.is terminal
do

node = arg maxscore node.children
path = path ∪ node

end while

The last missing piece of our algorithm is the recovery of
the path once MCTS has ﬁnished all of its iterations. MCTS
is often used to select the next action or state transition.
Our goal, however, is to create a serializable version of the
shortest mutation path so that we can apply it later against
a different classiﬁer. Therefore, we need to recover the
whole mutation path. The algorithm is very intuitive, as our
scoring ensures that the best evaluated nodes will have the
highest – although still negative – scores and visit counts.
With that, recovering the path is as simple as starting from
the root of the tree then always selecting the child with the
highest score until we reach a terminal node. If after this
process we cannot reach a terminal node, it means that the
search was unable to ﬁnd a mutation for this sample.

6. Results

Here we present the mutant feature discovery results and
their performance when evading the target classiﬁer intro-
duced in Section 4.2. Additionally, we compare the Monte
Carlo search and its corresponding classiﬁer evasion perfor-
mance to a Random Search baseline. The longest mutation
chain that was produced by MCTS was 5; as such, we limit
the Random Search to mutate the malware a maximum of 5
times for our comparison. The reasons will become apparent
soon through the rest of this section.

6.1. Search Results

We begin by introducing the feature changes discovered
by the MCTS. Using the surrogate Decision Tree, MCTS
was able to ﬁnd successful mutations for over 56% of the
total malware samples in the EMBER-2018 test set. This
is shown in Figure 1 with the distribution of a total number
of mutations needed for samples to be misclassiﬁed. If the
algorithm was unable to ﬁnd a successful mutation in the
given setup/time, the sample is classiﬁed as a ”Failed muta-
tion”. Around 52% of the malware needed only a certiﬁcate
signature change (Change Signature) to be misclassiﬁed as
shown in Table 2. Speciﬁcally, changing the signature alone
was enough to alter the prediction for 71% of the successful
mutations. This behavior could be an artifact of the binary

Evading Malware Classiﬁers via Monte Carlo Mutant Feature Discovery

Table 2. Statistics from mutation discovery via Monte Carlo and Random Search on the surrogate Decision Tree (DT). The Alone column
presents the number of malware where one type of feature change was enough for misclassiﬁcation. In Group shows the number of
samples where the mutation appeared in combination with other mutations. The Repeats column provides the instance count where the
mutation type appears more than once. Affected Instances is the number of mutated malware, and Total Occurrence is the total number
of times our tree search yielded the mutation across all of the successfully modiﬁed malware samples.
Mutations over DT with MCTS

Mutations over DT with Random Search

Alone

In Group Repeats Affected Instances Total Occurrence Alone

In Group Repeats

Affected Instances

Total Occurrence

Add String
Add String with Size
Change String Entropy
Change String Entropy with Size
Remove String
Add Section with size
Add Bytes
Add Code Bytes
Import Function
Change Timestamp
Remove Debug
Change Signature

0
6
0
9724
0
698
0
0
502
0
10
37214

79
137
2223
3223
35
249
33
31
634
32
39
1238

23
0
90
5
0
151
0
2
559
1
0
0

79
143
2223
12947
35
947
33
31
1136
32
49
38452

115
143
2313
12952
35
1153
33
37
1949
33
49
38452

0
1
0
1144
0
186
21
20
66
0
33
4150

496
4355
615
6754
4389
4609
4424
4416
4451
4465
686
17470

27
0
30
1
510
506
508
481
575
536
0
0

496
4356
615
7898
4389
4795
4445
4436
4517
4465
686
17470

523
4356
645
7899
4937
5339
4993
4951
5128
5036
686
17470

nature of the Decision Tree that was used as a surrogate
model.

Figure 1. Distribution of number of mutations needed for misclas-
siﬁcation over the EMBER test set while using MCTS on the
surrogate model. A sample failed mutation, when the algorithm
was not able to ﬁnd a mutation in the given setup/time (for example
number of iterations limit is reached).

The second most potent mutation with MCTS is adding a
string that would simultaneously modify the string entropy
of the sample, increase the ﬁle size, and the number of
strings (Change String Entropy with Size). This result is
not unexpected as it is an aggregate mutation affecting 3 of
the binary features simultaneously. Change String Entropy
with Size was a desired path for 19% of the mutations, and
9,724 malware instances were mutated (i.e. successfully
misclassiﬁed) with this modiﬁcation alone.

When using Random Search, the feature that was used the
most was Change Signature, and the distribution of the num-
ber of mutations needed until evasion can be seen in Figure
2. Random Search mutated signiﬁcantly fewer malware
samples. In the times where the mutation was successful, it
required a higher number of changes.

Figure 2. Distribution of the number of mutations needed for mis-
classiﬁcation over the EMBER test set while using Random
Search on the surrogate model. A sample failed mutation, when
the algorithm was not able to ﬁnd a mutation in the given setup/time
(for example number of iterations limit is reached).

6.2. Classiﬁer Evasion Results

The performance of the mutations is evaluated against the
target API, or victim model, after the modiﬁcations found
by MCTS and Random Search with the Decision Tree are
serialized to mutate the malware samples in the EMBER-
2018 test set. The mutated malware is then transformed
using the pre-processing pipeline introduced in Section 3.1.
Finally, the victim classiﬁer predicts the new labels, and the
results from the MCTS and Random Search are compared.

As seen in Figure 3, approximately 8.79% of the mutations
found by MCTS managed to evade the model. While the ma-
jority of the malware is still detected, the increased number
of malware passing through the security parameters carry
high risk. In comparison, 5.26% of the mutations found by
Random Search evaded detection, and it was able to mu-
tate less than 23% of the samples. The reduced mutation
rate of Random Search is an artifact of limiting the search
space to the maximum number of changes performed by

12345Failed MutationNumber of Mutations Until Evasion101102103104Affected Instances (log)51.876%3.203%0.951%0.124%0.012%43.834%12345Failed MutationNumber of Mutations Until Evasion104Affected Instances (log)6.023%5.367%4.837%4.225%3.717%75.831%Evading Malware Classiﬁers via Monte Carlo Mutant Feature Discovery

Conclusion

Growing popularity of ML based malware detection makes
the analysis of these systems against evasive attacks an es-
sential part of cyber defense. We show that a malicious actor
can reduce the detection rate of malware samples without
the knowledge of the output labels of the Machine Learning
model that the adversary camouﬂages against. Utilizing the
adjustable Monte Carlo tree search, with the custom rule
set, an adversary can discover the modiﬁcations that makes
the malware undetectable by checking the changes against
a surrogate veriﬁer.

Future work can consider model extraction attacks to create
a copy surrogate model to be used as the validator. This
can include the re-training attack and the equation-solving
attack. Another direction for future work is to perform direct
modiﬁcations on the real malicious binaries and verify their
functionality after feature alteration(s) via a sandbox setup.
Finally, it would also be interesting to try a black box setup
where the adversary does not know the features used during
the training of the target.

References

Anderson, H. and Roth, P. Ember: An open dataset for train-
ing static pe malware machine learning models. ArXiv,
abs/1804.04637, 2018.

Anderson, H. S., Kharkar, A., Filar, B., Evans, D., and Roth,
P. Learning to evade static pe machine learning mal-
ware models via reinforcement learning. arXiv preprint
arXiv:1801.08917, 2018.

Bissell, K. and Ponemon, L. The cost of cybercrime. Tech-

nical report, Accenture, Ponemon Institute, 2019.

Bissell, K., LaSalle, R., and Cin, P. Innovate for cyber re-
silience. Technical report, Accenture, Ponemon Institute,
2020.

Coulom, R. The monte-carlo revolution in go.

In The
Japanese-French Frontiers of Science Symposium (JFFoS
2008), Roscoff, France, 2009.

Fleshman, W., Raff, E., Zak, R., McLean, M., and Nicholas,
C. Static Malware Detection & Subterfuge: Quantifying
the Robustness of Machine Learning and Current
In 2018 13th International Conference
Anti-Virus.
on Malicious and Unwanted Software (MALWARE),
pp. 1–10. IEEE, oct 2018.
ISBN 978-1-7281-0155-2.
doi: 10.1109/MALWARE.2018.8659360. URL http:
//arxiv.org/abs/1806.04773https://
ieeexplore.ieee.org/document/8659360/.

Fleshman, W., Raff, E., Sylvester, J., Forsyth, S., and
McLean, M. Non-Negative Networks Against Adver-

Figure 3. Mutation and evasion results from MCTS and Random
Search. Blue: The amount of successfully mutated malware on the
surrogate. Failed mutations are excluded. Purple (with hatch):
Successful evasions, over the total malware, against the victim.

MCTS in order to create a more equal comparison. If we
were to allow Random Search an unlimited search space,
we would be measuring the number of mutations required
before miss-classiﬁcation, instead of ﬁnding targeted muta-
tions. In theory, if we were to apply an inﬁnite number of
mutations, malware would eventually evade the classiﬁer.
However, this would also be detrimental to the functionality
of the sample.

These results show MCTS being able to ﬁnd more mutations
and yield a higher evasion rate in comparison to Random
Search in our setup. It should also be noted that our Monte
Carlo implementation is ﬂexible for more targeted settings.
If we were to narrow down our samples and adjust the
various Monte Carlo search hyper-parameters, we could
potentially get better results even with the Decision Tree as
the surrogate model. In addition, our system is ﬂexible in
what model is used as the surrogate. Therefore, an attacker
may consider using a more complex surrogate model like a
deep learning architecture, or an ensemble of models.

Our intuition, along with the results we have seen in this
paper, points to the surrogate model playing a considerable
role in the performance of the search. Speciﬁcally, we
are using a binary decision tree as our surrogate which
will need to grow in depth in order to properly train over
the dataset. That makes depth-ﬁrst searches more likely
to ﬁnd and exploit the decision tree, whereas a breadth-
favoring approach might work better against an MLP model.
Furthermore, the fact that a completely binary mutation was
the most used one in both searches might also be an artifact
of the binary decision tree. Therefore, the future work can
include examining how different surrogate models affect the
search as well as experimenting with favoring mutations of
slightly longer length in MCTS.

MCTSRandom SearchMutation Searching Method01000020000300004000050000Number of Mutated Malware8.787%52.188%5.264%22.457%Total Mutated MalwareEvaded DetectionEvading Malware Classiﬁers via Monte Carlo Mutant Feature Discovery

sarial Attacks. AAAI-2019 Workshop on Artiﬁcial Intelli-
gence for Cyber Security, 2019. URL http://arxiv.
org/abs/1806.06108.

Incer, I., Theodorides, M., Afroz, S., and Wagner, D. Ad-
versarially Robust Malware Detection Using Monotonic
In Proceedings of the Fourth ACM In-
Classiﬁcation.
ternational Workshop on Security and Privacy Analyt-
ics, IWSPA ’18, pp. 54–63, New York, NY, USA, 2018.
ACM. ISBN 978-1-4503-5634-3. doi: 10.1145/3180445.
3180449. URL http://doi.acm.org/10.1145/
3180445.3180449.

Jia, H., Choquette-Choo, C. A., and Papernot, N. Entangled
watermarks as a defense against model extraction. arXiv
preprint arXiv:2002.12200, 2020.

Kingma, D. P. and Ba, J. A. A method for stochastic opti-
mization. arxiv 2014. arXiv preprint arXiv:1412.6980,
434, 2019.

Kocsis, L. and Szepesv´ari, C. Bandit based monte-carlo
planning. In In: ECML-06. Number 4212 in LNCS, pp.
282–293. Springer, 2006.

Kolosnjaji, B., Demontis, A., Biggio, B., Maiorca, D., Gi-
acinto, G., Eckert, C., and Roli, F. Adversarial Mal-
ware Binaries: Evading Deep Learning for Malware
In 26th European Signal
Detection in Executables.
Processing Conference (EUSIPCO ’18), 2018. URL
https://arxiv.org/pdf/1803.04173.pdf.

Kreuk, F., Barak, A., Aviv-Reuven, S., Baruch, M., Pinkas,
B., and Keshet, J. Adversarial Examples on Discrete
Sequences for Beating Whole-Binary Malware Detec-
tion. arXiv preprint, 2018. URL http://arxiv.
org/abs/1802.04528.

Microsoft 365 Defender Threat Intelligence Team. Mi-
crosoft researchers work with intel labs to explore
new deep learning approaches for malware classiﬁ-
https://www.microsoft.com/
cation, 2020.
security/blog/

Nair, V. and Hinton, G. E. Rectiﬁed linear units improve

restricted boltzmann machines. In ICML, 2010.

Nwankpa, C., Ijomah, W., Gachagan, A., and Marshall,
S. Activation functions: Comparison of trends in prac-
arXiv preprint
tice and research for deep learning.
arXiv:1811.03378, 2018.

Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik,
Z. B., and Swami, A. Practical black-box attacks against
machine learning. In Proceedings of the 2017 ACM on
Asia conference on computer and communications secu-
rity, pp. 506–519, 2017.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., et al. Scikit-learn: Machine learn-
ing in python. Journal of Machine Learning Research,
12(Oct):2825–2830, 2011.

Quintero, B. Virustotal += bitdefender theta, 2019a.
URL https://blog.virustotal.com/2019/
10/virustotal-bitdefender-theta.html.

Quintero, B. Virustotal += sangfor engine zero, 2019b. URL
https://blog.virustotal.com/2019/11/
virustotal-sangfor-engine-zero.html.

Raff, E., Barker, J., Sylvester, J., Brandon, R., Catanzaro, B.,
and Nicholas, C. Malware Detection by Eating a Whole
EXE. In AAAI Workshop on Artiﬁcial Intelligence for
Cyber Security, oct 2018. URL http://arxiv.org/
abs/1710.09435.

Raff, E., Fleshman, W., Zak, R., Anderson, H. S., Filar,
B., and McLean, M. Classifying Sequences of Extreme
Length with Constant Memory Applied to Malware De-
tection. In The Thirty-Fifth AAAI Conference on Artiﬁcial
Intelligence, 2021. URL http://arxiv.org/abs/
2012.09390.

Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,
M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-
pel, T., Lillicrap, T., Simonyan, K., and Hassabis, D.
A general reinforcement learning algorithm that mas-
ters chess, shogi, and go through self-play. Science,
ISSN 0036-8075. doi:
362(6419):1140–1144, 2018.
10.1126/science.aar6404. URL https://science.
sciencemag.org/content/362/6419/1140.

Song, W., Li, X., Afroz, S., Garg, D., Kuznetsov, D., and
Yin, H. Automatic generation of adversarial examples for
interpreting malware classiﬁers. pp. 18, 03 2020.

Suciu, O., Coull, S. E., and Johns, J. Exploring Adversarial
Examples in Malware Detection. In 2019 IEEE Security
and Privacy Workshops (SPW), pp. 8–14. IEEE, may
2019. ISBN 978-1-7281-3508-3. doi: 10.1109/SPW.2019.
00015. URL https://ieeexplore.ieee.org/
document/8844597/.

The Independent IT Security Institute. Malware statistics &
trends report: Av-test, Mar 2020. URL https://www.
av-test.org/en/statistics/malware/.

Tram`er, F., Kurakin, A., Papernot, N., Goodfellow, I.,
Ensemble adversar-
arXiv preprint

Boneh, D., and McDaniel, P.
ial training: Attacks and defenses.
arXiv:1705.07204, 2017.

