1
2
0
2

n
a
J

5
2

]
L
M

.
t
a
t
s
[

3
v
5
5
4
0
1
.
7
0
0
2
:
v
i
X
r
a

The Multilayer Random Dot Product Graph

Andrew Jones and Patrick Rubin-Delanchy

University of Bristol

Abstract

We present a comprehensive extension of the latent position network model known as the
random dot product graph to accommodate multiple graphs—both undirected and directed—
which share a common subset of nodes, and propose a method for jointly embedding the
associated adjacency matrices, or submatrices thereof, into a suitable latent space. Theoret-
ical results concerning the asymptotic behaviour of the node representations thus obtained
are established, showing that after the application of a linear transformation these converge
uniformly in the Euclidean norm to the latent positions with Gaussian error. Within this
framework, we present a generalisation of the stochastic block model to a number of diﬀerent
multiple graph settings, and demonstrate the eﬀectiveness of our joint embedding method
through several statistical inference tasks in which we achieve comparable or better results
than rival spectral methods. Empirical improvements in link prediction over single graph
embeddings are exhibited in a cyber-security example.

Contents

Contents

1 Introduction

2 The multilayer random dot product graph

2.1 Asymptotics and sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Theoretical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 The generalised multilayer stochastic block model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1 Undirected graphs
3.2 Directed graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Bipartite multilayer SBMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Rank considerations

4 Multiple graph inference: comparison with existing methods

4.1 Recovery of latent positions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Estimation of invariant subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Model estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Two-graph hypothesis testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Real data: Link prediction on a computer network

5.1 Dynamic link prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Port-speciﬁc link prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
5.3 Link prediction using mixed data sources

6 Chernoﬀ information and the GMSBM

7 Conclusion

1

1

2

5
6
8

11
11
13
14
15

16
16
17
18
19

21
21
22
22

23

27

 
 
 
 
 
 
Bibliography

Appendix

Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

29
41
43

1 Introduction

Networks permeate the world in which we live, and so developing an accurate understanding of
them is a matter of great interest to many branches of academia and industry, with applications
as diverse as identifying patterns in brain scans [12] and the detection of fraudulent behaviour in
the ﬁnancial services sector [1]. The mathematical study of random graphs has its roots in the
work of E. N. Gilbert [13] and, contemporaneously, Erd¨os and R´enyi [10], who considered graphs
in which edges between nodes occur independently according to Bernoulli random variables with
a ﬁxed probability p, in what can be considered the simplest probabilistic model of a naturally
occurring network (this type of graph now being universally referred to as an Erd¨os-R´enyi graph).
Among the more modern statistical treatments of networks is the concept of a latent position
model [15], in which the ith node of a graph is mapped to a vector Xi in some underlying latent
space X ⊆ Rd, and (conditional on this choice of latent positions) the ith and jth nodes connect
independently with probability κ(Xi, Xj), where the kernel function κ : X × X → [0, 1]. The
random dot product graph (RDPG, [25], [40], [5]), which uses the kernel function κ(x, y) = x(cid:62)y,
and its generalisation (GRDPG, [29]), using the kernel function κ(x, y) = x(cid:62)Ip,qy (where Ip,q is
the diagonal matrix whose entries are p ones followed by q minus ones, where p + q = d) are of
particular interest here due to their computational tractability and associated statistical estima-
tion theory: spectral embedding [38] the observed graph based on largest eigenvalues (respectively,
largest-in-magnitude) produces uniformly consistent estimates of the latent positions Xi of the
RDPG (respectively, GRDPG) model up to orthogonal (respectively, indeﬁnite orthogonal) trans-
formation, with asymptotically Gaussian error [32, 4, 23, 8, 6, 33, 29, 5]. The GRDPG can be used
to eﬀectively model networks which exhibit disassortative connectivity behaviour [18] in which
dissimilar nodes are the more likely to connect to each other, and is therefore the preferred model
for studying biological or technological networks, which typically exhibit such behaviour [24].

Recently, attention has turned to the joint study of multiple graphs. Often, the graphs of
interest share a common set of nodes but have diﬀerent edges, and such a collection of graphs
is known as a multilayer network [19] (more precisely, it is an example of a multiplex network).
This framework is of interest in the study of dynamic networks, in which each of the graphs may
represent a “snapshot” of a network at a given point in time, and has been used, for instance, for
link prediction in cyber-security applications [28]. Alternatively, one may be interested in detecting
diﬀerences in node behaviour between graphs, and this approach was used identify regions of the
brain associated with schizophrenia by comparing brain scans of both healthy and schizophrenic
patients [21].

Latent position models readily extend to the study of multiple graphs by allowing the kernel
functions κr to vary, while retaining a common set of latent positions Xi across the graphs,
and in particular there exist several RDPG-based methods for working with multiple graphs.
If each graph is drawn from the same distribution (that is, κr(x, y) = x(cid:62)y for each r) then
one can consider the mean embedding [35] by spectrally embedding the average of the adjacency
matrices ¯A = 1
r=1 A(r), or the omnibus embedding [21], in which each graph is assigned a
k
diﬀerent embedding in a common latent space. The mean embedding is known to perform well
asymptotically at the task of estimating the latent positions [35], while the omnibus embedding is
particularly suited to the task of testing whether the graphs are drawn from the same underlying
distribution.

(cid:80)k

Other RDPG-based methods are more general, allowing diﬀerent kernel functions κr across the
graphs. In the multiple random eigengraph (MREG, [39]) model, the kernel function κr(x, y) =
x(cid:62)Λry is used, where the matrices Λr ∈ Rd×d are diagonal with non-negative entries. The multiple
random dot product graph (multi-RDPG, [26]) loosens these restrictions by allowing the matrices
Λr to be non-diagonal (but symmetric and positive deﬁnite), while requiring that the matrix

2

of latent positions X have orthonormal columns. Expanding on this is the common subspace
independent edge (COSIE, [3]) graph model which allows the matrices Λr to have positive and
negative eigenvalues, while still requiring them to be symmetric. Each of these models is proposed
along with a spectral embedding technique for latent position estimation. Under the COSIE model,
the adjacency matrix of each component graph is embedded separately, into a dimension dr, say.
A second, joint, spectral decomposition is then applied to the point clouds, to ﬁnd a common
embedding of dimension d. The approach requires estimation of each dr as well as d, for which a
generic method based on the ‘elbow’ of the scree-plot is suggested [42].

Statistical discourse on network embedding is often inspired by the stochastic block model [16],
in which an unknown partition of the nodes exists so that the nodes of any group (or community) are
statistically indistinguishable. Under this model, a network embedding procedure can reasonably
be expected to ascribe identical positions to the nodes of one group, up to statistical error, and
diﬀerent embedding techniques can therefore be compared through the theoretical performance of
an appropriate clustering algorithm at recovering the communities [30, 33, 7].

Of the approaches referred to above, only the COSIE model allows estimation of a generic mul-
tilayer stochastic block model [16]. For example, if one graph has assortative community structure
(“birds of a feather ﬂock together”) and the other disassortative (“opposites attract”), then the
mean embedding can evidently eradicate all community structure visible in either individual graph.
As a model for multiple undirected graph embedding, the Multilayer Random Dot Product
Graph model (MRDPG), presented here, is equivalent to the COSIE model in terms of its likeli-
hood given latent positions, but the latent positions are themselves deﬁned diﬀerently. The spectral
embedding method to which this leads is materially diﬀerent and simpler, while estimation per-
formance is apparently superior (by numerical experiments). However, the MRDPG in fact allows
for far greater generalization than the models we have discussed; it not only extends naturally to
accommodate both symmetric and asymmetric adjacency matrices (and thus allow us to extract
information from directed graphs) but also non-square binary matrices, such as the non-zero oﬀ-
diagonal blocks appearing in the adjacency matrix of a bipartite graph, by allowing the columns
of such matrices to correspond to a second set of latent positions Yi. Moreover, provided that the
rows of each matrix correspond to a common set of latent positions Xi, we may in fact allow the
columns of each individual matrix to correspond to diﬀerent sets of latent positions Y(r)
(allowing
us, for example, to study the behaviour of a particular collection of nodes in a network through
observing their interactions—potentially in a number of diﬀerent settings—with other collections
of nodes).

i

We retain the use of the kernel functions κr(x, y) = x(cid:62)Λry for each graph, but now note that
κr : X × Yr → [0, 1] for subsets Yr ⊆ Rdr , where we allow arbitrary matrices Λr ∈ Rd×dr and
impose no restriction of orthogonality on the matrices of latent positions X and Y(r). These latent
position matrices are either independent of each other, or else satisfy certain dependence criteria;
such as the Y(r) being equal—potentially up to some linear transformation— with probability
one (a full discussion of this is presented in Section 2.1). Given a collection of binary matrices
A(1), . . . , A(k) with each A(r) ∈ {0, 1}n×nr , those latent positions are then estimated by the
following procedure: we form a matrix A—which we refer to as the unfolding of the matrices
A(r)—by adjoining the matrices A(r), and obtain left and right spectral embeddings of A by
scaling its left and right singular vectors by the square roots of the corresponding singular values.
We refer to these embeddings as the Unfolded Adjacency Spectral Embeddings (UASEs) of the
A(r). The left-sided embedding XA is our proposed estimate of X, that is, a single embedding of
the nodes that is common to all graphs; the right-sided embedding YA can be split into k distinct
embeddings Y(r)
A , which can be shown (under certain criteria) to provide estimates of the latent
position matrices Y(r).

We allow the matrices Λr to be of non-maximal rank, requiring instead that the matrix Λ =
[Λ1| . . . |Λk] be of maximal rank. This allows for the situation in which information about the
latent positions can be obscured in individual graphs. As a simple example, consider a three-
party political system in which members always vote along party lines, with graphs representing
the outcome of votes on particular motions (in which members can either support or oppose the
motion, and two members are linked if they vote in the same way). Suppose further that there are
no coalitions (that is, every pair of parties has at least one motion on which they vote diﬀerently).
Then any individual vote will only highlight two groups (those who support and those who oppose

3

that particular motion) and it is only with knowledge of multiple votes that one can correctly
identify the individual parties. In our method, no intermediate estimate of the rank dr of Λr is
required, in contrast to the COSIE-based approach.

We investigate the asymptotic behaviour of the left- and right-sided embeddings of the un-
folding A under an MRDPG model. It is shown that, up to linear transformation, the rows of
each embedding converge uniformly in the Euclidean norm to the latent positions Xi (Theorem
2) and, through the derivation of a central limit theorem (Theorem 3), that these rows are dis-
tributed around their corresponding latent positions according to a Gaussian mixture model, and
thereby signiﬁcantly extending the existing results of [5] and [29] to our more general model. These
distributional results show that, in particular, if the graphs are identically distributed then the
transformed rows of the left-sided embedding have the same limiting distribution as those of the
mean embedding (Corollaries 4 and 5). Consequently, if multiple graphs are identically drawn
according to a stochastic block model [16] then joint embedding will always be more eﬀective at
the task of cluster separation than any individual graph embedding (Proposition 6), where we
evaluate this eﬀectiveness via the Chernoﬀ information [14] of the limiting Gaussian distributions
of the embeddings. The Chernoﬀ information belongs to the class of f -divergences [2, 9] and is
therefore invariant under invertible linear transformations [22], an important requirement here
since distributional results hold only up to such transformation.

Using the framework of the MRDPG, we propose an extension to the multilayer stochastic
block model [16] which we refer to as the generalised multilayer stochastic block model (GMSBM).
Given a collection of graphs—which may be either directed or undirected—each of which follows
a stochastic blockmodel, the GMSBM allows us to study one or more communities common to all
graphs by observing their interactions with all communities. The point cloud obtained by jointly
embedding the unfolding of the resulting adjacency submatrices then exhibits the asymptotic be-
haviour detailed in our main results; allowing us, for example, to determine whether we can further
divide these communities given the information extracted from the multiple embeddings. We pro-
vide empirical evidence of the eﬀectiveness of our embedding method at the task of community
detection under the GMSBM, demonstrating that ﬁtting a Gaussian mixture model to the UASE
achieves better results than rival spectral methods in both the directed and undirected cases.

We assess the eﬀectiveness of unfolded adjacency spectral embedding for the general MRDPG at
the inference tasks of recovery of latent positions, estimation of the common invariant subspaces,
estimation of the underlying probabilistic model and two-graph hypothesis testing in simulated
data. We demonstrate that performance at the estimation tasks is often better than that of the
multiple adjacency spectral embedding (the method proposed in [3] to embed multiple graphs
distributed according to a COSIE model, which demonstrably yields state of the art performance
at such tasks), while its performance at the latter task is comparable with that of the omnibus
embedding for reasonably-sized graphs (those with at least 500 nodes). We also apply the UASE
to the task of link prediction, using connectivity data from the Los Alamos National Laboratory
computer network [37] to predict connections between computers across the entire network as an
example of a dynamic link prediction inference task, before restricting our attention to connections
occurring through speciﬁc ports, demonstrating that the majority of the time the UASE yields
greater accuracy than individual adjacency spectral embeddings. As a ﬁnal example, we show
how incorporating connection data between computers and ports into our model can increase the
accuracy of our link prediction method.

The remainder of this article is structured as follows. In Section 2 we present our model, and
corresponding asymptotic results. In Section 3 these results are explored within the context of a
stochastic block model, appropriately extended to diﬀerent multiple graph settings, all special cases
of the MRDPG. Section 4 presents a series of experiments comparing the performance of unfolded
adjacency spectral embedding with rival methods at diﬀerent inference tasks. Section 5 presents an
example from a real computer network, from which multiple graphs are extracted by considering
diﬀerent time windows or diﬀerent port numbers (indicating diﬀerent types of network service),
and are used together to improve link prediction. In Section 6, we investigate the statistical gain
of joint versus individual spectral embedding in terms of Chernoﬀ information, proving there is
deﬁnite improvement in one special (but interesting) case, leaving open a wider conjecture. Finally,
Section 7 concludes.

4

2 The multilayer random dot product graph

Deﬁnition 1. (The multilayer random dot product graph model).
For a positive integer k, ﬁx matrices Λr ∈ Rd×dr for each r ∈ {1, . . . , k} such that the matrix
Λ = [Λ1| · · · |Λk] is of rank d, and ﬁx bounded subsets X , Y1, . . . , Yk of Rd, Rd1, . . . , Rdk respectively
such that for each r, x(cid:62)Λryr ∈ [0, 1] for all x ∈ X and yr ∈ Yr.
1 × · · · × Y nk

Fix a joint distribution F supported on X n × Y n1
1 , . . . , Y(k)

for positive integers n, n1, . . . , nk
nk ) ∼ F. Deﬁne X = [X1| · · · |Xn](cid:62) ∈ Rn×d and Y = Y(1) ⊕
nr ](cid:62) ∈ Rnr×dr . Finally, deﬁne matrices P(r) =
∈ [0, 1]n×nr for each r, and deﬁne the unfolding P = [P(1)| · · · |P(k)] (which we note

and let (X1, . . . , Xn, Y(1)
· · · ⊕ Y(k), where each matrix Y(r) = [Y(r)
XΛrY(r)(cid:62)
satisﬁes P = XΛY(cid:62)).

1 | · · · |Y(r)

k

Given a set of matrices A(1), . . . , A(k) with each A(r) ∈ {0, 1}n×nr , we similarly deﬁne the
unfolding A = [A(1)| · · · |A(k)]. We say that (A, X, Y) ∼ MRDPG(F, Λ) if each matrix A(r)
satisﬁes one of the following:

• If, with probability one, there exists a matrix Gr ∈ Rd×dr such that Y(r) = XGr, then

conditional on X and Y(r), either:

– A(r) is hollow and symmetric, satisfying A(r)

– A(r) is hollow and asymmetric, satisfying A(r)

ij ∼ Bern(cid:0)P(r)
ij ∼ Bern(cid:0)P(r)

ij

ij

(cid:1) for all i > j, or
(cid:1) for all i (cid:54)= j.

• If, for all matrices Gr ∈ Rd×dr , the probability that Y(r) = XGr is strictly less than one
ij ∼ Bern(cid:0)P(r)

then, conditional on X and Y(r), A(r)

(cid:1) for all i and j;

ij

We say that the graph corresponding to the matrix A(r) is “undirected”, “directed” or “bipar-
tite” to distinguish between these cases, and allow a mixture of these cases among the matrices
A(1), . . . , A(k) (we note that for the sake of brevity—particularly within the proofs located in the
Appendix—we will occasionally abuse our terminology and refer to the matrices A(r) using these
terms).

The distributional results that we will obtain later can be reﬁned if we restrict our attention to
the case in which the matrices A(1), . . . , A(k) are identically distributed (corresponding, say, to a
multiple graph embedding in which we expect identical behaviour across the graphs). To this end,
of rank d and a distribution F, we write (A, X, Y) id∼ MRDPG(F, Λ)
for a ﬁxed matrix Λ ∈ Rd×d(cid:48)
if, under the distribution F, all of the matrices Y(r) are equal with probability one to Y ∈ Rn(cid:48)×d(cid:48)
,
and the matrices Λr are all equal to Λ.

We note that there is a degree of ambiguity in the choice of latent positions for the MRDPG,

as the following result shows:

Proposition 1. Let (A, X, Y) ∼ MRDPG(F, Λ). Then:

(i) (A, XG(cid:62), YH(cid:62)) ∼ MRDPG(FG,H, G−(cid:62)ΛH−1) for any matrices G ∈ GL(d) and H =
H1 ⊕ · · · ⊕ Hk ∈ GL(d1) × · · · × GL(dk), where the distribution FG,H is derived from F by
multiplying elements of X by G and elements of Yr by Hr for each r.

(ii) There is a joint distribution (cid:101)F and matrices of latent positions (cid:101)X and (cid:101)Y where each vector
j ∈ Rd such that (A, (cid:101)X, (cid:101)Y) ∼ MRDPG( (cid:101)F , Id,k), where Id,k = [Id| · · · |Id].

(cid:101)Xi, (cid:101)Y(r)

Proof.

(i) This follows from the fact that (XG(cid:62))(G−(cid:62)ΛH−1)(YH(cid:62))(cid:62) = Λ.

(ii) Let Λ admit the singular value decomposition Λ = UΣV(cid:62) with matrices U ∈ O(d), Σ ∈
Rd×d and V ∈ O((d1 + . . . + dk) × d). Then setting (cid:101)X = XUΣ1/2 and (cid:101)Y = YVΣ1/2 gives
the result.

5

Note that under the second transformation, while (cid:101)X is always of maximal rank, the rank of
(cid:101)Y(r) is equal to that of Λr and so it is possible to “lose” information about the latent positions
Y(r) in some sense by applying such a transformation.

Key to our study of the MRDPG will be the spectral embeddings of the unfolding A. Unlike
the GRDPG, in which one considers a single symmetric adjacency matrix, whose left and right
singular vectors therefore coincide, we obtain distinct embeddings by considering each side of A:

Deﬁnition 2. (Unfolded adjacency spectral embeddings).
Let (A, X, Y) ∼ MRDPG(F, Λ), and let A and P admit singular value decompositions

A = UAΣAV(cid:62)

A + UA,⊥ΣA,⊥V(cid:62)

A,⊥, P = UPΣPV(cid:62)
P,

(1)

where UA, UP ∈ O(n × d), VA, VP ∈ O((n1 + . . . + nk) × d), and ΣA, ΣP ∈ Rd×d are diagonal
containing the largest singular values of A and P respectively. We then deﬁne the following:

• The left UASE is the matrix XA ∈ Rn×d given by XA = UAΣ1/2
A .

• For r ∈ {1, . . . , k}, the rth right UASE is the matrix Y(r)

YA = VAΣ1/2
where we divide VA into k blocks V(1)

A into k blocks of sizes n1 × d, . . . , nk × d (equivalently, Y(r)

A , . . . , V(k)
A ).

A ∈ Rnr×d obtained by dividing
A Σ1/2
A ,

A = V(r)

We deﬁne the matrices XP, YP and Y(r)
P analogously.
The reader with a passing knowledge of tensor theory may wish to draw parallels between our
notion of an unfolding and that of the matrix unfoldings of a tensor. For the uninitiated, any
3-tensor M ∈ Rn1×n2×n3 can be represented as a matrix in one of 3 standard ways, each of which
is known as an unfolding of M (a precise description of these can be found in [20]). The unfoldings
of a tensor provide a more accessible means for studying its properties; in particular, there is a
notion of a singular value decomposition for tensors (see [20],Theorem 2) in which the “higher
order” analogues of singular values and vectors correspond to the standard matrix singular values
and vectors of the unfoldings.

In the special case in which the matrices of latent positions Y(r) are equal with probability
one (and so we may consider the A(r) to be adjacency submatrices for some ﬁxed subset of nodes
across a series of graphs) then the matrices A(r) give rise to a 3-tensor A by setting Aijr = A(r)
ij .
In this case, our notion of the unfolding of the matrices A(r) is a column permutations of the
ﬁrst standard unfolding of the tensor A, which therefore shares the same left singular values and
vectors, and is the natural choice to consider to allow us extend to the general case in which the
matrices Y(r) diﬀer (the second standard unfolding corresponds to the matrix (cid:2)A(1)(cid:62)
| · · · |A(k)(cid:62)(cid:3),
while the third is the matrix whose (i, j)th entry is the Frobenius inner product (cid:104)A(i), A(j)(cid:105)F ).

2.1 Asymptotics and sparsity

In our asymptotic analysis of the behaviour of the UASE, we will assume that the number of graphs
k
n = 0.
k is either ﬁxed, or at most grows at a rate much slower than n, in the sense that limk,n→∞
We will also assume that the number of latent positions Y(r)
grows at a comparable rate to the
number of latent positions Xi, in the sense that for each r ∈ {1, . . . , k}, there exists a positive
constant cr such that limnr,n→∞

i

nr
n = cr.

Before proceeding further, we establish some (standard) notation relating to the asymptotic
growth of various functions. In the following, f and g are real-valued functions of n, n1, . . . , nk,
and X and Y are real-valued random variables. We say that:

• f = Ω(g) if there is a constant c > 0 and integers N, N1, . . . , Nk such that for all n ≥ N and

nr ≥ Nr, f (n, n1, . . . , nk) ≥ cg(n, n1, . . . , nk);

• f = O(g) if there is a constant c > 0 and integers N, N1, . . . , Nk such that for all n ≥ N and

nr ≥ Nr, f (n, n1, . . . , nk) ≤ cg(n, n1, . . . , nk);

• f = Θ(g) if both f = Ω(g) and f = O(g);

6

• f = ω(g) if there is a constant c > 0 and integers N, N1, . . . , Nk such that for all n ≥ N and

nr ≥ Nr, f (n, n1, . . . , nk) ≥ cg(n, n1, . . . , nk) and limn,n1,...,nk→∞

(cid:12)
(cid:12)
(cid:12)

f (n,n1,...,nk)
g(n,n1,...,nk)

(cid:12)
(cid:12)
(cid:12) = ∞;

• |X| = O(f ) almost surely if, for any α > 0, there is a constant c > 0 and integers
N, N1, . . . , Nk such that for all n ≥ N and nr ≥ Nr, |X| ≤ cf (n, n1, . . . , nk) with prob-
ability at least 1 − n−α;

• |X| = O(f ) and |Y | = O(g) mutually almost surely if, for any α > 0, there is a constant
c > 0 and integers N, N1, . . . , Nk such that for all n ≥ N and nr ≥ Nr, the probability that
both |X| ≤ cf (n, n1, . . . , nk) and |Y | ≤ cg(n, n1, . . . , nk) is at least 1 − n−α.

The inter- and intra-dependence of X and the Y(r) under the joint distribution F in our
deﬁnition of the MRDPG has so far been left open. In order to establish asymptotic distributional
results for the UASE, however, we must impose certain restrictions on F. The ﬁrst of these is that:

• Each of the collections (X1, . . . , Xn) and (Y(r)

1 , . . . , Y(r)

distributions FX on X and FY,r on Yr for each r.

nr ) is marginally i.i.d., with marginal

Given such a joint distribution F, let ξ ∼ FX and υr ∼ FY,r for each r. Our next requirement

is that these marginal distributions be non-degenerate, in the sense that:

• The second moment matrices ∆X = E[ξξ(cid:62)] ∈ Rd×d and ∆Y,r = E[υrυ(cid:62)

r ] ∈ Rdr×dr for each r

are all invertible.

Our third and ﬁnal requirement is a technical condition which ensures that the growth of
the singular values of the unfolding P is regulated. We note ﬁrst that a standard application of
Hoeﬀding’s inequality shows that the spectral norm (cid:107)X(cid:62)X − n∆X (cid:107) is of order O (cid:0)n1/2 log(n)(cid:1)
almost surely, and one can argue similarly that the spectral norms (cid:107)Y(r)(cid:62)
Y(r) − nr∆Y,r(cid:107) are
of order O(n1/2
log(nr)) almost surely. Our ﬁnal requirement is that these bounds are attained
simultaneously with high probability, or in other words that:

r

• The matrices X, Y(1), . . . , Y(k) satisfy (cid:107)X(cid:62)X − n∆X (cid:107) = O(n1/2 log(n)) and, for each r,

(cid:107)Y(r)(cid:62)

Y(r) − nr∆Y,r(cid:107) = O(n1/2

r

log(nr)) mutually almost surely.

This condition is satisﬁed, for example, when X and the Y(r) are independent, or when some
or all of the Y(r) are equal (possibly to X) with probability one. Due to submultiplicativity of
the spectral norm, it also holds if we extend the latter example to allow equality up to linear
transformation.

When studying the asymptotic behaviour of graph embeddings, it is typical to impose some
control over how sparse or dense the graphs we are considering are allowed to be. This is often done
through the introduction of a sparsity factor, which is a real-valued function (depending on the size
of the graph) which is either equal to 1, or which tends to zero as the size of the graph increases
(corresponding to dense and sparse regimes respectively). This factor can then be incorporated
into the model either by scaling the latent positions themselves, or by scaling the kernel function.
In the case of a single graph which follows a GRDPG, these two options are equivalent (and for
example in [29] the sparsity factor is used to scale the latent positions Xi). For multiple graphs,
however, scaling the latent positions produces the same sparsity behaviour for all graphs, and does
not allow individual control over the density of each graph.

Our approach is to incorporate both a global sparsity function ρ and local sparsity functions
(cid:15)r into our model, where ρ applies a uniform scaling to the latent positions X and Y(r) and (cid:15)r an
individual scaling to the matrix Λr. Given a joint distribution F satisfying the above conditions,
let (ξ1, . . . , ξn, υ(1)
nk ) ∼ F, and let ρ, (cid:15)1, . . . , (cid:15)k : Z+ → [0, 1], where each such function is
either constant (and equal to 1) or tends to zero as n (and consequently the nr) tend to inﬁnity.
Given matrices Λ1, . . . , Λk, we deﬁne Λ(cid:15),r = (cid:15)rΛr for each r and Λ(cid:15) = [Λ(cid:15),1| . . . |Λ(cid:15),k], and set
k)1/2. Note that by submultiplicativity of the spectral norm, we have (cid:107)Λ(cid:15)(cid:107) ≤ (cid:15)(cid:107)Λ(cid:107),
(cid:15) = ((cid:15)2
and that we have the upper bound (cid:15) ≤ k1/2.

1 , . . . , υ(k)

1 + . . . + (cid:15)2

7

j

Y(r)

. We denote by Fρ this scaled distribution.

Uniform scaling is then overlaid onto the model by deﬁning latent positions Xi = ρ1/2ξi and
j = ρ1/2υ(r)
Throughout this paper, we will impose certain restrictions on the sparsity factors which are
necessary for our results to hold. If all the graphs are too sparsely connected, then the UASE is
unable to give us any useful information about the latent positions Xi and Y(r)
, and so we impose
the global condition that:

j

• The sparsity factor ρ satisﬁes ρ = ω

(cid:16) logc(n)
n1/2

(cid:17)

for some constant c.

As mentioned previously, we require a suﬃcient number of graphs to be dense enough for us
to be able to recover the latent positions Xi; if the only suﬃciently dense graphs are degenerate,
then it is entirely possible that we will be unable to do so. To avoid this, we impose the following
local conditions (where we assume that we have reordered the matrices A(r) appropriately):

• There exists an integer 1 ≤ κ ≤ k such that (cid:15)r = 1 for all r ≤ κ and (cid:15)r → 0 for all r > κ,

and that the matrix Λ∗ = [Λ1| . . . |Λκ] is of rank d.

This condition guarantees the existence of a suﬃciently dense subset of graphs that will allow
j ). If each Λr is

us to recover the latent positions Xi (although not necessarily the positions Y(r)
of rank d, then having κ = 1 is suﬃcient for our purposes.

2.2 Theoretical results

The main aim of this paper is to accurately describe the asymptotic behaviour of both sides of the
UASE, which we do by establishing two key results. The ﬁrst of these shows that the rows of the
left embedding approximate some invertible linear transformation (of bounded spectral norm) of
the latent positions Xi, and that by inverting this transformation we obtain a good approximation
to the latent positions themselves, in the sense that the maximum error vanishes. Similarly, we
show that the rows of the rth right embedding approximate some (not necessarily invertible) linear
transformation of the latent positions Y(r)
, and that under appropriate conditions we can invert
this transformation to obtain a good approximation to the latent positions themselves. This result
is stated using the two-to-inﬁnity norm [6] of the associated error matrix, which is the maximum
Euclidean norm of any of its rows.

i

Theorem 2 (Two-to-inﬁnity norm bound for the UASE).
Let (A, X, Y) ∼ MRDPG(Fρ, Λ(cid:15)) for a distribution F and sparsity factors ρ and (cid:15)r satisfying the
criteria stated in Section 2.1. Then there exist sequences of matrices L = L(n, n1, . . . , nk) ∈ GL(d)
and Rr = Rr(n, n1, . . . , nk) ∈ Rdr×d for each r satisfying LR(cid:62)

r = Λ(cid:15),r such that

(cid:107)XA − XL(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

, (cid:107)Y(r)

A − Y(r)Rr(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

(2)

almost surely. Moreover, we may invert the matrices L and (if Λr is of maximal rank) Rr, and
consequently ﬁnd that

(cid:107)XAL−1 − X(cid:107)2→∞ = O

(cid:16) (cid:15)1/2 log1/2(n)
ρ1/2n1/2

(cid:17)

, (cid:107)Y(r)

A R+

r − Y(r)(cid:107)2→∞ = O

(cid:16) log1/2(n)

(cid:15)1/2
r ρ1/2n1/2

(cid:17)

(3)

almost surely, where R+

r = R(cid:62)

r (RrR(cid:62)

r )−1 is the Moore-Penrose inverse of Rr.

The matrices L (and consequently Rr) can be described explicitly, and are constructed by
a two-step process; ﬁrst using a modiﬁed Procrustes-style argument to simultaneously align XA
with XP and YA with YP via an orthogonal transformation, and then applying a second linear
transformation which maps XP directly to X. For the ﬁrst step, let U(cid:62)
PVA admit the
singular value decomposition

PUA + V(cid:62)

U(cid:62)

PUA + V(cid:62)

PVA = W1ΣW(cid:62)
2 ,

(4)

8

and let W = W1W(cid:62)

2 . The matrix W solves the one mode orthogonal Procrustes problem

W = arg min
Q∈O(d)

(cid:107)UA − UPQ(cid:107)2

F + (cid:107)VA − VPQ(cid:107)2
F ,

(5)

and we use W(cid:62) to align XA with XP.

For the second step, we construct a matrix (cid:101)L which satisﬁes XP = X(cid:101)L and whose inverse can
be shown to have spectral norm of order O((cid:15)1/2) (see the Appendix for full details). The matrix
L is then given by L = (cid:101)LW. A similar process is repeated for the right-sided embedding.

The second of our main results centres on the error distribution of the estimates for the latent
established in Theorem 2, and shows that conditional

positions Xi and (if Λr is invertible) Y(r)
on the true position it is asymptotically Gaussian:

j

Theorem 3 (Central limit theorem for the UASE).
Let (A, X, Y) ∼ MRDPG(Fρ, Λ(cid:15)) for a distribution F and sparsity factors ρ and (cid:15)r satisfying
the criteria stated in Section 2.1, and let L and Rr be the transformation matrices speciﬁed in
Theorem 2.

Let ξ ∼ FX and υr ∼ FY,r for each r ∈ {1, . . . , κ}, where FX and FY,r are the marginal
distributions of F, and deﬁne ∆Y = c1∆Y,1 ⊕ . . . ⊕ cκ∆Y,κ, where the constants cr are as stated in
Section 2.1. Given x ∈ X , deﬁne

Σ(r)

Y (x) =

(cid:26) E[x(cid:62)Λrυr(1 − x(cid:62)Λrυr) · υrυ(cid:62)
r ]
E[x(cid:62)Λrυr · υrυ(cid:62)
r ]

if ρ = 1
if ρ → 0

for each r, and let ΣY = c1Σ(1)

Y ⊕ · · · ⊕ cκΣ(κ)

Y . Then, for all z ∈ Rd and for any ﬁxed i,

(cid:16)

P

n1/2(cid:0)XAL−1 − X(cid:1)(cid:62)

(cid:17)
i ≤ z | ξi = x

almost surely, where ∆Λ,Y = Λ∗∆Y Λ(cid:62)
∗ .

→ Φ(cid:0)z, ∆−1

Λ,Y Λ∗ΣY (x)Λ(cid:62)

∗ ∆−1
Λ,Y

(cid:1)

Moreover, for each r ∈ {1, . . . , κ}, if the matrix Λr is invertible, then given y ∈ Yr deﬁne
(cid:26) E (cid:2)ξ(cid:62)Λry(1 − ξ(cid:62)Λry) · ξξ(cid:62)(cid:3)
E (cid:2)ξ(cid:62)Λry · ξξ(cid:62)(cid:3)

if ρ = 1
if ρ → 0

X (y) =

Σ(r)

Then, for all z ∈ Rdr and for any ﬁxed i,

(6)

(7)

(8)

(cid:16)

P

n1/2(Y(r)

A R−1

r − Y(r))(cid:62)

i ≤ z | υ(r)

i = y

(cid:17)

→ Φ(cid:0)z, Λ−1

r ∆−1

X Σ(r)

X (y)∆−1

X Λ−(cid:62)

r

(cid:1)

(9)

almost surely.

The theorems, of which single-graph analogs were derived in [32, 4, 23, 8, 6, 33, 29, 5], also
have analogous methodological implications. Under a multilayer stochastic block model, discussed
in Section 3, the left UASE asymptotically follows a Gaussian mixture model with non-circular
components. Fitting this model is preferable to using K-means, which is implicitly ﬁtting circular
components. Apart from shape considerations, note that (as with the GRDPG) latent positions
under the MRDPG are only identiﬁable up to a distance-distorting transformation (here invertible
linear, there indeﬁnite orthogonal) to which partitions obtained using a Gaussian mixture model
are invariant but those obtained using K-means are not. Consistency in the two-to-inﬁnity norm
should imply the consistency of many subsequent statistical analyses: usually, if a method is
consistent, it remains so under a perturbation of the data of vanishing maximal error; one need
only then worry about the eﬀect of an unidentiﬁable linear transformation on conclusions; if the
estimand is invariant, this eﬀect will often vanish on account of the transformation having bounded
spectral norm.

In the special case in which the graphs A(1), . . . , A(k) are identically distributed, we can always

recover the latent positions:

9

Corollary 4. Let (A, X, Y) id∼ MRDPG(Fρ, Λ) for a distribution F and sparsity factor ρ satisfying
the criteria stated in Section 2.1. Then there exist sequences of matrices L = L(n, n(cid:48)) ∈ GL(d)
and R = R(n, n(cid:48)) ∈ Rd(cid:48)×d satisfying LR(cid:62) = Λ such that

(cid:107)XA − XL(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

, (cid:107)Y(r)

A − YR(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

(10)

almost surely. Moreover, we may invert the matrices L and (if Λ is of maximal rank) R, and
consequently ﬁnd that

(cid:107)XAL−1 − X(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

, (cid:107)Y(r)

A R+ − Y(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

(11)

almost surely, where R+ = R(cid:62)(RR(cid:62))−1 is the Moore-Penrose inverse of R.

We can similarly derive a more precise version of the Central Limit Theorem of Theorem 3:

Corollary 5. Let (A, X, Y) id∼ MRDPG(Fρ, Λ) for a distribution F and sparsity factor ρ satisfying
the criteria stated in Section 2.1, and let L be the transformation matrix speciﬁed in Corollary 4.
Let ξ ∼ FX and υ ∼ FY , where FX and FY are the marginal distributions of F. Given x ∈ X ,

deﬁne

ΣY (x) =

(cid:26) E[x(cid:62)Λυ(1 − x(cid:62)Λυ) · υυ(cid:62)]
E[x(cid:62)Λυ · υυ(cid:62)]

if ρ = 1
if ρ → 0

Then, for all z ∈ Rd and for any ﬁxed i,

(cid:16)

P

n1/2(cid:0)XAL−1 − X(cid:1)(cid:62)

i ≤ z | ξi = x

(cid:17)

→ Φ(cid:0)z, c

k Λ−(cid:62)∆−1

Y ΣY (x)∆−1

Y Λ−1(cid:1)

almost surely, where limn,n(cid:48)→∞

n(cid:48)
n = c.
Moreover, if the matrix Λr is invertible, then given y ∈ Y deﬁne

ΣX (y) =

(cid:26) E[ξ(cid:62)Λy(1 − ξ(cid:62)Λy) · ξξ(cid:62)]
E[ξ(cid:62)Λy · ξξ(cid:62)]

if ρ = 1
if ρ → 0

Then, for all z ∈ Rd(cid:48)

and for any ﬁxed i,

(cid:16)

P

almost surely.

n1/2(cid:0)Y(r)

A R−1 − Y(cid:1)(cid:62)

i ≤ z | υi = y

(cid:17)

→ Φ(cid:0)z, Λ−1∆−1

X ΣX (y)∆−1

X Λ−(cid:62)(cid:1)

(12)

(13)

(14)

(15)

If Y = X with probability one and Λ = Ip,q (the diagonal matrix whose ﬁrst p entries are equal
to 1 and remaining q entries are equal to −1) then the limiting distribution for the rows UASE are
the same as that for the ASE stated in [29], scaled by a factor of 1
k , and in particular coincides with
(cid:80)k
that obtained by spectrally embedding the average ¯A = 1
r=1 A(r) of the adjacency matrices
k
(see for example [35]).

In Corollaries 4 and 5 the matrices R are common across graphs, allowing a direct comparison
of their right-sided embeddings. By constrast, independently embedded graphs ﬁrst need to be
aligned before a meaningful comparison is possible. In [34] this is achieved using Procrustes, the
appropriate method of alignment under an RDPG model where latent positions are identiﬁable
only up to orthogonal transformation. Under the GRDPG, ﬁnding a best indeﬁnite orthogonal
alignment is less straightforward. Computational issues aside, alignment comes at a statistical cost
[21] when testing whether two point clouds diﬀer statistically, and the empirical performance of
using right-sided unfolded adjacency spectral embeddings for two-graph testing are investigated in
Section 4.4.

10

3 The generalised multilayer stochastic block model

Deﬁnition 3. (The generalised multilayer stochastic block model).
We say that the matrices A(1), . . . , A(k) are distributed as a K-community generalised multi-
layer stochastic block model for a tuple K = (K, K1, . . . , Kk) of positive integers if (A, X, Y) ∼
MRDPG(F, B) for a set of matrices B(r) ∈ [0, 1]K×Kr and a distribution F whose marginal distri-
butions FX and FY,r are supported on the sets {e1, . . . , eK} and {e1, . . . , eKr } of standard basis
vectors of RK and RKr respectively. If this is the case we write (A, X, Y) ∼ GMSBM(F, B).

If each of the Y(r) is equal to X with probability one (and consequently Kr = K for each r),
the distribution F assigns each latent position to the ith basis vector of RK with probability πi
for some tuple π = (π1, . . . , πK) whose entries sum to 1, and each of the matrices A(r) and B(r)
is symmetric, then the GMSBM reduces to the standard K-community multilayer SBM [16]. In
this case, the matrices A(r) are the adjacency matrices of a set of graphs generated independently
from a common set of vertices, in which, independently conditional on a partition of these vertices
into K disjoint communities, an edge is generated between the ith and jth vertices in the rth
graph with probability B(r)
zi,zj , where zi ∈ {1, . . . , K} denotes the community membership of the
ith vertex.

Note that, as per Proposition 1, we may consider alternative choices of latent positions (and
thus matrices Λr) for the GMSBM. We often use the second choice posited in Proposition 1,
in which we consider the singular value decomposition B = UΣV(cid:62) with U ∈ O(K) and V ∈
O((K1 + . . . + Kk) × K), and let the positions Xi be chosen from the rows of UΣ1/2 and Y(r)
be chosen from the rows of VrΣ (where we split V into k distinct blocks Vr ∈ RKr×K). In this
case, each matrix Λr is equal to the identity matrix. If k = 1, this choice closely resembles the
model presented in [29], with the signs of the eigenvalues of the matrix B being absorbed into the
latent positions Yj (if we impose the additional condition that Y is equal to X with probability
one, then Λ = Ip,q, exactly as in [29]).

j

3.1 Undirected graphs

As a ﬁrst demonstration of the UASE for the GMSBM, we consider the case in which the latent
positions Y(r) = X, and the matrices B(r) and A(r) are symmetric (that is, the standard multilayer
SBM). We begin with two examples. For the ﬁrst, we assume that the adjacency matrices A(1)
and A(2) are identically distributed according to a multilayer SBM with parameters

B =

(cid:18) 0.42
0.42

(cid:19)

0.42
0.5

, π = (0.6, 0.4)

(16)

(the Laplacian spectral embedding of a single graph generated with these parameters was studied
in [33]). Figure 1 plots the estimated latent positions for the ASE of the matrix A(1) (ﬁrst row)
and the UASE of the matrix A = [A(1)|A(2)] (second row) for n = 1000, 2000 and 4000. Also
displayed are the 95% level curves of the empirical distributions (dashed curves) and the theoretical
distributions speciﬁed by Theorem 3 (solid curves).

For the second example, we take a pair of graphs with adjacency matrices A(1) and A(2)

generated according to a multilayer SBM with parameters

B(1) =

(cid:18) 0.58
0.58

(cid:19)

0.58
0.5

, B(2) =

(cid:18) 0.42
0.42

(cid:19)

0.42
0.5

, π = (0.6, 0.4).

(17)

Since the matrices B(1) and B(2) have signatures (2, 0) and (1, 1) respectively, they exhibit diﬀerent
assortativity behaviours. Figure 2 plots the estimated latent positions for the ASE of the matrix
A(1) (ﬁrst row) and the UASE (second row) for n = 1000, 2000 and 4000, with the 95% level
curves displayed as in Figure 1.

In each example, the UASE demonstrates greater cluster separation over the ASE. To test em-
pirically whether this behaviour holds in general, we performed the following experiment: for each
value of n ∈ {50, 100, 250, 500, 750, 1000, 1500, 2000} we performed 500 trials in which we generate
two matrices B(r) with entries B(r)
ij ∼ Uniform[0, 1], and probability vectors π ∼ Dirichlet(1, 1)

11

Figure 1: Plots of the latent position estimates by the ASE and UASE of a pair of identically distributed graphs drawn
from a 2-community SBM on the same set of n nodes. Points are coloured according to the community membership of the
corresponding vertices. Ellipses give the 95% level curves of the empirical (dashed curves) and theoretical (solid curves)
distributions speciﬁed by Theorem 3.

Figure 2: Plots of the latent position estimates by the ASE and UASE of a pair of graphs drawn from a 2-community SBM
on the same set of n nodes with diﬀering distributions. Points are coloured according to the community membership of the
corresponding vertices. Ellipses give the 95% level curves of the empirical (dashed curves) and theoretical (solid curves)
distributions speciﬁed by Theorem 3.

(to ensure that both clusters were of a reasonable size, we discard vectors π for which either of
the πi was less than 0.2). Matrices A(r) were then generated according to the resulting multilayer
SBM, the UASE calculated, and nodes were then assigned to the most likely cluster predicted by
the Gaussian mixture model obtained via the MCLUST algorithm (see [31]). These were then
compared against the known cluster assignments given by the latent positions Xi, and the average

12

n = 1000ASEn = 2000n = 4000UASEn = 1000ASEn = 2000n = 4000UASEclassiﬁcation error rate calculated across all samples of a given size n (for the ASE, the average
error rate of the two embeddings was used). For added diﬀerentiation, we performed this test for
3 separate cases: one in which the matrices B(r) were identical; one in which they had a common
signature; and one in which they had diﬀerent signatures. Figure 3 displays these error rates in
the case of the identical and mixed parameter examples, as well as the average error rates for the
mean embedding—that is, the spectral embedding of the matrix ¯A = 1
2 (A(1) + A(2)), which we
denote by ASE(mean)—and, in the identically distributed case, the omnibus embedding—denoted
by OMNI (see [21]).

Figure 3: Classiﬁcation error rates for assignment of nodes to the most likely cluster predicted by the Gaussian mixture
model obtained via the MCLUST algorithm. Error rates are plotted on a logarithmic scale. See main text for details.

As expected, the UASE (black line) clearly outperforms the ASE (red line) in all cases. When
the two graphs are identically distributed, the UASE is comparable with the mean embedding
If the matrices B(r)
(blue line) and slightly outperforms the omnibus embedding (green line).
diﬀer then the UASE signiﬁcantly outperforms the mean embedding; in particular, if the matrices
B(r) have diﬀerent signatures (resulting in diﬀerent assortativity behaviours in the graphs A(r))
then the mean embedding performs worse than even the ASE. This is not particularly surprising;
if the adjacency matrices of diﬀerent graphs have diﬀerent signatures, then it is entirely possible
for the matrix ¯P to have non-maximal rank, causing some of the information in the system to be
lost when we spectrally embed the matrix ¯A. Conversely, one ﬁnds that the embedding XA is the
same as that of the positive-deﬁnite square root of the matrix (cid:80)k
r=1(A(r))2, which will always be
of maximal rank.

3.2 Directed graphs

Unlike the standard multilayer SBM, the GMSBM does not require that the matrices B(r) or A(r)
be symmetric, and so it allows us to consider directed graphs. In particular, given a single directed
graph whose adjacency matrix A ∈ {0, 1}n×n follows a GMSBM, Theorems 2 and 3 show us that
taking the standard ASE of A will provide us with consistent estimates of the latent positions X.
However, the ability of the UASE to evaluate multiple adjacency matrices presents an alterna-
tive method for working with directed graphs. Let (A, X, X) ∼ GMSBM(F, B) for some F and B,
and let Sym(A) be the hollow symmetric matrix whose upper-triangular part is equal to that of A.
Then we observe that (Sym(A), X, X) ∼ GMSBM(F, B), where we view Sym(A) as the adjacency
matrix of an undirected graph drawn on the same set of nodes as our original graph. Similarly, we
see that (Sym(A(cid:62)), X, X) ∼ GMSBM(F, B(cid:62)), and thus ( (cid:101)A, X, X ⊕ X) ∼ GMSBM( (cid:101)F, (cid:101)B), where
(cid:101)A = [Sym(A)|Sym(A(cid:62))], (cid:101)B = [B|B(cid:62)] and (cid:101)F is the natural extension of the distribution F.

We demonstrate the eﬀectiveness of this proposed method through the following experiment:
for each value of n ∈ {50, 100, 250, 500, 750, 1000, 1500, 2000}, we performed 1000 trials in which
a directed graph was generated according to a 2-community stochastic block model, where the

13

Mean error rate (log scale)Identical0500100015002000UASEASEASE(mean)OMNICommon signature0500100015002000Number of verticesMixed signature1e−061e−051e−041e−031e−021e−011e+000500100015002000(not necessarily symmetric) probability matrix B and community probabilities π were randomly
generated as before. The adjacency matrix A was then constructed, and the ASE of A, the UASE
of (cid:101)A, and the mean embedding we calculated. Using each of these, we assign nodes to their most
likely cluster using the MCLUST algorithm as before, and calculate the mean error rate across all
the trials of a given sample size. The results (in the form of the average classiﬁcation error rate) are
plotted in Figure 4, and indicate that on average using the UASE oﬀers signiﬁcant improvement
over the ASE, and a minor improvement over the mean embedding (we note however, that this
does not guarantee that the UASE will always oﬀer the best performance).

Figure 4: Classiﬁcation error rates for assignment of nodes from a directed graph to the most likely cluster predicted by
the Gaussian mixture model obtained via the MCLUST algorithm. Error rates are plotted on a logarithmic scale. See main
text for details.

w

3.3 Bipartite multilayer SBMs

The ﬂexibility of the GMSBM means that we do not have to restrict our attention to a standard
multiple graph embedding; we may restrict our attention to submatrices of the adjacency matrices
A(r) for each graph in order to focus only on the interactions involving a given set of nodes. As
an illustrative example, consider the “bipartite” situation in which we have a GMSBM with two
sets of latent positions Xi ∈ R2 and Yj ∈ R3, with probability matrices

B(1) =

(cid:18) 0.45
0.51

0.45
0.51

(cid:19)

0.52
0.54

, B(2) =

(cid:18) 0.46
0.42

0.51
0.47

(cid:19)

0.43
0.52

(18)

and community membership probabilities πX = (1/2, 1/2) and πY = (1/3, 1/3, 1/3).

Figure 5 plots the left embedding XA and the right embeddings Y(r)

A for the pairs (n, n(cid:48)) =
(1000, 1500), (2000, 3000) and (4000, 6000), where we use the latent positions (cid:101)X, (cid:101)Y(r) ∈ R2 posited
in Proposition 1 This demonstrates an important point: while Theorems 2 and 3 guarantee that
the embeddings Y(r)
A provide consistent estimates of the latent positions (cid:101)Y(r), we cannot guarantee
that they will distinguish between diﬀerent communities, as the rank of (cid:101)Y(r) is equal to the rank
of B(r), which may be less than the rank of rank(Y). In our example, the embedding Y(1)
A fails to
distinguish between all three communities, while Y(2)
A does distinguish between them. In general,
if the columns of the matrix B(r) are distinct, then the latent positions corresponding to diﬀerent
communities will be distinct.

14

Error detection rate (log scale)Number of vertices05001000150020000.00010.001 0.01  0.1   UASEASEASE(mean)Figure 5: Plots of the latent position estimates by the UASE of a pair of graphs drawn from a (2, 3)-community GMSBM.
Points are coloured according to the community membership of the corresponding vertices. Ellipses give the 95% level
curves of the empirical (dashed curves) and theoretical (solid curves) distributions speciﬁed by Theorem 3.

3.4 Rank considerations

One advantage of studying the MRDPG is that we do not require the matrices Λr to have max-
imal rank; this can lead to situations in which information about latent positions is obscured in
individual graphs, but becomes apparent when considering the joint embedding. As an example,
consider a dynamic network which can be modeled as a two graph 3-community multilayer SBM,
in which the matrices B(r) of probabilities take the form

B(1) =





p1
p1
q1

p1
p1
q1

q1
q1
r1


 , B(2) =





p2
q2
q2

q2
r2
r2





q2
r2
r2

(19)

for values pi, qi, ri ∈ [0, 1]. We could view this as a simple model for time-dependent snapshots
of the communication preferences between two departments in a company, in which a third team
moves from the ﬁrst department to the second in between snapshots, and inherits the communica-
tion preferences of the department to which they are assigned at the time. The matrices B(r) are
both of non-maximal rank, but B = [B1|B2] has maximal rank, provided that the pi, qi and ri are
distinct.

We demonstrate this with an example. Let n = 4000 and suppose that we have three com-
munities, containing 1750, 500 and 1750 nodes respectively. Let the probability matrices be given
by

B(1) =





0.47
0.47
0.39

0.47
0.47
0.39

0.39
0.39
0.56


 , B(2) =





0.53
0.61
0.61

0.61
0.44
0.44



 .

0.61
0.44
0.44

(20)

Figure 6 shows the embedded point clouds generated by the individual ASEs and the UASE in

15

n = 1000Left UASEn = 2000n = 40001st right UASE2nd right UASEthis situation. As one would expect, the individual ASEs display only the two communities that one
observes at that given snapshot in time, while the UASE clearly displays all three communities..

Figure 6: Plots of the ASEs of the adjacency matrices A1 and A2 and the UASE, with nodes coloured according to
community membership. Points are coloured according to the community to which their corresponding nodes belong, with
points in black representing those nodes that switch between communities.

4 Multiple graph inference: comparison with existing meth-

ods

The performance of unfolded adjacency spectral embedding is now compared with alternative
spectral approaches on the inference tasks of latent position recovery, subspace estimation, model
estimation, and two-graph hypothesis testing. We restrict our attention to the case in which the
matrices A(r) are true adjacency matrices of graphs, and thus work under the assumption that the
latent position matrices Y(r) are equal to X with probability one for all r.

4.1 Recovery of latent positions

An important estimation problem for the data of a random dot product graph is that of estimating
the latent positions Xi, and so we shall investigate the performance of the MRDPG in the context
of such an estimation problem. For comparison, we will consider the multiple adjacency spectral
embedding (MASE) [3], which is an alternative method of jointly embedding adjacency matrices
which follow a model that is essentially identical to the MRDPG, known as the common subspace
independent edge graph model.
In [3], the authors demonstrate that the MASE yields state-of-
the-art performance on subsequent inference tasks, ahead of other competing models for studying
multiple graph embeddings such as the multi-RDPG [26] and MREG [39] models, making it an
ideal method to compare the UASE against.

Deﬁnition 4. (Common Subspace Independent Edge graphs).
Let U = [U1| · · · |Un](cid:62) ∈ Rn×d have orthonormal columns, and let R(1), . . . , R(k) ∈ Rd×d be
symmetric matrices such that, for each r ∈ {1, . . . , k}, U(cid:62)
i R(r)Uj ∈ [0, 1] for all i, j ∈ {1, . . . , n}.
The random adjacency matrices A(1), . . . , A(k) are said to be jointly distributed according to the
common subspace independent-edge graph model with bounded rank d and parameters U and
(cid:1),
ij ∼ Bern(cid:0)P(r)
R(1), . . . , R(k) if for each r ∈ {1, . . . , k}, conditional upon U and R(r) we have A(r)
ij
where P(r) = UR(r)U(cid:62), in which case we write (A(1), . . . , A(k)) ∼ COSIE(U; R(1), . . . , R(k)).

For all intents and purposes, the COSIE and MRDPG models are equivalent. Any COSIE
model gives rise to a MRDPG by simply setting the latent positions Xi to be equal to the rows
Ui, and the matrices Λr = R(r) for each r. Conversely, given a MRDPG such that the matrix
X of latent positions is of rank d, we can deﬁne U = X(X(cid:62)X)−1/2, where we have taken the
positive-deﬁnite matrix square root of the matrix X(cid:62)X.
It is clear that the columns of U are
orthonormal, and we obtain a COSIE model by setting R(r) = (X(cid:62)X)1/2Λr(X(cid:62)X)1/2. In both
cases the two deﬁnitions of the matrices P(r) coincide.
Deﬁnition 5. (Multiple adjacency spectral embedding).
Let (A(1), . . . , A(m)) ∼ COSIE(U; R(1), . . . , R(k)). For each r ∈ {1, . . . , k} let dr denote the rank

16

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllASE_1llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllASE_2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllUASEof R(r), let XA(r) ∈ Rn×dr be the adjacency spectral embedding of A(r) and deﬁne the matrix
of concatenated spectral embeddings MA = [XA(1)| · · · |XA(k) ]. The multiple adjacency spectral
embedding of A(1), . . . , A(k) is the matrix (cid:98)UA ∈ Rn×d containing the d leading left singular vectors
of MA.

We note that it is possible to use the MASE to produce estimates for the latent positions in
a MRDPG in an analogous way to the method used for the UASE. Let (cid:98)XA = (cid:98)UAΣA, where
ΣA ∈ Rd×d is the diagonal matrix of the leading d singular vectors of MA, and deﬁne MP, (cid:98)UP
and (cid:98)XP analogously for the matrices P(r). Firstly, we note that adding columns of zeros to any
of the XP(r) will not alter (cid:98)XP, and so we may assume without loss of generality that the matrices
XP(r) ∈ Rn×d, and thus that there exist matrices L(r) ∈ Rd×d of rank dr such that XP(r) = XL(r).
One can prove the existence of a matrix L ∈ GL(d) such that (cid:98)XP = XL, and so performing a
Procrustes-style alignment between (cid:98)XA and (cid:98)XP and multiplying by L−1 produces a set of points
that in practice are a good approximation to the latent positions Xi.

We ﬁrst tested the performance of the two embeddings on graphs of diﬀerent sizes by performing,
for each value of n ∈ {10, 25, 50, 75, 100, 250, 500, 750, 1000}, 1000 independent trials in which the
latent positions Xi are drawn i.i.d. from a Dirichlet distribution with parameter (1, 1, 1)(cid:62) ∈ R3.
In each trial, we generate two graphs A(1), A(2) ∈ Rn×n, where A(r)
i ΛrXj) for i < j,
where each Λr is a randomly chosen matrix. We then calculate estimates (cid:98)X using the UASE and
MASE as described previously, and compare the average mean squared error 1
i=1 | (cid:98)Xi − Xi|2 of
n
the two embeddings across the 1000 trials.

ij ∼ Bern(X(cid:62)

(cid:80)n

We then investigated the eﬀect of changing the number of graphs to be embedded on the
accuracy of each embedding type. Fixing n = 750, we again performed 1000 independent trials as
above for m = 2, . . . , 10 embeddings, using the same procedure for generating the latent positions
and adjacency matrices, and again compared the average mean squared error between the estimated
and actual latent positions.

Figure 7 plots the results of the two experiments. While the MASE outperforms the UASE for
values of n < 75 (with the joint embedding performing signiﬁcantly worse for n < 50) the UASE
clearly demonstrates superior accuracy as the size of the graph grows, a trend which continues as
we increase the number of graphs to be embedded.

Figure 7: Mean squared error in recovery of latent positions in a 2-graph MRDPG model as a function of the number of
vertices (left-hand graph) and the number of embeddings (right-hand graph).

4.2 Estimation of invariant subspaces

We next investigate the performance of UASE at estimating the invariant subspace U in the COSIE
model. We do this by setting the matrix X of latent positions in the MRDPG to be equal to U,

17

lllllllllMean squared errorNumber of vertices0.00.20.40.60.81.01.202505007501000lllllllllUASEMASElllllllllNumber of embeddings0.000.050.100.150.20246810llllllllland considering the unscaled UASE, UA. Unlike the scaled embedding, which approximates the
latent positions only up to linear transformation, the unscaled UASE approximates the invariant
subspace U up to orthogonal transformation. Indeed, from our results for the scaled embedding
the matrix UP = UQX for some QX ∈ GL(d), whence the requirement that both U and UP have
orthonormal columns forces QX to in fact belong to O(d), while the transformation applied in the
Procrustes alignment between UA and UP is by deﬁnition orthogonal.

We can measure the distance between the estimate UA and the true invariant subspace U using
A −UU(cid:62)(cid:107) (and similarly for the
the spectral norm of the diﬀerence between the projections (cid:107)UAU(cid:62)
estimate (cid:98)U produced by the MASE). This distance is zero only when there exists an orthogonal
matrix W ∈ O(d) such that UA = UW (respectively (cid:98)U = UW).

As in the previous example, we investigated the eﬀect of changing both the size of the graphs and
the number of graphs to be embedded on the performance of the UASE and MASE. Again, we began
by performing 1000 independent trials for each value of n ∈ {10, 25, 50, 75, 100, 250, 500, 750, 1000},
but this time the adjacency matrices A(1) and A(2) were distributed according to a 3−community
multilayer stochastic block model, where the matrices B(r) were randomly chosen, and vertices
assigned to a community uniformly at random, discarding any trials for which the matrix X of
community assignments was not of full rank. We then calculated and compared the average of
the subspace distances (cid:107)UAU(cid:62)
A − UU(cid:62)(cid:107) and (cid:107) (cid:98)U (cid:98)U(cid:62) − UU(cid:62)(cid:107) across each of the 1000 trials. For
the second experiment, we again ﬁxed n = 750, performed 1000 independent trials as above for
k = 2, . . . , 10 embeddings, and compared the subspace distance between the estimated and actual
invariant subspaces.

Figure 8: Average distance between the estimated and actual invariant subspaces in a 3-community multilayer stochastic
block model as a function of the number of vertices (left-hand graph) and the number of embeddings (right-hand graph).

Figure 8 plots the results of the two experiments. For this task, although the performance
of the two embedding types is almost indistinguishable for very small graphs, as the number of
vertices grows the UASE consistently outstrips the MASE. As in the previous example, increasing
the number of embedded graphs results in greater accuracy for both methods, where again the
UASE oﬀers the best performance of the two.

4.3 Model estimation

As a ﬁnal comparison of the UASE and MASE methods, we investigate the eﬃciency of both at
the task of estimating the underlying matrices P(r) in the MRDPG and COSIE models, which is
of particular practical interest for link prediction tasks. To establish an appropriate estimate, we
ﬁrst consider the case of the standard GRDPG (that is, when k = 1). In this case, an estimate
(cid:98)P for the matrix P can be obtained by setting (cid:98)P = XAIp,qX(cid:62)
A. Note that due to orthogonality

18

lllllllllSubspace distanceNumber of vertices0.20.40.60.802505007501000lllllllllUASEMASElllllllllNumber of embeddings0.040.080.120.160.20246810lllllllllof the singular vectors, the matrix XA ∈ Rn×d of the leading d left singular vectors of A is the
projection of the full matrix of left singular vectors onto the d-dimensional subspace spanned by
UA. Since this projection corresponds to left multiplication by the matrix UAU(cid:62)
A, we have the
alternative description (cid:98)P = UAU(cid:62)

AAUAU(cid:62)
A.
Returning to the general case, we obtain an estimate (cid:98)P(r) = UAU(cid:62)

A for the ma-
trix P(r) for each r ∈ {1, . . . , k} using the unscaled UASE. For the MASE, we use the matrix
(cid:98)U (cid:98)U(cid:62)A(r) (cid:98)U (cid:98)U(cid:62) as our estimate. For each of the trials in the previous example, we calculated
these estimates, and measured the model estimation error in each case using the normalised mean
squared error

AA(r)UAU(cid:62)

(cid:107) (cid:98)P(r) − P(r)(cid:107)F
(cid:107)P(r)(cid:107)F

.

(21)

Figure 9 plots the results of the two experiments, in which we see that once again the UASE
consistently demonstrates greater accuracy than the MASE for all but the smallest of graphs, and
for all numbers of embedded graphs.

Figure 9: Model estimation error for the UASE and MASE in a 3-community multilayer stochastic block model as a
function of the number of vertices (left-hand graph) and the number of embeddings (right-hand graph).

4.4 Two-graph hypothesis testing

When A(1), . . . , A(k) are identically distributed, the right embeddings Y(r)
A are identically dis-
tributed too and each subject to the same unidentiﬁable linear transformation (Corollaries 4 and 5).
It is therefore natural to consider the eﬀectiveness of the UASE at testing the semiparametric hy-
pothesis that two observed graphs are drawn from the same underlying latent positions. This
problem was considered for the omnibus embedding in [21], and we shall use the framework estab-
lished there to test the UASE. Suppose, then, that we have points X1, . . . , Xn, Y1, . . . , Yn ∈ Rd,
and that we have two graphs G1 and G2 whose adjacency matrices A(1) and A(2) satisfy A(1)
ij ∼
Bern(X(cid:62)

i Ip,qYj). The UASE allows us to test the hypothesis:

i Ip,qXj) and A(2)

ij ∼ Bern(Y(cid:62)

H0 : Xi = Yi ∀i ∈ {1, . . . , n}

(22)

by comparing the right embeddings Y(1)
A are
identically (although not independently) distributed, whereas if H0 fails to hold then for some k
the kth row of Y(1)

A . If H0 holds, then the rows of the Y(r)

A should be distributionally distinct.

A and Y(2)

A and Y(2)

The framework used in [21] to test this hypothesis, which we shall repeat here, is as follows:
we begin by drawing X1, . . . , Xn, Z1, . . . , Zn ∈ R3 identically according to a Dirichlet distribution

19

lllllllllModel estimation errorNumber of vertices0.00.10.20.30.40.50.60.702505007501000lllllllllUASEMASElllllllllNumber of embeddings0.020.030.040.050.060.07246810lllllllllwith parameter α = (1, 1, 1)(cid:62), select a subset I of some ﬁxed size uniformly at random among all
such subsets of {1, . . . , n}, and deﬁne

Yi =

(cid:26) Zi
Xi

if i ∈ I
otherwise

(23)

We generate two graphs G1 and G2 with adjacency matrices A(1) and A(2) satisfying A(1)

ij ∼
Bern(X(cid:62)
i Yj), and estimate the latent positions (cid:98)X and (cid:98)Y of the two
graphs by using the right embeddings as described above, and (in the case of the omnibus embed-
ding) the ﬁrst and last n rows of the spectral embedding of the matrix

i Xj) and A(2)

ij ∼ Bern(Y(cid:62)

(cid:18)

M =

A(1)
2 (A(1) + A(2))

1

1

2 (A(1) + A(2))
A(2)

(cid:19)

.

(24)

We note that this is only possible due to our prior knowledge of the matrix P, which allows us to
construct the required transformations.

In both cases we use the test statistic T = (cid:80)n

i=1 (cid:107) (cid:98)Xi − (cid:98)Yi(cid:107)2; and accept or reject based on an
estimate of the critical value of T under the null hypothesis obtained by using 2000 Monte Carlo
iterates to estimate the distribution of T .

Figure 10: Empirical power of the UASE (black) and omnibus (red) tests to detect when the two graphs being tested diﬀer
in the speciﬁed number of their latent positions. Each point is the proportion of 2000 trials for which the given technique
correctly rejected the null hypothesis.

Figure 10 shows the power of each method for testing the null hypothesis for diﬀerent sized
graphs and for diﬀerent numbers of altered latent positions, by calculating the proportion (out of
2000 trials conducted for each sized graph) of trials for which we correctly reject the null hypothesis.
For smaller graphs, the omnibus embedding provides the most eﬀective method (although there
is not much diﬀerence between the two where only one latent position is altered - however in this
case the empirical power of both methods does not exceed 0.25). For larger graphs, particularly
those with more than 500 vertices, the two methods are almost indistinguishable, and in such cases
the UASE might be preferred based on size considerations, due to only requiring an n × kn rather
than an kn × kn matrix.

20

llllllllllllllllll05001000UASEOMNI1 latent position alteredEmpirical powerllllllllllllllllll05001000Number of vertices5 latent positions alteredllllllllllllllllll050010000.00.20.40.60.81.010 latent positions altered5 Real data: Link prediction on a computer network

5.1 Dynamic link prediction

The Los Alamos National Laboratory computer network [37] was studied in [29], in which it was
demonstrated empirically that the disassortative connectivity behaviour inherent in the network
leads to the GRDPG oﬀering a marked modelling improvement over the RDPG in the task of out-
of-sample link prediction between computers in the network. For a large-scale dynamic network
such as this, the MRDPG oﬀers the possibility of further reﬁnement by allowing us to consider
multiple “snapshots” of communication behaviour at diﬀerent points in time simultaneously.

As an example, we extract a ten minute sample at random from the “Network Event Data”
dataset, which we divide into two separate ﬁve minute samples. From the ﬁrst sample we generate
ﬁve graphs, each one describing the communication behaviour of the computers in the network over
a period of one minute, by assigning each IP address to a node (with this assignation being kept
consistent across all graphs), and recording an edge between two nodes if the corresponding edges
are observed to communicate at least once within this period, and then construct the corresponding
adjacency matrices A(r). Setting our embedding dimension d = 10 (an admittedly arbitrary choice)
we then generate estimates (cid:98)P(r) for the probability matrices as described in Section 4.3. We then
use the average of these matrices to give us estimates of the probabilities of a link being generated
between any given pair of computers.

In a similar manner, we generate estimates of the link probabilities for the mean adjacency
matrix ¯A. We also construct adjacency matrices A[1] and A[5] from the connectivity graphs for
the ﬁrst minute and the full ﬁve minute period respectively (both of which follow a standard
GRDPG) and generate link probability estimates accordingly.

Figure 11: Receiver Operating Characteristic curves for the UASE (black), mean embedding (red) and ASE (blue and
green) methods for out-of-sample link prediction on the Los Alamos National Laboratory computer network. See main text
for details.

Using these estimates, we attempt to predict which new edges will occur within the second ﬁve
minute window, disregarding those involving new nodes. Figure 11 shows the receiver operating
characteristic (ROC) curves for each model and for each port, where we treat the prediction task
as a binary classiﬁcation problem whose outcomes are either the presence or absence of an edge
between nodes, which we predict by thresholding the estimated link probabilities. As one would
expect, using the ASE of the graph corresponding to only a single minute of communication (the
blue curve) produces the least accurate predictions, but the UASE (black), mean embedding (red)
and the ASE for a ﬁve minute sample (green) all produce similar results, with the UASE slightly
outperforming the other two methods. This can be conﬁrmed numerically by calculating the area

21

0.00.20.40.60.81.00.00.20.40.60.81.0True positive rateFalse positive rateDynamic link predictionUASEASE(mean)ASE (1 minute)ASE (5 minutes)under each ROC curve (AUC) which is equivalent to the probability that a given classiﬁer will
rank a randomly chosen positive instance higher than a randomly chosen negative instance [11].
We calculate that the UASE has an AUC of 0.9734, which is a small improvement over the mean
embedding and the 5-minute ASE, which have AUC values of 0.9627 and 0.9635 respectively (the
1-minute ASE, by contrast, yields an AUC of 0.8767).

5.2 Port-speciﬁc link prediction

The LANL network data presents the opportunity to demonstrate another signiﬁcant improvement
oﬀered by the MRDPG over the GRDPG, namely its ability to integrate data from sources which
do not necessarily behave similarly. Each communication within the LANL network passes through
a source and destination port, the latter of which indicates the type of service being used, and it
is natural to expect that diﬀerent services may exhibit diﬀerent communication behaviours.

We consider the ﬁrst ﬁve minute sample from the previous section. During the ﬁrst minute
alone, there are a total of 121,737 (not necessarily unique) communications between 10,762 com-
puters, with 4,379 diﬀerent destination ports being used. Of these, the 8 most commonly-used
ports account for over 75% of communications, and Table 1 lists these, together with the purpose
to which each port is assigned.

Table 1: Purpose and proportion of traﬃc utilizing the 8 most frequently used ports during 1 minute of activity on the
Los Alamos National Laboratory computer network.

Port
53
443
80
514
389
427
88
445

Purpose
DNS
HTTPS
HTTP
Syslog
Lightweight Directory Access Protocol
Service Location Protocol
Kerberos authentication system
Microsoft–DS Active Directory

Proportion of traﬃc
27.7%
14.8%
11.9%
7.2%
4.3%
4.1%
3.4%
1.8%

For each of these 8 ports, we generate a graph of the communications made between computers
within the network through this speciﬁc port over the ﬁrst minute of our sample as in the previous
example. Figure 12 visualizes the adjacency matrix of each of these graphs as a 2-dimensional plot,
together with the adjacency matrix of the full network graph.

As before, we calculate estimates of the link probabilities for each port using the UASE, but
now rather than averaging them we consider each port individually. For comparison, we also
estimate link probabilities for each individual port using the corresponding GRDPG, and then
use both estimates to attempt to predict which new edges will occur within the remainder of our
ﬁve-minute window. Figure 13 shows the ROC curves for each model and for each port, while
Table 2 gives the AUC values for each curve. We note that for Port 53 (the busiest port) the
standard ASE actually outperforms the UASE, but for every other port the UASE is the superior
method, oﬀering a signiﬁcant improvement over the ASE for the less active ports.

Table 2: AUC values for the ROC curves in Figure 13.

Embedding
UASE
ASE

Port 53
0.8391
0.8568

Port 443
0.7850
0.6370

Port 80
0.9010
0.7185

Port 514
0.8931
0.7806

Port 389
0.8534
0.5532

Port 427
0.8580
0.6566

Port 88
0.8949
0.5668

Port 445
0.9836
0.5720

5.3 Link prediction using mixed data sources

Our ﬁnal example demonstrates how combining data from graphs with diﬀerent nodes can inform
our knowledge of a common subset. We begin by extracting a ﬁve minute sample at random
from the “Network Event Data” dataset similarly to Section 5.1, and construct the ASE of the
adjacency matrix A(1) obtained by restricting our attention to the ﬁrst minute of computer-to-
computer communication. For the UASE, we augment this data by constructing the submatrix
A(2) of computer-to-port communications, in which A(2)
ij = 1 if there is a connection involving

22

Figure 12: Visualization of adjacency matrices showing connections between computers on the Los Alamos National Labo-
ratory computer network during 1 minute of activity. The top-left image shows all connections during this time, while the
remaining images show only connections via the speciﬁed port.

the ith computer during this ﬁrst minute for which the jth port is the destination port. We then
generate estimates of the link probabilities for the ASE of A(1) and UASE of A = [A(1)|A(2)], and
generate link probability estimates accordingly. Figure 14 plots the resulting ROC curves, in which
we observe that augmenting the computer-to-computer connectivity data with computer-to-port
connectivity data yields an improvement in prediction power (similarly, we note the AUC values
of 0.949 for the UASE compared to 0.905 for the ASE).

6 Chernoﬀ information and the GMSBM

Given a collection of matrices A(r) that are distributed according to a GMSBM, it is reasonable
to ask whether there is any tangible beneﬁt to studying the UASE as opposed to the ASEs of
the individual matrices, and how one might quantify this. Tang and Priebe [33], in the context of
comparing the performance of the spectral embeddings of the Laplacian and adjacency matrix in
recovering block assignments from a stochastic block model graph, proposed using the Chernoﬀ
information [14] of the limiting Gaussian distributions obtained from the Central Limit Theorem
associated to each embedding as a means of doing so.
In a two-cluster problem, the Chernoﬀ
information is the exponential rate at which the Bayes error (from the decision rule which assigns
each data point to its most likely cluster a posteriori ) decreases asymptotically. The Chernoﬀ
information is an example of a f -divergence [2], [9] and therefore possesses the desirable attribute

23

All portsPort 53Port 443Port 80Port 514Port 389Port 427Port 88Port 445Figure 13: Receiver Operating Characteristic curves for the UASE (black) and ASE (red) for out-of-sample link prediction
via the speciﬁed port on the Los Alamos National Laboratory computer network. See main text for details.

of being invariant under invertible linear transformations [22].

If F1 and F2 are two absolutely continuous multivariate distributions supported on Ω ⊂ Rd,
with density functions f1 and f2 respectively, the Chernoﬀ information between F1 and F2 is
deﬁned by C(F1, F2) = sup
t∈(0,1)

Ct(F1, F2) [14] where the Chernoﬀ divergence

Ct(F1, F2) = − log(cid:0)

(cid:90)

Ω

f1(x)tf2(x)1−tdx(cid:1).

(25)

For a K-cluster problem, in which we have distributions F1, . . . , FK with corresponding density

functions f1, . . . , fK, we consider the Chernoﬀ information of the critical pair min
i(cid:54)=j

C(Fi, Fj).

If the Fi are multivariate normal distributions, it is known (see [27]) that the Chernoﬀ infor-

mation can be expressed as C(Fi, Fj) = sup
t∈(0,1)

Ct(Fi, Fj), where

Ct(Fi, Fj) =

(cid:16) t(1 − t)
2

(xi − xj)(cid:62)Σ−1

t (xi − xj) + 1

2 log

(cid:16)

|Σt|
|Σi|t|Σj|1−t

(cid:17)(cid:17)

,

(26)

where Fi ∼ N (xi, Σi) and Σt = tΣ1 + (1 − t)Σ2.

We now apply this to obtain an expression for the Chernoﬀ information of the left UASE of a

24

Port 53UASEASEPort 443UASEASEPort 80UASEASEPort 514UASEASEPort 389UASEASEPort 427UASEASEPort 88UASEASEPort 445UASEASEFigure 14: Receiver Operating Characteristic curves for the UASE (black) and ASE (red) methods for out-of-sample link
prediction on the Los Alamos National Laboratory computer network, in which the UASE is augmented with computer-
to-port connectivity data. See main text for details.

GMSBM. Let (A, X, Y) ∼ GMSBM(F, B), and deﬁne CA = min
i(cid:54)=j

sup
t∈(0,1)

Ct (Fi, Fj), where

Fi ∼ N (cid:0)ei, 1

n ∆−1

B,Y BΣY (ei)B(cid:62)∆−1

B,Y

(cid:1)

(27)

and ∆B,Y and ΣY (ei) are as deﬁned in Theorem 3 (for simplicity, we will assume that our graphs
have the same sparsity factor).

Some standard algebraic manipulation shows that this quantity is invariant under the trans-
formations of the latent positions listed in Proposition 1 (and thus the Chernoﬀ information of
the underlying MRDPG is well-deﬁned), and similarly that it is invariant under invertible linear
transformations of the UASE XA. Thus we may study XA rather than the estimate XAL of the
latent positions, which requires knowledge - that we will not typically possess - of the underlying
matrices of probabilities P(r). We note that we can similarly deﬁne the Chernoﬀ information for
the right UASEs (where they are deﬁned) and that this too would be invariant under invertible
transformations of the latent positions.

Given a collection A(1), . . . , A(k) of matrices and a subset K of {1, . . . , k}, let AK denote the
matrix obtained by concatenating the matrices A(r) for r ∈ K. If we have a collection of matrices
A(r) which are identically distributed as a GMSBM, the following result shows that it is always
preferable to embed as many matrices as possible:

Proposition 6. Let (A, X, Y) id∼ GMSBM(F, B) be identically distributed as a GMSBM. Then
CA ≥ CAK for any subset K of {1, . . . , k}.
Proof. Let Σi = B−(cid:62)∆−1
Limit Theorem for the single graph ASE given in Corollary 5 for our particular case. Then

Y B−1 denote the covariance matrix present in the Central

Y ΣY (ei)∆−1

CA = min
i(cid:54)=j

(cid:16) kt(1 − t)
2c

sup
t∈(0,1)

(ei − ej)(cid:62)Σ−1

i,j,t(ei − ej) + 1

2 log

(cid:16)

|Σi,j,t|
|Σi|t|Σj|1−t

(cid:17)(cid:17)

where Σi,j,t = tΣi + (1 − t)Σj, while for a subset K of size r we have

CAK = min
i(cid:54)=j

sup
t∈(0,1)

(cid:16) rt(1 − t)
2c

(ei − ej)(cid:62)Σ−1

i,j,t(ei − ej) + 1

2 log

(cid:16)

|Σi,j,t|
|Σi|t|Σj|1−t

(cid:17)(cid:17)

.

(28)

(29)

25

0.00.20.40.60.81.00.00.20.40.60.81.0True positive rateFalse positive rateUASEASESince the matrix Σi,j,t is positive semi-deﬁnite for any i and j, the ﬁrst term in each pair of

brackets is positive, and thus the term CA clearly dominates.

If the adjacency matrices A(r) are not identically distributed, however, the situation is not so
clear-cut, as it is entirely possible to encounter situations for which CAK > CA for some subset K.
For example, if we consider the two-graph multilayer stochastic block model with matrices

B(1) =

(cid:18) 0.67
0.46

(cid:19)

0.46
0.36

, B(2) =

(cid:18) 0.98
0.49

(cid:19)

,

0.49
0.10

(30)

then while the ratio CA/CA(1) tends to 11.98, the ratio CA/CA(2) tends to 0.96.

Before proceeding further, we note that any analysis of the Chernoﬀ information for large-scale
GMSBMs can be simpliﬁed by observing that the logarithmic term in the deﬁnition is independent
of the number of vertices n, and so becomes insigniﬁcant as n → ∞ if we impose the simplifying
assumption that the covariance matrices be non-singular. To this end, we consider instead the
truncated terms ρA, in which we simply omit the logarithmic term from the deﬁnition of CA.
Considering the function ρA gives an accurate means of comparison between the large-scale be-
haviour of two multilayer stochastic block models, as the ratio CA/CAK tends to ρA/ρAK as n
increases for any subset K.

To observe the eﬀect of embedding additional graphs on the Chernoﬀ information of a GMSBM,
we conducted the following experiment: for each k ∈ {2, . . . , 10}, we performed 1000 trials in which
a (2, 2, . . . , 2)-community GMSBM was generated by choosing k matrices B(r) ∈ [0, 1]2×2 and a
probability vector π at random (with the entries B(r)
ij ∼ Uniform[0, 1], while π ∼ Dirichlet(1, 1).
We then calculated the ratio ρA/ρAK for each subset K of {1, . . . , k} under the assumption that
the vectors Y(r) were identically distributed. We then repeated the experiment for a (2, 2, . . . , 2)-
community GMSBM in which we allowed diﬀerent probability vectors πr for each embedding, and
ﬁnally repeated both experiments for a (2, 3, . . . , 3)-community GMSBM.

The results of these simulations are presented in Figure 15. Entries above the diagonal (in blue)
correspond to trials in which the parameters πr are equal, while entries below the diagonal (in
red) correspond to trials in which the parameters πr are allowed to diﬀer; the (r, k)th and (k, r)th
coordinates in the respective cases indicate the proportion of embeddings of k matrices for which
ρA/ρAK > 1 for every r-element subset K.

Figure 15: Proportion of GMSBMs for which ρA/ρAK > 1 for all subsets K of a given size. See main text for details.

We make the following observations:
• On average, embedding more matrices seems to improve performance, with ρA/ρAK > 1
for at least 73% of the trials we conducted, and this improvement in performance seems to
increase as we allow the latent positions Y(r) to be taken from more communities.

• In general, ρA/ρAK > 1 more often the greater the diﬀerence in size between the subset K

and the total number of embeddings k.

• We detected little distinguishable diﬀerence between allowing the parameters πr to diﬀer as

opposed to keeping them all the same.

26

1234567891012345678910(2,2,...,2)−community GMSBM1234567891012345678910(2,3,...,3)−community GMSBM10.73• In each trial, we noted that there is always at least one subset K of any given size for which
ρA/ρAK > 1, and we conjecture that this should always hold. If true, this would mean in
particular that the UASE is always better than the worst of our embeddings (as opposed to,
say, the mean embedding, which we have seen can lead to degeneracy when the matrices P(r)
have diﬀering signatures).

7 Conclusion

The multilayer random dot product graph is a vast yet natural extension of the random dot
product graph, granting us insight into the behaviour of a common subset of nodes across a series
of graphs—both undirected and directed—in which we allow a mixture of assortativity behaviours.
Its simplicity and ﬂexibility make it an ideal model for a variety of situations, and it can be seen
to perform as well as (and in many cases better than) existing models at multiple graph inference
tasks such as community detection and graph-to-graph comparison, while allowing inference in
a much wider range of situations than current methods permit. These experimental results are
supported by theoretical results showing that the node representations obtained by the left- and
right-sided spectral embeddings converge uniformly in the Euclidean norm to the latent positions
with Gaussian error, in particular providing us with the ﬁrst known examples of such results for
bipartite graphs. Finally, we demonstrate the practical eﬀectiveness of our model by applying it
to the task of link prediction within a computer network, indicating its usefulness to the ﬁeld of
cyber-security.

Bibliography

[1] L. Akoglu and C. Faloutsos. Anomaly, event, and fraud detection in large network datasets.
In Proceedings of the sixth ACM international conference on Web search and data mining,
pages 773–774, 2013.

[2] S. M. Ali and S. D. Shelvey. A general class of coeﬃcients of divergence of one distribution

from another. Journal of the Royal Statistical Society: Series B, 28:121–132, 1966.

[3] J. Arroyo, A. Athreya, J. Cape, G Chen, C. E. Priebe, and J. T. Vogelstein.

Inference
for multiple heterogeneous networks with a common invariant subspace. arXiv preprints
arXiv:1906.10026, 2019.

[4] A. Athreya, C. E. Priebe, M. Tang, V. Lyzinski, D. J. Marchette, and D. L. Sussman. A limit
theorem for scaled eigenvectors of random dot product graphs. Sankhya A, 78(1):1–18, 2016.

[5] A. Athreya, D. E. Fishkind, M. Tang, C. E. Priebe, Y. Park, J. T. Vogelstein, K. Levin,
V. Lyzinski, and Y. Qin. Statistical inference on random dot product graphs: a survey. The
Journal of Machine Learning Research, 18(1):8393–8484, 2017.

[6] J. Cape, M. Tang, and C. E. Priebe. The two-to-inﬁnity norm and singular subspace geometry
with applications to high-dimensional statistics. The Annals of Statistics, 2017. To appear;
preprint available at http://arxiv.org/abs/1705.10735.

[7] J. Cape, M. Tang, and C. E. Priebe. On spectral embedding performance and elucidating
network structure in stochastic blockmodel graphs. Network Science, 7(3):269–291, 2019.

[8] J. Cape, M. Tang, and C. E. Priebe. Signal-plus-noise matrix models: eigenvector deviations

and ﬂuctuations. Biometrika, 106(1):243–250, 2019.

[9] I. Csiz´ar.

Information-type measures of diﬀerence of probability distributions and indirect

observations. Studia Scientiarum Mathematicarum Hungarica, 2:229–318, 1967.

[10] P. Erd¨os and A. R´enyi. On the evolution of random graphs. Proceedings of the Hungarian

Academy of Sciences, pages 17–61, 1960.

[11] T. Fawcett. An introduction to ROC analysis. Pattern Recogn. Lett., 27(8):861–874, 2006.

27

[12] A. Fornito, A. Zalesky, and E. Bullmore. Fundamentals of brain network analysis. Academic

Press, 2016.

[13] E. N. Gilbert. Random graphs. Annals of Mathematical Statistics, 30(4):1141–1144, 1959.

[14] Chernoﬀ. H. A measure of asymptotic eﬃciency for tests of a hypothesis based on the sum of

observations. The Annals of Mathematical Statistics, 23(4):493–507, 1952.

[15] P. D. Hoﬀ, A. E. Raftery, and M. S. Handcock. Latent space approaches to social network

analysis. Journal of the American Statistical Association, 97:1090–1098, 2002.

[16] P. W. Holland, K. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social

Networks, 5:109–137, 1983.

[17] R. Horn and C. Johnson. Matrix Analysis (Second Edition). Cambridge University Press,

New York, NY, 2012.

[18] S. Khor. Concurrency and network disassortativity. Artiﬁcial life, 16(3):225–232, 2010.

[19] M. Kivel¨a, A. Arenas, M. Barthelemy, J. P. Gleeson, Y. Moreno, and M. A. Porter. Multilayer

networks. Journal of complex networks, 2(3):203–271, 2014.

[20] L. Lathauwer, B. Moor, and J. Vandewalle. A multilinear singular value decomposition. SIAM

J. Matrix Anal. Appl, 21(4):1253–1278, 2000.

[21] K. Levin, A. Athreya, M. Tang, V. Lyzinski, and C. E. Priebe. A central limit theorem for an
omnibus embedding of random dot product graphs. arXiv preprint arXiv:1705.09355, 2017.

[22] F. Liese and I. Vadja. On divergences and informations in statistics and information theory.

IEEE Transactions on Information Theory, 52:4394–4412, 2006.

[23] V. Lyzinski, M. Tang, A. Athreya, Y. Park, and C. E. Priebe. Community detection and
classiﬁcation in hierarchical stochastic blockmodels. IEEE Transactions in Network Science
and Engineering, 4(1):13–26, 2017.

[24] M. E. J. Newman. Assortative mixing in networks. Physical Review Letters, 89:208701, 2002.

[25] C. Nickel. Random dot product graphs: a model for social networks. PhD thesis, Johns Hopkins

University, 2006.

[26] A. M. Nielsen and D. Witten. The multiple random dot product graph model. arXiv preprint

arXiv:1811.12172, 2018.

[27] L. Pardo. Statistical inference based on divergence measures. Chapman and Hall/CRC, 2005.

[28] F. S. Passino, A. S. Bertiger, J. C. Neil, and N. A. Heard. Link prediction in dynamic networks

using random dot product graphs. arXiv preprint arXiv:1912.10419, 2019.

[29] P. Rubin-Delanchy, J. Cape, C. E. Priebe, and M. Tang. A statistical interpretation of spectral
embedding: the generalised random dot product graph. arXiv preprint arXiv:1709.05506v3,
2020.

[30] P. Sarkar and P. J. Bickel. Role of normalization in spectral clustering for stochastic block-

models. The Annals of Statistics, 43(3):962–990, 2015.

[31] L. Scrucca, M. Fop, T. B. Murphy, and A. E. Raftery. mclust 5: clustering, classiﬁcation and
density estimation using Gaussian ﬁnite mixture models. The R Journal, 8(1):205–233, 2016.

[32] D. L. Sussman, M. Tang, D. E. Fishkind, and C. E. Priebe. A consistent adjacency spectral
embedding for stochastic blockmodel graphs. Journal of the American Statistical Association,
107(499):1119–1128, 2012.

[33] M. Tang and C. Priebe. Limit theorems for eigenvectors of the normalized Laplacian for

random graphs. The Annals of Statistics, 2019. To appear.

28

[34] M. Tang, A. Athreya, D. L. Sussman, V. Lyzinski, Y. Park, and C. E. Priebe. A semiparametric
two-sample hypothesis testing problem for random graphs. Journal of Computational and
Graphical Statistics, 26(2):344–354, 2017.

[35] R. Tang, M. Ketcha, A. Badea, E. Calabrese, D. Margulies, J. Vogelstein, C. Priebe, and
D. Sussman. Connectome smoothing via low-rank approximations. IEEE Transactions on
Medical Imaging, 38(6):1446–1456, 2019.

[36] J. A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends

in Machine Learning, 8(1–2):1–230, 2015.

[37] M. Turcotte, A. Kent, and C. Hash. Uniﬁed host and network data set. Data Science for

Cyber-Security, pages 1–22, 2018.

[38] U. Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416,

2007.

[39] S. Wang, J. D. Arroyo-Reli´on, J. T. Vogelstein, and C. E. Priebe. Joint embedding of graphs.

IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. To appear.

[40] Stephen J. Young and Edward R. Scheinerman. Random dot product graph models for social
networks. In International Workshop on Algorithms and Models for the Web-Graph,, pages
pages 138–149. Springer, 2007.

[41] Y. Yu, T. Wang, and R. J. Samworth. A useful variant of the Davis-Kahan theorem for

statisticians. Biometrika, 102:315–323, 2015.

[42] M. Zhu and A. Ghodsi. Automatic dimensionality selection from the scree plot via the use of

proﬁle likelihood. Computational Statistics & Data Analysis, 51(2):918–930, 2006.

Appendix

Before proving Theorems 2 and 3, we ﬁrst require some control over the asymptotic behaviour of
the singular values of the matrices P, A and A − P, which we establish using a series of results.
Throughout this section, we will assume that (A, X, Y) ∼ MRDPG(Fρ, Λ(cid:15)) for a distribution F
and sparsity factors ρ and (cid:15)r satisfying the criteria stated in Section 2.1.

Proposition 7. The non-zero singular values σi(P) for i ∈ {1, . . . , d} satisfy

σi(P)
ρn

(cid:113)

−→

(cid:0)∆X Λ∗∆Y Λ(cid:62)

∗

(cid:1)

λi

(31)

almost surely, where ∆Y = c1∆Y,1 ⊕ · · · ⊕ cκ∆Y,κ, and consequently σi(P) = O((cid:15)ρn) and σi(P) =
Ω(ρn) almost surely.

Proof. Since P = XΛ(cid:15)Y(cid:62), we see that

σi(P) =

(cid:113)

λi

(cid:0)XΛ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) X(cid:62)(cid:1) =

(cid:113)

(cid:0)X(cid:62)XΛ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15)

(cid:1),

λi

(32)

as the non-zero eigenvalues of a product of matrices are invariant under cyclic permutations of its
factors.

Our assumptions regarding the distribution F ensure that the spectral norms (cid:107)X(cid:62)X − ρn∆X (cid:107)
and (cid:107)Y(cid:62)Y − ρn∆Y (cid:107) are of order O(cid:0)ρn1/2 log1/2(n)(cid:1) mutually almost surely, and we note that
consequently

(cid:107)Y(cid:62)Y(cid:107) ≤ ρ(cid:107)∆Y (cid:107) + (cid:107)Y(cid:62)Y − ρ∆Y (cid:107) = O (ρn)

(33)

almost surely by a standard application of the triangle inequality.

29

Next, note that

X(cid:62)XΛ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) − ρ2n∆X Λ(cid:15)∆Y Λ(cid:62)

(cid:15) = (X(cid:62)X − ρn∆X )Λ(cid:15)Y(cid:62)YΛ(cid:62)
(cid:15)
+ ρn∆X Λ(cid:15)(Y(cid:62)Y − ρ∆Y )Λ(cid:62)
(cid:15)

and so

(cid:107)X(cid:62)XΛ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) − ρ2n∆X Λ(cid:15)∆Y Λ(cid:62)

(cid:15) (cid:107) ≤ (cid:107)Λ(cid:15)(cid:107)2(cid:0)(cid:107)X(cid:62)X − ρn∆X (cid:107)(cid:107)Y(cid:62)Y(cid:107)
+ ρn(cid:107)∆X (cid:107)(cid:107)Y(cid:62)Y − ρ∆Y (cid:107)(cid:1)

meaning that

almost surely.

(cid:107)X(cid:62)XΛ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) − ρ2n∆X Λ(cid:15)∆Y Λ(cid:62)

(cid:15) (cid:107) = O(cid:0)(cid:15)2ρ2n3/2 log1/2(n)(cid:1)

(34)

(35)

(36)

1

As a result, we see that

ρ2n2 X(cid:62)XΛ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) in the spectral norm,
and thus also in the Frobenius norm, implying in particular that the entries of the two matrices
converge in absolute value. Now, the eigenvalues of any matrix are the roots of its characteristic
polynomial, the coeﬃcients of which are polynomial functions of the entries of the matrix.
In
ρ2n X(cid:62)XΛ(cid:15)Y(cid:62)YΛ(cid:62)
(cid:15) and
particular, by continuity of the roots of polynomials, the eigenvalues of
∆X Λ∗∆Y Λ(cid:62)

∗ must converge, giving the ﬁrst result.

(cid:15) converges to ∆X Λ(cid:15)∆Y Λ(cid:62)

1

(cid:0)∆X Λ∗∆Y Λ(cid:62)

(cid:1) = λi

For the second, note that λi

X is the
unique positive deﬁnite matrix square root of ∆X ) and that the latter matrix is a sum of symmetric
matrices, allowing us to apply Weyl’s inequalities to obtain the upper and lower bounds.
Proposition 8. (cid:107)A − P(cid:107) = O(cid:0)(cid:15)1/2ρ1/2k1/4n1/2 log1/2(n)(cid:1) almost surely.
Proof. Condition on some choice of latent positions. We will make use of a matrix analogue of the
Bernstein inequality (see [36], Theorem 1.6.2):

X Λ∗∆Y Λ(cid:62)

X

∗

(cid:1) (where ∆1/2

∗ ∆1/2

(cid:0)∆1/2

Theorem 9 (Matrix Bernstein). Let M1, . . . , Mn be independent random matrices with common
dimensions m1 × m2, satisfying E[Mk] = 0 and (cid:107)Mk(cid:107) ≤ L for each 1 ≤ k ≤ n, for some ﬁxed
value L.

Let M = (cid:80) Mk and let v(M) = max{(cid:107)E[MM(cid:62)](cid:107), (cid:107)E[M(cid:62)M](cid:107)} denote the matrix variance

statistic of M. Then for all t ≥ 0, we have

P(cid:0)(cid:107)M(cid:107) ≥ t(cid:1) ≤ (m1 + m2) exp

(cid:16) −t2/2

v(M)+Lt/3

(cid:17)

.

(37)

We apply this as follows: given r ∈ {1, . . . , k}, deﬁne a matrix T(r)

i ∈ {1, . . . , n} and j ∈ {1, . . . , nr} whose (i, n1 + . . . + nr−1 + j)th entry is equal to A(r)
with all other entries equal to 0 (in other words, if we divide T(r)
ij
the rth block is the only non-zero one, and within this block only the (i, j)th entry is non-zero).

ij ∈ Rn×(n1+...+nk) for each
ij − P(r)
ij ,
into distinct n × nt blocks, then

We then deﬁne matrices M(r)
ij

for each r ∈ {1, . . . , k} as follows:

• If A(r) is bipartite, we deﬁne M(r)

• If A(r) is directed, we deﬁne M(r)

ij

ij = T(r)
ij = T(r)

ij

for all i ∈ {1, . . . , n} and j ∈ {1, . . . , nr};

for all i, j ∈ {1, . . . , nr} with i (cid:54)= j;

• If A(r) is undirected, we deﬁne M(r)

ij = T(r)
ij (cid:107) = |Aij − Pij| < 1, and by deﬁnition E[M(r)

ij + T(r)

ji

Observe that (cid:107)M(r)
i,j,r M(r)

ij ] = 0, so the matrix sum
M = (cid:80)
satisﬁes the criteria for Bernstein’s Theorem (where we sum over the variables
i, j and r according to the cases above). To bound the matrix variance statistic v(M), let Mr =
(cid:80)

ij

for all i, j ∈ {1, . . . , nr} with i < j.

i,j M(r)

ij , and note that


MrM(cid:62)

r =




nr(cid:88)

(cid:0)A(r)

il − P(r)

il

(cid:1)(cid:0)A(r)

jl − P(r)

jl

(cid:1)

if A(r) is bipartite

l=1

(cid:88)

l(cid:54)=i,j

(cid:0)A(r)

il − P(r)

il

(cid:1)(cid:0)A(r)

jl − P(r)

jl

(cid:1)

otherwise,

(38)

30

and therefore that

E[MrM(cid:62)

r ]ij =

nr(cid:88)

l=1

(cid:88)

l(cid:54)=i






P(r)
il

(cid:0)1 − P(r)

il

(cid:1)

if A(r) is bipartite and i = j

P(r)
il

(cid:0)1 − P(r)

il

(cid:1)

if A(r) is not bipartite and i = j

(39)

0

if i (cid:54)= j.

By deﬁnition, P(r)
(cid:107)E[MrM(cid:62)
(cid:107)E[MM(cid:62)](cid:107) ≤ ((cid:15)1n1 + . . . + (cid:15)knk)ρ ≤ (cid:15)ρk1/2nmax by the Cauchy–Schwarz inequality.

il ) ≤ (cid:15)rρ for all i and l, and so since E[MrM(cid:62)

r ] is diagonal, we see that
r and the Mr are independent, it follows that

r ](cid:107) ≤ (cid:15)rρnr. Since MM(cid:62) = (cid:80) MrM(cid:62)

il (1 − P(r)

Similarly,

[M(cid:62)

r Mr]ij =

and so

E[M(cid:62)

r Mr]ij =











n
(cid:88)

(cid:0)A(r)

li − P(r)

li

(cid:1)(cid:0)A(r)

lj − P(r)

lj

(cid:1)

if A(r) is bipartite

l=1

(cid:88)

l(cid:54)=i,j

(cid:0)A(r)

li − P(r)

li

(cid:1)(cid:0)A(r)

lj − P(r)

lj

(cid:1)

otherwise,

(40)

n
(cid:88)

l=1

(cid:88)

l(cid:54)=i

P(r)
li

(cid:0)1 − P(r)

li

(cid:1)

if A(r) is bipartite and i = j

P(r)
li

(cid:0)1 − P(r)

li

(cid:1)

if A(r) is not bipartite and i = j

(41)

0

if i (cid:54)= j,

and as before we see that (cid:107)E[M(cid:62)
s Mt](cid:107) = 0 if s (cid:54)= t since the matrices Mr
are independent. Thus the matrix E[M(cid:62)M] is block diagonal, and so it follows that (cid:107)E[M(cid:62)M](cid:107) ≤
ρn, and so certainly v(M) = O((cid:15)ρk1/2n).

r Mr](cid:107) ≤ (cid:15)rρn, while (cid:107)E[M(cid:62)

Substituting these into Bernstein’s Theorem and rearranging, we ﬁnd that, for any t ≥ 0,

P(cid:0)(cid:107)M(cid:107) ≥ t(cid:1) ≤ (n + n1 + . . . + nk) exp

(cid:18)

−3t2
6ρk1/2n + 2t

(cid:19)

.

(42)

The numerator of the exponential term dominates for n suﬃciently large if
t = O(cid:0)(cid:15)1/2ρ1/2k1/4n1/2 log1/2(n)(cid:1), and so (cid:107)M(cid:107) = O(cid:0)(cid:15)1/2ρ1/2k1/4n1/2 log1/2(n)(cid:1) almost surely.

Finally, we note that M = A − P + P0, where P0 comprises k distinct blocks of sizes n × nr,
with the rth such block either identically zero or diagonal, with entries equal to the diagonal entries
of P(r), depending on whether or not A(r) is bipartite, and satisﬁes (cid:107)P0(cid:107) ≤ (cid:15)ρ. The result then
follows from subadditivity of the spectral norm and integrating over all possible choices of latent
positions.

Corollary 10. The leading d singular values of A satisfy

σi(A)
ρn

(cid:113)

−→

(cid:0)∆X Λ∗∆Y Λ(cid:62)

∗

(cid:1)

λi

(43)

almost surely, where ∆Y = c1∆Y,1 ⊕ · · · ⊕ cκ∆Y,κ, and consequently σi(A) = O ((cid:15)ρn) and σi(A) =
Ω (ρn) almost surely.

Proof. A corollary of Weyl’s inequalities (see, for example, [17], Corollary 7.3.5) states that |σi(A)−
σi(P)| ≤ (cid:107)A − P(cid:107), and so in particular (applying the reverse triangle inequality where necessary)

σi(P) − (cid:107)A − P(cid:107) ≤ σi(A) ≤ σi(P) − (cid:107)A − P(cid:107).

(44)

The result then follows from Propositions 7 and 8.

31

The next few results provide bounds on the asymptotic growth of a number of residual terms
in the proofs of our main theorems. While the proofs are similar in nature to a number of results
in [23], there are some minor diﬀerences to account for the fact that the matrices A and P are
not symmetric, and so we reproduce them in full. We begin an analogue of a bound appearing in
Lemma 17 of [23]:

Proposition 11. (cid:107)U(cid:62)

P(A − P)VP(cid:107)F = O(cid:0)log1/2(n)(cid:1) almost surely.

Proof. Condition on some choice of latent positions. For any i, j ∈ {1, . . . , d} and r ∈ {1, . . . , k},
let

n
(cid:88)

nr(cid:88)

p=1

q=1

upv(r)
q

(cid:0)A(r)

pq − P(r)
pq

(cid:1)

if A(r) is bipartite

upv(r)
q

(cid:0)A(r)

pq − P(r)
pq

(cid:1)

(cid:88)

p(cid:54)=q

if A(r) is directed

(45)

E(r)

ij =






and

(cid:88)

(upv(r)

q + uqv(r)

p )(cid:0)A(r)

pq − P(r)
pq

(cid:1)

if A(r) is undirected

p<q

F(r)

ij =





n
(cid:88)

p=1

0

if A(r) is bipartite

upv(r)

p P(r)
pp

otherwise,

where u and v(r) denote the ith and jth columns of UP and V(r)

P respectively, so that

(cid:0)U(cid:62)

P(A − P)VP

(cid:1)

ij =

k
(cid:88)

r=1

E(r)

ij −

k
(cid:88)

r=1

F(r)
ij .

We can bound the latter term by applying the Cauchy–Schwarz inequality to ﬁnd that

(cid:12)
(cid:12)
(cid:12)

k
(cid:88)

r=1

F(r)
ij

(cid:12)
(cid:12)
(cid:12) ≤

(cid:16) k
(cid:88)

n
(cid:88)

r=1

p=1

pp |2(cid:17)1/2(cid:16) k

(cid:88)

|upP(r)

n
(cid:88)

p |2(cid:17)1/2
|v(r)

= O((cid:15)ρ),

r=1

p=1

and thus can be discounted in our asymptotic analysis.

Each of the E(r)
ij

terms bounded in absolute value by |upv(r)
in the undirected case. Applying Hoeﬀding’s inequality, we thus observe that

is a sum of independent zero-mean random variables, with each of the individual
q + uqv(r)
p |

| in the bipartite and directed cases and |upv(r)

q

(cid:32)
(cid:12)
(cid:12)
(cid:12)

P

k
(cid:88)

r=1

(cid:33)

(cid:12)
(cid:12)
(cid:12) > t

E(r)
ij

≤ 2exp







(cid:16)(cid:80)
4
r1

(cid:80)
p,q

|upv(r1)
q

−2t2
(cid:80)
p<q

|2 + (cid:80)
r3







,

(cid:17)

|2

|upv(r2)

q + uqv(r2)

p

where r1 sums over the bipartite and directed cases and r2 sums over the undirected cases.
|2 + |uqv(r)

Note that |upv(r)

p |2 + 2|upuqv(r)

p |2 ≤ |upv(r)

q + uqv(r)

p |, and so

q v(r)

q

(cid:32)
(cid:12)
(cid:12)
(cid:12)

P

k
(cid:88)

r=1

(cid:33)

(cid:12)
(cid:12)
(cid:12) > t

E(r)
ij

≤ 2exp








−2t2

(cid:16) k
(cid:80)
4
r=1

(cid:80)
p,q

|upv(r)
q

|2 + 2 (cid:80)
r2

(cid:80)
p<q

|upuqv(r2)

q

v(r2)
p

(cid:17)
|








.

32

(49)

(50)

(46)

(47)

(48)

Both summands are at most 1; the ﬁrst is clear, while for the second we apply the Cauchy–

Schwarz inequality and note that

(cid:88)

(cid:88)

r2

p<q

|upuqv(r2)

q

v(r2)
p

| ≤

(cid:16)(cid:88)

(cid:88)

|upv(r2)
q

|2(cid:17)1/2(cid:16)(cid:88)

(cid:88)

|uqv(r2)
p

|2(cid:17)1/2

r3
(cid:16)(cid:88)

p<q
(cid:88)

≤

r2

p,q

r2

p<q

|up|2|v(r2)

q

|2(cid:17)

= 1.

(51)

(52)

Thus (cid:80)k

r=1 E(r)

ij = O(cid:0)log1/2(n)(cid:1) almost surely, and the result follows after integrating over all

possible choices of latent positions.

Before establishing the next set of bounds (which relate to the left and right singular vectors
of the matrices A and P), we state the following variation of the Davis–Kahan theorem (see [41],
Theorem 4):
Theorem 12 (Variant of Davis–Kahan). Let M1, M2 ∈ Rm×n have singular value decompositions

Mi = UiΣiV(cid:62)

i + Ui,⊥Σi,⊥V(cid:62)

i,⊥,

(53)

where Ui ∈ O(m × d) has orthonormal columns corresponding to the d greatest singular values of
Mi, for some 1 ≤ d ≤ n. Then, if |σd(M1)2 − σd+1(M1)2| > 0, we have

(cid:107) sin Θ(U2, U1)(cid:107) ≤

√
2

d (2σ1(M1) + (cid:107)M2 − M1(cid:107)) (cid:107)M2 − M1(cid:107)

σd(M1)2 − σd+1(M1)2

.

(54)

where we take σn+1(M1) = −∞.

We note that the same inequality holds for (cid:107) sin Θ(V2, V1)(cid:107) since the right-hand side of (54)

is invariant under matrix transposition. Using this result, we can prove the following:

Proposition 13. The following bounds hold almost surely:

(i) (cid:107)UAU(cid:62)

A − UPU(cid:62)

P(cid:107), (cid:107)VAV(cid:62)

A − VPV(cid:62)

P(cid:107) = O

(cid:16) (cid:15)3/2k1/4 log1/2(n)
ρ1/2n1/2

(cid:17)
;

(ii) (cid:107)UA − UPU(cid:62)

PUA(cid:107)F , (cid:107)VA − VPV(cid:62)

PVA(cid:107)F = O

(cid:16) (cid:15)3/2k1/4 log1/2(n)
ρ1/2n1/2

(cid:17)

;

(iii) (cid:107)U(cid:62)

PUAΣA − ΣPV(cid:62)

PUA − V(cid:62)

PVAΣA(cid:107)F = O(cid:0)(cid:15)2k1/2 log(n)(cid:1);

(iv) (cid:107)U(cid:62)

PUA − V(cid:62)

PVA(cid:107)F = O

PVA(cid:107)F , (cid:107)ΣPU(cid:62)
(cid:16) (cid:15)2k1/2 log(n)
ρn

(cid:17)

Proof.

(i) Let σ1, . . . , σd denote the singular values of U(cid:62)

angles. It is a standard result that the non-zero eigenvalues of the matrix UAU(cid:62)
are precisely the sin(θi) (each occurring twice) and so by Davis–Kahan we have

PUA, and let θi = cos−1(σi) be the principal
A − UPU(cid:62)
P

(cid:107)UAU(cid:62)

A − UPU(cid:62)

P(cid:107) = max

i∈{1,...,d}

√

2

| sin(θi)| ≤

d (2σ1(P) + (cid:107)A − P(cid:107)) (cid:107)A − P(cid:107)

σd(P)2

for n suﬃciently large.

Applying the bounds from Propositions 7 and 8 then shows that
(cid:16) (cid:15)3/2k1/4 log1/2(n)
ρ1/2n1/2

A − UPU(cid:62)

(cid:107)UAU(cid:62)

P(cid:107) = O

(cid:17)

.

An identical argument gives the result for (cid:107)VAV(cid:62)

A − VPV(cid:62)

P(cid:107).

(ii) Using the bound from part (i), we ﬁnd that

(55)

(56)

(cid:107)UA − UPU(cid:62)

PUA(cid:107)F = (cid:107)(UAU(cid:62)

A − UPU(cid:62)

P)UA(cid:107)F = O

(cid:16) (cid:15)3/2k1/4 log1/2(n)
ρ1/2n1/2

(cid:17)

.

(57)

An identical argument bounds the term (cid:107)VA − VPV(cid:62)

PVA(cid:107)F .

33

(iii) Observe that

U(cid:62)

PUAΣA − ΣPV(cid:62)

PVA = U(cid:62)

P(A − P)VA

and that we may rewrite the right-hand term to ﬁnd that

U(cid:62)

PUAΣA − ΣPV(cid:62)

PVA = U(cid:62)
+ U(cid:62)

P(A − P)(VA − VPV(cid:62)
P(A − P)VPV(cid:62)
PVA.

PVA)

These terms satisfy

(cid:107)U(cid:62)

P(A − P)(VA − VPV(cid:62)

PVA)(cid:107)F = O(cid:0)(cid:15)2k1/2 log(n)(cid:1)

and

U(cid:62)

P(A − P)VPV(cid:62)

PVA = O(cid:0)log1/2(n)(cid:1)

by Propositions 8, 11 and the result from part (ii), and thus

(cid:107)U(cid:62)

PUAΣA − ΣPV(cid:62)

PVA(cid:107)F = O(cid:0)(cid:15)2k1/2 log(n)(cid:1).

An identical argument bounds the term (cid:107)ΣPU(cid:62)

PUA − V(cid:62)

PVAΣA(cid:107)F .

(iv) Note that

U(cid:62)

PUA − V(cid:62)

PVA = (cid:0)(U(cid:62)
− V(cid:62)

PUAΣA − ΣPV(cid:62)
PVAΣA)(cid:1)Σ−1

PVA) + (ΣPU(cid:62)

A − ΣP(U(cid:62)

PUA − V(cid:62)

PUA
PVA)Σ−1
A .

(58)

(59)

(60)

(61)

(62)

(63)

For any i, j we ﬁnd (after rearranging and bounding the absolute value of the right-hand
terms by the Frobenius norm):

(cid:12)
(cid:12)(U(cid:62)

PUA − V(cid:62)

PVA)ij

(cid:16)

(cid:12)
(cid:12)

1 + σi(P)
σj (A)

(cid:17)

≤ (cid:0)(cid:107)U(cid:62)

PUAΣA − ΣPV(cid:62)
PUA − V(cid:62)

+ (cid:107)ΣPU(cid:62)

PVAΣA(cid:107)F

PVA(cid:107)F

where we have used the result from part (iii) and Corollary 10.

Consequently, we ﬁnd that

(cid:12)
(cid:12)(U(cid:62)

PUA − V(cid:62)

PVA)ij

(cid:12)
(cid:12) = O

(cid:16) (cid:15)2k1/2 log(n)
ρn

(cid:17)

(cid:1)(cid:107)Σ−1

A (cid:107)F

(64)

(65)

by noting that

(cid:16)

1 + σi(P)
σj (A)

(cid:17)

≥ 1.

The following result (an analogue of [23], Proposition 16) relates to orthogonal matrix W used

to perform a simultaneous Procrustes alignment of XA with XP and YA with YP.

Proposition 14. Let U(cid:62)

PUA + V(cid:62)

PVA admit the singular value decomposition

and let W = W1W(cid:62)

2 . Then

U(cid:62)

PUA + V(cid:62)

PVA = W1ΣW(cid:62)
2 ,

max(cid:8)(cid:107)U(cid:62)

PUA − W(cid:107)F , (cid:107)V(cid:62)

PVA − W(cid:107)F

(cid:9) = O

(cid:16) (cid:15)3k1/2 log(n)
ρn

(cid:17)

(66)

(67)

almost surely.

34

Proof. A standard argument shows that W minimises the term (cid:107)U(cid:62)
among all Q ∈ O(d). Let U(cid:62)
and deﬁne WU ∈ O(d) by WU = WU,1W(cid:62)

PUA = WU,1ΣUW(cid:62)

U,2. Then

U,2 be the singular value decomposition of U(cid:62)

PUA − Q(cid:107)2

F + (cid:107)V(cid:62)

PVA − Q(cid:107)2
F
PUA,

(cid:107)U(cid:62)

PUA − WU(cid:107)F = (cid:107)Σ − I(cid:107)F =

(cid:16) d
(cid:88)

(1 − σi)2(cid:17)1/2

≤

d
(cid:88)

i=1

(1 − σi)

d
(cid:88)

(1 − σ2

i ) =

≤

i=1

i=1

d
(cid:88)

i=1

sin2(θi) ≤ d(cid:107)UAU(cid:62)

A − UPU(cid:62)

P(cid:107)2

and so

Also,

and so

(cid:107)U(cid:62)

PUA − WU(cid:107)F = O

(cid:16) (cid:15)3k1/2 log(n)
ρn

(cid:17)

.

(cid:107)V(cid:62)

PVA − WU(cid:107)F ≤ (cid:107)V(cid:62)

PVA − U(cid:62)

PUA(cid:107)F + (cid:107)U(cid:62)

PUA − WU(cid:107)F

(cid:107)V(cid:62)

PVA − WU(cid:107)F = O

(cid:16) (cid:15)3k1/2 log(n)
ρn

(cid:17)

by Proposition 13.

Combining these shows that

(cid:107)U(cid:62)

PUA − W(cid:107)2

F + (cid:107)V(cid:62)

PVA − W(cid:107)2

F ≤ (cid:107)U(cid:62)

PUA − WU(cid:107)2

F + (cid:107)V(cid:62)

PVA − WU(cid:107)2
F

and thus

as required.

max(cid:8)(cid:107)U(cid:62)

PUA − W(cid:107)F , (cid:107)V(cid:62)

PVA − W(cid:107)F

(cid:9) = O

(cid:16) (cid:15)3k1/2 log(n)
ρn

(cid:17)

The following bounds are a straightforward adaptation of [23], Lemma 17:

Proposition 15. The following bounds hold almost surely:

(i) (cid:107)WΣA − ΣPW(cid:107)F = O(cid:0)(cid:15)4k1/2 log(n)(cid:1);
(cid:17)
(cid:16) (cid:15)4k1/2 log(n)
ρ1/2n1/2

P W(cid:107)F = O

A − Σ1/2

(ii) (cid:107)WΣ1/2

;

(iii) (cid:107)WΣ−1/2

A − Σ−1/2

P W(cid:107)F = O

(cid:16) (cid:15)4k1/2 log(n)
ρ3/2n3/2

(cid:17)

.

Proof.

(i) Observe that

WΣA − ΣPW = (W − U(cid:62)

PUA)ΣA + U(cid:62)

PUAΣA − ΣPW

and that the right-hand expression may be rewritten as

(W − U(cid:62)

PUA)ΣA + (U(cid:62)

PUAΣA − ΣPV(cid:62)

PVA) + ΣP(V(cid:62)

PVA − W).

(68)

(69)

(70)

(71)

(72)

(73)

(74)

(75)

PVA − W)(cid:107)F are both O(cid:0)(cid:15)4k1/2 log(n)(cid:1) (as
PUA)ΣA(cid:107)F and (cid:107)ΣP(V(cid:62)
The terms (cid:107)(W − U(cid:62)
shown by Propositions 7, 14 and Corollary 10), while the term (cid:107)U(cid:62)
PVA(cid:107)F is
O(cid:0)(cid:15)2k1/2 log(n)(cid:1) by Proposition 13, and so (cid:107)WΣA−ΣPW(cid:107)F = O(cid:0)(cid:15)4k1/2 log(n)(cid:1) as required.

PUAΣA − ΣPV(cid:62)

35

(ii) Note that

(cid:0)WΣ1/2

A − Σ1/2

P W(cid:1)

ij = Wij

(cid:0)σj(A)1/2 − σi(P)1/2(cid:1)

(76)

(77)

(78)

(79)

(80)

=

=

Wij(σj(A) − σi(P))
σj(A)1/2 + σi(P)1/2
(cid:0)WΣA − ΣPW(cid:1)
ij
σj(A)1/2 + σi(P)1/2
(cid:16) (cid:15)4k1/2 log(n)
ρ1/2n1/2

(cid:17)

,

=

(cid:0)WΣ1/2

P W(cid:1)
A − Σ1/2
σi(P)1/2σj(A)1/2
(cid:16) (cid:15)4k1/2 log(n)
(cid:17)
ρ3/2n3/2

ij

and so we ﬁnd that (cid:107)WΣ1/2
over all i, j ∈ {1, . . . , d}.

A −Σ1/2

P W(cid:107)F = O

by applying part (i) and summing

(iii) Note that

(cid:0)WΣ−1/2

A − Σ−1/2

P W(cid:1)

ij =

Wij(σi(P)1/2 − σj(A)1/2)
σi(P)1/2σj(A)1/2

and so we ﬁnd that (cid:107)WΣ−1/2
summing over all i, j ∈ {1, . . . , d}.

A − Σ−1/2

P W(cid:107)F = O

by applying part (ii) and

The next results establish the existence of, and some properties relating to, the matrices (cid:101)L
and (cid:101)Rr which map the latent position matrices X and Y(r) to the embeddings XP and Y(r)
P
respectively. In particular, where they exist, we bound the growth of the spectral norm of the
inverses (or pseudo-inverses where appropriate) of these matrices, which is necessary in order to
be able to recover the latent positions from the UASE.

Proposition 16. If X is of rank d then there exists a matrix (cid:101)L ∈ GL(d) such that XP = X(cid:101)L.
In addition, if Y(r) is of rank dr then Y(r)
P = Y(r) (cid:101)Rr, where the matrix (cid:101)Rr ∈ Rdr×d satisﬁes
(cid:101)L (cid:101)R(cid:62)
Proof. Let ΠX, ΠY ∈ GL(d) satisfy ΠX = (X(cid:62)X)1/2 and ΠY = (Λ(cid:15)Y(cid:62)YΛ(cid:62)
the unique positive-deﬁnite matrix square root in each case.

r = Λ(cid:15),r. In particular, rank( (cid:101)Rr) = rank(Λr).

(cid:15) )1/2, where we take

Observe that

(cid:0)XPΣ1/2

P

(cid:1)(cid:0)XPΣ1/2

P

(cid:1)(cid:62)

= UPΣ2

PU(cid:62)

P = PP(cid:62) = (XΠY)(XΠY)(cid:62)

and similarly (noting that UPΣPV(r)
P

(cid:62)

= P(r)) that

(cid:0)Y(r)

P Σ1/2

P

(cid:1)(cid:0)Y(r)

P Σ1/2

P

(cid:1)(cid:62)

= P(r)(cid:62)

P(r) = (cid:0)Y(r)Λ(cid:62)

(cid:15),rΠX

(cid:1)(cid:0)Y(r)Λ(cid:62)

(cid:15),rΠX

(cid:1)(cid:62)

.

Thus there exist orthogonal matrices QP, Q(r)
P = XΠYQP, Y(r)

XPΣ1/2

P ∈ O(d) such that
P Σ1/2
P = Y(r)Λ(cid:62)

(cid:15),rΠXQ(r)

P

and so

(cid:101)L = ΠYQPΣ−1/2

P

∈ GL(d),

are our desired matrices.

For the ﬁnal statement, observe that

(cid:101)Rr = Λ(cid:62)

(cid:15),rΠXQ(r)

P Σ−1/2

P

∈ Rdr×d

X(cid:101)L (cid:101)R(cid:62)

r Y(r)(cid:62)

= XPY(r)
P

(cid:62)

= P(r) = XΛ(cid:15),rY(r)(cid:62)

,

(81)

(82)

(83)

(84)

(85)

and so the result follows after multiplying by (X(cid:62)X)−1X(cid:62) and Y(r)(Y(r)(cid:62)
right respectively.

Y(r))−1 on the left and

36

Corollary 17. The matrices (cid:101)L and (cid:101)Rr satisfy (cid:107)(cid:101)L(cid:107) = O((cid:15)), (cid:107)(cid:101)L−1(cid:107) = O((cid:15)1/2) and (cid:107) (cid:101)Rr(cid:107) = O((cid:15)r)
almost surely.

Moreover, if the matrix Λr is of rank dr, then (cid:107) (cid:101)R+
r )−1 is the Moore-Penrose inverse of (cid:101)Rr.
r ( (cid:101)Rr (cid:101)R(cid:62)

(cid:101)R(cid:62)

r (cid:107) = O(cid:0) 1

(cid:15)r

(cid:1) almost surely, where (cid:101)R+

r =

Proof. Proposition 7 shows us that (cid:107)ΣP(cid:107) = O((cid:15)ρn) and (cid:107)Σ−1
of reasoning shows that (cid:107)ΠΛ(cid:15),Y(cid:107) = O((cid:15)ρ1/2n1/2) and (cid:107)Π−1
(cid:107)ΠX(cid:107) = O(ρ1/2n1/2) and (cid:107)Π−1

X (cid:107) = O(cid:0)

1
ρ1/2n1/2

(cid:1).

Λ(cid:15),Y(cid:107) = O(cid:0)

P (cid:107) = O(cid:0) 1

(cid:1), and an identical line
(cid:1), and similarly that

ρn
1
ρ1/2n1/2

The ﬁrst three bounds then follow from submultiplicativity and unitary invariance of the spec-
r = Λ(cid:62)
XΛ(cid:15),r, and so we see

(cid:15),rΠXQ(r)

P Q(r)

P Σ−1

tral norm. For the ﬁnal result, note that (cid:101)Rr (cid:101)R(cid:62)
that

Π(cid:62)

(cid:62)

P

σdr

(cid:0)

(cid:101)Rr (cid:101)R(cid:62)
r

(cid:62)

(cid:0)Q(r)

(cid:1) = λdr
≥ σ1(P)−1λdr

P

Π(cid:62)
XΛ(cid:15),rΛ(cid:62)
(cid:62)
(cid:0)Q(r)

(cid:15),rΠXQ(r)
Π(cid:62)
XΛ(cid:15),rΛ(cid:62)

P Σ−1
(cid:15),rΠXQ(r)

P

(cid:1)

P

P

(cid:1)

(86)

(87)

by a standard application of the min-max theorem for eigenvalues of Hermitian matrices.

Now,

λdr

(cid:0)Q(r)

P

(cid:62)

Π(cid:62)

XΛ(cid:15),rΛ(cid:62)

(cid:15),rΠXQ(r)

P

(cid:1) = λdr

(cid:0)Λ(cid:62)

(cid:15),rΠXΠ(cid:62)

XΛ(cid:15),r

(cid:1) = λdr

(cid:0)Λ(cid:62)

(cid:15),rX(cid:62)XΛ(cid:15),r

(cid:1),

(88)

r) almost
and a similar line of reasoning to that in Proposition 7 shows that σdr
surely. The result then follows from submultiplicativity and unitary invariance of the spectral
norm.

(cid:0)

(cid:101)Rr (cid:101)R(cid:62)
r

(cid:1) = Ω((cid:15)2

Proposition 18. If X is of rank d and each Y(r) is of rank dr then

( (cid:101)RrΣ−1

P (cid:101)L−1)(cid:62) = (Λ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) )−1Λ(cid:15),r.

Moreover, if dr = d and the matrix Λr is invertible, then

((cid:101)LΣ−1

P (cid:101)R−1

r )(cid:62) = Λ−1

(cid:15),r (X(cid:62)X)−1.

Proof. Recall from Proposition 16 that (cid:101)L (cid:101)R(cid:62)

r = Λ(cid:15),r. Similarly, since

X(cid:101)LΣP (cid:101)L(cid:62)X(cid:62) = XPΣPX(cid:62)

P = PP(cid:62) = XΛ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) X(cid:62),

we ﬁnd that (cid:101)LΣP (cid:101)L(cid:62) = Λ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) . Thus

( (cid:101)RrΣ−1

P (cid:101)L−1)(cid:62) = (cid:101)L−(cid:62)Σ−1

P (cid:101)L−1 (cid:101)L (cid:101)R(cid:62)

= (Λ(cid:15)Y(cid:62)YΛ(cid:62)

r
(cid:15) )−1Λ(cid:15),r

and

((cid:101)LΣ−1

P (cid:101)R−1

PX(X(cid:62)X)−1
PXP (cid:101)L−1(X(cid:62)X)−1

P X(cid:62)
P X(cid:62)
P ΣP (cid:101)L−1(X(cid:62)X)−1

r )(cid:62) = (cid:101)R−(cid:62)
= (cid:101)R−(cid:62)
= (cid:101)R−(cid:62)
= (cid:101)R−(cid:62)
= Λ−1

r Σ−1
r Σ−1
r Σ−1
r (cid:101)L−1(X(cid:62)X)−1
(cid:15),r (X(cid:62)X)−1,

(89)

(90)

(91)

(92)

(93)

(94)

(95)

(96)

(97)

(98)

where we have used the identities (cid:101)L = (X(cid:62)X)−1X(cid:62)XP and X(cid:62)

PXP = ΣP.

The ﬁnal result before proving our main theorems is an adaptation of Lemma 4 in [5] to the
MRDPG, and utilises the previous results to establish upper bounds for the two-to-inﬁnity norms
of a number of residual terms that will appear in the proofs of Theorems 2 and 3:

37

Proposition 19. Let

and

R1,1 = UP(U(cid:62)

R1,2 = (I − UPU(cid:62)

R1,3 = −UPU(cid:62)

P W)

PUAΣ1/2

A − Σ1/2
P)(A − P)(VA − VPW)Σ−1/2
P(A − P)VPWΣ−1/2
A − Σ−1/2

A
P W)

A

R1,4 = (A − P)VP(WΣ−1/2

R2,1 = VP(V(cid:62)

R2,2 = (I − VPV(cid:62)

R2,3 = −VPV(cid:62)

P W)

PVAΣ1/2

A − Σ1/2
P)(A − P)(cid:62)(UA − UPW)Σ−1/2
P(A − P)(cid:62)UPWΣ−1/2
A − Σ−1/2

A
P W)

A

R2,4 = (A − P)(cid:62)UP(WΣ−1/2

(99)

(100)

(101)

(102)

(103)

(104)

(105)

(106)

Then the following bounds hold almost surely:

(i) (cid:107)R1,1(cid:107)2→∞ = O

(cid:16) (cid:15)5k1/2 log(n)
ρ1/2n

(cid:17)

and (cid:107)R2,1(cid:107)2→∞ = O

(cid:16) (cid:15)4k1/2 log(n)
ρ1/2n

(cid:17)
;

(ii) (cid:107)R1,2(cid:107)2→∞, (cid:107)R2,2(cid:107)2→∞ = O

(cid:16) (cid:15)2k1/2 log(n)
ρ1/2n3/4

(cid:17)

;

(iii) (cid:107)R1,3(cid:107)2→∞ = O

(cid:16) (cid:15) log1/2(n)
ρ1/2n

(cid:17)

and (cid:107)R2,3(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n

(cid:17)

;

(iv) (cid:107)R1,4(cid:107)2→∞, (cid:107)R2,4(cid:107)2→∞ = O

(cid:16) (cid:15)9/2k3/4 log3/2(n)
n

(cid:17)

.

Proof. We give full proofs of the bounds only for the terms R1,i, noting any diﬀerences for the
proofs for the terms R2,i.

(i) Recall that UPΣ1/2

P = X(cid:101)L, where the matrix (cid:101)L ∈ GL(d) satisﬁes (cid:107)(cid:101)L(cid:107) = O((cid:15)) by Corollary
17. Using the relation (cid:107)AB(cid:107)2→∞ ≤ (cid:107)A(cid:107)2→∞(cid:107)B(cid:107) (see, for example, [6], Proposition 6.5) we
(cid:1) as the rows of
ﬁnd that (cid:107)UP(cid:107)2→∞ ≤ (cid:107)X(cid:107)2→∞(cid:107)(cid:101)L(cid:107)(cid:107)Σ−1/2
(cid:1) by splitting
X are by deﬁnition of order O(ρ1/2) (similarly, we ﬁnd that (cid:107)VP(cid:107)2→∞ = O(cid:0) 1
VPΣ1/2
P Σ1/2
P and evaluating each separately, and noting that
(cid:15)r ≤ 1 for all r).
Thus

P into the separate terms V(r)

(cid:107), and thus (cid:107)UP(cid:107)2→∞ = O(cid:0) (cid:15)

n1/2

n1/2

P

(cid:107)R1,1(cid:107)2→∞ ≤ (cid:107)UP(cid:107)2→∞(cid:107)U(cid:62)

PUAΣ1/2

A − Σ1/2
PUA − W)Σ1/2
(cid:17)

≤ (cid:107)UP(cid:107)2→∞

(cid:0)(cid:107)(U(cid:62)

(cid:16) (cid:15)7/2k1/2 log(n)
ρ1/2n1/2

15 shows that the second is O

(cid:16) (cid:15)4k1/2 log(n)
ρ1/2n1/2

(cid:17)

, and so

P W(cid:107)
A (cid:107)F + (cid:107)WΣ1/2

A − Σ1/2

P W(cid:107)F

(cid:1)

(107)

(108)

The ﬁrst summand is O

by Proposition 14 and Corollary 10, while Proposition

(cid:107)R1,1(cid:107)2→∞ = O

(cid:16) (cid:15)5k1/2 log(n)
ρ1/2n

(cid:17)

.

(ii) We begin by splitting the term R1,2 = M1 + M2, where

P(A − P)(VA − VPW)Σ−1/2

M1 = UPU(cid:62)
M2 = (A − P)(VA − VPW)Σ−1/2

A

A

Now,

(cid:107)M1(cid:107)2→∞ ≤ (cid:107)UP(cid:107)2→∞(cid:107)A − P(cid:107)(cid:107)VA − VPW(cid:107)(cid:107)Σ−1/2

A (cid:107),

38

(109)

(110)

(111)

(112)

where the term

(cid:107)UP(cid:107)2→∞(cid:107)A − P(cid:107)(cid:107)Σ−1/2

A (cid:107) = O

(cid:16) (cid:15)3/2k1/4 log1/2(n)
n1/2

(cid:17)

by Proposition 8 and Corollary 10, while

(cid:107)VA − VPW(cid:107) ≤ (cid:107)VA − VPV(cid:62)

PVA(cid:107) + (cid:107)VP(V(cid:62)

PVA − W)(cid:107)

= O

(cid:16) (cid:15)3/2k1/4 log1/2(n)
ρ1/2n1/2

(cid:17)

by Propositions 13 and 14 and the asymptotic growth conditions imposed on ρ. Thus

(cid:107)M1(cid:107)2→∞ = O

(cid:16) (cid:15)3k1/2 log(n)
ρ1/2n

(cid:17)
.

Next, note that

M2 = (A − P)(I − VPV(cid:62)

P)VAΣ−1/2

A + (A − P)VP(V(cid:62)

PVA − W)Σ−1/2

A

where

(cid:107)(A − P)VP(V(cid:62)

PVA − W)Σ−1/2

A (cid:107)2→∞ ≤ (cid:107)A − P(cid:107)(cid:107)V(cid:62)
= O

(cid:16) (cid:15)7/2k3/4 log3/2(n)
ρ3/2n

(cid:17)

PVA − W(cid:107)(cid:107)Σ−1/2
A (cid:107)

by Propositions 8, 14 and Corollary 10.
To bound the remaining term, let M = (A − P)(I − VPV(cid:62)

P)VAV(cid:62)

A, so that

(A − P)(I − VPV(cid:62)

P)VAΣ−1/2

A = MVAΣ−1/2

A

and so

(cid:107)(A − P)(I − VPV(cid:62)

P)VAΣ−1/2

A (cid:107)2→∞ ≤ (cid:107)M(cid:107)2→∞(cid:107)VAΣ−1/2

A (cid:107).

(113)

(114)

(115)

(116)

(117)

(118)

(119)

(120)

(121)

A (cid:107) is O(cid:0)

(cid:1) by Corollary 10, so it suﬃces to bound (cid:107)M(cid:107)2→∞. To
The term (cid:107)VAΣ−1/2
do this, we claim that the Frobenius norms of the rows of the matrix M are exchangeable,
(cid:1) = nE(cid:0)(cid:107)Mi(cid:107)2(cid:1) for any
and thus have the same expectation, which implies that E(cid:0)(cid:107)M(cid:107)2
F
i ∈ {1, . . . , n}. Applying Markov’s inequality, we therefore see that

1
ρ1/2n1/2

P(cid:0)(cid:107)Mi(cid:107) > t(cid:1) ≤

E(cid:0)(cid:107)Mi(cid:107)2(cid:1)
t2

E(cid:0)(cid:107)M(cid:107)2
nt2

F

(cid:1)

.

=

Now,

(cid:107)M(cid:107)F ≤ (cid:107)A − P(cid:107)(cid:107)VA − VPV(cid:62)
= O(cid:0)(cid:15)2k1/2 log(n)(cid:1)

PVA(cid:107)F (cid:107)V(cid:62)

A(cid:107)F

by Propositions 8 and 13. It follows that

(cid:16)

P

(cid:107)Mi(cid:107) > (cid:15)2k1/2 log(n)

n1/4

(cid:17)

= O(cid:0) 1
n1/2

(cid:1)

(cid:107)M(cid:107)2→∞ = O

(cid:16) (cid:15)2k1/2 log(n)
n1/4

(cid:17)

and thus

and

(cid:107)(A − P)(I − VPV(cid:62)

P)VAΣ−1/2

A (cid:107)2→∞ = O

(cid:16) (cid:15)2k1/2 log(n)
ρ1/2n3/4

(122)

(123)

(124)

(125)

(126)

(127)

(cid:17)

39

almost surely.

We must therefore show that the Frobenius norms of the rows of M are exchangeable. Let
QL ∈ O(n) and, for each r ∈ {1, . . . , k}, QR,r ∈ O (nr) be permutation matrices (where we
require that QR,r = QL if Y(r) = XGr with probability one for some matrix Gr ∈ Rd×dr ,
and similarly that QR,r = QR,s if Y(s) = Y(r)Gr,s with probability one for some matrix
Gr,s ∈ Rdr×ds), and let QR = QR,1 ⊕ · · · ⊕ QR,k. For any matrix G, let Rd(G) denote
the projection onto the subspace spanned by the right singular vectors corresponding to the
leading d singular values of G, and let R⊥
d (G) denote the projections onto the orthogonal
complements of this subspace.

Note that

Rd(P) = VPV(cid:62)

P and Rd(A) = VAV(cid:62)
A,

(128)

while for any permutation matrices QL ∈ O(n) and QR,r ∈ O(nr) we have

Rd

(cid:0)QLPQ(cid:62)

R

(cid:1) = QRVPV(cid:62)

PQ(cid:62)

R and Rd

(cid:0)QLAQ(cid:62)

R

(cid:1) = QRVAV(cid:62)

AQ(cid:62)
R.

(129)

For any commensurate pair of matrices G and H, deﬁne an operator

PR,d(G, H) = (G − H)R⊥

d (H)Rd(G)

and note that PR,d(A, P) = M, while

PR,d

(cid:0)QLAQ(cid:62)

R, QLPQ(cid:62)
R

(cid:1) = QL(A − P)Q(cid:62)
= QLMQ(cid:62)
R.

RQR(I − VPV(cid:62)

P)Q(cid:62)

RQRVAV(cid:62)

AQ(cid:62)
R

(130)

(131)

(132)

Our assumptions regarding the distribution of the latent positions ensure that the entries of
(cid:1),
the pair (A, P) have the same joint distribution as those of the pair (cid:0)QLAQ(cid:62)
R permutes the columns of each A(r) and P(r) separately. Therefore, the en-
since Q(cid:62)
tries of the matrix PR,d(A, P) have the same joint distribution as those of the matrix
(cid:1), which implies that M has the same distribution as QLMQ(cid:62)
R,
PR,d
and consequently the Frobenius norms of the rows of M have the same distribution as those
of QLM, which proves our claim.
Combining these results, we see that

R, QLPQ(cid:62)
R

R, QLPQ(cid:62)
R

(cid:0)QLAQ(cid:62)

(cid:107)R1,2(cid:107)2→∞ = O

(cid:16) (cid:15)2k1/2 log(n)
ρ1/2n3/4

(cid:17)

(133)

almost surely, as required.

The proof of the bound for the term R2,2 follows similarly, and culminates in showing that
the term

satisﬁes

N = (A − P)(cid:62)(cid:0)I − UPU(cid:62)

P

(cid:1)UAU(cid:62)

A

(cid:107)N(cid:107)2→∞ = O

(cid:16) (cid:15)2k1/2 log(n)
n1/4

(cid:17)

(134)

(135)

almost surely. The matrix N ∈ R(n1+...+nk)×n splits into k distinct matrices N(r) ∈ Rnr×n.
This time, we must show that the Frobenius norms of the rows of each N(r) are interchange-
able, from which a similar argument to our previous one will allow us to derive our desired
bound for (cid:107)N(cid:107)2→∞.
Note that for any r ∈ {1, . . . , k}

Rd

(cid:0)P(r)(cid:62)(cid:1) = UPU(cid:62)

P and Rd

(cid:0)A(r)(cid:62)(cid:1) = UAU(cid:62)
A,

(136)

40

while for any permutation matrices QL ∈ O(nr) and QR ∈ O(n) we have

(cid:0)QLP(r)(cid:62)

Rd

Q(cid:62)
R

(cid:1) = QRUPU(cid:62)

PQ(cid:62)

R and Rd

(cid:0)QLA(r)(cid:62)

Q(cid:62)
R

(cid:1) = QRVAV(cid:62)

AQ(cid:62)
R.

(137)

Also, PR,d

(cid:0)A(r)(cid:62)

, P(r)(cid:62)(cid:1) = N(r), while we ﬁnd that

PR,d

(cid:0)QLA(r)(cid:62)

Q(cid:62)

R, QLP(r)(cid:62)

Q(cid:62)
R

(cid:1) = QLN(r)Q(cid:62)
R,

(138)

and so it follows from a similar argument to before that the rows of N(r) are interchangeable,
and thus we obtain our desired bound.

(iii) Similarly to part (i), we see that

(cid:107)R1,3(cid:107)2→∞ ≤ (cid:107)UP(cid:107)2→∞(cid:107)U(cid:62)
≤ (cid:107)UP(cid:107)2→∞(cid:107)U(cid:62)
(cid:16) (cid:15) log1/2(n)
ρ1/2n

= O

(cid:17)

P(A − P)VPWΣ−1/2
A (cid:107)
P(A − P)VP(cid:107)F (cid:107)WΣ−1/2

A (cid:107)F

by Proposition 11 and Corollary 10.

(iv) Observe that

(cid:107)R1,4(cid:107)2→∞ ≤ (cid:107)R1,4(cid:107)F

≤ (cid:107)A − P(cid:107)(cid:107)VP(cid:107)F (cid:107)WΣ−1/2

A − Σ−1/2

P W(cid:107)F

= O

(cid:16) (cid:15)9/2k3/4 log3/2(n)
n

(cid:17)

by Propositions 8 and 15.

Proof of Theorem 2

Proof. We ﬁrst consider the left embedding XA. Observe that

XA − XPW = UAΣ1/2
= UAΣ1/2
= UAΣ1/2

A − UPΣ1/2
A − UPU(cid:62)
A − UPU(cid:62)

P W
PUAΣ1/2
PUAΣ1/2

A + UP(U(cid:62)
A + R1.

PUAΣ1/2

A − Σ1/2

P W)

Noting that UAΣ1/2

A = AVAΣ−1/2

A

and UPU(cid:62)

PP = P, we see that

XA − XPW = AVAΣ−1/2

A − UPU(cid:62)

= (A − P)VAΣ−1/2
= (A − P)VAΣ−1/2
= (I − UPU(cid:62)
= (I − UPU(cid:62)
= (A − P)VPWΣ−1/2
= (A − P)VPΣ−1/2

PAVAΣ−1/2

A + R1,1
PA − P)VAΣ−1/2
A − (UPU(cid:62)
P(A − P)VAΣ−1/2
A − UPU(cid:62)
P)(A − P)VAΣ−1/2
A + R1,1
P)(A − P)(cid:0)VPW + (VA − VPW)(cid:1)Σ−1/2

A + R1,1
A + R1,1

A + R1,1

A + R1,3 + R1,2 + R1,1

P W + R1,4 + R1,3 + R1,2 + R1,1.

Applying Proposition 19, we ﬁnd that

(cid:107)XA − XPW(cid:107)2→∞ = (cid:107)(A − P)VPΣ−1/2

P

(cid:107)2→∞ + O

≤ σd(P)−1/2(cid:107)(A − P)VP(cid:107)2→∞ + O

(cid:17)

(cid:16) (cid:15)2k1/2 log(n)
ρ1/2n3/4
(cid:16) (cid:15)2k1/2 log(n)
ρ1/2n3/4

(cid:17)

.

41

(139)

(140)

(141)

(142)

(143)

(144)

(145)

(146)

(147)

(148)

(149)

(150)

(151)

(152)

(153)

(154)

(155)

(156)

almost surely.

We condition on some set of latent positions. For any i ∈ {1, . . . , n}, j ∈ {1, . . . , d} and

r ∈ {1, . . . , k}, let

E(r)

ij =






nr(cid:88)

l=1

(cid:88)

l(cid:54)=i

(cid:0)A(r)

il − P(r)

il

(cid:0)A(r)

il − P(r)

il

(cid:1)v(r)

l

(cid:1)v(r)

l

if A(r) is bipartite

otherwise

and

F(r)

ij =






0

if A(r) is bipartite

P(r)

ii v(r)

i

otherwise,

where v(r) denotes the jth column of V(r)

P , so that

(cid:0)(A − P)VP

(cid:1)

ij =

k
(cid:88)

r=1

E(r)

ij −

k
(cid:88)

r=1

F(r)
ij .

(157)

(158)

(159)

The latter term is of order O(cid:0)(cid:15)ρk1/2(cid:1) (as can be seen by applying the Cauchy–Schwarz inequal-
ity) and thus can be discounted from our asymptotic analysis. The former is a sum of independent,
zero-mean random variables, with each individual term bounded in absolute value by |v(r)
|, and
thus we can apply Hoeﬀding’s inequality to see that

l

(cid:32)
(cid:12)
(cid:12)
(cid:12)

P

k
(cid:88)

r=1

(cid:33)

(cid:12)
(cid:12)
(cid:12) > t

E(r)
ij

(cid:32)

≤ 2 exp

4 (cid:80)k

r=1

−2t2
(cid:80)nr

l=1 |v(r)

l

(cid:33)

|2

= 2 exp

(cid:16) −t2
2

(cid:17)

.

(160)

Thus (cid:0)(A − P)VP

(cid:12)
(cid:12) = O(cid:0)log1/2(n)(cid:1)
almost surely by summing over all j ∈ {1, . . . , d}. Taking the union bound over all n rows then
shows that

ij = O(cid:0)log1/2(n)(cid:1) almost surely, and hence (cid:12)
(cid:0)(A − P)VP
(cid:12)

(cid:1)

(cid:1)

i

σd(P)−1/2(cid:107)(A − P)VP(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

,

almost surely and consequently that

(cid:107)XA − XL(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

(161)

(162)

almost surely by setting L = (cid:101)LW. The second bound follows from Corollary 17 and the fact that
(cid:107)AB(cid:107)2→∞ ≤ (cid:107)A(cid:107)2→∞(cid:107)B(cid:107). Integrating over all possible sets of latent positions gives the result.

A similar argument is used for the right embedding. Observe that

YA − YPW = VAΣ1/2
= VAΣ1/2
= VAΣ1/2

A − VPΣ1/2
A − VPV(cid:62)
A − VPV(cid:62)

P W
PVAΣ1/2
PVAΣ1/2

A + VP(V(cid:62)
A + R2,1.

PVAΣ1/2

A − Σ1/2

P W)

(163)

(164)

(165)

42

Noting that A(cid:62)UAΣ−1/2

A = VAΣ1/2

A and VPV(cid:62)

PP(cid:62) = P(cid:62), we see that

YA − YPW = A(cid:62)UAΣ−1/2

A − VPV(cid:62)

PA(cid:62)UAΣ−1/2

A + R2,1

A + R2,1

A + R2,1

= (A − P)(cid:62)UAΣ−1/2
= (A − P)(cid:62)UAΣ−1/2
= (I − VPV(cid:62)
= (I − VPV(cid:62)
= (A − P)(cid:62)UPWΣ−1/2
= (A − P)(cid:62)UPΣ−1/2

PA(cid:62) − P(cid:62))UAΣ−1/2
P(A − P)(cid:62)UAΣ−1/2
A + R2,1

A − (VPV(cid:62)
A − VPV(cid:62)
P)(A − P)(cid:62)UAΣ−1/2
P)(A − P)(cid:62)(UPW + (UA − UPW))Σ−1/2
A + R2,3 + R2,2 + R2,1

P W + R2,4 + R2,3 + R2,2 + R2,1.

A + R2,1

Applying Proposition 19 once more, we ﬁnd that

(cid:107)YA − YPW(cid:107)2→∞ = (cid:107)(A − P)(cid:62)UPΣ−1/2

P

(cid:107)2→∞ + O

(cid:16) (cid:15)2k1/2 log(n)
ρ1/2n3/4

(cid:17)

almost surely and consequently that

(cid:107)Y(r)

A − Y(r)

P W(cid:107)2→∞ ≤ σd(P)−1/2(cid:107)(A(r) − P(r))(cid:62)UP(cid:107)2→∞ + O

(cid:16) (cid:15)2k1/2 log(n)
ρ1/2n3/4

(cid:17)
.

almost surely.

If we condition on a set of latent positions, an identical argument to before shows that

σd(P)−1/2(cid:107)(A(r) − P(r))(cid:62)UP(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

,

and consequently that

(cid:107)Y(r)

A − YRr(cid:107)2→∞ = O

(cid:16) log1/2(n)
ρ1/2n1/2

(cid:17)

(166)

(167)

(168)

(169)

(170)

(171)

(172)

(173)

(174)

(175)

(176)

by setting Rr = (cid:101)RrW. Proposition 16 shows that (cid:101)L (cid:101)R(cid:62)
r = Λr, and the remaining bounds follow
from Corollary 17 as for the left embedding. As before, integrating over all possible sets of latent
positions gives the ﬁnal result.

Proof of Theorem 3

Proof. We ﬁrst consider the left embedding XA. Recall from the proof of Theorem 2 that

n1/2(XAL−1 − X) = n1/2(A − P)VPΣ−1/2

P

(cid:101)L−1 + n1/2R,

(177)

where the residual term R satisﬁes (cid:107)n1/2R(cid:107)2→∞ → 0 by Proposition 19 and our assumptions in
Section 2.1.

The ﬁrst of the right-hand terms may be rewritten as

n1/2(A − P)VPΣ−1/2

P

(cid:101)L−1 = n1/2

k
(cid:88)

r=1

(cid:0)A(r) − P(r)(cid:1)Y(r) (cid:101)RrΣ−1

P (cid:101)L−1

(178)

by splitting (A − P)VP into the individual terms (cid:0)A(r) − P(r)(cid:1)V(r)

P and noting that

V(r)

P Σ−1/2

P = Y(r)

P Σ−1

P = Y(r) (cid:101)RrΣ−1
P .

Consequently,

n1/2(XAL−1 − X)(cid:62)

i → n1/2

k
(cid:88)

r=1

( (cid:101)RrΣ−1

P (cid:101)L−1)(cid:62)(cid:2)(A(r) − P(r))Y(r)(cid:3)(cid:62)

i

= n(Λ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) )−1

k
(cid:88)

r=1

(cid:104) 1
n1/2

43

nr(cid:88)

(A(r)

ij − P(r)

ij )Λ(cid:15),rY(r)

j

j=1

(179)

(180)

(181)

(cid:105)

almost surely by Proposition 18.
j = ρ1/2υ(r)

Noting that Y(r)
large numbers, we see that

j

and that n(Λ(cid:15)Y(cid:62)YΛ(cid:62)

(cid:15) )−1 → 1

ρ ∆−1

Λ,Y almost surely by the law of

n1/2(XAL−1 − X)(cid:62)

i → ∆−1
Λ,Y

k
(cid:104)
(cid:88)

r=1

1
ρ1/2n1/2

nr(cid:88)

j=1

(A(r)

ij − P(r)

ij )Λ(cid:15),rυ(r)

j

(cid:105)

(182)

almost surely.

If A(r) is not bipartite, we may disregard the diagonal terms in our asymptotic analysis, as

each of the k such terms satisﬁes

(cid:13)
(cid:13)

almost surely.

1

ρ1/2n1/2 P(r)

ii Λ(cid:15),rυ(r)

i

(cid:13)
(cid:13) → 0

(183)

Assume ﬁrst that each of the matrices Y(r) is independent of the others. Conditional on ξi = x,

we have P(r)

ij = ρx(cid:62)Λ(cid:15),rυ(r)

j

, and so

1
ρ1/2n1/2

(cid:88)

(A(r)

ij − P(r)

ij )Λ(cid:15),rυ(r)

j

(184)

is a scaled sum of independent, identically distributed, zero-mean random variables, each with
covariance matrix given by

j

E(cid:2)x(cid:62)Λ(cid:15),rυr(1 − ρx(cid:62)Λ(cid:15),rυr) · Λ(cid:15),rυrυ(cid:62)

r Λ(cid:62)
(cid:15),r

(cid:3)

which implies (by the multivariate central limit theorem) that

1
ρ1/2n1/2

(cid:88)

j

(A(r)

ij − P(r)

ij )Λ(cid:15),rυ(r)

j → N (cid:0)0, crΛrΣ(r)

Y (x)Λ(cid:62)

r

if (cid:15)r = 1, and vanishes otherwise, and thus we ﬁnd that
i → N (cid:0)0, ∆−1

n1/2(XAL−1 − X)(cid:62)

Λ,Y Λ∗ΣY (x)Λ(cid:62)

∗ ∆−1
Λ,Y

(cid:1)

(cid:1)

(185)

(186)

(187)

almost surely, where ΣY (x) = c1Σ(1)
by integrating over all possible values of x ∈ X .

Y (x) ⊕ · · · ⊕ cκΣ(κ)

Y (x). We deduce the Central Limit Theorem

The same statement holds even if there is dependence between any of the matrices Y(r). Indeed,
suppose that the matrices Y(r) are dependent for all r ∈ K, for some subset K of {1, . . . , k}. Then
for any i and j,

1
ρ1/2n1/2

(cid:88)

(cid:88)

j

r∈K

(A(r)

ij − P(r)

ij )Λ(cid:15),rυ(r)

j

(188)

is a sum of independent, identically distributed, zero-mean random variables, for which the covari-
ance matrices

(cid:104) (cid:88)

E

r,s∈K

reduce to

(A(r)

ij − P(r)

ij )(A(s)

ij − P(s)

ij ) · Λ(cid:15),rυ(r)

j υ(s)

j

(cid:105)

(cid:62)

Λ(cid:62)
(cid:15),s

(cid:104)(cid:88)

E

r∈K

x(cid:62)Λ(cid:15),rυr(1 − ρx(cid:62)Λ(cid:15),rυr) · Λ(cid:15),rυrυ(cid:62)

r Λ(cid:62)
(cid:15),r

(cid:105)

since the terms

(cid:104)
E

(A(r)

ij − P(r)

ij )(A(s)

ij − P(s)

ij ) · Λ(cid:15),rυ(r)

j υ(s)

j

(cid:105)

(cid:62)

Λ(cid:62)
(cid:15),s

vanish if r and s are not equal.

44

(189)

(190)

(191)

Similarly, for the right embedding we observe that

n1/2(Y(r)

A R−1

r − Y(r)) = n1/2(A(r) − P(r))(cid:62)UPΣ−1/2

P

(cid:101)R−1

r + n1/2R,

(192)

where the residual term R satisﬁes (cid:107)n1/2R(cid:107)2→∞ → 0.

Again, we may rewrite the ﬁrst of the right-hand terms, obtaining the expression

n1/2(A(r) − P(r))(cid:62)UPΣ−1/2

P

(cid:101)R−1

r = n1/2(A(r) − P(r))(cid:62)X(cid:101)LΣ−1

P (cid:101)R−1

r

by noting that UPΣ−1/2

P = X(cid:101)LΣ−1

P , and consequently ﬁnd that

n1/2(Y(r)

A R−1

r − Y(r)) → n1/2((cid:101)LΣ−1

r )(cid:62)(cid:2)(A(r) − P(r))(cid:62)X(cid:3)(cid:62)

i

P (cid:101)R−1
(cid:15),r (X(cid:62)X)−1(cid:104) 1

n1/2

= nΛ−1

n
(cid:88)

(A(r)

ji − P(r)

ji )Xj

(cid:105)

(193)

(194)

(195)

almost surely by Proposition 18.

j=1

Noting that Xj = ρ1/2ξj and that n(X(cid:62)X)−1 → 1

ρ ∆−1

X almost surely by the law of large

numbers, we see that

n1/2(Y(r)

A R−1

r − Y(r)) → Λ−1

r ∆−1
X

(cid:104)

1
ρ1/2n1/2

n
(cid:88)

j=1

(A(r)

ji − P(r)

ji )ξj

(cid:105)

(196)

almost surely.

As before, if A(r) is not bipartite we may disregard the diagonal term in our asymptotic analysis,

as it satisﬁes

almost surely.

(cid:13)
(cid:13)

1

ρ1/2n1/2 P(r)
ii ξi

(cid:13)
(cid:13) → 0

Conditional on υ(r)

i = y, we have P(r)

ji = ρξ(cid:62)

j Λ(cid:15),ry, and so

1
ρ1/2n1/2

(cid:88)

(A(r)

ji − P(r)

ji )ξj

(197)

(198)

is a scaled sum of independent, zero-mean random variables, each with covariance matrix given by

j

E(cid:2)ξ(cid:62)Λ(cid:15),ry(1 − ρξ(cid:62)Λ(cid:15),ry) · ξξ(cid:62)(cid:3)

which implies (by the multivariate central limit theorem) that, provided (cid:15)r = 1,

1
ρ1/2n1/2

(cid:88)

(A(r)

ji − P(r)

ji )ξj → N (cid:0)0, Σ(r)

X (y)(cid:1),

(199)

(200)

and thus we ﬁnd that, provided (cid:15)r = 1,

j

n1/2(cid:0)Y(r)

i → N (cid:0)0, Λ−1
almost surely. We deduce the Central Limit Theorem by integrating over all possible values of
y ∈ Yr.

r − Y(r)(cid:1)(cid:62)

X (y)∆−1

X Σ(r)

X Λ−(cid:62)

A R−1

r ∆−1

(201)

(cid:1)

r

45

