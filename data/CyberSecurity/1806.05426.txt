8
1
0
2

n
u
J

4
1

]

C
H
.
s
c
[

1
v
6
2
4
5
0
.
6
0
8
1
:
v
i
X
r
a

How to Design Browser Security and Privacy Alerts

Lynsay A. Shepherd
School of Design and Informatics
Abertay University
Dundee, United Kingdom
lynsay.shepherd@abertay.ac.uk

Karen Renaud
School of Design and Informatics
Abertay University
Dundee, United Kingdom
k.renaud@abertay.ac.uk

ABSTRACT
It is important to design browser security and privacy alerts
so as to maximise their value to the end user, and their ef-
ficacy in terms of communicating risk. We derived a list of
design guidelines from the research literature by carrying
out a systematic review. We analysed the papers both quan-
titatively and quantitatively to arrive at a comprehensive set
of guidelines. Our findings aim to to provide designers and
developers with guidance as to how to construct privacy and
security alerts. We conclude by providing an alert template,
and highlighting its adherence to the derived guidelines.

ACM Reference Format:
Lynsay A. Shepherd and Karen Renaud. 2018. How to Design Browser
Security and Privacy Alerts. In Proceedings of , AISB 2018 Symposium,
5th April 2018 (AISB’18), 9 pages.

1 INTRODUCTION
It is non-trivial to design effective alerts in the security and
privacy domain.

Browser designers do their best to inform users about
security-related aspects as they surf the web. Owing to the
number of potential pitfalls, this means end users can ef-
fectively be bombarded with security alerts [2], and users
often ignore them [8, 33]. Developers sometimes make un-
founded assumptions about the background knowledge of
alert recipients [30] making them incomprehensible.

Privacy alerts are not perfect either [35]. Users are often
overwhelmed by these alerts because there are too many,
[21] or because they do not know what actions to take as a
consequence [55].

This diminishes the impact of alerts, and leaves users vul-
nerable to unknowingly carrying out actions which will
compromise their privacy or security.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. The copyright for this
paper remains with the authors.

AISB’18, 5th April 2018, AISB 2018 Symposium
©
.

Traditional usability guidelines cannot necessarily be used
“as-is” in the security and privacy context. This is because
neither privacy nor security are the end user’s primary task
[11, 32]. Alerts interrupt the user’s pursuit of their primary
goal and are thus often perceived to be a nuisance [5]. We
therefore need specific guidelines to inform alert design in
the security and privacy context.

Much has been written about alert design, as can be seen
from the following section. Yet one can hardly expect busy
deadline-driven software engineers, the very people who are
producing these alerts, to keep up with the latest research in
the area.

We therefore carried out a systematic literature review
in order to consolidate all the published guidelines into one
coherent list (Section 3). Previously, Bauer et al. [13] pre-
sented a list of warning design guidelines in 2013. Our work
provides an updated, more comprehensive, list of guidelines,
which are specifically tailored towards browser-based alerts.
Having derived a comprehensive set of deadlines, we re-
alised that merely providing a list of guidelines is not an
optimal way of supporting designers. Luger and Rodden [38]
argue that such lists of guidelines are unlikely to be followed
in the pressured environment of software development and
design. Moreover, some of the published guidelines conflict
[39], which is unhelpful.

To make our consolidated guidelines as helpful as possible,
we decided to convey the spirit, rather than the letter, of the
guidelines in the form an example alert template (Section
4). This conveys the “how” of alert design, rather than the
“what”, as encapsulated in a linear set of alert guidelines.

Future work is explored in Section 5, and we conclude in

Section 6.

2 INFORMING END USERS
First we clarify the nomenclature used in this paper. We
then provide an overview of the human in the loop model of
human information processing. We conclude by explaining
the difference between the foundational security and privacy
concepts.

 
 
 
 
 
 
AISB’18, 5th April 2018, AISB 2018 Symposium

Lynsay A. Shepherd and Karen Renaud

2.1 Nomenclature
We investigated guidelines that inform the design of warn-
ings, alerts, notifications, prompts or provision of feedback.
The underlying concept is the same: provision of important
information to an end user that the system considers he or
she should be apprised of. We shall use the term ‘alert’ as
a unifying term to represent all the terminologies used by
papers cited in this paper.

2.2 Human Information Processing
Wogalter and Mayhorn [62] explain that warnings (what we
call alerts) are a type of risk communication. Wogalter [63]
explains that warnings have two purposes: (1) communicate
information, and (2) reduce unwise behaviours. To achieve
these aims the warnings have to be designed carefully. The
Web Content Accessibility Guidelines1 can also be applied
to alerts [7] i.e. that they should be perceivable, operable,
understandable and robust.

Shannon [56] and Lasswell [36] both proposed models of
human communication which help us to understand how
humans process alerts.

Wogalter, DeJoy, and Laughery [64] developed the C-HIP
model in the context of warning research. Their model builds
on the work of Shannon and Lasswell and can be considered
to be somewhat unrealistic because it does not include a noise
component. In a world of noisy communication such a model
is incomplete. Cranor [17] proposed a human-in-the-loop
framework which is more comprehensive and reflects the
factors impacting communications in the context of privacy
and security alerts.

Figure 1: Cranor’s Human in the Loop Security Frame-
work [17] ( Layout tweaked due to space constraints,
and acronyms added for later reference).

1https://www.w3.org/TR/WCAG21/

2.3 Security vs. Privacy
It is important to realise that security and privacy are funda-
mentally different concepts. Skinner et al. [58] argue that a
secure information system does not necessarily imply that
privacy will be preserved in the system. Gritzalis and Lam-
brinoudakis [24] make the distinction between privacy and
security as follows: “a piece of information is secure when its
content is protected, it is private when the identity of the owner
is protected’. As an example, they refer to a company that
collects customer information and stores it encrypted. This
ensures that the information is secured. Yet the same com-
pany may sell the information to another company, thereby
violating the owners’ privacy.

Bambauer [12, p. 667] explains: “Privacy discourse involves
difficult normative decisions about competing claims to legiti-
mate access to, use of and alteration of information.” Security,
on the other hand, is “the protection of information and in-
formation systems from unauthorized access, use, disclosure,
disruption, modification, or destruction in order to provide con-
fidentiality, integrity, and availability.” [20].

Privacy and security are clearly distinct concepts, but their
alerts still share some common characteristics in that they
exist to tell the end user to something important. We there-
fore present three lists of guidelines: (1) generic, (2) privacy-
specific, and (3) security-specific.

3 CONTEMPLATING THE ALERT

LITERATURE

We decided to focus on browser alerts firstly because of the
popularity of web applications [41] such as email, claimed to
be the most popular application in use [6] and video stream-
ing [31]. The second reason is that browsers run on all de-
vices, ranging from Desktops to Smartphones. We felt that
our guidelines could be maximally useful to developers if we
focused on guidelines for browser alerts.

The literature search was carried out between November

and December 2017 as follows:

Databases: ACM, Springer, Web of Science, Scopus, IEEE,
and then Google Scholar to identify publications that did not
appear in the databases.

Keywords: ‘design guidelines’ and ‘browser’and (‘secu-
rity’ or ‘privacy) and (‘feedback’ or ‘warnings’ or ‘alert’ or
‘notification’)

Time Range: 2007—2017
Exclusion Criteria: Patents, citations, non-peer reviewed,

not English or unobtainable.

3.1 Quantitative analysis
One particular measure of activity in a research field is the
number of papers published over the decade in question.
Figure 2 shows the number of papers, and also how many

Communication COMMUNICATION IMPEDIMENTS      Interference Environmental Stimuli INTENTIONS      Attitudes & Beliefs PERSONAL VARIABLES       Knowledge & Experience Demographics & Personal Characteristics Capabilities COMMUNICATION DELIVERY      Attention Maintenance Attention Switch COMMUNICATION PROCESSING      Knowledge Acquisition Comprehension APPLICATION      Knowledge Retention Knowledge Transfer Behavior HUMAN RECEIVER CI CD CP PV IN AP Motivation How to Design Browser Security and Privacy Alerts

AISB’18, 5th April 2018, AISB 2018 Symposium

Database

Papers

Papers

Returned

Excluded

Papers
Analysed

Scopus
ACM
Springer
Web of Science
Google Scholar
IEEE
Total

2
12
214
0
181
79

1
9
199
0
134
73

1
3
15
0
47
6
72

times the papers have been cited up to the date we carried
our our literature review.

It is interesting to note that 25 of the 72 papers had no
citations at all. The average number of citations is 7.38, the
mode is 0, and the median is 2. Only four of the papers had
been cited by more than 50 other publications. The top two
most-cited publications appeared in conferences and the
third most-cited publication appeared in a journal.

Figure 3 shows the citations for papers in each of the paper
focus areas. The top cited paper is a security paper, with the
next two most-cited papers being in the privacy area.

Figure 2: Number of Papers and Citation Numbers per
Year

Figure 4 demonstrates the nature of the references we
found. It is interesting that so many of the guidelines appear
in Masters and PhD theses (18). Of these, 10 were never cited.
The most-cited thesis, a PhD, was cited 13 times. Eight of
the 10 PhDs had never been cited. The average number of
citations across all theses was 2.47, but the mode and median
are both 0. This suggests that guidelines published in these
formats have not made a significant impact on the field.

Figure 3: Number of Citations per Paper (by paper fo-
cus)

Figure 4: Publication Types

3.2 Qualitative analysis
We analysed the guidelines using Thematic Analysis [25].
This approach supports pinpointing, examining, and record-
ing themes that emerge from the papers. We commenced by
familiarising ourselves with the papers. We then generated
initial codes and searched for themes as we collated these
codes. We then reviewed the themes, defining and naming
them.

Some of the guidelines applied equally to privacy and
security, but others were clearly specific to either privacy or
security. This is not unexpected because, as argued earlier,
privacy and security are fundamentally different concepts.
We shall thus present generic guidelines first, then concept-
specific guidelines separately, reflecting the fundamental
differences between the two concepts.

3.3 Generic Guidelines
We report first on the generic themes that coincide with
Cranor’s framework [17], depicted in Figure 1.

Communication Impediments (CI). Here mitigations to ame-
liorate the effects of impediments should be included. For

2 3 3 5 7 6 13 8 10 7 8 39 1 13 38 201 59 331 55 170 6 2 0 2 4 6 8 10 12 14 0 50 100 150 200 250 300 350 '2007' '2008' '2009' '2010' '2011' '2012' '2013' '2014' '2015' '2016' '2017' Number of Papers Number of Citations Publication Year Number of Papers Citations -20	0	20	40	60	80	100	120	140	160	180	0	10	20	30	40	50	60	70	General	Mobile	Privacy	Security	Conference 53% Journal 9% EU 7% PhD Thesis 15% TechRep 3% Masters Thesis 10% Chapter 3% AISB’18, 5th April 2018, AISB 2018 Symposium

Lynsay A. Shepherd and Karen Renaud

example, provide users with the means to recover from hasty
decisions [29].

Personal Variables & Capabilities (PV). Some users may
have low numeracy levels. Instead of providing them with
figures regarding risk, perhaps utilise emotions or mood.
Similarly, users may have different understanding of visuals
[45]. Only by testing can the efficacy of these be confirmed.

Communication Delivery (CD). Human attention is a finite
resource [16]. Do not squander it, and do not expect the
recipient to give you any as a matter of course.

CD1: Modality — Murphy-Hill & Murphy [42, 61] suggest
that pictures be used to ease communication. When deliv-
ering warning alerts, users prefer text and graphical-based
information, rather than auditory information[15].

On the other hand, Goldberg [23] suggests that text should
be used exclusively. This might be to maximise accessibil-
ity, and the middle road would be to ensure that alt text is
provided for all images.

Work conducted by Anderson et al. [9, 10] notes the use
of polymorphism in warning alerts to reduce habituation.
CD2: Timing — If alerts appear too often the recipients
may get annoyed and start ignoring them [4, 42]. Alerts
should be issued only when necessary, to avoid irritating the
user [60].

Westermann [61] found that people were most annoyed
by alerts if they are busy with a task, especially when the
task is complex, but less annoyed in between tasks or when
they were waiting for something.

It is important to prioritise the warnings so that only the
most important ones merit immediate interruption [4, 61].
CD3: Location — Westermann [61] considers alert loca-
tion to be important. Many browsers, for example, display
alerts either in the address bar (padlock and the word Secure
in Chrome), or at the bottom of the screen. These are eas-
ily missed by users. If we want people to notice the alerts
it ought to be displayed where they are currently focusing
their attention. In particular [4] point out that passive toolbar-
located warnings are less effective than full page warnings
that users are less likely to miss. Pala and Wang[48] also
suggest alerts should be placed where the users are focusing
their attention. In the study conducted by Chen, Zahedi, and
Abbasi, users preferred alerts to be placed in the centre of
the screen [15].

CD4: Appearance — Kelley [32] provides a number of
recommendations: (1) the alert should be surrounded by
a box to clearly demarcate it; (2) provide a title to assist
speedy recognition. . Be careful with colour use so as not to
disadvantage those with colour deficiencies [23]. A neutral
grey colour can be used for the background of alerts, as it is
unlikely to annoy the user [60].

Communication Processing (CP).
CP1: Make Essential Information Pertinent — Lin [37]
suggests highlighting the most important information. Keep
initial details about the risk to a minimum [45, 46]. Only the
most important information should be displayed to the user
immediately with links to more information should they
want it [60]. The granularity of information is important.
Too-wordy information will not be read, and information
that is too condensed can be obscure. In providing alerts and
alerts a balance must be found [11].

CP2: Maximise Understandability — [32, 42, 54, 61]
and consistency [7, 42]. Provide concrete explanations [47].
The importance of this aspect is confirmed by [44]. Keep
explanations simple [37]. Acronyms and jargon should be
avoided and the use of meaningful terminology encouraged
[11, 32, 54, 57]. Separate semantically different kinds of in-
formation [32, 59].

Text presented should be easy for users to comprehend
[60]. Short, simple sentences, devoid of complex grammat-
ical structures should be used. The use of technical words
should be avoided (i.e. words listed in the indexes of IT secu-
rity books)[18, 26, 27, 48]. Unclear alerts are more likely to
be ignored, and consideration should be given to the exact
meanings of words used [45].

Work by Bravo-Lillo et al. . investigated the use of re-
designed warning alerts. Longer warning alerts performed
poorly in user testing, suggesting users may have become
confused [14]. Although existing work highlights shorter
alerts are most effective at communicating security warn-
ings to the user, the challenge of delivering such alerts whilst
providing the user with an understanding has been acknowl-
edged [22].

Application (AP).
AP1: Be Specific — Bravo-Lillo et al. [14] state that “to be
successful, warnings should both motivate a user to respond,
and help users understand the risk, in that order”.

Always tell the users what actions to take, if indeed they

should take action. [7, 29].

AP2: End Goal — Consider the way in which you want
to communicate a risk to the user e.g. is the alert to draw
them away from a risky situation, or is the alert to help users
understand the risk [45]?

AP3: Effort does not Deter — Akhawe and Felt [4] explain
that designers should not use the number of clicks required
to bypass a warning to deter users. Their study showed that
users were not sensitive to the number of clicks once they
had made a decision.

Intentions (IN).

It is important to note that delivering warnings is worthwhile.
Silic et al. [57] found that people definitely took note of
displayed warning messages, suggesting that they thought

How to Design Browser Security and Privacy Alerts

AISB’18, 5th April 2018, AISB 2018 Symposium

about the information before making the decision to proceed.
If people are reading and thinking about messages, these
messages have a chance of changing attitudes and beliefs.

Vasalou [59] says alerts should give recipients “space for
interpretation”, so that they can interpret the information as
it applies to themselves personally.

Phrasing of alerts could be personalised, depending on
the skill level of the user, and their experience [15, 45, 46].
Personalised alerts were said to be successful when utilised
to inform users about two-factor authentication, and bullet-
points can be used to aid clarity of information presented
[51].

It is important for the user to retain a level of control
[29, 59, 65]. Schaub et al. [53] distinguish between three lev-
els of user control: (1) blocking, non-blocking and decoupled.
A designer has to decide whether the user has to acknowl-
edge the message (blocking) or not (non-blocking), whether
they can defer it (decoupled), or whether it will expire after
displaying for a certain period of time [42].

Users should be provided with the option to respond to
a risk they have been alerted to, and helped to visualise
potential consequences [45]. Work by Volkamer et al. [60]
concurs that the potential consequences of a risk should be
conveyed to the user, along with potential recommendations.
Make sure the user can easily get in touch with someone
to ask about warnings [23]. Information should be conspicu-
ously placed so worried users will be able to get help [29].

3.4 Privacy-Specific Guidelines
Allow users to make privacy choices that are (1) meaningful,
(2) informed, (3) timely [16].

P-CI: Inspire Trust — Trust should be deliberately built
and maintained [42] by framing the privacy alert very care-
fully [3]. Rather counter-intuitively, privacy alerts should not
provide justifications for information requests. Researchers
report that justifications potentially reduce the end-user’s
trust in the system [1, 3, 34, 50].

P-PV: Privacy Expectations — Lin [37] points out that
users have different privacy expectations, and that an alert
interface should reflect this reality.

P-CD: Specificity — Ensure that the sensitivity of the data

is communicated go to the user [43].

P-IN: Enhance Control — Ensure that control resides
with the user [43]. Do not merely report that some privacy
invasion has occurred: allow the user to control disclosure.
It is necessary to balance interruptions and ensuring that
the user retains a sense of control [16].

People have different levels of privacy concerns, and the
alerts should afford users the level of control matching their
personal privacy concern.

3.5 Security-Specific Guidelines
Herzog and Shahmehri highlight the importance of security
features in applications, stressing that “security is rarely the
primary user task” [28].

S-CI: Context-Sensitive Help — Constantly visible con-
text sensitive help may prove useful in helping the user
understand security. Help may be provided via the use of
an agent [28]. The user should be provided with the option
to find further information in a contextually-aware setting
[46, 48].

S-CD1: Provide Justification — The user needs to know

why the alert is being provided [42].

Provide information as to whether a component is secure
or insecure. By displaying this information in either case,
this provides a consistent interface for the user [48]. Ensure
the current state of the system is displayed to the user [46].

S-CD2: Colour —
Research regarding two-factor authentication suggests
the use of blue as a peaceful colour. Red might indicate an
incident has occurred [15, 51]. Felt et al. [22] suggest util-
ising “opinionated design”. For example, make the “correct”
response the more visually appealing option e.g. the button
should have a high contrast level against the background.
Others have utilised green as colour, noting that it is seen as
safe. Whilst users should be given options regarding how to
proceed with their tasks, it has been suggested that placing
the “correct” option in green serves to guide users towards
the safe choice [60].

Where colourblind users may have issues with warnings,
the use of secondary information (icons) aims to convey the
same message [60].

S-CD3: Graphics — In one study, participants felt the
inclusion of graphics in an alert about two-factor authenti-
cation conveyed a tone which was less serious, and suspi-
cious [51]. Conversely, other studies conclude graphics are
required in alerts, to convey reassurance, draw attention,
and to reduce cognitive effort [60]. This is a prime example
of a conflicting set of guidelines.

Eargle [19] suggests that facial expressions could be used
to convey threat levels in security alerts but this has not been
confirmed by any other studies in our studied group.

S-IN: Control Level — If a security issue is detected on
a page, users would prefer the security alert to block them
from visiting a malicious website [15]. Other research stated
the final security decisions should be left to the user, though
users should be provided with alternative options on how to
proceed with their task [60].

4 INFORMING DESIGNERS
The previous section provided a list of recommendations for
designing alerts (Figure 5). However, as pointed out by [52],

AISB’18, 5th April 2018, AISB 2018 Symposium

Lynsay A. Shepherd and Karen Renaud

and confirmed by [38], designers have difficulty benefiting
from these kinds of flat lists of guidelines.

Figure 5: Consolidated Guidelines

Luger and Rodden’s [38] designers spoke about the value
of examples in encapsulating the lessons of design guidelines
in a more palatable format. We will thus present a template
example of an alert for the benefit of designers, extending
the initial template produced by [13]. We thus present an
alert template in Figure 6 and explain how it satisfies the
guidelines.

Figure 6: Example alert template

Generic Guidelines: The template contains both an icon,
and text to communicate the contents of the alert (CD1). The
alert has a border and a headline title, along with the use of
colours and icons. The background colour of the template is
neutral (CD4). The text explaining the alert should be clear,
specific, and easy to understand, requiring minimal cognitive
effort (CP1, CP2, AP1). If a user would like to find out more
information, they should be presented with the opportunity
to access this, along with relevant contact details.

Privacy Guidelines: If the alert is being used to notify
the user about privacy, the sensitivity of the information

being dealt with should be clearly communicated to the user
(P-CD). Users are provided with a choice in the alert, ensuring
they remain in control (P-IN).

Security Guidelines: The text explaining the alert should
justify why it is being displayed (S-CD1). The safe choice but-
ton on the template alert is more visually appealing than the
unsafe choice, and it clearly contrasts with the background
of the alert (S-CP). Users are provided with a choice in the
alert, ensuring they remain in control (S-IN). Users should be
presented with the option to access context-sensitive help.
Colour and graphics should be used to aid in communicat-
ing the role of the alert, ensuring colourblind users are not
placed at a disadvantage (S-CD2, S-CD3).

Template Summary: Graphics and text are used to com-
municate the nature of the alert (CD1, CD4, S-CD2, S-CD3).
A headline title and a neutral background are used (CD4).
The text explaining the alert should be clear, specific, and
easy to understand, requiring minimal cognitive effort (CP1,
CP2, AP1, PC-D, S-CD1). Users should have the opportunity
to access further information, and relevant contact details
(CP1), along with context-sensitive help (S-CI). Users are
provided with a choice in the alert, ensuring they remain in
control (P-IN, S-IN). The safe choice button is more visually
appealing than the unsafe choice, clearly contrasting with
the background of the alert (S-CP).

Development Good Practice
Creating a well-designed environment can aid in establish-
ing trust [40]. Moreover, it is important to ensure that peo-
ple are receptive to alerts [61]. The best way to confirm
both trustworthiness and alert receptiveness is by means of
thorough testing [29]. Options are A/B testing in the wild,
controlled experiments, field studies [4], or case studies post-
deployment [42].

5 FUTURE WORK
The systematic literature review identified a lack of research
surrounding the optimal placement of security and privacy
alerts within a the web browser. Whilst work carried out by
Chen, Zahedi, and Abbasi [15] showed users preferred alerts
in the centre of the screen, usability studies have shown there
are a variety of patterns users exhibit when browsing web
content [49]. This suggests that further research is required
into the optimal placement of security and privacy alerts.

It is also interesting to note from Figure 5 that there are
no security or privacy-specific guidelines in terms of Com-
munication Processing or Application. These are certainly
areas for further investigation.

Several guidelines gathered from literature conflict, and
this issue has been highlighted by other guideline papers
[52]. Previous research has acknowledged that “Not all best

 COMMUNICATIONDELIVERYCOMMUNICATION PROCESSINGAPPLICATION AP 1-3  CD 1-4  CP 1-2  P-CD S-CD 1-3  INTENTIONSIN  P-IN S-IN COMMUNICATIONIMPEDIMENTSCI  P-CI S-CI PERSONALVARIABLESPV  P-PV How to Design Browser Security and Privacy Alerts

AISB’18, 5th April 2018, AISB 2018 Symposium

practices can be simultaneously satisfied”; therefore, trade-
offs must occur [22]. Masip et al. [39] have investigated the
development of a design process to assist with design choices
when there are potentially conflicting user interface guide-
lines. In the future, we plan to develop a methodology for
prioritising the guidelines to support security and privacy
alert design.

6 CONCLUSION
The systematic review process highlighted a large proportion
of the work found online relating to alerts were sourced
from student theses (both at Masters and PhD level). Whilst
conducting the analysis process, it became clear that some
alert guidelines were developed for security, and others were
developed for privacy. These seemed, in many cases, to be
fundamentally different, suggesting that different guidelines
are required for these two distinct areas.

We publish this work as a first attempt to provide guid-
ance to designers and developers who need to incorporate
alerts into their systems. In the future, we seek to prioritise
the guidelines, addressing the issue of potential conflicts,
and with feedback from practitioners, iteratively refine the
guideline list.

REFERENCES
[1] M Aagaard. 2013. How Privacy Policy Affects Sign-Ups — Surprising

Data From 4 A/B Tests. (2013). ContentVerve.com.

[2] Yasemin Acar, Sascha Fahl, and Michelle L Mazurek. 2016. You are not
your developer, either: A research agenda for usable security and pri-
vacy research beyond end users. In Cybersecurity Development (SecDev),
IEEE. IEEE, 3–8.

[3] Idris Adjerid, Alessandro Acquisti, Laura Brandimarte, and George
Loewenstein. 2013. Sleights of privacy: Framing, disclosures, and the
limits of transparency. In Proceedings of the Ninth Symposium on Usable
Privacy and Security. ACM, 9.

[4] Devdatta Akhawe and Adrienne Porter Felt. 2013. Alice in Warning-
land: A Large-Scale Field Study of Browser Security Warning Effec-
tiveness.. In USENIX Security Symposium, Vol. 13.

[5] Eirik Albrechtsen. 2007. A qualitative study of users’ view on infor-

mation security. Computers & Security 26, 4 (2007), 276–289.

[6] Saad Alharbi and Dimitrios Rigas. 2008. Graphical browsing of email
data: An empirical investigation. In Information Technology: New Gen-
erations, 2008. ITNG 2008. Fifth International Conference on. IEEE, 495–
499.

[7] Leonelo Dell Anhol Almeida and Maria Cecília Calani Baranauskas.
2010. Merging Technical Guidelines for Accessible Web Content with
Universal Design Principles. Technical Report IC-10-020.

[8] Bonnie Anderson, Tony Vance, Brock Kirwan, David Eargle, and Seth
Howard. 2014. Users aren’t (necessarily) lazy: using neuroIS to ex-
plain habituation to security warnings. In Thirty Fifth International
Conference on Information Systems. Auckland.

[9] Bonnie Brinton Anderson, C Brock Kirwan, Jeffrey L Jenkins, David
Eargle, Seth Howard, and Anthony Vance. 2015. How polymorphic
warnings reduce habituation in the brain: Insights from an fMRI study.
In Proceedings of the 33rd Annual ACM Conference on Human Factors
in Computing Systems. ACM, 2883–2892.

[10] Bonnie Brinton Anderson, Anthony Vance, C Brock Kirwan, Jeffrey L
Jenkins, and David Eargle. 2016. From warning to wallpaper: Why the
brain habituates to security warnings and what can be done about it.
Journal of Management Information Systems 33, 3 (2016), 713–743.
[11] Rebecca Balebako, Jaeyeon Jung, Wei Lu, Lorrie Faith Cranor, and Car-
olyn Nguyen. 2013. Little brothers watching you: Raising awareness
of data leaks on smartphones. In Proceedings of the Ninth Symposium
on Usable Privacy and Security. ACM, 12.

[12] Derek E Bambauer. 2013. Privacy versus security. J. Crim. L. & Crimi-

nology 103 (2013), 667.

[13] Lujo Bauer, Cristian Bravo-Lillo, Lorrie Cranor, and Elli Fragkaki. 2013.
Warning Design Guidelines. Technical Report CMU-CyLab-13-002.
Carnegie Mellon University.

[14] Cristian Bravo-Lillo, Lorrie Faith Cranor, Julie S. Downs, Saranga
Komanduri, and Manya Sleeper. 2011. Improving Computer Security
Dialogs. In Human-Computer Interaction - INTERACT 2011 -13th IFIP
TC 13 International Conference, Lisbon, Portugal, September 5-9, 2011,
Proceedings, Part IV. 18–35.

[15] Yan Chen, Fatemeh Zahedi, and Ahmed Abbasi. 2011. Interface Design
Elements for Anti-phishing Systems. In Proceedings of the 6th Inter-
national Conference on Service-oriented Perspectives in Design Science
Research (DESRIST’11). Springer-Verlag, Berlin, Heidelberg, 253–265.
[16] Jessica Helena Colnago. 2016. Privacy agents in the IoT: considerations
on how to balance agent autonomy and user control in privacy decisions.
Ph.D. Dissertation. Universidade Federal de São Carlos.

[17] Lorrie Faith Cranor. 2008. A framework for reasoning about the human

in the loop. UPSEC 8, 2008 (2008), 1–15.

[18] Xun Dong, John A. Clark, and Jeremy L. Jacob. 2010. Defending the
weakest link: phishing websites detection by analysing user behaviours.
Telecommunication Systems 45, 2-3 (2010), 215–226.

[19] David W. Eargle. 2017. Security Messages: Or, How I Learned to
Stop Disregarding and Heed the Warning. (August 2017). http:
//d-scholarship.pitt.edu/31614/

[20] Richard Kissel (Ed.). 2013. Glossary of Key Information Security Terms.

Technical Report NISTIR 7298 Revision 2.

[21] Serge Egelman, Lorrie Faith Cranor, and Jason Hong. 2008. You’ve
been warned: an empirical study of the effectiveness of web browser
phishing warnings. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems. ACM, 1065–1074.

[22] Adrienne Porter Felt, Alex Ainslie, Robert W. Reeder, Sunny Consolvo,
Somas Thyagaraja, Alan Bettes, Helen Harris, and Jeff Grimes. 2015.
Improving SSL Warnings: Comprehension and Adherence. In Proceed-
ings of the Conference on Human Factors and Computing Systems.
[23] Jeffrey S Goldberg. 2009. State of Texas Municipal Web Sites: A Descrip-
tion of Website Attributes and Features of Municipalities with Populations
Between 50,000-125,000. Master’s thesis. Public Administration.
[24] Stefanos Gritzalis and Costas Lambrinoudakis. 2008. Privacy in the
digital world. In Encyclopedia of Internet Technologies and Applications.
IGI Global, 411–417.

[25] Greg Guest, Namey MacQueen, and EE Namey. 2012. Introduction to

thematic analysis. Applied Thematic Analysis 12 (2012).

[26] Marian Harbach, Sascha Fahl, Thomas Muders, and Matthew Smith.
2012. Towards Measuring Warning Readability. In Proceedings of
the 2012 ACM Conference on Computer and Communications Security
(CCS ’12). ACM, New York, NY, USA, 989–991. https://doi.org/10.1145/
2382196.2382301

[27] Marian Harbach, Sascha Fahl, Polina Yakovleva, and Matthew Smith.
2013. Sorry, I Don’t Get It: An Analysis of Warning Message Texts. In
Proceedings of the 2013 International Conference on Financial Cryptog-
raphy and Data Security (FC13), Workshop on Usable Security (Lecture
Notes in Computer Science).

AISB’18, 5th April 2018, AISB 2018 Symposium

Lynsay A. Shepherd and Karen Renaud

[28] Almut Herzog and Nahid Shahmehri. 2007. User Help Techniques
for Usable Security. In Proceedings of the 2007 Symposium on Com-
puter Human Interaction for the Management of Information Technology
(CHIMIT ’07). ACM, New York, NY, USA, Article 11.

[29] John Sören Petterson (Ed.). 2008. HCI Guidelines. (2008). PRIME
(Privacy and Identity Management for Europe) EU Project Report.
[30] Michaela Kauer, Thomas Pfeiffer, Melanie Volkamer, Heike Theuerling,
and Ralph Bruder. 2012. It is not about the design - it is about the
content! Making warnings more efficient by communicating risks
appropriately. (2012).

[31] Aharon Kellerman. 2010. Mobile broadband services and the availabil-
ity of instant access to cyberspace. Environment and Planning A 42, 12
(2010), 2990–3005.

[32] Patrick Gage Kelley. 2009. Designing a privacy label: assisting con-
sumer understanding of online privacy practices. In CHI’09 Extended
Abstracts on Human Factors in Computing Systems. ACM, 3347–3352.
[33] Soyun Kim and Michael S Wogalter. 2009. Habituation, dishabitua-
tion, and recovery effects in visual warnings. In Proceedings of the
Human Factors and Ergonomics Society Annual Meeting, Vol. 53. Sage
Publications Sage CA: Los Angeles, CA, 1612–1616.

[34] Bart Piet Knijnenburg. 2015. A User-Tailored Approach to Privacy Deci-
sion Support. Ph.D. Dissertation. UC Irvine Information and Computer
Sciences.

[35] Robert LaRose and Nora J Rifon. 2007. Promoting i-safety: effects of
privacy warnings and privacy seals on risk assessment and online
privacy behavior. Journal of Consumer Affairs 41, 1 (2007), 127–149.

[36] Harold D Lasswell. 1948. The Structure and Function of Communica-
tion in Society. The Communication of Ideas 37 (1948), 215–228.
[37] Jialiu Lin. 2013. Understanding and capturing people’s mobile app
privacy preferences. Ph.D. Dissertation. Carnegie Mellon University.
[38] Ewa Luger and Tom Rodden. 2014. The value of consent: Discussions
with designers of ubiquitous computing systems. In Pervasive Comput-
ing and Communications Workshops (PERCOM Workshops), 2014 IEEE
International Conference on. IEEE, 388–393.

[39] Llúcia Masip, Célia Martinie, Marco Winckler, Philippe A. Palanque,
Toni Granollers, and Marta Oliva. 2012. A Design Process for Exhibit-
ing Design Choices and Trade-Offs in (Potentially) Conflicting User
Interface Guidelines. In Human-Centered Software Engineering - 4th
International Conference, HCSE 2012, Toulouse, France, October 29-31,
2012. Proceedings. 53–71.

[40] Ricardo Mendoza-González and Sandra Jimenez-González. 2015. Guide-
lines to Design Usable Security Feedback for Identity Management
Applications. In Mathematical Methods and Systems in Science and
Engineering. 256–264.

[41] Michael S Mikowski and Josh C Powell. 2013. Single Page Web Appli-

cations. Manning Publications.

[42] Emerson Murphy-Hill and Gail C Murphy. 2014. Recommendation
delivery. In Recommendation Systems in Software Engineering. Springer,
223–242.

[43] Stefan Nafra. 2014. Aligning Privacy and Usability: Designing a Privacy-
Aware Mobile Application that People Can Use. Master’s thesis. Vienna
University of Economics and Business.

[44] Annie W. Y. Ng and Alan H. S. Chan. 2017. Mental Models of Construc-
tion Workers for Safety-Sign Representation. Journal of Construction
Engineering Management 143, 2 (2017).

[45] Jason R.C. Nurse. 2013. Effective Communication of Cyber Security
Risks. In 7th International Scientific Conference on Security and Protec-
tion of Information (SPI 2013).

[46] Jason R.C. Nurse, Sadie Creese, Michael Goldsmith, and Koen Lamberts.
2011. Guidelines for Usable Cybersecurity: Past and Present. In The 3rd
International Workshop on Cyberspace Safety and Security (CSS 2011) at

The 5th International Conference on Network and System Security (NSS
2011). IEEE.

[47] A Ant Ozok, Quyin Fan, and Anthony F Norcio. 2010. Design guide-
lines for effective recommender system interfaces based on a usability
criteria conceptual model: results from a college student population.
Behaviour & Information Technology 29, 1 (2010), 57–83.

[48] Massimiliano Pala and Yifei Wang. 2010. On the Usability of User Inter-
faces for Secure Website Authentication in Browsers. In Proceedings of
the 6th European Conference on Public Key Infrastructures, Services and
Applications (EuroPKI’09). Springer-Verlag, Berlin, Heidelberg, 239–
254.

[49] Kara Pernice. 2017. F-Shaped Pattern of Reading on the Web: Misun-
derstood, But Still Relevant (Even on Mobile). (Nov 2017). https://
www.nngroup.com/articles/f-shaped-pattern-reading-web-content/
[50] Irene Pollach. 2007. What’s wrong with online privacy policies? Com-

mun. ACM 50, 9 (2007), 103–108.

[51] Elissa M Redmiles, Everest Liu, and Michelle L Mazurek. 2017. You
Want Me To Do What? A Design Study of Two-Factor Authentication
Messages. In Thirteenth Symposium on Usable Privacy and Security
(SOUPS 2017). USENIX Association, Santa Clara, CA.

[52] K Renaud and J van Biljon. 2017. Demarcating Mobile Phone Interface
Design Guidelines to Expedite Selection. South African Computing
Journal 29, 3 (2017).

[53] Florian Schaub, Rebecca Balebako, Adam L Durity, and Lorrie Faith
Cranor. 2015. A design space for effective privacy notices. In Eleventh
Symposium On Usable Privacy and Security (SOUPS 2015). USENIX
Association, 1–17.

[54] Ronak Shah and Kailas Patil. 2016. Evaluating Effectiveness of Mo-
bile Browser Security Warnings. ICTACT Journal on Communication
Technology 7, 3 (2016), 1373–1378.

[55] Umesh Shankar and Chris Karlof. 2006. Doppelganger: Better browser
privacy without the bother. In Proceedings of the 13th ACM Conference
on Computer and Communications Security. ACM, 154–167.

[56] Claude E Shannon. 2001. A mathematical theory of communication.
ACM SIGMOBILE Mobile Computing and Communications Review 5, 1
(2001), 3–55.

[57] Mario Silic, Jordan Barlow, and Dustin Ormond. 2015. Warning! A
comprehensive model of the effects of digital information security
warning messages. In Conference Proceedings The 2015 Dewald Roode
Workshop on Information Systems Security Research, October, IFIP. 1–32.
[58] Geoff Skinner, Song Han, and Elizabeth Chang. 2005. A framework
of privacy shield in organizational information systems. In Mobile
Business, 2005. ICMB 2005. International Conference on. IEEE, 647–650.
[59] Asimina Vasalou, Anne-Marie Oostveen, Chris Bowers, and Russell
Beale. 2015. Understanding engagement with the privacy domain
through design research. Journal of the Association for Information
Science and Technology 66, 6 (2015), 1263–1273.

[60] Melanie Volkamer, Karen Renaud, Gamze Canova, Benjamin Rein-
heimer, and Kristoffer Braun. 2015. Design and Field Evaluation of
PassSec: Raising and Sustaining Web Surfer Risk Awareness. In Trust
and Trustworthy Computing - 8th International Conference, TRUST 2015,
Heraklion, Greece, August 24-26, 2015, Proceedings. 104–122.

[61] Tilo Westermann. 2017. User Acceptance of Mobile Notifications. Ph.D.
Dissertation. Institute of Software Engineering and Theoretical Com-
puter Science, Berlin Institute of Technology Berlin, Germany.
[62] Michael Wogalter and Christopher Mayhorn. 2017. Warning Design.
In Information Design: Research and Practice, Alison Black, Paul Luna,
Ole Lund, and Sue Walker (Eds.). Chapter 20.

[63] Michael S Wogalter. 1999. Factors Influencing the Effectiveness of
Warnings. Visual Information for Everyday Use: Design and Research
Perspectives (1999), 93–110.

How to Design Browser Security and Privacy Alerts

AISB’18, 5th April 2018, AISB 2018 Symposium

[64] Michael S Wogalter, David M DeJoy, and Kenneth R Laughery. 1999.
Organizing theoretical framework: a consolidated communication-
human information processing (C-HIP) model. Warnings and Risk
Communication (1999), 15–23.

[65] Heng Xu, Robert E Crossler, and France BéLanger. 2012. A value sensi-
tive design investigation of privacy enhancing tools in web browsers.
Decision Support Systems 54, 1 (2012), 424–433.

