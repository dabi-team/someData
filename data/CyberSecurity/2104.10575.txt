Approved for Public Release; Distribution Unlimited. Public Release Case Number 21-0639

Towards Causal Models for Adversary Distractions

Ron Alford
ralford@mitre.org

Andy Applebaum
aapplebaum@mitre.org

1
2
0
2

r
p
A
1
2

]

R
C
.
s
c
[

1
v
5
7
5
0
1
.
4
0
1
2
:
v
i
X
r
a

Abstract

Automated adversary emulation is becoming an indispens-
able tool of network security operators in testing and evalu-
ating their cyber defenses. At the same time, it has exposed
how quickly adversaries can propagate through the network.
While research has greatly progressed on quality decoy gen-
eration to fool human adversaries, we may need diﬀerent
strategies to slow computer agents. In this paper, we show
that decoy generation can slow an automated agent’s deci-
sion process, but that the degree to which it is inhibited is
greatly dependent on the types of objects used. This points
to the need to explicitly evaluate decoy generation and place-
ment strategies against fast moving, automated adversaries.

1

Introduction

Automated cyber adversaries have the potential to
greatly change the speed at which networks are compro-
mised and exploited. Many of the technologies for slow-
ing and redirecting cyber adversaries revolve around de-
ceiving humans with careful constructed deceits[3, 11].
Where human red teams and adversaries are cautious
and introspective, current automated adversary tech-
niques are noisy and fast. In internal testing, MITRE’s
CALDERA[8] agent can laterally move through a chain
of network hosts in under thirty seconds per hop, mostly
limited by the windows scheduling technique it ex-
ploited. Given the diﬀerences between human-driven
and automated attacks, a defender’s strategies must ac-
count for both.

Numerous works advocate for deceptive emplace-
ments to enhance cybersecurity[32, 31]. Almeshekah
et al.[5] provide a high-level overview of the develop-
ment and deployment process of deception for cyber-
security, which others have shown increases the frustra-
tion and suspicion of human red teams, and presumably,
adversaries[14]. Common network deceptions include
honeypot hosts[29], fake user credentials[18], and gen-
erated content[11], covering both the exploration and
exploitation stages of a typical attack. These works, as
well as others, make an excellent case for the usage of
deception against human operators.

In this work, we focus on how deceptive emplace-
ments on a network can aﬀect an automated adversary’s

decision making. To prime the discussion, we ﬁrst pro-
vide an overview of the current means of automating
decoy placement, from human directed methods to ma-
chine learning methods for optimizing interaction with
malware. We then give a brief overview of automated
planning, which we use as a proxy for automated cyber
agents such as CALDERA. Using a planning model de-
rived from CALDERA, we show that performance of a
state-of-the-art automated planner can be signiﬁcantly
degraded by adding a single new relationship to the en-
vironment, but that the eﬀect is highly dependent on the
relationship’s type. Finally, we discuss the implications
of having diﬀerent defense strategies against human and
automated adversaries, and speculate on ways forward.

2 Related Work

Network topology is a common area of contention be-
tween deception and counter-deception technologies[4].
Achleitner et al.[3] give an architecture for creating large
numbers of honeypots with believable topological and
physical characteristics, using software-deﬁned network
(SDN) routers inserted throughout the network. Hon-
eypots must be implemented with signiﬁcant care, as
research continues to progress on identifying honeypots,
tarpits, and other defensive emplacements[25, 6]. While
honeypots are expensive to deploy, lighter weight rout-
ing policies can obfuscate router layouts, which can pre-
vent adversaries from having the knowledge to launch
targeted denial of service attacks[12, 27, 33].

Megatron[1], an Air Force Research Lab-sponsored
project, and the Active Deception Framework[21] also
provide tools for honeypot creation, as well as suite of
host and service deception tools. Neither framework
automatically deploy deceptions, and are instead struc-
tured around allowing operators to deploy, monitor, and
manage deception operations across a network. Both
these systems are reﬁnements over concepts explored in
earlier designs for network topology deception[26, 35].

The above systems are potential platforms for intel-
ligent automation of deception emplacement decisions.
Fraunholz and Zimmermann[15] consider network de-
ception as a deployment problem, where a set of decep-
tion services must be deployed throughout a network in
a way that they match their surrounding context (op-

©2021 The MITRE Corporation

 
 
 
 
 
 
erating systems, services, and IP addresses). They use
machine learning methods to automatically cluster net-
work features, and use these clusters as distributions to
generate deception system deployments. DodgeTron[30]
considers the task of malware analysis, and how to cre-
ate a honey network that maximizes interaction with a
malware sample. They use symbolic execution on a mal-
ware corpus to catalog system and API calls. The cor-
pus is then clustered by their call traces. New malware
is matched against the clusters, and a representative
sample from their match is used to select honey things
to populate a network. Overall, more than 90 percent
of their evaluation samples collected one or more honey
things from the network.

Game theory has a wide variety of tools for mod-
eling conﬂict in uncertain domains. An early applica-
tion to cybersecurity quantiﬁed how increasing decep-
tions can hit a point of diminishing returns[19], after
which an attacker switches from informed actions to
blind policies (attacking all hosts on a network, for ex-
ample). Ferguson-Walter et al.[13] argue that model-
ing cyber deception as a 2-player sequential hypergame
in order allows the defender to capture the uncertainty
about the parameters of conﬂict between adversary and
defender[23]. Jajodia et al.[22] deﬁne a probabilistic
logic game between attacker and defender around a net-
work scanning use case, where the defender must create
fake network scan results to steer an attacker towards
hosts which are harder to compromise or less critical
to its mission. To analyze dynamic routing to honey-
pots, La et al.[24] formulate the problem as a repeated
Bayesian game with imperfect information about the
type of the attacker, covering the tradeoﬀ between at-
tack detection, attack success, and unnecessary honey-
pot usage. Common to these game theoretic approaches
is a focus on the role that deception plays on the out-
come of the adversary’s action. Our work is comple-
mentary, showing that the adversary’s reasoning process
may be manipulated, slowing it down to provide crucial
time for the defenders to respond.

3 Background: Automated Planning

Automated planning is the study of how to act in an
environment to achieve some goal or reward. Clas-
sical planning restricts itself to deterministic,
fully-
observable, single agent environments with ﬁnite, dis-
crete action and state spaces. Other than full observ-
ability, these restrictions suit the execution environment
of automated red team agents such as CALDERA[28].
A classical planning problem P is a tuple
(L, O, s0, G), deﬁned as follows: L is a ﬁnite, ﬁrst or-
der, function-free logical language which contains terms
(both variables such as ?x, ?y and constants such

as computer01, user664), and predicate symbols to
denote relationships between terms, e.g., (has user
computer01 user664). We call sentences and predi-
cates from L ground if they contain only constants. A
state s in P is a set of ground predicates from L which
are true in an environment. All ground predicates not
in s are assumed to be false (the closed world assump-
tion). The set of all states is S, s0 ∈ S is the initial,
starting state for the problem. G is the goal condition
we want to achieve, expressed as a sentence from L.

O is a set of operators, each deﬁned as a tuple
o = (p, eadd, edel), where p is a logical sentence from
L expressing when o can be applied, eadd and edel are
positive and negative eﬀects of o, expressed as sets of
predicates which become true or false, respectively. An
operator without any free variables in its components is
called an action. We ground an operator o into an action
by substituting constants from L for free variables in
o. The set of actions A is the set of all possible
groundings of operators in O. We apply an action
whose precondition p is satisﬁed to a state by deleting
its negative eﬀects and adding its positive eﬀects. This
forms a state transition function γ : S × A → S, deﬁned
as:

γ (s, (p, eadd, edel)) =

(cid:26)

s
(s \ edel) ∪ eadd

if s (cid:50) p
if s (cid:15) p

We say a sequence of actions (cid:104)a0, a1, . . . ..., an(cid:105) is a
plan for a problem P if there are a sequence of states
(cid:104)s1, . . . , sn+1(cid:105) such that γ(si, ai) = si+1 and sn+1 (cid:15) G.
A diverse array of techniques exist for ﬁnding plans[16].
Solving planning problems can be computationally ex-
pensive, EXPSPACE in the worst case for the formal-
ism above[9]. However, many planning problems are
solvable in reasonable time frames[10].
In particular,
cybersecurity planning problems often have monotonic
state and action spaces[20], making them solvable in
polynomial time for a ﬁxed domain (L and O).

4 Preliminary Results: Predicate Sensitivity

As preliminary work, we ran a series of experiments
looking into the robustness of planners against changes
to speciﬁc predicates, motivated by the work of [34]. In
this work, the authors seek to understand the robustness
of domain-independent planners, running a series of
experiments where a hypothetical attacker modiﬁes a
planning problem for the purposes of slowing down
any planner that were to try to solve it, making
modiﬁcations semi-randomly. Their results are varied,
with some planners performing consistently across all of
the tests and others showing high variability.

Our work builds on this by moving away from over-
all robustness to instead focus on targeted robustness:

can we deliberately add knowledge to a planning prob-
lem for the express purpose of slowing down solvers?
This expands the work of [34] by changing the threat
model to an attacker who deliberately tries to slow down
the planner, as opposed to one behaving randomly.

This section outlines the scope and results of some
of our initial work, where we look to identify which
predicates within a planning problem are most sensitive
to modiﬁcation and cause the most slowdown for a
planner. Focusing on cyber, our work considers the case
where we are defending against an automated adversary
exploiting credential re-use during a post-compromise
intrusion. While still preliminary, we believe that these
results are important in understanding how deception
can be used against future automated adversaries.

4.1 Cyber as a Planning Problem For our tests,
we chose to use the “CALDERA” Planning Domain
Deﬁnition Language (PDDL) representation [7, 28],
which was used as part of the 2018 International Plan-
ning Competition (IPC) [2]. This domain features a
simple adversary model where we assume the adversary
already has a foothold within a Windows enterprise net-
work and is looking to exploit credential re-use to later-
ally move to each workstation within the network. The
domain features 8 actions, each of which corresponds
to a typical post-compromise activity such as obtaining
domain and host information, dumping credentials from
memory, mounting network shares, and copying and ex-
ecuting ﬁles remotely for lateral movement. The domain
also features several standard object types – i.e., num-
bers and strings – as well as custom types for system
components such as users, hosts, domains, etc.

Most important for our experiments are the do-
main’s predicates. In total, the CALDERA domain fea-
tures 30 predicates – 3 of which are status predicates,
which denote the adversary’s current status in the envi-
ronment, and 27 of which are property predicates which
describe the environment itself. The following are two
example property predicates:
( p r o p u s e r ? a − ObservedDomainCredential

?b − ObservedDomainUser )

( p r o p p a t h ? a − O b s e r v e d F i l e

?b − s t r i n g )

The ﬁrst, prop user, links a user object to a creden-
tial object, while the second links a ﬁle to its location.
Of the 27 property predicates, only 14 are con-
trollable:
the other 13 predicates used within the
CALDERA domain are explicitly controlled by the ad-
versary and thus unavailable as potential decoys. An
example here is prop path, which is set by the adversary
when copying a ﬁle. Table 1 lists the 14 controllable
predicates and provides a brief explanation of each.

4.2 Testing Procedure We use a simple process
similar to that of [34] for evaluating the importance of
each predicate. After starting with an initial domain
and problem ﬁle, we run a baseline planner – fast-
downward [17] in default conﬁguration – to determine
initial diﬃculty. We then iterate as follows:

1. Select a predicate at random.
2. Select objects at random to match the predicate.
3. Add a new assertion linking the object(s) and

predicate.

4. Run the planner on the new problem.

Due to the random nature of this process, we repeat
until achieving a set number of trials for each predicate,
ultimately recording the average time and standard
deviation for each predicate. Note that, by design, we
only add one observation to the problem ﬁle, leaving
more complex additions for future work. As an example,
we might have two users – e.g., john and pete – who each
have their own password; a valid addition would be to
link one of the users with the other’s password, such
as an addition where john’s password works for pete’s
account or vice-versa.

While time consuming, this process oﬀers some
advantages as it never creates logically inconsistent
problems: by only adding information, there is always
guaranteed to be a solution to the problem. However,
step 2 can introduce unusual structure (e.g., assigning
a user two usernames), but doing so avoids issues where
objects are created without enough information (e.g.,
creating a new user but not assigning that user a
username or a password).

4.3 Results We chose to focus on one speciﬁc prob-
lem within the CALDERA IPC problem set, selecting
problem 8 to work with. This problem encodes a net-
work with 6 hosts, 12 users, 3 administrators on each
host, and 3 cached credentials on each host. With no
modiﬁcations, it took fast-downward roughly 6.7 sec-
onds to ﬁnd a path from the start host to compromise
each other host.

Table 1 shows the results from running our testing
procedure in Section 4.2 between 30 and 50 times for
each predicate. This table shows that for 10 of the 14
controllable predicates, there was eﬀectively no change
observed for a modiﬁcation – e.g.,
linking a second
username to a user object with prop username had no
signiﬁcant impact on planner performance. The other
four predicates all showed some slowdown on average,
with adding an additional admin to a host having the
most slowdown (but highest variability) and adding
an additional Windows domain having least variability
but not a signiﬁcant slowdown, with the other two
predicates – associating a credential object with a user

Predicate
mem domain user admins
prop user
prop windows domain
mem cached domain creds
prop sid
prop is group
prop seconds
prop dc
prop hostname
prop timedelta
prop fqdn
prop dns domain name
prop dns domain
prop username

Avg. Time (s)
42.3
14.2
11.1
10.1
6.5
6.4
6.4
6.4
6.4
6.4
6.3
6.3
6.3
6.2

Std. Dev. (s) Description

229.2
37.4
0.9
13.4
0.6
0.6
0.6
0.6
0.6
0.6
0.6
0.5
0.5
0.6

Denotes a user is an admin on a host.
Connects a user to a credential.
Declares the name of a Windows domain.
Denotes credentials stored on a given host.
Denotes the security ID of a user.
Boolean for if the user is a group.
Current time’s seconds on a given host.
Boolean for if a host is a domain controller.
String name for a given host.
Links a host to a custom timedelta object.
String fully qualiﬁed domain name for a host.
DNS domain name for a host.
Links an object to a Windows domain.
String username for a user.

Table 1: Predicate sensitivity results

and adding additional cached credentials on a host –
falling between.

4.4 Discussion We note three interesting results
from our experiments. First, just adding information is
suﬃcient for slowing down a planner, even if that infor-
mation should make it easier for the planner to achieve
its objectives (as in the case of additional cached cre-
dentials or domain admins). Additional relationships
enable new actions, increasing both the ground prob-
lem size and the branching factor of the search space,
slowing down the planning process.

The second interesting feature was on the predi-
cates that were useful for slowing the planner. The
10 predicates that had no impact on the planner were
to largely be expected, and three of the signiﬁcant
predicates – mem domain user admins, prop user, and
mem cached domain creds – do not come as a surprise
given that modifying each of these ﬁelds is a common
deceptive maneuver. Less common, however,
is the
prop windows domain predicate: while it is indeed com-
mon to add a new Windows domain, that domain is
usually added substantially and not merely restricted
to just the name of the domain, as in our tests. We
believe that this is an important factor speciﬁcally for
automated adversaries. Most actions require informa-
tion regarding the Windows domain, and so if there are
two domains then the planner needs to investigate ac-
tions for each domain when constructing plans.

The last interesting result from our test is on the
variability of modifying the predicates. Adding an extra
Windows domain had consistent slowdown with little
variability – examining the domain structure further,
we ﬁnd that the Windows domain must be interacted
with immediately, and so that action’s execution is
consistent on all trials where this predicate is modiﬁed.

The other three signiﬁcant predicates, however, have
much more variability as they can occur in diﬀerent
orders depending on what speciﬁcally is being modiﬁed
– as an example, adding an additional cached credential
on the start host will lead to a very diﬀerent planning
problem than adding an additional cached credential on
the ﬁnal host to be compromised. While this is a fairly
intuitive concept – a decoy’s eﬃcacy is correlated to
where it lies on the attacker’s path – our results help
lend further credence to the notion that understanding
the attacker model is critical for composing eﬀective
decoys and distractions.

5 Conclusion & Future Work

Where many of the works on defensive cyber deception
focus on changing changing adversary actions, our pre-
liminary results point towards an additional defensive
goal – adversary delay. Our experiments show a signiﬁ-
cant delay with just a single change to the environment.
By analyzing causal models, network operators may be
able to more substantially delay intruders.

With regards to our preliminary results in Section 4,
we believe there are several ways to extend our work.
First, our current approach is time consuming: future
work that we plan to conduct will instead look to an-
alytically discover which predicates are most sensitive,
as opposed to experientially. Second, our approach only
examines one predicate modiﬁcation at a time. In a real
deployment, however, it’s likely we can make multiple
changes, and that our experiments should therefore con-
sider how sets of predicates interact. Lastly, our prelim-
inary testing methodology only adds predicates linking
to existing objects, which at times creates logical in-
consistencies; while we should be able to add predicates
with existing objects, we also would like to run more
tests where we are able to add new objects as well.

References

[1] Megatron:

deception

Cyber
tech.
Security,

report,

& catalog,
formation
n5vm24dpyjj1jvzaq12us84e-wpengine.
netdna-ssl.com/wp-content/uploads/2020/
01/AIS-Megatron.pdf.

Assured

2020,

framework
In-
https://

[2] A. Coles, A. Coles, A. Torralba, and F.
Pommerening, An Overview of the International
Planning Competition Part 1: Classical Tracks.
https://www.nms.kcl.ac.uk/andrew.coles/
PlanningCompetitionAAAISlides.pdf, 2018.

[3] S. Achleitner, T. F. La Porta, P. Mc-
Daniel, S. Sugrim, S. V. Krishnamurthy,
and R. Chadha, Deceiving network reconnais-
sance using SDN-based virtual topologies, IEEE
Transactions on Network and Service Management,
14 (2017), pp. 1098–1112.

[4] E. Al-Shaer, J. Wei, W. Kevin, and
C. Wang, Autonomous Cyber Deception, Springer,
2019.

[5] M. H. Almeshekah and E. H. Spafford, Plan-
ning and integrating deception into computer secu-
rity defenses, in Proceedings of the 2014 New Se-
curity Paradigms Workshop, 2014, pp. 127–138.

[6] L. Alt, R. Beverly, and A. Dainotti, Uncov-
ering network tarpits with degreaser, in Proceed-
ings of the 30th Annual Computer Security Appli-
cations Conference, 2014, pp. 156–165.

[7] A. Applebaum, CALDERA IPC-2018 domain.
https://bitbucket.org/ipc2018-classical/
domains/src/master/sat/caldera/,
Accessed: 2021-02-28.

2018.

[8] A. Applebaum, D. Miller, B. Strom, C. Ko-
rban, and R. Wolf, Intelligent, automated red
team emulation, in Proceedings of the 32nd An-
nual Conference on Computer Security Applica-
tions, 2016, pp. 363–373.

[9] T. Bylander, The computational complexity of
propositional STRIPS planning, Artiﬁcial Intelli-
gence, 69 (1994), pp. 165–204.

[10] I. Cenamor and A. Pozanco, Insights from
in Proceedings of the

the 2018 ipc benchmarks,
Workshop of the IPC WIPC, 2019.

Forge: A fake online repository generation engine
for cyber deception, IEEE Transactions on Depend-
able and Secure Computing, (2019).

[12] C.-Y. J. Chiang, Y. M. Gottlieb, S. J. Sug-
rim, R. Chadha, C. Serban, A. Poylisher,
L. M. Marvel, and J. Santos, Acyds: An adap-
tive cyber deception system, in Proceedings of the
IEEE Military Communications Conference (MIL-
COM), IEEE, 2016, pp. 800–805.

[13] K. Ferguson-Walter, S. Fugate, J. Mauger,
and M. Major, Game theory for adaptive defen-
sive cyber deception, in Proceedings of the 6th An-
nual Symposium on Hot Topics in the Science of
Security, 2019, pp. 1–8.

[14] K. Ferguson-Walter, T. Shade, A. Rogers,
M. C. S. Trumbo, K. S. Nauer, K. M. Divis,
A. Jones, A. Combs, and R. G. Abbott, The
Tularosa study: An experimental design and im-
plementation to quantify the eﬀectiveness of cyber
deception, in Proceedings of the 52nd Hawaii Inter-
national Conference on System Sciences, 2019.

[15] D. Fraunholz and M. Zimmermann, Towards
deployment strategies for deception systems, Ad-
vances in Science, Technology and Engineering Sys-
tems Journal, 2 (2017), pp. 1272–1279, https:
//doi.org/10.25046/aj0203161.

[16] M. Ghallab, D. Nau, and P. Traverso, Auto-
mated planning and acting, Cambridge University
Press, 2016.

[17] M. Helmert, The fast downward planning sys-
tem, Journal of Artiﬁcial Intelligence Research, 26
(2006), pp. 191–246.

[18] C. Herley and D. Florˆencio, Protecting ﬁ-
in
nancial
IFIP International Information Security Confer-
ence, Springer, 2008, pp. 681–685.

institutions from brute-force attacks,

[19] J. P. Hespanha, Y. S. Ateskan, H. Kizilo-
cak, et al., Deception in non-cooperative games
with partial information, in Proceedings of the 2nd
DARPA-JFACC Symposium on Advances in En-
terprise Control, 2000, pp. 1–9.

[20] J. Hoffmann, Simulated penetration testing:
From” dijkstra” to” turing test++”, in Proceed-
ings of the International Conference on Automated
Planning and Scheduling, vol. 25, 2015.

[11] T. Chakraborty, S. Jajodia, J. Katz, A. Pi-
cariello, G. Sperli, and V. Subrahmanian,

[21] M. M. Islam and E. Al-Shaer, Active deception
framework: an extensible development environment

[32] F. J. Stech, K. E. Heckman, and B. E.
Strom,
Integrating cyber-D&D into adversary
modeling for active cyber defense, in Cyber decep-
tion, Springer, 2016, pp. 1–22.

[33] S. T. Trassare, R. Beverly, and D. Alder-
son, A technique for network topology decep-
tion, in Proceedsings of the IEEE Military Com-
munications Conference (MILCOM), IEEE, 2013,
pp. 1795–1800.

[34] M. Vallati and L. Chrpa, On the robustness of
domain-independent planning engines: The impact
of poorly-engineered knowledge, in Proceedings of
the 10th International Conference on Knowledge
Capture, 2019, pp. 197–204.

[35] F. Yaman, T. C. Eskridge, A. Adler,
M. Atighetchi, B. I. Simidchieva, S. Jeter,
J. Cassetti, and J. DeMatteis, An au-
tonomous resiliency toolkit for cyber defense plat-
forms,
in Proceedings of the 12th International
Conference on Agents and Artiﬁcial Intelligence
(ICAART), 2020, pp. 240–248.

NOTICE

Portions of this technical data were produced for the U.
S. Government under Contract No. FA8702-19-C-0001
and W56KGU-18-D-0004, and is subject to the Rights
in Technical Data-Noncommercial Items Clause DFARS
252.227-7013 (FEB 2014)

©2021 The MITRE Corporation.

This work
is licensed under a Creative Commons Attribution-
NonCommercial-NoDerivatives 4.0 International Li-
cense.

for adaptive cyber deception, in 2020 IEEE Secure
Development (SecDev), IEEE, 2020, pp. 41–48.

[22] S.

Jajodia,

F. Pierazzi,
N. Park,
A. Pugliese, E. Serra, G. I. Simari, and
V. Subrahmanian, A probabilistic logic of cyber
deception,
IEEE Transactions on Information
Forensics and Security, 12 (2017), pp. 2532–2544.

[23] N. S. Kovach, A. S. Gibson, and G. B.
Lamont, Hypergame theory: a model for conﬂict,
misperception, and deception, Game Theory, 2015
(2015).

[24] Q. D. La, T. Q. Quek, J. Lee, S. Jin,
and H. Zhu, Deceptive attack and defense game
in honeypot-enabled networks for the internet of
things, IEEE Internet of Things Journal, 3 (2016),
pp. 1025–1035.

[25] G. F. Lyon, Nmap network scanning: The oﬃ-
cial Nmap project guide to network discovery and
security scanning, Insecure, 2009.

[26] R. Mehresh and S. Upadhyaya, A deception
framework for survivability against next generation
cyber attacks, in Proceedings of the International
Conference on Security and Management (SAM),
2012.

[27] R. Meier, P. Tsankov, V. Lenders, L. Van-
bever, and M. Vechev, NetHide: Secure and
in 27th
practical network topology obfuscation,
USENIX Security Symposium (USENIX Security
18), 2018, pp. 693–709.

[28] D. Miller, R. Alford, A. Applebaum,
H. Foster, C. Little, and B. Strom, Auto-
mated adversary emulation: A case for planning
and acting with unknowns, in ICAPS Workshop on
Integrated Planning, Acting and Execution, 2018.

[29] N. Provos et al., A virtual honeypot framework.,
in USENIX Security Symposium, vol. 173, 2004,
pp. 1–14.

[30] M. S. I. Sajid, J. Wei, M. R. Alam, E. Aghaei,
and E. Al-Shaer, DodgeTron: Towards au-
tonomous cyber deception using dynamic hybrid
analysis of malware, in Proceedings of the IEEE
Conference on Communications and Network Se-
curity (CNS), IEEE, 2020, pp. 1–9.

[31] J.

Singer,

Shields

up:

is

an
defense
port, MITRE,
org/publications/project-stories/
shields-up-a-good-cyber-defense-is-an-active-defense.

active
2020,

A good

cyber
re-
defense,
https://www.mitre.

tech.

