Detect & Reject for Transferability of Black-box Adversarial Attacks
Against Network Intrusion Detection Systems

Islam Debicha1,2, Thibault Debatty1, Jean-Michel Dricot2, Wim Mees1, and Tayeb Kenaza3

1 Royal Military Academy, Rue Hobbema 8, 1000 Brussels, Belgium,
{thibault.debatty,wim.mees}@rma.ac.be
2 Universit´e libre de Bruxelles, Avenue Franklin Roosevelt 50, 1050 Brussels, Belgium,
debichasislam@gmail.com, jean-michel.dricot@ulb.be
3 ´Ecole Militaire Polytechnique, Bordj El-Bahri 17, 16111 Algiers, Algeria,
ken.tayeb@gmail.com

Abstract. In the last decade, the use of Machine Learning techniques in anomaly-based intrusion de-
tection systems has seen much success. However, recent studies have shown that Machine learning in
general and deep learning speciﬁcally are vulnerable to adversarial attacks where the attacker attempts
to fool models by supplying deceptive input. Research in computer vision, where this vulnerability was
ﬁrst discovered, has shown that adversarial images designed to fool a speciﬁc model can deceive other
machine learning models. In this paper, we investigate the transferability of adversarial network trafﬁc
against multiple machine learning-based intrusion detection systems. Furthermore, we analyze the ro-
bustness of the ensemble intrusion detection system, which is notorious for its better accuracy compared
to a single model, against the transferability of adversarial attacks. Finally, we examine Detect & Reject
as a defensive mechanism to limit the effect of the transferability property of adversarial network trafﬁc
against machine learning-based intrusion detection systems.

Keywords: intrusion detection, machine learning, adversarial attacks, Transferability, black-box settings

1

Introduction

The computer and networking industry is becoming increasingly important due to their growing use in various
ﬁelds, which in turn leads to an escalation in the occurrence of cyberattacks. As a result, security is becoming
a key concern of any network architecture and an active research topic. Intrusion Detection Systems (IDS) are
one of the solutions presented to enhance network security by analyzing the trafﬁc to identify any suspicious
activity.

In order to detect intrusions, there are mainly two approaches: the ﬁrst one is based on comparing trafﬁc
with a list of all known attack patterns, also called signature-based intrusion detection. Intuitively, this method
gives excellent accuracy when dealing with known attacks, however, this type of detector is incapable of de-
tecting zero-day attacks, which is essentially what motivates the use of the second type of intrusion detection
techniques, called anomaly-based intrusion detection. The latter approach relies on modeling the normal
behavior of network trafﬁc and later examining new trafﬁc against this baseline.

Anomaly-based intrusion detection has been extensively studied, with most research using Machine
Learning (ML) techniques to create a trustworthy model of activity due to their high accuracy, including deep
learning which is considered a state-of-the-art technique in this ﬁeld. However, recent studies have shown
that machine learning in general, and deep learning in particular, are vulnerable to adversarial attacks where
the attacker seeks to fool the models by inserting slight but specially crafted distortions into the original input
[2].

Research in the ﬁeld of computer vision, where this vulnerability was ﬁrst discovered, has shown that
adversarial images designed to fool a speciﬁc model can, to some extent, fool other machine learning models

1
2
0
2

c
e
D
2
2

]

R
C
.
s
c
[

1
v
5
9
0
2
1
.
2
1
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Islam Debicha et al.

[14]. This is known as the transferability property of adversarial attacks. By exploiting this property, an
attacker can build a surrogate intrusion detection system, create adversarial trafﬁc for that detector, and then
attack another intrusion detection system without even knowing the internal architecture of that detector,
leading to a black-box attack.

To avoid this kind of vulnerability, we are conducting this research and the following are our contributions

in this paper:

– To the best of our knowledge, this is the ﬁrst study to examine the transferability of adversarial network
trafﬁc between multiple anomaly-based intrusion detection systems with different machine learning tech-
niques in black-box settings.

– In addition, we construct an ensemble intrusion detection system to examine its robustness against the

transferability property of adversarial attacks compared to single detectors.

– Finally, we investigate the effectiveness of the Detect & Reject method as a defensive mechanism to
mitigate the effect of the transferability property of adversarial network trafﬁc against machine learning-
based intrusion detection systems.

2 Background

2.1 Related Work

Studies have shown that incorporating machine learning techniques can help improving the performance of
intrusion detection systems. Aslahi-Shahri et al. [4] have proposed and explained the implementation of an
intrusion detection system based on a hybrid support vector machine (SVM) and genetic algorithm (GA)
method. In [6], the authors have proposed a novel evidential IDS based on Dempster-Shafer theory to take
into account source reliability. Vinayakumar et al. [20] have proposed a highly scalable intrusion detection
framework using deep neural networks (DNNs) after a comprehensive evaluation of their performance against
classical machine learning classiﬁers. Alamiedy et al.[3] proposed an improved anomaly-based IDS model
based on a multi-objective gray wolf optimization (GWO) algorithm, in which GWO is used as a feature se-
lection technique. Ghanem et al.[7] have proposed a cyber-intrusion detecting system classiﬁcation with MLP
trained by a hybrid metaheuristic algorithm and feature selection based on multi-objective wrapper method.
In [8], the authors proposed a new binary classiﬁcation model for intrusion detection, based on hybridiza-
tion of Artiﬁcial Bee Colony algorithm (ABC) and Dragonﬂy algorithm (DA) for training an artiﬁcial neural
network (ANN) in order to increase the classiﬁcation accuracy rate for malicious and non-malicious trafﬁc
in networks. Nevertheless, we noticed that little or no attention was paid to the effect of adversarial attacks
when proposing these solutions.

Szegedy et al.[18] was the ﬁrst work to report the vulnerability of DNN to adversarial samples where
they introduced imperceptible adversarial perturbations to handwritten digits images and succeeded to fool
the DNN model with high conﬁdence. This discovery has prompted a number of studies in the computer
vision community, where several attacks and defenses have been proposed [9,12,10]. There are some works
[14,22,11] dealing with the transferability of adversarial attacks. However, these studies were speciﬁcally
designed for the ﬁeld of image classiﬁcation, where this particular vulnerability was ﬁrst detected.

In recent studies, the effect of adversarial attacks on intrusion detection systems has been investigated.
Wang [21] Inspected the performance of state-of-the-art attack algorithms against deep learning-based in-
trusion detection on the NSL-KDD dataset. Pawlicki et al.[15] evaluated the possibility of deteriorating the
performance of a well-optimized intrusion detection algorithm at test time by generating adversarial attacks
and then offers a way to detect those attacks.

Through our literature review, we did not ﬁnd any studies on the transferability of adversarial network
trafﬁc between multiple anomaly-based intrusion detection systems with different machine learning tech-
niques in black-box settings. Hence, we propose to conduct this study in order to ﬁll this gap. Notice that

Adversarial Attacks against Intrusion Detection Systems

3

in our recent work [5], we investigated the effectiveness of adversarial training as a defense for intrusion
detection systems against these attacks.

2.2 Adversarial Attacks

Despite their considerable success in achieving high accuracy, machine learning algorithms in general and
deep learning, in particular, have proven vulnerable to adversarial attacks, where crafting an instance with
small intentional perturbations can lead a machine learning model to make an erroneous prediction [18].

The idea of generating adversarial examples is quite intuitive. It can be seen as the inverse process of
gradient descent where, for a given input x and its label y, one tries to ﬁnd model parameters θ that maximize
the accuracy of the model by minimizing the loss function J. On the other hand, the adversarial examples
are generated in order to minimize the accuracy of the model. Given the parameters θ, the loss function J
is differentiated with respect to the input data x so as to ﬁnd an instance x(cid:48), as close to x as possible, that
maximizes the loss function J.

One of the earliest and most popular adversarial attacks is called the Fast Gradient Sign Method (FGSM)
[9]. This attack uses a factor (cid:15) to limit the amount of distortion in the original instance such that (cid:107)x(cid:48) − x(cid:107)< (cid:15)
. One can think of (cid:15) as the attack strength or the size of the introduced perturbation. An adversarial instance
x(cid:48) is devised like following:

x(cid:48) = x + (cid:15)∇Jx(x, y, θ)

(1)

Projected Gradient Descent (PGD) [12] is another adversarial attack and is basically an iterative extension
of FGSM applying the attack repeatedly. However, PGD initializes the instance, at each iteration, to a random
point in the (cid:15)-ball around the original input.

Adversarial attacks can be classiﬁed into two types based on the attacker’s knowledge of the attacked
system: White-box setting where the attacker has full knowledge of the internal architecture of the attacked
system. Black-box setting where the attacker has no knowledge of the internal architecture of the attacked
system. In this paper, we create adversarial network trafﬁc records against a DNN-based IDS in a white-
box setting, and then the rest of the experiments are conducted in a black-box setting where we use these
adversarial instances to attack the other ML-based IDSs without having access to their internal architecture
or training data.

3 Proposed Approach

Previous work has shown that the accuracy of a DNN-based IDS can be signiﬁcantly reduced when exposed
to adversarial attacks [21,15,5]. In this section, we construct a DNN-based IDS and ﬁve other ML-based
IDSs to examine whether the same adversarial instances designed for a DNN-based IDS can be transferred to
other ML-based IDSs, which are trained on a different data set, without knowing anything about their internal
architectures. We also build an ensemble intrusion detection by clustering the ﬁve ML-based IDSs to study
whether having multiple classiﬁers voting prediction can be a defense against the transferability property
of adversarial attacks. Finally, we implement the Detect & Reject method as a defense mechanism for the
intrusion detection system and evaluate its robustness to transferable adversarial examples.

3.1 NSL-KDD Dataset

Many research papers in the intrusion detection community have used the NSL-KDD dataset to demonstrate
the performance of their proposed approaches and this is very useful as it creates a common basis for compar-
ison between these approaches. Published in 2009 by [19], this dataset is an improvement of the well-known
KDD-CUP’99 which has a fundamental problem of record redundancy which leads to the bias of classiﬁers

4

Islam Debicha et al.

towards frequent records. This problem has been solved in the NSL-KDD dataset by proposing a balanced
version of KDD-CUP’99 by removing the redundancy, thus providing a more accurate comparative analysis
of the performance of different proposed intrusion detection frameworks.

The dataset contains 41 network trafﬁc characteristics, covering three aspects: basic characteristics, con-
tent characteristics, and trafﬁc characteristics. Many attacks are covered in this dataset; they can be further
classiﬁed into four families of attacks: denial-of-service (DoS) attacks, probe attacks (Probe), root-to-local
(R2L) attacks, and user-to-root (U2R) attacks. We use KDDTrain+ in our experiments by dividing it into 80%
and 20% for training and test data respectively. The training data is divided into two almost equal parts A
and B to train the DNN-based IDS separately from other ML-based IDSs so as to examine the transferability
property. The data used in the experimental part are summarized in Table 1 and their partitioning is illustrated
in Figure 1 .

Table 1. Summary of the network trafﬁc dataset.

Normal DoS Probe R2L U2R
Training data A 26938 18371 4663 398 21
Training data B 26937 18371 4662 398 21
13468 9185 2331 199 10

Test data

Fig. 1. The partitioning of the dataset for training and testing of IDS

3.2 Preprocessing

The network trafﬁc included in this dataset is heterogeneous and contains both numerical and categorical
values. Many machine learning algorithms do not support categorical values, hence the need for the numer-
icalization step that transforms these categorical inputs into numerical values. In the case of the NSL-KDD
dataset, the categorical features are ”ﬂag”, ”protocol type” and ”service”. Another important aspect is fea-
ture scaling, which consists of converting all features to the same scale to ensure that all features contribute

Adversarial Attacks against Intrusion Detection Systems

5

equally to the result and also to help the gradient-based ML algorithms converge faster to the minima. We
restrict our study to a binary classiﬁcation where we consider any type of attack as ”intrusion” and the rest as
”normal” trafﬁc.

3.3 Building Anomaly-based Intrusion Detection Systems

TensorFlow [1] is used to build the DNN-based IDS, it consists of two hidden layers with 512 units each.
As an activation function, we use Rectiﬁed Linear Unit (ReLU) to increase the non-linearity. To prevent
overﬁtting, a dropout layer with a 20% dropout rate is placed after each hidden layer. ADAM and categorical
cross-entropy are used as an optimization algorithm and loss function respectively. In the end, the logits are
converted to probabilities using a softmax layer. The ﬁnal prediction is assigned to the highest probability
class.

We acknowledge the use of Scikit-learn [16] to build ﬁve ML-based IDSs. The default settings were
maintained. These ﬁve ML algorithms were selected due to their popularity in the ML community: Support
Vector Machines (SVM), Decision Tree (DT), Logistic Regression (LR), Random Forest (RF), and Linear
Discriminant Analysis (LDA). We also construct an ensemble IDS by grouping these ﬁve ML algorithms
where the ﬁnal prediction is made using the majority voting rule.

3.4 Transferability of Adversarial Attack in Black-box Settings

In order to test the transferability property of adversarial attacks, we build a DNN-based IDS where we
generate adversarial network trafﬁc records in a white-box setting and then test them against ﬁve different
ML-based IDSs. Note that the ﬁve ML-based IDSs are trained on a different dataset (Training data B) and the
adversary records were generated without assuming any knowledge of the internal architecture of these ﬁve
ML-based IDSs, which means that we are working under a black-box setting assumption.

Two adversarial attacks were implemented to generate adversarial network trafﬁc records: FGSM and
PDG. For this, we use Adversarial Robustness Toolbox (ART) [13]. The experiments are repeated by increas-
ing the attack strength (cid:15) to investigate the amount of perturbation required for the adversarial attack to be
transferred from the DNN-based IDS to the other ﬁve ML-based IDSs in black-box settings.

3.5 Defenses against the Transferability of Adversarial Attacks

Since the ensemble technique is known to increase accuracy over a single classiﬁer [17], we want to examine
whether it can also increase its robustness against the transferability of adversarial attacks in a black-box
setting. To do so, we construct an ensemble IDS based on the previous ﬁve ML-based IDSs and use the
majority voting rule to obtain the ﬁnal decision.

The second defense we consider is the Detect & Reject method [10], which involves training our IDSs to
detect not only ”abnormal” and ”normal” trafﬁc, but also a third class called ”adversarial”. Thus, whenever
the IDS decides that a network trafﬁc record is adversarial, it is rejected. We implement this method on the
ﬁve ML-based IDSs and examine their robustness to the adversarial attack transferability property.

4 Experimental Results

In this section, we present the results of experimenting our appraoch. Subsection (A) illustrates the effect of
the transferability property of adversarial attacks on the ﬁve ML-based IDSs. In subsection (B), we examine
the robustness of the ensemble IDS against these attacks in black-box settings. Subsection (C) illustrates the
robustness improvement of all IDSs after adding the detection and rejection mechanism to the ﬁve ML-based
IDSs.

6

Islam Debicha et al.

4.1 Transferability of Adversarial Attacks in Black-box Settings

(a) Transferability from DNN-based IDS to SVM-
based IDS

(b) Transferability from DNN-based IDS to DT-based
IDS

(c) Transferability from DNN-based IDS to LR-based
IDS

(d) Transferability from DNN-based IDS to RF-based
IDS

(e) Transferability from DNN-based IDS to LDA-
based IDS

Fig. 2. Transferability of adversarial attacks against ML-based intrusion detection systems in black-box settings

In this study, we use two adversarial attacks: FGSM and PGD to generate adversarial network trafﬁc
records from ”Test data”. These adversarial records are speciﬁcally designed to fool DNN-based IDSs since
both attacks have access to the internal architecture of DNNs. As mentioned earlier, we train the DNN-based
IDS using ”Training data A”, while the other 5 ML-based IDSs are trained using ”Training data B”. Adver-
sarial trafﬁc records are used at test time to attempt to mislead the IDSs. As shown in Figure 2, increasing the
attack strength ((cid:15)) further degrades the accuracy of the DNN-based IDS. When testing these adversarial net-
work trafﬁc records on the ﬁve ML-based IDSs, we ﬁnd that their accuracy decreases, even though the attacks
do not have access to their internal architectures. We also note that although the accuracy of the ML-based

0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM on DNN-IDSPGD on DNN-IDSFGSM on SVM-IDSPGD on SVM-IDS0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM on DNN-IDSPGD on DNN-IDSFGSM on DT-IDSPGD on DT-IDS0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM on DNN-IDSPGD on DNN-IDSFGSM on LR-IDSPGD on LR-IDS0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM on DNN-IDSPGD on DNN-IDSFGSM on RF-IDSPGD on RF-IDS0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM on DNN-IDSPGD on DNN-IDSFGSM on LDA-IDSPGD on LDA-IDSAdversarial Attacks against Intrusion Detection Systems

7

IDSs did not deteriorate as much as the DNN-based IDSs, some models were more vulnerable than others.
This may be due to their differentiability property, i.e., they are composed of differentiable elements, since
the decision tree and the random forest, whose accuracies were least affected, are non-differentiable models
that are not amenable to gradient descent due to their Boolean nature, unlike SVM or logistic regression for
example.

4.2 Ensemble Intrusion Detection System Robustness

Fig. 3. Adversarial attack transferability from DNN-based IDS to Ensemble IDS

Since the ensemble technique is known to improve accuracy over a single model, we investigate whether
it could also improve robustness. To this end, we construct an ensemble IDS based on the ﬁve ML-based
IDSs using the majority voting rule. The same setup as in the previous experiment is maintained, which
means that the 5 ML models used to build the ensemble model are trained using the ”Training data B”.
The adversarial trafﬁc records are generated from the ”Test data” using the FGSM and PGD attacks. These
adversarial records are designed to fool DNN-based IDS since both attacks can only access the internal
architecture of the DNN model. As shown in Figure 3, the ensemble IDS is not able to resist the transferability
property of adversarial attacks, even though no information about the ensemble IDS was used to generate
these adversarial records. This shows the ease of an evasion attack against an intrusion detection system
without even knowing its internal architecture, simply by building a surrogate IDS (DNN-based IDS in our
case) and generating adversarial network trafﬁc for this surrogate model.

In practice, if we consider malicious network trafﬁc, such as HTTP trafﬁc that seeks to connect to dan-
gerous URLs, like command-and-control [C&C] servers, an attacker could use adversarial attack techniques
to disguise this malicious network trafﬁc as normal trafﬁc for the intrusion detection system while retaining
its malicious aspects. This can be done by adding small amounts of specially designed data to that network
trafﬁc as a padding for example. Therefore, the attacker could evade the intrusion detection system.

0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM on DNN-IDSPGD on DNN-IDSFGSM on Ensemble IDSPGD on Ensemble IDS8

Islam Debicha et al.

4.3 Detect & Reject for Adversarial Network Trafﬁc

(a) Detect & Reject for SVM-based IDS

(b) Detect & Reject for DT-based IDS

(c) Detect & Reject for LR-based IDS

(d) Detect & Reject for RF-based IDS

(e) Detect & Reject for LDA-based IDS

Fig. 4. Detect & Reject as a defense against the transferability property of adversarial network trafﬁc

In order to limit the effect of adversarial attacks in a black-box context, we implement the Detect &
Reject method in each of the ﬁve ML-based IDSs. This method consists of re-training the model to detect
not only ”abnormal” and ”normal” trafﬁc but also ”adversarial” trafﬁc. PGD is used against ”Training data
B” to generate ”Adversarial data”. After that, the ML model uses a combination of ”Training data B” and
”Adversarial data” during the training phase to learn to distinguish the three classes. During the prediction
phase, any network trafﬁc record recognised as ”adversarial” will be rejected. As shown in Figure 4, all
ﬁve ML-based IDSs have improved their robustness against adversarial attacks. Decision Tree and Random
Forrest, which is an ensemble version of Decision Tree, have the highest detection rates of adversarial network
trafﬁc compared to the other IDSs.

0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM without defensePGD without defenseFGSM against D&RPGD against D&R0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM without defensePGD without defenseFGSM against D&RPGD against D&R0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM without defensePGD without defenseFGSM against D&RPGD against D&R0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM without defensePGD without defenseFGSM against D&RPGD against D&R0.00.51.00.00.51.0Attack strength (ε)AccuracyFGSM without defensePGD without defenseFGSM against D&RPGD against D&RAdversarial Attacks against Intrusion Detection Systems

9

5 Conclusion and Future Work

From an intrusion detection system perspective, adversarial attacks are a serious threat, as a small intentional
perturbation of network trafﬁc can mislead the system. To generate these adversarial records, the attacker
must have access to the internal architecture of the machine learning model. However, by exploiting the
transferability property of adversarial attacks, he can mislead other intrusion detection systems without having
any knowledge about them. Ensemble IDSs, although known to improve model accuracy, are vulnerable to
these attacks and thus cannot improve model robustness. On the other hand, Detect & Reject has shown
through our experiments to be a suitable built-in defense for intrusion detection systems against adversarial
attacks. An interesting future work would be to design more effective defenses to limit the effect of adversarial
attacks against intrusion detection systems.

References

1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin,
M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
Levenberg, J., Man´e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,
I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi´egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke,
M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015). URL https:
//www.tensorﬂow.org/. Software available from tensorﬂow.org

2. Akhtar, N., Mian, A.: Threat of adversarial attacks on deep learning in computer vision: A survey. IEEE Access 6,

14,410–14,430 (2018)

3. Alamiedy, T.A., Anbar, M., Alqattan, Z.N., Alzubi, Q.M.: Anomaly-based intrusion detection system using multi-
objective grey wolf optimisation algorithm. Journal of Ambient Intelligence and Humanized Computing pp. 1–22
(2019)

4. Aslahi-Shahri, B., Rahmani, R., Chizari, M., Maralani, A., Eslami, M., Golkar, M.J., Ebrahimi, A.: A hybrid method
consisting of ga and svm for intrusion detection system. Neural computing and applications 27(6), 1669–1676 (2016)
5. Debicha, I., Debatty, T., Dricot, J.M., Mees, W.: Adversarial training for deep learning-based intrusion detection

systems. In: ICONS 2021 : The Sixteenth International Conference on Systems (2021)

6. Debicha, I., Debatty, T., Mees, W., Dricot, J.M.: Efﬁcient intrusion detection using evidence theory. In: INTERNET

2020 : The Twelfth International Conference on Evolving Internet, pp. 28–32 (2020)

7. Ghanem, W.A.H., El-Ebiary, Y.A.B., Abdulnab, M., Tubishat, M., Alduais, N.A., Nasser, A.B., Abdullah, N., Al-
wesabi, O.A.: Metaheuristic based ids using multi-objective wrapper feature selection and neural network classiﬁca-
tion. In: International Conference on Advances in Cyber Security, pp. 384–401. Springer (2020)

8. Ghanem, W.A.H., Jantan, A., Ghaleb, S.A.A., Nasser, A.B.: An efﬁcient intrusion detection model based on hy-
IEEE Access 8,

bridization of artiﬁcial bee colony and dragonﬂy algorithms for training multilayer perceptrons.
130,452–130,475 (2020)

9. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples.

arXiv preprint

arXiv:1412.6572 (2014)

10. Grosse, K., Manoharan, P., Papernot, N., Backes, M., McDaniel, P.: On the (statistical) detection of adversarial

examples. arXiv preprint arXiv:1702.06280 (2017)

11. Lu, Y., Jia, Y., Wang, J., Li, B., Chai, W., Carin, L., Velipasalar, S.: Enhancing cross-task black-box transferability of
adversarial examples with dispersion reduction. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 940–949 (2020)

12. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial

attacks. arXiv preprint arXiv:1706.06083 (2017)

13. Nicolae, M.I., Sinn, M., Tran, M.N., Buesser, B., Rawat, A., Wistuba, M., Zantedeschi, V., Baracaldo, N., Chen, B.,

Ludwig, H., et al.: Adversarial robustness toolbox v1. 0.0. arXiv preprint arXiv:1807.01069 (2018)

14. Papernot, N., McDaniel, P., Goodfellow, I.: Transferability in machine learning: from phenomena to black-box at-

tacks using adversarial samples. arXiv preprint arXiv:1605.07277 (2016)

10

Islam Debicha et al.

15. Pawlicki, M., Chora´s, M., Kozik, R.: Defending network intrusion detection systems against adversarial evasion

attacks. Future Generation Computer Systems 110, 148–154 (2020)

16. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss,
R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research 12, 2825–2830 (2011)

17. Rokach, L.: Ensemble-based classiﬁers. Artiﬁcial intelligence review 33(1), 1–39 (2010)
18. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R.: Intriguing properties of

neural networks. arXiv preprint arXiv:1312.6199 (2013)

19. Tavallaee, M., Bagheri, E., Lu, W., Ghorbani, A.A.: A detailed analysis of the kdd cup 99 data set. In: 2009 IEEE

symposium on computational intelligence for security and defense applications, pp. 1–6. IEEE (2009)

20. Vinayakumar, R., Alazab, M., Soman, K., Poornachandran, P., Al-Nemrat, A., Venkatraman, S.: Deep learning ap-

proach for intelligent intrusion detection system. IEEE Access 7, 41,525–41,550 (2019)

21. Wang, Z.: Deep learning-based intrusion detection with adversaries. IEEE Access 6, 38,367–38,384 (2018)
22. Xie, C., Zhang, Z., Zhou, Y., Bai, S., Wang, J., Ren, Z., Yuille, A.L.: Improving transferability of adversarial examples
with input diversity. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
2730–2739 (2019)

