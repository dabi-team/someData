1
2
0
2

r
a

M
5

]

R
C
.
s
c
[

1
v
6
0
8
3
0
.
3
0
1
2
:
v
i
X
r
a

MALBERT: USING TRANSFORMERS FOR CYBERSECURITY AND
MALICIOUS SOFTWARE DETECTION

A PREPRINT

Abir Rahali
ear4587@umoncton.ca

Moulay A. Akhlouﬁ
moulay.akhloufi@umoncton.ca

Perception, Robotics, and Intelligent Machines Research Group (PRIME)
Dept of Computer Science
Université de Moncton
Moncton, NB, E1A 3E9, Canada

March 8, 2021

ABSTRACT

In recent years we have witnessed an increase in cyber threats and malicious software attacks on
different platforms with important consequences to persons and businesses. It has become critical to
ﬁnd automated machine learning techniques to proactively defend against malware. Transformers,
a category of attention-based deep learning techniques, have recently shown impressive results in
solving different tasks mainly related to the ﬁeld of Natural Language Processing (NLP). In this paper,
we propose the use of a Transformers’ architecture to automatically detect malicious software. We
propose a model based on BERT (Bidirectional Encoder Representations from Transformers) which
performs a static analysis on the source code of Android applications using preprocessed features to
characterize existing malware and classify it into different representative malware categories. The
obtained results are promising and show the high performance obtained by Transformer-based models
for malicious software detection.

Keywords Malware detection · Cybersecurity · Transformers · Attention models · Android

1

Introduction

Due to the exponential growth of digitalization usage and easy access to internet technology, cyber threats on information
systems such as computers and smartphones increased. Malicious software (malware) is the primary tool used
by attackers to perform cyber attacks. Different malware categories such as Trojans, Adwares, and Risktools are
expeditiously developed and updated with recent encryption and deformation technologies to become a more severe
threat to cyberspace. This rapid development incident often with harmful consequences to different data users at the
level of both persons and businesses. For example, IBM reported that the average cost of a data breach is $3.86 million
as of 2020, however, the average time to identify a breach was 207 days [1]. As result, IT security professionals and
researchers need to update the tools available to automatically detect new malware attacks.

A series of studies were conducted to solve this issue. Different analysis experiments were used on the static, dynamic
and hybrid level [2] to extract various types of features such as binaries, permissions, and API calls. The features
collected then are passed through detection models developed using machine learning and deep learning tools. The use
of deep learning algorithms, for example, helped security specialists to analyze the most complex and targeted attacks.

The wildly used type is static analysis [3]. It is a known way of identifying malicious applications among benign
applications, and this analysis focuses on the source code of software components that may be affected by malware. It is
less expensive in terms of resources and time since no need to activate the malware by executing the code to capture the
features, it can identify the maliciousness at the code level. For static analysis, there are mainly three practices to detect

 
 
 
 
 
 
A PREPRINT - MARCH 8, 2021

and classify malware: Permission-based (verify if the requested permissions are needed for the app to ensure the normal
behavior on the app access and usage of the user data), Signature-based (identify if the app signature matches one of the
malware signatures among the library collected in advance), and speciﬁcation-based (verify if the app violates the rules
set by experts to decide the maliciousness of a program under inspection).

Recent research in deep learning, focus mainly on Transformer-based models such as BERT [4] and XLNet [5]. These
approaches clearly showed impressive results in various state-of-the-art natural language processing (NLP) [6] and
computer vision [7] tasks. Thanks to the attention mechanisms layers [8] added to the encoder-decoder architecture,
Transformers can focus on the most important patterns in the data, leading to a remarkable boost in performance.

In this paper, we propose a malware detection approach using a Transformer-based algorithm. We experiment with
different Transformer model architectures on our data. The dataset includes 11 different malware categories namely
adware, spyware, ransomware, clicker, dropper, downloader, riskware, SMS-sender, horse-trojan, backdoor, and banker
[9]. Our methodology focuses on the static analysis level on the source code of Android applications, to identify
different categories of malware. Indeed, we did not limit the features to permission-based only but considered the
whole software code as an important set of feature representation for the analysis. We started with training the model
with the features after preprocessing, then a binary classiﬁcation of the apps to malicious and benign, and ﬁnally a
cross-category classiﬁcation at the malware level.

Our main contributions include:

1. We propose a novel approach for malware detection using a Transformer-based model. According to our best

knowledge, this is the ﬁrst Transformer-based method used for malware classiﬁcation.

2. We model Android malware detection as a binary and a multi-label text classiﬁcation problem and propose a
novel feature representation by considering the software applications’ source code as a set of features. We
apply text preprocessing on these features to keep the important information like permissions, intents, and
activities.

3. We conduct extensive experiments on our preprocessed Android dataset collected from public resources
with different category-annotated labels. This preprocessed dataset will be released publicly for the research
community.

2 Background

Generally, sequence-to-sequence tasks are performed using an encoder-decoder model. And the RNN (Recurrent and
Recursive Nets) architectures were the most widely-used architecture for both the encoder and decoder. But these
architectures have some limitations.

2.1 RNN

RNN or also called sequence modeling like LSTM [10], is a family of neural networks for processing sequential data.
A recurrent network that maps an input sequence of x values to a corresponding sequence of output o values [10].
A loss L measures how far each O is from the corresponding training target y [10]. The loss L internally computes
y = sof tmax(O) and compares this to the target y [10]. In terms of limitations, RNN based architectures are hard to
parallelize because the forward propagation graph is inherently sequential each time step may be computed only after
the previous one, where both the runtime and the memory cost are O(t) and cannot be reduced since states computed in
the forward pass must be sorted until they are reused during the backward pass.

2.2 Encoder-Decoder

The seq2seq model normally has an encoder-decoder architecture [11], composed of an encoder that processes the input
sequence and compresses the information into a context vector of a ﬁxed length. This representation is expected to be a
good summary of the meaning of the whole source sequence. And a decoder that is initialized with the context vector to
emit the transformed output. The mean limitation of the encoder-decoder model is the disadvantage of this ﬁxed-length
context vector design is the incapability of remembering long sentences. While the decoder needs different information
at different time steps.

2.3 Transformer

Thus, in order to solve the limitations of both RNN and endocer-decoder architectures, the authors of Transformers [8]
have proposed a solution. They rely on the seq2seq encoder-decoder by replacing RNN with attention mechanisms.

2

A PREPRINT - MARCH 8, 2021

The attention mechanism allows the Transformers to have a very long term memory. A Transformer model can "attend"
or "focus" on all the previous tokens that have been generated. The attention mechanism allows the decoder to go back
over the entire sentence and selectively extract the information it needs during decoding. Attention gives the decoder
access to all the hidden states of the encoder. However, the decoder still has to make a single prediction for the next
word, so we can not just pass a whole sequence to it (we have to pass it some kind of synthesis vector). So it asks the
decoder to choose which hidden states to use and which to ignore by weighting the hidden states. The decoder then
receives a weighted sum of hidden states to use to predict the next word.

In this section, we deﬁne the context of the Transformer-based approach by presenting most of the approaches related
to the architectures of the approaches. The Transformer in NLP is a new architecture that aims at solving sequence
to sequence tasks while easily managing long-range dependencies. The Transformer has been proposed in [8]. A
Transformer is an architecture that avoids recurrence and relies entirely on an attention mechanism to draw global
dependencies between input and output. Prior to Transformers, dominant sequence transduction models were based on
complex recurrent or convolutional neural networks that include an encoder and a decoder.

Transformers also use an encoder and a decoder, but the elimination of recurrence in favor of attention mechanisms
allows for much greater parallelization than methods such as RNNs and CNNs. The transformation is undoubtedly a
huge improvement over the seq2seq models based on RNN. But it has its own set of limitations. Attention can only
be paid to text strings of ﬁxed length. The text must be divided into a number of segments or pieces before being
introduced into the system as input and this causes context fragmentation. For example, BERT [4], a new linguistic
representation model from Google AI, uses pre-training and ﬁne-tuning to create state-of-the-art models for a wide
range of tasks. These tasks include question answering systems, sentiment analysis, and linguistic inference. Transfer
learning has been used in NLP using pretrained language models that have transformative architectures.

Figure 1: Transformer Encoder Architecture

BERT or Bidirectional Encoder Representations from Transformers. Figure 1 shows the different layers of the
Transformer encoder implemented by BERT. This implementation improves standard Transformers by removing the
constraint of unidirectionality through the use of a pre-formed Masked Language Model (MLM) objective [12]. The
Masked Language Model randomly masks certain elements of the input, and the objective is to predict the original
vocabulary identiﬁer of the masked word based solely on its context. Unlike pre-training the left to the right language

3

A PREPRINT - MARCH 8, 2021

model, the objective of the MLM allows the representation to merge the left and right context, which allows us to
pre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a Next Sentence
Prediction task that jointly pre-trains the text pair representations.

Also, RoBERTa [13] an extension of BERT with some modiﬁcations in the pre-training procedure of the architecture,
uses the same architecture. The modiﬁcations include training the model longer, with larger batches, and on a larger
amount of data. Also, the predictive objective of the following sentence was deleted. And in order to on longer
sequences, the authors applied dynamic modiﬁcation on the masking scheme [12] applied to the training data. They also
collect a new dataset of comparable size to other privately used datasets to better control the effects of training set size.

In order to reduce the computation and training time of BERT, [14] proposed DistilBERT, a small, fast, cheap, and
lightweight Transformer model based on the BERT architecture. This new extension uses knowledge distillation [15]
during the pre-training phase in order to reduce the size of a BERT model by 40 %. To exploit the inductive biases
learned by larger models during pre-training, the authors introduce a triple loss combining namely; language modeling
loss, distillation loss, and cosine-distance loss.

Another approach based on the Transformer architecture is the autoregressive Transformer. XLNet [5], an example
of this new implementation, exploits the best of modeling and automatic language encoding while trying to avoid its
limitations. Instead of using a ﬁxed forward or backward factorization order as in classical autoregressive models,
XLNet maximizes the expected log probability of a sequence with all possible permutations of the factorization order
[6]. Also, thanks to the permutation operation usage, the context of each position can be made up of both left and right
tokens, which enable XLNet to capture the bidirectional context information of all positions.

In this paper, we used the presented models in order to evaluate our proposed approach, by conducting a list of
experiments on both binary and cross-category malware detection task.

3 Related works

Over the past years, various approaches were proposed in deep learning methods to detect malware. While these existing
methods are mainly divided between; on one hand the training of recurrent neural networks (RNN), and convolutional
neural networks (CNN), on a different set of extracted features, and on another hand the usage of attention mechanisms
characteristics for malware classiﬁcation.

3.1 Neural network-based approaches

Among these approaches this study [16] builds AMalNet, a DN framework to learn multiple integration representations
and family assignment with Graph CNN (GCNs) to model high-level graph semantics and use an Independent RNN
(IndRNN) to decode deep semantic information. SeqMobile [17], is a behavior-based sequence approach. it uses
different recurrent neural networks (RNN). It extracts the semantic feature sequence, which can provide information
of certain malicious behaviors, from binary ﬁles under a certain time constraint. This paper [18], presents a new
approach based on OpCode-level FCG. The FCG is obtained through static analysis of Operation Code (OpCode)
using a Long Short-Term Memory (LSTM). the authors conduct experiments on a dataset on 1,796 Android malware
samples classiﬁed into two categories and 1,000 benign Android apps. The authors of [19] focused on step size as
an important factor in relation to input size using RNN. They tested the model with three different feature vectors
(hot-coding feature vector, random feature vector and Word2Vec feature vector) using hyper parameters. [20] transform
the android package kit (APK) ﬁle into a lightweight RGB image using a predeﬁned dictionary and intelligent mapping,
then apply a CNN on the obtained images for malware family classiﬁcation. Multiple other examples of DNN based
approach [21] and [22], have been developed, with varying the feature extraction, selection, and representation methods
in the aim of boosting the detection results.

3.2 Approaches using Attention Mechanisms

While our approach is, to the best of found knowledge, the ﬁrst study to implement Transformers directly on software
application and preprocessing features like text to detect malware, few approaches have attempted ﬁrst steps towards
using new methods such as attention mechanisms.

For example, [23] propose SLAM a malware detection framework build based on the characteristics of the attention
mechanism and the sliding window method. It use a feature extraction method of the API execution sequence according
to its semantics. [24] uses of an residual attention based mechanism, based on this study the proposed method
outperformed traditional CNN models. Similarly, [25] propose a static analysis framework based on a set of N-gram
opcodes sequence patches with a self-attention based CNN named SA-CNN. Another example proposed by [26],

4

A PREPRINT - MARCH 8, 2021

applies a CNN with an attention mechanism to images converted from binary datasets, by calculating an attention map
to extract characteristic byte sequence. The distinction of regions in the attention map shows regions having higher
importance for classiﬁcation in the image.

While in our work, we propose a noval approach, we used BERT to better detect malware, we reﬁned the pre-trained
model to efﬁciently learn representations of source code language syntax and semantics. Our Context-Aware network
learns contextual characteristics from a natural language sentences perspective thanks to the attention mechanism layers
in the Transformer-based architecture.

4 Methodology

This section explains the overall process of malware detection. The core idea of this work is to create, a malware
detection framework using a Transformer-based approach. To reach this goal, we conducted a static analysis on the
collected corpora from a natural language sentences perspective. So, we need a dataset including source code ﬁles
and different categories of malware types. Figure 2 explains the logical ﬂow of our Android malware detection. This
Process is mainly divided into 4 phases. First, the Android ﬁles collection, then the Decompilation phase of the APK
ﬁles, Feature Mining, and ﬁnally Deep Learning (DL) models training experiments.

Figure 2: Overview of the Methodology Steps

4.1 Data Collection

We collected the Android applications from the Androzoo public dataset. Androzoo, one of the stae of the art android
malware dataset [27], is a growing collection of Android applications from several sources, including the ofﬁcial Google
Play app market. It currently contains 13,320,014 different APKs, each of which has been analyzed by dozens of
different antivirus products to ﬁnd out which applications are detected as malware. This public data is up to date with
weekly analysis on the samples [28]. The data is labeled based on these analyses into malware and benign, and different
malware categories and families.

Based on state-of-the-art taxonomies for Android malware categories [29] & [30] We selected 11 categories 3 namely;
adware (displays advertising and entice a user to install it on their device), spyware (installs itself on the user device
with the aim of collecting and transferring information without the user is aware of it), ransomware (takes personal data
hostage), clicker (a type of trojan that performs a form of ad fraud. These “clickers” continuously make connections to
websites, consequently awarding threat actors with revenue on pay-per-click bases), dropper (a syringe program or
dropper virus, is a computer program created to install malicious software on a target system), downloader (a type of
Trojan horse that downloads and installs malicious ﬁles), riskware (a software whose installation can represent a risk
for the security of the computer, but however, not inevitably), SMS-sender (presents itself as a regular SMS messaging
application and uses its basic permissions to send/receive short messages), horse-trojan (is designed to damage, disrupt,
steal, or in general inﬂict some other harmful action on your data or network), backdoor (when introduced into the
device, usually without the user’s knowledge, turns the software into a Trojan horse, and banker (is designed to steal
data from users’ online bank accounts as well as data from online payment systems and plastic card systems). We select
the list of APKs to download based on the recent creation and analysis date, then re-analyze this list with VirusTotal
[31], to ﬁnally create our dataset list including 12,000 benign apps and 10,000 malware apps.

5

A PREPRINT - MARCH 8, 2021

Figure 3: Android Malware Categories

4.2 Preprocessing and Feature representations

Once the list of APKs is deﬁned, we write a script to download the ﬁles. Then, we decompiled the downloaded APKs
using Jdax [32], which creates folders of the apps’ ﬁles [33]. We extracted the AndroidManifest.xml ﬁle from each
sample. The manifest presents essential information about the application to the Android system, information the
system must have before it can run any of the application’s code, including the list of permissions, the activities, services,
broadcast receivers, content providers, the version, and the meta-data. These ﬁles are then treated as text ﬁles and passed
through the preprocessing phase, in this step and to conserve the important information about the features, we apply
speciﬁc cleaning of the not important, mostly repeated words, in the code. We manually analyzed different examples
and created a list of words and expressions that do not provide additional info, so the cleaning included lexicon removal,
punctuation removal and we conserved the digits and the cases of the characters. The purpose of the preprocessing is to
reduce the size of the input. The ﬁnal dataset format has 4 columns, the ID column, represented by the APK hash name,
the Text column representing the Manifest ﬁles after preprocessing, the Label column, a binary format equal to 1 if the
app is malware and 0 if not, and ﬁnally the Category column representing the malware type name (exp: adware).

4.3 Proposed Approach

Once the data is created and annotated in the right format, we split the data into train and test. We conduct all
our experiments using BERT, we ﬁne-tuned it on our train dataset. We ﬁxed the hyperparameters based on each
classiﬁcation type. We train BERT to predict Malware/Benign (i.e., binary classiﬁcation) for each sample, then, to
predict the categories of malware (i.e, multi-classiﬁcation). The Transformer architecture has speciﬁc input formatting
steps including the creation of special tokens and ids. We use the Transformers implementation of the hugging face
library [34] for the binary classiﬁcation of Android applications. Only the Transformer architecture, layers, and weights
are implemented, while all data formatting must be done beforehand to be compatible with the Transformer. While
most pre-trained Transformers have essentially the same steps. Here we test this approach with BERT. Figure 4 gives a
detailed overview of our approach.

5 Experimental results

To test the proposed approach, we evaluate it in terms of three main aspects: (1) the proﬁtability on large and recent
categorical datasets, (2) the feature representation ability for information context extraction from android apps, and (3)
the performance compared to the state-of-the-art approaches.

6

A PREPRINT - MARCH 8, 2021

Figure 4: BERT model ﬁne-tuning process

5.1 Experiments setup

In our model implementation, we used Adam optimizer and set its hyperparameters as follow; the learning rate α to
2e-5 for binary classiﬁcation and 1e-3 for multi-classiﬁcation, α1=0,9 and α2=0,999. We set the max sequence size to
512 and the training batch size to 32.

5.2 Baselines

We compare the proposed model with several state-of-the-art methods including LSTM [35]. We selected this sequence
model based on its performance in the reviewed studies. And, XLNet [5], RoBERTa [13], and DistilBERT [14] as
Transformer based baselines to compare their performance to BERT.

5.3 Evaluation Metrics

To evaluate the effectiveness of the model and avoid the contingency caused by the partitioning of the train and test sets,
we select three metrics to evaluate the model (Accuracy, MCC, and Loss), which are commonly used in classiﬁcation
problems. We deﬁne the following numbers; TP (true positives), FN (false negatives), FP (false positives), and TN (true
negatives).

1. ACC: The accuracy is a metric for evaluating classiﬁcation models. It is equal to the number of correct

predictions divided by the total number of predictions, as equation 1 present:

Accuracy =

T P + T F
T P + T F + F P + F N

(1)

2. MCC: The Matthews Correlation Coefﬁcient (MCC) is bast used for binary classiﬁcation with an unbalanced
dataset. It has a range of -1 to +1. We chose MCC over F1-score for binary classiﬁcation as recommended in
this study [36]. MCC equation is deﬁned as fellow :

MCC =

TP × TN − FP × FN
(cid:112)(TP + FP)(TP + FN )(TN + FP)(TN + FN )

(2)

3. F1: The formula for the standard F1-score is the harmonic mean of the precision and recall. A perfect model
has an F-score of 1. We used macro averaging in the results we presented in this paper. The formula of
F1-score is deﬁned as follow:

F 1 =

2 ∗ T P
2 ∗ T P + F P + F N

(3)

4. Loss: We used the cross-entropy loss, or log loss [37]. It measures the performance of a classiﬁcation
model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted

7

A PREPRINT - MARCH 8, 2021

probability diverge from the actual label. The cross-entropy H(p, q) of the probability distribution p relative
to a probability distribution q is given as:

H(p, q) = −

(cid:88)

x∈X

p(x) log q(x)

(4)

5.4 Results and Analysis

We conducted the experiments on the preprocessed dataset. Fine-tuning the pre-trained models, clearly gave the highest
accuracy results for this classiﬁcation task compared to the LSTM baseline. The best classiﬁcation model is BERT. The
test metrics results of Table 1 show that each Transformer learns differently depending on each architecture. The results
in Table 1 and Table 2 prove that BERT outperformed the other baseline models in both binary and multi-classiﬁcation
malware detection. For BERT, the best learning rate shows that only two epochs are required before the loss starts to
increase. Our ﬁne-tuning with the training set included changing the hyperparameters to boost the results. To evaluate
the ﬁnal results, we used different evaluation metrics. The pretrained models achieved good results overall, but BERT
obtained the best performance in both tasks.

Algos
LSTM
XLNet

ACC
0.9405
0.9579
RoBERTA 0.9533
DistilBERT 0.9542
0.9761

BERT

F1
0.9382
0.9549
0.9499
0.9542
0.9547

Loss
0.1521
0.1461
0.1430
0.1305
0.1274

MCC
0.9077
0.9164
0.9070
0.9087
0.9559

Table 1: Detection results using the feature representation approach across difference networks on the test dataset for
both binary classiﬁcation.

Algos
LSTM
XLNet

ACC
0.8507
0.6232
RoBERTA 0.6491
DistilBERT 0.5981
0.9102

BERT

F1
0.7816
0.7448
0.7670
0.7274
0.7804

Loss
0.3521
0.8106
0.7611
0.8505
0.2214

Table 2: Detection results using the feature representation approach across difference networks on the test dataset for
cross-category malware classiﬁcation.

6 Conclusion and future works

This paper studies the challenge of malware classiﬁcation using our novel approach for Transformer-based malware
detection. We detailed the malware text classiﬁcation methodology and used it for feature representation. The BERT
based model achieved high accuracy results for both binary and cross-category classiﬁcation, compared to the other
baseline pre-trained language models. The results from the experiments show that the best binary accuracy is 0.9761 and
for the multi-classiﬁcation, it is 0.9102. We can conclude that the proposed approach’s results of feature representation
as text input for a Transformer-based model, are very good. So, the implementation of pre-trained linguistic models
based on Transformer architectures in cybersecurity tasks can outperform standard RNN models like LSTM, when
applied on a state-of-the-art dataset like Androzoo. Future work will therefore consist of testing other models, testing
other types of data, and creating an API to detect malware in new applications.

References

[1] IBM. Cost of a data breach report 2020. 2020.
[2] Anusha Damodaran, Fabio Di Troia, Corrado Aaron Visaggio, Thomas H Austin, and Mark Stamp. A comparison
of static, dynamic, and hybrid analysis for malware detection. Journal of Computer Virology and Hacking
Techniques, 13(1):1–12, 2017.

[3] Ya Pan, Xiuting Ge, Chunrong Fang, and Yong Fan. A systematic literature review of android malware detection

using static analysis. IEEE Access, 8:116363–116379, 2020.

8

A PREPRINT - MARCH 8, 2021

[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional

transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[5] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet:
Generalized autoregressive pretraining for language understanding. In Advances in neural information processing
systems, pages 5753–5763, 2019.

[6] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv preprint

arXiv:2009.06732, 2020.

[7] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah.

Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021.

[8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages
5998–6008, 2017.

[9] kaspersky. Android mobile security threats. kaspersky, 2012.
[10] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
[11] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances

in neural information processing systems, pages 3104–3112, 2014.

[12] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to sequence pre-training

for language generation. arXiv preprint arXiv:1905.02450, 2019.

[13] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint
arXiv:1907.11692, 2019.

[14] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller,

faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

[15] Mary Phuong and Christoph Lampert. Towards understanding knowledge distillation. In International Conference

on Machine Learning, pages 5142–5151, 2019.

[16] Xinjun Pei, Long Yu, and Shengwei Tian. Amalnet: A deep learning framework based on graph convolutional

networks for malware detection. Computers & Security, page 101792, 2020.

[17] Ruitao Feng, Jing Qiang Lim, Sen Chen, Shang-Wei Lin, and Yang Liu. Seqmobile: A sequence based efﬁcient
android malware detection system using rnn on mobile devices. arXiv preprint arXiv:2011.05218, 2020.

[18] Weina Niu, Rong Cao, Xiaosong Zhang, Kangyi Ding, Kaimeng Zhang, and Ting Li. Opcode-level function call

graph based android malware classiﬁcation using deep learning. Sensors, 20(13):3645, 2020.

[19] Sudan Jha, Deepak Prashar, Hoang Viet Long, and David Taniar. Recurrent neural network for detecting malware.

Computers & Security, 99:102037, 2020.

[20] Farid Naït-Abdesselam, Asim Darwaish, and Chaﬁq Titouna. An intelligent malware detection and classiﬁcation
system using apps-to-images transformations and convolutional neural networks. In 2020 16th International
Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)(50308), pages 1–6.
IEEE, 2020.

[21] Zhiqiang Wang, Qian Liu, and Yaping Chi. Review of android malware detection based on deep learning. IEEE

Access, 8:181102–181126, 2020.

[22] Francesco Mercaldo and Antonella Santone. Deep learning for image-based mobile malware detection. Journal of

Computer Virology and Hacking Techniques, pages 1–15, 2020.

[23] Jun Chen, Shize Guo, Xin Ma, Haiying Li, Jinhong Guo, Ming Chen, and Zhisong Pan. Slam: A malware
detection method based on sliding local attention mechanism. Security and Communication Networks, 2020, 2020.

[24] Shamika Ganesan, Moez Krichen, Roobaea Alroobaea, Soman KP, et al. Robust malware detection using residual

attention network. TechRxiv, 2020.

[25] Bin Zhang, Wentao Xiao, Xi Xiao, Arun Kumar Sangaiah, Weizhe Zhang, and Jiajia Zhang. Ransomware
classiﬁcation using patch-based cnn and self-attention network on embedded n-grams of opcodes. Future
Generation Computer Systems, 110:708–720, 2020.

[26] Hiromu Yakura, Shinnosuke Shinozaki, Reon Nishimura, Yoshihiro Oyama, and Jun Sakuma. Malware analysis
of imaged binary samples by convolutional neural network with attention mechanism. In Proceedings of the
Eighth ACM Conference on Data and Application Security and Privacy, pages 127–134, 2018.

9

A PREPRINT - MARCH 8, 2021

[27] Kevin Allix, Tegawendé F. Bissyandé, Jacques Klein, and Yves Le Traon. Androzoo: Collecting millions of
android apps for the research community. In Proceedings of the 13th International Conference on Mining Software
Repositories, MSR ’16, pages 468–471, New York, NY, USA, 2016. ACM.

[28] Pei Liu, Li Li, Yanjie Zhao, Xiaoyu Sun, and John Grundy. Androzooopen: Collecting large-scale open source
android apps for the research community. In Proceedings of the 17th International Conference on Mining Software
Repositories, pages 548–552, 2020.

[29] Najiahtul Syaﬁqah Ismail, Halizah Saad, Robiah Yusof, and Mohd Faizal Abdollah. General android malware
behaviour taxonomy. SCIENCE & TECHNOLOGY RESEARCH INSTITUTE FOR DEFENCE (STRIDE), page
160, 2017.

[30] Rashmi Rupendra Chouhan and Alpa Kavin Shah. A preface on android malware: Taxonomy, techniques and
tools. International Journal on Recent and Innovation Trends in Computing and Communication, 5(6):1111–1117,
2017.

[31] Virus Total. Virustotal-free online virus, malware and url scanner. Online: https://www. virustotal. com/en, 2012.
[32] jadx. jadx - dex to java decompiler. Online: https://github.com/skylot/jadx, 2012.
[33] Nicolas Harrand, César Soto-Valero, Martin Monperrus, and Benoit Baudry. Java decompiler diversity and its

application to meta-decompilation. Journal of Systems and Software, page 110645, 2020.

[34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,
Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language
processing. ArXiv, pages arXiv–1910, 2019.

[35] Ralf C Staudemeyer and Eric Rothstein Morris. Understanding lstm–a tutorial into long short-term memory

recurrent neural networks. arXiv preprint arXiv:1909.09586, 2019.

[36] Davide Chicco and Giuseppe Jurman. The advantages of the matthews correlation coefﬁcient (mcc) over f1 score

and accuracy in binary classiﬁcation evaluation. BMC genomics, 21(1):6, 2020.

[37] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy

labels. Advances in neural information processing systems, 31:8778–8788, 2018.

10

