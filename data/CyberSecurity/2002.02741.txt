0
2
0
2

b
e
F
7

]

G
L
.
s
c
[

1
v
1
4
7
2
0
.
2
0
0
2
:
v
i
X
r
a

Can’t Boil This Frog: Robustness of Online-Trained
Autoencoder-Based Anomaly Detectors to
Adversarial Poisoning Attacks

Moshe Kravchik
Dept. of Software and Information Systems Engineering
Ben-Gurion University of the Negev
Beer-Sheva, Israel
moshekr@post.bgu.ac.il

Asaf Shabtai
Dept. of Software and Information Systems Engineering
Ben-Gurion University of the Negev
Beer-Sheva, Israel
shabtaia@bgu.ac.il

Abstract—In recent years, a variety of effective neural
network-based methods for anomaly and cyber attack detection
in industrial control systems (ICSs) have been demonstrated
in the literature. Given their successful
implementation and
widespread use, there is a need to study adversarial attacks on
such detection methods to better protect the systems that depend
upon them. The extensive research performed on adversarial
attacks on image and malware classiﬁcation has little relevance
to the physical system state prediction domain, which most of the
ICS attack detection systems belong to. Moreover, such detection
systems are typically retrained using new data collected from the
monitored system, thus the threat of adversarial data poisoning
is signiﬁcant, however this threat has not yet been addressed by
the research community. In this paper, we present the ﬁrst study
focused on poisoning attacks on online-trained autoencoder-based
attack detectors. We propose two algorithms for generating
poison samples, an interpolation-based algorithm and a back-
gradient optimization-based algorithm, which we evaluate on
both synthetic and real-world ICS data. We demonstrate that
the proposed algorithms can generate poison samples that cause
the target attack to go undetected by the autoencoder detector,
however the ability to poison the detector is limited to a small
set of attack types and magnitudes. When the poison-generating
algorithms are applied to the popular SWaT dataset, we show
that the autoencoder detector trained on the physical system state
data is resilient to poisoning in the face of all ten of the relevant
attacks in the dataset. This ﬁnding suggests that neural network-
based attack detectors used in the cyber-physical domain are
more robust to poisoning than in other problem domains, such
as malware detection and image processing.

Index Terms—Anomaly detection; industrial control systems;
autoencoders; adversarial machine learning; poisoning attacks;
adversarial robustness.

I. INTRODUCTION

Neural network-based anomaly and attack detection meth-
ods have attracted signiﬁcant attention in recent years. The
ability of neural networks (NNs) to accurately model complex
multivariate data has contributed to their use in detectors in
various areas ranging from medical diagnostics to malware
detection and cyber-physical systems monitoring. An attack
detection system’s effectiveness depends heavily on its robust-
ness to attacks that target the detection system itself. In the
context of NNs, such attacks are known as adversarial data

attacks. Adversarial attacks have been a major focus of the
NN research community, primarily in the image classiﬁcation
([1], [2]), malware detection ([1], [3]), and network intrusion
detection ([4], [5]) domains.

This research focuses on NN-based anomaly and cyber
attack detectors in industrial control systems (ICSs) which
are a subclass of cyber-physical systems (CPSs). ICSs are
central to many important areas of industry, energy production,
and critical infrastructure. The security and safety of ICSs are
therefore of the utmost importance. While a number of recent
studies have proposed using NN-based detectors to improve
ICS security, the resilience of such detectors to adversarial
attacks has received little attention.

Adversarial attacks on NNs can be broadly divided into
poisoning and evasion attacks. Both kinds of attacks use
maliciously crafted data to achieve their goals. Evasion attacks
aim to craft test data samples that will evade detection while
still producing the desired adversarial effect (e.g., service
disruption). Recently, three studies examining adversarial at-
tacks on NN ICS detectors were published, all of which were
dedicated to evasion attacks ([6], [7], [8]) performed using
different threat models.

On the other hand, poisoning attacks attempt to introduce
adversarial data during the model’s training. This data inﬂu-
ences the model in such a way that the target attack remains
undetected at test time. However, the model’s detection behav-
ior for all other inputs is maintained, ensuring that no suspicion
of an attack is raised. The importance of poisoning attack
research has increased in the light of the popularity of ICS
monitoring systems’ online training mode. With online train-
ing, the model is periodically trained with new data collected
from the protected system to accommodate for the concept
drift. The fact that this retraining provides the adversary with
the opportunity to poison the model underscores the need to
study poisoning attacks on ICS anomaly and attack detectors.
However, this task is challenging. According to [9], the impact
of adversarially manipulated training data on the output of
regression learning models (which are used in most NN ICS
detectors) is not well understood. Until now, the vast majority

 
 
 
 
 
 
of poisoning research has dealt with classiﬁcation problems,
while regression learning poisoning has received less attention,
with a focus on simpler machine learning algorithms, such as
linear regression [9].

To the best of our knowledge,

this is the ﬁrst study
addressing poisoning attacks on deep semi-supervised NN
detectors for physics-based multivariate time sequences, which
are common in ICSs. In this research, we investigate the
following unique combination of issues related to anomaly
and attack detection in ICSs.

1) It targets a semi-supervised machine learning detec-
tor, while prior research largely examined adversarial
attacks on supervised learning.

2) It aims to poison a regression-based deep NN detector,
while prior research dealt mostly with classiﬁcation
problems.

3) Our proposed solution is capable of identifying a se-
quence of series of vectors, while previous studies
considered single poisoning data points.

4) Given the online nature of the training, the order of
the poisoning inputs is relevant, an issue which has not
been considered in previous research.

5) Most NN ICS detectors process data in multiple over-
lapping time series, with the same data serving as the
target output for some predictions and the input for
others, magnifying the inﬂuence of the manipulation of
a single time point.

Our study aims to answer the following research questions.
1) What algorithms can be used to generate poisoning
input for a NN-based ICS anomaly detector operating
in online training mode?

2) How robust are the detectors proposed in [10], [7], [6]

to such poisoning attacks?

The contributions of this paper are as follows:
• We present the ﬁrst study of poisoning attacks on online
trained NN-based detectors for multivariate time series.
• We propose two algorithms for the generation of poi-
soning samples in such settings: an interpolation-based
algorithm and a back-gradient optimization-based algo-
rithm.

• We implement and validate both algorithms on synthetic
data and evaluate the inﬂuence of various test parameters
on the poisoning abilities of the algorithms.

• We apply the algorithms to an autoencoder-based detector
for real-world ICS data and study the detector’s robust-
ness to poisoning attacks.

• The implementation of both algorithms and the evaluation

tests code are open source and freely available.1

II. BACKGROUND

In this section, we introduce the notation and provide
background on ICSs, ICS anomaly detection, and poisoning
attacks.

1The link will be provided after the review to preserve anonymity

A. Industrial control systems and anomaly detection

ICSs are comprised of network-connected computers that
monitor and control physical processes. These computers
obtain feedback about the monitored process from sensors
and can inﬂuence the process using actuators, such as pumps,
engines, and valves. Typically, the sensors and actuators are
connected to a local computing element, a programmable logic
controller (PLC), which is a real-time specialized computer
that runs a control
loop supervising the physical process.
The PLCs, sensors, and actuators form a remote segment
of the ICS network. Other important ICS components reside
in another network segment,
the control segment, whose
constituents typically include a supervisory control and data
acquisition (SCADA) workstation, a human-machine interface
(HMI) machine, and a historian server. The SCADA computer
runs the software responsible for programming and controlling
the PLC. The HMI receives and displays the current state of
the controlled process, and the historian keeps a record of all
of the sensory and state data collected from the PLC.

In our research, we consider another component residing in
the control segment, an anomaly and attack detector, which
is sometimes also referred to as an intrusion detection system
(IDS). Its role is to analyze the system state and detect possible
anomalies and attacks on the system. For that purpose, the
detector can use all available information, including network
trafﬁc and sensory data.

Fig. 1. A schematic SCADA system diagram under the selected threat model.

Various approaches for building such detectors were sur-
veyed in [11], [12], and [13]; of these, physics-based attack
detectors are the most relevant to our research. Such detectors
are based on a fundamental idea that the behavior of the
protected system is bound by immutable laws of physics
and therefore can be modeled with sufﬁcient precision. The
anomalous behavior is detected when the observed physical
system state deviates from the model-based prediction. In a
recent paper, Giraldo et al. [14] reviewed close to ﬁfty physics-
based IDSs.

Our research focuses on detectors that use NNs to model the
monitored process. Recently, this approach has become very
popular (e.g., [15], [10], [16], [17], [6], [7]), due to the ability
of NNs to model complex multivariate systems with non-linear
dependencies between the variables. In the literature, various
NN architectures have been used for anomaly and intrusion
detection, including convolutional NNs, recurrent NNs, feed-
forward NNs, and autoencoders [18] (an extensive review was
provided in [19]).

While our research method is agnostic to the detector’s
architecture, we chose to evaluate our method using autoen-
coders due to their simplicity and popularity [10], [7], [6]. In
the sections that follow, we introduce the notation and provide
the necessary background on poisoning attacks.

B. Notation

We follow the notation used in [20] and [21]. Unlike
classiﬁcation problems that distinguish between instance space
and label space, our problem deals with a single feature space
Y. In the context of ICS IDSs, the elements of this space
are the sensor measurements and actuator states observed by
the PLC and collected in by the historian. These elements,
yi ∈ RN , are N-dimensional real-valued vectors representing
the values of features (e.g., different sensors’ readings) at time
i. In the general case, the goal of the model M is to predict
the future feature values based on the past:

(ˆyh+n, ˆyh+n+1, . . . , ˆyh+n+m) = f (yn−1−l, . . . , yn−1),

(1)

where yi is a feature vector at time i, ˆyi is the estimation
of the feature vector, l and m represent the input and output
sequence length respectively, and h is the prediction horizon.
In other words, the model uses historical values of the system
state for the last l time steps to predict the future values of
the state for some m time steps starting at the h-th time step
from now.

For undercomplete autoencoders [18], the model learns a
slightly different function - reconstructing the feature vector
sequence from its compact code representation:

(2)

zn = fe(yn−l, . . . , yn)
(ˆyn−l, . . . , ˆyn) = fd(zn),
where zn is a vector in code space Z, zi ∈ RM , and fe
and fd are the functions learned respectively by the model’s
encoder and decoder parts. An important property of the code
vector zn is its size which must be less than the size of the
input sequence, M < N · l. The learning is performed by
minimizing an objective function L(D, w) on a given training
set Dtr = {yi}n
i=1, where w are the model and learning
parameters and hyperparameters. This objective function L
might include regularization, and hence we introduce L(D, w)
to denote the loss function evaluated on the dataset D using
parameters w.

C. Poisoning attacks

This research focuses on poisoning attacks. In such attacks,
the goal of the adversary is to inﬂuence the model by tamper-
ing with its training input. There are two possible goals of such

tampering: harming the detector’s availability or its integrity.
In the context of regression-based anomaly detection, the aim
of an availability attack is to cause the detector to generate
numerous false alarms for valid data, while the aim of an
integrity attack is to cause the detector to fail to detect an
attack by predicting values close enough to the values of the
features during the attack.

Previous work on regression learning poisoning focused
on availability attacks [9]. Our research targets the detector’s
integrity: given an attack sequence Ya = (y1
a ), our
goal is to produce a sequence of poisoning input sequences
Dp = (y1
j=1 so that when trained with Dtr ∪ Dp,
the detector will not issue an alert upon encountering Ya or
issue false alerts for the untainted data.

p, . . . , ym

a, . . . , yn

p )k

The attacker’s capabilities depend on his/her knowledge
of the targeted system. This can include knowledge on the
training set Dtr, the feature set Y, the learning algorithm M
and its hyperparameters, the learning objective function L, and
the learned parameters w [21]. Thus, the attacker’s knowledge
can be described in terms of space Θ, comprised of elements
θ = (Dtr, Y, L, w). The white-box attack scenario in which
the attacker knows all of the components of θ represents the
worst case, and this is the scenario we are considering in this
study. Given an anomaly detection function A(D,w) which
returns the number of alerts for the given input set D and a
model with trained weights w, the goal of our research is to
ﬁnd such Dp that the following holds true:

A(Dval ∪ Ya, w) = 0,

(3)

where Dval is a validation dataset which doesn’t contain any
attacks.

As the anomaly detection function A depends on the value
of the loss function, we can restate our problem as a bilevel
optimization problem:

D(cid:63)

p ∈ arg min
D(cid:48)
p∈Φ(Dp)

L(Dval ∪ Ya, ˆw)

s.t. ˆw ∈ arg min

w(cid:48) ∈W

L(Dtr ∪ Dp, w

(cid:48)

),

(4)

(5)

where the inner problem is the learning problem, and the
function Φ expresses constraints on the values of the poisoning
sequences. The poisoning sequences inﬂuence our objective
indirectly, through the parameters ˆw of the model trained on
them. The algorithms used for solving this bilevel optimization
problem are described in Section V.

III. RELATED WORK

A number of recent studies have focused on evasion attacks
on CPS anomaly detectors. In [22], the authors showed that
generative adversarial networks (GANs) can be used for real-
time learning of an unknown ICS anomaly detector (more
speciﬁcally, a classiﬁer) and for the generation of malicious
sensor measurements that will go undetected.

The research in [23] presents an iterative algorithm for gen-
erating stealthy attacks on linear regression and feed-forward

neural network-based detectors. The algorithm uses mixed-
integer linear programming (MILP) to solve this problem.
For neural network detectors, the algorithm ﬁrst linearizes the
network at each operating point and then solves the MILP
problem. The paper demonstrates a successful evasion attack
on a simulated Tennessee Eastman process.

Recently, Erba et al. [6] demonstrated a successful real-time
evasion attack on an autoencoder-based detection mechanism
in water distribution systems. The authors of [6] considered a
white-box attacker that generates two different sets of spoofed
sensor values: one is sent to the PLC, and the other is sent to
the detector.

The most recent paper in this area [8] also focused on
an adversary that can manipulate sensor readings sent to the
detector. The authors showed that such attackers can conceal
most of the attacks present in the SWaT dataset. Our study
differs from these studies in a number of ways. First and
foremost, all of the abovementioned papers examined evasion
attacks, while our research focuses on poisoning attacks.
Second, [22], [6], [8] considered a threat model in which the
attacker manipulates the detector’s input data in addition to
manipulating the sensor data fed to the PLC. Such a model
provides a lot of freedom for the adversary to make changes to
both types of data. Our threat model considers a signiﬁcantly
more constrained attacker that can only change the sensory
data that is provided both to the PLC and the detector.

A few recently published papers have studied poisoning
attacks, however the authors considered them in a different
context. The study performed by Mu˜noz-Gonz´alez et al. [21]
was the ﬁrst one to successfully demonstrate poisoning attacks
on multiclass classiﬁcation problems. It also was the ﬁrst to
suggest generating poisoning data using back-gradient opti-
mization. Our research extends this method to semi-supervised
multivariate time series regression tasks in the online training
setting and evaluates the robustness of an autoencoder-based
detector to such attacks.

Shafani et al. [2] and Suciu et al. [1] studied clean-label
poisoning of classiﬁers. In targeted clean-label poisoning, the
attacker does not have control of the labels for the training
data and changes the classiﬁer’s behavior for a speciﬁc test
instance without degrading its overall performance for other
test inputs. These studies differ signiﬁcantly from ours, both
in terms of the learning task to be poisoned (classiﬁcation vs.
regression) and in the domain (images vs. long interdependent
multivariate time sequences).

Madani et al. [5] studied adversarial label contamination
of autoencoder-based intrusion detection for network trafﬁc
monitoring. Their research considered a black-box attacker
that gradually adds existing malicious samples to the training
set, labeling them as normal. Such a setting is very different
from the one studied in our work. First, we consider semi-
supervised training; thus, there is no labeling involved. Second,
we explored algorithms for generating adversarial poisoning
samples that will direct the detector’s outcome towards the
target goal.

In this section, we provided a brief review of related

published research. Although some of the previous research
has dealt with related topics or domains, to the best of our
knowledge, this study is the ﬁrst one addressing poisoning
attacks on multivariate regression learning algorithms, and
speciﬁcally on online-trained physics-based anomaly and in-
trusion detectors in CPSs.

IV. THREAT MODEL

In this study, we consider a malicious sensor threat model
widely studied in the wireless sensor network domain ([24],
[25]). This model was used in the context of adversarial
attacks on ICS detectors in [23] and [7] and is illustrated
in Figure 1. Under this model, the attacker possesses the
knowledge of the historical values measured by the sensors and
can spoof arbitrary values of the sensors’ readings, however
both the PLC and the detector see the same spoofed values.
We selected this model due to its high relevance to the ICS
domain. Consider an ICS with sensors distributed over a
large area, which send their data to a PLC residing at a
physically protected and monitored location. In this setup, the
adversary can replace the original sensor with a malicious
one, reprogram the sensor, change its calibration, inﬂuence
the sensor externally, or just send false data to the PLC over
the cable/wireless connection, but the attacker cannot penetrate
the physically protected PLC-to-SCADA network. We argue
that this setup is much more realistic than one in which an
attacker controls the internal network of the remote segment
or even the network of the control segment considered by [6]
and [8]. We also argue that our threat model presents more
constraints and challenges to the adversary who must achieve
his/her goals with a single point of data manipulation.

The attacker’s ultimate goal is to carry out a speciﬁc attack
involving signiﬁcant changes to the values measured by one or
more sensors. For example, the attacker might aim to report a
very low water level in a tank, while in reality the tank is full,
thus causing it to overﬂow. Without poisoning, the detector
would raise an alert upon encountering the spoofed value
for the level, as it deviates from the normal system physical
behavior patterns learned. Hence, the goal of the attacker is to
poison the detector’s model so that after the attack has been
launched the detector accepts a spoofed value as normal.

To poison the detector’s model, the attacker exploits the
detector’s online learning. Unlike in [7], we consider a
practice common in ICSs, in which the detector is periodically
retrained by using the newly collected data from the monitored
system. The goal of this online training is to compensate for
the concept drift common in physical systems. We therefore
assume that
the measured sensor values of the monitored
system are added to the training dataset, and thus the attacker
can gradually poison the detector to steer it towards the desired
attack. For simplicity, we assume that the attacker knows
when to inject the poisoning data so that it will be used for
retraining. We also assume that only the data that does not
trigger alerts will be used for retraining. This is a reasonable
assumption, as concept drift is a gradual phenomenon, and the
goal of the online training is to allow the detector to adjust to

these gradual changes. This assumption imposes constraints on
the level of manipulations the attacker applies to the sensor
values. On the other hand, the attacker can add poisoning
points gradually, allowing the system to learn from previous
poisoning and thus achieve his/her goal iteratively.

In this study, we assume a white-box attacker that knows
the parameters of learning, the learned weights, the detection
algorithm, and the hyperparameters, as well as the online
training schedule.

V. METHODOLOGY

In this section, we ﬁrst present the challenges of ﬁnding
poisoning examples within the context of our research. We
then present two algorithms used to solve this problem and
discuss how we applied them to two different modes of
data processing: complete signal reconstruction and short-
subsequence signal reconstruction. As mentioned in Section
II-C,
the ﬁrst challenge is to assess the inﬂuence of the
poisoning examples on the model’s prediction for the attack
and validation inputs (Equations 4-5). The difﬁculty lies in the
fact that the inﬂuence is indirect, through the weights updates
during the training process, and these updates are not available
even for a white-box attacker. Additional challenges arise from
the overlapping signal subsequence modeling common in ICS
detectors. As noted in Section V-B where this challenge is
discussed further, this will force the attacker to consider the
very long-term impact of the poisoning on the manipulated
signal. To clarify the terminology, in this section and those
that follow, we refer to a single manipulated sequence of
feature vectors as a poisoning point. As we are looking for a
sequence of sequences that will achieve the desired adversarial
goal, calling the individual sequences poisoning points helps
distinguish between the outcome of a single iteration of the
poisoning algorithm (a poisoning point) and the resulting set
of poisoning points.

A. Finding a poisoning sequence with back-gradient optimiza-
tion

First, we note that in order to reach the attacker’s goal
(Equation 3), the attacker would proceed by starting with an
empty set and iteratively ﬁnding the next poisoning point by
solving the bilevel optimization problem. Thus, each iteration
step can be rewritten as solving

y(cid:63)
c ∈ arg min
y(cid:48)
c∈Φ(yc)

A(y

(cid:48)

c) = L(Dval ∪ Ya, ˆw)

s.t. ˆw ∈ arg min

w(cid:48) ∈W

L(Dtr ∪ y

(cid:48)

(cid:48)

c, w

),

or, in a simpliﬁed form,

y(cid:63)
c ∈ arg min
y(cid:48)
c∈Φ(yc)

A(y

(cid:48)

c) = L(Ya, ˆw)

s.t. ˆw ∈ arg min

L(y

w(cid:48) ∈W

(cid:48)

(cid:48)

c, w

),

(6)

(7)

(8)

(9)

where yc is the poisoning point optimized in the current
iteration. The iterations stop when Equation 3 is satisﬁed.

Algorithm 1 Find ∇ycA given trained parameters wT , learn-
ing rate α, attack input ya, poisoning point yc, loss function
L, and learner’s objective L, using back-gradient descent for
T iterations.

1: function GETPOISONGRAD(wT , α, ya, yc, L, L)
2:
3:
4:
5:

dyc ← 0
dw ← ∇wL(ya, wT )
for t = T to 1 do

dyc ← dyc − αdw∇yc∇wL(yc, wt)
dw ← dw − αdw∇w∇wL(yc, wt)
gt−1 ← ∇wL(yc, wt)
wt−1 ← wt + αgt−1

6:
7:
8:

9:

return dyc

One approach for solving Equations 8-9 is to use gradient

ascent [21]:

∇ycA = ∇yc L +

T

δ ˆw
δyc

∇wL.

(10)

As neither the validation set Dval nor the attack input
contains the poisoning points, there is no explicit dependency
between the attacker’s objective A and the poisoning points.
Therefore, the ﬁrst part of Equation 10 is equal to zero, and
our goal is to ﬁnd the value for δ ˆw
that reﬂects the inﬂuence
δyc
of the poisoning point on the attacker’s objective through
the learned weights. However, it is impossible to use direct
numeric methods to calculate this inﬂuence, as the weights are
updated multiple times during the learning process, and their
intermediate values are not stored and are thus not available
to the adversary. To cope with this problem, we use back-
gradient optimization [26], as suggested in [21]. The core idea
of this approach is iterative backwards calculation of both the
weights’ updates and δ ˆw
, performed by reversing the learning
δyc
process and calculating the second gradients in each iteration.
In our research, we implemented back-gradient optimization
for stochastic gradient descent according to Algorithm 1
(based on [21]). Algorithm 1 starts with initializing the deriva-
tives of the loss relative to the attack input and the weights of
the trained model (lines 2 and 3). Then it iterates for a given
number T iterations, rolling back the weights’ updates made
by the training optimizer (lines 7 and 8). In each iteration, the
algorithm calculates the second derivatives of the loss relative
to the weights and the attack input at the current weights’
values (lines 5 and 6) and updates the values maintained for
both derivatives. The ﬁnal value of ∇ycA accumulates the
compound inﬂuence of the poison input through the weights’
updates. Calculating the second derivatives is very expensive
computationally, therefore Hessian-vector products were used
to optimize the calculation of dw∇yc ∇wL and dw∇w∇wL,
as proposed in [27].

B. Applying back-gradient optimization to periodic signals

Algorithm 2, which is one of the contributions of this
research, was used to apply Algorithm 1 to autoregression
learning of periodic signals.

Algorithm 2 Find poisoning sequence set Dp given Dtr, Dval,
learning rate α, adversarial learning rate λ, attack input ya,
initial poisoning point y0
c , loss function L, learner’s objective
L, and maximum number of iterations M.

1: Dp ← []
2: decay ← 0.9
3: eps ← 0.00001
4: origλ ← λ
5: for i = 1 to M do
6:

(cid:46) Minimal allowed λ

(wT , alerts) ← train test(Dtr, Dval, ya, Dp, yi
c)
if alerts == 0 then
Dp += (yi
c)
break

(cid:46) Add current poison

c, L, L)

c − λ · dyc/ max(dyc)

dyc ← getP oisonGrad(wT , α, ya, yi
yi+i
c ← yi
(cid:46) Check that the new poison does not generate alerts
(wT , alerts) ← train test(Dtr, Dval, ya, Dp, yi+i
if alerts > 0 then
Dp += (yi
c)
λ ← decay · λ
yi+i
c ← yi
c
if λ <= eps then

(cid:46) Add previous poison
(cid:46) Adjust learning rate
(cid:46) Revert to last good poison

)

c

break

(cid:46) Can’t ﬁnd anymore poisons

7:
8:
9:

10:

11:
12:
13:
14:
15:
16:

17:
18:
19:

else

20:
21:
22: return Dp

λ ← origλ

Algorithm 2 starts with an empty set of poisoning points
(line 1) and an initial poisoning value. Then it repeatedly uses
a train test function to perform the model retraining with the
current training and poisoning datasets (line 6). If the target
attack input and the clean validation data do not raise alerts,
the problem is solved (lines 7-9). Otherwise, the gradient of the
poisoning input is calculated using Algorithm 1, normalized,
and used to ﬁnd the next poison value (lines 10-11). The new
poison value is tested with the current detector (line 13). If
it raises alerts, the value is too large, and the last poison
value that did not raise an alert is added to Dp, the adversarial
learning rate is decreased, and the last good (capable of being
added without raising an alert) poison is used as a base for the
calculation in the next iteration (lines 14-17). If the learning
rate becomes too low, the algorithm terminates prematurely
(lines 18-19). If no alerts were raised, the learning rate is
restored to its original value for the next iteration, in order to
accelerate the poisoning progress (lines 20-21).

For simplicity, we omitted the adversarial learning rate’s (λ)
dynamic decay used in implementation from the description
of Algorithm 2. With the dynamic decay, λ is decreased if the
test error has not decreased and the iterations are terminated
early if λ < 0.00001. Another implementation optimization
not shown in the pseudocode of Algorithm 2 adds clean data
sequences to Dp if the detector raises alerts on validation data.
If this happens, the model is “over-poisoned” and the clean
data is added until the validation alerts disappear. This is done

Fig. 2. Wrapper model.

after the calls to train test.

Once the validity of Algorithm 2 was conﬁrmed, we needed
to adapt it to the way neural networks are applied to continuous
signals in anomaly and attack detectors (e.g., [16], [15], [10],
[7], and many others). The common practice is to apply
the neural networks to the multivariate sequences formed by
sliding a window of a speciﬁed length over the input signal.
These sequences are overlapping, hence a single time point
appears in multiple sequences. As a result, a change to a single
time point by an attacker affects the detector’s predictions for
the multiple sequences that include this point. Moreover, in
order for the changed point to remain undetected, its prediction
should also be close to its (changed) value based on multiple
past input sequences. These self-dependencies spread across
time, both forward and backwards, and must be taken into
account when creating the poisoning input, as this input must
therefore be much longer than the sequence of points changed
during the target attack . However, the model at the attacker’s
disposal deals only with the short sequences. In order to be
able to evaluate the total loss value of the attack for the entire
input, we performed the optimization on a wrapper model
(WM) built around the original trained model. The wrapper
model allows for calculating the gradients and optimizing the
adversarial input for an arbitrary long input sequence, similar
to the process described in [7]. The wrapper model illustrated
in Figure 2 extends the trained model’s graph to calculate the
gradient of the attacker’s objective relative to the entire input.
Speciﬁcally, the wrapper model preﬁxes the original model
with graph operations that divide the long input into over-
lapping subsequences and appends the model with operations
that combine the results of individual predictions and calculate
the combined output. The wrapper model allows us to use
Algorithm 2 and Algorithm 3 (described in Section V-D), as
is, to poison multivariate time series data in a setting in which
ICS anomaly detectors are commonly deployed.

C. Initial poison choice

Proper choice of the initial poisoning point is an important
factor in the performance of Algorithm 2. As the loss surface
of the optimized function is not smooth, some initialization
points will cause the algorithm to get stuck in local mini-
mum solutions. We explored two approaches for the initial
poison choice: a benign-data-based approach and an attack-

based approach. The benign-data-based approach uses the
original unpoisoned data as the initial poisoning value. For
the synthetic signals, it uses the pure generated signal (e.g.,
sine wave). For the real data, it initializes the poison with the
values of the sensors when not under attack.

The attack-based approach uses the sensors’ values that are
closest to the sensors’ values under attack but do not cause an
alert for the initial poison. This approach is more aggressive
and allows the areas with the local minimum of the loss
function, which the good-data-based approach is prone to get
stuck in, to be skipped. The attack-based approach was used in
the real-world data experiment we conducted in this research
(as described below in Section VI-D). Without physical access
to the SWaT testbed, we used the SWaT public dataset to
create the poisoning inputs for the attacks it contains. In this
setting, we had no access to the “untainted” data values that
would appear if the attack had not happened. The attack-based
approach proved to work much better than other techniques
tested, such as ﬁnding the most similar sequence in the training
dataset and using the detector’s trained model to generate
the initial poison (similar to the approach of [6]) In order to
determine the initial poisoning based on the desired attack to
conceal, we utilized gradient ascent, starting from the target
attack sequence and iteratively moving the poisoning point
in the direction of −∇ycL. As above, the optimization was
applied on the wrapper model.

D. Interpolative poisoning algorithm

In addition to the back-gradient optimization algorithm
(Algorithm 1), we propose a much simpler naive interpolation
algorithm to ﬁnd the poisoning sequence.

This algorithm is based on an observation that both the
initial poisoning point and the ﬁnal attack point are known in
advance. Similarly to Algorithm 1, the interpolative Algorithm
3 starts with an empty set of poisoning points, an initial
poisoning point, and an initial interpolation step (lines 1-5).
In each iteration, the algorithm attempts to add a poisoning
point that is an interpolation between the initial point and
the ﬁnal point (lines 7-9). If the new poisoning point does
not raise an alert, it is added to the result set, and the next
interpolation between it and the target attack is tested (lines
the interpolation step is decreased and
12-15). Otherwise,
the interpolation is recalculated (lines 10-11). The algorithm
continues until success is achieved or the interpolation step
becomes too small. As in Section V-B, the algorithm’s im-
plementation included the addition of clean data points to Dp
if the model is over-poisoned and causes alerts on validation
data. This optimization is omitted from the pseudocode for
brevity.

This concludes our description of our algorithms and
methodology, and the next section presents the experimental
results obtained by applying them to both synthetic and real-
world data.

Algorithm 3 Find poisoning sequence set Dp given Dtr, Dval,
decay rate δ, attack input ya, and initial poisoning point y0
c .
1: eps ← 0.0000001
2: Dp ← []
3: rate ← 1
4: step ← 1
5: yp ← y0
c
6: while max(|step|) > eps do
step = rate · (ya − yp)/2
7:
yc = yp + step
8:
(err, alerts) ← train test(Dtr, Dval, yc, Dp) (cid:46) Test
9:

10:
11:
12:
13:
14:

15:
16:

if current poison raises alert

if alerts then

rate ← rate · δ

else

(cid:46) Decrease the rate

yp ← yc (cid:46) Start interpolating from the new point
(cid:46) Add current poison
Dp += yc
rate ← rate/δ
(cid:46) Increase the rate
(err, alerts) ← train test(Dtr, Dval, ya, Dp) (cid:46)

Test if the attack raises alert after poisoning

if alerts == 0 then

17:
18:
19: return Dp

break

(cid:46) The goal is reached

Fig. 3. Autoencoder architecture used in ICS attack detector.

VI. EXPERIMENTS AND RESULTS

In this section, we ﬁrst evaluate the effectiveness of the
interpolation and back-gradient algorithms in executing poi-
soning attacks on synthetic periodic signals. Then we explore
the inﬂuence of different factors on the ability to poison the
given model, and ﬁnally, we present the results of producing
poisoning samples for attacks from the popular SWaT dataset.

A. Detector, training procedure and evaluation criteria

A simple undercomplete autoencoder (UAE) network was
used for the ICS detector under test. We used the network
architecture described in [7] for all tests, with both synthetic
and real data. The simplest instance of such a detector model
is presented in Figure 3.

The single difference of this architecture from the classic
UAE is the presence of inﬂating layers before both the encoder
and decoder parts. As explained in [7], these layers improve
the model performance by increasing its hypothesis space.

The detector was implemented in TensorFlow and trained
using the gradient descent optimizer until the test error (mea-
sured as the mean squared reconstruction error for the input

signal) decreased to less than 0.01. While we experimented
with various numbers of encoder and decoder layers, and
multiple inﬂation factors and input-to-code ratios, these vari-
ables mainly inﬂuenced the detector’s accuracy and not the
poisoning results. The results below are for the tests performed
with an inﬂation factor of two, an input-to-code ratio of two, a
single encoding and decoding layer, and a subsequence length
of two.

Our study focuses on online-trained ICS detectors. In the
online training mode,
is periodically retrained
the model
with part of the previously used training data blended with
the newly arrived data. For simplicity, we performed model
retraining with newly generated poisoning input after each
poisoning iteration, with the new poisoning input appended
to the existing training data. There are other possible ways of
combining the new and existing training data, e.g., randomly
selecting a ﬁxed number of data samples from both. We
experimented with this setup as well and discovered that it
causes a larger number of poisoning points to be added but
does not change the overall ﬁndings.

The following metrics were used for the poisoning effec-

tiveness evaluation:

• the attack magnitude, measured as the maximal dif-
ference from the original and the target spoofed sensor
value;

• the number of poisoning points in the generated se-

quence;

• the number of optimization iterations required to ﬁnd

the poisoning sequence.

We ran grid search tests for both poisoning algorithms,
with and without the initial poison optimization described in
Section V-C for multiple values of:

• the training set size,
• target attack magnitude,
• attack location,
• input signal length,
• subsequence length,
• training and adversarial iterations.

B. Data

Fig. 4. Different kinds of attack locations relative to the signal’s period.

tics correlated to each other due to the laws of physics, such
as water ﬂow and water level.

ICS testbed dataset. For the real-world data experiments,
we utilized the popular SWaT dataset [28]. The dataset was
collected from the Secure Water Treatment (SWaT) testbed at
the Singapore University of Technology and Design and has
been used in many studies since it was created. The testbed is
a scaled-down water treatment plant, running a six-stage water
puriﬁcation process. Each process stage is controlled by a PLC
with sensors and actuators connected to it. The sensors include
ﬂow meters, water level meters, and conductivity analyzers,
while the actuators are water pumps, chemical dosing pumps,
and inﬂow valves. The dataset contains 51 attributes capturing
the states of the sensors and actuators for every second for
seven days of recording under normal conditions and four days
of recording when the system was under attack (the data for
this time period contains 36 attacks). Each attack targets a
concrete physical effect, such as overﬂowing a water tank by
falsely reporting a low water level, thus causing the inﬂow to
continue.

Following our threat model (Section IV), we selected the
attacks that involved sensor value manipulations. Table I lists
and describes the SWaT attacks chosen for the poisoning
experiments.

There were ﬁve other sensor-spooﬁng attacks in the SWaT
dataset, however they involved features that did not preserve
their statistics between the training and the test datasets and
thus could only be modeled very poorly, as shown in [7], and
were therefore not included in our experiment.

Synthetic dataset. For

the synthetic data experiments
we used a number of simple periodic signals (sine, co-
sine, square function (numpy.square), and saw tooth function
(numpy.sawtooth)) of different time periods and lengths. The
signal amplitude was between −1 and 1, and a distorting
Gaussian noise with a mean of 0 and a standard deviation
of 0.05 was applied to the signal. To simulate the attacks, we
increased the signal amplitude by a speciﬁed value. A number
of different attack locations were tested, such as the highest
point of the signal (SIN TOP), the lowest point of the signal
(SIN BOTTOM), and the middle of the slope (SIN SIDE),
as illustrated in Figure 4. The rational behind testing various
attack locations was to model various adversaries’ attack
objectives.

For multiple synthetic signals, we used a number of linear
dependent sines to model a simple case of system characteris-

C. Synthetic signal poisoning

The goals of the experiments on synthetic signals were to
assess the algorithms’ effectiveness and to examine the inﬂu-
ence of different test parameters on the algorithms’ poisoning
ability. The results of our experiments conducted with two
kinds of signals (a sine and a double sine) are presented below.
All of the experiments were conducted with the anomaly
detection threshold of 0.2, using the SIN BOTTOM attack
location (unless stated otherwise), with the model and test
setup described in Section VI-A.

Figure 5 illustrates the results of successful poisoning -
given an input signal of a sine wave distorted by the attack in
the SIN BOTTOM location, the poisoned model produces a
prediction whose deviation from the attack signal is less than
the threshold of 0.2.

TABLE I
SWAT ATTACKS SELECTED FOR POISONING.

Start state

Description

Expected impact

#

3

7

8

Attacked
sensor(s)

LIT-101

LIT-301

Water level be-
tween L and H

Water level be-
tween L and H

DPIT-
301

Value of DPIT
is < 40kpa

10

FIT-401

11

FIT-401

16

LIT-301

31

32

33

36

LIT-401

LIT-301

LIT-101

LIT-101

Value of FIT-
401 above 1

Value of FIT-
401 above 1

Water level be-
tween L and H

Water level be-
tween L and H
Water level be-
tween L and H
Water level be-
tween L and H
Water level be-
tween L and H

by
every

Increase
1mm
second
Water level in-
creased above
HH

Set
value
of DPIT as
> 40kpa

Set value of
FIT-401 as <
0.7
Set value of
FIT-401 as 0
Decrease
water
level
by 1mm each
second
Set LIT-401 to
less than L
Set LIT-301 to
above HH
Set LIT-101 to
above H
Set LIT-101 to
less than LL

Tank underﬂow;
Damage P-101

is

inﬂow;
Stop of
Tank underﬂow;
Damage P-301
Backwash
process
repeatedly
started; Normal
operation stops
No
shutdown;
501 turns off
UV shutdown; P-
501 turns off

UV
P-

Tank overﬂow

Tank overﬂow

Tank underﬂow;
Damage P-302
Tank underﬂow;
Damage P-101

Tank overﬂow

Legend: L - low setpoint value, H - high setpoint value, LL -
dangerously low level, HH - dangerously high level.

Fig. 5. Successful poisoning results. The detector predicts a signal that is
close enough to the attack signal, and the residue is below the threshold of
0.2.

Figure 6 demonstrates the sequence of the poisoning points
leading to successful poisoning produced by the interpolative
algorithm. One can observe that the poisoning points form
a successive interpolation between the original unpoisoned
signal and the target attack signal.

The training and test datasets consisted of period-aligned
sine signals distorted by Gaussian noise (as described in
Section VI-B).

Fig. 6. Poisoning points generated by the interpolative algorithm.

We started by studying poisoning for relatively long (100
timepoints) independent signals that were modeled at once,
without breaking them into short overlapping subsequences
(subsequently called single-sequence data). The experiments
showed that the interpolative algorithm was able to success-
fully poison a trained model for attacks up to the amplitude of
0.4, and the amount of required poisoning samples increased
linearly with the target attack amplitude, as shown in Fig-
ure 7. We found that more poisoning samples than training
samples were required in order to reach the amplitude of 0.4,
which is an extremely high ratio. It was impossible to poison
the model for larger attack amplitudes without triggering an
alert either on the poisoning input or on a clean validation
sample. As Figure 7 shows, the number of training iterations
did not have a strong inﬂuence on the poisoning results.

For this single-sequence modeling, the back-gradient op-
timization algorithm reached the same attack magnitudes,
but in some cases required more poisoning points than the
interpolative one, as demonstrated in Figure 8.

After validating the ability of both algorithms to achieve
the target poisoning in the single-sequence setting, we tested
their performance with detectors, based on modeling multiple
overlapping short subsequences as commonly done in ICS
attack detectors, as described in Section V-B. We refer to
this setup as multi-sequence data in the text below. For multi-
sequence data, we observed that the ability to poison the model
strongly depends on the attack location. As shown in Figure 9,
neither algorithm was able to generate poisoning input for any
signiﬁcant attack in the SIN TOP location. The attacks in the
SIN TOP location aim to spoof the sensor value, increasing
it beyond its highest value and thus trigger the response of
the PLC; such attacks are usually the most valuable to the
attacker. As evident from Figure 9, for the SIN BOTTOM and
SIN SIDE locations, the back-optimization algorithm was able
to reach signiﬁcantly greater attack magnitudes than the inter-
polative one. This difference was consistent across different
training set sizes, as can be see in Figure 10. The reason for
this phenomenon is the ability of back-gradient optimization

Fig. 7. Dependency of the number of required poisoning points on the attack magnitude for single-sequence synthetic data with the interpolative algorithm.
The number of points increases nearly linearly with the attack amplitude and depends strongly on the training set size.

Fig. 8. Comparison of the number of poisoning points and attack iterations for
the interpolative and back-optimization algorithms for single-sequence data.

to ﬁnd local minima of the loss function which are missed by
the relatively large steps taken by the interpolative algorithm.
We also tested the inﬂuence of the gradient ascent poison
initialization described in Section V-C on the performance of
the back-gradient algorithm. Figure 11 presents the impact of
this initialization on the amount of poisoning points required
to reach the maximal attack magnitude and the size of the
maximal attack. It can be seen that the optimized initialization
allowed us to achieve the target poisoning with less poisoning
points, while maintaining the target attack magnitude, and even
increasing it in some cases.

Fig. 9. Comparison of maximal attack magnitude reached by poisoning for
different attack locations. The attack was performed on multi-sequence data.

Finally, we note that the execution time of the interpolative
algorithm’s iteration is signiﬁcantly shorter than the execution
time of the back-gradient algorithm, as shown in Table II.

Fig. 10. Comparison of maximal attack magnitude reached by the interpola-
tive and back-optimization algorithms poisoning for multi-sequence data and
SIN BOTTOM attacks.

Fig. 11. The inﬂuence of initial poison sequence optimization. The attack
was performed in the SIN BOTTOM location on multi-sequence data.

To summarize, the tests conducted on the synthetic data
demonstrated that both of the proposed algorithms can poison
autoencoder models. The interpolative algorithm is much
faster, but the back-gradient optimization algorithm produces
superior results in the learning setup usually deployed for
physics-based anomaly detection in ICSs. At the same time,
both algorithms required a long time and a very large number
of poisoning points to produce the desired poisoning unless the
target attack was of a small magnitude and the training data
set was small. In addition, both algorithms failed to produce
poisoning for even low magnitude attacks for the SIN TOP
attack location.

TABLE II
ITERATION EXECUTION TIME (IN SECONDS).

Model
Single sequence
Multiple sequences
The tests were run on Google’s Colab TPU for
sequences of 100 timepoints.

Back-gradient
33.16
67.37

Interpolative
1.27
1.24

D. Poisoning Attacks on SWaT

After validating the effectiveness of the algorithms, we
applied them to the ten relevant attacks from the SWaT
dataset described in Section VI-B. We started by modeling
the attacked sensor’s signal, along with an additional related
signal. For example, attack #3 (see Table I) spoofs the water
level sensor LIT-101 which is installed in the ﬁrst water
tank. We modeled sensor LIT-101 along with FIT-101, which
measures the water ﬂow into the same tank. The NN model
used had the autoencoder architecture described in Section
VI-A.

As the SWaT database contains almost one million records,
there was a need to choose the amount of training data to use
for model retraining. In online training, the model is retrained
with the most recent data, therefore we used the last part of the
training data and appended the generated poisoning points to
it. Considering the linear dependency of the required poisoning
points on the training set size, we selected a training set length
that was ten times longer that the desired attack sequence,
striking a balance between the test’s representativeness and its
running time. We subsampled the SWaT dataset at a rate of
ﬁve seconds and normalized both the training and test data
into the 0-1 interval using the training data as a normalization
base.

As evident from the synthetic test results, the attack location
heavily inﬂuences poisoning. The analysis of the selected
SWaT attacks revealed that most of the attacks belong to the
SIN TOP class, as in these attacks the spoofed value is set far
beyond the maximum value of the same sensor in the training
dataset; this is illustrated in Figure 12. The value of the LIT301
sensor is set to 1.814 during the attack, while 1.0 is the highest
value seen during training.

Fig. 12. Attack #32. The red arrows indicate the attack duration. During the
attack, the normalized value of the spoofed sensor reached 1.814.

In addition, most of the attacks have a large magnitude
(which we approximated by the deviation from the normal-
ization limits, namely below zero or above one), as shown in

TABLE III
SWAT ATTACKS’ POISONING RESULTS.

#

3

7

8

10-11

16

16

31

31

32

33

33

36

36

Modeled
sensor(s)

LIT-101,
FIT-101
LIT-301,
FIT-201
DPIT-
301,
P-602
FIT-401,
LIT-401
LIT-301,
FIT-201
LIT-301,
FIT-201,
FIT-301,
P-302
LIT-401,
FIT-401
LIT-401,
FIT-401
LIT-301,
FIT-201
LIT-101,
FIT-101
LIT-101,
FIT-101,
MV-101,
P-101
LIT-101,
FIT-101
LIT-101,
FIT-101,
MV-101,
P-101

Full mag-
nitude

Clipped
magni-
tude

Poisoned

Poisoning
points

1.325

1.809

2.19

0.25

0.25

0.25

22.597

0.25

0.893

0.551

0.893

0.25

0.769

0.769

1.814

0.582

0.35

0.25

0.25

0.41

0.582

0.25

1.213

0.41

1.213

0.25

No

No

No

No

Yes

No

Yes

No

No

Yes

No

Yes

No

-

-

-

-

2

-

02

-3

-

1

-

1

-

The maximal attack magnitude achieved by poisoning is presented.
A threshold of 0.2 was used in the experiment. The absolute values
of magnitudes are presented. The clipping magnitudes increased with
steps of 0.05.
1 Achieved by back-gradient optimization.
2 Residue was less than the threshold of 0.2.
3 Performed with the threshold of 0.1.

Table III. As demonstrated in Section VI-C, high magnitude
values make poisoning impossible, therefore we clipped the
attacks to lower values in order to evaluate the feasibility of
poisoning with more moderate attacks.

The test results are presented in Table III. The results
demonstrate both the validity of the proposed algorithms and
the robustness of autoencoders to poisoning. The ability of the
algorithms to ﬁnd poisoning input for real-world data serves
as a proof of the algorithms’ validity. In only four of the
ten attacks the algorithms were able to ﬁnd poisoning input
that allowed the attack to be carried out without triggering an
alert or introducing false positives for valid data. In all of the
successful cases, the back-gradient algorithm achieved greater
attack magnitudes than the interpolative one. At the same time,
it is evident that the successful attacks’ magnitude was low in
absolute value and substantially lower than the original attacks
present in the dataset.

On the other hand, for the majority of the attacks, no

poisoning could be found. Furthermore, all four of the initially
successful poisonings were easily eliminated in successive
tests. For three attacks, namely 16, 33, and 36, adding just two
additional related sensors to the model prevented poisoning
completely. The majority of the ICS detectors proposed in
previous publications [16], [15], [10], [7] modeled multiple
ﬁelds of the monitored system and in some cases combined
all of the ﬁelds. Consequently, we can conclude that adding
more ﬁelds is the recommended way of using autoencoders in
ICS IDSs and that doing so further increases their adversarial
poisoning robustness. For the remaining successfully poisoned
attack (#31), the residue of the model’s prediction for the
attack was lower than a selected threshold of 0.2, therefore
the poisoning process had not started. When we lowered the
threshold to 0.1, neither poisoning algorithm could produce
the desired poisoning.

Overall, the experiments on the attacks from the SWaT
dataset demonstrate the strong resilience of the tested NN
architecture to adversarial poisoning. The experiments indicate
that unlike other problem domains (e.g., image classiﬁcation),
physics-based NN-based anomaly and attack detectors cannot
easily be manipulated. Even a long optimization procedure
cannot cause such detectors to accept a signiﬁcant anomaly as
normal without generating false positives on the normal data.

VII. DISCUSSION AND CONCLUSIONS

ML-based autonomous systems promise enormous beneﬁts
to society. One of the major obstacles to their increased
adoption is the inability to fully trust ML-based decisions,
due, to a large extent, to the vulnerabilities of such systems to
adversarial attacks. In the ICS IDS context, an attacker’s ability
to escape detection by exploiting the NN model’s inherent
weaknesses signiﬁcantly diminishes the value of such NN-
based detectors. While previous research [6] [8] [7] studied
adversarial evasion attacks on NN ICS detectors, to the best
of our knowledge, our study is the ﬁrst to address poisoning
attacks, which are particularly relevant in the online training
setting common in ICS detectors.

Our results answer both of the research questions posed
in Section I. We proposed two algorithms for generating
poisoning sequences for multivariate time series data, given
the target attack and a trained model. Both algorithms were
found to be effective and capable of producing the desired
effect on the model under attack. The interpolative algorithm
is faster, but is in most of the cases less potent than the back-
gradient optimization algorithm, which was able to generate
poisoning data for attacks with greater magnitude. The results
also conﬁrmed the effectiveness of the initial poison choice
algorithm, as it led to shorter poisoning sequences.

After validating the algorithms’ effectiveness at generating
poisoning sequences, our experiments with simple synthetic
data revealed autoencoders’ resilience to such poisoning. Both
algorithms were unable to produce arbitrary attacks; the suc-
cessful poisoning was limited to moderate magnitudes and
certain attack locations. For some attack locations it was not
possible to ﬁnd poisoning for any signiﬁcant attack without

triggering an alert either on the attack or the valid input. We
demonstrated the near linear growth of the required amount
of poisoning points with the increasing size of training input.
This ﬁnding points at an additional impediment the attacker
will need to cope with.

The application of the proposed algorithms to the real data
from the SWaT dataset produced the most encouraging results.
While we were able to conﬁrm the algorithms’ ability to
poison the trained model for the minor attacks, in the vast
majority of cases, the out-of-the-box model was already robust
to poisoning attempts. Furthermore, adding more features
to the model
increased its robustness, delivering valuable
conﬁrmation of autoencoder-based ICS attack detectors’ trust-
worthiness.

In a broader context, our results demonstrate that not all NN-
based systems are equally vulnerable to adversarial attacks.
For data with a strong internal structure, as is the case of
the physics-bound characteristics of a real-world process, NN
models appear to be very challenging to manipulate adversar-
ially. Another possible reason for our models’ robustness is
their small number of parameters. While this small amount
was sufﬁcient for capturing the internal data dependencies, it
was not possible to tweak the parameters without inﬂuencing
the results the model produces on untainted data.

Some limitations and future directions of this study are
worth noting. First, this study only evaluated autoencoder-
based detectors. However, the poisoning algorithms proposed
are agnostic to the detector’s architecture; therefore, studying
the robustness of other NN architectures is a topic for future
research. Another important issue for future studies is increas-
ing the efﬁciency of the poisoning algorithms. In their current
form, they require a very long time to calculate, thus making
them inappropriate for real-time poisoning. Another direction
worth exploring is the application of the proposed algorithms
to other learning optimizers, in addition to the gradient descent
optimizer covered in this study. According to [26], other
popular optimizers, such as RMSProp [29] and Adam [30],
can be traced backwards and thus should be appropriate for
our back-gradient optimization poisoning. In addition, future
research will explore the transferability of poisoning between
models trained with different optimizers. Lastly, in this study
we used a data-only approach for the algorithms’ veriﬁcation.
Validating the ﬁndings in a real-world testbed would be a
natural next stage of this research, possibly after ﬁnding a
way to accelerate poisoning sequence generation.

To conclude,

in this study, we proposed and validated
two algorithms for poisoning online-trained NN regression-
based detectors for multivariate time series data. We found
that autoencoder-based detectors are robust to such poisoning
attacks. Beyond ICS attack detection, these algorithms can
be applied in other areas, such as signal processing, speech
processing, and medical diagnostics. Examining the robustness
of NNs to poisoning in these domains will be of great beneﬁt
to society.

VIII. ACKNOWLEDGMENT

[18] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning.

MIT press Cambridge, 2016, vol. 1.

[19] R. Chalapathy and S. Chawla, “Deep learning for anomaly detection: A

survey,” arXiv preprint arXiv:1901.03407, 2019.

[20] B. Biggio, G. Fumera, and F. Roli, “Security evaluation of pattern
classiﬁers under attack,” IEEE transactions on knowledge and data
engineering, vol. 26, no. 4, pp. 984–996, 2013.

[21] L. Mu˜noz-Gonz´alez, B. Biggio, A. Demontis, A. Paudice, V. Wongras-
samee, E. C. Lupu, and F. Roli, “Towards poisoning of deep learning
algorithms with back-gradient optimization,” in Proceedings of the 10th
ACM Workshop on Artiﬁcial Intelligence and Security. ACM, 2017,
pp. 27–38.

[22] C. Feng, T. Li, Z. Zhu, and D. Chana, “A deep learning-based framework
for conducting stealthy attacks in industrial control systems,” arXiv
preprint arXiv:1709.06397, 2017.

[23] A. Ghafouri, Y. Vorobeychik, and X. Koutsoukos, “Adversarial regres-
sion for detecting attacks in cyber-physical systems,” arXiv preprint
arXiv:1804.11022, 2018.

[24] W. R. Pires, T. H. de Paula Figueiredo, H. C. Wong, and A. A. F.
Loureiro, “Malicious node detection in wireless sensor networks,” in
18th International Parallel and Distributed Processing Symposium,
2004. Proceedings.

IEEE, 2004, p. 24.

[25] E. Shi and A. Perrig, “Designing secure sensor networks,” IEEE Wireless

Communications, vol. 11, no. 6, pp. 38–43, 2004.

[26] D. Maclaurin, D. Duvenaud, and R. Adams, “Gradient-based hyper-
parameter optimization through reversible learning,” in International
Conference on Machine Learning, 2015, pp. 2113–2122.

[27] B. A. Pearlmutter, “Fast exact multiplication by the hessian,” Neural

computation, vol. 6, no. 1, pp. 147–160, 1994.

[28] J. Goh, S. Adepu, K. N. Junejo, and A. Mathur, “A dataset to support re-
search in the design of secure water treatment systems,” in International
Conference on Critical Information Infrastructures Security. Springer,
2016, pp. 88–99.

[29] T. Tieleman and G. Hinton, “Lecture 6.5-rmsprop: Divide the gradient
by a running average of its recent magnitude,” COURSERA: Neural
networks for machine learning, vol. 4, no. 2, pp. 26–31, 2012.

[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

The authors thank iTrust Centre for Research in Cyber
Security, Singapore University of Technology and Design for
creating and providing the SWaT and WADI datasets, and Ishai
Rosenberg for his valuable insights.

REFERENCES

[1] O. Suciu, R. Marginean, Y. Kaya, H. Daume III, and T. Dumitras,
“When does machine learning {FAIL}? generalized transferability for
evasion and poisoning attacks,” in 27th {USENIX} Security Symposium
({USENIX} Security 18), 2018, pp. 1299–1316.

[2] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,
and T. Goldstein, “Poison frogs! targeted clean-label poisoning attacks
on neural networks,” in Advances in Neural Information Processing
Systems, 2018, pp. 6103–6113.

[3] I. Rosenberg, A. Shabtai, L. Rokach, and Y. Elovici, “Generic black-box
end-to-end attack against state of the art api call based malware clas-
siﬁers,” in International Symposium on Research in Attacks, Intrusions,
and Defenses. Springer, 2018, pp. 490–510.

[4] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau,
S. Rao, N. Taft, and J. D. Tygar, “Antidote: understanding and defending
against poisoning of anomaly detectors,” in Proceedings of the 9th ACM
SIGCOMM conference on Internet measurement, 2009, pp. 1–14.
[5] P. Madani and N. Vlajic, “Robustness of deep autoencoder in intrusion
detection under adversarial contamination,” in Proceedings of the 5th
Annual Symposium and Bootcamp on Hot Topics in the Science of
Security. ACM, 2018, p. 1.

[6] A. Erba, R. Taormina, S. Galelli, M. Pogliani, M. Carminati, S. Zanero,
and N. O. Tippenhauer, “Real-time evasion attacks with physical con-
straints on deep learning-based anomaly detectors in industrial control
systems,” arXiv preprint arXiv:1907.07487, 2019.

[7] M. Kravchik and A. Shabtai, “Efﬁcient cyber attacks detection in
industrial control systems using lightweight neural networks,” arXiv
preprint arXiv:1907.01216, 2019.

[8] G. Zizzo, C. Hankin, S. Maffeis, and K. Jones, “Intrusion detection for
industrial control systems: Evaluation analysis and adversarial attacks,”
arXiv preprint arXiv:1911.04278, 2019.

[9] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li,
“Manipulating machine learning: Poisoning attacks and countermeasures
for regression learning,” in 2018 IEEE Symposium on Security and
Privacy (SP).

IEEE, 2018, pp. 19–35.

[10] R. Taormina and S. Galelli, “Deep-learning approach to the detection
and localization of cyber-physical attacks on water distribution systems,”
Journal of Water Resources Planning and Management, vol. 144, no. 10,
p. 04018065, 2018.

[11] R. Mitchell and I.-R. Chen, “A survey of intrusion detection techniques
for cyber-physical systems,” ACM Computing Surveys (CSUR), vol. 46,
no. 4, p. 55, 2014.

[12] A. Humayed, J. Lin, F. Li, and B. Luo, “Cyber-physical systems security
a survey,” IEEE Internet of Things Journal, vol. 4, no. 6, pp. 1802–1831,
2017.

[13] J. Giraldo, E. Sarkar, A. A. Cardenas, M. Maniatakos, and M. Kantar-
cioglu, “Security and privacy in cyber-physical systems: A survey of
surveys,” IEEE Design & Test, vol. 34, no. 4, pp. 7–17, 2017.

[14] J. Giraldo, D. Urbina, A. Cardenas, J. Valente, M. Faisal, J. Ruths, N. O.
Tippenhauer, H. Sandberg, and R. Candell, “A survey of physics-based
attack detection in cyber-physical systems,” ACM Computing Surveys
(CSUR), vol. 51, no. 4, p. 76, 2018.

[15] M. Kravchik and A. Shabtai, “Detecting cyber attacks in industrial
control systems using convolutional neural networks,” in Proceedings of
the 2018 Workshop on Cyber-Physical Systems Security and PrivaCy.
ACM, 2018, pp. 72–83.

[16] Q. Lin, S. Adepu, S. Verwer, and A. Mathur, “Tabor: a graphical model-
based approach for anomaly detection in industrial control systems,”
in Proceedings of
the 2018 on Asia Conference on Computer and
Communications Security. ACM, 2018, pp. 525–536.

[17] R. Taormina, S. Galelli, N. O. Tippenhauer, E. Salomons, A. Ostfeld,
D. G. Eliades, M. Aghashahi, R. Sundararajan, M. Pourahmadi, M. K.
Banks et al., “Battle of the attack detection algorithms: Disclosing cyber
attacks on water distribution networks,” Journal of Water Resources
Planning and Management, vol. 144, no. 8, p. 04018048, 2018.

