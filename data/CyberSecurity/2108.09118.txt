IJCAI-21 1st International Workshop on Adaptive Cyber Defense

1
2
0
2

g
u
A
0
2

]

R
C
.
s
c
[

1
v
8
1
1
9
0
.
8
0
1
2
:
v
i
X
r
a

CybORG: A Gym for the Development of Autonomous Cyber Agents

Maxwell Standen , Martin Lucas , David Bowman , Toby J. Richer , Junae Kim and Damian
Marriott
Defence Science and Technology Group
{max.standen, martin.lucas, david.bowman, toby.richer, junae.kim,
damian.marriott}@dst.defence.gov.au

Abstract

Autonomous Cyber Operations (ACO) involves the
development of blue team (defender) and red team
(attacker) decision-making agents in adversarial
scenarios. To support the application of machine
learning algorithms to solve this problem, and to
encourage researchers in this ﬁeld to attend to prob-
lems in the ACO setting, we introduce CybORG, a
work-in-progress gym for ACO research. CybORG
features a simulation and emulation environment
with a common interface to facilitate the rapid
training of autonomous agents that can then be
tested on real-world systems. Initial testing demon-
strates the feasibility of this approach.

1 Background

Autonomous Cyber Operations (ACO) is concerned with
the defence of computer systems and networks through au-
tonomous decision-making and action.
It is particularly
needed where deploying security experts to cover every net-
work and location is becoming increasingly untenable, and
where systems cannot be reliably accessed by human defend-
ers, either due to unreliable communication channels or ad-
versary action.

The ACO domain is challenging to develop artiﬁcial intelli-
gence (AI) approaches for as it combines hard problems from
other domains of AI research. Like game AI, it is adversar-
ial: the effectiveness of a defensive cyber agent is determined
by its ability to respond to an adversary. Like autonomous
robotics, ACO is affected by the ‘reality gap’ [Ibarz et al.,
2021], as simulations of an environment will abstract away
information that could be critical to an agent’s effectiveness.
A further issue for the ACO domain is that the environment
and action set change as cyber security research progresses,
which is far more rapidly than either of the domains discussed
above.

The requirement to handle the varying actions of an adver-
sary, in a complex environment, precludes the use of static
data sets to learn ACO behaviour. A tool for learning in ad-
versarial environments is an AI Gym. AI Gyms such as the
one developed by OpenAI implement reinforcement learning

(RL) through direct interaction with a simulation of the prob-
lem. A path to addressing the ‘reality gap’, used in [Tan et
al., 2016], is to combine learning on simulations with testing
in a real environment.
In this case, the bulk of learning is
conducted on simulated systems. Successful agents are trans-
ferred to the real system to ﬁrstly validate their effectiveness,
and secondly to reﬁne the simulation.

We believe that AI Gyms, that can be validated and re-
ﬁned through experiments in real-world environments, are the
most promising tool for training effective intelligent cyber
agents. This paper presents the Cyber Operations Research
Gym (CybORG), an environment designed to facilitate the
development of ACO agents through this two-stage process.

2 Related Work
There are a growing number of cyber security environments
designed for experimentation. A summary of several environ-
ments, with an assessment of how they ﬁt our requirements,
can be found in Table 1.

DETERlab [Mirkovic et al., 2010] is a specialised cy-
ber security experimentation environment based on EMU-
lab [Stoller et al., 2008]. It supports cyber security experi-
mentation through the emulation of hosts and networks. As
it relies on local hardware, DETERlab has limited maximum
network size and takes a signiﬁcant amount of time to reset or
reconﬁgure. VINE [Eskridge et al., 2015], SmallWorld [Fur-
faro et al., 2018] and BRAWL [Corporation, 2018] lever-
age cloud-based Infrastructure as a Service (IaaS) and vir-
tualisation frameworks such as OpenStack to emulate larger
enterprise-like networks. These tools can simulate users act-
ing on a host which are designed to generate human-like ac-
tivity for the purpose of experimentation with different tools.
BRAWL can make use of CALDERA [Applebaum et al.,
2016; Miller et al., 2018] as a red team agent to produce a
more realistic experimentation testbed.

GALAXY [Schoonover et al., 2018] is an emulated cyber
security environment. It has been used for research into evo-
lutionary algorithms, training red agents to blend in with nor-
mal user trafﬁc. It is currently restricted to a single VM per
physical machine, but can use VM snapshotting to perform
fast resets of the environment.

Insight [Futoransky et al., 2010] is a highly scalable system
for simulating networks for the training of offensive agents.

 
 
 
 
 
 
Simulation Emulation
DETERlab [Mirkovic et al., 2010]
No
VINE [Eskridge et al., 2015]
No
SmallWorld [Furfaro et al., 2018]
No
BRAWL [Corporation, 2018]
No
Galaxy [Schoonover et al., 2018]
No
Insight [Futoransky et al., 2010]
Yes
CANDLES [Rush et al., 2015]
Yes
Pentesting Simulations [Niculae, 2018; Schwartz and Kurniawati, 2019] Yes
CyAMS [Brown et al., 2016]
Yes
CyberBattleSim [Team, 2021]
Yes
FARLAND [Molina-Markham et al., 2021]
Yes
Yes
CybORG

Yes
Yes
Yes
Yes
Yes
No
No
No
Yes
No
Yes
Yes

Scalable
Low
Med
Med
Med
Low
High
High
High
High
High
High
High

Flexible
Efﬁcient Adversarial CO Designed for RL
Yes
Low
Yes
Med
Yes
Med
Windows
Med
Debian-based
High
Yes
Med
Yes
High
Yes
Med
Yes
High
High
Yes
Networks only High
High
Yes

No
No
No
Limited
Yes
No
Yes
Yes
No
Yes
Yes
Yes

No
No
No
No
Limited
No
Limited
Limited
No
Limited
Yes
Yes

Table 1: Existing Environments and CybORG Design vs ACO Research Requirements

It can simulate hundreds of hosts on a single computer, but
does not support the training of defensive agents.

CANDLES [Rush et al., 2015] leverages a high speed net-
work security simulation to coevolve adversarial blue and red
agents using an evolutionary algorithm. CyAMS [Brown et
al., 2016] uses a combined emulation and simulation envi-
ronment. CyAMS simulates a cyber security environment us-
ing a Finite State Machine (FSM). CyAMS also features an
emulation environment and compares the simulation with the
emulation using a malware propagation scenario to demon-
strate the ﬁdelity of the simulation.

CyberBattleSim [Team, 2021] focuses on a red agent per-
forming post breach lateral movement in a windows enter-
prise environment. The simulation is node-based which al-
lows for a highly ﬂexible state and action space at the cost
of a high level observation space which lacks information on
second order effects. The simulation allows the use of a ﬁxed
defender agent but does not support the training of the de-
fender agent. CyberBattleSim is a pure simulation and is not
directly connected to an emulation.

FARLAND [Molina-Markham et al., 2021] is another
framework that is designed for the training of agents in simu-
lation and testing of agents in emulation. It provides some
useful extensions to CybORG’s current functionality, such
as probabilistic representations of state and support for ad-
versarial and deceptive red agents. However, FARLAND is
designed for network trafﬁc exploitation and defence, rather
than host-based exploitation and defence. It could serve as a
useful complement to CybORG for scenarios with more of a
focus on the properties of the network.

Each of these training environments fulﬁll some of our re-
quirements. We want to train red and blue agents that can
interact realistically with a variety of hosts. We require the
ability to rapidly train these agents in simulated environments
and to then test and validate them in emulated environments.
To our knowledge, all of these features are not available in
any single environment described above. In the next section,
we present the design of our system which targets these re-
quirements.

3 Overview of CybORG

CybORG is designed to implement a range of scenarios at
differing levels of ﬁdelity through a highly modular design.
Once a scenario is fully deﬁned within CybORG, an agent can
interact with that scenario modelled in a ﬁnite state machine

(deﬁned here as simulation) or fully implemented in virtual
infrastructure (deﬁned here as emulation).

The CybORG tool generates a scenario based on a pre-
generated description, initialises a set of agents to perform
roles within that scenario, then implements their actions and
assesses their effectiveness. CybORG then runs the scenario
in a series of discrete steps. At each step, each agent selects
an action to perform from its action space. An agent’s ac-
tion space is a subset of the overall available set of actions,
dependent on the role of the agent and deﬁned as part of the
scenario. Once each agent has selected an action, it is per-
formed. The agent receives an observation of the updated
state of the scenario. The scenario is run until it reaches its
termination condition – either a limit on the number of steps,
or the achievement of an agent’s goal. At this point, the agents
receive any ﬁnal information on the state of the scenario. The
scenario can be reset for further training or testing.

To implement CybORG’s two levels of ﬁdelity, the scenar-
ios and actions used by CybORG are deﬁned at two levels.
The scenario deﬁnition contains the required information to
simulate the scenario. It also contains the system images re-
quired to emulate the scenario. Each action used by CybORG
is deﬁned twice. For simulation, each action is deﬁned as a
state transition. For emulation, each action deﬁnes a com-
mand (with appropriate parameters) that can be executed to
achieve the desired effect.

4 Design

4.1 Scenarios

A CybORG scenario deﬁnes the ‘game’ that agents are aim-
ing to solve or compete in. This scenario deﬁnes what agents
exist, what actions they may perform, what information they
begin the game with, and how their reward is calculated. It
deﬁnes the conﬁguration of each host and the network con-
nections between them.

scenario involves

Deploying a particular

specifying
whether it will be simulated or emulated and providing a sce-
nario description ﬁle in the data-serialization format YAML.
The scenario ﬁle includes details for conﬁguring the envi-
ronment including hosts, networking and subnet information,
and the set of actions available to red and blue agents. The
scenario ﬁles are deliberately simple, requiring a minimum
of information and employing many default behaviours to re-
duce the burden on users when creating ﬁles.

The supported host conﬁgurations and actions are speci-
ﬁed in separate ‘Images’ and ‘Actions’ YAML ﬁles respec-
tively. The host ﬁle contains information on operating sys-
tem, services, processes, users and other system information.
It also contains an identiﬁer for a deployable image to be used
in emulation. The process information includes the process
id, the parent id, process network connections, the process
owner and the process name. The user information includes
usernames, UIDs, groups, GIDs, passwords, and password
hashes. The system information includes the operating sys-
tem type, distribution, version, and patches, and the hardware
architecture.

4.2 Actions
CybORG interacts with agents via the OpenAI interface
[Brockman et al., 2016]. This interface presents an obser-
vation and action space to the agent, and the agent responds
by selecting an action (with appropriate parameters) to per-
form. These actions are deﬁned as part of the scenario, and
are based on those that would be available to a cybersecu-
rity professional in similar circumstances. In the simulator,
the effect of these actions is modelled. In the emulator, these
actions are fully implemented where possible.

4.3 Observations
Once an agent selects an action it is performed and the agent
is returned an observation object describing the new state of
the system as observed by that agent. Each agent is only pro-
vided data for those parts of the scenario it could conceivably
observe, depending on the reconnaissance tools available to
that agent.

The observation from CybORG is a dictionary with key-
value pairs. One key in each observation, ‘success’, indicates
if the previous action performed by that agent was successful,
unsuccessful or ‘unknown’ (i.e. no information was received
regarding the success of the action). The other parts of the ob-
servation are split by host id, with each value corresponding
with a host id containing a dictionary that contains further ob-
servations from that host. These dictionaries divide the state
further into the categories Interface, Session, User, System
and Process.

In addition to the observation, the agent is presented with
a reward value and a ﬂag indicating if the current run is com-
plete. Once the run is complete, the reset() function returns
the scenario’s state to its initial state with re-randomised val-
ues where appropriate.

4.4 Simulator
The simulator represents the scenario as a ﬁnite state ma-
chine, where the current state represents the state of all sys-
tems and networks in the scenario. Actions use the values in
the current state to determine the next state, update the values
inside of the current state such that they match the next state,
and then return the observable subset of this updated state to
the agent.

Simulated actions are deﬁned by their preconditions and
effects. The preconditions of an action are the state condi-
tions that must be satisﬁed for the action to be successful.

The effects deﬁne how the action will change the state of the
environment if the action is successful.

To reduce the chance of the simulator model diverging
from the behaviour of the emulator – in particular, allowing
actions to succeed where they would not in real systems –
the state includes details such as the creation or deletion of
individual ﬁles or the making and breaking of network con-
nections.

4.5 Emulator
The emulator currently uses Amazon Web Services (AWS)
with virtual machines to create a high ﬁdelity cyber security
environment with which an agent can interact.

The emulator uses the description of the scenario from the
YAML scenario ﬁles to deploy and conﬁgure a virtual net-
work in Amazon Web Services (AWS) (as well as for tearing
them down). It does this by using SSH to access a virtual
gateway server in a private AWS cloud and then deploying
and conﬁguring environments using AWS’s Command Line
Interface (CLI) on that virtual (master) host using the follow-
ing functions:

• Automatically creating (and deleting) IPv4 subnets
based on the number of IP addresses required in each
subnet according to the scenario.

• Automatically conﬁguring routing between subnets ac-

cording to the scenario.

• Automatically creating (and deleting) instances of static
host images and assigning them to subnets according to
the scenario.

Using these high-level functions, CybORG is capable of
rapidly and concurrently deploying independent clusters of
hosts and subnets, allowing multiple instantiations of a sce-
nario to be run in parallel. By using IaaS it also has the ad-
vantages those offerings provide in that it is scalable, efﬁcient
and low cost.

The CybORG Emulator can run in two different modes:
pre-deployed or deployed. In the ﬁrst the VMs are already
running and can be interacted with as is and in the second
saved VM snapshots are started before CybORG can interact
with the environment.

To implement actions, the CybORG emulator uses a se-
ries of actuator objects. These connect to VMs using SSH
or specialised session handlers for third party tools. Third-
party tools currently used in CybORG include the Metasploit
Framework [Maynor, 2011] and Velociraptor [Lohanathan
and Marti, 2020]. Other session handlers can be added as
required. These actuator objects interact with security tools
and systems either through APIs or terminal commands. The
results of these actions are then ﬁltered and merged to present
a single observation back to an agent. This control method al-
lows for multiple adversarial agents to act simultaneously in
the environment, and will potentially allow human operators
to interact within the emulator in parallel with agents.

4.6 Summary
Through the approach described above, CybORG is able to
implement a novel capability for cyber autonomy – the abil-
ity to train and test the same agent, using the same body of

Figure 1: Diagram of CybORG Scenario.

code, at differing levels of ﬁdelity. Agents can be trained,
using standard learning approaches, in the simulator. The ef-
fectiveness of these agents can then be validated on virtual
In the next section, we describe a scenario
infrastructure.
implemented within CybORG for the development of an au-
tonomous penetration testing agent using RL.

5 Experimentation
5.1 Scenario
Our initial test scenario is shown in Figure 1. While only
having three hosts for an attacker to access, it is of sufﬁ-
cient complexity and detail to capture examples of most at-
tacker behaviours within the cyber killchain, as ﬁrst deﬁned
in [Hutchins et al., 2011].

The scenario consists of 3 hosts split into 2 subnets. The
attacker host runs a Metasploit server that allows the agent to
perform all parts of the killchain. The attacker is in its own
subnet, which does not block any trafﬁc and allows packets
to be sent to hosts in the other subnet.

The other two hosts are the Internal and Gateway hosts that
sit in the Internal subnet. The Gateway host is an Ubuntu 18
host that has an open SSH port. The Internal host is a Win-
dows 2008 server with open SSH and SMB ports. The SSH
services have sufﬁciently simple sets of credentials for the
server to be vulnerable to a brute force attack – the Gateway
host has the username “pi” and password “raspberry”, and the
Internal host has the username “vagrant” and password “va-
grant”. The SMB service is vulnerable to the Eternal Blue
(MS17-010) exploit.

There is a single agent in the scenario that is a red agent.
The goal of the red agent is to get a session on the Internal
host as the System user, and thereby have full access to the
internal host’s ﬁle system. In order to achieve this goal, the
agent must be able to perform reconnaissance on hosts, ex-
ploit these hosts, establish meterpreter sessions and use a me-
terpreter session to pivot between machines. An effective red
agent will be able to select appropriate actions, with correct
parameters, to complete each of these sub-goals in the proper
order.

To model these actions, and then implement these actions
in the Emulator, we use the Metasploit Framework and Me-

terpreter as the actuators and sensors for the agent. The ac-
tions available to the red agent are: SSH Bruteforce, Portscan,
Pingsweep, Upgrade to Meterpreter, IPConﬁg, MS17-010-
PSExec, Autoroute, and Sleep. Each of these actions has
associated parameters, for which the agent must learn the cor-
rect values.

The red agent receives a large reward for starting a ses-
sion on the Internal host as the System user. The agent also
receives moderate rewards for gaining user sessions on the In-
ternal and Gateway hosts, and minor rewards for discovering
new information about the network. For this simple scenario
the reward did not take into account any action costs.

Agent Training Methodology
In order to demonstrate the transferability of training from the
simulator to the emulator, an RL agent is initially trained on
the simulator then run on the emulator.

This RL agent uses a Deep Q-Network [Mnih et al., 2015]
to learn a policy which maps the current state of the environ-
ment to the discounted rewards for each action. Learning this
policy, over a large series of training runs, allows the agent
to select an action at each step that will produce the highest
reward.

For this scenario, the agent will be unable to learn an ef-
fective policy without remembering key features of the sce-
nario. This memory is implemented through a Long Short
Term Memory, as described in [Hausknecht and Stone, 2015].
For approaches such as Deep RL the observation and ac-
tion space provided by CybORG require modiﬁcation. We
construct a suitable observation space for the agent using a
wrapper around CybORG. This wrapper turns the elements
of the observation into a single vector of ﬂoating point num-
bers with a ﬁxed size. This is the input required for Deep
Q-learning as used here. We also use a wrapper that takes a
vector of integers from the RL agent and converts them into
an action (and parameters, if required) that can be performed
in CybORG.

To select the action and parameters, our agent uses a Neu-
ral Network that has a stem that takes in an observation, con-
structs a feature vector from that observation, interprets it
through an LSTM module, then splits into branches which
each output a Q-value for either an action or a parameter. We

provide the already determined parameters which the agent
uses to mask its own parameter selection. This masking pre-
vents the agent from randomly or intentionally selecting a pa-
rameter that it has not yet encountered. This improves the
realism of the scenario and the convergence rate of the agent.

5.2 Results

We trained each RL agent for up to 2500 iterations in the
simulator. This was selected as it was well above the average
number of iterations for a successful run in initial testing, but
short enough for multiple training runs to be conducted on
our hardware in a reasonable amount of time. Each iteration
took a maximum of 20 steps. An iteration was stopped if an
agent could get access to the Windows host using the System
account. If an agent was able to achieve this within 10 steps,
then that agent was deemed to be successful and the training
run was ﬁnished; otherwise the agent was deemed not suc-
cessful. A minimum of 7 steps was required for an agent to
get System access on the Windows host. With these parame-
ters, we were able to generate an effective red agent on every
training run.

To test the ability of these agents to transfer to the emulator,
we took a selection of these trained agents and reran them on
CybORG in emulation mode. In this case, they ran exploits
using the Metasploit framework on virtual hosts within our
testing network.

The initial training produced 21 independent RL agents.
Each RL agent was evaluated in the emulator 10 times. The
total number of successful tests was 139, giving a 66% rate
of success.

Figure 2 shows the distribution of success rates across in-
dependently trained RL agents. Almost half of the trained
agents were successful on every emulator run. Four trained
agents were never successful on emulator runs, with the rest
having a varying numbers of successes. The unsuccessful em-
ulator runs on one hand might be explained by occasional fail-
ures to be expected in pentesting activities. The agents that
consistently failed suggested that the simulation was ﬂawed
and needed reﬁnement. As one example, one agent was un-
successful in an emulator run because it did not use the au-
toroute action to route an attack around the external ﬁrewall.
The likely reason for this is that the RL agent learned a policy
function that overﬁt to an artefact in the observation received
from the simulation that was not in the observation received
from the emulation.

These results demonstrate that the underlying concept of
CybORG is feasible; that a simulation can be used to train
an agent which can then run effectively on virtualised in-
frastructure using professional security tools. While there
were a signiﬁcant number of failures to transfer trained agents
from simulator to emulator for this scenario, these failures
were sometimes associated with deﬁciencies in the simulator
model or in the emulator’s interface between the agent and
the emulation environment. These deﬁciencies could be de-
tected and used to reﬁne CybORG for future training, thus
demonstrating the feasibility of the approach of combining
simlator and emulator in order to reﬁne the simulator model
or emulator interface.

Figure 2: Distribution of Successful Runs for Trained Agents.

6 Conclusion
This paper introduced CybORG, a work-in-progress gym for
Autonomous Cyber Operations research.

Whilst there have been ongoing developments towards
simulation and emulation environments for cyber training
and/or experiments, the requirements of ACO motivate an in-
tegrated design comprising emulation and simulation modes
to support large scale RL across diverse scenarios.

We have made progress towards implementing this design,
with the ability to spawn and play games either in simula-
tion mode or emulation mode with cloud infrastructure. In
CybORG, we can now train an RL agent in simulation then
test its effectiveness in emulation. The results of this testing
can alternately validate the effectiveness of the agent and act
as a guide for further reﬁnement of the simulator model or
emulator interface.

CybORG or gyms like it could assist the RL research com-
munity to apply their techniques to solve cyber operations
problems by providing an easy to use toolkit, a set of bench-
mark problems for guiding and measuring progress, and a
pathway toward validation on virtual infrastructure.

7 Future Work
This paper shows results of red agent training. The ﬁrst focus
of future work is to implement the required actions, sensors
and actuators to conduct the same training for a blue agent.
The second focus of future work is to develop a more complex
scenario with the potential for blue to employ deception.

Acknowledgments
We would like to acknowledge Callum Baillie, Jonathon
Schwartz and Michael Docking for their previous contribu-
tions to CybORG.

Availability
Limited access to CybORG will be available for reviewers,
with a public release planned once the scenario library is ex-
panded.

References
[Applebaum et al., 2016] Andy Applebaum, Doug Miller,
Blake Strom, Chris Korban, and Ross Wolf.
Intelligent,
automated red team emulation. In Proceedings of the 32Nd
Annual Conference on Computer Security Applications,
ACSAC ’16, pages 363–373, New York, NY, USA, 2016.
ACM.

[Brockman et al., 2016] Greg Brockman, Vicki Cheung,
Ludwig Pettersson, Jonas Schneider, John Schulman, Jie
Tang, and Wojciech Zaremba. Openai gym. arXiv preprint
arXiv:1606.01540, 2016.

[Brown et al., 2016] Scott Brown, Harold Brown, Michael
Russell, Brian Henz, Michael Edwards, Frank Turner, and
Giorgio Bertoli. Validation of network simulation model
and scalability tests using example malware. In MILCOM
2016 - 2016 IEEE Military Communications Conference,
pages 491–496, Nov 2016.

[Corporation, 2018] The MITRE Corporation. BRAWL.
https://github.com/mitre/brawl-public-game-001, 2018.
[Eskridge et al., 2015] Thomas C Eskridge, Marco M Car-
valho, Evan Stoner, Troy Toggweiler, and Adrian Grana-
dos. VINE: A cyber emulation environment for mtd exper-
imentation. In Proceedings of the Second ACM Workshop
on Moving Target Defense, MTD ’15, pages 43–47, New
York, NY, USA, 2015. ACM.

[Furfaro et al., 2018] Angelo Furfaro, Antonio Piccolo, An-
drea Parise, Luciano Argento, and Domenico Sacca. A
cloud-based platform for the emulation of complex cyber-
security scenarios. Future Generation Computer Systems,
89:791–803, 2018.

[Futoransky et al., 2010] Ariel Futoransky, Fernando Mi-
randa, Jos´e Orlicki, and Carlos Sarraute. Simulating cyber-
attacks for fun and proﬁt. arXiv preprint arXiv:1006.1919,
2010.

[Hausknecht and Stone, 2015] Matthew J. Hausknecht and
Peter Stone. Deep recurrent q-learning for partially ob-
servable mdps. CoRR, abs/1507.06527, 2015.

[Hutchins et al., 2011] Eric M Hutchins, Michael J Clop-
pert, Rohan M Amin, et al. Intelligence-driven computer
network defense informed by analysis of adversary cam-
paigns and intrusion kill chains. Leading Issues in Infor-
mation Warfare & Security Research, 1(1):80, 2011.

[Ibarz et al., 2021] Julian Ibarz, Jie Tan, Chelsea Finn, Mri-
nal Kalakrishnan, Peter Pastor, and Sergey Levine. How to
train your robot with deep reinforcement learning: lessons
we have learned. The International Journal of Robotics
Research, 40(4-5):698–721, Jan 2021.

[Lohanathan and Marti, 2020] Sinthujan Lohanathan and
Severin Marti. Live Response Training Range mit Veloci-
raptor. PhD thesis, OST Ostschweizer Fachhochschule,
2020.

[Maynor, 2011] David Maynor. Metasploit toolkit for pene-
tration testing, exploit development, and vulnerability re-
search. Elsevier, 2011.

Henry

[Miller et al., 2018] Doug Miller, Ron Alford, Andy
and
Foster,
emula-
Strom.
Automated
A case for planning and acting with un-
https://www.mitre.org/publications/technical-

Applebaum,
Blake
tion:
knowns.
papers/automated-adversary-emulation-a-case-for-
planning-and-acting-with, 2018.

adversary

Little,

Caleb

[Mirkovic et al., 2010] Jelena Mirkovic, Terry V Benzel,
Ted Faber, Robert Braden, John T Wroclawski, and
Stephen Schwab. The DETER project: Advancing the
science of cyber security experimentation and test.
In
2010 IEEE International Conference on Technologies for
Homeland Security (HST), pages 1–7, Nov 2010.

[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,
David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. nature, 518(7540):529–533,
2015.

[Molina-Markham et al., 2021] Andres Molina-Markham,
Cory Miniter, Becky Powell, and Ahmad Ridley. Network
environment design for autonomous cyberdefense. CoRR,
abs/2103.07583, 2021.

[Niculae, 2018] S. Niculae. Reinforcement learning vs ge-
netic algorithms in game-theoretic cyber-security, Oct
2018.

[Rush et al., 2015] George Rush, Daniel R Tauritz, and
Alexander D Kent. Coevolutionary agent-based network
In Pro-
defense lightweight event system (CANDLES).
ceedings of the Companion Publication of the 2015 An-
nual Conference on Genetic and Evolutionary Computa-
tion, GECCO Companion ’15, pages 859–866, New York,
NY, USA, 2015. ACM.

[Schoonover et al., 2018] Kevin Schoonover, Eric Michalak,
Sean Harris, Adam Gausmann, Hannah Reinbolt, Daniel R
Tauritz, Chris Rawlings, and Aaron Scott Pope. Galaxy:
In
A network emulation framework for cybersecurity.
11th USENIX Workshop on Cyber Security Experimenta-
tion and Test (CSET 18), Baltimore, MD, 2018. USENIX
Association.

[Schwartz and Kurniawati, 2019] J. Schwartz and H. Kurni-
awati. Autonomous Penetration Testing using Reinforce-
ment Learning. arXiv:1905.05965, 2019.

[Stoller et al., 2008] M Hibler R Ricci L Stoller, Jonathon
Duerig, Shashi Guruprasad, Tim Stack, Kirk Webb, and
Jay Lepreau. Large-scale virtualization in the Emulab net-
work testbed. In USENIX 2008 Annual Technical Confer-
ence, ATC’08, pages 113–128, Berkeley, CA, USA, 2008.
USENIX Association.

[Tan et al., 2016] Jie Tan, Zhaoming Xie, Byron Boots, and
C. Karen Liu. Simulation-based design of dynamic con-
In 2016 IEEE/RSJ In-
trollers for humanoid balancing.
ternational Conference on Intelligent Robots and Systems
(IROS), pages 2729–2736, 2016.

[Team, 2021] Microsoft Defender Research Team.

Cy-
berbattlesim. https://github.com/microsoft/cyberbattlesim,

2021.
Created by Christian Seifert, Michael Betser,
William Blum, James Bono, Kate Farris, Emily Goren,
Justin Grana, Kristian Holsheimer, Brandon Marken,
Joshua Neil, Nicole Nichols, Jugal Parikh, Haoran Wei.

