Machine Learning With Feature Selection Using Principal
Component Analysis for Malware Detection: A Case Study

Jason Zhang, Ph.D.
Senior Threat Researcher
Sophos, Abingdon OX14 3YP, U.K.
jason.zhang@sophos.com

9
1
0
2

b
e
F
0
1

]

R
C
.
s
c
[

1
v
9
3
6
3
0
.
2
0
9
1
:
v
i
X
r
a

Abstract—Cyber security threats have been growing signiﬁ-
cantly in both volume and sophistication over the past decade.
This poses great challenges to malware detection without con-
siderable automation. In this paper, we have proposed a novel
approach by extending our recently suggested artiﬁcial neural
network (ANN) based model with feature selection using the
principal component analysis (PCA)
technique for malware
detection. The effectiveness of the approach has been successfully
demonstrated with the application in PDF malware detection.
A varying number of principal components is examined in
the comparative study. Our evaluation shows that the model
with PCA can signiﬁcantly reduce feature redundancy and
learning time with minimum impact on data information loss, as
conﬁrmed by both training and testing results based on around
105, 000 real-world PDF documents. Of the evaluated models
using PCA, the model with 32 principal feature components
exhibits very similar training accuracy to the model using the
48 original features, resulting in around 33% dimensionality
reduction and 22% less learning time. The testing results further
conﬁrm the effectiveness and show that the model is able to
achieve 93.17% true positive rate (TPR) while maintaining the
same low false positive rate (FPR) of 0.08% as the case when no
feature selection is applied, which signiﬁcantly outperforms all
evaluated seven well known commercial antivirus (AV) scanners
of which the best scanner only has a TPR of 84.53%.

Index Terms—machine learning (ML), artiﬁcial neural net-
work (ANN), multilayer perceptron (MLP), principal component
analysis (PCA), cyber security, PDF malware, malicious docu-
ments, antivirus (AV)

I. INTRODUCTION

During the past decade, machine learning (ML), deep
learning (DL) or generally termed artiﬁcial intelligence (AI)
have gained wide popularity with applications across industries
thanks to exponential improvement in computing hardware,
availability of big datasets and improved algorithms. Recent
work has witnessed breakthroughs from image classiﬁcation,
speech recognition to robot control and autonomous driving.
In the meantime, cyber security attacks have been growing
signiﬁcantly in both volume and sophistication over the past
decade as well. Typical examples include spamming cam-
paigns, phishing emails and application vulnerability exploits,
either via targeted or non-targeted attacks. It’s no surprise that
AI based methods have been increasingly studied or applied
in cyber security applications, varying from spam ﬁltering
[1], [2], intrusion detection [3], malware detection [4] and
classiﬁcation [5].

A crucial step in an ML workﬂow is feature extraction
which can be hand-crafted based on human expertise, or au-
tomatically learned by training modern deep learning models
such as convolutional neural networks (CNNs). It is natural to
believe that more extracted features are able to provide better
characterization of a learning task and more discriminating
power. However, increasing the dimension of the feature vector
could result in feature redundancy and noise. Redundant and
irrelevant features can cause performance deterioration of
an ML model with overﬁtting and generalization problems.
Additionally, excessively increased number of features could
signiﬁcantly slow down a learning process. Therefore it is
of fundamental
importance to only keep relevant features
before feeding them into an ML model, which leads to the
requirement of feature selection (or feature dimensionality
reduction). Feature selection can be seen as the process of
identifying and removing as much of the noisy and redundant
information as possible from extracted features.

To demonstrate the effectiveness of our proposed ML ap-
proach with feature selection using PCA in malware detection,
the suggested approach is evaluated with PDF malware detec-
tion as a case study.

Portable document format (PDF) is widely used for elec-
tronic documents exchange due to its ﬂexibility and inde-
pendence of platforms. PDF supports various types of data
including texts, images, JavaScript, Flash, interactive forms
and hyper links, etc. The popularity and ﬂexibility of PDF also
provide opportunities for hackers to carry out cyber attacks in
various ways. A common PDF based attack is phish, such
as PDF based order conﬁrmation and parcel delivery notice,
which typically uses social engineered texts to entice users
to click phishing links embedded in PDF documents. Another
typical attack using PDF is exploits which target vulnerable
PDF reading applications. For example, an attack detected
by Sophos targets four vulnerabilities of a popular PDF
reading application [6]: Collab.collectEmailInfo (CVE-2007-
5659), Util.printf () (CVE-2008-2992), Collab.getIcon (CVE-
2009-0927) and Escript.api plugin media player (CVE-2010-
4091). Each of the exploits affects a particular version of the
vulnerable application depending on the version installed on
a victim’s device.

There exist various approaches for the detection of PDF
based attacks. Typical traditional methods include signature
match and sandbox based analysis [7–9]. Given the popularity

Sophos Technical Papers, Feb, 2019

 
 
 
 
 
 
of ML in the past decade, it has also witnessed several ML
applications in PDF malware detection, such as methods for
detecting JavaScript in PDF ﬁles [10], [11]. Other learning-
based approaches for general PDF malware detection include
a decision tree based ensemble learning algorithm [12], as
well as methods using Naive Bayes, support vector machines
(SVM) and Random Forest [13–16]. The pros and cons of
the traditional methods and ML based approaches above are
discussed in [17].

In this paper, we have proposed a novel approach by
extending our recently proposed MLP neural network model
MLPdf [17] with feature selection using PCA for malware
detection. The performance is evaluated with PDF malware
detection before and after applying feature selection. Feature
selection with dimensionality reduction from 33% up to 79%
is studied. Our evaluation based on real-world data shows that
the model with PCA based feature selection can signiﬁcantly
reduce feature redundancy and learning time with minimum
impact on data information loss, as conﬁrmed by both training
and testing results.

The remainder of this paper is organized as follows: In the
following section, the MLPdf model from [17] is introduced.
Section III discusses feature extraction, followed by feature
selection using PCA. Then, Section IV contains evaluation
results illustrating the performance of the proposed approach
with comparison to other methods. Finally, Section V contains
our conclusions and suggested future work.

II. THE MLPdf MODEL
The MLPdf neural network model we recently proposed in
[17] has an input layer, an output layer, and two hidden layers
between them. It is a densely connected network, which means
each node in one layer fully connects to every node in the
following layer. In order to carry out the supervised learning
process, it is necessary to extract a digital representation x
of a given object or event that needs to be fed into the MLP
model. The learning task becomes to ﬁnd a multidimensional
function Ψ(·) which maps the input x to the target y, as shown
below

y ∼= Ψ(x)
(1)
where x ∈ RN ×1, a real-valued input feature vector x =
[x1, · · · , xN ]T in an N dimensional feature space, with (·)T
denoting the transpose operation. Details on features and
feature engineering will be discussed in Section III. Simi-
larly, y ∈ RM ×1, a real-valued target classiﬁcation vector
y = [y1, · · · , yM ]T in an M dimensional classiﬁcation space.
In the application of PDF based malware detection, y is a
scalar (M = 1).

Fig. 1 depicts the architecture of the MLPdf model, where
x and ˆy denote input feature vector and trained binary output,
k denotes the neuron unit k ∈ RJ,K in the ith
respectively. ui
hidden layer, with i = 1, 2 in this work. Each interconnection
between the layers in the model is associated with a scalar
weight wj,k which is adjusted during the training phase, where
j and k are neuron unit indices between two consecutive
layers.

u1
1

u1
2

u1
3

...
u1
J

u2
1

u2
2

u2
3

...
u2
K

x1

x2

x3

xN

...

Fig. 1. Architecture of the MLPdf Model [17]

ˆy ∈ (0, 1)

y

x

Feature Selection

c

MLP

ˆy

e(ˆy, y)

{wj,k}

BP with SGD

e

Fig. 2. MLPdf weights update via BP algorithm with SGD search.

The goal of the MLP learning process is to ﬁnd a set of
optimal weights {wj,k} over a training dataset in order to
produce the outputs as close as possible to the target values.
As Fig. 2 shows, the learning process with weights update is
realized through the back-propagation (BP) algorithm which
employs stochastic gradient decent (SGD) search through the
space of possible weight values to minimize the error signal
e(ˆy, y) between the trained output ˆy and the target value y. In
our training datasets, malicious PDF ﬁles are labelled with 1
and benign ﬁles are labelled with 0. The labels are then used
as our target values {y} for the training model. More details
on how the weights {wj,k} are updated during the learning
process are discussed in [17–19]. In Fig. 2, there exists a
process of feature selection between the original features x
and the MLP model. This is to remove irrelevant features and
only keep the most representative feature components c. It is
an important part of the so-called feature engineering process,
as discussed in the following section.

III. FEATURE ENGINEERING

Feature engineering is a crucial step in machine learning.
It is to make a task easier for an ML model to learn. One
should not expect an ML model to be able to learn from
completely arbitrary data. In many cases, data features rather
than the raw data are used as input signals for an ML model.
There is no exception when dealing with the detection of PDF
attacks herein. There exist hand-crafted feature engineering

2

using speciﬁc domain knowledge and automated feature engi-
neering such as CNN based models. In this work, the manual
process of feature engineering is carried out. It includes feature
extraction from raw PDF ﬁles and feature selection using PCA
technique.

Though we believe that the features extracted herein should
possess strong discriminative power to differentiate malicious
ﬁles from benign documents, it is unavoidable to have re-
dundancy among the features. This is why a feature selection
process is often applied prior to a learning process.

TABLE I
DATASETS AND FEATURE INFORMATION

Training: 90000

Testing: 15047

Features: 48

Benign Malicious

Benign Malicious

Structure, metadata

78684

11316

13101

1946

Objects, content stats, etc.

A. Feature Extraction

The information of the training and testing datasets used
in this work is shown in Table I. An initial group of 48
features are extracted from the datasets. In-house tools and off-
the-shelf PDF parsers are used to extract the features, which
comprise information from PDF structure, metadata, object
characteristics as well as content statistical properties, etc. Part
of the 48 features are listed in Table II. As it shows, features
are deﬁned using ﬁle size, JavaScript existence, page count,
object count, stream ﬁltering, entropy value of some content.
The full list of features and the procedure of extraction are
not discussed in this paper due to commercial reasons.

TABLE II
PART OF THE EXTRACTED FEATURES

Feature name

Description

F SIZE

F JS

F PGC

F OBJC

F FILT

PDF ﬁle size

PDF with JavaScript or not

Page count

Number of objects

Stream ﬁltering

F ENTRP1

Entropy of some content

F ENTRP2

Entropy of some content

· · ·

· · ·

B. Feature Selection Using PCA

Irrelevant and redundant features can lead to an ML clas-
siﬁer to converge slowly and perform less well or completely
fail. In this paper, the PCA technique (also known as the
eigenvector regression ﬁlter or the Karhunen-Loeve transform
[20]) is used for dimensionality reduction, which involves
zeroing out one or more of the weakest principal components,
resulting in a lower-dimensional projection of the raw feature
data that preserves the maximal data variance. The dimen-
sionality reduction process is achieved through an orthogonal,
linear projection operation. Without loss of generality, the PCA
operation can be deﬁned as

Y = XC

(3)

with Y ∈ RS×P is the projected data matrix that contains P
principal components of X with P ≤ N . So the key is to ﬁnd
the projection matrix C ∈ RN ×P , which is equivalent to ﬁnd
the eigenvectors of the covariance matrix of X, or alternatively
solve a singular value decomposition (SVD) problem for X
[19]

X = UΣVT

(4)

where U ∈ RS×S and V ∈ RN ×N are the orthogonal matrices
for the column and row spaces of X, and Σ is a diagonal matrix
containing the singular values, λn, for n = 0, · · · , N −1, non-
increasingly lying along the diagonal. It can be shown [19]
that the projection matrix C can be obtained from the ﬁrst P
columns of V with

V = (cid:2)v1, · · · , vN

(cid:3)

(5)

and

As part of data pre-processing, all features need to be nor-
malized before any further processes such as feature selection
and training. The primary aim of normalization is to avoid
large gradient updates during the SGD search and learning
process. Let

X = (cid:2)x1, · · · , xS
where X ∈ RS×N is a feature matrix, x ∈ RN ×1 a feature
vector deﬁned in (1) and S the size of a malware or benign
dataset. Typically normalization is applied for each feature
independently. More speciﬁcally, this is carried out along each
column vector of X. The normalized feature vector then has
a standard normal (or Gaussian) distribution with µ = 0 and
σ = 1 where µ and σ are the mean and standard deviation of
the scaled feature vector, respectively.

(2)

(cid:3)T

C = (cid:2)c1, · · · , cP
where vn ∈ RN ×1 is the nth right singular vector of X, and
cn = vn.

(6)

(cid:3)

In fact, the singular values contained in Σ in (4) are the
standard deviations of X along the principal directions in
the space spanned by the columns of C [19]. Therefore,
λ2
n becomes the variance of X projection along the nth
principal component direction. It is believed that variance can
be explained as a measurement of how much information a
component contributes to the data representation. One way to
examine this is to look at the cumulative explained variance
ratio of the principal components, given as

(cid:80)P

(cid:80)N

n=1 λ2
n
n=1 λ2
n

(7)

Rcev =

3

and illustrated in Fig. 3. It
indicates that keeping only a
few principal components could retain over 90% of the full
variance or information of X. As a comparative study, a
varying number of principal components has been used and
examined in the following evaluation section.

Fig. 3. Cumulative explained variance ratio over components.

IV. EVALUATION RESULTS

The evaluation is based on the datasets described in Table
I, which comprises 105, 000 real-world benign and malicious
PDF documents collected from Sophos database. The mali-
cious PDF documents were mostly collected over the few
months up to March 2018 while the benign dataset has a longer
timespan.

It is of fundamental importance to tackle the overﬁtting
problem in any ML process, there is no exception in our
application. A trained ML algorithm must perform well on new
data which was never seen during training process. Typical
techniques to mitigate overﬁtting include Batch normalization
and Dropout [18], [19]. To make the evaluated models learn
better during training and generalize well on new data, batch
normalization with a batch size of 64 data points is applied to
the input of each layer after the input layer. This is to re-scale
the input batch to have zero mean and unit variance, similar to
the feature normalization discussed in Section III-A. Similarly,
a dropout rate of 0.15 is used during training process, which
leads 15% of each hidden layer outputs to be zeroed out
before feeding into next layer. In addition, around 20% of the
training dataset is used as the validation dataset to help detect
overﬁtting and perform model selection during the learning
process.

In our previous work [17], the input layer of the MLPdf
model has 48 nodes corresponding to the number of original
features used, and the output
layer has a single Sigmoid
(binary) probability output with values in the range of (0, 1).
There are two hidden layers with 72 neurons each. In this

evaluation, the number of input nodes for the models with
PCA based feature selection will be dependent on the se-
lected number of principal feature components, the rest of
the model settings remains the same as the MLPdf model.
A varying number of principal components is used in the
comparative study (resulting in dimensionality reduction of
79%, 41% and 33%), and we term the corresponding models
as MLPdf +PCA10, MLPdf +PCA28 and MLPdf +PCA32, re-
spectively. These models are trained with 5000 epochs each.
An epoch refers to a complete training cycle and many epochs
are needed in order to accomplish an ANN training task. The
training results are compared in Fig. 4. As the ﬁgure shows,
all models quickly reach over 97% accuracy after a small
amount of training epochs, then the accuracies continue to
improve with relatively slow convergence rates. It indicates
that using the ﬁrst 10 principal feature components could
achieve around 98% training accuracy after 3000 Epochs, this
is consistent with the observation of the cumulative explained
variance ratio shown in Fig. 3. It
the
model with 28 principal feature components performs even
better. As the ﬁgure shows, the best model using PCA is
MLPdf +PCA32 which exhibits excellent accuracy similar to
the original MLPdf model while with around 33% dimension-
ality reduction and 22% less learning time.

is no surprise that

Fig. 4. Training results comparison with/out feature selection.

To further examine the performance consistence and gen-
eralization of the model using PCA based feature selection,
the model MLPdf +PCA32 is tested with the testing dataset
shown in Table I, and compared with the original MLPdf
model as well as other seven major commercial AV scanners.
The comparison results are shown in Fig. 5 where AV1 to AV7
denote the corresponding commercial AV scanners. The best
result is from MLPdf which achieves an excellent 95.12% TPR
while maintaining a very low FPR of 0.08%, as discussed in
[17]. This is closely followed by the proposed MLPdf +PCA32
which manages to reach 93.17% TPR with the same low FPR

4

will be interesting to compare how the ANN based models and
other commercial scanners perform with larger datasets in our
future work, particularly adding more recent PDF documents
to the benign corpus. Given the fact that PCA is restricted to a
linear transformation, it is worth exploring non-linear feature
selection techniques such as autoencoders.

REFERENCES

[1] C.-H. Wu, “Behavior-based spam detection using a hybrid method
of rule-based techniques and neural networks,” Expert Systems with
Applications, vol. 36(3), pp. 4321–4330, 2009.

[2] S. Kumar, X. Gao, I. Welch, and M. Mansoori, “A Machine Learning
Based Web Spam Filtering Approach,” in Proceedings of IEEE 30th
International Conference on Advanced Information Networking and
Applications, Crans-Montana, Switzerland, 2016.

[3] V. Engen, “Machine learning for network based intrusion detection,”

Ph.D. dissertation, Bournemouth University, 2010.

[4] Sophos, “Sophos Unmatched next-gen endpoint protection: Intercept-
X,” https://www.sophos.com/en-us/products/intercept-x.aspx, accessed:
2018-03.

[5] R. Pascanu, J. W. Stokes, H. Sanossian, M. Marinescu, and A. Thomas,
“Malware classiﬁcation with recurrent networks,” in Proceedings of
2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP, Brisbane, Queensland, Australia, 2015.

[6] J. Zhang, “Make “Invisible” Visible - Case Studies in PDF Malware,”

in Proceedings of Hacktivity 2015, Budapest, Hungary, 2015.

[7] Z. Tzermias, G. Sykiotakis, M. Polychronakis, and E. P. Markatos,
“Combining Static and Dynamic Analysis for the Detection of Malicious
the fourth Workshop on European
Documents,” in Proceedings of
Workshop on System Security, Salzburg, Austria, 2011.

[8] P. Ratanaworabhan, B. Livshits, and B. Zorn, “NOZZLE: A Defense
Against Heapspraying Code Injection Attacks,” in Proceedings of the
18th conference on USENIX security symposium, Berkeley, CA USA,
2009.

[9] C. Willems, T. Holz, and F. Freiling, “Toward Automated Dynamic
Malware Analysis Using CWSandbox,” IEEE Security & Privacy, vol.
5(2), 2007.

[10] Wepawet, http://wepawet.iseclab.org/, accessed: 2018-03.
[11] P. Laskov and N. Srndic, “Static Detection of Malicious JavaScript-
Bearing PDF Documents,” in Proceedings of the 27th Annual Computer
Security Applications Conference, Orlando, Florida USA, 2011.
[12] J. S. Cross and M. A. Munson, “Deep pdf parsing to extract features
for detecting embedded malware,” Sandia National Laboratories, Albu-
querque, New Mexico 87185 and Livermore, CA 94550, Tech. Rep.
SAND2011-7982, Sep. 2011.

[13] D. Maiorca, G. Giacinto, and I. Corona, “Machine Learning and Data
Mining in Pattern Recognition,” in volume 7376 of Lecture Notes in
Computer Science, Springer Berlin / Heidelberg, 2012.

[14] C. Smutz and A. Stavrou, “Malicious PDF Detection using Metadata
and StructuralFeatures,” in Proceedings of the 28th Annual Computer
Security Applications Conference, Orlando, Florida USA, 2012.
[15] N. Srndic and P. Laskov, “Detection of Malicious PDF Files Based on
Hierarchical Document Structure,” in Proceedings of the 20th Annual
Network & Distributed System Security Symposium, San Diego, CA
USA, 2013.

[16] B. Cuan, A. Damien, C. Delaplace, and M. Valois, “Malware Detection
in PDF Files Using Machine Learning,” REDOCS, Tech. Rep. Rapport
LAAS No. 18030, Feb. 2018.

[17] J. Zhang, “MLPdf : An Effective Machine Learning Based Approach
for PDF Malware Detection,” in Black Hat USA 2018, Las Vegas, NV,
USA, 2018.

[18] T. Mitchell, Machine Learning. McGraw Hill, 1997.
[19] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. The MIT

Press, 2016.

[20] R. C. Gonzalez and R. E. Woods, Digital Image Processing, 2nd Edition.

AddisonWesley, 2002.

Fig. 5. Testing results between MLPdf and major AV scanners.

of 0.08% maintained. Both models signiﬁcantly outperform all
commercial scanners. The best commercial scanner (denoted
as AV4) only has a TPR of 84.53%.

It is worth pointing out that the evaluated seven commercial
scanners perform well with zero or very low FPR with the
benign testing dataset as well. One possible reason could
be that our benign ﬁles are mostly collected from Sophos
clean PDF documents database which has a relatively longer
timespan, and the majority of them might have been already
known to the commercial scanners as well.

V. CONCLUSION

In this paper, we have proposed a novel approach by
extending our recently proposed ANN model MLPdf in [17]
with feature selection using PCA technique for malware de-
tection. As a case study, the effectiveness of the approach
has been successfully demonstrated with the application in
PDF malware detection. Our evaluation shows that the model
with PCA can signiﬁcantly reduce feature redundancy with
minimum impact on data information loss, as conﬁrmed by
both training and testing results based on around 105, 000 real-
world PDF documents. More speciﬁcally, a comparative study
has been carried out for the model with a varying number
of selected principal feature components. Of the evaluated
the model with 32 principal feature
models using PCA,
termed MLPdf +PCA32, exhibits very similar
components,
training accuracy to the MLPdf model while with around
33% dimensionality reduction and 22% less learning time. The
testing results further conﬁrm the effectiveness and show that
the proposed MLPdf +PCA32 model is able to achieve 93.17%
TPR while maintaining the same low FPR of 0.08% as the case
with MLPdf . On the other hand, the best commercial scanner
(denoted as AV4) only manages to have a TPR of 84.53%. As
previously pointed out, the commercial scanners perform well
on the benign testing ﬁles as well with zero or very low FPR. It

5

