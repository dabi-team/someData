1

Cyber LOPA: An Integrated Approach for the
Design of Dependable and Secure Cyber Physical
Systems

Ashraf Tantawy, Member, IEEE, Sherif Abdelwahed, Senior Member, IEEE,
and Abdelkarim Erradi, Member, IEEE

2
2
0
2

g
u
A
7
1

]

R
C
.
s
c
[

4
v
5
6
1
0
0
.
6
0
0
2
:
v
i
X
r
a

Abstract—Safety risk assessment is an essential process to
ensure a dependable Cyber-Physical System (CPS) design. Tra-
ditional risk assessment considers only physical failures. For
modern CPS, failures caused by cyber attacks are on the rise.
The focus of latest research effort is on safety-security lifecy-
cle integration and the expansion of modeling formalisms for
risk assessment to incorporate security failures. The interaction
between safety and security lifecycles and its impact on the
overall system design, as well as the reliability loss resulting from
ignoring security failures are some of the overlooked research
questions. This paper addresses these research questions by
presenting a new safety design method named Cyber Layer
Of Protection Analysis (CLOPA) that extends existing LOPA
framework to include failures caused by cyber attacks. The
proposed method provides a rigorous mathematical formulation
that expresses quantitatively the trade-off between designing a
highly-reliable versus a highly-secure CPS. We further propose a
co-design lifecycle process that integrates the safety and security
risk assessment processes. We evaluate the proposed CLOPA
approach and the integrated lifecycle on a practical case study
of a process reactor controlled by an industrial control testbed,
and provide a comparison between the proposed CLOPA and
current LOPA risk assessment practice.

Index Terms—Cyber Physical System, CPS, Security, IEC
61511, NIST SP 800-30, SCADA, LOPA, Safety Instrumented
System, Safety Integrity Level, Risk Assessment, HAZOP.

I. INTRODUCTION

A cyber physical system (CPS) is an integration of a phys-

ical process with computation and networking required
for physical system monitoring and control. The integration of
process dynamics with those of computation and networking
brings a plethora of engineering challenges. As the majority
the
of CPS are deployed in mission-critical applications,
dependability and resilience to failures is a key design property
for modern CPS.

To ensure a given CPS is dependable, a risk assessment
is carried out both at design time and operation time. The
risk assessment process highlights the system weaknesses and
helps deﬁne the safety requirements that need to be met to
achieve the target reliability measures. The classical approach

Ashraf Tantawy is with the School of Computer Science and In-
formatics, De Montfort University, Leicester LE1 9BH, UK. E-mail:
ashraf.tantavy@dmu.ac.uk

Sherif Abdelwahed is with the Department of Electrical and Computer
Engineering, Virginia Commonwealth University, Richmond, VA 23284, USA.
E-mail: sabdelwahed @vcu.edu

Abdelkarim Erradi

is with the Department of Computer Science and

Engineering, Qatar University, Doha, Qatar. Email: erradi@qu.edu.qa

to perform the risk assessment is to consider physical failures
only. As state of the art CPS designs move to open source
hardware and software, cyber attacks have become a source
of failure that cannot be ignored.

Realizing the critical nature of CPS cyber attacks and their
impact on the safety of people and environment, as well as the
potential catastrophic ﬁnancial losses, the research community
developed several approaches to integrate security aspects into
the safety risk assessment process. This integration has been
done mainly by extending the reliability modeling formalism
to incorporate security-related risks. One of the overlooked
research questions is how safety and security interact with each
other and how this interaction would impact the overall system
design. Putting this research question in a different format: Is
there a trade-off between designing a highly reliable and a
highly secure system? A related research question is: If we
ignore the cyber security attacks in the design process, what
is the impact on the overall system reliability? Is the reliability
gain worth the complexity introduced by integrating security
both at design and run-time? A follow-up research question
is: Under what conditions can we ignore security failures?

In order to better understand the interaction between safety
and security lifecycles in the system design process, we
consider in this paper the safety risk assessment process and
study the impact of overlooking failures caused by cyber
attacks. We refer to such failures as security failures in the
rest of the paper. By formally introducing the failures caused
by attacks into the risk assessment process, we can deﬁne
the reliability requirements for the cyber components of the
system as a function of both the failure rate of physical
components and the resilience to cyber attacks. This formal
requirements speciﬁcation enables us to understand the design
trade-off between higher reliability of physical components vs
higher resilience of cyber components, and the sensitivity of
the overall system performance to both types of failures. In
addition, we can gain insight into the interplay between safety
and security and how to integrate both lifecycles during the
design process. More speciﬁcally, we consider the Layer Of
Protection Analysis (LOPA), a widely adopted risk assessment
method that follows a hazard identiﬁcation study, such as
Hazard and Operability (HAZOP). LOPA is carried out to
identify whether an additional Safety Instrumented System
(SIS) is needed for speciﬁc hazardous scenarios to achieve
the target risk level. As modern SIS is typically an embedded
device, it has both physical and security failure modes. We

 
 
 
 
 
 
mathematically derive the SIS design constraints in terms of
both physical and security failure probabilities. Additionally,
we propose an integrated safety-security design process that
shows the ﬂow of information between both lifecycles.

We can classify the research work on combining safety
and security for CPS into two broad categories that try to
answer the following research questions: (1) Given the inde-
pendent safety and security lifecycles, what are the similari-
ties/differences and how could the two lifecycles be aligned or
uniﬁed? This research direction usually focuses on answering
the question ”what to do”, rather than ”how to do it”, (2) For
a given CPS, how can we carry out risk assessment (qual-
itative/quantitative) that considers both physical failures and
cyber attacks? Consequently, how can we unify the process of
safety and security requirements deﬁnition and veriﬁcation?
This research direction focuses on common modeling tech-
niques that can incorporate both safety and security failures,
and often extends model-based engineering body of knowledge
and tools to incorporate security requirements in the design
process. In section VI, we survey the main results for each
research direction. A more thorough survey is presented in
[1], [2].

Our Contribution. The work presented in this paper ad-
dresses both research directions with a new approach. First,
we integrate both safety and security lifecycles based on a rig-
orous mathematical formulation that captures their interaction.
The formulation enables the designer to assess how a security
design decision would impact system safety. This is in contrast
to existing research work that does not explicitly model
the dynamic safety-security lifecycle interaction. Second, we
develop an integrated safety-security design lifecycle and show
in details how to apply it to a real world design, distinguishing
the work from abstract research on risk assessment that does
not carry over to the design stage. Finally, our approach is
founded on LOPA, a practical approach that is extensively
used in industry, giving the approach the merit for industrial
implementation.

The rest of the paper is organized as follows: Section II
introduces the background information required for problem
setup, including IEC 61511 safety lifecycle and the LOPA
method, cyber dependence between control and safety systems,
and cyber security risk assessment. Section III proposes a new
LOPA mathematical formulation we call ”CLOPA”, that incor-
porates failures due to cyber attacks. Section IV proposes an
integrated safety-security lifecycle process. Section V presents
a case study for the design of a safety system for a chemical
reactor, comparing classical LOPA approach to the proposed
CLOPA formulation. Section VI summarizes the related work
on safety-security co-design. The work is concluded in section
VII.

II. SAFETY AND SECURITY RISK ASSESSMENT

There are two main embedded systems that control and
safeguard a given physical system, the control system and the
safety system. In the process industry, the control systems is
referred to as the Basic Process Control System (BPCS), and
the safety system is referred to as the Safety Instrumented

2

System (SIS). In practice, both systems typically have a
programmable controller architecture with one or more back
planes, processor cards, and a variety of input/output interface
cards [3]. For larger systems, BPCS and SIS architectures
comprise multiple distributed nodes connected via a com-
munication backbone. Figure 2 depicts the two systems and
their connectivity over a control network. In the following, we
brieﬂy discuss the SIS design lifecycle, BPCS and SIS security
lifecycles, and their interaction.

A. IEC 61511 Safety Lifecycle Process

Figure 1 shows the Safety Instrumented System (SIS) design
lifecycle according to IEC 61511 standard [4]. The design
starts with Hazard & Risk assessment, where systems hazards
are identiﬁed. HAzard and OPerability (HAZOP) study, What
If analysis, and Fault Tree analysis are the most common
methods at this stage [5]. The risk assessment phase ranks each
identiﬁed risk according to its likelihood and consequence,
either quantitatively or qualitatively, and associates a risk
ranking for each hazard. The resulting list of hazards and
associated risk ranking is used as an input to the second phase
focused on the allocation of safety functions to protection
layers. This phase deals only with hazards that exceed a
threshold risk rank that an organization is willing to accept.
For each hazardous scenario, there is a Target Mitigated Event
Likelihood (TMEL) measure that is deﬁned based on the risk
rank. The purpose of this phase is to check if the TMEL
is met with existing protection layers. If not, an additional
protection layer is recommended, often in the form of a
new Safety Instrumented Function (SIF) with a predeﬁned
Safety Integrity Level (SIL) to cover the gap to the TMEL.
The safety instrumented function comprises one or more
sensors, a logic solver, and one or more actuators. The logic
solver is commonly referred to as the Safety Instrumented
System (SIS). An example SIF is illustrated in Figure 6 for
an overﬂow hazardous scenario of a reactor system, which
will be discussed in details in section V. Risk Matrix, Risk
Graph, and Layer Of Protection Analysis (LOPA) are the most
commonly used methods for the allocation of safety functions
to protection layers [3].

The third phase is the development of the Safety Require-
ments Speciﬁcation (SRS), which documents all the functional
and timing requirements for each SIF. The fourth phase is
the detailed design and engineering. Phases 5 to 8 are con-
cerned with system installation and commissioning, operation,
modiﬁcation, and decommissioning. Phase 2 is where the
CPS control and safety systems are considered in the risk
assessment process. Therefore, we study this phase in depth in
this paper. Since LOPA is the predominant approach for this
phase, we limit our discussion to LOPA methodology. Other
approaches could be adopted in a similar way.

The underlying assumption in LOPA analysis is that all
protection layers, including the new SIF, are independent. In
other words, if one layer failed, this does not increase or
decrease the likelihood of failure of the other layers. This
assumption simpliﬁes the mathematical analysis signiﬁcantly,
as it allows the multiplication of individual probabilities to

Figure 1.

IEC 61511 SIS design lifecycle (adopted from [4])

Figure 2. A snapshot of an industrial control system architecture showing
BPCS-SIS connectivity and potential attack vectors.

obtain the required joint probability. The simplicity of LOPA
calculations is probably one of the key reasons behind its
widespread adoption by industry. Unfortunately, when cyber
security is considered as a potential failure in LOPA analysis,
the independence assumption between the control and safety
systems no longer holds, as explained in the next section.

B. Control and Safety Systems Cyber Dependence

Each of the control and safety systems has two modes of
failure; BPCS physical failures, Bp, BPCS security failure,
Bc, SIS physical failure, Sp, and SIS security failure Sc. For
physical failures, IEC 61511 standard strongly recommends
complete separation between the control and safety systems
of any plant. This separation includes sensors, computing
devices, as well as ﬁnal elements such as valves and motors.
Separation also includes any common utility such as power
supplies. The industry adopted this separation principle, hence
BPCS and SIS physical failures could be accurately assumed
to be independent, i.e., P [Bp, Sp] = P [Bp]P [Sp].

One exception to the separation between BPCS and SIS
is the cyber communication link between the control and
safety systems. Figure 2 is a snapshot of a typical indus-
trial control system architecture showing the communication
link between the BPCS and SIS. BPCS-SIS communication
could be over the control LAN or via a dedicated point to
point serial link. The communication protocol is typically an
open standard such as Modbus or DNP3 [6], [7]. This type
of communication exists in many industrial installations to
exchange plant data, as the data from ﬁeld devices connected
to the safety system is not accessible from the BPCS and
vice versa. Given this architecture, we can deﬁne two attack
vectors for SIS compromise: (1) a direct attack that exploits
an existing controller vulnerability could be launched against
the SIS node. This could be via any node on the control LAN
or using Man In The Middle (MITM) attack that exploits the
BPCS-SIS communication. We designate this attack event by

3

AS in Figure 2, and (2) by compromising the BPCS ﬁrst
then exploiting the BPCS-SIS link to compromise the SIS.
We designate this pivot attack by the sequence of events AB
and ABS in Figure 2. Further, we designate the attack event
from the SIS to the BPCS by ASB. The attack sequence
AB → ABS may be easier if the SIS is highly secured such
that direct attack may be infeasible. This is particularly true
if we consider the fact that the BPCS is a trusted node to the
SIS.

The above analysis shows a clear dependency between the
control and safety systems that violates the original LOPA
independence assumption. The security failures for the BPCS
and SIS are no longer independent due to the data commu-
nication coupling. We can formulate the different security
failure probabilities as in (1) to (3) using basic probability
laws with the aid of Figure 2, where P [Ai] is interpreted
as the probability of success of attack Ai. Furthermore, the
considered attack Ai should have the impact of stopping
the BPCS or SIS from performing its intended control or
safeguard function as related to the hazard under study. This
is important because not all attacks that exploit controller
vulnerabilities result in a process hazard. Therefore, the attacks
considered represent a subset of the complete set of attacks that
could exploit the BPCS or SIS vulnerabilities. Accordingly,
from hereafter, P [AB], P [AS], P [ABS], P [ASB] refer to the
relevant attacks that cause a process hazard. This concept
is revisited throughout
the paper and is made more clear
in the case study when sample attacks are presented. For a
case study example on the fact that not all cyber failures
have a system reliability consequence, we refer the interested
reader to [8] for a study on the impact of different software
failure modes on system reliability for the electric power grid
domain. Finally, it can be easily shown that if the BPCS-
SIS communication link does not exist, or fully secured, then
P [ASB] = P [ABS] = 0, and equation (3) reduces to the
independent case P [Sc, Bc] = P [Sc]P [Bc].

P [Bc] = P [AB] + P [AS]P [ASB] − P [AB]P [AS]P [ASB]
(1)
P [Sc] = P [AS] + P [AB]P [ABS] − P [AB]P [AS]P [ABS]
(2)

P [Sc, Bc] = P [AB](P [AS] + P [ABS]) + P [AS]P [ASB]
− P [AB]P [AS](P [ABS] + P [ASB])

(3)

C. Cyber Security Risk Assessment

The calculation of the probability of cyber attacks AS, AB,
and ABS could be performed during the cyber security risk
assessment process. This requires a detailed speciﬁcation
of the BPCS and SIS and their connectivity, including the
embedded system hardware, operating system, running soft-
ware services, as well as the network connectivity. According

1-Hazard and Risk Assessment2-Allocation of safety functions to protection layers3-SIS Safety Requirements Spec. 4-SIS Design & Engineering5-SIS Installation, commissioning, validation6-Operation and maintenance7-SIS modifications8-DecommissioningSafety Instrumented System (SIS)Basic Process Control System (BPCS)ASABSMITM AttackDirect AttackABASBto NIST SP 800-30 standard, ”Guide for Conducting Risk
Assessments”, the cyber security lifecycle process stages are:
(1) asset identiﬁcation, where the particular cyber components
and their criticality levels are identiﬁed, (2) vulnerability
identiﬁcation, along with the associated threats and attack
vectors, (3) development of relevant attack trees for each
attack scenario identiﬁed, (4) penetration testing to validate
the vulnerability ﬁndings and attack scenarios and to help
estimating the effort and probability for individual attack
steps for each scenario, and (5) risk assessment to identify
the scenarios with unacceptable risk [9]. Figure 5 shows the
BPCS and SIS cyber security lifecycles. In this work, we
follow the same cyber security lifecycle, but with the physical
process as the main focus. Therefore, for asset identiﬁcation,
the cyber component criticality is primarily identiﬁed by its
failure impact on the operation of the connected physical
component. Similarly, for vulnerability identiﬁcation, threats
and attack vectors are ﬁltered by their impact on the physical
process. Attacks that do not disturb the controlled process are
ignored as they have no direct impact on the process safety. In
addition, such impacts take place with much higher probability
at the corporate network level so they can be ignored with
minimum impact on the risk assessment at the control network
level. For more detailed discussion on process-driven attack
identiﬁcation, we refer the reader to [10].

For the presented architecture, the BPCS and SIS are the
critical components in direct contact with the process. The
calculation of the required BPCS and SIS security failure
probabilities could be typically carried out with the aid of
attack trees [11]. The attack tree enumerates all possible
routes to compromise the system, and each edge is assigned
a probability representing the likelihood of the associated
event. Using basic probability laws, the overall probability of a
system compromise could be calculated. Section V presents an
example of such calculation. We re-emphasize the fact that the
scope of cyber security risk assessment and attack trees in this
case will be limited to attacks targeting the physical process
to cause a process hazard. Although information security
attacks with objectives such as stealing information is possible,
most of this information is already available at the corporate
network level, and an attacker who penetrates down to the
control network level to compromise an BPCS or SIS will
conceivably have the goal of physical process attack.

III. CLOPA: LOPA WITH SECURITY FAILURES

A. Mathematical Formulation

In risk assessment, an initiating event

is an unplanned
event that when occurring may lead to a hazard. Examples
of initiating events include equipment failure, human error,
and cyber attacks. A system hazard will take place if one
or more of the initiating events occur and all the associated
protection layers against that hazard fail simultaneously. The
main objective of LOPA is to calculate the expected number
of hazardous events per time interval and compare it to the
TMEL. We designate the random variable representing the
number of events per unit time for a speciﬁc initiating event by
N , the random variable representing the simultaneous failure

4

of protection layers when the initiating event occurs by L,
where L is Bernoulli-distributed with success probability p,
and the random variable representing the number of hazards
per time interval by H. We then have:

N
(cid:88)

H =

IE(li)

(4)

i=1
where I is the indicator function, and the set E = {l : li =
1, i = 1 : N }. We note that for a given N = k, H is a
binomially-distributed random variable with expected value
E[H|N = k] = kp. Therefore:

∞
(cid:88)

E[H] =

E[H|N = k]P [N = k]

k=0
∞
(cid:88)

= p

k=0

kP [N = k] = pE[N ] = pλ

(5)

where λ represents the expected value of the number of
initiating events per unit time, N . Although N is typically
modeled by a Poisson random variable in reliability engi-
neering, we do not assume any speciﬁc distribution in the
analysis. This is particularly important because some initiating
events considered in the paper, such as security failures, are
not accurately modeled by a Poisson distribution.

Equation (5) is the underlying mathematical concept behind
LOPA analysis. Essentially, for each initiating event,
the
likelihood λ is estimated from ﬁeld data, and the probability
of simultaneous failure of all protection layers is speciﬁed.
Finally, the expected number of hazards per unit time, E[H],
considering all initiating events, is estimated and compared
to the pre-speciﬁed TMEL. If E[H] > TMEL, then a safety
instrumented system is required with a probability of failure
on demand P [Sp] (or equivalently a Risk Reduction Factor
RRF = 1/P [Sp]) that achieves E[H] ≤ TMEL.

In order to express the LOPA formula in (5) in terms
of all protection layers, including the BPCS and SIS, we
introduce some mathematical notation. We designate the set
of initiating events for a given hazardous scenario by I =
{I1, I2, . . . , In, Bp, Ar}, where n is the number of possible
initiating events excluding BPCS failures, Bp denotes BPCS
physical failure event, and Ar denotes the set of attacks
relevant to the hazard under study. We express the associated
likelihoods by Λ = {λ1, λ2, . . . , λn, λp, λc}.
set of event
Further, we denote the set of all possible protection layers
by L = {L1, L2, . . . , Lm}, where m is the number of
existing protection layers, excluding the BPCS and SIS. BPCS
protection is denoted by B, and SIS protection is denoted by
S. For each initiating event i, there is a subset of protection
layers Li ⊆ L ∪ {B, S} that could stop the propagation of a
hazard from causing its consequences. Table I shows a sample
LOPA table using the introduced terminology.

B. Semantically-relevant Attack Events Formulation

We designate the set of all possible attacks against the BPCS
by A. For a given hazard under consideration, the subset of
attacks that would lead to this hazard, i.e. the contextually or
semantically-relevant attacks, is designated by Ar. To estimate

(cid:88)

a∈A

P [A = a] =

(cid:88)

a∈A

αa = 1

(6)

P [S, B] = P [Sp] (P [Bp](1 − P [Bc] − P [Sc]) + P [Bc])

Initiating
Event
I1
. . .
In
Bp
Ar

Likelihood
λi (/yr)
λ1
. . .
λn
λp
λc

L1

. . .

Lm BPCS

TMEL

(B)

← P [L1] → P [B]

10−x

. . .

← P [Ln] → P [B]
← P [LB] → 1
← P [LB] → P [B]

Table I
SAMPLE LOPA TABLE. P [L] REFERS TO THE COMBINED PROBABILITY OF
FAILURE OF PROTECTION LAYERS APPLICABLE TO THE INITIATING EVENT
FROM L1 TO Lm.

the likelihood of relevant cyber attacks (expected number
per unit time), λc, we assume an attacker proﬁle with an
average rate of launching attacks per unit time λ. For every
launched attack, the attacker is presented with the complete set
of attacks A, and selects only one attack a with probability
P [A = a] = αa that is dependent on the attacker proﬁle, such
that:

The likelihood of cyber attack a ∈ Ar is then λa = λαa.
This cyber attack has the potential to cause a system hazard
if both BPCS and SIS fail jointly to stop the attack (either
physical or cyber failure). We designate this probability by
Pa[S, B]. The exact approach to include every attack a ∈ Ar
in the LOPA table is to treat each attack as an individual entry,
akin to the last row in Table I, with initiating event likelihood
λa. However, this approach has two main drawbacks: First,
the number of attacks could be large and this would grow the
LOPA sheet signiﬁcantly. Second, the treatment of each attack
individually would not allow us to utilize attack modeling
techniques such as attack trees that model collectively all
possible attack paths for one attack objective. Therefore,
we adopt an alternative approach where all relevant attacks
a ∈ Ar could be represented by one entry in the LOPA table.
The following lemma summarizes the approximate solution.
The proof is included in Appendix A.

Lemma III.1. Assume a given hazard scenario H, a control
system BPCS, an average rate of launching attacks against
BPCS λ, Hazard H semantically-relevant attack set Ar, and
probability αa of selecting attack a ∈ Ar. Then, the impact
of all initiating events a ∈ Ar on the LOPA calculation could
be approximated by a single initiating event with likelihood
λc = λ (cid:80)
αa and a BPCS failure probability with respect
to the combined set of attacks a ∈ Ar, where each attack
probability is weighted by the factor γa = αa/ (cid:80)

αa.

a∈Ar

a∈Ar

The lemma enables us to use attack trees with leaf nodes
weighted by γa to calculate the BPCS security failure prob-
ability in response to the combined set of attacks Ar with
likelihood λ (cid:80)
αa. For the special case where the cyber
attacker proﬁle results in random selection of the attack a ∈ A,
e.g., an attacker with no knowledge about the system, the
likelihood reduces to λ|Ar|/|A| and the leaf node weights
reduce to γa = 1/|Ar|, ∀a. We use this special case in the
case study in Section V.

a∈Ar

5

C. Cyber LOPA Formulation

With the introduced notation, the expected number of haz-
ards in (5), which should be less than the TMEL, could be
expanded as:

E[H] =P [S, B]

(cid:32) n
(cid:88)

(λiP [Li]) + λcP [LB]

+

(cid:33)

i=1
λpP [S]P [LB] ≤ TMEL

(7)

where LB is the set of protection layers for BPCS physical or
security failure event, and we assume that all protection layers
are independent from the BPCS and SIS, while keeping the
dependence between the BPCS and SIS. In addition, higher or-
der probability terms resulting from multiple initiating events
are ignored due to their insigniﬁcance.

To calculate the joint failure probability P [S, B], we use
basic probability laws and the fact that the BPCS and SIS
have both physical and cyber modes of failure as explained in
Section II-B to obtain:

+ P [Sc, Bc] (1 − P [Sp] − P [Bp] + P [Sp]P [Bp])
+ P [Sc]P [Bp]

(8)

Substituting (8) in (7), we obtain the general LOPA equation
this expanded version of LOPA hereafter

in (9). We call
CLOPA, standing for Cyber LOPA.

P [Sp] ≤

β − (α1P [Sc] + α2P [Sc, Bc])
α1 − α1P [Sc] + α2P [Bc] − α2P [Sc, Bc]

(9)

where:

α1 = P [Bp]

(cid:32) n
(cid:88)

i=1

α2 = (1 − P [Bp])

β = TMEL

(λiP [Li]) + λcP [LB]

+ λpP [LB] (10)

(cid:33)

(λiP [Li]) + λcP [LB]

(cid:33)

(cid:32) n
(cid:88)

i=1

(11)

(12)

In order to deﬁne the CLOPA formula in terms of the
actual design variables P [AS] and P [ABS] that represent the
probability of security failures of actual CPS components, we
substitute (1) - (3) into (9) to obtain:

P [Sp] ≤

β − γ1P [AS] − γ2P [ABS](1 − P [AS])
γ3 − γ3P [AS] − γ2P [ABS](1 − P [AS])

where:

γ1 = α1 + α2[P [AB] + P [ASB](1 − P [AB])]
γ2 = (α1 + α2)P [AB]
γ3 = α1 + α2P [AB]

(13)

(14)

(15)

(16)

Equation (13), along with (10) - (12) and (14) - (16), represent
the general CLOPA formulation to design the safety instru-
mented system. It represents an upper bound on the probability
of physical failure for the safety system in terms of the security
failure probabilities, showing clearly the coupling between
the safety system and security system design. The design
of the safety instrumented system should satisfy (13), where
the design variables are P [Sp], P [AS], and P [ABS]. The rest

are model parameters that are predetermined, including the
BPCS failure marginal probabilities. This is because the BPCS
design is independent of the SIS design, and usually takes
place earlier in the engineering design cycle. Note that we
assume here that P [ASB]
is a known parameter. This is
because by completely deﬁning the BPCS and its hardware
and software speciﬁcations, the probability of a cyber attack
compromising process safety could be estimated, even though
the SIS is not yet completely deﬁned. Table V in the appendix
summarizes the model variables, parameters, and how the
model parameters are calculated.

It should be noted that with existing LOPA methodology, se-
curity failures are ignored, i.e., P [AB] = P [AS] = P [ABS] =
P [ASB] = 0. Substituting these zero values in (13), we obtain
the classical LOPA formulation:

TMEL

P [Bp] (cid:80)n

i=1 (λiP [Li]) + P [LB](λp + λc)

(17)

P [Sp] ≤

β
α1

=

D. Design Space

Using the fact that P [Sp] ≥ 0 for a realizable safety system

in (13), we obtain:

γ1P [AS] + γ2P [ABS] − γ2P [AS]P [ABS] ≤ β

(18)

Figure 3 shows the shaded region deﬁned by the inequality
in (18). The boundary curve is deﬁned by (18) when equality
holds:

P [ABS] =





β
γ2

1 −

(cid:17)

(cid:16) γ1
β

P [AS]

1 − P [AS]





(19)

The ﬁrst order derivative of the boundary curve is negative for
γ1/β > 1 and positive otherwise. Since γ1 > β to require a
safety system (proof is straightforward by inspecting equations
(10),(11), (14), and (17)), the boundary curve is concave as
in Figure (3b). We note that any point in the shaded region
results in a feasible SIS. Points on the boundary curve result
in P [Sp] = 0, or equivalently RRF → ∞. Points closer to
the boundary would have high values for the RRF, requiring a
very highly reliable SIS that may not be achievable in practice.
Points closer to the origin result in lower RRF. It can be easily
shown that the contour lines for (13), where P [Sp] = C, could
be expressed as:

6

Figure 3. CLOPA Design Region (shaded). Any point in the shaded region
results in a feasible SIS. Points near the boundary requires a SIS with a very
high RRF value, hence difﬁcult to obtain in practice.

perfectly secured safety system where P [AS] = P [ABS] = 0,
with RRF given by :

P [Sp]max =

β
γ3

, RRFmin =

γ3
β

(21)

Clearly, points outside the shaded region result
in non-
realizable SIS. This result re-emphasizes the interplay between
the safety and security systems of a cyber physical system.

The design space highlights the major difference between
LOPA and CLOPA. In LOPA, the SIS requirement is related
to reliability in the form of the required safety integrity level.
In CLOPA, an additional requirement for the SIS is its security
resilience, in the form of an upper bound on the probability
of a security failure (cyber attack success), either directly or
indirectly via the BPCS.

E. Classical LOPA Error

To obtain the error resulting from using classical LOPA, we

subtract (17) from (13) to obtain:

eRRF =

ζ1 + ζ2P [AS] + ζ3P [ABS](1 − P [AS])
β [β − γ1P [AS] − γ2P [ABS](1 − P [AS])]

where:

ζ1 = β(γ3 − α1) = βα2P [AB]
ζ2 = α1γ1 − βγ3
ζ3 = γ2(α1 − β)

(22)

(23)

(24)

(25)

P [ABS] =

Cγ3 − β
γ2(C − 1)

1 −





(cid:17)

(cid:16) Cγ3−γ1
Cγ3−β
1 − P [AS]

P [AS]



 (20)

The minimum error occurs for a perfectly secured safety
system, i.e., P [AS] = P [ABS] = 0:

The contour line that represents the design boundary in Figure
3 can be derived from (20) by setting C = 0.

We can extract several information from this graph: (1) the
maximum probability of security failure for the safety sys-
tem by directed attacks is β/γ1. This probability results in
an unrealizable safety system, as the required RRF → ∞.
(2) The maximum probability of security failure for the safety
system by pivot attack via the BPCS is β/γ2. Likewise, this
probability does not result in a realizable safety system. Finally
(3) The minimum value of RRF is achieved at the origin for a

min eRRF = P [AB]

(cid:19)

(cid:18) α2
β

(26)

The error will be zero, i.e., classical LOPA result matches
CLOPA, if the probability of BPCS security failure via a direct
attack is zero.

IV. SAFETY-SECURITY CO-DESIGN

A. Design Process

The current industrial practice is to perform safety and
security risk assessments independently, treating the physical

and cyber components of a CPS as two separate entities.
As illustrated in section III, accurate safety risk assessment
requires knowledge about the cyber components and their se-
curity failure probabilities. Formally, the objective is to design
a safety instrumented system architecture A that satisﬁes (13)
in terms of both physical and security failure probabilities.
Suppose that the architecture A could be represented by a set
of design variables represented by the vector x. If we can relate
the physical and security failure probabilities to the vector x
by P [AS] = f (x), P [ABS] = g(x), P [Sp] = h(x), then we
can use these functions to substitute the relevant probabilities
in (13) and our design problem will be to ﬁnd a set of values
for the vector x that satisﬁes the CLOPA constraint (13).
Unfortunately, this design approach is not followed by industry
for several reasons. First, abstracting a given architectural
design A into a set of design variables is a very difﬁcult task,
not to mention that these design variables have to be linked to
both physical and security failures. Second, ﬁnding an exact or
approximate representation of the functions f (.), g(.), and h(.)
that relate the failure probabilities to the design variables may
not be possible, as it is not always clear how a design decision
would result in a higher or lower probability of failure. Finally,
even if we were able to make a perfect modeling, the resulting
problem to solve may turn into a discrete optimization problem
that is not possible to solve in polynomial time.

Due to these modeling limitations, the current industrial
practice to design safety instrumented systems (excluding
cyber attacks) is to follow an iterative process and rely on engi-
neering judgement during the design process. More precisely,
the required risk reduction factor RRFd is initially calculated,
then the engineering design proceeds to achieve RRFd using
both experience and industrial standard guidelines [4]. After
the design is completed, design veriﬁcation is conducted to
calculate the risk reduction factor of the proposed design
RRFv. If The resulting RRFv ≥ RRFd, then the design stops.
Otherwise, the design is reﬁned until the condition RRFv ≥
RRFd is satisﬁed. In the following, we will adopt the same
iterative design approach for CLOPA.

Figure 4 illustrates the iterative design process. We start with
initial values (P [AS], P [ABS], RRFd) that satisfy CLOPA
constraint in (13). We then proceed with SIS design to produce
an architecture A. The architecture is then veriﬁed to estimate
its probability of failure on demand, or equivalently its risk
reduction factor RRFv. The architecture is also used to carry
out a security risk assessment to estimate the probability of
security failures P [A(cid:48)
BS]. If the new set of ob-
tained values (P [A(cid:48)
BS],RRFv) still satisfy the CLOPA
equation, the design stops. Otherwise, a new iteration will
start to adjust the design in order to achieve the CLOPA
constraint. This adjustment could be by adding more security
controls, or by increasing the reliability of the system using
fault tolerant techniques. Algorithm 1 summarizes the iterative
design process.

S] and P [A(cid:48)

S], P [A(cid:48)

7

Figure 4. CLOPA Iterative Design Process - CSTR case study design values
are shown

reduction factor is well-documented in the standards using
Safety Integrity Levels (SIL), and the extra cost to move from
one SIL to a higher SIL is well quantiﬁed in industry. What
is not very clear, though, is what is an achievable value for
the probability of security failures. This is still not a well-
developed ﬁeld, and the argument of how to assess such
probabilities is still going on in the research community.

Another important question is whether there is any formal
guarantees that Algorithm 1 will terminate. To answer this
question, we need to know, or at least approximate, how the
architectural design A impacts the RRF, P [AS], and P [ABS].
As pointed out earlier, this is very hard in practice. Without
such relationship, the question of convergence to a solution for
algorithm termination cannot be precisely answered. However,
in practice, modifying the SIS design to increase the RRF is
usually done by changing sensor and actuator conﬁguration or
reliability ﬁgures, as they are often the weakest links in the
reliability chain, while the logic solver is minimally changed
[4]. Accordingly, for all practical purposes, we can assume
that the design process will converge after few runs.

Algorithm 1: Integrated Safety-Security Lifecycle De-
sign Algorithm
input : BPCS
output: A, θS
([P [AB], P [ASB]) ← BPCS-SecCycle(BPCS) ;
θB ← (P [AB], P [ASB]) ;
(P [AS], P [ABS], RRFd) ← DesignContour(θB) ;
θS ← (P [AS], P [ABS], RRFd) ;
do

S], P [A(cid:48)

A ← SIS-SafeCycle(θS) ;
RRFv ← SIS-Verify(A) ;
(P [A(cid:48)
RRF(cid:48) ← CLOPA(P [A(cid:48)
θS ← (cid:0)P [A(cid:48)
while RRFv < RRF(cid:48);
return A, θS ;

S], P [A(cid:48)
BS], RRF(cid:48)(cid:1) ;

S], P [A(cid:48)

BS]) ← SIS-SecCycle(A) ;

BS], θB) ;

One question is how can we choose the initial values
for RRF , P [AS], P [ABS]? This initial design point could
be selected with the aid of the design contour plot as in
ﬁgure 3, where the design point strikes a balance between
security and reliability. What is reasonable regarding the risk

B. Integrated Safety-Security Lifecycle

As the analysis in this paper shows a clear coupling between
safety and security design requirements, we propose the inte-
grated lifecycle in Figure 5. In the following, We present a

CLOPAStartSIS DesignSIS VerificationSIS Security Risk AssessmentCLOPAYesEndNo(0.003, 0.0426, 500)= 600(0.004, 0.03)= (0.004, 0.03, 643)= 643brief description of the lifecycle steps in the order of their
execution, according to the numbering labels in Figure 5.

2

Asset ID

(C, H)

1

HAZOP

1 SIS Safety Lifecycle - (HAZOP): The ﬁrst step is to
carry the hazard analysis for the physical system, often using
HAZOP. This process identiﬁes important assets that may be
subject to, or contribute to, risk scenarios. Then, the process
identiﬁes all feasible hazards and associated risk ranking, as
well as the associated cyber components for each identiﬁed
hazard. This constitutes an input to the BPCS security lifecy-
cle. If we designate the set of hazards by H, and the set of
cyber components by C, then the output from this process is
the function f : H (cid:55)→ R representing the risk ranking, and the
relation R ⊆ H × C representing the cyber components for
each hazard.
2 BPCS Cyber Security Lifecycle: The BPCS cyber security
lifecycle, including vulnerability analysis, attack tree gener-
ation, penetration testing, and risk assessment, is performed
on the BPCS. Ideally, the security risk assessment should
be carried out for each process hazard scenario identiﬁed
during HAZOP to identify the relevant vulnerabilities that may
cause a process disruption. However, for the given centralized
architecture, the BPCS is typically controlling a large number
of control loops, hence it may not be necessary to repeat
the security risk assessment process for each control loop,
as vulnerabilities may be applicable to several hazardous
scenarios. The output of this process is the BPCS security
failure probabilities P [AB] and P [ASB].
3 SIS Safety Lifecycle - CLOPA and SIS Design: The ﬁrst
iteration of CLOPA and SIS design will proceed according to
Algorithm 1 and Figure 4. The CLOPA calculates the design
requirement for the SIS in terms of its reliability as deﬁned
by the RRF, and its cyber security resilience as deﬁned by
P [AS] and P [ABS]. The SIS design then proceeds according
to IEC 61511 standard [4] to produce an architecture A. The
design includes the hardware architecture, redundancy scheme,
and software architecture. The speciﬁc design architecture can
vary across industries and organizations, but the design has to
achieve the required RRF, P [AS] and P [ABS], as calculated
by CLOPA. After the design is completed, SIS veriﬁcation
is carried out to calculate the risk reduction factor RRFv. It
should be highlighted that the SIS is one component only of
the Safety Instrumented Function (SIF). The SIF includes the
sensor, SIS, and the actuator. Therefore, the veriﬁcation is
carried out on the whole SIF. For a detailed discussion on
SIS design and veriﬁcation, the reader is referred to [3].
4 SIS Cyber Security Lifecycle: Using the resulting SIS
design hardware and software architecture A, the SIS security
lifecycle is carried out. Since the SIS is not yet implemented
at this stage, SIS penetration testing is not possible and hence
omitted from the security lifecycle. The output from this pro-
cess is the SIS security failure probabilities P [A(cid:48)
BS],
derived from SIS vulnerabilities that may lead to a process
hazard. It is noted that the SIS security lifecycle at the right of
ﬁgure 5 proceeds from bottom to top for a better presentation.
5 Safety Lifecycle - CLOPA: The CLOPA calculation is car-
ried out using the values obtained from the safety veriﬁcation
and SIS security lifecycle, (P [A(cid:48)
BS], RRFv), to verify

S], P [A(cid:48)

S], P [A(cid:48)

8

4

]

S B

] , P [ A (cid:48)

P [ A (cid:48)

B

Risk Assess.

Pen. Testing

3 5

CLOPA

SRS

Attack Trees

Vuln. ID

Attack Trees

]
B

],P[A S

P[A B

Pen.Testing

Design

Vuln. ID

A

Risk Assess.

6
Installation

Asset ID

BPCS Security Lifecycle

SIS Safety Lifecycle

SIS Security Lifecycle

Figure 5.
Integrated Safety and Security lifecycles. The process starts at
(1) HAZOP, followed by (2) BPCS complete security lifecycle, then (3) SIS
safety lifecycle up to the end of the design stage, followed by (4) complete
SIS security lifecycle, (5), CLOPA check, and possibly several iterations of
steps (3) (4), and (5), then terminates at the SIS installation stage.

that the architecture A satisﬁes the CLOPA constraint. The
process SIS safety lifecycle → SIS Cyber security lifecycle
→ CLOPA (designated by the blue arrowed arc in ﬁgure (5))
repeats until the CLOPA constraint is satisﬁed.
6 Installation: The ﬁnalized design then moves to the instal-
lation phase.

V. INTEGRATED DESIGN EXAMPLE

In this section, we present an integrated design example for
a process control system to illustrate the proposed CLOPA
and integrated lifecycle. The system described in this section
is a real testbed located in Qatar University, and comprises
the process simulator and the full plant control system. As
the integrated design lifecycle is substantial, with some steps
outside the scope of this work (e.g., SIS architectural design
and security risk assessment), it is not possible to present the
design process in full details. However, we try to focus on the
big picture as related to the proposed CLOPA, while discussing
brieﬂy each design step. Wherever needed, we refer the reader
to relevant references for further details.

A. CPS Description

We consider the Continuous Stirred Tank Reactor (CSTR)
process illustrated in Figure 6. The reactor vessel has an
inlet stream carrying the reactant A, an outlet stream carrying
the product B, and a cooling stream carrying the cooling
ﬂuid into the surrounding jacket to absorb the heat of the
exothermic reaction. A ﬁrst order reaction takes place where
a mole fraction of reactant A is consumed to produce product
B. The process has a level control loop (LT-01 → BPCS
→ CV-01) to maintain the liquid level in the reactor, and
a temperature control loop (TT-01 → BPCS → CV-02) to
control the reaction rate. The SIF (LT-02 → SIS → SDV-
01) protects the reactor from the overﬂow hazard, and will be
explained later in this section. For more detailed explanation
about the process including the state space model, the reader
is referred to [12].

9

Figure 6. Reactor Piping and Instrumentation Diagram (P&ID). ISA standard
symbols are not strictly followed for illustration purposes.

The CSTR process is controlled by the industrial control
system shown in Figure 7, which follows NIST 800-82 stan-
dard with one ﬁrewall and a DeMilitariZed (DMZ) zone [13].
The corporate network cannot communicate directly with the
control network. The only allowable information ﬂow paths via
the ﬁrewall are from the control network to the data logging
servers in the DMZ zone, and from the corporate network to
the DMZ for information retrieval. The BPCS and SIS have
Modbus/TCP communication over the control network [6].

B. Integrated Lifecycle

In the following discussion, we follow the integrated life-
cycle in Figure 5, and as per the itemized steps in section
IV-B
1 SIS Safety Lifecycle - HAZOP: Table II shows the
HAZOP sheet for the CSTR process. Each row contains:
(1) the possible hazard, (2) all possible initiating events
for each hazard whether mechanical or electronic failures,
(3) consequences if the hazard occurred,
including safety,
ﬁnancial, and environmental losses, (4) existing safeguards
that could prevent the hazard from propagating and causing
the consequences, and (5) the risk rank, which is typically a
function of the consequences. There are two identiﬁed hazards
for the reactor process; high level causing an overﬂow hazard,
and high temperature that may lead to reactor runaway and
possible meltdown. Both hazards have high and very high risk
rankings, therefore, the two risk scenarios qualify for further
LOPA assessment. In the following, we limit our discussion to
the high-level hazard scenario only. High temperature hazard
could be treated similarly.
2 BPCS Cyber Security Lifecycle: We need to calculate
P [AB] and P [ASB] for the BPCS, the probability that the
BPCS fails due to a direct attack and a SIS-pivot attack,
respectively, in a way that generates the high level process
hazard. We conducted vulnerability identiﬁcation on the CPS
network in Figure 7, constructed the attack trees, and carried
out penetration testing to verify the vulnerability ﬁndings.
We assumed an attacker proﬁle where attacks are selected
randomly. The total number of semantically-relevant attacks
is found to be |Ar| = 42. We assume that relevant attacks

Figure 7. CPS architecture for an industrial control system testbed, following
NIST 800-82 guidelines. The ﬁrewall blocks any direct communication be-
tween the corporate network and the control network. Plant ﬂoor information
is accessible only via the DMZ

represent 10−4 of all possible attacks, i.e., |Ar|/|A| = 10−4.
Therefore, according to Lemma III.1, we obtain an initiating
likelihood 10−4λ and a weight factor γa = 0.024
event
for each attack a at the leaf nodes of the attack tree. As
the full details of vulnerability analysis, attack design, and
penetration testing are beyond the scope of this paper, we refer
the interested reader to [10], [14].

To compromise the BPCS, we assume the more realistic
situation with no insider threat and no direct communication
from the corporate network to the control network. In this
scenario, the attacker has to detour to compromise the Real
Time (RT) server in the DMZ and use it as a pivot to attack
the BPCS, either directly or via the monitoring workstation
(designated HMI hereafter) that has legitimate communication
with the BPCS. We start with the assumption that one of the
corporate network PCs that has legitimate access to the RT
server is compromised. There are several well-known attack
vectors in the IT security domain to achieve such compromise,
such as a spam email, a web service vulnerability, or an
external malware USB, just to name a few. Figure 8 is an
abstract attack tree that summarizes the BPCS compromise
paths where the database server compromise is a pre-requisite
attack step. In the following, we expand each of the leaf nodes
in this abstract attack tree into the corresponding detailed
attack trees. More detailed treatment of each attack tree as
well as penetration testing could be found in [10].

Figure 9 shows the RT server attack tree. The basic idea
is to exploit mysql database vulnerabilities via SSH to obtain
the Linux server password Hash Dump and possibly crack the
password to achieve privilege escalation and gain full control
over the RT Server. The probability of success of such an
attack depends on several factors including mysql conﬁgu-
ration settings to allow brute-force login attack, mysql login
password strength, conﬁguration of mysql security monitoring

TT-01LT-01LT-02SISHV-02SDV-01CV-01CV-02BPCSR-100InletOutletCoolantSIF: LT-02 -> SIS -> SDV-01SISBPCSMonitoring WSProgramming WSReal Time Data ServerHistorian ServerHazard

Initiating Event (Cause)

Consequences

High Level
overﬂow)

(Reactor

Temperature

High
(Reactor
Meltdown/explosion)

BPCS failure OR Human error (mis-
aligned valves)

BPCS failure OR Coolant inlet control
valve fully (partially) closed OR Inlet
valve stuck fully open

2 or more fatalities (safety), Product loss (ﬁ-
nancial), Environmental contamination (en-
vironment)
10 or more fatalities
(safety), Product
loss (ﬁnancial), Environmental contamina-
tion (environment)
Table II
PARTIAL HAZOP SHEET FOR THE REACTOR PROCESS

10

Safeguards
(IPL)
Reactor
(Mitigation)

Risk Rank

dike

High

None

V. High

Reactor
overﬂow
Hazard

0.033

BPCS direct
compromise

BPCS indirect
compromise

0.003

0.03

RT server
compromise

0.1

Crack server
hashed pass

DB elevated
access

mysql
Apparmor
disabled

BPCS direct
attack

RT server
attack

BPCS indirect
attack

RT server
attack

Remote
connection

mysql
privileged
access

Figure 8. Abstract attack tree to compromise the BPCS to generate overﬂow
process hazard for the CSTR reactor. Leaf nodes are further expanded in
Figures 9, 10, and 11.

Remote
desktop
enabled

SSH enabled

Brute
force login

Password
crack

app, and whether SSH is enabled. For the purpose of this case
study, we choose this probability arbitrarily as 0.1. It should
be highlighted that the attack tree does not have the sequence
semantics to represent a sequence of attack steps. For example,
the remote connection step has to be executed before the mysql
privileged access in Figure 9. We represent this sequence by
the and gate aggregator, noting that in some other cases the
and gate may represent simultaneous attack steps. For more
information on attack trees and their semantics, the reader is
referred to [15].

Figure 10 shows the BPCS attack tree, which is divided
into two main parts; DoS attack and integrity attack. The DoS
attack may not lead to a reactor overﬂow unless there is a
concurrent process disturbance that could not be controlled
with the DoS-induced delayed BPCS control response. The
probability of such disturbance could be estimated from plant
information. The integrity attack injects a low level measure-
ment value for LT-01 to drive the BPCS controller to increase
valve CV-01 opening, or directly forces control valve CV-01 to
open 100%. This will cause a reactor overﬂow if the SIS is not
activated. The injection of the malicious value in the control
loop could be accomplished by either gaining access to the
controller and overwriting the control program, or more simply
sending Modbus packets to the controller with the malicious
values. Modbus attack is much easier to launch but requires
conﬁguration data to identify the Modbus register address for
either LT-01 or CV-01. The probability of BPCS indirect attack
is chosen arbitrarily as 0.03.

Finally, Figure 11 shows the attack tree for BPCS attack
via the HMI. The attack is launched by remote desktop
connection to the HMI and legitimately controlling CV-01 via
the GUI. This indirect attack is easier than targeting the BPCS
directly as it does not require knowledge about the controller

Bypass
mysql au-
thentication

Crack
hashed sql
password

Figure 9. RT Server attack tree. Database vulnerabilities are exploited to gain
root access and use the RT server as a pivot to attack the control network.
DB elevated access is a prerequisite to crack the server hashed password.
Leaf nodes that represent a distinct attack have a weight factor γa = 0.024
that is combined with their success probability. Therefore, Bypass mysql and
Crack hashed sql password, Brute force login, SSH enabled, Remote desktop
enabled, each has a weight factor combined with their success probability.

conﬁguration or Modbus register addresses associated with the
sensor and valve of the targeted control loop. This is because
all the information is already programmed in the GUI software.
The probability of BPCS indirect attack is estimated to be 0.3.
Using Figure 8 and the three presented attack trees in Figures
9, 10, and 11, the total probability of BPCS attack that leads
to an overﬂow hazard could be estimated by P [AB] ≈ 0.033.
It should be highlighted that the assignment of a probability
measure to the success of attack actions is subject to debate
in the research community, and there is no published agreed-
upon data as in the case of reliability failure data. One
approach is to use attack databases, such as NIST National
Vulnerability Database (NVD) [16], to estimate the probability
of a cyber attack success based on attributes such as required
knowledge level and attack difﬁculty. However, this approach
has the drawback that
the
it does not
speciﬁcs of each organization. In this work, we rely on the
experience obtained during the penetration testing carried out
by the research team in combination with NVD to assign the
probability measures. This does not impact the analysis as
the presented case study is meant for illustration purposes to
explain the design process.

take into account

11

BPCS direct
attack

0.03

DoS attack

Integrity Attack

Controller
under DoS

Process
disturbance

Control program
overwrite

Modbus
register write

Modbus
STOP attack

Controller down

Run malicious
process

I/O access

Random write hit

Educated write

BPCS
indirect
attack

0.3

HMI access

GUI injection

Shutdown
controller

Crack password

I/O access open

Crack password

Get control conﬁg

Modbus reg. write

Password
leak (insider)

Password
crack

Process
knowledge

GUI software
knowledge

No max passwd
conﬁgured

Brute-force
SSH login

Figure 10. Attack tree for BPCS compromise to generate a reactor overﬂow hazard. A DoS attack
synchronized with a process disturbance or a specially-crafted integrity attack would cause the CSTR to
overﬂow. Leaf nodes that represent a distinct attack have a weight factor γa = 0.024 that is combined
with their success probability

No RDP
lockout

Brute force
RDP pass
attack

Figure 11. HMI-BPCS indirect attack tree.
The compromised HMI is used to embed the
attack against the BPCS using the legitimate
trafﬁc between the GUI and the BPCS control
program. Leaf nodes that represent a distinct
attack have a weight factor γa = 0.024 that is
combined with their success probability

SIS-BPCS
attack

0.28

DoS attack

Integrity
attack

Unsupported
Modbus FC

Modify SIS
program

Process dist.

Random
Modbus
injection

Educated
Modbus
injection

Figure 12. SIS-BPCS attack tree. The SIS is used as a pivot to launch either
a DoS attack or a crafted integrity attack that results in reactor overﬂow. Leaf
nodes that represent a distinct attack have a weight factor γa = 0.024 that
is combined with their success probability.

To calculate the probability of BPCS cyber attack leading
to a process hazard given a SIS cyber compromise P [ASB],
we focus on Modbus attack vectors for both integrity and DoS
attacks. Integrity attacks target sensor LT-01 or valve CV-01
as before, either randomly or using leaked Modbus register
conﬁguration. DoS attack could be launched by utilizing non-
programmed Modbus function code hoping that it would crash
the BPCS Modbus master. Figure 12 summarizes the attack
tree, and the probability is chosen arbitrarily as P [ASB] ≈
0.2813. To summarize, the desired outcome from the BPCS
security lifecycle is (P [AB], P [ASB]) = (0.033, 0.2813).

It should be noted that complete attack trees for the given
BPCS and CPS architecture could span multiple pages. How-
ever, full attack trees may obscure the analysis and will
serve no additional insight. Therefore, the simpliﬁed attack
trees presented here act as a better illustration of the design
methodology. For more in-depth treatment of the cyber risk
assessment for the presented case study, refer to [10].
3 SIS Safety Lifecycle - CLOPA: Table III shows the
LOPA sheet for the CSTR overﬂow hazard identiﬁed from

the HAZOP, where the initiating event likelihoods and failure
probabilities are adopted from [17], [18]. The BPCS cyber
attack likelihood is calculated as λc = 10−4λ = 0.01
/yr, assuming λ = 100 /yr. Note that human intervention
is considered a protection layer assuming there is sufﬁcient
time for the operation team to manually isolate the reactor
in the ﬁeld. Some conservative approaches omit any human
intervention or safety procedure from the LOPA.

From the LOPA sheet, we extract the event likelihood values
to calculate the CLOPA model parameters using equations
(10) - (12) and (14) - (16), along with (P [AB], P [ASB]) =
(0.033, 0.2813) from the BPCS security lifecycle. Table IV
summarizes the parameter values. Substituting in the CLOPA
constraint (13), we obtain:

P [Sp] ≤

1 − 148.68P [AS] − 7.6P [ABS](1 − P [AS])
117(1 − P [AS]) − 7.6P [ABS](1 − P [AS])

(27)

Our objective now is to design a SIF with architecture A
that satisﬁes (27) in order to achieve the required process
safety objective as deﬁned by the TMEL in the LOPA anal-
ysis. Our initial design for the SIF will comprise a level
sensor (LT-02), a logic solver (SIS), and a shutdown valve
(SDV-01), as illustrated in Figure 6. The SIF will take an
independent action upon reactor overﬂow and will close the
inlet shutdown valve. The architecture of the SIF could vary
through design iterations to achieve the required safety. As an
example, sensors may be duplicated or sometimes triplicated to
achieve higher reliability, and the SIS architecture may include
redundant CPU modules. We note that for a perfectly-secured
SIS (P [AS] = P [ABS] = 0), P [Sp] ≤ 1/117, or equivalently
RRF ≥ 117. This is the minimum achievable RRF. Since
for practical systems there is no zero probability of cyber
security attack failures, our SIS design is expected to have
an RRF > 117.

Initiating Event

Likelihood λi (/yr)

Tank Dike

Inlet ﬂow surge
Downstream ﬂow blockage
Manual valves misalignment
BPCS physical Failure
BPCS attack Failure

10−1
10−1
10−1
10−1(λb)
10−2(λc)

10−2
10−2
10−2
10−2
10−2

Safety
Procedure
1
10−1
10−1
1
1

Human Interven-
tion
10−1
10−1
10−1
10−1
10−1

BPCS
(P [Bp])
10−1
10−1
10−1
1
1

TMEL

10−6
10−6
10−6
10−6
10−6

Table III
LOPA SHEET FOR THE CSTR OVERFLOW HAZARDOUS SCENARIO. NUMBERS IN EACH CELL REPRESENT THE PROBABILITY OF FAILURE OF THE
ASSOCIATED PROTECTION LAYER

12

LOPA Pa-
rameter
Σ3
i=1λi
P [L]
λb
λc
P [Bp]
α1
α2
β
γ1
γ2
γ3

Value

Source

0.3
0.001
0.01
0.01
0.01
1.13 × 10−4
1.17 × 10−4
10−6
1.4868 × 10−4
7.5785 × 10−6
1.1686 × 10−4

LOPA Sheet
LOPA Sheet
LOPA Sheet
LOPA Sheet
LOPA Sheet
CLOPA Parameter-Calculated Eq. (10)
CLOPA Parameter-Calculated Eq. (11)
CLOPA Parameter-Calculated Eq. (12)
CLOPA Parameter-Calculated Eq. (14)
CLOPA Parameter-Calculated Eq. (15)
CLOPA Parameter-Calculated Eq. (16)

Table IV
CSTR CLOPA - CALCULATED PARAMETER VALUES

Using the calculated LOPA parameter values, the design

region (18) and boundary (19) are deﬁned by:

P [ABS] ≤ 0.132

(cid:18) 1 − 148.68P [AS]
1 − P [AS]

(cid:19)

(28)

where the design boundary is deﬁned when the equality holds.
The contour lines for the RRF in (20) are deﬁned by:

P [ABS] =

(cid:18) 15.42
C − 1

(cid:19) (cid:18) (C − 0.008) − (C − 1.27)P [AS]

(cid:19)

1 − P [AS]

(29)

for different values C of the RRF. The design region as well as
the contour lines are plotted in Figure 13. We note that as we
approach the design boundary, either by increasing P [AS] or
P [ABS], the RRF rapidly increases such that it is not possible
to plot the contour lines in this region in a visible way. The
design in this region is very sensitive to input variations (i.e.,
a very small variation in probabilities will result in a very
large change in RRF). Therefore, the design point should be
selected as far as possible from the design boundary. To further
illustrate the increase in RRF, Figure 14 is a 3D plot for the
RRF as it varies with both P [AS] and P [ABS]. It should
be evident from the 3D plot that for small values of P [AS],
The function gradient is smaller, resulting in a less-sensitive
design to probability variations. At larger values of P [AS]
near the design boundary, the RRF increases exponentially
with P [ABS]. These results could be veriﬁed by calculating
the gradient of (13).

To proceed with the design process, we pick the point
P [AS] = 0.003 as a reasonable probability value for SIS
direct attack failure that is away from the steepest ascent
region in Figure 14. We now need to choose a practical value
of P [ABS] that results in an achievable target RRF. With
the help of Figure 13 and contour lines, P [AS] = 0.003

intersects the contour line for RRF = 500 at P [ABS] = 0.0426.
Alternatively, the value of P [ABS] could be obtained from
(29) by setting C = 500 and P [AS] = 0.003. The design
point (0.003, 0.0426, 500) is indicated in Figure 13 and 14.
The design and veriﬁcation of the SIF then resumes according
to IEC 61511 to develop an architecture A that satisﬁes the
combined CLOPA requirement: RRF ≥ 500, P [AS] ≤ 0.003,
and P [ABS] ≤ 0.0426. The detailed design and veriﬁcation
of SIF are outside the scope of the paper (refer to to [4] for
more details). To complete the case study, we will assume
the design engineer came up with an architecture A that was
veriﬁed using vendor data, resulting in reliability RRFv = 600,
with a design margin from the required RRF = 500.
4 SIS Cyber Security Lifecycle: The resulting SIF architec-
ture A is used to carry out the SIS security lifecycle, similar
to the BPCS security risk assessment in step 2 of the design
process. As the SIS detailed design and veriﬁcation is not
in the scope of the paper, we will assume for the sake of
illustration that the architecture A results in a cyber system
conﬁguration that has a higher probability of SIS security
attack failure P (cid:48)[AS] = 0.004 while reducing the BPCS pivot
attack failure probability to P (cid:48)[ABS] = 0.03 via securing the
BPCS-SIS link.
5 Safety Lifecycle - CLOPA: The architecture A results in
P (cid:48)[AS] = 0.004, P (cid:48)[ABS] = 0.03, and RRFv = 600. We need
to verify if these values satisfy the CLOPA constraint (27).
Plugging the probability values results in P [Sp] ≤ 1.54×10−3,
or equivalently RRF ≥ 643. As RRFv = 600 < 643, the
architecture has to be modiﬁed, either by reducing further
the cyber attack failure probabilities, or by increasing the
system reliability via fault tolerance techniques. It may take the
design engineer multiple iterations until the design achieves
the CLOPA constraint. In practice, the iterations do not involve
a complete architectural redesign, but rather changing the
redundancy scheme or security hardening in order to achieve
the design objective. To conclude the case study example, we
will assume the design engineer came up with an architecture
that preserves the aforementioned probability values while
increasing the RRF to 650. This concludes the design process
and the system moves to the implementation phase. The case
study design values are superimposed on the iterative design
(cid:4)
process in Figure 4 as an illustration.

C. Classical LOPA Error

Classical LOPA ignores cyber attack probabilities alto-
gether. For the given problem, it results in RRF = 113 as
per (17). The minimum CLOPA RRF occurs for a perfectly

13

Figure 13. CSTR Case Study: CLOPA design region with contour plot
for the Risk Reduction Factor (RRF).

Figure 14. CLOPA RRF as it varies with SIS security failure prob-
abilities. Steepest ascent region to the right should be avoided when
selecting the operating point.

D. Sensitivity Analysis

Calculating the probability of a security failure is a debat-
able subject in the research community, especially with lack
of statistical data that is available for physical failures. One
question that comes to mind is the robustness of the developed
CLOPA model
to probability variations. We conducted a
numerical analysis to calculate the partial derivatives of the
RRF with respect to P [AS], P [ABS].The two partial derivative
plots are very similar to Figure 14 and omitted for space
limitation. For small probability values, the change in the
RRF is in the range of 15% for 10−3 change in P [AS]. As
probabilities increase and we approach the decision boundary,
the change in RRF jumps to around 80% for 10−3 change
and increases exponentially as we get closer to the decision
boundary. A similar behavior is exhibited with P [ABS] change
(ﬁgure omitted for brevity). However, the change in RRF has
much lower percentage, ranging from 7% for small probability
values, and increasing to around 37% as we approach the
decision boundary. We highlight
the following three key
observations: (1) For small cyber failure probability values,
the model sensitivity is acceptable since the SIL levels have an
order of magnitude ratio, so a small percentage change would
likely keep the system requirement in the same SIL category.
However, this requires that the probability error is in the range
of 10−3. (2) The model is more sensitive to direct attack failure
probabilities than BPCS pivot attacks. (3) We should always
try to design our system as far as possible from the decision
boundary. The model sensitivity with respect to probability
changes increases as we approach the decision boundary.

VI. RELATED WORK

HAZOP has been the dominant risk assessment method
for the process industry for over 30 years [5], [19], [20].
LOPA has been used in conjunction with HAZOP to design
Safety Instrumented Systems (SIS) and specify the Safety
Integrity Level (SIL) for each Safety Instrumented Function
(SIF) [21]. Because of the wide adoption of LOPA by industry
due to its systematic approach and quantitative risk assessment

Figure 15.
ﬁxed value indicated for P [AS ]

Increase of RRF with P [ABS ]. Each curve corresponds to the

secured safety system where P [AS] = P [ABS] = 0, achieving
RRF = 117. Therefore, the minimum error between LOPA
and CLOPA RRF estimation is 4. The error gets worse as
security failure probabilities increase. For the given design
point P [AS], P [ABS] = (0.003, 0.0426), the classical LOPA
error is eRRF = 378. This is a signiﬁcant amount of error that
results in the design of a less reliable system that will not
achieve the target risk level. Figure 15 better illustrates the
error increase with increasing the security failure probability
P [ABS] for different values of P [AS]. For small values of
P [AS], the curves show slow increase in RRF with P [ABS].
As P [AS] increases, the RRF increase becomes exponential.
A similar contour ﬁgure for ﬁxed P [ABS] values could be
generated. The design point for the case study P [AS] = 0.003
was chosen as a trade-off between an achievable cyber attack
probability value and a moderate rate of increase for the
RRF. The 3D plot for the error in RRF vs P [AS], P [ABS] is
identical to Figure 14, except by shifting down the 3D curve
by 113, the LOPA RRF value, therefore it is omitted to avoid
repetition.

capability, LOPA has been included as one of the methods in
IEC 61511-3 standard with several illustrating examples [4].
The LOPA approach has been applied to physical security risk
analysis in [22]. However, to the best of author’s knowledge,
there is no research work on integrating security attacks in the
LOPA framework for safety instrumented systems design.

There are emergent standardization initiatives to address
safety and security coordination in cyber physical systems.
IEC 62443-4-1 (Security for industrial automation and control
systems - Part 4-1: Secure product development
lifecycle
requirements) is a standard developed by ISA-99 committee
with the purpose to extend existing safety lifecycle at different
phases to include security aspects to ensure safe CPS design
[23]. IEC TC65 AHG1 is a recently formed group linked to
the same technical committee developing IEC 61508 and IEC
62443 to consider how to bridge functional safety and cyber
security for industrial automation systems [24]. IEC 62859
(Nuclear power plants - Instrumentation and control systems -
Requirements for coordinating safety and cyber security) is
a standard derived from IEC 62645 for the nuclear power
industry to coordinate the design and operation efforts with
respect to safety and cyber security [25]. DO-326 (Airwor-
thiness Security Process Speciﬁcation) is a standard for the
avionics industry that augments existing guidelines for aircraft
certiﬁcation to include the threat of intentional unauthorized
electronic interaction to aircraft safety [26]. A taxonomy of
dependable and secure computing is introduced in [27] in
order to facilitate the communication among different research
communities. The concepts and taxonomy presented are a
result of a joint committee on Fundamental Concepts and
Terminology that was formed by the TC on Fault-Tolerant
Computing of the IEEE CS and the IFIP WG 10.4 Dependable
Computing and Fault Tolerance. A preliminary work on the
research in this paper that combines the two research directions
stated below is presented in [28].

A. Lifecycle Integration

The authors in [29] use fault tree analysis to combine both
safety and security failures in one uniﬁed risk assessment
framework for the aviation industry. The outcome of the
risk assessment is used to deﬁne both safety and security
requirements. A road-map for cyber safety engineering to
increase air trafﬁc management system resilience against cyber
attacks is proposed in [30]. The V-shaped model to develop
embedded software for CPS is augmented with security actions
in [31]. The integration of IEC 61508 safety standard and IEC
15408 for IT security is described in [32]–[34] for building
automation systems. The author in [35] describes in more
details the integration of IEC 61508 safety lifecycle and the
CORAS approach to identify security risks [36]. An approach
to align safety and security during different stages of system
lifecycle is proposed in [37]. The approach,
development
called Lifecycle Attribute Alignment, ensures compatibility
between safety and security controls developed and maintained
during the system development lifecycle. HAZOP, a predom-
inantly used method for safety risk assessment in the process
industry, is modiﬁed in [38] to include security failures. The

14

authors introduce new guide words, attributes, and modiﬁers
for security components akin to traditional HAZOP limited to
safety failures. Failure Mode and Effect Analysis (FMEA) is
extended in [39] to include security vulnerabilities, suggesting
the name Failure Mode Vulnerability and Effect Analysis
(FMVEA). For a survey on the integration of safety and
security in CPS, refer to [2].

B. Model-Based Risk Assessment

Several graphical methods have been used to combine safety
and security analysis. Goal Structuring Notation (GSN) is a
graphical notation used to model requirements, goals, claims,
and evidence of safety arguments [40]. The SafSec research
project for the avionics industry elaborate on the use of GSN to
integrate both safety and security arguments in one represen-
tation [41]. A similar approach is used in [42] where authors
apply the Non Functional Requirement (NFR) approach to
quantitatively assess the safety and security properties of an
oil pipeline CPS. NFR is a technique that allows simultaneous
safety and security graphical representation and evaluation at
the architectural level.

The simplicity and wide adoption of fault and attack trees
promoted the research work to merge both modeling tools. The
integration of fault trees and attack trees is considered in [43]
in order to extend traditional risk analysis to include cyber
attack risks. A quantitative analysis is proposed by assigning
probabilities to tree events. Similarly, fault tree analysis is used
in [44] to analyze safety/security risks in aviation software.
In [45], the authors extend Component Fault Trees (CFT) to
contain both safety and security events. Both qualitative and
quantitative analysis is performed to assess the overall risk.
The quantitative analysis is enabled by assigning probabilities
to safety events and categorical rating (low, medium, high)
for security events. The authors in [46] translate the com-
bined fault-attack tree into stochastic time automata to enable
quantitative risk analysis. The use of Bow-tie diagrams and
analysis in place of fault trees is reported in [47], where it is
integrated with attack trees for combined safety-security risk
assessment.

Given the limited semantics of fault trees, Boolean logic
Driven Markov Process (BDMP) graphical formalism intro-
duced in [48] has been used to integrate safety and secu-
rity events. The approach integrates fault trees with Markov
process at the leaf nodes level and associates a mean time
to success (MTTS) for security events and a mean time to
failure (MTTF) for safety events. This allows both a qualitative
and a quantitative risk assessment for the given system. The
formalism also enables the modeling of detection and response
mechanisms without a need for model change. The work in
[49] applies BDMP formalism to a pipeline case study, illus-
trating different types of safety-security inter-dependencies. In
[50], Stuxnet attack is modeled using BDMP and a quantitative
risk analysis is carried out on the industrial control system.

Petri nets have also been proposed to overcome the limi-
tations of fault trees. A formalism for safety analysis named
State/Event Fault Trees (SEFTs) is reported in [51]. In this
formalism, both deterministic state machines and Markov

chains are combined, while keeping the visualisation of causal
chains known from fault trees. This formalism is extended in
[52] to include an attacker model to deal with both safety
and security. Similarly, stochastic Petri nets have been used in
[53] to model the impact of intrusion detection and response
on CPS reliability, and in [54] to assess the vulnerabilities in
SCADA systems. Bayesian belief networks are also considered
as one of the model-based approaches. In [55], a Bayesian
Belief Network is used to assess the combined safety and
security risk for an oil pipeline example.

The Uniﬁed Modeling Language (UML) commonly used
in software engineering has also been used for safety and
security risk assessment. Misuse cases for UML diagrams have
been used to deﬁne safety requirements in [56] and security
requirements in [57], independently. A combined process for
Harm Assessment of Safety and Security has been proposed
in [58] based on both UML and HAZOP studies. UMLsafe
[59] and UMLsec [60] are two UML extensions that enable
modeling of safety and security requirements, respectively.
The combined UMLsafe/UMLsec is proposed in [61] for
safety-security co-development. SysML-sec, a SysML-based
model driven engineering environment, is used in [62] for the
formal veriﬁcation of safety and security properties.

System Theoretic Process Analysis (STPA) was developed
as a new hazard analysis technique to evaluate the safety of a
system [63]. The authors in [64] extend the STPA to include
system security aspects in the analysis. The expanded approach
is named STPA-SafeSec and demonstrated on a use case in the
power grid domain. The System Theoretical Accident Model
and Process (STAMP) is applied to the Stuxnet attack in [65],
showing that the attack could have been avoided if STAMP
was applied during design time.

VII. CONCLUSION
Classical safety assessment methods do not take into ac-
count failures due to cyber attacks. In this paper, we showed
quantitatively that overlooking security failures could bias
the risk assessment, resulting in under-designed protective
systems. In addition, the design of safety and security subsys-
tems for complex engineering systems cannot be carried out
independently, given their strong coupling as demonstrated in
this paper. Although the design becomes more complicated
when considering cyber attacks,
the development of new
software tools or the modiﬁcation of existing industrial tools
could automate the process.

In this work, we considered the control system (BPCS)
design as given, following common industrial practice. Joint
optimization of both BPCS and SIS designs, from both safety
is a potential extension for the
and security perspectives,
presented work. Also, the presented integrated lifecycle relies
in part on designer’s experience to make design decisions
to achieve the system requirements. Optimal system design
that captures possible safety and security design choices with
associated ﬁnancial cost could provide a better quantitative
approach to ﬁnd the optimal system operating point rather
than relying on design heuristics. Furthermore, the integration
of both the safety and security lifecycles into model-based
design toolchains is crucial for adoption by industry.

15

Finally,

the work presented in this paper discusses the
impact of cyber security failure on system safety. A closely-
related problem is how safety failures could impact cyber
security. There is not much work in this direction, perhaps
because the focus in cyber physical systems is always on
safety, considering the security of the cyber system as a
secondary issue. Nevertheless, this is an important problem.
On one hand, a simple safety failure may be injected to cause a
security compromise that may be exploited to produce a higher
security compromise that could lead to a greater safety hazard.
On the other hand, both directions, i.e., Safety → Security
and Security → Safety, are closely related and interacting,
and therefore optimizing a cyber physical system performance
with respect to safety/security or both cannot be fully achieved
without understanding the two types of interactions.

APPENDIX A
PROOF OF LEMMA III.1

Proof. The aggregate likelihood of all attacks to cause a
hazard taking into account BPCS and SIS protection could be
approximated by (neglecting higher order probability terms):

Λ = λ

(cid:88)

a∈Ar

αaPa[S, B]

Using (8) to expand the joint probability:

(cid:88)

Λ = λ

αa (η1 + η2P [Ba

c ] + η3P [Sc]P [Ba

c |Sc])

(30)

(31)

a∈Ar
where P [Ba
c ] represents the probability of BPCS security
failure with respect to attack a, and η1, η2, η3 are probability
terms not dependent on the attack a. Expanding:

(cid:32)

(cid:88)

(cid:33)

αa

×

Λ = λ

(cid:32)

a∈Ar

η1 + η2

(cid:88)

a∈Ar

γaP [Ba

c ] + η3

(cid:88)

a∈Ar

(32)

(cid:33)

γaP [Sc]P [Ba

c |Sc]

(33)

where γa = αa/ (cid:80)
ties:

a∈Ar

(cid:32)

(cid:88)

(cid:33)

αa

×

Λ ≈ λ

αa. Ignoring higher order probabili-

(34)

a∈Ar

(cid:32)

η1 + η2P

(cid:34)

(cid:88)

a∈Ar

(cid:35)

(cid:34)

γaBa
c

+ η3P

Sc,

(cid:35)(cid:33)

γaBa
c

(35)

(cid:88)

a∈Ar

Comparing (31) and (35), the second and third terms in (35)
represent an equivalent BPCS with a combined attack vector
Ar, where each attack a is weighted by γa. In addition, the
(cid:1).
likelihood of this combined attack vector is λ (cid:0)(cid:80)
(cid:4)

a∈Ar

αa

SOURCE CODE

The source code for the CLOPA in the form of Matlab m
ﬁles to regenerate the research results including the case study
are located at https://github.com/Ashraf-Tantawy/CLOPA.git

ACKNOWLEDGMENT

This research was made possible by NPRP 9-005-1-002
grant from the Qatar National Research Fund (a member of
The Qatar Foundation). The statements made herein are solely
the responsibility of the authors.

REFERENCES

[1] S. Kriaa, L. Pietre-Cambacedes, M. Bouissou, and Y. Halgand, “A
survey of approaches combining safety and security for industrial control
systems,” Reliability Engineering and System Safety, vol. 139, pp. 156–
178, 2015.

[2] X. Lyu, Y. Ding, and S. H. Yang, “Safety and security risk assessment
in cyber-physical systems,” IET Cyber-Physical Systems: Theory and
Applications, vol. 4, no. 3, pp. 221–232, 2019.

[3] P. P. Gruhn, Safety instrumented systems: design, analysis, and justiﬁ-

cation. ISA, 2006.

[4] IEC, “IEC 61511-1:2016, Functional safety - Safety instrumented sys-
tems for the process industry sector - Part 1: Framework, deﬁnitions,
system, hardware and application programming requirements,” tech.
rep., IEC, 2016.

[5] J. Dunj´o, V. Fthenakis, J. A. V´ılchez, and J. Arnaldos, “Hazard and op-
erability (HAZOP) analysis. A literature review,” Journal of Hazardous
Materials, vol. 173, no. 1-3, pp. 19–32, 2010.

[6] A. Swales and others, “Open modbus/tcp speciﬁcation,” Schneider

Electric, vol. 29, 1999.

[7] I. Fovino, A. Carcano, T. M. . t. I. . . . , and U. 2010, “Modbus/DNP3
state-based intrusion detection system,” in 24th IEEE International
Conference on Advanced Information Networking and Applications,
2010.

[8] A. Z. Faza, S. Sedigh, and B. M. McMillin, “Reliability analysis for the
advanced electric power grid: From cyber control and communication
to physical manifestations of failure,” in International Conference on
Computer Safety, Reliability, and Security, pp. 257–269, Springer, 2009.
[9] G. Stoneburner, A. Goguen, and A. Feringa, “Risk management guide
for information technology systems :,” National Institute of Standards
& Technology National Institute of Standards and Technology Special
Publication 800-30, 2002.

[10] A. Tantawy, S. Abdelwahed, A. Erradi, and K. Shaban, “Model-Based
Risk Assessment for Cyber Physical Systems Security,” Computers &
Security, 5 2020.

[11] A. P. Moore, R. J. Ellison, and R. C. Linger, “Attack modeling for infor-
mation security and survivability,” Technical Note CMUSEI2001TN001,
vol. 17, no. March, pp. 15–33, 2001.

[12] A. Tantawy, S. Abdelwahed, and Q. Chen, “Continuous Stirred Tank
Reactors: Modeling and Simulation for CPS Security Assessment,”
in The Internationl Conference on Computational Intelligence and
Communication Networks, CICN 2019, 2019.

[13] K. Stouffer, J. Falco, and K. Scarfone, “Guide to Industrial Control
Systems (ICS) Security,” National Institute of Standards and Technology
Special Publication 800-82, p. 164, 2011.

[14] A. Tantawy, “Automated malware design for cyber physical systems,”
in 2021 9th International Symposium on Digital Forensics and Security
(ISDFS), pp. 1–6, IEEE, 2021.

[15] S. Mauw and M. Oostdijk, “Foundations of attack trees,” in Lecture
Notes in Computer Science, vol. 3935 LNCS, pp. 186–198, Springer,
Berlin, Heidelberg, 2006.
[16] NIST, “NVD - Home,” 2016.
[17] SINTEF and NTNU, OREDA Offshore and Onshore Reliability Data

Volume 1 - Topside Equipment. DNV, 2015.

[18] Center for Chemical Process Safety, Guidelines for Initiating Events and

Independent Protection Layers in Layer of Protection Analysis. 2015.

[19] F. Crawley and B. Tyler, HAZOP: Guide to Best Practice. Elsevier,

2015.

[20] B. Skelton, “Hazop and Hazan: Identifying and Assessing Process
Industry Hazards. IChemE, (1999), ISBN: 0 85295 421 2,” 1999.
[21] A. M. Dowell, “Layer of Protection Analysis and Inherently Safer
Processes,” Process Safety Progress, vol. 18, no. 4, pp. 214–220, 1999.
[22] F. Garzia, M. Lombardi, M. Fargnoli, and S. Ramalingam, “PSA-LOPA-
A Novel Method for Physical Security Risk Analysis based on Layers
of Protection Analysis,” in Proceedings - International Carnahan Con-
ference on Security Technology, vol. 2018-Octob, Institute of Electrical
and Electronics Engineers Inc., 12 2018.

16

[23] IEC, “IEC 62443-4-1:2018 Security for industrial automation and con-
trol systems - Part 4-1: Secure product development lifecycle require-
ments,” tech. rep., 2018.

[24] H. Kanamaru, “Bridging functional safety and cyber security of
SIS/SCS,” in 2017 56th Annual Conference of the Society of Instrument
and Control Engineers of Japan, SICE 2017, vol. 2017-Novem, pp. 279–
284, IEEE, 9 2017.

[25] IEC, “IEC 62859:2016 Nuclear power plants - Instrumentation and con-
trol systems - Requirements for coordinating safety and cybersecurity,”
tech. rep., 2016.

[26] C. Torens, “Safety Versus Security in Aviation, Comparing DO-178C
with Security Standards,” in arc.aiaa.org, American Institute of Aero-
nautics and Astronautics (AIAA), 1 2020.

[27] A. Aviˇzienis, J. C. Laprie, B. Randell, and C. Landwehr, “Basic concepts
and taxonomy of dependable and secure computing,” IEEE Transactions
on Dependable and Secure Computing, 2004.

[28] A. Tantawy, A. Erradi, and S. Abdelwahed, “A Modiﬁed Layer of
Protection Analysis for Cyber-Physical Systems Security,” in 4th In-
ternational IConference on System Reliability and Safety, ICSRS 2019,
(Rome, Italy), 2019.

[29] A. J. Kornecki and M. Liu, “Fault tree analysis for safety/security
veriﬁcation in aviation software,” Electronics, vol. 2, pp. 41–56, 1 2013.
[30] C. W. Johnson, “CyberSafety: On the Interactions between Cyber-
Security and the Software Engineering of Safety-Critical Systems,”
Laboratory Medicine, vol. 21, no. 7, pp. 411–413, 2012.

[31] A. J. Kornecki and J. Zalewski, “Safety and security in industrial
control,” in ACM International Conference Proceeding Series, (New
York, New York, USA), p. 1, ACM Press, 2010.

[32] T. Novak, A. Treytl, and P. Palensky, “Common approach to functional
safety and system security in building automation and control systems,”
in IEEE International Conference on Emerging Technologies and Fac-
tory Automation, ETFA, pp. 1141–1148, 2007.

[33] T. Novak and A. Treytl, “Functional safety and system security in
automation systems - A life cycle model,” in IEEE International
Conference on Emerging Technologies and Factory Automation, ETFA,
pp. 311–318, 2008.

[34] T. Novak and A. Gerstinger, “Safety- and security-critical services
in building automation and control systems,” IEEE Transactions on
Industrial Electronics, vol. 57, pp. 3614–3621, 11 2010.

[35] K. Sørby, Relationship between security and safety in a security-safety
critical system: Safety consequences of security threats. PhD thesis,
NTNU, Trondheim, Norway, 2003.

[36] K. Stølen, F. Braber, T. Dimitrakos, R. Fredriksen, B. A. Gran, S.-H.
Houmb, Y. C. Stamatiou, and J. . Aagedal, “Model-Based Risk Assess-
ment in a Component-Based Software Engineering Process,” in Business
Component-Based Software Engineering, pp. 189–207, Springer US,
2003.

[37] B. Hunter, “Integrating Safety and Security into the System Lifecycle,”
in Improving Systems and Software Engineering Conference (ISSEC),
2009.

[38] R. Winther, O. A. Johnsen, and B. A. Gran, “Security assessments of
safety critical systems using HAZOPs,” in Lecture Notes in Computer
Science, vol. 2187, pp. 14–24, Springer Verlag, 2001.

[39] C. Schmittner, T. Gruber, P. Puschner, and E. Schoitsch, “Security
application of Failure Mode and Effect Analysis (FMEA),” in Lecture
Notes in Computer Science, vol. 8666 LNCS, pp. 310–325, Springer
Verlag, 2014.

[40] Origin Consulting, “GSN Community Standard Version 1.” 2011.
[41] S. Lautieri, D. Cooper, and D. Jackson, “SafSec: Commonalities Be-
tween Safety and Security Assurance,” in Constituents of Modern
System-safety Thinking, pp. 65–75, Springer London, 12 2007.

[42] N. Subramanian and J. Zalewski, “Quantitative assessment of safety and
security of system architectures for cyberphysical systems using the NFR
approach,” IEEE Systems Journal, vol. 10, no. 2, pp. 397–409, 2016.

[43] I. Nai Fovino, M. Masera, and A. De Cian, “Integrating cyber attacks
within fault trees,” Reliability Engineering and System Safety, vol. 94,
pp. 1394–1402, 9 2009.

[44] A. J. Kornecki and M. Liu, “Fault tree analysis for safety/security
veriﬁcation in aviation software,” Electronics, vol. 2, pp. 41–56, 1 2013.
[45] M. Steiner and P. Liggesmeyer, “Combination of Safety and Security
Analysis - Finding Security Problems That Threaten The Safety of a
System,” SAFECOMP 2013 - Workshop DECS (ERCIM/EWICS Work-
shop on Dependable Embedded and Cyber-physical Systems) of the 32nd
International Conference on Computer Safety, Reliability and Security,
pp. 1–8, 2013.

17

Symbol
λi
λp
λa
λc
λ
αa
γa
P [Li]

TMEL
P [Bc]
P [Bp]
P [AB]
P [ASB]
P [Sc]
P [Sp]
P [AS ]
P [ABS ]
P [Sc, Bc]

α1 − α2
γ1 − γ3
ζ1 − ζ3
β

Description
Initiating event i likelihood (/yr)
BPCS physical failure event likelihood (/yr)
BPCS cyber attack a likelihood (/yr)
BPCS semantically-related attacks likelihood (/yr)
BPCS cyber attack likelihood for all attacks
Probability of selecting attack a by the attacker
Weight factor for the attacks for the equivalent BPCS
Probability of failure of all protection layers for
initiating event i
Target Mitigated Event Likelihood
Probability of BPCS security failure
Probability of BPCS physical failure
Probability of BPCS direct security failure
Probability of BPCS SIS-pivot security failure
Probability of SIS security failure
Probability of SIS physical failure
Probability of SIS direct security failure
Probability of SIS BPCS-pivot security failure
Probability of simultaneous SIS and BPCS security
failure
-
-
-
-

Type
Parameter
Parameter
Parameter
Parameter
Parameter
Parameter
Parameter
Parameter

Parameter
Intermediate design variable
Parameter
Parameter
Parameter
Intermediate design variable
Design variable
Design variable
Design variable
Intermediate design variable

Auxiliary parameters
Auxiliary parameters
Auxiliary parameters
Auxiliary parameters

Calculation Method/ Data Source
Reliability data
Reliability data
Refer to Section III-B
Refer to Section III-B, Lemma III.1
Statistical attack data
Attacker proﬁle model
Refer to Lemma III.1
Reliability data

Determined by the corporate policy
BPCS security risk assessment
Reliability data
BPCS security risk assessment
BPCS security risk assessment
SIS security risk assessment
SIS security risk assessment
SIS security risk assessment
SIS security risk assessment
BPCS & SIS security risk assess-
ment
Eq. (10), (11)
Eq. (14) to (16)
Eq. (23) to (25)
Eq. (12)

Table V
CLOPA MODEL PARAMETERS. VARIABLES DESIGNATED AS ”DESIGN VARIABLE” ARE WITH RESPECT TO CLOPA, BUT COULD BE A DESIGN VARIABLE
OF ANOTHER ASSESSMENT, SUCH AS P [AB], DERIVED FROM BPCS SECURITY RISK ASSESSMENT. VARIABLES DESIGNATED AS ”INTERMEDIATE
DESIGN VARIABLES” COULD BE EXPRESSED IN TERMS OF DESIGN VARIABLES.

[46] R. Kumar and M. Stoelinga, “Quantitative security and safety analysis
with attack-fault trees,” in Proceedings of IEEE International Symposium
on High Assurance Systems Engineering, pp. 25–32, IEEE Computer
Society, 4 2017.

[47] H. Abdo, M. Kaouk, J. M. Flaus, and F. Masse, “A safety/security
risk analysis approach of Industrial Control Systems: A cyber bowtie
combining new version of attack tree with bowtie analysis,” Computers
and Security, vol. 72, pp. 175–195, 1 2018.

[48] M. Bouissou and J. L. Bon, “A new formalism that combines advan-
tages of fault-trees and Markov models: Boolean logic driven Markov
processes,” Reliability Engineering and System Safety, vol. 82, pp. 149–
163, 11 2003.

[49] S. Kriaa, M. Bouissou, F. Colin, Y. Halgand, and L. Pietre-Cambacedes,
“Safety and security interactions modeling using the BDMP formalism:
Case study of a pipeline,” in Lecture Notes in Computer Science,
vol. 8666 LNCS, pp. 326–341, Springer Verlag, 2014.

[50] S. Kriaa, M. Bouissou, and L. Pi`etre-Cambac´ed`es, “Modeling the
Stuxnet attack with BDMP: Towards more formal risk assessments,”
in 7th International Conference on Risks and Security of Internet and
Systems, CRiSIS 2012, 2012.

[51] B. Kaiser, C. Gramlich, and M. F¨orster, “State/event fault trees-A safety
analysis model for software-controlled systems,” Reliability Engineering
and System Safety, vol. 92, pp. 1521–1537, 11 2007.

[52] M. Roth and P. Liggesmeyer, “Modeling and Analysis of Safety-Critical
Cyber Physical Systems using State/Event Fault Trees,” SAFECOMP
2013 - Workshop DECS (ERCIM/EWICS Workshop on Dependable
Embedded and Cyber-physical Systems) of the 32nd International Con-
ference on Computer Safety, Reliability and Security, p. NA, 9 2013.

[53] R. Mitchell and I. R. Chen, “Effect of intrusion detection and response
on reliability of cyber physical systems,” IEEE Transactions on Relia-
bility, vol. 62, no. 1, pp. 199–210, 2013.

[54] C. W. Ten, C. C. Liu, and G. Manimaran, “Vulnerability assessment
of cybersecurity for SCADA systems,” IEEE Transactions on Power
Systems, vol. 23, no. 4, pp. 1836–1846, 2008.

[57] G. Sindre and A. L. Opdahl, “Eliciting security requirements with misuse

cases,” Requirements Engineering, vol. 10, pp. 34–44, 1 2005.

[55] A. J. Kornecki, N. Subramanian, and J. Zalewski, “Studying inter-
relationships of safety and security for software assurance in cyber-
physical systems: Approach based on bayesian belief networks,” in 2013
Federated Conference on Computer Science and Information Systems,
FedCSIS 2013, pp. 1393–1399, 2013.

[56] G. Sindre, “A look at misuse cases for safety concerns,” in IFIP
International Federation for Information Processing, vol. 244, pp. 252–
266, Springer, Boston, MA, 2007.

[58] C. Raspotnig, P. Karpati, and V. Katta, “A combined process for
elicitation and analysis of safety and security requirements,” in Lecture
Notes in Business Information Processing, vol. 113 LNBIP, pp. 347–361,
Springer Verlag, 2012.

[59] J. J¨urjens, “Developing safety-critical systems with UML,” Lecture

Notes in Computer Science, vol. 2863, pp. 360–372, 10 2003.

[60] J. J¨urjens, “UMLsec: Extending UML for secure systems development,”
in Lecture Notes in Computer Science, vol. 2460 LNCS, pp. 412–425,
Springer, Berlin, Heidelberg, 2002.

[61] J. J¨urjens, “Developing Safety-and Security-critical Systems with UML,”

in DARP Workshop, (Loughborough), 2003.

[62] G. Pedroza, L. Apvrille, and D. Knorreck, “AVATAR: A SysML envi-
ronment for the formal veriﬁcation of safety and security properties,”
in 2011 11th Annual International Conference on New Technologies of
Distributed Systems, NOTERE 2011 - Proceedings, 2011.

[63] J. Thomas, Extending and Automating STPA for Requirements Genera-
tion and Analysis. PhD thesis, MIT, Massachusetts, USA, 2013.

[64] I. Friedberg, K. McLaughlin, P. Smith, D. Laverty, and S. Sezer, “STPA-
SafeSec: Safety and security analysis for cyber-physical systems,” Jour-
nal of Information Security and Applications, vol. 34, pp. 183–196, 6
2017.

[65] A. Nourian and S. Madnick, “A Systems Theoretic Approach to the
Security Threats in Cyber Physical Systems Applied to Stuxnet,” IEEE
Transactions on Dependable and Secure Computing, vol. 15, pp. 2–13,
1 2018.

