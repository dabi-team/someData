An Evasion and Counter-Evasion Study in Malicious
Websites Detection

Li Xu†

Zhenxin Zhan†

Shouhuai Xu†

Keying Ye‡

†Deptartment of Computer Science, University of Texas at San Antonio
‡Department of Management Science and Statistics, University of Texas at San Antonio

4
1
0
2

g
u
A
8

]

R
C
.
s
c
[

1
v
3
9
9
1
.
8
0
4
1
:
v
i
X
r
a

Abstract—Malicious websites are a major cyber attack vector,
and effective detection of them is an important cyber defense task.
The main defense paradigm in this regard is that the defender
uses some kind of machine learning algorithms to train a detection
model, which is then used to classify websites in question. Unlike
other settings, the following issue is inherent to the problem of
malicious websites detection: the attacker essentially has access
to the same data that the defender uses to train his/her detection
models. This ‘symmetry’ can be exploited by the attacker, at least
in principle, to evade the defender’s detection models. In this
paper, we present a framework for characterizing the evasion
and counter-evasion interactions between the attacker and the
defender, where the attacker attempts to evade the defender’s
detection models by taking advantage of this symmetry. Within
this framework, we show that an adaptive attacker can make
malicious websites evade powerful detection models, but proactive
training can be an effective counter-evasion defense mechanism.
The framework is geared toward the popular detection model of
decision tree, but can be adapted to accommodate other classiﬁers.

Index Terms—Malicious websites, static analysis, dynamic anal-

ysis, evasion, adaptive attacks, proactive training.

I. INTRODUCTION

Compromising websites and abusing them to launch further
attacks (e.g., drive-by-download [7], [20]) have become one of
the mainstream attack vectors. Unfortunately, it is infeasible, if
not impossible, to completely eliminate such attacks, meaning
that we must have competent solutions that can detect com-
promised/malicious websites as soon as possible. The dynamic
approach, which is often based on client honeypots or variants,
can detect malicious websites with high accuracy, but is limited
in terms of its scalability. The static approach, which often ana-
lyzes the website contents and then uses some detection models
(e.g., decision trees) to classify them into benign/malicious
classes, is very efﬁcient, but suffers from its limited success in
dealing with sophisticated attacks (e.g. JavaScript obfuscation).
This hints that there is perhaps some inherent limitation in the
trade-off between scalability and detection effectiveness.

In this paper, we bring up another dimension of the problem,
which may have a fundamental impact on the aforementioned
inherent limitation. Unlike in other settings, the following issue
is inherent to the problem of malicious websites detection:
The attacker essentially has access to the same data that the
defender uses to train its detection models. This ‘symmetry’
could be exploited by the attacker to evade the defender’s
detection models. This is because the attacker can effectively

train and obtain (almost) the same detection models, and then
exploit them to make other malicious websites evasive. This is
possible because the attacker can manipulate the contents of
malicious websites during the course of compromising them,
or after they are compromised but before they are analyzed by
the defender’s detection models. This is feasible because the
attacker controls the malicious websites.

More speciﬁcally, we make two contributions. First, we pro-
pose a framework for characterizing the evasion and counter-
evasion interactions between the attacker and the defender. The
framework accommodates a set of adaptive attacks against a
class of detection models known as decision trees [22], which
have been widely used in this problem domain. The framework
also accommodates the novel idea of proactive training as
the counter-evasion mechanism against the adaptive attacks,
where the defender proactively trains its detection models
while taking adaptive attacks into consideration. Although the
framework is geared towards decision trees, it can be adapted
to accommodate other kinds of classiﬁers.

Second, we use a dataset that was collected during the span
of 40 days to evaluate the evasion power of adaptive attacks
and the counter-evasion effectiveness of proactive training.
Experimental results show that an adaptive attacker can make
malicious websites evade powerful detection models, but proac-
tive training can be an effective counter-evasion defense mech-
anism. In order to deepen our understanding of the evasion and
counter-evasion interactions, we also investigate which features
(or attributes) of websites have a high security signiﬁcance,
namely that
their manipulation causes the misclassiﬁcation
of malicious websites. Surprisingly, we ﬁnd that the features
of high security signiﬁcance are almost different from the
features that would be selected by the standard feature selection
algorithms. This suggests that we might need to design new
machine learning algorithms to best ﬁt the domain of security
problems. Moreover, we ﬁnd that the detection accuracy of
proactively-trained detection models increases with the degree
of the defender’s proactiveness (i.e. the number of training
iterations). Finally, we ﬁnd that if the defender does not know
the attacker’s adaptation strategy, the defender should adopt the
full adaptation strategy that will be described later.

The rest of the paper is organized as follows. Section II
brieﬂy reviews the context of the present study. Section III
investigates the framework of evasion and counter-evasion
interactions. Section IV evaluates the effectiveness of the

 
 
 
 
 
 
framework. Section V discusses related prior work. Section VI
concludes the paper.

II. PRELIMINARIES

In order to illustrate the power of adaptive attacks and the
effectiveness of our counter-measure against them, we need to
consider some concrete detection scheme. Since J48 classiﬁers
are known to be successful in detecting malicious websites [3],
[29], [30], [15], [6], [14], we adopt the detection scheme we
proposed in [30] as the starting point of the present study. We
showed in [30] that J48 classiﬁer outperforms Naive Bayes,
Logistic and SVM classiﬁers.

We also inherit

the data collection method described in
[30]. At a high level, a crawler is used to fetch the website
content corresponding to an input URL, benign and mali-
cious alike. Each URL is described by 105 application-layer
features and 19 network-layer features [30]. We now brieﬂy
review the following 16 features that will be encountered
later: URL_length (length of URL); Content_length
(the content-length ﬁeld in HTTP header, which may be ma-
nipulated by malicious websites); #Redirect (number of
redirects); #Scripts (number of scripts); #Embedded_URL
(number of URLs embedded); #Special_character
(number of special characters in a URL); #Iframe (number
of iframes); #JS_function (number of JavaScript func-
tions in a website); #Long_string (number of strings
in embedded JavaScript pro-
with 51 or more letters
grams); #Src_app_bytes (number of bytes communi-
to website); #Local_app_packet
cated from crawler
(number of crawler-to-website IP packets,
including redi-
rects and DNS queries); Dest_app_bytes (volume of
website-to-crawler communications); Duration (the time
takes for the crawler to fetch the contents of a web-
it
including rediects); #Dist_remote_tcp_port and
site,
#Dist_remote_IP (number of distinct TCP ports and
IP addresses
to fetch websites con-
tents, respectively); #DNS_query (number of DNS queries);
#DNS_answer (number of DNS server’s responses).
The main notations are summarized as follows.

the crawler uses

MLA machine learning algorithm

feature vector representing a website
feature Xz’s domain is [minz, maxz]
defender’s detection schemes (e.g., J48 classiﬁer)
training data (feature vectors) for learning M0

fv
Xz
M0, . . . , Mγ
D′
0
D0 D0 = D0.malicious ∪ D0.benign, where mali-
cious feature vectors in D0.malicious may have
been manipulated
feature vectors used by defender to proactively
train M1, . . . , Mγ ; D†
0.malicious ∪
D†
number of adaptation iterations
applying detection scheme Mi to classify feature
vectors Dα

α, γ
Mi(Dα)

0 = D†

0.benign

D†
0

M0-γ (Dα) majority vote of M0(Dα), . . . , Mγ (Dα)

ST, C, F

s R

← S

adaptation strategy ST, manipulation algorithm
F, manipulation constraints C
assigning s as a random member of set S

III. EVASION AND COUNTER-EVASION FRAMEWORK

Adaptive attacks are possible because an attacker can collect
the same data as what is used by the defender to train a
detection scheme. The attacker also knows the machine learning
algorithm(s) the defender uses or even the defender’s detection
scheme. To accommodate the worst-case scenario, we assume
there is a single attacker that coordinates the compromise of
websites (possibly by many sub-attackers). This means that
the attacker knows which websites are malicious, while the
defender aims to detect them. In order to evade detection, the at-
tacker can manipulate some features of the malicious websites.
The manipulation operations can take place during the course
of compromising websites, or after compromising websites but
before they are classiﬁed by the defender’s detection scheme.

A. Framework Overview

We describe adaptive attacks and countermeasures in a
modular fashion, by using eight algorithms whose caller-callee
relation is highlighted in Figure 1. Algorithm 1 is the attacker’s
main algorithm, which calls Algorithm 2 for preprocessing,
and calls Algorithm 4 or Algorithm 5 for selecting features
to manipulate and for determining the manipulated values for
the selected features. Both Algorithm 4 and Algorithm 5 call
Algorithm 3 to compute the escape intervals for the features
that are to be manipulated. An escape interval deﬁnes the
interval from which the manipulated value of a feature should
be taken so as to evade detection.

Algorithm  8: Evaluation (Eva)

Algorithm1: 
Adaptive Attack (AA)

Algorithm  6:
Proactive Detection (PD)

Algorithm  2: 
Preprocessing (PP)

Algorithm  7:
Proactive Training  (PT)

Algorithm  4 & 5: 
Manipulation (F1 & F2)

Algorithm  3: Computing 
Escape_Interval (Escape)

Fig. 1. Caller-callee relation between the algorithms.

Algorithm 6 is the defender’s main algorithm, which calls
Algorithm 7 for proactive training of detection schemes. For
this purpose, the defender can have access to its own proactive
manipulation algorithm. In our experiments, we let the defender
have access to the manipulations algorithms that are available
to the attacker, namely Algorithm 4 and Algorithm 5. This is
sufﬁcient for the purpose of understanding the effectiveness of
proactive training and detection against adaptive attacks under
various algorithm/parameter possibilities, such as: the defender
correctly or incorrectly “guess” the manipulation algorithm or
parameters that are used by the attacker, and the relatively more
effective proactive training strategy against a class of adaptive
attacks. In order to evaluate the effectiveness of proactive
training and detection against adaptive attacks, we use an
“artiﬁcial” Algorithm 8, which is often implicit in most real-life
defense operations.

B. Evasion Model and Algorithms

In our model, a website is represented by a feature vector.
We call the feature vector representing a benign website benign
feature vector, and the feature vector representing a malicious
website malicious feature vector. Denote by D′
0 the defender’s
training data, namely a set of feature vectors corresponding
to a set of benign websites (denoted by D′
0.benign) and
malicious websites (denoted by D′
0.malicious). The defender
uses a machine learning algorithm MLA to learn a detection
scheme M0 from D′
0 (i.e., M0 is learned from one portion
of D′
0). As mentioned
above, the attacker is given M0 to accommodate the worst-
case scenario. Denote by D0 the set of feature vectors that
are to be classiﬁed by M0 to determine which feature vectors
(i.e., the corresponding websites) are malicious. The attacker’s
objective is to manipulate the malicious feature vectors in D0
into some Dα so that M0(Dα) has a high false-negative rate,
where α > 0 represents the number of iterations (or rounds)
the attacker conducts the manipulation operations.

0 and tested via the other portion of D′

Algorithm 1 Adaptive attack AA(MLA, M0, D0, ST, C, F, α)
INPUT:M0 is defender’s detection scheme, D0 = D0.malicious ∪
D0.benign where malicious feature vectors (D0.malicious) are to be
manipulated (to evade detection of M0), ST is attacker’s adaptation
strategy, C is a set of manipulation constraints, F is attacker’s manip-
ulation algorithm, α is attacker’s number of adaptation rounds
OUTPUT: Dα
1: initialize array D1, . . . , Dα
2: for i=1 to α do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

Di−1 ← PP(D0, . . . , Di−2) {see Algorithm 2}
Di ← F(Mi−1, Di−1, C) {manipulated version of D0}

Di ← F(Mi−1, Di−1, C) {manipulated version of D0}

else if ST == sequential-adaptation then

Mi ← MLA(Di) {D1, . . . , Dα−1, M1, . . . , Mα−1 are not
used when ST==parallel-adaptation}

if ST == parallel-adaptation then

else if ST == full-adaptation then

{manipulated version of D0}

end if
if i < α then

Di ← F(M0, D0, C)

end if
13:
14: end for
15: return Dα

Algorithm 1 describes the adaptive attack. As highlighted in

Figure 2, we consider three basic adaptation strategies.

• ST == parallel-adaptation: The attacker sets the
manipulated Di = F(M0, D0, C), where i = 1, . . . , α, and
F is a randomized manipulation algorithm, meaning that
Di = Dj for i 6= j is unlikely.

• ST == sequential-adaptation: The attacker sets
the manipulated Di = F(Mi−1, Di−1, C) for i = 1, . . . , α,
where detection schemes M1, . . . , Mα are respectively
learned from D1, . . . , Dα using the defender’s machine
learning algorithm MLA (also known to the attacker).
• ST == full-adaptation: The attacker sets the ma-
nipulated Di = F(Mi−1, PP(D0, . . . , Di−1), C) for i =
1, 2, . . ., where PP(·, . . .) is a preprocessing algorithm for

“aggregating” sets of feature vectors D0, D1, . . . into a
single set of feature vectors, F is a manipulation algorithm,
M1, . . . , Mα are learned respectively from D1, . . . , Dα by
the attacker.

Algorithm 2 is a concrete preprocessing algorithm. Its basic
idea is the following: since each malicious website corresponds
to m malicious feature vectors that respectively belong to
D0, . . . , Dm−1, the preprocessing algorithm randomly picks
one of the m malicious feature vectors to represent the ma-
licious website in D. It is worth mentioning that one can
derive some hybrid attack strategies from the above three
basic strategies. We also note that the attack strategies and
the manipulation constraints are independent of the detection
schemes, but the manipulation algorithms would be speciﬁc to
the detection schemes.

vectors D0, . . . , Dm−1
to
corresponds

feature

website

Algorithm 2 Preprocessing PP(D0, . . . , Dm−1)
INPUT: m sets
of
zth
malicious
the
where
D0.malicious[z], . . . , Dm−1.malicious[z]
OUTPUT: D
1: D ← ∅
2: size ← sizeof(D0.malicious)
3: for z = 1 to size do
R
← {D0.malicious[z], . . . , Dm−1.malicious[z]}
4:
5:
6: end for
7: return D

D[z]
D ← D ∪ D0.benign

the attacker needs

to be manipulated,

Manipulation Constraints. For a feature X whose value
is
to compute
X.escape interval, which is a subset of feature X’s domain
domain(X) and can possibly cause the malicious feature
vector to evade detection. Feature X’s manipulated value is
randomly chosen from its escapte interval, which is calcu-
lated using Algorithm 3, while taking as input X’s domain
constraints and semantics constraints.

Algorithm 3 X’s escape interval Escape(X, M, C)
INPUT: X is feature for manipulation, M is detection scheme, C
represents constraints
OUTPUT: X’s escape interval
1: domain constraint ← C.domain map(X)
2: semantics constraint ← C.semantics map(X)
cannot be manipulate due to semantics constraints}

{∅ if X

3: escape interval

←

domain constraint

∩

semantics constraint
4: return escape interval

Algorithm 3 is called because the manipulation algorithm
needs to compute the interval from which a feature’s manip-
ulated value should be taken. Speciﬁcally, the constraints are
the following.

• Domain constraints: Each feature has its own domain
of possible values. This means that the new value of a
feature after manipulation must fall into the domain of
the feature. Let C.domain map be a table of (key, value)

%&

+,-./0/1

7869,6:+-;;0<0,=

7.-0/0/16=->-

 *&

 &

2
2
2

 !"#$%&’ &’()

 4"#$%&’ &’()

3

  5!"#$%&’ &’()

  6"#$%&’ &’()

 "

%&’()*)+

6(’*)*)+4<’=’
#$"

6748&49%’::*;*&<

#"

 !

,
,
,

  .!

%&’()*)+

#!/ 0 "1#"!23

4#5/ 0 !1#!!23

%&’()*)+

-

# .!/ 0  .51# .5!23

%&’()*)+

 "

7869&6:%’;;*<*&=

7(’*)*)+6=’>’

#$"

#"

 !

,
,
,

  .!

%&’()*)+

#!/ 0 "1#"123

#4/ 0 !1550#"1#!3123

%&’()*)+

-
# .!/ 0  .41550#"1-1# .43123

# / 0  .!1# .!!23

# 6/ 0  .!1550#"1-1# .!3123

(a) Parallel adaptation strategy

(b) Sequential adaptation strategy

(c) Full adaptation strategy

Fig. 2. Adaptive attack algorithm AA(MLA, M0, D0, ST, C, F, α), where D′
0 is the defender’s training data, M0 is the defender’s detection scheme that
0 by using MLA, D0 is the feature vectors that are examined by M0 in the absence of adaptive attacks, ST is the attacker’s adaptation
is learned from D′
strategy, C is a set of manipulation constraints, F is the attacker’s (deterministic or randomized) manipulation algorithm that maintains the set of constraints C,
α is the number of rounds the attacker runs its manipulation algorithms. Dα is the manipulated version of D0 with malicious feature vectors D0.malicious
manipulated. The attacker’s objective is make M0(Dα) have high false-negative rate.

pairs, where key is feature name and value is the feature’s
domain constraint. Let C.domain map(X) return feature
X’s domain as deﬁned in C.domain map.

• Semantics constraints: The manipulation of feature val-
ues should have no side-effect to the attack, or at least
cannot invalidate the attacks. For example,
if a mali-
cious website needs to use script to launch the drive-
by-download attack, the feature indicating the number of
scripts cannot be manipulated to 0. Let C.semantics map
be a table of (key, value) pairs, where key is feature
name and value is the feature’s semantics constraints.
Let C.semantics map(X) return feature X’s semantics
constraints as speciﬁed in C.attack map.

In general, constraints might have to be manually identiﬁed

based on feature deﬁnitions and domain knowledge.

v0

X9

≤13

>13

v10

≤0

>7
v8

X4

R R

>0

v4

(cid:20)
X4(cid:29)(cid:11)(cid:80)(cid:76)(cid:81)(cid:15)(cid:19)(cid:12)

v12

X10

>3.9

(cid:20)

≤3.9
v11

X1
≤1.7 >1.7

v13

X16

(cid:19)

(cid:20)

v9

≤7

R R

X9

v1

(cid:20)
X9(cid:29)(cid:11)(cid:26)(cid:15)(cid:20)(cid:22)(cid:12)

≤9.1

>9.1

v7

X18
≤2.3 >2.3

(cid:19)
(cid:20)
X18(cid:29)(cid:11)(cid:21)(cid:17)(cid:22)(cid:15)(cid:80)(cid:68)(cid:91)(cid:12)

v2

(cid:20)

v3

intermediate node

benign decision node

malicious decision node

feature name

escape interval

(cid:19)
(cid:20)
Xi
X9(cid:29)(cid:11)(cid:26)(cid:15)(cid:20)(cid:22)(cid:12)

X18(cid:29)(cid:11)(cid:21)(cid:17)(cid:22)(cid:15)(cid:80)(cid:68)(cid:91)(cid:12)jump-in interval

Fig. 3. Example J48 classiﬁer and feature manipulation. For inner node v10 on
the benign path ending at benign leaf v3, we have v10.f eature = “X4”
and v10.f eature.value = X4.value.

Manipulation Algorithms. As mentioned in Section II, we

adopt the J48 classiﬁer detection scheme, where a J48 classiﬁer
is trained by concatenating the application- and network-layer
features corresponding to the same URL [30]. We present two
manipulation algorithms, called F1 and F2, which exploit the
defender’s J48 classiﬁer to guide the manipulation of features.
Both algorithms neither manipulate the benign feature vectors
(which are not controlled by the attacker), nor manipulate
the malicious feature vectors that are already classiﬁed as
benign by the defender’s detection scheme (i.e., false-negative).
Both algorithms may fail, while brute-forcing may fail as well
because of the manipulation constraints.

Since the manipulation algorithms are inevitably compli-
cated, in the following we will present their basic ideas and
sketched algorithms. The notations used in the algorithms are:
for node v in the classiﬁer, v.f eature is the feature associated
to node v, and v.value is v.f eature’s “branching” value
as speciﬁed by the classiﬁer (a binary tree with all features
numericalized).

Manipulation Algorithm F1 is described as Algorithm 4.
The basic idea underlying this manipulation algorithm is the
following: for every malicious feature vector in D, there is a
unique path (in the J48 classiﬁer M ) that leads to a malicious
leaf, which indicates that the feature vector is malicious. We
call the path leading to malicious leaf a malicious path, and the
path leading to a benign leaf (which indicates a feature vector
as benign) a benign path.

inner node, namely v2,

By examining the path from the malicious leaf to the root,
say malicious leaf → v2 → . . . → root, and identifying
the algorithm attempts to
the ﬁrst
manipulate fv.(v2.f eature).value so that the classiﬁcation can
lead to malicious leaf ’s sibling, say v2,another child, which
is guaranteed to exist (otherwise, v2 cannot be an inner node).
Note that there must be a sub-path rooted at v2,another child
leads to a benign leaf (otherwise, v2 cannot be an
that
inner node as well), and that manipulation of values of the
features corresponding to the nodes on the sub-tree rooted at

Algorithm 4 Manipulation algorithm F1(M, D, C)
INPUT: J48 classiﬁer M , feature vector set D(malicious ∪ benign),
manipulation constraints C
OUTPUT: manipulated feature vectors
1: for all feature vector ∈ D.malicious do
2:
3:
4:
5:
6:
7:
8:
9:

v be the root node of M
maintain an interval for every feature in feature vector.
while v is not benign leaf do
if v is an inner node then

v ← v.Child based on decision tree rule
update corresponding feature interval

else if v is a malicious leaf then

compute the corresponding feature’s escape interval by
calling Algorithm 3
pick a value z in the escape interval uniformly at random

10:

D.malicious, F2 keeps track of the mismatches between fv
and all benign paths. The algorithm attempts to manipulate as
few “mismatched” features as possible to evade M .

Algorithm 5 Manipulation algorithm F2(M, D, C)
INPUT: J48 classiﬁer M , feature vector set D(malicious ∪ benign),
manipulation constraints C
OUTPUT: manipulated feature vectors

1: create interval vector for features along every benign path and

store in Paths.

2: for all feature vector ∈ D.malicious do
3:
4:

for all P ath ∈ Paths do

compare feature vector to P ath and record the mismatch
feature number

set the corresponding feature’s value to z
v ← v.sibling

11:
12:
13:
14:
15: end for
16: return manipulated feature vectors D

end if
end while

look at one example. At a high-level,

v2,another child will preserve the postﬁx v2 → . . . → root.
To help understand the manipulation algorithm,

let
us
the attacker
runs AA(“J48”, M0, D0, ST, C, F1, α = 1) and therefore
F1(M0, D0, C) to manipulate the feature vectors, where ST
can be any of the three strategies because they cause no
difference when α = 1 (see Figure 2). Consider the example
J48 classiﬁer M in Figure 3, where features and their values
are for illustration purpose, and the leaves are decision nodes
with class 0 indicating benign leaves and 1 indicating malicious
leaves. A website with feature vector

(X4 = −1, X9 = 5, X16 = 5, X18 = 5)

is classiﬁed as malicious because it leads to decision path

X9≤13
−−−−→ v10

X4≤0
−−−−→ v9

X9≤7
−−−−→ v1,

v0

which ends at malicious leaf v1. The manipulation algorithm
ﬁrst identiﬁes malicious leaf v1’s parent node v9, and manip-
ulates X9’s value to ﬁt into v1’s sibling (v8). Note that X9’s
escape interval is as:

([min9, max9] \ [min9, 7]) ∩ [min9, 13] = (7, 13],

where Domain(X9) = [min9, max9], [min9, 7] corresponds to
node v9 on the path, and [min0, 13] corresponds to node v0
on the path. The algorithm manipulates X9’s value to be a
random element from X9’s escapte interval, say 8 ∈ (7, 13],
which causes the manipulated feature vector to evade detection
because of decision path:

X9≤13
−−−−→ v10

X4≤0
−−−−→ v9

X9>7
−−−−→ v8

X16≤9.1
−−−−−→ v7

X18>2.3
−−−−−→ v3

v0

and ends at benign leaf v3.

Manipulation Algorithm F2 is described as Algorithm 5.
The basic idea underlying this manipulation algorithm is to
ﬁrst extract all benign paths. For each feature vector fv ∈

end for
sort Paths in ascending order of mismatch feature number
for all P ath ∈ Paths until successfully manipulated do

for all mismatch feature ∈ Paths do

5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for
15: return manipulated feature vectors D

end for

end for

get escape interval by calling Algorithm 3
pick a value n in escape interval at random
set feature vector’s corresponding feature value to n

To help understand this manipulation algorithm, let us look

at another example. Consider feature vector

(X4 = .3, X9 = 5.3, X16 = 7.9, X18 = 2.1, X10 = 3, X1 = 2.3),

which is classiﬁed as malicious because of path

X9≤13
−−−−→ v10

X4>0
−−−−→ v4.

v0

To evade detection, the attacker can compare the feature vector
to the matrix of two benign paths. For the benign path v3 →
v7 → v8 → v9 → v10 → v0, the feature vector has three
mismatches, namely features X4, X9, X18. For the benign path
v13 → v11 → v12 → v0, the feature vector has two mismatches,
namely X9 and X1. The algorithm ﬁrst processes the benign
path ending at node v13. The algorithm will try to manipulate
X9 and X1’s values to reach the benign leaf. Suppose on the
other hand, that X10 cannot be manipulated without violating
the constraints. The algorithm stops with this benign path and
considers the benign path end at node v3. If the algorithm fails
with this benign path again, the algorithm will not manipulate
the feature vector and leave it to be classiﬁed as malicious.

C. Counter-Evasion Algorithms

We have showed that adaptive attacks can ruin the defender’s
(non-proactive) detection schemes. Now we investigate counter-
measure against adaptive attacks. The counter-measure is based
on the idea of proactive training & detection. Algorithm 6
describes the proactive detection algorithm. The basic idea of
this algorithm is to call the proactive training algorithm to
generate a set of proactively trained detection schemes, denoted
by M †
γ. These detection schemes are derived from,
among other things, M0, which is learned from D′
0 using MLA.
0.benign ∪ D′
It is important to note that D′
0.malicious,

1 , . . . , M †

0 = D′

6

detection

Proactive

0 using MLA, D†

0, Dα, STD, C, FD, γ)

Algorithm
PD(MLA, M0, D†
INPUT: M0 is learned from D′
0.benign ∪
D†
0.malicious, Dα (α unknown to defender) is set of feature vec-
tors for classiﬁcation (where the malicious websites may have been
manipulated by the attacker), STD is defender’s adaptation strategy,
FD is defender’s manipulation algorithm, C is set of constraints, γ is
defender’s number of adaptations rounds
OUTPUT: malicious vectors fv ∈ Dα
1: M †

0, STD, C, FD, γ) {see Algo-

γ ← PT(MLA, M0, D†

1 , . . . , M †

0 = D†

rithm 7}

(M0(fv)
if
M0(fv), M †

says
1 (fv), . . . , M †
malicious ← malicious ∪ {fv}

fv

is malicious) OR (majority

of

γ (fv) say fv is malicious) then

2: malicious ← ∅
3: for all fv ∈ Dα do
4:

5:
end if
6:
7: end for
8: return malicious

0, D0, STA, FA, STD, FD, C, α, γ)

Algorithm 8 Proactive defense vs. adaptive attack evaluation
Eva(MLA, M0, D†
INPUT: detection scheme M0 (learned from D′
0, as in Algorithm 7),
D†
0 is set of feature vectors for defender’s proactive training, D0 =
D0.malicious ∪ D0.benign, STA (STD) is attacker’s (defender’s)
adaptation strategy, FA (FD) is attacker’s (defender’s) manipulation
algorithm, C is the constraints, α (γ) is the number of attacker’s
(defender’s) adaptation rounds
OUTPUT: ACC, FN, TP and FP
1: if α > 0 then
2: Dα ← AA(MLA, M0, D0, STA, C, FA, α)

{call Algorithm 1}

3: end if
4: M †

1 , . . . , M †

γ ← PT(MLA, M0, D†

0, STD, C, FD, γ)

{call Algorithm 7}

5: malicious ← PD(MLA, M0, D†

0, Dα, STD, C, FD, γ)

{call Algorithm 6}

6: benign ← Dα \ malicious
7: calculate ACC, FN, TP and FP w.r.t. D0
8: return ACC, FN, TP and FP

where D′
0.benign is a set of websites that are known to
be benign (ground truth) and cannot be manipulated by the
attacker, D′
0.malicious is a set of websites that are known
to be malicious (ground truth). A website is classiﬁed as
malicious if the non-proactive detection scheme M0 classiﬁes it
as malicious, or at least ⌊(γ+1)/2⌋+1 of the proactively trained
detection schemes classify it as malicious. This is meditated to
accommodate that the defender does not know a priori whether
the attacker is adaptive or not, When the attacker is not adaptive,
M0 can effectively deal with D0.

Algorithm 7 describes the proactive training algorithm. This
algorithm is similar to the adaptive attack algorithm AA because
it also consider three kinds of adaptation strategies. Speciﬁcally,
this algorithm aims to derive detection schemes M †
1 , . . . , M †
γ
from the starting-point detection scheme M0.

Essentially, the evaluation algorithm calls the defender’s protec-
tion detection to generate a set of proactively trained detection
schemes, and calls the attacker’s adaptive attack algorithm
to manipulate the malicious websites (i.e., selecting some of
their features and manipulating their values to evade a given
detection scheme). By varying the adaptation strategies and
the parameters, we can evaluate the effectiveness of proactive
training & detection against the adaptive attacks. The param-
eter space of the evaluation algorithm includes at least 108
scenarios: the basic adaptation strategy space STA × STD is
3×3 (i.e., not counting any hybrids of parallel-adaptation,
sequential-adaptation and full-adapatation), the ma-
nipulation algorithm space FA × FB is 2 × 2, and the adaptation
round parameter space is at least 3 (α >, =, < γ).

Proactive

training

IV. EVALUATING EFFECTIVENESS OF THE
COUNTER-EVASION ALGORITHMS

γ respectively as empty sets

0, STD, C, FD, γ)

7

Algorithm
PT(MLA, M0, D†
INPUT: same as in Algorithm 6
OUTPUT: M †
1: M †
2: initialize D†

1 , . . . , M †
γ

1, . . . , D†
and empty classiﬁers

0 ← M0 {for simplifying notations}
1 , . . . , M †

γ and M †

if STD == parallel-adaptation then

i .malicious ← FD(M †

0.malicious, C)
else if STD == sequential-adaptation then

0 , D†

i .malicious ← FD(M †

i−1, D†

i−1.malicious, C)

else if STD == full-adaptation then

i−1.malicious ← PP(D†
i .malicious ← FD(M †

i−1, D†

0, . . . , D†

i−2)
i−1, C)

D†

D†

3: for i=1 to γ do
4:
5:
6:
7:
8:
9:
10:
11:
12: D†
13: M †
14: end for
15: return M †

D†
D†
end if

i .benign ← D†
0.benign
i ← MLA(D†
i )

1 , . . . , M †
γ

We use the standard metrics, including false-negative and
false-positive rates [30], to evaluate the effectiveness of counter-
evasion algorithms.

A. Data Description

downloaded

consists of

zeustracker.abuse.ch
further

The dataset used in this paper
are

a 40-
day URLs. Malicious URLs
from
blacklists: compuweb.com/url-domain-bl.txt, malware.com.br,
and
malwaredomainlist.com,
spyeyetracker.abuse.ch and
the
high-interaction client honeypot Capture-HPC[24]. Benign
URLs are obtained from alexa.com, which lists the top 10,000
websites that are supposed to be well protected. The test
of blacklist URLs using high-interaction client honeypot
conﬁrmed our observation that some or many blacklist URLs
are not accessible any more and thus should not be counted as
malicious URLs.

conﬁrmed with

Algorithm 8 describes the algorithm for evaluating the ef-
fectiveness of the counter-measure against the adaptive attacks.

The daily average number of malicious websites listed in
blacklists is 6763 and the daily average number of malicious

websites after being veriﬁed by Capture-HPC is 838. The total
number of distinct malicious websites we found in 40 days
are 17091. The daily average number of benign websites is
10,000. By eliminating non-accessible benign websites,
the
daily average number of benign websites is 9501 in 40 days.
According to [26] and our experiment results, it can achieve
best detection rate, when the rate between the number of benign
websites and malicious websites are 4:1. So we choose all
malicious websites and 4 times of benign websites as our
training and testing data.

I

the

B. Effectiveness of the Evasion Attacks
summarizes

Table

adaptive

results of

attack
AA(“J48”, M0, D0, ST, C, F, α = 1) based on the 40-day
dataset mentioned above. The experiment can be more suc-
cinctly represented as M0(D1), meaning that the defender is
static (or non-proactive) and the attacker is adaptive with α = 1,
where D1 is the manipulated version of D0. Note that in the
case of α = 1, the three adaptation strategies lead to the
same D1 as shown in Figure 2. From Table I, we ﬁnd that
both manipulation algorithms can effectively evade detection
by manipulating on average 4.31-7.23 features while achieving
false-negative rate 87.6%-94.7% for F1, and by manipulating
on average 4.01-6.19 features while achieving false-negative
rate 89.1%-95.3% for F2

TABLE I
EXPERIMENT RESULTS WITH M0(D1) IN TERMS OF AVERAGE
FALSE-NEGATIVE RATE (FN), AVERAGE NUMBER OF MANIPULATED
FEATURES (#MF), AVERAGE PERCENTAGE OF FAILED ATTEMPTS (FA).

Detection Scheme
J48 Decision Tree

FN
87.6%

F1
#MF
7.23

FA
12.6%

FN
89.1%

F2
#MF
6.19

FA
11.0%

Having observed the phenomenon that manipulation of some
features’ values can essentially make the detection schemes
useless, it would be natural to ask which features are often
manipulated for evasion? To look into the question, we notice
that many features are manipulated over the 40 days, but only
a few are manipulated often.

F1 most often (i.e., > 150 times each day for over
features
the 40 days) manipulates three application-layer
— URL_length, Content_length, #Embedded_URLs
features — Duration and
— and two network-layer
#Local_app_packet. On the other hand, F2 most often
(i.e., > 150 times) manipulates two application-layer features
— #Special_characters and Content_length —
and one network-layer feature — Duration.

The above discrepancy between the frequencies that features
are manipulated can be attributed to the design of the manipu-
lation algorithms. Speciﬁcally, F1 seeks to manipulate features
that are associated to nodes that are close to the leaves. In
contrast, F2 emphasizes on the mismatches between a malicious
feature vector and an entire benign path, which represents a
kind of global search and also explains why F2 manipulates
fewer features.

We also want

to know why these features have such
high security/evasion signiﬁcance? The issue is important

because identifying the “important” features could lead to
deeper insights. We compare the manipulated features to the
features that would be selected by a feature selection algorithm
for the purpose of training classiﬁers. To be speciﬁc, we use
the InfoGain feature selection algorithm because it ranks the
contributions of individual features [30]. We ﬁnd that among
the manipulated features, URL_length is the only feature
among the ﬁve InfoGain-selected application-layer features,
and #Dist_remote_TCP_port is the only feature among
the four InfoGain-selected network-layer features. This sug-
gests that the feature selection algorithm does not necessarily
offer good insights into the importance of features from a
security perspective.

The standard feature-selection algorithms are almost useless
to ﬁnd indicative features from the perspective of evading
classiﬁers. There is still gap between our results and the
“optimal” solutions based on security semantics; it’s an open
problem to bridge the gap, because classiﬁers are “black-box”
that don’t really accommodate “security semantics” of features.

C. Effectiveness of the Counter-Evasion Algorithms

Table II summarizes the effectiveness of proactive defense
against adaptive attacks. We make the following observations.
First, if the defender is proactive (i.e., γ > 0) but the attacker
is non-adaptive (i.e., α = 0), the false-negative rate drops from
0.79% in the baseline case to some number belonging to inter-
val [0.23%, 0.56%]. The price is: the detection accuracy drops
from 99.68% in the baseline case to some number belonging
to interval [99.23%, 99.68%] the false-positive rate increases
from 0.14% in the baseline case to some number belonging
to interval [0.20%, 0.93%], The above observations suggest:
the defender can always use proactive detection without
worrying about side-effects (e.g., when the attacker is not
adaptive). This is because the proactive detection algorithm
PD uses M0(D0) as the ﬁrst line of detection.

1 , . . . , M †

Second, when STA = STD 6= 0, it has a signiﬁcant impact
whether or not they use the same manipulation algorithm. This
phenomenon also can be explained by that the features that are
often manipulated by F1 are very different from the features that
are often manipulated by F2. More speciﬁcally, when FA = FD,
the proactively learned classiﬁers M †
γ would capture
the “maliciousness” information embedded in the manipulated
data Dα; this would not be true when FA 6= FD. Moreover, the
sequential adaptation strategy appears to be more “oblivious”
than the other two strategies in the sense that Dα preserves
less information about D0. what adaptation strategy should the
defender use to counter STA = sequential? Table III shows
that the attacker does not have an obviously more effective
counter full adaptation strategy. This hints that the full
strategy may be a kind of equilibrium strategy because both
attacker and defender have no signiﬁcant gains by deviating
from it. This inspires an important problem for future research:
it) an
Is the full adaptation strategy (or variant of
equilibrium strategy?

Third, Table II shows that when STD = STA, the attacker
can beneﬁt by increasing its adaptiveness α. Table III exhibits

TABLE II
CROSS-LAYER PROACTIVE DETECTION WITH STA = STD . FOR BASELINE CASE M0(D0), ACC = 99.68%, TRUE-POSITIVE RATE TP =99.21%,
FALSE-NEGATIVE RATE FN=0.79%, AND FALSE-POSITIVE RATE FP=0.14%.

Strategy

M0-8(D0)

= parallel

STA = STD

STA = STD

Manipulation algorithm
FD = F1 vs. FA = F1
FD = F1 vs. FA = F2
FD = F2 vs. FA = F1
FD = F2 vs. FA = F2
FD = F1 vs. FA = F1
FD = F1 vs. FA = F2
FD = F2 vs. FA = F1
FD = F2 vs. FA = F2
FD = F1 vs. FA = F1
FD = F1 vs. FA = F2
FD = F2 vs. FA = F1
FD = F2 vs. FA = F2

FP
0.39
0.77
0.93
0.39
0.45
0.82
0.80
0.50
0.20
0.72
0.40
0.28
TABLE III
PROACTIVE DETECTION AGAINST ADAPTIVE ATTACKS WITH FD = FA . FOR THE BASELINE CASE M0(D0), WE HAVE ACC = 99.68%, TP =99.21%,
FN=0.79%, FP=0.14%.

M0-8(D1)
TP
92.03
25.50
19.32
90.25
77.48
20.88
29.03
78.70
96.32
40.32
51.84
95.60

M0-8(D9)
TP
92.00
32.18
39.77
92.77
59.33
30.03
40.93
62.30
92.03
29.99
72.99
90.09

FP
3.83
11.48
12.14
3.08
2.99
9.38
7.83
2.11
3.27
11.00
9.01
2.83

ACC
99.59
99.27
99.16
99.59
99.52
99.23
99.27
99.52
99.68
99.27
99.60
99.68

ACC
95.39
78.11
78.96
96.17
92.04
79.43
82.72
92.04
95.73
78.11
87.61
95.73

TP
99.71
99.77
99.76
99.62
99.69
99.70
99.67
99.53
99.44
99.58
99.66
99.60

FP
3.62
9.88
11.17
5.59
3.05
14.06
12.33
2.10
2.89
4.38
6.93
2.88

FN
8.00
67.82
60.23
7.23
30.67
69.97
59.07
37.70
7.97
70.01
27.01
9.91

FN
7.97
74.50
80.68
9.75
22.52
79.22
70.97
21.30
3.68
59.68
48.16
4.40

ACC
95.58
78.51
76.33
93.66
93.44
74.24
77.14
93.44
96.92
85.68
85.65
96.92

FN
0.29
0.23
0.24
0.38
0.31
0.30
0.33
0.47
0.56
0.42
0.34
0.40

STA = STD

= sequential

= full

STA = parallel
FN

STA = sequential

FN

FP

ACC

STA = full
FN

TP

STD vs. STA

M0-γ (Dα)

STD = parallel

STD = sequential

STD = full

STD = parallel

STA = sequential

STA = full

M0-8(D1)
M0-8(D9)
M0-8(D1)
M0-8(D9)
M0-8(D1)
M0-8(D9)

M0-8(D1)
M0-8(D9)
M0-8(D1)
M0-8(D9)
M0-8(D1)
M0-8(D9)

ACC

95.58
95.39
92.15
89.20
96.24
94.73

93.66
96.17
90.86
88.43
95.69
96.06

92.03
92.00
74.22
58.39
94.98
90.01

ACC

94.25
92.38
93.44
92.04
96.46
94.70

7.97
8.00
25.78
41.61
5.02
9.99

TP
TP
FP
Manipulation algorithm FD = FA = F1
90.89
3.62
80.03
3.83
77.48
3.93
59.33
4.11
94.99
3.42
90.03
4.21
Manipulation algorithm FD = FA = F2
88.91
5.59
77.89
3.08
78.70
4.82
62.30
3.97
94.98
3.88
90.99
2.89

9.75
7.23
29.02
46.68
6.11
8.54

94.25
92.38
93.44
92.04
96.46
94.70

90.25
92.77
70.98
53.32
93.89
91.46

FP

4.32
4.54
3.07
3.91
2.89
3.27

3.53
3.38
4.02
3.17
2.88
2.83

94.91
93.19
92.79
88.42
96.92
95.73

94.91
93.19
92.79
88.42
96.92
95.73

92.08
84.32
76.32
57.89
96.32
92.03

89.77
81.32
72.32
57.88
95.60
90.09

7.92
15.68
23.68
42.11
3.68
7.97

10.23
18.68
27.68
42.12
4.40
9.91

9.11
19.97
22.52
30.67
5.01
9.97

11.09
22.11
21.30
37.70
5.02
9.01

4.96
4.89
3.05
2.99
3.15
4.23

3.98
4.32
2.10
2.11
3.03
2.32

 96

)

 96

)

 93

)

%

(

 94.5

y
c
a
r
u
c
c
a

 93

 91.5

PAR  vs.  PAR
PAR  vs.  SEQ

PAR  vs.  FULL

%

(

y
c
a
r
u
c
c
a

 90

 87

 84

SEQ  vs.  PAR
SEQ  vs.  SEQ

-3 -2

-1

 0
γ - α

 1

 2

 3

 4

 5

-5

-4

-3

-2

-1

%

(

y
c
a
r
u
c
c
a

 95

 94

SEQ  vs.  FULL

 1

 2

 3

 4

 5

 93

-5

 0
γ - α

FULL  vs.  PAR
FULL  vs.  SEQ

-4

-3

-2

-1

FULL  vs.  FULL

 1

 2

 3

 4

 5

 0
γ - α

 90

-5 -4

(a) Fixed defender adaptation strategy against varying attacker adaptation strategies, where both the attacker and the defender use manipulation
algorithm F1. We observe that the FULL adaptation strategy leads to relatively better detection accuracy.

 96

 93

 96

)

%

(

 94.5

y
c
a
r
u
c
c
a

 93

 91.5

PAR  vs.  PAR
PAR  vs.  SEQ

PAR  vs.  FULL

)

%

(

y
c
a
r
u
c
c
a

 90

 87

 84

SEQ  vs.  PAR
SEQ  vs.  SEQ

-3 -2

-1

 0
γ - α

 1

 2

 3

 4

 5

-5

-4

-3

-2

-1

)

%

(

y
c
a
r
u
c
c
a

 95

 94

SEQ  vs.  FULL

 1

 2

 3

 4

 5

 93

-5

 0
γ - α

FULL  vs.  PAR
FULL  vs.  SEQ

-4

-3

-2

-1

FULL  vs.  FULL

 1

 2

 3

 4

 5

 0
γ - α

 90

-5 -4

(b) Fixed defender adaptation strategy against varying attacker adaptation strategies, where both the attacker and the defender use manipulation
algorithm F2. We observe that the FULL adaptation strategy leads to relatively better detection accuracy.

Impact of defender’s proactiveness γ vs. attacker’s adaptiveness α on detection accuracy (average over the 40 days) under various “STD × STA”
Fig. 4.
combinations, where α ∈ [0, 8], γ ∈ [0, 9], PAR, SEQ and FULL respectively stand for parallel, sequential and full adaptation strategy, “SEQ vs. APR”
means STD = sequential and STA = parallel etc. Note that γ − α = a is averaged over all possible combinations of (α, γ) as long as α ∈ [0, 8] and
γ ∈ [0, 9], and that the detection accuracy is averaged over the 40 days. We observe that the detection accuracy in most cases there is a signiﬁcant increase in
detection accuracy when the defender’s proactiveness matches the attacker’s adaptiveness.

 
 
 
 
 
 
the same phenomenon when STD 6= STA. In order to see the
impact of defender’s proactiveness as reﬂected by γ against
the defender’s adaptiveness as reﬂected by α, we plot
in
Figure 4 how the detection accuracy with respect to (γ − α)
under the condition FD = FA and under various STD × STA
combinations. We observe that roughly speaking, as γ increases
to exceed the attacker’s adaptiveness α (i.e., γ changes from
γ < α to γ = α to γ > α), the detection accuracy may have
a signiﬁcant increase at γ − α = 0. Moreover, we observe
that when STD = full, γ − α has no signiﬁcant impact on
the detection accuracy. This suggest that the defender should
always use the full adaptation strategy to alleviate the
uncertainty about the attacker’s adaptiveness α.

V. RELATED WORK

The problem of malicious websites detection has been an
active research topic (e.g., [3], [5], [11]). The dynamic detection
approach has been investigated in [7], [31], [4], [12]. The static
detection approach has been investigated in [2], [6], [16]. The
hybrid dynamic-static approach has been investigated in [3],
[30], [9]. Loosely related to the problem of malicious websites
detection are the detection of Phishing websites [16], [17],
[10], the detection of spams [26], [28], [21], [16], [17], the
detection of suspicious URLs embedded in twitter message
streams [23], and the detection of browser-related attacks [25],
[13]. However, none of these studies considered the issue of
evasion by adaptive attackers.

The evasion attack is closely related to the problem of
adversarial machine learning, where the attacker aims to evade
an detection scheme that is derived from some machine learning
method [1], [27]. Perdisci et al. [19] investigated how to make
the detection harder to evade. Nelson et al. [18] assumed the
attacker has black-box access to the detection mechanism. Dalvi
et al. [8] used Game Theoretic method to study this problem
in the setting of spam detection by assuming the attacker has
access to the detection mechanism. Our model actually gives
attacker more freedom because the attacker knows the data
defender collected.

VI. CONCLUSION AND FUTURE WORK

We formulated a model of adaptive attacks by which the
attacker can manipulate the malicious websites to evade detec-
tion. We also formulated a model of proactive defense against
the adaptive attacks. Experimental results based on a 40-day
dataset showed that adaptive attacks can evade non-proactive
defense, but can be effectively countered by proactive defense.
In the full version of the present paper, we will address
correlation constraints between features, which is omitted
due to space limitation. This study also introduces a set of
interesting research problems, including: Are the same kinds
of results/insights applicable to classiﬁers other than decision
trees? Is the full adaptation strategy indeed a kind of equi-
librium strategy? What is the optimal manipulation algorithm
(if exists)? How can we precisely characterize the evadability
caused by adaptive attacks in this context? What is the optimal

time resolution at which the defender should proactively train
its detection schemes (e.g., hour or day)?

Acknowledgement. This work was supported in part by ARO
Grant #W911NF-13-1-0141. This study was approved by IRB.

REFERENCES

[1] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar. Can
machine learning be secure? In ASIACCS’06, pages 16–25, 2006.

[2] P. K. C. Seifert, I. Welch.

Identiﬁcation of malicious web pages with

static heuristics. In ATNAC 2008, pages 91–96.

[3] D. Canali, M. Cova, G. Vigna, and C. Kruegel. Prophiler: a fast ﬁlter for

the large-scale detection of malicious web pages. In WWW’11.

[4] K. Z. Chen, G. Gu, J. Nazario, X. Han, and J. Zhuge. WebPatrol:
Automated collection and replay of web-based malware scenarios.
In
ASIACCS’11, 2011.

[5] H. Choi, B. B. Zhu, and H. Lee. Detecting malicious web links and

identifying their attack types. In WebApps’11.

[6] P. K. C. U. A. Christian Seifert, Lan Welch and B. Endicott-Popovsky.
Identiﬁcation of malicious web pages through analysis of underlying dns
and web server relationships. In LCN 2008, pages 935–941.

[7] M. Cova, C. Kruegel, and G. Vigna. Detection and analysis of drive-by-

download attacks and malicious javascript code. In WWW’10.

[8] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. Adversarial

classiﬁcation. In KDD’04, pages 99–108, 2004.

[9] B. Eshete. Effective analysis, characterization, and detection of malicious

web pages. WWW ’13, pages 355–360.

[10] S. Garera, N. Provos, M. Chew, and A. D. Rubin. A framework for

detection and measurement of phishing attacks. In WORM’07.

[11] L. Invernizzi, S. Benvenuti, M. Cova, P. M. Comparetti, C. Kruegel, and
G. Vigna. Evilseed: A guided approach to ﬁnding malicious web pages.
In S&P’12, pages 428–442.

[12] A. Kapravelos, Y. Shoshitaishvili, M. Cova, C. Kruegel, and G. Vigna.
Revolver: An Automated Approach to the Detection of Evasive Web-
based Malware. In USENIX Security, 2013.

[13] G. Y. Lei Liu, Xinwen Zhang and S. Chen. Chrome extensions: Threat

analysis and countermeasures. In NDSS’13.

[14] Z. Li, K. Zhang, Y. Xie, F. Yu, and X. Wang. Knowing your enemy:
Understanding and detecting malicious web advertising. In CCS’12.
[15] C. Ludl, S. Mcallister, E. Kirda, and C. Kruegel. On the effectiveness of

techniques to detect phishing sites. In DIMVA’07.

[16] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Beyond blacklists:
learning to detect malicious web sites from suspicious urls. In KDD’09.
[17] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Identifying suspicious

urls: an application of large-scale online learning. In ICML’09.

[18] B. Nelson, B. I. P. Rubinstein, L. Huang, A. D. Joseph, and J. D. Tygar.
Classiﬁer evasion: models and open problems. In ECML PKDD’11.
[19] R. Perdisci, G. Gu, and W. Lee. Using an ensemble of one-class
svm classiﬁers to harden payload-based anomaly detection systems. In
ICDM’06, pages 488–498.

[20] N. Provos, P. Mavrommatis, M. A. Rajab, and F. Monrose. All your

iframes point to us. In USENIX Security, 2008.

[21] Z. Qian, Z. M. Mao, Y. Xie, and F. Yu. On network-level clusters for

spam detection. In NDSS’10.

[22] R. Quinlan. C4.5:Programs for Machine Learning. 1993.
[23] J. K. Sangho Leey. Warningbird: Detecting suspicious urls in twitter

stream. In NDSS’12.

[24] C. Seifert and R. Steenson. Capture - Honeypot Client (Capture-HPC).

https://projects.honeynet.org/capture-hpc, 2006.

[25] V. S. Sooel Son. The postman always rings twice: Attacking and

defending postmessage in html5websites. In NDSS’13.

[26] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song. Design and
Evaluation of a Real-Time URL Spam Filtering Service. In S&P’11.
[27] S. Venkataraman, A. Blum, and D. Song. Limits of learning-based

signature generation with adversaries. In NDSS’08.

[28] C. Whittaker, B. Ryner, and M. Nazif. Large-scale automatic classiﬁcation

of phishing pages. In NDSS’10.

[29] G. Xiang, J. Hong, C. P. Rose, and L. Cranor. Cantina+: A feature-
rich machine learning framework for detecting phishing web sites. ACM
Trans. Inf. Syst. Secur., 2011.

[30] L. Xu, Z. Zhan, S. Xu, and K. Ye. Cross-layer detection of malicious

websites. In ACM CODASPY’13, pages 141–152.

[31] J. Zhang, C. Seifert, J. W. Stokes, and W. Lee. Arrow: Generating

signatures to detect drive-by downloads. In WWW’11, pages 187–196.

 100

)

%

(

e
t
a
r

e
v
i
t
i
s
o
p
-
e
u
r
t

 95

 90

 85

FULL  vs.  PAR
FULL  vs.  SEQ

-4

-3

-2

-1

 80

-5

FULL  vs.  FULL

 1

 2

 3

 4

 5

 0
α - γ

 
 
 100

)

%

(

e
t
a
r

e
v
i
t
i
s
o
p
-
e
u
r
t

 95

 90

 85

FULL  vs.  PAR
FULL  vs.  SEQ

-4

-3

-2

-1

 80

-5

FULL  vs.  FULL

 1

 2

 3

 4

 5

 0
α - γ

 
 
 "
 "

%&’()*)+
%&’()*)+

:(’*)*)+9@’A’
#$"

#$"

6789&8:%’;;*<*&=

:;9<&9=%’>>*?*&@

#"

#"

 !
 !

,
,
,
,
,
,
 *
  .!

%&’()*)+
%&’()*)+

#!./0 "1#"2

#!/ 0 "1#"123456

#3./0 !1#"1#!2
#7/ 0 !18(&80#"1#!6123456

%&’()*)+
%&’()*)+

-

-

-

#*./0 *4!1#"1-1#*4!2
# .!/ 0  .718(&80#"1-1# .76123456

#*5!./0 *1#"1-1#*2
# 9/ 0  .!18(&80#"1-1# .!6123456

Naive Bayes
SVM
 16

 8

Logistic 
J48

 24

 32

time

 100

)

%

(

y
c
a
r
u
c
c
a
n
o
i
t
c
e
t
e
d

 98

 96

 94

 92

 90

 0

 
 
Naive Bayes
SVM

Logistic 
J48

 20

 16

 12

 8

 4

)

%

(

t

e
a
r

e
v
i
t

a
g
e
n

e
s
a
f

l

 0

 0

 8

 16

time

 24

 32

 
 
 
Naive Bayes
SVM

Logistic 
J48

 3

 2

 1

)

%

(

t

e
a
r
e
v
i
t
i
s
o
p

e
s
a
f

l

 0

 0

 8

 16

time

 24

 32

 
 
 
(cid:30)(cid:11)(cid:15)(cid:0)(cid:5) (cid:16)(cid:18)(cid:19)

5(cid:4)(cid:5)(cid:4)

(cid:6)(cid:7)(cid:8)(cid:8)(cid:9)(cid:6)(cid:5)(cid:10)(cid:7)(cid:11)

(cid:1)(cid:0)(cid:2) (cid:3)(cid:4)(cid:5)(cid:4) (cid:6)(cid:7)(cid:8)(cid:8)(cid:9)(cid:6)(cid:5)(cid:10)(cid:7)(cid:11) (cid:12)(cid:13)(cid:12)(cid:5)(cid:9)(cid:14)

(cid:22)(cid:2)(cid:7)(cid:12)(cid:12)(cid:24)(cid:8)(cid:4)(cid:13)(cid:9)(cid:2) (cid:10)(cid:11)4(cid:7)(cid:2)(cid:14)(cid:4)(cid:5)(cid:10)(cid:7)(cid:11)

(cid:7)4 1(cid:9)(cid:26)(cid:15)(cid:4)

(cid:9)(cid:12)

6

3(cid:4)(cid:12)(cid:5) (cid:27)

(cid:12)(cid:6)(cid:4)(cid:8)(cid:4)(cid:26)(cid:8)(cid:9)

4(cid:2)(cid:7)(cid:11)(cid:5)(cid:24)(cid:9)(cid:11)(cid:3)

(cid:28)(cid:29)(cid:31) ! (cid:29)"# $%&(cid:31)(cid:31)’!()(cid:29)% *(cid:29)#(cid:29)$# &"

&+ ,(! $ &-(cid:31) .(cid:29)/(cid:31) #(cid:29)(cid:31)

(cid:4)(cid:11)(cid:4)(cid:8)(cid:13)(cid:12)(cid:10)(cid:12)

(cid:17)(cid:0)(cid:12)(cid:15)(cid:10)(cid:6)(cid:10)(cid:7)(cid:0)(cid:12) (cid:16)(cid:18)(cid:19)(cid:12)

(cid:1)(cid:15)(cid:5)(cid:10)(cid:7)(cid:11)(cid:4)(cid:8) (cid:26)(cid:4)(cid:6)(cid:21)(cid:24)

(cid:9)(cid:11)(cid:3) (cid:3)(cid:9)(cid:9)(cid:15)(cid:9)(cid:2)

(cid:22)(cid:8)(cid:10)(cid:9)(cid:11)(cid:5) (cid:20)(cid:7)(cid:11)(cid:9)(cid:13)(cid:15)(cid:7)(cid:5)(cid:24)(cid:8)(cid:10)(cid:21)(cid:9)

0(cid:12)(cid:8)(cid:7)1(cid:9)(cid:2)2

(cid:4)(cid:11)(cid:4)(cid:8)(cid:13)(cid:12)(cid:10)(cid:12)

(cid:26)(cid:9)(cid:23)(cid:4)(cid:25)(cid:10)(cid:7)(cid:2)(cid:24)(cid:26)(cid:4)(cid:12)(cid:9)(cid:3) (cid:3)(cid:9)(cid:5)(cid:9)(cid:6)(cid:5)(cid:10)(cid:7)(cid:11) (cid:27)

(cid:15)(cid:7)(cid:12)(cid:12)(cid:10)(cid:26)(cid:8)(cid:13) (cid:26)(cid:10)(cid:11)(cid:4)(cid:2)(cid:13) (cid:4)(cid:11)(cid:4)(cid:8)(cid:13)(cid:12)(cid:10)(cid:12)

 100

)

%

(

e
t
a
r

e
v
i
t
i
s
o
p
-
e
u
r
t

 95

 90

 85

 80

 75

 70

-5

PAR  vs.  PAR
PAR  vs.  SEQ

-4

-3

-2

-1

PAR  vs.  FULL

 1

 2

 3

 4

 5

 0
α - γ

 
 
 100

)

%

(

e
t
a
r

e
v
i
t
i
s
o
p
-
e
u
r
t

 95

 90

 85

 80

 75

 70

-5

PAR  vs.  PAR
PAR  vs.  SEQ

-4

-3

-2

-1

PAR  vs.  FULL

 1

 2

 3

 4

 5

 0
α - γ

 
 
*+,-./.0
./012324

5678+79*,::/;/+<
:;9</9=.0>>3?3/@

*+,-./.0

*+,-./.0

 &
%&

 !

1
1
1

5
5
5

 /

:10323249@0A0
")&

 -&

"&

 &

"!#$% &’"&(
 !"#$%&’ & ()*+,

"3#$% &’"&(
 7"#$%&’ & ()*+,

2

6
"/#$% &’"&(
  8!"#$%&’ & ()*+,

"/4!#$% &’"&(
  9"#$%&’ &’()*+,

SEQ  vs.  PAR
SEQ  vs.  SEQ

-4

-3

-2

-1

SEQ  vs.  FULL

 1

 2

 3

 4

 5

 0
α - γ

 100

)

%

(

e
t
a
r

e
v
i
t
i
s
o
p
-
e
u
r
t

 80

 60

 40

 20

 0

-5

 
 
 100

)

%

(

e
t
a
r

e
v
i
t
i
s
o
p
-
e
u
r
t

 80

 60

 40

 20

 0

-5

SEQ  vs.  PAR
SEQ  vs.  SEQ

-4

-3

-2

-1

SEQ  vs.  FULL

 1

 2

 3

 4

 5

 0
α - γ

 
 
 "
 "

 !
 !

,
,
,
,
,
,

 *
  .!

%&’()*)+

%&’()*)+

9(’*)*)+7?’@’
#$"

#$"

7839&3:%’;;*<*&=

9:7;&7<%’==*>*&?

#"

#"

%&’()*)+

%&’()*)+

#!./0 "1#"2
#!/ 0 "1#"!23456

3#4./0 !1#!2
7#8/ 0 !1#!!23456

%&’()*)+

%&’()*)+

-
-
# .!/ 0  .81# .8!23456

#*./0 *5!1#*5!2

#*6!./0 *1#*2

# / 0  .!1# .!!23456

