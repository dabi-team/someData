0
2
0
2

l
u
J

0
2

]

G
L
.
s
c
[

2
v
7
2
8
3
0
.
2
0
0
2
:
v
i
X
r
a

Chapter 1

Manipulating Reinforcement

Learning: Stealthy Attacks on

Cost Signals

Yunhan Huang1* and Quanyan Zhu1

1Department of Electrical and Computer Engineering, New York University, 11201, 370 Jay
Street, Brooklyn, New York, USA

*Corresponding Author: Yunhan Huang; yh.huang@nyu.edu

Abstract: This chapter studies emerging cyber attacks on reinforcement

learning (RL) and introduces a quantitative approach to analyze the vulner-

abilities of RL. Focusing on adversarial manipulation on the cost signals, we

analyze the performance degradation of Temporal Diﬀerence (TD) learning

and Q-learning algorithms under the manipulation. For TD learning, the

approximation learned from the manipulated costs has an approximation

error bounded by a constant times the magnitude of the attack. The eﬀect

of the adversarial attacks on the bound does not depend on the choice of

λ, the weighting factor. In Q-learning, we show that Q-learning algorithms

converge under stealthy attacks and bounded falsiﬁcations on cost signals.

We characterize the relation between the falsiﬁed cost and the Q-factors as

well as the policy learned by the learning agent which provides fundamen-

tal limits for feasible oﬀensive and defensive moves. We propose a robust

1

 
 
 
 
 
 
region in terms of the cost within which the adversary can never achieve

the targeted policy. We provide conditions on the falsiﬁed cost which can

mislead the agent to learn an adversary’s favored policy. A case study of

TD learning is provided to corroborate the results.

Keywords: Reinforcement Learning, Q-Learning, TD-Learning, Deception

and Counterdeception, Adversarial Learning.

Reinforcement learning (RL) is a powerful paradigm for online decision-

making in unknown environment. Recently, many advanced RL algorithms

have been developed and applied to various scenarios including video games

(e.g., Mnih et al. [2015]), transportation (e.g., Arel et al. [2010]), network secu-

rity (e.g., Huang and Zhu [2019a], Zhu and Ba¸sar [2009]), robotics (e.g., Kober

et al. [2013]) and critical infrastructures (e.g., Ernst et al. [2004]). However,

the implementation of RL techniques requires accurate and consistent feedback

from environment. It is straightforward to fulﬁll this requirement in simula-

tion while in practice, accurate and consistent feedback from the environment

is not guaranteed, especially in the presence of adversarial interventions. For

example, adversaries can manipulate cost signals by performing data injec-

tion attack and prevent an agent from receiving cost signals by jamming the

communication channel. With inconsistent and/or manipulated feedback from

environment, the RL algorithm can either fail to learn a policy or misled to a

pernicious policy. The failure of RL algorithms under adversarial intervention

can lead to a catastrophe to the system where the RL algorithm has been ap-

plied. For example, self-driving platooning vehicles can collide with each other

when their observation data are manipulated (see Behzadan and Munir [2019]);

drones equipped with RL techniques can be weaponized by terrorists to create

chaotic and vicious situations where they are commanded to collide to a crowd

or a building (see Huang and Zhu [2019b], Xu and Zhu [2015]).

2

Hence, it is imperative to study RL with maliciously intermittent or ma-

nipulated feedback under adversarial intervention. First, it is important to

understand the adversarial behaviors of the attacker. To do so, one has to

establish a framework that characterizes the objective of the attacker, the ac-

tions available to the attacker and the information at his disposal. Secondly, it

is also crucial to understand the impacts of the attacks on RL algorithms. The

problems include how to measure the impacts, how to analyze the behavior of

the RL algorithms under diﬀerent types of attacks and how to mathematically

and/or numerically compute the results of RL algorithms under attack. With

the understanding of the adversarial behavior and the impacts of the adver-

sarial behavior on RL algorithms, the third is to design defense mechanisms

that can protect RL algorithms from being degenerated. This could be done by

designing robust and secure RL algorithms that can automatically detect and

discard corrupted feedback, deploying cryptographic techniques to ensure con-

ﬁdentiality, integrity and building backup communication channels to ensure

availability.

Despite the importance of understanding RL in malicious setting, very few

works have studied RL with maliciously manipulated feedback or intermittent

feedback. One type of related works studies RL algorithms under corrupted

reward signals (see Everitt et al. [2017], Wang et al. [2018]). In Everitt et al.

[2017], the authors investigate RL problems where agents receive false rewards

from environment. Their results show that reward corruption can impede the

performance of agents, and can result in disastrous consequences for highly

intelligent agents. Another type of work studies delayed evaluative feedback

signals without the presence of malicious adversaries (see Tan et al. [2008],

Watkins [1989]). The study of RL under malicious attacks from a security point

3

of view appeared in the recent past (Huang and Zhu [2019b], Ma et al. [2019],

Lin et al. [2017], Behzadan and Munir [2018]). In Huang and Zhu [2019b], the

authors study RL under malicious falsiﬁcation on cost signals and introduces

a quantitative framework of attack models to understand the vulnerabilities

of RL. Ma et al. focuses on security threats on batch RL and control where

attacker aims to poison the learned policy (Ma et al. [2019]). Lin et al. [2017]

and Behzadan and Munir [2018] focus on deep RL which involves deep natural

networks (DNNs) for function approximation.

In this chapter, we ﬁrst introduce RL techniques built on a Markov decision

process framework and provide self contained background on RL before we dis-

cuss security problems of RL. Among RL techniques, of particular interest to

us are two frequently used learning algorithms: TD learning and Q-learning.

We then introduce general security concerns and problems in the ﬁeld of RL.

Security concerns arise from the fact that RL technique requires accurate and

consistent feedback from environment, timely deployed controls and reliable

agents, which are hard to guarantee under the presence of adversarial attacks.

The discussion of general security concerns in this chapter invokes a large num-

ber of interesting problems yet to be done. In this chapter, we focus on one

particular type of problems where the cost signals that the RL agent receives

are falsiﬁed or manipulated as a result of adversarial attacks. In this particular

type of problems, a general formulation of attack models is discussed by deﬁn-

ing the objectives, information structure and the capability of the adversary.

We analyze the attacks on both TD learning and Q-learning algorithms. We

develop important results that tell the fundamental limits of the adversarial at-

tacks. For TD learning, we characterize the bound on the approximation error

that can be induced by the adversarial attacks on the cost signals. The choice

4

of λ does not impact the bound of the induced approximation error. In the

Q-learning scenario, we aim to address two fundamental questions. The ﬁrst is

to understand the impact of the falsiﬁcation of cost signals on the convergence

of Q-learning algorithm. The second is to understand how the RL algorithm

can be misled under the malicious falsiﬁcations. This chapter ends with an

educational example that explains how the adversarial attacks on cost signals

can aﬀect the learning results in TD learning.

The rest of this chapter is organized as follows. Sec. 1.1 gives a basic intro-

duction of Markov decision process and RL techniques with a focus on TD(λ)

learning and Q-learning. In Sec. 1.2, we discuss general security concerns and

problems in the ﬁeld of RL. A particular type of attacks on the cost signals is

studied on both TD(λ) learning and Q-learning in Sec. 1.3. Sec. 1.4 comes an

educational example that illustrates the adversarial attacks on TD(λ) learning.

Conclusions and future works are included in Sec. 1.5.

1.1. Introduction of Reinforcement Learn-

ing

Consider an RL agent interacts with an unknown environment and attempts

to ﬁnd the optimal policy minimizing the received cumulative costs. The en-

vironment is formalized as a Markov Decision Process (MDP) denoted by

(cid:104)S, A, g, P, α(cid:105). The MDP has a ﬁnite state space denoted by S. Without loss of

generality, we assume that there are n states and S = {1, 2, · · · , n}. The state

transition depends on a control. The control space is also ﬁnite and denoted

by A. When at state i, the control must be chosen from a given ﬁnite subset of

5

A denoted by U (i). At state i, the choice of a control u ∈ U (i) determines the

transition probability pij(u) to the next state j. The state transition informa-

tion is encoded in P . The agent receives a running cost that accumulates addi-

tively over time and depends on the states visited and the controls chosen. At

the kth transition, the agent incurs a cost αkg(i, u, j), where g : S × A × S → R

is a given real-valued function that describes the cost associated with the states

visited and the control chosen, and α ∈ (0, 1] is a scalar called the discount

factor.

The agent is interested in policies, i.e., sequences π = {µ0, µ1, · · · } where

µk : S → A, k = 0, 1, · · · , is a function mapping states to controls with

µk(i) ∈ U (i) for all states i. Denote ik the state at time k. Once a policy

π is ﬁxed, the sequence of states ik becomes a Markov chain with transition

probabilities P (ik+1 = j|ik = i) = pij(µk(i)). In this chapter, we consider inﬁ-

nite horizon problems, where the cost accumulates indeﬁnitely. In the inﬁnite

horizon problem, the total expected cost starting from an initial state i and

using a policy π = {µ0, µ1, · · · } is

J π(i) = lim
N →∞

E

(cid:34) N
(cid:88)

k=0

αkg (ik, µk(ik), ik+1)

(cid:35)

i0 = i

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

the expected value is taken with respect to the probability distribution of the

Markov chain {i0, i1, i2, · · · }. This distribution depends on the initial state i0

and the policy π. The optimal cost-to-go starting from state i is denoted by

J ∗(i); that is,

J ∗(i) = min

π

J π(i).

We can view the costs J ∗(i), i = 1, 2, · · · , as the components of a vector

J ∗ ∈ Rn. Of particular interest in the inﬁnite-horizon problem are station-

6

ary policies, which are policies of the form π = {µ, µ, · · · }. The corresponding

cost-to-go vector is denoted by J µ ∈ Rn.

The optimal inﬁnite-horizon cost-to-go functions J ∗(i), i = 1, 2, 3, · · · , also

known as value functions, arise as a central component of algorithms as well

as performance metrics in many statistics and engineering applications. Com-

putation of the value functions relies on solving a system of equations

J ∗(i) = min
u∈U (i)

n
(cid:88)

j=1

pij(u)(g(i, u, j) + αJ ∗(j)),

i = 1, 2, · · · , n,

(1.1)

referred to as Bellman’s equation (Bertsekas and Tsitsiklis [1996], Sutton et al.

[1998]), which will be at the center of analysis and algorithms in RL. If µ(i)

attains the minimum in the right-hand side of Bellman’s equation (1.1) for

each i, then the stationary policy µ should be optimal (Bertsekas and Tsitsiklis

[1996]). That is, for each i ∈ S,

µ∗(i) = arg min

µ(i)∈U (i)

n
(cid:88)

j=1

pij (µ(i)) (g(i, µ(i), j) + αJ ∗(j)) .

The eﬃcient computation or approximation of J ∗ and an optimal policy

µ∗ is the major concern of RL. In MDP problems where the system model is

known and the state space is reasonably large, value iteration, policy iteration,

and linear programming are the general approaches to ﬁnd the value function

and the optimal policy. Readers unfamiliar with these approaches can refer to

Chapter 2 of Bertsekas and Tsitsiklis [1996].

It is well-known that RL refers to a collection of techniques for solving

MDP under two practical issues. One is the overwhelming computational re-

quirements of solving Bellman’s equations because of a colossal amount of

states and controls, which is often referred to as Bellman’s “curse of dimen-

7

sionality”. In such situations, an approximation method is necessary to obtain

sub-optimal solutions. In approximation methods, we replace the value func-

tion J ∗ with a suitable approximation ˜J(r), where r is a vector of parameters

which has much lower dimension than J ∗. There are two main function approxi-

mation architectures: linear and nonlinear approximations. The approximation

architecture is linear if ˜J(r) is linear in r. Otherwise, the approximation archi-

tecture is nonlinear. Frequently used nonlinear approximation methods include

polynomials based approximation, wavelet based approximation, and approxi-

mation using neural network. The topic of deep reinforcement learning studies

the cases where approximation ˜J(r) is represented by a deep neural network

(Mnih et al. [2015]).

Another issue comes from the unavailability of the environment dynamics;

i.e., the transition probability is either unknown or too complex to be kept in

memory. In this circumstance, one alternative is to simulate the system and the

cost structure. With given state space S and the control space A, a simulator

or a computer that generates a state trajectory using the probabilistic transi-

tion from any given state i to a generated successor state j for a given control

u. This transition accords with the transition probabilities pij(u), which is not

necessarily known to the simulator or the computer. Another alternative is to

attain state trajectories and corresponding costs through experiments. Both

methods allow the learning agent to observe their own behavior to learn how

to make good decisions. It is clearly feasible to use repeated simulations to

ﬁnd the approximate of the transition model of the system P and the cost

functions g by averaging the observed costs. This approach is usually referred

to as model-based RL. As an alternative, in model-free RL, transition prob-

abilities are not explicitly estimated, but instead the value function or the

8

approximated value function of a given policy is progressively calculated by

generating several sample system trajectories and associated costs. Of particu-

lar interest to us in this chapter is the security of model-free RL. This is because

ﬁrstly, model-free RL approaches are the most widely applicable and practical

approaches that have been extensively investigated and implemented; secondly,

model-free RL approaches receive observations or data from environment suc-

cessively and consistently. This makes model-free RL approaches vulnerable to

attacks. An attack can induce an accumulative impact on succeeding learning

process. The most frequently used algorithms in RL are TD learning algo-

rithms and Q-learning algorithms. Hence, in this chapter, we will focus on the

security problems of these two learning algorithms as well as their approximate

counterparts.

1.1.1. TD Learning

Temporal diﬀerence (TD) learning is an implementation of the Monte Carlo

policy evaluation algorithm that incrementally updates the cost-to-go estimates

of a given policy µ, which is an important sub-class of general RL methods.

TD learning algorithms, introduced in many references, including Bertsekas

and Tsitsiklis [1996], Sutton et al. [1998] and Tsitsiklis and Van Roy [1997],

generates an inﬁnitely long trajectory of the Markov Chain {i0, i1, i2, · · · } from

simulator or experiments by ﬁxing a policy µ, and at time t iteratively updates

the current estimate J µ

t of J µ using an iteration that depends on a ﬁxed scalar

λ ∈ [0, 1], and on the temporal diﬀerence

dt(ik, ik+1) = g(ik, ik+1) + αJ µ

t (ik+1) − J µ

t (ik), ∀t = 0, 1, · · · , ∀k ≤ t.

9

The incremental updates of TD(λ) have many variants. In the most straight-

forward implementation of TD(λ), all of the updates are carried out simultane-

ously after the entire trajectory has been simulated. This is called the oﬀ-line

version of the algorithm. On the contrary, in the on-line implementation of

the algorithm, the estimates update once at each transition. Under our dis-

count MDP, a trajectory may never end. If we use an oﬀ-line variant of TD(λ),

we may have to wait inﬁnitely long before a complete trajectory is obtained.

Hence, in this chapter, we focus on an on-line variant. The update equation

for this case becomes

t+1(i) = J µ
J µ

t (i) + γt(i)zt(i)dt(i),

∀i,

(1.2)

where the γt(i) are non-negative stepsize coeﬃcients and zt(i) is the eligibility

coeﬃcients deﬁned as

zt(i) =





αλzt−1(i),

if it (cid:54)= i,

αλzt−1(i) + 1,

if it = i.

This deﬁnition of eligibility coeﬃcients gives the every-visit TD(λ) method. In

every-visit TD(λ) method, if a state is visited more than once by the same

trajectory, the update should also be carried out more than once.

Under a very large number of states or controls, we have to resort to ap-

proximation methods. Here, we introduce TD(λ) with linear approximation

architectures. We consider a linear parametrization of the form:

r(k)φk(i).

˜J(i, r) =

K
(cid:88)

k=1

10

Here, r = (r(1), . . . , r(K)) is a vector of tunable parameters and φk(·) are

ﬁxed scalar functions deﬁned on the state space. The form can be written in a

compact form:

˜J(r) = ( ˜J(1, r), . . . , ˜J(n, r)) = Φr,

where



|

|

— φ(cid:48)(1) —







Φ =

φ1

· · · φK

· · ·

· · ·

· · ·

|

|

— φ(cid:48)(n) —









=

















,

with φk = (φk(1), . . . , φk(n)) and φ(i) = (φ1(i), . . . , φK(i)). We assume that Φ

has linearly independent columns. Otherwise, some components of r would be

redundant.

Let ηt be the eligibility vector for the approximated TD(λ) problem which

is of dimension K. With this notation, the approximated TD(λ) updates are

given by

where

rt+1 = rt + γtdtηt,

ηt+1 = αληt + φ(it+1).

(1.3)

(1.4)

Here, dt = g(it, it+1) + αr(cid:48)

tφ(it+1) − r(cid:48)

tφ(it).

The almost-sure convergence of rt generated by (1.3) and (1.4) is guaran-

teed if the conditions in Assumption 6.1 in Bertsekas and Tsitsiklis [1996] hold.

It will converge to the solution of

Ar + b = 0,

where A = Φ(cid:48)D(M − I)Φ and b = Φ(cid:48)Dq. Here, D is a diagonal matrix with

11

diagonal entries d(1), d(2), · · · , d(n), and d(i) is the steady-state probability of
state i; the matrix M is given by M = (1 − λ) (cid:80)∞
vector b is given by b = Φ(cid:48)Dq with q = (cid:80)∞
in Rn whose ith component is given by ¯g(i) = (cid:80)n

m=0 λm(αPµ)m+1 and the
m=0(αλPµ)m¯g, where ¯g is a vector

j=1 pij(µ(i))g(i, µ(i), j). The

matrix Pµ is the transition matrix deﬁned by Pµ := [Pµ]i,j = pij(µ(i)). A

detailed proof of the convergence is provided in Tsitsiklis and Van Roy [1997]

and Bertsekas and Tsitsiklis [1996].

Indeed, TD(λ) with linear approximations is a much more general frame-

work. The convergence of J µ
t

in TD(λ) without approximation follows imme-

diately if we let K = n and Φ = In where In is n × n identity matrix.

1.1.2. Q-Learning

Q-learning method is initially proposed in Watkins and Dayan [1992] which up-

dates estimates of the Q-factors associated with an optimal policy. Q-learning

is proven to be an eﬃcient computational method that can be used whenever

there is no explicit model of the system and the cost structure. First, we in-

troduce the ﬁrst notion of the Q-factor of a state-control pair (i, u), deﬁned

as

Q(i, u) =

n
(cid:88)

j=0

pij(u)(g(i, u, j) + αJ(j)).

(1.5)

The optimal Q-factor Q∗(i, u) corresponding to a pair (i, u) is deﬁned by (1.5)

with J(j) replaced by J ∗(j). It follows immediately from Bellman’s equation

that

Q∗(i, u) =

n
(cid:88)

j=0

(cid:18)

pij(u)

g(i, u, j) + α min
v∈U (j)

(cid:19)

Q∗(j, v)

.

(1.6)

12

Indeed, the optimal Q-factors Q∗(i, u) are the unique solution of the above

system by Banach ﬁxed-point theorem (see Kreyszig [1978]).

Basically, Q-learning computes the optimal Q-factors based on samples

in the absence of system model and cost structure. It updates the Q-factors

following

Qt+1(i, u) = (1 − γt)Qt(i, u) + γt

(cid:18)

g(i, u,¯i) + α min
v∈U (¯i)

(cid:19)

Qt(¯i, v)

,

(1.7)

where the successor state ¯i and g(i, u,¯i) is generated from the pair (i, u) by

simulation or experiments, i.e., according to the transition probabilities pi¯i(u).

For more intuition and interpretation about Q-learning algorithm, one can

refer to Chapter 5 of Bertsekas and Tsitsiklis [1996], Sutton et al. [1998] and

Watkins and Dayan [1992].

If we assume that stepsize γt satisﬁes (cid:80)∞

t=0 γt = ∞ and (cid:80)∞

t=0 γ2

t < ∞, we

obtain the convergence of Qt(i, u) generated by (1.7) to the optimal Q-factors

Q∗(i, u). A detailed proof of convergence in provided in Borkar and Meyn [2000]

and Chapter 5 of Bertsekas and Tsitsiklis [1996].

1.2. Security Problems of Reinforcement

Learning

Understanding adversarial attacks on RL systems is essential to develop eﬀec-

tive defense mechanisms and an important step toward trustworthy and safe

RL. The reliable implementation of RL techniques usually requires accurate

and consistent feedback from the environment, precisely and timely deployed

13

Figure 1.1: Attacks on RL systems: Adversaries can manipulate actions, costs, and

state signals in the feedback system. Agents can also be compromised in the learning

process.

controls to the environment and reliable agents (in multi-agent RL cases). Lack-

ing any one of the three factors will render failure to learn optimal decisions.

These factors can be used by adversaries as gateways to penetrate RL systems.

It is hence of paramount importance to understand and predict general adver-

sarial attacks on RL systems targeted at the three doorways. In these attacks,

the miscreants know that they are targeting RL systems, and therefore, they

tailor their attack strategy to mislead the learning agent. Hence, it is natural

to start with understanding the parts of RL that adversaries can target at.

Fig. 1.1 illustrates diﬀerent types of attacks on the RL system. One type of

attack aims at the state which is referred to as state attacks. Attacks on state

signals can happen if the remote sensors in the environment are compromised

14

or the communication channel between the agent and the sensors is jammed

or corrupted. In such circumstances, the learning agent may receive a false

state observation ˜ik of the actual state ik at time k and/or may receive a

delayed observation of the actual state or even never receive any information

regarding the state at time k. An example of eﬀortless state attacks is sequential

blinding/blurring of the cameras in a deep RL-based autonomous vehicle via

lasers/dirts on lens, which can lead to learning false policies and hence lead

to catastrophic consequences. Based on its impact on the RL systems, state

attacks can be classiﬁed into two groups: i) Denial of Service (DoS); and ii)

integrity attacks. The main purpose of DoS attacks is to deny access to sensor

information. Integrity attacks are characterized by the modiﬁcation of sensor

information, compromising their integrity.

Another type of attacks targets at cost signals. In this type of attacks, the

adversary aims to corrupt the cost signals that the learning agent has received

with a malicious purpose of misleading the agent. Instead of receiving the

actual cost signal gk = g(ik, uk, jk+1) at time k, the learning agent receives a

manipulated or falsiﬁed cost signal ˜gk. The corruption of cost signals comes

from the false observation of the state in cases where the cost is predetermined

by the learning agent. In other cases where cost signals are provided directly

by the environment or a remote supervisor, the cost signal can be corrupted

independently from the observation of the state. The learning agent receives

falsiﬁed cost signals even when the observation of the state is accurate. In

the example of autonomous vehicle, if the cost depends on the distance of the

deep RL agent to a destination as measure by GPS coordinates, spooﬁng of

GPS signals by the adversary may result in incorrect reward signals, which can

translate to incorrect navigation policies (See Behzadan and Munir [2018]).

15

The corruption of cost signals may lead to a doomed policies. For example,

Clark and Amodei [2016] has trained an RL agent on a boat racing game. High

observed reward misleads the agent to go repeatedly in a circle in a small lagoon

and hit the same targets, while losing every race. Or even worse, the learning

agent may be misled to lethal policies that would result in self-destruction.

The learning agent inﬂuences the environment by performing control u via

actuators. There is a type of attacks targeting the actuators. If the actuator

is exposed to the adversary, the control performed will be diﬀerent than the

one determined by the agent. Hence, during the learning process, the agent

will learn a deteriorated transition kernel and a matched reward function.

An example of this type of attack is RL-based unmanned aerial vehicles may

receive corrupted control signals which would cause malfunctioning of its servo

actuators.

To understand an attack model, one needs to specify three ingredients: ob-

jective of an adversary, actions available to the adversary, and information at

his disposal. The objective of an adversary describes the goal the adversary

aims to achieve out of the attacks. Objectives of an adversary include but not

limit to maximizing the agent’s accumulative costs, misleading the agent to

learn certain policies, or diverging the agent’s learning algorithm. To achieve

the objective, adversaries need a well-crafted strategy. An attack strategy is

developed based on the information and the actions at his disposal. An at-

tack strategy is a map from the information space to the action space. The

information structure of the adversary describes the knowledge the adversaries

have during the learning and the attack process. Whether the adversaries know

which learning algorithms the agent implements, whether the adversaries know

the actual state trajectory, controls and costs are decisive for the adversaries

16

to choose their strategies. The scope of the attack strategy also depends on

the actions at the adversary’s disposal. Due to adversaries’ capabilities, ad-

versaries can only launch certain attacks. For example, some adversaries can

only manipulate the costs induced at certain states or add white noise to the

observations.

In next section, we focus on cases where adversaries target at the cost

signals received by the agent. We analyze the performance of TD learning and

Q-learning under manipulated costs, respectively, and provide fundamental

limits under the attacks on cost signals.

1.3. Reinforcement Learning with Ma-

nipulated Cost Signals

Under malicious attacks on cost signals as we have discussed in Sec. 1.2, the

RL agent will fail to observe the actual cost feedback from the environment.

Instead, the agent receives a cost signal that might be falsiﬁed by the adversary.

Consider the following MDP with falsiﬁed cost denoted by (cid:104)S, A, g, ˜g, P, α(cid:105).

Under the falsiﬁcation, the agent, instead of receiving the actual cost signal

gt ∈ R at the t-th update, observes a falsiﬁed cost signal denoted by ˜gt ∈ R.

The remaining aspects of the MDP framework stay the same. The adversary’s

task here is to design falsiﬁed cost signals ˜g based on his information structure

and the actions available to him so that he can achieve certain objectives.

Suppose the adversary is an insider and he knows what the agent knows at

time t. In other words, at time t, the adversary knows the state trajectory, the

control trajectory and the cost signals up to time t. The adversary may or may

17

not know the system model. Suppose that the adversary falsiﬁes the cost signals

in a stealthy way. In this case, the cost associated with each state-control-state

triple is consistently falsiﬁed in the same way. The following deﬁnition gives a

formal deﬁnition of the attack.

Deﬁnition 1.1:

(Stealthy Attacks) If ˜gt takes the same value for the same

state-control-state triple for all t; i.e., for t (cid:54)= τ , we have ˜gt = ˜gτ if (it, ut, jt+1) =

(iτ , uτ , jτ +1), then we say the attacks on the cost signals are stealthy.

Under stealthy attacks, the falsiﬁed cost signals can be given by a function

˜g : S × A × S → R. Then, at time t, the falsiﬁed cost the agent receives is

˜gt = ˜g(it, ut, it+1). Since the transition from it to it+1 depends on the transi-

tion probability, without loss of generality, we consider only the cost structure

deﬁned on the state-control pair, i.e., g(i, u) for i ∈ S, u ∈ A.

1.3.1. TD Learning with Manipulated Cost Sig-

nals

If the RL agent updates the estimates of the cost-to-go function of a given

policy µ according to (1.3) and (1.4), then under the stealthy attacks, the

algorithm can be written as

˜rt+1 = ˜rt + γt ˜dtηt,

ηt+1 = αληt + φ(it+1),

(1.8)

where ˜dt = ˜g(it, ut) + α˜r(cid:48)

tφ(it+1) − ˜r(cid:48)

tφ(it).

Suppose the sequence of parameters {˜rt} generated by (1.8) under the

18

falsiﬁed cost signals is convergent and converges to ˜r∗. (We will show the

conditions under which the convergence of {˜rt} is guaranteed.) Let r∗ be the

solution of Ar +b. In TD(λ), the agent aims to estimate the cost-to-go function

J µ of a given policy µ. In approximated TD(λ) with linear approximation

architecture, ˜J(i, r) = φ(cid:48)(i)r serves as an approximation of J µ(i) for i ∈ S.

One objective of the adversary can be to deteriorate the approximation and

estimation of J µ by manipulating the costs. One way to achieve the objective

is to let ˜rt generated by (1.8) converge to ˜r∗ such that Φ(cid:48)˜r∗ is as a worse

approximate of J µ as possible.

Lemma 1.1:

If the sequence of parameters {˜rt} is generated by the TD(λ)

learning algorithm (1.8) with stealthy and bounded attacks on the cost signals,

then the sequence {˜rt} converges to ˜r∗ and ˜r∗ is a unique solution of Ar + ˜b =
0, where ˜b = Φ(cid:48)D (cid:80)∞

m=0(αλPµ)m˜g and ˜g is vector whose i-th component is

˜g(i, µ(i)).

The proof of Lemma 1.1 follows the proof of Proposition 6.4 in Bertsekas

and Tsitsiklis [1996] with g(i, µ(i), j) replaced by ˜g(i, µ(i)). If the adversary

performs stealthy and bounded attacks, he can mislead the agent to learn an

approximation Φ(cid:48)˜r∗ of J µ. The distance between Φ(cid:48)˜r∗ and J µ with respect

a norm (cid:107) · (cid:107) is what the adversary aims to maximize. The following lemma

provides an upper bound of the distance between Φ(cid:48)˜r∗ and J µ.

Lemma 1.2: Suppose that ˜r∗ is the parameters learned from the manipulated

TD(λ) (1.8). Then, the approximation error under the manipulated satisﬁes

(cid:107)Φ(cid:48)˜r∗ −J µ(cid:107)D ≤ (cid:107)Φ(cid:48)˜r∗ −Φ(cid:48)r∗(cid:107)D +

1 − λα
(cid:112)(1 − α)(1 + α − 2λα)

(cid:107)(Π−I)J µ(cid:107)D, (1.9)

19

where (cid:107) · (cid:107)D is the weighted quadratic norm deﬁned by (cid:107)J(cid:107)2
(cid:80)n

i=1 d(i)J(i)2 and Π = Φ(Φ(cid:48)DΦ)−1Φ(cid:48)D.

D = J (cid:48)DJ =

Proof. A direct application of triangle inequality gives us

(cid:107)Φ(cid:48)˜r∗ − J µ(cid:107) = (cid:107)Φ(cid:48)˜r∗ − Φ(cid:48)r∗ + Φ(cid:48)r∗ − J µ(cid:107) ≤ (cid:107)Φ(cid:48)˜r∗ − Φ(cid:48)r∗(cid:107) + (cid:107)Φ(cid:48)r∗ − J µ(cid:107).

This indicates that the distance between Φ(cid:48)˜r∗ and J µ is bounded by the dis-

tance between the falsiﬁed approximation Φ(cid:48)˜r∗ of J µ and the true approxima-

tion Φ(cid:48)r∗ of J µ plus the approximation error (cid:107)Φ(cid:48)r∗ − J µ(cid:107). Moreover, we know

from Theorem 1 in Tsitsiklis and Van Roy [1997] that

(cid:107)Φ(cid:48)r∗ − J µ(cid:107)D ≤

1 − λα
(cid:112)(1 − α)(1 + α − 2λα)

min
r

(cid:107)Φ(cid:48)r − J µ(cid:107)D.

(1.10)

From Lemma 6.8, we know there exists Π = Φ(Φ(cid:48)DΦ)−1Φ(cid:48)D such that for

every vector J, we have (cid:107)ΠJ − J(cid:107)D = minr (cid:107)Φr − J(cid:107)D. Hence, we arrive at

(cid:107)Φ(cid:48)˜r∗ − J µ(cid:107)D ≤ (cid:107)Φ(cid:48)˜r∗ − Φ(cid:48)r∗(cid:107)D +

1 − λα
(cid:112)(1 − α)(1 + α − 2λα)

(cid:107)(Π − I)J µ(cid:107)D.

Note that (cid:107)(Π − I)J µ(cid:107) can be further bounded by (cid:107)(Π − I)J µ(cid:107) = (cid:107)J µ(cid:107) −

(cid:107)ΠJ µ(cid:107) ≤ (cid:107)J µ(cid:107). But (1.9) provides a tighter bound. From (1.10), we know that

for the case λ = 1, the TD(λ) algorithm under the true costs (1.3) and (1.4)

gives the best approximation of J µ, i.e., Φ(cid:48)r∗ = ΠJ µ and (cid:107)Φ(cid:48)r∗ − J µ(cid:107)D =
minr (cid:107)Φ(cid:48)r − J µ(cid:107)D. As λ decreases, (1 − αλ)/(cid:112)(1 − α)(1 + α − 2λα) increases
and the bound deteriorates. The worst bound, namely, (cid:107)(Π − I)J µ(cid:107)/

1 − α2

√

is obtained when λ = 0. Although the result provides a bound, it suggests that

in the worst-case scenario, as λ decreases, the TD(λ) under costs manipulation

20

can suﬀer higher approximation error. From Lemma 1.10, we know that the

manipulation of the costs has no impact on the second part of the bound which
is (1 − λα)(cid:107)(Π − I)J µ(cid:107)D/(cid:112)(1 − α)(1 + α − 2λα). In the following theorem, we
analyze how much (cid:107)Φ(cid:48)(˜r∗ − r∗)(cid:107)D will change under a bounded manipulation

of the costs.

Theorem 1.1: Suppose the manipulation of the costs is given by η ∈ Rn

where η(i) = ˜g(i, µ(i)) − g(i, µ(i)) for i ∈ S. Then, the distance between the

manipulated TD(λ) estimate and the true TD(λ) estimate of J µ satisﬁes

(cid:107)Φ(cid:48)˜r∗ − Φ(cid:48)r∗(cid:107)D ≤

1
1 − α

(cid:107)η(cid:107)D,

and the approximation error of Φ(cid:48)˜r∗ satisﬁes

(cid:107)Φ(cid:48)˜r∗ − J µ(cid:107)D ≤ (cid:107)

1
1 − α

(cid:107)η(cid:107)D +

1 − λα
(cid:112)(1 − α)(1 + α − 2λα)

(cid:107)(Π − I)J µ(cid:107)D. (1.11)

Proof. Note that ˜r∗ and r∗ satisﬁes A˜r∗ + ˜b = 0 and Ar∗ + b = 0 where

b = Φ(cid:48)D (cid:80)∞

m=0(αλPµ)m¯g. Here, ¯g is vector whose i-th component is g(i, µ(i)).
Then, we have A(˜r∗ − r∗) + ˜b − b = 0 which implies (because Φ(cid:48)D is of full

rank)

M Φ(˜r∗ − r∗) +

∞
(cid:88)

(αλPµ)m(˜g − ¯g) = Φ(˜r∗ − r∗).

m=0

Applying (cid:107) · (cid:107)D on both side of the equation, we obtain

(cid:107)M Φ(˜r∗ − r∗)(cid:107)D + (cid:107)

∞
(cid:88)

(αλPµ)mη(cid:107)D = (cid:107)Φ(˜r∗ − r∗)(cid:107).

m=0

From Lemma 6.4 in Bertsekas and Tsitsiklis [1996], we know that for any

J ∈ Rn, we have (cid:107)PµJ(cid:107)D ≤ (cid:107)J(cid:107). From this, it easily follows that (cid:107)P m

µ J(cid:107)D ≤

21

(cid:107)J(cid:107)D. Note that M = (1−λ) (cid:80)∞

m=0 λm(αPµ)m+1. Using the triangle inequality,

we obtain

(cid:107)M J(cid:107)D ≤ (1 − λ)

∞
(cid:88)

m=0

λmαm+1(cid:107)J(cid:107)D =

α(1 − λ)
1 − αλ

(cid:107)J(cid:107)D.

Hence, we have

∞
(cid:88)

(cid:107)

(αλPµ)mη(cid:107)D ≥

m=0

1 − α
1 − αλ

(cid:107)Φ(˜r∗ − r∗)(cid:107).

Moreover, we can see that

∞
(cid:88)

(cid:107)

(αλPµ)mη(cid:107)D ≤ (cid:107)

∞
(cid:88)

(αλPµ)m(cid:107)D(cid:107)η(cid:107)D ≤

∞
(cid:88)

(αλ)m(cid:107)η(cid:107)D =

m=0

m=0

m=0

1
1 − αλ

(cid:107)η(cid:107)D,

which indicates that

1
1 − αλ

(cid:107)η(cid:107)D ≥

1 − α
1 − αλ

(cid:107)Φ(˜r∗ − r∗)(cid:107)D.

Thus, we have

(cid:107)Φ(˜r∗ − r∗)(cid:107)D ≤

1
1 − α

(cid:107)η(cid:107)D.

Together with Lemma 1.2, we have

(cid:107)Φ(cid:48)˜r∗ − J µ(cid:107)D ≤

1
1 − α

(cid:107)η(cid:107)D +

1 − λα
(cid:112)(1 − α)(1 + α − 2λα)

(cid:107)(Π − I)J µ(cid:107)D.

The distance between the true TD(λ) approximator Φ(cid:48)r∗ and the manipu-

lated TD(λ) approximator Φ(cid:48)˜r∗ is actually bounded by a constant term times

the distance of the true cost and the manipulated cost. And the constant term

1/(1 − α) does not depend on λ. This means the choice of λ by the agent does

22

not aﬀect the robustness of the TD(λ) algorithm. This means in the worst-case

scenario, TD(λ) algorithms with diﬀerent values of λ will suﬀer the same loss of

approximation accuracy. From (1.11), we can conclude that the approximation

error of the manipulated TD(λ) algorithm is bounded by the magnitude of the

costs manipulation and a ﬁxed value decided by the value of λ, the choice of

basis Φ and the properties of the MDP.

1.3.2. Q-Learning with Manipulated Cost Sig-

nals

If the RL agent learns an optimal policy by Q-learning algorithm given in (1.7),

then under stealthy attacks on cost, the algorithm can be written as

˜Qt+1(i, u) = (1 − γt) ˜Qt(i, u) + γt

(cid:18)

˜g(i, u) + α min
v∈U (¯i)

˜Qt(¯i, v)

(cid:19)

.

(1.12)

Note that if the attacks are not stealthy, we need to write ˜gt in lieu of ˜g(it, at).

There are two important questions regarding the Q-learning algorithm with

falsiﬁed cost (1.12): (1) Will the sequence of Qt-factors converge? (2) Where

will the sequence of Qt converge to?

Suppose that the sequence ˜Qt generated by the Q-learning algorithm (1.12)
converges. Let ˜Q∗ be the limit, i.e., ˜Q∗ = limn→∞ ˜Qt. Suppose the objective

of an adversary is to induce the RL agent to learn a particular policy µ†. The

adversary’s problem then is to design ˜g by applying the actions available to

him based on the information he has so that the limit Q-factors learned from

the Q-learning algorithm produce the policy favored by the adversary µ†, i.e,

23

˜Q∗ ∈ Vµ†, where

Vµ := {Q ∈ Rn×|A| : µ(i) = arg min
u

Q(i, u), ∀i ∈ S}.

In Q-learning algorithm (1.12), to guarantee almost sure convergence, the

agent usually takes tapering stepsize Borkar [2009] {γt} which satisﬁes 0 <
γt ≤ 1, t ≥ 0, and (cid:80)

t < ∞. Suppose in our problem, the

t γt = ∞, (cid:80)

t γ2

agent takes tapering stepsize. To address the convergence issues, we have the

following result.

Lemma 1.3:

If an adversary performs stealthy attacks with bounded ˜g(i, a, j)

for all i, j ∈ S, a ∈ A, then the Q-learning algorithm with falsiﬁed costs con-

verges to the ﬁxed point of ˜F (Q) almost surely where the mapping ˜F : Rn×|A| →

Rn×|A| is deﬁned as ˜F (Q) = [ ˜Fii(Q)]i,i with

˜Fiu(Q) = α

(cid:88)

j

pij(u) min

v

Q(j, v) + ˜g(i, u, j),

and the ﬁxed point is unique and denoted by ˜Q∗.

The proof of Lemma 1.1 is included in Huang and Zhu [2019b]. It is not

surprising that one of the conditions given in Lemma 1.3 is that an attacker

performs stealthy attacks. The convergence can be guaranteed because the

falsiﬁed cost signals are consistent over time for each state action pair. The

uniqueness of ˜Q∗ comes from the fact that if ˜g(i, u) is bounded for every (i, u) ∈

S × A, ˜F is a contraction mapping. By Banach’s ﬁxed point theorem Kreyszig

[1978], ˜F admits a unique ﬁxed point. With this lemma, we conclude that

an adversary can make the algorithm converge to a limit point by stealthily

falsifying the cost signals.

24

Remark 1.1: Whether an adversary aims for the convergence of the Q-

learning algorithm (1.12) or not depends on his objective. In our setting, the

adversary intends to mislead the RL agent to learn policy µ†, indicating that

the adversary promotes convergence and aim to have the limit point ˜Q∗ lie in

Vµ†.

It is interesting to analyze, from the adversary’s perspective, how to falsify

the cost signals so that the limit point that algorithm (1.12) converges to is

favored by the adversary. In later discussions, we consider stealthy attacks

where the falsiﬁed costs are consistent for the same state action pairs. Denote

the true cost by matrix g ∈ Rn×|A| with [g]i,u = g(i, u) and the falsiﬁed cost is

described by a matrix ˜g ∈ Rn×|A| with [˜g]i,u = ˜g(i, u). Given ˜g, the ﬁxed point

of ˜F is uniquely decided; i.e., the point that the algorithm (1.12) converges to

is uniquely determined. Thus, there is a mapping ˜g (cid:55)→ ˜Q∗ implicitly described

by the relation ˜F (Q) = Q. For convenience, this mapping is denoted by f :

Rn×|A| → Rn×|A|.

Theorem 1.2(No Butterﬂy Eﬀect): Let ˜Q∗ denote the Q-factor learned

from algorithm (1.7) with falsiﬁed cost signals and Q∗ be the Q-factor learned

from (1.12) with true cost signals. There exists a constant L < 1 such that

(cid:107) ˜Q∗ − Q∗(cid:107) ≤

1
1 − L

(cid:107)˜g − g(cid:107),

(1.13)

and L = α.

The proof of Theorem 1.2 can be found in Huang and Zhu [2019b]. In fact,

taking this argument just slightly further, one can conclude that falsiﬁcation

on cost g by a tiny perturbation does not cause signiﬁcant changes in the

25

limit point of algorithm (1.12), ˜Q∗. This feature indicates that an adversary

cannot cause a signiﬁcant change in the limit Q-factors by just applying a

small perturbation in the cost signals. This is a feature known as stability,

which is observed in problems that possess contraction mapping properties.

Also, Theorem 1.2 indicates that the mapping ˜g (cid:55)→ ˜Q∗ is continuous, and to

be more speciﬁc, it is uniformly Lipschitz continuous with Lipschitz constant

1/(1 − α).

With Theorem 1.2, we can characterize the minimum level of falsiﬁcation

required to change the policy from the true optimal policy µ∗ to the policy µ†.

First, note that Vµ ⊂ Rn×|A| and it can also be written as

Vµ = {Q ∈ Rn×|A| : Q(i, µ(i)) < Q(i, u), ∀i ∈ S, ∀u (cid:54)= µ(i)}.

(1.14)

Second, for any two diﬀerent policies µ1 and µ2, Vµ1 ∩ Vµ2 = ∅. Lemma 1.4

presents several important properties regarding the set Vµ.

Lemma 1.4: (a) For any given policy µ, Vµ is a convex set.

(b) For any two diﬀerent policies µ1 and µ2, Vµ1 ∩ Vµ2 = ∅.

(c) The distance between any two diﬀerent policies µ1 and µ2 deﬁned as D(µ1, µ2) :=

inf Q1∈Vµ1 ,Q2∈Vµ2

(cid:107)Q1 − Q2(cid:107) is zero.

Proof.

(a) Suppose Q1, Q2 ∈ Vµ. We show for every λ ∈ [0, 1], λQ1 +

(1 − λ)Q2 ∈ Vµ. This is true because Q1(i, µ(i)) < Q1(i, u) and Q2(i, µ(i)) <

Q2(i, u) imply

λQ1(i, µ(i)) + (1 − λ)Q2(i, µ(i)) < λQ1(i, u) + (1 − λ)Q2(i, u),

for all i ∈ S, u (cid:54)= µ(i).

26

(b) Suppose µ1 (cid:54)= µ2 and Vµ1 ∩ Vµ2 is not empty. Then there exists Q ∈

Vµ1 ∩ Vµ2. Since µ1 = µ2, there exists i such that µ1(i) (cid:54)= µ2(i). Let u = µ2.

Since Q ∈ Vµ1, we have Q(i, µ1(i)) < Q(i, µ2(i)). Hence, Q /∈ Vµ2, which is a
contradiction. Thus, Vµ1 ∩ Vµ2 = ∅.

(c) Suppose µ1 (cid:54)= µ2. Construct Q1 as a matrix whose entries are all one

except Q(i, µ1(i)) = 1 − (cid:15)/2 for every i ∈ S where (cid:15) > 0. Similarly, con-

struct Q2 as a matrix whose entries are all one except Q(i, µ2(i)) = 1 − (cid:15)/2

for every i ∈ S. It is easy to see that Q1 ∈ Vµ1 and Q2 ∈ Vµ2. Then

inf Q1∈Vµ1 ,Q2∈Vµ2

(cid:107)Q1 − Q2(cid:107)∞ ≤ (cid:107)Q1 − Q2(cid:107)∞ = (cid:15). Since (cid:15) can be arbitrar-

ily small, inf Q1∈Vµ1 ,Q2∈Vµ2

(cid:107)Q1 − Q2(cid:107)∞ = 0. Since norms are equivalent in

ﬁnite-dimensional space (see Sec. 2.4 in Kreyszig [1978]), we have D(µ1, µ2) =

inf Q1∈Vµ1 ,Q2∈Vµ2

(cid:107)Q1 − Q2(cid:107) = 0.

Suppose the true optimal policy µ∗ and the adversary desired policy µ† are

diﬀerent; otherwise, the optimal policy µ∗ is what the adversary desires, there

is no incentive for the adversary to attack. According to Lemma 1.4, D(µ∗, µ†)

is always zero. This counterintuitive result states that a small change in the Q-

value may result in any possible change of policy learned by the agent from the

Q-learning algorithm (1.12). Compared with Theorem 1.2 which is a negative

result to the adversary, this result is in favor of the adversary.

Deﬁne the point Q∗ to set Vµ† distance by DQ∗(µ†) := inf Q∈Vµ† (cid:107)Q − Q∗(cid:107).

Thus, if ˜Q∗ ∈ Vµ†, we have

0 = D(µ∗, µ†) ≤ DQ∗(µ†) ≤ (cid:107) ˜Q∗ − Q∗(cid:107) ≤

1
1 − α

(cid:107)˜g − g(cid:107),

(1.15)

where the ﬁrst inequality comes from the fact that Q∗ ∈ Vµ∗ and the second
inequality is due to ˜Q∗ ∈ Vµ†. The inequalities give us the following theorem.

27

Theorem 1.3(Robust Region): To make the agent learn the policy µ†, the

adversary has to manipulate the cost such that ˜g lies outside the ball B(g; (1 −

α)DQ∗(µ†)).

The robust region for the true cost g to the adversary’s targeted policy µ†

is given by B(g; (1 − α)DQ∗(µ†)) which is an open ball with center c and radius

(1 − α)DQ∗(µ†). That means the attacks on the cost needs to be ‘powerful’

enough to drive the falsiﬁed cost ˜g outside the ball B(g; (1 − α)DQ∗(µ†)) to

make the RL agent learn the policy µ†. If the falsiﬁed cost ˜g is within the

ball, the RL agent can never learn the adversary’s targeted policy µ†. The ball

B(g; (1−α)DQ∗(µ†)) depends only on the true cost g and the adversary desired

policy µ† (Once the MDP is given, Q∗ is uniquely determined by g). Thus, we

refer this ball as the robust region of the true cost g to the adversarial policy

µ†. As we have mentioned, if the actions available to the adversary only allows

him to perform bounded falsiﬁcation on cost signals and the bound is smaller

than the radius of the robust region, then the adversary can never mislead the

agent to learn policy µ†.

Remark 1.2: First, in discussions above, the adversary policy µ† can be

any possible polices and the discussion remains valid for any possible policies.

Second, set Vµ of Q-values is not just a convex set but also an open set. We

thus can see that DQ∗(µ†) > 0 for any µ† (cid:54)= µ∗ and the second inequality in

(1.15) can be replaced by a strict inequality. Third, the agent can estimate his

own robustness to falsiﬁcation if he can know the adversary desired policy µ†.

For attackers who have access to true cost signals and the system model, the

attacker can compute the robust region of the true cost to his desired policy

µ† to evaluate whether the objective is feasible or not. When it is not feasible,

28

the attacker can consider changing his objectives, e.g., selecting other favored

policies that have a smaller robust region.

We have discussed how falsiﬁcation aﬀects the change of Q-factors learned

by the agent in a distance sense. The problem now is to study how to falsify the

true cost in a right direction so that the resulted Q-factors fall into the favored

region of an adversary. One diﬃculty of analyzing this problem comes from

the fact that the mapping ˜g (cid:55)→ ˜Q∗ is not explicit known. The relation between

˜g and ˜g∗ is governed by the Q-learning algorithm (1.12). Another diﬃculty is

that due to the fact that both ˜g and ˜Q∗ lies in the space of Rn×|A|, we need to

resort to Fr´echet derivative or Gˆateaux derivative Cheney [2013] (if they exist)

to characterize how a small change of ˜g results in a change in ˜Q∗.

From Lemma 1.3 and Theorem 1.2, we know that Q-learning algorithm

converges to the unique ﬁxed point of ˜F and that f : ˜g (cid:55)→ ˜Q∗ is uniformly

Lipschitz continuous. Also, it is easy to see that the inverse of f , denoted by

f −1, exists since given ˜Q∗, ˜g is uniquely decided by the relation ˜F (Q) = Q.

Furthermore, by the relation ˜F (Q) = Q, we know f is both injective and sur-

jective and hence a bijection which can be simply shown by arguing that given

diﬀerent ˜g, the solution of ˜F (Q) = Q must be diﬀerent. This fact informs that

there is a one-to-one, onto correspondence between ˜g and ˜Q∗. One should note

that the mapping f : Rn×|A| → Rn×|A| is not uniformly Fr´echet diﬀerentiable

on Rn×|A| due to the min operator inside the relation ˜F (Q) = Q. However, for

any policy µ, f is Fr´echet diﬀerentiable on f −1(Vµ) which is an open set and

connected due to the fact that Vµ is open and connected (every convex set is

connected) and f is continuous. In the next lemma, we show that f is Fr´echet

diﬀerentiable on f −1(Vw) and the derivative is constant.

29

Lemma 1.5: The map f : Rn×|A| → Rn×|A| is Fr´echet diﬀerentiable on

f −1(Vµ) for any policy µ and the Fr´echet derivative of f at any point ˜g ∈ Vµ,

denoted by f (cid:48)(˜g), is a linear bounded map G : Rn×|A| → Rn×|A| that does not

depend on ˜g, and Gh is given as

[Gh]i,u = αP T

iu(I − αPµ)−1hµ + h(i, u)

(1.16)

for every i ∈ S, u ∈ A, where Piu = (pi1(u), · · · , pin(u)), Pµ := [Pµ]i,j =

pij(µ(i)).

The proof of Lemma 1.5 is provided in Huang and Zhu [2019b]. We can

see that f is Fr´echet diﬀerentiable on f −1(Vµ) and the derivative is constant,

i.e., f (cid:48)(˜g) = G for any ˜g ∈ f −1(Vµ). Note that G lies in the space of all linear

mappings that maps Rn×|A| to itself and G is determined only by the discount

factor α and the transition kernel P of the MDP problem. The region where the

diﬀerentiability may fail is f −1(Rn×|A|\(∪µVµ)), where Rn×|A|\(∪µVµ) is the

set {Q : ∃i, ∃u (cid:54)= u(cid:48), Q(i, u) = Q(i, u(cid:48)) = minv Q(i, v)}. This set contains the

places where a change of policy happens, i.e., Q(i, u) and Q(i, u(cid:48)) are both the

lowest value among the ith row of Q. Also, due to the fact that f is Lipschitz,

by Rademacher’s theorem, f is diﬀerentiable almost everywhere (w.r.t. the

Lebesgue measure).

Remark 1.3: One can view f as a ‘piece-wise linear function’ in the norm

vector space Rn×|A|. Actually, if the adversary can only falsify the cost at one

state-control pair, say (i, u), while costs at other pairs are ﬁxed, then for every

j ∈ S, v ∈ A, the function ˜g(i, u) (cid:55)→ [ ˜Q∗]j,v is a piece-wise linear function.

Given any g ∈ f −1(Vµ), if an adversary falsiﬁes the cost g by injecting value

30

h, i.e., ˜g = g + h, the adversary can see how the falsiﬁcation causes a change

in Q-values. To be more speciﬁc, if Q∗ is the Q-values learned from cost g by

Q-learning algorithm (1.7), after the falsiﬁcation ˜g, the Q-value learned from

Q-learning algorithm (1.12) becomes ˜Q∗ = Q∗ + Gh if ˜g ∈ f −1(Vµ). Then,

an adversary who knows the system model can utilize (1.16) to ﬁnd a way of

falsiﬁcation h such that ˜Q∗ can be driven to approach a desired set Vµ†. One
diﬃculty is to see whether ˜g ∈ f −1(Vµ) because the set f −1(Vµ) is now implicit

expressed. Thus, we resort to the following theorem.

Theorem 1.4: Let ˜Q∗ ∈ Rn×|A| be the Q-values learned from the Q-learning

algorithm (1.12) with the falsiﬁed cost ˜g ∈ Rn×|A|. Then ˜Q∗ ∈ Vµ† if and only

if the falsiﬁed cost signals ˜g designed by the adversary satisfy the following

conditions

˜g(i, u) > (1i − αPiu)T (I − αPµ†)−1˜gµ†.

(1.17)

for all i ∈ S, u ∈ A\{µ†(i)}, where 1i ∈ Rn a vector with n components whose

ith component is 1 and other components are 0.

With the results in Theorem 1.4, we can characterize the set f −1(Vµ). Ele-

ments in f −1(Vµ) have to satisfy the conditions given in (1.17). Also, Theorem

1.4 indicates that if an adversary intends to mislead the agent to learn policy

µ†, the falsiﬁed cost ˜g has to satisfy the conditions speciﬁed in (1.17).

If an adversary can only falsify at certain states ˜S ⊂ S, the adversary may

not be able to manipulate the agent to learn µ†. Next, we study under what

conditions the adversary can make the agent learn µ† by only manipulating

the costs on ˜S. Without loss of generality, suppose that the adversary can

only falsify the cost at a subset of states ˜S = {1, 2, ..., | ˜S|}. We rewrite the

31

conditions given in (1.17) into a more compact form:

˜gu ≥ (I − αPu)(I − αPµ†)−1˜gµ†, ∀ u ∈ A,

(1.18)

where ˜gu = (˜g(1, u), . . . , ˜g(n, u)), ˜gµ† = (cid:0)˜g(1, µ†(1)), ˜g(2, µ†(2)), . . . , ˜g(n, µ†(n))(cid:1)

and Pu = [Pu]i,j = pij(u). The equality only holds for one component of the

vector, i.e., the i-th component satisfying µ(i) = u. Partition the vector ˜gu and

˜gµ† in (1.18) into two parts, the part where the adversary can falsify the cost
µ† ∈ R| ˜S| and the part where the adversary cannot falsify
denoted by ˜gf al

u , ˜gf al

gtrue
u

, gtrue

µ† ∈ Rn−| ˜S|. Then (1.18) can be written as






˜gf al
u

gtrue
u






 ≥









Ru

Yu

Mu Nu






˜gf al
µ†
gtrue
µ†




 , ∀ u ∈ A,

(1.19)

where






Ru

Yu

Mu Nu






:= (I − αPu)(I − αPµ†)−1, ∀ u ∈ A

and Ru ∈ R ˜S× ˜S, Yu ∈ R| ˜S|×(n−| ˜S|), Mu ∈ R(n−| ˜S|)×| ˜S|, Nu ∈ R(n−| ˜S|)×(n−| ˜S|).

If the adversary aims to mislead the agent to learn µ†, the adversary needs

to design ˜gf al

u , u ∈ A such that the conditions in (1.19) hold. The following re-

sults state that under some conditions on the transition probability, no matter

what the true costs are, the adversary can ﬁnd proper ˜gf al

u , u ∈ A such that

conditions (1.19) are satisﬁed. For i ∈ S\ ˜S, if µ(i) = u, we remove the rows

of Mu that correspond to the state i ∈ S\ ˜S. Denote the new matrix after the
row removals by ¯Mu.

Proposition 1.1: Deﬁne H := [ ¯M (cid:48)
u1

¯M (cid:48)

u2 · · · ¯M (cid:48)

u|A|

](cid:48) ∈ R(|A|(n−| ˜S|)−| ˜S|)×| ˜S|.

32

If there exists x ∈ R| ˜S| such that Hx < 0, i.e., the column space of H intersects

the negative orthant of R|A|(n−| ˜S|)−| ˜S|, then for any true cost, the adversary can

ﬁnd ˜gf al

u , u ∈ A such that conditions (1.19) hold.

The proof can be found in Huang and Zhu [2019b]. Note that H only

depends on the transition probability and the discount factor, if an adversary

who knows the system model can only falsify cost signals at states denoted by

˜S, an adversary can check if the range space of H intersects with the negative

orthant of R|A|(n−| ˜S|) or not. If it does, the adversary can mislead the agent to

learn µ† by falsifying costs at a subset of state space no matter what the true

cost is.

Remark 1.4: To check whether the condition on H is true or not, one has

to resort to Gordan’s theorem Broyden [2001]: Either Hx < 0 has a solution

x, or H T y = 0 has a nonzero solution y with y ≥ 0. The adversary can use

linear/convex programming software to check if this is the case. For example,

by solving

min
y∈R|A|(n−| ˜S|)

(cid:107)H T y(cid:107) s.t.

(cid:107)y(cid:107) = 1, y ≥ 0,

(1.20)

the adversary can know whether the condition about H given in Proposition

1.1 is true or not. If the minimum of (1.20) is positive, there exists x such that

Hx < 0. The adversary can select ˜gf al

µ† = λx and choose a suﬃciently large λ
to make sure that conditions (1.19) hold, which means an adversary can make

the agent learn the policy µ† by falsifying costs at a subset of state space no

matter what the true costs are.

33

1.4. Case Study

Here, we consider TD learning on random walk. Given a policy µ, an MDP

can be considered as a Markov cost process, or MCP. In this MCP, we have

n = 20 states. The transition digram of the MCP is given in Fig. 1.2. At state

i = ik, k = 2, 3, . . . , n − 1, the process proceed either left to ik−1 or right to

ik+1, with equal probability. The transition at states from i2 to in−1 is similar

to symmetric one-dimensional random walk. At state i1, the process proceed to

state i2 with probability 1/2 or stays at the same state with equal probability.

At state in, the probabilities of transition to in−1 and staying at in are both

1

2 . That is, we have pikik+1(µ(ik)) = pikik−1(µ(ik)) = 1
2 and pinin = pinin−1 = 1
pi1i1 = pi1i2 = 1

2 . The cost at state ik is set to be k if

2 for k = 2, 3, . . . , n − 1,

k ≤ 10 and 21 − k if k > 10. That is

g(ik, µ(ik)) =



k

if k ≤ 10

.



21 − k

else

We consider the discount factor α = 0.9. The task here is to use approximate

TD(λ) learning algorithm to esitimate and approximate the cost-to-go function

J µ of this MCP. We consider a linear parametrization of the form

˜J(i, r) = r(3)i2 + r(2)i + r(1),

(1.21)

and r = (r(1), r(2), r(3)) ∈ R3. Suppose the learning agent updates rt based

on TD(λ) learning algorithm (1.3) and (1.4) and tries to ﬁnd an estimate of

J µ. We simulate the MCP and obtain a trajectory that long enough and its

associated cost signals. We need an inﬁnite long trajectory ideally. But here, we

set the length of the trajectory to be 105. We run respectively TD(1) and TD(0)

34

on the same simulated trajectory based on rules given in (1.3) and (1.4). The

black line indicates the cost-to-go function of the MCP. The blue markers are

the approximations of the cost-to-go function obtained by following the TD(λ)

algorithm (1.3) and (1.4) with λ = 1 and λ = 0. We can see that ˜JT D(1) and
˜JT D(0) is a quadratic function of i as we set in (1.21). Both ˜JT D(1) and ˜JT D(0)

can serve a fairly good approximation of J µ as we can see. The dimension of

the parameters we need to update goes from n = 20 in the TD(λ) algorithm

(1.2) to K = 3 in the approximation counterpart (1.3) which is more eﬃcient

computationally.

Figure 1.2: The diagram of the MCP task with 20 states denoted by i1, . . . , i20.

Suppose the adversary aims to deteriorate the TD(λ) algorithm by stealthily

manipulating the cost signals. Suppose the adversary can only manipulate the

cost signals at state i20 and the manipulated cost is ˜g(i20, µ(i20)) = 20. We can

see from Fig. 1.3 that the TD(λ) learning under manipulated cost signals fails

to provide accurate approximation of J µ. Although only the cost signal at one

state is manipulated, the approximation of cost-to-go function at other states

can also be largely deviated from the accurate value.

We generate 100 random falsiﬁcations denoted by η. Note that η = ˜g−g. For

35

Figure 1.3: The cost-to-go function of a given policy, denoted by J µ; the

approximations of the cost-to-go function under true cost signals which is marked
blue, denoted by ˜J; the approximations of the cost-to-go function under manipulated
cost signals which is marked red, denoted by ˜J †; The subscript TD(1) and TD(0)
denote the TD parameter λ is set to be 1 and 0 respectively.

36

Figure 1.4: The approximated TD(1) learned from 100 random falsiﬁcations of the

costs. For each falsiﬁcation, we plot the distance of the falsiﬁcation and the
approximation error of its associated approximate of J µ. the blue line is a

demonstration of the bound developed in (1.11).

37

each falsiﬁcation, we plot the norm (cid:107)η(cid:107)D of η and its associated approximation

error (cid:107)Φ(cid:48)˜r∗ − J µ(cid:107)D and the pair is marked by black circle in Fig. 1.4. Note that

˜r∗ is the parameter learned by the agent using TD(1) learning algorithm (1.8)

under the falsiﬁed costs. The blue line describes the map (cid:107)η(cid:107)D (cid:55)→ 1

1−α (cid:107)η(cid:107)D +
(cid:107)ΠJ µ − J µ(cid:107)D. The results in Fig. 1.4 collaborate the results we proved in

Theorem 1.1. This demonstrate how the falsiﬁcation on cost signals in only

one state can aﬀect the approximated value function on every single state and

how the resulted learning error is bounded.

For case study of Q-learning with falsiﬁed costs, one can refer to Huang

and Zhu [2019b].

1.5. Conclusion

In this chapter, we have discussed the potential threats in RL and a general

framework has been introduced to study RL under deceptive falsiﬁcations of

cost signals where a number of attack models have been presented. We have

provided theoretical underpinnings for understanding the fundamental limits

and performance bounds on the attack and the defense in RL systems. We

have shown that in TD(λ) the approximation learned from the manipulated

costs has an approximation error bounded by a constant times the magnitude

of the attack. The eﬀect of the adversarial attacks does not depend on the

choice of λ. In Q-learning, we have characterized a robust region within which

the adversarial attacks cannot achieve its objective. The robust region of the

cost can be utilized by both oﬀensive and defensive sides. An RL agent can

leverage the robust region to evaluate the robustness to malicious falsiﬁcations.

An adversary can leverage it to assess the attainability of his attack objectives.

38

Conditions given in Theorem 1.4 provide a fundamental understanding of the

possible strategic adversarial behavior of the adversary. Theorem 1.1 helps

understand the attainability of an adversary’s objective. Future work would

focus on investigating a particular attack model and develop countermeasures

to the attacks on cost signals.

Bibliography

Itamar Arel, Cong Liu, Tom Urbanik, and Airton G Kohls. Reinforcement

learning-based multi-agent system for network traﬃc signal control.

IET

Intelligent Transport Systems, 4(2):128–135, 2010.

Vahid Behzadan and Arslan Munir. The faults in our pi stars: Security is-

sues and open challenges in deep reinforcement learning. arXiv preprint

arXiv:1810.10369, 2018.

Vahid Behzadan and Arslan Munir. Adversarial reinforcement learning frame-

work for benchmarking collision avoidance mechanisms in autonomous vehi-

cles. IEEE Intelligent Transportation Systems Magazine, 2019.

Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, vol-

ume 5. Athena Scientiﬁc Belmont, MA, 1996.

Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint,

volume 48. Springer, 2009.

Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochas-

tic approximation and reinforcement learning. SIAM Journal on Control and

Optimization, 38(2):447–469, 2000.

39

CG Broyden. On theorems of the alternative. Optimization methods and

software, 16(1-4):101–111, 2001.

Ward Cheney. Analysis for applied mathematics, volume 208. Springer Science

& Business Media, 2013.

Jack Clark and Dario Amodei. Faulty reward functions in the wild. URL

https://blog. openai. com/faulty-reward-functions, 2016.

Damien Ernst, Mevludin Glavic, and Louis Wehenkel. Power systems stabil-

ity control: reinforcement learning framework. IEEE transactions on power

systems, 19(1):427–435, 2004.

Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane

Legg. Reinforcement learning with a corrupted reward channel. arXiv

preprint arXiv:1705.08417, 2017.

Linan Huang and Quanyan Zhu. Adaptive honeypot engagement through rein-

forcement learning of semi-markov decision processes. In International Con-

ference on Decision and Game Theory for Security, pages 196–216. Springer,

2019a.

Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under

adversarial manipulations on cost signals. In International Conference on

Decision and Game Theory for Security, pages 217–237. Springer, 2019b.

Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in

robotics: A survey. The International Journal of Robotics Research, 32(11):

1238–1274, 2013.

Erwin Kreyszig. Introductory functional analysis with applications, volume 1.

wiley New York, 1978.

40

Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu,

and Min Sun. Tactics of adversarial attack on deep reinforcement learning

agents. arXiv preprint arXiv:1703.06748, 2017.

Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in

batch reinforcement learning and control. In Advances in Neural Information

Processing Systems, pages 14543–14553, 2019.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Ve-

ness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidje-

land, Georg Ostrovski, et al. Human-level control through deep reinforce-

ment learning. Nature, 518(7540):529, 2015.

Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learn-

ing, volume 2. MIT press Cambridge, 1998.

Ah-Hwee Tan, Ning Lu, and Dan Xiao. Integrating temporal diﬀerence meth-

ods and self-organizing neural networks for reinforcement learning with de-

layed evaluative feedback. IEEE Transactions on Neural Networks, 19(2):

230–244, 2008.

John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diﬀference

learning with function approximation. In Advances in neural information

processing systems, pages 1075–1081, 1997.

Jingkang Wang, Yang Liu, and Bo Li. Reinforcement learning with perturbed

rewards. arXiv preprint arXiv:1810.01032, 2018.

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8

(3-4):279–292, 1992.

41

Christopher John Cornish Hellaby Watkins. Learning from delayed rewards.

1989.

Zhiheng Xu and Quanyan Zhu. A cyber-physical game framework for secure

and resilient multi-agent autonomous systems. In 2015 54th IEEE Confer-

ence on Decision and Control (CDC), pages 5156–5161. IEEE, 2015.

Quanyan Zhu and Tamer Ba¸sar. Dynamic policy-based ids conﬁguration. In

Proceedings of the 48h IEEE Conference on Decision and Control (CDC)

held jointly with 2009 28th Chinese Control Conference, pages 8600–8605.

IEEE, 2009.

42

