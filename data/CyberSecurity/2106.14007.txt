Scalable Feature Subset Selection for Big Data using Parallel 
Hybrid Evolutionary Algorithm based Wrapper in Apache Spark 

Yelleti Vivek1,2, Vadlamani Ravi1 and P. Radhakrishna2 
1Center of Excellence in Analytics,  
Institute for Development and Research in Banking Technology,  
Castle Hills Road #1, Masab Tank, Hyderabad-500076, India 
2Department of Computer Science and Engineering, National Institute of Technology,  
Warangal-506004, India 
yvivek@idrbt.ac.in; padmarav@idrbt.ac.in; prkrishna@nitw.ac.in 

Abstract 
Owing to the emergence of large  datasets,  applying current sequential  wrapper-based feature subset 
selection (FSS) algorithms increases the complexity. This limitation motivated us to propose a wrapper 
for  feature  subset  selection  (FSS)  based  on  parallel  and  distributed  hybrid  evolutionary  algorithms 
(EAs)  under  the  Apache  Spark  environment.  The  hybrid  EAs  are  based  on  the  BDE  and  Binary 
Threshold Accepting (BTA), a point-based EA, which is invoked to enhance the search capability and 
avoid premature convergence of the PB-DE. Thus, we designed the hybrid variants (i) parallel binary 
differential evolution and threshold accepting (PB-DETA), where DE and TA work in tandem in every 
iteration, and (ii) parallel binary threshold accepting and differential evolution (PB-TADE), where TA 
and DE work in tandem in every iteration under the Apache Spark environment. Both PB-DETA and 
PB-TADE are compared with the baseline, viz.,  the parallel version of the binary differential evolution 
(PB-DE).  All three proposed approaches use logistic regression (LR) to compute the fitness function, 
namely, the area under ROC curve (AUC). The effectiveness of the proposed algorithms is tested over 
the  five  large  datasets  of  varying  feature  space  dimension,  taken  from  cyber  security  and  biology 
domains. It is noteworthy that the PB-TADE turned out to be statistically significant compared to PB-
DE and PB-DETA.  We reported  the speedup analysis, average AUC  obtained by the most repeated 
feature subset, feature subset with high AUC and least cardinality. 

Keywords:  Feature  Subset  Selection,  Apache  Spark,  Differential  Evolution,  Threshold  Accepting, 
MapReduce, Multithreading. 

1. Introduction 

Selecting relevant and important feature subset is evidently a paramount pre-processing step in 
the  CRISP-DM  methodology  [72]  of  data  mining.  This  process  of  selecting  the  important  group  of 
features is popularly known as Feature subset selection (FSS) [1,2]. The main objective of FSS is to 
select the most relevant and highly discriminative feature subsets. The spectacular benefits of FSS are 
as follows: it improves the comprehensibility of the models, reduces the model complexity, improves 
the training time, avoids overfitting, and sometimes improves the model’s performance. Further, the 
resulting model becomes parsimonious. The ubiquitous presence of big datasets in every domain made 
FSS a mandatory pre-processing step.  

FSS can be performed primarily in three different ways: filter, wrapper, embedded approaches. 
The  main  difference  lies  in  the  fitness  value  determination  and  selecting  the  salient  features  either 
individually  or  group-wise.  The  filter  approaches  measure  the  fitness  value  based  on  the  statistical 

 Corresponding Author, Phone: +914023294310; FAX: +914023535157 

1 | P a g e  

 
 
 
 
 
                                                 
measure such as Information gain, Mutual information, Gain ratio, etc. These methods are fast but result 
in less accuracy and cannot account for the interaction effects amongst the features. Wrapper approaches 
comprise a metaheuristic optimization algorithm that searches for the best feature subsets as indicated 
by the  highest  fitness  value  determined  by  a  classifier  (for  a classification problem)  or  a regression 
model (for a regression problem). These approaches are computationally intensive but highly accurate 
while accounting for the interaction effects of the features. In Embedded approaches, the feature subset 
selection is embedded as a part of the model-building phase. These approaches combine the advantages 
of being less computationally expensive than wrapper and give better accuracy than filter approaches. 
Even  though  wrapper  approaches  impose  high  complexity,  the  selected  feature  subset  is  highly 
generalized to the underlying classifier. 

In current study, FSS is formulated as a combinatorial problem because if there are n features, 
the total possible number of feature subsets is 2(cid:3041) − 1. Accordingly, the total number of feature subsets 
that can be formed constitutes the search space. Now, the objective is to search for an efficient feature 
subset which comprises less redundant features. The best feature subset is found out by checking all the 
possible feature subsets. However, this is a brute force method and becomes unwieldy when the feature 
space dimensions n becomes large, as in big datasets. Metaheusristics (evolutionary algorithms (EAs) 
subsumed)  have  demonstrated  their  superiority  over  conventional  optimization  methods  in  solving 
various combinatorial and continuous optimization problems. Metaheuristics are of two different types: 
(i) point-search-based methods such as Threshold Accepting (TA), Simulated Annealing (SA) etc., (ii) 
EAs, which are population-based EAs such as genetic algorithgm (GA), differential evolution (DE), 
etc. The present research study posed the FSS in a single objective environment where the objective 
function maximizes the AUC, thereby  selecting the feature subsets of length less than or equal to  k 
(where k < n) while achieving the best possible AUC.   

As the data is generated in large volumes at a phenomenal rate, scalability becomes a major 
concern while developing solutions to analyse such big data. Therefore, designing scalable solutions 
gained its prominence. MapReduce [70] is a programming paradigm used in handling such big data. It 
mainly consists of two steps: map and reduce. MapReduce solutions are proven to be scalable. There 
are different big data frameworks available to design MapReduce solutions. Among them Apache Spark 
is  faster  in  nature  due  to  its  in-memory  computation  feature.  Apache  Spark  is  an  open-source,  fast 
computing  distributed  engine  used  to  handle  such  large  amounts  of  data.  Spark  uses  in-memory 
computing by using Resilient Distributed Datasets (RDD) that boost the performance, thereby avoiding 
the disk-access. RDD is inherently distributed in nature, follows the lazy evaluation and is immutable 
in nature. Apache Spark also provides versatility by leveraging to combine with other big data tools 
such as Hadoop.  

Extant EA -based wrapper algorithms are sequential and limited to small datasets. Even though 
they can  be  applied  on  larger  datasets,  they  perform  poorly.  In  the current  world,  the generation of 
datasets is growing by leaps and folds, thereby demanding the development of scalable wrappers for 
FSS. There is a growing need to develop such parallel wrappers for FSS in the context of big data. This 
motivated  us  to  propose a  scalable  wrapper  for  FSS  in  single  objective  environment.  In  the current 
study, the objective function is to select the feature subset of length less than or equal to k (where k < 
n) while achieving the best possible AUC. To the best of our knowledge, no work is reported so far, 
where  feature  subset  selection  is  either  performed  by  a  scalable  wrapper  involving  parallel  and 
distributed EC techniques or their hybrids under the Apache Spark environment.  

This paper’s significant contributions are as follows: (i) Parallel DE is designed under Apache 
Spark to develop wrappers for FSS. (ii) Binary versions of the TADE and DETA are developed and 
parallelized under Apache Spark. These are named as PB-TADE and PB-DETA respectively. (iii) Then, 
these are invoked to develop wrappers for FSS, where logistic regression is chosen as the classifier to 

2 | P a g e  

 
evaluate the fitness function, namely the AUC. (iv) To achieve scalability and algorithm parallelization, 
we proposed a novel MapReduce-multithread based framework. 

The remainder of the paper is structured as follows: Section 2 presents the literature review.  
Section 3 presents an overview of TA and DE. Section 4 presents the proposed methodology. Section 
5 describes the datasets and experimental setup. Section 6 discusses the results obtained by the models. 
Finally, section 7 concludes the paper. 

2. Literature Review 
Differential evolution, one of the widely used algorithms for the feature subset selection, is proposed 
by Storn and Price [3]. Table 1 presents the details the filter, wrapper sequential versions of DE, where 
feature selection is posed as a combinatorial optimization problem. Zhang et al.[4] proposed a modified 
DE with self-learning (MOFS-BDE). In [4], authors had introduced three different operators, namely: 
(i) modified binary mutation operator based on the probability difference, (ii) one bit purifying search 
operator (OPS) to improve the self-learning capability of the elite individuals, and (iii) non-dominated 
sorting in the selection phase to reduce the computational complexity involved in the selection operator. 
Vivekanandan and Iyengar [5] designed a two phase solution, where the critical features are selected 
by the DE  and fed  into the integrated model of the feed-forward neural network and fuzzy  analytic 
hierarchy  process  (AHP)  [6].  Nayak  et  al.[7],  proposed  FAEMODE,  a  filter  approach  using  elitism 
based multi-objective DE. It can handle both the linear and non-linear dependency among the features 
via both the correlation coefficient (PCC) and mutual information (MI). Mlakar et al. [8] designed a 
multi-objective DE (DEMO) based wrapper for facial recognition systems. In their approach, initially, 
the important features are extracted based on the histogram of oriented gradient descriptor (HOG) and 
fed to the DEMO to find the pareto optimal solutions. Khushaba et al. [9] proposed DE with a statistical 
repair  mechanism,  DEFS,  for  selecting  the  optimal  feature  subsets  in  datasets  with  varying 
dimensionality.  In  their  proposal,  the  probability  of  the  feature  distribution  is  fed  to  DEFS  by  the 
roulette wheel. Hancer et al. [10] proposed filter-based DE, MIFS, where the fisher score determines 
the mutual relevance between the features and class labels. The features are assigned a rank based on 
their fisher score. Hancer [11] proposed a new multi-objective differential evolution (MODE-CFS) with 
two stage mutation, (i) centroid-based mutation to perform clustering and (ii) feature-based mutation to 
perform feature selection. Non-dominated sorting is applied to determine the pareto optimal solution 
set. Ghosh et al. [12] proposed self-adaptive DE (SADE) based wrapper for the feature selection in the 
hyperspectral  image  data.  The  selected  feature  subsets  are  fed  into  fuzzy-KNN  to  obtain  accuracy. 
Bhadra and Bandyopadhyay [13] improved the modified DE. They proposed an unsupervised feature 
selection approach called MoDE with the objective functions as (i)  average dissimilarity of the selected 
feature subset, (ii) average similarity of the non-selected feature subset, and (iii) the average standard 
deviation of  the  selected  feature  subset. All  the  objectives  mentioned  above  use  normalized  mutual 
information. Baig et.al.[14] proposed modified DE based wrapper for the Motor Imagery EEG having 
a high dimensional dataset. SVM is used here as a classifier. Almasoudy et al. [15] designed a wrapper 
feature selection based on modified DE. The authors considered Extreme Learning Machine (ELM) as 
a classifier and tested its effectiveness over the intrusion detection dataset NSL-KDD.  Zorarpaci and 
Ozel [16] proposed a hybrid FS approach based on DE and Ant-Bee Colony (ABC), where the J48 
classifier from Weka computes the fitness score. This hybrid model achieved a significant F-score with 
less cardinal feature subset than the stand-alone DE and stand-alone ABC. Then, a quantum-inspired 
wrapper based on DE (QDE) with logistic regression as the classifier was proposed by Srikrishna et al. 
[17]. They reported that QDE achieved better repeatability than the BDE. Lopez et al. [18] proposed a 
wrapper  based  on  permutation  DE,  where  the  permutation-based  mutation  replaced  the  mutation 
operator,  and  the  diversity  of  the  generated  children  solutions  is  controlled  by  using  a  modified 

3 | P a g e  

 
 
recombination operator. Zhao et  al.  [19] developed a two-stage wrapper feature selection algorithm, 
where in the first stage, the fisher score and information gain are applied to filter the redundant features. 
Then  in  the  second  stage,  the  top-k  features  are  passed  to  the  modified  DE  to  perform  the  feature 
selection on four different breast cancer datasets. Hancer [20], for the first time, used fuzzy and kernel 
measures  as  filters  to calculate the mutual relevance and redundancy with DE  to handle continuous 
datasets. Li et al.[21] designed DE-SVM-FS and compared it with the default SVM approach, and they 
demonstrated  that  the  DE  and  SVM-based  FS  achieved  better  accuracy  than  the  stand-alone  SVM. 
Wang  et  al.[22]  proposed  DE-KNN,  where  the  KNN  is  the classifier.  DE-KNN  performed  both  the 
feature selection as well as the instance selection. 

Table 1: Sequential versions of DE and its wrapper variants 
Algorithm 
# Objectives  
Self-Learning DE 
Multi Objective 
Modified DE 
Single Objective 

Wrapper (classifier) / Filter 
Wrapper (KNN) 
Filter 

and 

and 

Multi Objective  
Multi Objective  
Single Objective 
Multi Objective 
Multi Objective 
Multi Objective 
Multi Objective 

Authors  
Zhang et.al. [4] 
Vivekanandan 
Sriraman [5] 
Nayak et.al. [7] 
Mlakar et.al. [8]  
Khushaba et.al.[9] 
Hancer [11] 
Hancer et.al. [10] 
Ghosh et.al. [12] 
Bhadra 
Bandyopadhyay [13] 
Bhaig [14] 
Multi Objective 
Almasoudy et.al. [15]  Multi Objective 
Single Objective 
Zorarpaci et.al. [16] 
Single Objective 
Srikrishna et.al[17] 
Single Objective 
Lopaez et.al. [18] 
Single Objective 
Al-ani [67] 
Single Objective 
Zhao et.al. [19] 
Multi Objective 
Hancer [20] 
Single Objective 
Li et.al. [21] 
Single Objective 
Wang et.al. [22] 
Single Objective 
Krishna and Ravi [68] 

FAEMODE 
DE+HOG 
DEFS 
MODE-CFS 
DE + MIFS 
SADE 
MoDE 

Filter 
Wrapper (SVM) 
Filter 
Filter 
Filter 
Wrapper (Fuzzy-KNN) 
Filter MI 

Modified DE 
Modified DE 
DE + ACO 
Quantum DE 
DE-FS(pm) 
DE +Wheel based strategy  Filter 
Modified DE 
DE 
DE 
DE 
Adaptive DE 

Wrapper (SVM) 
Wrapper (ELM) 
Weka J48 classifier 
Wrapper (LR) 
Wrapper (SVM) 

Wrapper (SVM) 
Filter(Fuzzy+Kernel) 
Wrapper (SVM) 
Wrapper (KNN) 
Wrapper (LR) 

Along with DE, several other evolutionary algorithms [23–27] considered the FSS problem like 
a combinatorial optimization problem. Khammassi and Krichen [28] proposed two schemes, namely, 
(i) The NSGA-BLR approach to handle binary-class datasets and (ii) The NSGA-MLR to handle multi-
class network intrusion datasets. Chaudari and Sahu [29] proposed a binary version of the popular crow 
search  algorithm  (CSA)  with  time-varying  flight  in  wrapper version  BCSA-VF.  Binary  Dragon  Fly 
algorithm (BDA) is proposed by Too and Mirjalili [30] by taking the Covid-19 dataset as a case study. 
Several variants of DE are employed, for example, in the estimation of tool-wear during the milling 
process  [31],  optimal  resource scheduling  [32],  energy-efficient  model  [33],  and  anomaly  detection 
[34]. 

Table 2 Parallel and distributed versions of DE and its variants  

Authors 

Algorithm 

Environment 

Problem solved 

4 | P a g e  

 
 
Zhou[39] 

DE 

Spark 

DE 
Teijeiroet.al. [40] 
Chou et.al.[31] 
DE 
Al-Sawwa and Ludwig[42]  DE 
Chen et.al.[43] 
Adhianto et.al.[44] 
Liu et.al. [69] 

Modified DE 
DE 
DE 

Spark + AWS 
Spark 
Spark 
SPMD 
OpenMP 
Distributed Cloud  

Deng et.al. [45] 

DE 

Spark  

Wong et.al. [47] 

Self-Adaptive DE 

CUDA 

He et.al. [46] 

Five variants of DE  Spark + Cloud  

Cao et.al. [48] 

DPCCMOEA 

MPI 

Ge et.al. [49] 

DDE-AMS 

MPI  

Falco et.al.[50] 
DE 
Veronse and Krohling [51]  DE 

MPI  
CUDA 

Glotik et.al. [52] 
Thomert et.al. [55] 
Daoudi et.al.[53] 
Kromer et.al. [54] 

PSADE 
NSDE-II 
DE 
DE 

MATLAB 
OpenMP 
Hadoop 
Unified Parallel C 

circuit 

electronic 

Pros  and  cons  of  various 
approaches is discussed 
Tested on benchmark functions 
Clustering 
Designed a DE based classifier 
Cluster Optimization 
Optical Network problem 
Power 
optimization 
Tested on benchmark functions 
and reported speedup 
Tested on benchmark functions 
and reported speedup 
topology 
Developed  a  ring 
model 
evaluated  on 
and 
benchmark  functions  to  report 
speedup 
co-evolutionary 
Developed 
based  DE  to  solve  large  scale 
optimization 
Developed adaptive population 
model  to  solve  large  scale 
optimization 
Resource allocation 
To 
solve 
optimization 
environment 
Hydro Scheduling algorithm 
Cloud work placements 
Clustering 
To 
optimization problems. 

scale 
GPU 

large 
in 

solve 

scale 

large 

Now we shift our attention to the works more relevant to the current study. Several parallel and 
distributed versions of the evolutionary algorithms [35–38] are proposed to handle high-dimensional 
datasets  and  big  data.  Various  parallel  and  distributed  implementations  of  the  DE  are  presented  in 
Table 2. Zhou [39] discussed various strategies for implementing parallel DE MapReduce versions and 
their pros and cons in the Hadoop distributed framework. Teijeiro et al. [40] designed parallel DE under 
Spark  environment,  and  the  experiments  were  conducted  in  AWS  cloud  environment  to  solve 
benchmark optimization problems. Recently, Cho et al. [41] designed a parallel version of DE to solve 
large-scale clustering problems. Another parallel version of DE Classifier (SCDE) was proposed by Al-
Sawwa and Ludwig [42] to handle the imbalanced data. SCDE finds the optimal centroid and assigns 
the class to the data point based on the Euclidian distance. Chen et al. [43] proposed a parallel version 
of the modified DE using single-program multiple-data (SPMD), with the improved genetic operators. 
They employed both fine-grained and coarse-grained approaches for cluster optimization. Adhianto et 
al. [44] proposed a fine-grained parallel version of DE using OpenMP to solve the optical networking 
problem, where the shared memory multi-processing is supported. Deng et al. [45] proposed a parallel 
DE for solving the benchmark functions and reported the speedup. He et al. [46] proposed the parallel 
framework for five variants of DE under the Spark cloud computing platform. They had analyzed the 
speedup by solving the benchmark functions. Wong et al. [47] developed the Computed Unified Device 
Architecture (CUDA) based framework for self-adaptive DE for solving the benchmark functions. Cao 

5 | P a g e  

 
 
et al. [48] proposed a message passing interface (MPI) based co-evolutionary version of DE, where the 
population is divided and co-evolved together to solve large-scale optimization problems. Ge et al. [49] 
proposed  an  adaptive  merge  and  split  strategy  for  DE,  namely,  DE-AMS  using  MPI,  to  improve 
resource  utilization,  which  is  a  vital  aspect  to  minimize  while  handling  large-scale  optimization 
problems. Falco et al. [50] designed MPI-based DE under a CUDA grid environment and tested it on 
different resource allocation strategies. Veronse and Krohling [51] developed the first implementation 
of the CUDA version of DE. The proposed algorithm was tested on well known benchmark functions, 
and  the  computing  time  had  been  compared  with  the  standalone  implementation.  Glotik  et.al.[52] 
parallelized  the  DE  using  MATLAB  to  solve  the  hydro-scheduling  problem.  Daoudi  et  al.  [53] 
developed  the  MapReduce  version  of  DE  under  the  Hadoop  environment  to  solve  the  clustering 
problem. Kromer et al. [54] developed a parallel version of DE using Unified-C to handle large-scale 
clustering problems. Thomert et al. [55] developed a parallel version of DE using OpenMP to achieve 
the optimized workflow placement into the realm of practical utility. 

The  drawbacks  in  the  extant  approaches  are  as  follows:  (i)  The  existing  EC  based  wrapper 
techniques, listed in Table 1, are limited to apply on small datasets and are sequential. Even though 
these  can  be  applied  on  large  datasets,  the  performance  will  be  poor  comapred  to  their  parallel 
counterparts. (ii) Moreover, the current parallel and distributing EC techniques ( refer to Table 2) are 
not yet applied to FSS. These drawbacks motivated us to design scalable, parallel EC based wrapper 
techinques. 

3. Overview of the methods employed 

Evolutionary  algorithms  (EAs)  are  effective  in  getting  global  or  near-global  optimal  solutions.  The 
designing of EAs heuristics is inspired by natural selection and social behaviour. EA is an approach 
where the solutions are evolved throughout the process using Darwinian evolution principles. They start 
by initializing the population of solutions randomly. This population is evolved in order to determine 
better solutions. For the evolution to occur, EA’s utilize specialized heuristics to generate new solutions 
and compute the corresponding fitness score given by the fitness function or (objective function).  EA’s 
are perfect for both continuous and binary search space.  

3.1 Binary Differential Evolution 

Differential Evolution (DE), a stochastic population-based global optimization algorithm, includes 
the  heuristics,  namely  mutation,  crossover,  and  selection.  The  mutation  operation  produces  the 
mutant vector, which along with the candidate solution vector, undergoes crossover operation to 
generate  the  trail  vector.  At  last,  the  selection  operation  is  applied  over  the  candidate  solution 
vectors  and  trail  vector  to  produce  offspring.  As  mentioned  earlier,  this  is  continued  till  the 
completion of maximum iterations or other convergence criteria, if any, is met. 

3.2 Binary Threshold Accepting 

Dueck and Scheuer [56] proposed Threshold Accepting (TA) algorithm. Later, Ravi and Zimmermann 
[57] optimized a fuzzy rule-based classification model using modified TA. They developed the solution 
in  three  phases:  feature  selection  was  used  as  a  preprocessing  step,  a  modified  fuzzy  rule-based 
classification system was invoked over the selected feature subset, and finally, modified TA (MTA) 
was invoked to minimize the rule base size while guaranteeing high accuracy. Ravi et al. [58] proposed 
a modified TA (MTA) minimize the number of rules in a fuzzy rule-based classification system. Then, 

6 | P a g e  

 
 
 
 
 
 
 
Ravi and Zimmerman [59] proposed a continuous version of TA as an alternative to the backpropagation 
algorithm to overcome its limitations while training a neural network model. The trained neural network 
was utilized for feature selection, and the selected features were fed to the fuzzy classifier optimized by 
the MTA proposed in [58]. Later, Ravi and Pramodh [60] proposed principal component neural network 
architecture  trained  by  TA  for  bankruptcy  prediction  of  banks.  Chauhan  and  Ravi  [61]  proposed  a 
hybrid model DE with TA to solve unconstrained benchmark problems.  

Threshold Accepting is a deterministic variant of simulated annealing. BTA, a binary version 
of the TA (named MTA in [58]), is presented in Algorithm 1. BTA performs neighbourhood search by 
flip-flopping the bits in the current solution vector one at a time, starting them in the left-most position. 
Each flip flop yields one neighbourhood solution. If the first neighbourhood solution is not accepted, 
then the bit is reversed to its original value. Likewise, 2nd bit is flip-flopped so on and so forth until a 
neighbourhood  solution  is  accepted.  However,  it  is  not  necessary  to  exhaustively  search  all  the 
neighbourhood  solutions.  Searching  a  few  of  them  would  be  sufficient,  and  that  number  is  a  user-
defined parameter. The rest of the heuristics of the BTA are typical to that of the TA. 
Algorithm 1: Threshold Accepting (TA) algorithm 

1: Choose an initial configuration 
2: Choose an initial THRESHOLD T > 0 and small number > 0 
3: Opt: choose a new solution which is a small stochastic       
   perturbation of the old solution. 
4: compute AE:=  

fitness (new solution) – fitness (old solution) 

5: IF AE < T THEN 
6:   old solution := new solution 
7:   IF in a long time, there is no increase in quality or  
8: 
9:    
10:   IF some time no change in quality anymore THEN stop 
11:   GOTO opt 

Lower threshold by the equation T= T*(1- T) 

too many iterations THEN 

4. Proposed Scalable, Distributed, Parallel Wrapper 

To perform FSS, we chose DE and TA, and built hybrids around them. DE explores and exploits the 
search space globally and is stochastic in nature. However, DE sometimes gets bogged down in local 
minima  before  convergence  [32],  thereby  slowing  down  the  convergence  rate.  If  the  search  space 
becomes large, this phenomenon gets accentuated. Hence, it needs support from a local search-based 
optimization algorithm. Here, we chose to employ binary threshold accepting (BTA) for that purpose. 
The deterministic way of accepting the candidate solutions in BTA helps in exploration, exploitation 
and fast convergence.  

Even though the EAs are intrinsically parallel, explicit parallel versions of EAs have to be designed 
so that they meet the following requirements: optimal utilization of the distributed resources, scalability, 
and low communication overhead. In general, the parallelism from the population perspective of EAs 
is achieved by two main models: 

1.  Master-slave (MS) strategy [40], which is also known as the global model. It has only a single 
global population.  Here, the master takes the responsibility of applying metaheuristics (EAs 
subsumed) while the slave manages the fitness function evaluation. 

7 | P a g e  

 
 
 
 
2.  The  island  strategy  [40]  is  where  the  population  is  divided  into  islands,  upon  which  the 

heuristics (operators) are applied independently. 

The types mentioned above differ in the underlying topology and the migration rules to determine 
the communication between the nodes. A hybrid strategy  can also  be designed  by combing the two 
strategies. In the present paper, we designed, MapReduce multithreaded framework, which mimics the 
combination of above-mentioned strategies.  

The distributed framework like Apache Spark is not affected by the underlying topology because, 
in these frameworks, independent of the underlying topology, migrant solutions are broadcast to all the 
partitions. 

The comparative analysis is carried out across PB-DE, PB-DETA, and PB-TADE to establish the 
importance of hybrid global and local search optimization heuristics. We designed parallel BTA, too, 
independent of the DE. If BTA alone is employed for FSS, then it consumed enormous computational 
time without yielding useful results. Hence, the comparative study excludes BTA. All the approaches 
follow the same solution encoding scheme and the population RDD encoding scheme as mentioned in 
section 4.1. 

4.1 Wrapper Based FSS and solution encoding 

Kohavi  and  John  [71]  are  the  pioneers  in  proposing  the  wrappers  for  FSS  by  posing  the  FSS  as  a 
combinatorial optimization problem. Here, wrappers take the help of a classifier or a regression model 
since it may involve the fitness score evaluation of a given feature subset.  

4.1.1 Solution Encoding scheme 

Wrappers using metaheuristics (EAs subsumed) require the fundamental step of solution encoding. EAs 
randomly initiate the  population consisting  of a set of solution  vectors.   Each solution vector in the 
population  represents  a feature  subset.  Such  a  solution  vector  comprises  bits,  where  1  indicates  the 
presence, and 0 indicates the absence of a feature. The length/dimension of this array is equal to the 
number of features in the dataset.  

4.1.2 Population RDD Encoding Scheme 

Let  the  population,  denoted  by  P,  be  initialized  randomly  using  biased  sampling  from  Uniform 
distribution between 0 and 1 as presented in Algorithm 2. Our objective is to select the less cardinal 
feature subsets yielding the highest AUC value. Hence, the parameter in the biased sampling is taken 
as 0.99. If the pseudo-random number is greater than 0.99, then the bit is assigned the value 1, indicating 
the  feature's  presence,  and  0,  otherwise,  meaning  the  absence  of  the  feature.  Thus,  according  to 
Algorithm 2, the initialized population is taken as population RDD and the dataset as different RDD. 
The population RDD is presented in Table 3. The key is solution-id, and the first index of the value is 
the solution vector of binary type. It conveys which feature subset is selected. The second index stores 
the names or ids of the features, which helps us form the column-reduced dataset and the construction 
of  pipeline  RDD  for  the  respective  solutions.  The  last  index  stores  the  AUC  corresponding  to  the 
solution vector. 

8 | P a g e  

 
 
 
 
 
 
 
 
 
Algorithm 2: Biased sampling driven population initialization 

Input: n: population size, nfeat: number of features 
Output: P: population RDD 

While (j<nfeat){ 

If rand() > 0.99: 

1: While (i<n){ 
2: 
3: 
4: 
5: 
6: 

} 

Feature is selected 
} 

Fig 1: Schema of the Population RDD 

Key 
Key{(cid:2869)}  <  Binary Vector{(cid:2869)}, selected Features{(cid:2869)}, AUC Scores{(cid:2869)} > 
Key{(cid:2870)}  <  Binary Vector{(cid:2870)}, selected Features{(cid:2870)}, AUC Scores{(cid:2870)} > 

Value : Solution Vector 

... 

… 

Key{(cid:2924)}  <  Binary Vector{(cid:2924)}, selected Features{(cid:2924)}, AUC Scores{(cid:2924)} > 

4.2 Parallel Binary Differential Evolutionary (PB-DE) 

This section discusses the proposed parallel binary differential evolution (PB-DE) for FSS under Spark. 
The PB-DE based wrapper algorithm is presented in Algorithm 3, and the flowchart of the execution 
flow is depicted in Appendix 1.  

4.2.1 Population Initialization 

The PB-DE algorithm starts by initializing the population according to Algorithm 2. Thus initialized 
population is stored in a different RDD and follows the structure as depicted in Fig.1. All the required 
parameters also get initialized. Then, the following information is broadcasted to all the nodes: mutation 
factor (MF), crossover rate (CR), number of features (nfeat), and population size (n). Once a variable is 
broadcasted, it is cached by the executor and utilized for other RDD operations, viz., transformations 
and actions. They are meant for read-only variables. By broadcasting, these variables are shared across 
the cluster, thereby reducing the communication overhead.  

4.2.2 Fitness Score Evaluation on the Initial Population 

Once the population is initialized, its fitness score is computed. As said earlier, the Population RDD is 
divided into different partitions, and each partition represents an island. These islands asynchronously 
undergo  BDE  heuristics  as  a  separate  thread  in  Spark.  Using  this  Binary  vector  information,  the 
corresponding  selected  features  are  obtained  and  updated  in  the  selected  feature  column  of  the 
population RDD. Every time there is a change in the binary vector, the id information also gets updated. 
EA  spends  most  of  the  time  computing  the  fitness  value.  Hence,  adopting an  asynchronous  way  of 
fitness score (namely, AUC) calculation is the major task. This is achieved by initiating the thread pool 
mapper, where the number of threads is equal to the number of worker nodes. One cannot create the 
number of threads arbitrarily. Hence, after rigorous trials, the ideal size of the thread pool is found to 

9 | P a g e  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
the number of worker nodes. The size of the thread pool also affects the speedup. A low size thread 
pool leads to poor performance, whereas a higher thread leads to huge communication overhead.  

Each thread in the thread pool is responsible for computing the AUC of the population island.  
Once this map is called, the reduced dataset is obtained based on the feature subset information using 
the selected feature  index of each  solution. Then, the ML pipeline is constructed where the reduced 
dataset and LR model are bound together, thereby giving pipelined RDD. Such a generated pipelined 
RDD  is  a  subclass  of  the  RDD,  an  immutable  and  partitioned  collection  of  elements  where  all  the 
operations are executed in parallel. LR model is trained by creating the vector assembler that has to be 
created  for  the  corresponding selected features.  Thus,  vector  assembler  is  created over  the  obtained 
reduced data frame. This kind of vector assembler is created for every solution in each iteration. As we 
have adopted the pipelined RDD, the above operations are executed in parallel and distributed across 
the nodes, thereby achieving fine-grained parallelism. Later, the AUC is evaluated with the training 
dataset in the same thread. All this process is repeated for each thread in the fitness value computation 
step. By adopting this strategy, the computation of AUC is performed in an asynchronous fashion. The 
same strategy is employed in all the proposed approaches to parallelize the fitness function evaluation. 
Thus, the AUC’s are updated in the population RDD accordingly. This serves as the parent population. 
All the subsequent iterations follow the same thread pool mapper strategy to evaluate AUC. 

4.2.3 Training phase 

Using the binary vector field information the population undergoes DE heuristics, viz., Crossover and 
Mutation and forms the offspring population following the structure depicted in Fig.1. Using the Binary 
Vector information of thus formed offspring, their corresponding selected feature is also updated. Then 
the thread pool mapper is called, where the LR model evaluates the AUC on the reduced datasets. With 
the obtained AUC of each solution, their corresponding AUC column is also updated. Selection operator 
is applied on both the parent and offspring populations and the worst parent with less AUC is replaced 
with the better offspring with higher AUC. These better solutions are formed as one population, which 
serves as the parent population for the next iteration. This process of computing AUC and selecting the 
better solutions, thereby achieving the evolution of the population, is repeated for maxIter iterations.  

4.2.4 Test phase 

Thus evolved population obtained after maxIter is evaluated on the test dataset in the test phase. Here 
also the AUC is obtained by using a thread pool mapper only. Later, the evolved population is nothing 
but the set of the required feature subsets along with their corresponding test AUC. 

10 | P a g e  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Algorithm 3: Proposed PB-DE based wrapper 

Input: P: population RDD, X: Input Dataset RDD, maxIter: maximum 
iterations, MF: Muation Factor, CR: Cross over rate, n: population 
size, nfeat: number of features   

Output: P: Evolved population after maxIter 

1: i 0; 
2: Initialize the population by biased sampling driven approach and 
create population RDD; //Population initialization 
3: Divide the population into partitions, each partition serves as an  
   Island. 
4: Create a thread pool of size number of nodes in the cluster; 
     // Fitness score evaluation on Initial population 
5: Evaluate the fitness function, each thread is responsible to train 
LR model and evaluate the AUC for a solution in the population; 
6: Broadcast the variables MR, CR, n, nfeat; 

7: While(i< maxIter){ // Training phase 
8:   Each sub population undergoes DE heuristics via BDE Mapper; 
9:   Create a thread pool of size number of nodes in the cluster; 
10:   Evaluate the fitness function, each thread is responsible to 
train LR and evaluate the AUC for every solution in the 
population. 

11:   Replace worst parent solutions with best children solutions; 
12:   Update the population RDD; 
13:   Increment i  i+1; 
14:   } 
15:  Evaluate on Test dataset // Test  phase 
16:   return Population, test AUC.  

4.3 Parallel BDETA (PB-DETA) 

It is important to note that the sequential DETA, which is parallelized and implemented in a distributed 
environment, is quite distinct from the DETA presented in Chauhan and Ravi [61]. The latter was a 
loosely–coupled  hybrid  system,  wherein  after  DE  converged,  TA was  invoked,  whereas  the  current 
version of DETA is a tightly coupled system, in that DE and TA are invoked in tandem in every iteration 
and  the  hybrid  algorithm  is  run  for  maxIter1  iterations.  Further,  in  the  former,  DE  and  TA  were 
independently for many iterations, whereas we employed relaxed convergence criterion viz. running 
the  two  algorithms  in  tandem  for  only  maxIter1  iterations.  This  is  strategy  is  designed  to  reduce 
computational time, primarily because we deal with big data sets in this paper.   

The  proposed  parallel  approach  PB-DETA  is  presented  in  Algorithm  4,  the  schematic 
representation is depicted in Fig.2, and the flow chart of the execution flow is depicted in Appendix 2.  

4.3.1 Population Initialization 

PB-DETA algorithm also follows the same population initialization strategy of PB-DE and follows the 
structure as depicted in Fig.1. All the initialized parameters such as mutation factor (MF), crossover 
rate (CR), number of features (nfeat) and population size (n) are broadcasted.  

11 | P a g e  

 
 
 
 
 
 
 
 
 
4.3.2 Fitness Score Evaluation on the Initial Population 

As explained earlier, EA algorithms spend most of the time evaluating the fitness scores. Hence, the 
same strategy which is used to parallelize the PB-DE evaluation phase is also used here. The fitness 
evaluation  is  done in  parallel and  asynchronously  by  creating  a thread pool  of  the  size  equal  to the 
number of nodes in the cluster.  

𝑆(cid:2869) 

𝑆(cid:2870) 

𝑆(cid:2871) 

… 

𝑆(cid:3041) 

Binary Differential Evolution 

(cid:4593) 
𝑆(cid:2869)

(cid:4593)  
𝑆(cid:2870)

(cid:4593)  
𝑆(cid:2871)

… 

(cid:4593)  
𝑆(cid:3041)

TA-1 

TA-2 

TA-3 

… 

TA-n 

(cid:4593)(cid:4593) 
𝑆(cid:2869)

(cid:4593)(cid:4593) 
𝑆(cid:2870)

(cid:4593)(cid:4593) 
𝑆(cid:2871)

… 

(cid:4593)(cid:4593) 
𝑆(cid:3041)

DE Phase 

Population 
based  
TA Phase 

Fig. 2 Schematic representation of the DETA based wrapper 

4.3.3 Training phase 

Using the binary vector field information, the parent population undergoes BDE heuristics, viz, 
crossover,  and  population,  forming  the  offspring  population.  Thread  pool  mapper  is  called  on  the 
offspring population to evaluate the AUC thereafter and the corresponding fields are updated. Then, the 
better  offspring  solutions  replace  the  parent  solutions.  Thus  evolved  population  undergoes  BTA 
heuristics as given in Algorithm 2. Then, the thread pool mapper is called on, thus forming the offspring 
population to evaluate AUC, and also the corresponding fields are also updated. Here is the offspring 
solution, which is not much worse as per the threshold limit value, replaces the solution in the parent 
population.  BTA  heuristics  are 
for maxIter2 times.  Thus  emerged  population 
after maxIter2 times serves as parent population.  

invoked 

After 

this, 

the  above  whole  process  of  BDE  and  BTA 

in 

tandem 

is  repeated 

until maxIter1 iterations are completed. 

12 | P a g e  

 
 
 
 
 
 
 
Algorithm 4: Proposed parallel PB-DETA based wrapper 

Input: P: population RDD, X: Input Dataset RDD, maxIter1: maximum DE 
iterations, maxIter2: maximum TA iterations, MF: Mutation Factor, CR: 
Cross over rate, n: population size, nfeat: number of features, T: 
Threshold rate, eps: steps by which epsilon is decreasing after each 
iteration 

Output: P: Evolved population after maxIter1 

1: i  0; 
2: j  0; 
3: T  0.05; 
4: eps  0.95; 
5: Initialise the population by using biased sampling driven  
   approach and create population RDD; //Population initialization 
6: Divide the population into partitions, each partition serves  
   as an island;  
    // Fitness score evaluation on Initial population 
7: Create a thread pool of size number of nodes in the cluster; 
8: Evaluate the fitness function, each thread is responsible to  
   evaluate the AUC for a solution in the population; 
9: Broadcast the variables MR, CR, n, nfeat; 

10: While(i< maxIter1){ // Training phase 
Each subpopulation undergoes DE heuristics via BDE Mapper; 
11: 
12: 
Create a thread pool of size number of nodes in the cluster; 
13:   Evaluate the fitness function, AUC for a solution in the population; 
Replace worst parent solutions with best children solutions; 
14: 
Update the population RDD; 
15: 
Increment i  i+1; 
16: 
j  0; 
17: 
While(j<maxIter2){ 
18: 
19: 
20: 
21: 

Each subpopulation undergoes TA heuristics via BTA Mapper; 
Create a thread pool of size number of nodes in the cluster; 
Evaluate the fitness function, each thread is responsible  
to evaluate the AUC for a solution in the population; 
Replace  parent  solutions  with  not  much  worse  children 

22: 
solutions; 
23: 
24: 
25: 
26: 
27: 
28: 
29: 

Update the population RDD; 
Increment j  j+1; 
eps  eps*(1-W); 
} 

} 
Evaluate on Test dataset  // Test phase 
return Population, test AUC.  

13 | P a g e  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4.3.4 Test phase 
Then, the test phase begins, where the obtained evolved population after maxIter1 number of iterations 
is evaluated on the test dataset. Here also the AUC is computed by using a thread pool mapper. Later, 
the so evolved population is nothing but the set of required feature subsets with their corresponding test 
AUC. 

4.4 Parallel TABDE (PB-TADE) 

The proposed parallel approach PB-TADE is presented in Algorithm 5, the schematic representation is 
depicted in Fig. 3, and the flow chart is depicted in Appendix 3.  

𝑆(cid:2869) 

𝑆(cid:2870) 

𝑆(cid:2871) 

… 

𝑆(cid:3041) 

TA-1 

TA-2 

TA-3 

… 

TA-n 

(cid:4593) 
𝑆(cid:2869)

(cid:4593)  
𝑆(cid:2870)

(cid:4593)  
𝑆(cid:2871)

… 

(cid:4593)  
𝑆(cid:3041)

Binary Differential Evolution 

(cid:4593)(cid:4593) 
𝑆(cid:2869)

(cid:4593)(cid:4593) 
𝑆(cid:2870)

(cid:4593)(cid:4593) 
𝑆(cid:2871)

… 

(cid:4593)(cid:4593) 
𝑆(cid:3041)

Population 
based TA 
 Phase 

DE Phase 

Fig.3: Schematic representation of the PB-TADE based wrapper 

4.4.1 Population Initialization 

PB-TADE algorithm also follows the same population initialization strategy of PB-DE and follows the 
structure as depicted in Fig.1. All the initialized parameters such as mutation factor (MF), crossover 
rate (CR), number of features (nfeat) and population size (n) are broadcasted.  

4.4.2 Fitness Score Evaluation on the Initial Population 

As explained earlier, EA algorithms spend most of the time evaluating the fitness scores. Hence, the 
same strategy which is used to parallelize the PB-DE evaluation phase is also used here. The fitness 
function is evaluated in parallel and asynchronously by creating a thread pool of the size equal to the 
number of nodes in the cluster. 

14 | P a g e  

 
 
 
 
 
 
 
 
 
 
 
Algorithm 5: Proposed parallel PB-TADE based wrapper  

Input P: Population RDD, X: Input Dataset RDD, maxIter1: maximum iterations 
BDE invokes, maxIter2: maximum iterations BTA invokes, MF: Mutation Factor, 
CR:  Crossover  Rate,  n:  population  size,  nfeat:  number  of  features,  T: 
Threshold  rate,  eps:  steps  by  which  epsilon  is  decreasing  after  each 
iteration 

Output P: Evolved Population after maxIter1  

1: i  0; 
2: j  0; 
3: T  0.05; 
4: eps  0.95; 
5: Initialise the population by using biased sampling driven approach  
   and create population RDD; //Population initialization 
6:  Divide  the  population  into  partitions,  each  partition  serves  as  an                
island; 
// Fitness score evaluation on Initial population 
7: Create a thread pool of size number of nodes in the cluster; 
8: Evaluate the fitness function, each thread is responsible to evaluate  
   the AUC for a solution in the population; 
9: Broadcast the variables MR, CR, n, nfeat; 
10: While(i< maxIter1){ // Training phase 
11:  j  0; 
12: 
13: 
14: 
15: 

Each subpopulation undergoes TA heuristics via BTA Mapper; 
Create a thread pool of size number of nodes in the cluster; 
Evaluate the fitness function, each thread is responsible  
to evaluate the AUC for a solution in the population; 
Replace parent solutions with not much worse children  
solutions; 
Update the population RDD; 
Increment j  j+1; 
eps  eps*(1 – W); 
} 

17: 
18: 
19: 
20: 
Each subpopulation undergoes DE heuristics via BDE Mapper; 
21: 
22: 
Create a thread pool of size number of nodes in the cluster; 
23:   Evaluate the fitness function, each thread is responsible to evaluate 

While(j<maxIter2){ 

16: 

the AUC for a solution in the population; 
Replace worst parent solutions with best children solutions; 
Update the population RDD; 
Increment i  i+1; 
} 
Evaluate on Test dataset // Test phase 
return Population, test AUC. 

24: 
25: 
26: 
27: 
28: 
29: 

15 | P a g e  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
4.4.3 Training phase 

This algorithm is the hybrid of BTA and BDE. Here, BTA is executed first for maxIter2 times, followed 
by BDE. It  means that first, the local  exploitation happens  in each solution individually.  Later, it is 
followed by global exploration and exploitation over the entire population.  

Using the binary vector field information, the parent population undergoes BTA heuristics as 
presented in Algorithm 2 and forms the offspring population. Then, AUC is evaluated by calling the 
thread pool mapper on the offspring population. Thus evolved population after undergoing BTA for 
maxIter2  times  forms  a  population  which  serves  as  the  initial  parent  population  for  BDE.  This 
population  undergoes  the  BDE  heuristics,  viz,  crossover  and  mutation.  After  that,  the  thread  pool 
mapper is called and the AUC of the offspring population is evaluated. Then, the selection operation is 
applied thereby replacing the worse parent solutions with their better offspring solutions. This serves 
as the parent population for the next iteration. 

After this, the whole process of BTA and BDE in tandem is repeated until maxIter1 iterations 

are completed. 

4.4.4 Test phase 

Then the test phase begins where the population obtained at the end of the training phase, is evaluated 
on the test dataset. Here also the AUC is evaluated by using a thread pool mapper only. Later, gives the 
so evolved population is nothing but the set of the required feature subsets with their corresponding test 
AUC. 

4.5 Classification Algorithm 

Logistic Regression (LR) is employed as the classifier for the proposed FSS. LR is chosen because it is 
fast to train and is nonparametric. It does not make any assumptions about the errors or variables, takes 
less time to converge, and has no hyperparameters to fine-tune.  

4.6 Fitness Function 

Area under the ROC Curve (AUC) is the fitness function for our proposed wrappers. It is proven to be 
robust measure than accuracy for unbalanced datasets. It is defined as the average of specificity and the 
sensitivity. The cut-off for the classification of probability in binary classes is taken as 0.5. Accordingly, 

𝐴𝑈𝐶  =  

(cid:3020)(cid:3032)(cid:3041)(cid:3046)(cid:3036)(cid:3047)(cid:3036)(cid:3049)(cid:3036)(cid:3047)(cid:3052)(cid:2878)(cid:3020)(cid:3043)(cid:3032)(cid:3030)(cid:3036)(cid:3033)(cid:3036)(cid:3030)(cid:3036)(cid:3047)  
(cid:2870)

  (7) 

where, sensitivity is the ratio of the positive samples that are correctly predicted to be positive to all the 
predicted positive samples. This is also called True Positive Rate (TPR). 

𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦  =  

(cid:3021)(cid:3017)
(cid:3021)(cid:3017)(cid:2878)(cid:3007)(cid:3015)

  (8) 

where, TP is true positive and FN is false negative and specificity is the ratio of the negative samples 
that are correctly predicted to be negative to all the predicted negative samples. This is also called 
True Negative Rate (TNR). 

𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦  =  

(cid:3021)(cid:3015)
(cid:3021)(cid:3015)(cid:2878)(cid:3007)(cid:3017)

  (9) 

16 | P a g e  

 
 
 
 
 
 
 
 
 
 
 
     
 
where, TN is true negative and FP is false positive. These are obtained from the confusion matrix. 

5 Dataset Description and Experimental setup 

The meta-information of the benchmark datasets is presented in Table 3. All other datasets except for 
the Microsoft Malware dataset, contain categorical features. Thus, categorical features are handled by 
using  one-hot  encoding  mechanism.  All  the  datasets  pertain  to  binary  classification  problems.  The 
Microsoft Malware dataset is accessed from the Kaggle repository [74], IEEE malware dataset from 
IEEE  data  port  [75],  whereas  OVM_Omentum  and  uterus  are  genomic  datasets  from  open  source 
OpenML datasets [77], and the Epsilon dataset from LIBSVM binary dataset repository [76]. 

All  the  experiments  are  conducted  in  a  Spark-HDFS  cluster  with  Spark  version  2.4.4  and 
Hadoop version 2.7, having one master node and four worker nodes with 32 GB RAM with Intel i5 8th 
generation.  

Table 3: Description of the benchmark datasets 

Name of the Dataset 

# Objects 

# Features 

# Classes 

Epsilon 
Microsoft Malware 
IEEE Malware  
OVM_Omentum 
OVM_Uterus 

5,00,000 
32,59,724 
15,00,000 
1584 
1584 

2000 
76 
1000 
10,935 
10,935 

2 
2 
2 
2 
2 

Size of the 
Dataset 
10.8 GB 
1.8 GB 
3.2 GB 
108.3 MB 
108.3 MB 

6 Results & Discussions 

All the datasets are divided into training and test sets in the ratio 80%:20%. Stratified random sampling 
is performed to maintain the similar proportion of the classes in the training and test datasets. It is well-
known that the performance of the EC techniques is susceptible to change in hyperparameters. Hence, 
after rigorous fine-tuning with several combinations, the hyperparameters are frozen and listed in Table 
4. For each algorithm and for datasets, the population size is fixed at 10, and the maximum number of 
generations is taken as 20. In the case of PB-DE, the DE is executed for 20 generations. However, in 
the case of the two hybrids (PB-DETA and PB-TADE) DE and TA are individually executed for 10 
generations each, thereby making it 20 generations in all. All the experiments are repeated for 20 runs 
to nullify the impact of the random seed, which is customary for all evolutionary computing techniques. 
The top solutions that achieved the highest AUC in each run are considered to report the average highest 
AUC and the corresponding average cardinality over 20 runs (see Table 5). 

Table 4: Hyperparameters for all the approaches 

Dataset 

Epsilon 
Microsoft Malware 
IEEE Malware 
OVM_Omentum 
OVM_Uterus 

PB-DE 
MF 
0.8 
0.8 
0.8 
0.75 
0.85 

CR 
0.8 
0.9 
0.9 
0.9 
0.9 

17 | P a g e  

PB-TADE 

PB-DETA 
MF 
0.8 
0.8 
0.8 
0.75 
0.85 

CR  MF 
0.8 
0.8 
0.8 
0.9 
0.8 
0.9 
0.75 
0.9 
0.85 
0.9 

CR 
0.8 
0.9 
0.9 
0.9 
0.9 

 
 
 
 
 
 
 
 
 
 
6.1 Three-way comparative analysis 

The 3-way comparison is conducted on the performance of the three models to establish the importance 
of proposed hybrid approaches over the PB-DE.  

The results in Table 5 show that PB-TADE can achieve the best AUC because PB-DE got stuck 
in the local minima. Both PB-DETA and PB-TADE avoided this as they have employed with TA either 
before or after DE. The advantage of finding the local search exploitation helps not to get entrapped in 
the local minima but also find the better maxima. The feature subsets selected by PB-DE achieved less 
accuracy when compared to both PB-DETA and PB-TADE. Also, the average cardinality obtained by 
PB-DE is relatively high compared to that of both the PB-DETA and PB-TADE. Both these cases are 
not ideal in obtaining an optimal solution. Invocation of TA is also necessary while designing the hybrid 
model. Hence, we worked on both the possibilities as part of the ablation study. We designed parallel 
BTA,  too,  independent  of  the  DE.  If  BTA  alone  is  employed  for  FSS,  then  it  consumed  enormous 
computational time with inferior results. Hence, the comparative study excludes BTA. 

In the ablation experiment involving PB-TADE and PB-DETA, the former achieved little lesser 
average cardinality of the feature subsets than the latter. In addition to that, the mean AUC is a little 
less  but  quite  comparable  in  the  case  of  IEEE  Malware  and  Microsoft  Malware.    The  former 
outperformed the latter because (i) BTA is essentially very good at local search by virtue of it being a 
point-based algorithm and a deterministic variant of simulated annealing (ii) even though we proposed 
a population-based  BTA,  in  these  hybrids, the  hallmark of  population-based  evolutionary algorithm 
namely passing on the knowledge learned by the individual solutions to one another from generation to 
generation is conspicuously missing by design. Therefore, they are at the most population size number 
of BTA instances running independently. (iii) Thus, in PB-TADE, after BTA does the exploitation well, 
the baton is passed on to the BDE, which is demonstrably superior in both exploration and exploitation. 
This cycle continues in every iteration. (iv) However, in PB-DETA, the BDE does the job of exploration 
and exploitation well before the BTA is invoked, which only minimizes the fitness values obtained by 
DE. Further, we should note that both BTA and BDE are run for 10 iterations each, which means that 
they are run with relaxed convergence criteria without adversely impacting the fitness value or the AUC. 
This is a significant departure from the traditional implementations of both BDE and BTA for solving 
combinatorial optimization problems, where they are typically run for many iterations for convergence.  
This strategy is designed to reduce the computational time, primarily because we deal with big data sets 
in a distributed manner in this paper.   

Table 5: Average Cardinality and mean AUC obtained 

Dataset 

PB-DE 

PB-DETA 

PB-TADE 

Average 
Cardinality 
617.3 
Epsilon 
Microsoft Malware  29.6 
IEEE Malware 
OVA_Omentum 
OVA_Uterus 

643.45 
47.28 
37.3 

Mean 
AUC 
0.7932 
0.6872 
0.7929 
0.8607 
0.8607 

Avg. 
Cardinality 
486.1 
21.7 
477.9 
35.54 
28.60 

Mean AUC  Avg. 

Mean AUC 

0.8029 
0.7002 
0.8035 
0.8722 
0.8712 

Cardinality 
457.7 
18.60 
463.9 
26.15 
27.12 

0.8098 
0.7054 
0.8109 
0.8817 
0.8802 

Further, no feature subset selection work is reported in analyzing Microsoft Malware and IEEE 
Malware  datasets  to  the  best  of  our  knowledge.  In  the  Epsilon  dataset,  Peralta  et  al.  [78]  designed 
MapReduce for evolutionary feature  selection. They used CHC as the evolutionary strategy, logistic 
regression as the classifier and achieved a 0.6985 AUC with 721 features, whereas PB-TADE obtained 
an  average  AUC  of  0.8098  with  457.7  average  number  of  features.  Moreover,  Pes  [79]  conducted 

18 | P a g e  

 
 
 
feature  selection  by  using  Symmetric  Uncertainty  (SU),  while  AUC  the  scores  are  computed  using 
Random Forest (RF). They reported an AUC of 0.695 and 0.6 in the OVA_Uterus and OVA_Omentum 
datasets, respectively. However, they did not report the optimal number of features that obtained these 
scores.  But,  PB-TADE  achieved  an  average  AUC  of  0.8802  and  0.8817.  In  comparison,  the 
OVA_Uterus  and  OVA_Omentum  datasets  have  an  average  number  of  features,  27.12  and  26.15, 
respectively. Thus, our proposed methods outperformed the state-of-the-art results in these datasets. 

6.2 Repeatability 

Repeatability is one of the critical criteria for how robust and stable the designed wrapper method is. 
The  more  an  optimal  feature  or  feature  subset  repeats  itself,  the  more  powerful  the  underlying 
evolutionary algorithm is said to be. In this subsection, repeatability analysis is conducted in two ways. 
Firstly, concerning the features repeated individually among the often-repeated feature subsets with the 
highest AUC and secondly, the repetition of a feature subset as a whole corresponding to the highest 
AUC. 

6.2.1 Repeatability of the Individual Features 

All the most repeated features part of an optimal feature subset with the highest AUC are identified and 
presented in Table 6. The features repeated for more than 50% of the total individuals obtained by 20 
runs are considered and presented. Results accommodate the most repeated top five features selected 
by each approach. It turns out that the repeated features selected by PB-DETA and PB-TADE are almost 
identical whereas, the features chosen by the PB-DE are slightly different. 

Table 6: Most repeated features selected by each approach 

Dataset 
Epsilon 

Microsoft 
Malware 

IEEE Malware 

OVA_Omentum 

OVA_Uterus 

Approach 
PB-DE 
PB-DETA 
PB-TADE 
PB-DE 

PB-DETA 
PB-TADE 
PB-DE 
PB-DETA 
PB-TADE 
PB-DE 
PB-DETA 
PB-TADE 
PB-DE 
PB-DETA 
PB-TADE 

Most repeated features 
1,3,5,7,9 
1,3,6,12,19 
1,3,6,12,19 
AVProductsInstalled,HasTpm,Isprotected,Census_OEMN_Name 
Identifier,SmartScreen 
AVProductsInstalled,HasTpm,IsPassiveMode, OsSuite,SmartScreen 
AVProductsInstalled,HasTpm,OsSuite, RipStateBuild,SmartScreen 
GetProcAddress,GetThreadId,Sleep,FindClose, RaiseException 
GetProcAddress,GetLastError,Sleep,ReadFile, RaiseException 
GetProcAddress,GetLastError,Sleep,ReadFile, RaiseException 
158765_at,201608_s_at, 206442_at,207096_s_at,210002_s_at 
1554436_s_at, 201669_s_at, 20644_s_at, 207442_s_at, , 208970_s_at 
1554436_s_at, 201669_s_at, 20644_s_at, 207442_s_at, , 208970_s_at 
205866_s_at,209682_s_at,217294_s_at,  222421_s_at,220148_s_at, 
202125_s_at,205866_s_at,218132_s_at, 222421_s_at,222784_s_at, 
202125_s_at,205866_s_at,218132_s_at, 222421_s_at,222784_s_at, 

6.2.3 Repeatability of the Feature Subsets 

19 | P a g e  

 
 
 
 
 
 
 
 
 
 
 
All the feature subsets that yielded the highest AUC and repeated often are reported in Table 7. The #s1 
represents  the  cardinality  of  the  most-repeated  feature  subset,  and  the  corresponding  AUC.  The 
cardinalities of the most repeated top most feature subset is presented in Table 7. In the case of the 
Epsilon dataset, PB-DE has selected 639 features resulting in 79.67%. PB-TADE outperformed the PB-
DE in terms of AUC by selecting a less cardinal feature subset. In the Microsoft Malware dataset, PB-
DE achieved a 69.74% AUC with 31 features, while both PB-DETA and PB-TADE can achieve a better 
AUC than PB-DE with a less cardinal feature subset. Even though in the case of the IEEE Malware 
dataset, the cardinality of the selected feature subsets are the same for the PB-DE, PB-DETA, and PB-
TADE, the AUC are different because the selected features are not identical. The same is the case in 
OVA_Omentum  and  OVA_Uterus  datasets.  Hence,  the  results  indicate  that  PB-TADE  can  achieve 
lower  cardinality  with  better  AUC  than  PB-DETA  and  PB-DE  in  all  the  datasets  in  terms  of 
repeatability. Similarly, PB-DETA outperformed PB-DE. 

Table 7:  Cardinalities and the corresponding AUC of the Top-most repeated feature subsets 

Dataset 

PB-DE 

PB-DETA 

PB-TADE 

Epsilon 
Microsoft Malware 
IEEE Malware 
OVA_Omentum 
OVA_Uterus 

#s1 
639 
31 
550 
55 
39 

AUC 
0.7967 
0.6974 
0.7956 
0.8701 
0.8598 

#s1 
564 
24 
486 
37 
42 

* where #s1 is the cardinality of the top-most repeated feature subset 

AUC 
0.8014 
0.7001 
0.8057 
0.8723 
0.8723 

#s1 
488 
17 
487 
31 
32 

AUC 
0.8097 
0.7061 
0.8058 
0.8779 
0.8850 

6.3 Least Cardinal Feature Subset with highest AUC 

In  this subsection, the  least  cardinal feature subset among the most repeated  feature subset with the 
highest  AUC  is  discussed.  The  results  are  presented  in  Table 8.  It  turns  out  that  the  PB-TADE 
outperformed PB-DETA and PB-DE in detecting the most repeatable least cardinal feature subset, while 
PB-DETA  stands  second  in  detecting  the  most  repeatable  least  cardinal  feature  subset.  Except  for 
OVA_Omentum dataset, PB-TADE achieved the better AUC with lesser number of features than the 
PB-DETA and PB-DE. In the case of OVA_Uterus, PB-DETA achieved better AUC with lesser number 
of features than the PB-DE. In all the cases, PB-DE is outperformed by the proposed hybrid models. 
This fact further reinforces the role played by the BTA in this hybridization.  

Table 8: Least Cardinal Feature subset selected by each approach with the most repetitions 

Dataset 

PB-DE 

PB-DETA 

AUC  
0.7967 
Epsilon 
0.6915 
Microsoft Malware 
0.7956 
IEEE Malware 
0.8504 
OVA_Omentum 
OVA_Uterus 
0.8504 
* where #s1 is the cardinality of the feature subset having least cardinal subset with highest AUC  

AUC  
0.8068 
0.7007 
0.8057 
0.8723 
0.8723 

#s1 
487 
17 
487 
33 
32 

#s1 
588 
27 
550 
41 
37 

#s1 
471 
22 
484 
29 
37 

PB-TADE 
AUC  
0.8097 
0.7061 
0.8197 
0.8699 
0.8750 

20 | P a g e  

 
 
 
 
 
6.4 Speedup 

Speedup is defined  as the gain  obtained by the parallel version of  the algorithm  with respect to the 
sequential algorithm executed on a single processor as follows in Eq.(11) 

Speed Up (S. U) =  

(cid:2904)(cid:2919)(cid:2923)(cid:2915) (cid:2930)(cid:2911)(cid:2921)(cid:2915)(cid:2924) (cid:2912)(cid:2935) (cid:2903)(cid:2915)(cid:2927)(cid:2931)(cid:2915)(cid:2924)(cid:2930)(cid:2919)(cid:2911)(cid:2922) (cid:2906)(cid:2915)(cid:2928)(cid:2929)(cid:2919)(cid:2925)(cid:2924)
(cid:2904)(cid:2919)(cid:2923)(cid:2915) (cid:2930)(cid:2911)(cid:2921)(cid:2915)(cid:2924) (cid:2912)(cid:2935) (cid:2900)(cid:2911)(cid:2928)(cid:2911)(cid:2922)(cid:2922)(cid:2915)(cid:2922) (cid:2906)(cid:2915)(cid:2928)(cid:2929)(cid:2919)(cid:2925)(cid:2924)

   (11) 

Table 9: Speedup Analysis of parallel versions over the sequential ones 

Dataset 

Epsilon 
Microsoft 
Malware 
IEEE Malware 
OVA_Omentum 
OVA_Uterus 

BDE 
PB-DE 
E.T 
E.T 
(sec) 
(sec) 
12780  4485 
16412  6741 

S.U  DETA 

E.T 
(sec) 

PB-DETA 
E.T (sec) 

S.U 

2.84  12680 
2.43  15781 

4361 
6447 

20412  8151 
14892  5428 
14108  5378 

2.50  19793 
2.74  14651 
2.62  13979 

7936 
5231 
5207 

TADE 
E.T 
(sec) 
12688 
15779 

19801 
14689 
13968 

PB-TADE 
E.T (sec) 

S.U 

4369 
6432 

7938 
5226 
5222 

2.90 
2.45 

2.49 
2.81 
2.67 

2.90 
2.44 

2.49 
2.80 
2.68 

* where E.T is the execution time given in seconds 

This stands as one of the essential characteristics in evaluating the performance of the parallel version 
of the algorithm. The results are presented in Table 9. It is to be noted that speedup results are rounded 
off to two decimals. We observed that the proposed parallel algorithms achieved significant speedup. 
Speedup  achieved  ranges  from  2.43  –  2.90  times  by  all  proposed  algorithms  over  their  sequential 
counterparts in all datasets. As the number of nodes in the cluster are 4; the maximum possible speedup 
could be achieved is 4. The linear speedup is not achieved because of the synchronization junctions in 
the parallel model. 

6.5 Statistical testing of the results 

The  two-tailed  t-test  at  5%  level  of  significance  at  38  (20+20-2)  degrees  of  freedom  is  conducted 
pairwise  on  the  three  proposed  algorithms  to  make  statistically  valid  statements  about  their 
performance. The null hypothesis is H0: both the algorithms are statistically equal, while the alternate 
hypothesis is, H1: both the algorithms are statistically not equal.  

Table 10: Pairwise Paired T-test analysis: A three way comparison 

Dataset 

Epsilon 
Microsoft Malware 
IEEE Malware 
OVA_Omentum 
OVA_Uterus 

PB-DE vs. PB-DETA 
t-statistic 
4.0930 

PB-DE vs. PB-TADE 
t-statistic 
7.72 

p-value 
0.00021 
0.002 
8.39x10-05 
2.36 x10-05 
0.00099 

4.62 

8.045 

4.168 

3.69 

3.318 

4.403 

4.81 

3.56 

p-value 
2.66 x10-09 
4.25 x10-05 
9.91 x10-10 
0.00017 

0.00069 

PB-DETA vs. PB-TADE 
t-statistic 
3.56 

p-value 
0.0010 

3.10 

3.63 

2.06 

1.74 

0.0035 

0.0008 

0.0453 

0.0891 

The top accuracy scores achieved by each approach over the 20 runs are considered for the t-
test evaluation. As the p-values for all datasets are less than 0.05, the null hypothesis is rejected, and 
alternate hypothesis is accepted. The t-statistic and p-value are reported in Table 10. We infer that PB-

21 | P a g e  

 
 
 
 
 
 
 
 
 
 
DETA and PB-TADE are significantly different from PB-DE, as the p-values are significantly small. 
Further,  in  all  datasets  except  for  the  OVA_Uterus,  the  PB-TADE  turned  out  to  be  statistically 
significantly better than PB-DETA.  

7. Conclusion 

This paper develops the parallel versions of the BDE, BDETA, and BTADE and employs them for the 
wrapper  based  feature  subset  selection,  where  logistic  regression  is  chosen  as  the  classifier.  We 
demonstrated their effectiveness on five high-dimensional datasets taken from literature. The results 
indicate  that  the  PB-TADE  is  statistically  significant  compared  to  PB-DETA  and  PB-DE  at  both 
exploration  and  exploration.  All  the  proposed  parallel  approaches  achieved  significant  speedup 
compared to their sequential counterparts. Although PB-TADE and PB-DETA consumed more time 
than the PB-DE, it is mainly due to more function evaluations. It is noteworthy that the resulting optimal 
feature subsets are much better than those of the PB-DE in terms of higher AUC and less cardinality. 
Further,  our  proposed  methods  outperformed  the  state-of-the-art  results,  wherever  the  results  were 
reported. The above analysis is conducted in a single-objective function environment. In the future, the 
investigation  will  be  carried  out  on  the  same  problem  but  in  bi-objective  and  multi-objective 
environments. 

22 | P a g e  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Appendix 

Start 

Initialization of the Population; i=0, maxItr, k 

LR-1 

LR-2 

... 

LR-k 

Thread pool mapper 

Evaluate Train AUC  

No 

i<maxItr1 

Yes 

Invoke BDE Algorithm Mapper 

LR-1 

LR-2 

... 

LR-k 

Thread pool mapper 

Evaluate Train AUC 

Update Population RDD; i=i+1 

Train Data 

Dataset 

Test Data 

Evaluate the Test AUC  

Return population, & corresponding Test AUC 

Stop 

Fig.1: Flowchart of PB-DE based wrapper 

23 | P a g e  

 
 
 
 
 
V 

Start 

Initialization of the Population; 
i=0,j=0,maxItr1,maxItr2,k 

Thread pool mapper 

LR-1 

LR-2 

LR-k 

Evaluate Train AUC  

i<maxItr

No 

Yes 
Invoke BDE Algorithm Mapper 

LR-1 

LR-2 

... 

LR-k 

Thread pool mapper 

Evaluate Train AUC  

Update Population RDD; i=i+1 

No 

j=0 

j<maxItr

Yes 

Invoke TA Algorithm Mapper 

LR-1 

LR-2 

…

LR-k 

Thread pool mapper 

Evaluate Train AUC  

Update Population RDD; j=j+1 

Train Data 

Dataset 

Test Data 

Evaluate Test AUC  

Return population, & 
corresponding Test AUC 

Stop 

Fig.2: Flow chart of the PB-DETA based wrapper 

24 | P a g e  

 
 
 
 
 
Start 

Initialization of the Population; i=0,j=0 

Thread pool mapper 

LR-1 

LR-2 

LR-k 

Evaluate AUC  

i<maxItr

Yes 

j<maxIt

No 

Yes 

No 

Invoke TA Algorithm Mapper 

Train Data 

LR-1 

LR-2 

... 

LR-k 

Thread pool mapper 

Evaluate Train AUC  

Dataset 

Test Data 

Update Population RDD; j=j+1 

Evaluate Test AUC  

j=0 

Invoke DE Mapper 

Thread pool mapper 

LR-1 

LR-2 

... 

LR-k 

Evaluate Train AUC  

Update Population RDD; j=j+1 

Return population, & 
corresponding Test AUC  

Stop 

Fig. 3: Flowchart of the parallel PB-DETA based wrapper  

25 | P a g e  

 
 
 
 
 
 
 
 
References 
[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

Chandrashekar, G., Sahin, F.: A survey on feature selection methods, Comput. Electr. Eng. 40, 16–28 (2014). 
https://doi.org/10.1016/j.compeleceng.2013.11.024. 
Xue,B., Zhang, M., Browne, W.N., Yao, X.: A Survey on Evolutionary Computation Approaches to Feature 
Selection, IEEE Trans. Evol. Comput. 20, 606–626. (2016). https://doi.org/10.1109/TEVC.2015.2504420. 
Price, K., Storn, R.: Differential Evolution – A Simple and Efficient Heuristic for global Optimization over 
Continuous Spaces, J. Glob. Optim. 11 , 341–359, (1997). https://doi.org/10.1023/A:1008202821328. 
Zhang, Y., Gong, D.W., Gao, X.Z., Tian, T., Sun, X.Y.: Binary differential evolution with self-learning for multi-
objective feature selection, Inf. Sci. (Ny). 507, 67–85 (2020). https://doi.org/10.1016/j.ins.2019.08.040. 
Vivekanandan, T., Iyengar, N.C.S.N.: Optimal feature selection using a modified differential evolution algorithm 
and its effectiveness for prediction of heart disease, Comput. Biol. Med. 90, 125–136, (2017). 
https://doi.org/10.1016/j.compbiomed.2017.09.011. 
Samuel, O.W., Asogbon, G.M., Sangaiah, A.K., Fang, P., Li, G.: An integrated decision support system based on 
ANN and Fuzzy_AHP for heart failure risk prediction, Expert Syst. Appl. 68, 163–172, (2017). 
https://doi.org/10.1016/j.eswa.2016.10.020. 
Nayak, S.K., Rout, P.K.,  Jagadev, A.K., Swarnkar, T.: Elitism based Multi-Objective Differential Evolution for 
feature selection: A filter approach with an efficient redundancy measure, J. King Saud Univ. - Comput. Inf. Sci. 
32, 174–187, (2020). https://doi.org/10.1016/j.jksuci.2017.08.001. 

[8]          Mlakar, U., Fister, I., Brest, J., Potočnik, B.: Multi-Objective Differential Evolution for feature selection in Facial 
               Expression Recognition systems, Expert Syst. Appl. 89, 129–137, (2017). 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

[19] 

[20] 

[21] 

https://doi.org/10.1016/j.eswa.2017.07.037. 
Khushaba, R.N., Al-Ani, A., Al-Jumaily, A.: Feature subset selection using differential evolution and a statistical 
repair mechanism, Expert Syst. Appl. 38, 11515–11526, (2011). https://doi.org/10.1016/j.eswa.2011.03.028. 
Hancer, E., Xue, B., Zhang, M.: Differential evolution for filter feature selection based on information theory and 
feature ranking, Knowledge-Based Syst. 140, 103–119, (2018). https://doi.org/10.1016/j.knosys.2017.10.028. 
Hancer, E.: A new multi-objective differential evolution approach for simultaneous clustering and feature selection, 
Eng. Appl. Artif. Intell. 87, 103307, (2020). https://doi.org/10.1016/j.engappai.2019.103307. 
Ghosh, A., Datta, A., Ghosh, S.: Self-adaptive differential evolution for feature selection in hyperspectral image 
data, Appl. Soft Comput. J. 13, 1969–1977, (2013). https://doi.org/10.1016/j.asoc.2012.11.042. 
Bhadra, T., Bandyopadhyay, S.: Unsupervised feature selection using an improved version of Differential 
Evolution, Expert Syst. Appl. 42, 4042–4053, (2015). https://doi.org/10.1016/j.eswa.2014.12.010. 
Baig, M.Z., Aslam, N., Shum, H.P.H., Zhang, L.: Differential evolution algorithm as a tool for optimal feature 
subset selection in motor imagery EEG, Expert Syst. Appl. 90, 184–195, (2017). 
https://doi.org/10.1016/j.eswa.2017.07.033. 
Almasoudy, F.H., Al-Yaseen, W.L., Idrees, A.K.: Differential Evolution Wrapper Feature Selection for Intrusion 
Detection System, Procedia Comput. Sci. 167, 1230–1239, (2020). https://doi.org/10.1016/j.procs.2020.03.438. 
ZorarpacI, E., Ozel, S.A.: A hybrid approach of differential evolution and artificial bee colony for feature selection, 
Expert Syst. Appl. 62, 91–103, (2016). https://doi.org/10.1016/j.eswa.2016.06.004. 
Srikrishna, V., Ghosh, R., Ravi, V., Deb, K.: Elitist quantum-inspired differential evolution based wrapper for 
feature subset selection, Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes 
Bioinformatics). 9426, 113–124, (2015). https://doi.org/10.1007/978-3-319-26181-2_11. 
Rivera-Lopez, R., Mezura-Montes, E., Canul-Reich, J., Cruz-Chávez, M.A.: A permutational-based Differential 
Evolution algorithm for feature subset selection, Pattern Recognit. Lett. 133, 86–93, (2020). 
https://doi.org/10.1016/j.patrec.2020.02.021. 
Zhao, X.S., Bao, L.L., Ning, Q., Ji, J.C., Zhao, X.W.: An Improved Binary Differential Evolution Algorithm for 
Feature Selection in Molecular Signatures, Mol. Inform. 37, 1–15, (2018). https://doi.org/10.1002/minf.201700081. 
Hancer, E.: Fuzzy kernel feature selection with multi-objective differential evolution algorithm, Conn. Sci. 3, 323–
341, (2019). https://doi.org/10.1080/09540091.2019.1639624. 
Li, J., Ding, L., Li, B.: Differential evolution-based parameters optimisation and feature selection for support vector 
machine, Int. J. Comput. Sci. Eng. 13, 355–363, (2016). https://doi.org/10.1504/IJCSE.2016.080212. 

[22]  Wang, J., Xue, B., Gao, X., Zhang, M: A differential evolution approach to feature selection and instance selection. 

In Proceedings of the 14th Pacific Rim International Conference on Trends in Artificial Intelligence (PRICAI'16). 
Springer, Gewerbestrassse 11 CH-6330, Cham (ZG), CHE, 588-602, (2016), https://doi.org/10.1007/978-3-319-
42911-3_49 
Carrasco, J.,  García, S., Rueda, M.M., Das, S., Herrera, F.: Recent trends in the use of statistical tests for 
comparing swarm and evolutionary computing algorithms: Practical guidelines and a critical review, ArXiv. 54 
(2020). 
Cao, B., Fan, S., Zhao, J., Yang, P., Muhammad, K., Tanveer, M.: Quantum-enhanced multiobjective large-scale 
optimization via parallelism, Swarm Evol. Comput. 57, (2020). https://doi.org/10.1016/j.swevo.2020.100697. 
BenSaid, F, Alimi, A.M, Moanofs: Multi-objective automated negotiation based online feature selection system for 
big data classification, ArXiv. 110 (2018). 
Khan, A, Baig, A.R.: Multi-objective feature subset selection using non-dominated sorting genetic algorithm, J. 
Appl. Res. Technol. 13, 145–159, (2015). https://doi.org/10.1016/S1665-6423(15)30013-4. 
Nguyen, B.H., Xue, B., Zhang, M.: A survey on swarm intelligence approaches to feature selection in data mining, 
Swarm Evol. Comput. 54, (2020). https://doi.org/10.1016/j.swevo.2020.100663. 

[23] 

[24] 

[25] 

[26] 

[27] 

26 | P a g e  

 
 
[28] 

[29] 

[30] 

[31] 

[32] 

[33] 

[34] 

[35] 

[36] 

[37] 

[38] 

[39] 

[40] 

[41] 

[42] 

[43] 

[44] 

[45] 

[46] 

Khammassi, C., Krichen, S.: A NSGA2-LR wrapper approach for feature selection in network intrusion detection, 
Comput. Networks. 172, 107183, (2020). https://doi.org/10.1016/j.comnet.2020.107183. 
Chaudhuri, A., Sahu, T.P.: Feature selection using Binary Crow Search Algorithm with time varying flight length, 
Expert Syst. Appl. 168, 114288, (2021). https://doi.org/10.1016/j.eswa.2020.114288. 
Too, J., Mirjalili, S.: A Hyper Learning Binary Dragonfly Algorithm for Feature Selection: A COVID-19 Case 
Study, Knowledge-Based Syst. 212, (2021). https://doi.org/10.1016/j.knosys.2020.106553. 
Yang, W.A., Zhou, Q., Tsui, K.L.: Differential evolution-based feature selection and parameter optimisation for 
extreme learning machine in tool wear estimation, Int. J. Prod. Res. 54, 4703–4721, (2016). 
Xie, X., Xu, K., Wang, X.: Cloud computing resource scheduling based on improved differential evolution ant 
colony algorithm, ACM Int. Conf. Proceeding Ser.,171–177, (2019). https://doi.org/10.1145/3335656.3335706. 
Silva-Filho, A.G., Nunes, L.J.C., Lacerda, H.F.: Differential evolution to reduce energy consumption in three-level 
memory hierarchy, Proceedings, SBCCI 2015 - 28th Symp. Integr. Circuits Syst. Des. Chip Bahia. (2015). 
https://doi.org/10.1145/2800986.2801005.. 
Krishna, G.J., Ravi, V.: Anomaly Detection Using Modified Differential Evolution: An Application to Banking and 
Insurance. Proceedings of the 11th International Conference on Soft Computing and Pattern Recognition, SoCPaR 
2019. Advances in Intelligent Systems and Computing,. Springer, Cham. 1182, (2019) https://doi.org/10.1007/978-
3-030-49345-5_11. 
Hammami, M., Bechikh, S., Hung, C.C., Ben Said, L., A Multi-objective hybrid filter-wrapper evolutionary 
approach for feature selection, Memetic Comput. 11 (2019) 193–208. https://doi.org/10.1007/s12293-018-0269-2. 
Harada, T., Kaidan, M., Thawonmas, R.: Comparison of synchronous and asynchronous parallelization of extreme 
surrogate-assisted multi-objective evolutionary algorithm, (2020). https://doi.org/10.1007/s11047-020-09806-2. 
Peralta, D., Del Río, S., Ramírez-Gallego, S., Triguero, I., Benitez, J.M., Herrera, F.: Evolutionary Feature 
Selection for Big Data Classification: A MapReduce Approach, Math. Probl. Eng. 2015 (2015). 
https://doi.org/10.1155/2015/246139. 
Rong, M.,  Gong, D.,  Gao, X.: Feature Selection and Its Use in Big Data: Challenges, Methods, and Trends, IEEE 
Access. 7, 19709–19725, (2019). https://doi.org/10.1109/ACCESS.2019.2894366. 
Zhou, C.: Fast parallelization of differential evolution algorithm Using MapReduce, Proc. 12th Annu. Genet. Evol. 
Comput. Conf. GECCO ’10, 1113–1114, (2010). https://doi.org/10.1145/1830483.1830689. 
Teijeiro, D.,  Pardo, X.C., González, P., Banga, J.R., Doallo, R.: Implementing Parallel Differential Evolution 
on Spark. In: Squillero G., Burelli P. (eds) Applications of Evolutionary Computation. EvoApplications 2016. 
Lecture Notes in Computer Science, Springer, Cham 9598,(2016) doi.org/10.1007/978-3-319-31153-1_6. 
Cho, P.P.W., Nyunt, T.T.S., Aung, T.T.: Differential evolution for large-scale clustering, Proc. 2019 9th Int. Work. 
Comput. Sci. Eng. WCSE 2019 SPRING. 58–62, (2019). https://doi.org/10.18178/wcse.2019.03.010. 
Al-Sawwa, J., Ludwig, S.A.: Performance evaluation of a cost-sensitive differential evolution classifier using spark 
– Imbalanced binary classification, J. Comput. Sci. 40, 101065, (2020). https://doi.org/10.1016/j.jocs.2019.101065. 
Chen, Z., Jiang, X., Li, J., Li, S., Wang, L.: PDECO: Parallel differential evolution for clusters optimization, J. 
Comput. Chem. 34, 1046–1059, (2013). https://doi.org/10.1002/jcc.23235. 
Adhianto, L, Banerjee, S., Fagan, M., Krentel, M., Marin, G., Mellor-Crummey, J., Tallent, N.R.: HPCTOOLKIT: 
Tools for performance analysis of optimized parallel programs, Concurr. Comput. Pract. Exp. 22, 685–701, (2010). 
https://doi.org/10.1002/cpe. 
Deng, C., Tan, X., Dong, X., Tan, Y.: A parallel version of differential evolution based on resilient distributed 
datasets model, Commun. Comput. Inf. Sci. 562, 84–93, (2015). https://doi.org/10.1007/978-3-662-49014-3_8. 
He, Z., Peng, H., Chen, J., Deng, C., Wu, Z.: A Spark-based differential evolution with grouping topology model 
for large-scale global optimization, Cluster Comput. 24, 515–535, (2021). https://doi.org/10.1007/s10586-020-
03124-z. 

[47]  Wong, T.H., Qin, A.K., Wang, S., Shi, Y.: cuSaDE: A CUDA-Based Parallel Self-adaptive Differential Evolution 

[48] 

[49] 

[50] 

[51] 

[52] 

[53] 

[54] 

Algorithm, 2, 375–388, (2015). https://doi.org/10.1007/978-3-319-13356-0_30. 
Cao, B, Zhao, J., Lv Z., and Liu, X.: A Distributed Parallel Cooperative Coevolutionary Multiobjective 
Evolutionary Algorithm for Large-Scale Optimization, IEEE Transactions on Industrial Informatics, 13, 2030-2038, 
(2017). https://doi.org/10.1109/TII.2017.2676000. 
Ge, Y., Yu, W., Lin, Y., Gong, Y., Zhan, Z., Chen, W., Zhang, J.: Distributed Differential Evolution Based on 
Adaptive Mergence and Split for Large-Scale Optimization, IEEE Transactions on Cybernetics, 48, 2166-2180, 
(2018). 10.1109/TCYB.2017.2728725. 
De Falco, I., Scafuri, U., Tarantino, E., Della Cioppa, A.: A Distributed Differential Evolution Approach for 
Mapping in a Grid Environment, 15th EUROMICRO International Conference on Parallel, Distributed and 
Network-Based Processing (PDP'07), 442-449, (2007). 10.1109/PDP.2007.6. 
Veronese, L.deP, Krohling, R. A.: Differential evolution algorithm on the GPU with C-CUDA, IEEE Congress on 
Evolutionary Computation, 1-7, (2010). 10.1109/CEC.2010.5586219. 
Glotic, A., Glotic, A., Kitak, P., Pihler, J., Ticar, I.: Parallel Self-Adaptive Differential Evolution Algorithm for 
Solving Short-Term Hydro Scheduling Problem, in IEEE Transactions on Power Systems,  29, 2347-2358, (2014). 
https://doi.org/10.1109/TPWRS.2014.2302033. 
Daoudi, M., Hamena, S., Benmounah, Z., Batouche, M., "Parallel diffrential evolution clustering algorithm based 
on MapReduce," 2014 6th International Conference of Soft Computing and Pattern Recognition (SoCPaR), 337-
341, (2014). https://doi.org/10.1109/SOCPAR.2014.7008029. 
Krömer, P., Platoš, J., Snášel, V.: "Scalable differential evolution for many-core and clusters in Unified Parallel 
C," 2013 IEEE International Conference on Cybernetics (CYBCO), 180-185, (2013). 
https://doi.org/10.1109/CYBConf.2013.6617451. 

27 | P a g e  

 
[58] 

[57] 

[56] 

[55] 

Thomert, D.B., Bhattacharya, A. K., Caron, E., Gadireddy, K., Lefevre, L.: "Parallel differential evolution approach 
for cloud workflow placements under simultaneous optimization of multiple objectives," 2016 IEEE Congress on 
Evolutionary Computation (CEC), 822-829, (2016), https://doi.org/10.1109/CEC.2016.7743876. 
Dueck, G, Scheuer, T.: Threshold accepting: A general purpose optimization algorithm appearing superior to 
simulated annealing, J. Comput. Phys. 90, 161–175, (1990). https://doi.org/10.1016/0021-9991(90)90201-B. 
Ravi, V., Zimmermann, H.J.: Fuzzy rule based classification with FeatureSelector and modified threshold 
accepting, Eur. J. Oper. Res. 123, 16–28, (2000). https://doi.org/10.1016/S0377-2217(99)00090-9. 
Ravi, V., Reddy, P.J., Zimmermann, H.J.: Fuzzy rule base generation for classification and its minimization via 
modified threshold accepting, Fuzzy Sets Syst. 120, 271–279, (2001). https://doi.org/10.1016/s0165-
0114(99)00100-1. 
Ravi, V, Zimmermann, H.-J.: A neural network and fuzzy rule base hybrid for pattern classification, Soft Comput. 5  
152–159, (2001). https://doi.org/10.1007/s005000000071. 
Ravi, V., Pramodh, C.: Threshold accepting trained principal component neural network and feature subset 
selection: Application to bankruptcy prediction in banks, Appl. Soft Comput. J. 8, 1539–1548, (2008). 
https://doi.org/10.1016/j.asoc.2007.12.003. 
Chauhan, N., Ravi, V., Differential evolution and threshold accepting hybrid algorithm for unconstrained 
optimization, Int. J. Bio-Inspired Comput. 2, 169–182, (2010). https://doi.org/10.1504/IJBIC.2010.033086. 
[62]  Meenachi, L., Ramakrishnan, S.: Differential evolution and ACO based global optimal feature selection with fuzzy 

[61] 

[60] 

[59] 

[63] 

rough set for cancer data classification, Soft Comput. 24, 18463–18475, (2020). https://doi.org/10.1007/s00500-
020-05070-9. 
Frausto-Solis, J., Hernández-Ramírez, L., Castilla-Valdez, G., González-Barbosa, J.J., Sánchez-Hernández, J.P.: 
Chaotic Multi-Objective Simulated Annealing and Threshold Accepting for Job Shop Scheduling Problem, Math. 
Comput. Appl. 8, 26, (2021). https://doi.org/10.3390/mca26010008. 

[65] 

[67] 

[66] 

[64]  Marimuthu, S., Ponnambalam, S.G., Jawahar, N.: Threshold accepting and Ant-colony optimization algorithms for 
scheduling m-machine flow shops with lot streaming, J. Mater. Process. Technol. 209, 1026–1041, (2009). 
https://doi.org/10.1016/j.jmatprotec.2008.03.013. 
Phu, J, Khuu, S.K., Agar, A., Kalloniatis, M.: Clinical Evaluation of Swedish Interactive Thresholding Algorithm–
Faster Compared With Swedish Interactive Thresholding Algorithm–Standard in Normal Subjects, Glaucoma 
Suspects, and Patients With Glaucoma, Am. J. Ophthalmol. 208, 251–264, (2019). 
https://doi.org/10.1016/j.ajo.2019.08.013. 
Gunel, K., Erdogdu, K., Polat, R., Ozarslan, Y.: An empirical study on evolutionary feature selection in intelligent 
tutors for learning concept detection, Expert Syst. 36, 1–12, (2019). https://doi.org/10.1111/exsy.12278. 
Al-Ani, A., Alsukker, A., Khushaba, R.N., Feature subset selection using differential evolution and a wheel based 
search strategy, Swarm Evol. Comput. 9, 15–26, (2013). https://doi.org/10.1016/j.swevo.2012.09.003. 
Krishna, G.J., Ravi, V.: Feature subset selection using adaptive differential evolution: An application to banking, 
ACM Int. Conf. Proceeding Ser. 157–163, (2019). https://doi.org/10.1145/3297001.3297021. 
Liu, X.F., Zhan, Z.H., Lin, J.H., Zhang, J.: Parallel differential evolution based on distributed cloud computing 
resources for power electronic circuit optimization, GECCO 2016 Companion - Proc. 2016 Genet. Evol. Comput. 
Conf. 117–118, (2016). https://doi.org/10.1145/2908961.2908972. 
Dean, J., Ghemawat, S.: MapReduce: Simplified Data Processing on Large Clusters, OSDI’04 Sixth Symp. Oper. 
Syst. Des. Implement. 137--150, (2004). https://doi.org/10.1190/segam2013-1277.1. 
Kohavi, R., John, G. H.: Wrappers for feature subset selection, Lect. Notes Comput. Sci. (Including Subser. Lect. 
Notes Artif. Intell. Lect. Notes Bioinformatics). 97 (1997) 273–324. https://doi.org/10.5555/1251254. 

[71] 

[68] 

[69] 

[70] 

[72]     CRISP DM : https://www.the-modeling-agency.com/crisp-dm.pdf -- retrieved on April 24, 2021 
[73]     Apache Spark https://spark.apache.org/ -- retrieved on January 26 2021.  
[dataset] [74] Kaggle Open source Datasets https://www.kaggle.com/c/microsoft-malware-prediction/data -- retrieved on 

March 27, 2021. 

[dataset] [75] IEEE Dataport https://ieee-dataport.org/ -- retrieved on March 27, 2021.      
[dataset] [76] LIBSVM repository for the binary class high dimensional datasets 

https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html -- retrieved on March 27, 2021. 
[dataset] [77] OpenML Open Source Datasets https://www.openml.org/home -- retrieved on March 27, 2021 
[78]  

Peralta, D., Río, S.d., Gallego, S.R., Triguero, I., Benitez, J.M., Herrera, F.: "Evolutionary Feature Selection for Big 
Data Classification: A MapReduce Approach", Mathematical Problems in 
Engineering, 11 pages, (2015). https://doi.org/10.1155/2015/246139 
Pes, B: "Learning From High-Dimensional Biomedical Datasets: The Issue of Class Imbalance," in IEEE Access, 8, 
13527-13540,(2020), doi: 10.1109/ACCESS.2020.2966296.   

[79] 

28 | P a g e  

 
 
 
 
