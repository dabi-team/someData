Neural Transformation Learning for Deep Anomaly Detection Beyond Images

Chen Qiu 1 2 Timo Pfrommer 1 Marius Kloft 2 Stephan Mandt 3 Maja Rudolph 1

2
2
0
2

b
e
F
3

]

G
L
.
s
c
[

4
v
0
4
4
6
1
.
3
0
1
2
:
v
i
X
r
a

Abstract

Data transformations (e.g. rotations, reﬂections,
and cropping) play an important role in self-
supervised learning. Typically, images are trans-
formed into different views, and neural networks
trained on tasks involving these views produce
useful feature representations for downstream
tasks, including anomaly detection. However, for
anomaly detection beyond image data, it is of-
ten unclear which transformations to use. Here
we present a simple end-to-end procedure for
anomaly detection with learnable transformations.
The key idea is to embed the transformed data into
a semantic space such that the transformed data
still resemble their untransformed form, while
different transformations are easily distinguish-
able. Extensive experiments on time series show
that our proposed method outperforms existing ap-
proaches in the one-vs.-rest setting and is compet-
itive in the more challenging n-vs.-rest anomaly-
detection task. On medical and cyber-security
tabular data, our method learns domain-speciﬁc
transformations and detects anomalies more accu-
rately than previous work.

1. Introduction

Many recent advances in anomaly detection rely on the
paradigm of data augmentation. In the self-supervised set-
ting, especially for image data, predeﬁned transformations
such as rotations, reﬂections, and cropping are used to gen-
erate varying views of the data. This idea has led to strong
anomaly detectors based on either transformation prediction
(Golan & El-Yaniv, 2018; Wang et al., 2019b; Hendrycks
et al., 2019) or using representations learned using these
views (Chen et al., 2020) for downstream anomaly detection
tasks (Sohn et al., 2021; Tack et al., 2020).

Unfortunately, for data other than images, such as time se-
ries or tabular data, it is much less well known which trans-

1Bosch Center for AI 2TU Kaiserslautern 3UC Irvine. Corre-
spondence to: Maja Rudolph <majarita.rudolph@de.bosch.com>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

formations are useful, and it is hard to design these trans-
formations manually. This paper studies self-supervised
anomaly detection for data types beyond images. We de-
velop neural transformation learning for anomaly detection
(NeuTraL AD): a simple end-to-end procedure for anomaly
detection with learnable transformations. Instead of man-
ually designing data transformations to construct auxiliary
prediction tasks that can be used for anomaly detection, we
derive a single objective function for jointly learning useful
data transformations and anomaly thresholding. As detailed
below, the idea is to learn a variety of transformations such
that the transformed samples share semantic information
with their untransformed form, while different views are
easily distinguishable.

NeuTraL AD has only two components: a ﬁxed set of learn-
able transformations and an encoder model. Both elements
are jointly trained on a noise-free, deterministic contrastive
loss (DCL) designed to learn faithful transformations. Our
DCL is different from other contrastive losses in represen-
tation learning (Gutmann & Hyv¨arinen, 2010, 2012; Mnih
& Kavukcuoglu, 2013; Oord et al., 2018; Bamler & Mandt,
2020; Chen et al., 2020) and image anomaly detection (Tack
et al., 2020; Sohn et al., 2021), all of which use negative
samples from a noise distribution. In contrast, our approach
leads to a non-stochastic objective that neither needs any ad-
ditional regularization or adversarial training (Tamkin et al.,
2021) and can be directly used as the anomaly score.

Our approach leads to a new state of the art in anomaly
detection beyond images. For time series and tabular data,
NeuTraL AD signiﬁcantly improves the anomaly detection
accuracy. For example, in an epilepsy time series dataset, we
raised the state-of-the-art from an AUC of 82.6% to 92.6%
(+10%). On an Arrhythmia tabular dataset, we raised the
F1 score by +2.9 percentage points to an accuracy of 60.3.

Our paper is structured as follows. We ﬁrst discuss related
work (Section 2) and present NeuTraL AD in Section 3. In
Section 3.2, we discuss its advantages for neural transforma-
tion learning in comparison to other self-supervised learning
objectives. Experimental results are presented in Section 4.
Section 5 concludes this paper.

 
 
 
 
 
 
Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

data
(e.g. spectograms)

neural
transformations

transformed
data (views)

encoder

deterministic contrastive
loss/anomaly score

Figure 1. NeuTraL AD is a end-to-end procedure for self-supervised anomaly detection with learnable neural transformations. Each
sample is transformed by a set of learned transformations and then embedded into a semantic space. The transformations and the encoder
are trained jointly on a contrastive objective (Equation (2)), which is also used to score anomalies.

2. Related Work

Recently,

Deep Anomaly Detection.
there has been
a rapidly growing interest in developing deep-learning
approaches for anomaly detection (Ruff et al., 2021).
While deep learning—by removing the burden of manual
feature engineering for complex problems—has brought
about tremendous technological advances, its application
to anomaly detection is rather recent. Related work on
deep anomaly detection includes deep autoencoder vari-
ants (Principi et al., 2017; Zhou & Paffenroth, 2017; Chen
& Konukoglu, 2018), deep one-class classiﬁcation (Erfani
et al., 2016; Ruff et al., 2019a,b), deep generative mod-
els (Schlegl et al., 2017; Deecke et al., 2018), and outlier
exposure (Hendrycks et al., 2018; Goyal et al., 2020).

Self-supervised anomaly detection has led to drastic im-
provements in detection accuracy (Golan & El-Yaniv, 2018;
Hendrycks et al., 2019; Wang et al., 2019b; Sohn et al.,
2021; Tack et al., 2020). For instance, Golan & El-Yaniv
(2018) and Wang et al. (2019b) augment the data and learn
to predict which transformation was applied. After training,
the resulting classiﬁer is used for anomaly detection.

An alternative approach to self-supervised anomaly detec-
tion is to train a classiﬁer on a contrastive loss to tell if two
views are of the same original image. This leads to strong
representations (Chen et al., 2020), which can be used for
anomaly detection (Sohn et al., 2021; Tack et al., 2020).

Bergman & Hoshen (2020) study how to extend self-
supervised anomaly detection to other domains beyond im-
ages. Similar to Golan & El-Yaniv (2018) and Wang et al.
(2019b), their approach is based on transformation predic-
tion, but they consider the open-set setting. For tabular data,
they use random afﬁne transformations. We study the same

datasets, but our method learns the transformations and
achieves consistently higher performance (see Section 4).

Self-Supervised Learning. Self-supervised learning typ-
ically relies on data augmentation for auxiliary tasks (Do-
ersch et al., 2015; Noroozi & Favaro, 2016; Misra et al.,
2016; Zhang et al., 2017; Gidaris et al., 2018). The net-
works trained on these auxiliary tasks (e.g. patch prediction
(Doersch et al., 2015), solving jigsaw-puzzles (Noroozi &
Favaro, 2016), cross-channel prediction (Zhang et al., 2017),
or rotation prediction (Gidaris et al., 2018)) are used as fea-
ture extractors for downstream tasks. While many of these
methods are developed for images, Misra et al. (2016) pro-
pose temporal order veriﬁcation as an auxiliary task for
self-supervised learning of time series representations.

Contrastive Representation Learning. Many recent self-
supervised methods have relied on the InfoMax principle
(Linsker, 1988; Hjelm et al., 2018). These methods are
trained on the task to maximize the mutual information
(MI) between the data and their context (Oord et al., 2018)
or between different “views” of the data (Bachman et al.,
2019). Computing the mutual information in these settings
is often intractable and various approximation schemes and
bounds have been introduced (Tschannen et al., 2019). By
using noise contrastive estimation (Gutmann & Hyv¨arinen,
2010, 2012) to bound MI, Oord et al. (2018) bridge the
gap between contrastive losses for MI-based representation
learning and the use of contrastive losses in discriminative
methods for representation learning (Hadsell et al., 2006;
Mnih & Kavukcuoglu, 2013; Dosovitskiy et al., 2015; Bach-
man et al., 2019; Chen et al., 2020). We also use a con-
trastive loss. But while the contrastive loss of Chen et al.
(2020) (which is used for anomaly detection of images in
Sohn et al. (2021); Tack et al. (2020),) contrast two views
of the same sample with views of other samples in the mini-

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

batch, NeuTraL AD is tasked with determining the original
version of a sample from different views of the same sample.
The dependence on only a single sample is advantageous
for scoring anomalies at test time and enables us to learn
the data transformations (discussed further in Section 3.2).

Learning Data Augmentation Schemes. The idea of
learning data augmentation schemes is not new. “AutoAug-
mentation” has usually relied on composing hand-crafted
data augmentations (Ratner et al., 2017; Cubuk et al., 2019;
Zhang et al., 2019; Ho et al., 2019; Lim et al., 2019). Tran
et al. (2017) learn Bayesian augmentation schemes for neu-
ral networks, and Wong & Kolter (2020) learn perturbation
sets for adversarial robustness. Though their setting and
approach are different, our work is most closely related
to Tamkin et al. (2021), who study how to generate views
for representation learning in the framework of Chen et al.
(2020). They parametrize their “viewmakers” as residual
perturbations, which are trained adversarially to avoid trivial
solutions where the views share no semantic information
with the original sample (discussed in Section 3.2).

NeuTraL AD falls into the area of deep, self-supervised
anomaly detection, with the core novelty of learning the
transformations so that we can effectively use them for
anomaly detection beyond images. Our method receives
whole time series or tabular data as input. For time series,
this is a remarkable difference to prevalent work on anomaly
detection within time series (e.g. Shen et al., 2020), which
output anomaly scores per time point, but not for the se-
quence as a whole. Additional approaches to time series
anomaly detection include shallow (Hyndman et al., 2015;
Baragona & Battaglia, 2007) and deep-learning approaches
based on modeling (Munir et al., 2018), on autoencoders
(Kieu et al., 2018), or one-class classiﬁcation (Shen et al.,
2020).

3. Neural Transformation Learning for Deep

Anomaly Detection

We develop neural transformation learning for anomaly de-
tection (NeuTraL AD), a deep anomaly detection method
based on contrastive learning for general data types. We
ﬁrst describe the approach in Section 3.1. In Section 3.2, we
provide theoretical arguments why alternative contrastive
loss functions are less suited for transformation learning.

3.1. Proposed Method: NeuTraL AD

Our method, NeuTraL AD, is a simple pipeline with two
components: a set of learnable transformations, and an en-
coder. Both are trained jointly on a deterministic contrastive
loss (DCL). The objective has two purposes. During train-
ing, it is optimized to ﬁnd the parameters of the encoder and
the transformations. During testing, the DCL is also used to

score each sample as either an inlier or an anomaly.

Learnable Data Transformations. We consider a data
space X with samples D = {x(i) ∼ X }N
i=1. We also con-
sider K transformations T := {T1, ..., TK|Tk : X → X }.
We assume here that the transformations are learnable, i.e.,
they can be modeled by any parameterized function whose
parameters are accessible to gradient-based optimization
and we denote the parameters of transformation Tk by θk.
In our experiments, we use feed-forward neural networks for
Tk. Note that in Section 3.2, we use the same notation also
for ﬁxed transformations (such as rotations and cropping).

Deterministic Contrastive Loss (DCL). A key ingredient
of NeuTraL AD is a new objective. The DCL encourages
each transformed sample xk = Tk(x) to be similar to its
original sample x, while encouraging it to be dissimilar
from other transformed versions of the same sample, xl =
Tl(x) with l (cid:54)= k. We deﬁne the score function of two
(transformed) samples as

h(xk, xl) = exp(sim(fφ(Tk(x)), fφ(Tl(x)))/τ ),

(1)

where τ denotes a temperature parameter, and the sim-
ilarity is deﬁned as the cosine similarity sim(z, z(cid:48)) :=
zT z(cid:48)/(cid:107)z(cid:107)(cid:107)z(cid:48)(cid:107) in an embedding space Z. The encoder
fφ(·) : X → Z serves as a features extractor. The DCL is

(cid:34)

L :=Ex∼D

−

K
(cid:88)

k=1

log

h(xk, x)

h(xk, x) + (cid:80)

l(cid:54)=k h(xk, xl)

(cid:35)

.

(2)

The term in the nominator of Equation (2) pulls the embed-
ding of each transformed sample close to the embedding of
the original sample. This encourages the transformations
to preserve relevant semantic information. The denomina-
tor pushes all the embeddings of the transformed samples
away from each other, thereby encouraging diverse trans-
formations. The parameters of NeuTraL AD θ = [φ, θ1:K]
consist of the parameters φ of the encoder, and the parame-
ters θ1:K of the learnable transformations. All parameters θ
are optimized jointly by minimizing Equation (2).

A schematic of NeuTraL AD is in Figure 1. Each sample is
transformed by a set of learnable transformations and then
embedded into a semantic space. The transformations and
the encoder are trained jointly on the DCL (Equation (2)),
which is also used to score anomalies.

Anomaly Score. One advantage of our approach over other
methods is that our training loss is also our anomaly score.
Based on Equation (2), we deﬁne an anomaly score S(x) as

S(x) = −

K
(cid:88)

k=1

log

h(xk, x)

h(xk, x) + (cid:80)

l(cid:54)=k h(xk, xl)

.

(3)

Since the score is deterministic, it can be straightforwardly
evaluated for new data points x without negative samples.

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

normal or an anomaly. And without variability in learning
transformations, the self-supervised learning goal is not met.

We now put the beneﬁts of NeuTraL AD into perspective by
comparing the approach with two other approaches that use
data transformations for anomaly detection:

1. The ﬁrst approach is the transformation prediction
approach by Wang et al. (2019b). Here, fφ(·)
:
X → RK is a deep classiﬁer1 that outputs K values
fφ(x)1 · · · fφ(x)K proportional to the log-probabilities
of the transformations. The transformation prediction
loss is a softmax classiﬁcation loss,

(cid:34)

LP :=Ex∼D

−

K
(cid:88)

k=1

log

exp fφ(xk)k
l=1:K exp fφ(xk)l

(cid:80)

(cid:35)

.

(4)

2. We also consider (Chen et al., 2020), who deﬁne a
contrastive loss on each minibatch of data M ⊂ D
of size N = |M|. For each gradient step, they sam-
ple a minibatch and two transformations T1, T2 ∼ T
from the family of transformations, which are applied
to all the samples to produce x(i)
k = Tk(x(i)). The loss
function is given by LC(M) := (cid:80)N
i=1 LC(x(i)
2 ) +
LC(x(i)

1 , x(i)

2 , x(i)
LC(x(i)


1 ), where
1 , x(i)

2 ) := − log h(x(i)

1 , x(i)
2 )

(5)



+ log



N
(cid:88)

j=1

h(x(i)

1 , x(j)

2 ) +

N
(cid:88)

j=1

1[j(cid:54)=i]h(x(i)

1 , x(j)
1 )

 .

With hand-crafted, ﬁxed transformations, these losses pro-
duce excellent anomaly detectors for images (Golan & El-
Yaniv, 2018; Wang et al., 2019b; Sohn et al., 2021; Tack
et al., 2020). Since it is not always easy to design trans-
formations for new application domains, we study their
suitability for learning data transformations.

We argue that LP and LC are less well suited for transfor-
mation learning than L:
Proposition 1. The ‘constant’ edge-case fφ(Tk(x)) =
Cck, where ck is a one-hot vector encoding the kth po-
sition (i.e. ckk = 1), tends towards the minimum of LP
(Equation (4)) as the constant C goes to inﬁnity.
Proposition 2. The ‘identity’ edge-case Tk(x) = x with
adequate encoder fφ is a minimizer of LC (Equation (5)).

The proof of these propositions is in Appendix A. The in-
tuition is simple. Transformations that only encode which
transformation was used make transformation prediction
easy (Proposition 1), whereas the identity makes any two
views of a sample identical, which can then be easily recog-
nized as a positive pair by LC (Proposition 2).

1even though here fφ is a classiﬁer, we refer to it as the encoder

in the discussion below.

(a) histogram before training

(b) histogram after training

Figure 2. While the histogram of anomaly scores (computed using
Equation (3)) is similar for inliers (blue) and anomalies (orange)
before training, this changes drastically after training, and held-out
inliers and anomalies become easily distinguishable. The data
come from the SAD experiments described in Section 4

By minimizing the DCL (Equation (2)), we minimize the
score (Equation (3)) for training examples (inliers). The
higher the anomaly score, the more likely that a sample is
an anomaly.

This concludes the proposed loss function. We stress that
it is extremely simple and does not need any additional
regularization. However, it is not trivial to see why other
proposed self-supervised approaches are less well suited
for anomaly detection without imposing constraints on the
types of transformations. To this end, we establish theoreti-
cal requirements and desiderata for neural transformation
learning.

3.2. A Theory of Neural Transformation Learning

To learn transformations for self-supervised anomaly detec-
tion we pose two requirements.

Requirement 1 (Semantics). The transformations should
produce views that share relevant semantic information with
the original data.

Requirement 2 (Diversity). The transformations should
produce diverse views of each sample.

A valid loss function for neural transformation learning
should avoid solutions that violate either of these require-
ments. There are plenty of transformations that would vi-
olate Requirement 1 or Requirement 2. For example, a
constant transformation Tk(x) = ck, where ck is a constant
that does not depend on x, would violate the semantic re-
quirement, whereas the identity T1(x) = · · · = TK(x) = x
violates the diversity requirement.

We argue thus that for self-supervised anomaly detection,
the learned transformations need to negotiate the trade-off
between semantics and diversity with the two examples as
edge-cases on a spectrum of possibilities. Without seman-
tics, i.e. without dependence on the input data, an anomaly
detection method can not decide whether a new sample is

2.082.092.10Score2060100Densitynormalabnormal0.51.52.5Score40120200Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

The propositions highlight a serious issue with using LP
or LC for transformation learning and anomaly detection.
Should the optimization reach the edge-cases of Proposi-
tions 1 and 2, LP and LC incur the same loss irrespective
of whether the inputs are normal or abnormal data. Here
are three remedies that can help avoid the trivial edge-cases:
Through careful parametrization, one can deﬁne Tk(·; θk)
in a way that explicitly excludes the edge cases. Beware
that the transformation family might contain other members
that violate Requirement 1 or Requirement 2. The second
potential remedy is regularization that explicitly encourages
Requirements 1 and 2, e.g. based on the InfoMax principle
(Linsker, 1988) or norm constraints. Finally, one can resort
to adversarial training.

Tamkin et al. (2021) use all three of these remedies to
learn “viewmakers” using the contrastive loss LC; They
parametrize the transformations as residual perturbations,
which are regularized to the (cid:96)p ball and trained adversarially.
In contrast, under NeuTraL AD there are no restrictions
on the architecture of the transformations, as long as Equa-
tion (2) can be optimized (i.e. the gradient is well deﬁned).
The DCL is an adequate objective for training the encoder
and transformations jointly as it manages the trade-off be-
tween Requirements 1 and 2.

Proposition 3. The edge-cases of Propositions 1 and 2 do
not minimize L (DCL, Equation (2)).

The proof is in Appendix A. The numerator of Equation (2)
encourages transformed samples to resemble their original
version (i.e. the semantic requirement) and the denomina-
tor encourages the diversity of transformations. The result
of our well-balanced objective is a heterogeneous set of
transformations that model various relevant aspects of the
data. The transformations and the encoder need to high-
light salient features of the data such that a low loss can
be achieved. After training, samples from the normal class
have a low anomaly score while anomalies are handled less
well by the model and as a result, have a high score.

Figure 2 shows empirical evidence for this. We observe that,
while the histogram of anomaly scores (computed using
Equation (3)) is similar for inliers and anomalies before
training, this changes drastically after training, and held-out
inliers and anomalies become easily distinguishable.

There’s another advantage of using the DCL for self-
supervised anomaly detection. Unlike most other contrastive
losses, the “negative samples” are not drawn from a noise
distribution (e.g. other samples in the minibatch) but con-
structed deterministically from x. Dependence on the mini-
batch for negative samples would need to be accounted for
at test time. In contrast, the deterministic nature of Equa-
tion (3) makes it a simple choice for anomaly detection.

4. Empirical Study: Deep Anomaly Detection

of Time Series and Tabular Data

We developed NeuTraL AD for deep anomaly detection
beyond images, so we consider various application domains
involving multiple data types. For image data, strong self-
supervised baselines exist that beneﬁt from hand-crafted
transformations. We do not expect any beneﬁt from using
NeuTraL AD there. Our focus here is on time series and
tabular data, which are important in many application do-
mains of anomaly detection. Our study ﬁnds that NeuTraL
AD improves detection accuracy over the state of the art.

4.1. Evaluation Protocol

We compare NeuTraL AD to prevalent shallow and deep
anomaly-detection baselines using two evaluation protocols:
the standard ‘one-vs.-rest’ and the more challenging ‘n-vs.-
rest’ evaluation protocol. Both settings turn a classiﬁcation
dataset into a quantiﬁable anomaly-detection benchmark.

one-vs-rest. This evaluation setup has been used in vir-
tually all papers on deep anomaly detection published at
top-tier venues (e.g. Ruff et al., 2019a; Hendrycks et al.,
2019; Ruff et al., 2018; Golan & El-Yaniv, 2018; Deecke
et al., 2018; Akcay et al., 2018; Abati et al., 2019; Perera
et al., 2019; Wang et al., 2019a; Bergman & Hoshen, 2020;
Kim et al., 2019). For ‘one-vs.-rest’, the dataset is split by
the N class labels, creating N one class classiﬁcation tasks;
the models are trained on data from one class and tested on
a test set with examples from all classes. The samples from
other classes should be detected as anomalies.

n-vs-rest. We also evaluate on the more challenging n-vs.-
rest protocol, where n classes (for 1 < n < N ) are treated
as normal and the remaining classes provide the anomalies
in the test and validation set. By increasing the variability
of what is considered normal data, one-class classiﬁcation
becomes more challenging.

4.2. Shallow and Deep Anomaly Detection Baselines

We study NeuTraL AD in comparison to a number of unsu-
pervised and self-supervised anomaly detection methods.

Traditional Anomaly Detection Baselines. We chose
three popular anomaly detection baselines: The one-class
SVM (OC-SVM), a kernel-based method, isolation forest
(IF), a tree-based model which aims to isolate anomalies
(Liu et al., 2008), and local outlier factor (LOF), which uses
density estimation with k-nearest neighbors.

Deep Anomaly Detection Baselines. Next, we include
three deep anomaly detection methods, Deep SVDD (Ruff
et al., 2018), which ﬁts a one-class SVM in the feature
space of a neural net, DROCC (Goyal et al., 2020), which
ﬁts a one-class classiﬁer with artiﬁcial outlier exposure, and

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

DAGMM (Zong et al., 2018), which estimates the density
in the latent space of an autoencoder.

Self-Supervised Anomaly Detection Baselines. We also
choose two self-supervised baselines, which are technically
also deep anomaly detection methods. GOAD (Bergman
& Hoshen, 2020) is a distance-based classiﬁcation method
based on random afﬁne transformations. Wang et al. (2019b)
is a softmax-based classiﬁcation method based on hand-
crafted transformations, which show impressive perfor-
mance on images. We adopt their pipeline to time series
here by crafting speciﬁc time series transformations (ﬁxed
Ts, described in Appendix B).

Anomaly Detection Baselines for Time Series. Finally,
we also include two baselines that are speciﬁcally designed
for time series data: The RNN-based model (RNN) directly
models the data distribution p(x1:T ) = (cid:81) p(xt|x<t) and
uses the log-likelihood as the anomaly score. Details on
the architecture are in Appendix B. LSTM-ED (Malhotra
et al., 2016) is an encoder-decoder time series model where
anomaly score is based on the reconstruction error.

4.3. Anomaly Detection of Time Series

Our goal is to detect abnormal time series on a whole-
sequence level. This is a different set-up than novelty detec-
tion within time series, but also very important in practice.

For example, one might want to detect abnormal sound or
ﬁnd production quality issues by detecting abnormal sensor
measurements recorded over the duration of producing a
batch. Other applications are sports and health monitoring;
an abnormal movement pattern during sports can be indica-
tive of fatigue or injury; whereas anomalies in health data
can point to more serious issues.

We study NeuTraL AD on a selection of datasets that are
representative of these varying domains. The datasets come
from the UEA multivariate time series classiﬁcation archive2
(Bagnall et al., 2018).

Time Series Datasets

• SAD: Sound of ten Arabic digits, spoken by 88 speak-
ers. The dataset has 8800 samples, which are stored as
13 Mel Frequency Cepstral Coefﬁcients (MFCCs). We
select sequences with the length between 20 and 50. The
sequences that are shorter than 50 are zero padded to have
the length of 50.

• Naval air training and operating procedures standardiza-
tion (NATOPS): The data is originally from a motion
detection competition of various movement patterns used

2from which we selected datasets on which supervised multi-
class classiﬁcation methods achieve strong results (Ruiz et al.,
2020). Only datasets with separable classes can be repurposed for
one-class classiﬁcation

to control planes in naval air training. The data has six
classes of distinct actions. Each sample is a sequence of
x, y, z coordinates for eight body parts of length 51.

• Character trajectories (CT): The data consists of 2858
character samples from 20 classes, captured using a WA-
COM tablet. Each instance is a 3-dimensional pen tip
velocity trajectory. The data is truncated to the length of
the shortest, which is 182.

• Epilepsy (EPSY): The data was generated with healthy
participants simulating four different activities: walking,
running, sawing with a saw, and seizure mimicking whilst
seated. The data has 275 cases in total, each being a
3-dimensional sequence of length 203.

• Racket sports (RS): The data is a record of university
students playing badminton or squash whilst wearing a
smart watch, which measures the x, y, z coordinates for
both the gyroscope and accelerometer. Sport and stroke
types separate the data into four classes. Each sample is a
6-d sequence with a length of 30.

We compare NeuTraL AD to all baselines from Section 4.2
on these datasets under the one-vs-rest setting. Additionally,
we study how the methods adapt to increased variability of
inliers by exploring SAD and NATOPS under the n-vs-rest
setting for a varying number of classes n considered normal.

Implementation Details We consider
the following
parametrizations of the learnable transformations: feed for-
ward Tk(x) := Mk(x), residual Tk(x) := Mk(x) + x, and
multiplicative Tk(x) := Mk(x) (cid:12) x, which differ in how
they combine the learnable masks Mk(·) with the data3. The
masks Mk are each a stack of three residual blocks of 1d
convolutional layers with instance normalization layers and
ReLU activations, as well as one 1d convolutional layer on
the top. For the multiplicative parameterization, a sigmoid
activation is applied to the masks. All bias terms are ﬁxed
as zero, and the learnable afﬁne parameters of the instance
normalization layers are frozen. For a fair comparison, we
use the same number of 12 transformations in NeuTraL AD,
GOAD, and the classiﬁcation-based method (ﬁxed Ts) for
which we manually designed appropriate transformations.
In Section 4.5 we make a more detailed comparison of vari-
ous design choices for NeuTraL AD and one of our ﬁndings
is that its anomaly detection results are robust to the number
of learnable transformations.

The same encoder architecture is used for NeuTraL AD,
Deep SVDD, DROCC, and with slight modiﬁcation to
achieve the appropriate number of outputs for DAGMM
and transformation prediction with ﬁxed Ts. The encoder is
a stack of residual blocks of 1d convolutional layers. The
number of blocks depends on the dimensionality of the data

3We use 10% of the test set as the validation set to allow

parameterization selection.

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

Table 1. Average AUC with standard deviation for one-vs-rest anomaly detection on time series datasets.

SAD
NATOPS
CT
EPSY
RS

OC-SVM IF
88.2
85.4
94.3
67.7
69.3

95.3
86.0
97.4
61.1
70.0

LOF
98.3
89.2
97.8
56.1
57.4

RNN
81.5±0.4
89.5±0.4
96.3±0.2
80.4±1.8
84.7±0.7

LSTM-ED Deep SVDD DAGMM GOAD
94.7±0.1
93.1±0.5
87.1±1.1
91.5±0.3
97.7±0.1
79.0±1.1
76.7±0.4
82.6±1.7
79.9±0.6
65.4 ±2.1

80.9±1.2
78.9±3.2
89.8±0.7
72.2±1.6
51.0±4.2

86.0±0.1
88.6±0.8
95.7±0.5
57.6±0.7
77.4±0.7

DROCC
85.8±0.8
87.2±1.4
95.3±0.3
85.8±2.1
80.0±1.0

ﬁxed Ts
96.7±0.1
78.4±0.4
97.9±0.1
80.4±2.2
87.7±0.8

NeuTraL AD
98.9±0.1
94.5±0.8
99.3±0.1
92.6±1.7
86.5±0.6

(a) normal data on X

(b) anomalies on X

(c) normal data on Z

(d) anomalies on Z

Figure 3. 3D visualization (projected using PCA) of how the origi-
nal samples (blue) from the SAD dataset and the different views
created by the neural transformations of NeuTraL AD (one color
per transformation type) cluster in data space (Figures 3a and 3b)
and in the embedding space of the encoder (Figures 3c and 3d).
The crisp separation of the different transformations of held-out
inliers (Figure 3c) in contrast to the overlap between transformed
anomalies (Figure 3d) visualizes how NeuTraL AD is able to detect
anomalies.

and is detailed in Appendix B.

Results. The results of NeuTraL AD in comparison to the
baselines from Section 4.2 on time series datasets from vari-
ous ﬁelds are reported in Table 1. NeuTraL AD outperforms
all shallow baselines in all experiments and outperforms
the deep learning baselines in 4 out of 5 experiments. Only
on the RS data, it is outperformed by transformation pre-
diction with ﬁxed transformations, which we designed to
understand the value of learning transformations with Neu-
TraL AD vs using hand-crafted transformations. The results
conﬁrm that designing the transformations only succeeds
sometimes, whereas with NeuTraL AD we can learn the ap-
propriate transformations. The learned transformations also
give NeuTraL AD a competitive advantage over the other
self-supervised baseline GOAD which uses random afﬁne
transformations. The performance of the shallow anomaly
detection baselines hints at the difﬁculty of each anomaly
detection task; the shallow methods perform well on SAD
and CT, but perform worse than the deep learning based
methods on other data.

What does NeuTraL AD learn? For visualization pur-

M1(x)

M2(x)

M3(x)

M4(x)

Figure 4. NeuTraL AD learns dissimilar masks for SAD spectro-
grams. Dark horizontal lines indicate where M1 and M2 mask out
frequency bands almost entirely, while the bright spot in the middle
left part of M4 indicates that this mask brings the intermediate
frequencies in the ﬁrst half of the recording into focus.

poses, we train NeuTraL AD with 4 learnable transforma-
tions on the SAD data. Figure 3 shows the structure in the
data space X and the embedding space of the encoder Z
after training. Held-out data samples (blue) are transformed
by each of the learned transformations Tk(x) = Mk(x) (cid:12) x
to produce K = 4 different views of each sample (the trans-
formations are color-coded by the other colors). Projection
to three principal components with PCA allows for visu-
alization in 3D. In Figures 3a and 3b, we can see that the
transformations already cluster together in the data space,
but only with the help of the encoder, the different views
of inliers are separated from each other Figure 3c. In com-
parison, the anomalies and their transformations are less
structured in Z (Figure 3d), visually explaining why they in-
cur a higher anomaly score and can be detected as anomalies.

The learned masks M1:4(x) of one inlier x are visualized in
Figure 4. We can see that the four masks are dissimilar from
each other, and have learned to focus on different aspects
of the spectrogram. The masks take values between 0 and
1, with dark areas corresponding to values close to 0 that
are zeroed out by the masks, while light colors correspond
to the areas of the spectrogram that are not masked out.
Interestingly, in M1, M2, and M3 we can see ‘black lines’
where they mask out entire frequency bands at least for part
of the sequence. In contrast, M4 has a bright spot in the
middle left part of the spectrogram; it creates views that
focus on the content of the intermediate frequencies at the
ﬁrst half of the recording.

How do the methods cope with an increased variability
of inliers?

To study this question empirically, we increase the number
of classes n considered to be inliers. We test all methods

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

Table 2. Average AUC with standard deviation for n-vs-rest (n = N − 1) anomaly detection on time series datasets

SAD
NATOPS
CT
EPSY
RS

OC-SVM IF
56.9
56.0
57.9
55.3
58.4

60.2
57.6
57.8
50.2
55.9

LOF
93.1
71.2
90.3
54.7
59.4

RNN
53.0±0.1
65.6±0.4
55.7±0.8
74.9±1.5
75.8±0.9

LSTM-ED Deep SVDD DAGMM GOAD
70.5±1.4
58.9±0.5
61.5±0.7
56.9±0.7
81.1±0.1
50.9±1.2
62.7±0.9
56.8±2.1
68.2±0.9
63.1 ±0.6

49.3±0.8
53.2±0.8
47.5±2.5
52.0±1.0
47.8±3.5

59.7±0.5
59.2±0.8
54.4±0.7
52.9±1.4
62.2±2.1

DROCC
58.8±0.5
60.7±1.6
57.6±1.5
55.5±1.9
60.9±0.2

ﬁxed Ts
74.8±1.3
70.8±1.3
63.0±0.6
69.8±1.6
81.6±1.2

NeuTraL AD
85.1±0.3
74.8±0.9
87.4±0.2
80.5±1.0
80.0±0.4

4.4. Anomaly Detection of Tabular Data

Tabular data is another important application area of
anomaly detection. For example, many types of health
data come in tabular form. To unleash the power of self-
supervised anomaly detection for these domains, Bergman &
Hoshen (2020) suggest using random afﬁne transformation.
Here we study the beneﬁt of learning the transformations
with NeuTraL AD. We base the empirical study on tabular
datasets used in previous work (Zong et al., 2018; Bergman
& Hoshen, 2020) and follow their precedent of reporting
results in terms of F1-scores.

Tabular Datasets. We study the four tabular datasets from
the empirical studies of Zong et al. (2018); Bergman &
Hoshen (2020). The datasets include the small-scale med-
ical datasets Arrhythmia and Thyroid as well as the large-
scale cyber intrusion detection datasets KDD and KDDRev
(see Appendix C for all relevant details). We follow the
conﬁguration of (Zong et al., 2018) to train all models on
half of the normal data, and test on the rest of the normal
data as well as the anomalies.

Baseline Models. We compare NeuTraL AD to shallow
and deep baselines outlined in Section 4.2, namely OC-
SVM, IF, LOF, and the deep anomaly detection methods
Deep SVDD, DAGMM, GOAD, and DROCC.

Implementation details. The implementation details of
OC-SVM, LOF, DAGMM, and GOAD are replicated from
Bergman & Hoshen (2020), as we report their results. The
implementation of DROCC is from their ofﬁcial code.

For neural transformations, we use the residual parametriza-
tion Tk(x) = Mk(x) + x on Thyroid and Arrhythmia, and
use the multiplicative parametrization Tk(x) = Mk(x) (cid:12) x
on KDD, and KDDRev. The masks Mk consists of two
bias-free linear layers with an intermediate ReLU activa-
tion. When using the multiplicative parametrization, it has
a sigmoid activation on the ﬁnal layer. The encoder con-
sists of ﬁve bias-free linear layers with ReLU activations.
The output dimensions of the encoder are 24 for Thyroid
and 32 for the other datasets. We set the number of neural
transformations K = 11.

Results. The results of OC-SVM, LOF, DAGMM, and
GOAD are taken from (Bergman & Hoshen, 2020). The

(a) AUCs on SAD

(b) AUCs on NATOPS

Figure 5. AUC result of n-vs-all experiments on SAD and
NATOPS with error bars (barely visible due to signiﬁcance). Neu-
TraL AD outperforms all baselines on NATOPS and all deep learn-
ing baselines on SAD. LOF, a method based on k-nearest neigh-
bors, outperforms NeuTraL AD, when n > 3 on SAD.

on SAD and NATOPS under the n-vs-rest setting with vary-
ing n. Since there are too many combinations of normal
classes when n approaches N − 1, we only consider com-
binations of n consecutive classes. From Figure 5 we can
observe that the performance of all methods drops as the
number of classes included in the normal data increases.
This shows that the increased variance in the nominal data
makes the task more challenging. NeuTraL AD outperforms
all baselines on NATOPS and all deep-learning baselines
on SAD. It is interesting that LOF, a method based on k-
nearest neighbors, performs better than our method (and
all other baselines) on SAD when n is larger than three.
We also include quantitative results for n = N − 1 under
the n-vs-rest setting for all time series datasets, where only
one class is considered abnormal, and the remaining N − 1
classes are normal. The results are reported in Table 2. We
can see, the performance of all deep learning based meth-
ods drops as the variability of normal data increases. Our
method outperforms other deep learning methods on 4 out
of 5 datasets. On RS, it is outperformed by transformation
prediction with hand-crafted transformations. The results
are consistent with the experiments under one-vs.-rest set-
ting in Table 1. The traditional method LOF performs better
than deep learning methods on CT and SAD.

13579n506070809010012345nNeural Transformation Learning for Anomaly Detection (NeuTraL AD)

Table 3. F1-score (%) with standard deviation for anomaly detec-
tion on tabular datasets (choice of F1-score consistent with prior
work).

OC-SVM
IF
LOF
Deep SVDD
DAGMM
GOAD
DROCC
NeuTraL AD

Arrhythmia
45.8
57.4
50.0
53.9±3.1
49.8
52.0±2.3
46
60.3±1.1

Thyroid
38.9
46.9
52.7
70.8±1.8
47.8
74.5±1.1
27
76.8±1.9

KDD
79.5
90.7
83.8
99.0±0.1
93.7
98.4±0.2
-
99.3±0.1

KDDRev
83.2
90.6
81.6
98.6±0.2
93.8
98.9±0.3
-
99.1±0.1

results of DROCC were provided by Goyal et al. (2020) 4.
NeuTraL AD outperforms all baselines on all datasets. Com-
pared with the self-supervised anomaly detection baseline
GOAD, we use much fewer transformations.

4.5. Design Choices for the Transformations

In Section 3.2, we have discussed the advantages of Neu-
TraL AD for neural transformation learning; no regulariza-
tion or restrictions on the transformation family are neces-
sary to ensure the transformations fulﬁll the semantics and
diversity requirements deﬁned in Section 3.2.

In this section, we study the performance of NeuTraL AD
under various design choices for the learnable transforma-
tions, including their parametrization, and their number K.
We consider the following parametrizations: feed forward
Tk(x) := Mk(x), residual Tk(x) := Mk(x) + x, and mul-
tiplicative Tk(x) := Mk(x) (cid:12) x, which differ in how they
combine the learnable masks Mk(·) with the data.

In Figure 6 we show the anomaly detection accuracy
achieved with each parametrization, as K varies from 2
to 15 on the time series data NATOPS and the tabular data
Arrhythmia. For large enough K, NeuTraL AD is robust
to the different parametrizations, since DCL ensures the
learned transformations satisfy the semantic requirement
and the diversity requirement. The performance of NeuTraL
AD improves as the number k increases, and becomes stable
when K is large enough. When K ≤ 4, the performance
has a larger variance, since the learned transformations are
not guaranteed to be useful for anomaly detection with-
out the guidance of any labels. When K is large enough,
the learned transformations contain with high likelihood
some transformations that are useful for anomaly detection.
The transformation based methods (including NeuTraL AD)
have roughly K times the memory requirement as other
deep learning methods (e.g. Deep SVDD). However, the
results in Figure 6 show that even with small K NeuTraL
AD achieves competitive results.

4Their empirical study uses a different experimental setting

while the results reported here are consistent with prior works.

(a) AUC on NATOPS

(b) F1-score on Arrhythmia

Figure 6. The outlier detection accuracy in terms of AUC of Neu-
TraL AD on NATOPS and in terms of F1-score of NeuTraL
AD on Arrhythmia increases as the number of transformations
K increases, but stabilizes when a certain threshold is reached
(K >≈ 10). With enough transformations, NeuTraL AD is robust
to the parametrization of the transformations.

5. Conclusion

We propose a self-supervised anomaly detection method
with learnable transformations. The key ingredient is a novel
training objective based on a deterministic contrastive loss,
which encourages the learned transformations to produce
diverse views that each share semantic information with the
original sample, while being dissimilar. This unleashes the
power of self-supervised anomaly detection to various data
types including time series and tabular data. Our extensive
empirical study ﬁnds, that on these data types, learning
transformations and detecting outliers with NeuTraL AD
improves over the state of the art.

Acknowledgements

MK acknowledges support by the Carl-Zeiss Foundation, by
the German Research Foundation (DFG) award KL 2698/2-
1, and by the Federal Ministry of Science and Education
(BMBF) awards 01IS18051A and 031B0770E.

SM was supported by the National Science Foundation
(NSF) under Grants 1928718, 2003237 and 2007719, by
Qualcomm, and by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR001120C0021.
Any opinions, ﬁndings and conclusions or recommendations
expressed in this material are those of the author(s) and do
not necessarily reﬂect the views of DARPA.

We acknowledge helpful discussions with Dennis Wagner
and Luis Augusto Weber Mercado’s contribution to Fig-
ure 1 and thank Sachin Goyal and Prateek Jain for being
impressively responsive by email and for updating their
experimental results.

References

Abati, D., Porrello, A., Calderara, S., and Cucchiara, R.
Latent space autoregression for novelty detection.
In
Proceedings of the IEEE Conference on Computer Vision

23471115K707580859095AUC (%)M(x)M(x)*xM(x)+x23471115K4045505560F1-score (%)M(x)M(x)*xM(x)+xNeural Transformation Learning for Anomaly Detection (NeuTraL AD)

and Pattern Recognition, pp. 481–490, 2019.

Akcay, S., Atapour-Abarghouei, A., and Breckon, T. P.
Ganomaly: Semi-supervised anomaly detection via adver-
sarial training. In Asian conference on computer vision,
pp. 622–637. Springer, 2018.

Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning
representations by maximizing mutual information across
views. arXiv preprint arXiv:1906.00910, 2019.

Bagnall, A., Dau, H. A., Lines, J., Flynn, M., Large, J.,
Bostrom, A., Southam, P., and Keogh, E. The uea mul-
tivariate time series classiﬁcation archive, 2018. arXiv
preprint arXiv:1811.00075, 2018.

Bamler, R. and Mandt, S. Extreme classiﬁcation via adver-
sarial softmax approximation. In International Confer-
ence on Learning Representations, 2020.

Baragona, R. and Battaglia, F. Outliers detection in multi-
variate time series by independent component analysis.
Neural computation, 19(7):1962–1984, 2007.

Bergman, L. and Hoshen, Y. Classiﬁcation-based anomaly
detection for general data. In International Conference
on Learning Representations, 2020.

Erfani, S. M., Rajasegarar, S., Karunasekera, S., and Leckie,
C. High-dimensional and large-scale anomaly detection
using a linear one-class svm with deep learning. Pattern
Recognition, 58:121–134, 2016.

Gidaris, S., Singh, P., and Komodakis, N. Unsupervised
representation learning by predicting image rotations. In
International Conference on Learning Representations,
2018.

Golan, I. and El-Yaniv, R. Deep anomaly detection us-
ing geometric transformations. In Advances in Neural
Information Processing Systems, pp. 9758–9769, 2018.

Goyal, S., Raghunathan, A., Jain, M., Simhadri, H. V., and
Jain, P. Drocc: Deep robust one-class classiﬁcation. In
International Conference on Machine Learning, pp. 3711–
3721. PMLR, 2020.

Gutmann, M. and Hyv¨arinen, A. Noise-contrastive estima-
tion: A new estimation principle for unnormalized statisti-
cal models. In Proceedings of the Thirteenth International
Conference on Artiﬁcial Intelligence and Statistics, pp.
297–304. JMLR Workshop and Conference Proceedings,
2010.

Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A
simple framework for contrastive learning of visual rep-
In International conference on machine
resentations.
learning, pp. 1597–1607. PMLR, 2020.

Gutmann, M. U. and Hyv¨arinen, A. Noise-contrastive es-
timation of unnormalized statistical models, with appli-
cations to natural image statistics. Journal of Machine
Learning Research, 13(2), 2012.

Chen, X. and Konukoglu, E. Unsupervised detection of
lesions in brain mri using constrained adversarial auto-
encoders. In MIDL Conference book. MIDL, 2018.

Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le,
Q. V. Autoaugment: Learning augmentation strategies
from data. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR),
June 2019.

Deecke, L., Vandermeulen, R., Ruff, L., Mandt, S., and
Kloft, M. Image anomaly detection with generative adver-
sarial networks. In Joint european conference on machine
learning and knowledge discovery in databases, pp. 3–17.
Springer, 2018.

Doersch, C., Gupta, A., and Efros, A. A. Unsupervised
visual representation learning by context prediction. In
Proceedings of the IEEE international conference on com-
puter vision, pp. 1422–1430, 2015.

Dosovitskiy, A., Fischer, P., Springenberg, J. T., Riedmiller,
M., and Brox, T. Discriminative unsupervised feature
learning with exemplar convolutional neural networks.
IEEE transactions on pattern analysis and machine intel-
ligence, 38(9):1734–1747, 2015.

Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality
In 2006
reduction by learning an invariant mapping.
IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR’06), volume 2, pp. 1735–
1742. IEEE, 2006.

Hendrycks, D., Mazeika, M., and Dietterich, T. Deep
anomaly detection with outlier exposure. In International
Conference on Learning Representations, 2018.

Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D.
Using self-supervised learning can improve model robust-
ness and uncertainty. In Advances in Neural Information
Processing Systems, pp. 15663–15674, 2019.

Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal,
K., Bachman, P., Trischler, A., and Bengio, Y. Learning
deep representations by mutual information estimation
and maximization. In International Conference on Learn-
ing Representations, 2018.

Ho, D., Liang, E., Chen, X., Stoica, I., and Abbeel, P. Popu-
lation based augmentation: Efﬁcient learning of augmen-
tation policy schedules. In International Conference on
Machine Learning, pp. 2731–2741. PMLR, 2019.

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

Hyndman, R. J., Wang, E., and Laptev, N. Large-scale
unusual time series detection. In 2015 IEEE international
conference on data mining workshop (ICDMW), pp. 1616–
1619. IEEE, 2015.

Kieu, T., Yang, B., and Jensen, C. S. Outlier detection for
multidimensional time series using deep neural networks.
In 2018 19th IEEE International Conference on Mobile
Data Management (MDM), pp. 125–134. IEEE, 2018.

Kim, K. H., Shim, S., Lim, Y., Jeon, J., Choi, J., Kim,
B., and Yoon, A. S. Rapp: Novelty detection with re-
construction along projection pathway. In International
Conference on Learning Representations, 2019.

Lim, S., Kim, I., Kim, T., Kim, C., and Kim, S. Fast au-
toaugment. Advances in Neural Information Processing
Systems, 32:6665–6675, 2019.

Linsker, R. Self-organization in a perceptual network. Com-

puter, 21(3):105–117, 1988.

Liu, F. T., Ting, K. M., and Zhou, Z.-H. Isolation forest.
In 2008 Eighth IEEE International Conference on Data
Mining, pp. 413–422. IEEE, 2008.

Malhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agar-
wal, P., and Shroff, G. Lstm-based encoder-decoder
arXiv preprint
for multi-sensor anomaly detection.
arXiv:1607.00148, 2016.

on Computer Vision and Pattern Recognition, pp. 2898–
2906, 2019.

Principi, E., Vesperini, F., Squartini, S., and Piazza, F.
Acoustic novelty detection with adversarial autoencoders.
In 2017 International Joint Conference on Neural Net-
works (IJCNN), pp. 3324–3330. IEEE, 2017.

Ratner, A. J., Ehrenberg, H. R., Hussain, Z., Dunnmon, J.,
and R´e, C. Learning to compose domain-speciﬁc trans-
formations for data augmentation. Advances in neural
information processing systems, 30:3239, 2017.

Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Sid-
diqui, S. A., Binder, A., M¨uller, E., and Kloft, M. Deep
one-class classiﬁcation. In International conference on
machine learning, pp. 4393–4402, 2018.

Ruff, L., Vandermeulen, R. A., G¨ornitz, N., Binder, A.,
M¨uller, E., M¨uller, K.-R., and Kloft, M. Deep semi-
supervised anomaly detection. In International Confer-
ence on Learning Representations, 2019a.

Ruff, L., Zemlyanskiy, Y., Vandermeulen, R., Schnake, T.,
and Kloft, M. Self-attentive, multi-context one-class
classiﬁcation for unsupervised anomaly detection on text.
In Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 4061–4071,
2019b.

Misra, I., Zitnick, C. L., and Hebert, M. Shufﬂe and learn:
unsupervised learning using temporal order veriﬁcation.
In European Conference on Computer Vision, pp. 527–
544. Springer, 2016.

Ruff, L., Kauffmann, J. R., Vandermeulen, R. A., Montavon,
G., Samek, W., Kloft, M., Dietterich, T. G., and M¨uller,
K.-R. A unifying review of deep and shallow anomaly
detection. Proceedings of the IEEE, 2021.

Mnih, A. and Kavukcuoglu, K. Learning word embeddings
efﬁciently with noise-contrastive estimation. Advances
in neural information processing systems, 26:2265–2273,
2013.

Munir, M., Siddiqui, S. A., Dengel, A., and Ahmed, S.
Deepant: A deep learning approach for unsupervised
anomaly detection in time series. IEEE Access, 7:1991–
2005, 2018.

Noroozi, M. and Favaro, P. Unsupervised learning of vi-
sual representations by solving jigsaw puzzles. In Euro-
pean conference on computer vision, pp. 69–84. Springer,
2016.

Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn-
ing with contrastive predictive coding. arXiv preprint
arXiv:1807.03748, 2018.

Perera, P., Nallapati, R., and Xiang, B. Ocgan: One-class
novelty detection using gans with constrained latent rep-
In Proceedings of the IEEE Conference
resentations.

Ruiz, A. P., Flynn, M., Large, J., Middlehurst, M., and Bag-
nall, A. The great multivariate time series classiﬁcation
bake off: a review and experimental evaluation of re-
cent algorithmic advances. Data Mining and Knowledge
Discovery, pp. 1–49, 2020.

Schlegl, T., Seeb¨ock, P., Waldstein, S. M., Schmidt-Erfurth,
U., and Langs, G. Unsupervised anomaly detection with
generative adversarial networks to guide marker discov-
ery. In International conference on information process-
ing in medical imaging, pp. 146–157. Springer, 2017.

Shen, L., Li, Z., and Kwok, J. Timeseries anomaly de-
tection using temporal hierarchical one-class network.
In Advances in Neural Information Processing Systems,
2020.

Sohn, K., Li, C.-L., Yoon, J., Jin, M., and Pﬁster, T. Learn-
ing and evaluating representations for deep one-class clas-
siﬁcation. In International Conference on Learning Rep-
resentations, 2021.

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

Tack, J., Mo, S., Jeong, J., and Shin, J. Csi: Novelty de-
tection via contrastive learning on distributionally shifted
In 34th Conference on Neural Information
instances.
Processing Systems (NeurIPS) 2020. Neural Information
Processing Systems, 2020.

Tamkin, A., Wu, M., and Goodman, N. Viewmaker net-
works: Learning views for unsupervised representation
learning. In International Conference on Learning Rep-
resentations, 2021.

Tran, T., Pham, T., Carneiro, G., Palmer, L., and Reid,
I. A bayesian data augmentation approach for learning
deep models. In Proceedings of the 31st International
Conference on Neural Information Processing Systems,
pp. 2794–2803, 2017.

Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S.,
and Lucic, M. On mutual information maximization for
representation learning. In International Conference on
Learning Representations, 2019.

Wang, J., Sun, S., and Yu, Y. Multivariate triangular quan-
tile maps for novelty detection. In Advances in Neural
Information Processing Systems, pp. 5060–5071, 2019a.

Wang, S., Zeng, Y., Liu, X., Zhu, E., Yin, J., Xu, C., and
Kloft, M. Effective end-to-end unsupervised outlier de-
tection via inlier priority of discriminative network. In
Advances in Neural Information Processing Systems, pp.
5962–5975, 2019b.

Wong, E. and Kolter, J. Z. Learning perturbation sets for ro-
bust machine learning. arXiv preprint arXiv:2007.08450,
2020.

Zhang, R., Isola, P., and Efros, A. A. Split-brain autoen-
coders: Unsupervised learning by cross-channel predic-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1058–1067, 2017.

Zhang, X., Wang, Q., Zhang, J., and Zhong, Z. Adversarial
autoaugment. In International Conference on Learning
Representations, 2019.

Zhou, C. and Paffenroth, R. C. Anomaly detection with
robust deep autoencoders. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 665–674, 2017.

Zong, B., Song, Q., Min, M. R., Cheng, W., Lumezanu,
C., Cho, D., and Chen, H. Deep autoencoding gaussian
mixture model for unsupervised anomaly detection. In
International Conference on Learning Representations,
2018.

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

A. Proofs for Section 3.2

In this Appendix, we prove the propositions in Section 3.2. The point is to compare various potential loss functions for
neural transformation learning in their ability to produce useful transformations for self-supervised anomaly detection.

Requirements 1 and 2 formalize what we consider useful transformations. The learned transformations should produce
diverse views the share semantic information with the original sample.

We give two example edge-cases that violate the requirements, the ‘constant’ edge-case in which the transformed views do
no longer depend on the original sample (this violates the semantic requirement) and the ‘identity’ edge-case in which all
transformations reproduce the original sample perfectly but violate the diversity requirement.

We compare what happens to various losses for self-supervised anomaly detection under these edge cases. Speciﬁcally, we
compare the loss of our method L (Equation (2)) to the transformation prediction loss LP (Equation (4)) of Wang et al.
(2019b), and the SimCLR loss LC (Equation (5), Chen et al. (2020)), which has been used for anomaly detection in Sohn
et al. (2021) and Tack et al. (2020). In the original sources these losses have been used with ﬁxed transformations (typically
image transformations like rotations, cropping, blurring, etc.). Here we consider the same losses but with the learnable
transformations parameterized as deﬁned in Section 3.1. All notation has been deﬁned in Section 3.1.

A.1. Proof of Proposition 1

We ﬁrst investigate whether we can optimize the transformation prediction loss Equation (4) with respect to the transformation
parameters θk and the parameters of fφ and obtain learned transformations that fulﬁll Requirements 1 and 2.

The proposition below states that a constant edge-case can achieve a global minimum of Equation (4), which means that
minimizing it can produce transformations the violate Requirement 1.

Proposition 1. The ‘constant’ edge-case fφ(Tk(x)) = Cck, where ck is a one-hot vector encoding the kth position (i.e.
ckk = 1) tends towards the minimum of LP (Equation (4)) as the constant C goes to inﬁnity.

Proof. As a negative log probability LP ≥ 0 is lower bounded by 0. We want to show that with fφ(Tk(x)) = Cck, (where
ck is a one hot vector and C is a constant,) L goes to 0 as C goes to inﬁnity. Plugging fφ(Tk(x)) = Cck into Lp and taking
the limit yields

lim
C→∞

LP = lim
C→∞

Ex∼D[−

K
(cid:88)

k=1

log

exp C
exp C + K − 1

] = lim
C→∞

−K log

exp C
exp C + K − 1

= lim
C→∞

−KC + K log(exp C + K − 1) = 0

A.2. Proof of Proposition 2

Next, we ask what would happen if we optimized the SimCLR loss LC (Equation (5)) with respect to the transformation
parameters and the encoder.

The result is, that if we allowed the encoder fφ to be as ﬂexible as necessary to achieve a global minimum of LC, then
we can derive another minimum of LC that relies only on identity transformations, thereby obtaining a solution to the
minimization problem that violates the diversity requirement.

Proposition 2. The ‘identity’ edge-case Tk(x) = x with adequate encoder fφ is a minimizer of LC (Equation (5)).

(cid:125)

(7)

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

Proof. Lc(M) can be separated as the alignment term and the uniformity term.

Lc(M) =

+

N
(cid:88)

i=1
(cid:124)

N
(cid:88)

i=1
(cid:124)

(cid:104)
− log h(x(i)

1 , x(i)

2 ) − log h(x(i)

2 , x(i)
1 )

(cid:123)(cid:122)
Lalignment



log





N
(cid:88)

j=1

h(x(i)

1 , x(j)

2 ) +

N
(cid:88)

j=1

(cid:105)

(cid:125)



1[j(cid:54)=i]h(x(i)

1 , x(j)
1 )

 + log

(6)





1[j(cid:54)=i]h(x(i)

2 , x(j)
2 )





.

h(x(i)

2 , x(j)

1 ) +

N
(cid:88)

j=1





N
(cid:88)

j=1

(cid:123)(cid:122)
Luniformity

A sufﬁcient condition of min(Lc(M)) is both Lalignment and Luniformity are minimized.

min(Lc(M)) ≥ min(Lalignment) + min(Luniformity) .

Given an adequate encoder f ∗
φ, that is ﬂexible enough to minimize both Lalignment and Luniformity for all transformation
pairs T1 and T2, we will show we can construct another solution to the minimization problem that relies only on identity
transformations.

The alignment term is only minimized for all T1, T2, if f ∗
that

φ(T1(x(i))) = f ∗

φ(T2(x(i))) for all x(i) ∼ M. So we know for f ∗
φ

f ∗
φ = arg min

fφ

Lalignment ⇐⇒ sim(f ∗

φ(T1(x(i))), f ∗

φ(T2(x(i)))) = 1 ∀ x(i) ∼ M

⇐⇒ f ∗

φ(T1(x(i))) = f ∗

φ(T2(x(i))) ∀ x(i) ∼ M.

Deﬁne ˜fφ = f ∗

φ ◦ T1. Since f ∗

φ(T1(x(i))) = f ∗

φ(T2(x(i))),

˜fφ(I(x(i))) = f ∗

φ(T1(x(i))) = f ∗

φ(T2(x(i))) ∀ x(i) ∼ M.

(8)

(9)

(10)

Using only the identity transformation I(x) = x for T1 and T2, and ˜fφ as the encoder in LC yields the same minimal loss as
under T1, T2 and f ∗
φ.

A.3. Proof of Proposition 3

Finally, we investigate the effect of the edge-cases from Propositions 1 and 2 on our objective Equation (2).
Proposition 3. The edge-cases of Propositions 1 and 2 do not minimize L (DCL, Equation (2)).

We divide the proposition and its proof into two parts.
Proposition 3, Part 1. The ‘constant’ edge-case fφ(Tk(x)) = Cck, where ck is a one-hot vector encoding the kth position
(i.e. ckk = 1) does not minimize L (DCL, Equation (2)) for any C, also not as C tends to inﬁnity.

Proof. For simplicity, we deﬁne the embeddings obtained by the transformations as zk := fφ(Tk(x)) for k = 1, · · · , K
and z0 := fφ(x). We prove that the gradient of L(x) with respect to embeddings ∇L(x) = [ ∂L(x)
]T (cid:54)= 0 at
∂z0
z1:K = Cc1:K, where ck is a one-hot vector encoding the kth position (i.e. ck,k = 1). Using the chain rule, the partial
derivative ∂L(x)
∂zn

∀ n ∈ {0, · · · , K} can be factorized as

, · · · , L(x)
∂zK

∂L(x)
∂zn

=

K
(cid:88)

k=1

(cid:80)

l∈{1,···,K}/{k} h(zk, zl)( ∂sim(zk,zl)
l∈{0,···,K}/{k} h(zk, zl)

(cid:80)

∂zn/(cid:107)zn(cid:107) − ∂sim(zk,z0)
∂zn/(cid:107)zn(cid:107) )

I − znzT

n /(cid:107)zn(cid:107)2

(cid:107)zn(cid:107)

.

(11)

We deﬁne An := I−znzT

n /(cid:107)zn(cid:107)2
(cid:107)zn(cid:107)

, and plug in z1:K = Cc1:K and sim(a, b) = aT b/(cid:107)a(cid:107)(cid:107)b(cid:107).

∂L(x)
∂zn

|z1:K =Cc1:K

=

(cid:88)

k∈{1,···,K}/{n}

cT
k An − zT
0 An/(cid:107)z0(cid:107)
h(Ccn, z0) + K − 1

+

cT
k An
h(Cck, z0) + K − 1

.

(12)

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

As C is ﬁnite, by assuming the kth (k (cid:54)= n) entry of ∂L(x)
∂zn

equals zero at z1:K = Cc1:K we get the kth entry of z0

zT
0,k =

(cid:107)z0(cid:107)
K − 1

(cT

k,k + cT
k,k

h(Ccn, z0) + K − 1
h(Cck, z0) + K − 1

).

Similarly, by assuming kth entry of ∂L(x)
∂zm

equals zero at z1:K = Cc1:K with m (cid:54)= n and m (cid:54)= k we have

zT
0,k =

(cid:107)z0(cid:107)
K − 1

(cT

k,k + cT
k,k

h(Ccm, z0) + K − 1
h(Cck, z0) + K − 1

).

(13)

(14)

As Equation (13) and Equation (14) are equal, we have h(Ccn, z0) = h(Ccm, z0). Since this should be hold by assuming
any entry of any partial derivative equals zero, by induction we have

h(Cck, z0) = r,

zT
0,k/(cid:107)z0(cid:107)=

2
K − 1

∀ k ∈ {1, · · · , K}.

By plugging in h(Cck, z0) = r and z1:K = Cc1:K to ∂L(x)
∂z0

we have

∂L(x)
∂z0

|h(Cck,z0)=r,z1:K =Cc1:K

=

1 − K
r + K − 1

K
(cid:88)

k=1

cT
k

I − z0zT

0 /(cid:107)z0(cid:107)2

(cid:107)z0(cid:107)

(15)

(16)

Equation (16) equals zero, if and only if every entry in the resulting vector equals zero. By assuming the kth (k ∈ {1, · · · , K})
entry of ∂L(x)
∂z0

equals zero at Equation (15) and z1:K = Cc1:K, we have

1 − K(

2
K − 1

)2 = 0 ,

(17)

which leads to a non-integral value of K. Since K is the number of transformations and is deﬁned as an integer, we have
, · · · , L(x)
Equation (16) is not zero. Therefore, ∇L(x) = [ ∂L(x)
∂zK
∂z0

]T (cid:54)= 0 at z1:K = Cc1:K.

Proposition 3, Part 2. The ‘identity’ edge-case Tk(x) = x does not minimize L (Equation (2)) for adequate encoder fφ.

Proof. Plugging Tk(x) = x for all k into Equation (2)

L =Ex∼D[−

K
(cid:88)

k=1

log

h(x, x)
Kh(x, x)

] = K log K.

(18)

This is K times the cross-entropy of the uniform distribution, meaning that using the identity transformation is equivalent to
random guessing for the task of the DCL, which is to predict which sample is the original given a transformed view. When
fφ is adequate (i.e. ﬂexible enough) we can do better than random on L. This can be seen in the anomaly scores in Figure 2b
which are much smaller than K log K after training (better than random).

We can also construct examples which achieve better than random performance. For example, for K = 2, taking z1 ⊥ z,
z2 ⊥ z, and z1 = −z2, does better than random. The loss values (with τ = 1) of the two cases are

• ‘Identity’ edge-case: z1 = z2 = z, so L(x) = −2 log(0.5) = 1.386.

• Counterexample: z1 ⊥ z, z2 ⊥ z, and z1 = −z2, so L(x) = −2 log(1/(1 + exp(−1))) = 0.627.

The counterexample achieves a lower loss value than the ‘identity’ edge-case. So the ‘identity’ edge-case is not the minimum
of L(x).

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

B. Implementation details

B.1. Implementations of NeuTraL AD on time series datasets

The networks in the neural transformations used in all experiments consist of one convolutional layer on the bottom, a stack
of three residual blocks of 1d convolutional layers with instance normalization layers and ReLU activations, as well as one
convolutional layer on the top. All convolutional layers are with the kernel size of 3, and the stride of 1. All bias terms
are ﬁxed as zero, and the learnable afﬁne parameters of the instance normalization layers are frozen. The dimension of the
residual blocks is the data dimension. The convolutional layer on the top has an output dimension as the data dimension. For
the multiplicative parameterization, a sigmoid activation is added to the end.

The encoder used in all experiments consists of residual blocks of 1d convolutional layers with ReLU activations, as well as
one 1d convolutional layer on the top of all residual blocks.The detailed network structure (from bottom to top) in each time
series dataset is:

• SAD: (i) one residual block with the kernel size of 3, the stride of 1, and the output dimension of 32. (ii) four residual
blocks with the kernel size of 3, the stride of 2, and the output dimensions of 32, 64, 128, 256. (iii) one 1d convolutional
layer with the kernel size of 6, the stride of 1, and the output dimension of 32.

• NATOPS: (i) one residual block with the kernel size of 3, the stride of 1, and the output dimension of 32. (ii) four
residual blocks with the kernel size of 3, with the stride of 2, and the output dimensions of 32, 64, 128, 256. (iii) one 1d
convolutional layer with the kernel size of 4, the stride of 1, and the output dimension of 64.

• CT: (i) one residual block with the kernel size of 3, the stride of 1, and the output dimension of 32. (ii) six residual
blocks with the kernel size of 3, the stride of 2, and the output dimensions of 32, 64, 128, 256, 512, 1024. (iii) one 1d
convolutional layer with the kernel size of 3, the stride of 1, and the output dimension of 64.

• EPSY: (i) one residual block with the kernel size of 3, the stride of 1, and the output dimension of 32. (ii) six residual
blocks with the kernel size of 3, the stride of 2, and the output dimensions of 32, 64, 128, 256, 512, 1024. (iii) one 1d
convolutional layer with the kernel size of 4, the stride of 1, and the output dimension of 128.

• RS: (i) one residual block with the kernel size of 3, the stride of 1, and the output dimension of 32. (ii) three residual
blocks with the kernel size of 3, the stride of 2, and the output dimensions of 32, 64, 128. (iii) one 1d convolutional layer
with the kernel size of 4, the stride of 1, and the output dimension of 64.

B.2. Implementations of baselines

• Traditional Anomaly Detection Baselines. OC-SVM, IF, and LOF are taken from scikit-learn library with default

parameters.

• Deep Anomaly Detection Baselines. The implementations of Deep SVDD, DROCC, and DAGMM are adopted from
the published codes with a similar encoder as NeuTraL AD. DAGMM has a hyperparameter of the number of mixture
components. We consider the number of components between 4 and 12 and select the best performing one.

• Self-supervised Anomaly Detection Baselines. The implementation of GOAD is taken from the published code. The
results of GOAD depend on the choice of the output dimension r of afﬁne transformations. We consider the reduced
dimension r ∈ {22, 23, ..., 26}, and select the best performing one. We craft speciﬁc time series transformations for the
designed classiﬁcation-based baseline. The hand-crafted transformations are the compositions of ﬂipping along the time
axis (true/false), ﬂipping along the channel axis (true/false), and shifting along the time axis by 0.25 of its time length
(forward/backward/none). By taking all possible compositions, we obtain a total of 2 ∗ 2 ∗ 3 = 12 transformations.

• Anomaly Detection Baselines for Time Series. The RNN is parameterized by two layers of recurrent neural networks, e.g.
GRU, and a stack of two linear layers with ReLU activation on the top of it which outputs the mean and variance at each
time step. The implementation of LSTM-ED is taken from the web.

C. Tabular datasets

The four used tabular datasets are:

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

• Arrhythmia: A cardiology dataset from the UCI repository contains 274 continuous attributes and 5 categorical attributes.
Following the data preparation of previous works, only 274 continuous attributes are considered. The abnormal classes
include 3, 4, 5, 7, 8, 9, 14, and 15. The rest classes are considered as normal.

• Thyroid: A medical dataset from the UCI repository contains attributes related to hyperthyroid diagnosis. Following the
data preparation of previous works, only 6 continuous attributes are considered. The hyperfunction class is treated as
abnormal, and the rest 2 classes are considered as normal.

• KDDCUP: The KDDCUP99 10 percent dataset from the UCI repository contains 34 continuous attributes and 7 categorical
attributes. Following the data preparation of previous works, 7 categorical attributes are represented by one-hot vectors.
Eventually, the data has 120 dimensions. The attack samples are considered as normal, and the non-attack samples are
considered as abnormal.

• KDDCUP-Rev: It is derived from the KDDCUP99 10 percent dataset. The non-attack samples are considered as
normal, and attack samples are considered as abnormal. Following the data preparation of previous works, attack data is
sub-sampled to consist of 25% of the number of non-attack samples.

D. Additional qualitative results

D.1. Results for time series data

In Figure 7, we show the learned transformations (parameterized as T (x) = M (x) (cid:12) x and K = 4) on spoken Arabic digits.
The learned transformations given one normal example are shown in the ﬁrst row. The learned transformations given one
example of each abnormal class are shown in the following rows.

In Figure 8, we show the learned transformations (parameterized as T (x) = M (x) and K = 4) on NATOPS. The learned
transformations given one normal example are shown in the ﬁrst row. The learned transformations given one example of
each abnormal class are shown in the following rows.

D.2. Results for tabular data

The learned transformations of thyroid, which are visualized in Figure 9, offer us the possible explanations of why a data
instance is an anomaly. We illustrate one normal example and three anomalies in the ﬁrst row. Since the anomaly score
Equation (3) is the sum of terms caused by each transformation. Each score shown in the last row has four bars indicating
the terms caused by each transformation. The score of the normal example is very low, and its bars are invisible from the
plot. The scores of three anomalies are mainly contributed by different terms (colored with orange). The four learned masks
are colored blue and listed in four rows. M4 focuses on checking the value of the fourth attribute and contributes high values
to the scores of all listed anomalies. In comparison, M2 is less useful for anomaly detection. NeuTraL AD is able to learn
diverse transformations but is not guaranteed to learn transformations that are useful for anomaly detection, since no label is
included in the training.

We project the score terms of test data contributed by T1, T3, and T4 to a simplex to visualize which transformation
dominates the anomaly score in Figure 10. From the left subplot, we can see, the scores of normal data (blue) are not
dominated by any single transformation, while the scores of anomalies are mainly dominated by T3 and T4. In the right
subplot, we visualize the magnitudes of scores via transparency. We can see, the score magnitudes of normal data are clearly
lower than the score magnitudes of anomalies.

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

x

T1(x)

T2(x)

T3(x)

T4(x)

Figure 7. The learned transformations (T (x) = M (x) (cid:12) x) on SAD. The ﬁrst row: the learned transformations of one given example of
the normal class. The rest rows: the learned transformations of one given example of each abnormal class.

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

x

T1(x)

T2(x)

T3(x)

T4(x)

Figure 8. The learned transformations (T (x) = M (x)) on NATOPS. The ﬁrst row: the learned transformations of one given example of
the normal class. The rest rows: the learned transformations of one given example of each abnormal class.

Neural Transformation Learning for Anomaly Detection (NeuTraL AD)

normal data

anomaly 1

anomaly 2

anomaly 3

x

M1(x)

M2(x)

S(x)

M3(x)

M4(x)

Figure 9. Visualizations of thyroid. The ﬁrst row: one normal example and three abnormal examples. The second to the fourth rows: the
learned four masks of them. The ﬁfth row: the scores contributed by each transformation of them, where the highest term is colored by
orange. The plots on each row are under the same scale.

Figure 10. Visualizations of thyroid. We project the scores contributed by T1, T3, and T4 to a simplex. The subplot on the left visualizes
which transformation dominates the score. The subplot on the right visualizes the scores via transparency.

T1T2T3T4T1T2T3T4T1T2T3T4T1T2T3T4T1T3T4normalabnormalT1T3T4