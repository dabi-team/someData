Metrics Towards Measuring Cyber Agility

Jose David Mireles, Eric Ficke, Jin-Hee Cho, Senior Member, IEEE, Patrick Hurley, and Shouhuai Xu

1

9
1
0
2

n
u
J

2
1

]

R
C
.
s
c
[

1
v
5
9
3
5
0
.
6
0
9
1
:
v
i
X
r
a

Abstract—In cyberspace, evolutionary strategies are commonly
used by both attackers and defenders. For example, an attacker’s
strategy often changes over the course of time, as new vulnera-
bilities are discovered and/or mitigated. Similarly, a defender’s
strategy changes over time. These changes may or may not be
in direct response to a change in the opponent’s strategy. In
any case, it is important to have a set of quantitative metrics
to characterize and understand the effectiveness of attackers’
and defenders’ evolutionary strategies, which reﬂect their cyber
agility. Despite its clear importance, few systematic metrics have
been developed to quantify the cyber agility of attackers and
defenders. In this paper, we propose the ﬁrst metric framework
for measuring cyber agility in terms of the effectiveness of the
dynamic evolution of cyber attacks and defenses. The proposed
framework is generic and applicable to transform any relevant,
quantitative, and/or conventional static security metrics (e.g.,
false positives and false negatives) into dynamic metrics to
capture dynamics of system behaviors. In order to validate the
usefulness of the proposed framework, we conduct case studies
on measuring the evolution of cyber attacks and defenses using
two real-world datasets. We discuss the limitations of the current
work and identify future research directions.

Index Terms—Security metrics, agility metrics, cyber agility,

cyber maneuverability, measurements, attack, defense

I. INTRODUCTION

I N order to maximize the effectiveness of cyber attacks

or defenses, both cyber attackers and defenders frequently
evolve their strategies. The rule-of-thumb is that cyber at-
tackers are more agile in adapting their strategies than cyber
defenders, because cyber defenders often tend to take reactive
responses to new attacks. Accordingly, cyber attack incidents
are frequently reported in news media. However, the state-of-
the-art technology does not provide quantitative metrics that
can measure how well cyber attackers or defenders are able to
adapt or update their resources over time. We call this problem
measuring cyber agility. Cyber agility and its quantiﬁcation
have recently been recognized as critical cybersecurity issues
that are little understood [1], [2], [3], [4], [5].

In this paper, we take a ﬁrst step towards tackling the
problem of measuring cyber agility. We propose a systematic
set of quantitative metrics to measure cyber attack and defense
evolution generations (or simply generations), which are cyber
attack and defense updates that can be considered as “building-
blocks” or “atomic moves” used by cyber attackers and de-
fenders in their operational practice. The notion of generations
is important because cyber attacks and defenses often evolve
over time. It is also important to see that the causality of
evolution generations may differ between them. For example,
some evolution generations are caused by speciﬁc opponent

J. Mireles, E. Ficke, and S. Xu are with the Department of Computer
Science, University of Texas at San Antonio. J.H. Cho is with the Department
of Computer Science, Virginia Tech. P. Hurley is with the U.S. Air Force
Research Laboratory, Rome, NY. Correspondence: shxu@cs.utsa.edu

moves, but others aren’t (e.g., moving-target defense is not
necessarily caused by any speciﬁc attacks). Intuitively, we can
quantify the relative agility of cyber attackers and defenders
by characterizing their evolving strategies and measuring their
effectiveness during the course of interplay between cyber
attackers and cyber defenders.

The aforementioned dynamic view of cybersecurity metrics,
which we pursue in the present paper, contrasts with the
conventional static view of cybersecurity metrics as follows:
the dynamic view reﬂects a system’s evolution over a period of
time, while the static view captures measurements of metrics at
a certain time point or in an aggregated way. To the best of our
knowledge, this is the ﬁrst work that deﬁnes systematic metrics
to quantify the effectiveness of cyber attack and defense
evolution gearing towards measuring cyber agility.

A. Key Contributions

This work makes the following key contributions:
• Development of a system-level evolutionary metric
framework: We develop a dynamic metric framework
that can deliver more information of system behaviors
than static metrics. There exist some time-dependent
metrics that measure the outcome of attack-defense in-
teractions at every time point t [6], [7], [8], [9], [10],
[11]. However, our proposed metric framework takes one
step beyond those time-dependent metrics because it is
capable of measuring, at time t, the system state at both
a past time t(cid:48) (where t(cid:48) < t) and a future time t(cid:48)(cid:48) (where
t(cid:48)(cid:48) > t). This means that the framework can be used
for retrospective security analysis (e.g., identifying what
a defender did right or wrong in the past). In contrast,
the time-dependent metrics [6], [7], [8], [9], [10], [11]
only measure the defense effectiveness at t when the
measurement is made at t.

• Transformation of static metrics into dynamic metrics:
We transform conventional static metrics into dynamic
metrics to measure attack and defense effectiveness at
each evolution generation. For example, static metrics
(e.g., false-positive or false-negative rate) can be trans-
formed into dynamic metrics to capture a dynamic se-
quence of evolution generations by attackers or defenders.
Thus, the framework can be viewed as a “compiler” that
transforms static metrics into dynamic metrics.

• Validation of the proposed metric framework using
real datasets: In order to validate the framework, we
apply it to analyze two real-world datasets, one is the
network trafﬁc collected at a honeypot instrument [12]
and the other corresponds to the DEFCON Capture The
Flag (CTF) exercise [13]. We use the Snort intrusion
detection system (IDS) [14] as a defense mechanism,

 
 
 
 
 
 
whose detection capability are frequently updated (re-
ﬂecting defense generations that may or may not be
caused by speciﬁc attack evolution generations).

In the present study we focus on proposing a systematic
framework with clear deﬁnitions of cyber agility metrics.
Although our case study is limited by the datasets we have
access to, we hope researchers having access to semantically
richer datasets can applying our framework to their datasets
to draw deeper insights towards taming cyber agility.

The remainder of the paper is organized as follows. Sec-
tion II discusses the background and related prior studies. Sec-
tion III presents the proposed metric framework. Section IV
discusses the insights obtained from Section III. Section V
presents the case study on using the framework to analyze two
real datasets. Section VI discusses limitations of this study and
future research directions. Section VII concludes this paper.

II. BACKGROUND & RELATED WORK

A. Concept of Agility and Agility Metrics

Agility has been recognized as one of the key system
metrics, but has been studied only in an ad-hoc manner [1],
[15]. It has been investigated in multiple domains [16], [17],
[18], [19], [20], [21], [22]. In an enterprise system, agility is
deﬁned as the ability to deal with sudden changes in an envi-
ronment (e.g., the latency of response to sudden or unexpected
changes [1], [16], [17], [18], [22]). In the systems engineering
domain, agility measures a system’s capability of the reactive
or proactive response to sudden environmental changes [23]. In
military settings, agility refers to the ability an entity takes an
effective action under dynamic, unexpected environments that
may threaten the system’s sustainability [19]. For example,
the qualitative notion of agility quotient is proposed to ac-
commodate six attributes, including responsiveness, versatility,
ﬂexibility, resilience, adaptiveness, and innovativeness [21].
In the cybersecurity domain, agility often refers to reasoned
changes to a system or environment in response to functional
or security needs, but has not been paid due attention until very
recently [15], [1], [2], [3], [4], [5]. While it would be intuitive
to understand agility as how fast (e.g., response ability [23])
and how effective a system can adapt its conﬁguration to un-
expected attacks against it (e.g., considering the cost incurred
by the degraded system performance or the cost incurred by
response actions [1], [24]), the concept of agility is, like other
metrics, elusive to formalize. Indeed, there is no rigorous or
quantitative deﬁnition of agility in the cybersecurity domain
and existing attempts to model agility have not been able
to produce concrete measurements [2], [25], [26]. Despite
the apparent importance, systematic and quantitative agility
metrics have not been studied and understood in-depth.

In this paper, we tackle the problem of measuring agility
in the cybersecurity domain and propose an agility metric
framework based on evolution generations of cyber attacks
and defenses. To the best of our knowledge, this is the ﬁrst
systematic metric framework for understanding and measuring
cyber agility. Our framework is both general and ﬂexible
because it can accommodate attack and defense evolution
generations that may or may not be incurred by speciﬁc
opponent evolution generations.

2

B. Dynamic Security Metrics vs. Agility Metrics

The importance of security metrics and the challenges
of developing useful security metrics have been recognized
by security communities [27], [28], [29], [30], [31]. Most
metrics proposed in the literature are static in nature because
they are often deﬁned without considering dynamics over
time [4], [5], [32]. This implies that static metrics can capture
either a system’s snapshot at a particular time or a system’s
overall behavior/state for a period of time; this static view
can easily overlook the evolution of attacks and/or defenses
over the time horizon. For example, the effectiveness of anti-
malware tools (e.g., the false positive rate or false negative
rate) is often measured based on malware samples collected
during a period of time (e.g., one year), while ignoring their
instantaneous evolution over time. Taking one step further
from static metrics, time-dependent security metrics have been
studied to characterize and quantify system states at different
times, such as the proportion of compromised computers in
a network [6], [7], [8], [9], [10], [11], [33], [34], [35], [36],
[37], [38], [39], [40], [41], [42].

The dynamic metrics proposed in this paper aim to capture
a system’s state, covering both a previous time t(cid:48) and a future
time t(cid:48)(cid:48), when a measurement is made at t where t(cid:48) < t < t(cid:48)(cid:48).
On the other hand, the time-dependent metrics measure the
system’s current state at time t, as mentioned in Section I-A.
For example, the effectiveness of an IDS can be characterized
by its true-positive rate and/or false-negative rate, which may
not stay the same over time because its decision engine (e.g.,
the rule set in the case of the Snort) is frequently updated.

There have been other proposals for dynamic security
metrics [4], including (i) metrics for measuring the strength of
preventive defense (e.g., reaction time between the observation
of an adversarial entity at time t and the blacklisting of the
adversarial entity at time t(cid:48) [43]); (ii) metrics for measuring
the strength of reactive defense (e.g., detection time between a
compromised computer starting to wage attacks at times t and
t(cid:48) at which the attack is ﬁrst observed by some cyber defense
instrument [44], [45]); (iii) metrics for measuring the strength
of overall defense (e.g., penetration resistance for measuring
the level of effort that is imposed on a red team in order to
penetrate into a cyber system [46], [47]); and (iv) metrics for
measuring and forecasting cyber threats and incidents [48],
[49], [50], [51], [52], [53]. Although these metrics are related
to time, they are geared towards individual security events. In
contrast, our framework is systematic and correlates attack-
defense over the horizon of time.

III. THE METRICS FRAMEWORK

The proposed framework aims to deﬁne metrics to measure
the effectiveness of attack and defense generations during the
course of attack-defense interactions.

A. Guiding Principles

The framework is designed under the following principles:
• Leveraging static security metrics: Although most ex-
isting security metrics measure the static aspects of a

system’s security, some of them are well deﬁned and
commonly accepted, such as detection errors (e.g., false-
positives or false-negatives) for an IDS or anti-malware
system. The framework aims to accommodate static met-
rics that were deﬁned in the past or may be deﬁned in
the future.

• Considering the evolution of both attack and defense
behaviors: The framework aims to understand and im-
prove defenders’ evolution over the course of time. To this
end, we will answer the following research questions:

– To what extent is an attacker or defender evolving

based on its opponent’s new strategy?

– Which evolutionary strategy is more effective in
terms of an attacker’s or defender’s perspective?
– Which party (i.e., an attacker or defender) is more
active over the course of attack-defense interactions?
• Identifying the core metrics to measure systems se-
curity: Since the effectiveness of attack and defense
generations may not be adequately reﬂected by a single
metric, we consider a suite of metrics that measure
evolution generations from multiple perspectives. These
metrics may be then aggregated using an appropriate
method (e.g., a weighted average).

• Coping with new or zero-day attacks: Current defenses
have a very limited power in detecting new or zero-day
attacks. For a clear understanding on the effectiveness of
attack and defense evolutions, we measure generations
by tracing recorded network trafﬁc and/or computer exe-
cution. This allows us to characterize defense failures in
retrospect.

B. Representation of Attacks and Defenses over Time

In this paper, the term “target system” is used to represent
a range of systems, from a single computer (or device) to an
enterprise network or a cloud. A target system is defended
by human defenders who can use a variety of defense tools.
Therefore, “attackers" and “defenders" refer to either humans,
automated systems, or a combination of them (depending on
the practice). The term “attack and defense generations” is
used to refer to the atomic evolution of behaviors by attackers
and defenders.

Speciﬁcally, we consider time horizon t ∈ [0, T ], where T
can be inﬁnite in theory (i.e., T = ∞) but is often ﬁnite in
practice (i.e., T < ∞). Since most metrics are often measured
at discrete times (e.g., daily or hourly), we consider discrete-
time over t = 0, 1, . . . , T . That is, we treat each generation as
if it happens at an instant time in the beginning of each time
unit (e.g., day or hour). In practice, it is possible that attacks
and defenses are respectively observed over time intervals, say,
[t1, t2] and [t(cid:48)
2. In this case,
we can treat min(t1, t(cid:48)
2) as time T .
We use the term “defense" at discrete time t to refer to the
defense tool and the human defender(s) employed at time t.
This is important because defense tools may be updated with
newer versions and human defenders may join/leave a defense
team at any point in time. Such a change in defense produces
a new defense “generation”. We denote the defense at time

1) as time 0 and max(t2, t(cid:48)

2], where t1 (cid:54)= t(cid:48)

1 and t2 (cid:54)= t(cid:48)

1, t(cid:48)

3

t by Dt , where D0, D1, . . . , DT represent the evolution of
the defense over time t = 0, 1, . . . , T . Note that Dt = Dt+1
indicates that the defense at time t and the defense at time
t + 1 are the same and therefore belong to the same defense
generation. Note also that Dt can be described by a nominal
scale, such as a version number for a defense tool or an action
by a human defender. In this way, we can check whether or not
the defense at time t belongs to a different generation than the
defense at time t + 1. For example, suppose a target system’s
defense is started at t = 0 (e.g., Jan. 1). Suppose the time unit
is a day and the defense at t = 0 consists of a human defender
and an attack-detection tool with version 10.1.2. Suppose the
attack-detection tool is updated on the ﬁrst day of each month.
This means that D0 = D1 = . . . = D30 (cid:54)= D31, where D31
refers to the same human defender but the attack-detection
tool is version (say) 10.1.3 on Feb. 1.

Similarly, we use the term “attack” to describe attack tools,
attack tactics, or attack vectors as well as the human attackers
exploiting these attack tools, attack tactics, or attack vectors.
Let At denote an attack against a target system at time t,
and A0, A1, . . . , AT represent the evolution of the attack over
t = 0, 1, . . . , T . Note also that At can be measured by a
nominal scale. Hence, we can detect if two attacks performed
at two different points in time belong to the same generation.
Remark. In the discrete-time model, attack and defense
generations are assumed to evolve at discrete, deterministic
time points. In practice, generations can evolve over stochastic
time, implying that the time resolution should be sufﬁciently
small. When the monitoring time interval is inﬁnitely small, a
continuous-time model should be used. In reality, the highest
time resolution is what can be measured by a computer clock.
The investigation on whether to consider a continuous-time
model or not is beyond the scope of this paper.

C. Representation of Defense Effectiveness over Time

The effectiveness of defense Dt against attack At at time t ∈
[0, T ] is measured by some static metrics (e.g., false-positive
rate or false-negative rate). A metric M is a mathematical
function that maps the target system (or a particular property
or attribute of the target system) at time t to a value in a range
(e.g., false-positive rate) [4]. In order to make the presentation
succinct, we assume that the range of any metric M ∈ M can
be normalized to [0, 1], where a larger value is more desirable
from a defender’s point of view (e.g., a larger value means
a higher level of security). Some metrics with an opposite
meaning (e.g., a smaller false-positive rate, denoted by f p, is
better) can be adjusted to be consistent with the scale in M
(e.g., using 1 − f p instead of f p).

Let M denote the universe of static metrics,

including
both existing metrics and metrics that are yet to be deﬁned.
For a metric M ∈ M, we use Dt(At, M ) to denote the
effectiveness of defense Dt against attack At at time t ∈ [0, T ]
in terms of metric M . By considering metric M ∈ M over
time t = 0, 1, . . . , T , we obtain a sequence of effectiveness
measurements, which can be leveraged to deﬁne cyber defense
agility as described below.

Table I summarizes the main notations used in this paper.

TABLE I
SUMMARY OF KEY NOTATIONS AND THEIR MEANINGS.

Notation
[0, T ]
Dt
At(cid:48)
X
(cid:96) (X )
M

Dt(At(cid:48) , M )

GT(D, t)
GT(A, t(cid:48))
EGT(D, t)
EGT(A, t(cid:48))
TT(D, t)
TT(A, t(cid:48))
LBT(X )
EE(D, t)
EE(A, t(cid:48))
RGI(X )
AGI(X )

Description
Time horizon, t ∈ {0, 1, . . . , T }
Defense at time t ∈ [0, T ]
Attack at time t(cid:48) ∈ [0, T ]
Attacker or defender
Number of generations made by X during [0, T ]
Universe of static security metrics for measuring de-
fense effectiveness, scaled in [0, 1]
Effectiveness of defense Dt at time t against attack At(cid:48)
at time t(cid:48) in terms of metric M ∈ M
Defender’s Generation-Time at time t
Attacker’s Generation-Time at time t(cid:48)
Defender’s Effective-Generation-Time at time t
Attacker’s Effective-Generation-Time at time t(cid:48)
Defender’s Triggering-Time at time t
Attacker’s Triggering-Time at time t(cid:48)
Lagging-Behind-Time of X
Defender’s Evolutionary-Effectiveness at time t
Attacker’s Evolutionary-Effectiveness at time t(cid:48)
Relative-Generational-Impact of X
Aggregated-Generational-Impact of X

D. Example Scenario

Fig. 1 shows an example used throughout the rest of this
section. Fig. 1 (a) illustrates that a defender evolved (e.g.,
updated a security software version) at t = 0, 3 and 4 while
making no changes at t = 1, 2, 5 and 6. An attacker evolved
(e.g., changed an attack strategy) at t = 0, 4 and 6 while
making no changes at t = 1, 2, 3 and 5. Fig. 1 (b) views
attack-defense generations at the same time scale axis, by
demonstrating how the attacker’s evolution times may or may
not coincide with the defender’s evolution times.

(a) Attack generations vs. defense generations: A solid-unﬁlled
circle indicates the start of a defense generation, a solid-ﬁlled
circle indicates the start of an attack generation, and a dashed-
unﬁlled circle indicates no change by the defender or attacker.

(b) Attack-defense generations: ‘⊗’ indicates that both defense
and attack evolve to new generations, a solid-unﬁlled circle
indicates a change in defense generation but not
in attack
generation, a solid-ﬁlled circle indicates a change in the attack
but no change in the defense, and a dashed-unﬁlled circle
indicates no change in both defense and attack.

Fig. 1. An example of attack and defense generations over t ∈ [0, 6].

E. Overview of Metrics

At a high level, we consider two dimensions of evolution:
timeliness and effectiveness. Timeliness reﬂects the time it

4

takes to evolve new generations while effectiveness reﬂects
timeliness-oriented
impacts of these generations. However,
metrics can use effectiveness as a reference, and effectiveness-
oriented metrics can use time as a reference. Fig. 2 sum-
marizes these metrics and the structural relationship between
them.

1) Timeliness-Oriented Metrics: This suite of metrics
measures how quickly one adversarial party (i.e., an attacker or
defender) evolves its strategies with or without considering the
resulting effectiveness. This suite contains 4 metrics, which are
equally applicable to both an attacker and defender, leading to
8 metrics in total. The 4 metrics are as follows:

• Generation-Time (GT) measures the time between two
consecutive generations of strategies that are observed by
the measuring party (i.e., an attacker or defender).

• Effective-Generation-Time (EGT) measures the time it
takes for a party to evolve a generation which indeed
increases the effectiveness against the opponent.

• Triggering-Time (TT) measures the length of time since
the opponent’s reference generation that (if observed)
may have triggered a particular generation.

• Lagging-Behind-Time (LBT) measures how long a party
lags behind its opponent with respect to a reference time.
These 4 metrics are random variables in nature, sampled over
the time horizon [0, T ].

2) Effectiveness-Oriented Metrics: This suite of metrics
measures the effectiveness of generations over the course of
the evolution. This suite contains 3 metrics, which are equally
applicable to both an attacker and defender,
leading to 6
metrics in total. The 3 metrics are as follows:

• Evolutionary-Effectiveness (EE) measures the overall ef-
fectiveness of generations with respect to the opponent’s
generation. This is a random variable over t ∈ [0, T ].
• Relative-Generational-Impact (RGI) measures the effec-
tiveness gained by generation i over that of generation
i − 1.

• Aggregated-Generational-Impact

(AGI) measures

the

gain in the effectiveness of all generations t ∈ [0, T ].
3) Relationship between the Metrics: Fig. 3 systematizes
the relationship between the aforementioned 14 metrics. These
metrics are organized in two dimensions: time (x-axis) and
effectiveness (y-axis). The metrics in the upper half of the
plane (i.e., y > 0) represent the defender’s perspective; the
metrics in the lower half of the plane (i.e., y < 0) represent
the attacker’s perspective. The metrics in the right-hand half
of the plane (i.e., x > 0) look forward in time based on some
reference point; the metrics in the left-hand half of the plane
(i.e., x < 0) look backward in time based on some reference
point.

A metric closer to the x-axis indicates a more time-oriented
perspective, while the other metrics are more oriented to-
wards an effectiveness perspective. A metric on the x-axis
is deﬁned primarily based on the time dimension, including
GT and LBT. Effectiveness-oriented metrics are RGI and
AGI, where AGI(D) = −AGI(A). The metrics deﬁned from
the defender’s perspective and the metrics deﬁned from the
attacker’s perspective are symmetric across the x-axis. EE(D)

time04312Defense generations at time t=0,3,4Attack generations at time t=0,4,665time04312D3(A3,M) D4(A4,M) 655

Fig. 2. Overview of metrics with respect to attackers and defenders, where RGI(D) = −RGI(A) and AGI(D) = −AGI(A) but this kind of relationship
does not apply to the other metrics.

a random variable because generations often evolve based on
some stochastic events.

Defenders’ GT, denoted by GT(D): Suppose the defense
is evolved at t0 = 0, t1, . . . , t(cid:96) ≤ T , namely {t0, t1, . . . , t(cid:96)} ⊆
[0, T ]. The defender’s GT, namely random variable GT(D), is
sampled by GT(D, i)’s, where

GT(D, i) = ti+1 − ti

for

i = 0, 1, . . . , (cid:96) − 1.

(1)

This implies that Dti+∆t = Dti for any ∆t < ti+1 − ti and
Dti+GT(D,i) = Dti+1 (cid:54)= Dti because the defense is not evolved
until time ti+1. Consider the example in Fig. 1 (a), where the
defense generations evolve at t = 0, 3 and 4, meaning t0 = 0,
t1 = 3, t2 = 4, D0 = D1 = D2, and D4 = D5 = D6.
Therefore, the defender’s GT is a random variable sampled
by GT(D, 0) = t1 − t0 = 3 and GT(D, 1) = t2 − t1 = 1 in
this toy example.

Fig. 3. The structural relationship between the aforementioned 14 metrics
according to the time and effectiveness dimensions. Intuitively, some metrics
focus on time (x-axis) and look either forward or backward in time, while
others are more oriented towards effectiveness (y-axis), where the defender’s
metrics are reﬂected across the x-axis to create the attacker’s perspective in
dashed lines. Note that EE(D) is deﬁned in Quadrants 1 and 3 and EE(A) is
deﬁned in Quadrants 2 and 4, while the other metrics are deﬁned in a single
Quadrant.

and EE(A) are deﬁned over the entire plane because they look
both backward and forward in time.

F. Timeliness-Oriented Metrics

1) Generation-Time (GT): This metric measures the time
it takes for a party to evolve its strategy, which may or may
not be induced by the opponent’s evolution in strategy. GT is

1, . . . , t(cid:48)

0 = 0, t(cid:48)

Attackers’ GT, denoted by GT(A): Suppose the attack
evolves at t(cid:48)
k} ⊆
[0, T ], where notation t(cid:48) (rather than t) is meant to further high-
light the perspective of the attacker’s. Then, the attacker’s GT,
namely random variable GT(A), is sampled by GT(A, j)’s,
where

k ≤ T , namely {t(cid:48)

1, . . . , t(cid:48)

0, t(cid:48)

GT(A, j) = t(cid:48)

j+1 − t(cid:48)
j

for j = 0, 1, . . . , k − 1.

(2)

j

j+1

j+1−t(cid:48)

for any ∆t < t(cid:48)

j +∆t = At(cid:48)
(cid:54)= At(cid:48)

This means that At(cid:48)
j and that
At(cid:48)
j +GT(A,j) = At(cid:48)
. Consider the example illustrated
in Fig. 1 (a), where the defense is evolved at t(cid:48) = 0, 4 and 6.
This means t(cid:48)
2 = 6, A0 = A1 = A2 = A3,
and A4 = A5. The attacker’s GT is a random variable sampled
by GT(A, 0) = t(cid:48)
1 = 2 in
this toy example.

0 = 4 and GT(A, 1) = t(cid:48)

0 = 0, t(cid:48)

1 = 4, t(cid:48)

1 − t(cid:48)

2 − t(cid:48)

j

Summarizing the preceding discussion, we have:

Timeliness-centric metricsDefender’s Generation-Time :  GT(D)Attacker’s Generation-Time :  GT(A)Evolution metricsRelative-Generational-Impact (RGI)Aggregated-Generational-Impact (AGI)Defender’s Triggering-Time: TT(D)Defender’s Effective-Generation-Time :  EGT(D)Effectiveness-centric metricsAttacker’s Effective-Generation-Time : EGT(A)Attacker’s Triggering-Time: TT(A)Defender’s Lagging-Behind-Time: LBT(D)Attacker’s Lagging-Behind-Time: LBT(A)Defender’s Evolutionary-Effectiveness: EE(D)Attacker’s Evolutionary-Effectiveness: EE(A)Generation-Time (GT)Effective Generation-Time (EGT)Triggering-Time (TT)Lagging-Behind-Time (LBT)Evolutionary-Effectiveness (EE)Defender’s Relative-Generational-Impact: RGI(D)Defender’s Aggregated-Generational-Impact: AGI(D)Attacker’s Relative-Generational-Impact : RGI(A)Attacker’s Aggregated-Generational-Impact: AGI(A)Deﬁnition 1: (GT) The defender’s GT is deﬁned as random

variable GT(D), sampled by:

GT(D, 0) = t1 − t0, . . . , GT(D, (cid:96) − 1) = t(cid:96) − t(cid:96)−1,

(3)

where t0 = 0, t1, . . . , t(cid:96) ≤ T are the sequence of points in
time the defense evolves. The attacker’s GT is deﬁned as a
random variable GT(A), sampled by:

GT(A, 0) = t(cid:48)

0, . . . , GT(A, (cid:96) − 1) = t(cid:48)

(cid:96) − t(cid:48)

(cid:96)−1,

(4)

1 − t(cid:48)
1, . . . , t(cid:48)

where t(cid:48)
to which the attack evolves.

0 = 0, t(cid:48)

k ≤ T are the sequence of time points

2) Effective-Generation-Time (EGT): This metric con-
siders the attack-defense generations as a whole by measuring
the time to make effective generations by an attacker or
defender. Note that EGT is different from GT because the latter
only focuses on timeliness. Moreover, GT may not be able to
reveal a relationship with respect to the opponent’s generations
because not every generation is the result of adversarial action.
Measuring EGT will allow a party to characterize in retrospect
the effectiveness of its strategies over the time horizon.

Defenders’ EGT, denoted by EGT(D): Suppose de-
fense generations evolve at t0 = 0, t1, . . . , t(cid:96) ≤ T , namely
{t0, t1, . . . , t(cid:96)} ⊆ [0, T ]. The defender’s EGT is a random
variable, denoted by EGT(D), because the evolution of de-
fense generations are stochastic in nature. The random variable
EGT(D) is sampled by EGT(D, i)’s for i = 0, . . . , (cid:96) − 1 such
that Dti+EGT(D,i) is the nearest future generation that leads
to a higher than Dti(Ati, M ) defense effectiveness. Formally,
EGT(D, i) is deﬁned as

EGT(D, i) = ti∗ − ti

when there exists some ti∗ ∈ {ti+1, . . . , t(cid:96)} such that

Dti+∆t(Ati, M ) ≤ Dti (Ati, M )
for any 0 < ∆t < EGT(D, i)

(5)

(6)

and

Dti+EGT(D,i)(Ati, M ) = Dti∗ (Ati, M ) > Dti(Ati, M ); (7)

otherwise, we deﬁne EGT(D, i) = ∞, indicating that from the
defender’s perspective, no further effective defense generation
is made against attack Ati after ti.

Let us continue to use the example in Fig. 1, where the
three defense generations are respectively evolved at t0 = 0,
t1 = 3, and t2 = 4 < T = 6. Fig. 4 (a) describes
the defense effectiveness of Dt(A0, M ) for t = 0, 1, 2, 3, 4.
Since D3(A0, M ) < D0(A0, M ), the defense generation at
t = 3 is not effective. Since D4(A0, M ) > D0(A0, M ),
the defense generation at t = 4 is effective. Therefore,
we have EGT(D, 0) = t2 − t0 = 4 > GT(D, 0) = 3.
Moreover, suppose D4(A3, M ) > D3(A3, M ), meaning that
time t2 = 4 is more effective
the generation evolved at
than the previous generation evolved at time t1 = 3. Then,
we have EGT(Dt1 , 1) = t2 − t1 = 1; otherwise, we have
EGT(Dt1, 1) = ∞, indicating that no more effective defense
evolution is made against attack At1 after time t1 = 3.

Attackers’ EGT, denoted by EGT(A): Suppose the at-
0 = 0, t(cid:48)
k ≤ T, namely

tack generation evolve at t(cid:48)

1, . . . , t(cid:48)

6

(a) Defense generations at time t = 0, 3, 4, where GT(D, 0) = 3 and
EGT(D, 0) = 4 as discussed in the text.

(b) Attack generations at t = 0, 4, 6, where GT(A, 0) = 4 and
EGT(A, 0) = 4 as discussed in the text.

Fig. 4. GT (Generation-Time) vs. EGT (Effective-Generation-Time).

0, t(cid:48)

1, . . . , t(cid:48)

{t(cid:48)
k} ⊆ [0, T ]. The attacker’s EGT is deﬁned as
a random variable EGT(A) because the evolution events
are stochastic. The random variable EGT(A) is sampled by
EGT(A, j)
j +EGT(D,j)
is the nearest-future defense generation that leads to defense
effectiveness that is smaller than Dt(cid:48)
, M ) in terms of a
metric M ∈ M. Formally, EGT(A, j) is deﬁned as

j = 0, . . . , k − 1 such that Dt(cid:48)

(At(cid:48)

for

j

j

EGT(A, j) = t(cid:48)

j∗ − t(cid:48)
j
k} such that
, M )
(At(cid:48)

j+1, . . . , t(cid:48)

j∗ ∈ {t(cid:48)
j +∆t, M ) ≥ Dt(cid:48)

(At(cid:48)
for any 0 < ∆t < EGT(A, j)

j

j

when there exists t(cid:48)

Dt(cid:48)

j

and

(8)

(9)

, M );

Dt(cid:48)

j

(At(cid:48)

j +EGT(A,j), M ) = Dt(cid:48)

j

(At(cid:48)

j∗ , M ) < Dt(cid:48)

j

(At(cid:48)

j

j

(10)
otherwise, we deﬁne EGT(A, j) = ∞, meaning that no
effective attack generation is evolved against Dt(cid:48)

at t(cid:48)
j.

0 = 0, t(cid:48)

1 = 4, and t(cid:48)

the attack generation at

Let us continue to use the example described in Fig. 1,
where the three attack generations are respectively evolved at
time t(cid:48)
2 = T = 6. Fig. 4 (b) describes
the defense effectiveness of D0(At(cid:48)) for t(cid:48) = 0, . . . , 6. Since
D0(A4, M ) > D0(A0, M ),
time
t = 4 is not effective from the attacker’s perspective. Since
D0(A6, M ) < D0(A0, M ), the attack generation at time t = 6
to D0 is effective from the attacker’s perspective. Therefore,
we have EGT(A, 0) = t(cid:48)

2 − t(cid:48)
0 = 6 > GT(A, 0) = 4.
Summarizing the preceding discussion, we have:
Deﬁnition 2: (EGT) Let M be the universe of metrics mea-
suring static defense effectiveness discussed above. Suppose
the defense generation evolve at t0 = 0, . . . , t(cid:96) where t(cid:96) ≤ T .
The defender’s EGT is deﬁned as a random variable EGT(D)
sampled by the EGT(D, i)’s, where i = 0, 1, . . . , (cid:96) − 1, that
satisfy conditions in Eqs. (6) and (7).

attack

the
1, . . . , t(cid:48)

at
generations
Suppose
t(cid:48)
0 = 0, t(cid:48)
k ≤ T . The attacker’s EGT is deﬁned
as a random variable EGT(A) sampled by the EGT(A, j)’s,
where j = 0, 1, . . . , k − 1, that satisfy conditions in Eqs. (9)
and (10).

evolve

Remark. When comparing Deﬁnitions 1 and 2, we can

derive:

GT(D, t) ≤ EGT(D, t) and GT(A, t) ≤ EGT(A, t)

(11)

for any t. In Deﬁnition (2), it is possible that EGT(D, i) = ∞
for some i ∈ [0, . . . , (cid:96)−1], indicating that the attack generation
Ati occurred at time at ti is not addressed or countered by
any later defense generation. Similarly, EGT(A, j) = ∞ for
some j ∈ [0, . . . , k − 1] indicates that the defense generation
occurred at time t(cid:48)
Dt(cid:48)
j is not countered by the attacker in any
later attack generation.

j

3) Triggering-Time (TT): This metric aims to answer the
intuitive question “which generations may have caused or
triggered which of the opponent’s generations”. This would
offer valuable insights into the opponent’s operational process,
especially a sense of responsiveness. However, the measure-
ment result does not necessarily represent the causal triggering
of a given generation (e.g., a moving-target defense may not
be causally related to any speciﬁc attack but may be used to
increase the attacker’s attack effort or cost).

Defender’s TT, denoted by TT(D): Fig. 5 is obtained by
splitting Fig. 1 (a) into two pictures to explain how TT is
measured. Recall that defense generations are evolved at time
t = 0, 3, 4, but here it sufﬁces to consider only the two defense
generations at t = 0 and t = 3 as an example. In Fig. 5(a),
the defense generation at t = 3, namely D3, may be triggered
by some of the attack generations A0, A1, and A2 that have
been made by the attacker. We may deﬁne the triggering-
event of D3 as Aj for some j ∈ [0, 2] such that D3(Aj, M )
has the greatest positive change in defense effectiveness when
compared to D0(Aj, M ), where D0 is considered because
it represents the previous defense generation at t = 0, and
j ∈ [0, 2] is considered because A0, A1 and A2 represent
the entire history of attacks in the time horizon. Suppose
D3(Aj, M ) − D0(Aj, M ) is maximized for some j ∈ [0, 2],
suggesting that the defense generation may be triggered by the
attack generation Aj. This leads us to deﬁne the Triggering-
Time (TT) for defense generation D3 as 3 − j, which is
a particular sample, denoted by TT(D, 3), of the random
variable TT(D) that is deﬁned over the defense generations,
except the D0 (because every sample needs to have a previous
reference for comparison).

Attacker’s TT, denoted by TT(A): For attack generations
evolved at time t = 0 and 4, as shown in Fig. 5 (b), attack
generation A4 may be triggered by D0, D1, D2, or D3. We
may deﬁne the triggering-event of A4 as Dj for some j ∈
[0, 3] such that Dj(A4, M ) has the greatest negative change
in defense effectiveness when compared to Dj(A0, M ), where
j ∈ [0, 3] is considered because D0, D1, D2 D3 represent the
history of defense generations up to time t = 4, and A0
refers to the previous attack generation (prior to A4). Suppose
Dj(A4, M ) − Dj(A0, M ) < 0 is minimized (i.e., maximized
in its absolute value) at j, then we can deﬁne TT for attack

7

(a) Defense TT

(b) Attack TT

Fig. 5. TT (Triggering-Time) metric: A dashed arrow from generation X to
the opponent’s generation Y indicates that “X may have triggered Y .”

generation A4 as 4 − j, which is a particular sample, denoted
by TT(A, 4), of the random variable TT(A) that is deﬁned
over the attack generations.

1, . . . , t(cid:48)

0 = 0, t(cid:48)

Summarizing the preceding discussion, we have:
Deﬁnition 3: (TT) Suppose defense generations are evolved
at t0 = 0, t1, . . . , t(cid:96) ≤ T and attack generations are evolved
at t(cid:48)
k ≤ T . The triggering-event for defense
generation Dti, where i ∈ [1, (cid:96)] is deﬁned to be attack At(cid:48) that
leads to the greatest positive change in defense effectiveness
relative to Dti−1 in terms of metric M ∈ M, namely
t(cid:48) = arg max0≤t(cid:48)<tiDti(At(cid:48), M ) − Dti−1(At(cid:48), M ),
where Dti(At(cid:48), M ) > Dti−1(At(cid:48), M ).

If such t(cid:48) exists, we deﬁne TT(D, i) = ti − t(cid:48); otherwise, we
deﬁne TT(D, i) = ∞, meaning that the defense generation is
not triggered by any past attack within the time horizon. The
TT of defense generations is a random variable, denoted by
TT(D), that is sampled by TT(D, 1), . . . , TT(D, (cid:96)).

Similarly, the triggering-event for attack generation At(cid:48)
,
where j ∈ [1, k] is deﬁned to be defense Dt that leads to
the greatest negative change in defense effectiveness relative
to At(cid:48)

in terms of metric M ∈ M, namely

j

j−1

t = arg min0≤t<t(cid:48)

Dt(At(cid:48)
where Dt(At(cid:48)

j

j

, M ) − Dt(At(cid:48)
, M ) < Dt(At(cid:48)

j−1

j

, M ).

j−1

, M ),

(12)

If such t(cid:48) exists, we deﬁne TT(A, j) = t(cid:48)
j − t; otherwise, we
deﬁne TT(D, j) = ∞, meaning that the attack generation is
not triggered by any past defense within the time horizon.
The TT of attack generations is a random variable, denoted
TT(A), that is sampled by TT(A, 1), . . . , TT(A, k).

Remark. The preceding deﬁnition of TT can be adapted
in many ﬂavors. In the preceding deﬁnition, we propose using
the maximization of Dti (At(cid:48), M )−Dti−1(At(cid:48), M ) in Eq. (12)

time04312Defense generations at time t = 0,3,4Attack generations at time t = 0,4time04312Defense generations at time t = 0,3,4Attack generations at time t = 0,4as the criterion for identifying triggering event. Alternatively,
the deﬁnition can be adapted to maximize, for example,
Dti(At(cid:48), M ), meaning that defense Dti
is most effective
against attack A(t(cid:48)). In the use case where both parties’
evolution generations are completely known, the TT metric
reﬂects the responsiveness of a party. In the case the oppo-
nent’s evolution generations are not completely known (but
the party’s own evolution generations are naturally known),
the TT metric can be used to identify, in retrospect, a party’s
evolution generation that may be the result of non-adversarial
changes that have a security effect (e.g., new feature releases
or patching of vulnerabilities). Such a retrospective security
analysis is important because it helps the defender identify
effective defense activities that would not be noticed by the
defender otherwise. This is important because these possibly
unconscious defense decisions and can offer insights into
effective defense (e.g., best practice).

4) Lagging-Behind Time (LBT): This metric aims to
measure how far one party is behind its opponent. Although
professionals have often said that the defender lags behind the
attacker, we deﬁne this metric from both the defender’s and
attacker’s perspectives because they could provide ways for
proactive defenses ahead of actions by the attacker.

(a) Dt(At−1, M )

(b) Dt(At+1, M )

Fig. 6. Attacker and Defender LBT (Lagging-Behind Time).

Defender’s LBT, denoted by LBT(D): For a security
metric M ∈ M of interest,
let ε, where 0 ≤ ε ≤ 1,
represent the acceptable defense effectiveness. The LBT metric
considers Dt(At−λ, M ) for λ = 0, . . . , T and t ≥ λ. Fig. 6 (a)
illustrates Dt(At−λ, M ) for λ = 1. Then, we deﬁne LBT(D)
to be the minimum λ such that

Dt(At−λ, M ) ≥ ε for λ = 0, . . . , T and t ≥ λ

(13)

if such λ exists; otherwise, we deﬁne LBT(D) = −∞,
meaning that defenses lag behind attacks at least for time T .
In other words, we have

LBT(D) = min{λ : Dt(At−λ, M ) ≥ ε, λ = 0, . . . , T, t ≥ λ}.
(14)
Note that ε may vary from attack-defense settings (e.g.,
malware detection vs.
intrusion detection). Note also that
LBT(D) = 0 means that the defender always keeps pace with
the attacker.

8

Attacker’s LBT, denoted by LBT(A): For a security
metric M ∈ M of interest, recall that ε, where 0 ≤ ε ≤ 1,
represents the acceptable defense effectiveness. This metric
considers Dt(At+λ, M ) for λ = 0, . . . , T . Fig. 6 (b) illustrates
Dt(At+λ, M ) for λ = 1. Then, we deﬁne LBT(A) to be the
maximum λ such that

Dt(At+λ, M ) ≥ ε for λ = 0, . . . , T and t ≤ T − λ, (15)

if such λ exists; otherwise, we deﬁne LBT(A) = −∞,
meaning that attacks lags behind defenses at least for time
T . In other words, we have

LBT(A) = max{λ : Dt(At+λ, M ) ≥ ε,

where λ = 0, . . . , T,

t ≥ T − λ}.

(16)

Note that LBT(A) = 0 means that the attacker does not lag
behind the defender.

Summarizing the preceding discussion, we have:
Deﬁnition 4: LBT(D) is deﬁned by Eq. (14) with the
minimum λ that satisﬁes Eq. (13); LBT(A) is deﬁned by
Eq. (16) with the maximum λ that satisﬁes Eq. (15).

Remark. Deﬁnition 4 can be relaxed by adjusting Eq. (13)

such that:

1
T − λ + 1

T
(cid:88)

t=λ

Dt(At−λ, M ) ≥ ε.

(17)

This relaxation is to demand that the average defense effec-
tiveness is acceptable, rather than to demand that the defense
effectiveness is always acceptable. Note that LBT(D) and
LBT(A) are “dual” to each other only in the sense that
LBT(D) looks backward in time while LBT(A) looks forward
in time.

G. Effectiveness-Oriented Metrics

1) Evolutionary Effectiveness (EE): This metric measures
each generation with respect to a reference generation. This
is a random variable for each generation, sampled by the
opponent’s generations.

Deﬁnition 5: (EE) Suppose defense generations are evolved
at time t0 = 0, t1, . . . , t(cid:96) and attack generation are evolved at
time t(cid:48)
k. Defender’s EE is deﬁned as a random
variable, denoted by EE(D), which is sampled by EE(D, j)
for j ∈ [1, k], where

0 = 0, t(cid:48)

1, . . . , t(cid:48)

EE(D, j) =

1
T + 1

T
(cid:88)

[Dt(At(cid:48)

j

t=0

, M )].

(18)

With respect to a reference defense generation Dt, the at-
tacker’s EE is deﬁned as a random variable, denoted by
EE(A), which is sampled by EE(A, i) for i ∈ [1, (cid:96)], where

EE(A, i) =

1
T + 1

T
(cid:88)

t(cid:48)=0

[Dti(At(cid:48), M )].

(19)

Remark. Deﬁnition 5 can be adapted by replacing Eq. (18)

with, for example,

EE(D, j) =

1
T − t(cid:48)
j + 1

T
(cid:88)

t=t(cid:48)
j

[Dt(At(cid:48)

j

, M )].

(20)

2) Relative-Generational-Impact (RGI): As illustrated in
Fig. 7, we propose comparing Dt(At, M ) and Dt−1(At−1, M )
for t = 1, . . . , T . At a speciﬁc point in time t ∈ [1, T ], there
are three possible scenarios:
(a) When Dt(At, M ) = Dt−1(At−1, M ),

the attacker’s

maneuver and the defender’s maneuver at t are equal.
(b) When Dt(At, M ) > Dt−1(At−1, M ), the defender is

out-maneuvering the attacker at t.

(c) When Dt(At, M ) < Dt−1(At−1, M ), the attacker is out-

maneuvering the defender at t.

Deﬁnition 6: (RGI) Defender’s RGI is a random variable,
denoted by RGI(D) and sampled by RGI(D, t) for t =
1, . . . , T , where

RGI(t) = Dt(At, M ) − Dt−1(At−1, M ).

(21)

Note that unlike the metrics mentioned above, we omit
an attacker’s RGI because it would hold that RGI(A, t) =
−RGI(D, t) for t = 1, . . . , T .

9

Fig. 8. Justiﬁcation on using an area to deﬁne security gain Gs(i).

is deﬁned as t = 0, 1, . . . , T . As such, one may observe that
Gs(i) as shown in Eq. (22) does not have to be interpreted as
using the areas of triangles corresponding to individual time
intervals; instead, it can be interpreted directly as (cid:80)T
i=1 Gs(i)
while ignoring the constant 1
2 . Nevertheless, it has two ad-
vantages to use areas of the triangles to deﬁne security gain
Gs(i): (i) This deﬁnition remains equally applicable when the
time intervals do not have the same length, as illustrated in
Fig. 8; and (ii) when there is a need to interpolate a continuous
and smooth curve of Dt(At, M ) over [0, T ], such as the curve
f (x) in Fig. 8, we can divide the curve f (x) into z segments
such that within each segment, f (x) is strictly monotonic.
In the example of Fig. 8, f (x) is divided into 5 segments,
f1(x), . . . , fz(x). Accordingly, we can extend Deﬁnition 7 to
deﬁne AGI over [0, T ] as:

Fig. 7. Dt(At, M ) over [0, T ]: Defense generations are evolved at time
t = 0, 3, 4 while attack generations are evolved at time t = 0, 4, 6.

3) Aggregated-Generational-Impact (AGI): This metric
aims to measure the overall security gained by the defender
over time horizon [0, T ]. We propose measuring the security
gain over time interval [ti−1, ti], denoted by Gs(i), for i =
1, . . . , T . As Fig. 7 describes, we assume that a straight-line is
used to link (ti−1, Dti−1(Ati−1, M )) and (ti, Dti(Ati, M )).
Then, Gs(i) is deﬁned as the area of the triangle that depends
on the sign of [Dti (Ati , M )−Dti−1(Ati−1, M )]. More specif-
ically, we deﬁne

where

=

AGI(T ) =

1
T

z
(cid:88)

i=1

Gs(i)

(24)

(25)

fi(x)dx − (ti − ti−1)fi(ti−1)

Gs(i)

(cid:82) ti

ti−1
(ti − ti−1)fi(ti−1) − (cid:82) ti
ti−1
0



if f (cid:48)
fi(x)dx if f (cid:48)
if f (cid:48)

i (x) > 0
i (x) < 0
i (x) = 0.

IV. DISCUSSION

Gs(i)

1
2 [Dti(Ati, M ) − Dti−1(Ati−1, M )],

if Dti(Ati, M ) > Dti−1(Ati−1, M )

=

− 1

2 [Dti(Ati, M ) − Dti−1(Ati−1, M )],

(22)

if Dti(Ati, M ) < Dti−1(Ati−1, M )
if Dti(Ati, M ) = Dti−1(Ati−1, M ),




0

where ti − ti−1 = 1. This leads to:

Deﬁnition 7: (AGI) Defender’s AGI over [0, T ], denoted by

AGI(D), is deﬁned by

AGI(D) =

1
T

T
(cid:88)

i=1

Gs(i).

(23)

An attacker’s AGI is omitted because its deﬁnition is trivial
as AGI(A) = −AGI(D).
Remark. Note that

is
ti − ti−1 = 1 for i = 1, . . . , T because the time horizon

the length of each time interval

In this section, we discuss use cases of the proposed metric
framework. Since our goal is to help defenders, the discussion
below will be from a defender’s point of view. We focus on
two issues: the amount of attack evolution generations that are
observed by the defender; and the number of attackers vs. the
number of defenders.

A. Use Case with respect to the Amount of Attack Generations
Observed

We differentiate two scenarios: all vs. some or no attack

evolution generations being observed.

1) Use Cases When All Evolution Generations Are Ob-
served: This is the ideal case and is possible when considering
(for example) white-hat attack defense experiments over a
period of time. This use case is also possible for retrospective
attack-defense analysis. This can happen because some attacks
(e.g., new attacks or even zero-day attacks) that take place at

timesecurity metric M010431265triangle area as security gain in [2,3]triangle area as security loss in [5,6]timesecurity metric010f(x) represents overall curve over [0, t5]t1t2t3t4f1(x)f2(x)f3(x)f4(x)f5(x)t5time t may not be recognized until time t(cid:48), where t < t(cid:48). If
all cyber activities (e.g., network trafﬁc and host execution)
are properly recorded, the metrics deﬁned above can be used
to measure attack and defense evolution in retrospect. The
measurement results tell a defender about its agility and may
lead to insights into explaining why the defense failed and
how the failure may be ﬁxed in the future.

2) Use Case When Some or No Evolution Generations
Are Observed:
In real-world cyber attack-defense practice,
the defender may only observe some or no attack evolution
generations. In this case, the defender can identify “probable”
attack evolution generations as follows. First, the defender
treats each attack A0, . . . , AT at time t ∈ [0, T ] as an evo-
lution generation, despite that some attacks are not evolution
generations. Then, the defender can identify the attacks that
disrupt
the existing defense most as an approximation of
attack evolution generations (i.e., “probable” generations as
an approximation to the unknown “ground-truth” generations).
For example, this can be done as follows: Given a threshold
τ where 0 < τ < 1, attack At(cid:48) can be treated as an evolution
generation if there exists t, where 0 ≤ t < t(cid:48), such that

Dt(At, M ) − Dt(At(cid:48), M ) > τ.

(26)

As a result, the defender can identify the probable generations
and then use them measure the metrics proposed in the paper.
From a conceptual point of view, the preceding use case
is reminiscent of the ground truth in supervised machine
learning. In principle, the training data in supervised machine
learning should be 100% accurate or correct (e.g., 0% false-
positives and 0% false-negatives). In practice, this is hard
to achieve. Still, supervised machine learning is useful and
successful even if the ground truth of the training data is not
guaranteed. In this sense, the preceding method we propose
using to identify attack evolution generations approximately
might be as useful as in the case of supervised machine
learning with an approximate ground-truth.

B. Use Cases with respect to the Number of Attackers vs. the
Number of Defenders

There are 4 scenarios: one attacker against one defender;
one attacker against multiple defenders; multiple attackers
against one defender; and multiple attackers against multiple
defenders. Since the description in Section III focused on the
scenario of one attacker against one defender, in what follows
we discuss how the metrics framework can be used in the
other three scenarios.

1) Use Case with One Attacker against Multiple Defenders:
This scenario is interesting when evaluating the collective
effectiveness and failures of multiple defenders. From a de-
fender’s point of view, this is a straightforward extension to
the preceding “one attacker against one defender” case because
the defense generations that are evolved by each defender is
known. In order to measure the collective defense effectiveness
and failures, we can treat the collection of defenders as a single
virtual defender.

10

2) Use Case with Multiple Attackers against One Defender:
This scenario is perhaps what happens in the real world where
a defender (of an enterprise) needs to cope with multiple
attackers. In this scenario,
the multiple attackers can be
represented by a single virtual attacker. This makes sense when
the attackers are coordinated because the coordinator can be
seen as the attacker (while noting that this insight has been
widely used in cryptographic models). This treatment also
makes sense even if the attackers are not coordinated with each
other because in the real world each defender (of an enterprise
network, for example) is indeed likely dealing with multiple
attackers. In this case, the attacks waged by different attackers
can be superimposed over each other, leading to attack gener-
ations that are a superset of the generations that are evolved
by individual attackers. As a result, the metrics framework
is equally applicable in this scenario. This generality of the
framework can be attributed to the fact that agility is about
the attacks rather than the identities of the attackers.

We stress that the preceding discussion does not mean that
effort should not be made to distinguish the attackers. This
is because there is a spectrum of situations: at one end of
the spectrum, the defender cannot tell the attackers apart; at
the other end of the same spectrum, the defender can tell all
of the attackers apart. In the former case, the defender has
to treat all of the attackers as a single entity or coordinated
one. In the latter case, the defender can measure cyber agility
with respect to each recognized attacker, which allows the
defender to measure which attacker(s) are more agile than
the other attackers and therefore possibly prioritize defense
resources against these more agile attackers. Therefore, the
defender should always strive to distinguish the attacks waged
or coordinated by different attackers. In any case, our metrics
framework is equally applicable.

3) Use Case with Multiple Attackers against Multiple De-
fenders: Similarly, this case can be seen as a simple extension
to the case of “one attacker against multiple defenders” or the
case of “multiple attackers against one defenders.”

V. CASE STUDY

In this section, we show the results from our case study by

applying the proposed metrics to two real datasets.

A. Experimental Setup

1) Defense Tool: The case study is based on replaying
network trafﬁc, which contains attacks, against
the Snort
intrusion detection system [54]. We used six versions of Snort
(v2.9.4 - v.9.8) released for Dec. 2012 - Dec. 2016. For each
version (e.g., v2.9.4), there are sub-versions (e.g., v2.9.4.1).
Each subversion is counted as a defense generation, leading to
18 versions or defense generations, denoted by Dt0, . . . , Dt17.
These 18 generations are made for 1,294 days, meaning that
the time horizon for the defender is [t0, t17] = [1, 1294]. These
Snort versions are tested on Virtual Machines (VMs) using the
Ubuntu 14 operating system after making few changes on the
default settings in the snort.conf ﬁle to best ﬁt for this
experiment.

11

(a) Defender’s GT and EGT

(b) Defender’s TT

(c) Defender’s RGI

Fig. 9. Evolution metrics measured for the defender (i.e., Snort) with the Honeypot dataset (x-axis unit: day).

(a) Defender’s GT and EGT

(b) Defender’s LBT

(c) Defender’s EE

Fig. 10. Evolution metrics measured for the defender (i.e., Snort) with the DEFCON dataset (x-axis unit: day).

2) Datasets: The 18 versions of Snort are tested against

the following two datasets:

• Honeypots dataset: This dataset was collected at a
low-interaction honeypot of approximately 7,000 IP ad-
dresses. This dataset contains trafﬁc spanning Feb. 2013
- Dec. 2015. The low-interaction honeypot consists of
programs, including Dionaea, Mwcollector, Amun, and
Nepenthes. These programs simulate services (e.g., SMB,
NetBIOS, HTTP, MySQL, and SSH) to some extent.
The dataset was collected over 1,029 days, with a time
0, t(cid:48)
horizon for the attacker [t(cid:48)
17] = [80, 1029]. This is
important because many datasets do not include such a
length, especially in continuous (or mostly continuous)
collections. Due to the absence of well-deﬁned attack
generations, we treat the attacks at each time unit (days)
as an attack generation. This changes the measurement
of some metrics, namely LBT and EE. There are some
missing data during shorter time periods within this
time horizon because the data collection system was
occasionally shutdown due to various reasons. Although
the dataset is not ideal because the attacker’s and de-
fender’s time horizons are not exactly the same, which
often happens in practice, the metrics framework can be
appropriately applied by adjusting a different time span.
• DEFCON CTF dataset: DEFCON is one of the world’s
premiere hacker-type conferences. We use the publicly
available pcap ﬁles collected from DEFCON 21 (2013),
DEFCON 22 (2014) and DEFCON 23 (2015), each of

0 = 238, t(cid:48)

which corresponds to a single day. This dataset has well
deﬁned attack generations (one per year). Putting into
the terms of the metrics framework, the time horizon of
the attacker’s generations are at t(cid:48)
1 = 609,
and t(cid:48)
2 = 973. The dataset consists of pcaps from
20 teams. We randomly selected a single team’s data
for our experiment, and followed the team through all
three DEFCON CTFs. Although DEFCON datasets are
available for a number of years, we only consider these
three because these three years correspond to the period
of time during which Snort was considered as discussed
above. It is critical that these years are distinct, because
we are able to clearly see the difference between attack
generations; this is not the case in the aforementioned
honeypot dataset.

In our experiments, we replay the network trafﬁc against
Snort, log the alerts generated by Snort’s preprocessor and
detection engine, and calculate the true-positive rate, tp. This
is possible because the datasets are composed of malicious
trafﬁc. In terms of the notations used above, the static metric
M used in the notation Dt(At(cid:48), M ) is the true-positive rate
tp. It is worth mentioning that as in any data-driven study, the
insights derived from a speciﬁc dataset may not be arbitrarily
generalized. When not all attack generations are observed,
the resulting insights may be speciﬁc to the speciﬁcation of
generations used in the analysis in question.

20835857076492171558599147126050100150200250Generation Time (days)Evolution Date (t)GT(D,t)EGT(D,t)0100200300400500600700800Triggering Time (days)Evolution Date (t)TT(D,t)Worst-case TT(D,t)-30.00%-20.00%-10.00%0.00%10.00%20.00%30.00%Relative Generational Impact (ΔTP %)Evolution Date (t)4944169050100150200250300350400450500Generation Time (days)Evolution Date (t)GT(D,t)EGT(D,t)0%5%10%15%20%25%-2-1012True PositiveDefender Lag Amount (λ= years)Defense 1Defense 2Defense 3Averageε=12%0.00%5.00%10.00%15.00%20.00%25.00%30.00%Evolutionary EffectivenessDefense Generation (t)A₂₃₈A₆₀₉A₉₇₃Dt(A238)Dt(A609)Dt(A973)B. Case Study with the Honeypot Dataset

that

17] = [80, 1108]. This means that

Fig. 9 plots the measured evolution metrics for the Hon-
eypot dataset. Recall
the defender’s time horizon is
[t0, t17] = [1, 1294] and the attacker’s time horizon is
[t(cid:48)
0, t(cid:48)
there are missing
attack generations in the defender’s time horizon, for which
some metrics cannot be measured. We measure the proposed
metrics by focusing on a defender’s point of view. To be spe-
ciﬁc, we do not consider the lagging-behind time (LBT) and
evolutionary-effectiveness (EE) metrics because these require
both attack and defense generations to be well deﬁned. The
remainder of the metrics are well deﬁned, given the constraints
of this dataset.

Analysis on Fig. 9 (a): This ﬁgure plots the sample
of a defender’s Generation Time, GT(D), and the sample
of the defender’s Effective GT, EGT(D). Note that some
blue bars representing GT(D) are not accompanied by red
bars representing EGT(D) due to the missing attack trafﬁc
data for the corresponding dates. Graphing both GT(D) and
EGT(D) together allows us to see any disparity between these
the mean ratio of EGT(D)
two metrics. We observe that
to GT(D) is 3.00. This implies that effective generations
evolve 3x slower than normal generations. However, when we
investigate individual generations, some defense generations
are less responsive than others. For example, attack generation
A0 is not effectively responded to by a defense generation until
208 days, as shown in EGT(D, 0) = 208.

Analysis on Fig. 9 (b) : This ﬁgure shows the defender’s
Triggering-Time (TT) metric. The red curve indicates the
hypothetical worst-case scenario in that every defense gen-
eration is triggered by the very ﬁrst attack, At0 = A80.
The blue curve corresponds to TT(D, t) as sampled at t =
88, 123, 138, 208, 235, 284, 346, 501, 586, 685, and 825. Note
that TT(D, 88) and TT(D, 123) are not shown because they
are not obtainable from the dataset as they reach ∞, based on
their deﬁnition. Note that TT(D, t) for t > 825 is not plotted
because of missing attack trafﬁc data at t = 825. We observe
that most defense generations respond to relatively old attacks,
except TT(D, 235) = 65 and TT(D, 586) = 59, indicating that
these defense generations respond to attacks that are almost
two months old.

Analysis on Fig. 9 (c): This ﬁgure plots the RGI metric,
where there are some t’s at which RGI(D, t) is missing
because of missing attack trafﬁc. From Fig. 9 (c), we can
notice the true-positive rate is observed with a typical range
of ±10%. The aggregated generational effectiveness (AGI) of
the defender (i.e., Snort) is very poor at 0.01%, implying little
signiﬁcant evolution relative to the attacker during the given
times.

Results Analysis: Our results show that Snort has a history
of being responsive to attacks in evolving its defense in a
timely manner. However, the attackers also evolved, offsetting
the previous defense gains. This explains why AGI(D) ≈
0.01%, indicating that the Snort community is in a stalemate
with the attacker. We notice that the static defense effective-
ness metric, Dt(At, M ), is low with a mean of 8.11%. The
cause of the low effectiveness is the low-interaction nature of

12

the honeypot, which makes it not as semantically rich as we
would like.

In summary, the defender’s agility is comparable to the at-
tacker’s. In addition, a frequent evolution does not necessarily
result in an effective evolution. This means that the defender’s
agility cannot be strictly related to defense effectiveness in
overall. Therefore, we need to measure cyber agility separately
from defense effectiveness.

C. Case Study with DEFCON Dataset

Fig. 10 demonstrates the measurement of evolution metrics
when replaying the DEFCON dataset against Snort. Since we
have three distinct DEFCON CTF attack trafﬁc datasets (i.e.,
, for j = 0, 1, 2, where t(cid:48)
0 = 238,
one capture per year), At(cid:48)
t(cid:48)
1 = 609, and t(cid:48)
2 = 973, we can naturally treat each of them
as an attack generation.

j

Analysis on Fig. 10 (a): This ﬁgure plots the sample of
the defender’s GT and the sample of the defender’s EGT
corresponding to the data available, where each release of
Snort is treated as a generation. Since there are only three
attack generations where each generation serves as a reference
point for deﬁning the corresponding metric EGT(D, t(cid:48)
j) for
j = 0, 1 and 2, we have only three measurements of EGT(D, t)
for t = 238, 609, 973, showing only three red bars. Notice that
the blue bars are the same as in Fig. 9 (a). Defense evolution
actions responding to A238 and A958 took relatively less
generation time, showing 49 days and 69 days, respectively.
However, the defense evolution took more than one year (i.e.,
441 days) to respond to A609. This may be because of a small
set of samples to draw conclusions about the evolution rates
of these two parties.
Analysis on Fig. 10 (b): This ﬁgure exhibits a the defender’s
LBT. In Fig. 10 (b), the purple curve indicates the average
of LBT(D) samples, which are shown in green, red, and
blue. The security threshold, ε (shown in teal),
is set at
a true-positive rate of 12%; in general, ε is the minimum
acceptable value for the metric chosen (i.e. true-positive rate).
The average LBT(D) falls below ε at x = 1.14 years, meaning
that for the threshold chosen, the defender lags behind the
attacker by 1.14 years.
Analysis on Fig. 10 (c): This ﬁgure addresses the sample
of defender’s EE based on the three attack generations with
t(cid:48) = 238, 609, 973, resulting in three curves, each correspond-
ing to one of the three At(cid:48)’s. We expect that for a ﬁxed
reference attack At(cid:48), the defense effectiveness Dt(At(cid:48), M )
should increase over time. However, this is not universally
true because the Snort rules are not monotonically increasing
(i.e., some rules are occasionally deleted in order to reduce
false-positives or replace poorly written rules). This explains
why the effectiveness is not monotonically increasing against
each given attack generation.

Results Analysis: Based on the result in Fig. 10 (c), we
examine the alert types from DEFCON 22. We learn that
the attackers in DEFCON 22 primarily attacked protocols
for which the Snort Preprocessor Plugins had more reliable
effectiveness, such as HTTP Inspect pre-processors (relative
to those in DEFCON 21). The subsequent year, DEFCON 23,

shows the worst detection across all defensive generations.
Because this attack generation was more effective than the
corresponding defensive generations, we can conclude that the
attacker outmaneuvers the defenders (i.e., the Snort commu-
nity) in this case.

From the defender’s EE with reference to DEFCON 22, we
also observe that the EE for A609 (DEFCON 22) increases
sharply when defense D208 is released. In this case,
the
defender predicted the attack generation. One situation which
could cause this is that the defender (i.e., the Snort community)
saw a proof-of-concept exploit, and then evolved in order to
mitigate this exploit. Later, the exploit became widespread
in the wild, and was used by the attackers. However, the
exploit was not successful when the attackers ﬁnally adopted
it because the defender had already prepared for it. The exploit
eventually fell out of popularity, so it ceased to show up in
the next attack generation. This example would explain why
the change appears in DEFCON 22 data, but not DEFCON
21 or DEFCON 23 data.

In summary, Snort exhibits a lower

responsiveness to
human-launched attacks, which are presumably most prevalent
in DEFCON CTF competitions. However, the static effective-
ness, interpreted as a static metric sequence Dt(At, M ) for
t = 0, 1, . . . , T , is actually higher than its counterpart, the
dataset observed by the honeypot, which is more likely to
contain mostly automated attacks. This is partly because Snort
has some aspects of the proactive defense capability. From
these observations, cyber agility and static effectiveness need
to be separately investigated.

D. Discussion

For the honeypot dataset, the mean of Dt(At, tp) over the
time horizon is 8.11%. For the DEFCON dataset, the mean of
Dt(At, tp) over t ∈ {238, 609, 973} is 17.67%. This means
that Snort is not effective overall. However, we suspect that the
lower true-positive rate of Snort against the honeypot dataset
is largely because these low-interaction honeypots limit the
depth to which the attacker may penetrate. If the dataset were
collected by a high-interaction honeypot, the true-positive rate
would have been higher. Snort appears to be more effective
in detecting reconnaissance activities (mainly shown in the
honeypot dataset) than detecting exploitation activities (mainly
shown in the DEFCON CTF activities). This is why Snort
appears to evolve more efﬁciently against the attacks observed
by the honeypot.

In summary, according to the perspective of the static
effectiveness, Snort
is more effective in detecting attacks
largely launched by human attackers than detecting attacks
observed by honeypots. One caveat is that the attacks observed
by low-interaction honeypots are not semantically rich enough
because they capture only a limited interaction with attackers.
Defense generations appear to be more effective in offsetting
attacks observed by honeypots than offsetting attacks launched
by human attackers. Nevertheless, Snort exhibits a potential
proactive defense capability, as discussed earlier. Therefore,
cyber agility needs to be separately investigated from static
effectiveness. Another application of these metrics is the

13

following: an attacker may be interested in predicting how long
a zero-day attack will be usable before an effective defense
will be deployed. For example, the attacker can calculate the
expected EGT in the response to zero-day attacks. This also
helps the defender to estimate the attacker’s expectation of
EGT, which may be leveraged to launch advanced defense
(e.g., deception).

VI. LIMITATIONS

In this section, we discuss the limitations of our study.
Addressing these limitations will guide us to strengthen and
reﬁne the current metrics framework and other related metrics
research.

First, the metrics require the defender to record the network
trafﬁc and/or computer execution traces in order to measure
At(Dt(cid:48)) in retrospect, where t < t(cid:48), t = t(cid:48) or t > t(cid:48).
This may not always be feasible, especially for high speed
networks that generate a large volume of network trafﬁc or
complex applications that may incur concurrent executions.
Nevertheless, this appears to be the only way to measure the
response to new or zero-day attacks.

Second, the used datasets are not ideal because they lack
rich semantics (i.e., the low-interaction honeypot dataset) or
continuity over a long period of time (i.e., the DEFCON
dataset). Nevertheless, we are able to validate most metrics
using one dataset or the other (with the exception of LBT). Be-
cause the datasets complement each other, these experiments
sufﬁciently demonstrate the usefulness of the framework.

Third, the TT metric aims to correlate the effects of adver-
sarial actions between the two parties. In the datasets used, this
may not accurately represent the causality of their evolution.
For example, in the DEFCON dataset, Snort was not updated
during a competition, so evolution by the attacker may not
have been caused by a change in Snort’s rules. Nevertheless,
an approximation to the “ground-truth” evolution generation
may, as discussed in Section IV, be practically useful enough.
Fourth, our study represents only the ﬁrst step towards
the ultimate goal of measuring cyber agility. Even if many
limitations exist as mentioned above, our case study clearly
shows that a detection tool, like Snort, evolves effectively and
has proactive defense capability. These have not been studied
in the literature.

Fifth, the proposed framework contains a set of metrics that
may only capture some aspects of cyber attack and defense
evolution. The framework may require further investigation to
fully establish a sense of completeness, which is important
because any security metric of interest can be derived from a
complete set of metrics.

VII. CONCLUSION

We have presented a suite of metrics to measure cyber
agility by estimating the degree of attack and defense gen-
erational evolution. The proposed set of metrics includes
generation time, effective generation time,
triggering time
(from detection to perform an adaptive defense), evolution-
ary effectiveness, lagging behind time, relative generational
impact, and aggregated generational impact. These metrics

mainly focus on measuring the timeliness and effectiveness in
order to capture the core concepts of agility. We demonstrated
the measurement of these metrics using the two real-world
datasets (i.e., honeypots and DEFCON) which were tested
using Snort. We discussed the underlying meanings of these
metrics as well as their implications.

The metrics proposed in this paper can provide valuable
insights for defense strategization because they allow a de-
fender to measure several aspects of cyber agility. These
aspects include the defender’s own responsiveness to attacks
over time, the identiﬁcation of which defense changes have
been targeted by attackers, and the ongoing effectiveness of
the defender against a single or set of attackers. Insights
derived from these metrics will enable the defender to become
more responsive, more targeted and more effective in their
competition to outmaneuver attackers. As one example, we
mention that the ability to measure security effectiveness in
retrospect makes it possible to characterize why the defense
failed against new or zero-day attacks. This may lead to
insights into how the failure may be prevented in the future.
As another example, we mention that being able to tell the
attackers apart would allow the defender to use our metrics
to tell which attackers are more agile than others. This would
suggest the defender to prioritize the defense correspondingly
(e.g., paying special attention to the more agile attackers). As
yet another example, we mention that the defender can use our
metrics to tell which defense changes have been particularly
targeted by attackers. This would suggest the defender to use
more advanced defense techniques (e.g., deception) to offset
the attacker’s agility against these defense changes.

Acknowledgment. We thank the reviewers for their insightful
comments that guided us in improving the paper. For example,
the term generation was suggested to replace our original term
of adaptation because the former can more broadly accom-
modate attack and defense updates that are not necessarily
incurred by a speciﬁc opponent move, effectively making the
metric framework more widely applicable.

This research was supported in part by the US Department
of Defense (DoD) through the ofﬁce of the Assistant Secretary
of Defense for Research and Engineering (ASD (R&E)),
ARO Grant #W911NF-17-1-0566, ARL Grant #W911NF-17-
2-0127, and NSF Grant #1814825. The views and opinions of
the authors do not reﬂect those of the US DoD, ASD (R&E),
Air Force Research Laboratory, US Army, or NSF. Approved
for Public Release; Distribution Unlimited: 88ABW-2019-
1731 Dated 15 April 2019.

REFERENCES

[1] P. McDaniel, T. Jaeger, T. F. La Porta, N. Papernot, R. J. Walls,
A. Kott, L. Marvel, A. Swami, P. Mohapatra, S. V. Krishnamurthy, and
I. Neamtiu, “Security and science of agility,” in Proceedings of the First
ACM Workshop on Moving Target Defense, MTD’14, pp. 13–19, 2014.
[2] L. M. Marvel, S. Brown, I. Neamtiu, R. Harang, D. Harman, and
B. Henz, “A framework to evaluate cyber agility,” in Military Com-
munications Conference, MILCOM 2015-2015 IEEE, pp. 31–36, IEEE,
2015.

[3] J.-H. Cho, P. Hurley, and S. Xu, “Metrics and measurement of trustwor-
thy systems,” in IEEE Military Communication Conference (MILCOM
2016), 2016.

14

[4] M. Pendleton, R. Garcia-Lebron, J.-H. Cho, and S. Xu, “A survey on
systems security metrics,” ACM Comput. Surv., vol. 49, pp. 62:1–62:35,
Dec. 2016.

[5] J. Cho, S. Xu, P. Hurley, M. Mackay, T. Benjamin, and M. Beaumont,
“Stram: Measuring the trustworthiness of computer-based systems,”
ACM Comput. Surv. (accepted for publication), 2018.

[6] S. Xu, W. Lu, and L. Xu, “Push- and pull-based epidemic spreading in
arbitrary networks: Thresholds and deeper insights,” ACM Transactions
on Autonomous and Adaptive Systems (ACM TAAS), vol. 7, no. 3,
pp. 32:1–32:26, 2012.

[7] S. Xu, W. Lu, and Z. Zhan, “A stochastic model of multivirus dynamics,”
IEEE Transactions on Dependable and Secure Computing, vol. 9, no. 1,
pp. 30–45, 2012.

[8] M. Xu and S. Xu, “An extended stochastic model for quantitative
security analysis of networked systems,” Internet Mathematics, vol. 8,
no. 3, pp. 288–320, 2012.

[9] R. Zheng, W. Lu, and S. Xu, “Preventive and reactive cyber defense
dynamics is globally stable,” IEEE Trans. Network Science and Engi-
neering, vol. 5, no. 2, pp. 156–170, 2018.

[10] S. Xu, W. Lu, L. Xu, and Z. Zhan, “Adaptive epidemic dynamics in
networks: Thresholds and control,” ACM Transactions on Autonomous
and Adaptive Systems (ACM TAAS), vol. 8, no. 4, p. 19, 2014.

[11] S. Xu, W. Lu, and H. Li, “A stochastic model of active cyber defense
dynamics,” Internet Mathematics, vol. 11, no. 1, pp. 23–61, 2015.
[12] N. Provos, “A virtual honeypot framework,” in USENIX Security Sym-

posium, pp. 1–14, 2004.

[13] DEFCON, “Defcon ctf datasets.” https://media.defcon.org/, 2017.
[14] M. Roesch, “Snort — lightweight intrusion detection for networks,” in
Proceedings of the 13th USENIX conference on System administration
(LISA’99), pp. 229–238, 1999.

[15] D. S. Alberts, “Agility quotient (aq),” in Proceedings of the 19th Inter-
national Command anc Control Research and Technology Symposium,
ICCRTS’14, 2014.

[16] P. T. Kidd, Agile Manufacturing: Forging New Frontiers. MA: Addison-

Wesley, 1994.

[17] Y. Yusuf, M. Sarhadi, and A. Gunasekaran, “Agile manufacturing: the
drivers, concepts and attributes,” International Journal of Production
Economics, vol. 62, 1999.

[18] B. Sherehiy, W. Karwowski, and J. K. Layer, “A review of enterprise
agility: Concepts, frameworks, and attributes,” International Journal of
Industrial Ergonomics, vol. 7, pp. 445–460, 2007.

[19] D. S. Alberts, “Agility, focus, and convergence: The future of command
and control,” The International C2 Journal, vol. 1, no. 1, pp. 1–30, 2007.
[20] K. Conboy, “Agility from ﬁrst principles: Reconstructing the concept
of agility in information systems development,” Information Systems
Research, vol. 20, pp. 329–354, Sept. 2009.

[21] D. S. Alberts, The Agility Advantage: A Survival Guide for Complex

Enterprises and Endeavors. CCRP, 2011.

[22] M. Salo, E. Markopoulos, H. Vanharanta, and J. I. Kantola, Advances
in Human Factors, Business Management, Training and Education,
Part XVII, vol. 498, ch. Degree of Agility with an Ontology Based
Application.

[23] R. Dove, “Fundamental principles for agile systems engineering,” in
Proc. Conference on Systems Engineering Research (CSER’05), 2005.
[24] R. Dove, Response Ability: The Language, Structure, and Culture of the

Agile Enterprise. John Wiley & Sons, 2001.

[25] F. Hult and G. Sivanesan, “What good cyber resilience looks like,”
Journal of business continuity & emergency planning, vol. 7, no. 2,
pp. 112–125, 2014.

[26] J. R. Morris-King and H. Cam, “Modeling risk and agility interaction on
tactical edge,” in NATO IST-128 Workshop on Cyber Attack Detection,
Forensics and Attribution for Assessment of Mission Impact, Istanbul,
Turkey, 2015.

[27] I. R. Council, “Hard problem list.” http://www.infosec-research.org/

docs_public/20051130-IRC-HPL-FINAL.pdf, 2007.

[28] N. Science and T. Council, “Trustworthy cyberspace: Strategic plan
for
the federal cybersecurity research and development program.”
https://www.nitrd.gov/SUBCOMMITTEE/csia/Fed_Cybersecurity_RD_
Strategic_Plan_2011.pdf, 2011.

[29] D. Nicol, B. Sanders, J. Katz, B. Scherlis, T. Dumitra, L. Williams, and
M. P. Singh, “The science of security 5 hard problems (august 2015).”
http://cps-vo.org/node/21590.

[30] E. Chew, M. Swanson, K. Stine, N. Bartol, A. Brown, and W. Robinson,
NIST Special Publication 800-55 Revision 1: Performance Measurement
Guide for Information Security.

[31] T. C. for Internet Security, “The cis security metrics (ver 1.1.0).” http:

//benchmarks.cisecurity.org/downloads/metrics/, 2010.

[32] P. Du, Z. Sun, H. Chen, J. Cho, and S. Xu, “Statistical estimation of
malware detection metrics in the absence of ground truth,” IEEE Trans.
Information Forensics and Security, vol. 13, no. 12, pp. 2965–2980,
2018.

[33] Z. Lin, W. Lu, and S. Xu, “Uniﬁed preventive and reactive cyber
defense dynamics is still globally convergent,” IEEE/ACM Transactions
on Networking, 2019 (accepted for publication).

[34] M. Xu, G. Da, and S. Xu, “Cyber epidemic models with dependences,”

Internet Mathematics, vol. 11, no. 1, pp. 62–92, 2015.

[35] Y. Han, W. Lu, and S. Xu, “Characterizing the power of moving target
defense via cyber epidemic dynamics,” in Proc. 2014 Symposium and
Bootcamp on the Science of Security (HotSoS’14), pp. 10:1–10:12, 2014.
[36] R. Zheng, W. Lu, and S. Xu, “Active cyber defense dynamics exhibiting
rich phenomena,” in Proc. 2015 Symposium and Bootcamp on the
Science of Security (HotSoS’15), pp. 2:1–2:12, 2015.

[37] W. Lu, S. Xu, and X. Yi, “Optimizing active cyber defense dynamics,” in
Proceedings of the 4th International Conference on Decision and Game
Theory for Security (GameSec’13), pp. 206–225, 2013.

[38] G. Da, M. Xu, and S. Xu, “A new approach to modeling and analyzing
security of networked systems,” in Proceedings of the 2014 Symposium
and Bootcamp on the Science of Security (HotSoS’14), pp. 6:1–6:12,
2014.

[39] S. Xu, “Cybersecurity dynamics,” in Proc. Symposium and Bootcamp

on the Science of Security (HotSoS’14), pp. 14:1–14:2, 2014.

[40] S. Xu, “Cybersecurity dynamics: A foundation for the science of
cybersecurity,” in Proactive and Dynamic Network Defense (Z. Lu and
C. Wang, eds.), Springer New York, 2018 (to appear).

[41] H. Chen, J. Cho, and S. Xu, “Quantifying the security effectiveness of
ﬁrewalls and dmzs,” in Proceedings of the 5th Annual Symposium on
Hot Topics in the Science of Security, HoTSoS 2018, Raleigh, North
Carolina, USA, April 10-11, 2018., pp. 9:1–9:11, 2018.

[42] H. Chen, J. Cho, and S. Xu, “Quantifying the security effectiveness of
network diversity: poster,” in Proceedings of the 5th Annual Symposium
on Hot Topics in the Science of Security, HoTSoS 2018, Raleigh, North
Carolina, USA, April 10-11, 2018., p. 24:1, 2018.

[43] M. Kührer, C. Rossow, and T. Holz, “Paint it black: Evaluating the
effectiveness of malware blacklists,” in Proc. Research in Attacks,
Intrusions and Defenses (RAID’14), pp. 1–21.

[44] M. A. Rajab, F. Monrose, and A. Terzis, “On the effectiveness of
distributed worm monitoring,” in Proceedings of the 14th Conference
on USENIX Security Symposium, 2005.

[45] M. Xu, L. Hua, and S. Xu, “A vine copula model for predicting the
effectiveness of cyber defense early-warning,” Technometrics, vol. 59,
no. 4, pp. 508–520, 2017.

[46] D. Levin, “Lessons learned in using live red teams in IA experiments,”
in 3rd DARPA Information Survivability Conference and Exposition
(DISCEX-III), pp. 110–119, 2003.

[47] L. Carin, G. Cybenko, and J. Hughes, “Cybersecurity strategies: The
queries methodology,” IEEE Computer, vol. 41, no. 8, pp. 20–26, 2008.
[48] Z. Zhan, M. Xu, and S. Xu, “Characterizing honeypot-captured cyber
attacks: Statistical framework and case study,” IEEE Transactions on
Information Forensics and Security, vol. 8, no. 11, pp. 1775–1789, 2013.
[49] Z. Zhan, M. Xu, and S. Xu, “Predicting cyber attack rates with extreme
values,” IEEE Transactions on Information Forensics and Security,
vol. 10, no. 8, pp. 1666–1677, 2015.

[50] Y.-Z. Chen, Z.-G. Huang, S. Xu, and Y.-C. Lai, “Spatiotemporal patterns
and predictability of cyberattacks,” PLoS One, vol. 10, p. e0124472, 05
2015.

[51] C. Peng, M. Xu, S. Xu, and T. Hu, “Modeling and predicting extreme
cyber attack rates via marked point processes,” Journal of Applied
Statistics, vol. 0, no. 0, pp. 1–30, 2016.

[52] C. Peng, M. Xu, S. Xu, and T. Hu, “Modeling multivariate cybersecurity
risks,” Journal of Applied Statistics, vol. 0, no. 0, pp. 1–23, 2018.
[53] M. Xu, K. M. Schweitzer, R. M. Bateman, and S. Xu, “Modeling and
predicting cyber hacking breaches,” IEEE Transactions on Information
Forensics and Security, vol. 13, pp. 2856–2871, Nov 2018.

[54] T. S. Team, “The ofﬁcial blog of the world leading open-source ids/ips

snort.” http://blog.snort.org/, 2017.

Jose David Mireles is cybersecurity researcher and graduate of University
of Texas at San Antonio (UTSA) where he received his M.S in Computer
Science in 2017. As a graduate student at UTSA, his research was focused
on the extraction and attribution of attack signatures from large data sets, and

15

also the measurement and analysis of attacker-defender interactions (cyber
agility). He is a recipient of the National Science Foundation’s Scholarship
for Service program, and a RSA Conference Security Scholar (2017).

Eric Ficke is a Cyber Security researcher at the University of Texas at San
Antonio. He is currently pursuing his PhD in Computer Science. His existing
research covers intrusion detection and cyber security metrics.

Patrick Hurley is a Principal Computer Engineer and Program Manager
at the Air Force Research Laboratory in Rome, New York. He received
a Master of Science degree in Computer Information Science from SUNY
Institute of Technology. Mr. Hurley is currently the lead on a major AFRL
program: Trusted and Resilient Systems. This program is focused on ﬁghting
through cyber-attacks while maintaining mission essential functions. For over
30 years, Mr. Hurley has been the lead technical agent for DARPA on cyber
defense programs that focus on survivable architectures, adaptive security, and
advanced distributed systems technologies that lead to more agile and better
managed systems. He is a member of the IEEE and has over 60 peer-reviewed
technical papers in leading journals and conferences.

Jin-Hee Cho is currently an associate professor in the Department of
Computer Science at Virginia Tech since Aug. 2018. Prior to joining the
Virginia Tech, she worked as a computer scientist at the U.S. Army Re-
search Laboratory (USARL), Adelphi, Maryland, since 2009. Dr. Cho has
published over 110 peer-reviewed technical papers in leading journals and
conferences in the areas of trust management, cybersecurity, metrics and
measurements, network performance analysis, resource allocation, agent-
based modeling, uncertainty reasoning and analysis, information fusion /
credibility, and social network analysis. She received the best paper awards
in IEEE TrustComâ ˘A ´Z2009, BRIMSâ ˘A ´Z2013, IEEE GLOBECOMâ ˘A ´Z2017,
2017 ARLâ ˘A ´Zs publication award, and IEEE CogSima 2018. She is a winner
of the 2015 IEEE Communications Society William R. Bennett Prize in the
Field of Communications Networking. In 2016, Dr. Cho was selected for the
2013 Presidential Early Career Award for Scientists and Engineers (PECASE),
which is the highest honor bestowed by the US government on outstanding
scientists and engineers in the early stages of their independent research
careers. Dr. Cho earned MS and PhD degrees in computer science from the
Virginia Tech in 2004 and 2008, respectively. She is a senior member of the
IEEE and a member of the ACM.

Shouhuai Xu is a Full Professor in the Department of Computer Science, Uni-
versity of Texas at San Antonio. He is the Founding Director of the Laboratory
for Cybersecurity Dynamics (http://www.cs.utsa.edu/~shxu/LCD/index.html).
He coined the notion of Cybersecurity Dynamics as a candidate foundation
for the emerging science of cybersecurity. His research interests include the
three pillar thrusts of Cybersecurity Dynamics: ﬁrst-principle cybersecurity
modeling and analysis (the x-axis, to which the present paper belongs); cy-
bersecurity data analytics (the y-axis); and cybersecurity metrics (the z-axis).
He co-initiated the International Conference on Science of Cyber Security
(http://www.sci-cs.net/) and the ACM Scalable Trusted Computing Work-
shop (ACM STC). He is/was a Program Committee co-chair of SciSec’19,
SciSec’18, ICICS’18, NSS’15 and Inscrypt’13. He is/was an Associate Editor
of IEEE Transactions on Dependable and Secure Computing (IEEE TDSC),
IEEE Transactions on Information Forensics and Security (IEEE T-IFS), and
IEEE Transactions on Network Science and Engineering (IEEE TNSE). He
received his PhD in Computer Science from Fudan University.

