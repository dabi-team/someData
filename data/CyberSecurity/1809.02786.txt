Structure-Preserving Transformation: Generating Diverse and Transferable
Adversarial Examples

Dan Peng1, Zizhan Zheng2, Xiaofeng Zhang1
1Harbin Institute of Technology (Shenzhen)
2Tulane University, USA
pengdan@stu.hit.edu.cn, zzheng3@tulane.edu, zhangxiaofeng@hit.edu.cn

8
1
0
2

c
e
D
2
2

]

G
L
.
s
c
[

3
v
6
8
7
2
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Adversarial examples are perturbed inputs designed to fool
machine learning models. Most recent works on adversar-
ial examples for image classiﬁcation focus on directly mod-
ifying pixels with minor perturbations. A common require-
ment in all these works is that the malicious perturbations
should be small enough (measured by an Lp norm for some
p) so that they are imperceptible to humans. However, small
perturbations can be unnecessarily restrictive and limit the
diversity of adversarial examples generated. Further, an Lp
norm based distance metric ignores important structure pat-
terns hidden in images that are important to human percep-
tion. Consequently, even the minor perturbation introduced in
recent works often makes the adversarial examples less nat-
ural to humans. More importantly, they often do not transfer
well and are therefore less effective when attacking black-box
models especially for those protected by a defense mecha-
nism. In this paper, we propose a structure-preserving trans-
formation (SPT) for generating natural and diverse adversar-
ial examples with extremely high transferability. The key idea
of our approach is to allow perceptible deviation in adversar-
ial examples while keeping structure patterns that are central
to a human classiﬁer. Empirical results on the MNIST and
the fashion-MNIST datasets show that adversarial examples
generated by our approach can easily bypass strong adversar-
ial training. Further, they transfer well to other target models
with no loss or little loss of successful attack rate.

Introduction

Deep neural networks (DNNs) have achieved phenomenal
success in computer vision by showing superber accuracy
over traditional machine learning algorithms. However, re-
cent works have demonstrated that DNNs are vulnerable to
adversarial examples that are generated for malicious pur-
poses by slightly twisting the original images (Szegedy et
al. 2014; Goodfellow, Shlens, and Szegedy 2015). This ob-
servation has raised serious concerns on the robustness of
the state-of-the-art DNNs and limited their applications in
various security-sensitive applications. On the contrary, the
adversarial examples can be aggregated to augment training
datasets for learning a more robust DNN, known as adver-
sarial training (Madry et al. 2018b).

Copyright c(cid:13) 2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Adversarial examples generated by SPT. The ﬁrst
one is the original image selected from MNIST. The last
three are adversarial examples crafted by SPT for attacking
three different target classiﬁers, where they are classiﬁed as
8, 8, 2, respectively.

Generally speaking, adversarial examples can be any
valid inputs to machine learning models that are intention-
ally designed to cause mistakes (Elsayed et al. 2018). For
visual object recognition, we rely on human labelers to ob-
tain the ground truth labels, which are unknown to the at-
tacker before adversarial examples are generated. To bypass
this dilemma, a common attack strategy is to start with a
clean image where the ground truth label is already known
and modify it so that the new image is visually similar to the
original image while its output label differs from the ground
truth label of the clean image. The intuition behind this ap-
proach is that by ensuring visual similarity, the two images
are likely to share the same ground truth label.

A simple approach for ensuring visual similarity that
has been intensively studied in the literature is to intro-
duce small perturbations into pixels such that the distor-
tion between the adversarial example and the original image
is human imperceptible (Goodfellow, Shlens, and Szegedy
2015; Su, Vargas, and Sakurai 2018; Chen et al. 2018;
Carlini and Wagner 2017b). However, the noisy and mean-
ingless worst-case perturbations introduced by these ap-
proaches often make the generated adversarial images less
natural (Goodfellow, Shlens, and Szegedy 2015). Moreover,
perturbation-based adversarial examples exhibit low trans-
ferability, which makes them less effective for black-box at-
tacks especially for those with a defense mechanism (Ku-
rakin, Goodfellow, and Bengio 2016). There are some re-
cent works (Zhao, Dua, and Singh 2018; Song et al. 2018)
that relax the small perturbation requirement by utilizing
generative networks (e.g., GANs). However, they attempt
to generate adversarial examples with the identical distribu-

 
 
 
 
 
 
tion as the original dataset which limits their transferability
and therefore the effectiveness in black-box attacks. To im-
prove the robustness of DNNs against adversarial examples,
a large number of defense mechanisms have been proposed
in the literature (Kurakin, Goodfellow, and Bengio 2016;
Madry et al. 2018b; Guo et al. 2018; Cisse et al. 2017). How-
ever, they are mainly designed to make classiﬁers more ro-
bust to small perturbations.

In this paper, we propose a structure-preserving transfor-
mation (SPT) for generating natural and diverse adversarial
examples with extremely high transferability. Our approach
is inspired by the fact that humans recognize object mainly
from their shapes and structure patterns, supplemented by
the color and brightness information. For example, consider
the four images of digit 7 in Figure 1 where the ﬁrst one
is taken from MNIST while the next three are adversarial
examples crafted by our algorithm against different target
models. Despite the different background and foreground
brightness in the four images, a human being can easily
recognize the digit in all the cases since they share similar
structure patterns. However, the last three images are mis-
predicted as digit 8 or 2 by DNNs. This inspires us to design
adversarial examples that allow perceptible deviation (such
as color and brightness) while keeping structure patterns that
are central to a human classiﬁer. We show that our structure-
preserving adversarial examples are highly transferable with
no loss or little loss of successful rate when applied to black-
box attacks even when a defense mechanism is applied. This
can be explained as follows. Traditional supervised learning
models including DNNs all rely on the i .i .d . assumption
and become much less effective when test data and training
data are drawn from different distributions. Current defense
mechanisms do not improve that. Our approach generates
adversarial examples that may have a totally different dis-
tribution from the original dataset and is therefore highly
transferable.

An important limitation of previous attacks is that they
can only generate a single type of adversarial examples. For
perturbation-based attacks, a crafted adversarial example is
a replica of the original image except for minor perturbation.
That is, the attacker’s action space (with respect to a single
clean image) is a small neighborhood of the clean image
under some metric. For generative-network-based attacks,
they generate adversarial examples that obey the same dis-
tribution of the original images. That is, the attacker’s action
space is the set of images that follow the same distribution as
the clean images. Although they have relaxed the small per-
turbation requirement, they still keep the same characteristic
of the original images. For instance, the adversarial exam-
ples generated from MNIST images always have the same
background color (black) and foreground color (white).

In contrast, the attacker’s action space of our approach
is much larger. It includes all the images that preserve the
structure of the clean image. As shown in Figure 1, by
preserving structure, our adversarial examples are natural,
clean, legible to human beings. More importantly, for dif-
ferent target models or different initialization settings, the
adversarial examples generated using our method are sufﬁ-
ciently diverse. They are distinct from the original images,

and at the same time, they are also different from each other.
For instance, the three adversarial examples shown in Fig-
ure 1 exhibit different background and foreground gray-level
(intensity), which do not follow the distribution of the orig-
inal dataset. This is why our approach can generate more
diverse adversarial examples, which in turn makes our ap-
proach more difﬁcult to defend. In particular, a defender that
is adversarially trained with one type of adversarial exam-
ples is still vulnerable to other types of adversarial examples.
This work broadens the scope of adversarial machine
learning by showing a new class of adversarial examples
that follow different distributions from the training dataset
while still being legible and natural to humans. Our study
reveals the weakness of current defense mechanisms in the
face of structure-preserving attacks that go beyond the small
perturbation constraint.

Our main contributions can be summarized as follows.

• We propose a structure-preserving transformation (SPT)
to generate natural and diverse adversarial examples. We
show that an SPT can be easily trained with its learning
parameters converging fast to the global optimum.

• We evaluate our approach on two datasets and demon-
strate that our structure-preserving adversarial examples
achieve extremely high transferability even when a de-
fense mechanism is applied.

• We show that our structure-preserving adversarial exam-
ples can easily bypass strong adversarial training in the
public MNIST challenge. The generated adversarial ex-
amples can dramatically reduce the accuracy of the white-
box Madry network from 88.79% to 9.79% and the ac-
curacy of the black-box Madry network from 92.76% to
9.80%.

Background

In this section, we brieﬂy review recent studies on adversar-
ial examples and defense mechanisms. We start with a sim-
pliﬁed view of adversarial examples that has been broadly
adopted in the literature and then discuss the more general
deﬁnition used in this paper.

Traditional Adversarial Examples

In most previous works, an adversarial example is deﬁned
as a modiﬁed version of a clean image so that the two im-
ages are visually similar but the adversarial example is mis-
classiﬁed by the DNN (that is, its output label differs from
the ground truth label of the clean image). A common re-
quirement in many of existing studies is that the distor-
tion between the adversarial example and the original image
should be small enough so that the modiﬁcation is human
imperceptible. Also, the distortion is typically measured in
an Lp norm for some p. In particular, the L∞ norm mea-
sures the maximum variation of the distortions in pixel val-
ues (Goodfellow, Shlens, and Szegedy 2015), the L1 and
L2 norms measure the total variations (Chen et al. 2018;
Carlini and Wagner 2017b), while the L0 norm corresponds
to the total number of modiﬁed pixels (Su, Vargas, and

Sakurai 2018). Moreover, psychometric perceptual similar-
ity measures that are more consistent with human perception
have also been studied (Rozsa, Rudd, and Boult 2016).

Various techniques for crafting adversarial examples have
been proposed. Most of them focus on adding small pertur-
bations to the inputs, where several techniques have been
studied. In gradient-based approaches such as the popular
Fast Gradient Sign Method (FGSM) (Goodfellow, Shlens,
and Szegedy 2015) and many of its variants, adversarial
examples are generated by superimposing small perturba-
tions along the gradient direction (with respect to the input
image). More powerful attacks can be obtained by consid-
ering multi-step variants of FGSM such as Projected Gra-
dient Descent (PGD) (Madry et al. 2018b). In optimiza-
tion based approaches, an optimization problem is solved
to identify the optimal perturbation over the original image
that can lead to misclassiﬁcation (Carlini and Wagner 2017b;
Szegedy et al. 2014). More recently, generative networks in-
cluding auto-encoders (Baluja and Fischer 2018) and gener-
ative adversarial networks (GANs) (Xiao et al. 2018) have
been used to generate adversarial examples, where the small
perturbation requirement is imposed on the image space and
is included in the objective function to be trained.

General Adversarial Examples
Essentially, adversarial examples are inputs to machine
learning models that are intentionally designed to make out-
puts differ from human labeled ground truth (Elsayed et al.
2018). There are two important points to be clariﬁed in this
deﬁnition. First, adversarial examples should be meaningful
and recognizable to humans (Song et al. 2018). Although
unrecognizable images can be generated to mislead classi-
ﬁers (Nguyen, Yosinski, and Clune 2015), they are hard to
label by humans to get the ground truth. Second, the small
perturbation requirement is not a necessity. Instead, it is in-
troduced to allow the adversarial examples share the ground
truth labels of the original images to simplify the design of
attacks. To cause a mistake in learning models, we need to
ensure that adversarial examples are classiﬁed into labels
that are different from their ground truth labels, while the
latter is unknown until the adversarial examples have been
generated and labelled by humans. To avoid this dilemma,
traditional approaches introduced the small perturbation re-
quirement to ensure the visual similarity between adversarial
examples and clean images so that they are likely to share the
ground truth labels. In contrast, we propose to use structure-
similarity to model semantic similarity in this paper. Both
perturbation-based and structure-preserving adversarial ex-
amples are consistent with the general deﬁnition of adver-
sarial examples.

Defense Techniques
To improve the robustness of machine learning models
against adversarial examples, a number of defense mecha-
nisms have been proposed in the literature. Some approaches
try to merge adversarial examples into the training dataset to
decrease the model susceptibility to malicious attacks (Ku-
rakin, Goodfellow, and Bengio 2016; Madry et al. 2018b).
Researcher have also considered techniques for removing

adversarial perturbations during the test stage (Guo et al.
2018) and modifying network structures to improve the
model robustness (Cisse et al. 2017). However, most defense
methods are ineffective to the newly proposed attacks (Atha-
lye, Carlini, and Wagner 2018; Carlini and Wagner 2017a).
Only a few state-of-the-art defense models have demon-
strated their robustness to adversarial examples (Madry et al.
2018b; Samangouei, Kabkab, and Chellappa 2018), which,
however, are mainly designed for perturbation-based at-
tacks. To shed light on the weakness of existing defense
mechanisms, we have evaluated our structure-preserving at-
tacks against adversarial training with PGD (Madry et al.
2018b), which has been demonstrated to be the most effec-
tive defense method on the MNIST dataset (Athalye, Carlini,
and Wagner 2018).

Structure-Preserving Attacks to DNNs
The proposed approach is called Structure-Preserving
Transformation (SPT), which attempts to transform the in-
put images into adversarial examples against a deep neural
network while keeping the structure patterns of the original
images. SPT can be either untargeted or targeted and can be
used for both white-box and black-box attacks. In this paper,
we focus on untargeted white-box and black-box attacks.

Let x ∈ X be an unlabeled image and Y the set of class
labels. Let f be the target network that outputs a probability
distribution across the class labels. The target model assigns
the most possible class C(x) = arg maxk∈Y [f (x)]k to the
input image x. Let x(cid:48) denote the crafted adversarial image
of x using the proposed transformation. The general trans-
formation function gf,θ can be deﬁned as

gf,θ : x ∈ X → x(cid:48),

(1)

where θ is the parameter vector of function gf,θ. Given the
ground truth class label of the original image x, Ctrue(x),
our approach tries to craft an adversarial image (transformed
image x(cid:48)) to mislead classiﬁer f so that C(x(cid:48)) (cid:54)= Ctrue(x).
Transformations in spatial domains are commonly used
in image enhancement where an image is transformed to a
new image for better visual quality by directly manipulating
pixels, which can be formalized as follows.

x(cid:48)(m, n) = Tm,n[x(m, n)]

(2)
where x(m, n) (resp. x(cid:48)(m, n)) are the intensity of the input
(resp. output) image at the coordinate (m, n), and Tm,n is
a transformation on x that depends on (m, n). Note that in
general, the transformed value of any pixel may depend on
all the pixels in some neighborhood of it, e.g., considering
the convolution and pooling operations in CNN (Krizhevsky,
Sutskever, and Hinton 2012). The simplest form of T is
when the neighborhood of a pixel is a singleton, that is, it
only contains the pixel itself. In this case, x(cid:48)(m, n) depends
only on x(m, n) and T becomes a gray-level transformation
that depends only on pixel values and is independent of their
coordinates. As a special case of such singleton-based trans-
formations, power functions are commonly used in image
enhancement to allow a variety of devices to print and dis-
play images (Kim 1999). In this paper, we deﬁne the trans-
formation function gf,θ as a linear combination of multiple

power functions. For simplicity, we use g(.) to denote gf,θ
in the rest of this paper. Accordingly, the general form of
g(.) is given as,

g(x) = sigmoid(

(cid:88)

wixγi)

(3)

i

where γi is the exponent of the i-th basis power function. In
general, these exponents can be either learned from data or
chosen based on domain knowledge and ﬁxed in advance.
For simplicity, we manually choose the values of γi’s in this
paper. The weight wi is a scalar shared with all the elements
in xγi. These weights {wi} are the parameters to be learned.
That is, θ = {wi}. The sigmoid function is adopted to fur-
ther guarantee the range of output falls into [0,1].

A notable property of any singleton-based transformation
including ours is that all the pixels with the same gray level
in the original image have the same gray level in the trans-
formed image. Further, our transformation is typically in-
jective, i.e., it ensures that pixels with different gray levels
in the original image typically have different gray levels in
the transformed image. That is, such a transformation keeps
the structure patterns of images, which is formally deﬁned
as follows.
Deﬁnition 1. Structure Pattern The set of all pixels with
the same grey-level in an image is called a structure pat-
tern of the image. Symbolically, given an image x, the set
{(m, n)|x(m, n) = c} is called a structure pattern with
gray level c.

Training
SPT is trained together with the target model by adding
an extra layer in front of the target network. For untargetd
attacks, our objective is to ﬁnd the parameters θ so that
C(x(cid:48)) (cid:54)= Ctrue(x). To improve the chance of successful at-
tacks, we aim to ﬁnd θ so that the distance between the pre-
dicted logits (that is, the output of the classiﬁer f (x)) and
the one-hot encoding of ground truth class on all the train-
ing data is maximized. We consider the following objective
function:

argmax
θ

(cid:88)

x∈X

L(f (g(x)), ltrue(x))) − α

(cid:88)

w2
i ,

i

(4)

where L is a loss function that measures the difference be-
tween the output logit f (g(x)) when the target model f is
applied to the crafted adversarial example g(x) and the one-
hot encoding of ground truth class of the original image.
Moreover, an L2 regularization term is introduced in (4),
which is used to penalize intensely dramatic image trans-
formations that may deteriorate the visual quality of images.
α is a scalar parameter that controls the strength of regular-
ization. Unlike most other approaches, the generated x(cid:48) is
not restricted to be similar enough to the original x. Instead,
our approach guarantees the perceptual similarity in human
vision in terms of structure patterns. Hence, the correspond-
ing objective function could be deﬁned in a concise manner
and is thus easy to converge.

In this paper, we deﬁne the loss function L as the
cross entropy between the one-hot encoding ground truth

class ltrue(x) of the original image and the predicted logit
f (g(x)) of the corresponding adversarial image. For un-
targted attacks, we then solve the following minimization
problem to ﬁnd the parameters θ of g(·):

argmin
θ

(cid:88)

x∈X

ltrue(x)T log(f (g(x))) + α

(cid:88)

w2
i .

i

(5)

Although we focus on untargeted attacks in this work,
our approach extends to targeted attacks by considering a
slightly different objective function:

argmin
θ

(cid:88)

x∈X

L(f (g(x)), ltarget(x(cid:48)))) + α

(cid:88)

w2
i ,

i

(6)

where ltarget(x(cid:48)) is the vectorized representation of target
class we want to mispredict, which can be simply deﬁned
as the one-hot encoding of the target class similar to the un-
targeted case. In addition to this simple encoding scheme
and the cross-entropy based loss function, other encoding
techniques and loss functions (Baluja and Fischer 2018;
Carlini and Wagner 2017b) can also be applied to our frame-
work for both untargeted attacks and target attacks, which
will be studied in the future.

To ﬁnd the learning parameters {wi}, we solve the above
optimization problem using the Adam method (Kingma and
Ba 2014) with a learning rate of 10−4. Due to the good con-
vergence property of the cross entropy loss function, one
epoch training is sufﬁcient for all the training data. More-
over, because a small number of parameters {wi} (about 10
parameters) need to be optimized, it only takes a few min-
utes to train.

Inference
After training a Structure-Preserving Transformation, we
generate adversarial examples by performing the trained
SPT on the original images. The process is fast and only
involves computing a linear combination of power functions
and does not require any information of the target models.

Experiment Results
To evaluate the performance of SPT, we compare it
with three baseline attack algorithms, FGSM (Goodfellow,
Shlens, and Szegedy 2015), PGD (Madry et al. 2018b),
and C&W (Carlini and Wagner 2017b) on two popular
image classiﬁcation datasets, MNIST (LeCun et al. 1998)
and Fashion-MNIST (F-MNIST) (Xiao, Rasul, and Vollgraf
2017). Both datasets consist of 60,000 training images and
10,000 testing images from 10 classes. As in most related
works, classiﬁcation accuracy is used as the evaluation crite-
rion. A stronger attack method has a lower classiﬁcation ac-
curacy. Moreover, the attack methods are evaluated both on
the original models as well as when a defense mechanism is
applied. In this paper, adversarial training with PGD is cho-
sen as the defense method, which has been shown to be the
most effective defense method on the MNIST dataset (Atha-
lye, Carlini, and Wagner 2018). Evaluations in both white-
box and black-box attack settings show that the proposed
SPT can generate more diverse and naturally distorted ad-
versarial images with high transferability.

Table 1: White-box attacks. Classiﬁcation accuracy of different models on the MNIST (top) and F-MNIST (bottom) datasets.
In each row, the best result is highlighted in bold and the second-best result is underlined.

Defense

Target No Attack

No
Defense

PGD Adv. Tr.
(Tr. Acc)
(cid:15) = 0.3
α = 0.01

No
Defense

PGD Adv. Tr.
(Tr. Acc)
(cid:15) = 0.3
α = 0.01

Cp
Ca0
Ca1
Ca2
Ca3
Cp
Ca0
Ca1
Ca2
Ca3

Cp
Ca0
Ca1
Ca2
Ca3
Cp
Ca0
Ca1
Ca2
Ca3

99.02%
98.83%
98.73%
98.33%
98.58%
98.08%
97.65%
98.13%
98.20%
96.90%

91.18%
91.35%
90.67%
89.79%
91.24%
74.80%
72.40%
73.27%
77.29%
71.47%

FGSM
9.28%
5.55%
7.18%
8.25%
11.44%
93.24%
84.51%
89.01%
87.78%
92.54%

7.73%
7.42%
7.88%
9.61%
4.42%
68.92%
63.17%
65.18%
74.52%
64.04%

PGD
0.00%
0.00%
0.03%
0.09%
0.00%
88.14%
68.07%
73.92%
73.16%
87.15%

0.00%
0.00%
0.00%
0.08%
0.00%
55.40%
50.54%
51.75%
66.41%
49.34%

C&W
0.00%
0.00%
0.00%
0.00%
0.00%
32.00%
5.00%
6.00%
4.00%
45.00%

0.00%
0.00%
0.00%
0.00%
0.00%
69.00%
78.00%
77.00%
78.00%
66.00%

SPT
9.74%
9.75%
10.52%
8.93%
9.91%
13.47%
5.10%
4.39%
8.98%
7.32%

10.00%
9.18%
10.45%
10.04%
10.06%
10.00%
7.24%
9.99%
5.71%
9.45%

Experiment Settings

Baseline Attacks: As aforementioned, three baseline attack
approaches are implemented for performance comparison.
• FGSM attack: FGSM superimposes small gradient-
based perturbation to the original images to mislead the
classiﬁcation models (Goodfellow, Shlens, and Szegedy
2015).

• PGD attack: It is a variant of iterative FGSM. In each
step, PGD restarts with a randomly perturbed version of
the previous result (Madry et al. 2018b).

• C&W attack: This attack generates adversarial examples
by solving L2 norm based optimization problem to ﬁnd
the adversarial examples that drop the accuracy of target
model as much as possible (Carlini and Wagner 2017b).

Parameter settings: For SPT, we consider a linear com-
bination of 11 power functions with exponents γ equal to
0.04, 0.10, 0.20, 0.40, 0.67, 1.0, 1.5, 2.5, 5.0, 10.0, 25.0, re-
spectively. The larger the γ is, the darker the transformed
image will be. The intuition behind choosing these expo-
nents is further explained in the our technical report (Peng,
Zheng, and Zhang 2018). Since most pixel values in the
MNIST dataset are close to black or white, even a large γ
will not signiﬁcantly distort an image, the coefﬁcient α of
the penalty term in (3) is to set to 0 for experiments on the
MNIST dataset. We set α = 0.6 for experiments on fashion-
MNIST dataset to avoid low image contrast for large γ val-
ues.

Both C&W and FGSM are implemented using the public
CleverHans package with the same conﬁgurations (Papernot
et al. 2016). PGD is implemented using the source code from
the MNIST challenge (Madry et al. 2018a).

Attack modes: Two attack modes are considered in the
evaluations. i.e., white-box attack and black-box attack.
For white-box attack, details of the target model are pub-
licly known to the malicious parties including model struc-
ture and learned weights, datasets, and the adopted defense
mechanisms. For black-box attack, only the training and
testing datasets could be manipulated but the rest keeps un-
known to the adversaries.

Evaluation Results for White-Box Attacks
In this section, we present the experimental results in the
white-box setting. Following the same network architecture
in (Baluja and Fischer 2018), we train ﬁve networks as tar-
get models on the MNIST training dataset and F-MNIST
training dataset, respectively. Each network is a combina-
tion of convolutional and fully connected layers. Details are
provided in our online technical report (Peng, Zheng, and
Zhang 2018). In Table 1, we report the classiﬁcation accu-
racy of these models under various attack-defense conﬁgu-
rations. On the attacker side, we consider ﬁve cases: when
there is no attack and when one of the four attack methods
is applied. On the defender side, two cases are considered:
when there is no defense and when PGD-based adversarial
training is applied. We observe that SPT achieves low accu-
racy (about 10% or lower) across all the scenarios. When no
defense is applied, both C&W and PGD achieve very low ac-
curacy (close to 0), while SPT is still comparable to FGSM.
When PGD-based adversarial training is applied, SPT ob-
tains overwhelming low accuracy than all the baseline at-
tacks. These results demonstrate the effectiveness of SPT
even in the presence of strong defense.

We further evaluate SPT adversarial examples on the most
competitive public MNIST challenge where researchers try

Figure 2: Visual illustration of adversarial examples generated by SPT. The ﬁrst row shows the original images selected from
MNIST (left) and F-MNIST (right), respectively. The other ﬁve rows show the adversarial examples crafted by SPT for each of
the ﬁve classiﬁers, respectively.

to attack the well-trained robust Madry net-
their best
works (Madry et al. 2018a). Remarkably, SPT drops the ac-
curacy of the white-box Madry network to 9.79% , which
is far below the current best result of 88.79% on the leader-
board (Zheng, Chen, and Ren 2018). We acknowledge that
the comparison is a bit unfair since SPT does not have the
small perturbation requirement while the public results in
the leaderboard do. However, the observed big gap does in-
dicate the strength of SPT.

Black-Box Attacks and Transferability
In the section, we present experimental results in the black-
box setting. The same ﬁve target networks as in the white-
box setting are used. Among them, Classiﬁer Cp is used as
the substitute model. We ﬁrst generate adversarial examples
against Cp and then test them on the other four target net-
works. As expected, SPT consistently obtains low accuracy
with or without defense. Thus, it has excellent transferabil-
ity and is extremely effective in the black-box setting. FGSM
and PGD have rather low accuracy when no defense is ap-
plied due to their moderate transferability. However, when
defense is applied, both of them exhibit poor transferabil-
ity. C&W exhibits poor transferability in all the scenarios
due to the excessive optimization applied (Kurakin, Good-
fellow, and Bengio 2016). Further, SPT reduces the accuracy
of the black-box Madry network to 9.80% in the MNIST
challenge, while the current best result on the leaderboard is
92.76% (Xiao et al. 2018).

Illustration of Adversarial Examples
Figure 2 shows the adversarial examples generated by SPT
where the ﬁrst row gives the original images and each other
row shows the generated adversarial examples against Cp,
Ca0, Ca1, Ca2, and Ca3, respectively. We observe that ad-
versarial examples on the same column (corresponding to
different target models) have distinct colors (brightness) but
they all keep the same structure as the original image. In
fact, SPT generates diverse adversarial examples not only

for different target classiﬁers but also for different initial-
ization settings (see Figure 4 in the technical report (Peng,
Zheng, and Zhang 2018) ).

The diversity of adversarial examples poses a challenge
to defense against SPT as it is difﬁcult to resist all kinds
of SPT adversarial examples. Moreover, all the adversarial
examples are clean and legible to humans. For MNIST, the
generated digits keep the same handwriting characteristics,
like the same person writing with different-colored pens and
papers. For F-MNIST, the generated clothes are of the same
style while they have different colors and textures.

Observations and Discussions

We make the following observations from the above results.
First, DNNs even with defense are vulnerable to adversar-
ial examples drawn from a distribution that is different from
the training dataset. That is the main reason why the pro-
posed SPT adversarial examples exhibit high attack success
rate and high transferability.

Second, as shown in Figure 2, adversarial examples in the
same column preserve the same structure pattern, but they
are classiﬁed into different classes by the target DNNs. This
indicates that DNNs are not very good at learning shapes
and structures especially when images are not drawn from
the same distribution as the training data.

Third, Figure 2 shows that adversarial examples on the
same row are often classiﬁed into the same class. The statis-
tics of the prediction results across labels given in Table 3 in
our detailed report (Peng, Zheng, and Zhang 2018) further
conﬁrms this observation. We note that these examples cor-
respond to different original images against the same target
model. Moreover, they exhibit different structures but have
similar brightness. We therefore speculate that in make pre-
dictions for image drawing from different distribution with
training data, DNNs mainly rely on the distribution of pixel
values rather than detailed structure patterns.

We hope our observations bring new insights into how

DNNs work.

Table 2: Black-box attacks. Classiﬁcation accuracy on the MNIST (top) and F-MNIST (bottom) datasets where Cp is used
as the substitute model and the other models are target models. In each row, the best result is highlighted in bold and the
second-best result is underlined.

Defense

No
Defense

PGD Adv. Tr.
(Tr. Acc)
(cid:15) = 0.3
α = 0.01

No
Defense

PGD Adv. Tr.
(Tr. Acc)
(cid:15) = 0.3
α = 0.01

Target No Attack
99.02%∗
98.83%
98.73%
98.33%
98.58%
98.08%
97.65%
98.13%
98.20%
96.90%

Cp
Ca0
Ca1
Ca2
Ca3
Cp
Ca0
Ca1
Ca2
Ca3

Cp
Ca0
Ca1
Ca2
Ca3
Cp
Ca0
Ca1
Ca2
Ca3

91.18%∗
91.35%
90.67%
89.79%
91.24%
74.80%
72.40%
73.27%
77.29%
71.47%

FGSM
9.42%∗
27.95%
41.57%
30.35%
28.78%
94.50%
87.38%
90.19%
85.67%
92.86%

7.73%∗
11.19%
11.23%
15.67%
11.72%
71.65%
67.58%
68.54%
72.77%
69.50%

PGD
0.00%∗
29.24%
45.81%
35.74%
29.51%
95.41%
90.33%
92.79%
91.30%
93.77%

0.00%∗
0.04%
0.58%
3.01%
1.68%
71.69%
67.78%
68.69%
73.28%
69.36%

C&W
0.00%∗
89.00%
88.00%
89.00%
87.00%
90.00%
89.00%
90.00%
90.00%
90.00%

0.00%∗
88.00%
90.00%
89.00%
90.00%
90.00%
90.00%
90.00%
90.00%
90.00%

SPT
9.74%∗
9.74%
11.72%
8.92%
10.17%
5.23%
6.97%
7.58%
8.60%
6.45%

10.00%∗
10.02%
10.00%
13.36%
10.00%
9.99%
10.00%
9.99%
10.00%
10.00%

Related Works
There are a few existing works that consider generating gen-
eral adversarial example beyond the small perturbation re-
quirement. Most of them are built upon the idea that the
ground truth of an original image can propagate to adver-
sarial examples by preserving the visual similarity between
them. In (Zhao, Dua, and Singh 2018; Song et al. 2018),
this is achieved by limiting the change of a latent repre-
sentation of images learned using GANs. Another approach
is to make perturbations resemble real yet inconspicuous
objects to hide them from humans (Evtimov et al. 2017;
Brown et al. 2017).

The intuition behind the recent work (Hosseini and
Poovendran 2018) is somewhat similar to ours, where they
propose the concept of “shape bias” and state that humans
prefer to categorize objects according to their shapes rather
than colors (Landau, Smith, and Jones 1988). However, the
technical approaches of the two works are very different. In
particular, the approach in (Hosseini and Poovendran 2018)
focuses on color images and the main idea is convert the
color space from RGB to HSV. To preserve the shape in-
formation, they keep the brightness component unchanged
and change color components (i.e., saturation and hue) with
the same amount for all the pixels. In contrast, we change
pixel values based on structure patterns, where the amount
of change is different for pixels in different structure pat-
terns. Moreover, our transformation is controlled by multi-
ple trainable parameters. These two differences are the key
to obtain more diverse adversarial examples compared to the
approach in (Hosseini and Poovendran 2018). We also note
that although our approach focuses on grey-scale images, the
key idea applies to color images as well.

Conclusion and Further Work
In this paper, we propose the technique of structure-
preserving transformation (SPT) to generate natural and di-
verse adversarial examples with high transferability. SPT
keeps the semantic similarity between adversarial examples
and original images by preserving the structure patterns be-
tween them. This new approach allows the ground truth of
original images to be shared with adversarial examples with-
out imposing the small perturbation requirement. Empirical
results on the MNIST and fashion-MNIST datasets show
that SPT can generate adversarial examples that are natu-
ral to humans while being sufﬁciently diverse. Further, the
adversarial examples signiﬁcantly reduce the classiﬁcation
accuracy of target models whether defenses are applied or
not. In particular, we show that SPT can easily bypass PGD-
based adversarial training. Moreover, SPT adversarial exam-
ples can transfer across different models with little or no loss
of successful attack rates. The high successful attack rates
and outstanding transferability stem from the key property
that SPT adversarial examples follow different distributions
from the training data of target models.

Although we focus on untargeted attacks and gray-scale
images in the paper, the core idea of SPT can extend to tar-
geted attacks and color images. In our preliminary experi-
ments for color images, the high successful attack rate is at
the expense of images being less discriminable and authen-
tic. How to ensure the quality of images while maintaining
a high successful attack rate for color images is an impor-
tant challenge that we will investigate in our future work.
In addition, we will test the performance of SPT-based ad-
versarial training and study the performance of SPT attacks
under other defense methods that do not rely on (cid:96)p attacks.

References
[Athalye, Carlini, and Wagner 2018] Athalye, A.; Carlini,
N.; and Wagner, D. A. 2018. Obfuscated gradients give
a false sense of security: Circumventing defenses to adver-
sarial examples. In ICML Workshop.
[Baluja and Fischer 2018] Baluja, S., and Fischer, I. 2018.
Learning to attack: Adversarial transformation networks. In
AAAI.
[Brown et al. 2017] Brown, T. B.; Man´e, D.; Roy, A.; Abadi,
M.; and Gilmer, J. 2017. Adversarial patch. arXiv preprint
arXiv:1712.09665.
[Carlini and Wagner 2017a] Carlini, N., and Wagner, D.
2017a. Adversarial examples are not easily detected: By-
passing ten detection methods. In AISec Workshop.
[Carlini and Wagner 2017b] Carlini, N., and Wagner, D.
2017b. Towards evaluating the robustness of neural net-
works. In S&P.
[Chen et al. 2018] Chen, P.; Sharma, Y.; Zhang, H.; Yi, J.;
and Hsieh, C. 2018. EAD: elastic-net attacks to deep neural
networks via adversarial examples. In AAAI.
[Cisse et al. 2017] Cisse, M.; Bojanowski, P.; Grave, E.;
Dauphin, Y.; and Usunier, N. 2017. Parseval networks: Im-
proving robustness to adversarial examples. In ICML.
[Elsayed et al. 2018] Elsayed, G. F.; Shankar, S.; Cheung,
B.; Papernot, N.; Kurakin, A.; Goodfellow, I.; and Sohl-
Dickstein, J. 2018. Adversarial examples that fool both hu-
man and computer vision. arXiv preprint arXiv:1802.08195.
[Evtimov et al. 2017] Evtimov, I.; Eykholt, K.; Fernandes,
E.; Kohno, T.; Li, B.; Prakash, A.; Rahmati, A.; and Song,
D. 2017. Robust physical-world attacks on deep learning
models. arXiv preprint arXiv:1707.08945 1.
I.;
[Goodfellow, Shlens, and Szegedy 2015] Goodfellow,
Shlens, J.; and Szegedy, C. 2015. Explaining and harnessing
adversarial examples. In ICLR.
[Guo et al. 2018] Guo, C.; Rana, M.; Cisse, M.; and van der
Maaten, L. 2018. Countering adversarial images using input
transformations. In ICLR.
[Hosseini and Poovendran 2018] Hosseini, H., and Pooven-
arXiv
dran, R.
preprint arXiv:1804.00499.
[Kim 1999] Kim, J.-g. 1999. Color correction device for
correcting color distortion and gamma characteristic. US
Patent 5,949,496.
[Kingma and Ba 2014] Kingma, D. P., and Ba, J. L. 2014.
Adam: Amethod for stochastic optimization. In ICLR.
[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky, A.;
Imagenet clas-
Sutskever, I.; and Hinton, G. E.
siﬁcation with deep convolutional neural networks.
In
NIPS.
[Kurakin, Goodfellow, and Bengio 2016] Kurakin,
A.;
Goodfellow, I.; and Bengio, S. 2016. Adversarial machine
learning at scale. arXiv preprint arXiv:1611.01236.
[Landau, Smith, and Jones 1988] Landau, B.; Smith, L. B.;
and Jones, S. S. 1988. The importance of shape in early
lexical learning. Cognitive Development 3:299–321.

2018. Semantic adversarial examples.

2012.

2018.

[LeCun et al. 1998] LeCun, Y.; Bottou, L.; Bengio, Y.; and
Haffner, P. 1998. Gradient-based learning applied to doc-
ument recognition. Proceedings of the IEEE 86(11):2278–
2324.
[Madry et al. 2018a] Madry, A.; Makelov, A.; Schmidt, L.;
Tsipras, D.; and Vladu, A.
2018a. MNIST Adver-
sarial Examples Challenge. https://github.com/
MadryLab/mnist_challenge. Accessed: 2018-09-
01.
[Madry et al. 2018b] Madry, A.; Makelov, A.; Schmidt, L.;
Tsipras, D.; and Vladu, A. 2018b. Towards deep learning
models resistant to adversarial attacks. In ICLR.
[Nguyen, Yosinski, and Clune 2015] Nguyen, A.; Yosinski,
J.; and Clune, J. 2015. Deep neural networks are easily
fooled: High conﬁdence predictions for unrecognizable im-
ages. In CVPR.
[Papernot et al. 2016] Papernot, N.; Carlini, N.; Goodfellow,
I.; Feinman, R.; Faghri, F.; Matyasko, A.; Hambardzumyan,
K.; Juang, Y.-L.; Kurakin, A.; Sheatsley, R.; et al. 2016.
cleverhans v2. 0.0: an adversarial machine learning library.
arXiv preprint arXiv:1610.00768.
[Peng, Zheng, and Zhang 2018] Peng, D.; Zheng, Z.; and
Zhang, X.
Structure-preserving transfor-
mation: Generating diverse and transferable adversar-
ial examples. https://github.com/lepangdan/
Structure-Preserving-Transformation.
[Rozsa, Rudd, and Boult 2016] Rozsa, A.; Rudd, E. M.; and
Boult, T. E. 2016. Adversarial diversity and hard positive
generation. In CVPR Workshop.
[Samangouei, Kabkab, and Chellappa 2018] Samangouei,
P.; Kabkab, M.; and Chellappa, R. 2018. Defense-GAN:
Protecting classiﬁers against adversarial attacks using
generative models. In ICLR.
[Song et al. 2018] Song, Y.; Shu, R.; Kushman, N.; and Er-
mon, S. 2018. Constructing unrestricted adversarial exam-
ples with generative models. In NeurIPS.
[Su, Vargas, and Sakurai 2018] Su, J.; Vargas, D. V.; and
Attacking convolutional neural
Sakurai, K.
2018.
arXiv preprint
network using differential evolution.
arXiv:1804.07062.
[Szegedy et al. 2014] Szegedy, C.; Zaremba, W.; Sutskever,
I.; Bruna, J.; Erhan, D.; Goodfellow, I.; and Fergus, R. 2014.
Intriguing properties of neural networks. In ICLR.
[Xiao et al. 2018] Xiao, C.; Li, B.; Zhu, J.-Y.; He, W.;
2018. Generating adversar-
Liu, M.; and Song, D.
arXiv preprint
ial examples with adversarial networks.
arXiv:1801.02610.
[Xiao, Rasul, and Vollgraf 2017] Xiao, H.; Rasul, K.; and
Vollgraf, R. 2017. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747.
[Zhao, Dua, and Singh 2018] Zhao, Z.; Dua, D.; and Singh,
S. 2018. Generating natural adversarial examples. In ICLR.
[Zheng, Chen, and Ren 2018] Zheng, T.; Chen, C.; and Ren,
K. 2018. Distributionally adversarial attack. arXiv preprint
arXiv:1808.05537.

