9
1
0
2

l
u
J

1

]

R
C
.
s
c
[

1
v
6
9
3
1
0
.
7
0
9
1
:
v
i
X
r
a

Strategic Learning for Active, Adaptive, and
Autonomous Cyber Defense

Linan Huang and Quanyan Zhu

Abstract The increasing instances of advanced attacks call for a new defense
paradigm that is active, autonomous, and adaptive, named as the â€˜3Aâ€™ defense
paradigm. This chapter introduces three defense schemes that actively interact with
attackers to increase the attack cost and gather threat information, i.e., defensive de-
ception for detection and counter-deception, feedback-driven Moving Target Defense
(MTD), and adaptive honeypot engagement. Due to the cyber deception, external
noise, and the absent knowledge of the other playersâ€™ behaviors and goals, these
schemes possess three progressive levels of information restrictions, i.e., from the
parameter uncertainty, the payoï¬€ uncertainty, to the environmental uncertainty. To
estimate the unknown and reduce the uncertainty, we adopt three diï¬€erent strategic
learning schemes that ï¬t the associated information restrictions. All three learning
schemes share the same feedback structure of sensation, estimation, and actions so
that the most rewarding policies get reinforced and converge to the optimal ones
in autonomous and adaptive fashions. This work aims to shed lights on proactive
defense strategies, lay a solid foundation for strategic learning under incomplete
information, and quantify the tradeoï¬€ between the security and costs.

1 Introduction

Recent instances of WannaCry ransomware, Petya cyberattack, and Stuxnet mal-
ware have demonstrated the trends of modern attacks and the corresponding new
security challenges as follows.

Linan Huang
Department of Electrical and Computer Engineering, New York University, 2 MetroTech Center,
Brooklyn, NY, 11201, USA, e-mail: lh2328@nyu.edu

Quanyan Zhu
Department of Electrical and Computer Engineering, New York University, 2 MetroTech Center,
Brooklyn, NY, 11201, USA, e-mail: qz494@nyu.edu

1

 
 
 
 
 
 
2

Linan Huang and Quanyan Zhu

â€¢ Advanced: Attackers leverage sophisticated attack tools to invalidate the oï¬€-the-

shelf defense schemes such as the ï¬rewall and intrusion detection systems.

â€¢ Targeted: Unlike automated probes, targeted attacks conduct thorough research in
advance to expose the system architecture, valuable assets, and defense schemes.
â€¢ Persistent: Attackers can restrain the adversaryâ€™s behaviors and bide their times

to launch critical attacks. They are persistent in achieving the goal.

â€¢ Adaptive: Attackers can learn the defense strategies and unpatched vulnerabilities
during the interaction with the defender and tailor their strategies accordingly.
â€¢ Stealthy and Deceptive: Attackers conceal their true intentions and disguise their
claws to evade detection. The adversarial cyber deception endows attackers an
information advantage over the defender.

Thus, defenders are urged to adopt active, adaptive, and autonomous defense
paradigms to deal with the above challenges and proactively protect the system prior
to the attack damages rather than passively compensate for the loss. In analogy to the
classical Kerckhoï¬€sâ€™s principle in the 19th century that attackers know the system,
we suggest a new security principle for modern cyber systems as follows:

Principle of 3A Defense: A cyber defense paradigm is considered to be
insuï¬ƒciently secure if its eï¬€ectiveness relies on

â€¢ Rule-abiding human behaviors.
â€¢ A perfect protection against vulnerabilities and a perfect prevention from

system penetration.

â€¢ A perfect knowledge of attacks.

Firstly, 30% of data breaches are caused by privilege misuse and error by insiders
according to Verizonâ€™s data breach report in 2019 [1]. Security administration does
not work well without the support of technology, and autonomous defense strategies
are required to deal with the increasing volume of sophisticated attacks. Secondly,
systems always have undiscovered vulnerabilities or unpatched vulnerabilities due to
the long supply chain of uncontrollable equipment providers [2] and the increasing
complexities in the system structure and functionality. Thus, an eï¬€ective paradigm
should assume a successful inï¬ltration and pursue strategic securities through inter-
acting with intelligent attackers. Finally, due to adversarial deception techniques and
external noises, the defender cannot expect a perfect attack model with predicable
behaviors. The defense mechanism should be robust under incomplete information
and adaptive to the evolution of attacks.

In this chapter, we illustrate three active defense schemes in our previous works,
which are designed based on the new cyber security principle. They are defensive
deception for detection and counter-deception [3, 4, 5] in Section 2, feedback-driven
Moving Target Defense (MTD) [6] in Section 3, and adaptive honeypot engagement
[7] in Section 4. All three schemes is of incomplete information, and we arrange
them based on three progressive levels of information restrictions as shown in the
left part of Fig. 1.

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

3

Fig. 1 The left part of the
ï¬gure describes the degree
of information restriction.
From bottom to top, the
defense scheme becomes
more autonomous and relies
less on an exact attack model,
which also result in more
uncertainties. The right part
is the associated feedback
learning schemes.

The ï¬rst scheme in Section 2 considers the obfuscation of characteristics of
known attacks and systems through a random parameter called the playerâ€™s type. The
only uncertainty origins from the playerâ€™s type, and the mapping from the type to the
utility is known deterministically. The MTD scheme in Section 3 considers unknown
attacks and systems whose utilities are completely uncertain, while the honeypot
engagement scheme in Section 4 further investigates environmental uncertainties
such as the transition probability, the sojourn distribution, and the investigation
reward.

To deal with these uncertainties caused by diï¬€erent information structures, we
suggest three associated learning schemes as shown in the right part of Fig. 1, i.e.,
Bayesian learning for the parameter estimation, distributed learning for the utility
acquisition without information sharing, and reinforcement learning for the optimal
policy obtainment under the unknown environment. All three learning methods
form a feedback loop that strategically incorporates the samples generated during
the interaction between attackers and defenders to persistently update the beliefs
of known and then take actions according to current optimal decision strategies.
The feedback structure makes the learning adaptive to behavioral and environmental
changes.

Another common point of these three schemes is the quantiï¬cation of the tradeoï¬€
between security and the diï¬€erent types of cost. In particular, the costs result from
the attackerâ€™s identiï¬cation of the defensive deception, the system usability, and the
risk of attackers penetrating production systems from the honeynet, respectively.

1.1 Literature

The idea of using deceptions defensively to detect and deter attacks has been studied
theoretically as listed in the taxonomic survey [8], implemented to the Adversar-
ial Tactics, Techniques and Common Knowledge (ATT&CKT M ) adversary model
system [9], and tested in the real-time cyber-wargame experiment [10]. Many previ-
ous works imply the similar idea of type obfuscation, e.g., creating social network
avatars (fake personas) on the major social networks [11], implementing honey ï¬les

4

Linan Huang and Quanyan Zhu

for ransomware actions [12], and disguising a production system as a honeypot to
scare attackers away [13].

Moving target defense (MTD) allows dynamic security strategies to limit the
exposure of vulnerabilities and the eï¬€ectiveness of the attackerâ€™s reconnaissance by
increasing complexities and costs of attacks [14]. To achieve an eï¬€ective MTD, [15]
proposes the instruction set and the address space layout randomization, [16] studies
the deceptive routing against jamming in multi-hop relay networks, and [17] uses
the Markov chain to model the MTD process and discusses the optimal strategy to
balance the defensive beneï¬t and the network service quality.

The previous two methods use the defensive deception to protect the system and
assets. To further gather threat information, the defender can implement honeypots to
lure attackers to conduct adversarial behaviors and reveal their TTPs in a controlled
and monitored environment. Previous works [18, 19] have investigated the adap-
tive honeypot deployment to eï¬€ectively engage attackers without their notices. The
authors in recent work [20] proposes a continuous-state Markov Decision Process
(MDP) model and focuses on the optimal timing of the attacker ejection.

Game-theoretic models are natural frameworks to capture the multistage inter-
action between attackers and defenders. Recently, game theory has been applied to
diï¬€erent sets of security problems, e.g., Stackelberg and signaling games for decep-
tion and proactive defenses [21, 6, 22, 23, 24, 16, 25, 26, 27], network games for
cyber-physical security [28, 29, 30, 31, 32, 33, 34, 35, 36, 37], dynamic games for
adaptive defense [38, 39, 40, 41, 3, 42, 43, 44, 45, 46], and mechanism design theory
for security [47, 48, 49, 50, 51, 52, 53, 54, 55].

Information asymmetry among the players in network security is a challenge to
deal with. The information asymmetry can be either leveraged or created by the
attacker or the defender for achieving a successful cyber deception. For example,
techniques such as honeynets [56, 22], moving target defense [6, 14, 3], obfuscation
[57, 58, 59, 60], and mix networks [61] have been introduced to create diï¬ƒculties
for attackers to map out the system information.

To overcome the created or inherent uncertainties of networks, many works have
studied the strategic learning in security games, e.g., Bayesian learning for un-
known adversarial strategies [62], heterogeneous and hybrid distributed learning
[46, 63], multiagent reinforcement learning for intrusion detection [64]. Moreover,
these learning schemes are combined to achieve better properties, e.g., distributed
Bayesian learning [65], Bayesian reinforcement learning [66], and distributed rein-
forcement learning [67].

1.2 Notation

Throughout the chapter, we use calligraphic letter A to deï¬ne a set and |A| as the
cardinality of the set. Let (cid:52)A represent the set of probability distributions over A.
If set A is discrete and ï¬nite, (cid:52)A := {p : A (cid:55)â†’ R+| (cid:205)
a âˆˆA p(a) = 1}, otherwise,
(cid:52)A := {p : A (cid:55)â†’ R+| âˆ«
a âˆˆA p(a) = 1}. Row player P1 is the defender (pronoun

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

5

â€˜sheâ€™) and P2 (pronoun â€˜heâ€™) is the user (or the attacker) who controls the column of
the game matrix. Both players want to maximize their own utilities. The indicator
function 1{x=y } equals one if x = y, and zero if x (cid:44) y.

1.3 Organization of the Chapter

The rest of the paper is organized as follows. In Section 2, we elaborate defen-
sive deception as a countermeasure of the adversarial deception under a multistage
setting where Bayesian learning is applied for the parameter uncertainty. Section
3 introduces a multistage MTD framework and the uncertainties of payoï¬€s result
in distributed learning schemes. Section 4 further considers reinforcement learn-
ing for environmental uncertainties under the honeypot engagement scenario. The
conclusion and discussion are presented in Section 5.

2 Bayesian Learning for Uncertain Parameters

Under the mild restrictive information structure, each playerâ€™ utility is completely
governed by a ï¬nite group of parameters which form his/her type. Each playerâ€™s
type characterizes all the uncertainties about this player during the game interaction,
e.g., the physical outcome, the payoï¬€ function, and the strategy feasibility, as an
equivalent utility uncertainty without loss of generality [68]. Thus, the revelation
of the type value directly results in a game of complete information. In the cyber
security scenario, a discrete type can distinguish either systems with diï¬€erent kinds
of vulnerabilities or attackers with diï¬€erent targets. The type can also be a continuous
random variable representing either the threat level or the security awareness level
[3, 4]. Since each player Pi takes actions to maximize his/her own type-dependent
utility, the other player Pj can form a belief to estimate Piâ€™s type based on the
observation of Piâ€™s action history. The utility optimization under the beliefs results in
the Perfect Bayesian Nash Equilibrium (PBNE) which generates new action samples
and updates the belief via the Bayesian rule. We plot the feedback Bayesian learning
process in Fig. 2 and elaborate each element in the following subsections based on
our previous work [5].

2.1 Type and Multistage Transition

Through adversarial deception techniques, attackers can disguise their subversive
actions as legitimate behaviors so that the defender P1 cannot judge whether a user
P2â€™s type Î¸2 âˆˆ Î˜2 := {Î¸g
2 . As a countermea-
sure, the defender can introduce the defensive deception so that the attacker cannot

2 or adversarial Î¸b

} is legitimate Î¸g

2, Î¸b

2

Linan Huang and Quanyan Zhu

6

Fig. 2 The feedback loop of
the Bayesian learning from
the initial stage k = 1 to
the terminal stage k = K.
Each player forms a belief
of the other playerâ€™s type
and persistently updates the
belief based on the actions
resulted from the PBNE
strategies which are the results
of Bayesian learning.

distinguish between a primitive system Î¸ L
1 , i.e., the
defender has a binary type Î¸1 âˆˆ Î˜1 := {Î¸ H
1 }. A sophisticated system is costly
yet deters attacks and causes damages to attackers. Thus, a primitive system can
disguise as a sophisticated one to draw the same threat level to attackers yet avoid
the implementation cost of sophisticated defense techniques.

1 and a sophisticated system Î¸ H
1 , Î¸ L

Many cyber networks contain hierarchical layers, and up-to-date attackers such as
Advanced Persistent Threats (APTs) aim to penetrate these layers and reach speciï¬c
targets at the ï¬nal stage as shown in Fig. 3.

Fig. 3: The multistage structure of APT kill chain is composed of reconnaissance,
initial compromise, privilege escalation, lateral movement, and mission execution.

i âˆˆ Ak

At stage k âˆˆ {0, 1, Â· Â· Â· , K }, Pi takes an action ak

i from a ï¬nite and discrete
set Ak
i . Both playersâ€™ actions become fully observable after applied and each action
does not directly reveal the private type. For example, both legitimate and adversarial
users can choose to access the sensor, and both primitive and sophisticated defenders
can choose to monitor the sensor. Both playersâ€™ actions up to stage k constitute the
history hk = {a0
i . Given history
hk at the current stage k, players at stage k + 1 obtain an updated history hk+1 = hk âˆª
2 . A state xk âˆˆ Xk at each stage k is the smallest
{ak
set of quantities that summarize information about actions in previous stages so that

} after the observation ak

} âˆˆ H k := (cid:206)2

2, Â· Â· Â· , akâˆ’1

1, Â· Â· Â· , akâˆ’1

(cid:206)kâˆ’1
Â¯k=0

1, ak

1, ak

A Â¯k

, a0

i=1

1

2

2

Common	InformationSet:	history	â„"or	state	ğ‘¥"Defenderâ€™s	Type	ğœƒ%SensePBNE	StrategyActUserâ€™s	Type	ğœƒ&PBNE	StrategyActUpdate	Attackerâ€™s		BeliefsEstimateUpdate	Defenderâ€™s	BeliefsEstimateSenseğ‘%"ğ‘&"ğ‘˜=0ğ‘˜=1ğ‘˜=ğ¾Initial compromisePhysical accessWeb phishingPrivilege escalationLateral movementMission completenessData collection and exfiltration Physical damage Social engineeringPrivate keyDatabaseSensorReconnaissance Insider threatsOSINTControllerStrategic Learning for Active, Adaptive, and Autonomous Cyber Defense

7

the initial state x0 âˆˆ X0 and the history at stage k uniquely determine xk through a
known state transition function f k, i.e., xk+1 = f k(xk, ak
), âˆ€k âˆˆ {0, 1, Â· Â· Â· , K âˆ’1}.
The state can represent the location of the user in the attack graph, and also other
quantities such as usersâ€™ privilege levels and status of sensor failures.

1, ak

2

: Ik
i

(cid:55)â†’ (cid:52)(Ak

A behavioral strategy Ïƒk

i ) maps Piâ€™s information set Ik

i âˆˆ Î£k
i at
i
stage k to a probability distribution over the action space Ak
i . At the initial stage 0,
since the only information available is the playerâ€™s type realization, the information
= Î˜i. The action is a realization of the behavioral strategy, or equivalently,
set I0
i
a sample drawn from the probability distribution Ïƒk
i ). With a slight abuse of
notation, we denote Ïƒk
i (ak
i |I k
i ) as the probability of Pi taking action ak
i given
i âˆˆ Ik
the available information I k
i .

i âˆˆ Ak

i (Â·|I k

2.2 Bayesian Update under Two Information Structure

Since the other playerâ€™s type is of private information, Pi forms a belief bk
(cid:55)â†’
(cid:52)(Î˜j), j (cid:44) i, on Pjâ€™s type using the available information Ik
i . Likewise, given
information I k
i âˆˆ Ik
i at stage k, Pi believes with a probability bk
i (Î¸ j |I k
i ) that Pj is
i : Î˜i (cid:55)â†’ (cid:52)Î˜j, âˆ€i, j âˆˆ {1, 2}, j (cid:44) i, is formed
of type Î¸ j âˆˆ Î˜j. The initial belief b0
based on an imperfect detection, side-channel information or the statistic estimation
resulted from past experiences.

i : Ik
i

= H k Ã— Î˜i, then players can update their

If the system has a perfect recall Ik
i

beliefs according to the Bayesian rule:

bk+1
i

(Î¸ j |hk âˆª {ak

i , ak

j }, Î¸i) =

Ïƒk
i (ak
(cid:205) Â¯Î¸ j âˆˆÎ˜ j Ïƒk

i |hk, Î¸i)Ïƒk
i (ak

j (ak
i |hk, Î¸i)Ïƒk

j |hk, Î¸ j)bk
j (ak

j |hk, Â¯Î¸ j)bk

i (Î¸ j |hk, Î¸i)

i ( Â¯Î¸ j |hk, Î¸i)

.

(1)

i based on the observation of the action ak

Here, Pi updates the belief bk
j . When
the denominator is 0, the history hk+1 is not reachable from hk, and a Bayesian
update does not apply. In this case, we let bk+1
i
= Xk Ã— Î˜i with the Markov property
If the information set is taken to be Ik
i
that Pr(xk+1|Î¸ j, xk, Â· Â· Â· , x1, x0, Î¸i) = Pr(xk+1|Î¸ j, xk, Î¸i), then the Bayesian update
between two consequent states is

j }, Î¸i) := b0

(Î¸ j |hk âˆª {ak

i (Î¸ j |Î¸i).

i , ak

i , ak

bk+1
i

(Î¸ j |xk+1, Î¸i) =

Pr(xk+1|Î¸ j, xk, Î¸i)bk
(cid:205) Â¯Î¸ j âˆˆÎ˜ j Pr(xk+1| Â¯Î¸ j, xk, Î¸i)bk

i (Î¸ j |xk, Î¸i)

i ( Â¯Î¸ j |xk, Î¸i)

.

(2)

The Markov belief update (2) can be regarded as an approximation of (1) us-
ing action aggregations. Unlike the history set H k, the dimension of the state set
|Xk | does not grow with the number of stages. Hence, the Markov approximation
signiï¬cantly reduces the memory and computational complexity.

8

Linan Huang and Quanyan Zhu

2.3 Utility and PBNE

Ã— Ak
2

: Xk Ã— Ak
1

i âˆˆ R with a known probability density function (cid:36)k

At each stage k, Piâ€™s stage utility Â¯Jk
Ã— Î¸1 Ã— Î¸2 Ã— R (cid:55)â†’ R
i
depends on both playersâ€™ types and actions, the current state xk âˆˆ Xk, and
an external noise wk
i . The
noise term models unknown or uncontrolled factors that can aï¬€ect the value
2, Î¸1, Î¸2) :=
of the stage utility. Denote the expected stage utility as Jk
Ewk

1, ak
i âˆ¼(cid:36) k
i
Given the type Î¸i âˆˆ Î˜i, the initial state xk0 âˆˆ Xk0, and both playersâ€™ strategies
Î£k
i from stage k0 to K, we can determine
k=k0
for Pi, i âˆˆ {1, 2}, by taking expectations over

Ïƒk0:K
i
the expected cumulative utility Uk0:K
i
the mixed-strategy distributions and the Piâ€™s belief on Pjâ€™s type, i.e.,

i |xk, Î¸i)]k=k0, Â·Â·Â· ,K âˆˆ (cid:206)K

2, Î¸1, Î¸2, wk

i ), âˆ€xk, ak

Â¯Jk
i (xk, ak

i (xk, ak

2, Î¸1, Î¸2.

:= [Ïƒk

i (ak

1, ak

1, ak

Uk0:K
i

(Ïƒk0:K
i

, Ïƒk0:K
j

, xk0, Î¸i) :=

K
(cid:213)

k=k0

EÎ¸ j âˆ¼bk

i ,ak

i âˆ¼Ïƒk

i ,ak

j âˆ¼Ïƒk
j

i (xk, ak
Jk

1, ak

2, Î¸1, Î¸2).

(3)

The attacker and the defender use the Bayesian update to reduce their uncertainties
on the other playerâ€™s type. Since their actions aï¬€ect the belief update, both players
at each stage should optimize their expected cumulative utilities concerning the
updated beliefs, which leads to the solution concept of PBNE in Deï¬nition 1.

Deï¬nition 1 Consider the two-person K-stage game with a double-sided incomplete
i , âˆ€k âˆˆ {0, Â· Â· Â· , K }, an expected cumulative
information, a sequence of beliefs bk
in (3), and a given scalar Îµ â‰¥ 0. A sequence of strategies Ïƒâˆ—,0:K
utility U0:K
âˆˆ
i
(cid:206)K
k=0 Î£k
i is called Îµ-perfect Bayesian Nash equilibrium for player i if the following

i

two conditions are satisï¬ed.
C1: Belief consistency: under the strategy pair (Ïƒâˆ—,0:K

i at each stage k = 0, Â· Â· Â· , K satisï¬es (2).
bk

, Ïƒâˆ—,0:K
2

), each playerâ€™s belief

1

C2: Sequential rationality: for all given initial state xk0 âˆˆ Xk0 at every initial stage
1, âˆ€Ïƒk0:K

k0 âˆˆ {0, Â· Â· Â· , K }, âˆ€Ïƒk0:K

k=0 Î£k
2 ,

k=0 Î£k

âˆˆ (cid:206)K

âˆˆ (cid:206)K

1

2

Uk0:K
1
Uk0:K
2

(Ïƒâˆ—,k0:K
1
(Ïƒâˆ—,k0:K
1

, Ïƒâˆ—,k0:K
2
, Ïƒâˆ—,k0:K
2

, xk0, Î¸1) + Îµ â‰¥ Uk:K
, xk0, Î¸2) + Îµ â‰¥ Uk:K

1

2

(Ïƒk0:K
1
(Ïƒâˆ—,k0:K
1

, Ïƒâˆ—,k0:K
2
, Ïƒk0:K
2

, xk0, Î¸1),
, xk0, Î¸2).

(4)

When Îµ = 0, the equilibrium is called Perfect Bayesian Nash Equilibrium (PBNE).

Solving PBNE is challenging. If the type space is discrete and ï¬nite, then given
each playerâ€™s belief at all stages, we can solve the equilibrium strategy satisfying
condition C2 via dynamic programming and a bilinear program. Next, we update
the belief at each stage based on the computed equilibrium strategy. We iterate the
above update on the equilibrium strategy and belief until they satisfy condition C1
as demonstrated in [5]. If the type space is continuous, then the Bayesian update can
be simpliï¬ed into a parametric update under the conjugate prior assumption. Next,

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

9

the parameter after each belief update can be assimilated into the backward dynamic
programming of equilibrium strategy with an expanded state space [4]. Although
no iterations are required, the inï¬nite dimension of continuous type space limits the
computation to two by two game matrices.

We apply the above framework and analysis to a case study of Tennessee Eastman
(TE) process and investigate both playersâ€™ multistage utilities under the adversarial
and the defensive deception in Fig. 4. Some insights are listed as follows.

Fig. 4 The cumulative util-
ities of the attacker and the
defender under the complete
information, the adversarial
deception, and the defensive
deception. The complete infor-
mation refers to the scenario
where both players know the
other playerâ€™s type. The de-
ception with the H-type or the
L-type means that the attacker
knows the defenderâ€™s type to
be Î¸ H
1 , respectively,
yet the defender has no infor-
mation about the userâ€™s type.
The double-sided deception
indicates that both players do
not know the other playerâ€™s
type.

1 or Î¸ L

First, the defenderâ€™s payoï¬€s under type Î¸ H

1 can increase as much as 56% than
those under type Î¸ L
1 . Second, the defender and the attacker receive the highest and
the lowest payoï¬€, respectively, under the complete information. When the attacker
introduces deceptions over his type, the attackerâ€™s utility increases and the system
utility decreases. Third, when the defender adopts defensive deceptions to introduce
double-sided incomplete information, we ï¬nd that the decrease of system utilities is
reduced by at most 64%, i.e., the decrease of system utilities changes from $55, 570
to $35, 570 under the internal state and type Î¸ H
1 . The double-sided incomplete
information also brings lower utilities to the attacker than the one-sided adversarial
deception. However, the system utility under the double-sided deception is still
less than the complete information case, which concludes that acquiring complete
information of the adversarial user is the most eï¬€ective defense. However, if the
complete information cannot be obtained, the defender can mitigate her loss by
introducing defensive deceptions.

150000170000190000210000230000250000270000290000310000330000External	State	Internal	StateDefender's	Utility	($)Complete	Information	with	the	H-TypeComplete	Information	with	the	L-TypeDeception	with	the	H-TypeDeception	with	the	L-TypeDouble	Deception	with	the	H-TypeDouble	Deception	with	the	L-Type400006000080000100000120000140000160000180000External	State	Internal	StateAttacker's	Utility	($)Complete	Information	with	the	H-TypeComplete	Information	with	the	L-TypeDeception	with	the	H-TypeDeception	with	the	L-TypeDouble	Deception	with	the	H-TypeDouble	Deception	with	the	L-Type10

Linan Huang and Quanyan Zhu

3 Distributed Learning for Uncertain Payoï¬€s

In the previous section, we study known attacks and systems that adopt cyber decep-
tion to conceal their types. We assume common knowledge of the prior probability
distribution of the unknown type, and also a common observation of either the ac-
tion history or the state at each stage. Thus, each player can use Bayesian learning
to reduce the other playerâ€™s type uncertainty.

In this section, we consider unknown attacks in the MTD game stated in [6]
where each player has no information on the past actions of the other player, and
the payoï¬€ functions are subject to noises and disturbances with unknown statis-
tical characteristics. Without information sharing between players, the learning is
distributed.

3.1 Static Game Model of MTD

We consider a system of N layers yet focus on the static game at layer l âˆˆ {1, 2, Â· Â· Â· , N }
because the technique can be employed at each layer of the system independently.
At layer l, Vl := {vl,1, vl,2, Â· Â· Â· , vl,nl } is the set of nl system vulnerabilities that an
attacker can exploit to compromise the system. Instead of a static conï¬guration at
layer l, the defender can choose to change her conï¬guration from a ï¬nite set of
ml feasible conï¬gurations Cl := {cl,1, cl,2, Â· Â· Â· , cl,ml }. Diï¬€erent conï¬gurations result
in diï¬€erent subsets of vulnerabilities among Vl, which are characterized by the
vulnerability map Ï€l : Cl â†’ 2Vl . We call Ï€l(cl, j) the attack surface at stage l under
conï¬guration cl, j.

Suppose that for each vulnerability vl, j, the attacker can take a corresponding
attack al, j = Î³l(vl, j) from the action set Al := {al,1, al,2, Â· Â· Â· , al,nl }. Attack action
al, j is only eï¬€ective and incurs a bounded cost Di j âˆˆ R+ when the vulnerability
vl, j = Î³âˆ’1
(al, j) exists in the current attack surface Ï€l(cl,k). Thus, the damage caused
l
by the attacker at stage l can be represented as

(cid:40)

rl(al, j, cl,i) =

Di j, Î³âˆ’1
0,

l

otherwise

(al, j) âˆˆ Ï€l(cl,k)

.

(5)

Since vulnerabilities are inevitable in a modern computing system, we can ran-
domize the conï¬guration and make it diï¬ƒcult for the attacker to learn and locate the
system vulnerability, which naturally leads to the mixed strategy equilibrium solution
concept of the game. At layer l, the defenderâ€™s strategy fl = { fl,1, fl,2, Â· Â· Â· , fl,ml } âˆˆ
(cid:52)Cl assigns probability fl, j âˆˆ [0, 1] to conï¬guration cl, j while the attackerâ€™s strategy
gl := {gl,1, gl,2, Â· Â· Â· , gl,nl } âˆˆ (cid:52)Al assigns probability gl,i âˆˆ [0, 1] to attack action
al,i. The zero-sum game possesses a mixed strategy saddle-point equilibrium (SPE)
l âˆˆ (cid:52)Cl, gâˆ—
(fâˆ—

l âˆˆ (cid:52)Al), and a unique game value r(fâˆ—
l , gâˆ—

l ), i.e.,
l ), âˆ€fl âˆˆ (cid:52)Cl, gl âˆˆ (cid:52)Al,

l , gl) â‰¤ rl(fâˆ—

l ) â‰¤ rl(fl, gâˆ—

rl(fâˆ—

l , gâˆ—

(6)

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

where the expected cost rl is given by

rl(fl, gl) := Efl,gl rl =

nl(cid:213)

ml(cid:213)

k=1

h=1

fl,hgl,krl(al,k, cl,h).

11

(7)

We illustrate the multistage MTD game in Fig. 5 and focus on the ï¬rst layer with
two available conï¬gurations C1 := {c1,1, c1,2} in the blue box. Conï¬guration c1,1 in
Fig. 5a has an attack surface Ï€1(c1,1) = {v1,1, v1,2} while conï¬guration c1,2 in Fig. 5b
reveals two vulnerabilities v1,2, v1,3 âˆˆ Ï€1(c1,2). Then, if the attacker takes action a1,1
and the defender changes the conï¬guration from c1,1 to c1,2, the attack is deterred at
the ï¬rst layer.

(a) Attack surface Ï€1(c1,1) = {v1,1, v1,2 }.

(b) Attack surface Ï€1(c1,2) = {v1,2, v1,3 }.

Fig. 5: Given a static conï¬guration c1,1, an attacker can succeed in reaching the
resources at deeper layers by forming an attack path v1,1 â†’ v2,2 â†’ Â· Â· Â· . A change
of conï¬guration to c1,2 can thwart the attacker at the ï¬rst layer.

3.2 Distributed Learning

In practical cybersecurity domain, the payoï¬€ function rl is subjected to noises of un-
known distributions. Then, each player reduces the payoï¬€ uncertainty by repeatedly
observing the payoï¬€ realizations during the interaction with the other player. We use
subscript t to denote the strategy or cost at time t.

There is no communication at any time between two agents due to the non-
cooperative environment, and the conï¬guration and attack action are kept private, i.e.,
each player cannot observe the other playerâ€™s action. Thus, each player independently
chooses action cl,t âˆˆ Cl or al,t âˆˆ Al to estimate the average risk of the system
l,t : Al â†’ R+ at layer l. Based on the estimated average risk
l,t : Cl â†’ R+ and Ë†r A
Ë†r S

v1,1v1,2v1,3v2,1v2,2v2,3v3,1v3,2v3,3v4,1v4,2v4,3c1,1c2,1c3,1c4,2v1,1v1,2v1,3v2,1v2,2v2,3v3,1v3,2v3,3v4,1v4,2v4,3c1,2c2,1c3,1c4,212

Linan Huang and Quanyan Zhu

Ë†r S
l,t and the previous policy fl,t , the defender can obtain her updated policy fl,t+1.
Likewise, the attacker can also update his policy gl,t+1 based on Ë†r A
l,t and gl,t . The new
policy pair (fl,t+1, gl,t+1) determines the next payoï¬€ sample. The entire distributed
learning feedback loop is illustrated in Fig. 6 where we distinguish the adversarial
and defensive learning in red and green, respectively.

Fig. 6 The distributed learn-
ing of the multistage MTD
game at layer l. Adversar-
ial learning in red does not
share information with defen-
sive learning in green. The
distributed learning fashion
means that the learning rule
does not depend on the other
playerâ€™s action, yet the ob-
served payoï¬€ depends on both
playersâ€™ actions.

In particular, players update their estimated average risks based on the payoï¬€
t be the

sample rl,t under the chosen action pair (cl,t, al,t ) as follows. Let ÂµS
payoï¬€ learning rate for the system and attacker, respectively.

t and ÂµA

l,t+1(cl,h) = Ë†r S
Ë†r S
l,t+1(al,h) = Ë†r A
Ë†r A

l,t (cl,h) + ÂµS
l,t (al,h) + ÂµA

t 1{cl, t =cl, h }(rl,t âˆ’ Ë†r S
t 1{al, t =al, h }(rl,t âˆ’ Ë†r A

l,t (cl,h)),
l,t (al,h)).

(8)

The indicators in (8) mean that both players only update the estimate average risk of
the current action.

3.2.1 Security versus Usability

Frequent conï¬guration changes may achieve the complete security yet also decrease
the system usability. To quantify the tradeoï¬€ between the security and the usability,
we introduce the switching cost of policy from fl,t to fl,t+1 as their entropy:

l,t :=
RS

ml(cid:213)

h=1

fl,h,t+1 ln

(cid:18) fl,h,t+1
fl,h,t

(cid:19)

.

(9)

Then, the total cost at time t combines the expected cost with the entropy penalty in
a ratio of (cid:15) S
l,t is high, the policy changes less and is more usable, yet may
cause a large loss and be less rational.

l,t . When (cid:15) S

Utility	Uncertainty	ğ‘Ÿ",$ğ•”",$,ğ•’",$of	Layer	ğ‘™at	time	ğ‘¡Defenderâ€™s	Risk	LearningSenseShift	Attack	SurfaceActğ‘Ÿ",$Attackerâ€™s	Risk	LearningSenseğ‘ŸÌ‚",$*+,(ğ•”",$)ğ‘ŸÌ‚",$*+/ (ğ•’",$)ğ’‡",$*+Exploit	Different	VulnerabilitiesActUpdate	Attack	Policy	EstimateUpdate	Configuration	Policy	Estimateğ’ˆ",$*+ğ•”",$ğ•’",$Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

(SP) :

sup
fl, t +1 âˆˆ(cid:52)Cl

âˆ’

ml(cid:213)

h=1

fl,h,t+1 Ë†r S

l,t (cl,h) âˆ’ (cid:15) S

l,t RS
l,t .

A similar learning cost is introduced for the attacker:

(AP) :

sup
gl, t +1 âˆˆ(cid:52)Al

âˆ’

nl(cid:213)

h=1

gl,h,t+1 Ë†r A

l,t (al,h) âˆ’ (cid:15) A
l,t

nl(cid:213)

h=1

gl,h,t+1 ln

(cid:18) gl,h,t+1
gl,h,t

(cid:19)

.

13

(10)

(11)

At any time t + 1, we are able to obtain the equilibrium strategy ( fl,h,t+1, gl,h,t+1)
l,t ) in closed form of the previous strategy and the estimated

and game value (W S
average risk at time t as follows.

l,t, W A

fl,h,t+1 =

âˆ’

Ë†rl, t (cl, h )
(cid:15) S
l, t

fl,h,t e

âˆ’

Ë†rl, t (cl, h(cid:48) )
(cid:15) S
l, t

fl,h(cid:48),t e

ml(cid:213)

h(cid:48)=1

,

gl,h,t+1 =

âˆ’

Ë†rl, t (al, h )
(cid:15) A
l, t

gl,h,t e

âˆ’

Ë†rl, t (al, h(cid:48) )
(cid:15) A
l, t

gl,h(cid:48),t e

nl(cid:213)

h(cid:48)=1

,

W S
l,t

= (cid:15) S

l,t ln

âˆ’

Ë†rl, t (cl, h )
(cid:15) S
l, t

(cid:33)

,

fl,h,t e

(cid:32) ml(cid:213)

h=1

W A
l,t

= (cid:15) A

l,t ln

(cid:33)

âˆ’

Ë†rl, t (al, h )
(cid:15) A
l, t

.

gl,h,t e

(cid:32) nl(cid:213)

h=1

(12)

3.2.2 Learning Dynamics and ODE Counterparts

The closed form of policy leads to the following learning dynamics with learning
rates Î»S

l,t, Î» A

l,t âˆˆ [0, 1].

fl,h,t+1 = (1 âˆ’ Î»S

l,t ) fl,h,t + Î»S

l,t

gl,h,t+1 = (1 âˆ’ Î» A

l,t )gl,h,t + Î» A

l,t

âˆ’

Ë†rl, t (cl, h )
(cid:15) S
l, t

fl,h,t e

âˆ’

Ë†rl, t (cl, h(cid:48) )
(cid:15) S
l, t

fl,h(cid:48),t e

ml(cid:213)

h(cid:48)=1

âˆ’

Ë†rl, t (al, h )
(cid:15) A
l, t

gl,h,t e

âˆ’

Ë†rl, t (al, h(cid:48) )
(cid:15) A
l, t

gl,h(cid:48),t e

nl(cid:213)

h(cid:48)=1

,

.

(13)

= 1, Î» A
l,t

= 1, (13) is the same as (12). According to the stochastic approxima-
If Î»S
l,t
tion theory, the convergence of the policy and the average risk requires the learning
rates Î» A
l,t to satisfy the regular condition of convergency in Deï¬nition
2.

l,t, ÂµA

l,t, ÂµS

l,t, Î»S

Deï¬nition 2 A number sequence {xt }, t = 1, 2, Â· Â· Â· , is said to satisfy the regular
condition of convergency if

14

Linan Huang and Quanyan Zhu

âˆ
(cid:213)

t=1

xt = +âˆ,

âˆ
(cid:213)

t=1

(xt )2 < +âˆ.

(14)

The coupled dynamics of the payoï¬€ learning (8) and policy learning(13) converge to
their Ordinary Diï¬€erential Equations (ODEs) counterparts in system dynamics (15)
and attacker dynamics (16), respectively. Let ecl, h âˆˆ (cid:52)Cl, eal, h âˆˆ (cid:52)Al be vectors of
proper dimensions with the h-th entry being 1 and others being 0.

fl,h,t = fl,h,t

âˆ’

Ë†rl, t (cl, h )
(cid:15) S
l, t

e

ml(cid:213)

âˆ’

fl,h(cid:48),t e

Ë†rl, t (cl, h(cid:48) )
(cid:15) S
l, t

(cid:169)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:171)

(cid:170)
(cid:174)
(cid:174)
âˆ’ 1
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:172)
l,t+1(cl,h), cl,h âˆˆ Cl.

h(cid:48)=1
l,t (cl,h) = âˆ’rl,t (ecl, h, gl,t ) âˆ’ Ë†r S
Ë†r S

, h = 1, 2, Â· Â· Â· , ml,

d
dt

d
dt

âˆ’

e

Ë†rl, t (al, h )
(cid:15) A
l, t

gl,h,t+1 = gl,h,t

(cid:169)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:171)
l,t+1(al,h) = rl,t (fl,t, eal, h ) âˆ’ Ë†r A
Ë†r A

(cid:170)
(cid:174)
(cid:174)
âˆ’ 1
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:172)
l,t+1(al,h), al,h âˆˆ Al.

Ë†rl, t (al, h(cid:48) )
(cid:15) A
l, t

gl,h(cid:48),t e

nl(cid:213)

h(cid:48)=1

âˆ’

, h = 1, 2, Â· Â· Â· , nl,

d
dt

d
dt

(15)

(16)

We can show that the SPE of the game is the steady state of the ODE dynamics
in (15), (16), and the interior stationary points of the dynamics are the SPE of the
game [6].

3.2.3 Heterogeneous and Hybrid Learning

The entropy regulation terms in (10) and (11) result in a closed form of strategies
and learning dynamics in (13). Without the closed form, distributed learners can
adopt general learning schemes which combine the payoï¬€ and the strategy update
as stated in [46]. Speciï¬cally, algorithm CRL0 mimics the replicator dynamics and
updates the strategy according to the current sample value of the utility. On the
other hand, algorithm CRL1 updates the strategy according to a soft-max function
of the estimated utilities so that the most rewarding policy get reinforced and will
be picked with a higher probability. The ï¬rst algorithm is robust yet ineï¬ƒcient, and
the second one is fragile yet eï¬ƒcient. Moreover, players are not obliged to adopt
the same learning scheme at diï¬€erent time. The heterogeneous learning focuses on
diï¬€erent players adopting diï¬€erent learning schemes [46], while hybrid learning
means that players can choose diï¬€erent learning schemes at diï¬€erent times based
on their rationalities and preferences [63]. According to stochastic approximation

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

15

techniques, these learning schemes with random updates can be studied using their
deterministic ODE counterparts.

4 Reinforcement Learning for Uncertain Environments

This section considers uncertainties on the entire environment, i.e., the state transi-
tion, the sojourn time, and the investigation payoï¬€, in the active defense scenario of
the honeypot engagement [7]. We use the Semi-Markov Decision Process (SMDP) to
capture these environmental uncertainties in the continuous time system. Although
the attackerâ€™s duration time is continuous at each honeypot, the defenderâ€™s engage-
ment action is applied at a discrete time epoch. Based on the observed samples
at each decision epoch, the defender can estimate the environment elements deter-
mined by attackersâ€™ characteristics, and use reinforcement learning methods to obtain
the optimal policy. We plot the entire feedback learning structure in Fig. 7. Since
the attacker should not identify the existence of the honeypot and the defenderâ€™s
engagement actions, he will not take actions to jeopardize the learning.

Fig. 7 The feedback structure
of reinforcement learning
methods on SMDP. The
red background means that
the attackerâ€™s characteristics
determine the environmental
uncertainties and the samples
observed in the honeynet. The
attacker is not involved in
parts of the green background.
The learning scheme in Fig.
7 extends the one in Section
3 to consider a continuous
time elapse and multistage
transitions.

4.1 Honeypot Network and SMDP Model

The honeypots form a network to emulate a production system. From an attackerâ€™s
viewpoint, two network structures are the same as shown in Fig. 8. Based on the
network topology, we introduce the continuous-time inï¬nite-horizon discounted
SMDPs, which can be summarized by the tuple {t âˆˆ [0, âˆ), S, A(sj), tr(sl |sj, aj),
z(Â·|sj, aj, sl), rÎ³(sj, aj, sl), Î³ âˆˆ [0, âˆ)}. We illustrate each element of the tuple through
a 13-state example in Fig. 9.

Discrete-Time	Decision	Process	ğ‘˜=0,1,â€¦Sampleğ‘ Ì…),	ğœÌ…),	ğ‘Ÿ,-),ğ‘Ÿ.-)based	on	attackersâ€™	characteristicsSenseUpdate	ğ‘„)(ğ‘ Ì…),ğ‘))EstimateChoose	action	ğ‘)to	balance	exploration	and	exploitationActContinuous-Time	System	ğ‘¡âˆˆ[0,âˆ)16

Linan Huang and Quanyan Zhu

Fig. 8: The honeynet in red emulates and shares the same structure as the targeted
production system in green.

Each node in Fig. 9 represents a state si âˆˆ S, i âˆˆ {1, 2, Â· Â· Â· , 13}. At time t âˆˆ
[0, âˆ), the attacker is either at one of the honeypot node denoted by state si âˆˆ
S, i âˆˆ {1, 2, Â· Â· Â· , 11}, at the normal zone s12, or at a virtual absorbing state s13
once attackers are ejected or terminate on their own. At each state si âˆˆ S, the
defender can choose an action ai âˆˆ A(si). For example, at honeypot nodes, the
defender can conduct action aE to eject the attacker, action aP to purely record the
attackerâ€™s activities, low-interactive action aL, or high-interactive action aH , i.e.,
A(si) := {aE, aP, aL, aH }, i âˆˆ {1, Â· Â· Â· , N }. The high-interactive action is costly to
implement yet can both increases the probability of a longer sojourn time at honeypot
ni, and reduces the probability of attackers penetrating the normal system from ni
if connected. If the attacker resides in the normal zone either from the beginning
or later through the pivot honeypots, the defender can choose either action aE to
eject the attacker immediately, or action aA to attract the attacker to the honeynet
by generating more deceptive inbound and outbound traï¬ƒcs in the honeynet, i.e.,
A(s12) := {aE, aA}. Based on the current state sj âˆˆ S and the defenderâ€™s action
aj âˆˆ A(sj), the attacker transits to state sl âˆˆ S with probability tr(sl |sj, aj) and
the sojourn time at state sj is a continuous random variable with probability density
z(Â·|sj, aj, sl). Once the attacker arrives at a new honeypot ni, the defender dynamically
applies an interaction action at honeypot ni from A(si) and keeps interacting with
the attacker until she transits to the next honeypot. If the defender changes the action
before the transition, the attacker may be able to detect the change and become

Access PointInternet / CloudFirewallSwitchSwitchAccess PointInternet / CloudIntrusion DetectionHoneypot192.168.1.10HoneywallGatewayRouterServerHoneypot192.168.1.45Data BaseComputer NetworkServerWork Station192.168.1.55Data Base192.168.1.90HoneywallSensorActuatorHoneypotHoneypot NetworkHoneypotHoneypotHoneynetProduction SystemsStrategic Learning for Active, Adaptive, and Autonomous Cyber Defense

17

Fig. 9 Honeypots emulate
diï¬€erent components of the
production system. Actions
aE, aP, aL, aH are denoted
in red, blue, purple, and green,
respectively. The size of node
ni represents the state value
v(si ), i âˆˆ {1, 2, Â· Â· Â· , 11}.

aware of the honeypot. Since the decision is made at the time of transition, we can
transform the above continuous time model on horizon t âˆˆ [0, âˆ) into a discrete
decision model at decision epoch k âˆˆ {0, 1, Â· Â· Â· , âˆ}. The time of the attackerâ€™s k th
transition is denoted by a random variable T k, the landing state is denoted as sk âˆˆ S,
and the adopted action after arriving at sk is denoted as ak âˆˆ A(sk).

The defender gains an investigation reward by engaging and analyzing the attacker
in the honeypot. To simplify the notation, we segment the investigation reward during
time t âˆˆ [0, âˆ) into ones at discrete decision epochs T k, k âˆˆ {0, 1, Â· Â· Â· , âˆ}. When
Ï„ âˆˆ [T k, T k+1] amount of time elapses at stage k, the defenderâ€™s investigation reward
r(sk, ak, sk+1, T k, T k+1, Ï„) = r1(sk, ak, sk+1)1{Ï„=0} + r2(sk, ak, T k, T k+1, Ï„), at time Ï„
of stage k, is the sum of two parts. The ï¬rst part is the immediate cost of applying
engagement action ak âˆˆ A(sk) at state sk âˆˆ S and the second part is the reward
rate of threat information acquisition minus the cost rate of persistently generating
deceptive traï¬ƒcs. Due to the randomness of the attackerâ€™s behavior, the information
acquisition can also be random, thus the actual reward rate r2 is perturbed by an
additive zero-mean noise wr . As the defender spends longer time interacting with
attackers, investigating their behaviors and acquires better understandings of their
targets and TTPs, less new information can be extracted. In addition, the same
intelligence becomes less valuable as time elapses due to the timeliness. Thus,
we use a discounted factor of Î³ âˆˆ [0, âˆ) to penalize the decreasing value of the
investigation reward as time elapses.

The defender aims at a policy Ï€ âˆˆ Î  which maps state sk âˆˆ S to action ak âˆˆ A(sk)

to maximize the long-term expected utility starting from state s0, i.e.,

u(s0, Ï€) = E[

âˆ
(cid:213)

âˆ« T k+1

k=0

T k

eâˆ’Î³(Ï„+T k )(r(Sk, Ak, Sk+1, T k, T k+1, Ï„) + wr )dÏ„].

(17)

At each decision epoch, the value function v(s0) = supÏ€ âˆˆÎ  u(s0, Ï€) can be repre-

sented by dynamic programming, i.e.,

ClientsServerSwitchNormal ZoneComputerNetworkEmulatedSensorsEmulatedDatabase12111012345679813Absorbing State18

Linan Huang and Quanyan Zhu

v(s0) = sup

E[

âˆ« T 1

a0 âˆˆA(s0)

T 0

eâˆ’Î³(Ï„+T 0)r(s0, a0, S1, T 0, T 1, Ï„)dÏ„ + eâˆ’Î³T 1

v(S1)].

(18)

We assume a constant reward rate r2(sk, ak, T k, T k+1, Ï„) = Â¯r2(sk, ak) for simplicity.

Then, (18) can be transformed into an equivalent MDP form, i.e., âˆ€s0 âˆˆ S,

v(s0) = sup

(cid:213)

tr(s1|s0, a0)(rÎ³(s0, a0, s1) + zÎ³(s0, a0, s1)v(s1)),

(19)

a0 âˆˆA(s0)

s1 âˆˆS
where zÎ³(s0, a0, s1) := âˆ« âˆ
0 eâˆ’Î³Ï„ z(Ï„|s0, a0, s1)dÏ„ âˆˆ [0, 1] is the Laplace transform of
the sojourn probability density z(Ï„|s0, a0, s1) and the equivalent reward rÎ³(s0, a0, s1)
:= r1(s0, a0, s1) + Â¯r2(s0,a0)
(1 âˆ’ zÎ³(s0, a0, s1)) âˆˆ [âˆ’mc, mc] is assumed to be bounded
by a constant mc.

Î³

Deï¬nition 3 There exists constants Î¸ âˆˆ (0, 1) and Î´ > 0 such that

tr(s1|s0, a0)z(Î´|s0, a0, s1) â‰¤ 1 âˆ’ Î¸, âˆ€s0 âˆˆ S, a0 âˆˆ A(s0).

(20)

(cid:213)

s1 âˆˆS

The right-hand side of (18) is a contraction mapping under the regulation
condition in Deï¬nition 3. Then, we can ï¬nd the unique optimal policy Ï€âˆ— =
arg maxÏ€ âˆˆÎ  u(s0, Ï€) by value iteration, policy iteration or linear programming. Fig.
9 illustrates the optimal policy and the state value by the color and the size of the
node, respectively. In the example scenario, the honeypot of database n10 and sensors
n11 are the main and secondary targets of the attacker, respectively. Thus, defenders
can obtain a higher investigation reward when they manage to engage the attacker in
these two honeypot nodes with a larger probability and for a longer time. However,
instead of naively adopting high interactive actions, a savvy defender also balances
the high implantation cost of aH . Our quantitative results indicate that the high in-
teractive action should only be applied at n10 to be cost-eï¬€ective. On the other hand,
although the bridge nodes n1, n2, n8 which connect to the normal zone n12 do not
contain higher investigation rewards than other nodes, the defender still takes action
aL at these nodes. The goal is to either increase the probability of attracting attackers
away from the normal zone or reduce the probability of attackers penetrating the
normal zone from these bridge nodes.

4.2 Reinforcement Learning of SMDP

The absent knowledge of the attackerâ€™s characteristics results in environmental
uncertainty of the investigation reward, the attackerâ€™s transition probability, and
the sojourn distribution. We use Q-learning algorithm to obtain the optimal en-
gagement policy based on the actual experience of the honeynet interactions, i.e.,
âˆ€ Â¯sk âˆˆ S, âˆ€ak âˆˆ A( Â¯sk),

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

Qk+1( Â¯sk, ak) =(1 âˆ’ Î±k( Â¯sk, ak))Qk( Â¯sk, ak) + Î±k( Â¯sk, ak)[ Â¯r1( Â¯sk, ak, Â¯sk+1)

+ Â¯r2( Â¯sk, ak)

(1 âˆ’ eâˆ’Î³ Â¯Ï„ k )
Î³

âˆ’ eâˆ’Î³ Â¯Ï„ k

max
a(cid:48) âˆˆA( Â¯sk+1)

Qk( Â¯sk+1, a(cid:48))],

19

(21)

where Î±k( Â¯sk, ak) âˆˆ (0, 1) is the learning rate, Â¯sk, Â¯sk+1 are the observed states at
stage k and k + 1, Â¯r1, Â¯r2 is the observed investigation rewards, and Â¯Ï„k is the observed
sojourn time at state sk. When the learning rate satisï¬es the condition of convergency
in Deï¬nition 2, i.e., (cid:205)âˆ
k=0(Î±k(sk, ak))2 < âˆ, âˆ€sk âˆˆ S, âˆ€ak âˆˆ
A(sk), and all state-action pairs are explored inï¬nitely, maxa(cid:48) âˆˆA(sk ) Qâˆ(sâˆ, a(cid:48)), in
(21) converges to value v(sk) with probability 1.

k=0 Î±k(sk, ak) = âˆ, (cid:205)âˆ

At each decision epoch k âˆˆ {0, 1, Â· Â· Â· }, the action ak is chosen according to the (cid:15)-
greedy policy, i.e., the defender chooses the optimal action arg maxa(cid:48) âˆˆA(sk ) Qk(sk, a(cid:48))
with a probability 1 âˆ’ (cid:15), and a random action with a probability (cid:15). Note that the
exploration rate (cid:15) âˆˆ (0, 1] should not be too small to guarantee suï¬ƒcient samples
of all state-action pairs. The Q-learning algorithm under a pure exploration policy
(cid:15) = 1 still converges yet at a slower rate.

Fig. 10 One instance of Q-
learning on SMDP where the
x-axis shows the sojourn time
and the y-axis represents the
state transition. The chosen
actions aE, aP, aL, aH are
denoted in red, blue, purple,
and green, respectively.

In our scenario, the defender knows the reward of ejection action aA and v(s13) =
0, thus does not need to explore action aA to learn it. We plot one learning trajectory
of the state transition and sojourn time under the (cid:15)-greedy exploration policy in
Fig. 10, where the chosen actions aE, aP, aL, aH are denoted in red, blue, purple,
and green, respectively. If the ejection reward is unknown, the defender should be
restrictive in exploring aA which terminates the learning process. Otherwise, the
defender may need to engage with a group of attackers who share similar behaviors
to obtain suï¬ƒcient samples to learn the optimal engagement policy.

In particular, we choose Î±k(sk, ak) =

, âˆ€sk âˆˆ S, âˆ€ak âˆˆ A(sk), to
guarantee the asymptotic convergence, where kc âˆˆ (0, âˆ) is a constant parameter
and k {sk,ak } âˆˆ {0, 1, Â· Â· Â· } is the number of visits to state-action pair {sk, ak } up to
stage k. We need to choose a proper value of kc to guarantee a good numerical

kc
k{s k , ak }âˆ’1+kc

2.48992.49942.50892.51842.52792.5374Time10412345678910111213State20

Linan Huang and Quanyan Zhu

performance of convergence in ï¬nite steps as shown in Fig. 11a. We shift the green
and blue lines vertically to avoid the overlap with the red line and represent the
corresponding theoretical values in dotted black lines. If kc is too small as shown
in the red line, the learning rate decreases so fast that new observed samples hardly
update the Q-value and the defender may need a long time to learn the right value.
However, if kc is too large as shown in the green line, the learning rate decreases
so slow that new samples contribute signiï¬cantly to the current Q-value. It causes a
large variation and a slower convergence rate of maxa(cid:48) âˆˆA(s12) Qk(s12, a(cid:48)).

We show the convergence of the policy and value under kc = 1, (cid:15) = 0.2, in
the video demo (See URL: https://bit.ly/2QUz3Ok). In the video, the color of each
node nk distinguishes the defenderâ€™s action ak at state sk and the size of the node
is proportional to maxa(cid:48) âˆˆA(sk ) Qk(sk, a(cid:48)) at stage k. To show the convergence, we
decrease the value of (cid:15) gradually to 0 after 5000 steps. Since the convergence
trajectory is stochastic, we run the simulation for 100 times and plot the mean and
the variance of Qk(s12, aP) of state s12 under the optimal policy Ï€(s12) = aP in Fig.
11. The mean in red converges to the theoretical value in about 400 steps and the
variance in blue reduces dramatically as step k increases.

(a) The convergence rate under diï¬€erent val-
ues of kc .

(b) The evolution of the mean and the vari-
ance of Qk (s12, aP ).

Fig. 11: Convergence results of Q-learning over SMDP.

5 Conclusion and Discussion

This chapter has introduced three defense schemes, i.e., defensive deception to
detect and counter adversarial deception, feedback-driven Moving Target Defense
(MTD) to increase the attackerâ€™s probing and reconnaissance costs, and adaptive
honeypot engagement to gather fundamental threat information. These schemes
satisfy the Principle of 3A Defense as they actively protect the system prior to
the attack damages, provide strategic defenses autonomously, and apply learning to
adapt to uncertainty and changes. These schemes possess three progressive levels

01234567Step k104Value01002003004005006007008009001000-7-6-5-4-3-2-10123VarianceMeanTheoretical ValueStrategic Learning for Active, Adaptive, and Autonomous Cyber Defense

21

of information restrictions, which lead to diï¬€erent strategic learning schemes to
estimate the parameter, the payoï¬€, and the environment. All these learning schemes,
however, have a feedback loop to sense samples, estimate the unknowns, and take
actions according to the estimate. Our work lays a solid foundation for strategic
learning in active, adaptive, autonomous defenses under incomplete information and
leads to the following challenges and future directions.

First, multi-agent learning in non-cooperative environments is challenging due
to the coupling and interaction between these heterogeneous agents. The learning
results depend on all involving agents yet other playersâ€™ behaviors, levels of ratio-
nality, and learning schemes are not controllable and may change abruptly. More-
over, as attackers become aware of the active defense techniques and the learning
scheme under incomplete information, the savvy attacker can attempt to interrupt
the learning process. For example, attackers may sacriï¬ce their immediate rewards
and take incomprehensible actions instead so that the defender learns incorrect at-
tack characteristics. The above challenges motivate robust learning methods under
non-cooperative and even adversarial environments.

Second, since the learning process is based on samples from real interactions,
the defender needs to concern the system safety and security during the learning
period, while in the same time, attempts to achieve more accurate learning results of
the attackâ€™s characteristics. Moreover, since the learning under non-cooperative and
adversarial environments may terminate unpredictably at any time, the asymptotic
convergence would not be critical for security. The defender needs to care more about
the time eï¬ƒciency of the learning, i.e., how to achieve a suï¬ƒciently good estimate
in a ï¬nite number of steps.

Third, instead of learning from scratch, the defender can attempt to reuse the
past experience with attackers of similar behaviors to expedite the learning process,
which motivates the investigation of transfer learning in reinforcement learning [69].
Some side-channel information may also contribute to the learning to allow agents
to learn faster.

References

1. â€œVerizon 2019 data breach investigations report,â€ 2019.
2. D. Shackleford, â€œCombatting cyber risks in the supply chain,â€ SANS. org, 2015.
3. L. Huang and Q. Zhu, â€œAdaptive strategic cyber defense for advanced persistent threats in
critical infrastructure networks,â€ ACM SIGMETRICS Performance Evaluation Review, vol. 46,
no. 2, pp. 52â€“56, 2019.

4. â€”â€”, â€œAnalysis and computation of adaptive defense strategies against advanced persistent
threats for cyber-physical systems,â€ in International Conference on Decision and Game Theory
for Security. Springer, 2018, pp. 205â€“226.

5. L. Huang and Q. Zhu, â€œA Dynamic Games Approach to Proactive Defense Strategies against
Advanced Persistent Threats in Cyber-Physical Systems,â€ arXiv e-prints, p. arXiv:1906.09687,
Jun 2019.

6. Q. Zhu and T. BaÅŸar, â€œGame-theoretic approach to feedback-driven multi-stage moving target
defense,â€ in International Conference on Decision and Game Theory for Security. Springer,
2013, pp. 246â€“263.

22

Linan Huang and Quanyan Zhu

7. L. Huang and Q. Zhu, â€œAdaptive Honeypot Engagement through Reinforcement Learning of

Semi-Markov Decision Processes,â€ arXiv e-prints, p. arXiv:1906.12182, Jun 2019.

8. J. Pawlick, E. Colbert, and Q. Zhu, â€œA game-theoretic taxonomy and survey of defensive

deception for cybersecurity and privacy,â€ arXiv preprint arXiv:1712.05441, 2017.

9. F. J. Stech, K. E. Heckman, and B. E. Strom, â€œIntegrating cyber-d&d into adversary modeling

for active cyber defense,â€ in Cyber deception. Springer, 2016, pp. 1â€“22.

10. K. E. Heckman, M. J. Walsh, F. J. Stech, T. A. Oâ€™boyle, S. R. DiCato, and A. F. Herber,
â€œActive cyber defense with denial and deception: A cyber-wargame experiment,â€ computers &
security, vol. 37, pp. 72â€“77, 2013.

11. J. GÃ³mez-HernÃ¡ndez, L. Ãlvarez-GonzÃ¡lez, and P. GarcÃ­a-Teodoro, â€œR-locker: Thwarting ran-
somware action through a honeyï¬le-based approach,â€ Computers & Security, vol. 73, pp.
389â€“398, 2018.

12. N. Virvilis, B. Vanautgaerden, and O. S. Serrano, â€œChanging the game: The art of deceiving
sophisticated attackers,â€ in 2014 6th International Conference On Cyber Conï¬‚ict (CyCon
2014).

IEEE, 2014, pp. 87â€“97.

13. J. Pawlick, E. Colbert, and Q. Zhu, â€œModeling and analysis of leaky deception using signaling
games with evidence,â€ IEEE Transactions on Information Forensics and Security, 2018.
14. S. Jajodia, A. K. Ghosh, V. Swarup, C. Wang, and X. S. Wang, Moving target defense: creating
asymmetric uncertainty for cyber threats. Springer Science & Business Media, 2011, vol. 54.
15. G. S. Kc, A. D. Keromytis, and V. Prevelakis, â€œCountering code-injection attacks with
instruction-set randomization,â€ in Proceedings of the 10th ACM conference on Computer
and communications security. ACM, 2003, pp. 272â€“280.

16. A. Clark, Q. Zhu, R. Poovendran, and T. BaÅŸar, â€œDeceptive routing in relay networks,â€ in
Springer, 2012, pp.

International Conference on Decision and Game Theory for Security.
171â€“185.

17. H. Maleki, S. Valizadeh, W. Koch, A. Bestavros, and M. van Dijk, â€œMarkov modeling of
moving target defense games,â€ in Proceedings of the 2016 ACM Workshop on Moving Target
Defense. ACM, 2016, pp. 81â€“92.

18. C. R. Hecker, â€œA methodology for intelligent honeypot deployment and active engagement of

attackers,â€ Ph.D. dissertation, 2012.

19. Q. D. La, T. Q. Quek, J. Lee, S. Jin, and H. Zhu, â€œDeceptive attack and defense game in
honeypot-enabled networks for the internet of things,â€ IEEE Internet of Things Journal, vol. 3,
no. 6, pp. 1025â€“1035, 2016.

20. J. Pawlick, T. T. H. Nguyen, and Q. Zhu, â€œOptimal timing in dynamic and robust attacker
engagement during advanced persistent threats,â€ CoRR, vol. abs/1707.08031, 2017. [Online].
Available: http://arxiv.org/abs/1707.08031

21. J. Pawlick and Q. Zhu, â€œA Stackelberg game perspective on the conï¬‚ict between
machine learning and data obfuscation,â€ in Information Forensics and Security (WIFS),
2016 IEEE International Workshop on.
[Online]. Available:
http://ieeexplore.ieee.org/abstract/document/7823893/

IEEE, 2016, pp. 1â€“6.

22. Q. Zhu, A. Clark, R. Poovendran, and T. Basar, â€œDeployment and exploitation of deceptive
honeybots in social networks,â€ in Decision and Control (CDC), 2013 IEEE 52nd Annual
Conference on.

IEEE, 2013, pp. 212â€“219.

23. Q. Zhu, H. Tembine, and T. Basar, â€œHybrid learning in stochastic games and its applications
in network security,â€ Reinforcement Learning and Approximate Dynamic Programming for
Feedback Control, pp. 305â€“329, 2013.

24. Q. Zhu, Z. Yuan, J. B. Song, Z. Han, and T. BaÅŸar, â€œInterference aware routing game for
cognitive radio multi-hop networks,â€ Selected Areas in Communications, IEEE Journal on,
vol. 30, no. 10, pp. 2006â€“2015, 2012.

25. Q. Zhu, L. Bushnell, and T. Basar, â€œGame-theoretic analysis of node capture and cloning attack
with multiple attackers in wireless sensor networks,â€ in Decision and Control (CDC), 2012
IEEE 51st Annual Conference on.

IEEE, 2012, pp. 3404â€“3411.

26. Q. Zhu, A. Clark, R. Poovendran, and T. BaÅŸar, â€œDeceptive routing games,â€ in Decision and

Control (CDC), 2012 IEEE 51st Annual Conference on.

IEEE, 2012, pp. 2704â€“2711.

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

23

27. Q. Zhu, H. Li, Z. Han, and T. Basar, â€œA stochastic game model for jamming in multi-channel

cognitive radio systems.â€ in ICC, 2010, pp. 1â€“6.

28. Z. Xu and Q. Zhu, â€œSecure and practical output feedback control for cloud-enabled cyber-
physical systems,â€ in Communications and Network Security (CNS), 2017 IEEE Conference
on.

IEEE, 2017, pp. 416â€“420.

29. â€”â€”, â€œA Game-Theoretic Approach to Secure Control of Communication-Based Train
Control Systems Under Jamming Attacks,â€ in Proceedings of the 1st International Workshop
on Safe Control of Connected and Autonomous Vehicles. ACM, 2017, pp. 27â€“34. [Online].
Available: http://dl.acm.org/citation.cfm?id=3055381

30. â€”â€”, â€œCross-layer secure cyber-physical control system design for networked 3d printers,â€
IEEE, 2016, pp. 1191â€“1196. [Online].

in American Control Conference (ACC), 2016.
Available: http://ieeexplore.ieee.org/abstract/document/7525079/

31. M. J. Farooq and Q. Zhu, â€œModeling, analysis, and mitigation of dynamic botnet formation in
wireless iot networks,â€ IEEE Transactions on Information Forensics and Security, 2019.
32. Z. Xu and Q. Zhu, â€œA cyber-physical game framework for secure and resilient multi-agent
autonomous systems,â€ in Decision and Control (CDC), 2015 IEEE 54th Annual Conference
on.

IEEE, 2015, pp. 5156â€“5161.

33. L. Huang, J. Chen, and Q. Zhu, â€œA large-scale markov game approach to dynamic protection of
interdependent infrastructure networks,â€ in International Conference on Decision and Game
Theory for Security. Springer, 2017, pp. 357â€“376.

34. J. Chen, C. Touati, and Q. Zhu, â€œA dynamic game analysis and design of infrastructure network
protection and recovery,â€ ACM SIGMETRICS Performance Evaluation Review, vol. 45, no. 2,
p. 128, 2017.

35. F. Miao, Q. Zhu, M. Pajic, and G. J. Pappas, â€œA hybrid stochastic game for secure control of

cyber-physical systems,â€ Automatica, vol. 93, pp. 55â€“63, 2018.

36. Y. Yuan, Q. Zhu, F. Sun, Q. Wang, and T. Basar, â€œResilient control of cyber-physical systems
against denial-of-service attacks,â€ in Resilient Control Systems (ISRCS), 2013 6th International
Symposium on.

IEEE, 2013, pp. 54â€“59.

37. S. Rass and Q. Zhu, â€œGADAPT: A Sequential Game-Theoretic Framework for Designing
Defense-in-Depth Strategies Against Advanced Persistent Threats,â€ in Decision and Game
Theory for Security, ser. Lecture Notes in Computer Science, Q. Zhu, T. Alpcan, E. Panaousis,
M. Tambe, and W. Casey, Eds. Cham: Springer International Publishing, 2016, vol. 9996,
pp. 314â€“326.

38. Q. Zhu, Z. Yuan, J. B. Song, Z. Han, and T. Basar, â€œDynamic interference minimization routing
game for on-demand cognitive pilot channel,â€ in Global Telecommunications Conference
(GLOBECOM 2010), 2010 IEEE.
IEEE, 2010, pp. 1â€“6.

39. T. Zhang and Q. Zhu, â€œStrategic defense against deceptive civilian gps spooï¬ng of unmanned
aerial vehicles,â€ in International Conference on Decision and Game Theory for Security.
Springer, 2017, pp. 213â€“233.

40. L. Huang and Q. Zhu, â€œAnalysis and computation of adaptive defense strategies against ad-
vanced persistent threats for cyber-physical systems,â€ in International Conference on Decision
and Game Theory for Security, 2018.

41. â€”â€”, â€œAdaptive strategic cyber defense for advanced persistent threats in critical infrastructure

networks,â€ in ACM SIGMETRICS Performance Evaluation Review, 2018.

42. J. Pawlick, S. Farhang, and Q. Zhu, â€œFlip the cloud: Cyber-physical signaling games in the
presence of advanced persistent threats,â€ in Decision and Game Theory for Security. Springer,
2015, pp. 289â€“308.

43. S. Farhang, M. H. Manshaei, M. N. Esfahani, and Q. Zhu, â€œA dynamic bayesian security
game framework for strategic defense mechanism design,â€ in Decision and Game Theory for
Security. Springer, 2014, pp. 319â€“328.

44. Q. Zhu and T. BaÅŸar, â€œDynamic policy-based ids conï¬guration,â€ in Decision and Control, 2009
held jointly with the 2009 28th Chinese Control Conference. CDC/CCC 2009. Proceedings of
the 48th IEEE Conference on.
IEEE, 2009, pp. 8600â€“8605.

45. Q. Zhu, H. Tembine, and T. Basar, â€œNetwork security conï¬gurations: A nonzero-sum stochastic
IEEE, 2010, pp. 1059â€“1064.

game approach,â€ in American Control Conference (ACC), 2010.

24

Linan Huang and Quanyan Zhu

46. Q. Zhu, H. Tembine, and T. BaÅŸar, â€œHeterogeneous learning in zero-sum stochastic games with
IEEE,

incomplete information,â€ in 49th IEEE conference on decision and control (CDC).
2010, pp. 219â€“224.

47. J. Chen and Q. Zhu,

for Cloud-Enabled Internet of
Controlled Things under Advanced Persistent Threats: A Contract Design Approach,â€
IEEE Transactions on Information Forensics and Security, 2017. [Online]. Available:
http://ieeexplore.ieee.org/abstract/document/7954676/

â€œSecurity as

a Service

48. R. Zhang, Q. Zhu, and Y. Hayel, â€œA Bi-Level Game Approach to Attack-Aware Cyber Insurance
of Computer Networks,â€ IEEE Journal on Selected Areas in Communications, vol. 35, no. 3, pp.
779â€“794, 2017. [Online]. Available: http://ieeexplore.ieee.org/abstract/document/7859343/
49. R. Zhang and Q. Zhu, â€œAttack-aware cyber insurance of interdependent computer networks,â€

2016.

50. W. A. Casey, Q. Zhu, J. A. Morales, and B. Mishra, â€œCompliance control: Managed vulnera-
bility surface in social-technological systems via signaling games,â€ in Proceedings of the 7th
ACM CCS International Workshop on Managing Insider Security Threats. ACM, 2015, pp.
53â€“62.

51. Y. Hayel and Q. Zhu, â€œAttack-aware cyber insurance for risk sharing in computer networks,â€

in Decision and Game Theory for Security. Springer, 2015, pp. 22â€“34.

52. â€”â€”, â€œEpidemic protection over heterogeneous networks using evolutionary poisson games,â€
IEEE Transactions on Information Forensics and Security, vol. 12, no. 8, pp. 1786â€“1800, 2017.
53. Q. Zhu, C. Fung, R. Boutaba, and T. BaÅŸar, â€œGuidex: A game-theoretic incentive-based mech-
anism for intrusion detection networks,â€ Selected Areas in Communications, IEEE Journal on,
vol. 30, no. 11, pp. 2220â€“2230, 2012.

54. Q. Zhu, C. A. Gunter, and T. Basar, â€œTragedy of anticommons in digital right management of

medical records.â€ in HealthSec, 2012.

55. Q. Zhu, C. Fung, R. Boutaba, and T. BaÅŸar, â€œA game-theoretical approach to incentive design in
collaborative intrusion detection networks,â€ in Game Theory for Networks, 2009. GameNetsâ€™
09. International Conference on.

IEEE, 2009, pp. 384â€“392.

56. T. E. Carroll and D. Grosu, â€œA game theoretic investigation of deception in network security,â€

Security and Commun. Nets., vol. 4, no. 10, pp. 1162â€“1172, 2011.

57. J. Pawlick and Q. Zhu, â€œA Stackelberg game perspective on the conï¬‚ict between machine
learning and data obfuscation,â€ IEEE Intl. Workshop on Inform. Forensics and Security, 2016.
58. T. Zhang and Q. Zhu, â€œDynamic diï¬€erential privacy for ADMM-based distributed classiï¬cation
learning,â€ IEEE Transactions on Information Forensics and Security, vol. 12, no. 1, pp.
172â€“187, 2017. [Online]. Available: http://ieeexplore.ieee.org/abstract/document/7563366/
59. S. Farhang, Y. Hayel, and Q. Zhu, â€œPhy-layer location privacy-preserving access point selection
mechanism in next-generation wireless networks,â€ in Communications and Network Security
(CNS), 2015 IEEE Conference on.

IEEE, 2015, pp. 263â€“271.

60. T. Zhang and Q. Zhu, â€œDistributed privacy-preserving collaborative intrusion detection systems
for vanets,â€ IEEE Transactions on Signal and Information Processing over Networks, vol. 4,
no. 1, pp. 148â€“161, 2018.

61. N. Zhang, W. Yu, X. Fu, and S. K. Das, â€œgPath: A game-theoretic path selection algorithm
to protect torâ€™s anonymity,â€ in Decision and Game Theory for Security. Springer, 2010, pp.
58â€“71.

62. A. Garnaev, M. Baykal-Gursoy, and H. V. Poor, â€œSecurity games with unknown adversarial

strategies,â€ IEEE transactions on cybernetics, vol. 46, no. 10, pp. 2291â€“2299, 2015.

63. Q. Zhu, H. Tembine, and T. BaÅŸar, â€œDistributed strategic learning with application to network
IEEE, 2011, pp. 4057â€“

security,â€ in Proceedings of the 2011 American Control Conference.
4062.

64. A. Servin and D. Kudenko, â€œMulti-agent reinforcement learning for intrusion detection: A case
study and evaluation,â€ in German Conference on Multiagent System Technologies. Springer,
2008, pp. 159â€“170.

65. P. M. DjuriÄ‡ and Y. Wang, â€œDistributed bayesian learning in multiagent systems: Improving our
understanding of its capabilities and limitations,â€ IEEE Signal Processing Magazine, vol. 29,
no. 2, pp. 65â€“76, 2012.

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

25

66. G. Chalkiadakis and C. Boutilier, â€œCoordination in multiagent reinforcement learning: a
bayesian approach,â€ in Proceedings of the second international joint conference on Autonomous
agents and multiagent systems. ACM, 2003, pp. 709â€“716.

67. Z. Chen and D. Marculescu, â€œDistributed reinforcement learning for power limited many-core
system performance optimization,â€ in Proceedings of the 2015 Design, Automation & Test in
Europe Conference & Exhibition. EDA Consortium, 2015, pp. 1521â€“1526.

68. J. C. Harsanyi, â€œGames with incomplete information played by â€œbayesianâ€ players, iâ€“iii part i.

the basic model,â€ Management science, vol. 14, no. 3, pp. 159â€“182, 1967.

69. M. E. Taylor and P. Stone, â€œTransfer learning for reinforcement learning domains: A survey,â€

Journal of Machine Learning Research, vol. 10, no. Jul, pp. 1633â€“1685, 2009.

