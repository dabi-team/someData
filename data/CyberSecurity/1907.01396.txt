9
1
0
2

l
u
J

1

]

R
C
.
s
c
[

1
v
6
9
3
1
0
.
7
0
9
1
:
v
i
X
r
a

Strategic Learning for Active, Adaptive, and
Autonomous Cyber Defense

Linan Huang and Quanyan Zhu

Abstract The increasing instances of advanced attacks call for a new defense
paradigm that is active, autonomous, and adaptive, named as the ‘3A’ defense
paradigm. This chapter introduces three defense schemes that actively interact with
attackers to increase the attack cost and gather threat information, i.e., defensive de-
ception for detection and counter-deception, feedback-driven Moving Target Defense
(MTD), and adaptive honeypot engagement. Due to the cyber deception, external
noise, and the absent knowledge of the other players’ behaviors and goals, these
schemes possess three progressive levels of information restrictions, i.e., from the
parameter uncertainty, the payoﬀ uncertainty, to the environmental uncertainty. To
estimate the unknown and reduce the uncertainty, we adopt three diﬀerent strategic
learning schemes that ﬁt the associated information restrictions. All three learning
schemes share the same feedback structure of sensation, estimation, and actions so
that the most rewarding policies get reinforced and converge to the optimal ones
in autonomous and adaptive fashions. This work aims to shed lights on proactive
defense strategies, lay a solid foundation for strategic learning under incomplete
information, and quantify the tradeoﬀ between the security and costs.

1 Introduction

Recent instances of WannaCry ransomware, Petya cyberattack, and Stuxnet mal-
ware have demonstrated the trends of modern attacks and the corresponding new
security challenges as follows.

Linan Huang
Department of Electrical and Computer Engineering, New York University, 2 MetroTech Center,
Brooklyn, NY, 11201, USA, e-mail: lh2328@nyu.edu

Quanyan Zhu
Department of Electrical and Computer Engineering, New York University, 2 MetroTech Center,
Brooklyn, NY, 11201, USA, e-mail: qz494@nyu.edu

1

 
 
 
 
 
 
2

Linan Huang and Quanyan Zhu

• Advanced: Attackers leverage sophisticated attack tools to invalidate the oﬀ-the-

shelf defense schemes such as the ﬁrewall and intrusion detection systems.

• Targeted: Unlike automated probes, targeted attacks conduct thorough research in
advance to expose the system architecture, valuable assets, and defense schemes.
• Persistent: Attackers can restrain the adversary’s behaviors and bide their times

to launch critical attacks. They are persistent in achieving the goal.

• Adaptive: Attackers can learn the defense strategies and unpatched vulnerabilities
during the interaction with the defender and tailor their strategies accordingly.
• Stealthy and Deceptive: Attackers conceal their true intentions and disguise their
claws to evade detection. The adversarial cyber deception endows attackers an
information advantage over the defender.

Thus, defenders are urged to adopt active, adaptive, and autonomous defense
paradigms to deal with the above challenges and proactively protect the system prior
to the attack damages rather than passively compensate for the loss. In analogy to the
classical Kerckhoﬀs’s principle in the 19th century that attackers know the system,
we suggest a new security principle for modern cyber systems as follows:

Principle of 3A Defense: A cyber defense paradigm is considered to be
insuﬃciently secure if its eﬀectiveness relies on

• Rule-abiding human behaviors.
• A perfect protection against vulnerabilities and a perfect prevention from

system penetration.

• A perfect knowledge of attacks.

Firstly, 30% of data breaches are caused by privilege misuse and error by insiders
according to Verizon’s data breach report in 2019 [1]. Security administration does
not work well without the support of technology, and autonomous defense strategies
are required to deal with the increasing volume of sophisticated attacks. Secondly,
systems always have undiscovered vulnerabilities or unpatched vulnerabilities due to
the long supply chain of uncontrollable equipment providers [2] and the increasing
complexities in the system structure and functionality. Thus, an eﬀective paradigm
should assume a successful inﬁltration and pursue strategic securities through inter-
acting with intelligent attackers. Finally, due to adversarial deception techniques and
external noises, the defender cannot expect a perfect attack model with predicable
behaviors. The defense mechanism should be robust under incomplete information
and adaptive to the evolution of attacks.

In this chapter, we illustrate three active defense schemes in our previous works,
which are designed based on the new cyber security principle. They are defensive
deception for detection and counter-deception [3, 4, 5] in Section 2, feedback-driven
Moving Target Defense (MTD) [6] in Section 3, and adaptive honeypot engagement
[7] in Section 4. All three schemes is of incomplete information, and we arrange
them based on three progressive levels of information restrictions as shown in the
left part of Fig. 1.

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

3

Fig. 1 The left part of the
ﬁgure describes the degree
of information restriction.
From bottom to top, the
defense scheme becomes
more autonomous and relies
less on an exact attack model,
which also result in more
uncertainties. The right part
is the associated feedback
learning schemes.

The ﬁrst scheme in Section 2 considers the obfuscation of characteristics of
known attacks and systems through a random parameter called the player’s type. The
only uncertainty origins from the player’s type, and the mapping from the type to the
utility is known deterministically. The MTD scheme in Section 3 considers unknown
attacks and systems whose utilities are completely uncertain, while the honeypot
engagement scheme in Section 4 further investigates environmental uncertainties
such as the transition probability, the sojourn distribution, and the investigation
reward.

To deal with these uncertainties caused by diﬀerent information structures, we
suggest three associated learning schemes as shown in the right part of Fig. 1, i.e.,
Bayesian learning for the parameter estimation, distributed learning for the utility
acquisition without information sharing, and reinforcement learning for the optimal
policy obtainment under the unknown environment. All three learning methods
form a feedback loop that strategically incorporates the samples generated during
the interaction between attackers and defenders to persistently update the beliefs
of known and then take actions according to current optimal decision strategies.
The feedback structure makes the learning adaptive to behavioral and environmental
changes.

Another common point of these three schemes is the quantiﬁcation of the tradeoﬀ
between security and the diﬀerent types of cost. In particular, the costs result from
the attacker’s identiﬁcation of the defensive deception, the system usability, and the
risk of attackers penetrating production systems from the honeynet, respectively.

1.1 Literature

The idea of using deceptions defensively to detect and deter attacks has been studied
theoretically as listed in the taxonomic survey [8], implemented to the Adversar-
ial Tactics, Techniques and Common Knowledge (ATT&CKT M ) adversary model
system [9], and tested in the real-time cyber-wargame experiment [10]. Many previ-
ous works imply the similar idea of type obfuscation, e.g., creating social network
avatars (fake personas) on the major social networks [11], implementing honey ﬁles

4

Linan Huang and Quanyan Zhu

for ransomware actions [12], and disguising a production system as a honeypot to
scare attackers away [13].

Moving target defense (MTD) allows dynamic security strategies to limit the
exposure of vulnerabilities and the eﬀectiveness of the attacker’s reconnaissance by
increasing complexities and costs of attacks [14]. To achieve an eﬀective MTD, [15]
proposes the instruction set and the address space layout randomization, [16] studies
the deceptive routing against jamming in multi-hop relay networks, and [17] uses
the Markov chain to model the MTD process and discusses the optimal strategy to
balance the defensive beneﬁt and the network service quality.

The previous two methods use the defensive deception to protect the system and
assets. To further gather threat information, the defender can implement honeypots to
lure attackers to conduct adversarial behaviors and reveal their TTPs in a controlled
and monitored environment. Previous works [18, 19] have investigated the adap-
tive honeypot deployment to eﬀectively engage attackers without their notices. The
authors in recent work [20] proposes a continuous-state Markov Decision Process
(MDP) model and focuses on the optimal timing of the attacker ejection.

Game-theoretic models are natural frameworks to capture the multistage inter-
action between attackers and defenders. Recently, game theory has been applied to
diﬀerent sets of security problems, e.g., Stackelberg and signaling games for decep-
tion and proactive defenses [21, 6, 22, 23, 24, 16, 25, 26, 27], network games for
cyber-physical security [28, 29, 30, 31, 32, 33, 34, 35, 36, 37], dynamic games for
adaptive defense [38, 39, 40, 41, 3, 42, 43, 44, 45, 46], and mechanism design theory
for security [47, 48, 49, 50, 51, 52, 53, 54, 55].

Information asymmetry among the players in network security is a challenge to
deal with. The information asymmetry can be either leveraged or created by the
attacker or the defender for achieving a successful cyber deception. For example,
techniques such as honeynets [56, 22], moving target defense [6, 14, 3], obfuscation
[57, 58, 59, 60], and mix networks [61] have been introduced to create diﬃculties
for attackers to map out the system information.

To overcome the created or inherent uncertainties of networks, many works have
studied the strategic learning in security games, e.g., Bayesian learning for un-
known adversarial strategies [62], heterogeneous and hybrid distributed learning
[46, 63], multiagent reinforcement learning for intrusion detection [64]. Moreover,
these learning schemes are combined to achieve better properties, e.g., distributed
Bayesian learning [65], Bayesian reinforcement learning [66], and distributed rein-
forcement learning [67].

1.2 Notation

Throughout the chapter, we use calligraphic letter A to deﬁne a set and |A| as the
cardinality of the set. Let (cid:52)A represent the set of probability distributions over A.
If set A is discrete and ﬁnite, (cid:52)A := {p : A (cid:55)→ R+| (cid:205)
a ∈A p(a) = 1}, otherwise,
(cid:52)A := {p : A (cid:55)→ R+| ∫
a ∈A p(a) = 1}. Row player P1 is the defender (pronoun

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

5

‘she’) and P2 (pronoun ‘he’) is the user (or the attacker) who controls the column of
the game matrix. Both players want to maximize their own utilities. The indicator
function 1{x=y } equals one if x = y, and zero if x (cid:44) y.

1.3 Organization of the Chapter

The rest of the paper is organized as follows. In Section 2, we elaborate defen-
sive deception as a countermeasure of the adversarial deception under a multistage
setting where Bayesian learning is applied for the parameter uncertainty. Section
3 introduces a multistage MTD framework and the uncertainties of payoﬀs result
in distributed learning schemes. Section 4 further considers reinforcement learn-
ing for environmental uncertainties under the honeypot engagement scenario. The
conclusion and discussion are presented in Section 5.

2 Bayesian Learning for Uncertain Parameters

Under the mild restrictive information structure, each player’ utility is completely
governed by a ﬁnite group of parameters which form his/her type. Each player’s
type characterizes all the uncertainties about this player during the game interaction,
e.g., the physical outcome, the payoﬀ function, and the strategy feasibility, as an
equivalent utility uncertainty without loss of generality [68]. Thus, the revelation
of the type value directly results in a game of complete information. In the cyber
security scenario, a discrete type can distinguish either systems with diﬀerent kinds
of vulnerabilities or attackers with diﬀerent targets. The type can also be a continuous
random variable representing either the threat level or the security awareness level
[3, 4]. Since each player Pi takes actions to maximize his/her own type-dependent
utility, the other player Pj can form a belief to estimate Pi’s type based on the
observation of Pi’s action history. The utility optimization under the beliefs results in
the Perfect Bayesian Nash Equilibrium (PBNE) which generates new action samples
and updates the belief via the Bayesian rule. We plot the feedback Bayesian learning
process in Fig. 2 and elaborate each element in the following subsections based on
our previous work [5].

2.1 Type and Multistage Transition

Through adversarial deception techniques, attackers can disguise their subversive
actions as legitimate behaviors so that the defender P1 cannot judge whether a user
P2’s type θ2 ∈ Θ2 := {θg
2 . As a countermea-
sure, the defender can introduce the defensive deception so that the attacker cannot

2 or adversarial θb

} is legitimate θg

2, θb

2

Linan Huang and Quanyan Zhu

6

Fig. 2 The feedback loop of
the Bayesian learning from
the initial stage k = 1 to
the terminal stage k = K.
Each player forms a belief
of the other player’s type
and persistently updates the
belief based on the actions
resulted from the PBNE
strategies which are the results
of Bayesian learning.

distinguish between a primitive system θ L
1 , i.e., the
defender has a binary type θ1 ∈ Θ1 := {θ H
1 }. A sophisticated system is costly
yet deters attacks and causes damages to attackers. Thus, a primitive system can
disguise as a sophisticated one to draw the same threat level to attackers yet avoid
the implementation cost of sophisticated defense techniques.

1 and a sophisticated system θ H
1 , θ L

Many cyber networks contain hierarchical layers, and up-to-date attackers such as
Advanced Persistent Threats (APTs) aim to penetrate these layers and reach speciﬁc
targets at the ﬁnal stage as shown in Fig. 3.

Fig. 3: The multistage structure of APT kill chain is composed of reconnaissance,
initial compromise, privilege escalation, lateral movement, and mission execution.

i ∈ Ak

At stage k ∈ {0, 1, · · · , K }, Pi takes an action ak

i from a ﬁnite and discrete
set Ak
i . Both players’ actions become fully observable after applied and each action
does not directly reveal the private type. For example, both legitimate and adversarial
users can choose to access the sensor, and both primitive and sophisticated defenders
can choose to monitor the sensor. Both players’ actions up to stage k constitute the
history hk = {a0
i . Given history
hk at the current stage k, players at stage k + 1 obtain an updated history hk+1 = hk ∪
2 . A state xk ∈ Xk at each stage k is the smallest
{ak
set of quantities that summarize information about actions in previous stages so that

} after the observation ak

} ∈ H k := (cid:206)2

2, · · · , ak−1

1, · · · , ak−1

(cid:206)k−1
¯k=0

1, ak

1, ak

A ¯k

, a0

i=1

1

2

2

Common	InformationSet:	history	ℎ"or	state	𝑥"Defender’s	Type	𝜃%SensePBNE	StrategyActUser’s	Type	𝜃&PBNE	StrategyActUpdate	Attacker’s		BeliefsEstimateUpdate	Defender’s	BeliefsEstimateSense𝑎%"𝑎&"𝑘=0𝑘=1𝑘=𝐾Initial compromisePhysical accessWeb phishingPrivilege escalationLateral movementMission completenessData collection and exfiltration Physical damage Social engineeringPrivate keyDatabaseSensorReconnaissance Insider threatsOSINTControllerStrategic Learning for Active, Adaptive, and Autonomous Cyber Defense

7

the initial state x0 ∈ X0 and the history at stage k uniquely determine xk through a
known state transition function f k, i.e., xk+1 = f k(xk, ak
), ∀k ∈ {0, 1, · · · , K −1}.
The state can represent the location of the user in the attack graph, and also other
quantities such as users’ privilege levels and status of sensor failures.

1, ak

2

: Ik
i

(cid:55)→ (cid:52)(Ak

A behavioral strategy σk

i ) maps Pi’s information set Ik

i ∈ Σk
i at
i
stage k to a probability distribution over the action space Ak
i . At the initial stage 0,
since the only information available is the player’s type realization, the information
= Θi. The action is a realization of the behavioral strategy, or equivalently,
set I0
i
a sample drawn from the probability distribution σk
i ). With a slight abuse of
notation, we denote σk
i (ak
i |I k
i ) as the probability of Pi taking action ak
i given
i ∈ Ik
the available information I k
i .

i ∈ Ak

i (·|I k

2.2 Bayesian Update under Two Information Structure

Since the other player’s type is of private information, Pi forms a belief bk
(cid:55)→
(cid:52)(Θj), j (cid:44) i, on Pj’s type using the available information Ik
i . Likewise, given
information I k
i ∈ Ik
i at stage k, Pi believes with a probability bk
i (θ j |I k
i ) that Pj is
i : Θi (cid:55)→ (cid:52)Θj, ∀i, j ∈ {1, 2}, j (cid:44) i, is formed
of type θ j ∈ Θj. The initial belief b0
based on an imperfect detection, side-channel information or the statistic estimation
resulted from past experiences.

i : Ik
i

= H k × Θi, then players can update their

If the system has a perfect recall Ik
i

beliefs according to the Bayesian rule:

bk+1
i

(θ j |hk ∪ {ak

i , ak

j }, θi) =

σk
i (ak
(cid:205) ¯θ j ∈Θ j σk

i |hk, θi)σk
i (ak

j (ak
i |hk, θi)σk

j |hk, θ j)bk
j (ak

j |hk, ¯θ j)bk

i (θ j |hk, θi)

i ( ¯θ j |hk, θi)

.

(1)

i based on the observation of the action ak

Here, Pi updates the belief bk
j . When
the denominator is 0, the history hk+1 is not reachable from hk, and a Bayesian
update does not apply. In this case, we let bk+1
i
= Xk × Θi with the Markov property
If the information set is taken to be Ik
i
that Pr(xk+1|θ j, xk, · · · , x1, x0, θi) = Pr(xk+1|θ j, xk, θi), then the Bayesian update
between two consequent states is

j }, θi) := b0

(θ j |hk ∪ {ak

i (θ j |θi).

i , ak

i , ak

bk+1
i

(θ j |xk+1, θi) =

Pr(xk+1|θ j, xk, θi)bk
(cid:205) ¯θ j ∈Θ j Pr(xk+1| ¯θ j, xk, θi)bk

i (θ j |xk, θi)

i ( ¯θ j |xk, θi)

.

(2)

The Markov belief update (2) can be regarded as an approximation of (1) us-
ing action aggregations. Unlike the history set H k, the dimension of the state set
|Xk | does not grow with the number of stages. Hence, the Markov approximation
signiﬁcantly reduces the memory and computational complexity.

8

Linan Huang and Quanyan Zhu

2.3 Utility and PBNE

× Ak
2

: Xk × Ak
1

i ∈ R with a known probability density function (cid:36)k

At each stage k, Pi’s stage utility ¯Jk
× θ1 × θ2 × R (cid:55)→ R
i
depends on both players’ types and actions, the current state xk ∈ Xk, and
an external noise wk
i . The
noise term models unknown or uncontrolled factors that can aﬀect the value
2, θ1, θ2) :=
of the stage utility. Denote the expected stage utility as Jk
Ewk

1, ak
i ∼(cid:36) k
i
Given the type θi ∈ Θi, the initial state xk0 ∈ Xk0, and both players’ strategies
Σk
i from stage k0 to K, we can determine
k=k0
for Pi, i ∈ {1, 2}, by taking expectations over

σk0:K
i
the expected cumulative utility Uk0:K
i
the mixed-strategy distributions and the Pi’s belief on Pj’s type, i.e.,

i |xk, θi)]k=k0, ··· ,K ∈ (cid:206)K

2, θ1, θ2, wk

i ), ∀xk, ak

¯Jk
i (xk, ak

i (xk, ak

2, θ1, θ2.

:= [σk

i (ak

1, ak

1, ak

Uk0:K
i

(σk0:K
i

, σk0:K
j

, xk0, θi) :=

K
(cid:213)

k=k0

Eθ j ∼bk

i ,ak

i ∼σk

i ,ak

j ∼σk
j

i (xk, ak
Jk

1, ak

2, θ1, θ2).

(3)

The attacker and the defender use the Bayesian update to reduce their uncertainties
on the other player’s type. Since their actions aﬀect the belief update, both players
at each stage should optimize their expected cumulative utilities concerning the
updated beliefs, which leads to the solution concept of PBNE in Deﬁnition 1.

Deﬁnition 1 Consider the two-person K-stage game with a double-sided incomplete
i , ∀k ∈ {0, · · · , K }, an expected cumulative
information, a sequence of beliefs bk
in (3), and a given scalar ε ≥ 0. A sequence of strategies σ∗,0:K
utility U0:K
∈
i
(cid:206)K
k=0 Σk
i is called ε-perfect Bayesian Nash equilibrium for player i if the following

i

two conditions are satisﬁed.
C1: Belief consistency: under the strategy pair (σ∗,0:K

i at each stage k = 0, · · · , K satisﬁes (2).
bk

, σ∗,0:K
2

), each player’s belief

1

C2: Sequential rationality: for all given initial state xk0 ∈ Xk0 at every initial stage
1, ∀σk0:K

k0 ∈ {0, · · · , K }, ∀σk0:K

k=0 Σk
2 ,

k=0 Σk

∈ (cid:206)K

∈ (cid:206)K

1

2

Uk0:K
1
Uk0:K
2

(σ∗,k0:K
1
(σ∗,k0:K
1

, σ∗,k0:K
2
, σ∗,k0:K
2

, xk0, θ1) + ε ≥ Uk:K
, xk0, θ2) + ε ≥ Uk:K

1

2

(σk0:K
1
(σ∗,k0:K
1

, σ∗,k0:K
2
, σk0:K
2

, xk0, θ1),
, xk0, θ2).

(4)

When ε = 0, the equilibrium is called Perfect Bayesian Nash Equilibrium (PBNE).

Solving PBNE is challenging. If the type space is discrete and ﬁnite, then given
each player’s belief at all stages, we can solve the equilibrium strategy satisfying
condition C2 via dynamic programming and a bilinear program. Next, we update
the belief at each stage based on the computed equilibrium strategy. We iterate the
above update on the equilibrium strategy and belief until they satisfy condition C1
as demonstrated in [5]. If the type space is continuous, then the Bayesian update can
be simpliﬁed into a parametric update under the conjugate prior assumption. Next,

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

9

the parameter after each belief update can be assimilated into the backward dynamic
programming of equilibrium strategy with an expanded state space [4]. Although
no iterations are required, the inﬁnite dimension of continuous type space limits the
computation to two by two game matrices.

We apply the above framework and analysis to a case study of Tennessee Eastman
(TE) process and investigate both players’ multistage utilities under the adversarial
and the defensive deception in Fig. 4. Some insights are listed as follows.

Fig. 4 The cumulative util-
ities of the attacker and the
defender under the complete
information, the adversarial
deception, and the defensive
deception. The complete infor-
mation refers to the scenario
where both players know the
other player’s type. The de-
ception with the H-type or the
L-type means that the attacker
knows the defender’s type to
be θ H
1 , respectively,
yet the defender has no infor-
mation about the user’s type.
The double-sided deception
indicates that both players do
not know the other player’s
type.

1 or θ L

First, the defender’s payoﬀs under type θ H

1 can increase as much as 56% than
those under type θ L
1 . Second, the defender and the attacker receive the highest and
the lowest payoﬀ, respectively, under the complete information. When the attacker
introduces deceptions over his type, the attacker’s utility increases and the system
utility decreases. Third, when the defender adopts defensive deceptions to introduce
double-sided incomplete information, we ﬁnd that the decrease of system utilities is
reduced by at most 64%, i.e., the decrease of system utilities changes from $55, 570
to $35, 570 under the internal state and type θ H
1 . The double-sided incomplete
information also brings lower utilities to the attacker than the one-sided adversarial
deception. However, the system utility under the double-sided deception is still
less than the complete information case, which concludes that acquiring complete
information of the adversarial user is the most eﬀective defense. However, if the
complete information cannot be obtained, the defender can mitigate her loss by
introducing defensive deceptions.

150000170000190000210000230000250000270000290000310000330000External	State	Internal	StateDefender's	Utility	($)Complete	Information	with	the	H-TypeComplete	Information	with	the	L-TypeDeception	with	the	H-TypeDeception	with	the	L-TypeDouble	Deception	with	the	H-TypeDouble	Deception	with	the	L-Type400006000080000100000120000140000160000180000External	State	Internal	StateAttacker's	Utility	($)Complete	Information	with	the	H-TypeComplete	Information	with	the	L-TypeDeception	with	the	H-TypeDeception	with	the	L-TypeDouble	Deception	with	the	H-TypeDouble	Deception	with	the	L-Type10

Linan Huang and Quanyan Zhu

3 Distributed Learning for Uncertain Payoﬀs

In the previous section, we study known attacks and systems that adopt cyber decep-
tion to conceal their types. We assume common knowledge of the prior probability
distribution of the unknown type, and also a common observation of either the ac-
tion history or the state at each stage. Thus, each player can use Bayesian learning
to reduce the other player’s type uncertainty.

In this section, we consider unknown attacks in the MTD game stated in [6]
where each player has no information on the past actions of the other player, and
the payoﬀ functions are subject to noises and disturbances with unknown statis-
tical characteristics. Without information sharing between players, the learning is
distributed.

3.1 Static Game Model of MTD

We consider a system of N layers yet focus on the static game at layer l ∈ {1, 2, · · · , N }
because the technique can be employed at each layer of the system independently.
At layer l, Vl := {vl,1, vl,2, · · · , vl,nl } is the set of nl system vulnerabilities that an
attacker can exploit to compromise the system. Instead of a static conﬁguration at
layer l, the defender can choose to change her conﬁguration from a ﬁnite set of
ml feasible conﬁgurations Cl := {cl,1, cl,2, · · · , cl,ml }. Diﬀerent conﬁgurations result
in diﬀerent subsets of vulnerabilities among Vl, which are characterized by the
vulnerability map πl : Cl → 2Vl . We call πl(cl, j) the attack surface at stage l under
conﬁguration cl, j.

Suppose that for each vulnerability vl, j, the attacker can take a corresponding
attack al, j = γl(vl, j) from the action set Al := {al,1, al,2, · · · , al,nl }. Attack action
al, j is only eﬀective and incurs a bounded cost Di j ∈ R+ when the vulnerability
vl, j = γ−1
(al, j) exists in the current attack surface πl(cl,k). Thus, the damage caused
l
by the attacker at stage l can be represented as

(cid:40)

rl(al, j, cl,i) =

Di j, γ−1
0,

l

otherwise

(al, j) ∈ πl(cl,k)

.

(5)

Since vulnerabilities are inevitable in a modern computing system, we can ran-
domize the conﬁguration and make it diﬃcult for the attacker to learn and locate the
system vulnerability, which naturally leads to the mixed strategy equilibrium solution
concept of the game. At layer l, the defender’s strategy fl = { fl,1, fl,2, · · · , fl,ml } ∈
(cid:52)Cl assigns probability fl, j ∈ [0, 1] to conﬁguration cl, j while the attacker’s strategy
gl := {gl,1, gl,2, · · · , gl,nl } ∈ (cid:52)Al assigns probability gl,i ∈ [0, 1] to attack action
al,i. The zero-sum game possesses a mixed strategy saddle-point equilibrium (SPE)
l ∈ (cid:52)Cl, g∗
(f∗

l ∈ (cid:52)Al), and a unique game value r(f∗
l , g∗

l ), i.e.,
l ), ∀fl ∈ (cid:52)Cl, gl ∈ (cid:52)Al,

l , gl) ≤ rl(f∗

l ) ≤ rl(fl, g∗

rl(f∗

l , g∗

(6)

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

where the expected cost rl is given by

rl(fl, gl) := Efl,gl rl =

nl(cid:213)

ml(cid:213)

k=1

h=1

fl,hgl,krl(al,k, cl,h).

11

(7)

We illustrate the multistage MTD game in Fig. 5 and focus on the ﬁrst layer with
two available conﬁgurations C1 := {c1,1, c1,2} in the blue box. Conﬁguration c1,1 in
Fig. 5a has an attack surface π1(c1,1) = {v1,1, v1,2} while conﬁguration c1,2 in Fig. 5b
reveals two vulnerabilities v1,2, v1,3 ∈ π1(c1,2). Then, if the attacker takes action a1,1
and the defender changes the conﬁguration from c1,1 to c1,2, the attack is deterred at
the ﬁrst layer.

(a) Attack surface π1(c1,1) = {v1,1, v1,2 }.

(b) Attack surface π1(c1,2) = {v1,2, v1,3 }.

Fig. 5: Given a static conﬁguration c1,1, an attacker can succeed in reaching the
resources at deeper layers by forming an attack path v1,1 → v2,2 → · · · . A change
of conﬁguration to c1,2 can thwart the attacker at the ﬁrst layer.

3.2 Distributed Learning

In practical cybersecurity domain, the payoﬀ function rl is subjected to noises of un-
known distributions. Then, each player reduces the payoﬀ uncertainty by repeatedly
observing the payoﬀ realizations during the interaction with the other player. We use
subscript t to denote the strategy or cost at time t.

There is no communication at any time between two agents due to the non-
cooperative environment, and the conﬁguration and attack action are kept private, i.e.,
each player cannot observe the other player’s action. Thus, each player independently
chooses action cl,t ∈ Cl or al,t ∈ Al to estimate the average risk of the system
l,t : Al → R+ at layer l. Based on the estimated average risk
l,t : Cl → R+ and ˆr A
ˆr S

v1,1v1,2v1,3v2,1v2,2v2,3v3,1v3,2v3,3v4,1v4,2v4,3c1,1c2,1c3,1c4,2v1,1v1,2v1,3v2,1v2,2v2,3v3,1v3,2v3,3v4,1v4,2v4,3c1,2c2,1c3,1c4,212

Linan Huang and Quanyan Zhu

ˆr S
l,t and the previous policy fl,t , the defender can obtain her updated policy fl,t+1.
Likewise, the attacker can also update his policy gl,t+1 based on ˆr A
l,t and gl,t . The new
policy pair (fl,t+1, gl,t+1) determines the next payoﬀ sample. The entire distributed
learning feedback loop is illustrated in Fig. 6 where we distinguish the adversarial
and defensive learning in red and green, respectively.

Fig. 6 The distributed learn-
ing of the multistage MTD
game at layer l. Adversar-
ial learning in red does not
share information with defen-
sive learning in green. The
distributed learning fashion
means that the learning rule
does not depend on the other
player’s action, yet the ob-
served payoﬀ depends on both
players’ actions.

In particular, players update their estimated average risks based on the payoﬀ
t be the

sample rl,t under the chosen action pair (cl,t, al,t ) as follows. Let µS
payoﬀ learning rate for the system and attacker, respectively.

t and µA

l,t+1(cl,h) = ˆr S
ˆr S
l,t+1(al,h) = ˆr A
ˆr A

l,t (cl,h) + µS
l,t (al,h) + µA

t 1{cl, t =cl, h }(rl,t − ˆr S
t 1{al, t =al, h }(rl,t − ˆr A

l,t (cl,h)),
l,t (al,h)).

(8)

The indicators in (8) mean that both players only update the estimate average risk of
the current action.

3.2.1 Security versus Usability

Frequent conﬁguration changes may achieve the complete security yet also decrease
the system usability. To quantify the tradeoﬀ between the security and the usability,
we introduce the switching cost of policy from fl,t to fl,t+1 as their entropy:

l,t :=
RS

ml(cid:213)

h=1

fl,h,t+1 ln

(cid:18) fl,h,t+1
fl,h,t

(cid:19)

.

(9)

Then, the total cost at time t combines the expected cost with the entropy penalty in
a ratio of (cid:15) S
l,t is high, the policy changes less and is more usable, yet may
cause a large loss and be less rational.

l,t . When (cid:15) S

Utility	Uncertainty	𝑟",$𝕔",$,𝕒",$of	Layer	𝑙at	time	𝑡Defender’s	Risk	LearningSenseShift	Attack	SurfaceAct𝑟",$Attacker’s	Risk	LearningSense𝑟̂",$*+,(𝕔",$)𝑟̂",$*+/ (𝕒",$)𝒇",$*+Exploit	Different	VulnerabilitiesActUpdate	Attack	Policy	EstimateUpdate	Configuration	Policy	Estimate𝒈",$*+𝕔",$𝕒",$Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

(SP) :

sup
fl, t +1 ∈(cid:52)Cl

−

ml(cid:213)

h=1

fl,h,t+1 ˆr S

l,t (cl,h) − (cid:15) S

l,t RS
l,t .

A similar learning cost is introduced for the attacker:

(AP) :

sup
gl, t +1 ∈(cid:52)Al

−

nl(cid:213)

h=1

gl,h,t+1 ˆr A

l,t (al,h) − (cid:15) A
l,t

nl(cid:213)

h=1

gl,h,t+1 ln

(cid:18) gl,h,t+1
gl,h,t

(cid:19)

.

13

(10)

(11)

At any time t + 1, we are able to obtain the equilibrium strategy ( fl,h,t+1, gl,h,t+1)
l,t ) in closed form of the previous strategy and the estimated

and game value (W S
average risk at time t as follows.

l,t, W A

fl,h,t+1 =

−

ˆrl, t (cl, h )
(cid:15) S
l, t

fl,h,t e

−

ˆrl, t (cl, h(cid:48) )
(cid:15) S
l, t

fl,h(cid:48),t e

ml(cid:213)

h(cid:48)=1

,

gl,h,t+1 =

−

ˆrl, t (al, h )
(cid:15) A
l, t

gl,h,t e

−

ˆrl, t (al, h(cid:48) )
(cid:15) A
l, t

gl,h(cid:48),t e

nl(cid:213)

h(cid:48)=1

,

W S
l,t

= (cid:15) S

l,t ln

−

ˆrl, t (cl, h )
(cid:15) S
l, t

(cid:33)

,

fl,h,t e

(cid:32) ml(cid:213)

h=1

W A
l,t

= (cid:15) A

l,t ln

(cid:33)

−

ˆrl, t (al, h )
(cid:15) A
l, t

.

gl,h,t e

(cid:32) nl(cid:213)

h=1

(12)

3.2.2 Learning Dynamics and ODE Counterparts

The closed form of policy leads to the following learning dynamics with learning
rates λS

l,t, λ A

l,t ∈ [0, 1].

fl,h,t+1 = (1 − λS

l,t ) fl,h,t + λS

l,t

gl,h,t+1 = (1 − λ A

l,t )gl,h,t + λ A

l,t

−

ˆrl, t (cl, h )
(cid:15) S
l, t

fl,h,t e

−

ˆrl, t (cl, h(cid:48) )
(cid:15) S
l, t

fl,h(cid:48),t e

ml(cid:213)

h(cid:48)=1

−

ˆrl, t (al, h )
(cid:15) A
l, t

gl,h,t e

−

ˆrl, t (al, h(cid:48) )
(cid:15) A
l, t

gl,h(cid:48),t e

nl(cid:213)

h(cid:48)=1

,

.

(13)

= 1, λ A
l,t

= 1, (13) is the same as (12). According to the stochastic approxima-
If λS
l,t
tion theory, the convergence of the policy and the average risk requires the learning
rates λ A
l,t to satisfy the regular condition of convergency in Deﬁnition
2.

l,t, µA

l,t, µS

l,t, λS

Deﬁnition 2 A number sequence {xt }, t = 1, 2, · · · , is said to satisfy the regular
condition of convergency if

14

Linan Huang and Quanyan Zhu

∞
(cid:213)

t=1

xt = +∞,

∞
(cid:213)

t=1

(xt )2 < +∞.

(14)

The coupled dynamics of the payoﬀ learning (8) and policy learning(13) converge to
their Ordinary Diﬀerential Equations (ODEs) counterparts in system dynamics (15)
and attacker dynamics (16), respectively. Let ecl, h ∈ (cid:52)Cl, eal, h ∈ (cid:52)Al be vectors of
proper dimensions with the h-th entry being 1 and others being 0.

fl,h,t = fl,h,t

−

ˆrl, t (cl, h )
(cid:15) S
l, t

e

ml(cid:213)

−

fl,h(cid:48),t e

ˆrl, t (cl, h(cid:48) )
(cid:15) S
l, t

(cid:169)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:171)

(cid:170)
(cid:174)
(cid:174)
− 1
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:172)
l,t+1(cl,h), cl,h ∈ Cl.

h(cid:48)=1
l,t (cl,h) = −rl,t (ecl, h, gl,t ) − ˆr S
ˆr S

, h = 1, 2, · · · , ml,

d
dt

d
dt

−

e

ˆrl, t (al, h )
(cid:15) A
l, t

gl,h,t+1 = gl,h,t

(cid:169)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:171)
l,t+1(al,h) = rl,t (fl,t, eal, h ) − ˆr A
ˆr A

(cid:170)
(cid:174)
(cid:174)
− 1
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:172)
l,t+1(al,h), al,h ∈ Al.

ˆrl, t (al, h(cid:48) )
(cid:15) A
l, t

gl,h(cid:48),t e

nl(cid:213)

h(cid:48)=1

−

, h = 1, 2, · · · , nl,

d
dt

d
dt

(15)

(16)

We can show that the SPE of the game is the steady state of the ODE dynamics
in (15), (16), and the interior stationary points of the dynamics are the SPE of the
game [6].

3.2.3 Heterogeneous and Hybrid Learning

The entropy regulation terms in (10) and (11) result in a closed form of strategies
and learning dynamics in (13). Without the closed form, distributed learners can
adopt general learning schemes which combine the payoﬀ and the strategy update
as stated in [46]. Speciﬁcally, algorithm CRL0 mimics the replicator dynamics and
updates the strategy according to the current sample value of the utility. On the
other hand, algorithm CRL1 updates the strategy according to a soft-max function
of the estimated utilities so that the most rewarding policy get reinforced and will
be picked with a higher probability. The ﬁrst algorithm is robust yet ineﬃcient, and
the second one is fragile yet eﬃcient. Moreover, players are not obliged to adopt
the same learning scheme at diﬀerent time. The heterogeneous learning focuses on
diﬀerent players adopting diﬀerent learning schemes [46], while hybrid learning
means that players can choose diﬀerent learning schemes at diﬀerent times based
on their rationalities and preferences [63]. According to stochastic approximation

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

15

techniques, these learning schemes with random updates can be studied using their
deterministic ODE counterparts.

4 Reinforcement Learning for Uncertain Environments

This section considers uncertainties on the entire environment, i.e., the state transi-
tion, the sojourn time, and the investigation payoﬀ, in the active defense scenario of
the honeypot engagement [7]. We use the Semi-Markov Decision Process (SMDP) to
capture these environmental uncertainties in the continuous time system. Although
the attacker’s duration time is continuous at each honeypot, the defender’s engage-
ment action is applied at a discrete time epoch. Based on the observed samples
at each decision epoch, the defender can estimate the environment elements deter-
mined by attackers’ characteristics, and use reinforcement learning methods to obtain
the optimal policy. We plot the entire feedback learning structure in Fig. 7. Since
the attacker should not identify the existence of the honeypot and the defender’s
engagement actions, he will not take actions to jeopardize the learning.

Fig. 7 The feedback structure
of reinforcement learning
methods on SMDP. The
red background means that
the attacker’s characteristics
determine the environmental
uncertainties and the samples
observed in the honeynet. The
attacker is not involved in
parts of the green background.
The learning scheme in Fig.
7 extends the one in Section
3 to consider a continuous
time elapse and multistage
transitions.

4.1 Honeypot Network and SMDP Model

The honeypots form a network to emulate a production system. From an attacker’s
viewpoint, two network structures are the same as shown in Fig. 8. Based on the
network topology, we introduce the continuous-time inﬁnite-horizon discounted
SMDPs, which can be summarized by the tuple {t ∈ [0, ∞), S, A(sj), tr(sl |sj, aj),
z(·|sj, aj, sl), rγ(sj, aj, sl), γ ∈ [0, ∞)}. We illustrate each element of the tuple through
a 13-state example in Fig. 9.

Discrete-Time	Decision	Process	𝑘=0,1,…Sample𝑠̅),	𝜏̅),	𝑟,-),𝑟.-)based	on	attackers’	characteristicsSenseUpdate	𝑄)(𝑠̅),𝑎))EstimateChoose	action	𝑎)to	balance	exploration	and	exploitationActContinuous-Time	System	𝑡∈[0,∞)16

Linan Huang and Quanyan Zhu

Fig. 8: The honeynet in red emulates and shares the same structure as the targeted
production system in green.

Each node in Fig. 9 represents a state si ∈ S, i ∈ {1, 2, · · · , 13}. At time t ∈
[0, ∞), the attacker is either at one of the honeypot node denoted by state si ∈
S, i ∈ {1, 2, · · · , 11}, at the normal zone s12, or at a virtual absorbing state s13
once attackers are ejected or terminate on their own. At each state si ∈ S, the
defender can choose an action ai ∈ A(si). For example, at honeypot nodes, the
defender can conduct action aE to eject the attacker, action aP to purely record the
attacker’s activities, low-interactive action aL, or high-interactive action aH , i.e.,
A(si) := {aE, aP, aL, aH }, i ∈ {1, · · · , N }. The high-interactive action is costly to
implement yet can both increases the probability of a longer sojourn time at honeypot
ni, and reduces the probability of attackers penetrating the normal system from ni
if connected. If the attacker resides in the normal zone either from the beginning
or later through the pivot honeypots, the defender can choose either action aE to
eject the attacker immediately, or action aA to attract the attacker to the honeynet
by generating more deceptive inbound and outbound traﬃcs in the honeynet, i.e.,
A(s12) := {aE, aA}. Based on the current state sj ∈ S and the defender’s action
aj ∈ A(sj), the attacker transits to state sl ∈ S with probability tr(sl |sj, aj) and
the sojourn time at state sj is a continuous random variable with probability density
z(·|sj, aj, sl). Once the attacker arrives at a new honeypot ni, the defender dynamically
applies an interaction action at honeypot ni from A(si) and keeps interacting with
the attacker until she transits to the next honeypot. If the defender changes the action
before the transition, the attacker may be able to detect the change and become

Access PointInternet / CloudFirewallSwitchSwitchAccess PointInternet / CloudIntrusion DetectionHoneypot192.168.1.10HoneywallGatewayRouterServerHoneypot192.168.1.45Data BaseComputer NetworkServerWork Station192.168.1.55Data Base192.168.1.90HoneywallSensorActuatorHoneypotHoneypot NetworkHoneypotHoneypotHoneynetProduction SystemsStrategic Learning for Active, Adaptive, and Autonomous Cyber Defense

17

Fig. 9 Honeypots emulate
diﬀerent components of the
production system. Actions
aE, aP, aL, aH are denoted
in red, blue, purple, and green,
respectively. The size of node
ni represents the state value
v(si ), i ∈ {1, 2, · · · , 11}.

aware of the honeypot. Since the decision is made at the time of transition, we can
transform the above continuous time model on horizon t ∈ [0, ∞) into a discrete
decision model at decision epoch k ∈ {0, 1, · · · , ∞}. The time of the attacker’s k th
transition is denoted by a random variable T k, the landing state is denoted as sk ∈ S,
and the adopted action after arriving at sk is denoted as ak ∈ A(sk).

The defender gains an investigation reward by engaging and analyzing the attacker
in the honeypot. To simplify the notation, we segment the investigation reward during
time t ∈ [0, ∞) into ones at discrete decision epochs T k, k ∈ {0, 1, · · · , ∞}. When
τ ∈ [T k, T k+1] amount of time elapses at stage k, the defender’s investigation reward
r(sk, ak, sk+1, T k, T k+1, τ) = r1(sk, ak, sk+1)1{τ=0} + r2(sk, ak, T k, T k+1, τ), at time τ
of stage k, is the sum of two parts. The ﬁrst part is the immediate cost of applying
engagement action ak ∈ A(sk) at state sk ∈ S and the second part is the reward
rate of threat information acquisition minus the cost rate of persistently generating
deceptive traﬃcs. Due to the randomness of the attacker’s behavior, the information
acquisition can also be random, thus the actual reward rate r2 is perturbed by an
additive zero-mean noise wr . As the defender spends longer time interacting with
attackers, investigating their behaviors and acquires better understandings of their
targets and TTPs, less new information can be extracted. In addition, the same
intelligence becomes less valuable as time elapses due to the timeliness. Thus,
we use a discounted factor of γ ∈ [0, ∞) to penalize the decreasing value of the
investigation reward as time elapses.

The defender aims at a policy π ∈ Π which maps state sk ∈ S to action ak ∈ A(sk)

to maximize the long-term expected utility starting from state s0, i.e.,

u(s0, π) = E[

∞
(cid:213)

∫ T k+1

k=0

T k

e−γ(τ+T k )(r(Sk, Ak, Sk+1, T k, T k+1, τ) + wr )dτ].

(17)

At each decision epoch, the value function v(s0) = supπ ∈Π u(s0, π) can be repre-

sented by dynamic programming, i.e.,

ClientsServerSwitchNormal ZoneComputerNetworkEmulatedSensorsEmulatedDatabase12111012345679813Absorbing State18

Linan Huang and Quanyan Zhu

v(s0) = sup

E[

∫ T 1

a0 ∈A(s0)

T 0

e−γ(τ+T 0)r(s0, a0, S1, T 0, T 1, τ)dτ + e−γT 1

v(S1)].

(18)

We assume a constant reward rate r2(sk, ak, T k, T k+1, τ) = ¯r2(sk, ak) for simplicity.

Then, (18) can be transformed into an equivalent MDP form, i.e., ∀s0 ∈ S,

v(s0) = sup

(cid:213)

tr(s1|s0, a0)(rγ(s0, a0, s1) + zγ(s0, a0, s1)v(s1)),

(19)

a0 ∈A(s0)

s1 ∈S
where zγ(s0, a0, s1) := ∫ ∞
0 e−γτ z(τ|s0, a0, s1)dτ ∈ [0, 1] is the Laplace transform of
the sojourn probability density z(τ|s0, a0, s1) and the equivalent reward rγ(s0, a0, s1)
:= r1(s0, a0, s1) + ¯r2(s0,a0)
(1 − zγ(s0, a0, s1)) ∈ [−mc, mc] is assumed to be bounded
by a constant mc.

γ

Deﬁnition 3 There exists constants θ ∈ (0, 1) and δ > 0 such that

tr(s1|s0, a0)z(δ|s0, a0, s1) ≤ 1 − θ, ∀s0 ∈ S, a0 ∈ A(s0).

(20)

(cid:213)

s1 ∈S

The right-hand side of (18) is a contraction mapping under the regulation
condition in Deﬁnition 3. Then, we can ﬁnd the unique optimal policy π∗ =
arg maxπ ∈Π u(s0, π) by value iteration, policy iteration or linear programming. Fig.
9 illustrates the optimal policy and the state value by the color and the size of the
node, respectively. In the example scenario, the honeypot of database n10 and sensors
n11 are the main and secondary targets of the attacker, respectively. Thus, defenders
can obtain a higher investigation reward when they manage to engage the attacker in
these two honeypot nodes with a larger probability and for a longer time. However,
instead of naively adopting high interactive actions, a savvy defender also balances
the high implantation cost of aH . Our quantitative results indicate that the high in-
teractive action should only be applied at n10 to be cost-eﬀective. On the other hand,
although the bridge nodes n1, n2, n8 which connect to the normal zone n12 do not
contain higher investigation rewards than other nodes, the defender still takes action
aL at these nodes. The goal is to either increase the probability of attracting attackers
away from the normal zone or reduce the probability of attackers penetrating the
normal zone from these bridge nodes.

4.2 Reinforcement Learning of SMDP

The absent knowledge of the attacker’s characteristics results in environmental
uncertainty of the investigation reward, the attacker’s transition probability, and
the sojourn distribution. We use Q-learning algorithm to obtain the optimal en-
gagement policy based on the actual experience of the honeynet interactions, i.e.,
∀ ¯sk ∈ S, ∀ak ∈ A( ¯sk),

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

Qk+1( ¯sk, ak) =(1 − αk( ¯sk, ak))Qk( ¯sk, ak) + αk( ¯sk, ak)[ ¯r1( ¯sk, ak, ¯sk+1)

+ ¯r2( ¯sk, ak)

(1 − e−γ ¯τ k )
γ

− e−γ ¯τ k

max
a(cid:48) ∈A( ¯sk+1)

Qk( ¯sk+1, a(cid:48))],

19

(21)

where αk( ¯sk, ak) ∈ (0, 1) is the learning rate, ¯sk, ¯sk+1 are the observed states at
stage k and k + 1, ¯r1, ¯r2 is the observed investigation rewards, and ¯τk is the observed
sojourn time at state sk. When the learning rate satisﬁes the condition of convergency
in Deﬁnition 2, i.e., (cid:205)∞
k=0(αk(sk, ak))2 < ∞, ∀sk ∈ S, ∀ak ∈
A(sk), and all state-action pairs are explored inﬁnitely, maxa(cid:48) ∈A(sk ) Q∞(s∞, a(cid:48)), in
(21) converges to value v(sk) with probability 1.

k=0 αk(sk, ak) = ∞, (cid:205)∞

At each decision epoch k ∈ {0, 1, · · · }, the action ak is chosen according to the (cid:15)-
greedy policy, i.e., the defender chooses the optimal action arg maxa(cid:48) ∈A(sk ) Qk(sk, a(cid:48))
with a probability 1 − (cid:15), and a random action with a probability (cid:15). Note that the
exploration rate (cid:15) ∈ (0, 1] should not be too small to guarantee suﬃcient samples
of all state-action pairs. The Q-learning algorithm under a pure exploration policy
(cid:15) = 1 still converges yet at a slower rate.

Fig. 10 One instance of Q-
learning on SMDP where the
x-axis shows the sojourn time
and the y-axis represents the
state transition. The chosen
actions aE, aP, aL, aH are
denoted in red, blue, purple,
and green, respectively.

In our scenario, the defender knows the reward of ejection action aA and v(s13) =
0, thus does not need to explore action aA to learn it. We plot one learning trajectory
of the state transition and sojourn time under the (cid:15)-greedy exploration policy in
Fig. 10, where the chosen actions aE, aP, aL, aH are denoted in red, blue, purple,
and green, respectively. If the ejection reward is unknown, the defender should be
restrictive in exploring aA which terminates the learning process. Otherwise, the
defender may need to engage with a group of attackers who share similar behaviors
to obtain suﬃcient samples to learn the optimal engagement policy.

In particular, we choose αk(sk, ak) =

, ∀sk ∈ S, ∀ak ∈ A(sk), to
guarantee the asymptotic convergence, where kc ∈ (0, ∞) is a constant parameter
and k {sk,ak } ∈ {0, 1, · · · } is the number of visits to state-action pair {sk, ak } up to
stage k. We need to choose a proper value of kc to guarantee a good numerical

kc
k{s k , ak }−1+kc

2.48992.49942.50892.51842.52792.5374Time10412345678910111213State20

Linan Huang and Quanyan Zhu

performance of convergence in ﬁnite steps as shown in Fig. 11a. We shift the green
and blue lines vertically to avoid the overlap with the red line and represent the
corresponding theoretical values in dotted black lines. If kc is too small as shown
in the red line, the learning rate decreases so fast that new observed samples hardly
update the Q-value and the defender may need a long time to learn the right value.
However, if kc is too large as shown in the green line, the learning rate decreases
so slow that new samples contribute signiﬁcantly to the current Q-value. It causes a
large variation and a slower convergence rate of maxa(cid:48) ∈A(s12) Qk(s12, a(cid:48)).

We show the convergence of the policy and value under kc = 1, (cid:15) = 0.2, in
the video demo (See URL: https://bit.ly/2QUz3Ok). In the video, the color of each
node nk distinguishes the defender’s action ak at state sk and the size of the node
is proportional to maxa(cid:48) ∈A(sk ) Qk(sk, a(cid:48)) at stage k. To show the convergence, we
decrease the value of (cid:15) gradually to 0 after 5000 steps. Since the convergence
trajectory is stochastic, we run the simulation for 100 times and plot the mean and
the variance of Qk(s12, aP) of state s12 under the optimal policy π(s12) = aP in Fig.
11. The mean in red converges to the theoretical value in about 400 steps and the
variance in blue reduces dramatically as step k increases.

(a) The convergence rate under diﬀerent val-
ues of kc .

(b) The evolution of the mean and the vari-
ance of Qk (s12, aP ).

Fig. 11: Convergence results of Q-learning over SMDP.

5 Conclusion and Discussion

This chapter has introduced three defense schemes, i.e., defensive deception to
detect and counter adversarial deception, feedback-driven Moving Target Defense
(MTD) to increase the attacker’s probing and reconnaissance costs, and adaptive
honeypot engagement to gather fundamental threat information. These schemes
satisfy the Principle of 3A Defense as they actively protect the system prior to
the attack damages, provide strategic defenses autonomously, and apply learning to
adapt to uncertainty and changes. These schemes possess three progressive levels

01234567Step k104Value01002003004005006007008009001000-7-6-5-4-3-2-10123VarianceMeanTheoretical ValueStrategic Learning for Active, Adaptive, and Autonomous Cyber Defense

21

of information restrictions, which lead to diﬀerent strategic learning schemes to
estimate the parameter, the payoﬀ, and the environment. All these learning schemes,
however, have a feedback loop to sense samples, estimate the unknowns, and take
actions according to the estimate. Our work lays a solid foundation for strategic
learning in active, adaptive, autonomous defenses under incomplete information and
leads to the following challenges and future directions.

First, multi-agent learning in non-cooperative environments is challenging due
to the coupling and interaction between these heterogeneous agents. The learning
results depend on all involving agents yet other players’ behaviors, levels of ratio-
nality, and learning schemes are not controllable and may change abruptly. More-
over, as attackers become aware of the active defense techniques and the learning
scheme under incomplete information, the savvy attacker can attempt to interrupt
the learning process. For example, attackers may sacriﬁce their immediate rewards
and take incomprehensible actions instead so that the defender learns incorrect at-
tack characteristics. The above challenges motivate robust learning methods under
non-cooperative and even adversarial environments.

Second, since the learning process is based on samples from real interactions,
the defender needs to concern the system safety and security during the learning
period, while in the same time, attempts to achieve more accurate learning results of
the attack’s characteristics. Moreover, since the learning under non-cooperative and
adversarial environments may terminate unpredictably at any time, the asymptotic
convergence would not be critical for security. The defender needs to care more about
the time eﬃciency of the learning, i.e., how to achieve a suﬃciently good estimate
in a ﬁnite number of steps.

Third, instead of learning from scratch, the defender can attempt to reuse the
past experience with attackers of similar behaviors to expedite the learning process,
which motivates the investigation of transfer learning in reinforcement learning [69].
Some side-channel information may also contribute to the learning to allow agents
to learn faster.

References

1. “Verizon 2019 data breach investigations report,” 2019.
2. D. Shackleford, “Combatting cyber risks in the supply chain,” SANS. org, 2015.
3. L. Huang and Q. Zhu, “Adaptive strategic cyber defense for advanced persistent threats in
critical infrastructure networks,” ACM SIGMETRICS Performance Evaluation Review, vol. 46,
no. 2, pp. 52–56, 2019.

4. ——, “Analysis and computation of adaptive defense strategies against advanced persistent
threats for cyber-physical systems,” in International Conference on Decision and Game Theory
for Security. Springer, 2018, pp. 205–226.

5. L. Huang and Q. Zhu, “A Dynamic Games Approach to Proactive Defense Strategies against
Advanced Persistent Threats in Cyber-Physical Systems,” arXiv e-prints, p. arXiv:1906.09687,
Jun 2019.

6. Q. Zhu and T. Başar, “Game-theoretic approach to feedback-driven multi-stage moving target
defense,” in International Conference on Decision and Game Theory for Security. Springer,
2013, pp. 246–263.

22

Linan Huang and Quanyan Zhu

7. L. Huang and Q. Zhu, “Adaptive Honeypot Engagement through Reinforcement Learning of

Semi-Markov Decision Processes,” arXiv e-prints, p. arXiv:1906.12182, Jun 2019.

8. J. Pawlick, E. Colbert, and Q. Zhu, “A game-theoretic taxonomy and survey of defensive

deception for cybersecurity and privacy,” arXiv preprint arXiv:1712.05441, 2017.

9. F. J. Stech, K. E. Heckman, and B. E. Strom, “Integrating cyber-d&d into adversary modeling

for active cyber defense,” in Cyber deception. Springer, 2016, pp. 1–22.

10. K. E. Heckman, M. J. Walsh, F. J. Stech, T. A. O’boyle, S. R. DiCato, and A. F. Herber,
“Active cyber defense with denial and deception: A cyber-wargame experiment,” computers &
security, vol. 37, pp. 72–77, 2013.

11. J. Gómez-Hernández, L. Álvarez-González, and P. García-Teodoro, “R-locker: Thwarting ran-
somware action through a honeyﬁle-based approach,” Computers & Security, vol. 73, pp.
389–398, 2018.

12. N. Virvilis, B. Vanautgaerden, and O. S. Serrano, “Changing the game: The art of deceiving
sophisticated attackers,” in 2014 6th International Conference On Cyber Conﬂict (CyCon
2014).

IEEE, 2014, pp. 87–97.

13. J. Pawlick, E. Colbert, and Q. Zhu, “Modeling and analysis of leaky deception using signaling
games with evidence,” IEEE Transactions on Information Forensics and Security, 2018.
14. S. Jajodia, A. K. Ghosh, V. Swarup, C. Wang, and X. S. Wang, Moving target defense: creating
asymmetric uncertainty for cyber threats. Springer Science & Business Media, 2011, vol. 54.
15. G. S. Kc, A. D. Keromytis, and V. Prevelakis, “Countering code-injection attacks with
instruction-set randomization,” in Proceedings of the 10th ACM conference on Computer
and communications security. ACM, 2003, pp. 272–280.

16. A. Clark, Q. Zhu, R. Poovendran, and T. Başar, “Deceptive routing in relay networks,” in
Springer, 2012, pp.

International Conference on Decision and Game Theory for Security.
171–185.

17. H. Maleki, S. Valizadeh, W. Koch, A. Bestavros, and M. van Dijk, “Markov modeling of
moving target defense games,” in Proceedings of the 2016 ACM Workshop on Moving Target
Defense. ACM, 2016, pp. 81–92.

18. C. R. Hecker, “A methodology for intelligent honeypot deployment and active engagement of

attackers,” Ph.D. dissertation, 2012.

19. Q. D. La, T. Q. Quek, J. Lee, S. Jin, and H. Zhu, “Deceptive attack and defense game in
honeypot-enabled networks for the internet of things,” IEEE Internet of Things Journal, vol. 3,
no. 6, pp. 1025–1035, 2016.

20. J. Pawlick, T. T. H. Nguyen, and Q. Zhu, “Optimal timing in dynamic and robust attacker
engagement during advanced persistent threats,” CoRR, vol. abs/1707.08031, 2017. [Online].
Available: http://arxiv.org/abs/1707.08031

21. J. Pawlick and Q. Zhu, “A Stackelberg game perspective on the conﬂict between
machine learning and data obfuscation,” in Information Forensics and Security (WIFS),
2016 IEEE International Workshop on.
[Online]. Available:
http://ieeexplore.ieee.org/abstract/document/7823893/

IEEE, 2016, pp. 1–6.

22. Q. Zhu, A. Clark, R. Poovendran, and T. Basar, “Deployment and exploitation of deceptive
honeybots in social networks,” in Decision and Control (CDC), 2013 IEEE 52nd Annual
Conference on.

IEEE, 2013, pp. 212–219.

23. Q. Zhu, H. Tembine, and T. Basar, “Hybrid learning in stochastic games and its applications
in network security,” Reinforcement Learning and Approximate Dynamic Programming for
Feedback Control, pp. 305–329, 2013.

24. Q. Zhu, Z. Yuan, J. B. Song, Z. Han, and T. Başar, “Interference aware routing game for
cognitive radio multi-hop networks,” Selected Areas in Communications, IEEE Journal on,
vol. 30, no. 10, pp. 2006–2015, 2012.

25. Q. Zhu, L. Bushnell, and T. Basar, “Game-theoretic analysis of node capture and cloning attack
with multiple attackers in wireless sensor networks,” in Decision and Control (CDC), 2012
IEEE 51st Annual Conference on.

IEEE, 2012, pp. 3404–3411.

26. Q. Zhu, A. Clark, R. Poovendran, and T. Başar, “Deceptive routing games,” in Decision and

Control (CDC), 2012 IEEE 51st Annual Conference on.

IEEE, 2012, pp. 2704–2711.

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

23

27. Q. Zhu, H. Li, Z. Han, and T. Basar, “A stochastic game model for jamming in multi-channel

cognitive radio systems.” in ICC, 2010, pp. 1–6.

28. Z. Xu and Q. Zhu, “Secure and practical output feedback control for cloud-enabled cyber-
physical systems,” in Communications and Network Security (CNS), 2017 IEEE Conference
on.

IEEE, 2017, pp. 416–420.

29. ——, “A Game-Theoretic Approach to Secure Control of Communication-Based Train
Control Systems Under Jamming Attacks,” in Proceedings of the 1st International Workshop
on Safe Control of Connected and Autonomous Vehicles. ACM, 2017, pp. 27–34. [Online].
Available: http://dl.acm.org/citation.cfm?id=3055381

30. ——, “Cross-layer secure cyber-physical control system design for networked 3d printers,”
IEEE, 2016, pp. 1191–1196. [Online].

in American Control Conference (ACC), 2016.
Available: http://ieeexplore.ieee.org/abstract/document/7525079/

31. M. J. Farooq and Q. Zhu, “Modeling, analysis, and mitigation of dynamic botnet formation in
wireless iot networks,” IEEE Transactions on Information Forensics and Security, 2019.
32. Z. Xu and Q. Zhu, “A cyber-physical game framework for secure and resilient multi-agent
autonomous systems,” in Decision and Control (CDC), 2015 IEEE 54th Annual Conference
on.

IEEE, 2015, pp. 5156–5161.

33. L. Huang, J. Chen, and Q. Zhu, “A large-scale markov game approach to dynamic protection of
interdependent infrastructure networks,” in International Conference on Decision and Game
Theory for Security. Springer, 2017, pp. 357–376.

34. J. Chen, C. Touati, and Q. Zhu, “A dynamic game analysis and design of infrastructure network
protection and recovery,” ACM SIGMETRICS Performance Evaluation Review, vol. 45, no. 2,
p. 128, 2017.

35. F. Miao, Q. Zhu, M. Pajic, and G. J. Pappas, “A hybrid stochastic game for secure control of

cyber-physical systems,” Automatica, vol. 93, pp. 55–63, 2018.

36. Y. Yuan, Q. Zhu, F. Sun, Q. Wang, and T. Basar, “Resilient control of cyber-physical systems
against denial-of-service attacks,” in Resilient Control Systems (ISRCS), 2013 6th International
Symposium on.

IEEE, 2013, pp. 54–59.

37. S. Rass and Q. Zhu, “GADAPT: A Sequential Game-Theoretic Framework for Designing
Defense-in-Depth Strategies Against Advanced Persistent Threats,” in Decision and Game
Theory for Security, ser. Lecture Notes in Computer Science, Q. Zhu, T. Alpcan, E. Panaousis,
M. Tambe, and W. Casey, Eds. Cham: Springer International Publishing, 2016, vol. 9996,
pp. 314–326.

38. Q. Zhu, Z. Yuan, J. B. Song, Z. Han, and T. Basar, “Dynamic interference minimization routing
game for on-demand cognitive pilot channel,” in Global Telecommunications Conference
(GLOBECOM 2010), 2010 IEEE.
IEEE, 2010, pp. 1–6.

39. T. Zhang and Q. Zhu, “Strategic defense against deceptive civilian gps spooﬁng of unmanned
aerial vehicles,” in International Conference on Decision and Game Theory for Security.
Springer, 2017, pp. 213–233.

40. L. Huang and Q. Zhu, “Analysis and computation of adaptive defense strategies against ad-
vanced persistent threats for cyber-physical systems,” in International Conference on Decision
and Game Theory for Security, 2018.

41. ——, “Adaptive strategic cyber defense for advanced persistent threats in critical infrastructure

networks,” in ACM SIGMETRICS Performance Evaluation Review, 2018.

42. J. Pawlick, S. Farhang, and Q. Zhu, “Flip the cloud: Cyber-physical signaling games in the
presence of advanced persistent threats,” in Decision and Game Theory for Security. Springer,
2015, pp. 289–308.

43. S. Farhang, M. H. Manshaei, M. N. Esfahani, and Q. Zhu, “A dynamic bayesian security
game framework for strategic defense mechanism design,” in Decision and Game Theory for
Security. Springer, 2014, pp. 319–328.

44. Q. Zhu and T. Başar, “Dynamic policy-based ids conﬁguration,” in Decision and Control, 2009
held jointly with the 2009 28th Chinese Control Conference. CDC/CCC 2009. Proceedings of
the 48th IEEE Conference on.
IEEE, 2009, pp. 8600–8605.

45. Q. Zhu, H. Tembine, and T. Basar, “Network security conﬁgurations: A nonzero-sum stochastic
IEEE, 2010, pp. 1059–1064.

game approach,” in American Control Conference (ACC), 2010.

24

Linan Huang and Quanyan Zhu

46. Q. Zhu, H. Tembine, and T. Başar, “Heterogeneous learning in zero-sum stochastic games with
IEEE,

incomplete information,” in 49th IEEE conference on decision and control (CDC).
2010, pp. 219–224.

47. J. Chen and Q. Zhu,

for Cloud-Enabled Internet of
Controlled Things under Advanced Persistent Threats: A Contract Design Approach,”
IEEE Transactions on Information Forensics and Security, 2017. [Online]. Available:
http://ieeexplore.ieee.org/abstract/document/7954676/

“Security as

a Service

48. R. Zhang, Q. Zhu, and Y. Hayel, “A Bi-Level Game Approach to Attack-Aware Cyber Insurance
of Computer Networks,” IEEE Journal on Selected Areas in Communications, vol. 35, no. 3, pp.
779–794, 2017. [Online]. Available: http://ieeexplore.ieee.org/abstract/document/7859343/
49. R. Zhang and Q. Zhu, “Attack-aware cyber insurance of interdependent computer networks,”

2016.

50. W. A. Casey, Q. Zhu, J. A. Morales, and B. Mishra, “Compliance control: Managed vulnera-
bility surface in social-technological systems via signaling games,” in Proceedings of the 7th
ACM CCS International Workshop on Managing Insider Security Threats. ACM, 2015, pp.
53–62.

51. Y. Hayel and Q. Zhu, “Attack-aware cyber insurance for risk sharing in computer networks,”

in Decision and Game Theory for Security. Springer, 2015, pp. 22–34.

52. ——, “Epidemic protection over heterogeneous networks using evolutionary poisson games,”
IEEE Transactions on Information Forensics and Security, vol. 12, no. 8, pp. 1786–1800, 2017.
53. Q. Zhu, C. Fung, R. Boutaba, and T. Başar, “Guidex: A game-theoretic incentive-based mech-
anism for intrusion detection networks,” Selected Areas in Communications, IEEE Journal on,
vol. 30, no. 11, pp. 2220–2230, 2012.

54. Q. Zhu, C. A. Gunter, and T. Basar, “Tragedy of anticommons in digital right management of

medical records.” in HealthSec, 2012.

55. Q. Zhu, C. Fung, R. Boutaba, and T. Başar, “A game-theoretical approach to incentive design in
collaborative intrusion detection networks,” in Game Theory for Networks, 2009. GameNets’
09. International Conference on.

IEEE, 2009, pp. 384–392.

56. T. E. Carroll and D. Grosu, “A game theoretic investigation of deception in network security,”

Security and Commun. Nets., vol. 4, no. 10, pp. 1162–1172, 2011.

57. J. Pawlick and Q. Zhu, “A Stackelberg game perspective on the conﬂict between machine
learning and data obfuscation,” IEEE Intl. Workshop on Inform. Forensics and Security, 2016.
58. T. Zhang and Q. Zhu, “Dynamic diﬀerential privacy for ADMM-based distributed classiﬁcation
learning,” IEEE Transactions on Information Forensics and Security, vol. 12, no. 1, pp.
172–187, 2017. [Online]. Available: http://ieeexplore.ieee.org/abstract/document/7563366/
59. S. Farhang, Y. Hayel, and Q. Zhu, “Phy-layer location privacy-preserving access point selection
mechanism in next-generation wireless networks,” in Communications and Network Security
(CNS), 2015 IEEE Conference on.

IEEE, 2015, pp. 263–271.

60. T. Zhang and Q. Zhu, “Distributed privacy-preserving collaborative intrusion detection systems
for vanets,” IEEE Transactions on Signal and Information Processing over Networks, vol. 4,
no. 1, pp. 148–161, 2018.

61. N. Zhang, W. Yu, X. Fu, and S. K. Das, “gPath: A game-theoretic path selection algorithm
to protect tor’s anonymity,” in Decision and Game Theory for Security. Springer, 2010, pp.
58–71.

62. A. Garnaev, M. Baykal-Gursoy, and H. V. Poor, “Security games with unknown adversarial

strategies,” IEEE transactions on cybernetics, vol. 46, no. 10, pp. 2291–2299, 2015.

63. Q. Zhu, H. Tembine, and T. Başar, “Distributed strategic learning with application to network
IEEE, 2011, pp. 4057–

security,” in Proceedings of the 2011 American Control Conference.
4062.

64. A. Servin and D. Kudenko, “Multi-agent reinforcement learning for intrusion detection: A case
study and evaluation,” in German Conference on Multiagent System Technologies. Springer,
2008, pp. 159–170.

65. P. M. Djurić and Y. Wang, “Distributed bayesian learning in multiagent systems: Improving our
understanding of its capabilities and limitations,” IEEE Signal Processing Magazine, vol. 29,
no. 2, pp. 65–76, 2012.

Strategic Learning for Active, Adaptive, and Autonomous Cyber Defense

25

66. G. Chalkiadakis and C. Boutilier, “Coordination in multiagent reinforcement learning: a
bayesian approach,” in Proceedings of the second international joint conference on Autonomous
agents and multiagent systems. ACM, 2003, pp. 709–716.

67. Z. Chen and D. Marculescu, “Distributed reinforcement learning for power limited many-core
system performance optimization,” in Proceedings of the 2015 Design, Automation & Test in
Europe Conference & Exhibition. EDA Consortium, 2015, pp. 1521–1526.

68. J. C. Harsanyi, “Games with incomplete information played by “bayesian” players, i–iii part i.

the basic model,” Management science, vol. 14, no. 3, pp. 159–182, 1967.

69. M. E. Taylor and P. Stone, “Transfer learning for reinforcement learning domains: A survey,”

Journal of Machine Learning Research, vol. 10, no. Jul, pp. 1633–1685, 2009.

