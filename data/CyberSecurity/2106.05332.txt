1
2
0
2

n
u
J

9

]

R
C
.
s
c
[

1
v
2
3
3
5
0
.
6
0
1
2
:
v
i
X
r
a

Reinforcement Learning for Industrial Control
Network Cyber Security Orchestration

John Mern∗
Department of Aeronautics and Astronautics
Stanford University
Stanford, CA 94305
jmern91@stanford.edu

Kyle Hatch†
Department of Computer Science
Stanford University
Stanford, CA 94305
khatch@stanford.edu

Ryan Silva
Applied Physics Laboratory
Johns Hopkins University
Laurel, MD 20723

Jeff Brush
Applied Physics Laboratory
Johns Hopkins University
Laurel, MD 20723

Mykel J. Kochenderfer‡
Department of Aeronautics and Astronautics
Stanford University
Stanford, CA 94305
mykel@stanford.edu

Abstract

Defending computer networks from cyber attack requires coordinating actions
across multiple nodes based on imperfect indicators of compromise while mini-
mizing disruptions to network operations. Advanced attacks can progress with few
observable signals over several months before execution. The resulting sequential
decision problem has large observation and action spaces and a long time-horizon,
making it difﬁcult to solve with existing methods. In this work, we present tech-
niques to scale deep reinforcement learning to solve the cyber security orchestration
problem for large industrial control networks. We propose a novel attention-based
neural architecture with size complexity that is invariant to the size of the network
under protection. A pre-training curriculum is presented to overcome early ex-
ploration difﬁculty. Experiments show in that the proposed approaches greatly
improve both the learning sample complexity and converged policy performance
over baseline methods in simulation.

1

Introduction

Cyber attacks have been increasingly targeting computer networks that are integrated with industrial
control systems (ICS) [1]. Attack techniques focusing on stealth and redundant access make them
difﬁcult to detect and stop [2]. Intrusion detection systems monitor networks and alert security
orchestrators to potential intrusions, though high false-alarm rates cause many alerts to receive no
security response. Advanced persistent threat (APT) attackers take advantage of this by spreading
across target networks undetected over long periods to prepare an eventual attack [3]. In addition

∗PhD Candidate, Stanford Intelligent Systems Laboratory
†Undergraduate Student, Stanford University
‡Associate Professor, Stanford Intelligent Systems Laboratory

Preprint. Under review.

 
 
 
 
 
 
to the theft of sensitive data common in APT attacks on standard networks, attacks on ICS can
additionally result in disruption of the controlled system, physical destruction of equipment, and even
loss of life [4]–[6].

It is not feasible for human security teams to respond to every potential intrusion alert and to reliably
mitigate all threats. This work seeks to demonstrate the feasibility of developing an automated cyber
security orchestrator (ACSO) to assist human analysts by automatically investigating and mitigating
potential attacks on ICS networks. We ﬁrst present a model of this task as a discrete-time sequential
decision making problem. The proposed model captures many of the challenges of the underlying
decision problem while remaining agnostic to speciﬁc network conﬁguration and communication
dynamics. To support this, we implemented a cyber attack simulator that can efﬁciently generate
large quantities of trial data.

Many sequential decision methods cannot solve this problem because explicit models of APT attack
dynamics and alert behaviors are not generally known. Reinforcement learning (RL) may be used to
train policies for complex tasks without explicit models [7], [8], however the ACSO problem poses
several challenges to existing reinforcement learning (RL) solvers. The observation and action spaces
grow with the number of nodes on the protected network, leading to very high-dimensional, discrete
observation and action spaces. Deep reinforcement learning is known to struggle to learn over large,
discrete action spaces due to the difﬁculty of generalizing over actions [9]. Similarly, large input
spaces have been found to decrease the sample efﬁciency of stochastic learning [10].

APT attacks are designed to be difﬁcult to detect, causing limited observability of the true compromise
state of nodes on the network. Partially observable problems are known to be signiﬁcantly more
difﬁcult to solve than their fully observable counterparts, requiring learning over sequences of
observations to infer unseen states [11]. Effective defense against APTs requires timely response to
compromises, though intrusion campaigns can last several months. Modeling the decision process
requires ﬁne resolution time-steps, leading to very long time horizons. Long time-horizons and
sparse rewards can drastically increase the sample complexity of learning, both through exploration
difﬁculty and temporally delayed credit assignment [12], [13].

This paper proposes a neural network architecture and training curriculum that scales to the large
problem space. We present an attention-based architecture that effectively learns over many cyber
network nodes without growth in the number of required parameters. The lower parameter complexity
network is shown to signiﬁcantly accelerate training while converging to a higher performing solution
than a larger baseline architecture. To overcome exploration difﬁculty, we introduce a supervised
pretraining step that minimizes a composite large-margin classiﬁcation and temporal-difference loss
over a set of expert trajectories.

We tested the proposed solution methods against baseline neural network architectures and training
approaches, as well as against expert-designed policies. These experiments demonstrate how each
component contributes to the proposed method’s scalability and suggest that deep RL is a viable
method to implement an autonomous network security agent. The methods presented may be
generalized to scale RL to other problems with large input spaces, output spaces, or long time
horizons.

2 Background

2.1 Reinforcement Learning

In sequential decision problems, the environment is modeled by states s that evolve according to
potentially stochastic dynamics. An agent takes actions a that condition state transition distributions
T (s(cid:48) | s, a) and generate rewards r(s, a, s(cid:48)). In many problems, the state of the world is not known.
In these partially observable domains, agents receive noisy observations according to o ∼ Z(o | s, a).

A sequential decision problem is solved by an action sequence that maximizes the expected value
V (s) = E(cid:2) (cid:80)
t γtr(st, at)(cid:3) for all states in the trajectory. Reinforcement learning methods learn
a policy π : ot−τ :t (cid:55)→ at that maps a history of observations to actions through repeated trial-and-
error with the environment. In each trial, actions are taken according to the current policy until a
terminal condition is reached. At the end of the trial, the policy is updated to improve the expected
performance.

2

Figure 1: APT Attack Progression: This shows the typical progression of an APT attack at the level
of tactics in the MITRE ATT&CK framework. The process starts on the left with initial intrusion,
proceeding over several months to eventual execution.

Reinforcement learning methods that use neural networks to represent the learned policy are known
as deep reinforcement learning. Deep Q-Network (DQN) learning is a popular method in which the
learned neural network predicts the expected value Q(ot−τ :t, a) = E(cid:2)r(st, a) + γV (st+1)(cid:3) of taking
each action in the action space for a given input history [14]. The Q-network is used as a policy by
taking the action with the highest predicted value a∗ = argmaxaQ(ot−τ :t, a).
Deep RL training typically requires large amounts of data [15]. The amount of trials required to solve
a task tends to grow with the task complexity and neural network size, with more complex tasks often
requiring larger neural networks. Reinforcement learning agents improve by stochastically exploring
new trajectories with each trial. Tasks with very large input spaces, output spaces, or very long time
horizons tend to require much more exploration because the odds of ﬁnding a successful trajectory
through random exploration are low [16].

2.2 Networked Industrial Control Systems

Industrial control systems are networks of devices used to monitor and control a coordinated physical
process, such as an assembly line or power plant. Simple operational technology computing devices
such as programmable logic controllers (PLCs) are installed on the industrial equipment to receive
sensor readings or to issue control commands. These PLCs are networked with information technology
networks to enable remote access and control [17].

Integration of ICS networks with internet-connected networks has made them vulnerable to cyber
attack [18]. Despite this increased risk, networking is often an operational requirement for processes
where system components are difﬁcult to access or geographically distant. For example, electrical
power grid management requires coordination of substations that may be distributed over very large
areas. To attempt to mitigate this vulnerability, many networks are organized into ﬁrewall-separated
levels, with low-privilege nodes on less restricted and internet-accessible levels, and process-critical
nodes on more isolated subnets [17].

Many recent, high-proﬁle infrastructure attacks were executed by advanced persistent threats (APTs).
Advanced persistent threats comprise the top two tiers of the Defense Science Board threat taxonomy
and are typically well-funded and willing to spend signiﬁcant time to achieve their goal [19]. APTs
overcome the layered ICS network structure by ﬁrst compromising a low-privilege node in the target
network [20]. Using this node, the APT will then conduct internal network reconnaissance and take
control over more privileged nodes until it gains the authority needed to achieve its primary objective,
for example to destroy equipment. A typical APT attack process, based on the tactics of the MITRE
ATT&CK framework [21], is shown in ﬁg. 1.

During reconnaissance and lateral movement, APT activity tends to be very difﬁcult to detect with
automated intrusion detection systems [3]. The lateral movement phase also comprises the majority
of the time of the attack campaign, often taking several months to complete. Once an attack is staged,
however, it can usually be executed in a matter of hours. Because of this, successfully defending
against an APT attack requires detecting and securing the compromised nodes in the staging phases.

Machine learning methods have been applied to many aspects of cyber security, most extensively to
perception tasks such as malware recognition and anomaly detection [20], [22]–[24]. Reinforcement
learning has been applied to cyber security in various capacities. Nguyen and Reddi [25] provide a
survey of recent work in this area. The scope of each work referenced in this survey is limited in the

3

Initial IntrusionThe attacker gains control over a low-privilege node, typically through social engineering Lateral MovementThe attacker gains control over additional nodes to ensure persistence and to gain visibility DiscoveryThe attacker searches for data on the network and the targeted physical processExecutionThe attacker delivers malicious code, resulting in data loss, process disruption, or equipment destruction.Privilege EscalationThe attacker mines credentials and compromise nodes necessary to execute its attackFigure 2: Simulated Network Architecture: The problem simulates level 2 and level 1 of the PERA
model. Each level contains an operations VLAN and a nominally empty quarantine VLAN. Level
2 contains twenty-ﬁve workstation nodes and three servers. Level 1 has ﬁve local human-machine
interface nodes and ﬁfty networked PLCs. All network message trafﬁc is simulated through virtual
switches, routers, and ﬁrewalls. The computing node for the ACSO is not explicitly modeled.

type of network under protection, the actions available to the defender agent, and/or to the type of
attack against which it is defending. For example, Gupta and Yang [26] propose a system restricted
to ﬁltering signals spooﬁng networked sensor measurements.

3 Problem Formulation

The objective is to prevent an APT from disrupting a set of PLCs while minimizing how much
the defensive actions interfere with network operations. This work is restricted to defending the
engineering level (2) and plant level (1) of the Purdue enterprise reference architecture (PERA) [27]
as shown in ﬁg. 2. Level 2 contains exclusively workstations and servers, and level 1 contains
workstations and all PLCs. The network studied in this work has twenty-ﬁve workstations in level 2,
ﬁve workstations in level 1, and ﬁfty PLCs for a total of eighty-three nodes.

The state space for the problem is the joint space of the ICS network states and the APT attacker
states. The ICS network state is the joint state of all nodes on the network. Server and workstation
node states deﬁne how much access and control the APT has and where on the network the node is
located. A PLC state deﬁnes whether or not it has been disrupted by an attack. The APT state deﬁnes
the current APT sub-goal, knowledge, and available resources.

The ACSO agent has full knowledge of the network conﬁguration and PLC states, but observes
node compromise state through alerts. Alerts are actively generated by investigative actions taken
by the ACSO, which may generate false negatives, and passively by an intrusion detection system
triggered by APT actions on compromised nodes with many false alarms. Each episode begins with a
single level 2 workstation compromised by an APT. The episode terminates when 25 PLCs have been
disrupted by APT attacks or after 5,000 hours have been simulated. Each decision step is one hour of
simulation world time and the discount factor is 0.999.

Each step, the ACSO can choose to take an action on a single node or do nothing. There are seven
valid action types for workstation nodes, six action types for server nodes, and two action types
for PLCs. A total of 329 unique actions may be taken across the nodes. With every step, the APT
also takes actions based on its state and a stochastic attacker model. Actions are not necessarily
instantaneous and some actions will complete over several time-steps. During this time, agents can
initiate additional actions.

4

VLAN 2.1 (Ops)VLAN 2.2 (Quarantine)VLAN 2.1 (Ops)Level 2 - EngineeringLevel 1 - PlantHistorianOPCDCWorkstations (25x)PLCs (50x)Local HMIVLAN 1.2 (Quarantine)Reward each step is based on the fraction of PLCs that are operating nominally and the inconvenience
caused by the ACSO actions taken. The reward function is deﬁned as

r(s, a) = rPLC(s, a) + rIT(s, a) + rTerm(s, a)
(cid:17)(cid:16)
(cid:88)

(cid:16)

=

1 − 0.04

1{p disrupted}

1.0 −

p∈PLCs

(cid:88)

a∈At

(cid:17)

cost(a)

+

1
γ

1{stime ≥ tmax}

(1)

(2)

where At is the set of all actions completing at time step t.

The ﬁrst term scales reward by the number disrupted PLCs, where disruption of twenty ﬁve PLCs
results in process shutdown. The second term imposes a penalty for ACSO actions, where each action
is assigned a cost based on its perceived burden to network operations. Action costs were elicited
from network and security experts. For example the low-disruption action of rebooting a workstation
has a cost of 0.01, while the more disruptive action of re-imaging a server has a cost of 0.05. The ﬁnal
term rewards the agent for reaching the episode time limit tmax. The 1/γ magnitude of the terminal
reward ensures the optimal state value does not drift with episode time.

To facilitate this and future research, we developed an ICS network attack simulation (INASIM) in the
Julia language. To the authors knowledge, this is the ﬁrst open-source APT attack simulation capable
of generating data in quantities required for deep reinforcement learning on a single workstation.
INASIM implements a conﬁgurable network simulation and a baseline APT attacker model, as
well as an API for custom attacker agents. The simulation is compatible with the POMDPs.jl [28]
framework as well as with the OpenAI Gym [29] framework through supplied Python bindings. The
simulation and baseline attacker model are described brieﬂy here. A description of states, actions,
and environment dynamics is provided in the appendix along with numerical settings used for the
studies in this work.

The network simulation deﬁnes the structure of the network, the state of the nodes, and the actions
available to the APT and ACSO agents. The ACSO can take actions to investigate node states or
to secure compromised nodes. The APT can take actions to gain further control over a node it has
already compromised, or to use a compromised node to ﬁnd and infect additional nodes. The APT
can only act from nodes it has already compromised, though the ACSO can act on any node in the
network. Each node’s state deﬁnes how much it has been compromised by the APT. Different types
of compromise enable the APT to take different actions from or on that node. Compromise levels
also change the way the defensive ACSO actions affect the node. For example, an APT can take
actions on a node in the “escalated privileges” state to resist ACSO actions to change the password.

The baseline APT agent is modeled as a stochastic ﬁnite state machine. The machine states correspond
to the attack-phase tactics shown in ﬁg. 1. Every time-step, the APT ﬁrst updates its machine state
based on its belief over the network. Each machine-state deﬁnes a set of exit-criteria and a stochastic
rules-based sub-policy. The APT may have one of two goals and may use one of two attack vectors
for each episode. The APT goal may be to disrupt the PLC process under control or to destroy the
PLC-controlled equipment.

4 Solution Methods

The proposed solution uses an augmented DQN algorithm to learn a policy deﬁned by our attention-
based neural network. We implemented several extensions to the baseline algorithm, based on
studies of Rainbow DQN [30]. The included extensions are double-DQN [31], prioritized experience
replay [32], and n-step TD loss [33]. The training loss for a given step is

LT D

(cid:0)Qπ(ht, a)(cid:1) =

(cid:16) t+n
(cid:13)
(cid:88)
(cid:13)
(cid:13)

τ =t+1

γτ −trτ +γnQφ

(cid:0)ht+n, argmaxa(cid:48)Qπ(ht+n, a(cid:48))(cid:1)(cid:17)

−Qπ(ht, a)

(cid:13)
(cid:13)
(cid:13) (3)

where ht is the history of observations at time t, Qπ is the action value estimate of the policy network
and Qφ is the action value estimate of the target network. The (cid:107) · (cid:107) represents the Huber-loss norm.
This loss is calculated over batches of importance-weighted samples from an experience replay buffer
and used to estimate the gradient for each network update.

In addition to the task reward presented in eq. (1), a shaping reward was deﬁned based on the
formulation of Ng, Harada, and Russell [34]. This reward was designed to incentivize the agent to

5

Figure 3: Attention Network: Inputs for each network node are passed into attention sub-graphs
based on node type. These sub-graphs embed each node’s history to a latent vector representation.
These vectors are stacked and passed through a self attention sub-graph, to provide global context to
each latent vector. These vectors are then passed through fully connected sub-graphs to output action
value estimates for each node.

secure compromised nodes. The shaping function was deﬁned as

rshape(s, a, s(cid:48)) = γ(AδW + BδS)

(4)

where δW and δS are changes in the number of workstations and servers compromised by the APT
from state s to s(cid:48), respectively, and A and B are weight factors. The weighted sum of eq. (1)
and eq. (4) were used for training. Only eq. (1) was used for evaluation.

Training hyper-parameters were tuned by a grid search training on a smaller ICS network with ten
level 2 workstation nodes, three level 1 workstations nodes, and thirty PLCs. We searched over the
shaping reward weight, the observation-history interval, the target network update frequency, and the
(cid:15)-greedy exploration decay schedule. The parameter set leading to the highest average performing
policy over several seeds after 500 episodes was selected. We used the PyTorch framework for neural
network implementation and training [35]. Numerical values for the training parameters as well as
additional practical details on the training process can be found in the appendix.

4.1 Neural Network

The ACSO problem has input and output spaces with dimensions that scale with the number of
nodes in the ICS network. Node observations are represented as vectors o(i)
, where each element of
t
the vector indicates whether the corresponding alert or action was observed on that node i during that
step t. Workstation node observations have 16 elements, server node observations have 14 elements,
and PLC observations have 7 elements. The complete observation for a given time-step contains the
observation vector for each node in the ICS network, with 792 total elements.

Because the problem is partially observable, a history of prior observations was used as the network
input each step. At time t, a history of the previous τ steps was input to the network, such that the
complete input was Ht = Ot−τ :t. In this work, 256 prior steps were used, for a total input size of
202,752. The output space consists of all valid actions at each time-step, for a total of 329 elements.

Specialized neural architectures can take advantage of structure inherent in input data to reduce
complexity. For example, convolutional nets leverage spatial invariance in images and recurrent
networks encode correlation in sequences [36], [37]. We designed the neural network shown in ﬁg. 3
using attention mechanisms [38] to accommodate the large input space without signiﬁcant degradation

6

30xHost Attention Host History3xServer Attention Server History50xPLC Attention PLC HistoryGlobal Attention Node State30xHost MLP Host Action Values3xServer MLPServer Action Values50xPLC MLP PLC Action ValuesNo-op MLPNo-Op ValueHost ObservationsServer ObservationsPLC Observations(a) Temporal Attention

(b) Global Attention

Figure 4: Attention Sub-Graphs: Figure (a) shows the temporal attention graph, which alternates
layers of multi-headed attention with 1D temporal convolution. Figure (b) shows the global attention
sub-graph, which alternates fully connected layers with multi-headed attention. A skip connection is
included to facilitate gradient back-propagation.

of explanatory capacity or intractable growth in size. Attention mechanisms have been shown to
improve learning efﬁciency on tasks with exchangeable inputs [39], [40].

Each node history is input to its own sub-graph which embeds its history array to a single latent state
vector. All sub-graphs of a given node type share the same parameter set, so a growth in the number
of nodes does not cause a growth in the total number of network parameters. The latent states of each
individual node are then stacked and input to a global attention sub-graph. This sub-graph allows the
network to learn which features of neighboring nodes are relevant to the value function of actions on
a given node. These contextualized node vectors are then passed to feed-forward output sub-graphs.
Like the input sub-graphs, network parameters are shared between nodes of the same type.

The temporal attention and global attention sub-graphs are shown in ﬁg. 4. The temporal attention
sub-graphs alternate 1D convolution in the time dimension with multi-headed, dot-product attention
layers so that the network may learn to attend to elements of the input sequence that are important to
solving the problem. The global self-attention sub-graph alternates multi-headed dot-product attention
mechanisms with afﬁne layers. This structure allows the output representation corresponding to a
node i to include relevant information from any other node in the network. Including this learned
global context in the latent representation of each node allows the output space to be factored to
individual sub-graphs for each node.

The proposed neural network architecture had a total of 683,108 parameters. This size does not
necessarily vary with the number of ICS nodes considered. A convolutional neural network was
implemented as a baseline for comparison. The baseline network had four 1D temporal convolutional
layers, followed by a fully connected layer. The number of parameters of the convolutional network
grows with the with the number of network nodes nodes, with 1,240,329 total parameters for the
tested network. We designed the baseline to be as close in size to the baseline network as possible
without shrinking the latent dimensions too quickly. Details of the convolutional network architecture
are given in the appendix.

4.2 Pre-Training Method

The problem was difﬁcult to effectively explore using (cid:15)-greedy random exploration. To accelerate
the learning process, we implemented a supervised pre-training step similar to the AlphaStar expert
training [7]. We used a stochastic rules based expert policy to generate a batch of example trajectories.
The agent was then trained to minimize a composite loss term over this data.

7

Attn-CNNLayer (3x)Conv-1DLeaky ReLUMulti-Head AttentionBatch-NormInputs (h x o)Outputs (1 x k)Self-AttnLayer (2x)Multi-Head AttentionFully ConnectedLeaky ReLUBatch-NormInputs (m x k)Outputs (m x z)Fully-Connected++(a) TD-Loss

(b) Discounted Return

Figure 5: Training Results: The left ﬁgure shows the average TD-loss per-episode for each architecture
and pre-training combination. The right ﬁgure shows the time discounted sum of rewards for each
episode. Both graphs show curves of the original data with an exponentially smoothed curve in bold.

The pre-training loss term was based on a composite loss for distributional DQN [41]. The loss is
deﬁned as

LP (Q) = LT D(Q) + λLLM (Q)
(5)
where LT D is the temporal difference loss deﬁned in eq. (3), LLM is a large-margin classiﬁcation
loss, and λ is a weighting hyper-parameter. The margin loss term is deﬁned as

LLM (Q) = max
a∈A

(cid:2)Q(h, a) + l(aE, a)(cid:3) − Q(h, aE)

l(aE, a) =

(cid:26)Q(h, a) − Q(h, aE) + δ,

0,

if aE (cid:54)= a
otherwise

(6)

(7)

where aE is the action selected by the expert policy and δ is the desired margin. This term encourages
the values of actions taken by the expert policy to have a value higher than all others by at least δ.
Including the TD-loss term in the pre-training loss ensures that the value estimates are approximately
Bellman-consistent, reducing forgetting early in RL learning.

5 Experiments

We trained our neural network to solve the ACSO task using the proposed pre-training method and
DQN algorithm. Trained policy performance was evaluated over a batch of 100 episodes, with APT
goal and attack vector ﬁxed across episodes. We ran experiments to evaluate the contribution of
each novel element by comparing it to baseline policies and training methods. These experiments
measured the performance of the converged policies and the sample efﬁciency of the learning.

Both network architectures were pre-trained using the proposed composite loss. These pre-trained
networks were then tuned with RL for 500 episodes each. RL training was conducted using the
hyper-parameters found during grid-search. Each architecture was also trained using DQN without
pre-training, using 5 randomly initialized networks to test the impact of pre-training. Training details
are in the appendix.

Figure 5 shows the learning performance of each experiment conﬁguration. For the randomly
initialized networks, the best-performing seed is shown. The episode average TD-loss of eq. (3) and
discounted task return for 1.25 million episode steps are shown. As can be seen, our neural network
TD loss decays much faster than the convolutional baseline both with and without pre-training, though
pre-training tended to slow loss decay. The action-value margin of eq. (6) causes the agent to more
often select actions similar to the pre-trained policy. This reduces how much the policy encounters
other observation-action pairs, slowing learning of their values. For the convolutional network, the
TD-decay was initially accelerated with pre-training, however, the network quickly diverged after
approximately 200,000 steps. This may be due to the the convolutional neural network being more
sensitive to inconsistent action-value estimates induced by the pre-training.

8

0.00.20.40.60.81.01.2Episode Steps1e60.050.100.150.200.25TD-LossDQN Training LossConvolutional RandomNovel ERRORConvolutional PretrainedNovel Random0.00.20.40.60.81.01.2Episode Steps1e6400500600700800900Discounted ReturnDQN Training ReturnConvolutional RandomNovel ERRORConvolutional PretrainedNovel RandomThe learning performance of the discounted return shows similar trends to the TD-loss, with our archi-
tecture generally reaching convergence faster the convolutional network. Additionally, our network’s
converged training performance was generally higher than the performance of the convolutional
network. Unlike the TD-loss trends, pre-training accelerated the improvement in discounted return,
allowing our architecture to approach converged performance much more quickly. The pre-training
negatively impacted the performance of the convolutional architecture.

The evaluation performance of the best trained policy from each conﬁguration is shown in table 1.
The mean performance over 100 episodes along with one standard error bounds are given for four
performance metrics. Task return is the time discounted sum of rewards deﬁned by eq. (1). The action
cost is 10× the average cost per-step of defensive actions taken by the ACSO. PLC downtime is the
total number of hours that PLCs were off-nominal over each 5,000 hour episode. Compromise time
is the average number of IT nodes with an off-nominal state per simulation hour. The performance of
the expert policy used to generate the pre-training data is also given.

Network

Pre-Train

Return

Action Cost

PLC Downtime Compromise Time

Ours

Conv

Expert

Yes
No

Yes
No

–

940.4 ± 0.0
941.0 ± 0.4

0.59 ± 0.01
0.59 ± 0.01

791.5 ± 2.7
738.6 ± 5.0

0.26 ± 0.01
0.57 ± 0.01

906.9 ± 4.5

0.72 ± 0.01

0.0 ± 0.0
0.0 ± 0.0

6.2 ± 0.2
5.0 ± 0.3

0.2 ± 0.0

0.2 ± 0.0
1.0 ± 0.0

3.9 ± 0.0
3.5 ± 0.1

1.3 ± 0.2

Table 1: Evaluation Performance: This table shows the results of evaluating each trained policy over
100 trials. Evaluations of the expert policy used for pre-training trajectories is also given. The best
performance for each metric is shown in bold.

As can be seen, our neural network greatly outperformed the convolutional network and the expert
policy, both with and without pre-training. Our pre-trained network discounted return and action
cost approximately equal those of the randomly initialized network. Interestingly, the pre-trained
policy had a signiﬁcantly lower average number of compromised nodes per hour than any other
policy, suggesting that it was able to beneﬁt from the increased learning rate to ﬁnd a more effective
policy. Both convolutional networks failed to outperform the expert policy, despite the expert
policy trajectories having been used to pre-train the convolutional network. The novel networks
outperformed the convolutional on task return despite having a higher action cost due to the fact
that it successfully prevented more PLC downtime. This suggests that the actions it learned to take
were both more aggressive and more effective at impeding APT progress. Conversely, the expert
policy incurred a higher average action cost than both novel networks. This suggests that though the
expert policy was successful in preventing PLCs from going down, it was inefﬁcient compared to the
learned strategies.

6 Conclusions

This work presented a POMDP formulation of the cyber network security orchestration problem
for ICS networks. To enable this work and future research in this domain, we implemented a low
computational cost ICS network attack simulation that is efﬁcient enough for deep RL methods.
A novel DQN architecture was shown to enable deep reinforcement learning to accommodate the
large observation and action spaces of the problem. The proposed pre-training improved the learning
performance of our architecture.

We ran several experiments to measure the effect each contribution had on learning efﬁciency and
converged policy performance. The experiments show that both our proposed network architecture
and the pre-training improve learning rate and converged performance. The results suggest that
deep reinforcement learning may be a viable method to implement automated security agents to
mitigate the threat of APT attacks. The proposed contributions may also be applied to solve other
large-scale, real world problems with discrete spaces. Results suggest that a neural architecture
encoding the correct implicit bias can not only improve training efﬁciency, but may be required to
learn an acceptable policy.

9

To build on this work, a simulation with increased ﬁdelity to real-world network dynamics is needed.
Some future directions this may take are to tune parametric network models to real-world network
data or to implement simulators with virtual machine kernels and real, known exploits. The former is
likely the more promising path, as the latter poses signiﬁcant computation and security challenges.
Including a model of an actual ICS process and linking the RL reward to process efﬁciency is also
another path to investigate. More robust attacker models with a wider set of attack vectors such as
IDS signal spooﬁng or compromise of the ACSO compute assets should be investigated.

More solution methods will also be explored with the proposed network architecture. Recurrent neural
networks and variational networks are alternatives to sliding window histories that may provide better
beliefs over the network state. Integrating attention mechanisms with graph neural networks [42] may
enable application of learned policies to ICS networks with dissimilar architectures. Policy gradient
methods, such as phasic policy gradient [43], will also be explored as an alternative to DQN.

The current work only considers a ﬁxed, stochastic attacker model. To improve the robustness of
learned policies, adversarial learning will be explored to generate more challenging attacker strategies.
Similarly, the alert and transition dynamics were modeled as stationary, though these may evolve in
real-world network operations. Stress testing of these dynamics will also be considered. Assurance
and veriﬁcation of ACSO actions, especially on safety-critical ICS will be investigated.

Acknowledgments and Disclosure of Funding

This material is based upon work supported by the Johns Hopkins University Applied Physics
Laboratory. The work was supported by the Stanford University Institute for Human-Centered AI
(HAI) and Google Cloud.

References

[1] R. Das and M. Z. Gündüz, “Analysis of cyber-attacks in iot-based critical infrastructures,”
International Journal of Information Security Science, vol. 8, no. 4, pp. 122–133, 2020.
[2] T. Alladi, V. Chamola, and S. Zeadally, “Industrial control systems: Cyberattack trends and
countermeasures,” Computer Communications, vol. 155, pp. 1–8, 2020. DOI: 10.1016/j.
comcom.2020.03.007.

[3] M. Li, W. Huang, Y. Wang, W. Fan, and J. Li, “The study of apt attack stage model,” in 2016
IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS), 2016,
pp. 1–5. DOI: 10.1109/ICIS.2016.7550947.

[4] R. Langner, “Stuxnet: Dissecting a cyberwarfare weapon,” IEEE Security and Privacy, vol. 9,

no. 3, pp. 49–51, 2011. DOI: 10.1109/MSP.2011.67.

[5] G. Liang, S. R. Weller, J. Zhao, F. Luo, and Z. Y. Dong, “The 2015 ukraine blackout: Impli-
cations for false data injection attacks,” IEEE Transactions on Power Systems, vol. 32, no. 4,
pp. 3317–3318, 2016.

[6] A. Di Pinto, Y. Dragoni, and A. Carcano, “TRITON: The ﬁrst ics cyber attack on safety

instrument systems,” in Black Hat USA, 2018, pp. 1–26.

[7] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi,
R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang,
L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V.
Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, Ç. Gülçehre, Z. Wang, T. Pfaff, Y.
Wu, R. Ring, D. Yogatama, D. Wünsch, K. McKinney, O. Smith, T. Schaul, T. P. Lillicrap,
K. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver, “Grandmaster level in starcraft II using
multi-agent reinforcement learning,” Nature, vol. 575, no. 7782, pp. 350–354, 2019. DOI:
10.1038/s41586-019-1724-z.

[8] C.-J. Hoel, K. Driggs-Campbell, K. Wolff, L. Laine, and M. J. Kochenderfer, “Combining
planning and deep reinforcement learning in tactical decision making for autonomous driving,”
IEEE Transactions on Intelligent Vehicles, vol. 5, no. 2, pp. 294–305, 2019.

[9] G. Dulac-Arnold, R. Evans, P. Sunehag, and B. Coppin, “Reinforcement learning in large

discrete action spaces,” Computing Research Repository, 2015. arXiv: 1512.07679.

10

[10] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning for multia-
gent systems: A review of challenges, solutions, and applications,” IEEE Transactions on
Cybernetics, vol. 50, no. 9, pp. 3826–3839, 2020. DOI: 10.1109/TCYB.2020.2977374.
[11] M. J. Hausknecht and P. Stone, “Deep recurrent q-learning for partially observable MDPs,” in

AAAI Conference on Artiﬁcial Intelligence (AAAI), 2015, pp. 29–37.

[12] M. Andrychowicz, D. Crow, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,
P. Abbeel, and W. Zaremba, “Hindsight experience replay,” in Advances in Neural Information
Processing Systems (NeurIPS), 2017, pp. 5048–5058.
J. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, J. Brandstetter, and S. Hochre-
iter, “RUDDER: return decomposition for delayed rewards,” in Advances in Neural Information
Processing Systems (NeurIPS), 2019, pp. 13 544–13 555.

[13]

[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A.
Riedmiller, “Playing atari with deep reinforcement learning,” Computing Research Repository,
2013. arXiv: 1312.5602.

[15] C. Wei and T. Ma, “Data-dependent sample complexity of deep neural networks via lipschitz
augmentation,” in Advances in Neural Information Processing Systems (NeurIPS), 2019,
pp. 9722–9733.

[16] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune, “First return, then explore,”

Nature, vol. 590, no. 7847, pp. 580–586, 2021.

[17] K. Stouffer, J. Falco, and K. Scarfone, “Guide to industrial control systems (ICS) security,”

NIST special publication, vol. 800, no. 82, pp. 16–16, 2011.

[18] C. Glenn, D. Sterbentz, and A. Wright, “Cyber threat and vulnerability analysis of the US

[19]

electric sector,” Idaho National Laboratory Technical Report, 6–16, 2016.
J. R. Gosler and L. Von Thaer, “Resilient military systems and the advanced cyber threat,”
Defense Science Board Technical Report, 30–31, 2013.

[20] S. Meckl, G. Tecuci, D. Marcu, M. Boicu, and A. B. Zaman, “Collaborative cognitive assistants
for advanced persistent threat detection,” in AAAI Conference on Artiﬁcial Intelligence (AAAI),
2017, pp. 171–178.

[21] B. E. Strom, A. Applebaum, D. P. Miller, K. C. Nickels, A. G. Pennington, and C. B. Thomas,

“MITRE ATT&CK®: Design and philosophy,” Mitre Technical Report, 2016.

[22] G. Apruzzese, M. Colajanni, L. Ferretti, A. Guido, and M. Marchetti, “On the effectiveness of
machine and deep learning for cyber security,” in IEEE Conference on Cyber Conﬂict (CyCon),
2018, pp. 371–390.

[23] S. M. Milajerdi, R. Gjomemo, B. Eshete, R. Sekar, and V. N. Venkatakrishnan, “HOLMES: real-
time APT detection through correlation of suspicious information ﬂows,” in IEEE Symposium
on Security and Privacy, 2019, pp. 1137–1152. DOI: 10.1109/SP.2019.00026.

[24] S. Naseer, Y. Saleem, S. Khalid, M. K. Bashir, J. Han, M. M. Iqbal, and K. Han, “Enhanced
network anomaly detection based on deep neural networks,” IEEE Access, vol. 6, pp. 48 231–
48 246, 2018.

[25] T. T. Nguyen and V. J. Reddi, “Deep reinforcement learning for cyber security,” Computing

Research Repository, 2019. arXiv: 1906.05799.

[26] A. Gupta and Z. Yang, “Adversarial reinforcement learning for observer design in autonomous

systems under cyber attacks,” Computing Research Repository, 2018. arXiv: 1809.06784.

[27] T. J. Williams, “The Purdue enterprise reference architecture,” in Information Infrastructure

Systems for Manufacturing, vol. B-14, 1993, pp. 43–64.

[28] M. Egorov, Z. N. Sunberg, E. Balaban, T. A. Wheeler, J. K. Gupta, and M. J. Kochenderfer,
“POMDPs.jl: A framework for sequential decision making under uncertainty,” Journal of
Machine Learning Research, vol. 18, 26:1–26:5, 2017.

[29] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba,

“Openai gym,” Computing Research Repository, 2016. arXiv: 1606.01540.

[30] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B.
Piot, M. G. Azar, and D. Silver, “Rainbow: Combining improvements in deep reinforcement
learning,” in AAAI Conference on Artiﬁcial Intelligence (AAAI), 2018, pp. 3215–3222.
[31] H. van Hasselt, “Double q-learning,” in Advances in Neural Information Processing Systems

(NeurIPS), 2010, pp. 2613–2621.

11

[32] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience replay,” in Interna-

tional Conference on Learning Representations (ICLR), 2016.

[33] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, Second. The MIT

Press, 2018, pp. 141–145.

[34] A. Y. Ng, D. Harada, and S. J. Russell, “Policy invariance under reward transformations:
Theory and application to reward shaping,” in International Conference on Machine Learning
(ICML), 1999, pp. 278–287.

[35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N.
Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S.
Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-
performance deep learning library,” in Advances in Neural Information Processing Systems
(NeurIPS), 2019, pp. 8024–8035.

[36] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation with deep convolu-
tional neural networks,” in Advances in Neural Information Processing Systems (NeurIPS),
2012, pp. 1106–1114.

[37] K. Cho, B. van Merrienboer, Ç. Gülçehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y.
Bengio, “Learning phrase representations using RNN encoder-decoder for statistical machine
translation,” in Conference on Empirical Methods in Natural Language Processing (EMNLP),
2014, pp. 1724–1734. DOI: 10.3115/v1/d14-1179.

[39]

[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I. Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing
Systems (NeurIPS), 2017, pp. 5998–6008.
J. Mern, D. Sadigh, and M. J. Kochenderfer, “Object exchangability in reinforcement learning,”
in International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2019,
pp. 2126–2128.
J. Mern, D. Sadigh, and M. J. Kochenderfer, “Exchangeable input representations for rein-
forcement learning,” in American Control Conference (ACC), 2020, pp. 3971–3976.
[41] T. Hester, M. Vecerík, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A.
Sendonaris, I. Osband, G. Dulac-Arnold, J. P. Agapiou, J. Z. Leibo, and A. Gruslys, “Deep
q-learning from demonstrations,” in AAAI Conference on Artiﬁcial Intelligence (AAAI), 2018,
pp. 3223–3230.

[40]

[42] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio, “Graph attention

networks,” in International Conference on Learning Representations (ICLR), 2018.

[43] K. Cobbe, J. Hilton, O. Klimov, and J. Schulman, “Phasic policy gradient,” Computing

Research Repository, 2020. arXiv: 2009.04416.

12

