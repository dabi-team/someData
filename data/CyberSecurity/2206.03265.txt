2
2
0
2

n
u
J

7

]

R
C
.
s
c
[

1
v
5
6
2
3
0
.
6
0
2
2
:
v
i
X
r
a

Marvolo: Programmatic Data Augmentation for
Practical ML-Driven Malware Detection

Michael D. Wong
Princeton University
mikedwong@cs.princeton.edu

Edward Raff
Booz Allen Hamilton
raff_edward@bah.com

James Holt
Laboratory for Physical Sciences
holt@lps.umd.edu

Ravi Netravali
Princeton University
ravian@cs.princeton.edu

Abstract

Data augmentation has been rare in the cyber security domain due to technical
difﬁculties in altering data in a manner that is semantically consistent with the
original data. This shortfall is particularly onerous given the unique difﬁculty of
acquiring benign and malicious training data that runs into copyright restrictions,
and that institutions like banks and governments receive targeted malware that will
never exist in large quantities.
We present MARVOLO, a binary mutator that programmatically grows malware
(and benign) datasets in a manner that boosts the accuracy of ML-driven mal-
ware detectors. MARVOLO employs semantics-preserving code transformations
that mimic the alterations that malware authors and defensive benign developers
routinely make in practice , allowing us to generate meaningful augmented data.
Crucially, semantics-preserving transformations also enable MARVOLO to safely
propagate labels from original to newly-generated data samples without mandating
expensive reverse engineering of binaries. Further, MARVOLO embeds several key
optimizations that keep costs low for practitioners by maximizing the density of di-
verse data samples generated within a given time (or resource) budget. Experiments
using wide-ranging commercial malware datasets and a recent ML-driven malware
detector show that MARVOLO boosts accuracies by up to 5%, while operating on
only a small fraction (15%) of the potential input binaries.

1

Introduction

Many approaches have been developed to aid practitioners in distinguishing malicious ﬁles (i.e.,
malware) from benign ones. Early solutions centered on employing static or dynamic analyses of
binaries to identify indicators of malicious behavior, e.g., stealing user credentials [1]. However,
each faces signiﬁcant drawbacks: static analysis relies on manually-speciﬁed signatures that struggle
to generalize to newer malware variants, while dynamic analyses bring high computational costs
and virtual environments that are detectable by malicious programs (which can then ﬂy under the
radar) [2]. More recently, a slew of data-driven strategies have been developed to sidestep the above
issues by training ML models to distinguish between benign and malicious executables [3, 4, 2, 5–14].

Despite their promise, ML-based solutions face a signiﬁcant practical challenge: obtaining represen-
tative and labeled training data is infeasible for many organizations. On the one hand, commercial
datasets with these properties exist, but are unattainable for many due to ﬁnancial constraints [5, 15],
with the licensing needed can cost $400k/year. On the other hand, home-grown datasets face scaling
and labeling challenges, e.g., benign samples are often closed-source or copyright-protected, and

Preprint. Under review.

 
 
 
 
 
 
labeling involves error-prone manual analysis to reverse engineer each binary. Consequently, many
security practitioners and researchers only have access to small datasets that lack the heterogeneity
seen in the wild [16]. For example, we ﬁnd that recent ML detectors achieve accuracies of only
60-71% when trained on small public datasets [17, 18] versus their large, commercial counterparts;
such degradations are unacceptable given that single-digit accuracy improvements (and any new
detected malware) are celebrated by malware analysts [19]. We note that these small datasets are also
realistic in representing the challenge applying ML to targeted or otherwise unique malware families
of interst (e.g., targeted banking malware) rather than broad and indiscriminate malware.

This problem is not unique to malware detection. To overcome the challenge of small and incom-
prehensive datasets, many researchers have successfully resorted to data augmentation to artiﬁcially
grow their datasets and has been especially prominent in the computer vision communities with
techniques such as image cropping and scaling [20–22]. In contrast, augmentation techniques for
malware datasets have been almost non-existing and operate over feature representations (instead of
actual data) due to the difﬁculty in analyzing and reasoning about raw binaries [2, 23, 24].

To overcome this obstacle, we present MARVOLO, a binary mutator that programmatically grows
(accessible) malware datasets in a manner that directly boosts the accuracy of ML-driven malware
detectors. Using MARVOLO, we show the feasibility of meaningfully applying data augmentation
techniques to the domain of malware detection. The driving insight behind MARVOLO’s data
augmentation strategy is drawn from our analysis of binaries in high-accuracy (but difﬁcult to access)
malware datasets (§3). In particular, we observe that these datasets routinely contain multiple versions
of a given malware ﬁle that differ based on the effects of semantics-preserving code transformations,
i.e., alterations to the code that change aesthetics, but not externalized behavior [25]. The reason
is intuitive: producing malware requires signiﬁcant effort, and once a malware binary becomes
detectable, code transformations are a quick way for malware authors to preserve malicious behavior
while sidestepping discernible patterns.

Building on the above observation, MARVOLO performs a wide range of semantics-preserving code
transformations on existing binaries in an input dataset. Crucially, this approach naturally results in
automatic (accurate) labeling of the augmented data samples. The reason is that semantics-preserving
transformations inherently preserve overall code behavior. Thus, labels of pre-transformed binaries
can be safely carried over to the transformed versions. To the best of our knowledge, MARVOLO is
the ﬁrst effort made in performing a deep-dive analysis on binaries in existing datasets to employ
meaningful data augmentation techniques to malware detection datasets.

Though conceptually straightforward, realizing MARVOLO’s approach in practice is complicated by
the fact that performing code transformations on binaries is time-consuming and resource-intensive.
Naively decompiling, mutating, and reassembling a binary can take tens of seconds to several hours.
Thus, employing this methodology on even the small datasets that practitioners have access to
can take thousands of hours, with the resource expenditure foregoing any cost savings from not
purchasing realistic commercial datasets. To overcome this, MARVOLO embeds two complementary
optimizations that collectively maximize the utility (i.e., number of realistic and diverse data samples)
of the performed transformations within a user-speciﬁed time budget: (1) Code similarity clustering,
which clusters binaries of similar compositions and operates on only a single binary from each cluster
to circumvent costly operations while preserving diverse interactions between code alterations and
other sections in a binary and (2) intermediate binary generation, which increases the number of
diverse binaries output from each pass through the pipeline using a lightweight check to determines
the efﬁcacy of outputting a binary – based on code discrepancies from the original and previously
output versions – after each transformation.

We evaluated MARVOLO using the recent MalConv [4] malware detector and multiple commercially-
available large/small-scale datasets, i.e., the large-scale Ember [5] dataset, as well as a small-scale
Brazilian dataset [17]. Overall, we ﬁnd that MARVOLO boosts MalConv’s accuracies by up to 6%,
with most wins coming from accurately detecting previously unseen binary families – such scenarios
are intuitively more difﬁcult to catch, but are the primary goal of any malware detection system,
highlighting the practical utility of MARVOLO. Further, relative to performing straightforward code
transformations, MARVOLO’s optimizations enable MalConv to reap these beneﬁts while operating on
85% fewer binaries (resulting in speedups of 79×). We will open-source MARVOLO post-publication.

2

2 Background and Related Work

Though prior attempts have been made in data augmentation for malware detection, they do not yet
perform meaningful data augmentation. In [26, 27], data augmentation for malware detection is done
by representing programs as sequences of opcodes and replacing one opcode with another without
preserving semantics. Further, [28] augments images generated from malware, which are known to
be ﬂawed representation [4]. In contrast to these efforts, MARVOLO’s contributions lie in (1) a deep-
dive analysis of large-scale malware datasets to uncover the usage patterns of semantics-preserving
code transformations by malware authors, and (2) a system that leverages those insights to efﬁciently
grow small datasets into larger ones with improved heterogeneity and realism that aid end-to-end
ML-based malware detection.

Due to its prevalence, and greater difﬁculty, our work focuses on Microsoft Windows Portable
Executable (PE) malware. Decades of development and compilation to machine code make PE
processing and parsing highly non-trivial [29], resulting in many levels of processing done by
malware detectors to trade off speed and accuracy [4, 30–32].

Malware detection involves both static and dynamic analysis techniques [2]. Classic Static analysis
approaches primarily involved using a tool such as Yara [31] to generate speciﬁc rules or patterns
for identifying malicious ﬁles. However, static signatures fail to keep pace with the rapidly evolving
space of deployed malware variants [19] and can take days of manual effort [33, 34]. Malware
detectors rooted in dynamic analysis [35] execute a binary in a sandbox to observe its behavior while
restricting potential damage. Dynamic approaches step past the limitations of static analyses, the
required analysis can be computationally expensive because each ﬁle often must be executed multiple
times to elicit harmful behavior. Worse, some malicious binaries embed checks to detect whether they
are running a virtual (sandbox) environment based on VM properties such as the amount of available
DRAM, the number of cores, the list of installed applications/tools, and even the temperature of the
CPU [2] and dynamically alter their behavior to evade detection.

To address the above limitations and deliver detection accuracy (and generalization), data-driven
techniques using deep learning models have seen signiﬁcant traction in recent years. These models
typically consist of neural networks that determine whether or not a given binary is malicious or
benign based on various, deﬁning features of that binary. For instance, certain models run inference
over PE header values, assembly code, network trafﬁc, and even the names of binaries [2, 36]. Others
follow a dynamic approach and perform manual feature engineering of API calls [37]. Most recently,
the MalConv CNN [4] performs malware detection by operating directly over the raw bytes in a
binary, thereby eschewing labor-intensive feature engineering and the need for domain expertise.

2.1 The Problem: Limited (Realistic) Data

The effectiveness of data-driven malware detectors heavily depends on the data used to train the
corresponding neural networks. Unfortunately, to date, it is practically difﬁcult for practitioners to
obtain access to training datasets that are sufﬁciently representative of malware in the wild.

Commercial datasets that contain massive amounts of labeled data samples for malware detection
do exist and have been used to train models that deliver excellent malware detection accuracy in the
wild [5]. For instance, the popular Ember dataset contains 1.1 million samples and close to 3,000
distinct malware families. However, obtaining the raw executables in the Ember dataset mandates
having a VirusTotal license, which can cost upwards of $400,000 per year!1 While there are datasets
consisting of raw malware binaries [15], they do not contain benign binaries. Thus, practitioners must
download only a limited number of binaries to prevent data imbalance which is far from trivial since
it is difﬁcult to determine a priori which binaries will allow the model to generalize. Consequently,
many cost-constrained practitioners and research groups must resort to far smaller datasets that are
publicly available, e.g., the Brazilian malware dataset contains 50K ﬁles [17], while the Microsoft
malware dataset contains 20K ﬁles with only 9 malware families [18].

1Ember’s free offering omits executables, and only presents a limited number of features per binary, e.g., size,
library functions. These features are insufﬁcient for most existing data-driven malware detectors, and cannot
support long term development: analysts must avoid having adversaries learn about the used features, and cannot
test new features without access to the binaries.

3

On the other hand, practitioners can opt to generate homegrown datasets using honeypots that attract
malware binaries [38]. However, such approaches face three challenges. First, the type of malware that
is gathered is dependent on the collection methodology set by the user, leading to biased datasets [2].
Second, collecting a sufﬁcient number of benign data samples is difﬁcult as benignware does not seek
to replicate across machines (like malware does), and software is often closed-source and copyright-
protected. Along these lines, many seminal works in malware detection have struggled to obtain
benign executables, often collecting them from clean installations [6, 7], but this fails to obtain more
than a few thousand samples. More recent works often rely on partnerships with anti-virus companies
in order to obtain sufﬁcient benign samples [9–14]. This naturally results in unsharable data, causing
reproducibility challenges [2], slows research by non-connected groups, and neglects the needs of
niche and targeted malware[33, 17]. Finally, even if practitioners were to obtain a large number of
samples, labeling them is not straightforward. Software reverse-engineering tools exist [30], but can
consume many hours to reverse engineer a single executable, even for expert analysts [2, 23, 24].

To demonstrate the sensitivity of ML-based malware de-
tectors to dataset composition and size, we ran experi-
ments comparing the efﬁcacy of models trained with com-
mercial large-scale (Ember) and small-scale datasets (the
public Microsoft and Brazilian datasets described above).
Results use the recent MalConv detector [4], and follow
the setup described in §5 (testing is done on the 200K
Ember test set). To contextualize these results, we note
that the implications of detecting even a single additional
malicious binary in the wild can be substantial (§1), and
that single-digit accuracy improvements are celebrated
by malware analysts [19].

Figure 1: Accuracy when training Mal-
Conv [4] on different subsets of the Em-
ber dataset [5].

Takeaway 1: small malware datasets lack heterogeneity, fail to generalize. Across the considered
free, small datasets that are sized between 20-75k samples, MalConv’s accuracy spanned only 60-71%
relative to a training on the full Ember training dataset (600k).

Takeaway 2: large (proven) malware datasets have important diversity that detectors capitalize on.
Figure 1 shows the diminishing accuracy of MalConv when trained on progressively fewer data
samples from the Ember dataset. Starting with the full 600K Ember training dataset, accuracy is at
91%. However, accuracy dips below 80% when trained on subsets sized similarly to existing free
datasets, e.g., 75k samples and less. These results indicate the data-hungry nature of ML-based
malware detectors, and highlight the heterogeneity in data samples in large datasets; we dig deeper
into these aspects in the following section.

3 Approach

Our results from Section 2 highlight the inadequacies of small malware datasets relative to the
large (commercial) datasets that have supported high accuracies for ML-driven malware detectors
in practical settings. However, given the superior attainability of small datasets, our main goal is to
determine whether they can be altered to more closely mimic the properties of their larger counterparts
and deliver similar efﬁcacy when used to train malware detectors. To do so, we programmatically
analyzed the binaries in the large Ember dataset to identify their deﬁning characteristics. We start
with representative case studies that illustrate our ﬁndings, before describing more general takeaways.

Case study I. Figure 2 shows code snippets
from two different malware families in the
Ember dataset: the Zenpak malware family,
and the Sivis malware family.2 The ﬁrst bi-
nary from Zenpak uses a code obfuscation
technique called junk code insertion [25].
Junk code is comprised of instructions that
are executed but do not affect the external-
ized output(s) of the program. Here, junk

Zenpak

inc eax
inc ecx
inc edx
inc ebx
inc esp
inc ebp
inc esi
inc edi

dec eax
dec ecx
dec edx
dec ebx
dec esp
dec ebp
dec esi
dec edi

Sivis
nop
nop
nop
xor eax, eax
inc ebx
dec ebx
inc ecx
dec ecx

inc eax
push edx
xor edx, edx
pop edx
inc eax
dec eax
cmp 0x17b8ef93, eax
jne 0x407033

Figure 2: Code snippets from two malware families
in the Ember dataset that exhibit semantics-preserving
code transformations.

2x86 assembly code samples are written in Intel syntax.

4

600k300k150k75k32k16kTraining Set Size0.7500.7750.8000.8250.8500.8750.9000.9250.950Accuracycode manifests as a series of inc instruc-
tions (line 1-8) that each increment a regis-
ter’s value, immediately followed by dec instructions (lines 9-16) that decrement them.

The binary from Sivis also uses multiple forms of junk code insertion: (1) the nop instructions (lines
1-3) which do not trigger any computation or data movement, (2) the interleaved inc and dec that
sequentially alter the same registers (lines 5-8, 13-14), and (3) lines 10-12 which push the value
of edx onto the stack, set the value of edx to 0 using xor, and then pop the old value of edx from
the stack and store it back into edx (rendering the xor operation useless). The Sivis binary embeds
another code obfuscation technique called opaque predicates [25], which are (typically) known a
priori by a programmer to always evaluate to true or false. This manifests in relation to eax. At the
start of the snippet, eax is deﬁnitively set to 0 after the xor instruction (line 4). However, at the point
of the cmp instruction in line 15, the value stored in eax is deﬁnitively 1 due to the series of inc and
dec operations in the preceding statements. In line 15, since eax (cid:54)= 0x17b8ef93, the jump in the
following jne instruction is always taken.

Binary 1

Binary 2

push ebx
push esi
mov esi,DWORD PTR [ebp+0x8]
push edi
mov eax,ds:0x470208
push 0x7
pop ecx
lea edi,DWORD PTR [ebp-0x2c]

Case study II. Figure 3 depicts snippets
from two sample binaries from the Em-
ber dataset that belong to the same fam-
ily. Unsurprisingly, the two code snippets
are similar at ﬁrst glance. However, there
exist minor differences due to two code
obfuscation techniques that they embed.
First, each binary uses a mov instruction
to write data from the data segment into
eax. However, the data is located in dif-
ferent memory locations across the two
version; the two binaries retrieve the value
from ds:0x470208 and ds:0x324e88, respectively. This pattern is also seen in the lea instructions
where the two binaries use different offsets from the stack base pointer, ebp, to retrieve their values.
In addition, the two binaries use instruction swapping to reorder instructions (in this case, the mov
instruction) in a manner that preserves overall semantics.

Figure 3: Snippets from two binaries in the same “In-
stallMonster” family that exhibit minor differences due
to code obfuscations.

mov eax,ds:0x423e88
push ebx
push esi
mov esi,DWORD PTR [ebp+0x8]
push edi
push 0x7
pop ecx
lea edi,DWORD PTR [ebp-0x28]

d
e
g
n
a
h
C
s
k
c
o
l
B

f
o
%

1.0
0.8
0.6
0.4
0.2
0.0

)

B
M

(

f
f
i

D

15

10

5

0

Function
inlining

Function
outlining

Obfuscating
sub.

Register
reassign

Swapping Transpose

Emotet High Install-
Monster

UrsuWannacry

(a) Percentages of code blocks in Ember’s binaries that are
affected by different code transformations.

(b) Pairwise byte diff results between binaries in
ﬁve representative malware families.

Figure 4: Left shows percentage of code blocks that are mutated by each transformation type. Right
the difference in ﬁle size for a subset of representative mlaware families.

Our case studies highlight two main points (which we repeatedly observed across the Ember dataset):

(1) Semantics-preserving code transformations. Malware authors routinely alter prior versions of
malicious programs using code obfuscation techniques that preserve program behavior. The reason
is intuitive: generating malware involves much manual labor and sophisticated code alteration.
As malware detectors discern already-deployed malware by recognizing patterns in their code
composition or execution regimes (§2), a far less challenging way for malware authors to continue
deploying their malicious code is to perform semantics-preserving code transformations. More
speciﬁcally, these transformations alter that code minimally, so as to preserve its malicious behavior

5

while deviating from the patterns used to detect its predecessor. Unsurprisingly, we did not observe
any remnants of semantics-preserving code transformations in the benign samples that we analyzed.

(2) Combinations of transformations. To ensure sufﬁcient differences from detected malware
versions, malware authors often resort to performing semantics-preserving transformations, e.g.,
as in case study II above. This approach is fruitful as such transformations are often (logically)
complementary, and the effect of each transformation depends on subtle interactions between the
transformation logic and binary code (ranges shown in Figure 4). Additionally, we ﬁnd that, to
further boost diversity with multiple transformations, each obfuscation is not necessarily applied to
all possible blocks in a binary, i.e., some binaries exhibited the effects of an obfuscation in all code
blocks that it applied to, while others demonstrated the effects in only a fraction of those blocks.

Taking a step back, these observations lead to two implications about the large datasets that have
been successfully used for ML-driven malware detection. First, there exist far fewer families of
malicious binaries than malicious binaries themselves; the Ember dataset includes 300K malicious
binary samples spread across only 332 families. There exist many binary versions per family: there
are 287 and 13,951 binaries in the median and 99th percentile families, respectively. Second, the
binaries within each family can differ quite substantially depending on the speciﬁc transformations
that are applied across versions. Figure 4 highlights this property, showing that for subsets of ﬁve
representative families, the constituent binaries exhibit median pairwise percent differences of 38-99%
(which equates to raw differences of 0.8–5.4 MB).

Our approach. The results above motivate a new approach to bolstering the efﬁcacy of the small
datasets that practitioners are often restricted to: data augmentation via semantics-preserving trans-
formations. In other words, we aim to grow small datasets by performing different combinations of
semantics-preserving code transformations on varying numbers of blocks in the constituent binaries.
Doing so mimics the techniques that malware authors use to sidestep malware detectors over time
[19], and yield data similar to that in (proven) large datasets. We employ further code transformations
done by optimizing compilers to generate new benign binaries. Perhaps more importantly, semantics-
preserving transformations provide a direct path to accurately labeling newly generated data without
manual effort since pre- and post-transformation binaries will exhibit the same behavior (and thus
can safely share labels). §4 describes how our system, MARVOLO, practically realizes this approach.

Figure 5: MARVOLO workﬂow shown in (left), and the breakdown of time spent on each state of the
pipeline in (right) for binaries of different sizes. Bars list medians with errors bars for the quartiles.

4 MARVOLO

4.1 Binary rewriting overview

Figure 5 illustrates MARVOLO’s binary mutation process for performing semantics-preserving trans-
formations on a single (malicious) binary. To begin mutation, MARVOLO decompiles existing PE32
binaries using the Ddisasm tool [39] and internally represents the binary as a series of basic instruction
(or code) blocks.

To operate on (i.e., mutate) instruction blocks, MARVOLO ﬁrst disassembles each block. The resulting
blocks are then passed into the MARVOLO code transformation engine, which (1) selects a set of
semantics-preserving code transformations to apply to the binary during a given iteration, (2) analyzes

6

Malicious binaryNew malicious binaryDdisasmUasmMarvoloIdentify basic blocksAnalyze instructionsSwap in new blocksGTIRB fileModified asm fileDisassemble blocksMore transformations?YesNoall blocks to determine which blocks each considered transformation is applicable to, (3) selects
the fraction of potential blocks to apply each transformation to, and (4) sequentially carries out the
transformations on the selected blocks; §4.2 details this process. After code transformations are
complete for a given iteration, MARVOLO then directly swaps out the corresponding (unmodiﬁed)
blocks with their transformed counterparts and invokes an assembler to get the output binary. This
binary is then added to the original dataset and tagged with the same label (i.e., malicious or benign)
as the one used during its generation. This end-to-end process repeats multiple times for each binary
in the dataset in accordance with a user-speciﬁed time or resource budget.

4.2 Code Transformations

MARVOLO currently supports 10 different semantics-preserving code transformations that cover
the set of mutations we observed in our analysis of the popular Ember dataset (§3), as well as
commonly used code obfuscations [40, 41, 25] and transformation techniques employed by off-
the-shelf optimizing compilers [42, 43]. Supported transformations include junk code insertion
and instruction swapping (both described in §3), as well as instruction substitution which replaces
an instruction with a (more complex) sequence of instructions that is semantically equivalent. To
ensure that a modiﬁed code block is semantically equivalent to the original block, static analysis is
performed after the code transformation is applied. This analysis tracks program reads and writes
and determines whether the reads from the registers and memory locations in that basic block would
still return the same values after the modiﬁcation. If a violation occurs from the code transformation,
it is reverted and a new transformation is attempted. Appendix A provides a comprehensive overview
of the transformations that MARVOLO supports, as well as the logistics to carrying out each one.

MARVOLO’s goal is to generate new versions of input binaries that differ in diverse ways from their
originals while adhering to a user-speciﬁed time and/or resource budget (which dictates potential
parallelism across mutation iterations). The main challenge is that it is difﬁcult to determine, a priori,
how a given transformation will alter a given binary – this depends on subtle interactions between
the transformation logic and the binary instructions, which collectively dictate how many blocks are
applicable for a transformation, and how many instructions will be modiﬁed, added, or deleted. Thus,
during each mutation iteration, MARVOLO instead opts to randomly select multiple transformations
for each mutation iteration and stochastically order them. This follows from our ﬁnding that malware
authors typically employ multiple transformations together, and that binaries in the same family can
differ by (largely) varying amounts (§3).

To further bolster variance across the transformed binaries, MARVOLO varies two parameters across
the mutation iterations for each input binary. m speciﬁes the number of transformation iterations to
perform on each binary, and c governs the fraction of blocks to mutate in each iteration. MARVOLO
maintains a running list of parameter values used for a given binary and selects subsequent values
to maximize diversity, i.e., maximizing the distance from all previously used values. Note that
the overarching time budget takes precedence over per-binary parameter values; to enforce this,
MARVOLO round robins through the input binaries, performing one mutation iteration on each one,
and circling back to fulﬁll the selected m per binary only if time permits.

4.3 Optimizations for Practicality

Sources of inefﬁciency. Binary mutation of a single executable with MARVOLO (as described thus
far) can be broken down into 3 stages: (1) invoking Ddisasm on the binary (decompilation), (2)
carrying out semantics-preserving code transformations (mutation), and (3) generating the output
binary (reasssembly). We proﬁled the runtime of each stage by passing 3K random binaries from
Ember through MARVOLO.

As shown in Figure 5, all three stages consume substantial time: median values for the three stages
across binary sizes are 0.6–33, 0.1–585, and 0.1–34 seconds, respectively. We additionally observed
that per-stage delays grow as binary sizes grow and span upwards of 460, 961, and 44 seconds.
Accordingly, aiming to even perform a single mutation iteration on each binary in existing small
datasets (which would not fully bridge the size gap with large datasets) could take up to several
thousand hours! The associated resource costs would forego the savings that practitioners reap by not
purchasing existing large datasets. Instead, MARVOLO embeds the following two optimizations to

7

boost MARVOLO’s utility for a given time budget; we evaluate the effectiveness of each one in §5,
and provide more details in Appendix B.

(1) Code similarity clustering. A clustering strategy to group binaries based on their compositions.
Only a single binary per cluster is operated on, and the resulting code blocks are rapidly (but safely,
from a semantics perspective) dropped into the other binaries in the same cluster. This approach
circumvents costly operations for all-but-one binary per cluster, while preserving diverse interactions
between code alterations and other sections in each binary.

(2) Intermediate binary generation. A technique to increase the number of diverse binaries output
from each pass through the pipeline. The main difﬁculty is that it is difﬁcult to (efﬁciently) determine,
a priori, the effects that a transformation will have on a given binary’s code blocks. Thus, MARVOLO
opts for a dynamic approach, whereby a lightweight runtime check determines the efﬁcacy of
outputting a binary – based on code discrepancies from the original and previously output versions –
after each transformation that is performed in a pipeline pass.

5 Evaluation

To evaluate MARVOLO, we used the recent MalConv CNN-powered malware detector [4]. For
context, MalConv’s model spans 5 layers, with an embedding layer that maps bytes to vectors, and
then a series of convolutional and recurrent layers. Our experiments consider 2 main datasets: (1)
the high-accuracy commercial Ember dataset that includes 1.1M samples (800K after removing ill-
formed binaries), and (2) the small-scale (free) Brazilian malware dataset [17] with 50K samples.
Given the realism of Ember observed by researchers and practitioners, we use its test set, which
consists of 200K benign and malicious samples, directly to reﬂect malware detection scenarios in the
wild. For training, we consider a subset of the 600K-sample Ember training dataset, as well as the full
Brazilian dataset; we train a separate MalConv model for each case. Our subsets consist of 10-30K
samples, which is consistent with the dataset sizes that many malware research groups currently work
with [44]. While we would have preferred to experiment with augmenting more datasets, we are
constrained since many existing datasets do not contain raw binaries as mentioned in 2.

For each dataset, we train MalConv to convergence, routinely around 5 epochs. Training involves
ﬁrst collecting (converged) “pre-trained” weights on the original training dataset, and then running
an additional training round (5 epochs) with the augmented dataset that MARVOLO generates. All
training was performed on an NVIDIA PH402 with two P100s 32GB. Unless otherwise noted,
MARVOLO employs combinations of all 10 of its supported transformations and generates a set of
mutated binaries (split evenly across malicious and benign ﬁles); the description of each experiment
speciﬁes the number of those mutated samples considered during retraining. Accuracy is reported
as the percentage of correct labels (i.e., benign or malicious) output by MalConv. We run each
experiment four times and report on the distributions.

5.1 Overall Accuracy Improvements

Figure 6a shows the accuracy improvements that MARVOLO brings to MalConv when augmenting the
Ember training dataset with different numbers of mutated samples (ranging from 3-12K). Accuracy
improvements range from 1–5% atop the baseline accuracy of 61.3% achieved when considering the
unmodiﬁed Ember dataset alone. Perhaps more importantly, these results highlight that accuracy
improvements typically come quickly, while operating on only a small number of binaries, e.g.,
adding only 3K and 6K mutated samples to the dataset delivers 3.5% and 5% of accuracy boosts,
respectively. The reason is that MARVOLO’s efﬁciency-centric optimizations promote rapid diversity
amongst the generated samples, which in turn enable MalConv to quickly strike a desirable balance
between (1) learning to detect obfuscation patterns, while (2) not overﬁtting to mutated samples.
Results on the smaller Brazilian malware dataset [17] were comparable: adding 2K mutated ﬁles
delivered median accuracy improvements of 2% (atop the 61% without MARVOLO).

Further analysis reveals that a key driver of the overall accuracy wins delivered by MARVOLO are
improvements on test samples from previously unseen malware families, i.e., families that did not
appear in the training dataset. Recall from §2 that such samples are the ones which static analysis and
small-scale ML approaches typically struggle to generalize to. Figure 6b illustrates this, showing that
MARVOLO’s accuracy boosts on only the subset of test binaries that were not seen during training are

8

)

%

(

.

p
m

I

.
c
c
A

4

2

)

%

(

.

p
m

I

.
c
c
A

4

2

3k 6k 9k 12k
Mutated Samples

3k 6k 9k 12k
Mutated Samples

(a) Accuracy on in-distribution.

(b) Accuracy on novel malware
families only.

(c) Time in each step of MARVOLO without
(left) and with (right) clustering optimization.

Figure 6: Results of MARVOLO augmented training when testing on in-distribution data from Ember
(a), novel malware families not seen in training (b). In addition the impact of our augmentations are
shown in (c) and result in a two order of magnitude speedup to create the augmented training samples.

Figure 7: MARVOLO accuracy and pipeline optimization improvements

Figure 8: MalConv’s accuracy improvements when using a version of MARVOLO that only performs
a single type of semantics-preserving code transformation during mutation. Results are for adding
1K mutated samples in each case to the Ember dataset.

on par with the wins on the complete test set (1–5%). The underlying reason for these improvements is
that code transformations provide a discernible pattern for MalConv to link across diverse binaries in
different families. In light of these results, we provide further analysis of MARVOLO in Appendix C.

5.2 Pipeline Optimization Improvements

Recall from §4 that MARVOLO embeds two optimizations to tackle the overheads in the mutation
process revealed in our proﬁling results. We proﬁled two runs of MARVOLO’s mutation pipeline, one
with the two optimizations enabled, and one without them. Each pipeline was used to generate 3K
mutated samples, and we note that the MalConv models trained on these mutated samples (atop the
Ember dataset) delivered accuracy within 1% of one another.

Figure 6c shows the total time spent (i.e., to generate all 3K mutated samples) in each MARVOLO
pipeline stage across these two variants. Overall, the optimized version of MARVOLO runs 79× faster
to generate 3K mutated samples of similar efﬁcacy (given the near-identical MalConv performance
across the two cases noted above). Speedups are primarily from the lower decompilation and
mutation/reassembly costs, which in turn are due to running only a single binary per cluster through
the pipeline (85% fewer binaries), with each run yielding a larger number of mutated samples. These
drastic drops dwarf the drop-in overheads used to mix (altered) code and data blocks, and the slight
(blocking) overhead of performing clustering prior to mutation; note that clustering overheads are
paid once and steadily decrease in relative importance as the target number of mutated samples grows.

6 Conclusion

MARVOLO is a data augmentation engine that boosts the efﬁcacy of the malware datasets that
practitioners commonly are restricted to by performing semantics-preserving code transformations on
the constituent binaries. To the best of our knowledge, we are the ﬁrst to leverage insights from a deep-

9

dive analysis of existing malware datasets to apply meaningful data augmentation to the domain of
malware detection. Key to MARVOLO’s practicality are its ability to (safely) propagate labels across
input and output binary samples, and its optimizations to boost the number of fruitful (i.e., diverse and
representative) data samples generated within a ﬁxed time budget. Experiments using commercial
malware datasets and a recent ML-driven malware detector show that MARVOLO boosts accuracies
by up to 5%, while operating on only 15% of the available binaries (mutation speedups of 79×).

References

[1] T. Z. Tebogo Mokoena, “Malware analysis and detection in enterprise systems,” in IPSA/IUCC, 2017.

[2] E. Raff and C. Nicholas, “A survey of machine learning methods and challenges for windows malware

classiﬁcation,” in ML-RSA, 2020.

[3] M. Krcal, O. Svec, O. Jasek, and M. Balek, “Deep convolutional malware classiﬁers can learn from raw

executables and labels only,” in ICLRW, 2018.

[4] E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, and C. Nicholas, “Malware Detection
[Online]. Available:

by Eating a Whole EXE,” arXiv preprint arXiv:1710.09435, oct 2017.
http://arxiv.org/abs/1710.09435

[5] H. S. Anderson and P. Roth, “EMBER: An Open Dataset for Training Static PE Malware Machine Learning

Models,” in arXiv 1804.04637, 2018.

[6] J. Z. Kolter and M. A. Maloof, “Learning to detect malicious executables in the wild,” in ACM SIGKDD,

2004.

[7] “Malware detection using statistical analysis of byte-level ﬁle content.” ACM Press, 2009, pp. 23–31.

[Online]. Available: http://portal.acm.org/citation.cfm?doid=1599272.1599278

[8] Y. Ye, D. Wang, T. Li, and D. Ye, “IMDS: Intelligent Malware Detection System,” in ACM SIGKDD, 2007,

pp. 1043–1047.

[9] Y. Fan, S. Hou, Y. Zhang, Y. Ye, and M. Abdulhayoglu, “Gotcha - Sly Malware! Scorpion: A Meta-

graph2vec Based Malware Detection System,” in ACM SIGKDD, 2018, pp. 253–262.

[10] A. Tamersoy, K. Roundy, and D. H. Chau, “Guilt by Association: Large Scale Malware Detection by

Mining File-relation Graphs,” in ACM SIGKDD, 2014, pp. 1524–1533.

[11] M. Bozorgi, L. K. Saul, S. Savage, and G. M. Voelker, “Beyond Heuristics: Learning to Classify Vulnera-

bilities and Predict Exploits,” in ACM SIGKDD, 2010, pp. 105–114.

[12] Y. Ye, T. Li, S. Zhu, W. Zhuang, E. Tas, U. Gupta, and M. Abdulhayoglu, “Combining File Content and

File Relations for Cloud Based Malware Detection,” in ACM SIGKDD, 2011, pp. 222–230.

[13] Y. Ye, T. Li, Y. Chen, and Q. Jiang, “Automatic Malware Categorization Using Cluster Ensemble,” in ACM

SIGKDD, 2010, pp. 95–104.

[14] E. Raff and C. Nicholas, “An Alternative to NCD for Large Sequences, Lempel-Ziv Jaccard Distance,” in

ACM SIGKDD, 2017, pp. 1007–1015.

[15] R. Harang and E. M. Rudd, “Sorel-20m: A large scale benchmark dataset for malicious pe detection,” 2020.

[16] R. J. Joyce, D. Amlani, C. Nicholas, and E. Raff, “MOTIF: A Large Malware Reference Dataset with
Ground Truth Family Labels,” in The AAAI-22 Workshop on Artiﬁcial Intelligence for Cyber Security
(AICS), 2022. [Online]. Available: https://github.com/boozallen/MOTIF

[17] F. Ceschin, F. Pinagé, M. Castilho, D. Menotti, L. S. Oliveira, and A. Grégio, “The Need for Speed: An

Analysis of Brazilian Malware Classiﬁers,” in IEEE Security & Privacy, 2018.

[18] R. Ronen, M. Radu, C. Feurstein, E. Yom-Tov, and M. Ahmadi, “Microsoft Malware Classiﬁcation

Challenge,” in arXiv 1802.10135, 2018.

[19] “Labs Report at RSA: Evasive Malware’s Gone Mainstream,” https://bit.ly/3p2lH5G, 2021, accessed:

2021-10-07.

[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural

networks,” in NeurIPS, 2012.

10

[21] Shorten, C. and Khoshgoftaar, T.M., “A survey on image data augmentation for deep learning.” in Journal

of Big Data, 2019.

[22] S. Zhao, Z. Liu, J. Lin, J.-Y. Zhu, and S. Han, “Differentiable augmentation for data-efﬁcient gan training,”

in NeurIPS, 2020.

[23] D. Votipka, S. M. Rabin, K. Micinski, J. S. Foster, and M. M. Mazurek, “An Observational Investigation of

Reverse Engineers’ Processes,” in USENIX Security, 2020.

[24] O. A. Abedelaziz Mohaisen, “Unveiling Zeus: Automated Classiﬁcation of Malware Samples,” in WWW

Companion, 2013.

[25] I. You and K. Yim, “Malware obfuscation techniques: A brief survey,” in BWCCA, 2010.

[26] J. M. d. R. Niall McLaughlin, “Data augmentation for opcode sequence based malware detection,” in arXiv

2106.11821, 2021.

[27] K. Z. Jason Wei, “Eda: Easy data augmentation techniques for boosting performance on text classiﬁcation

tasks,” in IJCNLP, 2020.

[28] Ferhat Ozgur Catak, Javed Ahmed, Kevser Sahinbas, Zahid Hussain Khand, “Data augmentation based

malware detection using convolutional neural networks,” in PeerJ Computer Science, 2021.

[29] “Portable Executable,” https://en.wikipedia.org/wiki/Executable_and_Linkable_Format, 2021, accessed:

2021-10-05.

[30] “Ghidra software reverse engineering framework,” https://ghidra-sre.org/, 2021, accessed: 2021-09-28.

[31] “Yara: The pattern matching swiss knife for malware researchers (and everyone else),” http://virustotal.

github.io/yara/, 2021, accessed: 2021-08-07.

[32] S. Jamalpur, Y. Sai Navya, P. Raja, G. Tagore, and G. Rama Koteswara Rao, “Dynamic malware analysis

using cuckoo sandbox,” in IEEE ICICCT), 2018.

[33] E. Raff, R. Zak, G. L. Munoz, W. Fleming, H. S. Anderson, B. Filar, C. Nicholas, and J. Holt, “Automatic

yara rule generation using biclustering,” in ACM CCS AISec, 2020.

[34] D. Votipka, S. M. Rabin, K. Micinski, J. S. Foster, and M. M. Mazurek, “An Observational Investigation of

Reverse Engineers ’ Processes,” in USENIX Security Symposium, 2019.

[35] Ri Or-meir, Nir Nissim, Yuval Elovici, Lior Rokach, “Dynamic malware analysis in the modern era—a

state of the art survey,” in ACM Computing Survey 52, 5, Article 88), 2018.

[36] A. T. Nguyen, E. Raff, and A. Sant-Miller, “Would a ﬁle by any other name seem as malicious?” in IEEE

Big Data, 2019.

[37] Huang, W., and Stokes, J. W., “Mt-Net: A Multi-Task Neural Network for Dynamic Malware Classiﬁcation,”

in DIMVA, 2016, pp. 6528–6537.

[38] N. Bhagat and B. Arora, “Intrusion detection using honeypots,” in PDGC, 2018.

[39] A. Flores-Montoya and E. Schulte, “Datalog Disassembly,” in 29th USENIX Security Symposium, 2020.

[40] P. Junod, J. Rinaldini, J. Wehrli, and J. Michielin, “Obfuscator-LLVM — Software Protection for the

Masses,” in SPRO, 2015.

[41] “The tigress diversifying c virtualizer,” https://tigress.wtf/, 2021, accessed: 2021-08-28.

[42] Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman, “Compilers: Principles, Techniques,

and Tools,” 1986.

[43] “Code obfuscation,” https://en.wikibooks.org/wiki/X86_Disassembly/Code_Obfuscation, 2021, accessed:

2021-08-20.

[44] Michael R. Smith, Nicholas T. Johnson, Joe B. Ingram, Armida J. Carbajal, Bridget I. Haus, Eva Domschot,
Ramyaa Ramyaa, Christopher C. Lamb, Stephen J. Verzi, W. Philip Kegelmeyer, “Mind the Gap: On
Bridging the Semantic Gap between Machine Learning and Malware Analysis,” in ACM CCS AISec, 2020.

[45] R. Wartell, Y. Zhou, K. W. Hanlen, M. Kantarcioglu, and B. Thuraisingham, “Differentiating code from

data in x86 binaries,” in ECML PKDD, 2011.

[46] A. H. Ibrahim, M. B. Abdelhalim, H. Hussein, and A. Fahmy, “Analysis of x86 instruction set usage for
windows 7 applications,” in International Conference on Computer Technologies and Development, 2011.

11

A MARVOLO’s Code Transformations

Junk code insertion. Insert instructions into the binary that don’t alter the output of the program
upon being executed. These instructions may change the state of the program (e.g., register values
and memory) but reverse the changes before progressing to subsequent instructions. The simplest
form of this transformation that we implement is the insertion of nop instructions. We also generate
semantic nops which consist of pushing values onto the stack, performing arithmetic and logical
operations, and then popping the values off once they’re completed. We augment this with additional
instructions that also read and write to memory. In the following semantic nop

eax, 0x1c

push eax
inc eax
or
add eax, dword ptr [esp - 0x34]
not eax
pop eax

the eax register is ﬁrst pushed to the stack. Then arithmetic and bitwise operations are performed on
eax. Lastly, the old value of eax is popped from the stack and written back into eax; since the value
of eax is not written elsewhere prior to pop eax, the computations are effectively useless.
Register reassignment. Changes the names of the variables or registers. Identify a live register, rX,
within a basic block and replace it with a new register, rY, that is unused within the block. The value
of rY is ﬁrst pushed onto the stack and is then written with the value stored in rX. After computations
are performed on rY, it is written to rX and the original value of rY is popped and written back to rY.
Function inlining. Identify functions and every time they are invoked, replace the call instructions
with the bodies of the identiﬁed functions. In our implementation, we solely focus on functions with
straight-line code. Function inlining is a common compiler optimization used to reduce the overhead
of invoking a function and to make basic blocks more amenable to subsequent optimizations.
Function outlining. Identify straight-line instructions within the current basic block and generate a
new function with those instructions. Replace the original instructions with a call instruction to the
newly-generated function. This is a compiler optimization for reducing code size.
Obfuscating Instruction substitution. Replace an instruction with a semantically equivalent se-
quence of new instructions. We currently support over 30 substitutions. We add simple substitutions
such as changing add rX, 1 to sub rX, -1. We adopt further instruction substitutions, including
many implemented in LLVM Obfuscator [40]. These substitutions are mostly comprised of more
complex bitwise and arithmetic instructions. For instance, MARVOLO would replace the instruction
or eax,0x4711 with

push esi
push edi
mov esi, eax
mov edi, 0x4711
and eax, edi
xor esi, edi
or
eax, esi
pop edi
pop esi

The transformation is effectively replacing a = b|c with a = (b & c)|(b ⊕ c).
Optimizing instruction substitution. Replace an instruction with an equivalent instruction that opti-
mizing compilers often emit [43]. While these instructions are often times not as intuitive as their
more straightforward counterparts, they are faster to execute. For instance, mov rX, 0 is often times
changed to xor rX, rX. Another instance is substituting arithmetic instructions, such as add, with
lea instructions. Applying this transformation more broadly captures the range of programs that can
be produced by different compiler toolchains and options.
Code transposition. This transformation reorders a sequence of instructions that changes the appear-
ance of the code without altering the behavior [25]. MARVOLO implements code transformation by
dividing a basic block into smaller slices. Then these slices are rearranged in a different order and are
each appended with an unconditional jmp instruction to ensure that the original execution order of
the initial basic block is preserved.

12

Instruction swapping. As another form of code transposition, we take 2 instructions and swap their
positions. While this transformation does not signiﬁcantly affect the readability of the code, it is used
by malware authors to evade anti-virus scanners. To ensure that the transformation preserves seman-
tics, analysis is performed to check that the swap doesn’t violate any computational dependencies.
We check that each of the destination registers for the instructions aren’t used as a source register for
other instructions. We also check that any source registers used by the two instructions aren’t written
to. Below we demonstrate an example; the left side shows the original program and the right side
shows the modiﬁed program after the add and sub instructions had been swapped.

Original

Mutated

add eax, ebx
sub ecx, 0x7c21
ret

sub ecx, 0x7c21
add eax, ebx
ret

On the other hand, the program

mov eax, 0x1af3
add ecx, eax

is not amenable to swapping since the add instruction would not use the updated value in eax after
the mov instruction.
Opaque predicate insertion. Opaque predicates are predicates that always evaluate to true or false
and are known a priory the programmer. While opaque predicates evaluate to the same value under
all inputs, they are still evaluated during runtime. To represent the instances where code and data are
interleaved within a binary [45], we generate a sequence of randomly-generated bytes following the
opaque predicate. An unconditional jmp instruction is inserted so that these generated bytes are not
executed and the next instructions within the program are run. Opaque predicates are commonly
inserted by code obfuscators. [40].
Function reordering. Functions are moved to different positions throughout the binary. This
transformation drastically changes the appearance of the binary without adding new instructions or
removing existing ones.

B MARVOLO Optimizations

Code similarity clustering. To reduce the number of binaries passed through the mutation pipeline,
MARVOLO employs a clustering strategy to group binaries together based on their compositions (and
thus, their interactions with the pipeline). Efﬁciency wins come from passing only a single binary
per cluster through the pipeline. Intuitively, the goal for clustering is thus to maximize cluster sizes
without masking differences between the binaries in the dataset.

Unfortunately, the straightforward clustering strategy of grouping binaries based on byte similarity
(i.e., cluster binaries whose byte-level differences are smaller than a pre-determined threshold) are
ill-suited for our task. The reason is that, even malicious binaries within the same family that
exhibit identical .text and .data sections may have vast byte-level differences (upwards of tens of
thousands of bytes). Though massive, these differences do not alter the overall behavior of the binary,
and thus should not map binaries to different clusters. Yet unearthing such insights requires passing
the binary through costly decompilation, foregoing many savings.

Instead, MARVOLO leverages our ﬁnding that, within a malware family, it is not uncommon for multi-
ple binaries to have equivalent code sections; note that these binaries commonly differ in their .data
and .rsrc sections – we discuss this below. Since MARVOLO only performs code transformations,
these are the only portions of the binary that MARVOLO modiﬁes; it is thus redundant to send binaries
with identical .text sections through MARVOLO’s pipeline. Consequently, MARVOLO operates on
only a single binary per observed code section. For each generated mutated version of the binary,
MARVOLO performs drop-in replacement (i.e., avoiding costly decompilation and reassembly) of the
transformed code section with other binaries in the same cluster; memory location offsets are quickly
updated in each affected binary. In effect, this rapidly simulates the process of passing all binaries in
a cluster through the end-to-end pipeline.

13

Note that modifying .data and .rsrc sections in a binary may not deliver semantic equivalence.
In contrast to semantics-preserving code transformations that guarantee equivalent behavior across
program inputs, data-level modiﬁcations can alter the taken control ﬂows in a program, resulting
in different externalized values. In light of this, MARVOLO only performs drop-in replacement for
binaries in the same cluster, i.e., that have identical code sections to the one which passed through the
mutation pipeline. This ensures that code-data relationships are unchanged since the same control
ﬂows would be traversed during binary execution, which in turn ensures safety in propagating labels
to newly generated binaries.

Intermediate binary generation. The goal of MARVOLO’s second optimization is to maximize the
useful binaries output during each pass through the mutation pipeline. Recall that, for each input
binary, MARVOLO’s pipeline (as described thus far) selects and performs a series of transformations
to generate a single mutated binary. Thus, a simple approach to increase pipeline outputs for a given
run would be to output a mutated binary after each successive transformation is performed. The issue
is that the generated binaries will only differ by a single transformation pass and thus will likely fail
to deliver the heterogeneity seen in large datasets; recall from §3 that malware authors commonly use
multiple transformations to ensure substantial differences from the original malware binaries. Instead,
we must ensure that the generated binaries diverge substantially from one another.

The challenge is that it is difﬁcult to know a priori how many bytes a transformation will change in
a given binary (§4.2). To handle this, MARVOLO employs a lightweight runtime check after each
transformation is applied to determine whether the code changes performed up until that point are
comprehensive enough to warrant a new binary generation (and thus assembly). Logically, the runtime
checks compare byte-level diffs between the current binary version, the original, and those output after
prior transformations; if all values exceed a pre-set threshold, MARVOLO deems the current binary
worthy of costly assembly (and thus, a new sample in the dataset).3 To ensure that discrepancies only
pertain to behavior-affecting portions of the binary without requiring costly assembly and binary-
wise diffs (which we ﬁnd can consume tens of seconds), MARVOLO approximates this behavior by
tracking the number of code blocks affected after each step (scaled based on the inherent intrusion
level of the applied transformation [46]).

C Analyzing MARVOLO

Importance of number of binaries mutated. Figures 6a and 6b show MARVOLO’s performance
as the number of added mutated binaries changes. As discussed, the beneﬁts from MARVOLO’s
mutations come early from the perspective that most accuracy wins can be realized by using only
a small fraction of the overall dataset as input; we observe this trend over multiple datasets of
different sizes. More generally, however, MARVOLO’s performance with regards to input size is
collectively governed by two factors – (1) the overall dataset size, and (2) the number of input samples
– that inﬂuence the relationship between the utility of malware detection insights from newly added
(mutated) samples and the risk of overﬁtting. Intuitively, larger datasets require larger numbers of
mutated samples to reap beneﬁts because they already exhibit a sufﬁcient amount of heterogeneity
(as shown in Figure 1), and they are also far less susceptible to overﬁtting (as the weight of each
added sample is relatively smaller).

Importance of different transformations. To study the effect that each of MARVOLO’s ten code
transformations have on accuracy improvements, for each transformation (in isolation), we generated
two sets of 1K mutated samples: one where all mutated samples were benign, and one where all
mutated samples were malicious. Figure 8 shows the accuracy improvements for MalConv running
on the Ember dataset plus each of the 20 mutated datasets (one at a time). For benign ﬁles, instruction
swapping, obfuscating substitutions, and function inlining yielded the largest accuracy wins, with
4%, 6%, and 4% performance gains, respectively. For malicious ﬁles, register reassignment, code
transposition, and opaque predicate insertion were the most fruitful with 5%, 4%, and 5% performance
gains, respectively. The reason is that the latter trio of transformations are more invasive (i.e., they
lead to larger code alterations and resultant diffs), and are hence more often applied by malware
authors to circumvent recently employed detection patterns.

3While most binaries within a family have signiﬁcant differences, some exhibit only minor differences
between one another. Thus, MARVOLO occasionally (10% of the time, by default) outputs binary versions even
if the diff threshold has not been exceeded.

14

Further, our results in Section 3 highlight that malware authors not only use many different kinds
of code transformations, but also diverse combinations of them. Thus, MARVOLO currently opts
for a general randomized selection of transformations and combinations during mutation. However,
to make the most use of (limited) compute resources, a practitioner could identify which code
transformations are present in the samples that they already have, and focus the augmentation process
on under-represented ones.

Using MARVOLO. Indeed MARVOLO is intended to complement existing ML-driven malware detec-
tors and we do not propose changing hyperparameters but we recommend keeping the hyperparameter-
tuning methodology the same after data augmentation. Beyond these hyperparameters, we note two
additional considerations:

1. Input seclection. MARVOLO performs best when presented with inputs comprising a diverse
set of binaries that differ (as the dataset allows) in family and composition, e.g., binaries with
large fractions of differing code portions. Doing so aids malware detectors in identifying
the underlying transformations (injected by MARVOLO) across wider-ranging contexts.
Further, as noted above, MARVOLO must balance generating sufﬁcient mutated samples to
boost heterogeneity in training datasets, while avoiding overﬁtting to those samples. Our
current implementation leverages that accuracy boosts come early (i.e., with few samples)
and overﬁtting occurs soon after, motivating an iterative process starting with only a small
number of samples.

2. Transformation selection. Our results in Section 3 highlight that malware authors not
only use many different kinds of code transformations, but also diverse combinations
of them. Thus, MARVOLO opts for a general randomized selection of transformations
and combinations during mutation. However, to make the most use of (limited) compute
resources, a practitioner could identify which code transformations are present in the samples
that they already have, and focus the augmentation process on under-represented ones.

15

