Date of publication

Digital Object Identiﬁer

Deep Q-Learning based Reinforcement
Learning Approach for Network Intrusion
Detection

HOOMAN ALAVIZADEH1 (Member, IEEE), JULIAN JANG-JACCARD1, and HOOTAN
ALAVIZADEH2
1Cyber Security Lab, Comp Sci/Info Tech, Massey University, Auckland, New Zealand (e-mail: {h.alavizadeh,j.jang-jaccard}@massey.ac.nz)
2Imam Reza International University, Mashhad, Iran (e-mail: hootan.alavizadeh@gmail.com)

Corresponding author: Julian Jang-Jaccard (e-mail: j.jang-jaccard@massey.ac.nz).

This work is supported by the Cyber Security Research Programme–Artiﬁcial Intelligence for Automating Response to Threats from the
Ministry of Business, Innovation, and Employment (MBIE) of New Zealand as a part of the Catalyst Strategy Funds under the grant
number MAUX1912.

ABSTRACT The rise of the new generation of cyber threats demands more sophisticated and intelligent
cyber defense solutions equipped with autonomous agents capable of learning to make decisions without
the knowledge of human experts. Several reinforcement learning methods (e.g., Markov) for automated
network intrusion tasks have been proposed in recent years. In this paper, we introduce a new generation of
network intrusion detection method that combines a Q-learning based reinforcement learning with a deep
feed forward neural network method for network intrusion detection. Our proposed Deep Q-Learning (DQL)
model provides an ongoing auto-learning capability for a network environment that can detect different
types of network intrusions using an automated trial-error approach and continuously enhance its detection
capabilities. We provide the details of ﬁne-tuning different hyperparameters involved in the DQL model
for more effective self-learning. According to our extensive experimental results based on the NSL-KDD
dataset, we conﬁrm that the lower discount factor which is set as 0.001 under 250 episodes of training yields
the best performance results. Our experimental results also show that our proposed DQL is highly effective
in detecting different intrusion classes and outperforms other similar machine learning approaches.

INDEX TERMS Network Security, Deep Q Networks, Deep Learning, Q-Learning, Reinforcement
Learning, Epsilon-greedy, Network Intrusion Detection, NSL-KDD, Artiﬁcial Intelligence

1
2
0
2

v
o
N
7
2

]

R
C
.
s
c
[

1
v
8
7
9
3
1
.
1
1
1
2
:
v
i
X
r
a

I. INTRODUCTION

N EW generation of Intrusion Detection Systems (IDSs)

increasingly demands automated and intelligent net-
work intrusion detection strategies to handle threats caused
by an increasing number of advanced attackers in the cyber
environment [1]–[3]. In particular, there have been high
demands for autonomous agent-based IDS solutions that
require as little human intervention as possible while being
able to evolve and improve itself (e.g., by taking appropriate
actions for a given environment), and to become more robust
to potential threats that have not seen before (e.g., zero-day
attacks) [4].

Reinforcement Learning (RL) has become a popular ap-
proach in detecting and classifying different attacks using
automated agents. The agent is able to learn different behav-
ior of attacks launched to speciﬁc environments and formu-

lates a defense strategy to better protect the environment in
the future. An RL approach can improve its capability for
protecting the environment by rewarding or penalizing its
action after receiving feedback from the environment (e.g.,
in a trial-and-error interaction to identify what works better
with a speciﬁc environment). An RL agent is capable of
enhancing its capabilities over time. Due to its powerful
conception, several RL-based intrusion detection techniques
have been proposed in recent years to provide autonomous
cyber defense solutions in various contexts and for different
application scenarios such as IoT, Wireless Networks [5], [6],
or Cloud [7], [8]. The RL agent is able to implement the self-
learning capabilities during the learning process based on its
observation without any supervision requirement that typi-
cally involves the expert knowledge from human [9]. Many
network intrusion detection techniques have been proposed

VOLUME 4, 2016

1

 
 
 
 
 
 
based on this RL concept [10].

However, most of the existing approaches suffer from
the uncertainty in detecting legitimate network trafﬁc with
appropriate accuracy and further lacks the capability to deal
with a large dataset. This is because an RL agent typically
needs to deal with very large learning states and would
encounter the state explosion problem. In recent years, deep
reinforcement learning (DRL) techniques have been pro-
posed that are capable of learning in an environment with
an unmanageable huge number of states to address the main
shortcoming of existing RL techniques. DRL techniques such
as deep Q-learning have shown to be a promising method to
handle the state explosion problem by leveraging deep neural
networks during the learning process [11].

Many DRL-based IDS for network intrusion detection
techniques have been proposed in the existing literature lever-
aging different types of intrusion datasets to train and evalu-
ate their models [12], [13]. However, most of these existing
proposals only focus on enhancing their detection ability
and performance compared to other similar approaches. The
majority of these existing works do not offer comprehensive
studies as to how best to develop and implement a DRL-
based IDS approach for a network environment without
providing the precise details such as how the DQL agent
can be formulated based on an RL theory or how to ﬁne-
tune hyperparameters for more effective self-learning and
interact with the underlying network environment. In this
paper, we address these shortcomings by introducing the
details of design, development, and implementation strategy
for the next generation of the DQL approach for network
intrusion detection.

The main contributions of our work are summarized as

follows:

• We introduce a new generation of network intrusion
detection methods that combine a Q-learning based
reinforcement learning with a deep feed forward neural
network method for network intrusion detection. Our
proposed model is equipped with the ongoing auto-
learning capability for a network environment it inter-
acts and can detect different types of network intrusions.
Its self-learning capabilities allow our model to contin-
uously enhance its detection capabilities.

• We provide intrinsic details of the best approaches in-
volved in ﬁne-tuning different hyperparameters of deep
learning-based reinforcement learning methods (e.g.,
learning rates, discount factor) for more effective self-
learning and interacting with the underlying network
environment for more optimized network intrusion de-
tection tasks.

• Our experimental

results based on the NSL-KDD
dataset demonstrate that our proposed DQL is highly
effective in detecting different intrusion classes and
outperforms other similar machine learning approaches
achieving more than 90% accuracy in the classiﬁcation
tasks involved in different network intrusion classes.

Alavizadeh et al.:

TABLE 1: List of features on NSL-KDD dataset

F#

Feature Name

F#

Feature Name

F#

Feature Name

Duration
Protocol_type
Service
Flag
Src bytes
Dst bytes
Land

F1
F2
F3
F4
F5
F6
F7
F8 Wrong fragment
F9
F10 Hot
F11 Num_failed_logins
F12 Logged_in
F13 Num compromised
F14 Root shell

Urgent

Su attempted

F15
F16 Num root
F17 Num ﬁle creation
F18 Num shells
F19 Num access ﬁles
F20 Num outbound cmds
Is host login
F21
F22
Is guest login
F23 Count
F24
F25
F26
F27 Rerror rate
F28

Srv count
Serror rate
Srv serror rate

Srv rerror rate

Same srv rate

Srv diff host rate

F29
F30 Diff srv rate
F31
F32 Dst host count
F33 Dst host srv count
F34 Dst host same srv rate
F35 Dst host diff srv rate
F36 Dst host same srv port rate
F37 Dst host srv ﬁff host rate
F38 Dst host serror rate
F39 Dst host srv serror rate
F40 Dst host rerror rate
F41 Dst host srv rerror rate
F42 Class label

The rest of the paper is organized as follows. Section II
presents the related work. Section III discusses the essen-
tial concepts and background associated with reinforcement
learning and deep neural network. Section IV represents
the NSL-KDD dataset used for this paper. The proposed
DQL-based anomaly detection approach is given in V. The
evaluation, experimental results, and analysis are given in
Section VI. Finally, we conclude the paper in Section VII.

II. RELATED WORK
Q-learning, considered to be a model-free method, has been
hailed to be a promising approach, especially when utilized in
challenging decision processes. This method is appropriate if
the other techniques such as traditional optimization methods
and supervised learning approaches are not applicable [14].
The advantages of Q-learning are its effective results, learn-
ing capabilities, and the potential combination with other
models.

The application of machine learning such as Deep Rein-
forcement Learning (DRL) [15]–[17], supervised and unsu-
pervised learning, in cybersecurity, has been investigated in
various studies [13], [14], [18], [19]. In [18], the authors
studied a comprehensive review of DRL for cybersecurity.
They studied the papers based on the live, real, and simulated
environment. In [19], the authors showed the applications
of DRL models in cybersecurity. They mainly focused on
the adversarial reinforcement learning methods. They also
presented the recent studies on the applications of multi-
agent adversarial RL models for IDS systems. In [13], the
authors studied several DRL algorithms such as Double Deep
Q-Network (DDQN), Deep Q-Network (DQN), Policy Gra-
dient (PG), and Actor-Critic (AC) to intrusion detection using
NSL-KDD [20] and AWID [21] datasets. Those datasets
were used for train purposes and classifying intrusion events
using the supervised machine learning algorithms. They
showed the advantages of using DRL in comparison with the
other machine learning approaches which are the application
of DRL on modern data networks that need rapid attention
and response. They showed that DDQN outperforms the
other approaches in terms of performance and learning.

In [22], [23], the authors proposed a deep reinforcement
learning technique based on stateful Markov Decision Pro-

2

VOLUME 4, 2016

Alavizadeh et al.:

cess (MDP), Q-learning. They evaluated the performance
of their method based on different factors such as learning
episodes, execution time, and cumulative reward and com-
pared the effectiveness of standard planning-based with a
deep reinforcement learning based approach.

In [24], the authors proposed a reinforcement learning
agent installed on routers to learn from trafﬁc passing through
the network and avoid trafﬁc to the victim server. Moreover,
[25] proposed a machine learning method to detect multi-
step attacks using hidden Markov models to predict the
next step of the attacker. In [26], the authors proposed a
decision-theoretic framework named ADRS based on the
cost-sensitive and self-optimizing operation to analyze the
behavior of anomaly detection and response systems in au-
tonomic networks.

In [27], the authors combined the multi-objective decision
problem with the evolutionary algorithms to make an efﬁ-
cient intrusion response system. They considered an intrusion
response system as a multi-attribute decision making prob-
lem that takes account of several aspects before responding to
the threats such as cost of implementation, resource restric-
tion, and effectiveness of the time, and modiﬁcation costs.
This multi-objective problem tried to ﬁnd an appropriate
response that was able to reduce the values of these functions.
Most of the existing works focused only on the enhance-
ment of their methods to provide better performance and on
the evaluation of the performance of the proposed techniques
through comparison with other similar machine learning
(ML)-based approaches.

III. BACKGROUND
A. REINFORCEMENT LEARNING
Modeling a system as a Markov Decision Process (MDP)
so that an agent can interact with the environment based
on different discrete time steps is an important aspect in
designing many decision-making related problems. MDP can
be shown as a 5-tuple: M=(S,A, T , R,γ) where S denotes a
set of possible states and A indicates a set of possible actions
that the agent can perform on the environment, and T denotes
the transition function from a state to another state. T deﬁnes
the (stationary) probability distribution on S to transit and
reach a new state s(cid:48). The value of R demotes the reward
function, and γ = [0, 1) indicates the discount factor.

A policy π can be deﬁned to determine the conditional
probability distribution of selecting different actions depend-
ing on each state s. The distribution of the reward sequence
can be determined once a stationary policy has opted. Then,
policy π can be evaluated by an action-value function which
can be deﬁned under π as the expected cumulative discounted
reward based on taking action from state s and following π
policy. By solving the MDP, the optimal policy π∗ can be
found that maximizes the expected cumulative discounted
reward based on all states. The corresponding optimal action
values satisfy Q∗(s, a) = max
Qπ(s, a), and the uniqueness
and existence of the ﬁxed-point solution of Bellman opti-

π

FIGURE 1: DQN model based on agent-environment inter-
action

mality equations can be obtained by Banach’s ﬁxed-point
theorem.

Q∗(s, a) = R(s, a) + γ

(cid:90)

s(cid:48)

T (s(cid:48)|s, a) max

a(cid:48)

Q∗(s(cid:48), a(cid:48))

The essential cyclic process

in the RL is agent-
environment interaction. The RL agent should interact with
the environment to explore and learn from different transition
and reward functions obtained from the action taken. This
process makes the RL agent able to ﬁnd out the optimal
policy, see Figure 1. During the interaction with the environ-
ment at time t, the RL agent observes the information about
the current state s, and then chooses an action a based on
a policy. Then, it receives a reward r from the environment
based on the action taken and moves to a new state s(cid:48). The
RL agent improves itself by experiences gained based on this
cyclic agent-environment interaction. The learning process
could be based on either (i) approximating the transition
probabilities and reward functions to learn the MDP model
and then ﬁnding an optimal policy using planning in the MDP
(i.e., known as the model-based approach), or (ii) trying to
learn the optimal value functions directly without learning
the model and deriving the optimal policy (e.g., model-free
approach).

Q-learning can be considered as a model-free approach
that updates the Q-values estimation based on the experience
samples on each time step as the following equation.

Q(s, a) ← Q(s, a) + α(r + γ max

a(cid:48)

Q∗(s(cid:48), a(cid:48)) − Q(s, a))

in which α is the learning rate, and Q(s, a) is simply the
current estimation.

B. FEED FORWARD NEURAL NETWORK
We utilized a feed forward neural network constructed based
on a fully connected neural network as the main module of
the DRL model for approximating the Q-values and training
the model based on the NSL-KDD Dataset. The intrusion
datasets will be fed into a pre-processing module ﬁrst for
cleansing and preparing the dataset and also extracting the
related features [28]–[30].

The fully connected neural network includes different fully
connected layers that link every single neuron in the layer

VOLUME 4, 2016

3

TABLE 2: NSL-KDD data-record classes

Categories Notation Deﬁnitions

# of Samples

Normal

DoS

Probe

U2R

R2L

N

D

P

U

P

Normal activities based on the features
Attacker tries to avoid users of a service
Denial of Service attack
Attacker tries to scan the target network to collect
information such as vulnerabilities
attackers with local access to victim’s machine
tries to get user privileges
attacker without a local account tries to send
packets to the target host to get access

148517

53385

14077

119

3882

to neurons of the previous layer. The neural network output
f (x) or y is represented in Equation (1) [31].

y = f (x) = F|f |(F|f | − 1(. . . F2(F1(x))))

(1)

where x is the input, Fi is a transformation function, and
|f | denotes the total number of computational layers which
can be either hidden layers and the output layer in the
neural network. The outputs from the preceding layers are
transferred using each perceptron by applying a non-linear
activation function. Finally, the ith perceptron in the tth layer
can be represented as:

(cid:18)

ot
i = ν

wt

i∗.ot−1 + bt
i

(cid:19)

(2)

where ot−1 is the output of the preceding layer, wt
weight vector of the perceptron, bt
non-linear activation function.

i∗ is the
i is its bias and ν is the

Activation functions play an essential role in the training
process of a neural network. Activation functions manage the
computations to be more effective and reasonable as there
is the complexity between the input units and the response
variable in a neural network. The main role of the activation
function is to convert an input unit of a neural network to an
output unit. Different activation functions can be used in a
neural network such as Sigmoid, Tanh, and ReLU. However,
the ReLU activation function is known as a more effective
one comparing with the other activation functions in many
detection problems with lower run-time and demands for less
expensive computation costs, see Equation 3 where z is the
input.

ReLU (z) =

(cid:40)

0
1

ifz < 0
ifz ≥ 0

(3)

IV. DATASET
NSL-KDD dataset is a labeled network intrusion detection
dataset that so far has been used in many tasks to evaluate
different deep learning-based algorithms for devising differ-
ent strategies for IDS [32], [33]. NSL-KDD dataset contains
41 features labeled as a normal or speciﬁc attack type (i.e.,
class). We utilized the one-hot-encoding method for the
dataset preprocessing to change the categorical features to the
corresponding numeral values as deep learning models can
only work with numerical or ﬂoating values. We normalized
the train and test datasets to the values between 0 and 1 using

Alavizadeh et al.:

a mix-max normalization strategy. The 41 features presented
in the NSL-KDD dataset can be grouped into four such
as basic, content-based, time-based, and host-based trafﬁc
features. The value of these features is mainly based on
continuous, discrete, and symbolic values. The NSL-KDD
dataset contains ﬁve attack classes such as Normal Denial-
of-Service (DoS), Probe, Root to Local (R2L), and Unautho-
rized to Root (U2R). These attack classes can be identiﬁed
using the features corresponding to each NSL-KDD data.
Table 2 deﬁnes the attack classes for NSL-KDD that we
consider in our study.

V. ANOMALY DETECTION USING DEEP Q LEARNING
A. DEEP Q-NETWORKS
One of the most effective types of RL is Q-learning in which
a function approximator such as either a neural network or
a deep neural network is used in RL as a Q-function to
estimate the value of the function. The Q-function integrated
with a deep neural network can be called Deep Q-Learning
(DQL). The Q-learning agent in the DQL can be represented
as Q(s, a; θ). The Q-function consists of some parameters
such as state s of the model, action a, and reward r value.
The DQL agent can select an action a and correspondingly
receives a reward for that speciﬁc action. The neural network
weights related to each layer in the Q-network at time t are
denoted by the θ parameter. Moreover, si+1 or s(cid:48) represents
the next state for DQL model. The DQL agent moves to
the next state based on the previous state s and the action
a performed in the previous state s. A deep neural network is
used as the deep Q-network to estimate and predict the target
Q-values. Then, the loss function for each learning activity
can be determined by Q-values obtained on the current and
previous states. In some cases, only one neural network is
used for estimating the Q-value. In this case, a feedback loop
is constructed to estimate the target Q-value so that the target
weights of the deep neural network are periodically updated.

B. DEEP R-LEARNING CONCEPTS
Here we deﬁne the important concepts related to DQL based
on the environment where the NSL-KDD dataset is used for
network intrusion detection tasks.

1) Environment
The environment for this study is the one where the pre-
processed and normalized NSL-KDD dataset is used where
the columns (features) of the NSL-KDD dataset denote the
states of the DQN. There are 42 features in NSK-KDD and
we utilize the ﬁrst 41 features as states. Feature 42 is the label
that will be used for computing the award vectors based on
model prediction. Note that in this DQN model, the agent
only obtains actions to compute the rewards vector, and there
is no real action performed to the environment.

2) Agent
DQL agent is utilized in the DQL model based on the
network structure so that there is at least one agent for the

4

VOLUME 4, 2016

Alavizadeh et al.:

FIGURE 2: DQN model prediction using states and deep neural network, the outputs are Q-values, and actions are computed
based on argmax Qi for the current state.

FIGURE 3: State transition Markov diagram for DQN agent training process based on current and next states prediction and
training.

context of a network. The agent interacts with the environ-
ment and applies rewards based on the current state and the
selected action. A DQL agent could be deﬁned as a value-
based RL agent that is able to trains the model to estimate the
future rewards values. A DQN can be trained by an agent
interacting with the environment based on the observation
and possibles action spaces. In the DQL training process, the
agent needs to explore the action space by applying a policy
such as epsilon-greedy exploration. The exploration helps the
agent to selects either a random action with a probability of
(cid:15) or an action greedily based on the value function with the
greatest value with probability 1 − (cid:15).

4) Actions
An action is considered as the decision chosen by the agent
after processing the environment during a given time window
such as after ﬁnishing the process of a mini-batch. The DQN
agent generates a list of actions as an action vector based
on the given input of the neural network and input features.
The ﬁnal Q-values are used to judge whether an attack was
captured succesfully. It feeds the state vector with the size of
the mini-batch to the current DQN. Then, the agent compares
the output of the current DQN based on threshold rates as Q-
values and determined the Q-threshold value for classifying
the attack classes.

3) States

States in DQL describe the input by the environment to an
agent for taking action. In the environment where the NSL-
KDD dataset is used, the dataset features (as in Table 1) are
used for state parameters for DQN. We use those 41 features
as the inputs of DQN such that si = Fi for training and
prediction using DQN.

5) Rewards
In DQL, the feedback from the environment for a corre-
sponding action done by an agent is called a reward. A
reward vector can be deﬁned based on the output values of
DQN and the size of the mini-batch. The DQL agent can
consider a positive reward when the classiﬁcation result of
DQN matches the actual result based on the labels in the
NSL-KDD. Otherwise, it may get a negative reward. The
reward value can be considered depending on the probability
of prediction by the classiﬁer. This value can be adjusted

VOLUME 4, 2016

5

Alavizadeh et al.:

FIGURE 4: DQL agent training phase ﬂowchart.

based on the Q-values obtained to enhance the classiﬁer’s
performance.

C. DEEP Q-LEARNING PROCESS
The standard Q-learning and DQN can be differentiated
based on the method of estimating the Q-value of each
state-action pair and the way in which this value can be
approximated using generalized state-action pair by the func-
tion Q(s,q). The process of DQL is based on the ﬂowchart
represented in Figure 4. The DQN agent deals with the
environment where the NSL-KDD is used. In the ﬁrst step,
the parameters of the algorithm and models are initialized
based on Table 3. The values of the features of NSL-KDD
(F1–F41 as in Table 1) indicate the state’s variables (s) of
the DQN. Note that the batch size (bs) for the DQN process
is set as 500. This means that for each state the amount of

500 records of NSL-KDD are fetched from memory and fed
into one state (S), see the batch table represented in Figure 2.
However, there are 41 features as the state variables each of
which can have various values. Thus, as the number of state-
value pairs becomes comparatively large, it is not possible
to keep them in a Q-table (or look-up table). Thus, the
DQN agent leverages a DNN as the function approximator
to compute Q values based on the states and actions.

Figure 4 presents the overall steps of DQL using an
agent. First, the normalized NSL-KDD dataset is fed into
the environment and the DQL agent initializes a vector for
Q values, state’s variables, actions according to batch size,
the DNN parameters, and weights are initialized. Then, the
learning iterations train the DQN based on the epsilon-greedy
approach. The outer iteration represents different episodes
of the learning process and after each iteration, the value

6

VOLUME 4, 2016

  (a) Environment = NSL-KDD  (b) DQN Agent (Qv,Sv,Av)  (c) Model parameters (weights)Start# InitializationEpisode<num-episode   Si = Fetch (Environment, bs)# States InitializationDetermine  (a) Choose bs random actions         AVi = Rnd (0,1)    (b) epsilon*=decoy-rate# ExplorationWith probability Epsilon (a) QVi =Predict (Current-state) (b) AVi =argmax (QVi) With probability 1-Epsilon# Prediction of current state (a) RVi = Reward_Function (AVi, Labels) (b) S'i = Fetch (Next-state, bs)# Reward-Function  (a) Q'Vi =Predict (S'i)  (b) A'Vi =argmax (Q'Vi) # Prediction of next state  QTi = RVi + Gamma * Q'Vi# Target Q Calculation  (a) QVi = Train (Si, QVi)  (b) Compute-Loss (QVi, QTi) # Learning improvementT< num-IterationYesEndSave TrainedDQN ModelNoEnvironmentSiQViAViAViClass LabelsLabelsRViTrain DNN modelQViSiFigure 2 Alavizadeh et al.:

Parameters

Description

TABLE 3: DQL agent and Neural Network parameters.

Number of episodes to train DQN
Number of iteration to improve Q-values in DQN
Number of hidden layers: Setting weights, producing outputs, based on activation function
number of hidden unit to improve the quality of prediction and training

num-episode
num-iteration
hidden_layers
num_units
Initial weight value Normal Initialization
Activation function Non-linear activation function
Epsilon (cid:15)
Decoy rate
Gamma γ
Batch-size (bs)

Degree of randomness for performing actions
Reducing the randomness probability for each iteration
Discount factor for target prediction
A batch of records NSL-KDD dataset fetched for processing

Values

200
100
2
2 × 100
Normal
ReLU
0.9
0.99
0.001
500

In the DQN learning procedure, there should be an ap-
propriate trade-off between exploitation and exploration. At
the ﬁrst rounds of learning, the exploration rate should be
set as a high probability with the value of approximately 1,
and gradually decreases using a decoy rate, see Figure 5.
The exploration is performed based on the epsilon-greedy
approach. An epsilon-greedy policy is implemented as a
training strategy based on reinforcement learning deﬁnition
which helps the agent to explore all possible actions and
ﬁnd the optimal policy as the number of explorations in-
creases. The action is chosen by applying the epsilon-greedy
approach which selects a random action with a probability of
(cid:15) or predicts the action with a probability of (1 − (cid:15)).

As the batch size for each state is equivalent to bs (deﬁned
in Table 3), bs numbers of random actions will be fed into the
action vector (AV) as AVi = Rnd(0, 5), ∀i ∈ bs, where 0–5
denotes Norman (N), Probe (P), DoS (D), U2R (R), and R2L
(R), respectively. In the ﬁrst set of iterations, the probability
of choosing random actions is high, but as time pasts this
probability gets lower by the epsilon-greedy approach as
in Figure 5. With the probability of 1 − (cid:15), the DQL agent
predicts the actions using the current state (including the ﬁrst
batch with the size of bs and state variables). Note that, the
amount of bs records of the environment denotes the current
state. The features (i.e., variable states) of the current state
are fed into the input layer of DNN architecture and the
Q-values are predicted based on the DNN parameters and
weights in the output layer. The action having the highest
Q values is selected for each record i in the current state as
AVi = argmax(QVi), ∀i ∈ bs which can be either normal
or malicious based on the four attack types.

In the next step, the action vector (AV) ﬁlled by either ran-
dom actions (i.e., with a higher chance in the ﬁrst iterations)
or predicted actions using DQN (i.e., with higher chance
after a while) is fed into the reward function for computing
the rewards based on comparing the AVi with the labels
in the dataset for corresponding data-record, see the reward
function represented in Figure 4. Then, the DQL agent needs
to compute the Q vectors and action vectors of the next state
denoted as Q(cid:48)Vi and A(cid:48)Vi for all i ∈ bs to complete the
training process and DQL principles as illustrated in Figure 3.

FIGURE 5: Exploration strategy using Epsilon-greedy ap-
proach

of states is initialized again. Note that the parameters of the
trained DNN are preserved and are not initialized for each
episode iteration. The state Sn at each discrete state is given
by the training sample (Batchn), see Figure 2. At the end of
each episode, a complete sequence of states, rewards, and
actions are obtained in the terminal state. During the start
of the training, the agents receive the ﬁrst batch (500 records
from the environment), and this is the starting state S1 of the
environment.

In the inner iteration, the DQN agent performs exploration,
action selection, and model training based on DQL. Note
that each iteration adjusts the Q-function approximator which
uses a DNN. In the standard Q-learning, a table of values
is kept and the agent separately updates each state-value
pair. However, DQN utilizes a deep-learning approach to
estimate the Q-function. We leverage a deep neural network
as a function approximator for the Q-function. We use a
deep neural network consisting of 4-layers as represented
in Figure 2, with ReLU activation for all layers, including
the last one to ensure a positive Q-value. Layer one is the
input layer which includes 41 neurons and is fed with the
state variables on each iteration. There are two hidden layers
with the size of 100 each for training purposes, and one
output layer with the size of 5 which keeps the output layer
corresponding to the related Q values for each attack class.
Note that after each training iteration based on the states and
batch size the q values predicted in the output later will be
fed into Q vectors (denoted as QV) as illustrated in Figure 2.

VOLUME 4, 2016

7

Alavizadeh et al.:

FIGURE 6: Comparing the loss and reward values of DQN learning process based on different discount factor values: (a)
γ = 0.001, (b) γ = 0.01, (c) γ = 0.1, and (d) γ = 0.9

Then, the target Q (denoted as QT) is computed based on the
rewards, discount factor for future rewards, and predicted Q
vectors as Equation 4.

QTi = RVi + γ.Q(cid:48)Vi

(4)

The results of QTi are further fed into the DQN for the
training process and computing the loss function as repre-
sented in the learning improvement phase in Figure 4. The
training of the neural network is performed with a Mean
Square Error (MSE) loss between the Q-value estimated by
the neural network for the current state and a target Q value
obtained by summing the current reward and the next state’s
Q-value multiplied by the value of discount factor (λ). The
computation of the loss function for the evaluation of the
DQN performance is critical. We compute the loss value
after each iteration episode for the DQN network based on
the current states and target network. The total loss can be
denoted as Equation 5.

Loss =

1
n

(cid:18)

(cid:88)

n

Q(s, a)
(cid:124) (cid:123)(cid:122) (cid:125)
Prediction

− r + γQ(s(cid:48), a(cid:48))
(cid:125)
(cid:123)(cid:122)
Target

(cid:124)

(cid:19)2

(5)

Once the training of the model is completed, the trained
NN is used for prediction. For each state, the Q-function pro-
vides the associated Q-value for each of the possible actions
for that speciﬁc state. The predicted action is determined
based on the maximum Q-value. The model is trained for
a number of iterations and episodes which are enough for
covering the complete dataset.

VI. EVALUATION OF DQL MODEL
A. EXPERIMENT SETUP AND PARAMETERS
We implemented our proposed model in Python using Ten-
sorﬂow framework version 1.13.2. In our study, we ana-
lyzed the performance of our DQL model using NSL-KDD
datasets. The training portion of the NSL-KDD dataset in-
cludes various samples for the network features and corre-
sponding labels for intrusion with different possible values

such as binary or multiclass anomaly. In this paper, we
considered the network features as states and the label values
as the actions to adapt these elements to DQN concepts.

The parameters and values associated with the DQL model
are shown in Table 3. For the fully connected architecture,
we used a total of two hidden layers with ‘relu’ activation
function apart from input and output layers. During the
training, various major parameters should be determined and
examined in order to ﬁnd the best values that are appropriate
and ideal for the model. At the beginning step for training,
the exploration rate (cid:15), is set to 0.9 for the agent to perform
exploration based on some degree of randomness with the
decoy rate of 0.99. The initial values for other values such
as batch-size, discount factor are illustrated in Table 3. How-
ever, we also evaluate the performance of the DQL model
based on different values. We examined the behavior of the
proposed DQL agent by varying the discount factor values.
This essentially determines how the DQL agent can improve
the performance of learning based on future awards.

Figure 6 demonstrates the loss and reward values obtained
during the DQL training process based on different values of
the discount factor. Figure ?? and Figure ?? show the reward
and loss values by setting the λ as 0.001 and 0.01, respec-
tively. As it shows, the loss value is lower in λ = 0.001.
However, we increased the learning rate signiﬁcantly and
evaluated the loss and reward values in Figures Figure ?? and
Figure ??. The results indicate that higher value for discount
factor leads to higher loss value. As it also shows, the loss
value in the worst-case reaches 1.7 based on λ = 0.001,
while it reaches 4 in the higher discount factor value λ = 0.9.
However, as demonstrated in Figure 6, we can observe that
reward values have a sharp increasing trend for all discount
factor values.

Based on the results obtained during the DQN agent learn-
ing process, we discovered that the lower discount factor
yields a lower loss value that leads to better results in terms
of learning the model especially when the episode numbers
are smaller.

8

VOLUME 4, 2016

Alavizadeh et al.:

FIGURE 7: Confusion Matrix based on the classiﬁcation categories for our DQL model for two different Discount Factors: (a)
γ = 0.001, (b) γ = 0.9.

B. PERFORMANCE METRICS
We use different measurements to evaluate the performance
of our proposed DQL model used for network intrusion
detection such as Accuracy, Precision, Recall, and F1 score.
However, the performance of the model cannot rely only
on the accuracy values since it evaluates the percentages
of the samples that are correctly classiﬁed. It ignores the
samples incorrectly classiﬁed. To perform better evaluation,
we analyzed the results based on the other performance
metrics as follows.

a: Accuracy
Accuracy is one of the most common metrics to evaluate
and judge a model. it measures the total number of cor-
rect predictions made out of all the predictions made by
the model. It can be obtained based on True Positive (TP)
value, True Negative (TN) rate, False Positive (FP) rate, and
False Negative (FN) value. Equation (6) shows the Accuracy
metric.

Accuracy =

T P + T N
T P + F P + T N + F N

(6)

b: Precision
Precision evaluates can be obtained based on the percentage
of positive instances against the total predicted positive in-
stances. In this case, the denominator is the sum of TP and
FP denoting the model prediction performed as positive from
the whole dataset. Indeed, it indicates that ‘how much the
model is right when it says it is right’, see Equation (7).

P recision =

T P
T P + F P

(7)

c: Recall
Recall (Sensitivity) shows the percentage of positive in-
stances against the total actual positive instances. The de-

nominator is the sum of TP and FN values which is the
actual number of positive instances presented in the dataset.
It indicates that ‘how many right ones the model missed when
it showed the right ones’. See Equation (8).

Recall =

T P
T P + F N

(8)

d: F1 score
The harmonic mean of precision and recall values is consid-
ered as the F1 score. It considers the contribution of both
values. Thus, the higher the F1 score indicates the better
results. Based on the numerator of Equation (9), if either
precision or recall value goes low, the ﬁnal value of the F1
score also decreases signiﬁcantly. We can conclude a model
as a good one based on the higher value of the F1 score.
Equation (9) shows how the F1 score is computed based on
both precision and recall values.

F 1 score =

2 × P recision × Recall
P recision + Recall

(9)

C. PERFORMANCE EVALUATION
We evaluated the performance of the DQN on the testing
phase based on the parameters set in Table 3 and training
based on 200 episodes.

The confusion matrix for the DQL model based on two
different discount factors of 0.001 and 0.9 are shown in
Figure 7. The confusion matrix represented the evaluation
of our model for the test data set. The rows in the confu-
sion matrix are associated with the predicted class and the
columns indicate the true class. The confusion matrix cells
on the main diagonal demonstrate the correctly classiﬁed
percentages such as those having been classiﬁed as TP or
TN. However, the incorrectly classiﬁed portion is located
in the off-diagonal cells such as FN and FP values. The
values located on the last columns (most right columns)

VOLUME 4, 2016

9

Alavizadeh et al.:

TABLE 4: Performance evaluation of DQL based on various
discount factor values.

Metric

Discount Factors

γ = 0.001

γ = 0.1

γ = 0.9

Precision
Recall
F1 score
Accuracy

0.7784
0.7676
0.8141
0.7807

0.6812
0.7466
0.7063
0.7473

0.6731
0.758
0.6911
0.7578

TABLE 5: Evaluation metrics for DQL Model based on each
classes

Metric

Attack Categories

Normal

DoS

Probe

R2L

Accuracy
F1 score
Precision
Recall

0.8094
0.8084
0.8552
0.8093

0.9247
0.9237
0.9249
0.83

0.9463
0.9449
0.9441
0.9247

0.8848
0.8370
0.8974
0.8848

We evaluated the performance or DQL model based on
both accuracy and time against the different numbers of
training episodes in Figure 9. We can observe that the ac-
curacy value has an ascending trend from 100 episodes to
250 episodes. However, this value decreased to 300 episodes,
while training based on 300 episodes lasts more than 20
minutes. It shows that the best number of episodes for the
DQL agent for training is 250 episodes which take a smaller
execution time of around 17 minutes based on our implemen-
tation.

Table 4 compares the overall performance of the DQL
based on two different discount factors in the DQL train-
ing process. The results show that all performance metrics
have higher values for the smaller value of discount factor
λ = 0.001. Table 5 shows the performance metrics for each
class separately while the DQL model is trained based on 200
episodes with the discount factor λ = 0.001.

D. COMPARISON WITH OTHER APPROACHES
In this section, we compare the results obtained from our
proposed DRL models with various common ML-based
models based on NSL-KDD datasets. We compare the results
with Self-organizing Map (SOM), Support Vector Machine
(SVM), Random Forest (RF), Naive Bayes (NB), Convolu-
tional Neural Network (CNN), and some hybrid models such
as BiLSTM and CNN-BiLSTM models presented in different
studies [34]–[36].

We compare the results based on the performance metrics
such as Accuracy, Recall, F1 Score, and Precision. Table 6
compares the accuracy and training time (in minutes) with
other studies in the literature.

As it shows, our model has a higher accuracy comparing
with the other approaches while it has a lower training time.
However, the worst accuracy obtained by the SVM approach
is about 68%. Both BiLSTM and CNN-BiLSTM hybrid ap-
proaches have high accuracy of 79% and %83, respectively.

FIGURE 8: # of samples against estimated attack types

FIGURE 9: Performance of DRL process based on different
episodes

indicate the percentages of incorrectly classiﬁed predictions
corresponding to each class.

Considering the graphs, we observe that the true-positive
rate for normal, DoS, and Probe classes for λ = 0.001 has
decreased from 0.96 to 0.82, and 0.68 to 0.93, 0.89, and 0.57
for λ = 0.9, respectively. This shows that the DQL agent
performs better for the smaller values of discount factor γ =
0.001 comparing to larger discount factor value such as γ =
0.9. However, the results for the minority class of R2L are
very low because of unbalanced distributions of the number
of samples for each class (see Table 2).

Figure 8 shows the performance of the proposed DQL
model for the correct estimations (TP and TN) together with
FP and FN values based on the number of samples. The
results are captured after performing 200 episodes for agent’s
learning with the discount factor of λ = 0.001. Considering
the graph, we observe higher correct estimations for Normal,
DoS, and Probe classes respectively, while these values are
lower for the minority classes due to the unbalanced distribu-
tion of class samples.

10

VOLUME 4, 2016

Alavizadeh et al.:

TABLE 6: Comparing our model’s accuracy and time with other studies in the literature

Approach

Reference

Adaptive-learning

Dataset

Accuracy

Time

SOM

Ibrahim et al. [34]

RF
BiLSTM
CNN-BiLSTM

Naive Bayes
SVM

Jiang et al. [35]

Yang et al. [36]

DQL

Our model

(cid:55)

(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)

(cid:51)

NSL-KDD

NSL-KDD
NSL-KDD
NSL-KDD

NSL-KDD
NSL-KDD

NSL-KDD

75%

74%
79%
83%

76%
68%

78%

NA

NA
115
72

NA
NA

21

TABLE 7: Comparing our model’s performance in classiﬁca-
tion with other studies in the literature

Method

RF

CNN
LSTM
CNN-BiLSTM

Reference

Normal

DoS

Probe

R2L

Jiang et al. [35]

0.7823

0.8695

0.7348

0.0412

Yang et al. [36]

0.9036
0.8484
0.9215

0.9014
0.8792
0.8958

0.6428
0.6374
0.7111

0.1169
0.0994
0.3469

DQL

Our model

0.8084

0.9237

0.9463

0.8848

However, those approaches have a higher training time than
our model.

Table 7 compares our model’s performance in terms of F1-
score for all classiﬁcations with other studies in the literature.
F1-score is known as a balance point between recall and pre-
cision scores and can be considered as the harmonic average
of both recall and precision. Based on the results summarized
in Table 7, the F1-score for the Normal class reaches about
81% in our study while this value is higher for CNN and
CNN-BiLSTM hybrid approaches with the values of around
90%. However, it can be seen that our models perform better
in terms of other attack classes such as DoS, Probe, R2L
comparing with other ML-based and hybrid approaches in
the literature. It can be seen that the RF method has the lowest
value comparing to the other approaches.

VII. CONCLUSION
We present a Deep Q-learning based (DQL) reinforcement
learning model to detect and classify different network intru-
sion attack classes. The proposed DQL model takes a labeled
dataset as input, then provides a deep reinforcement learning
strategy based on deep Q networks.

In our proposed model, a Q-learning based reinforce-
ment learning is combined with a deep feed-forward neural
network to interact with the network environment where
network trafﬁc is captured and analyzed to detect malicious
network payloads in a self-learning fashion by DQL agents
using an automated trial-error strategy without requiring
human knowledge. We present the implementation of our
proposed method in detail including the basic elements of
DQL such as the agent, the environment, together with the
other concepts such as the quality of actions (Q-values),
epsilon-greedy exploration, and rewards.

To enhance the learning capabilities of our proposed

method, we analyzed various (hyper) parameters of the DQL
agent such as discount factor, batch size, and the number
of learning episodes to ﬁnd the best ﬁne-tuning strategies to
self-learn for network intrusion tasks.

Our experimental results demonstrated that the proposed
DQL model can learn effectively from the environment in an
autonomous manner and is capable of classifying different
network intrusion attack types with high accuracy. Through
the extensive experiments on parameter ﬁne-tuning, we con-
ﬁrmed that the best discount factor for our proposed method
should be 0.001 with 250 episodes of learning.

For future work, we plan to deploy our proposed method
on a realistic cloud-based environment to enable the DQL
agent to improve its self-learning capabilities and classify the
threats with high accuracy in a real-time manner. We plan
to apply our proposed model in improving the self-learning
capabilities in detecting Android-based malware [37], [38]
and ransomware [39], [40] to test the generalizability and
practicability of our model. We also plan to deploy our
proposed model in other applications such as detecting unau-
thorized access in the smart city application [41] and outlier
detection of indoor air quality applications [42]–[44].

REFERENCES
[1] M. P. Stoecklin, “Deeplocker: How AI can power a stealthy new breed of

malware,” Security Intelligence, August, vol. 8, 2018.

[2] J. Hou, Q. Li, S. Cui, S. Meng, S. Zhang, Z. Ni, and Y. Tian, “Low-
cohesion differential privacy protection for industrial internet,” The Jour-
nal of Supercomputing, vol. 76, no. 11, pp. 8450–8472, 2020.

[3] M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garﬁnkel,
A. Dafoe, P. Scharre, T. Zeitzoff, B. Filar et al., “The malicious use
of artiﬁcial intelligence: Forecasting, prevention, and mitigation,” arXiv
preprint arXiv:1802.07228, 2018.

[4] D. Bodeau and R. Graubart, “Cyber resiliency design principles: selective
use throughout the lifecycle and in conjunction with related disciplines,”
McClean (VA): The MITRE Corporation, pp. 2017–0103, 2017.

[5] K. Toyoshima, T. Oda, M. Hirota, K. Katayama, and L. Barolli, “A dqn
based mobile actor node control in wsan: Simulation results of different
distributions of events considering three-dimensional environment,” in
International Conference on Emerging Internetworking, Data & Web
Technologies. Springer, 2020, pp. 197–209.

[6] N. Saito, T. Oda, A. Hirata, Y. Hirota, M. Hirota, and K. Katayama, “De-
sign and implementation of a dqn based aav,” in International Conference
on Broadband and Wireless Computing, Communication and Applications.
Springer, 2020, pp. 321–329.

[7] K. Sethi, R. Kumar, D. Mohanty, and P. Bera, “Robust adaptive cloud in-
trusion detection system using advanced deep reinforcement learning,” in
International Conference on Security, Privacy, and Applied Cryptography
Engineering. Springer, 2020, pp. 66–85.

[8] K. Sethi, R. Kumar, N. Prajapati, and P. Bera, “Deep reinforcement
learning based intrusion detection system for cloud infrastructure,” in

VOLUME 4, 2016

11

Alavizadeh et al.:

[31] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov, “Property
inference attacks on fully connected neural networks using permutation
invariant representations,” in Proceedings of the 2018 ACM SIGSAC
Conference on Computer and Communications Security, 2018, pp. 619–
633.

[32] K. A. da Costa, J. P. Papa, C. O. Lisboa, R. Munoz, and V. H. C. de
Albuquerque, “Internet of things: A survey on machine learning-based
intrusion detection approaches,” Computer Networks, vol. 151, pp. 147–
157, 2019. [Online]. Available: https://www.sciencedirect.com/science/
article/pii/S1389128618308739

[33] M. M. Hassan, A. Gumaei, A. Alsanad, M. Alrubaian, and G. Fortino,
“A hybrid deep learning model for efﬁcient intrusion detection in big data
environment,” Information Sciences, vol. 513, pp. 386–396, 2020.
[34] L. M. Ibrahim, D. T. Basheer, and M. S. Mahmod, “A comparison study for
intrusion database (kdd99, nsl-kdd) based on self organization map (som)
artiﬁcial neural network,” Journal of Engineering Science and Technology,
vol. 8, no. 1, pp. 107–119, 2013.

[35] K. Jiang, W. Wang, A. Wang, and H. Wu, “Network intrusion detection
combined hybrid sampling with deep hierarchical network,” IEEE Access,
vol. 8, pp. 32 464–32 476, 2020.

[36] K. Yang, J. Liu, C. Zhang, and Y. Fang, “Adversarial examples against
the deep learning based network intrusion detection systems,” in MIL-
COM 2018-2018 IEEE Military Communications Conference (MILCOM).
IEEE, 2018, pp. 559–564.

[37] J. Zhu, J. Jang-Jaccard, and P. A. Watters, “Multi-loss siamese neural
network with batch normalization layer for malware detection,” IEEE
Access, vol. 8, pp. 171 542–171 550, 2020.

[38] J. Zhu, J. Jang-Jaccard, A. Singh, P. A. Watters, and S. Camtepe, “Task-
aware meta learning-based siamese neural network for classifying obfus-
cated malware,” arXiv preprint arXiv:2110.13409, 2021.

[39] T. R. McIntosh, J. Jang-Jaccard, and P. A. Watters, “Large scale behavioral
analysis of ransomware attacks,” in International Conference on Neural
Information Processing. Springer, 2018, pp. 217–229.

[40] T. McIntosh, J. Jang-Jaccard, P. Watters, and T. Susnjak, “The inadequacy
of entropy-based ransomware detection,” in International Conference on
Neural Information Processing. Springer, 2019, pp. 181–189.

[41] F. Sabrina and J. Jang-Jaccard, “Entitlement-based access control for smart

cities using blockchain,” Sensors, vol. 21, no. 16, p. 5264, 2021.

[42] Y. Wei, J. Jang-Jaccard, F. Sabrina, and H. Alavizadeh, “Large-scale outlier
detection for low-cost pm18 sensors,” IEEE Access, vol. 8, pp. 229 033–
229 042, 2020.

[43] Y. Wei, J. Jang-Jaccard, F. Sabrina, and T. McIntosh, “Msd-kmeans: A
novel algorithm for efﬁcient detection of global and local outliers,” arXiv
preprint arXiv:1910.06588, 2019.

[44] R. Weyers, J. Jang-Jaccard, A. Moses, Y. Wang, M. Boulic, C. Chitty,
R. Phipps, and C. Cunningham, “Low-cost indoor air quality (iaq) platform
for healthier classrooms in new zealand: Engineering issues,” in 2017
4th Asia-Paciﬁc World Congress on Computer Science and Engineering
(APWC on CSE).

IEEE, 2017, pp. 208–215.

2020 International Conference on COMmunication Systems & NETworkS
(COMSNETS).

IEEE, 2020, pp. 1–6.

[9] K. Sethi, E. S. Rupesh, R. Kumar, P. Bera, and Y. V. Madhav, “A context-
aware robust intrusion detection system: a reinforcement learning-based
approach,” International Journal of Information Security, vol. 19, no. 6,
pp. 657–678, 2020.

[10] Q.-V. Dang and T.-H. Vo, “Reinforcement learning for the problem of
detecting intrusion in a computer system,” in Proceedings of Sixth In-
ternational Congress on Information and Communication Technology.
Springer, 2022, pp. 755–762.

[11] Q. Cappart, T. Moisan, L.-M. Rousseau, I. Prémont-Schwarz, and A. Cire,
“Combining reinforcement learning and constraint programming for com-
binatorial optimization,” arXiv preprint arXiv:2006.01610, 2020.

[12] X. Ma and W. Shi, “Aesmote: Adversarial reinforcement learning with
smote for anomaly detection,” IEEE Transactions on Network Science and
Engineering, 2020.

[13] M. Lopez-Martin, B. Carro, and A. Sanchez-Esguevillas, “Application of
deep reinforcement learning to intrusion detection for supervised prob-
lems,” Expert Systems with Applications, vol. 141, p. 112963, 2020.
[14] Z. S. Stefanova and K. M. Ramachandran, “Off-policy q-learning tech-
nique for intrusion response in network security,” World Academy of
Science, Engineering and Technology, International Science Index, vol.
136, pp. 262–268, 2018.

[15] V. François-Lavet, P. Henderson, R. Islam, M. G. Bellemare, and
J. Pineau, “An introduction to deep reinforcement learning,” arXiv preprint
arXiv:1811.12560, 2018.

[16] B. Hu and J. Li, “Shifting deep reinforcement learning algorithm towards
training directly in transient real-world environment: A case study in
powertrain control,” IEEE Transactions on Industrial Informatics, 2021.

[17] K. Sethi, Y. V. Madhav, R. Kumar, and P. Bera, “Attention based multi-
agent intrusion detection systems using reinforcement learning,” Journal
of Information Security and Applications, vol. 61, p. 102923, 2021.
[18] T. T. Nguyen and V. J. Reddi, “Deep reinforcement learning for cyber

security,” arXiv preprint arXiv:1906.05799, 2019.

[19] G. Caminero, M. Lopez-Martin, and B. Carro, “Adversarial environment
reinforcement learning algorithm for intrusion detection,” Computer Net-
works, vol. 159, pp. 96–109, 2019.

[20] M. Tavallaee, E. Bagheri, W. Lu, and A. A. Ghorbani, “A detailed analysis
of the kdd cup 99 data set,” in 2009 IEEE Symposium on Computational
Intelligence for Security and Defense Applications, 2009, pp. 1–6.
[21] C. Kolias, G. Kambourakis, A. Stavrou, and S. Gritzalis, “Intrusion de-
tection in 802.11 networks: Empirical evaluation of threats and a public
dataset,” IEEE Communications Surveys Tutorials, vol. 18, no. 1, pp. 184–
208, 2016.

[22] S. Iannucci, O. D. Barba, V. Cardellini, and I. Banicescu, “A performance
evaluation of deep reinforcement learning for model-based intrusion re-
sponse,” in 2019 IEEE 4th International Workshops on Foundations and
Applications of Self* Systems (FAS* W).

IEEE, 2019, pp. 158–163.

[23] S. Iannucci, V. Cardellini, O. D. Barba, and I. Banicescu, “A hybrid model-
free approach for the near-optimal intrusion response control of non-
stationary systems,” Future Generation Computer Systems, 2020.

[24] K. Malialis and D. Kudenko, “Distributed response to network intrusions
using multiagent reinforcement learning,” Engineering Applications of
Artiﬁcial Intelligence, vol. 41, pp. 270–284, 2015.

[25] P. Holgado, V. A. Villagrá, and L. Vazquez, “Real-time multistep attack
prediction based on hidden markov models,” IEEE Transactions on De-
pendable and Secure Computing, 2017.

[26] Z. Zhang, F. Naït-Abdesselam, P.-H. Ho, and Y. Kadobayashi, “Toward
cost-sensitive self-optimizing anomaly detection and response in auto-
nomic networks,” computers & security, vol. 30, no. 6-7, pp. 525–537,
2011.

[27] B. Fessi, S. Benabdallah, N. Boudriga, and M. Hamdi, “A multi-
attribute decision model for intrusion response system,” Information
Sciences, vol. 270, pp. 237–254, Jun. 2014.
[Online]. Available:
https://linkinghub.elsevier.com/retrieve/pii/S0020025514002527

[28] W. Xu, J. Jang-Jaccard, A. Singh, Y. Wei, and F. Sabrina, “Improving
performance of autoencoder-based network anomaly detection on nsl-kdd
dataset,” IEEE Access, vol. 9, pp. 140 136–140 146, 2021.

[29] J. Zhu, J. Jang-Jaccard, T. Liu, and J. Zhou, “Joint spectral clustering
based on optimal graph and feature selection,” Neural Processing Letters,
vol. 53, no. 1, pp. 257–273, 2021.

[30] W.-C. Shi and H.-M. Sun, “Deepbot: a time-based botnet detection with
deep learning,” Soft Computing, vol. 24, pp. 16 605–16 616, 2020.

12

VOLUME 4, 2016

