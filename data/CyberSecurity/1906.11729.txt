Using Intuition from Empirical Properties to
Simplify Adversarial Training Defense

Guanxiong Liu
ECE Department
New Jersey Institute of Technology
Newark, USA
gl236@njit.edu

Issa Khalil
QCRI
Hamad bin Khalifa University
Doha, Qatar
ikhalil@hbku.edu.qa

Abdallah Khreishah
ECE Department
New Jersey Institute of Technology
Newark, USA
abdallah@njit.edu

9
1
0
2

n
u
J

7
2

]

G
L
.
s
c
[

1
v
9
2
7
1
1
.
6
0
9
1
:
v
i
X
r
a

Abstract— Due to the surprisingly good representation power
of complex distributions, neural network (NN) classiﬁers are
widely used in many tasks which include natural
language
processing, computer vision and cyber security. In recent works,
people noticed the existence of adversarial examples. These
adversarial examples break the NN classiﬁers’ underlying as-
sumption that the environment is attack free and can easily
mislead fully trained NN classiﬁer without noticeable changes.
Among defensive methods, adversarial training is a popular
choice. However, original adversarial training with single-step ad-
versarial examples (Single-Adv) can not defend against iterative
adversarial examples. Although adversarial training with itera-
tive adversarial examples (Iter-Adv) can defend against iterative
adversarial examples, it consumes too much computational power
and hence is not scalable. In this paper, we analyze Iter-Adv tech-
niques and identify two of their empirical properties. Based on
these properties, we propose modiﬁcations which enhance Single-
Adv to perform competitively as Iter-Adv. Through preliminary
evaluation, we show that the proposed method enhances the test
accuracy of state-of-the-art (SOTA) Single-Adv defensive method
against iterative adversarial examples by up to 16.93% while
reducing its training cost by 28.75%.

Index Terms—adversarial training, adversarial example, neu-

ral network classiﬁer

I. INTRODUCTION
Adversarial examples were discovered by Szegedy et. al.
and presented in [14]. In the image classiﬁcation tasks, they
show that a specially designed perturbation which can be
ignored by human eyes can effectively mislead the fully
trained NN classiﬁer. Moreover, such perturbation is not a
special case but can be found for almost every example.
Yet, more scary,
the research shows that adversary could
arbitrarily control the prediction from NN classiﬁer through
carefully designed perturbations and can achieve high success
rate against classiﬁers without defenses [2] [8] [9].

Thereafter, great effort has been devoted to designing an
effective method to defend against adversarial examples. Some
of these methods utilize augmentation and regularization to
enhance test accuracy on adversarial examples [11]. Other
methods rely on building a protective shells around the
classiﬁer to either identify adversarial examples or mitigate
the adversarial perturbations [10] [12]. Among all existing
defensive approaches, adversarial training is shown to be the
most successful because unlike many other defensive ap-
proaches it does not rely on the false sense of security brought

by obfuscated gradient [1]. The fundamental idea is using
adversarial examples during the training. The stronger the
adversarial examples used the stronger the obtained defense.
Therefore, the research community originally performs adver-
sarial training with single-step adversarial examples (Single-
Adv) and now with iterative adversarial examples (Iter-Adv)
[5] [7] [9].

One of the biggest problems of Iter-Adv is the huge compu-
tational cost in preparing iterative adversarial examples during
the training [1]. For example, using Iter-Adv on ImageNet
dataset requires a cluster of GPU servers [6]. Nowadays, there
is a trend to make the NN classiﬁer more portable such that
it can be trained and utilized solely in a smart-phone [3].
Moreover, due to data privacy consideration, some applications
calculate the last few layers of NN on local device. Under these
scenarios, we need a lightweight adversarial training defense
since Iter-Adv is hard to be scaled with limited computational
resources [1]. In the following sections, we start with two
questions about Iter-Adv. Then, based on our empirical results,
we propose two modiﬁcations to Iter-Adv and share our idea
to simplify it. Our contributions are summarized as follows:
• We raise two questions about Iter-Adv and conduct
experiments that identify two empirical properties of it.
• Based on identiﬁed properties, we propose a Single-Adv
method that performs competitively as Iter-Adv methods.
• Through comparison with SOTA Single-Adv method, our
proposed method enhances test accuracy by 16.93% and
reduces training time by 28.75%.

The rest of the work is organized as follows. Section II
and III present our questions and the preliminary experiments.
Section IV proposes modiﬁcations that simplify Iter-Adv.
Section V presents our preliminary evaluation results. Section
VI and VII conclude the paper and present the future work.

II. IS IT WORTH TO USE TINY PER STEP PERTURBATIONS?
Let’s take a step back and recall the deﬁnition of gradient

based l∞ iterative adversarial examples.

δi = sign(∇xi−1 L(C(xi−1, θ), y)) × (cid:15)i
xi = clip(xi−1 + δi)

where x0 is the original example, xi
is the ith iteration
adversarial example, y is the ground truth, C is the classiﬁer,

 
 
 
 
 
 
(a)

(b)

(a)

(b)

(a)

(b)

Fig. 1: Test Accuracy on
BIM Examples with Differ-
ent Numbers of Iteration

Fig. 2: Test Accuracy on
Intermediate BIM Examples
after Each Iteration

Fig.
of
3: Flow Chart
Iter-Adv and the Proposed
Method

L is the loss function, (cid:15)i
is the perturbation limit in the
ith iteration, and δi is the calculated perturbation in the ith
iteration.

To generate iterative adversarial examples, adversaries apply
small per step perturbation several
times and update the
gradient direction based on their observation of the targeted
NN after each step. Generally speaking,
the smaller per
step perturbation they apply the better observation of NN’s
decision hyperplane they may get. With a better observation of
NN’s decision hyperplane, adversaries could greedily optimize
their objective function and generate more serious adversarial
examples.

However, there are still several questions to be answered.
Given the relation between iterative adversarial examples and
the per step perturbation, how much can we beneﬁt from a
smaller per step perturbation? Is there a limit on the per step
perturbation beyond which the improvement starts to vanish?

To answer these questions, we conduct experiments on
MNIST and Fashion-MNIST datasets, respectively. On each
dataset, we ﬁrstly train four different NN classiﬁers with
the same structure and hyper-parameter setting [13]. These
include: (1) a Vanilla classiﬁer trained on original examples
only, (2) a FGSM-Adv classiﬁer trained on a mixture original
and FGSM examples [5], (3) two BIM(·)-Adv classiﬁers
trained on a mixture of original and BIM (different numbers of
iteration) examples [7]. The number of iterations for the two
BIM-Adv classiﬁers is 10 and 30, respectively. We evaluate
these classiﬁers in terms of test accuracy when facing BIM
examples with different numbers of iteration, N . The total

perturbation, (cid:15), is ﬁxed to 0.3 (MNIST) and 0.2 (Fashion-
MNIST) with l∞ norm. The per step perturbation is set to
(cid:15)s = (cid:15)
N . The evaluation results are presented in Figure 1.
From Figure 1, it is clear that test accuracy results of all
four classiﬁers converge quite fast. The Vanilla and FGSM-
Adv classiﬁers can not defend BIM examples and their test
accuracy results drop below 10% (random guessing) when the
iteration number is larger than 4 on both MNIST and Fashion-
MNIST datasets. The BIM-Adv classiﬁers are shown to be
defensive against BIM examples and their test accuracy results
also converge after we set iteration number to 5 (MNIST) and
10 (Fashion-MNIST). Given the fact that adversarial training
uses adversarial examples to ﬁnd blind spots of the under-
training classiﬁer and train it, these experiments show that (1)
there is a limit for decreasing the per step perturbation
of iterative adversarial examples and (2) iterative adver-
sarial examples with per step perturbation lower than
the limit only marginally beneﬁt the adversarial training.
This conclusion can also be veriﬁed through comparing test
accuracy results of BIM-Adv classiﬁers. Although BIM(30)-
Adv is trained on BIM examples with much smaller per
step perturbation compared with BIM(10)-Adv, they both have
almost the same test accuracy on MNIST dataset. On Fashion-
MNIST, BIM(30)-Adv is shown to be more defensive than
BIM(10)-Adv. However, the difference on test accuracy does
not increase with the iteration number. The reason is that
BIM(10)-Adv has a stable test accuracy even when facing BIM
examples with smaller per step perturbation (N > 10).

Based on the results and the conclusion in this subsection,

we proposed the following. When facing the trade-off between
defensive performance and computational efﬁciency in training
with iterative adversarial examples, the per step perturbation is
not necessary to be very small since it only marginally beneﬁts
the adversarial training.

III. ANY BENEFIT OF USING INTERMEDIATE RESULTS?

Adversarial training originally uses single-step adversarial
examples and recently shifts to iterative adversarial examples.
Since iterative adversarial examples are more serious and able
to reveal more blind spots of under-training classiﬁer, such
shift improves the adversarial training in building classiﬁer’s
defense against iterative adversarial examples.

However, there are still several questions to be answered
during this shift. When we prepare iterative adversarial exam-
ples for training, can we also beneﬁt from using intermediate
results from this generation process as well?

To explore the answer for this question, we conduct another
set of experiments on MNIST and Fashion-MNIST datasets,
respectively. On each dataset, we continuously use the same
NN classiﬁers (one Vanilla classiﬁer, one FGSM-Adv classiﬁer
and two BIM-Adv classiﬁers) as before. During the evaluation,
we test all four classiﬁers in terms of test accuracy against
BIM examples. The total perturbation of test examples is set
to (cid:15) = 0.3 (MNIST) and (cid:15) = 0.2 (Fashion-MNIST). Different
from previous experiments, we generate BIM examples with
ﬁxed iteration number, N = 10, and evaluate the test accuracy
after each iteration. Therefore, the per step perturbation is ﬁxed
to (cid:15)s = (cid:15)
10 while perturbation is increasing after each iteration.
The evaluation results are presented in Figure 2.

The experiment results from Figure 2 show that the test
accuracy of all four classiﬁers is monotonically decreasing
with the number of iterations. The classiﬁers without defense,
Vanilla and FGSM-Adv, are defeated (perform worse than
random guessing) by the intermediate results before the it-
erative adversarial examples are ready (around 8 iterations
on both MNIST and Fashion-MNIST datasets). Although two
BIM-Adv classiﬁers obtain defense from training and can
correctly classify most of adversarial examples, the majority
of test accuracy degeneration still happens within ﬁrst couple
of iterations (about 6 iterations on both MNIST and Fashion-
MNIST datasets). Given the fact that adversarial training uses
adversarial examples to ﬁnd blind spots of the under-training
classiﬁer and train it, these experiment results show that (1) the
majority of blind spots can be revealed by intermediate
results during generating iterative adversarial examples
and (2) using intermediate results can beneﬁt adversarial
training before iterative adversarial examples are ready.

Based on the results and the conclusion they leads to,
we can summarize our second suggestion as follows. To
enhance the training efﬁciency in Iter-Adv, we could utilize the
intermediate results in training before the iterative adversarial
examples are ready since the majority of weaknesses can be
revealed by them at that time. It is worth to clarify that we do
not suggest to fully replace iterative adversarial examples with

intermediate results but the utilization of intermediate results
can reduce the total computation in Iter-Adv.

IV. SIMPLIFYING ADVERSARIAL TRAINING DEFENSE

In previous sections, we raise two questions regarding the
training with iterative adversarial examples and provide intu-
itions to answer them based on the results from the MNIST and
the Fashion-MNIST datasets. In this section, we propose our
modiﬁcations to simplify Iter-Adv which takes both defensive
performance and computational efﬁciency into consideration.
Before moving further, we want to ﬁrstly review the working
process of Iter-Adv. As we can see from Figure 3a, a ﬂow
chart of adversarial training is provided. At the beginning, the
clean examples are fed into the classiﬁer and the generator
of adversarial examples. The generator then interacts with the
classiﬁer for several iterations to prepare iterative adversarial
examples. When adversarial examples are ready, the classiﬁer
generates prediction logits for both clean and adversarial
examples through forward propagation [4]. Finally, a prede-
ﬁned loss function optimizer takes these prediction logits to
calculate the loss value and update NN parameters through
gradient descent and backward propagation [4]. During the
training, these steps are repeated for several epochs (separated
by the green dash line) and NN parameters in classiﬁer are
carried through epochs (the red dash line with arrow).

Inspired by the summarized empirical properties, we now
propose two modiﬁcations to simplify Iter-Adv. The ﬂow
chart of our proposed method can be found in Figure 3b.
Compared with Iter-Adv, our ﬁrst modiﬁcation is utilizing the
intermediate results during the training. In each training epoch,
our proposed method will not wait until the ﬁnal iterative
adversarial examples are ready. Instead, it utilizes the interme-
diate results and carry them to the next training epoch. As a
result, it only requires a single-step perturbation in each epoch.
The second modiﬁcation is selecting a relatively large per step
perturbation. With this modiﬁcation, the adversarial examples
can quickly reach large perturbation and reveal the majority of
blind spots. Therefore, it mitigates the disadvantage brought by
using single-step adversarial perturbation, training with weak
adversarial examples in the ﬁrst few training epochs. To catch
up the long term changes in classiﬁer’s parameters, this epoch-
wise iteration process will be reset after a certain number of
training epochs.

From Figure 3a and Figure 3b, it is clear that our proposed
method signiﬁcantly reduces computational cost required by
preparing iterative adversarial examples in each training epoch.
By reusing the adversarial examples in the next training epoch
and selecting large per step perturbation, we can also expect
the proposed method to perform better than FGSM-Adv and
closer to BIM-Adv.

V. PRELIMINARY RESULTS

In this section, we evaluate the proposed method. Since our
proposed method belongs to Single-Adv methods, we compare
it with several Single-Adv and Iter-Adv methods. Iter-Adv

FGSM-Adv
ATDA
Proposed
BIM(10)-Adv
BIM(30)-Adv

Original
99.10% 98.65%
97.80% 97.90%
99.08% 96.71%
98.97% 96.74%
99.06% 95.71%

MNIST
FGSM BIM(10)
2.16%
86.6%
94.21%
93.29%
93.39%

BIM(30)
2.13%
84.2%
94.04%
93.24%
93.51%

Original
92.09% 90.05%
80.3%
85.60%
89.19% 79.85%
88.59% 74.93%
86.52% 76.98%

Fashion-MNIST
FGSM BIM(10)
6.68%
56.8%
70.74%
67.45%
71.12%

BIM(30)
6.18%
49.1%
66.03%
65.96%
70.75%

Training Time per
Epoch (seconds)

17.12
26.21
18.68
56.47
142.37

TABLE I: Evaluation Results

methods include FGSM-Adv, BIM(10)-Adv and BIM(30)-
Adv. We also include a SOTA Single-Adv method which
is called ATDA [13]. This method trains the classiﬁer with
single-step adversarial examples and modiﬁes training loss to
achieve domain adaptation. Our proposed method trains the
classiﬁer with single-step adversarial examples and the per-
(cid:15)
10 . Moreover, the epoch-wise iteration
turbation is limited at
presented in Figure 3b is repeated every 20 training epochs.
The results from Single-Adv methods are represented by the
blue color bars while the results from Iter-Adv methods are
represented by the green color bars.

The evaluation is conducted on MNIST and Fashion-
MNIST datasets and measures both defensive power and
computational consumption in training. To measure defensive
power, we evaluate different methods against two white-box
l∞ iterative adversarial examples which are utilized in Section
II, BIM(10) and BIM(30). Since all methods are run on the
same workstation with a Tesla K20m GPU and converge after
the same number of epochs, we utilize the training time per
epoch as a measure of computational consumption.

From the results presented in Table I, it is clear that all
methods can correctly classify original and FGSM adversarial
examples while only ATDA, BIM(10)-Adv, BIM(30)-Adv and
our proposed method show resistance against iterative adver-
sarial examples. Our proposed method constantly outperforms
ATDA in term of test accuracy. Under MNIST dataset, the
enhancements are 7.61% on BIM(10) and 9.83% on BIM(30).
Compared with BIM(10)-Adv and BIM(30)-Adv, our proposed
method achieves the same level and even slightly higher test
accuracy. When the experiments are extended to Fashion-
MNIST dataset, the advantage of our proposed method over
ATDA is even more signiﬁcant. On BIM(10) and BIM(30)
adversarial examples, respectively, our proposed method en-
hances the test accuracy by 13.94% and 16.93% compared to
ATDA. More importantly, the performance of our proposed
method is still competitive compared with BIM(10)-Adv and
BIM(30)-Adv in terms of test accuracy.

Beside the test accuracy, we also evaluate all defensive
methods on average training time per epoch. From the results
presented in Table I,
is clear that Single-Adv methods
signiﬁcantly reduce the training time consumed by Iter-Adv
by up to 85%. More importantly, our proposed method can
further reduce 28.75% of that required by the SOTA Single-
Adv method, ATDA.

it

VI. CONCLUSION

In this work, we raise two questions about Iter-Adv and
identify two empirical properties. (1) Iterative adversarial

the adversarial

examples with per step perturbation lower than a certain
limit only marginally beneﬁt
training. (2)
The majority of blind spots can be revealed by intermediate
results during generating iterative adversarial examples. Based
on these two properties, we propose a Single-Adv method
which performs competitively as Iter-Adv methods. Through
preliminary results,
the proposed method outperforms the
SOTA Single-Adv method (by up-to 16.93% in test accuracy)
and achieves the same level of defense as Iter-Adv methods.
More importantly, the proposed method signiﬁcantly reduces
the training time required by Iter-Adv methods and even save
28.75% of that required by the SOTA Single-Adv method.

VII. FUTURE WORK

In the future, we are going to extend the experiments
on larger datasets and add more experiments to get deeper
understanding of Single-Adv and Iter-Adv.

REFERENCES

[1] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradients give a
false sense of security: Circumventing defenses to adversarial examples,”
arXiv preprint arXiv:1802.00420, 2018.

[2] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural

networks,” pp. 39–57, 2017.

[3] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,
C. Zhang, and Z. Zhang, “Mxnet: A ﬂexible and efﬁcient machine
learning library for heterogeneous distributed systems,” arXiv preprint
arXiv:1512.01274, 2015.

[4] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning.

MIT press Cambridge, 2016, vol. 1.

[5] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” International Conference on Learning Represen-
tations, 2015.

[6] H. Kannan, A. Kurakin, and I. Goodfellow, “Adversarial logit pairing,”

arXiv preprint arXiv:1803.06373, 2018.

[7] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the

physical world,” arXiv preprint arXiv:1607.02533, 2016.

[8] ——, “Adversarial machine learning at scale,” International Conference

on Learning Representations, 2017.

[9] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” arXiv preprint
arXiv:1706.06083, 2017.

[10] D. Meng and H. Chen, “Magnet: a two-pronged defense against adver-

sarial examples,” pp. 135–147, 2017.

[11] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in Security and Privacy (SP), 2016 IEEE Symposium on.
IEEE, 2016,
pp. 582–597.

[12] P. Samangouei, M. Kabkab, and R. Chellappa, “Defense-gan: Protecting
classiﬁers against adversarial attacks using generative models,” arXiv
preprint arXiv:1805.06605, 2018.

[13] C. Song, K. He, L. Wang, and J. E. Hopcroft, “Improving the general-
ization of adversarial training with domain adaptation,” arXiv preprint
arXiv:1810.00740, 2018.

[14] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus, “Intriguing properties of neural networks,” International
Conference on Learning Representations, 2014.

