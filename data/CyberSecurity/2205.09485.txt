2
2
0
2

g
u
A
0
2

]

G
L
.
s
c
[

3
v
5
8
4
9
0
.
5
0
2
2
:
v
i
X
r
a

A Boosting Algorithm for Positive-Unlabeled Learning

Yawen Zhao1, Mingzhe Zhang1, Chenhao Zhang1, Weitong Chen2, Nan Ye1,
and Miao Xu1

1The University of Queensland,

{yawen.zhao,mingzhe.zhang,chenhao.zhang,nan.ye,miao.xu}

@uq.edu.au
2The University of Adelaide,
t.chen@adelaide.edu.au

Abstract

Positive-unlabeled (PU) learning deals with binary classiﬁcation problems when only
positive (P) and unlabeled (U) data are available. A lot of PU methods based on linear
models and neural networks have been proposed; however, there is still a lack of study
on boosting algorithms for PU learning, while a traditional boosting algorithm with
simple base learners may perform better than neural networks. We propose a novel
boosting algorithm for PU learning: Ada-PU, which compares against neural networks.
Ada-PU follows the general procedure of AdaBoost, while P data are regarded as positive
and negative simultaneously. Three distributions of PU data are maintained and updated
in Ada-PU instead of one in the ordinary supervised (PN) learning. After a weak classiﬁer
is learned on the newly updated distribution, the corresponding weight of the classiﬁer for
the ﬁnal ensemble is estimated using only PU data. We demonstrated that the proposed
method is guaranteed to keep three theoretical properties of boosting algorithms with a
deﬁned set of base classiﬁers. In experiments, we showed that Ada-PU outperforms neural
networks on benchmark PU datasets. We also study a real-world dataset UNSW-NB15
in cyber security and demonstrated that Ada-PU has superior performance for malicious
activity detection.

1

Introduction

For ordinary binary classiﬁcation, both positive and negative labels need to be collected.
However, collecting all labels may be unrealistic in some real-world applications. For example,
in disease diagnosis, we can easily collect data of conﬁrmed patients provided by the doctors,
but not for patients with mild or asymptomatic symptoms that have not been diagnosed yet.
Such kind of problem in which only some reliable positive (P) data and unlabeled (U) data
can be collected occurs in almost every aspect of our daily life due to the high cost or even
impossibility to accurately annotate all data.

To meet the realistic needs, positive-unlabeled (PU) learning [6, 5, 7] has been widely
studied. In PU methods, a binary classiﬁer is trained with a few labeled positive (P) data

1

 
 
 
 
 
 
and suﬃcient unlabeled (U) data, which is a mixture of unlabeled positive and negative (N)
data. Since the model has to learn not only from the labeled data, but also from the unlabeled
data, PU learning is challenging. Early works tried to identify N data from U, then performed
ordinary supervised (PN) learning [23, 22]. More recent works focus on re-weighting P data
and U data, such as unbiased PU (uPU) [9] and non-negative PU (nnPU) [20]. Considering
the data generation process, there are two diﬀerent settings of PU learning [25]: censored PU
learning [10]where P and U data are drawn from the same set and case-control PU learning [8],
where P and U data are drawn independently. In this paper, we consider case-control PU
learning.

Most SOTA PU methods are based on neural networks [20, 3, 30]. Although these neural
network (NN)-based PU methods have been proved successful on benchmark data, NN may not
be the most suitable model for all classiﬁcation tasks. Other sophisticated machine learning
models are also powerful in practice. According to the 2021 Kaggle Data Science & Machine
Learning Survey1, which contains 25, 973 responses, boosting algorithms are more popular
than neural networks due to their simplicity and superior performance.

Considering that SOTA boosting methods achieve higher performance than neural networks
on tabular data and tabular data is one of the most classical and widely used data formats, we
are curious about the potential of boosting methods on only P and U data. Besides, boosting
algorithms are generally less computation-intensive and environmentally friendly than neural
networks. Motivated by these factors, we propose a boosting method for PU learning.

Among boosting methods, AdaBoost (Adaptive Boost) [11, 12] is one of the most classic
ones with a solid theoretical basis and good performance [28]. The principle of AdaBoost is
to adjust the data distribution in each iteration and give more weights to examples wrongly
predicted in previous iterations. The rigorous derivation process and theoretical foundation of
AdaBoost inspire us to follow the same principle of the AdaBoost framework in designing our
boosting method for PU data. Nevertheless, there remain three critical problems considering
there are only P and U data: i) How to update the data distribution as U data is unlabeled? ii)
Can the theoretical properties of AdaBoost be kept in PU learning? iii) Can the performance
of a boosting-based PU algorithm be comparable or even superior to neural network-based
ones?

To address these problems, in this paper, we propose Ada-PU, which is the ﬁrst AdaBoost
style algorithm for PU learning. The proposed algorithm solves the three challenges raised
above. In particular, we follow the AdaBoost to update the data distribution in each iteration
and ﬁnd that diﬀerent from AdaBoost, two diﬀerent distributions of P data are needed to be
maintained and updated simultaneously in each iteration as P data are regarded as positive
and negative at the same time. We use the unbiased way of uPU [9] to estimate the error rate
with only P and U data, while the error rate is used further to derive the combination weight
of each base classiﬁer. We show that in learning the base classiﬁer, it is essential to consider
the non-negativity of the risk to achieve the theoretical guarantee for the proposed method.
Finally, experiments have demonstrated the superiority of our proposed Ada-PU.

The remaining paper is organized as follows. In Sec. 2, we brieﬂy introduce PU learning
and AdaBoost. In Sec. 3, we give the proposed Ada-PU. We analyze Ada-PU’s theoretical
properties in Sec. 4. The experimental results are presented in Sec. 5, with conclusion in Sec. 6.

1https://www.kaggle.com/c/kaggle-survey-2021/overview

2

2 Formulation and Background

2.1 AdaBoost

Boosting is an ensemble method, which generates a set of base learners sequentially and
combines them into a model to achieve strong performance. Boosting boosts “weak” learners
into a “strong” learner. AdaBoost (adaptive boosting) is one of the most famous boosting
algorithm [11, 12], that this algorithm and many of its variants are applied widely to diﬀerent
ﬁelds, such as facial recognition [31], and ﬁnancial fraud detection [30].

Following [33], we review AdaBoost from the view of [14]. AdaBoost generates a sequence
of T hypotheses {ht(x) : t = 1, . . . , T } where h(x) ∈ {−1, +1} and combines them with weights
αt. The ﬁnal output of the algorithm is in a form of additive weighted combination of the
generated base learners, that is HT (x) = sign((cid:80)T
t=1 αtht(x)). The objective of AdaBoost is to
generate in the t-th iteration a hypothesis ht(x) that could correct the previous prediction
results by Ht−1(x) = sign((cid:80)t−1
In each iteration, AdaBoost updates the data
distribution D of the training samples by improving misclassiﬁed data’s weights and decreasing
correctly classiﬁed data’s weights. In particular, AdaBoost tries to minimize an exponential
lossexp(h) = E(x∼D,y)[e−yh(x)] in each iteration to generate a new hypothesis h. When the
context is clear, we omit the subscript t or t − 1 and use h and H respectively, where h
denotes the new hypothesis we generate in the current iteration and H denotes the weighted
combination of the hypotheses we have already achieved in the previous iterations. Using H
and h, the exponential loss will be

t=1 αtht(x)).

Rexp(H + αh) = E(x,y)[e(−y(H(x)+αh(x))].

(1)

Denoting (cid:15) = Ex∼D[y (cid:54)= h(x)], we can get the weight α that greedily minimizes the risk
Rexp(H + αh) as

α =

1
2

ln

1 − P (y (cid:54)= h(x))
P (y (cid:54)= h(x))

=

1
2

ln

1 − (cid:15)
(cid:15)

.

(2)

Expand Eq. (1) to second order about h(x) = 0 conditioned on x. we ﬁx α = 1 since it would
not lose generalization as long as α remains a constant. Then

Rexp(H + h|x) ≈ Ey[e−yH(x)(1 − yh(x) + y2h(x)2/2)|x]

= Ey[e−yH(x)(1 − yh(x) + 1/2)|x],

which is further minimized by

h∗(x) = arg max

Ey[e−yH(x)yh(x)|x].

h

(3)

From the above equation, getting h∗(x) is equal to training a classiﬁer h when data is under
the distribution Dt(x) = e−yHt−1(x)P (y|x). Therefore, the distribution is updated continuously
in each iteration as

D(t+1)(x) = e−y(Ht(x)+αht+1(x))P (y|x) = Dt(x) · e−αyht+1(x).

(4)

3

2.2 PU learning

Let X ∈ Rd and Y ∈ {+1, −1}(d ∈ N+) be the input and output random variables, respectively.
In the case-control PU learning problem, the training dataset X is composed of P data and U
data, where P data of size np is sampled from P (x|Y = +1) and U data of size nu is sampled
from P (x). Let πp = P (Y = +1), and πn = 1 − πp. Following [8, 20, 18, 3], we assume πp
as known throughout the paper. In PU learning, our objective is to learn a binary classiﬁer
g : Rd → {+1, −1}.

The study of PU learning can trace back to to [6, 5, 7]. Early works are based on the
sample selection approach, which relied on heuristics to select reliable N data from U and
performs PN learning [23, 22]. Such methods are often called the two-step method. A poor
selection strategy may lead to unsatisfactory performance. As far as we know, only one existing
work that combined a boosting method and PU learning together [32], which followed the idea
of sample selection. Due to the lack of consistent analysis in the learning objective and the
heuristical self-learning process, the approach fails to directly learn from PU data and does
not bear the theoretical soundness of boosting methods.

Since the two-step methods may lead to unsatisfactory performance, another group of
methods emerged based on the importance reweighting, which regards U data as N data
with smaller weights [21, 24]. The experimental results demonstrated the superiority of such
methods [24, 10]. Since then, PU learning methods based on the importance reweighting
approach are popular, among which the unbiased PU learning (uPU) [9] is a milestone. However,
uPU suﬀers from severe overﬁtting when utilizing neural networks. Therefore, non-negative
PU learning (nnPU) [20] was proposed. Many SOTA methods have been proposed based on
nnPU, such as [3, 30, 4], which are all NN-based methods. Below we give more details about
uPU and nnPU.

Let (cid:96) : R × {+1, −1} → R be a loss function, that is, (cid:96)(g(x), y) is the loss occurred when
the classiﬁer g outputs the prediction g(x) when the label of x is y. When both P and N data
are available, the risk of the classiﬁer g is

R(g) = πpEP (x|Y =+1)[(cid:96)(g(x), +1)] + πnEP (x|Y =−1)[(cid:96)(g(x), −1)].

In PU learning, without N data, we cannot estimate directly the EP (x|Y =−1)[(cid:96)(g(x), −1)].
However, since πnP (x|Y = −1) = P (x) − πpP (x|Y = 1), we have πnEP (x|Y =−1)[(cid:96)(g(x), −1)] =
EP (x)[(cid:96)(g(x), −1)] − πpEP (x|Y =+1)[(cid:96)(g(x), −1)] and [8, 9]

R(g) =πpEP (x|Y =+1)[(cid:96)(g(x), +1)]+

(cid:0)EP (x)[(cid:96)(g(x), −1)] − πpEP (x|Y =+1)[(cid:96)(g(x), −1)](cid:1) .

We use π to represent πp for convenience. Thus, we can estimate R(g) using only P and U
data by

(cid:98)Rpu(g) =

π
np

(cid:88)

x∈P

(cid:96)(g(x), +1) +

1
nu

(cid:88)

x∈U

(cid:96)(g(x),−1) −

π
np

(cid:88)

x∈P

(cid:96)(g(x),−1).

(5)

The above estimator is an unbiased risk estimator [8] that allows us to estimate the risk
based on P and U data. Optimizing it gives the uPU [9] method. However, overﬁtting appears
while using models with a strong ﬁtting ability (such as deep neural networks) as a base

4

optimizer. Therefore, a non-negative estimator [20] was used to address this problem, which is

(cid:98)Rnnpu(g) =

π
np

(cid:88)

(cid:96)(g(x), +1)+

x∈P
(cid:32)

max

0,

1
nu

(cid:96)(g(x), −1)−

(cid:88)

x∈U

(cid:33)

(cid:96)(g(x), −1))

.

π
np

(cid:88)

x∈P

Because πnEP (x|Y =−1)[(cid:96)(g(x), −1)] is the risk on negative data, thus it should always be non-
negative.

3 Proposed Method: Ada-PU

The same as AdaBoost, our proposed Ada-PU needs to solve two key problems: one of which
is generating the weighted hypotheses, and the other one is updating the data distribution. In
PU learning, we do not know the label of the unlabeled samples. Thus we cannot compute
their losses. Instead, we estimate the losses with P and U data.

3.1 Estimation of the combination weight

Note that even when only P and U data are available, the ultimate goal of learning is still to
minimize the risk Eq. (1), the expected exponential loss of H, α, and h on the data. Thus, we
still need to estimate α and h∗(x) as depicted in Eq. (2) and Eq. (3) respectively.

To estimate α following Eq. (2), we need to ﬁrst have (cid:15). The obstruction is that we do not
know the labels of all the data in PU learning. That is, (cid:15) = Ex∼D[y (cid:54)= h(x)] = P (y (cid:54)= h(x))
should be estimated on the given P and U data only. Ideally, when both P and N data are
labeled,

(cid:15) = πEx∼P (x|y=1)[(cid:96)01(h(x), +1)] + (1 − π)Ex∼P (x|y=−1)[(cid:96)01(h(x), −1)]

where Ex∼P (x|y=1)[(cid:96)01(h(x), +1)] can be estimated on P data. For Ex∼P (x|y=−1)[(cid:96)01(h(x), −1)],
although labeled data is not available, the following equality holds

Ex∼P (x|y=−1)[(cid:96)01(h(x), −1)]

=

1
1 − π

Ex∈U[(cid:96)01(h(x), −1)] −

π
1 − π

Ex∈P[(cid:96)01(h(x), −1)],

thus

(cid:15) = πEx∈P[(cid:96)01(h(x), +1)] + Ex∈U[(cid:96)01(h(x), −1)] − πEx∈P[(cid:96)01(h(x), −1)]

whose empirical estimation is

(cid:98)(cid:15)(P, U) =

π
np
1
nu

(cid:88)

[(cid:96)01(h(x), +1)] +

x∈P
(cid:88)

[(cid:96)01(h(x), −1)] −

x∈U

π
np

(cid:88)

x∈P

[(cid:96)01(h(x), −1)],

(6)

where the ﬁrst term can be seen as the empirical error of P data weighted by π using ground
truth +1; we denote such kind of positive data by P+. The second term is the empirical error

5

of U data using ground truth −1; we denote such unlabeled data by U−. Finally, the third
term is the negative empirical error of P data weighted by π using ground truth −1 by treating
them as negative; we denote such kind of positive data with ground truth −1 by P−. In the
training phase of AdaBoost, (cid:15) needs to be estimated in iteration t through the given Ht−1
learned from previous rounds, and the ht learned at the current round. If we denote



(7)

w0(x) =

,

π
np
1
,
nu
− π
np

,



for x ∈ P+
for x ∈ U−
for x ∈ P−,

then Eq. (6) estimated in the t-th iteration can be written in its equivalent form as

(cid:98)(cid:15)t(P, U) =

(cid:88)

x∈P+

[w0(x)e−Ht−1(x)(cid:96)01(h(x), +1)] +

[w0(x)eHt−1(x)(cid:96)01(h(x), −1)] −

(cid:88)

x∈U−

[w0(x)eHt−1(x)(cid:96)01(h(x), −1)],

(cid:88)

x∈P−

and the weight of the t-th hypothesis is

αt

PU =

1
2

ln

1 − (cid:98)(cid:15)t(P, U)
(cid:98)(cid:15)t(P, U)

.

(8)

(9)

The estimation of αPU in Ada-PU has a similar form as the α of AdaBoost in Eq. (2).
However, since we do not have N data, we use P and U data to estimate (cid:98)(cid:15)t(P, U). Such
an estimation based on P and U data is consistent with the α estimated in fully supervised
cases [27].

3.2 Updating the data weight

In this part, we focus on how to choose the optimal hypothesis and update the data weight. In
fully-supervised AdaBoost, the data weight is updated continuously in each iteration in Eq. (4),
which assumes that the label y for individual instance x is available. Given that no label is
available for U data in PU learning, it is diﬃcult or even impossible to update the weight for
individual instance in the same way as in PN learning, where we only need to maintain and
update a set of data distribution for all the data.

To solve this problem, we target the same learning objective Eq. (3), we want to maximize

E(x,y)[e−yH(x)yh(x)], which can be extended by

E(x,y)[e−yH(x)yh(x)] =
(cid:90)

=

e−H(x)h(x)P (x, y = 1)dx +

(cid:90)

−eH(x)h(x)P (x, y = −1)dx

(cid:90)

e−yH(x)yh(x)P (x, y)dxdy

(cid:90)

(cid:90)

= π

π

e−H(x)h(x)P (x|y = 1)dx +

eH(x)h(x)P (x|y = 1)dx −

(cid:90)

eH(x)h(x)P (x)dx.

(10)

(11)

The second equivalence is due to PU learning is a binary classiﬁcation problem, and there
are only two values for y: +1 or −1. The third equivalence is due to that P (x, y = −1) =

6

P (x) − P (y = 1)P (x|y = 1). By written E(x,y)[e−yH(x)yh(x)] in the sum of Eq. (10) and
Eq. (11), it can now be estimated by the given PU data. Specially, Eq. (10) and the ﬁrst part
of Eq. (11) can be estimated by P data; the second part of Eq. (11) can be estimated using U
data. In this way, the empirical estimation (cid:98)E for E(x,y)[e−yH(x)yh(x)] is

(cid:98)E =

π
np

(cid:88)

x∈P+

e−H(x)h(x) +

π
np

(cid:88)

x∈P−

eH(x)h(x) −

1
nu

(cid:88)

x∈U−

eH(x)h(x).

(12)

Recall that Ht(x) = Ht−1(x) + αt

PUht(x), which means e−H(x) = e−Ht−1(x)e−αt

PUht(x) and

eH(x) = eHt−1(x)eαt

PUht(x) for t ≥ 1. With the w0(x) deﬁned in Eq. (7), if we further deﬁne

wt+1(x) =






PUht(x),

wt(x)e−αt
wt(x)eαt
wt(x)eαt

PUht(x),
PUht(x),

for x ∈ P+
for x ∈ U−
for x ∈ P−

, then

(13)

(cid:98)Et =

=

=

(cid:88)

x∈P+
(cid:88)

x∈U−
(cid:88)

x∈P+
(cid:88)

w0(x)e− (cid:80)t−1

j=1 αj

PUhj (x)ht(x) +

w0(x)e

(cid:80)t−1

j=1 αj

PUhj (x)ht(x)

w1(x)e− (cid:80)t−1

j=2 αj

PUhj (x)ht(x) +

(cid:88)

x∈P−

(cid:88)

x∈P−

w1(x)e

(cid:80)t−1

j=2 αj

PUhj (x)ht(x)

w0(x)e

(cid:80)t−1

j=1 αj

PUhj (x)ht(x) −

w1(x)e

(cid:80)t−1

j=2 αj

PUhj (x)ht(x) −

x∈U−

(cid:88)

x∈P+∪P−

wt−1(x)ht(x) −

(cid:88)

x∈U−

wt−1(x)ht(x).

(14)

Eq. (14) tells us that when only P and U data are available, we need to keep updating
three distributions: two for P data, and one for U data, by wt(x) deﬁned in Eq. (13). After
re-weighting the data points by the updated distribution, maximize the risk in Eq. (14) could
lead to a greedy solution h∗

t (x) in the t-th iteration.

Using the same notation, the corresponding (cid:98)(cid:15)t(P, U) deﬁned in Eq. (8) can be written in a

more simple form as

(cid:98)(cid:15)t(P, U) =

(cid:88)

[wt−1(x)(cid:96)01(h(x), +1)] +

x∈P+
(cid:88)

x∈U−

[wt−1(x)(cid:96)01(h(x), −1)] −

(cid:88)

x∈P−

[wt−1(x)(cid:96)01(h(x), −1)],

(15)

where the second line is deﬁned as (cid:98)(cid:15)nn(P, U ), which in expectation should be equal to
Ex∼P (x|y=−1)[(cid:96)01(h(x), −1)].

3.3 Algorithm and implementation

In the previous two subsections, we have derived two critical components that are necessary
for the Ada-PU algorithm we plan to propose: the combining weights αPU and the updating of

7

distribution. In this subsection, we summarize the procedures to form the algorithm Ada-PU,
and give some implementation guides for practical consideration.

Sec. 3.2 has derived how to generate the h∗

t (x) by maximizing Eq. (14). In AdaBoost-style
algorithms, the base learner ht is usually a decision stump, i.e., a depth-one decision tree [11].
Thus, the algorithm Ada-PU StumpGenerator is given in Algorithm 1. In particular, the
decision stump is based on Ht−1(·) trained in previous rounds. The algorithm enumerates all
features in Line 3. For each feature, S values are randomly sampled in the value range of this
feature, and the best splitting value is selected from Line 4 to Line 16. For each selected value
v, a depth-one decision tree can be generated, and the corresponding (cid:98)(cid:15)t and (cid:98)(cid:15)nn are calculated
in Line 7. Since the Decision Stump should be a weak learner, whose error rate should be
smaller than 0.5; on the other hand, (cid:98)(cid:15)nn is used to approximate Ex∼P (x|y=−1)[(cid:96)(h(x), −1)], so
(cid:98)(cid:15)nn should be larger than zero. In this way, h that is larger than 0.5 or smaller than 0 are
skipped in Line 9. For those kept h, the empirical risk (cid:98)Et is calculated. The current largest
(cid:98)Et and its corresponding h∗
PU is
calculated in Line 18.

t are recorded in Line 13. Finally, based on the best h∗

t , αt

In the procedure of generating a decision stump, instead of enumerating all possible splitting
values, we only selected S values for each feature to improve eﬃciency. In Sec. 5, we have
also demonstrated that randomly sampling S candidate splitting values does not impact
performance.

Based on Alg. 1, Ada-PU is shown in Alg. 2. In this part, we initialize w0 in Line 2 and
then learn T classiﬁers from Line 3 to 7 and their corresponding weights. Specially, we set
additionally a “cool down” parameter β following the implementation of scikit-learn. In Sec. 5,
we demonstrate the choice of β for eﬀective training.

4 Theoretical Properties

AdaBoost is famous for its rigorous theoretical foundation. A question that can be raised is
whether Ada-PU holds the merits of AdaBoost with fully supervised data. In this part, we will
show that our proposed Ada-PU bears the same theoretical soundness as AdaBoost. Note that
in this section, we assume the data weight wt are always normalized by wt(x)/Z, where Z is a
normalization factor that Z = (cid:80)
x wt(x), without impacting the overall empirical performance.
Before giving theoretical results, we ﬁrst deﬁne a hypothesis class from which the base learner
is learned.

Deﬁnition 4.1. [Non-Negative Base Classiﬁer Set] Denote hypothesis class F = {h|hd →
{−1, +1}}, where d is the dimensionality of the feature space. A base classiﬁer set H is deﬁned
as Non-Negative Base Classiﬁer Set, which is a subset of F for given PU data. If for all linear
combinations of classiﬁers in H, any h ∈ H satisﬁes

µt(x)I(h(x) (cid:54)= +1) +

(cid:88)

x∈P+

µt(x)I(h(x) (cid:54)= −1)

(cid:88)

x∈P−

µt(x)I(h(x) (cid:54)= −1) > 0

(16)

(cid:88)

+

x∈U−

8

Randomly select a value v in the value range of f
Use v as a split value to split the tree and get h
Calculate (cid:98)(cid:15)t and (cid:98)(cid:15)nn based on Eq. (15)
if (cid:98)(cid:15)t ≥ 0.5 or (cid:98)(cid:15)nn < 0 then

Algorithm 1 Ada-PU StumpGenerator

Input π; wt−1(·); P; U; S
t , αt
Output h∗

for s = 1, . . . , S do

PU
1: procedure StumpGenerator
Initialize (cid:98)Emax = −∞
2:
for feature f do
3:
4:
5:
6:
7:
8:
9:
10:
11:

Continue

else

Calculate (cid:98)Et by Eq. (14)
if (cid:98)Et > (cid:98)Emax then
(cid:98)Emax = (cid:98)Et; h∗

t = h; (cid:15)min = (cid:98)(cid:15)t

end if

12:
13:
14:
15:
16:
17:
18:
19: end procedure

end for
αt

end for

end if

PU = 1/2 ln[(1 − (cid:15)min)/(cid:15)min]

Algorithm 2 Ada-PU

Input π; P; U; T ; β; S
Output F (x)

1: procedure Ada-PU
2:
3:
4:
5:
6:
7:
8:
9: end procedure

end for
F (x) = sign (HT (x))

j=1 αj

Calculate w0(·) based on Eq. (7)
for t = 1, . . . , T do

PU = StumpGenerator (π, wt−1, P, U, S)

t , αt
h∗
Calculate wt based on Eq. (13)
Ht = β((cid:80)t

PUh∗
j )

where H(·) is a linear combination of classiﬁers in H and

µt(x) =






π
np
1
nu
− π
np

exp (−H(x)) ,

exp (H(x)) ,

exp (H(x)) ,

for x ∈ P+
for x ∈ U−
for x ∈ P−

Remark Non-Negative Base Classiﬁer Set is a subset of the hypothesis class in fully-supervised
learning. This deﬁnition has a strong restriction on the hypothesis class. In practice, we testify
the learned decision stump belongs to H by ensuring (cid:98)(cid:15)nn > 0 in Line 8 of Alg. 1.

9

4.1 Greedy optimization

One of the fully-supervised AdaBoost’s theoretical properties is that the learned α and h∗(·)
greedily optimize the objective risk [16]. In this part, we will show our proposed Ada-PU has
the same theoretical property.

For AdaBoost, the loss function used is exponential loss. Using exponential loss in Eq. (5),

and replace g by Ht(x) = Ht−1(x) + αh(x), we have
π
np

pu (α, h) =

exp(−Ht−1(x) − αh(x)) +

(cid:98)Rexp

(cid:88)

x∈P+

1
nu

(cid:88)

x∈U−

exp(Ht−1(x) + αh(x)) −

π
np

(cid:88)

x∈P−

exp(Ht−1(x) + αh(x)).

Theorem 4.1. Fixing h ∈ H, α learned from Eq. (9) is the minimizer of (cid:98)Rexp
h(x) learned by maximizing Eq. (12) over H is the minimizer of (cid:98)Rexp

pu (·, h).

pu (α, ·); Fixing α,

Proof. We follow the proof from [16]. Specially, we denote the (cid:98)Rexp
as (cid:98)Rexp

pu [t − 1](αt−1, ht−1), then we have

pu (·, ·) in the (t − 1)th round

(cid:98)Rexp

pu (α, h)
pu [t − 1](αt−1, ht−1)

wt(x) exp(−αtht(x)) +

(cid:88)

x∈U−

wt(x) exp(αtht(x)) +

wt(x) exp(αtht(x))

=

(cid:98)Rexp
(cid:88)

x∈P+
(cid:88)

x∈P−

= e−αt + (eαt − e−αt)

(cid:32)

(cid:88)

x∈P+

wt(x)I(ht(x) (cid:54)= 1) +

(cid:88)

x∈U−∪P−

wt(x)I(ht(x) (cid:54)= −1)

.

(cid:33)

According to Deﬁnition 4.1, (cid:80)
non-negative, so we have

x∈P+ wt(x)I(ht(x) (cid:54)= 1) + (cid:80)

x∈U−∪P− wt(x)I(ht(x) (cid:54)= −1) is

αt

PU =

1
2

ln

1 − (cid:98)(cid:15)t(P, U)
(cid:98)(cid:15)t(P, U)

,

the same as Eq. (9). Due to that Eq. (12) is an unbiased estimation of E(x,y)[e−yH(x)yh(x)], we
can show that maximize Eq. (12) can lead to optimizer h∗ by the same derivation of Eq. (3).
We omit the details here.

Remark Theorem 4.1 shows that our proposed Ada-PU follows a greedy strategy to select
the optimum ht and αt in each iteration.

4.2 Weak learning implies strong learning

Another theoretical property of AdaBoost is that the empirical classiﬁcation error decreases
to zero at an exponential rate [11]. In this part, we will show our proposed Ada-PU bears a
similar theoretical property. To give the theoretical guarantee, we ﬁrst deﬁne the Weak Learner
by giving a constant γ (called edge), which is the same as the Weak Learner in [11] deﬁned on
fully-supervised data.

10

Deﬁnition 4.2. [Weak Learner for PU Learning] For any weight w on the data set, a
Weak Learner will learn h ∈ H with (cid:98)(cid:15)(P, U) ≤ 1−γ
2 , where 0 < γ ≤ 1, and (cid:98)(cid:15)(P, U) is given in
Eq. (6).

According to the above deﬁnition, the weak learner always gives a weak classiﬁer with some

edge over random guessing, based on which we give the following theoretical result.

Theorem 4.2. After T rounds of learning, if we deﬁne the empirical estimator on PU data
achieved by HT as

(cid:98)(cid:15)(HT ) =

π
np
π
np

(cid:88)

x∈P+
(cid:88)

x∈P−

[(cid:96)01(HT (x), +1)] +

[(cid:96)01(HT (x), −1)].

1
nu

(cid:88)

x∈U−

[(cid:96)01(HT (x), −1)] −

Then we have

(cid:98)(cid:15)(HT ) ≤

T
(cid:89)

t=1

2(cid:112)

(cid:98)(cid:15)t(P, U)(1 − (cid:98)(cid:15)t(P, U)).

where (cid:98)(cid:15)t(P, U) is deﬁned in Eq. (8). Using Weak Learner, the empirical classiﬁcation error
decreases to zero at an exponential rate, that is, (cid:98)(cid:15)(HT ) ≤ (1 − γ2) T
2 .
Proof. The proof is motivated by [11]. Denote the exponential loss achieved by HT as (cid:98)(cid:15)exp(HT ),
from the proof of Theorem 4.1, we have

(cid:98)(cid:15)exp(HT ) =

(cid:88)

w0(x) exp (−HT (x)) +

(cid:88)

w0(x) exp (HT (x))

x∈P+
(cid:16)
(cid:15)αT

P U +

=

(cid:16)

(cid:15)αT

P U +

(cid:16)

(cid:16)

eαT

P U − e−αT

P U

eαT

P U − e−αT

P U

(cid:17)(cid:17)

(cid:17)(cid:17)

x∈U−∪P−
(cid:98)(cid:15)t(P, U)

(cid:88)

w0(x) exp (−HT −1(x)) +

(cid:98)(cid:15)t(P, U)

x∈P+

(cid:88)

x∈U−∪P−

w0(x) exp (HT −1(x))

= 2(cid:112)

(cid:98)(cid:15)T (P, U)(1 − (cid:98)(cid:15)T (P, U))

(cid:88)

w0(x) exp (−HT −1(x)) +

2(cid:112)

(cid:98)(cid:15)T (P, U)(1 − (cid:98)(cid:15)T (P, U))

x∈P+

(cid:88)

x∈U−∪P−

=

T
(cid:89)

t=1

2(cid:112)

(cid:98)(cid:15)t(P, U)(1 − (cid:98)(cid:15)t(P, U)).

w0(x) exp (HT −1(x))

Since the exponential loss upper-bounds the 0/1 loss,

(cid:98)(cid:15)(HT ) ≤

T
(cid:89)

t=1

2(cid:112)

(cid:98)(cid:15)t(P, U)(1 − (cid:98)(cid:15)t(P, U)).

If γ > 0, then according to our weak learning assumption, each (cid:98)(cid:15)t(P, U) ≤ 1−γ
2(cid:112)
equality 1 − a ≤ e−a, ∀a ∈ R. When T = 2 ln n

2 , and
2 < e− γ2T
2 by using the in-
γ2 , where n denotes the number of the dataset,

(cid:98)(cid:15)t(P, U)(1 − (cid:98)(cid:15)t(P, U))≤ (cid:112)1 − γ2. Hence (cid:98)(cid:15)(HT ) ≤ (1 − γ2) T

11

2 = 1

we have e− γ2T
n which means that given a suﬃcient amount
of data, the empirical classiﬁcation error estimated by P and U data decreases to 0 at an
exponential rate.

n . This gives us (cid:98)(cid:15)(HT ) < 1

Remark Theorem 4.2 shows that as the number of rounds increases, the empirical error
decreases exponentially to zero. Even when we only have P and U data, the property to boost
a weak learner to a strong one still holds by Ada-PU.

4.3 Weak learning implies large margins

At T rounds of learning, the quantity yH(x) on an example (x, y) is the margin of H for
this training example. A large margin implies high conﬁdence. In [1], it is shown that fully
supervised AdaBoost increases the margins on training data. This part will show the same
property held by Ada-PU.

Theorem 4.3. For suﬃciently large T and ∀γ ∈ (0, 1], Ada-PU outputs H(x) ∈ co(H) with
minx yH(x) ≥ γ, where co(H) includes all linear combinations of classiﬁers in H. y = 1 if
x ∈ P+, and −1 if x ∈ P− ∪ U−.

Proof. The proof is motivated by [1], following which we ﬁrst show that for all the data, the
classiﬁer H given by Ada-PU satisﬁes PPU(yH(x) ≤ γ) ≤ (cid:81)T
(P, U)),
where PPU(yH(x) ≤ γ) denotes the probability that the margin of the classiﬁer H less than
the edge γ with respect to the empirical distribution by

(P, U)(1 − (cid:98)(cid:15)1−γ

(cid:98)(cid:15)1−γ

t=1 2

(cid:113)

t

t

PPU(yH(x) ≤ γ) = PPU(yH(x) − γ ≤ 0) =

(cid:88)

P+∪U−∪P−

w0(x)I(yH(x) − γ ≤ 0)

(cid:88)

w0(x)exp(−

P+∪U−∪P−

T
(cid:88)

t=1

αt(yht(x) − γ)) ≤ w0(x)exp(−yHt(x))exp(

T
(cid:88)

αtγ)

t=1

≤

≤

≤

T
(cid:89)

t=1
T
(cid:89)

t=1

2(cid:112)

(cid:98)(cid:15)t(P, U)(1 − (cid:98)(cid:15)t(P, U))

T
(cid:89)

t=1

(cid:115)(cid:18) 1 − (cid:98)(cid:15)t(P, U)
(cid:98)(cid:15)t(P, U)

(cid:19)γ

(cid:113)
2

(cid:98)(cid:15)1−γ

t

(P, U)(1 − (cid:98)(cid:15)t)1+γ(P, U).

We have (cid:98)(cid:15)t(P, U) ≤ 1−2γ

2

for any t for the weak learner assumption, thus

(cid:113)
2

(cid:98)(cid:15)1−γ

t

(P, U)(1 − (cid:15)t)1+γ(P, U) ≤ (cid:112)(1 − 2γ)1−γ(1 + 2γ)1+γ < 1.

Then (cid:98)PPU(yH(x) ≤ γ) ≤ ((1 − 2γ)1−γ(1 + 2γ)1+γ) T
decreasing to 0 at an exponential rate. Therefore, min yH(x) ≥ γ.

2 . For suﬃciently large T , PPU(yH(x) ≤ γ)

Remark A large margin indicates a good classiﬁer since it represents a correct classiﬁcation
with high conﬁdence. Theorem 4.3 means that Ada-PU is able to increase the margins on the
training set and achieve a positive lower bound given suﬃcient training rounds.

12

5 Experiments and Results

In this section, we study the empirical performance of our proposal from the following two
aspects: i) compare the performance of Ada-PU with NN-based PU methods; ii) compare the
performance of Ada-PU with Boosting-based PN methods. We also do an ablation study for
Ada-PU.

5.1 Dataset

In this work, we mainly study our proposal’s performance on tabular data following [20].
Details of the four used datasets are

• Epsilon2, a binary classiﬁcation text dataset, has 40, 000 training and 100, 000 test

examples, each with 2, 000 features. πp = 0.5.

• Breast Cancer3, a binary classiﬁcation dataset, has 455 training and 114 test examples.

Each example has 30 features. πP = 0.59.

• UNSW-NB154, a binary classiﬁcation dataset, has 175, 340 training and 82, 331 test

examples. Each example has 39 features. πP = 0.68.

• CIFAR-105, a multi-class dataset, has 50, 000 training and 10, 000 examples. Each
example has 3, 072 features. πp = 0.4. To make it binary, we follow [20] treating classes
‘airplane’, ‘automobile’, ‘ship’ and ‘truck’ as positive class.

We follow [20] To generate the PU and PN data from the given binary classiﬁcation
data. Speciﬁcally, for PU data np = 1, 000 and nu is the number of all training data; for PN
data, nn = (πn/2πp)2np for that [27] has provided theoretical results that PU learning could
outperform such scale of PN data.

5.2 Setting of methods

For each method, the experiments are conducted for given times to reduce the impact of
randomness. Mean and standard deviation (std) are reported.
Ada-PU We use the random feature value selection strategy in the training process on all
datasets and set S = 10. Speciﬁcally, for CIFAR-10, the early stage is to extract features
by using CNN, and we set β = 0.1. We set β = 0.9 on Epsilon for better performance. For
UNSW-NB15, we set β = 0.2. Finally, we set β = 0.00125 on BreastCancer. Ablation studies
on the impact of β are provided in Sec 5.4.
PU methods We compare our proposed Ada-PU with neural network based methods uPU [9]
and nnPU [20]. For Epsilon, BreastCancer and UNSW-NB15, we follow [20] to use multilayer
perceptron (MLP) with as network structure, where Epsilon uses Softsign [15] and other
two datasets use ReLU [26]. Hyperparameters are also tuned following [20]. For CIFAR-10,
Residual Network (ResNet) [17] with ReLU [26], which is one of the best model architectures

2https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html
3https://goo.gl/U2Uwz2
4https://research.unsw.edu.au/projects/unsw-nb15-dataset
5https://www.cs.toronto.edu/~kriz/cifar.html

13

(a) CIFAR-10

(b) Epsilon

(c) UNSW-NB15

(d) BreastCancer

Figure 1: Accuracy of NN-based methods with diﬀerent model structures where the dense
line shows the average of 5 trials, and the shadow area shows the standard deviation. For
CIFAR-10, we compare the performance between all convolutional net [29] (nnPU-CNN) and
ResNet. For the other datasets, we use MLP and the 1-hidden layer MLP -linear model. The
legend deﬁnes in (b) also applies to (c) and (d).

for CIFAR-10, is used as the network structure. In all neural networks, we use Adam [19] as
the optimizer and set the number of training epochs to be 100 to ensure convergence.

To make the model selection for MLP and ResNet, we demonstrate the empirical results
using MLP with the number of hidden layers 1, 3, 6, 7, 8, 9 (where 1 is the linear model) and
ResNet 18, 32, 56, 110 to test a variety of models. The results are demonstrated in Fig. 1 from
which we can tell that ResNet 18 has the best performance on CIFAR-10; 3-layer MLP has the
best performance on Epsilon and BreastCancer. On UNSW-NB15, we select 9-layer MLP.
PN methods We compare our proposed Ada-PU with SOTA boosting methods XGBoost [2],
AdaBoost [34] and GBDT [13] implemented by scikit-learn using default parameter. The
number of iterations is set as 100 as the number of epochs in neural networks.

5.3 Results

Compare to NN-based PU methods The results comparing with NN-based PU learning
methods are reported in Fig. 2. The results show that Ada-PU outperforms nnPU-MLP,
nnPU-Linear, and uPU on the four datasets. Ada-PU slightly outperforms nnPU-ResNet on
CIFAR-10. From the results, Ada-PU performs superior on tabular data. On image data with
pre-extracted features, Ada-PU performs comparably with the best NN-based method.
Comparing to PN boosting methods The results comparing Ada-PU with PN methods
are reported in Fig. 3. The results show that for PN methods, Ada-PU performs better on
CIFAR-10, Epsilon, and BreastCancer and comparable on UNSW-NB15. Note that in such
studies, instead of giving all N data, a limited number of N data is provided as speciﬁed in
Sec. 5.1. Although sampling a limited number of N data may disadvantage PN methods, such
a way to generate N data is motivated theoretically by [27] to show that PU methods can
sometimes beat PN methods if a limited number of N data is provided.
Result summarizing To demonstrate the performance comprehensively, we report all ex-
perimental results’ mean and standard deviation in numerical form in Table 1. From the
results, we can see that Ada-PU is superior to all NN-based PU methods and is comparable to
boosting-based PN methods.

14

(a) CIFAR-10

(b) Epsilon

(c) UNSW-NB15

(d) BreastCancer

Figure 2: Training zero-one loss and test zero-one loss of Ada-PU and NN-based methods, on
CIFAR-10, Epsilon, UNSW-NB15, and BreastCancer. The dense line shows the average of
5 trials, and the shadow area shows the standard deviation. The legend deﬁnes in (b) also
applies to (c) and (d).

(a) CIFAR-10

(b) Epsilon

(c) UNSW-NB15

(d) BreastCancer

Figure 3: Testing accuracy of Ada-PU and PN boosting methods, on CIFAR-10, Epsilon,
UNSW-NB15, and BreastCancer. The dense line shows the average of 5 trials, and the shadow
area shows the standard deviation.

5.4 Ablation study

Eﬀects of feature value selection strategy with randomness. The left side of each
subﬁgure of Fig. 4 shows a selection strategy with randomness that improves the performance
of Ada-PU on CIFAR-10 and Epsilon. We conducted another set of experiments to visualize
the features selected by Ada-PU during one training round on CIFAR-10 and Epsilon, shown
on the right side of each subﬁgure of Fig. 4. We can see that the selection strategy with
randomness helps the model avoid selecting the same feature too many times. Therefore, in
our implementation of Ada-PU, we apply the feature value selection strategy with randomness.
Eﬀects of β. We explore how the β aﬀects the performance of Ada-PU. We evaluate the
performance of Ada-PU with β ∈ {0.001, 0.01, 0.1, 0.2, 0.5, 0.7, 0.9, 1} in the experiment. Fig. 5
summarizes the comparison results of Ada-PU with diﬀerent β on CIFAR-10, Epsilon, UNSW-
NB15 and BreastCancer.

15

Table 1: Accuracy in the form of the mean (std) for all compared methods.

Method

CIFAR-10 % Epsilon % UNSW-NB15 % BreastCancer %

PU
Methods

PN
Methods

nnPU-ResNet 87.17(1.14)

N/A

nnPU-MLP
80.22(1.44) 70.76(1.12)
nnPU-Linear 60.00(0.00) 60.28(5.72)
79.00(2.22) 56.26(1.66)

uPU

N/A
73.93(1.07)
54.11(14.94)
73.36(1.28)

XGBoost
GBDT
AdaBoost

Ada-PU

72.48(0.50) 63.98(0.65)
71.90(0.67) 58.42(0.51)
72.83(0.81) 64.22(0.43)

89.91(0.36) 71.42(1.11)

77.46(0.60)
76.58(1.26)
77.16(0.42)

75.94(0.61)

N/A
88.95(6.22)
44.56(29.79)
56.67(12.86)

76.99(0.00)
84.24(6.89)
78.76(13.02)

92.11(3.77)

(a) CIFAR-10

(b) Epsilon

Figure 4: The performance of Ada-PU under diﬀerent feature value selection strategies and
the visualization of features selected during one training round on CIFAR-10 and Epsilon.

(a) CIFAR-10

(b) Epsilon

(c) UNSW-NB15

(d) BreastCancer

Figure 5: Experimental results given β ∈ {0.001, 0.01, 0.1, 0.2, 0.5, 0.7, 0.9} on CIFAR-10,
Epsilon, UNSW-NB15 and BreastCancer.

16

6 Conclusion

In this paper, we propose a novel boosting PU learning method Ada-PU. We solved the problem
of how to update the data and classiﬁer weights with only P and U data. We also verify the
eﬀectiveness of Ada-PU theoretically and empirically. In the future, we will explore other
boosting methods for PU data.

References

[1] Peter Bartlett, Yoav Freund, Wee Sun Lee, and Robert E Schapire. Boosting the margin:
A new explanation for the eﬀectiveness of voting methods. The annals of statistics, 26(5):
1651–1686, 1998.

[2] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In KDD, 2016.

[3] X. Chen, W. Chen, T. Chen, Y. Yuan, C. Gong, K. Chen, and Z. Wang. Self-pu: Self

boosted and calibrated positive-unlabeled training. In ICML, 2020.

[4] X. Chen, C. Gong, and J. Yang. Cost-sensitive positive and unlabeled learning. Information

Sciences, 2021.

[5] F. Comit´e, F. Denis, R. Gilleron, and F. Letouzey. Positive and unlabeled examples help

learning. In ALT, 1999.

[6] F. Denis. Pac learning from positive statistical queries. In ALT, 1998.

[7] F. Denis, R. Gilleron, and F. Letouzey. Learning from positive and unlabeled examples.

Theoretical Computer Science, 348(1):70–83, 2005.

[8] M.C. Du Plessis, G. Niu, and M. Sugiyama. Analysis of learning from positive and

unlabeled data. NIPS, 2014.

[9] M.C. Du Plessis, G. Niu, and M. Sugiyama. Convex formulation for learning from positive

and unlabeled data. In ICML, 2015.

[10] C. Elkan and K. Noto. Learning classiﬁers from only positive and unlabeled data. In

KDD, 2008.

[11] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. In COLT, pages 23–37, 1995.

[12] Y. Freund, R. Schapire, and N. Abe. A short introduction to boosting. Journal-Japanese

Society For Artiﬁcial Intelligence, 14(771-780):1612, 1999.

[13] J. Friedman. Greedy function approximation: A gradient boosting machine. The Annals

of Statistics, 29(5):1189–1232, 2001.

[14] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a

statistical view of boosting. The annals of statistics, 28(2):337–407, 2000.

17

[15] X. Glorot and Y. Bengio. Understanding the diﬃculty of training deep feedforward neural

networks. In AISTATS, 2010.

[16] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The
elements of statistical learning: data mining, inference, and prediction, volume 2. 2009.

[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for

image recognition. In CVPR, pages 770–778, 2016.

[18] Y. Hsieh, G. Niu, and M. Sugiyama. Classiﬁcation from positive, unlabeled and biased

negative data. In ICML, pages 2820–2829. PMLR, 2019.

[19] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.

[20] R. Kiryo, G. Niu, M.C. Du Plessis, and M. Sugiyama. Positive-unlabeled learning with

non-negative risk estimator. NIPS, 30, 2017.

[21] W.S. Lee and B. Liu. Learning with positive and unlabeled examples using weighted

logistic regression. In ICML, volume 3, pages 448–455, 2003.

[22] X. Li and B. Liu. Learning to classify texts using positive and unlabeled data. In IJCAI,

2003.

[23] B. Liu, W. Lee, P. Yu, and X. Li. Partially supervised classiﬁcation of text documents. In

ICML, 2002.

[24] B. Liu, Y. Dai, X. Li, W. Lee, and P. Yu. Building text classiﬁers using positive and

unlabeled examples. In ICDM, 2003.

[25] A. Menon, B. van Rooyen, C. Ong, and B. Williamson. Learning from corrupted binary

labels via class-probability estimation. In ICML, 2015.

[26] V. Nair and G. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In

ICML, 2010.

[27] G. Niu, M.C. Du Plessis, T. Sakai, Y. Ma, and M. Sugiyama. Theoretical comparisons of

positive-unlabeled learning against positive-negative learning. NIPS, 2016.

[28] R. Schapire. The strength of weak learnability. MLJ, 5(2):197–227, 1990.

[29] J. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The

all convolutional net. In ICLR, 2015.

[30] G. Su, W. Chen, and M. Xu. Positive-unlabeled learning from imbalanced data. In IJCAI,

2021.

[31] P. Viola and M. Jones. Robust real-time face detection. IJCV, 57(2), 2004.

[32] J. Zhang, Z. Wang, J. Meng, Y. Tan, and J. Yuan. Boosting positive and unlabeled
learning for anomaly detection with multi-features. IEEE TMM, 21(5):1332–1344, 2018.

[33] Zhi-Hua Zhou. Machine learning. 2021.

[34] J. Zhu, S. Rosset, H. Zou, and T. Hastie. Multi-class adaboost. Statistics and Its Interface,

2, 2006.

18

