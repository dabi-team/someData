1
2
0
2

l
u
J

4
2

]

R
C
.
s
c
[

1
v
4
1
5
1
1
.
7
0
1
2
:
v
i
X
r
a

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

1

Multi-Perspective Content Delivery Networks
Security Framework Using Optimized Unsupervised
Anomaly Detection

Li Yang∗, Abdallah Moubayed∗, Abdallah Shami∗, Parisa Heidari†, Amine Boukhtouta†, Adel Larabi†, Richard
Brunner†, Stere Preda†, and Daniel Migault†
∗Western University, London, Ontario, Canada; e-mails: {lyang339, amoubaye, abdallah.shami}@uwo.ca
†Ericsson, Montreal, Quebec, Canada; e-mails: {parisa.heidari, amine.boukhtouta, adel.larabi, richard.brunner,
stere.preda, daniel.migault}@ericsson.com

Abstract—Content delivery networks (CDNs) provide efﬁcient
content distribution over the Internet. CDNs improve the con-
nectivity and efﬁciency of global communications, but their
caching mechanisms may be breached by cyber-attackers. Among
the security mechanisms, effective anomaly detection forms an
important part of CDN security enhancement. In this work, we
propose a multi-perspective unsupervised learning framework
for anomaly detection in CDNs. In the proposed framework,
a multi-perspective feature engineering approach, an optimized
unsupervised anomaly detection model that utilizes an isolation
forest and a Gaussian mixture model, and a multi-perspective
validation method, are developed to detect abnormal behaviors
in CDNs mainly from the client Internet Protocol (IP) and node
perspectives, therefore to identify the denial of service (DoS) and
cache pollution attack (CPA) patterns. Experimental results are
presented based on the analytics of eight days of real-world
CDN log data provided by a major CDN operator. Through
experiments, the abnormal contents, compromised nodes, ma-
licious IPs, as well as their corresponding attack types, are
identiﬁed effectively by the proposed framework and validated
by multiple cybersecurity experts. This shows the effectiveness
of the proposed method when applied to real-world CDN data.

Index Terms—Cache Pollution Attacks; DoS Attacks; Anomaly
Detection; Content Delivery Networks; Gaussian Mixture Model;
Bayesian Optimization.

I. INTRODUCTION

With the increasing popularity of caching techniques in
Internet communications, it is estimated that 71% of Internet
trafﬁc will be delivered through content delivery networks
(CDNs) by 2021 [1]. A CDN is a geographically distributed
network of servers that work together to provide fast com-
munications of Internet contents, including hypertext markup
language (HTML) pages, JavaScript ﬁles,
images, audios,
videos, etc. [2]. CDNs are developed to improve the process
of content delivery through caching mechanisms and multiple
edge servers [3]. As a large-scale CDN service provider,
Akamai accounts for approximately 20% of all web trafﬁc
[4].

CDNs improve the connectivity and efﬁciency of global
communications, but also introduce vulnerabilities to the con-
nected networks. Caching mechanisms become a major target
of cyber-attacks since open proxy caches may be exploited
by attackers to transmit malicious trafﬁc and perform various

harmful activities, which causes network congestion, unavail-
ability, or other severe consequences [5].

Cache pollution attacks (CPAs) and denial of service (DoS)
attacks are the two major types of cyber-attacks launched on
CDNs to cause service unavailability or to degrade the caching
service by reducing the cache hit rate and increasing latency
[6] [7]. In high-rate networks, even a moderate degradation
of the cache hit rate, or a moderate increase of latency may
result
in severe network congestion or a massive amount
of additional data transmissions [5]. CPAs are launched by
polluting the cache space with a large number of unpopular
or illegitimate contents; therefore, legitimate clients will get
many cache misses for popular ﬁles, making the caching
mechanism ineffective [6]. Similarly, DoS attacks are launched
by sending a sudden burst of requests to exhaust the network
resources of certain targeted nodes. However, DoS attacks are
not necessarily launched by sending requests for unpopular
ﬁles [7].

To protect a CDN against DoS and CPAs, an effective
method is to explore and analyze network access logs for
the purpose of abnormal behavior analysis and attack pattern
detection in CDNs [8]. Machine learning (ML) algorithms
have been widely used in many anomaly detection problems
[9]-[12]. In this paper, we focus on anomaly detection in CDNs
by analyzing 169 gigabytes (GB) of unlabeled real-world CDN
access log data provided by a major CDN operator. This work
aims to detect DoS attacks and CPAs based on the behaviors
of abnormal network entities, including the malicious Internet
Protocol (IP) addresses, abnormal contents, and compromised
nodes,
through the analysis of access logs. The proposed
anomaly detection framework consists of a multi-perspective
feature engineering, an unsupervised ML model built with
optimized isolation forest (iForest) [13] and Gaussian mixture
models (GMM) [14], and a multi-perspective result validation
method. The proposed work can be considered a labeling
technique on unlabeled CDN log data for anomaly detection
use cases.

On the other hand, as the data is completely unlabeled,
multiple experts from Ericsson Inc. were involved in the
learning phase to help construct effective ML models, which
is a standard data learning process named human-in-the-loop

 
 
 
 
 
 
ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

2

(HITL). HITL is the process of creating ML models by lever-
aging the power of both machine and human intelligence [15]
[16]. Labeling massive amounts of data usually need HITL to
obtain accurate labels for unlabeled data in the unsupervised
learning process, as ML models themselves are often unable
to determine true labels by themselves [16]. Therefore, the
HITL process is included in the proposed framework to ensure
accurate anomaly detection on the unlabeled dataset. HITL in
the proposed framework mainly includes the attack pattern &
feature analysis, ML result analysis, and ﬁnal result validation.
Detecting potential cyber-attacks and affected abnormal
network entities can also trigger other network defense and
mitigation mechanisms, such as blacklisting malicious client
IPs, isolating compromised nodes, and removing abnormal
cached contents out of the cache space [5]. Thus, CDNs can
recover from cyber-attacks or be prevented from potential
attacks with the help of effective network anomaly detection
techniques.

This paper makes the following contributions:
1) It summarizes the potential patterns and characteristics
of DoS and CPA attacks to assist with anomaly detection
in CDNs;

2) It proposes a comprehensive network feature engineer-
ing model that generates features from multiple perspec-
tives, including content, client IP, service provider, and
account-offering perspectives;

3) It proposes an optimized unsupervised anomaly de-
tection model utilizing iForest, GMM, and Bayesian
optimization (BO), to detect cyber-attacks and affected
network entities effectively;

4) It proposes a multi-perspective result validation tech-
nique that can effectively reduce the false alarm rate
and improve the detection rate of unsupervised CDN
anomaly detection models.

This paper is organized as follows: Section II provides
an overview of CDNs and potential cyber-attacks. Section
III presents the related works regarding network anomaly
detection. Section IV discusses the proposed anomaly detec-
tion model in detail, including the model framework, feature
engineering, utilized algorithms, and validation procedures.
Section V presents and discusses the experimental results.
Section VI discusses the open issues and practical usage of
the proposed framework. Section VI concludes the paper.

II. PROBLEM STATEMENT

A. CDN Overview

In traditional Internet, the same contents are required to
be transmitted from servers to clients repeatedly. With the
rapidly increasing demand for large-scale content distribution,
content networks, or caching networks, have been developed
to improve content delivery efﬁciency [17] [18]. In content
networks, contents can be cached in servers to serve future
requests [19]. As a common type of content network and
an effective solution for large-scale content delivery, content
delivery networks (CDNs) have been widely deployed in mod-
ern networks [19]. As the most important strategy of caching
networks and CDNs, caching is the process of storing copies of

content in temporary storage locations or named caches, which
can largely reduce latency, enabling fast access to websites
or applications [1] [18]. Through caching mechanisms, CDNs
can cache content in edge servers that are closer to end-users
than the central server, making it more efﬁcient to deliver web
contents [2]. An overview of CDN is shown in Fig. 1.

CDN works in the following procedures [2]:

1) When a CDN receives a request for a content from a
client, this request is routed to the closest edge server
to the client.

2) The closest edge server will fetch the content from the

central server that has this content.

3) The edge server responds to the client with the requested

content.

4) A copy of the requested content is stored in the edge
server as the caching process for future requests. The
cached content will be retained in the cache space if the
end-users keep requesting the same content.

If the content requested by an end-user has been saved in
the cache space of the edge server, the content will be loaded
at a fast speed, so-called a cache hit. In contrast, if the content
has not been saved in the cache space, a cache miss will occur,
and the edge server will pass the request to the central server
to fetch the content and save it in the cache space to reduce
the future request processing speed. Thus, CDNs can handle
the rapidly growing volumes of network trafﬁc and Internet
content with low latency through edge servers and caching
mechanisms. Moreover, CDNs can mitigate DDoS attacks
because the trafﬁc can be dispersed to multiple edge servers
to keep responding to users’ requests, making it difﬁcult for
cyber-attackers to paralyze the entire network [20].

On the other hand, the research works for CDN anomaly
detection are very limited, because CDN operators (e.g.,
Akamai, Limelight) often keep CDN trafﬁc data private
due to liability implications [21]. Most research works of
anomaly detection in caching networks are for information-
centric networks (ICNs) [22]. Information-Centric Networking
(ICN), alternatively known as Named Data Networking (NDN)
or Content-Centric Networking (CCN), is a future Internet
architecture that can be regarded as an improved version
of CDNs to provide large-scale content delivery with less
resource footprint and system complexity [19] [23]. Caching
is an essential component in both ICNs and CDNs, since
they both aim to provide efﬁcient content delivery through
caching mechanisms [1] [19]. Thus, ICNs and CDNs are both
vulnerable to the cyber-attacks that aim to disrupt caching
services, like DoS attacks and CPAs. The main difference
between ICNs and CDNs is that ICNs assign a unique name
for each content as its identiﬁer to replace IP addresses for
content delivery, while IP address information is required in
CDNs [19] [24].

B. Service Targeting Attacks in CDN

However, the use of caching mechanisms introduces several
vulnerabilities to caching networks, especially CDNs [18].
Several service targeting attacks can be carried out on CDNs

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

3

Fig. 1. An overview of CDN and cyber-attack scenarios.

to disrupt their services by exploiting the caching mechanisms
of CDNs.

Firstly, if a cache misses occurs, even though the content
can be an unpopular or illegitimate ﬁle, the edge server close
to the end-user will fetch this content from the central server
and store it in the cache space. However, due to the memory
constraints of server machines, a cache can only save a limited
number of contents [25]. Based on this caching strategy, cyber-
attackers can send a large or moderate number of requests for
unpopular contents to occupy the cache space of certain edge
servers. This attack is called a cache pollution attack (CPA)
that aims to pollute the cache space of certain edge servers
[17]. CPAs are one of the most severe threats on emerging
content networks, including ICNs and CDNs, because the
caching mechanisms have made it easier for cyber-attackers
to corrupt cache by requesting unpopular or invalid contents
[6] [23]. The performance and services of caching networks
can be signiﬁcantly degraded by CPAs. After being attacked
by CPAs,
the cache of compromised nodes is ﬁlled with
unpopular contents, resulting in a high cache miss rate and
latency for popular contents requested by legitimate users [17].
Due to the latency issues caused by CPAs, CPAs are especially
damaging for the transmission of latency-sensitive contents,
like live-streaming contents [26]. A CPA scenario for CDNs
is shown in Fig. 1.

CPAs can be classiﬁed as locality-disruption attacks (LDAs)
and false-locality attacks (FLAs) based on their behaviors [5].
LDAs are launched by sending a moderate number of requests
for a large number of low-popularity contents to occupy the
cache space that should belong to popular contents, therefore
degrading the locality of cache ﬁles and cache efﬁciency. On
the other hand, FLAs are launched by repeatedly sending a
large number of requests for a few targeted low-popularity
contents to constantly refresh these polluted contents and
occupy certain areas of the cache space, thereby degrading
the cache hit rate of legitimate requests.

Secondly, since CDNs respond to every incoming request
with Internet content in chronological order, cyber-attackers
can send a sudden burst of requests to certain nodes to
overwhelm these edge servers, named denial of service (DoS)

attacks [7]. DoS attacks are the most common attack that can
stop the entire network from functioning shortly or indeﬁnitely
to prevent legitimate clients from accessing content [7].

Unlike ICNs that are vulnerable to a special type of DoS
attack, named Interest ﬂooding attacks (IFAs), due to its
Pending Interest Table (PIT) strategy for content caching,
common DoS attacks launched in CDNs are conventional
Hypertext Transfer Protocol (HTTP) ﬂooding attacks [27].
These DoS attacks intend to exhaust the network and memory
resources of CDN nodes, so that the requests from legitimate
users may be delayed or their requested contents cannot be
cached by these affected nodes, causing cache misses or late
responses [20]. A DoS scenario for CDNs is shown in Fig. 1.
As a special case of DoS attacks, distributed DoS (DDoS)
attacks are carried out by utilizing multiple compromised
devices, named bots,
to send a sudden burst of requests
together to certain nodes, while DoS attacks can be launched
by a single machine. It is more difﬁcult to detect DDoS attacks
than DoS attacks since they are carried out from multiple
locations instead of a single origin [3] [7].

As CPAs and DoS attacks are two common types of
service targeting attacks that pose severe threats to CDNs to
make caching mechanisms, the core of CDNs, unavailable to
legitimate users, the major purpose of this work is to protect
CDNs against these two attacks.

C. Anomaly Detection in CDN

CDNs themselves can mitigate DoS and DDoS attacks,
since they use highly distributed edge servers that can disperse
the trafﬁc to avoid the entire system breakdown [20]. However,
latency will still be increased due to the unavailability of edge
nodes caused by attacks. Additionally, if DoS attacks cannot be
detected and compromised nodes cannot recover, more CDN
edge servers and clients will be affected by the attacks, and the
entire system will fail eventually. Certain conventional mecha-
nisms, like ﬁrewalls and ﬁltering techniques, can mitigate DoS
attacks by limiting the amount of trafﬁc entering the CDN.
However, DoS attacks and crowd events have similar patterns,
making it difﬁcult for conventional mechanisms to ﬁlter only
the attacks [3]. Thus, there is a need to develop an anomaly

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

4

detection system to distinguish them. On the other hand,
compared to DoS attacks, CPAs are more ﬂexible, stealthy,
and challenging. CPAs can be launched by sending a moderate
number of requests for certain unpopular contents to retain
them in the cache through the normal content delivery process,
making it difﬁcult for conventional mechanisms to defend
against CPAs. This emphasizes the importance of anomaly
detection system development.

CDNs usually have ﬁrewalls and authentication mechanisms
as their ﬁrst layer of defense. Anomaly detection systems can
be incorporated into CDNs as the second layer of defense to
identify attacks that have breached the ﬁrst layer of defense.
Identifying those attacks is crucial because they may be more
malicious than the attacks that are blocked by the ﬁrst layer
of defense [28]. Additionally, anomaly detection systems can
adapt to the changing patterns of cyber-attacks by analyzing
the continuously-generated CDN log data that reﬂect
the
current network states. Thus, the work aims to propose an
anomaly detection system to detect service targeting attacks
(i.e., CPAs and DoS attacks) and abnormal network entities
from multiple perspectives to secure CDNs.

has high overhead costs. On the other hand, the above CPA
detection methods are all based on threshold mechanisms, but
they lack adaptability to complex and changeable network
environments.

Many recent research works have considered DoS attack
prevention and detection. Rahman et al. [7] proposed a dis-
tributed virtual honeypot method to mitigate DoS attacks in
CDNs. This method can maintain smooth content delivery
in CDN edge servers, but cannot protect the main server.
Moubayed et al. [33] [34] proposed an ensemble learning
classiﬁer to effectively detect several types of domain name
system (DNS) attacks, including cache poisoning and DoS
attacks. The proposed method achieves high accuracy but does
not analyze the detection results with related features to sum-
marize attack patterns for future anomaly detection. Kumar
et al. [35] [36] proposed a security framework based on the
software-deﬁned perimeter (SDP) to protect modern networks
from being breached by DoS attacks. Certain DoS attacks can
be effectively defended or prevented by the proposed method,
but it lacks the capacity to detect the malicious attacks that
have already breached the networks.

III. RELATED WORK

A. CPA & DoS Attack Detection

Service targeting attacks, including CPAs and DoS attacks,
pose a severe threat to caching networks, such as CDNs and
ICNs [5] [18]. As the leading CDN service provider, Akamai
Technologies reported in 2019 that more than 800 types of
DoS attacks were found in ﬁnancial services industries [29].
These DoS attacks can exploit vulnerabilities towards the
websites and disturb the ﬁnancial services, causing severe
ﬁnancial losses. Due to the lack of serious scrutiny for cached
contents, real-world CDNs are vulnerable to multiple caching-
related attacks, like CPAs, which have been recorded in the
“Vulnerability Notes Database” [30]. Moreover, Nguyen et al.
[25] proposed a new caching attack, Cache-Poisoned Denial-
of-Service (CPDoS), that targets CDNs and other vulnerable
caching systems. CPDoS attacks combine the ideas of CPA
and DoS attacks by sending a large number of requests with
malicious headers to pollute cache space and paralyze victim
websites.

Several research works have focused on CPA detection.
Conti et al. [22] conducted experiments to prove that CPAs
are a realistic threat to caching networks and proposed a
lightweight detection technique to detect CPAs accurately.
However,
the experiments were conducted on a simulated
network instead of a real-world network. Xie et al. [31]
proposed a novel method named CacheShield to avoid storing
the low-popularity contents (less than the popularity threshold)
in servers’ caches. However,
this strategy can also reject
certain legitimate contents, and is inefﬁcient for gradually
enhanced attacks. Karami et al. [32] proposed an Adaptive
Neuro-Fuzzy Inference System (ANFIS) to mitigate CPAs
in ICNs. In ANFIS, every content will be given a grade
to measure its goodness, and the system will replace low-
grade contents in caches with high-grade contents based on
a goodness threshold to mitigate cache pollution. However, it

B. Abnormal IP & Node Detection

Detecting abnormal network entities, including abnormal
client IP addresses and nodes, is important for CDN protection.
Several recent works have extracted network features from the
client IP point of view for anomaly detection. Lee et al. [27]
proposed CDN request routing and site allocation algorithms
to distinguish between the requests from DoS attackers and
legitimate users in CDNs, but they did not consider other
attack types. Chiba et al. [37] proposed a novel method that
can effectively extract features from the structures of IP ad-
dresses and applied the support vector machine (SVM) model
to detect malicious websites. Pinto et al. [38] presented and
utilized SVM on the network trafﬁc data to identify malicious
IP addresses and achieved the cross-validation accuracy of
83% to 95%. Fiadino et al. [39] used the HTTP ﬂow data
collected from a primary European Internet service provider
to detect trafﬁc anomalies in CDNs. However, only detecting
numerically abnormal trafﬁc is insufﬁcient to identify cyber-
attacks accurately.

Detecting compromised nodes is also a critical process to
ensure a quick recovery and reliable functioning of networks.
La et al. [40] proposed a misbehavior node detection algorithm
using a weighted-link method to secure a hierarchical sensor
network. Berjab et al. [41] proposed a novel framework based
on observed spatiotemporal (ST) and multivariate-attribute
(MVA) sensor correlations to detect abnormal nodes in wire-
less sensor networks (WSNs). Pandey et al. [42] proposed an
innovative method that uses intrusion detection system (IDS)
agents to identify compromised nodes in WSNs according
to their behavior, and the proposed method shows efﬁciency
in small networks. However, the above methods lack a root
cause analysis to ﬁnd what caused the abnormal nodes for the
purpose of future intrusion prevention.

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

5

C. Research Contributions

Many of the research works presented in this section are
promising and have achieved good outcomes. However, most
of them use software like the network simulator version 3 (NS-
3) to build a simulated network for data collection, which may
be biased or noisy. In our work, a recent real-world CDN
access log dataset collected from a major operator is used
to show the effectiveness of applying our proposed anomaly
detection model to real-world networks.

On the other hand, most existing research works only detect
anomalies from a single perspective, like the client IP or
the service provider perspective. This is often insufﬁcient to
validate whether the detected abnormal network behavior is
due to malicious cyber-attacks or legitimate network events.
Thus, many false alarms may be returned. Caltagirone et
al. [43] proposed a pivoting intrusion analysis model named
“diamond” that uses the communication information between
compromised nodes and malicious IP addresses to reveal the
detail of attackers, but it is only a theoretical model. On
the other hand, our proposed method extracts more CDN at-
tributes/features and conducts anomaly detection from several
different perspectives, and then performs a multi-perspective
analysis to validate anomaly detection results for the purpose
of false alarm reduction.

Moreover, many recent research works treat ML models
as black-box methods and do not analyze how the detected
anomalies and corresponding features can reﬂect a speciﬁc
type of cyber-attack. In our proposed method, the behaviors
and characteristics of CPA and DoS attacks from multiple
perspectives are summarized and analyzed together with the
ML-based anomaly detection results to perform a root cause
analysis and ﬁnd which type of cyber-attack or event causes
the anomalies. Ultimately, both the abnormal network entities
and their corresponding cyber-attack types will be detected
effectively.

IV. PROPOSED ANOMALY DETECTION FRAMEWORK

A. System Overview and Deployment

The proposed method aims to characterize abnormal events
and separate them from normal network events based on the
analytics of CDN access log data from different perspec-
tives, including content, client IP, account-offering, and node
perspectives. Fig. 2 depicts the framework of the proposed
anomaly detection model. The overall architecture of the
proposed system is divided into four parts: data pre-processing,
feature engineering, anomaly detection, and data labeling.
At the ﬁrst stage, the raw access log data is pre-processed
and cleaned to generate a sanitized dataset. A comprehensive
feature engineering method is then implemented to extract the
datasets that can effectively reﬂect the behaviors of network
attacks from different perspectives. Next, the extracted datasets
are trained by an optimized unsupervised anomaly detection
model based on GMM, iForest, and BO to discern between
abnormal and normal data patterns. At the last stage, a multi-
perspective validation analysis is conducted to reduce the
errors in the anomaly detection results. Ultimately, abnormal
network entities, including malicious IPs, abnormal contents,

and compromised nodes, as well as their corresponding cyber-
attack types, can be effectively detected to secure CDNs.

For the deployment in CDNs, the proposed anomaly detec-
tion system can be placed in both the central server and edge
servers, as shown in Fig. 3. In edge servers, the proposed
system can keep monitoring the network trafﬁc to detect
abnormal network entities and send alarms to the central server
as soon as an attack occurs; hence, the central server can notice
other edge servers and make corresponding countermeasures.
When placed in the central server, the proposed system can
have a comprehensive view of the operation of the entire
network, and can protect the central server when certain edge
servers have been exploited by attackers to breach the central
server. Speciﬁcally,
the
network trafﬁc that is not stopped by the ﬁrst layer of defense
(e.g., ﬁrewalls) will be captured by network taps or sniffers,
and then analyzed by the proposed anomaly detection system
[44]. The large trafﬁc can also be stored in a database for
comprehensive analysis by the proposed anomaly detection
system. If an attack is detected in an edge or cloud server,
all
the edge and cloud servers will receive an alarm. As
such, the network administrators in the central server and
edge servers can make corresponding countermeasures to stop
current attacks and prevent future attacks.

in each edge or cloud server, all

V. PROBLEM STATEMENT

A. Data Acquisition and Preprocessing

For the purpose of network anomaly detection, data acqui-
sition is the ﬁrst phase of any ML-based model framework.
Due to the complexity of modern network conﬁgurations,
numerous network ﬁelds are often recorded in network log
data to reﬂect network states and characteristics. For instance,
the authors in [45] provided 248 unique network features
that can be collected from the network packets transmitted
between clients and servers. However, considering hundreds of
network features for anomaly detection is often unrealistic due
to limited budgets and resources. Additionally, a large number
of features that include many irrelevant features may introduce
noises, which have an adverse impact on the performance of
ML models.

In this paper, we focus on anomaly detection in CDN
access logs. In network communications, access logs record
the information of all requests for the contents that users
have requested from web servers. The information includes
client IP addresses, timestamps, protocol, user-agent informa-
tion, uniform resource locator (URL) of content, etc. [46].
Analyzing access logs can help us recognize the states and
behaviors of a network in various time periods. Access logs
are often recorded in a combined log format that contains
multiple network ﬁelds for each request, as described in [46].
The following is an example of a record in general access
logs:

127.0.0.1

-
-
/support.html

[12/Dec/2016:04:54:20
HTTP/1.1”

200

“GET
“Mozilla/5.0(compatible;Googlebot/2.1;+http://www.g
oogle.com/bot.html)” . . .

11179

-4000]
-

The access log data used in our paper is a general network
access log acquired from real-world web servers. In this 169

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

6

Fig. 2. The framework of the proposed anomaly detection approach.

TABLE I
DESCRIPTION OF SELECTED NETWORK LOG ATTRIBUTES

Feature
IP
Timestamp
HTTP
method
Status code

Bytes
Delivery
time
Service type

Cache hit
indicator
Node
Account
offering
Content-URL
Content type

Description
Client IP address
Request start time in format: “[dd/mmm/yyyy:hh:mm:ss -zzzz]”
HTTP request method, e.g., GET, POST, etc.

HTTP status code: 2xx indicates a successful response; 3xx indi-
cates a redirection; 4xx indicates a client error; 5xx indicates a
server error
Bytes returned over the network without headers after a request
Duration from the beginning until the end of a request and all bytes
are delivered, in milliseconds
The type of service, e.g., static, live streaming, progressive down-
load, etc.
Service cache hit/miss indicator, hit or miss

Node name, representing a service provider
The network account offering name (e.g., streaming-1, static-2,
etc.), indicating a service accessed from the IP space
The content part of a URL, indicating unique content
The type of the content in each request, e.g., image, video, audio,
text, etc.

protocol, network name, referrer string, are removed from the
feature set, because they have either the same values or empty
values for almost all the requests. To ensure accurate feature
selection, HITL is used for feature validation. Thus,
this
preliminary feature selection process is validated by multiple
cybersecurity experts and industrial partner security network
engineers. The experts are from multiple organizations, includ-
ing third and disinterested parties. All the experts involved in
this work can guarantee their professionalism, objectivity, and
impartiality in the analysis procedures.

After obtaining the reduced raw data, several steps are

completed to clean and pre-process the data.

In the ﬁrst step, apparent noise or error data, mainly the
request samples with many empty ﬁelds, are discarded. Then,
certain features, like the timestamp, are formatted for easier

Fig. 3. The deployment of the proposed anomaly detection system.

GB access log data provided by the CDN operator, there
are more than 0.4 billion requests/samples and 30 ﬁelds that
characterize the behavior of each request. This CDN log
data contains data samples with various characteristics and
probability distributions, enabling the detection of different
types of cyber-attacks.

Although 30 network ﬁelds are available in the CDN access
logs, only part of them are useful for anomaly detection, and
other ﬁelds are irrelevant or for other uses. After preliminary
analysis on the CDN dataset and potential abnormal behaviors,
12 ﬁelds that might be helpful for anomaly detection are se-
lected, including IP address, timestamp, HTTP method, status
code, bytes returned, request delivery time, service type, cache
hit indicator, node name, account-offering, content-URL, and
content type. These features are also the standard access log
features described in [46]. The description of the preliminarily
selected network ﬁelds is shown in Table I. Other ﬁelds that
do not have a direct impact on cyber-attack detection, such as

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

7

comparison or calculation. Moreover, string features, like the
service category (dynamic or static) and cache hit indicator
(hit or miss), are converted to binary or numerical features for
easier calculation.

Data cleaning is then followed by data normalization. Data
normalization is beneﬁcial when features have different ranges
since features with larger ranges are often considered more
important than smaller range features in ML model training,
which can cause misleading results. Therefore, the datasets
are normalized by the min-max normalization method to be
in the range of 0 to 1 [28]. The normalized value of each
feature value, xn, is denoted by,

xn =

x − min
max − min

,

(1)

where x is the original feature value, min and max are the
minimum and maximum values of each original feature.

Min-max normalization is chosen in the proposed frame-

work due to two main reasons [47]:

1) Min-max scaling can transform all the features to have
the same range of [0, 1], making it easier for ML models
to process the dataset and for people to compare the
results; while many other normalization methods cannot
obtain the exact same range of features;

2) Compared to other methods, like standard scaler and
power transformer, the min-max scaler is more sensitive
to outliers, enabling the ML algorithms to detect anoma-
lies more accurately, as the purpose of this work is to
identify anomalies and attacks.

The use of min-max normalization can improve the accuracy
of ML models and reduce the difﬁculty of result analysis.
Nevertheless, the performance of using different normalization
methods is often very similar [47]. Additionally, as normal-
ization is a small part of ML pipelines, it does not have
a signiﬁcant impact on the ﬁnal anomaly detection results.
Other procedures, including feature engineering, unsupervised
learning model development, and multi-perspective validation,
are more important.

B. Feature Engineering

Although there are 12 features for each request in the initial
dataset, it is difﬁcult to identify abnormal network entities and
cyber-attacks using only the original request dataset since a
single request is often insufﬁcient to reﬂect network anomalies
or attacks. Thus, other features that can effectively reﬂect
abnormal network behaviors should be extracted or generated
for the purpose of anomaly detection.

Feature engineering aims to obtain useful features from
the log traces collected from CDN deployment based on
domain knowledge [3]. In this paper, a multi-perspective fea-
ture engineering method that extracts features from four main
perspectives: content, client IP, service provider (node), and
account-offering, is proposed to obtain the dedicated datasets
for different purposes. The generated datasets extracted from
different perspectives can be processed separately and then an-
alyzed together to detect abnormal network events more accu-
rately than any single perspective. In the proposed framework,

detecting malicious client IPs and compromised nodes is the
main objective; the information collected from the content and
account-offering perspectives is the supporting information for
more accurate abnormal IP and node detection. To detect CPAs
and DoS attacks, their main characteristics reﬂected from the
four considered perspectives are summarized below.

The content perspective is to monitor the properties of the
requested and cached contents. From the content perspective,
it is beneﬁcial to identify the abnormal contents that might
be used by attackers to launch cyber-attacks. The abnormal
trafﬁc that characterizes a CPA is a sudden burst or a periodic
sending of requests for low popularity contents. Additionally,
most CPAs are used to breach a few targeted nodes, so for
each abnormal content, the number of requests sent to each
targeted node should also be large. On the other hand, if a
content gets a large number of requests from many different
IPs, there might be a crowd event or a distributed denial of
service (DDoS) attack.

The service provider perspective is meant to monitor the
edge servers or nodes that receive requests and transmit
contents through CDNs. The cache hit rates and data trans-
fer rates of compromised nodes are often degraded due
to uncached legitimate contents and network unavailabil-
ity/congestion caused by CPAs or DoS attacks. The average
content popularity of each compromised node is also reduced
by CPAs since the node’s cache space will be occupied by
unpopular ﬁles. The affected nodes should be identiﬁed and
isolated as soon as an intrusion occurs so that other legitimate
nodes can avoid communicating with the compromised nodes
until recovery.

The client IP perspective is meant to monitor clients that
send requests for the contents provided by servers. Consid-
ering the client IP perspective enables us to detect potential
malicious clients, allowing us to stop or prevent cyber-attacks
by blocking the requests from these malicious IP addresses.
Detecting abnormal client IPs is crucial because these IP
addresses represent the origins of cyber-attackers. Attackers
that are launching different types of attacks exhibit differ-
ent behaviors. For CPA attackers, they send high-frequency
requests for low popularity contents, while DoS attackers
may send a sudden burst of requests for ﬁles with any
popularity. Additionally, for the two types of CPA attackers,
LDA attackers send requests for a large number of unpopular
contents, while FLA attackers only send requests for a few
targeted contents, but the number of requests sent for each
content (request per content ratio) is often large.

The account-offering (AO) perspective indicates the stream-
ing source that provides a speciﬁc type of service, such as
static, live streaming, and progressive download content dis-
tribution. An AO’s conﬁguration also determines the behavior
of its serviced client IPs. If certain IPs behave abnormally
and do not match the AO conﬁguration patterns, they have a
high probability of being anomalies. Thus, the analysis from
the AO perspective can help us validate whether the anomaly
detection results are real attacks or false alarms.

The extracted features from four perspectives are summa-
rized in Tables II - V. After attack pattern analysis in Section
II-B, we found that only some of the features have a direct

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

8

TABLE II
DESCRIPTION OF EXTRACTED FEATURES FROM THE CONTENT
PERSPECTIVE

Perspective

Content

Feature
Number of
requests
Popularity

Cache hit
rate
Request per
IP ratio

Request per
node ratio

Description
The total number of requests for per content

The popularity of per content, represented
by the normalized number of IPs that sent
requests to per content
The number of cache hits divided by the total
number of requests for per content
The ratio of the total number of requests sent
for per content to the total number of IPs
which sent requests for per content
The ratio of the total number of requests sent
for per content to the total number of nodes
which received requests for per content

impact on the detection of CPAs and DoS attacks. Considering
more features may cause misleading results or additional
computational time. Thus, the most important features that can
directly reﬂect the attack patterns are selected in the proposed
method, based on the potential feature patterns/characteristics
of CPAs and DoS attacks summarized in Tables VI and VII.
The selected features can reﬂect most scenarios of a CDN that
is under CPA or DoS attacks.

Considering the original features in Table I, except for the
HTTP method, all other raw features in Table I have been
used to create the ﬁnal features in Tables V - VII. Speciﬁcally,
“IP”, “Node”, and “Content-URL” are used to extract datasets
from the IP, node, and content perspectives, “Timestamp” is
used to calculate the average request interval of client IPs,
“Status code” is used to calculate the request error rate of
nodes, “Bytes” and “Delivery time” are used to calculate the
data transfer rate, “Cache hit indicator” is used to calculate the
cache hit rate of IPs and nodes, “Account-offering”, “Service
type”, and “Content type” are used to determine the behaviors
of account offerings for result validation purposes. Therefore,
the ﬁelds selected by the cybersecurity experts do not include
any noise. “HTTP method” is removed because DoS and CPAs
can be launched using any HTTP method, whether they use
GET, POST, or any other HTTP methods; hence, it is irrelevant
for DoS & CPA detection. Although the “HTTP method” is
irrelevant for CPA & DoS attack detection, it can still be used
for other tasks, like the detection of other types of attacks.

This feature extraction and selection process can be au-
tomated as a general feature engineering method by sum-
marizing the potential patterns of certain types of cyber-
attacks and selecting the features that can reﬂect the attack
patterns, as discussed in this subsection. Through this process,
the proposed multi-perspective feature engineering method
can extract and select the core features to reﬂect CPA and
DoS attacks for the anomaly detection model development
presented in the next subsection.

C. Unsupervised Anomaly Detection

1) Compromised Node Detection: The extracted node-
based dataset contains the information about 50 different
nodes. To detect abnormal or compromised nodes, isolation
forest (iForest) [13], an unsupervised outlier detection algo-
rithm that aims to separate isolated data samples (anomalies)

TABLE III
DESCRIPTION OF EXTRACTED FEATURES FROM THE NODE PERSPECTIVE

Perspective

Node

Feature
Cache hit
rate
Cache hit
rate of
legitimate
IPs
Data
transfer rate
(MB/s)
Request
error rate
Average
request
popularity
Account-
offering
request rate

Description
The number of cache hits divided by the total
number of requests received by per node
Average cache hit rate of IPs which only
requested for popular contents on per node

The total bytes returned divided by the total
delivery time for per node

requests with errors
The percentage of
(4xx/5xx status code) received by per node
Average content popularity of requests re-
ceived by per node

The percentage of requests sent
through
per account-offering for per node, e.g., “ac-
count1: 80%, account2: 20%”

TABLE IV
DESCRIPTION OF EXTRACTED FEATURES FROM THE CLIENT IP
PERSPECTIVE

Perspective

Client IP

Feature
Number of
requests
Average
request
interval
Number of
nodes
Number of
contents
Request per
content
ratio
Request per
node ratio

Average
request
popularity
Cache hit
rate
Request
error rate
Account-
offering
request rate

Description
The total number of requests sent by per IP

The average time interval between consecu-
tive requests sent by per IP

The total number of unique nodes that re-
ceived requests from per IP
The total number of unique contents re-
quested by per IP
The ratio of the total number of requests sent
by per IP to the total number of contents
requested by per IP
The ratio of the total number of requests sent
by per IP to the total number of nodes which
received requests from per IP
Average content popularity of requests sent
by per IP

The number of cache hits divided by the total
number of requests sent by per IP
The percentage of
(4xx/5xx status code) sent by per IP
The percentage of requests are sent through
per account-offering for per IP, e.g., “ac-
count1: 80%, account2: 20%”

requests with errors

TABLE V
DESCRIPTION OF EXTRACTED FEATURES FROM THE ACCOUNT-OFFERING
PERSPECTIVE

Perspective

Account-
offering
(AO)

Feature
Number of
requests
Number of
nodes
Service
type

Content
type
Cache hit
rate
Request
popularity

Description
The total number of requests through per AO

The total number of unique nodes that re-
ceived requests sent through per AO
The type of service provided by per AO, e.g.,
static, live streaming, progressive download,
etc.
The type of content provided by per AO,
e.g., image, video, audio, text, etc.
The number of cache hits divided by the total
number of requests sent through per AO
Average content popularity of requests sent
through per AO

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

9

TABLE VI
POTENTIAL PATTERNS OF CPAS

Attack
Type

Perspective

Feature

Node

CPA

Client IP

Content

AO

Cache hit rate
Cache hit rate of legitimate IPs
Data transfer rate (MB/s)
Average request popularity
Number of requests
Average request interval
Number of nodes
Request per content ratio

Average request popularity
Popularity
Request per IP ratio

Request per node ratio
Request popularity

Abnormal
Patterns
Low
Low
Low
Low
Large
Short
Small
LDA: Low
FLA: High
Low
Low
FLA: High
LDA: Low
High
Low

TABLE VII
POTENTIAL PATTERNS OF DOS ATTACKS

Attack
Type

DoS

Perspective

Feature

Node

Client IP

AO

Cache hit rate
Cache hit rate of legitimate IPs
Data transfer rate (MB/s)
Request error rate
Number of requests
Average request interval
Number of nodes
Cache hit rate
Request error rate
Cache hit rate

Abnormal
Patterns
Low
Low
Low
High
Large
Short
Small
Low
High
Low

from normal samples, is utilized in the proposed framework.
The proposed abnormal node detection method has two main
steps:

1) Use iForest, an outlier detection algorithm, to separate
numerically abnormal samples from normal samples.
2) Analyze the behaviors of each numerically abnormal
node, and label the nodes that match the summarized
patterns/characteristics of different types of attacks.
IForest is an ensemble learning algorithm constructed with
multiple binary search trees, named isolation trees (iTrees)
[48]. Each iTree is constructed by splitting the samples based
on feature values. The number of splittings required to isolate
a sample indicates its path length (i.e., the number of edges
from an iTree’s root node to its leaf node). The path length
of anomalies is often shorter than normal samples. This is
because normal samples are often the majority and in dense
areas, making it unlikely for an iTree to isolate them from
each other, while anomalies are the opposite. A score is given
to each sample based on the path length, so that the outliers
that are sparsely distributed and distant from the dense normal
samples can be detected [48].

IForest is chosen for abnormal node detection due to the

following reasons [13] [48]:

1) Unlike many other ML algorithms, iForest performs well
on small-scale data to which the node-based dataset
belong. This is because iForest uses the short path length
of data samples to indicate the anomalies, which can be
obtained regardless of data size.

2) Unlike clustering algorithms, iForest does not require

anomalies to have similar characteristics to detect them
since it discerns outliers from normal samples based on
data density.

3) IForest

is computationally efﬁcient because it has a
linear time complexity of O(N ), a low memory require-
ment, and parallel execution support.

4) IForest has good interpretability since it uses a tree-
structure to make decisions and split data samples.
To develop an effective ML model for a speciﬁc task,
hyper-parameter tuning should be implemented to detect the
hyper-parameter conﬁguration that can return the optimal
architecture of the ML model [49]-[52]. As an important
hyper-parameter of iForest, the contamination level determines
the proportion of data samples that will be detected as outliers.
To build an optimized iForest model, the contamination level
is tuned using Bayesian optimization (BO).

BO algorithms are a set of efﬁcient hyper-parameter op-
timization (HPO) methods that detect
the optimal hyper-
parameter based on the currently-evaluated results [53]. In BO,
a surrogate model is used to ﬁt all the currently-tested samples
into the objective function; an acquisition function is then used
to locate the next points by considering both the unexplored
regions and currently-promising regions in the search space
[54].

Gaussian process (GP) is a common surrogate model for
BO. In GP surrogate models, any ﬁnite combination of the
random variables follows a Gaussian distribution [55]:

p(y|x, D) = N (cid:0)y|ˆµ, ˆσ2(cid:1) ,

(2)

where D is the hyper-parameter conﬁguration space, y = f (x)
is the objective function value for each hyper-parameter con-
ﬁguration with its mean as ˆµ and covariance as ˆσ2.

The BO method using GP (BO-GP) has a time complexity
of O(N 3) and a space complexity of O(N 2) [55]. BO-GP
is inefﬁcient for a large hyper-parameter search space but
exhibits great performance on optimizing a small number
of continuous or discrete hyper-parameters. Thus, BO-GP
is used to optimize the contamination level of the iForest
model. The silhouette coefﬁcient [56] [57], a distance-based
metric that can measure the similarity of normal samples and
the difference between numerically normal and anomalous
samples, is chosen to be the metric of the iForest model and
used as the objective function to be optimized by BO-GP.

After using the optimized iForest model to detect abnormal
nodes, many false positives will be returned since many of the
detected compromised nodes do not match the cyber-attack
patterns. Mainly, the cache hit rate of the compromised nodes
and their serviced legitimate client IPs should be reduced
due to attacks. The data transfer rate and average popularity
of the abnormal nodes should also be low due to network
congestions and the ﬁlled cache space occupied by unpopular
ﬁles, respectively. On the other hand, although certain other
isolated data points, like the nodes that received a very small
number of requests or returned a very high data transfer rate,
are numerically different from most normal nodes, they are
unlikely to be under CPA or DoS attacks. Therefore,
the
patterns of the detected anomalous nodes are compared with
the behaviors of potentially compromised nodes summarized

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

10

in Tables VI & VII. Ultimately, the affected nodes that match
the abnormal patterns are preliminarily labeled abnormal.

2) Abnormal Content & Client IP Detection: Unlike the
node-based dataset that only contains 50 unique nodes, both
the content-based datasets and IP-based datasets have more
than one million unique contents or unique IPs. For large-
scale data, only using a binary outlier detection method (e.g.,
iForest) is insufﬁcient
to identify attack samples, because
using only two categories cannot describe various normal and
abnormal patterns, and numerical anomalies may not be the
true attack samples since cyber-attacks have their own speciﬁc
patterns or characteristics, as described in Section IV-C.

For large-sized data, clustering algorithms would be more
effective in identifying abnormal contents and IP addresses
since they can obtain multiple numerically abnormal clusters,
enabling us to determine the true anomalies by comparing
the characteristics of the abnormal clusters with the speciﬁc
attack patterns. Clustering algorithms are a set of unsupervised
learning models that aim to group data points into different
clusters. Data samples in the same cluster should have similar
patterns or properties, while those in different clusters should
have different patterns [58].

The proposed abnormal IP and content detection method

has two main steps:

1) Use a clustering algorithm to group the client IPs or

contents into a sufﬁcient number of clusters.

2) Analyze the characteristics of each cluster, and label the
IPs or contents in this cluster as “normal” or “abnormal”
based on the summarized patterns of different types of
attacks.

Gaussian mixture model (GMM) is a distribution-based
clustering constructed with multiple Gaussian distribution
components and a probability density function [14]. GMM
models data points by utilizing Gaussian distribution models
with parameters estimated by the expectation-maximization
(EM) algorithms. In GMM, each Gaussian component can be
denoted by a multivariate Gaussian distribution [14]:

G(x | µ, Σ) =

1
2 |Σ| 1
(2π) D

2

e− 1

2 (x−µ)T Σ−1(x−µ)

(3)

where x is the data points, µ is the mean or the expectation
of the Gaussian distribution, Σ is the covariance, and D is the
dimensionality of the dataset.

A GMM with K Gaussian components models the data by

the following probability density function [14]:

p(x | θ) =

K
(cid:88)

i=1

πiG (x | µi, Σi)

(4)

where θ = {πi, µi, Σi} are the parameters of GMM, πi is

the weight of each Gaussian component, and (cid:80)K

i=1 πi = 1.

The GMM parameters are obtained by the EM algorithm
the E-step
that repeats two main steps until convergence:
calculates the expectation of each Gaussian component, and
the M-step maximizes the calculated expectations to update
the parameters of Gaussian distributions [14]. The time com-
plexity of training a GMM is O(N KD2) for N data instances,
K Gaussian components, and D features or dimensions [59].

The main reasons for choosing GMM for the content-based

and IP-based datasets are:

1) Based on the visualization of the probability density
functions, most of the extracted features on the content
and client IP sides follow Gaussian or near-Gaussian
distributions. Additionally, GMM considers feature co-
variance and can model more ﬂexible cluster shapes than
many other clustering methods, like k-means, which can
only return globular cluster shapes. Therefore, GMM can
ﬁt the datasets effectively.

2) Unlike many other clustering algorithms, like k-means
and hierarchical clustering, which can only return a
cluster label or identity number, GMM is able to give
a conﬁdence value to each test sample, indicating the
probability of belonging to each cluster. The probability
can be used to ﬁnd uncertain samples and take further
actions to reduce errors.

is lower than GMM,

3) Although k-means has a training time complexity of
O(N KD) [60] that
the train-
ing time of GMM is still low because the proposed
feature engineering method has effectively reduced the
dimensionality of the data. On the other hand, the run-
time complexity of a GMM is also O(N KD), so the
execution time of running an trained GMM is low.

For the GMM applied to the CDN datasets,

it has a
major hyper-parameter that requires tuning, which is K, the
number of clusters or Gaussian components [55]. Identifying
an optimal value of K is crucial since it determines whether a
sufﬁcient number of Gaussian components are constructed to
describe and distinguish normal and abnormal data patterns.
On the other hand, a too-large K will lead to additional model
training time.

Since GMMs only have a discrete hyper-parameter, the
number of Gaussian components, that requires tuning in most
cases, BO-GP serves as an effective HPO method for GMMs
[55]. The silhouette coefﬁcient is also selected as the metric of
the GMM model since it measures how similar a data sample is
to other data samples within the same cluster and how different
a data sample is from the samples in other clusters.

After grouping the contents and IPs into optimized numbers
of clusters using GMM and BO-GP, the characteristics of each
cluster will be analyzed based on the DoS & CPA patterns, and
the clusters that match the abnormal content and IP patterns
summarized in Tables VI & VII are deemed to have passed
the initial detection. The IPs and contents in these clusters are
preliminarily labeled “abnormal” at this stage.

D. Multi-perspective Result Validation

Using unsupervised machine learning algorithms, including
GMM and iForest, enables us to distinguish numerically
abnormal content, nodes, and IPs from normal ones. However,
certain legitimate events, like misconﬁgurations and crowd
events, may perform similar behaviors as cyber-attacks and
be misclassiﬁed as anomalies. Therefore, a multi-perspective
result validation analysis is performed to eliminate the false
alarms and improve the detection rate, so as to identify the
real network entities affected by CPAs and DoS attacks.

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

11

1) Time-series Analysis: At the ﬁrst stage of result val-
idation, time series analysis is conducted by analyzing the
changes of certain features in periods (e.g., hourly and daily
changes) to ﬁnd the abnormal events and potential attacks.

The major feature changes to be monitored in general CDN

access log data for DoS & CPA detection include follows:

1) The changes in the hourly number of requests can
help us to ﬁnd potential crowd events and DoS attacks
when there is a sudden burst of requests in certain time
periods;

2) The changes in the hourly cache hit rate can help us to
detect potential DoS attacks when there is a sudden burst
of error requests that aim to exhaust network resources;
3) The changes in the hourly request popularity can be used
to identify potential CPAs when certain IPs start to send
a large number of requests for unpopular contents, or
certain nodes get many requests for unpopular contents.
The general process of time-series validation is shown
in Fig. 4. After we ﬁnd the time periods in which certain
features change abnormally, they will be compared with the
information about the known legitimate events provided by
the CDN operator. If known legitimate events did not occur in
these abnormal periods, there is a high probability that cyber-
attack occurred. Thus, the active IPs, nodes, and contents in
these abnormal periods will be analyzed using the proposed
optimized iForest or GMM methods to detect the abnormal
network entities affected in potential cyber-attacks. In most
real-world applications, legitimate event information should
be recorded for network maintenance purposes. In case of no
information about legitimate events, expert intervention can be
involved in the validation process as a HITL procedure to help
determine the real attacks. Moreover, network administrators
can still use authentication mechanisms to conﬁrm the identi-
ties of all these suspicious entities to identify real attacks.

The cross-perspective analysis is also utilized in the time-
series validation process to help validate the real abnormal
network entities. To perform the cross-perspective analysis, the
results from each perspective are used to validate the results
from the other two perspectives. To be speciﬁc, the information
about the detected compromised nodes can help us to validate
the potential malicious IPs which try to attack these nodes
and the potential abnormal contents which are used to pollute
these nodes. For the abnormal IPs and contents detected from
their own perspectives, these results can be used to validate
which nodes are targeted by the attackers who utilized these
abnormal IPs and contents. Through this process, the true
abnormal entities that are affected by attacks can be identiﬁed
effectively.

In conclusion, time series analysis enables us to locate the
speciﬁc days or time periods of potential attacks and legitimate
events (e.g., crowd events),
thus validating the results by
analyzing the nodes, IPs, and contents affected during these
periods.

2) Account-Offering Analysis: The account-offering (AO)
analysis is to analyze anomaly detection results based on the
AO conﬁgurations and behaviors to distinguish true attacks
from legitimate outliers. The potential behavior of an AO can
often be estimated based on its conﬁguration. If cyber-attacks

Fig. 4. The ﬂow chart of time-series analysis

occur,
the characteristics of the affected AOs can change
to abnormal, which can help us to validate the abnormal
network entities serviced by these AOs. For example, if an
AO that is conﬁgured to be the major stream of static content
services gets numerous requests for non-existent live streaming
contents, DoS attacks might be launched through this AO
to overwhelm certain nodes. On the other hand, if an AO
is used for the legitimate tests of old progressive download
videos, it may be misclassiﬁed as anomalies by the proposed
unsupervised models because, similar to CPAs, there will also
be a large number of requests for low-popularity contents;
thus, the affected nodes, IPs, and contents can be false alarms.
AO analysis requires the conﬁguration information of AOs to
determine whether the AOs’ behaviors match their conﬁgu-
rations. The general process of account-offering analysis is
shown in Fig. 5. Through this process, the false alarms from
legitimate events will be removed to improve the anomaly
detection accuracy.

After implementing the optimized unsupervised anomaly
detection method and performing the multi-perspective result
the abnormal contents, compromised nodes, and
analysis,

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

12

detection result validation. On the other hand, the dates of
crowd events are provided by the CDN operator, including
12th, 14th, and 15th, December 2016 (days 1, 3, and 4).
Additionally, the CDN operator has used many conventional
security mechanisms as the ﬁrst layer of defense, including
ﬁrewalls with ﬁltering mechanisms, authentication, hashing,
and load balancers, to mitigate cyber-attacks. Our proposed
anomaly detection system serves as the second layer of defense
to detect anomalies that are not stopped by the ﬁrst layer of
defense.

At the ﬁrst stage of the experiments, the anomaly detec-
tion model based on iForest and GMM is implemented on
the obtained content-based, client IP-based, and node-based
datasets individually to preliminarily detect abnormal contents,
malicious IPs, and compromised nodes. Once the numerically
abnormal network entities are separated from normal entities
by the proposed optimized unsupervised learning model, a
multi-perspective analysis is then performed to validate the
results as the second stage of the experiments. The validation
process includes the time-series analysis based on the known
legitimate events periods, and the account-offering analysis
based on the account-offering conﬁguration information and
their practical behaviors. As such, false alarms can be re-
duced, and real abnormal network entities can be identiﬁed
effectively.

B. Unsupervised Anomaly Detection Results

1) Compromised Node Detection Results: Based on the
extracted node-based datasets, there are 50 unique nodes that
have received requests in the 8 days dataset. Through the
optimized iForest method that returns the contamination level
of 0.22, as well as the comparison between node behaviors and
cyber-attack patterns, 11 nodes that have behaved abnormally
at least on one day are preliminarily identiﬁed as compromised
nodes, including the nodes number (No.) 0, 3, 4, 5, 7, 8, 9, 25,
36, 39, and 47. Among the detected abnormal nodes, nodes
No. 0, 5, 7, and 25 had a low cache hit rate (less than 0.5),
while other abnormal nodes had a relatively high request error
rate.

The mean value of each feature for normal and abnormal
nodes is shown in Table VIII. It is shown that the compromised
nodes have lower cache hit rates and data transfer rates while
having higher request error rates than the normal nodes,
which is due to potential network congestion and unavailability
caused by the attacks. Additionally, the cache hit rates of the
legitimate IPs serviced by the abnormal nodes are much lower
than the IPs serviced by the normal nodes. A multi-perspective
validation will be conducted at the next stage to evaluate the
detected abnormal nodes and reduce errors.

2) Abnormal Content Detection Results: 1,867,584 unique
contents have been requested in the 8 days dataset. A GMM
optimized by BO-GP is trained on the content-based dataset
to detect potential abnormal contents that might be used by
attackers to launch attacks. As the major hyper-parameter of
GMM, the optimal number of Gaussian components is found
to be 28, which returns the highest silhouette score of 0.96. As
shown in Table IX, the 169 low-popularity contents in cluster

Fig. 5. The ﬂow chart of account offering analysis

malicious IPs with their potential attack types can be identiﬁed
effectively.

VI. EXPERIMENTAL RESULTS & DISCUSSION

A. Experiments Setup

The experiments are conducted on a machine with a 6
Core i7-8700 processor and 16 GB of memory. The dataset
used for experiments is 169 GB of 8 days web access logs
collected by a major CDN operator from December 12th to
19th, 2016, including 452,264,816 unique requests/samples.
Through the proposed multi-perspective feature engineering,
we have obtained the IP-based dataset (1,268,027 unique IPs)
and the node-based dataset (50 unique nodes) for abnormal
IP and node detection. Additionally, the content-based dataset
(1,867,584 unique contents) and the account-offering dataset
(70 unique AOs) are also generated to support anomaly

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

13

TABLE VIII
MEAN VALUES OF EACH FEATURE OF NORMAL AND ABNORMAL NODES
IN PRELIMINARY ANOMALY DETECTION USING IFOREST

TABLE XI
AVERAGE FEATURE VALUES OF IP CLUSTERS IN PRELIMINARY CPA
DETECTION USING GMM

Node
Labels

Normal
Abnormal

Cache
hit
rate
0.886
0.286

Legitimate
IP cache
hit rate
0.928
0.299

Data
transfer
rate (MB/s)
0.696
0.374

Request
error
rate
0.003
0.052

Request
popularity

0.925
0.961

TABLE IX
AVERAGE FEATURE VALUES OF CONTENT CLUSTERS IN PRELIMINARY
ANOMALY DETECTION USING GMM

Content
cluster
No.
1
2
. . .
28

Avg
number of
requests
1.0
225.7
. . .
1021.4

Avg
request per
node ratio
1.0
8.2
. . .
601.2

Avg
request per
IP ratio
1.0
1.2
. . .
580.8

Avg
cache hit
rate
0.134
0.857
. . .
0.835

Avg
popularity

0.0
1.0
. . .
0.254

IP
cluster
No.

1
2
. . .
33
34

Avg
number
of
requests
1.3
200.4
. . .
4426.6
16028.4

Avg
number
of
nodes
1.14
28.4
. . .
1.90
5.18

Avg
request per
content
ratio
1.0
1.0
. . .
769.8
1.1

Avg
cache
hit
rate
0.057
0.699
. . .
0.856
0.077

Avg
request
error
rate
0.019
0.007
. . .
0.040
0.019

Avg
request
popularity

0.500
0.627
. . .
0.186
0.359

TABLE XII
MAIN FEATURE VALUES OF 250 CROWD EVENT IPS

Day

1
3
4

Avg
number of
requests
501600.6
459155.2
491830.9

Avg
number
of nodes
21.5
10.5
26.0

Avg
request
interval (s)
0.021
0.061
0.014

Avg
cache hit
rate
0.997
0.996
0.997

Avg
request
popularity
1.0
1.0
1.0

No. 28 have got a large number of requests on a couple of
target nodes. Therefore, these unpopular contents might have
been requested many times by CPA attackers to occupy the
cache space, making the legitimate and popular contents get
cache misses. Therefore, the 169 contents in cluster No. 28 are
preliminarily identiﬁed as potential abnormal contents. The
clusters No. 1-27 are classiﬁed as normal clusters based on
the comparison with cyber-attack patterns, and the behaviors
of the clusters No. 3-27 are omitted in Table IX.

3) Malicious Client

IP Detection Results: There are
1,268,027 unique IP addresses in the extracted IP-based
datasets. For abnormal IP detection, two GMMs are trained
on two different feature sets separately to detect CPAs and
DoS attacks, respectively.

For DoS attack detection, the major considered features are
the number of requests, requests per node ratio, and average
request interval. By using BO-GP to optimize the GMM, the
optimal number of clusters is found to be 47 with the highest
silhouette score (0.55). As shown in Table X, the 310 IPs in
cluster No. 47 are detected as potential DoS attack IPs, since
they have sent a large number of requests to several target
nodes at a very high frequency. Their request popularity is
very high (99.8%), so they are unlikely to be CPA IPs.

On the other hand, for CPA detection, the major considered
features are the number of requests, request per content ratio,
and request popularity. 34 clusters are found to be able to
effectively distinguish potential CPA IPs, returning an opti-
mized GMM with the silhouette score of 0.47. The behavior
of each cluster is compared with the FLA and LDA patterns

TABLE X
AVERAGE FEATURE VALUES OF IP CLUSTERS IN PRELIMINARY DOS
ATTACK DETECTION USING GMM

IP
cluster
No.

1
2
. . .
47

Avg
number
of
requests
4.2
116.4
. . .
405607.1

Avg
number
of
nodes
1.422
4.12
. . .
15.6

Avg
request
interval
(s)
1.44
365.8
. . .
0.673

Avg
cache
hit
rate
0.925
0.908
. . .
0.868

Avg
request
error
rate
0.0
0.011
. . .
0.124

Avg
request
popularity

0.976
0.957
. . .
0.998

summarized in Section IV-C. As shown in Table XI, the 21
IPs in cluster No. 33 are detected as the potential FLA IPs
since they have sent a large number of requests to a couple
of target nodes for several unpopular contents. Similarly, the
33 IPs in cluster No. 34 are detected as the potential LDA
IPs since they have sent a large number of total requests to
many unpopular contents to occupy the cache space. Thus, 54
unique IPs are preliminarily identiﬁed as potential CPA IPs.

C. Multi-perspective Results Validation

Although ML algorithms can identify the numerically ab-
normal IPs and nodes, many false alarms may occur, and some
real abnormal IPs and nodes may also be ignored. Thus, multi-
perspective validation is done based on the in-depth analysis
of the datasets, including time series analysis and account-
offering analysis.

1) Time Series Analysis: Since only the date information
about the crowd events is provided by the CDN operator, the
hourly number of requests is calculated to ﬁnd the speciﬁc
crowd event time periods, as shown in Fig. 6. According to
the hourly number of requests changes, the legitimate event
time periods are found to be 00:00-05:00 on day 1, 00:00-
02:00 on day 3, and 00:00-03:00 on day 4. By extracting and
analyzing the IP-based dataset in these time periods, 250 IPs
that have only sent requests in the crowd events have been
detected. The daily behavior of these 250 IPs is shown in
Table XII. Although they have sent a large number of requests
at a very high frequency, since they have only sent requests
in the crowd events, and other legitimate IPs in the crowd
events got more than 99% cache hit rates, these 250 IPs
should belong to legitimate IPs. After checking the 310 IPs
that are preliminarily detected as DoS attack IPs by the GMM
and described in the last subsection,
these 250 legitimate
event IPs are removed from the abnormal IP list and labeled
normal. Thus, 250 false alarms are removed, and the number
of potential DoS attack IPs is reduced to 60 after the ﬁrst stage
of time-series analysis.

At the next stage, in order to ﬁnd the potential cyber-attack
time periods, the hourly cache hit rate changes are calculated

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

14

TABLE XIII
MAIN FEATURE VALUES FOR COMPROMISED NODES UNDER POTENTIAL
DDOS ATTACKS

Node
No.

3
4
8
9
25
28
33
36
39
42
47

Cache
hit rate
on day 4
0.65
0.64
0.62
0.73
0.21
0.76
0.78
0.54
0.70
0.76
0.62

Avg cache
hit rate on
other days
0.70
0.74
0.78
0.79
-
0.89
0.86
0.75
0.75
0.87
0.85

Request
error rate
on day 4
0.11
0.02
0.10
0.03
0.61
0.08
0.04
0.13
0.04
0.09
0.19

Avg request
error rate on
other days
0.0009
0.0006
0.0008
0.0009
-
0.0014
0.0013
0.0014
0.0016
0.0023
0.0023

Number of
requests on
day 4
13,922
50,947
16,054
15,057
531,044
52,690
59,415
10,873
11,893
46,475
93,482

TABLE XIV
AVERAGE FEATURE VALUES OF IP CLUSTERS IN DDOS ATTACK
DETECTION USING GMM

Cluster
No.

1
2
. . .
10

Avg
number
of
requests
1.09
354.6
. . .
3039.9

Avg
number
of
nodes
1.08
3.78
. . .
2.39

Avg
request
interval
(s)
24.9
6.76
. . .
0.6

Avg
cache
hit
rate
0.850
0.579
. . .
0.026

Avg
request
error
rate
0.0
0.0001
. . .
0.972

Avg
request
popularity

0.941
0.810
. . .
0.947

requests during the abnormal period, but not as many as node
No. 25. As shown in Table XIII, node No. 25 had a very
low cache hit rate (0.21) and a very high request error rate
(0.61) on day 4. The other 10 compromised nodes’ cache hit
rates are also slightly reduced on day 4, and their request error
rates on day 4 are much higher than their error rates on other
days. By comparing the compromised nodes in this abnormal
event period with the abnormal nodes detected by iForest in
Section V-B, three more abnormal nodes missed by the iForest
but detected by the time-series validation process, nodes No.
28, 33, and 42, are added to the detected abnormal node list.
Although these three nodes’ cache hit rates are not very low
(more than 76%), they have also been affected by the potential
cyber-attacks on day 4. Thus, 14 unique nodes are labeled as
abnormal nodes at this stage.

After that, the 11,834 client IPs that have sent requests in
the abnormal event period are also analyzed. An optimized
GMM is utilized to cluster these 11,834 IPs into 10 clusters,
which has the highest silhouette score of 0.73. As shown in
Table XIV, among the 10 clusters, the 103 IPs in the cluster
No. 10 are found to be the malicious IPs since they have
sent a large number of 404-error requests to an average of
2.39 target nodes at a high frequency. Thus, these 103 IPs are
likely to have launched a distributed DoS (DDoS) attack in
the abnormal period 13:00-15:00, day 4. By comparing these
103 IPs with the 60 potential DoS attack IPs detected by the
optimized GMM and crowd event analysis, 41 IPs are the
overlaps, and the total number of DoS attack IPs is increased
to 122.

Lastly, it can be seen that this DDoS attack cannot be found
by only considering the number of requests shown in Fig.
6. This is because this DDoS attack mainly targeted node
No. 25. Although node No. 25 received a large number of

Fig. 6. The hourly number of requests change in 8 days dataset.

Fig. 7. The hourly cache hit rate change in 8 days dataset.

since the reduced cache hit rate is the most representative
behavior of a CDN node that is under a CPA or DoS attack. As
shown in Fig. 7, the hourly cache hit rate remains stationary
during the eight days except for 13:00-15:00, day 4. An
abnormal event might occur in this time period, so this time
period is labeled as an abnormal event period.

To determine the root cause of the signiﬁcant cache hit
rate drop, we have analyzed the requests, nodes, and client
IPs in this abnormal period. Firstly, most of the requests in
this abnormal period have got the status code 404, indicating
resource-not-found errors. Thus, attackers have sent many
requests for non-existing contents to cause many cache misses.
A large number of 404-error requests will exhaust the network
resources and cause network unavailability or congestion, so
a DoS attack might occur during this abnormal period.

In the next step, we have explored the compromised nodes
in this abnormal period. It is found that most of the 404-error
requests have been sent to node No. 25, so it is the major
target of the attack. Additionally, 10 other nodes are found to
be the secondary targets since they also got many 404-error

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

15

TABLE XV
THE BEHAVIORS OF SUSPICIOUS ACCOUNT-OFFERINGS

AO No.

1
2
3
4
5

6

7

8
Other
static AOs

Number of
requests
44,985,524
78,505,897
57,276,415
63,844,994
184,294

1,487,0894

7,557,320

127,665,987
6,501,377

Number
of nodes
27
28
34
25
34

47

48

49
5.06

Cache
hit rate
0.999
0.999
0.997
0.997
0.809

0.747

0.763

0.907
0.992

Service
type
Static
Static
Static
Static
Progressive
download
Live
streaming
Live
streaming
Static
Static

Request
popularity
1.0
1.0
1.0
1.0
0.262

0.762

0.751

0.985
0.999

TABLE XVI
MEAN VALUES OF EACH FEATURE ON ALL DAYS FOR THREE ABNORMAL
NODES

Node
No.

0
5
7

Avg
cache
hit
rate
0.49
0.06
0.0002

Avg
legitimate
IP cache
hit rate
0.0
0.08
0.0

Avg data
transfer
rate
(MB/s)
0.69
0.13
0.0009

Avg
request
error
rate
0.0
0.0
0.0002

Avg
number
of
requests
344
2383.8
531.6

Avg
request
popularity

0.935
0.905
0.881

malicious requests in this DDoS attack period, this number of
requests is still small compared to the total number of requests
received by all nodes or in the crowd events. This emphasizes
the reasons for monitoring the changes in different features
instead of only one feature for time-series analysis; otherwise,
real anomalies may be missed.

2) Account-offering (AO) Analysis:

In the ﬁnal stage of
result validation, account-offering analysis is performed on the
current anomaly detection results to remove false alarms.

There are 70 unique AOs in the 8 days dataset. To analyze
each AO, its number of requests, number of nodes, cache
hit rate, service type, and request popularity are calculated
and analyzed together with their conﬁguration information
provided by the CDN operator. The information about the
suspicious AOs is shown in Table XV. Firstly, the AOs No. 1-
4 are the major AOs that provide services during crowd events
since they have received a large number of requests for high-
popularity contents with more than 99% cache hit rate during
the crowd event periods. The 250 detected crowd event IPs
mainly used these 4 AOs, which further proves that they are
the legitimate IPs instead of DoS attack IPs. Thus, these 250
IPs are the false positives of the proposed unsupervised model.
For the detected abnormal IPs and nodes, the AOs No.
5-8 shown in Table XV are the four suspicious AOs since
almost all the requests of the detected potential abnormal IPs

TABLE XVII
MEAN VALUES OF EACH FEATURE FOR NORMAL AND DETECTED
ABNORMAL NODES

Node
label

Normal
Abnormal

Cache
hit
rate
0.936
0.405

Legitimate
IP cache
hit rate
0.928
0.416

Data
transfer
rate (MB/s)
0.805
0.506

Request
error
rate
0.004
0.049

Request
popularity

0.978
0.936

TABLE XVIII
MEAN VALUES OF EACH FEATURE FOR NORMAL AND DETECTED
ABNORMAL CONTENTS

Content
label

Normal
Abnormal

Avg
number
of
requests
95.6
1049.7

Avg
request
per node
ratio
7.7
228.9

Avg
request
per IP
ratio
1.5
284.3

Avg
cache hit
rate

Avg
popularity

0.21
0.852

0.19
0.333

TABLE XIX
MEAN VALUES OF EACH FEATURE FOR NORMAL AND DETECTED
POTENTIAL CPA IPS

IP
label

Normal
LDA
FLA
CPA

Avg
number
of
requests
161.3
9461.4
9418.8
9458.8

Avg
number
of
nodes
2.1
3.76
5.5
3.87

Avg
request per
content
ratio
1.56
1.13
84.74
6.36

Avg
cache
hit
rate
0.925
0.111
0.691
0.147

Avg
request
error
rate
0.006
0.0006
0.006
0.001

Avg
request
popularity

0.977
0.316
0.478
0.326

and nodes were sent through these 4 AOs. It is found in the
conﬁguration information that the node No. 5 is the major
progressive download content source, and certain unpopular
contents might have been requested by an IP many times for
being progressively downloaded. The AOs No. 6 & 7 are the
major live streaming content sources and have relatively high
cache hit rates and request popularity, about 75%. Thus, certain
IPs that have used any of these two AOs but got a very low
cache hit rate or request popularity are likely to be abnormal.
Moreover, the AO No. 8 is the major static content source.
It can be seen in the last row of Table XV that other static
AOs have got a more than 99% cache hit rate and request
popularity, much higher than the AO No. 8, so the AO No. 8
might be used by certain IPs to launch attacks.

Further result validation is done through the AO analysis.
Firstly, among the 14 detected abnormal nodes, the 11 com-
promised nodes under potential DDoS attacks listed in Table
XIII are validated to be the real compromised nodes since
their major AOs belong to the abnormal AOs No. 6-8. For
the other three detected abnormal nodes (nodes No. 0, 5, 7)
shown in Table XVI, since their AOs are legitimate AOs and
they have not received many requests (less than 2,500), so they
might have been breached before day 1, so any new legitimate
requests were unable to get cache hits. Additionally, nodes No.
5 & 7 were attacked by CPAs since they have much lower
request popularities than normal nodes (0.905 & 0.881 versus
0.978). Therefore, a total of 14 compromised nodes, including
3 nodes that have already been attacked before day 1, and 11
nodes that were under attack on day 4, have been identiﬁed and
illustrated in Tables XIII and XVI, and their average feature
values are shown in Table XVII.

According to the conﬁguration information provided by
the CDN operator, it is known that as a major source for
progressive download videos, the AO No. 5 was used for the
tests of old videos, most of which have low popularity. Thus,
AO No. 5 has some similar characteristics to CPAs since it is
also used to request for low popularity ﬁles many times, but
it was used by legitimate users to do tests. Therefore, for the
detected abnormal contents and CPA IPs, those using the AO

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

16

TABLE XX
MEAN VALUES OF EACH FEATURE FOR NORMAL AND DETECTED
POTENTIAL DOS ATTACK IPS

IP
label

Normal
DDoS
Other
DoS
All
DoS

Avg
number
of
requests
161.3
3039.9
59789.5

Avg
number
of
nodes
2.1
2.39
4.63

Avg
request
interval
(s)
1177.6
0.6
1.97

Avg
cache
hit
rate
0.925
0.026
0.708

Avg
request
error
rate
0.006
0.972
0.264

14465.3

2.98

1.44

0.16

0.821

Avg
request
popularity

0.977
0.947
0.983

0.943

No.5 as the major AO are found to be legitimate entities and
removed from the abnormal content and IP list. After removing
the false positives (113 contents and 21 IPs), the behaviors of
the detected 56 real abnormal contents and 33 real CPA IPs
are shown in Tables XVIII and XIX, respectively. For the two
types of CPAs, 30 IPs are labeled as LDA IPs, and 3 other
IPs are labeled as FLA IPs.

Lastly, through AO analysis, all the detected DoS IPs have
used the AOs No. 6-8 to make requests, and they are labeled
real DoS IPs. Among these 122 DoS IPs, 103 of them are the
DDoS attack IPs that have launched a DDoS attack together
on day 4, from 13:00 to 15:00. The other 19 abnormal IPs may
have tried to launch a DoS attack individually during different
time periods. Their behaviors are shown in Table XX.

D. Results Summary

In summary, 14 compromised nodes, 56 abnormal contents,
33 CPA IPs, and 122 DoS attack IPs were detected by the
proposed anomaly detection model, as shown in Table XXI.
Among the detected abnormal network entities, 12 nodes and
122 IPs were affected by DoS attacks, while 2 nodes, 33 IPs,
and 56 contents were affected by CPAs. Additionally, 384
false positives (FPs) and 65 false negatives (FNs) were re-
moved through the multi-perspective validation process. As the
utilized datasets are completely unlabeled, all these anomaly
detection results have been analyzed and veriﬁed to be 100%
accurate by multiple cybersecurity experts and industrial part-
ner security network engineers. This veriﬁcation process is a
necessary HITL procedure in real-world applications, as the
experts and engineers have in-depth knowledge of the dataset,
legitimate events, and cyber-attack patterns. This process also
validates the effectiveness of the proposed framework.

Lastly, the performance of the proposed framework is com-
pared with several data analytics and anomaly detection tech-
niques. As discussed in Section IV-D, binary outlier detection
algorithms are more suitable for the node-based dataset that
only has 50 samples, so two standard binary outlier detection
algorithms, iForest [13] and one-class support vector machine
(OC-SVM) [61], are used for the comparison of abnormal
node detection. On the other hand, clustering algorithms are
suitable for IP & content-based datasets that have more than
1 million samples, so two common clustering algorithms, k-
means [62] and GMM [14], are used for the comparison
of abnormal IP & content detection. For the performance
metrics, since the datasets are highly imbalanced, precision
(Pre), Recall (Rec), and F1-score are used with accuracy (Acc)

TABLE XXI
SUMMARY OF DETECTED ANOMALIES

Label
Normal
DoS
CPA
Removed FPs
Removed FNs

Number of nodes
36
12
2
0
3

Number of IPs
1,267,872
122
33
271
62

Number of contents
1,867,528
-
56
113
0

TABLE XXII
PERFORMANCE COMPARISON WITH REGULAR ANOMALY DETECTION
TECHNIQUES

Method

Proposed
IForest [13]
OC-SVM [61]
GMM [14]
K-means [62]
GMM [14]
K-means [62]

Detection
perspective
All

Node

IP

Content

Acc
(%)
100.0
94.0
86.0
99.97
99.96
99.99
99.99

Pre
(%)
100.0
100.0
73.3
25.6
21.1
33.1
25.8

Rec
(%)
100.0
78.6
78.6
60.0
69.7
100.0
100.0

F1
(%)
100.0
88.0
75.9
35.8
32.3
49.8
41.0

for model evaluation. By calculating the harmonic mean of the
precision and recall, F1-score is a reliable metric to measure
the classiﬁcation performance on imbalanced datasets [28].

The model performance comparison is shown in Table
XXII. The accuracy of most methods are larger than 99%,
but this is mainly due to the imbalanced dataset (more than
99% of data samples are normal data). The iForest and GMM
models used for the result comparison are only themselves
without the proposed validation procedures; hence, many FPs
and FNs occurred, which reduced the F1-scores of iForest
and GMM to 88.0%, 35.8%, and 49.8% on the node, IP,
and content-based datasets, respectively. This emphasizes the
importance of implementing the proposed multi-perspective
validation method. Moreover, the F1-scores of OC-SVM and
k-means algorithms are lower than the iForest and GMM
models from all three perspectives, which justiﬁes the rationale
for choosing iForest and GMM in our proposed framework.

As the CDN requests/trafﬁc are continuously generated and
we aim to detect abnormal network entities (i.e., node, IP,
content) and corresponding attacks, the node, IP, and content-
based datasets need to be continuously updated by imple-
menting the proposed feature engineering method on the new
CDN trafﬁc data. Then, the proposed unsupervised learning
models continuously detect anomalies on the updated datasets.
These are the major procedures that take time. The reason
for the reaction or detection time is because the proposed
system needs to process a number of requests to update the

TABLE XXIII
ANOMALY DETECTION TIME COMPARISON

Detection
Perspective

Method

Node

IP

Content

Proposed
OC-SVM [61]
Proposed
K-means [62]
Proposed
K-means [62]

9.8

12.4

5.8

Feature
Engineering
Avg Max

Total

Detection Time of Each Anomaly (s)
Unsupervised
Detection
Avg Max
0.3
0.1
1.6
0.5
25.6
6.8
15.3
3.1
10.4
4.5
7.7
2.3

Avg Max
29.9
9.9
31.2
10.3
58.5
19.2
48.2
15.5
22.9
10.3
20.2
8.1

12.5

29.6

32.9

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

17

behaviors of nodes, IPs, and contents; thus, the anomalies
can be identiﬁed. To protect CDNs against DoS and CPA,
the anomalies should be detected in time. Hence, assuming
the trafﬁc is continuously generated, we have evaluated the
detection speed of the proposed system by measuring the total
execution time from the time each abnormal entity is affected
by an attack to the time the proposed method detects this
anomaly. This anomaly detection time has been further divided
into the feature engineering time and the unsupervised model
detection time. Moreover, as different anomalies/attacks show
different patterns and need different detection time, we have
measured the average (Avg) and maximum (Max) execution
time of all abnormal nodes, IPs, and contents in seconds.

The anomaly detection time of the proposed method and
the two compared methods (OC-SVM and k-means) is shown
in Table XXIII. From the node perspective,
the proposed
method can detect the abnormal nodes in the average time
of 9.9s and the maximum time of 29.9s, which is slighter
faster than OC-SVM. The unsupervised detection time for
abnormal node detection is low, because the small number
of nodes (50) has made it faster for the proposed method to
detect the abnormal nodes than the abnormal IPs or contents.
From the IP and content perspectives, the proposed method
can detect abnormal IPs and contents in the average time of
19.2s and 10.3s, and in the maximum time of 58.5s and 22.9s,
respectively. The detection time of the abnormal IPs is higher
than the time of the abnormal contents, because detecting
certain DoS IPs needs to analyze a large number of requests.
Nevertheless, the maximum abnormal IP detection time is still
at a low level (58.5s). This shows that the proposed solution
can detect anomalies at the early stages of attacks, which can
help CDNs stop current attacks in time and prevent future
attacks. Although the anomaly detection time of k-means is
lower than the proposed method, the proposed method can
achieve much higher performance than k-means, as shown in
Table XXII. Therefore, our proposed method still performs
the best among the comparisons by considering both detection
accuracy and speed.

VII. PRACTICAL USAGE AND OPEN ISSUES

Given that the proposed framework is working with un-
labeled data, the framework can be used as the ﬁrst level
of the anomaly detection process. Unlike traditional anomaly
detection processes for unlabeled data, which requires mas-
sive manual analysis and expert knowledge,
the proposed
framework can reduce much overhead using the automatically-
tuned ML algorithms and a systematic multi-perspective result
validation process proposed in Section IV. Additionally, the
high performance of the proposed framework has been veriﬁed
by multiple security experts.

One open issue with this work is that it only considers two
common service targeting attacks, i.e., CPAs and DoS attacks.
Nevertheless, the proposed framework can be extended to new
attacks based on the same procedures:

1) Collect sufﬁcient network log data that contains the new

attack samples;

2) Analyze the new attack, summarize the attack patterns,
and select appropriate features that can reﬂect the new
attack patterns;

3) Utilize the proposed unsupervised anomaly detection
model to detect suspicious activities and compare them
with the summarized characteristics of new attacks to
determine the real attacks;

4) Utilize the proposed multi-perspective result validation
method to remove false alarms and false negatives.
Through this process, any new attacks that can be reﬂected
from CDN log data can be effectively detected by the proposed
method.

On the other hand, although the proposed framework can
reduce much manual analysis and labeling process, certain
expert knowledge and legitimate event information are still re-
quired in the proposed framework to achieve accurate anomaly
detection. This is because certain numerical anomalies identi-
ﬁed by unsupervised ML algorithms are not true anomalies
and require further analysis to distinguish between certain
legitimate abnormal events and cyber-attacks.

Lastly, several procedures in addition to our work should
be implemented for real-world CDN applications. Since this
research work can be considered a pseudo labeling process
on an unlabeled raw CDN log dataset, the labeled content,
client IP, and node-based datasets can be used to train reliable
supervised classiﬁers. Through this procedure, human efforts
will not be required in the future anomaly detection process,
and the system can automatically detect anomalies by contin-
uously processing the incoming data. Thus, supervised model
development is one direction for future work.

Moreover, corresponding countermeasures should be made
along with attack detection to stop or prevent cyber-attacks.
The proposed multi-perspective anomaly detection framework
enables the implementation of countermeasures on both at-
tacker and victim sides.

Firstly, on the client or attacker side, trafﬁc ﬁltering and
blacklisting are potential response mechanisms to prevent the
detected malicious clients from sending requests to CDN
servers [63]. Detecting abnormal IPs enables network adminis-
trators to locate the origins of cyber-attacks [64]. Additionally,
the domains and speciﬁc geographic locations can be found
based on the detected malicious client IP addresses. To make
countermeasures, the detected malicious IPs, as well as the
IPs from the same domains and locations, can be added to
the blacklist to block or limit the trafﬁc from these IPs until
the identities of these suspicious clients are veriﬁed. Hence,
the current attacks can be stopped, and future attacks can be
prevented by blacklisting the detected malicious IPs [7].

Secondly, on the victim side, countermeasures can be made
from both content and node perspectives. Blacklisting can also
be used for detected abnormal contents [63]. The contents
in the blacklist will be deleted from the cache space of
any edge servers. Additionally, the edge servers can reject
the requests sent for these abnormal contents or never cache
them. Through this process, CPAs can be stopped because the
detected abnormal contents cannot be used by CPA attackers to
pollute the cache space of CDN servers. On the other hand, the
compromised nodes can be isolated until they recover. Specif-

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

18

ically, the compromised node information will be notiﬁed to
other nodes, so that other nodes can avoid communicating with
the affected nodes that are under attack [63]. The nodes can
recover by limiting the requests sent to them or implementing
the blacklisting strategies from the client IP and content
perspectives. Once recovered, the nodes can be removed from
the isolation list and continue communications.

In conclusion, as the proposed anomaly detection can con-
tinuously detect anomalies, any new malicious or compro-
mised network entities can be identiﬁed, and corresponding
countermeasures can be made to defend against cyber-attacks
when they occur. Moreover, the multi-perspective countermea-
sures enhance the defense capabilities of CDNs. For example,
an attacker cannot continue an attack by simply changing IP
addresses, because the countermeasures from other perspec-
tives, like isolating nodes and blacklisting contents, can still
stop the attack.

Since the development of supervised models and counter-
measures is outside the scope of this work, it will be our future
work.

VIII. CONCLUSION

CDNs have become a major content distribution technology
in modern networks. However, their caching mechanism in-
troduces additional vulnerabilities. In this paper, we proposed
a multi-perspective anomaly detection approach based on a
real-world general CDN access log dataset to identify abnor-
mal network entities and cyber-attacks. To detect DoS and
cache pollution attacks, we ﬁrst summarized their patterns
and then extracted features from four main perspectives:
content, client IP, account-offering, and node perspectives.
After obtaining the extracted datasets from multiple per-
spectives, the anomalies were identiﬁed using the optimized
unsupervised learning model constructed with the optimized
isolation forest and Gaussian mixture models. A comprehen-
sive validation method, including multi-perspective analysis,
time-series analysis, and account-offering analysis, was im-
plemented to validate the detected abnormal network entities
and the corresponding cyber-attacks. Thus, detection errors
can be effectively reduced. Ultimately, the abnormal contents,
compromised nodes, and malicious IPs were detected and
labeled. In future work, the labeled anomaly detection results
can be used for classiﬁer development so that an automated
process can be developed to detect new attacks and abnormal
network entities effectively. Certain security mechanisms, such
as isolating and blacklisting the detected abnormal network
entities, can be utilized after anomaly detection to secure
CDNs.

IX. ACKNOWLEDGMENT

This work is partially supported by the Natural Sciences and
Engineering Research Council of Canada (NSERC) [NSERC
Strategic Partnership Grant STPGP – 521537] and Ericsson
Canada.

REFERENCES

[1] J. Zhao, P. Liang, W. Liufu, and Z. Fan, “Recent Developments in Content
Delivery Network: A Survey,” in Parallel Architectures, Algorithms and
Programming, 2020, pp. 98–106.

[2] M. Z. Shaﬁq, A. R. Khakpour, and A. X. Liu, “Characterizing caching
workload of a large commercial Content Delivery Network,” Proc. - IEEE
INFOCOM, vol. 2016-July, pp. 1–9, 2016.

[3] A. Boukhtouta, M. Pourzandi, R. Brunner, and S. Dault, “Fingerprinting
Crowd Events in Content Delivery Networks: A Semi-supervised Method-
ology,” in IFIP Annual Conference on Data and Applications Security and
Privacy, 2018, pp. 312–329.

[4] E. Nygren, R. K. Sitaraman, and J. Sun, “The Akamai network: A
platform for high-performance Internet applications,” Oper. Syst. Rev.,
vol. 44, no. 3, pp. 2–19, 2010.

[5] L. Deng, Y. Gao, Y. Chen, and A. Kuzmanovic, “Pollution attacks and
defenses for Internet caching systems,” Comput. Networks, vol. 52, no.
5, pp. 935–956, 2008.

[6] R. Aliyev, D. Seo and H. Lee, ”DROP-FAST: Defending against DDoS
Attacks using Cloud Technology”, Int. Conf. on Security and Manage-
ment, 2013.

[7] M. M. Rahman, S. Roy, and M. A. Yousuf, “DDoS Mitigation and
Intrusion Prevention in Content Delivery Networks using Distributed
Virtual Honeypots,” 1st Int. Conf. Adv. Sci. Eng. Robot. Technol. 2019,
ICASERT 2019, vol. 2019, no. Icasert, 2019.

[8] K. L. Moore, T. J. Bihl, K. W. Bauer, and T. E. Dube, “Feature extraction
and feature selection for classifying cyber trafﬁc threats,” J. Def. Model.
Simul., vol. 14, no. 3, pp. 217–231, 2017.

[9] S. Aburakhia, T. Tayeh, R. Myers, and A. Shami, ”A Transfer Learning
Framework for Anomaly Detection Using Model of Normality,” in 2020
IEEE IEMCON, Vancouver, BC, Canada, Nov. 2020.

[10] T. Tayeh, S. Aburakhia, R. Myers and A. Shami, ”Distance-Based
Anomaly Detection for Industrial Surfaces using Triplet Networks,” in
2020 IEEE IEMCON, Vancouver, Canada, Nov. 2020.

[11] L. Yang, R. Muresan, A. Al-Dweik and L. J. Hadjileontiadis, ”Image-
Based Visibility Estimation Algorithm for Intelligent Transportation Sys-
tems,” in IEEE Access, vol. 6, pp. 76728-76740, 2018.

[12] L. Yang, ”Comprehensive Visibility Indicator Algorithm for Adaptable
Speed Limit Control in Intelligent Transportation Systems”, M.A.Sc.
thesis, University of Guelph, 2018.

[13] L. Sun, S. Versteeg, and A. Rao, “Detecting Anomalous User Be-
havior Using an Extended Isolation Forest Algorithm: An Enterprise
Case Study.” arXiv Prepr. arXiv1609.06676, pp. 1–13, 2016, [Online].
Available: https://arxiv.org/abs/1609.06676.

[14] M. Bitaab and S. Hashemi, “Hybrid Intrusion Detection: Combining
Decision Tree and Gaussian Mixture Model,” in 2017 14th International
ISC (Iranian Society of Cryptology) Conference on Information Security
and Cryptology (ISCISC), 2017, pp. 8–12.

[15] A. Holzinger, “Interactive machine learning for health informatics: when
do we need the human-in-the-loop?,” Brain Informatics, vol. 3, no. 2, pp.
119–131, 2016.

[16] D. R. Honeycutt, M. Nourani, and E. D. Ragan, “Soliciting human-in-
the-loop user feedback for interactive machine learning reduces user trust
and impressions of model accuracy,” arXiv Prepr. arXiv2008.12735, pp.
1–10, 2020, [Online]. Available: https://arxiv.org/abs/2008.12735.

[17] H. Park, I. Widjaja, and H. Lee, “Detection of cache pollution attacks
using randomness checks,” IEEE Int. Conf. Commun., pp. 1096–1100,
2012.

[18] J. B. Gouge, “A targeted denial of service attack on data caching

networks”, UNF Graduate Theses and Dissertations, 2015.

[19] C. Ghasemi, H. Youseﬁ, and B. Zhang, “Far Cry: Will CDNs
Hear NDN’s Call?,” in Proceedings of the 7th ACM Conference on
Information-Centric Networking, 2020, pp. 89–98.

[20] S. Triukose, Z. Al-Qudah, and M. Rabinovich, “Content Delivery
Networks: Protection or Threat?,” in Computer Security – ESORICS 2009,
2009, pp. 371–389.

[21] B. Zolfaghari et al., “Content Delivery Networks: State of the Art,
Trends, and Future Roadmap,” ACM Comput. Surv., vol. 53, no. 2, Apr.
2020.

[22] M. Conti, P. Gasti, and M. Teoli, “A lightweight mechanism for
detection of cache pollution attacks in Named Data Networking,” Comput.
Networks, vol. 57, no. 16, pp. 3178–3191, 2013.

[23] J. Chen, H. Xu, S. Penugonde, Y. Zhang, and D. Raychaudhuri,
“Exploiting ICN for efﬁcient content dissemination in CDNs,” Proc. -
4th IEEE Work. Hot Top. Web Syst. Technol. HotWeb 2016, pp. 14–19,
2016.

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

19

[24] C. Ghasemi, H. Youseﬁ, and B. Zhang, “ICDN: An NDN-Based CDN,”
the 7th ACM Conference on Information-Centric

in Proceedings of
Networking, 2020, pp. 99–105.

[25] H. V. Nguyen, L. Lo Iacono, and H. Federrath, “Your cache has fallen:
Cache-poisoned denial-of-service attack,” Proc. ACM Conf. Comput.
Commun. Secur., pp. 1915–1930, 2019.

[26] H. Wang, X. Chen, W. Wang, and M. Y. Chan, “Content pollution
propagation in the overlay network of peer-to-peer live streaming systems:
Modelling and analysis,” IET Commun., vol. 12, no. 17, pp. 2119–2131,
2018.

[27] K.-W. Lee, S. Chari, A. Shaikh, S. Sahu, and P.-C. Cheng, “Improving
the resilience of content distribution networks to large scale distributed
denial of service attacks,” Comput. Networks, vol. 51, no. 10, pp.
2753–2770, 2007.

[28] L. Yang, A. Moubayed, I. Hamieh, and A. Shami, “Tree-based Intelligent
Intrusion Detection System in Internet of Vehicles,” 2019 IEEE Glob.
Commun. Conf., no. Ml, pp. 1–6, 2019.

[29] E. Shuster, L. LaSeur, O. Katz, and S. Ragan, “Financial Services Attack

Economy,” Akamai Technol., vol. 5, no. 4, pp. 1–40, 2019.

[30] “Vulnerability Note VU# 335217: Content Delivery Networks handle
HTTP headers in different and unexpected ways”, Technical report
US CERT Vulnerability Notes Database, Jan 2020, [online] Available:
http://www.kb.cert.org/vuls/id/836068.

[31] M. Xie, I. Widjaja, and H. Wang, “Enhancing cache robustness for
content-centric networking,” Proc. - IEEE INFOCOM, pp. 2426–2434,
2012.

[32] A. Karami and M. Guerrero-Zapata, “An ANFIS-based cache replace-
ment method for mitigating cache pollution attacks in Named Data
Networking,” Comput. Networks, vol. 80, pp. 51–65, 2015.

[33] A. Moubayed, M. Injadat, A. Shami, and H. Lutﬁyya, “DNS Typo-
Squatting Domain Detection: A Data Analytics & Machine Learning
Based Approach,” 2018 IEEE Glob. Commun. Conf. GLOBECOM 2018
- Proc., 2018.

[34] A. Moubayed, E. Aqeeli, and A. Shami, “Ensemble-based Feature Selec-
tion and Classiﬁcation Model for DNS Typo-squatting Detection,” in 2020
IEEE Canadian Conference on Electrical and Computer Engineering
(CCECE), 2020.

[35] P. Kumar, A. Moubayed, A. Refaey, A. Shami, and J. Koilpillai,
“Performance Analysis of SDP For Secure Internal Enterprises,” IEEE
Wirel. Commun. Netw. Conf. WCNC, vol. 2019-April, 2019.

[36] A. Moubayed, A. Refaey, and A. Shami, “Software-Deﬁned Perimeter
(SDP): State of the Art Secure Solution for Modern Networks,” IEEE
Netw., vol. 33, no. 5, pp. 226–233, 2019.

[37] D. Chiba, K. Tobe, T. Mori, and S. Goto, “Detecting malicious websites
by learning IP address features,” Proc. - 2012 IEEE/IPSJ 12th Int. Symp.
Appl. Internet, SAINT, 2012, no. July, pp. 29–39, 2012.

[38] A. Pinto, “Defending Networks with Incomplete Information: A Ma-
chine Learning Approach,” Black Hat USA, pp. 1–10, 2013, [Online].
Available: https://www.blackhat.com/us-13/brieﬁngs.html#Pinto.

[39] P. Fiadino, A. D’Alconzo, A. B¨ar, A. Finamore, and P. Casas, “On
the detection of network trafﬁc anomalies in content delivery network
services,” 2014 26th Int. Teletrafﬁc Congr. ITC 2014, 2014.

[40] V. H. La and A. R. Cavalli, “A Misbehavior Node Detection Algorithm
for 6LoWPAN Wireless Sensor Networks,” Proc. - 2016 IEEE 36th Int.
Conf. Distrib. Comput. Syst. Work. ICDCSW 2016, pp. 49–54, 2016.
[41] N. Berjab, H. H. Le, C. M. Yu, S. Y. Kuo, and H. Yokota, “Abnormal-
Node detection based on spatio-Temporal and Multivariate-Attribute
correlation in wireless sensor networks,” Proc. - IEEE 16th Int. Conf.
Dependable, Auton. Secur. Comput., pp. 568–575, 2018.

[42] S. K. Pandey, P. Kumar, J. P. Singh, and M. P. Singh, “Intrusion detection
system using anomaly technique in wireless sensor network,” Proceeding
- IEEE Int. Conf. Comput. Commun. Autom. ICCCA 2016, pp. 611–615,
2017.

[43] S. Caltagirone, A. Pendergast, and C. Betz, “The Diamond Model of

Intrusion Analysis,” DTIC Doc. Tech. Rep., 2013.

[44] L. Yang, A. Moubayed, and A. Shami, “MTH-IDS: A Multi-Tiered Hy-
brid Intrusion Detection System for Internet of Vehicles,” IEEE Internet
Things J., 2021.

[45] D. Z. and M. C. Andrew Moore, “Discriminators for use in ﬂow-based
classiﬁcation,” Dept. Comput. Sci., Queen Mary Univ., London, U.K.,
Tech. Rep. RR-05-13, 2005.

[46] A. Juvonen, T. Sipola, and T. H¨am¨al¨ainen, “Online anomaly detection
using dimensionality reduction techniques for HTTP log analysis,” Com-
put. Networks, vol. 91, pp. 46–56, 2015.

[47] T. Zehelein, S. Schuck, and M. Lienkamp, “Automotive Damper Defect
the
Detection Using Novelty Detection Methods,” in Proceedings of
ASME 2019 Dynamic Systems and Control Conference, 2019.

[48] G. A. Susto, A. Beghi, and S. McLoone, “Anomaly detection through
on-line isolation Forest: An application to plasma etching,” in 2017
28th Annual SEMI Advanced Semiconductor Manufacturing Conference
(ASMC), May 2017, pp. 89–94.

[49] M. N. Injadat, A. Moubayed, A. B. Nassif, and A. Shami, “Systematic
Ensemble Model Selection Approach for Educational Data Mining,”
Knowledge-Based Syst., vol. 200, p. 105992, 2020.

[50] M. Injadat, A. Moubayed, A. B. Nassif, and A. Shami, “Multi-split Op-
timized Bagging Ensemble Model Selection for Multi-class Educational
Data Mining,” Appl. Intell., 2020.

[51] D. M. Manias, M. Jammal, H. Hawilo, A. Shami, et al., ”Machine
Learning for Performance-Aware Virtual Network Function Placement,”
2019 IEEE Glob. Commun. Conf., Waikolao, HI, USA, Dec. 2019.
[52] L. Yang and A. Shami, “A Lightweight Concept Drift Detection and
Adaptation Framework for IoT Data Streams,” IEEE Internet Things
Mag., 2021.

[53] M. Injadat, F. Salo, A. B. Nassif, A. Essex, and A. Shami, “Bayesian
Optimization with Machine Learning Algorithms Towards Anomaly De-
tection,” 2018 IEEE Glob. Commun. Conf. GLOBECOM 2018 - Proc.,
2018.

[54] M. Injadat, A. Moubayed, A. B. Nassif and A. Shami, “Multi-Stage Op-
timized Machine Learning Framework for Network Intrusion Detection,”
IEEE Trans. Netw. Serv. Manag., 2020.

[55] L. Yang and A. Shami, “On Hyperparameter Optimization of Machine
Learning Algorithms: Theory and Practice,” Neurocomputing, vol. 415,
pp. 295–316, 2020.

[56] F. Pedregosa et al., “Scikit-learn: Machine Learning in Python,” J.
Mach. Learn. Res., vol. 12, pp. 2825–2830, 2011, [Online]. Available:
http://scikit-learn.sourceforge.net.

[57] A. Moubayed, M. Injadat, A. Shami, and H. Lutﬁyya, “Student Engage-
ment Level in e-Learning Environment: Clustering Using K-means,” Am.
J. Distance Educ., vol. 34, no. 02, pp. 1–20, 2020.

[58] F. Salo, M. N. Injadat, A. Moubayed, A. B. Nassif, and A. Essex,
“Clustering Enabled Classiﬁcation using Ensemble Feature Selection for
Intrusion Detection,” 2019 Int. Conf. Comput. Netw. Commun. ICNC
2019, pp. 276–281, 2019.

[59] R. C. Pinto and P. M. Engel, “Scalable and Incremental Learning of
Gaussian Mixture Models.” arXiv Prepr. arXiv1701.03940, pp. 1–13,
2017, [Online]. Available: http://arxiv.org/abs/1701.03940.

[60] N. Shi, X. Liu, and Y. Guan, “Research on k-means clustering algorithm:
An improved k-means clustering algorithm,” 3rd Int. Symp. Intell. Inf.
Technol. Secur. Informatics, IITSI 2010, pp. 63–67, 2010.

[61] C. She, W. Wen, Z. Lin, and K. Zheng, “Application-Layer DDOS
Detection Based on a One-Class Support Vector Machine,” Int. J. Netw.
Secur. Its Appl., vol. 9, no. 1, pp. 13–24, 2017.

[62] A. Gupta and P. Nahar, “Detection of Cache Pollution Attacks in a Se-
cure Information-Centric Network,” in Data Analytics and Management,
2021, pp. 377–397.

[63] A. Fiandrotti, R. Gaeta, and M. Grangetto, “Simple countermeasures to
mitigate the effect of pollution attack in network coding based peer-to-
peer live streaming,” IEEE Trans. Multimed., vol. 17, no. 4, pp. 562–573,
2017.

[64] A. Carlin, M. Hammoudeh, and O. Aldabbas, “Intrusion Detection and
Countermeasure of Virtual Cloud Systems - State of the Art and Current
Challenges,” Int. J. Adv. Comput. Sci. Appl., vol. 6, no. 6, pp. 1–15, 2015.

Li Yang received the B.E. degree in computer
science from Wuhan University of Science and
Technology, Wuhan, China in 2016 and the MASc
degree in Engineering from University of Guelph,
Guelph, Canada, 2018. Since 2018 he has been
working toward the Ph.D. degree in the Department
of Electrical and Computer Engineering, Western
University, London, Canada. His research interests
include cybersecurity, machine learning, deep learn-
ing, and time-series analysis.

ACCEPTED AND TO APPEAR IN IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT

20

Adel Larabi is a Senior Solution Architect at Er-
icsson with over 25 years of leadership experience
in designing innovative business solutions for Telco.
Helping bridging academia research projects with
commercial grade enterprise solutions. Core quali-
ﬁcations in CDN, Edge Computing, Big data, IMS,
Media, and OSS with interest on AI applied to these
domain. He is an expert in the Ericsson cybersecurity
team.

Richard Brunner joined Ericsson in 1988 and with
broad career experience in System Management,
Standardization and Strategic Product management.
Richard possesses international management experi-
ence, with deep knowledge of the wireless telecom-
munication market. Richard is actively engaged in
setting Ericsson’s research and partnership activities
both internally and externally towards industry and
universities. He is an expert in the Ericsson cyber-
security team.

Stere Preda received his PhD in Computer Sci-
ence from TELECOM Bretagne, France. Senior Re-
searcher with expertise in cybersecurity at Ericsson,
he is an active contributor to ETSI NFV security
standardization.

Daniel Migault is an expert in the Ericsson cyberse-
curity team and is actively involved in standardizing
security protocols at the IETF.

Abdallah Moubayed received his Ph.D. in Electri-
cal & Computer Engineering from the University of
Western Ontario in August 2018, his M.Sc. degree
in Electrical Engineering from King Abdullah Uni-
versity of Science and Technology, Thuwal, Saudi
Arabia in 2014, and his B.E. degree in Electrical
Engineering from the Lebanese American Univer-
sity, Beirut, Lebanon in 2012. Currently, he is a
Postdoctoral Associate in the Optimized Comput-
ing and Communications (OC2) lab at University
of Western Ontario. His research interests include
wireless communication, resource allocation, wireless network virtualization,
performance & optimization modeling, machine learning & data analytics,
computer network security, cloud computing, and e-learning.

Abdallah Shami is a professor with the ECE De-
partment at Western University, Ontario, Canada. He
is the Director of the Optimized Computing and
Communications Laboratory at Western University
(https://www.eng.uwo.ca/oc2/). He is currently an
associate editor for IEEE Transactions on Mobile
Computing, IEEE Network, and IEEE Communi-
cations Surveys and Tutorials. He has chaired key
symposia for IEEE GLOBECOM, IEEE ICC, IEEE
ICNC, and ICCIT. He was the elected Chair of the
IEEE Communications Society Technical Commit-
tee on Communications Software (2016-2017) and the IEEE London Ontario
Section Chair (2016-2018).

Parisa Heidari is working as a security master and
IoT developer at Ericsson. She received her Masters
and PhD in computer engineering from Ecole Poly-
technique de Montreal, Canada in 2007 and 2012,
respectively. She worked as research associate at
Concordia University in collaboration with Ericsson
from 2013-2014 and joined Ericsson in 2015. Her
research interests include Internet of Things, Edge
Computing, container technologies, function as a
Service and different aspects of QoS in cloud system
such as optimal resource dimensioning, placement
and security of cloud applications. She holds to her credit several patent and
publications. She is an expert in the Ericsson cybersecurity team.

Amine Boukhtouta is an Experienced Researcher at
Ericsson Security Research Group. He received the
computer science engineering degree from USTHB
University, Algiers, Algeria, in 2005 and the Master
of Applied Science degree in information systems
security degree and the Ph.D. degree in electrical and
computer engineering from Concordia University,
Montreal, Canada, in 2009 and 2016, respectively.
He was part of Cyber-Forensics Training Alliance
Canada, doing research on the generation of cyber-
threat intelligence based on malware and network
traces. He joined a Post-doctoral industrial program in 2016, where he worked
on ﬁnding malicious indicators in evolving delivery network by applying big
data analytics and machine learning. His current research interests include
prevention, detection of cyber-threats by applying machine learning, and
artiﬁcial intelligence. He published 5 journal papers and 11 conference papers
in peer-reviewed venues. He was a recipient of OCTAS Prize in 2009
University Competition, the FQRNT Doctoral Scholarship in 2010–2011,
the Best Paper Award, and the MITACS as well as PROMPT Postdoctoral
Fellowships in 2016–2017. He is an expert in the Ericsson cybersecurity team.

