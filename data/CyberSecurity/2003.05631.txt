0
2
0
2

v
o
N
4
2

]

R
C
.
s
c
[

3
v
1
3
6
5
0
.
3
0
0
2
:
v
i
X
r
a

ConAML: Constrained Adversarial Machine Learning for
Cyber-Physical Systems

Jiangnan Li
jli103@vols.utk.edu
University of Tennessee, Knoxville

Yingyuan Yang
yyang260@uis.edu
University of Illinois Springfield

Jinyuan Stella Sun
jysun@utk.edu
University of Tennessee, Knoxville

Kevin Tomsovic
tomsovic@utk.edu
University of Tennessee, Knoxville

Hairong Qi
hqi@utk.edu
University of Tennessee, Knoxville

ABSTRACT
Recent research demonstrated that the superficially well-trained
machine learning (ML) models are highly vulnerable to adversarial
examples. As ML techniques are becoming a popular solution for
cyber-physical systems (CPSs) applications in research literatures,
the security of these applications is of concern. However, current
studies on adversarial machine learning (AML) mainly focus on
pure cyberspace domains. The risks the adversarial examples can
bring to the CPS applications have not been well investigated. In
particular, due to the distributed property of data sources and the
inherent physical constraints imposed by CPSs, the widely-used
threat models and the state-of-the-art AML algorithms in previous
cyberspace research become infeasible.

We study the potential vulnerabilities of ML applied in CPSs by
proposing Constrained Adversarial Machine Learning (ConAML),
which generates adversarial examples that satisfy the intrinsic con-
straints of the physical systems. We first summarize the difference
between AML in CPSs and AML in existing cyberspace systems and
propose a general threat model for ConAML. We then design a best-
effort search algorithm to iteratively generate adversarial examples
with linear physical constraints. We evaluate our algorithms with
simulations of two typical CPSs, the power grids and the water
treatment system. The results show that our ConAML algorithms
can effectively generate adversarial examples which significantly
decrease the performance of the ML models even under practical
constraints.

KEYWORDS
adversarial machine learning; cyber-physical system; intrusion de-
tection

1 INTRODUCTION
Machine learning (ML) has shown promising performance in many
real-world applications, such as image classification [20], speech
recognition [18], and malware detection [49]. In recent years, moti-
vated by the promotion of cutting-edge communication and compu-
tational technologies, there is a trend to adopt ML in various cyber-
physical system (CPS) applications, such as data center thermal
management [29], agriculture ecosystem management [9], power
grid attack detection [37], and industrial control system anomaly
detection [24].

Recent research has demonstrated that the superficially well-
trained ML models are highly vulnerable to adversarial examples

[10, 17, 26, 34, 35, 40, 42]. In particular, adversarial machine learning
(AML) technologies enable attackers to deceive ML models with
well-crafted adversarial examples by adding small perturbations to
legitimate inputs. As CPSs have become synonymous to security-
critical infrastructures such as the power grid, nuclear systems,
avionics, and transportation systems, such vulnerabilities can be
exploited leading to devastating consequences.

Figure 1: A CPS example (power grids).

AML research has received considerable attention in artificial
intelligence (AI) communities and it mainly focuses on compu-
tational applications such as computer vision. However, it is not
applicable to CPSs because the inherent properties of CPSs render
the widely-used threat models and AML algorithms in previous
research infeasible. The existing AML research makes common
assumptions on the attacker’s knowledge and the adversarial ex-
amples. In most AML research, the attacker is assumed to have full
knowledge of the ML inputs and these features are assumed to be
mutually independent. For example, in computer vision [17], the
attacker is assumed to know all the values of pixels of an image
and there is no strict dependency among the pixels. However, this
is not realistic for attacks targeting CPSs. CPSs are usually large
and complex systems whose data sources are heterogeneous and
geographically distributed. The attacker may compromise a subset
of sensors and modify their measurement data. Generally, for the
uncompromised data sources, the attacker cannot even know the
measurements, let alone making modifications. Furthermore, for
robustness and resilience reasons, CPSs usually employ redundant
data sources and incorporate faulty data detection mechanisms. For
example, in the power grid, redundant phasor measurement units
(PMUs) are deployed in the field to measure frequency and phase
angle, and residue-based bad data detection is employed to detect
and recover from faulty data for state estimation [45]. Therefore, the

 
 
 
 
 
 
features of ML applications in CPS are not only dependent but also
subject to the physical constraints of the system. A simple example
of constraints is shown in Figure 1. All three meters are measur-
ing the electric current (Ampere) data. If an attacker compromises
Meter1, Meter2, and Meter3, no matter what modification the at-
tacker makes to the measurements, the compromised measurement
of Meter1 should always be the sum of that of Meter2 and Meter3
due to Kirchhoff’s laws. Otherwise, the crafted measurements will
be detected by the bad data detection mechanism and obviously
anomalous to the power system operators. In addition to distributed
data sources and physical constraints, sensors in real-world CPSs
are generally configured to collect data with a specific sampling
rate. A valid adversarial attack needs to be finished within the CPS’
sampling period.

The intrinsic properties of CPS pose stringent requirements for
the adversarial attackers. The attacker is now required to overcome:

• Knowledge constraint: No access to the ML models and

the measurement values of uncompromised sensors.

• Physical constraint: The adversarial examples need to

meet the physical constraints defined by the system.

• Time constraint: Attacks needs to be completed within a

sample period of the sensors.

to launch an effective attack that deceives the ML applications
deployed in CPSs. However, in this paper, we show that the ML
applications in CPSs are susceptible to handcrafted adversarial
examples even though such systems naturally pose a greater barrier
for the attacker.

In this paper, we propose constrained adversarial machine learn-
ing (ConAML), a general AML framework that incorporates the
above constraints of CPSs. We firstly design a universal adversarial
measurement algorithm to solve the knowledge constraint. After
that, without loss of generality, we present a practical best-effort
search algorithm to effectively generate adversarial examples under
linear physical constraints which are one of the most common con-
straints in real-world CPS applications, such as power grids [33]
and water pipelines [16]. Meanwhile, we set the maximum iteration
number to control the time cost of the attack. We implement our
algorithms with ML models used in two CPSs and mainly focus on
neural networks due to its transferability. Our main contributions
are summarized as follows:

• We highlight the potential vulnerability of deploying ML in
CPSs, analyze the different requirements for AML applied in
CPSs with regard to the general computational applications,
and present a practical threat model for AML in CPSs.

• We formulate the mathematical model of ConAML by incor-
porating the physical constraints of the underlying system.
• We proposed ConAML, an AML framework that contains a
series of AML algorithms to generate adversarial examples
under the corresponding constraints.

• We assess our algorithms with two typical CPSs, the power
grids and water treatment system, where ML are intensively
investigated for attack detection in the research literature
[1, 3, 7, 12, 13, 21–24, 36, 37, 44, 46]. The evaluation results
show that the adversarial examples generated by our algo-
rithms can effectively bypass the ML-powered attack detec-
tion systems in the two CPSs.

Related research is discussed in Section 2. We analyze the prop-
erties of AML in CPSs and give the mathematical definition and
the threat model in Section 3. Section 4 presents the algorithm
design. Section 5 uses two CPSs as proofs of concept to carry our
experiments. Discussions and future work are given in Section 6.
Section 7 concludes the paper.

2 RELATED WORK
AML of deep neural network (DNN) was discovered by Szegedy et
al. [42] in 2013. They found that a DNN used for image classification
can be fooled by adding a hardly perceptible perturbation to the
legitimate image. The same perturbation can cause a different DNN
to misclassify the same image even when the DNN has a different
structure and is trained with a different dataset, which is referred
to as the transferability property of adversarial examples. In 2015,
Goodfellow et al. [17] proposed the Fast Gradient Sign Method
(FGSM), an efficient algorithm to generate adversarial examples.
The Fast Gradient Value (FGV) method by Rozsa et al. [40] is a
variant of FGSM and utilizes the raw gradient instead of the sign
values. Moosavi-Dezfooli et al. presented DeepFool to iteratively
search for the closest distance between the original input and the
decision boundary [34]. Single-step attacks have better transferabil-
ity but can be easily defended [26]. Therefore, multi-steps methods,
such as iterative methods [26] and momentum-based methods [10],
are presented. The above methods generate individual adversarial
examples for each input. In 2017, Moosavi-Dezfooli et al. designed
universal adversarial perturbations to generate perturbations re-
gardless of the ML model inputs [35].

Research on AML applications continues growing rapidly. Sharif
et al. launched adversarial attacks to a face-recognition system
and achieved a notable result [41]. Grossee et al. constructed ad-
versarial attacks against Android malware detection models [19].
In 2014, Laskov et al. developed a taxonomy for practical adver-
sarial attacks based on the attackers’ capability and launched eva-
sion attacks to PDFRATE, a real-world online machine learning
system to detect malicious PDF malware [39]. In 2018, Li et al. pre-
sented TEXTBUGGER, a framework to generate adversarial text
against deep learning-based text understanding (DLTU) systems
and achieved state-of-the-art attack performance [28].

AML techniques that involve the physical domain are drawing
more and more attention. Kurakin et al. presented that ML models
are vulnerable to adversarial examples in physical world scenar-
ios by feeding a phone camera captured adversarial image to an
ImageNet classifier [25]. In 2016, Carlini et al. presented that well-
crafted voice commands which are unintelligible to human listeners,
can be interpreted as commands by voice controllable systems [5].
[43] and [32] investigated the security of ML models used in au-
tonomous driving cars. In 2018, [15] showed that an attacker can
generate adversarial examples by modifying a portion of measure-
ments in CPSs, and presented an anomaly detection model where
each sensor’s reading is predicted as a function of other sensors’
readings. After that, Erba et al. also studied the AML in CPS and con-
sider the physical constraints [11]. They employed an autoencoder
that is trained on normal system data to reconstruct the bad inputs
to match the physical behavior. However, both [15] and [11] allow

the attacker to know all the measurements which may be imprac-
tical in real-world attacks. Meanwhile, the generated adversarial
examples of [11] may still violate the physical constraints.

More related work on adversarial attacks, including the adver-

sarial example generation and applications, can be found in [48].

3 SYSTEM AND THREAT MODEL
3.1 ML-Assisted CPSs

usually placed in the control centers and other centralized locations
which employ comprehensive and advanced security measures
such as air-gapped networks. It is highly unlikely for the attacker
to have access to the models and a black-box attack should be
considered. Second, we assume that the attacker cannot access the
training dataset for the same reason as above, but has access to
an alternative dataset such as historical data that follows a similar
distribution to train their models. It is possible for the attacker to
obtain historical data in practice, for instance, temperature data for
load forecasting, earthquake sensor data, flood water flow data, and
traffic flow data, since these data are usually published or shared
among multiple parties.

To launch adversarial attacks, the attacker is assumed to com-
promise a certain number of sensors, and can freely eavesdrop and
modify their measurement data. These sensors are deployed in the
wild and their security is hard to guarantee. In real attack scenar-
ios, this can be implemented by either directly compromising the
sensors, such as device intrusion or attacking the communication
network, such as man-in-the-middle attacks. However, due to the
vastly distributed nature of sensors in CPS, it is only reasonable
for the attacker to compromise a subset of the data sources but
not all of them. For the uncompromised sensors, the attacker can
neither know their measurement values nor make modifications.
This constraint indicates that the attacker has limited knowledge
of the ML inputs.

Figure 2: Machine learning-assisted CPS architecture.

Generally, a CPS can be simplified as a system that consists of
four parts, namely sensors, actuators, the communication network,
and the control center [7], as shown in Figure 2. The sensors mea-
sure and quantify the data from the physical environment, and
send the measurement data to the control center through the com-
munication network. In practice, the raw measurement data will
be filtered and processed by the gateway according to the error
checking mechanism whose rules are defined by human experts
based on the properties of the physical system. Measurement data
that violates the physically defined rules will be removed.

Similar to [11], we consider the scenario that the control center
utilizes ML model(s) to make decisions (classification) based on
the filtered measurement data from the gateway directly, and the
features used to train the ML models are the measurements of
sensors respectively. The target of the attacker will be deceiving
the ML model(s) in CPSs to output wrong (classification) results
without being detected by the gateway by adding perturbations to
the measurements of the compromised sensors.

3.2 Threat Model
Adversarial attacks can be classified according to the attacker’s
capability and attack goals [6, 39, 48]. In this work, we consider the
integrity attack that the attacker generates adversarial perturba-
tions to the ML inputs to deceive the ML model to make incorrect
classification outputs.

There are several inherent properties of CPS that pose specific
requirements for adversarial attacks. First, in CPS, ML models are

Figure 3: A CPS example (water pipelines).

Meanwhile, the attacker is further required to generate adversar-
ial examples that meet the constraints imposed by the physical laws
and system topology and evade any built-in detection mechanisms
in the system. Specifically, since they are very common in real-
world CPSs, we will mainly focus on linear constraints in this paper,
including both linear equality constraints and linear inequality con-
straints. An example of the linear inequality constraint is shown in
Figure 3. All the meters in Figure 3 are measuring water flow which
follows the arrows’ direction. If an attacker wants to defraud the
anomaly detection ML model of a water treatment system by modi-
fying the meters’ readings, the adversarial measurement of Meter1
should always be larger than the sum of Meter2 and Meter3 due
to the physical structure of the pipelines. Otherwise, the poisoned
inputs will be obviously anomalous to the victim (system operator)
and detected automatically by the error checking mechanisms. In
practice, many of the linear constraints can be explicitly abstracted
by the attacker if she/he obtained enough measurement data by
observing the compromised sensors. Meanwhile, the practical CPSs

usually have built-in tolerance for noise and normal fluctuation in
the measurements so that the approximately estimated constraints
will still be effective for the adversarial attackers. Therefore, we
assume that the attacker know the linear constraints among the
compromised measurements. We discuss the nonlinear equality
constraints at Appendix A.

The real-world CPSs, such as the Supervisory Control and Data
Acquisition (SCADA), will have a constant measurement sampling
rate (frequency) configured for their sensors. The attacker who
targets CPSs’ ML applications is then required to generate a valid
adversarial example within a measurement sampling period.

We summarize the threat model as follows:

• We assume the attacker has no access to the system op-
erator’s trained model in the control center, including the
hyper-parameters and the related dataset. However, the at-
tacker has an alternative dataset as an approximation of
the defender’s (system operator’s) training dataset to train
his/her ML models.

• The attacker can compromise a subset of sensors in the CPS
and make modifications to their measurement data. However,
the attack can neither know nor modify the measurements
of uncompromised sensors.

• The attacker can know the linear constraints of the measure-

ments imposed by the physical system.

3.3 Physical Constraint Mathematical

Representation

In this subsection, we present the mathematical definition of the
physical linear constraints of the ML inputs and represent the AML
as a constrained optimization problem.

3.3.1 Notations. To simplify the mathematical representation,
we will use 𝐴𝐵 = (cid:2)𝑎𝑏0
(cid:3) to denote a sampled vector of
𝐴 = [𝑎0, 𝑎1, ..., 𝑎𝑚−1] according to 𝐵, where 𝐵 = [𝑏0, 𝑏1, ..., 𝑏𝑛−1]
is a vector of sampling index. For example, if 𝐴 = [𝑎, 𝑏, 𝑐, 𝑑, 𝑒] and
𝐵 = [0, 2, 4], we have 𝐴𝐵 = [𝑎, 𝑐, 𝑒].

, ..., 𝑎𝑏𝑛−1

, 𝑎𝑏1

We assume there are totally 𝑑 sensors in a CPS, and each sensor’s
measurement is a feature of the ML model 𝑓𝜃 in the control center.
We use 𝑆 = [𝑠0, 𝑠1, ..., 𝑠𝑑−1]𝑇 and 𝑀 = [𝑚0, 𝑚1, ..., 𝑚𝑑−1]𝑇 to denote
all the sensors and their measurements respectively. The attacker
compromised 𝑟 sensors in the CPS and 𝐶 = [𝑐0, 𝑐1, ..., 𝑐𝑟 −1] denotes
the index vector of the compromised sensors. Obviously, we have
∥𝐶 ∥ = 𝑟 and 0 < 𝑟 ≤ 𝑑. Meanwhile, the uncompromised sensors’
indexes are denoted as 𝑈 = [𝑢0, 𝑢1, ..., 𝑢𝑑−𝑟 −1] (∥𝑈 ∥ = 𝑑 − 𝑟 ).

𝐶 = 𝑀𝐶 +Δ𝐶 , and 𝑚∗

Δ = [𝛿0, 𝛿1, ..., 𝛿𝑑−1]𝑇 is the adversarial perturbation to be added
(cid:3)𝑇
to 𝑀. However, the attacker can only inject Δ𝐶 = (cid:2)𝛿𝑐0, 𝛿𝑐1, ..., 𝛿𝑐𝑟 −1
to 𝑀𝐶 while Δ𝑈 = 0. The polluted adversarial measurements be-
come 𝑀∗
𝑐𝑖 = 𝑚𝑐𝑖 +𝛿𝑐𝑖 (0 ≤ 𝑖 ≤ 𝑟 −1). Apparently,
we have 𝛿𝑖 = 𝛿𝑐 𝑗 when 𝑖 = 𝑐 𝑗 , 𝑖 ∈ 𝐶, and 𝛿𝑖 = 0 when 𝑖 ∉ 𝐶. Sim-
ilarly, the crafted adversarial example 𝑀∗ =
=
𝑀 + Δ is fed into 𝑓𝜃 . We have 𝑚∗
when 𝑖 = 𝑐 𝑗 , 𝑖 ∈ 𝐶 and
𝑚∗
𝑖 = 𝑚𝑖 when 𝑖 ∉ 𝐶. All the notations are summarized in Table 1.

𝑖 = 𝑚∗
𝑐 𝑗

1, ..., 𝑚∗

0, 𝑚∗

(cid:104)
𝑚∗

𝑑−1

(cid:105)

3.3.2 Mathematical Presentation. For linear equality constraints,
such as the current measurements (Amperes) of the three meters
in Figure 1, we suppose there are 𝑘 constraints of the compromised

Table 1: List of Notations

Symbol
𝑓𝜃
𝑆
𝑀
Δ
𝑀∗

𝐶

𝑈

𝑌
Φ

Description

The trained model with hyperparameter 𝜃
The vector of sensors
The vector of measurements of 𝑆
The perturbations vector added to 𝑀
The sum of Δ and 𝑀. The vector of
compromised input
The vector of the indexes of compromised
sensors or measurements
The vector of the indexes of uncompromised
sensors or measurements
The original class of the measurement 𝑀
The linear constraint matrix

measurements 𝑀𝐶 that the attacker needs to meet, and the 𝑘 con-
straints can be represented as follow:





𝜙0,0 · 𝑚𝑐0 + ... + 𝜙0,𝑟 −1 · 𝑚𝑐𝑟 −1 = 𝜙0,𝑟
𝜙1,0 · 𝑚𝑐0 + ... + 𝜙1,𝑟 −1 · 𝑚𝑐𝑟 −1 = 𝜙1,𝑟
...
𝜙𝑘−1,0 · 𝑚𝑐0 + ... + 𝜙𝑘−1,𝑟 −1 · 𝑚𝑐𝑟 −1 = 𝜙𝑘−1,𝑟
The above constraints can be represented as (2). We have Φ𝑘×𝑟 =
[Φ0, Φ1, ..., Φ𝑘−1]𝑇 , where Φ𝑖 = (cid:2)𝜙𝑖,0, 𝜙𝑖,1, ..., 𝜙𝑖,𝑟 −1(cid:3) (0 ≤ 𝑖 ≤ 𝑘 −1),
Φ𝑖,𝑗 = 𝜙𝑖,𝑗 (0 ≤ 𝑖 ≤ 𝑘−1, 0 ≤ 𝑗 ≤ 𝑟 −1) and ˜Φ = (cid:2)𝜙0,𝑟 , 𝜙1,𝑟 , ..., 𝜙𝑘−1,𝑟

(1)

(cid:3)𝑇 .

Φ𝑘×𝑟 𝑀𝐶 = ˜Φ
(2)
The attacker generates the perturbation vector Δ𝐶 and adds it
to 𝑀𝐶 such that 𝑓𝜃 will predict the different output. Meanwhile,
the crafted measurements 𝑀∗
𝐶 = Δ𝐶 + 𝑀𝐶 should also meet the
constraints in (2) to avoid being noticed by the system operator or
detected by the error checking mechanism.

Formally, the attacker who launches AML attacks needs to solve

the following optimization problem:

𝐿(𝑓𝜃 (𝑀∗), 𝑌 )

max
Δ𝐶
𝑠.𝑡 . 𝑀∗

𝐶 = 𝑀𝐶 + Δ𝐶
Φ𝑘×𝑟 𝑀𝐶 = ˜Φ
𝐶 = ˜Φ
Φ𝑘×𝑟 𝑀∗
𝑀∗ = 𝑀 + Δ
Δ𝑈 = 0

(3a)

(3b)

(3c)

(3d)

(3e)

(3f)

where 𝐿 is a loss function, and Y is the original class label of the

input vector 𝑀.

In addition, the linear inequality constraints among the com-
promised measurements can be represented as equation (4), and
the constrained optimization problem to be solved is also similar to
𝐶 ≤ ˜Φ
(3) but replacing (3c) with Φ𝑘×𝑟 𝑀𝐶 ≤ ˜Φ and (3d) with Φ𝑘×𝑟 𝑀∗
respectively.

Φ𝑘×𝑟 𝑀𝐶 ≤ ˜Φ

(4)

4 DESIGN OF CONAML
The universal adversarial measurements algorithm is proposed in
subsection 4.1 to solve the knowledge constraint of the attacker.
Subsection 4.2 and subsection 4.4 analyze the properties of physical
linear equality constraints and linear inequality constraints in AML
respectively and present the adversarial algorithms. We set the
maximum numbers of searching step in different algorithms to
control the attack’s time cost.

4.1 Universal Adversarial Measurements

Algorithm 2: Sample Evaluation
1 Input: 𝑓𝜃 , 𝑌 , 𝑀𝑈𝐶, Δ
2 Output: Classification Accuracy
3 function sampleEva(𝑓𝜃 , 𝑌, 𝑀𝑈𝐶, Δ)
4

add perturbation Δ to all vectors in 𝑀𝑈𝐶
evaluate 𝑀𝑈𝐶 with 𝑓𝜃 and label 𝑌
return the classification accuracy of 𝑓𝜃 (𝑀𝑈𝐶)

5

6
7 end

Algorithm 1: Universal Adv-Measur Algorithm
1 Input: 𝑓𝜃 , 𝑀𝑈 , 𝑀𝐶 , 𝜆, 𝑌 , 𝑀𝑎𝑥𝐼𝑡𝑒𝑟𝑎
2 Output: 𝑀∗
3 function uniAdvMeasur(𝑓𝜃 , 𝑀𝑈 , 𝑀𝐶, 𝜆, 𝑌, 𝑀𝑎𝑥𝐼𝑡𝑒𝑟𝑎)
4

, ..., 𝑀𝐶 |𝑈𝑁

(cid:9)

initialize Δ = 0
build set 𝑀𝑈 𝐶 = (cid:8)𝑀𝐶 |𝑈0
set counter 𝑐𝑦𝑐𝑁𝑢𝑚 = 0
while 𝑐𝑦𝑐𝑁𝑢𝑚 < 𝑀𝑎𝑥𝐼𝑡𝑒𝑟𝑎 do

, 𝑀𝐶 |𝑈1

set 𝑓 𝑙𝑎𝑔 to 0
for 𝑀𝐶 |𝑈𝑖 in 𝑀𝑈𝐶 do

Δ = onePerturGenAlgorithm(Δ, 𝑀𝐶 |𝑈𝑖 )
if sampleEva(𝑓𝜃 , 𝑌, 𝑀𝑈 𝐶, Δ) < 𝜆 then

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

set 𝑓 𝑙𝑎𝑔 to 1

break

end

end
if 𝑓 𝑙𝑎𝑔 equals 1 then

break

end
𝑐𝑦𝑐𝑁𝑢𝑚++

end
return 𝑀∗ = 𝑀 + Δ

21
22 end

We first deal with the challenge of the attacker’s limited knowl-
edge on the uncompromised measurements 𝑀𝑈 . This challenge
is difficult to tackle since the complete measurement vector 𝑀 is
needed to obtain the gradient values in many AML algorithms
[17, 26, 34, 35, 40]. In 2017, Moosavi-Dezfooli et al. proposed the
universal adversarial perturbation scheme which generates image-
agnostic adversarial perturbation [35]. The identical universal ad-
versarial perturbation vector can cause different images to be mis-
classified by the state-of-the-art ML-based image classifiers with
high probability. The basic philosophy of [35] is to iteratively and
incrementally build a perturbation vector that can misclassify a set
of images sampled from the whole dataset.

Inspired by their approach, we now present our universal adver-
sarial measurements algorithm. We define an ordered set of 𝑁 sam-
pled uncompromised measurements 𝑀𝑈 = (cid:8)𝑀𝑈0, 𝑀𝑈1, ..., 𝑀𝑈𝑁 −1
(cid:9),
to denote the crafted measurement vector from
and use 𝑀𝐶 |𝑈𝑖
𝑀𝐶 and the sampled uncompromised measurement vector 𝑀𝑈𝑖
.
(cid:13)
is a crafted measurement vector with (cid:13)
(cid:13) = 𝑑.
Here, 𝑀𝐶 |𝑈𝑖

(cid:13)𝑀𝐶 |𝑈𝑖

The uncompromised measurement vectors in 𝑀𝑈 can be randomly
selected from the attacker’s alternative dataset.

Algorithm 1 describes a high-level approach to generate adver-
sarial perturbations regardless of uncompromised measurements.
The algorithm first builds a set of crafted measurement vector 𝑀𝑈𝐶
based on 𝑀𝑈 and 𝑀𝐶 , and then starts an iteration over 𝑀𝑈𝐶. The
iteration process is limited to 𝑀𝑎𝑥𝐼𝑡𝑒𝑟𝑎 times to control the maxi-
mum time cost. The purpose is to find a universal Δ that can cause
a portion of the vectors in 𝑀𝑈𝐶 misclassified by 𝑓𝜃 . The function
sampleEva described in Algorithm 2 evaluates 𝑀𝑈𝐶 and 𝑌 with
the ML model 𝑓𝜃 and returns the classification accuracy. 𝜆 ∈ (0, 1]
is a constant chosen by the attacker to determine the attack’s suc-
cess rate in 𝑀𝑈𝐶 according to Δ. During each searching iteration,
algorithm 1 builds and maintains the perturbation Δ increasingly
using an adversarial perturbation generation algorithms, as shown
by Line 10 in Algorithm 1. We will propose our methods to handle
this problem in the next subsections.

Figure 4: Iteration illustration.

Figure 4 presents a simple illustration of the iteration process in
Algorithm 1. We assume there are three sensors’ measurements 𝑀 =
[𝑚0, 𝑚1, 𝑚2] in a CPS and only one sensor’s measurement 𝑚0 = 𝛼
is compromised by the attacker. We set the the sample number
𝑁 = 3 and the yellow, green and orange shallow areas in the plane
𝑀0 = 𝛼 represent the possible adversarial examples of the crafted
measurement vector 𝑀𝐶 |𝑈0
, respectively, where
𝑈𝑖 are randomly sampled measurements of uncompromised sensors
(𝑚1 and 𝑚2). The initial point 𝑀 (red ⋆) iterates twice (𝑟0 and
𝑟1) and finally reaches 𝑀∗ with the universal perturbation vector
Δ. Therefore, 𝑀∗ is a valid adversarial example for all 𝑀𝐶 |𝑈𝑖 (𝑖 ∈
{0, 1, 2}).

, and 𝑀𝐶 |𝑈2

, 𝑀𝐶 |𝑈1

Comparison of Methods: Our approach is different from [35]
in several aspects. First, the approach proposed in [35] has iden-
tical adversarial perturbations for different ML inputs while our
approach actually generates distinct perturbations for each 𝑀. Sec-
ond, the approach in [35] builds universal perturbations regardless
of the real-time ML inputs. However, as the attacker has already
compromised a portion of measurements, it is more effective to
take advantage of the obtained knowledge. In other words, our
perturbations are ‘universal’ for 𝑀𝑈 but ‘distinct’ for 𝑀. Finally,
the intrinsic properties of CPSs require the attacker to generate a
valid adversarial example within a sampling period while there is
no enforced limitation of the iteration time in [35].

4.2 Linear Equality Constraints Analysis
As shown in [17] and [40], the fundamental philosophy of AML
can be represented as (5).

𝑀∗ = 𝑀 + Δ = 𝑀 + 𝜖∇𝑀 𝐿(𝑓𝜃 (𝑀), 𝑌 )
(5)
However, directly following the gradient will not guarantee the
adversarial examples meet the constraints in (2) and (4). With the
constraints imposed by the physical system, the attacker is no
longer able to freely add perturbation to original input using the
raw gradient of the input vector. In this subsection, we will analyze
how the linear equality constraints will affect the way to generate
perturbation and use a simple example for illustration. The proofs
of all the theorems and corollaries can be found in Appendix B.
Under the threat model proposed in Section 3.2, the constraint
of (3c) is always met due to the properties of the physical systems.
We then consider the constraint (3d).

Theorem 4.1. The sufficient and necessary condition to meet con-

straint (3d) is Φ𝑘×𝑟 Δ𝐶 = 0.

From Theorem 4.1 we can also derive a very useful corollary, as

shown below.

Corollary 4.2. If Δ𝐶0 , Δ𝐶1 , ..., Δ𝐶𝑛 are valid perturbation vectors
𝑖=0 𝑎𝑖 · Δ𝐶𝑖 is also

that follow the constraints, then we have Δ𝐶′ = (cid:205)𝑛
a valid perturbation for the constraint Φ𝑘×𝑟 .

Theorem 4.1 indicates that the perturbation vector to be added to
the original measurements must be a solution of the homogeneous
linear equations Φ𝑘×𝑟 𝑋 = 0. However, is this condition always met?
Theorem 4.3. In practical scenarios, the attacker can always find a
valid solution (perturbation) that meets the linear equality constraints
imposed by the physical systems.

We utilize a simplified example to illustrate how the constraints
will affect the generation of perturbations, as shown in Figure 5.
According to 5, measurement 𝑀 should move a small step (pertur-
bation) to the gradient direction (direction 1 in Figure 5) to increase
the loss most rapidly. However, as shown by the contour lines in
Figure 5, the measurement 𝑀 is always forced to be on the straight
line 𝑦 = 2 − 2𝑥 (2-dimension), which is the projection of the in-
tersection of the two surfaces 𝑧 (𝑥, 𝑦) = 2𝑥 2 + 2𝑦2 and 2𝑥 + 𝑦 = 2
(3-dimension). Accordingly, instead of following the raw gradient,
𝑀 should move forward to direction 2 to increase the loss. There-
fore, although at a relatively slow rate, it is still possible for the
attacker to increase the loss under the constraints.

Figure 5: Linear equality constraint illustration. We consider
a simple ML model 𝑓 that only has two dimensions inputs
(𝑥, 𝑦) with a loss function 𝐿(𝑓𝜃 (𝑀), 𝑌 ) = 𝑧 (𝑥, 𝑦) = 2𝑥 2 + 2𝑦2.
Meanwhile, we suppose the input measurements 𝑥 and 𝑦
need to meet the linear constraints 2𝑥 + 𝑦 = 2 and the cur-
rent measurement vector 𝑀 = (0.4, 1.2).

4.3 Adversarial Example Generation under

Linear Equality Constraint

The common method of solving optimization problems using gradi-
ent descent under constraints is projected gradient descent (PGD).
However, since neural networks are generally not considered as
convex functions [8], PGD cannot be used to generate adversarial
examples directly. We propose the design of a simple but effec-
tive search algorithm to generate the adversarial examples under
physical linear equality constraints.

Algorithm 3: Best-Effort Search (Linear Equality)
1 Input: Δ, 𝑓𝜃 , 𝐶, 𝑀, 𝑠𝑡𝑒𝑝, 𝑠𝑖𝑧𝑒, Φ, 𝑌
2 Output: 𝑣
3 function genEqPer(Δ, 𝑓𝜃 , 𝐶, 𝑀, 𝑠𝑡𝑒𝑝, 𝑠𝑖𝑧𝑒, Φ, 𝑌 )
4

initialize 𝑣 = Δ
initialize 𝑠𝑡𝑒𝑝𝑁𝑢𝑚 = 0
while 𝑠𝑡𝑒𝑝𝑁𝑢𝑚 ≤ 𝑠𝑡𝑒𝑝 − 1 do

if 𝑓 ′

𝜃 ′ (𝑀 + 𝑣) doesn’t equals 𝑌 then
return 𝑣

end
𝑟 = eqOneStep(𝑓𝜃 , 𝐶, 𝑀 + 𝑣, 𝑠𝑖𝑧𝑒, Φ, 𝑌 )
update 𝑣 = 𝑣 + 𝑟
𝑠𝑡𝑒𝑝𝑁𝑢𝑚 = 𝑠𝑡𝑒𝑝𝑁𝑢𝑚 + 1

5

6

7

8

9

10

11

12

13

end
return 𝑣

14
15 end

As discussed in subsection 4.2, the perturbation Δ𝐶 needs to be a
solution of Φ𝑘×𝑟 𝑋 = 0. We use 𝑛 = 𝑅𝑎𝑛𝑘 (Φ𝑘×𝑟 ) to denote the rank
of the matrix Φ𝑘×𝑟 , where 0 < 𝑛 < 𝑟 . It is obvious that the solution
set of homogeneous linear equation Φ𝑘×𝑟 𝑋 = 0 will have 𝑟 −𝑛 basic
solution vectors. We use 𝐼 = [𝑖0, 𝑖1, ..., 𝑖𝑟 −𝑛−1]𝑇 to denote the index

-1-0.500.511.52x-1-0.500.511.52y-20246810121412(0.4,1.2)of independent variables in the solution set, 𝐷 = [𝑑0, 𝑑1, ..., 𝑑𝑛−1]𝑇
to denote the index of corresponding dependent variables, and
𝐵𝑛×(𝑟 −𝑛) to denote the linear dependency matrix of 𝑋𝐼 and 𝑋𝐷 .
Clearly, we have 𝑋𝐷𝑛×1 = 𝐵𝑛×(𝑟 −𝑛)𝑋𝐼 (𝑟 −𝑛)×1
. For convenience, we
will use [𝐼, 𝐷, 𝐵] = dependency(Φ𝑘×𝑟 ) to describe the process of
getting 𝐼 , 𝐷, 𝐵 from matrix Φ𝑘×𝑟 .

Algorithm 4: One Step Attack Constraint Δ𝐶
1 Input: 𝑓𝜃 , 𝐶, 𝑀, 𝑠𝑖𝑧𝑒, Φ, 𝑌
2 Output: 𝑟
3 function eqOneStep(𝑓𝜃 , 𝐶, 𝑀, 𝑠𝑖𝑧𝑒, Φ𝑘×𝑟 , 𝑌 )

4

5

6

7

8

9

calculate gradient vector 𝐺 = ∇𝑀 𝐿(𝑓𝜃 (𝑀), 𝑌 )
set all elements of 𝐺𝑈 in 𝐺 to zero
define 𝐺 ′ = 𝐺𝐶
obtain tuple [𝐼, 𝐷, 𝐵] = dependency(Φ𝑘×𝑟 )
update 𝐺 ′
𝐼 in 𝐺 ′
𝐷 = 𝐵𝐺 ′
𝜖 = 𝑠𝑖𝑧𝑒/max(abs(𝐺 ′))
return 𝑟 = 𝜖𝐺

10
11 end

As shown in Algorithm 3, the function genEqPer takes Δ as
an input and outputs a valid perturbation 𝑣 for 𝑀. Algorithm 3
keeps executing eqOneStep for multiple times defined by 𝑠𝑡𝑒𝑝
to generate a valid 𝑣 increasingly. Function eqOneStep performs
a single-step attack for the input vector and returns a one-step
perturbation 𝑟 that matches the constraints defined by Φ, which is
shown in Algorithm 4. Due to Corollary 4.2, Δ and 𝑣 will also follow
the constraints. To decrease the iteration time, similar to [34], the
algorithm will return the crafted adversarial examples immediately
as long as 𝑓 ′
𝜃 ′ misclassifies the input measurement vector 𝑀 + 𝑣, as
shown by Line 7 in Algorithm 3.

The philosophy of function eqOneStep in algorithm 4 is very
straightforward. From the constraint Matrix Φ, we can get the
independent variables 𝐼 , dependent variables 𝐷 and the dependency
matrix 𝐵 between them. We will simply keep the gradient values of
𝐼 and use them to compute the corresponding values of 𝐷 (Line 8) so
that the final output perturbation 𝑟 will follow Φ. The constant 𝑠𝑖𝑧𝑒
defines the largest modification of a specific measurement value in
one iteration to control the search speed.

4.4 Adversarial Example Generation under

Linear Inequality Constraint

𝜃 ′, 𝑈 , 𝑀, 𝑠𝑖𝑧𝑒, 𝑌

Algorithm 5: Non-Constraint Perturbation.
1 Input: 𝑓 ′
2 Output: 𝑟
3 function freeStep(𝑓 ′
4

𝜃 ′, 𝑈 , 𝑀, 𝑠𝑖𝑧𝑒, 𝑌 )

calculate gradient vector 𝐺 = ∇𝑀 𝐿(𝑓 ′
set elements in 𝐺𝑈 to zero
𝜖 = 𝑠𝑖𝑧𝑒/max(abs(𝐺))
return 𝑟 = 𝜖𝐺

5

6

7
8 end

𝜃 ′ (𝑀), 𝑌 )

Linear inequality constraints are very common in real-world
CPS applications, like the water flow constraints in Figure 3. Due
to measurement noise, real-world systems usually tolerate distinc-
tions between measurements and expectation values as long as
the distinctions are smaller than predefined thresholds, which also
brings inequality constraints to data. Meanwhile, a linear equality
constraint can be represented by two linear inequality constraints.
As shown in equation (4), linear inequality constraints define the
valid measurement subspace whose boundary hyper-planes are
defined by equation (2). In general, the search process under linear
inequality constraints can be categorized into two situations. The
first situation is when a point (measurement vector) is in the sub-
space and meets all constraints, while the second situation happens
when the point reaches boundaries.

Algorithm 6: Best-Effort Search (Linear Inequality)
𝜃 ′, 𝐶, 𝑈 , 𝑀, 𝑠𝑡𝑒𝑝, 𝑠𝑖𝑧𝑒, Φ, ˜Φ, 𝑌
1 Input: Δ, 𝑓 ′
2 Output: 𝑣
3 function genIqPer(Δ, 𝑓 ′
4

𝜃 ′, 𝐶, 𝑈 , 𝑀, 𝑠𝑡𝑒𝑝, 𝑠𝑖𝑧𝑒, Φ, ˜Φ, 𝑌 )

initialize 𝑝𝑖𝑜𝑛𝑒𝑒𝑟 = Δ, 𝑣𝑎𝑙𝑖𝑑 = 𝑝𝑖𝑜𝑛𝑒𝑒𝑟
initialize 𝑠𝑡𝑒𝑝𝑁𝑢𝑚 = 0
initialize 𝑉 as empty // violated constrain index
while 𝑠𝑡𝑒𝑝𝑁𝑢𝑚 ≤ 𝑠𝑡𝑒𝑝 − 1 do

if 𝑓 ′

𝜃 ′ (𝑀 + 𝑣𝑎𝑙𝑖𝑑) doesn’t equals 𝑌 then
break

end
𝑐ℎ𝑘𝑅𝑠𝑡 = chkIq(Φ, ˜Φ, 𝑀 + 𝑝𝑖𝑜𝑛𝑒𝑒𝑟, 𝐶)
if 𝑐ℎ𝑘𝑅𝑠𝑡 is empty then
𝑣𝑎𝑙𝑖𝑑 = 𝑝𝑖𝑜𝑛𝑒𝑒𝑟
𝑟 = freeStep(𝑓 ′
𝑝𝑖𝑜𝑛𝑒𝑒𝑟 = 𝑣𝑎𝑙𝑖𝑑 + 𝑟
reset 𝑉 to empty

𝜃 ′, 𝑈 , 𝑀 + 𝑣𝑎𝑙𝑖𝑑, 𝑠𝑖𝑧𝑒, 𝑌 )

else

extend 𝑉 with 𝑐ℎ𝑘𝑅𝑠𝑡
define Φ′ = Φ𝑉 // real-time constraints
𝑟 = eqOneStep(𝑓 ′
𝑝𝑖𝑜𝑛𝑒𝑒𝑟 = 𝑣𝑎𝑙𝑖𝑑 + 𝑟

𝜃 ′, 𝐶, 𝑀 + 𝑣𝑎𝑙𝑖𝑑, 𝑠𝑖𝑧𝑒, Φ′, 𝑌 )

end
𝑠𝑡𝑒𝑝𝑁𝑢𝑚 = 𝑠𝑡𝑒𝑝𝑁𝑢𝑚 + 1

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

end
return 𝑣 = 𝑣𝑎𝑙𝑖𝑑

25
26 end

Due to the property of physical systems, the original point 𝑀
will naturally meet all the constraints. As shown in Algorithm 6,
to increase the loss, the original point will first try to move a step
following the gradient direction through the function freeStep
defined in Algorithm 5. Algorithm 5 is very similar to the FGM
algorithm [40] but no perturbation is added to 𝑀𝑈 , namely 𝑟𝑈 = 0,
which is similar to the saliency map function used in [38]. After that,
the new point 𝑀 ′ is checked with equation (4) to find if all inequality
constraints are met. If all constraints were met, the moved step was
valid and we can update 𝑀 = 𝑀 ′. If 𝑀 ′ violates some constraints

in Φ, we will take all the violated constraints and make a real-
time constraint matrix Φ𝑉 , where 𝑉 is the index vector of violated
constraints. We now convert the inequality constraint problem to
the equality constraint problem with the new constraint matrix
Φ𝑉 and the original point 𝑀. 𝑀 will then try to take a step using
the eqOneStep function described in Algorithm 4 with the new
constraint matrix Φ𝑉 . Again, we check whether the new reached
point meets all the constraints. If there are still violated constraints,
we extend 𝑉 with the new violated constraints. The search process
repeats until reaching a valid 𝑀 ′ that meets all the constraints. For
simplicity, we will use 𝑐ℎ𝑘𝑅𝑠𝑡 = chkIq(Φ, ˜Φ, 𝑀 ′, 𝐶) to denote the
checking process of a single search in one step movement, where
𝑐ℎ𝑘𝑅𝑠𝑡 is the index vector of the violated constraints in the search.

Figure 6: Best-Effort Search (linear inequality). We have the
loss function 𝐿(𝑓𝜃 (𝑀), 𝑌 ) = 2𝑥 2 + 2𝑦2 with inequality con-
straints 𝑦 ≤ 2 − 2𝑥.

Similar to Figure 5, a simple example is shown in Figure 6. To
increase the loss, the initial point 𝑎 will take a small step following
the gradient direction and reach point 𝑏. Since 𝑏 meets the con-
straints, it is a valid point. After that, 𝑏 will move a step following
the gradient direction and reach point 𝑐 ′. However, point 𝑐 ′ violates
the constraint 𝛽 and the movement is not valid. As we have point
𝑏 is valid, we construct a linear equality constraint problem with
constraint 𝛼 which is parallel to 𝛽. With constraint 𝛼, point 𝑏 will
move a step to point 𝑐 which is also a valid point. Point 𝑐 then
repeats the search process and increases the loss gradually. The
real-time equality constraint is only used once. When a new valid
point is reached, it empties the previous equality constraints and
tries the gradient direction first.

5 EXPERIMENTAL EVALUATION
We evaluate our ConAML frameworks with two CPS study cases.
The first study case is the ML-based false data injection attack
(FDIA) detection in the power grids to show the impact of phys-
ical linear equality constraints, and the second case is the deep
learning-based anomaly detection in the water treatment system
to demonstrate linear inequality constraints.
Scenarios: For each CPS study, we consider four attack scenarios
regarded to the different knowledge constraints, as summarized

in Table 2. The black-box scenario is the knowledge constraint we
presented in our threat model in subsection 3.2 and is the most
practical scenario for the attacker. We consider different scenarios to
show the impact of different constraints and to study the robustness
of ML models in CPSs under different circumstances. For white-box
and gray-box1 scenarios, the attacker won’t execute Algorithm 1
since the measurements of 𝑀𝑈 are given and there is no knowledge
constraint. We note that attackers under all the scenarios in Table
2 can only modify 𝑀𝐶 and should also follow the physical and time
constraints.

Table 2: Attack Scenarios

Scenario
white-box
gray-box1
gray-box2
black-box

Constraint
know both 𝑀𝐶 and 𝑀𝑈 , has access to 𝑓𝜃
know both 𝑀𝐶 and 𝑀𝑈 , no access to 𝑓𝜃
only know 𝑀𝐶 , has access to 𝑓𝜃
only know 𝑀𝐶 , no access to 𝑓𝜃

Baselines: We set two evaluation baselines in addition to the above
scenarios. The first baseline is a supreme white-box attacker who
has full access to the CPS ML model without considering any con-
straints and utilizes a state-of-the-art AML algorithm [40] to gen-
erate adversarial examples. We compare the performance of the
ConAML framework with the supreme attack to demonstrate the
impact of the constraints. The second baseline the autoencoder
generator proposed in [11] by Erba et al. in 2019. In [11], an au-
toencoder is trained with the normal CPS measurement data and is
expected to learn the physical constraints of measurements. The
adversarial examples are fed to the autoencoder to be transferred
into examples that meet the physical constraints. However, [11]
allows the attacker to know complete 𝑀 (same as the gray-box1
scenario in Table 2), which is less practical compared with our
threat model. Meanwhile, since the patterns are learned by neural
networks (autoencoder), the transferred measurements may not
meet the linear constraints strictly. In our experiment, we show
that the generated examples from the autoencoder may still violate
the physical constraints.
Metrics: The evaluation metrics of ConAML can be different ac-
cording to the attack purpose and the CPS properties. We set three
metrics to evaluate the attack performance in this study. The first
metric is detection accuracy of the defender’s model under attack
and a lower detection accuracy indicates a better attack perfor-
mance. The second metric is the magnitude of the noise injected
to the legitimate measurement. The attacker needs the adversarial
examples to bypass the detection while maintaining their malicious
behavior. A small bad noise will violate the attack’s original inten-
tion even it can bypass the detection. We select the 𝐿2-Norm of the
valid noise vector as the second metric to compare the magnitude
of the malicious injected data. Finally, as the attack needs to be
finished within a sampling period of the CPS, we will compare the
time cost of the adversarial example generation.

5.1 Case Study: State Estimation in Power Grids
5.1.1 Background: State Estimation and FDIA. State estimation is a
backbone of various crucial applications in power system control

-1-0.500.511.52x-1-0.500.511.52y-202468101214(0.2, 0.7)ac'bcd'de'that has been enabled by large scale sensing and communication
technologies, such as SCADA. It is used to estimate the state of
each bus, such as voltage angles and magnitudes, in the power
grid through analyzing other measurements. A DC model of state
estimation can be represented as (6), where x is the state, z is
the measurement, and H𝑚×𝑛 is a matrix that determined by the
topology, physical parameters and configurations of the power grid.

attacker to train their ML models. Through tuning the parameters,
the overall detection accuracy of the defender’s model 𝑓𝜃 is 98.3%
and the attacker’s model 𝑓 ′
𝜃 ′ is 97.5%. After that, we assume there
are 10, 13, and 15 measurements being compromised by the attacker
and simulate the corresponding test datasets that only contain the
false measurements. A more detailed description of the experiment
can be found in Appendix C.2.

z = Hx + e

(6)

Table 3: Evaluation Result Summary

Due to possible meter instability and cyber attacks, bad measure-
ments e may be introduced to z. To solve this, the power system
employs a residual-based detection scheme to remove the error mea-
surements [33]. The residual-based detection involves non-linear
computation (𝐿2-Norm), however, research has shown that a false
measurement vector following linear equality constraints can be
used to pollute the normal measurements without being detected.
In 2009, Liu et al. proposed the false data injection attack (FDIA)
that can bypass the residual-based detection scheme and finally
pollute the result of state estimation [31]. In particular, if the at-
tacker knows H, she/he could construct a faulty vector a that meets
the linear constraint Ba = 0, where B = H(H𝑇 H)−1H𝑇 − I, and
the crafted faulty measurements z + a will not be detected by the
system. A detailed introduction of state estimation, residual-based
error detection, and FDIA can be found in Appendix C.1

Many detection and mitigation schemes to defend FDIA are
proposed, including strategical measurement protection [4] and
PMU-based protection [47]. In recent years, detection based on ML,
especially neural networks, become popular in the literature [3, 21,
23, 36, 37, 44, 46]. The ML-based detection does not require extra
hardware equipment and achieve the state-of-the-art detection
performance. However, in this section, we will demonstrate that
the attacker can construct an adversarial false measurement vector
z𝑎𝑑𝑣 that can bypass both the residual-based detection and the ML-
based detection. The ML models in previous research are trained
to distinguish normal measurement z and poisoned measurement
z + a. Our ConAML algorithms allow the attacker to generate an
adversarial perturbation v that meets the constraint Bv = 0 for
his/her original false measurement z+a and obtain a new adversarial
false measurement vector z𝑎𝑑𝑣 = z + a + v that will be classified as
normal measurements by the ML-based FDIA detection models. The
matrix B then acts as the constraint matrix Φ defined in equation (3).
Meanwhile, z𝑎𝑑𝑣 can naturally bypass the traditional residual-based
detection approach since the total injected false vector a + v meets
the constraint B(a + v) = Ba + Bv = 0. Our experiment in the next
subsection will show that our ConAML algorithms can significantly
decrease the detection accuracy of the ML-based detection schemes.

5.1.2 Experiment Design and Evaluation. We select the IEEE stan-
dard 10-machine 39-bus system as the power grid system as it is
one of the benchmark systems in related research [30, 36]. The
features used for ML model training are the power flow (Ampere)
measurements of each branches. The system has 46 branches so
that there there will be 46 features for the ML models.

The goal of the attacker is to implement a false-negative attack
that makes z𝑎𝑑𝑣 bypass the detections. We utilize the MATPOWER
[50] library to derive the H matrix and simulate related datasets.
We simulate two training datasets for the system operator and the

Attack

Supreme

Erba [11]

white-box

gray-box1

gray-box2

black-box

Case Accu 𝐿2-Norm Time (ms)
4077.43
10
8403.84
13
7979.26
15
1049.52
10
1164.71
13
1578.87
15
2527.8
10
4984.03
13
7029.26
15
2404.76
10
5356.09
13
9133.15
15
2247.21
10
4882.95
13
6610.6
15
1843.2
10
4786.72
13
9079.02
15

5.8
12.9
6.8
5.7
5.84
5.94
42
96.8
52.9
34.2
87.1
7.96
400.25
222.4
126.9
131.9
209.6
163.3

0%
0%
0%
0%
0%
0%
0%
0%
0%
21.1%
48.9%
30.0%
0%
5.4%
8.1%
14.4%
4.3%
28.1%

Table 3 summarizes the detection performance of 𝑓𝜃 under differ-
ent adversarial attacks generated by our ConAML algorithms. From
the table, we can learn that the ConAML attacks can effectively
decrease the detection accuracy of the ML models used for FDIA
detection and inject considerable bad data to the state estimation,
even under black-box scenario. The autoencoder generator methods
[11] can transfer the adversarial examples to follow the manifolds
of the normal measurements (0% detection accuracy). However, the
size of the successful bad data is very smaller compared with the
supreme attack and ConAML, which decreases the effect of the
FDIA attack. In addition, we check the adversarial examples gen-
erated by [11] and find that most of the generated examples (over
90%) of still violate the physical constraints and will be removed by
the residual-based detection in state estimation.

As shown in Figure 7, by comparing the evaluation results of dif-
ferent cases, we can learn that compromising more sensors cannot
guarantee better performances in attack detection. This is due to
the different physical constraints imposed by the system. However,
with more compromised sensors, the attacker can usually obtain a
larger size of the injected bad data.

In our experiments, the time cost of gray-box2 and black-box is
much higher than other attack scenarios due to the universal ad-
versarial measurements algorithm, as shown in Figure 8. However,
the time cost is still efficient for many CPS applications in practice.
For example, the sampling period of the traditional SCADA system
used in power systems is 2 to 4 seconds. In practical scenarios,

Table 4: SWaT Analog Components

Symbol
LIT
FIT
AIT
PIT
DPIT

Description
Level Indication Transmitter
Flow Indication Transmitter
Analyzer Indication Transmitter
Pressure Indication Transmitter
Differential Pressure Ind Transmitter

Unit
𝑚𝑚
𝑚3/ℎ𝑟
𝑢𝑆/𝑐𝑚
𝑘𝑃𝑎
𝑘𝑃𝑎

anomaly detection ML models. Our experiments aims to demon-
strate that the ML models used for anomaly detection are vulnerable
to adversarial attacks. However, due to the physical properties of
the SWaT testbed, the sensor’s measurements are not independent
but with linear inequality constraints.

In our experiment, we consider the scenario that the attacker
compromises the FIT components to inject bad adversarial water
flow measurements. We examined the SWaT testbed structure and
find out that there are apparent linear inequality constraints among
the FIT measurements. The linear inequality constraints of the
seven FIT measurements in the dataset are defined by the structure
of the water pipelines and the placement of the sensors, as shown
in equation 7, where 𝜖1 and 𝜖2 are two constants of the system’s
noise tolerance. We checked the SWaT dataset and observed that
all the normal examples in the dataset meet the constraints. We
also contacted the managers of the SWaT testbed and verified our
find.

FIT301 ≤ FIT201
∥FIT401 − FIT501∥ ≤ 𝜖1
∥(FIT502 + FIT503) − (FIT501 + FIT504) ∥ ≤ 𝜖2

(7a)

(7b)

(7c)

In our experiment, we show that the attacker can construct
adversarial FIT measurements that can bypass the ML anomaly
detection proposed in previous research. Meanwhile, our adver-
sarial measurements will also follow the same linear inequality
constraints to avoid being noticed by the system operator.

5.2.2 Experimental Design and Evaluation. Similar to the power
system study case, we generate two training datasets for the de-
fender’s model 𝑓𝜃 and the attacker’s model 𝑓 ′
𝜃 ′ respectively by poi-
soning the normal measurements with Gaussian noise. The ML
models are trained to distinguish the normal measurement data
and the poisoned measurements (anomaly). In our experiment, the
overall classification accuracy of 𝑓𝜃 and 𝑓 ′
𝜃 ′ is 97.2% and 96.7% re-
spectively. After that, we consider the scenarios that there were 2, 5,
and 7 FIT measurements compromised by the attacker and gener-
ate the related test datasets. The goal of the attacker is to generate
the adversarial FIT measurements with the constraints defined by
equation (7) so that the poisoned measurements can be classified
as ‘normal’ by 𝑓𝜃 . A more detailed introduction of the experiment
design and implementation, including the specific compromised
measurements and the corresponding constraint matrix Φ, can be
found in Appendix D.1 and D.2.

Table 5 summarizes the evaluation performances of different
scenarios of ConAML attacks. From the table, we can learn that

Figure 7: Performance of black-box attacks according to 𝜆
with 𝑠𝑡𝑒𝑝 = 40, 𝑠𝑖𝑧𝑒 = 20.

Figure 8: Time cost of black-box attacks according to 𝜆 with
𝑠𝑡𝑒𝑝 = 40, 𝑠𝑖𝑧𝑒 = 20.

the time cost also depends on the computational resource of the
attacker. With the possible optimization and upgrade in software
and hardware, the time cost can be further reduced.

5.2 Case Study: Water Treatment System
5.2.1 Background: SWaT Dataset. In this section, we study the
linear inequality physical constraints based on the Secure Water
Treatment (SWaT) proposed in [16]. SWaT is a scaled-down system
but with fully operational water treatment functions. The testbed
has six main processes and consists of cyber control (PLCs) and
physical components of the water treatment facility. The SWaT
dataset, generated by the SWaT testbed, is a public dataset to in-
vestigate the cyber attacks on CPSs. The raw dataset has 946,722
samples with each sample comprised of 51 attributes, including the
measurements of 25 sensors and the states of 26 actuators. Each
sample in the dataset was labeled with normal or attack. [16] inves-
tigated four kinds of attacks based on the number of attack points
and places. The detailed description of the SWaT dataset can be
found in [16] and [27].

The SWaT dataset is an important resource to study anomaly
detection in CPSs. Inoue et al. used unsupervised machine learning,
including Long Short-Term Memory (LSTM) and SVM, to perform
anomaly detection based on the SWaT dataset [22]. By comparison,
Kravchik et al. employed Convolutional Neural Networks (CNN)
and achieved a better false positive rate [24]. In 2019, [13] proposed
a data-driven framework to derive invariant rules for anomaly
detection for CPS and utilized SWaT to evaluate their approach.
Other research related to the SWaT dataset can be found in [1, 7, 12].
As shown in Table 4, the SWaT dataset includes the measure-
ments from five kinds of analog components (25 sensors in total)
whose measurements are used as the input features in previous

0.20.40.60.800.10.20.30.40.50.6Detection Accuracy Under AttackCase 10Case 13Case 150.20.40.60.80200040006000800010000L2-Norm Injected Bad DataThe time cost of black-box attacks0.250.350.450.550.650.750.850200400600Time Cost(ms)Case 10Case 13Case 15Table 5: Evaluation Result Summary

Attack

Supreme

Erba [11]

white-box

gray-box1

gray-box2

black-box

Case Accu 𝐿2-Norm Time (ms)
3.24
2.85
4.31
0.176
0.017
0.184
0.741
0.75
1.05
0.522
0.466
0.835
0.52
0.627
0.841
0.309
0.340
0.411

0%
0%
0%
85.3%
86.1%
88.4%
0%
0%
2%
0%
53.2%
89.3%
1.0%
1.2%
1.3%
1.3%
2.3%
1.14%

3.59
6.58
9.21
4.72
4.12
6.18
21.7
24.8
71.5
21.4
51.0
136.4
42.9
94.2
256.1
17.5
111.7
451.8

2
5
7
2
5
7
2
5
7
2
5
7
2
5
7
2
5
7

the ConAML framework can still effectively decrease the detection
accuracy of the ML models, even for black-box attacks. Meanwhile,
even the black-box attack achieves a better performance on both the
detection accuracy and bad data size compared with the baseline
[11]. The size of the injected bad data of the ConAML attacks is
smaller than the supreme attacker. We explain that this is due to
the stringent constraints between the FIT measurements. Similar
to the power system study case, a larger number of compromised
sensors cannot produce a better performance in bypassing the de-
tection. The reason for this result is that more compromised sensors
will also have more complex constraints between their measure-
ments. Meanwhile, more constraints will increase the computation
overhead of the best effort search algorithms since there will be a
‘larger’ constraint matrix.

Figure 9: Performance of black-box attacks according to 𝜆
with 𝑠𝑡𝑒𝑝 = 50, 𝑠𝑖𝑧𝑒 = 0.06.

Figure 9 demonstrated the trend of the detection accuracy and
injected bad data size according to 𝜆. From the figure, we can learn
that, with the 𝜆 increases, the probability of the adversarial ex-
amples being detected also increases. This matches the intuition
that if an adversarial example can obtain higher successful attack
probability with the sampling measurement set, its probability of

evading detection will also increase. Meanwhile, a smaller injected
data size is expected to make the adversarial examples look more
‘normal’ to the detection model.

6 DISCUSSION AND FUTURE WORK
As we mentioned in Section 1, in this paper, we mainly investigate
the linear constraints of input measurements in CPSs and neural
network-based ML algorithms. In the future, research on ConAML
of nonlinear constraints and other general ML algorithms, such as
SVM, KNN will be proposed. We encourage related communities to
present different CPSs that require special constraints.

As summarized in [48], defense mechanisms like adversarial
re-training and adversarial detecting can increase the robustness
of neural networks and are likely to mitigate ConAML attacks.
However, most defenses in previous research target adversarial
examples in computer vision tasks. In future work, we will study
the state-of-the-art defense mechanisms in previous research and
evaluate their performance with adversarial examples generated by
ConAML. We will also investigate the defense mechanisms which
take advantage of the properties of physical systems directly, such
as the best deployment of sensors that will make the attackers’
constraint more stringent.

7 CONCLUSION
The potential vulnerability of ML applications in CPSs need to be
concerned. In this paper, we investigate the input constraints of
AML algorithms in CPSs. We analyze the difference of adversarial
examples between CPS and computational applications, like com-
puter vision, and give the formal threat model of AML in CPS. We
propose the best-effort search algorithms to effectively generate
the adversarial examples that meet the linear constraints. Finally, as
proofs of concept, we study the vulnerabilities of ML models used
in FDIA in power grids and anomaly detection in water treatment
systems. The evaluation results show that even with the constraints
imposed by the physical systems, our approach can still effectively
generate the adversarial examples that will significantly decrease
the detection accuracy of the defender’s ML models.

REFERENCES
[1] Chuadhry Mujeeb Ahmed, Jianying Zhou, and Aditya P Mathur. Noise matters:
Using sensor and process noise fingerprint to detect stealthy cyber attacks and
authenticate sensors in cps. In Proceedings of the 34th Annual Computer Security
Applications Conference, pages 566–581. ACM, 2018.

[2] T Athay, R Podmore, and S Virmani. A practical method for the direct analysis of
transient stability. IEEE Transactions on Power Apparatus and Systems, (2):573–584,
1979.

[3] Abdelrahman Ayad, Hany EZ Farag, Amr Youssef, and Ehab F El-Saadany. Detec-
tion of false data injection attacks in smart grids using recurrent neural networks.
In 2018 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference
(ISGT), pages 1–5. IEEE, 2018.

[4] Suzhi Bi and Ying Jun Zhang. Defending mechanisms against false-data injection
attacks in the power system state estimation. In 2011 IEEE GLOBECOM Workshops
(GC Wkshps), pages 1162–1167. IEEE, 2011.

[5] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David Wagner, and Wenchao Zhou. Hidden voice commands. In
25th {USENIX} Security Symposium ({USENIX} Security 16), pages 513–530, 2016.
[6] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and
Debdeep Mukhopadhyay. Adversarial attacks and defences: A survey. arXiv
preprint arXiv:1810.00069, 2018.

[7] Yuqi Chen, Christopher M Poskitt, and Jun Sun. Learning from mutants: Using
code mutation to learn and monitor invariants of a cyber-physical system. In
2018 IEEE Symposium on Security and Privacy (SP), pages 648–660. IEEE, 2018.

00.20.40.60.8100.050.10.150.2Detection Accuracy Under AttackCase 2Case 5Case 700.20.40.60.810.20.30.40.50.6L2-Norm Injected Bad DataCase 2Case 5Case 7[8] Anna Choromanska, Mikael Henaff, Michaël Mathieu, Gérard Ben Arous, and
Yann LeCun. The loss surface of multilayer networks. CoRR, abs/1412.0233, 2014.
[9] Joel Janek Dabrowski, Ashfaqur Rahman, Andrew George, Stuart Arnold, and
John McCulloch. State space models for forecasting water quality variables:
an application in aquaculture prawn farming. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pages
177–185. ACM, 2018.

[10] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
Jianguo Li. Boosting adversarial attacks with momentum. In Proceedings of the
IEEE CVPR, pages 9185–9193, 2018.

[11] Alessandro Erba, Riccardo Taormina, Stefano Galelli, Marcello Pogliani, Michele
Carminati, Stefano Zanero, and Nils Ole Tippenhauer. Real-time evasion attacks
with physical constraints on deep learning-based anomaly detectors in industrial
control systems. arXiv preprint arXiv:1907.07487, 2019.

[12] Cheng Feng, Tingting Li, Zhanxing Zhu, and Deeph Chana. A deep learning-
based framework for conducting stealthy attacks in industrial control systems.
arXiv preprint arXiv:1709.06397, 2017.

[13] Cheng Feng, Venkata Reddy Palleti, Aditya Mathur, and Deeph Chana. A sys-
tematic framework to generate invariants for anomaly detection in industrial
control systems. In NDSS, 2019.

[14] Illinois Center for a Smarter Electric Grid. IEEE 39-Bus System. https://icseg.iti.

illinois.edu/ieee-39-bus-system/. [Online; accessed 16-Aug-2020].

[15] Amin Ghafouri, Yevgeniy Vorobeychik, and Xenofon Koutsoukos. Adversarial
regression for detecting attacks in cyber-physical systems. In Proceedings of the
Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18,
pages 3769–3775, 7 2018.

[16] Jonathan Goh, Sridhar Adepu, Khurum Nazir Junejo, and Aditya Mathur. A
dataset to support research in the design of secure water treatment systems. In
International Conference on Critical Information Infrastructures Security, pages
88–99. Springer, 2016.

[17] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and har-

nessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[18] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition
with deep recurrent neural networks. In 2013 IEEE international conference on
acoustics, speech and signal processing, pages 6645–6649. IEEE, 2013.

[19] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick McDaniel. Adversarial examples for malware detection. In European
Symposium on Research in Computer Security, pages 62–79. Springer, 2017.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770–778, 2016.

[21] Youbiao He, Gihan J Mendis, and Jin Wei. Real-time detection of false data
injection attacks in smart grid: A deep learning-based intelligent mechanism.
IEEE Transactions on Smart Grid, 8(5):2505–2516, 2017.

[22] Jun Inoue, Yoriyuki Yamagata, Yuqi Chen, Christopher M Poskitt, and Jun Sun.
Anomaly detection for a water treatment system using unsupervised machine
learning.
In 2017 IEEE International Conference on Data Mining Workshops
(ICDMW), pages 1058–1065. IEEE, 2017.

[23] JQ James, Yunhe Hou, and Victor OK Li. Online false data injection attack
detection with wavelet transform and deep neural networks. IEEE Transactions
on Industrial Informatics, 14(7):3271–3280, 2018.

[24] Moshe Kravchik and Asaf Shabtai. Detecting cyber attacks in industrial control
systems using convolutional neural networks. In Proceedings of the 2018 Workshop
on Cyber-Physical Systems Security and PrivaCy, pages 72–83. ACM, 2018.
[25] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the

physical world. arXiv preprint arXiv:1607.02533, 2016.

[26] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning

at scale. arXiv preprint arXiv:1611.01236, 2016.

[27] ITrust Labs. Secure Water Treatment (SWaT) Dataset. https://itrust.sutd.edu.sg/

itrust-labs_datasets, 2019. [Online; accessed 15-08-2019].

[28] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating
adversarial text against real-world applications. arXiv preprint arXiv:1812.05271,
2018.

[29] Lei Li, Chieh-Jan Mike Liang, Jie Liu, Suman Nath, Andreas Terzis, and Christos
Faloutsos. Thermocast: A cyber-physical forecasting model for datacenters. In
Proceedings of the 17th ACM SIGKDD, KDD ’11, pages 1370–1378. ACM, 2011.

[30] Xuan Liu, Zhiyi Li, Xingdong Liu, and Zuyi Li. Masking transmission line outages
via false data injection attacks. IEEE Transactions on Information Forensics and
Security, 11(7):1592–1602, 2016.

[31] Yao Liu, Peng Ning, and Michael K Reiter. False data injection attacks against
state estimation in electric power grids. In Proceedings of the 16th ACM conference
on Computer and communications security, pages 21–32, 2009.

[32] Jiajun Lu, Hussein Sibai, Evan Fabry, and David Forsyth. No need to worry about
adversarial examples in object detection in autonomous vehicles. arXiv preprint
arXiv:1707.03501, 2017.

[33] Alcir Monticelli. State estimation in electric power systems: a generalized approach.

Springer Science & Business Media, 2012.

[34] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: A simple and accurate
method to fool deep neural networks. In 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2574–2582, June 2016.

[35] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. Universal adversarial perturbations. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 1765–1773, 2017.

[36] Xiangyu Niu, Jiangnan Li, Jinyuan Sun, and Kevin Tomsovic. Dynamic detection
of false data injection attack in smart grid using deep learning. In 2019 IEEE
Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT),
pages 1–6. IEEE, 2019.

[37] Mete Ozay, Inaki Esnaola, Fatos Tunay Yarman Vural, Sanjeev R Kulkarni, and
H Vincent Poor. Machine learning methods for attack detection in the smart grid.
IEEE transactions on neural networks and learning systems, 27(8):1773–1786, 2015.
[38] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay
Celik, and Ananthram Swami. The limitations of deep learning in adversarial
settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P),
pages 372–387. IEEE, 2016.

[39] Nedim Rndic and Pavel Laskov. Practical evasion of a learning-based classifier:
A case study. In 2014 IEEE symposium on security and privacy, pages 197–211.
IEEE, 2014.

[40] Andras Rozsa, Ethan M Rudd, and Terrance E Boult. Adversarial diversity and
hard positive generation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops, pages 25–32, 2016.

[41] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Acces-
sorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In
Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security, pages 1528–1540. ACM, 2016.

[42] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv
preprint arXiv:1312.6199, 2013.

[43] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. Deeptest: Automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the
40th international conference on software engineering, pages 303–314. ACM, 2018.
[44] Chenguang Wang, Simon Tindemans, Kaikai Pan, and Peter Palensky. Detection
of false data injection attacks using the autoencoder approach. arXiv preprint
arXiv:2003.02229, 2020.

[45] Allen J Wood, Bruce F Wollenberg, and Gerald B Sheblé. Power generation,

operation, and control. John Wiley & Sons, 2013.

[46] Jun Yan, Bo Tang, and Haibo He. Detection of false data attacks in smart grid with
supervised learning. In 2016 International Joint Conference on Neural Networks
(IJCNN), pages 1395–1402. IEEE, 2016.

[47] Qingyu Yang, Dou An, Rui Min, Wei Yu, Xinyu Yang, and Wei Zhao. On optimal
pmu placement-based defense against data integrity attacks in smart grid. IEEE
Transactions on Information Forensics and Security, 12(7):1735–1750, 2017.
[48] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples: Attacks
and defenses for deep learning. IEEE transactions on neural networks and learning
systems, 2019.

[49] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo Xue. Droid-sec: deep
learning in android malware detection. In ACM SIGCOMM Computer Communi-
cation Review, volume 44, pages 371–372. ACM, 2014.

[50] Ray Daniel Zimmerman, Carlos Edmundo Murillo-Sánchez, and Robert John
Thomas. Matpower: Steady-state operations, planning, and analysis tools for
power systems research and education.
IEEE Transactions on power systems,
26(1):12–19, 2010.

A NON-LINEAR CONSTRAINTS
Many other ML applications in the CPS domains, for instance,
load forecasting in power and water systems, traffic forecasting in
transportation systems, may have nonlinear constraints. The non-
linear constraints can be very complex in various CPSs and cannot
be covered in one study. In general, similar to linear constraints,
the 𝑘 nonlinear constraints of the compromised measurements can
be represented as equation (8), where 𝜇𝑖 is a nonlinear function of
𝑀𝐶 .

𝜇0 (𝑚𝑐0, 𝑚𝑐1, ..., 𝑚𝑐𝑟 −1 ) = 0
𝜇1 (𝑚𝑐0, 𝑚𝑐1, ..., 𝑚𝑐𝑟 −1 ) = 0
...
𝜇𝑘−1 (𝑚𝑐0, 𝑚𝑐1, ..., 𝑚𝑐𝑟 −1 ) = 0
We now investigate a special case of the nonlinear constraints. If
there exists a subset of the compromised measurements, in which





(8)

each measurement can be represented as an explicit function of
the measurements in the complement set, the attacker will also
be able to generate the perturbation accordingly. We use 𝑃 =
[𝑝0, 𝑝1, ..., 𝑝𝑛−1] to denote the index vector of the former measure-
ment set, and use 𝑄 = [𝑞0, 𝑞1, ..., 𝑞𝑟 −𝑛−1] to denote the index vector
of the complement set. We can then represent (8) as (9), where
Ξ = [𝜉0, 𝜉1, ..., 𝜉𝑛−1] is a vector of explicit functions.

𝑚𝑝0 = 𝜉0 (𝑚𝑞0, 𝑚𝑞1, ..., 𝑚𝑞𝑟 −𝑛−1 )
𝑚𝑝1 = 𝜉1 (𝑚𝑞0, 𝑚𝑞1, ..., 𝑚𝑞𝑟 −𝑛−1 )
...
𝑚𝑝𝑛−1 = 𝜉𝑛−1 (𝑚𝑞0, 𝑚𝑞1, ..., 𝑚𝑞𝑟 −𝑛−1 )





Apparently, the roles of 𝑀𝑄 and 𝑀𝑃 in (9) are similar to the 𝑀𝐼
and 𝑀𝐷 in linear constraints correspondingly. Instead of a linear
matrix, the function set Ξ represents the dependency between
𝑀𝑃 and 𝑀𝑄 . The nonlinear constraints make properties such as
Theorem 1 infeasible. To meet the constraints, the attacker needs
to find the perturbation Δ𝑄 first and obtain 𝑀∗
𝑄 by adding it to 𝑀𝑄 .
After that, the attacker can compute 𝑀∗

𝑃 = Ξ(𝑀∗

𝑄 ) .

The above case of nonlinear constraints is special and may not
be scalable to various practical applications. Although there are
different types of nonlinear systems, they can be generalized using
piece-wise linear constraints by setting proper ranges and break-
points. We leave this as an open problem for future work.

B PROOFS
B.1 Theorem 4.1

Proof. If we replace 𝑀∗

𝐶 in equation (3d) with equation (3b),
𝐶 = Φ𝑘×𝑟 (𝑀𝐶 + Δ𝐶 ) = Φ𝑘×𝑟 𝑀𝐶 + Φ𝑘×𝑟 Δ𝐶 = ˜Φ.
we can get Φ𝑘×𝑟 𝑀∗
From equation (3c) we can learn that Φ𝑘×𝑟 𝑀𝐶 = ˜Φ. Therefore, we
□
have Φ𝑘×𝑟 Δ𝐶 = 0 and prove Theorem 4.1.

B.2 Corollary 4.2

Proof. We have Φ𝑘×𝑟 Δ𝐶′ = Φ𝑘×𝑟 (cid:205)𝑛

Φ𝑘×𝑟 Δ𝐶𝑖
we have Φ𝑘×𝑟 Δ𝐶′ = 0 and prove Corollary 4.2.

𝑖=0 𝑎𝑖 ·
is a valid perturbation vector and ΦΔ𝐶𝑖 = 0,
□

𝑖=0 𝑎𝑖 · Δ𝐶𝑖 = (cid:205)𝑛

. Since Δ𝐶𝑖

B.3 Theorem 4.3

Proof. Due to the intrinsic property of the targeted system,
equation (3c) is naturally met, which indicates that there is always a
solution for the nonhomogeneous linear equations Φ𝑘×𝑟 𝑋 = ˜Φ. Ac-
cordingly, we have 𝑅𝑎𝑛𝑘 (Φ𝑘×𝑟 ) ≤ 𝑟 . Moreover, if 𝑅𝑎𝑛𝑘 (Φ𝑘×𝑟 ) = 𝑟 ,
there will be one unique solution for equation (3c), which means
the measurements of compromised sensors are constant. The con-
stant measurements are contradictory to the purpose of deploy-
ing CPSs. In practical scenarios, 𝑀 is changing over time, so that
𝑅𝑎𝑛𝑘 (Φ𝑘×𝑟 ) < 𝑟 and the homogeneous linear equation Φ𝑘×𝑟 𝑋 = 0
will have infinite solutions. Therefore, the attacker can always build
a valid adversarial example that meets the constraints.

□

C POWER SYSTEM CASE STUDY
C.1 State Estimation and FDIA
We give the mathematical description of state estimation and how
a false data injection attack (FDIA) can be launched. To be clear, we
will employ the widely used notations in related research publica-
tions to denote the variables; the corresponding explanation will
also be given to avoid confusion.

In general, the AC power flow measurement state estimation

model can be represented as follow:

(9)

z = h(x) + e

(10)

where h is a function of x, x is the state variables, z is the mea-
surements, and e is the measurement errors. The task of state esti-
mation is to find an estimated ˆx that best fits z of (10). In practical
application, a DC measurement model is also used to decrease the
process time and (10) can then be represented as follow:

z = Hx + e

(11)

where H𝑚×𝑛 is a matrix that determined by the topology, physical
parameters and configurations of the power grid.

Typically, if a weighted least squares estimation scheme is used,

the system state variable vector ˆx can be obtained through (12):

𝑇
ˆx = (H

WH)−1

𝑇
H

Wz

(12)

where W is the covariance matrix of the variances of meter errors.
Due to possible meter instability and cyber attacks, bad measure-
ments may be introduced to the measurement vector z. To solve
this, various bad measurement detection methods are proposed
[33]. One commonly used detection approach is to calculate the
measurement residual between the raw measurement z and de-
rived measurements H ˆx. If the 𝐿2-norm ∥z − H ˆx∥ > 𝜏, where 𝜏 is
a threshold selected according to the false alarm rate, the measure-
ment z will be considered as a bad measurement.

The above detection method contains non-linear computation
(𝐿2-Norm), however, research has shown that a false measurement
vector follows linear equality constraints can be used to pollute the
normal measurements without being detected. In 2009, Liu et al.
proposed the false data injection attack (FDIA) that can bypass the
detection scheme described above and pollute the result of state
estimation [31]. FDIA assumes that the attacker knows the topology
and configuration information H of the power system. Let z𝑎 = z+a
denote the compromised measurement vector that is observed by
the state estimation, where a is the malicious data added by the
attacker. Thereafter, let ˆx𝑏𝑎𝑑 = ˆx + c denote the polluted state that
is estimated by za, where c represents the estimation error brought
by the attack. Liu et al. demonstrated that, as long as the attacker
builds the injection vector a = Hc, the polluted measurements za
will not be detected by the measurement residual scheme.

Proof. If the original measurements z can pass the detection, the
residual ∥z − H ˆx∥ ≤ 𝜏. Through (13) from [31], we learn that the
measurement residual will be the same when a = Hc. Therefore, the
□
crafted measurements from the attacker will not be detected.

∥z𝑎 − H ˆx𝑏𝑎𝑑 ∥ = ∥z + 𝑎 − H( ˆx + c)∥

= ∥z − H ˆx + (a − Hc)∥
= ∥z − H ˆx∥ ≤ 𝜏

(13a)

(13b)

(13c)

Besides, [31] also provided the approach to effectively find vector
𝑎 that will meet the attack requirement. Let P = H(H𝑇 H)−1H𝑇
and matrix B = P − I. In order to have a = Hc, a needs to be a
solution of the homogeneous equation BX = 0, as shown in (14).

a = Hc ⇔ Pa = PHc ⇔ Pa = Hc ⇔ Pa = a

⇔ Pa − a = 0 ⇔ (P − I)a = 0
⇔ Ba = 0

(14a)

(14b)

(14c)

Another problem of generating a is when will (14c) have a so-
lution. Liu et al. prove that, suppose the attacker compromises 𝑘
meters, as long as 𝑘 > 𝑚 − 𝑛, there always exists non-zero attack
vector a = Hc. We refer the readers to [31] for the detailed proof.

C.2 Experiment Implementation

Figure 10: IEEE 39-Bus System [2] [14].

The structure of the IEEE 39-bus system is shown in Figure 10.
We utilize the MATPOWER [50] library to derive the H matrix of
the system and simulate the power flow measurement data. We
also implement the FDIA using MATLAB to generate false measure-
ments. Both the power flow measurements and false measurements
follow Gaussian distributions. We make two datasets for the de-
fender and the attacker respectively. For each dataset, there are
around 25,000 records with half records are polluted with FDIA.
We label the normal measurements as 0 and false measurements as
1 and use one-hot encoding for the labels.

We investigate the scenarios that there are 10, 13, and 15 mea-
surements being compromised by the attacker, with the randomly
generated compromised index vector 𝐶 and corresponding con-
straint matrix Φ (B𝐶 in (14)). We generate 1,000 false measurement
vectors in each test datasets.

After that, we train two deep learning models based on the
training datasets accordingly, with 75% records in the dataset used
for training and 25% for testing. We use simple fully connected
neural networks as the ML models and the model structures are
shown in Table 6. Both the models are trained with a 0.0001 leaning
rate, 512 batch size, a mean squared error loss function, and a
Stochastic Gradient Descent (SGD) optimizer. The deep learning
models are implemented using Tensorflow and the Keras library
and are trained on a Windows 10 machine with an Intel i7 CPU.
The training process is around one minutes for each model.

Table 6: Model Structure - FDIA

Layer
0
1
2
3
4
5
6
7
8
9

𝑓
46 Input
32 Dense ReLU
48 Dense ReLU
56 Dense ReLU
48 Dense ReLU
32 Dense ReLU
Dropout 0.25
16 Dense ReLU
Dropout 0.25
2 Dense Softmax

𝑓 ′
46 Input
30 Dense ReLU
40 Dense ReLU
30 Dense ReLU
Dropout 0.25
20 Dense ReLU
Dropout 0.25
2 Dense Softmax
-
-

D WATER TREATMENT CASE STUDY
D.1 SWaT Measurement Constraints
We examined the user manual of the SWaT system and check the
structure of the water pipelines. We found some FIT measure-
ments in SWaT should always follow inequality constraints when
the whole system is working steadily. Based on the component
names described in [16], the constraints can be represented as (15),
where 𝜖1 and 𝜖2 are the allowed measurement errors. We utilized
the double value of the maximum difference of the corresponding
measurements in the SWaT dataset to estimate 𝜖1 and 𝜖2, and we
had 𝜖1 = 0.0403 and 𝜖2 = 0.153.

FIT301 ≤ FIT201
∥FIT401 − FIT501∥ ≤ 𝜖1
∥(FIT502 + FIT503) − (FIT501 + FIT504) ∥ ≤ 𝜖2

(15a)

(15b)

(15c)

Based on (4), we can represent (15) as follow. And 𝑀𝐶 is the vector
of measurements of FIT201, FIT301, FIT401, FIT501, FIT502,
FIT503 and FIT504 accordingly.

Φ5×7 =

−1
0
0
0
0

0
1
0
1
0 −1
0
0
0
0

0
−1
1
−1
1

0
0
0
0
0
0
1
1
−1 −1












0


0


0


−1


1



˜Φ =

0


0.0403


0.0403


0.153


0.153














We consider three scenarios that there are 2, 5, and 7 FIT mea-
surements being compromised by the attacker, and the compro-
mised sensors are {FIT201, FIT301}, {FIT401, FIT501, FIT502,
FIT503, FIT504}, and all the seven FIT sensors respectively. The
constraint matrix of each scenario can be derived from the corre-
sponding rows of the Φ5×7 matrix.

D.2 Experimental Implementation
In the Swat dataset, we extracted the normal records which were
sampled when the whole system was working steadily. We also
removed all the actuators’ features. Here, we denote the extracted
records as 𝐷𝑒 . After that, we randomly picked out three test datasets
from 𝐷𝑒 as the with each test dataset contains 1000 records. We
added Gaussian noise to the compromised measurements of records
in all test datasets. We checked the polluted record every time when
a noise vector was added to ensure all the records in test datasets
meet the linear inequality constraints. Here, we denote the rest
records of 𝐷𝑒 as 𝐷𝑡𝑟𝑎𝑖𝑛 which contains 120,093 records with each
record having 25 features in our implementation. We randomly and
equally split 𝐷𝑡𝑟𝑎𝑖𝑛 into 𝐷𝑑𝑒 𝑓 𝑒𝑛𝑑𝑒𝑟 and 𝐷𝑎𝑡𝑡𝑎𝑐𝑘𝑒𝑟 for the defender
and attacker respectively and pollute half records with normally-
distributed random noise in 𝐷𝑡𝑟𝑎𝑖𝑛 and 𝐷𝑑𝑒 𝑓 𝑒𝑛𝑑𝑒𝑟 . The polluted
records in 𝐷𝑑𝑒 𝑓 𝑒𝑛𝑑 and 𝐷𝑎𝑡𝑡𝑎𝑐𝑘𝑒𝑟 are labeled with 1 and the rest
with 0. We allow the records in 𝐷𝑡𝑟𝑎𝑖𝑛 and 𝐷𝑑𝑒 𝑓 𝑒𝑛𝑑 with label 1 to
violate the constraints since the ML models are also expected to
detect the obviously anomalous measurements.

We utilize 𝐷𝑑𝑒 𝑓 𝑒𝑛𝑑 and 𝐷𝑎𝑡𝑡𝑎𝑐𝑘 to train the ML models 𝑓𝜃 and
𝑓 ′
𝜃 ′ for the defender and attacker respectively. Again, 75% records in
the both datasets were used for training the 25% records for testing.
Similar to the FDIA experiment, we utilize fully connected neural
networks and the structures are shown in Table 7. Through param-
eter tuning, model 𝑓𝜃 and 𝑓 ′
𝜃 ′ achieves 97.2% and 96.7% accuracy
respectively.

Table 7: Model Structure - Water Treatment

Layer
0
1
2
3
4
5
6
7

𝑓
25 Input
20 Dense ReLU
40 Dense ReLU
30 Dense ReLU
Dropout 0.25
20 Dense ReLU
Dropout 0.25
2 Dense Softmax

𝑓 ′
25 Input
24 Dense ReLU
32 Dense ReLU
32 Dense ReLU
16 Dense ReLU
2 Dense Softmax
-
-

