IJCAI-21 1st International Workshop on Adaptive Cyber Defense

1
2
0
2

g
u
A
1
3

]

R
C
.
s
c
[

1
v
6
6
0
0
0
.
9
0
1
2
:
v
i
X
r
a

Informing Autonomous Deception Systems with Cyber Expert Performance Data

Maxine M. Major1 , Brian J. Souza1 , Joseph DiVita1 and Kimberly J. Ferguson-Walter2
1Naval Information Warfare Center Paciﬁc
2Laboratory for Advanced Cybersecurity Research
{mmajor, bsouza}@niwc.navy.mil, joseph.divita@navy.mil, kimberly.j.ferguson-walter.civ@mail.mil

Abstract

The performance of artiﬁcial intelligence (AI) al-
gorithms in practice depends on the realism and
correctness of the data, models, and feedback (la-
bels or rewards) provided to the algorithm. This
paper discusses methods for improving the realism
and ecological validity of AI used for autonomous
cyber defense by exploring the potential to use In-
verse Reinforcement Learning (IRL) to gain insight
into attacker actions, utilities of those actions, and
ultimately decision points which cyber deception
could thwart. The Tularosa study, as one exam-
ple, provides experimental data of real-world tech-
niques and tools commonly used by attackers, from
which core data vectors can be leveraged to inform
an autonomous cyber defense system.

1 Introduction
One of the most pervasive problems in technology today
is that given enough time and resources, a cyber attacker
nearly always has the advantage over a defender. If persis-
tent, they will ultimately unveil a weakness that can then be
exploited. The difﬁculty of maintaining complex networks
has historically constrained defenders to be truthful about the
topology and conﬁguration of networks, which has had un-
intended consequences that beneﬁt malicious actors. How-
ever, given the option of using cyber deception, a defender
can level the playing ﬁeld by casting doubt on the attacker’s
understanding of the networks they wish to attack. The con-
cept of adding deception to cyber defense has been discussed
for decades [49][52] [39], with recent experiments proving
the effectiveness of various deception techniques [22][4][45].
However, with the addition of this new layer of detection and
mitigation techniques come a new set of challenges. With
an established deception ruleset, cyber adversaries may learn
what kinds of deception to expect, and in turn learn to avoid
it. As attackers continuously discover ways to work around
or avoid the deceptive defenses, cyber defenders must in
turn adapt and change the defenses in continuous attempts
to thwart attacks.

An autonomous cyber deception solution can ease the bur-
den on defenders and make intelligent, timely decisions on
their behalf. A real-time, adaptive deception system, which

can ingest inputs from defensive and deceptive sensors, can
learn from adversarial activity in order to track and respond
with customized deception to deter, delay, or misinform po-
tential attackers [27]. Based on the defender’s own terrain,
this system would naturally be limited to data that is available
to the defender, either through their own Intrusion Detection
System (IDS) or security systems, or through deception tools
such as decoys or tripwires planted in the network. Addi-
tionally, prior research and statistical analysis about attacker
behavior — including how attack patterns change in the pres-
ence of deception — can inform such models.

A major component driving the decisions for the type and
placement of deception is to learn the reward for taking a par-
ticular deceptive action, which in turn is based on the utility
of that action - which requires a predictive look-ahead at the
probabilities of the attacker taking a particular course of ac-
tion. This requires an intelligent cognition of the current state
of defense, awareness of which data have been discovered
and interacted with by an attacker, and what the attacker’s
own likely action rewards and utilities might drive them to
do next. This paper explores the data available to a defender
to learn the current state of attack and to estimate the possi-
ble rewards and utilities of both the attacker and a deceptive
response, with the goal of using this information to inform
an autonomous cyber deception defense. The paper draws on
observations from the Tularosa study where red teamer be-
havior was compared in a large scale cyber deception exper-
iment [24]. Network and host actions were recorded and be-
haviors and beliefs about their own performance and decep-
tion were self-reported. We discuss how insights from data
of this type can be used for modeling attacker and defender
rewards for autonomous defense systems.

2 Background & Related Work
The concept of using deception to gain an advantage over
the enemy is not a new one—deception has been employed
throughout history to great wartime advantage. Modern cy-
ber deception can be used for early threat detection, and to
understand and subvert the cyber enemy. Advances in de-
ception techniques have demonstrated that cyber deception
is applicable across all attack surfaces [16]. Along with ad-
vancements made in automating cyber defense, a real-time,
autonomous deception system to respond to an unseen adver-
sary’s advances is a natural technological progression.

 
 
 
 
 
 
2.1 Autonomous Defenses
Cyber defense is a multi-faceted role, and a human defender
is limited in their ability to monitor all threat vectors si-
multaneously. As a result, the goal of increased automa-
tion is quite prevalent in cyber defense technologies. Au-
tonomous cyber defense also requires automating decision
making, which is well-suited for artiﬁcial intelligence (AI)
solutions—reinforcement learning (RL) in particular, can ad-
dress many of the challenges. Using RL to select various cy-
ber defense policies has been shown to improve cyber risk,
compared to a random selection of valid actions [5].
In-
telligent automation can also learn to detect and automati-
cally patch cyber threats, as demonstrated by several of the
ﬁnal round solutions in the DARPA Cyber Grand Challenge
(CGC) [15] and RL scenarios defending against a random-
ized adversary [42]. Furthermore, it is clear that adaptive au-
tomation which can keep up with changes in technology and
attack strategies is ideal for the ever-evolving technological
landscape [12].

2.2 Deception Technologies
Deception is becoming a mainstream feature for many com-
mercial cyber defense tools, along with their standard IDS
feature set. One strategy is to scan the existing network
topology, including users, services, and assets, to design de-
coys or decoy subnets which mimic the features of that net-
work [13] [14] [46]. Due to the proprietary nature of com-
mercial technologies, comparing the robustness of the under-
lying deception technologies is difﬁcult. Much work remains
to be done to make these tools truly autonomous; While a few
claim to automatically adapt their deception strategy based on
observed malicious activity, many of these act based on preset
rules or have limited adaptability, and several still use default
deception conﬁgurations or require a defender’s hands-on as-
sistance to implement new deception strategies [44]. Ad-
vancements in optimizing the type and placement of cyber
deception, along with removing the burden of management
from the defender, will improve the robustness of cyber de-
ception platforms and the state of cyber defense overall.

2.3 Defensive Deception Research
Deception in a cyber context aims to disrupt cyber attacks by
presenting false information to an attacker, which they can
believe or act upon. Deception encompasses more than sim-
ple honeynets and virtual machines meant to distract attack-
ers; they can be classiﬁed into four categories; conceal facts,
reveal ﬁctions, reveal facts, and conceal ﬁctions, as per the
Cyber Denial & Deception (D&D) Matrix [32].

Ongoing research aims to produce new methods for de-
tecting and mitigating cyber threats through dynamic, in-
telligent cyber deception, based on attacker activity.
IDS
alerts have been used in order to launch pre-conﬁgured hon-
eynets [3]. On a host-level, machine-learning and malware
behavioral analysis techniques can turn attackers’ own mal-
ware deception back onto them, altering the attacker’s deci-
sion making [43]. A prototype called KAGE used software
deﬁned networking (SDN) to establish an “alternate reality”
among distributed systems, deploying deceptive counterparts

to attacker-desired services in real time, based on pattern-
matching rules [47].

Game theory has often been used to model the interaction
between the cyber attacker and defender and investigate the
effect of deception on attacker performance [7][9][19][37].
Human subjects research (HSR) can both complement and
expand the scope of cyber game theory by including the cog-
nitive decision-making biases of the attacker [29][35].

Notable research evaluated different non-deceptive defense
techniques against an active attacker in a cyber security sim-
ulation modeled as a Markov game with incomplete infor-
mation. Techniques evaluated include Monte Carlo and Q-
Learning, with the best performance from those able to adapt
to the changing attack and defense scenario [18]. Cyber de-
ception strategies similarly need to adapt to changes in pace
with the attack strategy. Attacker proﬁles developed via an
attacker taxonomy which inferred their goals and skill level,
were pitted against simulated cyber deception to evaluate the
difference in how attackers of varying skill level are affected
by deception [31].

Although automation seems ideal for effective cyber de-
fense including defensive deception, we note that this could
lead to an over-reliance on the system by a human defender,
thus weakening the effectiveness of the defender, and other
"ironies of automation" [28]. Further research will need to
examine how to maximize the beneﬁts for, and minimize the
overhead on human defenders.

2.4 The Human Attacker
Considering deception to be primarily a cyber defense tech-
nology helps to scope the problem space, but is limiting due
to the nature of the cyberpsychology behind deception. De-
ception has an impact on the target’s cognitive state, and thus
defensive cyber deception has the potential to learn about and
impact the decision-making human behind the attack, in or-
der to to improve predictiveness and effectiveness. Behav-
ioral psychology has demonstrated the ability to completely
ﬂip the outcome of risk-aversion versus risk-seeking, by ma-
nipulating the context, or frame, under which the decision is
made. Because of this, human decision-making may be ma-
nipulated by controlling the context of the decision [36][50].
If a cyber defender can frame a decision that the attacker
must make, using behavioral science research to predict with
great likelihood the outcome of the attacker’s decision, then
the defender may anticipate the attacker’s next move and gain
an exploitable advantage. The most meaningful actions an at-
tacker takes toward their goal likely occur on the defender-
owned domain, and these actions progressively build upon
prior work performed toward those goals. Each action in
the cyber attack can be decomposed into incremental actions
within stages of a cyber attack, such as the stages of the Cyber
Kill Chain [10]. By explicitly discovering the decisions, and
actions relevant to each detectable stage of an attack, decep-
tion may be used to impact that decision. Attacker motivation
is a difﬁcult problem to solve. In behavioral research, deci-
sions under conditions of uncertainty are posed in terms of
the probabilities of receiving a reward or penalty [36]. In the
context of a cyber attack, the reward would refer to the per-
ceived value of the target to the attacker. The probability of

receiving this reward corresponds to the probability of suc-
cess the attacker associates in carrying out the attack. This
comparison suggests two immediate research questions (both
of which are impacted by deception):

1. How does an attacker determine the value of a target?
I.e., what attributes and signals from a target do attackers
use to determine the value of attacking that target?

2. How do attackers evaluate the probability of launching a
successful attack? I.e., what attributes and signals from
a target do attackers use to determine the vulnerabilities
and likelihood of successfully compromising a target?

By learning these attributes and features, experimentation
may be designed to test their effect on the attacker’s deci-
sion making, and to predict whether the attacker’s decisions
exhibit risk-seeking or risk-averse behavior. For attacker be-
havior that conforms to these predictions, deception can be
deployed to bait the attacker into making a series of decisions
that the defender anticipates and further exploits. Effective
deception can be used to manipulate an attacker’s assessment
of a target’s value, the attacker’s perception of the likelihood
of achieving their goals against that target, and then strategi-
cally incorporate these strategies into cyber defense systems.
While most adaptive systems utilizing AI require the abil-
ity to reason under uncertainty, when utilizing deception in
an adversarial scenario, the system will likely need to repre-
sent the attacker’s incorrect perception of the network that the
defender’s deception is providing to the attacker. This can be
modeled as a hypergame [21].

Information about attackers’ internal motivations and be-
liefs is elusive to capture in the wild. However, experimenta-
tion providing cyber expert performance data, such as the Tu-
larosa Study [24], can provide a rare glimpse into the cogni-
tion of hacker-like populations: professional red-teamers and
penetration testers, which is discussed in more detail in the
following Section 2.5.

2.5 The Tularosa Study

The Tularosa Study was a controlled human-subjects re-
search (HSR) experiment which aimed to gain insight into
the human side of attacker strategies and beliefs. For two
days, professional red team hackers were given a cyber hack-
ing exercise on identical copies of a simulated network [24].
Participants were either presented a network with decoys
(deception-present condition), were only told that there might
be deception (psychological condition, "reveal ﬁctions"),
both ("conceal & reveal ﬁctions"), or neither (for the con-
trol condition). Data collected from this experiment includes
cyber data from sources that would only be visible to the at-
tacker (keylog, screen recordings), data usually accessible to
the defender (network trafﬁc, IDS alerts), and data only avail-
able when deception is present (decoy alerts). Additional data
which would not be available in the wild, but can help charac-
terize attacker behavior and improve defenses includes: cog-
nitive tests, self-reports, and demographics data. Results from
the cognitive and self-report data helps demonstrate decep-
tion’s impact to both cyber and psychological performance,

reinforces the utility of defensive deception, and illustrates
deception’s impact on attacker thoughts and behaviors.

Data analysis reveals that deception — both cyber and psy-
chological — impacts the attacker’s actual ability to make
progress toward compromising real targets [22]. Cognitive
data analysis reveals that red teamers who believed in decep-
tion were more likely to express confusion, self-doubt, and
become frustrated during this task [20]. The information from
this experiment supports the development of a rewards model
and autonomous deception solution via two main contribu-
tions: 1) provide context for how defender data can inform
deception responses and provide feedback with respect to the
effectiveness of those responses on an attacker, and 2) inform
the development of an attacker model and implicitly measure
the attacker’s response to that deception.

Throughout this paper, references will be made to data and
insights gleaned from the Tularosa Study—as an example–
demonstrating how observations gained from experiments
can support the understanding of how deception impacts hu-
man attacker cognition, supports the development of a real-
istic reward function, and assists in developing attacker pro-
ﬁles. Further details on the study can be found in previous
publications [23][24][25].

Data points collected from the defensive, deceptive envi-
ronment which can be used to detect attackers and moni-
tor for deception’s effectiveness are discussed further in Sec-
tion 3. A method which can be used to infer attacker decision
points and beliefs, and derive a reward function which can
help drive autonomous deception solutions, is Inverse Rein-
forcement Learning (IRL), detailed in Section 4.

3 Data for Adaptive Cyber Deception
In the real-world, cyber defenders typically do not have visi-
bility into the attacker’s goals, strategies, beliefs, skills and
individual techniques leveraged to accomplish their nefari-
ous deeds. Early stages of attack—initial reconnaissance in
particular—tend to be quiet; the defender often learns about a
skilled, successful attacker during the later stages of an attack,
if at all. Even after an attack has fully succeeded, the defender
can only make guesses about the attacker’s true goals and
skills by piecing together artifacts from the attacker’s tracks
after the fact. This is a result of extremely limited data avail-
able for a defender to detect malicious actors or evaluate the
results of deception using only data gleaned from their own
network and resources.

Cyber deception provides a unique advantage for defenders
to gain early alerting of cyber attacks through simple meth-
ods such as canaries and other tripwires, and information on
attacker preferences and behaviors through honeypots, hon-
eytokens, and decoys. There also exists a psychological com-
ponent by which the defender gains the potential to direct the
attacker’s own actions by leveraging intelligent, adaptive de-
ception to alter the attacker’s beliefs about the network and
their own success. Standard defender-available network and
host data can directly support the model and decisions made
by an intelligent deception response tool.

The following sections discuss types of standard informa-
tion available to a cyber defender, which, along with knowl-

edge of known attacker attributes, can model a feature space
of networked resources (targets), actions available to both at-
tackers and deceptive defenders, a model of the state space,
and aspects of creating a custom proﬁle for the attacker.

3.1 Feature Space
With full visibility into network trafﬁc activity, and limited
visibility into host-based activity, along with certain types of
deception, such as decoys and false network packets, defend-
ers can create a custom feature space to gather attacker activ-
ity and respond with deception. The feature space for mod-
eling a deceptive response consists of practical information
that is readily available to the defender, and with which the at-
tacker is likely to interact with or use in executing their strate-
gies. This information includes data about the network and
hosts, but also includes activities detected on the network and
how deception or adversaries might alter properties of these
features. Ultimately, the goal is for an autonomous defen-
sive deception capability to implement and optimize the de-
ception’s effectiveness by continuously evaluating malicious
interactions on these resources.

We present a small subset of the possible features in or-
der to highlight the process in which experimental results can
be used to inform intelligent defensive systems, to ensure the
learning is based on models and rewards that are grounded in
reality. For the simple rewards model presented in this paper,
we consider features in Table 1. This highly-simpliﬁed col-
lection of network and host knowledge only represents a very
narrow subset of thousands of unique features available from
any given host which may be impacted by attacker activity.

Feature Category

Feature

Domain host properties

Domain accounts

IP_address, OS, is_decoy,
services, OS_ver,
svc_versions, user_account

user_accounts, is_decoy,
priv_level, password,
pwd_hash

Interconnectivity

node_IP, conn_type

Vulnerabilities

Host metadata

service, svc_version, svc_vuln

purpose, value

Network Activity

traffic_alert, IDS_alert

Host Activity

Metadata

HBSS_alert, local_alert

num_hosts, num_decoys,
num_adversaries,
time_since_alert

Table 1: Features: A simpliﬁed set of characteristics of the network
and hosts which a deceptive defender can monitor for changes, or
about which to implement deception. Features also include metadata
to model the current attack state.

Each time an adversary interacts with or modiﬁes one of
these resources, or when a deception is applied to that re-
source, these changes will be logged as part of that particular
state. At a minimum, each of these features can be treated as

a tripwire, while nuanced changes and interactions with these
features can track a target’s interest, strategies, and overall re-
sponse to deception. For example, often attackers who gain a
foothold on a target will attempt to do so through leveraging
an existing user account, ideally one with administrator priv-
ileges, or attempt to escalate privileges to administrator level.
In the Tularosa experiment, several participants attempted
to use an existing domain admin account, however, partic-
ipants with deception present—who were also informed of
deception—attempted to use a compromised domain admin
account less than participants in the control condition [22].
Not only does this demonstrate that attackers may alter or
dampen their attack in the presence of deception, but that
knowing about the deception may cause reduced or altered
interactions with these features.

Data on the patterns of domain user account usage is al-
ready provided via defender-owned resources, which can also
help a cyber deception capability track the effectiveness of the
current deception response. The host information is simple in
this example, but other more nuanced host-based evidence of
malicious activity such as tampering with system ﬁles or the
start of novel processes can also reveal the presence of an
attacker, and is the focus for much ongoing research toward
novel threat detection.

Cyber defenders normally leverage network and host-
based monitoring to detect and observe adversarial activ-
ity. Common tools such as an IDS are designed to gener-
ate alerts for suspicious network trafﬁc, however, tracking
the frequency and severity of these alerts can also reveal pat-
terns about adversarial behaviors. As an example, Tularosa
Study participants with deception present sent less bytes to
real targets, and less packets overall. The popular Eternal-
Blue exploit yielded success on several real targets, however,
Tularosa participants whose EternalBlue exploit attempts on
real targets were detected by the Suricata IDS yielded twice
as many alerts in the deception-absent condition as those with
deception-present, and the least number of alerts were trig-
gered by participants who were also informed of deception.

By tracking these fundamental features of both real and
deceptive resources, and through careful manipulation of the
properties of these resources, deception can provide not only
early warning of adversarial interactions, but overall patterns
of behavior and changes in response to both real and decep-
tive features on the network.

3.2 Action Space

Cyber defenders and attackers have diametrically opposed
goals, and this is reﬂected in the diversity of actions and
strategies utilized by each. Attackers aim to discover, ex-
ploit, and claim ownership while remaining undetected; de-
fenders want to protect resources, preserve ownership, pre-
vent exploitation, and reduce the threats posed by attackers
when possible. Deception includes the added goals of delay-
ing, deterring, and polluting the knowledge of the attacker.
Each of the attacker’s and defender’s actions can be consid-
ered to occur in relation to the feature types listed in Table 1,
along with timestamped metadata such as properties added,
modiﬁed, or changed. Evaluating a history of these actions

over time helps to ﬂesh out implicit values for those features,
learn the most effective deception response, and curate a pro-
ﬁle of the attacker, as depicted in Section 3.4. While it may
be difﬁcult to deﬁnitively learn the intrinsic motivations and
skills of an attacker, the best place to start is by using data
that can reveal evidence of the attacker’s actions, and through
that data, predict the attacker’s preferences during an attack.

Attacker Action Space
The actions of an attacker are loosely inspired by the com-
monly adopted, high-level cyber attack sequence model, the
Cyber Kill Chain [33]. The actions depicted in Table 2 de-
scribe categories of activity that an attacker may take, which
roughly correspond to the order of actions taken in the Cy-
ber Kill Chain, but are simpliﬁed for the sake of focusing on
activities to which a defender may discover and respond.

Adversarial Action

Description

do_nothing

passive_recon

active_recon

vuln_search

explore_service

exploit

actions_target

Null or “’wait“. The state has not
changed, or no action recommended.

Observe network trafﬁc, identify net-
work hosts and host roles. Not de-
tected unless HBSS monitoring for
this activity resides on the host being
used for passive recon.

Probe individual hosts to learn spe-
ciﬁc features, e.g., OS, ports, ser-
vices, and service versions.

Search for weaknesses on known ser-
vices. Can be performed “ofﬂine” on
the attacker’s own machine, or via a
vulnerability scan, which is visible by
the defender.

Discover information not included in
scans which does not require an ex-
ploit or login, e.g., web pages, or
unauthenticated services (e.g. telnet).

Exploit a vulnerability to allow entry
or obtain advanced permissions.

Malicious activity after foothold ob-
tained, e.g., exﬁltration, backdoors,
sabotaging resources.

Table 2: Adversarial Action Space: Simpliﬁed list of high level
adversarial actions.

Although theoretically all actions in Table 2 are available
to an attacker with a foothold, not all actions are equally valu-
able for the state the attacker is in. For example, actions
which provide information, e.g., active_recon, should be
of higher value than exploit, if launching an exploit from
the current state is likely to fail if vulnerabilities are not
known. If an attacker has not yet scanned the network, then
passive_recon would have more utility than vuln_search
on targets whose identities are unknown. However, despite
the fact that these actions seem logically taken in an orderly
sequence, once the attacker has gained minimal information
from the target domain, even as early as the passive_recon

stage, depending on the attacker’s goals, strategies, and even
skill set, the utility of taking any one action on any number of
possible targets can make it difﬁcult to predict the likely next
actions taken, and thus the best deception to mitigate these
actions. Toward predictive modeling of an attacker’s likely
next move, based on prior knowledge of attacker actions, an
autonomous deceptive agent could be ideal for tracking and
responding predictively to these patterns of activity.

While our description is based on the premise that every at-
tacker activity will be detected by defenders, this is purely for
the purpose of demonstrating that deception can be effective
against every attacker action, detectable or not. Highly skilled
cyber actors often avoid detection by acting under the guise
of normal trafﬁc and user activity. However, as we see in Ta-
ble 3, several types of deception can serve as traps to alert the
defender to the type of early stage or covert adversarial ac-
tivity that would normally go undetected. In order to identify
the types of deception that are appropriate to deploy against
potential attackers, a defender should consider the types of re-
sources in their domain, and plan counter-deception strategies
against the same attack tools and techniques that are com-
monly used against those types of targets. Automatic net-
work discovery tools, commonly found bundled with com-
mercial IDS products, could partner with detailed attack tax-
onomies to reduce overhead for the defender. For example,
the ATT&CK [40] matrix outlines very speciﬁc tools and
techniques that could be taken by an attacker, and catego-
rizes them along an expanded attack chain which includes
details about tools or exploits that may be used, any ﬁles al-
tered, and residual effects in the target system. ATT&CK
is leveraged by tools including Atomic Red Team [8], and
CALDERA™ [11] to emulate and automate a given cyber
actor or campaign. These, and similar resources, can support
additional research to learn and deﬁne the attack space, the
feature space for each attack technique, and ultimately sup-
port deceptive responses targeting these actions.

Defensive Deception Action Space
The actions available to a deceptive defender fall roughly into
one of three categories: 1) actions typically available to the
cyber defender’s role, which pertain to normal day-to-day ac-
tivity, such as monitoring and maintenance, 2) non-deceptive
responses to suspicious or malicious activity, such as black-
listing malicious IPs, and 3) actions speciﬁc to deception,
which complement defenses by adding deceptive features and
strategies. Rather than enumerating the types of defensive
actions, or classifying as per the Cyber D&D Matrix [32],
here, we consider a simpliﬁed list of data-collecting activities
which support deception-based actions in Table 3. For a list
of non-deceptive active responses we direct readers to consult
the Open Command and Control (OpenC2) Language Spec-
iﬁcation [34] which provides 32 actions, none of which are
deception-speciﬁc.

The deception action space is where the potential to really
disrupt and learn from a cyber attacker comes into play [27].
Deception does not need to be complicated; the Tularosa
Study was limited to static, low-interaction decoys. During
scans the decoys appeared to be legitimate Windows or Linux
hosts with a few believable services running. This created

delays for attackers who wasted time investigating, search-
ing for vulnerabilities, or attempting to exploit these hosts.
By deﬁnition, any time spent interacting with decoys is time
wasted, as the presence and properties of these decoys pro-
vided no beneﬁt toward compromising real hosts on the net-
work. Although the red teamers eventually moved on to ﬁnd
other targets that responded more predictably, the result of
wasting time on these false targets resulted in less IDS alerts,
less attempts to use a domain admin account, less exﬁltrated
ﬁles, and less success on real hosts overall [22]. From the per-
spective of wasted time alone, static, low-interaction decoys
were more effective than no deception at all.

Table 3 includes a list of potential deception actions which
can be used for either initial deception or dynamically mod-
iﬁed for continuous disruption of a given attacker’s goals.
Several of the deception actions itemized in Table 3 address
multiple attacker actions from Table 2, since a single type
of deception, particularly those early in the Kill Chain, has
the potential for lingering effects throughout the attack life-
cyle. For example, a ping_responder in place, can serve as
an early warning for quiet reconnaissance techniques while
convincing the attacker that “something” resides at that net-
work location. With this early response, deceptive tools such
as traffic_control or TCP_reset can slow or drop net-
work scans, allowing for launch_decoy to plant false tar-
gets deeper into the subnet before scanning can complete, or
to even create false subnets (or a sandbox) customized with
similar targets to those the attacker is known to probe. The
actions listed are the highest-level description of actions that
will typical have a hierarchical structure. For example once
the launch_decoy action is selected by the AI, the conﬁg-
uration of the decoy will also need to be determined such as
the OS, patch level, open ports, and services running. Details
from the how the participants responded to particular decoys
in the Tularosa Study, for example, can be used to build ini-
tial attacker proﬁles. This can help determine which decoy
conﬁguration is optimal given the defender’s goal in relation
to manipulation of the attacker.

With tools such as portspoof [17], the properties of both
real and decoy targets can be falsely presented, which disrupts
the attacker’s active_recon, vuln_search, and exploit
activity. High interaction decoys can disrupt the attacker’s
progress further, with working exploits and a fully-modeled
OS for actions_target to succeed on a false target. Any
information gained about the targets, users, properties of the
network and the objectives for the attack can be polluted at
every stage, to give the attacker a false sense of success. Even
if the attacker learns of deception, untangling which features
of the target network are deceptive wastes attacker time and
resources, and gives a defensive observer more knowledge
about the attacker’s response to deception.

High-interaction decoys and responsive deception tactics
have the potential to magnify disruption to the attacker. Other
taxonomies of deception actions exist to guide the creation of
a more robust deception action space [47][41].

3.3 State Space
For an autonomous solution to dynamically determine which
deception feature to employ, it will have to calculate the util-

Defensive Actions

Description

do_nothing

Null or “’wait“. The state has not
changed, or no action recommended.

monitor_IDS

Evaluate network intrusion alerts

monitor_HBSS

Evaluate host intrusion alerts

Deception Action

Description

launch_decoy

ping_responder

portspoof

Creation and deployment of custom
decoys. Complementary actions in-
clude creating a decoy_user ac-
count or plant_creds on a real ma-
chine to lure attacker to this decoy.

Respond to ping request, even if no
target exists at that address.

Falsify ports and services on real or
decoy hosts.

falsify_response Falsify a success or fail message for a
login or information retrieval attempt

traffic_control

TCP_reset

create_user

plant_creds

Change speed of trafﬁc, resulting in a
delay or frustration for attacker.

Drop network connection. Delays or
deters (redirects) attacks.

Create a false user proﬁle to lure the
attacker toward decoy services.

Plant fake credentials to waste at-
tacker’s time, or working credentials
to lure attacker towards a decoy.

Table 3: Defender’s Action Space: Simpliﬁed list of defensive (ob-
servational) and deceptive actions to plant false information and dis-
rupt an attacker’s progress.

ity of all possible actions given the current state. This state
is deﬁned by the actions taken until this point in time, and
the features of the current state. The probability of transition-
ing from one state to the next will drive the type and place-
ment of the deception. These transition probabilities and their
calculations over {state, action} pairs are expanded upon in
Section 4. Different features will hold different values during
each state for the attacker, and will drive their actions for-
ward. The defender may not know or share those same val-
ues, and it is up to the AI algorithm to determine those values
based on prior patterns of behavior and the attacker proﬁle.

A simple example of adversarial state transitions is de-
picted in Figure 1. Without the presence of deception, given
enough tries, the attacker will achieve their objectives. A de-
ceptive defender’s simple action transition is depicted in Fig-
ure 2, however the decisions to refresh the state will depend
heavily on observations of the target network and the AI al-
gorithms making deception decisions.

3.4 Adversary Proﬁling
The observable network features and attacker and defender
actions discussed previously can provide a stream of infor-
mation about current and prior states to an autonomous de-
ception system. In particular, metadata about features and ac-

Figure 1: Attack State Transition: Extremely simpliﬁed action-
transition diagram for a cyber adversary, with no deception present.

Figure 2: Deception State Transition: Action-transition diagram
for a deceptive defender.

tions that are deﬁnitively associated with the attacker’s activ-
ity can provide contextual detail toward proﬁling inferences
about the attacker and their skills, goals, and tactics, which
can in turn inform customized deception responses to exploit
the attackers feelings and beliefs. Some of these metadata
data are depicted in Table 4.

Characteristic

Description

Target interactions

Attack duration

The level of interaction, and repeated
interactions

Time since ﬁrst alert, time to each
success/failure

Stealth

Alert severity and frequency

Table 4: Attacker Proﬁle: Metadata about ground truth activities
can inform inferences about the attacker’s goals and strategies.

Information which can be inferred about the attacker re-
quires support from prior research and relies heavily on cog-
nitive observations of adversarial activity[1][30]. Data from
observable behavior needs to be translated through the lens
of foundational research in order to deduce implicit charac-
teristics. For example, patterns of adversarial activity can re-
veal possible associations to known APTs, as depicted in the
ATT&CK framework [40]. Some inferred characteristics of
an attacker are listed in Table 5.

Proﬁling changes in attack strategy relies far less on infer-
ence than measuring an attacker’s beliefs or cognitive state
in response to deception. While the Tularosa Study demon-
strated increased confusion and frustration among partici-
pants experiencing deception, the exact reason was not de-
tailed in the ﬁndings [24].

Tularosa participants were surveyed for cognitive abilities
that might be relevant to the task, via well-established sur-
veys and tests to measure overall cognitive ability, ﬂuid in-
telligence, decision-making conﬁdence, operational memory,
and convergent creative thinking. Personality assessments in-

Inferred Data

Description

Sentience

Is the adversary human or auto-
mated? (e.g., a bot or scripted attack)

Number of attackers How many unique adversaries are
present? Are they working collabo-
ratively? Competitively?

Expertise

Skills, experience, and possibly de-
mographic (e.g. nation state)

Deception-Aware

Is the attacker aware of deception?

Emotional State

Confusion, Frustration, Self-Doubt

Goal

Strategy

Threat level

Speciﬁc target, attack purpose

Tools, techniques, attack patterns

Based on expertise and goals, how
dangerous is this adversary?

Fingerprint

Previously seen? APT ﬁngerprinted?

Table 5: Inferred Attacker Proﬁle Information about the attacker
that can be determined implicitly via attack patterns and external
research.

cluded the Big Five Inventory, the General Decision-Making
Style Inventory, the Indecisiveness Scale, and the Need for
Cognition [24]. While the answers to these assessments
are not something expected to be available on attackers in
the wild, they could be combined with existing research on
known cyber attacker proﬁles to help construct attack proﬁles
linked to cyber behavior which can be observed by defenders.

4 Reward Modeling with IRL
Deception can be leveraged as a cyber attack deterrent, but
optimizing the placement and type of deception requires more
than a “set and forget” deception policy. This is where au-
tonomous solutions can shine, by strategically taking in data
on the state of an attack and learning the attacker’s belief
in the utility of their own actions to enable predictive and
iteratively-optimized deception. RL requires a reward func-
tion that is able to provide feedback to an agent based on the
actions it selects and the states it arrives in, so it can learn the
best policy [48]. However, in a cyber scenario, the actions of
the attacker cannot be directly controlled, only observed, and
the expected utility of responding to those actions relies on
the attacker’s prior pattern of behavior. Engineering a mean-
ingful reward function for such a complex domain is not a
simple task, however, RL can be used to calculate a suitable
reward function based on modeled behavior. IRL has already
demonstrated effectiveness in capturing expert human route
planning [53] and mimicking human ﬁne motor control in
robots [26]. The standard approach to IRL can be difﬁcult
to implement when working with static datasets, however,
batch IRL with counterfactuals has been used on static health-
care data to understand the decision making of expert doc-
tors. Introducing counterfactuals to batch IRL allows for off-
policy evaluations and considers what would happen if differ-
ent actions were taken at given observation points [6]. Thus,
IRL provides potential to reverse engineer rewards from static

datasets such as the Tularosa Study, and also has application
to learn from cyber defense data on a network in real-time.

Understanding how humans learn to achieve a goal can
help construct the components of an attack learning model.
When humans perform apprenticeship learning, they do not
usually mimic the exact behavior they are trying to learn [2].
Rather, humans observe behavior and presume the intent,
then “copy” the intention in the sense that their actions ac-
complish the same goal of the behavior they are trying to
learn, but the actions themselves are not necessarily exact
replicas of what they have observed [51]. This variability in
actions suggests that human behavior is directed by a “soft
optimality” as opposed to a strictly deterministic, maximal
policy for behavior. Probabilistic graphical models can model
the soft optimality of human behavior; inference in these
models quantiﬁes the probability that an observed trajectory
of state and actions pairs was accomplished by a human act-
ing optimally. The probability that a human is behaving opti-
mally for a given state-action pair is modeled with a Bernoulli
random variable p(Ot|st, at) = exp(r(st, at)), where r(s, a)
is the reward associated for that state-action pair. The most
optimal trajectories are the most likely, whereas suboptimal
trajectories are exponentially less likely as a function of re-
wards associated with state action pairs that comprise those
trajectories.

Traditionally, goal directed behavior is modeled by RL,
where state space, s ∈ S, and action space, a ∈ A, are de-
ﬁned such that an action a, taken in state s, may change the
current state to a new state, s(cid:48). These changes may be speci-
ﬁed by transition probabilities p(s(cid:48)|s, a), which give the prob-
ability of a transition to state s(cid:48) from state s when action a is
taken in state s. These transition probabilities may be given
or observed. We also must deﬁne a reward function r(s, a).
The goal of RL is to learn an optimal policy π∗(a|s), which
speciﬁes what action to take in each state so as to maximize
the expected value of the reward.

In general, a reward function may be difﬁcult to specify;
however, data associated with performing a given action may
successfully provide the opportunity to use IRL techniques
to capture that reward function, which can then be used to
re-optimize the reward function with forward RL algorithms.
For example, in the Tularosa Study the reward function that
is driving the experts’ behavior is unknown, but data provides
states, actions, and sample trajectories τi observed from par-
ticipant data. Without knowing the optimal policy, we may
assume that we have observed a close approximation of the
optimal policy through observing the behavior of many ex-
perts. From this, the reward function is modeled as rψ(s, a),
where ψ is a vector that parameterizes the reward that the
expert’s policy optimized to produce the observed set of tra-
jectories, τi. The reward function may be parameterized in
several ways, for example, as a linear reward function as a
weighted sum of features associated with a state-action pair,
rψ(s, a) = (cid:80)
i ψifi(s, a). Alternatively, a neural network
with parameter vector ψ may be used to specify the reward
function. IRL holds a strong potential to capture the reward
function from the observed trajectories of behavior performed
by an expert. Instead of trying to learn the probability of an
attack action given a reward, which amounts to inference in

the graphical probabilistic model, IRL uses the same graph-
ical model to learn the reward function. By observing the
given trajectories, we can learn the parameters of the rewards
so that the likelihood of the observed trajectories in the graph-
ical model are maximized [38].

The data from the Tularosa Study provides observations
of human experts performing a cyber-attack task. Often the
reward functions use for cyber security domains can seem ar-
bitrary and it is unclear as to whether they will transfer to
actual human cyber-attack behavior. If instead, IRL can be
used to learn an attacker’s reward function from experimental
(e.g., Tularosa), capture-the-ﬂag (CTF), or real-world data,
then this could: 1) be applied to an attacking RL agent that
will help train an autonomous defender and 2) be used to infer
the reward function of the defender (assuming zero-sum) and
used directly to train an autonomous deceptive defender. Fur-
ther work can also include new HSR experimentation focused
on defender actions used in conjunction with IRL to measure
the defender’s reward function directly.

5 Discussion
While the current conversation around intelligent cyber de-
fense is often centered around the discovery of malicious
or suspicious activity, this is primarily a reactive measure.
Armed with defensive deception along with visibility into
network and host data, a defender can proactively detect,
monitor, and neutralize attacks at every stage. There is an in-
creasing need for AI applied to cyber defense to handle timely
and effective deployment of reactive and proactive responses,
as well as to customize the deceptive responses to the needs
of the defender and the learned proﬁles of the attacker.

In this paper we present a simpliﬁed set of features avail-
able to a cyber defender, which include deceptive actions,
each of which can learn to adapt in real time to a malicious ac-
tor. Examples of the effectiveness of deception as presented
in the Tularosa Study serve to showcase that these data can
be measured to evaluate—or support the use of—this type of
deception. Attacker proﬁling is another layer of continually-
learned metadata collected by the defender’s own sensors,
and can be used to inform type and placement of deception to
disrupt an attacker’s decision points and pollute their forward
progress. A strong reward function is the backbone of any au-
tonomous solution. We provide an example of how learning
from data inputs via IRL can reveal the attacker’s own reward
function, which can help inform the attacker proﬁle, and the
defender’s deceptive response.

Through this research we aim to instill an understand-
ing of how and why insights from real-world and HSR cy-
ber attacker data are necessary components to guide the ad-
vancement of intelligent defense systems. Autonomous cy-
ber deception systems can leverage HSR and attack data to
learn adversarial reward functions, and exploit this knowl-
edge through optimized deception placement and conﬁgura-
tion. Future work entails implementing IRL algorithms on
Tularosa data, as well as utilizing real-world data or design-
ing new HSR to investigate defensive decision points to infer
how experts counter and remediate attacks.

Acknowledgments
This work was partially funded by Cyber Technologies,
C5ISREW Directorate, Ofﬁce of the Under Secretary of De-
fense Research and Engineering as well as the Laboratory for
Advanced Cybersecurity.

References
[1] Yasaman D. Abbasi, Noam Ben-Asher, Cleotilde Gonza-
lez, Debarun Kar, Don Morrison, Nicole Sintov, and Milind
Tambe. Know your adversary: Insights for a better adversarial
behavioral model. In Cognitive Science, 2000.

[2] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via
inverse reinforcement learning. In Proceedings of the Twenty-
First International Conference on Machine Learning, ICML
’04, New York, NY, USA, 2004. ACM.

[3]

Jaime C. Acosta, Anjon Basak, Christopher Kiekintveld,
Nandi Leslie, and Charles Kamhoua. Cybersecurity Deception
Experimentation System. In 2020 IEEE Secure Development
(SecDev), pages 34–40, Atlanta, GA, USA, September 2020.
IEEE.

[4] Palvi Aggarwal, Aksh Gautam, Vaibhav Agarwal, Cleotilde
Gonzalez, and Varun Dutt. HackIT: A human-in-the-loop sim-
ulation tool for realistic cyber deception experiments. In Tareq
Ahram and Waldemar Karwowski, editors, Advances in Hu-
man Factors in Cybersecurity, pages 109–121, Cham, 2020.
Springer International Publishing.

[5] Luc Beaudoin. Autonomic Computer Network Defence Using
Risk States and Reinforcement Learning. PhD thesis, Univer-
sity Of Ottawa, 2009.

[6]

Ioana Bica, Daniel Jarrett, Alihan Hüyük, and Mihaela
van der Schaar. Learning ”what-if” explanations for sequen-
tial decision-making. In International Conference on Learning
Representations, 2021.

[7] Mark Bilinski, Kimberly J. Ferguson-Walter, Sunny J. Fugate,
Ryan Gabrys, Justin Mauger, and Brian J. Souza. You only
lie twice: A multi-round cyber deception game of question-
able veracity. In T. Alpcan, Y. Vorobeychik, J. Baras, and Dan
G., editors, Decision and Game Theory for Security. GameSec
2019, volume 11836 of Lecture Notes in Computer Science,
pages 115–126, Cham, 2019. Springer International Publish-
ing.

[8] Red Canary. Atomic Red Team. https://atomicredteam.io/,

Accessed: 4 Mar. 2021.

[9] Thomas E. Carroll and Daniel Grosu. A Game Theoretic In-
vestigation of Deception in Network Security. In 18th Inter-
national Conference on Computer Communications and Net-
works, pages 1–6, San Francisco, CA, USA, August 2009.
IEEE.

[10] Lockheed Martin Corporation.

Chain®.
us/capabilities/cyber/cyber-kill-chain.html, Accessed:
Aug. 2021.

The Cyber Kill
https://www.lockheedmartin.com/en-
06

[11] MITRE

Corporation.

CALDERA™.

https://github.com/mitre/caldera, Accessed: 25 Feb. 2021.
[12] Jennifer Crawford. The Impact of Artiﬁcial Intelligence on Au-
tonomous Cyber Defense. Capstone Project, Utica College,
2017.

[13] Fidelis Cybersecurity.

Fidelis Deception,

https://ﬁdelissecurity.com/products/deception/,
4 Mar. 2021.

2021.
Accessed:

[14] CYBERTRAP. CYBERTRAP: Deception Technology from

Austria. https://cybertrap.com/en/, Accessed: 4, Mar. 2021.

[15] Defense Advanced Research Projects Agency(DARPA). Seven
Teams Hack Their Way to the 2016 DARPA Cyber Grand
Challenge Final Competition, July 2015.

[16] Kyle Dickinson.

Implementer’s Guide to Deception Tech-
nologies. SANS Institute Information Security Reading Room,
page 16, 2020.

[17] Piotr Duszy´nski. Portspoof. http://drk1wi.github.io/portspoof/,

Accessed: 06 Aug. 2021.

[18] Richard Elderman, Leon J. J. Pater, Albert S. Thie, Madalina
M. Drugan, and Marco M. Wiering. Adversarial Reinforce-
ment Learning in a Cyber Security Simulation:. In Proceedings
of the 9th International Conference on Agents and Artiﬁcial In-
telligence, pages 559–566, Porto, Portugal, 2017. Science and
Technology Publications (SCITEPRESS).

[19] Xiaotao Feng, Zizhan Zheng, Prasant Mohapatra, and Derya
Cansever. A Stackelberg Game and Markov Modeling of Mov-
ing Target Defense. In Stefan Rass, Bo An, Christopher Kiek-
intveld, Fei Fang, and Stefan Schauer, editors, Decision and
Game Theory for Security, volume 10575, pages 315–335.
Springer International Publishing, Cham, 2017.

[20] Kimberly J. Ferguson-Walter. An Empirical Assessment of the
Effectiveness of Deception for Cyber Defense. PhD thesis,
University of Massachusetts Amherst, Feb 2020.

[21] Kimberly J. Ferguson-Walter, Sunny J. Fugate, Justin Mauger,
and Maxine M. Major. Game theory for adaptive defensive
cyber deception. ACM Hot Topics in the Science of Security
Symposium (HotSoS), March 2019.

[22] Kimberly J. Ferguson-Walter, Maxine M. Major, Chelsea. K.
Johnson, and H. Muhleman, Daniel. Examining the efﬁcacy of
decoy-based and psychological cyber deception. In USENIX
Security Symposuim, April 2021.

[23] K.J. Ferguson-Walter, M.M. Major, D.C. Van Bruggen, S.J.
Fugate, and R.S. Gutzwiller. The world of CTF is not enough
data: Lessons learning from a cyber deception experiment. In
Proceedings of First IEEE Workshop on Human Aspects of Cy-
ber Security (HACS), 2019.

[24] K.J. Ferguson-Walter, T.B. Shade, A.V. Rogers, E.M. Nied-
bala, M.C. Trumbo, K. Nauer, K. Divis, A.P. Jones, A. Combs,
and R.G. Abbott. The Tularosa Study: An Experimental De-
sign and Implementation to Quantify the Effectiveness of Cy-
ber Deception. In Hawaii International Conference on System
Sciences (HICSS), 2019.

[25] K.J. Ferguson-Walter, T.B. Shade, A.V. Rogers, M.C. Trumbo,
K. Nauer, K. Divis, A.P. Jones, A. Combs, and R.G. Ab-
bott. Appendix to The Tularosa Study: An Experimental
Design and Implementation to Quantify the Effectiveness of
Cyber Deception, 2019. https://cfwebprod.sandia.gov/cfdocs/
CompResearch/docs/TularosaAppendix.pdf.

[26] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided Cost
Learning: Deep Inverse Optimal Control via Policy Optimiza-
tion. Proceedings of The 33rd International Conference on
Machine Learning (ICML), 48:49–58, 2016.

[27] Sunny Fugate and Kimberly Ferguson-Walter. Artiﬁcial In-
telligence and Game Theory Models for Defending Critical

Networks with Cyber Deception. AI Magazine, 40(1):49–62,
March 2019.

[42] Ahmad Ridley. Machine learning for autonomous cyber de-

fense. The Next Wave, 22:7–14, 2018.

[43] Md Sajidul Islam Sajid, Jinpeng Wei, Md Rabbi Alam, Ehsan
Aghaei, and Ehab Al-Shaer. DodgeTron: Towards Au-
tonomous Cyber Deception Using Dynamic Hybrid Analysis
In 2020 IEEE Conference on Communications
of Malware.
and Network Security (CNS), pages 1–9, Avignon, France,
June 2020. IEEE.
[44] TrapX Security.

Active Defense:
ForeScout CounterACT®,

TrapX® De-
2017.

ceptionGrid® and
https://www.forescout.com/company/resources/active-
defense-trapx-deceptiongrid-and-forescout/, Accessed:
Mar. 2021.

4

[45] Temmie B. Shade, Andrew V. Rogers, Kimberly J. Ferguson-
Walter, Sara Beth Elsen, Daniel Fayette, and Kristin E. Heck-
man. The Moonraker Study: An Experimental Evaluation of
In Hawaii International Conference
Host-Based Deception.
on System Sciences (HICSS), Maui, Hawaii, January 2020.

[46] Smokescreen.

Smokescreen Deception Technology, 2021.

https://www.smokescreen.io, Accessed: 4 Mar. 2021.

[47] Nathaniel Soule, Partha Pal, Shane Clark, Brian Krisler, and
Anthony Macera. Enabling defensive deception in distributed
system environments. In 2016 Resilience Week (RWS), pages
73–76, Chicago, IL, USA, August 2016. IEEE.

[48] Richard S. Sutton and Andrew G. Barto. Reinforcement Learn-
ing: An Introduction. MIT Press, Cambridge, MA, 2nd edition,
2017.

[49] L. Tinnel, O. S. Saydjari, and D. Farrell. Cyberwar strategy and
tactics an analysis of cyber goals , strategies, tactics, and tech-
In IEEE Workshop on Information Assurance, June
niques.
2002.

[50] A. Tversky and D. Kahneman.

Judgment under Uncer-
tainty: Heuristics and Biases. Science, 185(4157):1124–1131,
September 1974.

[51] F. Warneken. Altruistic Helping in Human Infants and Young

Chimpanzees. Science, 311(5765):1301–1303, March 2006.

[52] J. Yuill, D. Denning, and F. Feer. Using Deception to
Hide Things from Hackers. Journal Of Information Warfare,
5(3):26–40, 2006.

[53] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and
Anind K. Dey. Maximum Entropy Inverse Reinforcement
Learning. In Proceedings of the Twenty-Third AAAI Confer-
ence on Artiﬁcial Intelligence, pages 1433–1438, 2008.

[28] Robert Gutzwiller and Dirk Van Bruggen. Human Factors in
Automating Cyber Operations. In Hawaii International Con-
ference on System Sciences, 2021.

[29] R.S. Gutzwiller, K.J. Ferguson-Walter, and S.J. Fugate. Are
cyber attackers thinking fast and slow? Evidence for cognitive
biases in red teamers reveals a method for disruption. Proceed-
ings of the Human Factors and Ergonomics Society (HFES)
Annual Meeting, 2019.

[30] R.S. Gutzwiller, K.J. Ferguson-Walter, S.J. Fugate, and A.V.
Rogers. ’Oh, Look, A butterﬂy!’ A framework for distracting
In Human Factors and
attackers to improve cyber defense.
Ergonomics Society (HFES), 2018.
[31] Sharif Hassan and Ratan Guha.

A probabilistic study
on the relationship of deceptions and attacker skills.
In
2017 IEEE 15th Intl Conf on Dependable, Autonomic and
Secure Computing, 15th Intl Conf on Pervasive Intelli-
gence and Computing, 3rd Intl Conf on Big Data Intelli-
gence and Computing and Cyber Science and Technology
Congress(DASC/PiCom/DataCom/CyberSciTech), pages 693–
698, 2017.

[32] K.E. Heckman, F.J. Stech, R.K. Thomas, Be. Schmoker, and
A.W. Tsow. Cyber Denial, Deception and Counter Decep-
tion: A Framework for Supporting Active Cyber Defense. Ad-
vances in Information Security. Springer International Publish-
ing, 2015.

[33] E.M. Hutchins, M.J. Cloppert, and R.M. Amin. Intelligence-
driven computer network defense informed by analysis of ad-
versary campaigns and intrusion kill chains. Leading Issues in
Information Warfare & Security Research, 1(1):80, 2011.

[34] Jason Romano and Duncan Sparrell, Eds.

Open com-
mand and control (OpenC2) language speciﬁcation version
1.0. https://docs.oasis-open.org/openc2/oc2ls/v1.0/cs02/oc2ls-
v1.0-cs02.html, Nov 2019. OASIS Committee Speciﬁcation
02.

[35] Chelsea K. Johnson, Robert S. Gutzwiller, Kimberly J.
Ferguson-Walter, and Sunny J. Fugate. A cyber-relevant table
of decision making biases and their deﬁnitions. ResearchGate,
2020.

[36] Daniel Kahneman and Amos Tversky. Prospect Theory: An
Analysis of Decision under Risk. Econometrica, 47(2):263,
March 1979.

[37] Nicholas S. Kovach, Alan S. Gibson, and Gary B. Lamont.
Hypergame Theory: A Model for Conﬂict, Misperception, and
Deception. Game Theory, 2015:1–20, August 2015.

[38] Sergey Levine. Reinforcement Learning and Control as Proba-
bilistic Inference: Tutorial and Review. arXiv:1805.00909 [cs,
stat], May 2018.

[39] J.B. Michael, N.C. Rowe, H.S. Rothstein, T.C. Wingﬁeld,
M. Auguston, and D. Drusinsky. Phase II report on intelli-
gent software decoys: intelligent software decoy tools for cy-
ber counterintelligence and security countermeasures. Techni-
cal Report NPS-CS-04-001, 2004.

[40] MITRE. MITRE ATT&CK®. https://attack.mitre.org/, Ac-

cessed: 25 Feb. 2021.

[41] Changwook Park and Young-gab Kim. Deception Tree Model
In 2019 International Conference on
for Cyber Operation.
Platform Technology and Service (PlatCon), pages 1–4, Jeju,
Korea (South), January 2019. IEEE.

