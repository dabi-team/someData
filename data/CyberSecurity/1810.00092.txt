The Partially Observable Games We Play for Cyber Deception

Mohamadreza Ahmadi, Murat Cubuktepe, Nils Jansen, Sebastian Junges
Joost-Pieter Katoen and Ufuk Topcu

8
1
0
2

p
e
S
8
2

]
I

A
.
s
c
[

1
v
2
9
0
0
0
.
0
1
8
1
:
v
i
X
r
a

Abstract— Progressively intricate cyber inﬁltration mecha-
nisms have made conventional means of defense, such as
ﬁrewalls and malware detectors, incompetent. These sophisti-
cated inﬁltration mechanisms can study the defender’s behavior,
identify security caveats, and modify their actions adaptively. To
tackle these security challenges, cyber-infrastructures require
active defense techniques that incorporate cyber deception, in
which the defender (deceiver) implements a strategy to mislead
the inﬁltrator. To this end, we use a two-player partially
observable stochastic game (POSG) framework, wherein the
deceiver has full observability over the states of the POSG,
and the inﬁltrator has partial observability. Then, the deception
problem is to compute a strategy for the deceiver that minimizes
the expected cost of deception against all strategies of the
inﬁltrator. We ﬁrst show that the underlying problem is a
robust mixed-integer linear program, which is intractable to
solve in general. Towards a scalable approach, we compute
optimal ﬁnite-memory strategies for the inﬁltrator by a reduc-
tion to a series of synthesis problems for parametric Markov
decision processes. We use these inﬁltration strategies to ﬁnd
robust strategies for the deceiver using mixed-integer linear
programming. We illustrate the performance of our technique
on a POSG model for network security. Our experiments
demonstrate that the proposed approach handles scenarios
considerably larger than those of the state-of-the-art methods.

I. INTRODUCTION
“If deception1 can be used for cyber-attacks, can it also
be used for defense?”. This question was raised by the
renowned hacker, Kevin Mitnick, in his famous book The
Art of Deception [25]. The reason for asking for such
advanced cyber-defense mechanisms is the ever-escalating
progress of cyber-inﬁltration [28]. Furthermore, the proliﬁc
reliance of governments, industries, and individuals on cyber-
infrastructure, thanks to the growth of applications of AI and
machine learning tools, makes them particularly attractive for
cyber-terrorism and cyber-crime [16].

Deception is a familiar technique for war zone strate-
gists [7] and for cyber-inﬁltrators or hackers [8]. In cyber-
inﬁltration, for example, deception can be achieved by
changing malware signatures, social engineering, concealing
codes, and encrypting exploits. On the other hand, deception

M. Ahmadi, M. Cubuktepe, and U. Topcu are with the Department of
Aerospace Engineering and Engineering Mechanics, University of Texas,
201 E 24th St, Austin, TX 78712, USA. N. Jansen is with the Department of
Software Science, Radboud University Nijmegen, Comeniuslaan 4, 6525 HP
Nijmegen, Netherlands. S. Junges and J-P. Katoen are with the Department
of Computer Science, RWTH Aachen University, Templergraben
({mrahmadi,mcubuktepe,
55,
utopcu}@utexas.edu,n.jansen@science.ru.nl,{sebastian.junges,
katoen}@cs.rwth-aachen.de).

52062 Aachen, Germany.

e-mail:

1Deceiving means to “deliberately cause (someone) to believe something
that is not true, especially for personal gain” according to the Oxford English
Dictionary.

defense strategies can use deceits and feints to thwart an
inﬁltrator’s cognitive processes, disrupt the breach process,
and delay inﬁltration activities. Such deception can be car-
ried out via misleading, obfuscations, and fake responses.
These methods rely on the inﬁltrator’s belief in the network
responses and data. For instance, honeypot servers (fake
servers that mimic actual servers) are commonly used to
actively detect malicious activity and reveal the inﬁltrator’s
strategies [27]. From a cyber-deception standpoint, two fac-
tors, namely, the amount of deception and the frequency of
deception, characterize the cost of implementing a deception
mechanism.

Cyber-deception can be mathematically formalized as a
non-cooperative two-player dynamic game [1], [24], [29],
which can represent the adversarial sequential decision mak-
ing nature of a deception/inﬁltration scenario and the limited
cyber-defense/inﬁltration resources. In particular, games with
imperfect information can represent the information asymme-
try, which is at the heart of cyber-deception, i.e., the deceiver
has full knowledge over the cyber-infrastructure, such as
servers, but the inﬁltrator does not. Recently, in [18], the
authors proposed a modeling framework for cyber-deception
in terms of a one-sided partially observable Markov decision
processes-a special class of partially observable Markov
decision processes (POSGs), where one player has full ob-
servability and the second one does not. Despite this unique
modeling paradigm, POSGs are notoriously intractable to
solve in general [15]. Three approximate methods have been
proposed in the literature for solving POSGs either by a
memory bounded representation of the value function [11],
by approximating it by a series of smaller, related Bayesian
games using heuristics [21], or by using heuristic search
value iteration [17].

For POSGs, the optimal strategies for the deceiver may
require inﬁnite-memory, or they may require unbounded-
memory. To obtain a tractable formulation, we use ﬁnite-
memory strategies for the inﬁltrator. Using ﬁnite memory,
we have a compact representation of the inﬁltrator strategies.
Then, we search for strong ﬁnite-memory inﬁltrator strategies
that induces a low cost for the inﬁltrator. The set of all ﬁnite-
memory inﬁltrator strategies is uncountable, and the measure
of the ﬁnite-memory strategies that induces a low cost for
the inﬁltrator may be small compared to the set of all ﬁnite-
memory strategies. Therefore, methods such as evolutionary
algorithms or Bayesian methods may not be applicable in this
setting. We deﬁne the inﬁltrator strategy synthesis problem
as a synthesis problem in parametric MDPs [14], and we use
our previous work [6] to synthesize strategies in a parametric

 
 
 
 
 
 
case wherein the inﬁltrator is detected and therefore we can
choose either actions “engage” or “block”.

The arrows illustrate the transitions in the game character-
ized by a transition function T . We assume all transitions are
deterministic, except transitions from d = false to d = true.
The inﬁltrator starts the attack at a computer outside the
network xA = 0. The inﬁltrator attempts to access computers
in deeper layers by compromiseing them. He/she also has
the option to wait at any layer, takedown, which reveals
the presence of the inﬁltrator and the inﬁltrator is forced to
exit the network and restart the attack. Another course of
action is to incur small amount of damage by exf iltrateing
of data, which does not attract the deceiver’s attention.

The inﬁltrator does not receive observations for detection
state d, but is aware of xA (the network Layer being inﬁl-
trated). The deceiver has no information over the inﬁltrator
until the inﬁltrator has been detected.

III. PROBLEM STATEMENT

In this section, we ﬁrst lay the formal foundations for
POSGs, followed by a formal problem statement together
with the underlying optimization problem.

A. Foundations

A probability distribution over a ﬁnite or countably inﬁnite
set X is a function µ : X → [0, 1] ⊆ R with
Px∈X µ(x) =
µ(X) = 1. The set of all distributions on X is Distr (X).
The support of a distribution µ is supp(µ) = {x ∈
X | µ(x) > 0}. A distribution is Dirac if |supp(µ)| = 1.
Let V = {x1, . . . , xn} be a ﬁnite set of variables over the
real numbers R. The set of multivariate polynomials over V
is Q[V ]. An instantiation for V is a function u : V → R.

Deﬁnition 1 (SG) A stochastic game (SG) is a tuple G =
(S◦, S(cid:3), sI, Act, P) with a ﬁnite set S = S◦ ∪S(cid:3) of states, a
set S◦ of Player 1 states, a set S(cid:3) of Player 2 state, the initial
state sI ∈ S, a ﬁnite set Act of actions, and a transition
function P : S × Act → Distr(S). We deﬁne costs using a
state-action cost function C : S × Act → R≥0.

A Markov decision process (MDP) is an SG in which
S◦ = ∅, and consequently S = S(cid:3). A path of an SG G is
a1−→ s, where s0 = sI,
an (in)ﬁnite sequence π = s0
si ∈ S, ai ∈ Act, and P(si, ai)(si+1) 6= 0 for all i ∈ N.
For ﬁnite π, last(π) denotes the last state of π. The set of
(in)ﬁnite paths of G is PathsG

a0−→ s1

ﬁn (PathsG).

Deﬁnition 2 (SG strategy) A strategy σ for G is a pair σ =
(σ1, σ2) of functions σi : {π ∈ PathsG
ﬁn | last(π) ∈ Si} →
Distr (Act) such that for all π ∈ PathsG
ﬁn , {a | σi(π)(a) >
0} ⊆ Act
.

last(π)
(cid:1)
(cid:0)

A Player-i strategy σi (for i ∈ {1, 2}) is memoryless if
last(π) = last(π′) implies σi(π) = σi(π′) for all π, π′ ∈
dom(σi). It is deterministic if σi(π) is a Dirac distribution
for all π ∈ dom(σi).

Fig. 1. Multi-layer network topology.

MDP using convex-concave approach [23]. Finally, using the
inﬁltrator strategies, we compute a robust deceiver strategy
that maximizes the worst-case cost of the all strategies using
mixed-integer linear programming (MILP). We demonstrate
that, we can compute strategies for a deception game with
higher number of states and action than those that can be
tackled by the state-of-the-art methods.

The rest of the paper is organized as follows. In the
next section, we motivate the problem studied in this paper
with a motivating example. In Section III, we provide some
preliminary deﬁnitions and formulate the problem. In Section
IV, we describe our two-stage tractable approach. In Section
V, we apply the proposed approach to the motivating example.
Finally, we conclude the paper in Section VI and give
directions for future research.

II. MOTIVATING EXAMPLE:
ACTIVE DECEPTION FOR NETWORK SECURITY

We motivate the problem under study in this paper by
a computer network security example adapted from [18].
Consider a multilayer network used typically in critical
network operations such as power plants and manufactur-
ing facilities [20], [4] (see Figure 1). The inﬁltrator starts
the attack from outside of the network and proceeds by
accessing deeper layers, where more sensitive/valuable assets
are located. The middle layers are composed of databases
containing conﬁdential data and the last layer provides access
to physical devices. Inﬁltration to the last
layer of the
network can lead to critical damage to the facility [22].

The network we consider consists of n layers as illustrated
in Figure 2. Each column in Figure 2 corresponds to a
network layer, e.g., emails, databases, actuators and sensors,
and etc.

The deceiver’s task here is to manipulate the inﬁltrator’s
belief over whether he/she is being detected. The deceived
inﬁltrator is hence convinced to take wrong actions due to the
uncertainty about the inﬁltration progress. The deceiver has
to manipulate the inﬁltrator’s belief and keep him engaged
in the network.

One way to represent the network security problem de-
picted in Figure 2 is to represent it by a one-sided POSG (to
be described formally in the sequel), which is comprised of
two parts. The upper-half corresponds to the states, wherein
the presence of the inﬁltrator in the network has not been
detected (d = false) and the deception mechanism cannot
be implemented; whereas, the lower half corresponds to the

d
e
t
c
e
t
e
D
t
o
N

e
s
l
a
f

=
d

d
e
t
c
e
t
e
D

e
u
r
t

=
d

xA = 0

xA = layer1
Layer 1

xA = layern

do w n

take

wait

exﬁltrate

exﬁltrate

compromise

compromise

compromise
engage

exﬁltrate
engage

exﬁltrate
engage

any action
block

Fig. 2.
whereas, the rest belong to the inﬁltrator.

Schematic diagram of the transition system of the one-sided POSG model for network security. The bold actions correspond to the deceiver;

Deﬁnition 3 (One-sided POSG) A one-sided partially ob-
servable stochastic game (POSG) is a tuple G = (G, Z, O),
with G = (S◦, S(cid:3), sI, Act, P) the underlying SG of G, Z
a ﬁnite set of observations, and O : S → Z the observation
function for Player 2.

POMDPs and optimal strategies require inﬁnite memory in
general [5]. To represent observation-based strategies with
ﬁnite memory, we use ﬁnite-state controllers (FSCs). If such
an FSC has n memory states, we speak of memory size n
for the underlying strategy σ.

A partially observable Markov decision process (POMDP)
is an one-sided POSG in which S◦ = ∅, and consequently
S = S(cid:3). In a POSG, players cannot make any choice when
|Act(s)| = 1. Thus, for a POSG G with states S◦ ∪ S(cid:3), the
◦ = S◦ \ {s ∈ S◦ | |Act(s)| = 1}
POSG G′ with states S′
(cid:3) = S(cid:3) ∪ {s ∈ S◦ | |Act(s)| = 1} and all transitions
and S′
unchanged is equivalent w.r.t. the properties considered in
the paper. Consequently, a POSG where one player never
has a choice, i.e. |Act(s)| = 1, ∀s ∈ S◦ is also an MDP.

Without loss of generality, we assume Player 2 can observe
its own available actions, thus |Act(s)| = |Act (s′)| for all
s, s′ ∈ S(cid:3) with O(s) = O(s′). Essentially, in a one-sided
POSG, Player 1 has full observability, while Player 2 has
only partial observability. We lift the observation function
a1−→ ssn ∈ PathsM
a0−→ s1
ﬁn , the
to paths: For π = s0
a0−→
associated observation sequence is O(π) = O(s0)
O(s1)

a1−→ sO(sn).

Deﬁnition 4 (POSG Strategy) An observation-based strat-
egy σ = (σ1, σ2) for a one-sided POSG G is a strategy
(σ1, σ2) for the underlying SG G such that σ2(π) = σ2(π′)
for all π, π′ ∈ PathsG
ﬁn with O(π) = O(π′). σ1 is the
Player 1 strategy, and σ2 is the (observation-based) Player 2
strategy.

Applying the strategy σ = (σ1, σ2) to a POSG G resolves
all nondeterminism and partial observability, resulting in
the induced Markov chain Gσ. In general, POSGs extend

Deﬁnition 5 (FSC) A ﬁnite-state controller (FSC) for a
POMDP M is a tuple A = (N, nI, γ, δ), where N is a
ﬁnite set of memory nodes, nI ∈ N is the initial memory
node, γ is the action mapping γ : N × Z → Distr (Act), and
δ is the memory update δ : N × Z × Act → Distr (N ). The
set FSC M
k denotes the set of FSCs with k memory nodes,
called k-FSCs.

From a node n and the observation z in the current state
of the POMDP, the next action a is chosen from Act(z)
randomly as given by γ(n, z). Then, the successor node of
the FSC is determined randomly via δ(n, z, a).

max(♦T ) (PrG

Speciﬁcations. For a POSG G and a set T ⊆ S of target
states, we consider the maximal (or minimal) probability
PrG
min(♦T )) to reach T , as well as the maximal
max(♦T ) (ECG
(or minimal) expected cost ECG
min(♦T )). For
a probability bound λ ∈ [0, 1] and an expected cost bound
κ ∈ R, we also consider speciﬁcations of the form ϕ =
P≤λ(♦T ) and ψ = EC≤κ(♦T ), where the probability or the
expected cost to reach T shall be at most λ or κ, respectively.
The speciﬁcation ϕ is satisﬁed for a strategy σ = (σ1, σ2)
and the POSG G if the probability PrGσ
(♦T ) of reaching a
target state in Gσ is at most λ, denoted by Mσ |= ϕ. This
satisfaction relation is analogous for expected cost.

Sufﬁciently strong strategies. For a POSG G and a property
ϕ, a strategy σ1 for Player 1 is called sufﬁciently strong for
ϕ against a set S ⊆ Σ2 of strategies for Player 2, if for each
strategy σ2 ∈ S of Player 2, G(σ1,σ2) |= ϕ.

POSG – Deception Game

Inﬁltrator

Player 1 with
Partial Observability

Deceiver

Player 2 with
Full Observability

Deception and
Inﬁltration
Properties

Inﬁltrator’s
Strategies

csI

minimize
s◦
cs◦ ,cs(cid:3) ,δ
a
subject to
cs = 0,
σz
a = 1,

∀s ∈ T

X
a∈Act

∀z ∈ Z

δs◦
a = 1,

∀s◦ ∈ S◦

X
a∈Act

(1)

(2)

(3)

(4)

Robust Deception
Strategies

cs(cid:3) = C(s(cid:3), a) +

P(s(cid:3), a, s′

◦ ,
◦) cs′

Fig. 3. The Robust Deception Problem.

B. Deception Games

cs◦ ≤ C(s◦, a) + γ

cs◦ ≥ C(s◦, a) + γ

a

σO(s(cid:3))
a X
s′
◦∈§◦

X
a∈Act
∀s(cid:3) ∈ S(cid:3) \ T, ∀σO(s(cid:3))
P(s◦, a, s′

X
s′
(cid:3)∈S(cid:3)
∀s◦ ∈ S◦ \ T, ∀a ∈ Act
P(s◦, a, s′

(cid:3)) cs′

(cid:3)) cs′

X
s′
(cid:3)∈S(cid:3)
∀s◦ ∈ S◦ \ T, ∀a ∈ Act.

∈ σ2

(5)
(cid:3) − M (1 − δs◦

a ),

(6)
(cid:3) + M (1 − δs◦

a ),

(7)

Deﬁnition 6 (Deception Game) A deception game is a one-
sided POSG, where Player 1 (with full observability) is
called the inﬁltrator and Player 2 (with partial observability)
is called the deceiver. See Figure 3 for an illustration of the
robust deception problem.

In the POSG that captures the motivating example, we
identify the target states T ⊆ S as the states that correspond
to Layer n. The goal is to deceive the inﬁltrator such that the
expected cost to reach those states is maximized. We state
the formal problem.

Problem 1: Given a deception game G = (G, Z, O),
a set of target states T ⊆ S, memory bound n, and
speciﬁcation ϕ = ECG
min(♦T ), compute a sufﬁciently
strong strategy for the deceiver, against
inﬁltration-
strategies with memory n′ ≤ n.

The straightforward solution to Problem 1 can be given by
solving a robust mixed-integer linear program (MILP) with
variables as described next.

For s◦ ∈ S◦ and each action a ∈ Act, the strategy variable
δs◦
a ∈ {0, 1} denotes which action is active in each state s◦.
If δs◦
a = 1, then Player 1 takes action a in state s◦. For
s◦ ∈ S◦, and s(cid:3) ∈ S(cid:3), the cost variables cs◦ ≥ 0 cs(cid:3) ≥ 0
and represent the expected cost of reaching T ⊆ S. M is
a large constant that automatically satisﬁes the constraints
in (6) and (7) if δs◦
a = 0, which means the deceiver does not
select action a in state s◦. γ ∈ [0, 1) is the discount factor
to ensure that we have ﬁnite expected cost.

We then have the following robust MILP that solves Prob-

lem 1.

The objective in (1) minimizes the expected cost of the
deceiver. We assign the expected cost of the states in the
target set to 0 by the constraints in (2). We ensure that the
strategies of the inﬁltrator and the deceiver are well-deﬁned
with the constraints in (3) and (4). The constraints in (5)–(7)
gives the computation for the expected cost in the states of
the POSG.

Proposition 1 The robust MILP in (1) – (7) computes the
maximal probability of reaching T under a (maximizing)
randomized memoryless observation-based strategy.

Unfortunately, robust MILP’s are notoriously hard to
solve, except in special cases of small problems with few
constraints [12] or only having binary variables [3]. Re-
cently, [26] proposes a heuristic relaxation for robust MILP’s
using the afﬁnely adjustable robust counterpart [2]. However,
large robust MILP’s, such as the robust MILP above for
Problem 1, remain intractable in general.

IV. TRACTABLE APPROACH

In this section, we propose a two-stage approach for
solving Problem 1. We begin by giving some key formalisms.
1) Parametric MDPs: Instead of having ﬁxed probabilities
in the transition function of an MDP, we allow to describe
the transition probabilities of an MDP as polynomials over
a ﬁxed set of variables. By having different values for the
parameters, we can induce different MDPs.

Deﬁnition 7 (pMDP) A parametric Markov decision pro-
cess (pMDP) M is a tuple M = (S, sI, Act, V, P) with a
ﬁnite (or countably inﬁnite) set S of states, initial state sI ∈
S, a ﬁnite set Act of actions, a ﬁnite set V of parameters,
and a transition function P : S × Act × S → Q[V ].

Deception Game

One-sided POSG

Memory

Finite State Controller

Symbolic Representation
of Inﬁltration Strategies

Parametric Markov
Decision Process

Set of Inﬁltration
Strategies

Strategies satisfying the
Inﬁltration Objectives

s5

1

a1

s0

a3

1

s4

a2

0.5

0.5

a1

s1

a2

0.5

s2

0.5

1

1

1

a2

s3

a1

s5

s0

s4

s1

0.5q

1−p1−p2

1−0.5q

0.5p2

p1+0.5p2

1−q

q

s2

s3

(a) a POSG

(b) the pMDP

Fig. 5. From POSG to pMDP

Fig. 4. Computing Inﬁltration Strategies for Stage 1.

Applying an instantiation u : V → R to a pMDP M ,
denoted M [u], replaces each polynomial Q[V ] in M by Q[u].
M [u] is also called the instantiation of M at u. Instantiation
u is well-deﬁned for M if the replacement yields probability
distributions, i. e., if M [u] is an MDP.

Deﬁnition 8 (pMDP Synthesis Problem) Given a pMDP
M and a property ϕ, the synthesis problem is to compute an
instantiation u such that M [u] |= ϕ, if one exists.

The pMDP synthesis problem amounts to solving the

following NLP.

csI

maximize
cs,V
subject to
csI ≤ κ,
cs = 0,
P(s, a, s′) ≥ 0,

∀s ∈ T

∀s, s′ ∈ S, ∀a ∈ Act

P(s, a, s′) = 1,

∀s ∈ S, ∀a ∈ Act

X
s′∈S

cs ≥ C(s, a) + γ

P(s, a, s′) cs′,

X
s′∈S

∀s ∈ S \ T, ∀a ∈ Act.

(8)

(9)

(10)
(11)

(12)

(13)

For s ∈ S, the cost variable cs ≥ 0 represents an upper
bound of expected cost of reaching target set T ⊆ S, and
the parameters in set V enter the NLP as part of the functions
from Q[V ] in the transition function P.

One particular efﬁcient method for ﬁnding parameter
instantiations is given in [6], which solves the NLP in (8)–
(13) via a reformulation to a convex-concave programming
problem [23] and an efﬁcient integration of model checking
calls to improve its performance. We are now ready to outline
the proposed tractable approach.

A. Stage 1: Computing Inﬁltration Strategies

In this section, we are given a POSG and are interested
in computing several sufﬁciently strong inﬁltrator strategies.
For an overview of the approach in Stage 1, see Figure 4.

Task 1 Given a POSG G, compute several sufﬁciently
strong strategies for the inﬁltrator.

We detail the approach ﬁrst for an inﬁltrator that uses
memoryless strategies. We then generalize the approach to
any (ﬁnite but ﬁxed) amount of memory for the inﬁltrator.
The approach extends the reduction from POMDPs to pMCs
in [19].

1) Memoryless strategies: A memoryless strategy maps
observations to distributions over actions. Equivalently,
such strategy maps observation-action pairs to probabilities
(ObAct-probabilities). Any memoryless strategy is uniquely
deﬁned by its ObAct-probabilities. Instead of ﬁnding suit-
able strategies for the inﬁltrator, we can thus reformulate
our approach to ﬁnding suitable ObAct-probabilities. Hence,
a suitable strategy can be described as parameter values
that satisfy the property of an pMDP. To ﬁnd the ObAct-
probabilities, we construct an equivalent pMDP as illustrated
in the following example.

Example 1 Consider the POSG in Fig. 5(a). The states
for Player 1 are given by circles. Similarly, the rectangles
depict the states for Player 2. We indicate the actions from
the states with solid lines to black dots. The corresponding
probabilities from the black dots give the probability of
transitioning to the next state. Thus, we draw dashed (direct)
lines from the Player 1 (Player 2) nodes. The colors of states
indicate the corresponding observations.

The corresponding pMDP for a memoryless, observation-
based strategy for player 2 is given in Fig. 5(b). The actions
of player 1 are unchanged. As there is no more nondeter-
minism in the states of player 2, we can also view these
states as player 1 states. Moreover, to avoid clutter, we omit
action indications. Consider state s3 with outgoing action a2,
which leads with probability 1 to s4. Under a strategy, we
take this action with probability q: Thus, in the pMDP there
is an arc with probability q from s3 to s4. If we take action
a2 with probability q, we take action a1 with probability
1−q. The same probabilities for taking the actions have
to hold in state s1. Observe that the probabilities are not
immediately reﬂected, as there are two paths from s1 to
s2. The parameters in s0 are different. Indeed, there are 2
parameters as there are three actions to take.

We formalize the construction below for situations, where the
inﬁltrator has three available actions in each state, to simplify
the notation.

Deﬁnition 9 Given a POSG G = (G, Z, O) with SG G =
(S◦, S(cid:3), sI, Act, P). Let ∀s ∈ S(cid:3) |Act(s)| = 3. Without
loss of generality, refer to Act(s) = {a1, a2, a3} if s ∈ S(cid:3).
′, Act ′, V, P ′) is the corresponding
The pMDP M ′ = (S′, sI
I = sI, Act ′ =
pMDP for POSG G, if S′ = S◦ ∪ S(cid:3), s′
Act ∪ {⊥}, V = {pz
1, pz

2 | z ∈ Z} and P ′(s, a, s′) =




P(s, a, s′)
i=1 f z

3
P
0

i P(s, ai, s′)

if s ∈ S◦
if s ∈ S(cid:3), a = ⊥, z = O(s)
otherwise.


where f z

i denotes pz

1, pz

2 and 1 − pz

1 − pz

2, respectively.

The construction is straightforward to adapt to a varying
number of actions per state: Indeed, we just need additional
variables pz
i indicating that we take action i in a state with
observation z.

We denote the memoryless strategy IStrat(u) for the inﬁl-
trator in the deception game that is induced by a valuation
u in the inﬁltration-pMDP.

Theorem 1 Given a deception game G, with its correspond-
ing inﬁltration-pMDP I(G), and a property ϕ. Let u be a
solution for the pMDP synthesis problem for the property
ϕ. Then IStrat(u) is a sufﬁciently strong strategy for the
inﬁltrator on the G and ϕ.

2) Adding Memory to the Inﬁltrator Strategy: The ideas
for the synthesis of memoryless strategies can be lifted to a
ﬁnite (k) memory setting. We apply an k−unfolding of the
POSG, and then search for a memoryless inﬁltrator strategy.
The idea is to create k copies of the POSG, and allow
the inﬁltrator to freely switch between copies. The different
copies correspond to the internal memory of the inﬁltrator.

B. Stage 2: Computing a Robust Deception Strategy

In this section, we assume that the deceiver obtained N
strategies for the inﬁltrator. Then, the task of the deceiver is
to compute a strategy that minimizes the worst-case expected
cost ψ = EC≤κ(♦T ) to reach a target set T against any
of the N inﬁltration strategies under all deceiver policies.
If the deception strategy has access to the memory node
of the inﬁltration strategy, we call the inﬁltration memory
transparent.

1) Deceiving against Transparent-Memory

Inﬁltrator
Strategies: We focus on memoryless inﬁltration strategies.
Memoryless inﬁltration strategies are the most prominent
transparent memory case, and for transparent memory, each
ﬁnite memory strategy can be reduced to a memoryless
strategy by unfolding the memory into the MDP, as in
Stage 1.

To compute a robust deception strategy against transparent
inﬁltrator strategies, we remove the uncertain constraints of
the robust MILP in (1) – (7) by using the inﬁltrator strategies
that we obtained in Stage 1. Then, we reduce the problem of
synthesizing a robust deception strategy problem to solving
an MILP instead of solving a robust MILP. We now give the
details of the resulting MILP with the inﬁltrator strategies.

We deﬁne the following variables for the following MILP:
For s◦ ∈ S◦, s(cid:3) ∈ S(cid:3), and for each inﬁltrator strategy
σi,O(s(cid:3)), i = 1, . . . , N , the cost variables
ci
s◦ ≥ 0 and
ci
s(cid:3) ≥ 0 give the expected cost of reaching T ⊆ S for each
inﬁltrator strategies. For s◦ ∈ S◦ and each action a ∈ Act,
the strategy variable δs◦
a ∈ {0, 1} denotes which action is
active in each state s◦. If δs◦
a = 1, then the deceiver takes
action a in state s◦.

For observation z ∈ Z, for each action a ∈ Act and
for each inﬁltrator strategy σi,O(s(cid:3)), i = 1, . . . , N , σi,O(s(cid:3))
represents the probability of choosing action a ∈ Act upon
observation z for each strategy of Player 2. Similar to the
Problem 1, M is a large constant that automatically satisﬁes
the constraints in (18) and (19) if δs◦
a = 0, which means the
deceiver does not select action a in state s◦.

a

We thus have the following MILP:

(14)

(15)

(16)

ri
sI

max
i

,δ

minimize
s◦
ci
s◦ ,ci
a
s(cid:3)
subject to
ci
s = 0,
δs◦
a = 1,

X
a∈Act
ci
s(cid:3) = C(s(cid:3), a) +

∀s ∈ T, i = 1, . . . , N

∀s◦ ∈ S◦

σi,O(s(cid:3))
a

P(s(cid:3), a, s′

◦) ci
◦ ,
s′

X
a∈Act

X
s′
◦∈§◦
∀s(cid:3) ∈ S(cid:3) \ T, i = 1, . . . , N

(17)

ci
s◦ ≥ C(s◦, a) + γ

P(s◦, a, s′

(cid:3)) ci
s′
(cid:3)

− M (1 − δs◦

a ),

X
s′
(cid:3)∈S(cid:3)
∀s◦ ∈ S◦ \ T, ∀a ∈ Act, i = 1, . . . , N
(18)

ci
s◦ ≤ C(s◦, a) + γ

P(s◦, a, s′

(cid:3)) ci
s′
(cid:3)

+ M (1 − δs◦

a ),

X
s′
(cid:3)∈S(cid:3)
∀s◦ ∈ S◦ \ T, ∀a ∈ Act, i = 1, . . . , N.
(19)

Task 2 Given a deception game, a set of transparent in-
ﬁltration strategies, compute a deception strategy that is
optimal under all deception strategies against the set of
inﬁltration strategies.

The objective in (14) minimize the worst-case expected
cost of deception against the inﬁltrator strategies. The con-
straint in (15) sets the expected cost of the states in T to be
0. Constraint (16) ensures that the deceiver picks one of the
actions in Act for each state in S◦. The constraints (17)–(19)

concerns the cost computation for each state in S◦, and S(cid:3).
By solving the MILP in (14) – (19), we construct a strategy
for the deceiver that is sufﬁciently strong against inﬁltrator
strategies. By construction, we have the following theorem.

a 12-layer network that incurs a less expected loss compared
to the optimal strategy in a 4-layer network. However, if
the deceiver always blocks or engages, the expected cost
increases with increasing number of layers.

Theorem 2 The MILP in (14) – (19) computes a sufﬁciently
strong strategy for the deceiver against N strategies of the
inﬁltrator.

V. NUMERICAL EXPERIMENTS

At this point, we are ready to return to the motivating
example. We deﬁne the expected discounted cost for the
deceiver corresponding to each action the inﬁltrator takes
by

∞

γt−1 · l(t),

L =

X
t=1

where γ is the discount factor and l(t) denotes the loss at
stage t. For this example, the speciﬁcation of the deceiver is
ψ = ECmin, which is to minimize L against all inﬁltration
strategies. Table IV-B.1 outlines the elements of the one-
sided deception POSG. Note that since the players take
actions concurrently, the costs depend on their joint actions.
Furthermore, some of the costs are dependent on the layer
index i the inﬁltrator has accessed. For more details about
the example, the interested reader is referred to [18].

We ﬁrst describe our results on a 4-layer network, given
by [18]. Then, we demonstrate our approach on a 12-layer
network. For each network, we ﬁrst compute inﬁltration
strategies for the deceiver using the approach in Stage 1.
Then, using these inﬁltration strategies, we compute a robust
deceiver strategy, which is given by in Stage 2. The arising
pMDP problems from Stage 1 is solved using the approach
in [6]. We solve the MILP problems from Stage 2 using the
MILP solver GUROBI [13].

For the 4-layer network, we ﬁrst construct the one-sided
deception POSG with 2 memory nodes. The POSG con-
sists of 49 states, 8 action choices for the deceiver, and
34 actions for the inﬁltrator. After computing an optimal
deceiver strategy, the worst-case induced cost against 1000
inﬁltration strategies obtained from the approach in Stage 1
is 282.22. The obtained cost is comparable to the approach
given by [18]. The procedure for obtaining the inﬁltration
strategies took 193.29 seconds, and time to compute the
optimal deceiver strategy is 216.92 seconds.

The optimal deceiver strategy that we obtained is to engage
the inﬁltrator in ﬁrst 2 layers, then blocking the inﬁltrator
in the last layers. This seems to be beneﬁcial compared to
always blocking the inﬁltrator, which leads to a worst-case
expected cost of 341.72, and always engaging the inﬁltrator,
which leads to a worst-case expected cost of 304.05.

We also give the results with different number of inﬁltrator
strategies on a 4-layer and 12-layer network in Table V. The
expected cost of the defender increases with increasing num-
ber of inﬁltrator strategies in all cases. Also, by increasing
number of layers, the deceiver can craft an optimal strategy in

VI. CONCLUSION

We presented an approach to solve a partially observable
stochastic game (POSG), where one of the player has full
observability over the states, and the other player only
has partial observability. We formulated the problem as a
robust mixed-integer linear program, which is intractable
to solve in general. To obtain a more scalable approach,
we computed a robust optimal strategy for the deceiver by
synthesizing a set of inﬁltration strategies using parameter
synthesis in parametric Markov decision processes. Using a
mixed-integer linear program and the inﬁltration strategies,
we computed the robust deception strategy. We illustrated
our approach on a POSG model for network security and
we showed that we can handle larger networks compared to
the previous approaches in the literature.

Future work concerns removing the transparency approach,
which means the deceiver has access to the memory node
of the inﬁltration strategy,
if the inﬁltration strategy is
memory-based. Also, we will explore some of the recent
methods to solve mixed-integer linear problems proposed in
the literature, such as methods proposed in [9], [10].

REFERENCES

[1] T. Basar and Geert J. Olsder. Dynamic noncooperative game theory,

volume 23. Siam, 1999.

[2] A. Ben-Tal, A. Goryashko, E. Guslitzer, and A. Nemirovski. Ad-
justable robust solutions of uncertain linear programs. Mathematical
Programming, 99(2):351–376, 2004.

[3] Dimitris Bertsimas and Melvyn Sim. Robust discrete optimization and
network ﬂows. Mathematical programming, 98(1-3):49–71, 2003.
[4] E. Byres and J. Lowe. The myths and facts behind cyber security risks
for industrial control systems. In Proceedings of the VDE Kongress,
volume 116, pages 213–218, 2004.

[5] Krishnendu Chatterjee, Martin Chmel´ık, and Mathieu Tracol. What
is decidable about partially observable markov decision processes
with ω-regular objectives. Journal of Computer and System Sciences,
82(5):878–911, 2016.

[6] Murat Cubuktepe, Nils Jansen, Sebastian Junges, Joost-Pieter Katoen,
and Ufuk Topcu. Synthesis in pMDPs: A tale of 1001 parameters.
CoRR, abs/1803.02884, 2018.

[7] Donald C Daniel and Katherine L Herbig. Strategic Military Decep-
tion: Pergamon Policy Studies on Security Affairs. Elsevier, 2013.
[8] Dorothy Elizabeth Robling Denning. Information warfare and security,

volume 4. Addison-Wesley Reading, MA, 1999.

[9] Souradeep Dutta, Susmit Jha, Sriram Sanakaranarayanan, and Ashish
Tiwari. Output range analysis for deep neural networks. arXiv preprint
arXiv:1709.09130, 2017.

[10] Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish
Tiwari. Learning and veriﬁcation of feedback control systems using
IFAC-PapersOnLine, 51(16):151–156,
feedforward neural networks.
2018.

[11] R. Emery-Montemerlo, G. Gordon, J. Schneider, and S. Thrun. Ap-
proximate solutions for partially observable stochastic games with
In Autonomous Agents and Multiagent Systems,
common payoffs.
2004. AAMAS 2004. Proceedings of
the Third International Joint
Conference on, pages 136–143. IEEE, 2004.

[12] Kai-Simon Goetzmann, Sebastian Stiller, and Claudio Telha. Opti-
mization over integers with robustness in cost and few constraints.
In International Workshop on Approximation and Online Algorithms,
pages 89–101. Springer, 2011.

[13] Gurobi Optimization,

Inc.

Gurobi optimizer

reference manual.

http://www.gurobi.com, 2013.

Inﬁltrator’s
(xA)
any
layeri
layeri
any
layeri
layeri
any
any
any

States

Actions

loss (l)

Position

Detection mode (d)

Inﬁltrator (aI )

Deceiver (aD)

false
false
false
true
true
true
true
true
true

compromise
exﬁltrate
takedown
compromise
exﬁltrate
takedown
compromise
exﬁltrate
takedown

–
–
–
engage
engage
engage
block
block
block

−2
15i
25i
−4
−2
25i
−2
0
0

TABLE I
THE LOSSES INCURRED WHEN BOTH PLAYERS TAKE SIMULTANEOUS ACTIONS AT DIFFERENT STATES.

[29] Q. Zhu and T. Basar. Game-theoretic methods for robustness, security,
and resilience of cyberphysical control systems: games-in-games prin-
ciple for optimal cross-layer resilient control systems. IEEE control
systems, 35(1):46–65, 2015.

Number of inﬁltrator strategies
Number of Layers & deceiver strategies
4 & Always engage
4 & Always block
4 & Optimal
12 & Always engage
12 & Always block
12 & Optimal

10

100

260.17
246.12
228.64
277.71
256.39
217.76

297.45
286.19
256.81
301.46
294.65
239.71

1000

304.05
341.72
282.22
313.91
308.12
261.49

TABLE II
EXPECTED LOSS OF THE DEFENDER IN VARIOUS SCENARIOS.

[14] Ernst Moritz Hahn, Tingting Han, and Lijun Zhang. Synthesis for pctl
in parametric markov decision processes. In NASA Formal Methods
Symposium, pages 146–161. Springer, 2011.

[15] E. A Hansen, D. S. Bernstein, and S. Zilberstein. Dynamic program-
In AAAI, volume 4,

ming for partially observable stochastic games.
pages 709–715, 2004.

[16] J. P. Holdren and E. S. Lander. Big data and privacy: a technological
perspective. Report to the President (Executive Ofﬁce of the President
President’s Council of Advisors on Science and Technology), 2014.

[17] K. Hor´ak, B. Bosansk`y, and M. Pechoucek. Heuristic search value
iteration for one-sided partially observable stochastic games. In AAAI,
pages 558–564, 2017.

[18] K. Hor´ak, Q. Zhu, and B. Boˇsansk`y. Manipulating adversary’s belief:
A dynamic game approach to deception by design for proactive
network security. In International Conference on Decision and Game
Theory for Security, pages 273–294. Springer, 2017.

[19] Sebastian Junges, Nils Jansen, Ralf Wimmer, Tim Quatmann, Leonore
Winterer, Joost-Pieter Katoen, and Bernd Becker. Finite state con-
trollers for pomdps via parameter synthesis. In UAI, 2018.

[20] D. Kuipers and M. Fabro. Control systems cyber security: Defense in

depth strategies. United States. Department of Energy, 2006.

[21] A. Kumar and S. Zilberstein. Dynamic programming approximations
In FLAIRS Conference,

for partially observable stochastic games.
2009.

[22] D. Kushner. The real story of Stuxnet. IEEE Spectrum, 3(50):48–53,

2013.

[23] Thomas Lipp and Stephen Boyd. Variations and extension of the
convex–concave procedure. Optimization and Engineering, 17(2):263–
287, 2016.

[24] M. H. Manshaei, Q. Zhu, T. Alpcan, T. Basar, and J. Hubaux. Game
theory meets network security and privacy. ACM Computing Surveys
(CSUR), 45(3):25, 2013.

[25] K. D. Mitnick and W. L. Simon. The art of deception: Controlling

the human element of security. John Wiley & Sons, 2011.

[26] Jean Pauphilet, Diego Kiner, Damien Faille, and Laurent El Ghaoui.
A tractable numerical strategy for robust milp and application to
In Decision and Control (CDC), 2016 IEEE
energy management.
55th Conference on, pages 1490–1495. IEEE, 2016.

[27] Lance Spitzner. Honeypots: tracking hackers, volume 1. Addison-

Wesley Reading, 2003.

[28] Colin Tankard. Advanced persistent threats and how to monitor and

deter them. Network security, 2011(8):16–19, 2011.

