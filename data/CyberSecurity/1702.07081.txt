1

DyAdHyTM: A Low Overhead Dynamically Adaptive
Hybrid Transactional Memory on Big Data Graphs

Mohammad Qayum and Abdel-Hameed Badawy, New Mexico State University
Jeanine Cook, Sandia National Laboratories

Big data is a buzzword used to describe massive volumes of data that provides opportunities of exploring new insights through
data analytics. However, big data is mostly structured but can be semi-structured or unstructured. It is normally so large that
it is not only diﬃcult but also slow to process using traditional computing systems. One of the solutions is to format the data
as graph data structures and process them on shared memory architecture to use fast and novel policies such as transactional
memory. In most graph applications in big data type problems such as bioinformatics, social networks, and cybersecurity, graphs
are sparse in nature. Due to this sparsity, we have the opportunity to use Transactional Memory (TM) as the synchronization
policy for critical sections to speedup applications. At low conﬂict probability TM performs better than most synchronization
policies due to its inherent non-blocking characteristics. TM can be implemented in Software, Hardware or a combination of
both. However, hardware TM implementations are fast but limited by scarce hardware resources while software implementations
have high overheads which can degrade performance. In this paper, we develop a low overhead, yet simple, dynamically adaptive
(i.e. at runtime) hybrid (i.e. combines hardware and software) TM (DyAdHyTM) scheme that combines the best features of both
Hardware TM (HTM) and Software TM (STM) while adapting to application’s requirements. It performs better than coarse-
grain lock by up to 8.12x, a low overhead STM by up to 2.68x, a couple of implementations of HTMs (by up to 2.59x), and
other HyTMs (by up to 1.55x) for SSCA-2 graph benchmark running on a multicore machine with a large shared memory.

Categories and Subject Descriptors: C.5.5 [Servers]: Clusters—Symmetric Multiprocessors

General Terms: Dynamically Adaptive Hybrid Transactional Memory (DyAdHyTM)

Additional Key Words and Phrases: Big Data, Graph Application, Synchronization, Hybrid Transactional Memory

7
1
0
2

r
a

M
2

]

C
D
.
s
c
[

2
v
1
8
0
7
0
.
2
0
7
1
:
v
i
X
r
a

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

 
 
 
 
 
 
1:2

•

M. Qayum, A. Badawy and J. Cook

1.

INTRODUCTION

Though we are in the era of multicore and manycore processors era, we are not able to utilize the full
computing power of all the cores due to challenges with critical sections in parallel workloads[13]. To
exploit all the cores in a system, programmers have to put signiﬁcant effort to solve this issue of critical
section with an appropriate synchronization ﬁrst hand. A synchronization policy controls the access
to shared data in a critical sections by multiple threads so that consistency and coherency of the data
is maintained. The common synchronization policy used in shared data is coarse-grain synchroniza-
tion (e.g. locks) that is easy to program, but not optimized as it unnecessarily sequentializes program
execution due to its blocking characteristics. Examples of coarse grain synchronization based on locks
include semaphores, mutexes, monitors, and barriers. These synchronization polices are used in con-
ventional multi-threading programming libraries such as Pthread[4] or OpenMP[9]. The alternative is
a ﬁne-grain synchronization policy which is notoriously complex but provides better scalability due to
its smaller granularity. For example, in CRAY processors, locks are used at the cacheline granularity
using a full/empty bit[3]. This type of synchronization is not composable, which means locks cannot
be combined in a modular fashion. In comparison to these kinds of conventional locking mechanisms,
transactional memory provides easier programmability, better scalability, composabilty and sometimes
lower overheads depending on workload behavior[29].

A collection of real-life big data problems that uses graph algorithms are already presented in the
Graph500 benchmark [28]. Due to the sparse nature of data structures in most real word graph
applications[24], TM will outperform most existing synchronization policies as it is inherently non-
blocking[21]. TM can be implemented in software, hardware, or in a combination of both that is hybrid.
Hybrid Transactional Memory (HyTM) scheme optimizes the combination of Software Transactional
Memory (STM) and Hardware Transactional Memory (HTM) depending on the requirement of the
application[12]. A Dynamically Adaptive Hybrid Transactional Memory (DyAdHyTM) is a TM design
that adapts to appropriate policies (i.e. HTM or STM) at different phases of program execution on
runtime based on the application’s behavior. HTM is bound by limited hardware resources since micro-
architecture implementation of TM policies can use up costly chip area[10]. On the other hand, STM is
limited by slow speed and high overheads due to high level abstractions implemented in Software[26].
HyTM is even better approach than naive HTM or STM implementation since there is no best TM
implementation for all applications. A HyTM not only combines fast HTM as the primary execution
path and slow STM as the fall back execution path but also amends HTM with software extensions to
co-operate with STM. Moreover, since synchronization is mostly executed in HTM due to adaptability
of HyTM, few costly STM executions do not degrade performance much.

Currently, there are only a few commercial hardware implementations of transactional memory in-
troduced by IBM and Intel. The most prominent implementation is the Transactional Synchronization
Extensions (TSX)[31] implemented in the Intel 4th Generation CoreTM Processors. Restricted Trans-
actional Memory (RTM)[31] in TSX is a new instruction set extension to the popular x86 instruction
set architecture (ISA) comprising of four new instructions. For our DyAdHyTM, we used Intel’s RTM
as best effort HTM due to its fastness and simple design. For STMs, we used GCC’s STM due to its
low overhead. Results with our designed DyAdHyTM show that it provides better performance than
conventional synchronization methods such as locks, STMs and even HTMs for a standard graph ap-
plication of large size as we scale the core/thread count, memory size and problem size.

We tested our implementation on a large SMP machine of 28 cores and 64 GB memory for large
scale graphs with up to 100s of millions of vertices and billions of edges to demonstrate that TM, and
particularly, DyAdHyTM is a better synchronization scheme. The contributions of this paper can be
summarized as follows:

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

A Dynamically Adaptive Hybrid Transactional Memory for Big Data

•

1:3

(1) We develop a low overhead dynamically adaptive hybrid transactional memory that adapts to the

workloads to optimize performance.

(2) We conduct a detailed performance comparison against various synchronization strategies includ-

ing native HTM.

(3) We provide through statistics collected from the hardware insights into why the various techniques

perform the way they preform.

The rest of the paper is organized as follows: section 2 discusses TM in detail. Section 3 describes
our DyAdHyTM implementation and Section 4 discusses our experimental results. Section 5 discusses
related works. Appendix ?? shows all the graphs in the paper in larger formatting.

2. BACKGROUND

Transactional memory (TM) is a non-blocking synchronization scheme for shared regions that atom-
ically updates memory (read or write to a chunk of memory). If it fails (i.e. memory should not have
been updated), then TM rolls back to the previous state of memory. In TM, transactions (regions of
code that use the TM paradigm) can work in parallel without being blocked. If any of the transactions
conﬂict (i.e. several transactions try to access a shared data simultaneously) with other transactions
it will abort and retry to execute later, but one of the transactions will commit (write updates to the
memory) to guarantee progress.

In STM, all the transactional semantics and policies are implemented in software. However, it can
also take advantage of hardware features such as atomic instructions. STM typically has high over-
heads but is ﬂexible enough to suit different types of workloads, and it is also customizable. STM allows
researchers to easily explore different TM designs. Furthermore, STM can be modiﬁed to accommodate
changes in a compiler or OS, and it is relatively simple to integrate STM into existing system and lan-
guage features. This is because STM has fewer intrinsic limitations than HTM, which is limited by
cache size and cache hierarchy.

Herlihy and Moss [20] proposed hardware transactional memory (HTM) in the early 90s. After two
decades, Intel implemented the HTM (Intel’s TSX protocol) in the Haswell processor family [19]. In
Intel’s HTM, extensions to the microarchitecture and the instruction set are added to accommodate
TM. The CPU handle the various TM policies e.g. versioning, conﬂict detection, and conﬂict resolution.
The hardware has to buffer old copies of the data in the caches or store buffers to allow for roll backs
if need be. The coherence protocol is extended to accommodate such changes. The memory controller
detects conﬂicts among transactions. New status registers are added to the CPU for performance eval-
uation/monitoring. A fallback mechanism is needed in case a transaction fails repeatedly to avoid a
live-lock situation.

Typically, the overhead of the extensions and modiﬁcations to implement HTM is lower than that of
an STM. Even with low overheads, HTM has some inherent limitations basically the bounded capacity
of the CPU. In an HTM, the read and write sets must ﬁt in the cache(s). This gives rise to a cache ca-
pacity limit. Also, cache associativity, hyperthreading, TLB size and replacement policies can limit the
number of in-ﬂight transactions in an HTM. Moreover, duration of transactions can be limited by hard-
ware events e.g. context switching, interrupts, exception handling, page-faults, and thread migrations.
One of the solutions is to use HyTM to fall back to STM when transactions are aborted frequently due
to HTM’s limitations.

2.1 Hybrid TMs

A Hybrid Transactional Memory (HyTM) scheme uses a combination of STM and HTM depending on
the application requirements [12]. In most HyTMs, an HTM provides a fast execution path, while an

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

1:4

•

M. Qayum, A. Badawy and J. Cook

STM serves as a fallback to handle situations where the HTM cannot execute the transaction suc-
cessfully. Supporting the semantics of both HTM and STM is not trivial. For example, strong isolation
can be lost if an HTM provides it whereas the STM does not. In HyTM, the hardware and software
transactions not only coexist but also monitor common memory locations related to the status of a
transaction.

In a HyTM design, best-effort HTM [12] is combined with the best applicable STM through support-
ing libraries. Best-effort HTM tries to execute as many transactions as possible within its implemen-
tation constraints (e.g. cache size) but some transactions are bound to abort no matter what. Aborted
and any remaining transactions are executed in via an STM that suits the application requirements.

Depending on how the HTM and STM are combined, there are three types of HyTMs that are com-

mon in the literature [26], more details in Section 3:

(1) A fast path HTM with a slow STM as a fallback e.g. Hybrid NOrec [10], ASF [8] and Reduced

Hardware NOrec [26]

(2) HTM and STM in phases e.g. PhTM [23]
(3) HTM and STM in parallel e.g. Unbounded TM [1] and the proposed HyTM of Damron et al. [12].

In a HyTM scheme, at any given time several HTM and STM transactions can work on the same
shared region. However, this creates complexity in conﬂict detection and resolution due to the dispar-
ity between HTMs and STMs syntaxes. For example, issues arises such as how to handle a situation
where an object (STM granularity) in an STM transaction conﬂicts with two cache lines (HTM granu-
larity) of two HTM transactions but both HTM transactions are not conﬂicting with each other. Solving
these issues and optimizing for different scenarios leads to highly sophisticated and complicated de-
signs. Since graph applications are for the most part HTM bound due to small task size of critical
sections [21], a HyTM with simple, low overhead STM and best-effort HTM co-operation may sufﬁce
to provide good performance.

Most of the current HyTM designs take existing best-effort HTM from simulators or commercial im-
plementations and add STM to them. Most of the complexity in HyTM is in implementing the software
extensions. Therefore, there are many research opportunities for novel HyTM ideas such as TM exten-
sions to higher level caches, a larger speculative store buffer, or using more hardware status registers
to assist STM-HTM collaboration.

The sparsity of graphs in most real applications allows TM to outperform most existing synchroniza-
tion techniques which are mostly blocking [30]. In next section we discus why our HyTM approach is
a better than any TM approach since it not only combines the best features of HTM and STM, but also
adapts to the application requirements on runtime.

3.

IMPLEMENTATION

3.1 Adaptive HyTM

An Adaptive Hybrid Transactional Memory (AdHyTM) is a TM design that adapts the different TM
policies at different phases of program execution based on the behavior of the application. HTM is
limited by the hardware resources depending on the particular microarchitecture implementation.
On the other hand, STM is limited by slow speed due to high overheads since all the policies are
implemented in high level abstractions in software. AdHyTM may be the best approach because there
is no size ﬁts all TM i.e. no TM performs best for all applications. AdHyTM combines a fast HTM as
the primary execution path and a slow STM as the fall back execution path. It also forces the HTM
to cooperate with the STM to speed up synchronization of parallel applications. Since transactions are
mostly executed in HTM, it is beneﬁcial to make a few costly software extensions in the interface in

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

A Dynamically Adaptive Hybrid Transactional Memory for Big Data

•

1:5

order to efﬁciently fallback to a simple but slow STM path upon failure(s). For adaptability, AdHyTM
can take several approaches on how HTM and STM can cooperate:

(1) Never run on HTM but run on STM if task sizes are large due to limited hardware resources
(2) Always run on HTM if task sizes are small
(3) Run cooperatively with HTM and STM if task sizes are not large and not small or over populates
HTM. In that case, since HTM cannot execute all the transactions due to capacity limit, STM can
execute the remaining transactions.

3.2 Our Approach

Our proposed HyTM also extends the HTM with a software extension in the interface to allow cooper-
ation with the STM. The purpose of these extensions is to simplify the process of falling back to STM
upon failure to execute in HTM. For designing our AdHyTM, we considered following possibilities:

(1) One possible solution is to have a non-adaptable HyTM. In such a version, HTM will switch to STM

when it fails to complete after a ﬁxed number of retries.

(2) Another solution is to have a HyTM where HTM will switch to STM when it fails to complete after

a random number of retries in stead of ﬁxed retries.

(3) A third solution is to tune the number of retries through manual (i.e. static) proﬁling before falling

back to STM for better performance [30].

(4) A fourth option is to design an STM friendly HTM. Since HTM provides ﬂags describing transaction
abort causes such as capacity limits, we can use these ﬂags to adapt the HyTM to the application
behavior.

Researches have used the ﬁrst three policies to design HyTMs [36; 30]. We applied a simplistic
design based on lower overheads than complex algorithms to fallback to STM. The implementation
in this paper uses the fourth step mentioned above that means it adapts dynamically on runtime to
provide the optimum results.

3.3 Random HyTM (RNDHyTM) Implementation

In a RNDHyTM implementation as shown in the code listing in Figure 1a, there is no dynamic adap-
tation. At ﬁrst, a transaction tries to execute in HTM and if no STM transaction has captured the
critical section, then it will commit. However, when a transaction fails, it tries assigned a number of
possible retries in HTM. When its retrial quota ends, it will take a global lock and execute in STM. The
retrial quota is set with a random number ranges such as 1-20, 20-50, 50-100 etc. There is an overhead
due to random number generation which is quite signiﬁcant as we will discuss in section 4. The whole
algorithm is similar to DyAdHyTM which will be discussed in following section.

3.4 Fixed HyTM (FxHyTM) Implementation

In FxHyTM implementation as shown in the code listing in Figure 1a, there is no dynamic adaptation
and the retrial number is ﬁxed. It works in similar fashion to RNDHyTM. However, the retrial quota
is set with a ﬁxed random number such as 43, 23 or 76 without any design space exploration (DSE).
Performance of this FxHyTM is unpredictable due to random retrial numbers.

3.5 Statically Adaptive HyTM (StAdHyTM) Implementation

In StAdHyTM implementation as shown in the code listing in Figure 1a, there is an adaptation which is
implemented statically. It works in similar fashion to RNDHyTM. However, the retrial quota is tuned.
The retrial quota is tuned with design space explorations with different random number ranges, such

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

1:6

•

M. Qayum, A. Badawy and J. Cook

s e t according t o p o l i c y

// t r i e s
t r i e s = RANDOM RETRIES( ) or
Fixed NUM RETRIES or Tuned NUM RETRIES
i f HW BEGIN begins s u c c e s s f u l l y

i f

( g b l l o c k i s locked )

abort ;

else

t r a n s a c t i o n a l code ;
HW COMMIT;

else i f

( t r i e s >= 0 )

t r i e s −−;
r e t r y in HW;

else // r e t r i a l s quota ends

atomic add ( g b l l o c
SW BEGIN
i f

( no c o n f l i c t

, 1 ) ;

in SW)

t r i e s = Fixed NUM RETRIES ;
i f HW BEGIN begins s u c c e s s f u l l y

i f

( g b l l o c k i s locked )

abort ;

else

t r a n s a c t i o n a l code ;
HW COMMIT;

else i f
i f

( t r i e s >= 0 )

( c a p a c i t y l i m i t reached )

t r i e s = 0 ; // s w i t c h e s t o STM

else

t r i e s −−; r e t r y in HW;

else // r e t r i a l s quota ends

atomic add ( g b l l o c
SW BEGIN
i f

( no c o n f l i c t

, 1 ) ;

in SW)

t r a n s a c t i o n a l code ;
SW COMMIT;

else

SW ABORT ; r e t r y in SW;
atomic sub ( g b l l o c

, 1 ) ;

t r a n s a c t i o n a l code ;
SW COMMIT;

else

SW ABORT ; r e t r y in SW;
atomic sub ( g b l l o c

, 1 ) ;

return

return

(a) RNDHyTM / FxHyTM / StAdHyTM

(b) DyAdHyTM

Fig. 1: Random HyTM (RNDHyTM), Fixed HyTM (FxHyTM), Statically Adaptive HyTM (StAdHyTM)
pseudo code (a) and Dynamically Adaptive HyTM (DyAdHyTM) pseudo code(b)

as 1-20, 20-50, 50-100 etc. We explore the best range and then chose a ﬁxed number from the range.
The overhead is that we have to run the desired applications several times before hand to come up
with tuned retrial numbers

3.6 Dynamically Adaptive HyTM (DyAdHyTM) Implementation

In our DyAdHyTM implementation, as shown in the code listing in Figure 1b, a transaction ﬁrst tries
to execute in HTM (i.e. HW BEGIN). If no STM transaction has captured the critical section (i.e. gblloc
is free), HTM will commit (i.e. HW COMMIT). If it fails to begin in HTM due to capacity limits, the
number of retrials is forced to zero. In that case, transaction retries for the last time in HTM. If it fails,
then it falls back voluntarily to STM without any retrials. For conﬂicts, or other reasons (e.g. context
switch), it retries in HTM for a ﬁxed number of retrials. The number of retries (NUM RETRIES) is
set to a ﬁxed randomly (i.e. similar to FxHyTM). When a transaction fails it gets assigned a number
of possible retries. It takes a global lock and executes in STM (SW BEGIN). However, every time a
transaction tries to enter the critical section in HTM, it will check for the global lock’s availability
(i.e. if the critical section is already locked, the transaction will abort in HTM) which could be already
acquired by another STM. The global lock can be captured by several STMs. When an STM takes
the global lock, it increases its value by one. Therefore, when the value of the global lock decreases
to zero, the HTM transactions can go forward. Since an STM transaction can conﬂict with other STM
transactions, in that case, one of the STM transactions will succeed and commit (SW COMMIT) and the

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

A Dynamically Adaptive Hybrid Transactional Memory for Big Data

•

1:7

global lock’s value will be restored (decreased). Even if an STM transaction fails, it restores the lock’s
value. Therefore, the lock will be released eventually when an STM transaction commits or aborts.
There is no overhead of random number generator and running applications several times beforehand
to have a tuned retrial number. Only overhead is to test the ﬂags for abortion in runtime compare to
FxHyTM.

3.7 HTMs and STM Implementation

We have also implemented three versions of HTMs. (1) HTM with atomic lock (HTMALock) similar
to FxHyTM in the code listing in Figure 1a. Instead of an STM fallback, it fall backs to a lock based
execution. However, when a transaction fall backs by using a lock, it waits for the lock to be free
from other transactions before it can take the lock exclusively. This part is implemented with a while
loop. (2) The second version uses a spinlock and similar to previous HTM implementation. Hardware
transactions frequently check the availability of the lock by spinning on the lock, while in the atomic
lock version of HTM, hardware transactions atomically check for the availability of lock. (3) The last
version is Intel’s Hardware Lock Elision (HLE). The transaction ﬁrst attempts in speculative mode
and if it fails, in the next attempt it executes in non-speculative mode. This aborts other concurrent
speculative transactions. We used the STM in GCC TM draft [33].

4. EXPERIMENTAL RESULTS

In this section, we discuss the experimental results of DyAdHyTM on the SSCA-2 benchmark [2]. We
test our DyAdHyTM against three versions of HyTM, three versions of HTM, an STM and coarse grain
locking. We run the benchmark on an SMP machine that have 28 (with hyperthreading) cores backed
by 64GBs of main memory.

We choose the Scalable Synthetic Compact Applications graph analysis 2 (SSCA-2) benchmark [2]
since it is mostly used as a benchmark to test state of art TM implementations with the STAMP
benchmark [27]. The benchmark has large integer operations, a large memory footprint, and irregular
memory access pattern that mimics real world large graphs. It has multiple kernels accessing a single
data structure representing a weighted, directed multigraph. In addition to a kernel to construct the
graph from the input tuple list, there are three additional computational kernels that operate on the
graph. We only use two of the three kernels, namely, the generation kernel and the computation kernel.
The generation kernel generates a large graph with scale as input. The generated graph is based on
power-law [16] and R-MAT format [6]. The computation kernel extracts edges by weight from the
generated graph and forms a list of the selected edges.

This benchmark is readily available with an OpenMP version with its critical section having a built-
in OpenMP lock. We modiﬁed the critical section to support STM, HTM(s) and HyTM(s). All experi-
ments are conducted 20 times and the average execution time is reported. In comparing DyAdHyTM
with other state-of-the-art STMs, HTMs and HyTMs, most STMs and HyTMs [10; 25] have large over-
heads since they have more complexity in their designs which can take signiﬁcant time and cost just to
implement within the benchmark, and even then gains are not guaranteed. Therefore, we believe that
low overhead STM and HTMs implementation on the native machine are the fastest schemes that we
can compare against.

We conducted our experiments for larger scales (i.e. 23 – 27) with the SSCA-2 benchmark for all
the policies on a workstation named “Mickey”. Mickey is a single SMP node with 14 cores or 28 logical
cores if hyperthreading is enabled. It has 64GBs of memory. The SMP node has a single Xeon Broadwell
processor which has HTM implemented at the L1 and L2 caches. We show all our results for thread/core
counts 4 – 28 for better display purposes. It takes 2016.71 seconds with coarse grain lock for the two
kernels with a single core but for 14 and 28 cores, it takes 321.50 and 250.52 seconds respectability.

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

1:8

•

M. Qayum, A. Badawy and J. Cook

(a) Scale 26: Two Kernels

(b) Scale 26: Generate

(c) Scale 26: Compute

(d) Scale 27: Two Kernel

(e) Scale 27: Generate
Fig. 2: Performance improvement for 2 kernels (a) and (d), generation kernel (b) and (e), computation
kernel (c) and (f) with DyAdHyTM over coarse grain lock, STM, HLE, HTM for large scale 26 and 27 on
a 28-core SMP node with 64 GBs memory, x-axis represents thread counts, y-axis represents execution
time in seconds

(f) Scale 27: Compute

Figures 2(a) and 2(d) show the total execution time of both graph generation and computation ker-
nels of the SSCA-2 benchmark with all the synchronization policies for scales 26 and 27. Though, we
have experimented with scales 23 – 27, we only report results for 26 and 27 for space limitations.

The results show that a simplistic STM implementation outperforms coarse grain lock for all scales
and all thread counts. For scale 26, DyAdHyTM performs best among the policies with up to 1.62x
speedup compared to coarse grain lock, 1.29x speedup compared to STM, 1.50x speedup compared to
Hardware Lock Elision (HLE) version of HTM, and 1.18x speedup compared to the next best policy
(i.e. HTM with Spinlock at a thread count of 28). HLE’s performance closely matches lock, we recall
the reason from section 3 that HLE falls back to locked execution after ﬁrst attempt of speculative
execution.

Interestingly, for higher thread counts (e.g. 20–28), HTMs performs similar to STM. We believe that
some transactions are falling back to locks after they fail to execute in HTM because of conﬂicts. The
overhead of aborts of HTM transactions, falling back overheads, the cost of serial execution in a lock
are comparable to the overheads of STM since the only overhead in STMs is due to aborts due to
conﬂicts of software transactions.

The highest scale we can run on our target machine is 27. At this scale, most of the machine’s memory
(64GBs) is utilized for storing the graph (134 millions vertices and 1.03 billion edges). DyAdHyTM
implementation gains 1.62x speedup compared to coarse grain lock and 1.29x speedup compared to
STM in total execution time for both kernels at the largest thread count of 28 as is shown in Figure 2(d).

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

A Dynamically Adaptive Hybrid Transactional Memory for Big Data

•

1:9

(a) Scale 27: Two Kernels

(b) Scale 27: Computation
Fig. 3: Execution time for two kernels (a), generation kernel(b), for computation kernel (c) for RND-
HyTM, FxHyTM, StAdHyTM vs DyAdHyTM for large scale 27 on a 28-cores SMP node with 64 GBs
memory

(c) Scale 27: Computation

Both HTM policies outperform lock and STM. Finally, DyAdHyTM outperforms the next best policy
(HTM with Spinlock) with 1.23x speedup. In DyAdHyTM, since both HTM and STM transactions can
execute concurrently, we get better performance than just vanilla HTMs. However, very few HTM
transactions fail to execute even after several retries and eventually, fall back to STM although our
goal is to maximize the number of transactions that succeed in committing in HTM. This is obviously
the case since HTMs are the fastest. However, we can design an HTM without lock fallback that just
keeps retrying with HTM, but its performance will be worse than falling back to a lock or STM.

Figures 2(b) and 2(e) show the results of a performance comparison of the execution time of the
graph generation kernel only of the SSCA-2 benchmark for all the policies. The graph generation
kernel is a simple kernel with symmetric concurrency (i.e. conﬂict probability is similar). Therefore, for
all thread counts, most policies performs similarly.

Figures 2(c) and 2(f) shows results for the computation kernel. This kernel posses dynamic con-
ﬂict scenarios where threads compete against each others to update a critical section. For scale 27,
(Figure 2(f)), the computation kernel at 14 threads, DyAdHyTM has a speedup of 8.1x compared to
coarse-grain locking; and more than 2.5x compared HTM with spinlock. At thread count of 14, DyAd-
HyTM achieves the best execution time (i.e. 17.442 seconds). Beyond 14 threads, we are using Hyper-
threading (HT). In HT, the hardware threads share the caches, the micro-architecture and functional
components, and therefore threads become more resource constrained and eventually become slower
and transactions conﬂicts arise due to resource limitations. Therefore, beyond 14 threads, performance
worsens. At 28 threads, overheads and conﬂicts kill the beneﬁts of concurrency for the compute kernel.
However, overall gain in total execution time for both kernels comes from the generation kernel since
it takes 9x more time than the computation kernel (i.e. the generation kernel dominates execution of
the two kernels).

Figures 3(a) and (b) show results for all HyTM policies for the two kernels, generation and com-
putation kernels respectively. DyAdHyTM outperforms StAdHyTM by 1.4%, FxHyTM by 3.81%, and
RNDHyTM by 24.8% for two kernels for 28 threads. Recall that, FxHyTM has the lowest overhead
but has unpredictable performance and StAdHyTM has high unreported proﬁling overhead due to the
manual tuning of the number of retries. For computation kernel and 28 threads, DyAdHyTM outper-
forms StAdHyTM by 4.2%, FxHyTM by 21.8% and RNDHyTM by 155.1%. Due to the dominance of
the generation kernel, the beneﬁt are skewed. However, in a typical computation kernel on a graph,
DyAdHyTM gets the performance beneﬁts since it is adaptive at runtime with low overhead.

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

1:10

•

M. Qayum, A. Badawy and J. Cook

(a) Scale 27: HTM transactions

(b) Scale 27: Retries
Fig. 4: HTM Transactions per thread(a), HTM Retries numbers per thread(b), Number of STM trans-
actions for RNDHyTM, FxHyTM, StAdHyTM vs DyAdHyTM for large scale 27 on a 28-cores SMP node
with 64 GBs memory (b)

(c) Scale 27: STM fallbacks

Figures 4(a) and (b) show the number of hardware transactions and retries per thread for RND-
HyTM, FxHyTM, StAdHyTM and DyAdHyTM receptively. The number of retries for both StAdHyTM
and DyAdHyTM is much smaller than RNDHyTM for all thread counts at scale 27. This explains that
the speedup in both StAdHyTM and DyAdHyTM, come from higher commit success in HTM. In other
words, HTM retries are lower as it is proﬁled for StAdHyTM and adapts dynamically for DyAdHyTM.
In case of FxHyTM, its number of retries is unpredictable since there is no intelligence in selecting
the retries. For DyAdHyTM, it is further tuned by dynamically adapting to the executions by noticing
causes of aborts and adopting retries accordingly. For example, at scale 27 and 28 threads, retries are
161.4M, 171M, 6.95M and 6.78M for RNDHyTM, FxHyTM, StAdHyTM and DyAdHyTM respectively.
RNDHyTM takes a random number from 1–50 (Section 3). Also, DyAdHyTM has low overhead very
close to FxHyTM since it only has to check status of HTM aborts.

Figure 4(c)shows the number of STM transactions per thread for RNDHyTM, FxHyTM, StAdHyTM
and DyAdHyTM. The number of STM executions for both FxHyTM, StAdHyTM and DyAdHyTM are
much smaller than RNDHyTM for all the thread counts at scale 27. In case of 28 threads, DyAdHyTM
wins but StAdHyTM’s STM retries is close to RNDHyTM. Also, we are forcing HTM to switch to STM
more frequently in StAdHyTM and DyAdHyTM, therefore, STM counts are higher than FxHyTM.
Gains mostly come from a less retries or less aborts in HTM. Moreover, at higher thread counts, STM
transactions per thread is much higher since conﬂicts arise as the number of threads increase but
overall speedup comes from parallel execution of large number of threads.

5. RELATED WORK

The performance of most TM schemes depend on the implementation details. For example, most
STM designs have special purposes behind their designs e.g. NOrec [11] which works on maximiz-
ing performance at low thread counts. SwissTM [15] is optimized for high contention scenarios, and
TinySTM [17] is a lightweight word-based STM implementation suitable for low contention scenarios.
The TL2 STM [14] has high overheads but provides better scalability than NOrec.

Most HTM implementations are agnostic of the applications’ behaviors. Goel et al. [18] have exper-
imented with HTM based on RTM similar to our HTM. They showed HTM scaling well for higher
thread counts, with better execution times and lower energy consumption than TinySTM for eight
threads. Also, our STM is a low overhead implementation of TinySTM. Christie et al. [7] have shown
that AMDs proposed Advanced Synchronization Facility (ASF) which is a simulator implemented HTM

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

A Dynamically Adaptive Hybrid Transactional Memory for Big Data

•

1:11

shows better scalability and performance than STMs such as TinySTM for SSCA-2. However, ASF is
not implemented in real hardware yet and comparing it to read hardware HTMs is not possible.

The very ﬁrst papers that discussed HyTMs are Damron et al. [12] and Kumar et al. [22]. Later on,
other works included PhTM [23] which is based on the Sun ultrasparc HTM. Riegel et al. [32] proposed
a HyTM based AMD-ASF. Wang et al. [35] proposed another HyTM based on IBM Blue Gene/Qs best-
effort HTM with a Single-Global-Lock fallback. The most recent state of the art HyTMs are Hybrid
NOrec [10], Reduced Hardware-NOrec [26], and Invyswell [5]. All of them are based on existing HTMs
such as Intel’s TSX or AMD’s ASF with a single global lock.

In-memory big data processing is a major area of research due to the availability of novel policies
to utilize both large main memory and memory hierarchies as a data storage layer [37]. TM is one
such policy that can be used efﬁciently for in-memory processing of big data applications. Kang and
Bader [21] have applied STM for minimum spanning forest (MSF) problem. It is one of the earliest
experiments with TM for graph applications. Shang et al. [34] have applied a two phase synchroniza-
tion scheme in a graph application. They used coarse-grain lock (blocking) for high degree vertices
and a non-blocking scheme for low degrees vertices. They assumed that in high conﬂict scenarios,
non-blocking schemes may not work well. Our DyAdHyTM can be used in their non-blocking imple-
mentations.

Our DyAdHyTM is based on Intel’s TSX and adapts dynamically at run time. Its a low overhead so-
lution that adapts to the available resources. It is also designed with a single global lock for cooperation
between HTM and STM. Most state of the art HyTMs are designed for general purpose applications
and are tested with SSCA-2 benchmark mostly for small scales (less than 22) and with low thread
counts (≤ 8) [10; 26; 5]. Up to our knowledge, scalability for large graphs on HyTM was not considered
before.

6. CONCLUSION

There is no best synchronization policy that can ﬁt for all purposes and applications. Researchers are
looking for the appropriate synchronization policy for parallel applications that is scalable, provides
easier semantic, and adaptive. Among the available policies, TM is one of the novel, well researched
and rapidly evolving policies due to its properties of inherently non-blocking nature, availability in
some commercial processors, ongoing research, and, best of all, ease of customization for a speciﬁc
workload. Since most big data applications can be represented as large graphs which exhibits sparsity,
TM is the logical solution for such concurrent applications. However, HTM limits task sizes due to it
bounded resources while STM has speed limit due to high overheads. Therefore, we strongly believe
that adaptive hybrid TM (HyTM) is a smart choice since it combines the best features of both HTM
and STM, and can adapt to application requirements, specially to large scale graphs. Moreover, we
designed a dynamically adaptive HyTM (DyAdHyTM) with little overhead which performs best among
the all TM policies. Our DyAdHyTM provides speedup of up to 8.12x compared to coarse-grain lock for
a large graph of 134M vertices and 1.03B edges on a manycore machine.

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

1:12

•

M. Qayum, A. Badawy and J. Cook

REFERENCES

Ananian, C.S., Asanovic, K., Kuszmaul, B.C., Leiserson, C.E., Lie, S.: Unbounded transactional memory. In: High-Performance

Computer Architecture, 2005. HPCA-11. 11th International Symposium on, pp. 316–327. IEEE (2005)

Bader, D.A., Feo, J., Gilbert, J., Kepner, J., Koester, D., Loh, E., Madduri, K., Mann, B., Meuse, T.: Hpcs scalable synthetic

compact applications 2 graph analysis. SSCA 2, v2 (2006)

Bader, D.A., Madduri, K.: Designing multithreaded algorithms for breadth-ﬁrst search and st-connectivity on the cray mta-2.

In: Parallel Processing, 2006. ICPP 2006. International Conference on, pp. 523–530. IEEE (2006)

Butenhof, D.R.: Programming with POSIX threads. Addison-Wesley Professional (1997)
Calciu, I., Gottschlich, J., Shpeisman, T., Pokam, G., Herlihy, M.: Invyswell: a hybrid transactional memory for haswell’s re-
stricted transactional memory. In: Proceedings of the 23rd international conference on Parallel architectures and compilation,
pp. 187–200. ACM (2014)

Chakrabarti, D., Zhan, Y., Faloutsos, C.: R-mat: A recursive model for graph mining. In: Proceedings of the 2004 SIAM Interna-

tional Conference on Data Mining, pp. 442–446. SIAM (2004)

Christie, D., Chung, J.W., Diestelhorst, S., Hohmuth, M., Pohlack, M., Fetzer, C., Nowack, M., Riegel, T., Felber, P., Marlier, P.,
et al.: Evaluation of amd’s advanced synchronization facility within a complete transactional memory stack. In: Proceedings
of the 5th European conference on Computer systems, pp. 27–40. ACM (2010)

Chung, J., Yen, L., Diestelhorst, S., Pohlack, M., Hohmuth, M., Christie, D., Grossman, D.: Asf: Amd64 extension for lock-
free data structures and transactional memory. In: Microarchitecture (MICRO), 2010 43rd Annual IEEE/ACM International
Symposium on, pp. 39–50. IEEE (2010)

Dagum, L., Menon, R.: Openmp: an industry standard api for shared-memory programming. Computational Science & Engi-

neering, IEEE 5(1), 46–55 (1998)

Dalessandro, L., Carouge, F., White, S., Lev, Y., Moir, M., Scott, M.L., Spear, M.F.: Hybrid norec: A case study in the effectiveness

of best effort hardware transactional memory. ACM SIGPLAN Notices 46(3), 39–52 (2011)

Dalessandro, L., Spear, M.F., Scott, M.L.: Norec: streamlining stm by abolishing ownership records. In: ACM Sigplan Notices,

vol. 45, pp. 67–78. ACM (2010)

Damron, P., Fedorova, A., Lev, Y., Luchangco, V., Moir, M., Nussbaum, D.: Hybrid transactional memory. In: ACM Sigplan

Notices, vol. 41, pp. 336–346. ACM (2006)

David, T., Guerraoui, R., Trigonakis, V.: Everything you always wanted to know about synchronization but were afraid to ask.

In: Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, pp. 33–48. ACM (2013)

Dice, D., Shalev, O., Shavit, N.: Transactional locking ii. In: Distributed Computing, pp. 194–208. Springer (2006)
Dragojevi´c, A., Guerraoui, R., Kapalka, M.: Stretching transactional memory. In: ACM Sigplan Notices, vol. 44, pp. 155–165.

ACM (2009)

Faloutsos, M., Faloutsos, P., Faloutsos, C.: On power-law relationships of the internet topology. In: ACM SIGCOMM computer

communication review, vol. 29, pp. 251–262. ACM (1999)

Felber, P., Fetzer, C., Marlier, P., Riegel, T.: Time-based software transactional memory. Parallel and Distributed Systems, IEEE

Transactions on 21(12), 1793–1807 (2010)

Goel, B., Titos-Gil, R., Negi, A., McKee, S.A., Stenstrom, P.: Performance and energy analysis of the restricted transactional
memory implementation on haswell. In: Parallel and Distributed Processing Symposium, 2014 IEEE 28th International, pp.
615–624. IEEE (2014)

Hammarlund, P., Chappell, R., Rajwar, R., Osborne, R., Singhal, R., Dixon, M., D’Sa, R., Hill, D., Chennupaty, S., Hallnor, E.,

et al.: 4th generation intel R(cid:13) core processor, codenamed haswell 34(2) (2013)

Herlihy, M., Moss, J.E.B.: Transactional memory: Architectural support for lock-free data structures, vol. 21. ACM (1993)
Kang, S., Bader, D.A.: An efﬁcient transactional memory algorithm for computing minimum spanning forest of sparse graphs.

In: ACM Sigplan Notices, vol. 44, pp. 15–24. ACM (2009)

Kumar, S., Chu, M., Hughes, C.J., Kundu, P., Nguyen, A.: Hybrid transactional memory. In: Proceedings of the eleventh ACM

SIGPLAN symposium on Principles and practice of parallel programming, pp. 209–220. ACM (2006)

Lev, Y., Moir, M., Nussbaum, D.: Phtm: Phased transactional memory. In: Workshop on Transactional Computing (Transact)

(2007)

Lumsdaine, A., Gregor, D., Hendrickson, B., Berry, J.: Challenges in parallel graph processing. Parallel Processing Letters

17(01), 5–20 (2007)

Matveev, A., Shavit, N.: Reduced hardware transactions: a new approach to hybrid transactional memory. In: Proceedings of the

25th ACM symposium on Parallelism in algorithms and architectures, pp. 11–22. ACM (2013)

Matveev, A., Shavit, N.: Reduced hardware norec: An opaque obstruction-free and privatizing hytm. TRANSACT (2014)

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

A Dynamically Adaptive Hybrid Transactional Memory for Big Data

•

1:13

Minh, C.C., Chung, J., Kozyrakis, C., Olukotun, K.: Stamp: Stanford transactional applications for multi-processing. In: Work-

load Characterization, 2008. IISWC 2008. IEEE International Symposium on, pp. 35–46. IEEE (2008)

Murphy, R.C., Wheeler, K.B., Barrett, B.W., Ang, J.A.: Introducing the graph 500. Cray Users Group (CUG) (2010)
Pankratius, V., Adl-Tabatabai, A.R.: A study of transactional memory vs. locks in practice. In: Proceedings of the twenty-third

annual ACM symposium on Parallelism in algorithms and architectures, pp. 43–52. ACM (2011)

Qayum, M., Badawy, A.H., Cook, J.: Stadhytm: A statically adaptive hybrid transactional memory a scalability study on large
parallel graphsk. In: Proceedings of the The 7th IEEE Annual Computing and Communication Workshop and Conference.
IEEE (2017)

Rajwar, R., Dixon, M.: Intel transactional synchronization extensions. In: Intel Developer Forum San Francisco, vol. 2012 (2012)
Riegel, T., Marlier, P., Nowack, M., Felber, P., Fetzer, C.: Optimizing hybrid transactional memory: The importance of nonspecu-
lative operations. In: Proceedings of the twenty-third annual ACM symposium on Parallelism in algorithms and architectures,
pp. 53–64. ACM (2011)

Schindewolf, M., Cohen, A., Karl, W., Marongiu, A., Benini, L.: Towards transactional memory support for gcc. In: 1st GCC

Research Opportunities Workshop (2009)

Shang, Z., Li, F., Yu, J.X., Zhang, Z., Cheng, H.: Graph analytics through ﬁne-grained parallelism
Wang, A., Gaudet, M., Wu, P., Amaral, J.N., Ohmacht, M., Barton, C., Silvera, R., Michael, M.: Evaluation of blue gene/q hard-
ware support for transactional memories. In: Proceedings of the 21st international conference on Parallel architectures and
compilation techniques, pp. 127–136. ACM (2012)

Wang, Q., Kulkarni, S., Cavazos, J., Spear, M.: A transactional memory with automatic performance tuning. ACM Transactions

on Architecture and Code Optimization (TACO) 8(4), 54 (2012)

Zhang, H., Chen, G., Ooi, B.C., Tan, K.L., Zhang, M.: In-memory big data management and processing: A survey. IEEE Trans-

actions on Knowledge and Data Engineering (2015)

XXXX, Vol. 2, No. 3, Article 1, Publication date: February 2017.

