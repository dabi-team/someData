1
2
0
2

v
o
N
4

]

R
C
.
s
c
[

2
v
7
1
4
5
1
.
0
1
1
2
:
v
i
X
r
a

Vulnerability Characterization and Privacy
Quantiﬁcation for Cyber-Physical Systems

Arpan Bhattacharjee∗, Shahriar Badsha†, Md Tamjid Hossain‡, Charalambos Konstantinou§, Xueping Liang¶
University of Nevada, Reno, NV∗†‡, KAUST, Saudi Arabia§, University of North Carolina, Greensboro, NC¶
∗‡{abhattacharjee,mdtamjidh}@nevada.unr.edu, †sbadsha@unr.edu, §charalambos.konstantinou@kaust.edu.sa,
¶x.liang@uncg.edu

Abstract—Cyber-physical systems (CPS) data privacy pro-
tection during sharing, aggregating, and publishing is a chal-
lenging problem. Several privacy protection mechanisms have
been developed in the literature to protect sensitive data from
adversarial analysis and eliminate the risk of re-identifying
the original properties of shared data. However, most of
the existing solutions have drawbacks, such as (i) lack of
a proper vulnerability characterization model to accurately
identify where privacy is needed, (ii) ignoring data providers
privacy preference, (iii) using uniform privacy protection which
may create inadequate privacy for some provider while over-
protecting others, and (iv) lack of a comprehensive privacy
quantiﬁcation model assuring data privacy-preservation. To
address these issues, we propose a personalized privacy pref-
erence framework by characterizing and quantifying the CPS
vulnerabilities as well as ensuring privacy. First, we introduce a
Standard Vulnerability Proﬁling Library (SVPL) by arranging
the nodes of an energy-CPS from maximum to minimum
vulnerable based on their privacy loss. Based on this model, we
present our personalized privacy framework (PDP) in which
Laplace noise is added based on the individual node’s selected
privacy preferences. Finally, combining these two proposed
methods, we demonstrate that our privacy characterization and
quantiﬁcation model can attain better privacy preservation by
eliminating the trade-off between privacy, utility, and risk of
losing information.

Index Terms—Cyber-physical systems (CPS), Privacy, Vul-
nerabilities, Differential Privacy (DP), Personalized Differential
Privacy.

I. INTRODUCTION

Cyber-physical systems (CPS) such as manufacturing plants
and smart grids generate, transmit, and collect a tremendous
amount of privacy-sensitive data. This data ensures a wide
range of advantages to all involved system’s parties but
also raises a privacy risk to data providers whose shared
data can be statistically correlated to identify private in-
formation [1]. Additionally, the lack of a comprehensive
vulnerability assessment, robust data protection, and attack-
defense mechanism drives adversaries to compromise CPS
nodes [2]. Topological investigations by researchers have
shown that energy-CPS smart grids have inter-dependencies
between their different nodes [3]. Some of these nodes have
relatively higher importance as they generate essential data
for the grid operation. Identifying the vulnerabilities of these
critical nodes and ensuring a robust privacy-preservation
mechanism can improve the system’s resiliency against
privacy-compromising attacks [4]. Privacy preservation of
sensitive data (e.g., electricity usage data, utilities power

generation data, or electrical appliances location data) carries
paramount importance since their compromise can affect
the secure and reliable operation of the grid. To counter
this privacy problem, several solutions (e.g., encryption,
differential privacy (DP) [5], k-anonymity, l-diversity, etc.)
have been proposed [6]–[8]. However, these solutions lack a
proper privacy characterization and measurement technique
to identify the expected privacy requirements of individual
data providers.

Currently, uniform data privacy is provided to all the data
providers of the smart grid infrastructure, leading to inad-
equate privacy preservation for some data providers while
over-protecting others [9]. This uniform privacy protection
also introduces an unacceptable amount of noise, resulting
in poor data utility and system performance degradation.
To eliminate privacy problems in CPS, and speciﬁcally in
smart grids, our work focuses on designing a comprehensive
CPS (power) data privacy characterization and quantiﬁcation
model to identify the vulnerabilities that occur during data
sharing, publishing, or aggregating between the associated
parties. This privacy quantiﬁcation model is subsequently
integrated with the Personalized Differential Privacy (PDP)
solution in which data providers specify their personal data’s
privacy requirements. This integrated privacy characteri-
zation and protection model ensures comprehensive grid
vulnerability assessment as well as data point diversity,
and indistinguishability. As a result, adversaries who run
malicious analytics on these private datasets will not be able
to identify sensitive private data.

In this paper, we focus on introducing a vulnerability
characterization and privacy quantiﬁcation model for CPS.
The contributions can be summarized as follows:

• We introduce a Standard Vulnerability Proﬁling Library
(SVPL) using an attack graph and networks centrality
metrics to combine the risk sources, data privacy weak-
nesses, feared events, and associated harms to arrange
nodes in a rank from minimum to maximum vulnerable
based on their privacy loss scenario.

• We develop a Personalized Differential Privacy (PDP)
model where data privacy is guaranteed based on indi-
vidual data providers’ selected privacy preferences. Our
PDP model provides better data utility and privacy over
traditional Uniform Differential Privacy (UDP).

 
 
 
 
 
 
The rest of this paper is assembled as follows: Section
II provides an overview of the vulnerability characterization
and privacy-protection research in literature. The formula-
tion of the Standard Vulnerability Proﬁling Library (SVPL)
module is demonstrated step by step in section III. Section
IV describes the formulation of a PDP model where privacy
is dependent on the individual node’s privacy preference.
Section V validates the proposal using experimental analysis.
Finally, Section VI concludes the paper.

II. RELATED WORK

Due to the integration of CPS in critical infrastructures and
the increasing number of attack incidents in such interop-
erable and interconnected architectures [10], characterizing
and quantifying the vulnerabilities of CPS is of paramount
importance. Researchers have proposed vulnerability anal-
ysis based on attack graphs [11], standard vulnerability
scoring systems (CVSS) [12], and network centrality metrics
[13]. However, defense mechanisms such as deployments of
ﬁrewall or intrusion detection systems are proved ineffective
[14] to protect the CPS. Moreover, the lack of proper vul-
nerability characterization models also makes it difﬁcult to
accurately identify the system’s vulnerabilities and provide
an appropriate situational awareness capability.

Nowadays, data privacy preservation has become an issue
of paramount importance. Aggrawal et al. [15] ﬁrst intro-
duced data privacy preservation in 2008 to mitigate privacy-
compromising access and analytics to industrial system’s
private data. The goal of privacy-preserving techniques is
to protect private information against being published or
revealed by unauthorized as well as authorized users [16].
Several privacy-preserving approaches such as encryption-
based, perturbation-based, authentication-based, and tech-
nologies like blockchain have been proposed in the literature
to protect the sensitive information of CPS effectively [17]–
[21]. Regarding privacy preservation works in CPS, Gope
et al. [22] proposed a privacy-preserving scheme to analyze
the energy consumption data of end-users securely. Other
researchers like Gai et al. [23] have introduced a blockchain-
based technique to address privacy leakage challenges in
smart grids.

There are some existing studies on differential privacy
(DP) and Laplace noise addition mechanism in the existing
literature to accomplish effective data privacy protection
[24]–[26]. However, not many researchers have focused
on personalized differential privacy (PDP) least of PDP
in CPS to improve the data utility while preserving CPS
data privacy. Recently, PDP-based techniques have become
popular in social networks [27], location-based applications
[28], and recommendation systems [29] because of their
optimization power to improve data utility while assuring
privacy. To counter these limitations of existing works, we
have adopted PDP for the CPS system by characterizing
the system’s vulnerabilities, which will help achieve optimal
data privacy while maintaining utility and reducing the risk
of data disclosure.

Fig. 1. Vulnerability Characterization Framework.

III. PROPOSED VULNERABILITY CHARACTERIZATION

A. Smart Grid Topological Relationship

Electric grids can be represented with nodes (system buses)
and their associated connectivity links (e.g., transmission
and distribution lines). Such relationship can be modelled
as a graph G(N , L) where N and L represents the number
of nodes and communication links. Multiple communication
links connected to the same node pair are considered as one
link here. The interconnectivity of the graph G(N , L) is
represented by a N × N adjoint matrix A where Aik = 1
if the nodes i to k are coupled by a direct link; otherwise
Aik = 0. The N × N adjacent matrix is presented as a
Laplacian matrix L to present the properties of the grid
topology in L = ∆ - A, where ∆ = diag (e1, . . . , eN ) is
the diagonal edges of the matrix connecting the centers of
opposite nodes with the edges and their associated elements
Ei = (cid:80)N
B. Vulnerability Characterization Framework

k=1 aik.

In this work, we use an attribute-based attack graph to assess
the network’s vulnerabilities. The vulnerability characteriza-
tion framework incorporates ﬁve parts as shown in Fig. 1.
That are CPS network properties, attack graph creation, vul-
nerability proﬁle generation, privacy loss assessment using
SVPL and maximum privacy loss calculation, and at last
vulnerability characterization and quantiﬁcation module. The
evaluation includes the following steps:

• First, we generate an attack graph by analyzing existing
information like the grid’s topological relationship to
traverse and assess the previous attacked incidents to
collect the vulnerabilities of the compromised nodes.
• This attack graph is then leveraged to build SVPL by
calculating the vulnerability scores generated from risk
sources, data privacy weaknesses, feared events, and
associated harms.

• According to the computed vulnerability score,

the
maximum to minimum privacy loss is calculated by
combining the privacy loss magnitude and the fre-
quency of the successful privacy-compromising attacks.
• Finally, this maximum privacy loss assists in identifying
the network’s key fragile points by creating the best
attack proﬁle that will help the grid utilities to reinforce
their budgets appropriately and ensure defense before
an attack occurs.

C. Vulnerability Assessment by Building an Attack Proﬁle

D. Standard Vulnerability Proﬁle Generation

In order to perform a complete privacy risk assessment and
develop the vulnerability attack proﬁle, we need at ﬁrst
to establish a common link by deﬁning the relationship
between the “feared events”, “risk sources”, and “privacy
weaknesses” to model their impacts called “privacy harm”.
Deﬁnition 1 (Risk Source): Risk sources deﬁne an individ-
ual or organizational entities whose legitimate or illegitimate
owning of a dataset and actions causes intentional or
unintentional privacy harm to the system.

Deﬁnition 2 (Privacy Weaknesses): Privacy weakness is
a limitation of the data protection mechanisms’ security
measures, including poor choice of security functionality or
error-prone system design, which ultimately causes privacy
harms in the system.

Deﬁnition 3 (Feared Event): Feared events are the mali-
cious actions on the system that exploit the system’s privacy
weaknesses, leading to privacy harm.

Deﬁnition 4 (Privacy Harm): Privacy harm is the neg-
ative impact on datasets, individual data providers, or the
network resulting from one or more feared events to com-
promise the system’s physical, cyber or ﬁnancial stability.

Based on these four attributes and traditional attack graph
model [30], we develop our vulnerability assessment attack
graph (VAAG), which comprises eight tuples as follows:
V AAG = (N, E, P C, M, D, R, P LM, F P LE)

• N represents the nodes of the system which consists of
start node Sn, attacked nodes An as well as attacked
path Ap, where, N = Sn ∪ An ∪ Ap. If An ∈ Ap, then
An is a compromised node on the attack path.

• E represents the edge set of all N nodes where E ⊂
(Sn × Ap) → (Ap × An) , and where every privacy
attack is denoted by an action a in E.

• P C denotes the physical connectivity of the nodes,
link

is the physical

where P C ⊆ (N × Nl). Nl
between the nodes.

• M is a mapping function that maps the edge sets E
to the compromised set C, which is M : E → C. In
the VAAG, every malicious attack (a ∈ E) is the result
of a vulnerability C(e) on that node according to two
privacy risk factors: Privacy Loss Magnitude (PLM) and
Frequency of Privacy Loss Events (FPLE).

• D denotes the vulnerability dependencies of the nodes
presented by di : vi → vj,. This means if adversaries
try to compromise node N then both vi and vj vulner-
ability set is needed.

• The privacy risk score is calculated by decomposing
two risk factors: PLM which denotes the privacy attacks
on the data points; and FPLE, which presents the suc-
cessful occurrences of an attack by a malicious actor.
This PLM and FPLE are calculated from the existing
vulnerability information of the CPS infrastructure.
The overall risk is calculated by combining these risk
factors: R = P LM * F P LE.

The formulation of our proposed SVPL to identify and
characterize the vulnerabilities of the CPS nodes mainly
consists of three parts: traversing the compromised nodes,
developing an attack graph, and using these sub-level attack
graphs to generate the nodes’ global attack proﬁle. In
this process of traversing and identifying the compromised
nodes, we start our process by going through the existing
vulnerability information of the nodes V and the attack
incidents AI and using it as a baseline proﬁle to generate
global vulnerability assessment attack graph “VAAG”. The
attack incident set AI is the set of attack preconditions
AIP and attack consequences AIC. The vulnerability proﬁle
generation follows Algorithm 1, where AI at ﬁrst checks
if there is an available attack incident by traversing and
matching the current information with the system’s existing
information. If abnormalities are found in the new node
N then it
is considered as a compromised node. This
compromised node N will be used to ﬁnd the previously
connected nodes by linking through the edges and putting
the whole compromised node-set in the CN and their edges
in E. If this newly vulnerable node is not included in the
V set, it will be added to the existing vulnerable node set.
Finally, every node of CN will be matched with the AI set
to proﬁle the different vulnerabilities.

Algorithm 1: Vulnerability Proﬁle Generation
Input: Nodes existing vulnerability set V , attack
incident set AI and newly added nodes N .
Output: Newly Compromised nodes CN and their
edge sets E.
Step 1: if AI (cid:54)= ∅ then Return
Step 2: for each ai ∈ AI
if N ∈ ai· AIp ∧ (ai · AIp − {N }) ⊂ V ; then
1. Start attack node creation An
2. CN ← CN ∪ {An}
3. for each targeted nodes Tn ∈ ai · AIp
4. E ← E ∪ {< Tn, An >}
5. for each Tn ∈ ai · AIc
if Tn /∈ V then
6.
7. V ← V ∪{ Tn} }; End

Step 3: for each An ← N

8. CN Match (V, AI, AIp & AIc)
End; Return CN, E

IV. PERSONALIZED DIFFERENTIAL PRIVACY

This section demonstrates our proposed PDP solution based
on traditional differential privacy. We assume every source
node is a data provider who owns the system’s private
data. Similarly, any destination node (fog, cloud) is involved
in data transmission and aggregation. This paper aims to
ensure every source node will share their sensitive data with
the destination nodes under a PDP protection mechanism.
Our privacy characterization module provides an SVPL to
identify the adversaries and vulnerabilities beforehand but

can not guarantee privacy preservation of raw data in real-
time. To eliminate this limitation, our proposed personalized
privacy protection model ensures privacy by adding cus-
tomized noise generated from Laplace distribution which
depends on every source node’s preference that is shown
in Table I. For example, based on the criteria of access to
nodes distance is crucial because longer distances (more
hops) will force the transmitting data to travel
through
more malicious nodes and raises the likelihood of getting
compromised. To measure this distance, we follow network
centrality metrics (e.g., degree, eigenvector, betweenness,
and closeness centrality) to rank the nodes in the graph and
formulate the weighted smart grid architecture and employ
the shortest path algorithm to ﬁnd the actual distances
between the nodes.

A. Personalized Differential Privacy Modelling

In this section, we modeled the nodes (source, destination)
based on their privacy preference. We assume that a source
node Sn when transferring its private data pd ∈ PD, where
PD presents the private data needs privacy protection before
sending the data to the destination. The privacy protection
mechanism starts with the source node Sn when releasing
the data to its adjacent fog nodes Fn; it at ﬁrst calculates
a approximate data d(cid:48)
s by mixing different noise with the
original data and release this d(cid:48)
s data instead of the sources
original data PD. This approximate data d(cid:48)
s needs to satisfy
(cid:17)
the source nodes privacy protection preference ε ∈
where ∆ij is the distance function D : ∆ij : Sn → D and
ε : Dκ → Dκ presents the mapping function that transforms
(cid:16) 1
the distance ∆ij into ε ∈
personalized differential
∆ij
privacy preference level.

(cid:16) 1
∆ij

(cid:17)

B. Calculating Distance between Nodes of the Graph

In this layered grid architecture, attackers can compro-
mise private data from any source/destination node. This
means the more signiﬁcant transmission distance between
the source and destination node causes more nodes to be
involved in each one-hop distance for receiving, storing, and
sending the transmitted data. This causes more untrusted
nodes to be involved, raising the risk of getting compromised
by malicious adversarial attacks. However,
to keep the
privacy protection approach stable and efﬁcient, we set a
maximum threshold distance T Hd beyond which we will
provide a random privacy loss (ε). So, the nodes’ effective
distance calculation in our weighted graph follows a least-
cost approach using the “Dijkstra Algorithm” where the
shortest path is calculated by ﬁnding the least cost from
one node to every other connected node. So, the distance
∆ij between the source and destination node is calculated
as follows: ∆ij = D(n) − S(n).

C. Laplace Distribution and ε-Personalized Privacy

The Laplace mechanism adds preferred noise to the private
data from the Laplace distribution to fulﬁll each node’s
personal differential privacy preference. The privacy loss is

TABLE I
CRITERIA MAPPING TO PERSONALIZED PRIVACY PRESERVATION.

Criteria/Nodes
Access to nodes
Access frequency to Private Data
Attackers background knowledge
Communication medium
Operational complexity

Edge
High
High
Low
Low
High

Cloud

Fog
Medium Low
Medium Low
Medium High
Medium High
Medium Low

employed by using the Laplace mechanism L to inject ε-
personalized private noise Lap( ∆f
ε ) to the sensitive data D,
that can be denoted as: L = f (D) + Lap( ∆f
ε ). This ensures
ε-differential privacy to transform each nodes’ private data
PD to a noisy data d(cid:48)
s.

D. Mechanism of Achieving PDP

The formulation of PDP follows sequential composition
mechanism which guarantees the composition of (cid:80)l
i εi.
there is a set of source nodes S =
For example,
{S1, S2 . . . Sn} , and their corresponding Laplacian mecha-
nisms L = {L1, L2 . . . Lm} , generates ε1, ε2 . . . εm privacy.
For every Li needs ensuring ε-differential privacy which
ultimately guarantees the composition of (cid:80)l
i εi-differential
privacy. Here, (cid:80)l
i εi refers upper privacy bound of sequential
composition mechanism. So, when a dataset is released from
a node it goes through randomized privacy loss (ε) based on
individual nodes preference and the ﬁnal privacy guarantee
will be the summation of total privacy loss ε.

E. Criteria Mapping to Personalized Privacy Preference

The traditional differential privacy follows a uniform privacy
protection for all nodes; thus, it compromises the data utility.
To overcome this limitation, we have adopted a privacy
preference proﬁle that is assigned to all of the nodes based on
some criterion as shown in Table I. The privacy preference
of each node is mapped as high, medium, and low depending
on the desired privacy and utility requirement. For example,
it is comparatively easier to access the edge nodes than the
fog and cloud nodes. Hence, privacy and security attacks
(e.g., eavesdropping, membership inference, data poisoning,
etc.) are more likely to occur in the edge nodes than the other
nodes which is why high data privacy is desired in the edge
nodes. Here to ﬁnd the appropriate ε-differential privacy, we
exp(Kt
i x)
use the following mapping function: L (εi) =
m=1 expKM ;
where, i = 1, K presents a parameter, and KM is the
mapping parameter.

(cid:80)M

F. Proof: Personalized Differential Privacy (PDP)
To guarantee personalized ε-privacy, we at ﬁrst take a source
node with two different privacy preference εp and εp+1,
where εp+1 > εp and similarly we take another node with
two different privacy level εq and εq+1, where εq+1 > εq.
The sequential composition mechanism of our system is
built upon the upper bound which guarantees (cid:80)n
i εi privacy.
So, the PDP for the two nodes is as follows: P DP (εp +
εq) = P DP (εp + 1) or P DP (εp + εq) = P DP (εq +

1). Upon observing this equation, we can see εp < εq+1
and εq < εp+1 which refers that the two nodes with noisy
responses generate different level of privacy-preserved data.

V. EXPERIMENTAL ANALYSIS

We conduct the model evaluation based on following criteria:

A. Utility Analysis

In the CPS infrastructure, the data is generated from the edge
nodes and transmitted to the cloud node through the fog
nodes. In edge nodes, the data utility is ‘1’ and data privacy
is ‘0’ since edge nodes generate and preserve granular level
data where no privacy is added to the data. Privacy is added
only when edge nodes start sharing their data with the
fog and cloud nodes. Any privacy-preserving mechanism
(e.g., differential privacy) incurs utility loss over the original
value which can be measured through different methods
(e.g., MSE, MAE, Max Error, RMSE, MedAE, etc.). In
our experiment, we interpret the utility loss of the fog and
cloud nodes by Mean Absolute Error (MAE) which can be
i=1|yi−xi|
calculated using the following formula: MAE =
n
where, MAE = mean absolute error; yi= privacy preserved
value; xi= original value and n = total number of data points.
To measure the data utility loss on the scale of ‘0’ to ‘1’, we
have calculated the percentage deviation (PD) using MAE
and Mean (i.e., P D = M AE/M ean) and then subtracting
the percentage deviation from ‘1’. Here, the data utility of
‘1’ reﬂects the highest possible utility of the nodes when no
privacy preservation mechanism is applied and vice versa.

(cid:80)n

B. Privacy Analysis

Our privacy analysis is conducted by generating an inter-
active privacy preference proﬁle based on some criteria as
shown in Table I for every edge, fog, and cloud node. Based
on the discussion on Subsection IV-E, edge nodes require
high data privacy as they are more frequently accessed
than other nodes, and their resources are limited which in
turn restricts them to applying other privacy and security
measures (e.g., encryption, ﬁrewall, etc.) [31]. However, if
the attacker can compromise the fog or central cloud he
can exploit the aggregated data to conduct more devastating
attacks. So, in this case, the fog and cloud nodes require
better privacy than the edge node.

C. Risk Factor Analysis

In risk factor analysis, we consider the risk of disclosure and
re-identiﬁcation of individual data points. The risk factor is
high when the data privacy level is low. More speciﬁcally, if
the noise of the differential privacy mechanism is low then
the attacker can easily re-identify individual data points by
conducting multiple aggregation queries and averaging out
the results. This correlation is also reﬂected in our empirical
evaluation outlined in the discussed part of this paper.

D. Simulation and Discussion

1) DP Evaluation:

In this section, we conducted our
experimental analysis using a micro-grid data-set of 443
homes collected from UMASS Repository [32]. The dataset
contains minute-level data of a particular day (24 hours) for
these 443 homes. Fig. 2 (a) illustrates the corresponding
features of the dataset (e.g., consumption, home id, and
timestamp). We can see that during the busy hours starting
from 8 a.m to 6 p.m the consumption is signiﬁcantly higher
than other hours. Moreover, approximately, the ﬁrst 100
homes consume higher energy than the rest of the homes. We
considered these minute-level data as the granular data and
the homes as the edge nodes. Then, we observed the impact
of differential privacy over all the granular data of these
edge nodes (Fig. 2 (b). It can be inferred that higher privacy
loss (alternatively, lower noise) yields better data utility.
That means if the privacy loss is increased (e.g., ε increased
gradually from 0.1 to 0.4 in the ﬁgure) the privatized value
remains more close to the granular value. Furthermore, we
investigated the probability distribution of the differentially
private data as shown in ﬁg 2 (c). The probability distribution
function shows that the laplacian noise of the differential
privacy mechanism is more clustered around the mean when
we increase the privacy loss (i.e., when ε = 0.4). Contrarily,
the noise spreads out more and more if we continue to
decrease the privacy loss (ε). Here, the PDF of the fatter
tailed curve (ε = 0.1 in Fig. 2 (c)) provides more noise and
thus ensures better privacy.

2) Comparative Analysis between Personalized and Uni-
form Privacy: To achieve a comprehensive understanding of
the importance of applying PDP over UDP, we selected a
set of two different privacy levels for both the fog and cloud
aggregation which is ε = 0.6 and ε = 0.8. Data aggregation

Fig. 2. DP Evaluation: (a) Actual Consumption, (b) Impact of DP over actual consumption with varying privacy Loss, and (c) Distribution of Noise.

Fig. 3. Distribution of losses over uniform and personalized privacy.

is not required in the edge nodes; rather aggregation function
is performed while transferring the data from the edge node
to the fog and fog to the cloud. In UDP setting, two cases
are demonstrated where both the fog and cloud aggregation
adopts the same privacy levels at a time (i.e., case 1: (εf , εc)
= {0.6, 0.6}; case 4: (εf , εc) = {0.8, 0.8}). However, in the
PDP set up we considered the same privacy level but in
an alternative manner (i.e., case 2: (εf , εc) = {0.6, 0.8};
case 3: (εf , εc) = {0.8, 0.6}). Fig. 3 (a) and (b) depict the
loss distribution of these four cases for uniform and PDP
accordingly while Fig. 3 (c) presents the mean and standard
deviation of these loss distributions. Generally, the higher
standard deviation yields better privacy since the noise is
distributed widely from the mean value. Also, the higher
mean yields to higher loss (alternatively, lower data utility).
So, for the case 1, where both the standard deviation and
mean are high, privacy will be highest but the data utility will
be lowest. In contrast, for the case 4, the privacy will be the
lowest, and data utility will be the highest. So, if we consider
uniform privacy, it may offer excessive privacy control to
a subset of nodes while applying insufﬁcient protection to
another subset.

However, if we consider the PDP setting, we can achieve
optimized data privacy and security for this we need to
consider cases 2 and 3. In the application, where data privacy
is more desirable than the data utility case 2 will be preferred
over case 3 and vice versa. We have considered privacy loss
0.6 and 0.8 in our experiment but different sets of privacy
loss can also be opted. Nevertheless, the distribution follows

the same trend as Fig. 3 for any set of privacy loss.

3) Correlation among Privacy Loss, Data Utility and
Data Disclosure Risk : We outlined the data disclosure or
re-identiﬁcation risk for the same four cases under uniform
and PDP settings as shown in Fig. 4. The correlation among
the privacy loss, data utility, and data disclosure risk shows
that risk and utility decrease along with the increment of
privacy loss (ε) in the UDP settings. For instance, in case of
house id (HID) 114 if we increase the privacy loss from 0.6
to 0.8, the risk decreases approximately from 0.81 (Fig. 4(a))
to 0.79 (Fig. 4(d)) for both the fog and cloud aggregation.
However, if we consider the PDP for the same house, the
minimum attainable risks can be 0.61 (for cloud aggregation,
Fig. 4(b)) and 0.79 (for fog aggregation, Fig. 4(c))). If the
CPS analysts or customers pull the data from the cloud node,
then the case 2 (i.e., εf , εc = {0.6, 0.8}, Fig. 4(b)) is more
preferable over the rest three cases since the cloud risk factor
is the lowest for this case.

Moreover, a little increment in the privacy loss (ε) of fog
aggregation (εf ) yields to larger risk increment in the cloud
aggregation. For example, for the same house id (i.e., 114),
increasing εf from 0.6 to 0.8 increases the cloud aggregation
risk from 0.81 to 1.01 (Fig. 4(a) and Fig. 4(c)). Although
the UDP decreases the disclosure risk factor by increasing
the fog and cloud privacy losses (εf , εc), it does not consider
the data utility in the process. In contrast, a PDP mechanism
tunes both the fog and cloud privacy loss levels to the
optimized values (ε∗
c ) to attain optimal data privacy and
data utility with optimal disclosure risk.

f , ε∗

Fig. 4. Comparative analysis between privacy loss and disclosure risk of private information for uniform and personalized privacy.

VI. CONCLUSION AND FUTURE WORK

This paper demonstrates a comprehensive vulnerability char-
acterization and privacy quantiﬁcation as well as a protection
model to deal against the security and privacy-related issues
in CPS during data generation, transmission, and collection.
The proposed SVPL model helps to identify security and
privacy vulnerabilities of the CPS system accurately by
building a standard vulnerability proﬁle considering each
data provider’s privacy loss scenario. Furthermore, a cus-
is also introduced to improve the
tomized, PDP model
data utility by reducing data disclosure risk while assuring
high-level data privacy protection for the CPS. Finally,
our experimental analysis compares proposed personalized
differential privacy (PDP) with uniform differential privacy
and proves that this PDP model ensures better data privacy
protection while maintaining the data utility and minimizing
the risk of data disclosure.

In the future, we plan to demonstrate a real-life privacy
characterization model using IEEE bus systems and formu-
lating different node attack strategies to show the SVPL
model’s effectiveness, which is currently out of this paper’s
scope.

REFERENCES

[1] J. Giraldo, E. Sarkar, A. A. Cardenas, M. Maniatakos, and M. Kantar-
cioglu, “Security and privacy in cyber-physical systems: A survey of
surveys,” IEEE Design & Test, vol. 34, no. 4, pp. 7–17, 2017.
[2] G. Lee, G. Epiphaniou, H. Al-Khateeb, and C. Maple, “Security and
privacy of things: Regulatory challenges and gaps for the secure in-
tegration of cyber-physical systems,” in Third International Congress
on Information and Communication Technology. Springer, 2019, pp.
1–12.

[3] H. Cetinay, K. Devriendt, and P. Van Mieghem, “Nodal vulnerability
to targeted attacks in power grids,” Applied network science, vol. 3,
no. 1, pp. 1–19, 2018.

[4] M. N. Aman, M. H. Basheer, and B. Sikdar, “Data provenance for
iot with light weight authentication and privacy preservation,” IEEE
Internet of Things Journal, vol. 6, no. 6, pp. 10 441–10 457, 2019.
[5] M. T. Hossain, S. Badsha, and H. Shen, “Privacy, security, and
utility analysis of differentially private cpes data,” arXiv preprint
arXiv:2109.09963, 2021.

[6] S. H. Begum and F. Nausheen, “A comparative analysis of differential
privacy vs other privacy mechanisms for big data,” in 2018 2nd
International Conference on Inventive Systems and Control (ICISC).
IEEE, 2018, pp. 512–516.

[7] A. Bhattacharjee, S. Badsha, and S. Sengupta, “Personalized privacy
preservation for smart grid,” in 2021 IEEE International Smart Cities
Conference (ISC2), 2021, pp. 1–7.

[8] A. Bhattacharjee, “Integrity and privacy protection for cyber-physical
systems (cps),” Ph.D. dissertation, University of Nevada, Reno, 2021.
[9] Z. Jorgensen, T. Yu, and G. Cormode, “Conservative or liberal?
personalized differential privacy,” in 2015 IEEE 31St international
conference on data engineering.

IEEE, 2015, pp. 1023–1034.

[10] D. Peterson, “Ics-cert: Stuxnet lessons learned,” Digital Bond, 2010.
[11] T. Gonda, T. Pascal, R. Puzis, G. Shani, and B. Shapira, “Analysis of
attack graph representations for ranking vulnerability ﬁxes.” in GCAI,
2018, pp. 215–228.

[12] V. Mavroeidis and S. Bromander, “Cyber threat intelligence model:
an evaluation of taxonomies, sharing standards, and ontologies within
cyber threat intelligence,” in 2017 European Intelligence and Security
Informatics Conference (EISIC).

IEEE, 2017, pp. 91–98.

[13] H. Peng, C. Liu, D. Zhao, H. Ye, Z. Fang, and W. Wang, “Security
analysis of cps systems under different swapping strategies in iot
environments,” IEEE Access, vol. 8, pp. 63 567–63 576, 2020.

[14] J.-P. A. Yaacoub, O. Salman, H. N. Noura, N. Kaaniche, A. Chehab,
and M. Malli, “Cyber-physical systems security: Limitations, issues
and future trends,” Microprocessors and Microsystems, vol. 77, p.
103201, 2020.

[15] C. C. Aggarwal and S. Y. Philip, “A general survey of privacy-
preserving data mining models and algorithms,” in Privacy-preserving
data mining. Springer, 2008, pp. 11–52.

[16] M. Keshk, E. Sitnikova, N. Moustafa, J. Hu, and I. Khalil, “An
integrated framework for privacy-preserving based anomaly detection
for cyber-physical systems,” IEEE Transactions on Sustainable Com-
puting, 2019.

[17] M. Pan, J. Wang, S. M. Errapotu, X. Zhang, J. Ding, and Z. Han, Big
Springer,

Data Privacy Preservation for Cyber-Physical Systems.
2019.

[18] A. Bhattacharjee, S. Badsha, A. R. Shahid, H. Livani, and S. Sengupta,
“Block-phasor: A decentralized blockchain framework to enhance
security of synchrophasor,” in 2020 IEEE Kansas Power and Energy
Conference (KPEC).
IEEE, 2020, pp. 1–6.

[19] M. T. Hossain, S. Badsha, and H. Shen, “Porch: A novel consensus
mechanism for blockchain-enabled future scada systems in smart grids
and industry 4.0,” in 2020 IEEE International IOT, Electronics and
Mechatronics Conference (IEMTRONICS).

IEEE, 2020, pp. 1–7.

[20] S. Islam, S. Badsha, and S. Sengupta, “A light-weight blockchain
architecture for v2v knowledge sharing at vehicular edges,” in 2020
IEEE International Smart Cities Conference (ISC2), 2020, pp. 1–8.

[21] A. Bhattacharjee, S. Badsha, and S. Sengupta, “Blockchain-based
secure and reliable manufacturing system,” in 2020 International Con-
ferences on Internet of Things (iThings) and IEEE Green Computing
and Communications (GreenCom) and IEEE Cyber, Physical and
Social Computing (CPSCom) and IEEE Smart Data (SmartData) and
IEEE Congress on Cybermatics (Cybermatics), 2020, pp. 228–233.

[22] P. Gope and B. Sikdar, “An efﬁcient privacy-preserving authentication
scheme for energy internet-based vehicle-to-grid communication,”
IEEE Transactions on Smart Grid, vol. 10, no. 6, pp. 6607–6618,
2019.

[23] K. Gai, Y. Wu, L. Zhu, M. Qiu, and M. Shen, “Privacy-preserving
energy trading using consortium blockchain in smart grid,” IEEE
Transactions on Industrial Informatics, vol. 15, no. 6, pp. 3548–3558,
2019.

[24] M. Keshk, B. Turnbull, N. Moustafa, D. Vatsalan, and K.-K. R.
Choo, “A privacy-preserving-framework-based blockchain and deep
learning for protecting smart power networks,” IEEE Transactions on
Industrial Informatics, vol. 16, no. 8, pp. 5110–5118, 2019.

[25] M. U. Hassan, M. H. Rehmani, and J. Chen, “Differential privacy
techniques for cyber physical systems: A survey,” IEEE Communica-
tions Surveys Tutorials, vol. 22, no. 1, pp. 746–789, 2020.

[26] M. T. Hossain, S. Islam, S. Badsha, and H. Shen, “Desmp: Differ-
ential privacy-exploited stealthy model poisoning attacks in federated
learning,” arXiv preprint arXiv:2109.09955, 2021.

[27] X. Gong, X. Chen, K. Xing, D.-H. Shin, M. Zhang, and J. Zhang,
“Personalized location privacy in mobile networks: A social group
utility approach,” in 2015 IEEE Conference on Computer Communi-
cations (INFOCOM).

IEEE, 2015, pp. 1008–1016.

[28] R. Wen, W. Cheng, H. Huang, W. Miao, and C. Wang, “Pri-
vacy preserving trajectory data publishing with personalized dif-
ferential privacy,” in 2020 IEEE Intl Conf on Parallel Distributed
Processing with Applications, Big Data Cloud Computing, Sus-
tainable Computing Communications, Social Computing Networking
(ISPA/BDCloud/SocialCom/SustainCom), 2020, pp. 313–320.
[29] T. Bao, L. Xu, L. Zhu, L. Wang, and T. Li, “Successive point-of-
interest recommendation with personalized local differential privacy,”
IEEE Transactions on Vehicular Technology, pp. 1–1, 2021.

[30] S.-c. Liu and Y. Liu, “Network security risk assessment method based
on hmm and attack graph model,” in 2016 17th IEEE/ACIS inter-
national conference on software engineering, artiﬁcial intelligence,
networking and parallel/distributed computing (SNPD).
IEEE, 2016,
pp. 517–522.

[31] S. Islam, I. Zografopoulos, M. T. Hossain, S. Badsha, and C. Kon-
stantinou, “A resource allocation scheme for energy demand manage-
ment in 6g-enabled smart grid,” 2021.

[32] T. Zhu, A. Mishra, D. Irwin, N. Sharma, P. Shenoy, and D. Towsley,
“The case for efﬁcient renewable energy management
in smart
homes,” in Proceedings of the Third ACM Workshop on Embedded
Sensing Systems for Energy-Efﬁciency in Buildings, 2011, pp. 67–72.

