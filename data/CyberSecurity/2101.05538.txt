1
2
0
2

n
a
J

4
1

]

R
C
.
s
c
[

1
v
8
3
5
5
0
.
1
0
1
2
:
v
i
X
r
a

Cyber Taxi:
A Taxonomy of Interactive Cyber Training
and Education Systems

Marcus Kn¨upfer1, Tore Bierwirth1,2, Lars Stiemert1,2, Matthias Schopp1,2,
Sebastian Seeber1,2, Daniela P¨ohn1,2, and Peter Hillmann1

1 Universit¨at der Bundeswehr M¨unchen, 85577 Neubiberg, Germany
{marcus.knuepfer, peter.hillmann}@unibw.de
2 Team localos, Munich, Germany
{tore.bierwirth, lars.stiemert, matthias.schopp, sebastian.seeber,
daniela.poehn}@localos.io

Abstract. The lack of guided exercises and practical opportunities to
learn about cybersecurity in a practical way makes it diﬃcult for secu-
rity experts to improve their proﬁciency. Capture the Flag events and
Cyber Ranges are ideal for cybersecurity training. Thereby, the par-
ticipants usually compete in teams against each other, or have to de-
fend themselves in a speciﬁc scenario. As organizers of yearly events, we
present a taxonomy for interactive cyber training and education. The
proposed taxonomy includes diﬀerent factors of the technical setup, au-
dience, training environment, and training setup. By the comprehensive
taxonomy, diﬀerent aspects of interactive training are considered. This
can help trainings to improve and to be established successfully. The
provided taxonomy is extendable and can be used in further application
areas as research on new security technologies.

Keywords: Capture the ﬂag · Cyber Range · Cybersecurity ·
Cyber Defence · Cyber Education.

1

INTRODUCTION

With increasing digitalization and the integration of computers into the daily
life, the number of threats is also rising. The amount of sophisticated attacks
is increasing every year and poses a challenge in terms of eﬃcient detection
and countermeasures. At the same time, the required technical knowledge of an
intruder decreases with the development of more automated tools [27,6]. The
defence against these attacks is based on the continuous use and monitoring
of security tools. Well-trained personnel is required for this. However, current
statistics show a global shortage of approximately four million information se-
curity professionals [17]. In recent years, many diﬀerent training systems have
been developed with focus on cybersecurity. Companies have emerged oﬀering
new certiﬁcations and universities are developing cybersecurity degree programs.
Interactive training systems are further approaches that follow our motivation

 
 
 
 
 
 
2

M. Kn¨upfer et al.

as cybersecurity trainers: Theoretical knowledge is good, practical proﬁciency is
better. These systems utilize gamiﬁcation and playful scenarios to train partici-
pants in speciﬁc topics [1].

Interactive training systems oﬀer the opportunity of real-time feedback and
speciﬁc education. The focus is on instructing the participants with an eﬃcient
method and the desirable knowledge. These systems create awareness for security
threats and the resulting impact in real life. In order to provide an overview of a
large range of practical possibilities, we present a taxonomy to structure interac-
tive cyber training and education. This overview allows universities, companies,
and other institutions to

– identify gaps in their training system.
– improve existing training systems and intensify the training.
– provide a guideline for establishing new training possibilities.

The taxonomy includes surrounding aspects, e.g., training of teamwork, com-
munication capabilities as well as reporting processes. As far as we know, there
is no holistic taxonomy for practical and interactive cyber training and educa-
tion. Such a taxonomy is mandatory to support education, structure the entire
area, and accelerate further research. It enables a comprehensive evaluation of
interactive training systems and the visualization of the requirements gap.

The paper is structured as follows: Section 2 establishes the requirements for
a taxonomy. In the following section, we brieﬂy discuss related approaches with
their advantages and disadvantages. In Section 4, we describe our taxonomy in
detail. Afterwards, we classify other examples with our taxonomy and show the
practical beneﬁt. Beside this, we discuss the presented aspects on our case study.
Finally, we conclude the paper and give future directions.

2 DEFINITION OF TERMS AND REQUIREMENTS

We deﬁne Interactive Cyber Training and Education (ICTE) as follows:

Deﬁnition: ICTE is a comprehensive set of hands-on approaches in a secure and
observable environment that enables participants to become engaged in learning
and practice their cyber skills and to acquire new skills.

For a better understanding of the following taxonomy, a common basis of
deﬁnition is mandatory. In general, a taxonomy structures a knowledge ﬁeld to
provide an overview about a speciﬁc area and its possibilities. It divides the topic
hierarchical into main groups and subcategories. A taxonomy should focus on
the following, ideal properties [24,26]:

– The categories have to be mutually exclusive, i.e., no overlapping between

the categories.

– Clear and unambiguous classiﬁcation criteria.
– Comprehensible and useful as well as comply with established terminology.

Cyber Taxi

3

Beside the requirements for the taxonomy of ICTE, there are the following

demands on systems:

– What are the optimal approaches and motivation for developing new cyber

skills?

– What skills and competencies in security are required to move to a more

proactive position?

– Which kind of training system requires which functionalities and possibili-

ties?

– What changes in terms of process, technology, and staﬀ are required in the

operational environment to support new abilities?

– What are the business objectives and strategic goals of an organization from

a security point of view?

Furthermore, there are the following assessment criteria to further evaluate
a classiﬁed training system. These criteria follow the National Institute of Stan-
dards and Technology (NIST) [31], which developed the cybersecurity workforce
framework for the National Initiative for Cybersecurity Education (NICE). The
requirements are tantamount to the ISO/IEC 25010 [19] and ISO 9126 [18] stan-
dards, which focus more on system and software quality properties. These are
also valid to our scenario.

– Functional Completeness: The degree of realization to cover all the spec-

iﬁed tasks and user objectives.

– Functional Correctness: The degree to which the cyber training system
provides the correct and reproducible results with the needed degree of pre-
cision.

– Learnability: Eﬃciency to achieve the speciﬁed goal of learning individual

and as cooperative team.

– Operability: The diﬃculty to run the training.
– Accessibility: Amount of expertise to be successful in a scenario.
– Adaptability: The degree to customize the scenarios to the expertise of the

participants.

– Portability: The degree to transfer scenarios from an education system to

another.

– Maintainability: Diﬃculty to operate the system and to fulﬁll a training

session.

– Modularity: Possibility to adapt and extend the system to current needs.

3 RELATED WORK

Several diﬀerent taxonomies were developed, e.g., taxonomies for Computer Sys-
tem Attack Classiﬁcation [2,32,29,51,23,47,14]. These list a comprehensive set of
attacks and focus on a structural overview of attacks. Jouini et al. [21] clas-
sify security threats in information systems by threat source, agent, motivation,

4

M. Kn¨upfer et al.

intention, and impact. Easttom and Butler [12] describe a taxonomy of cyber
attacks based on a modiﬁed McCumber cubes. Amongst others, they classify
the categories transmission, storage, technology, policy and practices, educa-
tion, training, and awareness. Simmons et al. [39] propose a taxonomy of cyber
attacks called AVOIDIT, classifying attack vector, operational impact, defence,
informational impact, and target. All these taxonomies can be used to develop
security challenges, such that a wide spectrum of knowledge is necessary to solve
them.

Diﬀerent approaches focus on Cyber Range Training. ECSO explains cyber
ranges in their WG5 Paper [13], but neither a deﬁnition nor a taxonomy is
given. Priyadarshini [34] analyses and classiﬁes existing cyber ranges based on
infrastructure association, cloud usage, teams, and deployment. Other aspects
are left out. Yamin et al. [52] build a taxonomy based on literature review.
The taxonomy includes scenarios, monitoring, learning, management, teaming,
and environment. Based on the taxonomy the authors describe several tools used
within cyber ranges. The taxonomy is speciﬁc for cyber ranges and does not take
target audience, proﬁciency level, and scoring, amongst others, into account.

Several papers describe Cybersecurity Exercises. INCIBE [11] analyses cyber
exercises and builds a short taxonomy based on the factors focus, model, verti-
cal sector, scope for participation, and dissemination of results. It has a rough
structure and a high level focus. The perspective is only for coverage of subject
areas and educational view. Beyer and Brummel [4] describe diﬀerent factors for
eﬀective cybersecurity training. Kick [22] depicts playbooks in detail. Others an-
alyze diﬀerent environments for cyber training [42,44,48,3,28]. Diﬀerent papers
relate to aspects of gamiﬁcation, serious games, and education [43,20,45,25]. As
a result, a holistic taxonomy for the management and organization of ICTE is
missing.

4 TAXONOMY

Within this Section, our taxonomy for ICTE and its components is described in
detail. Figure 1 provides an overview about the taxonomy. During the design,
attention was paid to the complete coverage of all necessary capabilities with
regard to the cyber exercise life cycle [22] and training competencies [31].

4.1 Technical Setup

The technical setup consists of environment structure, deployment, and orches-
tration. These are described in detail in the following.

Environment Structure: The environment structure refers to the basic char-
acteristic of the event. This characteristic is composed of the following sub-
characteristics:

– Tabletop Style: A session that involves the movement of counters or other

objects round a board or on a ﬂat surface.

Cyber Taxi

5

Fig. 1. Taxonomy of Interactive Cyber Training and Education Systems

– Online Platform: The digital service describes a wide range of interactive
possibilities available on the internet including marketplaces, search engines,
social media, creative content outlets, app stores, communications services,
payment systems, services comprising the collaborative economy.

• Collaboration Platform: The environment allows organizations to in-
corporate real-time communication capabilities and providing remote ac-
cess to other systems. This includes the exchange of ﬁles and messages
in text, audio, and video formats between diﬀerent computers or users.
• E-Learning Platform: A software application for the administration,
documentation, tracking, reporting, and delivery of educational courses,
training programs, or learning and development programs.

6

M. Kn¨upfer et al.

– Hosting: A cyber training based on single hosts uses primarily a personal
computer to providing tasks and challenges for a user. It allows a direct
interaction with the systems.

– Network Infrastructure: Dependent of the realization type - simulated,
emulated, or real - a network-based environment consists of servers and
clients, which are connected to each other in a local area network (LAN)
or wide area network (WAN).

• Real: Physical components are used to connect the systems and to setup

a scenario.

• Simulated: A simulation copies the network components from the real
world into a virtual environment. It provides an idea about how some-
thing works. It simulates the basic behavior but does not necessarily
abide to all the rules of the real systems.

• Emulated: An emulator duplicates things exactly as they exist in real
life. The emulation is eﬀectively a complete imitation of the real thing.
It operates in a virtual environment instead of the real world.

Deployment: The environment of cyber training can either be deployed on
premise or on cloud infrastructures, as shown in the following.

– On Premise: The environment for the training can either run on physical
or virtual machines. Either way, the data is stored locally and not on cloud;
nor is a third party involved. The beneﬁt of virtual machines is the maximum
of conﬁgurability. The advantages of on premise solutions are the physical
accessibility, which makes it possible to use the complete range of cyber
challenges.

– Cloud: A training setup deployed in the cloud has on-demand availability
of computer system resources, especially data storage and computing power,
without direct active management by the user. In contrast to on premise
setups, cloud solutions are rapid elastic on request. So the training can be
adapted ﬂexible on a large amount of users and is easily usable world wide.

Orchestration: We understand orchestration as the composition of parts and
components of a pool of tasks. The goal is to setup a holistic scenario and inte-
grate cyber training session. Furthermore, it includes a declarative description
of the overall process in the form of a composite and harmonic collaboration.
A system typically exists of functions, processes, and data. It provides a com-
mon service embedded in an environment for the speciﬁed purpose. Beside this,
orchestration has also a strong relation to the deployment strategy and the cus-
tomization possibilities. Well known approaches are tools like Chef [7], Puppet
[35], Ansible [37], and SaltStack [38].

A ﬂexible concept for orchestration and maintainability is important for the
administration of the cyber training system. Especially, the possibility of a fast
troubleshooting in case of live events is mandatory. Nevertheless, it also has an
impact on the participants in relation to user experience and quality of service
in providing a training with adequate atmosphere.

Cyber Taxi

7

The criterion orchestration is further divided in the degree of process au-

tomation, portability, maintainability, and compatibility.

– Automation: It speciﬁes the automation of processes and the amount of hu-
man interaction with the system to maintain and administrate, especially for
repetitive exercise. Subclasses for automation are non-automation, partially-
automation, and full-automation.

– Portability: The possibility to exchange data, challenges, or entire scenar-
ios to other environments or locations. The portability can be separated
in miscellaneous approaches and the usage of common data format for ex-
change like YAML, Extensible Markup Language (XML), and JavaScript
Object Notation (JSON) [41,16]. The objective of future research direction
is an exchange format for entire cyber scenarios to ﬂexible deploy these in
diﬀerent environments and locations.

– Maintainability: Maintainability represents eﬀectiveness and eﬃciency with
which a session can be modiﬁed or adapted to changes. A modular concept
has advantages in reusability and combinability.

– Compatibility: The Compatibility deals with the technical interaction pos-

sibilities via interfaces to other applications, data, and protocols.

4.2 Audience

The target of cyber training and education is the audience, which is further char-
acterized in the following. The audience has the characteristics sector, purpose,
proﬁciency level, and target audience.

Sector: The sector from which the audience comes determines the nature of
the training. The following categories can be distinguished [5,8].

– Academic: This includes universities and schools. The focus is on the prin-

ciples underlying cybersecurity, ranging from theoretical to applied.

– Private: The private sector and industry focuses more on protecting its
investments. The eﬀectiveness of security mechanisms and people are more
important than principles they embody.

– Public: This includes amongst others Government, NGO, and Military. Cy-
bersecurity is seen as tool to protect the public interest. Hence, it emphasizes
on developing policies and systems to implement laws and regulations.

Purpose: Purpose answered the question for which reason trainings should be
used. Training can address diﬀerent objectives which are listed in the following.

– Awareness: To raise the awareness in multiple and diﬀerent security threats.
– Skill: To recognize the diﬀerent skill levels of the participants so that can

they be improved in a targeted manner.

– Collaboration: To improve the cooperation within a team or beyond.
– Communication: To increase the eﬃciency of internal and external com-

munication in case of an incident.

– Leadership: To improve the management and coordination of the respon-

sible entities.

8

M. Kn¨upfer et al.

Proﬁciency Level: Proﬁciency describes the knowledge of users and what they
are able to do. The proﬁciency is grouped into three diﬀerent levels.

– Beginner: The lowest level. Beginner are limited in abilities and knowledge.
They have the possibility to use foundational conceptual and procedural
knowledge in a controlled and limited environment. Beginners cannot solve
critical tasks and need signiﬁcant supervision. They are able to perform daily
processing tasks. The focus is on learning.

– Professional: The mid level. Professionals have deeper knowledge and un-
derstanding in speciﬁc sectors. For these sectors they are able to complete
tasks as requested. Sometimes supervision is needed but usually they perform
independently. The focus is on enhancing and applying existing knowledge.
– Expert: The highest level. Experts have deeper knowledge and understand-
ing in diﬀerent sectors. They complete tasks self-dependent and have the
possibilities to achieve goals in the most eﬀective and eﬃcient way. Experts
have comprehensive understanding and abilities to lead and train others.
The focus is on strategic action.

Target Audience: Target audience describes the audience, which is targeted
by the training. This can be condensed to the following.

– Student/Trainee: Student and trainees have little to none practical knowl-
edge. Training can be used for students and trainees, to enhance their knowl-
edge and to practice theoretical courses, see [43,20,45,25].

– IT User: IT users use the IT but have little to none knowledge about IT
security. Users can get trained to understand principles of IT security and
to grow awareness.

– IT Professional: Professionals have little to medium knowledge about IT
security. Their professional focus is in speciﬁc sectors, therefore, they receive
IT security knowledge for their sectors.

– IT Specialist: Specialists already have a comprehensive knowledge in IT

security. Therefore, the training is focussed on speciﬁc aspects.

– Management: Management has little knowledge about IT security, but
a broad overview. By the training, management can understand changed
settings better.

4.3 Training Environment

The training environment details the environment around the training, consisting
of training type and scenario. Both characteristics are described in the following.

Training Type: Education in cybersecurity follows diﬀerent approaches [53].
The level of interaction and hands-on experience distinguishes diﬀerent types of
training. For interactive cyber training, the following training types exist.

Cyber Taxi

9

– Table Top: This type is a lightweight, but intellectually intense exercise. In
this setting, the involved teams or participants focus on opposing missions.
On a theoretical basis, the teams develop diﬀerent strategies and countermea-
sures to explore the oﬀensive cyber eﬀects on operations. Table top trainings
could be based on speech-only, text-only, or multimedia [46,22].

– Project Approach: In this type of training, hands-on projects are to be
completed during the training. Thereby, the participants learn and under-
stand the basic concepts of security. During the projects, the teachers can
intervene and control the learning process [53].

– Capture the Flag: Capture the Flag (CTF) is a well-known cybersecurity
contest in which participants compete in real-time. Several distinct kinds of
CTF have evolved in the recent years, including quiz, jeopardy, attack-only,
defence-only, and attack-defence [9].

– Cyber Training Range: A cyber range provides an environment to prac-
tice network operation skills. It should represent real-world scenarios and
oﬀer isolation from other networks to contain malicious activity. In this train-
ing type, complex attacks take place in a simulated environment. The partici-
pants perform divers educational hands-on activities according to their role.
Possible trainings are classroom practice, single team, and multiple team
trainings. In these trainings the roles that are not covered by participants
are simulated or covered by the instructors [10,48].

Scenario: The scenario is a main component of cybersecurity training. Scenarios
are needed to reach the goal of the training and are described by the following
characteristics.

– Supervision: Either the training is supervised or unsupervised. Cyber range
trainings are typically supervised, while jeopardy CTFs are unsupervised.
– Style: The style describes how the diﬀerent challenges within the training are
setup. Free-/Multi Choice can be the case with CTFs. Other directions are
problem-driven and storyline-driven, if the challenges are arranged around
a problem or a central storyline.

– Challenges: The challenges are the content of the training. These are de-
ﬁned by target and type. The target of the training can be a network, host,
application, protocol, data, person, or physical. For solving the challenge
types, foot-printing, scanning, enumeration, exploitation, pivoting, privi-
lege escalation, covering tracks, and maintaining access may be needed.
Souissi [40] and Lehto [25] use similar characterisations for attacks respec-
tively competence areas.

4.4 Training Setup

The training setup further describes the training itself with the scoring, roles,
the training mode as well as the customization level.

10

M. Kn¨upfer et al.

Scoring: Scoring is an important component of a cyber training. Depending on
the purpose of the training, the scoring provides means to motivate the partic-
ipants and a way to give feedback. It is also used to track the progress during
a training. For competition-oriented trainings, like CTFs, a scoring is necessary.
The scoring can be based, but is not limited to monitoring systems, deﬁned
objectives, or over-the-shoulder evaluation mechanisms.

– Awarding: In this variant of scoring, participants get awards for predeﬁned
actions or achievements. These awards can be granted both manually and
automatically. Furthermore, a mixed approach is possible, e.g., by automat-
ically giving awards for general objectives and manually giving awards for
outstanding achievements. In general, awarding has a lower granularity than
the detailed assessment and requires less administrative eﬀort, but gives rea-
sonable feedback and motivation for the participants.

– Assessment: This scoring variant is more complex than awarding and allows
to assess participants and compare them to each other. The assessment scores
can be assigned in diﬀerent ways. One type is the static setting of diﬀerent
scores for tasks and objectives. In order to distinguish it from awarding, the
degree of diﬃculty can be included here. Furthermore, the scores for diﬀerent
tasks can be set dynamically using mathematical functions. But also other
dynamic methods, such as the Elo Rating System [33], are covered by this
variant.

– No Scoring: Depending on the training, a scoring is not necessarily needed.

Roles: Participants in a training are split in diﬀerent teams, according to their
skills, role, and tasks during a training. For the identiﬁcation, each team has
a color assigned based on its role. The following teams are commonly used in
cyber trainings and exercises [22,49].

– Green Team: The operators that are responsible for the exercise infras-
tructure build this team. Before a training, this team sets up and conﬁgures
the environment and takes it down afterwards. During a training, it also
monitors the environments health and handles problems that may arise.
– White Team: This team consists of instructors, referees, organizers, and
training managers. They design the training scenario including objectives,
rules, background story, and tasks. During the training, this team controls
the progress and assigns tasks to the teams. These so-called injects also in-
clude simulated media, operation coordination, or law enforcement agencies.
Giving hints for the training teams could also be part of this team.

– Red Team: This team consists of people authorized and organized to model
security adversaries. They are responsible to identify and exploit potential
vulnerabilities present in the training environment. Depending on the train-
ing environment, the tasks can follow a predeﬁned attack path.

– Blue Team: The group of individuals that is responsible for defending the
training environment. They deal with the red team’s attacks and secure the
compromised networks. Guidelines for that team are the training rules and
local cyber law.

Cyber Taxi

11

Additionally, there are further roles involved in training, which are summa-

rized in the following teams [50,52].

– Transparent Team: Members of this team observe the training. Usually,
these people have a deﬁned purpose, but have no inﬂuence on the train-
ing itself. Possible purposes are learning about the training topic and roles,
studying strategies of participants, or supervising employees.

– Yellow Team: Members of this team perform not only tasks like generat-
ing legitimate network traﬃc and user behavior but also perform erroneous
actions that lead to vulnerabilities and attacks. This team can also include
the regular system builders, like programmers, developers, and software en-
gineers and architects.

– Purple Team: In a training, this team is a bridge between red and blue
teams that helps to improve the performance of both. Through joint red-
blue activities it improves the scope of the training participants. Goals are
to maximize the Blue Teams capability and the eﬀectiveness of Red Teams
activities.

– Gray Team: Bystanders of a training form this team. They do not neces-
sarily have a speciﬁc intention or purpose, but an interest in the training
event itself. It is also possible that this team interacts with participants and
thereby unintentionally inﬂuences the training.

– No speciﬁc role: Individuals who do not ﬁt into the deﬁned teams can be

assigned to this role.

According to [50], the orange team completes the so called Color Wheel. This

team is of special importance in a holistic system development process.

Training Mode: Training mode deﬁnes the mode in which the training is
accomplished. The training mode has three diﬀerent alignments.

– Single: A single player plays against others. Others can be real persons, but

also scripted opponents.

– Team: A team plays against others. In this alignments, each player can bring
its expertise into the training, focussing on diﬀerent aspects. Examples are
Blue and Red Teams.

– Group: A group plays against others. In this setting, the group members
might not know each other. Example are CTF competitions and training for
the entire organization in a breach scenario.

Customization Level: Depending on the goal of the training, the training
setup can be customized. A distinction is made here between three variants.

– General: A general purpose training setup is not, or only little customized.
This variant is suited for an entry level training or to learn about general
processes without regard to the underlying setup.

12

M. Kn¨upfer et al.

– Speciﬁc: The training setup can be customized for a speciﬁc training goal
or target audience. Examples for this variant are speciﬁc trainings within
the High School education [15] or for the health sector [36].

– Individual: The most tailored variant is an individual customization. Hereby,
the training setup corresponds to a real environment in the best possible way.
Exemplary uses of this variant are the training of teams in their environment
or the training of new expert-level employees.

5 EXAMPLES AND CASE STUDIES OF CYBER
TRAINING AND EDUCATION SYSTEMS

A tailored modiﬁcation of a CTF for educational purposes is described by [28],
named Class CTF (CCTF). The idea behind this approach is to maximize the
learning outcome and minimize the time spent for a CTF event. They observed
a higher motivation of their students helping each other solving the challenges.
Also much more interest in learning and practicing skills during hands-on ex-
ercises was noticed. This seems to be a usual behavior if changing the learning
setup to more hands-on exercises. Nevertheless, this approach can be categorized
in our taxonomy as follows: The training setup is based on Red and Blue team
roles. To give all participants the same chances, the presented scoring is based on
ﬁxed solutions for every challenge and, therefore, static. CCTFs follow a team
based approach and are designed for students in universities to improve their
skills and foster communication and collaboration abilities. The proﬁciency level
starts from beginner level and can evolve during a series of CCTFs.

The presented taxonomy progressed during our own Capture the Flag ex-
periences. Whereas this taxonomy was very basic during the ﬁrst phase of our
initial CTF in 2015, it evolved further during the next events. Table 2 sum-
marizes our past events with some details. In the next section, our CTFs are
categorized based on the taxonomy. This is followed by our experiences during
the organization of various CTF events.

Duration Teams Attendees Challenges

Tracks

Year Title
2015 The Beginning

2016 A New Hope

2017 24:

The Revolution

2018 Dark Fiber

9 h

8 h

24 h

18 h

2019 The 5th Element

18 h

11

14

18

24

29

31

49

69

92

129

34 + 1 Easteregg Beginner,
Advanced
18 + 2 Eastereggs Beginner,
Advanced,
Professional

48 + 2 Eastereggs Beginner,
Advanced
Jeopardy,
Attack-Defence

39 + 3 Eastereggs,
3 Scenarios/Maps
50 + 2 Eastereggs Jeopardy

Fig. 2. Overview of accomplished CTF events. Each session consisted of an additional
Online-Qualifying over one week.

Cyber Taxi

13

5.1 Application of the Taxonomy

Since our goal was to organize an event for the students of our university, we
followed an approach to support their skills, their team spirit, and nevertheless
fun during the event. That said we classify our CTFs based on our taxonomy as
follows.

– Audience

• Target Audience: Students and IT professionals.
• Sector: Public (NGOs, government agencies), and academic.
• Proﬁciency Level: Beginner up to expert level.
• Purpose: Improve skills and collaboration abilities within the teams.

– Training Environment

• Training Type: Capture the ﬂag including quiz and jeopardy. In 2019,

we also set up an attack-defence track.

• Scenario: Storyline-driven including some free-/multiple choice ques-

tions..
– Training Setup

• Scoring: Assessment, as it uses dynamic scoring.
• Roles: No speciﬁc roles.
• Training Mode: Team.
• Customization Level: Individual, because the incentive is to provide

an environment near to real world scenarios.

– Technical Setup

• Environment Structure: Online platform in sense of E-Learning plat-

form, and hosting.

• Deployment: On-premise.
• Orchestration: Partial degree of automation and modular approach for

designing the services.

5.2 Phase 1: Organization and Development

In the ﬁrst step, it starts with an idea to organize a CTF event. Meetings are
scheduled and all participants are on a hype that the event will be a great suc-
cess. But latest during the ﬁrst crunch-time, the hype ends and all participants
realize the hard work. Therefore, it is necessary to structure and organize the
phases. Choose a project management that ﬁts for the needs. It is important to
have regular meetings, but also a ticketing system and version control for the
challenges and underlying infrastructure.

During the ﬁrst meetings, the audience for the event and the requirements
need to be deﬁned. We used our taxonomy, as shown in the previous section,
to help ourselves. The classiﬁcation of a planned event allows organizers to de-
rive additional constraints, e.g., to support multiple proﬁciency levels based on
categories or changing the level of diﬃculty based on successful solves of chal-
lenges. We followed an approach to support their skills, their team spirit, and
nevertheless fun during the event. This includes a story-driven scenario including

14

M. Kn¨upfer et al.

some free-/multiple choice questions, e.g., eastereggs. Thereby, we develop sev-
eral challenges for beginner as well as experts. In order to provide equality and
increase the motivation, we provide diﬀerent tracks with separate scoring based
on the proﬁciency level. We have learned from experience that dynamic scoring
makes it easier to determine the individual diﬃculty of the various challenges.
As a result, the training setup includes our own developed scoring system. We
promote no speciﬁc roles in our setup, but in the last years some teams joined
the event with observer participants.

As an advice from our own experiences during the development of an event,
we recommend planning suﬃcient buﬀers for unexpected issues regarding chal-
lenges and infrastructure. This saves the organizers from excessive crunch-time.
Our technical setup was complex so far. It includes real network infrastructure
and on-premise hardware for hosting the environment. On top of these hard-
ware machines, we setup a virtual environment consisting of virtual machines
and virtual networking infrastructure comparable to a real data centre. To min-
imize the workload during the event, we tried to orchestrate availability checks
and restarts of vital services.

5.3 Phase 2: Testing and Dry Run

The main part of the second phase is testing. During this phase, all challenges
are checked by an individual or small team independent from the developers of
a challenge. If the event is based on a story line, all transitions to following chal-
lenges and activations of challenges after a successful solve need to be checked.
Furthermore, it is necessary to keep in mind that nothing is more frustrating for
participants than investing a lot of time in solving a challenge that is buggy or
not solvable. Therefore, testing is more than essential for a successful event. The
testing phase is also a good point in time to check the overall progress in devel-
oping challenges. To support the participants during the event, walk-throughs
should be developed and tested in parallel. If a hint system is planned for the
main event, it should also be checked during this phase. Nevertheless, every hint
system has its own pros and cons. We did not ﬁnd a satisfactory hint system
for all participants so far; except for providing no hints for a single team. Fi-
nally in this phase, check the infrastructure readiness for the number of planned
participants.

5.4 Phase 3: Accomplishment

The accomplishment phase usually takes place at two points in time: (1) qual-
ifying and (2) main event. During the qualifying event, it is possible to gain
experiences for the main event. Are the challenges too easy to solve? Is it nec-
essary to split the participants into diﬀerent tracks based on their skills? Are
infrastructure resources planned adequately? Did the challenges need a supervi-
sion or are they robust enough? Taking these possible questions into account, the
setup can be adjusted where appropriate. In case of diﬀerent categories based

Cyber Taxi

15

on proﬁciency level, the participants should decide which track to choose. Addi-
tionally, it is important to plan enough resources in infrastructure and staﬀ for
the main event. It is usual that not all challenges and storylines run as expected.
Preparation should also include unforeseen events such as network and power
failures, unavailable challenges or challenges that cannot be solved even if tested
in advance.

The event should start with a short introduction about the ”Do’s and Don’ts”
during the event. All participant need to understand that any attempt to break
the infrastructure or manipulation of the scoring system leads to disqualiﬁcation.
Have a system in place to monitor such attempts. If photos are taken during the
event or names of persons or teams are published afterwards, the participants
have to be asked for consent. Consideration should be given in advance to where
this information will be placed afterwards. This is necessary for the terms under
which it is allowed to do so, keeping in mind regulatory requirements. Further-
more, a monitoring system should be in place to track the availability of all
challenges and scoring systems. Necessary services should restart automatically
in case of a failure as there is no time for manual troubleshooting during the
event.

Regarding the IT security of the participants and infrastructure, the ”Do’s
and Don’ts” are presented in the ﬁrst step. Yet, various technical measures are
feasible to prevent, e.g., manipulation of virtual machines, containers or net-
work infrastructure. Virtual Machines (VMs) virtualize an underlying computer,
whereas with containers the operating system is virtualized. Each way has conse-
quences for the security, resources, reproducibility of exploits, and permissions.
If a participant is able to break out of a docker container, other challenges could
be manipulated in other containers or directly on the VM. Furthermore, if a
participant is able to break out the virtual machine, also other VMs on the host
or even the hypervisor can be manipulated. Therefore, it is vital to make even
a short risk assessment of your infrastructure hosting the challenges. If a team
is manipulating their own challenges and infrastructure, it might be acceptable
to disqualify the whole team. However, if it cannot be determined who has ma-
nipulated a challenge or infrastructure, it may be necessary to cancel the entire
event. That would be very frustrating for the other teams playing a fair game.

5.5 Phase 4: Cleanup and Maintenance

The last phase starts with the collection of evaluation sheets handed out during
the event. The feedback of the participants is a good measure to improve the
quality of a CTF and at the same time input for further events. Dependent on the
feedback form and participants, it includes feedback about the diﬃculty of the
challenges, the setup, but also about the fringe. The traﬃc and log ﬁles generated
during the event are a good starting point for further analytics. Outcomes could
include information on team strategies, toolsets used, and additional solutions to
your challenges If desirable, initiate a call for write-ups and provide a platform for
the teams to share their solutions. It can be inspiring to read write-up of diﬀerent
solutions. Furthermore, traﬃc collected during the event showing various kinds

16

M. Kn¨upfer et al.

of attacks to provided services is very valuable for security research. To reuse the
hosted challenges for further events, clean-up the provided machines afterwards.
An easy solution is either using container or making a snapshot of all resources
in advance. This enables the reset to this snapshot after the event. To keep the
system save, shutdown all resources if they are not in use between events.

It must be ensured that all participants have given their consent for relevant
information to be collected from evaluation sheets, log ﬁles, traﬃc, pictures,
and team names. In case of collected data from a participant that did not gave
the consent, these data need to be deleted immediately. Subsequently, the data
should be prepared for publishing, depending on the needs. Additionally, spon-
sors deﬁnitely welcome a short report about the event. Further, if the event was
a success, publishing the results gives the organization and development team a
good standing in organizing cybersecurity events.

6 CONCLUSION AND FUTURE WORK

Cyber training and education systems are an important aspect in education of
diﬀerent persons in each sector. Interactive training can help to improve the
security knowledge in a practical way. The development of such systems as well
as the extension and improvement can be hampered because of a missing general
classiﬁcation. This is especially the case for speciﬁc systems with focus on an
individual use case.

To overcome this shortcoming, we developed a ﬂexible taxonomy for ICTE
systems. The taxonomy provides a detailed description of all components with
a focus on technical realization. All phases of the exercise life cycle are covered
within the taxonomy to obtain a holistic approach. It supports the education
and training of diﬀerent roles under consideration of the current skill level. This
allows a targeted teaching in speciﬁc scenarios. In a next step, we showed exam-
ples of education and training systems, before we provided a case study based
on conducted trainings. The trainings were categorized by our taxonomy. These
examples further explain diﬀerent training types.

In the future, we will conduct a survey about training systems, in order to
apply the taxonomy to further systems. For example, we will show the fully com-
patibility and extensibility with the NIST NICE Framework [30]. This can help
to enrich the taxonomy with more details. Further non-technical possibilities are
ICTE in context of assurances and certiﬁcation levels. Beside this, the taxonomy
motivates the discussion and highlights the advantages of such systems. In order
to improve the trainings, we will compare diﬀerent types of scoring method-
ologies and provide a better suited one. As diﬀerent scenarios are developed, a
universal scenario description format or language helps to compare and exchange
scenarios. This will be designed in addition.

References

1. Amorim, J.A., Hendrix, M., Andler, S.F., Gustavsson, P.M.: Gamiﬁed Training for
Cyber Defence: Methods and Automated Tools for Situation and Threat Assess-

Cyber Taxi

17

ment. Nato Modelling & Simulation Group (NMSG) Multi-Workshop, MSG-111
(2013)

2. Amoroso, E.: Fundamentals of Computer Security Technology. Prentice-Hall (1994)
3. Beuran, R., Pham, C., Tang, D., Chinen, K.i., Tan, Y., Shinoda, Y.: Cyberse-
curity Education and Training Support System: CyRIS. IEICE Transactions on
Information and Systems E101.D, 740–749 (Mar 2018)

4. Beyer, R.E., Brummel, B.: Implementing Eﬀective Cyber Security Training for
End Users of Computer Networks. SHRM-SIOP Science of HR Series: Promoting
Evidence-Based HR 3(10), 2018 (2015)

5. Bishop, M.: What do we mean by ”computer security education”? In: 22nd Na-

tional Information Systems Security Conference (1999)

6. CERT Division: CERT Coordination Center - 2002 Annual Report. Tech. rep.,

Carnegie Mellon University, Software Engineering Institute (2003)

7. Chef Software Inc.: Chef (2020), https://www.chef.io, (Accessed on August 28,

2020)

8. CONCORDIA: Courses and Trainings for Professionals (2020), https://www.
concordia-h2020.eu/map-courses-cyber-professionals/, (Accessed on August
28, 2020)

9. Davis, A., Leek, T., Zhivich, M., Gwinnup, K., Leonard, W.: The Fun and Future of
CTF. In: 2014 USENIX Summit on Gaming, Games, and Gamiﬁcation in Security
Education (3GSE 14) (2014)

10. Davis, J., Magrath, S.: A Survey of Cyber Ranges and Testbeds. Tech. rep., Defence
Science and Technology Organisation Edinburgh (Australia) Cyber and Electronic
Warfare Div (2013)

11. D´ıez, E.G., Pereira, D.F., Merino, M.A.L., Su´arez, H.R., Juan, D.B.: Cyber exer-
cises taxonomy. INCIBE (2015), https://www.incibe.es/extfrontinteco/img/
File/intecocert/EstudiosInformes/incibe_cyberexercises_taxonomy.pdf,
(Accessed on August 28, 2020)

12. Easttom, C., Butler, W.: A Modiﬁed McCumber Cube as a Basis for a Taxonomy
of Cyber Attacks. In: 2019 IEEE 9th Annual Computing and Communication
Workshop and Conference (CCWC). pp. 943–949 (2019)

13. European Cyber Security Organisation: WG5 Paper - Understanding Cyber

Ranges: From Hype to Reality. Tech. rep. (Mar 2020)

14. Hansman, S., Hunt, R.: A taxonomy of network and computer attacks. Computers

& Security 24, 31–43 (Feb 2005)

15. Hembroﬀ, G., Hanson, L., Vanwagner, T., Wambold, S., Wang, X.: The Develop-
ment of a Computer & Network Security Education Interactive Gaming Architec-
ture for High School Age Students. The USENIX Journal of Education in System
Administration 25 (2015)

16. Howard, J.D., Longstaﬀ, T.A.: A Common Language for ComputerSecurity Inci-

dents. Tech. rep., Sandia National Laboratories (1998)
and Growing

Strategies
for Building
Cybersecurity Workforce

17. (ISC)2:
Teams.
org/-/media/ISC2/Research/2019-Cybersecurity-Workforce-Study/
ISC2-Cybersecurity-Workforce-Study-2019.ashx,
28, 2020)

(Accessed

(2019),

Study

Strong Cybersecurity
https://www.isc2.

on August

18. ISO/IEC: ISO/IEC 9126. Software engineering – Product quality. ISO/IEC (2001)
19. ISO/IEC 25010: ISO/IEC 25010:2011, Systems and software engineering — Sys-
tems and software Quality Requirements and Evaluation (SQuaRE) — System and
software quality models. ISO/IEC (2011)

18

M. Kn¨upfer et al.

20. Jin, G., Tu, M., Kim, T.H., Heﬀron, J., White, J.: Game Based Cybersecurity
Training for High School Students. In: Proceedings of the 49th ACM Technical
Symposium on Computer Science Education. pp. 68–73. SIGCSE ’18, Association
for Computing Machinery, New York, NY, USA (2018)

21. Jouini, M., Rabai, L.B.A., Aissa, A.B.: Classiﬁcation of Security Threats in Infor-
mation Systems. The 5th International Conference on Ambient Systems, Networks
and Technologies (ANT-2014), Procedia Computer Science 32, 489–496 (2014)
22. Kick, J.: Cyber Exercise Playbook. MITRE (2014), https://www.mitre.org/

sites/default/files/publications/pr_14-3929-cyber-exercise-playbook.
pdf, (Accessed on August 28, 2020)

23. Kumar, S.: Classiﬁcation and Detection of Computer Intrusions. Ph.D. thesis, Pur-

due University, USA (1996)

24. Landwehr, C.E., Bull, A.R., McDermott, J.P., Choi, W.S.: A taxonomy of com-

puter program security ﬂaw. ACM Computing Surveys (1994)

25. Lehto, M.: Cyber Security Education and Research in the Finland’s Universities
and Universities of Applied Sciences. International Journal of Cyber Warfare and
Terrorism 6, 15–31 (Apr 2016)

26. Lindqvist, U., Jonsson, E.: How to Systematically Classify Computer Security In-

trusions. IEEE Symposium Security and Privacy pp. 154–163 (1997)

27. Lipson, H.F.: Tracking and Tracing Cyber-Attacks: Technical Challenges and
Global Policy Issues. Software Engineering Institute, CERT Coordination Center
(2002)

28. Mirkovic, J., Peterson, P.A.H.: Class Capture-the-Flag Exercises. In: 2014 USENIX
Summit on Gaming, Games, and Gamiﬁcation in Security Education (3GSE 14).
USENIX Association, San Diego, CA (Aug 2014)

29. Neumann, P.G., Parker, D.B.: A Summary of Computer Misuse Techniques. In:
12th National Computer Security Conference, Baltimore, MD. pp. 396–406 (Oct
1989)

30. Newhouse, W., Keith, S., Scribner, B., Witte, G.: National initiative for cybersecu-
rity education (nice) cybersecurity workforce framework. NIST Special Publication
800-181 (2017)

31. Newhouse, W., Keith, S., Scribner, B., Witte, G.: National Initiative for Cyber-
security Education (NICE), Cybersecurity Workforce Framework, NIST Special
Publication 800 -181. National Institute of Standards and Technology, US De-
partment of Homeland Security, National Initiative for Cybersecurity Careers and
Studies (NICCS) (2017)

32. Paulauskas, N., Garsva, E.: Computer System Attack Classiﬁcation. IEEE Au-

tomation and Robotics (2006)

33. Pel´anek, R.: Applications of the Elo rating system in adaptive educational systems.

Computers & Education 98, 169–179 (2016)

34. Priyadarshini, I.: Features and Architecture of The Modern Cyber Range: A Qual-

itative Analysis and Survey. Ph.D. thesis, University of Delaware (2018)

35. Puppet, Inc.: puppet (2020), https://www.puppet.com, (Accessed on August 28,

2020)

36. Rajam¨aki, J., Nevmerzhitskaya, J., Vir´ag, C.: Cybersecurity education and training
in hospitals: Proactive resilience educational framework (Prosilience EF). In: 2018
IEEE Global Engineering Education Conference (EDUCON). pp. 2042–2046. IEEE
(2018)

37. Red Hat, Inc.: Red Hat Ansible (2020), https://www.ansible.com, (Accessed on

August 28, 2020)

Cyber Taxi

19

38. SaltStack, Inc.: Saltstack (2020), https://www.saltstack.com, (Accessed on Au-

gust 28, 2020)

39. Simmons, C., Ellis, C., Shiva, S., Dasgupta, D., Wu, Q.: AVOIDIT: A Cyber Attack
Taxonomy. In: 9th Annual Symposium on Information Assurance (ASIA’14). pp.
2–12 (2014)

40. Souissi, S.: A novel response-oriented attack classiﬁcation. In: 2015 International
Conference on Protocol Engineering (ICPE) and International Conference on New
Technologies of Distributed Systems (NTDS). pp. 1–6 (2015)

41. Steinberger, J., Sperotto, A., Golling, M., Baier, H.: How to exchange security
events? Overview and evaluation of formats and protocols. In: Badonnel, R., Xiao,
J., Ata, S., Turck, F.D., Groza, V., dos Santos, C.R.P. (eds.) IFIP/IEEE Inter-
national Symposium on Integrated Network Management, IM 2015. pp. 261–269.
IEEE (2015)

42. Suba¸su, G., Ro¸su, L., B˘adoi, I.: Modeling and simulation architecture for training
in cyber defence education. In: 2017 9th International Conference on Electronics,
Computers and Artiﬁcial Intelligence (ECAI). pp. 1–4 (2017)

43. ˇSv´abensk´y, V., Vykopal, J., Cermak, M., Laˇstoviˇcka, M.: Enhancing Cybersecurity
Skills by Creating Serious Games. In: Proceedings of the 23rd Annual ACM Confer-
ence on Innovation and Technology in Computer Science Education. pp. 194–199.
ITiCSE 2018, Association for Computing Machinery, New York, NY, USA (2018)
44. Taylor, C., Arias, P., Klopchic, J., Matarazzo, C., Dube, E.: CTF: State-of-the-Art
and Building the Next Generation. In: 2017 USENIX Workshop on Advances in
Security Education (ASE 17). USENIX Association, Vancouver, BC (Aug 2017)

45. Urias, V.E., Van Leeuwen, B., Stout, W.M.S., Lin, H.W.: Dynamic cybersecurity
training environments for an evolving cyber workforce. In: 2017 IEEE International
Symposium on Technologies for Homeland Security (HST). pp. 1–6 (2017)

46. US Department of Defense: The Department of Defense Cyber Table Top
Guidebook (2018), https://www.dau.edu/cop/test/DAUSponsoredDocuments/
TheDoDCyberTableTopGuidebookv1.pdf, (Accessed on August 28, 2020)

47. Val´u˘sek, M.: Classiﬁcation of Network Attacks and Detection Methods. Tech. rep.,

Masaryk University, Czech Republic (2016)

48. Vykopal, J., O˘slej˘sek, R., Celeda, P., Vizv´ary, M., Tovar˘n´ak, D.: KYPO Cyber
Range: Design and Use Cases. In: Proceedings of the 12th International Conference
on Software Technologies - Volume 1: ICSOFT. pp. 310–321 (Jan 2017)

49. Vykopal, J., Vizv´ary, M., O˘slej˘sek, R., Celeda, P., Tovar˘n´ak, D.: Lessons Learned
From Complex Hands-on Defence Exercises in a Cyber Range. In: 2017 IEEE
Frontiers in Education Conference (FIE). pp. 1–8. IEEE (2017)
conference pre-
Is The New Purple. Blackhat
https://www.blackhat.com/docs/us-17/wednesday/
on August

sentation
us-17-Wright-Orange-Is-The-New-Purple-wp.pdf,
28, 2020)

50. Wright, A.C.: Orange

(Accessed

(2017),

51. Wu, Z., Ou, Y., Liu, Y.: A Taxonomy of Network and Computer Attacks Based on
Responses. In: 2011 International Conference of Information Technology, Computer
Engineering and Management Sciences. vol. 1, pp. 26–29 (2011)

52. Yamin, M.M., Katt, B., Gkioulos, V.: Cyber Ranges and Security Testbeds: Scenar-
ios, Functions, Tools and Architecture. Computers & Security 88, 101636 (2020)
53. Yurcik, W., Doss, D.: Diﬀerent Approaches in the Teaching of Information Systems
Security. In: Proceedings of the Information Systems Education Conference. pp.
32–33 (2001)

