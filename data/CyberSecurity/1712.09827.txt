A Syntactic Approach to Domain-Speciﬁc Automatic Question Generation

Guy Danon
Department of Software and
Information Systems Engineering
Ben-Gurion University of the Negev
Beer-Sheva 84105, Israel
guy.danon@gmail.com

Mark Last
Department of Software and
Information Systems Engineering
Ben-Gurion University of the Negev
Beer-Sheva 84105, Israel
mlast@bgu.ac.il

Abstract

1

Introduction

Factoid questions are questions that re-
quire short fact-based answers. Auto-
matic generation (AQG) of factoid ques-
tions from a given text can contribute
to educational activities, interactive ques-
tion answering systems, search engines,
and other applications. The goal of our
research is to generate factoid source-
question-answer triplets based on a spe-
ciﬁc domain.
We propose a four-
component pipeline, which obtains as in-
put a training corpus of domain-speciﬁc
documents, along with a set of declara-
tive sentences from the same domain, and
generates as output a set of factoid ques-
tions that refer to the source sentences
but are slightly different from them, so
that a question-answering system or a per-
son can be asked a question that requires
a deeper understanding and knowledge
than a simple word-matching. Contrary
to existing domain-speciﬁc AQG systems
that utilize the template-based approach to
question generation, we propose to trans-
form each source sentence into a set of
questions by applying a series of domain–
independent rules (a syntactic-based ap-
proach). Our pipeline was evaluated in
the domain of cyber security using a series
of experiments on each component of the
pipeline separately and on the end-to-end
system. The proposed approach generated
a higher percentage of acceptable ques-
tions than a prior state-of-the-art AQG sys-
tem.

Automatic Question Generation (AQG) is deﬁned
by Rus et al. (2008) as the task of automat-
ically generating questions from various inputs
such as raw text, database, or semantic represen-
tation. Question generation is an important el-
ement of learning and educational environments,
search engines, automated help systems, and other
applications. Manual writing of good questions,
though, is a challenging and time-consuming task.
The increasing availability of electronic informa-
tion, along with the growth of various question–
answering applications, has stimulated the re-
search in automatic question generation.

Studies on automatic question generation re-
gard the question generation task as a four–stage
process: 1) deﬁning the trigger to ask the ques-
tion (relevant mostly to dialogs and question-
answering systems), 2) text preprocessing and rel-
evant content selection, 3) question construction,
4) ranking the constructed questions.

The main step in the AQG process is the con-
struction of questions from the selected content.
Nearly all existing AQG systems generate ques-
tions from one sentence at a time. Existing ques-
tion generation approaches can be classiﬁed into
three categories:
syntax–based, template–based
and semantics–based (Yao and Zhang, 2010).
Heilman (2011) presents a syntax–based system,
which breaks the AQG process into several steps.
Simpliﬁed factual statements are ﬁrst extracted
from complex inputs, by (optionally) altering or
transforming lexical items, syntactic structure, and
semantics. Next,
the sentences are separately
transformed into questions by applying sequences
of simple,
linguistically–motivated transforma-
tions such as subject–auxiliary inversion and WH-
movement. Heilman employs some core NLP
tools in his system in order to analyze the linguis-

7
1
0
2
c
e
D
8
2

]
L
C
.
s
c
[

1
v
7
2
8
9
0
.
2
1
7
1
:
v
i
X
r
a

 
 
 
 
 
 
tic properties of input sentences.

The template-based approaches rely on the idea
that a question template can capture a class of con-
text speciﬁc questions having a common structure.
For example, Chen et al. (2009) developed tem-
plates such as: ”What would happen if < X >?”
for conditional text, and ”Why < auxiliary −
verb >< X >?” for linguistic modality, where
< X > is the place-holder mapped to seman-
tic roles annotated by a semantic role labeler. In
a more recent paper, Serban et al. (2016) gener-
ate a 30M factoid question–answer corpus, where
subject–relationship–object triples are transduced
into questions about the subjects with the objects
being the correct answer. Such question templates
can only be used for speciﬁc entity relationships.
These approaches are most suitable for special–
purpose applications within a closed domain. For
example, Lee et al. (2016a) use a small portion of
the above 30M question–answer pairs for training
an academic instance of the IBM Watson system
for the domain of location–related questions.

Each of the different AQG approaches has its
own advantages. While the template-based ap-
proaches usually generate questions that are gram-
matically correct,
the syntax-based approaches
provide better coverage of the text, and the
semantic-based approaches use some background
resources, such as WordNet and Wikipedia, in or-
der to provide more challenging questions. Some
of the recent studies try to combine the different
approaches. Mazidi and Nielsen (2015) present
a template-based question generation system that
is built on multiple views of text: syntactic struc-
ture retrieved from dependency parsing, coupled
with information from semantic role labels and
discourse cues.

Using various AQG approaches, an exhaustive
list of questions can be generated from each text.
High percentage of these questions might be un-
acceptable, due to incorrect syntactic structures,
non-relevance to the main topics in the text or
other various reasons.
It is time–consuming to
check the generated questions manually. There-
fore in the AQG task, high precision rates are
much more important than high recall (Afzal and
Mitkov, 2014) and it can be worthwhile to apply
a ranking method to the list of candidate ques-
tions. A statistical model of question acceptability,
which is based on least squares linear regression,
is proposed by Heilman (2011). He models the

question quality as a linear function from a vector
of feature values to a continuous variable ranging
from 1 to 5, representing linguistic factors such as
grammaticality, vagueness, use of an appropriate
WH word, etc. In total, his question ranker uses
179 features that were identiﬁed by an analysis of
questions generated from development data.

Most AQG systems build wh–questions by ap-
plying some transformation rules to the original
sentence. One drawback of these systems is that
generated questions may be too close to the source
text, as generation usually relies on transforming
the syntactic structure from declarative to inter-
rogative, without changing the words used. Such
questions can be easily answered using the words
in the source text even without a good compre-
hension of the topic. To generate more chal-
lenging questions, the source text should be para-
phrased in the process of question generation. In
the context of AQG, paraphrasing was used in
only a few studies. Bernhard et al. (2012) de-
scribe an AQG system for French, which reformu-
lates questions based on variations in the question
words, inducing answers with different granulari-
ties, and nominalizations of action verbs. Tseng
et al. (2014) generate multiple-choice questions
for reading comprehension tests. Their Paraphrase
Generation System generates a ranked list of para-
phrases given an input sentence and a source arti-
cle, by combining multiple paraphrase resources.

Our research is aimed at generating a diverse
set of factoid source-question-answer triplets rep-
resenting a speciﬁc domain (e.g., health care or
cyber security). These triplets can be used for
both training and testing an advanced QA sys-
tem such as IBM Watson. Since such a system
is expected to answer natural language questions,
its training set should not be limited to questions
having a speciﬁc structure, which are usually pro-
duced by template–based AQG systems. Preci-
sion of the automatically generated questions is
less important than the recall of the domain knowl-
edge, since manual ﬁltering of these questions by
human experts is much less labor–intensive than
composing new questions from scratch. Using
their background knowledge, humans may ask a
question about the same fact in many different
forms. To provide a variety of human–like ques-
tions, our AQG system should be able to para-
phrase the source texts before transducing them
into questions. Too general questions, like ”Who

was born in Hawaii?”, should be automatically re-
placed with more speciﬁc questions (e.g., ”Which
US President was born in Hawaii?”). We extend a
state–of–the–art syntax–based AQG system (Heil-
man, 2011) to deal with the above challenges and
evaluate our approach on questions from the cyber
security domain.

2 Proposed AQG Methodology

Our pipeline for automatic question generation
consists of four components, which together pro-
vide solutions to some of the gaps that we have
found between the human generated questions and
the questions which were generated using Heil-
man’s (2011) AQG system.
First we train a
Word2Vec model (Mikolov et al., 2013) based on
a domain speciﬁc corpus, after lemmatization and
part-of-speech tagging. The input to the question
generation process is a set of sentences related to
the domain on which the Word2Vec model was
trained. In the paraphrasing component, we iden-
tify all verbs in the sentence and try to substitute
them with nearly synonymous verbs, while keep-
ing the meaning and the syntactic correctness of
the sentence. The paraphrased sentence is the in-
put to Heilman’s AQG system (Heilman, 2011),
which generates wh–questions using transforma-
tion rules. The purpose of the fourth component in
the pipeline is to identify the hypernym of each an-
swer and add it to a ”what type of” question. The
generated questions using Heilman’s system may
be ambiguous or meaningless, because it replaces
the whole answer-phrase with the wh-word, which
yields, for example, ”Who was born in Switzer-
land?”. We try to identify the answer’s hypernym
using two strategies: from background knowledge
resources or from the sentence itself. If a hyper-
nym is found, it is used to generate a more speciﬁc
question.

The pipeline is shown in 1. The green com-
ponents are our extensions to the baseline state-
of-art AQG system (?). Using these extensions
we would like to generate better factoid questions
from a domain speciﬁc corpus. A detailed descrip-
tion of each extension is provided below.

The embedded word vectors are trained over large
collections of text using variants of neural net-
works, in order to capture attributional similarities
between vocabulary items: words that appear in
similar contexts will be close to each other in the
projected space (Levy and Goldberg, 2014). The
extraordinary abilities of this representation allow
to capture valuable semantic information, which is
difﬁcult to ﬁnd in other sources. For example: in
WordNet the synonyms of install are instal, put in,
set up, and establish, but using a domain-speciﬁc
embedding model some other synonymous terms
can be found (such as run or execute, which are
relevant to the computing domain). The word em-
bedding models can contribute to the AQG task in
two ways: (1) There is no need to spend the ex-
pensive time of domain experts on representing all
domain–related entities and their relations, and (2)
we can use these models to ﬁnd similarities and
relations between words in order to create some
paraphrases by word substitutions.

We trained the Word2Vec model (Mikolov
et al., 2013) over a corpus of more than 1.1 million
cyber-related documents. Before training, the cor-
pus was lemmatized and tagged by basic POS tags
to improve the quality of the paraphrasing process.
Thus, we would like to generate two different vec-
tors for the word run: run nn (run as NOUN) and
run vb (run as VERB). However, different forms
of the verb run (run , runs , running , ran) should
be represented by the same vector (run vb). The
Word2Vec training parameters were adjusted us-
ing a set of domain-speciﬁc examples such as:

• The similarity between virus nn and mal-

ware nn should be relatively high.

• The most similar words of run vb should in-

clude verbs such as execute vb.

• The most similar words of virus nn + in-

stall vb should include infect vb.

• One of the answers to Melissa nn is to
virus nn as DDOS nn is to ? should be at-
tack nn.

2.2 Verb Paraphrasing

2.1 Training a Domain–Speciﬁc Word

Embedding Model

Word embedding models embed an entire vocabu-
lary into a relatively low-dimensional linear space,
whose dimensions are latent continuous features.

To generate a question, which is slightly differ-
ent from the source text, we substitute the verbs in
the original sentence by other verbs having a sim-
ilar meaning and then submit the paraphrased sen-
tence to a question generation system. We identify

Figure 1: AQG pipeline

the verbs in each sentence by the following POS
tags from Stanford Parser: VB, VBD, VBG, VBN,
VBP, and VBZ, except words with the lemma be.
Adverbial particles are attached to the verbs ac-
cording to the dependency tree of the sentence in
order to consider them as a part of a compound
verb. For example: shut down and boost up are
compound verbs, and not just shut or boost. When
a word in a sentence is tagged as RP (particle), we
iterate over all its dependencies and attach it to the
verb with which it has the prt (phrasal verb parti-
cle) relation.

Inspired by the studies like (Bolshakov and Gel-
bukh, 2004) that used WordNet for paraphrasing,
we search the WordNet for the synonyms of all
verbs in each source sentence. Since the synonyms
in WordNet are grouped by senses, we retrieve all
synonyms from all verb synsets of each word iden-
tiﬁed as a verb. The main drawback of WordNet is
that it is a general purpose lexicon / ontology and
therefore it may miss some domain-speciﬁc ac-
tions. On the other hand, since WordNet was gen-
erated by human experts, the retrieved synonyms
are certainly right (high precision).

Multiple potential synonyms may be retrieved
for each verb. For example, the verb run has 41
different senses and most of them contain several
synonyms. Following Lee et al. (2016b), we use a
domain-speciﬁc Word2Vec model to disambiguate
the verb senses. The Word2Vec objective function
causes words that occur in similar contexts to have

similar embeddings but in different domains the
context of the same word will probably be differ-
ent. For each potential synonym, we compute its
similarity to the original verb from the source sen-
tence, which is the cosine similarity between the
vectors of the two words. The synonym which has
the highest similarity with the source verb is se-
lected to substitute it in the sentence, unless this
similarity is below a user-deﬁned min similarity
threshold.

The words in the WordNet and in a domain-
speciﬁc Word2Vec model appear in their base
form (lemma). To substitute the original verb with
the selected synonym, the synonym has ﬁrst to be
transformed to the form of the original verb in
the sentence. We have deﬁned a set of domain–
independent rules in SimpleNLG (Gatt and Re-
iter, 2009) for this purpose. The rules refer to the
following verb parameters: (1) tense, (2) number
(singular / plural), (3) progressive (yes / no) and
(4) passive (yes / no).

2.3 Question Generation

Given the paraphrased sentence, we use a state-
of-art system to generate various questions based
on the factual statements in the sentence (Heil-
man, 2011). As mentioned in our ﬁrst princi-
ple, we use the syntactic-based approach to ques-
tion generation in order to achieve good coverage
(high recall) of the text, albeit at the expense of
precision. The AQG system presented by Heil-

man (2011) has three stages: NLP transformations
on the source sentences, question transducer, and
question ranker. We have integrated the ﬁrst two
stages in our question generation pipeline. The
ranking stage was not included since we assume
that the automatically generated questions can be
easily ﬁltered by a human expert before being re-
leased for any practical usage.

Heilman’s system uses a Java reimplementation
of the supersense tagger described by Ciaramita
and Altun (2006) as part of mapping potential an-
swer phrases to WH words. It uses features of both
the current word (e.g., part of speech, capitaliza-
tion features, the word itself) and the surrounding
words to make predictions. They use a set of con-
ditions to determine the WH-word, but claim that
”It is worth noting that this step encodes a con-
siderable amount of knowledge and yet still does
not cover all relevant linguistic phenomena.” Fol-
lowing their error analysis and some of our exper-
iments, we have decided to revise the WH-words
of Heilman’s questions based on the NE type of
the answer phrase as determined by the Stanford
parser (i.e. ”who” for PERSON, ”where” for LO-
CATION, etc.).

2.4 Hypernym Identiﬁcation

Good factoid questions must be speciﬁc, ﬂuent
and clear. The answer phrase determines whether
the question type will be a when / who / when, etc.
Syntactic-based AQG approaches usually map the
answer phrase to the relevant WH-question word
using techniques involving a NER tool. For ex-
ample, consider taking the following sentence and
generating a question for which Hillary Clinton is
the answer phrase:
Sentence: Hillary Clinton was the United States
Secretary of State from 2009 to 2013.
After recognizing that the answer is a person, the
following question is generated:
Question: Who was the United States Secretary of
State from 2009 to 2013?
The generated question above is considered a good
factoid question, since it provides a single and
unambiguous answer. However, exploring some
other question-answer pairs automatically gener-
ated from texts using the same technique reveals
that a lot of them might be ambiguous and even
ridiculous. For example, see the following source
sentence and the question generated for the answer
phrase Tennis star Roger Federer:

Sentence: Tennis star Roger Federer was born in
Switzerland.
Question: Who was born in Switzerland?
Obviously, this question is not speciﬁc enough to
be used in an exam or some other learning task.
We propose to narrow the scope of factoid ques-
tions using two methods:
(1) adding hypernym
from knowledge resources, and (2) adding content
from the source text.

2.4.1 Adding hypernym from knowledge

resources

Given a pair of two words < X, Y >, X is consid-
ered a hypernym of a word Y if native speakers ac-
cept the sentence Y is a (kind of) X. This semantic
relation organizes the meanings of words and con-
cepts into a hierarchical structure. It is also known
as a generic/speciﬁc, a taxonomic, is-a or instance-
of relation. Adding the hypernym of an answer to
a question can produce a type-of (or what-type)
question, which should be considered a better fac-
toid question.

Consider again the sentence about Roger Fed-
erer. Generating a question by Heilmans algorithm
for the answer Switzerland will lead to the follow-
ing questions:
Question 1: Where was tennis star Roger Federer
born? (After recognizing the answer as a location).
Question 2: What was tennis star Roger Federer
born in?
As explained earlier, these are ambiguous ques-
tions, since there are some other correct answers
besides Switzerland, such as the name of the hos-
pital where he was born or even other places (bath,
airplane, home, hospital). Adding the hypernym
of Switzerland (country) to the question will nar-
row the scope and make it a better question:
Question 3: What country was tennis star Roger
Federer born in?
For each generated question, we search the Word-
Net for the related hypernym of the answer phrase.
If the answer phrase exists in the WordNet, its di-
rect hypernym in the hierarchy is returned like in
the following examples:
unix → operating system
malware → sof tware
red → chromatic color
BillGates → computer scientist

2.4.2 Adding content from the source

sentence

The source sentence and the answer phrase to-
gether with its POS (part-of-speech) tags and NE
(named-entity) tags in particular, can contain some
valuable information, which can be utilized to en-
hance questions generated by Heilman.
If the
Heilman generated answer contains a noun phrase,
we try to include a generalized noun phrase in the
question using some domain–independent rules.

For answer phrases that contain NE, the tag of
the NE can be added to the generated question.
For example, the sentence Bill Gates founded Mi-
crosoft in 1975 contains the NE Microsoft, that
does not exist in WordNet (version 3.1 database
ﬁles), but it is recognized by Stanford Parser as
an ORGANIZATION. Adding this tag name to
the question What did Bill Gates found in 1975?
will create the question What organization did Bill
Gates found in 1975?, which is more speciﬁc. The
tag name of the NE is added to the question only
if the NE part-of-speech is noun to avoid generat-
ing, for example, questions like ”What ordinal?”,
when the word ﬁrst appears in the source.

If the answer phrase contains NP prior to the
NE, this NP can be added to the question. For ex-
ample, Tennis star is prior to the NE Roger Fed-
erer in the sentence Tennis star Roger Federer was
born in Switzerland. When generating question
with the answer phrase Tennis star Roger Federer,
the following question can be generated instead of
”Who was born in Switzerland?”: ”What tennis
star was born in Switzerland?”

For answer phrases that are not found in the lex-
ical database and do not contain named-entities,
the head noun of the phrase can add a valuable
contribution to the question. The head noun is
the major noun which reﬂects the focus of a noun
phrase and it often describes the function or the
property of the noun phrase. It has been presented
to play an important role in classifying what-
type questions (Li et al., 2008). We retrieve the
head nouns of the answer phrases using Stanford
CoreNLP library (Manning et al., 2014). Here is
an example of improving a Heilman’s question us-
ing a head noun:
Source sentence: Polymorphic virus infects ﬁles
with an encrypted copy of itself.
Question generated by Heilman: What infects
ﬁles with an encrypted copy of itself?
Answer phrase: Polymorphic virus.

Head Noun: Virus.
New question (without paraphrasing): What
virus infects ﬁles with an encrypted copy of itself?
The ﬁnal step in adding information to the ques-
tion is changing the WH-question word. Given a
question, if a hypernym has been found using one
of the above methods / rules, the WH-word is re-
placed by What followed by the hypernym (e.g.,
”What tennis star...” instead of ”Who...”).

3 Experimental Evaluation

In this section, we evaluate the proposed syntactic
approach to domain-speciﬁc automatic question
generation on the cyber security domain. Since
our pipeline consists of four different components,
we perform two types of experiments: evalua-
tions of the individual components of the pro-
posed system and evaluation of the end-to-end
AQG system by measuring the quality of ques-
tions at the sentence-level. The training corpus
of the Word2Vec model consists of three differ-
ent sources: 622 books on cyber security, 2800
academic papers on the cyber domain, and 250
cyber reports collected from the Internet between
2008-2014. The total number of sentences in the
whole corpus is more than 4.6 million (after sen-
tence splitting using Stanford Parser). Our evalu-
ation corpus contains 69 sentences extracted from
about 30 different articles on cyber security and
composed of 1, 130 words. Most sentences were
used to generate more than one question.

3.1 Word2Vec Model Evaluation

For training the model, we used the implementa-
tion of Word2Vec in Python, as found in Gensim
by Radim Rehurek https://github.com/
RaRe-Technologies/gensim.
This tool
provides an efﬁcient implementation of the con-
tinuous bag-of-words and skip-gram architectures
for computing vector representations of words.
Lemmatization and POS tagging were done as pre-
processing to the training process, using Stanford
Parser.

While we used the default values for most of the
training parameters (including the architecture of
CBOW), two parameters were tuned: (1) window
size, which is the maximum distance between the
current and predicted word within a sentence, and
(2) vector size, which is the dimensionality of the
feature vectors. Based on (Le et al., 2014), we
have experimented with different combinations of

the window size parameter (in the scope of 5 to
12) and the vector size (between 50 and 200).

To evaluate the quality of a model, we built a set
of 20 examples from the cyber domain and deﬁned
the expected answer of each example. For a given
model, each example is scored between 0 and 1, so
the maximal model’s total score is 20. The score
of an example is given according to the following
semantic NLP tasks:

• Does not match (8 examples)

In these ex-
amples, the model has to choose which word
from the input list does not match the others.
If the model provides the correct answer the
score is 1, otherwise it is 0.

• Similarity (7 examples) The similarity score
between two words is the cosine similarity
between their two vectors. If the expectation
is to have high similarity (e.g. between worm
the answer is scored by the re-
and virus)
turned similarity value. Otherwise (if we as-
sume that there is no high similarity between
the words, e.g. table and install) the answer
is scored as 1 minus the returned value.

• Most similar (5 examples)

In these exam-
ples, the task is to ﬁnd the top-N most sim-
ilar words, while positive words contribute
positively towards the similarity and negative
words negatively. This method computes co-
sine similarity between a simple mean of the
projection weight vectors of the given words
and the vectors for each word in the model.
In our evaluation, we use the default value of
N = 10. The model’s score for the ”most sim-
ilar” example is the calculated similarity of
the expected answer in the returned vector. If
the expected answer is not one of the top 10
answers, the given score is 0.

The model, which achieved the highest score ac-
cording to the above method, was selected for
use in the paraphrasing component. The selected
model was trained with the values of 8 and 100
for the parameters window size and vector size,
respectively.

3.2 Verb Paraphrasing

As explained in the methodology section above,
we perform a simple paraphrasing of the source
sentence by substituting the sentence verbs with
one of their synonyms. We use the WordNet

to retrieve the list of the verb synonyms and the
Word2Vec model to disambiguate the verb senses
and select the most appropriate verb for each sub-
stitution. By default, the senses in the WordNet are
sorted in the decreasing order of their estimated
frequency and hence we consider as a baseline the
ﬁrst synonym (sense) found in the WordNet.

We applied the baseline approach along with the
Word2Vec–based sense disambiguation method to
195 verbs in different tenses that were found in the
69 sentences of the evaluation corpus. The com-
parative evaluation was performed by two raters
using two discretely-valued measures: the selec-
tion of the synonym for each verb in the sentence
correct synonym, wrong synonym, no substitute
has been found and the overall quality of the para-
phrased sentence acceptable, unacceptable. Us-
ing domain–speciﬁc word embeddings vs.
the
baseline, the number of wrong synonyms was de-
creased signiﬁcantly (from 91 to 35), as a result of
selecting 23 additional correct synonyms and leav-
ing the original verb in 33 other cases, where the
similarity value was below the predeﬁned thresh-
old of 0.5. Out of the set of 69 sentences, only 27
sentences paraphrased by the baseline were found
acceptable, compared to 56 acceptable sentences
produced by domain–speciﬁc word embeddings.

3.3 Hypernym Identiﬁcation

The question transducer stage in the AQG sys-
tem (Heilman, 2011) generates multiple questions
from each input sentence, according to different
answer-phrases that can be found in the sentence.
As mentioned above, we try to identify the hyper-
nym of each QA pair in order to build more spe-
ciﬁc questions. All paraphrased sentences that we
have built in the previous section were provided
as an input to Heilman’s system, which generated
217 QA pairs. We have manually analyzed these
pairs and determined which answers can be gen-
eralized (a hypernym can be found) as a bench-
mark for evaluating the automatically generated
hypernyms. According to this evaluation, the pre-
cision of the hypernym identiﬁcation component
was 0.76 and the recall was 0.97.

3.4 Evaluation of the AQG Pipeline

We compare our questions (after verb paraphras-
ing and the addition of the identiﬁed hypernym
of the answer) to the questions which were gen-
erated by the baseline system (Heilman, 2011).
Two cyber security experts were asked to deter-

Figure 2: Summary of Experimental Results

mine together which of the generated questions
can be accepted as good questions in terms of ﬂu-
ency, clarity, and semantic correctness. In case of
disagreement, there was a discussion between the
two raters to reach a consensus on each generated
question. Based on this assessment, we have an-
alyzed three different cases: (1) questions which
were generated by the baseline and were not mod-
iﬁed by our system, (2) questions which were im-
proved by our pipeline, and (3) questions which
were corrupted by our pipeline.

The classiﬁcation of the generated questions
is summarized in Fig. 2. The averages and the
standard deviations (avg, st dev) in Fig. 2 refer
to the rankings generated by Heilman’s system
for the corresponding questions.
It can be seen
from Fig. 2 that good (acceptable) questions have
slightly higher rankings than wrong (unaccept-
able) questions. Out of 217 questions in total that
were generated by Heilman’s system, 148 ques-
tions were modiﬁed by our pipeline with an ac-
ceptance rate of 42% (62 questions were accepted
by the two rankers). These questions, listed in the
Supplementary Material, can be considered as the
main contribution of our system, since they were
enhanced by paraphrasing and by adding automat-
ically detected hypernyms. The other 86 questions
were not accepted due to the following reasons:

• Not interesting (trivial) QA - 26

• Wrong QA by Heilman - 31

• Out of context - 18

• Wrong paraphrasing - 15

• Wrong hypernym - 33

Considering the fact that only three out of 86 re-
jected questions from our system (3.5%) and only
six out of 69 questions that were not modiﬁed by
our system (8.7%) were actually good questions, it
can be concluded that we can improve the baseline
QAs by generating good factoid questions that are
different from the source text.

4 Conclusions and Future Research

In this paper, a novel syntactic-based approach
for automatic question generation in a speciﬁc do-
main has been presented. The proposed pipeline
is based on a combination of deep linguistic anal-
ysis (part of speech, named entity recognition) and
knowledge resources (WordNet, Word2Vec model
trained on a domain corpus) for detecting and
matching entities and relations. After generating
a simple paraphrased sentence by substituting the
verbs of the source sentence, we use a state-of-the-
art AQG system (Heilman, 2011) and add hyper-
nyms to generate speciﬁc questions (usually in the
form of What type of ).

In a series of experiments that we have per-
formed, it was shown that (1) the proposed method
of verb paraphrasing using Word2Vec and Word-
Net outperforms the baseline method of (Heilman,

2011); (2) the recall of the hypernym identiﬁcation
component is very high and its precision is 76%,
and (3) the proposed pipeline increases the number
of acceptable QAs in the task of generating good
factoid questions that are different from the source
text. Further research can improve each one of the
existing pipeline components and extend it with
additional components, such as content selection
and question ranking.

Acknowledgments

This research was partially supported by IBM Cy-
ber Security Center of Excellence (CCoE), Beer
Sheva, Israel.

References

Naveed Afzal and Ruslan Mitkov. 2014.

Auto-
matic generation of multiple choice questions using
dependency-based semantic relations. Soft Comput-
ing 18(7):1269–1281.

Delphine Bernhard, Louis De Viron, V´eronique
Moriceau, Xavier Tannier, et al. 2012. Ques-
tion generation for french: Collating parsers and
paraphrasing questions. Dialogue and Discourse
3(2):43–74.

Igor A Bolshakov and Alexander Gelbukh. 2004. Syn-
onymous paraphrasing using wordnet and internet.
In International Conference on Application of Nat-
ural Language to Information Systems. Springer,
pages 312–323.

W Chen, G. Aist, and J. Mostow. 2009. Generating
questions automatically from informational text. In
Proceedings of the 2nd Workshop on Question Gen-
eration (AIED 2009). pages 17–24.

Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 594–602.

Albert Gatt and Ehud Reiter. 2009. Simplenlg: A re-
In Pro-
alisation engine for practical applications.
ceedings of the 12th European Workshop on Natu-
ral Language Generation. Association for Compu-
tational Linguistics, pages 90–93.

Michael Heilman. 2011. Automatic factual question
generation from text. Ph.D. thesis, Carnegie Mellon
University.

Jangho Lee, Gyuwan Kim, Jaeyoon Yoo, Changwoo
Jung, Minseok Kim, and Sungroh Yoon. 2016a.
Training IBM watson using automatically gener-
ated question-answer pairs. CoRR abs/1611.03932.
http://arxiv.org/abs/1611.03932.

Yang-Yin Lee, Hao Ke, Hen-Hsen Huang, and Hsin-
Hsi Chen. 2016b. Combining word embedding and
lexical database for semantic relatedness measure-
ment. In Proceedings of the 25th International Con-
ference Companion on World Wide Web. Interna-
tional World Wide Web Conferences Steering Com-
mittee, pages 73–74.

Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems.
pages 2177–2185.

Fangtao Li, Xian Zhang, Jinhui Yuan, and Xiaoyan
Zhu. 2008. Classifying what-type questions by head
In Proceedings of the 22nd Inter-
noun tagging.
national Conference on Computational Linguistics-
Volume 1. Association for Computational Linguis-
tics, pages 481–488.

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In ACL (System Demon-
strations). pages 55–60.

Karen Mazidi and Rodney D Nielsen. 2015. Lever-
aging multiple views of text for automatic question
In International Conference on Artiﬁ-
generation.
cial Intelligence in Education. Springer, pages 257–
266.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efﬁcient estimation of word rep-
resentations in vector space. CoRR abs/1301.3781.
http://arxiv.org/abs/1301.3781.

Vasile Rus, Zhiqiang Cai, and Art Graesser. 2008.
Question generation: Example of a multi-year eval-
uation campaign. Proc WS on the QGSTEC .

Iulian Vlad Serban, Alberto Garc´ıa-Dur´an, Caglar
Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron
Courville, and Yoshua Bengio. 2016. Generat-
ing factoid questions with recurrent neural net-
works: The 30m factoid question-answer corpus.
In Proceedings of
the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 588–598.
http://www.aclweb.org/anthology/P16-1056.

Nguyen-Thinh Le, Tomoko Kojiri, and Niels Pinkwart.
2014. Automatic question generation for educa-
In Advanced
tional applications–the state of art.
Computational Methods for Knowledge Engineer-
ing, Springer, pages 325–338.

Ya-Min Tseng, Yi-Ting Huang, Meng Chang Chen,
and Yeali S Sun. 2014. Generating comprehen-
In Technologies
sion questions using paraphrase.
and Applications of Artiﬁcial Intelligence, Springer,
pages 310–321.

Xuchen Yao and Yi Zhang. 2010. Question generation
with minimal recursion semantics. In Proceedings
of QG2010: The Third Workshop on Question Gen-
eration. Citeseer, pages 68–75.

