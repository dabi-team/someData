Attacker Behaviour Proﬁling using Stochastic Ensemble of Hidden Markov
Models

Soham Deshmukh, Rahul Rade, Faruk Kazi

Centre of Excellence in Complex and Nonlinear Dynamical Systems, VJTI
{ssdeshmukh b15, rsrade b15, fskazi}@el.vjti.ac.in

1
2
0
2

n
u
J

6

]

G
L
.
s
c
[

2
v
4
2
8
1
1
.
5
0
9
1
:
v
i
X
r
a

to noise,

to obtain better

Abstract
Cyber threat intelligence is one of the emerging areas of focus
in information security. Much of the recent work has focused
on rule-based methods and detection of network attacks using
Intrusion Detection algorithms.
In this paper, we propose
a framework for inspecting and modelling the behavioural
aspect of an attacker
insight predictive
power on his future actions. For modelling, we propose a
novel semi-supervised algorithm called Fusion Hidden Markov
Model (FHMM) which is more robust
requires
comparatively less training time, and utilizes the beneﬁts of
ensemble learning to better model temporal relationships in
data. This paper evaluates the performance of FHMM and
compares it with both traditional algorithms like Markov Chain,
Hidden Markov Model (HMM) and recently developed Deep
Recurrent Neural Network (Deep RNN) architectures. We
conduct the experiments on a dataset consisting of real data
FHMM provides
attacks on a Cowrie honeypot system.
accuracy comparable to deep RNN architectures at signiﬁcant
lower training time. Given these experimental results, we
recommend using FHMM for modelling discrete temporal data
for signiﬁcantly faster training and better performance than
existing methods.
Keywords:
intelligence, hidden markov model, honeypot,
modelling

threat
statistical

security, machine

learning,

cyber

1. Introduction

The existing cyber security tools focus on reactive methods
and algorithms as a major part of their cyber security arsenal.
In current world where organizations are highly digital, a
single vulnerability can lead to penetrative attack negatively
affecting business on a large scale. Moreover, attackers now
are leveraging automation and cloud to scale their attacks faster
and inﬁltrate systems in record break time. Therefore, it is
advisable for organization to stay one step ahead of attackers
and be able quickly foresee where and when they will strike.
Knowing the potential strike points or actions of attacker, the
organization can take necessary steps for mitigating cyber risks
to organization’s business.

The attacker may use multiple access point to deploy his
attack. Some attackers use persistent attack strategy consisting
of a sequence of attack behaviours continuously until
the
intended target system is compromised [1]. Some attackers tend
to stay in the system for long time do trivial task to bypass
IDS and other things then later perform malicious act. These
type of attacks are difﬁcult to track and detect. Moreover,
insider threats have become more prevalent and the breach
level index shows that almost 40% of the breaches are due to
poor employee awareness of cyber security [2]. Although raw
data in the form of logs is abundantly available, it is difﬁcult

and time-consuming to extract meaningful information based
on which proactive measures can be employed. Thus, it has
become indispensable to develop a solution which provides
threat intelligence capabilities to combat these attacks by being
both proactive and responsive.

In order to identify and mitigate security breaches, three
major types of network analysis are done: signature based,
anomaly based, and hybrid. Signature based techniques try to
detect attacks based on their signature [3]. They have less false
positive cases but are useless against new (zero-day) attacks or
attack not present in their database. Anomaly based techniques
[4] model normal behaviour and detect deviations from the
normal behaviour. They have the capacity to detect zero-day
attacks. Additionally, they can be used to generate dataset for
signature based methods. However, the user or employee can
deviate from standard pattern of operation and this leads to high
amount of false alarms generated by anomaly based approaches.
Hybrid methods combine both signature and anomaly detection
to increase detection [5] of threats and reduce false positive
alarms. Most of the Machine learning and Deep learning based
methods are hybrid methods. However, all of these methods
are supervised learning methods where the algorithms need to
be provided with labelled data in the form of database, or time-
series to accurately make predictions.

There is a need of unsupervised or semi-supervised
algorithm which can work on raw data dumps to provide
intimation of a potential attack or threat beforehand. For the
algorithm to be effective and employable for predicting threats
at organization level -

• Modelling capacity: The algorithm must have sufﬁcient
modelling capacity for detecting nonlinear patterns in
sequential data.

• Time constraint: The algorithm must be scalable and
parallelizable to train on large data the organization
generates. It should provide predictions with low latency
and have low training time as the attackers can utilize the
downtime of algorithm to its advantage.

• Data Imbalance constraint: The algorithm should be
able to handle imbalanced data distribution where threat
or breaches are needle in a hack stack of ‘normal’
behaviour user or employee behaviour pattern.

• False Positives: The algorithm should be able to
incorporate uncertainty and possibly inform uncertain
predictions rather than generating false positives.

With view to satisfy these constraints, we propose a new
algorithm called Fusion Hidden Markov Model which exploits
the beneﬁts of ensemble learning [6] to produce better results.
We train a set of diverse HMMs on K different low-correlated
partitions of data and amalgamate the predictions of these
models using a nonlinear weight function. The nonlinear

 
 
 
 
 
 
function uses the posterior distribution over these HMMs to
generate a single output. One advantage of ﬁtting a set of
HMMs is that each model learns temporal features unique to
a group of similar attack patterns, thus making FHMM less
susceptible to noise. We employ this approach to effectively
model attack patterns on ﬁle system based honeypots. The
predictive power of this model can be used to comprehend the
mindset of the attacker beforehand and would help in preventing
systems from being compromised.

The remainder of the paper is structured as follows. Recent
work in the ﬁeld of cyber security has been elaborated in
Section 2. Section 3 provides a detailed problem formulation
along with an overview of the proposed system architecture. In
Section 4, we explain the proposed FHMM algorithm. Section 5
includes experiments performed. We analyze the results of
these experiments and provide a comprehensive comparison
of FHMM with other commonly used sequence models for
modelling attacker behaviour. Lastly, Section 6 summarizes and
concludes the paper.

2. Related Work

Threat intelligence or predictive threat intelligence is a newly
coined term which focuses on predictive methods for cyber
[7] suggested the use of big data analytics for
security.
analyzing and handling the huge amount of data trafﬁc. One of
the most prominently used machine learning based approach for
insider threat or breach detection is user behaviour proﬁling [8].
In this approach, the sequential actions of employee or student
are modelled to form a proﬁle for that speciﬁc user. The most
common patterns of actions are termed as normal behaviour
and any deviation from this predeﬁned path is considered
as a deviation.
This anomalous activity is then marked
for further investigation or potential attack. For modelling
‘normal’ behaviour of employees’ variety of strategies are
used. For detecting long-term anomalies in cloud data, [9]
proposed using Extreme Studentized Deviate test (ESD) and
employing time series decomposition and robust statistics
for detecting anomalies. While other methods in literature
rely on combining user action data with behavioural and
personality characteristics, [10, 11] propose the use of bagging
and boosting algorithms for intrusion detection [12]. Apart
from using collected or logged data on attacker actions, past
research literature has shown that behavioural and personality
characteristics can provide great insight in proﬁling the attack.
This particular example is shown by [13], which uses structural
anomaly detection combined with psychological proﬁling, in
order to reduce false positives compared to traditional anomaly
detection. Work by [14] explores how honeypot technology
can be used to detect, identify and gather information on these
speciﬁc threats. Ultimately the task is to ﬁnd out if there is an
activity being planned, and if so, ﬁnd what stage the planning
is in. [15] showed how Bernoulli ﬁlter can be used to detect
the presence of HMM in structured action data along with
minimum complexity that an HMM would need to involve in
order that it be detectable with reasonable ﬁdelity.

Recently, a lot of work has focused on rigorous analysis
of data collected by honeypots using machine learning and
statistical techniques. Despite potential limitations on the
availability of data in the majority of prior research, intrinsic
patterns in cyber attacks have been identiﬁed. For example,
[16] demonstrated the presence of
long-range temporal
dependencies in attack patterns captured by low-interaction
honeypots. Owning to the presence of long-range dependencies

in the rich information captured by honeypots, various
frameworks have been proposed which exploit these statistical
properties for modelling cyber attacks [17]. Other frameworks
proposed involve [18] which deals with graph-based clustering
of time-series attack patterns thereby identifying the activities
of worms and botnets present in the honeypot trafﬁc. A
technique proposed in [19] employs a similarity metric called
squared prediction error (SPE) for computing distance between
observation projected in residual space and the k-dimensional
hyperspace determined by principal component analysis (PCA).
This metric was used to identify new attack patterns in the
honeypot logs. On the other hand, the intent behind cyber
attacks also provide a fair indication of the target of the attacker
and several approaches for intent prediction using HMMs [20]
have been developed. To detect intrusions, [21] employed
two EWMA techniques to detect anomalous changes in event
intensity speciﬁcally for correlated and uncorrelated data.
Behaviour-rule based intrusion detection methods where also
investigated in domain of cyber physical systems particularly
in Smart Grid Applications by [22] which demonstrated that
detecting attackers based on behaviour features led to low false
positives. On the other hand, [23] used stochastic modelling
framework particularly using ﬁnite state Hidden Markov Model
to solve joint state and attack estimation problem. [24] found
time feature to be extremely important in modelling attacker
behaviour and the attack process can be split into three phases
namely the learning phase, standard phase and innovative
attack phase. [25] suggested the use of game theory approach
by formulating a Bayesian game to understand the attacker-
defender interactions in honeypot-enabled networks.

The use of Hidden Markov Models for modelling normal
behaviour (against attacker actions) was proposed by [26]
for detection insider threat. There is signiﬁcant work been
done in improving HMM and its learning algorithm. The
ability of HMM has to model sequence is dependent on
the structure of HMM. Though the integration over all
possible model structures are not possible, structures suited
to speciﬁc domain like proﬁle HMM [27] are developed.
In the context of biological sequence analysis, researchers
have used genetic algorithm to determine the structure of
HMM [28].
[29] employs a uniﬁed Bayesian treatment to
derive posterior probability for different model structures within
class of multinomial, Markov, Hidden Markov models without
assuming prior knowledge of transition probabilities. The
searching of HMM structure followed by learning makes the
approach infeasible in domains of cyber security where latency
is of utmost importance.

3. Preliminary Research Context

3.1. Problem Formulation

Given previous actions Si|t of attacker at each timestep t, we
intend to predict the next action attacker will likely take. Here
Si are discrete states corresponding to attacker action where
i ∈ (0, N ).

The goal of our system is to achieve accurate attack
prediction with low prediction latency and training time. The
algorithm, beforehand, should determine the optimal sequence
length and number of HMMs to employ to achieve some
global minima. Given the predictions of HMMs, the algorithm
must use them to accurately predict future states. Another
challenging task is to update models when necessary, which is
aimed to correct the model without incurring large overhead to

the monitored infrastructure. We proposed a possible scheme
for these issues in this paper.

3.2. Brief Introduction to Hidden Markov Model

FHMM uses a Hidden Markov Model (HMM) as its backbone
algorithm for learning relationship between deﬁned discrete
states. An HMM [30, 31] is a statistical Markov model with
hidden states. These hidden states are not directly visible
to the observer. The HMM can be completely deﬁned by
parameters A (transition matrix), B (observation matrix), π
(prior probability) as

λ = θ(A, B, π)

(1)

Two assumptions are made by the model. The ﬁrst, called the
Markov assumption, states that the current state is dependent
only on the previous state. The second, called the independence
assumption, states that the output observation at time t is
dependent only on the current state;
is independent of
previous observations and states. Given a set of examples from
a process, we would be able to estimate the model parameters
λ = θ(A, B, π) that best describe that process. Then, we could
discover the hidden state sequence that was most likely to have
produced a given observation sequence. More details can be
found in [30].

it

3.3. HMM from Bayesian Perspective

In the Bayesian approach we assume some prior knowledge
about the learning process or structure employed. For the
case of HMM, the prior knowledge in encoded in terms of
arcs of HMM, and model parameters. This prior knowledge
is represented in terms of prior distribution is used to obtain
posterior distribution over model structure and parameters.
More formally, assuming a prior distribution over model
structure P (M ) and a prior distribution over parameters for
each model structure P (θ|M), a data set D is used to form a
posterior distribution over models using Bayes rule ([32]):

P (M |D) =

(cid:82) P (D|θ, M )P (θ|M )dθP (M )
P (D)

(2)

which averages over uncertainty in parameters. The posterior
distribution over parameters is computed as:

P (θ|M, D) =

P (D|θ, M )P (θ|M )
P (D|M )

(3)

If we wish to predict the next observation, YT +1 based on our
data and models, the Bayesian prediction

P (YT +1|D) =

(cid:90)

P (YT +1|θ, M, D)P (θ|M, D)P (M |D)dθdM

(4)
averages over both the uncertainty in the model structure and its
parameters. This is known as the predictive distribution for the
model [32].

For complex attacker behaviour modelling problems, using
sequences with different characteristics to learn one Hidden
Markov Model leads to too much generalization and thus losing
the discriminating characteristics of the different attackers.
Thus, it is evident that the use of diverse attack sessions, to
train a single model, would lead to loss of information which
might be unique to a small number of attack sequences. For a
threat intelligence model, capturing this information becomes

important since it might a crude attack to hack into the system.
The proposed approach, called fusion HMM (FHMM), attempts
to partition the training data according to the distribution of
the number of attack sessions with respect to their lengths, and
then train multiple HMMs in a semi-supervised fashion. The
predictions of these models are combined using a nonlinear
network to provide better, robust predictions. This approach,
thus aims to capture the characteristics of the attack sequences
that would be lost while using a single global model.

4. Methodology

Using a single Bayesian structure like HMM to model the joint
distribution of all the observations and hidden state, makes
optimising θ to maximise the likelihood intractable. Particularly
when the data set D consists of mixture of distributions
over different sequence types where the individual distribution
might not be a Gaussian in nature, optimising θ to maximise
the likelihood P (D|θ, M ) becomes challenging. Hence we
propose segmenting data set D into sub parts such that
D = (cid:8)D(1), D(2), ...D(K)(cid:9) where D(i) are independent and
identically distributed observation sets. Empirical methodology
into K parts is provided in this
for segmenting data set
section. The beneﬁts of this segmentation are made apparent
in further equations. Intuitively, this is equivalent to breaking
down the problem into smaller independent sub-problem of
P (YT +1|D(i)), which allows using different θ values to reach
a local minima for that sub-problem. Modifying the original
equation to incorporate sub data set:

K
(cid:89)

i=1

P (YT +1|D(i)) =

(cid:90) K
(cid:89)

P (YT +1|θ, M, D(i)) P (θ|M, D(i))

i=1

P (M |D(i)) dθdM

(5)

In the limit of large data set and an uninformative or
uniform prior over parameters, the posterior P (θ |M, D(i)) will
be sharply peaked around the maxima of the likelihood, and
therefore the predictions of a single maximum likelihood model
will be similar to those obtained by Bayesian integration over
parameters.

K
(cid:89)

i=1

P (θ|M, D(i)) =

K
(cid:89)

i=1

P (D(i)|θ, M )

(6)

The maximum likelihood (ML) model parameters are

obtained by maximising the likelihood or log likelihood.

L(θ, M ) =

K
(cid:89)

i=1

P (D(i)|θ, M )

(7)

Further we assume a limiting case of Bayesian approach to
learning if we assume a single model structure M and we
estimate the parameter vector θ that maximising the likelihood
P (Y (i)|θ) under that model.
If model structure is assumed
constant,

L(θ) =

K
(cid:89)

i=1

P (D(i)|θ)

log L(θ) = log

K
(cid:89)

i=1

P (D(i)|θ)

(8)

(9)

log L(θ) = log P (D(1)|θ)+log P (D(2)|θ)+.. log P (D(K)|θ)
(10)

log L(θ) = log L1(θ) + log L2(θ) + .. log LK (θ)

(11)

In order to maximize the overall log likelihood with respect
to model θ, we need to maximize the individual log likelihood
terms where θ in each term can be different corresponding to
differing model parameters required to aptly represent the joint
distribution observation and hidden state.

log L(θ) =

K
(cid:88)

i=1

log Li(θi)

(12)

These individual models are represented by HMM whose
parameters are estimated by Baum Welch algorithm, a special
case of EM algorithm. We train K HMM independently
on each sub-data using Baum Welch, such that λ(i) =
If Y (i) =
arg max
θ
(cid:8)Y (i)

P (D(i)|θ) where i ∈ (0, ..K).
, ...(cid:9) is a sample sequence in D(i) then,

, Y (i)
1

0

P (Y (i)

N +1|Y (i)

1:N , λi) =

P (Y (i)

N +1|λi)

1:N , Y (i)
P (Y (i)

1:N |λi)

P (Y (i)

N +1|Y (i)

1:N , λ) ∝ P (Y (i)

1:N , Y (i)

N +1|λi)

Xi = arg max

YN +1

P (Y (i)

1:N , Y (i)

N +1|λi)

(13)

(14)

(15)

1:N , Y (i)

The denominator being independent

from YN +1, we
compute P (Y (i)
N +1|λi) for each possible YN +1. The value
of YN +1 which provides maximum likelihood can be estimated
as the best guess for the next observation for model λi. We
now have Xi which are estimates obtained from multiple
distributions P (YT +1|D(i)) instead of a estimate derived from
a single distribution P (YT +1|D).

The true estimate given D = (cid:8)D(1), D(2), ...D(K)(cid:9) is a
combination of Xi’s which might not be a linear combination.
Let function f be used to represent the non linear deterministic
component of the approximation, wt the random noise, then the
true output YT +1 is given by:

YT +1 = f (Xi) + wt

(16)

is

this

Expressing true outcome YT +1
in
circumventing time-invariant assumption of state transition and
emission matrix of earlier trained HMM λi. Therefore the
function f along with incorporating the Bayesian analysis of
HMM, now incorporates time-step t as input which acts as a
deciding factor for providing non-linear weight-age to each Xi

form, helps

YT +1 = f (Xi, t) + wt

(17)

We choose to approximate this mapping of Xi, t to YT +1 by
neural network.

Thus, the proposed Fusion Hidden Markov Model (FHMM)

algorithm can be subdivided into three major steps -

1. Finding optimum value of K according to the
distribution of input sequences and clustering data into
K diverse groups.

2. Training K different Hidden Markov Models on the K

subgroups.

3. Learning mapping of individual HMM predictions to a

single prediction using a Neural Network.

The basic terminology used in this paper is as follows -

T = length of the observation sequence
N = number of states in the model
M = number of observation symbols
Q = distinct states of Markov process = (cid:8)q0, q1, .., qN −1
V = set of possible observations = (cid:8)0, 1, ..., M − 1(cid:9)
π = initial state distribution
A = state transition probabilities = aij with shape N xN
where aij = P (state qj at t + 1 | state qi at t)

(cid:9)

= P (xt+1|xt)

B = observation probability matrix
= {bj(k)} with shape N xM

where bj(k) = P (observation k at t | state qj at t)

= P (yt|xt)

O = (O0, O1, ..., OT −1)

= observation sequence

where Oi ∈ Vx for i ∈ {0, 1, ..., T − 1}

An HMM is completely deﬁned by parameters A, B, π and
denoted by λ = θ(A, B, π).

4.1. Partitioning Data into K Groups

An important part of any ensemble learning algorithm is to
partition data into non-correlated subsets which will make
learning from it fruitful. The non-correlated K subsets of data
also make sure HMM’s are ﬁtted to different information subset
which when combined will characterise the whose time-series
data. The number K is a hyperparameter which characterizes
the complexity of the data. There exists a tradeoff between
K used and time required to train HMM as complexity of
data increases. To satisfy the above conditions FHMM uses a
dissimilarity function f (F ) to divide data into K subsets such
that each subset captures a particular pattern of temporal data.
The dissimilarity function is a custom deﬁned distance function
whose deﬁnition would change depending on the type of data
used. Initially, dividing data depending upon length of sequence
provides two beneﬁt-

• The data of similar length generally originates from
same distribution. To illustrate, in case of cyber attacks,
the attacks of similar length generally employ same
attack strategy and pattern. This makes sure that we
are capturing attacker mindset or bot attack patterns in
subset of data. This will help HMM to learn the pattern
quickly with higher accuracy.

• From implementation and operational point of view.
HMMs train on data of same length comparatively faster
as compared to data consisting of variable length.

• Diverse state transitions are present in attack sequences
it becomes

of signiﬁcantly different lengths. Thus,
difﬁcult for a single HMM to accurately model it.

The data partitioned into different subsets by length is then
chosen for the ensemble depending on its correlation with each
other. For each sub dataset Di where i (cid:15) int(0, max(l)) we
compute a frequency occurrence array. This frequency array is
computed for each sub dataset Di. The frequency array consists
of occurence of discrete states in the sub dataset Di. This
frequency occurrence array consists of occurence of each state

Algorithm 1 Choosing and Training HMM

Input: j = [O0, O1, . . . .Oi], N, M ,
Dissimilarity function (f (F ))

Output: [λ0, λ1, . . . .λB]

1: L = max length (j)
2: for all 0 in j do
3: Di = O where i ∈ int(0, L)
4: end for
5: for all Di do
6:
7: end for
8: Construct similarity sets S by computing Euclidean

Compute Frequency array F [i] where i ∈ V

distance between frequency arrays F [i]
9: Assign Ranks Ri, where i ∈ V using S
10: Use Ri to obtain lengths li
11: return li

divided by total states seen. The frequency array characterises
the sub-dataset Di quantitatively which makes it possible to
compare its ‘similarity’ with other Di. The G sub dataset Di
is represented by a 1 × N frequency array vector where N is
number of discrete states. Then the G × N frequency matrix is
used to obtain a set of datasets Dj called similarity set, which
is similar to a particular dataset Di by computing euclidean
distance between the corresponding frequency arrays. These
similar sets are used to rank datasets Di to select K datasets
Di from total G sub dataset Di such that these K datasets are
diverse, have low correlation with each other and cover most of
the training data.

Algorithm 2 Individual Hidden Markov Model Training and
Updating Process

Input: O, N, M
Output: HMM Parameters λ = (A, B, π)

1: Random Initialize λ = (A, B, π)
2: Forward pass: Compute forward probabilities αt(i)

Normalize computed αt(i)

3: Backward pass: Compute backward probabilities βt(i)

Normalize computed βt(i)

4: Compute Di-gammas and Gammas
5: Re-estimate A, B, and π
6: Compute log[P (O|λ)]
7: if log[P (O|λ)]new > log[P (O|λ)]old then
8:
9: else
10:
11: end if
12: End

return λ = (A, B, π)

Go to step 2

4.2. Training K HMMs

In the second step, a group of attack sequences in the training
data Di,
is used to learn an HMM model λi. Although
the use of small set of attack sequences to learn an HMM
λi might lead to overﬁtting, in a broader context, it ensures
that each model captures the set of characteristics distinct
to that group of sequences. Each Hidden Markov Model
λi is trained using Baum-Welch algorithm [30] along with
Expectation Maximization step (EM) [33] in order to train the
parameters (transition, emission and prior probabilities) of the
model. EM maximizes the likelihood of each sequence with

respect to the corresponding model λi i.e., P (Di|λi). Thus,
each model is capable of accurately predicting future steps in a
sequence which belongs to the probability distribution learnt by
it. The detailed training procedure is given in Algorithm 2. In
each model the number of hidden states can be viewed a hyper-
parameter which can be tuned.

4.3. Combining Predictions of K HMMs

The resulting predictions from K Hidden Markov Models are
then aggregated by neural network to generate a combined
prediction for next step. The neural network layer consists of
a linear weight component followed by a nonlinear function.
It minimizes the mean squared error of the HMM predictions
on the training data and assigns a probabilistic weight
to
each HMM. Thus, the neural network layer learns a nonlinear
mapping from the predictions of HMMs to the next state
output. The K Hidden Markov Model predictions are then
assigned a weight W where equation (summation of W over
all K HMM = 1). This importance to particular HMM’s
output characterized by weight W is learnt by iterating through
samples.

Algorithm 3 Individual Hidden Markov Model Prediction
Process

Input: O, λ, V
Output: Xk(t)
1: Initialize: U array
2: for k in V do
3: O.append(V, k)
4:
5:
6: end for
7: return argmax(U )
8: Train individual HMM on Di where i(cid:15)li
9: return [λ0, λ1, . . . .λB]

Compute Gamma
U .append(Uk)

The problem is now deﬁned as given predictions
[X0, X1, . . . .XK ] from HMM [λ0, λ1, . . . .λK ] at timestep
t = T − 1, what is the likely action the attacker will take
at timestep t = T . FHMM uses supervised neural network,
as shown in Figure 1, for learning the weight W in training
phase from N data samples and then uses the learned weight
W to estimating the future action taken. The network used is
characterized by -

Figure 1: Mapping estimates using neural network

αl

j = σ(

(cid:88)

(wl

jkαl−1

k + bl
j)

(18)

k

4.4. Overall Algorithm

During training stage, FHMM algorithm takes multiple attack
sequence consisting of discrete states as input. Depending on
sequence length and rank computed using dissimilarity function
it selects and divides data into least correlated K sub data. Then
the predictions obtained from K HMM trained to overﬁt on
the K sub data are fed to an neural network for learning the
weightage of each HMM.

phase,

During

prediction

intermediate HMM
predictions are fed to neural network which outputs a single
value of next state in sequence of attack. Figure refﬁg:ﬁg2
illustrates the training and prediction phases in FHMM
algorithm.

the

where al
layer.

j is the activation and bl

j of the jth neuron in the lth

We deﬁne a cost C which is used as a measure to indicate
the offset of predicted action from actual action taken. We
deﬁne the cost C over all samples n of data as:

C =

1
2n

(cid:88)

x

||y(x) − αL(x)||

(19)

This equation characterises the difference in the true output
y(x) and the output predicted by network y(cid:48) = aL(x). Then we
use backpropagation ([34]) to compute the gradients and update
the weights of the network.

The depth of network required in terms of layers L
increases as the complexity of pattern increases. Apart from
neural networks superior ability to model nonlinear complex
relationship in data, it has following beneﬁts over traditional
parametric approaches -

• Neural Network generalize well to unseen data and
can infer unseen patterns not initially present in data
provided in training.
in
cyber security applications where new attacks consists
of previously employed strategies intermittently spread
throughout the attack. The neural network can detect this
type of sub-pattern in attack and therefore complements
the ability of HMM.

This is extremely crucial

• Unlike HMM and other parametric techniques, neural
network does not impose restriction on the distribution
of the input variables. Moreover, neural networks can
better model heteroscedasticity, while traditional model
fail
in to model data with high volatility and non-
constant variance which is common in cyber security
applications.

The training and evaluating phase is given in Algorithm 4.

Figure 2: Overall FHMM Algorithm

Algorithm 4 Second Stage Data Collection

Input: O, [λ0, λ1, ..., λB]
Output: Xk
1: for k in n do
2: Xk = Predictions(λ, O, V )
3: end for

Training phase:

Input: [X0, X1, ..., Xk], Y, lr = learning rate
Output: f (x; θ, w)
1: Initialize: W, w randomly
2: Y (cid:48) = wT . max(0, W T x + c) + b
3: Compute C = Cost(Y, Y (cid:48))
4: Update parameters:

Wnew = Wold − ∆C.lr

Go to step 1

5: if Cost(Ynew, O) < Cost(Yold, O) then
6:
7: else
8:
9: end if

return f (x; θ, w)

Evaluation phase:

Input: [X0, X1, ..., Xk]
Output: Y (cid:48)

1: Y (cid:48) = wT . max(0, W T x + c) + b

5. Experiments and Discussion

5.1. Dataset Used

For collecting real attack logs, we had setup Cowrie honeypot
[35] which is a medium interaction SSH and telnet honeypot.
Honeypot is a decoy system with the sole intention of tricking
attacker with an easy target to log his attack patterns. In case
of Cowrie, the attack patterns are logged in JSON format.
The detailed description of attack features logged and dataset
description is provided by [36]. We processed events in Cowrie
logs and divided them into 19 commands consisting of:

client.size, client.version, command.failed,
command.input/delete, command.input/dir-sudo,
command.input/other, command.input/system,
command.input/write, command.success, direct-tcpip.data,
direct-tcpip.request, log.closed, log.open, login.failed,
login.success, session.closed, session.connect,
session.ﬁle-download, session.input

We encoded these events as discrete states labelled 0 to
18. These 19 states were used for modelling and prediction
in HMM, LSTM and FHMM algorithm. The data is grouped
by session id for considering each sequence where each
session id corresponds to the sequence of actions taken by
hacker. The assumption made here is different session id are

Figure 3: Distribution of attacker sessions.

Figure 4: 2-D plot of frequency arrays.

Figure 5: FHMM error rate vs number of
HMMs.

independent of individual attacker characteristics and hence
dividing depending on session ID rather than source IP would
not affect the modelling and prediction by a large factor.

The dataset used for training FHMM was extracted from
the logs generated by Cowrie honeypot from April 2017 to
July 2017. By processing these logs, we generated 22,499
distinct attack sessions involving the sequence of steps taken by
a particular source IP. The attack sessions lasted from 2 steps to
over 1400 steps. These sessions are raw logs of shell interaction
performed by the attacker. We evaluate the performance of
FHMM on a separate test set comprising of real time logs of
attackers’ actions for 1 month.

5.2. Partitioning Data into K groups

In cyber security logs, there are multiple attacks from different
hackers of variable attack lengths. This results in logs being
generated which are varied in terms of types, pattern, sequence
length, and duration of attack or inﬁltration. The data can be
divided into sub-datasets depending on either of the factors,
each with its merits and demerits. Due to phenomena prevalent
in cyber security domain where attacks of similar length
following similar patterns, we have considered the distribution
of the number of attack sessions with respect to their lengths and
partitioned the preprocessed training data according to lengths.
Figure 3 shows the distribution of number of attack sessions
with respect to their lengths.

Once the data is divided into sub-datasets we can train N
number of HMM on the sub-datasets and pass their predictions
to Neural Network. But to provide fast response time to prevent
real-time attack, training and predictions of N HMMs where
N can be in thousands is not practically feasible. Hence
we use similarity measures to reduce the sub-datasets from
In order to divide training data into K sets, we
N to K.
construct 19-dimensional frequency arrays F [i] for each dataset
Di consisting of sequences of length i where 1 ≤ i ≤ L.
The frequency arrays describe the distribution of different states
in terms of probability of occurrence of each state across
multiple attacks of same length. Figure 4 shows these frequency
arrays reduced to 2-dimensional vector space by employing
PCA [37]. Using frequency array F [i] to characterise all
attacks of particular length, is sufﬁciently accurate to compute
dissimilarity measure between attacks of different lengths. It is
also essential the sub-dataset K produced from N be containing
maximum information along with being mutually exclusive and
dissimilar with one another. Euclidean distance between these
arrays along each of 19 dimension provides the dissimilarity
between the arrays. Hence we compute euclidean distance
between these arrays F [i] to ﬁnd dissimilar sets and select K

datasets such that these K datasets cover maximum information
present in the training data. The selected K datasets out of
N total datasets helps in reducing training and prediction time
without signiﬁcantly affecting accuracy of deployed system.

The Figure 6 depicts individual HMM error curve while

training.

5.3. Training K HMMs

The K datasets selected, are used to train K different HMMs
This HMMs are trained until convergence using Baum Welsh
algorithm. HMM is implemented in cython and trained
parallely, thus reducing computation time by a large factor.
Then each of the K HMMs predicts the next possible state,
resulting in K predictions for next state which are fed to neural
network.

To illustrate the practical need of splitting data and training
K different HMM, see the Figures 4, 7 and 8. The Figure 7
shows HMM trained on different lengths and the distribution
of output states predicted by HMM. Here each state type is
represented by different color and clearly shows why different
K HMMs are required to model entire data. Moreover, it also
depicts that each HMM is learning different data patterns unique
to an attacking type.

5.4. Combining Predictions of K HMMs

The predictions of these HMMs are combined by a neural
network containing 60 units with ReLU activation. Selecting
the optimum value of K is a classical bias-variance tradeoff.
Large values of k (>50) lead to poor generalization on test data.
Table 1 shows the weight assigned to each feature input given
to neural network in descending order. Here the features are
prediction made by HMM of trained on particular length. The
table clearly indicates temporal feature count has the highest
importance in determining the next state taken by attacker,
followed by predictions from HMM trained on length 11 and
44.

5.5. Quantitative Results

Selecting the optimum value of K is a tradeoff between error
rate and computational requirements. Increasing the value of
K reduces the error rate. Figure 5 shows a plot of error rate vs
number of models in FHMM.

As evident, there is a signiﬁcant reduction in error due to
adding learners to the fusion. For our dataset, much of the
reduction appears after 20-25 classiﬁers. One reason for this
is the diverse set of features learnt by HMMs - 23, 24 and 25.
This is evident from the correlation between the predictions of

Figure 6: Error rate while training a single
HMM.

Figure 7: State distribution of different
HMMs.

Figure 8: Correlation between HMM’s
predictions.

individual HMMs as shown in Figure 8.

Table 1: Weights Assigned by Neural Network to Individual
HMMs

Weight
12.0151 ± 0.0308
4.6469 ± 0.0179
3.9593 ± 0.0131
3.9512 ± 0.0261
2.9618 ± 0.0069
2.6922 ± 0.0071
2.6831 ± 0.0089
2.6451 ± 0.0069
2.5365 ± 0.0214
2.3346 ± 0.0166
2.2671 ± 0.0220
2.1762 ± 0.0204
2.0389 ± 0.0093
1.7186 ± 0.0198
1.6456 ± 0.0043
1.5216 ± 0.0082
1.4252 ± 0.0136
1.3633 ± 0.0047
1.3594 ± 0.0037
1.2440 ± 0.0063
1.2393 ± 0.0072
1.1861 ± 0.0085
1.1355 ± 0.0061

Feature
count
hmm 11.0
hmm 44.0
hmm 13.0
hmm 188.0
hmm 106.0
hmm 23.0
hmm 127.0
hmm 28.0
hmm 18.0
hmm 76.0
hmm 129.0
hmm 199.0
hmm 4.0
hmm 80.0
hmm 9.0
hmm 6.0
hmm 20.0
hmm 69.0
hmm 194.0
hmm 197.0
hmm 14.0
hmm 31.0

After 35-40 models, the error reduction for FHMM appears
to have nearly reached a plateau. So, we primarily focus
on the performance of FHMM for K = 38. Table 2 shows
the prediction accuracy attained by FHMM with K = 25 and
K = 38 along with that of other sequence models such as
Markov chain, single HMM and LSTM. Depending on how the
HMMs in FHMM are trained, the training time differs. As the
training of individual HMM is independent of other HMMs, this
allows parallel training with faster training times compared to
sequential training pipeline.

One obvious conclusion drawn from the results is that the

Table 2: Comparison of accuracy obtained by different models

Model
Markov Chain
HMM
LSTM
FHMM (K=25, sequential)
FHMM (K=38, sequential)
FHMM (K=25, parallel)
FHMM (K=38, parallel)

Accuracy
72
77
86
87.19
90.82
87.19
90.82

Training Time (in hrs)
0.3
1
5
2.3
2.5
1.3
1.5

reduction in error rate provided by FHMM is very large as
compared to that of a single learner. Additionally, FHMM
has a better generalization ability than single models which
may be attributed to the following reasons. The training
data contains considerably diverse information and it becomes
difﬁcult for a single learner to learn a generalized joint
probability distribution over the data. Thus, we use many
learners which perform well on parts of data. These learners
may learn different distributions over the data and combining
them is a convenient choice. Experimentally Table 3 illustrates
this result, where accuracy of each state as predicted by HMM
trained on different lengths is compared with FHMM. Training
many learners also circumvents the imperfect search process
of HMM. In HMM, we assume some prior knowledge about
the learning process and the model structure and the desired
in the
complex input-output mapping may not be present
hypothesis space being searched by the learning algorithm.
In such cases, exploiting multiple learners provides a better
estimate.

5.6. Limitations

Although FHMM algorithm is robust to noise and provides
a signiﬁcant reduction in error rate while modelling attack
sequences, it has some limitations common to other ensemble
methods. The basic requirement of FHMM is that the base
HMMs should be diverse and must have low correlation with
each other for a signiﬁcant reduction in error rate over train
distribution. However, creating diverse base models is not
always possible. Moreover, it requires the use of techniques
such as partitioning data into diverse groups and initializing the
base learners differently to induce heterogeneity. In addition,
FHMM is complex and computationally expensive as compared
to simple probabilistic algorithms such as Markov chain and
HMM. With FHMM, learning time and memory constraints
need to be taken care of.

5.7. Other Applications

is to predict

The proposed FHMM algorithm can be easily extended to
other sequence problems where the goal
the
next state in the sequence. While the hidden state and the
observation spaces are discrete in the above FHMM algorithm,
the FHMM can also be used to model continuous observations.
Potential applications include stock prices prediction, speech
synthesis, time-series analysis, gene prediction and parts-of-
speech tagging. For these applications, the major portion of
the algorithm would remain identical with a change in the
criteria for partitioning data into K groups. For this purpose,
other methods such as clustering and similarity measures like

Table 3: Accuracy obtained for different states by FHMM (k=38)

State
FHMM
HMM 9
HMM 11
HMM 44
HMM 71
HMM 199

0
0
0
0
0
0
0

1
0
100
0
100
0
0

2
59.1
0
82.9
0.1
53.4
0

3
70.2
0
66.3
12.7
35.3
73

4
92.9
0
0
0
0
73.2

5
68.2
0
0
0
72.4
0

6
0
0
0
0
0
0

7
97.2
0
0
0
17.3
0

8
99.6
0
2.1
6.3
81.3
96.9

9
59
63.2
0
0
0
0

10
54.3
35.5
0
9.7
0
0

11
53.6
0
0
13.2
9.4
0

12
99.1
0
97.5
0.8
0
97.5

13
41.6
0
0
0
0
0

14
37.3
37.2
62.5
37.2
0
62.5

15
42.4
0
0
0.1
40.8
0

16
-
-
-
-
-
-

17
25.6
0
0
7.1
0
0

18
42.9
0
0
0
0
0

cosine distance can be employed depending on the training data
and the application. After incorporating a suitable partitioning
technique, the FHMM algorithm can be identically applied to
other sequencing tasks.

6. Conclusion

This paper proposes Fusion Hidden Markov Model which
exploits the beneﬁt of ensemble learning for modelling
behavioural aspect of attacker to obtain better insight on
predicting his future actions. FHMM provides compelling
results while modelling temporal patterns due to its higher
modelling capacity, robustness to noise, and reduced training
time.
FHMM’s superiority is substantiated by comparing
against traditional approaches of Markov Chain, HMM, deep
LSTM. The model is evaluated on Cowrie Honeypot dataset
which consists of large number of diverse real-time attack
sessions. Keeping initial conditions and preprocessing constant,
the proposed architecture outperforms other traditional and
benchmark models.
In addition, we explored FHMM in
depth, with highlights to individual parameter contribution
to the overall model. The architecture of FHMM allows it
to be generalized to other domains which involves temporal
modelling of sequential data.

7. References
[1] A. Razzaq, A. Hur, H. F. Ahmad, and M. Masood, “Cyber
security: Threats, reasons, challenges, methodologies and state
of the art solutions for industrial applications,” in 2013 IEEE
Eleventh International Symposium on Autonomous Decentralized
Systems (ISADS), March 2013, pp. 1–6.

[2] Gemalto, “Data breach statistics by year,

industry, more.”

[Online]. Available: http://www.breachlevelindex.com/

[3] C. N. Modi and K. Acha, “Virtualization layer security challenges
and intrusion detection/prevention systems in cloud computing:
a comprehensive review,” The Journal of Supercomputing,
vol. 73, no. 3, pp. 1192–1234, Mar 2017. [Online]. Available:
https://doi.org/10.1007/s11227-016-1805-9

[4] B. Mukherjee, L. T. Heberlein, and K. N. Levitt, “Network
intrusion detection,” IEEE Network, vol. 8, no. 3, pp. 26–41, May
1994.

[5] E. Viegas, A. O. Santin, A. Franc¸a, R. Jasinski, V. A. Pedroni,
and L. S. Oliveira, “Towards an energy-efﬁcient anomaly-
based intrusion detection engine for embedded systems,” IEEE
Transactions on Computers, vol. 66, no. 1, pp. 163–177, Jan 2017.

[6] T. G. Dietterich, “Ensemble methods in machine learning,” in
Multiple Classiﬁer Systems. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2000, pp. 1–15.

[7] A. A. C´ardenas, P. K. Manadhata, and S. P. Rajan, “Big data
analytics for security,” IEEE Security Privacy, vol. 11, no. 6, pp.
74–76, Nov 2013.

[8] M. Al-Qurishi, M. S. Hossain, M. Alrubaian, S. M. M.
Rahman, and A. Alamri, “Leveraging analysis of user behavior to
identify malicious activities in large-scale social networks,” IEEE
Transactions on Industrial Informatics, vol. 14, no. 2, pp. 799–
813, Feb 2018.

[9] O. Vallis,

J. Hochenbaum,

and A. Kejariwal,

“A novel
technique for long-term anomaly detection in the cloud,” in
Proceedings of the 6th USENIX Conference on Hot Topics in
Cloud Computing, ser. HotCloud’14. Berkeley, CA, USA:
USENIX Association, 2014, pp. 15–15. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2696535.2696550

[10] W. Hu, W. Hu, and S. Maybank, “Adaboost-based algorithm
for network intrusion detection,” IEEE Transactions on Systems,
Man, and Cybernetics, Part B (Cybernetics), vol. 38, no. 2, pp.
577–583, April 2008.

[11] J. Zhang, M. Zulkernine, and A. Haque, “Random-forests-
based network intrusion detection systems,” IEEE Transactions
on Systems, Man, and Cybernetics, Part C (Applications and
Reviews), vol. 38, no. 5, pp. 649–659, Sep. 2008.

[12] D. E. Denning,

IEEE
Transactions on Software Engineering, vol. SE-13, no. 2,
pp. 222–232, Feb 1987.

“An intrusion-detection model,”

[13] O. Brdiczka, J. Liu, B. Price, J. Shen, A. Patil, R. Chow,
E. Bart, and N. Ducheneaut, “Proactive insider threat detection
through graph learning and psychological context,” in 2012 IEEE
Symposium on Security and Privacy Workshops, May 2012, pp.
142–149.

[14] L. Spitzner, “Honeypots:

threat,” in
19th Annual Computer Security Applications Conference, 2003.
Proceedings., Dec 2003, pp. 170–179.

catching the insider

[15] K. Granstr¨om, P. Willett, and Y. Bar-Shalom, “Asymmetric
threat modeling using hmms: Bernoulli ﬁltering and detectability
analysis,” IEEE Transactions on Signal Processing, vol. 64,
no. 10, pp. 2587–2601, May 2016.

[16] Z. Zhan, M. Xu, and S. Xu, “Characterizing honeypot-captured
cyber attacks: Statistical framework and case study,” IEEE
Transactions on Information Forensics and Security, vol. 8,
no. 11, pp. 1775–1789, Nov 2013.

[17] M. Kaaniche, Y. Deswarte, E. Alata, M. Dacier, and V. Nicomette,
“Empirical analysis and statistical modeling of attack processes
based on honeypots,” CoRR, vol. abs/0704.0861, 01 2007.

[18] O. Thonnard and M. Dacier, “A framework for attack patterns’
discovery in honeynet data,” Digital Investigation, vol. 5, 09 2008.

[19] S. Almotairi, A. Clark, G. Mohay, and J. Zimmermann, “A
technique for detecting new attacks in low-interaction honeypot
trafﬁc,” in 2009 Fourth International Conference on Internet
Monitoring and Protection, May 2009, pp. 7–13.

[20] Q. Zhang, D. Man, and W. Yang, “Using hmm for intent
recognition in cyber security situation awareness,” Knowledge
Acquisition and Modeling, International Symposium on, vol. 2,
pp. 166–169, 11 2009.

[21] Nong Ye, S. Vilbert, and Qiang Chen, “Computer intrusion
detection through ewma for autocorrelated and uncorrelated data,”
IEEE Transactions on Reliability, vol. 52, no. 1, pp. 75–82, March
2003.

[22] R. Mitchell and I. Chen, “Behavior-rule based intrusion detection
systems for safety critical smart grid applications,” IEEE
Transactions on Smart Grid, vol. 4, no. 3, pp. 1254–1263, Sep.
2013.

[23] D. Shi, R. J. Elliott, and T. Chen, “On ﬁnite-state stochastic
modeling and secure estimation of cyber-physical systems,” IEEE
Transactions on Automatic Control, vol. 62, no. 1, pp. 65–80, Jan
2017.

[24] E. Jonsson and T. Olovsson, “A quantitative model of the security
intrusion process based on attacker behavior,” IEEE Transactions
on Software Engineering, vol. 23, no. 4, pp. 235–245, April 1997.

[25] Q. D. La, T. Q. S. Quek, J. Lee, S. Jin, and H. Zhu, “Deceptive
attack and defense game in honeypot-enabled networks for the
internet of things,” IEEE Internet of Things Journal, vol. 3, no. 6,
pp. 1025–1035, Dec 2016.

[26] T. Rashid, I. Agraﬁotis, and J. R. Nurse, “A new take on detecting
insider threats: Exploring the use of hidden markov models,”
in Proceedings of the 8th ACM CCS International Workshop
on Managing Insider Security Threats, ser. MIST ’16. New
York, NY, USA: ACM, 2016, pp. 47–56. [Online]. Available:
http://doi.acm.org/10.1145/2995959.2995964

[27] A. Krogh, M. Brown, I. Mian, K. Sj¨olander, and D. Haussler,
“Hidden markov models in computational biology: Applications
to protein modeling,” Journal of Molecular Biology, vol. 235,
no. 5, pp. 1501 – 1531, 1994. [Online]. Available: http://www.
sciencedirect.com/science/article/pii/S0022283684711041

[28] Kyoung-Jae Won, A. Prugel-Bennett, and A. Krogh, “Evolving
the structure of hidden markov models,” IEEE Transactions on
Evolutionary Computation, vol. 10, no. 1, pp. 39–49, Feb 2006.

[29] M. Johansson and T. Olofsson, “Bayesian model selection for
markov, hidden markov, and multinomial models,” IEEE Signal
Processing Letters, vol. 14, no. 2, pp. 129–132, Feb 2007.

[30] L. Rabiner and B. Juang, “An introduction to hidden markov
models,” IEEE ASSP Magazine, vol. 3, no. 1, pp. 4–16, Jan 1986.

[31] R. Alghamdi, “Hidden markov models (hmms) and security
applications,” International Journal of Advanced Computer
Science and Applications, vol. 7, 02 2016.

[32] Z. Ghahramani, “An introduction to hidden markov models and

bayesian networks.” IJPRAI, vol. 15, pp. 9–42, 02 2001.

[33] T. K. Moon, “The expectation-maximization algorithm,” IEEE
Signal Processing Magazine, vol. 13, no. 6, pp. 47–60, Nov 1996.

[34] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning
representations by back-propagating errors,” Nature, vol. 323, pp.
533–536, 1986.

[35] M. Oosterhof, “Cowrie - medium-interaction honeypot,” Jul 2019.

[Online]. Available: https://github.com/micheloosterhof/cowrie

[36] R. Rade, S. Deshmukh, R. Nene, A. S. Wadekar, and A. Unny,
“Temporal and stochastic modelling of attacker behaviour,” in
Advances in Data Science, L. Akoglu, E. Ferrara, M. Deivamani,
R. Baeza-Yates, and P. Yogesh, Eds.
Singapore: Springer
Singapore, 2019, pp. 30–45.

[37] J. Shlens, “A tutorial on principal component analysis,” ArXiv,

vol. abs/1404.1100, 2014.

