CTD: Fast, Accurate, and Interpretable Method for Static and
Dynamic Tensor Decompositions

Jungwoo Lee
Seoul National University
muon9401@gmail.com

Dongjin Choi
Seoul National University
skywalker5@snu.ac.kr

Lee Sael
The State University of New York
(SUNY) Korea
sael@sunykorea.ac.kr

7
1
0
2

t
c
O
9

]

A
N
.
s
c
[

1
v
8
0
6
3
0
.
0
1
7
1
:
v
i
X
r
a

ABSTRACT
How can we find patterns and anomalies in a tensor, or multi-
dimensional array, in an efficient and directly interpretable way?
How can we do this in an online environment, where a new tensor
arrives each time step? Finding patterns and anomalies in a tensor
is a crucial problem with many applications, including building
safety monitoring, patient health monitoring, cyber security, terror-
ist detection, and fake user detection in social networks. Standard
PARAFAC and Tucker decomposition results are not directly inter-
pretable. Although a few sampling-based methods have previously
been proposed towards better interpretability, they need to be made
faster, more memory efficient, and more accurate.

In this paper, we propose CTD, a fast, accurate, and directly
interpretable tensor decomposition method based on sampling.
CTD-S, the static version of CTD, provably guarantees a high ac-
curacy that is 17∼83× more accurate than that of the state-of-the-
art method. Also, CTD-S is made 5∼86× faster, and 7∼12× more
memory-efficient than the state-of-the-art method by removing
redundancy. CTD-D, the dynamic version of CTD, is the first in-
terpretable dynamic tensor decomposition method ever proposed.
Also, it is made 2∼3× faster than already fast CTD-S by exploiting
factors at previous time step and by reordering operations. With
CTD, we demonstrate how the results can be effectively interpreted
in the online distributed denial of service (DDoS) attack detection.

CCS CONCEPTS
• Information systems → Data mining;

1 INTRODUCTION
Given a tensor, or multi-dimensional array, how can we find pat-
terns and anomalies in an efficient and directly interpretable way?
How can we do this in an online environment, where a new tensor
arrives each time step? Many real-world data are multi-dimensional
and can be modeled as sparse tensors. Examples include network
traffic data (source IP - destination IP - time), movie rating data
(user - movie - time), IoT sensor data, and healthcare data. Finding
patterns and anomalies in those tensor data is a very important
problem with many applications such as building safety monitor-
ing [10], patient health monitoring [5, 13, 15, 22], cyber security
[19], terrorist detection [1, 2, 14], and fake user detection in social
networks [4, 11]. Tensor decomposition method, a widely-used
tool in tensor analysis, has been used for this task. However, the
standard tensor decomposition methods such as PARAFAC [9] and
Tucker [21] do not provide interpretability and are not applicable
for real-time analysis in environments with high-velocity data.

Table 1: Comparison of our proposed CTD and the exist-
ing tensor-CUR. The static method CTD-S outperforms the
state-of-the-art tensor-CUR in terms of time, memory usage,
and accuracy. The dynamic method CTD-D is the fastest and
the most accurate.

Existing

[Proposed]

Tensor-CUR [12]

Interpretability
Time
Memory usage
Accuracy
Online

✓
fast
low
low

CTD-S

✓
faster
lower
high

CTD-D

✓
fastest
low
highest
✓

Sampling-based tensor decomposition methods [3, 7, 12] arose
as an alternative to due to their direct interpretability. The direct
interpretability not only reduces time and effort involved in find-
ing patterns and anomalies from the decomposed tensors but also
provides clarity in interpreting the result. A sampling-based de-
composition method for sparse tensors is also memory-efficient
since it preserves the sparsity of the original tensors on the sampled
factor matrices. However, existing sampling-based tensor decom-
position methods are slow, have high memory usage, and produce
low accuracy. For example, tensor-CUR [12], the state-of-the-art
sampling-based static tensor decomposition method, has many re-
dundant fibers including duplicates in its factors. These redundant
fibers cause higher memory usage and longer running time. Tensor-
CUR is also not accurate enough for real-world tensor analysis.

In addition to interpretability, demands for online method ap-
plicable in a dynamic environment, where multi-dimensional data
are generated continuously at a fast rate, are also increasing. A
real-time analysis is not feasible with static methods since all the
data, i.e., historical and incoming tensors, need to be decomposed
over again at each time step. There are a few dynamic tensor de-
composition methods proposed [16, 17, 23]. However, proposed
methods are not directly interpretable and do not preserve sparsity.
To the best of our knowledge, there has been no sampling-based
dynamic tensor decomposition method proposed.

In this paper, we propose CTD (Compact Tensor Decomposition),
a fast, accurate, and interpretable sampling-based tensor decom-
position method. CTD has two versions: CTD-S for static tensors,
and CTD-D for dynamic tensors. CTD-S is optimal after sampling,
and results in a compact tensor decomposition through careful
sampling and redundancy elimination, thereby providing much
better running time and memory efficiency than previous methods.
CTD-D, the first sampling-based dynamic tensor decomposition

 
 
 
 
 
 
(a) Hypertext 2009

(b) Haggle

(c) Manufacturing emails

(d) Infectious

(e) Hypertext 2009

(f) Haggle

(g) Manufacturing emails

(h) Infectious

Figure 1: Error, running time, and memory usage of CTD-S compared to those of tensor-CUR. CTD-S is more accurate, faster
and more memory-efficient than tensor-CUR.

method in literature, updates and modifies minimally on the compo-
nents altered by the incoming data, making the method applicable
for real-time analysis on a dynamic environment. Table 1 shows
the comparison of CTD and the existing method, tensor-CUR. Our
main contributions are as follows:

• Method. We propose CTD, a fast, accurate, and directly
interpretable tensor decomposition method. We prove the
optimality of the static method CTD-S which makes it more
accurate than the state-of-the-art method. Also, to the best
of our knowledge, the dynamic method CTD-D is the first
sampling-based dynamic tensor decomposition method.
• Performance. CTD-S is 17∼83× more accurate, 5∼86× faster,
and 7∼12× more memory-efficient compared to tensor-CUR,
the state-of-the-art competitor as shown in Figure 1. CTD-D
is up to 3× faster than CTD-S as shown in Figure 5.

• Interpretable Analysis. We show how CTD results are
directly interpreted to successfully detect DDoS attacks in
network traffic data.

The code for CTD and datasets are available at https://datalab.snu.
ac.kr/CTD/. The rest of this paper is organized as follows. Section 2
describes preliminaries and related works for tensor and sampling-
based decomposition. Section 3 describes our proposed method
CTD. Section 4 presents the experimental results. After presenting
CTD at work in Section 5, we conclude in Section 6.

2 PRELIMINARIES AND RELATED WORKS
In this section, we describe preliminaries and related works for ten-
sor and sampling-based decompositions. Table 2 lists the definitions
of symbols used in this paper.

2

2.1 Tensor
A tensor is a multi-dimensional array and is denoted by the boldface
Euler script, e.g. X ∈ RI1×···×IN where N denotes the order (the
number of axes) of X. Each axis of a tensor is also known as mode
or way. A fiber is a vector (1-mode tensor) which has fixed indices
except one. Every index of a mode-n fiber is fixed except n-th index.
A fiber can be regarded as a higher-order version of a matrix row
and column. A matrix column and row each correspond to mode-1
fiber and mode-2 fiber, respectively. A slab is an (N − 1)-mode
tensor which has only one fixed index. X(α ) ∈ RIα ×Nα denotes a
mode-α matricization of X, where Nα = (cid:206)
n(cid:44)α In . X(α ) is made by
rearranging mode-α fibers of X to be the columns of X(α ). ∥X∥F
is the Frobenius norm of X and is defined by Equation 1.

∥X∥2
F

= (cid:213)

i1,i2, ··· ,iN

x

2
i1i2 ···iN

(1)

X ×n U ∈ RI1×···×In−1×J ×In+1×···×IN denotes the n-mode product
of a tensor X ∈ RI1×···×IN with a matrix U ∈ RJ ×In . Elementwise,

(X ×n U)i1 ···in−1 jin+1 ···iN

=

In(cid:213)

in =1

xi1 ···in−1in in+1 ···iN ujin

(2)

X ×n U has a property shown in Equation 3.

Y = X ×n U ⇔ Y(n) = UX(n)

(3)

We assume that a matrix or tensor is stored in a sparse-unordered
representation (i.e. only nonzero entries are stored in a form of pair
of indices and the corresponding value). nnz(X) denotes the number
of nonzero elements in X.

We describe existing sampling-based matrix and tensor decom-

position methods in the following subsections.

00.51Relative Error10-1100101102Running Time (sec) Best5.4x58x00.51Relative Error10-1100101102Running Time (sec) Best6.2x17x00.51Relative Error10-1100101102Running Time (sec) Best7.8x83x00.51Relative Error10-2100102104Running Time (sec) Best86x30x00.51Relative Error10-1100101102Memory Usage Best7.6x00.51Relative Error10-1100101102Memory Usage Best6.6x00.51Relative Error10-1100101102Memory Usage Best7.5x00.51Relative Error10-2100102Memory Usage Best12xMethods :   CTD-S              Tensor-CURTable 2: Table of symbols.

Symbol Definition

X
X
x
x
X(n)
X†
N
×n
∥ • ∥F
nnz(X)

tensor (Euler script, bold letter)
matrix (uppercase, bold letter)
column vector (lower case, bold letter)
scalar (lower case, italic letter)
mode-n matricization of a tensor X
pseudoinverse of X
order (number of modes) of a tensor
n-mode product
Frobenius norm
number of nonzero elements in X

2.2 Sampling Based Matrix Decomposition
Sampling-based matrix decomposition methods sample columns
or rows from a given matrix and use them to make their factors.
They produce directly interpretable factors which preserve spar-
sity since those factors directly reflect the sparsity of the original
data. In contrast, a singular value decomposition (SVD) generates
factors which are hard to understand and dense because the factors
are in a form of linear combination of columns or rows from the
given matrix. Definition 2.1 shows the definition for CX matrix
decomposition [8], a kind of sampling-based matrix decomposition.

Definition 2.1. Given a matrix A ∈ Rm×n , the matrix ˜A = CX is a
CX matrix decomposition of A, where a matrix C ∈ Rm×c consists
of actual columns of A and a matrix X is any matrix of size c × n.

We introduce well-known CX matrix decomposition methods:

LinearTimeCUR, CMD, and Colibri.

LinearTimeCUR and CMD. Drineas et al. [6] proposed LinearTime-
CUR and Sun et al. [18] proposed CMD. In the initial step, Lin-
earTimeCUR and CMD sample columns from an original matrix
A according to the probabilities proportional to the norm of each
column with replacement. Drineas et al. [6] has proven that this
biased sampling provides an optimal approximation. Then, they
project A into the column space spanned by those sampled columns
and use the projection as the low-rank approximation of A. Lin-
earTimeCUR has many duplicates in its factors because a column
or row with a higher norm is likely to be selected multiple times.
These duplicates make LinearTimeCUR slow and require a large
amount of memory. CMD handles the duplication issue by remov-
ing duplicate columns and rows in the factors of LinearTimeCUR,
thereby reducing running time and memory significantly.

Colibri. Tong et al. [20] proposed Colibri-S which improves CMD
by removing all types of linear dependencies including duplicates.
Colibri-S is much faster and memory-efficient compared to Lin-
earTimeCUR and CMD because the dimension of factors is much
smaller than that of LinearTimeCUR and CMD. Tong et al. [20] also
proposed the dynamic version Colibri-D. Although Colibri-D can
update its factors incrementally, it fixes the indices of the initially
sampled columns which need to be updated over time. Our CTD-D
not only handles general dynamic tensors but also does not have
to fix those indices.

3

2.3 Sampling Based Tensor Decomposition
Sampling-based tensor decomposition method samples actual fibers
or slabs from an original tensor. In contrast to PARAFAC and Tucker,
the most famous tensor decomposition methods, the resulting fac-
tors of sampling-based tensor decomposition method are easy to
understand and usually sparse. There are two types of sampling
based tensor decomposition: one based on Tucker and the other
based on LR tensor decomposition which is defined in Definition 2.2.
In Tucker-type sampling based tensor decomposition (e.g., Approx-
TensorSVD [7] and FBTD (fiber-based tensor decomposition) [3]),
factor matrices for all modes are either sampled or generated; the
overhead of generating a factor matrix for each mode makes these
methods too slow for applications to real-time analysis. We focus
on sampling methods based on LR tensor decomposition which is
faster than those based on Tucker decomposition.

Definition 2.2. (LR tensor decomposition) Given a tensor X ∈
RI1×I2×···×IN , ˜X = L ×α R is a mode-α LR tensor decomposition of
X, where a matrix R ∈ RIα ×c consists of actual mode-α fibers of X
and a tensor L is any tensor of size I1 ×· · ·×Iα −1 ×c ×Iα +1 ×· · ·×IN .
Tensor-CUR. Mahoney et al. [12] proposed tensor-CUR, a mode-α
LR tensor decomposition method. Tensor-CUR is an n-dimensional
extension of LinearTimeCUR. Tensor-CUR samples fibers and slabs
from an original tensor and builds its factors using the sampled
ones. The only difference between LinearTimeCUR and tensor-
CUR is that tensor-CUR exploits fibers and slabs instead of columns
and rows. Thus, tensor-CUR has drawbacks similar to those of
LinearTimeCUR. Tensor-CUR has many redundant fibers in its
factors and these fibers make tensor-CUR slow and use a large
amount of memory.

3 PROPOSED METHOD
In this section, we describe our proposed CTD (Compact Tensor De-
composition), an efficient and interpretable sampling-based tensor
decomposition method. We first describe the static version CTD-S,
and then the dynamic version CTD-D of CTD.

3.1 CTD-S for Static Tensors

Overview. How can we design an efficient sampling-based static
tensor decomposition method? Tensor-CUR, the existing state-of-
the-art, has many redundant fibers in its factors and these fibers
make tensor-CUR slow and use large memory. Our proposed CTD-
S method removes all dependencies from the sampled fibers and
maintains only independent fibers; thus, CTD-S is faster and more
memory-efficient than tensor-CUR.

Algorithm. Figure 2 shows the scheme for CTD-S. CTD-S first
samples fibers biased toward a norm of each fiber. Three different
fibers (red, blue, green) are sampled in Figure 2. There are many
duplicates after biased sampling process since CTD-S samples fibers
multiple times with replacement and a fiber with a higher norm
is likely to be sampled many times. There also exist linearly de-
pendent fibers such as the green fiber which can be expressed as a
linear combination of the red one and the blue one. Those linearly
dependent fibers including duplicates are redundant in that they
do not give new information when interpreting the result. CTD-S
removes those redundant fibers and stores only the independent

always consist of independent mode-α fibers through the loop. In
each iteration, CTD-S checks whether one of the sampled fibers is
linearly independent of the column space spanned by R or not in
lines 7-8. If the fiber is independent, CTD-S updates U and expands
R with the fiber in lines 10-13. Finally, CTD-S computes C with X
and R in line 16.

Lemma 3.1 shows the computational cost of CTD-S.

Lemma 3.1. The computational complexity of CTD-S is O((˜sIα +
2 +nnz(R)) +s log s +nnz(X)), where Nα is (cid:206)
n(cid:44)α In and

s)Nα +s ′(˜s
˜s ≪ s ′ ≤ s.

Proof. The mode-α matricization of X in line 1 needs O(nnz(X))
operations. Computing column distribution in line 2 takes O(nnz(X)+
Nα ) and sampling s columns in line 3 takes O(sNα ). O(s log s) op-
eration is required in computing unique elements in I in line 4.
2 +nnz(R))) as proved
Computing R and U in lines 5-15 takes O(s ′(˜s
in Lemma 1 in [20]. Computing C in line 16 takes O(˜sIα Nα ). Over-
2 +nnz(R)) +s log s +nnz(X))
all, CTD-S needs O((˜sIα +s)Nα +s ′(˜s
□
operations.

Lemma 3.2 shows that CTD-S has the optimal accuracy for given

sampled fibers, and thus is more accurate than tensor-CUR.

Lemma 3.2. CTD-S is more accurate than tensor-CUR. For a given
R0 consisting of initially sampled fibers, CTD-S has the minimum
error.

Proof. CTD-S and tensor-CUR are both mode-α LR tensor de-

composition methods and have errors in the form of Equation 4.

||X − L ×α R||F = ||X(α ) − RL(α )||F
where the equality comes from Equation 3. They both sample fibers
from X in the same way in their initial step. Let R0 be the matrix
consisting of those initially sampled fibers. Assume the same R0 is
given for both methods. Then, the error is now a function of L(α ) as
shown in Equation 5. Equation 6 shows the L(α ) which minimizes
the error.

(4)

f (L(α )) = ||X(α ) − R0L(α )||F
f (L(α )) = R†
arg min
L(α )

0X(α )

(5)

(6)

We show CTD-S has the minimum error. Let R consists of the
independent columns of R0.
||X(α ) − R0R†

0X(α )||F = ||X(α ) − RR†X(α )||F
= ||X(α ) − R(RT R)−1
= ||X(α ) − RUC(α )||F
= Error of CTD-S

RT X(α )||F

(7)

The first equality in Equation 7 holds because R0R†
0X(α ) means the
projection of X(α ) onto the column space of R0, and R and R0 have
the same column space. The third equality holds because CTD-S
uses (RT R)−1 for its factor U (theorem 1 in [20]), and RT X(α ) for
its factor C. In contrast, tensor-CUR does not have the minimum
error because f (L(α )) in the Equation 5 is convex and tensor-CUR
has L(α ) which is different from R†
0X(α ). Specifically, tensor-CUR
further samples rows (called slabs) from X(α ) to construct its L(α ).
□

Figure 2: The scheme for CTD-S.

fibers in its factor R to keep result compact. CTD-S only keeps one
red fiber and one blue fiber in R in Figure 2.

CTD-S decomposes a tensor X ∈ RI1×I2×···×IN into one tensor
C ∈ RI1×···×Iα −1× ˜s×Iα +1×···×IN , and two matrices U ∈ R ˜s× ˜s and
R ∈ RIα × ˜s such that X ≈ C ×α RU. CTD-S is a mode-α LR ten-
sor decomposition method and is interpretable since R consists of
independent fibers sampled from X.

Algorithm 1 CTD-S for Static Tensor
Input: Tensor X ∈ RI1×I2×···×IN , mode α ∈ {1, · · · , N }, sample size

s ∈ {1, · · · , Nα }, and tolerance ϵ

Output: C ∈ RI1×···×Iα −1× ˜s ×Iα +1×···×IN , U ∈ R ˜s × ˜s , R ∈ RIα × ˜s
1: Let X(α ) be the mode-α matricization of X
2: Compute column distribution for i = 1, · · · , Nα :

P (i) ←

|X(α )(:, i )|2
∥X(α ) ∥2
F

1, · · · , i ′

3: Sample s columns from X(α ) based on P (i). Let I = {i1, · · · , is }
4: Let I ′ = {i ′
s′ } be a set consisting of unique elements in I
5: Initialize R ← [X(α )(:, i ′
6: for k = 2 : s ′ do
7:

1)] and U ← 1/(X(α )(:, i′

1)T X(α )(:, i ′

1))

8:
9:
10:
11:
12:

Compute the residual:
−−→
r es ← (X(α )(:, i′
k
if | |−−→

r es | | ≤ ϵ | |X(α )(:, i′
k
continue

) − RURT X(α )(:, i ′
k

))

)| | then

else

Compute: δ ← | |−−→
Update U:

r es | |2 and −→

−→
y T /δ

(cid:18)U + −→
y
U ←
−−→
y T /δ
Expand R : R ← [R, X(α )(:, i ′
k

−−→
y /δ
1/δ

(cid:19)

)]

13:
14:
end if
15: end for
16: Compute C ← X ×α RT
17: return C, U, R

y ← URT X(α )(:, i ′
k

)

Algorithm 1 shows the procedure of CTD-S. First, CTD-S com-
putes the probabilities of mode-α fibers of X, which are propor-
tional to the norm of each fiber, and then samples s fibers from X
according to the probabilities with replacement, in lines 1-3. Redun-
dant fibers exist in the sampled fibers in this step. CTD-S selects
unique fibers from the initially sampled s fibers in line 4 where s ′
denotes the number of those unique fibers. This step reduces the
number of iterations in lines 6-15 from s − 1 to s ′ − 1. R is initialized
by the first sampled fiber in line 5. In lines 6-15, CTD-S removes
redundant mode-α fibers in the sampled fibers. The matrices U
and R are computed incrementally in this step. The columns of R

4

𝝌𝑪𝑼MatricizationBiasedSamplingOriginal tensor“Check linear dependency to remove redundant fibers”𝑹(green= 0.3×red+ blue)Algorithm 2 CTD-D for Dynamic Tensor
Input: Tensor ∆X ∈ RI1×···×IN −1×1, mode α ∈ {1, · · · , N − 1}, C(t ), U(t ),

R(t ), sample size d ∈ {1, · · · , ∆Nα }, and tolerance ϵ

Output: C(t +1), U(t +1), R(t +1)
1: Let ∆X(α ) be the mode-α matricization of ∆X
2: Compute column distribution for i = 1, · · · , ∆Nα :

P (i) ←

|∆X(α )(:,i )|2
∥∆X(α ) ∥2
F

1, · · · , i ′

d ′ } be a set consisting of unique elements in I

3: Sample d columns from ∆X(α ) based on P (i). Let I = {i1, · · · , id }
4: Let I ′ = {i ′
5: Initialize R(t +1) ← R(t ), U(t +1) ← U(t ), and ∆R ← []
6: for k = 1 : d ′ do
7:
8:

Let x ← ∆X(α )(:, i′
)
k
Compute the residual:
−−→
r es ← (x − R(t +1)U(t +1)(R(t +1))T x)
if | |−−→

r es | | ≤ ϵ | |x | | then

continue

else

r es | |2 and −→

y ← U(t +1)(R(t +1))T x

Compute: δ ← | |−−→
Update U(t +1):
U(t +1) ←

(cid:18)U(t +1) + −→
y
−−→
y T /δ

−→
y T /δ

(cid:19)

−−→
y /δ
1/δ

Expand R(t +1) and ∆R : R(t +1) ← [R(t +1), x] and ∆R ← [∆R, x]

9:
10:
11:
12:
13:

14:
15:
end if
16: end for
Update C

:

(t +1)
(α )
17: if ∆R is not empty then
(t )
(α )

(cid:32)

18:

C

(t +1)
(α )

←

C
(∆R)T R(t )U(t )C

19: else
20:
C

(cid:16)

C

←

(t )
(α )

(t +1)
(α )
21: end if
22: Fold C
23: return C(t +1), U(t +1), R(t +1)

into C(t +1)

(t +1)
(α )

(R(t ))T ∆X(α )

(t )
(α )

(cid:33)

(R(t ))T ∆X(α )
(∆R)T ∆X(α )

(cid:17)

unique d ′ fibers in line 4 and initializes R(t +1), U(t +1), and ∆R with
R(t ), U(t ), and an empty matrix respectively in line 5, where ∆R con-
sists of differences between R(t ) and R(t +1). In lines 6-16, CTD-D
expands R(t +1) with those sampled fibers by sequentially evaluat-
ing linear dependency of each fiber with the column space of R(t +1).
R(t +1) and U(t +1) are updated in this step. Finally, C
is updated
in lines 17-21.

(t +1)
(α )

In the following, we describe two main ideas of CTD-D to update
(t +1)
, R(t +1), and U(t +1) efficiently while preserving accuracy:
C
(α )
exploiting factors at previous time step, and reordering operations.
(1) Exploiting factors at previous time step: First, we explain
how we update R(t +1) and U(t +1) using the idea. In line 5 of Algo-
rithm 1, CTD-S initializes R and U using one of the sampled fibers.
This is because CTD-S requires R to consist of linearly independent
columns and it is satisfied when R has only one fiber. Since R(t ) al-
ready consists of linearly independent columns, we initialize R(t +1)
and U(t +1) with R(t ) and U(t ) respectively in line 5 of Algorithm 2.
In lines 6-16, we check linear independence of each sampled fiber
from ∆X(α ) with R(t +1) . If the fiber is linearly independent, we
expand R(t +1) and update U(t +1) as in the lines 11-13 of Algorithm
1.

5

Figure 3: The scheme for CTD-D.

3.2 CTD-D for Dynamic Tensors

Overview. How can we design an efficient sampling-based dy-
namic tensor decomposition method? In a dynamic setting, a new
tensor arrives at every time step and we want to keep track of
sampling-based tensor decomposition. The main challenge is to
update factors quickly while preserving accuracy. Note that there
has been no sampling-based dynamic tensor decomposition method
in the literature. Applying CTD-S at every time step is not a feasible
option since it starts from scratch to update its factors, and thus
running time increases rapidly as tensor grows. We propose CTD-D,
the first sampling-based dynamic tensor decomposition method.
CTD-D samples mode-α fibers only from the newly arrived tensor,
and then updates the factors appropriately using those sampled
ones. The main idea of CTD-D is to update the factors of CTD-S
incrementally by (1) exploiting factors at previous time step and (2)
reordering operations.

Algorithm. Figure 3 shows the scheme for CTD-D. At each time
step, CTD-D samples fibers from newly arrived tensor and updates
factors by checking linear dependency of sampled fibers with the
factor at previous time step. Purple and green fiber are sampled
from newly arrived tensor in Figure 3. Note that the purple fiber is
added to the factor R since it is linearly independent of the fibers in
the factor at the previous time step, while the linearly dependent
green fiber is ignored.

For any time step t, CTD-D maintains its factors
dt × ˜
dt ×Iα +1×···×IN −1×t , U(t ) ∈ R ˜
C(t ) ∈ RI1×···×Iα −1× ˜
dt , and R(t ) ∈
RIα × ˜
dt such that X(t ) ≈ C(t ) ×α R(t )U(t ), where the upper sub-
script (t) indicates that the factor is at time step t. X(t ) grows
along the time mode and we assume that N -th mode is the time
mode in a dynamic setting, where N denotes the order of X(t ).
At the next time step t + 1, CTD-D receives newly arrived tensor
∆X ∈ RI1×I2×···×···×IN −1×1 and updates C(t ), U(t ), and R(t ) into
C(t +1) ∈ RI1×···×Iα −1× ˜
dt +1 ,
and R(t +1) ∈ RIα × ˜
dt +1 , respectively such that X(t +1) ≈ C(t +1) ×α
R(t +1)U(t +1).

dt +1×Iα +1×···×IN −1×(t +1), U(t +1) ∈ R ˜

dt +1× ˜

Algorithm 2 shows the procedure of CTD-D. First, CTD-D com-
putes the probabilities of mode-α fibers of ∆X, which are propor-
tional to the norm of each fiber, and then samples d fibers according
to the probabilities with replacement in lines 1-3. CTD-D selects

∆𝝌𝑪(𝒕)𝑼(𝒕)𝑪(𝒕+𝟏)𝑼(𝒕+𝟏)MatricizationBiased samplingUpdate factors using sampled fibersNewly arrived tensorTime step : 𝑡Time step : 𝑡+1Check linear dependency𝑹(𝒕)𝑹(𝒕+𝟏)Second, we describe how we update C(t +1) using the idea. We
assume that ∆R is not empty after line 16 of Algorithm 2. At time
step t and its successor step t + 1, CTD-S satisfies Equations 8
has the size
and 9, where C
˜dt +1 × N

has the size ˜dt × N

(t +1)
(α )

and C

(t )
(α )

(t )
α

(t +1)
α

.

C

C

(8)

← (R(t ))T X

(t )
(α )
(t +1)
(α )

We can rewrite R(t +1) and X

(t )
(α )
(t +1)
← (R(t +1))T X
(α )
(t +1)
as Equations 10 and 11 re-
(α )
spectively, where ∆R has the size Iα × ∆ ˜d and ∆X(α ) has the size
(t +1)
(t )
Iα × ∆Nα such that N
α
α
R(t +1) = (cid:2)R(t ) ∆R(cid:3)

+ ∆Nα and ˜dt +1 = ˜dt + ∆ ˜d.

= N

(10)

(9)

X

(t +1)
(α )

= (cid:104)

(cid:105)

(t )
X
(α ) ∆X(α )
in Equation 9 with those in Equa-

(11)

We replace R(t +1) and X

(t +1)
(α )
tions 10 and 11, respectively, to obtain the Equation 12.
(cid:20)(R(t ))T
(∆R)T

(t )
(α ) ∆X(α )
X

(t +1)
(α )

(cid:21) (cid:104)

←

C

(cid:105)

(12)

(t )
(α )
(t )
(α )

=

(t )
(α )

(t )
(α )

(R(t ))T X



(∆R)T X




(R(t ))T ∆X(α )
(∆R)T ∆X(α )

CTD-S computes all the 4 elements ((R(t ))T X







, (R(t ))T ∆X(α ),
, and (∆R)T ∆X(α )) in Equation 12 from scratch, hence
(t +1)
(α )
, R(t ),
as in the Equation

(∆R)T X
requires a lot of computations. To make computation of C
(t )
incremental, we exploit existing factors at time step t : C
(α )
and U(t ). First, we use C
8. Second, we should replace X
at time step t, since CTD-D does not have X
CTD-S. We substitute R(t )U(t )C
ensures X(t ) ≈ C(t ) ×α R(t )U(t ) which can be rewritten as X
R(t )U(t )C
(t +1)
(α )

instead of (R(t ))T X
in (∆R)T X
(t )
(α )

by Equation 3. Equation 13 shows the final form of

which is the same as line 18 in Algorithm 2.

. This is because CTD-S

as its input unlike

with the factors

(t )
(α )
(t )
(α )

for X

(t )
(α )

(t )
(α )

(t )
(α )

(t )
(α )

(t )
(α )

(t )
(α )

C

≈

C

(t +1)
(α )

←

(t )
(α )

C
(∆R)T R(t )U(t )C







and (∆R)T ∆X(α ) are ignored when ∆R is empty

(R(t ))T ∆X(α )
(∆R)T ∆X(α )

(t )
(α )

(13)








(t )
(α )

(∆R)T R(t )U(t )C
as expressed in line 20 of Algorithm 2.
(2) Reordering computations: The computation order for the
element (∆R)T R(t )U(t )C
is important since each order has a dif-
ferent computation cost. We want to determine the optimal paren-
thesization among possible parenthesizations. It can be shown that
is the optimal one with O((∆ ˜d) ˜dt (Iα + ˜dt +
(((∆R)T R(t ))U(t ))C
(t )
α )) operations and can be done by parenthesizing from the left.
We prove that CTD-D is faster than CTD-S in Lemma 3.3.

(t )
(α )

(t )
(α )

N

Table 3: Summary of the tensor data used.

Name
Hypertext 20091
Haggle2
Manufacturing emails3
Infectious4
CAIDA (synthetic)

I1

112
77
167
407
189

I2

113
274
166
410
189

I3

5,246
1,567
2,615
1,392
1,000

Nonzeros

20,818
27,972
70,523
17,298
20,511

Lemma 3.3. CTD-D is faster than CTD-S. The computational com-
+Iα )+( ˜dt +1Iα +d)∆Nα +d ′( ˜d
2
t +1+

plexity of CTD-D is O((∆ ˜d) ˜dt (N
(t )
α
nnz(R(t +1))) + d log d + nnz(∆X)).

Proof. The lines 1-4 of Algorithm 2 for CTD-D are similar
to those of Algorithm 1 for CTD-S. The only difference is that
CTD-D samples d columns from ∆X(α ) while CTD-S samples s
columns from X(α ). Thus, lines 1-4 takes O(nnz(∆X) + d∆Nα +
d log d). Updating R(t +1) and U(t +1) in lines 5-16 needs O(d ′( ˜d
2
t +1 +
nnz(R(t +1)))) operations as proved in Lemma 1 in [20]. In updat-
ing C(t +1) in lines 17-18, (R(t ))T ∆X(α ) takes computational cost of
O( ˜dt Iα ∆Nα ). (∆R)T ∆X(α ) takes O(∆ ˜dIα ∆Nα ) and (∆R)T R(t )U(t )C
takes O((∆ ˜d) ˜dt (Iα + ˜dt +N
α )). Overall, CTD-D takes O((∆ ˜d) ˜dt (N
(t )
(t )
α
Iα ) + ( ˜dt +1Iα +d)∆Nα +d ′( ˜d
t +1 +nnz(R(t +1))) +d log d +nnz(∆X)).
2
CTD-D is faster than CTD-S because CTD-S has ˜sIα Nα in its
complexity, which is much larger than all the terms in the complex-
□
ity of CTD-D.

(t )
(α )
+

4 EXPERIMENTS
In this and the next sections, we perform experiments to answer
the following questions.

Q1 : What is the performance of our static method CTD-S compared
to the competing method tensor-CUR? (Section 4.2)
Q2 : How do the performance of CTD-S and tensor-CUR change
with regard to the sample size parameter? (Section 4.2)
Q3 : What is the performance of our dynamic method CTD-D
compared to the static method CTD-S? (Section 4.3)
Q4 : What is the result of applying CTD-D for online DDoS attack
detection? (Section 5)

4.1 Experimental Settings

Measure. We define three metrics (1. relative error, 2. memory
usage, and 3. running time) as follows. First, a relative error is
defined as Equation 14. X denotes the original tensor and ˜X is
the tensor reconstructed from the factors of X. For example, ˜X =
C ×α RU in CTD-S.

RelativeError =

|| ˜X − X||2
F
||X||2
F

(14)

Second, memory usage is defined as Equation 15. Memory usage
measures the relative amount of memory for storing the resulting

1http://konect.uni-koblenz.de/networks/sociopatterns-hypertext
2http://konect.uni-koblenz.de/networks/contact
3http://konect.uni-koblenz.de/networks/radoslaw_email
4http://konect.uni-koblenz.de/networks/sociopatterns-infectious

6

factors. The denominator and numerator indicate the amount of
memory for storing the original tensor and the resulting factors,
respectively.

MemoryU saдe = nnz(C) + nnz(U) + nnz(R)

nnz(X)
Finally, running time is measured in seconds.

(15)

Data. Table 3 shows the data we used in our experiments.

Machine. All the experiments are performed on a machine with

a 10-core Intel 2.20 GHz CPU and 256 GB RAM.

Competing method. We compare our proposed method CTD with
tensor-CUR [12], the state-of-the-art sampling-based tensor decom-
position method. Both methods are implemented in MATLAB.

4.2 Performance of CTD-S
We measure the performance of CTD-S to answer Q1 and Q2. As a
result, CTD-S is 17∼83× more accurate for the same level of running
time, 5∼86× faster, and 7∼12× more memory-efficient for the same
level of error compared to tensor-CUR. CTD-S is more accurate
over various sample sizes and its running time and memory usage
are relatively constant compared to the tensor-CUR. The detail of
the experiment is as follows.

Both CTD-S and tensor-CUR takes a given tensor X, mode α, and
a sample size s as input because they are LR tensor decomposition
methods. In each experiment, we give the same input and compare
the performance. We set α = 1 and perform experiments under
various sample sizes s. We set the number of slabs to sample r = s
and the rank k = 10 in tensor-CUR, and set ϵ = 10−6 in CTD-S.

Figure 1 shows the running time vs. error and memory usage vs.
error of CTD-S compared to tensor-CUR, which are the answers for
Q1. We use sample sizes from 1 to 1000. The error of tensor-CUR is
much larger than that of CTD-S. This phenomenon coincides with
the Lemma 3.2, which guarantees that CTD-S is more accurate than
tensor-CUR theoretically. We compare running time and memory
usage under the same level of error, not under the same sample size,
because there is a huge gap between the error of CTD-S and that
of tensor-CUR under the same sample size.

Figure 4 shows the error, running time, and memory usage of
CTD-S compared to those of tensor-CUR over increasing sample
sizes s for the Haggle dataset, which are the answers for Q2. The
error of CTD-S decreases as s increases because it gets more chance
to sample important fibers which describe the original tensor well.
The running time and memory usage of CTD-S are relatively con-
stant compared to those of tensor-CUR. This is because CTD-S
keeps only the linearly independent fibers, the number of which
is bound to the rank of X(α ). There are small fluctuations in the
graphs since the sampling process of both CTD-S and tensor-CUR
are based on randomness.

4.3 Performance of CTD-D
We compare the performance of CTD-D with those of CTD-S to
answer Q3. As a result, CTD-D is 2∼3× faster for the same level of
error compared to CTD-S.

To simulate a dynamic environment, we divide a given dataset
into two parts along the time mode. We use the first 80% of the

7

dataset as historical data and the later 20% as incoming data. We
assume that historical data is already given and incoming data
arrives sequentially at every time step, such that the whole data
grows along the time mode. We measure the performance of CTD-D
and CTD-S at each time step and calculate the average. We set the
sample size d of CTD-D to be much smaller than that of CTD-S
because CTD-D samples fibers only from the increment ∆X while
CTD-S samples from the whole data X. We set d of CTD-D to be
0.01 times of s of CTD-S, α = 1, and ϵ = 10−6.

Figure 5 shows the error vs. running time and error vs. memory
usage relation of CTD-D compared to those of CTD-S. Note that
CTD-D is much faster than CTD-S though CTD-D uses the same
or slightly more memory than CTD-S does. This is because mul-
tiplication between sparse matrices used in updating C does not
always produce sparse output, thus the number of nonzero entries
in C increases slightly over time steps.

5 CTD AT WORK
In this section, we use CTD-D for mining real-world network traffic
tensor data. Our goal is to detect DDoS attacks in network traffic
data efficiently in an online fashion; detecting DDoS attacks is a
crucial task in network forensic. We propose a novel online DDoS
attack detection method based on CTD-D’s interpretability. We
show that CTD-D is one of the feasible options for online DDoS
attack detection and show how it detects DDoS attacks successfully.
In contrast to the standard PARAFAC and Tucker decomposition
methods, CTD-D can determine DDoS attacks from its decompo-
sition result without expensive overhead. We aim to dynamically
find a victim (destination host) and corresponding attackers (source
hosts) of each DDOS attack in network traffic data. The victim
receives a huge amount of traffic from a large number of attackers.
The online DDoS attack detection method based by CTD-D is
as follows. First, we apply CTD-D on network traffic data which is
a 3-mode tensor in the form of (source IP - destination IP - time).
We assume an online environment where each slab of the network
traffic data in the form of (source IP - destination IP) arrives se-
quentially at every time step. We use source IP mode as mode α.
Second, we inspect the factor R of CTD-D, which consists of actual
mode-α fibers from the original data. R is composed of important
mode-α fibers which signify major activities such as DDoS attack
or heavy traffic to the main server. Thanks to CTD, we directly
find out destination host and occurrence time of a major activity
represented in a fiber in R, by simply tracking the indices of fibers.
We regard fibers with the same destination host index represent the
same major activity, and consider the first fiber among those with
the same destination host index to be the representative of each
major activity. Then, we select fibers with the norm higher than
the average among the first fibers and suggest them as candidates
of DDoS attack. This is because DDoS attacks have much higher
norms than normal traffic does.

We generate network traffic data by injecting DDoS attacks on
the real-world CAIDA network traffic dataset. We assume that
randomly selected 20% of source hosts participate in each DDoS
attack. Table 4 shows the result of DDoS attack detection method
of CTD-D. CTD-D achieves high F1 score for various number n

(a) Relative error vs. sample size

(b) Running time vs. sample size

(c) Memory usage vs. sample size

Figure 4: Error, running time, and memory usage of CTD-S compared to those of tensor-CUR over sample size s for Haggle
dataset. CTD-S is more accurate over various sample sizes, and its running time and memory usage are relatively constant
compared to the tensor-CUR.

(a) Hypertext 2009

(b) Haggle

(c) Manufacturing emails

(d) Infectious

(e) Hypertext 2009

(f) Haggle

(g) Manufacturing emails

(h) Infectious

Figure 5: Error, running time, and memory usage relation of CTD-D compared to those of CTD-S. CTD-D is faster and has
smaller error while using the same or slightly larger memory space compared to CTD-S.

Table 4: The result of online DDoS attack detection method
based on CTD-D. CTD-D achieves high F1 score for various
n with notable precision, where n denotes the number of in-
jected DDoS attacks.

n

1
3
5
7

Recall

Precision

F1 score

1.000
1.000
0.880
0.857

1.000
1.000
1.000
1.000

1.000
1.000
0.931
0.921

6 CONCLUSION
We propose CTD, a fast, accurate, and directly interpretable tensor
decomposition method based on sampling. The static version CTD-S
is 17∼83× more accurate, 5∼86× faster, and 7∼12× more memory-
efficient compared to tensor-CUR, the state-of-the-art method. The
dynamic version CTD-D is 2∼3× faster than CTD-S for an online
environment. CTD-D is the first method providing interpretable
dynamic tensor decomposition. We show the effectiveness of CTD
for online DDoS attack detection.

of injected DDoS attacks with notable precision. We set d = 10,
ϵ = 0.15.

REFERENCES
[1] J. Allanach, H. Tu, S. Singh, P. Willett, and K. Pattipati. 2004. Detecting, tracking,
and counteracting terrorist networks via hidden Markov models. IEEE Aerospace.

8

02004006008001000Sample Size00.51Relative Error Best02004006008001000Sample Size010203040Running Time (sec) Best02004006008001000Sample Size0102030Memory Usage BestMethods :   CTD-S              Tensor-CUR00.050.10.150.2Relative Error00.10.2Running Time (sec) Best1.8x00.010.020.03Relative Error00.050.10.15Running Time (sec) Best3.4x00.050.10.150.2Relative Error00.10.20.3Running Time (sec) Best2.7x00.20.40.6Relative Error00.20.40.60.8Running Time (sec) Best3.4x00.050.10.150.2Relative Error00.511.52Memory Usage Best00.010.020.03Relative Error012Memory Usage Best00.050.10.150.2Relative Error02468Memory Usage Best00.20.40.6Relative Error00.511.52Memory Usage BestMethods :   CTD-D              CTD-S[2] A. Arulselvan, C. W. Commander, L. Elefteriadou, and P. M. Pardalos. 2009.
Detecting Critical Nodes in Sparse Graphs. Comput. Oper. Res. (July 2009), 8.
[3] C. F. Caiafa and A. Cichocki. 2010. Generalizing the column–row matrix decom-

position to multi-way arrays. Linear Algebra Appl. (2010).

[4] Q. Cao, M. Sirivianos, X. Yang, and T. Pregueiro. 2012. Aiding the Detection of

Fake Accounts in Large Scale Social Online Services. NSDI, 15–15.

[5] B. Cyganek and M. Woźniak. 2015. Tensor based representation and analysis of

the electronic healthcare record data. BIBM.

[6] P. Drineas, R Kannan, and M. Mahoney. 2006. Fast Monte Carlo algorithms for
matrices III: Computing a compressed approximate matrix decomposition. SIAM
J. Comput. (2006).

[7] P. Drineas and M. W. Mahoney. 2007. A randomized algorithm for a tensor-
based generalization of the singular value decomposition. Linear algebra and its
applications (2007).

[8] P. Drineas, M. W. Mahoney, and S Muthukrishnan. 2008. Relative-error CUR

matrix decompositions. SIAM J. Matrix Anal. Appl. (2008).

[9] R. A. Harshman. 1970. Foundations of the PARAFAC procedure: Models and

conditions for an" explanatory" multi-modal factor analysis. (1970).

[10] N. L. D. Khoa, B. Zhang, Y. Wang, W. Liu, F. Chen, S. Mustapha, and P. Runcie.
2015. On Damage Identification in Civil Structures Using Tensor Analysis. (2015).
[11] G. Kontaxis, I. Polakis, S. Ioannidis, and E. P. Markatos. 2011. Detecting social

network profile cloning. PERCOM.

[12] M. W. Mahoney, M. Maggioni, and P. Drineas. 2008. Tensor-CUR decompositions

for tensor-based data. SIAM J. Matrix Anal. Appl. (2008).

[13] I. Perros, R. Chen, R. Vuduc, and J. Sun. 2015. Sparse Hierarchical Tucker Factor-

ization and Its Application to Healthcare. ICDM.

[14] C. Phua, V. C. S. Lee, K. Smith-Miles, and R. W. Gayler. 2010. A Comprehensive

Survey of Data Mining-based Fraud Detection Research. CoRR.

[15] M. A. Prada, J. Toivola, J. Kullaa, and J. HollméN. 2012. Three-way Analysis of

Structural Health Monitoring Data. Neurocomput. (2012).

[16] J. Sun, S. Papadimitriou, and P. S. Yu. 2006. Window-based Tensor Analysis on

High-dimensional and Multi-aspect Streams. ICDM.

[17] J. Sun, D. Tao, and C. Faloutsos. 2006. Beyond streams and graphs: dynamic

tensor analysis. KDD.

[18] J. Sun, Y. Xie, H. Zhang, and C. Faloutsos. 2007. Less is More: Compact Matrix

Decomposition for Large Sparse Graphs. SDM.

[19] B. M. Thuraisingham. 2006. Data Mining for Security Applications. WISI.
[20] H. Tong, S. Papadimitriou, J. Sun, P. S. Yu, and C. Faloutsos. 2008. Colibri: fast

mining of large static and dynamic graphs. KDD.

[21] L. R. Tucker. 1966. Some mathematical notes on three-mode factor analysis.

Psychometrika (1966).

[22] Y. Wang, R. Chen, J. Ghosh, J. C. Denny, A. Kho, Y. Chen, B. A. Malin, and J. Sun.
2015. Rubik: Knowledge Guided Tensor Factorization and Completion for Health
Data Analytics. KDD.

[23] S. Zhou, X. V. Nguyen, J. Bailey, Y. Jia, and I. Davidson. 2016. Accelerating Online

CP Decompositions for Higher Order Tensors. KDD.

9

