0
2
0
2

n
a
J

1
2

]

R
C
.
s
c
[

2
v
3
7
1
3
1
.
4
0
9
1
:
v
i
X
r
a

An Argumentation-Based Reasoner to Assist
Digital Investigation and Attribution of
Cyber-Attacks

Erisa Karaﬁli, Linna Wang, and Emil C. Lupu

Department of Computing, Imperial College London
180 Queens Gate, SW7 2AZ, London, UK
{e.karafili, linna.wang15, e.c.lupu}@imperial.ac.uk

Abstract. We expect an increase in the frequency and severity of cyber-
attacks that comes along with the need for eﬃcient security countermea-
sures. The process of attributing a cyber-attack helps to construct eﬃ-
cient and targeted mitigating and preventive security measures. In this
work, we propose an argumentation-based reasoner (ABR) as a proof-
of-concept tool that can help a forensics analyst during the analysis of
forensic evidence and the attribution process. Given the evidence col-
lected from a cyber-attack, our reasoner can assist the analyst during
the investigation process, by helping him/her to analyze the evidence
and identify who performed the attack. Furthermore, it suggests to the
analyst where to focus further analyses by giving hints of the missing ev-
idence or new investigation paths to follow. ABR is the ﬁrst automatic
reasoner that can combine both technical and social evidence in the
analysis of a cyber-attack, and that can also cope with incomplete and
conﬂicting information. To illustrate how ABR can assist in the analysis
and attribution of cyber-attacks we have used examples of cyber-attacks
and their analyses as reported in publicly available reports and online
literature. We do not mean to either agree or disagree with the analyses
presented therein or reach attribution conclusions.

1

Introduction

The increase in cyber-attacks we are currently facing [33] is expected to continue,
especially given the exponential increase in the usage of IoT and smart devices,
which drastically increases the attack surface of systems. The increasing depen-
dency users have on these connected devices raises the users’ exposure to cyber-
attacks. The growth in frequency and severity of cyber-attacks comes along with
the increased economic costs associated to the damages caused by such cyber-
attacks [16]. Existing protective and mitigating measures are not suﬃcient to
cope with the sophistication of current attacks. This brings the need to enforce
eﬃcient preventive and mitigating measures that are attacker-oriented, i.e., coun-
termeasures that are speciﬁc to the attacker or group of attackers performing
the attack. Furthermore, discovering who performed an attack and bringing the
perpetrators to justice, can act as a deterrent for future cyber-attacks.

 
 
 
 
 
 
Attacker-oriented countermeasures require to discover the perpetrator of the
attack or the entity related to it. Attribution is the process of assigning an action
of a cyber-attack to a particular entity/attacker/group of attackers. Currently,
the attribution of cyber-attacks is mainly a manual process, performed by the
forensic analyst, and is strictly related to the knowledge of the analyst, thus, is
easily human biased and error-prone. Attributing cyber-attacks is not trivial, as
attackers often use deceptive and anti-forensics techniques [18], and the analysts
need to analyze an enormous amount of data, ﬁlter [23,38] and classify them.
The increasing use of IoT devices aggravates the work of the analysts and makes
the attribution process more expensive, as the analysts might need to physically
access the devices to retrieve their data.

Digital forensics helps during the attribution process, as it collects and ana-
lyzes the evidence left by the attack, but it is not able to deal with conﬂicting
or incomplete information. It only works with technical evidence, and fails to
consider other aspects such as geopolitical situations and social-cultural contexts
that provide useful leads during an investigation. Digital forensics tools mainly
focus on collecting the evidence, which is then given to the analyst for analysis.
This makes the process often extremely human-intensive, requiring many skilled
analysts to work for weeks or even months [31,45]. The problem is aggravated by
the large proportion of unstructured data, which makes the automated analysis
challenging.

In this work, we propose an automatic reasoner (ABR), based on argumenta-
tion and abductive reasoning that helps the forensic analyst during the evidence
analysis and attribution process. Given the pieces of cyber forensic and social
evidence of a cyber-attack, the proposed reasoner analyzes them and derives
new information that is provided to the analyst. In particular, ABR can answer
queries, such as, who is a possible perpetrator of an attack, who has the motives
to perform it, what are the capabilities needed to perform an attack or what are
the similarities with past attacks. Furthermore, ABR can suggest to the analyst
other paths of investigation, by giving hints on what other pieces of evidence can
be collected to arrive at a conclusion thus, enabling a prioritized evidence col-
lection. Our reasoner is based on our preliminary work [25,24] where we brieﬂy
presented the main intuition behind ABR. To the best of our knowledge, this is
the ﬁrst automatic reasoner that helps with the analysis of cyber-attacks using
both technical and social evidence, and that is able to reason with conﬂicting
and incomplete knowledge.

The reasoner uses a set of reasoning rules, preferences between them, and
background knowledge. The rules of the reasoner are constructed from the input
provided by the expert user and from the analyses of past attacks. To illus-
trate ABR, we have used in this paper rules extrapolated from the analysis of
well-known cyber-attacks as published in the public literature (e.g., APT1 [27],
Wannacry [32]). In particular, the rules were generalised so that they can be ap-
plied across diﬀerent attack scenarios. The background knowledge incorporates
typical common knowledge that analysts may use during the analysis process.
In this work, we have used some of the knowledge extracted from the examples,

2

as well as other public reports such as [19,7,8]. Our reasoner can assist in the at-
tribution of an attack by using both technical evidence and social considerations
that are represented thanks to the use of a social model [39].

ABR is able to work with incomplete and conﬂicting evidence. We decided
to base ABR on an argumentation framework, in particular, a preference-based
argumentation framework [22], which permits to reason with conﬂicting pieces of
evidence by introducing preferences between the applied rules. We use preference-
based argumentation as it is similar to the decision-making process followed by
digital forensics investigators. ABR is constructed using the Gorgias [20] tool,
which uses abductive reasoning [21] combined with preference-based argumen-
tation. The use of abduction allows us to reach conclusions even with incomplete
information, as the missing information is abduced (hypothesized) and then sug-
gested to the analyst as hints of possible further evidence to be collected.

ABR is a proof-of-concept tool that aims to assist the analyst during the
analysis process. Therefore, together with the answer to a query it also provides
the explanation of the reasoning process, applied rules and the information used
to reach that conclusion. Furthermore, ABR gives hints to the analyst for missing
evidence, that, if provided, allows to pursue other investigation paths. ABR is
ﬂexible and adaptable to user requests and changes. The use of ABR helps to
promote best practices and to share lessons learned from past experience as
rules and background knowledge can be constructed with expert input and then
shared and re-used across investigations.

In Section 2 we present the relevant related work. We introduce our argumentation-

based reasoner (ABR) in Section 3. In Section 4 and 5 we present ABR’s main
components, correspondingly its reasoning rules and its background knowledge.
We give an overall evaluation and discussion in Section 6. In Section 7 we con-
clude and present some interesting future research directions.

2 Related Work

Attribution of a cyber-attack is the process of “determining the identity or loca-
tion of an attacker or attackers intermediary” [46]. Tracing the origin of a cyber-
attack is diﬃcult as attackers can easily forge or obscure information sources,
and use anti-forensics tools, to avoid being detected and identiﬁed [18]. Digital
forensics plays a signiﬁcant role in attribution by collecting, examining, analyz-
ing and reporting the evidence [26]. Other techniques created for protecting the
systems are also used to collect forensic data, e.g., traceback techniques [46],
honeypots [4], or other deception techniques [1,2,44].

Digital forensics comes with its own challenges [6], that can mainly be cat-
egorised into: complexity problems as the collected data are in the lowest raw
format and require high resources to analyze them and quantity problems as the
enormous amount of collected data is too large to be analyzed manually [9].
Forensics techniques identify and collect the evidence that is later managed and
analyzed by the forensic analyst. Since often the data are collected from diﬀer-
ent sources and the attackers can plant false evidence to lead the investigator

3

oﬀ his/her trail, the latter is likely to be in a situation with multiple pieces of
conﬂicting evidence. Digital forensics techniques can deal with conﬂicting infor-
mation during the evidence collection phase [5,15], but lack the ability to work
with conﬂicting pieces of evidence during the analysis and attribution process.
These techniques can collect pieces of evidence [42], but have diﬃculties reason-
ing with incomplete information, and reaching conclusions without having all the
needed pieces of evidence. Digital forensics only uses technical evidence [45] and
fails to consider other factors such as geopolitical situations and social-cultural
contexts, which could provide useful leads during the investigations.

A theoretical social science model is proposed in [39], called the Q-Model
that describes how the analysts combine technical and social evidence during
the attribution process. In this model, attribution is described as an incremental
process passing from one level of attribution to the other. The Q-Model repre-
sents how the forensic investigators perform the attribution process and partic-
ular attention is placed on the social evidence, where contextual knowledge such
as ongoing conﬂicts between countries or rivalry between corporations are very
useful in detecting motives of potential culprits.

We decided to use argumentation for our reasoner, as argumentation helps
during the analysis and attribution process because it is transparent and encour-
ages the evaluation of the arguments, by assessing the relative importance of vari-
ous factors when making decisions [35]. Argumentation captures the fact that the
ﬁnal decision might change if more information is available (i.e., non-monotonic
reasoning [12]), where more information may reveal new arguments that are
in conﬂict with the original ones and are stronger than them. Non-monotonic
reasoning has previously been proposed to tackle the attribution challenge. For
example, in [34,43] the authors propose the DeLP3E framework to attribute op-
erations of cyber-attacks. This theoretical framework is based on the extension
of Defeasible Logic Programming with probabilistic uncertainty. The DeLP3E
framework does not deal with incomplete evidence, and thus, cannot make as-
sumptions to reach a conclusion and cannot suggest new paths of investigation
or new evidence to be collected. It also lacks in general technical and social com-
mon knowledge, e.g., ongoing conﬂicts/rivalries between countries/corporations,
information about past attacks, cyber-security capabilities of entities, which can
be very useful in detecting motives, capabilities and potential culprits. DeLP3E
uses as a measure for its conclusion the probabilities of an event being true.
However, this requires the user to provide the probability of being true for each
of the given pieces of evidence. It also does not distinguish the diﬀerent levels of
reasoning that can be applied to reach certain conclusions.

Despite the advances in using digital forensics or defeasible reasoning in at-
tribution, some shortcomings still remain to be addressed. The most important
one is that none of the current works considers the social aspects of attribution.
The current state of the art does not deal with incomplete evidence, which is an
important aspect of forensic investigations, as usually not all evidence can be col-
lected due to time/resource constraints, and anti-forensics tools used by attackers
can hide some of the evidence. We believe, our reasoner is the ﬁrst attempt to

4

use a social model to categorize evidence and rules in an argumentation-based
framework, which leads to a more accurate and explainable attribution that
helps the investigator during the analysis process also in case of conﬂicting and
incomplete evidence.

3 Argumentation-Based Reasoner for Attribution

Let us now introduce our argumentation-based reasoner (ABR) that is based
on a preference-based argumentation framework. ABR is composed of two main
components the reasoning rules, and the background knowledge, see Figure 1.
Given the evidence presented in input, ABR analyzes it and attempts to answer
queries about the possible perpetrators of the attack, or provides suggestions
for further pieces of evidence needed to reach a conclusion or perform a more
precise or a diﬀerent analysis. The reasoning rules used by ABR were extracted1
from public reports about past cyber-attacks and formalized in the argumen-
tation framework2. In actual use, rules could be speciﬁed by expert users or
extracted automatically from diﬀerent analyses and then reviewed by expert an-
alysts. Rules are divided into three layers: technical, operational and strategic
layer, following the social model structure proposed in [39]. In this paper, we
have used background knowledge based on the information extracted from on-
line analyses of past cyber-attacks and relevant information for these attacks.
ABR takes as input from the user the pieces of evidence (technical and social
evidence) relevant to the current investigation and then analyzes them by using
the reasoning rules and the background knowledge. It gives as result to the user
answers to the user’s queries, e.g., if a given entity is a possible culprit of the
attack, together with an explanation on how the conclusion was reached, hints
about what other pieces of evidence the user can provide to perform a more
precise or a new analysis.

3.1 Argumentation Framework for Attribution

We base our reasoner on a preference-based argumentation framework [22,20], as
it permits the user to take decisions while working with conﬂicting evidence, and
it naturally encodes the diﬀerent reasoning layers with its preference relations
between rules. The used framework best simulates the analysis and attribution
process made by an investigator, who needs to use diﬀerent reasoning rules that
work with technical and social aspects of the attack, have exceptions, and can
derive conﬂicting conclusions.

Our framework allows the investigator to work with conﬂicting evidence and
reasoning rules that derive conﬂicting conclusions, by introducing preferences
between them. The introduced preferences can be considered as exceptions to

1 Currently the extraction of the rules is done manually by analyzing various reports

and articles about the analysis and attribution of past cyber-attacks.

2 The rules extracted have not been evaluated for correctness and might not be com-
plete i.e., they might not capture the complexity of the situations encountered.

5

Fig. 1. ABR Overview

other rules, or preferences that are context dependent. The use of argumentation
permits to provide an explanation of the given results. Let us brieﬂy introduce
the used framework.

An argumentation theory is a pair (T , P) of argument rules T and preference

rules P. The argument rules T are a set of labeled formulas of the form:

rulei : L ← L1, . . . , Ln

where L, L1, . . . , Ln are positive or negative ground literals, and rulei is the label
denoting the rule name. In the above argument rule, L denotes the conclusion
of the argument rule and L1, . . . , Ln denote its premises. The premise of an
argument rule is the set of conditions required for the conclusion to be true. In
our framework, the argument rules are the reasoning rules used by ABR. Let us
show below a reasoning rule that is part of ABR:

str1 : isCulprit(C, Att) ← ClaimResp(C, Att)

where the rule name is the label of the rule, in this case str1; the head is
the second argument and represents the conclusion of the rule, in this case
isCulprit(C, Att); the body predicates are the literals following the head, and
represent the premises of the rule, in this case ClaimResp(C, Att).

The preference rules P are a set of labelled formulas of the form:

pi : rule1 > rule2

where pi is the label denoting the rule name, the head of the rule is rule1 > rule2,
and rule1, rule2 are labels of rules deﬁned in T , and > refers to an irreﬂexive,
transitive and antisymmetric higher priority relation between rules. The above

6

Investors  Technical  Layer Reasoning Rulesimport  OperationalLayer   Strategic Layer importimportimportimportimportUserABR InputABR OutputABRDomain-SpeciﬁcKnowledgeGeneral KnowledgeBackground Knowledgerule means that rule1 has higher priority than rule2, or better rule1 is preferred
over rule2. The preference rules, also called priority rules are true always or in
certain conditions or contexts. We show below a priority rule, p1, denoting that
rule str2 is preferred over rule str1.

p1 : str2 > str1

We have priority rules between rules that are in conﬂict with each other or
better that derive conﬂicting conclusions. Preference-based argumentation allows
the investigator to handle non-monotonic reasoning [12] in attribution, where
the introduction of new evidence might change the result of the attribution
(due to conﬂicting arguments) and the investigator’s conﬁdence in the results.
Argumentation is particularly useful as it permits to represent the reasoning
rules in an intuitive and simple way.

Let us introduce the following rule that is part of ABR:

str2 : ¬isCulprit(X, Att) ← ¬hasCap(X, Att).

Rule str2 describes that entity X is not the possible culprit for the attack Att,
because it does not have the capabilities for performing it. Rule str1 and str2 are
in conﬂict with each other because when both preconditions are met, they derive
conﬂicting conclusions. Given the above preference rule p1, rule str2 is preferred
over rule str1. Thus, in case both preconditions for str1 and str2 are given, we
take into consideration only the conclusion from str2 , ¬isCulprit(C, Att).

The inputs of ABR are pieces of evidence that are used together with the
background knowledge by the reasoning rules to derive new information. The
reasoning rules and the preferences used in this paper were extracted from real
cyber-attacks analyses and attribution taken from online public reports, such
as [27,32].

ABR is the ﬁrst tool that is able to work with incomplete evidence. It provides
hints of missing evidence or new investigation paths to the user, thanks to the use
of abductive reasoning [21]. The use of abducible predicates permits to ﬁll the
knowledge gaps in the reasoning, by allowing ABR to perform the analysis and
to reach a conclusion even when there are insuﬃcient pieces of evidence. This
feature is extremely important to the investigator who is provided with new
possible conclusions and new evidence to be collected. To construct ABR we use
the Gorgias [20] tool, which is a preference-based argumentation reasoning tool
that uses abduction.

Let us now introduce the following rule from ABR:

op1 : hasM otive(X, Att) ← target(T, Att), industry(T ),

hasEconM ot(X, T ),
contextOf Att(econ, Att),
specif icT arget(Att).

which states that X has the motives to perform attack Att, when it has econom-
ical motives against the target T of Att, where T is an industrial company, the

7

context of Att was economical (econ), and Att had a speciﬁc target. ABR treats
specif icT arget as an abducible predicate. For every abducible predicate we have
the rules that derive the predicate or its negation. For the specif icT arget ab-
ducible we can prove that it is not true by using the following rule.

op2 : ¬specif icT arget(Att) ← target(T1, Att),

target(T2, Att), T1 (cid:54)= T2.

In case, we are not able to derive ¬specif icT arget(Att), then we can abduce
(hypothesize) that specif icT arget(Att) is true, and we can use this result to
derive hasM otive(X, Att), in case we have the rest of the preconditions.

3.2 Technical and Social Attribution

The main goal of the ABR reasoner is to assist the forensic analyst during
the evidence analysis. Given the pieces of evidence of an attack, the reasoner
analyzes the evidence and derives new information, if possible attributes this
attack to one or diﬀerent possible entities, or provides suggestions on other
pieces of evidence that the user can provide to better analyze and attribute the
attack. To perform the attribution process, ABR also needs to work with non-
technical evidence, usually called social evidence. To deal with these aspects, we
have used a social model for attribution, called the Q-Model [39]. This model
represents how the investigators perform the attribution process of cyber-attacks.
Following the Q-Model, we categorize the evidence and the reasoning rules into
three layers: technical, operational and strategic. The combination of information
in these layers permits the attribution of a cyber-attack, as it aims to emulate
the investigator’s attribution process. Depending on the layer a rule/evidence is
part of, we call it a technical, operational, or strategic rule/evidence and denote
its name starting correspondingly with t, op, or str.

The technical layer is composed of rules that deal with pieces of evidence
obtained from digital forensics processes, related to technical evidence of the
attack, and how it was carried out, e.g., the IP address from which the attack
was originated, time of the attack, logs, type of attack, code used. Let us give
below an example of a technical layer reasoning rule that is part of ABR:

t1 : reqHighRes(Att) ← usesZeroDay(Att).

Rule t1 denotes that if the attack Att uses zero-day vulnerabilities, usesZeroDay(Att),
then this attack requires a lot of resources, reqHighRes(Att).

The operational layer is composed of rules that deal with non-technical pieces
of evidence that relate to the social aspects where the attack took place, e.g.,
the motives of the attack, the needed capabilities to perform it, the political or
economical context where it took place. Let us give below an operational layer
reasoning rule that is part of ABR:

op3 : hasCap(X, Att) ← reqHighRes(Att),
hasResources(X).

8

Rule op3 denotes that if Att requires a large amount of resources, and an entity X
has (large amounts of) resources, hasResources(X), then X has the capability
to carry out the attack, hasCap(X, Att).

The strategic layer is composed of rules that deal with who performed the
attack, or who is obtaining advantage from it. Let us give below a strategic layer
reasoning rule that is part of ABR:

str3 : isCulprit(X, Att) ← hasM otive(X, Att),

hasCap(X, Att).

Rule str3 denotes that if X has both the capability, hasCap(X, Att), and the
motive, hasM otive(X, Att), to carry out the attack Att, then X is a possible
culprit of the attack, isCulprit(X, Att).

As shown in Figure 1, the operational rules use information derived from
the technical layer, and the strategic rules use information derived from the
technical and operational layers. All three layers use the evidence given by the
user and the background knowledge. This categorization of the evidence and
rules in three layers, following from the Q-Model, aims to emulate the forensic
investigator’s analysis during the attribution process, where s/he moves from the
technical layer, to the operational, and ﬁnally to the strategic one, by using the
conclusions from the previous layers. Furthermore, this categorization improves
ABR’s usability, given the investigator’s familiarity with these three layers.

4 ABR’s Reasoning Rules

To illustrate the use of ABR we have extracted around 200 reasoning rules
from the analyses of diﬀerent cyber-attacks reported in the public literature
(e.g., APT1 [27] and Wannacry [32]). These rules have then been translated into
generic argumentation rules to be used within the framework. These reasoning
rules are considered as one of the main components of ABR as they permit to
perform the reasoning behind the analysis and attribution of cyber-attacks. We
brieﬂy present some of these rules in this section.

As described in the previous section, the reasoning rules, also called simply
rules, are divided into three layers: technical, operational and strategic. Let us
give an overview of some of the strategic rules of the reasoner and show how the
rules of the diﬀerent layers are related to each other. The following rules describe
some of the circumstances in which we can derive that an entity X is a possible
culprit (isCulprit(X, Att)) or not (¬isCulprit(X, Att)) of an attack Att.

9

str3 : isCulprit(X, Att) ← hasM otive(X, Att),

hasCap(X, Att).

str4 : isCulprit(X, Att) ← malwareU sed(M 1, Att),

similar(M 1, M 2),
notBlackM arket(M 1),
notBlackM arket(M 2),
malwareLinked(M 2, X).

str5 : ¬isCulprit(X, Att) ← ¬attackOrig(X, Att).
str6 : ¬isCulprit(X, Att) ← target(X, Att).

Let us use the strategic rule str3, to show the relations of the reasoning rules be-
tween the diﬀerent layers. Rule str3 uses the predicates hasM otive(X, Att) and
hasCap(X, Att); where the ﬁrst is a derived predicate of the operational layer,
indicating that X has motives to perform the attack Att and hasCap(X, Att) is
a derived predicate of the technical and operational layer, indicating that entity
X has the capabilities to perform Att.

The hasM otive predicate can be derived using the rule introduced in Sec-

tion 3.1, represented as below:

op1 : hasM otive(X, Att) ← target(T, Att), industry(T ),

hasEconM ot(X, T ),
contextOf Att(econ, Att),
specif icT arget(Att).

The above rule says that an entity X has the motives to perform Att, when X has
economical motives to attack a particular entity T , which is an industry, and the
attack was designed to target entity T , and the context of Att was economical.
The predicates used in op1 are: target(T, Att) is an evidence, stating that T is
the target of Att; industry(T ) is a background fact, stating that T is an industry;
hasEconM ot(X, T ) is an evidence, stating that entity X beneﬁts economically
from attacking industry T , (for example, if countryC has identiﬁed industryY
as a strategic industry, we say that hasEconM ot(countryC, industryY ) is true);
specif icT arget(Att) is an evidence that is true when Att was constructed to at-
tack a particular target; contextOf Att(Y, Att) is an evidence stating that: if the
target of an attack was a “normal”3 industry, then the context was economical
(econ), if the target was a “political” industry, then the context was political
(pol).

We introduce below one of the rules that derives the hasCap predicate4.

op3 : hasCap(X, Att) ← reqHighRes(Att),
hasResources(X).

3 “Normal” industries are companies that are not closely related to a country’s national
interests. A “political” industry is a company that is closely related to a country’s
national interests, e.g., the defence or energy sector.

4 Numerous factors can be used to determine the capability. However, for the sake
of space, we introduce only one of the possible rules that can derive the capability
(hasCap) predicate.

10

Rule op3 states that X has the capability to perform Att, when Att requires high
resources and X has the needed resources. Predicate reqHighRes(Att) can be
derived from the following technical rules:

t2 : reqHighRes(Att) ← target(T, Att), highSecurity(T ).
t3 : reqHighRes(Att) ← highV olAtt(Att), longDurAtt(Att).
t4 : reqHighRes(Att) ← highLevelSkill(Att).

where highSecurity(T ) means that entity T has high security measures in place;
highV olAtt(Att) means that Att has a high volume; longDurAtt(Att) means
that Att was performed over a long duration (few months or even years), and
highLevelSkill means that Att is a complex attack and requires high level skills
to be performed. Rule t2 states that Att requires high resources if its target
has put in place high security measures, rule t3 states that Att requires high
resources if the attack has a high volume and a long duration, and rule t4 states
that Att requires high resources if it requires advanced skills.

ABR’s rules are used to analyze the evidence and to derive new conclusions,

in order to oﬀer new insights to the analyst, as shown in the example below.

Example 1. Let us consider the example of the US bank hack [17], that occurred
in 2012. During this attack, US banks faced denial of service (DoS) attacks,
causing websites of many banks to suﬀer slowdowns and even be unreachable
for many customers. The banks’ web hosting services were infected by a sophis-
ticated malware called Itsoknoproblembro, (denoted as itsOKnp). Earlier that
year, US government placed economic sanctions against Iran. Some of the pieces
of evidence provided to ABR for this attack (usBHack) are as below:

target(us banks, usBHack).
targetCountry(usa, usBHack).
attackP eriod(usBHack, [2012, 9]).
highLevelSkill(usBHack).
malwareU sed(itsOKnp, usBHack).
imposedSanc(usa, iran, [2012, 2]).

By using rule t4 and the evidence that this attack required a high level of skill,
ABR derives that this attack requires high resources, reqHighRes(usBHack).
Another rule capturing that an entity might have a political motive if it has

been the target of sanctions can be written as follows:

op4 : hasP olM otive(C, T, Date) ← imposedSanc(T, C, Date).

Rule op4 would then derive5 that Iran might have political motives against US
because of the sanctions imposed by US against Iran [17], hasP olM otive(iran, us, [2012, 2]).

(cid:50)

5 ABR’s derived evidence is derived from the application of the rules to the input

evidence and thus depends on both rules and evidence being correct.

11

5 ABR’s Background Knowledge

ABR uses background knowledge comprising non-case-speciﬁc information and
divided into general knowledge and domain-speciﬁc knowledge. Some of the back-
ground knowledge predicates used in this paper are shown in Table 1. The use of
the background knowledge alleviates the analysts’ work and helps avoid human
errors and bias. It comprises of pieces of information that are used as precon-
ditions by the reasoning rules to answer the users’ queries. ABR’s background
knowledge can be updated and enriched by the user. Note that reaching meaning-
ful conclusions through the application of the rules to the background knowledge
relies on the correctness of the background knowledge given.

Explanation
Type predicate for industries
Political industries
Non political industries
Type predicate for countries
List of cyber superpowers

Predicate example
industry(infocomm)
polIndustry(military)
norIndustry(infocomm)
country(united states)
cybersuperpower(united states)
gci tier(afghanistan,initiating)
gci tier(poland, maturing)
gci tier(russian federation,leading)
ﬁrstLanguage(english, united states)
goodRelation(united states, australia)
poorRelation(united states, north korea)
prominentGroup(fancyBear)
groupOrigin(fancyBear, russian federation) Country of origin of a group
pastTargets(fancyBear, [france,...,poland]) Past targets of a hacker group
malwareLinked(trojanMiniduke,cozyBear) Past attribution of malware
malwareUsedInAttack(ﬂame, ﬂameattack)
ccServer(gowin7, ﬂame)
domainRegisteredDetails(gowin7,
adolph dybevek, prinsen gate 6)

First language used in the country
Good relations between countries
Poor relations between countries
Prominent hacker groups

Global Cybersecurity Index (GCI)

C&C servers of malware

Dom. registration details of C&C servers

Table 1. Some of ABR’s background knowledge, divided into: general knowledge (yel-
low) and domain-speciﬁc knowledge (orange)

5.1 General Knowledge

The general knowledge consists of information about countries characteristics, in-
ternational relations between nations, and classiﬁcation of the types of industry.
This information is used together with the given pieces of evidence, to perform
the analysis. Below we illustrate how these predicates are used by ABR’s rules.
Language indicators in malware can provide useful clues regarding the pos-
sible origin of attacks. We use two language artifacts: default system language
settings, sysLang, and language used in code, langInCode. We present below

12

two rules of ABR, t5 and t6, that use the language evidence to derive the possible
origin of the attack attackP Orig, when the country’s ﬁrst language f irstLang
matches the one found in the system/code.

t5 : attackP Orig(X, Att) ← sysLang(L, Att),
f irstLang(L, X).

t6 : attackP Orig(X, Att) ← langInCode(L, Att),

f irstLang(L, X).

The cyber capability of a nation is another interesting information as it limits
the type of attacks that an entity can carry out. We have used the Global
Cybersecurity Index (GCI) Group [19] and the cyber capabilities of countries in
cyberwar [7] as sources for this information. There are three GCI groups: leading,
maturing and initiating, from where we classify the countries according to their
capabilities. Furthermore, based on the cyber capabilities in cyberwar [7] we
identify some countries as cyber “superpower ”. We show below three of ABR’s
rules that use the countries’ cyber capability.

t7 : hasResources(X) ← gci tier(X, leading).
t8 : hasResources(X) ← cybersuperpower(X).
t9 : hasN oResources(X) ← gci tier(X, initiating).

A country hasResources if it is in the ‘leading’ GCI group or is a cyber “super-
power ”. Countries in the ‘initiating’ GCI group are considered as hasN oResources.

Example 2. Let us continue with the usBHack introduced in Example 1. In
the background knowledge, we have that Iran is a cyber “superpower ”6. Thus,
using rule t8 ABR derives that Iran has the resources to carry out sophisticated
attacks, hasResources(iran). Similarly the application of the operational rule
op3 derives that Iran has the capabilities7 to perform the US bank hack, as shown
below.

op3 : hasCap(iran, usBHack) ← reqHighRes(usBHack),

hasResources(iran).

(cid:50)

Good international relations between two countries can indicate that a state-
sponsored attack is unlikely. We encoded this information in ABR by creating
a list of countries that have good relations with each other (goodRelation) and
a list of countries that may have poor relations with each other (poorRelation)
according to [47,8]. This information can then be used to narrow down the

6 Iran is mentioned as a “notable player” in [7]. Thus, we identify it as a cyber “su-

perpower”.

7 Note that we have a list of countries that have the resources to perform the attack,
given their cyber capabilities. Rule op3 is applied to all these countries. For the sake
of simplicity, we only show the entities that are of interest to the discussed example.

13

countries that might have or not a motive to carry out an attack, as shown by
the following rule.

op5 : ¬hasM otive(C, Att) ← targetC(T, Att), country(T ),

country(C), goodRelation(C, T ).

Rule op5 derives that country C does not have any motive to perform Att,
as it has good relations with T that is the country that Att has as target,
(targetC(T, Att)).

5.2 Domain-Speciﬁc Knowledge

Domain-speciﬁc knowledge consists of information about prominent groups of at-
tackers and past attacks. These facts are primarily used in the strategic and tech-
nical layers. We encoded information on prominent APT groups taken from [14,29],
where for each group we have their: name or ID; country of origin; countries/or-
ganisations targeted by the group in the past; malware or pieces of malicious
software (suspected or conﬁrmed) linked to the group, as well as relations of
this group with other entities (e.g., governments). We assume these groups have
the capabilities of conducting long term and signiﬁcant attacks. Thus, we derive
that an entity X has the capabilities to perform an attack, if X is a prominent
group of attackers, as shown by the rule below op6.

op6 : hasCap(X, Att) ← prominentGroup(X).

Another important part of the domain-speciﬁc knowledge is the similarity
with past attacks. For example, similarity to an APT-linked malware may in-
dicate that the same APT group may be responsible. This is presented in rule
str4.

str4 : isCulprit(X, A1) ← malwareU sed(M 1, A1),

similar(M 1, M 2),
malwareLinked(M 2, X),
notBlackM arket(M 1),
notBlackM arket(M 2).

In rule str4, we derive that the attacker of Att is most likely entity X, because the
malware used is similar to another malware linked to X, and both malware codes
were not found on the black market (notBlackM arket)8. We use the predicate
similar(M 1, M 2) to denote that two malwares are similar to each other. In
the rules below we deﬁne that M 1 and M 2 are similar if they use a similar
code obfuscation (similarCodeObf ) mechanism, or they share code, or M 1 is
derived by modifying M 2, or they have similar command and communication
(C&C) servers9, where the similarity of C&C servers of two diﬀerent malwares

8 Currently, ABR is not able to derive the evidence notBlackM arket. This evidence

is either provided by the user or given in the background knowledge.

9 For the sake of simplicity, in this paper we introduce only a subset of ABR’s rules

that identify similarities between malwares or malicious software.

14

can be derived by using other ABR’s technical rules.

t10 : similar(M 1, M 2) ← similarCodeObf (M 1, M 2).
t11 : similar(M 1, M 2) ← sharedCode(M 1, M 2).
t12 : similar(M 1, M 2) ← modif iedF rom(M 1, M 2).
t13 : similar(M 1, M 2) ← similarCCServer(M 1, M 2).

6 Evaluation and Discussion

ABR aims to be a ﬂexible tool designed to be part of an iterative process, where
the user can add other pieces of evidence, rules or preferences after evaluating the
answers produced by the tool. ABR’s input is given manually by the analyst,
or could be collected, in part, automatically through an automatic extraction
process by using digital forensics tools.

6.1 Evaluation

We have tested ABR’s performance and usability using examples of cyber-
attacks published in the online literature. During the evaluation, ABR used the
reasoning rules correctly to identify possible attackers. The explanations pro-
vided, in the textual and the graphical representations, helped to improve the
usage of ABR as they provided information that was used by the user for the
next iterations. ABR answered the queries requested (e.g., if a country had the
motives or capabilities to perform the attack, or if a particular group of attack-
ers could be related to the attack following the technical evidence of the used
malware) as expected, given the pieces of evidence provided as input. Note that
ABR assumes that the input evidence is correct, and providing inaccurate or
incomplete information may lead to incorrect conclusions.

For every tested example, we ran ABR using a subset of the input evidence.
Depending on the use-case and the provided evidence, ABR was able to reach
some conclusions by abducing (hypothesizing) some of the missing predicates.
ABR gave interesting results when asked to provide suggestions for missing ev-
idence, as it proposed useful missing evidence and also new (not predicted)
investigation paths. When a signiﬁcant part of the evidence was provided to
ABR, its results coincided with those in the publicly available analyses or the
entity attributed in the publicly available analyses was contained in ABR’s list
of possible culprits, which also contained other possibilities.

Let us now brieﬂy introduce some of the cyber-attacks used to evaluate ABR
and its conclusions. For the sake of space, we decided to show some well-known
attacks where ABR was tested, as they do not need a detailed introduction.

ABR analyzed evidence of the Stuxnet attack [48,30] and derived two dif-
ferent entities as possible culprits: US and Israel. The Stuxnet attack was ﬁrst
discovered in 2010 at the uranium enrichment plant in Iran. The code used was
complex, using four zero-day vulnerabilities and mainly targeted Iran. ABR ex-
plained the conclusion based on the high resources required to perform such a

15

sophisticated attack, and the political conﬂicts that existed in that period be-
tween Iran and US, and between Iran and Israel. In this case, the social evidence
provided to ABR mainly included the political conﬂicts between Iran and these
two countries. However, ABR would have listed as possible source of the attacks
any entity for which it could derive a motive and that had the resources to
perform such sophisticated attacks.

ABR analyzed evidence of the Sony Pictures attack [40]. The Sony Pictures
attack represents the 2014 attack when hackers inﬁltrated Sony’s computers and
stole data from Sony’s servers. A group called “Guardians of Peace” claimed
credit for the attack, but several US government organisations claimed that
the attack was state-sponsored by North Korea [10,11,40]. ABR attributed this
attack to three possible culprits: the attackers group called “Guardians of Peace”
and to two countries, Iran and North Korea. The attribution to Iran came as
a consequence of low diplomatic relations between US and Iran. ABR’s results
were unexpected, with respect to the attribution given in [10,11,40]. In this case
we see that ABR can suggest new possible paths of investigation.

ABR analyzed evidence from the Conﬁcker [41] attack and, using the ev-
idence provided, was not able to reach a result. Some of ABR’s suggestions
were to ﬁnd entities that operated/worked in Ukraine or that had interest in
Ukraine, as the ﬁrst version of the attack was constructed to avoid machines
with Ukrainian keyboards [37], thus to avoid a speciﬁc country (Ukraine). ABR
suggested also to ﬁnd evidence about political or economical motivations for
the attack, as the attack was sophisticated and it could either be a nation-state
attack or performed by a cyber-criminal organization.

Example 3. Let us now show ABR’s ﬁnal steps of the analysis and attribution
of the usBHack. Following from Example 1 and 2, ABR derived the follow-
ing predicates: hasCap (iran, usBHack) and hasP olM otive(iran, us, [2012, 2]).
ABR can now apply the following operational rule:

op7 : hasM otive(C, Att) ← targetCountry(T, Att),

attackP eriod(Att, Date1),
hasP olM otive(C, T, Date2),
specif icT arget(Att)
dateApplicable(Date1, Date2).

Rule op7 permits ABR to derive that Iran has motives to perform the attack,
as it has political motives. Furthermore, these motives are applicable for the
attack, as they occurred less than 1 year before the attack took place. By ap-
plying rule str3, ABR derives that Iran is a possible culprit for this attack
(isCulprit(iran, usBHack)). This result is in line with the attribution reported
in [28], while it does not match another attribution reported in [36], which at-
tributed the attack to a group of hackers. ABR provides its conclusion to the
analyst together with its derivation tree with all the rules and evidence used.

ABR provides further results of possible culprits, when new information is
provided. For example, when new evidence is provided that a leader of the al-

16

Qassam Cyber Fighters hackers group has publicly claimed this attack [36], then
ABR derives that this group is also one of the possible attackers.

(cid:50)

6.2 Discussion

Together with the answers to the queries, ABR also provides the diﬀerent ways in
which the result was derived. Furthermore, every result comes with its explana-
tion that is composed of the rules and pieces of evidence used. The explanations
are in the form of text and graphical representation. The given explanations
make ABR’s result and analysis process transparent to the user and provides
her/him further information that can be used for the analysis. ABR does not re-
quire the user to be familiar with the argumentation reasoning framework used
as the user needs only to provide the evidence (in some cases the evidence is
automatically extracted), and to launch the queries. The main goal of ABR is to
help the investigator during the analysis process and provide useful information.
ABR’s results include hypothesized but missing evidence and suggestions
about other investigations paths that could be followed by the analyst. The
missing evidence suggested can be collected by the analyst in a second moment
and given to ABR as part of an iterative process. We decided to provide only
the ﬁrst list of results of the “missing” pieces of evidence, together with the
conclusions that could be derived from them, to keep ABR’s running time and
complexity polynomial. Hence, ABR does not provide an exhaustive list of all
the possible hypotheses/missing evidence. On the other hand, limiting the sug-
gested evidence to be collected can be beneﬁcial for the analyst, who can focus
his/her attention on particular evidence, instead of spending time and resources
on checking an exhaustive list.

ABR promotes best practice and helps to share lessons learned between an-
alysts, as its reasoning rules can be constructed using the analysts’ reasoning
process and be used by multiple investigators across diﬀerent events. It also
helps investigators cope with large numbers of rules and large knowledge bases.
The attribution process is mainly human-based, and thus can be easily biased,
e.g., by the resources invested [13]. In some cases, it may be diﬃcult for the
analyst to abandon a path of investigation when substantial resources have been
devoted to it. ABR permits to reduce the human bias through the rigorous
application of rules and by suggesting new paths of investigation.

ABR relies on the reasoning rules with which it has been provided. Thus,
ABR can fail to deal with new evidence that has not been encountered before and
which is not included in the reasoning rules. Furthermore, ABR relies on the rules
being correct and complete. To illustrate its operation we have extracted 200
rules from public reports and analyses of cyber-attacks. However, the rules need
to be validated with expert analysts and the rule base would need to be enriched
with further rules for broader use. To facilitate the extraction and update of
the reasoning rules and the background knowledge, we plan to investigate the
automated extraction of rules and knowledge through the use of NLP techniques
in conjunction with ontologies for cyber-attack investigations.

17

As ABR uses in its reasoning information from past attacks and past attri-
butions, it will derive the wrong conclusions if the information is incorrect. In
particular, if a past attribution was incorrect, the error can be propagated to
ABR’s new results. For example, the Sony attack attribution [40] was built on
the (alleged) claim that North Korea was responsible for the assault on South
Korean banks in 2013 [3]. We can avoid this problem, by not using past attri-
bution decisions as part of the knowledge, but it would make the attribution
more diﬃcult or cumbersome as it would require a larger amount of additional
evidence. Furthermore, using results of past attributions is a common practice
adopted by forensic analysts during their analysis and attribution process, as it
permits to identify existing groups of attackers and to use their modus operandi
as an important factor for the attribution.

7 Conclusion and Future Work

In this work, we proposed a method and proof-of-concept argumentation-based
reasoner (ABR) that aims to help forensic investigators during the analysis and
attribution process of cyber-attacks. Our aim was to demonstrate how such a tool
can be constructed using the proposed argumentation framework. ABR aims to
help attribute cyber-attacks by leveraging both social and technical evidence. It
provides explanations of the given results and hints of new investigation paths.
The use of preference-based argumentation and abductive reasoning permits
ABR to work with conﬂicting pieces of evidence and to ﬁll the knowledge gaps
that derive from incomplete ones. We introduced ABR’s main components that
are its reasoning rules (that are based on past analyses and expert knowledge),
and its background knowledge. We improve ABR usability by applying the Q-
Model and categorizing the evidence and rules in three layers, thus following a
model familiar to the forensic analysts. Our reasoner emphasises the incremental
and iterative nature of attribution, by making the derivations of the solutions
fully transparent to the user.

In our future work, we plan to increase ABR’s reasoning capabilities by
adding new reasoning rules, and new background knowledge. In this work, we
mainly focused on constructing the ABR reasoner, addressing its usability and
showing its possible use. We leave a careful and possibly semi-automated pop-
ulation of the reasoning rules and background knowledge for future work. In
particular, we plan to use NLP techniques to automatically extract the reason-
ing rules and social evidence used by forensic analysts. Furthermore, we intend
to enhance the expressive power of ABR’s reasoner using ontologies. We also aim
to address its integration with forensic tools and data mining techiniques. We
plan to apply ABR to other cyber-attacks, across a broad range of threats and to
improve its usability using feedback from forensic analysts. Another interesting
future work is to include probabilities for our pieces of evidence and reasoning
rules, in order to provide probabilistic measures for the analysis and attribution
results.

18

Acknowledgments

Erisa Karaﬁli was supported by the European Union’s H2020 research and inno-
vation programme under the Marie Sk(cid:32)lodowska-Curie grant agreement No. 746667.

References

1. Mohammed H. Almeshekah and Eugene H. Spaﬀord. Planning and integrating
deception into computer security defenses. In NSPW, pages 127–138. ACM, 2014.
2. Mohammed H. Almeshekah and Eugene H. Spaﬀord. Cyber security deception. In
Sushil Jajodia, V.S. Subrahmanian, Vipin Swarup, and Cliﬀ Wang, editors, Cyber
Deception: Building the Scientiﬁc Foundation, pages 23–50. Springer, 2016.

3. Alex Altman and Zeke J Miller. Sony Hack: FBI Accuses North Korea in Attack
That Nixed The Interview. http://time.com/3642161/sony-hack-north-korea-
the-interview-fbi/, 2014. Last accessed: 2019-12-19.

4. Kostas G. Anagnostakis, Stelios Sidiroglou, Periklis Akritidis, Konstantinos Xini-
dis, Evangelos P. Markatos, and Angelos D. Keromytis. Detecting targeted attacks
using shadow honeypots. In USENIX, pages 129–144, 2005.

5. Benjamin Aziz. Modelling and reﬁnement of forensic data acquisition speciﬁca-

tions. Digital Investigation, 11(2):90–101, 2014.

6. Nicole Beebe. Digital forensic research: The good, the bad and the unaddressed. In
Advances in Digital Forensics V - Fifth IFIP WG 11.9 International Conference
on Digital Forensics, pages 17–36, 2009.

7. Keith Breene. Who are the cyberwar superpowers? http://www.weforum.org/
Last accessed:

agenda/2016/05/who-are-the-cyberwar-superpowers/, 2016.
2019-12-19.

8. Brilliant Maps. Who Americans Consider Their Allies, Friends and Enemies. http:
//brilliantmaps.com/us-allies-enemies/, 2017. Last accessed: 2019-12-19.
9. Brian Carrier. Deﬁning digital forensic examination and analysis tools using ab-

straction layers. Intern. Journal of Digital Evidence, 1(4):1–12, 2003.

10. Department of Justice.

North korean regime-backed programmer charged
intrusions.

conspiracy

with
http://www.justice.gov/opa/pr/north-korean-regime-backed-programmer-
charged-conspiracy-conduct-multiple-cyber-attacks-and,
accessed: 2019-12-19.

conduct multiple

attacks

cyber

2018.

Last

and

to

11. Antonio DeSimone and Nicholas Horton. SONY’s nightmare before christmas.
http://www.jhuapl.edu/Content/documents/SonyNightmareBeforeChristmas.
pdf, 2017. Last accessed: 2019-12-19.

12. Phan Minh Dung. On the acceptability of arguments and its fundamental role in
nonmonotonic reasoning, logic programming and n-person games. Artif. Intell.,
77(2):321–358, 1995.

13. Diane Felmlee and Susan Sprecher. Close relationships and social psychology:
Intersections and future paths. Social Psychology Quarterly, 63:365–376, 2000.

14. FireEye.

Advanced Persistent Threat Groups.

http://www.fireeye.com/

current-threats/apt-groups.html, 2019.

15. M. Fontani, T. Bianchi, A. De Rosa, A. Piva, and M. Barni. A framework for
decision fusion in image forensics based on dempster-shafer theory of evidence.
IEEE Transactions on Information Forensics and Security, 8(4):593–607, 2013.

19

16. Josh Fruhlinger.

Top cybersecurity facts, ﬁgures and statistics for 2018.

https://www.csoonline.com/article/3153707/top-cybersecurity-facts-
figures-and-statistics.html, 2018. Last accessed: 2019-12-19.

17. David Goldman. Major banks hit with biggest cyberattacks in history. http://
money.cnn.com/2012/09/27/technology/bank-cyberattacks/index.html, 2012.
Last accessed: 2019-12-19.

18. Rajesh Kumar Goutam. The problem of attribution in cyber security. Intern. J.
of Computer Applications, Foundation of computer science, 131(7):34–36, 2015.
19. International Telecommunication Union. Global Cybersecurity Index (GCI)
2017. https://www.itu.int/dms_pub/itu-d/opb/str/D-STR-GCI.01-2017-PDF-
E.pdf, 2017. Last accessed: 2019-12-19.

20. Antonis Kakas and Pavlos Moraitis. Argumentation based decision making for

autonomous agents. In AAMAS ’03, pages 883–890, 2003.

21. Antonis C. Kakas, Robert A. Kowalski, and Francesca Toni. Abductive logic pro-

gramming. J. Log. Comput., 2(6):719–770, 1992.

22. Antonis C. Kakas, Paolo Mancarella, and Phan Minh Dung. The acceptability

semantics for logic programs. In ICLP, pages 504–519, 1994.

23. Erisa Karaﬁli, Matteo Cristani, and Luca Vigan`o. A formal approach to analyz-
ing cyber-forensics evidence. In ESORICS (1), volume 11098 of Lecture Notes in
Computer Science, pages 281–301. Springer, 2018.

24. Erisa Karaﬁli, Antonis C. Kakas, Nikolaos I. Spanoudakis, and Emil C. Lupu.
Argumentation-based Security for Social Good. In AAAI Fall Symposium Series,
pages 164–170, 2017.

25. Erisa Karaﬁli, Linna Wang, Antonis C. Kakas, and Emil Lupu. Helping forensic
analysts to attribute cyber-attacks: An argumentation-based reasoner. In PRIMA,
volume 11224, pages 510–518. Springer, 2018.

26. Karen Kent, Suzanne Chevalier, Timothy Grance, and Hung Dang. SP 800-86.
Guide to Integrating Forensic Techniques into Incident Response. Technical report,
NIST, 2006.

27. Mandiant. Exposing One of China’s Cyber Espionage Units. Technical report,

Mandiant, 2013.

28. Steve Mansﬁeld-Devine. Us banks attacked - but by whom? Network Security,

2013(1):2, 2013.

29. Sean Martin.

8 Active APT Groups To Watch.

https://www.darkreading.
com/endpoint/8-active-apt-groups-to-watch/d/d-id/1325161, 2016. Last ac-
cessed: 2019-12-19.
30. McAfee. What

http://www.mcafee.com/enterprise/en-us/
security-awareness/ransomware/what-is-stuxnet.html, 2019. Last accessed:
2019-12-19.

is Stuxnet.

31. L. F. D. C. Nassif and E. R. Hruschka. Document clustering for forensic analysis:
an approach for improving computer inspection. IEEE Trans. Inf. Forensic Secur.,
8(1):46–54, 2013.

32. National Audit Oﬃce.

Investigation: WannaCry cyber attack and the
NHS. https://www.nao.org.uk/wp-content/uploads/2017/10/Investigation-
WannaCry-cyber-attack-and-the-NHS.pdf, 2017. Last accessed: 2019-12-19.
33. Lily Hay Newman. The Biggest Cybersecurity Disasters of 2017 So Far. https://
www.wired.com/story/2017-biggest-hacks-so-far/, 2017. Last accessed: 2019-
12-19.

34. Eric Nunes, Paulo Shakarian, and Gerardo I. Simari. Toward argumentation-based

cyber attribution. In AAAI Workshops, pages 177–184, 2016.

20

35. Wassila Ouerdane, Nicolas Maudet, and Alexis Tsoukias. Argumentation theory
and decision aiding. In Trends in Multiple Criteria Decision Analysis, pages 177–
208. Springer, 2010.

36. Nicole Perlroth and Quentin Hardy. Bank Hacking Was the Work of Irani-
ans, Oﬃcials Say. https://www.nytimes.com/2013/01/09/technology/online-
banking-attacks-were-work-of-iran-us-officials-say.html, 2013. Last ac-
cessed: 2019-12-19.

37. Phillip Porras, Hassen Saidi, and Vinod Yegneswaran. An analysis of Conﬁcker’s
logic and rendezvous points. https://www.usenix.org/legacy/events/leet09/
tech/full_papers/porras/porras_html/index2.html, 2009. Last accessed: 2019-
12-19.

38. Jo˜ao Rasga, Cristina Sernadas, Erisa Karaﬁli, and Luca Vigan`o. Time-stamped

claim logic. https://arxiv.org/abs/1907.06541, 2019.

39. Thomas Rid and Ben Buchanan. Attributing Cyber Attacks. Journal of Strategic

Studies, 38(1-2):4–37, 2015.

40. Jeﬀrey Roman.

FBI Defends Sony Hack Attribution.

https://www.

bankinfosecurity.com/sony-a-7762, 2015. Last accessed: 2019-12-19.

41. Jason Sattler. What we’ve learned from 10 years of the Conﬁcker mys-
https://blog.f-secure.com/what-weve-learned-from-10-years-of-

tery.
the-conficker-mystery/, 2019. Last accessed: 2019-12-19.

42. Bradley L. Schatz. Wirespeed: Extending the AFF4 forensic container format for
scalable acquisition and live analysis. Digital Investigation, 14:45 – 54, 2015.
43. Paulo Shakarian, Gerardo I. Simari, Geoﬀrey Moores, Damon Paulo, Simon Par-
sons, Marcelo A. Falappa, and Ashkan Aleali. Belief revision in structured proba-
bilistic argumentation - model and application to cyber security. Ann. Math. Artif.
Intell., 78(3-4):259–301, 2016.

44. Fulvio Valenza, Cataldo Basile, Daniele Canavese, and Antonio Lioy. Classiﬁcation
and analysis of communication protection policy anomalies. IEEE/ACM Transac-
tions on Networking, 25(5):2601–2614, 2017.

45. Timothy Vidas, Brian Kaplan, and Matthew Geiger. OpenLV: Empowering inves-
tigators and ﬁrst responders in the digital forensics process. Digital Investigation,
11:45–53, 2014.

46. David A Wheeler and Gregory N Larsen. Techniques for cyber attack attribution.

Technical report, Institute for Defense Analyses Alexandria VA, 2003.

47. YouGov. America’s Friends and Enemies. https://today.yougov.com/topics/
politics/articles-reports/2017/02/02/americas-friends-and-enemies,
2017. Last accessed: 2019-12-19.

48. Kim Zetter. Countdown to Zero Day: Stuxnet and the launch of the world’s ﬁrst

digital weapon. Crown Publishing Group, 2014.

21

