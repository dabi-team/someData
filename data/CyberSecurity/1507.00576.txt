5
1
0
2

p
e
S
4
1

]

R
C
.
s
c
[

2
v
6
7
5
0
0
.
7
0
5
1
:
v
i
X
r
a

Flip the Cloud: Cyber-physical Signaling Games
in the Presence of Advanced Persistent Threats

Jeﬀrey Pawlick, Sadegh Farhang, and Quanyan Zhu

Department of Electrical and Computer Engineering,
Polytechnic School of Engineering, New York University, New York, USA
{jpawlick,farhang,quanyan.zhu}@nyu.edu

Abstract. Access to the cloud has the potential to provide scalable and
cost eﬀective enhancements of physical devices through the use of ad-
vanced computational processes run on apparently limitless cyber infras-
tructure. On the other hand, cyber-physical systems and cloud-controlled
devices are subject to numerous design challenges; among them is that of
security. In particular, recent advances in adversary technology pose Ad-
vanced Persistent Threats (APTs) which may stealthily and completely
compromise a cyber system. In this paper, we design a framework for
the security of cloud-based systems that speciﬁes when a device should
trust commands from the cloud which may be compromised. This in-
teraction can be considered as a game between three players: a cloud
defender/administrator, an attacker, and a device. We use traditional
signaling games to model the interaction between the cloud and the de-
vice, and we use the recently proposed FlipIt game to model the strug-
gle between the defender and attacker for control of the cloud. Because
attacks upon the cloud can occur without knowledge of the defender,
we assume that strategies in both games are picked according to prior
commitment. This framework requires a new equilibrium concept, which
we call Gestalt Equilibrium, a ﬁxed-point that expresses the interdepen-
dence of the signaling and FlipIt games. We present the solution to this
ﬁxed-point problem under certain parameter cases, and illustrate an ex-
ample application of cloud control of an unmanned vehicle. Our results
contribute to the growing understanding of cloud-controlled systems.

1

Introduction

Advances in computation and information analysis have expanded the capabil-
ities of the physical plants and devices in cyber-physical systems (CPS)[4,13].
Fostered by advances in cloud computing, CPS have garnered signiﬁcant atten-
tion from both industry and academia. Access to the cloud gives administrators
the opportunity to build virtual machines that provide to computational re-
sources with precision, scalability, and accessibility.

Despite the advantages that cloud computing provides, it also has some draw-
backs. They include - but are not limited to - accountability, virtualization, and
security and privacy concerns. In this paper, we focus especially on providing ac-
curate signals to a cloud-connected device and deciding whether to accept those
signals in the face of security challenges.

 
 
 
 
 
 
Recently, system designers face security challenges in the form of Advanced
Persistent Threats (APTs) [19]. APTs arise from sophisticated attackers who can
infer a user’s cryptographic key or leverage zero-day vulnerabilities in order to
completely compromise a system without detection by the system administrator
[16]. This type of stealthy and complete compromise has demanded new types
of models [6,20] for prediction and design.

In this paper, we propose a model in which a device decides whether to trust
commands from a cloud which is vulnerable to APTs and may fall under ad-
versarial control. We synthesize a mathematical framework that enables devices
controlled by the cloud to intelligently decide whether to obey commands from
the possibly-compromised cloud or to rely on their own lower-level control.

We model the cyber layer of the cloud-based system using the recently pro-
posed FlipIt game [6,20]. This game is especially suited for studying systems
under APTs. We model the interaction between the cloud and the connected de-
vice using a signaling game, which provides a framework for modeling dynamic
interactions in which one player operates based on a belief about the private
information of the other. A signiﬁcant body of research has utilized this frame-
work for security [7,9,15,21,8]. The signaling and FlipIt games are coupled,
because the outcome of the FlipIt game determines the likelihood of benign
and malicious attackers in the robotic signaling game. Because the attacker is
able to compromise the cloud without detection by the defender, we consider
the strategies of the attacker and defender to be chosen with prior commitment.
The circular dependence in our game requires a new equilibrium concept which
we call a Gestalt equilibrium 1. We specify the parameter cases under which the
Gestalt equilibrium varies, and solve a case study of the game to give an idea of
how the Gestalt equilibrium can be found in general. Our proposed framework
has versatile applications to diﬀerent cloud-connected systems such as urban
traﬃc control, drone delivery, design of smart homes, etc. We study one particu-
lar application in this paper:ef control of an unmanned vehicle under the threat
of a compromised cloud.

Our contributions are summarized as follows:

i) We model the interaction of the attacker, defender/cloud administrator, and
cloud-connected device by introducing a novel game consisting of two coupled
games: a traditional signaling game and the recently proposed FlipIt game.
ii) We provide a general framework by which a device connected to a cloud can
decide whether to follow its own limited control ability or to trust the signal
of a possibly-malicious cloud.

iii) We propose a new equilibrium deﬁnition for this combined game: Gestalt
equilibrium, which involves a ﬁxed-point in the mappings between the two
component games.

iv) Finally, we apply our framework to the problem of unmanned vehicle control.

In the sections that follow, we ﬁrst outline the system model, then describe
the equilibrium concept. Next, we use this concept to ﬁnd the equilibria of the

1 Gestalt is a noun which means something that is composed of multiple parts and

yet is diﬀerent from the combination of the parts [2].

game under selected parameter regimes. Finally, we apply our results to the
control of an unmanned vehicle. In each of these sections, we ﬁrst consider the
signaling game, then consider the FlipIt game, and last discuss the synthesis
of the two games. Finally, we conclude the paper and suggest areas for future
research.

2 System Model

A

We model a cloud-based system in which a cloud is subject to APTs. In this
model, an attacker, denoted by
, capable of APTs can pay an attack cost to
completely compromise the cloud without knowledge of the cloud defender. The
defender, or cloud administrator, denoted by
, does not observe these attacks,
but has the capability to pay a cost to reclaim control of the cloud. The cloud
. The device may
transmits a message to a robot or other device, denoted by
follow this command, but it is also equipped with an on-board control system
for autonomous operation. It may elect to use its autonomous operation system
rather than obey commands from the cloud.

R

D

This scenario involves two games: the FlipIt game introduced in [20], and the
well-known signaling game. The FlipIt game takes place between the attacker
and cloud defender, while the signaling game takes place between the possibly-
compromized cloud and the device. For brevity, denote the FlipIt game by GF,
the signaling game by GS, and the combined game - call it CloudControl - by
GCC as shown in Fig. 1. In the next subsections, we formalize this game model.

2.1 Cloud-Device Signaling Game

}

∈

∈

−

A =

M =

mH , mL

For the device

Let θ denote the type of the cloud. Denote compromized and safe types of clouds
by θA and θD in the set Θ. Denote the probabilities that θ = θA and that θ = θD
by p and 1
p. Signaling games typically give these probabilities apriori, but in
CloudControl they are determined by the equilibrium of the FlipIt game GF.
Let mH and mL denote messages of high and low risk, respectively, and
receives the
, where aT represents trusting
}
UR, where UR

let m
{
message, it chooses an action, a
the cloud and aN represents not trusting the cloud.
, let uS
R : Θ
R is a
utility function such that uS
R (θ, m, a) gives the device’s utility when the type
R and
is θ, the message is m, and the action is a. Let uS
A : M
R be utility functions for the attacker and defender. Note
uS
D : M
that these players only receive utility in GS if their own type controls the cloud
in GF, so that type is not longer a necessary argument for uS
[0, 1], such that σS

represent a message in general. After
aT , aN
{

Denote the strategy of
mixed-strategy probability that
R
role of the sender may be played by
determined by GF. Let σS
A : M
she controls the cloud, so that σS

m) gives the
R : A
plays action a when the message is m. The
depending on the state of the cloud,
plays when
A
sends message

A
[0, 1] denote the strategy that

→
A (m) gives the probability that

A and uS
D.
R (a

⊂
UA

R. uS

by σS

UD

→

→

→

→

M

or

R

R

R

×

×

⊂

×

⊂

×

D

A

A

A

|

A

D : M

m. (The superscript S speciﬁes that this strategy concerns the signaling game.)
Similarly, let σS
when he controls
the cloud. Then σS
sends message m. Let Γ S
R,
A, and Γ S
Γ S
For

D denote the sets of mixed strategies for each player.

D (m) gives the probability that

[0, 1] denote the strategy played by

, deﬁne functions ¯uS

→

D

D

,

X ∈ {D

A}
gives the expected utility to sender
and the receiver plays mixed-strategy σS

X

R×

Γ S
X →

UX , such that ¯uS
X

X : Γ S
R, σS
σS
X
when he or she plays mixed-strategy σS
X
(cid:1)
R. Equation (1) gives ¯uS
X .

(cid:0)

a∈A
(cid:88)

m∈M
(cid:88)

¯uS
X

R, σS
σS
X

=

X (m, a) σS
uS

R (a

m) σS

X (m) ,

|

,
X ∈ {A

D}

(1)

(cid:1)

[0, 1] represent the belief of

(cid:0)
m) gives the
Next, let µ : Θ
→
believes that a sender who issues message m is of type θ.
likelihood with which
UR such that ¯uS
R : Γ S
Then deﬁne ¯uS
σS
gives the expected
m, µ (
R
R |
when it has belief µ, the message is m, and it plays strategy σS
R.
utility for
¯uS
R is given by

, such that µ (θ

R
R →

m)

• |

R

R

(cid:0)

(cid:1)

|

¯uS
R

σS
R |

(cid:0)

m, µ

=

uS
R (θ, m, a) µR (θ

θ∈Θ
(cid:88)

a∈A
(cid:88)

(cid:1)

m) σS

R (a

|

m) .

|

(2)

The expected utilities to the sender and receiver will determine their incen-

tives to control the cloud in the game GF described in the next subsection.

2.2 FlipIt Game for Cloud Control

The basic version of FlipIt [20]2 is played in continuous time. Assume that the
defender controls the resource - here, the cloud - at t = 0. Moves for both players
obtain control of the cloud if it is under the other player’s control. In this paper,
we limit our analysis to periodic strategies, in which the moves of the attacker
and the moves of the defender are both spaced equally apart, and their phases
R+
are chosen randomly from a uniform distribution. Let fA
(where R+ represents non-negative real numbers) denote the attack and renewal
frequencies, respectively.

R+ and fD

∈

∈

Players beneﬁt from controlling the cloud, and incur costs from moving. Let
wX (t) denote the average proportion of the time that player
has
controlled the cloud up to time t. Denote the number of moves up to t per
unit time of player
by zX (t). Let αD and αA represent the costs of each
defender and attacker move. In the original formulation of FlipIt, the authors
consider a ﬁxed beneﬁt for controlling the cloud. In our formulation, the beneﬁt
depends on the equilibrium outcomes of the signaling game GS. Denote these
A . These give the expected beneﬁt
equilibrium utilities of

D and ¯uS∗

X ∈ {D

by ¯uS∗

and

A}

X

,

D

A

2 See [20] for a more comprehensive deﬁnition of the players, time, game state, and
moves in FlipIt. Here, we move on to describing aspects of our game important for
analyzing GCC.

Fig. 1. The CloudControl game. The FlipIt game models the interaction between
an attacker and a cloud administrator for control of the cloud. The outcome of this
game determines the type of the cloud in a signaling game in which the cloud conveys
commands to the robot or device. The device then decides whether to accept these
commands or rely on its own lower-level control. The FlipIt and signaling games are
played concurrently.

of controlling the cloud. Finally, let uF
beneﬁt of

up to time t in GF. Then

D (t) and uF

and

D

A

A (t) denote the time-averaged

X (t) = ¯uS∗
uF

X wX (t)

αX zX (t) ,

,

,

−
and, as time continues to evolve, the average beneﬁts over all time become

X ∈ {D

A}

lim inf
t→∞

¯uS∗
X wX (t)

−

αX zX (t) ,

X ∈ {D

,

.

A}

(3)

(4)

We next express these expected utilities over all time as a function of periodic
R+
X : R+
be
X ∈ {D
D (fD, fA) and ¯uF
A (fD, fA) give the average
, respectively, when they play with frequencies fD and fA. If

strategies that
expected utility functions such that ¯uF
utility to
fD

fA > 0, it can be shown that

employ. Let ¯uF

and

and

A}

R,

→

A

A

×

D

D

,

≥

D (fD, fA) = ¯uS∗
¯uF
D

1

−

(cid:18)

fA
2fD (cid:19)

−

αDfD,

A (fD, fA) = ¯uS∗
¯uF
A

fA
2fD −

αAfA,

(5)

(6)

while if 0

≤

fD < fA, then

D (fD, fA) = ¯uS∗
¯uF
D

fD
2fA −

αDfD,

A (fD, fA) = ¯uS∗
¯uF
A

1

−

(cid:18)

fD
2fA (cid:19)

−

αAfA,

and if fA = 0, we have

A (fD, fA) = 0, ¯uF
¯uF

D (fD, fA) = ¯uS∗

D −

αDfD.

(7)

(8)

(9)

Equations (5)-(9) with Equation (1) for ¯uS
X ,
and Equation (2)
R will be main ingredients in our equilibrium concept in the next section.

X ∈ {D

A}

,

for ¯uS

3 Solution Concept

In this section, we develop a new equilibrium concept for our CloudControl game
GCC. We study the equilibria of the FlipIt and signaling games individually,
and then show how they can be related through a ﬁxed-point equation in order
to obtain an overall equilibrium for GCC.

3.1 Signaling Game Equilibrium

Signaling games are a class of dynamic Bayesian games. Applying the concept
of perfect Bayesian equilibrium (as it e.g., [10]) to GS, we have Deﬁnition 1.

R, σS
σS
Deﬁnition 1 Let the functions ¯uS
X
X
formulated according to Equation (1) and Equation (2), respectively. Then a
(cid:0)
perfect Bayesian equilibrium of the signaling game GS is a strategy proﬁle
D , σS∗
σS∗

and posterior beliefs µ (

m) such that

and ¯uS
R

A , σS∗
R

X ∈ {D

σS
R

A}

be

(cid:0)

(cid:1)

(cid:1)

,

,

• |

(cid:0)

(cid:1)

∀X ∈ {D

, σS∗

X (

,

A}

)
•

∈

arg max
σS
X

¯uS
X

R , σS
σS∗
X

,

(cid:0)

(cid:1)

m

∀

∈

M, σS∗

R (

• |

m)

∈

m) =

µ (θ

|

1

θ = θA
{

}

arg max
σS
R

¯uS
R

σS
R |

m, µ (

• |

m)

,

(cid:0)
σS∗
θ = θD
A (m) p + 1
{
}
A (m) p + σS∗
σS∗
D (m) (1

(cid:1)
σS∗
D (m) (1
p)

−

−

if σS∗

A (m) p + σS∗

D (m) (1

−
µ (θ

if σS∗

A (m) p + σS∗

D (m) (1

−

p)

= 0, and

m) = any distribution on Θ,

|
p) = 0.

(10)

(11)

p)

,

(12)

(13)

(cid:54)
• |

D , ¯uS∗

Next, let ¯uS∗

A , and ¯uS∗

R be the utilities for the defender, attacker, and de-
A , σS∗
vice, respectively, when they play according to a strategy proﬁle
R
m) that satisfy the conditions for a perfect Bayesian equilibrium.
and belief µ (
(cid:1)
Deﬁne a set-valued mapping T S :
2UD×UA such that T S (p; GS) gives
the set of equilibrium utilities of the defender and attacker when the prior prob-
p and the signaling game utilities are parameterized by
abilities are p and 1
GS

3. We have

D , σS∗
σS∗

[0, 1]

(14)
We will employ T S as part of the deﬁnition of an overall equilibrium for GCC
(cid:8)(cid:0)
after examining the equilibrium of the FlipIt game.

= T S (p; GS) .

D , ¯uS∗
¯uS∗
A

(cid:1)(cid:9)

→

−

(cid:0)

3.2 FlipIt Game Equilibrium

are
The appropriate equilibrium concept for the FlipIt game, when
restricted to periodic strategies, is Nash equilibrium [14]. Deﬁnition 2 applies the
concept of Nash Equilibrim to GF.

and

A

D

Deﬁnition 2 A Nash equilibrium of the game GF is a strategy proﬁle (f ∗
such that

D, f ∗
A)

f ∗
D ∈

arg max
fD

¯uF
D (fD, f ∗

A) ,

f ∗
A ∈

arg max
fA

D (f ∗
¯uF

D, fA) ,

(15)

(16)

fA

≥

where ¯uF
and Equation (7) and Equation (8) if fD

A are computed by Equation (5) and Equation (6) if fD
fA.

D and ¯uF

≤
To ﬁnd an overall equilibrium of GCC, we are interested in the proportion
control the cloud. As before, denote these proportions by
p, respectively. These proportions (as in [6]) can be found from the

and

of time that
p and 1
equilibrium frequencies by

A

−

D

0, if
fA
p = 
2fD

1

−

, if
fD
2fA

fA = 0
fD
≥
fA > fD

fA > 0
0

≥

, if

.

(17)

D


[0, 1] such that the expression T F

Let GF parameterize the FlipIt game. Now, we can deﬁne a mapping T F :
gives the proportion
U
of time that the attacker controls the cloud in equilibrium from the values of
controlling the cloud for the defender and the attacker. This mapping gives

D , ¯uS∗
¯uS∗

A ; GF

×U

→

(cid:1)

(cid:0)

A

D , ¯uS∗
¯uS∗
In addition to interpreting p as the proportion of time that the attacker
controls the cloud, we can view it as the likelihood that, at any random time,

p = T F

A ; GF

(18)

(cid:1)

(cid:0)

.

3 Since R does not take part in GS, it is not necessary to include ¯uS∗

R as an output of

the mapping.

Fig. 2. GS and GF interact because the utilities in the FlipIt game are derived from
the output of the signaling game, and the output of the FlipIt game is used to deﬁne
prior probabilities in the signaling game. We call the ﬁxed-point of the composition of
these two relationships a Gestalt equilibrium.

the cloud will be controlled by the attacker. Of course, this is precisely the value
p of interest in GS. Clearly, GF and GS are coupled by Equations (14) and (18).
These two equations specify the overall equilibrium for the CloudControl game
GCC through a ﬁxed-point equation, which we describe next.

3.3 Gestalt Equilibrium of GCC

When the CloudControl game GCC is in equilibrium the mapping from the pa-
rameters of GS to that game’s equilibrium and the mapping from the parameters
of GF to that game’s equilibrium are simultaneously satisﬁed as shown in Fig.
2. Deﬁnition 3 formalizes this equilibrium, which we call Gestalt equilibrium.

Deﬁnition 3 (Gestalt equilibrium) The cloud control ratio p†
[0, 1] and
equilibrium signaling game utilities ¯uS†
A constitute a Gestalt equilibrium
of the game GCC composed of coupled games GS and GF if the two components
of Equation (19) are simultaneously satisﬁed.

D and ¯uS†

∈

D , ¯uS†
¯uS†

A

(cid:16)

∈

(cid:17)

In short, the signaling game utilities

equation

D , ¯uS†
¯uS†

A

∈

T S

p†; GS

, p† = T F

D , ¯uS†
¯uS†

A ; GF

(19)

(cid:0)

(cid:1)

(cid:16)

(cid:17)
must satisfy the ﬁxed-point

D , ¯uS†
¯uS†

A

T S

T F

(cid:16)
D , ¯uS†
¯uS†

(cid:17)
A ; GF

; GS

.

(20)

In this equilibrium,

(cid:16)

or Equation (9),
Equation (9), and

D
R

(cid:16)

A

(cid:16)
(cid:17)
receives ¯uF
A according to Equation (6), Equation (8),
D according to Equation (5), Equation (7), or
R according to Equation (2).

receives ¯uF
receives ¯uS

(cid:17)

(cid:17)

FlipItGameTF(cid:0)¯uS∗D,¯uS∗A;GF(cid:1)SignalingGameTS(p;GS)p¯uS∗D¯uS∗ASolving for the equilibrium of GCC requires a ﬁxed-point equation essentially
because the games GF and GS are played according to prior committment.
Prior commitment speciﬁes that players in GS do not know the outcome of GF.
This structure prohibits us from using a sequential concept such as sub-game
perfection and suggests instead a ﬁxed-point equation.

4 Analysis

In this section, we analyze the game proposed in Section 2 based on our solution
concept in Section 3. First, we analyze the signaling game and calculate the
corresponding equilibria. Then, we solve the FlipIt game for diﬀerent values of
expected payoﬀs resulting from signaling game. Finally, we describe the solution
of the combined game.

4.1 Signaling Game Analysis

The premise of GCC allows us to make some basic assumptions about the utility
parameters that simpliﬁes the search for equilibria. We expect these assumptions
to be true across many diﬀerent contexts.

A1) uR(θD, mL, aT ) > uR(θD, mL, aN ): It is beneﬁcial for the receiver to trust

a low risk message from the defender.

A2) uR(θA, mH , aT ) < uR(θA, mH , aN ): It is harmful for the receiver to trust

a high risk message from the attacker.

A3)

m, m(cid:48)
∀

M, uA(m, aT ) > uA(m(cid:48), aN ) and

M , uD(m, aT ) >
uD(m(cid:48), aN ): Both types of sender prefer that either of their messages is
trusted rather than that either of their messages is rejected.

m, m(cid:48)
∀

∈

∈

A4) uA(mH , aT ) > uA(mL, aT ): The attacker prefers an outcome in which the
receiver trusts his high risk message to an outcome in which the receiver
trusts his low risk message.

Pooling equilibria of the signaling game diﬀer depending on the prior prob-
pool and the
depend on quantities in Equations (21) and (22) which

1. Speciﬁcally, the messages on which

and

A

−

D

abilities p and p
equilibrium action of
we call trust beneﬁts.

R

T BH (p) =

T BL (p) =

p [uR (θA, mH , aT )

uR (θA, mH , aN )]

+ (1

p) [uR (θD, mH , aT )

−
p [uR (θA, mL, aT )

p) [uR (θD, mL, aT )

−

−

+ (1

−

uR (θD, mH , aN )]

−

uR (θA, mL, aN )]

uR (θD, mL, aN )]

−

(21)

(22)

T BH (p) and T BL (p) give the beneﬁt of trusting (compared to not trust-
ing) high and low messages, respectively, when the prior probability is p. These
will trust a message that it receives in a pooling
quantities specify whether
equilibrium. If T BH (p) (respectively, T BL (p)) is positive, then, in equilibrium,
will trust all messages when the senders pool on mH (respectively, mL).

R

R

Fig. 3. The four quadrants represent parameter regions of GS. The regions vary based
on the tpyes of pooling equilibria that they support. For instance, quadrant IV supports
a pooling equilibrium in which A and D both send mH and R plays aN , as well as
a pooling equilibrium in which A and D both send mL and R plays aT . The shaded
regions denote special equilibria that occur under further parameter restrictions.

We illustrate the diﬀerent possible combinations of T BH (p) and T BL (p) in
the quadrants of Fig. 4.1. The labeled messages and actions for the sender and
receiver, respectively, in each quadrant denote these pooling equilibria. These
pooling equilibria apply throughout each entire quadrant. Note that we have
not listed the requirements on belief µ here. These are addressed in the Ap-
pendix A.2, and become especially important for various equilibrium reﬁnement
procedures.

The shaded regions of Fig. 4.1 denote additional special equilibria which
only occur under the additional parameter constraints listed within the regions.
(The geometrical shapes of the shaded regions are not meaningful, but their
overlap and location relative to the four quadrants are accurate.) The dotted
and uniformly shaded zones contain equilibria similar to those already denoted
in the equilibria for each quadrant, except that they do not require restrictions on
µ. The zone with horizontal bars denotes the game’s only separating equilibrium.
It is a rather unproductive one for
, since their messages are not trusted.
(See the derivation in Appendix A.1.) The equilibria depicted in Fig. 4.1 will
become the basis of analyzing the mapping T S (p; GS), which will be crucial for
forming our ﬁxed-point equation that deﬁnes the Gestalt equilibrium. Before
studying this mapping, however, we ﬁrst analyze the equilibria of the FlipIt
game on its own.

and

A

D

A:mHD:mHR:aTA:mLD:mLR:aTA:mHD:mHR:aTA:mLD:mLR:aNA:mHD:mHR:aNA:mLD:mLR:aNA:mHD:mHR:aNA:mLD:mLR:aTTBH(p)TBL(p)uD(mL,aT)≤uD(mH,aT)uD(mL,aT)≥uD(mH,aT)uR(θA,mL,aT)≤uR(θA,mL,aN)uR(θD,mH,aT)≤uR(θD,mH,aN)PoolingwithBeliefRestritionsPoolingNoBeliefRestritionsPoolingNoBeliefRestritionsPoolingwithBeliefRestritionsPoolingwithBeliefRestritionsPoolingwithBeliefRestritionsSeparatingEquilibrium4.2 FlipIt Analysis

In this subsection, we calculate the Nash equilibrium in the FlipIt game. Equa-
tions (5)-(9) represent both players’ utilities in FlipIt game. The solution of
this game is similar to what has presented in [20,6], except that the reward of
controlling the resource may vary. To calculate Nash equilibrium, we normalize
both players’ beneﬁt with respect to the reward of controlling the resource. For
diﬀerent cases, the frequencies of move at Nash equilibrium are:
D > 0:

A , ¯uS∗

and ¯uS∗

<

αD
¯uS∗
D

•

αA
¯uS∗
A

αD
¯uS∗
D

>

αA
¯uS∗
A

αD
¯uS∗
D

=

αA
¯uS∗
A

•

•

¯uS∗
A ≤

0:

•

A > 0 and ¯uS∗
¯uS∗

D ≤

0:

•

f ∗
D =

¯uS∗
A
2αA

, f ∗

A =

αD
2α2

A ×

(¯uS∗
A )2
¯uS∗
D

,

and ¯uS∗

A , ¯uS∗

D > 0:

f ∗
D =

αA
2α2

D ×

D )2
(¯uS∗
¯uS∗
A

, f ∗

A =

¯uS∗
D
2αD

,

and ¯uS∗

A , ¯uS∗

D > 0:

f ∗
D =

¯uS∗
A
2αA

, f ∗

A =

¯uS∗
D
2αD

,

D = f ∗
f ∗

A = 0,

D = 0 f ∗
f ∗

A = 0+.

(23)

(24)

(25)

(26)

(27)

A ≤

In the case that ¯uS∗

0, the attacker has no incentive to attack the cloud.
In this case, the defender need not move since we assume that she controls the
cloud initially. In the case that ¯uS∗
0, only the attacker has
A = 0+ to signify that the attacker
an incentive to control the cloud. We use f ∗
moves only once. Since the defender never moves, the attacker’s single move is
enough to retain control of the cloud at all times.

A > 0 and ¯uS∗

D ≤

Next, we put together the analysis of GS and GF in order to study the

Gestalt equilibria of the entire game.

4.3 GCC Analysis

To identify the Gestalt Equilibrium of GCC, it is necessary to examine the
mapping T S (p; GS) for all p
[0, 1]. As noted in Section 4.1, this mapping
depends on T BH (p) and T BL (p). From assumptions A1-A4, it is possible to
verify that (T BL (0) , T BH (0)) must fall in Quadrant I or Quadrant IV and
that (T BL (1) , T BH (1)) must lie in Quadrant III or Quadrant IV. There are
[0, 1] can transverse
numerous ways in which the set (T BL (p) , T BH (p)) , p

∈

∈

Fig. 4. For the parameter values overlayed on the ﬁgure, as p ranges from 0 to 1,
T BH (p) and T BL (p) move from Quadrant I to Quadrant IV. The equilibria supported
in each of these quadrants, as well as the equilibria supported on the interface between
them, are presented in Table 1.

diﬀerent parameter regions. Rather than enumerating all of them, we consider
one here.

Consider parameters such that T BL (0) , T BH (0) > 0 and T BL (1) > 0 but
T BH (1) < 04. This leads to an L that will traverse from Quadrant I to Quad-
rant IV. Let us also assume that uD (mL, aT ) < uD (mH , aT ), so that Equilib-
rium 5 is not feasible. In Fig. 4.3, we give speciﬁc values of parameters that
satisfy these conditions, and we plot (T BL (p) , T BH (p)) for p
[0, 1]. Then, in
Table 1, we give the equilibria in each region that the line segment traverses.
The equilibrium numbers refer to the derivations in the Appendix A.2.

∈

4 These parameters must

satisfy uR (θD, mH , aT ) > uR (θD, mH , aN ) and
uR (θA, mL, aT ) > uR (θA, mL, aN ). Here, we give them speciﬁc values in order
to plot the data.

TBH(p)TBL(p)PoolingwithBeliefRestritionsPoolingwithBeliefRestritions551010551010ParameterValuesuR(θD,mH,aT)=3uR(θD,mL,aT)=5uR(θA,mH,aT)=−8uR(θA,mL,aT)=1uD(mH,aT)=4uD(mL,aT)=3uA(mH,aT)=6uA(mL,aT)=2uR(θD,mH,aN)=1uR(θD,mL,aN)=0uR(θA,mH,aN)=1uR(θA,mL,aN)=0uD(mH,aN)=−2uD(mL,aN)=−1uA(mH,aN)=−5uA(mL,aN)=−1A:mHD:mHR:aTA:mLD:mLR:aTTable 1. Signaling game equilibria by region for a game that traverses between Quad-
rant I and Quadrant IV. Some of the equilibria are feasible only for constrained beliefs
µ, speciﬁed in Appendix A.2. We argue that the equilibria in each region marked by
(*) will be selected.

Region

Quadrant I

T BH (p) = 0 Axis

Quadrant IV

Equilibria

Equilibrium 3: Pool on mL; µ constrained; R plays aT
*Equilibrium 8: Pool on mH ; µ unconstrained; R plays aT
*Equilibrium 3: Pool on mL; µ constrained; R plays aT
Equilibrium 8: Pool on mH ; µ unconstrained; R plays aT
Equilibrium 6: Pool on mH ; µ constrained; R plays aN
*Equilibrium 3: Pool on mL; µ constrained; R plays aT
Equilibrium 6: Pool on mH ; µ constrained; R plays aN

D

A

R

and

If p is such that the signaling game is played in Quadrant I, then both senders
prefer pooling on mH . By the ﬁrst mover advantage, they will select Equilibrium
both prefer
8. On the border between Quadrant I and Quadrant IV,
plays aT . If they pool on mL, this is guaranteed. If
an equilibrium in which
R
they pool on mH , however,
receives equal utility for playing aT and aN ; thus,
the senders cannot guarantee that the receiver will play aT . Here, we assume
that the senders maximize their worst-case utility, and thus pool on mL. This is
Equilibrium 3. Finally, in Quadrant IV, both senders prefer to be trusted, and
so select Equilibrium 3. From the table, we can see that the utilities will have a
jump at the border between Quadrant I and Quadrant IV. The solid line in Fig.
5 plots the ratio ¯uS∗
D of the utilities as a function of p.
D , ¯uS∗
¯uS∗
A

. As we have noted, p depends
5 . Indeed, it is continuous in that ratio when the
only on the ratio ¯uS∗
(cid:0)
outcome at the endpoints is appropriately deﬁned. This mapping is represented
by the dashed line in Fig. 5, with the independent variable on the vertical axis.
= T S (p).
This shown by the intersection of the solid and dashed curves plotted in Fig. 5.
(cid:1)
At these points, the mappings between the signaling and the FlipIt games are
mutually satisﬁed, and we have a Gestalt equilibrium.6

A /¯uS∗
Next, consider the mapping p = T F

We seek a ﬁxed-point, in which p = T F

D , ¯uS∗
¯uS∗
A

D , ¯uS∗
¯uS∗
A

A /¯uS∗
D

and

(cid:1)

(cid:0)

(cid:0)

(cid:1)

5 When ¯uS∗

A = ¯uS∗

D = 0, we deﬁne that ratio to be equal to zero, since this will yield
fA = 0 and p = 0, as in Equations (9) and (17). When ¯uS∗
A > 0, it is
convenient to consider the ratio to be positively inﬁnite since this is consistent with
p → 1.

D = 0 and ¯uS∗

6 Note that this example featured a discontinuity in signaling game utilities on the
border between equilibrium regions. Interestingly, even when the pooling equilibria
diﬀer between regions, it is possible that the equilibrium on the border admits a
mixed strategy that provides continuity between the diﬀerent equilibria in the two
regions, and thus makes T S continuous. This could allow GCC to have multiple
Gestalt equilibria.

Fig. 5. T F and T S are combined on a single set of axis. In T S (the solid line), the
independent variable is on the horizontal axis. In T F (the dashed line), the independent
variable is on the vertical axis. The intersection of the two curves represents the Gestalt
equilibrium.

5 Cloud Control Application

In this section, we describe one possible application of our model: a cyber-
physical system composed of autonomous vehicles with some on-board control
but also with the ability to trust commands from the cloud. Access to the cloud
can oﬀer automated vehicles several beneﬁts [12]. First, it allows access to mas-
sive computational resources - i.e., infrastructure as a service (IaaS ). (See [5].)
Second, it allows access to large datasets. These datasets can oﬀer super-additive
beneﬁts to the sensing capabilities of the vehicle itself, as in the case of the de-
tailed road and terrain maps that automated cars such as those created by
Google and Delphi combine with data collected by lidar, radar and vision-based
cameras [1,11]. Third, interfacing with the cloud allows access to data collected
or processed by humans through crowd-sourcing applications; consider, for in-
stance, location-based services [17,18] that feature recommendations from other
users. Finally, the cloud can allow vehicles to collectively learn through experi-
ence [12].

Attackers may attempt to inﬂuence cloud control of the vehicle through sev-
eral means. In one type of attack, adversaries may be able to steal or infer cryp-
tographic keys that allow them authorization into the network. These attacks
are of the complete compromise and stealth types that are studied in the FlipIt
framework [20], [6] and thus are appropriate for a CloudControl game. FlipIt
also provides the ability to model zero-day exploits, vulnerabilities for which a
patch is not currently available. Each of these types of attacks on the cloud pose
threats to unmanned vehicle security and involve the complete compromise and
steathiness that motivate the FlipIt framework.

p00.10.20.30.40.50.60.70.80.91¯uS∗A¯uS∗D00.511.522.535.1 Dynamic Model for Cloud Controlled Unmanned Vehicles

In this subsection, we use a dynamic model of an autonomous car to illustrate one
speciﬁc context in which a cloud-connected device could be making a decision
of whether to trust the commands that it would receive or to follow its own
on-board control.

Fig. 6. A bicycle model is a type of representation of vehicle steering control. Here,
δ (t) is used to denote the angle between the orientation of the front wheel and the
heading θ (t). The deviation of the vehicle from a straight line is given by z (t)

T

We consider a car moving in two-dimensional space with a ﬁxed speed v0 but
with steering that can be controlled. (See Fig. 6, which illustrates the “bicycle
model” of steering control from [3].) For simplicity, assume that we are interested
in the car’s deviation from a straight line. (This line might, e.g., run along the
center of the proper driving lane.) Let z (t) denote the car’s vertical distance
from the horizontal line, and let θ (t) denote the heading of the car at time t.
The state of the car can be represented by a two-dimensional vector w (t) (cid:44)
z (t) θ (t)
. Let δ (t) denote the angle between the orientation of the front
wheel - which implements steering - and the orientation of the length of the car.
(cid:2)
We can consider δ (t) to be the input to the system. Finally, let y (t) represent
a vector of outputs available to the car’s control system. The self-driving cars
of both Google and Delphi employ radar, lidar, and vision-based cameras for
localization. Assume that these allow accurate measurement of both states, such
, then
that y1 (t) = z (t) and y2 (t) = θ (t). If the car stays near w (t) =
we can approximate the system with a linear model. Let a and b denote the
distances from the rear wheel to the center of gravity and the rear wheel to the
front wheel of the car, respectively. Then the linearized system is given in [3] by
the equations:

0 0

(cid:3)

(cid:2)

(cid:3)

T

z (t)
θ (t)

d
dt

(cid:20)

=

(cid:21)

(cid:20)

0 v0
0 0

z (t)
θ (t)

+

(cid:21)

(cid:20)

av0
b
v0
b (cid:21)

(cid:21) (cid:20)

δ (t) ,

y1 (t)
y2 (t)

=

(cid:21)

(cid:20)

1 0
0 1

z (t)
θ (t)

.

(cid:21)

(cid:21) (cid:20)

(cid:20)

(28)

(29)

z(t)θ(t)δ(t)5.2 Control of Unmanned Vehicle

Assume that the unmanned car has some capacity for automatic control with-
out the help of the cloud, but that the cloud typically provides more advanced
navigation.

Speciﬁcally, consider a control system onboard the unmanned vehicle de-
signed to return it to the equilibrium w (t) =
. Because the car has access
to both of the states, it can implement a state-feedback control. Consider a
linear, time-invariant control of the form

0 0

(cid:2)

(cid:3)

T

δcar (t) =

k1 k2

−

(cid:2)

(cid:3)

z (t)
θ (t)

.

(cid:21)

(cid:20)

(30)

This proportional control results in the closed-loop system

d
dt

z (t)
θ (t)

=

0 v0
0 0

−

av0
b
v0
b (cid:21)

k1 k2

.

(31)

z (t)
θ (t)

(cid:2)

(cid:20)

(cid:21)

(cid:21)

(cid:21)

(cid:20)

R

(cid:18)(cid:20)

The unmanned car

(cid:19) (cid:20)
(cid:3)
may also elect to obtain data or computational re-
sources from the cloud. Typically, this additional access would improve the con-
trol of the car. The cloud administrator (defender
), however, may issue faulty
commands or there may be a breakdown in communication of the desired sig-
nals. In addition, the cloud may be compromised by
in a way that is stealthy.
A
sometimes beneﬁts from rejecting the cloud’s com-
Because of these factors,
mand and relying on its own navigational abilities. Denote the command issued
by the cloud at time t by δcloud (t)
δA (t) , δD (t), depending on who controls
the cloud. With this command, the system is given by

R

D

∈

z (t)
θ (t)

d
dt

(cid:20)

=

(cid:21)

(cid:20)

0 v0
0 0

z (t)
θ (t)

+

(cid:21)

(cid:20)

av0
b
v0
b (cid:21)

(cid:21) (cid:20)

δcloud (t) .

(32)

5.3 Filter for High Risk Cloud Commands

In cloud control of an unmanned vehicle, the self-navigation state feedback input
given by δcar (t) in Equation (30) represents the control that is expected by the
vehicle given its state. If the signal from the cloud diﬀers signiﬁcantly from the
signal given by the self-navigation system, then the vehicle may classify the
message as “high-risk.” Speciﬁcally, deﬁne a diﬀerence threshold τ , and let

m =

mH , if
mL, if

(cid:40)

δcloud (t)
|
δcloud (t)
|

−
−

δcar (t)
|
δcar (t)
| ≤

> τ
τ

.

(33)

Equation (33) translates the actual command from the cloud (controlled by
or

) into a message in the cloud signaling game.

A
Equations (31) and (32) give the dynamics of the unmanned car electing to
trust and not trust the cloud. Based on these equations, Fig. 7 illustrates the
combined self-navigating and cloud controlled system for vehicle control.

D

Fig. 7. Block-diagram model for unmanned vehicle navigation control. At any time,
the vehicle uses strategy σS
R to decide whether to follow its own control or the control
signal from the cloud, which may be δA or δD, depending on the probabilities p, 1 − p
with which A and D control the cloud. Its own control signal, δcar, is obtained via
feedback control.

6 Conclusion and Future Work

In this paper, we have proposed a general framework for the interaction between
an attacker, cloud administrator/defender, and cloud-connected device. We have
described the struggle for control of the cloud using the FlipIt game and the
interaction between the cloud and the connected device using a traditional sig-
naling game. Because these two games are played by prior commitment, they are
coupled. We have deﬁned a new equilibrium concept - i.e., Gestalt equilibrium,
which deﬁnes a solution to the combined game using a ﬁxed-point equation. Af-
ter illustrating various parameter regions under which the game may be played,
we solved the game in a sample parameter region. Finally, we showed how the
framework may be applied to unmanned vehicle control.

Several directions remain open for future work. First, the physical compo-
nent of the cyber-physical system can be further examined. Tools from optimal
control such as the linear-quadratic regulator could oﬀer a rigerous framework
for deﬁning the costs associated with the physical dynamic system, which in turn
would deﬁne the payoﬀs of the signaling game. Second, future work could search
for conditions under which a Gestalt equilibrium of the CloudControl game is
guaranteed to exist. Finally, devices that use this framework should be equipped
to learn online. Towards that end, a learning algorithm could be developed that
is guaranteed to converge to the Gestalt equilibrium. Together with the frame-

AB(cid:31)−kδAδDδcarddtw(t)w(t)σSRpFlipItwork developed in the present paper, these directions would help to advance our
ability to secure cloud-connected and cyber-physical systems.

References

1. Delphi

drive,

Delphi

Automotive,

[Online].

Available:

http://www.delphi.com/delphi-drive.

2. Gestalt, Mirium-Webster,

[Online].

Available:

http://www.merriam-

webster.com/dictionary/gestalt.

3. K.l J. Astr¨om and R. M. Murray, Feedback systems: an introduction for scientists

and engineers, Princeton university press, 2010.

4. R. Baheti and H. Gill, Cyber-physical systems, The impact of control technology

12 (2011), 161–166.

5. S. Bhardwaj, L. Jain, and S. Jain, Cloud computing: A study of infrastructure as
a service (iaas), International Journal of engineering and information Technology
2 (2010), no. 1, 60–63.

6. K. D. Bowers, M. Van Dijk, R. Griﬃn, A. Juels, A. Oprea, R. L. Rivest, and
N. Triandopoulos, Defending against the unknown enemy: Applying ﬂipit to system
security, Decision and Game Theory for Security, Springer, 2012, pp. 248–263.
7. Thomas E Carroll and Daniel Grosu, A game theoretic investigation of deception
in network security, Security and Communication Networks 4 (2011), no. 10, 1162–
1172.

8. William Casey, Jose A. Morales, Thomson Nguyen, Jonathan Spring, Rhiannon
Weaver, Evan Wright, Leigh Metcalf, and Bud Mishra, Cyber security via signaling
games: Toward a science of cyber security, Distributed Computing and Internet
Technology (Raja Natarajan, ed.), Lecture Notes in Computer Science, vol. 8337,
Springer International Publishing, 2014, pp. 34–42 (English).

9. S. Farhang, M. H. Manshaei, M. N. Esfahani, and Q. Zhu, A dynamic bayesian
security game framework for strategic defense mechanism design, Decision and
Game Theory for Security, Springer, 2014, pp. 319–328.

10. D. Fudenberg and J. Tirole, Game theory. 1991, Cambridge, Massachusetts 393

(1991).

11. E. Guizzo, How googles self-driving car works, IEEE Spectrum Online, October 18

(2011).

12. B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, A survey of research on cloud
robotics and automation, Automation Science and Engineering, IEEE Transactions
on 12 (2015), no. 2, 398–409.

13. E. A. Lee, Cyber physical systems: Design challenges, Object Oriented Real-Time
Distributed Computing (ISORC), 2008 11th IEEE International Symposium on,
IEEE, 2008, pp. 363–369.

14. John F Nash et al., Equilibrium points in n-person games, Proc. Nat. Acad. Sci.

USA 36 (1950), no. 1, 48–49.

15. J. Pawlick and Q. Zhu, Deception by design: Evidence-based signaling games for

network defense, arXiv preprint arXiv:1503.05458 (2015).

16. G. Portokalidis, A. Slowinska, and H. Bos, Argos: an emulator for ﬁngerprint-
ing zero-day attacks for advertised honeypots with automatic signature generation,
ACM SIGOPS Operating Systems Review, vol. 40, ACM, 2006, pp. 15–27.

17. K. Sampigethaya, L. Huang, M. Li, R. Poovendran, K. Matsuura, and K. Sezaki,
Caravan: Providing location privacy for vanet, Tech. report, DTIC Document,
2005.

18. K. Sampigethaya, M. Li, L. Huang, and R. Poovendran, Amoeba: Robust location
privacy scheme for vanet, Selected Areas in Communications, IEEE Journal on 25
(2007), no. 8, 1569–1589.

19. C. Tankard, Advanced persistent threats and how to monitor and deter them, Net-

work security 2011 (2011), no. 8, 16–19.

20. M. van Dijk, A. Juels, A. Oprea, and R. L. Rivest, Flipit: The game of “stealthy

takeover, Journal of Cryptology 26 (2013), no. 4, 655–713.

21. J. Zhuang, V. M. Bier, and O. Alagoz, Modeling secrecy and deception in a multiple-
period attacker–defender signaling game, European Journal of Operational Re-
search 203 (2010), no. 2, 409–418.

A Derivation of Signaling Game Equilibria

In this appendix, we solve for the equilibria of GS.

A.1 Separating Equilibria

First, we search for separating equilibria of GS. In separating equilibria,
with certainty the type of the cloud.

R

knows

D plays mL and A plays mH If
A
D
plays mH , then the receiver rejects any mH according to assumption A2. The
best action for

is to deviate to mL. Thus, this is not an equilibrium.

plays mL (as a pure strategy) and

A

’s best re-
D plays mH and A plays mL If
sponse depends on the utility parameters. If uS
uS
R (θA, mL, aN )
uS
and uS
plays aN in response to both
R (θD, mH , aN ), then
messages. There is no incentive to deviate. Denote this separating equilibrium
as Equilibrium #2.

A
R (θA, mL, aT )

R (θD, mH , aT )

plays mH and

plays mL, the

R

R

≤

≤

D

If uS

R (θA, mL, aT )

R (θA, mL, aN ) and uS
uS

R (θD, mH , aT ) > uS

R (θD, mH , aN ),

then aN is within the set of best responses to mL, whereas aT is the unique best
response to mH . Assuming that he prefers to certainty receive a higher utility,

≤

A

deviates to mH .
If uS

R (θA, mL, aT ) > uS

R (θA, mL, aN ) and uS

R (θD, mH , aT )

uS
R (θD, mH , aN ),

≤

then aN is within the set of best responses to mH , whereas aT is the unique best
response to mL. Thus,

If uS

R (θA, mL, aT ) > uS
plays aT in response to both messages. We have assumed, however, that

R (θD, mH , aT ) > uS

R (θD, mH , aN ),

then

D

deviates to mL.
R (θA, mL, aN ) and uS

prefers to be trusted on mH compared to being trusted on mL (A4), so

R

A
deviates and this is not an equilibrium.

A

A.2 Pooling Equilibria

Next, we search for pooling equilibria of GS. In pooling equilibria,
relies only
p in order to form his belief about the type
on the prior probabilities p and 1
of the cloud. The existence of pooling equilibria depend essentially on the trust
beneﬁts T BH (p) and T BL (p) .

R

−

Pooling on mL If T BL (p) < 0, then
’s best response is aN . This will only
be an equilibrium if his best response to mH would also be aN . This is the case
only when the belief satisﬁes

R

µ (θA
|
µ (θA

≤

|

mH ) uR (θA, mH , aT ) + (1
−
mH ) uR (θA, mH , aN ) + (1

µ (θA
|
µ (θA

mH )) uR (θD, mH , aT )
mH )) uR (θD, mH , aN )

|

.

(34)

−

Moreover, this can only be an equilibrium when neither
tive to deviate: i.e., when

A

nor

D

have an incen-

uS
A (mH , aN )

≤

A (mL, aN ) and uS
uS

D (mH , aN )

uS
D (mL, aN ) .

(35)

≤

0, then

If T BL (p)

If these conditions are satisﬁed, then denote this equilibrium by Equilibrium #1.
’s best response us aT . Whether this represents
≥
R
have incentives to deviate from mL. If
or
an equilibrium depends on if
uS
uS
D (mL, aT ) and uS
uS
A (mL, aT ), then neither has
A (mH , aT )
D (mH , aT )
an incentive to deviate. This is Equilibrium #5. If one of these inequalities does
would play
not hold, then the player who prefers mH to mL will deviate if
aT in response to the deviation. The equilibrium condition is narrowed to when
the belief makes
not trust mH ; when Equation (34) is satisﬁed. Call this
Equilibrium #3.

R

R

A

≤

≤

D

Pooling on mH The pattern of equilibria for pooling on mH follows a similar
structure to the pattern of equilibria for pooling on mL.

If T BH (p) < 0, then

’s best response is aN . This will only be an equilibrium
if his best response to mL would also be aN . This is the case only when the belief
satisﬁes

R

µ (θA
|
µ (θA

≤

|

mL) uR (θA, mL, aT ) + (1
−
mL) uR (θA, mL, aN ) + (1

µ (θA
|
µ (θA

mL)) uR (θD, mL, aT )
mL)) uR (θD, mL, aN )

.

(36)

−
do not deviate, we require

|

To guarantee that

A

and

uS
A (mH , aN )

≥

D
A (mL, aN ) and uS
uS

D (mH , aN )

uS
D (mL, aN ) .

(37)

≥

If these conditions are satisﬁed, then we have Equilibrium #6.

R

’s best response is aT . If uS

0, then
uS
A (mL, aT ), then neither

uS
D (mL, aT ) and
If T BH
≥
uS
have an incentive to deviate.
A (mH , aT )
Call this Equilibrium #8. If one of these inequalities does not hold, then the
belief must satisfy Equation (36) for an equilibrium to be sustained. Denote this
equilibrium by Equilibrium #7.

D (mH , aT )

nor

A

≥

≥

D

