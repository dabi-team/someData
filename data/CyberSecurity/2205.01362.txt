TracInAD: Measuring Inﬂuence for Anomaly
Detection

Hugo Thimonier∗, Fabrice Popineau∗, Arpad Rimmel∗, Bich-Liên Doan∗ and Fabrice Daniel†
∗ Université Paris-Saclay, CNRS, CentraleSupélec, Laboratoire Interdisciplinaire des Sciences du Numérique,
91190, Gif-sur-Yvette, France.
Email: name.surname@lisn.fr
†Artiﬁcial Intelligence Department of Lusis, Paris, France.

2
2
0
2

l
u
J

7
2

]

G
L
.
s
c
[

3
v
2
6
3
1
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—As with many other tasks, neural networks prove
very effective for anomaly detection purposes. However, very
few deep-learning models are suited for detecting anomalies on
tabular datasets. This paper proposes a novel methodology to
ﬂag anomalies based on TracIn, an inﬂuence measure initially
introduced for explicability purposes. The proposed methods
can serve to augment any unsupervised deep anomaly detection
method. We test our approach using Variational Autoencoders
and show that the average inﬂuence of a subsample of training
points on a test point can serve as a proxy for abnormality. Our
model proves to be competitive in comparison with state-of-the-
art approaches: it achieves comparable or better performance
in terms of detection accuracy on medical and cyber-security
tabular benchmark data.

I. INTRODUCTION

As a research direction, anomaly detection has caught more
and more attention in recent years due to its many successful
applications across a large number of domains. This ﬁeld
of research bears several names that point towards relatively
similar methods and approaches [13]: outlier, novelty and
anomaly detection. The difference in the pursued objectives
can account for the dissimilarity in semantics.

Scholars often consider its utility to be two-fold. On the
one hand, it is well-suited for fraud and intrusion detection
problems that involve highly imbalanced datasets for which
standard supervised learning methods often fail. Most of these
supervised approaches fail when over-sampling and under-
sampling cannot be efﬁciently applied, which is often the case
for tabular datasets. On the other hand, anomaly detection has
also proven to be very effective when very few or no labels
are available.

In short, scholars have deﬁned anomaly detection as the
process of identifying points that deviate from a pre-deﬁned
notion of normality. Common examples of anomaly detection
include ﬂagging frauds among standard credit card payments,
identifying mislabeled samples within a dataset or detecting
computer intrusion.

The literature investigates two approaches to anomaly de-
tection. The traditional supervised approach involves giving
classiﬁers anomalous and normal samples as inputs in the
training phase. A somewhat more hegemonic approach to
anomaly detection involves learning or characterizing a dis-
tribution during training by considering a training set only
composed of normal data. Anomalies are then identiﬁed in

the inference stage by evaluating how well each sample ﬁts
the estimated distribution.

Many researchers have recently proposed different methods
to try and tackle the critical problem of detecting anomalies.
Most of today’s state-of-the-art methods rely on deep learning
models such as Deep SVDD [14], which mimic the canonical
approach of SVDD [20] without using the computationally
costly kernels. More recently, methods that rely on self-
supervision, such as [3] which uses pretext tasks, or [12]
which involves a contrastive loss, have also proven to perform
well. Except for [3] and [12], most proposed methods focus
on applications related to image or textual datasets.

This paper introduces a novel approach to anomaly de-
tection based on inﬂuence measures. Measuring inﬂuence
here describes the task of evaluating how much a sample
contributed to increasing or decreasing the loss of another
sample in the course of training. Our methodology relies on
TracIn proposed in [11], to evaluate the average inﬂuence
of a subsample of training points on a sample. Following up
on [10] who showed that self-inﬂuence in the unsupervised
set-up is correlated with the loss of the sample, we propose
a standalone method based on Variational Autoencoders [8]
which outperforms or shows comparable results with state-of-
the-art anomaly detection methods.

The main contributions involved in our methodology are:
• A novel anomaly detection method based on Variational
Autoencoders and inﬂuence measures which offers com-
petitive results on several benchmark datasets.

• We display evidence that the proposed approach can be

extended to any deep anomaly detection method.

II. RELATED WORKS

A. Anomaly Detection

Anomaly detection can be divided into the following non-

exhaustive categories:

One-Class Classiﬁcation. One-Class Classiﬁcation in-
volves discriminative models for anomaly detection: such
methods avoid the challenge of estimating the normal distri-
bution as a means of identifying anomalies. Instead, those
methods aim at characterizing the density by proposing a
decision boundary. In short, One-Class Classiﬁcation describes
methods that use exclusively normal data in the training

 
 
 
 
 
 
stage to learn a decision function to ﬂag anomalies. This
sub-ﬁeld includes both shallow and deep anomaly detection
methods. For instance, in OCSVM [16], and SVDD [20],
authors propose to learn a decision function in the form of a
hyperplane and a hypersphere respectively, in a Hilbert space
by using the kernel trick. Regarding deep methods, [14], [15]
put forward Deep-SVDD in which a deep neural network
replaces kernels to map data points to a latent space so that
normal points will be contained in a hypersphere. Deep-SVDD
can suffer from model collapse, which designates a trivial
mapping to a unique point in the latent space for certain
network architectures. Thus, in recent studies [5], [4] have
proposed regularization methods to avoid such pitfalls.

Reconstruction based methods. Reconstruction based
methods focus on learning a model which reconstructs well
normal instances while failing to reconstruct abnormal points.
Thus,
the reconstruction error can serve as a proxy for
anomaly: the higher the error, the higher the probability of
a point being an anomaly. Such reconstruction-based methods
encompass deterministic methods such as autoencoders, PCA,
or probabilistic variants. For instance, PCA has been adapted
to anomaly detection in [17] or [6]. Other methods have relied
on autoencoders to estimate the normal data distribution, such
as [7] in which authors augment the reconstruction error with
differences in the latent representations of the original and
reconstructed samples.

Self-supervised methods. In a large spectrum of applica-
tions including anomaly detection, many methods now involve
self-supervision as a means of improving models’ learning
capacity. For instance, in [3] authors propose to transform data
samples and use the self-supervised pretext task of identifying
which transformation was applied to a sample. Failure to
correctly predict the applied transformation can serve as a
proxy measuring anomaly. In a similar approach, [12] authors
also rely on transformations to ﬂag anomalies but propose
to learn them instead of using afﬁne transformation as in
[3]. Another recent approach, [18], fetches internal contrastive
learning to ﬂag anomalies on tabular data. Other recent work
in [19] proposed a methodology for contrastive pretraining to
improve on anomaly detection methods through a two-staged
framework.

B. Inﬂuence Estimation

Related to our work, inﬂuence estimation has also attracted
growing interest from the research community. It designates
the identiﬁcation of the training samples most responsible for
the prediction of a particular test sample x. This involves
the computation of an inﬂuence score which can take many
different forms [1], [2], [11] [21]. Recently, [9] proposed the
inﬂuence function, which relies on the idea that if an inﬂuential
sample for a particular test sample is removed from the set
used for training, then the test sample’s loss should signiﬁ-
cantly increase. Since estimating such function may be hard, in
[11] authors propose TracIn a computationally efﬁcient ﬁrst-
order approximation to the inﬂuence function. As mentioned
by the authors, such an inﬂuence function can be used to ﬂag

odd points in a dataset: mislabeled samples, outliers or even
anomalies. For instance, in an unsupervised set-up, where they
use β-Variational Autoencoders, [10] analyze the behaviour of
the inﬂuence function. They show that, for points that stand
out from the rest of the dataset, self-inﬂuence, which measures
the inﬂuence of a sample on its own loss/likelihood, tends to
be larger than for normal points. Based on their diagnostic, we
hypothesize that not only do self-inﬂuence behaviours differ
between normal and abnormal points, but the inﬂuence of
normal points on abnormal points should signiﬁcantly differ
from the inﬂuence of normal points on normal points.

In this work, we propose using inﬂuence measures to
ﬂag anomalies among normal samples. Firstly, we propose a
standalone method based on a Variational Autoencoder, which
shows that alone inﬂuence can be used efﬁciently for detecting
anomalies. Secondly, we discuss that self-inﬂuence can serve
to augment anomaly scores in many deep anomaly detection
methods.

A. TracIn

III. METHOD

Let us brieﬂy discuss TracIn [11] as a means of measuring
inﬂuence. Consider the following set up where X ⊆ Rd rep-
resents the data space, and a dataset Dn = {xj}n
j=1, xj ∈ X .
Let fθ denote a model, parametrized by θ ∈ Θ ⊆ Rp, whose
parameters are obtained through optimizing a loss function
ℓ : Θ × X → R. The loss of the model parametrized by θ for
a data point x ∈ X is ℓ(θ, x). For a training set Dtrain ⊆ Dn,
the parameters of the model are obtained through minimizing

Px∈Dtrain

ℓ(θ, x).

Deﬁne the inﬂuence of a sample x on a test sample x′ as
the difference in the loss for the sample x′ incurred by having
included x in the training set. Formally, the inﬂuence function
of a sample x on the test sample x′ is:

IF (x, x′) = ℓ(θ, x′) − ℓ(θ−x, x′)

(1)

where θ−x = arg minθ∈Θ Pz∈Dtrain\{x} ℓ(θ, z).

Consider an iterative optimization process, e.g. Stochastic
Gradient Descent (SGD), where θt denotes the obtained pa-
rameters after iteration t, Bt a minibatch of size b at iteration
t, and a step size ηt at iteration t. TracIn gives the following
measure of inﬂuence of x on the test sample x′ when SGD is
the optimizer1:

TracIn(x, x′) =

1
b X
t:x∈Bt

ηt∇ℓ(θt, x′) · ∇ℓ(θt, x)

(2)

where ∇ℓ(θt, x′) denotes the gradient of the loss function
evaluated for the sample x′ w.r.t. the parameter θt.

To avoid excessive computational overhead, one uses
TracInCP given in (3). It consists in storing checkpoints (CP)
i.e. parameters θt1, θt2 , . . . , θtk
along the training process,
corresponding to iteration t1, t2, ..., tk assuming that between
checkpoints each training sample is visited only once, e.g.

1We refer the reader to [11] for more detail on how (2) is derived.

Algorithm 1 Pseudo Python Code for TracInAD
Require: Dtrain, Dval, {θt1, . . . , θtn }, {ηt1, . . . , ηtn },

ℓ(θ, .), m

TracInAD ← dict()
B ← random sample of size m from Dtrain
for t ∈ {t1, . . . , tn} do

θ ← θt
η ← ηt
for x ∈ Dval do

end for

end for

TracInAD[x] += 1

m Px′∈B η∇ℓ(θ, x′) · ∇ℓ(θ, x)

one epoch, and that the step size remains constant between
checkpoints.

TracInCP(x, x′) =

k

X
i=1

ηi∇ℓ(θti, x′) · ∇ℓ(θti , x)

(3)

B. Inﬂuence as an anomaly score: TracInAD

Consider a deep anomaly detection model fθ whose parame-
ters were optimized through minimizing a loss function ℓ(., θ)
as detailed in section III-A, and a training set Dtrain solely
composed of normal samples.

Under the hypothesis that the normal samples belonging to
the training set and the validation set were obtained from a
similar distribution pnormal, the overall inﬂuence of training
samples on normal validation samples should be positive. In
other words, training samples should mostly reduce the loss
of normal samples. Using the terminology proposed in [11],
training samples should mostly be strong proponents for any
normal sample in the validation set. On the other hand, since
anomalies’ distributions differ from pnormal, training points
should mostly be opponents, in the sense that they contributed
to increase the loss, or weak proponents to the anomalies in
the test set.

Based on these premises, we propose to derive an anomaly
score based on TracInCP. Consider a sample x′ ∈ Dval for
which we wish to assess whether it is an anomaly. At each
checkpoint ti, randomly select a subsample of the training
set of ﬁxed size m, denoted Bt ⊆ Dtrain, and compute
the average TracInCP inﬂuence. This gives the following
anomaly score:

k

1
m X
x∈Bt

TracInAD(x′) =

X
i=1
Pseudo code of the TracInAD algorithm is presented in
Algorithm 1.

ηi∇ℓ(θti, x′) · ∇ℓ(θti , x) (4)

C. Application

To evaluate the pertinence of our methodology, we experi-
ment using a vastly used deep anomaly detection method based
on Variational Autoencoders (VAE) and the reconstruction
error.

1) Variational Autoencoders: First proposed in [8], Varia-
tional Autoencoders (VAE) are a particular form of autoen-
coders that rely on a Bayesian framework. Assume that every
sample x ∈ Dtrain is obtained from some random process
involving an unobserved continuous variable z. The latter
is sampled from some prior distribution pψ∗ (z), while x is
obtained from a conditional distribution pψ∗(x|z). In the VAE
framework, the core objective is two-fold: (i) estimating the
latent variable z given a sample x ∈ X and (ii) estimating
the parameter ψ∗ which fully describes the conditional and
prior distributions. To do so, a recognition model qφ(z|x), in
the form of a neural network, is involved in the process to
compensate for the intractability of the true posterior distribu-
tion pψ∗(z|x). Note that the recognition process qφ(z|x) and
the conditional distribution pψ∗(x|z) are also referred to as,
respectively, the encoder and the decoder.

One seeks to ﬁnd the parameter ψ of the marginal distri-
butions pψ(x) such that the log-likelihood of the training set,
Px∈Dtrain log pψ(x), is maximized. Conjointly, the recogni-
tion model is estimated such that it is as close as possible to
the true posterior distribution. This objective is met through
minimizing the Kullback-Leibler divergence between the two
distributions, DKL(qφ(z|x)kpψ(z|x)). In the end, the VAE
loss which is minimized w.r.t. {ψ, φ}, can be expressed as
LV AE(ψ, φ) = −Ez∼qφ(z|x) log pψ(x|z)
+DKL(qφ(z|x)kpψ(z|x))

(5)

since log pψ(x) = Ez∼q(z|x) log pψ(x|z). This loss can me
minimized through backpropagation using the reparameteriza-
tion trick [8].

Given (5), the inﬂuence function described in (1) is ex-

pressed as follows:

IFV AE(x, x′) = −(Ez∼qφ(z|x′) log pψ(x′|z)

−Ez∼qφ−x (z|x′) log pψ−x(x′|z))
+((DKL(qφ(z|x′)kpψ(z|x′))
−DKL(qφ−x(z|x′)kpψ−x(z|x′)))

(6)

In the present case, we consider a Gaussian prior pψ(z) =
N (z; 0, I). Similarly, let the conditional distribution pψ(x|z)
be a multivariate Gaussian whose distribution parameters are
computed from z using a neural network. The variational
approximate posterior is set to be a multivariate gaussian with
a diagonal covariance matrix. Let a sample x ∈ Dtrain, then
log qφ(z|x) = log N (z; µ(x), σ2(x) · I).

Empirically, a sample x ∈ Dtrain is fed to the encoder
which outputs parameters {µ(x), σ(x)}. Those parameters
then allow sampling a latent vector z corresponding to the
original sample x. Then, the latent vector is fed to the decoder,
which outputs a reconstructed sample ˜x.

2) Reconstruction Error: A standard approach to anomaly
detection that relies on Autoencoders and VAE consists in
using exclusively normal samples in the training phase to
estimate the normal distribution. During the inference phase,
the anomaly score is derived from the reconstruction error. For-
mally, consider a sample x′ ∈ Dval, and a trained autoencoder
fθ∗ whose parameters θ∗ were obtained through an iterative

process described in section III-A. Denote by ˜x′ = fθ∗(x′) the
reconstructed sample x′ using the learned autoencoder. The
anomaly score s is obtained as follows

s(x′) = kx′ − ˜x′k2
2

(7)

The described method relies on the same hypothesis as de-
scribed in section III-B.

3) TracInAD applied to VAE: Algorithm 1 can also be
applied to such set-up: TracInAD can be used as an anomaly
score instead of s(x′). Though, as seen in (6), computing the
exact inﬂuence function for a VAE can be challenging since
it would involve computing an expectation over the encoder.
However, [10] have proven that under mild conditions, the
empirical average of the inﬂuence function over l i.i.d. samples
is close to the true inﬂuence function with high probability
when l is properly selected. Therefore, empirically we compute
the average loss over l reconstructed samples for each sample
considered.

IV. EXPERIMENTS

A. Tabular Datasets

We propose to evaluate the performances of the model
presented in III-C concerning anomaly detection in a tabular
dataset set-up. We experiment on four benchmark datasets
used in the literature. We follow the procedure ﬁrst proposed
in [22].

Arrhythmia dataset. It consists of a cardiology dataset
it contains attributes related to the
from the UCI repo:
diagnosis of cardiac arrhythmia in patients. The dataset is
comprised of 16 classes: class 1 are normal patients, class
2 to 15 are arrhythmia suffering patients. The smallest classes
(3, 4, 5, 7, 8, 9, 14, 15) are taken to be anomalous while the rest
normal. Note that categorical attributes are dropped: in total,
there are 274 attributes. The train set is composed of 193
samples, while the validation set contains 259 samples.

Thyroid Dataset. It is another medical dataset from the
UCI repo which contains attributes corresponding to whether
a patient suffers from hyperthyroid. There are three classes
in the dataset among which hyper-function is designated as
the anomalous class while the rest is normal. Only the 6
continuous attributes are used. The train set and validation
set are composed of 1, 839 and 1, 933 samples, respectively.
KDD and KDDRev Datasets. KDD is an intrusion dataset
that was created by an extensive simulation of a US Air Force
LAN Network. The dataset consists of a normal data class
and 4 simulated attack types (denial of service, unauthorized
access from a remote machine, unauthorized access from local
superuser and probing). 41 different attributes are considered:
34 are continuous, and 7 are categorical. Categorical features
are encoded using one-hot encoding.

This dataset is used to construct two datasets. Firstly, the
KDD dataset in which non-attack classes, which correspond to
20% of the dataset, are treated as the anomaly class. Secondly,
the KDD Reverse dataset (KDDRev), in which the attack class
is subsampled to consist of 25% of the number of non-attack
samples and is considered the anomaly class. The train set of

KDDRev (resp. KDD) is composed of 48, 639 (resp. 198, 371)
samples, while the validation contains 72, 958 (resp. 295, 650)
samples.

Following the conﬁguration of [22], each training set is
composed of 50% of the normal data. The 50% remaining and
the entirety of the anomaly samples compose the validation set.
The share of anomalies in each validation set is the following:
(i) Arrhythmia: 25.48% (ii) Thyroid: 4.8%, (iii) KDD: 32.9%
(iv) KDDRev: 33.33%.

B. Hyperparameters

As discussed in section III-C2, we train a VAE for each of
the datasets presented in the previous section exclusively on
normal samples. For the smaller datasets, namely Arrhythmia
and Thyroid, we consider an encoder and decoder comprised
of 3 layers for the former dataset while 2 layers for the latter.
Regarding the larger datasets, KDD and KDDRev, we resort
to 6-layer networks for both the encoder and decoder.
We consider a batch size of 32 for the Arrhythmia dataset
while 16 for Thyroid, the models were trained for 20 epochs
and 250 epochs respectively. Note that
the small size of
both datasets involves a swift training process allowing a
large number of epochs without any signiﬁcant computational
overhead. Regarding the KDD and KDDRev datasets, models
were trained for 20 epochs, with a batch size of 32 and 128
respectively. We used the SGD algorithm to optimize the
parameters of the network along the training process with
learning rate set to 10−4 for both Thyroid and Arrhythmia
datasets, while 10−5 for KDDRev and 10−7 for KDD. We
refer the reader to the code made available online for more
detail on the hyperparameter set-up2.

Regarding the computation of the anomaly score, we con-
sider a step size between each saved checkpoint of 1 for
Arrhythmia, 10 for Thyroid and 2 for both KDD and KDDRev.
In other words, we compute the TracIn measure of inﬂuence
every 10 epochs, i.e. for 25 CP in the case of Thyroid. The
size m of the batch containing random samples belonging to
Dtrain used to compute TracInAD, as detailed in 4, is set
to 128 for Arrhythmia, 64 for Thyroid datasets while 256 for
KDD and KDDRev. We set l to 8 for the Thyroid dataset, 32
for Arrhythmia. Note that for faster training, we considered for
both larger datasets l = 1 and did not observe any deterioration
of the performances in comparison with larger values.

C. Results

The results of the experiments can be observed in Table I.
We compare our model to both shallow and deep anomaly de-
tection methods. Note that results for models such as OCSVM,
Local Outlier Factor (LOF), DAGMM [22] and GOAD [3]
were directly taken from [3], while results for NeuTraL AD
[12] were taken from their paper. The F1-Score was chosen
as the metric to compare the competing models in accordance
with the literature.

For our model, we performed 10 iterations for the smaller
datasets and 5 for the larger datasets. The rows of the table

2https://github.com/TracInAD/TracInAD

Method

Dataset

OC-SVM
E2E-AE
LOF
DAGMM [22]
GOAD [3]
NeuTraL AD [12]
Shenkar et al. [18]
TracIn AD

Arthythmia

F1 Score
45.8
45.9
50.0
49.8
52.0
60.3
61.8
54.6

σ

σ

Thyroid

F1 Score
38.9
11.8
52.7
47.8
74.5
76.8
76.8
77.6
TABLE I
ANOMALY DETECTION ACCURACY

KDD
F1 Score
79.5
0.3
83.8
93.7
98.4
99.3
99.4
82.1

2.3
1.1
1.8
2.1

1.1
1.9
1.2
5.4

KDDRev

F1 Score
83.2
74.5
81.6
93.8
98.9
99.1
99.2
98.8

σ

0.3
0.1
0.3
0.3

σ

0.2
0.1
0.1
0.6

indicate the model used while columns indicate obtained mean
metric for the F1-Score over the different iterations (the higher,
the better) as well as its standard deviation in the columns
denoted by σ for every dataset.

the original model’s performance. On the one hand, when
fθ performs well, it tends to outperform the original model.
On the other hand, when the original model performs poorly,
TracInAD appears to perform worst than the original model.

We observe that our model outperforms all state of the
art models on the Thyroid dataset. This may indicate that
our model performs well on severe imbalanced set-ups: the
Thyroid dataset stands out from the other three since it
disposes of a much lower proportion of anomalies in the
validation set. However, we also observe a relatively high
standard deviation compared to other approaches. Our model
competes well with [18], NeuTraL AD [12] and GOAD [3]
on the KDDRev dataset and outperforms all other considered
approaches. Also, note that for the arrthythmia dataset our
method performs on par in comparison with other methods
while it fails to perform well on the KDD dataset.

V. DISCUSSION
Generality of TracInAD As discussed in section III-B,
measuring inﬂuence as an anomaly score can be used for any
deep anomaly detection method. For instance, one can easily
apply TracInAD in the Deep SVDD [14] framework, where
ℓ(., θ) is taken to be the standard Deep-SVDD loss used to
optimize the network’s parameters. To support our statement
regarding the generality of our approach, we experiment on the
Thyroid dataset in the Deep SVDD framework. We train a 3-
layer encoder by minimizing the Deep SVDD loss. Note here
that no pre-training was performed to initialize the weights of
the network. The model is trained for 50 epochs with batch
size 16. The step used to compute the TracInAD score is set
to 10 as in the VAE set up. We compare the accuracy obtained
with the standard Deep SVDD anomaly score, with the score
computed as in (4) where ℓ(θ, x) is the standard Deep-SVDD
loss used to optimize the parameters of the network. We obtain
a mean F1-Score of 50.1% for TracInAD-DSVDD over 200
iterations, in comparison with standard Deep-SVDD anomaly
score, which obtains a F1-score of 40.1%. The difference is
statistically signiﬁcant. However, note that obtained results
suffer from high standard deviation, which may mitigate the
strength of our statement.

High Standard Deviation We observe that our model suf-
fers from a higher standard deviation than competing models.
We hypothesize that our model performances ﬂuctuate more
than other approaches mostly because it tends to exacerbate

VI. CONCLUSION

We presented a methodology to detect anomalies based
on a measure of inﬂuence which was ﬁrst proposed in the
explicability literature. Our methodology has the advantage
of being adaptable to any deep anomaly detection and can
improve on the standard anomaly score as discussed in the
previous section. We also showed through an experiment
using VAEs that our model competes well on tabular datasets,
especially the most challenging ones, in comparison with the
current state-of-the-art models.

ACKNOWLEDGMENT

This work was performed using HPC resources from the
"Mésocentre" computing center of CentraleSupélec and École
Normale Supérieure Paris-Saclay supported by CNRS and Ré-
gion Île-de-France (http://mesocentre.centralesupelec.fr/). This
research publication is supported by the Chair "Artiﬁcial intel-
ligence applied to credit card fraud detection and automated
trading" led by CentraleSupelec and sponsored by the LUSIS
company.

REFERENCES

[1] E. Barshan, M.-E. Brunet, and G. K. Dziugaite, “Relatif: Identifying
explanatory training samples via relative inﬂuence,” in Proceedings of
the Twenty Third International Conference on Artiﬁcial Intelligence and
Statistics, ser. Proceedings of Machine Learning Research, S. Chiappa
and R. Calandra, Eds., vol. 108. PMLR, 26–28 Aug 2020, pp. 1899–
1909.

[2] S. Basu, X. You, and S. Feizi, “On second-order group inﬂuence func-
tions for black-box predictions,” in Proceedings of the 37th International
Conference on Machine Learning, ser. Proceedings of Machine Learning
Research, H. D. III and A. Singh, Eds., vol. 119.
PMLR, 13–18 Jul
2020, pp. 715–724.

[3] L. Bergman and Y. Hoshen, “Classiﬁcation-based anomaly detection for
general data,” in International Conference on Learning Representations,
2020. [Online]. Available: https://openreview.net/forum?id=H1lK_lBtvS
[4] P. Chong, L. Ruff, M. Kloft, and A. Binder, “Simple and Effective
Prevention of Mode Collapse in Deep One-Class Classiﬁcation,” 2020
International Joint Conference on Neural Networks (IJCNN), pp. 1–9,
Jul. 2020.

[5] S. Goyal, A. Raghunathan, M. Jain, H. V. Simhadri, and P. Jain,
“Drocc: Deep robust one-class classiﬁcation,” in Proceedings of the
37th International Conference on Machine Learning, ser. Proceedings
of Machine Learning Research, H. D. III and A. Singh, Eds., vol.
119.
PMLR, 13–18 Jul 2020, pp. 3711–3721. [Online]. Available:
https://proceedings.mlr.press/v119/goyal20c.html
in multivariate data
[6] D. M. Hawkins, “The detection of errors
using principal components,” Journal of
the American Statistical
Association, vol. 69, no. 346, pp. 340–344, 1974. [Online]. Available:
http://www.jstor.org/stable/2285654

[7] K. H. Kim, S. Shim, Y. Lim, J. Jeon, J. Choi, B. Kim, and A. S. Yoon,
“Rapp: Novelty detection with reconstruction along projection pathway,”
in International Conference on Learning Representations, 2020.

[8] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” 2014.
[9] P. W. Koh and P. Liang, “Understanding black-box predictions via in-
ﬂuence functions,” in Proceedings of the 34th International Conference
on Machine Learning, ser. Proceedings of Machine Learning Research,
D. Precup and Y. W. Teh, Eds., vol. 70. PMLR, 06–11 Aug 2017, pp.
1885–1894.
[10] Z. Kong

“Understanding

and K. Chaudhuri,

instance-based
interpretability of variational auto-encoders,” in Advances in Neural
Information Processing Systems, M. Ranzato, A. Beygelzimer,
Y. Dauphin, P. Liang,
J. W. Vaughan, Eds., vol. 34.
Curran Associates, Inc., 2021, pp. 2400–2412. [Online]. Available:
https://proceedings.neurips.cc/paper/2021/ﬁle/13d7dc096493e1f77fb4ccf3eaf79df1-Paper.pdf

and

[11] G. Pruthi, F. Liu, M. Sundararajan, and S. Kale, “Estimating training

data inﬂuence by tracing gradient descent,” 2020.

[12] C. Qiu, T. Pfrommer, M. Kloft, S. Mandt, and M. Rudolph,
transformation learning for deep anomaly detection beyond
“Neural
images,” in Proceedings of
the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser.
Proceedings of Machine Learning Research, M. Meila and T. Zhang,
Eds., vol. 139.
PMLR, 2021, pp. 8703–8714. [Online]. Available:
http://proceedings.mlr.press/v139/qiu21a.html

[13] L. Ruff, J. R. Kauffmann, R. A. Vandermeulen, G. Montavon, W. Samek,
M. Kloft, T. G. Dietterich, and K.-R. Müller, “A Unifying Review of
Deep and Shallow Anomaly Detection,” Proceedings of the IEEE, vol.
109, no. 5, pp. 756–795, May 2021.

[14] L. Ruff, R. Vandermeulen, N. Goernitz, L. Deecke, S. A.
Siddiqui, A. Binder, E. Müller, and M. Kloft, “Deep one-class
classiﬁcation,” in Proceedings of the 35th International Conference on
Machine Learning, ser. Proceedings of Machine Learning Research,
vol. 80. PMLR, 10–15 Jul 2018, pp. 4393–4402. [Online]. Available:
http://proceedings.mlr.press/v80/ruff18a.html

[15] L. Ruff, R. A. Vandermeulen, N. Görnitz, A. Binder, E. Müller, K.-R.
Müller, and M. Kloft, “Deep semi-supervised anomaly detection,” in
International Conference on Learning Representations, 2020. [Online].
Available: https://openreview.net/forum?id=HkgH0TEYwH

[16] B. Schölkopf, R. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt,
“Support vector method for novelty detection,” in Proceedings of the 12th
International Conference on Neural Information Processing Systems, ser.
NIPS’99. Cambridge, MA, USA: MIT Press, 1999, p. 582–588.
[17] V. Sharan, P. Gopalan, and U. Wieder, “Efﬁcient anomaly detection
via matrix sketching,” in Advances in Neural Information Processing
Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, Eds., vol. 31. Curran Associates, Inc., 2018.
[18] T. Shenkar and L. Wolf, “Anomaly detection for tabular data with
internal contrastive learning,” in International Conference on Learning
Representations, 2022.

[19] K. Sohn, C.-L. Li, J. Yoon, M. Jin, and T. Pﬁster, “Learning and evalu-
ating representations for deep one-class classiﬁcation,” in International
Conference on Learning Representations, 2021.

[20] D. Tax and R. Duin, “Support vector data description,” Machine Learn-

ing, vol. 54, pp. 45–66, 01 2004.

[21] C.-K. Yeh, J. S. Kim, I. E. Yen, and P. Ravikumar, “Representer
point selection for explaining deep neural networks,” in Proceedings
of the 32nd International Conference on Neural Information Processing
Systems, ser. NIPS’18. Red Hook, NY, USA: Curran Associates Inc.,
2018, p. 9311–9321.

[22] B. Zong, Q. Song, M. R. Min, W. Cheng, C. Lumezanu, D. Cho, and
H. Chen, “Deep autoencoding gaussian mixture model for unsupervised
anomaly detection,” in International Conference on Learning Represen-
tations, 2018.

