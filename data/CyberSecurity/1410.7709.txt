Anomaly Detection Framework Using Rule Extraction
for Eﬃcient Intrusion Detection

Antti Juvonen∗, Tuomo Sipola

Department of Mathematical Information Technology, University of Jyv¨askyl¨a, Finland

Abstract

Huge datasets in cyber security, such as network traﬃc logs, can be analyzed
using machine learning and data mining methods. However, the amount of
collected data is increasing, which makes analysis more diﬃcult. Many ma-
chine learning methods have not been designed for big datasets, and conse-
quently are slow and diﬃcult to understand. We address the issue of eﬃcient
network traﬃc classiﬁcation by creating an intrusion detection framework
that applies dimensionality reduction and conjunctive rule extraction. The
system can perform unsupervised anomaly detection and use this information
to create conjunctive rules that classify huge amounts of traﬃc in real time.
We test the implemented system with the widely used KDD Cup 99 dataset
and real-world network logs to conﬁrm that the performance is satisfactory.
This system is transparent and does not work like a black box, making it
intuitive for domain experts, such as network administrators.

Keywords:
diﬀusion map, rule extraction

intrusion detection, dimensionality reduction, cyber security,

1. Introduction

Cyber security has become a very important topic in the past years as com-
puter networks, services and systems face new threats from attackers, which
has lead to increased interest towards these matters from companies and

4
1
0
2

t
c
O
8
2

]

G
L
.
s
c
[

1
v
9
0
7
7
.
0
1
4
1
:
v
i
X
r
a

∗Corresponding author.

Information Technology,
P.O. Box 35 (Agora), FI-40014 University of Jyv¨askyl¨a, Finland. Tel. +358 40 357 3875.

Department of Mathematical

Email addresses: antti.k.a.juvonen@jyu.fi (Antti Juvonen),

tuomo.sipola@jyu.fi (Tuomo Sipola)

Preprint submitted to arXiv.org

October 30, 2014

 
 
 
 
 
 
Intrusion detection systems (IDS) are an important part of
governments.
cyber security. They detect intrusions and abnormal behavior in networks
or other systems producing big data. Practical environments are always dif-
ferent, and this aﬀects the choice of the IDS and it’s detection algorithms
(Molina et al., 2012; Liao et al., 2013). These systems usually apply one
of two detection principles: signature-based or anomaly-based (Scarfone and
Mell, 2007). Signature-based approach means using manually created rules
that detect intrusions, whereas anomaly-based systems try to proﬁle nor-
mal behavior and detect abnormal action dynamically.
It is also possible
to combine these approaches to form a hybrid IDS, as we have done in this
paper. This means creating signatures automatically and they can be period-
ically updated. In addition to the previous methodologies, stateful protocol
analysis can be applied (Liao et al., 2013). This means ﬁnding unexpected
sequences of commands. Moreover, anomaly detection systems could be used
in combination with existing next-generation ﬁrewalls and other systems, so
that the IDS beneﬁts from the best properties of both approaches. Many
diﬀerent algorithms can be used for anomaly detection, e.g. self-organizing
maps (Ramadas et al., 2003) and support vector machines (Tran et al., 2004).
Another approach is to use genetic algorithms in intrusion detection context
(Li et al., 2012; Goyal and Aggarwal, 2012).

Machine learning oﬀers many beneﬁts for intrusion detection (Chandola
et al., 2009). However, it has limits that should be considered. Semantic
gain, i.e. the practical meaning of the results, is usually more valuable in
practical cases than marginal increases in performance accuracy (Sommer
and Paxson, 2010). Anomaly detection should be used in combination with
existing systems to bring added value. One problem with machine learning
algorithms used for anomaly detection is the fact that many of them work
It is not an easy task to
like a black box from the end user perspective.
know how the algorithm internally functions, and because of this, companies
have diﬃculties deploying these systems. To overcome this problem, rule ex-
traction algorithms have been proposed (Craven and Shavlik, 1994). These
algorithms aim to create rules that replicate or approximate classiﬁcation
results. The main beneﬁt of deploying rule-based systems is that they are
fast and they might reveal comprehensible information to the users better
than black box algorithms. However, it seems that many rule extraction
algorithms depend on neural networks (Huysmans et al., 2006). Further-
more, rule extraction itself is usually a supervised learning task and needs
previously created classiﬁcation and labeling information in order to work.

2

Figure 1: Diﬀerent IDS principles.

We propose an intrusion detection framework that uses diﬀusion map
methodology for dimensionality reduction and clustering to automatically
label network traﬃc data. Figure 1 shows the diﬀerent IDS principles, and
how the rule extraction framework relates to them. Our system follows the
three overall stages of network IDS: data parametrization, training stage and
detection stage (Garc´ıa-Teodoro et al., 2009). The learning phase to label
the traﬃc is unsupervised. Subsequently, the classiﬁcation information is for-
warded to a rule extraction algorithm that creates conjunctive rules. These
automatically generated rules are used to classify new incoming traﬃc data.
The rules can also be analyzed by a domain expert in order to acquire new in-
formation about the nature of the data. The presented framework combines
knowledge-based expert system and machine learning-based clustering ap-
proaches (Garc´ıa-Teodoro et al., 2009). Consequently, the black box nature
of some of the existing algorithms is avoided. The framework does not need
preceding information about the intrusions that are present in the training
data. In addition, the structure of the system is dynamic and individual al-
gorithms can be changed if necessary. Low throughput and high cost, as well
as lack of appropriate metrics and assessment methodologies have been iden-

3

Known attacksSignature-based intrusion detection systemSignaturecreationTraining dataLearning rules usinganomaly detectionAnomaly-based intrusion detection systemRule extraction frameworkRule matchingRule-baseddetectionNetwork logSignature matchingKnown attack alertsTraining dataTraffic profilingNetwork logAnomaly detectionAnomaly alertsNetwork logtiﬁed as common problems in anomaly-based IDSes (Garc´ıa-Teodoro et al.,
2009).

This paper contributes to methodology and practical analysis in the ﬁeld
of network security. Firstly, the framework addresses the above issues by
generating rules that can classify incoming traﬃc with low computational
cost. Secondly, this research uses a well-known public dataset and appropri-
ate metrics to assess the performance. Finally, real-world network data is
used to further investigate the eﬀectiveness of the framework in a practical
situation. The main contribution of this paper is ﬁnalizing and detailed de-
scription of the framework (Juvonen and Sipola, 2013) and tests with more
varied data that show better detection rates than before.

The paper is structured as follows. First, we go through related reseach
concerning dimensionality reduction methodologies and rule extraction. In
the methodology section the overall system, all of the used algorithms and
performance metrics are described. Then, we introduce our experimental
cases and present the obtained results. Finally, we discuss the beneﬁts and
limitations of the system as well as future research directions.

2. Related work

This section brieﬂy discusses research that is related to the methods used
in this article. There are ﬁve areas that are covered: dimensionality reduc-
tion, anomaly detection, rule extraction, big data approaches and the earlier
frameworks designed by the authors. These points have contributed to the
design of the current framework.

Firstly, dimensionality reduction has been widely researched in the in-
trusion detection context. Perhaps the most well-known method is principal
component analysis (PCA) (Jolliﬀe, 2005), which has been used in network
anomaly detection (Ringberg et al., 2007; Callegari et al., 2011). However, it
has some problems, such as the fact that it cannot handle non-linear data. In
general, manifold learning approaches try to learn the structure of the data,
retaining some meaningful qualities of the data mining problem. The point is
to contain most of the information in the data using fewer dimensions. The
goals in such a setting are understanding and classiﬁcation of data and gener-
alization for use with new data (Lee and Verleysen, 2007). Several manifold
learning methods have been used for intrusion detection, including Isomap
and locally linear embedding (LLE) (Zheng et al., 2009a,b; Yuancheng et al.,
2010). This paper uses the diﬀusion map manifold learning method, which

4

is a non-linear dimensionality reduction method (Coifman and Lafon, 2006).
Diﬀusion map methodology has been used for network traﬃc classiﬁcation
and SQL intrusion detection (David, 2009; David et al., 2010; David and
Averbuch, 2011). We have previously used it for anomaly detection from
network logs (Sipola et al., 2011, 2012). This framework is enhanced by us-
ing clustering to detect multiple behaviors in the data (Juvonen and Sipola,
2012). In these experiments, the diﬀusion map algorithm acts like a black
box, which is a drawback that makes the system hard to understand for
people who are not familiar with data mining technologies.

Secondly, related research on anomaly detection in intrusion detection
context is explored. Some of the common general approaches to intrusion
detection are statistical methods, machine learning and data mining (Patcha
and Park, 2007). Statistical methods do not require prior knowledge about
attacks, but they need a certain statistical distribution, which is not al-
ways the case. Machine learning based methods (e.g., Bayesian networks
and principal component analysis (PCA)) aim to learn from the behavior
and improve the performance over time. Advantages and drawbacks depend
on the used algorithm, e.g., PCA cannot handle non-linear data, which is
why we use diﬀusion map methodology for dimensionality reduction. Fi-
nally, data mining methods such as genetic algorithms and artiﬁcial neural
networks attempt to ﬁnd patterns and deviations automatically from the
data.
In addition,
anomaly detection techniques can be divided into classiﬁcation-based, near-
est neighbor-based and clustering-based methods (Chandola et al., 2009).
Also, artiﬁcial immune systems have been used extensively in intrusion de-
tection (Kim et al., 2007). Ensemble systems have also been successful in the
area Mukkamala et al. (2005). Using computational and artiﬁcial intelligence
methods makes it possible to create adaptive, fault tolerant and fast systems
(Wu and Banzhaf, 2010; Liao et al., 2013). Our system combines many of
the previously mentioned approaches such as clustering-based anomaly de-
tection and dimensionality reduction. The system, its diﬀerences to existing
methodologies and our contributions are explained in more detail at the end
of this section.

It is also possible to use some kind of hybrid system.

Thirdly, rule extraction is considered. In order to understand the black-
box nature of non-linear classiﬁers, rule extraction methods can create sets
of rules that describe the behavior of such systems in a more understand-
able manner (Martens et al., 2008). Our implementation of the presented
framework is based on the conjunctive rule extraction algorithm (Craven

5

and Shavlik, 1994). However, the overall framework does not depend on any
speciﬁc clustering or classiﬁcation algorithm. The choice of the used classiﬁ-
cation method inﬂuences the selection of other algorithms in the framework,
because of varying performance and robustness. The approach presented in
this research resembles spectral clustering frameworks and can be though as
a special case of them (Bach and Jordan, 2006; von Luxburg, 2007). There
are many algorithms that extract rules from trained neural networks, such
as TREPAN (Craven and Shavlik, 1996) and Re-RX (Setiono et al., 2008).
These algorithms depend on the neural network architecture and the associ-
ated weights. Another common approach is to use support vector machines
as a basis for rule extraction (N´u˜nez et al., 2002; Barakat and Diederich,
2004; Barakat and Bradley, 2010). One example for generating signatures
for detecting polymorphic worms is Polygraph, which uses disjoint content
substrings to identify them (Newsome et al., 2005). Regular expressions gen-
erated with a supervised domain expert dataset have also been used to screen
unwanted traﬃc (Prasse et al., 2012). In addition, many swarm intelligence
algorithms can be used for diﬀerent intrusion detection applications. For ex-
ample, ant colony optimization (ACO) can be utilized to detect intrusions,
detect the origin of an attack and also induction of classiﬁcation rules (Kolias
et al., 2011). The ﬁeld of swarm intelligence contains many other algorithms
that are useful for IDS purposes, such as particle swarm optimization (PSO)
and ant colony clustering (ACC).

Next, many of the above mentioned algorithms can be scaled up for big
data applications using various parallel and distributed approaches (Bekker-
man et al., 2012). For example, the map-reduce framework has proved itself
as a feasible method for machine learning (Chu et al., 2006). The k-means
algorithm used in our system can also be parallelized using map-reduce (Zhao
et al., 2009). Moreover, the traditional intrusion detection methods can be
sped up using graphics processors (Vasiliadis et al., 2008).

Finally, the authors have already proposed a system for extracting con-
junctive rules from HTTP network log from real-life web servers (Juvonen
and Sipola, 2013). The data mining approach to network security is similar to
the methodology of this paper. We now extend and improve this framework
for diﬀerent kinds of data. Our framework works in an unsupervised manner
and does not depend on the selection of the used classiﬁcation algorithm.

The system proposed and tested in this paper combines many of the ap-
proaches mentioned previously in this section. The dimensionality reduction
phase could have been done using principal component analysis, but it some-

6

Figure 2: Block diagram of the overall approach.

times fails with non-linear data. We have learned through experiments that
diﬀusion map sometimes gives more accurate results. In addition, cluster-
ing based anomaly detection has also been used previously in the literature.
However, our approach is diﬀerent because we do not cluster the original
data, but the data points in low-dimensional space after diﬀusion map phase.
The diﬀusion map already helps to separate the points, reducing the error
that k-means clustering might introduce. Finally, because the diﬀusion map
is not computationally very eﬃcient, we use conjunctive rule extraction to
approximate the clustering results and classify data. This way we aim to
combine accurate results and feasible computational speed.

3. Methodology

Our system is divided into two phases: training phase and testing phase.
Training consists of preprocessing and ruleset learning, and as an end result
it produces a ruleset that can be used for traﬃc classiﬁcation. Testing phase
uses the created rules to classify testing data. Performance of the testing
phase is also measured. The overall approach is described in Figure 2.

The training phase consists of the following steps:

• Training data selection

• Feature extraction

– Discretization of continuous data (binning)

– Binarization

• Unsupervised learning

7

Rule set learningphaseTraining datasetTesting datasetTraffic classificationphaseRule setClassified trafficFigure 3: Block diagram of the ruleset learning process.

– Dimensionality reduction

– Clustering

• Detecting normal traﬃc

• Rule extraction

Figure 3 shows this ruleset learning phase in detail. Previously unknown
data (network log) is used as an input, and this data is automatically labeled
in an unsupervised manner. However, detecting the normal data cluster
might need expert input. This classiﬁcation information can then be used
to create the ruleset, which tries to simplify the process of dimensionality
reduction and clustering to human-readable rules.

Testing, or ruleset matching phase can be summarized as follows:

• Test data feature extraction

• Rule matching

8

Network logFeature extractionand normalizationClusteringDimensionalityreductionRule extractionRule setLow-dimensional dataOriginal dimensionality dataUnsupervised learningRule extractionDetecting normaldata clustersLabelsPreprocessed dataThis phase preprocesses new data and uses the rules created in the learn-
ing phase to classify the testing data as normal or intrusive.
If needed,
classiﬁcation into more than two classes is also possible if this is taken into
account in the training phase. The new traﬃc that the system identiﬁes as
normal is naturally not ﬂagged. Traﬃc corresponding to intrusion rules are
ﬂagged as attacks. This ﬂagging resembles misuse detection. In addition,
traﬃc that does not match any rules, is ﬂagged as unknown anomalies.

3.1. Dimensionality reduction using diﬀusion map
Dimensionality of the feature space is reduced using diﬀusion maps. This
training produces a low-dimensional model of the data, which facilitates clus-
tering and identiﬁcation of internal structure of the data. Diﬀusion maps are
useful in ﬁnding non-linear dependencies. At the same time the diﬀusion dis-
tances in the initial feature space correspond proportionally to the Euclidean
distances in the low-dimensional space (Coifman et al., 2005; Coifman and
Lafon, 2006; Nadler et al., 2006).

Each data point is represented by a feature vector xi ∈ Rn. The whole
dataset X = {x1, x2, x3, . . . xN } is a collection of these feature vectors. To
perform the diﬀusion map, an aﬃnity matrix

W (xi, xj) = exp

(cid:18) −||xi − xj||2
(cid:15)

(cid:19)

describing the similarity between the data points is calculated. Here aﬃnity
is deﬁned using the Gaussian kernel. The parameter (cid:15), which is responsible
for the neighborhood size, is selected from the optimal region in the weight
matrix sum

L =

N
(cid:88)

N
(cid:88)

i=1

j=1

Wi,j,

which can be plotted on a logarithmic scale for the identiﬁcation of the middle
region (Coifman et al., 2008), while (cid:15) is changed.

The row sums of W are collected to the diagonal of matrix D. This

matrix is used to normalize W in order to create transition matrix

This transition matrix is symmetrized as

P = D−1W.

9

1

˜P = D

2 P D− 1

2 = D− 1
The singular value decomposition (SVD) of ˜P ﬁnds the eigenvectors vk
and eigenvalues λk, which can be used to construct low-dimensional coordi-
nates for the data points. Each data point in the original data gets corre-
sponding diﬀusion map coordinates:

2 W D− 1
2 .

xi → [λ1v1(xi), λ2v2(xi) . . . λdvd(xi)].

The ﬁrst eigenvectors retain most of the information in the data, which
is why the later eigenvectors are left out. Some information is lost but the
error is bounded and lower dimensionality facilitates clustering.

3.2. Clustering
The k-means method groups the data points into clusters. The algorithm is
widely used in data mining and its implementation is simple. It ﬁnds the
centroid point of clusters and then assigns each point to the nearest cluster
centroid. This process is iterated until the clustering does not change after
updating centroids. The algorithm and use cases are described in more detail
in literature (Jain and Dubes, 1988; Tan et al., 2006; Jain, 2010). We chose to
use k-means clustering together with diﬀusion map because its use is justiﬁed
in the literature (Lafon and Lee, 2006).

In order to use k-means, the number of clusters needs to be determined.
The clustering is repeated many times and the quality of the obtained clusters
is measured. For ith data point, silhouette

s(i) =

b(i) − a(i)
max{a(i), b(i)}

expresses the quality of the clustering. Here a(i) is the average dissimilar-
ity of the point to all the other points in the same cluster, and b(i) is the
smallest of the average dissimilarities to all the other clusters. The larger the
silhouette value, the better the clustering is for the data point. The average
of the silhouette values for all data points is used to evaluate the quality of
clustering. These qualities are used to compare diﬀerent numbers of clusters
(Rousseeuw, 1987).

10

3.3. Rule extraction
A rule deﬁnes a number of easy-to-understand conditions for the features in
order to classify the data points according to those conditions. A ruleset
In an
consists of many rules and replicates a certain classiﬁcation result.
ideal situation a rule should be understandable by network experts without
the eﬀort of knowing the underlying classiﬁers. Rules correspond to data
features, and are useful as such for ﬁnding the root cause of intrusions or
other anomalous behavior.

It is not feasible to ﬁnd all the possible rules because they span such a
huge space.
In an optimal setting all the rules should be checked, but in
practice a sub-optimal but eﬃcient method is used. Such rule extraction
systems have been used to learn the information provided by, e.g., neural
networks (Craven and Shavlik, 1994; Ryman-Tubb and d’Avila Garcez, 2010)
or support vector machines (N´u˜nez et al., 2002; Barakat and Diederich, 2004;
Barakat and Bradley, 2010).

There are two main approaches to rule extraction. Firstly, decomposi-
tional rule extraction algorithms exploit the internal mathematical or algo-
rithmic features of the underlying data mining method (d’Avila Garcez et al.,
2001). Secondly, pedagogical algorithms do not depend on the method used
(Craven and Shavlik, 1994). They only take into account the input data and
classiﬁcation result. We take the pedagogical approach, because this makes
it possible to use diﬀerent kinds of algorithms for data classiﬁcation.

A conjunctive rule is a logical expression that combines logical symbols
using the conjunction (i.e. AND) operation. Such a symbol states whether
the value of a binary feature must be true or false. The absence of the symbol
means that the feature can be anything. Note that in this research a binary
feature means a truth value either belonging to a symbolic category, or a
truth value that tells in which bin a continuous value belongs to.

Let us consider an example where we have ﬁve features a, b, c, d, e. This
means that the feature matrix contains ﬁve columns corresponding to each
binary feature. Each data point corresponds to a row in the matrix. Here is
an example ruleset containing three rules classifying data into two separate
classes c1 and c2:

r1 =¬a
r2 =a ∧ b ∧ c ∧ ¬d ∧ e
r3 =a ∧ b ∧ ¬c

for class c1,
for class c1,
for class c2.

11

In this case, rule r1 means that if the value of the feature a is false for a
single data point, it belongs to class c1. The values for the rest of features
do not matter. Other rules work in a similar way: rule r3 states that a and
b have to be true while c must be false. If this rule r3 holds true, the point
is classiﬁed to class c2.

In our actual implementation the truth values are converted to 1 and −1,
and the values that do not matter are expressed as 0. Therefore, the rule r1
can be expressed using vector (cid:0)−1 0 0 0 0(cid:1). This approach makes rule
matching easy from the computational perspective.

The conjunctive rule algorithm (3.1) creates a ruleset for the training
data. It creates new rules until the whole dataset is covered without mis-
classiﬁcation (Craven and Shavlik, 1994). If the value of a symbol in a rule
does not aﬀect the classiﬁcation result using the rules, the symbol can be
omitted. The ﬁnal rules contain only the symbols that are essential for the
classiﬁcation. The rules generated using the algorithm can be used for simple
matching in the testing phase.

Algorithm 3.1: Conjunctive rule extraction (Craven and Shavlik,
1994).

Input: data points E, classes C
Output: rules Rc that cover E with classiﬁcation C

repeat

e := get new training observation from E
c := get the classiﬁcation of e from C
if e not covered by the rules Rc then
r := use e as basis for new rule r
for all symbols si in r do

r(cid:48) = r with symbol si dropped
if all instances covered by r are of the same
class as e then

r := r(cid:48)

end if
end for
add rule r to the ruleset Rc

end if

until all training data analyzed

12

Table 1: Confusion matrix.

predicted

normal

attack

l normal
a
u
t
c
a

attack

true negative (tn)

false positive (fp)

false negative (fn)

true positive (tp)

3.4. Performance metrics
After classifying data points as normal or intrusive, and running the testing
phase, the following performance metrics are calculated. Table 1 shows the
confusion matrix and some abbreviations used in this research. Note that
correct alarms of intrusions are regarded as true positive identiﬁcations.

Sensitivity or true positive rate (TPR) tells how well intrusive data points

are identiﬁed:

sensitivity =

tp
tp + f n

.

False positive rate (FPR) is calculated in a similar way. False positives
In

are data points that are classiﬁed as attacks but are actually normal.
intrusion detection context this is often called false alarm rate:

fpr =

f p
f p + tn

.

Speciﬁcity or true negative rate (TNR) explains how well normal data

points are found:

tn
f p + tn
Accuracy tells how many correct results there are compared to the whole

speciﬁcity =

= 1 − f pr.

data:

accuracy =

tp + tn
tp + f p + f n + tn

.

Precision explains the proportion of true positive classiﬁcations in all

positive results, i.e. how many alarms were correct and not false alarms:

precision =

tp
tp + f p

.

13

The previous metrics are extensively used but cannot alone describe the
results in a satisfactory way. For this reason, we also use Matthews correla-
tion coeﬃcient (Baldi et al., 2000; Brown and Davis, 2006; Fawcett, 2006).
It can measure the quality of binary classiﬁcation and is regarded as a good
metric for measuring the overall classiﬁcation performance:

MCC =

tp × tn − f p × f n
(cid:112)(tp + f p)(tp + f n)(tn + f p)(tn + f n)

.

4. Iris data: an illustrative example

In this section a toy problem is presented to clarify certain concepts in the
framework. The rule extraction algorithm described in subsection 3.3 is illus-
trated here using a practical example. In machine learning community, the
IRIS dataset (Frank and Asuncion, 2010) is commonly used to illustrate the
algorithms in a simple case. We use this well-known dataset to demonstrate
the rule generation process and what kind of information is produced as a
result. The data contains 150 samples from 3 diﬀerent species of ﬂowers.
There are four diﬀerent measurements made from each ﬂower: sepal length
and width as well as petal length and width. All these features are continu-
ous. The task is to cluster the data and ﬁnd corresponding rules using our
framework.

Feature generation is performed by ﬁrst dividing all 4 measurements into
3 bins. Larger number of bins is possible but in this case it was not neces-
sary. The binning process works in the following way. For this dataset, the
minimum value of petal width measurement is 0.1 cm and maximum value
2.5 cm. For this feature, the boundaries of the bins are as follows:

Bin1

0.1

0.9

Bin2

1.7

Bin3

2.5

Other features are binned using the same principle. After binning, the
values are binarized from multi-category features to single-category binary
features for rule extraction. Since 3 bins are used for each of the 4 features,
we get 12 binary columns in the processed matrix. Consequently, the ﬁnal
feature matrix is of size 150 × 12. These 12 binary values also correspond to
the rules, so that each rule has 12 elements.

14

Figure 4: Rules generated from clustered IRIS data. Each line represents one rule. Note
that classes 1, 2 and 3 correspond to the three species.

The data is clustered into 3 groups using the k-means clustering algo-
rithm. There is no dimensionality reduction step beforehand because 12-
dimensional feature space is still rather low-dimensional. The clustering is
used to label ﬂower samples using 3 labels. Subsequently, the rules are ex-
tracted.

The resulting ruleset can be seen in Figure 4. There are 9 rules in total.
There is only one rule generated for class 3 (setosa species). The rule states
that if the fourth measurement (petal width) belongs to bin 1, the ﬂower’s
species is setosa. Bin 1 means that the petal width must be between 0.1
and 0.9 cm, as demonstrated above. The rules with mostly ’must not have’
(i.e. black) symbols are usually created for single data points, which might
be outliers. This is an example of how information can be gained from the
rules. We can see that the classiﬁcation process no longer works like a black
box, but is based on human-readable information.

5. KDD Cup 99 data

For this experiment we used KDD Cup 99 10% data (Frank and Asuncion,
2010). It has some known limitations (Mahoney and Chan, 2003; Sabhnani
and Serpen, 2004; Tavallaee et al., 2009) stemming from its source, the like-
wise problematic DARPA dataset (McHugh, 2000), but it is publicly available

15

Rules. Black = must not have, gray = any, white = must have.FeatureClass123456789101112111111223and labeled, which makes it easy for researchers to use the data and compare
results (Davis and Clark, 2011). A serious concern is the unrealistic distribu-
tion of intrusions. Therefore, it is common to use the smaller 10% dataset or
a custom version of it (Kolias et al., 2011). The dataset was created by gath-
ering data from a LAN that was targeted with multiple attacks. It contains
494,021 traﬃc samples and each sample includes 41 features. The features
include both continuous and symbolic values. The attacks fall into four gen-
eral categories: denial-of-service, remote-to-local, user-to-root and probing.
There are 24 attack categories in total in the training data. Data features
include things such as duration of the connection, protocol type, network
service, transferred bytes and failed login attempts. Even though the actual
labels (normal or intrusive) are known, this information is not used in the
rule creation phase. We use unsupervised learning for the framework, and
only use the actual labels for getting the performance metrics and evaluating
the results.

5.1. Feature extraction
The data that we use for testing and evaluation contains both continuous
and discrete values. This must be taken into account in the feature extrac-
tion phase. In network traﬃc context, features such as the number of bytes
transferred during a connection are continuous, and, e.g., used protocol type
in contrast is a discrete valued feature.

During the preprocessing phase, continuous feature values must be trans-
formed into discrete ones for the used rule extraction algorithm. Symbolic
discrete values can be left alone at this stage. Discretization is performed
using binning. We do this by acquiring the maximum and minimum values
of each continuous variable, and dividing the measurements into n bins using
equal intervals. The number of bins can be chosen according to the situation,
e.g. n = 3 or n = 10. There is a practical example in section 4. Let us now
present another example, where we assume that we have 3 bins for each vari-
able and there are 3 diﬀerent variables (f 1, f 2, f 3) and 3 data points (rows).
The number of the chosen bin for each measurement is placed in a feature
matrix:

f1 f2 f3
1
2
1
3
2
3
1
1
2

16

This kind of matrix is not yet usable for rule extraction.

In the ﬁnal
preprocessing phase, we binarize the matrix. This gives us a matrix that
uses the following format:

f1=1 f1=2 f1=3 f2=1 f2=2 f3=1 f3=3
0
0
1

1
0
1

1
1
0

0
1
0

1
0
0

0
0
1

0
1
0

For example, the ﬁrst column f1=1 means that if the chosen bin for the
ﬁrst variable is 1 on a certain row, the binary value in this binarized matrix
will be 1, otherwise 0. In other words, a column tells whether a speciﬁc vari-
able measurement belongs to a certain bin or not. This way we get nominal
values for all the features. Columns that would be all zeros can be omitted.
Now the matrix is in a format that can be directly used with the rule extrac-
tion algorithm. In addition, the rule extraction requires labeling information.
The process of acquiring this is explained in the following subsections.

5.2. Random sampling from the 10% KDD set
For initial testing to showcase the feasibility of the approach, 5,000 data
lines are randomly selected for testing and 5,000 for training, totaling 10,000
lines. The purpose of this initial experiment is to see if a rather limited
training sample can represent the whole data. The feature matrix is binned
and binarized as described previously. After this, dimensionality is reduced
using a diﬀusion map.

In order to perform the dimensionality reduction in an optimal way, the
value for parameter (cid:15) must be determined. Selecting (cid:15) for the diﬀusion map
is performed by selecting the value from the region between the extremes of a
log-log plot, as seen in Figure 5. The quality L is calculated by summing the
weight matrix, as explained in subsection 3.1. For each value of (cid:15), the sum
is calculated using 200 randomly selected samples from the training data.

The eigenvalues λk from the diﬀusion map method are presented in Fig-
ure 6. The ﬁrst eigenvalue is always 1 and is therefore ignored. Next three
are used in the analysis, since they seem to cover most of the data. The
eigengap is visible after them. The rest of the eigenvalues contain little in-
formation and are discarded. The underlying assumption of the analysis is
that the ﬁrst few eigenvalues and eigenvectors contain the most useful clus-
tering information.

17

Figure 5: Finding the optimal region for epsilon selection for the diﬀusion map algorithm.

After dimensionality reduction the data is clustered using k-means algo-
rithm. This step is believed to separate the normal data points from the
attacks. To evaluate the best number of clusters, clustering process is re-
peated with diﬀerent number of clusters, from 2 to 20. Figure 7 shows the
quality of clustering with the average silhouette metric. Out of the silhouette
values, the highest one should be selected. Based on this metric, the data is
divided into 7 clusters.

Figure 8 shows the obtained clusters after dimensionality reduction and
k-means clustering. The normal data is assumed to lie in cluster number 4,
and all the others are regarded as attacks for labeling purposes. Normally
we would assume that the largest cluster is the one containing normal data
points, meaning that whole process could be completed automatically. How-
ever, due to the fact that KDD Cup 99 dataset unnaturally contains more
intrusions than actual normal traﬃc, we had to choose the normal cluster
using manual inspection.

The clustering is assumed to represent the classiﬁcation of the training
dataset. This information is used to create labeling, which is needed for rule
creation phase. This is an example of how the rule extraction algorithm can
be used in an unsupervised manner, even though it is in principle based on
supervised learning. The conjunctive rules for this dataset are illustrated in
Figure 9. There are 18 rules for the normal class and 15 rules for the attack
class, 33 in total. This shows that the data can be represented and classiﬁed

18

10−810−610−410−21001021041061081010104.1104.2104.3104.4104.5104.6The weight matrix sum as a function of epsilonεLFigure 6: Eigenvalues of the diﬀusion map algorithm. The eigengap is visible between 4th
and 5th eigenvalue.

Table 2: Performance metrics for the 5,000 testing data lines.

Metric

Value %

Sensitivity (TPR)

FPR

Speciﬁcity (TNR)

Accuracy

Precision

Matthews corr. coef.

98.44

1.25

98.75

98.50

99.70

95.31

by a relatively small and simple set of rules.

Table 2 shows the performance metrics for the 5,000 training lines setup.
The confusion matrix is shown in Table 3. From these it can be seen that we
get a good intrusion detection accuracy with a relatively low number of false
alarms. It is important to note that Matthews correlation coeﬃcient usually
gives lower values than other commonly used metrics. Using the created
rules to classify new incoming traﬃc is very fast, which is the main beneﬁt
of this method. Furthermore, the rules reveal comprehensible information
about normal and intrusive data points.

19

5101520253000.10.20.30.40.50.60.70.80.91First 32 eigenvaluesNumber iValue λiFigure 7: Cluster number quality measured using silhouette.

Table 3: Confusion matrix for the 5,000 testing data lines.

predicted

normal

attack

l normal
a
u
t
c
a

attack

947

12

63

3,978

5.3. The whole 10% KDD set
For a more comprehensive experiment we use the whole KDD Cup 99 10%
dataset. A randomly selected subset of 5% (24,701 data points) is used for
training phase, leaving 469,320 data points for testing. The goal is to use
small training set size compared to the testing set to alleviate the scaling
of the diﬀusion map method. Data preprocessing, binning and binarization
as well as diﬀusion map dimensionality reduction and clustering are done
in the same way as described in the previous case. As earlier, the normal
clusters had to be identiﬁed using manual inspection. Rule extraction is
again performed using the conjunctive rule algorithm.

Table 4 shows the performance measures for the testing data. From the

20

0246810121416182000.10.20.30.40.50.60.70.80.91Number of clustersAverage silhouette valueSilhouette evaluation of cluster numbers. Best at 7.Figure 8: Found clusters mapped to the two ﬁrst coordinates in the low-dimensional space.

Table 4: Performance metrics for the whole 10% dataset.

Metric

Value %

Sensitivity (TPR)

FPR

Speciﬁcity (TNR)

Accuracy

Precision

Matthews corr. coef.

98.46

5.59

94.41

97.67

98.63

92.64

table we can see that the overall performance is not as good as in the previous
case. However, true positive rate is slightly better. The most important
metric to look at is the Matthews correlation coeﬃcient. The whole dataset
has more variability in it and is not as compact as the smaller one. For
these reasons the Matthew’s correlation coeﬃcient seems to be lower with
this dataset. There are also more false alarms. Table 5 shows the confusion
matrix, which supports the results in the performance metrics table. Note
that attacks are regarded as positive classiﬁcations.

The obtained results conﬁrm, using a bigger dataset, that a manifold
learning framework can perform intrusion detection adequately. Earlier re-
search supports the use of similar techniques (Zheng et al., 2009a,b; Yuancheng

21

−0.4−0.200.20.40.60.811.21.4−0.2−0.100.10.20.30.40.50.60.71st coordinate2nd coordinateDiffusion map and clustering  1234567Figure 9: Conjunctive rules. Each line represents one rule. Note that class 1 means normal
and class 9 intrusive.

Table 5: Confusion matrix for the whole 10% dataset, 469,320 testing data lines.

predicted

normal

attack

87,228

5,162

5,795

371,135

l normal
a
u
t
c
a

attack

et al., 2010). This also validates the assumption that a system based on con-
junctive rules can achieve promising performance. Next, we will investigate
real-life data.

5.4. Ruleset size
Training data size and characteristics aﬀect the size of the created conjunctive
ruleset. From Figure 10 we can see that the ruleset size does not increase
as fast as the amount of training data points. Because the algorithm is not
deterministic (random starting rule), it is also possible that the ruleset size
decreases. If the data points are very similar, the ruleset might converge and

22

Rules. Black = must not have, gray = any, white = must have.FeatureClass50100150200250300350111111111111111111999999999999999Figure 10: Training data amount eﬀect on rule set size.

additional rules are not created. This means that the ruleset size created in
training phase stays manageable even with bigger training data.

6. Real-world network data

Finally, we use real-world network server log data as a case study to describe
the feasibility of the approach. This is the most realistic scenario presented
in the article. This dataset does not contain labeling, and thus the diﬀusion
map is used to cluster and classify the data for ruleset evaluation. These
results have been explored earlier (Juvonen and Sipola, 2013). The dataset
comes from a real web server, which serves the company’s web services and
pages. The logs are received from Apache servers running in a company
network, and the lines are formatted as follows:

127.0.0.1 - -
[01/January/2012:00:00:01 +0300]
"GET /resource.php?
parameter1=value1&parameter2=value2
HTTP/1.1" 200 2680
"http://www.address.com/webpage.html"
"Mozilla/5.0 (SymbianOS/9.2;...)"

The log contains IP address, timestamp, HTTP request, server response

code and browser information.

23

01234567x 1042224262830323436384042Number of data pointsNumber of created rulesConjunctive ruleset size6.1. Feature extraction
In order to be able to use our methods to analyze the log data, textual log
lines must be transformed into numerical vectors. For this purpose, we use
In this context, an n-gram is a substring of
n-grams (Damashek, 1995).
characters obtained by sliding window with size n through the string. For
example, let’s assume that we have two strings, anomaly and analysis. If
we use bigrams (2-grams), we get the following two feature vectors that can
be placed into a matrix:

an no
1
1
0
1

om ma
1
1
0
0

al
1
1

ly na ys
0
0
1
1
1
1

si
0
1

is
0
1

For this study, bigrams are used. The frequencies of each individual
bigram are stored in a feature matrix, where each row corresponds to one log
line, as demonstrated in the example above. Longer n-grams could also be
used, but due to added columns for individual n-grams the feature matrix
becomes very large.

6.2. Training size eﬀect
The ﬁrst real-world dataset contains 4,292 log lines. We extract 490 unique
bigrams from the data. Accordingly, after preprocessing the resulting matrix
contains 4,292 rows and 490 columns. Because there is no existing label-
ing, the clustering analysis produces a label for each line, which identiﬁes it
as normal of anomalous. We select randomly 2,000 data points for ruleset
creation. The labeling information of these lines is used for the rule extrac-
tion process. This leaves us with 2,292 log lines that are not present during
ruleset learning phase, and these lines are used as a testing set which is clas-
siﬁed using the ruleset. Labels for testing set are only used for performance
evaluation, not the classiﬁcation process.

The ruleset for the ﬁrst real-world dataset contains 6 rules, 2 for classify-
ing detecting normal traﬃc and 4 for anomalies. Then we classify the testing
set using these rules. We discover that one anomalous sample is not covered
by any rule, and therefore not classiﬁed. The rest are correctly classiﬁed. All
the normal traﬃc falls under a single rule. In this case the system works with
almost 100% accuracy, which suggests that the amount of data is limited.

The second dataset contains 10,935 log lines. We use the same procedure
In this data, 414 unique bigrams are found,

as with the smaller dataset.

24

Figure 11: Optimal cluster number for k-means.

resulting in a matrix of size 10, 935 × 414. For ruleset learning phase, a small
number of data points are used. After dimensionality reduction, the number
of clusters k is determined as described in section 3.2. Figure 11 shows that
the best number of clusters is 4. Figure 12 shows all of the data points after
dimensionality reduction and clustering used for unsupervised labeling step.
As we can see from this visualization, cluster c4 contains clearly more points
than the others, which suggests that it represents the most common behavior
in the data.

We randomly select training data for the rule extraction algorithm. The
remaining lines are used for traﬃc classiﬁcation testing. With 500 randomly
selected training data points, the rule extraction algorithm ﬁnds a ruleset
that covers the data perfectly. This might suggest overlearning. Table 6
shows these results.
In order to generalize the ruleset, training with 100
randomly selected training data points was also performed. The confusion
matrix in this case is shown in Table 7. The smaller training dataset is
only about 1% of the whole data. This introduces some inaccuracies to
the classiﬁcation results. Because the real labeling is not known, no actual
performance metrics can be calculated.

6.3. Discussion of data selection
When using real-world data for training, it must be selected carefully be-
cause it is possible to insert delusive information that will result in improper

25

123456789100.40.50.60.70.80.91Number of clustersAverage silhouette valueSilhouette evaluation of cluster numbers. Best at 4.Figure 12: Two-dimensional visualization of the diﬀusion map of the whole dataset.

Table 6: Confusion matrix for log data with 500 training samples.

predicted

class 1

class 2

class 3

class 4

class 1

697

0

l
a
u
t
c
a

class 2

class 3

class 4

0

0

0

1,191

0

0

0

0

539

0

0

0

0

8,008

Table 7: Confusion matrix for log data with 100 training samples.

predicted

class 1

class 2

class 3

class 4

class 1

725

0

l
a
u
t
c
a

class 2

class 3

class 4

0

43

0

1,236

0

55

0

0

519

0

0

0

0

8,257

26

−0.200.20.40.60.811.21.41.6−1−0.8−0.6−0.4−0.200.20.41st coordinate2nd coordinateDiffusion map  1 N=7322 N=12493 N=5674 N=8387training. Therefore limits exist for automatic signature generation in this
setting (Newsome et al., 2006; Chung and Mok, 2007; Venkataraman et al.,
2008). With intelligent selection of data it is possible to make the training
phase more robust against attacks. One way to overcome this is to delib-
erately insert malicious traﬃc by the security experts to the training data.
Alternatively, the training data can be pre-screened before training to ensure
its authenticity (Newsome et al., 2006). In this case the data was as given.
We had no control on the data collection and it contains real attacks.

7. Conclusion

We present an anomaly detection system based on the concept of conjunctive
rules, which could be used for automated big data monitoring. In particular,
we use network log data to demonstrate the feasibility of the system. To
achieve this, the structure of the data is learned in an unsupervised manner
using the diﬀusion map dimensionality reduction methodology and labeling
is acquired using clustering. Finally, conjunctive rules are extracted using
the learned data classiﬁcation.

Performance of the presented system was tested using KDD Cup 99
dataset, and the practical feasibility was evaluated with real-world data.
The results are satisfactory even with a limited amount of training data.
In the KDD Cup dataset, the small random subset has a higher Matthews
correlation coeﬃcient because the small sample does not include the whole
variety of attacks, while the whole 10% dataset is more diverse. The KDD
Cup dataset has its own issues, for example the high amount of attacks
compared to normal traﬃc. This makes automatic anomaly detection very
challenging. The rule learning process achieves good performance with the
real-world dataset. However, because the actual attack labeling is not known,
performance metrics cannot be calculated. Nonetheless, manual inspection
suggests that normal and anomalous traﬃc are separated, and we conﬁrm
that actual intrusions are detected.

The system is aimed to be a tool for analyzing big datasets and detecting

anomalies. It has the following main beneﬁts:

• The resulting ruleset classiﬁes traﬃc very eﬃciently. This enables big
data classiﬁcation in real-time. The traﬃc classiﬁcation phase is much
faster than the preceding rule creation phase.

27

• The unsupervised learning phase enables label creation without prior

information about the data.

• The system aims to explain the classiﬁcation result of a non-linear black
box algorithm with rules that are easy to understand and practical for
domain experts.

• The resulting rule-based classiﬁer is simple to use and to implement,
since only speciﬁc features need to be extracted and it is easy to com-
pare the feature vectors to the conjunctive rules.

The main limitation of the system is the fact that training data has to
represent the structure of the new incoming network traﬃc as accurately as
possible. If the training data diﬀers signiﬁcantly from the current network
data, created rules will not work in a satisfactory way. This can be prevented
by using periodical training with new, more comprehensive data. Another
concern is the performance of the learning phase. A bigger training set might
give better results but training will take more time. However, rule creation
is performed only periodically, while the fast traﬃc classiﬁcation is done in
real-time.

For future research, more testing with diﬀerent kinds of data should be
done in order to validate the feasibility of the rule extraction framework. So
far we have used actual server HTTP log data and KDD Cup 99 dataset.
The system could also be expanded to suit other cyber security and big data
scenarios, not just network intrusion detection. This involves modiﬁcations
In addition,
to preprocessing but does not change the other components.
some parts of the algorithms can be easily parallelized. This speeds up the
processing and makes the system more scalable.

Acknowledgment

This research was partially supported by the Foundation of Nokia Corpo-
ration and the Finnish Foundation for Technology Promotion. Thanks are
extended to Kilosoft Oy and Pardco Group Oy. The authors thank Profes-
sors Amir Averbuch, Timo H¨am¨al¨ainen and Tapani Ristaniemi for support
and guidance.

28

References

Bach FR, Jordan MI. Learning spectral clustering, with application to speech
separation. The Journal of Machine Learning Research 2006;7:1963–2001.

Baldi P, Brunak S, Chauvin Y, Andersen CAF, Nielsen H. Assessing the
accuracy of prediction algorithms for classiﬁcation: an overview. Bioinfor-
matics 2000;16(5):412–24.

Barakat N, Bradley AP. Rule extraction from support vector machines: A

review. Neurocomputing 2010;74(1–3):178–90.

Barakat N, Diederich J. Learning-based rule-extraction from support vector
In: The 14th International Conference on Computer Theory

machines.
and applications ICCTA’2004. 2004. .

Bekkerman R, Bilenko M, Langford J. Scaling up machine learning: Parallel

and distributed approaches. Cambridge University Press, 2012.

Brown C, Davis H. Receiver operating characteristics curves and related
decision measures: A tutorial. Chemometrics and Intelligent Laboratory
Systems 2006;80(1):24–38.

Callegari C, Gazzarrini L, Giordano S, Pagano M, Pepe T. A novel PCA-
based network anomaly detection. In: Communications (ICC), 2011 IEEE
International Conference on. IEEE; 2011. p. 1–5.

Chandola V, Banerjee A, Kumar V. Anomaly detection: A survey. ACM

Computing Surveys (CSUR) 2009;41(3):15.

Chu CT, Kim SK, Lin YA, Yu Y, Bradski G, Ng AY, Olukotun K. Map-
reduce for machine learning on multicore. In: NIPS. volume 6; 2006. p.
281–8.

Chung SP, Mok AK. Advanced allergy attacks: Does a corpus really help?
In: Recent Advances in Intrusion Detection. Springer; 2007. p. 236–55.

Coifman R, Lafon S, Lee A, Maggioni M, Nadler B, Warner F, Zucker S. Ge-
ometric diﬀusions as a tool for harmonic analysis and structure deﬁnition
of data: Diﬀusion maps. Proceedings of the National Academy of Sciences
of the United States of America 2005;102(21):7426–31.

29

Coifman R, Shkolnisky Y, Sigworth F, Singer A. Graph laplacian tomography
from unknown random projections. Image Processing, IEEE Transactions
on 2008;17(10):1891–9.

Coifman RR, Lafon S. Diﬀusion maps. Applied and Computational Harmonic

Analysis 2006;21(1):5–30.

Craven MW, Shavlik JW. Using sampling and queries to extract rules from
trained neural networks. In: In Proceedings of the Eleventh International
Conference on Machine Learning. Morgan Kaufmann; 1994. p. 37–45.

Craven MW, Shavlik JW. Extracting tree-structured representations of
trained networks. Advances in neural information processing systems
1996;:24–30.

Damashek M. Gauging similarity with n-grams: Language-independent cat-

egorization of text. Science 1995;267(5199):843.

David G. Anomaly Detection and Classiﬁcation via Diﬀusion Processes in

Hyper-Networks. Ph.D. thesis; Tel-Aviv University; 2009.

David G, Averbuch A. Hierarchical data organization, clustering and denois-
ing via localized diﬀusion folders. Applied and Computational Harmonic
Analysis 2011;.

David G, Averbuch A, Coifman R. Hierarchical clustering via localized dif-
fusion folders. In: Proceedings of the Association for the Advancement of
Artiﬁcial Intelligence (AAAI) Fall Symposium Series. 2010. .

Davis JJ, Clark AJ. Data preprocessing for anomaly based network intrusion

detection: A review. Computers & Security 2011;30(6–7):353–75.

Fawcett T. An introduction to ROC analysis. Pattern recognition letters

2006;27(8):861–74.

Frank A, Asuncion A. UCI machine learning repository. 2010. URL: http:

//archive.ics.uci.edu/ml.

d’Avila Garcez A, Broda K, Gabbay D. Symbolic knowledge extraction
from trained neural networks: A sound approach. Artiﬁcial Intelligence
2001;125(1):155–207.

30

Garc´ıa-Teodoro P, D´ıaz-Verdejo J, Maci´a-Fern´andez G, V´azquez E.
Anomaly-based network intrusion detection: Techniques, systems and
challenges. Computers & Security 2009;28(1–2):18–28.

Goyal M, Aggarwal A. Composing signatures for misuse intrusion detection
system using genetic algorithm in an oﬄine environment. In: Meghanathan
N, Nagamalai D, Chaki N, editors. Advances in Computing and Informa-
tion Technology. Springer Berlin Heidelberg; volume 176 of Advances in
Intelligent Systems and Computing; 2012. p. 151–7.

Huysmans J, Baesens B, Vanthienen J. Using rule extraction to improve
the comprehensibility of predictive models. Available at SSRN: http:
//ssrn.com/abstract=961358; 2006.

Jain AK. Data clustering: 50 years beyond k-means. Pattern Recognition

Letters 2010;31(8):651–66.

Jain AK, Dubes RC. Algorithms for clustering data. Prentice Hall., 1988.

Jolliﬀe I. Principal component analysis. Wiley Online Library, 2005.

Juvonen A, Sipola T. Adaptive framework for network traﬃc classiﬁcation
using dimensionality reduction and clustering. In: Ultra Modern Telecom-
munications and Control Systems and Workshops (ICUMT), 2012 4th In-
ternational Congress on. St. Petersburg, Russia: IEEE; 2012. p. 274–9.

Juvonen A, Sipola T. Combining conjunctive rule extraction with diﬀusion
In: The Eighteenth IEEE Sym-
maps for network intrusion detection.
posium on Computers and Communications (ISCC 2013). Split, Croatia:
IEEE; 2013. .

Kim J, Bentley PJ, Aickelin U, Greensmith J, Tedesco G, Twycross J. Im-
mune system approaches to intrusion detection–a review. Natural com-
puting 2007;6(4):413–66.

Kolias C, Kambourakis G, Maragoudakis M. Swarm intelligence in intrusion

detection: A survey. computers & security 2011;30(8):625–42.

Lafon S, Lee AB. Diﬀusion maps and coarse-graining: A uniﬁed framework
for dimensionality reduction, graph partitioning, and data set parameteri-
zation. Pattern Analysis and Machine Intelligence, IEEE Transactions on
2006;28(9):1393–403.

31

Lee J, Verleysen M. Nonlinear dimensionality reduction. Springer Verlag,

2007.

Li L, Zhang G, Nie J, Niu Y, Yao A. The application of genetic algorithm
to intrusion detection in MP2P network. In: Tan Y, Shi Y, Ji Z, editors.
Advances in Swarm Intelligence. Springer Berlin Heidelberg; volume 7331
of Lecture Notes in Computer Science; 2012. p. 390–7.

Liao HJ, Lin CHR, Lin YC, Tung KY.

Intrusion detection system: A
comprehensive review. Journal of Network and Computer Applications
2013;36(1):16–24.

von Luxburg U. A tutorial on spectral clustering. Statistics and Computing

2007;17:395–416.

Mahoney MV, Chan PK. An analysis of the 1999 darpa/lincoln laboratory
evaluation data for network anomaly detection. In: Recent Advances in
Intrusion Detection. Springer; 2003. p. 220–37.

Martens D, Huysmans J, Setiono R, Vanthienen J, Baesens B. Rule extrac-
tion from support vector machines: An overview of issues and application
in credit scoring. In: Diederich J, editor. Rule Extraction from Support
Vector Machines. Springer Berlin Heidelberg; volume 80 of Studies in Com-
putational Intelligence; 2008. p. 33–63.

McHugh J. Testing intrusion detection systems: a critique of the 1998 and
1999 DARPA intrusion detection system evaluations as performed by Lin-
coln laboratory. ACM transactions on Information and system Security
2000;3(4):262–94.

Molina M, Paredes-Oliva I, Routly W, Barlet-Ros P. Operational experiences
with anomaly detection in backbone networks. Computers & Security
2012;31(3):273–85.

Mukkamala S, Sung AH, Abraham A. Intrusion detection using an ensemble
of intelligent paradigms. Journal of Network and Computer Applications
2005;28(2):167–82.

Nadler B, Lafon S, Coifman R, Kevrekidis I. Diﬀusion maps, spectral clus-
tering and reaction coordinates of dynamical systems. Applied and Com-
putational Harmonic Analysis 2006;21(1):113–27.

32

Newsome J, Karp B, Song D. Polygraph: Automatically generating sig-
In: Security and Privacy, 2005 IEEE

natures for polymorphic worms.
Symposium on. IEEE; 2005. p. 226–41.

Newsome J, Karp B, Song D. Paragraph: Thwarting signature learning by
training maliciously. In: Recent advances in intrusion detection. Springer;
2006. p. 81–105.

N´u˜nez H, Angulo C, Catal`a A. Rule extraction from support vector machines.
In: In European Symposium on Artiﬁcial Neural Networks Proceedings.
2002. p. 107–12.

Patcha A, Park JM. An overview of anomaly detection techniques: Ex-
isting solutions and latest technological trends. Computer Networks
2007;51(12):3448–70.

Prasse P, Sawade C, Landwehr N, Scheﬀer T. Learning to identify regular
expressions that describe email campaigns. arXiv preprint arXiv:12064637
2012;.

Ramadas M, Ostermann S, Tjaden B. Detecting anomalous network traﬃc
with self-organizing maps. In: Vigna G, Jonsson E, Kruegel C, editors.
Recent Advances in Intrusion Detection. Springer; 2003. p. 36–54.

Ringberg H, Soule A, Rexford J, Diot C. Sensitivity of PCA for traﬃc
anomaly detection. ACM SIGMETRICS Performance Evaluation Review
2007;35(1):109–20.

Rousseeuw PJ. Silhouettes: A graphical aid to the interpretation and vali-
dation of cluster analysis. Journal of Computational and Applied Mathe-
matics 1987;20(0):53 – 65.

Ryman-Tubb N, d’Avila Garcez A. SOAR – Sparse oracle-based adaptive
rule extraction: Knowledge extraction from large-scale datasets to detect
credit card fraud. In: Neural Networks (IJCNN), The 2010 International
Joint Conference on. IEEE; 2010. p. 1–9.

Sabhnani M, Serpen G. Why machine learning algorithms fail in misuse
detection on kdd intrusion detection data set. Intelligent Data Analysis
2004;8(4):403–15.

33

Scarfone K, Mell P. Guide to intrusion detection and prevention systems

(IDPS). NIST Special Publication 2007;800(2007):94.

Setiono R, Baesens B, Mues C. Recursive neural network rule extraction
for data with mixed attributes. Neural Networks, IEEE Transactions on
2008;19(2):299 –307.

Sipola T, Juvonen A, Lehtonen J. Anomaly detection from network logs using
diﬀusion maps. In: Iliadis L, Jayne C, editors. Engineering Applications
of Neural Networks. Springer Boston; volume 363 of IFIP Advances in
Information and Communication Technology; 2011. p. 172–81.

Sipola T, Juvonen A, Lehtonen J. Dimensionality reduction framework for
detecting anomalies from network logs. Engineering Intelligent Systems
2012;20:87–97.

Sommer R, Paxson V. Outside the closed world: On using machine learning
for network intrusion detection. In: Security and Privacy (SP), 2010 IEEE
Symposium on. IEEE; 2010. p. 305–16.

Tan P, Steinbach M, Kumar V. Cluster analysis: Basic concepts and algo-

rithms. Introduction to data mining 2006;:487–568.

Tavallaee M, Bagheri E, Lu W, Ghorbani AA. A detailed analysis of the
KDD CUP 99 data set. In: Proceedings of the Second IEEE Symposium
on Computational Intelligence for Security and Defence Applications 2009.
2009. .

Tran Q, Duan H, Li X. One-class support vector machine for anomaly net-
work traﬃc detection. China Education and Research Network (CER-
NET), Tsinghua University, Main Building 2004;310.

Vasiliadis G, Antonatos S, Polychronakis M, Markatos EP, Ioannidis S.
Gnort: High performance network intrusion detection using graphics pro-
cessors. In: Recent Advances in Intrusion Detection. Springer; 2008. p.
116–34.

Venkataraman S, Blum A, Song D. Limits of learning-based signature gen-

eration with adversaries. 2008.

34

Wu SX, Banzhaf W. The use of computational intelligence in intrusion de-
tection systems: A review. Applied Soft Computing 2010;10(1):1–35.

Yuancheng L, Pan L, Runhai J. An intrusion detection method based on
LLE and BVM. In: Information Networking and Automation (ICINA),
2010 International Conference on. volume 2; 2010. p. 264–7.

Zhao W, Ma H, He Q. Parallel k-means clustering based on mapreduce. In:

Cloud Computing. Springer; 2009. p. 674–9.

Zheng Km, Qian X, Wang Pc. Dimension reduction in intrusion detection us-
ing manifold learning. In: Computational Intelligence and Security, 2009.
CIS’09. International Conference on. IEEE; volume 2; 2009a. p. 464–8.

Zheng Km, Qian X, Zhou Y, Jia Lj. Intrusion detection using isomap and
In: Artiﬁcial Intelligence and Computational
support vector machine.
Intelligence, 2009. AICI’09. International Conference on. IEEE; volume 3;
2009b. p. 235–9.

35

