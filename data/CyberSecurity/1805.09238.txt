8
1
0
2

y
a
M
3
2

]
L
M

.
t
a
t
s
[

1
v
8
3
2
9
0
.
5
0
8
1
:
v
i
X
r
a

Highway State Gating for Recurrent Highway
Networks: improving information ﬂow through
time

Ron Shoham and Haim Permuter

Ben-Gurion University, beer-sheva 8410501, Israel
ronshoh@post.bgu.ac.il
haimp@bgu.ac.il

Abstract. Recurrent Neural Networks (RNNs) play a major role in the
ﬁeld of sequential learning, and have outperformed traditional algorithms
on many benchmarks. Training deep RNNs still remains a challenge, and
most of the state-of-the-art models are structured with a transition depth
of 2-4 layers. Recurrent Highway Networks (RHNs) were introduced in
order to tackle this issue. These have achieved state-of-the-art perfor-
mance on a few benchmarks using a depth of 10 layers. However, the
performance of this architecture suﬀers from a bottleneck, and ceases to
improve when an attempt is made to add more layers. In this work, we
analyze the causes for this, and postulate that the main source is the way
that the information ﬂows through time. We introduce a novel and simple
variation for the RHN cell, called Highway State Gating (HSG), which
allows adding more layers, while continuing to improve performance. By
using a gating mechanism for the state, we allow the net to ”choose”
whether to pass information directly through time, or to gate it. This
mechanism also allows the gradient to back-propagate directly through
time and, therefore, results in a slightly faster convergence. We use the
Penn Treebank (PTB) dataset as a platform for empirical proof of con-
cept. Empirical results show that the improvement due to Highway State
Gating is for all depths, and as the depth increases, the improvement also
increases.

Keywords: Deep-Learning · Machine-Learning · Recurrent-Highway-
Network · Recurrent-Neural-Networks · Sequential-Learning · Deep RNN.

1

Introduction

Training very deep neural networks has become very common in the last few
years. Both theoretical and empirical evidence points to the fact that deeper
networks can represent more eﬃciently speciﬁc functions (Bengio et al. [1], Bian-
chini and Scarselli [2]). Some commonly used architectures for deep feed-forward
networks are Resnet[6], Highway Networks [17] and Dense-Net[9]. These archi-
tectures can be structured with tens, and sometimes even hundreds of layers.
Unfortunately, training a very deep Recurrent Neural Network (RNN) still re-
mains a challenge.

 
 
 
 
 
 
2

R. Shoham, H. Permuter

Zilly et al. [19] introduced the Recurrent Highway Network (RHN) in order
to address this issue. Its main diﬀerence from previous deep RNN architectures,
was incorporating Highway layers inside the recurrent transition. By using a
transition depth of 10 Highway layers, RHN managed to achieve state-of-the-
art results on several benchmarks of word and character prediction. However,
increasing the transition depth of a similar RHN, does not improve the results
signiﬁcantly.

In this paper, we ﬁrst analyze the reasons for this phenomena. Based on the
results of our analysis, we suggest a simple solution which adds a non-signiﬁcant
number of parameters. This variant is called a Highway State Gating cell or a
HSG. By using the HSG mechanism, the new state is generated by a weighted
combination of the previous state and the output of the RHN cell. The main idea
behind the HSG cell is to provide a fast route for the information to ﬂow through
time. That way, we also provide a shorter path for the back-propagation through
time (BPTT). This enables the use of a deeper transition depth, together with
signiﬁcant performance improvement on a widely used benchmark.

2 Related Work

Gated-Recurrent-Units (GRUs) [3] were suggested in order to reduce the number
of parameters of the traditional and commonly used Long-Short-Term-Memory
(LSTM) cell (Hochreiter and Schmidhuber [8]). Similarly to HSG, in GRUs the
new state is a weighted sum of the previous state and a non-linear transition
of the current input and the previous state. The main diﬀerence is that the
transition is of a depth of a single layer and, therefore, less robust.

Kim et al. [11] introduced a diﬀerent variant of the LSTM cell which is
inspired by Resnet[6]. They proposed adding to the LSTM cell a residual con-
nection from its input to the reset gate projection output. By that they allowed
another route for the information to ﬂow directly through. They managed to
train a net of 10 residual LSTM layers which outperformed other architectures.
In their work, they focused on the way that the information passes through
layers in the feed-forward manner, and not on the way it passes through time.
Wang and Tian [18] used residual connections in time. In their work they
talked about the way information passes through time. They managed to improve
performance on some benchmarks, while reducing the number of parameters.
The diﬀerence is that they needed to work with a ﬁxed residual length that is
a hyper-parameter. Also, their work focused on cells with a one layer transition
depth.

Another article, relating to Zoneout regularization (Krueger et al. [12]) also
relates to information ﬂow through time. The authors introduced a new regu-
larization method for RNNs, where the idea is very similar to dropout[16]. The
diﬀerence is that the dropped neurons in the state vectors get their values in
the former time-step, instead of being zeroed. They mentioned that one of the
beneﬁts of this method is that the BPTT skips a time-step on its path back
through time. In our work, there is a direct (weighted) connection between the

Highway State Gating for RHN: improving information ﬂow through time

3

current state and the former one, which is used similarly both for training and
inference.

Another relevant issue is the slowness regularizers (Hinton [7], F¨oldi´ak [4],
Luciw and Schmidhuber [13], Jonschkowski and Brock [10], Merity et al. [15])
which add a penalty for large changes in state through time. In our work we do
not add such a penalty, but we allow a direct route for the state to pass through
time-steps, and therefore we ’encourage’ the state not to change when it is not
needed.

3 Revisiting Vanilla Recurrent Highway Networks

Let L be the transition depth of the RHN cell, and x[t] ∈ Rm be the cell’s input at
time t. Let WH,T,C ∈ Rn×m and RHl,Tl,Cl ∈ Rn×n represent the weight matrices
of H nonlinear transforms and the T and C gates at layer l ∈ {1, . . . , L}. The
biases are denoted by bHl,Tl,Cl ∈ Rn, and let s[t]
l denote the intermediate output
at layer l at time t, with s[t]
. The gates T and C utilize a sigmoid
(σ) non-linearity and ”·” denotes element-wise multiplication. An RHN layer is
described by

0 = s[t−1]

L

l = h[t]
s[t]

l

where

· t[t]

l + s[t]

l−1 · c[t]

l

,

h[t]
l = tanh(WH x[t]I
t[t]
σ(WT x[t]I
l =
c[t]
σ(WC x[t]I
l =

{l=1} + RHl s[t]
{l=1} + RTl s[t]
{l=1} + RCls[t]

l−1 + bHl),
l−1 + bTl),
l−1 + bCl),

(1)

(2)

(3)

(4)

and I is the indicator function. A very common variant for this is coupling gate
C to gate T , i.e. C = 1 − T . Figure 1 illustrates the RHN cell.

PSfrag replacements

l = 1

l = 2

l = L

H

h[t]

1

c[t]

1

t[t]

1

s[t]

1

s[t−1]

L

x[t]

s[t]
L−1

s[t]

L

Fig. 1. Schematic showing RHN cell computation. The Feed-Forward route goes
from bottom to top through L stacked Highway layers. On the right side there is the
memory unit, followed by the recurrent connection.

4

R. Shoham, H. Permuter

According to Zilly et al. [19], one of the main advantages of using deep RHN
instead of stacked RNNs, is the path length. While the path length of L stacked
RNNs from time t to time t + T is L + T − 1 (ﬁgure 2), the path length of a RHN
of depth L is L × T (ﬁgure 3). The high recurrence depth can add signiﬁcantly
higher modeling power.

PSfrag replacements
layer L

layer 2

layer 1

PSfrag replacements

Fig. 2. The ﬁgure illustrates an unfolded RNN with L stacked layers. Here the path
length from time t to time t + T is L + T − 1.

xt

t

xt+1

xt+T

t + 1

t + T

s[t]

L

layer L

s[t+1]

L

layer L

layer 1

layer 1

s[t−1]

L

xt

t

xt+1

t + 1

s[t+T ]

L

layer L

layer 1

xt+T

t + T

Fig. 3. The ﬁgure illustrates an unfolded RHN with L layers. Here the path length
from time t to time t + T is L × T .

We believe that its power might, sometimes, also be its weakness. Let us
examine a case where information that is relevant for a large number of time
steps is given at time t; for example in stocks forecasting, where we expect a
sharp movement to occur in the next few time steps. We would like the state to
remain the same until the event happens (unless any dramatic event changes the
forecast). In this case, we probably prefer the net state to remain stable without
dramatic changes. However, when using a deep RHN, the information must pass
through many layers, and that might cause an unwanted change of the state.
For example, with a RHN of depth 30, the input state at time t has to pass 300
layers in order to propagate 10 time steps. To the best of our knowledge, there
is no use of a feed-forward Highway Network of this depth in any ﬁeld. This fact

Highway State Gating for RHN: improving information ﬂow through time

5

also aﬀects the vanishing gradient issue using BPTT. The fact that the gradient
needs to back-propagate through hundreds of layers causes it to vanish and not
be eﬀective. The empirical results support our assumption, and it seems like a
performance bottleneck occurs when we use deeper nets.

4 Highway State Gate in time

We suggest a simple, yet eﬃcient, solution for the depth-performance bottleneck
issue. Let WR,F ∈ Rn×n represent the weight matrices, and let bG ∈ Rn be a
bias vector. Let s[t]
L represent the output of the RHN cell at time t. ˆs[t] is the
output of the HSG cell at time t. The HSG cell is described by

where

ˆs[t] = g · ˆs[t−1] + (1 − g) · s[t]
L ,

g[t] = σ(WR ˆs[t−1] + WF s[t]

L + bG).

(5)

(6)

A scheme of the HSG cell and an unfolded RHN with HSG is depicted in ﬁgure
4 and ﬁgure 5, respectively. The direct outcome of adding an HSG cell is giving
the information an alternative and fast route to ﬂow through time.

ˆs[t]

g

G

1 − g

PSfrag replacements

ˆs[t−1]

s[t]

L

Fig. 4. The ﬁgure illustrates a zoom into the HSG cell.

Since gate g utilizes a Sigmoid, its values are in the range [0, 1]. When g = 0,
i.e. HSG is closed, ˆs[t] = s[t]
L . When G = 1, i.e. the gate is opened, ˆs[t] = ˆs[t−1].
In the ﬁrst case, the net functions as a vanilla RHN. In this case the information
from the former state passes only through the functionality of the RHN. This

PSfrag replacements

6

R. Shoham, H. Permuter

ˆs[t−1]

ˆs[t]
HSG
s[t]

L

layer L

ˆs[t+1]

HSG
s[t+1]

L

layer L

layer 1

layer 1

ˆs[t−1]

xt

t

xt+1

t + 1

ˆs[t+T ]

HSG
s[t+L]

L

layer L

layer 1

xt+T

t + T

Fig. 5. A macro scheme of an unfolded RHN with HSG cell. The state feeds both the
RHN and the next time-step HSG cell.

means that the functionality of a regular RHN can be achieved easily even after
stacking the HSG layer.

One of the strengths of this architecture is that each state neuron has its
own stand-alone gate. This means that some of the neurons can pass informa-
tion easily through many time-steps, whereas other neurons learn short time
dependencies.

Now let us examine the example we mentioned above, when using RHN with
the HSG cell. The net depth is 30, and a state needs to propagate 10 time-steps.
In this case, the state has multiple routes to propagate through. The propagation
lengths are now 10 + 30j, with j ∈ {0, 1 . . . 10}. This means that the information
has multiple routes, and even if we use a really deep net, it still has a short path
to ﬂow through. For this reason, we expect our variant to enable training deeper
RHNs more eﬃciently. The results below support our claim.

5 Results

Our experiments study the beneﬁt of adding depth to a RHN with and without
stacking HSG cells at its output. We conducted our experiments on the Penn
Treebank (PTB) benchmark.

PTB: The Penn Treebank1, presented by Marcus et al. [14], is a well known
data set for experiments in the ﬁeld of language modeling. The goal is predicting
the next word at each time step, based on the past. Its vocabulary size is 10k
unique words. All words that are not in the vocabulary are labeled to a single
token. The database is structured of 929k training words, 73k validation words,
and 82k test words.

1 http://www.ﬁt.vutbr.cz/imikolov/rnnlm/simple-examples.tgz

Highway State Gating for RHN: improving information ﬂow through time

7

We used a hidden size of 830, similarly to that used by Zilly et al. [19]. For
regularization, we use variational dropout [5], and L2 weight decay. The learning
rate exponentially decreased at each epoch. An initial bias of −2.5 was used for
both the RHN and the HSG gates. That way, the gates are closed at the begin-
ning of training. We tried RHN depths from {10, 20, 30, 40}. Results are shown
in table 1. It can be well seen from the results that a performance bottleneck oc-
curs when adding more layers to the vanilla RHN. However, adding more layers
to the RHN network with the HSG cell results in a steady improvement. Figure
6 also illustrates the diﬀerence between both architectures during training. It
can be seen that not only does the net with HSG achieve better results, it also
converges a bit faster than the vanilla one. Another interesting aspect is the
histogram of the gate values of the HSG cell in ﬁgure 8. It can be seen that most
of the gates are usually closed (small valued). However, in a signiﬁcant number
of cases the gates open, which means that the model passes a very similar state
to the next time step.

Table 1. Single RHN model test and validation perplexity of the PTB dataset

Validation set
with HSG

RHN
depth=10 67.5
depth=20 65.6
depth=30 64.8
depth=40 64.7

Test set

w/o HSG with HSG w/o HSG
67.9
66.4
66.4
66.7

65.0
62.9
62.0
61.7

65.4
63.2
63.4
63.6

*Note that the HSG is more signiﬁcant as the depth of the RHN increases.

perplexity using RHN with depth 30

with HSG
regular

90

85

80

75

70

65

y
t
i
x
e
p
r
e
p

l

d

i
l

a
v

50

100

150

200

250

300

with HSG
regular

y
t
i
x
e
p
r
e
p

l

t
s
e
t

85

80

75

70

65

50

100

150

200

250

300

epochs

Fig. 6. Comparison of the learning curve between RHN with (green) and without (red)
HSG cell. The upper and the lower graphs show the perplexity on the validation and
test sets respectively.

 
 
8

R. Shoham, H. Permuter

Histogram of state gates values

1600

1400

1200

1000

800

600

400

200

0

0

0.1

0.2

0.3

0.4
0.6
0.5
state gates values

0.7

0.8

0.9

1

Fig. 7. Histogram of HSG cell gates values. The values were drawn from a trained
RHN of depth 30, with a hidden size of 830. There are 66400 values from a 80 random
time steps. The gates utilize a Sigmoid function and, therefore, the values are in the
range of [0, 1]. We see that in most of the cases the gate values are relatively low, which
means that the state gates are closed, and the new state is generated in a feed-forward
manner. However, for a substantial number of times, the values are high, which means
that the information ﬂows directly through time.

l

y
t
i
x
e
p
r
e
P

65.5

65

64.5

64

63.5

63

62.5

62

61.5

61

10

Test set Perplexity vs depth

with HSG
Without HSG

15

20

25
depth

30

35

40

Fig. 8. Graph of Perplexity vs depth of the RHN over the test set with (blue) and
without(orange) HSG cell. This ﬁgure illustrates the depth-performance bottleneck
phenomena. It can be seen that by the depth of 20 layers both architectures give
similar results. However, when we stack more layers, the vanilla RHN stops improving
(and even deteriorating), whereas RHN with HSG cell keeps improving.

Highway State Gating for RHN: improving information ﬂow through time

9

6 Conclusion

In this work, we revisit a widely used RNN model. We analyze its limits and
issues, and propose a variant for it called Highway State Gate (HSG). The main
idea behind HSG is to generate an alternative fast route for the information to
ﬂow through time. The HSG uses a gating mechanism to assemble a new state
out of a weighted sum of the former state and the RHN output. We show that
when using our method, training deeper nets results in better performance. To
the best of our knowledge, this is the ﬁrst time in the ﬁeld of Recurrent Nets
that adding layers to this scale resulted in a steady improvement.

Bibliography

[1] Yoshua Bengio, Yann LeCun, et al. Scaling learning algorithms towards ai.

Large-scale kernel machines, 34(5):1–41, 2007.

[2] Monica Bianchini and Franco Scarselli. On the complexity of neural net-
work classiﬁers: A comparison between shallow and deep architectures.
IEEE transactions on neural networks and learning systems, 25(8):1553–
1565, 2014.

[3] Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bahdanau, and Yoshua
Bengio. On the properties of neural machine translation: Encoder-decoder
approaches. arXiv preprint arXiv:1409.1259, 2014.

[4] Peter F¨oldi´ak. Learning invariance from transformation sequences. Neural

Computation, 3(2):194–200, 1991.

[5] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application
of dropout in recurrent neural networks. In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems 29, pages 1019–1027. Curran Associates, Inc., 2016.
[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual
learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.
[7] Geoﬀrey E Hinton. Connectionist learning procedures. In Machine Learn-

ing, Volume III, pages 555–610. Elsevier, 1990.

[8] Sepp Hochreiter and J?rgen Schmidhuber. Long short-term memory. Neural

Computation, 1997.

[9] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten.
Densely connected convolutional networks. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, volume 1, page 3, 2017.
[10] Rico Jonschkowski and Oliver Brock. Learning state representations with

robotic priors. Autonomous Robots, 39(3):407–428, 2015.

[11] Jaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. Residual lstm: De-
sign of a deep recurrent architecture for distant speech recognition. arXiv
preprint arXiv:1701.03360, 2017.

[12] David Krueger, Tegan Maharaj, J´anos Kram´ar, Mohammad Pezeshki, Nico-
las Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron
Courville, and Chris Pal. Zoneout: Regularizing rnns by randomly pre-
serving hidden activations. arXiv preprint arXiv:1606.01305, 2016.

[13] Matthew Luciw and Juergen Schmidhuber. Low complexity proto-value
function learning from sensory observations with incremental slow feature
analysis. In International Conference on Artiﬁcial Neural Networks, pages
279–287. Springer, 2012.

[14] Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.
Building a large annotated corpus of english: The penn treebank. Com-
put. Linguist., 19(2):313–330, June 1993. ISSN 0891-2017.

[15] Stephen Merity, Bryan McCann, and Richard Socher. Revisiting activation

regularization for language rnns. arXiv preprint arXiv:1708.01009, 2017.

Highway State Gating for RHN: improving information ﬂow through time

11

[16] Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and
Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Research, 15:1929–1958,
2014. URL http://jmlr.org/papers/v15/srivastava14a.html.

[17] Rupesh Kumar Srivastava, Klaus Greﬀ, and J¨urgen Schmidhuber. Highway

networks. arXiv preprint arXiv:1505.00387, 2015.

[18] Yiren Wang and Fei Tian. Recurrent residual learning for sequence clas-
siﬁcation. In Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing, pages 938–943, 2016.

[19] Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn?k, and
arXiv preprint

Recurrent highway networks.

J?rgen Schmidhuber.
arXiv:1607.03474, 2016.

