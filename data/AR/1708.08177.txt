7
1
0
2

g
u
A
8
2

]

G
L
.
s
c
[

1
v
7
7
1
8
0
.
8
0
7
1
:
v
i
X
r
a

Hyperprior on symmetric Dirichlet distribution

Jun Lu
Computer Science, EPFL, Lausanne
jun.lu.locky@gmail.com

November 5, 2018

Abstract

In this article we introduce how to put vague hyperprior on Dirichlet distribution, and we update the
parameter of it by adaptive rejection sampling (ARS). Finally we analyze this hyperprior in an over-ﬁtted
mixture model by some synthetic experiments.

1

Introduction

It has become popular to use over-ﬁtted mixture models in which number of cluster K is chosen as a
conservative upper bound on the number of components under the expectation that only relatively few
of the components K (cid:48) will be occupied by data points in the samples X . This kind of over-ﬁtted mixture
models has been successfully due to the ease in computation.

Previously Rousseau & Mengersen (2011) proved that quite generally, the posterior behaviour of
overﬁtted mixtures depends on the chosen prior on the weights, and on the number of free parameters in
the emission distributions (here D, i.e. the dimension of data). Speciﬁcally, they have proved that (a) If
α=min(αk, k ≤ K)>D/2 and if the number of components is larger than it should be, asymptotically two
or more components in an overﬁtted mixture model will tend to merge with non-negligible weights. (b)
In contrast, if α=max(αk, k (cid:54) K) < D/2, the extra components are emptied at a rate of N −1/2. Hence,
if none of the components are small, it implies that K is probably not larger than K0. In the intermediate
case, if min(αk, k ≤ K) ≤ D/2 ≤ max(αk, k (cid:54) K), then the situation varies depending on the αk’s and
on the diﬀerence between K and K0. In particular, in the case where all αk’s are equal to D/2, then
although the author does not prove deﬁnite result, they conjecture that the posterior distribution does
not have a stable limit.

2 Hyperprior on symmetric Dirichlet distribution

Inspired by Rasmussen (1999) and further discussed by Görür & Edward Rasmussen (2010), they in-
troduced a hyperprior on symmetric Dirichlet distribution prior. We here put a vague prior of Gamma
shape on the concentration parameter α and use the standard α setting where αk = α = α+/K for
k = 1, . . . , K.

α|a, b ∼ Gamma(a, b) =⇒ p(α|a, b) ∝ αa−1e−bα.

To get the conditioned posterior distributions on α we need to derive the conditioned posterior
distributions on all the other parameters,. But for a graphical model, this conditional distribution is a
function only of the nodes in the Markov blanket. In our case, the Bayesian ﬁnite Gaussian mixture
model, a Directed acyclic graphical (DAG) model, the Markov blanket includes the parents, the children,
and the co-parents, as shown in Figure 1. From this graphical representation, we can ﬁnd the Markov
blanket for each parameter in the model, and then ﬁgure out their conditional posterior distribution to
be derived:

p(α|π, a, b) ∝ p(α|a, b)p(π|α)

K
(cid:89)

πα−1
k

∝ αa−1e−bα Γ(Kα)

(cid:81)K

k=1 Γ(α)
= αa−1e−bα(π1 . . . πK )α−1 Γ(Kα)
[Γ(α)]K .

k=1

1

 
 
 
 
 
 
Figure 1: A Bayesian ﬁnite GMM with hyperprior on concentration parameter.

The following two theorems give the proof that the conditional posterior distribution of α is log-

concave.

Theorem 2.1. Deﬁne

G(x) =

Γ(Kx)
[Γ(x)]K .

For x > 0 and an arbitrary positive integer K, the function G is strictly log-concave.

Proof. From Abramowitz et al. (1966) we get Γ(Kx) = (2π)

1

2 (1−K)K Kx− 1

2 (cid:81)K−1

i=0 Γ(x + i

K ). Then

log G(x) =

1
2

(1 − K) log(2π) + (Kx −

1
2

) log K +

K−1
(cid:88)

i=0

log Γ(x +

i
K

) − K log Γ(x)

and

[log G(x)](cid:48) = K log K +

K−1
(cid:88)

i=0

Ψ(x +

i
K

) − KΨ(x),

where Ψ(x) is the Digamma function, and

Ψ(cid:48)(x) =

∞
(cid:88)

h=0

1
(x + h)2 .

(1)

Thus

[log G(x)](cid:48)(cid:48) =

(cid:34)K−1
(cid:88)

i=0

Ψ(cid:48)(x +

(cid:35)
)

i
K

− KΨ(cid:48)(x) < 0,

(x > 0).

The last inequality comes from (1) and concludes the theorem.

This theorem is a general case of Theorem 1 in Merkle (1997).

Theorem 2.2. In p(α|π, a, b), when a ≥ 1, p(α|π, a, b) is log-concave

Proof. It is easy to verify that αa−1e−bα(π1 . . . πK )α−1 is log-concave when a ≥ 1. In view of that the
product of two log-concave functions is log-concave and Theorem 2, it follows that Γ(Kα)
[Γ(α)]K is log-concave.
This concludes the proof.

2

KKNa,bziµkΣkβxiαπkFrom the two theorems above, we can ﬁnd the conditional posterior for α depends only on the weight
of each cluster. The distribution p(α|π, a, b) is log-concave, so we may eﬃciently generate independent
samples from this distribution using Adaptive Rejection Sampling (ARS) technique (Gilks & Wild, 1992).
Although the proposed hyperprior on Dirichlet distribution prior for mixture model is generic, we
focus on its application in Gaussian mixture models for concreteness. We develop a collapsed Gibbs
sampling algorithm based on Neal (2000) for posterior computation.

Let X be the observations, assumed to follow a mixture of multivariate Gaussian distributions. We
use a conjugate Normal-Inverse-Wishart (NIW) prior p(µ, Σ|β) for the mean vector µ and covariance
matrix Σ in each multivariate Gaussian component, where β consists of all the hyperparameters in NIW.
A key quantity in a collapsed Gibbs sampler is the probability of each customer i sitting with table k:
p(zi = k|z−i, X , α, β), where z−i are the seating assignments of all the other customers and α is the
concentration parameter in Dirichlet distribution. This probability is calculated as follows:

p(zi = k|z−i, X , α, β) ∝ p(zi = k|z−i, α,(cid:19)β)p(X |zi = k, z−i, (cid:1)α, β)

= p(zi = k|z−i, α)p(xi|X−i, zi = k, z−i, β)p(X−i|(cid:24)(cid:24)(cid:24)
∝ p(zi = k|z−i, α)p(xi|X−i, zi = k, z−i, β)
∝ p(zi = k|z−i, α)p(xi|Xk,−i, β),

zi = k, z−i, β)

where Xk,−i are the observations in table k excluding the ith observation. Algorithm 1 gives the pseudo
code of the collapsed Gibbs sampler to implement hyperprior for Dirichlet distribution prior in Gaussian
mixture models. Note that ARS may require even 10-20 times the computational eﬀort per iteration
over sampling once from a gamma density and there is the issue of mixing being worse if we donâĂŹt
marginalize out the π in updating α. So this might have a very large impact on eﬀective sample size
(ESS) of the Markov chain. Hence, marginalizing out π and using an approximation to the conditional
distribution (perhaps with correction through an accept/reject step via usual Metropolis-Hastings or even
just using importance weighting without the accept/reject) or even just a Metropolis-Hastings normal
random walk for log(α) may be much more eﬃcient than ARS in practice. We here only introduce the
updating by ARS.

input : Choose an initial z, α and β;

for T iterations do

for i ← 1 to N do

Remove xi’s statistics from component zi ;
for k ← 1 to K do

Calculate p(zi = k|z−i, α) ;
Calculate p(xi|Xk,−i, β);
Calculate p(zi = k|z−i, X , α, β) ∝ p(zi = k|z−i, α)p(xi|Xk,−i, β);

end
Sample knew from p(zi|z−i, X , α, β) after normalizing;
Add xi’s statistics to the component zi = knew ;

end
(cid:63) Draw current weight variable π = {π1, π2, . . . , πK} ;
(cid:63) Update α using ARS;

end

Algorithm 1: Collapsed Gibbs sampler for a ﬁnite Gaussian mixture model with hyperprior on
Dirichlet distribution

3 Experiments

In the following experiments we evaluate the eﬀect of a hyperprior on symmetric Dirichlet prior in ﬁnite
Bayesian mixture model.

3

3.1 Synthetic simulation

The parameters of the simulations are as follows, where K0 is the true cluster number. And we use K
to indicate the cluster number we used in the test:

Sim 1: K0 = 3, with N =300, π={0.5, 0.3, 0.2}, µ={-5, 0, 5} and σ={1, 1, 1};
In the test we put α ∼ Gamma(1, 1) as the hyperprior. Figure 2 shows the result on Sim 1 with
diﬀerent set of K. Figure 3 shows the posterior density of α in each set of K. We can ﬁnd that the
larger K − K0, the smaller the poserior mean of α. This is what we expect, as the larger overﬁtting, the
smaller α will shrink the weight vector in the edge of a probability simplex.

Sim 1

K = 3

K = 4

K = 5

K = 6

NMI
(SE)
0.931
(8.5e-5)
0.869
(2.0e-4)
0.843
(2.7e-4)
0.846
(3.3e-4)

VI
(SE)
0.203
(2.5e-4)
0.437
(7.7e-4)
0.560
(11.1e-4)
0.564
( 14.9e-4)

K
(SE)
3.0
(0.0)
3.842
(2.6e-3)
4.508
(3.2e-3)
4.703
(5.2e-3)

α
(SE)
1.84
(5.4e-3)
1.43
(5.2e-3)
1.01
(4.5e-3)
0.618
(3.9e-3)

π−

0.0

0.08

0.12

0.11

Figure 2 & Table 1: Left: An example of traceplot of Gibbs sampling using hyperprior on Dirichlet distribution
for variances, weights and means when K = 3 in Sim 1 (Upper one: variance; Middle one: weights; Bottom one:
mean). Right: Summary of posterior distribution in Sim 1: NMI is the normalized mutual information between
true clustering and the resulting clustering. VI is the variation of information between true clustering and resulting
clustering. SE is the standard error of mean. K is the average occupied number of cluster, α is the average α during
sampling, π− is the average of extra weight.

4 Conclusion

We have proposed a new hyperprior on symmetric Dirichlet distribution in ﬁnite Bayesian mixture model.
This hyperprior can learn the concentration parameter in Dirichlet prior due to over-ﬁtting of the mixture
model. The larger the overﬁtting (i.e. K − K0 is larger, more overﬁtting), the smaller the concentration
parameter.

Although Rousseau & Mengersen (2011) proved that α=max(αk, k (cid:54) K) < D/2, the extra compo-
nents are emptied at a rate of N −1/2, it is still risky to use such small α in practice, for example, how
much do we over-ﬁt (i.e. how large the K − K0).
If K − K0 is small, we will get very poor mixing
from MCMC. Some eﬀorts has been done further by van Havre et al. (2015). But simple hyperprior on
Dirichlet distribution will somewhat release the burden.

4

0.60.81.01.21.41.61.82.00123Frequencyvariance0250500750100012501500175020000.51.01.52.0Sample valuevariance0.20.30.40.5051015Frequencyweights0250500750100012501500175020000.20.4Sample valueweights42024024Frequencymean025050075010001250150017502000505Sample valuemean(a) K=3

(b) K=4

(c) K=5

(d) K=6

Figure 3: Posterior distribution for α in diﬀerent overﬁtting settings in Sim 1.

5

01234567890501001502002503003500246810120501001502002503003500123456789050100150200250300350012345678050100150200250300350References

Milton Abramowitz, Irene A Stegun, et al. Handbook of mathematical functions. Applied mathematics

series, 55(62):39, 1966.

Walter R Gilks and Pascal Wild. Adaptive rejection sampling for gibbs sampling. Applied Statistics, pp.

337–348, 1992.

Dilan Görür and Carl Edward Rasmussen. Dirichlet process gaussian mixture models: Choice of the base

distribution. Journal of Computer Science and Technology, 25(4):653–664, 2010.

Milan Merkle. On log-convexity of a ratio of gamma functions. Publikacije Elektrotehničkog fakulteta.

Serija Matematika, pp. 114–119, 1997.

Radford M Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of

Computational and Graphical Statistics, 9(2):249–265, 2000.

Carl Edward Rasmussen. The inﬁnite gaussian mixture model.

In Advances in Neural Information

Processing Systems, volume 12, pp. 554–560, 1999.

Judith Rousseau and Kerrie Mengersen. Asymptotic behaviour of the posterior distribution in overﬁtted
mixture models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(5):
689–710, 2011.

Zoé van Havre, Nicole White, Judith Rousseau, and Kerrie Mengersen. Overﬁtting Bayesian mixture

models with an unknown number of components. PloS one, 10(7):e0131739, 2015.

6

