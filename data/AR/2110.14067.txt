Statistical inference on AR(p) models with non-i.i.d.

innovations

Yunyi Zhang and Dimitris N. Politis

October 28, 2021

Abstract

The autoregressive process is one of the fundamental and most important models that

analyze a time series. Theoretical results and practical tools for ﬁtting an autoregressive

process with i.i.d. innovations are well-established. However, when the innovations are white

noise but not i.i.d., those tools fail to generate a consistent conﬁdence interval for the autore-

gressive coeﬃcients. Focus on an autoregressive process with dependent and non-stationary

innovations, this paper provides a consistent result and a Gaussian approximation theorem

for the Yule-Walker estimator. Moreover, it introduces the second order wild bootstrap that

constructs a consistent conﬁdence interval for the estimator. Numerical experiments conﬁrm

the validity of the proposed algorithm with diﬀerent kinds of white noise innovations. Mean-

while, the classical method(e.g., AR(Sieve) bootstrap) fails to generate a correct conﬁdence

interval when the innovations are dependent.

According to Kreiss et al.

[1] and the Wold decomposition, assuming a real-life time

series satisﬁes an autoregressive process is reasonable. However, innovations in that process

are more likely to be white noises instead of i.i.d.. Therefore, our method should provide a

practical tool that handles real-life problems.

1
2
0
2

t
c
O
6
2

]
T
S
.
h
t
a
m

[

1
v
7
6
0
4
1
.
0
1
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
1 Introduction

The autoregressive process with lag p (AR(p))

Xt =

p

j=1
X

ajXt

−

j + ǫt, t

∈

Z, ǫt

∼

W N (0, σ2)

(1)

W N stands for ‘white noise’ and ǫt is called innovations

is one of the fundamental and most important models for modelling a time series. The

reason mainly comes from the following 4 aspects:

1. Fitting an AR(p) model is straightforward. The Yule-Walker equation(e.g., section

8.1 of Brockwell and Davis [2]) is a classical method that solves this problem. Besides, an

AR(p) model assumes a linear relationship between Xt and

Xt
{

−

1, ..., Xt

p

−

. Therefore,

}

linear regression algorithms(e.g. Lasso or weighted least squares, see Nardi and Rinaldo [3]

and Li and Politis [4]) can be applied to ﬁt the model. We also refer Han et al. [5], Basu and

Michailidis [6] and Krampe and Paparoditis [7] as an overview for ﬁtting a multivariate(high-

dimensional) autoregressive process. Meanwhile, ﬁtting a general time series is more diﬃcult.

Fan and Yao [8] and Li and Politis [4] applied the Kernel estimator; Kirch and Kamgaing [9]

considered a neural network and Cai et al. [10] applied the functional-coeﬃcient autoregres-

sive model to ﬁt a non-linear time series. Notably, even if a time series is linear, it still can

be hard to ﬁt. For example, ﬁtting a M A(moving average) process requires the innovation

algorithm, and ﬁtting an ARMA(autoregressive and moving average) process relies on the

maximum likelihood estimation, see section 8.3 in Brockwell and Davis [2]. Compared to

the Yule-Walker equation, the innovation algorithm estimator does not have a closed-form

formula, making it hard to derive the theoretical guarantees.

2. AR(p) model approximates a wide range of (nonlinear) time series. Suppose Xi, i

Z

∈

is a stationary time series. According to Kreiss et al.

[1], under some assumptions there

exists a sequence of coeﬃcients

ai
{

i=1,2,... such that
}

Xi =

∞

j=1
X

ajXi

−

j + ǫi, ǫi

∼

W N (0, σ2)

(2)

so the time series Xi can be expressed as an AR(

) process. This result is a counterpart

∞

of the famous Wold decomposition(theorem 5.7.1 in Brockwell and Davis [2]) that expresses

Xi as a M A(

) process.

∞

2

3. Even if a stationary mean 0 time series Xi cannot be expressed as an AR(p) process,

ﬁtting an AR(p) model on Xi is still beneﬁcial. For a ﬁxed index i, deﬁne the Hilbert space

H spanned by Xi

−

1, Xi

−

2, ..., Xi

−

projection of Xi on H will be

p with the inner product < X, Y >= EXY . Then the
′

p
j=1 a

′
jXi

−

j . Here (a

′
1, ..., a

p)T satisﬁes the Yule-Walker

equation Γ(a

′

′
1, ..., a

p)T = (γ1, ..., γp)T , Γ = (γ
P

|
Brockwell and Davis [2] and section 3.2 in Fan and Yao [10]. Therefore, ﬁtting an AR(p)

|

)i,j=1,...,p, γi = EX0Xi, see section 2 in

j

i
−

model on Xi contributes to the optimal linear predictor for future observations.

4. Statisticians can use a bunch of AR(p) processes(with diﬀerent autoregressive coeﬃ-

cients) to model a nonlinear / non-stationary time series. This idea leads to the ‘threshold

autoregressive model’(section 4.1.1 in Fan and Yao [8]) and the ‘functional coeﬃcient au-

toregressive model’(Chen and Tsay [11], Cai et al. [10] and Xia and Li [12]).

Despite its simplicity and universality, statistical inference on an AR(p) process is a big

topic. Section 8.10 in Brockwell and Davis [2] derived the asymptotic normality of the Yule-

Walker estimator; Kreiss [13] considered the AR bootstrap and B¨uhlmann [14] adjusted

the AR bootstrap to an AR(

∞

) model. We also refer Paparoditis [15] for bootstrapping

a vector autoregressive model and Braumann et al.

[16] for a simultaneous inference on

autocovariance. A surprising fact is that the Yule-Walker estimator’s asymptotic covariance

matrix only depends on the second order structures, i.e., the innovation’s variance and the

autocovariance of the time series. However, these results rely on the fact that the innovation

ǫt, t

Z in (1) are i.i.d..

∈
This paper starts with the question

What happens if the innovations in model 1 are white noise, but dependent(or even

worse, non-stationary)?

This question is meaningful for the following reasons: one the one hand, a general(nonlinear)

time series may still have AR(

) or M A(

∞

∞

) expression according to Kreiss et al. [1] and

the Wold decomposition. So statisticians can truncate the corresponding AR(

) expression

∞

and use an AR(p)(with suﬃciently large p) process to model the time series. Neverthe-

less, innovations in those expressions are white noise rather than i.i.d.. On the other hand,

whether or not the innovations are white noise can be tested by methods introduced in Li

et al.

[17], Bagchi et al.

[18] and their reference. But it is harder to test the innovations

being i.i.d..

Example 1

3

Suppose ei, i

∈

Z are i.i.d. normal random variables with mean 0 and variance 1. Suppose 3

types of innovations: 1. normal: ǫi = ei; 2. product normal: ǫi = eiei

−

1; 3. non-stationary:

ǫ2i = ei and ǫ2i

−

1 = eiei

−

1. All of them are white noise, i.e., Eǫi = 0, Eǫiǫj = 0 for i

= j

and Eǫ2

i = 1. However, for case 2 Eǫ2

i ǫ2
i
−

1 = Ee2

i ×

Ee2
i
−

2 ×

Ee4
i
−

1 = 3

= 1, so innovations

in case 2 are dependent. Moreover, innovations in case 3 are non-stationary.

Deﬁne Xi = ǫi +

∞j=1 ρjǫi

j , i
< 1, EXi = 0 and EXiXi+k = ρk

P

∈

−

Z, then Xi = ρXi

−

process. Here

ρ
|

|

ρ2 , k

0 for all innovations. Table 1

≥

1

−

1 + ǫi, i.e., Xi is an AR(1)

calculates the variance of the Yule-Walker estimator

ρ as well as the sample autocovariance

γ1 = 1
n
−

1

n
i=2 XiXi

−

1. Case 1 coincides with theorem 8.1.1 in Brockwell and Davis [2], i.e.,

b

P

the asymptotic variance of
b
in all cases, in case 2 and 3 the second order structures cannot decide the variance of

ρ2. However, despite the estimators’ consistency is assured

ρ and

ρ is 1

−

b

γ1. Notably, this phenomenon does not have conﬂict with Braumann et al. [16] and Xiao and

b

Wu [19], for their works control √n max1
b
and require s

→ ∞

; while our work focuses on some speciﬁc sample autocovariance.

b

b

s

i
≤

γi
|

γi

γk is the k
(
|

−

−

th sample autocovariance)

≤

b

b

ρ))

Residual type
Normal

ρ, the sample autocovariance

γ1 and their variances. Here
Table 1: The Yule-Walker estimator
the variances are derived by 10000 simulations and we choose ρ = 0.7. n denotes the sample size.
V ar(√n(
ρ
0.52
0.51
0.52
1.03
1.02
1.04
1.58
1.55
1.53

V ar(√n(
20.74
20.83
21.00
57.34
57.78
57.54
70.51
72.18
70.68

n
10000
50000
100000
10000
50000
100000
10000
50000
100000

γ1
1.45
b
1.39
1.37
1.31
1.40
1.40
1.49
1.42
1.40

ρ
0.70
b
0.70
0.70
0.69
0.70
0.70
0.71
0.70
0.70

product normal

non-stationary

γ1))

γ1

−

−

b

b

If innovations in a time series are not i.i.d., example 1 shows that the Yule-Walker es-

timator’s variance no longer coincides with the i.i.d. situation. Therefore, the validity of

other results, including the central limit theorem and the bootstrap, is not obvious. To com-

plement the theoretical results, Dette et al.

[20] derived the central limit theorem for the

variance estimator of a piece-wise stationary time series; Jiang and Politis [21] constructed

an AR-based spectral density estimator for nonlinear time series and proved its consistency;

Zhang and Wu [22] derived oracle inequalities for the spectral density estimator of a high

dimensional time series; McMurry and Politis [23], Jentsch and Politis [24], Kreiss et al. [1]

4

6
6
and Fragkeskou and Paparoditis [25] discussed the validity of diﬀerent kinds of bootstrap

algorithms. Based on their discussion, the consistency of the AR(Sieve) bootstrap is assured

if the estimator’s asymptotic variance only depends on the second order structures of the

time series. Otherwise, the AR(Sieve) bootstrap approximates the distribution of a ‘com-

panion’ autoregressive process(see Kreiss et al.

[1]), i.e., the linear process whose second

order structures coincide with the original time series but has i.i.d.

innovations. Many

important estimators, including the sample mean, the Xiao and Wu’s test statistics [19] and

the sample spectral density, suit this case. However, -as example 1 demonstrates-, second

order structures of a time series is not suﬃcient to determine the Yule-Walker estimator’s

variance. So the AR bootstrap fails to provide a consistent conﬁdence interval.

This paper aims at providing a consistent conﬁdence interval for the Yule-Walker esti-

mator. Focus on an AR(p) process with white noise(non-i.i.d.) innovations, we establish

the Gaussian approximation theorem for the Yule-Walker estimator. Moreover, we provide

a bootstrap algorithm(named second-order wild bootstrap) that automatically generates the

consistent simultaneous conﬁdence intervals for the estimator. Notably, our work does not

require the ‘strict stationary’ assumptions. So our work is able to handle the AR(p) process

with non-stationary white noise innovations like case 3 in example 1.

The remaining parts of this paper are as follows: section 2 introduces the frequently used

notations and the basic assumptions for this paper. Section 3 presents some properties of

a non-stationary time series. Section 4 derives the consistency and the Gaussian approxi-

mation theorem for the Yule-Walker estimator. Section 5 introduces the second order wild

bootstrap algorithm for making conﬁdence intervals / testing statistical hypothesis. Section

6 provides some numerical experiments that illustrate the ﬁnite sample performance of our

algorithm. Section 7 makes the conclusion. The theoretical proofs will be postponed to an

appendix.

2 Preliminary

Suppose random variables ..., e

−

1, e0, e1, ... are independent(not necessarily identically dis-

tributed) and random variables ǫi, i

Z satisfy

∈

ǫi = gi(..., ei

−

2, ei

−

1, ei)

(3)

5

In other words, ǫi is measurable with the σ-ﬁeld generated by ..., ei

−

2, ei

−

1, ei. The subscript

i means that gi can be diﬀerent with respect to diﬀerent i. This system was ﬁrst introduced

by Wu [26]. Then it became an ubiquitous condition, see Wu and Wu [27], Zhang and Wu

[28, 29, 22], Zhou [30, 31], Wang and Shao [32], Braumann et al. [16] and their reference.

For a random variable X and a number m

1, denote

X

k

m = (E
k

≥

any i, deﬁne e†i as the random variable being independent with ej, j

∈

m)1/m. For
|

X
|
Z, and having

the same distribution as ei. Moreover, ..., e†
−
i as the σ-ﬁeld generated by ..., ei

the ﬁlter

F

ei

−

j, ei

−

j+1, ..., ei. Here i

∈

Z and j

≥

0. Then ǫi is

F

variable X = f (..., ei

−

1, ei) for a ﬁxed i, deﬁne

1, e†0, e†1, ... are mutually independent. Deﬁne

1, ei and

−

i,j as the σ

ﬁeld generated by

F
i measurable. Suppose a random

−

f (..., ei

−

j

−

2, ei

−

j

−

1, e†i
−

j, ei

−

j+1, ..., ei) if j

0

≥

X if j < 0

X(j) = 




Deﬁne

δi,j,m =

ǫi
k

−

ǫi(j)

m and δj,m = sup
k
Z

i
∈

δi,j,m

According to (4), δi,j,m = δj,m = 0 if j < 0.

(4)

(5)

This paper uses the notation

O(

), o(
·

), op(
), Op(
·
·
ical sequences

for ‘for all’, and

for ‘there exists’. The notations

∀

∃
) have the same meaning as in section 1.5.1, Shao [33], i.e., two numer-
·
n=1,...,
}

n=1,... satisfy an = O(bn) if
}

a constant C > 0 such that

bn
{

an
{
for all n; and an = o(bn) if

∃
0 as n

C

| ≤

bn
/
an
|
|
|
ables sequences Xn, Yn satisfy Xn = Op(Yn) if

an
|

bn
|

|

| →
given 0 < ε < 1,

→ ∞

. Two random vari-

a constant Cε > 0

∃
ε for any n; and Xn = op(Yn) if

∀

such that P rob (

Xn
|

| ≤

Cε

Yn
|

)
|

≥

1

−

n

→ ∞

. For two numbers x, y, denote x

∨

y = max(x, y) and x

a = (a1, ..., ap)T

Rp and k

≥
A, deﬁne the matrix 2-norm as

∈

1, deﬁne the vector norm

A
|

2 = sup
|
|

a

|

Aa

=1 |

Xn
|

Yn
/
|
|
y = min(x, y). For a vector

p 0 as

| →

∧
k = (
|

a
|

p
i=1 |
2. Deﬁne P rob∗(
|

P

ai

k)1/k. For a matrix
|

) = P rob(
·

X1, ..., Xn)

·|

and E∗

= E

·

X1, ..., Xn as the probability and the expectation in the bootstrap world, i.e.,

· |

conditional on the observed data.

This paper supposes the weakly stationary(see deﬁnition 1.3.2 in Brockwell and Davis

[2]) random variable sequence Xi, i

∈

Z satisﬁes an autoregressive representation

Xi = ǫi +

p

j=1
X

ajXi

−

j , here P (x) = 1

p

−

j=1
X

ajxj

= 0 for x

C,

x
|

| ≤

1

∈

(6)

6

6
Here p

≥

1 is a constant and aj

∈

applies the following assumptions:

Assumptions

R, j = 1, ..., p are ﬁxed numbers. Besides, this paper

1. ǫi, i

∈

Z are white noise, i.e., Eǫi = 0; Eǫiǫj = 0 if i

= j; Eǫ2

i = σ2 for a constant

σ > 0. Here i, j

Z.

∈

2.

∃

constants m

≥

8, αǫ > 1 such that

sup
k=0,1,...

(1 + k)αǫ

∞

Xj=k

δj,m <

∞

and sup

ǫi

m <
k

∞

Z k

i
∈

(7)

R.

∈

3. Xi, i

Z are weakly stationary, i.e., EXi = 0 and EXiXi+j = EX0Xj = γj

Here i

∈
Z, j

0.

∈
4. Xi, i

≥
Z and the coeﬃcients aj, j = 1, 2, ..., p satisfy (6)

∈
5. The matrix Γ =

γ

{

j

i
−

|

|}

that the minimum eigenvalue of the matrix

is greater than cζ for suﬃciently large n.

1
n

n
i1=k+1

n

P

P

i,j=1,2,...,p is non-singular. Besides,

a constant cζ > 0 such

∃
i2=j+1 E(Xi1 Xi1−

n

k

−

γk)

(Xi2 Xi2−

j

γj)

−

×

k,j=0,...,p

o

The following lemma indicates that Xi is causal, i.e., satisﬁes deﬁnition 3.1.3 in Brockwell

and Davis [2].

Lemma 1 (Theorem 3.1.1 in Brockwell and Davis [2])

Suppose assumption 1 to 4, then

∃

constants ψj , j = 0, 1, 2, ... such that

Xi =

∞

j=0
X

ψj ǫi

−

j almost surely

(8)

Here ψj is determined by the relation P (x)

∞j=0 ψj xj = 1 for

x
|

| ≤

×

1. In particular,

δ > 0 and a constant C such that

∃

ψj
|

| ≤

C(1 + δ)−

P

j.

A direct corollary of lemma 1 is

Xi

sup
i
∈

Z k

m

k

≤

sup
Z
i
∈

∞

j=0
X

ψj
|

ǫi

−

| × k

j

m

k

≤

∞

j=0
X

ψj
|

| ×

ǫi

sup
i
∈

Z k

m <
k

∞

(9)

Example 2 (An example satisfying (7))

Suppose ei, i

∈

Z are independent random variables such that

are real numbers such that (1 +

j
|

αǫ +1)
|

aij

× |

|

< C <

∞

ei
k
for

m = 1 and aij, i, j
k
i, j

Z. Deﬁne Xi =

Z

∈

∀

∈

∞j=0 aijei

−

j . From Kolmogorov’s three-series theorem, Xi exists almost surely. Notice that

P
Xi
k

−

Xi(j)

m

k

≤

aij
2
|

|

<

(1+

|

2C
αǫ +1) . So (7) is satisﬁed.
j

|

7

6
Remark 1

1. If we assume that ǫi are stationary, then δj,m =

ǫj
k

−

ǫj (j)

m. In this case (7) coincides
k

with (2.8) in Wu and Wu [27]. An alternative assumption for non-stationary time series is

the ‘locally stationary’ assumption, see Zhang and Wu [22]. They required that the change

in marginal distributions for consecutive data be asymptotically negligible. So the locally

stationary assumption cannot handle the innovations like case 3 in example 1.

2. For any real numbers aj, j = 0, 1, ..., p, from theorem 1 (in section 4) we have

p

k

j=0
X

aj

1
√n

n

(XiXi

j

−

γj)

km/2 = O

−

i=j+1
X

p

j=0
X

a2
j 



(10)





v
u
u
t

so the largest eigenvalue of the matrix

has order O(1).

1
n

n
i1=k+1

n

i2=j+1 E(Xi1 Xi1−

n

P

P

k

−

γk)

(Xi2 Xi2−

j

γj)

−

×

k,j=0,...,p

o

3 Some corollaries of assumption 2

Suppose random variables ǫi, i

∈

Z satisfy (3), (7) and Eǫi = 0. For any coeﬃcients

ai, i = 1, 2, ..., n and s

∈

Z, deﬁne εi,j = Eǫi

i,j and Mi,s =

|F
1 and Mi+1,s

n
j=n+1

i aj (εj,s

εj,s

−

1).

P
Mi,s = an

−

−

i,s

−

−

−
i(εn

−
εn

−

Then Mi,s is measurable in the ﬁlter

Apply π

−

λ theorem to the λ

−

n,i+s

−

F
system

1).

i,s

−

(11)

A

{

n,i+s

−

∈ F

1 : Eεn

−

i,s

×

1A = Eεn

−

i,s

−

1

×

1A

}

and the π

system

−

E(εn

−

i,s

−

εn

−

i,s

−

An
{
1)

|F

An

×

n,i+s

−

1

−

×

...

An

. Here Ai is generated by ei. We have
}
1 = 0 almost surely.

In particular,

i+1

×

−

−

s

Mi,s
{

i=1,2,...,n form a
}

martingale. From Burkholder’s inequality(theorem 1.1 in [34]),

Mn,s
k

m

k

≤

C

n

k

v
u
u
t

i=1
X

a2
i (εi,s

εi,s

−

1)2

−

km/2 ≤

C

n

v
u
u
t

i=1
X

Here C is a constant depending on m. Since

a2
i ×

max
i=1,...,n k

εi,s

−

εi,s

−

1

m (12)
k

εi,s
k

−

εi,s

−

1

m =
k

E(ǫi
k

−

ǫi(s))

i,s

m

k

|F

≤

δi,s,m

(13)

8

Combine with theorem 2 in [35],

n

k

i=1
X

aiǫi

m

k

≤ k

n

i=1
X

aiεi,0

m +
k

∞

s=1
X

Mn,s
k

m

k

n

(14)

C

≤

n

v
u
u
t

i=1
X

ǫi

a2
i k

2
m + C
k

n

v
u
u
t

i=1
X

∞

a2
i

δs,m = O

s=1
X

a2
i 



v
u
u
t

i=1
X



0, deﬁne ζi,k = ǫiǫi+k

≥
0, deﬁne νi,t,j,k = ζi,j ζi+t,k

≥
i+k, and νi,t,j,k = Hi,i+j,i+t,i+t+k(..., e(i+j)

−

Eζi,jζi+t,k. ζi,k = hi,i+k(...,ei+k−1,ei+k )

(i+t+k)) is measurable in

∨

Eǫiǫi+k. For

−

Here C is a constant. For a ﬁxed integer k

ﬁxed integers t, j, k

is measurable in

F

F(i+j)

∨

(i+t+k).

ǫiǫi+k
k

km/2 ≤ k

ǫi

m

k

ǫi+k
k

m

k

⇒

sup
Z,k

≥

i
∈

ǫiǫi+k

0 k

km/2 <

∞

(15)

ζi,j ζi+t,k
k

km/4 ≤ k

ζi,j

km/2 × k

ζi+t,k

km/2 ⇒

sup
Z,j,t,k

i
∈

≥

ζi,jζi+t,k

0 k

km/4 <

∞

In particular, this implies that ζi,k and νi,t,j,k are well-deﬁned.

We have the following lemma:

Lemma 2

Suppose random variables ǫi, i

Z satisfy (3) and (7).

∈

1. For any ﬁxed k

0, deﬁne ∆(i,k),j,m/2 =

ζi,k
k

−

ζi,k(j)

km/2 and ∆k,j,m/2 =

≥

supi
∈

Z ∆(i,k),j,m/2. Then

sup
j=0,1,...

(1 + j)αǫ

∞

Xl=j

∆k,l,m/2 <

∞

2. For any ﬁxed k, j

≥

0, deﬁne Θ(i,t,j,k),l,m/4 =

νi,t,j,k
k

−

νi,t,j,k(l)

km/4. Then

sup
Z,t=0,1,...

i
∈

∞

Xl=0

Θ(i,t,j,k),l,m/4 <

∞

(16)

(17)

If the innovations satisfy (7), then lemma 3 shows that the autoregressive process Xi, i

∈

Z also satisﬁes (7).

Lemma 3

9

Suppose assumption 1 to 4. Deﬁne ∆i,j,m =

Xi
k

−

Xi(j)

m, then we have
k

sup
k=0,1,...

(1 + k)αǫ

∞

Xj=k

sup
Z
i
∈

∆i,j,m <

∞

Notably, from lemma 1

Xi

sup
i
∈

Z k

m

k

≤

∞

j=0
X

ψj
|

| ×

ǫi

sup
i
∈

Z k

m

k

(18)

(19)

so supi
∈

Z

ǫi
k

m <
k

∞ ⇒

supi
∈

Z

Xi
k

m <
k

.

∞

Remark 2

A real-world time series Zi, i

∈

Z may not have a linear expression with i.i.d. innovations.

If a statistician ﬁts an AR(p) model on Zi, the model is wrong. However, is it possible for

this wrong model to fulﬁll the requirement of statistical inference, i.e., provide a consistent

estimator for the autoregressive coeﬃcients and make consistent conﬁdence intervals?

Classical theories and methods rely on the innovations being i.i.d., so they fail to fulﬁll

this requirement(as illustrated in example 1 and table 2).

For a ﬁxed lag p, deﬁne

Zi =

p
j=1 ajZi

j . Here Γ(a1, ..., ap)T = (γ1, ..., γp)T , Γ is

−
deﬁned in assumption 5. Suppose Γ is non-singular, then the solution (a1, ..., ap)T exists.

P

b

Deﬁne

δi = Zi

−

Zi and set a0 = 1. Suppose Zi satisﬁes assumption 2,

b

b

δi
k

−

δi(j)

m

k

⇒

∞

Xk=j

b
δi(k)

b

−

m

k

≤

δi

sup
i
∈

Z k

b

b

p

ak
|

≤

Xk=0
p

al
|

| ×

Xl=0

Zi

k

−

Zi

−

k(j

k)

m

k

−

−

| × k

Zi(k

l)

m

k

−

−

(20)

∞

Xk=j

Zi

Z k

sup
i
∈
p

(1 + 0

≤

Xl=0

al
|
(j

C

∨

|
−

l))αǫ

Here C is a constant. Therefore, as long as the lag p is not very large, the innovation

should still satisfy assumption 2.

δi

b

On the other hand, suppose i1 > i2. Then E

δi1 Zi2 = 0 for i2 = i1

1, ..., i1

−

−

p. Suppose

b

10

i2

i1

p

−

−

1,

≤

E
|

δi1 Zi2 | ≤
b

p

j=0
X

aj
|

EZi1−

| × |

j Zi2 | ≤

p

j=0
X

aj
|

Zi2 k
|k

2

Zi1−
k

j

EZi1 |F

−

i1−

j,i1−

i2−

j

−

1

2
k

p

(1 + i1

≤

j=0
X

(21)

j)αǫ

−

C

aj
|
|
i2
−

So

E
|

p

p/2

⌋

⌊

≤

Xk=i2+p+1
−

i1

j=0
X

p

p

p

ak
|

E

| × |

δi1 Zi2−

k

| ≤

Xk=i2+p+1
−
p

i1

p

j=0
X

δi1

δi2 | ≤
b

b

Xk=0
aj ak
C
|
i2
(1 + i1 + k

−

C
(p/2 + 2)αǫ

≤

b

+

p/2)αǫ

|
−

Xk=i2+p+1
−
p/2
⌋
⌊

p

i1

p/2
Xj=
⌋
⌊

+1

p

Xk=i2+p+1
−

i1

j=0
X

aj ak
|

+ C

|

Xk=i2+p+1
−

i1

p/2
Xj=
⌋
⌊

+1

aj ak
|

|

(22)

C

aj ak
|
|
i2
(1 + i1 + k

−

C

aj ak
|
|
i2
(1 + i1 + k

j)αǫ

−

p)αǫ

−

−
p

Here

x
⌊

⌋

denotes the largest integer that is smaller than or equal to x. If we impose additional

assumptions on aj(e.g., (1+j)αǫ+1

aj
|
behave like a white noise series as p

| ≤

C for any j), then E

δi2 ≈
. According to (20) and (22), a statistician needs
b

0 and the innovations

δi1

b

→ ∞

to ﬁne-tune the lag p to maintain assumption 2 and make the innovations behave like a

white noise series. But after doing that, our method can be applied to the ‘wrong model’ and

generate a consistent conﬁdence interval.

4 Consistency and Gaussian approximation

Deﬁne

γi =

n

1
n

j=i+1
X
Then deﬁne the estimator

b

Xj Xj

−

i, here we assume n > p and i = 0, 1, ..., p

(23)

γ = (

γ1,

γ2, ...,

γp)T and the matrix

Γ =

i,j=1,...,p. From

assumption 1 to 5 and lemma 1, EǫiXj =

b

b

b

b
P

∞k=0 ψkEǫiǫj

−

E

γi =

n

i

−
n

γi and γj = EXj X0 =

|}
k = 0 for j < i. So

{

|

j

i
−

b

γ

b

p

Xk=1

akγj

−

k for j

1

≥

In other words, the Yule-Walker equation holds. So we deﬁne the statistics

b

(24)

(25)

a = (

a1, ...,

ap)T =

Γ−

1

γ

b

b

b

b

b

11

If

Γ is singular, then

Γ−

1 stands for the pseudo-inverse of

Γ. The ﬁrst theorem involves

demonstrating the consistency of

b

b

Theorem 1

Suppose assumption 1 to 5. Then

γ.

b

b

γj
k

γj

km/2 = O(1/√n) and

ak
|

ak

|

−

−

= Op(1/√n)

(26)

Here j = 0, 1, ..., p and k = 1, 2, ..., p.

b

b

Then we focus on deriving a Gaussian approximation theorem for linear combinations

of

γ0, ...,

γp. The Gaussian approximation theorems are powerful tools in high-dimensional

statistics, see Chernozhukov et al.

[36], Zhang and Politis [37] and Zhang and Wu [28].

b

b

Compared to the central limit theorem(e.g., theorem 1.15 in Shao [33]), the validity of

a Gaussian approximation theorem only depends on the moments of random variables.

Therefore, it is suitable for non-stationary time series as well.

For a given matrix B =

bij
{

i=1,...,p1,j=0,1,...,p such that
}

p

j=0 b2

ij > 0 and p1

p, this

≤

paper focuses on constructing the simultaneous conﬁdence interval for the parameter vector

P

(

p
j=0 b1j γj, ...,

p

j=0 bp1jγj )T . So we need to derive its asymptotic distribution. Deﬁne the

P
function H(x) as

P

H(x) = P rob

max
i=1,...,p1 |

ξi

| ≤

x

, x

(cid:19)

R

∈

(cid:18)

(27)

Here (ξ1, ..., ξp1 )T are joint normal random variables with mean Eξi = 0 and covariance

Eξi1 ξi2 =

1
n

p

p

n

n

j1=0
X

j2=0
X

Xk1=j1+1

Xk2=j2+1

bi1j1 bi2j2 E(Xk1 Xk1−

j1 −

γj1 )

(Xk2 Xk2−

j2 −

×

γj2 )

(28)

For the time series Xi, i

Z is not assumed to be stationary, the matrix

∈
i2=j+1 E(Xi1 Xi1−

n

. Correspondingly, H changes when n varies. Yet theorem 2 shows that the

P

k

−

γk)

(Xi2 Xi2−

j

γj)

−

×

k,j=0,...,p

o

may not have a limit

1
n

n
i1=k+1

n
as n

P

→ ∞

diﬀerence between maxi=1,2,...,p1 |
and H is asymptotically negligible.

√n

P

Theorem 2

Suppose assumption 1 to 5. Then

p
j=0 bij(

γj

γj)

’s cumulative distribution function
|

−

b

P rob

sup
x

R |

∈

max
i=1,2,...,p1 |

√n

p

j=0
X

bij (

γj

γj)

x

| ≤

−

H(x)

|

= o(1)

! −

(29)

b

12

 
Here B =

and p1

≤

bij
{
p.

i=1,...,p1,j=0,...,p is a ﬁxed matrix such that
}

P

p

j=0 b2

ij > 0 for i = 1, 2, ..., p1

From theorem 1, assumption 5 and (5.8.4) in Horn and Johnson [38]

a

(
|

−

a)

−

1

(

Γ−

Γ−

1)γ

Γ−

1(

γ

γ)

2
|

−

Γ−

≤ |

1

Γ−

1

−

−

−

γ

× |

γ

2 = Op(1/n)
|

−

2
|

(30)

b

b

From corollary 5.6.16 in [38], if

b
1(
Γ

Γ−
|

b

b

Γ)

2 < 1/2, then
|

−

b

1

Γ−

(
⇒ |

−

Γ−

1) + Γ−

1(

Γ

1

Γ)Γ−

−

b
Γ−

≤ |

1

2
|

2
|

×

Γ−

1 =

1)k(Γ−

1(

Γ

−

Γ))k

−

!

1

Γ−

1(

Γ

Γ−
|

−

b
Γ)

k
2 = Op(1/n)
|

(31)

∞

(
Xk=0
∞

Xk=2

b
Deﬁne the p

p matrix Ti =

×
Then deﬁne δi = (0, 0, ..., 0

b
ti,jk
{
, 1, 0, ..., 0)T . For a ﬁxed matrix B =

j,k=1,2,...,p such that ti,jk = 1 if j
}

b

k = i and 0 otherwise.

i=1,...,p1,j=1,...,p, set
}

−
bij
{

bi = (bi1, ..., bip)T ,

|

1

i
−
{z

p

}

√n(B

a

−

Ba) = √n

(

γi

i=1
X

p

√n

bij(

aj

b

−

b

⇒

j=1
X

γi)BΓ−

1δi

√n

−

−

p

1

−

p

i=1
X
−

(

γ

i
| −

|

γ

i
|

|

)BΓ−

1TiΓ−

1γ + Op(1/√n)

aj) = bT

i Γ−

1δp

√n(

γp

×

−

bT
i Γ−

1T0Γ−

1γ

−

√n(

γ0

γ0)

−

×

b
γp)

+

1

p
b

−

j=1
X

(bT

i Γ−

1δj

−

b
1(Tj + T

bT
i Γ−

j)Γ−

1γ)

−

×

√n(

γj

−

b
γj) + Op(1/√n)

b

(32)

Therefore we have the following corollary.

Corollary 1

Suppose assumption 1 to 5. Then

P rob

sup
x

R |

∈

max
i=1,...,p1 |

√n

p

j=1
X

bij (

aj

aj)

x

| ≤

−

(x)

! − H

= o(1)

|

(33)

b

Here B =

bij
{

i=1,...,p1,j=1,...,p is a ﬁxed matrix such that
}

(bT

i Γ−

1δp)2 + (bT

i Γ−

2γ)2 +

p

1

−

j=1
X

(bT

i Γ−

1δj

bT
i Γ−

1(Tj + T

−

−

j )Γ−

1γ)2 > 0

(34)

for i = 1, 2, ..., p1, p1

p, and

ξi
(x) = P rob(maxi=1,...,p1 |

H

| ≤

≤

x). ξ1, ..., ξp1 are joint

13

 
 
normal random variables with Eξi = 0,

Eξi1 ξi2 =

1
n

p

p

n

n

j1=0
X

j2=0
X

Xk1=j1+1

Xk2=j2+1

ci1j1 ci2j2 E(Xk1 Xk1−

j1 −

γj1 )

(Xk2 Xk2−

j2 −

×

γj2 )

(35)

ci0 =

−
1, 2, ..., p

bT
i Γ−

2γ, cip = bT

i Γ−

1δp and cij = (bT

i Γ−

1δj

bT
i Γ−

1(Tj + T

−

−

j)Γ−

1γ) for j =

1.

−

5 Bootstrap conﬁdence interval & hypothesis test-

ing

So far, we have derived the Gaussian approximation theorem for the Yule-Walker estimator.

Then statisticians may try to estimate the Yule-Walker estimator’s variance and generate a

consistent conﬁdence interval based on the normal distribution.

Another idea is to use a computer intensive method, like subsampling or bootstrap

[39], that generates a consistent conﬁdence interval / performs hypothesis testing through

simulations. The advantage of this idea is that it implicitly derives the estimator’s variance,

so statisticians do not need to calculate anything. This section adopts the second idea and

introduces a bootstrap algorithm, called the second order wild bootstrap. We are interested

in testing the statistical hypothesis

null: Ba = c versus alternative Ba

= c

(36)

Here B is a known p1

×

p matrix and c is a known p1

×

to 5, this section requires an additional assumption.

1 vector. Apart from assumption 1

Additional assumption

6. Suppose a function K(

) : R
·

→

[0,

∞

) is symmetric and continuously diﬀerentiable.

K(0) = 1,

R K(x)dx <

. K(x) is decreasing on [0,

). Deﬁne the Fourier transforma-

∞

∞
R K(t) exp(

2i

−

×

πtx)dt. Assume

K(x)

F

≥

0 for

. Suppose a bandwidth parameter kn > 0 satisﬁes limn

R

x

∀
∈
kn =

R, and

and

∞

→∞

R
tion of K as

K(x) =

F
K(x)dx <

R F
R
kn = o(√n).

∞

Remark 3

According to Shao [40] and the Fourier inversion theorem(theorem 8.26 in Folland [41]),

14

6
x = (x1, ..., xn)T

∀

Rn,

∈

n

n

s=1
X

j=1
X

xsxjK

s

j
−
kn (cid:19)

(cid:18)

=

n

n

R

Z

s=1
X

j=1
X

xsxj

F

n

K(z) exp

2πiz

(cid:18)

s

j
−
kn (cid:19)

dz

(37)

K(z)

=

R F

Z

× |

s=1
X

xs exp

2πiz

(cid:18)

s
kn (cid:19)

2
|

0

≥

so the matrix

K

{

s
j
−
kn

(cid:17)
kernel K is K(x) = exp(
−

(cid:16)

s,j=1,2,...,n is symmetric positive semi-deﬁnite. One satisfactory
}
x2/2) whose Fourier transformation is

K(x) = √2π exp(

2π2x2).

F

−

Algorithm 1 presents the second order wild bootstrap.

Algorithm 1: second order wild bootstrap

), a bandwidth kn > 0, the

α, the number of bootstrap replicates T , and the
bij

Input: observations X1, ..., Xn, a kernel function K(
·
nominal coverage probability 1
linear combination matrix B =
}
Additional input for testing (36): The expected vector c = (c1, ..., cp1)T
1. Estimate coeﬃcients
2. Generate joint normal distributed random variables ε1, ..., εn such that E∗εi = 0,
E∗εiεj = K (cid:16)
3. Deﬁne

ap according to (23) and (25).
b

i=1,...,p1,j=1,...,p.

γp and
b

i−j
kn (cid:17).

γ0, ...,

a1, ...,

−
{

b

b

Then deﬁne

Γ∗ =
b

1
n

n

X
j=i+1

γi +
b

γ∗
i =
b
γ∗
|i−j|}
b

{

(XjXj−i

εj, i = 0, 1, ..., p

×

−

γi)
b
γ∗
1 , ...,
b
p)T =
a∗
b

i,j=1,...,p and

γ∗ = (
b
a∗
a∗ = (
1, ...,
b
b

p )T . Calculate
γ∗
b
Γ∗−1
b

γ∗

b

(38)

(39)

Γ∗ is singular, then
b

Γ∗−1 represents the pseudo-inverse of
If
4. Calculate the statistics δb = maxi=1,...,p1 √n
b
5. Repeat step 2 to 4 for b = 1, 2, ..., T . Then calculate the 1
of δ1, ..., δB.
6.a (for making conﬁdence interval) The 1

p
j=1 bij
| P

Γ∗.
p
a∗
j=1 bij
b
j − P
α sample quantile C∗
b
−

aj
b

|

1−α

α conﬁdence interval for Ba will be

−

Rp1 : max

i=1,...,p1 |

xi

∈




x



p

−

X
j=1

bij

aj
b

| ≤

C∗
1−α
√n






6.b (for hypothesis testing) Reject the null hypothesis if

p

max
i=1,...,p1 |

X
j=1

bij

aj
b

ci

|

−

>

C∗
1−α
√n

(40)

(41)

Remark 4

15

Algorithm 1 adopts the idea of the dependent wild bootstrap introduced by Shao [40]. How-

ever, it treats the sequence

XiXi
{

−

k

γk

i=k+1,...,n(k is a ﬁxed number) as a new time series
}

−

and resamples on this sequence rather than Xi. After applying this modiﬁcation, algorithm 1

is able to capture the second order structure of the sequence

XiXi
{

−

k

γk

i=k+1,...,n, which
}

−

is the high order structure of the original time series.

According to theorem 1.2.1 in Politis et al. [39], the consistency of algorithm 1 is ensured

if one can show that the distribution of the bootstrapped estimator maxi=1,...,p1 √n

p
j=1 bij

a∗j −

|

(deﬁned in corollary 1) in probability. Theorem 3 justiﬁes this
b

P

H

converges to

p
j=1 bij

aj

|

P
result.

b

Theorem 3

Suppose assumption 1 to 6, then

p

p

P rob∗

sup
x

R |

∈

max
i=1,...,p1

√n

|

Here

H

is deﬁned in corollary 1 and

bij

a∗j −

j=1
X

b
a∗1, ...,
a∗ = (

j=1
X

bij

aj

x

| ≤

(x)

! − H

= op(1)

|

(42)

b

a∗p)T is deﬁned in algorithm 1.

6 Numerical result

b

b

b

Suppose ei, i

∈

Z are i.i.d. normal random variables with mean 0 and variance 1. Following

example 1, we consider three types of white noise innovations: normal:

ǫi = ei, product

normal:

ǫi = eiei

−

1 and non-stationary:

ǫ2i = ei, ǫ2i

−

1 = eiei

−

1. Then we consider the

following 4 time series:

1. AR(1) model Xi = 0.7Xi

−

1 + ǫi

2. AR(2) model Xi = 0.3Xi

−

1 + 0.5Xi

−

2 + ǫi

3. M A(1) model Xi = ǫi + 2.0ǫi

−

1

4. non-linear model Xi = 0.6 sin(Xi

−

1) + ǫi

3 is a linear time series that is not invertible according to Keriss et al.

[1], and 4

is considered in Fragkeskou and Paparoditis [25]. For 3 and 4 do not have explicit AR

coeﬃcients, we calculate them through simulating 50000 samples. The order of Xi in this

situation is determined by the aic criterion implemented in R’s ‘ar’ function.

Selecting bandwidth kn

Politis and White [42] introduced an automatic bandwidth selection algorithm and Shao

16

 
[40] applied this algorithm to select the bandwidth of the dependent wild bootstrap. The

R package ‘np’ [43] implemented this method. Other methods include Fragkeskou and

Paparoditis [44](also introduced in [25]). For convenience, we apply the Politis and White’s

method. However, their method was decided for a stationary time series. So it may result

in selecting a sub-optimal bandwidth.

Simulation results

We demonstrate the simulation results in table 2. To accelerate calculation, we apply

the warp-speed algorithm introduced by Giacomini et al.

[45] to calculate the coverage

probability. We compare the performance of the second order wild bootstrap and the AR

bootstrap in table 2.

Despite diﬀerent types of white noise innovations are incorporated to the time series

model, the AR bootstrap’s 90% quantile remains the same for each model. Therefore, its

conﬁdence interval has correct coverage probability when the innovations are indeed i.i.d.

but is too narrow when the innovations become dependent. In other words, the AR bootstrap

fails to capture the high order structures of a time series that aﬀect the width of a conﬁdence

interval.

On the other hand, the second order wild bootstrap’s 90% quantile is close to the AR

bootstrap’s 90% quantile when the innovations are i.i.d., but is signiﬁcantly larger than that

when the innovations are dependent. Therefore, the impact of the time series’s high order

structures on the conﬁdence interval can be captured by the second order wild bootstrap.

7 Conclusion

Theoretical results and practical tools for ﬁtting an AR(p) model with i.i.d. innovations are

well-established. However, when the innovations in the AR(p) model is no longer i.i.d., the

existing results fail. This paper establishes the consistency and the Gaussian approximation

theorem for the Yule-Walker estimator when the innovations in the AR(p) model is dependent

and non-stationary. Moreover, it derives the second order wild bootstrap that automatically

generates a consistent conﬁdence interval for the Yule-Walker estimator.

When the innovations are dependent, the Yule-Walker estimator’s conﬁdence interval

will be aﬀected by the high order structures of the time series. Therefore, classical methods

like AR bootstrap fails to generate a consistent conﬁdence interval. Since a general weakly

17

Table 2: Performance of bootstrap algorithms on various time series. This simulation chooses
B = Ip, the identity matrix with dimension p(equals lag) ‘AR’ abbreviates ‘AR bootstrap’ and
‘P’ represents the coverage probability
‘Wild’ abbreviates ‘the second order wild bootstrap’.
derived by the warp speed algorithm and ‘C’ denotes the quantile of maxi=1,...,p √n
(see
algorithm 1) calculated by the corresponding bootstrap algorithm. ‘Lag’ denotes the lag of the
autoregressive model. For model 3 and 4, the lag is chosen by the aic criterion implemented in
the R’s ‘ar’ function. The bandwidth kn for the second order wild bootstrap is selected based
on Politis and White [42]. The nominal coverage probability is 90%.

a∗
i −
|
b

ai
b

|

Model
1

2

3

4

Innovation
normal
normal
product normal
product normal
non-stationary
non-stationary
normal
normal
product normal
product normal
non-stationary
non-stationary
normal
normal
product normal
product normal
non-stationary
non-stationary
normal
normal
product normal
product normal
non-stationary
non-stationary

Sample size
500
1000
500
1000
500
1000
500
1000
500
1000
500
1000
500
1000
500
1000
500
1000
500
1000
500
1000
500
1000

Lag Wild P Wild C AR P
1
1
1
1
1
1
2
2
2
2
2
2
4
4
3
7
3
4
1
1
1
7
1
1

81.9%
89.0%
95.3%
91.7%
88.1%
88.0%
90.7%
92.0%
92.2%
96.0%
89.1%
88.7%
98.7%
97.7%
95.9%
98.6%
93.9%
94.1%
87.4%
89.3%
92.3%
99.3%
85.1%
85.2%

AR C
82.4% 1.182
90.6% 1.296
73.9% 1.160
70.9% 1.250
68.6% 1.155
65.4% 1.139
87.6% 1.671
87.1% 1.848
65.2% 1.827
73.0% 1.722
62.5% 1.604
60.1% 1.637
91.5% 2.291
91.5% 2.344
73.9% 2.196
76.5% 2.687
61.7% 2.402
55.1% 2.356
87.2% 1.379
90.6% 1.388
75.1% 1.334
79.3% 2.667
64.5% 1.303
65.7% 1.321

1.022
1.209
1.564
1.949
1.656
1.512
1.754
2.305
3.173
2.494
3.248
2.965
2.814
2.746
4.111
4.033
5.100
4.438
1.262
1.448
2.372
4.523
2.011
1.810

18

stationary time series is likely to have a linear expression(because of Kreiss et al.

[1] and

the Wold decomposition) with dependent innovations, our result can be a good choice to

solve real-life problems.

References

[1] Jens-Peter Kreiss, Efstathios Paparoditis, and Dimitris N. Politis. On the range of

validity of the autoregressive sieve bootstrap. The Annals of Statistics, 39(4):2103 –

2130, 2011.

[2] Peter J Brockwell and Richard A Davis. Time Series: Theory and Methods. Springer-

Verlag, Berlin, Heidelberg, 1986.

[3] Y. Nardi and A. Rinaldo. Autoregressive process modeling via the lasso procedure.

Journal of Multivariate Analysis, 102(3):528–549, 2011.

[4] Li Pan and Dimitris N. Politis. Bootstrap prediction intervals for linear, nonlinear and

nonparametric autoregressions. Journal of Statistical Planning and Inference, 177:1 –

27, 2016.

[5] Fang Han, Huanran Lu, and Han Liu. A direct estimation of high dimensional stationary

vector autoregressions. Journal of Machine Learning Research, 16(97):3115–3150, 2015.

[6] Sumanta Basu and George Michailidis. Regularized estimation in sparse high-

dimensional time series models. The Annals of Statistics, 43(4):1535 – 1567, 2015.

[7] Jonas Krampe and Efstathios Paparoditis. Sparsity concepts and estimation procedures

for high-dimensional vector autoregressive models. Journal of Time Series Analysis,

42(5-6):554–579, 2021.

[8] Jianqing Fan and Qiwei Yao. Nonlinear Time Series. Springer-Verlag, New York, 2003.

[9] Claudia Kirch and Joseph Tadjuidje Kamgaing. Testing for parameter stability in

nonlinear autoregressive models. Journal of Time Series Analysis, 33(3):365–385, 2012.

[10] Zongwu Cai, Jianqing Fan, and Qiwei Yao. Functional-coeﬃcient regression models for

nonlinear time series. Journal of the American Statistical Association, 95(451):941–956,

2000.

[11] Rong Chen and Ruey S. Tsay. Functional-coeﬃcient autoregressive models. Journal of

the American Statistical Association, 88(421):298–308, 1993.

19

[12] Yingcun Xia and W. K. Li. On the estimation and testing of functional-coeﬃcient

linear models. Statistica Sinica, 9(3):735–757, 1999.

[13] Jens-Peter Kreiss. Bootstrap procedures for ar (

) — processes. In Karl-Heinz J¨ockel,

∞

G¨unter Rothe, and Wolfgang Sendler, editors, Bootstrapping and Related Techniques,

pages 107–113, Berlin, Heidelberg, 1992. Springer Berlin Heidelberg.

[14] Peter B¨uhlmann. Sieve bootstrap for time series. Bernoulli, 3(2):123 – 148, 1997.

[15] Efstathios Paparoditis. Bootstrapping autoregressive and moving average parameter

estimates of inﬁnite order vector autoregressive processes. Journal of Multivariate Anal-

ysis, 57(2):277–296, 1996.

[16] Alexander Braumann, Jens-Peter Kreiss, and Marco Meyer. Simultaneous inference

for autocovariances based on autoregressive sieve bootstrap. Journal of Time Series

Analysis, 42(5-6):534–553, 2021.

[17] Zeng Li, Cliﬀord Lam, Jianfeng Yao, and Qiwei Yao. On testing for high-dimensional

white noise. The Annals of Statistics, 47(6):3382 – 3412, 2019.

[18] Pramita Bagchi, Vaidotas Characiejus, and Holger Dette. A simple test for white noise

in functional time series. Journal of Time Series Analysis, 39(1):54–74, 2018.

[19] Han Xiao and Wei Biao Wu. Portmanteau test and simultaneous inference for serial

covariances. Statistica Sinica, 24(2):577–599, 2014.

[20] Holger Dette, Weichi Wu, and Zhou Zhou. Change point analysis of second order

characteristics in non-stationary time series. (arXiv: 1503.08610), 2015.

[21] Jiang Wang and Dimitris N. Politis. Consistent autoregressive spectral estimates: Non-

linear time series and large autocovariance matrices. Journal of Time Series Analysis,

42(5-6):580–596, 2021.

[22] Danna Zhang and Wei Biao Wu. Convergence of covariance and spectral density es-

timates for high-dimensional locally stationary processes. The Annals of Statistics,

49(1):233 – 254, 2021.

[23] Timothy L. McMurry and Dimitris N. Politis. Banded and tapered estimates for auto-

covariance matrices and the linear process bootstrap. Journal of Time Series Analysis,

31(6):471–482, 2010.

20

[24] Carsten Jentsch and Dimitris N. Politis. Covariance matrix estimation and linear pro-

cess bootstrap for multivariate time series of possibly increasing dimension. The Annals

of Statistics, 43(3):1117 – 1140, 2015.

[25] Maria Fragkeskou and Efstathios Paparoditis. Extending the range of validity of the

autoregressive (sieve) bootstrap. Journal of Time Series Analysis, 39(3):356–379, 2018.

[26] Wei Biao Wu. Nonlinear system theory: Another look at dependence. Proceedings of

the National Academy of Sciences, 102(40):14150–14154, 2005.

[27] Wei-Biao Wu and Ying Nian Wu. Performance bounds for parameter estimates of high-

dimensional linear models with correlated errors. Electron. J. Statist., 10(1):352–379,

2016.

[28] Danna Zhang and Wei Biao Wu. Gaussian approximation for high dimensional time

series. The Annals of Statistics, 45(5):1895 – 1919, 2017.

[29] Danna Zhang and Wei Biao Wu. Asymptotic theory for estimators of high-order statis-

tics of stationary processes. IEEE Transactions on Information Theory, 64(7):4907–

4922, 2018.

[30] Zhou Zhou.

Inference of weighted V -statistics for nonstationary time series and its

applications. The Annals of Statistics, 42(1):87 – 114, 2014.

[31] Zhou Zhou.

Inference for non-stationary time series regression with or without in-

equality constraints. Journal of the Royal Statistical Society: Series B (Statistical

Methodology), 77(2):349–371, 2015.

[32] Runmin Wang and Xiaofeng Shao. Hypothesis testing for high-dimensional time series

via self-normalization. The Annals of Statistics, 48(5):2728 – 2758, 2020.

[33] Jun Shao. Mathematical Statistics. Springer-Verlag New York, 2003.

[34] D. L. Burkholder, B. J. Davis, and R. F. Gundy.

Integral inequalities for convex

functions of operators on martingales. In Proceedings of the Sixth Berkeley Symposium

on Mathematical Statistics and Probability, Volume 2: Probability Theory, pages 223–

240, Berkeley, Calif., 1972. University of California Press.

[35] Peter Whittle. Bounds for the moments of linear and quadratic forms in independent

variables. Theory of Probability & Its Applications, 5(3):302–305, 1960.

21

[36] Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Gaussian approximations

and multiplier bootstrap for maxima of sums of high-dimensional random vectors. Ann.

Statist., 41(6):2786–2819, 12 2013.

[37] Yunyi Zhang and D. N. Politis. Ridge regression revisited: Debiasing, thresholding and

bootstrap. (arXiv:2009.08071), 2020.

[38] Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press,

2012.

[39] Dimitris N. Politis, Joseph P. Romano, and Michael Wolf. Subsampling. Springer-Verlag

New York, 1999.

[40] Xiaofeng Shao. The dependent wild bootstrap. Journal of the American Statistical

Association, 105(489):218–235, 2010.

[41] Gerald B. Folland. Real Analysis. John Wiley & Sons, 2nd edition, 1999.

[42] Dimitris N. Politis and Halbert White. Automatic block-length selection for the de-

pendent bootstrap. Econometric Reviews, 23(1):53–70, 2004.

[43] Tristen Hayﬁeld and Jeﬀrey S. Racine. Nonparametric econometrics: The np package.

Journal of Statistical Software, 27(5), 2008.

[44] Maria Fragkeskou and Efstathios Paparoditis. Inference for the fourth-order innovation

cumulant in linear time series. Journal of Time Series Analysis, 37(2):240–266, 2016.

[45] Raﬀaella Giacomini, Dimitris N. Politis, and Halbert White. A warp-speed method

for conducting monte carlo experiments involving bootstrap estimators. Econometric

Theory, 29(3):567 – 589, 2013.

[46] Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Gaussian approximations

and multiplier bootstrap for maxima of sums of high-dimensional random vectors. The

Annals of Statistics, 41(6):2786 – 2819, 2013.

[47] Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Comparison and anti-

concentration bounds for maxima of gaussian random vectors. Probability Theory and

Related Fields, 162:47–70, 2015.

22

A Preliminary

This section introduces some preliminary deﬁnitions and results used in this paper.

For any τ, ψ > 0, z

∈
n
i=1 exp(τ xi) +

1
τ log

R, deﬁne Fτ (x1, ..., xn) = 1

τ log

n
i=1 exp(τ xi)

; Gτ (x1, ..., xn) =

n
i=1 exp(

τ xi)

−

= Fτ (x1, ..., xn,

x1, ...,

(cid:0)P
−

xn); g0(x) = (1

(cid:1)

−

−

min(1, max(x, 0))4)4;

gψ,z(x) = g0(ψ(x

(cid:0)P

−

z)); and hτ,ψ,z(x1, ..., xn) = gψ,z(Gτ (x1, ..., xn)). From lemma A.2 and

P

(cid:1)

(8) in Chernozhukov et al. [36] and (S1) to (S5) in Zhang and Wu [28], g

′′
0 (x)

+

g
|
|
ψ, supx,z

g
|
g

∗

′′′
0 (x)
′′

) <
|
ψ,z(x)

R

∈

g
|

∞

. And 1x

z

≤

≤
ψ2 and supx,z

g

∗

| ≤

gψ,z(x)
′′′

≤
ψ,z(x)

R

∈

g
|

1x

≤

z+1/ψ. Also supx,z

R

∈

g
|

ψ3.

g

∗

| ≤

= supx

∗

∈

R(

′
0(x)

g
|
ψ,z(x)

′

+

|

| ≤

Deﬁne

=

∂i

·

∂
·
∂xi

, ∂i,j

=

·

∂2
∂xi∂xj

·

and ∂i,j,k

∂3
∂xi∂xj∂xk

·

=

·

(43)

we have ∂iFτ

0,

≥
6τ 2. Besides, Fτ (x1, ..., xn)

P

n
i=1 ∂iFτ = 1,

n
i=1

∂i,j Fτ

n
j=1 |
maxi=1,...,n xi

P

log(n)

P
τ ≤

−

2τ and

n
i=1

n
j=1

| ≤

n
k=1 |

∂i,j,kFτ

| ≤

Fτ (x1, ..., xn). Therefore,

P

P

P

≤

Gτ (x1, ..., xn)

log(2n)
τ

−

max
i=1,...,n |

xi

≤

| ≤

Gτ (x1, ..., xn)

(44)

and

∂iGτ = ∂iFτ (x1, ..., xn,

x1, ...,

xn)

−

−

−

∂i+nFτ (x1, ..., xn,

x1, ...,

xn)

−

⇒

−

n

i=1
X

∂iGi
|

| ≤

1

(45)

Similarly ∂i,jGτ = ∂i,jFτ

∂i+n,jFτ

−

−

∂i,j+nFτ + ∂i+n,j+nFτ

n
i=1

n
j=1 |

∂2Gτ
∂xi∂xj | ≤

2τ ;

⇒

and

P

P

∂i,j,kGτ = ∂i,j,kFτ + ∂i+n,j+n,kFτ + ∂i+n,j,k+nFτ + ∂i,j+n,k+nFτ

∂i+n,j,kFτ

−

−

∂i,j+n,kFτ

−

∂i,j,k+nFτ

−

∂i+n,j+n,k+nFτ

⇒

n

n

n

i=1
X

j=1
X

Xk=1

∂i,j,kGτ
|

| ≤

6τ 2

(46)

(47)

Since ∂ihτ,ψ,z = g

′

ψ,z(Gτ (x1, ..., xn))∂iGτ , we get

n
i=1 |

∂ihτ,ψ,z

ψ.

g

∗

| ≤

∂i,j hτ,ψ,z = g

′′
ψ,z(Gτ (x1, ..., xn))∂iGτ ∂jGτ + g

P

′
ψ,z(Gτ (x1, ..., xn))∂i,jGτ

n

n

⇒

i=1
X

j=1
X

∂i,jhτ,ψ,z
|

| ≤

g

∗

ψ2 + 2g

ψτ

∗

23

and

∂i,j,khτ,ψ,z = g

′′′
ψ,z(Gτ (x1, ..., xn))∂iGτ ∂jGτ ∂kGτ + g

′
ψ,z(Gτ (x1, ..., xn))∂i,j,kGτ

′′
ψ,z(Gτ (x1, ..., xn)) (∂i,jGτ ∂kGτ + ∂j,kGτ ∂iGτ + ∂i,kGτ ∂j Gτ )

+g

(48)

n

n

n

⇒

i=1
X

j=1
X

Xk=1

∂i,j,khτ,ψ,z
|

| ≤

g

∗

ψ3 + 6g

ψτ 2 + 6g

ψ2τ

∗

∗

We end this section by introducing a lemma about joint normal random variables from

Chernozhukov et al.[36].

Lemma 4

Suppose ǫ = (ǫ1, ..., ǫn)T are joint normal random variables, Eǫ = 0, EǫǫT = Σ = (σij)i,j=1,...,n;

and

∃

0 < c0

≤

C0 <

∞

such that c0

σii

≤

≤

C0, i = 1, ..., n. Then

P rob

sup
x

R |

∈

max
i=1,...,n |

ǫi

(cid:18)

x + δ

| ≤

−

(cid:19)

P rob

max
i=1,...,n |

ǫi

(cid:18)

| ≤

x

|

(cid:19)

(49)

≤

Cδ

1 +

log(n) +

(cid:16)

p

log(δ)

|
(cid:17)

|

p

In addition suppose ξ = (ξ1, ..., ξn)T are joint normal random variables with Eξ = 0, EξξT =

Σ∗ = (σ∗ij )i,j=1,...,n, and ∆ = maxi,j=1,...,n

σij
|

σ∗ij|

−

< 1. Then

P rob

sup
x

R |

∈

max
i=1,...,n |

ǫi

(cid:18)

| ≤

x

P rob

−

max
i=1,...,n |

ξi

x

| ≤

|

(cid:19)

≤

C ∗

∆1/3

(cid:18)

×

(cid:0)

(cid:18)

(cid:19)
1 + log3(n)

∆1/6
1 + log1/4(n) (cid:19)

+

(cid:1)

(50)

Here C and C ∗ only depend on c0 and C0.

We emphasize that Σ and Σ∗ in lemma 4 can be singular. Theorem 3 in Chernozhukov

et al. [46] holds true even if Σ is singular. On the other hand, if Σ∗ is singular, from theorem

4.1.5 in Horn and Johnson [38], Σ∗ = QΛQT with Λ = diag(λ1, ..., λr, 0, 0, ..., 0), 0

r < n,

≤

and Q =

qij
{

i,j=1,...,n satisﬁes QQT = QT Q = In, the n dimensional identity matrix.
}

Deﬁne τ = QT ξ = (τ1, ..., τr, 0, ..., 0)T almost surely. For any continuous diﬀerentiable

function such that E

n
i=1 |

∂if

|

<

∞

, lemma 2 in [47] implies

P

r

Eξif (ξ) =

qij Eτjf (Qτ ) =

j=1
X

r

r

n

j=1
X

Xk=1

Xl=1

qij qlkE(τj τk)

E∂lf (Qτ )

×

n

=

σ∗ilE∂lf (Qτ )

Xl=1

(51)

24

The validity of (51) for degenerated ξ makes sure that (50) works for degenerated ξ.

proof of lemma 4. Since

ǫi
|
ǫ has the same joint distribution as ǫ,

= max(ǫi,

ǫi)

−

|

and

−

maxi=1,...,n

ǫi
|

|

= max (maxi=1,...,n ǫi, maxi=1,...,n

ǫi),

−

⇒

P rob

sup
R
x

∈

(cid:18)

(cid:18)

max
i=1,...,n |

ǫi

| ≤

x + δ

P rob

(cid:18)

−

(cid:19)

max
i=1,...,n |

ǫi

| ≤

x

(cid:19)(cid:19)

P rob

≤

sup
R
x

∈

x < max

i=1,...,n

(cid:18)

ǫi

≤

x + δ

+ sup
R
x

∈

(cid:19)

P rob

x < max

i=1,...,n −

ǫi

≤

x + δ

(cid:18)

P rob

≤

2 sup
R
x

∈

max
i=1,...,n

ǫi

x

−

| ≤

δ

|

(cid:18)

(52)

(cid:19)

(cid:19)

From theorem 3 and (18), (19) in Chernozhukov et al. [46], deﬁne σ = mini=1,...,n Eǫ2

i and

σ = maxi=1,...,n Eǫ2
i ,

P rob

sup
R
x

∈

|

(cid:18)

max
i=1,...,n

ǫi

x

−

| ≤

+

√2δ
c0
≤
4√2δC0
c2
0

+

(1 +

2

×

|

log(c0)
c0

|

+

|

≤   p

and we prove (49).

δ

≤
(cid:19)
4√2δσ
σ2

√2δ
σ

(cid:16)p

log(n) +

max(1, log(σ)

(cid:16)p
log(n) + 2 +

p

max(0, log(σ)

−

−

log(δ))

(cid:17)

log(δ))

log(c0)

p
+

|

log(n) +

1 +

|

(cid:16)p

p

log(n) + 2 +

log(c0)

+

|

(cid:16)p
log(C0)

)
|

+

|
p
4√2C0
c2
0 ×

log(C0)

log(C0)

+

+

|

|

p

|

|

log(δ)

log(δ)

|

|

p
log(C0)

log(c0)

+

|

|

|

!

|
(cid:17)

log(n) +

log(δ)

(cid:16)

p

p

(cid:17)

2 +

(cid:16)

δ

×

p
1 +

(cid:17)

|
(cid:17)

|
(cid:17)

(53)

If ∆ = 0, then (50) holds true. So we assume ∆ > 0. Without loss of generality, assume

ǫ is independent of ξ. Similar to Chernozhukov et al. [46], for any 0

t

≤

≤

1, deﬁne random

variables Zi(t) = √tǫi + √1

−

tξi. According to (48), (51) and theorem 2.27 in Folland [41],

Ehτ,ψ,x(ǫ1, ..., ǫn)

−

Ehτ,ψ,x(ξ1, ..., ξn) =

1
2

n

i=1 Z[0,1]
X

t−

1/2E (ǫi

(1

t)−

1/2E (ξi

−

∂ihτ,ψ,x(Z1(t), ..., Zn(t))) dt

∂ihτ,ψ,x(Z1(t), ..., Zn(t))) dt

×

×

n

1
2

−

i=1 Z[0,1]
X
n
n
1
2

=

i=1
X

Xl=1

(σil

σ∗il)

−

Z[0,1]

E∂i,lhτ,ψ,x(Z1(t), ..., Zn(t))dt

Ehτ,ψ,x(ǫ1, ..., ǫn)

⇒ |

Ehτ,ψ,x(ξ1, ..., ξn)

−

∆

(g

∗

×

| ≤

ψ2 + g

ψτ )
∗
(54)

25

max
i=1,...,n |

ξi

| ≤

x

max
i=1,...,n |

ξi

| ≤

x

(cid:18)

(cid:18)

(cid:19)

(cid:19)

p
P rob

P rob

(cid:18)

(cid:18)

p
max
i=1,...,n |

ξi

x

| ≤

max
i=1,...,n |

ξi

| ≤

x

(cid:19)

(cid:19)

For any x

∈

R and given τ, ψ > 0, deﬁne t = 1

ψ + log(2n)

τ

,

P rob

(cid:18)

max
i=1,...,n |

ǫi

| ≤

x

−

(cid:19)

P rob

P rob

≤

max
i=1,...,n |

ǫi

(cid:18)

x

−

| ≤

(cid:19)

t

+ Ct(1 +

log(n) +

Ehτ,ψ,x

1
ψ

−

≤

(ǫ1, ..., ǫn)

p
Ehτ,ψ,x

1
ψ

−

−

log(t)

)
|

−

P rob

|

p

(ξ1, ..., ξn) + Ct(1 +

log(n) +

log(t)

)
|

|

P rob

(cid:18)

max
i=1,...,n |

ǫi

| ≤

x

−

(cid:19)

P rob

(cid:18)

max
i=1,...,n |

ǫi

| ≤

x + t

−

(cid:19)

Ct

1 +

log(n) +

(cid:16)

p

log(t)

−

|
(cid:17)

|

p

Eh

τ,ψ,x+ log(2n)

τ

(ǫ1, ..., ǫn)

−

Eh

τ,ψ,x+ log(2n)

τ

≥

≥

(ξ1, ..., ξn)

−

Ct

1 +

log(n) +

(cid:16)

p

log(t)

|
(cid:17)(55)

|

p

Therefore,

P rob

sup
x

R |

∈

max
i=1,...,n |

ǫi

(cid:18)

x

| ≤

−

(cid:19)

P rob

(cid:18)

max
i=1,...,n |

ξi

| ≤

x

(cid:19)

∆

g

∗

×

| ≤

(ψ2 + ψτ )

(56)

+Ct

1 +

log(n) +

(cid:16)

p

log(t)

|
(cid:17)

|

p

Choose τ = ψ = 1+log3/2(n)

∆1/3

, then

0 < t =

∆1/3(1 + log(2))
1 + log3/2(n)

+

∆1/3 log(n)
1 + log3/2(n) ≤

1 + log(2)
1 + log3/2(n)

+

3

1 + log1/2(n) ≤

4+log(2) (57)

for any n. So

∃

a constant C ∗ such that

∆(ψ2 +ψτ )+Ct(1+

log(n)+

g

∗

p

log(t)

)
|

≤

|

p

and we prove (50).

C ∗∆1/3(1+log3(n))+C ∗

∆1/6
1 + log1/4(n)

(58)

26

B Proofs of lemmas in section 3

proof of lemma 2. For ζi,k(j) = ǫi(j

k)ǫi+k(j)

−

−

Eǫiǫi+k. According to Cauchy - Schwarz

inequality and (7),

ζi,k
k

−

ζi,k(j)

ǫiǫi+k
k

−

ǫi(j

−

k)ǫi+k(j)

km/2

km/2 =
m +
k

ǫi(j
k

ǫi

≤ k

−

ǫi(j

−

⇒

k)

m

ǫi+k

k
× k
∆k,j,m/2 ≤

k)

m

k

−

ǫi+k

× k

−

ǫi+k(j)

m

k

Cδj

−

k,m + Cδj,m, Here C is a constant

(59)

(1 + j)αǫ

⇒

∞

Xl=j

∆k,l,m/2 ≤

C(1 + j)αǫ

δl

−

∞

Xl=j

k,m + C(1 + j)αǫ

δl,m

∞

Xl=j

and we prove (16).

On the other hand, since ∆k,l,m/2 = 0 for l < 0,

=

ζi,j ζi+t,k
k

−

ζi,j(l + j

j

−

∨

(k + t))ζi+t,k(l + k + t

j

∨

−

(k + t))

νi,t,j,k
k

−

νi,t,j,k(l)

ζi,j

≤ k

−

ζi,j(l + j

j

∨

−

(k + t))

km/2 × k

ζi+t,k

(k + t))

j

∨

km/2 × k

ζi+t,k

−

ζi+t,k(l + k + t

j

∨

−

(k + t))

km/4

km/4

km/2

km/2

(60)

j

∨

−

(k+t),m/2 + C∆k,l+k+t

j

∨

−

(k+t),m/2, C is a constant

⇒

∞

Xl=0

Θ(i,t,j,k),l,m/4 ≤

C

∆j,l,m/2 + C

∞

Xl=0

∆k,l,m/2

∞

Xl=0

+

ζi,j(l + j
k

−
Θ(i,t,j,k),l,m/4 ≤

⇒

C∆j,l+j

and we prove (17).

proof of lemma 3. For any ﬁxed i, j, the random variables ...e

1, e0, e1, ... and the random

−

variables ...ei

−

j

−

1, e†i
−

j, ei

−

1. From lemma 1,

j+1, ... have the same joint distribution. Deﬁne ψl as in lemma

∆i,j,m

≤

∞

Xk=0

ψl
|

δj
|

−

l,m

≤

⇒

∞

Xj=k

sup
Z
i
∈

∆i,j,m

≤

∞

∞

Xj=k

Xl=0

ψk
|

∞

ǫi

k

−

k(j

ǫi

−

k)

m

k

−

−

| × k

ψl
|

| ×

(1 + (k

C

l)

∨

−

0)αǫ

(61)

Xl=0
⌊

k/2

⌋

C
(1 + k/2)αǫ

≤

+ C

ψl
|

|

∞

k/2
Xl=
⌋
⌊

+1

ψl
|

|

Xl=0

Here C is a constant and

x
⌊

⌋

denotes the largest integer a such that a

x. For

ψl
|

| ≤

≤

27

C(1 + δ)−

l with a constant δ > 0, we prove (18).

C Proofs of theorems in section 4

proof of theorem 1. For

γi
k

−

γi

km/2 ≤ k

γi

E

γi

km/2 + i
n |

γi

−

, form lemma 3, lemma 2 and
|

(14)

b

b

b

γi
k

−

E

γi

km/2 =

k

1
n

n

(XjXj

j=i+1
X

i

−

−

γi)

km/2

2C√n
n

i

−

≤

Xj

sup
j

Z k

∈

m

k

Xj

× k

b
m +
k

i

−

∞

Xl=0

b
sup
j

∈

Z k

Xj Xj

Xj (l)Xj

i

−

−

i(l

−

−

i)

km/2

!

(62)

= O(1/√n)

In particular,

γi

−

γi = Op(1/√n). From assumption 5 and (5.8.4) in [38], we have

1

Γ−

2 = Op(1/√n). And
b
|

Γ is non-singular with probability tending to 1. So

a
|

a

2
|

−

and we prove (26).

b

b
Γ−

≤ |

b

1

1

Γ−

γ

× |

2 +
|

Γ−
|

1

2
|

2
|

−

γ

× |

γ

2 = Op(1/√n)
|

−

b

b

1

Γ−
|

−

b

(63)

Then we provide the theoretical justiﬁcation for theorem 2.

proof of theorem 2. First notice that

max
i=1,...,p1 |

bij (E

γj

γj )

|

−

=

1
n

max
i=1,...,p1 |

p

j=0
X

b
a constant C > 0 such that

So

∃

p

j=0
X

bij

jγj

|

×

= O(1/n)

(64)

P rob

max
i=1,2,...,p1 |

√n

P rob

≤

max
i=1,2,...,p1 |

√n

p

j=0
X

P rob

max
i=1,2,...,p1 |

p

j=0
X

−

p

j=0
X

bij (

γj

b
√n

bij(

γj

γj)

−

| ≤

x

!

E

b
γj)

| ≤

x + C/n

!

b
bij(

γj

γj)

−

| ≤

x

!

(65)

P rob

≥

max
i=1,2,...,p1 |

√n

p

j=0
X

bij (

γj

E

−

b
γj)

| ≤

x

−

C/n

!

b

b

28

 
 
 
 
 
for any n. From assumption 5,

Eξ2

i =

p

p

j1=0
X

j2=0
X

bij1 bij2 ×

1
n

n

n

Xl1=j1+1

Xl2=j2+1

for suﬃciently large n. From theorem 1

E(Xl1 Xl1−

j1 −

γj1 )(Xl2 Xl2−

j2 −

γj2 )

(66)

cζ

≥

p

j=0
X

b2
ij > 0

√n
k

p

j=0
X

bij (

γj

E

γj)

2
k

≤

−

b

b

p

j=0
X

bij
|

√n(

| × k

γj

γj)

2
k

≤

−

C

p + 1

b

p

p

× v
u
u
t

j=0
X

b2
ij

(67)

so from lemma 4 and (55), for any τ, ψ > 0, deﬁne t = 1

ψ + log(2p1)

τ

,

P rob

sup
x

R |

∈

√n max

i=1,...,p1 |

p

j=0
X

bij(

γj

E

γj)

x

| ≤

−

! −

P rob

max
i=1,...,p1 |

ξi

| ≤

x

|

(cid:19)

(cid:18)

b

b

Ct(1 +

log(p1) +

≤

log(t)

)
|

|

Ehτ,ψ,x

+ sup
x

R |

∈

p

√n

b1j (

γj

j=0
X

E

γj), ..., √n

−

b

b

p

j=0
X

bp1j (

γj

E

γj )

−

p

! −

p
Ehτ,ψ,x(ξ1, ..., ξp1 )

|

b

b

(68)

Here C is a constant. For any integer s

≥
0. From lemma 2, lemma 3 and (14),

j

s

≥

≥

p + 1

p + 1, deﬁne ιi,j,s = E(XiXi

−

j

−

γj)

|F

i,s, here

a constant C such that for j = 0, 1, ..., p and

∃

1
n

k

n

(XiXi

γj )

j

−

−

i=j+1
X

n

i=j+1
X

1
n

−

n

ιi,j,s

km/2 ≤

1
n

k

∞

t=s+1
X

n

i=j+1
X

ιi,j,t

1
n

−

n

i=j+1
X

∞

≤

t=s+1
X

1
n

k

i=j+1
X

n

XiXi

−

j

−

1
n

Xi(t)Xi

−

j (t

j)

−

km/2 ≤

√n

i=j+1
X

ιi,j,t

1

km/2

−

C
(1 + s)αǫ

(69)

×

Therefore, deﬁne rj,s = 1
n

n
i=j+1 ιi,j,s,

P

p

Ehτ,ψ,x

sup
x

R |

∈

√n

b1j (

γj

−

j=0
X

p

E

γj), ..., √n

bp1j(

γj

p

Ehτ,ψ,x

√n

b1j rj,s, ..., √n

E

γj)

−

! −

p

j=0
X

bp1jrj,s

! |

b

b

b

b

j=0
X

ψ√n

g

∗

E max

i=1,...,p1 |

×

≤

bij (

γj

E

γj

rj,s)

|

−

−

j=0
X

p

j=0
X

n

ιl,j,s

Xl=j+1

(70)

b
km/2 ≤

b
Cψ
(1 + s)αǫ

ψ√n

g

∗

×

≤

p1

p

i=1
X

j=0
X

bij
|

| × k

1
n

n

(XlXl

Xl=j+1

j

−

−

γj)

−

1
n

29

 
 
 
 
For any i,

p

√n

bij rj,s =

j=0
X

1
√n

n

(l

p

∧

−

1)

bij ιl,j,s

Xl=1

j=0
X

(71)

deﬁne Li,l,s =

1)

p
(l
∧
j=0

−

bij ιl,j,s for l = 1, 2, ..., n. Then Li,l,s is

l,s measurable. For any

F

given k > s, deﬁne the big block Si,l,s = 1
√n

P

((l
−
z=(l

×
1)

−

×

1)

(k+s)+k)

n

(k+s)+1 Li,z,s and the small block

∧

si,l,s = 1
√n

(l
×
z=(l

(k+s))
1)

n
(k+s)+k+1 Li,z,s, here l = 1, 2, ..., R, R =
∧

P

−

×

n
.
k+s ⌉

⌈

x
⌈

⌉

denotes the small-

est integer that is larger than or equal to x. Then the random vectors (S1,l,s, ..., Sp1,l,s)T , l =

P

1, 2, ..., R are mutually independent; and the random vectors (s1,l,s, ...sp1,l,s)T are mutually

independent. We also have

p

R

R

√n

bij rj,s =

Si,l,s +

si,l,s

j=0
X

Xl=1

Xl=1

(72)

From theorem 2 in [35],

Ehτ,ψ,x
|

g

∗

≤

ψE max

i=1,...,p1 |

R

Xl=1

p

p

√n

b1j rj,s, ..., √n

bp1j rj,s

j=0
X

j=0
X
R

p1

Ehτ,ψ,x

! −

R

Xl=1

R

Si,l,s, ...,

Si,l,s

Xl=1

! |

si,l,s

ψ

g

∗

| ≤

k

i=1
X

Xl=1

si,l,s

km/2 = O

ψ

(cid:18)

×

√R

max
l=1,...,R k

si,l,s

×

km/2

(cid:19)
(73)

Deﬁne ∆i,l,s,j = ELi,l,s

l,j

|F

−

ELi,l,s

l,j

−

|F

1, we have

Li,l,s = ELi,l,s

l,0 +

|F

s

j=1
X

and ∆i,l,s,j =

From (19)

(l

p

∧

−

1)

ν=0
X

(ELi,l,s

l,j

|F

−

ELi,l,s

l,j

−

|F

1) = ELi,l,s

|F

s

l,0 +

∆i,l,s,j

j=1
X

(74)

biν (EXlXl

ν

−

|F

l,j

−

EXlXl

ν

−

|F

l,j

−

1) for j

s

≤

Li,l,s
k

km/2 ≤

(l

p

∧

−

1)

j=0
X

bij
|

| ×

(

XlXl
k

j

−

km/2 +

γj
|

)
|

⇒

l

sup
Z,s

Li,l,s

0 k

km/2 <

∞

(75)

∈

≥

and

∆i,l,s,j
k

km/2 ≤

(l

p

∧

−

1)

ν=0
X

(76)

biν
|

XlXl

| × k

ν

−

−

Xl(j)Xl

ν (j

−

−

ν)

km/2

Xl

sup
l

Z k

∈

Xl(j

ν)

m

k

−

−

C

≤

p

ν=0
X

30

 
 
for a constant C. Notice that

si,l,s
k

km/2 ≤ k

1
√n

s

+

from theorem 2 in Whittle [35]

1
√n

k

j=1
X

(l

×

(k+s))

n

∧

(k+s)+k+1

1)
Xz=(l
−
(l
×

×

(k+s))

n

∧

ELi,z,s

z,0

km/2

|F

1)
Xz=(l
−

×

(k+s)+k+1

(77)

∆i,z,s,j

km/2

1
√n

k

from (12),

(l

×

(k+s))

n

∧

1)
Xz=(l
−

×

(k+s)+k+1

ELi,z,s

z,0

km/2 ≤

|F

C√s
√n ×

Li,z,s

sup
z

Z k

∈

km/2

(78)

(l

×

(k+s))

n

∧

1
√n

k

1)
Xz=(l
−
Therefore, from lemma 2 and 3

×

(k+s)+k+1

∆i,z,s,j

km/2 ≤

C√s
√n

∆i,z,s,j

sup
z

Z k

∈

km/2

(79)

Ehτ,ψ,x
|

p

√n

b1j rj,s, ..., √n

j=0
X

p

j=0
X

bp1j rj,s

! −

Ehτ,ψ,x

R

Xl=1

R

Si,l,s, ...,

Si,l,s

! |

(80)

Xl=1

= O

ψ

(cid:18)

r

s
k

(cid:19)

Deﬁne S∗j,s = (S∗1,j,s, ..., S∗p1,j,s)T , j = 1, ..., R as joint normal random vectors with ES∗i,j,s =

0 and ES∗i1,j,sS∗i2,j,s = ESi1,j,sSi2,j,s. S∗j,s is independent with S ∗k,s for j
S∗j,s is independent with (S1,k,s, ..., Sp1,k,s)T for any k. Here i, i1, i2 = 1, ..., p1. Deﬁne

= k; and

1

Hi,j,s =

j
k=1 Si,k,s +
−

R
k=j+1 S∗i,k,s, then Hi,j,s + Si,j,s = Hi,j+1,s + S∗i,j+1,s. Deﬁne
H j,s = (H1,j,s, ..., Hp1,j,s)T and Sj,s = (S1,j,s, ..., Sp1,j,s)T . From Taylor’s theorem and (48)

P

P

E
|

p1

p1

(cid:0)

hτ,ψ,x(H j,s + Sj,s)
p1

hτ,ψ,x(H j,s + S∗j,s)

−

H j,s|
|

E

≤ |

i=1
X

∂ihτ,ψ,x(H j,s)(Si,j,s

S∗i,j,s)

−

(cid:1)
! |

H j,s|

∂i1,i2 hτ,ψ,x(H j,s)(Si1,j,sSi2,j,s

S∗i1,j,sS∗i2,j,s)

−

H j,s|

! |

(81)

+

1
2 |

E

i1=1
X

i2=1
X

g

∗

+

(ψ3 + 6ψτ 2 + 6ψ2τ )
6

E max

i=1,...,p1 |

Si,j,s

×

(cid:18)

3 + E max
|
p1

i=1,...,p1 |

3

S∗i,j,s|

(cid:19)

(ψ3 + ψ2τ + ψτ 2)

g

∗

≤

Si,j,s
k
(cid:0)

i=1
X

3
3 +
k

S∗i,j,sk
k

3
3

(cid:1)

31

 
 
6
 
 
So

Ehτ,ψ,x

sup
x

R |

∈

R

j=1
X

Sj,s

! −

Ehτ,ψ,x

R

j=1
X

S∗j,s

! | ≤

R

j=1
X

Ehτ,ψ,x(H j,s + Sj,s)

sup
x

R |

∈

Ehτ,ψ,x(H j,s + S∗j,s)

|

−

= O

(ψ3 + ψ2τ + ψτ 2)

R

p1

j=1
X

i=1
X

(cid:0)

Si,j,s
k

3
3 +
k

S∗i,j,sk
k

3
3

(82)

!
(cid:1)

For S∗i,j,s has normal distribution,

a constant C such that

∃

S∗i,j,sk
k

3
3 ≤

C

S∗i,j,sk
k

3
2 ≤

C

Si,j,s
k

3
3. Similar to (77),
k

Si,j,s
k

3
k

Si,j,s

≤ k

km/2 ≤

k
n

C

r

(83)

for any j. Here C is a constant. From section A,

Ehτ,ψ,x

R

j=1
X

S ∗j,s

! −

Ehτ,ψ,x(ξ1, ..., ξp1 )

S∗i,j,s| ≤

x +

1
ψ ! −

P rob

max
i=1,...,p1 |

ξi

(cid:18)
R

x

−

| ≤

log(2p1)
τ

(cid:19)

Ehτ,ψ,x

S ∗j,s

! −

j=1
X

Ehτ,ψ,x(ξ1, ..., ξp1 )

S∗i,j,s| ≤

x

−

log(2p1)
τ

P rob

! −

max
i=1,...,p1 |

ξi

(cid:18)

x +

| ≤

1
ψ

(cid:19)

(84)

P rob

≤

max
i=1,...,p1 |

P rob

≥

max
i=1,...,p1 |

R

j=1
X

R

j=1
X

≤

P rob

sup
x

R |

∈

R

max
i=1,...,p1 |

j=1
X
p1

⇒

Ehτ,ψ,x

sup
x

R |

∈

R

j=1
X

S∗j,s

! −

Ehτ,ψ,x(ξ1, ..., ξp1 )

S∗i,j,s| ≤

x

! −

P rob

(cid:18)

max
i=1,...,p1 |

ξi

| ≤

x

(cid:19)

+

sup
R
x

∈

i=1
X

P rob

x

(cid:18)

log(2p1)
τ

−

ξi

≤ |

| ≤

x +

1
ψ

(cid:19)

|

|

Since √n

p
j=0 bij (

γj

E

γj) = 1
√n

−

n
l=1

1)

p
(l
∧
j=0

−

bij (XlXl

γj), deﬁne Ti,l =

j

−

−

1)

p
(l
∧
j=0

−

bij(XlXl

j

−

−

P

b

b

P

P

P

32

 
 
 
 
 
 
 
 
 
γj) for l = 1, 2, ..., n. Since ES∗i1,j1,sS∗i2,j2,s = ESi1,j1,sSi2,j2,s = 0 if j1

= j2, we have

R

R

R

R

ES∗i1,j1,sS∗i2,j2,s −

Eξi1 ξi2 |

=

|

|

j1=1
X

j2=1
X

R

R

≤ |

j1=1
X

j2=1
X

ESi1,j1,sSi2,j2,s

E

−

n

n

Xl1=1

Xl2=1

−

1
n

R

ETi1,l1 Ti2,l2 |

R

! ×  

Xl=1

Si2,l,s +

si2,l,s

Xl=1

! |

ESi1,j1,sSi2,j2,s

j1=1
X
R

j2=1
X

R

Si1,l,s +

si1,l,s

Xl=1
n

n

Xl=1

+

1
n |

(ELi1,l1,sLi2,l2,s

Xl1=1
R

Xl2=1

R

Si2,j,s +

si2,j,s

j=1
X
1
n k

j=1
X

Ti2,l

2
k

× k

n

Xl=1

n

Xl=1

ETi1,l1 Ti2,l2 )

|

−

2
k

× k

R

j=1
X

si1,j,s

2
k

(Li1,l,s

−

Ti1,l)

2
k
(85)

R

≤ k

n

j=1
X

Si1,j,s

2
k

× k

si2,j,s

2 +
k

k

R

j=1
X

+

1
n k

Li1,l,s

2
k

× k

Xl=1

n

Xl=1

(Li2,l,s

Ti2,l)

−

2 +
k

For Li,l,s = ETi,l

|F

l,s, from (14) and lemma 2

n

k

Xl=1

n

(Li1,l,s

Ti1,l)

2
k

≤

−

Ti,l

2
k

≤ k

k

Xl=1

n

Xl=1

∞

n

k

Xk=s+1

Xl=1
km/2 +

Li,l,0

(Li,l,k

∞

−

n

k

Xl=1

Xk=1

Li,l,k

1)

km/2 = O

−

√n
(1 + s)αǫ

(cid:18)

(cid:19)

(Li,l,k

Li,l,k

1)

km/2 = O(√n)

−

−

(86)

According to (74) to (79) and (83)

R

k

j=1
X

Si1,j,s

2
2 =
k

R

j=1
X

ES2

i1,j,s = O (1) ;

R

k

j=1
X

si1,j,s

2
2 =
k

R

j=1
X

Es2

i1,j,s = O

s
k

(cid:16)

(cid:17)

(87)

From lemma 4

R

R

|

j1=1
X

j2=1
X

ES∗i1,j,sS∗i2,j,s −

Eξi1 ξi2 |

= O

⇒

P rob

sup
x

R |

∈

max
i=1,...,p1 |

R

j=1
X

S∗i,j,s| ≤

x

! −

P rob

max
i=1,...,p1 |

ξi

(cid:18)

= O

| ≤

x

|

(cid:19)

s
k

(cid:18)(cid:16)

(cid:17)

(cid:18)r

1/12

s
k

+

1
(1 + s)αǫ

+

1
(1 + s)αǫ/6

(88)

(cid:19)

(cid:19)

From assumption 5, P rob

x

H(x

C
n )

p1
i=1 supx

−

| ≤
Select k =

∈
, s =
⌋
(70), (80), (82) and (88), we prove (29).

n1/8
⌊

P
√n
⌊

−

⌋

−
(cid:16)
R P rob

x

≤ |
C
n <

| ≤
ǫi
|
and τ = ψ = log4(n)
(cid:0)

| ≤

x

(cid:1)

log(2p1)
τ

ξi

x + 1
ψ

= O

1

ψ + 1

τ

(cid:17)

(cid:16)
= O(1/n).

and supx

R

∈

H(x)
|

−

(cid:17)

t = O(1/ log4(n)). From (65), (68),

⇒

33

6
 
 
D Proofs of theorems in section 5

This section justiﬁes the validity of bootstrap algorithm 1. We ﬁrst introduce a lemma.

Lemma 5

Suppose assumption 1 to 6. Deﬁne

ζi,k = XiXi

−

k

−

γk and ζi,k = XiXi

−

γk for k =

k

−

0, 1, ..., p and i = k + 1, ..., n.

γk is deﬁned in (23). Then for any given k1, k2 = 0, 1, 2, ..., p,

b

b

1
n

|

n

n

b
ζj2,k2 K

ζj1,k1

Xj1=k1+1

Xj2=k2+1

b

b

j1

j2
−
kn (cid:19)
−

1
n

(cid:18)

n

n

Xj1=k1+1

Xj2=k2+1

Eζj1,k1 ζj2,k2 |

= Op

kn
√n

(cid:18)

+ vn

(89)

(cid:19)

Here

vn =

k1
n

−

αǫ

if 1 < αǫ < 2

log(kn)/kn if αǫ = 2

1/kn if αǫ > 2






(90)

proof of lemma 5. Deﬁne

ζi,k = ζi,k = 0 for i = 1, 2, ..., k. Then for any i, t > 0 such that

i + t

n

≤

b

Eζi,k1 ζi+t,k2 |
|

=

Eζi,k1 ×
|

(ζi+t,k2 −

Eζi+t,k2 |F

i+t,t

1)

|

−

Eζi+t,k2 |F
k

i+t,j

Eζi+t,k2 |F

−

i+t,j

1

2
k

−

≤

C
(1 + t)αǫ

×

∞

j=t
X

ζi,k1 k

2

≤ k

for a constant C. From section 0.9.7 in [38]

1
n

|

n

n

j1=1
X

j2=1
X

Eζj1,k1 ζj2,k2 K

n

n

C
n

≤

j1=1
X

j2=1
X

(1 +

j1
|

1
n

−

j1

(cid:18)

j2
−
kn (cid:19)
1

j2

)αǫ ×
|

1

−

∞

n

n

j1=1
X

j2=1
X

Eζj1,k1 ζj2,k2 |

j1

K

−

1

(cid:18)

(cid:18)

j2
−
kn (cid:19)(cid:19)
s
kn (cid:19)(cid:19)

(cid:18)

(1 + s)αǫ ×

s=0
X

K

1

−

(cid:18)

2C

≤

K is continuous diﬀerentiable, so

(91)

(92)

1

(1 + s)αǫ ×

K

−

1

(cid:18)

(cid:18)

s
kn (cid:19)(cid:19)

≤

maxx

∈

K

[0,1] |
kn

∞

s=0
X

kn

′

(x)

|

s=0
X

s
(1 + s)αǫ

+

1
(1 + s)αǫ

∞

Xs=kn+1

= O

1
kn

+

1
kn Z[1,kn]

x1

−

αǫ dx +

x−

αǫ dx

Z[kn,

∞

)

!
(93)

34

 
so

1
n

n
j1=1

n

j2=1 Eζj1,k1 ζj2,k2 K

|
P
On the other hand

P

j2

j1−
kn

(cid:16)

(cid:17)

1
n

−

n
j1=1

n

j2=1 Eζj1,k1 ζj2,k2 |

= O(vn).

P

P

1
n

k

≤

n

n

j2=1
X
1

j1=1
X
n
1
−
n

(ζj1,k1 ζj2,k2 −

Eζj1,k1 ζj2,k2 ) K

j1

j2
−
kn (cid:19)

(cid:18)

km/4

K

K

(cid:18)

(cid:18)

l
kn (cid:19)
l
kn (cid:19)

× k

× k

l

n

−

j=1
X
l
n
−

j=1
X

(ζj,k1 ζj+l,k2 −

Eζj,k1 ζj+l,k2 )

km/4

(94)

(ζj+l,k1ζj,k2 −

Eζj+l,k1 ζj,k2 )

km/4

Xl=0
1
n
−

Xl=1

+

1
n

From (13) and (14)

l

n

−

k

j=1
X

(ζj,k1 ζj+l,k2 −

Eζj,k1 ζj+l,k2 )

km/4 ≤

C

l

n

−

ζj,k1 ζj+l,k2 −
k

Eζj,k1 ζj+l,k2 k

2
m/4

∞

l

n

−

+C

j=1
X

v
u
u
t
ζj,k1 ζj+l,k2 −
k

v
u
u
t

−

j=1
X
km/2 + C

l)

ζj,k1 (s

−

l)ζj+l,k2 (s)

2
m/4
k

∞

′

√n

s=1
X

sup
j

Z k

ζj,k2 −

∈

ζj,k2 (s)

km/2

(95)

≤

′

C

√n + C

∞

′

√n

s=1
X

sup
j

Z k

ζj,k1 −

∈

s=1
X

ζj,k1 (s

Here C, C

′

are two constants. So

1
n

k

n

n

j1=1
X

j2=1
X

(ζj1,k1 ζj2,k2 −

Eζj1,k1 ζj2,k2 ) K

j1

j2
−
kn (cid:19)

(cid:18)

km/4 = O

1
√n

∞

K

Xl=0

l
kn (cid:19)!

(cid:18)

(96)

Since

∞l=0 K

l
kn

= O(1 + kn

) K(x)dx) = O(kn), we have

[0,

∞

(cid:16)

(cid:17)

P
n

n

ζj1,k1 ζj2,k2 K

j1=1
X

j2=1
X

1
n

|

j1

R
j2
−
kn (cid:19)

(cid:18)

1
n

−

n

n

Xj1=k1+1

Xj2=k2+1

Eζj1,k1 ζj2,k2 |

= Op

kn
√n

(cid:18)

+ vn

(cid:19)
(97)

35

 
For

1
n

|

1
n

≤ |

n

n

j1=1
X

j2=1
X

n

n

ζj1,k1 ζj2,k2 K

n

n

ζj1,k1

ζj2,k2 K

j1

(cid:18)

j1=1
X

j2=1
X
γk2 −

ζj1,k1 (

b
b
γk2 )K

j1

(cid:18)

j2
−
kn (cid:19)
1
n

+

|

|

n

n

1
n

−

n

j1=1
X
n

j2=1
X
γk1 −

(
j2=1
j1=1
X
X
γk1 −

b
γk1 )

j2
−
kn (cid:19)
1
n

+

|

b

γk1 )ζj2,k2 K

(

γk2 −

×

γk2 )K

≤

γk2 −
2
|

γk2 |
√n v
u
u
t

b

n

j=1
X

ζ 2
j,k1 ×

∞

K

s=0
X

(
j2=1
X

j1=1
X
s
kn (cid:19)

(cid:18)

(
+2
|

b
γk1 −

b

b
γk1 −
2
|

+

b

n

γk1 |
√n v
u
u
t
γk2 −

γk1 )

×

j=1
X

(

ζ 2
j,k2 ×

γk2 )

| ×

b

j1

j1

j1

j2
−
kn (cid:19)
j2
−
kn (cid:19)
j2
−
kn (cid:19)

|

|

|

(cid:18)

(cid:18)

(cid:18)

∞

K

K

s
kn (cid:19)
s
kn (cid:19)
(98)

(cid:18)

(cid:18)

s=0
X
∞

s=0
X

Since E

n

j=1 ζ 2

j,k1 ≤

Cn for a constant C, from theorem 1 we have

1
n

|

P
n

n

ζj1,k1

ζj2,k2 K

j1=1
X

j2=1
X

b

b

j1

j2
−
kn (cid:19)

1
n

−

(cid:18)

n

n

j1=1
X

j2=1
X

ζj1,k1 ζj2,k2 K

j1

j2
−
kn (cid:19)

|

(cid:18)

= Op

(cid:18)

kn
√n

(cid:19)
(99)

From (97) and (99), we prove (89).

In particular, from lemma 5, for given numbers bij , i = i1, i2, j = 0, 2, ..., p,

1
n

|

p

p

n

n

bi1k1 bi2k2

ζj1,k1

ζj2,k2 K

Xk1=0

Xk2=0

Xj1=k1+1

Xj2=k2+1

b

b

j1

j2
−
kn (cid:19)

1
n

−

(cid:18)

p

p

n

n

bi1k1 bi2k2

Xk1=0

Xk2=0

Xj1=k1+1
(100)

Xj2=k2+1

Eζj1,k1 ζj2,k2 |

has order Op

kn
√n + vn

.

(cid:16)

(cid:17)

proof of theorem 3. Form lemma 5

n

n

nE∗(

γ∗i −

γi)2 =

1
n

n

b

b
=

1
n

j1=i+1
X
n

j2=i+1
X
E(Xj1 Xj1 −

j1=i+1
X

j2=i+1
X

(Xj1 Xj1−

γi)

i

−

×

(Xj2 Xj2−

i

−

γi)K

b

γi)(Xj2 Xj2−

i

−

i

−

b

γi) + Op

(cid:18)

j1

(cid:18)

kn
√n

j2
−
kn (cid:19)

+ vn

(cid:19)

(101)

vn is deﬁned in (90). So for any given 0 < δ < 1,

a constant C such that P rob∗ (

∃

δ with probability tending to 1. From assumption 5 and theorem 1, the event

> C/√n) <

γi

γ∗i −
|
|
a constant
b

b
∃

c > 0 such that the smallest eigenvalue of

Γ is greater than c has probability tending to 1.

If

Γ’s smallest eigenvalue is greater than c and

b

1(

Γ∗

Γ−
|

Γ)

2 < 1/2, from corollary 5.6.16
|

−

b

b

b

b

36

in [38],

Γ∗ is non-singular and

Γ∗−

1 = (

∞k=0(

−

1)k(

Γ−

1(

Γ∗

b

√n

B
|

2
|

≤

γ∗

× |

2
|

Γ−

× |

1

(B

a∗

P
B

−

a)

b
Γ−

B

b
1(
Γ∗

−

−

(cid:16)
b
k
2 + √n
B
|
|

b
√n
|

∞

Xk=2

b

b
a constant C > 0 such that

b

b

and

∀

δ > 0,

∃

| ×

1(
b
Γ−
|

Γ∗

b
Γ)
−

b
2
× |
|

b
Γ−

b
2
1
b
2 × |
|

Γ∗

b
Γ
−

2
|

γ∗

b
× |

b

b

b

b

b

Γ))k)

Γ−

1. So

b
Γ)

b
1
γ + B

Γ−

Γ−

1(

γ∗

γ)

−

−

−

2
|

(cid:17)
γ

b
2
|
−
(102)
b

P rob∗

√n

(cid:16)

(B

a∗

|

−

B

a)

−

−

(cid:16)

b

b

with probability tending to 1.

B

Γ−

1(

Γ∗

−

Γ)

Γ−

1

γ + B

Γ−

1(

γ∗

γ)

−

b

b

b

b

b

b

b

b

2 > C/√n
|

(cid:17)

(cid:17)

< δ

(103)

Deﬁne the p

ti,jk
{
×
otherwise. And deﬁne δi = (0, 0, ..., 0

p matrix Ti =

, 1, 0, ..., 0)T . Then

j,k=1,...,p such that ti,jk = 1 if j
}

−

k = i, and 0

1

i
−
{z

|

}

√nB

Γ−

1(

Γ∗

−

p

=

i=1
X

√n(

γ∗i −

γi)B

Γ−

b
1δi

−

b
b
√n(

b
γ∗
i
| −
|

γ

i
|

|

b
Γ−
)B

1Ti
b

Γ−

1
b
γ

−
1

p
b

−

p

i=1
X
−

1

Γ)

Γ−

γ + √nB

Γ−

1(

γ∗

γ)

−

√n

⇒

p

j=1
X

bij (

a∗j −

aj) = √n(

b
γ∗p −

b
γp)

×

b
bT
i B

Γ−

1δp

√n(

−

b
γ0)bT

i B

b
Γ−

1T0

b
Γ−

1

b
γ

b
γ∗0 −

(104)

p

1
b
−

b

+

√n(

j=1
X

b
γ∗j −

γj )

b

×

b

b

b
1δj
Γ−

bT
i B

(cid:16)

b

b
b
1(Tj + T
Γ−

bT
i B

j )

−

−

b
Γ−

1

γ

b
b
+ Ai

p

b
=

cij √n(

b
γ∗j −

(cid:17)

b
γj) + Ai

Here

ci0 =

−

bT
i B

Γ−

1T0

Γ−

1

γ,

cip = bT

i B

Γ−

1δp, and

j=0
X

b
cij = bT

i B

b
1δj
Γ−

b

−

bT
i B

Γ−

1(Tj +

1

T

j)

Γ−
b

−

γ for j = 1, 2, ..., p
b

b

b
−

b

1. From theorem 1,

b

cij
|

cij

|

b
−

= Op(1/√n). cij is deﬁned

b

b

in corollary 1. From (103) A satisﬁes P rob∗(

b

b

1. From lemma 5, for any i1, i2,

Ai
|

|

> C/√n) < δ with probability tending to

b

p

p

nE∗

p

j1=0
X

p

j2=0
X

ci1j1 (

γ∗j1 −

γj1 )

×

ci2j2 (

γ∗j2 −

γj2 ) = n

ci1j1

ci2j2 E∗(

γ∗j1 −

γj1 )

(

γ∗j2 −

×

γj2 )

j1=0
X

j2=0
X

=

b
1
n

p

p
b

p

b
ci1j1

j1=0
X
p

j2=0
X

b

ci1j1 ci2j2

ci2j2

n

b

n

b

b

n

b

Xl1=j1+1
n

Xl2=j2+1
E(Xl1 Xl1−

j1=0
X

j2=0
X

Xl1=j1+1

Xl2=j2+1

=

1
n

(Xl1 Xl1−

j1 −

b
b
γj1 )(Xl2 Xl2−

b
j2 −

b
γj2 )

K

×

b

l1

(cid:18)

b
l2
−
kn (cid:19)

b

γj1 )(Xl2 Xl2−

j2 −

j1 −

b

γj2 ) + Op(kn/√n + vn)

(105)

37

From lemma 4, we prove (42).

38

