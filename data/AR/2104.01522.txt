TSNAT: Two-Step Non-Autoregressvie Transformer Models
for Speech Recognition

Zhengkun Tian1,2, Jiangyan Yi1, Jianhua Tao1,2,3, Ye Bai1,2, Shuai Zhang1,2,
Zhengqi Wen1, Xuefei Liu1

1NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China
2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences, Beijing, China
3 CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China
{zhengkun.tian, jiangyan.yi, jhtao, ye.bai, shuai.zhang, zqwen, xuefei.liu}@nlpr.ia.ac.cn

1
2
0
2

r
p
A
4

]
S
A
.
s
s
e
e
[

1
v
2
2
5
1
0
.
4
0
1
2
:
v
i
X
r
a

Abstract

The autoregressive (AR) models, such as attention-based
encoder-decoder models and RNN-Transducer, have achieved
great success in speech recognition. They predict the out-
put sequence conditioned on the previous tokens and acous-
tic encoded states, which is inefﬁcient on GPUs. The non-
autoregressive (NAR) models can get rid of the temporal de-
pendency between the output tokens and predict the entire out-
put tokens in at least one step. However, the NAR model still
faces two major problems. On the one hand, there is still a
great gap in performance between the NAR models and the
advanced AR models. On the other hand, it’s difﬁcult for
most of the NAR models to train and converge. To address
these two problems, we propose a new model named the two-
step non-autoregressive transformer(TSNAT), which improves
the performance and accelerating the convergence of the NAR
model by learning prior knowledge from a parameters-sharing
AR model. Furthermore, we introduce the two-stage method
into the inference process, which improves the model perfor-
mance greatly. All the experiments are conducted on a public
Chinese mandarin dataset ASIEHLL-1. The results show that
the TSNAT can achieve a competitive performance with the AR
model and outperform many complicated NAR models.
Index Terms: Autoregressive, Non-Autoregressive, Trans-
former, Two-Step, Speech Recognition

1. Introduction

End-to-end models have achieved great success in speech
recognition, especially attention-based encoder-decoder models
[1, 2, 3, 4] and transducer-based models [5, 6, 7, 8, 9]. Most of
these end-to-end models generate the target sequence in an au-
toregressive fashion, which predicts the next token conditioned
on the previously generated tokens and the acoustic encoded se-
quence. The autoregressive characteristic makes the inference
process must be carried out step by step, which cannot be im-
plemented in parallel and results in a large latency. By contrast,
the non-autoregressive model (NAR) can get rid of the temporal
dependency and directly generate the target sequences based on
the encoded acoustic states in at least one step.

Although the Non-autoregressive models can perform in-
ference very efﬁciently, it still faces two signiﬁcant problems.
On the one hand, there is still a great gap in performance be-
tween the advanced autoregressive (AR) model and the non-
autoregressive model. Chen et.al proposed two kinds of itera-
tive inference methods to alleviate the problem [10]. Too many
iterations have a negative impact on the speed of inference. Be-
sides, some researchers also utilized a CTC Model to generate

the preliminary predictions and then correct the previous pre-
diction by a non-autoregressive decoder [11, 12, 13]. However,
it will introduce some new problems.
It is hard to eliminate
accumulated error caused by the CTC Models and carry out
the frame-wise operation of the CTC module in parallel. On
the other hand, it is difﬁcult for most of the non-autoregressive
transformer models to train and converge. To our knowledge,
there are three major ways to alleviate this problem. Firstly,
some works try to introduce an auxiliary CTC loss to acceler-
ate the training and convergence [12, 14]. Secondly, more it-
erations in the training process also bring the improvement on
the performance [10, 15], which is very simple and straightfor-
ward. Thirdly, instead of learning from a sequence partially or
fully ﬁlled with <MASK> [10, 15], some works try to improve
training efﬁciency by providing a preliminary sequence with the
information of target sequence to the non-autoregressive trans-
former decoder [11, 12, 13, 14]. These methods are either time-
consuming or difﬁcult to implement.

The traditional NAR model predicts the output tokens con-
ditional on the acoustic encoded states and a sequence that is
completely empty and does not contain any prior knowledge. In
order to make the training match the inference, the NAR model
tries to learn from the sequence fully ﬁlled with <MASK> in
the training process, which hinders the improvement of train-
ing speed and performance. To address this problem, inspired
by the dual-mode ASR (streaming and non-streaming) [16] and
two-pass end-to-end models [17, 18, 19], we propose a model
named two-step non-autoregressive transformer (TSNAT). The
TSNAT utilizes an AR model to accelerate the NAR model
convergence and improve the performance. Our model con-
sists of an optional frond-end block, an acoustic encoder, and
a dual-mode transformer decoder. The dual-mode transformer
decoder can model the context in both an autoregressive and a
non-autoregressive way. Different from the traditional method
that transfer knowledge into the NAR model from a pre-trained
AR model [20], we train an AR model and a NAR model
from scratch simultaneously. The NAR model can learn some
linguistic prior knowledge from the AR model depending on
the dual-mode transformer decoder. During the inference, our
model can perform two-step decoding without depending on the
external attention decoder. All the experiments are conducted
on a public Chinese mandarin dataset ASIEHLL-1. The results
show that the TSNAT can achieve a competitive performance
with the AR model and outperform other NAR models.

The remainder of this paper is organized as follows. Sec-
tion 2 describes our proposed model. Section 3 presents our
experimental setup and results. The conclusions will be given
in Section 4.

 
 
 
 
 
 
(a) The Structure of Two-Step Non-Autoregressive Transformer

(b) The First-Step Inference Graph

Figure 1: (a) illustrates the structure of our proposed two-step non-autoregressive transformer (TSNAT) and the inference process. The
TSNAT model consists of an optional front-end block, an acoustic encoder, and a dual-mode transformer decoder. The inference can be
divided into two-step, pre-selection and rescoring. The dual-mode decoder ﬁrst generates n-best hypothesizes in a non-autoregressive
and then rescores these hypotheses in an autoregressive way. (b) illustrates an output probability graph of the non-autoregressive
model. The ﬁrst red circle of each column means the end-of-sentence token <EOS>. The other black circle represents different tokens
of vocabulary. Each path that starts with the blank circle and ends with a red circle in the graph represents a possible hypothesis.

2. Two-Step Non-Autoregressive
Transformer

where parameters W∗ and b∗ are learnable weight matrix and
bias respectively.

Our proposed two-step non-autoregressive transformer, as
shown in Fig.1(a), consists of three components, an optional
convolutional front end block, an acoustic encoder, and a dual-
mode transformer decoder.

2.1. The Convolutional Front End and Acoustic Encoder
The convolutional front end block generally consists of two 2D-
Convolution layers and an output project layer [4]. The convo-
lution layer is mainly responsible for the processing and down-
sampling of the low-level acoustic features. The last output
project layer will project the processed acoustic feature into the
dimension that the encoder required. The convolutional front
end block also can be replaced with the linear project layers
with splicing frame operation [21].

We utilize the transformer encoder [22, 4] as the acoustic
encoder, which contains a positional embedding, N repetitive
multi-head self-attention layers (MHA), and feed-forward net-
work layer (FFN). The sine-cosine positional embedding pro-
posed by [22] is applied for all the experiments in this paper.
Besides, the model also applies residual connection and layer
normalization.

The multi-head self-attention layers depend on all contexts
to compute the attention weights from different perspectives,
which makes it powerful to model long-range temporal infor-
mation.

SAi(Q, K, V ) = Softmax((Qt)K (cid:62)/

(cid:112)

dk)V

MHA(Q, K, V ) = Concat(SA0, SA1, ..., SAH )

(1)

where Q, K and V indicate the query, key, and value matrix re-
spectively. dk is the last dimension of K. The multi-head atten-
tion concatenates H output vectors of the self-attention mecha-
nism. Besides, both the input of self-attention and the output of
multi-head attention should equip with an independent weight
matrix respectively. The feed-forward network (FFN) contains
two linear layers and a gated liner unit (GLU) [23] activation
function [7].

FFN(x) = GLU(xW1 + b1)W2 + b2

(2)

L
(cid:89)

2.2. The Dual-Mode Transformer Decoder
The major difference between the autoregressive model and the
non-autoregressive model focus on the structure of the decoder.
The AR models force themselves to learn the linguistic depen-
dencies by blocking out the future information. This character-
istic makes the AR model predict the output sequence step by
step. The conditional probability PAR(Y |X) can be expressed
as:

PAR(Y |X) = P (y1|X)

P (yi|y<i, X)

(3)

i=2

where X is the acoustic encoded sequence with length T and
yi denotes the i-th token of predicted sequence with length L.
However, the non-autoregressive models get rid of the depen-
dencies on the previously predicted tokens. Each step in the
inference process is independent of each other, which makes
the inference step can be implemented in parallel and greatly
improves the reasoning speed of the model. The conditional
probability PN AR(Y |X) can be rewritten as:

PN AR(Y |X) =

L
(cid:89)

i=1

P (yi|X)

(4)

Both the AR decoder and the NAR decoder can apply the
transformer as the basic structure. There are two signiﬁcant dif-
ferences between these two kinds of decoders. On the one hand,
the AR transformer needs to apply the masking operation to the
previous output to model the linguistic dependencies, but the
NAR transformer needs to model the bidirectional dependen-
cies between the output tokens without any masking operations.
On the other hand, the AR decoder adopts all the tokens of vo-
cabulary and some special tokens as the modeling units, while
the NAR decoder only requires one <MASK> token. Their mod-
eling units are not coincident.

Inspired by the dual-mode ASR [16], which models the
streaming ASR and non-streaming ASR by sharing an encoder,
we propose a novel method that uniﬁes the AR and NAR de-
coder into one dual-mode transformer decoder (DMTD). We
assume that the AR decoder will accelerate the training and

RescoringPre-SelectionAcousticEncoderConv Front End（optional）N-Best Hypothesises[MASK]  [MASK]  [MASK]  [MASK]  [MASK]Best Predicted SequenceAcoustic Feature SequenceParameters SharingAutoregressive DecoderNon-Autoregressive Decoder∙∙∙∙∙∙∙∙∙∙∙∙12∙∙∙T‐1T<EOS>𝑉(cid:2869)𝑉(cid:3012)(cid:2879)(cid:2869)𝑉(cid:3012)convergence of the NAR decoder. The linguistic dependencies
learned by the AR decoder might be able to provide some prior
knowledge for the NAR decoder, which will make the train-
ing of the NAR decoder more efﬁcient than guessing based on
the empty sequence. The self-attention of DMTD applies to
the masking-optional operation. During training, the encoder
forwards once, and the decoder forwards twice, once in autore-
gressive mode and once in non-autoregressive mode. During
the AR forward, the DMTD will adopt the truth token sequences
that begin with the begin-of-sentence token <BOS> as the input,
and then mask future information of self-attention and calculate
the cross-entropy (CE) loss LAR. For the NAR forward, the
decoder will utilize a sequence ﬁlled with <MASK> as the input
and calculate the CE loss LN AR. The ﬁnal loss is equal to the
weighted sum of LAR and LN AR.

L = (1 − α)LN AR + αLAR

(5)

where α dictates the weight of AR loss. The model is optimized
by these two losses jointly.

2.3. The Two Step Inference

Most of the non-autoregressive models just select the token
which has the highest probability at each position and con-
catenates them from left to right as the ﬁnal predicted result
[10, 14, 15]. This inference method can be regarded as a greedy
search. As shown in Fig.1(b), we consider the conditional prob-
ability matrix generated by the decoder as a graph, which con-
tains numerous possible hypothesize. Each hypothesis starts
with any blank circle in the ﬁrst column and ends with a red
circle (end-of-sentence token <EOS>). We could select a bet-
ter hypothesis than the one predicted by greedy search by de-
pending on the external language model or the second attention
decoder as [17, 19, 18]. Now that the dual-mode transformer
decoder is able to be trained in the AR and NAR fashion simul-
taneously, the decoder can perform dual-mode inference natu-
rally.

The inference process can be divided into two steps, pre-
selection and rescoring. During the ﬁrst pre-selection, the dual-
mode decoder will generate the conditional probability matrix
from a full-mask sequence in a NAR way and then select the
N -best hypothesizes.
In order to avoid that the model tends
to choose shorter sentences, we utilize the length-normalized
scores. The selection of N -best hypothesizes from the proba-
bility graph contains only simple addition operations, and does
not take much time and computing resources. During the sec-
ond step, the dual-mode decoder will insert begin-of-sentence
token <BOS> into the head of the N -best hypothesizes. Then it
utilizes the processed candidates as the input and predict them
in an AR fashion. Due to the transformer can carry out efﬁ-
cient computing in parallel, the entire inference process can be
ﬁnished in two step.

3. Experiments and Results

3.1. Dataset
In this work, all experiments are conducted on a public Man-
darin speech corpus AISHELL-11.The training set contains
about 150 hours of speech (120,098 utterances) recorded by 340
speakers. The development set contains about 20 hours (14,326
utterances) recorded by 40 speakers. And about 10 hours (7,176
utterances / 36109 seconds) of speech is used as the test set. The
speakers of different sets are not overlapped.

1https://openslr.org/33/

3.2. Experimental Setup
For all experiments, we use 80-dimensional FBANK features
computed on a 25ms window with a 10ms shift. We choose
4234 characters (including a padding symbol <PAD>, an un-
known token <UNK>, a begin-of-sentence token <BOS>) and
an end-of-sentence token <EOS> as modeling units.

Our model consists of 12 encoder blocks and 6 decoder
blocks. There are 4 heads in multi-head attention. The 2D con-
volution front end utilizes two-layer time-axis CNN with ReLU
activation, stride size 2, channels 384, and kernel size 3. Both
the output size of the multi-head attention and the feed-forward
layers are 384. The hidden size of the feed-forward layers is
768. We adopt an Adam optimizer with warmup steps 12000
and the learning rate scheduler reported in [22]. After 100
epochs, we average the parameters saved in the last 20 epochs.
We also use the time mask and frequency mask method pro-
posed in [24] instead of speed perturbation.

We use the character error rate (CER) to evaluate the perfor-
mance of different models. For evaluating the inference speed
of different models, we decode utterances one by one to com-
pute real-time factor (RTF) on the test set. The RTF is the time
taken to decode one second of speech. All experiments are con-
ducted on a GeForce GTX TITAN X 12G GPU.

3.3. Results

3.3.1. Comparison of The Model With Different AR Weights α
This section compares the models with the different AR weights
α. When the weight α is equal to 0, the NAR model is equiva-
lent to the one that adopts an empty sequence as the input with-
out any prior knowledge. If α is equal to 1, the model will be
completely transformed into an AR model. When the weight is
between 0 and 1, the model can perform the two-step inference.
For all one-step inference in this section (marked as OneStep),
the NAR models (α ∈ [0, 1)) apply greedy search and the
AR (α = 1.0) models apply the beam search with width 10.
The two-step inference of this section (marked as TwoStep)
rescores the 10-best candidate sequences. As shown in Table 1,
it’s obvious that the hybrid AR and NAR training (α ∈ (0, 1))
can improve the performance of the NAR model. We guess that
the NAR decoder can get some prior knowledge from the AR
decoder by sharing the parameters, which reduces the difﬁculty
of training the NAR model from scratch. In the case of one-step
inference, there is still a great performance gap between the AR
model(α = 1.0) and the NAR model (α ∈ [0, 1)). However,
the introduced two-step inference method can improve the per-
formance of the NAR model greatly and achieve comparable
results with the AR model.

Table 1: Comparison of the model with different AR weights α
(CER %).

Weight α

Dev

Test

OneStep
TwoStep
OneStep
TwoStep

0.0
6.5
-
7.2
-

0.3
5.9
5.6
6.5
6.2

0.5
5.8
5.5
6.5
6.1

0.7
5.8
5.4
6.4
6.0

0.9
6.5
5.6
6.5
6.2

1.0
5.3
-
6.0
-

3.3.2. The inﬂuence of the N -best Sequences on The Model
Performance

This section pays attention to the inference of the N -best se-
quence on the NAR model performance. We select the different
number of candidate sequences. Under the condition that N
is equal to 1, the two-step inference makes no sense. There-
fore, we replace it with the result of the greedy search. The

results in Table 2 indicate that the more candidate sequences,
the better performance it archives, which is consistent with the
results previously reported [19]. When the number of candidate
sequences is greater than 10, the performance tends to be sta-
ble. However, with the increase of N , the model requires more
computing resources, which leads to an increase in latency and
real-time-factor (RTF). Taken together, we will select the 10-
best candidates for the second-step rescoring by balancing the
model performance and the inference speed.

Table 2: Comparison of the inﬂuence of the N -best sequences
on the model performance (CER %).

N
Dev
Test
RTF

1
5.8
6.4
0.0054

5
5.4
6.0
0.0167

10
5.4
5.9
0.0173

20
5.4
5.9
0.0181

50
5.3
5.9
0.0220

3.3.3. Comparison with Other Non-Autoregressive Models

To further study the upper
limit of model performance,
we readjust the parameter conﬁguration of the model and
named three models of different sizes, named TSNAT-Small,
TSNAT-Middle and TSNAT-Big. The three models have the
same depth (12 encoder blocks and 6 decoder blocks) and are
trained under the same conditions. Their differences focus on
the dimensions of the model (dmodel) and the hidden size of the
feed-forward network (df f ), as shown in Table 3.

Table 3: The Parameter Conﬁguration of The Model.

Mode
dmodel
df f
#Params

TSNAT-Small
384
384
34M

TSNAT-Middle
512
512
59M

TSNAT-Big
512
512
87M

Table 4: Comparison with other non-autoregressive models
(CER %). All models in this table use SpecAugment to improve
the performance.

LM Dev
Model
6.2
w/o
A-FMLM(K=1) [10]
6.1
w/o
Insertion-NAT [25]
LASO-big [15] ♦
5.8
w/o
CASS-NAT [26] ♦
5.3
w
CTC-enhanced NAR [13] ♦ w/o
5.3
6.9
w/o
ST-NAT [14]
6.4
w
ST-NAT [14]
5.8
w/o
TSNAT-Small (34M)
5.4
w/o
5.4
w/o
5.2
w/o
5.3
w/o
5.1
w/o

TSNAT-Middle (59M)

+ Two Step Inference

+ Two Step Inference

+ Two Step Inference

TSNAT-Big (87M)

Test
6.7
6.7
6.4
5.8
5.9
7.7
7.0
6.4
5.9
6.0
5.7
6.0
5.6

RTF
-
-
-
-
-
0.0056
0.0292
0.0054
0.0173
0.0063
0.0176
0.0077
0.0185

♦ These models additionally use speed-perturb to augment the

speech data.
Compared with the other non-autoregressive models, we
proposed TSNAT model can achieve quite competitive perfor-
mance. To our best knowledge, the TSNAT-Big with two-step
inference achieves the state-of-the-art (SOTA) performance
without depending on any external language models. The re-
sults also show that the two-step inference is faster than ST-NAT
with a neural language model because the two-step inference of
the dual-mode transformer decoder can be implemented in par-
allel without iteration step by step.

(a) Self-Attention of NAR Mode

(b) Self-Attention of AR Mode

(c) EncDec-Attention of NAR Mode

(d) EncDec-Attention of AR Mode

Figure 2: The Visualization of Attention Weights in The
Dual-Mode Decoder. We extract the self-attention weights
and encoder-decoder attention weights from the last block
of the dual-mode transformer decoder. The pictures come
from the test set, and its corresponding sentence ID is
’BAC009S0764W0121’.

3.3.4. The Analysis of Attention Weights in The Dual-Mode De-
coder

We want to ﬁnd out the difference between the AR and NAR
modes of the dual-mode transformer decoder. Therefore, we
extract the attention weights of these two modes from the last
decoder block. It’s clear that the self-attention mechanism of
the NAR model focuses on the whole input sequence, while
the self-attention mechanism of the AR mode pays attention to
the previous sequence. This is consistent with their own char-
acteristics. Besides, the encoder-decoder attention weights of
the NAR mode are more dispersed than the one under the AR
mode, which also shows the autoregressive model has stronger
modeling ability.

4. Conclusions

Compared with the autoregressive model for speech recogni-
tion, the non-autoregressive model can get rid of the temporal
dependency and predict the entire tokens in at least one step.
However, the non-autoregressive model still faces two prob-
lems. On the one hand, there is still a great performance gap
between the advanced autoregressive (AR) model and the non-
autoregressive (NAR) transformer model. On the other hand,
it’s difﬁcult for most of the non-autoregressive models to train
and converge. In this paper, we try to address these problems
from two aspects. Firstly, we propose a parameters-sharing
training method to improve the performance of the NAR model
by learning some valuable knowledge from the AR model. Then
we make full use of the dual-mode decoder by introducing the
two-step inference method. We conduct our experiments on a
public Chinese mandarin dataset ASIEHLL-1. The results show
that our proposed model can achieve comparable performance

[18] K. Hu, T. N. Sainath, R. Pang, and R. Prabhavalkar, “Deliberation
model based two-pass end-to-end speech recognition,” in ICASSP
2020-2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP).
IEEE, 2020, pp. 7799–7803.

[19] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, and Z. Wen, “One
in a hundred: Select the best predicted sequence from numer-
ous candidates for streaming speech recognition,” arXiv preprint
arXiv:2010.14791, 2020.

[20] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y.
Liu, “Fastspeech: Fast, robust and controllable text to speech,”
in Advances in Neural Information Processing Systems, 2019, pp.
3171–3180.

[21] Y. zhao, J. Li, X. Wang, and Y. Li, “The speechtransformer
for large-scale mandarin chinese speech recognition,” in ICASSP
2019 - 2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2019, pp. 7095–7099.

[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
in Advances in neural information processing systems, 2017, pp.
5998–6008.

[23] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language
modeling with gated convolutional networks,” in Proceedings of
the 34th International Conference on Machine Learning-Volume
70.

JMLR. org, 2017, pp. 933–941.

[24] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.
Cubuk, and Q. V. Le, “Specaugment: A simple data augmen-
tation method for automatic speech recognition,” arXiv preprint
arXiv:1904.08779, 2019.

[25] Y. Fujita, S. Watanabe, M. Omachi, and X. Chan, “Insertion-based
modeling for end-to-end automatic speech recognition,” arXiv
preprint arXiv:2005.13211, 2020.

[26] R. Fan, W. Chu, P. Chang, and J. Xiao, “Cass-nat: Ctc
alignment-based single step non-autoregressive transformer for
speech recognition,” arXiv preprint arXiv:2010.14725, 2020.

with the AR model and outperform other typical NAR models.

5. References
[1] C. J. K, B. Dzmitry, S. Dmitriy, C. Kyunghyun, and B. Yoshua,
“Attention-based models for speech recognition,” in Advances in
neural information processing systems, 2015, pp. 577–585.

[2] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend
and spell: A neural network for large vocabulary conversational
speech recognition,” in 2016 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2016,
pp. 4960–4964.

[3] K. Suyoun, H. Takaaki, and W. Shinji, “Joint ctc-attention based
end-to-end speech recognition using multi-task learning,” in 2017
IEEE international conference on acoustics, speech and signal
processing (ICASSP).

IEEE, 2017, pp. 4835–4839.

[4] D. Linhao, X. Shuang, and X. Bo, “Speech-transformer: a no-
recurrence sequence-to-sequence model for speech recognition,”
in 2018 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP).

IEEE, 2018, pp. 5884–5888.

[5] A. Graves, “Sequence transduction with recurrent neural net-

works,” arXiv preprint arXiv:1211.3711, 2012.

[6] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez,
D. Zhao, D. Rybach, A. Kannan, Y. Wu, R. Pang et al., “Stream-
ing end-to-end speech recognition for mobile devices,” in ICASSP
2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP).
IEEE, 2019, pp. 6381–6385.

[7] Z. Tian, J. Yi, J. Tao, Y. Bai, and Z. Wen, “Self-Attention Trans-
ducers for End-to-End Speech Recognition,” in Proc. Interspeech
2019, 2019, pp. 4395–4399.

[8] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and
S. Kumar, “Transformer transducer: A streamable speech recog-
nition model with transformer encoders and rnn-t loss,” in ICASSP
2020-2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP).
IEEE, 2020, pp. 7829–7833.

[9] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y. Wang, D. Le,
M. Jain, K. Schubert, C. Fuegen, and M. L. Seltzer, “Transformer-
transducer: End-to-end speech recognition with self-attention,”
arXiv preprint arXiv:1910.12977, 2019.

[10] N. Chen, S. Watanabe, J. Villalba, and N. Dehak, “Non-
autoregressive transformer automatic speech recognition,” arXiv
preprint arXiv:1911.04908, 2019.

[11] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,
“Mask ctc: Non-autoregressive end-to-end asr with ctc and mask
predict,” arXiv preprint arXiv:2005.08700, 2020.

[12] Y. Higuchi, H.

and
T. Kobayashi, “Improved mask-ctc for non-autoregressive end-to-
end asr,” arXiv preprint arXiv:2010.13270, 2020.

Inaguma, S. Watanabe, T. Ogawa,

[13] X. Song, Z. Wu, Y. Huang, C. Weng, D. Su, and H. Meng, “Non-
autoregressive transformer asr with ctc-enhanced decoder input,”
arXiv preprint arXiv:2010.15025, 2020.

[14] Z. Tian,

J. Yi,

J. Tao, Y. Bai, S. Zhang, and Z. Wen,
“Spike-Triggered Non-Autoregressive Transformer for End-to-
End Speech Recognition,” in Proc. Interspeech 2020, 2020,
pp. 5026–5030. [Online]. Available: http://dx.doi.org/10.21437/
Interspeech.2020-2086

[15] Y. Bai, J. Yi, J. Tao, Z. Tian, Z. Wen, and S. Zhang, “Listen At-
tentively, and Spell Once: Whole Sentence Generation via a Non-
Autoregressive Architecture for Low-Latency Speech Recogni-
tion,” in Proc. Interspeech 2020, 2020, pp. 3381–3385. [Online].
Available: http://dx.doi.org/10.21437/Interspeech.2020-1600

[16] J. Yu, W. Han, A. Gulati, C.-C. Chiu, B. Li, T. N. Sainath, Y. Wu,
and R. Pang, “Dual-mode asr: Unify and improve streaming asr
with full-context modeling,” Proceedings of ICLR, 2021.

[17] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,
M. Visontai, Q. Liang, T. Strohman, Y. Wu et al., “Two-pass end-
to-end speech recognition,” arXiv preprint arXiv:1908.10992,
2019.

