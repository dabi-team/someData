T-statistic for Autoregressive process

,
Eric Benhamou ∗

,

†

‡

Abstract

In this paper, we discuss the distribution of the t-statistic under the assumption of nor-
mal autoregressive distribution for the underlying discrete time process. This result
generalizes the classical result of the traditional t-distribution where the underlying
discrete time process follows an uncorrelated normal distribution. However, for AR(1),
the underlying process is correlated. All traditional results break down and the re-
sulting t-statistic is a new distribution that converges asymptotically to a normal. We
give an explicit formula for this new distribution obtained as the ratio of two depen-
dent distribution (a normal and the distribution of the norm of another independent
normal distribution). We also provide a modiﬁed statistic that is follows a non central
t-distribution. Its derivation comes from ﬁnding an orthogonal basis for the the initial
circulant Toeplitz covariance matrix. Our ﬁndings are consistent with the asymptotic
distribution for the t-statistic derived for the asympotic case of large number of obser-
vations or zero correlation. This exact ﬁnding of this distribution has applications in
multiple ﬁelds and in particular provides a way to derive the exact distribution of the
Sharpe ratio under normal AR(1) assumptions.

AMS 1991 subject classiﬁcation: 62E10, 62E15

Keywords: t-Student, Auto regressive process, Toeplitz matrix, circulant matrix, non cen-
tered Student distribution

8
1
0
2

p
e
S
1
1

]
T
S
.
h
t
a
m

[

1
v
8
1
0
4
0
.
9
0
8
1
:
v
i
X
r
a

∗A.I. SQUARE CONNECT, 35 Boulevard d’Inkermann 92200 Neuilly sur Seine, France
†LAMSADE, Universit Paris Dauphine, Place du Marchal de Lattre de Tassigny,75016 Paris, France
‡E-mail: eric.benhamou@aisquareconnect.com, eric.benhamou@dauphine.eu

 
 
 
 
 
 
1.

Introduction

Let X1, . . . , Xn be a random sample from a cumulative distribution function (cdf) F ()

with a constant mean µ and let deﬁne the following statistics referred to as the t-statistic

Tn = T (Xn) =

√n( ¯Xn −
sn

µ)

(1)

where ¯Xn is the empirical mean, s2
Xn the regular full history of the random sample deﬁned by:

n the empirical Bessel corrected empirical variance, and

¯Xn =

1
n

n

i=1
X

Xi,

s2
n =

1

−

1

n

n

i=1
X

(Xi −

¯Xn)2, Xn = (X1, . . . , Xn)T

(2)

It is well known that if the sample comes from a normal distribution, N(0, σ), Tn has the
Student t-distribution with n
1 degrees of freedom. The proof is quite simple (we provide
a few in the appendix section in A.1). If the variables (Xi)i=1..n have a mean µ non equal
to zero, the distribution is referred to as a non-central t-distribution with non centrality
parameter given by

−

η = √n

µ
σ

(3)

Extension to weaker condition for the t-statistics has been widely studied.

Mauldon Mauldon (1956) raised the question for which pdfs the t-statistic as deﬁned by

1 is t-distributed with d
1 degrees of freedom. Indeed, this characterization problem can
be generalized to the one of ﬁnding all the pdfs for which a certain statistic possesses the

−

property which is a characteristic for these pdfs. Kagan, Linnik, and Rao (1973), Bondesson
(1974) and Bondesson (1983) to cite a few tackled Mauldons problem. Bondesson (1983)
proved the necessary and suﬃcient condition for a t-statistic to have Students t-distribution

It is not necessary that X1, ..., Xn is an independent sample.

with d
1 degrees of freedom for all sample sizes is the normality of the underlying dis-
−
tribution.
Indeed consider
X1, ..., Xn as a random vector Xn = (X1, ..., Xn)T each component of which having the
same marginal distribution function, F (). Efron (1969) has pointed out that the weaker
condition of symmetry can replace the normality assumption. Later, Fang, Yang, and Kotz
(2001) showed that if the vector Xn has a spherical distribution, then the t-statistic has a
t-distribution. A natural question that gives birth to this paper was to check if the Student
resulting distribution is conserved in the case of an underlying process (Xi)i=1,... following an
AR(1) process. This question and its answer has more implication than a simple theoretical
Indeed, if by any chance, one may want to test the statistical signiﬁcance of a
problem.

coeﬃcient in a regression, one may do a t-test and rely upon the fact that the resulting

1

distribution is a Student one. If by any chance, the observations are not independent but
suﬀer from auto-correlation, the building blocks supporting the test break down. Surpris-
ingly, as this problem is not easy, there has been few research on this problem. Even if

this is related to the Dickey Fuller statistic (whose distribution is not closed form and needs
to be computed by Monte Carlo simulation), this is not the same statistics. Mikusheva

(2015) applied an Edgeworth expansion precisely to the Dickey Fuller statistics but not to
the original t-statistic. The neighboring Dickey Fuller statistic has the great advantage to
be equal to the ratio of two known continuous time stochastic process making the problem

easier. In the sequel, we will ﬁrst review the problem, comment on the particular case of zero
correlation and the resulting consequence of the t-statistic. We will emphasize the diﬀerence

and challenge when suddenly, the underlying observations are not any more independent.
We will study the numerator and denominator of the t-statistic and derive their underlying
distribution. We will in particular prove that it is only in the case of normal noise in the

underlying AR(1) process, that the numerator and denominator are independent. We will
then provide a few approximation for this statistic and conclude.

2. AR(1) process

The assumptions that the underlying process (or observations) (Xi)i=1,...;n follows an

AR(1) writes :

(

Xt = µ + ǫt
ǫt = ρǫt

1 + σvt

−

1;

2;

t

t

≥

≥

(4)

where vt is an independent white noise processes (i.i.d. variables with zero mean and unit
constant variance). To assume a stationary process, we impose

It is easy to check that equation 4 is equivalent to

ρ

| ≤

|

1

Xt = µ + ρ(Xt

−

µ) + σvt

1 −

2;

t

≥

(5)

(6)

We can also easily check that the variance and covariance of the returns are given by

V (Xt)
Cov(Xt, Xu) = σ2ρ|t−u|
ρ2

= σ2
ρ2
1
−

1

−

for t

1

≥
for t, u

1

≥

(7)

Both expressions in 7 are independent of time t and the covariance only depends on

2

t

|

−

u
|

implying that Xt is a stationary process.

2.1. Case of Normal errors

If in addition, we assume that vt are distributed according to a normal distribution,
we can fully characterize the distribution of X and rewrite our model in reduced matrix
formulations as follows:

X = 



= µ

· 1n + σ

·

ǫ = µ 



+ σ 

X1
...
Xn








ρ|i−1|
ρ2
1

ǫ1
...
ǫn







(8)





1
...
1







· 1n, σ2Ω).

where ǫ

N

0, Ω =

, hence, X

N (µ

∼

ij
(cid:19)
The Ω matrix is a Toeplitz circulant matrix deﬁned as

(cid:18)

(cid:17)

(cid:16)

−

∼

1
ρ
...
ρn
ρn

−

−

ρ
1
...
2 ρn
1 ρn

3

2

−

−

. . . ρn
. . . ρn
...
. . .
. . . 1

. . . ρ

1

2

−

−

−

−

2 ρn
3 ρn
...
ρ

1

Ω =

1

ρ2

1

−












Its Chlolesky decomposition is given by

= M T M

(9)














1

ρ2

1

−

1
ρ
...
ρn
ρn

0

...
p
2 ρn
1 ρn

M =

1

ρ2









p
It is worth splitting M into In and another matrix as follows:

1
1
p
p

ρ2
ρ2

−
−

p

−

1

−

−

−

−

2

3

ρ2
ρ2

−
1

−

. . . 0
. . . 0
...
. . .
. . .
. . . ρ
p

√1
1 + 1
−
√1

−
ρ2

ρ2

−

ρ2

ρ
√1
...

−

ρn−2
√1
−
ρn−1
√1

−

ρ2

ρ2

M =















0

1
...
ρn

3

−

. . . 0 0

. . . 0 0
...
...
. . .
. . . 1 0

ρn

−

2

. . . ρ 1





























= In +

In +

3

|

ρ2

−
ρ2

−

1

√1
−
√1
ρ
√1
...

−

ρ2

ρn−2
√1
−
ρn−1
√1

−

ρ2

ρ2

0
0
...
0

p

0

0
...
ρn












ρ2

1

−

. . . 0 0

. . . 0 0
...
...
. . .
. . . 0 0

3

−

ρn

−

2

. . . ρ 0

N

{z

(10)

(11)















}

The inverse of Ω is given by

1

ρ
−
ρ 1 + ρ2
...
0

−
...
0

0

0

. . . 0
. . . 0
...
. . .
. . . 1 + ρ2
. . .

ρ

−

A = Ω−

1 =












Its Cholesky decomposition L is given by

= LT L

(12)

0
0
...

ρ

−
1












0












(13)

L =

−

1

ρ
p
−
...
0
0












ρ2 0 . . . 0
1 . . . 0
...
...
. . .
0 . . . 1
0 . . .

0
...
0
ρ 1

−

Notice in the various matrix the dissymmetry between the ﬁrst term and the rest. This
ρ2, while all other diagonal
shows up for instance in the ﬁrst diagonal term of L which is
terms are equal to 1. Similarly, in the matrix N, we can notice that the ﬁrst column is quite

−

1

diﬀerent from the other ones as it is a fraction over

1

−

p

2.2. T-statistics issue

p
ρ2.

The T-statistic given by equation 1 is not easy to compute. For the numerator, we
µ follows a normal distribution. The proof is immediate as ¯Xn is a linear
have that ¯Xn −
combination of the Gaussian vector generated by the AR(1) process. We have ¯Xn = 1
X.
N(µ, σ2
It follows that ¯Xn ∼
· 1n) (for a quick proof of the fact that any linear
combination of a Gaussian vector is normal, see B.1).
In section 3, we will come back
on the exact computation of the characteristics of the distribution of the numerator and

n 1n ·

T
n ·

n2 1

Ω

denominator as this will be useful in the rest of the paper.

As for the denominator, for a non null correlation ρ, the distribution of s2

n is not a known

distribution.

The distributions of the variables
1
(δi −
with σYi = (δi −
Hence the square of these normal variables Zi = Y 2
i

n 1n)T

Ω

·

·

Yi = Xi −
n 1n). where δi = (0, 0, . . . , 1, . . . , 0, 0)T
1
(cid:0)

i=2,...,n are normal given by Yi ∼
(cid:1)

¯Xn

However, we cannot obtain a closed form for the distribution s2

|

1 at ith position
is the sum of Gamma distributions.
n as the variance of the

{z

}

N(0, σYi)

4

−

diﬀerent terms are diﬀerent and the terms are neither independent. If the correlation is null,
and only in this speciﬁc case, we can apply the Cochrans Theorem to prove that s2
n follows
a Chi square distribution with n
1 degree of freedom. However, in the general case, we

need to rely on approximation that will be presented in the rest of the paper.

Another interesting result is to use the Cholesky decomposition of the inverse of the

covariance matrix of our process to infer a modiﬁed t-statistic that has now independent
terms and is deﬁned as follows

Let us take the modiﬁed process deﬁned by

U = LX

(14)

The variables U is distributed according to a normal U
the modiﬁed T-statistic ˜Tn on U as follows:

∼

N(µL1, Idn). We can compute

where

¯Un =

1
n

˜Tn =

√n( ¯Un −
˜sn

µ)

n

Ui,

˜s2
n =

i=1
X

1

−

1

n

n

i=1
X

(15)

(16)

¯Un)2

(Ui −

In this speciﬁc case, the distribution of ˜Tn is a Student distribution of degree n

1. We
will now work on the numerator and denominator of the T-statistic in the speciﬁc case of
AR(1) with a non null correlation ρ.

−

3. Expectation and variance of numerator and denom-

inator

The numerator of the T-statistic writes

√n( ¯Xn −

µ) =

1
√n

n

i=1
X

(Xi −

µ),

Its expectation is null as each term is of zero expectation. Its variance is given by

Lemma 3.1.

Var(√n( ¯Xn −

µ)) =

=

1 + ρ
1

ρ −

σ2

1

ρ2
−
σ2

(cid:20)

ρ)2

(cid:20)

(1

−

5

−
1

−

n(1

−

2ρ(1
n(1
2ρ(1

ρn)
−
ρ)2
−
ρn)
−
ρ)(1 + ρ)

(cid:21)

(cid:21)

(17)

(18)

(19)

ρ2.

−

(20)

Proof. : See B.2

The proposition 3.1 is interesting as it states that the sample mean variance converges

to

(1

σ2

−

ρ)2

for large n. It is useful to keep the two forms of the variance. The ﬁrst one

(equation (18)) is useful in following computation as it shares the denominator term 1

The second form (equation 19) gives the asymptotic form.

The denominator writes:

n

1

n

1

(Xi −

¯Xn)2,

sn =

v
u
u
t

i=1
X
In the following, we denote by Yi = Xi −

−

µ the zero mean variable and work with these
variables to make computation easier. We also write Y ⊥i
the variable orthogonal to Yi whose
variance (we sometimes refer to it as its squared norm to make notation easier) is equal to
the one of Yi :
iYi +
Yj = ρj

. To see the impact of correlation, we can write for any j > i,
.

k

−

2

As studying this denominator is not easy because of the presence of the square root, it

Yik
1
−

2 =
ρ2(j
(cid:13)
−
(cid:13)

Y ⊥i
i)Y ⊥i
(cid:13)
(cid:13)

p

is easier to investigate the properties of its squared given by

s2
n =

n

i=1(Yi −
n
1
−

P

¯Yn)2

=

n
i=1 Y 2
n

i −
1
−

P

n ¯Y 2
n

(21)

We have that the mean of ¯Yn is zero while proposition 3.1 gives its variance :

σ2

1 + ρ
1

2ρ(1
n(1

ρn)
ρ)2

Var( ¯Yn) =

n(1

−
−
Lemma 3.2. The covariance between ¯Yn and each stochastic variable Yj is useful and given
by

ρ −

n(1

n(1

ρ2)

ρ)2

−

−

−

−

−

(cid:20)

(cid:21)

(cid:20)

(cid:21)

(22)

=

1

2ρ(1

ρn)
−
ρ)(1 + ρ)

σ2

Cov( ¯Yn, Yj) =

1 + ρ

σ2

−

n(1

ρ2)

(cid:20)

ρj

j

−

ρn+1
−
−
ρ
1

−

(cid:21)

In addition, we have a few remarkable identities

1 + ρ
1

−

ρ −

2ρ(1
n(1

ρn)
ρ)2

−
−

(cid:21)

n

j=1
X

Cov( ¯Yn, Yj) =

σ2

ρ2)

(1

−

(cid:20)

6

(23)

(24)

n

Cov( ¯Yn, Yj)

2

=

j=1
X

(cid:0)

(cid:1)

Proof. : See B.3

σ4

(1 + ρ)2 + 2ρn+1

ρ2)2

(1

−

(cid:20)

ρ)2

(1

−

1
n −

4(1 + ρ)2ρ(1

ρn)
−
ρ)2(1

ρ2n)

2ρ2(1
ρ2)

−

1
n2

(25)
(cid:21)

−
−

(1

−

We can now compute easily the expectation and variance of the denominators as follows

Proposition 1. The expectation of s2

n is given by:

Es2

n =

σ2

ρ2

1

−

1
(cid:18)

2ρ
ρ)(n

+

1)

−

−

(1

−

2ρ(1

ρn)

−
1)(1

n(n

−

ρ)2

(cid:19)

−

Proof. : See B.4

Proposition 2. The second moment of s2

n is given by:

E[s4

n] =

σ4

ρ2)2

(n

(1

−

1

−

1)2

n2
(cid:20)

−

1 + ρ

nA1 + A2 +

(cid:18)

1
n

A3 +

1
n2 A4

(cid:19)(cid:21)

with

1

4
A1 = −
ρ2
−
2 (3 + 9ρ + 11ρ2 + 3ρ3 + 6ρn + 12ρn+1 + 6ρn+2
(1
−
8ρn+1)

A2 = −

ρ2)2

4(1

2ρ2n+2)

−

3ρ + 4ρ2
r)3(1 + r)

−

A3 =

−

ρn)(1
−
(1
−
ρn)2
−
ρ)4

12ρ(1
(1

−

A4 =

Proof. : See B.5

(26)

(27)

(28)

(29)

(30)

(31)

Combining the two results leads to

Proposition 3. The variance of s2

n is given by:

Var[s4

n] =

σ4

ρ2)2

(n

(1

−

1

−

2 +

1)

(cid:20)

ρ

−

1

n

nB1 + B2 +

(cid:18)

1
n

B3 +

1
n2 B4

(cid:19)(cid:21)

(32)

7

with

2
B1 = −
1 + ρ
2

B2 =

4ρ2

1

ρ −
−
2 (12ρn+1 + 6ρn+2

(1

−

−

ρ)2 −

−

−

B3 =

(1

−

ρn)(13

−

ρn)
ρ)2

2 (1
(1

−
−

(1

ρ2)2

−
ρn

−
−
r)3(1 + r)

(1
−
ρn)2

2ρ2n+2 + 6ρn + 3ρ3 + 11ρ2 + 9ρ + 3)

4ρ + 15ρ2

32ρn+1 + ρn+2)

4(1

B4 = −

Proof. : See B.6

3ρ)(1
ρ)4

−
(1

−

−

(33)

(34)

(35)

(36)

(37)

It is worth noting that a direct approach as explained in Benhamou (2018) could also give
the results for the ﬁrst, second moments and variance for the numerator and denominator.

4. Resulting distribution

The previous section shows that under the AR(1) assumptions, the t-statistic is no longer
a Student distribution but the ratio of a normal whose ﬁrst and second moments have been

given above and the norm of a Gaussian whose moments have also been provided. To go
further, one need to rely on numerical integration. This is the subject of further research.

5. Conclusion

In this paper, we have given the explicit ﬁrst, second moment and variance of the numer-
ator of the t statistic under the assumption of AR(1) underlying process. We have seen that

these moments are very sensitive to the correlation ρ assumptions and that the distribution
is far from a Student distribution.

8

Appendix A. Various Proofs for the Student density

A.1. Deriving the t-student density

Let us ﬁrst remark that in the T-statistic, the √n factor cancels out to show the degree

of freedom √n

1 as follows:

−

Tn =

¯X
µ
−
sn/√n

¯X

=

µ

1
sn
σ

−
σ
√n

= U

1
sn
σ

= √n

1

−

U

¯X)2

P(Xi−
σ2

= √n

1

U
V

−

(38)

q

In the above expression, it is well know that if X
¯X)2

∼

−

µ)
σ/√n ∼

variable U = ( ¯X
independent. Hence, we need to prove that the distribution of T = U/
distribution with U

1) as well as U and V are
q
V /k is a Student
k mutually independent, and k is the degree of

N(0, 1) and V =

N(0, 1), and V

P(Xi−
σ2

χ2

∼

−

N(µ, σ), then the renormalized
χ2
(n

p

∼

∼

freedom of the chi squared distribution.

The core of the proof relies on two steps that can be proved by various means.

Step 1 is to prove that the distribution of T is given by

fT (t) =

1
k+1
2 √πk Z

0

∞

e−

Γ( k

2 )2

2

w( t

2k + 1

2 )w

k−1
2 dw

(39)

Step 2 is to compute explicitly the integral in equation 39
Step 1 can be done by transformation theory using the Jacobian of the inverse transformation

or the property of the ratio distribution. Step 2 can be done by Gamma function, Gamma
distribution properties, Mellin transform or Laplace transform.

A.2. Proving step 1

A.2.1. Using transformation theory

The joint density of U and V is:

fU,V (u, v) =

1
(2π)1/2 e−

u2/2

1
2 ) 2k/2

Γ( k

v(k/2)
−

1 e−

v/2

(40)

pdf N (0,1)

pdf χ2
k

with the distribution support given by

|

Making the transformation t = u

{z
−∞

}
|
< u <

√v/k

{z
and 0 < v <

}

.

∞
and w = v, we can compute the inverse: u =

∞

9

1/2 and v = w. The Jacobian 1 is given by

t

w
k

(cid:0)

(cid:1)

J(t, w) =

1/2

w
k

0
(cid:1)

t
2(kw)1/2
1

(cid:12)
(cid:0)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

whose value is (w/k)1/2. The marginal pdf is therefore given by:

fT (t) =

=

=

which proves the result

∞

∞

0
Z

0
Z

Γ( k

2 )2

w
k

fU,V

t (

)1/2, w

J(t, w) dw

(cid:18)
1
(2π)1/2 e−
1
k+1
2 √πk Z

(t2 w

k )/2

∞

e−

0

(cid:19)

1
2 ) 2k/2
2k + 1
2 )w

2

Γ( k

w( t

k−1
2 dw

w(k/2)
−

1 e−

w/2(w/k)1/2 dw

(41)

(42)

(43)

(44)

A.2.2. Using ratio distribution

The square-root of V , √V

freedom, which has density

≡

ˆV is distributed as a chi-distribution with k degrees of

f ˆV (ˆv) =

k
2

21
−
k
Γ
2

ˆvk

1 exp

−

ˆv2
2

o

−

n

Deﬁne X

ˆV
√k

≡

. Then by change-of-variable, we can compute the density of X:

(cid:0)

(cid:1)

fX(x) = f ˆV (√kx)

∂ ˆV
∂X

=

k
2

21
−
k
Γ
2

k

k

(cid:12)
(cid:12)
2 xk
(cid:12)

(cid:12)
(cid:12)
1 exp
(cid:12)

−

k x2
2

o

−

n

(45)

(46)

(47)

The student’s t random variable deﬁned as T = Z

X has a distribution given by the ratio

(cid:0)

(cid:1)

distribution:

fT (t) =

∞

Z

−∞

x
|

|

fU (xt)fX (x)dx

(48)

, 0] since X is a non-negative random
We can notice that fX(x) = 0 over the interval [
variable. We are therefore entitled to eliminate the absolute value. This means that the

−∞

1determinant of the Jacobian matrix of the transformation

10

integral reduces to

fT (t) =

=

=

∞

∞

0
Z

0
Z

1
√2π

xfU (xt)fX (x)dx

x

1
√2π
21
−
k
Γ
2

k
2

(xt)2
2

o
xk exp

−

n
∞

exp

k
2

k

0

Z

n

k
2

21
−
k
Γ
2

k

2 xk

k

1 exp

−

(cid:0)
−

1
(cid:1)
2

(k + t2)x2

o

k
2

x2

dx

o

−

n
dx

(cid:0)
To conclude, we make the following change of variable x =

(cid:1)

w
k that leads to

fT (t) =

1
k+1
2 √πk Z

0

∞

e−

Γ( k

2 )2

2

w( t

2k + 1

2 )w

p
k−1
2 dw

(49)

(50)

(51)

(52)

A.3. Proving step 2

The ﬁrst step is quite relevant as it proves that the integral to compute takes various

form depending on the change of variable done.

A.3.1. Using Gamma function

Using the change of variable w = 2ku

t2+k and knowing that Γ(n) =

0 e−
∞

uun

−

1 du, we can

easily conclude as follows:

fT (t) =

=

=

=

∞

e−

2

w( t

2k + 1

2 )w

R

k−1
2 dw

2k
t2 + k

2k
t2 + k

k+1
2

(cid:19)

k+1
2

(cid:19)
k+1
2

∞

e−

uu

k+1
2 −

1 du

0

Z

Γ

(cid:16)

k + 1
2

(cid:17)

0

1
k+1
2 √πk Z
1
k+1
2 √πk (cid:18)
1
k+1
2 √πk (cid:18)
1
√πk (cid:18)

Γ( k

2 )2

Γ( k

2 )2

Γ( k

2 )2
Γ( k+1
2 )
Γ( k
2 )

k
t2 + k

(cid:19)

(53)

(54)

(55)

(56)

11

A.3.2. Using Gamma distribution properties

Another way to conclude is to notice the kernel of a gamma distribution pdf given by
1 ex λ in the integral of 39 with parameters α = (k + 1)/2, λ = (1/2)(1 + t2/k). The
], hence

xα
−
generic pdf for the gamma distribution is λα

1 ex λ and it sums to one over [0,

Γ(α) xα

−

∞

fT (t) =

Γ( k

2 )2

∞

e−

2

w( t

2k + 1

2 )w

1
k+1
2 √πk Z
1
k+1
2 √πk

1
√πk (cid:18)

k+1
2

0
Γ( k+1
2 )
( t2+k
2k )
k
t2 + k

(cid:19)

k+1
2

Γ( k

2 )2
Γ( k+1
2 )
Γ( k
2 )

k−1
2 dw

(57)

(58)

(59)

=

=

A.3.3. Using Mellin transform

The integral of equation 39 can be seen as a Mellin transform for the function g(x) =
2k + 1
w( t

2 ), whose solution is well known and given by

2

e−

Mg(

k + 1
2

)

≡

0

Z

∞

k+1

2 −

x

1g(x)dx =

Γ( k+1
2 )
( t2+k
2k )

k+1
2

Like previously, this concludes the proof.

A.3.4. Using Laplace transform

We can use a result of Laplace transform for the function f (u) = uα as folllows:

Lf (s) =

0
Z

∞

e−

usuαdu =

Γ(α + 1)
sα+1

(60)

(61)

k+1
2 −

0 e−
∞

Hence the integral

uu
the polynomial function taken for s = 1, whose value is Γ
variable w = 2ku
function

1 du is simply the the value of the Laplace transform of
k+1
. Making the change of
2
t2+k in equation 39 enables to conclude similarly to the proof for the Gamma

(cid:17)

(cid:16)

R

12

A.3.5. Using other transforms

Indeed, as the Laplace transform is related to other transform, we could also prove the

result with LaplaceStieltjes, Fourier, Z or Borel transform.

A.4. Sum of independent normals

We want to prove that if Xi ∼

multiple proofs for this results:

N(0, 1) then 1
n
−

1

n

i=1(Xi −

¯Xn)2

χ2
n

−

∼

1. There are

P

Recursive derivation

Cochran’s theorem

•

•

A.4.1. Recursive derivation

Lemma A.1. Let us remind a simple lemma:

•

•

If Z is a N(0, 1) random variable, then Z 2
standard normal random variable is a chi-squared random variable.
If X1, . . . , Xn are independent and Xi ∼
p1+...+pn, which states
that independent chi-squared variables add to a chi-squared variable with its degree of
freedom equal to the sum of individual degree of freedom.

χ2
pi then X1+. . .+Xn ∼

1; which states that the square of a

χ2

χ2

∼

The proof of this simple lemma can be established with variable transformations for the
ﬁst part and by moment generating function for the second part. We can now prove the
following proposition

Proposition 4. If X1, . . . , Xn is a random sample from a N(µ, σ2) distribution, then

n are independent random variables.

¯Xn and s2
¯Xn has a N(µ, σ2/n) distribution where N denotes the normal distribution.
(n

n/σ2 has a chi-squared distribution with n

1 degrees of freedom.

1)s2

•

•
•

−

−

Proof. Without loss of generality, we assume that µ = 0 and σ = 1. We ﬁrst show that sn
can be written only in terms of

i=2,...,n. This comes from:

¯Xn

Xi −
(cid:0)

(cid:1)

s2
n =

=

1

−
1

−

n

n

n

1

(Xi −

i=1
X

n

¯Xn)2 =

1

−

n

(X1 −

1 "
n

¯Xn)2 +

n

i=2
X

(Xi −

¯Xn)2

#

(
1 "

i=2
X

(Xi −

¯Xn))2 +

i=2
X
13

¯Xn)2

(Xi −

#

(62)

(63)

where we have use the fact that

We now show that s2

n

¯Xn).
n and ¯Xn are independent as follows: The joint pdf of the sample

¯Xn) = 0, hence X1 −

i=2(Xi −

i=1(Xi −

¯Xn =

−

n

P

P

X1, . . . , Xn is given by

f (x1, . . . , xn) =

1
(2π)n/2 e−

1
2 Pn

i=1 x2
i ,

< xi <

.

∞

−∞

We make the

y1 = ¯x
y2 = x2 −
...
yn = xn −

¯x

¯x

The Jacobian of the transformation is equal to 1/n. Hence

f (y1, , . . . , yn) =

n
(2π)n/2 e−

1
2 (y1

−

Pn

i=2 yi)2

e−

1

2 Pn

i=2(yi+y1)2

,

< xi <

∞

−∞

= [(

n
2π

)1/2e−

2 y2
n

1 ][

n1/2
(2π)(n

−

1)/2 e−

1
2 [Pn

i=2 y2

i +(Pn

i=2 yi)2]]

(64)

(65)

(66)

(67)

(68)

(69)

(70)

which proves that Y1 = ¯Xn is independent of Y2, . . . , Yn, or equivalently, ¯Xn is independent
n. To ﬁnalize the proof, we need to derive a recursive equation for s2
n as follows: We ﬁrst

of s2
notice that there is a relationship between ¯xn and ¯xn+1 as follows:

¯xn+1 =

n+1
i=1 xi
n + 1

P

=

xn+1 + n¯xn
n + 1

= ¯xn +

1
n + 1

(xn+1 −

¯xn),

We have therefore:

n+1

¯xn+1)2 =

(xi −

[(xi −

¯xn)

−

1
n + 1

(xn+1 −

¯xn)]2

[(xi −

¯xn)2

−

¯xn)(

¯xn

xn+1 −
n + 1

) +

1

(n + 1)2 (xn+1 −

¯xn)2]

i=1
X
2(xi −

n+1

i=1
X
n+1

i=1
X
n+1

ns2

n+1 =

=

=

i=1
X
= (n

(xi −
1)s2

¯xn)2 + (xn+1 −
n
n + 1

(xn+1 −

n +

−

¯xn)2

2

−

¯xn)2

(xn+1 −
n + 1

+

(n + 1)
(n + 1)2 (xn+1 −

¯xn)2 (74)

¯xn)2

14

(75)

(71)

(72)

(73)

We can now get the result by induction. The result is true for n = 2 since s2

with x2
x1
N(0, 1), hence s2
−
√2 ∼
n + n
since ns2
1)s2
n+1 = (n
which is independent of sn and distributed as χ2
lemma, this means that ns2

χ2
2 ∼
n+1(xn+1 −

n+1 is the sum of a χ2
n
1 since xn+1 −
n. This concludes the proof.

1. Suppose it is true for n, that is (n

¯xn)2, s2

¯xn ∼

χ2

−

−

n+1 ∼

x1)2
2 = (x2
−
2
1)s2
χ2
1, then
n ∼
n
−
1 and n
¯xn)2
n+1(xn+1 −
N(0, n+1
n . Using our

−

A.4.2. Cochran’s theorem

Proof. We deﬁne the sub vectorial space F spanned by the vector 1n = (1, . . . , 1)2 which is
T
one for each coordinate. Its projection matrix is given by PF = 1n(1
n .
n 1n1
The orthogonal sub vectorial space of Rn, denoted by F ⊥, has its projection matrix given
PF . The projection of the (xi)i=1,...,n over F (respectively F ⊥) is given by
by PF ⊥ = Idn −
ˆxn)T . The Cochran’s theorem states that these
(ˆxn, . . . , ˆxn)T (respectively) (x1 −
two vectors are independent and that

n = 1
T

T
n 1n)−

2 = (n

χ2(n

1)s2

1)

1

1

ˆxn, . . . , xn −
PF ⊥X
||

||

−

n ∼

−

Appendix B. Various proof around Normal

B.1. Linear combination of Correlated Normal

For any d-dimensional multivariate normal distribution X

Nd(µ, Σ) where Nd stands
for the multi dimensional normal distribution, µ = (µ1, . . . , µd)T and Σjk = cov(Xj, Xk) j, k =
1, . . . , d, the characteristic function is given by:

∼

ϕX(t) = E

exp(itT X)

= exp

itT µ

(cid:2)
= exp

d

i

j=1
X

(cid:3)
tjµj −

1
2

(cid:18)

d

d

j=1
X

Xk=1

1
2

−

tT Σt

(cid:19)

tjtkΣjk

!

(76)

(77)

For a new random variable Z = aT X =

d
j=1 ajXj, the characteristic function for Z

writes:

P

ϕZ(t) = E [exp(itZ)] = E

= exp

it

(cid:2)
ajµj −

d

j=1
X

1
2

exp(itaT X)
d

d

t2

j=1
X

Xk=1

= ϕX (ta)

(cid:3)
ajakΣjk

!

(78)

(79)

This proves that Z is normally distributed with mean given by µZ =

d
j=1 ajµj and
d
k=1 ajakΣjk. We can simplify the expression for the variance

P

variance given by σ2

Z =

d
j=1

P

P

15

 
 
since Σjk = Σkj get:

d

σ2
Z =

a2
j Σjj + 2

j=1
X

d

j

1

−

j=2
X

Xk=1

ajakΣjk

B.2. Variance of the sample mean in AR(1) process

The computation is given as follows

Var(√n( ¯Xn −

µ)) =

=

1
n

1
n

Var(

n

i=1
X
n

(Xi −

µ)) =

E

1
n

(
"

n

i=1
X

(Xi −

µ))2

#

(Xi −

µ)2 + 2

E

"

i=1
X

i=1..n,j=1...i
X

−

(Xi −

1

µ)(Xj −

µ)

#

We have

(80)

(81)

(82)

(83)

n

E

"

i=1
X

(Xi −

µ)2

and

E

"

i=1..n,j=1...i
X

−

(Xi −

1

µ)(Xj −

µ)

#

#

=

=

=

nσ2

ρ2)

(1

−

σ2

ρ2)

(1

−

σ2

j

ρi

−

(84)

i=1..n
X
j=1...i
−

1

i)ρi

(n

−

(85)

ρ2)

(1

−

i=1..n
X

At this stage, we can use rules about geometric series. We have

n

i=1
X

ρi = ρ

= ρ

n

1

−

ρi

i=0
X
1
−
1
−

ρn
ρ

This leads in particular to

n

i=1
X

n(1

i)ρi = ρ

n

i=1
X

(n

−

16

iρi = ρ

∂
∂ρ

n

ρi

1

= ρ

−

i=0
X
(n + 1)ρn + nρn+1
ρ)2

(1

−

ρn)

−

−

ρ)
(1

(1
ρ)2

−
−

(86)

(87)

(88)

Hence,

Var(√n( ¯Xn −

µ)) =

=

=

=

σ2

(1

ρ2)n

−
σ2

ρ2)n

(1

−
σ2

1

ρ2
−
σ2

(cid:20)

n + 2

(cid:20)

(cid:18)

n + 2

(cid:20)
1 + ρ
1

ρ −

−
1

nρ

1
−
1
−
nρ(1
−

ρn
ρ −
ρ)
(1
ρn)
−
ρ)2
−
ρn)
−
ρ)(1 + ρ)

(cid:21)

−
−

(cid:18)
2ρ(1
n(1
2ρ(1

(cid:21)

ρ)2

(1

−

(cid:20)

−

n(1

−

1

ρ

−

(n + 1)ρn + nρn+1
ρ)2

ρ(1
ρ)2

−

B.3. Variance of the sample mean in AR(1) process

The computation of 23 is easy and given by

Cov( ¯Yn, Yj) =

E

1
n

n

i=1
X

"

σ2

YiYj

#
n

ρ2) "

i=1
X
j
n
−

i

ρ|

−

j

|

ρi +

#
j

−

1

=

=

=

=

n(1

n(1

n(1

n(1

−
σ2

−
σ2

−
σ2

−

ρi

−

ρ2) "

ρ2)

ρ2)

(cid:20)

(cid:20)

i=0
X
ρn+1
j
−
ρ

i=0
X
1
−
1
−
1 + ρ

+

ρn+1
−
−
ρ
1

−

#
ρj
ρ −
ρj

1
1
j

−
−
−

(cid:21)

1

(cid:21)

(1
ρn)

−

(cid:19)(cid:21)

1

(cid:19)(cid:21)

(89)

(90)

(91)

(92)

(93)

(94)

(95)

(96)

(97)

The second equation 24 is trivial as

¯Yn =

n
i=1 Yi
n

.

P

Hence

n

j=1
X

Cov( ¯Yn, Yj) = nVar( ¯Yn) =

σ2

ρ)2

(1

−

1
(cid:20)

2ρ(1

ρn)
−
ρ)(1 + ρ)

−

n(1

−

(98)

(cid:21)

For the last equation, we can compute and get the result as follows

17

n

j=1
X

(cid:0)

Cov( ¯Yn, Yj)

2

=

(cid:1)

=

Expanding the square leads to

σ4

1 + ρ

(cid:20)

n

n

j=1
X

n2(1

ρ2)2

−
σ4
ρ2)2(1

n2(1

−

j

−

ρn+1
−
−
ρ
1

−

2

ρj

(cid:21)

ρ)2

−

1 + ρ

j=1
X

(cid:2)

ρn+1
−

j

−

2

ρj

−

(cid:3)

(99)

(100)

n

j=1
X

(cid:2)

1 + ρ

−

ρn+1
−

j

−

ρj

2

=

(1+ρ)2+(ρ2)n+1
−

j+(ρ2)j

2(1+ρ)ρn+1
−

j

−

−

2(1+ρ)ρj +2ρn+1

(101)
Denoting by S the summation, computing the diﬀerent terms and summing them up

n

(cid:3)

j=1
X

leads to

S = n

(1 + ρ)2 + 2ρn+1

since

(cid:0)

n

+ 2ρ2 1
1

ρ2n
ρ2 −

−
−

(cid:1)

4(1 + ρ)ρ

ρn
ρ

1
−
1
−

j=1
X
Regrouping all the terms leads to

ρj + ρn+1
−

j = 2ρ

ρ2
ρ

1
1

−
−

n

Cov( ¯Yn, Yj)

2 =

j=1
X

(cid:0)

(cid:1)

σ4

(1 + ρ)2 + 2ρn+1

ρ2)2

(1

−

(cid:20)

ρ)2

(1

−

1
n −

4(1 + ρ)2ρ(1

ρn)
−
ρ)2(1

(1

−

(102)

(103)

ρ2n)

2ρ2(1
ρ2)

−

1
n2

(cid:21)

−
−

B.4. Expectation of denominator

Lemma 3.1) states that

E

n ¯Y 2
n

=

(cid:3)
We can compute as follows:

(cid:2)

σ2

ρ2

1

−

(cid:18)

1 + ρ
1

−

ρ −

2ρ(1
n(1

ρn)
ρ)2

−
−

(cid:19)

(104)

18

(105)

(106)

(107)

(108)

(109)

(110)

(111)

(112)

Es2

n =

1

−

n

1

n

E

i=1
X

"
σ2
1)(1
σ2
1)(1

(n

−

(n

−
σ2

1

1

ρ2

−
σ2

ρ2

−

1
(cid:18)
1
(cid:18)

−

(1

−

(1

−

−

Y 2
i −

n ¯Y 2
n

n

−

(n

(cid:18)
1)

ρ2)

ρ2)

(cid:20)

(cid:20)

−

−

#
1 + ρ
1

−

2ρ(1
n(1

ρ −

−
−

ρn)
ρ)2
2ρ(1
n(1
ρn
ρ)

(cid:19)
ρn)

2ρ

1

−

−

ρ −
1
−
n(1
−
2ρ(1

−
2ρ
ρ)(n
2ρ
ρ)(n

−

1)

1)

(cid:18)
(1

+

−

−

−
1)(1

n(n

−

ρ)2

(cid:19)

−

(cid:19)(cid:21)
−
−

ρn)
ρ)2

(cid:19)(cid:21)

=

=

=

=

B.5. Second moment of denominator

We have

Es4

n =

=

=

(n

(n

1

−
1

−
1

1)2

1)2

(n

1)2

E

E

E

n

(
"

(
"

i=1
X
n

i=1
X
n

Y 2
i −

n ¯Y 2

n )2

#

i )2 + n2 ¯Y 4
Y 2

n −

n

Y 4
i + 2

"

n

2n ¯Y 2
n (

Y 2
i )

i=1
X
k + n2 ¯Y 4

i Y 2
Y 2

#

n −

n

2n ¯Y 2
n (

Y 2
i )

#

Xi=1,k=i+1
We can compute the fourth moment successively. Since both Yi and ¯Yn are two normal,

i=1
X

i=1
X

−

their fourth moment is three times the squared variance. This gives:

2ρ(1
n(1

2

ρn)
ρ)2

−
−

(cid:21)

(113)

(114)

1 + ρ
1

−

ρ −

n

E

"

i=1
X
E

Y 4
i

#

= 3n

n2 ¯Y 4
n

= 3

(cid:2)

(cid:3)

σ4

ρ2)2

×

(1

−
σ4

×

(1

ρ2)2

(cid:20)

−

19

(115)

(116)

(117)

(118)

(119)

(120)

(121)

(122)

The cross terms between Yi Yk is more involved and is computed as follows:

n

E

(cid:20)

i=1
X
k=i+1

i Y 2
Y 2
k

= E
(cid:20)

(cid:21)

n

i=1
X
k=i+1
σ4

ρ2(k

−

i)Y 4

i + (1

−

ρ2(k

−

i))Y 2

i (Y ⊥i )2

(cid:21)

=

=

=

n

2ρ2(k

i) + 1

−

ρ2)2

(1

−

σ4

(1

ρ2)2

−
σ4

ρ2)2

(1

−

(cid:20)

i=1
X
k=i+1
n

(n

2
(cid:20)
i=1
X
2ρ2 n(1
(cid:20)

−

−

(cid:21)

i)ρ2i +

n(n

−
2

1)

ρ2n)

ρ2)
(1

−
−

(1
ρ2)2

−

(cid:21)

+

n(n

1)

−
2

(cid:21)

For the cross term between ¯Y 2

i , we wan use the fact that ¯Yn and Yi are
n
i=1 Y 2
two correlated Gaussians. Remember that for two Gaussians, E[U 2V 2] = E[U 2]E[V 2] +
P
2(Cov(U, V ))2. We apply this trick to get:

n and

E

¯Y 2
n (
"

n

i=1
X

Y 2
i )

#

=

=

n

E

¯Y 2
n Y 2
i

i=1
X
n

(cid:2)

E

¯Y 2
n

i=1
X

(cid:2)

(cid:3)

n

= E

¯Y 2
n

(cid:3)
E

Y 2
i

+ 2(Cov( ¯Yn, Yi))2

(cid:3)

(cid:2)
Y 2
i

E

n

+ 2

(Cov( ¯Yn, Yi))2

(cid:2)

i=1
X

(cid:3)

(cid:2)

(cid:3)

i=1
X

The ﬁrst term is given by

n

E

¯Y 2
n

E

Y 2
i

=

(cid:2)
The second term is given by

(cid:3)

(cid:2)

i=1
X

(cid:3)

σ4

ρ2)2

(1

−

(cid:20)

1 + ρ
1

−

ρ −

2ρ(1
n(1

ρn)
ρ)2

−
−

(cid:21)

4(1 + ρ)2ρ(1

ρn)
−
ρ)2(1

(1

−

−
−

ρ2n)

2ρ2(1
ρ2)

−

1
n2

(cid:21)

(123)

n

2

(Cov( ¯Yn, Yi))2 =

i=1
X

2σ4

(1 + ρ)2 + 2ρn+1

ρ2)2

(1

−

(cid:20)

ρ)2

(1

−

1
n −

Summing up all quantities leads to

20

1)2Es4

n =

(n

−

σ4

(1

−

ρ2)2

(cid:20)

3n + n(n

−

1) + 4ρ2 n(1

−

ρ2)
(1

−
−

(1
ρ2)2

−

ρ2n)

+ 3

(cid:20)

1 + ρ
1

−

ρ −

2ρ(1
n(1

2

(cid:21)

ρn)
−
ρ)2
−
(124)

2n

−

(cid:20)

1 + ρ
1

−

ρ −

2ρ(1
n(1

−
−

ρn)
ρ)2 +

(1 + ρ)2 + 2ρn+1

ρ)2

(1

−

2
n −

(1 + ρ)2ρ(1
(1

ρn)
ρ)2(1

−
−

−
−

2ρ2(1
ρ2)

ρ2n)

−

8
n2

(cid:21) (cid:21)

Regrouping all the terms leads to

E[s4

n] =

σ4

ρ2)2

(n

(1

−

1

−

1)2

n2
(cid:20)

−

1 + ρ

nA1 + A2 +

(cid:18)

1
n

A3 +

1
n2 A4

(cid:19)(cid:21)

with

1

4
A1 = −
ρ2
−
2 (3 + 9ρ + 11ρ2 + 3ρ3 + 6ρn + 12ρn+1 + 6ρn+2
(1
−
8ρn+1)

A2 = −

ρ2)2

4(1

3ρ + 4ρ2
r)3(1 + r)

−

A3 =

−

A4 =

12ρ(1
(1

−

ρn)(1
−
(1
−
ρn)2
−
ρ)4

2ρ2n+2)

−

(125)

(126)

(127)

(128)

(129)

(130)

B.6. Variance of denominator

The result is obtained from meticulously computing the variance knowing that

Var[s4

n] = E[s4
n]

(E[s2

n])2

−

(131)

The terms for the part E[s4

n] have already been computed in proposition 3. As for the

term coming from the square of the expectation, they write as:

21

(E[s2

n])2 =

=

σ4

ρ2)2
σ4
1)2(1

(1

−

(n

−

1
(cid:18)

−

(1

−

ρ2)2

−

2ρ
ρ)(n

2ρ(1

ρn)

+

1)

n(n

−
1)(1

−
1)

−

−

−
2ρ(1
n(1

+

ρ)2
−
ρn)
ρ)2

−
−

(cid:19)

2ρ

ρ

1

−

2

(cid:19)
2

(132)

(133)

Let us write the square as E1 =

(n

as follows

(cid:16)

−

2ρ
1

ρ + 2ρ(1

n(1

−

ρn)
ρ)2

−
−

. We can expand the square

2

(cid:17)

(n

(cid:18)

1)

−

E1 = (n

1)2 +

−

4(n

−
n(1

1)ρ(1

−
ρ)

−

ρn)

−

4ρ(n
1

−
ρ

−

1)

+

2ρ

ρ)2

(1

−

(cid:18)

1
−
n(1

ρn
ρ) −

−

2

1

(cid:19)

(134)

Rearranging the terms leads then to the ﬁnal result.

22

References

Benhamou, E., 2018. A few properties of sample variance. arxiv .

Bondesson, L., 1974. Characterizations of probability laws through constant regression.

Z.Wahrscheinlichkeitstheorie verw. Gebiete pp. 93–115.

Bondesson, L., 1983. When is the t-statistic t-distributed. Sankhya, Ser. A pp. 338–345.

Efron, B., 1969. Student’s t-test under symmetry conditions. J. Amer. Statist. Assoc. pp.

1278–1302.

Fang, K., Yang, Z., Kotz, S., 2001. Generation of multivariate distributions by vertical

density representation. Statistics pp. 281–293.

Kagan, A., Linnik, Y., Rao, C., 1973. Characterization problems in mathematical statistics.

Mauldon, J., 1956. Characterizing properties of statistical distributions. Quart. J. Math. pp.

155–160.

Mikusheva, A., 2015. Second order expansion of the t-statistics in ar(1) models. Econometric

Theory 31, 426–448.

23

