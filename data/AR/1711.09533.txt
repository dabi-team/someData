7
1
0
2

v
o
N
7
2

]
E
M

.
t
a
t
s
[

1
v
3
3
5
9
0
.
1
1
7
1
:
v
i
X
r
a

Empirical Likelihood for Change Point Detection

in Autoregressive Models

Ramadha D. Piyadi Gamage1, Wei Ning2∗

1Department of Mathematics

Western Washington University, Bellingham, WA 98225, USA

2Department of Mathematics and Statistics

Bowling Green State University, Bowling Green, OH 43403, USA

Abstract

Change point analysis has become an important research topic in many ﬁelds of

applications. Several research work has been carried out to detect changes and its

locations in time series data. In this paper, a nonparametric method based on the

empirical likelihood is proposed to detect the structural changes of the parameters in

autoregressive (AR) models . Under certain conditions, the asymptotic null distribu-

tion of the empirical likelihood ratio test statistic is proved to be the extreme value

∗Corresponding author. Email: wning@bgsu.edu

1

 
 
 
 
 
 
distribution. Further, the consistency of the test statistic has been proved. Simula-

tions have been carried out to show that the power of the proposed test statistic is

signiﬁcant. The proposed method is applied to real world data set to further illustrate

the testing procedure.

Keywords: Autoregressive model; Change point analysis; Empirical Likelihood;

Extreme value distribution; Consistency.

1

Introduction

Change point analysis introduced by Page (1954, 1955) has become popular due to its

usage in wide variety of ﬁelds, such as stock market analysis, quality control, trafﬁc mor-

tality rate, geology data analysis, genetics, etc. It concerns both detecting whether or not

a change(s) has (have) occurred, and identifying the location(s) of any such change(s).

Several methods to identify and estimate the change points in the change point problem

are proposed by scholars. Bayesian approach to detect changes in the mean has been

discussed by Chernoff and Zacks (1964) and Sen Srivastava (1975). Further, Cs¨org´o and

Horv´ath (1997) and Chen and Gupta (2000) established asymptotic results on parametric

change point models. Hawkins (1977), Worsley (1986) and Gombay and Horv´ath (1994)

are a few among the many researchers who discussed change point problem under the

parametric settings. However, the parametric methods are no longer applicable if the un-

derlying distribution is completely unknown. In such a case, a nonparametric approach

2

should be considered as an alternative. One such popular nonparametric approach is the

Cumulative Sum (CUSUM) method. Most authors have assumed that the observations are

independent and studied the case where two distributions differ only in location. Com-

bining nonparametric approaches along with the change point detection has been studied

by many scholars over the past years. Aue and Horv´ath (2012) discussed two methods,

namely, Cumulative Sum (CUSUM) and Likelihood Ratio Test (LRT), on how they can

be modiﬁed for data exhibiting serial dependence. Further, they provided some insight to

the sequential procedure as well. Lee et. al. (2003) also discussed about the Cusum test

for changes of parameters in time series models and considered the changes of the param-

eters in a random coefﬁcient autoregressive model AR(1) and that of the autocovariances

of a linear process.

The change point problem may be viewed as a two-sample test adjusted for the un-

known break location, thus leading to max-type procedures. Correspondingly, asymptotic

relationships are derived to obtain critical values for the tests.

In general, the change

point problem can described as follows. Let x1, x2, ..., xn be a sequence of independent

random vectors (variables) with probability distribution functions F1, F2, ..., Fn, respec-

tively. More speciﬁcally, suppose that the distributions F1, F2, ..., Fn belong to a common

parametric family F(θ), where θ ∈ Rp, then the change point problem is to test the hy-

potheses about the population parameters θi, i = 1, ..., n

H0 : θ1 = θ2 = ... = θn = θ(unknown),

3

versus the alternative

H1 : θ1 = ... = θk1 (cid:54)= θk1+1 = ... = θk2 (cid:54)= ... (cid:54)= θkq−1 = ... = θkq (cid:54)= θkq+1... = θn,

where q and k1, k2, ..., kq are unknown and need to be estimated.

Empirical likelihood introduced by Owen (1988, 1990) is one of the popular and pow-

erful nonparametric approaches. It has been widely used due to the robustness of its non-

parametric nature and the efﬁciency of its likelihood construction. Kolaczyk (1994) used

empirical likelihood with generalized linear models. Further, Qin and Lawless (1994) ob-

tained estimating equations and derived asymptotic properties of the test statistic. Many

scholars have discussed about the empirical likelihood ratio test for a change point in lin-

ear models, such as Zou et al. (2007) , Liu et al. (2008) , and Ning (2012). Since the

empirical likelihood was originally proposed for independent data, it is difﬁcult to apply

it to dependent data such as time series data. Several approaches suggested to reduce the

dependent data problem into an independent data problem. Owen (2001) suggested using

the conditional likelihood to remove the dependence structure and generate the estimat-

ing equations. Kitamura (1997) used block-wise empirical likelihood method which pre-

serves the dependence of data, and the resulting likelihood ratios have been used to con-

struct asymptotically valid conﬁdence intervals. Ogata (2005) and Nordman and Lahiri

(2006) independently formulated a frequency domain empirical likelihood (FDEL) using

spectral estimating equations which can be used for short- and long- range dependent

data. Bai and Perron (1998) proposed CUSUM and F-based statistics for change point

4

detection. Baragona et al. (2013) compared it with the test they proposed for change

point detection based on the empirical likelihood approach for change point detection.

To deal with the situation of multiple changes, it traditionally uses the binary segmen-

tation method proposed by Vostrikova (1981). The advantage of using this method is that

it detects number of change points and estimates their locations simultaneously as well

as the consistency of this method has been established. Hence, the general hypothesis of

the change point problem can be simpliﬁed as the hypothesis of no change point versus a

single change point, i.e. the alternative hypothesis is:

H1 : θ1 = ... = θk (cid:54)= θk+1 = ... = θn,

where k is the location of the single change point at this stage. If H0 is not rejected, then

the process is stopped and we conclude that there is no change. If H0 is rejected, then

there is a change point and the two subsequences before and after the change point found

are tested for a change. This process is repeated until there are no subsequences having

change points.

In this paper, we propose a test statistic based on the empirical likelihood approach for

detecting changes in a time series model. In Section 2, the change point problem in time

series models has been introduced for AR(p) model. The empirical likelihood procedure

for change point detection is described in Section 3. The null asymptotic distribution of

the test statistic and the consistency of the test along with the proofs are provided under

Section 4. Simulations are carried out in Section 5 and a real data application is given

5

in Section 6. Section 7 provides some discussion and proofs of results are given in the

Appendix.

2 Changepoint Problem in AR(p) Model

Consider the stationary AR(p) model with the mean 0.

Xt =





(cid:80)p

i=1 φiXt−i + (cid:15)t; 1 ≤ t ≤ k

(cid:80)p

i=1 φ∗

i Xt−i + (cid:15)t; k + 1 ≤ t ≤ n,

where (cid:15)t’s are independent random variables with mean zero and variance σ2, (i.e. White

noise process),φ1, φ2, ..., φp, φ∗

1, φ∗

2, ..., φ∗

p are all unknown parameters, and k is the un-

known change location which needs to be estimated. Denote δ = Φ∗ − Φ, where

Φ = (φ1, φ2, ..., φp)(cid:48) and Φ∗ = (φ∗

1, φ∗

2, ..., φ∗

p)(cid:48). Therefore, the change point problem

is to test the null hypothesis of no change in the autoregressive parameters versus the

alternative hypothesis of one unknown change, i.e.,

H0 : δ = 0

vs H1 : δ (cid:54)= 0, at least one δi (cid:54)= 0.

Hence, under the alternative hypothesis, there is a change in at least one of the p param-

eters at an unknown location. We denote β0 = (Φ, 0)(cid:48) ∈ R2p and β1 = (Φ, δ)(cid:48) ∈ R2p

to be the parameter vectors under the null and the alternative hypothesis respectively.

According to Owen (1991), we derive the estimating functions to be

g1(Xi, β0) = (Xi, Xi−1(cid:15)i, ..., Xi−p(cid:15)i, (cid:15)i

2 − σ2),

(1)

6

where (cid:15)i = Xi − (cid:80)p

r=1 φiXi−r, and

g2(Xj, β1) = (Xj, Xj−1(cid:15)j, ..., Xj−p(cid:15)j, (cid:15)j

2 − σ2),

(2)

where (cid:15)j = Xj − (cid:80)p

r=1 φ∗

j Xj−r. It is easy to see that

E[g1(Xi, β0)] = 0,

E[g2(Xj, β1)] = 0.

for every 1 ≤ i ≤ k and k + 1 ≤ j ≤ n.

3 Empirical Likelihood for AR(p) Changepoint Model

WLOG, we assume one change point at an unknown location k. Let

ΩH0 = (cid:8)(p, q, β0)|

(cid:88)

i

pig1(Xi, β0) =

qjg2(Xj, β0) = 0(cid:9),

(cid:88)

j

and

ΩH1 = (cid:8)(p, q, β1)|

(cid:88)

i

pig1(Xi, β0) = 0,

qjg2(Xj, β1) = 0(cid:9)

(cid:88)

j

(3)

(4)

be the parameter spaces under H0 and H1, respectively, where p = (p1, p2, ..., pk) and

q = (qk+1, qk+2, ..., qn) are the probability vectors such that (cid:80)k

i=1 pi = 1, (cid:80)n

j=k+1 qj = 1

and pi ≥ 0, qj ≥ 0. If a change occurs at k, then the empirical likelihood ratio test statistic

is deﬁned as,

−2 log Λk = −2 log

(cid:110)(cid:81)

(cid:110)(cid:81)

sup
H0

sup
H1

(cid:81)

i pi

j qj|(p, q, β0) ∈ ΩH0

(cid:81)

i pi

j qj|(p, q, β1) ∈ ΩH1

(cid:111)

(cid:111)

7

= ZH0,k − ZH1,k,

where

ZH0,k = −2sup
H0

ZH1,k = −2sup
H1

(cid:40)

(cid:88)

i

(cid:40)

(cid:88)

i

log kpi +

log kpi +

(cid:88)

j

(cid:88)

j

log(n − k)qj|(p, q, β0) ∈ ΩH0

log(n − k)qj|(p, q, β1) ∈ ΩH1

(cid:41)

(cid:41)

,

.

The null hypothesis is rejected for a sufﬁciently large value of max
1<k<n

− 2 log Λk. Let

θnk = k

n . A Lagrangian argument gives,

pi =

1
nθnk(1 + θ−1
nk λ(cid:48)

1g1(Xi, ·))

and

qj =

1

n(1 − θnk)(1 + (1 − θnk)−1λ(cid:48)

2g2(Xj, ·))

where λ1 and λ2 are chosen such that (cid:80)

i pig1(Xi, ·) = 0 and (cid:80)

j qjg2(Xj, ·) = 0. There-

fore, under H0, we obtain

ZH0,k = 2inf
β0

sup
λ1,λ2

(cid:40)

(cid:88)

i

log(1 + θ−1

nk λ(cid:48)

1g1(Xi, β0)) +

log(1 + (1 − θnk)−1λ(cid:48)

2g2(Xi, β0))

.

(cid:41)

(cid:88)

j

Let λ = (λ(cid:48)

1, λ(cid:48)

2)(cid:48). The score functions are deﬁned as:

Q1n(β0, λ) =

∂ZH0,k
∂λ

=

1
n

(cid:88)

m

1

1 + θ−1

m λ(cid:48)g(Xm, β0)

θ−1
m g(Xm, β0),

and

Q2n(β0, λ) =

∂ZH0,k
∂β0

=

1
n

(cid:88)

m

1

1 + θ−1

m λ(cid:48)g(Xm, β0)

θ−1
m

(cid:18) ∂g(Xm, β0)
∂β0

(cid:19)(cid:48)

g(Xm, β0),

8

where

m = θ−1
θ−1

nk 1{1≤m≤k} + (1 − θnk)−11{k+1≤m≤n},

g(Xm, β0) = g1(Xm, β0)1{1≤m≤k} + g2(Xm, β0)1{k+1≤m≤n}.

Under certain regularity conditions, Qin and Lawless (1994) showed, there exists ( ˜β0, ˜λ)

such that,

Q1n( ˜β0, ˜λ) = 0 and Q2n( ˜β0, ˜λ) = 0.

Hence, we obtain ZH0,k = 2lE( ˜Φ◦, ˜µ◦, 0).

Similarly, under H1 we have, ZH1,k = 2lE( ˜Φ, ˜µ, δ). Then the empirical likelihood

ratio statistic can be rewritten as

−2 log Λk = 2lE( ˜Φ◦, ˜µ◦, 0) − 2lE( ˜Φ, ˜µ, δ).

(5)

Since k is unknown, H0 is rejected when the maximally selected log-likelihood ratio

statistic,

Zn = max
θnk∈Θn

{−2 log Λk},

where Θn = {k/n : k = 1, 2, ..., n}, is sufﬁciently large.

When k or n − k is too small, then the minimax estimators of empirical likelihood

( ˜β1, ˜λ) may not exist. Hence we consider the trimmed likelihood ratio statistic where

the range of k is selected arbitrarily as follows. The Trimmed likelihood ratio statistic is

deﬁned as,

Z ∗

n = max
θnk∈Θ∗
n

{−2 log Λk},

(6)

9

where Θ∗

nk = {k/n : k = nT1, nT1 + 1, ..., n − nT2}. According to Perron and Vogelsang

(1992), the selection of nT1 and nT2 can be arbitrary. In our work, we choose nT1 = nT2 =

2[n 1

2 ], where [x] means the largest integer not larger than x. If H0 is true, then Z ∗

n follows

an asymptotic extreme value limit distribution. The convergence to the extreme value

limit can be slow and asymptotic test often tends to be too conservative in ﬁnite samples.

4 Main Results

The results are similar to the ones by Cs¨org´o and Horv´ath (1997). Under mild regularity

conditions, the following theorem holds.

Theorem 1. Let β∗ be the true parameter. Suppose that E||g(X, β∗)||3 < ∞, E||(cid:15)||4 <

∞, and E((cid:15)(cid:15)(cid:48)) is positive deﬁnite. If H0 is true, then we have

lim
n→∞

P r{A(log u(n))(Z ∗
n)

1

2 ≤ t + Dr(log u(n))} = exp(−e−t)

for all t, where A(x) = (2 log x) 1

2 , Dr(x) = 2 log x + (r/2) log log x − log Γ(r/2),

u(n) = n2+(2(cid:98)n

1
2 (cid:99))2−2n(cid:98)n

1
2 (cid:99)

(2(cid:98)n

1
2 (cid:99))2

, and r is the dimension of the parameter δ.

Theorem 2. Under the conditions of Theorem 1 and the condition that for every ﬁxed

parameter δ = β∗ − β (cid:54)= 0, there exists a positive constant c0 > 0 satisfy that ∞ >

inf
δ(cid:54)=0

sup
λ

E log (cid:2)1 + λ(cid:48)x(x(cid:48)δ + e)(cid:3) ≥ c0 > 0, holds. If H1 is true, assume that θk0 = k0

n →

θ0 ∈ (0, 1) as n → ∞, then ELR test statistic is consistent, i.e. there exists a constant

c > 0 such that

P (Zn > cn) → 1.

10

Theorem 3. Under the conditions of Theorem 1 and the condition that for every ﬁxed

parameter δ = β∗ − β (cid:54)= 0, there exists a positive constant c0 > 0 satisfy that ∞ >

inf
δ(cid:54)=0

sup
λ

E log (cid:2)1 + λ(cid:48)x(x(cid:48)δ + e)(cid:3) ≥ c0 > 0, holds. If H1 is true, assume that θk0 = k0

n →

θ0 ∈ (0, 1) as n → ∞, we have ˆθk → θ0 in probability as n → ∞.

Proofs are given in the Appendix.

5 Simulation Study

A Monte Carlo simulation has been conducted to illustrate the performance of the pro-

posed method. Consider the following AR(1) model with mean 0:

Xt =





0.1Xt−1 + (cid:15)t; 1 ≤ t ≤ k

0.5Xt−1 + (cid:15)t; k + 1 ≤ t ≤ n,

where (cid:15)t is the white noise with mean zero and variance σ2. Four different distributions

are considered for (cid:15)t: (i) N (0, 1), (ii) exp(1) − 1, (iii)

1
√

2

(χ2

4 − 4), and (iv) 1√
2

2

t4. The

power of the proposed test in detecting changes in parameters of the AR(1) model has

been calculated for two different sample sizes: n=100, 150 and 250. Different change

locations have been considered under each sample size. Additional simulations have been

carried out to compute the empirical critical values under different signiﬁcance levels

which are turned out to be close to the theoretical critical values for the corresponding

signiﬁcance level. Hence, we use the theoretical critical value 2.9702 with α = 0.05 for

power calculations with 1000 simulations. The results are listed in Table 1. It can be

11

seen that the power of the hypothesis test of AR(1) model increases with the sample

size. The power values under a given change location are approximately similar for the

four different error distributions. This maybe due to the fact that the three distributions

exp(1) − 1,

1
√

2

(χ2

4 − 4) and 1√
2

2

t4 are standardized. When the change location is farther

away from the starting location, then the power tends to decrease. Intuitively, this maybe

due to the dependency existing in the data set.

12

Table 1: Power of the hypothesis test for AR(1) model

k N (0, 1)

exp(1) − 1

1
√

2

2

(χ2

4 − 4)

1√
2

t4

n = 100

20

30

40

50

80

30

45

60

75

0.802

0.816

0.815

0.808

0.765

0.775

0.774

0.779

0.723

0.732

0.754

0.747

0.656

0.669

0.627

0.674

0.296

0.292

0.283

0.331

n = 150

0.929

0.929

0.924

0.903

0.913

0.901

0.893

0.872

0.862

0.844

0.853

0.845

0.806

0.799

0.813

0.804

120

0.385

0.397

0.426

0.450

n = 250

50

80

0.993

0.988

0.990

0.976

0.983

0.981

0.976

0.965

100

0.966

0.966

0.976

0.948

125

0.941

0.927

0.928

0.926

200

0.621

0.603

0.626

0.622

13

6 Application

In this section, we study the data which consists of monthly average soybean prices

achieved by farmers in Illinois from January 1960 to November 2008 with the sample

size 587. The prices are given in dollars per bushel. This data was analyzed by Balcombe

et al. (2007) who considered the threshold AR(1) models for modeling the prices of agri-

cultural products. Berkes et al. (2011) studied this data set by proposing the likelihood

ratio test to detect the structural change of an AR model to threshold AR model. We apply

the proposed EL method for AR(1) changepoint model to detect the structural change in

the same data set. Figure 1 shows the time series plot for the given data. In order to test if

Figure 1: Time Series for the Monthly average soybean prices

there are signiﬁcant changes, we use the Z ∗

n from (6). The value of Z ∗

n is 16.07426. Us-

ing the critical values derived under the Theorem are given in Table 2, we have sufﬁcient

evidence to reject the null hypothesis that there is no change.

14

Table 2: Theoretical Critical values

α = 0.01 α = 0.05 α = 0.10

4.600149

2.970195

2.250367

7 Discussion

In this paper, we discuss developing an EL-based detecting procedure for structural changes

in time series data, i.e. testing null hypothesis of no change versus alternative hypothesis

of one change. A test statistic is derived for a ﬁxed change location and the max-type of

test statistic over all possible change locations is considered. The asymptotic null distri-

bution of the test statistic has been established as extreme value distribution. Simulations

to compute the power in AR(1) model have been carried out with different sample sizes

and different error distributions in order to illustrate the performance of the proposed test

statistic. The results indicate that the proposed method is efﬁciently identify the changes

in a given time series data set. We should point out that, due to the slow convergence

of the proposed test statistic in Theorem 1, the moderate or the large sample size is rec-

ommended to achieve the good approximation (See Cs¨org˝o and Horv´ath, 1997). If the

sample size is small, the bootstrap is suggested to obtain the approximated p-values in

practice.

As for future work, we plan to extend the proposed method to other stationary time

series models such as MA, ARMA, GARCH models along with corresponding analytic

15

results and simulations. Comparisons to other existing methods will be done. Further,

sequential change point detection based on EL method is to be studied where the sample

size is a random variable and the null hypothesis of sequential structural stability will be

rejected as soon as a change is detected. Hence, the objective in sequential change point

detection is to detect such a change with a minimum number of false alarms. A nonpara-

metric testing procedure based on EL method will be proposed and related asymptotic

results will be studied.

Appendix

In order to prove Theorem 1, we need following Lemmas.

Lemma 1. Assume that for i = 1, 2, E[gi(X, β0)g(cid:48)

continuous in a neighborhood of the true value β0, E

i(X, β0)] is positive deﬁnite, ∂gi(X,β)
(cid:17)(cid:16) ∂gi(X,β0)
(cid:104)(cid:16) ∂gi(X,β0)
∂β(cid:48)

∂β
(cid:104) ∂2gi(X,β)
∂β∂β(cid:48)

(cid:17)(cid:48)(cid:105)

, E

∂β(cid:48)

is

(cid:105)

,

(cid:104)(cid:16) ∂gi(X,β)
∂β(cid:48)

E

(cid:17)(cid:48)

(cid:105)
gi(X, β)

and E (cid:107) gi(X, β) (cid:107)3 are all bounded in the neighborhood of the

true value β0. Then, as n → ∞, ∃ ˜β, ˜λ = λ( ˜β) with probability 1 satisfying,

Q1n( ˜β, ˜λ) = 0, Q2n( ˜β, ˜λ) = 0 and (cid:107) ˜β − β0 (cid:107)= Op(m− 1

2 ),

where

Q1n(β, λ) =

Q2n(β, λ) =

(cid:88)

l

(cid:88)

l

1 + λ(cid:48)(β)θ−1

1
l g(xl, β)

θ−1
l g(xl, β),

1 + λ(cid:48)(β)θ−1

1
l g(xl, β)

θ−1
l

(cid:19)(cid:48)

(cid:18) ∂g(xl, β)
∂β

λ(β).

16

Proof. First we will show

λ(β) = (cid:15)kOp(m− 1
2 )

=

(cid:104) 1
n

n
(cid:88)

l=1

θ−2
l g(xl, β)g(cid:48)(xl, β)

(cid:105)−1(cid:104) 1
n

n
(cid:88)

l=1

(cid:105)
θ−1
l g(xl, β)

+ (cid:15)kop(m− 1

2 ),

where (cid:15)k = min{θk, 1 − θk} and m = n(cid:15)k = min{k, n − k}.

Let β − β0 = um− 1

2 for β ∈ {β :(cid:107) β − β0 (cid:107)= m− 1

2 } where (cid:107) u (cid:107)= 1. Let λ be the

solution of the function f (λ) given by the ﬁrst score function deﬁned in Section 3.

f (λ) =

1
n

n
(cid:88)

l=1

θ−1
l
1 + λ(cid:48)(β)θ−1

l g(xl, β)

g(xl, β) = 0.

(A.1)

Let λ = ρu where u = (β − β0)m 1

2 and (cid:107) u (cid:107)= 1.

0 =(cid:107) f (ρu) (cid:107)

(cid:12)
(cid:12)

=

1
n

≥ |u(cid:48)f (ρu)|
(cid:12)u(cid:48)(cid:16) (cid:88)
l
l
θ−2
l g(xl, β)u(cid:48)g(xl, β)
1 + ρu(cid:48)θ−1

θ−1
l g(xl, β) − ρ

u(cid:48) (cid:88)

ρ
n

≥

(cid:88)

l

l

θ−2
l g(xl, β)u(cid:48)g(xl, β)
1 + ρu(cid:48)θ−1
l g(xl, β)

(cid:17)(cid:12)
(cid:12)
(cid:12)

u −

(cid:12)
(cid:12)
(cid:12)

1
n

p
(cid:88)

(cid:88)

ej

j=1

l

(cid:12)
θ−1
(cid:12)
l g(xl, β)
(cid:12)

( where ej is the unit vector in the jth coordinate direction.)

≥

ρu(cid:48)Su
1 + ρθlg∗ − Op(m− 1

2 ),

(where g∗ = max

g(xl, β) and S = 1
n

l

(cid:80)

l θ−2

l g(xl, β)g(cid:48)(xl, β).)

Since u(cid:48)Su ≥ σp + op(1), where σp > 0 is the smallest eigen value of Σ, then

ρ

1 + ρθlg∗ = Op(m− 1

2 )

17

So, (cid:107) λ (cid:107)= ρ = Op(m− 1

2 ).

Let γl = λ(cid:48)(β)θ−1

l g(xl, β). Then, max

|γl| = Op(m− 1

2 )o(m 1

2 ) = op(1).

l

Expanding (A.1),

0 = f (λ) =

=

1
n

1
n

(cid:88)

l

(cid:88)

l

l g(xl, β)(cid:2)1 − γl +
θ−1

γ2
l
1 + γl

(cid:3)

θ−1
l g(xl, β) −

1
n

(cid:88)

θ−1
l g(xl, β) · γ +

(cid:88)

θ−1
l g(xl, β)

1
n

l

.

l

1
n

(cid:88)

l

θ−1
l g(xl, β)

γ2
l
1 + γl

γ2
l
1 + γl

(A.2)

= E(θ−1

l g(xl, β)) − Sλ +

The last equality is since 1
n

(cid:80)

l θ−1

l g(xl, β) · γ = 1

n

(cid:80)

l θ−1

l g(xl, β)θ−1

l g(cid:48)(xl, β)λ = Sλ.

By substituting γl, we have the ﬁnal term of (A.2);

(cid:107) θ−1

l g(xl, β) (cid:107)3(cid:107) λ (cid:107)2 |1 + γl|−1 = op(m

1
n

(cid:88)

l

Therefore,

1

2 )Op(m−1)op(1) = op(m− 1

2 ).

0 = E(θ−1

l g(xl, β)) − Sλ + op(m− 1
2 )

⇒ λ = S−1E(θ−1

l g(xl, β)) + op(m− 1
2 )

⇒ λ =

(cid:104) 1
n

n
(cid:88)

l=1

θ−2
l g(xl, β)g(cid:48)(xl, β)

(cid:105)−1(cid:104) 1
n

n
(cid:88)

l=1

θ−1
l g(xl, β)

(cid:105)

+ op(m− 1

2 ).

(A.3)

Now, denote Vn(β) = 1
n

(cid:80)n

l=1 θ−2

l g(xl, β)g(cid:48)(xl, β), ¯g(β) = 1

n

(cid:80)n

l=1 θ−1

l g(xl, β), and ε =

(cid:15)kop(m− 1

2 ). So (A.2) can be rewritten as,

λ(β) = Vn(β)−1¯g(β) + ε.

Since γl = λ(cid:48)(β)θ−1

l g(xl, β), so (cid:80)n

l=1 |rl|3 = op(1).

Let am be any constant sequence such that am → ∞, and amm− 1

2 → 0. Denote the ball

18

B(β0, am) = {β| (cid:107) β − β0 (cid:107)≤ amm− 1

2 } and the surface of the ball ∂B(β0, am) = {β| (cid:107)

β − β0 (cid:107)= φamm− 1

2 , (cid:107) φ (cid:107)= 1}. For any β ∈ ∂B(β0, am), we have

Vn(β) =

1
n

n
(cid:88)

l=1

θ−2
l g(xl, β)g(cid:48)(xl, β)

=

=

n
k

1
k

k
(cid:88)

l=1

g1(xl, β0)g(cid:48)

1(xl, β0) +

n
n − k

1
n − k

n
(cid:88)

l=k+1

g2(xl, β0)g(cid:48)

2(xl, β0) + op((cid:15)−1
k )

n
k

Eg1(xl, β0)g(cid:48)

1(xl, β0) +

n
n − k

Eg2(xl, β0)g(cid:48)

2(xl, β0) + op((cid:15)−1
k )

(cid:2)Eg1(xl, β0)g(cid:48)

1(xl, β0) + Eg2(xl, β0)g(cid:48)

2(xl, β0)(cid:3) + op((cid:15)−1
k ),

≤ (cid:15)−1
k

and

¯g(β0) =

=

=

1
n

1
k

1
k

n
(cid:88)

l=1
k
(cid:88)

l=1

θ−1
l g(xl, β)

g1(xl, β0) +

1
n − k

n
(cid:88)

l=k+1

g2(xl, β0)

op(k

1
2 ) +

1
n − k

op((n − k)

1
2 )

= op(k− 1

2 ) + op((n − k)− 1
2 )

= op(m− 1

2 ).

By the Taylor expansion, for any β ∈ ∂B(β0, am), we have

lE(β) =

(cid:88)

l

λ(cid:48)(β)θ−1

l g(xl, β) −

1
2

(cid:88)

(cid:104)

l

λ(cid:48)(β)θ−1

l g(xl, β)

(cid:105)2

+ op(1).

(A.4)

The ﬁrst term of (A.4) is;

(cid:88)

l

λ(cid:48)(β)θ−1

l g(xl, β) =

(cid:104) 1
n

n
(cid:88)

l=1

θ−1
l g(xl, β)

θ−2
l g(xl, β)g(cid:48)(xl, β)

(cid:105)−1(cid:104) 1
n

θ−1
l g(xl, β)

(cid:105)

n
(cid:88)

l=1

(cid:105)(cid:48)(cid:104) 1
n

n
(cid:88)

l=1

19

+ op(1).

(A.4.1)

The second term of (A.4) is:

1
2

=

=

(cid:104) 1
n

=

(cid:88)

(cid:104)

λ(cid:48)(β)θ−1

l g(xl, β)

(cid:105)2

l

1
2

n
2

(cid:88)

λ(cid:48)(β)θ−2

l g(xl, β)g(cid:48)(xl, β)

l
(cid:104) 1
n

n
(cid:88)

l=1

θ−1
l g(xl, β)

(cid:105)(cid:48)(cid:104) 1
n

n
(cid:88)

l=1

θ−2
l g(xl, β)g(cid:48)(xl, β)

(cid:105)−1

n
(cid:88)

l=1
(cid:104) 1
n

n
2

n
(cid:88)

l=1

θ−2
l g(xl, β)g(cid:48)(xl, β)

(cid:105)(cid:104) 1
n

n
(cid:88)

l=1

θ−2
l g(xl, β)g(cid:48)(xl, β)

(cid:105)−1(cid:104) 1
n

n
(cid:88)

l=1

(cid:105)
θ−1
l g(xl, β)

+ op(1)

θ−1
l g(xl, β)

(cid:105)(cid:48)(cid:104) 1
n

n
(cid:88)

l=1

θ−2
l g(xl, β)g(cid:48)(xl, β)

(cid:105)−1(cid:104) 1
n

n
(cid:88)

l=1

(cid:105)
θ−1
l g(xl, β)

+ op(1).

(A.4.2)

Now,

(A.4.1) − (A.4.2)

=

n
2

(cid:16) 1
n

(cid:88)

l

θ−1
l g(xl, β)

(cid:17)(cid:48)(cid:16) 1
n

(cid:88)

l

θ−2
l g(xl, β)g(cid:48)(xl, β)

(cid:17)−1(cid:16) 1
n

(cid:88)

(cid:17)
θ−1
l g(xl, β)

+ op(1).

l

So we can rewrite (A.4) as,

lE(β) =

n
2

(cid:16) 1
n

(cid:88)

l

θ−1
l g(xl, β)

(cid:17)(cid:48)(cid:16) 1
n

(cid:88)

l

θ−2
l g(xl, β)g(cid:48)(xl, β)

(cid:17)−1(cid:16) 1
n

(cid:88)

l

θ−1
l g(xl, β)

(cid:17)

+ op(1)

n
2

n
2

=

=

(cid:40)

¯g(cid:48)(β)(Vn(β))−1¯g(β) + op(1)
(cid:40)

¯g(β0) +

1
n

(cid:88)

l

θ−1
l

∂g(xl, β0)
∂β(cid:48)

¯g(β0) +

1
n

(cid:88)

l

θ−1
l

∂g(xl, β0)
∂β(cid:48)

φamm− 1

2 + O

(cid:104)
(amm− 1

2 )2(cid:105)

(cid:41)(cid:48)

(cid:16)

×

(cid:17)−1

Vn(β)

×

φamm− 1

2 + O

(cid:104)
(amm− 1

(cid:41)
2 )2(cid:105)

+ op(1)

(By Taylor expansion of each term.)

20

(cid:40)

¯g(β0) +

1
n

(cid:88)

l

θ−1
l

∂g(xl, β0)
∂β(cid:48)

n(cid:15)k
2

≥

(cid:40)

¯g(β0) +

1
n

(cid:88)

l

θ−1
l

∂g(xl, β0)
∂β(cid:48)

φamm− 1

2 + O

φamm− 1

2 + O

(cid:41)(cid:48)

(cid:104)
(amm− 1

2 )2(cid:105)

(cid:16)

×

(cid:17)−1

Vn(β)

×

(cid:104)
(amm− 1

(cid:41)
2 )2(cid:105)

+ op(1).

As n → ∞, lE(β) → ∞.

Similarly,

lE(β0) =

n
2

¯g(cid:48)(β0)Vn(β0)−1¯g(β0) + op(1),

Vn(β0) =

n
k

Eg1(xl, β0)g(cid:48)

1(xl, β0) +

n
n − k

Eg2(xl, β0)g(cid:48)

2(xl, β0) + op((cid:15)−1

k ).

Thus, lE(β0) = Op(1) implies that for any β ∈ ∂B(β0, am), lE(β) can not arrive its

minimum value with the probability approaching to 1. Since lE(β) is a continuous func-

tion about β, as β ∈ B(β0, am), lE(β) has a minimum value in the interior of this ball

satisfying,

(cid:18)

∂λ(cid:48)(β)
∂β

0 =

∂lE(β)
∂β

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)β= ˜β

(cid:88)

=

l

(cid:19)

l g(xl, β) + θ−1
θ−1

l

(cid:18)

(cid:19)(cid:48)

λ(β)

∂g(xl,β)
∂β

1 + λ(cid:48)(β)θ−1

l g(xl, β)

θ−1
l g(xl, β)
1 + λ(cid:48)(β)θ−1

l g(xl, β)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)β= ˜β

(cid:88)

+

l

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)β= ˜β
(cid:18)

θ−1
l

∂g(xl,β)
∂β

(cid:19)(cid:48)

λ(β)

1 + λ(cid:48)(β)θ−1

l g(xl, β)

=

∂λ(cid:48)(β)
∂β

(cid:88)

l
(cid:18)

(cid:88)

=

l

(cid:19)(cid:48)

λ(β)

∂g(xl,β)
∂β

θ−1
l

1 + λ(cid:48)(β)θ−1

l g(xl, β)

(Since (cid:80)

θ−1
l g(xl,β)
1+λ(cid:48)(β)θ−1

l g(xl,β)

l

(cid:12)
(cid:12)
(cid:12)β= ˜β

= Q1n( ˜β, ˜λ) = 0)

= Q2n( ˜β, ˜λ).

Hence, Q1n( ˜β, ˜λ) = 0 and Q2n( ˜β, ˜λ) = 0. That is, (cid:107) ˜β − β0 (cid:107)= Op(amm− 1

2 ). But am is

arbitrary, hence (cid:107) ˜β − β0 (cid:107)= Op(m− 1

2 ).

21

Remark 1. From Lemma 3 of Airchison and Silvey (1957), the partitioned matrix








A B

B(cid:48)

0











A B

B(cid:48)

0



−1











=

P Q

Q(cid:48) R







is non-singular. Hence,

where

P = A−1 − A−1B(B(cid:48)A−1B)−1B(cid:48)A−1,

Q = A−1B(B(cid:48)A−1B)−1,

Q(cid:48) = (B(cid:48)A−1B)−1B(cid:48)A−1,

R = −(B(cid:48)A−1B)−1

Remark 2.







If







A B

C D

is a n × n symmetric positive deﬁnite matrix, and the partitioned matrices A ∈ Rm×m,

B ∈ Rm×n−m, and D ∈ R(n−m)×(n−m), then

1. the matrix (D − CA−1B) is symmetric and positive deﬁnite,

2.







A B

C D



−1











≥

Remark 3. β(cid:48) = ((β(cid:48), µ(cid:48)), δ(cid:48)).



.




0

A−1 0

0

∂Q1n(β, 0)
∂λ(cid:48)

= −

1
n

(cid:88)

l

θ−2
l g(xl, β)g(cid:48)(xl, β),

∂Q1n(β, 0)
∂β(cid:48)

=

1
n

θ−1
l

∂g(xl, β)
∂β(cid:48)

(cid:88)

l

∂Q2n(β, 0)
∂λ(cid:48)

=

1
n

(cid:18)

(cid:88)

l

θ−1
l

∂g(xl, β)
∂β(cid:48)

(cid:19)(cid:48)

,

∂Q2n(β, 0)
∂β(cid:48)

= 0

22







∂Q1n
∂λ(cid:48)

∂Q1n
∂β(cid:48)

∂Q2n
∂λ(cid:48)

0







−→







S11 S12

S21

0







= S(β) ≡ S

where

S11(β) = −θ−1

S12(β) = θ−1

l E

l E(cid:2)g1(xl, β)g(cid:48)
(cid:104) ∂g1(xl,β0)
∂β(cid:48)

1(xl, β)(cid:3) − (1 − θl)−1E(cid:2)g2(xl, β)g(cid:48)
(cid:104) ∂g2(xl,β0)
∂β(cid:48)

+ (1 − θl)−1E

(cid:105)
,

(cid:105)

2(xl, β)(cid:3),

S21(β) = S(cid:48)

12(β),

S12,i(β) = θ−1

l E

(cid:104) ∂g1(xl,β0)
∂β(cid:48)
i

(cid:105)

+ (1 − θl)−1E

(cid:104) ∂g2(xl,β0)
∂β(cid:48)
i

(cid:105)

, i = 1, 2.

By, Remark 1,

where

S−1 =







S11 S12

S21

0



−1











=







P Q

Q(cid:48) R

P = S−1

11 −S−1

11 S12(S21S−1

11 S12)−1S21S−1

11 = S−1

11 +S−1

11 S12ΣS21S−1

11 ; Σ = (S21(−S−1

11 )S12)−1,

Q = −S−1

11 S12(S21S−1

11 S12)−1 = −S−1

11 S12Σ,

Q(cid:48) = −ΣS21S−1

11 , R = −(S21S−1

11 S12)−1 = Σ.

Lemma 2. Under the conditions in Lemma 1 and H0, as n → ∞ we have

√

nΣ− 1

2 ( ˜β − β0) → N (0, I2p+q),

where Σ = [S21(−S11)−1S12]−1.

23

Proof. Expanding Q1n( ˜β, ˜λ) and Q2n( ˜β, ˜λ) at (θ0, 0), by the conditions of the H0 and

Lemma 1, we have,

0 = Q1n( ˜β, ˜λ)

= Q1n(β0, 0) +

∂Q1n(β0, 0)
∂β(cid:48)

( ˜β − β0) +

∂Q1n(β0, 0)
∂λ(cid:48)

(˜λ − 0) + Op(m−1),

0 = Q2n( ˜β, ˜λ)

= Q2n(β0, 0) +

∂Q2n(β0, 0)
∂β(cid:48)

( ˜β − β0) +

∂Q2n(β0, 0)
∂λ(cid:48)

(˜λ − 0) + Op(m−1),







−Q1n(β0, 0) + Op(m−1)

(cid:15)kOp(m−1)







=







∂Q1n
∂λ(cid:48)

∂Q1n
∂β(cid:48)

∂Q2n
∂λ(cid:48)

0



















.

˜λ

˜β − β0

By LLN,







˜λ

˜β − β0







−→ S−1(β0)







−Q1n(β0, 0) + Op(m−1)

(cid:15)kOp(m−1).

By Remark 1,





˜β − β0 = (0 I)S−1





−Q1n(β0, 0) + Op(m−1)

(cid:15)kOp(m−1)







.





Therefore,







˜λ

˜β − β0













−→

11 + S−1
S−1

11 S12 −S−1

11 S12Σ

−Q1n(β0, 0) + Op(m−1)















.





−ΣS21S−1
11

Σ

(cid:15)kOp(m−1)

˜β − β0 → −ΣS21S−1

11

(cid:18)

− Q1n(β0, 0) + Op(m−1)

(cid:19)

+ Σ(cid:15)kOp(m−1)

24

= (S21S−1

11 S12)−1S21S−1

11 Q1n(β0, 0) − ΣS21S−1
11 S12)−1S21(−S11)−1/2(−S11)−1/2√

(S21S−1

=

1
√
n

11 Op(m−1) + Σ(cid:15)kOp(m−1)

nQ1n(β0, 0) + (cid:15)kOp(m− 1
2 )

Since (−S11)−1/2√

nQ1n(β0, 0) → N (0, I2(p+q)),

√

nS21S−1

11 ( ˜β−β0) → N (0, I2p+q).

Lemma 3.

−2 log Λk = 2lE( ˜β0

1, 0) − 2lE( ˜β0

1, ˜β0
2),

where ˜β0

1 minimizes lE(β, 0) with respect to β1 under H0,

(−S11)−1/2√
(cid:104)

nQ1n(β0, 0)

(cid:105)(cid:48)

(−S11)−1/2√
(cid:104)
∆

(cid:105)
nQ1n(β0, 0)

+ Op(m− 1
2 )

−2 log Λk =

where

∆ = (−S11)−1/2(cid:110)

S12[S21(−S11)−1S12]−1S21−S12,1[S21,1(−S11)−1S12,1]−1S21,1

(cid:111)

(−S11)−1/2 ≥ 0.

Proof. Similar to Qin and Lawless (1994), we can derive,

lE( ˜β0

1, ˜β0

2) = −

n
2

Q(cid:48)

1n(β0, 0)BQ1n(β0, 0) + Op(m− 1

2 ),

where B = S−1

11 + S−1

11 S12ΣS21S−1

11 , and

lE( ˜β0

1, 0) = −

n
2

Q(cid:48)

1n(β0, 0)AQ1n(β0, 0) + Op(m− 1

2 ),

where A = S−1

11 + S−1

11 S12,1(S21,1S−1

11 S12,1)−1S21,1S−1

11 . Then,

(cid:104)
2

lE( ˜β0

1, 0) − lE( ˜β0

(cid:105)
1, ˜β0
2)

(cid:20)

=

− Q(cid:48)

1n(β0, 0)AQ1n(β0, 0) + Op(m− 1
2 )

(cid:21)

+

(cid:20)

nQ(cid:48)

1n(β0, 0)BQ1n(β0, 0) + Op(m− 1
2 )

(cid:21)

25

= nQ(cid:48)

1n(β0, 0)(B − A)Q1n(β0, 0) + Op(m− 1
2 )

= nQ(cid:48)

1n(β0, 0)S−1
11

(cid:104)
S12ΣS21 − S12,1Σ∗S12,2

(cid:105)

11 Q1n(β0, 0) + Op(m− 1
S−1
2 )

(B − A = S−1

11 + S−1

11 S12ΣS21S−1

11 − S−1

11 − S−1

11 S12,1(S21,1S−1

11 S12,1)−1S21,1S−1
11 .)

(−S11)−1/2√
(cid:104)

=

nQ1n(β0, 0)

(cid:105)(cid:48)

S12ΣS21 − S12,1Σ∗S12,2

(cid:105)

(−S11)−1/2(cid:104)

(−S11)−1/2√

nQ1n(β0, 0)

11 S12,1)−1)

(So, Σ∗ = (S21,1S−1
(−S11)−1/2(cid:104)
(cid:105)

+ Op(m− 1

2 ).

Take ∆ = (−S11)−1/2(cid:104)

S12ΣS21 − S12,1Σ∗S12,2

(cid:105)

(−S11)−1/2. Now,

∆ = (−S11)−1/2(cid:104)

S12

(cid:16)
S21(−S−1

11 )S12

(cid:17)−1

S21 − S12,1

(cid:16)

(cid:40)

= (−S11)−1/2(S12,1, S12,2)

[S21(−S11)−1S12]−1 −

(−S11)−1/2







S21,1

S21,2







×

≥ 0.

11 S12,1

S21,1S−1


(cid:16)





S21,1S−1

11 S12,1

0

(cid:17)−1

(cid:105)

S12,2

(−S11)−1/2

(cid:17)−1

(cid:41)


0



0

(By Remark 2)

Lemma 4. Under the conditions of Theorem 1 and the null hypothesis, denote Unk =
(cid:110) k

, for all δ > 0, we can ﬁnd C = C(δ), T = T (δ) and N = N (δ) such

(cid:111)

n : T

n ≤ (1 − T
n )

that

(cid:18)

P

max
k
n ∈Unk

(cid:16) m

(cid:17)1/2

log log m

(cid:107)

˜λ
(cid:15)k

(cid:19)

(cid:107)> C

≤ δ, P

(cid:18)

n−1/2 max
k
n ∈Unk

m (cid:107)

˜λ
(cid:15)k

(cid:19)

(cid:107)> C

≤ δ,

26

(cid:18)

P

max
k
n ∈Unk

(cid:16) m

(cid:17)1/2

log log m

(cid:107) ˜θ −θ0 (cid:107)> C

(cid:19)

≤ δ, P

(cid:18)

n−1/2 max
k
n ∈Unk

m (cid:107) ˜θ −θ0 (cid:107)> C

(cid:19)

≤ δ.

Proof. The proof is similar to Lemma 1.2.2 of Cs¨org´o and Horv´ath (1997).

Lemma 5. Under the conditions of Theorem 1 and H0, for all 0 ≤ α < 1

2 we have:

nα max
k
n ∈Unk

(cid:2)θk(1 − θk)(cid:3)α| − 2 log Λ − Rk| = Op(1),

(cid:2)θk(1 − θk)(cid:3)α| − 2 log Λ − Rk| = Op(n− 1

2 (log log n)

3
2 ),

max
k
n ∈Unk

where Θnk = {k : δ1 ≤ k ≤ n − δ2}

Proof of Theorem 1:

Proof. The proof of Theorem 1 is similar to the proof of Theorem 1.3.1 (Theorem A.3.4)

of Cs¨org´o and Horv´ath (1997) which derives the null distribution of the trimmed test

statistic.

Proof of Theorem 2:

Proof. The ELR test statistic is,

−2 log Λk = ZH0,k0 − ZH1,k0.

Under H1, ZH1,k0 also follows an asymptotic χ2 distribution. Therefore, ZH1,k0 = Op(1).

We only need to prove that P (ZH0,k0 > cn) → 1 for a positive constant c under H1. For

any ﬁxed ε, we can obtain

1
2n

ZH0,k0 = sup
λ

1
n

n
(cid:88)

l=1

(cid:104)
1 + θ−1

l λ(cid:48)g(xl, ε)

(cid:105)

log

27

= sup
λ1

1
n

k0(cid:88)

l=1

log

(cid:104)
1 + θ−1
k0

(cid:105)
λ(cid:48)
1g1(xl, ε)

+ sup
λ2

1
n

n
(cid:88)

l=k0+1

(cid:104)
1 + (1 − θk0)−1λ(cid:48)

(cid:105)
2g2(xl, ε)

log

a.s.−→ sup
λ1

θ0E log

(cid:16)

By Jensen’s inequality,

1 + θ−1

0 λ(cid:48)

1g1(xl, ε)

(cid:17)

(1 − θ0)E(cid:2) log

(cid:16)

+ sup
λ2

1 + (1 − θ0)−1λ(cid:48)

2g2(xl, ε)

(cid:17)(cid:3)

1 + θ−1

0 λ(cid:48)

1g1(xl, ε)

(cid:17)

(cid:20)

≤ log

E

(cid:16)

1 + θ−1

0 λ(cid:48)

1g1(xl, ε)

(cid:17)(cid:21)

= 0

(cid:16)

=⇒ sup
λ1

θ0E log

1 + θ−1

0 λ(cid:48)

1g1(xl, ε)

(cid:17)

= 0.

(cid:16)

E log

Thus,

1
2n

ZH0,k0

a.s.−→ sup
λ2

(1 − θ0)E(cid:2) log

(cid:16)

≤ (1 − θ0)c0

1 + (1 − θ0)−1λ(cid:48)

2g2(xl, ε)

(cid:17)(cid:3)

Hence, P (ZH0,k0 ≥ (1 − θ0c0) → 1. Thus, the proof.

Proof of Theorem 3:

Proof. To prove: For arbitrary small θ0

2 > η, | k0−k

n | ≥ η, −2 log Λk cannot arrive at its

maximum with probability approaching to 1.

Without loss of generality, suppose k < k0 and k0−k

n ≥ η. Then we have,

−2 log Λk0 − (−2 log Λk) = (ZH0,k0 − ZH1,k0) − (ZH0,k − ZH1,k).

Since ZH1,k0 = Op(1)

1
2n

(ZH0,k0 − ZH0,k + ZH1,k)

28

k0(cid:88)

(cid:104)

log

1 + θ−1
k0

(cid:105)
λ(cid:48)
1g1(xl, β0)

+ sup
λ2

1
n

(cid:104)
1 + θ−1

k λ(cid:48)

1g1(xl, β0)

(cid:105)

+ sup
λ2

(cid:104)
1 + θ−1

k λ(cid:48)

(cid:105)
1g1(xl, β0)

+ sup
λ2

1
n

1
n

n
(cid:88)

(cid:104)

log

1 + (1 − θk0)−1λ(cid:48)

(cid:105)
2g2(xl, β0)

l=k0+1
n
(cid:88)

log

(cid:104)
1 + (1 − θk)−1λ(cid:48)

2g2(xl, β0)

(cid:105)

l=k+1

n
(cid:88)

l=k+1

(cid:104)
1 + (1 − θk)−1λ(cid:48)

(cid:105)
2g2(xl, β1)

log

(cid:104)

log

1 + (1 − θk0)−1λ(cid:48)

(cid:105)
2g2(xl, β0)

− sup
λ2

1
n

n
(cid:88)

l=k+1

log

(cid:104)
1 + (1 − θk)−1λ(cid:48)

2g2(xl, β0)

(cid:105)

(cid:104)
1 + (1 − θk0)−1λ(cid:48)

(cid:105)
2g2(xl, β0)

log

l=1

k
(cid:88)

l=1

k
(cid:88)

log

log

l=1
n
(cid:88)

l=k0+1
n
(cid:88)

= sup
λ1

1
n

− sup
λ1

+ sup
λ1

1
n

1
n

≥ sup
λ2

= sup
λ2

1
n

1
n

− sup
λ2

= sup
λ2

− sup
λ2

1
n

1
n

1
n

l=k0+1
n
(cid:88)

log

(cid:104)

1 + ρk(1 − θk0)−1λ(cid:48)

2g2(xl, β0)

(cid:105)

(ρk = n−k0
n−k )

l=k+1
n
(cid:88)

(cid:104)
1 + (1 − θk0)−1λ(cid:48)

(cid:105)
2g2(xl, β0)

log

l=k0+1
n
(cid:88)

log

(cid:104)

l=k+1

1 + (1 − θk0)−1λ(cid:48)

(cid:105)
2g2(xl, β0)

(Since, n−k0

n ≤ ρk ≤ n−k0

n−k0+nη . So, 1 − θ0 ≤ lim ρk ≤ lim ρk ≤ 1−θ0
(cid:17)(cid:3)

1−θ0+η )

1 + (1 − θ0)−1λ(cid:48)

2g2(xl, β0)

(1 − θ0)E(cid:2) log

(cid:16)

a.s.−→ sup
λ1
(cid:8) k0 − k
n

− sup
λ1

(cid:16)

E(cid:2) log

1 + (1 − θ0)−1λ(cid:48)

2g2(xl, β0)

(cid:17)(cid:3)

+ (1 − θ0)E(cid:2) log

(cid:16)

1 + (1 − θ0)−1λ(cid:48)

2g2(xl, β0)

(cid:17)(cid:3)(cid:9)

(1 − θ0)E(cid:2) log

≥ sup
λ1

(cid:16)

1 + (1 − θ0)−1λ(cid:48)

2g2(xl, β0)

(cid:17)(cid:3)

(cid:8)ηE(cid:2) log

− sup
λ1

(cid:16)

1 + (1 − θ0)−1λ(cid:48)

2g2(xl, β0)

(cid:17)(cid:3)

+ (1 − θ0)E(cid:2) log

(cid:16)

1 + (1 − θ0)−1λ(cid:48)

2g2(xl, β0)

(cid:17)(cid:3)(cid:9)

(By Jensen’s inequality and k0−k

n ≥ η.)

29

Assume that sup
λ1

θ0)−1λ(cid:48)

2g2(xl, β0)

(cid:16)

(cid:8)ηE(cid:2) log
1 + (1 − θ0)−1λ(cid:48)
(cid:17)(cid:3)(cid:9) attains its maximum at δ∗

2. Then we have,

2g2(xl, β0)

(cid:17)(cid:3) + (1 − θ0)E(cid:2) log

(cid:16)

1 + (1 −

1
2n

(ZH0,k0 − ZH0,k + ZH1,k)




sup
λ1
−ηE(cid:2) log

≥

(cid:16)

(1 − θ0)E(cid:2) log

(cid:16)

1 + (1 − θ0)−1λ(cid:48)

2g2(xl, β0)

(cid:17)(cid:3), if δ∗

2 = 0,

1 + (1 − θ0)−1λ∗(cid:48)

2 g2(xl, β0)

(cid:17)(cid:3), if δ∗

2 (cid:54)= 0.

Therefore, by the condition that for every ﬁxed parameter δ = β∗ − β (cid:54)= 0, there exists

a positive constant c0 > 0 satisfy that ∞ > inf
δ(cid:54)=0

sup
λ

E log (cid:2)1 + λ(cid:48)x(x(cid:48)δ + e)(cid:3) ≥ c0 > 0

and Jensen’s inequality, there exists a constant c0 > 0, such that P (cid:0) 1

2n(ZH0,k0 − ZH0,k +
(cid:1) → 1 as n → ∞. Thus, we have, P (cid:2)(cid:0)−2 log Λk0 −(−2 log Λk)(cid:1) > cn(cid:3) → 1,

ZH1,k) > c0

since ZH1,k0 = Op(1). So, −2 log Λk cannot arrive at its maximum with probability

approaching to 1. By the deﬁnition of ˆk, we have | k0−ˆk

n | ≤ η with probability approaching

to 1. Since η is arbitrary, thus the proof.

References

AUE, A. AND HORV ´ARTH, L. (2012). Structural breaks in time series. Journal of Time

Series Analysis 34, 1-16.

30

BAI, J. S. & PERRON, P. (1998). Estimating and testing linear models with multiple

structural changes. Econometrica 66, 47-78.

BALCOMBE, K., BAILEY, A. & BROOKS, J. (2007). Threshold effects in price trans-

missions: the case of Brazilian wheat, maize and soya prices. American Journal of

Agricultural Economics 89, 308-323.

BARAGONA, R., BATTAGLIA, F. & CUCINA, D. (2013). Empirical likelihood for break

detection in time series. Electronic Journal of Statistics 7, 3089-3123.

BERKES, I., HORV ´ATH, L., LING, S. & SCHAUER, J. (2011). Testing for structural

change of AR model to threshold AR model. Journal of Time Series Analysis 32(5),

547-565.

CHEN, J. & GUPTA, A. K. (2000). Parametric Statistical Change Point Analysis. Boston:

Birkh¨auser.

CHERNOFF, H. & ZACKS, S. (1964). Estimating the current mean of a normal distribu-

tion which is subjected to changes in time. The Annals of Mathematical Statistics 35,

999-1018.

CS ¨ORGO , M. & HORV ´ATH, L. (1997). Limit Theorems in Change-Point Analysis. New

York: Wiley & Sons.

HAWKINS, D. M. (1977). Testing a sequence of observations for a shift in location.

Journal of the American Statistical Association 72, 180-186.

31

KOLACZYK, E. D. (1994). Empirical likelihood for generalized linear models. Statistica

Sinica 4, 199-218.

LEE, S., HA, J., NA, O. AND NA, S. (2003). The Cusum test for parameter change in

time series models. Scandinavian Journal of Statistics 30, 781-796.

LEE, S., NA, O., NA, S. (2003). On the cusum of squares test for variance change in non-

stationary and nonparametric time series models. Annals of the Institute of Statistical

Mathematics 55, 467-485.

LIU, Y., ZOU, C. & Zhang, R. (2008). Empirical likelihood ratio test for a change-point

in linear regression model. Communication in Statistics-Theory and Methods 37, 1-13.

NING, W. (2012). Empirical likelihood ratio test for a mean change point model with a

linear trend followed by an abrupt change. Journal of Applied Statistics 39(5), 947-961.

NORDMAN, D. J. & LAHIRI, S. N. (2014). A review of empirical likelihood methods

for time series. Journal of Statistical Planning and Inference 155, 1-18.

OGATA, H. (2005). Empirical Llikelihood for non Gaussian stationary processes. Scien-

tiae Mathematicae Japonicae Online e-2005, 465-473.

OWEN, A.B. (1988). Empirical likelihood ratio conﬁdence Intervals for a single func-

tional. Biometrika 75, 237-49.

32

OWEN, A.B. (1990). Empirical likelihood ratio conﬁdence regions.

The Annals of

Statistics 18, 90-120.

OWEN, A.B. (1991). Empirical likelihood for linear models. The Annals of Statistics 19,

1725-47.

OWEN, A.B. (2001). Empirical Likelihood. New York: Chapman & Hall/CRC.

PAGE, E.S. (1954). Continuous inspection schemes. Biometrika 41, 100-116.

PAGE, E.S. (1955). A test for a change in a parameter occurring at an unknown point

Biometrika 42, 523-527.

PERRON, P. & VOGELSANG, T. J. (1992). Testing for a unit root in a time series with a

changing mean: corrections and extensions. Journal of Business & Economic Statistics

10(4), 467-470.

QIN, J. & LAWLESS, J. (1994). Empirical likelihood and general estimating equations.

The Annals of Statistics 22, 300-325.

SEN, A. K. & SRIVASTAVA, M.S. (1975). On tests for detecting changes in mean. The

Annals of Statistics 3, 98-108.

VOSTRIKOVA, L.J. (1981). Detecting “disorder” in multidimenstional random processes.

Soviet Mathematics Doklady 24, 55-59.

33

WORSLEY, K.J. (1986). Conﬁdence regions and test for a change point in a sequence of

exponential family random variables. Biometrika 73, 91-104.

ZOU, C.L., LIU, Y.K., QIN, P. AND WANG, Z. (2007) Empirical likelihood ratio test

for the change-point problem. Statistics & Probability Letters 77 , 374-382.

34

