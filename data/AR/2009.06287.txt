Draft version September 15, 2020
Typeset using LATEX twocolumn style in AASTeX62

0
2
0
2

p
e
S
4
1

]

R
S
.
h
p
-
o
r
t
s
a
[

1
v
7
8
2
6
0
.
9
0
0
2
:
v
i
X
r
a

Application and interpretation of deep learning for identifying pre-emergence
magnetic-ﬁeld patterns
Dattaraj B. Dhuri,1 Shravan M. Hanasoge,1 Aaron C. Birch,2 and Hannah Schunker2, 3

1Department of Astronomy and Astrophysics, Tata Institute of Fundamental Research, Mumbai, India 400005
2Max-Planck-Institut f¨ur Sonnensystemforschung, G¨ottingen, Germany
3School of Mathematical and Physical Sciences, The University of Newcastle, New South Wales, Australia

ABSTRACT

Magnetic ﬂux generated within the solar interior emerges to the surface, forming active regions (ARs)
and sunspots. Flux emergence may trigger explosive events - such as ﬂares and coronal mass ejections
and therefore understanding emergence is useful for space-weather forecasting. Evidence of any pre-
emergence signatures will also shed light on sub-surface processes responsible for emergence. In this
paper, we present a ﬁrst analysis of emerging ARs from the Solar Dynamics Observatory/Helioseismic
Emerging Active Regions (SDO/HEAR) dataset (Schunker et al. 2016) using deep convolutional neural
networks (CNN) to characterize pre-emergence surface magnetic-ﬁeld properties. The trained CNN
classiﬁes between pre-emergence (PE) line-of-sight magnetograms and a control set of non-emergence
(NE) magnetograms with a True Skill Statistic (TSS) score of ∼ 85%, 3 h prior to emergence and
∼ 40%, 24 h prior to emergence. Our results are better than a baseline classiﬁcation TSS obtained using
discriminant analysis of only the unsigned magnetic ﬂux. We develop a network pruning algorithm to
interpret the trained CNN and show that the CNN incorporates ﬁlters that respond positively as well
as negatively to the unsigned magnetic ﬂux of the magnetograms. Using synthetic magnetograms, we
demonstrate that the CNN output is sensitive to the length-scale of the magnetic regions with small-
scale and intense ﬁelds producing maximum CNN output and possibly a characteristic pre-emergence
pattern. Given increasing popularity of deep learning, techniques developed here for interpretation of
the trained CNN — using network pruning and synthetic data — are relevant for future applications
in solar and astrophysical data analysis.

Keywords: Sun: magnetic ﬁelds — methods: data analysis — methods: miscellaneous — methods:

statistical

1. INTRODUCTION

Magnetic ﬂux in the Sun, generated by the dynamo
operating within the interior, rises to the visible sur-
face, the photosphere, and further into the solar atmo-
sphere (Cheung & Isobe 2014; Stein 2012). The rising
ﬂux appears at the photosphere in the form of large-
scale (∼ 10 − 100 Mm) structures of concentrated mag-
netic ﬂux (∼ kG), known as sunspots and active regions
(ARs). In the low-β solar atmosphere and corona, these
ﬂux tubes expand signiﬁcantly and form gigantic loop
structures rooted in the ARs. The rising magnetic ﬂux
also pumps into ARs non-potential energy that powers

Corresponding author: Dattaraj B. Dhuri
dattaraj.dhuri@tifr.res.in

explosive events such as ﬂares and coronal mass ejections
(CMEs) that can lead to severe space weather conse-
quences (Shibata & Magara 2011; Eastwood et al. 2017).
Knowledge of the ﬂux emergence mechanism in ARs is
thus useful for gaining warning time for space-weather
forecasting. Additionally, a comprehensive picture of
the solar dynamo includes processes responsible for gen-
eration as well as transport of the magnetic ﬂux (Cheung
& Isobe 2014). Understanding the formation and evolu-
tion of ARs is therefore necessary for constraining solar
dynamo models (Schunker et al. 2016; Birch et al. 2012;
Cameron et al. 2016).

The physical mechanism that leads to the appearance
of new ARs on the photosphere, henceforth referred to
as “emergence”, are not fully known (Leka et al. 2012;
Schunker et al. 2016; Cheung & Isobe 2014). Broadly,
two categories of emergence scenarios are considered in

 
 
 
 
 
 
2

the literature. In the ﬁrst scenario, ﬂux tubes may form
deep in the convection zone and rise intact to emerge
on the surface (Fan 2009). Alternatively, small-scale
ﬂux tubes, which may be generated within the bulk of
the convection zone or in the near-surface layers, coag-
ulate and form ARs on the surface (Brandenburg 2005;
Brandenburg et al. 2014). Case-studies for detecting
sub-surface pre-emergence signatures, e.g. using helio-
seismology (Birch et al. 2010), may not be successful
because of the signal being weak in comparison to the
uncertainties in detection. Therefore, for unambiguous
detection of pre-emergence signatures, a statistical study
of a large sample of emerging ARs is in order. A sta-
tistical study, such as Komm et al. (2009, 2011), is ex-
pected to capture common characteristics of emerging
ARs which may be universal to the emergence mecha-
nism (Schunker et al. 2016).

Leka et al. (2012); Birch et al. (2012); Barnes et al.
(2014) carried out an extensive survey of emerging ARs,
henceforth referred to as the LBB survey, using helio-
seismic holography. Analyzing SOHO/MDI line-of-sight
magnetograms and Global Oscillation Network Group
(GONG) dopplergrams, the LBB survey detected weak
converging ﬂows and reduced travel-times to distinguish
emerging active regions (EARs) from a set of control re-
gions (CRs) which did not show emergence. The study
also identiﬁed pre-emergence magnetic ﬁeld signal cor-
related with converging ﬂows in the EAR population.
Although a causal relationship between the converging
ﬂow and the pre-emergence magnetic ﬁeld signal could
not be established, the average unsigned radial mag-
netic ﬁeld of EARs was demonstrated to be the leading
discriminator between the EAR and CR populations,
yielding True Skill Statistics (Peirce Skill Score) (Peirce
1884; Bobra & Couvidat 2015) of ∼ 50%, 3 h prior to
the emergence. This warrants further investigation of
pre-emergence surface magnetic-ﬁeld characteristics.

Motivated by the results of the LBB survey, here
we perform comprehensive statistical analysis to at-
tempt to uncover spatio-temporal patterns associated
with pre-emergence surface magnetic ﬁelds.
Surface
magnetic-ﬁeld data of superior spatial and temporal
resolution is available from Helioseismic and Magnetic
Imager (HMI) (Schou et al. 2012) on-board Solar dy-
namics Observatory (SDO) (Pesnell et al. 2012). Fol-
lowing the LBB survey, Schunker et al. (2016) assembled
the Helioseismic Emerging Active Region (SDO/HEAR)
dataset, also comprising pre-emergence magnetograms
of emerging ARs (EARs) and control regions (CRs).
Although originally designed for measuring sub-surface
pre-emergence activity,
the SDO/HEAR dataset is
also useful to study surface magnetic-ﬁeld properties

of emerging ARs (Schunker et al. 2016, 2019). The
SDO/HEAR dataset comprises some ∼ 200 samples
of EARs and CRs each, with 60◦ × 60◦ spatial extent
and tracked over a duration of up to one week before
emergence. Detecting spatio-temporal correlations and
patterns over such a large dataset of magnetograms is
statistically challenging. With the advent of machine
learning (ML), however, advanced algorithms to analyze
and classify images to detect complex correlations are
available (Hastie et al. 2001). In particular, deep convo-
lutional neural networks (CNNs) have proven immensely
successful for visual recognition tasks, such as image and
video classiﬁcation, by accurately characterising struc-
tural information from the images (Goodfellow et al.
2016; Krizhevsky et al. 2012; LeCun et al. 2015). Here,
we use CNNs to discriminate between magnetograms
from EAR and CR populations and in this process, un-
cover associated pre-emergence surface magnetic-ﬁeld
characteristics.

The classiﬁcation problem considered here can be nat-
urally formulated as a supervised learning (Hastie et al.
2001) problem, where the CNN is trained using the
dataset of magnetograms labeled as pre-emergence (PE)
and non-emergence (NE). The CNN is trained to opti-
mize the True Skill Statistics (TSS) score for classiﬁ-
cation of PE and NE magnetograms taken at diﬀerent
pre-emergence times. CNNs, and deep neural networks
in general, may be trained for challenging tasks. How-
ever, it is diﬃcult to interrogate the operation of neural
networks into comprehensible components and thus in-
terpret their performance. For the present work, the
interpretation of the trained network is important for
obtaining quantiﬁable information about pre-emergence
surface-magnetic-ﬁeld characteristics. Our analysis in
this work therefore focuses on understanding the perfor-
mance of the network using synthetic magnetograms and
advanced techniques such as functionality based network
pruning (Cun et al. 1990; Han et al. 2015; Qin et al. 2018;
Frankle et al. 2019).

The paper is organized as follows.

In Section 2,
we describe the processing of EARs and CRs in the
SDO/HEAR dataset for deep learning analysis using a
CNN. In Section 3, we explain the CNN architecture
and training methodology. In Section 4, we describe in
detail the results of the classiﬁcation of pre-emergence
(PE) and non-emergence (NE) magnetograms and com-
pare with earlier work using discriminant analysis. In
particular, we explain the working of the CNN using
the network-pruning algorithm developed here to facil-
itate the CNN interpretation. We detail various statis-
tical analyses performed to comprehend the CNN per-
formance. We also explain probing of the trained CNN

using synthetic magnetograms. In Section 5, we sum-
marize our ﬁndings.

2. DATA

We use the SDO/HEAR dataset comprising pre-
emergence line-of-sight magnetograms of active regions
observed by SDO/HMI between May 2010 and July 2014
(Schunker et al. 2016). EARs in the dataset are selected
to be emergences into relatively quiet Sun to minimize
diﬃculty in distinguishing signatures of emerging ﬂux
from any pre-existing magnetic ﬁeld. Each EAR is
paired with a control region (CR) which does not show
emergence. Each CR is co-spatial with the correspond-
ing EAR, in terms of the latitude and distance from
the central meridian, and is separated by no more than
two solar rotation periods. The SDO/HEAR dataset,
although originally prepared for a helioseismology sur-
vey, is also useful for studying pre-emergence surface
magnetic-ﬁeld properties (Schunker et al. 2016). The
dataset provides Postel-projected EAR and CR maps
covering 60◦ × 60◦ about the emergence location. How-
ever, following the LBB survey, we restrict our analysis
to the central 30◦ × 30◦ region. This region is pro-
jected onto a 256 × 256-pixel grid with a plate scale of
1.4 Mm per pixel. In the following, we explain in detail
the problem at hand to classify between 10◦ × 10◦ pre-
emergence (PE) and non-emergence (NE) magnetogram

3

segments sub-sampled from the original 30◦ × 30◦ mag-
netograms of EARs and CRs in the SDO/HEAR dataset
(see Figure 1). We use the following deﬁnitions of the
magnetograms referred to in this work (see Table 1).

• EARs - Emerging active regions of size 30◦ × 30◦

taken from the SDO/HEAR dataset.

• CRs - Control regions of spatial extent 30◦ ×
taken from the

30◦ not showing emergence,
SDO/HEAR dataset.

• PEs - Pre-emergence magnetograms of spatial ex-
tent 10◦×10◦ randomly sub-sampled from the cen-
tral regions of EARs, as shown in Figure 1 (red).
• NEs - Non-emergence magnetograms of spatial ex-
tent 10◦ × 10◦ randomly sub-sampled from CRs
and non-central regions of EARs, as shown in Fig-
ure 1 (blue).

s

The SDO/HEAR dataset is compiled following sev-
eral criteria to facilitate characterization of EARs and
CRs using statistical analysis (Schunker et al. 2016).
Only those EARs are selected which (1) are visible
in the continuum, (2) reach total area of at least
10 µH or 30 Mm2, and (3) are within ±50◦ of the cen-
tral meridian at the time of emergence tNOAA
, as deﬁned
by National Oceanic and Atmospheric Administration
(NOAA). Consistent with the LBB survey, the emer-
gence time t0 of an EAR is deﬁned as the time when
the absolute ﬂux of a EAR is 10% of the maximum
observed value within a 36 h interval following tNOAA
.
Moreover, EARs are selected only when the absolute
ﬂux rises monotonically after the emergence from a low,
steady value. Following these restrictions, 182 EAR
samples between May 2010 and July 2014 are included
in the SDO/HEAR dataset. CRs in the dataset are se-
lected to match the spatial distribution of EARs. This
is achieved by selecting a CR co-spatial with each EAR
and also from the identical phase of the solar cycle. The

s

Figure 1. Sub-sampling pre-emergence (PE, red) and non-
emergence (NE, blue) magnetogram segments from the EARs
(left) and CRs (right) in the SDO/HEAR dataset (Schunker
et al. 2016). The 10◦ × 10◦ segments are randomly sub-
sampled from the original 30◦ × 30◦ EARs and CRs. Sub-
sample locations are randomly selected for each EAR/CR,
thus they are diﬀerent for diﬀerent EARs and CRs. We con-
strain the sub-sampling to ensure that the emergence region
(the central 5◦×5◦ of EARs) is fully included (fully excluded)
within PE (NE) magnetogram segments. We sub-sample NE
magnetogram segments from CRs as well as non-central re-
gion of EARs. PE and NE segments are Postel projections of
size 85 × 85 pixels, with a plate scale of 1.4 Mm per pixel, i.e.
same as the original EARs/CRs. We label PE and NE seg-
ments as 1 and 0 respectively for supervised learning using
the deep neural network.

time before EARs CRs
emergence
(h)

30◦ × 30◦
(original)

PEs

NEs

10◦ × 10◦
(sub-sampled)

-3.2
-8.5
-13.9
-19.2
-24.5

178
174
173
172
158

180
165
169
167
158

17800
17400
17300
17200
15800

18080
17115
17269
17117
15958

Table 1. The SDO/HMI survey of emerging active regions
(SDO/HEAR) dataset: We consider emerging active regions
(EARs) and control regions (CRs) between May 2010 and
July 2014. We sub-sample pre-emergence (PE) and non-
emergence (NE) segments from original EARs and CRs to
furnish suﬃcient data for training a deep neural network.

4

Figure 2. Example emerging active region (EAR, top) and control region (CR, bottom) from the SDO/HEAR dataset (Schunker
et al. 2016). Temporally averaged pre-emergence Postel-projected magnetograms for EAR 11776 and corresponding CR are
shown. The magnetograms are spread over 30◦ × 30◦ about the emergence location and projected onto a 256 × 256-pixel grid
with a plate scale of 1.4 Mm per pixel. The emergence location is conﬁned to the central 10◦ × 10◦ region. The bipolar signature
of the EAR makes a clear appearance as the emergence time approaches. Pre-emergence surface magnetic ﬁelds of EARs and
CRs are otherwise diﬃcult to distinguish. The color scale is saturated at ± 100 G.

Figure 3. Average emerging active region (EAR, top) and control region (CR, bottom) from the SDO/HEAR dataset (Schunker
et al. 2016). The magnetograms from the dataset at each pre-emergence time are averaged after accounting for Hale’s and Joy’s
laws. Patterns in the pre-emergence bipolar surface magnetic ﬁeld may be seen distinctly in EARs. The deep convolutional
neural network (CNN) may learn this faint bipolar ﬁeld signature and correlated patterns. The color scale is saturated at
± 15 G.

absolute diﬀerence between the average pre-emergence
magnetic ﬁeld within the central 10◦ × 10◦ of a CR and
associated EAR is restricted to be less than 10 G or
1.4 × 1021 Mx (see Figure 3 in Schunker et al. (2016)).
Importantly, the CRs do not show marked increase in
the absolute ﬂux similar to emergence. CRs are as-
signed a mock-emergence time when their Stonyhurst
coordinates (Thompson, W. T. 2006) are identical to
the corresponding EARs, at the time of emergence t0.

The EARs and CRs are tracked up to a consecu-
tive two-week duration, up to one week on either side
of emergence, at the Carrington rotation rate. The
SDO/HMI line-of-sight magnetograms are available at
a cadence of 45 s. The magnetograms are temporally

averaged over intervals of 410.25 min (547 frames) and
spaced by 320.25 min (427 frames) to smooth out tempo-
ral ﬂuctuations. Thus, there is an overlap of 90 min (120
frames) between consecutive temporal-averaging inter-
vals (see Figure 5 in Schunker et al. 2016). Consistent
with the LBB survey, we only consider pre-emergence
temporally averaged magnetograms up to a day prior to
emergence. The number of EAR and CR samples con-
sidered in our analyses, at diﬀerent pre-emergence times,
are listed in Table 1. We include only those EARs and
CRs for which magnetogram samples are available at all
pre-emergence times and therefore the number of EARs
and CRs is not exactly equal.

-1000100x (Mm)-1000100y (Mm)-24.5 h-1000100x (Mm)-1000100y (Mm)-1000100x (Mm)-19.2 h-1000100x (Mm)-1000100x (Mm)-13.9 h-1000100x (Mm)-1000100x (Mm)-8.5 h-1000100x (Mm)-1000100x (Mm)-3.2 h-1000100x (Mm)-1000100x (Mm)2.1 h-1000100x (Mm)B (G)1007550250255075100-1000100x (Mm)-1000100y (Mm)-24.5 h-1000100x (Mm)-1000100y (Mm)-1000100x (Mm)-19.2 h-1000100x (Mm)-1000100x (Mm)-13.9 h-1000100x (Mm)-1000100x (Mm)-8.5 h-1000100x (Mm)-1000100x (Mm)-3.2 h-1000100x (Mm)-1000100x (Mm)2.1 h-1000100x (Mm)B (G)15105051015Figure 2 shows time-averaged pre-emergence magne-
tograms of an example EAR and corresponding CR. As
the emergence time approaches, we see the early ap-
pearance of a bipolar ﬁeld within the central 10◦ × 10◦
of EARs. Apart from that, we see no obvious diﬀerence
between EARs and CRs prior to emergence. The sig-
nature of the pre-emergence bipolar ﬁeld becomes ev-
ident when we consider the average of all EARs and
CRs in the dataset, shown in Figure 3, after accounting
for Hale’s polarity law and Joys’s law (Schunker et al.
2016). To account for Hale’s law, we reverse the sign of
the magnetic ﬁeld for the magnetograms in the southern
hemisphere. To account for Joy’s law, we ﬂip the mag-
netograms in the southern hemisphere in the latitudinal
direction, i.e +y direction points away from the equator.
For the average EAR, the pre-emergence ﬁeld within the
central 10◦ × 10◦ region may be distinctly seen, even up
to a day prior to the emergence. As mentioned earlier,
this pre-emergence magnetic ﬁeld was a leading factor in
the discriminant analysis performed in the LBB survey
to diﬀerentiate the EAR and CR samples. The present
analysis, using deep learning, is expected to identify this
pre-emergence ﬁeld as well as other correlated patterns.
As in the case of the LBB survey, the SDO/HEAR
dataset used in this analysis is speciﬁcally designed for
examination of pre-emergence signatures using helio-
seismology and surface magnetic-ﬁeld properties. The
EARs and CRs are selected with a bias of prior knowl-
edge of where emergence occurred. Therefore, our work
is not an attempt of “forecasting” emergence. Rather,
the focus is on characterizing EARs and CRs based on
their surface magnetic-ﬁeld properties and understand-
ing the process of emergence. The SDO/HEAR dataset
is well suited for studying spatio-temporal character-
istics of the pre-emergence surface magnetic ﬁeld, as
demonstrated in Schunker et al. (2016).

A major obstacle for our analysis is that the num-
ber of EAR samples is indeed small in the SDO/HEAR
dataset for training a deep neural network. It is diﬃcult
to know a priori the exact number of samples required
for successfully training a deep neural network. The
number of parameters in such a network are O(100, 000)
- many of these may be redundant, but for success-
ful training, thousands of examples from each category
(class) may be necessary (Goodfellow et al. 2016). In or-
der to have suﬃcient number of examples for training,
we consider pre-emergence magnetograms of smaller size
in our analysis. These are sub-sampled from the orig-
inal 30◦ × 30◦ Postel-projected maps. By design, the
emergence is restricted to the central 10◦ × 10◦ area
of EARs in the SDO/HEAR dataset. Thus, we obtain
magnetogram segments of 10◦ × 10◦ selected randomly

5

from the original EAR and CR magnetograms. Because
of the random selection the sub-sampled segments are
diﬀerent geometric regions in diﬀerent EARs and CRs.
We constrain the random selection to ensure that the
sub-sampled 10◦ × 10◦ segments either fully contain or
fully exclude the emergence location, i.e., segments that
contain the emergence location even partially are dis-
allowed (see Figure 1). However, there is no minimum
distance between two random PE or NE from an EAR
or CR i.e. there may be signiﬁcant overlap. Note that
the NE segments are sub-sampled from the CRs as well
as from the non-central region of the EARs. Also, the
NE segments in the training set could include regions
where ﬂux does emerge, that were not classiﬁed as PE
regions in the SDO/HEAR survey. This could increase
the chances of the machine learning algorithm return-
ing false negatives. The PE and NE segments are now
Postel-projected maps on 85 × 85 pixel grids. The plate
scale of the magnetogram segments is 1.4 Mm per pixel,
i.e. similar to EARs and CRs. We subsample exactly
100 PEs from the central part of each EAR and approx-
imately 50 NEs each from the non-central part of each
EAR and from each CR. Table 1 lists the total number
of PE and NE segments used for analysis. We train a
deep neural network, a machine learning algorithm, to
classify between these PE and NE segments labeled as
class 1 and class 0 respectively.

3. METHODS

Machine learning is a set of algorithms applied to ana-
lyze patterns and correlations in large, high-dimensional
datasets without being explicitly programmed for it.
Here, we apply supervised learning, where the machine
is trained using a known set of input-output pairs from
a part of the available data (Hastie et al. 2001). PE
and NE magnetograms, at a given pre-emergence time
t, are inputs and class labels 1 and 0 respectively are the
desired outputs (see Figure 4). The machine’s parame-
ters, i.e. weights and biases (see Appendix A), are tuned
during the training process when it learns the probabil-
ity distribution of the inputs. The trained machine is
used to make predictions on the remaining part of the
data which is called validation or test data. We use deep

Figure 4. Schematic for deep learning classiﬁcation of pre-
emergence (PE) and non-emergence (NE) magnetogram seg-
ments.

6

Figure 5. Operation of a 3 × 3 convolutional ﬁlter with
the identity activation function. For illustration purposes,
a 4 × 4 input is considered. Zero padding is applied to the
input to extract the output of the convolution operation of
the same size as that of the input. The 3×3 convolution ﬁlter
consists of nine neurons, i.e. nine weights (see Appendix A),
as shown. The ﬁlter slides over each 3 × 3-pixel sub-region
from the input in order. Assuming that the bias b of the ﬁlter
is 0, the identity activation of the neurons yields the output
(cid:80) wixi for each sub-region. The outputs for the sub-regions
shaded in orange and blue color are respectively highlighted.

convolutional neural networks (CNN) as our method of
choice for the classiﬁcation of PE and NE magnetograms
(Krizhevsky et al. 2012; LeCun et al. 2015; Goodfellow
et al. 2016). CNNs have been used extensively for anal-
ysis of images and proven widely successful.

The CNN consists of layers of neurons with 2D con-
volutional ﬁlters, particularly designed for analyzing im-
ages. A convolutional ﬁlter of K ×K neurons slides over
input magnetograms (Figure 5), transforming a K × K-
pixel sub-region at a time according to the neuron acti-
vation function (Eq. A1). Unlike typical image classiﬁ-
cation problems, the magnetograms contain inputs with
both positive and negative values representing opposite
magnetic polarities. Therefore, we use the identity acti-
vation function for CNN ﬁlters, i.e., every neuron in the
CNN ﬁlter outputs y = (cid:80)
i wixi + b, where x are inputs
to the neuron with weights w and b is the bias of the
ﬁlter (Appendix A). The identity activation function en-
sures that positive and negative (magnetic-ﬁeld) values
from the input are treated equally by the network. In
addition to the convolutional layers, max-pooling lay-
ers are also used in CNNs. A max-pooling layer of size
M × M extracts the maximum value from an M × M -
pixel sub-region, thus downsampling the input by a fac-
tor of M . A combination of convolutional and pooling
layers, placed at increasingly deeper levels, are sensi-
tive to features of increasing length scales of the original
image. We use a CNN architecture based on VGGNet
(Simonyan & Zisserman 2014) which uses 3 × 3 convolu-
tional ﬁlters and 2×2 max-pooling ﬁlters (see Figure 6).
The VGGNet architecture includes fully connected (FC)
layers to connect the ﬁnal convolutional layer to the out-

put. To facilitate interpretation of the trained CNN by
analysis of the features learned by convolutional ﬁlters in
the ﬁnal layer, we incorporate a CNN without any FC
layers i.e. a fully convolutional neural network (Shel-
hamer et al. 2017). The 5-layer deep CNN that we use
reduces the original 85 × 85-pixel input to 6 × 6 pixels
via max-pooling operations.

During training, error in the predicted output, as
measured by a loss function, is minimized by tuning
the weights and biases of the network via stochastic
gradient descent (see Appendix A). Here, we use bi-
nary cross-entropy loss function LCE (Eq. A2). Train-
ing a neural network also involves ﬁne tuning the net-
work hyper-parameters. The hyper-parameters associ-
ated with stochastic gradient descent for training in-
clude learning rate lr and batch size NBS. The learning
rate determines the step of the gradient descent. Too
small a learning rate slows down the training and too
large a learning rate diverges the loss function value LCE
(Eq. A2) (Hastie et al. 2001). Batch size determines the
number of training examples considered for the gradi-
ent descent at each iteration. We tune learning rate,
batch size and depth of the CNN (number of convolu-
tional layers) to optimize the classiﬁcation of the PE and
NE magnetograms as measured by True Skill Statistics
(TSS, see below).

Subsequent to the ﬁnal convolutional layer, we per-
form a 6 × 6 max-pooling operation to reduce the out-
put to the one neuron in the ﬁnal layer. We choose the
activation of the output neuron as a sigmoid function
that yields a value between 0 and 1 (Hastie et al. 2001).
This corresponds to the probability of the input magne-
togram containing the emergence. The predicted classes
are labeled as positive (1) and negative (0), by thresh-
olding the CNN output at 0.5. The CNN output thus
falls in one of the following categories.

• True Positives (TPs) - Sub-population of emerging
magnetic ﬁeld segments (PE) accurately classiﬁed
as positive (1).

• True Negatives (TNs) - Sub-population of non-
emerging magnetic ﬁeld segments (NE) accurately
classiﬁed as negative (0).

• False Positives (FPs) - Sub-population of non-
emerging magnetic ﬁeld segments (NE) inaccu-
rately classiﬁed as positive (1).

• False Negatives (FNs) - Sub-population of emerg-
ing magnetic ﬁeld segments (PE) inaccurately
classiﬁed as negative (0).

Note that the FPs categorized here as such may still
include true emergences that do not fall under the
SDO/HEAR criteria.

7

Figure 6. Fully convolutional neural network (CNN) (LeCun et al. 2015; Goodfellow et al. 2016) used for classiﬁcation of pre-
emergence (PE) and non-emergence (NE) magnetogram segments. The network architecture is based on VGGNet (Simonyan
& Zisserman 2014) and incorporates 3 × 3 convolutional ﬁlters (Figure 5) and 2 × 2 max-pooling ﬁlters. The max-pooling ﬁlters
downsize the output of each convolutional layer by a factor of two. Similar to the VGGNet, the number of convolutional ﬁlters
double in successive convolutional layers. Unlike the VGGNet, there are no fully connected layers of neurons connecting the
ﬁnal convolutional layer to the output. Instead, the ﬁnal convolution layer is connected to the output via 6 × 6 max-pooling
operation. This facilitates interpretation of the trained neural network.

Following the LBB survey, we use the Peirce Skill
Score or True Skill Statistics (TSS) to measure the ma-
chine performance (Barnes et al. 2014; Peirce 1884; Bo-
bra & Couvidat 2015). The TSS is deﬁned as TSS =
T P/(T P + F N ) − F P/(F P + T N ). The TSS is 0 for
both random and unskilled predictions and 1 for the
perfect classiﬁcation.

4. RESULTS

4.1. Training

We train the CNN (Figure 6) using supervised learn-
ing (Hastie et al. 2001) with PE and NE magnetograms

Figure 7. 5-fold cross-validation of the convolutional neu-
ral network for classiﬁcation of pre-emergence (PE) and
non-emergence (NE) magnetogram segments. The emerg-
ing active regions (EARs) and control regions (CRs) from
the SDO/HEAR dataset are randomly split in ﬁve approx-
imately equal parts. PE and NE segments from EARs and
CRs from the four parts are used for training and the re-
maining part is used for validation or test. This is performed
ﬁve times for the ﬁve diﬀerent validation sets. CNN hyper-
parameters are tuned to achieve maximum average 5-fold
cross-validation True Skill Statistics (TSS).

as inputs and labels 1 and 0 respectively as outputs.
Typically, the available data are split into three cate-
gories — training, validation and testing. The training
data are used to determine weights and biases of the net-
work and validation data are used to tune the network
hyper-parameters (Section 3). The trained machine is
then used to make predictions on the test data. A well-
trained network is able to reproduce the high accuracy
obtained during training on test data.

Since the number of EARs and CRs available are lim-
ited (see Table 1), we use 5-fold cross-validation for
tuning network hyper-parameters instead of a dedicated
testing set (Figure 7) (Hastie et al. 2001). The EARs
and CRs are each randomly split in ﬁve (approximately)
equal parts. We use PE and NE magnetogram seg-
ments from the four parts for training and the remain-
ing part for validation. Note that the sub-sampled PE
and NE magnetogram segments from an EAR or CR
are included in either the training data or the valida-
tion data and not both. We perform the training ﬁve
times, with EARs and CRs from ﬁve diﬀerent parts used
for validation. After each training, we obtain TSS us-
ing predictions on PE and NE magnetograms from val-
idation data. Because PE and NE magnetograms are
sub-sampled randomly from EARs/CRs, PE and NE
segments obtained from an EAR or CR contain over-
lapping regions. Therefore, while obtaining TSS only
non-overlapping segments are considered. There are
nine such non-overlapping 10◦ × 10◦ segments from each
EAR and CR (which are 30◦ × 30◦) i.e. 17 NE segments
for every PE segments. Thus the classiﬁcation problem
considered here is class-imbalanced. The network hyper-
parameters are tuned to obtain the maximum average
TSS value over 5-fold cross-validation. TSS is a good
metric for such class-imbalanced problems.

8

Figure 8. The variation of binary cross-entropy loss LCE
(Eq. A2) or misﬁt between true and predicted outputs for
training and validation data as a function of training iter-
ations or epochs. The training and validation loss is ap-
proximately equal and both monotonically decrease as the
training progresses indicating that parameters of the CNN
are not overﬁtted.

We set up the CNN using Python’s deep learning li-
brary keras (Chollet et al. 2015). At the beginning of
training, weights are initialized using the Glorot uni-
form initializer (Glorot & Bengio 2010) and biases are
initialized as zeros. The input magnetograms are stan-
dardized, i.e., the mean is subtracted and divided by the
standard deviation. Note that the mean and standard
deviation used for standardization is calculated over all
PE and NE magnetogram samples. This limits the oper-
ational range of the pixel values, representing the mag-
netic ﬁeld.

We obtain optimum training by setting learning rate
lr = 1.0 × 10−7 and batch size NBS = 32. We ob-
serve that the performance of CNN gradually improves
with increasing depth (Table 2). The max-pooling oper-
ation downsamples feature maps obtained in each con-
volution layer by half. Therefore, we limit the depth of

depth of CNN
(No. of hidden layers)

5-fold cross-validation
TSS (%)

1
2
3
4
5

8.89 ± 17.57
52.29 ± 7.88
70.34 ± 3.64
82.33 ± 2.87
84.57 ± 6.40

Table 2. Mean 5-fold cross-validation True Skill Statis-
tics (TSS) for classiﬁcation of pre-emergence (PE) and
non-emergence (NE) magnetogram segments at -3.2 h pre-
emergence time obtained using the convolutional neural net-
work (CNN, Figure 6) with increasing number of hidden con-
volutional layers. The 1σ error is quoted.

time before
emergence (h)

5-fold cross-validation
TSS (%)

-3.2
-8.5
-13.9
-19.2
-24.5

84.57 ± 6.40
61.97 ± 7.54
48.68 ± 5.45
40.80 ± 7.25
43.30 ± 6.93

Table 3. Mean 5-fold cross-validation True Skill Statis-
tics (TSS) for classiﬁcation of pre-emergence (PE) and non-
emergence (NE) magnetograms at diﬀerent pre-emergence
times obtained using the convolutional neural network
(CNN, Figure 6) with 5 hidden convolutional layers. The
1σ error is quoted.

the CNN to 5 layers, to yield ﬁnal layer feature maps
(the ﬁnal convolution layer outputs) of 6 × 6 pixel grid.
Each pixel from the ﬁnal layer feature map corresponds
to approximately 20 × 20 Mm2 of the input PE and
NE magnetograms, suﬃciently large to incorporate the
pre-emergence bipolar magnetic-ﬁeld pattern (Schunker
et al. 2019).

Using the set of PE and NE magnetograms available at
diﬀerent pre-emergence times (see Table 1), we train the
CNN to yield maximum average 5-fold cross-validation
TSS. We monitor training and validation losses at each
iteration to ensure that they decrease monotonically and
converge as training progresses, indicating that the net-
work is well-trained and does not suﬀer from overﬁtting.
(Figure 8).

The 5-fold cross-validation TSS obtained using the
CNN trained with the set of magnetograms taken at
diﬀerent pre-emergence times, are listed in the Table 3.
The TSS is ∼ 40% at 24.5 h before emergence, compara-
ble to the LBB survey results using the average unsigned
radial magnetic ﬁeld of EARs and CRs as discriminator.
TSS increases as emergence approaches, yielding ∼ 85%
at 3.2 h before emergence, which is signiﬁcantly higher
compared to ∼ 53% obtained in the LBB survey.

4.2. Discriminant analysis of unsigned line-of-sight

magnetic ﬂux.

The LBB survey used a non-parametric discriminant
analysis of radial magnetic ﬁeld |Br|, of 30◦ × 30◦ EARs
(CRs), averaged over 45.5 Mm about the emergence
(center) location. The radial magnetic ﬁeld was ob-
tained from the line-of-sight SOHO/MDI data using a
potential-ﬁeld model. The present work is concerned
with the classiﬁcation of 10◦ × 10◦ line-of-sight PE and
NE magnetogram segments, obtained from EARs and
CRs, using a CNN. We obtain a baseline for compar-
ing the performance of the CNN using a non-parametric

0255075100125150175200Epochs0.00.10.20.30.40.50.60.70.8Binary Cross-entropy Losstrainingvalidation9

Figure 9. Left: Discriminant analysis of pre-emergence (PE) and non-emergence (NE) magnetogram segments, taken 3.2 h
before emergence, based on the total unsigned line-of-sight magnetic ﬂux MTOT = (cid:80) |BLOS| dA. The True Skill Statistics (TSS)
achieved is ∼ 65%, signiﬁcantly lower than the TSS obtained using the CNN. Dashed lines show discriminant boundaries, where
PE and NE population densities are equal. Right: Comparison of TSS achieved for classiﬁcation of PE and NE magnetogram
segments using the CNN (this work) and discriminant analysis using MTOT of PE and NE magnetograms taken at diﬀerent
pre-emergence times. Also shown are results reported in the LBB survey (Barnes et al. 2014) for classiﬁcation of EARs and
CRs using the discriminant analysis applied to average unsigned radial magnetic ﬁeld. The CNN outperforms the discriminant-
analysis classiﬁcation, most signiﬁcantly for magnetograms at pre-emergence times -3.2 h and -8.5 h. The 1σ error bars are
shown.

discriminant analysis, similar to the LBB survey, of
the total unsigned line-of-sight magnetic ﬂux MTOT =
(cid:80) |BLOS| dA of PE and NE magnetogram segments.
MTOT is a well-deﬁned keyword in the SDO database
(Bobra et al. 2014). The MTOT measure used here is
slightly diﬀerent in that we do not require membership
of a coherent magnetic ﬁeld structure, but rather more
simply all of the magnetic ﬁeld within a speciﬁc area of
10◦ × 10◦.

For the discriminant analysis, we estimate probability
density of MTOT of the PE and NE magnetogram seg-
ments using the Epanechnikov kernel (Silverman 1986;
Barnes et al. 2014). We choose the kernel smoothing
parameter value that is optimum for a normal distribu-
tion. We estimate the probability density for magne-
tograms in the training data. As shown in the left panel
in Figure 9, we ﬁnd the discriminant boundaries where
the estimated probability densities of MTOT of PE and
NE magnetograms are equal. These are used to classify
magnetograms in the validation data and a TSS is ob-
tained. Similar to the CNN, this process is performed
for the ﬁve cross-validation sets.

From the left panel in Figure 9, the distributions of
MTOT for PE and NE magnetogram segments are well-
separated. For pre-emergence time -3.2 h, we ﬁnd dis-
criminant boundaries at 7.9×1020 Mx and 20.2×1020 Mx
and at 0.2 × 1020 Mx and 2.7 × 1020 Mx. The discrimi-
nant analysis yields cross-validation TSS of 65.8 ± 4.5%.
Repeating the analysis, we obtain TSS for classiﬁcation

of PE and NE magnetogram segments at diﬀerent pre-
emergence times. The right panel in Figure 9 shows
that the CNN outperforms the discriminant analyses us-
ing the unsigned magnetic ﬁeld (|Br| or MTOT), most
signiﬁcantly for pre-emergence times -3.2 h and -8.5 h.
The CNN yields ∼ 20% higher TSS for classiﬁcation
of PE and NE magnetograms compared to the baseline
classiﬁcation using discriminant analysis of MTOT. In
the following, we attempt to interpret the CNN perfor-
mance to understand the information learned for the
classiﬁcation of PE and NE magnetograms.

4.3. The mapping between the unsigned line-of-sight

magnetic ﬂux and the CNN output.

Given an input PE or NE magnetogram segment, the
trained CNN outputs a value between 0 and 1. The
predicted binary class labels 0 and 1 are obtained by
thresholding the output at 0.5. The original CNN out-
put can be interpreted as the probability of the input
magnetogram segment showing emergence after a cer-
tain time. Thus, the trained CNN maps surface mag-
netic ﬁeld (input) to the probability of emergence (out-
put). Patterns common to the magnetograms for which
the CNN output is y ∼ 0 are weakly correlated with
emergence and patterns common to the magnetograms
for which the CNN output y ∼ 1 are strongly correlated
with emergence.

We visually inspect patterns common to the PE and
NE magnetogram segments by arranging according to

061218243036424854MTOT (1020 Mx)0.000.010.020.030.040.05Population DensityNEPE-24.5-19.2-13.9-8.5-3.2Pre-emergence Time (h)0.020.040.060.080.0100.0True Skill Statistics (%)MTOTBarnes et. al. (2014)CNN10

Figure 10. Categorizing input pre-emergence (PE) and non-emergence (NE) magnetogram samples (3.2 h before emergence)
into bins corresponding to the CNN output values. For each bin, three samples with minimum total unsigned line-of-sight
magnetic ﬂux MTOT (left) and three samples with maximum MTOT (right) are shown. The bins are arranged such that the
corresponding CNN output increases from top to bottom. Left: Magnetograms with accurately predicted class labels are shown.
PE magnetograms classiﬁed accurately are true positives (TPs) and NE magnetograms classiﬁed accurately are true negatives
(TNs). Right: Magnetograms with inaccurately predicted class labels are shown. PE magnetograms classiﬁed inaccurately
are false negatives (FNs) and NE magnetograms classiﬁed inaccurately are false positives (FPs). The magnetogram plots are
saturated at 150 G (white) and -150 G (black). For TNs (y ∼ 0) magnetograms, MTOT value can be very low as well as very
high. For TPs (y ∼ 1) magnetograms, MTOT lies in an intermediate range.

corresponding CNN output into six bins centered at
CNN output values y = {1/12, 3/12, 5/12, 7/12, 9/12,
11/12}, bounded by y ± ∆/2 where bin-width ∆ = 1/6.
The left panel of Figure 10 shows representative magne-
tograms (taken at -3.2 h pre-emergence) with accurate
predicted class labels from the six CNN output bins. We
see that the CNN output is low y ∼ 0 (top row) for the
magnetograms with very low as well as very high value of
MTOT. With the CNN output progressively increasing
for the magnetograms in the subsequent bins, the corre-
sponding value of MTOT either progressively increases
from the low value or progressively decreases from the
high value. The magnetograms that yield y ∼ 1 (bottom
row) are the magnetograms with MTOT value within an
intermediate range. The high value of TSS ∼ 85%, for
classiﬁcation of PE and NE magnetogram segments us-
ing the CNN, implies that the distribution of MTOT
of magnetograms within this intermediate range closely
matches the distribution of MTOT for the PE samples.
The right panel of Figure 10 similarly shows magne-
togram samples with inaccurately predicted class labels
arranged in the six bins of CNN output. Overall, there
are signiﬁcantly fewer such magnetogram segments com-

pared to the number of magnetograms with accurately
predicted class labels (Figure 11). The PE magnetogram
segments that yield y ∼ 0 are relatively unclean emer-
gences i.e. emerging in the vicinity of strong pre-existing
magnetic ﬁeld. The NE magnetogram segments that
yield y ∼ 1 show distinct bipolar magnetic-ﬁeld struc-
ture akin to the pre-emergence active region (Figure 3).
Few NE magnetogram segments with moderately high
MTOT also yield high CNN output y ∼ 1.

Figure 10 clearly shows that there is a wide range of
MTOT values for many of the CNN output bins. There-
fore, we analyse MTOT of PE and NE samples from each
category of the CNN output bins. We only consider a
sub-population of PE and NE magnetograms between
MTOT of 4×1020 Mx and 16×1020 Mx (Figure 20) and
discard a small number of PE and NE samples with very
high and very low values of MTOT (see Appendix B).
The left panel of Figure 11 shows the distribution of PE
and NE magnetograms binned according to the corre-
sponding CNN output. The PE distribution clusters at
y ∼ 1 and the NE distribution clusters at y ∼ 0.
We calculate the average value (cid:104)MTOT(cid:105)bin

PE/NE =
MTOT, over PE and NE samples

(1/N bin

PE/NE) (cid:80)

N bin

PE/NE

y(0,1/6)11969 EAR-NE11200 CR-NE11450 EAR-NE11780 CR-NE11400 CR-NE11404 EAR-NEy(1/6,2/6)11736 EAR-NE11843 EAR-NE11605 CR-NE11103 CR-NE11446 CR-NE11560 EAR-NEy(2/6,3/6)11849 CR-NE11182 CR-NE11066 CR-NE11152 CR-NE11597 CR-NE11098 CR-NEy(3/6,4/6)11088 EAR-PE11297 EAR-PE11156 EAR-PE11381 EAR-PE11951 EAR-PE11322 EAR-PEy(4/6,5/6)11680 EAR-PE12011 EAR-PE11696 EAR-PE11334 EAR-PE11902 EAR-PE12078 EAR-PEy(5/6,1)11449 EAR-PE11072 EAR-PE11414 EAR-PE11242 EAR-PE11915 EAR-PE11267 EAR-PEaccurately classified inputs (TP and TN)CNN output10°10°y(0,1/6)11081 EAR-PE11752 EAR-PE11080 EAR-PE12062 EAR-PE11833 EAR-PE11223 EAR-PEy(1/6,2/6)11697 EAR-PE11607 EAR-PE11786 EAR-PE11157 EAR-PE12105 EAR-PE11894 EAR-PEy(2/6,3/6)11511 EAR-PE11406 EAR-PE11198 EAR-PE11456 EAR-PE12003 EAR-PE11304 EAR-PEy(3/6,4/6)11152 CR-NE11267 EAR-NE11699 EAR-NE11222 CR-NE11551 CR-NE11764 CR-NEy(4/6,5/6)11154 EAR-NE11645 CR-NE11603 CR-NE11750 CR-NE11554 CR-NE11156 CR-NEy(5/6,1)12099 CR-NE11114 CR-NE12003 CR-NE11431 CR-NE11712 EAR-NE11924 EAR-NEinaccurately classified inputs (FP and FN)CNN output10°10°11

PE and N bin

Figure 11. Statistical analysis of the total unsigned line-of-sight magnetic ﬂux MTOT of pre-emergence (PE) and non-
emergence (NE) magnetograms taken at 3.2 h before emergence. The PE and NE samples are distributed in bins as per the
corresponding CNN output. The bins of CNN output are centered at y = {1/12, 3/12, 5/12, 7/12, 9/12, 11/12} and bounded by
y ± ∆/2 where ∆ = 1/6. Left: The population density of PE and NE samples binned by the CNN output. For a signiﬁcant
majority of the PE samples y ∼ 1 and for a signiﬁcant majority of the NE samples y ∼ 0. Right: The average (cid:104)MTOT(cid:105)
calculated over PE and NE samples from each bin of the CNN output. The PE and NE samples for which the CNN output
y ≤ 0.5 are labeled as non-emerging and y > 0.5 are labeled as emerging. The (cid:104)MTOT(cid:105)PE decreases from a high value as the
CNN output increases, whereas (cid:104)MTOT(cid:105)NE increases from low value as the CNN output increases. Thus, NE samples with low
CNN output y ∼ 0 have low values of (cid:104)MTOT(cid:105)NE and PE samples with low CNN output y ∼ 0 have high values of (cid:104)MTOT(cid:105)NE.
For PE and NE samples with high CNN output y ∼ 1, (cid:104)MTOT(cid:105)PE and (cid:104)MTOT(cid:105)NE values are in the intermediate range. The
3σ error bars are shown.
(N bin
NE ) from each bin of the CNN output. The
right panel of Figure 11 shows the variation of the mean
value of (cid:104)MTOT(cid:105)PE/NE with CNN output. PE segments
with low CNN output y ∼ 0.1 have high (cid:104)MTOT(cid:105)PE ∼
12 × 1020 Mx and NE segments with low CNN out-
put y ∼ 0.1 have low (cid:104)MTOT(cid:105)NE ∼ 7 × 1020 Mx. As
the CNN output increases, (cid:104)MTOT(cid:105)PE decreases and
(cid:104)MTOT(cid:105)NE increases. The PE and NE samples that
produce output y > 0.5 fall within an intermediate range
of (cid:104)MTOT(cid:105) between 9 × 1020 Mx − 11 × 1020 Mx. Thus,
MTOT is an important factor contributing to the CNN
performance such that magnetograms with the CNN
output y ≥ 0.5 have average MTOT value within an in-
termediate range (between 9 × 1020 Mx − 11 × 1020 Mx).
As shown in the left panel of Figure 9, the popula-
tion fraction of the PE magnetograms peaks within the
MTOT range of 9 × 1020 Mx − 11 × 1020 Mx. Therefore,
(cid:104)MTOT(cid:105) values in the right panel of Figure 11 are inﬂu-
enced by the original distribution of PE and NE mag-
netograms with respect to MTOT. Explicit analysis of
CNN output as a function of MTOT shows that there
are other factors contributing to the CNN performance
(see Appendix G).

convolutional layer of the CNN (Figure 6), which is di-
rectly connected to the output layer. The ﬁnal layer
consists of 256 convolutional ﬁlters (Figure 5) and may
learn up to 256 feature maps, i.e., surface magnetic ﬁeld
patterns correlated with emergence (a signiﬁcant num-
ber of which may be redundant).
It is challenging to
analyse all 256 ﬁlters in detail and understand the con-
tribution of each ﬁlter. It is however possible to system-
atically reduce the number of ﬁlters in the ﬁnal layer,
during training, to only few ﬁlters which are easier to
interpret. This process is known as network pruning
(Cun et al. 1990; Han et al. 2015; Frankle et al. 2019;
Qin et al. 2018).

We develop a network pruning algorithm to iteratively
reduce the ﬁnal convolutional layer ﬁlters from N = 256
to N = 4. The pruning algorithm is based on identifying
top N/4 ﬁlters maximally correlated and top N/4 ﬁlters
maximally anti-correlated with surface magnetic-ﬁeld
patterns associated with emergence and removing the
remaining relatively neutral N/2 ﬁlters (Appendix C).
The four ﬁlters in the ﬁnal convolutional layer at the
end of the training are the dominant four ﬁlters con-
tributing towards the CNN output, two corresponding
each to surface magnetic-ﬁeld patterns correlated and
anti-correlated with emergence.

4.4. Mapping between unsigned line-of-sight magnetic

ﬂux and the convolutional ﬁlter outputs.

To further understand the performance of the CNN,
we analyse the contribution of diﬀerent convolutional
ﬁlters towards the CNN output. We focus on the ﬁnal

4.4.1. Outputs of the ﬁlters in the ﬁnal convolutional layer.

We analyse the response of the four convolutional ﬁl-
ters in the ﬁnal layer of the pruned CNN with respect

01/62/63/64/65/61CNN Output0.00.10.20.30.40.5Population FractionPENE01/62/63/64/65/61CNN Output6789101112Mean MTOT (1020 Mx)identified as non-emergingidentified asemergingPENE12

Figure 12. Filter contributions (maximum positive value
times the ﬁlter weight, Eq. C3) of the ﬁnal convolutional
layer in the CNN trained with pruning algorithm (Ap-
pendix C). The PE and NE samples are categorized in ﬁve
bins according to the total unsigned line-of-sight magnetic
ﬂux (MTOT). Average ﬁlter contribution (cid:104)Fi(cid:105)bin corre-
sponding to PE and NE samples from each bin and each
ﬁlter i is plotted. Contributions from ﬁlters 1 and 2 increase
with growing MTOT, whereas ﬁlters 3 and 4 show decreasing
contribution with increasing MTOT. Thus, the CNN incor-
porates ﬁlters that respond positively as well as negatively
to increasing MTOT. 20σ error bars are shown.

to MTOT. We divide the sub-population of PE and
NE samples (Appendix B) in ﬁve bins of equal widths.
For PE and NE samples in each bin, we calculate ﬁl-
ter contribution Fi (maximum positive value multiplied
by the ﬁlter weight, Eq. C3) for each of the four ﬁl-
ters in the pruned CNN. We obtain the average value
of ﬁlter contribution (cid:104)Fi(cid:105)bin for each bin, for each ﬁl-
ter i. Figure 12, shows ﬁlters 1 and 2, which corre-
spond to surface magnetic-ﬁeld patterns that are cor-
related with emergence, (cid:104)Fi(cid:105)bin increases with increas-
ing MTOT. Similarly, for ﬁlters 3 and 4, which corre-
spond to surface magnetic-ﬁeld patterns that are anti-
correlated with emergence, (cid:104)Fi(cid:105)bin decreases with in-
creasing MTOT. Thus, the trained CNN incorporates
ﬁlters which respond both positively as well as nega-
tively to MTOT of the PE and NE magnetograms. The
magnitude of ﬁlter contributions (cid:104)Fi(cid:105)bin increases mono-
tonically with increasing MTOT.

To further illustrate the operation of the convolutional
ﬁlters in the pruned CNN, we visibly inspect the out-
put of each ﬁlter for a few representative PE and NE
samples. Figure 13 shows select examples of outputs
of each convolutional ﬁlter, binned by the normalized
contributions (Eq. C3) from the ﬁlter. Filters 1 and
2 contribute positively towards the CNN output. Also

the (minimum) MTOT of the magnetogram samples in-
creases with ﬁlter contribution. We see that the ﬁlter
output is maximum for the prominent magnetic-ﬂux re-
gions in the input magnetograms, yielding positive ﬁlter
output. Filters 3 and 4 contribute negatively to ma-
chine prediction. Also, the (minimum) MTOT of the
magnetogram samples decreases as ﬁlter contribution in-
creases. Filter 3 produces positive and negative outputs
corresponding to magnetic-ﬂux areas. Filter 4 produces
negative output corresponding to magnetic-ﬂux areas of
the input magnetograms. Information about the polar-
ity of the magnetic ﬂux areas may not be necessary to
train a CNN that performs comparably to one trained on
data including the polarity of the ﬁeld (see Appendix F).
Also, these ﬁlter outputs spatially conform to the edges
of the magnetic regions (see Figure 14).

4.5. CNN output and the length-scale of magnetic

regions.

The aforementioned analysis shows that the outputs
of the convolutional ﬁlters in the ﬁnal layer strongly de-
pend on the total unsigned line-of-sight ﬂux MTOT of
PE and NE regions. MTOT is also an important fac-
tor deciding the ﬁnal CNN output, although there are
other factors important for the classiﬁcation (see Ap-
pendices E and G). The value of MTOT depends on the
size of magnetic regions as well as the magnetic ﬁeld
intensity. We use synthetic magnetograms to explicitly
test the dependence of the CNN output on both these
factors.

We use circular synthetic bipolar magnetic regions
with uniform magnetic ﬁeld intensity, as shown in the
left panel of Figure 15, to probe the CNN output. The
right panel of Figure 15 shows the pruned CNN output
for synthetic bipoles of diﬀerent magnetic ﬁeld inten-
sity and length-scales (radius). We see that the CNN
output for a synthetic bipole of a given size generally
increases with increasing value of the magnetic ﬁeld in-
tensity. The CNN output saturates for magnetic ﬁeld
value of ∼ 1000 G and falls sharply to 0 beyond the
length-scale of ∼ 30 Mm. For the 100 G bipole, the CNN
output peaks (y ∼ 0.9) at the length-scale of 15 Mm and
falls sharply to 0 beyond the length-scale of ∼ 30 Mm.
For the 10 G bipole, the CNN output is consistently low
(y ∼ 0.2). Thus, the CNN associates magnetic bipoles
of small length-scales and intense ﬁelds, as per Figure 3,
with emergence.

Similar to Figure 13, Figure 14 shows outputs of the
ﬁlters of the pruned CNN for a synthetic bipole. As
discussed earlier, the ﬁlter outputs spatially coincide
with the edges of the magnetic regions. Figure 16 shows
contributions (maximum positive value times the ﬁlter

5.06.58.09.511.012.514.015.5MTOT (1020 Mx)-6.0-4.0-2.00.02.04.06.0Filter Contribution FiFilter1Filter2Filter3Filter413

Figure 13. Outputs of the ﬁlters in the ﬁnal convolutional layer of the CNN for representative PE and NE samples. The CNN
is trained using network pruning to retain the four most important ﬁlters in the ﬁnal convolutional layer (Appendix C). The PE
and NE samples are binned by the normalized ﬁlter contribution Fi (maximum positive value times the ﬁlter weight, Eq. C3),
arranged from top to bottom in increasing order of the ﬁlter output. For each bin, three PE or NE samples with minimum
total unsigned line-of-sight magnetic ﬂux (MTOT) are shown (except the bottom bin for ﬁlter 1 in panel b, where only two
are available). The input PE and NE magnetograms are plotted with magnetic ﬁeld saturated at 150 G (white) and -150 G
(black) and are clipped below an absolute value of 60 G. Filter outputs are saturated at 1 (red) and -1 (blue). 100 G (white)
and -100 G (black) contours from the magnetograms are shown on top of the ﬁlter outputs for clarity. Filters 1 and 2 produce
positive contributions Fi, while ﬁlters 3 and 4 produce negative contributions. For ﬁlters 1 and 2, the minimum MTOT of PE
and NE samples increases as the ﬁlter contribution increases and conversely for ﬁlters 3 and 4. For ﬁlters 1 and 2, the output
is maximum in the region of large magnetic ﬂux in the input. The ﬁlter 4 output also scales with the local magnetic ﬂux in the
input, but with a negative sign. For ﬁlter 3, the output is positive and negative corresponding to magnetic-ﬂux regions.

F  (0.0,0.2)11736 EAR-NE11969 EAR-NE11696 EAR-NEF  (0.2,0.4)11431 EAR-PE11211 EAR-PE11624 EAR-PEF  (0.4,0.6)11446 CR-NE11631 EAR-NE12099 CR-NEF  (0.6,0.8)11400 CR-NE11631 EAR-NE11894 EAR-NEinputF  (0.8,1.0)11829 CR-NEfilter  outputinput11400 CR-NEfilter  outputinput11404 EAR-NEfilter  output Filter ContributionFilter 1F  (0.0,0.2)11736 EAR-NE11969 EAR-NE11696 EAR-NEF  (0.2,0.4)11211 EAR-PE12039 EAR-PE11222 EAR-PEF  (0.4,0.6)11631 EAR-NE11446 CR-NE11152 CR-NEF  (0.6,0.8)11400 CR-NE11631 EAR-NE11911 EAR-NEinputF  (0.8,1.0)11400 CR-NEfilter  outputinput11400 CR-NEfilter  output Filter ContributionFilter 2F  (1.0,0.8)11962 CR-NE12041 EAR-NE11396 CR-NEF  (0.8,0.6)11449 EAR-NE11894 EAR-NE11807 EAR-NEF  (0.6,0.4)11624 EAR-NE11547 CR-NE11813 EAR-NEF  (0.4,0.2)11137 CR-NE11561 EAR-NE11932 EAR-NEinputF  (0.2,0.0)11736 EAR-NEfilter  outputinput11969 EAR-NEfilter  outputinput11696 EAR-NEfilter  output Filter ContributionFilter 3F  (1.0,0.8)11400 CR-NE11789 EAR-NE11736 CR-NEF  (0.8,0.6)11400 CR-NE11631 EAR-NE11752 EAR-NEF  (0.6,0.4)11446 CR-NE11631 EAR-NE11103 EAR-NEF  (0.4,0.2)11874 CR-NE11152 CR-NE11605 CR-NEinputF  (0.2,0.0)11736 EAR-NEfilter  outputinput11969 EAR-NEfilter  outputinput11696 EAR-NEfilter  output Filter ContributionFilter 414

Figure 14. Outputs of ﬁlters in the ﬁnal convolutional layer of the pruned CNN for a synthetic bipole of 100 G uniform ﬁeld
and radius 25 Mm (Figure 15). Filter outputs are saturated at 1 (red) and -1 (blue). 100 G (white) and -100 G (black) contours
from the synthetic bipole are shown on top of the ﬁlter outputs. Filter outputs conform to the edges of the positive and negative
polarity regions.

put decreases to 0 is therefore a cumulative result of the
contributions from all ﬁlters.

Figure 17 shows the dependence of the characteristic
length-scale for the CNN output on the depth of the
CNN (without pruning ﬁlters in the ﬁnal layer). A cor-
relation between the length-scale of the magnetic regions
and the CNN output forms for a CNN with more than
two layers which results in increased classiﬁcation TSS
(Table 2). For CNNs with up to 5 layers, the length-
scale beyond which the CNN output drops to 0 increases
with increasing number of CNN layers. This is result of
downsampling of the input via max-pooling operation as
the CNN captures patterns at increasing length-scales
with increasing depth of the network. The characteris-
tic length-scale continues to increase for CNN with layer
6 (∼ 40 Mm). after which it falls abruptly (∼ 25 Mm).

Figure 15. Left: Circular synthetic bipole of uniform mag-
netic ﬁeld strength used for probing the trained CNN. Right:
Output of the pruned CNN as a function of (uniform) inten-
sity of the magnetic ﬁeld and synthetic bipole length-scale
(radius). For 100 G and 1000 G, there is a characteristic
length-scale for the CNN beyond which output sharply drops
to 0.

weight, Eq. C3) of the ﬁlters of the pruned CNN as a
function of the synthetic bipole length-scale. The con-
tribution of ﬁlters 1 and 2, which are correlated with
emergence, increases with increasing bipole length-scale
upto ∼ 30 Mm and saturates.
In contrast, the con-
tribution of ﬁlters 3 and 4, which are anti-correlated
with emergence, continuously decreases with increasing
bipole length-scale. No single ﬁlter is particularly sensi-
tive to a speciﬁc length-scale of magnetic regions. The
characteristic length-scale beyond which the CNN out-

Figure 16. Filter contributions Fi (maximum positive value
times the ﬁlter weight, Eq. C3) of the pruned CNN as a func-
tion of the synthetic bipole (Figure 15) length-scale (radius)
for 100 G uniform magnetic ﬁeld.

Figure 17. The CNN output for synthetic bipoles for CNNs
with increasing number of convolutional layers. The cor-
relation between the CNN output and length-scale of the
synthetic bipoles exists for CNN with more than two con-
volutional layers. The characteristic length-scale, beyond
which the CNN output sharply drops, increases with increas-
ing number of CNN layers.

InputFilter 1Filter 2Filter 3Filter 4-4°-2°0°2°4°-4°-2°0°2°4°0102030405060Synthetic bipole scale (Mm)0.00.20.40.60.81.0CNN Output10.0 G100.0 G1000.0 G0102030405060Synthetic bipole scale (Mm)-15.0-10.0-5.00.05.0Filter contributionFilter 1Filter 2Filter 3Filter 40102030405060Synthetic bipole scale (Mm)0.00.20.40.60.81.0CNN Output# CNN Layers:210.0 G100.0 G1000.0 G0102030405060Synthetic bipole scale (Mm)0.00.20.40.60.81.0CNN Output# CNN Layers:30102030405060Synthetic bipole scale (Mm)0.00.20.40.60.81.0CNN Output# CNN Layers:40102030405060Synthetic bipole scale (Mm)0.00.20.40.60.81.0CNN Output# CNN Layers:515

Figure 18. Visual depictions from the trained CNN using occlusion maps (Zeiler & Fergus 2014), obtained by systematically
masking patches in the input and noting the change in the CNN output. The positive (negative) pixels in the occlusion map
indicate regions in the input which are correlated (anti-correlated) with emergence as seen by the CNN. The CNN output and
occlusion maps, calculated using Eqs. D7 and D8 respectively, are shown for two control regions (CRs, top) and two emerging
active regions (EARs, bottom)) taken from 3.2 h before emergence. The magnetic ﬁeld in EARs and CRs are saturated at 150
G (white) and -150 G (black). The output is saturated at 0 (blue) and 1 (red). The occlusion map is calibrated at -1 (blue)
and 1 (red). Note that the emergence takes place within the central 10◦ × 10◦ region of the EARs. Magnetic regions of interest,
classiﬁed as true positives (TP, red), true negatives (TN, blue) and false positives (FP, black) are encircled.

This is possibly a result of severe reduction of the input
magnetograms (size 2 × 2 and 1 × 1 for layers 7 and 8
respectively) via max-pooling. Thus, the characteristic
length-scale depends on downsampling of the input as a
result of increasing number of CNN layers as well as the
size of max-pooling kernel.

4.6. CNN Visualizations.

The analysis has thus far focused on deriving quanti-
tative information about surface magnetic-ﬁeld patterns
that the CNN uses to discriminate between PE and NE
magnetogram samples. The approach is speciﬁc to the
problem at hand and has provided suﬃcient informa-
tion to understand the operation of the trained CNN.
The machine learning literature also provides us with
tools for qualitative interpretation or visual explanations
of deep neural networks (Simonyan et al. 2013; Zeiler
& Fergus 2014; Selvaraju et al. 2017). Such tools, like
saliency maps (Simonyan et al. 2013) and gradient-based
class-activation maps (Selvaraju et al. 2017), make use of
the gradient information backpropagated (Hastie et al.
2001) from the output layer to the hidden and input
layers.

Here, we generate visualizations of the trained CNN
using a simpler technique known as occlusion maps
(Zeiler & Fergus 2014). Occlusion maps are obtained by
measuring changes in the output when diﬀerent patches

MTOT (1020 Mx)

MTOT (1020 Mx)

TP1
TP2
FP1
FP2
FP3

10.8
15.2
7.2
55.2
8.6

TN1
TN2
TN3
TN4
TN5

5.2
23.3
6.5
6.5
6.7

Table 4. The total unsigned line-of-sight magnetic ﬂux
(MTOT) of 85 × 85-pixel segments including various regions
of interests labeled in Figure 18.

in the input are masked. We use 25 × 25 pixel mask
for input PE and NE magnetogram segments to gener-
ate occlusion maps. The mask size is chosen to be large
enough to cause a signiﬁcant diﬀerence in the CNN out-
put after occlusion. For a given PE or NE sample, the
occlusion map is of the same size as the magnetogram
and initialized with the uniform value of the predicted
class label Ypred = 1 or 0 for the sample. A new pre-
dicted class label Ymask is obtained after masking or oc-
cluding a 25 × 25 pixel patch in the input image. The
values at the corresponding pixels in the occlusion map
are updated to Ypred − Ymask. Thus, the positive (nega-
tive) values in the occlusion map represent regions in the
input image that are correlated (anti-correlated) with
emergence as seen by the CNN.

-1000100x (Mm)-1000100y (Mm)FP1NOAA 11789, -3hOutputOcclusion-1.0-0.50.00.51.0-1000100x (Mm)-1000100y (Mm)FP2TN1TN2NOAA 11829, -3hOutputOcclusion-1.0-0.50.00.51.0-1000100x (Mm)-1000100y (Mm)TP1TN3NOAA 11824, -3hOutputOcclusion-1.0-0.50.00.51.0-1000100x (Mm)-1000100y (Mm)TP2FP3TN5TN4NOAA 11322, -3hOutputOcclusion-1.0-0.50.00.51.016

For comprehensive visualisation of occlusion maps cor-
responding to all PE and/or NE segments from an EAR
or CR, we recombine sub-sampled magnetograms, cor-
responding CNN outputs and occlusion maps (see Ap-
pendix D). Figure 18 shows the CNN outputs and oc-
clusion maps for two CR and two EAR samples. These
samples are taken from 3.2 h before emergence. The
MTOT values of 85 × 85-pixel segments including vari-
ous regions of interest are quoted in Table 4. Note that,
overall, MTOT values of TN regions are low, with ex-
ception of TN2 for which MTOT value is high. Also,
overall, MTOT values of TP and FP regions are inter-
mediate, with exception of FP2 for which MTOT value
is very high. This is largely consistent with our earlier
analyses. Regions TP1, TP2, FP1, FP2 and FP3 are
highlighted positively in the occlusion map. Regions
FP1 and FP3 show small-scale bipolar ﬁeld patterns,
which are an early sign of emergence, similar to regions
TP1 and TP2. Regions TN2, TN3 and TN4 are high-
lighted negatively in the occlusion map, whereas, regions
TN1 and TN5 are neutral in the occlusion map. These
regions include bipoles, which may also be a factor for
the CNN classiﬁcation (Appendix E). The CNN output
y ∼ 1.0 for FP2 suggests that there may be other factors
besides MTOT and bipolarity for the CNN classiﬁcation
(see Appendix G). These factors, however, are not easily
interpreted.

5. SUMMARY

We perform a statistical analysis of emerging ARs in
the SDO/HEAR dataset (Schunker et al. 2016) using
convolutional neural networks (CNN). The trained CNN
described here successfully classiﬁes PE and NE magne-
tograms with a TSS of ∼ 85%, 3.2 h before emergence,
which is signiﬁcantly better than the TSS obtained in
the LBB survey (Leka et al. 2012; Birch et al. 2012;
Barnes et al. 2014). The TSS score decreases to ∼ 40%,
24.5 h before emergence, similar to the LBB survey.

The discriminant analysis in the LBB survey (Barnes
et al. 2014) showed that the pre-emergence surface mag-
netic ﬁeld is a leading factor in distinguishing between
EARs and CRs (Figure 3). We therefore perform a dis-
criminant analysis of PE and NE magnetograms using
the total unsigned line-of-sight magnetic ﬂux MTOT =
(cid:80) |BLOS| dA to establish a baseline for comparing the
CNN performance. We ﬁnd that the maximum TSS
achieved using MTOT for classiﬁcation of the PE and
NE magnetograms is ∼ 65%, 3.2 h before emergence.
Thus, the trained CNN performs better than the base-
line model of discriminant analysis using only MTOT.
Through visual inspection of the PE and NE mag-
netograms, binned by the corresponding CNN output,

we identify MTOT as an important attribute of mag-
netograms for the CNN classiﬁcation. We ﬁnd that
the CNN produces maximum output y ∼ 1 for mag-
netograms with MTOT in an intermediate range. Mag-
netograms with very low as well as very high values of
MTOT yield low CNN output y ∼ 0. Further, statistical
analysis of MTOT of PE and NE magnetograms, also
binned by CNN output, reveals that the average value of
MTOT for magnetograms which are predicted as emerg-
ing by the CNN, lies between 9×1020 Mx−11×1020 Mx.
These ﬁndings are validated by visual insights into the
operation of the CNN obtained using occlusion maps
(Zeiler & Fergus 2014). The TSS for classiﬁcation de-
creases for PE and NE magnetograms from earlier pre-
emergence times as the diﬀerence between distributions
of MTOT for the PE and NE samples diminishes (Fig-
ure 3). Explicit analysis of CNN output as a func-
tion of MTOT shows that there are other factors be-
sides MTOT that contribute to the CNN performance
(Appendix G). These factors may be associated with
length-scales, ﬁeld intensities and geometry of the pre-
emergence small-scale bipole shown in Figure 3.

The fully convolutional CNN (Figure 6) for this work
was speciﬁcally chosen to facilitate subsequent assess-
ment of the contribution of the convolutional ﬁlters in
the ﬁnal layer. Still, all ﬁlters in the ﬁnal layer N = 256
are diﬃcult to analyse in detail to understand the spe-
ciﬁc patterns learned. We therefore develop a network-
pruning algorithm to systematically remove unimpor-
tant ﬁlters from the ﬁnal layer during training and retain
only the most important (four) ﬁlters (Appendix C).
From visual inspection as well as statistical analysis of
ﬁlter outputs, we ﬁnd that the CNN incorporates convo-
lutional ﬁlters that are sensitive to total unsigned line-of-
sight magnetic ﬂux of PE and NE magnetograms. Some
of these ﬁlters yield positive output in response to in-
creasing magnetic ﬂux, whereas others yield a negative
output. The CNN output is the sum total of outputs
from all ﬁlters. A multi-variable discriminant analysis
of MTOT and a measure of dipole moments of mag-
netograms yields the classiﬁcation TSS of 75.6 ± 3.3%
(3.2 h before emergence) which is comparable to the
CNN (Appendix E).

Using synthetic bipolar magnetograms of circular
shape, we demonstrate that there exists a character-
istic length-scale of magnetic regions beyond which the
CNN output drops to 0 irrespective of the magnetic
ﬁeld intensity. The characteristic length-scale is also a
cumulative result of contributions from all ﬁlters and
is an artifact of the number of layers in the CNN and
max-pooling operation. Explicit dependence on these
factors remains to be explored.

The CNN output peaks for small-scale (∼ 30 Mm) and
intense (∼ 100 G) synthetic bipoles. Our analysis does
not conclusively show that bipolarity of magnetograms
is an important factor for the CNN classiﬁcation. Inter-
estingly, a CNN trained on PE and NE segments without
any polarity information also yields a comparable value
of the TSS (Appendix F). Altogether this suggests that
the CNN identiﬁes small-scale and (unipolar or bipolar)
intense ﬁelds as a characteristic pre-emergence pattern.
A time-dependent analysis is necessary to shed light on
formation of such small-scale ﬁelds by either rising in-
tact from deep within the convection zone (Fan 2009) or
by forming over a less localized area near surface (Bran-
denburg 2005). Further, Birch, A. C. et al. (2019) show
that the emerging ﬂux interacts with the supergranula-
tion pattern and therefore emergence locations are corre-
lated with supergranulation. The network is highlighted
in PE and NE magnetograms since magnetic ﬂux tends
to collect in supergranular lanes. Thus, the CNN perfor-
mance here may also be dependent on factors associated
with the supergranulation. These remain to be explored.
It is to be noted that the SDO/HEAR dataset consid-
ered here contains limited (∼ 200) independent samples
of EARs and CRs. The quantity of data required for
training the CNN to gain deeper and concrete insights
about pre-emergence patterns may not be known in ad-
vance. However, the limited dataset hinders the CNN
from learning the convolutional ﬁlters associated with
more and general complex patterns than just the total
unsigned pre-emergence magnetic ﬁeld, e.g., small-scale
pre-emergence bipoles. The SDO/HMI instrument pro-
vides terabytes of high resolution, high cadence mag-
netic ﬁeld data every day. With these data, it may be
possible to construct a large enough dataset, with more

17

general selection criteria compared to the SDO/HEAR
dataset, to study emergence. Such a dataset is also
likely to conform to real-time emergence scenarios and
therefore may also be analysed with a focus on fore-
casting emergence. Alternatively, it may be possible
to train the CNN convolutional ﬁlters to exactly de-
tect the pre-emergence bipolar magnetic-ﬁeld and cor-
related patterns using additional constraints on convo-
lutional ﬁlters (Bayar & Stamm 2018) and/or to learn
the emergence location along with a probability (Zhao
et al. 2018). A machine successful for such a task will
also be useful for the analysis of pre-emergence temporal
evolution. These analyses are deferred to future work.
With Petabyte-scale astronomical datasets expected in
the upcoming decade, machine learning is expected to
become increasingly relevant in analysing data (Baron
2019). Insights obtained from this work may therefore
be useful for training and interpreting deep neural net-
works in future solar and astrophysical data-analysis ap-
plications.

D.B.D., S.M.H., A.C.B. and H.S. designed the re-
search. D.B.D. performed the data analysis. All au-
thors contributed to the interpretation of the results.
D.B.D. wrote the manuscript with contributions from
all authors.

S.M.H acknowledges funding support from the Max-
Planck partner group program. The HMI data used here
are courtesy of NASA/SDO and the HMI science team.
We acknowledge partial support from the European Re-
search Council Synergy Grant WHOLE SUN #810218.
Observations are courtesy of NASA/SDO and the HMI
science teams. We thank the anonymous reviewer for
their comments and suggestions which helped improve
the analysis and clarity of the paper.

REFERENCES

Barnes, G., Birch, A. C., Leka, K. D., & Braun, D. C. 2014,

Birch, A. C., Schunker, H., Braun, D. C., & Gizon, L. 2019,

The Astrophysical Journal, 786, 19. https:

//doi.org/10.1088%2F0004-637x%2F786%2F1%2F19

Baron, D. 2019, arXiv e-prints, arXiv:1904.07248

Bayar, B., & Stamm, M. C. 2018, IEEE Transactions on

Information Forensics and Security, 13, 2691

Birch, A. C., Braun, D. C., & Fan, Y. 2010, The

Astrophysical Journal, 723, L190. https:

//doi.org/10.1088%2F2041-8205%2F723%2F2%2Fl190

Birch, A. C., Braun, D. C., Leka, K. D., Barnes, G., &

Javornik, B. 2012, The Astrophysical Journal, 762, 131.

https:

A&A, 628, A37.

https://doi.org/10.1051/0004-6361/201935591

Bobra, M. G., & Couvidat, S. 2015, The Astrophysical

Journal, 798, 135.

http://stacks.iop.org/0004-637X/798/i=2/a=135

Bobra, M. G., Sun, X., Hoeksema, J. T., et al. 2014, Solar

Physics, 289, 3549.

https://doi.org/10.1007/s11207-014-0529-3

Brandenburg, A. 2005, The Astrophysical Journal, 625,

//doi.org/10.1088%2F0004-637x%2F762%2F2%2F131

539. https://doi.org/10.1086%2F429584

18

Brandenburg, A., Gressel, O., Jabbari, S., Kleeorin, N., &

Peirce, C. S. 1884, Science, ns-4, 453.

Rogachevskii, I. 2014, A&A, 562, A53.
https://doi.org/10.1051/0004-6361/201322681

https://science.sciencemag.org/content/ns-4/93/453.2

Pesnell, W. D., Thompson, B. J., & Chamberlin, P. C.

Cameron, R., Dikpati, M., & Brandenburg, A. 2016, Space

2012, Solar Physics, 275, 3

Science Reviews, doi:10.1007/s11214-015-0230-3

Cheung, M. C. M., & Isobe, H. 2014, Living Reviews in

Solar Physics, 11, 3.
https://doi.org/10.12942/lrsp-2014-3

Chollet, F., et al. 2015, Keras, https://keras.io, ,
Cun, Y. L., Denker, J. S., & Solla, S. A. 1990, in Advances

in Neural Information Processing Systems (Morgan
Kaufmann), 598–605

Eastwood, J. P., Biﬃs, E., Hapgood, M. A., et al. 2017,

Risk Analysis, 37, 206.
http://dx.doi.org/10.1111/risa.12765

Qin, Z., Yu, F., Liu, C., & Chen, X. 2018, arXiv

Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, Solar

Physics, 275, 229.

https://doi.org/10.1007/s11207-011-9842-2

Schunker, H., Birch, A. C., Cameron, R. H., et al. 2019,

A&A, 625, A53.

https://doi.org/10.1051/0004-6361/201834627

Schunker, H., Braun, D. C., Birch, A. C., Burston, R. B., &

Gizon, L. 2016, A&A, 595, A107.

https://doi.org/10.1051/0004-6361/201628388

Fan, Y. 2009, Living Reviews in Solar Physics, 6, 4.

https://doi.org/10.12942/lrsp-2009-4

Selvaraju, R. R., Cogswell, M., Das, A., et al. 2017, 2017

IEEE International Conference on Computer Vision

Frankle, J., Dziugaite, G. K., Roy, D. M., & Carbin, M.

(ICCV), 618

2019, in arXiv

Shelhamer, E., Long, J., & Darrell, T. 2017, IEEE Trans.

Glorot, X., & Bengio, Y. 2010, in Proceedings of Machine

Pattern Anal. Mach. Intell., 39, 640.

Learning Research, Vol. 9, Proceedings of the Thirteenth
International Conference on Artiﬁcial Intelligence and
Statistics, ed. Y. W. Teh & M. Titterington (Chia
Laguna Resort, Sardinia, Italy: PMLR), 249–256.
http://proceedings.mlr.press/v9/glorot10a.html

Goodfellow, I., Bengio, Y., & Courville, A. 2016, Deep

Learning (The MIT Press)

Han, S., Pool, J., Tran, J., & Dally, W. J. 2015, in NIPS
Hastie, T., Tibshirani, R., & Friedman, J. 2001, The
Elements of Statistical Learning, Springer Series in
Statistics (New York, NY, USA: Springer New York Inc.)

Illarionov, E., Tlatov, A., & Sokoloﬀ, D. 2015, Solar

Physics, 290, 351.
https://doi.org/10.1007/s11207-014-0612-9

https://doi.org/10.1109/TPAMI.2016.2572683

Shibata, K., & Magara, T. 2011, Living Reviews in Solar

Physics, 8, 6. https://doi.org/10.12942/lrsp-2011-6

Silverman, B. W. 1986, Density Estimation for Statistics

and Data Analysis (London: Chapman & Hall)

Simonyan, K., Vedaldi, A., & Zisserman, A. 2013, CoRR,

abs/1312.6034

Simonyan, K., & Zisserman, A. 2014, CoRR, abs/1409.1556

Stein, R. F. 2012, Living Reviews in Solar Physics, 9, 4.

https://doi.org/10.12942/lrsp-2012-4

Thompson, W. T. 2006, A&A, 449, 791.

https://doi.org/10.1051/0004-6361:20054262

Wilson, P. R. 1994, The origin of the large-scale ﬁelds,

Komm, R., Howe, R., & Hill, F. 2009, Solar Physics, 258,

Cambridge Astrophysics (Cambridge University Press),

13. https://doi.org/10.1007/s11207-009-9398-6

142172

—. 2011, Solar Physics, 268, 407.

https://doi.org/10.1007/s11207-010-9692-3

Krizhevsky, A., Sutskever, I., & Hinton, G. E. 2012, in
Advances in Neural Information Processing Systems

LeCun, Y., Bengio, Y., & Hinton, G. 2015, Nature, 521, 436

EP . https://doi.org/10.1038/nature14539

Leka, K. D., Barnes, G., Birch, A. C., et al. 2012, The

Astrophysical Journal, 762, 130. https:
//doi.org/10.1088%2F0004-637x%2F762%2F2%2F130

Zeiler, M., & Fergus, R. 2014, in Lecture Notes in

Computer Science (including subseries Lecture Notes in

Artiﬁcial Intelligence and Lecture Notes in

Bioinformatics), Vol. 8689 LNCS, Computer Vision,

ECCV 2014 - 13th European Conference, Proceedings,

part 1 edn. (Springer Verlag), 818–833

Zhao, Z.-Q., Zheng, P., Xu, S.-t., & Wu, X. 2018, arXiv

e-prints, arXiv:1807.05511

A deep neural network consists of many layers of neurons as shown in Figure 19. The ith neuron in the nth layer

performs the following generic operation

APPENDIX

A. NEURAL NETWORKS.

19

N
(cid:88)

xn
i = f (

j=1

wn

ijxn−1

j + bn

i ).

(A1)

Here, xn−1 are the outputs from the neurons in the (n − 1)th layer (the 0th layer is the input layer), wn
ij is the weight
for the jth input from the (n − 1)th layer, bn
is termed the bias and f is the activation function for the neuron. The
i
activation applied on the ﬁnal layer produces the output ypred. The error in the predicted output is determined by a loss
function L(y, ypred). During training, weights and biases of the neurons in the network are determined via stochastic
gradient descent to minimize the loss L(y, ypred). Here, we use binary cross-entropy (CE) as the loss function, which
is a popular choice for classiﬁcation problems (Hastie et al. 2001). It is deﬁned as

LCE(y, ypred) = −

1
N

N
(cid:88)

i=1

(cid:104)
yilog

(cid:16)

ypred
i

(cid:17)

+ (1 − yi) log

(cid:16)

1 − ypred
i

(cid:17)(cid:105)

,

(A2)

where N is the batch-size, i.e., the number of examples considered in the stochastic gradient descent. The training
process involves many cycles of iterations (epochs) over the entire training set, and loss function LCE converges
gradually during training.

B. SUB-POPULATION OF PE AND NE SAMPLES.

The attribute of the PE and NE magnetograms that stands out for the CNN classiﬁcation is the total unsigned line-
of-sight magnetic ﬂux MTOT (Section 4). Therefore, we analyse MTOT of PE and NE samples from each category
of the CNN output bins. The distribution of PE and NE samples as per MTOT is shown in Figure 20. For the
statistical analysis binned as per MTOT (Figure 13), it is desirable that each bin contains suﬃcient number of samples
for meaningful statistics. We therefore select samples with MTOT value between 4 × 1020 Mx and 16 × 1020 Mx (both
inclusive). For each bin within this MTOT range, number of samples available are large > 1500.

Neural network pruning was primarily proposed to reduce the computation and memory required for tasks such as
vision and speech recognition or natural language processing on embedded mobile applications (Han et al. 2015). The
pruned network, signiﬁcantly reduced in size compared to the original, is also easier to interpret (Frankle et al. 2019).

C. NETWORK PRUNING

Figure 19. A two-layer deep fully connected (FC) neural network with four inputs and one output,

Input LayerHidden Layer 1Hidden Layer 2Output Layer20

Figure 20. Distribution of pre-emergence (PE) and non-emergence (NE) samples taken at 3.2 h before emergence according
to the total unsigned line-of-sight magnetic ﬂux (MTOT). We select samples with 4.0 × 1020 Mx ≤MTOT≤ 16.0 × 1020 Mx for
the statistical analysis.

A straightforward approach to network pruning is to identify important neuron connections during the training step
and remove all connections with weights below a threshold value (Han et al. 2015). This process may be repeated in
an iterative manner during training to obtain a compressed trained network. In the functionality based approach, one
can remove the neurons from the network that are functionally similar to the other neurons (Qin et al. 2018).

Here, we perform pruning of the ﬁnal convolutional layer of the CNN, in an iterative manner, to reduce the number
of ﬁlters from 256 to 4. Our pruning strategy is based on identifying the convolutional ﬁlters that maximally contribute
towards the network output. In the CNN (Figure 6), the ﬁnal convolutional layer is connected to the output layer
through a 6 × 6 max-pooling layer. The max-pooling layer picks out the pixel with the maximum value from the 6 × 6
output of a convolutional ﬁlter. Thus for a given input, the contribution of each ﬁlter in the ﬁnal layer is given by

Fi = max(Oi)wi,

(C3)

where Oi is the 6 × 6 output of the i − th convolutional ﬁlter in the ﬁnal convolutional layer and wi is the corresponding
weight. The output of the CNN is obtained by applying sigmoid function (Hastie et al. 2001) to the sum of the output
of each convolutional ﬁlter

y = sigmoid(

N
(cid:88)

i=1

Fi + b),

(C4)

where b is the bias of the output neuron and N is the number of convolutional ﬁlters in the ﬁnal layer. The output y, a
value between 0 and 1, is a measure of the probability of the input magnetogram showing emergence in a certain time t.
The contribution of CNN ﬁlters Fi, which correspond to the surface magnetic-ﬁeld patterns correlated (anti-correlated)
with emergence, increases (decreases) the CNN output y.

We develop a pruning algorithm to retain top four ﬁlters while maintaining prediction accuracy — two each corre-
sponding to patterns correlated and anti-correlated with emergence. We initiate the training with N = 256 ﬁlters in
the ﬁnal convolutional layer and perform the pruning operation during training, repeatedly applied every few epochs.
At each pruning step, half the number of ﬁlters associated with patterns minimally correlated and anti-correlated with
emergence are removed.

We identify the ﬁlters to be pruned in the following manner. At each pruning step during training, we calculate
of each convolution ﬁlter i for all samples nTP predicted as true positives (TPs) from the

the total contribution STP
training data

i

STP

i =

nTP
(cid:88)

j=1

Fi.

(C5)

048121620242832MTOT (1020 Mx)015003000450060007500Count21

Similarly, we calculate the total contribution STN
data. Finally, we calculate net positive contribution Snet
samples from the training data

i

i

for all samples predicted as true negatives (TNs) from the training
of each convolutional ﬁlter i for all accurately classiﬁed

i = STP
Snet

i − STN

i

.

(C6)

For a convolutional ﬁlter associated with patterns maximally correlated with emergence Snet
i >> 0. Likewise, for
a convolutional ﬁlter associated with patterns maximally anti-correlated with emergence Snet
i << 0. Thus, at each
value and N/4 ﬁlters with minimum Snet
pruning step, we retain N/4 ﬁlters with maximum Snet
value. The remaining
i
N/2 ﬁlters, which are relatively neutral to the patterns associated with emergence, are removed. The network is trained
further for a pre-deﬁned number of epochs, after which it is pruned again to remove half the number of neutral ﬁlters.
This is repeated until only four ﬁlters remain in the ﬁnal layer — the dominant two corresponding each to patterns
correlated and anti-correlated with emergence. The step by step algorithm is as follows.

i

1. Initialize the CNN (Figure 6) with the ﬁnal convolutional layer ﬁlters N = 256.
2. Initialize a function for the number of training epochs E(N ) as a function of the number of the ﬁnal convolutional

layer ﬁlters.

3. Train the network for epochs E(N ).
4. Using the trained network, obtain TP samples nTP and TN samples nTN of all PE and NE magnetograms in the

training data.

5. Calculate the net contribution of each of the i ﬁlters Snet

i

(Eq. C6) towards the CNN output for all accurately

predicted PE and NE samples (TPs and TNs) in the training data.

6. Identify N/4 ﬁlters with maximum (positive) Snet

i

and N/4 ﬁlters with minimum (negative) Snet

i

. Remove the

remaining N/2 ﬁlters.

7. If N = 4, i.e. after pruning, stop and output the trained CNN model. Else go to 3.

Using the aforementioned algorithm, we train the CNN and iteratively prune the ﬁnal convolution ﬁlters from
N = 256 to N = 4. Training and validation losses as training progresses are shown in Figure 21. We see that the
losses converge in an identical manner, with discontinuities encountered at each pruning stage. The number of ﬁlters
in the ﬁnal convolutional layer is shown below the curve corresponding to each pruning stage. At each pruning step,
half the convolutional ﬁlters are removed according to the pruning algorithm. At the end of the ﬁnal epoch, pruning
is carried out for the last time, reducing the convolution ﬁlters from N = 8 to N = 4. The network is not retrained
subsequently.

Figure 21. Convergence of training and validation loss (Eq. A2) during training of the CNN using network pruning. At a
pruning stage, half the number of the CNN ﬁlters from the ﬁnal convolutional layer, which are least correlated with the surface
ﬁeld patterns associated with emergence, are removed. The pruning shows up as discontinuities in the training and validation
losses. The number of CNN ﬁlters in the ﬁnal convolutional layer at each training stage are indicated below the curve for that
corresponding stage. Pruning is carried out after every 32 epochs, except at the last stage, when it is allowed to run for 50
epochs. The ﬁnal pruning step is performed at the end of the training, yielding the trained CNN with 4 convolutional ﬁlters in
the ﬁnal layer. The CNN is not retrained subsequent to the ﬁnal pruning. We use slightly higher learning rate, lr = 5 × 10−7.

0255075100125150175200Epochs0.00.10.20.30.40.50.60.70.8Binary Cross-entropy Loss2561286432168trainingvalidation22

The CNN, with the ﬁnal convolution layer pruned to N = 4 ﬁlters, yields TSS = 82.50% ± 4.52% for classiﬁcation of
PE and NE samples (-3.2 h pre-emergence) which is comparable to the original trained CNN with N = 256 ﬁlters (Table
3). The CNN obtained from pruning a random assortment of N/2 ﬁlters in the ﬁnal convolution layer at each step
yields TSS = 34.71% ± 28.86%. This validates the algorithm developed for identifying ﬁlters in the ﬁnal convolutional
layer correlated and anti-correlated with surface magnetic-ﬁeld patterns associated with emergence. We analyse the
top four ﬁlters that are retained after the pruning to interpret the performance of the CNN for classiﬁcation of the PE
and NE magnetograms (Section 4). Note that, in this case, the CNN with N = 4 ﬁlters in the ﬁnal convolutional layer
from the outset and trained for an identical number of epochs yields TSS = 78.38%±6.60% which is also comparable to
the original trained CNN. Since, in general, the number for optimum ﬁlters (neurons) in a CNN (deep neural network)
is not known in advance, the functionality based pruning approach developed here is useful for future applications as
well. Further pruning the CNN to N = 2 ﬁlters in the ﬁnal layer yields a mean TSS of 68.9% with a signiﬁcantly
larger standard deviation of 17.1%.

D. RECOMBINING SUB-SAMPLED PE AND NE MAGNETOGRAMS.

The 10◦ × 10◦ PE and NE magnetograms used for training the CNN are randomly sub-sampled from 30◦ × 30◦ EARs
and CRs. Hence, PE and/or NE magnetograms sub-sampled from an EAR or CR may contain overlapping regions
(see Section 2). These PE and/or NE segments sub-sampled from an EAR or CR may be recombined to recover the
original EAR or CR in the following manner. Let the (i, j) pixel from an EAR or CR be included in Cij number of
PE and/or NE magnetogram samples from the EAR or CR. We obtain the net CNN output corresponding to each
pixel of the EAR or CR as

where yij
occlusion map A for each EAR or CR pixel is obtained as

k is the CNN output corresponding to k − th PE and/or NE sample from the EAR or CR. Similarly, the

yij = (1/Cij)

Cij
(cid:88)

k=1

yk
ij,

(D7)

Aij = (1/Cij)

Cij
(cid:88)

k=1

Ak
ij,

(D8)

k is the occlusion map corresponding to the k−th PE and/or NE sample from the EAR or CR. By recombining
where Aij
PE and/or NE magnetogram segments into the original EAR or CR, we comprehensively inspect outputs and occlusion
maps of all PE and/or NE magnetograms that are part of the respective EAR or CR.

E. MULTI-VARIABLE DISCRIMINANT ANALYSIS OF THE LINE-OF-SIGHT UNSIGNED MAGNETIC FLUX

AND DIPOLE MOMENT.

Through visual inspection and statistical analysis of accurately classiﬁed PE and NE magnetograms, we ﬁnd that
MTOT is an important factor for the CNN classiﬁcation. As noted in Figure 2, appearance of a small-scale bipolar
ﬁeld is an early sign of emergence. Therefore, bipolarity of the magnetograms may also be a factor important for the
CNN classiﬁcation.

We calculated a baseline TSS using a non-parametric discriminant analysis of only the unsigned line-of-sight magnetic
ﬂux (MTOT). We extend the baseline model to include a measure of ‘dipole moment’ of the magnetograms (Illarionov
et al. 2015; Wilson 1994) calculated as

DM =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i

(cid:12)
(cid:12)
(cid:12)
Bi (ri − r0)
(cid:12)
(cid:12)

,

(E9)

where Bi is the magnetic ﬁeld of the ith pixel located at ri, r0 is a reference pixel and the sum is over all pixels in
the magnetogram. We estimate probability density of PE and NE magnetograms as a function of two variables —
MTOT and DM — using the Epanechnikov kernel (Silverman 1986; Barnes et al. 2014). Similar to the non-parametric
discriminant analysis of MTOT, the smoothing parameter for the density estimation is chosen to be optimum for a
normal distribution. For each cross-validation set, probability density for PE and NE magnetograms is estimated
using the training data. The trained density estimator is then used to obtained the probability score for PE and NE

23

Figure 22. Comparison of TSS obtained using non-parametric multi-variable discriminant analysis of the total unsigned line-
of-sight ﬂux (MTOT) and a measure of dipole moment (DM, E9) at diﬀerent pre-emergence times. The classiﬁcation TSS of
multi-variable discriminant analysis is comparable with the classiﬁcation TSS of the CNN and signiﬁcantly better than the TSS
obtained using discriminant analysis of only MTOT. The 1σ error bars are shown.

magnetograms in the validation data and a classiﬁcation label is obtained for calculating the TSS. Cross-validation
TSS for the multi-variable discriminant analysis for diﬀerent pre-emergence times thus obtained is plotted in Figure 22.
We see that the multi-variable discriminant analysis using MTOT and DM is comparable to the CNN classiﬁcation
results considering the error bars.

F. CNN CLASSIFICATION OF UNIFORM POSITIVE POLARITY PE AND NE MAGNETOGRAMS.

Our analysis shows that the CNN classiﬁcation depends on MTOT and outperforms the discriminant analysis of only
MTOT. We also show that the discriminant analysis of MTOT and DM — a measure of bipolarity — is comparable
to the CNN classiﬁcation. This suggests that the bipolarity of magnetograms may also be an important factor for
the CNN classiﬁcation. To validate this, we train the CNN (Figure 6) with uniform positive polarity PE and NE
magnetograms i.e. the absolute value of the line-of-sight ﬁeld. We ﬁnd that this CNN yields TSS comparable to the
CNN trained on original PE and NE magnetograms (see Table 5). Therefore, the polarity information in the data is
not necessary for the CNN to yield the classiﬁcation performance superior to the discriminant analysis of only MTOT.
To understand the operation of the CNN trained on uniform-polarity data, we train a CNN with N = 4 ﬁlters from
the outset in the ﬁnal layer and visualize the ﬁlter outputs in the ﬁnal layer corresponding to a synthetic magnetogram
with uniform ﬁeld similar to Figure 14. The CNN with N = 4 ﬁlters in the ﬁnal layer yields TSS of 71.75 ± 8.68%. The
ﬁnal layer ﬁlter outputs are shown in Figure 23. We ﬁnd that the ﬁlter outputs conform to the edges of magnetic-ﬂux
regions and are both positive as well as negative. Since the synthetic magnetogram contains no information about
the polarity, the positive and negative ﬁlter outputs do not necessarily correspond to the polarity of the magnetic-ﬂux
regions in Figure 23 and also in Figure 14. The ﬁlter outputs are a result of complex operations performed by the
CNN in the hidden layers and may not be easily interpreted.

time before
emergence (h)

5-fold cross-validation
TSS (%)

-3.2
-8.5
-13.9
-19.2
-24.5

88.20 ± 4.78
60.07 ± 1.53
50.66 ± 7.32
43.89 ± 7.30
40.68 ± 2.03

Table 5. Mean 5-fold cross-validation True Skill Statistics (TSS) for classiﬁcation of uniform-polarity pre-emergence (PE) and
non-emergence (NE) magnetograms at diﬀerent pre-emergence times obtained using the convolutional neural network (CNN,
Figure 6) with 5 hidden convolutional layers. The 1σ error is quoted.

-24.5-19.2-13.9-8.5-3.2Pre-emergence Time (h)0.020.040.060.080.0100.0True Skill Statistics (%)DA MTOTDA MTOT-DMCNN24

Figure 23. Outputs of ﬁlters in the ﬁnal convolutional layer of the CNN, with N = 4 ﬁlters in the ﬁnal layer, trained on
uniform-polarity PE and NE segments. The input is a synthetic magnetogram with 100 G uniform-polarity ﬁeld and radius
25 Mm.

Figure 24. The average CNN output (cid:104)y(cid:105) of PE and NE magnetogram samples binned by the unsigned line-of-sight magnetic
ﬂux MTOT. PE and NE samples considered are from MTOT range of 0 − 25 × 1020Mx. (cid:104)y(cid:105)NE is consistently lower than 0.2.
(cid:104)y(cid:105)PE is greater than 0.5 for an intermediate range of MTOT between 5 × 1020 − 20 × 1020Mx. 5σ error bars are shown.

G. CNN OUTPUT VS MTOT.

In Figure 11, we see that the average unsigned line-of-sight magnetic ﬂux (cid:104)MTOT(cid:105) of PE and NE samples binned
by the CNN output lies in an intermediate range of 9 × 1020 − 11 × 1020 Mx for samples with the CNN output y ∼ 1.
While this indicates a dependence of the CNN output on MTOT, the actual values of MTOT of samples with y ∼ 1
may not all lie in the intermediate range and may be higher as well as lower. To explicitly study the dependence of the
CNN output on MTOT, we consider the average CNN output of PE and NE samples binned by MTOT. We consider
samples from a slightly wider range of MTOT of 0 − 25 × 1020 Mx than considered earlier (Appendix B). The average
CNN output (cid:104)y(cid:105) is plotted in Figure 24. While (cid:104)y(cid:105)NE is consistently low < 0.2 and mostly independent of MTOT,
(cid:104)y(cid:105)PE is > 0.5 for samples with MTOT values between 5 × 1020 − 20 × 1020 Mx and < 0.5 for samples with lower or
higher MTOT values. Importantly, (cid:104)y(cid:105)P E is consistently and signiﬁcantly higher than (cid:104)y(cid:105)N E irrespective of MTOT.
This suggests that the CNN output depends on other important factors besides MTOT.

InputFilter 1Filter 2Filter 3Filter 40.55.410.315.220.125.0MTOT (1020 Mx)0.00.20.40.60.81.0mean CNN OutputPENE