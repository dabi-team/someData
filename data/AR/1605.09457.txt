6
1
0
2

y
a
M
1
3

]
T
S
.
h
t
a
m

[

1
v
7
5
4
9
0
.
5
0
6
1
:
v
i
X
r
a

Asymptotic properties of the maximum likelihood estimator for
nonlinear AR processes with markov-switching

Luis-Angel Rodr´ıguez
Dpto. de Matem´aticas, FACYT, Universidad de Carabobo, Venezuela
CIMFAV, Facultad de Ingenier´ıa, Universidad de Valpara´ıso, Chile.
Corresponding: larodri@uc.edu.ve

October 1, 2018

Abstract

In this note, we propose a new approach for the proof of the consistency and normality
of the maximum likelihood estimator for nonlinear AR processes with markov-switching un-
der the assumptions of uniform exponential forgetting of the prediction ﬁlter and α-mixing
property. We show that in the linear and Gaussian case our assumptions are fully satisﬁed.
Keywords: Nonlinear autoregressive process, Markov switching asymptotic normality,consistency,
hidden Markov chain.
MSC: Primary: 60G17, Secondary:62G07

Switching autoregressive processes with Markov regime can be considered as a combination
of hidden Markov models (HMM) and threshold regression models. They have been introduced
in an econometric context by Goldfeld and Quandt (1973) [5] and they have become quite pop-
ular in the literature since Hamilton (1989) [6] employed them in the analysis of the the rate
of growth of USA GNP series for two regimes: one of contraction and another of expansion.
This family of models describes the evolution of a time series subject to discrete shifts and the
transition is controlled by a HMM.

We consider a nonlinear AR process with markov-switching (abbreviated MS-NAR)

deﬁned for integers n

1 by

≥

Yn = r(Yn−1, θXn) + en, Yn

R.

∈

Yn
{

n≥0
}

(1)

Here the process
en
{
neous Markov chain with state space

n≥1 are i.i.d. random variables and the sequence
}

1, . . . , m
{

.
}

Xn
{

n≥1 is an homoge-
}

F

=

Let

, θ) : θ
r(
·
{
parameter θ = (θ1, . . . , θm)
transition matrix of the Markov chain
The parameter space is the set

}
∈

Θ

∈

a family of real valued functions deﬁned on Rm+1, indexed by a
Θ and Θ is a compact set of Rm. We denote by A the probability
Xn−1 = i).
|

n≥1, i.e. A = [aij], with aij = P(Xn = j
}

Xn
{

Ψ =






ψ = (θ, A) : θ

Θ, aij

∈

∈

[0, 1] and

1

m

j=1
X

.

aij = 1






 
 
 
 
 
 
We assume that the variable Y0, the Markov chain

mutually independent. The process
be carried out in terms of the observable process

Xn
{

n≥1 are
}
, called regime, is not observable and inference has to
}

n≥1 and the sequence
}

Xn
{

en
{

Yn
{

.
}

The consistency of the maximum likelihood estimator for the parameter ψ in the MS-NAR
model is given in Krishnarmurthy and Ryden (1998) [8], while the consistency and asymptotic
normality are proved in a more general context in the work of Douc et al. (2004) [3]. In the
section 2 we prove the consistency and asymptotic normality of the maximum likelihood esti-
mator for functional AR processes with markov-switching under the assumptions of exponential
uniform forgetting property for prediction ﬁlter and an α-mixing property.

1 General properties for MS-NAR model

In this section we review the key properties of the MS-NAR model that we need for proving our
results.

1.1 Stability and existence of moments

The study of the stability of the model MS-NAR is relatively complex. In this section we recall
known results about the stability of this model given by Yao and Attali [15]. Our aim is to
resume the suﬃcient conditions which ensure the existence and the uniqueness of a stationary
ergodic solution for the model, as well as the existence of moments of order s
1 of the respective
stationary distribution.

≥

E1 The Markov chain

Xn
{

n≥1 is positive recurrent. Hence, it has an invariant distribution
}

that we denote by µ = (µ1, . . . , µm).

E2 The functions y

→

r(y, θi), for i = 1, ..., m, are continuous.

E3 There exist positive constants ρi, bi, i = 1, ..., m, such that for y

inequality holds

E4 γ =

m
i=1 log ρiµi < 0.

r(y, θi)
|

| ≤

ρi

y
|

|

+ bi.

R, the following

∈

E5 E(
s) <
P
e1|
∞
|
E6 The sequence

, for some s

1.

≥

Φ with respect to the Lebesgue measure.

en
{

n≥1 of random variables admits a common density probability function
}

E7 There exist b > 0 and C a compact set of R such that infe∈C Φ(e) > b.

Condition E1 implies that

is a Markov pro-
cess. Under condition E2 this Markov process is a Feller chain and it is a strong Feller chain if
in addition the condition E6 holds.

n≥1 with states space R
(Yn, Xn)
}
{

1, . . . , m

× {

}

The model is called sublinear if conditions E2 and E3 hold. For the sublinear MS-NAR

model, Yao and Attali [15] proved the following result.

2

Proposition 1.1 Consider a sublinear MS-NAR
that

Yn
{

n≥0. Under assumptions E1-E7, we have
}

i) There exists a unique stationary geometric ergodic solution.

ii) If the spectral radius of the matrix Qs =

in E5, then E(
Yn
|

s) <
|

.

∞

ρs
j aij

(cid:16)

i,j=1...m

(cid:17)

is strictly less than 1, with s given

Remark 1.1 The Markov chain is stable under the moment condition s
totic properties of the MLE it will be necessary to assume s > 2.

≥

1, but for the asymp-

Now we introduce some notations:

V1:n stands for the random vector (V1, . . . , Vn), and by v1:n = (v1, . . . , vn) we mean a
realization of the respective random vector.

The symbol 1IB(x) denotes the indicator function of set B, which assigns the value 1 if
x

B and 0 otherwise.

∈

p(V1:n = v1:n) denotes the density distribution of random vector V1:n evaluated at v1:n.

•

•

•

We consider the following assumption :

D1 The random variable Y0 admits a density function p(Y0 = y0) with respect to Lebesgue

measure.

Under conditions D1 and E6, the random vector (Y0:n, X1:n) admits the probability density

p(Y0:n = y0:n, X1:n = x1:n) equal to

Φ(yn

−

r(yn−1, θxn))

Φ(y1 −

· · ·

with respect to the product measure λ
measures respectively. For a proof of this result see Ferm´ın et al [9].

⊗

r(y0, θxn))axn−1xn · · ·
µc, where λ and µc denote Lebesgue and counting

ax1x2µx1p(Y0 = y0),

1.2 Strong mixing

A strictly stationary stochastic process Y =

Yn
{

n∈Z is called strongly mixing, if
}

αn := sup

{|
b
a, with a, b
where
M
mixing if

∈

P(A

B)

P(A)P(B)
|

: A

0
−∞, B
∈ M

−

∩
Z, is the σ-algebra generated by

0,

as n

∞
n } →
k=a:b, and is absolutely regular
}

→ ∞

(2)

,

∈ M

Yk
{

βn := E

P(B
ess sup
{

0
−∞)
|M

−

P(B) : B

∈ M

0,

as n

.

→ ∞

→

(3)

The values αn and βn are called α-mixing and β-mixing coeﬃcients respectively. For proper-
ties and examples of processes under mixing assumptions, see Doukhan [4]. In general, we have
the inequality 2αn

βn

1.

(cid:0)

≤

≤

∞
n }
(cid:1)

3

Note that the α-mixing coeﬃcients can be rewritten as:

αn := sup

cov(φ, ξ)
|

{|

: 0

≤

φ, ξ

≤

1, φ

0
−∞, ξ
∈ M

∞
.
n }

∈ M

(4)

In the case of a strictly stationary Markov process X, with state space (E,
), kernel probability
transition A and invariant probability measure µ, the β-mixing coeﬃcients take the following
form (see Doukhan [4], section 2.4):

B

βn := E

sup

A(n)(X, B)

{|

(cid:16)

µ(B)
|

−

: B

∈ B}

(cid:17)

.

(5)

Lemma 1.1 Under conditions E1-E7 the process MS-NAR is α-mixing with α-mixing coeﬃ-
cients decreasing geometrically.

Proof: For the proof of this lemma see Ferm´ın et al [9].

Example 1.1 (Linear autoregressive with Markov switching (MS-AR) nonmixing)

In the case where r(y, (bi, ρi)t) = ρiy + bi, the model is a MS-AR and it is deﬁned by:

Yn = ρXnYn−1 + bXn + en.

(6)

For each 1

i

≤

≤

m, we denote θi = (bi, ρi)t and

θ =

(cid:18)

b1
b2
ρ1 ρ2

· · ·
· · ·

bm
ρm (cid:19)

.

More speciﬁcally consider the process MS-AR with θi = (0, ρi)t for all i = 1, . . . , m and such
that the random variable e1 follows a Bernoulli distribution with parameter q and Y0 = 0. In
this case, we have

n−1

Yn =

ρXk · · ·

ρX1ek+1,

and we adopt the convention that ρXk · · ·
fact, according to D. Andrews [1] if 0 < ρi
n , with P(A) > 0, P(Bs)
∞
Bt

∈ M

Xk=0
ρX1 = 1 for k = 0. This process is non α-mixing. In
0
−∞,
∈ M
A) = 1, therefore
|

∈
c for some constant c < 1 such that P(Bt

N there exist some sets A

1/2, for t

≤

αt(Y )

P(A

Bt)

∩

−

≥

≤
P(A)P(Bt) = P(A)(P(Bt

P(Bt))

A)
|

−

≥

P(A)(1

c).

−

This implies that αt(Y ) does not tend to 0 as t

and so Y is a non α-mixing process.

→ ∞

Lemma 1.2 Under conditions E1-E7, the MS-NAR process

Yn
{

n≥0 satisﬁes,
}

i) For all function ϕ such that E(ϕ(Yk)) <

∞

, we have the strong law of large numbers,

1
n

n

Xk=1

ϕ(Yk)

→

E(ϕ(Y1)), a.s.

4

ii) Suppose that E(ϕ(Y1)) = 0, E

∞
k=1 k + E(ϕ(Y1)ϕ(Yk)) <

s <
ϕ(Y1)
|
|
and if Γ
∞

∞
= 0,

2

P

1
√n

n

k=1
X

ϕ(Yk)

→ N

(0, Γ),

, for some s > 2. Then Γ = E(ϕ(Y1)2) +

for n

→ ∞

, in distribution.

Proof: i) This result is a direct consequence of the Collolary 3.1 in Rio [10].

ii) Let Uk = ϕ(Yk), then
E(
Uk
|

r) <
|

∞

k≥0 is a strictly stationary sequence and is strongly α-mixing, with
Uk
}
{
, for r > 2. For α−1(u) = inf

N : αk

, we have to prove
u
}

≤

k
{

∈

1

0

Z

α−1(u)Q2(u)du <

∞

(7)

where Q is the associate quantile function of the process

Uk
{

. The condition (7) is implied by
}

(i + 1)

2
r−2 αi <

i≥0
X

,

∞

(8)

and in our case this is valid, since from geometric α-mixing property exist 0 < ζ < 1 such that
αi

Cζ i, we have

≤

2
r−2 αi

(i + 1)

i≥0
X

C

≤

(i + 1)

i≥0
X

2

r−2 ζ i <

.

∞

Thus, we can apply Theorem 4.2. in E. R´ıo [10], obtaining that √nUn converges in distribution
to

(0, Γ).

N

2 Maximum likelihood estimation

Using pψ as a generic simbol for densities and distributions parameterized for ψ. We deﬁned
the conditional log-likelihood as ln(ψ) = log pψ(Y1:n

Y0) and we can expressed as
|

n

ln(ψ) =

log pψ(Yk

k=1
X

Y0:k−1).
|

We denote by ψ∗ the true parameter wich is consider as ﬁxed. A maximum likelihood

estimator (MLE) is deﬁned by

ˆψn = arg max

ψ

ln(ψ).

The MLE is consistent if ˆψn

ψ∗ as n

a.s.

→ ∞

→

The techniques standard used to prove consistency follows the steps:

1. To show that there exists a continuous deterministic function l(ψ) such that

lim
n→∞

1
n

ln(ψ) = l(ψ) a.s.

5

6
2. To show that l(ψ) a.s. has a unique maximum at ψ = ψ∗.

3. To conclude that ˆψn = arg maxψ n−1ln(ψ)

arg maxψ l(ψ) = ψ∗.

→

For MS-NAR processes a strong law of large numbers of the log-likelihood is obtained in
Rynkiewicz [12], Krishnamurthy [7] using an additive function of the extended Markov chain
(Yn, Xn, Pψ(Xk
Y0:n)). In Douc et. al. [3] the law of large numbers of the log-likelihood follow
|
from uniform exponential forgetting of the initial distribution for prediction ﬁlter.

In this work, following the approach of consistency proof of Handel, chapter 7 in [13], for
HMM, and joined to the α-mixing property we obtain a new proof of the consistency for the MLE.

The following lemma shows that we can express pψ(Yk

ﬁlter Pψ(Xk

Y0:n).
|

Lemma 2.1 Let δ = infi,j=1:m aij. Deﬁne,

Y0:k−1) as a functional of the prediction
|

Dψ

k,l = log

pψ(Yk

Z Z

Y0:k−1, xk)axk−1,xk
|

Yl:k−1)µc(dxk)µc(dxk−1),

for 0 < l < k. Under assumptions E1 and E7 then

k,l −
Proof: First, we bound from below the quantities exp(Dψ
we have

Dψ

k,0| ≤
k,0) y exp(Dψ

2δ−1(1

δ)k−1−l.

−

k,l), by the Fubini Theorem

P(xk−1|
Dψ
|

exp(Dψ

k,0)

and the same for exp(Dψ

k,l), thus

δ

≥

Z

pψ(Yk

y0:k−1, xk)µc(dxk)
|

min(exp(Dψ

k,0), exp(Dψ

k,l))

δ

≥

Z

pψ(Yk

Y0:k−1, xk)µc(dxk).
|

Using inequality

log x

log y

x

| ≤ |

y

/ min(x, y), we estimate
|

−

−

|

Dψ
|

k,l −

Dψ

k,0|
pψ(Yk

Y0:k−1, xk)axk−1,xk (Pψ(xk−1|
Pψ(xk−1|
|
−
Y0:k−1, xk)µc(dxk)
pψ(Yk
δ
|
Pψ(Xk−1 ∈ ·|

y0:k−1)
V T
k

Yl:k−1)

Yl:k−1)

−

R

Pψ(Xk−1 ∈ ·|

≤ R R
1
δ k

≤

Y0:k−1))µc(dxk)µc(dxk−1)

Applying the Proposition 4.3.26 (iii) in Cappe et. al.

[2], p´ag 109,

Pψ(Xk
k
Dψ
|

Dψ

∈ ·|

Yl:k)

−
2δ−1(1

Pψ(Xk

∈ ·|
δ)k−1−l.

We conclude that

k,l −
This lemma shows that the quantity Dψ

k,0| ≤

−

Y0:k)
V T
k

2(1

−

≤

δ)k−l

approximated by Dψ

k,l which is a function of only a ﬁxed number of observations Yl:k.

k,0, which depends on the observations Y0:k, can be

6

Proposition 2.1 Under assumptions E1-E7, suppose Ψ is a compact set and the condition
Eψ∗(pψ(Yk
, for i = 1, . . . , m. Then ln(ψ) is a continuous function and l(ψ) =
limn→∞ n−1ln(ψ) exist a.s for each ψ

Yk−1, i)) <
|

Ψ.

∞

∈

Proof: The proof is done in two steps. First, we have

l(ψ) = lim
k→∞

Eψ∗ (Dψ

k ), exists for every ψ

Ψ.

∈

Second, we show

then conclude

1
n

ln(ψ) =

1
n

1
n

n

Xk=1

n

(Dψ

k −

Eψ∗(Dψ

k ))

→

0 a.s.

(Dψ

k −

Xk=1

Eψ∗(Dψ

k )) +

1
n

n

Xk=1

Eψ∗(Dψ
k )

→

l(ψ) a.s.

Step 1. Let ∆k = Eψ∗(Dψ

k,l) by Lemma 2.1,

m+n,0 −
hence supn |
, i.e.,
∆m
gent. By Ces`aros theorem Eψ∗(ln(ψ)) = n−1(

∆m+n
|
∆m+n

0 as m

→ ∞

∆m

| →

Eψ∗ (Dψ
|

−

=

−

|

Eψ∗(Dψ

m+n,m))

| ≤

2δ−1(1

δ)m−1

−

is a Cauchy sequence and therefore conver-

∆k
}
{
n−1
k=0 ∆k) also converges.

Dψ
Step 2. According to Proposition 1.1 the sequence
k }
{
Dψ
cients αk. We demostrate that E(
, in fact
k |
|
Pψ(xk−1|

Y0:k−1, xk)axk−1,xk
|

pψ(Yk

) <

∞

P

Yl:k−1)µc(dxk)µc(dxk−1)

Z Z
and

k≥1 is α-mixing with geometric coeﬃ-

pψ(Yk

Y0:k−1, xk)µc(dxk)
|

≤

Z

m

i=1
X

pψ(Yk

Y0:k−1, xk)µc(dxk) =
|

Z

under assumption E(pψ∗(Yk

Yk−1, i)) <
|

∞

pψ(Yk

Yk−1, i)µi
|

≤

m max

i=1:m{

pψ(Yk

Yk−1, i)µi
|

}

Dψ
, then E(
k |
|

) <

∞

and by Lemma 1.2, i) we obtain

1
n

n

Xk=1

(Dψ

k −

Eψ∗(Dψ

k ))

→

0 a.s.

We prove the validity of step three under uniform convergence, supψ∈Ψ |

ln(ψ)

−

l(ψ)

| →

0.

Lemma 2.2 Suppose Ψ is a compact set. Let ln : Ψ
that converges uniformly to a function l : Ψ

→
R. Then

→

R be a sequence of continuous functions

ˆψn = arg max

ψ

ln(ψ)

arg max

ψ

l(ψ)

→

7

Proof: As a continuous function on a compact space attains its maximum, we can ﬁnd a
ψn

arg maxψ ln(ψ) for all n. Which show using an argument that goes to Wald (1949) that

∈

lim
n→∞

l(ψn) = sup
ψ∈Ψ

l(ψ).

(9)

Suppose that the sequence

does not converge to the set

ψn
{

}

ψ′
By compactness there exists a subsequence
n} ⊂ {
{
. But l(ψ) is continuous, so l(ψ′
maxψ∈Ψ l(ψ)
n)
→
}
this is a contradiction.

˜ψ :
{
which converges to ψ′

l( ˜ψ) = maxψ∈Ψ l(ψ)
.
}
l( ˜ψ) =
ψn
l(ψ′) < supψ∈Ψ l(ψ) and according to (9),

˜ψ :

6∈ {

}

Theorem 2.1 Suppose Ψ is a compact set. Assume that

1. ψ = ψ∗ iﬀ Pψ = Pψ∗.

2. For all i, j

1, . . . , m

and all y, y′

y

Y0 = y′, X1 = i) are continous.
|

∈ {

}

R

×

∈

R the functions ψ

aij and ψ

pψ(Y1 =

→

→

3. There is a c <

such that

∞

Dψ
|

k −

Dψ′

k | ≤

ψ
c
k

−

ψ′

k

for all k > 1

Then the maximum likelihood estimate ˆψn is consistent.

Proof: By Theorem 7.5 in Handel [13], the Lipschitz condition 3. and compactness implies that
the sequence ln

l a.s uniformly. According to Lemma 2.1

→

and this value is unique under identiﬁability.

ˆψn

→

ψ∗ = arg max

ψ

l(ψ),

In the Gaussian and linear case we can prove directly identiﬁability and equicontinuity. This
allows us obtain the consistency of the MLE without assuming a condition of Lipschitz for the
parameters.

Example 2.1 (MS-AR gaussian linear)

}

en
{

Let the model deﬁned by (6). Let

are gaussian i.i.d. random variables. Our goal in this
example is check that the conditions for consistency apply in this case. In fact, if we assume that
for the true model Ψ∗ the vector components
m
i=1 are diﬀerent; thus, for every n,
(αi, bi, σi)
}
{
m
(αiYn−1 + bi, σi)
there exists a point Yn−1 ∈
i=1 are diﬀerent. Therefore, in agree-
}
{
ment with Remark 2.10 of Krishnamurthy and Yin [7] the model is identiﬁable in the following
sense: If K stands for the Kullback-Leibler divergence K(ψ, ψ∗) = 0 then, ψ = ψ∗, which proves
the identiﬁability. On the another hand, the Lemma 4.1 in [11] follows that 1
Y0 = y0)
is an equicontinuos sequence a.s-Pψ∗ . We conclude that in this case the MLE is consistent.

n log pψ(Y n
1 |

R such that

There is a standard technique for prove asymptotic normality of maximum likelihood esti-
mates. The idea is that the ﬁrst derivatives of a smooth function must vanish at its maximum.
If we expand in Taylor series the likelihood gradient around ψ∗, we can write

0 =

∇

ψln( ˆψn) =

ψln(ψ∗) +

ψln( ˜ψ)( ˆψn
2

ψ∗)

−

∇

∇

8

where ˜ψ = t ˆψn + (1

−

t)ψ∗. Normalizing this expansion with √n we obtain

√n( ˆψn

ψ∗) =

(
∇

−

−

ψln( ˜ψ))−1(
2

∇

ψln(ψ∗))√n.

In order to obtain the asymptotic normality of the maximum likelihood estimator we assume

that exist an open neighborhood Br(ψ∗) of ψ∗ such that the following statements hold.

H1 The functions ψ

A and ψ

→
H2 There exist functions f0, f1, f2 such that

→

pψ(Y1|

Y0, i) are twice continuously diﬀerentiable on Br(ψ∗).

sup

ψ∈Br(ψ∗) k∇

and

ψpψ(y1|

y0, i)

k ≤

f0(y1, y0),

sup

ψ∈Br (ψ∗) k∇

2

ψpψ(y1|

y0, i)

k ≤

f1(y1, y0),

sup

ψ∈Br(ψ∗) k∇

ψpψ(y1|

y0, i)

k ≤

with E(fs(Y1, Y0)) <

∞

, s = 0, 1 and E(f2(Y1, Y0)r) <

f2(y1, y0),

, r > 2.

∞

Theorem 2.2 Under assumptions of Theorem 2.1 and H1-H2, assume that J(ψ∗) = var(
is non-singular and ψ∗

˚Ψ. Then, as n

,

∇

ψl(ψ∗))

∈

→ ∞
J(ψ∗), in probability.

(
∇

2

ψn−1ln( ˜ψ))

→

i)

−
ii) √n

ψn−1ln(ψ∗)

∇

→

N (0, J(ψ∗)), in distribution.

Moreover, we conclude that √n( ˆψn

ψ∗)

−

→ N

(0, J(ψ∗)−1), in distribution.

Proof: Under H1-H2 if we take ϕ() = ∂2 log n−1ln

∂ψ2

the Lemma 1.2 implies that

and if ϕ() = ∂n−1 log ln

∂ψ

, then

ψn−1ln(ψ∗)

√n

∇

→

N (0, J(ψ∗))

ψn−1ln(ψ∗))
2

(
∇

−
ψ∗ we can prove

J(ψ∗), a.s.

→

lim
n→∞

1
n ∇

2
ψln(ψn)

1
n ∇

−

2
ψln(ψ∗) = 0,

For a sequence ψn

→

in probability.

Let us ﬁrst observe that 1

ψln(ψn) = 1
2
n

n
k=1 ∇

2
ψ log pψ(Yk

Y0:k−1). Another hand,
|

n ∇
∂2 log pψ
∂ψj∂ψi

=

P
∂2pψ
∂ψj∂ψi −

1
pψ

∂ log pψ
∂ψj

∂ log pψ
∂ψj

1
p2
ψ

9

hence,

∂2 log pψn
∂ψj∂ψi −

∂2 log pψ∗
∂ψj∂ψi

1
pψn

∂2pψn
∂ψj∂ψi −

1
pψ∗

∂2pψ∗
∂ψj∂ψi (cid:19)

+

∂pψ∗
∂ψj

∂pψ∗
∂ψj

1
p2
ψ∗ −

∂pψn
∂ψj

∂pψn
∂ψj

1
p2
ψn !

=

(cid:18)

= T1 + T2.

For term T2, by deﬁnition of ψ∗,

∂pψ∗
∂ψj

= 0 and by the Ergodic theorem we have

lim
n→∞

1
n

n

Xk=1

∂pψn
∂ψj

= 0.

For the term T1,

1
pψ∗

1
pψn

∂2pψ∗
∂ψj∂ψi

∂2pψn
∂ψj∂ψi −

1
pψn (cid:18)
pψn}
{
1
pψ∗ , a.s. Using H2 we obtain

Under equicontinuity of the sequence
E7 1

=

pψn →

∂2pψn
∂ψj∂ψi −

∂2pψ∗
∂ψj ∂ψi (cid:19)
n≥1 we have, pψn →

+

1
pψn −

1
pψ∗

∂2pψ∗
∂ψj∂ψi

(cid:18)
pψ∗ and by conditions E1 and

(cid:19)

E

∂2pψ∗
∂ψj∂ψi (cid:19)

(cid:18)

<

.

∞

Let

w(r, Y0:k) = sup

∂2pψn(Yk

Y0:k)
|
∂ψj∂ψi

∂2pψ∗ (Yk

Y0:k)
|
∂ψj∂ψi

−

ψn∈Br(ψ∗) (cid:12)
(cid:12)
(cid:12)
proceeding as in Lemma 3 of Vandekerkhove [14], by Markov inequality
(cid:12)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
k

n

k=1
X
P

1
k

P

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤

∂2pψn(Yk

Y0:k)
|
∂ψj ∂ψi

1
k

−

n

w(r, Y0:k) > ǫ

n

∂2pψ∗ (Yk

Y0:k)
|
∂ψj∂ψi

k=1
X
E(w(r, Y0:k))

−

!

> ǫ

!

(cid:12)
(cid:12)
(cid:12)
(cid:12)
+ P (ψn
(cid:12)

Br(ψ∗))

6∈

Xk=1
E(w(r, Y0:k))

≤

ǫ

−

E(w(r, Y0:k))

ψn
{

}
ψ∗) =

+ P (ψn

6∈

Br(ψ∗)) .

(10)

The condicion H2 implies that E(w(r, Y0:k))

we obtain that E(w(r, Y0:k))
strong convergence of

0, as n

→
→ ∞
to ψ∗. Hence (10) goes to 0.

2f1. Using the Lebesgue continuity theorem,
. The second term goes to 0 as n to inﬁnity by

≤

Finally, as √n( ˆψn

√n, using i) the ﬁrst factor in the
above expression tends to J(ψ∗). The second factor converges weakly to N (0, J(ψ∗)) by ii).
Slutsky’s theorem implies that √n( ˆψn

(0, J(ψ∗)−1).

ψln(ψ∗)
(cid:17)

ψln( ˜ψ))−1(

ψ∗)

∇

∇

−

−

(cid:16)

2

−

→ N

Example 2.2 (MS-AR gaussian again)

10

 
 
We employ the asymptotic results obtained to verify the validity of a likelihood test for identi-
ﬁng when the parameter ρ, of a MS-AR is the zero vector. In this case the MS-AR process is a
hidden Markov model.

Expanding ln(ρ) in Taylor series around ˆρ, we have

2(ln(ˆρ)

−

−

ln(0)) = ˆρ2

∂2ln(˜ρ)
∂ρ2

(cid:19)

−

(cid:18)

and by Theorem 2.1 ˆρ

J(0)

(0, 1) and as J(˜ρ)/J(0)

→ N

1 then ˆρ2J(0)

χ2
1.

→

→

p

Acknowledgments. The author is grateful for the partial support on the projects Anillo
ACT1112 and GEMINI-CONICYT 2012 NO. 32120025 CR 211291055 REXE 04464-14. Re-
search facilities and hospitality in CIMFAV of the Universidad de Valpara´ıso and by the sa-
bbatical support from the Universidad de Carabobo. The author also thanks Dr K. Bertin for
carefully reading a preliminary version.

References

[1] D. Andrews. Non-strong mixing autoregressive procesess. J. Appl Prob., 21:930–934, 1984.

[2] O. Cappe, E. Moulines, and T. Ryd´en. Inference in Hidden Markov Models. Springer-

Verlag, 2005.

[3] R. Douc, E. Moulines, and T. Ryd´en. Asymptotic properties of the maximum likelihood
estimator in autoregressive models with Markov regime. Ann. Statist., 32:2254–2304, 2004.

[4] P. Doukhan. Mixing: Propierties and Examples., volume 85. Lecture Notes in Statist.,

1994.

[5] S. M. Goldfeld and R. Quandt. A Markov Model for Switching Regressions. Journal of

Econometrics, 1:3–16, 1973.

[6] J.D. Hamilton. A new approach to the economic analysis of non stationary time series and

the business cycle. Econometrica, pages 357–384, 1989.

[7] V. Krishnamurthy. Recursive Algorithms for estimation of hidden Markov Models with

markov regime. IEEE Trans. Information theory, 48(2):458–476, 2002.

[8] V. Krishnamurthy and T. Ryd´en. Consistent estimation of linear and non-linear autore-

gressive models with Markov regime. Journal of Time Series Analysis, 19:291–307, 1998.

[9] Ferm´ın L., R´ıos, and L. A. Rodr´ıguez. A Robbins Monro algorithm for nonparametric
estimation of NAR process with Markov-Switching: consistency. arXiv:1407.3747v6, 2014.

[10] E. Rio. Th´eorie asymptotique des processus faiblement d´ependents, volume 31. Springer-

SMAI: Paris., 2000.

[11] R. R´ıos and L. A. Rodr´ıguez. Penalized estimate of the number of states in gaussian linear

ar with markov regime. Electronic Journal of Statistics, pages 1111–1128, 2008.

11

[12] J. Rynkiewicz. Mod´eles hybrides int´egrant des r´eseaux de neurones artiﬁciels `a des modeles
de chaˆınes de Markov cachee: application `a la prediction de series temporelles . PhD thesis,
Universite Par´ıs I, 2000.

[13] R. v. Handel. Hidden Markov Models. Lecture notes: https://www.princeton.edu/ rvan/,

2008.

[14] P. Vandekerkhove. Consistent and asymptotically normal parameter estimates for hidden

Markov mixtures of Markov models. Bernoulli, 11:103–129, 2005.

[15] J. Yao and J. G. Attali. On stability of nonlinear AR process with Markov switching. Adv.

Applied Probab, 1999.

12

