Diformer: Directional Transformer for Neural Machine Translation

Minghan Wang1, Jiaxin Guo1, Yuxia Wang2, Daimeng Wei1, Hengchao Shang1,
Chang Su1, Yimeng Chen1, Yinglu Li1, Min Zhang1, Shimin Tao1, Hao Yang1
1Huawei Translation Services Center, Beijing, China
2The University of Melbourne, Melbourne, Australia
{wangminghan,guojiaxin1,weidaimeng,shanghengchao,
suchang8,chenyimeng,liyinglu,zhangmin186,
taoshimin,yanghao30}@huawei.com
yuxiaw@student.unimelb.edu.au

Abstract

Autoregressive (AR) and Non-autoregressive
(NAR) models have their own superiority
on the performance and latency, combining
them into one model may take advantage of
both. Current combination frameworks focus
more on the integration of multiple decoding
paradigms with a uniﬁed generative model, e.g.
Masked Language Model. However, the gen-
eralization can be harmful on the performance
due to the gap between training objective and
inference.
In this paper, we aim to close
the gap by preserving the original objective
of AR and NAR under a uniﬁed framework.
Speciﬁcally, we propose the Directional Trans-
former (Diformer) by jointly modelling AR
and NAR into three generation directions (left-
to-right, right-to-left and straight) with a newly
introduced direction variable, which works by
controlling the prediction of each token to
have speciﬁc dependencies under that direc-
tion. The uniﬁcation achieved by direction suc-
cessfully preserves the original dependency as-
sumption used in AR and NAR, retaining both
generalization and performance. Experiments
on 4 WMT benchmarks demonstrate that Di-
former outperforms current united-modelling
works with more than 1.5 BLEU points for
both AR and NAR decoding, and is also com-
petitive to the state-of-the-art independent AR
and NAR models.

1

Introduction

Machine translation can be considered as a condi-
tional generation task, which has been dominated
by neural networks, especially after Transformer
(Vaswani et al., 2017). Conventional autoregressive
(AR) NMT models obtain the impressive perfor-
mance, but it’s time-consuming to decode token
one by one sequentially (Sutskever et al., 2014;
Bahdanau et al., 2015). Aiming at fast inference,
non-autoregressive (NAR) NMT models enhance
the parallelizability by reducing or removing the

sequential dependency on the translation preﬁx in-
side the decoder, but suffering from performance
degradation owing to the multi-modality problem,
which is still an open-question (Gu et al., 2018;
Shu et al., 2020; Ghazvininejad et al., 2020; Lee
et al., 2018; Ghazvininejad et al., 2019; Stern et al.,
2019; Welleck et al., 2019; Gu et al., 2019a,b).

It’s always non-trivial to balance high perfor-
mance and low latency in a single model perfectly.
Therefore, another branch focuses on the uniﬁed-
modeling of multiple decoding paradigms so that
decoding with AR or NAR in different scenarios
(AR for quality-ﬁrst and NAR for speed-ﬁrst) with
one model can be achieved (Mansimov et al., 2020;
Tian et al., 2020; Qi et al., 2021), making the perfor-
mance and speed can be pursued more practically.
Whereas, challenges still exist. For example,
a generalized conditional language model is of-
ten required to support the generation with cus-
tomized orders or positions (Mansimov et al., 2020;
Tian et al., 2020), which actually prevents the
model from being fully trained on speciﬁc decoding
method, leading to the declines in overall perfor-
mance. In addition, in some works, AR and NAR
decoding may needs to be trained separately in the
stage of pretraining or ﬁne-tuning (Qi et al., 2021),
making the training more expensive.

To ameliorate these issues, we propose Direc-
tional Transformer (Diformer) which resolve the
uniﬁcation of AR and NAR in a more practical
way. First of all, we abandon the compatible of
multiple ﬂexible decoding strategies, but focusing
on the modeling of some commonly used strategies
that have good performance. For the AR decoding,
it has been proved that monotonic linear genera-
tion is still considered as the best strategy (Mansi-
mov et al., 2020; Tian et al., 2020), so we choose
to only model the left-to-right (L2R) and right-to-
left (R2L) generation. For the NAR decoding, we
choose to follow the stream of masked-language

1
2
0
2
c
e
D
1
3

]
L
C
.
s
c
[

2
v
2
3
6
1
1
.
2
1
1
2
:
v
i
X
r
a

 
 
 
 
 
 
model, like mask-predict in CMLM (Ghazvinine-
jad et al., 2019) or parallel easy-ﬁrst in Disco
(Kasai et al., 2020), since they are simpler than
insertion-based method but still being effective.

To this end, we unify two decoding paradigms
into three generation directions — L2R, R2L and
straight, and formulate it through a new objective
named as Directional Language Model (DLM),
making the prediction of tokens conditioned on
contexts controlled by a newly introduced direction
variable. It ties AR and NAR into a uniﬁed genera-
tion framework while still preserving the original
dependency assumptions of AR and NAR, retain-
ing both generalization and performance. Mean-
while, all directions can be trained simultaneously
with the time spent equally to the training of an
independent NAR model, which greatly reduces
the training cost compared to two-stages methods.
Experimental results on the WMT14 En↔De
and WMT16 En↔Ro datasets for all three direc-
tions indicate that Diformer performs better than
previous uniﬁcation-based works by more than 1.5
BLEU points. Comparing to other state-of-the-art
independent AR and NAR models, Diformer is also
competitive when decoding in the same mode. We
summarize contributions of our work as:

• We unify the AR and NAR decoding into three
generation direction and formulate it with the
Directional Language Model.

• We propose the Diformer, a Transformer-
based model that can be trained with DLM,
where all direction can be trained simultane-
ously.

• Experiments on WMT14 En↔De and
WMT16 En↔Ro demonstrate the ability of
Diformer with competitive results compared
to uniﬁed or independent models.

1.1 Related Work

(Mansimov et al., 2020) uniﬁes decoding in di-
rected and undirected models by a generalized
framework, in which the generating process is fac-
torized as the position selection and the symbol
replacement, where the ﬁrst step is achieved by
Gibbs sampling or learned adaptive strategies, the
second step can be handled by a masked language
model pretrained on monolingual corpora and ﬁne-
tuned on the NMT task. Their model supports at
least 5 decoding strategies including hand-crafted
and learned, all of them can be used for both linear

time decoding (AR) and constant time decoding
(NAR).

Similarly, Tian et al. (2020) uniﬁed AR and NAR
by adapting permutation language modeling objec-
tive of XLNet to conditional generation, making it
possible to generate a sentence in any order. The
model is evaluated to decode in monotonic and non-
monotonic AR, semi-AR and NAR with at least 8
position selection strategies including pre-deﬁned
and adaptive.

Both of them achieves the compatible to cus-
tomized decoding through position selection and
applying the selected positions/orders on a gener-
alized generative model, which leads to the gap
between training and inference. In contrast to the
position selection, we directly model the decod-
ing process with three generation directions in a
task-speciﬁc manner, thereby without introducing
additional complexity to the task and close the gap
between training objective and inference strategy.
We consider it is worthwhile to obtain performance
improvements by abandon some ﬂexibility.

2 Method

2.1 Background

Before the description of Diformer, the conven-
tional AR model and the iterative mask prediction
based NAR model that applied in Diformer will be
introduced ﬁrst.

The likelihood of an AR model is a factorization
following the product rule, assuming each token
is conditioned on all previous generated context.
Taking the L2R and R2L AR model as examples:

LL2R =

LR2L =

N
(cid:88)

i=1
N
(cid:88)

i=1

log P (yi|y1:i−1, X; θ)

(1)

log P (yi|yi+1:N , X; θ)

(2)

where X is the source text, y1:i−1 and yi+1:N are
previous outputs in opposite direction, θ is the
learnable parameters, N is the target length.

In the iterative-reﬁnement based NAR model
like CMLM (Ghazvininejad et al., 2019), the con-
ditional dependency is loosed, assuming the pre-
diction of target token can be independent with
each other, but conditioned on the output tokens
(context) from last iteration:

LCMLM =

(cid:88)

yi∈Y (t)
mask

logP (yi|X, Y (t)

obs ; θ).

(3)

where t is the iteration step t = {1, ..., T }, Yobs
are observable tokens (context), Ymask = Y \ Yobs
are masked tokens for predicting. In each iteration,
N T −t
T of predicted tokens with low conﬁdence will
be re-masked and predicted again in the next iter-
ation, conditioned on remaining high-conﬁdence
predictions as observable context until the last it-
eration. At the initial iteration, the model deter-
mines the target length N based on the source text
P (N |X) and makes the ﬁrst step prediction with
N − 2 mask symbols as well as [BOS] and [EOS]
input to the decoder, equivalent to merely condi-
tioned on the source.

Instead of using the global context, in DisCo
(Kasai et al., 2020), the target token at each posi-
tion is predicted with different context, namely, the
disentangled context. In such case, all tokens can
be used for training and updated at each iteration
during inference:

LDisCo =

N
(cid:88)

i=1

logP (yi|X, Y i,t

obs; θ),

(4)

where Y i,t
obs is the context only for yi. The parallel
easy-ﬁrst decoding strategy is proposed (we call it
easy ﬁrst in following sections for simplicity) to
improve the decoding efﬁciency, where the context
of each token is composed by predictions at easier
positions determined in the ﬁrst iteration:

Y i,t
obs = {yt−1

j

|z(j) < z(i)},

(5)

where z(i) denotes the descending ordered rank of
the probability Pi computed in the ﬁrst iteration.
During the training of CMLM and DisCo, a sub-
set of tokens are selected as the context, CMLM
updates parameters only with the loss on masked
tokens while DisCO uses all tokens for updating.

In the Diformer, we aim to unify the two exclu-
sive dependency assumptions (Yang et al., 2019)
of AR and NAR essentially by proposing a new
training objective and model architecture that can
make them trained jointly.

2.2 Directional Language Model

We aim to unify the AR and NAR decoding
into three generation directions — L2R, R2L and
straight, i.e. making prediction on the target token
at the rightward, leftward and the original position.
How to realize this goal is an open-question. In this
work, we achieve it by explicitly providing a direc-
tion instruction and corresponded contexts to the

model. Taking an example on the target sequence
Y = [A, B, C, D, E], the probability of y3 = C
generated from three directions can be expressed
as:

P3 =






P (y3 = C|X, {A, B})
P (y3 = C|X, {D, E})
P (y3 = C|X, {A, B, ?, D, E})

L2R
R2L
straight

where ? can be a mask symbol performing like a
placeholder.

Formally, given the target sequence Y =
[y1, ..., yN ], token yi can be generated from direc-
tion zi ∈ Z = {R, S, L} (i.e. L2R, straight and
R2L) given the context Yzi and X:

P (yzi|X, Yzi),

where Yzi is determined by the direction zi:

Yzi =






y1:i−1
yi+1:N
Y i
obs

zi = R
zi = L
zi = S

When zi = R or L, the model works exactly same
to the conventional AR model by conditioning on
previously generated tokens at leftwards or right-
wards. When zi = S, the model works in an
iterative-reﬁnement manner (e.g. mask-predict in
CMLM or parallel easy-ﬁrst in DisCO) by condi-
tioning on a partially observed sequence Y i
obs with
multiple tokens being masked including yi, same
as the disentangled context in DisCo.

We can thereby formulate the objective of direc-
tional language model as the expectation over all
possible generation directions on each token:

P (Y |X) = Ezi∈Z

N
(cid:89)

(cid:2)

i=1

P (yzi|X, Yzi)(cid:3)

(6)

The expectation can be approximated with sam-
pling, similar to the permutation language model
in (Yang et al., 2019; Tian et al., 2020), where
a permutation of the sequence is sampled during
training, we, instead, sample the direction for each
token. In this way, the factorization of DLM incor-
porates both conditional dependency assumption of
AR, and conditional independence assumption of
NAR, thereby makes the training objective closely
related to the decoding methods.

Training The sampling of direction in DLM al-
lows us to train the generation of all directions
simultaneously, we introduce the detailed method
in this section.

As we all know that the training of Trans-
former (Vaswani et al., 2017) can be paralleled with
teacher forcing, achieved by feeding y1:N −1 (con-
text) to the model at once and computing the loss
on y2:N (target). The context and target sequence
can be easily created by a shifting operation that
aligns yi−1 to yi.

Diformer can also be trained in a similar way, but
before that, we have to make a slight change when
implementing the computation of the likelihood in
Eq 6 due to the difﬁculty of creating the context
sequence Yzi with complicated dependencies. The
original equation aims to compute the likelihood
on the ground-truth sequence Y where each token
is conditioned on a customized context determined
by the sampled direction, meaning that the context
sequence cannot be shared as Transformer does.
Creating specialized context for every token is non-
trivial especially when encountered with position
changing caused by the shifting when zi = R or L.
For the convenience of the implementation, we
ﬁx the input sequence y1:N and create a new target
sequence Y ∗ where tokens are accordingly shifted
with the sampled directions:

P (Y ∗|X) =

N
(cid:89)

i=1

P (yj|X, Yzi),

(7)

where j = i + 1 for zi = R, j = i − 1 for zi = L
and j = i for zi = S. When training on large
corpus with random sampling on directions, we
can say that P (Y ∗|X) ≈ P (Y |X) theoretically.

Formally, let the source and target sequence as
X = [x1, ..., x|X|] and Y = [y1, ..., yN ] where N
is the target length. Then, we uniformly sample
a direction instruction sequence Z = [z1, ..., zN ]
with N elements, where z1 and zN are ﬁxed to be
R and L as they are [BOS] and [EOS], which can
only be used to predict tokens inside the sequence
for the AR setting, and can never be masked in the
NAR setting.

The input sequence Yin is created by directly
copying from ground-truth Y , which will be
masked accordingly in the decoder to create the
disentangled context.

According to the sampled direction sequence Z,
we can now create the modiﬁed target sequence

Figure 1: An example of training Diformer with DLM,
where values in grids are the relative distance of K, V
w.r.t Q, attention masks are indicated by dark grids.

Y ∗ by shifting tokens in Y based on zi, which is
shown in Figure 1.

To be compatible with the NAR decoding, we
also predict the target length P (N |X) with the
same way as (Ghazvininejad et al., 2019). Note that
the predicted length is only used for NAR decoding,
the AR decoding still terminates when [EOS] or
[BOS] is generated for L2R and R2L setting.

Finally, the cross-entropy loss is used for both
generation (LDLM) and length prediction (LLEN)
task, the overall loss can be obtained by adding
them together:

LDiformer = LDLM + λLLEN,

(8)

where λ is the factor on which the best performance
can be obtained with the value of 0.1, after searched
from 0.1 to 1.0 in the experiment.

2.3 Directional Transformer

Diformer is mainly built upon the Transformer
(Vaswani et al., 2017) architecture but with sev-
eral modiﬁcations for the compatible of the multi-
directional generation, especially for avoiding the
information leakage during training.

Speciﬁcally, we directly use the standard Trans-
former encoder in the Diformer, except that an ad-
ditional MLP layer is added on top of it for length
prediction. For the decoder, several modiﬁcations
are performed: 1) We introduce an additional em-
bedding matrix to encode the direction instruction.
2) The original uni-directed positional embedding
is expended to a bi-directed positional embedding.

012345-101234-1-20000111223-1-1-1-2-2-2-3-3-3-4-4-5++++++3) We follow the work in DisCo to disentangle the
context by de-contextualizing K, V only with word
embedding, and replacing the input of Q in the ﬁrst
layer only with direction and position signal. 4) To
compensate the removed positional information in
K, V , we integrate the relative positional embed-
ding in the self-attention, successfully resolved the
problem on information leakage and the compatible
of bi-directional generation. (see Appendix)

Directional Embeddings An embedding matrix
is used to map the categorical variable zi into the
hidden space, denoted as δ, δ(zi) ∈ Rdmodel where
dmodel is hidden size of the model. For simplic-
ity, we directly use zi to represent the embedded
direction at position i in following sections.

The joint training of L2R and R2L can be prob-
lematic with the positional embedding of the orig-
inal Transformer since the index is counted in a
uni-directed order, which can be used for cheating
under the bi-directional scenario since future posi-
tional index can leak information of the sentence
length.

To solve this, we propose to make the positional
embedding directed, achieved by encoding the po-
sition index counted oppositely based on the direc-
tion with separate parameters:

pzi =

(cid:40)−→
P e(
←−
P e(

−→
i )
←−
i )

zi = R or zi = S
zi = L

−→
P e and

←−
P e are different embedding matrics
−→
i )
←−
i ) accordingly. More detailed description

where
to encode position indices counting from L2R (
or R2L (
can be found in Figure 1.

Finally, we add encoded position and direction
embeddings together as the initial hidden-state
for the computation of Ql=0 in the ﬁrst self-
hl=0
i
attention layer h0

i = pzi + zi:

Directional Self-Attention In DisCo, to prevent
the information leakage from the disentangled con-
text, the input representation for computing K, V
is de-contextualized by directly reusing the projec-
tion of input embeddings ki, vi = Proj(wi + pi).
In Diformer, we have to further remove the po-
sitional information since the directed positional
embedding can still be used for cheating in the
computation of self-attention across layers.

Completely removing the the positional informa-
tion on K, V and only using the word-embedding
wi can be harmful to the performance. Therefore,

we propose an alternative solution by replacing the
removed absolute positional embedding with the
relative positional embedding proposed in (Shaw
et al., 2018) for two reasons: 1) The relative posi-
tion is computed in a 2 dimensional space, meaning
that pij and pkj for token yj is not shared between
yi and yk, which satisﬁes our requirements that
each token in the context should have the position
information only used for yi but not shared for yk.
2) The position information is only injected during
the computation of self-attention without affecting
the original word embedding used in K, V .

Formally, we directly use the method in (Shaw
et al., 2018) but replace the hidden representation
for computing K, V with word embeddings:

hl(cid:48)
i =

αij =

eij =

N
(cid:88)

j=1

αij(wjW V + pV
ij)

(cid:80)N

exp eij
k=1 exp eik
h(l−1)
i W Q(wjW K + pK
√
dhead

ij )(cid:62)

(9)

(10)

(11)

where hl(cid:48)
is the output of the self-attention in
i
current layer, wj is the word embedding, pV
ij, pK
ij
are embedded relative positions, W Q, W K, W V
are parameters for Q, K and V , hl−1
is the last
layer’s hidden state, dhead is the hidden size of a
single head. Two parameter matrics are used as
embeddings — ReK and ReV , with the shape of
[2k + 1, dhead], where k is the max length. pij is
obtained by embedding the distance between i and
j clipped by the maximum length k.

i

Finally, a customized attention mask (see Figure
1) is created during training to simulate speciﬁc
dependencies based on the sampled direction se-
quence Z with following rules:

• If zi = R, all tokens for j > i will be masked.

• If zi = L, all tokens for j < i will be masked.

• If zi = S, yi and a subset of randomly se-
lected tokens will be masked following the
method in (Ghazvininejad et al., 2020), ex-
cluding [BOS] and [EOS].

Inference Diformer can generate a sequence
with 4 modes including L2R and R2L for AR
decoding, mask-predict and parallel easy-ﬁrst for
NAR decoding.

For the AR decoding, the model works exactly
same as the conventional Transformer, except that

for each step, a ﬁxed direction zi = R or L should
also be also be given, together with previously gen-
erated tokens, making it a pure-linear autoregres-
sive generation. Beam search can be directly used
in both L2R and R2L decoding. For the NAR de-
coding, the model uses mask-predict or easy-ﬁrst
by applying speciﬁc masking operation during each
iteration, where all tokens are assigned with z = S.
Length beam can be used to further improve the
performance. Detailed examples are shown in the
Appendix.

More importantly, we ﬁnd that

the multi-
directional property of Diformer can be used for
reranking, which is quite beneﬁcial for the NAR de-
coding. Speciﬁcally, compared to other NAR mod-
els that uses an external AR model for reranking,
Diformer can do it all by its own without introduc-
ing additional computational costs. For example,
it ﬁrst reﬁnes 5 candidates with 8 iterations and
performs reranking with the rest of 2 iterations by
re-using the encoder states and scoring candidates
with L2R and R2L modes, which is equivalent to
the computational cost of a 10-stepped reﬁnement
reported in CMLM. The scores computed in two
directions are averaged to obtain the ﬁnal rank. Ex-
perimental results show that 8 steps of reﬁnement
+ 2 steps of reranking obtains signiﬁcant perfor-
mance improvements compared to 10 steps of re-
ﬁnement without re-ranking. It can also be used
for AR decoding, where all tokens are scored under
the reversed direction, e.g. generating with L2R
and scoring with R2L. We name this method as
self-reranking.

3 Experiments

3.1 Experimental Setup

Data We evaluate Diformer on 4 benchmarks in-
cluding WMT14 En↔De (4.5M sentence pairs)
and WMT16 En↔Ro (610k sentence pairs). The
data is preprocessed in the same way with (Vaswani
et al., 2017; Lee et al., 2018), where each sen-
tence is tokenized with Moses toolkit (Koehn et al.,
2007) and encoded into subwords using BPE (Sen-
nrich et al., 2016). We follow (Gu et al., 2018;
Ghazvininejad et al., 2019; Zhou et al., 2020) to
create the knowledge distilled (KD) data with L2R
Transformer-big and Transformer-base for En↔De
and En↔Ro, the reported performance in the over-
all results are all obtained by training on the KD
data.

Conﬁguration We follow the same conﬁgura-
tions with previous works (Vaswani et al., 2017;
Ghazvininejad et al., 2019, 2020) on hyperparam-
eters: n(encoder+decoder) layers = 6 + 6, nheads =
8, dhidden = 512, dF F N = 2048. For customized
components in Diformer, we tune the max relative
distance k in [1,8,16,256] and ﬁnd that k = 256
obtains best performance. Adam (Kingma and Ba,
2015) is used for optimization with 128k tokens per
batch on 8 V100 GPUs. The learning rate warms
up for 10k steps to 5e-4 and decays with inversed-
sqrt. Models for En↔De and En↔Ro are trained
for 300k and 100k steps, last 5 checkpoints are av-
eraged for ﬁnal evaluation. We set beam size as 4
and 5 for AR and NAR decoding. When decoding
in NAR mode, we set the max iteration for mask-
predict and easy-ﬁst decoding as 10 without using
any early-stopping strategy. For fair comparison,
we reduce the max iteration to 8 when decoding
with self-reranking in NAR model. Our model is
implemented with pytorch (Paszke et al., 2019) and
fairseq (Ott et al., 2019). BLEU (Papineni et al.,
2002) is used for evaluation.

3.2 Results & Analysis

We perform experiments on Diformer to evaluate
its performance on three generation directions with
four decoding strategies. We mainly compare Di-
former to three types of models: 1) the uniﬁed-
models that is able to decode with multiple strate-
gies, 2) pure AR model, i.e. standard Transformer,
3) pure NAR models. (see Table 1)

Comparison with uniﬁed models For the com-
parison to uniﬁed-models (Mansimov et al., 2020;
Tian et al., 2020), Diformer outperforms others in
all generation directions, by obtaining more than
1.5 BLEU.

As discussed in the section 1, their support on
multiple generation strategies is achieved by ap-
plying certain position selection strategy on the
masked language model or generating with certain
permutation with the permutation language model.
This creates the gap between the training and infer-
ence since a speciﬁc decoding strategy might not
be fully trained with the generalized objective as
analyzed in (Mansimov et al., 2020). So, compared
to both, we use the task-speciﬁc modelling in ex-
change for better performance by abandon certain
ﬂexibility, thus makes the learned distribution to be
same with the one used in decoding, which answers
why Diformer performs better.

T-big (Vaswani et al., 2017)
T-base (Vaswani et al., 2017)
T-big (our impl, En↔De teacher)
T-base (our impl, En↔Ro teacher)
T-base + distill

NAT (Gu et al., 2018)
iNAT (Lee et al., 2018)
InsT (Stern et al., 2019)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019b)
DisCO (Kasai et al., 2020)

Mansimov et al. (2020)
Tian et al. (2020)

Diformer (ours)
- L2R
- R2L
- mask-predict
- easy-ﬁrst

En-De

De-En

En-Ro

Ro-En

AR

NAR

AR

NAR

AR

NAR

AR

NAR

28.4
27.3
28.52
27.67
28.41

-
-
27.29
-
-
-

25.66
27.23

AR Models

-
-
32.10
31.12
31.69

NAR models

-
-
-
-
-
-

Uniﬁed models

30.58
-

-
-
-
-
-

23.20
25.48
-
30.53
-
31.31

28.63
-

-
-
-
-
-

19.17
21.61
27.41
27.03
27.27
27.34

24.53
26.35

-
-
-
35.29
35.21

-
-
-
-
-
-

-
-

-
-
-
-
-

29.79
29.32
-
33.08
-
33.22

-
-

-
-
-
34.02
33.87

-
-
-
-
-
-

-
-

-
-
-
-
-

31.44
30.19
-
33.31
33.26
33.25

-
-

28.35/28.68
28.58/28.50
-
-

-
-
27.51/27.99
27.35/27.84

31.58/31.76
32.00/31.78
-
-

-
-
31.05/31.35
31.21/31.68

35.06/35.16
35.17/35.13
-
-

-
-
33.62/34.37
33.58/34.23

33.84/33.92
33.90/33.90
-
-

-
-
32.68/33.11
32.97/33.34

Table 1: This table shows the overall performance of Diformer compared to the AR, NAR and uniﬁed models
when decoding with AR or NAR strategies. T-big/-base is the abbreviation of Transformer-big/-base. The BLEU
score using self-rerank (right) or not (left) is separated by /.

Comparison with AR models For the En↔De
dataset, since we use a larger teacher model
(Transformer-big), therefore, we only compare Di-
former with same sized Transformer-base trained
on the raw and distilled data. The Diformer out-
performs Transformer trained on the raw data with
a large margin and reaches the same level to the
one trained on distilled data. Interesting, the best
performance of Diformer are usually obtained by
the R2L decoding and the reranked results on L2R,
the reason of it will be further discussed in ablation
study sections. For the En↔Ro dataset, Diformer
can also obtain similar performance compared to
the same sized Transformer trained on the distilled
data produced by a same sized teacher.

Comparison with NAR models Diformer is
also competitive to a series of NAR models in-
cluding iterative-reﬁnement based and fully NAR
models. We speculate the strong performance of
Diformer comes from the joint training of AR and
NAR, since it is similar to the multi-task scenario,
where tasks are closely correlated but not same.
This could be beneﬁcial for the task that is more
difﬁcult i.e. NAR, because the learned common
knowledge on AR tasks could be directly used in it.
By applying the self-reranking method, Diformer
could obtain additional 0.5 BLEU over the strong
baseline.

Data Condition

R

L

mask-predict

easy-ﬁrst

T-base (our impl)
Raw data
Distilled data

Diformer
Raw data
Raw data (ﬁxed right)
Distilled data

27.67
28.41

-
-

27.21
27.63
28.35

27.08
-
28.55

-
-

24.12
-
27.51

-
-

24.18
-
27.35

Table 2: This table shows the performance of Trans-
former and Diformer trained on raw and distilled data
where T-base represents for Transformer-base. An ad-
ditional experiment with ﬁxed zi = R for all tokens is
also presented.

3.3 Ablation Study

In this section, we perform extra experiments to
investigate factors that could inﬂuence the perfor-
mance of Diformer and the mechanism behind it.
All experiments of ablation study are performed on
the WMT14 En→De dataset.

The inﬂuence of Knowledge Distillation We
train Diformer not only with distilled data but also
with raw data as shown in table 2. The degradation
of NAR decoding when training on raw data is not
surprising which is a common problem faced by
all NAR models. However, the performance of AR
decoding also degrades. We speculate that on the
raw data, the difﬁculty of learning to generate from

max k

R

L

mask-predict

easy-ﬁrst

256
16
8
1

28.35
28.51
28.13
26.81

28.58
28.48
28.25
26.85

27.51
27.25
26.58
18.78

27.35
27.32
26.71
19.53

Table 3: This table shows the performance of Diformer
with different max k.

straight and R2L increased signiﬁcantly, making
the model to allocate more capacity to ﬁt them,
resulting in the negative inﬂuence on the perfor-
mance of L2R. We verify this by ﬁxing zi = R for
all tokens and train the model on raw data. The re-
sult conﬁrms it because the performance recovers
to its original level. On the contrary, the knowl-
edge distilled data is cleaner and more monotonous
(Zhou et al., 2020), making it easier to learn for
all directions, and allows the model to allocate bal-
anced capacity on each direction. As for the better
performance obtained by R2L decoding, we con-
sider the reason is that, the R2L is able to learn
the distilled data generated by the L2R teacher in a
complementary manner, making it more efﬁcient
to learn the knowledge that cannot be learned by
L2R due to the same modeling method.

The Importance of Relative Position We also
demonstrate the importance of the relative posi-
tional embedding by evaluating the model with
different maximum relative distance k and obtain
the same conclusion (Shaw et al., 2018) — the dis-
tance should be at least 8. Meanwhile, we observe
that NAR is more sensitive to the positional infor-
mation, which is reasonable, since the decoding
of NAR is conditioned on the bi-directional con-
text, where the positional information contains both
distance and direction thereby is more important
compared to that in AR.

Improvements of Self-Reranking As shown in
the overall results, self-reranking is a useful method
to improve the performance especially for NAR de-
coding. For the AR decoding, the improvements is
not that signiﬁcant since the outputs are already
good enough for L2R or R2L, the tiny gap be-
tween reranking and generation direction cannot
provide enough help, which indicates that using
self-reranking for AR is not that proﬁtable com-
pared to NAR.

We further investigate its ability on NAR decod-
ing (mask-predict) given different max iteration
number and length beam size, as shown in Figure 2.

Figure 2: The heatmap shows the BLEU score decoded
with mask-predict when using self-reranking or not un-
der different max iteration number and length beam
size.

It clearly shows that without reranking, the incor-
rect selection on beam candidates may even reduce
the performance with larger beam size. The use
of self-reranking actually lets the performance and
beam-size positively correlated, meaning that ex-
changing 2 steps of iteration with self-reranking
can be proﬁtable with larger beam size. In prac-
tical usage of self-reranking, it is critical to ﬁnd
the optimal combination by balancing the beam
size and max iteration number so that both high
performance and low latency can be obtained.

4 Conclusion

In this paper, we present Directional Transformer
which is able to model the autoregressive and non-
autoregressive generation with a uniﬁed framework
named Directional Language Model which essen-
tially links two types of conditional language model
with three generation directions. Compared to
previous works, Diformer exchanges the gener-
alization on decoding strategies for better perfor-
mance and thereby only support 4 decoding strate-
gies. Experimental results on WMT14 En↔De
and WMT16 En↔Ro demonstrate that the uniﬁ-
cation of AR and NAR can be achieved by Di-
former without losing any performance. The bi-
directional property of Diformer allows it to per-
form self-reranking which is especially useful for
NAR decoding to improve performance with no
additional computational cost.

Except from machine translation, Diformer can
be easily extended to other tasks like language mod-
eling by removing the dependency on X. It has the
potential to unify the representation learning and
generation with a single model, which is actually
our ongoing work.

        E H D P  V L ] H         P D [  L W H U D W L R Q  Q X P E H U                                                                                                                                                                                                                   : L W K R X W  V H O I  U H U D Q N        E H D P  V L ] H                                                                                                                                                                                                                   : L W K  V H O I  U H U D Q NReferences

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In 3rd Inter-
learning to align and translate.
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Marjan Ghazvininejad, Vladimir Karpukhin, Luke
Zettlemoyer, and Omer Levy. 2020. Aligned cross
entropy for non-autoregressive machine translation.
In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020,
Virtual Event, volume 119 of Proceedings of Ma-
chine Learning Research, pages 3515–3523. PMLR.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel
decoding of conditional masked language models.
In Proceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019, pages 6111–
6120. Association for Computational Linguistics.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
tor O. K. Li, and Richard Socher. 2018. Non-
In 6th
autoregressive neural machine translation.
International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings. Open-
Review.net.

Jiatao Gu, Qi Liu, and Kyunghyun Cho. 2019a.
Insertion-based decoding with automatically in-
ferred generation order. Trans. Assoc. Comput. Lin-
guistics, 7:661–676.

Jiatao Gu, Changhan Wang, and Junbo Zhao. 2019b.
In Advances in Neural
Levenshtein transformer.
Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems
2019, NeurIPS 2019, December 8-14, 2019, Vancou-
ver, BC, Canada, pages 11179–11189.

Jungo Kasai, James Cross, Marjan Ghazvininejad, and
Jiatao Gu. 2020. Non-autoregressive machine trans-
lation with disentangled context transformer. In Pro-
ceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020,
Virtual Event, volume 119 of Proceedings of Ma-
chine Learning Research, pages 5144–5155. PMLR.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open

source toolkit for statistical machine translation. In
ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
June 23-30, 2007, Prague, Czech Republic. The As-
sociation for Computational Linguistics.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
In Pro-
quence modeling by iterative reﬁnement.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, Brussels, Bel-
gium, October 31 - November 4, 2018, pages 1173–
1182. Association for Computational Linguistics.

Elman Mansimov, Alex Wang, Sean Welleck, and
Kyunghyun Cho. 2020. A generalized framework of
sequence generation with application to undirected
sequence models.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
fairseq: A fast, extensible
Michael Auli. 2019.
In Proceedings of
toolkit for sequence modeling.
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019,
Minneapolis, MN, USA, June 2-7, 2019, Demonstra-
tions, pages 48–53. Association for Computational
Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA, pages 311–318. ACL.

Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch:
An imperative style, high-performance deep learn-
ing library. In Advances in Neural Information Pro-
cessing Systems 32, pages 8024–8035. Curran Asso-
ciates, Inc.

Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu
Chen, Dayiheng Liu, Kewen Tang, Houqiang Li,
Jiusheng Chen, Ruofei Zhang, Ming Zhou, and Nan
Duan. 2021. BANG: bridging autoregressive and
non-autoregressive generation with large scale pre-
In Proceedings of the 38th International
training.
Conference on Machine Learning, ICML 2021, 18-
24 July 2021, Virtual Event, volume 139 of Proceed-
ings of Machine Learning Research, pages 8630–
8639. PMLR.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-
bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.
Xlnet: Generalized autoregressive pretraining for
In Advances in Neural
language understanding.
Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems
2019, NeurIPS 2019, December 8-14, 2019, Vancou-
ver, BC, Canada, pages 5754–5764.

Chunting Zhou,

Jiatao Gu, and Graham Neubig.
2020. Understanding knowledge distillation in non-
In 8th Inter-
autoregressive machine translation.
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.

A Appendices

Meeting of the Association for Computational Lin-
guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers. The Association for
Computer Linguistics.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position repre-
In Proceedings of the 2018 Confer-
sentations.
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, NAACL-HLT, New Orleans,
Louisiana, USA, June 1-6, 2018, Volume 2 (Short
Papers), pages 464–468. Association for Computa-
tional Linguistics.

Raphael Shu, Jason Lee, Hideki Nakayama, and
Kyunghyun Cho. 2020.
Latent-variable non-
autoregressive neural machine translation with deter-
In The
ministic inference using a delta posterior.
Thirty-Fourth AAAI Conference on Artiﬁcial Intelli-
gence, AAAI 2020, The Thirty-Second Innovative Ap-
plications of Artiﬁcial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020, pages 8846–
8853. AAAI Press.

Mitchell Stern, William Chan, Jamie Kiros, and Jakob
Uszkoreit. 2019. Insertion transformer: Flexible se-
quence generation via insertion operations. In Pro-
ceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA, volume 97 of Pro-
ceedings of Machine Learning Research, pages
5976–5985. PMLR.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems 27: Annual Conference on Neural Informa-
tion Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada, pages 3104–3112.

Chao Tian, Yifei Wang, Hao Cheng, Yijiang Lian, and
Zhihua Zhang. 2020. Train once, and decode as
In Proceedings of the 28th International
you like.
Conference on Computational Linguistics, COLING
2020, Barcelona, Spain (Online), December 8-13,
2020, pages 280–293. International Committee on
Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, pages 5998–6008.

Sean Welleck, Kiant´e Brantley, Hal Daum´e III, and
Kyunghyun Cho. 2019. Non-monotonic sequential
In Proceedings of the 36th Inter-
text generation.
national Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Re-
search, pages 6716–6726. PMLR.

Figure 3: The architecture of the model (left) with the example of corresponded operations in key components
during training (right).

Step L2R

R2L

Mask-Predict

Easy-First

A
[B]
R

A B
[B] A
R R

A B C
[B] A B
R R R

A B C D
[B] A B C
R R R R

1

2

3

4

5

6

E
[E]
L

D E
E [E]
L L

[B] - - B - - [E]
[B] - - - - - [E]
S S S S S

[B] A A C C D [E]
[B] - - - - - [E]
S S S S S

[B] A B - - - [E]
[B] - - B - - [E]
S S S S S

[B] A B C C E [E]
[B] A A C C D [E]
S S S S S

C D E
D E [E]
L L L

[B] A B - - E [E]
[B] A B - - - [E]
S S S S S

[B] A B C D E [E]
[B] A B C C E [E]
S S S S S

B C D E
C D E [E]
L L L L

[B] A B C - E [E]
[B] A B - - E [E]
S S S S S

A B C D E
[B] A B C D
R R R R R

A B C D E
B C D E [E]
L L L L L

[B] A B C D E [E]
[B] A B C - E [E]
S S S S S

A B C D E [E]
[B] A B C D E
R R R R R R

[B] A B C D E
A B C D E [E]
L L L L L L

-

-

-

-

Table 4: An example of 4 decoding strategies with the output, context input and direction input placed in three
sub-rows, where [B] and [E] represent for [BOS] and [EOS] token, - represents for mask.

012345-101234-1-20000111223-1-1-1-2-2-2-3-3-3-4-4-5++++++Add &NormFeed ForwardAdd &NormMulti-Head AttentionInput EmbeddingAdd &NormFeed ForwardAdd &NormDirectional Self-AttentionDirectional EmbeddingsMulti-Head AttentionLinearSoftmaxAdd &NormOutputsLengthPredictorWordEmbedding