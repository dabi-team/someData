Diformer: Directional Transformer for Neural Machine Translation

Minghan Wang1, Jiaxin Guo1, Yuxia Wang2, Daimeng Wei1, Hengchao Shang1,
Chang Su1, Yimeng Chen1, Yinglu Li1, Min Zhang1, Shimin Tao1, Hao Yang1
1Huawei Translation Services Center, Beijing, China
2The University of Melbourne, Melbourne, Australia
{wangminghan,guojiaxin1,weidaimeng,shanghengchao,
suchang8,chenyimeng,liyinglu,zhangmin186,
taoshimin,yanghao30}@huawei.com
yuxiaw@student.unimelb.edu.au

Abstract

Autoregressive (AR) and Non-autoregressive
(NAR) models have their own superiority
on the performance and latency, combining
them into one model may take advantage of
both. Current combination frameworks focus
more on the integration of multiple decoding
paradigms with a uniÔ¨Åed generative model, e.g.
Masked Language Model. However, the gen-
eralization can be harmful on the performance
due to the gap between training objective and
inference.
In this paper, we aim to close
the gap by preserving the original objective
of AR and NAR under a uniÔ¨Åed framework.
SpeciÔ¨Åcally, we propose the Directional Trans-
former (Diformer) by jointly modelling AR
and NAR into three generation directions (left-
to-right, right-to-left and straight) with a newly
introduced direction variable, which works by
controlling the prediction of each token to
have speciÔ¨Åc dependencies under that direc-
tion. The uniÔ¨Åcation achieved by direction suc-
cessfully preserves the original dependency as-
sumption used in AR and NAR, retaining both
generalization and performance. Experiments
on 4 WMT benchmarks demonstrate that Di-
former outperforms current united-modelling
works with more than 1.5 BLEU points for
both AR and NAR decoding, and is also com-
petitive to the state-of-the-art independent AR
and NAR models.

1

Introduction

Machine translation can be considered as a condi-
tional generation task, which has been dominated
by neural networks, especially after Transformer
(Vaswani et al., 2017). Conventional autoregressive
(AR) NMT models obtain the impressive perfor-
mance, but it‚Äôs time-consuming to decode token
one by one sequentially (Sutskever et al., 2014;
Bahdanau et al., 2015). Aiming at fast inference,
non-autoregressive (NAR) NMT models enhance
the parallelizability by reducing or removing the

sequential dependency on the translation preÔ¨Åx in-
side the decoder, but suffering from performance
degradation owing to the multi-modality problem,
which is still an open-question (Gu et al., 2018;
Shu et al., 2020; Ghazvininejad et al., 2020; Lee
et al., 2018; Ghazvininejad et al., 2019; Stern et al.,
2019; Welleck et al., 2019; Gu et al., 2019a,b).

It‚Äôs always non-trivial to balance high perfor-
mance and low latency in a single model perfectly.
Therefore, another branch focuses on the uniÔ¨Åed-
modeling of multiple decoding paradigms so that
decoding with AR or NAR in different scenarios
(AR for quality-Ô¨Årst and NAR for speed-Ô¨Årst) with
one model can be achieved (Mansimov et al., 2020;
Tian et al., 2020; Qi et al., 2021), making the perfor-
mance and speed can be pursued more practically.
Whereas, challenges still exist. For example,
a generalized conditional language model is of-
ten required to support the generation with cus-
tomized orders or positions (Mansimov et al., 2020;
Tian et al., 2020), which actually prevents the
model from being fully trained on speciÔ¨Åc decoding
method, leading to the declines in overall perfor-
mance. In addition, in some works, AR and NAR
decoding may needs to be trained separately in the
stage of pretraining or Ô¨Åne-tuning (Qi et al., 2021),
making the training more expensive.

To ameliorate these issues, we propose Direc-
tional Transformer (Diformer) which resolve the
uniÔ¨Åcation of AR and NAR in a more practical
way. First of all, we abandon the compatible of
multiple Ô¨Çexible decoding strategies, but focusing
on the modeling of some commonly used strategies
that have good performance. For the AR decoding,
it has been proved that monotonic linear genera-
tion is still considered as the best strategy (Mansi-
mov et al., 2020; Tian et al., 2020), so we choose
to only model the left-to-right (L2R) and right-to-
left (R2L) generation. For the NAR decoding, we
choose to follow the stream of masked-language

1
2
0
2
c
e
D
1
3

]
L
C
.
s
c
[

2
v
2
3
6
1
1
.
2
1
1
2
:
v
i
X
r
a

 
 
 
 
 
 
model, like mask-predict in CMLM (Ghazvinine-
jad et al., 2019) or parallel easy-Ô¨Årst in Disco
(Kasai et al., 2020), since they are simpler than
insertion-based method but still being effective.

To this end, we unify two decoding paradigms
into three generation directions ‚Äî L2R, R2L and
straight, and formulate it through a new objective
named as Directional Language Model (DLM),
making the prediction of tokens conditioned on
contexts controlled by a newly introduced direction
variable. It ties AR and NAR into a uniÔ¨Åed genera-
tion framework while still preserving the original
dependency assumptions of AR and NAR, retain-
ing both generalization and performance. Mean-
while, all directions can be trained simultaneously
with the time spent equally to the training of an
independent NAR model, which greatly reduces
the training cost compared to two-stages methods.
Experimental results on the WMT14 En‚ÜîDe
and WMT16 En‚ÜîRo datasets for all three direc-
tions indicate that Diformer performs better than
previous uniÔ¨Åcation-based works by more than 1.5
BLEU points. Comparing to other state-of-the-art
independent AR and NAR models, Diformer is also
competitive when decoding in the same mode. We
summarize contributions of our work as:

‚Ä¢ We unify the AR and NAR decoding into three
generation direction and formulate it with the
Directional Language Model.

‚Ä¢ We propose the Diformer, a Transformer-
based model that can be trained with DLM,
where all direction can be trained simultane-
ously.

‚Ä¢ Experiments on WMT14 En‚ÜîDe and
WMT16 En‚ÜîRo demonstrate the ability of
Diformer with competitive results compared
to uniÔ¨Åed or independent models.

1.1 Related Work

(Mansimov et al., 2020) uniÔ¨Åes decoding in di-
rected and undirected models by a generalized
framework, in which the generating process is fac-
torized as the position selection and the symbol
replacement, where the Ô¨Årst step is achieved by
Gibbs sampling or learned adaptive strategies, the
second step can be handled by a masked language
model pretrained on monolingual corpora and Ô¨Åne-
tuned on the NMT task. Their model supports at
least 5 decoding strategies including hand-crafted
and learned, all of them can be used for both linear

time decoding (AR) and constant time decoding
(NAR).

Similarly, Tian et al. (2020) uniÔ¨Åed AR and NAR
by adapting permutation language modeling objec-
tive of XLNet to conditional generation, making it
possible to generate a sentence in any order. The
model is evaluated to decode in monotonic and non-
monotonic AR, semi-AR and NAR with at least 8
position selection strategies including pre-deÔ¨Åned
and adaptive.

Both of them achieves the compatible to cus-
tomized decoding through position selection and
applying the selected positions/orders on a gener-
alized generative model, which leads to the gap
between training and inference. In contrast to the
position selection, we directly model the decod-
ing process with three generation directions in a
task-speciÔ¨Åc manner, thereby without introducing
additional complexity to the task and close the gap
between training objective and inference strategy.
We consider it is worthwhile to obtain performance
improvements by abandon some Ô¨Çexibility.

2 Method

2.1 Background

Before the description of Diformer, the conven-
tional AR model and the iterative mask prediction
based NAR model that applied in Diformer will be
introduced Ô¨Årst.

The likelihood of an AR model is a factorization
following the product rule, assuming each token
is conditioned on all previous generated context.
Taking the L2R and R2L AR model as examples:

LL2R =

LR2L =

N
(cid:88)

i=1
N
(cid:88)

i=1

log P (yi|y1:i‚àí1, X; Œ∏)

(1)

log P (yi|yi+1:N , X; Œ∏)

(2)

where X is the source text, y1:i‚àí1 and yi+1:N are
previous outputs in opposite direction, Œ∏ is the
learnable parameters, N is the target length.

In the iterative-reÔ¨Ånement based NAR model
like CMLM (Ghazvininejad et al., 2019), the con-
ditional dependency is loosed, assuming the pre-
diction of target token can be independent with
each other, but conditioned on the output tokens
(context) from last iteration:

LCMLM =

(cid:88)

yi‚ààY (t)
mask

logP (yi|X, Y (t)

obs ; Œ∏).

(3)

where t is the iteration step t = {1, ..., T }, Yobs
are observable tokens (context), Ymask = Y \ Yobs
are masked tokens for predicting. In each iteration,
N T ‚àít
T of predicted tokens with low conÔ¨Ådence will
be re-masked and predicted again in the next iter-
ation, conditioned on remaining high-conÔ¨Ådence
predictions as observable context until the last it-
eration. At the initial iteration, the model deter-
mines the target length N based on the source text
P (N |X) and makes the Ô¨Årst step prediction with
N ‚àí 2 mask symbols as well as [BOS] and [EOS]
input to the decoder, equivalent to merely condi-
tioned on the source.

Instead of using the global context, in DisCo
(Kasai et al., 2020), the target token at each posi-
tion is predicted with different context, namely, the
disentangled context. In such case, all tokens can
be used for training and updated at each iteration
during inference:

LDisCo =

N
(cid:88)

i=1

logP (yi|X, Y i,t

obs; Œ∏),

(4)

where Y i,t
obs is the context only for yi. The parallel
easy-Ô¨Årst decoding strategy is proposed (we call it
easy Ô¨Årst in following sections for simplicity) to
improve the decoding efÔ¨Åciency, where the context
of each token is composed by predictions at easier
positions determined in the Ô¨Årst iteration:

Y i,t
obs = {yt‚àí1

j

|z(j) < z(i)},

(5)

where z(i) denotes the descending ordered rank of
the probability Pi computed in the Ô¨Årst iteration.
During the training of CMLM and DisCo, a sub-
set of tokens are selected as the context, CMLM
updates parameters only with the loss on masked
tokens while DisCO uses all tokens for updating.

In the Diformer, we aim to unify the two exclu-
sive dependency assumptions (Yang et al., 2019)
of AR and NAR essentially by proposing a new
training objective and model architecture that can
make them trained jointly.

2.2 Directional Language Model

We aim to unify the AR and NAR decoding
into three generation directions ‚Äî L2R, R2L and
straight, i.e. making prediction on the target token
at the rightward, leftward and the original position.
How to realize this goal is an open-question. In this
work, we achieve it by explicitly providing a direc-
tion instruction and corresponded contexts to the

model. Taking an example on the target sequence
Y = [A, B, C, D, E], the probability of y3 = C
generated from three directions can be expressed
as:

P3 =

Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

P (y3 = C|X, {A, B})
P (y3 = C|X, {D, E})
P (y3 = C|X, {A, B, ?, D, E})

L2R
R2L
straight

where ? can be a mask symbol performing like a
placeholder.

Formally, given the target sequence Y =
[y1, ..., yN ], token yi can be generated from direc-
tion zi ‚àà Z = {R, S, L} (i.e. L2R, straight and
R2L) given the context Yzi and X:

P (yzi|X, Yzi),

where Yzi is determined by the direction zi:

Yzi =

Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

y1:i‚àí1
yi+1:N
Y i
obs

zi = R
zi = L
zi = S

When zi = R or L, the model works exactly same
to the conventional AR model by conditioning on
previously generated tokens at leftwards or right-
wards. When zi = S, the model works in an
iterative-reÔ¨Ånement manner (e.g. mask-predict in
CMLM or parallel easy-Ô¨Årst in DisCO) by condi-
tioning on a partially observed sequence Y i
obs with
multiple tokens being masked including yi, same
as the disentangled context in DisCo.

We can thereby formulate the objective of direc-
tional language model as the expectation over all
possible generation directions on each token:

P (Y |X) = Ezi‚ààZ

N
(cid:89)

(cid:2)

i=1

P (yzi|X, Yzi)(cid:3)

(6)

The expectation can be approximated with sam-
pling, similar to the permutation language model
in (Yang et al., 2019; Tian et al., 2020), where
a permutation of the sequence is sampled during
training, we, instead, sample the direction for each
token. In this way, the factorization of DLM incor-
porates both conditional dependency assumption of
AR, and conditional independence assumption of
NAR, thereby makes the training objective closely
related to the decoding methods.

Training The sampling of direction in DLM al-
lows us to train the generation of all directions
simultaneously, we introduce the detailed method
in this section.

As we all know that the training of Trans-
former (Vaswani et al., 2017) can be paralleled with
teacher forcing, achieved by feeding y1:N ‚àí1 (con-
text) to the model at once and computing the loss
on y2:N (target). The context and target sequence
can be easily created by a shifting operation that
aligns yi‚àí1 to yi.

Diformer can also be trained in a similar way, but
before that, we have to make a slight change when
implementing the computation of the likelihood in
Eq 6 due to the difÔ¨Åculty of creating the context
sequence Yzi with complicated dependencies. The
original equation aims to compute the likelihood
on the ground-truth sequence Y where each token
is conditioned on a customized context determined
by the sampled direction, meaning that the context
sequence cannot be shared as Transformer does.
Creating specialized context for every token is non-
trivial especially when encountered with position
changing caused by the shifting when zi = R or L.
For the convenience of the implementation, we
Ô¨Åx the input sequence y1:N and create a new target
sequence Y ‚àó where tokens are accordingly shifted
with the sampled directions:

P (Y ‚àó|X) =

N
(cid:89)

i=1

P (yj|X, Yzi),

(7)

where j = i + 1 for zi = R, j = i ‚àí 1 for zi = L
and j = i for zi = S. When training on large
corpus with random sampling on directions, we
can say that P (Y ‚àó|X) ‚âà P (Y |X) theoretically.

Formally, let the source and target sequence as
X = [x1, ..., x|X|] and Y = [y1, ..., yN ] where N
is the target length. Then, we uniformly sample
a direction instruction sequence Z = [z1, ..., zN ]
with N elements, where z1 and zN are Ô¨Åxed to be
R and L as they are [BOS] and [EOS], which can
only be used to predict tokens inside the sequence
for the AR setting, and can never be masked in the
NAR setting.

The input sequence Yin is created by directly
copying from ground-truth Y , which will be
masked accordingly in the decoder to create the
disentangled context.

According to the sampled direction sequence Z,
we can now create the modiÔ¨Åed target sequence

Figure 1: An example of training Diformer with DLM,
where values in grids are the relative distance of K, V
w.r.t Q, attention masks are indicated by dark grids.

Y ‚àó by shifting tokens in Y based on zi, which is
shown in Figure 1.

To be compatible with the NAR decoding, we
also predict the target length P (N |X) with the
same way as (Ghazvininejad et al., 2019). Note that
the predicted length is only used for NAR decoding,
the AR decoding still terminates when [EOS] or
[BOS] is generated for L2R and R2L setting.

Finally, the cross-entropy loss is used for both
generation (LDLM) and length prediction (LLEN)
task, the overall loss can be obtained by adding
them together:

LDiformer = LDLM + ŒªLLEN,

(8)

where Œª is the factor on which the best performance
can be obtained with the value of 0.1, after searched
from 0.1 to 1.0 in the experiment.

2.3 Directional Transformer

Diformer is mainly built upon the Transformer
(Vaswani et al., 2017) architecture but with sev-
eral modiÔ¨Åcations for the compatible of the multi-
directional generation, especially for avoiding the
information leakage during training.

SpeciÔ¨Åcally, we directly use the standard Trans-
former encoder in the Diformer, except that an ad-
ditional MLP layer is added on top of it for length
prediction. For the decoder, several modiÔ¨Åcations
are performed: 1) We introduce an additional em-
bedding matrix to encode the direction instruction.
2) The original uni-directed positional embedding
is expended to a bi-directed positional embedding.

012345-101234-1-20000111223-1-1-1-2-2-2-3-3-3-4-4-5++++++3) We follow the work in DisCo to disentangle the
context by de-contextualizing K, V only with word
embedding, and replacing the input of Q in the Ô¨Årst
layer only with direction and position signal. 4) To
compensate the removed positional information in
K, V , we integrate the relative positional embed-
ding in the self-attention, successfully resolved the
problem on information leakage and the compatible
of bi-directional generation. (see Appendix)

Directional Embeddings An embedding matrix
is used to map the categorical variable zi into the
hidden space, denoted as Œ¥, Œ¥(zi) ‚àà Rdmodel where
dmodel is hidden size of the model. For simplic-
ity, we directly use zi to represent the embedded
direction at position i in following sections.

The joint training of L2R and R2L can be prob-
lematic with the positional embedding of the orig-
inal Transformer since the index is counted in a
uni-directed order, which can be used for cheating
under the bi-directional scenario since future posi-
tional index can leak information of the sentence
length.

To solve this, we propose to make the positional
embedding directed, achieved by encoding the po-
sition index counted oppositely based on the direc-
tion with separate parameters:

pzi =

(cid:40)‚àí‚Üí
P e(
‚Üê‚àí
P e(

‚àí‚Üí
i )
‚Üê‚àí
i )

zi = R or zi = S
zi = L

‚àí‚Üí
P e and

‚Üê‚àí
P e are different embedding matrics
‚àí‚Üí
i )
‚Üê‚àí
i ) accordingly. More detailed description

where
to encode position indices counting from L2R (
or R2L (
can be found in Figure 1.

Finally, we add encoded position and direction
embeddings together as the initial hidden-state
for the computation of Ql=0 in the Ô¨Årst self-
hl=0
i
attention layer h0

i = pzi + zi:

Directional Self-Attention In DisCo, to prevent
the information leakage from the disentangled con-
text, the input representation for computing K, V
is de-contextualized by directly reusing the projec-
tion of input embeddings ki, vi = Proj(wi + pi).
In Diformer, we have to further remove the po-
sitional information since the directed positional
embedding can still be used for cheating in the
computation of self-attention across layers.

Completely removing the the positional informa-
tion on K, V and only using the word-embedding
wi can be harmful to the performance. Therefore,

we propose an alternative solution by replacing the
removed absolute positional embedding with the
relative positional embedding proposed in (Shaw
et al., 2018) for two reasons: 1) The relative posi-
tion is computed in a 2 dimensional space, meaning
that pij and pkj for token yj is not shared between
yi and yk, which satisÔ¨Åes our requirements that
each token in the context should have the position
information only used for yi but not shared for yk.
2) The position information is only injected during
the computation of self-attention without affecting
the original word embedding used in K, V .

Formally, we directly use the method in (Shaw
et al., 2018) but replace the hidden representation
for computing K, V with word embeddings:

hl(cid:48)
i =

Œ±ij =

eij =

N
(cid:88)

j=1

Œ±ij(wjW V + pV
ij)

(cid:80)N

exp eij
k=1 exp eik
h(l‚àí1)
i W Q(wjW K + pK
‚àö
dhead

ij )(cid:62)

(9)

(10)

(11)

where hl(cid:48)
is the output of the self-attention in
i
current layer, wj is the word embedding, pV
ij, pK
ij
are embedded relative positions, W Q, W K, W V
are parameters for Q, K and V , hl‚àí1
is the last
layer‚Äôs hidden state, dhead is the hidden size of a
single head. Two parameter matrics are used as
embeddings ‚Äî ReK and ReV , with the shape of
[2k + 1, dhead], where k is the max length. pij is
obtained by embedding the distance between i and
j clipped by the maximum length k.

i

Finally, a customized attention mask (see Figure
1) is created during training to simulate speciÔ¨Åc
dependencies based on the sampled direction se-
quence Z with following rules:

‚Ä¢ If zi = R, all tokens for j > i will be masked.

‚Ä¢ If zi = L, all tokens for j < i will be masked.

‚Ä¢ If zi = S, yi and a subset of randomly se-
lected tokens will be masked following the
method in (Ghazvininejad et al., 2020), ex-
cluding [BOS] and [EOS].

Inference Diformer can generate a sequence
with 4 modes including L2R and R2L for AR
decoding, mask-predict and parallel easy-Ô¨Årst for
NAR decoding.

For the AR decoding, the model works exactly
same as the conventional Transformer, except that

for each step, a Ô¨Åxed direction zi = R or L should
also be also be given, together with previously gen-
erated tokens, making it a pure-linear autoregres-
sive generation. Beam search can be directly used
in both L2R and R2L decoding. For the NAR de-
coding, the model uses mask-predict or easy-Ô¨Årst
by applying speciÔ¨Åc masking operation during each
iteration, where all tokens are assigned with z = S.
Length beam can be used to further improve the
performance. Detailed examples are shown in the
Appendix.

More importantly, we Ô¨Ånd that

the multi-
directional property of Diformer can be used for
reranking, which is quite beneÔ¨Åcial for the NAR de-
coding. SpeciÔ¨Åcally, compared to other NAR mod-
els that uses an external AR model for reranking,
Diformer can do it all by its own without introduc-
ing additional computational costs. For example,
it Ô¨Årst reÔ¨Ånes 5 candidates with 8 iterations and
performs reranking with the rest of 2 iterations by
re-using the encoder states and scoring candidates
with L2R and R2L modes, which is equivalent to
the computational cost of a 10-stepped reÔ¨Ånement
reported in CMLM. The scores computed in two
directions are averaged to obtain the Ô¨Ånal rank. Ex-
perimental results show that 8 steps of reÔ¨Ånement
+ 2 steps of reranking obtains signiÔ¨Åcant perfor-
mance improvements compared to 10 steps of re-
Ô¨Ånement without re-ranking. It can also be used
for AR decoding, where all tokens are scored under
the reversed direction, e.g. generating with L2R
and scoring with R2L. We name this method as
self-reranking.

3 Experiments

3.1 Experimental Setup

Data We evaluate Diformer on 4 benchmarks in-
cluding WMT14 En‚ÜîDe (4.5M sentence pairs)
and WMT16 En‚ÜîRo (610k sentence pairs). The
data is preprocessed in the same way with (Vaswani
et al., 2017; Lee et al., 2018), where each sen-
tence is tokenized with Moses toolkit (Koehn et al.,
2007) and encoded into subwords using BPE (Sen-
nrich et al., 2016). We follow (Gu et al., 2018;
Ghazvininejad et al., 2019; Zhou et al., 2020) to
create the knowledge distilled (KD) data with L2R
Transformer-big and Transformer-base for En‚ÜîDe
and En‚ÜîRo, the reported performance in the over-
all results are all obtained by training on the KD
data.

ConÔ¨Åguration We follow the same conÔ¨Ågura-
tions with previous works (Vaswani et al., 2017;
Ghazvininejad et al., 2019, 2020) on hyperparam-
eters: n(encoder+decoder) layers = 6 + 6, nheads =
8, dhidden = 512, dF F N = 2048. For customized
components in Diformer, we tune the max relative
distance k in [1,8,16,256] and Ô¨Ånd that k = 256
obtains best performance. Adam (Kingma and Ba,
2015) is used for optimization with 128k tokens per
batch on 8 V100 GPUs. The learning rate warms
up for 10k steps to 5e-4 and decays with inversed-
sqrt. Models for En‚ÜîDe and En‚ÜîRo are trained
for 300k and 100k steps, last 5 checkpoints are av-
eraged for Ô¨Ånal evaluation. We set beam size as 4
and 5 for AR and NAR decoding. When decoding
in NAR mode, we set the max iteration for mask-
predict and easy-Ô¨Åst decoding as 10 without using
any early-stopping strategy. For fair comparison,
we reduce the max iteration to 8 when decoding
with self-reranking in NAR model. Our model is
implemented with pytorch (Paszke et al., 2019) and
fairseq (Ott et al., 2019). BLEU (Papineni et al.,
2002) is used for evaluation.

3.2 Results & Analysis

We perform experiments on Diformer to evaluate
its performance on three generation directions with
four decoding strategies. We mainly compare Di-
former to three types of models: 1) the uniÔ¨Åed-
models that is able to decode with multiple strate-
gies, 2) pure AR model, i.e. standard Transformer,
3) pure NAR models. (see Table 1)

Comparison with uniÔ¨Åed models For the com-
parison to uniÔ¨Åed-models (Mansimov et al., 2020;
Tian et al., 2020), Diformer outperforms others in
all generation directions, by obtaining more than
1.5 BLEU.

As discussed in the section 1, their support on
multiple generation strategies is achieved by ap-
plying certain position selection strategy on the
masked language model or generating with certain
permutation with the permutation language model.
This creates the gap between the training and infer-
ence since a speciÔ¨Åc decoding strategy might not
be fully trained with the generalized objective as
analyzed in (Mansimov et al., 2020). So, compared
to both, we use the task-speciÔ¨Åc modelling in ex-
change for better performance by abandon certain
Ô¨Çexibility, thus makes the learned distribution to be
same with the one used in decoding, which answers
why Diformer performs better.

T-big (Vaswani et al., 2017)
T-base (Vaswani et al., 2017)
T-big (our impl, En‚ÜîDe teacher)
T-base (our impl, En‚ÜîRo teacher)
T-base + distill

NAT (Gu et al., 2018)
iNAT (Lee et al., 2018)
InsT (Stern et al., 2019)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019b)
DisCO (Kasai et al., 2020)

Mansimov et al. (2020)
Tian et al. (2020)

Diformer (ours)
- L2R
- R2L
- mask-predict
- easy-Ô¨Årst

En-De

De-En

En-Ro

Ro-En

AR

NAR

AR

NAR

AR

NAR

AR

NAR

28.4
27.3
28.52
27.67
28.41

-
-
27.29
-
-
-

25.66
27.23

AR Models

-
-
32.10
31.12
31.69

NAR models

-
-
-
-
-
-

UniÔ¨Åed models

30.58
-

-
-
-
-
-

23.20
25.48
-
30.53
-
31.31

28.63
-

-
-
-
-
-

19.17
21.61
27.41
27.03
27.27
27.34

24.53
26.35

-
-
-
35.29
35.21

-
-
-
-
-
-

-
-

-
-
-
-
-

29.79
29.32
-
33.08
-
33.22

-
-

-
-
-
34.02
33.87

-
-
-
-
-
-

-
-

-
-
-
-
-

31.44
30.19
-
33.31
33.26
33.25

-
-

28.35/28.68
28.58/28.50
-
-

-
-
27.51/27.99
27.35/27.84

31.58/31.76
32.00/31.78
-
-

-
-
31.05/31.35
31.21/31.68

35.06/35.16
35.17/35.13
-
-

-
-
33.62/34.37
33.58/34.23

33.84/33.92
33.90/33.90
-
-

-
-
32.68/33.11
32.97/33.34

Table 1: This table shows the overall performance of Diformer compared to the AR, NAR and uniÔ¨Åed models
when decoding with AR or NAR strategies. T-big/-base is the abbreviation of Transformer-big/-base. The BLEU
score using self-rerank (right) or not (left) is separated by /.

Comparison with AR models For the En‚ÜîDe
dataset, since we use a larger teacher model
(Transformer-big), therefore, we only compare Di-
former with same sized Transformer-base trained
on the raw and distilled data. The Diformer out-
performs Transformer trained on the raw data with
a large margin and reaches the same level to the
one trained on distilled data. Interesting, the best
performance of Diformer are usually obtained by
the R2L decoding and the reranked results on L2R,
the reason of it will be further discussed in ablation
study sections. For the En‚ÜîRo dataset, Diformer
can also obtain similar performance compared to
the same sized Transformer trained on the distilled
data produced by a same sized teacher.

Comparison with NAR models Diformer is
also competitive to a series of NAR models in-
cluding iterative-reÔ¨Ånement based and fully NAR
models. We speculate the strong performance of
Diformer comes from the joint training of AR and
NAR, since it is similar to the multi-task scenario,
where tasks are closely correlated but not same.
This could be beneÔ¨Åcial for the task that is more
difÔ¨Åcult i.e. NAR, because the learned common
knowledge on AR tasks could be directly used in it.
By applying the self-reranking method, Diformer
could obtain additional 0.5 BLEU over the strong
baseline.

Data Condition

R

L

mask-predict

easy-Ô¨Årst

T-base (our impl)
Raw data
Distilled data

Diformer
Raw data
Raw data (Ô¨Åxed right)
Distilled data

27.67
28.41

-
-

27.21
27.63
28.35

27.08
-
28.55

-
-

24.12
-
27.51

-
-

24.18
-
27.35

Table 2: This table shows the performance of Trans-
former and Diformer trained on raw and distilled data
where T-base represents for Transformer-base. An ad-
ditional experiment with Ô¨Åxed zi = R for all tokens is
also presented.

3.3 Ablation Study

In this section, we perform extra experiments to
investigate factors that could inÔ¨Çuence the perfor-
mance of Diformer and the mechanism behind it.
All experiments of ablation study are performed on
the WMT14 En‚ÜíDe dataset.

The inÔ¨Çuence of Knowledge Distillation We
train Diformer not only with distilled data but also
with raw data as shown in table 2. The degradation
of NAR decoding when training on raw data is not
surprising which is a common problem faced by
all NAR models. However, the performance of AR
decoding also degrades. We speculate that on the
raw data, the difÔ¨Åculty of learning to generate from

max k

R

L

mask-predict

easy-Ô¨Årst

256
16
8
1

28.35
28.51
28.13
26.81

28.58
28.48
28.25
26.85

27.51
27.25
26.58
18.78

27.35
27.32
26.71
19.53

Table 3: This table shows the performance of Diformer
with different max k.

straight and R2L increased signiÔ¨Åcantly, making
the model to allocate more capacity to Ô¨Åt them,
resulting in the negative inÔ¨Çuence on the perfor-
mance of L2R. We verify this by Ô¨Åxing zi = R for
all tokens and train the model on raw data. The re-
sult conÔ¨Årms it because the performance recovers
to its original level. On the contrary, the knowl-
edge distilled data is cleaner and more monotonous
(Zhou et al., 2020), making it easier to learn for
all directions, and allows the model to allocate bal-
anced capacity on each direction. As for the better
performance obtained by R2L decoding, we con-
sider the reason is that, the R2L is able to learn
the distilled data generated by the L2R teacher in a
complementary manner, making it more efÔ¨Åcient
to learn the knowledge that cannot be learned by
L2R due to the same modeling method.

The Importance of Relative Position We also
demonstrate the importance of the relative posi-
tional embedding by evaluating the model with
different maximum relative distance k and obtain
the same conclusion (Shaw et al., 2018) ‚Äî the dis-
tance should be at least 8. Meanwhile, we observe
that NAR is more sensitive to the positional infor-
mation, which is reasonable, since the decoding
of NAR is conditioned on the bi-directional con-
text, where the positional information contains both
distance and direction thereby is more important
compared to that in AR.

Improvements of Self-Reranking As shown in
the overall results, self-reranking is a useful method
to improve the performance especially for NAR de-
coding. For the AR decoding, the improvements is
not that signiÔ¨Åcant since the outputs are already
good enough for L2R or R2L, the tiny gap be-
tween reranking and generation direction cannot
provide enough help, which indicates that using
self-reranking for AR is not that proÔ¨Åtable com-
pared to NAR.

We further investigate its ability on NAR decod-
ing (mask-predict) given different max iteration
number and length beam size, as shown in Figure 2.

Figure 2: The heatmap shows the BLEU score decoded
with mask-predict when using self-reranking or not un-
der different max iteration number and length beam
size.

It clearly shows that without reranking, the incor-
rect selection on beam candidates may even reduce
the performance with larger beam size. The use
of self-reranking actually lets the performance and
beam-size positively correlated, meaning that ex-
changing 2 steps of iteration with self-reranking
can be proÔ¨Åtable with larger beam size. In prac-
tical usage of self-reranking, it is critical to Ô¨Ånd
the optimal combination by balancing the beam
size and max iteration number so that both high
performance and low latency can be obtained.

4 Conclusion

In this paper, we present Directional Transformer
which is able to model the autoregressive and non-
autoregressive generation with a uniÔ¨Åed framework
named Directional Language Model which essen-
tially links two types of conditional language model
with three generation directions. Compared to
previous works, Diformer exchanges the gener-
alization on decoding strategies for better perfor-
mance and thereby only support 4 decoding strate-
gies. Experimental results on WMT14 En‚ÜîDe
and WMT16 En‚ÜîRo demonstrate that the uniÔ¨Å-
cation of AR and NAR can be achieved by Di-
former without losing any performance. The bi-
directional property of Diformer allows it to per-
form self-reranking which is especially useful for
NAR decoding to improve performance with no
additional computational cost.

Except from machine translation, Diformer can
be easily extended to other tasks like language mod-
eling by removing the dependency on X. It has the
potential to unify the representation learning and
generation with a single model, which is actually
our ongoing work.

        E H D P  V L ] H         P D [  L W H U D W L R Q  Q X P E H U                                                                                                                                                                                                                   : L W K R X W  V H O I  U H U D Q N        E H D P  V L ] H                                                                                                                                                                                                                   : L W K  V H O I  U H U D Q NReferences

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In 3rd Inter-
learning to align and translate.
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Marjan Ghazvininejad, Vladimir Karpukhin, Luke
Zettlemoyer, and Omer Levy. 2020. Aligned cross
entropy for non-autoregressive machine translation.
In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020,
Virtual Event, volume 119 of Proceedings of Ma-
chine Learning Research, pages 3515‚Äì3523. PMLR.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel
decoding of conditional masked language models.
In Proceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019, pages 6111‚Äì
6120. Association for Computational Linguistics.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
tor O. K. Li, and Richard Socher. 2018. Non-
In 6th
autoregressive neural machine translation.
International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada, April 30
- May 3, 2018, Conference Track Proceedings. Open-
Review.net.

Jiatao Gu, Qi Liu, and Kyunghyun Cho. 2019a.
Insertion-based decoding with automatically in-
ferred generation order. Trans. Assoc. Comput. Lin-
guistics, 7:661‚Äì676.

Jiatao Gu, Changhan Wang, and Junbo Zhao. 2019b.
In Advances in Neural
Levenshtein transformer.
Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems
2019, NeurIPS 2019, December 8-14, 2019, Vancou-
ver, BC, Canada, pages 11179‚Äì11189.

Jungo Kasai, James Cross, Marjan Ghazvininejad, and
Jiatao Gu. 2020. Non-autoregressive machine trans-
lation with disentangled context transformer. In Pro-
ceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020,
Virtual Event, volume 119 of Proceedings of Ma-
chine Learning Research, pages 5144‚Äì5155. PMLR.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open

source toolkit for statistical machine translation. In
ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
June 23-30, 2007, Prague, Czech Republic. The As-
sociation for Computational Linguistics.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
In Pro-
quence modeling by iterative reÔ¨Ånement.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, Brussels, Bel-
gium, October 31 - November 4, 2018, pages 1173‚Äì
1182. Association for Computational Linguistics.

Elman Mansimov, Alex Wang, Sean Welleck, and
Kyunghyun Cho. 2020. A generalized framework of
sequence generation with application to undirected
sequence models.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
fairseq: A fast, extensible
Michael Auli. 2019.
In Proceedings of
toolkit for sequence modeling.
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019,
Minneapolis, MN, USA, June 2-7, 2019, Demonstra-
tions, pages 48‚Äì53. Association for Computational
Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA, pages 311‚Äì318. ACL.

Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch:
An imperative style, high-performance deep learn-
ing library. In Advances in Neural Information Pro-
cessing Systems 32, pages 8024‚Äì8035. Curran Asso-
ciates, Inc.

Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu
Chen, Dayiheng Liu, Kewen Tang, Houqiang Li,
Jiusheng Chen, Ruofei Zhang, Ming Zhou, and Nan
Duan. 2021. BANG: bridging autoregressive and
non-autoregressive generation with large scale pre-
In Proceedings of the 38th International
training.
Conference on Machine Learning, ICML 2021, 18-
24 July 2021, Virtual Event, volume 139 of Proceed-
ings of Machine Learning Research, pages 8630‚Äì
8639. PMLR.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-
bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.
Xlnet: Generalized autoregressive pretraining for
In Advances in Neural
language understanding.
Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems
2019, NeurIPS 2019, December 8-14, 2019, Vancou-
ver, BC, Canada, pages 5754‚Äì5764.

Chunting Zhou,

Jiatao Gu, and Graham Neubig.
2020. Understanding knowledge distillation in non-
In 8th Inter-
autoregressive machine translation.
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.

A Appendices

Meeting of the Association for Computational Lin-
guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers. The Association for
Computer Linguistics.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position repre-
In Proceedings of the 2018 Confer-
sentations.
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, NAACL-HLT, New Orleans,
Louisiana, USA, June 1-6, 2018, Volume 2 (Short
Papers), pages 464‚Äì468. Association for Computa-
tional Linguistics.

Raphael Shu, Jason Lee, Hideki Nakayama, and
Kyunghyun Cho. 2020.
Latent-variable non-
autoregressive neural machine translation with deter-
In The
ministic inference using a delta posterior.
Thirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelli-
gence, AAAI 2020, The Thirty-Second Innovative Ap-
plications of ArtiÔ¨Åcial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational
Advances in ArtiÔ¨Åcial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020, pages 8846‚Äì
8853. AAAI Press.

Mitchell Stern, William Chan, Jamie Kiros, and Jakob
Uszkoreit. 2019. Insertion transformer: Flexible se-
quence generation via insertion operations. In Pro-
ceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA, volume 97 of Pro-
ceedings of Machine Learning Research, pages
5976‚Äì5985. PMLR.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems 27: Annual Conference on Neural Informa-
tion Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada, pages 3104‚Äì3112.

Chao Tian, Yifei Wang, Hao Cheng, Yijiang Lian, and
Zhihua Zhang. 2020. Train once, and decode as
In Proceedings of the 28th International
you like.
Conference on Computational Linguistics, COLING
2020, Barcelona, Spain (Online), December 8-13,
2020, pages 280‚Äì293. International Committee on
Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, pages 5998‚Äì6008.

Sean Welleck, Kiant¬¥e Brantley, Hal Daum¬¥e III, and
Kyunghyun Cho. 2019. Non-monotonic sequential
In Proceedings of the 36th Inter-
text generation.
national Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Re-
search, pages 6716‚Äì6726. PMLR.

Figure 3: The architecture of the model (left) with the example of corresponded operations in key components
during training (right).

Step L2R

R2L

Mask-Predict

Easy-First

A
[B]
R

A B
[B] A
R R

A B C
[B] A B
R R R

A B C D
[B] A B C
R R R R

1

2

3

4

5

6

E
[E]
L

D E
E [E]
L L

[B] - - B - - [E]
[B] - - - - - [E]
S S S S S

[B] A A C C D [E]
[B] - - - - - [E]
S S S S S

[B] A B - - - [E]
[B] - - B - - [E]
S S S S S

[B] A B C C E [E]
[B] A A C C D [E]
S S S S S

C D E
D E [E]
L L L

[B] A B - - E [E]
[B] A B - - - [E]
S S S S S

[B] A B C D E [E]
[B] A B C C E [E]
S S S S S

B C D E
C D E [E]
L L L L

[B] A B C - E [E]
[B] A B - - E [E]
S S S S S

A B C D E
[B] A B C D
R R R R R

A B C D E
B C D E [E]
L L L L L

[B] A B C D E [E]
[B] A B C - E [E]
S S S S S

A B C D E [E]
[B] A B C D E
R R R R R R

[B] A B C D E
A B C D E [E]
L L L L L L

-

-

-

-

Table 4: An example of 4 decoding strategies with the output, context input and direction input placed in three
sub-rows, where [B] and [E] represent for [BOS] and [EOS] token, - represents for mask.

012345-101234-1-20000111223-1-1-1-2-2-2-3-3-3-4-4-5++++++Add &NormFeed ForwardAdd &NormMulti-Head AttentionInput EmbeddingAdd &NormFeed ForwardAdd &NormDirectional Self-AttentionDirectional EmbeddingsMulti-Head AttentionLinearSoftmaxAdd &NormOutputsLengthPredictorWordEmbedding