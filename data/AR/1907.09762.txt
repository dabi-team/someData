9
1
0
2

l
u
J

3
2

]
T
S
.
h
t
a
m

[

1
v
2
6
7
9
0
.
7
0
9
1
:
v
i
X
r
a

Electronic Journal of Statistics
ISSN: 1935-7524

Consistent model selection criteria and goodness-of-ﬁt
test for aﬃne causal processes

Jean-Marc Bardet and Kare Kamila∗

S.A.M.M., Université Paris 1, Panthéon-Sorbonne,
90, rue de Tolbiac, 75634, Paris, France
e-mail: Jean-Marc.Bardet@univ-paris1.fr and kamilakare@gmail.com

William Kengne

THEMA, Université de Cergy-Pontoise, FRANCE.
e-mail: william.kengne@gmail.com

Abstract: This paper studies the model selection problem in a large class of causal time se-
ries models, which includes both the ARMA or AR(∞) processes, as well as the GARCH or
ARCH(∞), APARCH, ARMA-GARCH and many others processes. To tackle this issue, we
consider a penalized contrast based on the quasi-likelihood of the model. We provide suﬃcient
conditions for the penalty term to ensure the consistency of the proposed procedure as well as the
consistency and the asymptotic normality of the quasi-maximum likelihood estimator of the cho-
sen model. It appears from these conditions that the Bayesian Information Criterion (BIC) does
not always guarantee the consistency. We also propose a tool for diagnosing the goodness-of-ﬁt
of the chosen model based on the portmanteau Test. Numerical simulations and an illustrative
example on the FTSE index are performed to highlight the obtained asymptotic results, includ-
ing a numerical evidence of the non consistency of the usual BIC penalty for order selection of
an AR(p) models with ARCH(∞) errors.

MSC 2010 subject classiﬁcations: Primary 60K35, 60K35; secondary 60K35.
Keywords and phrases: model selection, aﬃne causal processes, consistency, BIC, Portman-
teau Test.

1. Introduction

Model selection is an important tool for statisticians and all those who process data. This issue has
received considerable attention in the recent literature. There are several model selection procedures,
the main ones are : cross validation and penalized contrast based.

The cross validation ([43], [2]) consists in splitting the data into learning sample, which will be used for
computing estimators of the parameters and the test sample which allows to assess these estimators
by evaluate their risks.

The procedures using penalized objective function search for a model, minimizing a trade-oﬀ between
a sum of an empirical risk (for instance least squares, −2×log-likelihood), which indicates how well
the model ﬁts the data, and a measure of model’s complexity so-called a penalty.
The idea of penalizing dates back to the 1970s with the works of [34] and [1]. By using the ordinary least
squares in regression framework, Mallows obtained the Cp criterion. Meanwhile, Akaike derived AIC
for density estimation using log-likelihood contrast. A few years later, following Akaike, [38] proposed
an alternative approach to density estimation and derived the Bayesian Information Criteria (BIC).
The penalty term of these criteria is proportional to the dimension of the model. In the recent decades,
diﬀerent approaches of penalization have emerged such as the L2 norm for the Ridge penalisation [18],
the L1 norm used by [45] that provides the LASSO procedure and the elastic-net that mixes the L1
and L2 norms [50].

Model selection procedures can have two diﬀerent objectives: consistency and eﬃciency. A procedure
is said to be consistent if given a family of models, including the "true model", the probability of
choosing the correct model approaches one as the sample size tends to inﬁnity. On the other hand, a
procedure is eﬃcient when its risk is asymptotically equivalent to the risk of the oracle. In this work,

∗Developed within the European Union’s Horizon 2020 research and innovation programme under the Marie

Sklodowska-Curie grant agreement No 754362.

†Developed within the ANR BREAKRISK : ANR-17-CE26-0001-01.

1

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

 
 
 
 
 
 
Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

2

we are interested to construct a consistent procedure for the general class of times series known as
aﬃne causal processes.

This class of aﬃne causal time series can be deﬁned as follows. Let R∞ be the space of sequences of
real numbers with a ﬁnite number of non zero, if M , f : R∞ → R are two measurable functions, then
an aﬃne causal class is

Class AC(M, f ) : A process X = (Xt)t∈Z belongs to AC(M, f ) if it satisﬁes:

Xt = M (cid:0)(Xt−i)i∈N∗

(cid:1) ξt + f (cid:0)(Xt−i)i∈N∗

(cid:1) for any t ∈ Z;

(1.1)

where (ξ)t∈Z is a sequence of zero-mean independent identically distributed random vectors (i.i.d.r.v)
satisfying E(|ξ0|r) < ∞ for some r ≥ 2 and E[ξ2
0] = 1.

For instance,

• if M (cid:0)(Xt−i)i∈N∗

(cid:1) = σ and f (cid:0)(Xt−i)i∈N∗

(cid:1) = φ1Xt−1 + · · · + φpXt−p, then (Xt)t∈Z is an AR(p)

process;

• if M (cid:0)(Xt−i)i∈N∗

(cid:1) =

ARCH(p) process.

(cid:113)

a0 + a1X 2

t−1 + · · · + apX 2

t−p and f (cid:0)(Xt−i)i∈N∗

(cid:1) = 0, then (Xt)t∈Z is an

Note that, numerous classical time series models such as ARMA(p, q), GARCH(p, q), ARMA(p, q)-
GARCH(p, q) (see [12] and [33]) or APARCH(δ, p, q) processes (see [12]) belongs to AC(M, f ). The
existence of stationary and ergodic solutions of this class has been studied in [13] and [7].

We consider a trajectory (X1, . . . , Xn) of a stationary aﬃne causal process AC(M ∗, f ∗), where M ∗
and f ∗ are unknown. We also consider a ﬁnite set M of parametric models m, which are aﬃne causal
time series. We assume that the "true" model m∗ corresponds to M ∗ and f ∗. The aim is to obtain an
estimator (cid:98)m of m∗ and testing the goodness-of-ﬁt of the chosen model.

There already exist several important contributions devoted to the model selection for time series ;

we refer to the book of [35] and the references therein for an overview on this topic.
As we have pointed above, two properties are often used to evaluate a quality of a model selection pro-
cedure : consistency and eﬃciency. The ﬁrst measure is often used when the true model is included in
the collection of model’s candidate ; otherwise, eﬃciency is the well-deﬁned property. In many research
in this framework, the main goal is to develop a procedure that fulﬁlls one of these properties. So, in
some classical linear time series models, the consistency of the BIC procedure has been established,
see for instance [17] or [46] ; and the asymptotic eﬃciency of the AIC has been proved, see, among
others, [41], [20] for a corrected version of AIC for small samples, [23], [21], [22] for the case of inﬁnite
order autoregressive model. [40] propose the (consistent) residual information criteria (RIC) for regres-
sion model (including regression models with ARMA errors) selection. In the framework of nonlinear
threshold models, [25] proved consistency results of a large class of information criteria, whereas [16]
focussed on cross-validation type procedure for model selection in a class of semiparametric time series
regression model. Let us recall that, the time series model selection literature is very extensive and still
growing ; we refer to the monograph of [36], which provided an excellent summary of existing model
selection procedure, including the case of time series models as well as the recent review paper of [11].

The adaptive lasso, introduced by [49] for variable selection in linear regression models has been
extended by [37] to vector autoregressive models, [26] carried out this procedure in stationary and
nonstationary autoregressive models ; the oracle eﬃcient is established. [28] considers model selection
for density estimation under mixing conditions and derived oracle inequalities of the slope heuristic
procedure ([9] or [5]) ; whereas [3] develop oracle inequalities for model selection for weakly depen-
dent time series forecasting. Recently, [39] have considered the model selection for ARMA time series
with trend, and proved the consistency of BIC for the detrended residual sequence, while [4] devel-
oped oracle inequalities of sequential model selection method for nonparametric autoregression. [19]
pointed out that most existing model selection procedure cannot simultaneously enjoy consistency
and (asymptotic) eﬃciency. They propose a misspeciﬁcation-resistant information criterion that can
achieve consistency and asymptotic eﬃciency for prediction using model selection.

In this paper, we focus on the class of models (1.1), and addressed the following questions :

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

3

1. What regularity conditions are suﬃcient to build a consistent model selection procedure? Does
the classic criterion such as BIC, still have consistent property for choosing a model among the
collections M?

2. How can we test the goodness-of-ﬁt of the chosen model?

These questions have not yet been answered for the class of models and the framework considered
here, in particular in case of inﬁnite memory processes. This new contribution provides theoretical and
numerical response of these issues.

(i) The estimator (cid:98)m of m∗ is chosen by minimizing a penalized criterion (cid:98)C(m) = −2(cid:98)Ln(m) +
|m| κn, where (cid:98)Ln(m) is a Gaussian quasi-log-likelihood of the model m, |m| is the number of estimated
parameters of the model m and κn is a non-decreasing sequence of real numbers (see more details in
Section 2). Note that, in the cases κn = 2 or κn = log n we respectively consider the usual AIC and
BIC criteria. We provide suﬃcient conditions (essentially depending on the decreasing of the Lipschitz
coeﬃcients of the functions f and M ) for obtaining consistency of the model selection procedure. We
also theoretically and numerically exhibit an example of order selection (weak AR(p) processes with
ARCH(∞) errors) such that the consistency of the classical BIC penalty is not ensured.

(ii) We provide an asymptotic goodness-of-ﬁt test for the selected model that is very simple to be
used (with the usual Chi-square distribution limit), which successively completes the model selection
procedure. Numerical applications show the accuracy of this test under the null hypothesis as well as
an eﬃcient test power under an alternative hypothesis. Note that, similar test has been proposed by
[31] under the Gaussian assumption on the observations, whereas [32] focused for multivariate time
series with multivariate ARCH-type errors. Also, [14] proposed a portmanteau test statistic based on
generalized inverses and {2}-inverses for diagnostic checking in the class of model (1.1). Unlike these
authors, we apply the test to a model obtained from a model selection procedure.

The paper is organized as follows. Some deﬁnitions, notations and assumptions are described in Section
2. The consistency of the criteria and the asymptotic normality of the post-model-selection estimator
are studied in Section 3. In Section 4, the examples of AR(∞), ARCH(∞), AP ARCH(δ, p, q) and
ARMA(p, q)-GARCH(p(cid:48), q(cid:48)) processes are detailed. The goodness-of-ﬁt test is presented in Section 5.
Finally, numerical results are presented in Section 6 and Section 7 contains the proofs.

2. Deﬁnitions and Assumptions

Let us introduce some deﬁnitions and assumptions in order to facilitate the presentation.

2.1. Notation and assumptions

In the sequel, we will consider a subset Θ of Rd (d ∈ N). We will use the following norms:

• (cid:107).(cid:107) denotes the usual Euclidean norm on Rν, with ν ≥ 1;
• if X is Rν-random variable with r ≥ 1 order moment, we set (cid:107)X(cid:107)r = (cid:0)E((cid:107)X(cid:107)r(cid:1)1/r
(cid:8)(cid:107)g(θ)(cid:107)(cid:9).
• for any set Θ ⊆ Rd and for any g : Θ → Rd(cid:48)

, d(cid:48) ≥ 1, denote (cid:107)g(cid:107)Θ = sup
θ∈Θ

;

In the introduction, to be more concise, we have presented the problem of time series model selection
in a very general form. In reality, we will limit our ﬁeld of study a little bit by considering a semi-
parametric framework. Hence, let (fθ)θ∈Θ and (Mθ)θ∈Θ be two families of known functions such as for
any θ ∈ Θ, both fθ, Mθ with real values deﬁned on R∞.

We begin by giving a condition on fθ and Mθ which ensure the existence of a r-order moment, stationary
and ergodic time series belonging to AC(Mθ, fθ). This condition, initially obtained in [13], is written
in terms of Lipschitz coeﬃcients of both these functions. Hence, for Ψθ = fθ or Mθ, deﬁne:

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

4

Assumption A(Ψθ, Θ): Assume that (cid:107)Ψθ(0)(cid:107)Θ < ∞ and there exists a sequence of non-negative real
numbers (cid:0)αk(Ψθ, Θ)(cid:1)

k=1 αk(Ψθ, Θ) < ∞ satisfying:

k≥1 such that (cid:80)∞

(cid:107)Ψθ(x) − Ψθ(y)(cid:107)Θ ≤

∞
(cid:88)

k=1

αk(Ψθ, Θ)|xk − yk| f or all x, y ∈ R∞.

Now for r ≥ 1, where (cid:107)ξ0(cid:107)r < ∞, deﬁne:

Θ(r) =

(cid:110)

θ ∈ Rd, A(fθ, {θ}) and A(Mθ, {θ}) hold with

Then, for any θ ∈ Θ(r), there exists a stationary and ergodic solution with r-order moment belonging
to AC(Mθ, fθ). (see [13] and [7]).

∞
(cid:88)

k=1

αk(fθ, {θ}) + (cid:107)ξ0(cid:107)r

αk(Mθ, {θ}) < 1

(cid:111)
.

(2.1)

∞
(cid:88)

k=1

2.2. The framework

Let us start with an example to better understand the framework and the approach of model selection
we will follow.

Example: Assume that the observed trajectory (X1, . . . , Xn) is generated from an AR(2) process
and we would like to identify this family of process and its order. Then, we consider the collection M
of ARMA(p, q) and GARCH(p(cid:48), q(cid:48)) processes for 0 ≤ p, q, p(cid:48), q(cid:48) ≤ 9 and we would like to chose in this
family a "best" model for ﬁtting (X1, . . . , Xn). Note that there is 200 possible models and we expect
to recognize the AR(2) as the selected model, at least when n is large enough.

We begin with the following property that allow to enlarge the family of models by extending the

dimension d of the parameter θ:
Proposition 1. Let d1, d2 ∈ N, Θ1 ⊂ Rd1 and Θ2 ⊂ Rd2, and for i = 1, 2, deﬁne f (i)
: R∞ → R
θi
and for θi ∈ Θi. Then there exist max(d1, d2) ≤ d ≤ d1 + d2, Θ ⊂ Rd, and a family of functions
fθ : R∞ → R and Mθ : R∞ → [0, ∞) with θ ∈ Θ, such that for any θ1 ∈ Θ1 and θ2 ∈ Θ2, there exists
θ ∈ Θ satisfying

, M (i)
θi

AC(cid:0)M (1)
θ1

, f (1)
θ1

(cid:1) (cid:91)

AC(cid:0)M (2)
θ2

, f (2)
θ2

(cid:1) ⊂ AC(cid:0)Mθ, fθ

(cid:1).

The proof of this proposition, as well as the other proofs, can be found in Section 7. This proposition
says that it is always possible to embed two parametric causal aﬃne models in a larger one. Hence, for
instance, we can consider as well AR processes and ARCH processes in a unique representation, i.e.

(cid:40)

(cid:40)






AR

ARCH

(cid:0)(Xt−i)i∈N∗
M (1)
θ1
(cid:0)(Xt−i)i∈N∗
f (1)
θ1

(cid:1) = σ
(cid:1) = φ1Xt−1 + · · · + φpXt−p

(cid:0)(Xt−i)i∈N∗
M (2)
θ2
(cid:0)(Xt−i)i∈N∗
f (2)
θ2

(cid:1) =
(cid:1) = 0

(cid:113)

a0 + a1X 2

t−1 + · · · + aqX 2

t−q

(cid:40)

=⇒

(cid:0)(Xt−i)i∈N∗
(cid:0)(Xt−i)i∈N∗

θ0 + θ1X 2

(cid:1) =
t−1 + · · · + θqX 2
(cid:1) = θq+1Xt−1 + · · · + θq+pXt−p

Mθ
fθ

t−q

.

(cid:113)

From now and in all the sequel, we ﬁx d ∈ N∗, and the family of functions fθ, Mθ : R∞ → R for
θ ∈ Θ ⊂ Θ(r) ⊂ Rd.

Let (X1, . . . , Xn) be an observed trajectory of an aﬃne causal process X belonging to AC(Mθ∗ , fθ∗ ),

where θ∗ is an unknown vector of Θ, and therefore:
(cid:1) ξt + fθ∗

(cid:0)(Xt−i)i∈N∗

Xt = Mθ∗

(cid:0)(Xt−i)i∈N∗

(cid:1) for any t ∈ Z.

(2.2)

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

5

In the sequel, we will consider several models, which all are particular cases of AC(Mθ, fθ) with θ ∈
Θ ⊂ Rd. More precisely deﬁne:

• a model m as a subset of {1, . . . , d} and denote |m| = #(m);
• Θ(m) = (cid:8)(θi)1≤i≤d ∈ Rd, θi = 0 if i /∈ m(cid:9) ∩ Θ;
• M as a family of models, i.e. M ⊂ P(cid:0){1, . . . , d}(cid:1).

Finally, for all m ∈ M, m ∈ AC(Mθ, fθ) when θ ∈ Θ(m) and denote m∗ the "true" model. We could
as well consider hierarchical or exhaustive families of models.

Example: From the previous example, we can consider:
• a family M1 of models m1 such as M1 = (cid:8){1}, {1, 2}, . . . , {1, . . . , q + 1}(cid:9): this family is the hierar-
chical one of ARCH processes with orders varying from 0 to q.
• a family M2 of models m2 such as M2 = P(cid:0){1, . . . , p + q + 1}(cid:1): this family is the exhaustive one
and contains as well the AR(2) process Xt = φ2Xt−2 + θ0 ξt as the process Xt = φ1Xt−1 + φ3Xt−3 +
ξt

θ0 + a2X 2

(cid:113)

t−2.

To establish the consistency of the selected model, we will need to assume that the "true" model
m∗ with the parameter θ∗, is included in the model family M.

2.3. The special case of NLARCH(∞) processes

As in [7], in the special case of NLARCH(∞) processes, including for instance GARCH(p, q) or
ARCH(∞) processes, a particular treatment can be realized for obtaining sharper results than us-
ing the previous framework. In such case, deﬁne the class:

Class (cid:102)AC( (cid:101)Hθ): A process X = (Xt)t∈Z belongs to (cid:102)AC( (cid:101)Hθ) if it satisﬁes:

(cid:113)

Xt = ξt

t−i)i∈N∗
(cid:1) then, (cid:102)AC( (cid:101)Hθ) = AC(Mθ, 0). In
(cid:1) = (cid:101)Hθ
Therefore, if M 2
θ
case of the class (cid:102)AC( (cid:101)Hθ), we will use the assumption A( (cid:101)Hθ, Θ). By this way, we will obtain a new set
of stationary solutions. For r ≥ 2 deﬁne:

(cid:101)Hθ
(cid:0)(Xt−i)i∈N∗

(cid:0)(Xt−i)i∈N∗

(cid:1) = Hθ

t−i)i∈N∗

(cid:0)(X 2

(2.3)

(cid:0)(X 2

(cid:1) for any t ∈ Z.

(cid:101)Θ(r) =

(cid:110)
θ ∈ Rd, A( (cid:101)Hθ, {θ}) holds with (cid:0)(cid:107)ξ0(cid:107)r

(cid:1)2

αk( (cid:101)Hθ, {θ}) < 1

(cid:111)
.

(2.4)

∞
(cid:88)

k=1

Then, for θ ∈ Θ(r), a process (Xt)t∈Z belonging to the class (cid:102)AC( (cid:101)Hθ) is stationary ergodic and satisﬁes
(cid:107)X0(cid:107)r < ∞.

2.4. The Gaussian quasi-maximum likelihood estimation and the model selection

criterion

In the sequel, for a model m ∈ M, a family of models of AC(Mθ, fθ) with θ ∈ Θ ⊂ Rd, where θ → Mθ
and θ → fθ are two ﬁxed functions, we are going to consider Gaussian quasi-maximum likelihood
estimators (QMLE) of θ for each speciﬁc model m.

This approach as semi-parametric estimation has been successively introduced for GARCH(p, q) pro-
cesses in [24] where its consistency is also proved, and the asymptotic normality of this estimator has
been established in [8] and [15]. In [7], those results have been extended to aﬃne causal processes, and
an extension to Laplacian QMLE has been also proposed in [6].
The Gaussian QMLE is derived from the conditional (with respect to the ﬁltration σ(cid:8)(Xt)t≤0
(cid:9)) log-
likelihood of (X1, . . . , Xn) when (ξt) is supposed to be a Gaussian standard white noise. Due to the
linearity of a causal aﬃne process, we deduce that this conditional log-likelihood (up to an additional
constant) Ln is deﬁned for all θ ∈ Θ by:

Ln(θ) := −

1
2

n
(cid:88)

t=1

qt(θ) , with qt(θ) :=

θ)2

(Xt − f t
H t
θ

+ log(H t
θ)

(2.5)

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

6

θ := fθ(Xt−1, Xt−2, · · · ), M t

where f t
θ := Mθ(Xt−1, Xt−2, · · · ) and H t
. Since Ln(θ) depends on
(Xt)t≤0 that are unknown, the idea of the quasi log-likelihood is to replace qt(θ) by an approximation
(cid:98)qt(θ) and to compute (cid:98)θ as in equation (2.7) even if the white noise is not Gaussian. Hence, the
conditional quasi log-likelihood (up to an additional constant) is given for all θ ∈ Θ by

θ

θ = (cid:0)M t

(cid:1)2

(cid:98)Ln(θ) := −

1
2

n
(cid:88)

t=1

(cid:98)qt(θ) , with (cid:98)qt(θ) :=

θ)2

(Xt − (cid:98)f t
(cid:98)H t
θ

+ log( (cid:98)H t
θ)

where






(cid:98)f t
θ
(cid:99)M t
θ
(cid:98)H t
θ

:= fθ(Xt−1, Xt−2, · · · , X1, u)
:= Mθ(Xt−1, Xt−2, · · · , X1, u)
:= ( (cid:99)M t

θ)2

(2.6)

for any deterministic sequence u = (un) with ﬁnitely many non-zero values (u = 0 is very often chosen
without loss of generality).

However, the deﬁnitions of the conditional log-likelihood and quasi log-likelihood require that their
denominators do not vanish. Hence, we will suppose in the sequel that the lower bound of Hθ(·) =
(cid:0)Mθ(·)(cid:1)2

(which is reached since Θ is compact) is strictly positive:

Assumption D(Θ): ∃h > 0 such that inf
θ∈Θ

(Hθ(x)) ≥ h for all x ∈ R∞.

Finally, under this assumption, for each speciﬁc model m ∈ M, we deﬁne the Gaussian QMLE (cid:98)θ(m)
as

(cid:98)θ(m) = argmax
θ∈Θ(m)

(cid:98)Ln(θ).

(2.7)

To select the "best" model m ∈ M, we chose a penalized contrast (cid:98)C(m) ensuring a trade-oﬀ between
−2 times the maximized quasi log-likelihood, which decreases with the size of the model, and a penalty
increasing with the size of the model. Therefore, the choice of the "best" model (cid:98)m among the estimated
can be performed by minimizing the following criteria

(cid:98)m = argmin

m∈M

(cid:98)C(m) with

(cid:98)C(m) = −2(cid:98)Ln

(cid:0)

(cid:98)θ(m)(cid:1) + |m| κn,

(2.8)

where

• (κn)n an increasing sequence depending on the number of observations n.
• |m| denotes the dimension of the model m, i.e. the cardinal of m, subset of {1, . . . , d}, which is

also the number of estimated components of θ (the others are ﬁxed to zero).

The consistency of the criterion (cid:98)C, i.e.

P( (cid:98)m = m∗) −→

n→∞

1;

(2.9)

will be established after showing that both of following probabilities are zero:

• the asymptotic probability of selecting a larger model containing the true model (overﬁtting

case);

• the asymptotic probability of selecting a false model that is a model not containing m∗.

3. Asymptotic results

3.1. Assumptions required for the asymptotic study

The following classical assumption ensures the identiﬁability of the model considered.

Assumption Id(Θ): For all θ, θ(cid:48) ∈ Θ, (f 0

θ = f 0

θ(cid:48) and M 0

θ = M 0

θ(cid:48)) a.s. =⇒ θ = θ(cid:48).

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

7

Another required assumption concerns the diﬀerentiability of Ψθ = fθ or Mθ on Θ. This type of
assumption has already been considered in order to apply the QMLE procedure (see [7], [44], [48]).
First, the following Assumption Var(Θ) provides the invertibility of the "Fisher’s information matrix"
of X and is important to prove the asymptotic normality of the QMLE.

Assumption Var: (cid:0) (cid:80)d
∀i = 1, . . . , d, αi = 0 a.s(cid:1).

i=1 αi

∂f 0
θ

∂θ(i) = 0 =⇒ ∀i = 1, . . . , d, αi = 0 a.s(cid:1) or (cid:0) (cid:80)d

i=1 αi

∂H 0
∂θ(i) = 0 =⇒
θ

Moreover, one of the following technical assumption is required to establish the consistency of the
model selection procedure.

Assumption K(Θ): Assumptions A(fθ, Θ), A(Mθ, Θ), A(∂θfθ, Θ), A(∂θMθ, Θ) and B(Θ) hold and
there exists r ≥ 2 such that θ∗ ∈ Θ(r). Moreover, with s = min(1, r/3), assume that the sequence
(κn)n∈N satisﬁes

(cid:88)

(
k≥1

1
κk

)s(cid:16) (cid:88)

j≥k

αj(fθ, Θ) + αj(Mθ, Θ) + αj(∂θfθ, Θ) + αj(∂θMθ, Θ)

(cid:17)s

< ∞.

Assumption (cid:102)K(Θ): Assumptions A( (cid:101)Hθ, Θ), A(∂θ (cid:101)Hθ, Θ) and B(Θ) hold and there exists r ≥ 2 such
that θ∗ ∈ Θ(r). Moreover, with s = min(1, r/4), assume that the sequence (κn)n∈N satisﬁes

(

1
κk

(cid:88)

k≥1

)s(cid:16) (cid:88)

j≥k

αj( (cid:101)Hθ, Θ) + αj(∂θ (cid:101)Hθ, Θ)

(cid:17)s

< ∞.

Remark 1. These conditions on (κn)n∈N have been deduced from conditions for strong law of large
numbers obtained in [27] and are not too restrictive: for instance, if the Lipschitzian coeﬃcients of fθ,
Mθ (the case using (cid:101)Hθ can be treated similarly) and their derivatives are bounded by a geometric or
Riemanian decrease:

1. the geometric case: αj(fθ, Θ) + αj(Mθ, Θ) + αj(∂θfθ, Θ) + αj(∂θMθ, Θ) = O(aj) with 0 ≤ a < 1,
then any (κn) such as 1/κn = o(1) can be chosen; for instance κn = log n or log(log n); this is
the case for instance of ARMA, GARCH, APARCH or ARMA-GARCH processes.

2. the Riemanian case: αj(fθ, Θ) + αj(Mθ, Θ) + αj(∂θfθ, Θ) + αj(∂θMθ, Θ) = O(j−γ) with γ > 1:

• if r ≥ 3 then

– if γ > 2 then any sequence such as 1/κn = o(1) can be chosen;
– if 1 < γ < 2, any (κn) such as κn = O(nδ) with δ > 2 − γ can be chosen.

• if 1 ≤ r < 3

– if γ > (r + 3)/r then any sequence such as 1/κn = o(1) can be chosen;
– if 1 < γ < (r + 3)/r then any (κn) such as κn = nδ with δ > (r + 3)/r − γ can be

chosen.

In the last case of these two conditions on r, we can see the usual BIC choice, κn = log n
does not fulﬁll the assumption in general.

3.2. New versions of limit theorems in [7]

These assumptions K(Θ) and (cid:101)K(Θ) used in Lemmas 1 and 2 (see Section 7) and the detailed Riema-
nian convergence rates of the previous remark, provide an improvement of the two main limit theorems
established in [7]. More precisely, we obtain:

New version of Theorem 1 in [7]
Let (X1, . . . , Xn) be an observed trajectory of an aﬃne causal process X belonging to AC(Mθ∗ , fθ∗ ) (or

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

8

(cid:102)AC( (cid:101)Hθ)) where θ∗ is an unknown vector of Θ, a compact set included in Θ(r) ⊂ Rd (or (cid:101)Θ(r) ⊂ Rd)
with r ≥ 2. Then, if assumptions A(fθ, Θ), A(Mθ, Θ) (or A( (cid:101)Hθ, Θ)), D(Θ), Id(Θ) hold with

(cid:26) αj(fθ, Θ) + αj(Mθ, Θ) = O(cid:0)j−(cid:96)(cid:1)

or αj( (cid:101)Hθ, Θ) = O(cid:0)j−(cid:101)(cid:96)(cid:1)

for some (cid:96) > max(1 , 3/r)
for some (cid:101)(cid:96) > max(1 , 4/r)

,

(3.1)

then the QMLE (cid:98)θ(m∗) satisﬁes (cid:98)θ(m∗)

a.s.−→
n→+∞

θ∗.

Proof. We use the same proof as in [7] except for establishing 1
n

(cid:13)(cid:98)Ln(θ) − Ln(θ)(cid:13)
(cid:13)
(cid:13)Θ

a.s.−→
n→+∞

0. Indeed,

we can apply Lemma 1 with κn = n. Hence, this is checked under assumption K(Θ) under Riemanian
condition of Remark 1 if r ≥ 3 when γ = (cid:96) > 1 and if 2 ≤ r ≤ 3, when γ = (cid:96) > 3/r, implying the ﬁrst
new conditions of the Theorem.
Under assumption (cid:102)K(Θ), an adaptation of Remark 1 implies that for r ≥ 4 we should have γ = (cid:101)(cid:96) > 1
(cid:4)
and if 2 ≤ r ≤ 4, when γ = (cid:101)(cid:96) > 4/r.
Therefore, in all the previous cases and when r = 4, we obtain a limiting decrease rate O(cid:0)j−γ(cid:1) with
γ > 1 instead of γ > 3/2 in [7]. This can also be used to improve Theorem 2 in [7]:

New version of Theorem 2 in [7]
If r ≥ 4 and under the assumptions of the previous new version of Theorem 1 in [7], and Var(Θ), and if
assumptions A(∂θfθ, Θ), A(∂θMθ, Θ), A(∂2
θ2fθ, Θ) and A(∂2
θ2 (cid:101)Hθ, Θ)
) hold with

θ2 Mθ, Θ) (or A(∂θ (cid:101)Hθ, Θ) and A(∂2

(cid:26) αj(∂θfθ, Θ) + αj(∂θMθ, Θ) = O(cid:0)j−(cid:96)(cid:48)(cid:1)

or αj(∂θ (cid:101)Hθ, Θ) = O(cid:0)j−(cid:96)(cid:48)(cid:1)

for some (cid:96)(cid:48) > 1,

(3.2)

then the QMLE (cid:98)θn(m∗) satisﬁes

√

n

(cid:16)(cid:0)

(cid:98)θ(m∗)(cid:1)

i − (θ∗)i

(cid:17)

L−→
n→+∞

N|m∗|

i∈m∗

(cid:0)0 , F (θ∗, m∗)−1G(θ∗, m∗)F (θ∗, m∗)−1(cid:1),

(3.3)

with (cid:0)F (θ∗, m∗)(cid:1)

i,j = E

(cid:104) ∂2q0(θ∗)
∂θi∂θj

(cid:105)

and (G(θ∗, m∗))i,j = E

(cid:104) ∂q0(θ∗)
∂θi

(cid:105)

∂q0(θ∗)
∂θj

for i, j ∈ m∗.

3.3. Asymptotic model selection

Using the above assumptions, we can establish the limit theorem below, which provides suﬃcient
conditions for the consistency of the model selection procedure.

Theorem 3.1. Let (X1, . . . , Xn) be an observed trajectory of an aﬃne causal process X belonging to
AC(Mθ∗ , fθ∗ ) (or (cid:102)AC( (cid:101)Hθ)) where θ∗ is an unknown vector of Θ a compact set included in Θ(r) ⊂ Rd (or
(cid:101)Θ(r) ⊂ Rd) with r ≥ 4. If assumptions D(Θ), Id(Θ), K(Θ) (or (cid:101)K(Θ)), A(∂2
θ2Mθ, Θ)
(or A(∂2

θ2 fθ, Θ) and A(∂2

θ2 (cid:101)Hθ, Θ)) also hold, then

P( (cid:98)m = m∗) −→

n→∞

1 and (cid:98)θ( (cid:98)m)

P−→
n→∞

θ∗.

(3.4)

The following theorem shows the asymptotic normality of the QMLE of the chosen model.

Theorem 3.2. Under the assumptions of Theorem 3.1 and if θ∗ ∈

o
Θ and Var(Θ) hold, then

√

n

(cid:16)(cid:0)

(cid:98)θ( (cid:98)m)(cid:1)

i − (θ∗)i

(cid:17)

L−→
n→+∞

N|m∗|

i∈m∗

(cid:0)0 , F (θ∗, m∗)−1G(θ∗, m∗)F (θ∗, m∗)−1(cid:1),

(3.5)

where F and G are deﬁned in (3.3).

Remark 2. In Remark 1, we detailed some situations where the assumption K(Θ) (or (cid:101)K(Θ)) holds,
which leads to the results of Theorem 3.1 and 3.2. In particular, the log n penalty usually linked to
BIC is consistent in the case of a geometric decrease of the Lipschitz coeﬃcients of the functions fθ
and Mθ (and their ﬁrst order derivative). In the case of a Riemanian rate, the consistency of BIC is
not ensured; see also the next section.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

9

4. Examples

In this section, some examples of time series satisfying the conditions of previous results are considered.
These examples include AR(∞), ARCH(∞), AP ARCH(δ, p, q) and ARMA(p, q)-GARCH(p(cid:48), q(cid:48)).

4.1. AR(∞) models

For (ψk(θ))k∈N a sequence of real numbers depending on θ ∈ Rd, let us consider an AR(∞) process
deﬁned by:

(cid:88)

Xt =

ψk(θ∗)Xt−k + σ ξt

for any t ∈ Z,

(4.1)

k≥1

where (ξt)t admits 4-order moments, and θ∗ ∈ Θ ⊂ Θ(4), the set of θ ∈ Rd such that (cid:80)
k≥1 (cid:107)ψk(θ)(cid:107)Θ <
1 and σ > 0. This process corresponds to (2.2) with fθ
k≥1 ψk(θ)xk and Mθ ≡ σ for any
θ ∈ Θ. The Lipschitz coeﬃcients of fθ are αk(fθ) = (cid:107)ψk(θ)(cid:107)Θ. Moreover, Assumption D(Θ) holds with
h = σ2 > 0.

(cid:0)(xi)i≥1

(cid:1) = (cid:80)

Let us consider M a ﬁnite family of models. Of course, the main example of such family of models
is given by the one of ARMA(p, q) processes with 0 ≤ p ≤ pmax and 0 ≤ q ≤ qmax, providing
(pmax + 1)(qmax + 1) models and θ ∈ Rpmax+qmax+1.

Besides, assume that Id(Θ), Var(Θ) hold and that the sequence (ψk) is twice diﬀerentiable (with
respect to θ) on Θ, with (cid:80)
θ ψk(θ)(cid:107)Θ < ∞ and (cid:107)ψk(θ)(cid:107)Θ + (cid:107)∂θψk(θ)(cid:107)Θ = O(k−γ) with γ > 1.
From Remark 1,

k (cid:107)∂2

√

• if γ > 2, the condition κn −→
n→∞

∞ (for instance, the BIC penalization with κn = log(n), or

o
Θ;

κn =

n) ensures the consistency of (cid:98)m and the Theorem (3.2) holds if in addition θ∗ ∈
• if 1 < γ < 2, κn = O(nδ) with δ > 2 − γ has to be chosen (and we cannot insure the consistency

of (cid:98)m in case of classical BIC penalization).

Finally, in the particular case of the family of ARMA processes, the stationarity condition implies that
n), since the decreases
any κn −→
n→∞
of ψk and its derivative are exponential.

∞ can be chosen (BIC penalization with κn = log(n), or κn =

√

4.2. ARCH(∞) models

For (ψk(θ))k∈N a sequence of nonnegative real numbers depending on θ ∈ Rd, with ψ0 > 0, let us
consider an ARCH(∞) process deﬁned by :

Xt =

(cid:16)

ψ0(θ∗) +

∞
(cid:88)

k=1

ψk(θ∗)X 2

t−k

(cid:17)1/2

ξt

for any t ∈ Z,

(4.2)

0

(cid:3) < ∞, and θ∗ ∈ Θ ⊂ (cid:101)Θ(4), the set of θ ∈ Rd such that (cid:80)

where E(cid:2)ξ4
process corresponds to (2.2) with fθ
(cid:1) = ψ0(θ) + (cid:80)∞
(cid:101)Hθ
(cid:107)ψk(θ)(cid:107)Θ. Moreover, Assumption D(Θ) holds if h = inf θ∈Θ ψ0(θ) > 0.

k≥1 (cid:107)ψk(θ)(cid:107)Θ < 1. This
(cid:0)(xi)i≥1
k=1 ψk(θ)x2
k, i.e.
k=1 ψk(θ)yk, for any θ ∈ Θ. The Lipschitz coeﬃcients of (cid:101)Hθ are αk( (cid:101)Hθ) =

(cid:1) = ψ0(θ) + (cid:80)∞

(cid:1) ≡ 0 and Hθ

(cid:0)(xi)i≥1

(cid:0)(yi)i≥1

Let us consider M a ﬁnite family of models. The main example of such family of models is given by the
GARCH(p, q) processes with 0 ≤ p ≤ pmax and 0 ≤ q ≤ qmax, providing (pmax + 1)(qmax + 1) models
and θ ∈ Rpmax+qmax+1.

Moreover, assume that Id(Θ), Var(Θ) hold and that the sequence (ψk) is twice diﬀerentiable (with
respect to θ) on Θ, with (cid:80)
θ ψk(θ)(cid:107)Θ < ∞ and (cid:107)ψk(θ)(cid:107)Θ + (cid:107)∂θψk(θ)(cid:107)Θ = O(k−γ) with γ > 1.
From Remark 1,

k (cid:107)∂2

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

10

√

• if γ > 2, the condition κn −→
n→∞

∞ (for instance, the BIC penalization with κn = log(n), or

o
Θ;

κn =

n) ensures the consistency of (cid:98)m and the Theorem (3.2) holds if in addition, θ∗ ∈
• if 1 < γ < 2, κn = O(nδ) with δ > 2 − γ has to be chosen (and we cannot insure the consistency

of (cid:98)m in the case of the classical BIC penalization).

Finally, in the particular case of the family of GARCH processes, the stationarity condition implies
n), since the
that any κn −→
n→∞
decreases of ψk and its derivative are exponential.

∞ can be chosen (BIC penalization with κn = log(n), or κn =

√

4.3. APARCH(δ, p, q) models

For δ ≥ 1 and from [12], (Xt)t∈Z is an APARCH(δ, p, q) process with p, q ≥ 0 if:

(cid:40)

Xt = σt ξt
(σt)δ = ω + (cid:80)p

i=1 αi(|Xt−i| − γiXt−i)δ + (cid:80)q

j=1 βj(σt−j)δ

for any t ∈ Z,

(4.3)

where ω > 0, −1 < γi < 1, αi ≥ 0, βj ≥ 0 for 1 ≤ i ≤ p and 1 ≤ j ≤ q, αp > 0, βq > 0 and
(cid:80)q
j=1 βj < 1. From [6], with θ = (ω, α1, . . . , αp, γ1, . . . , γp, β1, . . . , βp)(cid:48), the conditional variance σt can

be rewritten as follows

σδ
t = b0(θ) +

(cid:88)

(cid:16)

k≥1

k (θ)(max(Xt−k, 0))δ − b−
b+

k (θ)(min(Xt−k, 0))δ(cid:17)

;

with fθ ≡ 0 and M t
assumption (cid:80)q
stationarity set for r ≥ 1 is

θ = σt, we deduce that αk(Mθ, Θ) = max((cid:107)b+

Θ ), and from the
j=1 βj < 1, the Lipschitz coeﬃcients αk(Mθ, Θ) decrease exponentially fast. Then, the

Θ , (cid:107)b−

k (θ)(cid:107)1/δ

k (θ)(cid:107)1/δ

Θ(r) =

(cid:110)
θ ∈ R2p+q+1 (cid:46)

(cid:107)ξ0(cid:107)r

∞
(cid:88)

j=1

max (cid:0)|b+

j (θ)|1/δ, |b−

j (θ)|1/δ(cid:1) < 1

(cid:111)
.

Now, assume that (Xt)t∈Z is an APARCH(δ, p∗, q∗) where 0 ≤ p∗ ≤ pmax and 0 ≤ q∗ ≤ qmax are
unknown orders as well as the other parameters: ω∗ > 0, −1 < γ∗
j ≥ 0 for 1 ≤ i ≤ pmax
and 1 ≤ j ≤ qmax, αp∗ > 0, βq∗ > 0.

i < 1, α∗

i ≥ 0, β∗

Let M be the family of APARCH(δ, p, q) processes, with 0 ≤ p ≤ pmax and 0 ≤ q ≤ qmax. As a
consequence, we consider here d = 2pmax + qmax + 1, and

θ∗ = t(cid:0)ω∗, α∗

1, . . . , α∗

p∗ , 0, . . . , 0, γ∗

1 , . . . , γ∗

p∗ , 0, . . . , 0, β∗

1 , . . . , β∗

q∗ , 0, . . . , 0(cid:1) ∈ Rd.

With all the previous conditions, assumptions D(Θ), Id(Θ), Var(Θ) are satisﬁed. Moreover, since the
Lipschitz coeﬃcients decrease exponentially fast, K(Θ) is satisﬁed when κn → ∞. Therefore, the
consistency Theorem (3.1) and the Theorem (3.2) of the estimator of the chosen model are satisﬁed
when r = 4 and κn → ∞ (for instance with the typical BIC penalty κn = log n).

4.4. ARMA(p, q)-GARCH(p(cid:48), q(cid:48)) models

From [12] and [33], we deﬁne (Xt)t∈Z as an (invertible) ARMA(p, q)-GARCH(p(cid:48), q(cid:48)) process with
p, q, p(cid:48), q(cid:48) ≥ 0 if:

(cid:40)

Xt = (cid:80)p
εt = σt ξt, with σ2

i=1 ai Xt−i + εt − (cid:80)q
t = c0 + (cid:80)p(cid:48)

i=1 bi εt−i
i=1 ci ε2

t−i + (cid:80)q(cid:48)

i=1 di σ2

t−i

for all t ∈ Z,

where

• c0 > 0, cp(cid:48) > 0, ci ≥ 0 for i = 1, · · · , p(cid:48) − 1 and dq(cid:48) > 0, di ≥ 0 for i = 1, · · · , q(cid:48) − 1;
• P (x) = 1 − (cid:80)p

i=1 bixi are coprime polynomials.

i=1 aixi and Q(x) = 1 − (cid:80)q

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

11

Here we will consider the case of a stationary invertible ARMA(p, q)-GARCH(p(cid:48), q(cid:48)) process such as
(cid:107)X0(cid:107)4 < ∞ and therefore we will consider:

Θp,q,p(cid:48),q(cid:48) =

(cid:110)
(a1, . . . , dq(cid:48)) ∈ Rp+q+p(cid:48)+1+q(cid:48)

,

q(cid:48)
(cid:88)

j=1

dj + (cid:107)ξ0(cid:107)4

and (cid:0)1 −

p
(cid:88)

j=1

p(cid:48)
(cid:88)

j=1

cj < 1

ajzj(cid:1) (cid:0)1 −

q
(cid:88)

j=1

bjzj(cid:1) (cid:54)= 0 for all |z| ≤ 1

(cid:111)
.

Therefore, if (a1, . . . , dq(cid:48)) ∈ Θp,q,p(cid:48),q(cid:48), (εt)t is a stationary GARCH(p(cid:48), q(cid:48)) process and (Xt)t is a sta-
tionary weak invertible ARMA(p, q) process.
Moreover, following Lemma 2.1. of [6], we know that a stationary ARMA(p, q)-GARCH(p(cid:48), q(cid:48)) process
is a stationary aﬃne causal process with functions fθ and Mθ satisfying the Assumption A(fθ, Θ) and
A(Mθ, Θ) with Lipschitzian coeﬃcients decreasing exponentially fast, as well as their derivatives. Fi-
nally, if Θ is a bounded subset of Θp,q,p(cid:48),q(cid:48), then assumptions D(Θ), Id(Θ) and Var(Θ) are automatically
satisﬁed.

Assume now that (Xt)t∈Z is an ARMA(p∗, q∗)-GARCH(p
q∗ ≤ qmax, 0 ≤ p
0, . . . , c∗
c∗

max and 0 ≤ q
p∗ , b∗
1, . . . , a∗

(cid:48)∗ ≤ q(cid:48)
1, . . . , bq∗ .

1, . . . , d∗

(cid:48)∗ ≤ p(cid:48)

p(cid:48) ∗ , d∗

q(cid:48) ∗ , a∗

(cid:48)∗) process where 0 ≤ p∗ ≤ pmax, 0 ≤
max are unknown orders with also unknown parameters:

(cid:48)∗, q

(cid:48)
Let M be the family of ARMA(p, q)-GARCH(p
0 ≤ p(cid:48) ≤ p(cid:48)

max and 0 ≤ q(cid:48) ≤ q(cid:48)

(cid:48)

, q

max. Hence, we consider here d = pmax + qmax + p(cid:48)

) processes, with 0 ≤ p ≤ pmax, 0 ≤ q ≤ qmax,

θ∗ = (cid:0)c∗

0, . . . , c∗

p(cid:48) ∗ , 0, . . . , 0, d∗

1, . . . , d∗

q(cid:48) ∗ , 0, . . . , 0, a∗

1, . . . , a∗

p∗ , 0, . . . , 0, b∗

max + q(cid:48)

max + 1, and
1, . . . , bq∗ , 0, . . . , 0(cid:1) ∈ Rd.

, all the previous assumptions D(Θ), Id(Θ), Var(Θ)
With Θ a bounded subset of Θpmax,qmax,p(cid:48)
are satisﬁed and K(Θ) is also satisﬁed as soon as κn → ∞. As a consequence, in this framework the
consistency Theorem (3.1) and the Theorem (3.2) of the estimator of the chosen model are satisﬁed
when r = 4 and κn → ∞ (for instance with the typical BIC penalty κn = log n).

max,q(cid:48)

max

5. Portmanteau test

From the above section, we are now able to asymptotically pick up a best model in a family of models.
We can also obtain asymptotic conﬁdent regions of the estimated parameter of the chosen model.
However, it is also important to check whether the chosen model is appropriate. This section attempts
to answer this question by constructing a portmanteau test as a diagnostic tool based on the squares
of the residuals sequence of the chosen model.
This test has been widely considered in the time series literature, with procedures based on the squared
residual correlogram (see for instance [31], [32] ) and the absolute residual (or usual residuals) correl-
ogram (see for instance [30], [14], [29]), among others.
Since our goal is to provide an eﬃcient test for the entire aﬃne class that contains weak white noise
processes, we consider in this setting the autocorrelation of the squared residuals and then we will
follow the same scheme of procedure used in ([31], [32]) while relying on some of their results.

For m ∈ M, for K a positive integer, denote the vector of adjusted correlogram of squares residuals
by:

(cid:98)ρ(m) := (cid:0)

(cid:98)ρ1(m), . . . , (cid:98)ρK(m)(cid:1)(cid:48)

,

where for k = 1, . . . , K, (cid:98)ρk(m) := (cid:98)γk(m)
(cid:98)γ0(m)

with

(cid:98)γk(m) :=

1
n

n
(cid:88)

t=k+1

(cid:0)

t (m) − 1(cid:1)(cid:0)
(cid:98)e2

t−k(m) − 1(cid:1) and (cid:98)et(m) := (cid:0)
(cid:98)e2

(cid:99)M t

(cid:98)θ(m)

(cid:1)−1(cid:0)Xt − (cid:98)f t

(cid:98)θ(m)

(cid:1).

Finally, the following theorem provides central limit theorems for (cid:98)ρ(m∗) and (cid:98)ρ( (cid:98)m) as well as for a
portmanteau test statistic.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

12

Theorem 5.1. Under the assumptions of Theorem 3.2, if

• E[ξ3
∞
(cid:88)

0] = 0;
t−1/4(cid:16) (cid:88)

•

t=1

j≥t

then,

αj(fθ, Θ) + αj(Mθ, Θ)

(cid:17)1/2

< ∞ or

∞
(cid:88)

t=1

t−1/4(cid:16) (cid:88)

αj( (cid:101)Hθ, Θ)

(cid:17)1/2

< ∞;

j≥t

1. With V (θ∗, m∗) deﬁned in (7.37), it holds that

√

n (cid:98)ρ(m∗)

L−→
n→+∞

NK

(cid:0)0 , V (θ∗, m∗)(cid:1).

2. With (cid:98)QK(m∗) := n (cid:98)ρ(m∗)(cid:48)(cid:0)V ((cid:98)θ(m∗), m∗)(cid:1)−1

(cid:98)ρ(m∗), we have

(cid:98)QK(m∗)

L−→
n→+∞

χ2(K).

3. The previous points 1. and 2. also hold when m∗ is replaced by (cid:98)m.

Using the Theorem 5.1, we can asymptotically test:






H0 : ∃m∗ ∈ M, such as (X1, . . . , Xn) is a trajectory of X ∈ AC(Mθ, fθ∗ ) with θ∗ ∈ Θ(m∗)

H1 : (cid:64)m∗ ∈ M, such as (X1, . . . , Xn) is a trajectory of X ∈ AC(Mθ, fθ∗ ) with θ∗ ∈ Θ(m∗)

(5.1)

(5.2)

.

Therefore, (cid:98)QK( (cid:98)m) can be used as a portmanteau test statistic to decide between H0 and H1 and
diagnose the goodness-of-ﬁt of the selected model.

(cid:1)(cid:105)

(cid:104)(cid:0)ξ2

1. Like in [31], it is important to point out that for ARCH(p) model, since f t
0 −1(cid:1) ∂θ log (cid:0)M k

θ = 0, we
Remark 3.
have E
= 0 for all k > p. Hence, for these models, the matrix V (θ∗, m∗)−IK
will have approximately zero entries from the (p + 1)th row onwards and then the standard errors
of (cid:98)ρ(m∗)i are in this case equal to 1/
n for i = p + 1, . . . , K. The statistic (cid:98)QK(m∗) yields to
(cid:98)Q(p, K) := n (cid:80)K
i=p+1[(cid:98)ρ(m∗)i]2 which will be asymptotically χ2 distributed with K − p degrees
of freedom.

√

θ∗

2. In practice the constant µ4 and the rows of the matrix V ((cid:98)θ(m∗), m∗) involved in the previ-
ous theorem are estimated by the correspondent sample average; they are respectively (cid:98)µ4 =
(cid:80)n

(cid:80)n

t=1((cid:98)et( (cid:98)m))4 and (cid:0)

(cid:98)V ((cid:98)θ(( (cid:98)m)), ( (cid:98)m))(cid:1)

k,. = 1

n

t=k+1[((cid:98)et( (cid:98)m))2 − 1][∂θ log (cid:0)M k

1
n

θ )](θ=(cid:98)θ( (cid:98)m)).

6. Numerical Results

This section features some simulation experiments that are performed to assess the usefulness of the
asymptotic results obtained in Section 3. The various conﬁgurations studied are presented below and
n. The process used to generate the trajectory is
we compare the performance of penalties log n and
indicated each time.

√

Each model is generated independently 1000 times over a trajectory of length n. Diﬀerent sample
sizes are considered to identify possible discrepancies between asymptotically expected properties and
those obtained at ﬁnite distance. We will consider n belongs to {100, 500, 1000, 2000}. Throughout this
section, (ξt) represents a Gaussian white noise with variance unity.

6.1. Classical conﬁgurations

We ﬁrst simulate some classical model illustrated as follows and the results are displayed in the Table
1.

1. Model 1, AR(2) process: Xt = 0.4Xt−1 + 0.4Xt−2 + ξt.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

13

2. Model 2, ARMA(1, 1) process: Xt = 0.3Xt−1 + ξt + 0.5ξt−1.
t−1 + 0.2X 2
3. Model 3, ARCH(2) process: Xt = ξt

0.2 + 0.4X 2

(cid:113)

t−2.

We considered as competitive models all the models in the family M deﬁned by:

M = (cid:8)ARMA(p, q) or GARCH(p(cid:48), q(cid:48)) processes with 0 ≤ p, q, p(cid:48) ≤ 5, 1 ≤ q(cid:48) ≤ 5(cid:9).

As a consequence, there are 66 candidate models.

The Table 1 shows for each penalty (log n and
n) the percentage of times the associated crite-
rion selects respectively a wrong model, the true model and an overﬁtted model (here a model which
contains the true model).

√

Table 1: Percentage of selected order based on 1000 replications depending on sample’s length for
Model 1, 2 and 3 respectively.

1000

√

n

2000

log n

Sample length n
Penalty

100

log n

√

n

500

log n

√

n

Wrong
True
Overﬁtted

Wrong
True
Overﬁtted

Wrong
True
Overﬁtted

21
74.6
4.4

81.8
16.1
2.1

78.9
20.4
0.1

32.3
67.5
0.2

97.5
2.5
0

92.9
7.0
0.1

3
95.8
1.2

30.1
69.1
0.8

25.7
73.2
1.1

0.9
99.1
0

67.4
32.6
0

70.5
29.5
0

Model 1

Model 2

Model 3

log n

0.9
98.2
0.9

19.9
79.5
0.6

11.6.
88.1
0.3

0
100
0

33.2
66.8
0

39.2
60.8
0

0.2
99
0.8

10.2
89.4
0.4

5.4
94.3
0.3

√

n

0
100
0

10.5
89.5
0

11.4
88.6
0

From these results, it is clear that the consistency of our model selection procedure is numerically
convincing, which is in accordance with Theorem 3.1, where both the criteria are consistent for Model
1, 2 and 3. Note also that the typical BIC log n penalty is the most interesting for retrieving the true
model than the
n-penalized likelihood for a small sample size. But the larger the sample size, the
√
more accurate the

n penalty case.

√

For each of the three models, we also applied the portmanteau test statistic (cid:98)QK( (cid:98)m), using the
n
penalty. Table 2 shows the empirical size and empirical power of this test. We call by empirical
size, the percentage of falsely rejecting the null hypothesis H0. On the other hand, the empirical
power represents the percentage of rejection of H0 when we arbitrary chose a false model, which is a
AR(3) process Xt = 0.2Xt−1 + 0.2Xt−2 + 0.4Xt−1 + ξt for Model 1 and 2, and a ARCH(3) process
Xt = ξt
t−3 for Model 3.

t−1 + 0.2X 2

t−2 + 0.2X 2

0.4 + 0.2X 2

(cid:113)

√

It is important to note that choosing the maximum number of lags K is sometimes tricky. To our
knowledge, there is no real theoretical study to justify the choice of one value or another. However,
some Monte Carlo simulations have suggested some ways to make a good choice . For instance [31]
suggested that the autocorrelations (cid:98)ρk( (cid:98)m) with 1 ≤ k ≤ K have a better asymptotic behaviour for
small values of k. Therefore, the ﬁnite sample performance of the size and power of the test may also
vary with the choice of K and could be better for small values of K. On the other hand, [47] suggested
that K = p + q + 1 may be an appropriate choice for the GARCH(p, q) family.
Thus, in our tests, we consider K = 3 and K = 6 so that the rejection is based on the upper 5th
percentile of the χ2(3) distribution on the one hand and χ2(6) on the other hand.

Table 2: The empirical size and empirical power of the portmanteau test statistic (cid:98)QK( (cid:98)m) based on
1000 independent replications (in %) with K = 3 and K = 6.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

14

Sample length

100

500

1000

2000

size

power

size

power

size

power

size

power

K = 3

K = 6

Model 1
Model 2
Model 3

Model 1
Model 2
Model 3

3.5
4.0
4.3

3.5
2.1
3

13.6
6.7
52.7

9.4
6.3
18.3

3.8
5
4.2

4.8
4.9
3.1

48.1
21.7
98.6

43.1
18
91.5

3.5
4.8
3.2

5.3
4.5
3.4

82.7
38.6
99.6

74.6
32.2
99.6

3.2
4.4
3.6

4.5
6.4
6.8

97.7
64.2
99.9

97.6
61.3
99.7

Once again, the results of Table 2 numerically conﬁrms the asymptotic results of Theorem 5.1. Remark
that the test is more powerful by using values of K not too large as mentioned above especially for
small samples.

6.2. Subset model selection

Now, we exhibit the performance of the criteria on a particular case of dimension selection. The process
generated data is considered as follows:

Model 4 : Xt = 0.4Xt−3 + 0.4Xt−4 + ξt.
Here, we will consider the case of a nonhierarchical but exhaustive family M of AR(4) models , i.e.

M = P({1, 2, 3, 4})

=⇒ Xt = θ1Xt−1 + θ2Xt−2 + θ3Xt−3 + θ4Xt−4 + ξt and θ = (θ1, θ2, θ3, θ4)(cid:48) ∈ Θ(m).

As a consequence, 16 = 24 candidate models are considered and Table 3 presents the results of the
selection procedure.

Table 3: Percentage of selected model based on 1000 replications depending on sample’s length for
Model 4

Sample length

100

true model
overﬁtted
false model

log n

85.9
7.9
6.2

√

n

68
2
30

500

log n

97.5
2.5
0

√

n

100
0
0

log n

96.8
3.2
0

1000

√

2000

√

n

n

log n

100
0
0

98.9
1.1
0

100
0
0

We deduce that the consistency of our model selection procedure is also numerically convincing in this
case of exhaustive model selection, which is in accordance with Theorem 3.1

6.3. Slow decrease of the Lipschitz coeﬃcients

In this subsection, we consider an AR(2) − ARCH(∞) with a slow decrease of its Lipschitz coeﬃcients
in order to numerically show that the penalty log n is not consistent in all cases. The considered data
generating process is featured as follows:

Model 5 : Xt = −0.45 Xt−1 + 0.4 Xt−2 + ξt with ξt = εt

(cid:115)

0.5 + 0.1

t−i/i3,
ξ2

(cid:88)

i≥1

where εt is an i.i.d random sequence with mean 0 and variance 1. The sequence (αi)i≥1 veriﬁes αi =
O(i−3) so that the sequence of Lipschitz coeﬃcients of M ξ
θ ) = O(i−1.5) and then
the decrease rate of the sequence (cid:0)αi(M X
θ )(cid:1) is equal to O(i−1.5). From Remark 1, all penalties such
as nδ with δ > 2 − 1.5 = 0.5 will lead to a consistent model selection criterion and this is not the case
for the typical BIC log n penalty. We have considered δ = 2/3 as in the Bridge Criteria (BC) recently
proposed in [10]. Here the family of model M is deﬁned by
M = (cid:8)AR(p)-ARCH(∞) processes with 1 ≤ p ≤ 8, where the ARCH(∞) is deﬁned as in Model 5(cid:9).

θ is given by αi(M ξ

The results of simulations are featured in Table 4.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

15

Table 4: Percentage of selected order based on 1000 replications depending on sample’s length for
model 5

Sample length

100
log n n2/3

500
log n n2/3

1000
log n n2/3

2000
log n n2/3

p < 2
p = 2
p > 2

8.9
88.9
2.2

69.1
30.9
0

0.1
75.3
24.6

17.6
82.4
0

0
71.4
28.6

6.1
93.9
0

0
72.5
27.5

0.9
99.1
0

Note that we also computed the log n criterion for n = 5000 and n = 10000 in additional numerical
experiments and its frequencies of choice of the true order p = 2 were almost 72%. As a consequence,
for this model selection framework of the inﬁnite memory process with a slow decrease of Lipschitz
coeﬃcients, the usual BIC penalty log n seems numerically not suﬃcient to avoid overﬁtting in contrast
with a n2/3 penalty that leads to a consistent criterion.

6.4. Illustrative Example

We consider the returns of the daily closing prices of the FTSE index of the London Stock Exchange
100. They are 2273 observations from January 4th, 2010 to December 31st, 2018. The mean and
standard deviation of the returns are -0.54 and 57.67, respectively. The Time plot and the correlograns
for the log returns and squared log returns are plotted in Figure 1.

The Figures (1a) and (1c) exhibit the conditional heteroskedasticity in the log return time series.
Moreover, Figure (1b) shows that more than 5 per cent of the autocorrelations are out of the conﬁdence
2273 and specially the Figure (1d) suggests that the strong white noise assumption
interval ±1.96/
cannot be sustained for this log-returns sequence of FTSE index.

√

×

1; 10
(cid:75)

Therefore, the GARCH(p, q) family was considered for the modelling of the FTSE index with
(p, q) ∈
n have
been applied to identify the best order and the goodness-of-ﬁt of the selected model has been tested
by the portmanteau test. Based on the results of the simulations, we set K = 3 for the portmanteau
test statistic.

which lead us to 110 candidate models. The penalization log n and

0; 10
(cid:75)

√

(cid:74)

(cid:74)

The GARCH(1, 1) is the "best" model according to both criteria (related to above penalizations)
and the portmanteau statistic (cid:98)Q3( (cid:98)m) (cid:39) 2.13 is associated with a p-value of 0.55. Hence, the selected
model GARCH(1, 1) is adequate to model the FTSE 100 index using either criterion.

7. Proofs

We start with the proof of the Proposition 1.

α1 = hα + (cid:96)(1)
α(cid:48)
1

Proof. For ease of writing, consider only the general case where f (i)
where
θi
θi = t(αi, βi) for i = 1, 2. Now, assume that there exist α ∈ Rδ, where 0 ≤ δ ≤ min(d1, d2) and a
function hα such as g(1)
0 = 0.
Similarly, assume that there exist β ∈ Rδ(cid:48)
, where 0 ≤ δ(cid:48) ≤ min(d1, d2) and a function Rβ such as
N (1)
, N (2)
= Rβ + m(1)
β(cid:48)
β2
β1
1
Consider now θ = t(α, α(cid:48)
(cid:96)(1)
α(cid:48)
1

with β1 = t(β, β(cid:48)
2) ∈ Rd (and therefore max(d1, d2) ≤ d ≤ d1 + d2), fθ = hα +
. Then if X ∈ AC(cid:0)Mθ, fθ

= Rβ + m(2)
β(cid:48)
2
1, β(cid:48)
2, β, β(cid:48)
1, α(cid:48)
+ m(2)
β(cid:48)
2

and Mθ = Rβ + m(1)
β(cid:48)
1

1) and α2 = t(α, α(cid:48)

1) and β2 = t(β, β(cid:48)

α2 = hα + (cid:96)(2)
α(cid:48)
2

(cid:1), for any t ∈ Z,

with α1 = t(α, α(cid:48)

αi and M (i)
θi

2) and m(i)

2) and (cid:96)(i)

= N (i)
βi

+ (cid:96)(2)
α(cid:48)
2

0 = 0.

= g(i)

, f (2)

Xt = (cid:0)Rβ((Xt−k)k≥1) + m(1)
β(cid:48)
1

((Xt−k)k≥1) + m(2)
β(cid:48)
2

((Xt−k)k≥1)(cid:1) ξt
+ (cid:0)hα((Xt−k)k≥1) + (cid:96)(1)
α(cid:48)
1

((Xt−k)k≥1) + (cid:96)(2)
α(cid:48)
2

((Xt−k)k≥1)(cid:1).

Then, for α(cid:48)

2 = β(cid:48)

2 = 0, X ∈ AC(cid:0)M (1)

θ1

, f (1)
θ1

(cid:1) and for α(cid:48)

1 = β(cid:48)

1 = 0, X ∈ AC(cid:0)M (2)

θ2

, f (2)
θ2

(cid:1).

(cid:4)

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

16

(a) Time plot of log returns.

(b) Correlograms of log returns.

(c) Time plot of squared log returns.

(d) Correlograms of squared log returns.

Figure 1: Daily closing FTSE 100 index (January 4th, 2010 to December 31 st, 2018).

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

17

In the sequel, some lemmas are stated and theirs proofs are given.

Lemma 1. Let X ∈ AC(Mθ, fθ) (or (cid:102)AC( (cid:101)Hθ)) and Θ ⊆ Θ(r) (or Θ ⊆ (cid:101)Θ(r)) with r ≥ 2. Assume that
the assumptions D(Θ) and K(Θ) (or (cid:101)K(Θ)) hold. Then:

1
κn

(cid:13)(cid:98)Ln(θ) − Ln(θ)(cid:13)
(cid:13)
(cid:13)Θ

a.s.−→
n→+∞

0.

Proof. We have |(cid:98)Ln(θ) − Ln(θ)| ≤ (cid:80)n

t=1 |(cid:98)qt(θ) − qt(θ)|. Then,

1
κn

(cid:13)(cid:98)Ln(θ) − Ln(θ)(cid:13)
(cid:13)

(cid:13)Θ ≤

1
κn

n
(cid:88)

t=1

(cid:107)(cid:98)qt(θ) − qt(θ)(cid:107)Θ.

By Corollary 1 of [27], with r ≤ 3, (7.1) is established when:

(cid:88)

(
k≥1

1
κk

)r/3E(cid:0)(cid:107)(cid:98)qk(θ) − qk(θ)(cid:107)r/3

Θ

(cid:1) < ∞.

(7.1)

(7.2)

With r ≥ 3, and under the assumptions, we ﬁrst recall some results already obtained in [7]: for any
t ∈ Z,

θ(cid:107)r

• E(cid:2)|Xt|r + (cid:107)f t
Θ + (cid:107) (cid:98)f t
θ(cid:107)r
(cid:3) ≤ C
E(cid:2)(cid:107)f t
θ − (cid:98)f t
θ(cid:107)r
Θ
E(cid:2)(cid:107)M t
θ(cid:107)r
θ − (cid:99)M t
Θ
E(cid:2)(cid:107)H t
θ(cid:107)r/2





θ − (cid:98)H t

•

Θ

(cid:3) ≤ C
(cid:3) ≤ C

Θ + (cid:107)M t
(cid:16) (cid:80)

θ(cid:107)r

Θ + (cid:107) (cid:99)M t
θ(cid:107)r
(cid:17)r

j≥t αj(fθ, Θ)

(cid:16) (cid:80)

j≥t αj(Mθ, Θ)

(cid:17)r

Θ + (cid:107)H t

θ(cid:107)r/2

Θ + (cid:107) (cid:98)H t

θ(cid:107)r/2

Θ

(cid:3) < ∞

(7.3)

(7.4)

(cid:16)

min

(cid:110) (cid:80)

j≥t αj(Mθ, Θ) , (cid:80)

j≥t αj(Hθ, Θ)

(cid:111)(cid:17)r/2

.

For any θ ∈ Θ, we have:

|(cid:98)qt(θ) − qt(θ)| =

θ)2

(cid:12)
(cid:12)
(cid:12)

(Xt − (cid:98)f t
(cid:98)H t
θ
θ(Xt − (cid:98)f t
θ − (cid:98)H t

θ (cid:98)H t
θ (cid:98)H t

(cid:12)H t
(cid:12)(H t

θ)−1(cid:12)
≤ (H t
θ)−1(cid:12)
≤ (H t
≤ h−3/2(cid:0)|Xt|2 + 2|Xt(cid:107)f t
≤ h−3/2(cid:0)|Xt|2 + 2|Xt| × (cid:107)f t

θ)2 − (cid:98)H t
θ)(Xt − f t
θ| + |f t

θ)2

θ) −

+ log( (cid:98)H t

(cid:12)
− log(H t
(cid:12)
θ)
(cid:12)

(Xt − f t
H t
θ
θ)(cid:12)
θ)2(cid:12)
(cid:12) + (cid:12)
θ) − log(H t
(cid:12) log( (cid:98)H t
θ(Xt − f t
(cid:12)
(cid:12) + (cid:12)
θ)2(cid:12)
θ(Xt − (cid:98)f t
θ(Xt − f t
θ)2 + H t
(cid:12) log( (cid:98)H t
θ)2 − H t
θ|(cid:1) (cid:12)
θ|2(cid:1) (cid:12)
(cid:12)
(cid:12) + h−1(cid:0)2|Xt| + |f t
θ − (cid:98)f t
(cid:12)f t
θ| + | (cid:98)f t
θ − (cid:99)M t
(cid:12)M t
θ
θ
(cid:1) (cid:107)M t
θ(cid:107)2
θ(cid:107)Θ + (cid:107)f t
θ − (cid:99)M t
θ(cid:107)Θ
Θ
+ h−1(cid:0)2|Xt| + (cid:107)f t
θ(cid:107)Θ + (cid:107) (cid:98)f t

θ − (cid:98)f t

(cid:1) (cid:107)f t

θ(cid:107)Θ

θ)(cid:12)
θ) − log(H t
(cid:12)
(cid:12) + 2 (cid:12)
(cid:12)
(cid:12) log( (cid:99)M t

θ)(cid:12)
θ) − log(M t
(cid:12)

θ(cid:107)Θ + 2 h−1/2(cid:107) (cid:99)M t

θ − M t

θ(cid:107)Θ.

1/ If X ⊂ AC(Mθ, fθ), we deduce

E(cid:2)(cid:107)(cid:98)qt(θ) − qt(θ)(cid:107)r/3

Θ

(cid:3) ≤ C

(cid:16)

(cid:104)(cid:0)(cid:107)Xt + f t
E

θ(cid:107)2

Θ + 1(cid:1)r/3

(cid:105)

θ(cid:107)r/3

Θ

θ − (cid:99)M t
(cid:107)M t
(cid:104)(cid:0)2|Xt| + (cid:107)f t

+ E

θ(cid:107)Θ + (cid:107) (cid:98)f t

θ(cid:107)Θ

(cid:1)r/3

(cid:107)f t

θ − (cid:98)f t

θ(cid:107)r/3

Θ

(cid:105)(cid:17)

.

(7.5)

Then, by Hölder’s inequality and (7.3) we have:

(cid:104)(cid:0)(cid:107)Xt + f t
E

θ(cid:107)2

(cid:107)M t

θ − (cid:99)M t

θ(cid:107)r/3

(cid:105)

Θ + 1(cid:1)r/3
(cid:16)

Θ
(cid:3)(cid:17)2/3 (cid:16)

≤

E(cid:2)(cid:107)Xt + f t

θ + 1(cid:107)r
Θ

E(cid:2)(cid:107)M t

θ − (cid:99)M t

θ(cid:107)r
Θ

(cid:3)(cid:17)1/3

(cid:16)

≤ C

E(cid:2)(cid:107)M t

θ − (cid:99)M t

θ(cid:107)r
Θ

(cid:105)(cid:17)1/3

.

(7.6)

Again with Hölder’s inequality and (7.3) ,

E(cid:2)(cid:0)(2|Xt| + (cid:107)f t

θ(cid:107)Θ + (cid:107) (cid:98)f t

θ(cid:107)Θ)(cid:107)f t

θ − (cid:98)f t

θ(cid:107)Θ

(cid:1)r/3(cid:3) ≤ C (cid:0)E(cid:2)(cid:107)f t

θ − (cid:98)f t

θ(cid:107)r

Θ](cid:1)1/3

.

(7.7)

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

18

Therefore, from (7.6), (7.7) and (7.4), there exists a constant C such that

E(cid:2)(cid:107)((cid:98)qt(θ) − qt(θ)(cid:107)r/3

Θ

(cid:3) ≤ C

(cid:16) (cid:88)

j≥t

αj(fθ, Θ) +

(cid:17)r/3

αj(Mθ, Θ)

.

(cid:88)

j≥t

(7.8)

Hence,

(cid:88)

(
k≥1

1
κk

)r/3E(cid:2)(cid:107)(cid:98)qk(θ) − qk(θ)(cid:107)r/3

Θ

(cid:3) ≤ C

(cid:88)

(
k≥1

1
κk

)r/3(cid:16) (cid:88)

j≥k

αj(fθ, Θ) + αj(Mθ, Θ)

(cid:17)r/3

,

which is ﬁnite by assumption K(Θ), and this achieves the proof.

2/ If X ⊂ (cid:103)AC( (cid:101)Hθ) and using Corollary 1 of [27], with r ≤ 4, (7.1) is established when:

(cid:88)

(
k≥1

1
κk

)r/4E(cid:0)(cid:107)(cid:98)qk(θ) − qk(θ)(cid:107)r/4

Θ

(cid:1) < ∞.

By proceeding as in the previous case, we deduce

|(cid:98)qt(θ) − qt(θ)| ≤ h−2|Xt|2 (cid:107)H t

θ − (cid:98)H t

θ(cid:107)Θ + h−1(cid:107) (cid:98)H t

θ − H t

θ(cid:107)Θ.

In addition, we deduce that there exists a constant C such that

E(cid:2)(cid:107)((cid:98)qt(θ) − qt(θ)(cid:107)r/4

Θ

(cid:3) ≤ C

(cid:17)r/4

αj(Hθ, Θ)

.

(cid:16) (cid:88)

j≥t

(7.9)

(7.10)

(cid:4)

Lemma 2. Let X ∈ AC(Mθ, fθ) (or (cid:102)AC( (cid:101)Hθ)) and Θ ⊆ Θ(r) (or Θ ⊆ (cid:101)Θ(r)) with r ≥ 2. Assume that
the assumptions D(Θ) and K(Θ) (or (cid:101)K(Θ)) hold. Then:

1
κn

(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)Ln(θ)
∂θ

−

∂Ln(θ)
∂θ

(cid:13)
(cid:13)
(cid:13)Θ

a.s.−→
n→+∞

0.

(7.11)

Proof. We will go along similar lines as in the proof of Lemma 1. We have:

1
κn

(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)Ln(θ)
∂θ

−

∂Ln(θ)
∂θ

(cid:13)
(cid:13)
(cid:13)Θ

≤

1
κn

n
(cid:88)

t=1

(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)qt(θ)
∂θi

−

∂qt(θ)
∂θi

(cid:13)
(cid:13)
(cid:13)Θ

.

Using again Corollary 1 of [27], it is suﬃcient to prove for r ≤ 3 that

(

1
κk

(cid:88)

k≥1

)r/3 E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)qt(θ)
∂θi

−

∂qt(θ)
∂θi

(cid:13)
(cid:13)
(cid:13)

Θ

r/3

(cid:105)

< ∞.

(7.12)

For any θ ∈ Θ, with Hθ = M 2

θ , the ﬁrst partial derivatives of qt(θ) are

∂qt(θ)
∂θi

=

−2(Xt − f t
θ)
H t
θ

∂f t
θ
∂θi

−

θ)2

(Xt − f t
(H t
θ)2

∂H t
θ
∂θi

+

1
H t
θ

∂H t
θ
∂θi

for i = 1, · · · , d. Hence,

= −2(H t

θ)−1(Xt − f t
θ)

∂f t
θ
∂θi

+ (Xt − f t

θ)−1
θ)2 ∂(H t
∂θi

+ (H t

θ)−1 ∂H t
θ
∂θi

,

(cid:12)
(cid:12)
(cid:12)

∂ (cid:98)qt(θ)
∂θi

−

∂qt(θ)
∂θi

(cid:12)
(cid:12)
(cid:12)(ht
(cid:12)
(cid:12)
(cid:12) ≤ 2
(cid:12)
(cid:12)(Xt − (cid:98)f t
(cid:12)

∂f t
θ)−1(Xt − f t
θ
θ)
∂θi
θ)−1
θ)2 ∂( (cid:98)H t
∂θi

+

− ((cid:98)ht

(cid:12)
(cid:12)
(cid:12)

∂ (cid:98)f t
θ)−1(Xt − (cid:98)f t
θ
θ)
∂θi
θ)−1
θ)2 ∂(H t
(cid:12)
(cid:12)
(cid:12) +
∂θi

− (Xt − f t

(cid:12)
(cid:12)( (cid:98)H t
(cid:12)

θ)−1 ∂ (cid:98)H t
θ
∂θi

− (H t

θ)−1 ∂H t
θ
∂θi

(cid:12)
(cid:12)
(cid:12).

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

19

Then, using |a1b1c1−a2b2c2| ≤ |a1−a2| |b2| |c2|+|a1| |b1−b2| |c2|+|a1| |b1| |c1−c2| for any a1, a2, b1, b2, c1, c2
in R, we obtain

(cid:12)
(cid:12)
(cid:12)

∂ (cid:98)qt(θ)
∂θi

−

∂qt(θ)
∂θi

(cid:12)
(cid:12)
(cid:12) ≤ 2

(cid:16)(cid:12)
(cid:12)(H t

θ)−1 − ( (cid:98)H t

θ)−1(cid:12)

(cid:12) × (cid:12)

(cid:12)Xt − (cid:98)f t
θ

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

+ (cid:12)

(cid:12)(H t

θ)−1(cid:12)

(cid:12) × (cid:12)

(cid:12)Xt − f t
θ

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

∂f t
θ
∂θi

−

(cid:17)

∂ (cid:98)f t
θ
∂θi

(cid:12)
(cid:12)
(cid:12)

∂ (cid:98)f t
θ
∂θi
+ (cid:12)

(cid:12)Xt − (cid:98)f t
θ

+ 2

(cid:12)
(cid:12)
(cid:12)

∂(H t
θ)−1
∂θi

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)Xt

(cid:12)
(cid:12)

(cid:12)
(cid:12)f t

θ − (cid:98)f t
θ

Thus,

(cid:12) + (cid:12)
(cid:12)

(cid:12)( (cid:98)H t

θ)−1(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

∂ (cid:98)H t
θ
∂θi

−

(cid:12)
(cid:12) + (cid:12)
(cid:12)

(cid:12)(H t

θ)−1(cid:12)

(cid:12) × (cid:12)

(cid:12) (cid:98)f t

θ − f t
θ

(cid:12)
(cid:12)

2 (cid:12)
(cid:12)
(cid:12)

−

θ)−1
∂( (cid:98)H t
∂θi
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) +

∂H t
θ
∂θi

(cid:12)
(cid:12)
(cid:12)

∂H t
θ
∂θi

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

∂ (cid:98)f t
(cid:12)
θ
(cid:12)
(cid:12)
∂θi
θ)−1
∂(H t
∂θi

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)( (cid:98)H t

θ)−1 − (H t

θ)−1(cid:12)
(cid:12).

(cid:13)
(cid:13)
(cid:13)

−

∂ (cid:98)qt(θ)
∂θi
+ 2 (cid:13)

∂qt(θ)
∂θi

(cid:13)
(cid:13)
(cid:13)Θ

≤ 2 h−1(cid:16)(cid:13)

(cid:13) (cid:98)f t

θ − f t
θ

(cid:13)
(cid:13)Θ

(cid:13)
(cid:13)
(cid:13)

(cid:13)(H t

θ)−1 − ( (cid:98)H t

θ)−1(cid:13)
(cid:13)Θ

(cid:13)
(cid:13)Xt − (cid:98)f t
θ

(cid:13)
(cid:13)Θ

(cid:13)
(cid:13)
(cid:13)

+ 2 (cid:12)

(cid:12)Xt

(cid:12)
(cid:12)

(cid:13)
(cid:13)f t

θ − (cid:98)f t
θ

(cid:13)
(cid:13)Θ

(cid:13)
(cid:13)
(cid:13)

∂(H t
θ)−1
∂θi

(cid:13)
(cid:13)
(cid:13)Θ

+ (cid:13)

(cid:13)( (cid:98)H t

∂ (cid:98)f t
(cid:13)
θ
(cid:13)
(cid:13)Θ
∂θi
∂ (cid:98)f t
(cid:13)
θ
(cid:13)
(cid:13)Θ
∂θi
θ)−1(cid:13)
(cid:13)Θ

+ (cid:13)

(cid:13)Xt − f t
θ

(cid:13)
(cid:13)Θ

(cid:13)
(cid:13)
(cid:13)

+ (cid:13)

(cid:13)Xt − (cid:98)f t
θ

(cid:13)
(cid:13)

2(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)H t
θ
∂θi

−

∂H t
θ
∂θi

−

∂f t
θ
∂θi
∂( (cid:98)H t
θ)−1
∂θi
+ (cid:13)

(cid:13)
(cid:13)
(cid:13)Θ

∂ (cid:98)f t
θ
∂θi

−

(cid:17)

(cid:13)
(cid:13)
(cid:13)Θ
∂(H t
θ)−1
∂θi

(cid:13)
(cid:13)
(cid:13)

(cid:13)( (cid:98)H t

θ)−1 − (H t

θ)−1(cid:107)Θ

(cid:13)
(cid:13)
(cid:13)

∂H t
θ
∂θi

(cid:13)
(cid:13)
(cid:13)Θ

.

Using again the results of [7], we know that:

• E

∂f t
(cid:104)(cid:13)
θ
(cid:13)
(cid:13)
∂θi
E(cid:2)(cid:13)

(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)f t
(cid:13)
(cid:13)
r
r
θ
(cid:13)
(cid:13)
+
+
(cid:13)
(cid:13)
∂θi
Θ
Θ
θ)−1 − ( (cid:98)H t

∂M t
(cid:13)
θ
(cid:13)
(cid:13)
∂θi
θ)−1(cid:13)
r
(cid:13)
Θ

∂ (cid:99)M t
(cid:13)
(cid:13)
r
θ
(cid:13)
(cid:13)
+
(cid:13)
(cid:13)
∂θi
Θ
(cid:16) (cid:88)
(cid:3) ≤ C

(cid:13)(H t

(cid:13)
r
(cid:13)
+
(cid:13)
Θ

(cid:13)
(cid:13)
(cid:13)

∂H t
θ
∂θi

αj(Mθ, Θ)

r/2
+

(cid:13)
(cid:13)
(cid:13)
Θ
(cid:17)r

(cid:13)
(cid:13)
(cid:13)

θ)−1
∂(H t
∂θi

(cid:13)
r
(cid:13)
(cid:13)

Θ

(cid:105)

< ∞

(7.13)






•

(cid:13)
r
(cid:13)
(cid:13)

Θ

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)f t
θ
∂θi
∂ (cid:98)H t
θ
∂θi

−

∂f t
θ
∂θi
∂H t
θ
∂θi
∂(H t
θ)−1
∂θi

−

r/2

(cid:13)
(cid:13)
(cid:13)

Θ

j≥t

(cid:105)

≤ C

(cid:16) (cid:88)

αj(∂fθ, Θ)

(cid:17)r

(cid:105)

j≥t
(cid:16) (cid:88)

≤ C

(cid:0)αj(Mθ, Θ) + αj(∂Mθ, Θ)(cid:1)(cid:17)r/2

(7.14)

−

∂( (cid:98)H t
θ)−1
∂θi

(cid:13)
(cid:13)
(cid:13)

Θ

r/2

j≥t
(cid:105)

≤ C

(cid:16) (cid:88)

(cid:0)αj(Mθ, Θ) + αj(∂Mθ, Θ)(cid:1)(cid:17)r/2

j≥t

1. If X ⊂ AC(Mθ, fθ), we deduce from the Hölder’s Inequality that,

−

∂qt(θ)
∂θi

(cid:13)
(cid:13)
(cid:13)

r/3

(cid:105)

≤ C

(cid:104)(cid:0)E(cid:2)(cid:13)

(cid:13) (cid:98)f t

θ − f t
θ

(cid:3)(cid:1)1/3(cid:16)

(cid:13)
r
(cid:13)
Θ

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)f t
θ
∂θi

(cid:13)
(cid:13)
(cid:13)

Θ

r/2

(cid:105)(cid:17)2/3

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)qt(θ)
∂θi
+ (cid:0)E(cid:2)(cid:13)

Θ
(cid:3)(cid:1)1/2(cid:16)

(cid:13)Xt − f t
θ

(cid:13)
(cid:13)

2r/3
Θ

(cid:105)(cid:17)1/3

−

∂ (cid:98)f t
θ
∂θi

(cid:13)
r
(cid:13)
(cid:13)

Θ

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂f t
θ
∂θi
(cid:3)(cid:1)1/3 (cid:16)

+ (cid:0)E(cid:2)(cid:13)

(cid:13)(H t

θ)−1 − ( (cid:98)H t

θ)−1(cid:13)
r
(cid:13)
Θ

E(cid:2)(cid:13)

(cid:13)Xt − (cid:98)f t
θ

(cid:13)
r
(cid:13)
Θ

(cid:3) E

+ (cid:0)E(cid:2)(cid:13)

+

(cid:16)

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

(cid:13)Xt − (cid:98)f t
θ

(cid:13)
r
(cid:13)
Θ
∂(H t
θ)−1
∂θi

(cid:13)
r
(cid:13)
(cid:13)

(cid:3)(cid:17)1/3(cid:16)

E

(cid:105)(cid:17)1/3(cid:16)

−

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂( (cid:98)H t
θ)−1
∂θi
E(cid:2)|Xt|r(cid:3) E(cid:2)(cid:13)

(cid:13)f t

∂(H t
θ)−1
∂θi
(cid:13)
r
(cid:13)
Θ

θ − (cid:98)f t
θ

(cid:13)
(cid:13)
(cid:13)
(cid:3)(cid:17)1/3

(cid:105)(cid:17)1/3

(cid:13)
r
(cid:13)
(cid:13)

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)f t
θ
∂θi
r/2(cid:105)(cid:17)2/3

Θ

+

(cid:16)

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)H t
θ
∂θi

−

r/3

(cid:13)
(cid:13)
(cid:13)

Θ

(cid:105)

+

(cid:16)

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂H t
θ
∂θi

(cid:13)
(cid:13)
(cid:13)

Θ

r/2

(cid:105)(cid:17)2/3(cid:0)E(cid:2)(cid:13)

(cid:13)( (cid:98)H t

θ)−1 − (H t

θ)−1(cid:13)
r
(cid:13)
Θ

(cid:3)(cid:17)1/3(cid:105)

.

Θ
∂H t
θ
∂θi

Using (7.13) and (7.14), we deduce

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)qt(θ)
∂θi

−

∂qt(θ)
∂θi

(cid:13)
(cid:13)
(cid:13)

Θ

r/3

(cid:105)

≤ C

(cid:16) (cid:88)

j≥t

αj(fθ, Θ) + αj(Mθ, Θ) + αj(∂fθ, Θ) + αj(∂Mθ, Θ)

(cid:17)r/3

.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

20

Therefore,

(cid:88)

k≥1

1
κr/3
k

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)qk(θ)
∂θi

−

∂qk(θ)
∂θi

r/3

(cid:105)

(cid:13)
(cid:13)
(cid:13)

Θ

≤ C

(cid:88)

k≥1

1
κr/3
k

(cid:16) (cid:88)

j≥t

αj(fθ, Θ) + αj(Mθ, Θ) + αj(∂fθ, Θ) + αj(∂Mθ, Θ)

(cid:17)r/3

.

We conclude the proof of (7.12) from assumption K(Θ).

2. If X ⊂ (cid:103)AC( (cid:101)Hθ), we deduce

(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)qt(θ)
∂θi

−

∂qt(θ)
∂θi

(cid:13)
(cid:13)
(cid:13)Θ

≤ (cid:12)

(cid:12)Xt

(cid:12)
(cid:12)

2 (cid:13)
(cid:13)
(cid:13)

∂( (cid:98)H t
θ)−1
∂θi

−

∂(H t
θ)−1
∂θi
+ h−1(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)Θ
∂ (cid:98)H t
θ
∂θi

−

∂H t
θ
∂θi

(cid:13)
(cid:13)
(cid:13)Θ

+ (cid:13)

(cid:13)( (cid:98)H t

θ)−1 − (H t

θ)−1(cid:107)Θ

(cid:13)
(cid:13)
(cid:13)

∂H t
θ
∂θi

(cid:13)
(cid:13)
(cid:13)Θ

.

As a consequence,

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)qt(θ)
∂θi

−

∂qt(θ)
∂θi

r/4

(cid:13)
(cid:13)
(cid:13)

Θ

(cid:105)

(cid:16)

E(cid:2)(cid:12)

(cid:12)Xt

(cid:12)
(cid:12)

≤

r E

(cid:104)(cid:13)
(cid:13)
(cid:13)

+ h−r/4E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)H t
θ
∂θi

−

∂H t
θ
∂θi

(cid:13)
(cid:13)
(cid:13)

Θ

∂( (cid:98)H t
θ)−1
∂θi
(cid:105)
r/4

+

−

∂(H t
θ)−1
∂θi

(cid:13)
(cid:13)
(cid:13)

Θ

r/2

(cid:105)(cid:17)1/2

(cid:16)

E(cid:2)(cid:13)

(cid:13)( (cid:98)H t

θ)−1 − (H t

θ)−1(cid:107)r/2

Θ

(cid:3) E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂H t
θ
∂θi

(cid:13)
(cid:13)
(cid:13)

Θ

r/2

implying

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

∂ (cid:98)qt(θ)
∂θi

−

∂qt(θ)
∂θi

r/4

(cid:13)
(cid:13)
(cid:13)

Θ

(cid:105)

≤ C

(cid:16) (cid:88)

j≥t

αj(Hθ, Θ) + αj(∂Hθ, Θ)

(cid:17)r/4

,

which achieves the proof, according to Corollary 1 of [27].

Lemma 3. Under the assumptions of Theorem 3.1 and if a model m ∈ M is such that θ∗ ∈ Θ(m),
then:

1
κn

(cid:12)(cid:98)Ln((cid:98)θ(m)) − (cid:98)Ln(θ∗)(cid:12)
(cid:12)

(cid:12) = oP (1).

Proof. We have:

1
κn

(cid:12)(cid:98)Ln((cid:98)θ(m)) − (cid:98)Ln(θ∗)(cid:12)
(cid:12)

(cid:12) =

≤

1
κn
2
κn

(cid:12)(cid:98)Ln((cid:98)θ(m)) − Ln((cid:98)θ(m)) + Ln((cid:98)θ(m)) − Ln(θ∗) + Ln(θ∗) − (cid:98)Ln(θ∗)(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)(cid:98)Ln(θ) − Ln(θ)(cid:13)

(cid:13)Θ(r) +

1
κn

(cid:12)
(cid:12)Ln((cid:98)θ(m)) − Ln(θ∗)(cid:12)
(cid:12).

According to Lemma 1, 1
κn

(cid:13)
(cid:13)(cid:98)Ln(θ) − Ln(θ)(cid:13)

(cid:13)Θ(r)

a.s.−→
n→+∞

0. The proof will be achieved if we prove

1
κn

(cid:12)
(cid:12)Ln((cid:98)θ(m)) − Ln(θ∗)(cid:12)

(cid:12) = oP (1).

(7.16)

Applying a second order Taylor expansion of Ln around (cid:98)θ(m) for n suﬃciently large such that θ(m) ∈
Θ(m) which are between (cid:98)θ(m) and θ∗, yields:

1
κn

(cid:0)Ln((cid:98)θ(m)) − Ln(θ∗)(cid:1) =

(cid:0)

1
κn

(cid:98)θ(m) − θ∗(cid:1) ∂Ln((cid:98)θ(m))

∂θ

+

(cid:0)

1
2κn

(cid:98)θ(m) − θ∗(cid:1)(cid:48) ∂2Ln(θ(m))

∂θ2

(cid:0)

(cid:98)θ(m) − θ∗(cid:1).

(7.17)

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

(cid:105)(cid:17)1/2

,

(cid:4)

(7.15)

•

n (cid:0)

• −

2
n

(cid:98)θ(m) − θ∗(cid:1)
(cid:16) ∂2Ln(θ(m))
∂θi∂θj

L−→
n→+∞
(cid:17)

i,j∈m

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

21

Let us deal ﬁrst with the ﬁrst term on the right hand side of last equality:

(cid:98)θ(m) − θ∗(cid:1) ∂Ln((cid:98)θ(m))

(cid:0)

1
κn

√

=

n(cid:0)

1
κn

(cid:98)θ(m) − θ∗(cid:1) 1
√
n
(cid:98)θ(m) − θ∗(cid:1) = OP (1) and 1√

∂θ
√

n(cid:0)

∂Ln((cid:98)θ(m))
∂θ

.

Since 1
κn
that:

= o(1) and from [7] we have

(cid:0)

1
κn

(cid:98)θ(m) − θ∗(cid:1) ∂Ln((cid:98)θ(m))

∂θ

= oP (1).

n

∂Ln((cid:98)θ(m))
∂θ

= oP (1), it follows

(7.18)

On the other hand, for the second term of the right hand side of equality (7.17), let us note that, we
have from [7]:
√

Aθ∗,m a Gaussian random variable from (3.3).

insuring that the matrix F (θ∗, m) exists and is deﬁnite positive (see [7]).

a.s.−→
n→+∞

F (θ∗, m) since (cid:98)θ(m)

a.s.−→
n→+∞

θ∗ and using the assumption Var(Θ)

Hence,

(cid:0)

(cid:17)

(cid:98)θ(m) − θ∗(cid:1)(cid:48) (cid:16) ∂2Ln(θ(m))
√
−1
2

∂θi∂θj

=

((cid:98)θ(m) − θ∗)

i,j∈m

n(cid:0)

(cid:98)θ(m) − θ∗(cid:1)(cid:48) (cid:0)F (θ∗, m) + oP (1)(cid:1) √

n(cid:0)

(cid:98)θ(m) − θ∗(cid:1)
−1
2

A(cid:48)

P−→
n→∞

θ∗,m F (θ∗, m) Aθ∗,m.

We deduce that

(cid:0)

(cid:98)θ(m) − θ∗(cid:1)(cid:48) (cid:16) ∂2Ln(θ(m))

∂θi∂θj

(cid:17)

i,j∈m

((cid:98)θ(m) − θ∗) = OP (1)

=⇒

(cid:0)

1
κn

(cid:98)θ(m) − θ∗(cid:1)(cid:48) (cid:16) ∂2Ln(θ(m))

∂θi∂θj

(cid:17)

i,j∈m

((cid:98)θ(m) − θ∗) = oP (1).

(7.19)

Thus, (7.16) follows from (7.17), (7.18) and (7.19); which completes the proof of Lemma 3.

(cid:4)

7.1. Misspeciﬁed model

When a model m is misspeciﬁed, we will show that P( (cid:98)m = m∗) −→
proof than in [42]. Before dealing with this proof, we state some useful results.

n→∞

0 following the same scheme of

Proposition 2. Let X ∈ AC(Mθ, fθ) (or (cid:102)AC( (cid:101)Hθ)) and Θ ⊆ Θ(r) (or Θ ⊆ (cid:101)Θ(r)) with r ≥ 2. Then,
when the assumption D(Θ) holds,

(cid:13)
(cid:13)
(cid:13)Θ
Proof. See the proof of Theorem 1 in [7].

Ln(θ) − L(θ)

1
n

(cid:13)
(cid:13)
(cid:13)

a.s.−→
n→+∞

0 with L(θ) := −

1
2

E[q0(θ)].

(7.20)

(cid:4)

Lemma 4. Under the assumptions of Theorem 3.1 and for m ∈ M such as m∗ ⊂ m, then:

Ln((cid:98)θ(m)) − Ln(θ∗) = OP (1).
Proof. Applying a second order Taylor expansion of Ln around (cid:98)θ(m∗) for n suﬃciently large such that
θ(m) ∈ Θ(m) which are between θ∗ and (cid:98)θ(m∗), yields:

(7.21)

Ln((cid:98)θ(m)) − Ln(θ∗) = ((cid:98)θ(m) − θ∗)

√

=

n((cid:98)θ(m) − θ∗)

1
√
n

=
= OP (1),

op(1)

∂Ln((cid:98)θ(m))
∂θ
∂Ln((cid:98)θ(m))
∂θ

+

1
2

+

√

∂θ∂θ(cid:48)

((cid:98)θ(m) − θ∗)(cid:48) ∂2Ln(θ(m))
1
2
∂2Ln(θ(m))
n((cid:98)θ(m) − θ∗)(cid:48) 1
∂θ∂θ(cid:48)
n
OP (1)

+

((cid:98)θ(m) − θ∗)

√

n((cid:98)θ(m) − θ∗)

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

by using equality (7.19).

7.2. Proof of Theorem 3.1

As we point out in Subsection 2.4, the proof is divided into two parts.

22

(cid:4)

Proof. 1. For m ∈ M such as m∗ ⊂ m and m (cid:54)= m∗ (overﬁtting), then using with (cid:98)C(m) =
−2(cid:98)Ln

(cid:0)

(cid:98)θ(m)(cid:1) + |m| κn (see (2.8)), we have:
P(cid:0)
P( (cid:98)m = m)
(cid:98)C(m) ≤ −2(cid:98)Ln
(cid:16)
− 2(cid:0)

≤

≤

P

(cid:1)

(cid:0)θ∗(cid:1) + |m∗| κn
(cid:98)Ln((cid:98)θ) − (cid:98)Ln(θ∗)(cid:1) ≤ κn(|m∗| − |m|)
(cid:17)

(cid:98)Ln(θ∗) − (cid:98)Ln((cid:98)θ)(cid:1) ≤

(|m∗| − |m|)
2

(cid:17)

(cid:0)

(cid:16) 1
κn

P

0

≤

−→
n→∞

by virtue of Lemma 3 and because |m| − |m∗| ≥ 1.

2. Let m ∈ M such as m∗ (cid:54)⊂ m. Then,
(cid:98)Ln((cid:98)θ(m∗)) − (cid:98)Ln((cid:98)θ(m)) = (cid:0)

(cid:98)Ln((cid:98)θ(m∗)) − Ln((cid:98)θ(m∗))(cid:1) − (cid:0)

(cid:98)Ln((cid:98)θ(m)) − Ln((cid:98)θ(m))(cid:1)

It follows from Lemma 1 that the ﬁrst and the second term of the right part of (7.22) are equal to
oP (κn). Moreover, the third term can be written as follows:

Ln((cid:98)θ(m∗)) − Ln((cid:98)θ(m)) = (cid:0)Ln((cid:98)θ(m∗)) − Ln(θ∗)(cid:1) + (cid:0)Ln(θ∗) − Ln((cid:98)θ(m))(cid:1).

+ (cid:0)Ln((cid:98)θ(m∗)) − Ln((cid:98)θ(m))(cid:1).

(7.22)

From Lemma 4, one deduces Ln((cid:98)θ(m∗)) − Ln(θ∗) = OP (1). In addition, in the sequel, we are going to
show that

Ln(θ∗) − Ln((cid:98)θ(m)) = n (cid:0)A(m) + oP (1)(cid:1), with A(m) > 0.

For any θ ∈ Θ(m), we have from Proposition 2

Ln(θ∗) − Ln(θ) = (cid:0)Ln(θ∗) − n L(θ∗)(cid:1) − (cid:0)Ln(θ) − n L(θ)(cid:1) + n (cid:0)L(θ∗)) − L(θ))(cid:1)

= oP (n) + n (cid:0)L(θ∗)) − L(θ)(cid:1).
Let us denote by Ft := σ(cid:0)Xt−1, Xt−2, · · · (cid:1). Using conditional expectation, we obtain

L(θ∗) − L(θ) = −

(cid:104)
E(cid:2)q0(θ) − q0(θ∗) | F0
E

(cid:3)(cid:105)
.

1
2

(7.23)

(7.24)

But,

E(cid:2)q0(θ) − q0(θ∗) | F0

(cid:3) = E

(cid:104) (X0 − f 0

θ )2

+ log(H 0

θ ) −

− log(H 0

θ∗ ) | F0

(cid:105)

θ∗ )2

(X0 − f 0
H 0
θ∗
(cid:3)

θ )2 | F0

E(cid:2)(X0 − f 0
H 0
θ
E(cid:2)(X0 − f 0

−

E(cid:2)(X0 − f 0
H 0
θ∗
(cid:3)
θ )2 | F0

θ∗ − f 0

θ∗ )2 | F0

(cid:3)

H 0
θ
(cid:17)

+

(cid:17)

− 1 +

= log

= log

(cid:16) H 0
θ
H 0
θ∗
(cid:16) H 0
θ
H 0
θ∗

θ∗ + f 0
H 0
θ
θ )2
θ∗ − f 0
H 0
θ

(f 0

=

H 0
θ∗
H 0
θ

− log

(cid:17)

(cid:16) H 0
θ∗
H 0
θ

− 1 +

As a consequence, from (7.24),

A(m)

= E

:= 2 (cid:0)L(θ∗) − L(θ)(cid:1)
(cid:16) H 0
θ∗
H 0
θ
(cid:104) H 0
(cid:16)
θ∗
H 0
θ

(cid:104) H 0
θ∗
H 0
θ
(cid:104) H 0
θ∗
H 0
θ

≥ E

− log

− log

E

(cid:17)

(cid:105)

− 1 +

(cid:105)

(f 0

θ )2

θ∗ − f 0
H 0
θ
(cid:104) (f 0

(cid:105)(cid:17)

− 1 + E

θ )2

θ∗ − f 0
H 0
θ

(cid:105)

by Jensen Inequality.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

23

Since x − log(x) − 1 > 0 for any x > 0, x (cid:54)= 1 and x − log(x) − 1 = 0 for x = 1, we deduce that

• If f 0
θ∗ (cid:54)= f 0
• Otherwise, if f 0

θ then E

θ∗ = f 0

θ )2

θ∗ −f 0
H 0
θ
θ , then

(cid:104) (f 0

(cid:105)

> 0 and A(m) > 0.

A(m) = E

(cid:104) H 0
θ∗
H 0
θ

− log

(cid:16) H 0
θ∗
H 0
θ

(cid:17)

(cid:105)
,

− 1

From Assumption ID(Θ), when θ∗ /∈ Θ(m) and if f 0
that H 0
θ∗
H 0
θ

(cid:54)= 1. Then A(m) > 0.

θ∗ = f 0

θ , we necessarily have H 0

θ∗ (cid:54)= H 0

θ so

Therefore A(m) > 0 for any θ ∈ Θ(m) and particularly for θ = (cid:98)θ(m) and (7.23) holds. Thus, (7.22)
yields to

(cid:98)Ln((cid:98)θ(m∗)) − (cid:98)Ln((cid:98)θ(m)) = oP (κn) + OP (1) + n A(m) + oP (n) = OP (1) + n A(m) + oP (n).

Finally, when m ∈ M such as m∗ (cid:54)⊂ m, we have

(cid:98)C(m) − (cid:98)C(m∗) = 2 n A(m) + oP (n) + OP (1) + κn(|m| − |m∗|)

P−→
n→∞

+ ∞

since κn = o(n), therefore P(cid:0)

(cid:98)C(m) > (cid:98)C(m∗)(cid:1) −→

1.

n→∞
Thus we have proved the ﬁrst and most diﬃcult part of Theorem (3.1). The next lines show the second
part which is about the consistency of (cid:98)θ( (cid:98)m).

Given (cid:15) > 0, we have :

(cid:16)

P

(cid:17)
(cid:107)(cid:98)θ( (cid:98)m) − θ∗(cid:107)i∈m∗ > (cid:15)

(cid:16)

= P

(cid:107)(cid:98)θ( (cid:98)m) − θ∗(cid:107)i∈m∗ > (cid:15)| (cid:98)m = m∗(cid:17)

P(cid:0)

(cid:98)m = m∗(cid:1)
(cid:107)(cid:98)θ( (cid:98)m) − θ∗(cid:107)i∈m∗ > (cid:15)| (cid:98)m (cid:54)= m∗(cid:17)

(cid:16)

+P

P(cid:0)

(cid:98)m (cid:54)= m∗(cid:1).

From the strong consistency of the QMLE (see New version of Theorem 1 of [7]), the ﬁrst term of
the right hand side of the above equation is asymptotically zero and also the second one under the
assumptions of the ﬁrst part of Theorem 3.1 which gives P(cid:0)

0.

(cid:98)m (cid:54)= m∗(cid:1) −→

n→∞

(cid:4)

7.3. Proof of Theorem 3.2

Proof. For x = (xi)1≤i≤d ∈ Rd, denote Fn(x) = P

(cid:16) (cid:92)

√

n (cid:0)

(cid:98)θ( (cid:98)m) − θ∗(cid:1)

i ≤ xi

(cid:17)

.

1≤i≤d

First, we have:

Fn(x) = P

(cid:16) (cid:92)

√

n (cid:0)

(cid:98)θ( (cid:98)m) − θ∗(cid:1)

i ≤ xi

(cid:12)

(cid:12) (cid:98)m = m∗(cid:17)

P(cid:0)

(cid:98)m = m∗(cid:1)

1≤i≤d

(cid:16) (cid:92)

+P

√

n (cid:0)

(cid:98)θ( (cid:98)m) − θ∗(cid:1)

i ≤ xi

(cid:12)

(cid:12) (cid:98)m (cid:54)= m∗(cid:17)

P(cid:0)

(cid:98)m (cid:54)= m∗(cid:1).

1≤i≤d

(cid:98)m = m∗(cid:1) −→
Under the assumptions of Theorem 3.1, P(cid:0)
second term in the right side of the previous equality asymptotically vanishes. For the ﬁrst term, we
can write,

(cid:98)m (cid:54)= m∗(cid:1) −→

0. Therefore the

1 and P(cid:0)

n→∞

n→∞

(cid:16) (cid:92)

P

√

n (cid:0)

(cid:98)θ( (cid:98)m) − θ∗(cid:1)

i ≤ xi

(cid:12)

(cid:12) (cid:98)m = m∗(cid:17)

1≤i≤d

(cid:16)(cid:110) (cid:92)

= P

√

n (cid:0)

(cid:98)θ(m∗) − θ∗(cid:1)

i ≤ xi

(cid:111) (cid:92) (cid:110) (cid:92)

√

n (cid:0)

(cid:98)θ(m∗) − θ∗(cid:1)

i ≤ xi

(cid:111)(cid:17)

.

i∈m∗

i /∈m∗

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

24

Since θ(m∗) ∈ Θ(m∗), (cid:0)(cid:0)
numbers we have:

(cid:98)θ(m∗)(cid:1)

(cid:1)

i /∈m∗ = (cid:0)θ∗

i

i

(cid:1)

i /∈m∗ = 0, for (xi)i /∈m∗ a family of non negative real

(cid:16)(cid:110) (cid:92)

P

√

i∈m∗

n (cid:0)

(cid:98)θ(m∗) − θ∗(cid:1)

i ≤ xi

(cid:111) (cid:92) (cid:110) (cid:92)

√

n (cid:0)

(cid:98)θ(m∗) − θ∗(cid:1)

i ≤ xi

(cid:111)(cid:17)

(cid:16) (cid:92)

= P

√

i∈m∗

i /∈m∗

n (cid:0)

(cid:98)θ(m∗) − θ∗(cid:1)

i ≤ xi

(cid:17)

(cid:16)(cid:0)F (θ∗, m∗)−1G(θ∗, m∗)F (θ∗, m∗)−1(cid:1)−1/2
P

Z ≤ (xi)i∈m∗

(cid:17)

,

−→
n→∞

with Z a standard Gaussian random vector in R|m∗| from the central limit theorem (3.3), and this
(cid:4)
achieves the proof of 3.5 of Theorem 3.2.

7.4. Proof of Theorem 5.1

Consider the following notation: for θ ∈ Θ and m ∈ M, denote the residuals and quasi-residuals by:

(cid:40)

et(θ)
et(m)

:= (cid:0)M t
:= (cid:0)M t

θ

(cid:1)−1(cid:0)Xt − f t

θ

(cid:1)

(cid:1)−1(cid:0)Xt − f t

(cid:98)θ(m)

(cid:98)θ(m)

(cid:1)

and (cid:98)et(θ)
and (cid:98)et(m)

:= (cid:0)
(cid:99)M t
θ
:= (cid:0)M t

(cid:1)−1(cid:0)Xt − (cid:98)f t

θ

(cid:1)

(cid:1)−1(cid:0)Xt − (cid:98)f t

(cid:98)θ(m)

(cid:98)θ(m)

(cid:1) .

For k ∈ {0, 1, . . . , n − 1}, θ ∈ Θ and m ∈ M, deﬁne also the adjusted lag-k covariograms and correlo-
grams of the squared (standardized) residual by:






γk(θ) :=

γk(m) :=

1
n

1
n

n−k
(cid:88)

(cid:0)e2

t (θ) − 1(cid:1)(cid:0)e2

t+k(θ) − 1(cid:1)

and (cid:98)γk(θ) :=

t=1
n−k
(cid:88)

(cid:0)e2

t (m) − 1(cid:1)(cid:0)e2

t+k(m) − 1(cid:1) and (cid:98)γk(m) :=

t=1

1
n

1
n

n−k
(cid:88)

(cid:0)

t (θ) − 1(cid:1)(cid:0)
(cid:98)e2

t+k(θ) − 1(cid:1)
(cid:98)e2

t=1
n−k
(cid:88)

(cid:0)

t=1

t (m) − 1(cid:1)(cid:0)
(cid:98)e2

t+k(m) − 1(cid:1)
(cid:98)e2

and ρk(θ) :=

γk(θ)
γ0(θ)

, (cid:98)ρk(θ) := (cid:98)γk(θ)
(cid:98)γ0(θ)

, ρk(m) :=

γk(m)
γ0(m)

and (cid:98)ρk(m) := (cid:98)γk(m)
(cid:98)γ0(m)

.

(cid:98)ρ(θ) := (cid:0)

Finally, for K a positive integer, denote the vector of adjusted correlogram:
(cid:98)ρ1(θ), . . . , (cid:98)ρK(θ)(cid:1)(cid:48)

and (cid:98)ρ(m) := (cid:0)
Proof.
(1) This proof is divided into two parts. In (i) we prove a result that ensures that the
asymptotic distributions of the vectors (cid:98)ρ(θ) and ρ(θ) are the same. In (ii) we show that the large
nρ(m∗) is normal with a covariance matrix V (θ∗, m∗) . Those two conditions
sample distribution of
do lead well to the asymptotic normality (5.1).

(cid:98)ρ1(m), . . . , (cid:98)ρK(m)(cid:1)(cid:48)

√

.

(i) In this part, we ﬁrst show that for any k ∈ N,
n (cid:13)
(cid:13)(cid:98)γk(θ) − γk(θ)(cid:13)
(cid:13)Θ

√

a.s.−→
n→∞

0.

(7.25)

We have:

√

n(cid:0)

(cid:98)γk(θ) − γk(θ)(cid:1) =

=

1
√
n

1
√
n

t=k+1
n
(cid:88)

(cid:0)

t=k+1

n
(cid:88)

(cid:0)

t (θ) − 1(cid:1)(cid:0)
(cid:98)e2

t−k(θ) − 1(cid:1) −
(cid:98)e2

1
√
n

n
(cid:88)

t=k+1

(cid:0)e2

t (θ) − 1(cid:1)(cid:0)e2

t−k(θ) − 1(cid:1)

t (θ)(cid:98)e2
(cid:98)e2

t−k(θ) − e2

t (θ)e2

t−k(θ)(cid:1) +

1
√
n

=:

I1 + I2 + I3.

+

1
√
n

n
(cid:88)

(cid:0)

t (θ) − e2
(cid:98)e2

t (θ)(cid:1)

t=k+1
n
(cid:88)

(cid:0)e2

t−k(θ) − (cid:98)e2

t−k(θ)(cid:1)

t=k+1

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

25

Now, we show that (cid:107)I1(cid:107)Θ

a.s.−→
n→+∞

0. We can rewrite I1 as follows

I1 =

=

1
√
n

1
√
n

n
(cid:88)

t=k+1
n
(cid:88)

t=k+1

t−k(θ)(cid:0)
(cid:98)e2

t (θ) − e2
(cid:98)e2

t (θ)(cid:1) +

1
√
n

n
(cid:88)

t=k+1

t (θ)(cid:0)
e2

t−k(θ) − e2
(cid:98)e2

t−k(θ)(cid:1)

(cid:0)

t−k(θ) − e2
(cid:98)e2

t−k(θ)(cid:1)(cid:0)

t (θ) − e2
(cid:98)e2

t (θ)(cid:1) +

1
√
n

n
(cid:88)

t=k+1

t−k(θ)(cid:0)
e2

t (θ) − e2
(cid:98)e2

t (θ)(cid:1)

+

1
√
n

n
(cid:88)

t=k+1

t (θ)(cid:0)
e2

t−k(θ) − e2
(cid:98)e2

t−k(θ)(cid:1)

:= I 1

1 + I 2

1 + I 3
1 .

Let us show that (cid:107)I 1

1 (cid:107)Θ

a.s.−→
n→+∞

0 in our two frameworks.

a/ If X ⊂ AC(Mθ, fθ), by Hölder’s inequality, it follows from (7.8) that,
t (θ)(cid:1)(cid:13)
(cid:13)
(cid:13)

t−k(θ) − e2
(cid:98)e2

t (θ) − e2
(cid:98)e2

t−k(θ)(cid:1)(cid:0)

t (θ) − e2

t (θ)(cid:13)
(cid:13)Θ

(cid:104)(cid:13)
(cid:0)
(cid:13)
(cid:13)

(cid:13)(cid:98)e2

E(cid:2)(cid:13)

1/2

≤

E

(cid:16)

(cid:105)

Θ

(cid:3) × E(cid:2)(cid:13)

(cid:13)(cid:98)e2

t−k(θ) − e2

t−k(θ)(cid:13)
(cid:13)Θ

(cid:3)(cid:17)1/2

.

But we have

(cid:13)
(cid:13)(cid:98)e2

t (θ) − e2

t (θ)(cid:13)

(cid:13)Θ ≤

1
h

(cid:0)2|Xt| + (cid:107) (cid:98)f t

θ(cid:107)Θ + (cid:107)f t

θ(cid:107)Θ

(cid:1)(cid:13)
(cid:13) (cid:98)f t

θ − f t

θ(cid:107)Θ +

4
h3/2

(cid:0)|Xt|2 + (cid:107)f t

θ(cid:107)2
Θ

(cid:1)(cid:13)
(cid:13) (cid:99)M t

θ − M t

θ(cid:107)Θ.

Therefore,

E(cid:2)(cid:13)

(cid:13)(cid:98)e2

t (θ) − e2

t (θ)(cid:13)
(cid:13)Θ

(cid:3) ≤ C

(cid:16)

(cid:16)

≤ C

θ(cid:107)2
Θ

Θ + (cid:107)f t

E(cid:2)(cid:0)|Xt|2 + (cid:107) (cid:98)f t
θ(cid:107)2
(cid:16)
E(cid:2)(cid:0)|Xt|4 + (cid:107)f t
2(cid:105)(cid:17)1/2

+C

(cid:88)

αj(fθ, Θ)Xt−j

(cid:12)
(cid:12)

E

(cid:104)(cid:12)
(cid:12)
(cid:12)

j≥t

(cid:1)(cid:3) × E(cid:2)(cid:13)

(cid:13) (cid:98)f t

θ − f t

θ(cid:107)2
Θ

(cid:3)(cid:17)1/2

θ(cid:107)2
Θ

(cid:1)(cid:3) × E(cid:2)(cid:13)

(cid:13) (cid:99)M t

θ − M t

θ(cid:107)2
Θ

(cid:3)(cid:17)1/2

+ C

(cid:16)

E

(cid:104)(cid:12)
(cid:12)
(cid:12)

αj(Mθ, Θ)Xt−j

2(cid:105)(cid:17)1/2
(cid:12)
(cid:12)

(cid:88)

j≥t

≤ C

(cid:88)

j≥t

αj(fθ, Θ) + αj(Mθ, Θ),

using E(cid:2)|Xt|4 + (cid:107)f t

θ(cid:107)2

Θ + (cid:107) (cid:98)f t

θ(cid:107)2
Θ

(cid:3) < ∞ and Cauchy-Schwarz Inequality. Hence,

E

(cid:104)(cid:13)
(cid:0)
(cid:13)
(cid:13)

t−k(θ) − e2
(cid:98)e2

t−k(θ)(cid:1)(cid:0)

t (θ) − e2
(cid:98)e2

t (θ)(cid:1)(cid:13)
(cid:13)
(cid:13)

1/2

Θ

(cid:105)

≤ C

(cid:88)

j≥t−k

αj(fθ, Θ) + αj(Mθ, Θ).

Therefore, from [27], (cid:107)I 1

1 (cid:107)Θ

a.s.−→
n→+∞

0 when

∞
(cid:88)

t=1

t−1/4 (cid:88)

j≥t

αj(fθ, Θ) + αj(Mθ, Θ) < ∞.

(7.26)

b/ if X ⊂ (cid:103)AC( (cid:101)Hθ), same computations imply (cid:107)I 1

1 (cid:107)Θ

a.s.−→
n→+∞

0 when

∞
(cid:88)

t=1

t−1/4 (cid:88)

j≥t

αj( (cid:101)Hθ, Θ) < ∞.

(7.27)

Since E(cid:2)(cid:107)e2
(cid:3) < ∞ and similarly E(cid:2)(cid:107)(cid:98)e2
θ(cid:107)2
Θ
1 that (cid:107)I 2
same inequalities as in the ﬁrst case of I 1

(cid:3) ≤ 2 h−1E(cid:2)X 2

t + (cid:107)f t

t (θ)(cid:107)Θ

1 (cid:107)Θ

0 and (cid:107)I 3

t (θ)(cid:107)Θ
1 (cid:107)Θ

a.s.−→
n→+∞

(cid:3) < ∞, we deduce from the
0 when

a.s.−→
n→+∞

∞
(cid:88)

t=1

t−1/4(cid:16) (cid:88)

j≥t

αj(fθ, Θ) + αj(Mθ, Θ) + αj( (cid:101)Hθ, Θ)

(cid:17)1/2

< ∞,

(7.28)

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

26

which is also the condition for insuring that (cid:107)I2(cid:107)Θ

a.s.−→
n→+∞

0 and (cid:107)I3(cid:107)Θ

a.s.−→
n→+∞

0. This ends the proof

of (7.25).
Finally, since (cid:98)ρk(θ) = (cid:98)γk(θ)/(cid:98)γ0(θ) and ρk(θ) = γk(θ)/γ0(θ), with γ0(θ) > 0, we deduce under condition
(7.28) that

√

(cid:13)(cid:98)ρk(θ) − ρk(θ)(cid:13)
n(cid:13)
(cid:13)Θ

a.s.−→
n→+∞

0 for any k ≥ 1.

This also implies

√

(cid:12)(cid:98)ρk(m∗) − ρk(m∗)(cid:12)
n(cid:12)
(cid:12)

a.s.−→
n→+∞

0 for any k ≥ 1.

(7.29)

(7.30)

(ii) The proof of this result has already been done in [31] but in a Gaussian framework. We recall
here the main lines while avoiding the Gaussian assumption. The ﬁrst step is to use a Taylor expansion
of the function γ. Hence, we have for each k = 1, . . . , K,

√

n γk(m∗) =

√

n γk((cid:98)θ(m∗)) =

√

n γk(θ∗) + ∂θγk(θ

(k)

√
)

n (cid:0)((cid:98)θ(m∗))i − θ∗

i

(cid:1)

i∈m∗ ,

(7.31)

where ∂θγk = t(cid:0)∂γk/∂θi
also have

(cid:1)

i∈m∗ , and θ

(k)

is in the ball of centre θ∗ and radius (cid:107)((cid:98)θ(m∗) − θ∗)i∈m∗ (cid:107). We

∂θγk(θ) = −

2
n

(cid:16) n
(cid:88)

t=k+1

t (θ) (cid:0)e2
e2

θ

+ et(θ)(cid:0)e2

t−k(θ) − 1(cid:1) ∂θM t
M t
θ
t (θ) − 1(cid:1) ∂θf t−k
θ
M t−k
θ

+ et−k(θ) (cid:0)e2

t−k(θ) − 1(cid:1) ∂θf t
θ
M t
θ

+ e2

t−k(θ) (cid:0)e2

t (θ) − 1(cid:1) ∂θM t−k
θ
M t−k
θ

(cid:17)

.

(7.32)

t (θ∗) − 1(cid:1) ∂f t−k
θ∗
M t−k
θ∗

t (θ∗) − 1(cid:3) = 0 since we have
∂f t
t−k(θ∗) −
θ∗
M t
θ∗
(cid:3) = 0. As a consequence, the expectation of the three last terms of (7.32) vanishes for θ = θ∗.

We have E(cid:2)et−k(θ∗) (cid:0)e2
assumed E[ξ2
1(cid:1) ∂f t
θ∗
M t
θ∗
By using the Ergodic Theorem, we ﬁnally obtained:

(cid:3) = 0 and this implies E(cid:2)et(θ∗)(cid:0)e2

| σ(cid:0)(ξs)s≤t−k
(cid:3) = E(cid:2)ξt

0] = 1. Moreover, E(cid:2)et(θ∗) ∂f t

(cid:1)(cid:3) = et−k(θ∗) ∂f t−k
θ∗
M t−k
θ∗

E(cid:2)e2

θ∗
M t
θ∗

∂θγk(θ∗)

a.s.−→
n→+∞

(cid:104)
k(θ∗) (cid:0)e2
− 2 E
e2

0(θ∗) − 1(cid:1) ∂θM k
M k
θ∗

θ∗

(cid:105)

= −2 E

(cid:104)(cid:0)ξ2

0 − 1(cid:1) ∂θ log (cid:0)M k

θ∗

(cid:1)(cid:105)
.

Moreover, since ∂2

θ2 fθ and ∂2

θ2Mθ exist, and since (cid:98)θ(m∗)

a.s.−→
n→+∞

θ∗, we deduce that the same almost

sure convergence occurs for ∂θγk(θ

(k)

(cid:0)∂θγk(θ

(k)

)(cid:1)

1≤k≤K

a.s.−→
n→+∞

). Then, we ﬁnally obtain
0 − 1(cid:1) ∂
∂θj

(cid:104)(cid:0)ξ2
E

JK(m∗) = −2

(cid:16)

log (cid:0)M i
θ∗

(cid:1)(cid:105)(cid:17)

1≤i≤K, j∈m∗

.

(7.33)

We also established a central limit theorem for (cid:98)θ(m∗) in (3.3), and this implies

(cid:0)∂θγk(θ

(k)

)(cid:1)

1≤k≤K

√

n (cid:0)((cid:98)θ(m∗))i − θ∗

i

(cid:1)

i∈m∗

L−→
n→+∞

NK

(cid:16)
0 , JK(m∗) F (θ∗, m∗)−1G(θ∗, m∗)F (θ∗, m∗)−1J (cid:48)

K(m∗)

(cid:17)

.

(7.34)

On the other hand, when θ = θ∗, e2
(cid:0)e2
such as case, the asymptotic behavior of the covariograms is well known and we deduce:

t is a sequence of centred iid random variables with variance µ4 − 1 with µ4 = E[ξ4

0] = 1, we deduce that
0]. In

for any t ∈ Z and since E[ξ2

t (θ∗) − 1(cid:1)

t (θ∗) = ξ2
t

√

n (cid:0)γk(θ∗)(cid:1)

1≤k≤K

L−→
n→+∞

NK

(cid:0)0 , (µ4 − 1)2 IK

(cid:1),

(7.35)

with Ik the (K × K) identity matrix.

We would like to use (7.31) for obtaining the asymptotic behavior of γ(m∗). In (7.34) and (7.35), we
obtained the asymptotic normality of each of the two terms composing γ(m∗). Now we need to study
the joint asymptotic behavior of

n γ(θ∗) and

n (cid:0)((cid:98)θ(m∗))i − θ∗

√

√

(cid:1)

i∈m∗ .

i

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

27

Using the proof of the asymptotic normality of the QMLE (see for instance [7]), a Taylor expansion of
log-likelihood for large n leads to

Therefore, the asymptotic cross expectation between (cid:0)∂θγk(θ
is equal to:

(cid:0)((cid:98)θ(m∗))i − θ∗

i

(cid:1)

i∈m∗ ≈ −(cid:0)F (θ∗, m∗)(cid:1)−1 1
)(cid:1)

(k)

n
√

Ln(θ∗).

∂
∂θ
n (cid:0)((cid:98)θ(m∗))i−θ∗

i

k

− JK(m∗) F (θ∗, m∗)−1E

(cid:104) ∂
∂θ

Ln(θ∗) γ(θ∗)(cid:48)(cid:105)
.

(cid:1)

i∈m∗ and

√

n γ(θ∗)

(7.36)

From (2.5), a direct diﬀerentiation of Ln provides

∂
∂θ

Ln(θ∗) =

n
(cid:88)

t=1

t (θ∗) − 1(cid:1) ∂
(cid:0)e2
∂θ

log (cid:0)M t
θ∗

(cid:1) +

n
(cid:88)

t=1

et(θ∗)

∂
∂θ

f t
θ∗

so that,

E

(cid:104) ∂
∂θ

(cid:105)
Ln(θ∗) γk(θ∗)

=

1
n

(cid:104) n
(cid:88)

E

i=1

i (θ∗) − 1(cid:1) ∂
(cid:0)e2
∂θ

log (cid:0)M i
θ∗

(cid:1)

n
(cid:88)

(cid:0)e2

j (θ∗) − 1(cid:1) (cid:0)e2

j−k(θ∗) − 1(cid:1)(cid:105)

+

1
n

(cid:104) n
(cid:88)

E

i=1

ei(θ∗)

∂
∂θ

j=k+1
n
(cid:88)

f i
θ∗

j=k+1

(cid:0)e2

j (θ∗) − 1(cid:1) (cid:0)e2

j−k(θ∗) − 1(cid:1)(cid:105)

=

1
n

n
(cid:88)

n
(cid:88)

i=1

j=k+1

(cid:104)(cid:0)ξ2
E

i − 1(cid:1) (cid:0)ξ2

j − 1(cid:1) (cid:0)ξ2

j−k − 1(cid:1) ∂
∂θ

log (cid:0)M i
θ∗

(cid:1)(cid:105)

+

1
n

n
(cid:88)

n
(cid:88)

(cid:104)
E
ξi

i=1

j=k+1

(cid:0)ξ2

j − 1(cid:1) (cid:0)ξ2

j−k − 1(cid:1) ∂
∂θ

(cid:105)
.

f i
θ∗

(cid:104)(cid:0)ξ2

i − 1(cid:1) (cid:0)ξ2

j − 1(cid:1) (cid:0)ξ2

Using conditional expectations, we have E
since k ≥ 1. Moreover, for i = j, we obtain:
j−k − 1(cid:1) ∂
∂θ
JK(m∗). Similarly, and using the assumption E(cid:2)ξ3
= 0 for any i, j and k. As a consequence,

which is the row k of matrix − (µ4−1)
(cid:104)
(cid:105)
j − 1(cid:1) (cid:0)ξ2
obtain E

i−k − 1(cid:1) ∂
∂θ

= (µ4 − 1) E

j − 1(cid:1) (cid:0)ξ2

i − 1(cid:1) (cid:0)ξ2

j−k − 1(cid:1) ∂

log (cid:0)M i
θ∗

log (cid:0)M i
θ∗

(cid:104)(cid:0)ξ2
E

(cid:104)(cid:0)ξ2

(cid:0)ξ2

(cid:1)(cid:105)

ξi

j−k − 1(cid:1) ∂

∂θ log (cid:0)M i

∂θ f i
θ∗

θ∗

2

(cid:1)(cid:105)
,

(cid:1)(cid:105)

= 0 for i (cid:54)= j

0] = 0, we

Cov (cid:0)√

n γ(θ∗) , (cid:0)∂θγk(θ

(k)

√

)(cid:1)

k

n (cid:0)((cid:98)θ(m∗))i − θ∗

i

(cid:1)

(cid:1)

i∈m∗

Finally, we deduce the asymptotic covariance matrix of

n γ(m∗), which is

1
2

(µ4 − 1) JK(m∗) F (θ∗, m∗)−1 J (cid:48)

K(m∗).

−→
n→∞
√

(µ4 − 1)2 IK + JK(m∗) F (θ∗, m∗)−1G(θ∗, m∗)F (θ∗, m∗)−1J (cid:48)

K(m∗)

+ (µ4 − 1) JK(m∗) F (θ∗, m∗)−1 J (cid:48)

K(m∗).

Moreover the vector γ(m∗) is normal distributed from Lemma 3.3 of [32].
Thus, using Slutsky Lemma and with γ0(m∗)
limit theorem (5.1) holds with

a.s.−→
n→+∞

µ4 − 1, and with ρk(m∗) = γk(m∗)/γ0(m∗), the

V (θ∗, m∗) := IK + (µ4 − 1)−2 JK(m∗) F (θ∗, m∗)−1G(θ∗, m∗)F (θ∗, m∗)−1J (cid:48)

K(m∗)

+ (µ4 − 1)−1 JK(m∗) F (θ∗, m∗)−1 J (cid:48)

K(m∗).

(7.37)

The proof is achieved after using the limit theorem (7.30).

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

28

(2) (5.2) follows directly from (5.1).

(3) We follow a same reasoning like in the proof of Theorem 3.2. For x = (xk)1≤k≤K ∈ RK, denote

by Fn(x) = P

(cid:16) (cid:92)

√

n (cid:0)

(cid:98)ρ( (cid:98)m)(cid:1)

k ≤ xk

(cid:17)

the distribution function of

√

n(cid:98)ρ( (cid:98)m).

Applying the Total Probability Rule and by virtue of Theorem 3.1, we obtain:

1≤k≤K

Fn(x) = P

(cid:16) (cid:92)

√

n (cid:0)

(cid:98)ρ(m∗)(cid:1)

k ≤ xk

(cid:17)

.

1≤k≤K

Therefore, the vectors

√

n(cid:98)ρ( (cid:98)m) and

√

n(cid:98)ρ(m∗) have exactly the same distribution.

(cid:4)

References

[1] Akaike, H. Information theory and an extension of the maximum likelihood principle. Proceed-
ings of the 2nd international symposium on information, Akademiai Kiado, Budapest (1973).
[2] Allen, D. The relationship between variable selection and data agumentation and a method for

prediction. Technometrics 16, 1 (1974), 125–127.

[3] Alquier, P., and Wintenberger, O. Model selection for weakly dependent time series fore-

casting. Bernoulli 18, 3 (2012), 883–913.

[4] Arkoun, O., Brua, J.-Y., and Pergamenshchikov, S. Sequential model selection method

for nonparametric autoregression. arXiv preprint arXiv:1809.02241 (2018).

[5] Arlot, S., and Massart, P. Data-driven calibration of penalties for least-squares regression.

Journal of Machine learning research 10 (2009), 245–279.

[6] Bardet, J.-M., Boularouk, Y., and Djaballah, K. Asymptotic behavior of the laplacian
quasi-maximum likelihood estimator of aﬃne causal processes. Electronic journal of statistics 11,
1 (2017), 452–479.

[7] Bardet, J.-M., and Wintenberger, O. Asymptotic normality of the quasi-maximum like-
lihood estimator for multidimensional causal processes. The Annals of Statistics 37, 5B (2009),
2730–2759.

[8] Berkes, I., Horváth, L., and Kokoszka, P. GARCH processes: structure and estimation.

Bernoulli 9 (2003), 201–227.

[9] Birgé, L., and Massart, P. Minimal penalties for gaussian model selection. Probability theory

and related ﬁelds 138, 1-2 (2007), 33–73.

[10] Ding, J., Tarokh, V., and Yang, Y. Bridging aic and bic: a new criterion for autoregression.

IEEE Transactions on Information Theory 64, 6 (2018), 4024–4043.

[11] Ding, J., Tarokh, V., and Yang, Y. Model selection techniques: An overview. IEEE Signal

Processing Magazine 35, 6 (2018), 16–34.

[12] Ding, Z., Granger, C., and Engle, R. A long memory property of stock market returns and

a new model. Journal of empirical ﬁnance 1, 1 (1993), 83–106.

[13] Doukhan, P., and Wintenberger, O. Weakly dependent chains with inﬁnite memory.

Stochastic Processes and their Applications 118, 11 (2008), 1997–2013.

[14] Duchesne, P., and Francq, C. On diagnostic checking time series models with portmanteau
test statistics based on generalized inverses and. In COMPSTAT 2008. Springer, 2008, pp. 143–
154.

[15] Francq, C., and Zakoïan, J.-M. Maximum likelihood estimation of pure garch and arma-garch

processes. Bernoulli 10 (2004), 605–637.

[16] Gao, J., and Tong, H. Semiparametric non-linear time series model selection. Journal of the

Royal Statistical Society: Series B 66, 2 (2004), 321–336.

[17] Hannan, E. The estimation of the order of an arma process. The Annals of Statistics 8, 5 (1980),

1071–1081.

[18] Hoerl, A., and Kennard, R. Ridge regression: Biased estimation for nonorthogonal problems.

Technometrics 12, 1 (1970), 55–67.

[19] Hsu, H.-L., Ing, C.-K., and Tong, H. On model selection from a ﬁnite family of possibly

misspeciﬁed time series models. The Annals of Statistics 47, 2 (2019), 1061–1087.

[20] Hurvich, C., and Tsai, C.-L. Regression and time series model selection in small samples.

Biometrika 76, 2 (1989), 297–307.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

29

[21] Ing, C.-K. Accumulated prediction errors, information criteria and optimal forecasting for au-

toregressive time series. The Annals of Statistics 35, 3 (2007), 1238–1277.

[22] Ing, C.-K., Sin, C.-Y., and Yu, S.-H. Model selection for integrated autoregressive processes

of inﬁnite order. Journal of Multivariate Analysis 106 (2012), 57–71.

[23] Ing, C.-K., and Wei, C.-Z. Order selection for same-realization predictions in autoregressive

processes. The Annals of Statistics 33, 5 (2005), 2423–2474.

[24] Jeantheau, T. Strong consistency of estimators for multivariate arch models. Econometric

Theory 14, 1 (1998), 70–86.

[25] Kapetanios, G. Model selection in threshold models. Journal of Time Series Analysis 22, 6

(2001), 733–754.

[26] Kock, A. Consistent and conservative model selection with the adaptive lasso in stationary and

nonstationary autoregressions. Econometric Theory 32, 1 (2016), 243–259.

[27] Kounias, E., and Weng, T. An inequality and almost sure convergence. The Annals of

Mathematical Statistics 40, 3 (1969), 1091–1093.

[28] Lerasle, M. Optimal model selection for density estimation of stationary data under various

mixing conditions. The Annals of Statistics 39, 4 (2011), 1852–1877.

[29] Li, G., and Li, W. Least absolute deviation estimation for fractionally integrated autoregressive
moving average time series models with conditional heteroscedasticity. Biometrika 95, 2 (2008),
399–414.

[30] Li, W. On the asymptotic standard errors of residual autocorrelations in nonlinear time series

modelling. Biometrika 79, 2 (1992), 435–437.

[31] Li, W., and Mak, T. On the squared residual autocorrelations in non-linear time series with

conditional heteroskedasticity. Journal of Time Series Analysis 15, 6 (1994), 627–636.

[32] Ling, S., and Li, W.-K. Diagnostic checking of nonlinear multivariate time series with multi-

variate arch errors. Journal of Time Series Analysis 18, 5 (1997), 447–464.

[33] Ling, S., and McAleer, M. Asymptotic theory for a vector arma-garch model. Econometric

theory 19, 2 (2003), 280–310.

[34] Mallows, C. Some comments on cp. Technometrics 15, 4 (1973), 661–675.
[35] McQuarrie, A., and Tsai, C. Regression and Time Series Model Selection. World Scientiﬁc

Pub Co Inc, 1998.

[36] Rao, C., Wu, Y., Konishi, S., and Mukerjee, R. On model selection. Lecture Notes-

Monograph Series (2001), 1–64.

[37] Ren, Y., and Zhang, X. Subset selection for vector autoregressive processes via adaptive lasso.

Statistics & probability letters 80, 23-24 (2010), 1705–1712.

[38] Schwarz, G. Estimating the dimension of a model. The annals of statistics 6, 2 (1978), 461–464.
[39] Shao, Q., and Yang, L. Oracally eﬃcient estimation and consistent model selection for auto-
regressive moving average time series with trend. Journal of the Royal Statistical Society: Series
B 79, 2 (2017), 507–524.

[40] Shi, P., and Tsai, C.-L. Regression model selection-a residual likelihood approach. Journal of

the Royal Statistical Society: Series B 64, 2 (2002), 237–252.

[41] Shibata, R. Asymptotically eﬃcient selection of the order of the model for estimating parameters

of a linear process. The Annals of Statistics (1980), 147–164.

[42] Sin, C.-Y., and White, H. Information criteria for selecting possibly misspeciﬁed parametric

models. Journal of Econometrics 71, 1-2 (1996), 207–225.

[43] Stone, M. Cross-validatory choice and assessment of statistical predictions. Journal of the royal

statistical society. Series B (1974), 111–147.

[44] Straumann, D., and Mikosch, T. Quasi-maximum-likelihood estimation in conditionally het-
eroscedastic time series: A stochastic recurrence equations approach. The Annals of Statistics 34,
5 (2006), 2449–2495.

[45] Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society. Series B (1996), 267–288.

[46] Tsay, R. Order selection in nonstationary autoregressive models. The Annals of Statistics 12, 4

(1984), 1425–1433.

[47] Tse, Y., and Zuo, X. Testing for conditional heteroscedasticity: Some monte carlo results.

Journal of Statistical Computation and Simulation 58, 3 (1997), 237–253.

[48] White, H. Maximum likelihood estimation of misspeciﬁed models. Econometrica (1982), 1–25.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

Bardet et al./Consistent model selection criteria and goodness-of-ﬁt test for aﬃne causal processes

30

[49] Zou, H. The adaptive lasso and its oracle properties. Journal of the American Statistical Asso-

ciation 101, 476 (2006), 1418–1429.

[50] Zou, H., and Hastie, T. Regularization and variable selection via the elastic net. Journal of

the Royal Statistical Society: Series B 67, 2 (2005), 301–320.

imsart-ejs ver. 2014/10/16 file: final_5.tex date: July 24, 2019

