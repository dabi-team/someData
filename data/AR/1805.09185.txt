9
1
0
2

l
u
J

1

]

C
O
.
h
t
a
m

[

2
v
5
8
1
9
0
.
5
0
8
1
:
v
i
X
r
a

Alternating Randomized Block Coordinate Descent

Jelena Diakonikolas 1 Lorenzo Orecchia 1

Abstract

Block-coordinate descent algorithms and alternat-
ing minimization methods are fundamental opti-
mization algorithms and an important primitive
in large-scale optimization and machine learn-
ing. While various block-coordinate-descent-type
methods have been studied extensively, only alter-
nating minimization – which applies to the setting
of only two blocks – is known to have conver-
gence time that scales independently of the least
smooth block. A natural question is then: is the
setting of two blocks special?

We show that the answer is “no” as long as the
least smooth block can be optimized exactly –
an assumption that is also needed in the setting
of alternating minimization. We do so by intro-
ducing a novel algorithm AR-BCD, whose con-
vergence time scales independently of the least
smooth (possibly non-smooth) block. The basic
algorithm generalizes both alternating minimiza-
tion and randomized block coordinate (gradient)
descent, and we also provide its accelerated ver-
sion – AAR-BCD. As a special case of AAR-
BCD, we obtain the ﬁrst nontrivial accelerated
alternating minimization algorithm.

perform a gradient descent step on a single block at every
iteration, while leaving the remaining variable blocks ﬁxed.
A paradigmatic example of this approach is the randomized
Kaczmarz algorithm of (Strohmer & Vershynin, 2009) for
linear systems and its generalization (Nesterov, 2012). The
second class is that of alternating minimization methods,
i.e., algorithms that partition the variable set into only n = 2
blocks and alternate between exactly optimizing one block
or the other at each iteration (see, e.g., (Beck, 2015) and
references therein).

Besides the computational advantage in only having to up-
date a subset of variables at each iteration, methods in these
two classes are also able to exploit better the structure of
the problem, which, for instance, may be computationally
expensive only in a small number of variables. To formalize
this statement, assume that the set of variables is partitioned
into n ≤ N mutually disjoint blocks, where the ith block
of variable x is denoted by xi, and the gradient correspond-
ing to the ith block is denoted by ∇if (x). Each block i
will be associated with a smoothness parameter Li, I.e.,
∀x, y ∈ RN :

(cid:107)∇if (x + I i

N y) − ∇if (x)(cid:107)∗ ≤ Li(cid:107)yi(cid:107),

(1.1)

where I i
one for coordinates from block i, and are zero otherwise.

N is a diagonal matrix whose diagonal entries equal

1. Introduction

First-order methods for minimizing smooth convex func-
tions are a cornerstone of large-scale optimization and ma-
chine learning. Given the size and heterogeneity of the data
in these applications, there is a particular interest in design-
ing iterative methods that, at each iteration, only optimize
over a subset of the decision variables (Wright, 2015).

This paper focuses on two classes of methods that consti-
tute important instantiations of this idea. The ﬁrst class
is that of block-coordinate descent methods, i.e., methods
that partition the set of variables into n ≥ 2 blocks and

1Department of Computer Science, Boston University, Boston,
Jelena Diakonikolas <jele-

MA, USA. Correspondence to:
nad@bu.edu>, Lorenzo Orecchia <orecchia@bu.edu>.

(cid:15)

(cid:17)

(cid:16) (cid:80)

In this setting, the convergence time of standard randomized
block-coordinate descent methods, such as those in (Nes-
i Li
, where (cid:15) is the desired
terov, 2012), scales as O
(cid:15)
additive error. By contrast, when n = 2, the convergence
time of the alternating minimization method (Beck, 2015)
(cid:1) , where Lmin is the minimum smooth-
scales as O (cid:0) Lmin
ness parameter of the two blocks. This means that one of
the two blocks can have arbitrarily poor smoothness (in-
cluding ∞), as long it is easy to optimize over it. Some
important examples with a nonsmooth block (with smooth-
ness parameter equal to inﬁnity) can be found in (Beck,
2015). Additional examples of problems for which exact
optimization over the least smooth block can be performed
efﬁciently are provided in Appendix B.

In this paper, we address the following open question, which
was implicitly raised by (Beck & Tetruashvili, 2013): can we
design algorithms that combine the features of randomized
block-coordinate descent and alternating minimization? In

 
 
 
 
 
 
Alternating Randomized Block Coordinate Descent

particular, assuming we can perform exact optimization
on block n, can we construct a block-coordinate descent
algorithm whose running time scales with O((cid:80)n−1
i=1 Li), i.e.,
independently of the smoothness Ln of the nth block? This
would generalize both existing block-coordinate descent
methods, by allowing one block to be optimized exactly,
and existing alternating minimization methods, by allowing
n to be larger than 2 and requiring exact optimization only
on a single block.

We answer these questions in the afﬁrmative by presenting
a novel algorithm: alternating randomized block coordinate
descent (AR-BCD). The algorithm alternates between an
exact optimization over a ﬁxed, possibly non-smooth block,
and a gradient descent or exact optimization over a randomly
selected block among the remaining blocks. For two blocks,
the method reduces to the standard alternating minimization,
while when the non-smooth block is empty (not optimized
over), we get randomized block coordinate descent (RCDM)
from (Nesterov, 2012).

Our second contribution is AAR-BCD, an accelerated ver-
sion of AR-BCD, which achieves the accelerated rate of
1
k2 without incurring any dependence on the smoothness
of block n. Furthermore, when the non-smooth block is
empty, AAR-BCD recovers the fastest known convergence
bounds for block-coordinate descent (Qu & Richt´arik, 2016;
Allen-Zhu et al., 2016; Nesterov, 2012; Lin et al., 2014;
Nesterov & Stich, 2017). As a special case, AAR-BCD
provides the ﬁrst accelerated alternating minimization algo-
rithm, obtained directly from AAR-BCD when the number
of blocks equals two.1 Another conceptual contribution
is our extension of the approximate duality gap technique
of (Diakonikolas & Orecchia, 2017), which leads to a gen-
eral and more streamlined analysis.

Finally, to illustrate the results, we perform a preliminary
experimental evaluation of our methods against existing
block-coordinate algorithms and discuss how their perfor-
mance depends on the smoothness and size of the blocks.

Related Work Alternating minimization and cyclic block
coordinate descent are old and fundamental algorithms (Or-
tega & Rheinboldt, 1970) whose convergence (to a station-
ary point) has been studied even in the non-convex setting,
in which they were shown to converge asymptotically under
the additional assumptions that the blocks are optimized
exactly and their minimizers are unique (Bertsekas, 1999).

1The remarks about accelerated alternating minimization have
been added in the second version of the paper, in July 2019, partly
to clarify the relationship to methods obtained in (Guminov et al.,
2019), which was posted to the arXiv for the ﬁrst time in June
2019. At a technical level, nothing new is introduced compared
to the ﬁrst version of the paper – everything stated in Section 4.2
follows either as a special case or a simple corollary of the results
that appeared in the ﬁrst version of the paper in May 2018.

However, even in the non-smooth convex case, methods
that perform exact minimization over a ﬁxed set of blocks
may converge arbitrarily slowly. This has lead scholars
to focus on the case of smooth convex minimization, for
which nonasymptotic convergence rates were obtained re-
cently in (Beck & Tetruashvili, 2013; Beck, 2015; Sun &
Hong, 2015; Saha & Tewari, 2013). However, prior to
our work, convergence bounds that are independent of the
largest smoothness parameter were only known for the set-
ting of two blocks.

Randomized coordinate descent methods, in which steps
over coordinate blocks are taken in a non-cyclic random-
ized order (i.e., in each iteration one block is sampled with
replacement) were originally analyzed in (Nesterov, 2012).
The same paper (Nesterov, 2012) also provided an accel-
erated version of these methods. The results of (Nesterov,
2012) were subsequently improved and generalized to vari-
ous other settings (such as, e.g., composite minimization) in
(Lee & Sidford, 2013; Allen-Zhu et al., 2016; Nesterov &
Stich, 2017; Richt´arik & Tak´aˇc, 2014; Fercoq & Richt´arik,
2015; Lin et al., 2014). The analysis of the different block
coordinate descent methods under various sampling proba-
bilities (that, unlike in our setting, are non-zero over all the
blocks) was uniﬁed in (Qu & Richt´arik, 2016) and extended
to a more general class of steps within each block in (Gower
& Richt´arik, 2015; Qu et al., 2016).

Our results should be carefully compared to a number of
proximal block-coordinate methods that rely on different
assumptions (Tseng & Yun, 2009; Richt´arik & Tak´aˇc, 2014;
Lin et al., 2014; Fercoq & Richt´arik, 2015). In this setting,
the function f is assumed to have the structure f0(x) +
Ψ(x), where f0 is smooth, the non-smooth convex function
Ψ is separable over the blocks, i.e., Ψ(x) = (cid:80)n
i=1 Ψi(xi),
and we can efﬁciently compute the proximal operator of
each Ψi. This strong assumption allows these methods to
make use of the standard proximal optimization framework.
By contrast, in our paper, the convex objective can be taken
to have an arbitrary form, where the non-smoothness of a
block need not be separable, though the function is assumed
to be differentiable.

2. Preliminaries

We assume that we are given oracle access to the gradients of
a continuously differentiable convex function f : RN → R,
where computing gradients over only a subset of coordinates
is computationally much cheaper than computing the full
gradient. We are interested in minimizing f (·) over RN , and
we denote x∗ = argminx∈RN f (x). We let (cid:107) · (cid:107) denote an
arbitrary (but ﬁxed) norm, and (cid:107)·(cid:107)∗ denote its dual norm, de-

Alternating Randomized Block Coordinate Descent

ﬁned in the standard way: (cid:107)z(cid:107)∗ = supx∈RN :(cid:107)x(cid:107)=1 (cid:104)z, x(cid:105).2
Let IN be the identity matrix of size N , I i
N be a diagonal
matrix whose diagonal elements j are equal to one if vari-
able j is in the ith block, and zero otherwise. Notice that
IN = (cid:80)n
N . Let Si(x) = {y ∈ RN : (IN − I i
i=1 I i
N )y =
N )x}, that is, Si contains all the points from RN
(IN − I i
whose coordinates differ from those of x only over block i.

We denote the smoothness parameter of block i by Li, as
deﬁned in Equation (1.1). Equivalently, ∀x, y ∈ RN :

f (x + I i

N y) ≤ f (x) + (cid:10)∇if (x), yi(cid:11) +

Li
2

(cid:107)yi(cid:107)2.

(2.1)

n = 2.

The algorithm is deﬁned as follows.

ˆxk = argmin

f (x),

x∈S1(xk−1)

xk = argmin
x∈S2(ˆxk)
x1 ∈ RN is an arbitrary initial point.

f (x),

(AM)

We note that for the standard analysis of alternating mini-
mization (Beck, 2015), the exact minimization step over
the smoother block can be replaced by a gradient step
(Equation (2.2)), while still leading to convergence that is
only dependent on the smaller smoothness parameter.

The gradient step over block i is then deﬁned as:

2.2. Randomized Block Coordinate (Gradient) Descent

Ti(x)

(cid:110)

= argmin
y∈Si(x)

(cid:104)∇f (x), y − x(cid:105) +

(cid:107)y − x(cid:107)2(cid:111)
.

(2.2)

Li
2

By standard arguments (see, e.g., Exercise 3.27 in (Boyd &
Vandenberghe, 2004)):

f (Ti(x)) − f (x) ≤ −

1
2Li

(cid:107)∇if (x)(cid:107)2
∗.

(2.3)

Without loss of generality, we will assume that the nth
block has the largest smoothness parameter and is possi-
bly non-smooth (i.e., it can be Ln = ∞). The standing
assumption is that exact minimization over the nth block
is “easy”, meaning that it is computationally inexpensive
and possibly solvable in closed form; for some important
examples that have this property, see Appendix B. Observe
that when block n contains a small number of variables, it
is often computationally inexpensive to use second-order
optimization methods, such as, e.g., interior point method.

We assume that f (·) is strongly convex with parameter µ ≥
0, where it could be µ = 0 (in which case f (·) is not strongly
convex). Namely, ∀x, y:

f (y) ≥ f (x) + (cid:104)∇f (x), y − x(cid:105) +

µ
2

(cid:107)y − x(cid:107)2.

(2.4)

When µ > 0, we take (cid:107) · (cid:107) = (cid:107) · (cid:107)2, which is customary for
smooth and strongly convex minimization (Bubeck, 2014).

Throughout the paper, whenever we take unconditional ex-
pectation, it is with respect to all randomness in the algo-
rithm.

2.1. Alternating Minimization

In
there

(standard)

alternating minimization

are only two blocks of

coordinates,

(AM),
i.e.,

2Note that the analysis extends in a straightforward way to the
case where each block is associated with a different norm (see,
e.g., (Nesterov, 2012)); for simplicity of presentation, we take the
same norm over all blocks.

The simplest version of randomized block coordinate (gra-
dient) descent (RCDM) can be stated as (Nesterov, 2012):

Select ik ∈ {1, . . . , n} w.p. pik > 0,
xk = Tik (xk−1),
x1 ∈ RN is an arbitrary initial point,

(RCDM)

where (cid:80)n
i=1 pi = 1. A standard choice of the probability
distribution is pi ∼ Li, leading to the convergence rate that
depends on the sum of block smoothness parameters.

3. AR-BCD

The basic version of alternating randomized block co-
ordinate descent (AR-BCD) is a direct generalization
of (AM) and (RCDM): when n = 2,
is equiva-
lent to (AM), while when the size of the nth block is
zero, it reduces to (RCDM). The method is stated as follows:

it

Select ik ∈ {1, . . . , n − 1} w.p. pik > 0,
ˆxk = Tik (xk−1),

xk = argmin
x∈Sn(ˆxk)
x1 ∈ RN is an arbitrary initial point,

f (x),

(AR-BCD)

where (cid:80)n−1
i=1 pi = 1. We note that nothing will
change in the analysis if the step ˆxk = Tik (xk−1)
is replaced by ˆxk = argminx∈Sik (xk−1) f (x), since
minx∈Sik (xk−1) f (x) ≤ f (Tik (xk−1)).
In the rest of the section, we show that (AR-BCD) leads
to a convergence bound that interpolates between the con-
vergence bounds of (AM) and (RCDM): it depends on the
sum of the smoothness parameters of the ﬁrst n − 1 blocks,
while the dependence on the remaining problem parameters
is the same for all these methods.

3.1. Approximate Duality Gap

Using (2.3), if i2 = i, then:

Alternating Randomized Block Coordinate Descent

To analyze (AR-BCD), we extend the approximate duality
gap technique (Diakonikolas & Orecchia, 2017) to the set-
ting of randomized block coordinate descent methods. The
approximate duality gap Gk is deﬁned as the difference of
an upper bound Uk and a lower bound Lk to the minimum
function value f (x∗). For (AR-BCD), we choose the upper
bound to simply be Uk = f (xk+1).

The generic construction of the lower bound is as follows.
Let x1, x2, ..., xk be any sequence of points from RN (in
fact we will choose them to be exactly the sequence con-
structed by (AR-BCD)). Then, by (strong) convexity of
f (·), f (x∗) ≥ f (xj) + (cid:104)∇f (xj), x∗ − xj(cid:105) + µ
2 (cid:107)x∗ − xj(cid:107)2,
∀j ∈ {1, . . . , k}. In particular, if aj > 0 is a sequence of
(deterministic, independent of ij) positive real numbers and
Ak = (cid:80)k

j=1 aj, then:

f (x∗) ≥

(cid:80)k

j=1 ajf (xj) + (cid:80)k

j=1 aj (cid:104)∇f (xj), x∗ − xj(cid:105)
Ak
j=1 aj(cid:107)x∗ − xj(cid:107)2

def= Lk.

(3.1)

(cid:80)k

µ
2

+

Ak

3.2. Convergence Analysis

The main idea in the analysis is to show that E[AkGk −
Ak−1Gk−1] ≤ Ek, for some deterministic Ek. Then, using
linearity of expectation, E[f (xk+1)] − f (x∗) ≤ E[Gk] ≤
E[A1G1]
. The bound in expectation can then
Ak
be turned into a bound in probability, using well-known
concentration bounds. The main observation that allows us
not to pay for the non-smooth block is:

j=2 Ej
Ak

(cid:80)k

+

Observation 3.1. For xk’s constructed by (AR-BCD),
∇nf (xk) = 0, ∀k, where 0 is the vector of all zeros.

This observation is essentially what allows us to sample ik
only from the ﬁrst n − 1 blocks, and holds due to the step
xk = argminx∈Sn(ˆxk) f (x) from (AR-BCD).
= maxx∈RN {(cid:107)I i
Denote Rxi
f (x1)}, and let us bound the initial gap A1G1.
Proposition 3.2. E[A1G1] ≤ E1, where E1 =
a1

N (x∗ − x)(cid:107)2 : f (x) ≤

Rxi

(cid:80)n−1
i=1

− µ
2

(cid:16) Li
2pi

(cid:17)

.

∗

∗

Proof. By linearity of expectation, E[A1G1] = E[A1U1] −
E[A1L1]. The initial lower bound is deterministic, and, by
∇nf (x1) = 0 and duality of norms, is bounded as:

E[A1L1] ≥a1f (x1) − a1

n−1
(cid:80)
i=0

(cid:107)∇if (x1)(cid:107)∗(cid:107)xi

∗ − xi
1(cid:107)

U1 = f (x2) ≤ f (ˆx2) ≤ f (x1) −

1
2Li

(cid:107)∇if (x1)(cid:107)2
∗.

Since block i is selected with probability pi and A1 = a1:

E[A1U1] ≤a1f (x1) −

n−1
(cid:80)
i=1

a1pi
2Li

(cid:107)∇if (xi)(cid:107)2
∗.

Since the inequality 2ab − a2 ≤ b2 holds ∀a, b, we have:

a1(cid:107)∇if (x1)(cid:107)∗(cid:107)xi

∗ − xi

1(cid:107) −

a1pi
2Li

(cid:107)∇if (xi)(cid:107)2
∗

≤

a1Li
2pi

(cid:107)xi

∗ − xi

1(cid:107)2, ∀i ∈ {1, . . . , n − 1}

Hence, when µ = 0, E[A1G1] ≤ (cid:80)n−1
1(cid:107)2.
When µ > 0, since in that case we are assuming (cid:107)·(cid:107) = (cid:107)·(cid:107)2
(Section 2), (cid:107)x∗ − x1(cid:107)2 ≥ (cid:80)n−1
1(cid:107)2, leading to
i=1 (cid:107)xi
(cid:17)
(cid:80)n−1
E[A1G1] ≤ a1
(cid:107)xi
1(cid:107)2.
i=1

∗ − xi
∗ − xi

∗ − xi

a1Li
2pi

(cid:107)xi

− µ
2

(cid:16) Li
2pi

i=1

We now show how to bound the error in the decrease of the
scaled gap AkGk.
Lemma 3.3. E[AkGk − Ak−1Gk−1] ≤ Ek, where Ek =
.
ak

Rxi

(cid:80)n−1
i=1

(cid:16) akLi
2Akpi

− µ
2

(cid:17)

∗

Proof. Let Fk denote the natural ﬁltration up to iteration k.
By linearity of expectation and AkLk − Ak−1Lk−1 being
measurable w.r.t. Fk,

E[AkGk − Ak−1Gk−1|Fk]

= E[AkUk − Ak−1Uk−1|Fk] − (AkLk − Ak−1Lk−1).

With probability pi and as f (xk+1) ≤ f (ˆxk+1), the change
in the upper bound is:

AkUk − Ak−1Uk−1 ≤Akf (ˆxk+1) − Ak−1f (xk)

≤akf (xk) −

Ak
2Li

(cid:107)∇if (xk)(cid:107)2
∗,

where the second line follows from ˆxk+1 = Tik (xk) and
Equation (2.3). Hence:

E[AkUk − Ak−1Uk−1|Fk]

≤ akf (xk) − Ak

n−1
(cid:80)
i=1

pi
2Li

(cid:107)∇if (xk)(cid:107)2
∗.

+ a1

µ
2

(cid:107)x∗ − x1(cid:107)2.

On the other hand, using the duality of norms, the change

Alternating Randomized Block Coordinate Descent

in the lower bound is:

AkLk − Ak−1Lk−1

≥ akf (xk) − ak

(cid:107)∇if (xk)(cid:107)∗(cid:107)xi

∗ − xi

k(cid:107)

n−1
(cid:80)
i=1
(cid:107)x∗ − xk(cid:107)2

+ ak

µ
2

≥ akf (xk) − ak

n−1
(cid:80)
i=1

(cid:107)∇if (xk)(cid:107)∗

(cid:113)

Rxi

∗

+ ak

µ
2

(cid:107)x∗ − xk(cid:107)2.

By the same argument as in the proof of Proposi-
tion 3.2, it follows that: E[AkGk − Ak−1Gk−1|Fk] ≤
= Ek. Taking expectations
Rxi
ak
on both sides, as Ek is deterministic, the proof follows.

(cid:16) Liak
2Akpi

(cid:80)n−1
i=1

− µ
2

(cid:17)

∗

We are now ready to prove the convergence bound for (AR-
BCD), as follows.

Theorem 3.4. Let xk evolve according to (AR-BCD). Then,
∀k ≥ 1:

1. If µ = 0 : E[f (xk+1)] − f (x∗) ≤

particular, for pi =

Li
(cid:80)n−1
i(cid:48)=1

Li(cid:48)

, 1 ≤ i ≤ n − 1:

2 (cid:80)n−1
k+3

i=1

Li
pi

Rxi
∗

. In

E[f (xk+1)] − f (x∗) ≤

2((cid:80)n−1

i(cid:48)=1 Li(cid:48)) (cid:80)n−1
k + 3

i=1 Rxi

∗

Similarly, for pi = 1

n−1 , 1 ≤ i ≤ n − 1 :

E[f (xk+1)] − f (x∗) ≤

2(n − 1) (cid:80)n−1
k + 3

i=1 LiRxi

∗

2. If µ > 0, pi =

Li
(cid:80)n−1
i(cid:48)=1

Li(cid:48)

and (cid:107) · (cid:107) = (cid:107) · (cid:107)2 :

E[f (xk+1)] − f (x∗)
(cid:16)

≤

1 −

(cid:17)k

µ
(cid:80)n−1
i(cid:48)=1 Li(cid:48)

((cid:80)n−1

i(cid:48)=1 Li(cid:48))(cid:107)(IN − I n

N )(x∗ − x1)(cid:107)2

2

·

.

.

2

= (j+1)2

j(j+3) ≤ 1, and thus:

µ = 0. Let aj = j+1
Li
(cid:80)k
pi

2 . Then aj
Rxi
∗

i=1

Aj

2 (cid:80)n−1
k+3

j=1 Ej
Ak

≤

,which proves the ﬁrst part of the
theorem, up to concrete choices of pi’s, which follow by
simple computations.

(cid:17)

Li(cid:48)

Rxi

Li
(cid:80)n−1
i(cid:48)=1

(cid:16) aj Li
2Aj pi
=

For the second part of the theorem, as µ > 0, we are as-
suming that (cid:107) · (cid:107) = (cid:107) · (cid:107)2, as discussed in Section 2. From
, ∀j ≥ 2.
Lemma 3.3, Ej = aj

(cid:80)n−1
i=1
, if we take aj
Aj

− µ
2
µ
As pi =
(cid:80)n−1
i(cid:48)=1
that Ej = 0, ∀j ≥ 2. Let a1 = A1 = 1 and aj
Aj
for j ≥ 2. Then: E[f (xk+1)]−f (x∗) ≤ E[Gk] ≤
As A1
Ak

· · · · · Ak−1
Ak

= 1 − aj
= A1
A2
Aj
E[f (xk+1)] − f (x∗) ≤
E[G1]. It re-
1 −
mains to observe that, from Proposition 3.2, E[G1] ≤
Li(cid:48) )(cid:107)(IN −I n
(cid:0)1 −

and Aj−1
Aj
(cid:17)k−1

∗
, it follows
µ
(cid:80)n−1
Li(cid:48)
i(cid:48)=1
E[A1G1]
.
Ak

N )(x∗−x1)(cid:107)2

µ
(cid:80)n−1
i(cid:48)=1

(cid:1) ((cid:80)n−1
i(cid:48)=1

· A2
A3

Li(cid:48)

Li(cid:48)

=

(cid:16)

:

.

2

µ
(cid:80)n−1
i(cid:48)=1

Li(cid:48)

We note that when n = 2, the asymptotic convergence
of AR-BCD coincides with the convergence of alternating
minimization (Beck, 2015). When nth block is empty (i.e.,
when all blocks are sampled with non-zero probability and
there is no exact minimization over a least-smooth block),
we obtain the convergence bound of the standard random-
ized coordinate descent method (Nesterov, 2012).

4. Accelerated AR-BCD

In this section, we show how to accelerate (AR-BCD) when
f (·) is smooth. We believe it is possible to obtain similar
results in the smooth and strongly convex case, which we
defer to a future version of the paper. Denote:

∆k = I ik

N ∇f (xk)/pik ,

aj (cid:104)∆j, u(cid:105)

vk = argmin

u

+

(cid:110) k
(cid:80)
j=1
n
σi
(cid:80)
2
i=1

(cid:107)ui − xi

1(cid:107)2(cid:111)
,

(4.1)

where σi > 0, ∀i, will be speciﬁed later. Accel-
is deﬁned as follows:
erated AR-BCD (AAR-BCD)

Proof. From Proposition 3.2 and Lemma 3.3, by linearity
of expectation and the deﬁnition of Gk:

Select ik from {1, . . . , n − 1} w.p. pik ,

E[f (xk+1)] − f (x∗) ≤ E[Gk] ≤

(cid:80)k

j=1 Ej
Ak

,

(3.2)

where Ej = aj
Aj

2

(cid:80)n−1
i=1

Li
2pi

Rxi

∗

.

Notice that the algorithm does not depend on the sequence
{aj} and thus we can choose it arbitrarily. Suppose that

ˆxk =

Ak−1
Ak

yk−1 +

vk−1,

ak
Ak
f (x),

xk = argmin
x∈Sn(ˆxk)
ak
I ik
N (vk − vk−1),
pik Ak

yk = xk +

x1 is an arbitrary initial point,

(AAR-BCD)

Alternating Randomized Block Coordinate Descent

(v1 − x1).

where (cid:80)n−1
i=1 pi = 1, pi > 0, ∀i ∈ {1, . . . , n − 1}, and
vk is deﬁned by (4.1). To seed the algorithm, we further
assume that y1 = x1 + I i1
1
N
pi1
Remark 4.1. Iteration complexity of (AAR-BCD) is domi-
nated by the computation of ˆxk, which requires updating an
entire vector. This type of an update is not unusual for accel-
erated block coordinate descent methods, and in fact appears
in all such methods we are aware of (Nesterov, 2012; Lee &
Sidford, 2013; Lin et al., 2014; Fercoq & Richt´arik, 2015;
Allen-Zhu et al., 2016). In most cases of practical interest,
however, it is possible to implement this step efﬁciently
(using that vk changes only over block ik in iteration k).
More details are provided in Appendix B.

To analyze the convergence of AAR-BCD, we will need
to construct a more sophisticated duality gap than in the
previous section, as follows.

4.1. Approximate Duality Gap

We deﬁne the upper bound to be Uk = f (yk). The con-
structed lower bound Lk from previous subsection is not
directly useful for the analysis of (AAR-BCD). Instead, we
will construct a random variable Λk, which in expectation
is upper bounded by f (x∗). The general idea, as in previ-
ous subsection, is to show that some notion of approximate
duality gap decreases in expectation.

Towards constructing Λk, we ﬁrst prove the following tech-
nical proposition, whose proof is in Appendix A.
Proposition 4.2. Let xk be as in (AAR-BCD). Then:

Adding and subtracting (deterministic) (cid:80)n−1
i=1
to/from (4.3) and using that:

k
(cid:80)
j=1

aj (cid:104)∆j, x∗ − xj(cid:105) +

n−1
(cid:80)
i=1

σi
2

(cid:107)xi

∗ − xi

1(cid:107)2

σi
2 (cid:107)xi

∗−xi

1(cid:107)2

aj (cid:104)∆j, u − xj(cid:105) +

(cid:110) k
(cid:80)
j=1
mk(u),

≥ min

u

= min

u

n−1
(cid:80)
i=1

σi
2

(cid:107)ui − xi

1(cid:107)2(cid:111)

j=1 aj (cid:104)∆j, u − xj(cid:105) + (cid:80)n−1

i=1

σi
2 (cid:107)ui −

where mk(u) = (cid:80)k
1(cid:107)2, it follows that:
xi
(cid:104) (cid:80)k

f (x∗) ≥ E

j=1 ajf (xj) − (cid:80)n−1

i=1

σi
2 (cid:107)xi

∗ − xi

1(cid:107)2

Ak
(cid:105)
minu∈RN mk(u)
Ak

,

+

which is equal to E[Λk], and completes the proof.

Similar as before, deﬁne the approximate gap as Γk =
Uk − Λk. Then, we can bound the initial gap as follows.
≤ σipi
Proposition 4.4. If a1 = a1
A1
Li
then E[A1Γ1] ≤ (cid:80)n−1
σi
2 (cid:107)x∗ − x1(cid:107)2.

, ∀i ∈ {1, ..., n − 1},

i=1

2

2

Proof. As a1 = A1 and y1 differs from x1 only over block
i = i1, by smoothness of f (·):

A1U1 = A1f (y1)

≤ a1f (x1) + a1

(cid:10)∇if (x1), yi

1 − xi
1

(cid:11) +

a1Li
2

(cid:107)yi

1 − xi

1(cid:107)2.

E[

k
(cid:80)
j=1

aj (cid:104)∆j, x∗ − xj(cid:105)] = E[

k
(cid:80)
j=1

aj (cid:104)∇f (xj), x∗ − xj(cid:105)].

On the other hand, the initial lower bound is:

Deﬁne the randomized lower bound as in Eq. (4.2), and ob-
serve that (4.1) deﬁnes vk as the argument of the minimum
from Λk. The crucial property of Λk is that it lower bounds
f (x∗) in expectation, as shown in the following lemma.
Lemma 4.3. Let xk be as in (AAR-BCD). Then f (x∗) ≥
E[Λk].

Ak

(cid:80)k

j=1 aj (f (˜xj )+(cid:104)∇f (˜xj ),x∗−˜xj (cid:105))

Proof. By convexity of f (·), for any sequence {˜xj} from
RN , f (x∗) ≥
. Since the
statement holds for any sequence {˜xj}, it also holds if {˜xj}
is selected according to some probability distribution. In
particular, for {˜xj} = {xj}:
(cid:104) (cid:80)k

j=1 aj(f (xj) + (cid:104)∇f (xj), x∗ − xj(cid:105))
Ak

f (x∗) ≥E

(cid:105)
.

By linearity of expectation and Proposition 4.2:

A1Λ1 =a1(f (x1) + (cid:104)∆1, v1 − x1(cid:105))
n−1
σi
(cid:80)
2
i=1

n−1
(cid:80)
i=1

1 − xi

1(cid:107)2 −

(cid:107)vi

+

σi
2

(cid:107)xi

∗ − xi

1(cid:107)2.

(vi

1 + 1
pi

1 = xi

Recall that yi
1 −xi
1). Using A1Γ1 = A1U1 −
A1Λ1 and the bounds on U1, Λ1 from the above: A1Γ1 ≤
(cid:80)n−1
, and, thus, E[A1Γ1] ≤
i=1
(cid:80)n−1
i=1

1(cid:107)2, as a1 ≤ pi
1(cid:107)2.

∗ −xi
∗ − xi

σi
2 (cid:107)xi
σi
2 (cid:107)xi

2 σi
Li

The next part of the proof is to show that AkΓk is a super-
martingale. The proof is provided in Appendix A.
2σi
≤ pi
Lemma 4.5. If ak
Ak
Li
E[AkΓk|Fk−1] ≤ Ak−1Γk−1.

, ∀i ∈ {1, . . . , n − 1}, then

2

Finally, we bound the convergence of (AAR-BCD).
Theorem 4.6. Let xk, yk evolve according to (AAR-BCD),
for ak
Ak

= const. Then, ∀k ≥ 1:

= min1≤i≤n−1

σipi
Li

2

2

(cid:104) (cid:80)k

f (x∗) ≥ E

j=1 aj(f (xj) + (cid:104)∆j, x∗ − xj(cid:105))
Ak

(cid:105)
.

(4.3)

E[f (yk)] − f (x∗) ≤

(cid:80)n−1

i=1 σi(cid:107)xi
2Ak

∗ − xi

a(cid:107)2

.

Alternating Randomized Block Coordinate Descent

(cid:80)k

j=1 ajf (xj) + minu∈RN

Λk =

(cid:110)(cid:80)k

j=1 aj (cid:104)∆j, u − xj(cid:105) + (cid:80)n−1

i=1

σi
2 (cid:107)ui − xi

1(cid:107)2(cid:111)

− (cid:80)n−1
i=1

Ak

σi
2 (cid:107)xi

∗ − xi

1(cid:107)2

.

(4.2)

In particular, if pi =

and a1 = 1, then:

√

Li
√

(cid:80)n−1
i(cid:48)=1

, σi = ((cid:80)n−1
i(cid:48)=1

√

Li(cid:48))2,

Li(cid:48)

E[f (yk)] − f (x∗) ≤

√

2((cid:80)n−1
i(cid:48)=1

Li(cid:48))2 (cid:80)n−1
k(k + 3)

i=1 (cid:107)xi

∗ − xi

1(cid:107)2

.

Alternatively, if pi = 1

n−1 , σi = Li, and a1 = 1

(n−1)2 :

E[f (yk)] − f (x∗) ≤

2(n − 1)2 (cid:80)n−1

i=1 Li(cid:107)xi
k(k + 3)

∗ − xi

1(cid:107)2

.

Proof. The ﬁrst part of the proof follows immediately by
applying Proposition 4.4 and Lemma 4.5. The second part
follows by plugging in the particular choice of parameters
and observing that aj grows faster than j+1
in the former,
2
and faster than

2(n−1)2 in the latter case.

j+1

√

Finally, we make a few remarks regarding Theorem 4.6. In
the setting without a non-smooth block (when nth block is
empty), (AAR-BCD) with sampling probabilities pi ∼
Li
has the same convergence bound as the NU ACDM algo-
rithm (Allen-Zhu et al., 2016) and the ALPHA algorithm for
smooth minimization (Qu & Richt´arik, 2016). Further, when
the sampling probabilities are uniform, (AAR-BCD) con-
verges at the same rate as the ACDM algorithm (Nesterov,
2012) and the APCG algorithm applied to non-composite
functions (Lin et al., 2014).

4.2. Accelerated Alternating Minimization

Before making speciﬁc remarks about accelerated alternat-
ing minimization (case n = 2), we ﬁrst note that the con-
vergence analysis of AAR-BCD applies generically even if
the step yk is replaced by exact minimization over block ik
(namely, if, instead of the current of choice of yk in AAR-
BCD we set yk = argminx∈Sik (xk) f (x)). This change is
relevant only at the beginning of the proof of Lemma 4.5,
and, to see that the same analysis still applies, observe that:

f (yk) = min

x∈Sik (xk)

f (x) ≤ f (xk+

ak
pik Ak

I ik
N (vk−vk−1)).

That is, replacing a particular step over block ik with the
exact minimization over the same block can only reduce the
function value, which can only make the upper bound Uk
(and, thus, the gap Γk) lower.

of the method with exact minimization over both blocks, it
simply sufﬁces to replace the yk step with the exact mini-
mization over the ﬁrst block, and, as already discussed, the
same analysis applies.

To obtain a method that is symmetric over blocks 1 and 2,
one only needs to replace the roles of blocks 1 and 2 in, say,
even iterations. Again, the same analysis applies, except
that in even iterations one would need to have ak
≤ σ2
,
Ak
L2
whereas in odd iterations it would still be ak
≤ σ1
;3 this
L1
Ak
affects the convergence bound in Theorem 4.6 by at most a
factor of 2. Observe further that when L2 → ∞, we have
ak → 0 in even iterations, and the method reduces to the
non-symmetric version obtained directly from AAR-BCD.

2

2

Finally, in the case of two blocks (n = 2), it is straight-
forward to obtain a parameter-free version of the method.
Indeed, all that is needed for the proof is that AkΓk ≤
Ak−1Γk−1 (as in Lemma 4.5. As discussed before, when
n = 2, there is no randomness in the algorithm. Suppose
that we want to implement a parameter-free version of the
symmetric method and consider odd iterations (which are
obtained as special cases of AAR-BCD for n = 2, with or
without the exact minimization in the yk-step). Then, from
Lemma 4.5, all that needs to be satisﬁed is that ak
≤ σ1
.
L1
Ak
Hence, if we ﬁnd a∗
k for which AkΓk ≤ Ak−1Γk−1, then
AkΓk ≤ Ak−1Γk−1 for any ak ≤ a∗
k. This is sufﬁcient for
implementing a backtracking line search over ak to ensure
AkΓk ≤ Ak−1Γk−1. To do so, one can use:

2

AkΓk−Ak−1Γk−1

=Akf (yk) − Ak−1f (yk−1) − akf (xk)

− ak

(cid:10)∇1f (xk), v1

k − x1
k

(cid:11) −

σ1
2

(cid:107)v1

k − v1

k−1(cid:107)2,

where we have used Uk = f (yk) (by the deﬁnition of Uk)
and the equivalent expression for AkΛk − Ak−1Λk−1 from
Eq. (A.4). As a practical matter, in even iterations, if L2
is very large and potentially approaching ∞, one can halt
the backtracking line search and set ak = 0 as soon as the
search reaches some preset “sufﬁciently small” value of ak.

5. Numerical Experiments

To illustrate the results, we solve the least squares problem
on the BlogFeedback Data Set (Buza, 2014) obtained from
UCI Machine Learning Repository (Lichman, 2013). The

Accelerated alternating minimization with exact minimiza-
tion over one block is immediately obtained from AAR-
BCD as a special case when n = 2. To obtain a version

3Observe that when n = 2, {pi} is supported on a single
element – block 1 is selected deterministically in even iterations;
block 2 is selected deterministically in odd iterations.

Alternating Randomized Block Coordinate Descent

(a) N/n = 5

(b) N/n = 10

(c) N/n = 20

(d) N/n = 40

(e) N/n = 5

(f) N/n = 10

(g) N/n = 20

(h) N/n = 40

(i) N/n = 5

(j) N/n = 10

(k) N/n = 20

(l) N/n = 40

Figure 1. Comparison of different block coordinate descent methods: (a)-(d) distribution of smoothness parameters over blocks, (e)-(h)
comparison of non-accelerated methods, and (i)-(l) comparison of accelerated methods. Block sizes N/n increase going left to right.

data set contains 280 attributes and 52,396 data points. The
attributes correspond to various metrics of crawled blog
posts. The data is labeled, and the labels correspond to the
number of comments that were posted within 24 hours from
a ﬁxed basetime. The goal of a regression method is to
predict the number of comments that a blog post receives.

What makes linear regression with least squares on this
dataset particularly suitable to our setting is that the smooth-
ness parameters of individual coordinates in the least squares
problem take values from a large interval, even when the
data matrix A is scaled by its maximum absolute value (the
values are between 0 and ∼354).4 The minimum eigenvalue
of AT A is zero (i.e., AT A is not a full-rank matrix), and
thus the problem is not strongly convex.

We partition the data into blocks as follows. We ﬁrst sort
the coordinates by their individual smoothness parameters.
Then, we group the ﬁrst N/n coordinates (from the sorted
list of coordinates) into the ﬁrst block, the second N/n
coordinates into the second block, and so on. The chosen
block sizes N/n are 5, 10, 20, 40, corresponding to n =

4We did not compare AR-BCD and AAR-BCD to other meth-
ods on problems with a non-smooth block (Ln = ∞), as no other
methods have any known theoretical guarantees in such a setting.

{56, 28, 14, 7} coordinate blocks, respectively.

The distribution of the smoothness parameters over blocks,
for all chosen block sizes, is shown in Fig. 1(a)-1(d). Ob-
serve that as the block size increases (going from left to
right in Fig. 1(a)-1(d)), the discrepancy between the two
largest smoothness parameters increases.

In all the comparisons between the different methods, we
deﬁne an epoch to be equal to n iterations (this would corre-
spond to a single iteration of a full-gradient method). The
graphs plot the optimality gap of the methods over epochs,
where the optimal objective value f ∗ is estimated via a
higher precision method and denoted by ˆf ∗. All the results
are shown for 50 method repetitions, with bold lines repre-
senting the median5 optimality gap over those 50 runs. The
norm used in all the experiments is (cid:96)2, i.e., (cid:107) · (cid:107) = (cid:107) · (cid:107)2.

Non-accelerated methods We ﬁrst compare AR-BCD
with a gradient step to RCDM (Nesterov, 2012) and stan-
dard cyclic BCD – C-BCD (see, e.g., (Beck & Tetruashvili,
2013)). To make the comparison fair, as AR-BCD makes

5We choose to show the median as opposed to the mean, as it is
well-known that in the presence of outliers the median is a robust
estimator of the true mean (Hampel et al., 2011).

Alternating Randomized Block Coordinate Descent

two steps per iteration, we slow it down by a factor of two
compared to the other methods (i.e., we count one iteration
of AR-BCD as two). In the comparison, we consider two
cases for RCDM and C-BCD: (i) the case in which these
two algorithms perform gradient steps on the ﬁrst n − 1
blocks and exact minimization on the nth block (denoted
by RCDM and C-BCD in the ﬁgure), and (ii) the case in
which the algorithms perform gradient steps on all blocks
(denoted by RCDM-G and C-BCD-G in the ﬁgure). The
sampling probabilities for RCDM and AR-BCD are propor-
tional to the block smoothness parameters. The permutation
for C-BCD is random, but ﬁxed in each method run.

Fig. 1(e)-1(h) shows the comparison of the described
for block sizes N/n ∈
non-accelerated algorithms,
{5, 10, 20, 40}. The ﬁrst observation to make is that adding
exact minimization over the least smooth block speeds up
the convergence of both C-BCD and RCDM, suggesting
that the existing analysis of these two methods is not tight.
Second, AR-BCD generally converges to a lower optimality
gap. While RCDM makes a large initial progress, it stag-
nates afterwards due to the highly non-uniform sampling
probabilities, whereas AR-BCD keeps making progress.

Accelerated methods Finally, we compare AAR-BCD
to NU ACDM (Allen-Zhu et al., 2016), APCG (Lin et al.,
2014), and accelerated C-BCD (ABCGD) from (Beck &
Tetruashvili, 2013). As AAR-BCD makes three steps per
iteration (as opposed to two steps normally taken by other
methods), we slow it down by a factor 1.5 (i.e., we count
one iteration of AAR-BCD as 1.5). We chose the sampling
probabilities of NU ACDM and AAR-BCD to be propor-
Li, while the sampling probabilities for APCG
tional to
are uniform6. Similar as before, each full run of ABCGD is
performed on a random but ﬁxed permutation of the blocks.

√

The results are shown in Fig. 1(i)-1(l). Compared to APCG
(and ABCGD), NU ACDM and AAR-BCD converge much
faster, which is expected, as the distribution of the smooth-
ness parameters is highly non-uniform and the meethods
with non-uniform sampling are theoretically faster by factor
n (Allen-Zhu et al., 2016). As the block size
of the order
is increased (going left to right), the discrepancy between
the smoothness parameters of the least smooth block and the
remaining blocks increases, and, as expected, AAR-BCD
exhibits more dramatic improvements compared to the other
methods.

√

6. Conclusion

tion AAR-BCD. Our work answers the open question
of (Beck & Tetruashvili, 2013) whether the convergence
of block coordinate descent methods intrinsically depends
on the largest smoothness parameter over all the blocks by
showing that such a dependence is not necessary, as long as
exact minimization over the least smooth block is possible.
Before our work, such a result only existed for the setting
of two blocks, using the alternating minimization method.

There are several research directions that merit further inves-
tigation. For example, we observed empirically that exact
optimization over the non-smooth block improves the per-
formance of RCDM and C-BCD, which is not justiﬁed by
the existing analytical bounds. We expect that in both of
these methods the dependence on the least smooth block
can be removed, possibly at the cost of a worse dependence
on the number of blocks. Further, AR-BCD and AAR-BCD
are mainly useful when the discrepancy between the largest
block smoothness parameter and the remaining smoothness
parameters is large, while under uniform distribution of the
smoothness parameters it can be slower than other methods
by a factor 1.5-2. It is an interesting question whether there
are modiﬁcations to AR-BCD and AAR-BCD that would
make them uniformly better than the alternatives.

7. Acknowledgements

Part of this work was done while the authors were visiting
the Simons Institute for the Theory of Computing. It was
partially supported by NSF grant #CCF-1718342 and by
the DIMACS/Simons Collaboration on Bridging Continu-
ous and Discrete Optimization through NSF grant #CCF-
1740425.

References

Allen-Zhu, Zeyuan, Qu, Zheng, Richt´arik, Peter, and Yuan,
Yang. Even faster accelerated coordinate descent using
non-uniform sampling. In Proc. ICML’16, 2016.

Beck, Amir. On the convergence of alternating minimization
for convex programming with applications to iteratively
reweighted least squares and decomposition schemes.
SIAM J. Optimiz., 25(1):185–209, 2015.

Beck, Amir and Tetruashvili, Luba. On the convergence of
block coordinate descent type methods. SIAM J. Optimiz.,
23(4):2037–2060, 2013.

Bertsekas, Dimitri P. Nonlinear programming. Athena

scientiﬁc Belmont, 1999.

We presented a novel block coordinate descent algorithm
AR-BCD and its accelerated version for smooth minimiza-

6The theoretical results for APCG were only presented for

uniform sampling (Lin et al., 2014).

Boyd, Stephen and Vandenberghe, Lieven. Convex optimiza-

tion. Cambridge university press, 2004.

Bubeck, S´ebastien.

Theory of Convex Optimization

Alternating Randomized Block Coordinate Descent

for Machine Learning.
arXiv:1405.4980v1.

2014.

arXiv preprint,

Buza, Krisztian. Feedback prediction for blogs. In Data
analysis, machine learning and knowledge discovery, pp.
145–152. Springer, 2014.

Diakonikolas, Jelena and Orecchia, Lorenzo. The approxi-
mate duality gap technique: A uniﬁed theory of ﬁrst-order
methods, 2017. arXiv preprint, arXiv:1712.02485.

Fercoq, Olivier and Richt´arik, Peter. Accelerated, parallel,
and proximal coordinate descent. SIAM J. Optimiz., 25
(4):1997–2023, 2015.

Gower, Robert Mansel and Richt´arik, Peter. Stochastic
dual ascent for solving linear systems. arXiv preprint
arXiv:1512.06890, 2015.

Guminov, Sergey, Dvurechensky, Pavel, and Gasnikov,
Alexander. Accelerated alternating minimization. arXiv
preprint arXiv:1906.03622, 2019.

Hampel, Frank R, Ronchetti, Elvezio M, Rousseeuw, Peter J,
and Stahel, Werner A. Robust statistics: the approach
based on inﬂuence functions, volume 196. John Wiley &
Sons, 2011.

Lee, Yin Tat and Sidford, Aaron. Efﬁcient accelerated
coordinate descent methods and faster algorithms for
solving linear systems. In Proc. IEEE FOCS’13, 2013.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Nesterov, Yurii and Stich, Sebastian U. Efﬁciency of the
accelerated coordinate descent method on structured op-
timization problems. SIAM J. Optimiz., 27(1):110–123,
2017.

Ortega, James M and Rheinboldt, Werner C. Iterative so-
lution of nonlinear equations in several variables, vol-
ume 30. SIAM, 1970.

Qu, Zheng and Richt´arik, Peter. Coordinate descent with
arbitrary sampling i: Algorithms and complexity. Opti-
mization Methods and Software, 31(5):829–857, 2016.

Qu, Zheng, Richt´arik, Peter, Tak´aˇc, Martin, and Fercoq,
Olivier. SDNA: Stochastic dual Newton ascent for empir-
ical risk minimization. In Proc. ICML’16, 2016.

Richt´arik, Peter and Tak´aˇc, Martin. Iteration complexity of
randomized block-coordinate descent methods for min-
imizing a composite function. Math. Prog., 144(1-2):
1–38, 2014.

Saha, Ankan and Tewari, Ambuj. On the nonasymptotic
convergence of cyclic coordinate descent methods. SIAM
J. Optimiz., 23(1):576–601, 2013.

Strohmer, Thomas and Vershynin, Roman. A Randomized
Kaczmarz Algorithm with Exponential Convergence. J.
Fourier Anal. Appl., 15(2):262–278, 2009.

Sun, Ruoyu and Hong, Mingyi. Improved iteration complex-
ity bounds of cyclic block coordinate descent for convex
problems. In Proc. NIPS’15, 2015.

Lin, Qihang, Lu, Zhaosong, and Xiao, Lin. An accelerated
proximal coordinate gradient method. In Proc. NIPS’14,
2014.

Tseng, Paul and Yun, Sangwoon. A coordinate gradient
descent method for nonsmooth separable minimization.
Math. Prog., 117(1-2):387–423, 2009.

Nesterov, Yu. Efﬁciency of coordinate descent methods on
huge-scale optimization problems. SIAM J. Optimiz., 22
(2):341–362, 2012.

Wright, Stephen J. Coordinate descent algorithms. Math.

Prog., 151(1):3–34, 2015.

A. Omitted Proofs from Section 4

Alternating Randomized Block Coordinate Descent

Proof of Proposition 4.2. Let Fk−1 be the natural ﬁltration up to iteration k − 1. Observe that, as ∇nf (xk) = 0:

E[∆k|Fk−1] = ∇f (xk).

Since x1 is deterministic (ﬁxed initial point) and the only random variable ∆1 depends on is i1, we have:

E[a1 (cid:104)∆1, x∗ − x1(cid:105)] = a1 (cid:104)∇f (x1), x∗ − x1(cid:105)

= E[a1 (cid:104)∇f (x1), x∗ − x1(cid:105)].

(A.1)

(A.2)

Let k > 1. Observe that aj (cid:104)∆j, x∗ − xj(cid:105) is measurable with respect to Fk−1 for j ≤ k − 1. By linearity of expectation,
using (A.1):

E[

k
(cid:80)
j=1

aj (cid:104)∆j, x∗ − xj(cid:105) |Fk−1] = ak (cid:104)∇f (xk), x∗ − xk(cid:105) +

k−1
(cid:80)
j=1

aj (cid:104)∆j, x∗ − xj(cid:105) .

Taking expectations on both sides of the last equality gives a recursion on E[(cid:80)k
with (A.2), completes the proof.

j=1 aj (cid:104)∆j, x∗ − xj(cid:105)], which, combined

Proof of Lemma 4.5. As Ak−1Γk−1 is measurable with respect to the natural ﬁltration Fk−1, E[AkΓk|Fk−1] ≤ Ak−1Γk−1
is equivalent to E[AkΓk − Ak−1Γk−1|Fk−1] ≤ 0.

The change in the upper bound is:

AkUk − Ak−1Uk−1 = Ak(f (yk) − f (xk)) + Ak−1(f (xk) − f (yk−1)) + akf (xk).

By convexity, f (xk) − f (yk−1) ≤ (cid:104)∇f (xk), xk − yk−1(cid:105). Further, as yk = xk + I ik
N
(cid:68)
2
+ Lik ak
∇f (xk), I ik
N
2Ak
2pik

smoothness of f (·), that f (yk) − f (xk) ≤

(vk − vk−1)

ak
pik Ak

(cid:69)

ak
pik Ak
2 (cid:107)vik

(vk − vk−1), we have, by
k − vik

k−1(cid:107)2. Hence:

AkUk − Ak−1Uk−1

(cid:28)

≤ akf (xk) +

∇f (xk), Ak−1(xk − yk−1) + I ik
N

(cid:29)

(vk − vk−1)

+

ak
pik

2

Lik ak
2pik

2Ak

(cid:107)vik

k − vik

k−1(cid:107)2.

(A.3)

Let mk(u) = (cid:80)k
i=1
Λk. Observe that mk(u) = mk−1(u) + ak (cid:104)∆k, u − xk(cid:105) and vk = argminu mk(u). Then:

j=1 aj (cid:104)∆j, u − xj(cid:105) + (cid:80)n

σi
2 (cid:107)ui − xi

1(cid:107)2 denote the function under the minimum in the deﬁnition of

mk−1(vk) =mk−1(vk−1) + (cid:104)∇mk−1(vk−1), vk − vk−1(cid:105) +

=mk−1(vk−1) +

σik
2

(cid:107)vik

k − vik

k−1(cid:107)2,

n−1
(cid:80)
i=1

σi
2

(cid:107)vi

k − vi

k−1(cid:107)2

as vk and vk−1 only differ over the block ik and vk−1 = argminu mk−1(u) (and, thus, ∇mk−1(vk−1) = 0).
Hence, it follows that mk(vk) − mk−1(vk−1) = ak (cid:104)∆k, vk − xk(cid:105) + σik

k−1(cid:107)2, and, thus:

2 (cid:107)vik

AkΛk − Ak−1Λk−1 = akf (xk) + ak (cid:104)∆k, vk − xk(cid:105) +

(cid:107)vik

k − vik

k−1(cid:107)2.

(A.4)

k − vik
σik
2

Combining (A.3) and (A.4):

AkΓk − Ak−1Γk−1 ≤

(cid:28)

∇f (xk), Ak−1(xk − yk−1) + I ik
N

(cid:29)

(vk − vk−1)

− ak (cid:104)∆k, vk − xk(cid:105)

ak
pik

(cid:107)vik

k − vik

k−1(cid:107)2 −

σik
2

(cid:107)vik

k − vik

k−1(cid:107)2

2

Lik ak
2pik
(cid:28)

2Ak

+

≤

∇f (xk), Ak−1(xk − yk−1) + I ik
N

(cid:29)

(vk − vk−1)

− ak (cid:104)∆k, vk − xk(cid:105) ,

ak
pik

Alternating Randomized Block Coordinate Descent

as, by the initial assumptions, ak
Ak

2

≤

σik

p2
ik
Lik

.

Finally, taking expectations on both sides, and as xk, yk−1, vk−1 are all measurable w.r.t. Fk−1 and by the separability of
the terms in the deﬁnition of vk:

E[AkΓk − Ak−1Γk−1|Fk−1] ≤ (cid:104)∇f (xk), Akxk − Ak−1yk−1 − akvk−1(cid:105) = 0,

as, from (AAR-BCD), xk = ˆxk = Ak−1
Ak
as xk is the minimizer of f over block n, when other blocks in ˆxk are ﬁxed.7

yk−1 + ak
Ak

vk−1 over all the blocks except for the block n, while ∇nf (xk) = 0,

B. Efﬁcient Implementation of AAR-BCD Iterations

Using similar ideas as in (Fercoq & Richt´arik, 2015; Lin et al., 2014; Lee & Sidford, 2013), here we discuss how to
efﬁciently implement iterations of AAR-BCD, without requiring full-vector updates. First, due to the separability of the
terms inside the minimum, between successive iterations vk changes only over a single block. This is formalized in the
following simple proposition.
Proposition B.1. In each iteration k ≥ 1, vi

k−1, ∀i (cid:54)= ik and vik

k−1 + wik , where:

k = vik

k = vi

wik = argmin

{ak

uik

(cid:10)∆ik

k , u(cid:11) +

σik
2

(cid:107)uik − vik

k−1(cid:107)2}.

Proof. Recall the deﬁnition of vk. We have:

vk = argmin

u

= argmin
u

= argmin
u

(cid:110) k
(cid:88)

j=1
(cid:110) k−1
(cid:88)

j=1
(cid:110) k−1
(cid:88)

j=1

(cid:104)∆j, u(cid:105) +

n−1
(cid:88)

i=1

σi
2

(cid:107)ui − xi

1(cid:107)2(cid:111)

(cid:104)∆j, u(cid:105) + (cid:104)∆k, u(cid:105) +

n−1
(cid:88)

i=1

σi
2

(cid:107)ui − xi

1(cid:107)2(cid:111)

(cid:104)∆j, u(cid:105) + (cid:10)∆ik

k , uik (cid:11) +

n−1
(cid:88)

i=1

σi
2

(cid:107)ui − xi

1(cid:107)2(cid:111)

=vk−1 + argmin

uik

(cid:110) (cid:10)∆ik

k , uik (cid:11) +

σik
2

(cid:107)uik − vik

k−1(cid:107)2(cid:111)
,

where the third equality is by the deﬁnition of ∆k (∆i
of the terms under the min.

k = 0 for i (cid:54)= ik) and the last equality follows from block-separability

Since vk only changes over a single block, this will imply that the changes in xk and yk can be localized. In particular, let
us observe the patterns in changes between successive iterations. We have that, ∀i (cid:54)= n :

xi

k =

Ak−1
Ak

yi

k−1 +

ak
Ak

vi

k−1 =

Ak−1
Ak

(cid:0)yi

k−1 − vi

k−1

(cid:1) + vi

k−1

and

k = xi
yi

k +

=

Ak−1
Ak

1
ak
pi
Ak
(cid:0)yi

(cid:0)vi

k − vi

k−1

(cid:1)

k−1 − vi

k−1

(cid:18)

1 −

(cid:1) +

(cid:19)

1
pi

ak
Ak

(cid:0)vi

k−1 − vi
k

(cid:1) + vi
k.

(B.1)

(B.2)

Due to Proposition B.1, vk and vk−1 can be computed without full-vector operations (assuming the gradients can be
computed without full-vector operations, which we will show later in this section). Hence, we need to show that it is possible

7Previous version of the proof provided an incomplete justiﬁcation for the last expression in the proof being equal to zero; we thank

Sergey Guminov for pointing this out.

Alternating Randomized Block Coordinate Descent

to replace Ak−1
Ak
y0 − v0 = 0 (from the initialization of (AAR-BCD)) and that, from (B.2):

k−1 − vi

k−1

(cid:0)yi

(cid:1) with a quantity that can be computed without the full-vector operations. Observe that

k − vi
yi

k =

Ak−1
Ak

(cid:0)yi

k−1 − vi

k−1

(cid:18)

(cid:1) +

1 −

(cid:19)

1
pi

ak
Ak

(cid:0)vi

k−1 − vi
k

(cid:1) .

Dividing both sides by ak
Ak

2 and assuming that ak
Ak

2

2

is constant over iterations, we get:

2

Ak
2
ak

(cid:0)yi

k − vi
k

(cid:1) =

2

Ak−1
2
ak−1

(cid:0)yi

k−1 − vi

k−1

(cid:1) +

2

Ak
2
ak

(cid:18)

1 −

(cid:19)

1
pi

ak
Ak

(cid:0)vi

k−1 − vi
k

(cid:1) .

(B.3)

Let Nn denote the size of the nth block and deﬁne the (N − Nn)-length vector uk by ui
ak
k−1 + Ak
k−1 − vi
Then (from (B.3)) ui
k
Ak
ak
Combining with (B.1) and (B.2), we have the following lemma.

(cid:1), ∀i (cid:54)= n.
(cid:1) , and, hence, in iteration k, uk changes only over block ik.

k = Ak
ak

1 − 1
pi

k = ui

k − vi
k

(cid:17) (cid:0)vi

(cid:0)yi

(cid:16)

2

2

2

2

Lemma B.2. Assume that ak
Ak
deﬁned recursively as u0 = 0, ui

2

Then, ∀i ∈ {1, ..., n − 1}: xi

is kept constant over the iterations of AAR-BCD. Let uk be the (N − Nn)-dimensional vector
(cid:1).
k = ui
2 ui

k−1 for i ∈ {1, ..., n − 1}, i (cid:54)= ik and uik
k−1 + vi

k = uik
k−1 + Ak
ak
(cid:17) (cid:0)vi
ak
1 − 1
k−1 − vi
k
Ak
pi

ak
1 − 1
pi
Ak
(cid:1) + vi
k.

k−1 and yi

k−1 − vi
k

k = ak
Ak

(cid:17) (cid:0)vi

k−1 +

2 ui

(cid:16)

(cid:16)

2

2

2

2

k = ak
Ak

Note that we will never need to explicitly compute xk, yk, except for the last iteration K, which outputs yK. To formalize
this claim, we need to show that we can compute the gradients ∇if (xk) without explicitly computing xk and that we can
efﬁciently perform the exact minimization over the nth block. This will only be possible by assuming speciﬁc structure of
the objective function, as is typical for accelerated block-coordinate descent methods (Fercoq & Richt´arik, 2015; Lee &
Sidford, 2013; Lin et al., 2014). In particular, we assume that for some m × N dimensional matrix M :

f (x) =

m
(cid:88)

j=1

φj(eT

j Mx) + ψ(x),

(B.4)

where φj : R → R and ψ = (cid:80)n

i=1 ψi : RN → R is block-separable.

Efﬁcient Gradient Computations. Assume for now that xn
k can be computed efﬁciently (we will address this at the end
of this section). Let ind denote the set of indices of the coordinates from blocks {1, 2, ..., n − 1} and denote by B the
matrix obtained by selecting the columns of M that are indexed by ind. Similarly, let indn denote the set of indices of the
coordinates from block n and let C denote the submatrix of M obtained by selecting the columns of M that are indexed by
indn. Denote ruk = Buk, rvk = B[v1
]T , rn = Cxn
k . Let indik be the set of indices corresponding to the
coordinates from block ik. Then:

k, ..., vn−1

k, v2

k

∇ik f (xk) =

m
(cid:88)

(Mj,indik )T φ(cid:48)
j

j=1

2

(cid:18) ak
Ak

2 rj

uk−1

(cid:19)

+ rj

vk−1

+ rj
n

+ ∇ik ψ(x).

(B.5)

Hence, as long as we maintain ruk , rvk , and rn (which do not require full-vector operations), we can efﬁciently compute
the partial gradients ∇ik f (xk) without ever needing to perform any full-vector operations.

Efﬁcient Exact Minimization. Suppose ﬁrst that ψ(x) ≡ 0. Then:

rn = argmin
r∈Rm




m
(cid:88)



j=1

φj

2

(cid:18) ak
Ak

2 rj

uk−1

+ rj

vk−1

+ rj




(cid:19)



,

and rn can be computed but solving m single-variable minimization problems, which can be done in closed form or with a
very low complexity. Computing rn is sufﬁcient for deﬁning all algorithm iterations, except for the last one (that outputs a
solution). Hence, we only need to compute xn
k once – in the last iteration.

Alternating Randomized Block Coordinate Descent

More generally, xn

k is determined by solving:

xn

k = argmin
x∈RNn




m
(cid:88)



j=1

φj

2

(cid:18) ak
Ak

2 rj

uk−1

+ rj

vk−1

+ (Cx)j

(cid:19)

+ ψn(x)






.

When m and Nn are small, high-accuracy polynomial-time convex optimization algorithms are computationally inexpensive,
and xn

k can be computed efﬁciently.

In the special case of linear and ridge regression, xn
if b is the vector of labels, then the problem becomes:

k can be computed in closed form, with minor preprocessing. In particular,

xn

k = argmin
x∈RNn




m
(cid:88)



j=1

2

(cid:18) ak
Ak

2 rj

uk−1

+ rj

vk−1

+ (Cx)j − bj

(cid:19)2

+

λ
2

(cid:107)x(cid:107)2
2






,

where λ = 0 in the case of (simple) linear regression. Let b(cid:48) = b − ak
Ak

2

2 ruk−1 − rvk−1. Then:

k = (CT C + λI)†(CT b(cid:48)),
xn

where (·)† denotes the matrix pseudoinverse, and I is the identity matrix. Since CT C + λI does not change over iterations,
(CT C + λI)† can be computed only once at the initialization. Recall that CT C + λI is an Nn × Nn matrix, where Nn is
the size of the nth block, and thus inverting CT C + λI is computationally inexpensive as long as Nn is not too large. This
reduces the overall per-iteration cost of the exact minimization to about the same cost as for performing gradient steps.

