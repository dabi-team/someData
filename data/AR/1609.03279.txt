IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

1

Semi-Supervised Sparse Representation Based
Classiﬁcation for Face Recognition with Insufﬁcient
Labeled Samples

Yuan Gao, Jiayi Ma, and Alan L. Yuille Fellow, IEEE

7
1
0
2

r
a

M
4
2

]

V
C
.
s
c
[

2
v
9
7
2
3
0
.
9
0
6
1
:
v
i
X
r
a

Abstract—This paper addresses the problem of face recognition
when there is only few, or even only a single, labeled examples
of the face that we wish to recognize. Moreover, these examples
are typically corrupted by nuisance variables, both linear (i.e.
additive nuisance variables such as bad lighting, wearing of
glasses) and non-linear (i.e. non-additive pixel-wise nuisance
variables such as expression changes). The small number of
labeled examples means that it is hard to remove these nuisance
variables between the training and testing faces to obtain good
recognition performance. To address the problem we propose
a method called Semi-Supervised Sparse Representation based
Classiﬁcation (S3RC). This is based on recent work on sparsity
where faces are represented in terms of two dictionaries: a gallery
dictionary consisting of one or more examples of each person,
and a variation dictionary representing linear nuisance variables
(e.g. different lighting conditions, different glasses). The main
idea is that (i) we use the variation dictionary to characterize
the linear nuisance variables via the sparsity framework, then
(ii) prototype face images are estimated as a gallery dictionary
via a Gaussian Mixture Model (GMM), with mixed labeled and
unlabeled samples in a semi-supervised manner, to deal with the
non-linear nuisance variations between labeled and unlabeled
samples. We have done experiments with insufﬁcient labeled
samples, even when there is only a single labeled sample per
person. Our results on the AR, Multi-PIE, CAS-PEAL, and LFW
databases demonstrate that the proposed method is able to deliver
signiﬁcantly improved performance over existing methods.

Index Terms—Gallery dictionary learning, semi-supervised
learning, face recognition, sparse representation based classiﬁ-
cation, single labeled sample per person.

I. INTRODUCTION

F ACE Recognition is one of the most fundamental prob-

lems in computer vision and pattern recognition. In the
past decades, it has been extensively studied because of its
wide range of applications, such as automatic access con-
trol system, e-passport, criminal recognition, to name just a
few. Recently, the Sparse Representation based Classiﬁcation

The authors would like to thank Weichao Qiu and Mingbo Zhao for giving
feedbacks on the manuscript. This work was supported in part by the National
Natural Science Foundation of China under Grant 61503288, in part by the
China Postdoctoral Science Foundation under Grant 2016T90725, and in part
by the NSF award CCF-1317376. (Corresponding author: Jiayi Ma.)

Y. Gao is with the Electronic Information School, Wuhan University, Wuhan
430072, China, and also with the Tencent AI Laboratory, Shenzhen 518057,
China. Email: Ethan.Y.Gao@gmail.com.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. Email: jyma2010@gmail.com.

A. Yuille is with the Department of Statistics, University of California at
Los Angeles, Los Angeles, CA 90095 USA, and also with the Department of
Cognitive Science and the Department of Computer Science, Johns Hopkins
University, Baltimore, MD 21218 USA. Email: yuille@stat.ucla.edu.

(SRC) method, introduced by Wright et al. [1], has received
a lot of attention for face recognition [2]–[5]. In SRC, a
sparse coefﬁcient vector was introduced in order to represent
the test image by a small number of training images. Then
the SRC model was formulated by jointly minimizing the
reconstruction error and the (cid:96)1-norm on the sparse coefﬁcient
vector [1]. The main advantages of SRC have been pointed out
in [1], [6]: i) it is simple to use without carefully crafted feature
extraction, and ii) it is robust to occlusion and corruption.

One of the most challenging problems for practical face
recognition application is the shortage of labeled samples [7].
This is due to the high cost of labeling training samples by
human effort, and because labeling multiple face instances
may be impossible in some cases. For example, for terrorist
recognition, there may be only one sample of the terrorist, e.g.
his/her ID photo. As a result, nuisance variables (or so called
intra-class variance) can exist between the testing images and
the limited amount of training images, e.g. the ID photo of the
terrorist (the training image) is a standard front-on face with
neutral lighting, but the testing images captured from the crime
scene can often include bad lighting conditions and/or various
occlusions (e.g. the terrorist may wear a hat or sunglasses).
In addition, the training and testing images may also vary in
expressions (e.g. neutral and smile) or resolution. The SRC
methods may fail in these cases because of the insufﬁciency
of the labeled samples to model nuisance variables [8]–[12].
In order to address the insufﬁcient labeled samples problem,
Extended SRC (ESRC) [13] assumed that a testing image
equals a prototype image plus some (linear) variations. For
example, a image with sunglasses is assumed to equal to
the image without sunglasses plus the sunglasses. Therefore,
ESRC introduced two dictionaries: (i) a gallery dictionary con-
taining the prototype of each person (these are the persons to
be recognized), and (ii) a variation dictionary which contains
nuisance variations that can be shared by different persons
(e.g. different persons may wear the same sunglasses). Recent
improvements on ESRC can give good results for this problem
even when the subject only has a single labeled sample
(namely the Single Labeled Sample Per Person problem, i.e.
SLSPP) [14]–[17].

However, various non-linear nuisance variables also exist
in human face images, which makes prototype images hard
to obtain. In other words, the nuisance variables often occur
pixel-wise, which are not additive and cannot shared by dif-
ferent persons. For example, we cannot simply add a speciﬁc
variation to a neutral image (i.e. the labeled training image)

 
 
 
 
 
 
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

2

Fig. 1. Comparisons of the gallery dictionaries estimated by SSRC (i.e. the
mean of the labeled data) and our method (i.e. one Gaussian centroid of GMM
by semi-supervised EM initialized by the labeled data mean) using ﬁrst 300
Principal Components (PCs, dimensional reduction by PCA). This illustrates
that our method can estimate a better gallery dictionary with very few labeled
images which contains both linear (i.e. occlusion) and non-linear (i.e. smiling)
variations. The gallery from our method is learned by 5 semi-supervised EM
iterations.

to get its smile images (i.e. the testing images). Therefore,
the limited number of training images may not yield a good
prototype to represent the testing images, especially when
non-linear variations exist between them. Attempts were to
learn the gallery dictionary (i.e. better prototype images) in
Superposed SRC (SSRC) [18]. However, it requires multiple
labeled samples per subject, and still used simple linear
operations (i.e. averaging the labeled faces w.r.t each subject)
to get the gallery dictionary.

In this paper, we propose a probabilistic framework called
Semi-Supervised Sparse Representation based Classiﬁcation
(S3RC) to deal with the insufﬁcient labeled sample problem
in face recognition, even when there is only one labeled sample
per person. Both linear and non-linear variations between the
training labeled and the testing samples are considered. We
deal with the linear variations by a variation dictionary. After
eliminated the linear variation (by simple subtraction), the
non-linear variation is addressed by pursuing a better gallery
dictionary (i.e. better prototype images) via a Gaussian Mix-
ture Model (GMM). Speciﬁcally, in our proposed S3RC, the
testing samples (without label information) are also exploited
to learn a better model (i.e. better prototype images) in a
semi-supervised manner to eliminate the non-linear variation
between the labeled and unlabeled samples. This is because the
labeled samples are insufﬁcient, and exploiting the unlabeled
samples ensures that the learned gallery (i.e. the better pro-
totype) can well represent the testing samples and give better
results. An illustrative example which compares the prototype
image learned from our method and the existing SSRC is given
in Fig. 1. Clearly from Fig.1, we can see that, with insufﬁcient
labeled samples, a better gallery dictionary is learned by S3RC
that can well address the non-linear variations. Also Figs. 8
and 12 in the later sections show that the learned gallery
dictionary of our method can well represent the testing images
for better recognition results.

In brief, since the linear variations can be shared by different
persons (e.g. different persons can wear the same sunglasses),
therefore, we model the linear variations by a variation dictio-
nary, where the variation dictionary is constructed by a large
pre-labeled database which is independent of the training or
testing. Then, we rectify the data to eliminate linear variations
using the variation dictionary. After that, a GMM is applied to
the rectiﬁed data, in order to learn a better gallery dictionary

that can well represent the testing data which contains non-
linear variation from the labeled training. Speciﬁcally, all
the images from the same subject are treated as a Gaussian
with its Gaussian mean as a better gallery. Then, the GMM
is optimized to get the mean of each Gaussian using the
semi-supervised Expectation-Maximization (EM) algorithm,
initialized from the labeled data, and treating the unknown
class assignment of the unlabeled data as the latent variable.
Finally, the learned Gaussian means are used as the gallery
dictionary for sparse representation based classiﬁcation. The
major contributions of our model are:

• Our model can deal with both linear and non-linear
variations between the labeled training and unlabeled
testing samples.

• A novel gallery dictionary learning method is proposed
which can exploit the unlabeled data to deal with the
non-linear variations.

• Existing variation dictionary learning methods are com-
plementary to our method, i.e. our method can be applied
to other variation dictionary learning method to achieve
improved performance.

The rest of the paper is organized as follows. We ﬁrst sum-
marize the notation and terminology in the next subsection.
Section II describes background material and related work.
SSRC and ESRC are described in Section III. In Section
IV, starting with the insufﬁcient training samples problem,
we introduce the proposed S3RC model, discuss the EM
optimization, and then we extend S3RC to the SLSPP problem.
Extensive simulations have been conducted in Section V,
where we show that by using our method as a classiﬁer,
further improved performance can be achieved using Deep
Convolution Neural Network (DCNN) features. Section VI
discusses the experimental results, and is followed by con-
cluding remarks in Section VII.

A. Summary of notation and terminology

In this paper, capital bold and lowercase bold symbols are
used to represent matrices and vectors, respectively. 1d ∈
Rd×1 denotes the unit column vector, and I is the identity
matrix. || · ||1, || · ||2, || · ||F denote the (cid:96)1, (cid:96)2, and Frobenius
norms, respectively. ˆa is the estimation of parameter a.

In the following, we demonstrate the promising performance
of our method on two problems with strictly limited labeled
data: i) the insufﬁcient uncontrolled gallery samples problem
without generic training data, and ii) the SLSPP problem
with generic training data. Here, uncontrolled samples are
images containing nuisance variables such as different illu-
mination, expression, occlusion, etc. We call these nuisance
variables as intra-class variance in the rest of the paper. The
generic training dataset is an independent dataset w.r.t the
training/testing dataset. It contains multiple samples per person
to represent the intra-class variance. In the following, we use
the insufﬁcient training samples problem to refer to the former
problem, and the SLSPP problem is short for the latter one. We
do not distinguish the terms training/gallery/labeled samples,
testing/unlabeled samples in the following. But note that the

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

3

gallery samples and gallery dictionary are not identical. The
latter means the learned dictionary for recognition.

The promising performance of our method is obtained
by estimating the prototype of each person as the gallery
dictionary, and the prototype is estimated using both labeled
and unlabeled data. Here, the prototype means a learned image
that represents the discriminative features of all the images
from a speciﬁc subject. There is only one prototype for each
subject. Typically, the prototype can be the neutral image of a
speciﬁc subject without occlusion and obtained under uniform
illumination. Our method learn the prototype by estimating
the true centroid for both labeled and unlabeled data of each
person, thus we do not distinguish the prototype and true
centroid in the following.

II. RELATED WORK
The proposed method is a Sparse Representation based
Classiﬁcation (SRC) method. Many research works have been
inspired by the original SRC method [1]. In order to learn a
more discriminative dictionary, instead of using the training
data itself, Yang et al. introduced the Fisher discrimination
criterion to constrain the sparse code in the reconstructed error
[19], [20]. Ma et al. learned another discriminative dictionary
by imposing low-rank constraints on it [21]. Following these
approaches, a model unifying [19] and [21] was proposed
by Li et al. [22], [23]. Alternatively, Zhang et al. proposed
a model to indirectly learn the discriminative dictionary by
constraining the coefﬁcient matrix to be low-rank [24]. Chi
and Porikli incorporated SRC and Nearest Subspace Classi-
ﬁer (NSC) into a uniﬁed framework, and balanced them by
a regularization parameter [25]. However, this category of
methods need sufﬁcient samples of each subject to construct
an over-complete dictionary for modeling the variations of the
uncontrolled samples [8]–[10], and hence is not suitable for the
insufﬁcient training samples problem and the SLSPP problem.
Recently, ESRC was proposed to address the limitations of
SRC when the number of samples per class is insufﬁcient
to obtain an over-complete dictionary, where a variation dic-
tionary is introduced to represent the linear variation [13].
Motivated by ESRC, Yang et al. proposed the Sparse Variation
Dictionary Learning (SVDL) model
to learn the variation
dictionary V, more precisely [14]. In addition to modeling the
variation dictionary by a linear illumination model, Zhuang et
al. [15], [16] also integrated auto-alignment into their method.
Gao et al. [26] extended the ESRC model by dividing the
image samples into several patches for recognition. Wei and
Wang proposed robust auxiliary dictionary learning to learn
the intra-class variation [17]. The aforementioned methods did
not learn a better gallery dictionary to deal with non-linear
variation, therefore good prototype images (i.e. the gallery
dictionary) were hard to obtain. To address this issue, Deng et
al. proposed SSRC to learn the prototype images as the gallery
dictionary [18]. But this uses only simple linear operations
to estimate the gallery dictionary, which requires sufﬁcient
labeled gallery samples and it is still difﬁcult to model the
non-linear variation.

There are semi-supervised learning (SSL) methods which
use sparse/low-rank techniques. For example, Yan and Wang

[27] used sparse representation to construct the weight of the
pairwise relationship graph for SSL. He et al. [28] proposed a
nonnegative sparse algorithm to derive the graph weights for
graph-based SSL. Besides the sparsity property, Zhuang et al.
[29], [30] also imposed low-rank constraints to estimate the
weight matrix of the pairwise relationship graph for SSL. The
main difference between them and our proposed method S3RC
is that the previous works used sparse/low-rank technologies
to learn the weight matrix for graph-based SSL, which are
essentially SSL methods. By contrast our method aims at
learning a precise gallery dictionary in the ESRC framework,
and the gallery dictionary learning was assisted by probability-
based SSL (GMM), which is essentially a SRC method.
Also note that as a general tool, GMM has been used for
face recognition for a long time since Wang and Tang [31].
However, to the best of our knowledge, GMM has not been
previously used for gallery dictionary learning in SRC based
face recognitions.

III. SEMI-SUPERVISED SPARSE REPRESENTATION BASED
CLASSIFICATION WITH EM ALGORITHM

In this section, we present our proposed S3RC method in
detail. Firstly, we introduce the general SRC formulation with
the gallery plus variation framework, in which the linear vari-
ation is directly modeled by the variation dictionary. Then, we
prove that, after eliminating linear variations of each sample
(which we call rectiﬁcation), the rectiﬁed data (both labeled
and unlabeled) from one person can be modeled as a Gaussian
to learn the non-linear variations. Following this, the whole
rectiﬁed dataset including both labeled and unlabeled samples
are formulated by a GMM. Next, initialized by the labeled
data, the semi-supervised EM algorithm is used to learn the
mean of each Gaussian as the prototype images. Then, the
learned gallery dictionary is used for face recognition by the
gallery plus variation framework. After that, we describe the
way to apply S3RC to the SLSPP problem. Finally, the overall
algorithm is summarized.

We use the gallery plus variation framework to address
both linear and non-linear variations. Speciﬁcally, the linear
variation (such as illumination changes, different occlusions)
is modeled by the variation dictionary. After eliminating the
linear variation, we address the non-linear variation (e.g. ex-
pression changes) between the labeled and unlabeled samples
by estimating the centroid (prototype) of each Gaussian of the
GMM. Note that GMM learn the class centroid (prototype)
by semi-supervised clustering, i.e. we only use the ground
truth label as supervised information, the class assignment
of the unlabeled data is treated as the latent variable in
EM and updated iteratively during learning the class centroid
(prototype).

A. The gallery plus variation framework

The SRC with gallery plus variation framework has been ap-
plied to the face recognition problem as follows. The observed
images are considered as a combination of two different sub-

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

4

signals, i.e. a gallery dictionary P plus a variation dictionary
V in the linear additive model [13]:

B. Construct the data from each class as a Gaussian after
eliminating linear variations

y = Pα + Vβ + e,

(1)

where α is a sparse vector that selects a limited number of
bases from the gallery dictionary P, and β is another sparse
vector that selects a limited number of bases from the universal
linear variation dictionary V, and e is a small noise.

The sparse coefﬁcients α, β can be estimated by solving the

following (cid:96)1 minimization problem:

We rectify the data to eliminate linear variations of each
sample (e.g. illumination changes, occlusions), so that the data
from one person can be modeled as a Gaussian. This can be
achieved by solving Eq. (1) using Eq. (5) or (6) to represent
the gallery dictionary and using Eq. (3) or (4) to represent the
variation dictionary:

ˆy = y − V ˆβ = Pˆα + e,

(7)

where ˆy is the rectiﬁed unlabeled image without linear varia-
tion, ˆα and ˆβ can be initialized by Eq. (2).

(2)

(cid:21)
(cid:20)ˆα
ˆβ

= arg min

α,β

(cid:13)
(cid:13)
(cid:2)P V(cid:3)
(cid:13)
(cid:13)

(cid:20)α
β

(cid:21)

− y

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

+ λ

(cid:13)
(cid:20)α
(cid:13)
(cid:13)
β
(cid:13)

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)1

,

where λ is a regularization parameter. Finally, recognition can
be conducted by calculating the reconstruction residuals for
each class using ˆα (according to each class) and ˆβ, i.e. the test
sample y is classiﬁed to the class with the smallest residual.
In this process, the linear additive variation (e.g. illumi-
nation changes, different occlusions) of human faces can be
directly modeled by the variation dictionary, given the fact
that the linear additive variation can be shared by different
subjects, e.g. different persons may wear the same sunglasses.
Let A = [A1, ..., Ai, ..., AK] ∈ RD×n denote a set of n
labeled images with multiple images per subject (class), where
Ai ∈ RD×ni
is the stacked ni sample vectors of subject
i, and D is the data/feature dimension, the (linear) variation
dictionary can be constructed by:

],

(3)

, ..., A−

1 − a∗

K − a∗

V = [A−

11T
n1
V = [A1 − c11T
n1

K1T
nK
, ..., AK − cK1T
nK
Ai1ni ∈ RD×1 is the i-th class centroid of
where ci = 1
ni
i ∈ RD×1 is the prototype of class i that
the labeled data. a∗
can best represent the discriminative features of all the images
from subject i, A−
i according
i
to Ai.

is the complementary set of a∗

(4)

],

The gallery dictionary P can then be set accordingly using

one of the following equations:

P = A,

P = [c1, ..., cK],

(5)

(6)

The aforementioned formulations of the gallery dictionary
P and variation dictionary V works well when a large
amount of labeled data is available. However, in practical
applications such as recognizing a terrorist by his ID photo, the
labeled/training data is often limited and the unlabeled/testing
images are often taken under severely different conditions
from the labeled/training data. Therefore, it is hard to obtain
good prototype images to represent the unlabeled/testing im-
ages from the labeled/training data only. In order to address
the non-linear variation between the labeled and unlabeled
samples, in the following we learn a prototype a∗
i for each
class by estimating the true centroid for both the labeled
and unlabeled data of each subject, and represent the gallery
dictionary P using the learned prototype a∗
i . (The importance
of learning the gallery dictionary is shown in the previous Fig.
1 and Figs. 8 and 12 in the later sections.)

Then, the problem becomes to ﬁnd the relationship between
the rectiﬁed unlabeled data ˆy and its corresponding class
centroid a∗. Note that the sparse coefﬁcient ˆα is sparse and
typically there is only one entry of P that represents each
class. For an unlabeled sample, y, Pˆα actually selects the
most signiﬁcant entry of P, i.e. , it selects the class centroid
that is nearest to ˆy.

However, the “class centroid” selected by Pˆα cannot be
directly used as the initial class centroid for each Gaussian,
because the biggest element of the sparse coefﬁcient ˆα typi-
cally does not take value 1. In other words, Pˆα can introduce
scaling on the class centroid and additional (small) noise. More
speciﬁcally, assume that the most signiﬁcant entry of ˆα is
associated with class i, thus we have

Pˆα = P[(cid:15), (cid:15), ..., s (i-th entry), ..., (cid:15)]T

= sa∗

i + Pˆα−

i = sa∗

i + e(cid:48),

(8)

i is the i-th column of P, e(cid:48) = Pˆα−
i

where the sparse coefﬁcient ˆα = [(cid:15), (cid:15), ..., s (i-th entry), ..., (cid:15)]T
consisting of small values (cid:15) and a signiﬁcant value s in its i-th
entry. a∗
is the summation
of the “noise” class centroids selected by ˆα−
i , in which ˆα−
i
contains only the small values (i.e. (cid:15)’s) is the complementary
set of s according to ˆα.

Recall that the gallery dictionary P has been normalized
to have column unit (cid:96)2-norm in Eq. (2), therefore, the scale
parameter s can be eliminated by normalizing y − V ˆβ to have
unit (cid:96)2-norm:

ˆynorm = norm(y − V ˆβ) = norm(sa∗ + e(cid:48) + e)

≈ a∗

i + e∗,

(9)

where e∗ is a small noise which is assumed to be a zero-
mean Gaussian. Since there are insufﬁcient samples from each
subject, we assign the Gaussian noise of each class (subject)
to be different from each other, i.e. e∗
i = N (0, Σi), so as
to estimate the gallery dictionary more precisely. Thus, the
normalized ˆy obeys the following distribution:

ˆynorm ≈ a∗

i + e∗

i ∈ N (a∗

i , Σi).

(10)

C. GMM Formulation

After modeling the rectiﬁed data for each subject as a
Gaussian, we construct a GMM for the whole data to estimate
the true centroids a∗,
to address the non-linear variation
between the labeled/training and unlabeled/testing samples.
Speciﬁcally, the unknown assignment for the unlabeled data is

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

5

used as the latent variable. The detailed formulation is given
in the following.

(label) of the unlabeled data and maximizing the log likelihood
log p( ˆDnorm|θ) [33]–[36].

Let D = {(y1, l1)..., (yn, ln), yn+1, ..., yN } denote a set
of images of K classes including both labeled and unlabeled
samples, i.e. {(y1, l1)..., (yn, ln)} are n labeled samples with
{yi ∈ Ali, i = 1, ..., n}; and {yn+1, ..., yN } are N − n unla-
beled samples. Based on Eq. (10), a GMM can be formulated
to model the data and the EM algorithm can be used to more
precisely estimate the true class centroids by clustering the
normalized rectiﬁed images (that exclude the linear variations).
ˆDnorm =
Firstly,
{(ˆynorm
} must be calcu-
1
lated in order to construct the GMM. The calculation of the
normalized rectiﬁcations includes two parts: i) for the labeled
data, the normalized rectiﬁcations are the roughly estimated
class centroids;
the normalized
ii) for the unlabeled data,
rectiﬁcations can be estimated by Eq. (9):

the normalized rectiﬁed dataset
, ln), ˆynorm

n+1 , ..., ˆynorm

, l1)..., (ˆynorm

N

n

ˆynorm
i

=

(cid:40) cli ,

if i ∈ {1, ..., n},

norm(yi − V ˆβi),

if i ∈ {n + 1, ..., N },

(11)

where cli is the mean of the labeled data of the li-th subject,
i.e. it is the roughly estimated centroid of class li, V is the
variation dictionary, and ˆβi is the sparse coefﬁcient vector
estimated by Eq. (2).

Following this, the GMM can be constructed as described
in [32]. Speciﬁcally, let πj denote the prior probability of
class j, i.e. p(j) = πj, and θ be a set of unknown model
parameter: θ = {a∗
j , Σj, πj, for j = (1, ..., K)}. For the
incomplete samples (the unlabeled data), an latent indicator
zi,j is introduced to denote their label. That is, zi,j = 1, if
ˆynorm
i

∈ class j; otherwise zi,j = 0.

the objective function to optimize θ can be

Therefore,
obtained as:

For the EM iterations, a∗

j , Σj and πj for j = 1, ..., K are
initialized by pj, I, and ni/n, respectively. Here, ni is the
number of labeled samples in each class and n is the total
number of labeled samples.

E-Step: this aims at estimating the latent indicator, zi,j, of
the unlabeled data using the current estimate of θ. For the
labeled data, zi,j is known then it is set to its label, which is
the main difference between semi-supervised EM and original
EM. (This has already been applied in Eq. (13)):

ˆzi,j =

(cid:40)

1,

0,

if i ∈ {1, ..., n} and j = li,
if i ∈ {1, ..., n} and j (cid:54)= li.

(14)

Equation (14) ensures that the “estimated labels” of the labeled
samples are ﬁxed by their true labels. Thus, the labeled data
plays a role of anchors, which encourage the EM, applied to
the whole dataset, to converge to the true gallery.

For the unlabeled data, the algorithm uses the expectation
of zi,j from the previously estimated model θold, to give a
good estimation of zi,j:

ˆzi,j = E[zi,j]
πold
j

=

1
|Σold
j
k=1 πold
if i ∈ {n + 1, ..., N }.

|1/2 exp (− 1
k |1/2 exp (− 1

1
|Σold

(cid:80)K

k

2 ||ˆynorm

− (a∗

j )old||2

)

i
2 ||ˆynorm
i

Σold
j
k)old||2

− (a∗

,

)

Σold
k

(15)

M-Step: zi,j in Eq. (13) can be substituted by ˆzi,j, so that
we can optimize the model parameter θ. By using Maximum-
Likelihood Estimation (MLE), the optimized model parame-
ters can be obtained by:

ˆθ = arg max

log p( ˆDnorm|θ),

s.t.

K
(cid:88)

θ

j
where the log likelihood log p( ˆDnorm|θ) is:

πj = 1,

(12)

ˆNj =

N
(cid:88)

i=1

ˆzi,j,

ˆπj =

ˆNj
N

.

log p( ˆDnorm|θ) = log

p(ˆynorm
i

|θ)zi,j

N
(cid:89)

i=1

(cid:32) n
(cid:89)

i=1

p(ˆynorm
i

, li|θ)

N
(cid:89)

i=n+1

(cid:33)

p(ˆynorm
i

|θ)zi,j

= log

n
(cid:88)

=

log πli N (ˆynorm

i

|a∗
li

, Σli) +

i=1
N
(cid:88)

K
(cid:88)

i=n+1

j=1

zi,j log πjN (ˆynorm

i

|a∗

i , Σi).

(13)

In Eq. (13) the label of sample i, i.e. li, was used as a subscript
to index the mean and variance, i.e. a∗
, Σli, of the cluster
li
which sample i belongs to.

D. Estimation of the gallery dictionary by semi-supervised EM
algorithm

The EM algorithm is applied to estimate the unknown
indicator

parameters θ by iteratively calculating the latent

(16)

(17)

ˆzi,j ˆynorm
i

,

ˆa∗
j =

ˆΣj =

1
ˆNj

1
ˆNj

N
(cid:88)

i=1
N
(cid:88)

i=1

ˆzi,j(ˆynorm
i

− ˆa∗

j )(ˆynorm
i

− ˆa∗

j )T ,

(18)

The E-Step and M-Step iterate until

log p( ˆDnorm|θ) converges.

the log likelihood

Initialization: The EM algorithm needs good initialization
to avoid the local minima. To initialize the mean of each
Gaussian a∗
i , it is natural to use the roughly estimated class
centroids, i.e. the mean of labeled data, ci (See Fig.1 as an
example). The variance for each class is initialized by the
identity matrix, i.e. Σi = I, i = 1, ..., K.

E. Classiﬁcation using the estimated gallery dictionary and
SRC

The estimated ˆa∗
1, ..., ˆa∗

j is used as the gallery dictionary, P∗ =
K]. Then P∗ is used to substitute P in Eq. (2),
[ˆa∗
to estimate the new sparse coefﬁcients, α∗ and β∗. Finally,

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

6

Fig. 2. The illustration of the procedures of the proposed semi-supervised gallery dictionary learning method.

the residuals for each class k are computed for the ﬁnal
classiﬁcation by:

rk(y) =

(cid:13)
(cid:13)
y − (cid:2)P∗ V(cid:3)
(cid:13)
(cid:13)

(cid:20)δk(ˆα∗)
ˆβ∗

(cid:21)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

,

(19)

where δk(ˆα∗) is a vector whose nonzero entries are the entries
in ˆα∗ that are associated with class k. Then the testing
sample y is classiﬁed into the class with smallest rk(y), i.e.
Label(y) = arg mink rk(y).

F. S3RC model for SLSPP problem with generic dataset

Recently, a lot of researchers have introduced an extra
generic dataset for addressing the face recognition problem
with SLSPP [13], [14], [26], [37], [38], of which the SRC
methods [13], [14], [26] have achieved state-of-the-art results.
Here, the generic dataset can be an independent dataset from
the training and testing dataset D. When a generic dataset is
given in advance, our model can also be easily applied to the
SLSPP problem.

In the SLSPP problem,

the input data has N samples,
D = {(y1, 1), ..., (yK, K), yK+1, ..., yN }. From D, the set
of the labeled data, T = {y1, ..., yK}, is known as the gallery
dataset, where there is only one sample for each subject.
G = {G1, ..., GKg } ∈ RD×N g
denotes a labeled generic
dataset with N g samples and K g subjects in total. Here, D is
the data dimension shared by gallery, generic and testing data,
and Gi ∈ RD×ng
i vectors of the samples
from class i.

is the stacked ng

i

Due to the limited number of gallery samples, the initial
class center is set to the only labeled sample of each class,
and the corresponding variation dictionary can be constructed
similar to Eq. (4):

P = T,
V = [G1 − cg

11T
ng
1

, ..., GK − cg

11T
ng

Kg

(20)

(21)

],

where cg
in generic dataset, i.e. cg

i is the average of the samples according to i-th subject
Gi1ng

.

i = 1
ng
i

The obtained initial gallery dictionary and variation dictio-
nary can then be applied in our model, as discussed in Section
III-B – Section III-E.

i

G. Summary of the algorithm

The overall algorithm is summarized in Algorithm 1. We
also gave an illustrated procedures of the proposed method in
Fig. 2. The inputs to the algorithm, besides the regularization
parameter λ, are:

• For the insufﬁcient training samples problem: a set of
images including both labeled and unlabeled samples
D = {(y1, l1)..., (yn, ln), yn+1, ..., yN }, where yi ∈
RD, i = 1, ..., N are image vectors, and li, i = 1, ..., n
are the labels. We denote ni to be the number of labeled
samples of each class.

• For

the SLSPP problem with generic dataset:

the
including gallery and testing data D =
dataset
{(y1, 1)..., (yK, K), yK+1, ..., yN },
in which T =
{y1, ..., yK} is the gallery set with SLSPP and 1, ..., K
are the labels. A labeled generic dataset with N g samples
from K g subjects, G = {G1, ..., GN g }.

To investigate the convergence of the semi-supervised EM,
Fig. 3 illustrates the typical performance of change in negative
log-likelihood (from a randomly selected run on the AR
database), which convergence with less than 5 iterations. A
converged example is illustrated in Fig. 1 (i.e. the rightmost
subﬁgure, obtained by 5 iterations). From both Figs. 1 and 3, it
can be observed that the algorithm converges quickly and the
discriminative features of each speciﬁc subject can be learned
in a few steps.

Note that our algorithm is related to the SRC methods
which also used the gallery plus variation framework, such
as ESRC [13], SSRC [18], SVDL [14] and SILT [15], [16].
Among these method, only SSRC aims to learn the gallery

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

7

Algorithm 1: Semi-Supervised Sparse Representation
based Classiﬁcation
1 Compute the prototype matrix, P, by Eq. (6) (for the

insufﬁcient training samples problem), or by Eq. (20)
(for the SLSPP problem).

2 Compute the universal linear variation matrix, V, by Eq.
(4) (for the insufﬁcient training samples problem), or by
Eq. (21) (for the SLSPP problem).
/* V can also be calculated by other

variation dict. learning methods. */

3 Apply dimensional reduction (e.g. PCA) on the whole

dataset as well as P and V, and then normalize them to
have column unit (cid:96)2 norm.

4 Solve the Sparse Representation problem to estimate ˆα

and ˆβ for all the unlabeled y by Eq. (2).

5 Rectify the samples to eliminate linear variation and
normalize them to have unit (cid:96)2-norm by Eq. (11).
6 Initialize each Gaussian of the GMM by N (pi, I) for

i = 1, ...K, where pi is the i-th column of P.
7 Initialize the prior of GMM, πi = ni/n (for the

insufﬁcient training samples problem), or πi = 1/K (for
SLSPP problem) for i = 1, ...K.

8 repeat
9

10

E-Step: Calculate ˆzij by Eqs. (14) and (15).
M-Step: Optimize the model parameter
θ = {µj, Σj, πj, for j = 1, ..., K} by Eqs.
(16)–(18).
11 until Eq. (13) converges;
12 Let P∗ = [ˆµ1, ..., ˆµK], estimate ˆα∗, ˆβ∗ by Eq. (2).
13 Compute the residual rk(y) by Eq. (19).
14 Output: Label(y) = arg mink rk(y).

dictionary, but it needs sufﬁcient label/training data. Also it is
not ensured that the learned prototype from SSRC can well
represent the testing data due to the possible severe variation
between the labeled/training and the testing samples. While
ESRC, SVDL and SILT focus to learn a more representative
variation dictionary. The variation dictionary learning in these
methods are complementary to our proposed method. In other
words, we can use them to replace Eqs. (3), (4) or (21) for
better performance, which will be veriﬁed in the experiments

by using SVDL to construct our variation dictionary.

IV. RESULTS

In this section, our model is tested to verify the performance
for both the insufﬁcient training samples problem and the
the performance of the proposed
SLSPP problem. Firstly,
method on the insufﬁcient training samples problem using the
AR database [39] is shown. Then, we test our method on the
SLSPP problem on both the Multi-PIE [40] and the CAS-
PEAL [41] databases, using one facial image with neutral
expression from each person as the only labeled gallery
sample. Next, in order to further investigate the performance
of the proposed semi-supervised gallery dictionary learning,
we re-do the SLSPP experiments with one randomly selected
image (i.e. uncontrolled image) per person as the only gallery
sample. The performance has been evaluated on the Multi-
PIE [40], and the more challenging LFW [42] databases.
After that, the inﬂuence of different amounts of labeled and/or
unlabeled data is investigated. Finally, we have illustrated the
performance of our method through a practical system with
automatic face detection and automatic face alignment.

For all the experiments, we report the results from both
transductive and inductive experimental settings. Speciﬁcally,
in the transductive setting, the data is partitioned into two
parts, i.e. the labeled training and the unlabeled testing. We
focus on the performance on the unlabeled/testing data, where
we do not distinguish the unlabeled and the testing data. In
inductive setting, we split the data into three parts, i.e. the
labeled training, the unlabeled training and the testing, where
the model is learned by the labeled training and the unlabeled
training data, the performance is evaluated on the testing data.
In order to provide comparable results, the Homotopy method
[43]–[45] was used to solve the (cid:96)1 minimization problem for
all the methods involved.

A. Performance on the insufﬁcient training samples problem

In the AR database [39], there are 126 subjects with 26
images for each of them taken at two different sessions (dates),
each session containing 13 images. The variations in the AR
database include illumination change, expressions and facial
occlusions. In our experiment, a cropped and normalized
subset of the AR database that contains 100 subjects with
50 males and 50 females is used. The corresponding 2600
images are cropped to 165 × 120. This subset of AR used
in our experiment has been selected and cropped by the data
provider [46]1. Figure 4 illustrates one session (13 images) for
a speciﬁc subject.

Here we conduct four experiments to investigate the perfor-
mance of our methods, and the results are reported from both
the transductive and the inductive settings. No extra generic
dataset is used in either of the experiments. The experimental
settings are:

• Transductive: There are two transductive experiments.
One is an extension of the experiment in [8], [18], by

Fig. 3. The typical performance of the change in negative log-likelihood
(from a randomly selected trial on the AR database), which convergence with
less than 5 iterations.

1The

data

can

be

downloaded

at

http://cbcsl.ece.ohio-state.edu/

protected-dir/AR warp zip.zip upon authorization.

05101520−3.5−2.5−1.5−0.50.5x 106Number of iterationsNegative log−likelihoodIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

8

Fig. 4. The cropped image instances of one subject from the AR database
(one complete session).

using different amounts of labeled data. In the experi-
ment, 2-13 uncontrolled images (of 26 images in total) of
each subject are randomly chosen as the labeled data, and
the remaining 24-13 images are used as unlabeled/query
data for EM clustering and testing. The two sessions
are not separated in this experiment. The results of this
experiment are shown in Figure 5a. Figure 5b is a more
challenging experiment, in which the labeled and unla-
beled images are from different sessions. More explicitly,
we ﬁrst randomly choose a session, then randomly select
2-13 images of each subject from that session to be the
labeled data. The remaining 13 images from the other
sessions are used as unlabeled/query data.

• Inductive: There are also two inductive experiments as
shown in Figs. 5c and 5d, where the labeled training is
selected by the same strategy used in the transductive
settings. Speciﬁcally, the experiment in Fig. 5c uses 2-13
(of 26 images in total) randomly selected uncontrolled
images as labeled training samples. Thereafter, the re-
maining 24-13 images are randomly separated into two
i.e. half as unlabeled training samples and the
parts,
other half as testing samples. Similarly, Fig. 5d shows
another more challenging experiment, where the training
(including both labeled and unlabeled) and the testing
samples are chosen from different sessions to ensure the
large differences between them. That is, for each person,
a session is randomly selected at ﬁrst. From that session,
then, 2-12 randomly selected images are used as the
labeled training samples, and the remaining 11-1 images
are used as the unlabeled training samples. All the 13
images from the other session are used as testing samples.

State-of-the-art methods for the insufﬁcient training sam-
ples problem are used for comparison, including sparse/dense
representation methods SRC [1] and ProCRC [47], low-rank
models DLRD SR [21], D2L2R2 [22], [23], gallery plus
variation representation ESRC [13], SSRC [18], and RADL
[17]. We follow the same settings of other parameters as in
[18], i.e. ﬁrst 300 PCs from PCA [48] have been used and
λ is set to 0.005. The results are illustrated by the mean and
standard deviation of 20 runs. Particularly, in order to show
that the generalizability of the proposed gallery dictionary
learning S3RC, we also investigate the performance of our
i.e. we replace the gallery
method in RADL framework,
dictionary in RADL by the learned gallery from S3RC and
keep the remaining part of the RADL model unchanged. We
denote it by S3RC-RADL.

Fig. 5. The results (Recognition Rate) from the AR database. We used
the ﬁrst 300 PCs (dimensional reduction by PCA) and λ is set to 0.005
(as identical of [18]). Each value was obtained from 20 runs. The Left and
Right Columns denote experiments with Combined and Separated Session,
the Top and Bottom Rows represent the transductive and inductive settings,
respectively. Zoom-in ﬁgures are provided in subﬁgs (a), (c), and (d).

Figure 5 shows that our results,

i.e. S3RC and S3RC-
RADL, consistently outperform other counterparts under the
same conﬁgurations. Moreover, signiﬁcant improvements in
the results are observed when there are few labeled samples
per subject. Especially, it is observed that the results of SSRC
are less satisfactory comparing with more recent state-of-the-
art methods RADL and ProCRC, however, its performance
is boosted to the second highest by utilizing the proposed
gallery dictionary method S3RC. For example, the accuracy
of S3RC is higher by around 10% than SSRC when using 2-4
labeled samples per person. Furthermore, by combining with
more recent RADL method, S3RC-RADL achieves the best
performance. It is also noted that the size of outperformance
decreases when more labeled data is used. This is because the
class centroids estimated by SSRC (averaging the labeled data
according to same label) are less likely to be the true gallery
when the number of labeled data are small. Thus, by improving
the estimates of the true gallery from the initialization of
the averaged labeled data, better results can be obtained by
our method. Conversely, if the number of labeled data is
sufﬁciently large, then the averaged labeled data becomes good
estimates of the true gallery, which results in less improvement
compared with our method.

The results of Figs. 5a and 5c (i.e. the Left Column) are
higher than those of Figs. 5b and 5d (i.e. the Right Column)
for all methods, because the labeled and unlabeled samples
of the Right Column of Fig. 5 are obtained from different
sessions. Interestingly, the higher outperformance of S3RC
can be observed in the more challenging experiment shown in
Fig. 5b. This observation further demonstrates the effectiveness
of our proposed semi-supervised gallery dictionary learning
the results of the transductive experiments
method. Also,
(i.e. Figs. 5a and 5b) are better than those of the inductive

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

9

experiments (i.e. Figs. 5c and 5d), because the testing samples
have been directly used to learn the model in the transductive
settings.

The above results have demonstrated the effectiveness of

our method for the insufﬁcient training samples problem.

B. The performance on the SLSPP problem using facial image
with neutral expression as gallery

1) The Multi-PIE Database : The large-scale Multi-PIE
database [40] consists of images of four sessions (dates) with
variations of pose, expression, and illumination. For each
subject in each session, there are 20 illuminations, with indices
from 0 to 19, per pose per expression. In our experiments, all
the images are cropped to the size of 100 × 82. Since the
data provider did not label the eye centers of each image in
advance, we average the 4 labeled points of each eye ball
(Points 38, 39, 41, 42 for the left eye and Points 44, 45, 47,
48 for the right eye) as the eye center, then crop them by
locating the two eye centers at (19, 28) and (63, 28) of the
100 × 82 images.

This experiment is a reproduction of the experiment on
the SLSPP problem using the Multi-PIE database in [14].
Speciﬁcally, the images with illumination 7 from the ﬁrst
100 subjects (among all 249 subjects) in Session 1 of the
facial image with neutral expression (Session 1, Camera 051,
Recording 1. S1 Ca051 R1 for short) are used as gallery. The
remaining images under various illuminations of the other 149
subjects in S1 Ca051 R1 are used as the generic training
data. For the testing data, we use the images of the ﬁrst
100 subjects from other subsets of the Multi-PIE database,
i.e. the image subsets that are with different illuminations
(S2 Ca051 R1, S3 Ca051 R1, S4 Ca051 R1), different
illuminations and poses (S1 Ca050 R1, S2 Ca050 R1,
S3 Ca050 R1, S4 Ca050 R1), different illuminations and
expressions (S1 Ca051 R2, S2 Ca051 R2, S2 Ca051 R3,
S3 Ca051 R2), different
illuminations, expressions and
poses (S1 Ca050 R2, S1 Ca140 R2). The gallery image
from a speciﬁc subject and its corresponding unlabeled/testing
images with a randomly selected illumination are shown in
Fig. 6.

The results from classical classiﬁers NN [48], SVM [49],
sparse/dense representation SRC [1], CRC [9], ProCRC [47],
gallery plus variation representation ESRC [13], SVDL [14],
RADL [17] are chosen for comparison2 . In order to further
investigate the generalizability of our method and to show
the power of the gallery dictionary estimation, besides the
evaluation on S3RC-RADL, we also report
the results of
S3RC using the variation dictionary learned by SVDL (S3RC-
SVDL), i.e. initializing the ﬁrst four steps of the Algorithm 1
by SVDL. The parameters were identical to those in [14], i.e.

2 Note that the DLRD SR [21] and D2L2R2 [22], [23] methods, which
we compared in the insufﬁcient training samples problem, are less suitable
for comparison here due to the SLSPP problem. It is because in order to learn
a low-rank (sub-)dictionary for each subject, both of them assume low-rank
property of the gallery dictionary, which requires multiple gallery samples per
subject. SSRC also requires multiple gallery samples per subject to learn the
gallery dictionary and thus is less suitable for comparison either.

Fig. 6. The cropped image instances of one subject from various subsets of
the Multi-PIE database.

the 90 PCA dimension and λ = 0.001. The transductive and
inductive experimental settings are:

• Transductive: Here we use all the images from the cor-
responding session as unlabeled testing data, the results
are summarized in the top subﬁgure of Fig. 7.

• Inductive: The bottom subﬁgure of Fig. 7 summarizes
the inductive experimental results, where the 20 images
of the corresponding session were partitioned into two
parts, i.e. half for unlabeled training and the other half for
testing. The inductive results are obtained by averaging
20 replicates.

the subsets. In particular,

Figure 7 shows that the proposed gallery dictionary learning
methods, i.e. S3RC, S3RC-SVDL, S3RC-RADL achieve the
top recognition rates compared with the other categories of
methods in recognizing all
the
highest enhancements can be observed for recognition with
varying expressions. The reason might be that the samples
with different expressions cannot be properly aligned by using
only the eye centers, so the gallery dictionary learned by
S3RC can achieve better alignment with the testing images.
This demonstrates that gallery dictionary learning plays an
important role in SRC based face recognition. In addition, our
inductive results are comparable to the transductive results,
which implies we might do not need as many as 20 images as
unlabeled training to learn the model on Multi-PIE database.
It is also observed that in this challenging SLSPP problem, all
the gallery plus variation representation methods (i.e. ESRC,
SVDL, RADL) outperform the best sparse/dense representa-
tion method (i.e. ProCRC), suggesting that the gallery plus
variation representation methods are more suitable for the
SLSPP problem. As a generally applicable gallery dictionary
learning method to the gallery plus variation framework, our
method further improves the performance on this challenging
tasks.

Furthermore, by integrating the variation dictionary learned
by SVDL into S3RC, S3RC-SVDL, S3RC-RADL also im-

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

10

Fig. 7. The results (Recognition Rate, %) from the Multi-PIE database with controlled single gallery sample per person, Top: RecRate for transductive
experiments, Bottom: RecRate for inductive experiments. The bars from Left to Right are: NN, SVM, SRC, CRC, ProCRC, ESRC, SVDL, S3RC, S3RC-SVDL,
and S3RC-RADL.

Fig. 8.
The labeled gallery is controlled facial image with neutral expression under standard illumination.

Illustrations of the learned gallery samples when there is non-linear variation between the (input) labeled gallery and the unlabeled/testing samples.

proves the performance of S3RC in most cases. This also
demonstrates the generalizablity of our method. These perfor-
mance enhancements of S3RC, S3RC-SVDL and S3RC-RADL
(w.r.t ESRC, SVDL, and RADL) are beneﬁted from using the
unlabeled samples for estimating the true gallery instead of
relying on the labeled samples only. When given insufﬁcient
labeled samples, the other methods ﬁnd it is hard to achieve
satisfactory recognition rates in some cases.

The learned gallery is also investigated. We are especially
interested in examining the learned gallery when there is
a large difference between the input
labeled samples and
the unlabeled/testing images. Therefore, we use the neutral
image as the labeled gallery and randomly choose 10 images
with smile (i.e. from S1 Ca051 R2 and S2 Ca051 R2), the
learned gallery is shown in Fig. 8. Figure 8 illustrates that by
using the unlabeled training samples with smile, the learned
gallery can also possess desirable (non-linear) smile attributes
(see the mouth region), which better represents the prototype
of the unlabeled/testing images. In fact, the proposed semi-
supervised gallery dictionary learning method can be regarded
as a pixel-wise alignment between the labeled gallery and the
unlabeled/testing images.

The analysis in this section demonstrates the promising
performance of our method for the SLSPP problem on Multi-
PIE database.

2) The CAS-PEAL Database: The CAS-PEAL database
[41] contains 99594 images with different illuminations, facing
directions, expressions, accessories, etc. It is considered to be
the largest database available that contains occluded images.
Note that although the occlusions may commonly occurs on
the objects of interests in practice [50], [51], the Multi-PIE
database does not contain images with occlusions. Thus, as a
complementary experiment, we use all the 434 subjects from
the Accessory category for testing, and their corresponding
images from the Normal category as gallery. In this experi-
mental setting, there are 1 neutral image, 3 images with hats,
and 3 images with glasses/sunglasses for each subject. All the
images are cropped to 100 × 82, with the centers of both eyes
located at (19, 28) and (63, 28). Figure 9 illustrates the gallery
and testing images for a speciﬁc subject.

Among the 434 subjects used, 300 subjects are selected
for training and testing, and the remaining 134 subjects are
used as generic training data. The ﬁrst 100 dimension PCs
(dimensional reduction by PCA) are used and λ is set to

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

11

Fig. 9. The cropped image instances for one subject from the Normal and
the Accessory categories of the CAS-PEAL database.

0.001. We also compare our results with NN [48], SVM [49],
SRC [1], CRC [9], ProCRC [47], ESRC [13], SVDL [14]
and RADL [17]. The results of the transductive and inductive
experiments are reported in Table I. The experimental settings
are:

• Transductive: All the 6 images of the Accessory category

shown in Fig. 9 are used for unlabeled/testing.

• Inductive: Of all the 6 images, we randomly select 3
images as unlabeled training and the remaining 3 images
are used as testing. The inductive results are obtained by
averaging 20 replicates.

Table I shows that S3RC, S3RC-SVDL and S3RC-RADL
achieve top three recognition rates in the CAS-PEAL database,
only except S3RC-RADL vs. RADL in the Inductive case.
This is the only case that our method slightly inferior than
our counterpart among all of our experiments (including the
experiments in the previous and following sections). The
reason is that in the inductive experiments, there is too few
(i.e. 3 samples per subject) unlabeled data to guarantee the
generalizibility of them. Also RADL used weighted (cid:96)2 norm
(i.e. (cid:96)2 norm with projection) to calculate the data error
term, which already gains some robustness to the less perfect
gallery dictionary. The results in Table I verify the promising
performance of our method for SLSPP problem on CAS-PEAL
database.

TABLE I
THE RESULTS (RECOGNITION RATE, %) FROM THE CAS-PEAL
DATABASE ON THE Normal AND Accessory SUBSETS, FOR BOTH
transductive AND inductive EXPERIMENTS. IN THE BRACKETS, WE SHOW
THE IMPROVEMENT OF S3RC w.r.t ESRC, S3RC-SVDL w.r.t SVDL AND
S3RC-RADL w.r.t RADL, SINCE THE SAME VARIATION DICTIONARY HAS
BEEN USED IN THESE THREE PAIRS. WE USED THE FIRST 100 PCS
(DIMENSIONAL REDUCTION BY PCA) AND λ IS SET TO 0.001.

Method

Transductive

Inductive

NN

SVM

SRC

CRC

ProCRC

ESRC

SVDL

RADL
S3RC
S3RC-SVDL
S3RC-RADL

41.00

41.00

57.22

54.56

54.89

71.78

69.67

75.27

41.08

41.08

56.70

53.84

54.67

71.76

69.74

73.78

75.39 (↑3.61)

74.66 (↑2.90)

72.06 (↑2.39)

71.79 (↑2.05)

75.89 (↑0.62)

72.56 (↓1.23)

C. The performance on the SLSPP problem using uncontrolled
image as gallery

1) The Multi-PIE Database :

In order to validate the
proposed S3RC methods, additional more challenging exper-
iments are performed on the Multi-PIE Database, where an
uncontrolled image is used as the labeled gallery. Speciﬁcally,
for each unlabeled/testing subset illustrated in Fig. 10, we
randomly choose one image per subject from the other subsets
(excluding the unlabeled/testing subset) as the labeled gallery.
It should be noted that the well controlled gallery, i.e. the neu-
tral images from S1 Ca050 R1, is not used in this section.
Both the transductive and the inductive experiments are also
reported as the same protocol used in Sect. IV-B1.

Fig. 10. The illustrations of the data used in Sect. IV-C1. We ﬁrst randomly
select a subset as unlabeled/testing data, then a gallery sample is randomly
chosen from other subsets (excluding the unlabeled/testing select).

• Transductive: Here we use all the images from the cor-
responding session as unlabeled testing data, the results
are summarized in top subﬁgure of Fig. 11.

• Inductive: Bottom subﬁgure of Fig. 11 summarizes the
inductive experimental results, where the 20 images of
the corresponding session are partitioned into two parts,
i.e. half for unlabeled training and the other half for
testing. The inductive results are obtained by averaging
20 replicates.

The results are shown in Fig. 11, in which the proposed
S3RC is compared with NN, SVM, SRC [1], CRC, and Pro-
CRC methods3. Figure 11 shows that our method consistently
outperforms the other outline methods. In fact, although the
overall accuracy decreases due to the uncontrolled labeled
gallery, all the conclusions made in Sect. IV-B1 are supported
and veriﬁed by Fig. 11 here.

We also investigated the learned gallery when a uncontrolled
labeled gallery. Figure 12 shows
image is used as input
the results when using the squint image (i.e. a image from
S2 Ca051 R3) as the single labeled input gallery for each
subject. It is observed (see eye and mouth regions) that the
learned gallery can better represent the testing images (i.e.
smile) by the non-linear semi-supervised gallery dictionary
learning. The reason is the same as the previous experiments
in Fig. 8, i.e. the proposed semi-supervised method conducts a
pixel-wise alignment between the labeled gallery and the unla-

3Note that the SVDL and RADL method is less suitable to the SLSPP
problem with uncontrolled image as gallery. It
is because the variation
dictionary learning of SVDL or RADL requires reference images for all the
subjects in the generic training data, where the reference images should have
the same type of variation as the gallery. However, such information cannot
be inferred due to the uncontrolled gallery images used.

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

12

Fig. 11. The results (Recognition Rate, %) from the Multi-PIE database with uncontrolled single gallery sample per person, Top: RecRate for transductive
experiments, Bottom: RecRate for inductive experiments. The bars from Left to Right are: NN, SVM, SRC, CRC, ProCRC, ESRC, S3RC.

Fig. 12.
The labeled gallery is uncontrolled (i.e. the squint image).

Illustrations of the learned gallery samples when there is non-linear variation between the (input) labeled gallery and the unlabeled/testing samples.

beled/testing images so that the non-linear variations between
them are well addressed.

2) The LFW Database: The Labeled Face in the Wild
(LFW) [42] is the latest benchmark database for face recog-
nition, which has been used to test several advanced methods
with dense or deep features for face veriﬁcation, such as
[52]–[56]. In this section, our method has been tested on the
LFW database to create a more challenging face identiﬁcation
problem.

Speciﬁcally,

the face images of the LFW database are
collected from the Internet, as long as they can be detected by
the Viola-Jones face detector [42]. As a result, there are more
than 13,000 images in the LFW database containing enor-
mous intra-class variations, where controlled (e.g. neutral and
facial) faces may not be available. Considering the previous
experiments on the Multi-PIE and CAS-PEAL databases dealt
with speciﬁc kinds of variations separately, as an important
extended experiment, the effectiveness of our S3RC method
can be further validated by its performance on the LFW
dataset.

A pre-aligned database by deep funneling [57] was used
in our experiment. We select a subset of the LFW database

containing more than 10 images for each person, with 4324
images from 158 subjects in total. In the experiments, we ran-
domly select 100 subjects for training and testing the model,
the remaining 58 subjects are used to construct the variation
dictionary. The experimental results for both transductive and
inductive learning are reported. The only gallery image is
randomly chosen from each subject, then the transductive and
the inductive experimental settings are:

• Transductive: All the remaining images from a speciﬁc
subject are used for unlabeled/testing. The results are
obtained by averaging 20 replicates due to the randomly
selected subjects.

• Inductive: For each subject, we randomly select half of
the remaining images as unlabeled training and the other
half are used for testing. The results are obtained by
averaging 50 replicates due to the randomly selected
subjects and random unlabeled-testing split.

The results are shown in Table II, where we use NN,
SVM, SRC [1], CRC, and ProCRC for comparison. First,
we have tested our method using simple features obtained
from an unsupervised dimensional reduction by PCA. The

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

13

results in the left two columns of Table II show that, although
none of the methods achieve a satisfactory performance, our
method, based on the semi-supervised gallery dictionary, still
signiﬁcantly outperforms the baseline methods.

Nowadays, Deep Convolution Neural Network (DCNN)
based methods have achieved state-of-the-art performance on
the LFW database [52]–[56]. It is noticed that the DCNN
methods often use basic classiﬁers to do the classiﬁcation,
such as softmax, linear SVM or (cid:96)2 distance. Recently, it is
showed that by coupling with the deep-learned CNN features,
the SRC methods can achieve signiﬁcantly improved results
[47]. Motivated by this, we also aim to verify that by utilizing
the same deep-learned features, our method (i.e. our classiﬁer)
is able to further improve the results obtained by the basic
classiﬁers.

Speciﬁcally, we utilize a recent and public DCNN model
named VGG-face [52] to extract the 4096-dimensional fea-
tures, then our method, as well as the baseline methods, are
implied to perform the classiﬁcation. The results, shown in
the left two-column of Table II, demonstrate the signiﬁcantly
improved results from the proposed methods using the DCNN
features, whereas such an investigation cannot be observed by
comparing other SRC methods, e.g. SRC, CRC, ESRC, with
the basic NN classiﬁer. It is noted that the original classiﬁer
used in [52] is the (cid:96)2 distance in face veriﬁcation, which is
equivalent to KNN (K = 1) in face identiﬁcation with SLSPP.
Therefore, the results in Table II demonstrate that with the
state-of-the-art DCNN features, the performance on the LFW
database can be further boosted by using the proposed semi-
supervised gallery dictionary learning method.

TABLE II
THE RESULTS (RECOGNITION RATE, %) FROM THE LFW DATABASE, FOR
BOTH transductive AND inductive EXPERIMENTS WITH SIMPLE PCA
FEATURES AND DEEP LEARNED FEATURE BY [52]. IN THE BRACKETS, WE
SHOW THE IMPROVEMENT OF S3RC w.r.t ESRC. WE USED THE FIRST 100
PCS (DIMENSIONAL REDUCTION BY PCA) AND THE DEEP LEARNED
FEATURES BY [52] IS OF 4096 DIMENSIONS. λ IS SET TO 0.001.

Method

NN

SVM

SRC

CRC

ProCRC

ESRC
S3RC

PCA fea. (100)

DCNN fea. by [52] (4096)

Transductive

Inductive

Transductive

Inductive

5.57

5.37

10.92

10.47

10.77

15.51

5.82

5.82

11.13

10.69

10.99

16.23

89.28

89.28

89.50

89.18

90.85

90.58

90.19

90.19

90.23

89.86

90.13

90.73

17.99 (↑2.48)

17.90 (↑1.67)

92.55 (↑1.98)

92.57 (↑1.84)

D. Analysis of the inﬂuence of different amounts of labeled
and/or unlabeled data

The impact of different amounts of unlabeled data in S3RC
is analyzed on different amounts of labeled data using AR
database. In this experiment, we ﬁrst randomly choose a
session, and then select 1-13 unlabeled data for each subject
from that session to investigate the inﬂuence of different
amounts of unlabeled data. 2, 4 and 6 labeled samples of

Fig. 13. The analysis of the impact of different amounts of unlabeled data
from 1-13 on AR database. Three different amounts of labeled data are chosen
for analysis, i.e. 2, 4 and 6 labeled samples per subject, as shown in Left,
Middle, Right subﬁgures, respectively. The results are obtained by averaging
20 runs, number of PCs is 300 (dimensional reduction by PCA) and λ is
0.005.

each subject are randomly chosen from the other session.
For comparison, we also illustrate the results of SRC, ESRC,
SSRC with the same conﬁgurations. The results are shown in
Fig. 13, which is obtained by averaging 20 runs.

It can be observed from Fig. 13 that: i) our method is
effective (i.e. can outperform the state-of-the-art) even when
there is only 1 unlabeled sample per subject; ii) when more
unlabeled samples are used, we observe signiﬁcant increased
accuracy from our method, while the accuracies of the state-
of-the-art methods do not change much, because unlabeled
information is not considered in these methods; iii) the better
performance of our method compared with the alternatives is
not affected by different amounts of labeled data used.

Furthermore, we are also interested in illustrating the
learned galleries. Compared with the experiments on the AR
database stated above, the experiments similar to those on the
Multi-PIE database in Sect.IV-B1 are more suitable to our pur-
pose. It is because in the above AR experiments, the inﬂuence
of randomly selected gallery and unlabeled/testing samples can
be eliminated by averaging the RecRate of multiple replicates.
However, the learned galleries from multiple replicates cannot
be averaged for illustration. Therefore, the only (ﬁxed) labeled
gallery sample per subject and the similarity between the
unlabeled/testing samples in the Multi-PIE database enable
to alleviate such inﬂuence. In addition, the large difference
between the labeled gallery and the unlabeled/testing samples
is more suitable to illustrate the effectiveness of the proposed
semi-supervised gallery learning method.

Speciﬁcally, the same neutral image from Sect.IV-B1 is used
as the only labeled gallery sample per subject. Images from
S1 Ca051 R2 are used as the unlabeled/testing images. We
randomly choose 1-10 testing images to do the experiments,
each trail is used to draw the learned galleries as each subﬁgure
of Fig. 15.

Figure 15 shows that with more unlabeled training samples,
the gallery samples learned by our proposed S3RC method
can better represent the unlabeled data (i.e. smile, see the
mouth region). In fact, we note that
the proposed semi-
supervised learning method can be regarded as nonlinear
pixel-wise/local alignments, e.g. to align the neutral gallery
and the smiling unlabeled/testing faces in Fig. 15, therefore
enabling a better representation of the unlabeled/testing data
to achieve improved performance over the existing linear and
global SRC methods (e.g. SRC, ESRC, SSRC, etc. ).

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

14

(a)

(b)

(c)

Fig. 14. The examples of detected and aligned faces. (a) An example of detected faces (by Viola-Jones detector, green dash box) and aligned faces (by MRR,
red solid box) on the original image. (b) The cropped detected faces. (c) The cropped aligned faces.

the output aligned data are cropped into 60 × 48 images. An
example of detected and aligned result for a speciﬁc subject
on the original image is shown in Fig. 14(a). Other detected
and aligned face examples are shown in Figs. 14(b) and 14(c),
respectively.

The classiﬁcation results using Viola-Jones for detection
and MRR for alignment are shown in Table III for some
subsets of Multi-PIE database with transductive experimental
settings. The classiﬁcation parameters are identical with them
in Sections 3.2. Table III clearly shows that S3RC and S3RC-
SVDL (i.e. S3RC plus the variation dictionary learned by
SVDL) achieve the two highest accuracies. By using the same
aligned data as input for all the methods, we show that the
strong performance of S3RC is due to the utilization of the
unlabeled information, no matter whether the alignment is
obtained manually or by automatic MRR.

TABLE III
THE RESULTS (RECOGNITION RATE, %) FOR THE MULTI-PIE DATABASE
USING VIOLA-JONES FOR DETECTION AND MRR FOR ALIGNMENT. IN
THE BRACKETS, WE SHOW THE IMPROVEMENT OF S3RC w.r.t ESRC AND
S3RC-SVDL w.r.t SVDL, SINCE THE SAME VARIATION DICTIONARY HAS
BEEN USED IN BOTH PAIRS. WE USED THE FIRST 90 PCS (DIMENSIONAL
REDUCTION BY PCA) AND λ IS SET TO 0.001.

Method

S2 Ca051 R1

S3 Ca051 R1

S4 Ca051 R1

SRC

ESRC

SVDL
S3RC
S3RC-SVDL

55.75

86.78

91.78

51.47

85.07

89.30

53.64

86.17

89.68

95.75 (↑8.97)

94.58 (↑9.51)

93.96 (↑7.79)

97.74 (↑5.96)

95.28 (↑5.98)

95.32 (↑5.64)

V. DISCUSSION

A semi-supervised sparse representation based classiﬁcation
method is proposed in this paper. By exploiting the unlabeled
data, it can well address both linear and non-linear variations
between the labeled/training and unlabeled/testing samples.
This is particularly useful when the amount of labeled data
is limited. Speciﬁcally, we ﬁrst use the gallery plus variation
model to estimate the rectiﬁed unlabeled samples excluding
linear variations. After that, the rectiﬁed labeled and unlabeled
samples are used to learn a GMM using EM clustering to
address the non-linear variations between them. These rectiﬁed

Fig. 15. The illustrations of the learned gallery with different amount (1-10
per subject) of unlabeled training samples. The reconstructed learned galleries
are illustrated in the Red Box, with 1-10 unlabeled samples per subject from
Left to Right, then Top to Bottom. Better representation can be observed with
more unlabeled training samples (i.e. smile, see the mouth region).

E. The performance of our method with different alignments

In the previous experiments, the performance of our method
is investigated using face images that are aligned manually
by eye centers. Here, we show the results using different
alignments in a fully automatic pipeline. That is, for a given
image, we use the Viola-Jones detector [58] for face detection,
Misalignment-Robust Representation (MRR) [59] for face
alignment, and the proposed S3RC for recognition.

Note that the aim of this section is to prove that i) the better
performance of S3RC is not affected by different alignments,
and ii) S3RC can be integrated into a fully automatic pipeline
for practical use. MRR is chosen for alignment, because the
code is available online4. Researchers can also use other
techniques (e.g. SILT [15], [16], DSRC [2] by
alignment
aligning the gallery ﬁrst, then align the query data to the
well aligned gallery; or TIPCA [60]), but the comparison of
different alignment methods is beyond the scope of this paper.
For MRR alignment, we use the default settings of the demo
codes, and only change the input data. That is, the well aligned
neutral images of the ﬁrst 50 subjects of Multi-PIE database
(as provided in the MRR demo codes) are used as the gallery to
align the whole Multi-PIE database of 337 subjects, λ of MRR
is set to 0.05, the number of selected candidates is set to 8 and

4MRR codes can be downloaded at http://www4.comp.polyu.edu.hk/

∼cslzhang/code/MRR eccv12.zip.

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

15

labeled data is also used as the initial mean of the Gaussians in
the EM optimization. Finally, the query samples are classiﬁed
by SRC with the precise gallery dictionary estimated by EM.
The proposed method is ﬂexible, in which the gallery dic-
tionary learning method is complementary to existing methods
which focus on learning the (linear) variation dictionary, such
as ESRC [13], SVDL [14], SILT [15], [16], RADL [17] etc.
For the ESRC, SVDL and RADL methods, we have shown
by experiments that the combination of the proposed gallery
dictionary learning and ESRC (namely S3RC), SVDL (namely
S3RC-SVDL) and RADL (namely S3RC-RADL) achieve sig-
niﬁcantly improved performance on all the tasks.

It is also noted by coupling with state-of-the-art DCNN
features, our method, as a better classiﬁer, can further im-
prove the recognition performance. This has been veriﬁed by
using VGG-face [52] features on LFW database, where our
method outperforms the other baselines by 1.98%. While less
improvement is observed by feeding DCNN feature to other
classiﬁers, including SRC, CRC, ProCRC and ESRC, when
comparing with the basic nearest neighbor classiﬁer.

Moreover, our method can be combined with SRC methods
that incorporate auto-alignment, such as SILT [15], [16], MRR
[59], and DSRC [2]. Note that these methods will degrade
to SRC after the alignment (except SILT, while by which the
learned illumination dictionary can also be utilized in S3RC by
the same approach as S3RC-SVDL). Thus, these methods can
be used to align the images ﬁrst, then S3RC can be applied for
classiﬁcation utilizing the unlabeled information. In practical
face recognition tasks, our method can be used following on
automatic pipeline of face detection (e.g. Viola-Jones detector
[58]), followed by alignment (e.g. one of [2], [15], [16], [59]–
[62]), and then S3RC classiﬁcation.

VI. CONCLUSION

In this paper, we propose a semi-supervised gallery dictio-
nary learning method called S3RC, which improves the SRC
based face recognition by modeling both linear and non-linear
variation between the labeled/training and unlabeled/testing
samples, and leveraging the unlabeled data to learn a more
precise gallery dictionary. These better characterize the dis-
criminative features of each subject. Through extensive simu-
lations, we can draw the following conclusions: i) S3RC can
deliver signiﬁcantly improved results for both the insufﬁcient
training samples problem and the SLSPP problems. ii) Our
method can be combined with state-of-the-art method that
focus on learning the (linear) variation dictionary, so that
we can obtain further improved the results (e.g. see ESRC
v.s. S3RC, SVDL v.s. S3RC-SVDL, and RADL v.s. S3RC-
RADL). iii) The promising performance of S3RC is robust to
different face alignment methods. A future direction is to use
SSL methods other than GMM to better estimate the gallery
dictionary.

REFERENCES

[1] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust face
recognition via sparse representation,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 31, no. 2, pp. 210–227, 2009.

[2] A. Wagner, J. Wright, A. Ganesh, Z. Zhou, H. Mobahi, and Y. Ma,
“Toward a practical face recognition system: Robust alignment and
illumination by sparse representation,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 34, no. 2, pp. 372–386, 2012.
[3] A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Fast l1-minimization
algorithms and an application in robust face recognition: A review,” in
ICIP, 2010, pp. 1849–1852.

[4] M. Yang, L. Zhang, J. Yang, and D. Zhang, “Robust sparse coding for

face recognition,” in CVPR, 2011.

[5] J. Ma, J. Zhao, Y. Ma, and J. Tian, “Non-rigid visible and infrared face
registration via regularized gaussian ﬁelds criterion,” Pattern Recognit.,
vol. 48, no. 3, pp. 772–784, 2015.

[6] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and S. Yan, “Sparse
representation for computer vision and pattern recognition,” Proceedings
of the IEEE, vol. 98, no. 6, pp. 1031–1044, 2010.

[7] X. Tan, S. Chen, Z.-H. Zhou, and F. Zhang, “Face recognition from a
single image per person: A survey,” Pattern recognition, vol. 39, no. 9,
pp. 1725–1745, 2006.

[8] Q. Shi, A. Eriksson, A. van den Hengel, and C. Shen, “Is face
recognition really a compressive sensing problem?” in CVPR, 2011.
[9] L. Zhang, M. Yang, and X. Feng, “Sparse representation or collaborative

representation: Which helps face recognition?” in ICCV, 2011.

[10] P. Zhu, L. Zhang, Q. Hu, and S. C. Shiu, “Multi-scale patch based
collaborative representation for face recognition with margin distribution
optimization,” in ECCV, 2012.

[11] J. Jiang, R. Hu, C. Liang, Z. Han, and C. Zhang, “Face image super-
resolution through locality-induced support regression,” Signal Process.,
vol. 103, pp. 168–183, 2014.

[12] J. Jiang, C. Chen, J. Ma, Z. Wang, Z. Wang, and R. Hu, “Srlsp: A face
image super-resolution algorithm using smooth regression with local
structure prior,” IEEE Trans. Multimedia, vol. 19, no. 1, pp. 27–40,
2017.

[13] W. Deng, J. Hu, and J. Guo, “Extended SRC: Undersampled face
recognition via intraclass variant dictionary,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1864–
1870, 2012.

[14] M. Yang, L. V. Gool, and L. Zhang, “Sparse variation dictionary learning
for face recognition with a single training sample per person,” in ICCV,
2013.

[15] L. Zhuang, A. Y. Yang, Z. Zhou, S. S. Sastry, and Y. Ma, “Single-
sample face recognition with image corruption and misalignment via
sparse illumination transfer,” in CVPR, 2013, pp. 3546–3553.

[16] L. Zhuang, T.-H. Chan, A. Y. Yang, S. S. Sastry, and Y. Ma, “Sparse
illumination learning and transfer for single-sample face recognition
with image corruption and misalignment,” International Journal of
Computer Vision, 2014.

[17] C.-P. Wei and Y.-C. F. Wang, “Undersampled face recognition via robust
auxiliary dictionary learning,” IEEE Transactions on Image Processing,
vol. 24, no. 6, pp. 1722–1734, 2015.

[18] W. Deng, J. Hu, and J. Guo, “In defense of sparsity based face

recognition,” in CVPR, 2013, pp. 399–406.

[19] M. Yang, L. Zhang, X. Feng, and D. Zhang, “Fisher discrimination

dictionary learning for sparse representation,” in ICCV, 2011.

[20] ——, “Sparse representation based ﬁsher discrimination dictionary
learning for image classiﬁcation,” International Journal of Computer
Vision, vol. 109, no. 3, pp. 209–232, 2014.

[21] L. Ma, C. Wang, B. Xiao, and W. Zhou, “Sparse representation for face
recognition based on discriminative low-rank dictionary learning,” in
CVPR, 2012.

[22] L. Li, S. Li, and Y. Fu, “Discriminative dictionary learning with low-
rank regularization for face recognition,” in Automatic Face and Gesture
Recognition (FG), 2013.

[23] ——, “Learning low-rank and discriminative dictionary for image clas-
siﬁcation,” Image and Vision Computing, vol. 32, no. 10, pp. 814–823,
2014.

[24] Y. Zhang, Z. Jiang, and L. S. Davis, “Learning structured low-rank

representations for image classiﬁcation,” in CVPR, 2013.

[25] Y. Chi and F. Porikli, “Connecting the dots in multi-class classiﬁcation:
From nearest subspace to collaborative representation,” in CVPR, 2012,
pp. 3602–3609.

[26] S. Gao, K. Jia, L. Zhuang, and Y. Ma, “Neither global nor local:
Regularized patch-based representation for single sample per person face
recognition,” International Journal of Computer Vision, 2014.

[27] S. Yan and H. Wang, “Semi-supervised learning by sparse representa-

tion,” in SDM, 2009, pp. 792–801.

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX

16

[28] R. He, W.-S. Zheng, B.-G. Hu, and X.-W. Kong, “Nonnegative sparse
coding for discriminative semi-supervised learning,” in CVPR, 2011, pp.
2849–2856.

[29] L. Zhuang, H. Gao, Z. Lin, Y. Ma, X. Zhang, and N. Yu, “Non-negative
low rank and sparse graph for semi-supervised learning,” in CVPR, 2012.
[30] L. Zhuang, S. Gao, J. Tang, J. Wang, Z. Lin, Y. Ma, and N. Yu,
“Constructing a non-negative low-rank and sparse graph with data-
adaptive features,” IEEE Transactions on Image Processing, 2015.
[31] X. Wang and X. Tang, “Bayesian face recognition based on gaussian

mixture models,” in ICPR, 2004.

[32] X. Zhu and A. B. Goldberg, Introduction to Semi-Supervised Learning.

Morgan & Claypool, 2009, ch. 3, pp. 21–35.

[33] J. Ma, H. Zhou, J. Zhao, Y. Gao, J. Jiang, and J. Tian, “Robust
feature matching for remote sensing image registration via locally linear
transforming,” IEEE Transactions on Geoscience and Remote Sensing,
vol. 53, no. 12, pp. 6469–6481, 2015.

[34] Y. Gao, J. Ma, J. Zhao, J. Tian, and D. Zhang, “A robust and outlier-
adaptive method for non-rigid point registration,” Pattern Analysis and
Applications, vol. 17, no. 2, pp. 379–388, 2014.

[35] J. Ma, J. Zhao, J. Tian, X. Bai, and Z. Tu, “Regularized vector ﬁeld
learning with sparse approximation for mismatch removal,” Pattern
Recognit., vol. 46, no. 12, pp. 3519–3532, 2013.

[36] J. Ma, J. Zhao, and A. L. Yuille, “Non-rigid point set registration by
preserving global and local structures,” IEEE Transactions on Image
Processing, 2016.

[37] M. Kan, S. Shan, Y. Su, D. Xu, and X. Chen, “Adaptive discriminant
learning for face recognition,” Pattern Recognition, vol. 46, no. 9, pp.
2497–2509, 2013.

[38] Y. Su, S. Shan, X. Chen, and W. Gao, “Adaptive generic learning for
face recognition from a single sample per person,” in CVPR, 2010.
[39] A. Martinez and R. Benavente, “The AR face database,” CVC, Tech.

Rep. 24, 1998.

[40] R. Gross, I. Matthews, J. F. Cohn, T. Kanade, and S. Baker, “Multi-PIE,”
Image and Vision Computing, vol. 28, no. 5, pp. 807–813, 2010.
[41] W. Gao, B. Cao, S. Shan, X. Chen, D. Zhou, X. Zhang, and D. Zhao,
“The cas-peal large-scale chinese face database and baseline evalua-
tions,” IEEE Transactions on Systems, Man and Cybernetics, Part A:
Systems and Humans, vol. 38, no. 1, pp. 149–161, 2008.

[42] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, “Labeled
faces in the wild: A database for studying face recognition in uncon-
strained environments,” University of Massachusetts, Amherst, Tech.
Rep. 07-49, October 2007.

[43] M. S. Asif and J. Romberg, “Dynamic updating for l1 minimization,”

IEEE Journal of selected topics in signal processing, 2010.

[44] D. Donoho and Y. Tsaig, “Fast solution of l1-norm minimization
problems when the solution may be sparse,” IEEE Transactions on
Information Theory, vol. 54, no. 11, pp. 4789–4812, 2008.

[45] M. Osborne, B. Presnell, and B. Turlach, “A new approach to variable
selection in least squares problems,” IMA journal of numerical analysis,
vol. 20, no. 3, p. 389, 2000.

[46] A. Martinez and A. C. Kak, “PCA versus LDA,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 23, no. 2, pp. 228–233,
2001.

[47] S. Cai, L. Zhang, W. Zuo, and X. Feng, “A probabilistic collaborative
representation based approach for pattern classiﬁcation,” in CVPR, 2012.
[48] C. M. Bishop, Pattern Recognition and Machine Learning, M. Jordan,
J. Kleinberg, and B. Sch¨olkopf, Eds. New York: Springer, 2006.
[49] C.-C. Chang and C.-J. Lin., “Libsvm : a library for support vector
machines.” ACM Transactions on Intelligent Systems and Technology,
vol. 2, no. 27, pp. 1–27, 2011.

[50] Y. Gao and A. L. Yuille, “Symmetry non-rigid structure from motion
for category-speciﬁc object structure estimation,” in ECCV, 2016.
[51] ——, “Exploiting symmetry and/or Manhattan properties for 3D object
structure estimation from single and multiple images,” in CVPR, 2017.
[52] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,”

in Proceedings of the British Machine Vision, 2015.

[53] Y. Sun, Y. Chen, X. Wang, and X. Tang, “Deep learning face rep-
resentation by joint identiﬁcation-veriﬁcation,” in Advances in Neural
Information Processing Systems, 2014, pp. 1988–1996.

[54] Y. Sun, X. Wang, and X. Tang, “Deeply learned face representations are
sparse, selective, and robust,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2015, pp. 2892–2900.

[55] D. Chen, X. Cao, F. Wen, and J. Sun, “Blessing of dimensionality: High-
dimensional feature and its efﬁcient compression for face veriﬁcation,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2013, pp. 3025–3032.

[56] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing
the gap to human-level performance in face veriﬁcation,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2014, pp. 1701–1708.

[57] G. B. Huang, M. Mattar, H. Lee, and E. Learned-Miller, “Learning to

align from scratch,” in NIPS, 2012.

[58] P. Viola and M. J. Jones, “Robust real-time face detection,” International
Journal of Computer Vision, vol. 57, no. 2, pp. 137–154, 2004.
[59] M. Yang, L. Zhang, and D. Zhang, “Efﬁcient misalignment-robust
representation for real-time face recognition,” in ECCV, 2012, pp. 850–
863.

[60] W. Deng, J. Hu, J. Lu, and J. Guo, “Transform-invariant pca: A uniﬁed
approach to fully automatic face alignment, representation, and recogni-
tion,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 36, no. 6, pp. 1275–1284, 2014.

[61] J. Ma, C. Chen, C. Li, and J. Huang, “Infrared and visible image fusion
transfer and total variation minimization,” Information

via gradient
Fusion, vol. 31, pp. 100–109, 2016.

[62] J. Ma, W. Qiu, J. Zhao, Y. Ma, A. L. Yuille, and Z. Tu, “Robust l2e
estimation of transformation for non-rigid registration.” IEEE Trans.
Signal Processing, vol. 63, no. 5, pp. 1115–1129, 2015.

Yuan Gao received the B.S. degree in biomedical
engineering and the M.S. degree in pattern recogni-
tion and intelligent systems from the Huazhong Uni-
versity of Science and Technology, Wuhan, China, in
2009 and 2012, respectively. He completed his Ph.D.
in Electronic Engineering, City University of Hong
Kong, Kowloon, Hong Kong, in 2016. Currently, he
is a computer vision researcher in Tencent AI Lab.
He was a visiting student with the Department of
Statistics, University of California, Los Angeles in
2015. His research interests include computer vision,

pattern recognition, and machine learning.

Jiayi Ma received the B.S. degree from the Depart-
ment of Mathematics, and the Ph.D. Degree from
the School of Automation, Huazhong University of
Science and Technology, Wuhan, China, in 2008 and
2014, respectively. From 2012 to 2013, he was an
exchange student with the Department of Statistics,
University of California, Los Angeles. He is now an
Associate Professor with the Electronic Information
School, Wuhan University, where he has been a Post-
Doctoral during 2014 to 2015. His current research
interests include in the areas of computer vision,

machine learning, and pattern recognition.

Alan L. Yuille received his B.A. in mathematics
from the University of Cambridge in 1976, and com-
pleted his Ph.D. in theoretical physics at Cambridge
in 1980 studying under Stephen Hawking. Following
this, he held a postdoc position with the Physics
Department, University of Texas at Austin, and the
Institute for Theoretical Physics, Santa Barbara. He
then joined the Artiﬁcial Intelligence Laboratory at
MIT (1982-1986), and followed this with a fac-
ulty position in the Division of Applied Sciences
at Harvard (1986-1995), rising to the position of
associate professor. From 1995-2002 Alan worked as a senior scientist at
the Smith-Kettlewell Eye Research Institute in San Francisco. From 2002-
2016 he was a full professor in the Department of Statistics at UCLA with
joint appointments in Psychology, Computer Science, and Psychiatry. In 2016
he became a Bloomberg Distinguished Professor in Cognitive Science and
Computer Science at Johns Hopkins University. He has over two hundred
peer-reviewed publications in vision, neural networks, and physics, and has co-
authored two books: Data Fusion for Sensory Information Processing Systems
(with J. J. Clark) and Two- and Three-Dimensional Patterns of the Face (with
P. W. Hallinan, G. G. Gordon, P. J. Giblin and D. B. Mumford); he also co-
edited the book Active Vision (with A. Blake). He has won several academic
prizes and is a Fellow of IEEE.

