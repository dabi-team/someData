A Hybrid Learner for Simultaneous Localization
and Mapping

Thangarajah Akilan, Member, IEEE, Edna Johnson, Japneet Sandhu, Ritika Chadha, Gaurav Taluja

1
2
0
2

n
a
J

4

]

O
R
.
s
c
[

1
v
8
5
1
1
0
.
1
0
1
2
:
v
i
X
r
a

Abstract—Simultaneous localization and mapping (SLAM) is
used to predict the dynamic motion path of a moving platform
based on the location coordinates and the precise mapping of the
physical environment. SLAM has great potential in augmented
reality (AR), autonomous vehicles, viz. self-driving cars, drones,
Autonomous navigation robots (ANR). This work introduces
a hybrid learning model that explores beyond feature fusion
and conducts a multimodal weight sewing strategy towards
improving the performance of a baseline SLAM algorithm.
It carries out weight enhancement of the front end feature
extractor of the SLAM via mutation of different deep networks’
top layers. At the same time, the trajectory predictions from
independently trained models are amalgamated to reﬁne the
location detail. Thus, the integration of the aforesaid early
and late fusion techniques under a hybrid learning framework
minimizes the translation and rotation errors of the SLAM
model. This study exploits some well-known deep learning
including ResNet18, ResNet34, ResNet50,
(DL) architectures,
ResNet101, VGG16, VGG19, and AlexNet
for experimental
analysis. An extensive experimental analysis proves that hybrid
learner (HL) achieves signiﬁcantly better results
than the
unimodal approaches and multimodal approaches with early or
late fusion strategies. Hence, it is found that the Apolloscape
dataset taken in this work has never been used in the literature
under SLAM with fusion techniques, which makes this work
unique and insightful.

Index Terms—SLAM, deep learning, hybrid learning

I. INTRODUCTION

SLAM is a technological process that enables a device
to build a map of the environment, at the same time, helps
compute the relative location on predeﬁned map. It can be used
for range of applications from self-driving vehicles (SDV) to
space and maritime exploration, and from indoor positioning
to search and rescue operations. The primary responsibility
of a SLAM algorithm is to produce an understanding of a
moving platform’s environment and the location of the vehicle
by providing the value of its coordinates; thus, improving the
formation of a trajectory to determine the view at a particular
instance. As a SLAM is one of the emerging technologies,
numerous implementations have been introduced but the DL-
based approaches surmount others by their efﬁciency in ex-
tracting the ﬁnest features and giving better results even in a
feature-scarce environment.

This study aims to improve the performance of a self-
localization module based on PoseNet [1] architecture through
the concept of hybrid learning that does a multimodal weight
mutation for enhancing the weights of a feature extractor layer
and reﬁnes the trajectory predictions using amalgamation of
multimodal scores. The ablation study is carried out on the

Apolloscape [2], [3], as per our knowledge, there has been no
research work performed on the self-localization repository of
the Apolloscape dataset, in which the proposed HL has been
evaluated extensively. The experimental analysis presented in
this work consists of three parts, in which initial two parts form
the base for the third. The ﬁrst part concentrates on an exten-
sive evaluation of several DL models, as feature extractors.
The second part analyzes of two proposed multimodal fusion
approaches: (i). an early fusion via layer weight enhancement
of feature extractor, and (ii). a late fusion via score reﬁnement
of the trajectory (pose) regressor. Finally, the third part aims
at the combination of early and late fusion models forming a
hybrid learner with addition or multiplication operation. Here,
the late fusion model harnesses ﬁve pretrained deep convo-
lutional neural networks (DCNNs), viz. ResNet18, ResNet34,
ResNet101. VGG16, and VGG19 as the feature extractor for
pose regressor module. While, the early fusion model and
the HL focuses on exploiting the best DCNNs, ResNet101
and VGG19 based on their individual performance on the
Apolloscape self-localization dataset.

it

is observed that

When analyzing the results of the early and late fusion
models,
the early fusion encompasses
14.842m of translation error and 0.673◦ of rotation error. On
the other hand, the late fusion achieves 9.763m of translation
error and 0.945◦ of rotation error. On analyzing the hybrid
learners, the additive hybrid learner (AHL) gets 10.400m of
translation error and 0.828◦ of rotation error, whereas the mul-
tiplicative hybrid learner (MHL) records 9.307m and 1.206◦
of translation and rotation errors, respectively. By fusing the
predictions of AHL and MHL called hybrid learner full-fusion
(HLFF) produces better results than all other models with
7.762m and 0.886◦ of translation rotation errors, respectively.
The rest of the paper is organized as follows. Section II
reviews relevant SLAM literature and provides basic detail
of the PoseNet, unimodality, and multimodality. Section III
elaborates the proposed hybrid learner including required pre-
processing operations. Section IV describes the experimental
setup and analyzes the obtained results from various models.
Section V concludes the research work.

II. BACKGROUND

A. SLAM

Simultaneous localization and mapping is an active research
domain in robotics and artiﬁcial intelligence (AI). It enables
a remotely automated moving vehicle to be placed in an
unknown environment and location. According to Whyte et
al. [4] and Montemerlo et al. [5], SLAM should build a

 
 
 
 
 
 
consistent map of this unknown environment and determine
the location relative to the map. Through SLAM, robots and
vehicles can be truly and completely automated without any
or minimal human intervention. But the estimation of maps
consists of various other entities, such as large storage issues,
precise location coordinates, which makes SLAM a rather
intriguing task, especially in the real-time domain.

Many researches have been done worldwide to determine
the efﬁcient method to perform SLAM. In [6], Montemerlo
et al. propose a model named FastSLAM, as an efﬁcient
solution to the problem. FastSLAM is a recursive algorithm
that calculates the posterior distribution spanning over au-
tonomous vehicle’s pose and landmark locations, yet, it scales
logarithmically with the total number of landmarks. This
algorithm relies on an exact factorization of the posterior into
a product of landmark distributions and a distribution over
the paths of the robot. The research on SLAM originates
on the work of Smith and Cheeseman [7] that propose the
use of the extended Kalman ﬁlter (EKF). It is based on the
notion that pose errors and errors in the map are correlated,
and the covariance matrix obtained by the EKF represents this
covariance. There are two main approaches for the localization
of an autonomous vehicle: metric SLAM and appearance-
this research focuses on the
based SLAM [1]. However,
appearance-based SLAM that is trained by giving a set of
visual samples collected at multiple discrete locations.

B. PoseNet

The neural network (NN) comprises of several intercon-
nected nodes and associated parameters,
like weights and
biases. The weights are adjusted through a series of trials and
experiments in the training phase so that the network can learn
and can be used to predict the outcomes at a later stage. There
are various kinds of NN’s available, for instance, Feed-forward
neural network (FFNN), Radial basis neural network (RBNN),
DCNN, Recurrent neural network (RNN), etc. Among them,
the DCNN’s have been highly regarded for the adaptability and
ﬁner interpretability with accurate and justiﬁable predictions
in applications range from ﬁnance to medical analysis and
from science to engineering. Thus, the PoseNet model for
SLAM shown in Fig. 1, harness the DCNN to be ﬁrm against
difﬁcult lighting, blurredness, and varying camera instincts [1].
Figure 1 depicts the underlying architecture of the PoseNet. It
subsumes a front-end with a feature extractor and a back-end
with a regression subnetwork. The feature extractor can be a
pretrained DCNN, like ResNet34, VGG16, or AlexNet. The
regression subnetwork consists of three stages: a dropout, an
average pooling, a dense layer interconnected, sequentially.
It receives the high dimensional vector from the feature
extractor. Through the average pooling and dropout layers,
it is then reduced to a lower dimension for generalization
and faster computation [8]. The predicted poses are in Six-
degree of freedom (6-DoF), which deﬁne the six parameters
in translation and rotation [1]. The translation consists of
forward-backward, left-right, and up-down parameters forming
the axis of 3D space as x − axis, y − axis, and z − axis,

Front-end
Feature Extractor:
A pretrained CNN Dropout→ Pooling→ Dense

Pose Regressor

Back-end

Output: Poses
Translation & Rotation

Fig. 1: PoseNet Architecture Subsuming a Feature Extractor
and a Pose Regressor Subnetwork.

respectively. Likewise, the rotation includes yaw, pitch, and
roll parameters of the same 3D space noted as normal−axis,
transverse − axis, and longitudinal − axis, respectively.

Then, these six core parameters are converted to seven
coordinates: x1, x2, and x3 of translation coordinates, and
y1, y2, y3, y4 of rotation coordinates. It is because the actual
rotation poses are in Euler angles. Thus, a pre-processing
operation converts the Euler angles into quaternions. The
quaternions are the set of four values (xo, y1, y2 and y3),
where xo represents a scalar rotation of the vector - y1, y2
and y3. This conversion is governed by the expressions given
in Eq. (1) - (4).

√

x0 = (

1 + c1c2 + c1c3 − s1s2s3 + c2c3)/2,

y1 = (c2s3 + c1s3 + s1s2c3)/4x0,

y2 = (s1c2 + s1c3 + c1s2s3)/4x0,

y3 = (−s1s3 + c1s2c3 + s2)/4x0,

(1)

(2)

(3)

(4)

where c1 = cos(roll/2), c2 = cos(yaw/2), c3 = cos(pitch/2),
s1 = sin(roll/2), s2 = sin(yaw/2), and s3 = sin(pitch/2).

The pose regressor subnetwork is to be trained to minimize
the translation and rotation errors. These errors are combined
into a single objective function, Lβ as deﬁned in Eq. (5) [9].

Lβ(I) = Lx(I) + βLq(I),

(5)

where Lx, Lq are the losses of translation and rotation
respectively, and I is the input vector representing the discrete
location in the map. β is a scaling factor that is used to balance
both the losses and calculated using homoscedastic uncertainty
that combines the losses as deﬁned in (6).

Lσ(I) =

Lx(I)
ˆσ2
x

+ log ˆσ2

x +

Lq(I)
ˆσ2
q

+ log ˆσ2
q ,

(6)

where ˆσx and ˆσq are the uncertainties for translation and
rotation respectively. Here, the regularizers log ˆσ2
x and log ˆσ2
q
the values from becoming too big [9]. It can be
prevent
calculated using a more stable form as in Eq. (7), which is
very handy for training the PoseNet.

Lσ(I) = Lx(I)−ˆsx + ˆsx + Lq(I)−ˆsq + ˆsq,

(7)

where the learning parameter s = log ˆσ2. Following [9], in
this work, ˆsx and ˆsq are set to 0 and −3.0, respectively.

C. The Front-end Feature Extractor

As discussed earlier the PoseNet take advantage of transfer
learning (TL), whereby it uses pretrained DCNN as feature
extractor. TL differs from traditional learning, as, in latter, the
models or tasks are isolated and function separately. They do
not retain any knowledge, whereas TL learns from the older
problem and leverages the new set of problems [10]. Thus, in
this work, versions of ResNet, versions of VGG, and AlexNet
are investigated. Some basic information of these DCNN’s are
given in the following subsections.

1) AlexNet:

It was the winner in 2012 ImageNet Large
Scale Visual Recognition Competition (ILSVRC’12) with a
breakthrough performance [11]. It consists of ﬁve convolution
(Conv) layers taking up to 60 million trainable parameters
and 650,000 neurons making it one of the huge models in
DCNN’s. The ﬁrst and second Conv layers are followed by a
max pooling operation. But the third, fourth, and ﬁfth Conv
layers are connected directly, and the ﬁnal stage is a dense
layer and a thousand-way Softmax layer. It was the ﬁrst time
for a DCNN to adopt rectiﬁed linear units (ReLU) instead of
the tanh activation function and to use of dropout layer to
eradicate overﬁtting issues of DL.

2) VGG (16, 19): Simonayan and Zisserman [12] proposed
the ﬁrst version of VGG network named VGG16 for the
ILSVRC’14. It stood 2nd in the image classiﬁcation challenge
with the error of top-5 as 7.32%. VGG16 and 19 consist of
16 and 19 Conv layers, respectively with max pooling layer
after set of two or three Conv layers. It comprises of two
fully connected layers and a thousand-way Softmax top layer,
similar to AlexNet. The main drawbacks of VGG models are
high training time and high network weights.

3) ResNet (18, 34, 50, 101): ResNet18 [13] was intro-
duced to compete in ILSVRC’15, where it outperformed
other models, like VGG, GoogLeNet, and Inception. All the
ResNet models used in this work are trained on the ImageNet
database that consists more than million images. Experiments
have depicted that even though ResNet18 is a subspace of
ResNet34, yet
its performance is more or less equivalent
to ResNet34. ResNet18, 34, 50, and 101 consist of 18, 34,
50, and 101 layers, respectively. This paper, ﬁrstly, evaluates
the performance of the PoseNet individually using the above
mentioned ResNet models besides other feature extractors.
Consequently, it chooses the best ones to be used in the fusion
modalities and in the hybrid learner, thereby, establishing a
good trade-off between depth and performance. The ResNet
models constitute of residual blocks, whereas ResNet18 and
34 have two stack of deep residual blocks, while ResNet50
and 101 have three deep residual blocks. A residual block
subsumes ﬁve convolutional stages, which is followed by
average pooling layer. Hence, each ResNet model has a fully
connected layer followed by a thousand-way Softmax layer to
generate a thousand-class labels.

D. Multimodal Feature Fusion

There are many existing researches that have taken the
advantage of various strategies for feature extraction and

fusion. For an instance, Xu et al. [14] modify the Inception-
ResNet-v1 model to have four layers followed by a fully
connected layer in order to reduce effect of overﬁtting, as
their problem domain has less number of samples and ﬁfteen
classes. On the other hand, Akilan et al. [15] continue with
a TL technique in the feature fusion, whereby they extract
features using multiple DCNN’s, namely AlexNet, VGG16
and Inception-v3. As these extractors will result into a varied
feature dimensions and sub-spaces, feature space transforma-
tion and energy-level normalisation are performed to embed
the features into a common sub-space using dimensionality
reduction techniques like PCA. Finally, the features are fused
together using fusion rules, such as concatenation, feature
product, summation, mean value pooling, and maximum value
pooling.

Fu et al. [16] also consider the dimension normalization
techniques to produce a consistently uniform dimensional
feature space. It presents supervised and unsupervised learn-
ing sub-space learning method for dimensionality reduction
and multimodal feature fusion. The work also introduces a
new technique called, Tensor-Based discriminative sub-space
learning. This technique gives better results, as it produces
the ﬁnal fused feature vector of adequate length, i.e., the long
vector if the number of features are too large and the shorter
vector if number of features are small. Hence, Bahrampour et
al. [17] introduce a multimodal task-driven dictionary learning
algorithm for information that is obtained either homogenously
or heterogeneously. These multimodal task-driven dictionaries
produce the features from the input data for classiﬁcation
problems.

E. Hybrid Learning

Sun et al. [18] proposes a hybrid convolutional neural
network for face veriﬁcation in wild conditions. Instead of
extracting the features separately from the images, the features
from the two images are jointly extracted by ﬁlter pairs. The
extracted features are then processed through multiple layers
of the DCNN to extract high-level and global features. The
higher layers in the DCNN discussed in their work locally
share the weights, which is quite contrary to conventional
CNNs. In this way, feature extraction and recognition are
combined under the hybrid model.

Similarly, Pawar et al. [19] develope an efﬁcient hybrid
approach involving invariant scale features for object recogni-
tion. In the feature extraction phase, the invariant features, like
color, shape, and texture are extracted and subsequently fused
together to improve the recognition performance. The fused
feature set is then fed to the pattern recognition algorithms,
such as support vector machine (SVM), discriminant canonical
correlation, and locality preserving projections, which likely
produces either three distinct or identical numbered false
positives. To hybridize the process entirely, a decision module
is developed using NN’s that takes in the match values from
the chosen pattern recognition algorithm as input, and then
returns the result based on those match values.

Fig. 2: Operational Flow of the Proposed Hybrid Learner with a Weight Sewing Strategy and a Late Fusion Phase of the
Predicted Poses Towards Improving the Localization Capability of a SLAM Model - PoseNet.

However, the hybrid learner (Fig. 2) introduced in this work
is more unique and insightful than the existing hybrid fusion
approaches. It focuses on enhancing and updating the weights
of the pretrained unimodals before using them as front-end
feature extractors of the PoseNet. Besides that it not only does
a mutation of multimodal weights of the feature extraction
dense layer, but also fuses the predicted scores of the pose
regressor.

fusion models obtained through fusing the dense layer weights
of ResNet101 and VGG19 by addition or multiplication. In late
fusion, the predicted scores of multiple pose regressors with
the weight enhanced above feature extractors are amalgamated
using average ﬁltering to achieve better results.

B. Preprocessing

III. PROPOSED METHOD

A. Hybrid Weight Swing and Score Fusion Model

The Fig. 2 shows a detailed ﬂow diagram of the hybrid
learner. It consists of two parts, wherein the ﬁrst part (Step
1 - weight enhancement) carries out an early fusion by layer
weight enhancement of the feature extractor and the second
part (Step 3) does a late fusion via score reﬁnement of the
models involved in the early fusion. The two best feature
extractors chosen for early fusion based on their individual per-
formances are used for forming the hybrid learner. The early

Before passing the images and poses to the PoseNet model,
it is required to preprocess the data adequately. The prepro-
cessing involves checking the consistency of the images, resiz-
ing and center cropping of the images, extraction of mean and
standard deviation, and normalization of the poses. The images
are resized to 260×260, and center cropped to 250×250. The
translation is used to get the minimum, maximum, mean and
standard deviation. The rotation values are read as Euler angles
which suffer from wrap around inﬁnities, gimbal lock and
interpolation problems. To overcome these challenges, Euler
rotations is converted to quaternions [20].

C. Multimodal Weight Sewing via Early Fusion (EF)

Model Name

The preprocessed data is fed to the feature extractors:
ResNet101 and VGG19. These two models are selected based
on their
individual performance on the Apolloscape test
dataset. Using these two feature extractors the PoseNet has
produced minimum translation and rotation errors, as recorded
in Table I. The weights of the top feature extracting dense
layers of the two feature extractors are fused via addition or
multiplication operation. The fused values are used to update
the weights of the respective dense layer of the ResNet101
and VGG19 feature extractors. The updated models are then
used as new feature extractors for the regressor subnetwork.
Then, the regressor is trained on the training dataset.

D. Pose Reﬁnement via Late Fusion (LF)

The trained models with the updated ResNet101 and
VGG19 using early fusion of multiplication and addition
operations are moved onto the late fusion phase as shown
in Step 3 in Fig. 2. Where,
the loaded weight enhanced
the poses for
early fusion models, simultaneously predict
each input visual. The predicted scores from these models
(in this case, ResNet101 and VGG19) are amalgamated with
average ﬁltering. This way of fusion is denoted as AHL.
Similarly, the predicted scores of the early fusion models can
be reﬁned using multiplication and it is denoted as MHL.
Finally, the predicted scores of the four early fusion models
using addition and multiplication are fused together using
average mathematical operation to achieve the predicted scores
for the full hybrid fusion model stated earlier (Section I) in
the paper as HLFF. These predicted poses are then compared
with the ground truth poses to calculate the mean and median
of translation and rotational errors.

IV. EXPERIMENTAL SETUP AND RESULTS

A. Dataset

Apolloscape dataset

is used for many computer vision
tasks related to autonomous driving. The Apolloscape dataset
consists of modules including instance segmentation, scene
parsing, lanemark parsing and self-localization that are used
to understand and make a vehicle to act and reason accord-
ingly [2], [3]. The Apolloscape dataset for self-localization is
made up of images and poses on different roads at discrete
locations. The images and the poses are created from the
recordings of the videos. Each record of the dataset has
multiple images and poses corresponding to every image. The
road chosen for the ablation study of this research is zpark. It
consists of a total of 3000 stereo vision road scenes. For each
image, there is a ground truth of poses with 6-DoF. From this
entire dataset, a mutually exclusive training and test sets are
created with the ratio of 3 : 1.

B. Evaluation Metric

Measuring the performance of the machine learning model
is pivotal to comparing the various CNN models. Since every
CNN model is trained and tested on different datasets with
varied hyperparameters, it is necessary to choose the right

M1 - ResNet18
M2 - ResNet34
M3 - ResNet50
M4 - ResNet101
M5 - VGG16
M6 - VGG19
M7 - AlexNet
M8 - LF
M9 - AEFResN et101
M10 - MEFResN et101
M11 - AEFV GG19
M12 - MEFV GG19
M13 - AHL
M14 - MHL
M15 - HLFF

Median
et(m)
21.194
20.990
18.583
16.227
17.150
16.820
46.992
9.763
14.870
14.842
11.047
10.730
10.400
9.307
7.762

Mean
et(m)
24.029
23.597
20.803
19.427
21.571
19.935
53.004
10.561
18.256
18.013
13.840
14.181
12.193
11.420
8.829

Median
er(◦)
0.778
0.673
0.903
0.966
1.079
0.899
4.282
0.945
0.673
0.779
0.742
0.756
0.828
1.206
1.008

Mean
er(◦)
0.900
0.824
1.434
1.230
1.758
1.378
7.177
4.645
0.784
0.977
1.024
1.141
5.155
5.455
4.618

MAPST
(s)
0.092
0.093
0.095
0.098
0.103
0.111
0.108
0.146
0.134
0.131
0.137
0.135
0.142
0.142
0.144

TABLE I: Performance Analysis of Various Models: et -
translation error, er - rotation error, MAPST - mean average
per sample processing time.

evaluation metric. As the domain of this work is a regression
problem, the mean absolute error (MAE) is used to measure
the performance of the set of models ranging from unimodals
to the proposed hybrid learner. MAE is a linear score, which is
calculated as an average of the absolute difference between the
target variables and the predicted variables using the formula
given in Eq. (8).

M AE =

1
n

n
(cid:88)

i=1

| xi − x |,

(8)

where, n is the total number of samples in the validation
dataset, xi and x are the predicted and ground truth poses,
respectively. Since, it is an average measure, it implies that
all the distinct values are weighted equally ignoring any bias
involvement.

C. Performance Analysis

This Section elaborates the results obtained from each of

the model introduced earlier in this paper.

1) Translation and Rotation Errors: Table I tabulates the
the PoseNet with various front-end uni-
performance of
model and multimodal feature extractors, along with the
proposed hybrid learners. The results in this Table can be
described in three subdivisions. The primary section from
M1 to M7 is the outcomes of the unimodal PoseNet with
unimodality-based feature extractors. The subsequent section
extending from M8 to M12 depicts the performances of
ﬁve multimodality-based learners. M8 represents late fusion
(LF), M9 (AEFResN et101) and M10 (MEFResN et101) represent
early fusion on ResNet101 as feature extractor with addition
and multiplication, respectively. M11 (AEFV GG19) and M12
(MEFV GG19) are the results for early fusion on VGG19 with
addition and multiplication, respectively. The third section
consists of proposed hybrid learners, where M13, M14, and
M15 stand for AHL that combines the early fusion models,
M9 and M11, MHL, which combines the early fusion models,

Model Name

LF
EFResN et
EFV GG
HLFF

Improvement in
er (%)
et (%)
3
37
31
6
24
31
−4
50

Timing
Overhead (ms)
48
36
39
46

TABLE II: Performance Improvement of Proposed Hybrid
Learner When Compared to the Baseline PoseNet with
ResNet101 as Front-end.

and VGG19 give the best two outcomes among others. In
terms of translation error, the late fusion model shows better
performance than unimodality-based learners, but not good as
compared to the early fusion model. Let’s pick the ResNet101-
based PoseNet as baseline model for rest of the comparative
analysis because it has the best performance amongst the all
the unimodals. Here, the late fusion shows a 66% decrease
in the translation’s median errors and an 84% decrease in the
translation’s mean errors when compared to ResNet101 (M4).
The median of rotation errors in the late fusion model shows
a decrease of 2% but the mean of rotation errors increase by
73%.

it

is seen that

On comparison of the baseline model (M4) with the early
fusion model using addition having ResNet101 as a feature
extractor (M9),
there is a 9% decrease in
translation’s median error and a 6% decrease in translation’s
mean error. On the other hand, the median of rotation error
shows a 43% decrease and mean of rotation error shows a 41%
decrease. The comparison with the early fusion model (M10)
using multiplication on VGG19 exhibits a 51% decrease in
translation’s median error and a 37% decrease in translation’s
mean error. While the rotation errors drop in median value by
28% and in the mean value by 8%.

(a) Error in Terms of Translation.

(b) Error in Terms of Rotation.

It is evident from the Table I that hybrid learners show
much better performance than the unimodal and early fusion
models. The hybrid learner using average ﬁltering shows a
109% decrease in the translation’s median errors and a 120%
decrease in the translation’s mean error when compared to the
ResNet101-based PoseNet. While for rotation errors, there is
a 4% increase in the median and a 73% increase in the mean.
In holistic analysis, it is observed that the late fusion shows
an improvement of 37% in translation and 3% in rotation.
On considering the early fusion model using ResNet as a
feature extractor, the translation shows 6% improvement while
rotation shows 31%. The improvement of the early fusion
using VGG19 as a feature extractor in terms of translation
and rotation is 31% and 24%, respectively. It is quite evident
from the Table II that the proposed HL has a negligible low
results for rotation with a decrease of 4% nevertheless, it is
the best model considering a huge improvement in translation
by 50% across all the modals.

D. Timing Analysis

The timing analysis is conducted on a machine that uses
an Intel Core i5 processor that uses the Google Colaboratory

(c) Average Processing Time per Sample.

Fig. 3: Performance Analysis of All Different Models.

M10 and M12, and the HLFF obtained after averaging the
predicted scores of the four models, M9, M10, M11, and M12.
The results are computed as the mean and median values
of translation and rotation errors. The translation errors are
measured in terms of meters (m), while the rotation error
is measured in degrees (◦). Considering the unimodal-based
PoseNet implementation, it is very apparent that ResNet101

[5] M. Montemerlo, S. Thrun, D. Koller, B. Wegbreit, et al., “Fastslam 2.0:
An improved particle ﬁltering algorithm for simultaneous localization
and mapping that provably converges,” in IJCAI, pp. 1151–1156, 2003.
[6] M. Montemerlo, S. Thrun, D. Koller, B. Wegbreit, et al., “Fastslam: A
factored solution to the simultaneous localization and mapping problem,”
Aaai/iaai, vol. 593598, 2002.

[7] R. C. Smith and P. Cheeseman, “On the representation and estimation
of spatial uncertainty,” The international journal of Robotics Research,
vol. 5, no. 4, pp. 56–68, 1986.

[8] F. Walch, C. Hazirbas, L. Leal-Taix´e, T. Sattler, S. Hilsenbeck, and
lstms,” CoRR,

D. Cremers, “Image-based localization with spatial
vol. abs/1611.07890, 2016.

[9] A. Kendall and R. Cipolla, “Geometric loss functions for camera pose
regression with deep learning,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 5974–5983, 2017.

[10] M. E. Taylor and P. Stone, “Transfer learning for reinforcement learning
domains: A survey,” Journal of Machine Learning Research, vol. 10,
no. Jul, pp. 1633–1685, 2009.

[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, pp. 1097–1105, 2012.

[12] K. Simonyan and A. Zisserman, “Very deep convolutional neural net-
works for large-scale image recognition,” in The International Confer-
ence on Learning Representations, 2015, pp. 1–5, 2015.

[13] R. S. Paolo Napoletano, Flavio Piccoli, “Anomaly detection in nanoﬁ-

brous materials by cnn-based similarity,” MDPI, 2018.

[14] J. Xu, Y. Zhao, J. Jiang, Y. Dou, Z. Liu, and K. Chen, “Fusion model
based on convolutional neural networks with two features for acoustic
scene classiﬁcation,” in Proc. of the Detection and Classiﬁcation of
Acoustic Scenes and Events 2017 Workshop (DCASE2017), Munich,
Germany, 2017.

[15] T. Akilan, Q. M. J. Wu, and H. Zhang, “Effect of fusing features from
multiple dcnn architectures in image classiﬁcation,” The Institution of
Engineering and Technology, Feb 2018.

[16] Y. Fu, L. Cao, G. Guo, and T. S. Huang, “Multiple feature fusion by
subspace learning,” in Proceedings of the 2008 international conference
on Content-based image and video retrieval, pp. 127–134, 2008.
[17] S. Bahrampour, N. Nasrabadi, A. Ray, and W. Jenkins, “Multimodal
task-driven dictionary learning for image classiﬁcation,” The IEEE
Transactions on Image Processing, 2015.

[18] Y. Sun, X. Wang, and X. Tang, “Hybrid deep learning for face veriﬁca-
tion,” in Proceedings of the IEEE international conference on computer
vision, pp. 1489–1496, 2013.

[19] V. Pawar and S. Talbar, “Hybrid machine learning approach for object
recognition: Fusion of features and decisions,” Machine Graphics and
Vision, vol. 19, no. 4, pp. 411–428, 2010.

[20] P. Bouthellier, “Rotations and orientations in r3,” 27th International

Conference on Technology in Collegiate Mathematics, vol. 27, 2015.

having a GPU chip Tesla K80 with 2496 CUDA cores, a
hard disk space of 319GB and 12.6GB RAM. Table I shows
the mean average processing time calculated for processing a
batch of ten samples.

As seen from the Table I and Figure 3, the fusion models
which involve early, late, and hybrid learner take slightly extra
time compared to the unimodality-based baseline PoseNet.
The late fusion model (M8) takes more processing time in
comparison to all the other models, as it uses ﬁve pretrained
modalities, which are trained and tested individually, thereby,
increasing the time overhead. The early fusion models also
show an increase in the processing time in comparison to the
unimodels but lesser than the late fusion model, as training the
pretrained model after weight enhancement takes more time.
The hybrid learners also show the same trend because of the
underlying fact that it is a combination of the early and late
fusion methods. These models employ weight enhanced early
fusion models adding to the time overhead, besides fusing the
scores from different models after validation.

Note that the hyper-parameters have been ﬁxed throughout
the experimental analysis on various models to avoid uncer-
tainties in the comparative study. The learning rate used in
all the models is 0.01, dropout rate for the dropout layer of
the PoseNet is set to 0.5, and the batch size during training
is ﬁxed to 34. Hence, every model is trained for 1000 epochs
with Adam optimizer.

V. CONCLUSION
This work introduces a the hybrid learner to improve the
localization accuracy of a pose regressor model for SLAM.
The hybrid learner is a combination of multimodal early and
late fusion algorithms to harness the best properties of the
both. The extensive experiments on the Apolloscape self-
localization dataset show that the proposed hybrid leaner is
capable of reducing the translation error nearly by a 50%
decrease, although the rotation error gets worse by a negligible
4% when compared to unimodal PoseNet with ResNet101 as
a feature extractor.

Thus, the future work aims at minimizing the rotation errors

and overcoming the little overhead in the processing time.

ACKNOWLEDGMENT
This work acknowledges the Google for generosity of
providing the HPC on the Colab machine learning platform
and the organizer of Apollo Scape dataset.

REFERENCES

[1] A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional
network for real-time 6-dof camera relocalization,” in Proceedings of
the IEEE international conference on computer vision, pp. 2938–2946,
2015.

[2] P. Wang, R. Yang, B. Cao, W. Xu, and Y. Lin, “Dels-3d: Deep
localization and segmentation with a 3d semantic map,” in CVPR,
pp. 5860–5869, 2018.

[3] P. Wang, X. Huang, X. Cheng, D. Zhou, Q. Geng, and R. Yang, “The
apolloscape open dataset for autonomous driving and its application,”
IEEE transactions on pattern analysis and machine intelligence, 2019.
[4] T. Bailey and H. Durrant-Whyte, “Simultaneous localization and map-
ping (slam): Part ii,” IEEE robotics & automation magazine, vol. 13,
no. 3, pp. 108–117, 2006.

