Draft version February 26, 2020
Typeset using LATEX default style in AASTeX62

0
2
0
2

b
e
F
2
2

]

R
S
.
h
p
-
o
r
t
s
a
[

1
v
3
5
9
0
1
.
2
0
0
2
:
v
i
X
r
a

Predicting Coronal Mass Ejections Using SDO/HMI Vector Magnetic Data Products
and Recurrent Neural Networks
Hao Liu,1, 2 Chang Liu,1, 3, 4 Jason T. L. Wang,1, 2 and Haimin Wang1, 3, 4

1Institute for Space Weather Sciences, New Jersey Institute of Technology, University Heights, Newark, NJ 07102-1982, USA
hl422@njit.edu, chang.liu@njit.edu, wangj@njit.edu, haimin.wang@njit.edu
2Department of Computer Science, New Jersey Institute of Technology, University Heights, Newark, NJ 07102-1982, USA
3Big Bear Solar Observatory, New Jersey Institute of Technology, 40386 North Shore Lane, Big Bear City, CA 92314-9672, USA
4Center for Solar-Terrestrial Research, New Jersey Institute of Technology, University Heights, Newark, NJ 07102-1982, USA

ABSTRACT

We present two recurrent neural networks (RNNs), one based on gated recurrent units and the other
based on long short-term memory, for predicting whether an active region (AR) that produces an M- or
X-class ﬂare will also produce a coronal mass ejection (CME). We model data samples in an AR as time
series and use the RNNs to capture temporal information of the data samples. Each data sample has
18 physical parameters, or features, derived from photospheric vector magnetic ﬁeld data taken by the
Helioseismic and Magnetic Imager (HMI) on board the Solar Dynamics Observatory (SDO). We survey
M- and X-class ﬂares that occurred from 2010 May to 2019 May using the Geostationary Operational
Environmental Satellite’s X-ray ﬂare catalogs provided by the National Centers for Environmental
Information (NCEI), and select those ﬂares with identiﬁed ARs in the NCEI catalogs. In addition,
we extract the associations of ﬂares and CMEs from the Space Weather Database Of Notiﬁcations,
Knowledge, Information (DONKI). We use the information gathered above to build the labels (positive
versus negative) of the data samples at hand. Experimental results demonstrate the superiority of our
RNNs over closely related machine learning methods in predicting the labels of the data samples. We
also discuss an extension of our approach to predict a probabilistic estimate of how likely an M- or
X-class ﬂare will initiate a CME, with good performance results. To our knowledge this is the ﬁrst
time that RNNs have been used for CME prediction.

Keywords: Sun: activity − Sun: ﬂares − Sun: coronal mass ejections (CMEs)

1. INTRODUCTION

Coronal mass ejections (CMEs) are intense bursts of magnetic ﬂux and plasma that are ejected from the Sun into
interplanetary space (Lin & Forbes 2000). They are often associated with solar ﬂares and originated from active
regions (ARs) on the Sun’s photosphere where magnetic ﬁelds are strong and evolve rapidly. Major CMEs and their
associated ﬂares can cause severe inﬂuences on the near-Earth environment, resulting in potentially life-threatening
consequences (Baker et al. 2004). Therefore, substantial eﬀorts are being invested on developing new technologies for
early detection and forecasting of ﬂares and CMEs (Bobra & Ilonidis 2016; Inceoglu et al. 2018).

Both ﬂares and CMEs are believed to be magnetically-driven events; evidence shows that they may constitute
diﬀerent manifestations of the same physical process (Harrison 1995; Berkebile-Stoiser et al. 2012; Gosling 2013).
However, solar observations over the past few decades have clearly indicated that there may not be a one-to-one
correspondence between ﬂares and CMEs, and their relationship is still under active investigation (see, e.g., Yashiro
& Gopalswamy 2008; Kawabata et al. 2018). Much eﬀort has been devoted to analyzing the structural properties of
coronal magnetic ﬁelds, which may play an important role in determining whether an eruption evolves into a CME or
remains as a conﬁned ﬂare (T¨or¨ok & Kliem 2005; DeVore & Antiochos 2008; Liu 2008; Baumgartner et al. 2018). In
the meantime, many researchers have investigated the relationship between CME productivity and the features of the
photospheric magnetic ﬁeld of ﬂare-productive ARs where the features can be directly derived from photospheric vector
magnetograms. For example, Qahwaji et al. (2008) used properties such as ﬂare duration along with machine learning
algorithms to predict whether a ﬂare is likely to initiate a CME. Bobra & Ilonidis (2016) used 18 physical features,
or predictive parameters, derived from the photospheric vector magnetic ﬁeld data provided by the Helioseismic and

 
 
 
 
 
 
2

Liu et al.

Magnetic Imager (HMI; Schou et al. 2010) on board the Solar Dynamics Observatory (SDO; Pesnell et al. 2011)
to forecast whether a CME would be associated with an M- or X-class ﬂare. The ﬂares are classiﬁed according to
the peak ﬂux (in watts per square meter, W/m2) of 1 to 8 ˚A X-rays as measured by the Geostationary Operational
Environmental Satellite (GOES ). These authors found that using a combination of six intensive parameters captured
most of the relevant information contained in the photospheric magnetic ﬁeld. Inceoglu et al. (2018) later developed
methods to forecast whether ﬂares would be associated with CMEs and solar energetic particles (SEPs). The authors
employed two machine learning algorithms, support vector machines (SVMs) and multilayer perceptrons (MLPs), and
showed that SVMs performed slightly better than MLPs in the forecasting task.

Machine learning is a non-physics-based technology used in predictive analytics. It is a subﬁeld of artiﬁcial intel-
ligence, which grants computers abilities to learn from the past data and make predictions on unseen future data
(Alpaydin 2016; Goodfellow et al. 2016). Many diﬀerent machine learning-based techniques have been developed for
solar ﬂare prediction (see, e.g., Liu et al. 2017; Florios et al. 2018; Jonas et al. 2018; Nishizuka et al. 2018; Liu et al.
2019). However, CME prediction has been mainly based on SVMs (Bobra & Ilonidis 2016) and MLPs (Inceoglu et al.
2018).

In this paper, we attempt to extend the work of Bobra & Ilonidis (2016) by proposing new machine learning
algorithms and applying the algorithms to SDO/HMI vector magnetic ﬁeld data to predict whether an AR that
produces an M- or X-class ﬂare will also produce a CME. The machine learning algorithms we explore here include
two kinds of recurrent neural networks (RNNs; Hopﬁeld 1982): a long short-term memory (LSTM) network (Hochreiter
& Schmidhuber 1997) and a gated recurrent unit (GRU) network (Cho et al. 2014). LSTM cells and GRUs diﬀer in the
number and type of gates employed in the networks—an LSTM cell has three gates (input, output and forget gates)
whereas a GRU has two gates (reset and update gates). RNNs can use their internal state (memory) and gates to
process sequences of inputs, which makes them suitable for tasks such as speech recognition, handwriting recognition
and time series forecasting (LeCun et al. 2015; Goodfellow et al. 2016). In a CME prediction task, the observations in
each AR form time series data, and hence RNNs work well in this task. To our knowledge, this is the ﬁrst time that
RNNs are used for CME prediction.

The rest of this paper is organized as follows. Section 2 describes our data collection scheme and predictive parameters
used in the study. Section 3 presents our RNN architectures and algorithms. Section 4 reports experimental results.
Section 5 concludes the paper.

2. DATA AND PREDICTIVE PARAMETERS

We adopt the data products, named Space-weather HMI Active Region Patches (SHARP; Bobra et al. 2014),
produced by the SDO/HMI team. These data were released at the end of 2012 (Bobra et al. 2014) and can be found
in the hmi.sharp data series at the Joint Science Operations Center (JSOC).1 The SHARP data series contains ARs
tracked throughout their lifetime and provides many physical parameters suitable for ﬂare/CME predictions.

We collect data samples at a cadence of 12 minutes where the data samples are retrieved from the hmi.sharp cea 720s
deﬁnitive data series on the JSOC website by using SunPy (SunPy Community et al. 2015). We use the same 18
features, or SHARP parameters, as described in Table 1 of Bobra & Ilonidis (2016) that characterize AR magnetic ﬁeld
properties for CME prediction. These 18 features or predictive parameters include MEANPOT (mean photospheric
magnetic free energy), SHRGT45 (fraction of area with shear > 45◦), TOTPOT (total photospheric magnetic free
energy density), USFLUX (total unsigned ﬂux), MEANJZH (mean current helicity), ABSNJZH (absolute value of the
net current helicity), SAVNCPP (sum of the modulus of the net current per polarity), MEANALP (mean characteristic
twist parameter), MEANSHR (mean shear angle), TOTUSJZ (total unsigned vertical current), TOTUSJH (total
unsigned current helicity), MEANGAM (mean angle of ﬁeld from radial), MEANGBZ (mean gradient of vertical ﬁeld),
MEANJZD (mean vertical current density), AREA ACR (area of strong ﬁeld pixels in the active region), R VALUE
(sum of ﬂux near polarity inversion line), MEANGBT (mean gradient of total ﬁeld) and MEANGBH (mean gradient
of horizontal ﬁeld). Because the features have diﬀerent units and scales, we normalize the feature values as done in
Liu et al. (2019). Data samples with incomplete features are excluded from our dataset (Bobra & Ilonidis 2016).

Our proposed RNNs require labeled training samples. We survey M- and X-class ﬂares that occurred in the period
between 2010 May and 2019 May, using the GOES X-ray ﬂare catalogs provided by the National Centers for Envi-
ronmental Information (NCEI), and select M- and X-class ﬂares with identiﬁed ARs in the NCEI ﬂare catalogs. As in

1 http://jsoc.stanford.edu/

Predicting CMEs Using SDO/HMI Vector Magnetic Data Products and RNNs

3

Table 1. Numbers of Positive and Negative Samples Collected for Diﬀerent Hours Used in This Study

12 hr

36 hr
Positive Negative Positive Negative Positive Negative Positive Negative Positive Negative

48 hr

24 hr

60 hr

Training
Testing

3,387
550

16,960
762

6,323
922

27,281
1,059

9,109
1,214

34,994
1,253

11,613
1,540

41,402
1,360

13,775
1,814

46,994
1,483

Bobra & Ilonidis (2016), ﬂares that are outside ± 70◦ of the central meridian during the GOES X-ray ﬂux peak time
are excluded from our dataset. Flares where the (1) absolute value of the radial velocity of SDO is larger than 3500
m s−1 or (2) HMI data are of low quality as described in Hoeksema et al. (2014) are also excluded from our dataset.
In this way, we exclude data with noise or low quality, and keep data of high quality in our study. In addition, we
extract data from a NASA Space Weather Research Center database called Space Weather Database Of Notiﬁcations,
Knowledge, Information (DONKI)2 to label whether or not any given ﬂare produced a CME. This yields a database
of 129 M- and X-class ﬂares that are associated with CMEs and 610 M- and X-class ﬂares that are not associated with
CMEs.

3. METHODOLOGY

3.1. Prediction Task

Following Bobra & Ilonidis (2016), we intend to use past observations of a ﬂaring AR to predict its future CME
productivity. Speciﬁcally, we want to solve the following binary classiﬁcation problem: will an AR that produces an
M- or X-class ﬂare within the next T hours also produce a CME associated with the ﬂare? We consider T ranging
from 12 to 60 in 12 hr intervals. These prediction times are commonly used by researchers (Ahmed et al. 2013; Bobra
& Ilonidis 2016; Inceoglu et al. 2018).

Our dataset contains data samples collected within the T hours prior to the peak time of an M- or X-class ﬂare
regardless of whether the ﬂare produces a CME. Those data samples collected within the T hours prior to the peak
time of an M- or X-class ﬂare that produces a CME belong to the positive class; the other data samples belong to the
negative class. Data samples collected in years 2010–2014 are used for training, and those in years 2015–2019 are used
for testing. Thus, the training set and test set are disjoint, and hence our proposed RNNs will make predictions on
ARs that they have never seen before. Table 1 summarizes the numbers of positive and negative samples for diﬀerent
T values used for training and testing respectively.

If a data sample is missing at some time point or if there are insuﬃcient data samples within the T hours prior to
the peak time of an M- or X-class ﬂare, we adopt a zero-padding strategy by adding synthetic data samples with zeros
for all feature values to yield a complete, non-gapped time-series dataset. This zero-padding method is used after
normalizing the feature values. Therefore, the zero-padding method does not aﬀect the normalization procedure. For
a given time point t and an AR that produces an M- or X-class ﬂare within the next T hours of t, the proposed RNNs
predict whether or not the ﬂare will initiate a CME.

3.2. Prediction Methods

We employ two kinds of RNNs: a GRU network (Cho et al. 2014; Goodfellow et al. 2016) and an LSTM network
(Hochreiter & Schmidhuber 1997; Goodfellow et al. 2016). A GRU contains three interactive parts including a memory
content, an update gate and a reset gate, as illustrated in Figure 1 where boldface is used for matrix notations. The
new memory content ht is updated by the previous memory content ht−1 and the candidate memory content ˜ht as
follows (Cho et al. 2014; Goodfellow et al. 2016):

ht = zt (cid:12) ht−1 + (1 − zt) (cid:12) ˜ht,

(1)

where the update gate zt that determines how much of the past information from previous time steps needs to be
passed to the future is calculated as follows (Cho et al. 2014; Goodfellow et al. 2016):

zt = σ(Wz · [ht−1, xt] + Bz),

(2)

2 http://kauai.ccmc.gsfc.nasa.gov/DONKI/

4

Liu et al.

Figure 1. Illustration of a GRU where zt is the update gate, rt is the reset gate, ht is the output vector (memory content)
and xt is the input vector to the GRU. The subscript t indexes the time step.

and the reset gate rt that determines how much of the past information to forget is computed as follows (Cho et al.
2014; Goodfellow et al. 2016):

rt = σ(Wr · [ht−1, xt] + Br).
(3)
Here xt represents the input vector at time step t. The candidate memory content ˜ht is computed as follows (Cho
et al. 2014; Goodfellow et al. 2016):

˜ht = tanh(Wh · [rt (cid:12) ht−1, xt] + Bh).

(4)

In the above equations, W and B contain weights and biases respectively, which need to be learned during training;
[.] denotes the concatenation of two vectors; σ(·) is the sigmoid function, i.e., σ(z) = 1
1+e−z ; tanh(·) is the hyperbolic
tangent function, i.e., tanh(z) = ez−e−z

ez+e−z ; (cid:12) denotes the Hadamard product (element-wise multiplication).

Our GRU network contains a GRU layer with m GRUs (in the study presented here, m is set to 20). We add an
attention layer with m neurons above the GRU layer to focus on information in relevant time steps as done in Liu
et al. (2019). We then add a fully connected layer with 100 neurons on top of the attention layer. Finally, the output
layer with one neuron, which is activated by the sigmoid function, produces predicted values. Our LSTM network is
similar to the GRU network except that the GRU layer is replaced by an LSTM layer with m LSTM cells. This is
reminiscent of the LSTM network presented in Liu et al. (2019).

Let xt represent the data sample collected at time point t. During training, for each time point t, we take m
consecutive data samples xt−m+1, xt−m+2, . . . , xt−1, xt from the training set and use the m consecutive data samples
to train the proposed RNNs including the GRU and LSTM networks. The label of these m consecutive data samples
is deﬁned to be the label of the last data sample xt. Thus, if xt belongs to the positive class, then the input sequence
xt−m+1, xt−m+2, . . . , xt−1, xt is deﬁned as positive; otherwise the sequence is deﬁned as negative. Because the data
samples are collected continuously at a cadence of 12 minutes and missing values are ﬁlled up by our zero-padding
strategy, the input sequence spans m

5 hours.

Also during training, we use a weighted cross entropy cost function for optimizing model parameters where the cost

function is computed as follows (Goodfellow et al. 2016):

J =

N
(cid:88)

n=1

ω0ynlog(ˆyn) + ω1(1 − yn)log(1 − ˆyn).

(5)

Here, N is the total number of sequences each having m consecutive data samples in the training set, ω0 and ω1
are the weights of the positive and negative classes respectively, which are derived by the ratio of the sizes of the
positive and negative classes with more weight given to the minority class.3 We use yn and ˆyn to denote the observed

3 Refer to the training data in Table 1. The minority class is the positive class.

Predicting CMEs Using SDO/HMI Vector Magnetic Data Products and RNNs

5

probability (which is equal to 1 if the nth sequence belongs to the positive class) and the estimated probability of the
nth sequence, respectively.

The proposed RNN methods are implemented in Python, TensorFlow and Keras. A mini-batch strategy (Goodfellow
et al. 2016) is used to achieve faster convergence during backpropagation. The optimizer used is RMSprop, which is
a method for gradient descent, where the learning rate is set to 0.001. The batch size is set to 256 and the number of
epochs is set to 20. The length of each input sequence, m, is set to 20, meaning that every time 20 consecutive data
samples are used as input to our RNNs. The hyperparameter values, the optimizer, and the cost function in Equation
(5) are chosen to maximize the TSS scores to be deﬁned in the experiments section.

During testing, to predict whether a given AR that produces an M- or X-class ﬂare within the next T hours of a
time point t will also produce a CME associated with the ﬂare, we take xt and its preceding m − 1 data samples, and
then feed the m consecutive test data samples xt−m+1, xt−m+2, . . . , xt−1, xt into the trained RNNs. The output of the
RNNs, i.e., the predicted result, is a scalar number with a value of 1 or 0, indicating xt is positive (i.e., the AR will
also produce a CME associated with the ﬂare) or xt is negative (i.e., the AR will not produce a CME associated with
the ﬂare). This value is determined by comparing the probability calculated by the sigmoid function in the output
layer of the RNNs with a threshold. If the probability is greater than or equal to the threshold, then xt is predicted to
be positive; otherwise xt is predicted to be negative. It should be pointed out that, the way we use the m consecutive
test data samples xt−m+1, xt−m+2, . . . , xt−1, xt to predict whether a given AR that produces an M- or X-class ﬂare
within the next T hours of a time point t will also produce a CME associated with the ﬂare is diﬀerent from the
previously published machine learning methods (Bobra & Ilonidis 2016), which used only the test data sample xt to
make the prediction.

4. RESULTS

4.1. Metrics and Experimental Setup

Given an AR that produces an M- or X-class ﬂare within the next T hours of a time point t and a data sample xt
observed at time point t, we deﬁne xt to be a true positive (TP) if our RNNs predict that xt is positive, and xt is
indeed positive, i.e., the M- or X-class ﬂare produces, or is associated with, a CME. We deﬁne xt to be a false positive
(FP) if our RNNs predict that xt is positive while xt is actually negative, i.e., the M- or X-class ﬂare does not produce,
or is not associated with, a CME. We say xt is a true negative (TN) if our RNNs predict xt to be negative and xt is
indeed negative; xt is a false negative (FN) if our RNNs predict xt to be negative while xt is actually positive. We
also use TP (FP, TN, FN, respectively) to represent the total number of true positives (false positives, true negatives,
false negatives, respectively) produced by our RNNs.

The performance metrics used in this study include the following:

Recall =

TP
TP + FN

,

Precision =

TP
TP + FP

,

Accuracy (ACC) =

TP + TN
TP + FP + TN + FN

,

Heidke Skill Score (HSS) =

2(TP × TN − FP × FN)
(TP+FN)(FN+TN) + (TP+FP)(FP+TN)

,

True Skill Statistics (TSS) =

TP
TP + FN

−

FP
TN + FP

.

(6)

(7)

(8)

(9)

(10)

The Heidke Skill Score (HSS) (Heidke 1926) is used to measure the fractional improvement of our prediction over
the random prediction (Florios et al. 2018). The TSS score is the recall subtracted by the false alarm rate (Bloomﬁeld
et al. 2012). We also use the area under the curve (AUC) in a receiver operating characteristic (ROC) curve (Marzban
2004), which represents the degree of separability, indicating how well a method is capable of distinguishing between
two classes with the ideal value of one. These performance metrics are commonly used when dealing with binary
classiﬁcation problems such as the one at hand as deﬁned in the beginning of Section 3.1. In general, the larger HSS,
TSS and AUC score a binary classiﬁcation method has, the better performance the method achieves.

6

Liu et al.

Table 2. Rankings of the 18 SDO/HMI Magnetic Parameters Used in Our Study

12 hr

36 hr
GRU LSTM GRU LSTM GRU LSTM GRU LSTM GRU LSTM

48 hr

24 hr

60 hr

SHARP
Keyword
1
MEANPOT
2
SHRGT45
12
TOTPOT
9
USFLUX
5
MEANJZH
3
ABSNJZH
4
SAVNCPP
7
MEANALP
17
MEANSHR
8
TOTUSJZ
TOTUSJH
14
MEANGAM 18
11
MEANGBZ
6
MEANJZD
10
AREA ACR
15
R VALUE
13
MEANGBT
16
MEANGBH

1
2
10
12
5
4
6
7
3
8
18
17
13
9
11
16
14
15

1
2
4
6
3
8
9
7
5
10
15
14
11
12
13
16
17
18

1
2
9
4
5
8
7
10
3
11
17
6
12
13
14
15
16
18

1
3
2
5
7
9
8
6
4
11
10
12
14
16
15
13
17
18

1
3
4
9
7
8
6
10
2
11
12
5
14
15
16
13
17
18

1
3
2
5
12
8
7
9
4
10
11
16
14
17
16
13
15
18

1
4
2
5
8
7
9
12
3
10
11
6
14
15
17
13
16
18

1
3
2
12
6
5
7
10
4
9
11
8
18
16
14
13
17
15

1
4
2
5
7
6
8
12
3
9
11
10
14
15
16
13
17
18

To gain a better understanding of the behavior of the proposed RNNs, we adopt the following cross-validation
methodology. We partition the training (test, respectively) set into 10 equal-sized folds. For every two training (test,
respectively) folds i and j, i (cid:54)= j, fold i and fold j are disjoint; furthermore, fold i and fold j contain approximately
the same number of positive training (test, respectively) data samples and approximately the same number of negative
training (test, respectively) data samples. In the ith run, 1 ≤ i ≤ 10, all training data samples except those in training
fold i are used to train a model, and the trained model is used to make predictions on all test data samples except
those in test fold i. We calculate the performance metric values based on the predictions made in the ith run. There
are 10 runs. The means and standard deviations over the 10 runs are calculated and recorded.

4.2. Feature Assessment

We conduct a series of experiments to analyze the importance of each of the 18 features studied here using the
cross-validation methodology described above and the feature assessment method introduced in Section 4.3 of Liu
et al. (2019). Each time only one feature is used to make predictions. The probability threshold used by our RNNs
is set to maximize the TSS score in each run. There are 10 runs and the corresponding mean TSS score is calculated
and recorded. There are 18 features, so 18 mean individual TSS scores are recorded. These 18 mean individual TSS
scores are sorted in descending order, and the 18 corresponding features are ranked from the most important (with the
highest mean individual TSS score) to the least important (with the lowest mean individual TSS score) accordingly.
Table 2 presents the 18 features ranked by our GRU and LSTM networks respectively for diﬀerent T values where T
ranges from 12 to 60 in 12 hr intervals. It can be seen from the table that MEANPOT, SHRGT45, ABSNJZH and
SAVNCPP are consistently ranked in the top 10 list by both LSTM and GRU networks. In particular, MEANPOTS
plays the most important role in CME prediction, which is ranked as the top one in all cases. Other features such as
TOTPOT, USFLUX, MEANJZH, MEANALP and MEANSHR also show strong predictive power and are ranked in
the top 10 list in most cases. Compared to the top 10 lists of Bobra & Ilonidis (2016), who used a diﬀerent feature
ranking method for T = 24, 48 (see Table 1 of Bobra & Ilonidis 2016), these lists overlap to some extent (seven features
simultaneously occur in the top 10 lists given by Bobra & Ilonidis (2016) and our methods).

Next, according to the ranked features, mean cumulative TSS scores are calculated. Specially, the mean cumulative
TSS score of the top k, 1 ≤ k ≤ 18, most important features is equal to the mean TSS score of using the top k most
important features altogether for CME prediction. We calculate the mean cumulative TSS scores for T =12, 24, 36,

Predicting CMEs Using SDO/HMI Vector Magnetic Data Products and RNNs

7

48 and 60 hours. It is found that using all the 18 features together does not yield the highest mean cumulative TSS
scores. In fact, using the top 16, 12, 9, 14 and 5 features for T = 12, 24, 36, 48 and 60 hours respectively yields
the highest mean cumulative TSS scores, achieving the best performance for our GRU network. Using the top 15,
12, 8, 15 and 6 features for T =12, 24, 36, 48, 60 hours respectively yields the highest mean cumulative TSS scores
for our LSTM network. This happens probably because low ranked features are noisy features, and using them may
deteriorate the performance of the methods. In subsequent experiments, we use the best features for each method.

4.3. Performance Comparison

We compare our proposed RNNs with three closely related machine learning methods including a multilayer percep-
tron (MLP) (Inceoglu et al. 2018), a support vector machine (SVM) (Bobra & Ilonidis 2016) and the random forest
algorithm (RF) (Liu et al. 2017). MLP and SVM have been previously used for CME prediction (Bobra & Ilonidis
2016; Inceoglu et al. 2018). RF has been used in ﬂare prediction with good performance (Liu et al. 2017; Florios et al.
2018; Liu et al. 2019). These three machine learning methods are inherently probabilistic forecasting models (Florios
et al. 2018) in the sense that each of them predicts a probability. Since we are dealing with a binary classiﬁcation
problem, as deﬁned in the beginning of Section 3.1, we convert each probabilistic forecasting model into a binary
classiﬁcation model (Nishizuka et al. 2018; Jonas et al. 2018) by comparing the predicted probability with a threshold.
If the predicted probability is greater than or equal to the threshold, then the model predicts that a ﬂare will produce,
or is associated with, a CME; otherwise, the model predicts that the ﬂare will not produce a CME.

MLP consists of an input layer, an output layer and two hidden layers both with 200 neurons. SVM uses the radial
basis function (RBF) kernel. RF has two parameters: mtry (number of features randomly selected to split a node)
and ntree (number of trees to grow in the forest). We vary the values of ntree ∈ {300, 500, 1,000} and mtry ∈ [2, 8],
and set ntree to 500 and mtry to 3. The hyperparameter and parameter values used by these three related machine
learning methods are chosen to maximize their TSS scores. As in the proposed RNNs, we use data samples collected
in years 2010-2014 for training and data samples in years 2015-2019 for testing. To deal with the imbalanced datasets
described in Table 1, we give more weight to the minority class during training as done for the RNNs. The same
cross-validation methodology as described in Section 4.1 is adopted to evaluate the performance of the three related
machine learning methods.

Table 3 presents the confusion matrix in which mean TP, FP, FN, TN (with standard deviations enclosed in paren-
theses) of the ﬁve machine learning methods for diﬀerent T values where T ranges from 12 to 60 hours in 12 hr
intervals are listed. The probability thresholds used by the machine learning methods are set to maximize their TSS
scores. Table 4 presents the mean performance metric values (with standard deviations enclosed in parentheses) of
the ﬁve machine learning methods. The best performance metric values are highlighted in boldface. Figure 2 shows
the ROC curves for the ﬁve machine learning methods. It can be seen from Table 4 and Figure 2 that our GRU and
LSTM networks perform better than the three related machine learning methods in terms of HSS, TSS, AUC and
ROC curves. There is no clear distinction between the GRU and LSTM networks. These results indicate that both of
the proposed RNNs are suitable for solving the binary classiﬁcation problem at hand.

4.4. Probabilistic Forecasting

The three related machine learning methods are inherently probabilistic forecasting models. Our proposed RNNs
can be easily converted from a binary classiﬁcation model to a probabilistic forecasting model as follows. Instead of
comparing the probability, calculated by the sigmoid function in the output layer of the RNNs, with a threshold, the
RNNs simply output the probability. For a given time point t and an AR that will produce an M- or X-class ﬂare
within the next T hours of t, this output now represents a probabilistic estimate of how likely the ﬂare will initiate a
CME.

We use the Brier Score (BS) (Brier 1950) and the Brier Skill Score (BSS) (Wilks 2010) to quantitatively assess the

performance of a probabilistic forecasting model, where

Brier Score (BS) =

1
N

N
(cid:88)

(yn − ˆyn)2,

n=1

Brier Skill Score (BSS) = 1 −

BS

1
N

(cid:80)N

n=1(yn − y)2

.

(11)

(12)

8

Liu et al.

Table 3. Confusion Matrix for Our GRU and LSTM Networks and Three Related Machine Learning Methods

12 hr
252 (69)
SVM
280 (12)
RF
395 (15)
MLP
LSTM 453 (24)
432 (40)
GRU
SVM 233 (105)
153 (9)
RF
MLP
270 (33)
LSTM 317 (48)
294 (73)
GRU
245 (69)
SVM
217 (12)
RF
MLP
102 (15)
LSTM 42 (24)
GRU
65 (40)
SVM 455 (105)
535 (9)
RF
MLP
418 (32)
LSTM 369 (48)
394 (73)
GRU

24 hr
138 (153)
580 (13)
642 (25)
716 (72)
770 (32)
110 (142)
314 (10)
260 (16)
292 (41)
326 (64)
692 (153)
250 (13)
188 (25)
114 (71)
60 (32)
843 (142)
640 (11)
693 (16)
661 (42)
627 (64)

36 hr
2 (4)
747 (60)
875 (36)
964 (57)
984 (49)
0 (0)
472 (11)
511 (38)
389 (55)
339 (49)
1091 (4)
345 (60)
217 (37)
128 (57)
109 (48)
1127 (1)
656 (11)
617 (38)
739 (55)
789 (49)

48 hr
22 (65)
872 (38)
1016 (25)
958 (66)
1107 (60)
0 (0)
454 (13)
295 (27)
274 (42)
262 (42)
1364 (65)
514 (38)
370 (25)
428 (66)
279 (60)
1224 (1)
770 (13)
929 (27)
951 (42)
962 (42)

60 hr
112 (141)
883 (78)
1188 (87)
969 (168)
1127 (137)
72 (144)
485 (9)
520 (37)
49 (50)
119 (58)
1540 (142)
768 (78)
464 (88)
682 (169)
524 (138)
1264 (144)
851 (8)
816 (37)
1287 (49)
1216 (57)

TP

FP

FN

TN

Here N is the total number of sequences each having m consecutive data samples in the test set, yn and ˆyn denote the
observed probability and the estimated probability of the nth sequence respectively as deﬁned in Equation (5), and
y = 1
n=1 yn is the average value of all the observed probabilities. The values of BS range from 0 to 1 with the
N
perfect score being 0. The values of BSS range from minus inﬁnity to 1 with the perfect score being 1.

(cid:80)N

Table 5 presents the mean BS and BSS scores and standard deviations of the ﬁve machine learning methods for
diﬀerent T values where T ranges from 12 to 60 hours in 12 hr intervals. It can be seen from the table that the proposed
GRU and LSTM networks are comparable, and again outperform the three related machine learning methods in terms
of BS and BSS. These results in Table 4 and Table 5 suggest that our RNNs work well when used as either binary
classiﬁcation models or probabilistic forecasting models.

5. DISCUSSION AND CONCLUSIONS

We develop two RNNs, where one is a GRU network and the other is an LSTM network, for CME prediction. Given
a time point t and an AR that produces an M- or X-class ﬂare within the next T hours of t where T ranges from 12
to 60 in 12 hr intervals, our RNNs, when used as binary classiﬁcation models, can predict whether the AR will also
produce a CME associated with the ﬂare. In addition, our RNNs, when used as probabilistic forecasting models, can
produce a probabilistic estimate of how likely the M- or X-class ﬂare will initiate a CME.

We build a dataset of samples, gathered from the JSOC website, in the period from 2010 May to 2019 May; each
data sample has 18 magnetic parameters provided by SHARP. We use the data samples during the years of 2010–
2014 for training, and the data samples during the years of 2015–2019 for testing. The training set and test set
are disjoint, and hence our RNNs can make predictions on ARs that they have never seen before. With extensive
experiments, we evaluate the performance of the RNNs and compare them with three closely related machine learning
methods including MLP (Inceoglu et al. 2018), RF (Liu et al. 2017) and SVM (Bobra & Ilonidis 2016) using diﬀerent
performance metrics. All these machine learning methods including ours can be used as binary classiﬁcation models
or probabilistic forecasting models. The main results are summarized as follows:

1. Solar data samples in an AR are modeled as time series here. Unlike the previous method (Bobra &
Ilonidis 2016), which uses one data sample gathered at the time point t to make prediction, our RNNs use
the m data samples gathered at t and preceding m − 1 time points to make prediction (m is set to 20 in the

Predicting CMEs Using SDO/HMI Vector Magnetic Data Products and RNNs

9

Table 4. CME Prediction Results of Our GRU and LSTM Networks and Three Related Machine Learning Methods

Recall

Precision

ACC

HSS

TSS

AUC

12 hr

SVM 0.481 (0.142)
0.627 (0.017)
RF
0.783 (0.030)
MLP
LSTM 0.945 (0.032)
0.907 (0.064)
GRU
SVM 0.546 (0.086)
0.621 (0.016)
RF
MLP
0.616 (0.024)
LSTM 0.586 (0.032)
GRU
0.598 (0.065)
SVM 0.607 (0.068)
0.683 (0.011)
RF
MLP
0.703 (0.018)
LSTM 0.694 (0.038)
GRU
0.696 (0.044)
SVM 0.182 (0.126)
0.349 (0.022)
RF
MLP
0.413 (0.030)
LSTM 0.423 (0.065)
0.422 (0.075)
GRU
SVM 0.178 (0.125)
0.350 (0.022)
RF
0.428 (0.027)
MLP
LSTM 0.459 (0.066)
GRU
0.452 (0.068)
SVM 0.592 (0.043)
0.720 (0.007)
RF
MLP
0.760 (0.014)
LSTM 0.791 (0.023)
0.779 (0.041)
GRU

24 hr
0.197 (0.201)
0.742 (0.012)
0.787 (0.027)
0.901 (0.071)
0.913 (0.048)
0.380 (0.326)
0.641 (0.010)
0.708 (0.012)
0.699 (0.020)
0.712 (0.033)
0.550 (0.039)
0.686 (0.010)
0.750 (0.011)
0.773 (0.019)
0.786 (0.026)
0.057 (0.088)
0.376 (0.020)
0.501 (0.023)
0.551 (0.039)
0.577 (0.050)
0.056 (0.084)
0.380 (0.020)
0.505 (0.023)
0.562 (0.042)
0.588 (0.049)
0.482 (0.010)
0.759 (0.006)
0.810 (0.016)
0.851 (0.014)
0.850 (0.017)

36 hr
0.006 (0.016)
0.655 (0.054)
0.772 (0.036)
0.895 (0.053)
0.883 (0.051)
0.164 (0.334)
0.619 (0.023)
0.640 (0.011)
0.710 (0.023)
0.754 (0.021)
0.511 (0.008)
0.633 (0.026)
0.673 (0.011)
0.768 (0.024)
0.800 (0.016)
0.006 (0.016)
0.266 (0.054)
0.349 (0.023)
0.537 (0.047)
0.601 (0.033)
0.006 (0.016)
0.267 (0.054)
0.350 (0.023)
0.540 (0.048)
0.602 (0.033)
0.337 (0.072)
0.670 (0.012)
0.728 (0.011)
0.858 (0.018)
0.874 (0.016)

48 hr
0.015 (0.044)
0.680 (0.028)
0.727 (0.018)
0.767 (0.046)
0.815 (0.039)
0.100 (0.300)
0.648 (0.013)
0.779 (0.013)
0.785 (0.022)
0.803 (0.020)
0.477 (0.023)
0.634 (0.016)
0.745 (0.007)
0.765 (0.019)
0.795 (0.016)
0.014 (0.042)
0.263 (0.032)
0.491 (0.014)
0.528 (0.038)
0.588 (0.032)
0.015 (0.044)
0.262 (0.032)
0.493 (0.015)
0.529 (0.037)
0.588 (0.032)
0.373 (0.014)
0.695 (0.009)
0.759 (0.007)
0.801 (0.023)
0.822 (0.009)

60 hr
0.390 (0.287)
0.572 (0.037)
0.733 (0.049)
0.644 (0.086)
0.722 (0.055)
0.705 (0.177)
0.642 (0.017)
0.694 (0.009)
0.916 (0.056)
0.884 (0.041)
0.510 (0.068)
0.587 (0.021)
0.673 (0.018)
0.767 (0.031)
0.791 (0.015)
0.042 (0.161)
0.176 (0.037)
0.335 (0.033)
0.544 (0.056)
0.587 (0.028)
0.049 (0.160)
0.178 (0.038)
0.332 (0.031)
0.562 (0.054)
0.600 (0.027)
0.478 (0.121)
0.594 (0.009)
0.720 (0.033)
0.836 (0.019)
0.870 (0.021)

study presented here). To our knowledge, this is the ﬁrst time that RNNs, which can capture dependencies
in the temporal domain of the data samples, are used for CME prediction.

2. We evaluate the importance of each of the 18 magnetic parameters, or features, used in this study. Our
experimental results show that using the most important 5-16 features, depending on diﬀerent T values,
can achieve better performance than using all the 18 features together. These results are consistent with
the ﬁndings in the literature, which indicate that using fewer, high quality features is often better than
using all, including low quality features (Alpaydin 2016; Bobra & Ilonidis 2016; Goodfellow et al. 2016).
Developing eﬀective feature ranking and selection procedures has been an active area of research in machine
learning and related ﬁelds. In general, to ﬁnd the optimal feature subset among n features, one has to try
all 2n − 1 combinations of the n features. This exhaustive search algorithm becomes impractical when n
is large, as in our case where n = 18. Consequently, various heuristics based on statistics, randomization,
optimization, sampling, clustering, machine learning, evolutionary computation, genetic algorithms, branch
& bound algorithms and principal component analysis, to name a few, have been developed (Mitra et al.
2002; Gevrey et al. 2003; Guyon & Elisseeﬀ 2003; Liu et al. 2004; Oh et al. 2004; Olden et al. 2004; Somol
et al. 2004; Yoon et al. 2005; Chandrashekar & Sahin 2014; Xue et al. 2016; Fisher et al. 2018; Al-Tashi
et al. 2019; Liu et al. 2020). In this work, we use the single feature testing heuristic originated from (Laing
et al. 2012) to rank and select features. In related work, Bobra & Ilonidis (2016) used an F-score heuristic

10

Liu et al.

(a) T = 12 hr

(b) T = 24 hr

(c) T = 36 hr

(d) T = 48 hr

(e) T = 60 hr

Figure 2. ROC curves for our GRU and LSTM networks and three related machine learning methods.

to rank and select features. Identifying the best feature selection heuristic with the optimal performance
in terms of accuracy and execution time remains an open problem. We plan to further investigate this
problem in the future.

3. Our GRU and LSTM networks are comparable; there is no clear distinction between them in terms
of the performance metrics studied here. Both of the networks outperform the related machine learning
methods including MLP, RF and SVM whether they are used as binary classiﬁcation models or probabilistic
forecasting models. These ﬁndings are based on the data collection scheme in which data samples in years
2010-2014 are used for training and those in years 2015-2019 are used for testing. To further understand
the behavior of the machine learning methods, we have performed additional experiments as follows. In
each experiment, data samples collected in one of the ten years during the period of 2010-2019 are used
for testing and data samples in all the other nine years together are used for training. There are ten
years in the period and hence there are ten experiments. The average values of the performance metrics
are calculated. The results based on these additional experiments are consistent—our GRU and LSTM
networks are comparable, and both of them perform better than the related machine learning methods
MLP, RF and SVM.

Based on our experimental results, we conclude that the proposed RNNs are valid methods for CME prediction.
It should be pointed out that the CME prediction is performed based on the assumption that an M- or X-class ﬂare
already exists. In practice, how does one know whether an AR will produce an M- or X-class ﬂare within the next
T hours of some time point t? This question can be answered by using a ﬂare prediction method (e.g., Liu et al.
2017; Florios et al. 2018; Jonas et al. 2018; Nishizuka et al. 2018; Liu et al. 2019). Our software package has two

Predicting CMEs Using SDO/HMI Vector Magnetic Data Products and RNNs

11

Table 5. Probabilistic Forecasting Results of Our GRU and LSTM Networks and Three Related Machine Learning Methods

12 hr

SVM 0.308 (0.018)
0.286 (0.003)
RF
0.213 (0.005)
MLP
LSTM 0.182 (0.105)
0.192 (0.009)
GRU
SVM -0.264 (0.075)
-0.175 (0.011)
RF
MLP
0.125 (0.021)
LSTM 0.253 (0.043)
0.213 (0.037)
GRU

24 hr
0.345 (0.012)
0.295 (0.003)
0.182 (0.005)
0.158 (0.007)
0.159 (0.004)
-0.387 (0.048)
-0.186 (0.011)
0.267 (0.019)
0.366 (0.027)
0.363 (0.018)

36 hr
0.347 (0.007)
0.342 (0.004)
0.210 (0.006)
0.165 (0.005)
0.163 (0.006)
-0.390 (0.029)
-0.367 (0.018)
0.160 (0.023)
0.340 (0.018)
0.349 (0.024)

48 hr
0.376 (0.014)
0.339 (0.002)
0.202 (0.004)
0.183 (0.013)
0.173 (0.005)
-0.512 (0.055)
-0.361 (0.009)
0.188 (0.014)
0.265 (0.050)
0.304 (0.021)

60 hr
0.359 (0.007)
0.400 (0.007)
0.229 (0.015)
0.175 (0.010)
0.172 (0.005)
-0.452 (0.028)
-0.618 (0.026)
0.073 (0.062)
0.293 (0.039)
0.305 (0.022)

BS

BSS

components. The ﬁrst component is our previously developed deep learning program (Liu et al. 2019) where one can
use the program to predict whether there is an M- or X-class ﬂare within the next T hours of t. If the answer is yes,
then one can use the RNN tools developed here to predict whether the ﬂare produces, or is associated with, a CME.
In future work, we plan to further extend the software package to predict other events (e.g., ﬁlament eruptions, SEPs).
We thank the referees for very helpful and thoughtful comments. We also thank the team of SDO/HMI for producing
vector magnetic ﬁeld data products. The ﬂare catalogs were prepared by and made available through NOAA NCEI. The
CME event records were provided by DONKI. The related machine learning methods studied here were implemented
in Python. This work was supported by NSF grant AGS-1927578. C.L. and H.W. acknowledge the support of NASA
under grants NNX16AF72G, 80NSSC17K0016, 80NSSC18K0673 and 80NSSC18K1705.

REFERENCES

Ahmed, O. W., Qahwaji, R., Colak, T., et al. 2013, Solar

Chandrashekar, G., & Sahin, F. 2014, Computers &

Physics, 283, 157, doi: 10.1007/s11207-011-9896-1

Al-Tashi, Q., Abdulkadir, S. J., Rais, H. M., Mirjalili, S., &

Electrical Engineering, 40, 16,
doi: 10.1016/j.compeleceng.2013.11.024

Alhussian, H. 2019, IEEE Access, 7, 39496,
doi: 10.1109/ACCESS.2019.2906757

Alpaydin, E. 2016, Machine Learning: The New AI (MIT

Press).
https://mitpress.mit.edu/books/machine-learning
Baker, D. N., Daly, E., Daglis, I., Kappenman, J. G., &

Panasyuk, M. 2004, Space Weather, 2,
doi: 10.1029/2003sw000044

Baumgartner, C., Thalmann, J. K., & Veronig, A. M. 2018,

ApJ, 853, 105, doi: 10.3847/1538-4357/aaa243
Berkebile-Stoiser, S., Veronig, A. M., Bein, B. M., &

Temmer, M. 2012, ApJ, 753, 88,
doi: 10.1088/0004-637X/753/1/88

Bloomﬁeld, D. S., Higgins, P. A., McAteer, R. T. J., &

Gallagher, P. T. 2012, ApJL, 747, L41,
doi: 10.1088/2041-8205/747/2/l41

Bobra, M. G., & Ilonidis, S. 2016, ApJ, 821, 127,

doi: 10.3847/0004-637x/821/2/127

Bobra, M. G., Sun, X., Hoeksema, J. T., et al. 2014, Solar

Cho, K., van Merrienboer, B., Gulcehre, C., et al. 2014, in

Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP)
(Association for Computational Linguistics).
https://doi.org/10.3115%2Fv1%2Fd14-1179

DeVore, C. R., & Antiochos, S. K. 2008, ApJ, 680, 740,

doi: 10.1086/588011

Fisher, A., Rudin, C., & Dominici, F. 2018, All Models are

Wrong, but Many are Useful: Learning a Variable’s
Importance by Studying an Entire Class of Prediction
Models Simultaneously.
https://arxiv.org/abs/1801.01489

Florios, K., Kontogiannis, I., Park, S.-H., et al. 2018, Solar

Physics, 293, 28, doi: 10.1007/s11207-018-1250-4
Gevrey, M., Dimopoulos, I., & Lek, S. 2003, Ecological

Modelling, 160, 249, doi: 10.1016/s0304-3800(02)00257-0
Goodfellow, I. J., Bengio, Y., & Courville, A. C. 2016, Deep

Learning (MIT Press).
http://www.deeplearningbook.org/

Physics, 289, 3549, doi: 10.1007/s11207-014-0529-3

Gosling, J. T. 2013, in Coronal Mass Ejections, Vol. 99

Brier, G. W. 1950, Monthly Weather Review, 78, 1,

doi: 10.1175/1520-0493(1950)078(cid:104)0001:VOFEIT(cid:105)2.0.CO;2

(American Geophysical Union).
https://doi.org/10.1029/GM099p0009

12

Liu et al.

Guyon, I., & Elisseeﬀ, A. 2003, Journal of Machine

Learning Research, 3, 1157

Harrison, R. A. 1995, A&A, 304, 585
Heidke, P. 1926, Geograﬁska Annaler, 8, 301,

doi: 10.1080/20014422.1926.11881138

Hochreiter, S., & Schmidhuber, J. 1997, Neural

Computation, 9, 1735, doi: 10.1162/neco.1997.9.8.1735
Hoeksema, J. T., Liu, Y., Hayashi, K., et al. 2014, Solar
Physics, 289, 3483, doi: 10.1007/s11207-014-0516-8

Hopﬁeld, J. J. 1982, Proceedings of the National Academy

of Sciences, 79, 2554, doi: 10.1073/pnas.79.8.2554
Inceoglu, F., Jeppesen, J. H., Kongstad, P., et al. 2018,

ApJ, 861, 128, doi: 10.3847/1538-4357/aac81e

Jonas, E., Bobra, M., Shankar, V., Hoeksema, J. T., &

Recht, B. 2018, Solar Physics, 293, 48,
doi: 10.1007/s11207-018-1258-9

Kawabata, Y., Iida, Y., Doi, T., et al. 2018, ApJ, 869, 99,

doi: 10.3847/1538-4357/aaebfc

Laing, C., Wen, D., Wang, J. T. L., & Schlick, T. 2012,

Nucleic Acids Research, 40, 487, doi: 10.1093/nar/gkr629

LeCun, Y., Bengio, Y., & Hinton, G. 2015, Nature, 521,

436, doi: 10.1038/nature14539

Lin, J., & Forbes, T. G. 2000, J. Geophys. Res., 105, 2375,

doi: 10.1029/1999JA900477

Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., & Ishii, M.
2018, ApJ, 858, 113, doi: 10.3847/1538-4357/aab9a7

Oh, I.-S., Lee, J.-S., & Moon, B.-R. 2004, IEEE

Transactions on Pattern Analysis and Machine
Intelligence, 26, 1424, doi: 10.1109/tpami.2004.105

Olden, J. D., Joy, M. K., & Death, R. G. 2004, Ecological
Modelling, 178, 389, doi: 10.1016/s0304-3800(04)00156-5

Pesnell, W. D., Thompson, B. J., & Chamberlin, P. C. 2011,
in The Solar Dynamics Observatory, ed. W. D. Pesnell,
P. C. Chamberlin, & B. J. Thompson (Springer US),
3–15. https://doi.org/10.1007%2F978-1-4614-3673-7 2

Qahwaji, R., Colak, T., Al-Omari, M., & Ipson, S. 2008, in
Solar Image Analysis and Visualization, ed. J. Ireland &
C. A. Young (Springer New York), 261–273.
https://doi.org/10.1007%2F978-0-387-98154-3 19

Schou, J., Borrero, J. M., Norton, A. A., et al. 2010, Solar

Physics, 275, 327, doi: 10.1007/s11207-010-9639-8

Somol, P., Pudil, P., & Kittler, J. 2004, IEEE Transactions
on Pattern Analysis and Machine Intelligence, 26, 900,
doi: 10.1109/tpami.2004.28

SunPy Community, Mumford, S. J., Christe, S., et al. 2015,

Computational Science & Discovery, 8, 014009,
doi: 10.1088/1749-4699/8/1/014009

Liu, C., Deng, N., Wang, J. T. L., & Wang, H. 2017, ApJ,

T¨or¨ok, T., & Kliem, B. 2005, ApJL, 630, L97,

843, 104, doi: 10.3847/1538-4357/aa789b

doi: 10.1086/462412

Liu, C., Zheng, C.-T., Wu, S., Yu, Z., & Wong, H.-S. 2020,

Wilks, D. S. 2010, Quarterly Journal of the Royal

IEEE Transactions on Cybernetics, 50, 74,
doi: 10.1109/tcyb.2018.2864107

Liu, H., Liu, C., Wang, J. T. L., & Wang, H. 2019, ApJ,

877, 121, doi: 10.3847/1538-4357/ab1b3c

Liu, H., Motoda, H., & Yu, L. 2004, Artiﬁcial Intelligence,

159, 49, doi: 10.1016/j.artint.2004.05.009

Liu, Y. 2008, ApJL, 679, L151, doi: 10.1086/589282
Marzban, C. 2004, Weather and Forecasting, 19, 1106,

doi: 10.1175/825.1

Mitra, P., Murthy, C. A., & Pal, S. K. 2002, IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 24, 301, doi: 10.1109/34.990133

Meteorological Society, 136, 2109, doi: 10.1002/qj.709

Xue, B., Zhang, M., Browne, W. N., & Yao, X. 2016, IEEE

Transactions on Evolutionary Computation, 20, 606,
doi: 10.1109/tevc.2015.2504420

Yashiro, S., & Gopalswamy, N. 2008, in IAU Symposium,

Vol. 4, Universal Heliophysical Processes, ed.
N. Gopalswamy & D. F. Webb, 233–243.
https://doi.org/10.1017/S1743921309029342

Yoon, H., Yang, K., & Shahabi, C. 2005, IEEE

Transactions on Knowledge and Data Engineering, 17,
1186, doi: 10.1109/tkde.2005.144

