Estimating Metric Poses of Dynamic Objects
Using Monocular Visual-Inertial Fusion

Kejie Qiu∗, Tong Qin∗, Hongwen Xie†, and Shaojie Shen∗

8
1
0
2

g
u
A
1
2

]

O
R
.
s
c
[

1
v
3
5
7
6
0
.
8
0
8
1
:
v
i
X
r
a

Abstract— A monocular 3D object tracking system generally
has only up-to-scale pose estimation results without any prior
knowledge of the tracked object. In this paper, we propose
a novel
idea to recover the metric scale of an arbitrary
dynamic object by optimizing the trajectory of the objects in the
world frame, without motion assumptions. By introducing an
additional constraint in the time domain, our monocular visual-
inertial tracking system can obtain continuous six degree of
freedom (6-DoF) pose estimation without scale ambiguity. Our
method requires neither ﬁxed multi-camera nor depth sensor
settings for scale observability,
instead, the IMU inside the
monocular sensing suite provides scale information for both
camera itself and the tracked object. We build the proposed
system on top of our monocular visual-inertial system (VINS)
to obtain accurate state estimation of the monocular camera
in the world frame. The whole system consists of a 2D object
tracker, an object region-based visual bundle adjustment (BA),
VINS and a correlation analysis-based metric scale estimator.
Experimental comparisons with ground truth demonstrate the
tracking accuracy of our 3D tracking performance while a
mobile augmented reality (AR) demo shows the feasibility of
potential applications.

I. INTRODUCTION

A complete robotic perception system consists of robust
state estimation (localization), static environment mapping
and dynamic objects tracking, through the use of multiple
onboard sensors. Among these diverse sensing options, we
are particularly interested in the minimal sensor suite that
consists of only one camera and an IMU, due to its ultra
light-weight and low-cost. Equipped with state estimation
and mapping that comprise a simultaneous localization and
mapping (SLAM) system, an agent like a quadrotor can
execute autonomous navigation within an unknown but static
environment [1]. However, for a complete perception system,
dynamic objects in the real world also have to be considered
seriously. Previously, dynamic objects are often regarded
as outliers in a SLAM system, and the main consideration
is how to make the autonomous system robust against
these outliers. In other words, static environment is a basic
assumption. Nevertheless, a better way is to actively track
the dynamic objects regarding 6-DoF pose estimation, thus
completing the whole perception system as shown in Fig.
1. And a lot of beneﬁts will follow if the speciﬁc position

∗Kejie Qiu, Tong Qin and Shaojie Shen are with the Department of
Electronic and Computer Engineering, Hong Kong University of Science
and Technology, Hong Kong, China. kqiuaa@connect.ust.hk,
tqinab@connect.ust.hk, eeshaojie@ust.hk.

†Hongwen Xie is from Pattern Recognition Center of WeChat, Tencent

Inc, Beijing, China. hongwenxie@tencent.com.

This work was supported by the WeChat-HKUST Joint Laboratory on

Artiﬁcial Intelligence Technology (WHAT LAB).

Fig. 1: The complete perception system based on monocular visual-
inertial sensing.

and orientation of the dynamic object are known. More
robust state estimation, active obstacle avoidance, and path
planning, manipulation of the objects, even augmented reality
effect on the moving objects can become possible.

Actually, although promising results on state estimate
and static environment mapping have been achieved using
monocular visual-inertial fusion [2], [1], six degree of free-
dom (6-DoF) metric tracking of dynamic objects remains a
signiﬁcant challenge. It is obvious that the metric scale or the
real depth value of dynamic objects is of signiﬁcant impor-
tance for various real-time applications. While the primary
difﬁculty comes from the fact that the metric scale is not
directly observable from only one camera. To handle this, one
common solution is to utilize multiple ﬁxed cameras such
that the triangulation constraint is still valid even the objects
are moving. Motion capture system such as OptiTrack 1 and
Vicon 2 have been a mature tracking system for moderately
sized application scenarios but the tracked objects have to
be attached with reﬂective markers.

On the other hand, single sensor-based methods using an
RGBD sensor [3] or a stereo camera [4] relax the multi-
sensor condition at
the cost of limited sensing distance.
Consequently, these methods can only handle small-scale
application situations such as vision-based object manipula-
tion. we regard the camera motion and object motion as two
isolated signals and recover the metric object motion from
the perspective of signal processing, say, signal correlation
analysis. This process is like the signal decomposition prob-
lem discussed in the telecommunication community. Thus,
we propose to estimate the metric scale by analyzing the
temporally statistical relationship between the camera motion
and the recovered object motion.

The rest of the paper is structured as follows. Section II
introduces relevant work on 6-DoF object pose estimation
and fundamental work that our system relies on. In sec-

1http://optitrack.com/
2http://www.vicon.com/

IMUCameraState estimation6DOF trackingMappingdynamicobjectsstaticworldsensorstrinity 
 
 
 
 
 
tion III, our system structure and major frame relationships
are illustrated brieﬂy. Section IV describes how the objective
functions are formulated and solved for up-to-scale 3D
tracker and metric scale estimation. Section V gives the
experimental results with comparison against ground truth.
Conclusion and directions for future work are presented in
Section VI.

II. RELATED WORK

Estimating the 6-DoF pose of a dynamic object using mul-
tiple ﬁxed cameras is a well-studied formulation. Besides the
commercial active-tracking system with specialized markers,
passive-tracking systems for sports scenes analyzing and
trafﬁc surveillance were also proposed since the targets are
usually non-cooperative in these cases. A UAV trajectory
estimation system using ﬂight dynamics as a prior was
proposed based on ﬁxed ground cameras, assuming that all
the camera videos are synchronized [5]. [6] proposed a spa-
tiotemporal Bundle Adjustment framework to simultaneously
estimate the temporal alignment between cameras. However,
all of these methods are limited by the ﬁxed-camera setting.
Oppositely, few methods adopt a monocular camera frame-
work to recover the whole 6-DoF pose of dynamic objects,
since the tracking results may lack metric scale unless the
tracked target is studied or modeled carefully in advance.
However, solving this problem only using single camera
has attracted even more attention thanks to its ultra light-
weight, strong adaption, and synchronization-free features
compared to the ﬁxed multi-camera setting. Model-based
tracking is one branch of monocular-based methods. Given
a set of known 2D-to-3D correspondences, the relative pose
can easily be solved by using Perspective-n-Point (PnP) [7]
or view alignment using edge features [8], [9]. For instance,
ARUCO tag [10] made use of pre-designed tags as the known
3D information to mark tracked targets, and calculate the
camera pose in terms of the tags. Instead of using pre-
designed landmarks, discriminative feature points such as
BRISK [11] are also practical for 2D-to-3D matching. In ad-
dition to sparse point features, dense method [12] and edge-
based method [13] are good alternatives for textureless object
tracking. All the methods work well as long as the 3D model
of the tracked object was carefully modeled in advance.
Another signiﬁcant branch is learning-based tracking [14],
Georgios Pavlakos et al. proposed an efﬁcient convolutional
network to ﬁrst locate reliable pre-deﬁned semantic keypoints
and then estimate the 6-DoF pose with only a monocular
camera [15]. However, a faked scenario can destroy learning-
based methods easily especially in AR applications. For
example, a real car and a car model can fool learning-based
methods for lack of metric scale measurement.

In order to estimate the metric object scale, the camera
pose has to be known in advance such that the object pose
computed from the region-based BA can be projected from
the camera frame to the world frame. And we make use
of our visual-inertial system (VINS) for accurate and robust
camera pose estimation in the world frame, interested readers
are referred to our previous work on state estimation with

Fig. 2: 2D object tracking results with bounding boxes.

Fig. 3: Overall system structure.

visual-inertial fusion [16], [17]. Also, our method is designed
to handle 6-DoF tracking of an arbitrary rigid object by
using an object region-based visual bundle adjustment (BA)
with online scale estimation. In order to obtain accurate
object regions for region-based visual BA, a robust 2D
tracker is needed. The 2D tracking problem has already been
well discussed and lots of classic tracking methods such as
CMT [18], STRUCK [19] are robust enough for most cases,
resulting in sequential bounding boxes. Given accurate object
areas on the sequentially images, a region-based BA could
be used to get up-to-scale relative 6-DoF pose.

III. SYSTEM OVERVIEW

The captured images are ﬁrst processed by 2D object
tracking, resulting in object regions represented by 2D
bounding boxes as shown in Fig. 2. VINS [17] takes both
IMU and masked images for tightly-coupled camera pose
estimation in the world frame. Given consecutive image
regions of the object, a region-based visual BA is running for
up-to-scale object pose estimation. Finally, the camera pose
in the world frame and the object pose in the camera frame
are collected together for metric scale estimation. Once the
metric scale estimation obtained, the metric object poses in
the world frame is recovered. The overall system structure is
shown in Fig. 3. The combination of the region-based BA and
metric scale estimation is also called 3D tracker, while up-to-
scale 3D tracker particularly refers to the region-based BA.
In this paper, we assume that a 2D object detector is provided
for 2D tracking initialization and focus on region-based
visual bundle adjustment and online metric scale estimation.
The relationship between relevant frames is shown in Fig.
4, where (·)w is the world frame, (·)c the camera frame, and

Raw imagefor VINSfor 3DtrackerVINSCameraIMU2D trackerregion-based         BAmetric scale estimation3D trackerFig. 4: Coordinate frames in the system.

y and Rx

c and pw

(·)o is the object frame. The IMU frame is ignored since it
only constrains the metric scale of VINS. px
y are the
3D translation and rotation of frame (·)y respectively with
respect to frame (·)x. The camera pose in the world provided
c , and ¯(·) denotes the
by VINS is represented by Rw
up-to-scale pose results. Thus Rc
o together denotes
the relative transformation of the object frame in terms of
the camera frame provided by the region-based visual BA
module. What the metric scale estimation module estimates
is the scale ratio s that makes the up-to-scale position to
o and pw
scaled one (pc
o
together to denote the global transformation from the world
frame to the object frame. The detailed representation of the
global object pose can be found in Section IV.

o). In the end, we use Rw

o = s · ¯pc

o and ¯pc

IV. METHODOLOGY

A. Up-to-scale 3D tracker

The 3D tracker we design is based on a region-based
visual bundle adjustment, we construct a purely vision-based
graph optimization for 3D pose tracking of general objects.
In this way, any rigid objects with arbitrary shapes could
be tracked since no assumption about the object shape is
needed. Note that this bundle adjustment is performed with
respect to the object frame, that is, the 3D tracker is to
estimate the camera motion w.r.t the object frame, which is
different from the visual BA deﬁned in VINS with estimating
the camera motion w.r.t the world frame. Since both camera
frame and object frame are dynamic, either camera motion or
object motion can cause the relative motion detected by the
region-based visual BA. That is to say, the estimated motion
of the 3D tracker is a compound motion coupled with two
independent physical motions.

The full state of a sliding window with N image frames

and M object features is deﬁned as follows:

X = [x0, x1, · · · xN −1, µ0, µ1, · · · µM −1]
xk = (cid:2)¯pck
ok

(cid:3) , k ∈ [0, N − 1],

, qck
ok

(1)

ok , and orientation qck

where the k-th camera state consists of the up-to-scale
position ¯pck
ok of camera frame ck with
respect to object frame ok. 3D features are parameterized by
their inverse depth µ when ﬁrst observed in camera frame.
The bundle adjustment can be formulated as a nonlinear
least-square problem,

Fig. 5: Object coordinate initialization process.

min
X

(cid:88)

(cid:13)
(cid:13)rC(ˆzcj

l , X )(cid:13)
2
2 ,
(cid:13)

(l,j)∈C

(2)

where rC(ˆzcj
l , X ) is the nonlinear residual function of visual
measurements. The vector zcj
results from the procedure
l
where the lth feature is projected to camera frame cj. And
the visual residual is deﬁned as the sum of the reprojection
error between the projected 3D features and the observed
2D features. Since this formulation takes the object regions
of the captured images as input, the object is used as the
reference frame for camera pose estimation. The front end
of the region-based visual BA for feature association adopts
the same corner features as that used in VINS, which could
be replaced by other feature-based methods, such as edge-
based and dense methods to handle featureless objects.

The direct optimization results are camera transformations
in terms of the object frame, while what we need for 3D
object tracking is the object pose in the camera frame. Thus,
the object coordinate has to be located around the object
itself, which is not guaranteed by a regular visual BA. To
solve this problem, we further modify the initial poses by
estimating the initial object pose in the camera frame. As
shown in Fig. 5, we move the object coordinate from the
initial position where the initial camera optical center locates
to the object surface along the direction denoted by the
2D object region center, and normalize the object depth by
scaling the object point cloud simultaneously. In fact, the
object coordinate can be anywhere around the point cloud on
the object, once it’s determined in the initialization process,
it will be continuously tracked in the bundle adjustment
framework.

B. Metric scale estimation

Before doing metric scale estimation, we already have
accurate enough camera poses in the world frame, and up-to-
scale object poses in the camera frame. Fundamentally, the
metric scale of a tracked rigid object is unobservable based
on a monocular camera. Consider an extreme case that the
monocular sensor suite is always static when tracking the
object, the IMU that involves scale information will have no
contribution to the object scale recovery. However, the metric
scale estimation problem becomes conditionally solvable if
the camera motion and the object motion follow a speciﬁc
condition, or scale observability condition. Since the intrinsic
scale estimation could work with the region-based bundle to
provide continuous metric pose tracking once the scale is
estimated, so we can accumulate a period of observations,

camera frameobject frameworld framedynamicstaticcamera coordinatecameraobject coordinateobjectregion centerobject regionwaiting for an opportunity that the designed observability
condition is satisﬁed.

The metric scale estimation problem in this paper is solved
from the perspective of statistics and probability. As we
know the direct observation from the region-based bundle
adjustment is a compound motion of the object motion and
camera motion, while the object motion and camera motion
are supposed to be independent since they have no real
physical connections. Thus the scale could be estimated
based on the statistical independence of the object motion
and camera motion in the world frame.

it

= s · ¯pct

We start by analyzing the scale factor (s) we want to
estimate,
is actually a scale ratio of the real metric
scale over the inherent scale exists in the region-based BA
(pct
ot). It is time-invariant if the region-based BA
ot
is accurate enough since it reﬂects the intrinsic scale of the
tracked object. For the same reason, the intrinsic metric scale
estimator needs not to perform for each input image. We call
this scale ratio as scale in the following content for simplicity.
Accordingly, the estimated scale ˆs could be represented as
the sum of the true scale s and the estimation error ∆s:

ˆs = s + ∆s.

Recall the recovered object trajectory:

ˆpw
ot

= ˆs · Rw
ct

¯pct
ot

+ pw
ct

= ˆs · pw
dt

+ pw
ct

,

where

pw
dt

= Rw
ct

¯pct
ot

.

Particularly, when ˆs equals to s:

pw
ot

= s · pw
dt

+ pw
ct

.

Thus:

ot = ˆs · pw
ˆpw
= pw

ct = (s + ∆s) · pw

dt + pw
ot + ∆s · pw
∆s
s

)pw

dt = pw
∆s
s

ot −

ot +

pw
ct ,

∆s
s

= (1 +

dt + pw
ct
ot − pw
(pw
ct )

(3)

(4)

(5)

(6)

(7)

from the recovered object trajectory representation we can
see that uncorrected scale estimation results in insufﬁcient
motion decomposition, which means the recovered object
motion has some correlation with the camera motion. Thus
the optimal scale could be estimated by correlation analysis
between the recovered object motion and the corresponding
camera motion. If the object motion and camera motion sat-
isﬁes a certain observability condition within a time domain,
we argue that an incorrect metric scale estimate, no matter
larger or smaller than the true scale value, corresponds to a
kind of salient correlation relationship between the recovered
object motion and the camera motion. Thus a good scale
estimate (∆s → 0) could be obtained by minimizing the
correlation between the recovered object motion and the
camera motion. Thus, the key objective function formulation
is all about how to quantize the correlation properly.

We ﬁrst model the recovered object motion and camera
motion as two random variables mo and mc, and deﬁne
another random variable md to denote the up-to-scale motion

difference, here we adopt a certain order derivative of a
motion trajectory for motion representation:

mc =[mx

c , my

c , mz

c ]T =

md =[mx

d, my

d, mz

d]T =

ˆmo = ˆmo(ˆs) = [ ˆmx

o , ˆmy

,

,

c (t)

∂npw
∂tn
∂npw
∂tn
o]T =

d (t)

o, ˆmz
∆s
s

o (t)

∂n ˆpw
∂tn
∆s
s

=ˆsmd + mc = (1 +

)mo −

mc.

(8)

For example, n = 2 corresponds to the acceleration, and we
directly approximate the motions from the raw data:

mc =

md =

∂2pw

c (t)
∂t2 ≈
d (t)
∂t2 ≈

∂2pw

ct+2∆t − 2pw
pw
∆t2
dt+2∆t − 2pw
pw
∆t2

ct+∆t + pw
ct

dt+∆t + pw
dt

,

.

(9)

We will further design an observability condition later such
that the quantitative correlation value between ˆmo and mc
could be minimized for a good scale estimation ˆs. Take a
look at this covariance matrix:

Cov( ˆmo, mc)

=E[( ˆmo − E[ ˆmo])(mc − E[mc])T ]
=E[(ˆsmd + mc − E[ˆsmd + mc])(mc − E[mc])T ]
=ˆsCov(md, mc) + Cov(mc, mc).

(10)

Particularly, when ˆs equals to s:

Cov(mo, mc) = sCov(md, mc) + Cov(mc, mc).
All the population covariances could be estimated from
the latest No observations (No is a big enough but a limited
number for bounded computation complexity consideration).

(11)

Cov(md, mc) ≈

Cov(mc, mc) ≈

1
No − 1

1
No − 1

No(cid:88)

(mk

d − ˆmd)(mk

c − ˆmc)T ,

k=1

No(cid:88)

(mk

c − ˆmc)(mk

c − ˆmc)T ,

(12)

k=1

ˆmd =

1
No

No(cid:88)

k=1

mk

d, ˆmc =

1
No

No(cid:88)

k=1

mk
c .

The standard correlation measurement between two mul-
tivariate random variables is to use trace correlation [20].
Instead, we utilize a simpliﬁed measurement in terms of in-
dividual covariances such that a closed form scale estimation
could be derived. The objective function f (ˆs) is formulated
as the sum of all the covariance squares of Equation 10:

f (ˆs) =

(cid:88)

Cov2( ˆmi

o, mj
c)

i;j∈x,y,z
(cid:88)

i;j∈x,y,z
(cid:32)

(cid:88)

=

=

i;j∈x,y,z
(cid:32)

(cid:88)

+

+

(ˆsCov(mi

d, mj

c) + Cov(mi

c, mj

c))2

(cid:33)

Cov2(mi

d, mj
c)

ˆs2

2Cov(mi

d, mj

c)Cov(mi

c, mj
c)

i;j∈x,y,z
(cid:88)

Cov2(mi

c, mj

c),

i;j∈x,y,z

(13)

(cid:33)

ˆs

which is a quadratic function in terms of ˆs, thus the optimal
scale estimation could be represented in a closed form
solution through minimizing f (ˆs):

ˆs∗ = −

(cid:80)
i;j∈x,y,z

Cov(mi

d, mj

c)Cov(mi

c, mj
c)

(cid:80)
i;j∈x,y,z

Cov2(mi

d, mj
c)

.

(14)

C. Observability condition derivation

One key point we want to highlight is that

the scale
estimator has degenerated cases using a monocular sensor
scheme. In other words, our estimator will not work for all
camera and object motion combinations. Take the case that
the camera is almost static during the data accumulation for
example, the optimal scale estimation result ˆs will be tending
to zero, which is deﬁnitely not correct. This is because
the observed compound object motion in the camera frame
that combines object motion and camera motion degenerates
to object motion only, and the constraint derived from the
motion decomposition process doesn’t hold anymore.

In addition, note that all the computation is based on
the motion independence assumption, thus another possible
degenerated case is that the camera moves exactly as how the
object moves, resulting in an invariable observation of the ob-
ject in the camera frame. Although this case is statistically of
very low probability, the corresponding optimal scale value
ˆs can be arbitrary numbers. On the other hand, however,
it is also almost impossible that the object motion and the
camera motion are strictly uncorrelated in a limited sample
duration time. Thus an error term ∆s exists in practical
analysis within a speciﬁc observation duration. Nevertheless,
the estimation is still valid if the error term is limited to an
acceptable range, and we call the corresponding condition as
the observability condition. And our intrinsic scale estimator
do not have to be running all the time since what we estimate
is the intrinsic scale of the tracked object instead of object
depths of the input sequential images. Estimating the intrinsic
scale once in a while when the observability condition is
satisﬁed is totally acceptable because of the region-based
BA in our system.

In fact, the observability condition is more important than
the closed form scale estimation to some extent, since it
determines if we can accept the scale estimation or not.
One intuitive condition is that the objective function value
with the optimal scale should be close to zero, but intuitive
way may miss some degenerated cases, so a better way is to
study the observability condition through error term analysis
(substitute Equation 3 and 11 into Equation 14):

(cid:88)

i;j∈x,y,z

(cid:80)
i;j∈x,y,z

Cov2(mi

o, mj

c) → 0,

1

Cov2(mi

c, mj
c)

→ 0,

(16)

where the ﬁrst condition just meets the intuitive condition
if we use the reconstructed object motion with the optimal
scale ( ˆmo(ˆs∗)) instead of the real object motion (mo), so
it’s a necessary condition. The second condition is totally
under control on the other hand, since although we cannot
control the camera motion, but we can at least observe and
analyze the camera motion to decide if the scale estimation
is acceptable or not.

In practice, we use the threshold-based criteria instead:

(cid:88)

i;j∈x,y,z
(cid:88)

i;j∈x,y,z

Cov2(mi

ˆo, mj

c) <= (cid:15)t1 ,

Cov2(mi

c, mj

c) >= ρt1 .

Also, for the numerical stability of Equation 14:

(cid:88)

i;j∈x,y,z

Cov2(mi

d, mj

c) >= ρt2.

(17)

(18)

So far, we can derive the complete observability condition

as follows:

i

ii

iii

(cid:88)

f (ˆs∗) <= (cid:15)t1
c, mj
c) >= ρt1

Cov2(mi

i;j∈x,y,z
(cid:88)

Cov2(mi

d, mj

c) >= ρt2,

(19)

i;j∈x,y,z

where (cid:15)t1 is a threshold with small positive value, ρt1 and
ρt2 are two thresholds with large enough positive values.
The ﬁrst two subconditions are used to satisfy the estima-
tion accuracy requirement, and the last subcondition is for
numerical stability consideration. All the thresholds are set
empirically in implementation. The more strict the condition
is, the more accurate and robust the metric scale estimator
will be, but with lower scale acceptance rate. Overall, the
observability condition makes our system robust against
multiple degenerated cases that may occur in the tracking
process.

Given the estimated metric scale with the observability
condition satisﬁed, we can ﬁrst recover the object position
in the camera frame as:

ˆpct
ot

= ˆs · ˆ¯pct
ot

,

(20)

∆s∗
s

=

(cid:80)
i;j∈x,y,z

Cov(mi

o, mj

c)(Cov(mi

c, mj

c) − Cov(mi

o, mj

c))

(cid:80)
i;j∈x,y,z

(Cov(mi

c, mj

c) − Cov(mi

o, mj

2
c))

The desired scale estimation should satisfy ∆s∗
we summarize it as:

(15)
s → 0, and

then we can recover the object poses in the world frame:

.

ˆRw
ot
ˆpw
ot

= ˆRw
ˆRct
,
ot
ct
= ˆs · ˆRw
ˆ¯pct
ot
ct

+ ˆpw
ct

.

(21)

It is evident that the ﬁnal object localization performance
ct), up-to-scale 3D tracking (ˆ¯pct
relies on VINS (ˆpw
ot
and ˆRct

ot) and metric scale estimation results (ˆs).

ct and ˆRw

(a) The sensor suite

(b) The objects for tracking

(a) position

Fig. 6: The minimal sensor suite and irregular objects we use for
experiments. Our method do not need any prior knowledge, special
markers or tags of the tracking object.

V. EXPERIMENTAL RESULTS

A. Implementation details

Our monocular sensor suite (Fig. 6(a)) consists of an
mvBlueFOX-MLC202bG grayscale camera with a wide-
angle lens that captures 1280 by 960 images at 24 Hz and a
Microstrain 3DM-GX4 IMU that runs at 500 Hz. Both VINS
and 2D tracker work on the downsampled images of 640 by
480 resolution. We use CMT tracker [18] as the 2D tracker in
our implementation and manually draw the object area for 2D
tracking initialization. While the cropped images according
to 2D tracking results are used for region-based bundle
adjustment (N = 20 in sliding window). For robust region-
based BA estimation, the object area extracted from the
original image cannot be too small, which means the object
cannot be too far away from the camera. In practice, we
notice that at least 50 features should be extracted from the
object region if we use feature-based tracking frontend. The
number of observations we use for metric scale estimation
is 200 (No = 200). The motion feature extracted is velocity
(n = 1 in Equation 8). The whole system runs on a desktop
computer in real time.

o and pw

The goal of the experiments is to validate the recovered
object pose in the world frame (Rw
o ) by involving
the camera pose in the world frame and the estimated metric
scale. Since there are no similar formulations shown in
relevant works, we evaluate our estimation accuracy with
a comparison to a motion capture system (OptiTrack) as
groundtruth. And we use several general objects as the
tracking targets. Two sample objects are shown in Fig. 6(b),
where the tapes attached on the objects are used to cover
the excessively shining parts that may deactivate the motion
capture system.

B. 6-DoF dynamic object pose estimation in world frame

Given the estimated metric scale, the 3D tracking results
could be projected to the world frame using Equation 21.
For trial 1, a manually controlled toy robot is adopted as the
tracking target. While for trial 2, a remote controlled racing
car is used as the tracking target, which is controlled to run
from ground to a higher platform with directions changing all
the time. More details and AR demonstration will be shown
in the attached video.

Fig. 7 and Fig. 8 show part of the position and orien-
tation comparison between the estimation and groundtruth,

(b) orientation

Fig. 7: The comparison of 6-DoF object pose estimation (Trial 1:
tracking a manually controlled toy robot).

(a) position

(b) orientation

Fig. 8: The comparison of 6-DoF object pose estimation (Trial 2:
tracking a remote controlled racing car).

with a standard deviation of {0.0486, 0.0271, 0.1245}
(rad)
in yaw, pitch and roll, a standard deviation of
{0.0218, 0.0310, 0.0344} (m) in the x, y and z positions of
trial 1, and a standard deviation of {0.0739, 0.0254, 0.0239}
(rad)
in yaw, pitch and roll, a standard deviation of
{0.0505, 0.1169, 0.0397} (m) in the x, y and z positions
of trial 2. Generally, the experiment shows acceptable per-
formance of our 6-DoF object tracking system. We can also
notice the orientation drift of trial 1 and translation drift of
trial 2 caused by the up-to-scale 3D tracker module, which
could be corrected by visual relocalization or loop closure if
a loop is detected.

01020304050x(m)-20201020304050y(m)-113Ground TruthOurstime(s)01020304050z(m)-11301020304050yaw(rad)-10101020304050pitch(rad)-101Ground TruthOurstime(s)01020304050roll(rad)-101051015-404x(m)051015-202y(m)Ground TruthOurs051015time(s)-101z(m)051015-505yaw(rad)051015-101pitch(rad)Ground TruthOurs051015time(s)-101roll(rad)TABLE I: Degenerated cases

Cases

true scale s

sample estimations ˆs

observability subcondition

C1

C2

C3

C4

0.43

0.43

0.43

0.43

(-5.99 -4.62 -2.71 -1.13 16.87 35.85 12.27 17.19 -0.38 -0.26)

(i) not satisﬁed

(0.047 -0.008 -0.039 -0.041 -0.007 -0.031 -0.067 0.017 0.008 0.009)

(ii) not satisﬁed

(-0.085 -0.088 -0.0085 -0.078 -0.077 -0.085 -0.078 -0.085 -0.078 -0.076)

(ii) not satisﬁed

(21.81 20.82 19.76 18.82 27.38 25.14 21.57 22.77 21.57 24.15)

(iii) not satisﬁed

C. Degenerated cases

We further test three typical degenerated cases that will
generate inaccurate scale estimation without checking the
observability condition: the camera moves exactly as how
the car moves in terms of translation (C1);
the camera
is inactive, static (C2), constant velocity (C3); and static
object observation in the camera frame although both the
car and the camera are moving (C4), which correspond to
the three observability subconditions discussed in Equation
19 respectively. And all these degenerated cases could be
avoided by checking the observability condition. The true
scale, sample scale estimations under these cases, and cor-
responding subconditions are listed in Table I. According to
Equation 15, the scale estimations may have large estimation
o, mj
errors for case C1 (Cov(mi
c) not → 0). According to
Equation 14, the scale estimations tend to be approaching
to zero for case C2 and case C3 (Cov(mi
c) → 0).
And according to Equation 14, the scale estimations tend
to have large absolute values for case C4 (Cov(mi
c) →
0). Fortunately, all the degenerated cases could be avoided
by checking the observability condition during the tracking
process.

d, mj

c, mj

VI. CONCLUSION AND FUTURE WORK
In this paper, we propose a new understanding of the 6-
DoF object tracking problem using a monocular sensor suite
with visual-inertial fusion. We have proven the feasibility of
the proposed 3D tracking system with observability condition
analysis. The experimental results validate our idea and show
an acceptable estimation accuracy of the proposed method.
At the same time, each module in our system has upgrading
potential. For example, the region-based bundle adjustment
implemented in this paper is purely a BA without relocal-
ization, loop-closure which are widely used in the visual
odometry community. Thanks to the building block design
of our system, fortunately, each module can be conveniently
replaced, and the whole system will be boosted accordingly.

REFERENCES

[1] Y. Lin, F. Gao, T. Qin, W. Gao, T. Liu, W. Wu, Z. Yang, and
S. Shen, “Autonomous aerial navigation using monocular visual-
inertial fusion,” Journal of Field Robotics, 2017.

[2] Z. Yang, F. Gao, and S. Shen, “Real-time monocular dense mapping on
aerial robots using visual-inertial fusion,” in Robotics and Automation
(ICRA), 2017 IEEE International Conference on.
IEEE, 2017, pp.
4552–4559.

[3] A. Aldoma, F. Tombari, J. Prankl, A. Richtsfeld, L. Di Stefano,
and M. Vincze, “Multimodal cue integration through hypotheses
veriﬁcation for rgb-d object recognition and 6dof pose estimation,” in
Robotics and Automation (ICRA), 2013 IEEE International Conference
on.

IEEE, 2013, pp. 2104–2111.

[4] M.-S. Wang et al., “3d object pose estimation using stereo vision for
object manipulation system,” in Applied System Innovation (ICASI),
2017 International Conference on.

IEEE, 2017, pp. 1532–1535.

[5] A. Rozantsev, S. N. Sinha, D. Dey, and P. Fua, “Flight dynamics-based
recovery of a uav trajectory using ground cameras,” arXiv preprint
arXiv:1612.00192, 2016.

[6] M. Vo, S. G. Narasimhan, and Y. Sheikh, “Spatiotemporal bundle
adjustment for dynamic 3d reconstruction,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2016, pp.
1710–1718.

[7] S. Garrido-Jurado, R. Mu˜noz-Salinas, F. J. Madrid-Cuevas, and M. J.
Mar´ın-Jim´enez, “Automatic generation and detection of highly reliable
ﬁducial markers under occlusion,” Pattern Recognition, vol. 47, no. 6,
pp. 2280–2292, 2014.

[8] K. Qiu, T. Liu, and S. Shen, “Model-based global localization for
aerial robots using edge alignment,” IEEE Robotics and Automation
Letters, vol. 2, no. 3, pp. 1256–1263, 2017.

[9] K. Qiu and S. Shen, “Model-aided monocular visual-inertial state
estimation and dense mapping,” in Intelligent Robots and Systems
(IROS), 2017 IEEE/RSJ International Conference on.
IEEE, 2017,
pp. 1783–1789.

[10] S. Garrido-Jurado, R. M. noz Salinas, F. Madrid-Cuevas, and
M. Mar´ın-Jim´enez, “Automatic generation and detection of highly re-
liable ﬁducial markers under occlusion,” Pattern Recognition, vol. 47,
no. 6, pp. 2280 – 2292, 2014.

[11] S. Leutenegger, M. Chli, and R. Y. Siegwart, “Brisk: Binary robust
invariant scalable keypoints,” in Computer Vision (ICCV), 2011 IEEE
International Conference on.

IEEE, 2011, pp. 2548–2555.

[12] E. Brachmann, F. Michel, A. Krull, M. Ying Yang, S. Gumhold
et al., “Uncertainty-driven 6d pose estimation of objects and scenes
from a single rgb image,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2016, pp. 3364–3372.
[13] C. Choi and H. I. Christensen, “3d textureless object detection and
tracking: An edge-based approach,” in Intelligent Robots and Systems
(IROS), 2012 IEEE/RSJ International Conference on.
IEEE, 2012,
pp. 3877–3884.

[14] A. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker, A. Rodriguez, and
J. Xiao, “Multi-view self-supervised deep learning for 6d pose esti-
mation in the amazon picking challenge,” in Robotics and Automation
(ICRA), 2017 IEEE International Conference on.
IEEE, 2017, pp.
1386–1383.

[15] G. Pavlakos, X. Zhou, A. Chan, K. G. Derpanis, and K. Dani-
ilidis, “6-dof object pose from semantic keypoints,” arXiv preprint
arXiv:1703.04670, 2017.

[16] Z. Yang and S. Shen, “Monocular visual-inertial state estimation
with online initialization and camera-imu extrinsic calibration,” IEEE
Transactions on Automation Science and Engineering, no. 99, p. 1,
2016.

[17] T. Qin and S. Shen, “Robust initialization of monocular visual-inertial
estimation on aerial robots.” in Proc. of the IEEE/RSJ Intl. Conf. on
Intell. Robots and Syst., Vancouver, Canada, 2017, accepted.

[18] G. Nebehay and R. Pﬂugfelder, “Clustering of static-adaptive corre-
spondences for deformable object tracking,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2015,
pp. 2784–2791.

[19] S. Hare, S. Golodetz, A. Saffari, V. Vineet, M.-M. Cheng, S. L. Hicks,
and P. H. Torr, “Struck: Structured output tracking with kernels,” IEEE
transactions on pattern analysis and machine intelligence, vol. 38,
no. 10, pp. 2096–2109, 2016.

[20] J. W. Hooper, “Simultaneous equations and canonical correlation
theory,” Econometrica: Journal of the Econometric Society, pp. 245–
256, 1959.

