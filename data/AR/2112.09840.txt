IMPROVING UPON THE EFFECTIVE SAMPLE SIZE BASED ON  
GODAMBE INFORMATION FOR BLOCK LIKELIHOOD INFERENCE 

Rahul Mukerjee 
Indian Institute of Management Calcutta 
Joka, Diamond Harbour Road 
Kolkata 700 104, India 
E-mail: rmuk0902@gmail.com 

Abstract: We consider the effective sample size, based on Godambe information, for block likelihood 

inference which is an attractive and computationally feasible alternative to full likelihood inference 

for large correlated datasets. With reference to a Gaussian random field having a constant mean, we 

explore how the choice of blocks impacts this effective sample size. It is seen that spreading out the 

spatial points  within each block,  instead of keeping them close together,  can lead to  considerable 

gains while retaining computational simplicity. Analytical results in this direction are obtained under 

the AR(1) model. The insights so  found facilitate the study of other models, including correlation 

models on a plane, where closed form expressions are intractable.   

Key words: AR(1) model; column-wise blocking; Kronecker correlation; Matérn model; monotonic-

ity; row-wise blocking. 

1. Introduction 

The notion of effective sample size (ESS) dates back to Kish (1965, Chapter 8) who defined it, for a 

given sampling design, as the sample size required by simple random sampling to achieve the same 

variance of a parameter estimator as in the given design. In the context of identically distributed but 

correlated observations, it can be viewed as the number of observations that would carry the same 

information  on a parameter  of interest,  had the observations been independent.  Thus ESS  plays a 

crucial role in assessing the information content of the data, eliminating the duplicated information 

due to correlation. As a result, an understanding of ESS becomes important whenever the data are 

likely to show an appreciable amount of correlation as happens in diverse areas such as time series 

and spatial data analysis (Cressie, 1993, Chapters 1, 4), repeated measurements (Faes et al., 2009), 

Bayesian  model  selection  (Berger  et  al.,  2014),  importance  sampling  and  MCMC  (Martino  et  al., 

2017; Chatterjee and Diaconis, 2018), to name a few.  

  The present work is inspired by a recent paper by Acosta et al. (2021) who proposed and investi-

gated an ingenious and useful formulation of ESS, via Godambe information, for block likelihood 

inference. This was done in the framework of a Gaussian random field having a constant mean. Ear-

lier, for such a random field, Vallejos and Osorio (2014) studied ESS on the basis of Fisher infor-

mation for full likelihood inference. However, as noted by Acosta et al. (2021), full likelihood infer-

ence becomes infeasible for large correlated datasets because of the challenge in inverting the corre-

lation  matrix.  They  suggested  block  likelihood  inference  (Caragea  and  Smith,  2007;  Varin  et  al., 

1 

 
 
 
 
2011) as a statistically efficient and viable alternative that requires inversion of smaller correlation 

matrices and hence significantly reduces the computational burden. Interestingly, they showed that 

the resulting Godambe information-based ESS underestimates but, under a wide variety of parametric 

correlation structures, well approximates the Fisher information-based ESS for full likelihood infer-

ence. See their paper for more details on block likelihood inference as well as a concise but informa-

tive review of ESS in general, with further references.    

For ease in presentation, hereafter, the Godambe information-based formulation of ESS in Acosta 

et al. (2021) for block likelihood inference is called ESSB, while the Fisher information-based ESS 

in Vallejos and Osorio (2014) for full likelihood inference is referred to simply as ESS. 

In their approach to block likelihood inference, Acosta et al. (2021), kept the spatial points within 

each block as close together as possible; see their Sections 4 and 5. At the same time, they remarked 

in their Section 3 that ESSB can strongly depend on the block partitioning. From this perspective, we 

explore how the choice of blocks impacts ESSB. It is found that spreading out the spatial points within 

each block, instead of keeping them close together, can lead to considerable gains in ESSB, thus en-

tailing still better approximations to ESS. Analytical results in this direction are obtained in Section 

3 under the AR(1) model, after presenting the preliminaries in Section 2. The ideas emerging from 

these analytical results facilitate the study of other models, such as one-dimensional correlation mod-

els in Section 4, and correlation models on a plane in Section 5, where closed form expressions are 

intractable. Finally, the paper ends in Section 6 with some concluding remarks.  

2. Preliminaries 

As in Vallejos and Osorio (2014) and Acosta et al. (2021), consider a spatial random field {X(s), s ϵ  

Rp}, and let Xi = X(si),  = 1,…, n, be a realization at n spatial points s1,…, sn. Write X = (X1,…, Xn)T, 

where the superscript T stands for transpose. Suppose the random field is Gaussian with a constant 

mean μ R, and a constant variance σ2 (> 0). Then X is multivariate normal with  E(X) = μ1n and 

cov(X) = σ2R, where R = R(θ) is the n x n correlation matrix of X, with θ as a possibly vector-valued 

parameter, and 1a is the a x 1 vector of ones for any positive integer a. In what follows, R is supposed 

to be positive definite.  

In the above setup, based on a normalized version of Fisher information about μ for full likelihood 

inference, Vallejos and Osorio (2014) defined ESS as  

ESS = 

.   

(1) 

As  mentioned  in  the  introduction,  however,  full  likelihood  inference  becomes  infeasible  for  large 

correlated datasets. From this viewpoint, Acosta et al. (2021) considered a block likelihood approach 

based on a partition  {B1,…, Bm} of {1,…, n} and the  resulting product  of likelihoods of the data 

2 

innR111T− 
 
 
 
 
 
 
 
 
 
 
 
blocks X(u) = {Xi :  ϵ Bu}, u = 1,…, m. This can be viewed as a misspecified likelihood which pretends 

independence among the data blocks X(1),…, X(m). Each subset Bu, hereafter called a block, signifies 

a block of the data, and hence the partition is also called a blocking. For u, v = 1,…, m, let Ruv  be the 

b(u) x b(v)  cross-correlation matrix of X(u) and X(v), where b(u)  is the cardinality of Bu and b(1) +…+ 

b(m) = n. Note that each Ruv is a submatrix of R and any Ruu  is the correlation matrix of X(u). Based 

on a normalized version of the Godambe information about μ for block likelihood inference, Acosta 

et al. (2021) defined the effective sample size for the blocking {B1,…, Bm} as 

ESSB = 

,   

where 

=

, 

  u, v = 1,…, m.  

They observed that if m = 1, then block likelihood reduces to full likelihood and 

while, at the other extreme, if m = n, then (2) simplifies to  

ESSB = ESS = 

, 

  ESSB =

.   

(2) 

(3) 

(4) 

(5) 

Acosta et al. (2021) also noted that more generally, ESSB ≤ ESS for every blocking; cf. Godambe and 

Kale (1991, pp. 3-20). Hence it makes sense to define the efficiency of any such blocking as the ratio 

  EffB = ESSB/ESS. 

(6)  

Naturally, for any given number of blocks m and block sizes b(1),…, b(m), a blocking with a higher 

ESSB, and hence an EffB which is closer to 1, is preferred. 

Let R = 

. In the next section, we focus attention on the AR(1) model, where 

=

 (

= 

1,…, n) and 0 ≤ ρ < 1. This is a one-dimensional stationary correlation model where 

 depends on 

and j only through 

, i.e.,  

=

, 

= 1,…, n,   

(7) 

for scalars 

, with  =1. The AR(1) model allows us to obtain analytical results on ESSB 

which, in turn, facilitate an understanding of other stationary correlation models as in (7). Like AR(1), 

these models are natural candidates for consideration, for instance, when the n spatial points are eq-

uispaced over R. Moreover, as seen in Section 5, the insights gained through the AR(1) model apply 

to two-dimensional correlation models as well. An extension of the AR(1) model to the case of non-

equispaced spatial points appears in Subsection 4.2. 

For any h (≥ 0) and any integer a (≥ 2), write  

3 

iuvmvmuuumu1121)(===uv)(11T)(11vbvvuvuuubRRR−−nnR111T−)11/(T2nnRn)(jirjir||ji−ji,jiri||ji−jir||jir−ji,110,...,,−nrrr0r 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
where 

is the a x 1 vector with 1 at the first and last positions and zeros elsewhere.  

=

,   

(8) 

Proposition 1. Under the AR(1) model,  

(a) 

=

, (b) ESS = 

. 

Proof. Under the AR(1) model, for  = 1,…, n, the elements in the ith row of R sum to 

= 

=

=

,  

(9)  

so that, by (8), the ith element of 

equals  

and (a) follows. Now, (b) is immediate from (1) and (8).  

  □ 

  Part (b) of Proposition 1 is available in Vallejos and Osorio (2014), while part (a) will be seen to 

= 1, 

have implications later in a proof in the appendix. . 

3. Row- and column-wise blockings 

3.1. Two kinds of blocking 

In this paper, we shall be primarily concerned with blocks of equal size, though the case of unequal 

block size will also  be  discussed  later in  Subsection  4.3. Let  n = mb, where  m  and  b are positive 

integers, and suppose it is intended to have m blocks each of size b.  

In the framework of a one-dimensional correlation model such as the AR(1) model, we consider 

two kinds of blocking, namely, row-wise (RW) blocking and column-wise (CW) blocking. The RW 

blocking partitions {1,…, n} into the m blocks, each of size b, as given by 

 = {(u –1)b + j: j =1,…, b},  

  u = 1,…, m.  

(10) 

We call it RW as these blocks correspond to the rows of an arrangement of 1,…,  n  in the natural 

order as an m x b  array, e.g., if m = 3 and b = 5, then they are given by the rows of  

 1   2   3   4   5 
 6   7   8   9  10 . 
  11 12 13 14 15  

The CW blocking, on the other hand, partitions {1,…, n} into the m blocks  

= {u + ( j – 1)m: j =1,…, b},   u = 1,…, m, 

(11) 

each of size b. It is called CW as these blocks correspond to the columns of an arrangement of 1,…, 

n  in the natural order as a b x m  array, e.g., if m = 3 and b = 5, then they are given by the columns of  

 1   2   3 
 4   5   6 

                                                                   7   8   9 . 
  10 11 12 
  13 14 15 

4 

)(hya)1/(}1)1{(hhhaa++−anR11−)(ny)1/(}2)1({++−ni][iR||1jinj−=ijnijjiij−=−−=+11)1/()1(1−−−++−ini)(nyR)1/()}(][)1{(1+++−−−iniiRrowuBcoluB 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
For the AR(1) model, Acosta et al. (2021, Subsection 5.2) considered RW blocking which is, in-

deed,  more  intuitive  than  CW  blocking;  e.g.,  with  m  =  3  and  b  =  5,  the  blocking  {1,2,3,4,5}, 

{6,7,8,9,10}, {11,12,13, 14,15} certainly looks more natural than {1,4,7,10,13}, {2,5,8,11,14}, {3,6, 

9,12,15}, where the points in each block are much more spread out. Our analytical and computational 

results, however, show that CW blocking can entail considerable improvement over RW blocking 

with regard to ESSB under the AR(1) model and beyond. Moreover, this improvement in ESSB comes 

without sacrificing the features of RW blocking that simplify computation in these models; see Re-

mark 1 below. It is also seen in Section 5 that there is scope for a gainful extension of the idea of CW 

blocking to two- or potentially multi-dimensional correlation models.  

  We denote the ESSB for RW and CW blockings by ESSrow and ESScol, respectively. Also, follow-

ing (6), the efficiencies of these blockings are defined as 

Effrow = ESSrow/ESS, 

 Effcol = ESScol/ESS. 

(12) 

If (b, m) = (n, 1) or (1, n), then these two blockings become identical and they both satisfy (4) or (5), 

respectively. Interestingly, as the next result shows, both these blockings satisfy (5) for b = 2 as well, 

under any stationary correlation model of the form (7). 

Proposition 2. If b = 2, then under any stationary correlation model, 

 ESSrow = ESScol = 

. 

Proof. Let (7) hold. By (10) and (11), then for both RW and CW blockings, R11 = …= Rmm. Hence if 

b = 2, then for each of these blockings, the vectors

,…,

 equal the same scalar multiple 

of 

. The result now follows from (2) and (3), on simplification. 

  □ 

Remark 1. In the spirit of Proposition 2, even for general b, under any stationary correlation model, 

CW blocking enjoys the same simplifying features as RW blocking. For both these blockings, by (10) 

and (11), under such a model, any submatrix Ruv of R depends on u and v only through u – v. There-

fore, due to the symmetry of R, it follows from (2) and (3) that ESSrow and ESScol can be computed 

by obtaining the respective 

alone.  

  □ 

3.2. The AR(1) model 

We now compare RW and CW blockings under the AR(1) model. For this model, by (9),  

=

= 

, 

(13) 

Hence, from (5) and Proposition 2, the following is evident. 

Proposition 3. If b = 1 or 2, then under the AR(1) model,  

  ESSrow = ESScol =

. 

5 

)11/(T2nnRnbR1111−bmmR11−b1m111,...,nnR11T][1iRni=22)1/()}1(2)1({−−−−nn)1(2)1()1(222nnn−−−− 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In view of Proposition 3, under the AR(1) model, one needs to compare ESSrow and ESScol for 3 

≤ b < n. Theorem 1, proved in the appendix, presents useful analytical results for this purpose. 

Theorem 1. Under the AR(1) model, let n = mb, where 3 ≤ b < n. Then the ESSB for RW and CW 

blockings are given, respectively, by 

ESSrow  =

ESScol = 

, 

. 

Remark 2. Theorem 1 remains valid for b = n and b = 2 as well, with the expressions for ESSrow and 

ESScol obtained there reducing, respectively, to their counterparts in (4) and Proposition 3. For b = 1, 

however, only the expression for ESSrow in Theorem 1, and not that of ESScol in Theorem 1, matches 

its counterpart in Proposition 3. This is because the proof of Theorem 1 for ESScol, unlike that for 

ESSrow, critically involves 

which is defined only for b ≥ 2. Also, as expected, ESS, ESSrow and 

ESScol in Proposition 1(b), Proposition 3 and Theorem 1 equal n when  = 0. Furthermore, applying 

L’Hospital’s rule, one can verify that each of these quantities tends to 1 as  → 1-. 

  □ 

  The explicit results in Theorem 1 immediately allow comparison of ESSrow and ESScol  under the 

AR(1) model, for any n, b, m and ρ. Equivalently, one can as well compare the corresponding effi-

ciencies Effrow and Effcol, relative to ESS, using (12) and Proposition 1(b). Our findings, which show 

clear gains via CW blocking under the AR(1) model, are summarized below: 

(a) For every n ≤ 100000, 3 ≤ b ≤ n/2, i.e., m ≥ 2, and every ρ in {0.001, 0.002,…, 0.999}, ESScol 

exceeds ESSrow, and hence Effcol exceeds Effrow, their difference being more conspicuous for rela-

tively higher values of  ρ, e.g., if (b, m) = (30, 30), then for ρ = 0.6, 0.7, 0.8 and 0.9, the pair (Effrow, 

Effcol) equals (0.961, 0.999), (0.941, 0.998), (0.919, 0.996) and (0.913, 0.992), i.e., Effcol exceeds  

Effrow, or equivalently,  ESScol exceeds  ESSrow by 3.95%, 6.06%, 8.38% and 8.65%, respectively. 

(b) Given n, b and m, let minEffrow and minEffcol be the smallest possible Effrow and Effcol, re-

spectively, over ρ in {0.001, 0.002,…, 0.999}. These reflect the worst-scenario performances of RW 

and CW blockings and are of interest as ρ is unknown in practice. For n ≤ 100000, minEffcol exceeds 

0.9, 0.95 and 0.98 whenever b ≥ 3, 9 and 26, respectively. On the other hand, minEffrow can be con-

siderably smaller even for relatively large b, e.g., for (b, m) = (20, 10), (30, 30) and (60, 40), the pair 

(minEffrow, minEffcol) equals (0.879, 0.980), (0.878, 0.984) and (0.878, 0.991), respectively.  

6 

−−−−+++−++−bnbmmnmn111)1(22)1()1(}2)1({2)1(2}2)1)(2){(1()1(}2)1({22222mmmmmmnmn−−+−−−−+−b 
 
 
 
(c) For n ≤ 100000, the difference ESS – ESScol never exceeds 0.497, irrespective of b (≥ 3), m 

and ρ in {0.001, 0.002,…, 0.999}. Remarkably, this closeness of ESScol to ESS holds even without 

normalization through division by n. In contrast, the difference ESS – ESSrow tends to be much larger, 

e.g., for  (b, m)  = (20, 10), (30, 30)  and (60, 40),  the pair (maxDiffrow,  maxDiffcol) equals  (2.404, 

0.314),  (9.366,  0.415)  and  (17.261,  0.433),  respectively,  where  maxDiffrow  is  the  largest  possible 

value of ESS – ESSrow over ρ in {0.001, 0.002,…, 0.999} and maxDiffcol is similarly defined.  

(d) The gains via CW blocking persist even for n > 100000. For instance, if (b, m) = (100, 5000), 

then (minEffrow, minEffcol) = (0.896, 0.998), while (maxDiffrow, maxDiffcol) = (2616.123, 0.498). 

(e) The CW blocking has an attractive monotonicity property, apart from being more efficient than 

RW blocking. For each fixed n (≤ 100000) and ρ in {0.001, 0.002,…, 0.999}, ESScol is nondecreasing 

in b, which is intuitively appealing because a larger b in a sense makes block likelihood closer to full 

likelihood. Contrary to intuition, this monotonicity is not shared by RW blocking, e.g., if n = 100 and 

ρ = 0.6, then ESSrow equals 24.977, 24.763 and 24.361 for b = 4, 5 and 10, respectively.  

  Because of (a)-(d) above, one can reasonably conjecture that ESScol > ESSrow in general. Though 

all computational evidence goes in support of this, a proof remains elusive due to the complicated 

forms of ESScol and ESSrow in Theorem 1, as a result of which the plot of ESScol – ESSrow against ρ 

often has multiple turning points. However, for all practical purposes, we hope that the aforesaid facts 

(a)-(d), which quantify the gains via CW blocking, will be as informative as such a general result. 

3.3. On the possibility of still more efficient blocking 

One may wonder if, given n, b and m, there can be another blocking which is more efficient than CW 

blocking. In view of the facts (b) and (c) in the last subsection, this appears to be unlikely in general. 

Extensive simulations in search for such a better alternative, if any, show some promise only for a 

variant of CW blocking. This variant, called modified column-wise (MCW) blocking, is obtained by 

(i) arranging 1,…, n  in the natural order as a b x m  array, (ii) then reversing the even rows of this 

array, and (iii) finally forming blocks as per the columns of the array reached in (ii). For example if 

m = 3 and b = 5, then MCW blocking consists of blocks as given by the columns of  

 1   2   3 
 6   5   4 

                                                                   7   8   9 . 
  12 11 10 
  13 14 15 

The difference between MCW and CW blocking is that the latter goes straight from (i) to (iii) above 

without the reversal of even rows in (ii). Let Effmod denote the efficiency of MCW blocking, defined 

as in (6). Then the following hold under the AR(1) model. 

7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
If m = 2, then Effmod > Effcol, for 2 ≤ b ≤ 100 and ρ in {0.1, 0.2,…, 0.9}, though Effmod exceeds  

Effcol by less than 1%, for every such ρ whenever b ≥ 6, and their difference tapers off quickly with 

further increase in b. On the other hand, if m > 2, then both Effmod > Effcol and Effmod < Effcol can 

happen depending on ρ. Indeed, then it is also possible that Effmod < Effcol, for every ρ in {0.1, 0.2,…, 

0.9}, as happens, e.g., with (b, m) = (5, 50), (10, 35), (20, 30), (30, 25) etc. Thus, MCW blocking has 

a higher efficiency than CW blocking only in some special situations and there too the improvement 

is often marginal. At the same time, any such gain comes at the cost of much additional computation 

because MCW blocking does not share the simplifying features of CW blocking as observed in Re-

mark 1. Due to these reasons, we do not consider MCW blocking any more.  

3.4. AR(1) model: case of block size two 

Before concluding this section, we briefly touch upon the case b = 2. By Proposition 2, then ESSrow 

= ESScol, under any stationary correlation model as in (7), including the AR(1) model. A perturbed 

version of RW blocking turns out to be more efficient than both RW and CW blockings in this case. 

Let n = 2m, b = 2, and consider a blocking with m blocks  

{u, 2m + 1 – u} (u = 1,…, g)   

and 

 {g + 2u – 1, g + 2u} (u = 1,…, m – g), 

where the positive integer g satisfies 1 ≤ g ≤ m – 1. This is called perturbed row-wise blocking of 

order g, or PRW(g) blocking, as the last m – g blocks also arise in RW blocking when g is even. For 

example, if n = 12, b = 2 and g = 2, then the six blocks are {1,12}, {2,11}, {3,4}, {5,6}, {7,8} and 

{9,10}, the last four of which also appear in RW blocking.  

  Under the AR(1) model, PRW(g) blocking is seen to perform the best vis-à-vis RW and CW block-

ings when g = 2. With b = 2 and efficiency defined as in (6), we find that PRW(2) blocking has higher 

efficiency than RW and CW blockings for every even n  ≤ 1000 and every ρ in {0.1, 0.2,…, 0.9}, 

though the gains are less prominent for larger n, because then PRW(2) and RW blockings have a vast 

majority of blocks in common. On a positive note, for the same reason, the computational burden for 

PRW(2) blocking is also about the same as that for RW blocking when n is large. 

4. More on one-dimensional correlation models 

4.1. Two more stationary correlation models 

The results under the AR(1) model, based on closed-form expressions for ESSrow and ESScol, suggest 

a similar outcome also for other one-dimensional stationary correlation models as in (7) even if they 

do not allow analytical derivation. We consider two such models as given by 

I. (linear)    

  R = 

, with 

II. (inverse linear)   R = 

, with 

=

=

 (

=1,…, n) and 0 < ρ ≤ 1/(n –1). 

 (

=1,…, n) and ρ > 0. 

8 

)(jirjir||1ji−−ji,)(jirjir|)|1/(1ji−+ji, 
 
 
Like the AR(1) model, both these are stationary and ensure 

≥ 0, with the value of 

tapering off 

with increase in the distance 

, as one expects intuitively. The rate of tapering, however, is not 

the same for these models: it is  exponential  under the  AR(1) model, constant under model  I, and 

inverse quadratic under model II. Note that all 

are positive under model II. The same holds under 

model I as well, except for the solitary case of 

=

= 0 when ρ = 1/(n –1).  

  Our  findings  under  models  I  and  II  are  similar  to  those  under  the  AR(1)  model  as  reported  in 

Subsection 3.2, though the computations are now less extensive due to lack of analytical results as in 

Theorem 1. We consider ten values of ρ under either model I or model  II, namely, (n –1)ρ = 0.1, 

0.2,…,1.0 under I, and ρ = 0.2, 0.4,…,2.0 under II. Accordingly, given n, b and m, under any of these 

models, now we write minEffrow and minEffcol to denote the smallest possible Effrow and Effcol, re-

spectively, over the corresponding ten values ρ. As indicated below, CW blocking again has a clear 

edge over RW blocking: 

(a) Under both models I and II, for every n ≤ 1000, 3 ≤ b ≤ n/2, i.e., m ≥ 2, and every ρ as stated 

above, ESScol > ESSrow, and hence Effcol > Effrow. The gains can be quite substantial under model I, 

e.g., if n = 900, (b, m) = (30, 30), then under this model, the pair (Effrow, Effcol) equals (0.923, 0.995), 

(0.875, 0.991), (0.819, 0.986) and (0.751, 0.979) for (n –1)ρ = 0.4, 0.6, 0.8 and 1.0, respectively.  

(b) For n ≤ 1000, the following hold. Under model I, minEffcol exceeds 0.9, 0.95 and 0.98 when-

ever b ≥ 6, 13 and 32, respectively, and minEffrow can be much smaller even for relatively large b. 

On the other hand, under model II, minEffcol exceeds 0.95 and 0.98 whenever b ≥ 3 and 11, respec-

tively, and compared to model I, minEffrow comes closer to minEffcol. For instance, with (b, m) = (20, 

10) and (30, 30), the pair (minEffrow, minEffcol) equals (0.753, 0.972) and (0.751, 0.979), respec-

tively, under model I, and (0.946, 0.988) and (0.960, 0.991), respectively, under model II. 

(c) Additional computations for n > 1000 yield results very similar to the above, e.g., if (b, m) = 

(60, 40), then the pair (minEffrow, minEffcol) equals (0.750, 0.989) under model I, and (0.964, 0.994) 

under model II.   

(d) Under both models I and II, for each fixed n (≤ 500) and ρ as stated above, ESScol is nonde-

creasing in b, i.e., the monotonicity property of CW blocking continues to hold under models I and 

II. Our computations suggest that RW blocking shares this property only under model I but not under 

model II.  

Turning to the case of block size two, as in Subsection 3.4, we find that PRW(2) blocking improves 

upon the common efficiency of RW and CW blockings, under models I and II as well. Under both 

9 

jirjir||ji−jirnr11nr 
 
models, PRW(g) blockings for g > 2 can lead to further improvement in efficiency. Because such 

gains are marginal and come at the expense of increased computational burden, we omit the details. 

4.2. Non-equispaced AR(1) model 

Stationary correlation models as in (7), such as the AR(1) model or models I and II in the last sub-

section implicitly assume that the n spatial points are equispaced over R. Such equal spacing is com-

mon in practice and, at least for the AR(1) model, is known to be optimal, in the sense of maximizing 

ESS, among all spacings over a fixed interval in R; see Kiselák and Stehlík (2008).  

The spatial  points  can, however, be non-equispaced in observational  studies when they are not 

chosen by design. To address this issue, we now consider a more general AR(1) model. With n spatial 

points s1,…, sn, satisfying s1 < …< sn, it is given by R = 

, where 

=

, 

= 1,…, n,   

(14) 

and 0 ≤ ρ < 1. Reassuringly, under (14), CW blocking continues to have higher efficiency than RW 

blocking when the spatial points do not deviate too much from being equispaced. For brevity, only 

two of many illustrative situations are presented below. Here t = (s2 – s1, s3 – s2,…, sn – sn –1) and 

is the mean of the elements of t.  

(i) n = 200, (b, m) = (20, 10), t = (t11

, t12

,

), with t11 = 0.8, t12 = 0.9; 

(ii) n = 384, (b, m) = (16, 24), t = (t21

, t22

, 

, t22

, t21

), with t21 = 1.2, t22 = 1.1. 

In (i), t is asymmetric about its centre and  < 1, whereas in (ii), t is symmetric about its centre and 

> 1. However, under both (i) and (ii), Effcol exceeds Effrow for every ρ in {0.1, 0.2,…, 0.9}, their 

difference being more prominent, as in the equispaced case, when ρ is relatively large, e.g., for ρ = 

0.6, 0.7, 0.8, 0.9, the pair (Effrow, Effcol) equals (0.948, 0.991), (0.930, 0.988), (0.915, 0.985), (0.914, 

0.981) under (i), and (0.956, 0.995), (0.938, 0.993), (0.924, 0.989), (0.931, 0.982) under (ii).  

4.3 Blocks of unequal size 

So far, we have focused on blocking of n = mb spatial points into m blocks each of the same size b, 

where 1 < b < n. This is infeasible if n is a prime. Even otherwise, blocks of equal size may become 

unattractive due to the structure of n, e.g., if n = 2q, where q is a large odd prime, then b has to be 

either 2 or q, which may be considered too small or too large. From this perspective, we now consider 

the situation where n is not necessarily an integral multiple of the number of blocks, m, and the block 

sizes are as nearly equal as possible. Writing b for the largest integer in n/m and f = n – mb, then there 

are f  blocks, each of size b + 1, and m – f  blocks, each of size b.  

As a generalization of (10), now RW blocking partitions {1,…, n} into the m blocks 

 {(u – 1)(b + 1) +  j:  j =1,..., b + 1},  u =1,..., f,   and  { f + (u – 1)b + j:  j =1,..., b},  u = f + 1,..., m. 

Similarly, generalizing (11), CW blocking now partitions {1,…, n} into the m blocks 

10 

)(jirjir||jsis−ji,tT301T1291T401T1001T801T231T801T1001tt 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  {u + ( j – 1)m:  j =1,…, b + 1},  u = 1,…, f,   and  {u + ( j – 1)m:  j =1,…, b}, u = f + 1,…, m. 

For example, if n = 17, m = 3, then b =5,  f =2, and the RW blocks are given by the rows of  

 1   2   3   4   5    6 
 7   8   9  10 11  12 

  13 14 15 16 17 

while the CW blocks are given by the columns of  

 1   2   3 
 4   5   6 

                                                                   7   8   9 . 
  10 11 12 
  13 14 15 
  16 17 

  We compare RW and CW blockings, as indicated above, under three stationary correlation models, 

namely, the original AR(1) model, 

=

, with ρ in {0.1, 0.2,…, 0.9}, and models I and II of 

Subsection 4.1, each with ten values of ρ as mentioned there. Under such stationary models, both 

blockings retain to some extent their simplifying features as noted in Remark 1. Their comparison 

over a range of n and m, however, requires more effort because the possibilities for m are now much 

more numerous than before as n/m need not be an integer. Hence, we consider n ≤ 200 and 2 ≤ m < 

n/2, so that each block has size at least two and not all blocks have size two. Our findings, summarized 

below for this range of n and m, greatly resemble their counterparts for blocks of equal size.  

(a) Under all three models and for every ρ as stated above, Effcol > Effrow, whenever each block 

size is at least four. Moreover, Effcol > 0.95 for every such ρ if each block size is at least 9, 13 or 3, 

under the AR(1) model, model I or model II, respectively. 

(b) Under model I, for every ρ as stated above, Effcol always exceeds Effrow. 

(c) Under the AR(1) model or model II, even if Effcol < Effrow, for any ρ in some cases that involve 

blocks of size two or three, it always holds that Effcol > Effrow – 0.011, so that the shortfall is never 

significant. At any rate, typically, blocks of size two or three are not of much interest. 

(d) In contrast to (c) above, over the aforesaid range of n and m and also beyond it, there are many 

situations where Effcol is appreciably larger Effrow, especially under the AR(1) model and model I. 

As an illustration of the point in (d) above, let (n, m) = (890, 30). Then the pair (Effrow, Effcol) 

equals (0.961, 0.999), (0.941, 0.998), (0.918, 0.996), (0.913, 0.992) for ρ = 0.6, 0.7, 0.8, 0.9, respec-

tively, under the AR(1) model, and (0.924, 0.995), (0.876, 0.991), (0.819, 0.986), (0.752, 0.979) for 

(n –1)ρ = 0.4, 0.6, 0.8 and 1.0, respectively, under model I. These figures are almost identical to the 

ones seen earlier for the equal block size case n = 900, (b, m) = (30, 30). Similar examples abound. 

11 

jir||ji− 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Matching our intuition, they suggest that the figures for a close-by equal block size case, where com-

putations are less demanding, can well indicate the outcome when n/m is not an integer.  

5. Two-dimensional correlation models 

We now turn to two-dimensional correlation models. The discussion here can, in principle, be ex-

tended to the multi-dimensional situation as well at the expense of heavier notation.  

Let R be a positive definite correlation matrix of order 

(

≥ 2), with rows and columns 

indexed by ordered pairs 

 ( =1,…,

; k = 1,2) representing coordinates of spatial points on a 

plane. Let I be the set of these 

 pairs, and suppose

=

 (k = 1,2), where 

,

,

,

 are 

positive integers. One can extend the concepts of RW and CW blocking to this two-dimensional setup 

using Cartesian products of sets, in such a manner that they both partition I into m = 

blocks, 

each of size b = 

. The blocks in RW blocking are  

 = 

( =1,…,

; 

= 1,…, 

), 

(15) 

while those in CW blocking are 

 = 

( =1,…,

; 

= 1,…, 

), 

(16) 

where  stands for Cartesian product, and for 

=1,…,

(k = 1,2),  

= {

:  j = 1,…,

},  

= {

:  j =1,…,

}.   

In view of (10) and (11), for k = 1,2, 

= {

: 

=1,…,

}    and  

={

: 

=1,…,

},   

(17) 

are RW and CW blockings, respectively, for the set {1,…,

} in one dimension. 

  As a simple example, if  =8, 

=6, 

=2,  =4, 

=2,

=3, then  

= {1, 2, 3, 4}, 

= {5, 6, 7, 8},  

= {1, 2, 3}, 

= {4, 5, 6}, 

= {1, 3, 5, 7},  

= {2, 4, 6, 8},   

= {1, 3, 5},  

 = {2, 4, 6}. 

Hence by (15), the RW blocking in two dimensions involves the blocks 

   {11, 12, 13, 21, 22, 23, 31, 32, 33, 41, 42, 43},    {14, 15, 16, 24, 25, 26, 34, 35, 36, 44, 35, 46}, 

   {51, 52, 53, 61, 62, 63, 71, 72, 73, 81, 82, 83},     {54, 55, 56, 64, 65, 66, 74, 75, 76, 84, 85, 86}, 

and by (16), the CW blocking in two dimensions involves the blocks 

   {11, 13, 15, 31, 33, 35, 51, 53, 55, 71, 73, 75},     {12, 14, 16, 32, 34, 36, 52, 54, 56, 72, 74, 76}, 

   {21, 23, 25, 41, 43, 45, 61, 63, 65, 81, 83, 85},    {22, 24, 26, 42, 44, 46, 62, 64, 66, 82, 84, 86}. 

In the spirit of Acosta et al. (2021) (see their Subsection 5.3 on Matérn correlation on a plane), 

RW blocking keeps the paired indices within each block, and hence the points represented by them, 

12 

21nn21,nn21iikikn21nnknkkbm1m1b2m2b21mm21bbrow21uuB(2)row2(1)row1uuBB1u1m2u2mcol21uuB(2)col2(1)col1uuBB1u1m2u2mkukm)row(kkuBjbukk+−)1(kb)col(kkuBkkmju)1(−+kbrow)(kB)row(kkuBkukmcol)(kB)col(kkuBkukmkn1n2n1m1b2m2b(1)row1B(1)row2B(2)row1B(2)row2B(1)col1B(1)col2B(2)col1B(2)col2B 
 
  
 
 
 
    
 
 
 
 
 
 
 
 
 
close together. On the other hand, in CW blocking these are much more spread out within each block 

and, as in the one-dimensional situation, this can lead to appreciable gains in  ESSB. Proposition 4 

below, which refers to a Kronecker correlation structure, motivates the ideas. We write 

for Kron-

ecker product and continue to denote the effective sample sizes under full likelihood inference and 

under the RW and CW blockings in (15) and (16) by ESS, ESSrow and ESScol, respectively. 

Proposition 4. Let R =

, where 

and 

are correlation matrices of orders 

 and 

, respectively. Then 

(a) ESS = ESS(1)ESS(2),  (b) ESSrow = 

,  (c) ESScol =

,   

where ESS(k),

and 

are the effective sample sizes, apropos of 

, under full likeli-

hood inference and under the one-dimensional blockings 

and 

, respectively, k = 1,2. 

Proof.  The  truth  of  (a)  is  evident  from  (1).  Next,  (b)  follows  from  (2)  and  (3)  noting,  after  some 

refection, that the submatrices of R induced by the RW blocking in (15) are Kronecker products of 

the corresponding submatrices of 

and 

induced by 

and 

in (17). Similarly, (c) 

follows invoking (16) and (17).   

  □ 

In view of Proposition 4 and the facts noted in Sections 3 and 4, if R =

, with

and 

given by the AR(1) model, model I or model II, then CW blocking in (16) has a distinct ad-

vantage over RW blocking in (15). Moreover, if both 

and 

correspond to the AR(1) model, 

then analytical formulae ESSrow and ESScol emerge from Proposition 4, together with Theorem 1. 

One specific instance of such a Kronecker correlation structure is the Matérn model based on the L1-

distance, with smoothness parameter 1/2, as given by  

= 

 = 

, 

> 0, 

(18) 

where ρ =

 (0 < ρ <1), 

is the 

element of R, and d1 = 

+ 

 is the L1-distance between 

and 

. In this case, 

and 

are given by the AR(1) 

model with the same ρ. 

If R has a Kronecker structure, then by (6) and Proposition 4, the efficiency of RW blocking in 

(15) equals the product of the efficiencies of the corresponding one-dimensional blockings. The same 

happens with CW blocking in (16) as well. Thus, the efficiencies in two dimensions are typically 

smaller than their counterparts in one dimension. This phenomenon, which conforms to the curse of 

dimensionality, holds also when R does not have a Kronecker structure; see, e.g., Table 1 below. At 

any rate, as indicated in the previous paragraph and confirmed by the computations reported below, 

CW blocking continues to outperform RW blocking with regard to efficiency in two dimensions.  

13 

)2()1(RR)1(R)2(R1n2n)2(row)1(rowESSESS)2(col)1(colESSESS)(rowESSk)(colESSk)(kRrow)(kBcol)(kB)1(R)2(Rrow)1(Brow)2(B)2()1(RR)1(R)2(R)1(R)2(R),(2121jjiir)/exp(1d−1d)/1exp(−),(2121jjiirth),(2121jjii||11ji−||22ji−21ii21jj)1(R)2(R 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  Turning to models without a Kronecker correlation structure, we consider Matérn models based 

on the L2-distance, with smoothness parameter 1/2 and 3/2, as given, respectively, by 

= 

 = 

, 

> 0, 

= 

=

,   

> 0, 

(19) 

(20) 

where d2 = 

and, as in (18), ρ =

 (0 < ρ <1). Along the lines of 

Remark 1, under these models, CW blocking shares the features of RW blocking that facilitate com-

putation, e.g., by (15), the submatrix of R, given by rows 

ϵ

and columns

ϵ

, de-

pends on 

and 

only through 

 and 

, and by (16), the same happens for CW 

blocking as well.   

  Under all  three  correlation  models (18)-(20), we find  that  ESScol > ESSrow, and hence Effcol > 

Effrow, for every n ≤ 500 with

,

≥ 3 and 

,

 ≥ 2, and every ρ in {0.1, 0.2,…, 0.9}. The same 

happens also for n > 500 in each of the many situations that we studied. Some examples appear in 

Table 1. In all these, whether n ≤ 500 or not, Effcol exceeds Effrow for every ρ in {0.1, 0.2,…, 0.9}, 

but the pair (Effrow, Effcol) is shown only for ρ = 0.6, 0.7, 0.8 and 0.9, where they differ more sub-

stantially under all three models. Indeed, under the Matérn model (20), based on the L2-distance and 

with smoothness parameter 3/2, which is attractive  due to its smoothness properties, Effcol can be 

appreciably larger than Effrow even for smaller ρ, e.g., for the case n = 2400, (

,

,

,

) = (5, 6, 

8, 10) in Table 1, the pair (Effrow, Effcol) equals (0.890, 0.971), (0.850, 0.950), (0.827, 0.924) and 

(0.818, 0.895) under this model, for ρ = 0.2, 0.3, 0.4 and 0.5, respectively. 

Table 1. The pair (Effrow, Effcol) in some examples 

      (

,

,

,

)     Model                                                     ρ   

0.6                     0.7                    

 0.8                    0.9  

216 
(6, 4, 3, 3)            (18)   (0.888, 0.943)  
                                           (19)   (0.841, 0.905)  
                                          (20)   (0.750, 0.845)  

(0.858, 0.934)  
(0.810, 0.898)  
(0.730, 0.846)  

(0.834, 0.931)  
(0.787, 0.902)  
(0.729, 0.863)  

(0.841, 0.943)  
(0.803, 0.925)  
(0.781, 0.908) 

1680      (8, 6, 7, 5)   

2400   

(5, 8, 6, 10)  

(18)  (0.895, 0.959)  
(19)   (0.862, 0.924)  
(20)  (0.798, 0.867)  

(0.870, 0.943)  
(0.839, 0.902)  
(0.775, 0.846)  

(0.843, 0.928)  
(0.804, 0.888)  
(0.725, 0.836)  

(0.804, 0.923) 
(0.751, 0.892)  
(0.666, 0.858)  

(18)   (0.899, 0.960)  
(19)  (0.870, 0.926)  
(20)  (0.810, 0.865)  

(0.878, 0.943)  
(0.850, 0.899)  
(0.788, 0.837)  

(0.854, 0.923)  
(0.817, 0.876)  
(0.738, 0.820)  

(0.817, 0.912)  
(0.763, 0.874)  
(0.670, 0.835)  

14 

),(2121jjiir)/exp(2d−2d),(2121jjiir)/exp()}/(1{22dd−+22}log1{dd−2/1222211})(){(jiji−+−)/1exp(−21iirow21uuB21jjrow21vvB21uu21vv11vu−22vu−1b2b1m2m1b2b1m2mn1b2b1m2m 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  We finally demonstrate how CW blocking entails improved efficiency in a big data setup. This 

example relates to the forest dataset considered in Acosta et al. (2021, Section 7). Let n = 21026304, 

(

,

,

,

) =  (54, 36, 104, 104). Here n is too large to allow computation of ESS under models 

(19) and (20) which do not have a Kronecker correlation structure. However, invoking the simplifying 

features of CW and RW blockings in these models as noted earlier, one can obtain ESScol and ESSrow, 

and  hence  the  percentage  gain  in  efficiency  via  CW  blocking  over  RW  blocking  as  given  by 

100x(ESScol – ESSrow)/ESSrow. Table 2 shows these percentage gains under models (18)-(20) for ρ 

in {0.1, 0.2,…, 0.9}. Again, under all three models, these gains are impressive for relatively large ρ, 

and under model (20), they remain so even for smaller ρ. 

Table 2. Percentage gains in efficiency via CW blocking over RW blocking for n = 21026304 and 

,

,

 (

,
Model                                                        
 0.4 
  0.2    

) = (54, 36, 104, 104). 

    0.3  

 0.1    

  ρ   
   0.5   

 0.8           0.9  
  (18)   0.10        0.42         1.05         2.10         3.78         6.38        10.37     16.30       21.32 
  (19)      0.23        1.03         2.50         4.73         7.83        11.92       16.94     21.68        18.60 
  (20)      1.63        4.57         8.36        12.82       17.64       22.27       25.39     23.25        8.53 

  0.6      

 0.7     

6. Concluding Remarks 

In this paper, we explored the impact of the choice of blocks on block likelihood inference for Gauss-

ian random fields with a constant mean. Under a wide variety of correlation models in one and two 

dimensions, CW blocking was found to be considerably more efficient than RW blocking while re-

taining the computational simplicity of the latter. Several open issues emerge from the current work. 

  Under the AR(1) model, all computational evidence based on Theorem 1 suggest that ESScol is 

always  greater than ESSrow. A formal proof of this  is  as yet intractable,  but  will  be of theoretical 

interest. Furthermore, for any of the one- or two-dimensional correlation models studied here, it will 

be worthwhile to know if there exists a blocking that shares the simplifying features of CW blocking 

but  enjoys still higher efficiency under  wide generality.  Again, our findings indicate that no such 

blocking exists, but any sufficiently general result in this direction will be welcome. 

In addition, the points discussed by Acosta et al. (2021, Section 8) deserve attention also in the 

context of the present  paper.  The foremost of these concerns block likelihood inference  in spatial 

regression  models  (Acosta  and  Vallejos,  2018).  As  a  first  attempt  towards  understanding  the  role 

played there by the choice of blocks, we considered one-dimensional stationary correlation models 

in the special case of a single predictor and no intercept. Then ESSB and EffB can be defined along 

the lines of (2) and (6) replacing the vectors of ones in (1) or (3) by the predictor vector or subvectors 

thereof, and our initial findings again go in favour of CW blocking. To cite just one of many such 

examples, with a single predictor vector z = (z1,…,zn)T which plays the role of 1n in Section 2 , if n = 

15 

1b2b1m2m1b2b1m2m 
 
 
 
 
 
 
900, zi 

( = 1,…, n) and we have  m = 30 blocks each of size b = 30, then under the AR(1) model, 

ESScol > ESSrow for every ρ in {0.1,…, 0.9} and the pair (Effrow, Effcol) equals (0.962, 0.997), (0.943, 

0.995), (0.919, 0.990), (0.907, 0.979) for ρ = 0.6, 0.7, 0.8 and 0.9, respectively. These figures are akin 

to the ones seen in Subsection 3.2 for n = 900, (b, m) = (30, 30), in the setup of a constant mean. The 

outcome remains similar for models I and II of Section 4, and the gains through CW blocking turn 

out to be even more significant under model I.  

The picture mentioned in the last paragraph, though promising, is of course quite incomplete and, 

more  generally,  much  will  depend  on  the  predictors.  Following  Acosta  and  Vallejos  (2018),  with 

multiple predictors as well as an intercept, ESS can be defined as the trace of a matrix valued version 

of (1) after scaling each predictor properly. Similar considerations should also apply to EESB. Despite 

these new features, the basic idea behind CW blocking, i.e., spreading out the spatial points within 

each block in some sense, may continue to be useful.  

  Another issue mentioned by Acosta et al. (2021) that remains relevant here too concerns the case 

of irregularly spaced spatial data. It will be of interest to examine how the strategy of spreading out 

the points within each block can work there retaining, at the same time, the computational advantage 

of CW blocking as seen in this paper. Further directions for future research include study of alterna-

tive sample size reduction techniques as well as extension to non-Gaussian random fields in the pres-

ence of asymmetry or heavy tails; see e.g., Bevilacqua and Gaetan (2015), Sun et al. (2018) and Xu 

and Genton (2017). However, as noted by Acosta et al. (2021), obtaining a convenient expression for 

the Godambe information is likely to be a challenge in these situations  

  We  conclude  with  the  hope  that  the  present  endeavour  will  generate  interest  in  the  above  and 

related problems. 

Appendix 

Proof of Theorem 1. 

(a) First consider ESSrow. For u = 1,…, m, by (10), Ruu is b x b, with 

element 

 (

= 1, 

…, b), so that by Proposition 1(a), 

and hence by (3) and (8), 

=

, 

(A.1) 

=

=

=

.   

(A.2) 

Again, by (10), for u < v, Ruv is b x b, with 

element 

 (

=1,…, b), i.e., 

  Ruv =

, 

(A.3) 

16 

2iith),(ji||ji−ji,buuR11−)(byuumu1=buubmuR111T1−=)(1Tbbym)1/(}2)1({++−mnth),(jiijbuv−+−)(ji,T21)(buv− 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
where 

and 

.  Because  by  (8), 

=

  and 

 = 1, from (3), (A.1) and (A.3), one obtains

=

, for u < v. As a result, for u > 

v, the symmetry of R yields 

=

. Thus, 

=

, for every u ≠ v. So,  

=

=

,  

(A.4) 

where 

is the m x m correlation matrix under an AR(1) model with

changed to

. Since by (13),  

=

 =

, 

from (A.4) one gets 

=

. 

(A.5) 

Substituting (A.2) and (A.5) in (2), the expression for ESSrow follows. 

(b) We next consider ESScol. For u = 1,…, m, by (11), Ruu is b x b, with 

element 

(

= 1,…, b), so that analogously to (A.1) and (A.2) by Proposition 1(a),  

and  

=

,  

=

.  

(A.6) 

(A.7) 

  The Ruv, however, no longer admit a factorization as in (A.3), and hence their handling now re-

quires more effort. To that effect, observe that by (3), (8) and (A.6), for u, v = 1,…, m, 

 =

= 

= 

Now, by (13), 

=

= 

, 

. 

.  

(A.8) 

(A.9) 

Next, recall that 

 denotes the sum of the elements of the ith row of R, and from (11) observe that, 

for u = 1,…, m, the vector 

is b x 1, with jth element given by R[u + ( j – 1)m]  ( j = 1,…, 

b). Hence, by (9) and the definition of 

, 

= R[u] + R[u + (b – 1)m] 

 =

. 

Since n = mb, summing the above over u = 1,…, m, after some simplification, 

17 

T)1(11),...,,1(−−−=bT12),...,,1(−=b)(T1by)1(−−b)(T2byuv)1()(−−−bbuvuv)1()(−−−bbvuuv)1(||−−−bbvuuvmuvmu1)(1==bvumuvmub||1)(1)1(−==−−)1~1(T)1(mRmmb−−−R~bmmR1~1T22)1()1(2)1(bmbbbm−−−−−−−+−bnbbbm1)1(2)1(11uvmuvmu1)(1==−−−−bnbm1112th),(jimji||−ji,buuR11−)(mbyuumu1=)1/(}2)1({mmmmn++−uv)()(TmbuvmbyRy2T)1/(}1)1{(}1)1{(mbmbmuvbmbmR++−+−2T2TT2)1/(}1)1(211)1{(mbuvbmbuvbmmbuvbmRRR++−+−buvbmvmuR11T11==nnR11T22)1/()}1(2)1({−−−−nn][iRbuvmvR11=b)1(1TbuvmvbR=)1/(})1(2{1)1()1(1−−−−−++−−−−++−mbunmbuunu 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
       
 
 
 
 
 
 
=

. 

(A.10) 

For u, v = 1,…, m, from (11), now note that the (1,1)th, (1,b)th, (b,1)th and (b,b)th elements of Ruv 

equal 

, 

,

 and

, respectively. Since v – u + (b – 1)m and u – v + (b 

– 1)m and are both nonnegative as b ≥ 3 and |u – v| ≤ m –1, recalling the definition of 

, 

=

 =

=

because  

analogously to (13), and 

.  

,   

(A.11) 

=

,   

=

=

. 

Finally, by (A.8)-(A.11), after a long algebra, 

=

.  

(A.12) 

Substituting (A.7) and (A.12) in (2), the expression for ESScol follows.  

  □ 

Acknowledgement. This work was supported by a grant from the Science and Engineering Research 
Board, Government of India. 

References 

Acosta, J., Alegría, A., Osorio, F. and Vallejos, R. (2021). Assessing the effective sample size for 
large spatial datasets: A block likelihood approach. Comput. Stat. Data Anal. 162, 107282. 

Acosta, J. and Vallejos, R. (2018). Effective sample size for spatial regression models. Electron. J. 
Stat. 12, 3147-3180. 

Berger, J., Bayarri, M.J. and Pericchi, L.R. (2014). The effective sample size. Econom. Rev. 33, 197-
217. 

Bevilacqua, M. and Gaetan, C. (2015). Comparing composite likelihood methods based on pairs for 
spatial Gaussian random fields. Stat. Comput. 25, 877-892. 

Caragea, P.C. and Smith, R.L. (2007). Asymptotic properties of computationally efficient alternative 
estimators for a class of multivariate normal models. J. Multivar. Anal. 98, 1417-1440. 

Chatterjee, S. and Diaconis, P. (2018). The sample size required in importance sampling. Ann. Appl. 
Probab. 28, 1099-1135. 

Cressie, N.A.C. (1993). Statistics for Spatial Data. Wiley, New York. 

Faes, C., Molenberghs, G., Aerts, M., Verbeke, G. and Kenward, M. (2009). The effective sample 
size and an alternative small-sample degrees-of-freedom method. Amer. Stat. 63, 389-399. 

Godambe, V.P. and Kale, B.K. (1991). Estimating functions: an overview. In: Godambe, V.P. (Ed.), 
Estimating Functions. Clarendon Press, Oxford. 

18 

buvbmvmuR1T11==22)1/()}1)(1()1({2−−+−−−mmnm||vu−|)1(|mbvu−−−|)1(|mbvu−+−||vu−bbuvbmvmuRT11==)2()1()1(||11mbvumbuvvumvmu−+−−+−−==++}(2)1(||11mbvuvumvmu−+−−==+22122)1/(})1()1(2)1({2−−+−−−+−mmnmm||11vumvmu−==22)1/()}1(2)1({−−−−mmmbvumvmu)1(11−+−==))((11)1(vmvumumb−==−2212)1/()1(−−+−mmnuvmvmu11==22222)1()1()1(2}2)1)(2){(1(−+−−+−−−mmmmmn 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Kiselák, J. and Stehlík, M. (2008). Equidistant and D-optimal designs for parameters of Ornstein-
Uhlenbeck process. Stat. Probab. Lett. 78, 1388-1396. 

Kish, L. (1965). Survey Sampling. Wiley, New York.  

Martino, L., Elvira, V. and Louzada, F. (2017). Effective sample size for importance sampling based 
on discrepancy measures. Signal Process. 131, 386-401. 

Sun, Y., Chang, X. and Guan, Y. (2018). Flexible and efficient estimating equations for variogram 
estimation. Comput. Stat. Data Anal. 122, 45-58. 

Vallejos, R. and Osorio, F. (2014). Effective sample size of spatial process models. Spat. Stat. 9, 66-
92. 

Varin, C., Reid, N. and Firth, D. (2011). An overview of composite likelihood methods. Stat. Sin 21, 
5-42. 

Xu, G. and Genton, M.G. (2017). Tukey g-and-h random fields. J. Am. Stat. Assoc. 112, 1236-1249. 

19 

 
 
 
 
 
 
 
 
 
 
 
