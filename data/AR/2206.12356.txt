2
2
0
2

n
u
J

4
2

]

V
C
.
s
c
[

1
v
6
5
3
2
1
.
6
0
2
2
:
v
i
X
r
a

HM3D-ABO: A Photo-realistic Dataset for Object-centric Multi-view 3D Reconstruction
Technical Report

Zhenpei Yang∗ Zaiwei Zhang Qixing Huang
The University of Texas at Austin

Abstract

ShapeNet Choy [9]

Things3D [42]

HM3D-ABO

Reconstructing 3D objects is an important computer vi-
sion task that has wide application in AR/VR. Deep learn-
ing algorithm developed for this task usually relies on an
unrealistic synthetic dataset, such as ShapeNet [7, 9] and
Things3D [42]. On the other hand, existing real-captured
object-centric datasets usually do not have enough anno-
tation to enable supervised training or reliable evaluation.
In this technical report, we present a photo-realistic object-
centric dataset HM3D-ABO. It is constructed by composing
realistic indoor scene [29] and realistic object [10]. For
each conﬁguration, we provide multi-view RGB observa-
tions, a water-tight mesh model for the object, ground truth
depth map and object mask. The proposed dataset could
also be useful for tasks such as camera pose estimation
and novel-view synthesis. The dataset generation code is
released at https://github.com/zhenpeiyang/
HM3D-ABO.

1. Introduction

Reconstructing 3D object has been studied for
decades [3, 19, 21, 27, 28, 39, 43–45, 50] and has been
receiving more and more interest due to the recent pop-
ularity AR/VR applications. With multi-view renderings
from object model collection datasets [7, 10, 12, 14, 30, 41],
researchers can develop data-driven approaches for object
reconstruction but the clean backgrounds introduce a do-
main gap with real-world images. Things3D [42] has tried
to attach different backgrounds to those object renderings
by placing CAD models in synthetic 3D indoor scenes.
However, their renderings of synthetic indoor environments
is still far from realistic. On the other hand, several works
have also tried to create 3D object annotations from real
2D images, such as Pix3D [35] and Scan2CAD [2]. Yet
these works either only provide a single-view image [35]
setting or does not provide an accurate 3D model [2].
Redwood [8] contains multi-view 2D images and 3D depth
scans but it only provides reconstructed 3D models for very
few instances. Recently, Google Objectron [1] dataset has
released a collection of short, object-centric video clips

∗ yzp@utexas.edu

Figure 1. Qualitative comparison among three object-centric
multi-view datasets. Our proposed HM3D-ABO dataset has both
realistic foreground objects and realistic background scenes.

with 3D object-level annotations but it does not provide
reconstructed 3D models.
In light of the challenge of
creating accurate annotations for real-world object scans,
in this technical report, we propose an approach to generate
a photo-realistic object-centric dataset based on existing
high-quality scene and object models.

2. HM3D-ABO Dataset

Dataset Assets.
In order to build our dataset, we uti-
lize two types of assets: scene and object. We use the
recently released Habitat-Matterport 3D dataset [29] as
our scene assets. Those scenes are high quality capturing
of real-world indoor scenes. We use Amazon-Berkeley
Object [10] dataset as our object assets, because it comes
with high-quality texture and accurate geometry. Since
their provided meshes are not all watertight and may con-
tain some internal structures, we follow the procedure of
OccNet [26] to render hundreds of depth maps covering the
object surface, then running TSDF-Fusion and marching
cube [22] to get the water-tight mesh.
Pipeline. The overall procedure of creating synthetic scans
goes as follows. We randomly choose a scene and an ob-
ject. Then we ﬁnd a placement location of such an object
in that scene by sampling the navigable position [25, 36]
in the scene. We avoid physical collision between the in-
serted object and the rest of the scene by ensuring there
is an empty region in the surrounding of the chosen loca-
tion. We also make sure that the object is placed on the
ﬂoor of the scene instead of hovering in the air by running
a few steps of physical simulation. We then sample a set
of camera pose around the inserted object and ensure that

 
 
 
 
 
 
Real Multi-view Realistic Object Realistic Scene Cam Pose 3D Model

Method
(cid:51)
Pix3D [35]
(cid:51)
Redwood [8]
(cid:51)
Objectron [1]
(cid:55)
ShapeNet Choy [9]
(cid:55)
ShapeNet DISN [9]
(cid:55)
Things3D [42]
HM3D-ABO (Ours) (cid:55)

(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

(cid:51)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

Table 1. Comparison of the existing object-centric datasets with our proposed HM3D-ABO dataset. We compare based on whether it
contains real or synthetic capture (Real), multi-view capture (Multi-view), realistic object (Realistic Object), realistic background scene
(Realistic Scene), camera pose annotation (Cam Pose), and accurate 3D model for the object (3D Model).

the camera does not collide with the scene. We then render
the image at each location using habitat-sim [25,36]’s built-
in renderer. After rendering, we do a post-ﬁltering to ﬁlter
out low-quality capture. We calculate the ratio of the area
of the bounding rectangle of the object mask to the size of
the image and ﬁlter out images that have a ratio less than
0.2. Finally, we use the physical-based rendering engine
Blender Cycles [11] to render the ﬁltered conﬁgurations at
640 × 480 resolution. Each image takes around 10 seconds
to render using one NVIDIA V100 GPU.
Statistics. HM3D-ABO has 1966 objects placed in 500 in-
door scenes. In total, there are 3196 scene-shape conﬁg-
urations.
In Fig. 1 we show qualitative comparisons be-
tween HM3D-ABO and existing object-centric multi-view
datasets. More examples of our dataset are shown in Fig. 3.

3. Benchmark

We benchmark two tasks with the HM3D-ABO dataset:
absolute camera pose estimation and few-view 3D object
reconstruction. Our experimental setup follows FvOR [47].

3.1. Camera Pose Estimation

Camera pose estimation is a core step of many multi-
view 3D reconstruction algorithm. Camera pose estimation
approaches can be categorized into two types, i.e.
rela-
tive pose approach and absolute pose approach. Relative
pose approach estimates the relative pose between an image
pair. Traditional approach relies on matching hand-crafted
key-points [4,17,23], which could not handle much appear-
ance variation and requires large overlap between image
pair. The advance of deep learning also leads to algorithms
that reduce the sensitivity to appearance change and low vi-
sual overlap [6, 18, 24, 31–34, 46, 48]. The second type is
more tight to object-centric scenario. The absolute camera
pose approach directly estimates the camera pose w.r.t. the
canonical object coordinate system [43, 47]. Compared to
relative pose estimation, these type of methods may be less
generalizable compared to relative pose approach due to the
assumption of canonical object coordinate system. But they
are usually simpler as they directly output the camera poses
for each image, without the need to perform pairwise rel-

Method

Pixel Error↓ Rot Error(◦)↓ Trans Error(cm)↓

DISN [43]
Cai et al. [6]
FvOR-Pose [47]

23.0/9.6
38.9/18.0
18.0/5.0

18.6/4.1
31.3/7.7
14.3/1.3

9.1/6.2
12.3/9.9
7.6/5.0

Table 2. Absolute camera pose estimation results on HM3D-ABO
dataset. We show the mean/median statistics for all metrics.

ative camera pose estimation followed by synchronization
step to get absolute camera pose.

Here we provide a benchmark of absolute camera pose
estimation. The input is a bag of RGB images capturing
an object, and the output is the absolute camera pose for
each of the images, w.r.t the canonical object coordinate.
We follow the experimental setup of FvOR [43, 47] and
use 4 images as input. We report the mean and median
pixel error, rotation error, and translation error for each
algorithm. The pixel error is computed w.r.t a 512x384
input image. We benchmark three methods as in [47]. The
ﬁrst one is based on DISN [43] which parametrizes camera
poses by orthogonal vectors. The second one is based
on Cai et.al [6] which use a classiﬁcation plus regression
approach. The last one we run is from FvOR [47]’s pose
initialization module(denoted as FvOR-Pose in the follow-
ing text), which uses a scene-coordinate as intermediate
pose representation followed by RANSAC PnP [5] for pose
extraction. The results can be found in Table 2.

3.2. Few-view 3D Object Reconstruction

Traditional research on 3D object reconstruction mainly
focuses on SfM (structure-from-motion) pipelines from
dense views, which are difﬁcult to acquire for many appli-
cations. Single-view 3D reconstruction [13, 15, 16, 20, 26,
38, 40] receive a lot attention in recent years, but their re-
construction quality is often found to be limited [37]. Few-
view 3D Object Reconstruction recovers the shape of the
underlying object given a few RGB observations. Such a
setting strikes a balance between capture complexity and re-
construction accuracy, and has been studied in several prior
works [3, 9, 42, 43, 47, 51].

Existing approaches can be categorized into three types.
The ﬁrst type does not use camera pose for each image,
such as 3D-R2N2 [9] and OccNet†, which is an multi-

Method

OccNet† [3, 26]
IDR [49]
FvOR w/ GT Pose [47]
FvOR w/o Joint [47]
FvOR [47]

GT

Noise@L1

Noise@L2

Noise@L3

Predict

IoU↑
0.762/0.812
0.645/0.677
0.861/0.891
0.859/0.889
0.853/0.885

Chamfer-L1↓
2.01/1.63
3.67/2.78
0.791/0.680
0.855/0.741
0.921/0.793

IoU↑
0.762/0.812
0.643/0.684
0.821/0.863
0.849/0.882
0.852/0.883

Chamfer-L1↓
2.01/1.63
3.75/2.90
1.22/1.05
0.944/0.825
0.941/0.806

IoU↑
0.762/0.812
0.642/0.679
0.761/0.815
0.819/0.864
0.849/0.880

Chamfer-L1↓
2.01/1.63
3.70/2.84
2.04/1.76
1.28/1.16
0.980/0.843

IoU↑
0.762/0.812
0.638/0.665
0.703/0.768
0.776/0.831
0.845/0.876

Chamfer-L1↓
2.01/1.63
3.78/3.00
2.90/2.47
1.90/1.65
1.05/0.897

IoU↑
0.762/0.812
0.612/0.650
0.816/0.870
0.834/0.877
0.839/0.878

Chamfer-L1↓
2.01/1.63
4.40/3.37
1.40/0.93
1.21/0.886
1.19/0.867

Table 3. Evaluating the robustness of few-view 3D reconstruction on HM3D-ABO dataset (mean/median, top-2 results highlighted).
We report the results using ground truth poses, perturbed poses with different perturbation levels, and predicted poses by FvOR-Pose.
Chamfer-L1 is multiplied by 100.

Few-view Input

IDR [49]

OccNet† [3, 26]

FvOR [47]

Figure 2. Qualitative comparisons between three approaches on HM3D-ABO. For each column, we show the 4 input views on the left and
the reconstruction of each method on the right. FvOR [47] and IDR [49] use pose predicted by FvOR-Pose [47].

view version implementation of OccNet [26] proposed by
3D43D [3]. The second type assume known camera pose,
such as 3D43D [3] and FvOR w/ GT Pose [47]. The other
type uses the estimated camera pose, such as FvOR [47]. In
particular, we experimented with the following baselines:
OccNet† [3, 26]. This is a multi-view version of Occ-
Net [26] by 3D43D [3]. It does not require camera pose.
IDR [49] is an optimization-based algorithm that does not
learn a prior from training data.
IDR achieves good per-
formance when reconstructing objects with tens of images
paired with ground truth object masks. We run IDR [49] on
each test input for 1000 epochs for best results.
FvOR w/ GT Pose is FvOR [47]’s standalone 3D recon-
struction module trained with ground truth camera poses.
FvOR w/o Joint is FvOR [47]’s approach without perform-
ing joint reﬁnement.
FvOR is FvOR [47]’s approach with joint reﬁnement.

During training, for each scene-object pair, we randomly

select 4 views to form an input set. For testing, we select
one bag of 4 views from each scene-object pair, yielding
500 testing examples in total. We compare the IoU metric
and the Chamfer-L1 metric [26, 47]. To evaluate the shape
quality invariant to similarity transform, we follow the pro-
cedure of FvOR [47] to ﬁnd the similarity transform that
aligns the predicted shape and G.T. shape before evaluation.
We also follow FvOR [47] and compare all methods under
three pose initialization settings. The ﬁrst setting assumes
we have access to the G.T. camera pose. The second setting
assumes we have G.T. pose corrupted by different magni-
tudes of Gaussian noise. The pose perturbation magnitude
is controlled by σ ∈ (cid:8)0.75e−2, 1.5e−2, 2.25e−2(cid:9) with
three values (called Noise@L1, Noise@L2, and Noise@L3
respectively). The last setting uses predicted camera pose
by FvOR [47]’s pose initialization module FvOR-Pose.
Note that these three pose initialization settings do not af-
fect OccNet† as it does not utilize the camera pose at all.

Figure 3. Examples of HM3D-ABO dataset. 3D assets from Amazon-Berkeley Object dataset [10] are placed in scenes from Habitat-
Matterport 3D dataset [29] in a physically plausible way. For each object-scene conﬁguration we render multiple images around the object.
We show here for each conﬁguration only 4 out of tens views.

The quantitative results can be found in Table 3. FvOR
performs the best under most of the settings. IDR does not
give satisfying results, which is probably due to the too-
few input views. Pose-free method OccNet† gives decent
results, and the fact that they do not need to estimate camera
pose at all makes their method much simpler.

4. Discussion

In this report, we introduced a pipeline to create a
photo-realistic object-centric dataset, HM3D-ABO. The
synthesized images have realistic object and backgrounds
compared to dataset based on ShapeNet
[9, 43] and
It also comes with high quality
synthetic scene [42].
3D models which are absent from several real-captured
object-centric datasets [1, 8, 30, 35]. Such a dataset can
be used for multiple purposes, for example, evaluating
algorithms for few-view 3D reconstruction, novel-view
synthesis, and camera pose estimation.

While we argue the HM3D-ABO dataset is a more
realistic dataset than the previous synthetic object-centric
datasets, we also note several limitations. First, the object
assets are limited in terms of category. In the current ver-
sion, most of the objects are chairs and tables, which are the
major categories in ABO [10] dataset. With the increased
object-level assets we could also expand our HM3D-ABO
dataset. Secondly, the current object placement scheme is
limited. We only place the single object on an empty region
of the walkable surface, which does not include some more
challenging scenarios that could also happen in practice,
such as reconstructing a teddy bear sitting on the sofa.

References

[1] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jian-
ing Wei, and Matthias Grundmann. Objectron: A large
scale dataset of object-centric videos in the wild with pose
In Proceedings of the IEEE/CVF Conference
annotations.
on Computer Vision and Pattern Recognition, pages 7822–
7831, 2021. 1, 2, 5

[2] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis
Savva, Angel X. Chang, and Matthias Niessner. Scan2cad:
Learning cad model alignment in rgb-d scans. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019. 1

[3] Miguel ´Angel Bautista, Walter Talbott, Shuangfei Zhai, Ni-
tish Srivastava, and Joshua M. Susskind. On the generaliza-
tion of learning-based 3D reconstruction. IEEE Winter Con-
ference on Applications of Computer Vision (WACV), 2021.
1, 2, 3

[4] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf:
Speeded up robust features. In European conference on com-
puter vision, pages 404–417. Springer, 2006. 2

[5] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of

Software Tools, 2000. 2

[6] Ruojin Cai, Bharath Hariharan, Noah Snavely, and Hadar
Averbuch-Elor. Extreme rotation estimation using dense cor-
In Proceedings of the IEEE Conference
relation volumes.

on Computer Vision and Pattern Recognition (CVPR), pages
14566–14575, 2021. 2

[7] Angel X. Chang, Thomas A. Funkhouser, Leonidas J.
Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio
Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong
Xiao, Li Yi, and Fisher Yu. Shapenet: An information-rich
3d model repository. CoRR, abs/1512.03012, 2015. 1
[8] Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen
Koltun. A large dataset of object scans. arXiv:1602.02481,
2016. 1, 2, 5

[9] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese. 3D-R2N2: A uniﬁed approach
for single and multi-view 3d object reconstruction. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), 2016. 1, 2, 5

[10] Jasmine Collins, Shubham Goel, Achleshwar Luthra, Leon
Xu, Kenan Deng, Xi Zhang, Tomas F Yago Vicente, Himan-
shu Arora, Thomas Dideriksen, Matthieu Guillaumin, and
Jitendra Malik. Abo: Dataset and benchmarks for real-world
3d object understanding. arXiv preprint arXiv:2110.06199,
2021. 1, 4, 5

[11] Blender Online Community. Blender - a 3D modelling and
rendering package. Blender Foundation, Stichting Blender
Foundation, Amsterdam, 2018. 2

[12] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-
man, Ryan Hickman, Krista Reymann, Thomas B McHugh,
and Vincent Vanhoucke. Google scanned objects: A high-
arXiv
quality dataset of 3d scanned household items.
preprint arXiv:2204.11918, 2022. 1

[13] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set
generation network for 3D object reconstruction from a sin-
gle image. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 605–
613, 2017. 2

[14] Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, and
Jiajun Wu. Objectfolder: A dataset of objects with implicit
visual, auditory, and tactile representations. arXiv preprint
arXiv:2109.07991, 2021. 1

[15] Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab-
hinav Gupta. Learning a predictable and generative vector
representation for objects. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 484–499.
Springer, 2016. 2

[16] Thibault Groueix, Matthew Fisher, Vladimir G Kim,
Bryan C Russell, and Mathieu Aubry. A papier-mˆach´e ap-
In Proceedings
proach to learning 3D surface generation.
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 216–224, 2018. 2

[17] Chris Harris, Mike Stephens, et al. A combined corner and
edge detector. In Alvey vision conference, volume 15, pages
10–5244. Citeseer, 1988. 2

[18] Linyi Jin, Shengyi Qian, Andrew Owens, and David F
Fouhey. Planar surface reconstruction from sparse views. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 12991–13000, 2021. 2

[19] Abhishek Kar, Christian H¨ane, and Jitendra Malik. Learn-
In Advances in Neural
ing a multi-view stereo machine.
Information Processing Systems (NeurIPS), pages 364–375,
2017. 1

[20] Abhishek Kar, Shubham Tulsiani, Joao Carreira, and Jiten-
dra Malik. Category-speciﬁc object reconstruction from a
In Proceedings of the IEEE Conference on
single image.
Computer Vision and Pattern Recognition (CVPR), pages
1966–1974, 2015. 2

[21] Abhijit Kundu, Yin Li, and James M Rehg.

3d-rcnn:
Instance-level 3d object
reconstruction via render-and-
compare. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 3559–
3568, 2018. 1

[22] William E Lorensen and Harvey E Cline. Marching cubes:
A high resolution 3d surface construction algorithm. ACM
siggraph computer graphics, 21(4):163–169, 1987. 1
[23] David G Lowe. Distinctive image features from scale-
International journal of computer vi-

invariant keypoints.
sion, 60(2):91–110, 2004. 2

[24] Wei-Chiu Ma, Anqi Joyce Yang, Shenlong Wang, Raquel
Urtasun, and Antonio Torralba. Virtual correspondence: Hu-
mans as a cue for extreme-view geometry. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 15924–15934, June 2022. 2

[25] Manolis

Savva*,

Abhishek Kadian*,

Oleksandr
Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi
Parikh, and Dhruv Batra. Habitat: A Platform for Embodied
AI Research. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), 2019. 1, 2

[26] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
In Proceed-
Learning 3d reconstruction in function space.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 4460–4470, 2019. 1, 2, 3
[27] Michael Oechsle, Songyou Peng, and Andreas Geiger.
Unisurf: Unifying neural implicit surfaces and radiance
ﬁelds for multi-view reconstruction. Proceedings of the
IEEE International Conference on Computer Vision (ICCV),
2021. 1

[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,
Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Implicit neural representations with structured latent codes
for novel view synthesis of dynamic humans. Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2021. 1

[29] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans,
Oleksandr Maksymets, Alex Clegg, John Turner, Eric Un-
dersander, Wojciech Galuba, Andrew Westbury, Angel X
Chang, et al. Habitat-Matterport 3D dataset (HM3D): 1000
large-scale 3D environments for embodied ai. arXiv preprint
arXiv:2109.08238, 2021. 1, 4

[30] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon objects in 3D: Large-scale learning and evaluation of
In Proceedings of the
real-life 3d category reconstruction.
IEEE/CVF International Conference on Computer Vision
(ICCV), pages 10901–10911, October 2021. 1, 5

[31] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
Superglue: Learning feature
and Andrew Rabinovich.
In Proceedings of
matching with graph neural networks.
the IEEE/CVF conference on computer vision and pattern
recognition, pages 4938–4947, 2020. 2

[32] Mohammad Amin Shabani, Weilian Song, Makoto
Odamaki, Hirochika Fujiki, and Yasutaka Furukawa. Ex-
treme structure from motion for indoor panoramas without
visual overlaps. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 5703–5711,
2021. 2

[33] Che Sun, Yunde Jia, Yi Guo, and Yuwei Wu. Global-aware
In Proceedings of

registration of less-overlap rgb-d scans.

the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6357–6366, 2022. 2

[34] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
Xiaowei Zhou. Loftr: Detector-free local feature matching
with transformers. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
8922–8931, 2021. 2

[35] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong
Zhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum,
and William T Freeman. Pix3d: Dataset and methods for
In IEEE Conference on
single-image 3d shape modeling.
Computer Vision and Pattern Recognition (CVPR), 2018. 1,
2, 5

[36] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans,
Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam,
Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan,
Vladimir Vondrus, Sameer Dharur, Franziska Meier, Woj-
ciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Ji-
tendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0:
In Ad-
Training home assistants to rearrange their habitat.
vances in Neural Information Processing Systems (NeurIPS),
2021. 1, 2

[37] Maxim Tatarchenko, Stephan R Richter, Ren´e Ranftl,
Zhuwen Li, Vladlen Koltun, and Thomas Brox. What do
single-view 3D reconstruction networks learn? In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 3405–3414, 2019. 2

[38] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei
Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3D mesh
models from single RGB images. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), pages 52–
67, 2018. 2

[39] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural im-
plicit surfaces by volume rendering for multi-view recon-
struction. Advances in Neural Information Processing Sys-
tems (NeurIPS), 2021. 1

[40] Jiajun Wu, Tianfan Xue, Joseph J Lim, Yuandong Tian,
Joshua B Tenenbaum, Antonio Torralba, and William T
In Pro-
Freeman. Single image 3D interpreter network.
ceedings of the European Conference on Computer Vision
(ECCV), pages 365–382. Springer, 2016. 2

[41] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao.
3d
shapenets: A deep representation for volumetric shapes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1912–1920, 2015. 1

[42] Haozhe Xie, Hongxun Yao, Shengping Zhang, Shangchen
Zhou, and Wenxiu Sun. Pix2vox++: multi-scale context-
aware 3d object reconstruction from single and multiple im-
International Journal of Computer Vision (IJCV),
ages.
128(12):2919–2935, 2020. 1, 2, 5

[43] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir
Mech, and Ulrich Neumann. DISN: Deep implicit surface
network for high-quality single-view 3d reconstruction. Ad-
vances in Neural Information Processing Systems (NeurIPS),
pages 492–502, 2019. 1, 2, 5

[44] Mingyue Yang, Yuxin Wen, Weikai Chen, Yongwei Chen,
and Kui Jia. Deep optimized priors for 3d shape modeling
and reconstruction. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
3269–3278, 2021. 1

[45] Shichao Yang and Sebastian Scherer. Cubeslam: Monocular
3D object slam. IEEE Transactions on Robotics, 35(4):925–
938, 2019. 1

[46] Zhenpei Yang, Jeffrey Z Pan, Linjie Luo, Xiaowei Zhou,
Kristen Grauman, and Qixing Huang. Extreme relative pose
estimation for rgb-d scans via scene completion. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 4531–4540, 2019. 2

[47] Zhenpei Yang, Zhile Ren, Miguel Angel Bautista, Zaiwei
Zhang, Qi Shan, and Qixing Huang. Fvor: Robust joint
shape and pose optimization for few-view object reconstruc-
tion, 2022. 2, 3

[48] Zhenpei Yang, Siming Yan, and Qixing Huang. Extreme
relative pose network under hybrid representations. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 2455–2464, 2020. 2
[49] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan
Atzmon, Basri Ronen, and Yaron Lipman. Multiview neu-
ral surface reconstruction by disentangling geometry and ap-
pearance. Advances in Neural Information Processing Sys-
tems (NeurIPS), 33, 2020. 3

[50] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance ﬁelds from one or few images.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 4578–4587, 2021. 1
[51] Jason Y Zhang, Gengshan Yang, Shubham Tulsiani, and
Deva Ramanan. Ners: Neural reﬂectance surfaces for sparse-
view 3d reconstruction in the wild. Advances in Neural In-
formation Processing Systems (NeurIPS), 2021. 2

