PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in
Context Disambiguation

Shuyi Xie, Jian Ma, Haiqin Yang§, Lianxin Jiang, Yang Mo, and Jianping Shen
Ping An Life Insurance, Ltd.
Shenzhen, Guangdong province, China
{XIESHUYI542, MAJIAN446, JIANGLIANXIN769, MOYANG853, SHENJIANPING324}@pingan.com.cn
§ the corresponding author, email: hqyang@ieee.org

Abstract

This paper presents the PALI team’s winning
system for SemEval-2021 Task 2: Multilin-
gual and Cross-lingual Word-in-Context Dis-
ambiguation. We ﬁne-tune XLM-RoBERTa
model to solve the task of word in context dis-
ambiguation, i.e., to determine whether the tar-
get word in the two contexts contains the same
In implementation, we ﬁrst
meaning or not.
speciﬁcally design an input tag to emphasize
the target word in the contexts. Second, we
construct a new vector on the ﬁne-tuned em-
beddings from XLM-RoBERTa and feed it to
a fully-connected network to output the prob-
ability of whether the target word in the con-
text has the same meaning or not. The new
vector is attained by concatenating the embed-
ding of the [CLS] token and the embeddings
of the target word in the contexts. In training,
we explore several tricks, such as the Ranger
optimizer, data augmentation, and adversar-
ial training, to improve the model prediction.
Consequently, we attain the ﬁrst place in all
four cross-lingual tasks.

1

Introduction

This year, the SemEval-2021 task 2, multilingual
and cross-lingual word-in-context (WiC) disam-
biguation (Martelli et al., 2021), deﬁnes the task of
identifying the polysemous nature of words with-
out relying on a ﬁxed sense inventory in a multi-
lingual and cross-lingual setting. The task aims to
perform a binary classiﬁcation task to determine
whether the target word contains the same meaning
or not in two given contexts under both the same
language (multilingual) setting and the different
languages (cross-lingual) setting. In the multilin-
gual setting, the tasks consist of English-English
(En-En), Arabic-Arabic (Ar-Ar), French-French
(Fr-Fr), Chinese-Chinese (Zh-Zh) and Russian-
Russian (Ru-Ru) while in the cross-lingual set-
ting, the tasks consist of English-Chinese (En-Zh),

English-French (En-Fr), English-Russian (En-Ru),
and English-Arabic (En-Ar).

The tasks contain the following challenges:
• The same word may deliver different mean-
ings in different context (Lei et al., 2021).
• The training data is scarce. For example, in
the multilingual tasks, there is only training
data in En-En, while in the cross-lingual tasks,
there is no training data.

To overcome these challenges, we explore the
uniqueness of the tasks and implement several key
technologies:

• First, we follow (Botha et al., 2020) to spe-
cially design an input tag for the multilingual
pre-training XLM-RoBERTa model to empha-
size the target word in the contexts. That is,
the target word is encompassed by the special
symbols of <t> and </t>. Meanwhile, the
given two contexts are concatenated by the
<SEP> token.

• Second, we apply data augmentation and add
external data from WordNet to enrich the train-
ing data. It is noted that we only expand the
data in the task of En-En and do not consider
other techniques, e.g., back-translation, for
the cross-lingual tasks. Adversarial training is
also applied to learn more robust embeddings
for target words. The Ranger optimizer with
the look-ahead mechanism in the AdamW op-
timizer is adopted to speed up the convergence
of training.

• Finally, we construct a new vector on the fune-
tuned embeddings, i.e., concatenating the em-
beding of the [CLS] token and the learned
embeddings of the target words’ in both con-
texts. The new vector is then fed into a fully-
connected network to produce the binary clas-
siﬁcation prediction. Cross-validation and
model ensemble are also applied to attain a
robust output.

1
2
0
2

n
u
J

7

]
I

A
.
s
c
[

2
v
5
7
3
0
1
.
4
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
The rest of this paper is organized as follows: In
Sec. 2, we brieﬂy introduce related work. In Sec. 3,
we detail our proposed system.
In Sec. 4, we
present the experimental setup, procedure, and the
results. Finally, we conclude our work in Sec. 5.

2 Related Work

The SemEval-2021 task 2 aims to handling the
tasks of multilingual and cross-lingual word-in-
context disambiguation (Martelli et al., 2021), i.e.,
to determine whether the target word contains the
same meaning in both given contexts. In the fol-
lowing, we will elaborate several related work.

Some recent effort, e.g., (Pilehvar and Camacho-
Collados, 2018), has been conducted to curate and
release datasets to solve the task of WiC disam-
biguation. Though it can be narrowed down to
binary classiﬁcation, some techniques have to be
implemented to enhance the model performance.
For example, the trick of input highlighting mech-
anism (Botha et al., 2020) can be facilitated to
promote the importance of the target word. The
idea of unifying entity linking and word sense dis-
ambiguation (Moro et al., 2014) can be borrowed
to solve the task. The idea of freezing the trained
model for other languages (Artetxe et al., 2020)
can be explored to relieve the issue of no training
data in the cross-lingual tasks.

Recently, due to the superior performance in
tackling NLP tasks (???), pre-trained language
models, such as BERT (Devlin et al., 2019) and
RoBERTa (Liu et al., 2019), start to dominate the
way of word representations than static word em-
bedding methods, e.g., Word2Vec (Mikolov et al.,
2013) and FastText (Joulin et al., 2017). Especially,
the XLM-RoBERTa (Conneau et al., 2020) model
is a newly released large cross-lingual language
model based on RoBERTa and is trained on 2.5TB
ﬁltered CommonCrawl data in 100 languages. Dif-
ferent from other XLM models, XLM-RoBERTa
does not require the language token to understand
which language is used and can determine the cor-
rect language from the input ids. It is a powerful
tool for understanding multilingual languages and
is very helpful for solving the WiC disambiguation
task under the cross-lingual setting. Hence, we
choose XLM-RoBERTa in our system.

2017), can be applied to enrich the data, we mainly
explore the usage of WordNet (Fellbaum, 1998)
and the technique of pseudo labelling (Wu and
Prasad, 2018) because WordNet contains rich syn-
onyms while pseudo labelling is effective to utilize
the abundant unlabeled data via their pseudo labels.
Adversarial training (Tram`er et al., 2018) is an
effective method to regularize parameters by in-
troducing noise and to improve model robustness
and generalization. We also explore its possibil-
ity in ﬁne-tuning XLM-RoBERTa to increase the
robustness of the learned the word embeddings.

3 Overview

In the following, we present the task deﬁnition, data
preprocessing, and our proposed system design.

3.1 Task Deﬁntion

The task of WiC disambiguation is framed by a
binary classiﬁcation task. Each instance in WiC
has a target word w, whose part-of-speech is in
{NOUN, VERB, ADJ, ADV}, with two given con-
texts, c1 and c2. Each of these contexts triggers a
speciﬁc meaning of w. The task is to identify if
the occurrences of w in c1 and c2 correspond to
the same meaning or not. Figure 1 illustrates an
example from the dataset.

{
"target word":
"sentence1":

"play",

"In that

context of coordination
and integration, Bolivia
holds a key play in any
process of infrastructure
development.",

"sentence2":

"In schools,

when water is needed, it
is girls who are sent to
fetch it, taking time away
from their studies and
play."

}

Figure 1: An example from the WiC disamguation task.

3.2 Data Preprocessing

A critical issue of the task is lack of training
data. Though existing methods, e.g., lexical substi-
tution (Zhang et al., 2015), back translation (Xie
et al., 2020), and data augmentation (Fadaee et al.,

The training dataset consists of two ﬁles in the
the .data ﬁle and the .gold ﬁle.
JSON format:
The .data ﬁle contains the following information:
unique id of the pair, target lemma, part-of-speech

Figure 2: Fine-tuned XLM-RoBERTa model architecture.

No. of target words
No. of pairs
Min. tokens
Avg. tokens (original)
Max. tokens (original)
Max. tokens (post-proc.)

Training Test
491
1,000
5
26
116
81

3,726
8,000
6
24
88
81

Table 1: Statistics of the data

in {NOUN, VERB, ADJ, ADV}, the ﬁrst sentence,
the second sentence, the start and the end indices
(zero-based numbering) of the target word in the
ﬁrst and the second context, respectively. The .gold
ﬁle contains unique id of the pair and the label,
which is represented by T or F.

For the training dataset, we clean up the text
by completing word abbreviation, removing spe-
cial punctuation, and segmenting the sentences into
subword lists by Byte-Pair Encoding (BPE) (Sen-
nrich et al., 2015). Since it is difﬁcult to capture the
meaning of the target word in the context for long
sentences (Pan et al., 2019; Zhu et al., 2021), we
limit the length of each sentence with maximum
40 words before and after the target word.

We include additional resource, WordNet, to aug-
ment our training data because WordNet is a large
lexical database of English. Nouns, verbs, adjec-
tives and adverbs are grouped into sets of cognitive
synonyms (synsets), each expressing a distinct con-

cepts. Here, we randomly select example sentences
of target word in WordNet to expand our training
corpus, which increases around 30% of training
data. By such preprocessing, we obtain the dataset
and report the statistics in Table 1.

3.3 Model Design

Figure 2 outlines our model architecture, which
consists of four modules, i.e., input design, model
learning, ﬁnal feature construction, and the classi-
ﬁer. The whole framework is based on ﬁne-tuning
the pre-trained XLM-RoBERTa model to conduct
binary classiﬁcation on two given contexts. Differ-
ent from the inputs for XLM-RoBERTa, the input
of our system contains of the following modiﬁca-
tions: ﬁrst, in order to highlight the target word in
the contexts, we borrow the setting in (Lei et al.,
2017; Botha et al., 2020) by adding special sym-
bols <t> and </t> to embrace the target word in
the contexts. Given the example presented in Fig. 1,
the target word of “play” is then embraced by the
additional symbols, <t> and </t> in the contexts.
Second, we concatenate the given two contexts by
<SEP>. Figure 2 illustrates the result in the input
module. Moreover, in the experiment, we exchange
the order of the contexts to get more training data.
After learning the tokens’ representations by
XLM-RoBERTa, we construct a new vector by con-
catenating the [CLS] token’s representation in the
last layer of XLM-RoBERTa and the representa-
tions of the target word in both sentences. As BPE

tokenization may separate a target word into sev-
eral subwords, we compute its representation by
averaging the corresponding representations. Next,
the newly constructed feature is fed into a fully-
connected network to compute the ﬁnal binary pre-
diction probability.

During training, we conduct the following tech-
niques to increase the model convergence and ro-
bustness:

• Optimizer. We adopt the Ranger (Yong et al.,
2020) optimizer to replace the AdamW be-
cause it is a more synergistic optimizer com-
bining rectiﬁed Adam and the look-ahead
mechanism with gradient centralization in one
optimizer.

• Adversarial training. We apply the fast gra-
dient method (Miyato et al., 2017) in the train-
ing to obtain more stable word representa-
tions.

• Cross validation. We also apply stratiﬁed K-
fold cross validation on the training set and the
development set. For each fold, we hold the
group as a local test set and set the remaining
groups as the training set. We then average
the model prediction on each fold as the ﬁnal
prediction to obtain more robust results.

• Pseudo labelling. Pseudo labelling (Wu and
Prasad, 2018) is an effective semi-supervised
learning method to utilize the abundant un-
labeled data via their pseudo labels. In this
work, we ﬁrst train our model on the train-
ing set. Next, we apply the trained model
to predict the En-En multi-lingual test set
and use the predicted labels as our pseudo
labels. Finally, both the training set and the
En-En pseudo labels are included to train a
ﬁnal model. Especially, we observe that by
this trick, this ﬁnal model can improve the
prediction performance on cross-lingual tasks
slightly.

It is noted that in the cross-lingual tasks, we do not
back-translate the subwords to English but apply
the same model trained from the En-En dataset
because it allows us to maintain the target word in
the corresponding languages seamlessly. This is
similar to the procedure in (Artetxe et al., 2020).

4 Experiments

4.1 Setup

Our code is written in Pytorch based on the Hug-
gingface Transformer library 1 for XLM-RoBERTa.
Other hyperparameters are set based on our hand-
on experience. For example, the seed for the ran-
dom generator is set to 3,999. The batch size is set
to 10 and the hidden feature size is 1,024. The max-
imum length limit of a context is 240 though it is
unreachable because we have conducted trimming
in the data preprocessing procedure. The dropout
rate is tested from {0.2, 0.3, 0.25, 0.28} and ﬁnally
ﬁxed to 0.28. K in the stratiﬁed cross validation
is set to 5. The two special tokens, <t> and </t>,
are included into the word dictionary for learning.
The training data consists of the ofﬁcial En-En
multilingual training corpus and the contexts from
WordNet. At the beginning, we choose XLM-
RoBERTaBase as the backbone of our system to ex-
plore the possibility of our implementation tricks.
After identifying the effectiveness of the designed
input in Sec. 3.2, we apply XLM-RoBERTaLarge
to tune the corresponding hyperparameters, such
as changing the learning rate, the batch size, the
dropout rate, and the early stop mechanism. Fur-
thermore, we observe that long contexts may ignore
the importance of the target word in the contexts.
Hence, we center on the target words to cut off the
contexts at both ends with a certain length. To fur-
ther strengthen the inﬂuence of the target word in
a context, we concatenate the embedding of the
[CLS] token with the embeddings of the target
word in the contexts as the ﬁnal input for the logit
fully-connected network. From our experiment,
this strategy can signiﬁcantly boost the model per-
formance while improving the convergence.

4.2 Results

Table 2 reports the results of different implemen-
tation strategies on the tasks. From the results, we
observe that

• By replacing XLM-RoBERTaBase with XLM-
RoBERTaLarge, we can gain at least 3% im-
provement on all tasks.

• By applying Ranger optimizer, we attain the
results in Large+RO, which gain an average in-
crease of 0.2% per task. We conjecture the im-
provement comes from the fact that the model
converges to a more optimal solution.

• In Large+RO+LRA, we vary the learning rate

In the following, we detail our experimental setup
and present the results with analysis.

1https://github.com/huggingface/

transformers

80.7
84.2
85.3
84.9
84.8
85.8
85.7

Avg En-En Fr-Fr Ru-Ru Zh-Zh Ar-Ar En-Ru En-Zh En-Fr En-Ar
Strategy
80.4
80.8 85.5
Base
83.9
85.1 88.2
Large
83.6
85.4 88.7
Large + RO
84.1
85.4 89.2
Large + RO + LRA
84.1
85.5 89.0
Large + RO + LRA + ES
86.2
Large + RO + CTWE
86.3 90.0
Large + RO + CTWE + HC 86.3 89.9
86.3
Large + RO + CTWE + HC
+ WordNet
Large + RO + CTWE + HC
+ WordNet + AT
Large + RO + CTWE + HC
+ WordNet + AT + PL

81.0
84.6
84.7
84.8
85.2
86.0
85.6

80.9
87.0
86.9
86.8
86.9
87.1
87.2

79.1
82.6
83.3
83.2
83.1
83.9
84.2

81.9
85.7
85.9
85.6
85.2
86.1
86.7

78.7
84.3
85.1
85.1
85.5
85.9
85.9

79.1
85.6
84.7
85.2
85.4
85.4
85.0

88.1 91.7

87.0 91.1

86.5 91.6

87.2

86.9

86.0

86.3

85.7

87.9

85.6

86.9

89.2

87.1

85.8

87.1

84.0

85.3

85.9

86.5

86.3

86.3

87.9

87.2

88.6

85.1

88.0

86.5

Table 2: Results of ﬁne-tuning XLM-RoBERTa under different strategies. The abbreviation is deﬁned as follows:
Base: XML-RoBERTaBase; Large: XML-RoBERTaLarge; RO: Ranger Optimizer; LRA: learning rate adjustment;
ES: early stop; CTWE: concatenating target words’ embeddings; HC: the best parameters for LRA and ES; AT:
adversarial training; PL: pseudo labels.

from 1.5e-5, 1.3e-5, 1.2e-5, to 1.21e-5 pro-
gressively and ﬁnally ﬁnd that when the learn-
ing rate is 1.2e-5, we attain the best perfor-
mance. We then search the optimal epoch for
the early stop by setting the maximum num-
ber of epoch to 10. In Large+RO+LRA+ES,
we observe the optimal epoch for early stop
(the patience value) is 3. These parameters are
then ﬁxed for HC. From the results, we notice
that tuning the learning rate and adopting the
early stop mechanism can improve the model
performance accordingly.

• By concatenating target the word embedding,
we obtain the results in Large+RO+CTWE,
and actually, our model can be trained with
fewer epochs and attain around 1.1% improve-
ment on average.

• By adding more training data from WordNet,
we get another 0.2% average improvement in
Large+RO+CTWE+HC+WordNet. We con-
jecture the improvement mainly comes from
the increase of the training data.

• By adding the Pseudo label data, we can
gain another 0.8% average improvement. The
score of EN-EN test dataset is generally
higher than other test dataset. We discover
that the ﬁrst 462 pieces of English test dataset
have the same target word as test dataset in
other tasks. Therefore, adding EN-EN pseudo
label helps predict other tasks.

In sum, we conclude that by applying XLM-

RoBERTaLarge on the Ranger optimizer, the target
word embedding concatenation mechanism, more
external training data, and pseudo labels, we can
improve the model performance accordingly.

Finally, our system attains the champion on the
En-Ar, En-Fr, En-Ru, and En-Zh cross-lingual
tasks. In multilingual tasks, we also sit at eighth
place, seventh place, sixth place, seventh place,
and ﬁfth place for the En-En, Ar-Ar, Fr-Fr, Ru-Ru,
Zh-Zh tasks, respectively.

5 Conclusion

In this paper, we present our system to tackle the
word-in-context disambiguation task. We ﬁne-tune
the XLM-RoBERTa model to solve both multilin-
gual and cross-lingual word-in-context disambigua-
tion tasks. We speciﬁcally design the input format
to emphasize the target word in two contexts and
promote the importance of the target word by con-
catenating the embeddings in the corresponding
context with the [CLS] token to output the classiﬁ-
cation probability. We apply several training tricks
to improve the robustness of model and attain im-
provement during this procedure. The competition
results demonstrate the effectiveness of our imple-
mentation. In the future, we plan to explore more
model architecture to boost the performance for
multilingual tasks.

References

Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
2020. On the cross-lingual transferability of mono-
lingual representations. In Proceedings of the 58th
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2020, Online, July 5-10,
2020, pages 4623–4637. Association for Computa-
tional Linguistics.

Jan A. Botha, Zifei Shan, and Daniel Gillick. 2020.
Entity linking in 100 languages. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, Online,
November 16-20, 2020, pages 7833–7845. Associ-
ation for Computational Linguistics.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzm´an, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale.
In
Proceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2020,
Online, July 5-10, 2020, pages 8440–8451. Associa-
tion for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers), pages 4171–4186. Association for Computa-
tional Linguistics.

Marzieh Fadaee, Arianna Bisazza, and Christof Monz.
2017. Data augmentation for low-resource neural
machine translation. CoRR, abs/1705.00440.

Christiane Fellbaum. 1998. Wordnet: An electronic

lexical database.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tom´as Mikolov. 2017. Bag of tricks for efﬁcient
text classiﬁcation. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL 2017, Valen-
cia, Spain, April 3-7, 2017, Volume 2: Short Papers,
pages 427–431. Association for Computational Lin-
guistics.

Wenqiang Lei, Yisong Miao, Runpeng Xie, Bonnie
Webber, Meichun Liu, Tat-Seng Chua, and Nancy F
Chen. 2021. Have we solved the hard problem? it’s
not easy! contextual lexical contrast as a means to
probe neural coherence. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence.

Wenqiang Lei, Xuancong Wang, Meichun Liu, Ilija
Ilievski, Xiangnan He, and Min-Yen Kan. 2017.
Swim: a simple word interaction model for implicit
discourse relation recognition. In Proceedings of the

26th International Joint Conference on Artiﬁcial In-
telligence, pages 4026–4032.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR, abs/1907.11692.

Federico Martelli, Najla Kalach, Gabriele Tola, and
Roberto Navigli. 2021. SemEval-2021 Task 2: Mul-
tilingual and Cross-lingual Word-in-Context Disam-
In Proceedings of the Fif-
biguation (MCL-WiC).
teenth Workshop on Semantic Evaluation (SemEval-
2021).

Tom´as Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word represen-
In 1st International Con-
tations in vector space.
ference on Learning Representations, ICLR 2013,
Scottsdale, Arizona, USA, May 2-4, 2013, Workshop
Track Proceedings.

Takeru Miyato, Andrew M. Dai, and Ian J. Goodfel-
low. 2017. Adversarial training methods for semi-
In ICLR. OpenRe-
supervised text classiﬁcation.
view.net.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity linking meets word sense disam-
biguation: a uniﬁed approach. Trans. Assoc. Com-
put. Linguistics, 2:231–244.

Liangming Pan, Wenqiang Lei, Tat-Seng Chua,
Recent advances
arXiv preprint

and Min-Yen Kan. 2019.
in neural question generation.
arXiv:1905.08949.

Mohammad Taher Pilehvar and Jos´e Camacho-
Collados. 2018. Wic: 10,000 example pairs for
evaluating context-sensitive representations. CoRR,
abs/1808.09121.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. CoRR, abs/1508.07909.

Florian Tram`er, Alexey Kurakin, Nicolas Papernot,
Ian J. Goodfellow, Dan Boneh, and Patrick D. Mc-
Daniel. 2018. Ensemble adversarial training: At-
tacks and defenses. In 6th International Conference
on Learning Representations, ICLR 2018, Vancou-
ver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings. OpenReview.net.

Hao Wu and Saurabh Prasad. 2018. Semi-supervised
deep learning using pseudo labels for hyperspectral
IEEE Trans. Image Process.,
image classiﬁcation.
27(3):1259–1270.

Qizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Luong,
and Quoc Le. 2020. Unsupervised data augmenta-
tion for consistency training. In Advances in Neural
Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual.

Hongwei Yong, Jianqiang Huang, Xiansheng Hua, and
Lei Zhang. 2020. Gradient centralization: A new
optimization technique for deep neural networks. In
ECCV, volume 12346 of Lecture Notes in Computer
Science, pages 635–652. Springer.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. In Advances in Neural Information Pro-
cessing Systems 28: Annual Conference on Neural
Information Processing Systems 2015, December 7-
12, 2015, Montreal, Quebec, Canada, pages 649–
657.

Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming
Zheng, Soujanya Poria, and Tat-Seng Chua. 2021.
Retrieving and reading: A comprehensive survey on
open-domain question answering. arXiv preprint
arXiv:2101.00774.

