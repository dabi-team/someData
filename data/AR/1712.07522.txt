Cointegration in functional autoregressive processes

MASSIMO FRANCHI AND PAOLO PARUOLO

OCTOBER 30, 2018

8
1
0
2

t
c
O
9
2

]

M
E
.
n
o
c
e
[

2
v
2
2
5
7
0
.
2
1
7
1
:
v
i
X
r
a

Abstract. This paper deﬁnes the class of H-valued autoregressive (AR) processes with a unit root
of ﬁnite type, where H is an inﬁnite dimensional separable Hilbert space, and derives a generalization
of the Granger-Johansen Representation Theorem valid for any integration order d = 1, 2, . . . . An
existence theorem shows that the solution of an AR with a unit root of ﬁnite type is necessarily
integrated of some ﬁnite integer d and displays a common trends representation with a ﬁnite number
of common stochastic trends of the type of (cumulated) bilateral random walks and an inﬁnite
dimensional cointegrating space. A characterization theorem clariﬁes the connections between the
structure of the AR operators and (i) the order of integration, (ii) the structure of the attractor
space and the cointegrating space, (iii) the expression of the cointegrating relations, and (iv) the
Triangular representation of the process. Except for the fact that the number of cointegrating
relations that are integrated of order 0 is inﬁnite, the representation of H-valued ARs with a unit
root of ﬁnite type coincides with that of usual ﬁnite dimensional VARs, which corresponds to the
special case H = Rp.

1. Introduction

The theory of time series that take values in inﬁnite dimensional separable Hilbert spaces, or

H-valued processes, is receiving increasing attention in econometrics. H-valued processes allow to

represent directly the dynamics of inﬁnite-dimensional objects, such as Lebesgue square-integrable

functions on a compact domain. In this way, they allow greater modeling generality with respect

to models for conditional means and variances, see e.g. Horv´ath and Kokoszka (2012).

One notable special case is given by H-valued processes h = ψ(f ), where f a generic probability

density function (pdf) and ψ is an invertible transformation; the transformation is needed because

Key words and phrases. Functional autoregressive process, Unit roots, Cointegration, Common Trends, Granger-

Johansen Representation Theorem, Triangular representation.

M. Franchi, Sapienza University of Rome, P.le A. Moro 5, 00185 Rome, Italy; e-mail: massimo.franchi@uniroma1.it.

P. Paruolo, European Commission, Joint Research Centre (JRC), Via E.Fermi 2749, I-3027 Ispra (VA), Italy;
e-mail: paolo.paruolo@ec.europa.eu, corresponding author.

The paper beneﬁted from useful comments from the Editor, Peter C.B. Phillips, three anonymous referees
and participants at 2018 NBER-NSF Time Series Conference, University of California San Diego. The ﬁrst author
acknowledges partial ﬁnancial support from MIUR PRIN grant 2010J3LZEN.
The idea of the present paper was conceived while the ﬁrst author was visiting the Department of Economics, Indiana
University, in January 2017; the hospitality of Yoosoon Chang and Joon Park is gratefully acknowledged. During
the revision of the paper in September 2018, the ﬁrst author visited the Department of Economics, University of
California San Diego, and the hospitality of Brendan K. Beare is gratefully acknowledged.

1

 
 
 
 
 
 
2

the space of pdfs is convex but not linear, see Petersen and M¨uller (2016). Modeling dynamics of

a whole pdf appears of interest e.g. for the income distribution, see e.g. Bourguignon et al. (2005),

Piketty (2014) and Chang et al. (2016b).

An important early contribution to the theory of functional time series is Bosq (2000), where

a theoretical treatment of linear processes in Banach and Hilbert spaces is developed. There,

emphasis is given to the derivations of laws of large numbers and central limit theorems that allow

to discuss estimation and inference for H-valued stationary autoregressive (AR) models.

Economic applications of functional time series analysis include studies on the term struc-

ture of interest rates, see Kargin and Onatski (2008), and intraday volatility, see H¨ormann et al.

(2013) and Gabrys et al. (2013); additional applications can be found in the recent monographs

Horv´ath and Kokoszka (2012) and Kokoszka and Reimherr (2017) and in the review article H¨ormann and Kokoszka

(2012).

Recently Chang et al. (2016b) applied Functional Principal Components Analysis (FPCA) di-

rectly on the space of densities for individual earnings and intra-month distributions of stock re-
turns.1 They found evidence of unit root persistence in a handful of coordinates of these cross-

sectional distributions. The framework proposed by Chang et al. (2016b) has (by construction) a

ﬁnite number of I(1) stochastic trends and an inﬁnite dimensional cointegrating space. The theory

is developed starting from the inﬁnite moving average representation of the ﬁrst diﬀerences of the

process and the potential unit roots are identiﬁed and tested through FPCA.

Representation of H-valued AR processes with unit roots has been recently considered in the

literature. Hu and Park (2016) consider H-valued AR(1) processes with compact operator and

prove that an extension of the Granger-Johansen Representation Theorem, see Theorem 4.2 in

Johansen (1996), holds in the I(1) case. The corresponding common trends representation, or

functional Beveridge-Nelson decomposition, displays a ﬁnite number of I(1) stochastic trends and

an inﬁnite dimensional cointegrating space. They further propose an estimator for the functional

autoregressive operator which builds on the results in Chang et al. (2016b).

Beare et al. (2017) consider H-valued AR(k), k ≥ 1, with compact operators if k > 1 and no

compactness assumption if k = 1, and show that the Granger-Johansen Representation Theorem

holds in the I(1) case. If k > 1, the number of I(1) stochastic trends is ﬁnite and the dimension

of the cointegrating space is inﬁnite, while if k = 1 this is not necessarily the case. In order to

obtain the common trends representation of H-valued AR(k), k ≥ 1, with compact operators.

Beare and Seo (2018b) are the ﬁrst to employ a theorem on the inversion of analytic operator
functions in Gohberg et al. (1990).2 They also present results on the I(2) case that show that the

number of I(2) stochastic trends is ﬁnite and the dimension of the cointegrating space is inﬁnite.

1Beare (2017) pointed out the issue that the space of density is not linear, see also Beare and Seo (2018a).
2The same theorem is used here to discuss the existence of a common trends representation in Section 3.

3

Finally, Chang et al. (2016a) consider an error correction form with compact error correction

operator and show that in this case the number of I(1) stochastic trends is inﬁnite and the dimension

of the cointegrating space is ﬁnite. Moreover, they show that Granger-Johansen Representation

Theorem continues to hold.

This paper considers a more general class of AR processes, called the class of ARs with a unit

root of ﬁnite type. This class contains H-valued ARs with compact operators as a special case.

This paper derives a generalization of the Granger-Johansen Representation Theorem for this class,

valid for any integration order d = 1, 2, . . . .

An existence theorem is provided; this shows that the solution of an AR with a unit root of ﬁnite

type is necessarily I(d) for some ﬁnite integer d and displays a common trends representation with

a ﬁnite number of common stochastic trends of the type of (cumulated) bilateral random walks

and an inﬁnite dimensional cointegrating space. This result is a direct consequence of a well known

theorem in operator theory, and ﬁrst employed in Beare and Seo (2018b) in the context of H-valued

ARs with compact operators.

Despite these interesting implications, this existence result does not address a number of impor-

tant issues, such as the connections between the structure of the AR operators and (i) the order of

integration of the process, (ii) the structure of the attractor space and the cointegrating space and

(iii) the expression of the cointegrating relations. The characterization of these links in the generic

I(d) case constitutes the main contribution of the present paper. More speciﬁcally, a necessary

and suﬃcient condition for the order of integration d is given in terms of the decomposition of the

space H into the direct sum of d + 1 orthogonal subspaces τh, h = 0, . . . , d, that are expressed
recursively in terms of the AR operators. This condition is called the ‘pole(d) condition’, because

it is a necessary and suﬃcient condition for the inverse of the A(z) function to have a pole of order

d at z = 1.

A crucial feature of the present pole(d) conditions is that the subspaces in the orthogonal direct

sum decomposition H = τ0 ⊕ τ1 ⊕ · · · ⊕ τd, τd 6= {0}, identify the directions in which the properties

of the process diﬀer. Speciﬁcally, for any nonzero v ∈ τ0, which is inﬁnite dimensional, one can
combine hv, xti with diﬀerences ∆nxt for n = 1, . . . , d − 1 to ﬁnd I(0) polynomial cointegrating
relations. For any nonzero v ∈ τ1, with dimension 0 ≤ dim τ1 < ∞, one can combine hv, xti with
diﬀerences ∆nxt for n = 1, . . . , d − 2 and ﬁnd I(1) polynomial cointegrating relations.

This kind of feature is valid for τ2, . . . , τd−2; for τd−1, with 0 ≤ dim τd−1 < ∞, one has hv, xti ∼

I(d − 1) for any nonzero v in H, without polynomial cointegration. Finally for nonzero v in τd,

with with 0 < dim τd < ∞ one has hv, xti ∼ I(d) for any nonzero v, i.e. all v-characteristics

have no cointegration. These results parallel the ones in the Triangular Representation in the
ﬁnite dimensional case H = Rp discussed in Phillips (1991) and Stock and Watson (1993); see also

Franchi and Paruolo (2018).

4

These results show that conditions and properties of ARs with a unit root of ﬁnite type extend
those that apply in the usual ﬁnite dimensional VAR case; in particular for H = Rp one ﬁnds the

I(1) and I(2) results in Johansen (1996), and for the generic I(d) case, one ﬁnds the results in

Franchi and Paruolo (2018). Except for the fact that the number of I(0) cointegrating relations is

inﬁnite, the inﬁnite dimensionality of H does not introduce additional elements in the representation

analysis of ARs with a unit root of ﬁnite type.

The present results are based on orthogonal decomposition of the embedding Hilbert space.

Orthogonal and non-orthogonal projections are well known concepts in econometrics. Students

are usually introduced to these concepts when learning OLS and GLS, where the choice between

the two is usually discussed in terms of estimation eﬃciency; see ? for how these arguments are

modiﬁed for spectral GLS regressions methods in a cointegration context. In the context of the

representation theory considered here, results can be obtained using either orthogonal or non-

orthogonal projections. The present choice of orthogonal projections is found to ease exposition

and to simplify the characterization of the cointegrating v-characteristics of the process.

The rest of the paper is organized as follows: Section 2 presents basic deﬁnitions and concepts,

Section 3 discussed the assumption of unit root of ﬁnite type and reports initial existence results for

a pole of ﬁnite order; Section 4 provides a characterization of I(1) and I(2) ARs with a unit root

of ﬁnite type and Section 5 extends the analysis to the general I(d), d = 1, 2, . . . , case. Section 6

concludes.

Three Appendices collect background deﬁnitions, novel inversion results and proofs of the state-

ments in the paper. Speciﬁcally, Appendix A reviews notions on operators acting on a separable

Hilbert space H and on H-valued random variables; Appendix B presents novel results on the

inversion of a meromorphic operator function and Appendix C reports proofs of the results in the

paper.

2. H-valued linear process, order of integration and cointegration

This section introduces the notions of weakly stationary, white noise, linear, integrated, and

cointegrated processes that take values in a separable Hilbert space H, where separable means

that H admits a countable orthonormal basis. Basic deﬁnitions of operators acting on H and of

H-valued random variables are reported in Appendix A.

2.1. Deﬁnitions. The deﬁnitions of weakly stationary and white noise process are taken from

Bosq (2000, Deﬁnitions 2.4, 3.1, 7.1), while those of linear, integrated and cointegrated process

are adapted from Johansen (1996); they are similar to those employed in Chang et al. (2016b),

Beare et al. (2017), Beare and Seo (2018b). The deﬁnition of expectation E(·), covariance operator

and cross-covariance function used in the following are reported in Appendix A.2.

Deﬁnition 2.1 (Weakly stationary process). An H-valued stochastic process {εt, t ∈ Z} is said
to be weakly stationary if (i) 0 < E(kεtk2) < ∞, (ii) E(εt) and the covariance operator of εt

5

do not depend on t and (iii) the cross-covariance function of εt and εs, cεt,εs(h, v), is such that
cεt,εs(h, v) = cεt+u,εs+u(h, v) for all h, v ∈ H and all s, t, u ∈ Z.

The notion of H-valued white noise is introduced next.

Deﬁnition 2.2 (White noise process). An H-valued weakly stationary stochastic process {εt, t ∈ Z}
is said to be white noise if (i) E(εt) = 0 and (ii) cεt,εs(h, v) = 0 for all h, v ∈ H and all s 6= t, s, t ∈ Z,
where cεt,εs(h, v) is the cross-covariance function of εt and εs; it is called strong white noise if (i)
holds, and (ii) is replaced by the requirement that εt is an i.i.d. sequence of H-valued random
variables.

Note that by deﬁnition any strong white noise is white noise, and any white noise process is

weakly stationary. The same property holds for linear combinations of lags of a white noise process

with suitable weights; this leads to the class of linear processes, introduced in Deﬁnition 2.3 below.

In the deﬁnition below, the following notation is employed: D(z0, ρ) denotes the open disc {z ∈
C : |z −z0| < ρ} with center z0 ∈ C and radius 0 < ρ ∈ R and LH indicates the set of bounded linear
∞
n=0 Bn(z−z0)n,
operators on H with norm kAkLH = supkvk=1 kAvk; an operator function B(z) =
∞
n=0 kBnkLH|z − z0|n < ∞ for
where Bn ∈ LH, is said to be absolutely convergent on D(z0, ρ) if
all z ∈ D(z0, ρ).3 The lag operator is denoted by L and ∆ = 1 − L is the diﬀerence operator.

P

P

Deﬁnition 2.3 (Linear process). Let {εt, t ∈ Z} be white noise; an H-valued stochastic process
{ut, t ∈ Z} with expectation {µt, t ∈ Z}, µt = E(ut), is said to be a linear process if

∞

ut − µt =

Bnεt−n,

Bn ∈ LH,

B0 = I,

where B(z) =

∞
n=0 Bnzn, z ∈ C, is absolutely convergent on the open disc D(0, ρ) for some ρ > 1.

n=0
X

P

∞
n=0 kBnk2

As discussed in Section 7.1 in Bosq (2000), existence and weak stationarity of ut − µt =
∞
LH < ∞. Observe
n=0 Bnεt−n are guaranteed by the square summability condition
that the requirement that B(z) is absolutely convergent on D(0, ρ) for some ρ > 1 is stronger. In
P
P
∞
n=0 kBnkLH < ∞ and hence
fact,
∞
n=0 kBnk2
LH < ∞. This shows that ut − µt in Deﬁnition 2.3 is well deﬁned and weakly stationary.
P
Moreover, B(z) ∈ LH for all z ∈ D(0, ρ), ρ > 1, implies that B(1) is a bounded linear operator.

∞
n=0 kBnkLH|z|n < ∞ for all z ∈ D(0, ρ), ρ > 1, implies

P
Finally note that B(z) is inﬁnitely diﬀerentiable on D(0, ρ), ρ > 1, and the series obtained by
∞
n=k n(n − 1) · · · (n − k + 1)Bnzn−k, is absolutely convergent
∞
n=k n(n − 1) · · · (n −
∞
n=1 nkBnkLH < ∞; this condition is employed in

and coincides with the k-th derivative of B(z) for each z ∈ D(0, ρ). Hence

k + 1)kBnkLH < ∞, which for k = 1 reads

termwise k times diﬀerentiation,

P

P

P

Chang et al. (2016b).

P

The notions of integration and integral operator are introduced next.

3Note that P

∞

n=0 kBnkLH |z − z0|n < ∞ for all z ∈ D(z0, ρ) implies that P

∞

n=0 Bn(z − z0)n converges in the

operator norm to B(z) ∈ LH for all z ∈ D(z0, ρ), i.e. k PN

n=0 Bn(z − z0)n − B(z)kLH → 0 as N → ∞.

6

Deﬁnition 2.4 (Order of integration). A linear process ut − µt = B(L)εt is said to be integrated
of order 0, written ut ∼ I(0), if B(1) 6= 0. If ∆dzt is I(0) for some ﬁnite integer d = 1, 2, . . . ,
{zt, t ∈ Z} is said to be integrated of order d, indicated zt ∼ I(d).

This deﬁnition coincides with Deﬁnition 3.3 in Johansen (1996) of an I(d) process for the special

case H = Rp.

Observe that a white noise process is I(0) and that an I(0) process is weakly stationary.

In

order to see that a weakly stationary is not necessarily I(0), take for instance ut = εt − εt−1; this

process is weakly stationary, with B(1) = 0 and hence it does not satisfy the deﬁnition of an I(0)

process, showing that the two concepts do not coincide. The distinction between weak stationarity

and I(0)-ness is relevant for the deﬁnition of order of integration: in fact, the cumulation of an I(0)

process is necessarily I(1) while the cumulation of stationary process is not necessarily so.

Following Hu and Park (2016), one can deﬁne the v-characteristic of xt as the scalar process

hv, xti, for any v ∈ H. From Deﬁnition 2.4, one can see that a generic v-characteristic of xt ∼ I(d)

is itself at most integrated of order d; the case when a v-characteristic of xt ∼ I(d) is integrated of

lower order b < d is associated with the notion of cointegration.

Deﬁnition 2.5 (Cointegrated process). An I(d) process zt is said to be cointegrated if there exists
a nonzero v-characteristic v ∈ H such that hv, zti is I(b) for some b < d. The set {v ∈ H :
hv, zti ∼ I(b), b < d} ∪ {0} is called the cointegrating space and its orthogonal complement is called
the attractor space.

As in the usual ﬁnite dimensional case, zt is cointegrated if there exists a nonzero linear com-

bination v of zt (i.e. a v-characteristic of zt) that has lower order of integration than the original

process. Observe that the attractor space (respectively the cointegrating space) contains 0 ∈ H

and all nonzero v ∈ H that correspond to a v-characteristic of zt with the same (respectively lower)

order of integration of the original process zt. The null vector 0 ∈ H is added so as to make the

cointegrating space a vector spaces.

The cases that have been studied in the literature correspond to ﬁnite dimensions either for the

attractor or for the cointegrating space. When both of them have ﬁnite dimension, H is ﬁnite

dimensional, so that the standard results in the literature apply. The case in which the attractor

space is inﬁnite dimensional and the cointegrating space is ﬁnite dimensional corresponds to a

process with an inﬁnite number of I(d) stochastic trends and a ﬁnite dimensional cointegrating

space. For d = 1, this case has been discussed in Chang et al. (2016a) and in Beare et al. (2017)

for k = 1, see Proposition 4.4 below.

Most of the contributions in the literature have studied instead the case of an attractor space of

ﬁnite dimension and a cointegrating space of inﬁnite dimension, i.e. the case where the process has

a ﬁnite number of I(d) v-characteristics and an inﬁnite number of I(b) v-characteristics with b < d.

This is the setup studied in Chang et al. (2016b), Hu and Park (2016) and Beare et al. (2017) for

d = 1 and in Beare and Seo (2018b) for d = 1 and d = 2. This is the setting considered in the

present paper as well, and it is motivated also by the next example.

2.2. Yield curve example. As an example of a Hilbert space of economic interest, consider the

yield curve x◦,t(s), where s denotes maturity and t time. In this section t is omitted, unless needed

for clarity.

7

Let H be the set of Lebesgue measurable functions x◦(s) such that

x2
◦(s)ds < ∞, where
smax is the maximal maturity. One can rescale the maturity s into u = s/smax and deﬁne the
R
1
0 x2(u)du < ∞. The vector
space operations on H are deﬁned in a natural way as (x+y)(u) = x(u)+y(u) and (αx) (u) = αx(u)
R
where α ∈ R. Next deﬁne the inner product

rescaled yield curve x(u) by x(u) = x◦(u · smax), with u ∈ (0, 1] and

smax
0

1

hx, yi =

x(u)y(u)du.

0
Z

(2.1)

This space of Lebesgue square-integrable functions equipped with the inner product (2.1) is a

complete, separable Hilbert space, see e.g. Kokoszka and Reimherr (2017, p. 214).

The yield curve is often described in terms of the three features of level, slope and curvature,

see e.g. Cochrane and Piazzesi (2005). These features of the yield curve can be associated with

the following v-characteristics of x. Deﬁne πj,1, . . . , πj,j as a partition of the unit interval (0, 1] into
j segments πj,i of length 1/j, πj,i = ( i−1
j ], and let 1{u∈πj,i} be the indicator function that takes
value one when u ∈ πj,i and equals 0 otherwise.

j , i

Next deﬁne the following v functions

v0 = 1{u∈π1,1},

v1 =

1
2

1{u∈π2,2} − 1{u∈π2,1}

,

v2 =

1
4

(cid:0)
1{u∈π4,4} − 1{u∈π4,3}

−

1
4

1{u∈π4,2} − 1{u∈π4,1}

(cid:1)

,

(cid:0)
and observe that they belong to H, because they are Lebesgue square-integrable functions. Finally

(cid:0)

(cid:1)

(cid:1)

let x denote the rescaled yield curve and note that

v0(u)x(u)du =

x(u)du,

1

v1(u)x(u)du =

hv0, xi =

hv1, xi =

1

1

1

0

Z

0

Z

hv2, xi =

v2(u)x(u)du =

0

Z

0
Z
1
2  Z
1
4  Z

1

1
2

1

3
4

x(u)du −

x(u)du −

1
2

3
4

0
Z

1
2

Z

x(u)du

,

!

x(u)du

−

!

1
2

1
4  Z

1
4

x(u)du −

1
4

0

Z

x(u)du

.

!

One can see that hv0, xi computes the average yield curve, and hence can be associated with the

level of the yield curve. Similarly hv1, xi computes the diﬀerence between the average yield on the

longer maturities and the one on the shorter maturities; hence it can be associated with the slope

of the yield curve. Finally hv2, xi computes the diﬀerence of the slopes on the longer maturities

and the shorter maturities; hence it can be associated with the curvature of the yield curve.

8

This shows that v0, v1, v2 deﬁne interesting v-characteristics for the yield curve x. If the yield

curve x is modeled as a functional time series, xt, then it is interesting to ask questions of the type:

“what is the order of integration of the level (or slope, or curvature) of the yield curve?”. These

questions translate into “what is the order of integration of the vj-characteristics, j = 0, 1, 2, of the

yield curve xt?”.

This illustrates how interesting hypotheses can be formulated in this context; clearly other types

of hypotheses can be formulated in a similar way. Moreover, it is of interest to determine how

many and which characteristics are nonstationary, which corresponds to estimating the (dimension

of the) attractor space.

It appears natural in this context to assume (or test) that there are only a ﬁnite number of factors

driving the dynamics of the yield curve. This translated into the hypothesis that there are only

a ﬁnite number of nonstationary v-characteristics; in this case, xt would have a ﬁnite dimensional

attractor space and an inﬁnite dimensional cointegrating space. This seems to be a reasonable

assumption to be tested empirically also beyond the case of the yield curve; this case is the one

studied in the present paper, see Corollary 3.6 below.

3. ARs with a unit root of finite type

This section introduces the class of H-valued ARs that is studied in the present paper, called

ARs with a unit root of ﬁnite type. It also presents an existence result about their common trends

representation, which shows that the solution of an AR with a unit root of ﬁnite type is necessarily

I(d) for some ﬁnite integer d and displays a common trends representation with a ﬁnite number
of common stochastic trends of the type of (cumulated) bilateral random walks.4 The relations of

ARs with a unit root of ﬁnite type with the ARs studied in literature are also discussed in this

section, and an example of an AR with a unit root of ﬁnite type with a non-compact AR operator

is given.

3.1. Main assumption. Consider an H-valued AR process

xt = A◦

1xt−1 + · · · + A◦

kxt−k + εt,

A◦

n ∈ LH,

t ∈ Z,

(3.1)

where I indicates the identity operator in LH, {εt, t ∈ Z} is white noise and the operator function

A(z) = I −

k

Xh=1

A◦

hzh,

z ∈ C,

A(1) 6= 0,

is non-invertible at z = 1 and invertible in the punctured disc D(0, ρ) \ {1} for some ρ > 1.

This requirement restricts attention to unit roots at frequency zero, corresponding to the point

z = 1 on the unit disc. Note that there is no loss of generality in assuming that A(1) 6= 0. In fact,

4This result is a direct consequence of a well known theorem in operator theory, reported in Theorem A.1 in
Appendix A.1, and ﬁrst employed in Beare and Seo (2018b) in the context of H-valued ARs with compact operators.

if A(1) = 0, one can factorize (1 − z)s from A(z), A(z) = (1 − z)s

A(z) for some

A(1) 6= {0} and

9

some s > 0, and rewrite the AR equations A(L)xt = εt as

A(L)yt = εt for yt = ∆sxt.
e

e

In order to state the key assumption, it is useful to expand the operator function A(z) = I −
k
h=1 A◦

hzh around 1, obtaining

e

P

A(z) =

∞

n=0
X

An(1 − z)n,

An =

I −

(−1)n+1

(

k
h=1 A◦
h
n+h
n

k−n
h=0
P

A◦

n+h

for n = 0
for n = 1, 2, . . .

,

(3.2)

where empty sums are deﬁned to be 0 and hence An = 0 for n > k.

P

(cid:0)

(cid:1)

The notion of eigenvalue of ﬁnite type, see Gohberg et al. (1990, section XI.9), is central in the

present setup and it is reported next. For any A ∈ LH the subspace {v ∈ H : Av = 0}, written

Ker A, is called the kernel of A and the subspace {Av : v ∈ H}, written Im A, is called the image

of A. The dimension of Im A, written dim Im A, is called the rank of A.

Deﬁnition 3.1 (Eigenvalue of ﬁnite type). A point z0 ∈ C is said to be an eigenvalue of ﬁnite type
of A(z) if
(i) A(z0) is Fredholm, i.e. n = dim Ker A(z0) < ∞ and q = dim(Im A(z0))⊥ < ∞, of index n − q,
(ii) A(z0)v = 0 for some nonzero v ∈ H,
(iii) A(z) is invertible for all z in some punctured disc D(z0, δ) \ {z0}.

Direct consequences of this deﬁnition are listed in the following remark.

Remark 3.2. If A(z) has an eigenvalue of ﬁnite type at z = z0, A(z0) is necessarily Fredholm of

index 0, see Gohberg et al. (1990, Section XI.9). Combining this with (i) and (ii) in Deﬁnition
3.1 one thus has that 0 < dim Ker A(z0) = dim(Im A(z0))⊥ < ∞.5 Moreover, Im(A(z0)) is nec-
essarily closed, see Theorem 2.1 in Gohberg et al. (2003, Section 15.2), and hence, see Theorem

3 in Ben-Israel and Greville (2003, Chapter 9), the generalized maximal Tseng inverse of A(z0)
exists, written A(z0)+, and it is unique. In the following the ‘generalized maximal Tseng inverse’
is abbreviated in the ‘generalized inverse’.

The key assumption is introduced next.

Assumption 3.3 (AR with a unit root of ﬁnite type at z = 1). Let A(z) be as in (3.1) and (3.2),

with an eigenvalue of ﬁnite type at z = 1 in the disc z ∈ D(0, ρ), ρ > 1; then A(z) is said to be an

AR with a unit root of ﬁnite type at z = 1, or simply an AR with a unit root of ﬁnite type.

That is, an AR with a unit root of ﬁnite type is such that A(z) is invertible for all z ∈ D(0, ρ)\{1}
for some ρ > 1, 0 < dim Ker A0 = dim(Im A0)⊥ < ∞ and Im A0 is closed, where A0 is as in (3.2).

5Remark that when H is ﬁnite dimensional any operator is Fredholm of index 0 and any eigenvalue is of ﬁnite

type.

10

3.2. Existence of a common trends representation. Under Assumption 3.3, one can apply

the results in Section XI.9 of Gohberg et al. (1990), reported in Theorem A.1 in Appendix A.1, and

ﬁrst employed in Beare and Seo (2018b) in the context of H-valued ARs with compact operators.

These results guarantee that there exist a ﬁnite integer d = 1, 2, . . . and ﬁnite rank operators

C0, C1, . . . , Cd−1 such that

∞

A(z)−1 =

Cn(1 − z)n−d,

z ∈ D(0, ρ) \ {1},

ρ > 1,

(3.3)

so that the inverse of A(z) has a pole of ﬁnite order d at z = 1.

n=0
X

This implies that the solution of the AR equations is I(d) for some ﬁnite integer d. Moreover,
because the operators that make up the principal part of A(z)−1 around z = 1 have ﬁnite rank, xt
displays a common trends representation with a ﬁnite number of common stochastic trends of the

type of (cumulated) bilateral random walks, as reported in Theorem 3.5 below.

In order to state Theorem 3.5, the cumulation operator S in introduced, following Gregoir (1999).

Deﬁnition 3.4 (Integral operator S). For a generic process {wt, t ∈ Z} the integral operator S is
deﬁned as

t

0

Swt = 1(t≥1) ·

wi − 1(t≤−1) ·

wi.

(3.4)

When wt = εt is white noise, the notation sh,t = S hεt, h = 1, 2, . . . , is employed.

i=1
X

i=t+1
X

Remark that by deﬁnition S assigns value 0 to the cumulated process at time 0. In fact, applying

the deﬁnition, also see Properties 2.1, 2.2 in Gregoir (1999), one has

∆Swt = wt,

S∆wt = wt − w0,

t ∈ Z.

(3.5)

Eq. (3.5) shows that S applied to ∆wt regenerates the level of the process wt, up to a constant;

this parallels the constant of integration in indeﬁnite integrals. The integral operator S is hence

the inverse of the diﬀerence operator ∆ up a constant, which is set by Deﬁnition 3.4 so as to make

the cumulated process S∆wt equal to 0 at time 0.

Note that when wt = εt is white noise, (3.4) implies that s1,t = Sεt is a bilateral H-valued

random walk, see Bosq (2000, example 1.9 on p. 20); because ∆s1,t = ∆Sεt = εt is I(0), this shows

that s1,t is I(1). Similarly, for h = 2, 3, . . . , sh,t = Ssh−1,t ∼ I(h) is the (h − 1)-fold cumulation of

the bilateral random walk s1,t ∼ I(1).

The following results connects ARs with a unit root of ﬁnite typewith the existence of a common

trend representation in terms of stochastic trends of the above type.

Theorem 3.5 (Existence of a common trends representation). Let A(L)xt = εt be an AR with

a unit root of ﬁnite type. Then there exist a ﬁnite integer d = 1, 2, . . . and ﬁnite rank operators

C0, C1, . . . , Cd−1 such that xt has common trends representation

xt = C0sd,t + C1sd−1,t + · · · + Cd−1s1,t + yt + µt,

t ∈ Z,

(3.6)

11

where sh,t = S hεt ∼ I(h) is the (h − 1)-fold cumulation of the bilateral random walk s1,t ∼ I(1),
d−1
n=0 vntn is the expectation of xt, where v0, . . . , vd−1 ∈ H
yt = C ⋆
depend on the initial values of xt, yt, εt for t = −d, . . . , 0.

d (L)εt is a linear process, µt =

P

In the common trends representation (3.6) the operators C0, C1, . . . , Cd−1 have ﬁnite rank; this

implies that xt depends only on a ﬁnite number of bilateral (cumulated) random walks. In fact, these

common stochastic trends are selected from sh,t ∼ I(h), h = 1, . . . , d, by the ﬁnite rank operators

C0, C1, . . . , Cd−1 that load onto xt only a ﬁnite number of characteristics from sh,t, h = 1, . . . , d.

Theorem 3.5 implies a number of properties for ARs with a unit root of ﬁnite type, some of

which are listed in the following corollary.

Corollary 3.6 (Cointegration properties). Let A(L)xt = εt be an AR with a unit root of ﬁnite
type. Then

(i) xt ∼ I(d) for some ﬁnite integer d = 1, 2, . . . ,
(ii) xt ∼ I(d) is cointegrated,
(iii) Im C0 is the ﬁnite dimensional attractor space,
(iv) (Im C0)⊥ is the inﬁnite dimensional cointegrating space.

Corollary 3.6 lists some implications of Theorem 3.5, namely that d (the order of the pole of the

inverse of A(z) at z = 1) is ﬁnite, the process is cointegrated, the number of common trends is

ﬁnite and the number of cointegrating relations is inﬁnite.

Despite these interesting implications of Theorem 3.5, these existence results do not address a

number of important issues, such as the connection between the structure of A(z) and the order of

integration d of the process. In fact, one cannot determine the order of integration of the solution

of the AR equations using Theorem 3.5. Moreover, Theorem 3.5 does not specify the connection

between Im C0 and the AR operators, so that one does not know how to construct the attractor

space and the cointegrating space in terms of the AR operators. Finally, the relations among the

ﬁnite rank operators C0, C1, . . . , Cd−1 are not speciﬁed and hence Theorem 3.5 is silent about the

structure of the cointegrating relations.

These additional characterization results form the main contribution of the present paper; they

go beyond Theorem 3.5 and Corollary 3.6, and they are presented in full generality in Section 5 for

the generic I(d) case. For ease of presentation, Section 4 starts with the I(1) and I(2) cases.

3.3. Relations with the literature. Before turning to these results, the present section discusses

the relationship between Assumption 3.3 and the assumptions employed in the literature. An

example in the next section illustrates the diﬀerences.

The following proposition discusses the relation with Chang et al. (2016b), who study I(1) pro-

cesses xt satisfying ∆xt = B(L)εt, where

∞
n=1 nkBnkLH < ∞ and dim Im B(1) < ∞.

P

12

Proposition 3.7. Let A(L)xt = εt be an AR with a unit root of ﬁnite type with d = 1. Then
∞
∞
n=0 Bnzn, z ∈ C, is such that
n=1 nkBnkLH < ∞ and Im B(1)
∆xt = B(L)εt, where B(z) =
is ﬁnite dimensional. The converse does not necessarily hold.
P

P

This shows that I(1) ARs with a unit root of ﬁnite type necessarily satisfy Assumption 2.1 in

Chang et al. (2016b); hence their asymptotic analysis applies and their test can be employed in the

present setup.

The next proposition discusses the relation with Hu and Park (2016), who consider (3.1) with

k = 1 and compact A◦
k > 1 and Beare and Seo (2018b) consider (3.1) with compact A◦

1. Similarly, Beare et al. (2017) consider (3.1) with compact A◦

1, . . . , A◦

k for k ≥ 1.

1, . . . , A◦

k if

Proposition 3.8. Assume that A◦

1, . . . , A◦

k, k ≥ 1, in (3.1) are compact. Then (3.1) is an AR with

a unit root of ﬁnite type. The converse does not necessarily hold.

This shows that the present results can be applied to the setups of Hu and Park (2016), Beare et al.

(2017) and Beare and Seo (2018b). Beare et al. (2017) also consider xt = A◦
pactness assumption on A◦

1, see Proposition 4.4 below.

1xt−1 + εt with no com-

Finally, Chang et al. (2016a) consider an error correction form with compact error correction

operator and show that in this case the number of I(1) common trends is inﬁnite and the dimension

of the cointegrating space is ﬁnite. This case is not covered by the present results.

3.4. Example of a non-compact operator. This section illustrates the relevance of Assump-

tion 3.3 with a simple example. This example is considered again in Section 4.3 to illustrate the

characterization results in the I(1) case.
1xt−1 + εt where A◦

Consider xt = A◦

1 is a band operator. Band operators are deﬁned as follows:
let ϕ1, ϕ2, . . . be an orthonormal basis of H and let (aij), where aij = hAϕj, ϕii, be the matrix

representation of A ∈ LH corresponding to ϕ1, ϕ2, . . . , see e.g. Gohberg et al. (2003, Section 2.4);

A ∈ LH is called a band operator if all nonzero entries in its matrix representation (aij) are in a

ﬁnite number of diagonals parallel to the main diagonal, i.e. there exists an integer N such that

aij = 0 if |i − j| > N , see e.g. Gohberg et al. (2003, Section 2.16).

Note that a band operator is compact if and only if limi,j→∞ aij = 0, see Theorem 16.4 in

Gohberg et al. (2003, Section 2.16). Here limi,j→∞ aij = 0 is not assumed, hence the operator
A◦
1 is non-necessarily compact. Finally, let zi,t = hϕi, zti be the i-th coordinate of the process
zt = xt, εt, and note that from (3.2) one has A0 = I − A◦
1 and An = 0 for n = 2, 3, . . . , in
∞
n=0 An(1 − z)n, so that A(z) = A0 + A1(1 − z).
A(z) =

1, A1 = A◦

Let (aij) be the matrix representation of A◦

P

where αi ∈ R, α1 = 1 and 0 < |αi| < 1, i = 2, 3, . . . , so that A◦
xt = A◦

1xt−1 + εt reads

1 and assume that aij = 0 for |i − j| > 0 and aii = αi,
1 is a band operator. Observe that

x1,t = x1,t−1 + ε1,t,

xi,t = αixi,t−1 + εi,t,

i = 2, 3, . . . .

Remark that A◦
invertible for all z ∈ D(0, ρ) \ {1} for some ρ > 1 and consider the matrix representation of A◦
and A0 = I − A◦

1 is not compact because limi,j→∞ aij = 0 is not imposed. Next note that A(z) is
1 = A1

1, i.e.

13

1

A◦

1 = A1 = 

α2

0



,

A0 = 

1 − α2





. . .









where empty entries are equal to 0, and compute

(3.7)

. . .



,





(Im A0)⊥ = (sp{ϕ2, ϕ3, . . . })⊥ = sp {ϕ1},

Ker A0 = sp {ϕ1},

where sp{·} and sp{·} indicate the span of the set of vectors in curly brackets and its closure
respectively. Because 0 < dim Ker A0 = dim(Im A0)⊥ < ∞, this shows that A0 is Fredholm of
index 0, so that Assumption 3.3 holds and xt = A◦
1xt−1 + εt is an AR with a unit root of ﬁnite type
with non-compact operator.

4. A characterization of I(1) and I(2) ARs with a unit root of finite type

This section presents a characterization of I(1) and I(2) ARs with a unit root of ﬁnite type. The

I(1) case parallels the results in Hu and Park (2016), Beare et al. (2017), Beare and Seo (2018b),

and it is discussed in Theorem 4.1. The results for the I(2) case are novel, and they are given in

Theorem 4.6.

4.1. I(1) case. The following notation is employed: write A(z) =

∞
n=0 An(1 − z)n as in (3.2) and

deﬁne

P

S0 = A0,

S1 = Pζ ⊥

0

A1Pτ ⊥

0

,

ζ0 = Im S0,

ζ1 = Im S1,

τ0 = (Ker S0)⊥,

τ1 = (Ker S1)⊥,

(4.1)

(4.2)

where Pη ∈ LH indicates the orthogonal projection on η, i.e. P 2

η = Pη, Im Pη = η and Ker Pη = η⊥.

Observe that

ζ1 ⊆ ζ ⊥
0 ,

τ1 ⊆ τ ⊥
0

by construction; that is, ζ1 is orthogonal to ζ0 and τ1 is orthogonal to τ0. Moreover, because 1 is an
0 = dim ζ ⊥
eigenvalue of ﬁnite type, one has 0 < dim τ ⊥
0 < ∞, see Remark 3.2, so that the subspaces
ζ1, τ1 are ﬁnite dimensional. In the following, a ⇒ b indicates that a implies b and the orthogonal

direct sum decomposition

is called the pole(1) condition.

H = τ0 ⊕ τ1,

τ1 6= {0},

(4.3)

Theorem 4.1 (A characterization of I(1) ARs with a unit root of ﬁnite type). Consider an AR with

a unit root of ﬁnite type A(L)xt = εt and let τ0, τ1 be as in (4.1), (4.2) respectively. Then xt is I(1)
if and only if the pole(1) condition in (4.3) holds; in this case, the common trends representation

14

of xt is found by setting d = 1 in (3.6). Moreover, Im C0 = τ1 is the ﬁnite dimensional attractor
space, τ0 is the inﬁnite dimensional cointegrating space and for any nonzero v ∈ H one has

v ∈ τ0

v ∈ τ1

⇒

⇒

hv, xti ∼ I(0),

hv, xti ∼ I(1),

(4.4)

(4.5)

where τ1 = τ ⊥

0 6= {0}.

Theorem 4.1 shows that an AR with a unit root of ﬁnite type generates an I(1) process if and

only if τ1 = τ ⊥
0 6= {0}. The common trends representation of xt shows that the I(1) stochastic
trends s1,t are loaded into the process by C0; because Im C0 coincides with τ1, τ1 is the ﬁnite

dimensional attractor space and the number of I(1) trends in xt is ﬁnite and equal to dim τ1.

Moreover, because Im C0 = τ1 = τ ⊥

0 , for any nonzero v ∈ τ0 one has hv, C0yi = 0 for all y ∈ H,
and hence also for y = sd,t in (3.6); this implies that hv, xti is stationary, i.e. τ0 is the inﬁnite

dimensional cointegrating space. Note that this decomposition is orthogonal.

Using orthogonal projections, one can see that this orthogonal direct sum decomposition can be

employed in general to characterize the degree of integration of any v-characteristic of the process.

In fact, note that (4.3) implies Pτ0 + Pτ1 = I, where Pτh is the orthogonal projection onto τh; hence
for any nonzero v ∈ H one has hv, xti = hv0, xti + hv1, xti, where vh = Pτhv ∈ τh, so that (4.4) and
(4.5) describe the order of integration of any nonzero v-characteristic hv, xti of xt. In particular

one has hv, xti ∼ I(1) if and only if v1 6= 0, because hv0, xti is I(0).

Theorem 4.1 further shows that for any nonzero v ∈ τ0, hv, xti is not only stationary, but I(0).

This echoes the ﬁnite dimensional case, see Theorem 4.2 in Johansen (1996), except for the fact

that the number of I(0) cointegrating relations is inﬁnite.

Remark 4.2. Let sp{a} indicate sp{a1, . . . , ak} when its argument a is a matrix with k columns ai,
a = (a1, . . . , ak). In the ﬁnite dimensional case H = Rp, Franchi and Paruolo (2016) show that the
I(1) condition in Theorem 4.2 in Johansen (1996) can be equivalently stated as Rp = ζ0⊕ζ1 = τ0⊕τ1,
ζ1 6= {0} and τ1 6= {0}, where ζh = sp{αh}, τh = sp{βh}, h = 0, 1, and the bases αh, βh are deﬁned
by the rank factorizations A0 = α0β′
1, i.e. αh, βh are full-column-rank
matrices that respectively span the column space ζh and the row space τh of the corresponding
matrix. Except for the fact that dim ζ0 = dim τ0 is ﬁnite when H = Rp, this mirrors what happens
in the present inﬁnite dimensional case.

0 and Pζ ⊥

= α1β′

A1Pτ ⊥

0

0

Remark 4.3. The pole(1) condition in (4.3) is equivalent to τ1 = τ ⊥
0 6= {0}. Moreover, Theorem
B.4 in Appendix B shows that it can be equivalently stated as (i) H = ζ0 ⊕ ζ1, ζ1 6= {0}, (ii)
ζ1 = ζ ⊥

0 6= {0}, (iii) Im C0 = τ1, (iv) Ker C0 = ζ0.

The pole(1) condition is next compared to equivalent conditions in the literature. Beare et al.

(2017, Deﬁnition 4.3) deﬁne the following non-orthogonal direct sum decomposition

H = Im A0 ⊕ A1 Ker A0,

(4.6)

where A0, A1 are as in (3.2); they call (4.6) the ‘Johansen I(1) condition’. Their Theorem 4.1
assumes that xt = A◦
1 and that the k = 1 version
of (4.6) holds, i.e. H = Im A0 ⊕ Ker A0. Under these conditions, they ﬁnds the common trends

1xt−1 + εt with no compactness assumption on A◦

representation (3.6) with d = 1 and Im C0 = Ker A0. The following proposition clariﬁes the

connection between ARs with a unit root of ﬁnite type and their result.

15

Proposition 4.4. Consider xt = A◦
H = Im A0 ⊕ Ker A0. If Ker A0 is ﬁnite dimensional then xt = A◦
root of ﬁnite type.

1xt−1 + εt with no compactness assumption on A◦

1 and let
1xt−1 + εt is an AR with a unit

One can observe that the case with inﬁnite dimensional Ker A0, which correspons to an inﬁnite

dimensional attractor space, is not covered by the present results.

Finally, the following proposition proves the equivalence of the orthogonal direct sum condition

in (4.3) and the nonorthogonal direct sum conditions in (4.6).

Proposition 4.5. Let A(L)xt = εt be an AR with a unit root of ﬁnite type; then the I(1) condition
in (4.6) is equivalent to the pole(1) condition in (4.3).

4.2. I(2) case. The I(2) case is considered next. Consider A(z) =

∞
n=0 An(1 − z)n in (3.2), and

let ζ0, τ0 be as in (4.1), consider ζ1, τ1 as in (4.2), and deﬁne

P

S2 = PZ ⊥
2

A2,1PT ⊥
2

,

ζ2 = Im S2,

τ2 = (Ker S2)⊥,

(4.7)

where Z2 = ζ0 ⊕ ζ1, T2 = τ0 ⊕ τ1 and A2,1 = A2 − A1A+
exists and it is unique, see Remark 3.2.

0 A1, where the generalized inverse A+
0

Observe that

ζ2 ⊆ (ζ0 ⊕ ζ1)⊥,

τ2 ⊆ (τ0 ⊕ τ1)⊥

by construction; that is, for 0 < j < h, ζh is orthogonal to ζj, and τh is orthogonal to τj. Moreover,
because 0 < dim ζ ⊥
0 < ∞, the subspaces ζ2, τ2 are ﬁnite dimensional. In the following,

0 = dim τ ⊥

the orthogonal direct sum decomposition

H = τ0 ⊕ τ1 ⊕ τ2,

τ2 6= {0},

(4.8)

is called the pole(2) condition.

Theorem 4.6 (A characterization of I(2) ARs with a unit root of ﬁnite type). Consider an AR

with a unit root of ﬁnite type A(L)xt = εt, let τ0, τ1, τ2 be as in (4.1), (4.2), (4.7) respectively
and let A+
0 be the generalized inverse of A0; then xt is I(2) if and only if the pole(2) condition in
(4.8) holds. In this case, the common trends representation of xt is found by setting d = 2 in (3.6).
Moreover, Im C0 = τ2 is the ﬁnite dimensional attractor space, τ0 ⊕ τ1 is the inﬁnite dimensional

16

cointegrating space and for any nonzero v-characteristics v ∈ H one has

v ∈ τ0

v ∈ τ1

v ∈ τ2

⇒

⇒

⇒

hv, xti + hv, A+

0 A1∆xti ∼ I(0),

hv, xti ∼ I(1),

hv, xti ∼ I(2),

(4.9)

(4.10)

(4.11)

where τ1 ⊂ τ ⊥

0 and τ2 = (τ0 ⊕ τ1)⊥ 6= {0}.

Some remarks on Theorem 4.6 are in order.

Remark 4.7. An AR with a unit root of ﬁnite type generates an I(2) process if and only if τ2 =
(τ0 ⊕ τ1)⊥ 6= {0}. The common trends representation of xt shows that the I(2) stochastic trends
s2,t are loaded into the process by C0; because Im C0 coincides with τ2, τ2 is the ﬁnite dimensional

attractor space and the number of I(2) trends in xt is ﬁnite and equal to dim τ2.

Remark 4.8. Moreover, because Im C0 = τ2 = (τ0 ⊕ τ1)⊥, for any nonzero v ∈ τ0 ⊕ τ1 one has
hv, C0yi = 0 for all y ∈ H, and hence also for y = sd,t in (3.6); this implies that hv, xti is at most

I(1), i.e. τ0 ⊕ τ1 is the inﬁnite dimensional cointegrating space. Note that this decomposition is

orthogonal. Using orthogonal projections, one can see that this orthogonal direct sum decompo-

sition can be employed in general to characterize the degree of integration of any v-characteristic

of the process. In fact, note that (4.8) implies Pτ0 + Pτ1 + Pτ2 = I, where Pτh is the orthogonal
projection onto τh; hence for any nonzero v ∈ H one has hv, xti = hv0, xti + hv1, xti + hv2, xti, where

vh = Pτhv ∈ τh, so that (4.9), (4.10) and (4.11) describe the order of integration of any nonzero
v-characteristic hv, xti of xt. In particular, one has hv, xti ∼ I(2) if and only if v2 6= 0, because

hv0, xti + hv1, xti is at most I(1).

Remark 4.9. Theorem 4.6 further shows that in τ0, which is inﬁnite dimensional, one ﬁnds the

cointegrating vectors that allow for polynomial cointegration of order 0 and in τ1, with 0 ≤ dim τ1 <

∞, those that don’t allow for polynomial cointegration. Speciﬁcally, any nonzero v0 ∈ τ0, if
one combines levels and ﬁrst diﬀerences as in hv0, xti + hv0, A+
0 A1∆xti one ﬁnds an I(0) process;
given that hv0, A+
0 A1∆xti can as well be equal to 0, there may exist a nonzero v0 ∈ τ0 such that
hv0, xti ∼ I(0). This cannot happen in the τ1 subspace, in which every nonzero v1 ∈ τ1 is such that

hv1, xti ∼ I(1). Apart from the fact that the number of I(0) cointegrating relations is inﬁnite, this

mimics the ﬁnite dimensional case, see Theorem 4.6 in Johansen (1996).

Remark 4.10. In the ﬁnite dimensional case H = Rp, Franchi and Paruolo (2016) show that the
I(2) condition in Theorem 4.6 in Johansen (1996) can be equivalently stated as Rp = ζ0 ⊕ ζ1 ⊕ ζ2 =
τ0 ⊕ τ1 ⊕ τ2, ζ2 6= {0} and τ2 6= {0}, where ζh = sp{αh}, τh = sp{βh}, h = 0, 1, 2, and the bases αh,
= α2β′
βh are deﬁned by the rank factorizations A0 = α0β′
2
where A2,1 = A2 − A1 ¯β0 ¯α′
0 and ¯η = η(η′η)−1 for a generic full-column-rank

0)+ = ¯β0 ¯α′

0A1, (α0β′

1 and PZ ⊥

A2,1PT ⊥
2

= α1β′

A1Pτ ⊥

0, Pζ ⊥

0

0

2

matrix η. Again here, apart from the fact that dim ζ0 = dim τ0 is ﬁnite when H = Rp, this is
exactly what happens in the inﬁnite dimensional case.

17

Remark 4.11. The pole(2) condition in (4.8) is equivalent to τ2 = (τ0 ⊕ τ1)⊥ 6= {0}. Moreover,
Theorem B.4 in Appendix B shows that it can be equivalently stated as (i) H = ζ0 ⊕ ζ1 ⊕ ζ2,
ζ2 6= {0}, (ii) ζ2 = (ζ0 ⊕ ζ1)⊥ 6= {0}, (iii) Im C0 = τ2, (iv) Ker C0 = ζ0 ⊕ ζ1.

4.3. Illustrations. This section illustrates Theorems 4.1 and 4.6 via two simple examples, called

the I(1) and the I(2) examples.

I(1) example. Consider the setup in Section 3.4. Here the analysis should deliver that xt is I(1),

the attractor space coincides with sp{ϕ1} and the cointegrating space with sp{ϕ2, ϕ3, . . . }. Since

hv, xti is I(0) for any nonzero v ∈ sp{ϕ2, ϕ3, . . . } and hv, xti is I(1) for any nonzero v ∈ sp{ϕ1},

the analysis should further convey that τ0 = sp{ϕ2, ϕ3, . . . } and τ1 = sp{ϕ1}.

From (3.7), one has

ζ0 = Im A0 = sp{ϕ2, ϕ3, . . . },

τ0 = (Ker A0)⊥ = (sp {ϕ1})⊥ = sp{ϕ2, ϕ3, . . . },

ζ1 = Im Pζ ⊥

0

A1Pτ ⊥

0

= sp {ϕ1},

τ1 = (Ker Pζ ⊥

0

A1Pτ ⊥

0

)⊥ = (sp{ϕ2, ϕ3, . . . })⊥ = sp {ϕ1}.

This shows that H = τ0 ⊕ τ1, τ1 6= {0}, so that the pole(1) condition in (4.3) holds and

Theorem 4.1 applies: the common trends representation of xt is found by setting d = 1 in (3.6),

Im C0 = τ1 = sp {ϕ1} is the ﬁnite dimensional attractor space and τ0 = sp{ϕ2, ϕ3, . . . } is the

inﬁnite dimensional cointegrating space.

I(2) example. Let (aij) be the matrix representation of A◦

1 and assume that aij = 0 for
|i − j| > 1, a12 = 1 and aii = αi, where αi ∈ R, α1 = α2 = α3 = 1 and 0 < |αi| < 1, i = 4, 5, . . . .
Again here, A◦
1xt−1 + εt is an AR with a unit root of ﬁnite
type, as shown below. Observe that xt = A◦

1 is not necessarily compact but xt = A◦

1xt−1 + εt reads

x1,t = x1,t−1 + x2,t−1 + ε1,t,
xi,t = αixi,t−1 + εi,t,

x2,t = x2,t−1 + ε2,t,
i = 4, 5, . . . .

x3,t = x3,t−1 + ε3,t,

Hence the analysis should deliver that xt is I(2), the attractor space coincides with sp{ϕ1} and

the cointegrating space with sp{ϕ2, ϕ3, . . . }. Next note that hv, xti is I(0) for any nonzero v ∈

sp{ϕ4, ϕ5, . . . } and hv, xti is I(1) for any nonzero v ∈ sp{ϕ2, ϕ3}. Moreover, because ∆x1,t =

x2,t−1 + ε1,t = x2,t − ε2,t + ε1,t, one has that x2,t − ∆x1,t is I(0), i.e. hϕ2, xti − hϕ1, ∆xti is I(0), so

that hϕ2, xti allows for polynomial cointegration while hϕ3, xti does not. Hence the analysis should
further convey that τ0 = sp{ϕ2, ϕ4, ϕ5, . . . }, τ1 = sp{ϕ3}, τ2 = sp{ϕ1}, hϕ2, A+
0 A1∆xi,ti = −∆xi,t,
and hϕi, A+

0 A1∆xi,ti = 0 for i = 4, 5, . . . .

18

Consider the matrix representation of A◦

1 = A1 and A0 = I − A◦

1, i.e.

1 1
1



A◦

1 = A1 =

1















where empty entries are equal to 0. Compute

. . .

α4



,

A0 =

0 −1
0











0

1 − α4

,











. . .

ζ0 = Im A0 = sp{ϕ1, ϕ4, ϕ5, . . . },

τ0 = (Ker A0)⊥ = (sp {ϕ1, ϕ3})⊥ = sp{ϕ2, ϕ4, ϕ5, . . . },

0 = sp {ϕ2, ϕ3} and τ ⊥

so that ζ ⊥
shows that A0 is Fredholm of index 0 and because A(z) = I −A◦
for some ρ > 1, xt = A◦

0 = sp {ϕ1, ϕ3}; because 0 < dim Ker A0 = dim(Im A0)⊥ < ∞, this
1z is invertible for all z ∈ D(0, ρ)\{1}
1xt−1 +εt is an AR with a unit root of ﬁnite type with non-compact operator.

Next compute

ζ1 = Im Pζ ⊥

0

A1Pτ ⊥

0

= sp {ϕ3},

τ1 = (Ker Pζ ⊥

0

A1Pτ ⊥

0

)⊥ = (sp{ϕ1, ϕ2, ϕ4, ϕ5, . . . })⊥ = sp {ϕ3}.

This shows that τ1 ⊂ τ ⊥
I(d) for some ﬁnite d = 2, 3, . . . .

0 , so that the pole(1) condition in (4.3) does not hold and the process is

Now consider PZ ⊥
2

in (4.7); since Z2 = ζ0 ⊕ ζ1 = sp{ϕ1, ϕ3, ϕ4, . . . } and T2 = τ0 ⊕ τ1 =
A2,1PT ⊥
2
sp{ϕ2, ϕ3, . . . }, one has Z ⊥
2 = sp {ϕ1}. Note that A2 = 0 and hence A2,1 =
−A1A+
0 A1Psp{ϕ1} and because Psp{ϕ2}A1 = Psp{ϕ2} and
0 A1; thus PZ ⊥
0 Psp{ϕ1}. Next the matrix representation
A1Psp{ϕ1} = Psp{ϕ1}, one has PZ ⊥
of A+
0 = (Im A0)⊥, A+
0 A0 = P(Ker A0)⊥ and
because (Im A0)⊥ = sp {ϕ2, ϕ3} and (Ker A0)⊥ = sp{ϕ2, ϕ4, ϕ5, . . . } one has

= −Psp{ϕ2}A+
0 is investigated; from Lemma B.1 one has Ker A+

2 = sp {ϕ2} and T ⊥
= −Psp{ϕ2}A1A+

A2,1PT ⊥
2

A2,1PT ⊥
2

2

2

0
−1 0



A+

0 =

0



.








This implies that the only nonzero element in the matrix representation of PZ ⊥
2









1
1−α4

. . .

A2,1PT ⊥
2

= −Psp{ϕ2}A+

0 Psp{ϕ1}

is a one in row 2 and column 1, so that

ζ2 = Im PZ ⊥
2

A2,1PT ⊥
2

= sp {ϕ2},

τ2 = (Ker PZ ⊥
2

A2,1PT ⊥
2

)⊥ = (sp{ϕ2, ϕ3, . . . })⊥ = sp {ϕ1}.

Hence H = τ0 ⊕ τ1 ⊕ τ2, τ2 6= {0}, i.e. the pole(2) condition in (4.8) holds and Theorem 4.6 applies:

the common trends representation of xt is found by setting d = 2 in (3.6), Im C0 = τ2 = sp {ϕ1}

is the ﬁnite dimensional attractor space and τ0 ⊕ τ1 = sp{ϕ2, ϕ3, . . . } is the inﬁnite dimensional

cointegrating space. Moreover, for any nonzero v ∈ H one has

v ∈ τ0 = sp{ϕ2, ϕ4, ϕ5, . . . }

v ∈ τ1 = sp {ϕ3}

v ∈ τ2 = sp {ϕ1}

⇒

⇒

⇒

hv, xti + hv, A+

0 A1∆xti ∼ I(0),

hv, xti ∼ I(1),

hv, xti ∼ I(2).

19

0 and τ2 = (τ0 ⊕ τ1)⊥ 6= {0}. Moreover, hϕ2, A+
∆xi,t, for i = 4, 5, . . . ; hence hϕ2, xti + hϕ2, A+

Note that τ1 ⊂ τ ⊥
0 A1∆xti = αi
hϕi, A+
1−αi
and, for i = 4, 5, . . . , hϕi, xti + hϕi, A+
0 A1∆xti contains stationary terms (∆x2,t and αi
hv, A+
1−αi
0 A1Pτ2 ∆xti instead of hv, A+
gration; these can be eliminated by considering hv, A+

0 A1∆xti = xi,t + αi
1−αi

0 A1∆xti = −∆x1,t − ∆x2,t and
0 A1∆xti = x2,t − ∆x1,t − ∆x2,t
∆xi,t. This shows that hv, xti +

∆xi,t) that are not necessary for cointe-
0 A1∆xti, as in

the ﬁnite dimensional I(2) case, see Theorem 4.6 in Johansen (1996).

5. A characterization of I(d) ARs with a unit root of finite type

This section extends the results in Section 4 to the general I(d), d = 1, 2, · · · < ∞, case. Theorem

5.3 provides a necessary and suﬃcient condition for ARs with a unit root of ﬁnite type to be I(d)

and it is shown that under this condition the space H is decomposed into the direct sum of d + 1

orthogonal subspaces τh, H = τ0 ⊕ τ1 ⊕ · · · ⊕ τd, τd 6= {0}, that are deﬁned in terms of A0, A1, . . . , Ad

in (3.2), see Deﬁnition 5.1 below.

The ﬁnite dimensional attractor space coincides with τd and τ0 ⊕ τ1 ⊕ · · · ⊕ τd−1 is the inﬁnite

dimensional cointegrating space. In τ0, which is inﬁnite dimensional, one ﬁnds the cointegrating

vectors that allow for polynomial cointegration of order 0 and in τh, h = 1, . . . , d − 2, which is

ﬁnite dimensional and can as well be equal to 0, those that allow for polynomial cointegration of

order h. In τd−1, with 0 ≤ dim τd−1 < ∞, those that are I(d − 1) and don’t allow for polynomial

cointegration. Finally, any nonzero v ∈ τd is such that hv, xti ∼ I(d). The results in Section 4

are found as special cases for d = 1 and d = 2. Before stating the results, some deﬁnitions are

introduced.

Deﬁnition 5.1 (Sh, ζh, τh, and Ah,n). Consider an AR with a unit root of ﬁnite type A(L)xt = εt,
where A(z) =

∞
n=0 An(1 − z)n is as in (3.2). Let

P

S0 = A0,

ζ0 = Im S0,

τ0 = (Ker S0)⊥

and for h = 1, 2, . . . deﬁne

Sh = PZ ⊥
h

Ah,1PT ⊥

h

,

ζh = Im Sh,

τh = (Ker Sh)⊥,

Zh = ζ0 ⊕ · · · ⊕ ζh−1,

Th = τ0 ⊕ · · · ⊕ τh−1

(5.1)

(5.2)

where

and

Ah,n =

An
Ah−1,n+1 − Ah−1,1

(

h−2
j=0 S+

j Aj+1,n

for h = 1
for h = 2, 3, . . .

,

n = 1, 2, . . . .

(5.3)

P

20

A few remarks are in order.

Remark 5.2. First note that for h = 1, 2 (5.1), (5.2) and (5.3) deliver (4.2) and (4.7) respectively.

Next observe that for h = 1, 2, . . . one has

ζh ⊆ (ζ0 ⊕ · · · ⊕ ζh−1)⊥,

τh ⊆ (τ0 ⊕ · · · ⊕ τh−1)⊥

(5.4)

by construction; that is, for 0 < j < h, ζh is orthogonal to ζj and τh is orthogonal to τj. Moreover,
because 0 < dim ζ ⊥
0 < ∞, for h = 1, 2, . . . the subspaces ζh and τh are ﬁnite dimensional

0 = dim τ ⊥

and possibly of dimension equal to 0.

T ⊥

Note also that, as h increases, the ﬁnite dimensional subspaces Z ⊥
h = (τ0 ⊕ · · · ⊕ τh−1)⊥ have non-increasing dimension and, because 0 < dim ζ ⊥

h = (ζ0 ⊕ · · · ⊕ ζh−1)⊥ and
0 < ∞,
they will eventually have dimension 0. This shows that only a ﬁnite number of ζh, τh are nonzero.
Let s be the value of h such that Z ⊥
s
Theorem B.4 in Appendix B, the integer s is precisely the order of the pole of A(z)−1 at z = 1.

s = {0}. As shown in

6= {0} and Z ⊥

6= {0}, T ⊥
s

0 = dim τ ⊥

s = T ⊥

Finally observe that the generalized inverse of Sh, S+

h , exists and it is unique for h = 0, 1, . . . ,
because Im Sh, h = 0, 1, . . . , is closed; in fact, S0 is Fredholm of index 0, see Remark 3.2, and

dim Im Sh < ∞ for h = 1, 2, . . . .

In the following, the orthogonal direct sum decomposition

H = τ0 ⊕ τ1 ⊕ · · · ⊕ τd,

τd 6= {0},

(5.5)

is called the pole(d) condition.

Theorem 5.3 (A characterization of I(d) ARs with a unit root of ﬁnite type). Consider an AR

with a unit root of ﬁnite type A(L)xt = εt and let Sh, τh and Ah,n be as in Deﬁnition 5.1. Then
xt is I(d) if and only if the pole(d) condition in (5.5) holds. In this case, the common trends
representation of xt is found in (3.6). Moreover, Im C0 = τd is the ﬁnite dimensional attractor
space, τ0 ⊕ τ1 ⊕ · · · ⊕ τd−1 is the inﬁnite dimensional cointegrating space and for any nonzero
v-characteristic v ∈ H and for h = 0, 1, . . . , d, one has

d−h−1

v ∈ τh

⇒

hv, xti +

hv, S+

h Ah+1,n∆nxti ∼ I(h),

(5.6)

n=1
X
where empty sums are deﬁned to be 0, τh ⊂ (τ0 ⊕ · · · ⊕ τh−1)⊥ for h = 1, . . . , d − 1 and τd =
(τ0 ⊕ · · · ⊕ τd−1)⊥ 6= {0}.

Remark 5.4. Theorem 5.3 provides a full description of the properties of an I(d) AR with a unit

root of ﬁnite type for a generic d = 1, 2, · · · < ∞. For d = 1 and d = 2 one ﬁnds the most empirically

relevant I(1) and I(2) cases discussed in Theorems 4.1, 4.6. Remark that all the relevant quantities

in Theorem 5.3 are expressed in terms of the AR operators via Deﬁnition 5.1.

21

Remark 5.5. Also note that (5.6) provides information that parallels the Triangular Representation

for ﬁnite dimensional process discussed in Phillips (1991) and Stock and Watson (1993); see also

Franchi and Paruolo (2018, Corollary 4.6).

Remark 5.6. An AR with a unit root of ﬁnite type generates an I(d) process if and only if τd =
(τ0 ⊕ · · · ⊕ τd−1)⊥ 6= {0}. The common trends representation of xt shows that the I(d) stochastic
trends sd,t are loaded into the process by C0; because Im C0 coincides with τd, τd is the ﬁnite

dimensional attractor space and the number of I(d) trends in xt is ﬁnite and equal to dim τd.

Moreover, because Im C0 = τd = (τ0 ⊕ · · · ⊕ τd−1)⊥, for any nonzero v ∈ τ0 ⊕ τ1 ⊕ · · · ⊕ τd−1 one
has hv, C0yi = 0 for all y ∈ H, and hence also for y = sd,t in (3.6); this implies that hv, xti is at

most I(d − 1), i.e. τ0 ⊕ τ1 ⊕ · · · ⊕ τd−1 is the inﬁnite dimensional cointegrating space. Note that

this decomposition is orthogonal.

Using orthogonal projections, one can see that this orthogonal direct sum decomposition can be

employed in general to characterize the degree of integration of any v-characteristic of the process.

In fact, note that (5.5) implies Pτ0 + Pτ1 + · · · + Pτd = I, where Pτh is the orthogonal projection
onto τh; hence for any nonzero v ∈ H one has hv, xti = hv0, xti + hv1, xti + · · · + hvd, xti, where

vh = Pτh v ∈ τh. (5.6) describes the order of integration of any nonzero characteristic hv, xti of xt.
In particular one has hv, xti ∼ I(d) if and only if vd 6= 0, because hv0, xti + hv1, xti + · · · + hvd−1, xti

is at most I(d − 1).

Remark 5.7. Theorem 5.3 further shows how the properties of hv, xti vary with v ∈ τ0⊕τ1⊕· · ·⊕τd−1:
in τ0, which is inﬁnite dimensional, one ﬁnds the cointegrating vectors that allow for polynomial
0 An∆nxti ∼ I(0),
while in τh, h = 1, . . . , d − 2, which is ﬁnite dimensional and can as well be equal to 0, those

cointegration of order 0, i.e. for any nonzero v ∈ τ0, one has hv, xti +

d−1
n=1hv, A+

P

d−2
n=1hv, S+

that allow for polynomial cointegration of order h, i.e.

1 A2,n∆nxti ∼ I(1), for any nonzero v ∈ τ2, one has hv, xti +

for any nonzero v ∈ τ1, one has hv, xti +
d−3
n=1hv, S+
2 A3,n∆nxti ∼
I(2) and so on up to nonzero v ∈ τd−2, for which hv, xti + hv, S+
d−2Ad−1,1∆xti ∼ I(d − 2). In τd−1,
P
with 0 ≤ dim τd−1 < ∞, every nonzero linear combination of xt is I(d − 1) and does not allow for

P

polynomial cointegration and in τd, with 0 < dim τd < ∞, every nonzero linear combination of xt

is I(d).

As discussed in the next remark, the only diﬀerence with the ﬁnite dimensional case, see

Franchi and Paruolo (2018), is that in that case the number of cointegrating relations of order

0 is ﬁnite.

Remark 5.8. In the ﬁnite dimensional case H = Rp, Franchi and Paruolo (2016) show that d =
1, 2, . . . if and only if Rp = ζ0 ⊕· · ·⊕ζd = τ0 ⊕· · ·⊕τd, where ζh = sp{αh}, τh = sp{βh}, h = 0, 1, . . . ,
h, where Ah,1 is as
and the bases αh, βh are deﬁned by the rank factorizations PZ ⊥
h
in Deﬁnition 5.1 with S+
h. Again here, apart from the fact that dim ζ0 = dim τ0 is ﬁnite
when H = Rp, this mirrors what happens in the inﬁnite dimensional case.

h = ¯βh ¯α′

Ah,1PT ⊥

= αhβ′

h

22

Remark 5.9. The pole(d) condition in (5.5) is equivalent to τd = (τ0⊕· · ·⊕τd−1)⊥ 6= {0}. Moreover,
Theorem B.4 in Appendix B shows that it can be equivalently stated as (i) H = ζ0 ⊕ ζ1 ⊕ · · · ⊕ ζd,
ζd 6= {0}, (ii) ζd = (ζ0 ⊕ · · · ⊕ ζd−1)⊥ 6= {0}, (iii) Im C0 = τd, (iv) Ker C0 = τ0 ⊕ · · · ⊕ τd−1.

In order to complete the discussion of the relation of the present results with the existing lit-

erature, the equivalence of the pole(d) condition in (5.5) to the condition in Hu and Park (2016)

reported in eq. (5.7) below is discussed.
Hu and Park (2016) consider xt = A◦

1xt−1 + εt with A◦
1 compact and formulate an I(d) condition
and then study the I(1) case. In order to state their I(d) condition, they employ the nonorthogonal

direct sum decomposition H = HP ⊕ HT , where HP is the ﬁnite dimensional image of the Riesz

projection associated with the isolated eigenvalue z = 1 and HT is the inﬁnite dimensional image

of the Riesz projections associated with the remaining stable eigenvalues. Using the nonorthogonal

projections associated to the nonorthogonal direct sum decomposition H = HP ⊕ HT , they decom-
t = AX xX
pose the process into xt = xP
t ∈ HX and AX is the restriction of A◦
1
to HX, X = T, P . Their I(d) condition is stated as

t , where xX

t + εX

t + xT

AP − I is a nilpotent matrix of order d,

(5.7)

i.e. (AP − I)d−1 6= 0 and (AP − I)d = 0, which simpliﬁes to AP = I in the I(1) case studied in that
paper.

Proposition 5.10. Let A(L)xt = εt be an AR with a unit root of ﬁnite type; then the I(d) condition
in (5.7) is equivalent to the pole(d) condition in (5.5).

6. Conclusion

The present paper characterizes the cointegration properties of ARs with a unit root of ﬁnite

type, i.e. H-valued AR processes A(L)xt = εt such that A(z) has an eigenvalue of ﬁnite type at

z = 1 and it is invertible in the punctured disc D(0, ρ) \ {1} for some ρ > 1. It is shown that ARs

with a unit root of ﬁnite type are necessarily integrated of ﬁnite order d and necessarily have a

ﬁnite number of I(d) trends and an inﬁnite dimensional cointegrating space. This is in line with

the setup employed in most contributions in the literature and seems to be the most empirically

relevant framework.

A necessary and suﬃcient condition on the AR operators that establishes the value of d is given

in terms of the orthogonal direct sum decomposition H = τ0 ⊕ τ1 ⊕ · · · ⊕ τd, τd 6= {0}, where τ0 is

inﬁnite dimensional, 0 ≤ dim τh < ∞, h = 1, . . . , d − 1, with strict inequality for h = d.

A full description of how the properties of the characteristic hv, xti vary with v ∈ H is given: in

τ0, one can combine hv, xti with diﬀerences of the process and ﬁnd at most I(0) polynomial cointe-

grating relations, in τ1, one can combine hv, xti with diﬀerences and ﬁnd at most I(1) polynomial

cointegrating relations, and so on up to τd−2, in which one can combine hv, xti with diﬀerences and

ﬁnd at most I(d − 2) polynomial cointegrating relations. Finally, any nonzero v ∈ τd−1 is such that

23

hv, xti is I(d − 1) and does not allow for polynomial cointegration and any nonzero v ∈ τd is such

that hv, xti is I(d). This shows that the inﬁnite dimensional subspace τ0 ⊕ τ1 ⊕ · · · ⊕ τd−1 is the

cointegrating space while the ﬁnite dimensional subspace τd is the attractor space.

For any nonzero v in the cointegrating space, the expression of the polynomial cointegrating

relations is provided in terms of operators that are deﬁned recursively in terms of the AR operators

together with the τh.

The present results show that, under the assumption that 1 is an eigenvalue of ﬁnite type of

the AR operator function, the inﬁnite dimensionality of the space does not introduce additional

elements in the analysis. That is, apart from the fact that the number of I(0) cointegrating relations

is inﬁnite, conditions and properties of H-valued AR processes coincide with those that apply in

the usual ﬁnite dimensional VAR case.

24

Appendix A. Notation and background results

In the present paper H is an inﬁnite dimensional separable Hilbert space and a random variable

that takes values in H is said to be an H-valued random variable and a sequence of H-valued

random variables is called an H-valued stochastic process. Section A.1 reviews notions and results

on separable Hilbert spaces and on operators acting on them and Section A.2 presents the deﬁnitions

of expectation and covariance operator for H-valued random variables.

A.1. Separable Hilbert spaces and operators acting on them. The material in this section

is based on Chapters I, II in Gohberg et al. (2003) and Chapter XI in Gohberg et al. (1990). Let

1
H be a separable Hilbert space with inner product h · , · i and norm kxk = hx, xi
2 ; a function
A : H → H, is called a linear operator if for all v, w ∈ H and c ∈ C, A(v + w) = Av + Aw and

A(cv) = cAv, where Au and A[u] both indicate the action of A on u ∈ H. A linear operator A is

called bounded if its norm kAkLH = supkvk=1 kAvk is ﬁnite and the set of bounded linear operators
with norm k · kLH is denoted as LH. For any A ∈ LH the subspace {v ∈ H : Av = 0}, written

Ker A, is called the kernel of A and the subspace {Av : v ∈ H}, written Im A, is called the image

of A. The dimension of Im A, written dim Im A, is called the rank of A, written rank A.

H is said to be the direct sum of subspaces S and U , written H = S ⊕ U , if S ∩ U = 0 and every

vector v ∈ H can be written as v = s + u, where s ∈ S and u ∈ U . The set {v ∈ H : hv, si =
0 for all s ∈ S ⊆ H} is called the orthogonal complement of S, written S⊥. For U = S⊥, one has
the orthogonal direct sum H = S ⊕ S⊥. The orthogonal projection on η, written Pη, is such that
Pη ∈ LH, P 2

η = Pη, Im Pη = η and Ker Pη = η⊥; moreover, I = Pη + Pη⊥ .

An operator A ∈ LH is said to be invertible if there exists an operator B ∈ LH such that
BAv = ABv = v for every v ∈ H; in this case B is called the inverse of A, written A−1. An
operator A ∈ LH such that n(A) = dim Ker A < ∞ and d(A) = dim(Im A)⊥ < ∞ is said to be
Fredholm of index n(A) − d(A). Remark that if H is ﬁnite dimensional, any A ∈ LH is Fredholm

of index 0.

Corollary 8.4 in Section XI.8 in Gohberg et al. (1990) states that the inverse of an operator

function that is Fredholm of index 0 and non-invertible at some isolated point has a pole at that

point. Moreover, the operators that make up the principal part of its Laurent representation around

that point have ﬁnite rank. If z0 is an eigenvalue of ﬁnite type of W (z), see Deﬁnition 3.1, then
z0 is an isolated singularity of W (z)−1, W (z0) is Fredholm of index 0 and non-invertible at z0, so
that Theorem A.1 below applies.

Theorem A.1. Let z0 be an eigenvalue of ﬁnite type of an operator function W (z). Then there
exist a ﬁnite integer d = 1, 2, . . . and ﬁnite rank operators U0, U1, . . . , Ud−1 such that

W (z)−1 =

where Ud is Fredholm of index 0.

∞

n=0
X

Un(z − z0)n−d,

z ∈ D(z0, δ) \ {z0},

Proof. See Section XI.9 in Gohberg et al. (1990).

25

(cid:4)

A.2. Random variables in separable Hilbert spaces. The deﬁnitions in this section are taken

from Chapter 1 in Bosq (2000). Let H be a separable Hilbert space with inner product h · , · i, norm

kwk = hw, wi

1
2 , and Borel σ-algebra σ(H) and let (Ω, A, P ) be a probability space. A function

Z : Ω → H is called an H-valued random variable on (Ω, A, P ) if it is measurable, i.e. for every
subset S ∈ σ(H), {ω : Z(ω) ∈ S} ∈ A. For a C-valued random variable X on (Ω, A, P ), deﬁne

E(X) =

Ω X(ω)dP (ω); the expectation of an H-valued random variable Z, written E(Z), is deﬁned

as the unique element µ of H such that

R

E(hv, Zi) = hv, µi for all v ∈ H.

It can be shown that the existence of E(Z) is guaranteed by the condition E(kZk) < ∞. The

covariance function of an H-valued random variable Z is deﬁned as

cZ (v, w) = E(hv, Z − E(Z)ihw, Z − E(Z)i),

v, w ∈ H.

It is immediate to see that cZ (v, w) = E(hv, W i) − hv, E(Z)ihw, E(Z)i, where W = hw, ZiZ. If

E(kW k) < ∞, the expectation of the H-valued random variable W exists and it is the unique

element of H such that E(hv, W i) = hv, E(W )i for all v ∈ H. One thus has

cZ (v, w) = hv, E(W )i − hv, E(Z)ihw, E(Z)i,

v, w ∈ H,

W = hw, ZiZ.

Because kW k = |hw, Zi|kZk ≤ kwkkZk2, the existence of the covariance function of Z is guaranteed
by the condition E(kZk2) < ∞. Deﬁne the operator CZ : H → H that maps w into E(W ) and
rewrite the covariance function as cZ (v, w) = hv, CZ wi − hv, E(Z)ihw, E(Z)i, v, w ∈ H. CZ is fully

determined by the covariance function and it is called the covariance operator of Z. Similarly, the

cross-covariance function of two H-valued random variables Z and U is deﬁned as

cZ,U (v, w) = E(hv, Z − E(Z)ihw, U − E(U )i),

v, w ∈ H.

This also completely determines the cross-covariance operators of Z and U , CZ,U and CU,Z , respec-

tively deﬁned as the mappings w 7→ E(hw, ZiU ) and w 7→ E(hw, U iZ).

Appendix B. Inversion of an operator function around a singular point

This Appendix presents novel results on the inversion of a meromorphic operator function which

are used in Appendix C to prove the results in the text.

The inversion results are derived from system (B.1) below, see e.g. Howlett et al. (2009). When
the inverse A(z)−1 has a pole of order d from the identity A(z)A(z)−1 = I = A(z)−1A(z) one ﬁnds

26

the following linear systems in the An, Cn operators deﬁned in (3.2) and (3.3),

A0C0 = 0 = C0A0

A0C1 + A1C0 = 0 = C0A1 + C1A0
...

A0Cd−1 + · · · + Ad−1C0 = 0 = C0Ad−1 + · · · + Cd−1A0

(B.1)

A0Cd + A1Cd−1 + · · · + AdC0 = I = C0Ad + C1Ad−1 + · · · + CdA0

A0Cd+1 + A1Cd + · · · + Ad+1C0 = 0 = C0Ad+1 + C1Ad + · · · + Cd+1A0
...

In the following, equations in system (B.1) are numbered according to the highest value of the

subscript of Cn. Note that the identity appears in equation d, which is the order of the pole. The
equations that derive from A(z)A(z)−1 = I are called left versions (and correspond to the left side
of (B.1)) and those that derive from I = A(z)−1A(z) are called right versions (and correspond to

the right side of (B.1)). For instance A0Cd + A1Cd−1 + · · · + AdC0 = I is called the left version of

equation d.

Recall that Pη ∈ LH indicates the orthogonal projection on η and A+ and A∗ respectively denote

the generalized inverse and the adjoint of A.

Lemma B.1. Consider Deﬁnition 5.1. Then Ker S+

h = (Im Sh)⊥, S+

h Sh = Pτh and S+

h PZ ⊥

= S+
h ,

h

h = 0, 1, . . . , d.

Proof. From Theorem 3 in Ben-Israel and Greville (2003, Chapter 9), one has S+

and Ker S+
h = Ker S∗
(Ker Sh)⊥ and Ker S∗
and hence S+
hence Zh ⊆ Ker S+

h and from Theorem 11.4 in Gohberg et al. (2003, Chapter II) one has Im S∗
h = (Im Sh)⊥, so that Ker S+

h Sh = PIm S∗
h =
h = (Im Sh)⊥. By Deﬁnition 5.1, (Ker Sh)⊥ = τh
h ⊇ ζ0 ⊕ · · · ⊕ ζh−1 = Zh and
(cid:4)

h Sh = Pτh. Moreover, by Deﬁnition 5.1, (Im Sh)⊥ = ζ ⊥
h = S+

h , which implies S+

h PZ ⊥

.

h

h

Lemma B.2 (Subspace decompositions of system (B.1)). Consider Deﬁnition 5.1 and further

deﬁne PZ ⊥

0

= PT ⊥
0

= I. Then the left version of equation n + h ≤ d in system (B.1) implies

ShCn + PZ ⊥
h

n

Xk=1

Ah+1,kCn−k = δn+h,dPZ ⊥

h

,

h = 0, 1, . . . , d − n,

(B.2)

where δhj is the Kronecker delta. Similarly, the right version of equation n + h ≤ d in system (B.1)
implies

CnSh +

n

Xk=1

Cn−kAh+1,kPT ⊥

h

= δn+h,dPT ⊥

h

,

h = 0, 1, . . . , d − n.

(B.3)

Proof. The proof of (B.2) is by induction and consists in showing that the left version of equation

27

n ≤ d in system (B.1) implies

ShCn−h + PZ ⊥

h

n−h

Xk=1

Ah+1,kCn−h−k = δn,dPZ ⊥

h

,

h = 0, 1, . . . , n;

(B.4)

replacing n with n + h one ﬁnds (B.2). In order to show that (B.4) holds for h = 0, observe that

the left version of equation n in system (B.1) reads A0Cn +

n
k=1 AkCn−k = δn,dI. By deﬁnition,
= I, S0 = A0 and A1,k = Ak and this shows that (B.4) holds for h = 0. Next assume that

P

PZ ⊥
0

(B.4) holds for h = 0, . . . , ℓ − 1 for some 1 < ℓ ≤ d; one wishes to show that it also holds for h = ℓ.
First note that S+

h , see Lemma B.1; thus the induction assumption

h Sh = Pτh and S+

h PZ ⊥

= S+

h

implies

PτhCn−h + S+
h

n−h

Xk=1

Ah+1,kCn−h−k = δn,dS+
h ,

h = 0, 1, . . . , ℓ − 1,

and replacing n with n − ℓ + h and h with i, one has

PτiCn−ℓ = −S+
i

n−ℓ

Xk=1

Ai+1,kCn−ℓ−k + δn−ℓ+i,dS+
i ,

i = 0, 1, . . . , ℓ − 1.

Observe that for i = 0, 1, . . . , ℓ − 1 one has n − ℓ + i ≤ n − 1 < d; hence δn−ℓ+i,d = 0 and one ﬁnds

PτiCn−ℓ = −S+
i

n−ℓ

Xk=1

Next write (B.4) for h = ℓ − 1,

Ai+1,kCn−ℓ−k,

i = 0, 1, . . . , ℓ − 1.

(B.5)

Sℓ−1Cn−ℓ+1 + PZ ⊥
ℓ−1

n−ℓ+1

Xk=1

Aℓ,kCn−ℓ+1−k = δn,dPZ ⊥
ℓ−1

,

where Im Sℓ−1 = ζℓ−1, see Deﬁnition 5.1; applying PZ ⊥

ℓ

, where Zℓ = ζ0 ⊕ · · · ⊕ ζℓ−1, one has

PZ ⊥
ℓ

Sℓ−1 = 0 and rearranging one ﬁnds

PZ ⊥
ℓ

Aℓ,1Cn−ℓ + PZ ⊥

ℓ

n−ℓ

Xk=1

Aℓ,k+1Cn−ℓ−k = δn,dPZ ⊥

ℓ

.

(B.6)

Next consider Tℓ = τ0 ⊕ · · · ⊕ τℓ−1 and use projections, inserting I = PT ⊥
Cn−ℓ in PZ ⊥

Aℓ,1Cn−ℓ = U , say; one ﬁnds

ℓ

ℓ

+ PTℓ between Aℓ,1 and

U =

PZ ⊥
ℓ

Aℓ,1PT ⊥

ℓ

Cn−ℓ + PZ ⊥

ℓ

Aℓ,1PTℓCn−ℓ = U1 + U2, say.

By Deﬁnition 5.1, PZ ⊥
ℓ

(cid:16)
Aℓ,1PT ⊥

ℓ

in U2, one has U2 = PZ ⊥
ℓ

Aℓ,1

(cid:17)

= Sℓ, so that U = SℓCn−ℓ +U2. Substituting PTℓ = Pτ0 +· · ·+Pτℓ−1
ℓ−1
i=0 PτiCn−ℓ and by the induction assumption, see (B.5), one ﬁnds

P

n−ℓ

ℓ−1

U2 = −PZ ⊥
ℓ

Aℓ,1

S+

i Ai+1,k

i=0
X

Xk=1  

Cn−ℓ−k.

!

28

Substituting the expression of U2 into U = SℓCn−ℓ+U2 and using Aℓ+1,k = Aℓ,k+1−Aℓ,1

see Deﬁnition 5.1, one hence rewrites (B.6) as

n−ℓ

ℓ−1
i=0 S+

i Ai+1,k,

P

SℓCn−ℓ + PZ ⊥

ℓ

Aℓ+1,kCn−ℓ−k = δn,dPZ ⊥

ℓ

.

Xk=1

This shows that (B.4) holds for h = ℓ and completes the proof of (B.2). A similar induction on the
(cid:4)

right version of system (B.1) leads to (B.3).

Lemma B.3. Consider Deﬁnition 5.1. Then Im C0 ⊆ T ⊥

d and Zd ⊆ Ker C0.

Proof. For n = 0, (B.2) and (B.3) read

ShC0 = δh,dPZ ⊥
h

,

C0Sh = δh,dPT ⊥
h

h = 0, 1, . . . , d,

(B.7)

where Sh = PZ ⊥
h

Ah,1PT ⊥

h

, see Deﬁnition 5.1. (B.7) implies ShC0 = C0Sh = 0 for h = 0, 1, . . . , d −

1 ∩ · · · ∩ τ ⊥
d−1

1. From ShC0 = 0, h = 0, 1, . . . , d − 1, one has Im C0 ⊆ Ker Sh for h = 0, 1, . . . , d − 1, i.e.
Im C0 ⊆ (Ker S0 ∩ Ker S1 ∩ · · · ∩ Ker Sd−1). By Deﬁnition 5.1, Ker Sh = τ ⊥
h and hence Im C0 ⊆
= (τ0 ⊕ τ1 ⊕ · · · ⊕ τd−1)⊥ = T ⊥
0 ∩ τ ⊥
τ ⊥
From C0Sh = 0, h = 0, 1, . . . , d − 1, one has Im Sh ⊆ Ker C0 for h = 0, 1, . . . , d − 1, i.e.

(cid:0)
(Im S0 ⊕ Im S1 ⊕ · · · ⊕ Im Sd−1) ⊆ Ker C0. By Deﬁnition 5.1, Im Sh = ζh and hence ζ0 ⊕ ζ1 ⊕
(cid:4)
· · · ⊕ ζd−1 = Zd ⊆ Ker C0.

d . This proves the ﬁrst statement.

(cid:1)

Theorem B.4 (Order of the pole). Consider Deﬁnition 5.1. The following statements are equiva-

lent:

(i) A(z)−1 has a pole of order d at z = 1,

(ii) the identity is in equation d of system (B.1),
(iii) ζd = (ζ0 ⊕ · · · ⊕ ζd−1)⊥ 6= {0},
(iv) Ker C0 = ζ ⊥
d ,
(v) τd = (τ0 ⊕ · · · ⊕ τd−1)⊥ 6= {0},
(vi) Im C0 = τd.

Proof.

(i) ⇔ (ii) By deﬁnition.

d and because Im Sd ⊂ Z ⊥
d

6= {0}; by Deﬁnition 5.1, Im Sd ⊆ Z ⊥

d . By Deﬁnition 5.1, Im Sd = ζd and Z ⊥

(ii) ⇒ (iii) ⇒ (iv). Under (ii), one has h = d in the left equation in (B.7), i.e. SdC0 = PZ ⊥
d
Z ⊥
contradicts SdC0 = PZ ⊥
,
d
d
d = (ζ0 ⊕ · · · ⊕ ζd−1)⊥, and hence (iii).
, one

one has Im Sd = Z ⊥
Moreover, by Lemma B.3, Zd ⊆ Ker C0 and because Zd ⊂ Ker C0 contradicts SdC0 = PZ ⊥
has Zd = Ker C0. Using Zd = ζ ⊥
(iv) ⇒ (ii). Let Ker C0 = ζ ⊥
d and proceed by contradiction, assuming that the identity is not in
equation d, so that the right equation in (B.7) reads C0Sd = 0, which implies Im Sd ⊆ Ker C0,
where Im Sd = ζd and Ker C0 = ζ ⊥
d = H. This
contradicts C0 6= 0, i.e. that the pole has order d, and proves that (ii) holds.

d , so that ζd = {0} and thus ζ ⊥

d , see (iii), one ﬁnds (iv).

d . Hence ζd ⊆ ζ ⊥

,

d

29

(ii) ⇒ (v) ⇒ (vi). Under (ii), one has h = d in the right equation in (B.7), i.e. C0Sd = PT ⊥
d
T ⊥
6= {0}; by Deﬁnition 5.1, Td ⊆ Ker Sd and because Td ⊂ Ker Sd contradicts C0Sd = PT ⊥
,
d
d and Td = τ0 ⊕ · · · ⊕ τd−1, and hence (v).
, one

one has Td = Ker Sd. By Deﬁnition 5.1, Ker Sd = τ ⊥
Moreover, by Lemma B.3, Im C0 ⊆ T ⊥
d . Using T ⊥
has Im C0 = T ⊥
(vi) ⇒ (ii). Let Im C0 = τd and proceed by contradiction, assuming that the identity is not in

d and because Im C0 ⊂ T ⊥
d

d = τd, see (v), one ﬁnds (vi).

contradicts C0Sd = PT ⊥
d

,

d

equation d, so that the left equation in (B.7) reads SdC0 = 0, which implies Im C0 ⊆ Ker Sd, where
Im C0 = τd and Ker Sd = τ ⊥
d , so that τd = {0}. This contradicts C0 6= 0, i.e. that
(cid:4)
the pole has order d, and proves that (ii) holds.

d . Hence τd ⊆ τ ⊥

Theorem B.5 (Pole cancellations in A(z)−1). Consider Deﬁnition 5.1 and for h = 0, 1, . . . , d

deﬁne

γh(z) = Pτh + S+
h

Ah+1,n(1 − z)n.

d−h−1

Then hv, γh(z)A(z)−1yi has a pole of order h = 0, 1, . . . , d for any nonzero v ∈ τh and y ∈ H.

n=1
X

Proof. Applying S+

h to (B.2) and using S+

h Sh = Pτh and S+

h PZ ⊥

= S+

h , see Lemma B.1, one

h

ﬁnds

n

PτhCn + S+
h

Ah+1,kCn−k = δn+h,dS+
h ,

h = 0, 1, . . . , d − n.

(B.8)

Write A(z)−1 =

Xk=1

∞
n=0 Cn(1 − z)n−d as
d−h−1

P

A(z)−1 = C0(1 − z)−d +

Cn(1 − z)n−d + (1 − z)−hR0(z),

R0(1) = Cd−h,

and apply Pτh to ﬁnd

n=1
X

d−h−1

PτhA(z)−1 = PτhC0(1 − z)−d +

PτhCn(1 − z)n−d + (1 − z)−hPτhR0(z).

n=1
X

First consider h = 0, . . . , d − 1. Setting n = 0 in (B.8) one has PτhC0 = 0 and hence

d−h−1

Pτh A(z)−1 =

PτhCn(1 − z)n−d + (1 − z)−hPτhR0(z).

(B.9)

From (B.8), for n ≤ d − h one has PτhCn = −S+
h
δn+h,d = 0 for n = 1, . . . , d − h − 1, one has

n=1
X

d−h−1

n=1
X

Pτh Cn(1 − z)n−d = −

d−h−1

S+
h

n=1  
X

Rearraging one thus ﬁnds

n
k=1 Ah+1,kCn−k + δn+h,dS+

h and because

P

n

Xk=1

Ah+1,kCn−k

!

(1 − z)n−d.

d−h−1

n=1
X

PτhCn(1 − z)n−d = −S+
h

Ah+1,k

Cn−k(1 − z)n−d

d−h−1

d−h−1

Xk=1

Xn=k

.

!

 
30

Next write

(1 − z)kA(z)−1 =

d−h−1

Xn=k

Cn−k(1 − z)n−d

!

+ (1 − z)−hRk(z),

Rk(1) = Cd−h−k,

so that

d−h−1

d−h−1

PτhCn(1 − z)n−d = −

S+
h

Ah+1,k(1 − z)k

n=1
X

Xk=1

Substituting in (B.9) and rearraging one thus ﬁnds

A(z)−1 + (1 − z)−hS+
h

!

d−h−1

Xk=1

Ah+1,kRk(z).

γh(z)A(z)−1 = (1 − z)−h

γh(z),

where

e

γh(z) = Pτh + S+
h

Ah+1,k(1 − z)k,

γh(z) = PτhR0(z) + S+
h

d−h−1

Note that, because Rk(1) = Cd−h−k, one has

Xk=1

γh(1) = PτhCd−h + S+
h

e

d−h−1

Xk=1

Ah+1,kCd−h−k;

d−h−1

Xk=1

Ah+1,kRk(z).

e

h Sh = Pτh and C0Sh = 0 one ﬁnds

d−h
k=1 Ah+1,kCd−h−k = S+

h , so that

γh(1) =

from (B.8) for n = d − h one ﬁnds PτhCd−h + S+
h
h (I − Ah+1,d−hC0). Using S+
S+
hv,

γh(1)Sh = Pτh. This shows that
γh(1)yi 6= 0 for any nonzero v ∈ τh and any nonzero y ∈ H and hence hv, γh(z)A(z)−1yi has a
pole of order h for any nonzero v ∈ τh, h = 0, . . . , d − 1, and any nonzero y ∈ H. Finally consider
h = d. Setting n = 0 and h = d in (B.8) one has PτdC0 = S+
d Sd = Pτd one ﬁnds
PτdC0Sd = Pτd; this shows that hv, C0yi 6= 0 for any nonzero v ∈ τd and any nonzero y ∈ H. Hence
hv, A(z)−1yi has a pole of order d for any nonzero v ∈ τd and any nonzero y ∈ H. This completes
(cid:4)
the proof.

d and using S+

P

e

e

e

Appendix C. Proofs

This Appendix contains the proofs of the results in the text. The proof Theorem 3.5 makes use

of the following fact, which is proven in Franchi and Paruolo (2018): for t ∈ Z, one has

S s∆hut = S s−hut −

ςn,t∆h−s+nu0,

0 < h ≤ s,

(C.1)

s−1

where ςn,t is a polynomial of order n in t.

Xn=s−h

Proof of Theorem 3.5. The result is a direct consequence of Theorem A.1 in Appendix A.1. By

deﬁnition, an AR with a unit root of ﬁnite type A(L)xt = εt is such that A(1) 6= 0, A(z) has an

eigenvalue of ﬁnite type at z = 1 and A(z) is invertible in the punctured disc D(0, ρ) \ {1} for some

 
 
ρ > 1. Letting z0 = 1 and Cn = Un(−1)n−d, Theorem A.1 states that there exist a ﬁnite integer
d = 1, 2, . . . and ﬁnite rank operators C0, C1, . . . , Cd−1 such that

∞

A(z)−1 =

Cn(1 − z)n−d,

z ∈ D(1, δ) \ {1}.

(C.2)

n=0
X

31

P

d−1
n=0 Cn(1 − z)n−d + C ⋆

d (z), where C ⋆
Write A(z)−1 =
d (z) is absolutely convergent in D(0, ρ)
for some ρ > 1; applying A(L)−1 on both sides of A(L)xt = εt one ﬁnds the common trends
representation xt = C0sd,t + C1sd−1,t + · · · + Cd−1s1,t + yt + µt, where sh,t = S hεt ∼ I(h) is the
d−1
n=0 vntn, vn ∈ H,
h-fold integrated bilateral random walk, yt = C ⋆
is a polynomial of time, with coeﬃcients v0, . . . , vd−1 ∈ H, depend on the initial values of xt, yt, εt
(cid:4)

d (L)εt is a linear process, µt =

for t = −d, . . . , 0, see (C.1).

P

Proof of Corollary 3.6. The order d of the pole of the inverse of A(z) is ﬁnite by Theorem A.1 in

Appendix A.1; this implies xt ∼ I(d) via (3.6). The I(d) trends sd,t are loaded onto xt by C0, which
has ﬁnite rank, implication (iii), and hence any nonzero v ∈ (Im C0)⊥ is such that hv, C0yi = 0
for all y ∈ H. This shows that xt is cointegrated, which is implication (ii). Because H is inﬁnite
(cid:4)
dimensional, one has (Im C0)⊥ is also inﬁnite dimensional, which is implication (iv).

Proof of Proposition 3.7. First note that for d = 1, Theorem 3.5 and Corollary 3.6 imply
∞
n=0 Bnzn is absolutely convergent on D(0, ρ), ρ > 1, B(1) = C0 6= 0
and Im B(1) = Im C0 is ﬁnite dimensional. Because B(z) is inﬁnitely diﬀerentiable on D(0, ρ),

∆xt = B(L)εt, where B(z) =

P

ρ > 1, the series obtained by termwise diﬀerentiation coincides with the ﬁrst derivative of B(z) for
(cid:4)

each z ∈ D(0, ρ), and hence one has

∞
n=1 nkBnkLH < ∞.

Proof of Proposition 3.8. Because the sum of compact operators is compact, see Theorem 16.1

P

in Gohberg et al. (2003, Chapter II), and if K is compact then I − K is Fredholm of index 0, see

Theorem 4.2 in Gohberg et al. (2003, Chapter XV), then A0 = I −

n is Fredholm of index
0. Because A0 is non-invertible, by the Fredholm alternative there exist for nonzero v ∈ H such

k
n=1 A◦

P

that A0v = 0, see Theorem 4.1 in Gohberg et al. (2003, Chapter XIII). Finally, since z = 1 is
assumed to be the only isolated singularity of A(z)−1 within D(0, ρ), ρ > 1, this shows that z = 1
(cid:4)

is a eigenvalue of ﬁnite type of A(z).

Proof of Theorem 4.1. Set d = 1 in Theorem 5.3.

(cid:4)

Proof of Proposition 4.4. When k = 1, (4.6) reads H = Im A0 ⊕ Ker A0. By assumption, Ker A0
has ﬁnite dimension; hence H = Im A0 ⊕ Ker A0 implies that (Im A0)⊥ has ﬁnite dimension equal
to dim Ker A0. This shows that A0 is Fredholm of index 0. Because dim Ker A0 > 0, there exist for

nonzero v ∈ H such that A0v = 0 and since z = 1 is assumed to be the only isolated singularity of
(cid:4)
A(z)−1 within D(0, ρ), ρ > 1, this shows that z = 1 is a eigenvalue of ﬁnite type of A(z).

Proof of Proposition 4.5. The notation ζ0 = Im A0 and τ0 = (Ker A0)⊥, see (4.1), is employed.
0 , where by assumption of AR with a unit root

In the present notation, (4.6) reads H = ζ0 ⊕ A1τ ⊥

32

of ﬁnite type, see Remark 3.2, one has 0 < dim τ ⊥

0 = dim ζ ⊥

0 < ∞. Observe that (4.3) is equivalent

to A0C1 + A1C0 = I, see (ii) in Theorem B.4 in Appendix B.

(4.3) ⇒ (4.6). Let (4.3) hold, which is equivalent to A0C1 + A1C0 = I by Theorem B.4. This

0 because Im C0 = τ ⊥

implies that for any v ∈ H one has v = u + s, where u = A0C1v ∈ ζ0 because Im A0 = ζ0 and
s = A1C0v ∈ A1τ ⊥
0 , see (v) and (vi) in Theorem B.4. In order to show that
ζ0 ∩ A1τ ⊥
0 = {0}, note that Ker C0 = ζ0, see (iii) and (iv) in Theorem B.4. This implies that for
v 6= 0. However, s 6= 0 belongs to ζ0
any v ∈ ζ0 one has s = A1C0v = 0, i.e. s 6= 0 implies Pζ ⊥
if and only if Pζ ⊥
s = Pζ ⊥
A1C0 = Pζ ⊥
Pζ ⊥
that there does not exist s 6= 0 that belongs to ζ0, i.e. ζ0 ∩ A1τ ⊥
(4.6) ⇒ (4.3). Assume that H = ζ0 ⊕ A1τ ⊥
nonzero v ∈ H such that v = u + s = 0, where u = A0C1v ∈ ζ0 and s = A1C0v ∈ A1τ ⊥
contradiction and hence (4.3) holds.

0 ; because A0C1 + A1C0 = 0 implies that there exists a
0 , this is a
(cid:4)

0
v 6= 0 which gives a contradiction. This shows

on both sides of A0C1 + A1C0 = I one gets

0 = {0}, so that (4.6) holds.

s = Pζ ⊥
; hence Pζ ⊥

A1C0v = 0. Applying Pζ ⊥

A1C0v = Pζ ⊥

0

0

0

0

0

0

0

0

Proof of Theorem 4.6. Set d = 2 in Theorem 5.3.

(cid:4)

Proof of Theorem 5.3. The proof makes use of Theorem B.4 in Appendix B, which establishes

the order of integration of the process, and Theorem B.5 in Appendix B, which describes the pole
cancellations that give rise to cointegration. By Theorem B.4 one has A(z)−1 has a pole of order
d at z = 1, i.e. xt ∼ I(d), if and only if Im C0 = τd, where τd = (τ0 ⊕ · · · ⊕ τd−1)⊥ 6= {0} . The
common trends representation is found in (3.6); because Im C0 = τd and τd = (τ0 ⊕ · · · ⊕ τd−1)⊥ 6=
{0}, this shows that hv, xti ∼ I(d) for any nonzero v ∈ τd, so that τd is the ﬁnite dimensional

attractor space and τ0 ⊕ · · · ⊕ τd−1 is the inﬁnite dimensional cointegrating space. Finally, applying
d−h−1
Theorem B.5 one has that hv, γh(z)A(z)−1yi, where γh(z) = Pτh + S+
n=1 Ah+1,n(1 − z)n, has
h
a pole of order h = 0, 1, . . . , d for any nonzero v ∈ τh and any nonzero y ∈ H. This shows that
(cid:4)

h Ah+1,n∆nxti ∼ I(h) for any nonzero v ∈ τh, h = 0, 1, . . . , d.

hv, xti +

d−h−1
n=1

hv, S+

P

Proof of Proposition 5.10. AP − I is a nilpotent matrix of order d if and only if the largest

P

Jordan block of AP with eigenvalue 1 has dimension d, see e.g. Horn and Johnson (2013, p. 181).

In section Section 4.4 of Franchi and Paruolo (2018), it is proved that the size of the largest Jordan
(cid:4)

block of a matrix is equal to d if and only if the pole(d) condition holds.

References

Beare, B. (2017). The Chang-Kim-Park model of cointegrated density-valued time series cannot

accommodate a stochastic trend. Econ Journal Watch 14, 133–137.

Beare, B., J. Seo, and W. Seo (2017). Cointegrated Linear Processes in Hilbert Space. Journal of

Time Series Analysis 38, 1010–1027.

Beare, B. and W. Seo (2018a). Cointegrated linear processes in Bayes Hilbert space. Mimeo,

University of California San Diego.

33

Beare, B. and W. Seo (2018b). Representation of I(1) and I(2) autoregressive Hilbertian processes.

Arxiv e-print, arXiv:1701.08149v2 [math.ST].

Ben-Israel, A. and T. Greville (2003). Generalized Inverses: Theory and Applications, 2nd ed.

Springer.

Bosq, D. (2000). Linear Processes in Function Spaces. Springer-Verlag.

Bourguignon, F., F. H. Ferreira, and N. Lustig (2005). The Microeconomics of Income Distribution

Dynamics in East Asia and Latin America. Washington, DC: World Bank and Oxford University

Press; available online under License: CC BY 3.0 IGO.

Chang, Y., B. Hu, and J. Park (2016a). On the Error Correction Model for Functional Time Series

with Unit Roots. Mimeo, Indiana University.

Chang, Y., C. Kim, and J. Park (2016b). Nonstationarity in time series of state densities. Journal

of Econometrics 192, 152 – 167.

Cochrane, J. H. and M. Piazzesi (2005). Bond risk premia. American Economic Review 95 (1),

138–160.

Franchi, M. and P. Paruolo (2016). Inverting a matrix function around a singularity via local rank

factorization. SIAM Journal of Matrix Analysis and Applications 37, 774–797.

Franchi, M. and P. Paruolo (2018). A general inversion theorem for cointegration. Econometric

Reviews, forthcoming.

Gabrys, R., S. H¨ormann, and P. Kokoszka (2013). Monitoring the Intraday Volatility Pattern.

Journal of Time Series Econometrics 5, 87–116.

Gohberg, I., S. Goldberg, and M. Kaashoek (1990). Classes of linear operators, vol. 1. Operator

theory. Birkh¨auser Verlag.

Gohberg, I., S. Goldberg, and M. Kaashoek (2003). Basic Classes of Linear Operators. Birkh¨auser,

Basel-Boston-Berlin.

Gregoir, S. (1999). Multivariate time series with various hidden unit roots, Part I. Econometric

Theory 15, 435–468.

H¨ormann, S., L. Horv´ath, and R. Reeder (2013). A functional version of the ARCH model. Econo-

metric Theory 29, 267–288.

H¨ormann, S. and P. Kokoszka (2012). Functional time series. Handbook of Statistics 30, 157–186.

Horn, R. and C. Johnson (2013). Matrix Analysis. Cambridge University Press.

Horv´ath, L. and P. Kokoszka (2012). Inference for Functional Data with Application. Springer.

Howlett, P., K. Avrachenkov, C. Pearce, and V. Ejov (2009). Inversion of analytically perturbed

linear operators that are singular at the origin. Journal of Mathematical Analysis and Applica-

tions 353, 68–84.

Hu, B. and J. Park (2016). Econometric Analysis of Functional Dynamics in the Presence of

Persistence. Mimeo, Indiana University.

34

Johansen, S. (1996). Likelihood-based Inference in Cointegrated Vector Auto-Regressive Models.

Oxford University Press.

Kargin, V. and A. Onatski (2008). Curve forecasting by functional autoregression. Journal of

Multivariate Analysis 99, 2508 – 2526.

Kokoszka, P. and M. Reimherr (2017). Introduction to Functional Data Analysis. Chapman and

Hall.

Petersen, A. and H.-G. M¨uller (2016). Functional data analysis for density functions by transfor-

mation to a Hilbert space. Ann. Statist. 44 (1), 183–218.

Phillips, P. (1991). Optimal inference in cointegrated systems. Econometrica 59, 283–306.

Piketty, T. (2014). Capital in the Twenty-First Century. London: The Belknap Press of Harvard

University Press.

Stock, J. and M. Watson (1993). A simple estimator of cointegrating vectors in higher order

integrated systems. Econometrica 61, 783–820.

