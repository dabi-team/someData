2
2
0
2

b
e
F
8

]

C
H
.
s
c
[

1
v
1
4
1
4
0
.
2
0
2
2
:
v
i
X
r
a

Utility of Optical See-Through Head Mounted Displays in Augmented
Reality-Assisted Surgery: A systematic review

Manuel Birlo a, P.J. “Eddie” Edwards a Matthew Clarkson a Danail Stoyanov a

a Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), UCL, Charles Bell House, 43-45 Foley Street,
London W1W 7TS, UK

Abstract

This article presents a systematic review of optical see-through head mounted display (OST-HMD) usage in augmented
reality (AR) surgery applications from 2013 to 2020. Articles were categorised by: OST-HMD device, surgical speciality,
surgical application context, visualisation content, experimental design and evaluation, accuracy and human factors of
human-computer interaction. 91 articles fulﬁlled all inclusion criteria. Some clear trends emerge. The Microsoft HoloLens
increasingly dominates the ﬁeld, with orthopaedic surgery being the most popular application (28.6%). By far the most
common surgical context is surgical guidance (n=58) and segmented preoperative models dominate visualisation (n =
40). Experiments mainly involve phantoms (n = 43) or system setup (n = 21), with patient case studies ranking third (n
= 19), reﬂecting the comparative infancy of the ﬁeld. Experiments cover issues from registration to perception with very
diﬀerent accuracy results. Human factors emerge as signiﬁcant to OST-HMD utility. Some factors are addressed by the
systems proposed, such as attention shift away from the surgical site and mental mapping of 2D images to 3D patient
anatomy. Other persistent human factors remain or are caused by OST-HMD solutions, including ease of use, comfort
and spatial perception issues. The signiﬁcant upward trend in published articles is clear, but such devices are not yet
established in the operating room and clinical studies showing beneﬁt are lacking. A focused eﬀort addressing technical
registration and perceptual factors in the lab coupled with design that incorporates human factors considerations to
solve clear clinical problems should ensure that the signiﬁcant current research eﬀorts will succeed.

Keywords: Augmented reality, Head-mounted displays, Optical see-through, Human factors

1

Introduction

Augmented reality (AR) surgical guidance was originally
proposed in neurosurgery over 35 years ago (Kelly et al.,
1982; Roberts et al., 1986). The ability to view patient
models directly on the surgeon’s view promises numerous
beneﬁts, including better perception, ergonomics, hand-
eye coordination, safety, reliability, repeatability, and ul-
timately improved surgical outcomes. But despite more
than three decades of research, the promise of AR has not
yet translated into routine clinical practice. The develop-
ment of commercial optical see-through head-mounted dis-
plays (OST-HMDs) including Google Glass, Moverio and
the HoloLens have led to an increasing interest in such
devices for surgical guidance (see Fig. 1). While there is
occasional critical analysis (Carbone et al., 2020), the ma-
jority of authors tend to emphasise the great potential of
AR in surgical applications. Multiple review papers for dif-
ferent specialities follow a similar pattern of positivity, but
note that further research is needed. The lack of research
demonstrating clinical beneﬁt has been widely noted.

There is a risk that enthusiasm for the newly available
OST-HMD devices may lead research along a similar path
to earlier work and fail to achieve translation into routine
surgery. The purpose of this paper is to provide a summary
of current research in OST-AR in surgery and examine

Corresponding author email: manuel.birlo.18@ucl.ac.uk

possible barriers to clinical adoption of such devices. We
categorise papers according to application area, consider
registration and validation methodologies as well as choice
of visual content. Human factors emerge as a signiﬁcant
set of issues potentially limiting the applicability of AR
and we provide a description of the most common issues
encountered.

We believe that clinically successful AR can be achieved
by clear identiﬁcation of the critical points in speciﬁc sur-
gical procedures where guidance is needed, identiﬁcation
of the relevant information segmented from preoperative
scans and attention to human factors issues regarding the
modes of visualisation and interaction. We hope that this
review will be useful to clinicians and engineers collaborat-
ing on new OST-HMD AR projects and recommend con-
sideration of human factors at an early stage.

2 Background

AR systems can be realised using diﬀerent types of display
media such as conventional displays, projectors or head-
mounted displays (HMDs) (Sielhorst et al., 2008; Okamoto
et al., 2015). Of these three display media categories,
HMDs oﬀer the most user-friendly solution for manual
tasks since the user can work both from a self-centered per-
spective and hands-free (Condino et al., 2020). HMDs can
be classiﬁed according to their underlying AR paradigm:
Video see-through (VST) or optical see-through (OST).

In VST systems, a video image feed is combined with
superimposed computer generated images such as 3D re-

 
 
 
 
 
 
2

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

constructed organs. VST systems have been adopted in
surgical applications via computer displays and HMDs,
and have several potential advantages including improved
synchronisation between video feed and overlay as well as
video processing for image segmentation or registration.
Also the contrast between video feed and overlaid graphical
content can be easily controlled and the real scene can be
occluded by a virtual overlay. The disadvantages of VST
systems include limitations in terms of video bandwidth,
the risk of losing vision of the real scene in the case of
system errors and geometric aberrations such as distorted
spatial perception (Cutolo et al., 2018). Though video and
overlay may be well synchronised, there is inevitably some
delay between actual motion and perception of the mo-
tion, both real and overlaid, which can slow down surgical
motion and may increase errors. Guo et al. (2019) also re-
ported that the absence of a direct view to the real world
makes surgeons nervous.

In OST-HMDs a transparent monitor displaying graph-
ical content is located between the surgeon’s line of vi-
sion and the target organ structures. This provides an
unhindered view of reality, natural stereo vision capabili-
ties with no lag or loss of resolution associated with the
real scene. The downsides, however, are dynamic registra-
tion errors for the augmented view, latency when moving,
static registration errors, complex calibration and unnat-
ural perceptual issues, such as the fact that nearer virtual
objects don’t occlude real objects in the background (Rol-
land et al., 1995).

2.1 Commercial OST-HMD Devices

Prior to 2013 research in OST-HMD relied largely on cus-
tom built devices. It is a technically diﬃcult challenge to
make such a device, incorporating miniaturised displays
into a wearable headset with half-silvered mirrors enabling
free view of the real scene. The optical setup to display a
bright image with good contrast and resolution covering a
wide ﬁeld-of-view is technically hard to achieve. Only two
of the papers in our review use custom devices.

The potential commercial beneﬁts of being able to place
graphical information directly overlaid on the wearer’s view
of the real world is a vision that has led to the development
of a number of commercial devices. Google Glass, released
in 2013, is a lightweight monocular AR device enabling
display of information while continuing daily life. The Mi-
crosoft HoloLens, released in 2017, is a larger HMD that
incorporates stereo vision, low latency room mapping and
head tracking as well as gesture-based interaction using
only the wearer’s hands. Numerous other devices have ap-
peared oﬀering diﬀerent levels of comfort and function (for
a more detailed list of OST-HMD devices see section 4.3).
Though none of these devices were speciﬁcally designed
for surgical tasks, the potential for convenient display of
information to the surgeon has led to the explosion of re-
search detailed in this review. In common with any medi-
cal intervention, the fundamental questions concern safety
and eﬃcacy.

2.2 Safety

Convenient overlay of information comes with inherent
risks. Where the aim is that the overlay directly guides
surgery, accuracy is key. Some authors are critical of OST-
HMD device accuracy. Condino et al. (2020) concluded
from their quantitative study that the HoloLens should not
be used for high-precision manual tasks. Carbone et al.
(2020) also conclude that OST-HMDs are unsuitable for
surgical guidance, suggesting that research should focus
on addressing perceptual issues that play a critical role in
limiting user accuracy. Fida et al. (2018) performed a sys-
tematic review of augmented reality in open surgery and
conclude that such perceptual issues limit their usage to
the augmentation of simple virtual elements such as mod-
els, icons or text.

Even accurately overlaid could distract from or ham-
per the surgeon’s view of the patient, potentially slowing
the response to critical situations such as bleeding. Dil-
ley et al. (2019) propose nearby presentation of correctly
oriented but not registered models. Gesture interactions
with the AR view may prove diﬃcult to combine with the
manual surgical task itself (Solovjova et al., 2019). Cog-
nitive overload can occur if too much extra information is
presented to the surgeon at the same time (Katić et al.,
2015).

Cometti et al. (2018) analyzed the eﬀects of mixed re-
ality HMDs on cognitive and physiological functions dur-
ing intellectual and manual tasks that last for 90 minutes.
Their experiment consisted of 12 volunteers performing
and manual tasks with and without the HoloLens while
their physical and mental conditions (cognitive, cardiovas-
cular and neuromuscular) were measured. They conclude
that using the HoloLens is safe since it does not impact
safety-critical human functionalities like balance and cog-
nitive and physical fatigue. However, despite the positive
outcome of the study, the authors also state that one of
the prerequisites of a safe and eﬀective usage of HMDs is
that users should be receptive to the device.

While some of the technological limitations of OST-
HMDs are currently being addressed, such as limited
ﬁeld of view and automatic eye-to-eye calibration (e.g.
HoloLens 2, Microsoft Corporation, Redmond, USA),
human-factor limitations remain the major hurdles that
prevent the commercial success of OST-HMD AR solutions
within surgical applications (Cutolo et al., 2018).

2.3 Eﬃcacy

The fundamental advantage of augmented reality surgery
lies in the convenient display of graphical, image, icon or
text information directly on the surgeon’s view of the pa-
tient. There is no need to look away from the surgical
scene or stop the operation to obtain potentially useful
visual input.

When displaying guidance information, accuracy be-
comes a measure of system performance and the majority
of the papers included in this review perform some ac-

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

3

Fig. 1: Google Scholar search results for surgery "Head Mounted
Display" "Augmented Reality" OR "Mixed Reality" surgery "Head
Mounted Display" "Augmented Reality" OR "Mixed Reality" "optical
see through" OR "Hololens" OR "Magic Leap" OR "Google Glass" in
the last 20 years (2020 results have a more recent search date)

curacy or precision experiments.
It is important to dis-
tinguish registration or tracking accuracy, which is often
based on an external tracking or guidance system, from
perceptual accuracy achieved by the AR system.

The ultimate test of eﬃcacy would be improved patient
outcome, but the systems reviewed are not currently at the
stage of large scale clinical trials that would be needed to
demonstrate patient beneﬁt.

This review aims to give an overview of the current state
of the art in OST-HMD assisted surgery by analysis of sev-
eral components of the selected literature, including OST-
HMD device, surgical speciality, surgical application con-
text, surgical procedure, AR visualisations, conducted ex-
periments and accuracy results. A special focus is given to
the identiﬁcation of human factors in each article.

3 Methods

3.1 Literature search

A systematic review was performed according to the pre-
ferred reporting items for systematic review and meta-
analysis (PRISMA) guidelines Liberati et al. (2009). The
literature search was conducted via a Google Scholar
with the search terms [surgery “Head Mounted Display”
“Augmented Reality” OR “Mixed Reality” surgery “Head
Mounted Display” “Augmented Reality” OR “Mixed Real-
ity” “optical see through” OR “Hololens” OR “Magic Leap”
OR “Google Glass”]. An initial Google Scholar including all
articles between 2013 and 2020 was conducted on February
21, 2020. An updated Google Scholar search for 2020 only
was subsequently performed on January 27, 2021.

Fig. 2: Systematic review search strategy

A general review of all areas of AR, including medical
and surgical, is provided by Dey et al. (2018), who ex-
amine the usability of AR over a 10 year period. Chen
et al. (2017) review medical applications of mixed reality
and provide a broad taxonomy. A comprehensive review
of medical AR is provided by Eckert et al. (2019) who con-
clude that there is no proof of clinical eﬀectiveness as yet.
Kersten-Oertel et al. (2013) give a comprehensive review
using the DVV taxonomy and provide suggestions for ar-
eas that need attention, including speciﬁc overlays for im-
portant phases of the operation as well as optimisation of
interaction and system validation.

We have also previously identiﬁed some of the barriers to
adoption of surgical AR in general (Edwards et al., 2021).
Existing comprehensive reviews of related surgical areas
were found, including robotics (Qian et al., 2020) and la-
paroscopic surgery (Bernhardt et al., 2017). Orthopaedics
is the dominant application area in this review and three
other reviews cover this speciﬁc ﬁeld well (Laverdière et al.,
2019; Jud et al., 2020; Verhey et al., 2020).

It is worth noting that nearly all the review papers sug-
gest the potential of AR in surgical applications, but cite
technological hurdles to user acceptability and the lack of
any clinical validation. None of these reviews cover OST-
HMDs speciﬁcally, which is an increasingly popular choice.
We aim to provide a critical analysis of the important char-
acteristics of OST-HMDs, looking speciﬁcally at human
factors issues, which emerged as signiﬁcant area potentially
limiting user acceptability of systems.

3.2 Other review papers

Since we want to analyze only original research papers,
other review papers are not considered. Our search did
return a number of these, however, which deserve some
attention.

3.3 Literature analysis strategy

In order to narrow down the publication year search range
and focus on more recent research, the number of publi-
cations resulting from the google scholar search terms in
the last 20 years were analyzed (ﬁgure 1), which shows a
steady increase from 2013, coinciding with the release of

4

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

36

24

ments, no controlled randomised trials) a meta-analysis
could not be performed. Therefore, publication bias could
not be reduced and should be taken into account.

Data extracted from the included publications were 1.
Clinical setting (surgical speciality, surgical application
context, surgical procedure), 2. The assessed OST-HMD
device, 3. Methods (AR visualisations, Conducted experi-
ments), 4. Key results (Accuracy), 5. Human factors

13

8

6

2

2

0

2013

2014

2015

2016

2017

2018

2019

2020

4 Analysis of the literature search

This section summarises the results of the included 91 ar-
ticles that were identiﬁed in the literature review process.
An overview of used OST-HMD device, surgical applica-
tion context and surgical procedure can be found in table 2.
Appendix table A1 contains details about AR visualisa-
tions, conducted experiments and accuracy results.

s
e
l
c
i
t
r
a
#

35

30

25

20

15

10

5

0

Fig. 3: Systematic review results overview: Annual Distribution of se-
lected 91 studies from 2013-2020

4.1 Annual distribution of selected arti-

Google Glass. Due to the clear increase from that time, we
chose 2013 as the starting year for our literature review.

The review process is shown in Fig. 2 and includes the
results from both the original search (February 21, 2020,
numbers in black colour) and the updated search (Jan-
uary 21, 2021, numbers in red colour). The Google Scholar
search initially resulted in 998 (486) records. In a subse-
quent screening phase, title, abstract and BibTex informa-
tion were read to decide whether the record seems to be
a relevant publication. We exclude records that are either
a duplicate or contain duplicated content from the same
authors compared to another record. A total of 15 (7)
duplicates were excluded. During the screening the fol-
lowing inclusion criteria were used: The article has to 1.)
be a peer-reviewed original journal article, 2.) describe an
OST-HMD focused application with surgical context, 3.)
is not an overview or systematic review publication (which
are considered separately). Records whose full text wasn’t
available were excluded. 907 (441) records that didn’t meet
the inclusion criteria were excluded. Together with the 15
(7) excluded duplicates, a total of 923 (448) records were
excluded during the screening phase, which led to 76 (38)
remaining full text articles that were assessed for eligibil-
ity. Full text articles had to meet the following inclusion
criteria: The article 1.) describes the usage of an OST-
HMD, 2.) with a clear focus on a surgical application, 3.)
investigates the potential utility of OST-HMDs in surgi-
cal settings and 4.) is neither optics nor hardware design
focused. 13 (10) articles that didn’t meet these inclusion
criteria were excluded. The remaining 63 + 28 = 91 stud-
ies that met all predeﬁned inclusion criteria form the ﬁnal
set of papers examined in this review.

When reporting the results, the PRISMA guidelines
were followed. Due to the inherent characteristics of the
studies (small case series, subjective qualitative assess-

cles

Fig. 3 shows the annual distribution of the 91 studies dur-
ing 2013-2020. There were no articles in the year 2013 that
fulﬁlled the inclusion criteria. Starting from 2014 there has
been a steady increase in the number of publications. The
increasing trend tends to be related to the release of ma-
jor OST-HMDs like Google Glass and Microsoft HoloLens,
and will be discussed in more detail in section 4.3.

4.2 Surgical speciality

We found that OST-HMDs have been applied in a variety
of surgical specialities. Fig. 4 shows a graphical illus-
tration of all articles grouped into their surgical special-
ity and placed at their respective body region. Fig. 5
shows the proportion of publications for each surgical spe-
ciality. Orthopaedic surgery dominates (28.6%, n = 26),
perhaps since proximity to bone requires only rigid regis-
tration and somewhat lower accuracy is required compared
to applications such as neurosurgery. General surgery, neu-
rosurgery, applications without a concrete surgical special-
ity and vascular surgery follow with more than ﬁve ar-
ticles each. Dental surgery is represented with ﬁve arti-
cles, followed by heart surgery and Otolaryngology (n =
4 each). Other surgical specialities include reconstructive
surgery, urology and maxillofacial surgery (n = 3 each).
A few attempts have been made to explore potential ben-
eﬁts of OST-HMDs in robot-assisted surgery and paedi-
atric surgery (n = 2 each).
Interventional oncology, la-
paroscopic surgery, visceral surgery and anaesthesiology
are represented with one article. Speciﬁc articles per sur-
gical speciality are detailed in table 1. While orthopaedics
still dominates, other applications, including general, vas-
cular and neurosurgery, are increasingly represented in the
latter half of the survey period as interest in AR applica-
tions spreads to other surgical ﬁelds.

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

5

Table 1
Distribution of the included articles per surgical speciality

Surgical speciality

Orthopaedic Surgery

General Surgery

Neurosurgery

Vascular Surgery

General surgical applications

Dental Surgery

Heart Surgery
Otolaryngology - head and neck surgery

Reconstructive Surgery
Maxillofacial Surgery
Urological surgery
Robot-assisted surgery
Paediatric surgery
Visceral Surgery
Interventional Oncology
Laparoscopic Surgery
Anaesthesiology

Articles

Armstrong et al. (2014) Ponce et al. (2014) Chen et al. (2015) Wang et al. (2016) Stewart
and Billinghurst (2016) Hiranaka et al. (2017) Jalaliniya et al. (2017) Unberath et al. (2018)
El-Hariri et al. (2018) Deib et al. (2018) Andress et al. (2018) Condino et al. (2018) Gibby
et al. (2019) de Oliveira et al. (2019) Aaskov et al. (2019) Liebmann et al. (2019) Fotouhi
et al. (2019a) Fotouhi et al. (2019b) Pietruski et al. (2020) Laguna et al. (2020) Gibby
et al. (2020) Gu et al. (2020) Viehöfer et al. (2020) Dennler et al. (2020) Kriechling et al.
(2020) Matsukawa and Yato (2020)
Lin et al. (2018) Wu et al. (2018) Mahmood et al. (2018) Rojas-Muñoz et al. (2019) Li
et al. (2019) Pelanis et al. (2020) Zhou et al. (2020) Al Janabi et al. (2020) Jud et al.
(2020) Galati et al. (2020) Li et al. (2020a)
Yoon et al. (2017) Karmonik et al. (2018) Frantz et al. (2018) Nguyen et al. (2020) Zhang
et al. (2019) Heinrich et al. (2019) Baum et al. (2020) Liounakos et al. (2020) Sun et al.
(2020b)
Kaneko et al. (2016) Kuhlemann et al. (2017) Zhou et al. (2019b) Rynio et al. (2019)
Rojas-Muñoz et al. (2020a) Park et al. (2020) Mendes et al. (2020)
Meulstee et al. (2019) Chien et al. (2019) Boillat et al. (2019) Guo et al. (2019) Dallas-Orr
et al. (2020) Luzon et al. (2020) Cartucho et al. (2020) Kumar et al. (2020)
Katić et al. (2015) Liebert et al. (2016) Song et al. (2018) Zhou et al. (2019a) Zafar and
Zachar (2020)
Li et al. (2017) Zou et al. (2017) Brun et al. (2019) Liu et al. (2019)
Rojas-Muñoz et al. (2020b) Gnanasegaram et al. (2020) Scherl et al. (2020) Creighton et al.
(2020)
Mitsuno et al. (2017) Pratt et al. (2018) Jiang et al. (2020)
Pietruski et al. (2019) Pepe et al. (2019) Sun et al. (2020a)
Borgmann et al. (2016) Dickey et al. (2016) Schoeb et al. (2020)
Qian et al. (2018) Fotouhi et al. (2020)
Wellens et al. (2019) Fitski et al. (2020)
Sauer et al. (2017)
Li et al. (2020b)
Zorzal et al. (2020)
Schlosser et al. (2019)

4.3 Optical

see-through head-mounted

displays (OST-HMDs)

5 Surgical application context

Fig. 6 depicts the annual distribution by OST-HMD be-
tween 2014-2000. Google Glass, the device with the second
highest number of articles (n = 8), dominates the distri-
bution in 2014, but interest decreases from 2015 to 2017,
perhaps due to diminishing support from Google. The Mi-
crosoft HoloLens was released in 2016 and has dominated
the ﬁeld of OST-HMD assisted surgery since then, with
a steady increase in papers from 2017 and accounting for
the majority of articles (n = 66). Following HoloLens and
Google Glass, the Moverio BT-200, which was released in
2014, has the third highest number of articles (n = 4) and
was used once in 2016, 2017, 2019 and 2020. Its successor,
the Moverio BT-300, was released in late 2016 and has only
one application in 2020. The Magic Leap One, released in
2018 and attracting huge initial investment, has not estab-
lished itself in OST-HMD assisted surgery, generating only
one article in 2019. Other devices include the NVIS nVi-
sor ST (n = 2), Vuzix M300, Brother AirScouter WD-100,
Aryzon headset, Metavision Meta 2 and PicoLinker (n =
1 each).

To summarise, the HoloLens clearly dominates the ﬁeld,
but there is interest in other devices such as Moverio BT.
This is a rapidly developing ﬁeld at present and we can
expect further devices to appear on the market in the next
few years.

Surgical application contexts deﬁne how OST-HMD as-
sistance is intended to improve surgical practice. Fig. 7
shows the distribution of all identiﬁed contexts. Surgical
guidance is by far the most popular (n=58), followed by
preoperative surgical planning (n=12) and surgical train-
ing (n = 11) then Teleconsultation and telementoring (n
= 5 each). Four articles were included where the surgeon
views a 3D patient anatomy hologram to aid clinical deci-
sion making rather than intraoperative guidance, which we
called intraoperative surgical anatomy assessment. The re-
maining applications that have been identiﬁed are intraop-
erative review of preoperative 2D imaging and/or patient
records, intraoperative documentation, patient monitoring
and preparation of robot-assisted MIS (n = 2 each). We
expand on some of the surgical application contexts and
respective articles in the following subsections.

5.1 Surgical guidance

We use the deﬁnition of surgical guidance or image-guided
surgery from Cleary and Peters (2010): a medical proce-
dure in which a surgeon uses computer-based virtual pre-
or intraoperative image overlays to visualise and target pa-
tient anatomy. They also state that an image-guided in-
tervention includes registration and tracking methods, but
we also consider an OST-HMD based solution to be of the
category image guidance if it uses registered holographic
image overlays without tracking if these overlays support
a clinician in visualizing and targeting the surgical site.

6

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

s
e
l
c
i
t
r
a
#

35

30

25

20

15

10

5

0

Google Glass
HoloLens
Moverio BT-200
Moverio BT-300
NVIS nVisor ST60
Brother AirScouter WD-100
PicoLinker smart glasses
Custom Device
Vuzix M300
Magic Leap One
Arzyon headset
Metavision Meta 2

1
1
1
1
1

31

1
1
1

20

1
1
1
1

3

2

1
1
1

3

9

2

1
1

2014

2015

2016

2017

2018

1

2019

2020

Fig. 6: Annual Distribution of articles by OST-HMD device from 2014-
2020

58

s
e
l
c
i
t
r
a
#

60

50

40

30

20

10

0

Surgical Guidance
Preoperative surgical planning
Surgical training
Teleconsultation
Telementoring
Intraoperative Surgical Anatomy Assessment
Intraoperative review of 2D patient data
Intraoperative documentation
Patient Monitoring
Robot placement (robot-assisted MIS)

12

11

5

5

4

2

2

2

2

SG

PS

ST

TELC TELM SAA REV DOC

PM

RP

Fig. 7: Distribution of articles by surgical application context

s
e
l
c
i
t
r
a
#

14

12

10

8

6

4

2

0

Needle insertion (NI)
K-Wire insertion (KWI)
Surgical saw navigation (SSN)
MIS endoscopy guidance (EG)
Image overlay for navigation (IO)

Screw insertion (SI)
Drill trajectory guidance (DTG)
Robot placement (RP)
Imaging probe navigation (PN)
Dissection guidance (DG)

Catheter insertion (CI)
Surgical tool placement (TP)
Stent-graft placement (SP)
C-arm Positioning Guidance (CA)
Anatomy Identiﬁcation (AI)

10

8

7

7

4

4

3

4

4

2

2

1

1

1

1

NI

SI

CI KWI DTG TP

SSN RP

SP

EG PN

CA

IO

DG

AI

Fig. 8: Surgical guidance applications: Distribution of the subset of
ﬁnal 91 articles (n = 59) by applications of surgical guidance, grouped
into the four categories 1. navigation of a linear path, 2. navigation
of surgical tools or equipment, 3. navigation of an imaging device, 4.
general guidance to help spatial awareness not associated with a speciﬁc
task

Fig. 4: Graphical illustration of included articles grouped by surgical
speciality and placed at respective human body regions

General Surgery

Orthopaedic Surgery

Neurosurgery

12.1%

28.6%

9.9%

8.8%

7.7%

General Applications

Vascular Surgery

4.4%
2.2% Other Surgical Specialities
2.2% Robot-assisted Surgery
3.3%
Paediatric Surgery
Maxillofacial Surgery

3.3%

3.3%

Urological Surgery
Reconstructive Surgery

5.5%

4.4%

4.4%

Dental Surgery

Heart Surgery

Otolaryngology

Fig. 5: Pie chart showing the distribution of the included 91 papers
among the identiﬁed surgical specialities

Since this broad deﬁnition covers over half the included
papers, we further split OST-HMD assisted surgical guid-
ance into diﬀerent applications, whose distribution is pre-
sented in Fig. 8. General image overlay for navigation
systems (n = 10) overlay a registered 3D anatomy model
in order to provide surgical guidance, including applica-
tions in neuronavigation (Frantz et al., 2018; Nguyen et al.,
2020), orthopaedic procedures (de Oliveira et al., 2019),
algorithm-focused registration approaches (Wu et al., 2018;
Aaskov et al., 2019; Chien et al., 2019) and maxillo-facial

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

7

(a)

(b)

(c)

Fig. 9: Guided screw insertion and needle insertion examples. (a) A surgeon uses a custom-made navigation device in an experimental setup (b).
Augmented drill entry points (shown in blue) are used to start the navigation. During the guided drill procedure, the 3D angle between current and
targeted screw trajectory and their deviation angle are displayed. (source: Liebmann et al. (2019) Fig. 5b and 5d). (c) Mixed reality needle insertion
navigation system for low dose-rate (LDR) brachytherapy (source: Zhou et al. (2019b)) Fig. 1.

tumor resection (Pepe et al., 2019).

Needle insertion (n = 8) has emerged as an applica-
tion since 2018, mostly using the HoloLens, and was in-
vestigated in percutaneous spine procedures (Deib et al.,
2018), needle biopsy (Lin et al., 2018), thoracoabdomi-
nal brachytherapy (Zhou et al., 2019b, 2020) and needle-
based spinal interventions (Heinrich et al., 2019). Zhou
et al. (2019b) presented a mixed reality based needle in-
sertion navigation system for low-dose-rate brachytherapy
that was tested in animal (Fig. 9 (c)) and phantom experi-
ments. Reported beneﬁts of this needle insertion approach
include clinically acceptable needle insertion accuracy and
a reduction of the number of required CT scans.

Tool placement examples (n = 7) include investigated
attentiveness to the surgical ﬁeld during navigation (Stew-
art and Billinghurst, 2016), a ﬁrst assistant’s task per-
formance during robot-assisted laparoscopic surgery based
tool manipulation (Qian et al., 2018), bone localisation (El-
Hariri et al., 2018), an optical navigation concept (Meul-
stee et al., 2019), liver tumor puncture (Li et al., 2019),
craniotomy assistance (Zhang et al., 2019) and percuta-
neous orthopaedic treatments (Fotouhi et al., 2019b).

OST-HMD assisted screw insertion (n = 7) has been ex-
plored with diﬀerent holographic visualisations. Yoon et al.
(2017) presented an application for pedicle screw place-
ment in spine instrumentation that streamed 2D neuron-
avigation images onto a Google Glass. Surgeons reported
an overall positive AR-experience. Liebmann et al. (2019)
developed a HoloLens pedicle screw placement approach
for spinal fusion surgery that uses holopgraphic 3D angles
between current and targeted screw trajectory, using de-
viation in angle to guide the surgeon (Fig. 9 (a) and (b)).
The reported results of a lumbar spine phantom experi-
ment indicate a promising screw insertion accuracy with
the caveat that surrounding tissue was not taken into ac-
count. Other articles describing pedicle screw insertion
include Yoon et al. (2017) and Gibby et al. (2019). Percu-

taneous implantation of sacroiliac joint screws is presented
in Chen et al. (2015) and Wang et al. (2016).

Catheter insertion (n=4) also has to deal with the
manipulation of ﬂexible structures and has been ap-
plied to US-guided central venous catheterisation (Kaneko
et al., 2016), radiaton-free endovascular stenting of aor-
tic aneurysm (Kuhlemann et al., 2017) and transcatheter
procedures for structural heart disease (Liu et al., 2019).
K-wire insertion in orthopaedic procedures (n = 4) was
addressed by experiments investigating ﬂuoroscopy con-
trolled wire insertion into femur (Hiranaka et al., 2017),
percutaneous orthopaedic surgical procedures (Andress
et al., 2018) and C-arm ﬂuoroscopy guidance (Fotouhi
et al., 2019a). The exploration of potential beneﬁts of holo-
graphic camera views for endoscopy guidance (n = 4) has
been conducted in ﬁrst assistant support in robot-assisted
laparoscopic surgery (Qian et al., 2018) (Fig. 10 (a)), per-
cutaneous endoscopic lumbar discectomy (Liounakos et al.,
2020) and ureteroscopy (Al Janabi et al., 2020).

Drill trajectory guidance (n = 3) explores potential ad-
vantages of holographic guidance information such as drill
angle and deviation between actual and planned drill path
and has been used in dental implant surgery (Katić et al.,
2015) and endodontic treatments (Song et al., 2018). Sur-
gical saw navigation using holographic cutting guides (n =
2) was presented in mandibular resection (Pietruski et al.,
2019) and free ïňĄbula ïňĆap harvest (Pietruski et al.,
2020).

In addition to surgeons themselves, other clinical staﬀ
in the operating theatre can beneﬁt from OST-HMD assi-
tance. In minimally invasive robotic surgery it is usually
the ﬁrst assistant’s responsibility to set up the robot arms
prior to intraoperative robot control conducted by a sur-
geon. We identiﬁed 2 articles that present HoloLens appli-
cations aiming to support the ﬁrst assistant during robot-
assisted surgery: 1.) Qian et al. (2018) robotic instrument
placement in laparoscopic surgery from (Fig. 10 (a)) and

8

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

(a)

(b)

Fig. 10:
(a) Robotic instrument placement and endoscopy guidance: Navigation aids for the ﬁrst assistant: Real-time renderings of a robotic
endoscope and robotic instruments that are superimposed on their physical counterparts. In addition, endoscopy guidance is realised via an endoscopy
visualisation being registered with a viewing frustrum (source: Fig. 4 (f ) of Qian et al. (2018)) (b) Robot placement: Reﬂective-AR Display aided
alignment between a real robot arm and its virtual counterpart and subsequent robot placement to its intended position in preparation for robotic
surgery (source. Fig. 4 of Fotouhi et al. (2020))

(a)

(b)

Fig. 11: (a) Dissection Guidance example in reconstructive surgery: HoloLens based identiﬁcation of vascular pedunculated ﬂaps: a CTA-based 3D
model of a female patient’s leg consisting of segmented skin, bone, bone, vessels and vascular perforators lower leg is superimposed on the patient
anatomy. The surgeon conﬁrms perforator location with audible Doppler ultrasonography (source: Fig. 3 of Pratt et al. (2018)) (b) Surgical Anatomy
Assessment example in plastic surgery: AR views of the Moverio BT-200 smart glasses showing a patient with osteoma and holographic facial anatomy
(face surface and facial bones including the osteoma) superimposed onto a patient’s face (source: Fig. 8 of Mitsuno et al. (2017))

2.) full robot arm placement in minimally invasive gastrec-
tomy (abdominal surgery) from Fotouhi et al. (2020) (Fig.
10 (b)). The remaining applications of surgical guidance
cover topics such as stent-graft placement in endovascu-
lar aortic repair (Rynio et al., 2019), imaging probe nav-
igation for tooth decay management (Zhou et al., 2019a),
C-arm positioning guidance in percutaneous orthopaedic
procedures (Unberath et al., 2018), identiﬁcation of spinal
anatomy underneath the skin (Aaskov et al., 2019) and
dissection guidance for vascular pedunculated ﬂaps of the
lower extremities presented by Pratt et al. (2018) (Fig.
11 (a)). A HoloLens based mixed reality approach was
decised in which the surgeon has to manually register a
CTA-based 3D model of a patient’s leg to the respective
patient anatomy using HoloLens hand gesture and voice

command interaction. After a surgical patient case study,
surgeons conﬁrmed that this mixed reality solution is more
reliable and less time consuming than audible Doppler ul-
trasound which is the conventional non-AR method.

With the main research focus being image-guidance, it
is very important to consider safety and accuracy in such
systems. Overconﬁdence in the accuracy of guidance or
visual clutter of the viewed scene may lead to an increase
in surgical errors and a careful balance needs to be struck to
provide useful information rather than cognitive overload.

5.2 Other surgical application contexts

Preoperative planning applications from Zou et al. (2017)
and Li et al. (2017) addressed human-computer interaction

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

9

(a)

(b)

Fig. 12: Surgical Training Example Application: Ultrasound Education. (a) Multiple users can see holographic anatomical cross sections mapped on
a patient simulator and the ultrasound scan plane. (b) Holograhic subcostal four-chamber view coming out of the simulator probe. Source: Fig. 3
and 7 of Mahmood et al. (2018)

(a)

(b)

Fig. 13: Telementoring Applications. (a) Overview of a Google Glass systeming using a composite surgical ﬁeld Source: Fig. 3 of Ponce et al.
(2014). (b) First-person view of HoloLens-based holographic instructions consisting of 3D models and 3D lines Source: Fig. 2 of Rojas-Muñoz et al.
(2019).

issues of conventional approaches in preoperative diagnosis
of coronary heart disease that lead to inaccurate diagnosis
results and propose a hand gesture based interactive holo-
graphic diagnosis system aiming to provide a natural and
intuitive interaction. Karmonik et al. (2018) used holo-
graphic 3D vascular structures to improve the extraction
and communication of complex MRI image data in the con-
text of aneurysm rupture prediction. Pelanis et al. (2020)
addressed planning of liver resection surgery and found
that 3D holographic liver anatomy visualisations improve
the user’s spatial understanding. Other articles that were
categorised as preoperative surgical planning investigate
potential planning improvements for repair of complex con-
genital heart disease (Brun et al., 2019) and preoperative
anatomy assessment for nephron-sparing surgery (Wellens
et al., 2019).

Beneﬁts of OST-HMD AR during surgical training have
been explored for preoperative diagnosis and planning of
coronary heart disease (Li et al., 2017),
intraoperative
surgical tool guidance during hip arthroplasty simulation
(Condino et al., 2018), neurosurgical burr hole localisation
(Baum et al., 2020) and transesophageal echocardiography
examination from Mahmood et al. (2018) shown in Fig. 12.

Telementoring also belongs to the broader scope of sur-
gical training, but involves a surgical trainee being men-
tored by an expert surgeon during a surgical procedure
rather than training outside the operating room. Ponce
et al. (2014) used a Google Glass based mentoring system
for shoulder arthroplasty (Fig. 13 (a)). The student sur-
geon and teacher surgeon can both see a composite surgi-
cal ﬁeld in which hands and surgical tools of both surgeons
can be seen at the same time. Rojas-Muñoz et al. (2019)
used a HoloLens mentoring system in which an expert sur-
geon can place virtual 3D annotations (surgical tools and
incision guidance lines) which are seen by the student sur-
geon in real time (Fig. 13 (b)). The authors reported
improved information exchange between student and men-
tor, reduced number of focus shifts and reduced placement
error. A similar mentoring is presented in Rojas-Muñoz
et al. (2020a), where trainees performed leg fasciotomies
and reported an improved surgical conﬁdence.

In contrast to telementoring, where the dialogue is con-
tinuous, teleconsultation (n = 5) focuses on a consulta-
tion based on-demand communication between colleagues.
Sauer et al. (2017) explored potential beneﬁts of using the
HoloLens to establish a web-service based real-time video

10

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

(a)

(b)

Fig. 14: Surgical Anatomy Assessment and Teleconsultation Applications in Visceral Surgery: (a) Intraoperative visualisation of a preoperative model
of the vascular anatomy of the cranio-ventral liver and tumor to be dissected. (b) Intraoperative tele-consulting: real-time video communication with
a remote surgeon (Source: Figure 3 (C and F) of Sauer et al. (2017))

and audio communication with a remote colleague during
visceral-surgical interventions (Fig. 14). In addition, the
remote surgeon could mark anatomical structures within
the surgical site using a tablet computer. Borgmann et al.
(2016) used a Google Glass for hands-free teleconsultation
during diﬀerent urological surgical procedures. Other ex-
amples use the Google Glass for consultation during re-
constructive limb salvage (Armstrong et al., 2014) and or-
thopaedic procedures (Jalaliniya et al., 2017).

Applications where holographic 3D anatomy is displayed
an intraoperative setup without trying to guide the surgical
procedure are categorised as surgical anatomy assessment.
This involves intraoperative assessment of preoperatively
aquired patient anatomy that aids clinicial decision making
without trying to guide the procedure itself.

Sauer et al. (2017) used a HoloLens based 3D visuali-
sation of a liver cranio-ventral incl. tumor (Fig. 14 (a))
to improve a surgeon’s spatial understanding of the target
anatomy during dissection of the liver parenchyma in com-
plex visceral-surgical interventions. Mitsuno et al. (2017)
used Moverio BT-200 smart glasses and registered holo-
graphic 3D face and facial bones surfaces (Fig. 11 (b)) to
aid clinical decision making for more objective assessment
of the improvement of a patient’s body surface contour in
plastic surgery.

A further category of display shows preoperatively ac-
quired 2D patient imaging data and medical records in the
surgeon’s ﬁeld of view using AR rather than a separate
monitor. Borgmann et al. (2016) asked surgeons to rate
their perceived usefulness of displaying patients’ medical
records and CT scans on a Google Glass during urological
surgical procedures. They found that reviewing patient
images was rated less useful, whereas reviewing medical
Jalaliniya and Pederson
records received a high rating.
(2015) used a Google Glass to view and manipulate X-ray
and MRI images.

6 AR visualisations

Conventional computer-assisted surgery uses diﬀerent
types of visualisations to aid preoperative planning or in-
traoperative procedures and a similar range of visualisa-
tions have been adopted for AR-assisted applications (see
table A1).

Fig. 15 shows the distribution of articles by type of AR
visualisation. The majority of articles use preoperative
models (n = 66), usually consisting of 3D reconstructed pa-
tient anatomy generated from CT or MRI imaging content,
sometimes in conjunction with preoperative planning com-
ponents. Liebmann et al. (2019) used holographic preop-
eratively planned screw trajectories and drill entry points
to aid pedicle screw placement in spinal fusion surgery.
Pratt et al. (2018) investigated the usefulness of CT re-
constructed 3D patient leg models including bony, vascu-
lar, skin and soft tissue structures, vascular perforators
and a surrounding bounding box that facilitated manual
registration.

We also consider non-anatomical content as a preopera-
tive model such as holographic user interaction menus or
graphical annotations. Rojas-Muñoz et al. (2020a), for ex-
ample, used graphical annotations of incision lines and a
model of surgical tools in a telementoring system. Condino
et al. (2018) implemented a virtual menu with toggle but-
tons for a hybrid simulator for orthopaedic open surgery
training.

Applications where 3D visualisations are generated in-
traoperatively in order to take updated live information
into account, usually for surgical guidance, we refer to as
intraoperative model visualisation (n = 13). Katić et al.
(2015) used live drill trajectory guidance information such
as position and depth of dental drill and injury avoidance
warnings in dental implant surgery. Lin et al. (2018) inves-
tigated utility aspects of intraoperatively generated needle
visualisations such as needle position, orientation, shape
and a tangential ray during needle biopsy.

Live intraoperative images (n = 12) can be displayed in
a surgeon’s ﬁeld of view using AR in order to have cru-

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

11

Table 2
Studies listed by OST-HMD, Surgical context and surgical procedure : Acronyms: SG: Surgical guidance. PS: Preoperative surgical planning. SAA:
Intraoperative surgical anatomy assessment. ST: Surgical training. REV: Intraoperative review of preoperative 2D imaging and/or patient records. TELC:
Teleconsultation during surgery. TELM: Telementoring. DOC: Intraoperative documentation. PM: Patient monitoring. Acronyms for surgical guidance
applications: TP: Surgical tool placement. IO: Image overlay for navigation. SI: Screw insertion. NI: Needle insertion. CI: Catheter insertion. KWI:
K-Wire insertion. EG: MIS Endoscopy guidance. SP: Stent-graft placement. DTG: Drill trajectory guidance. PN: Imaging probe navigation. SNN:
Surgical saw navigation. CA: C-arm positioning guidance. RP: robot placement. DG: dissection guidance. AI: anatomy identiﬁcation.

Study

OST-HMD

Surgical context

Surgical procedure

Armstrong et al. (2014)
Ponce et al. (2014)
Chen et al. (2015)
Katić et al. (2015)
Borgmann et al. (2016)

Dickey et al. (2016)
Liebert et al. (2016)
Wang et al. (2016)
Stewart and Billinghurst (2016)

Kaneko et al. (2016)
Yoon et al. (2017)
Jalaliniya et al. (2017)
Li et al. (2017)
Kuhlemann et al. (2017)
Sauer et al. (2017)
Mitsuno et al. (2017)
Hiranaka et al. (2017)

Zou et al. (2017)
Deib et al. (2018)
Andress et al. (2018)
Song et al. (2018)
Condino et al. (2018)
Qian et al. (2018)

El-Hariri et al. (2018)
Karmonik et al. (2018)
Lin et al. (2018)
Frantz et al. (2018)
Pratt et al. (2018)

Unberath et al. (2018)
Mahmood et al. (2018)
Wu et al. (2018)
Boillat et al. (2019)
Meulstee et al. (2019)
Gibby et al. (2019)
Brun et al. (2019)
de Oliveira et al. (2019)
Fotouhi et al. (2019a)
Aaskov et al. (2019)
Guo et al. (2019)
Liebmann et al. (2019)
Liu et al. (2019)
Rojas-Muñoz et al. (2019)
Rojas-Muñoz et al. (2020a)
Li et al. (2019)
Pepe et al. (2019)
Zhou et al. (2019b)
Chien et al. (2019)
Zhang et al. (2019)
Heinrich et al. (2019)
Wellens et al. (2019)
Fotouhi et al. (2019b)
Rynio et al. (2019)
Zhou et al. (2019a)
Pietruski et al. (2019)
Schlosser et al. (2019)
Fotouhi et al. (2020)

Pelanis et al. (2020)
Nguyen et al. (2020)
Zhou et al. (2020)
Baum et al. (2020)
Al Janabi et al. (2020)
Pietruski et al. (2020)
Liounakos et al. (2020)
Gnanasegaram et al. (2020)
Sun et al. (2020b)
Park et al. (2020)
Mendes et al. (2020)
Laguna et al. (2020)
Dallas-Orr et al. (2020)
Zafar and Zachar (2020)

Google Glass
Google Glass
nVisor ST60
Custom Device
Google Glass

Google Glass
Google Glass
nVisor ST60
Brother
AirScouter
WD-100G
Moverio BT-200
Google Glass
Google Glass
HoloLens
HoloLens
HoloLens
Moverio BT-200
PicoLinker
glasses
Custom Device
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens

HoloLens
HoloLens
HoloLens
HoloLens
HoloLens

HoloLens
HoloLens
HoloLens
Google Glass
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
Hololens
Magic Leap One
Moverio BT-200
Vuzix M300
HoloLens

HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
Moverio BT-200
Moverio BT-300
HoloLens
HoloLens
HoloLens
Arzyon headset
HoloLens
HoloLens
HoloLens

TELC
TELM
SG (SI)
SG (DTG)
REV, ST, DOC,
TELC
ST, TELM
PM
SG (SI)
SG (TP)

SG (CI)
SG (SI)
REV, TELC
PS, ST
SG (CI)
SAA, TELC
SAA
SG (KWI)

PS
SG (NI)
SG (KWI)
SG (DTG)
ST
SG (RP, TP, EG)

SG (TP)
PS
SG (NI)
SG (IO)
SG (DG)

SG (CA)
ST
SG (IO)
DOC
SG (TP)
SG (SI)
PS
SG (IO)
SG (KWI)
SG (AI)
SG (IO)
SG (SI)
SG (CI)
TELM
TELM
SG (TP)
SG (IO)
SG (NI)
SG (IO)
SG (TP)
SG (NI)
PS
SG (TP)
SG (SP)
SG (PN)
SG (SSN)
PM
RP

PS
SG (IO)
SG (NI)
ST
SG (EG)
SG (SSN)
SG (EG)
ST
SG(CI)
PS
ST
PS
PS
ST

Reconstructive limb salvage procedures
shoulder arthroplasty
Percutaneous implantation of sacroiliac joint screw
Dental implant surgery
Diﬀerent urological surgical procedures

Inﬂatable penile prosthesis placement
Bronchoscopy
Percutaneous implantation of sacroiliac joint screw
General intra-operative guidance (no concrete application, only measure-
ment of attentiveness to the surgical ﬁeld)

Central venous catheterisation under US guidance
spine instrumentation (pedicle screw placement)
Orthopaedic procedures
Preoperative diagnosis & planning of coronary heart disease
Interventional endovascular stenting of aortic aneurysm
Visceral-surgical interventions
Improvement of the body surface contour in plastic surgery.
Fluoroscopy controlled K-wire insertion into femur

Preoperative diagnosis of coronary heart disease
Percutaneous vertebroplasty, kyphoplasty and discectomy procedures
Percutaneous orthopaedic surgical procedures
Access cavity Preparation in Endodontic treatment
Hip arthroplasty
Increase the First Assistant’s task performance during robot-assisted la-
paroscopic surgeries
Intra-operative bone localisation
Identiﬁcation of a hemodynamic scenario that predicts an aneurysm rupture
Needle biopsy
Neurosurgical applications
Vascular pedunculated ﬂaps of the lower extremities (reconstruction
surgery)
percutaneous orthopaedic procedures
example: transesophageal echocardiography examination
N/A
Surgical time-out checklist execution
N/A
pedicle screw placement
Repair for complex congenital heart disease
Orthopaedic surgery (no speciﬁc procedure)
C-arm ﬂuoroscopy guided k-wire placement
Identiﬁcation of spinal anatomy underneath the skin
General image-guided surgical navigation (no speciﬁc application)
Placement of pedicle screws in spinal fusion surgery
transcatheter procedures for structural heart disease
Abdominal incision
Leg fasciotomy
liver tumor puncture
Head and neck tumor resections
Seed implantation thoracoabdominal tumor brachytherapy
General SG (no speciﬁc surgical application)
Craniotomy
Needle-based spinal interventions
Nephron-sparing surgery
Percutaneous orthopaedic treatments
Endovascular aortic repair
Tooth decay management
Mandibular resection
None
Set up of robotic arms by surgical staﬀ (especially minimally invasive gas-
trectomy (abdominal surgery))
Liver resection
Neurosurgical applications
Seed implantation thoracoabdminal brachytherapy
Neurosurgical burr hole localisation
Ureteroscopy
Free ﬁbula ﬂap
Percutaneous endoscopic lumbar discectomy
N/A
External ventricular drainage (EVD)
Endovascular procedures
Central venous catheterisation
Repair of complex paediatric elbow fractures
Complex surgical procedures
No direct surgical procedure (teaching of dental anatomy)

(continued on next page)

12

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

Table 2

(continued)

Study

OST-HMD

Surgical context

Surgical procedure

Fitski et al. (2020)
Schoeb et al. (2020)
Luzon et al. (2020)
Matsukawa and Yato (2020)

Yang et al. (2020)
Li et al. (2020b)
Kumar et al. (2020)
Li et al. (2020a)

Gibby et al. (2020)
Gu et al. (2020)
Galati et al. (2020)
Viehöfer et al. (2020)
Dennler et al. (2020)
Kriechling et al. (2020)
Zorzal et al. (2020)

Cartucho et al. (2020)
Rojas-Muñoz et al. (2020b)
Scherl et al. (2020)
Creighton et al. (2020)
Jiang et al. (2020)
Sun et al. (2020a)

HoloLens
HoloLens
HoloLens
PicoLinker
glasses
HoloLens
HoloLens
HoloLens
HoloLens

HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
Metavision
Meta 2
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens
HoloLens

PS
ST
SG (DG)
SG (SI)

Nephron-Sparing Surgery in Wilms’ Tumor Surgery
Urologic surgical procedures (bladder catheter placement)
Right colectomy with extended lymphadenectomy
Single-segment posterior lumbar interbody fusion

SG (NI)
SG (NI)
PS
SG (DG), PS,
TELC, ST
SG (NI)
SG (DTG)
SAA
SG (SNN)
SG (SI)
SG (KWI)
SG (EG)

SAA
TELM
SG (IO)
SG (IO)
SG (DG)
SG (IO)

Transjugular intrahepatic portosystemic shunt (TIPS)
Percutaneous needle interventions
Example use cases: laparoscopic liver resection and congenital heart surgery
Laparoscopic partial nephrectomy / Laparoscopic radical nephrectomy

Percutaneous image-guided spine procedures
Total shoulder arthroplasty
Open Abdomen Surgery
Hallux Valgus correction
Spinal instrumentation
Reverse total shoulder arthroplasty (RSA)
Laparoscopic procedures

N/A
Cricothyroidotomy
Surgery of the parotid gland
Lateral Skull Base Surgery
Perforator ﬂap transfer
Mandibular reconstruction

cial patient data available without the need to look at
a separate monitor. Deib et al. (2018) displayed radio-
graphic images to aid percutaneous vertebroplasty, kypho-
plasty and discectomy procedures. Qian et al. (2018) used
an endoscopy visualisation in the form of a 3D plane with
video streaming content that aimed to increase the ﬁrst
assistant’s task performance in robot-assisted laparoscopic
surgery. Fotouhi et al. (2019a) explored potential beneﬁts
of holgraphic C-arm interventional X-ray images registered
to the C-arm view frustrum for guided k-wire placement
in fracture care surgery.

66

s
e
l
c
i
t
r
a
#

60

50

40

30

20

10

0

Preoperative Model
Intraoperative Model
Intraoperative Image
Preoperative Image
Intraoperative live streaming video
Intraoperative Numerical Data
2D plane with video communication
Preoperatively recorded video
Documents

13

12

11

5

3

2

2

1

PM

IM

II

PI

IV

IND COMM PV

DOC

Fig. 15: Distribution of included articles by type of AR visualisation

The standard method of viewing preoperative images on
a separate monitor away from the surgical site is often cited
as a reason for pursuing AR guidance. Holographic visu-
alisation of preoperative images (n = 5) was proposed to
allow visualisation on or near the surgical site. Song et al.
(2018) incorporated 2D radiographic images with guidance

information in their HoloLens-based endodontic treatment
approach. Rynio et al. (2019) used 2D images with vol-
ume rendering, arterial diameters and planning notes to
support endovascular aortic repair.

The remaining categories of AR visualisations we iden-
tiﬁed in this review have only a few applications. Intra-
operative live video streaming (n = 5) is mostly used in
telementoring applications. Ponce et al. (2014) used a hy-
brid image approach in which the mentee’s surgical ﬁeld
is combined with the hands of the remote expert surgeon.
Dickey et al. (2016) presented an application in which an
interactive video display is visible to the mentee that shows
a cursor moved by the supervising physician. Intraopera-
tive numerical data (n = 3) is usually displayed as a 2D
plane containing numerical data that aid clinical decision
making or surgical guidance. Pietruski et al. (2019) dis-
played a cutting guide deviation coordinate system sup-
porting a surgeon during mandibular resection. Schlosser
et al. (2019) implemented a patient monitoring applica-
tion comprising a holographic 2D screen that shows pa-
tient heart rate, blood pressure, blood oxygen saturation
and alarm notiﬁcations. Another AR visualisation cate-
gory uses a 2D plane with video communication software
(n = 2) and has been applied in reconstructive limb salvage
procedures (Armstrong et al., 2014) and orthopaedic pro-
cedures (Jalaliniya et al., 2017). Preoperatively recorded
video (n = 2) was explored by Dickey et al. (2016) as
a video guide during surgical training. Armstrong et al.
(2014) used holographic visualisation of documents (n =
1), with articles from a senior author being displayed in
the surgical ﬁeld of view.

7 Validation of AR

All papers included in this review perform some kind of
experiments to verify usability and the associated poten-
tial utility of their proposed OST-HMD assisted surgery

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

13

solution. In this section we analyze the conducted experi-
ments including a categorisation into an either quantitative
or qualitative evaluation. An overview can be found in in
the Experiments column of table A1.

7.1 Experimental setting

43

45

40

35

30

25

20

15

10

5

0

21

19

Phantom Experiment
System Setup Experiment
Patient Case Study
Simulator Experiment
Animal Experiment
Human Cadaver Experiment
Animal Cadaver Experiment
Simulated Clinical Environment

8

5

4

1

1

PE

SSE

PS

SE

AE

HCE

ACE

SCE

Fig. 16: Experimental setting, from phantom to animal to clinical stud-
ies. Phantom studies dominate and though a number of clinical case
studies have been reported (19), we are some way from proving clinical
eﬀectiveness of OST-HMDs at present.

Phantom experiments dominate the list of papers (n =
43). Phantoms may be stylistic or try to mimic anatomical
correct structures and are either self-made, 3D printed or
acquired from specialised companies. Researchers can test
their developed methods on phantoms without involving
real human or animal anatomy. Chen et al. (2015) used
a 3D-printed cranio-maxillofacial model to verify the reg-
istration accuracy of their presented surgical navigation
system, and a 3D pelvis model to test their navigation
system. Deib et al. (2018) incorporated a lumbar spine
phantom into the validation of their presented application
for image guided percutaneous spine procedures. A guid-
ance approach for pedicle screw placement, developed by
Gibby et al. (2019), was tested using a phantom consisting
of L1-L3 vertebrae in opaque silicone that mimics tissue
properties.

System setup experiments (n = 21) don’t use realistic
target anatomy structures but verify the system’s intrin-
sic characteristics by conducting accuracy experiments in
speciﬁc areas, such as system registration and calibration.
Andress et al. (2018), for example, test the calibration step
of their presented OST-assited ﬂuoroscopic x-ray guidance
system that uses a multimodal ﬁducial. The calibration
experiment consists only of a HoloLens, a C-arm and a
multimodality marker. Fotouhi et al. (2019a) conducted
a similar experiment incorporating a hand-eye calibration
experiment including a HoloLens, a C-arm and an opti-
cal tracker in their system that provides spatially aware

surgical data visualisation. In order to verify the calibra-
tion accuracy of their proposed online calibration method
for the HoloLens, Guo et al. (2019) used a calibration box
with visual markers, a tracking device based on computer
vision.

Patient case studies (n = 19) present surgical procedures
that were tested on one or more patients. Yoon et al.
(2017) validated their OST-assisted spine instrumentation
approach in which neuronavigation images were streamed
onto a Google Glass on 10 patients. Mitsuno et al. (2017)
tested their intraoperative body surface improvement ap-
proach on 8 patients, each with a diﬀerent diagnosis. These
clinical evaluations are very useful, but further studies will
be required to establish clinical eﬀectiveness and to demon-
strate improved patient outcome.

The remaining ﬁve types of experiments that have been
identiﬁed in this review have a comparatively small num-
ber of associated articles. Simulator experiments (n = 8)
take advantage of available simulation hardware allowing
researchers or surgeons to mimic speciﬁc surgical proce-
dures. Mahmood et al. (2018) used a physical simulator
model (Fig. 12, section 5.2) that allows users wearing a
HoloLens to simulate a transesophageal echocardiography
(TEE) examination.

Animal experiments (n = 5) involve living animals that
are anaesthetised and enable surgeons to test surgical ap-
plications under realistic conditions that consider physi-
ological aspects such as respiratory motion. Zhou et al.
(2019b) and Zhou et al. (2020) tested their surgical nav-
igation system for LDR brachytherapy on a live porcine
model (Fig. 9 (c), section 5). Li et al. (2019) performed a
similar in vivo test of their respiratory liver tumor punc-
ture navigation system that takes respiratory liver motion
into account. An animal cadaver experiment (n = 1) was
also performed by Katić et al. (2015), who used a pig ca-
daver to test their application for intraoperative guidance
in dental implant surgery.

Human cadaver experiments (n = 4) have the inherent
beneﬁt of allowing surgeons to test novel surgical proce-
dures on real anatomic structures without risk to patients.
Wang et al. (2016), for example, used six frozen cadav-
ers with intact pelvises to investigate a novel method for
insertion of percutaneous sacroiliac screws.

Finally, Jalaliniya et al. (2017) proposed a simulated
clinical environment (n = 1) to test clinical infrastructure
elements and workﬂows rather than surgical procedures. A
Google Glass based wearable personal assistant that allows
surgeons to use a videconferencing application, visualise
patient records and enables touchless interaction with pre-
operative X-ray and MRI images displayed on a separate
screen without the need to use mouse or keyboard. The
application was tested in diﬀerent clinical setups compris-
ing a simulation doll, human actors and real surgeons and
nurses.

14

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

7.2 Evaluation methods

Evaluations may be quantitative experiments that collect
measurable data such as registration accuracy or quali-
tative experiments gather descriptive information such as
surgeons’ observations or opinions that cannot be mea-
sured. Most of the articles in this review contain some
sort of quantitative experiments (n = 88), whereas quali-
tative experiments have much fewer associated articles (n
= 11).

Quantitative experiments include registration accuracy
evaluation (Chen et al., 2015; Condino et al., 2018; Gibby
et al., 2019)), calibration accuracy evaluation (Andress
et al., 2018; Fotouhi et al., 2019a; Qian et al., 2018)) or
intraoperative guidance veriﬁcation such as tool position-
ing (Stewart and Billinghurst, 2016) or guide wire place-
ment (Liebmann et al., 2019). Experiments in which a user
has to give speciﬁc survey-based feedback is also classed as
quantitative where the survey is predetermined and can be
evaluated on a numerical basis.

Qualitative experiments are usually questionnaire based
in which the participants detail speciﬁc observations that
cannot be evaluated numerically. Deib et al. (2018), for
example, designed an experiment in which the user had to
complete a questionnaire following a surgical image-guided
spine procedure describing beneﬁts, limitations and per-
sonal preferences.

8 Registration and tracking in sur-

gical AR

Whenever holographic anatomy visualisations need to be
superimposed on respective patient anatomy, the question
of registration accuracy arises. Registration refers to the
establishment of a spatial alignment between the coordi-
nate system of the patient space and the digital image
space (Liu et al., 2017).
In the context of AR-guided
surgery it can be deﬁned as achieving correspondence be-
tween superimposed visualisation and patient anatomy.
Devices such as the HoloLens deﬁne their own coordinate
system for the room and the user’s head is tracked within
this space. The registration process places the preopera-
tive model in HoloLens coordinates. If an external tracking
system is used a further alignment between the devices is
required. Tracking and registration each have potential
errors and should be considered separately.

In most cases a rigid coordinate system transformation
involving translation and rotation is optimised given some
corresponding features (Wyawahare et al., 2009). The re-
quired accuracy of the established registration depends on
the application. For OST-HMD AR-guided procedures
deviations between visualisation and true target anatomy
may lead to surgical errors resulting from misinterpreted
spatial relationships. For OST-HMD AR, overall accuracy
also depends on the user’s perceptual accuracy.

Appendix table A1 lists the main reported accuracy re-
sults of all included articles and the associated type of con-

ducted experiment or experiments. Because of wide varia-
tion in experimental setup and diﬀerent accuracy metrics
used in the literature, direct comparison of articles based
on the reported accuracy is diﬃcult. Some articles report
speciﬁc registration accuracy experiments (Chen et al.,
2015; Gibby et al., 2019; Li et al., 2019; Nguyen et al.,
2020; Heinrich et al., 2019), while others report the ac-
curacy of speciﬁc experimental guidance task results that
result from a preceding registration (Wang et al., 2016;
Stewart and Billinghurst, 2016; Lin et al., 2018; Hiranaka
et al., 2017).

A number of papers consider manual alignment of the
virtual model by the surgeon for registration. When
matching corresponding features the most common meth-
ods are point-based landmark registration and surface reg-
istration Liu et al. (2017).

8.1 Manual alignment

Pratt et al. (2018) propose manual registration for extrem-
ity reconstruction using the HoloLens. Manual registration
directly aligns the model to the HoloLens coordinate sys-
tem, so no further tracking calculation is required. Also,
since the alignment is achieved by the user’s and to their
satisfaction, no correction for individual’s 3D perception
is needed. Fotouhi et al. (2020) propose manual regis-
tration for virtual-to-real alignment of a robotic arm that
uses two reﬂective AR displays (Fig. 10 (b)). The re-
ﬂective AR displays act as holographic mirrors that allow
the ﬁrst assistant to see the virtual robot arm from mul-
tiple perspectives and therefore act as a registration aid.
Experiments showed that using the reﬂective AR displays
improved the accuracy from 30.2 ± 23.9 mm to 16.5 ± 11.0
mm. Nguyen et al. (2020) compared three manual regis-
tration methods for neuronavigation using the HoloLens:
tap to place, 3-point correspondence matching and key-
board control. The authors also presented a novel statistics
based method allowing researchers to quantify registration
accuracy for AR-assisted neuronavigation approaches. The
keyboard method was found to be the most accurate (for
detailed accuracy results see appendix table A1).

Frantz et al. (2018) presented a neuronavigation ap-
proach which is based on manual registration using ﬁducial
markers. Users can manually register a holographic visual-
isation of a 3D reconstructed CT scan human skull model
to its physical counterpart via the help of virtual axes (Fig.
19 (a)). Registration accuracy was measured by both lo-
calisation accuracy (Fig. 19 (b)) and perceived holographic
drift (Fig. 19 (c)). The mean perceived holographic drift of
the manual registration was 4.39 ± 1.29 mm. Maintaining
hologram registration via continuous tracking of a marker
resulted in a lower perceived hologram drift of 1.4 ± 0.67
mm.

These manual methods may not be of suﬃcient accuracy
to meet the clinical requirements for guidance of surgical
dissection, but the ability to orient structures can improve
spatial awareness and may be useful in broader surgical
decision making.

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

15

(a)

(b)

Fig. 17: Accuracy veriﬁcation experiment examples using optical trackers: (a) Accuracy veriﬁcation block including a metal base with taper holes
(for distance and angular error measuring) and 3D-printed cranio-maxillofacial model. (b) A user is conducting the accuracy veriﬁcation experiment
using the accuracy veriﬁcation block, a tracked calibration tool and tracked OST-HMD (source: Chen et al. (2015)Fig. 7 and 8(c)). Registration
accuracy validation using a 3D-printed skull with 10 landmarks (red dots) and a k-wire with attached optical marker (source: Li et al. (2019) Fig. 6)

(c)

8.2 Point-based registration

Point-based registration matches corresponding pairs of
ﬁducial points from one coordinate system to another. Ex-
ternal ﬁducial markers may be attached to speciﬁc patient
anatomy, such as bony structures in orthopaedic surgery or
the skull in neurosurgery. Alternatively existing anatom-
ical landmarks may be used. The same virtual ﬁducial
points are usually marked using an external tracking device
and can also be displayed on the holographic 3D anatomy
model. A common accuracy measure for point-based meth-
ods is the ﬁducial registration error (FRE), which is the
residual error of the mismatch between pairs of correspond-
ing points after alignment. A better metric with more clin-
ical relevance is the target registration error (TRE) at the
surgical target (Seginer, 2011).

Chen et al. (2015) perform point-based registration as
an initial alignment before surface-based reﬁnement (sec-
tion 8.3) and conducted an accuracy experiment using a
veriﬁcation block (Fig. 17 (a)) using an optical tracking
system with reﬂective markers 17 (b)). The authors re-
ported mean distance and angular errors of 0.809 ± 0.05

mm and 1.038◦ ± 0.05◦ respectively. Addressing the prob-
lem of incorrect needle placement and associated failed tu-
mor ablation, Li et al. (2019) proposed a manual registra-
tion method using a HoloLens with an optical tracker to
superimpose 3D liver models on patients for liver tumor
puncture navigation. Optical markers rigidly attached to
the HoloLens, anatomical marks on the patient and a k-
wire with attached reﬂective spheres serving as an optical
marker are used for an initial manual registration step.
The tracked k-wire is then used for automatic temporal
registration during the procedure. The authors performed
a registration accuracy validation experiment using a 3D-
printed skull with 10 landmarks (Fig. 17 (c)) and reported
an average target registration error of 2.24 mm.

Another point-based registration approach for catheter
navigation was presented by Kuhlemann et al. (2017) and
tested on a human body phantom (Fig. 18 (a)): A CT
reconstructed 3D body surface mesh including marching
cubes segmentation of a vessel tree was registered to a
body phantom using landmarks with a reported accuracy
of 4.34 ± 0.709 mm (FRE).

16

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

(a)

(b)

Fig. 18: (a) Point-based registration: Human body surface mesh including vessel tree registered to a phantom by landmarks, where surface registration
to the HoloLens surface failed (source: Kuhlemann et al. (2017) Fig. 1). (b) Surface registration result: Dummy Head with superimposed 3D CT
scan reconstruction of head and intracranial vasculature. HoloLens camera detection of the QR code provides tracking (source: Wu et al. (2018),
part of Fig. 9b)

(a)

(b)

(c)

Fig. 19: Registration accuracy veriﬁcation using a sheet of millimeter paper: (a) Manual and point-based registration: Virtual axes allow the user
to translate and rotate a human skull model in order to align it with a phantom. Fiducial markers serving as registration aids are present on both
the virtual model and the phantom. (b) Localisation accuracy measurement is realised by placing the tip of a stylus into the center of a holographic
ﬁducial marker. (c) By calculating the diﬀerence in similar points the perceived hologram drift is measured. (source: Frantz et al. (2018) Fig. 4a,
4b and 5a).

8.3 Surface registration

Point-based registration is an alignment process that
matches anatomical or ﬁducial landmarks. Surface reg-
istration oﬀers the possibility of alignment without spe-
ciﬁc ﬁducial markers. Using a laser range scanner or a
tracked probe, a point cloud is collected from the surface
of the patient’s target anatomy (e.g. the head) (Liu et al.,
2017). Another surface or point cloud is derived from the
image space and an algorithm is then used to match both
point clouds. Most surface registration methods require a
coarse manual or point-based registration step to place the
image-based point cloud must be placed close to the target
registration pose before the algorithm proceeds. Iterative
closest point (ICP) is a popular realisation of a surface
based registration and has been applied in several of our
selected articles.

tion. A CT scan derived body surface was matched to the
HoloLens surface mesh. But the HoloLens mesh resolution
was found to be too coarse. In addition, Frantz et al. (2018)
also reported that the HoloLens’ built-in spatial mesh and
simultaneous localisation and mapping (SLAM) system is
unsuitable for registration and subsequent tracking due to
the low vertex density and surface bias of the generated
mesh and uncertainty in the SLAM realisation. Wu et al.
(2018) presented an improved version of the ICP algorithm
for medical image alignment that aims to provide a global
optimum via a stochastic perturbation. A dummy head
alignment test revealed an average target registration er-
ror of < 3 mm. Fig. 18 (b) shows an example registration
result.

8.4 Other registration methods

The HoloLens internal tracking method produces a gen-
erated surface mesh and Kuhlemann et al. (2017) inves-
tigated whether this could be used for surface registra-

Other types of registration have also been explored. Liu
et al. (2019) applied a Fourier transformation based regis-
tration method in their intraoperative guidance approach

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

17

Table 3
Papers by tracking method

8.5.2 Tracking of markers using the OST-HMD

Tracking method totals

Tracking marker totals

device

External tracker
NDI Polaris
NDI EM/Aurora
OptiTrack
PST Base
VICON
Custom webcam tracker

19
11
4
1
1
1
1

Reﬂective spheres
EM

14
4

Coloured catheter segments

1

Tracking with HMD camera
HoloLens
Other

20 Optical Markers
17
3

AprilTag
Aruco
ARToolkit
Custom
Vuforia

18
1
1
3
4
9

Markerless tracking

11

for structural heart disease for transcatheter procedures.
The authors used a 3D reconstructed spine image and a
segmented spine from an intraoperative ﬂuoroscopy to cal-
culate a Fourier-based scale and rotational shift which was
then used to register the ﬂuoroscopic image to the respec-
tive 3D model of the spine. The Fourier based registration
achieved an accuracy of 0.42 ± 0.02mm.

A HoloLens speciﬁc marker-less automatic registration
method for maxillofacial surgery is presented by Pepe et al.
(2019). Their algorithm accesses the HoloLens’ built-in
RGB camera and extracts facial landmarks from the cam-
era’s video stream. Via known virtual-to-real world trans-
formations of the landmarks and spatial mapping informa-
tion from the HoloLens’ Spatial Mapping API, the algo-
rithm then computes the registration. The achieved aver-
age positioning error of the x, y, z axes was 3.3 ± 2.3 mm
y: -4.5 ± 2.9 mm and z: -9.3 ± 6.1 mm respectively.

8.5 Tracking

Having established a registration, any subsequent motion
of either the patient or the surgeon must be tracked to
maintain the alignment. A summary of tracking methods
is given in table 3.

8.5.1 Markerless tracking

The HoloLens inherently tracks the surgeon’s head and
providing the patient position is ﬁxed within the operat-
ing room, this may be a suﬃcient method in itself. Eleven
papers use only the HoloLens tracking and these are as-
sociated with the manual registration process described in
section 8.1. The advantage of this method is that no exter-
nal measurement device is required and no markers need
to be physically attached to the patient, hence the name
markerless tracking. This can be a signiﬁcant advantage
in terms of sterility, convenience and operative workﬂow
integration. However, the accuracy is user dependent and
may not be suﬃcient for some surgical tasks. Pratt et al.
(2018) and Scherl et al. (2020) use manual alignment to
the anatomy, whereas Creighton et al. (2020) register to
ﬁducial markers for guidance of targets in the skull base.

OST-HMD devices such as the HoloLens incorporate cam-
eras into their tracking process. These cameras can be
used to track surface features or markers placed in the sur-
gical ﬁeld, accounting for 20 of our papers. It is common
for these markers to be small planar identiﬁable markers
modelled on QR codes. Several quite similar free libraries
are available for this purpose, including Aruco, ARToolkit
and AprilTags. Andress et al. (2018) use ARToolkit mark-
ers that are also visible in X-ray to align to ﬂuoroscopic
views for orthopaedics. Liebmann et al. (2019) use the
stereo HoloLens camera sensors in research mode to track
planar sterile markers for pedicle screw navigation. Some
authors use their own custom markers, such as the cube
and hexagonal markers used by Zhou et al. (2019b) in their
system for brachytherapy. The commercial Vuforia pack-
age can also be used to track any planar printed image and
accounts for half of the marker-based tracking through the
OST-HMD (9 papers).

One advantage is that the OST-HMD camera’s position
is relative to the surgeon, so no extra registration is needed
and the direction of the camera shoud be towards the sur-
gical ﬁeld. While this can be eﬀective, the resolution and
ﬁeld of view of the cameras may not be best designed for
tracking within the surgical target area.

8.5.3 External tracking devices

There are several commercially available devices that are
It is
able to track markers within the operating room.
clear from table 3 that Northern Digital Inc. (NDI) domi-
nate this ﬁeld, with the Polaris optical tracker accounting
for 11 papers and their electromagnetic tracker, Aurora, a
further four papers. Li et al. (2019) use the Polaris for liver
biopsy in the presence of breathing, whereas Kuhlemann
et al. (2017) use EM tracking for endovascular interven-
tions. Other system are optical and account for one pa-
per each (OptiTrack, PST Base and VICON). Apart from
one custom tracker based on a webcam for catheter track-
ing (Sun et al., 2020b) all optical systems use passive re-
ﬂective spherical markers.

It may be invasive to attach such markers rigidly to the
patient, but such methods form part of several commer-
cial image guidance systems and this is probably the most
accurate way to achieve and maintain alignment.

9 Human factors

OST-HMDs are wearable technological devices that enable
the user to visualise and/or interact with 3D virtual ob-
jects placed within their normal view of the world. These
unfamiliar devices present a novel form of human-computer
interaction (HCI) and their acceptability by surgeons will
depend on HCI factors. Technological aspects, such as the
size of the augmented ﬁeld of view or system lag during
streaming of video content, can aﬀect user acceptance. But

18

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

beyond these are human factors that may vary from user to
user but are crucial to the utility of a technological inter-
action device. They encompass perceptual, cognitive and
sensory-motor aspects of human behavior that drive the
design of HCI interfaces to optimise operator performance
Papantoniou et al. (2016).

However, attempts to identify consistent generic human
factors that capture basic human behavior and cognition
that apply to the design of optical HCI systems has been
problematic and HCI design guidelines incorporating con-
sistent human factors have not yet been established. When
addressing the negative side eﬀects of HCI aspects only,
human factors are sometimes considered as human limita-
tions. Highlighting the aspect of human error, Lowndes
and Hallbeck (2014) addressed aspects of human factors
and ergonomics in the operating room in general with a
focus on MIS and found that most medical errors are a re-
sult of suboptimal system design causing predicable human
mistakes. They also state that despite eﬀorts made by hu-
man factors and ergonomics professionals to improve safety
in the operating room for over a century, increasingly com-
plex surgical procedures and advances in technology mean
that consideration of human interaction will be required to
help users cope with increasing information content. We
believe that similar safety aspects of human factors also
apply in OST-HMD assisted surgical applications.

9.1 Human factors in AR

In more general non-surgical AR applications, human fac-
tors have played an important role and have been explored
in the context of HCI. Livingston (2005) evaluated human
factors in AR in 2005 and found that apart from techno-
logical limitations, human factors are a major hurdle when
it comes to translation of AR applications from laboratory
prototypes into commercial products. To determine the
eﬀectiveness of AR systems requires usability veriﬁcation,
which led them to the following two research questions:
1.) How to determine the AR user’s key perceptual needs
and the best methods of meeting them via an AR inter-
face? 2.) Which cognitive tasks can be solved better with
AR methods than with conventional methods? They at-
tempt to a solution for these two questions by conducting
limited but well-designed tests aiming to provide insights
into HCI-design aspects that lead to utility for perceptive
and cognitive tasks. These consist of low-level perceptual
tests of speciﬁc designed visualisations on the one hand
and task-based tests that focus only on the well-designed
part of the user interface.
In Huang et al. (2012), Liv-
ingston points out that designing cognitive tasks for usabil-
ity evaluation seems to be easier than designing low-level
perceptual tasks, since cognitive tasks naturally arise from
the given AR application, whereas it is rather challeng-
ing to design general low-level perceptual tasks that have
wider applicability. He also states that the design of a per-
ceptual task determines how generalisable the evaluation
results are beyond the speciﬁc experimental scenario and
indicates that a solution may be to design general percep-

tual tasks that verify the usability of hardware. Finding
general perceptual tasks is not always easy when hardware
limitations interfere with the task design. If the eﬀect of
a hardware related feature inﬂuences a user’s cognition on
top of his/her perception, the dependence on the percep-
tual task will be increased as well. An example for such an
eﬀect is system latency in a tracking device.

9.2 HCI design considerations in OST-

assisted surgery

Though human factors in the context of HCI design con-
siderations in OST-HMD assisted surgery are likely to be
very important to the success of any system, only a few ex-
amples of such research can be found in the literature. In
addition to the need for careful experimental design that
allows a generalised result, there are also technical aspects
that can be addressed to minimise unwanted human behav-
ior when using OST-HMDs. Such technical aspects were
explored by Tang et al. (2003), who evaluated human fac-
tors in variants of the Single point active alignment method
(SPAAM) for OST-HMD calibration that require human-
computer interaction. They aimed to answer the question
why calibration of OST-HMDs is challenging for users; and
found that human factors have a major impact on cali-
bration error and therefore lead to signiﬁcantly diﬀerent
accuracy results for diﬀerent users. They proposed the fol-
lowing guidelines for the design of OST-HMD calibration
procedures: 1.) Calibration should not rely on head move-
ments only, 2.) The user’s head should be kept stabilised by
minimizing extrinsic body movements and 3.) Careful con-
sideration of the data collection sequence for the left and
right eye so that calibration error does not bias towards a
dominant eye.

Guo et al. (2019) also described the importance of hu-
man factors in the context of OST-HMD calibration. They
proposed an online calibration method for the HoloLens
and concluded that the accuracy of their calibration
method is diﬃcult to measure objectively since human fac-
tors impact the overall HCI experience as well as inﬂuenc-
ing the calibration accuracy.

The relationship between conscious and unconscious
cognitive processes should be considered as well when con-
sidering the importance of human factors in HCI. Jalaliniya
and Pederson (2015) addressed the necessity to consider an
egocentric interaction when designing wearable HCI sys-
tems and replaced the terms input and output with action
and perception. According to the authors, an improved un-
derstanding of a human’s perception, cognition and actions
are necessary prerequisite when it comes to the design of
a HCI system that oﬀers better cognitive support.

9.3 Human factors identiﬁcation

We identiﬁed numerous human factors in the 91 included
articles. Even though the majority of articles didn’t use the
term human factors explicitly, we included all user-related

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

19

Table 4
Identiﬁed Human Factors, grouped into the categories 1.) Information Perception, 2.) Cognitive Processing and 3.) Control Actions

Abbreviation

Human Factor

SPATIAL_PERC
INC
DPPC
EYE
COMF
PER_REAL_AUG
IMMR

Information Perception

Spatial perception/awareness
Inconvenience
Missing/impaired depth perception
Individually diﬀerent visual processing capabilities between dominant and non-dominant eye
Perceived comfort level when wearing OST-HMD
Perception of spatial relationships between real and virtual objects
Personal degree of perceived immersion

Cognitive Processing

ATTN_SHIFT
MM
SLC
EXP_OUTCOME
DIST
INTPN_2D_DETAIL
INTRA_OP_NAV
COMM_3D
CONF
FRUS
SUBJ_MEAS_OUTCOME
EASE_HCI
CLIN_EXP_2D
EMP_EST_2D
ANAT_PLN
CONC_LS
MIP
STRESS
ENG_MOT
PREF_HOL
USEF
ANX

Attention switch between surgical site and separate computer monitor
Error-prone and cognitively demanding mental mapping of 2D image data to 3D word
Steep learning curve
Inﬂuence of clinician’s experience on surgical outcome
Distraction
Risk of incorrect interpretation of 2D image details
Impaired intraoperative navigation abilities due to absence of visual aids
Personal 3D anatomical imagination capabilities aﬀect communication between experts
Conﬁdence
Frustration
Subjective measurement of surgical outcome
Perceived degree of ease and intuitiveness of HCI
Dependence on clinical experience for interpretation of 2D image data
Inaccurate empirical estimation of target locations in 2D anatomy images
Impaired anatomical understanding during preoperative planning due to 2D imaging data
Loss of concentration
Limited mental information processing abilities
Experience of stress
Engagement and motivation
Preferred degree of superimposition of 3D objects onto the surgical ﬁeld (precise vs shifted superimposition)
Perceived usefulness of OST-HMD
Anxiety

VIS_OPT
SURG
HEC
TOOL_ADJUST
FAT

Control Actions

Selection of preferred mode of visualisation
Increased risk of surgical error
Unfamiliar/cognitively demanding hand-eye coordination
Error-prone manual tool adjustment
Visual Fatigue

aspects described by the authors that have a potential im-
pact on the acceptance, utility and performance of surgery
with or without the proposed OST-HMD solution.

We identiﬁed 34 human factors that are described in
table 4. The human factors are grouped into two cate-
gories: 1.) Human factors of conventional surgery that are
addressed by OST-HMD AR and 2.) Persistent human
factors that remain or are an inherent part of the pro-
posed OST-HMD solutions. We further categorize them
into three phases of user interaction that are deﬁned by
the US Food and Drug Administration as part of a medi-
cal device user interface in an operational context (Center
for Devices and Radiological Health, 2016): 1.) Informa-
tion Perception (IP), where the information from the de-
vice is received by the user 2.) Cognitive Processing (CP),
where the information is understood and interpreted and
3.) Control Actions (CA), where this interpretation leads
to actions.

9.4 Human

factors

conventional
surgery that are addressed by OST-
HMD AR

of

Fig. 20 shows the distribution of all human factors (out
of the identiﬁed 34 ones) described as being a limitation
of conventional non-AR surgical methods that the authors
aimed to address with their proposed OST-HMD solution.
We group these into the categories IP, CP and CA and

40

30

20

20

10

0

)
P
I
(
C
R
E
P
_
L
A
T
A
P
S

I

1

1

1

1

)
P
I
(
C
N

I

)
P
I
(
E
Y
E

)
P
I
(
C
P
P
D

)
P
I
(

F
M
O
C

41

)
P
C
(
T
F
I
H
S
_
N
T
T
A

24

20

)
P
C
(

M
M

)
P
C
(
C
L
S

7

)
P
C
(
E
M
O
C
T
U
O
_
P
X
E

23

18

5

4

3

3

2

2

2

2

2

2

1

1

1

1

1

2

2

)
A
C
(
C
E
H

)
A
C
(

G
R
U
S

)
P
C
(

S
L
_
C
N
O
C

)
P
C
(
N
L
P
_
T
A
N
A

)
P
C
(
P
I
M

)
P
C
(

S
S
E
R
T
S

)
P
C
(
T
O
M
_
G
N
E

)
P
C
(

I
C
H
_
E
S
A
E

)
P
C
(
D
2
_
P
X
E
_
N
I
L
C

)
P
C
(
D
2
_
T
S
E
_
P
M
E

)
A
C
(
T
A
F

)
A
C
(
T
S
U
J
D
A
_
L
O
O
T

)
P
C
(

F
N
O
C

)
P
C
(

S
U
R
F

)
P
C
(
D
3
_
M
M
O
C

)
P
C
(
T
S
I
D

)
P
C
(
V
A
N
_
P
O
_
A
R
T
N

I

)
P
C
(

L
I
A
T
E
D
_
D
2
_
N
P
T
N

I

)
P
C
(
E
M
O
C
T
U
O
_
S
A
E
M
_
J
B
U
S

Fig. 20: Distribution of human factors of conventional non-AR surgical
approaches alleviated by the use of OST-HMD AR, grouped into the three
categories 1. Information Perception (IP), 2. Cognitive Processing (CP),
3. Control Actions (CA)

describe the most popular in more detail.

9.4.1

Information Perception related Human Fac-
tors of conventional surgery

Spatial perception/awareness (SPATIAL_PERC): A fun-
damental limitation of conventional image guidance meth-
ods appears to be the fact that crucial patient anatomy
can only be perceived in 2D and hence prevents the sur-

20

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

geon from developing a personal sense of spatial percep-
tion and spatial awareness, which is a human factor we
identiﬁed in (n = 20) articles and hence the dominating
human factor of conventional surgery in the IP category.
Impaired spatial awareness has the unwanted side eﬀect of
an increased likelihood of surgical errors due to misinter-
pretation of anatomical spatial relationships. Qian et al.
(2018) aim to increase a ﬁrst assistant’s spatial awareness
during robot-assisted laparoscopic surgery by providing a
HoloLens solution in which a holographic endoscopy visual-
isation is registered within the personal viewing frustrum.
Fotouhi et al. (2019a) addressed the problem of missing
spatial context when looking at C-arm X-ray anatomy im-
ages on an external 2D monitor and presented a spatially
aware HoloLens visualisation in which X-ray images are
displayed in the correct spatial position of the patient’s
anatomy with a surgeon’s view frustrum.

9.4.2 Cognitive Processing related Human Fac-

tors of conventional surgery

Attention switch between surgical site and separate com-
puter monitor (ATTN_SHIFT): The human factor with
the highest number of articles (n = 41) in the CP category
(and the dominating factor accross all three categories IP,
CP and CA) is the attention switch between the surgical site
In computer-assisted
and a separate computer monitor.
surgery a surgeon has to look away from the surgical site
in order to see patient anatomy or surgical navigation infor-
mation, and even switch between the surgical site and the
screen multiple times during an operation. This inability
to see both the surgical site and important patient anatomy
or guidance information at the same time causes unwanted
human behavior such as inconvenience and may also im-
pact the continuity of the surgery Chen et al. (2015). Espe-
cially during image-based surgical navigation, the surgeon
has to constantly switch his attention while manipulating
surgical navigation tools which comes with unwanted side
eﬀect such as unfamiliar hand-eye coordination, distraction
and loss of concentration Wang et al. (2016).

Mental Mapping of 2D image data to the 3D World
(MM): An inherent problem of conventional computer-
assisted surgery is that patient imaging data and surgical
navigation information is displayed in 2D. This leads to
the fact that the surgeon has to mentally map (or project)
2D image data onto the 3D world in order to translate the
information seen on the 2D screen to the patient or surgi-
cal navigation tool. We identiﬁed mental mapping of 2D
image data to the 3D world in (n = 24) articles. Andress
et al. (2018), for example, addressed the problem of men-
tal mapping in the context of intraoperative guidance in
percutaneous orthopaedic surgical procedures in which a
surgeon has to place tools or implants precisely under C-
arm based ﬂuoroscopic imaging. The mental projection is
counterintuitive and error-prone as a result of high mental
workload and mental projective simpliﬁcation.

Steep learning curve (SLC): Some conventional surgical
procedures, especially those related to image guidance, re-

quire surgeons to overcome a Steep Learning Curve (n =
20) due to the method’s inherent complexity. Lin et al.
(2018) address this problem in needle guidance procedures
that require considerable learning eﬀorts due to the fact
that physicians have to recover 3D information from 2D
images, that the needle may cause artifacts in the images
which hinder correct identiﬁcation of needle tip and tar-
get and that complex hand-eye coordinationis required to
register the 2D images seen on a separate monitor to the
patient anatomy Lin et al. (2018). Their OST-HMD AR
system aims to reduce this learning curve.

9.4.3 Control Action related Human Factors of

conventional surgery

Increased risk of surgical error (SURG): Several re-
searchers addressed the risk of surgical error (n = 23) which
appears to be a common problem in some conventional
image-guided procedures. El-Hariri et al. (2018) highlights
the fact that conventional surgical navigation systems can-
not observe the surgical scene and the external navigation
computer monitor at the same time as being a potential
problem that OST-HMD based solutions aim to solve. In
another example, Song et al. (2018) aim to prevent or re-
duce errors in root canal treatments such as accidental per-
foration during access cavity creation.

Unfamiliar hand-eye coordination (HEC): The fact that
a surgeon has to look away from the surgical site to a sep-
arate screen (see section 9.4.2) while simultaneously ma-
noeuvring surgical tools in image guided navigation causes
unfamiliar hand-eye coordination because the surgeon can-
not see his hands while looking on the separate screen
Wang et al. (2016). Unfamiliar hand-eye coordination is
tackled in (n = 18) articles. Qian et al. (2018), for ex-
ample, addressed a ﬁrst assistant’s impaired hand-eye co-
ordination during blind placement of robotic and hand-
held instruments in conventional robot-assisted laparo-
scopic surgery by registering the holographic endoscopy
visualisation with the visualised endoscope view frustrum
(see Fig. 10 (a) of section 5). Another example is given by
Deib et al. (2018) who mention that a fundamental prob-
lem of conventional image guided percutaneous spine lies
in an indirect guidance visualisation because radiography
monitors showing ﬂuoroscopic images are not aligned with
the surgical site, which in turn hinders hand-eye coordina-
tion.

9.5 Persistent human factors of proposed

OST-HMD solutions

Since OST-HMDs expose the user to new and possibly un-
familiar visual perception and interpretation as well as in-
teraction options, the proposed OST-HMD solutions also
introduce new human factors that should be taken into ac-
count when designing eﬀective HCI. We refer to these as
persistent human factors because they remain as issues of
the proposed OST-HMD based solution. Table 21 shows
the distribution of persistent human factors, some of which

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

21

we discuss in the following sections. Analogous to section
9.4, the human factors are grouped into the categories IP,
CP and CA and the most popular are detailed.

20

15

10

5

0

15

14

)
P
I
(

F
M
O
C

)
P
I
(
C
R
E
P
_
L
A
I
T
A
P
S

7

)
P
I
(
C
P
P
D

4

)
P
I
(

G
U
A
_
L
A
E
R
_
R
E
P

20

7

4

3

1

1

1

1

)
P
I
(
C
N

I

)
P
I
(
E
Y
E

)
P
I
(
R
M
M

I

)
P
C
(

L
O
H
_
F
E
R
P

)
P
C
(

I
C
H
_
E
S
A
E

2

2

2

2

1

1

1

1

1

)
P
C
(

F
E
S
U

)
P
C
(

F
N
O
C

)
P
C
(

S
U
R
F

)
P
C
(
C
L
S

)
P
C
(
T
S
I
D

)
P
C
(

S
S
E
R
T
S

)
P
C
(
D
3
_
M
M
O
C

)
P
C
(
T
O
M
_
G
N
E

)
P
C
(
X
N
A

)
P
C
(

S
L
_
C
N
O
C

)
P
C
(

M
O
C
T
U
O
_
P
X
E

)
P
C
(
E
M
O
C
T
U
O
_
S
A
E
M
_
J
B
U
S

6

)
P
C
(
T
P
O
_
S
I
V

3

)
A
C
(
T
A
F

1

)
A
C
(

G
R
U
S

Fig. 21: Distribution of persistent human factors of the proposed AR
surgical approaches, grouped into the three categories 1.
Information
Perception (IP), 2. Cognitive Processing (CP), 3. Control Actions (CA)

9.5.1

Information Perception related Human Fac-
tors of AR-assisted surgery

comfort

level when wearing OST-HMD
Perceived
(COMF): As is the case for all HCI devices includ-
ing computers or laptops, one of the most important
factors that inﬂuence user acceptance is the comfort level.
Discomfort will inevitably prevent a device from becoming
a routine instrument that users enjoy working with.
Perceived comfort
level when wearing OST-HMD was
mentioned in (n = 15) articles, and is therefore one of the
human factors that dominate the IP category. Pietruski
et al. (2019) presented a Movierio BT-200 Smart Glasses
based intraoperative navigation system that supports
mandibular resection and conducted a phantom exper-
iment in which osteotomies were performed. Surgeons
reported good long term wear work ergonomics. Rojas-
Muñoz et al. (2020a) created a HoloLens telementoring
system that allows surgeons to perform mentored leg
fasciotomies. Participants reported that the weight of
the HoloLens has a negative impact on their posture and
comfort.

Spatial perception/awareness (SPATIAL_PERC): Spa-
tial perception and spatial awareness was already described
in section 9.4 as a human factor of conventional non-AR
methods, where 3D patient anatomy had to be inferred
from 2D data. However, these spatial processing capa-
bilities area also factors of 3D holographic visualisations
and should also be taken into account for OST-HMD so-
lutions, as reported in (n = 14) articles. Given that AR
exposes users to new perceptual stimuli that are usually
not part of their normal experience, it is likely that users
processes this new visual information diﬀerently, which in
turn impacts the quality of the HCI during OST-HMD as-
sisted surgical procedures. Condino et al. (2018) presented

a HoloLens based hybrid simulator for orthopaedic open
surgery that allows users to visualise 3D anatomy prior to
performing a virtual viewﬁnder assisted surgical incision.
Study participants who conducted a simulator experiment
were engineers and clinicians. Results from a 5-point Lik-
ert questionnaire indicate that both user groups found it
rather easy to perceive spatial relationships between real
and virtual content; however, engineers tend to rate the
ease of spatial relationship perception slightly higher than
clinicians. Gnanasegaram et al. (2020) investigated in how
far ear anatomy learning can be improved compared to con-
ventional didactic lectures and computer modules. Study
participants performed a spatial exploration of holographic
ear models displayed on a HoloLens and rated the OST-
HMD higher than didactic lectures and computer modules
in terms of 1.) overall learning eﬀectiveness, 2.) the learn-
ing platform’s ability to convey anatomic spatial relation-
ships and 3.) learner engagement and motivation.

Missing/impaired depth perception (DPPC): Individual
depth perception capabilities inﬂuence the ability to under-
stand three dimensional holographic relationships as well
as relationships between real and virtual objects. This may
decrease the utility of systems that require perceptual pre-
cision. Several articles indicate missing or impaired depth
perception as one of the limitations of the proposed OST-
HMD approach (n = 7). Andress et al. (2018) developed
a ﬂuoroscopic X-ray guidance system for percutaneous or-
thopaedic surgery that is based on a cocalibration of a
C-arm to a HoloLens and aims to facilitate the percep-
tion of spatial relationships between patient anatomy and
surgical tools. A phantom based K-wire insertion exper-
iment revealed that the HoloLen’s build-in characteristic
of rendering all holographic content at a focal distance of
around 2m impacts the the user’s depth perception, and
hence leads to an impaired interaction between real and
virtual objects.

9.5.2 Cognitive Processing related Human Fac-

tors of AR-assisted surgery

Perceived degree of ease and intuitiveness of HCI
(EASE_HCI): A fundamental aspect that plays a piv-
otal role when it comes to user acceptance of a proposed
OST-HMD solution is the perceived degree of ease and in-
tuitiveness of HCI which was the human factor with the
most associated articles (n = 20) in the CP category. Deib
et al. (2018) presented a HoloLens based application for im-
age guided percutaneous spine procedures that was tested
in a phantom experiment in which percutaneous verte-
broplasty, kyphoplasty and discectomy interventions were
performed. Participants could select their preferred holo-
graphic visualisation mode and questionnaire results re-
vealed that initially the most popular mode was the option
that was closest to a conventional 2D monitor and hence
the most intuitive one. However, after the user became
familiar with the OST-HMD environment, the preferred
mode of visualisation changed to one that oﬀers more ben-
eﬁts of the new mixed reality environment. Jalaliniya et al.

22

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

(2017) designed a Google Glass based personal assistant
for surgeons for which users who conducted experiments
had to complete a questionnaire which revealed that some
users prefer hand gesture interaction over voice interaction
because voice interferes with their patient communication.
Perceived usefulness of OST-HMD (USEF): Since OST-
HMDs are not well established in operating theatres and
part of routine surgical procedures clinicians can always
compare OST-HMD solutions with conventional methods
and thus decide for themselves whether the new AR ap-
proach is useful or not. It is therefore not surprising that
perceived usefulness is a human factor mentioned in sev-
eral articles (n = 7). Borgmann et al. (2016) conducted
a feasibility of Google Glass assisted urological procedures
in which surgeons could access holographic preoperative
CT scans. A patient case study with a ﬁve-point Likert
scale evaluation involving 7 surgeons over 10 procedures
totalling 31 procedures revealed that the system’s overall
usefulness was rated as very high or high by 74% of the
surgeons.

9.5.3 Control Action related Human Factors of

AR-assisted surgery

Selection of preferred mode of visualisation (VIS_OPT):
Sometimes users are given the possibility to optimize their
HCI experience by selecting one out of several diﬀerent
modes of visualization. The Selection of preferred mode of
visualisation (VIS_OPT) considers personal optimisation
of the HCI (n = 6) is the human factor with the most as-
sociated number of articles (n = 6) in the CA category.
An example is described in Qian et al. (2018): a ﬁrst as-
sistant has the option between two modes of endoscopy
visualization- during robotic surgery: 1.) A holographic
monitor capturing the endoscopy camera stream or 2.) an
endoscopy visualization that is registered with the viewing
frustrum (Fig. 10 (a)).

10 Discussion

In this review we summarise the current proposed applica-
tions of OST-HMDs in surgery. Orthopaedic surgery ap-
plications are the most popular (30.16%) and are mainly
assisted intraoperative guidance applications, perhaps be-
cause it involves rigid bony structures and is a ﬁeld where
conventional guidance systems to achieve good implant
alignment have become commonplace.

Image guidance, where a preoperative segmented imag-
ing model is aligned to the operative view, is the dominat-
ing application across several surgical specialities. When
providing such guidance and navigation, safety and accu-
racy becomes crucial. We summarised the achieved accu-
racy results in section 8 and noted that there is consid-
erable variation in the reported results, which is related
to large variation in terms of conducted experiments and
diﬀerent accuracy measures. Registration can be achieved
manually or by identiﬁcation of point or surface features

using an external tracking device. There is no general so-
lution to the problem of registration as yet.

The most common visualisation is of preoperative mod-
els. When these are generated from a preoperative scan
this requires a segmentation process that must be incorpo-
rated into the surgical planning workﬂow. Medical image
segmentation is a huge research area in its own right, with
great progress being made. Though this is a vital compo-
nent of image guidance, we have chosen not include it in
this review of OST-HMD AR.

Beyond surgical guidance, other surgical application
contexts where accuracy of superimposed holographic con-
tent may be less important have been analyzed in this re-
view, such as preoperative planning or surgical training.
Due to the variety of surgical contexts, diﬀerent AR visu-
alisations have been used, such as preoperative models, in-
traoperative images and intraoperative streaming of video,
which all serve diﬀerent purposes and are rated diﬀerently
by users in terms of their usefulness.

Phantom experiments dominate, underlining the fact
that many such systems are some way from clinical use.
Aside from technological limitations, human factors have
a major inﬂuence on the establishment of OST-HMD as-
sisted applications in the operating room. Attention shift
between the surgical site and an external computer moni-
tor is the dominating human factor researchers aim to solve
with OST-HMD solutions. These devices lead to other hu-
man factor issues, however, such as impaired hand-eye co-
ordination and increased cognitive load that may increase
rather than decrease the risk of surgical errors.

10.1 Human factor classiﬁcation

The presented human factors of this review reﬂect our at-
tempt to identify human user’s individual HCI character-
istics in context of OST-HMD assisted surgery, providing
an overview of perceptual and HCI related human charac-
teristics that may impact the utility of a proposed novel
AR-assisted system. Given that OST-HMD based surgi-
cal applications have not replaced respective conventional
state of the art methods yet, we feel there is a need to in-
crease awareness of all aspects that may inﬂuence the end
user’s acceptance of a novel technology being introduced
in the operating room. Despite addressing several human
factors, OST-HMD based solutions also expose the user to
new human factors that may hinder an acceptance of this
novel technology in the operating room.

The dominating persistent human factor is the perceived
degree of ease and intuitiveness of HCI. These new HCI
possibilities may reveal individual performance diﬀerences
and user preferences even more than conventional com-
puter assisted surgical methods. Overall, it appears that
the combination of OST-HMD device, surgical speciality,
surgical application context, surgical procedure, proposed
AR visualisation and conducted experiments triggers dif-
ferent individual human HCI responses that lead to vari-
ation in individual perceived utility. Some attempts have
been made to provide standardised analysis in image guid-

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

23

Table 5
Addressed and persistent human factors (notation: human factor(s) on the left side of the arrow cause other human factor(s) on the right side of
the arrow).

Study

Addressed Human Factors

Reported Persistent Human Factors

Lin et al. (2018)
Qian et al. (2018)

Chen et al. (2015)
Wang et al. (2016)
Deib et al. (2018)
Andress et al. (2018)
Condino et al. (2018)

MM, INTPN_2D_DETAIL, HEC, SLC
ATTN_SHIFT → HEC, TOOL_ADJUST, DPPC,
EXP_OUTCOME, SPATIAL_PERC
ATTN_SHIFT → INC
[ATTN_SHIFT → HEC, DIST, CONC_LS], SLC
ATTN_SHIFT → HEC
ATTN_SHIFT → DIST, MM, SLC
SUBJ_MEAS_OUTCOME, SLC

et

Stewart and Billinghurst
(2016)
Gibby et al. (2019)
Yoon et al. (2017)
de Oliveira et al. (2019)
Aaskov et al. (2019)
Liebmann et al. (2019)
El-Hariri et al. (2018)
Fotouhi et al. (2019a)
Meulstee et al. (2019)
Song et al. (2018)
Mitsuno et al. (2017)
Pratt et al. (2018)
Brun et al. (2019)
Li et al. (2017)
Zou et al. (2017)
Liu et al. (2019)
Pietruski et al. (2019)
Kaneko et al. (2016)
Kuhlemann et al. (2017)
Karmonik et al. (2018)
Frantz et al. (2018)
Fotouhi et al. (2020)
Hiranaka et al. (2017)
Katić et al. (2015)
Borgmann et al. (2016)
Unberath et al. (2018)
Sauer et al. (2017)
Armstrong et al. (2014)
Mahmood et al. (2018)
Rojas-Muñoz et al. (2019)
Rojas-Muñoz
al.
(2020a)
Pelanis et al. (2020)
Nguyen et al. (2020)
Zhou et al. (2019b)
Pietruski et al. (2020)
Chien et al. (2019)
Zhang et al. (2019)
Heinrich et al. (2019)
Zhou et al. (2020)
Wellens et al. (2019)
Fotouhi et al. (2019b)
Baum et al. (2020)
Liounakos et al. (2020)
Jalaliniya et al. (2017)
Rynio et al. (2019)
Boillat et al. (2019)
Zhou et al. (2019a)
Schlosser et al. (2019)
Ponce et al. (2014)
Guo et al. (2019)
Dickey et al. (2016)
Al Janabi et al. (2020)
Li et al. (2019)
Pepe et al. (2019)
Wu et al. (2018)
Liebert et al. (2016)
Gnanasegaram et
(2020)
Sun et al. (2020b)
Park et al. (2020)
Mendes et al. (2020)
Laguna et al. (2020)
Dallas-Orr et al. (2020)
Zafar and Zachar (2020)
Fitski et al. (2020)
Schoeb et al. (2020)
Luzon et al. (2020)
Matsukawa
(2020)
Yang et al. (2020)

and Yato

al.

ATTN_SHIFT → DIST

ATTN_SHIFT → HEC
ATTN_SHIFT, HEC
ATTN_SHIFT, SLC
[INTPN_2D_DETAIL → SURG]
INTRA_OP_NAV
[ATTN_SHIFT → SURG_ERR], [MM → SLC]
[ATTN_SHIFT → HEC], [MM → FRUS], SPATIAL_PERC
ATTN_SHIFT, MM
[ATTN_SHIFT → SLC & SURG_ERR], DPPC
ATTN_SHIFT, SUBJ_MEAS_OUTCOME
[INTPN_2D_DETAIL → SURG_ERR], DPPC
DPPC, COMM_3D, MM
EMP_EST_2D, EASE_HCI, CLIN_EXP_2D
EMP_EST_2D, EASE_HCI
[DPPC, INTPN_2D_DETAIL → INTRA_OP_NAV]
[ATTN_SHIFT → HEC], DPPC, SPATIAL_PERC
[ATTN_SHIFT → HEC]
MM
COMM_3D
MM, ATTN_SHIFT
SPATIAL_PERC, TOOL_ADJUST, SLC
[ATTN_SHIFT → SURG_ERR]
STRESS, MIP, [ATTN_SHIFT → ERG, SURG_ERR]
N/A
MM, INTRA_OP_NAV
MM, [ATTN_SHIFT → HEC], SPATIAL_PERC
SLC
SLC, MM, SPATIAL_PERC
ATTN_SHIFT, MM, FRUS, DPPC
CLIN_EXP_2D, SLC, HEC, EXP_OUTCOME

MM, SPATIAL_PERC
[ATTN_SHIFT → SURG]
MM, SLC
MM, ATTN_SHIFT
ATTN_SHIFT
[ATTN_SHIFT → MM, HEC], SPATIAL_PERC
ATTN_SHIFT, MM
ATTN_SHIFT, SURG
[ANAT_PLN → SURG]
MM
MM, EXP_OUTCOME, SLC, SPATIAL_PERC
[ATTN_SHIFT → HEC]
ATTN_SHIFT
SPATIAL_PERC
SURG
SURG, ATTN_SHIFT, HEC
DIST, FAT
SLC
EYE, SUBJ_MEAS_OUTCOME
SLC, DIST
[ATTN_SHIFT, HEC → SURG, SPATIAL_PERC]
SLC, ATTN_SHIFT, HEC, SPATIAL_PERC
SURG
ATTN_SHIFT
ATTN_SHIFT
ENG_MOT

EXP_OUTCOME
ATTN_SHIFT, SPATIAL_PERC
SLC
SPATIAL_PERC, CONF
N/A
SPATIAL_PERC, CONF
DPPC, SPATIAL_PERC
N/A
N/A
[ATTN_SHIFT → SURG, INC]

N/A
VIS_OPT

N/A
N/A
EASE_HCI, SLC, VIS_OPT, COMF
DPPC
PER_REAL_AUG, FAT, IMMR, EASE_HCI,
SPATIAL_PERC
EYE

VIS_OPT
CONC_LS, ANX
N/A
PER_REAL_AUG
N/A
N/A, DPPC
N/A
N/A
N/A
PREF_HOL, PER_REAL_AUG
N/A
EASE_HCI
EASE_HCI
EASE_HCI
EASE_HCI, VIS_OPT
COMF
N/A
EASE_HCI
EASE_HCI
SPATIAL_PERC
PER_REAL_AUG
N/A
VIS_OPT
USEF
SPATIAL_PERC, SUBJ_MEAS_OUTCOME
COMM_3D
N/A
SPATIAL_PERC, FAT
FRUS
ANX, COMF, CONF

SPATIAL_PERC, COMF
N/A
N/A
N/A
N/A
COMF
DPPC
SLC
SPATIAL_PERC
SPATIAL_PERC
SPATIAL_PERC
COMF
EASE_HCI
N/A
N/A
DPPC
DIST
COMF
SUBJ_MEAS_OUTCOME
DIST, USEF, EASE_HCI
COMF, SPATIAL_PERC
N/A
N/A
N/A
N/A
ENG_MOT, SPATIAL_PERC

EASE_HCI
SPATIAL_PERC
USEF, EASE_HCI, FRUS
SPATIAL_PERC
SPATIAL_PERC
EASE_HCI, COMF, USEF
USEF, CONF
EASE_HCI, CONF, SLC
CONF, SPATIAL_PERC
N/A

SURG, SPATIAL_PERC

SURG

(continued on next page)

24

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

Table 5

(continued)

Study

Addressed Human Factors

Reported Persistent Human Factors

Li et al. (2020b)
Kumar et al. (2020)
Li et al. (2020a)
Gibby et al. (2020)
Gu et al. (2020)
Galati et al. (2020)
Viehöfer et al. (2020)
Dennler et al. (2020)
Kriechling et al. (2020)
Zorzal et al. (2020)
Cartucho et al. (2020)
Rojas-Muñoz
(2020b)
Scherl et al. (2020)
Creighton et al. (2020)
Jiang et al. (2020)
Sun et al. (2020a)

et

al.

[HEC → SURG]
SPATIAL_PERC, MM
SPATIAL_PERC, SURG
MM
ATTN_SHIFT, SURG
ATTN_SHIFT, SURG, MM, SLC
SLC, EXP_OUTCOME, SURG
SLC, EXP_OUTCOME, SURG, ATTN_SHIFT
SURG, EXP_OUTCOME
[ATTN_SHIFT → HEC], SLC, COMF, FAT
N/A
[ATTN_SHIFT → MM, SURG]

N/A
SPATIAL_PERC
DPPC, MM
N/A

FAT, INC, COMF
DPPC, COMF
N/A
EASE_HCI
N/A
COMF, STRESS, EASE_HCI, DPPC
EXP_OUTCOME
N/A
N/A
DPPC, COMF, EASE_HCI, USEF
EASE_HCI, USEF, VIS_OPT, COMF
EASE_HCI, FRUS

EASE_HCI, COMF
N/A
N/A
N/A

ance applications. Zuo et al. (2020) proposed a novel
multi-indicator evaluation model for mixed reality surgi-
cal navigation systems that evaluate the user’s perception
in regards to safety, comfort and eﬃciency and combines
subjective and objective evaluation criteria. Doswell and
Skinner (2014) identiﬁed the need for HMD-based, scien-
tiﬁcally grounded methods that identify HCI related in-
teraction modalities to be addressed to optimise user per-
formance and cognitive load. These comprise information
presentation, user input and system feedback. the suggest
that an ideal HCI system should be able to adapt these
interaction modalities in real-time and in response to the
given task as well as environmental and user psychophysi-
ological states.

A taxonomy for mixed reality visualisation in image
guided surgery has been proposed by Kersten-Oertel et al.
(2012) aiming to introduce a new common framework
that facilitates the establishment of validation criteria and
should lead to more mixed reality systems being used in
daily surgical routine. The paper is well cited and the com-
prehensive literature review of AR in laparoscopic surgery
from Bernhardt et al. (2017) categorises articles according
to their taxonomy. But the translation into commercial
applications that are used on a daily basis in operating
rooms has not materialised as yet. A similar taxonomy
tailored to OST-HMD AR would be desirable but is hard
to achieve given the widely varying needs of the implemen-
tations presented in this review.

10.2 Potential machine learning applica-

tions

Given the increasing trend of machine learning (ML) ap-
plications for medical image processing, such methods are
likely to be applied to OST-HMD solutions. However, none
of the selected 91 articles contained such a ML applica-
tion. OST-HMD systems provide a 3D world with a wealth
of data, including video images, gesture-based interaction
data, eye tracking and generated surface meshes. These
could provide rich training data for ML algorithms.

ML algorithms have been proposed for surgical mixed
reality applications. Azimi et al. (2018) presented an in-
teractive training and operation ecosystem for mixed real-

ity related surgical tasks that includes data collection for
potential ML algorithms. Their system records data from
multiple users, such as gaze tracking to indicate which lo-
cations in 3D space a surgeon is paying attention to. ML
algorithms could then use this data to identify novice sur-
geons and activate guidance support.

Another example aiming to expand the user’s hands-
free interaction possibilities when wearing a HMD was pro-
posed by Chen et al. (2019). Using a self-made HMD with
eye-tracking cameras, the authors proposed a deep con-
volutional neural network to classify gaze trajectories and
gaze trajectory gestures. The classiﬁed gestures in turn
can then trigger diﬀerent HCI operations. The HoloLens
2 comes with built-in gaze tracking and oﬀers new HCI
possibilities that still need to be explored, especially in a
surgical setup. A fundamental step in terms of accelerat-
ing ML research in the surgical ﬁeld will be the creation of
data bases with relevant user data, which can then serve
as inputs for ML algorithms.

10.3 Conclusions

The ﬁeld of OST-HMD assisted surgery has shown a sig-
niﬁcant recent upward trend in the number of publications
as well as the diversity of surgical applications that could
beneﬁt from this technology. The release of the Microsoft
HoloLens has boosted research into mixed reality surgi-
cal applications from 2017 onwards (see table 2). How-
ever, comparatively few systems have been used clinically
to date and demonstration of utility is rare.

It is worth noting that Dilley et al. (2019), in a screen-
based simulation system, compared direct AR with nearby
unregistered guidance. Overlaid AR was found to cause
inattention blindness, where the augmented view distracts
from important events in the real view. This problem
arises even when registration is perfect. One option is that
guidance information could be presented near to, but not
overlaid directly on the surgical view. Such a side-by-side
visualisation would allow correctly oriented, but not fully
registered model data to be readily available without ob-
scuring or confusing the real view.

The training aspect of OST-HMD visualisation should
not be underestimated. The ability to view 3D anatomy

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

25

and pathology in situ may improve spatial understanding
in novice surgeons and reduce the learning curve. Louis
et al. (2020) demonstrated improved learning with AR un-
der high ﬁdelity conditions. There is a case for similar ex-
periments to be conducted using OST-HMD AR to provide
evidence of proven beneﬁt to learning with these devices.
One potential direction for research is a human factors
approach that starts by identifying explicit points or mo-
ments in a procedure that may aﬀect patient outcome
and then tailors visualisations to improve performance and
decision-making for these speciﬁc tasks. Demonstration of
improved performance on similar speciﬁc tasks in the lab-
oratory setting might also lead to a better understanding
of the optimal role of AR.

Increasing exposure to AR devices may also improve ac-
ceptability of such technology. Taking both technologi-
cal and human factors into consideration from the outset
should lead research towards eﬀective clinical implementa-
tions that ﬁnally realise the full potential of surgical AR.

11 Acknowledgements

The work was supported by the Wellcome/EPSRC Cen-
tre for Interventional and Surgical Sciences (WEISS)
[203145Z/16/Z]; Engineering and Physical Sciences Re-
search Council (EPSRC) [EP/P027938/1, EP/R004080/1,
EP/P012841/1]; The Royal Academy of Engineering
[CiET1819/2/36].

References

Jacob Aaskov, Gregory N Kawchuk, Kenton D Hamaluik, Pierre
Boulanger, and Jan Hartvigsen. X-ray vision: the accuracy and
repeatability of a technology that allows clinicians to see spinal
x-rays superimposed on a person’s back. PeerJ, 7:e6333, 2019.

Hasaneen Fathy Al Janabi, Abdullatif Aydin, Sharanya Palaneer,
Nicola Macchione, Ahmed Al-Jabir, Muhammad Shamim Khan,
Prokar Dasgupta, and Kamran Ahmed. Eﬀectiveness of the
hololens mixed-reality headset in minimally invasive surgery: a
simulation-based feasibility study.
Surgical Endoscopy, 34(3):
1143–1149, 2020.

Sebastian Andress, Alex Johnson M.D., Mathias Unberath, Alexan-
der F. Winkler, Kevin Yu, Javad Fotouhi, Simon Weidert M.D.,
Greg M. Osgood M.D., and Nassir Navab. On-the-ﬂy augmented
reality for orthopedic surgery using a multimodal ﬁducial. Journal
of Medical Imaging, 5(2):1 – 12, 2018. doi: 10.1117/1.JMI.5.2.02
1209. URL https://doi.org/10.1117/1.JMI.5.2.021209.

David G Armstrong, Timothy M Rankin, Nicholas A Giovinco,
Joseph L Mills, and Yoky Matsuoka. A heads-up display for di-
abetic limb salvage surgery: a view through the google looking
glass. Journal of diabetes science and technology, 8(5):951–956,
2014.

Ehsan Azimi, Camilo Molina, Alexander Chang, Judy Huang, Chien-
Ming Huang, and Peter Kazanzides. Interactive training and op-
eration ecosystem for surgical tasks in mixed reality. In OR 2.0
Context-Aware Operating Theaters, Computer Assisted Robotic
Endoscopy, Clinical Image-Based Procedures, and Skin Image
Analysis, pages 20–29. Springer, 2018.

ZM Baum, Andras Lasso, Sarah Ryan, Tamas Ungi, Emily Rae, Boris
Zevin, Ron Levy, and Gabor Fichtinger. Augmented reality train-
ing platform for neurosurgical burr hole localization. J Med Robot
Res, pages 194–2001, 2020.

Sylvain Bernhardt, Stéphane A Nicolau, Luc Soler, and Christophe
Doignon. The status of augmented reality in laparoscopic surgery
as of 2016. Medical image analysis, 37:66–90, 2017.

Thomas Boillat, Peter Grantcharov, and Homero Rivas. Increasing
completion rate and beneﬁts of checklists: Prospective evaluation
of surgical safety checklists with smart glasses. JMIR mHealth and
uHealth, 7(4):e13447, 2019.

H Borgmann, M Rodríguez Socarrás, J Salem, I Tsaur, J Gomez
Rivas, E Barret, and L Tortolero. Feasibility and safety of aug-
mented reality-assisted urological surgery using smartglass. World
Journal of Urology, 6(35):967–972, 2016.

H Brun, RAB Bugge, LKR Suther, S Birkeland, R Kumar, E Pelanis,
and OJ Elle. Mixed reality holograms for heart surgery planning:
ﬁrst user experience in congenital heart disease. European Heart
Journal-Cardiovascular Imaging, 20(8):883–888, 2019.

Marina Carbone, Roberta Piazza, and Sara Condino. Commercially
available head-mounted displays are unsuitable for augmented re-
ality surgical guidance: a call for focused research for surgical
applications, 2020.

João Cartucho, David Shapira, Hutan Ashraﬁan, and Stamatia Gian-
narou. Multimodal mixed reality visualisation for intraoperative
surgical guidance. International journal of computer assisted ra-
diology and surgery, 15(5):819–826, 2020.

Center for Devices and Radiological Health. Applying human factors
and usability engineering to medical devices. Guidance for Indus-
try and Food and Drug Administration Staﬀ. FDA-2011-D-0469,
2016.

L. Chen, T. W. Day, W. Tang, and N. W. John. Recent develop-
ments and future challenges in medical mixed reality.
In 2017
IEEE International Symposium on Mixed and Augmented Reality
(ISMAR), pages 123–135, 2017. doi: 10.1109/ISMAR.2017.29.

WX Chen, XY Cui, J Zheng, JM Zhang, S Chen, and YD Yao. Gaze
gestures and their applications in human-computer interaction
with a head-mounted display. arXiv preprint arXiv:1910.07428,
2019.

Xiaojun Chen, Lu Xu, Yiping Wang, Huixiang Wang, Fang Wang,
Xiangsen Zeng, Qiugen Wang, and Jan Egger. Development of a
surgical navigation system based on augmented reality using an
optical see-through head-mounted display. Journal of biomedical
informatics, 55:124–131, 2015.

Jong-Chih Chien, Yao-Ren Tsai, Chieh-Tsai Wu, and Jiann-Der Lee.
Hololens-based ar system with a robust point set registration al-
gorithm. Sensors, 19(16):3555, 2019.

Kevin Cleary and Terry M Peters. Image-guided interventions: tech-
nology review and clinical applications. Annual review of biomed-
ical engineering, 12:119–142, 2010.

Carole Cometti, Christos Païzis, Audrey Casteleira, Guillaume Pons,
and Nicolas Babault. Eﬀects of mixed reality head-mounted glasses
during 90 minutes of mental and manual tasks on cognitive and
physiological functions. PeerJ, 6:e5847, 2018.

S. Condino, M. Carbone, R. Piazza, M. Ferrari, and V. Ferrari. Per-
ceptual limits of optical see-through visors for augmented reality
guidance of manual tasks. IEEE Transactions on Biomedical En-
gineering, 67(2):411–419, 2020.

26

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

Sara Condino, Giuseppe Turini, Paolo D Parchi, Rosanna M
Viglialoro, Nicola Piolanti, Marco Gesi, Mauro Ferrari, and Vin-
cenzo Ferrari. How to build a patient-speciﬁc hybrid simulator
for orthopaedic open surgery: beneﬁts and limits of mixed-reality
using the microsoft hololens. Journal of Healthcare Engineering,
2018, 2018.

Francis X Creighton, Mathias Unberath, Tianyu Song, Zhuokai Zhao,
Mehran Armand, and John Carey. Early feasibility studies of aug-
mented reality navigation for lateral skull base surgery. Otology &
Neurotology, 41(7):883–888, 2020.

Fabrizio Cutolo, Umberto Fontana, and Vincenzo Ferrari. Perspec-
tive preserving solution for quasi-orthoscopic video see-through
hmds. Technologies, 6, 01 2018. doi: 10.3390/technologies6010009.

David Dallas-Orr, Yordan Penev, Robert Schultz, and Jesse Courtier.
Comparing computed tomography–derived augmented reality
holograms to a standard picture archiving and communication
systems viewer for presurgical planning: Feasibility study. JMIR
Perioperative Medicine, 3(2):e18367, 2020.

Marcelo E de Oliveira, Henrique G Debarba, Alexandre Lädermann,
Sylvain Chagué, and Caecilia Charbonnier. A hand-eye calibration
method for augmented reality applied to computer-assisted ortho-
pedic surgery. The International Journal of Medical Robotics and
Computer Assisted Surgery, 15(2):e1969, 2019.

Gerard Deib, Alex Johnson, Mathias Unberath, Kevin Yu, Sebastian
Andress, Long Qian, Gregory Osgood, Nassir Navab, Ferdinand
Hui, and Philippe Gailloud.
Image guided percutaneous spine
procedures using an optical see-through head mounted display:
proof of concept and rationale. Journal of neurointerventional
surgery, 10(12):1187–1191, 2018.

Cyrill Dennler, Laurenz Jaberg, José Spirig, Christoph Agten, To-
bias Götschi, Philipp Fürnstahl, and Mazda Farshad. Augmented
reality-based navigation increases precision of pedicle screw inser-
tion. Journal of orthopaedic surgery and research, 15:1–8, 2020.

Arindam Dey, Mark Billinghurst, Robert W Lindeman, and J Swan.
A systematic review of 10 years of augmented reality usability
studies: 2005 to 2014. Frontiers in Robotics and AI, 5:37, 2018.

Ryan M Dickey, Neel Srikishen, Larry I Lipshultz, Philippe E Spiess,
Rafael E Carrion, and Tariq S Hakky. Augmented reality assisted
surgery: a urologic training tool. Asian journal of andrology, 18
(5):732, 2016.

James WR Dilley, Archie Hughes-Hallett, Philip J Pratt, Philip H
Pucher, Mafalda Camara, Ara W Darzi, and Erik K Mayer. Perfect
registration leads to imperfect performance: A randomized trial of
multimodal intraoperative image guidance. Annals of surgery, 269
(2):236–242, 2019.

Jayfus T Doswell and Anna Skinner. Augmenting human cognition
with adaptive augmented reality. In International Conference on
Augmented Cognition, pages 104–113. Springer, 2014.

Martin Eckert, Julia S Volmerg, and Christoph M Friedrich. Aug-
mented reality in medicine: systematic and bibliographic review.
JMIR mHealth and uHealth, 7(4):e10967, 2019.

Benish Fida, Fabrizio Cutolo, Gregorio di Franco, Mauro Ferrari, and
Vincenzo Ferrari. Augmented reality in open surgery. Updates in
Surgery, 70(3):389–400, Sep 2018. ISSN 2038-3312. doi: 10.1007/
s13304-018-0567-8. URL https://doi.org/10.1007/s13304-018
-0567-8.

Matthijs Fitski, Jene W Meulstee, Annemieke S Littooij, Cornelis P
van de Ven, Alida FW van der Steeg, and Marc HWA Wijnen. Mri-
based 3-dimensional visualization workﬂow for the preoperative
planning of nephron-sparing surgery in wilmsâĂŹ tumor surgery:
A pilot study. Journal of Healthcare Engineering, 2020, 2020.

Javad Fotouhi, Mathias Unberath, Tianyu Song, Wenhao Gu, Alex
Johnson, Greg Osgood, Mehran Armand, and Nassir Navab. In-
teractive ﬂying frustums (iﬀs): spatially aware surgical data vi-
sualization. International journal of computer assisted radiology
and surgery, 14(6):913–922, 2019a.

Javad Fotouhi, Mathias Unberath, Tianyu Song, Jonas Hajek,
Sing Chun Lee, Bastian Bier, Andreas Maier, Greg Osgood,
Mehran Armand, and Nassir Navab. Co-localized augmented hu-
man and x-ray observers in collaborative surgical ecosystem. In-
ternational journal of computer assisted radiology and surgery, 14
(9):1553–1563, 2019b.

Javad Fotouhi, Tianyu Song, Arian Mehrfard, Giacomo Taylor,
Qiaochu Wang, Fengfan Xian, Alejandro Martin-Gomez, Bernhard
Fuerst, Mehran Armand, Mathias Unberath, et al. Reﬂective-ar
display: An interaction methodology for virtual-to-real alignment
in medical robotics. IEEE Robotics and Automation Letters, 5(2):
2722–2729, 2020.

Taylor Frantz, Bart Jansen, Johnny Duerinck, and Jef Vandemeule-
broucke. Augmenting microsoft’s hololens with vuforia tracking
for neuronavigation. Healthcare technology letters, 5(5):221–225,
2018.

Rocco Galati, Michele Simone, Graziana Barile, Raﬀaele De Luca,
Carmine Cartanese, and G Grassi. Experimental setup employed
in the operating room based on virtual and mixed reality: analysis
of pros and cons in open abdomen surgery. Journal of Healthcare
Engineering, 2020, 2020.

Jacob Gibby, Steve Cvetko, Ramin Javan, Ryan Parr, and Wendell
Gibby. Use of augmented reality for image-guided spine proce-
dures. European Spine Journal, 29(8):1823–1832, 2020.

Jacob T Gibby, Samuel A Swenson, Steve Cvetko, Raj Rao, and
Ramin Javan. Head-mounted display augmented reality to guide
pedicle screw placement utilizing computed tomography. Interna-
tional journal of computer assisted radiology and surgery, 14(3):
525–535, 2019.

Joshua J Gnanasegaram, Regina Leung, and Jason A Beyea. Evalu-
ating the eﬀectiveness of learning ear anatomy using holographic
models. Journal of Otolaryngology-Head & Neck Surgery, 49(1):
1–8, 2020.

Wenhao Gu, Kinjal Shah, Jonathan Knopf, Nassir Navab, and Math-
ias Unberath. Feasibility of image-based augmented reality guid-
ance of total shoulder arthroplasty using microsoft hololens 1.
Computer Methods in Biomechanics and Biomedical Engineering:
Imaging & Visualization, pages 1–10, 2020.

P.J Edwards, M. Chand, M. Birlo, and D. Stoyanov. The challenge
In S. Atallah, editor, Digital
of augmented reality in surgery.
Surgery, chapter 10, pages 121–135. Springer, Cham, 2021. doi:
https://doi.org/10.1007/978-3-030-49100-0\_10.

Na Guo, Tianmiao Wang, Biao Yang, Lei Hu, Hongsheng Liu, and
Yuhan Wang. An online calibration method for microsoft hololens.
IEEE Access, 7:101795–101803, 2019.

Houssam El-Hariri, Prashant Pandey, Antony J Hodgson, and Rafeef
Garbi. Augmented reality visualisation for orthopaedic surgical
guidance with pre- and intra-operative multimodal image data fu-
sion. Healthcare Technology Letters, 5(5):189–193, 2018.

Florian Heinrich, Luisa Schwenderling, Mathias Becker, Martin
Skalej, and Christian Hansen. Holoinjection: augmented reality
support for ct-guided spinal needle injections. Healthcare Tech-
nology Letters, 6(6):165–171, 2019.

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

27

Takafumi Hiranaka, Takaaki Fujishiro, Yuichi Hida, Yosaku Shi-
bata, Masanori Tsubosaka, Yuta Nakanishi, Kenjiro Okimura, and
Harunobu Uemoto. Augmented reality: the use of the picolinker
smart glasses improves wire insertion under ﬂuoroscopy. World
journal of orthopedics, 8(12):891, 2017.

Weidong Huang, Leila Alem, and Mark A Livingston. Human factors
in augmented reality environments. Springer Science & Business
Media, 2012.

Shahram Jalaliniya and Thomas Pederson. Designing wearable per-
sonal assistants for surgeons: An egocentric approach. IEEE Per-
vasive Computing, 14(3):22–31, 2015.

Shahram Jalaliniya, Thomas Pederson, and Diako Mardanbegi. A
wearable personal assistant for surgeons: Design, evaluation, and
future prospects. EAI Endorsed Transactions on Pervasive Health
and Technology, 3(12), 2017.

Taoran Jiang, Dewang Yu, Yuqi Wang, Tao Zan, Shuyi Wang, and
Qingfeng Li. Hololens-based vascular localization system: Pre-
cision evaluation study with a three-dimensional printed model.
Journal of medical Internet research, 22(4):e16852, 2020.

B Laguna, K Livingston, R Brar, J Jagodzinski, N Pandya, C Saba-
tini, and J Courtier. Assessing the value of a novel augmented
reality application for presurgical planning in adolescent elbow
fractures. front. Virtual Real. 1: 528810. doi: 10.3389/frvir, 2020.

Carl Laverdière, Jason Corban, Jason Khoury, Susan Mengxiao Ge,
Justin Schupbach, Edward J Harvey, Rudy Reindl, and Paul A
Martineau. Augmented reality in orthopaedics: a systematic re-
view and a window on future possibilities. The Bone & Joint
Journal, 101(12):1479–1488, 2019.

Guan Li, Jie Dong, Jinbao Wang, Dongbing Cao, Xin Zhang,
Zhiqiang Cao, and Guangming Lu. The clinical application
value of mixed-reality-assisted surgical navigation for laparoscopic
nephrectomy. Cancer Medicine, 9(15):5480–5489, 2020a.

Ming Li, Reza Seifabadi, Dilara Long, Quirina De Ruiter, Nicole
Varble, Rachel Hecht, Ayele H Negussie, Venkatesh Krish-
nasamy, Sheng Xu, and Bradford J Wood. Smartphone-versus
smartglasses-based augmented reality (ar) for percutaneous nee-
dle interventions: system accuracy and feasibility study. Interna-
tional Journal of Computer Assisted Radiology and Surgery, 15
(11):1921–1930, 2020b.

Lukas Jud, Javad Fotouhi, Octavian Andronic, Alexander Aichmair,
Greg Osgood, Nassir Navab, and Mazda Farshad. Applicability
of augmented reality in orthopedic surgery–a systematic review.
BMC musculoskeletal disorders, 21(1):1–13, 2020.

Qiming Li, Chen Huang, Shengqing Lv, Zeyu Li, Yimin Chen, and
Lizhuang Ma. An human-computer interactive augmented reality
system for coronary artery diagnosis planning and training. Jour-
nal of medical systems, 41(10):159, 2017.

Naoki Kaneko, Makoto Sato, Taro Takeshima, Yoshihide Sehara, and
Eiju Watanabe. Ultrasound-guided central venous catheterization
using an optical see-through head-mounted display: A pilot study.
Journal of Clinical Ultrasound, 44(8):487–491, 2016.

Ruotong Li, Weixin Si, Xiangyun Liao, Qiong Wang, Reinhard Klein,
and Pheng-Ann Heng. Mixed reality based respiratory liver tumor
puncture navigation. Computational Visual Media, 5(4):363–374,
2019.

Christof Karmonik, Saba N Elias, Jonathan Y Zhang, Orlando Diaz,
Richard P Klucznik, Robert G Grossman, and Gavin W Britz.
Augmented reality with virtual cerebral aneurysms: A feasibility
study. World neurosurgery, 119:e617–e622, 2018.

Darko Katić, Patrick Spengler, Sebastian Bodenstedt, Gregor
Castrillon-Oberndorfer, Robin Seeberger, Juergen Hoﬀmann,
Ruediger Dillmann, and Stefanie Speidel. A system for context-
aware intraoperative augmented reality in dental implant surgery.
International journal of computer assisted radiology and surgery,
10(1):101–108, 2015.

Patrick J. Kelly, Jr. Alker, George J., and Stephan Goerss.
Computer-assisted Stereotactic Laser Microsurgery for the Treat-
ment of Intracranial Neoplasms. Neurosurgery, 10(3):324–331,
1982.

Marta Kersten-Oertel, Pierre Jannin, and D Louis Collins. Dvv: A
taxonomy for mixed reality visualization in image guided surgery.
IEEE Transactions on Visualization and Computer Graphics, 2
(18):332–352, 2012.

Marta Kersten-Oertel, Pierre Jannin, and D Louis Collins. The state
of the art of visualization in mixed reality image guided surgery.
Computerized Medical Imaging and Graphics, 37(2):98–112, 2013.

Philipp Kriechling, Simon Roner, Florentin Liebmann, Fabio Casari,
Philipp Fürnstahl, and Karl Wieser. Augmented reality for base
plate component placement in reverse total shoulder arthroplasty:
a feasibility study. Archives of orthopaedic and trauma surgery,
pages 1–7, 2020.

Ivo Kuhlemann, Markus Kleemann, Philipp Jauer, Achim
Schweikard, and Floris Ernst. Towards x-ray free endovascular
interventions–using hololens for on-line holographic visualisation.
Healthcare technology letters, 4(5):184–187, 2017.

Rahul Prasanna Kumar, Egidijus Pelanis, Robin Bugge, Henrik
Brun, Rafael Palomar, Davit L Aghayan, âĎńsmund Avdem Fret-
land, Bjørn Edwin, and Ole Jakob Elle. Use of mixed reality for
surgery planning: Assessment and development workﬂow. Journal
of Biomedical Informatics: X, 8:100077, 2020.

Alessandro Liberati, Douglas G Altman, Jennifer Tetzlaﬀ, Cynthia
Mulrow, Peter C Gøtzsche, John PA Ioannidis, Mike Clarke,
Philip J Devereaux, Jos Kleijnen, and David Moher. The prisma
statement for reporting systematic reviews and meta-analyses of
studies that evaluate health care interventions: explanation and
elaboration. Annals of internal medicine, 151(4):W–65, 2009.

Cara A Liebert, Mohamed A Zayed, Oliver Aalami, Jennifer Tran,
and James N Lau. Novel use of google glass for procedural wireless
vital sign monitoring. Surgical innovation, 23(4):366–373, 2016.

Florentin Liebmann, Simon Roner, Marco von Atzigen, Davide Scara-
muzza, Reto Sutter, Jess Snedeker, Mazda Farshad, and Philipp
Fürnstahl. Pedicle screw navigation using surface digitization on
the microsoft hololens. International journal of computer assisted
radiology and surgery, 14(7):1157–1165, 2019.

Michael A Lin, Alexa F Siu, Jung Hwa Bae, Mark R Cutkosky, and
Bruce L Daniel. Holoneedle: augmented reality guidance sys-
tem for needle placement investigating the advantages of three-
dimensional needle shape reconstruction. IEEE Robotics and Au-
tomation Letters, 3(4):4156–4162, 2018.

Jason I Liounakos, Timur Urakov, and Michael Y Wang. Head-up
display assisted endoscopic lumbar discectomyâĂŤa technical note.
The International Journal of Medical Robotics and Computer As-
sisted Surgery, 16(3):e2089, 2020.

Jun Liu, Subhi J AlâĂŹAref, Gurpreet Singh, Alexandre Caprio,
Amir Ali Amiri Moghadam, Sun-Joo Jang, S Chiu Wong, James K
Min, Simon Dunham, and Bobak Mosadegh. An augmented re-
ality system for image guidance of transcatheter procedures for
structural heart disease. PloS one, 14(7), 2019.

Yinlong Liu, Zhijian Song, and Manning Wang. A new robust
markerless method for automatic image-to-patient registration in
image-guided neurosurgery system. Computer Assisted Surgery,
22(sup1):319–325, 2017.

M. A. Livingston. Evaluating human factors in augmented reality
systems. IEEE Computer Graphics and Applications, 25(6):6–9,
2005.

28

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

Thibault Louis, Jocelyne Troccaz, Amélie Rochet-Capellan, Nady
Hoyek, and François Bérard. When high ﬁdelity matters: Ar and
In Proceedings of the
vr improve the learning of a 3d object.
International Conference on Advanced Visual Interfaces, pages
1–9, 2020.

Antonio Pepe, Gianpaolo Francesco Trotta, Peter Mohr-Ziak,
Christina Gsaxner, Jürgen Wallner, Vitoantonio Bevilacqua, and
Jan Egger. A marker-less registration approach for mixed reality–
aided maxillofacial surgery: a pilot evaluation. Journal of digital
imaging, 32(6):1008–1018, 2019.

Bethany R Lowndes and M Susan Hallbeck. Overview of human
factors and ergonomics in the or, with an emphasis on minimally
invasive surgeries. Human Factors and Ergonomics in Manufac-
turing & Service Industries, 24(3):308–317, 2014.

Javier A Luzon, Bojan V Stimec, Arne O Bakka, Bjørn Edwin, and
Dejan Ignjatovic. Value of the surgeonâĂŹs sightline on hologram
registration and targeting in mixed reality. International Journal
of Computer Assisted Radiology and Surgery, 15(12):2027–2039,
2020.

Faraz Mahmood, Eitezaz Mahmood, Robert Gregory Dorfman, John
Mitchell, Feroze-Udin Mahmood, Stephanie B Jones, and Robina
Matyal. Augmented reality and ultrasound education: initial ex-
perience. Journal of cardiothoracic and vascular anesthesia, 32
(3):1363–1367, 2018.

Piotr Pietruski, Marcin Majak, Ewelina Światek-Najwer, Magdalena
Żuk, Michał Popek, Maciej Mazurek, Marta Świecka, and Janusz
Jaworowski. Supporting mandibular resection with intraopera-
tive navigation utilizing augmented reality technology–a proof of
concept study. Journal of Cranio-Maxillofacial Surgery, 47(6):
854–859, 2019.

Piotr Pietruski, Marcin Majak, Ewelina Świątek-Najwer, Magdalena
Żuk, Michał Popek, Janusz Jaworowski, and Maciej Mazurek. Sup-
porting ﬁbula free ﬂap harvest with augmented reality: A proof-
of-concept study. The Laryngoscope, 130(5):1173–1179, 2020.

Brent A Ponce, Mariano E Menendez, Lasun O Oladeji, Charles T
Fryberger, and Phani K Dantuluri. Emerging technology in sur-
gical education: combining real-time augmented reality and wear-
able computing devices. Orthopedics, 37(11):751–757, 2014.

Keitaro Matsukawa and Yoshiyuki Yato. Smart glasses display de-
vice for ﬂuoroscopically guided minimally invasive spinal instru-
mentation surgery: a preliminary study. Journal of Neurosurgery:
Spine, 1(aop):1–6, 2020.

Helena Catarina Margarido Mendes, Cátia Isabel Andrade Botelho
Costa, Nuno André da Silva, Francisca Pais Leite, Augusto Es-
teves, and Daniel Simões Lopes. Piñata: Pinpoint insertion of in-
travenous needles via augmented reality training assistance. Com-
puterized Medical Imaging and Graphics, 82:101731, 2020.

Philip Pratt, Matthew Ives, Graham Lawton, Jonathan Simmons,
Nasko Radev, Liana Spyropoulou, and Dimitri Amiras. Through
the hololensâĎć looking glass: augmented reality for extremity
reconstruction surgery using 3d vascular models with perforating
vessels. European radiology experimental, 2(1):2, 2018.

L. Qian, J. Y. Wu, S. P. DiMaio, N. Navab, and P. Kazanzides.
A review of augmented reality in robotic-assisted surgery. IEEE
Transactions on Medical Robotics and Bionics, 2(1):1–16, 2020.
doi: 10.1109/TMRB.2019.2957061.

Jene W Meulstee, Johan Nijsink, Ruud Schreurs, Luc M Verhamme,
Tong Xi, Hans HK Delye, Wilfred A Borstlap, and Thomas JJ
Maal. Toward holographic-guided surgery. Surgical innovation,
26(1):86–94, 2019.

Long Qian, Anton Deguet, and Peter Kazanzides. Arssist: aug-
mented reality on a head-mounted display for the ﬁrst assistant
in robotic surgery. Healthcare technology letters, 5(5):194–200,
2018.

Daisuke Mitsuno, Koichi Ueda, Tomoki Itamiya, Takashi Nuri, and
Yuki Otsuki. Intraoperative evaluation of body surface improve-
ment by an augmented reality system that a clinician can modify.
Plastic and Reconstructive Surgery Global Open, 5(8), 2017.

David W. Roberts, John W. Strohbehn, John F. Hatch, William Mur-
ray, and Hans Kettenberger. A frameless stereotaxic integration of
computerized tomographic imaging and the operating microscope.
Journal of Neurosurgery, 65(4):545 – 549, 1986.

Nhu Q Nguyen, Jillian Cardinell, Joel M Ramjist, Philips Lai, Yuta
Dobashi, Daipayan Guha, Dimitrios Androutsos, and Victor XD
Yang. An augmented reality system characterization of placement
accuracy in neurosurgery. Journal of Clinical Neuroscience, 72:
392–396, 2020.

Edgar Rojas-Muñoz, Maria Eugenia Cabrera, Daniel Andersen,
Voicu Popescu, Sherri Marley, Brian Mullis, Ben Zarzaur, and
Juan Wachs. Surgical telementoring without encumbrance: a com-
parative study of see-through augmented reality-based approaches.
Annals of surgery, 270(2):384–389, 2019.

Tomoyoshi Okamoto, Shinji Onda, Katsuhiko Yanaga, Naoki Suzuki,
and Asaki Hattori. Clinical application of navigation surgery using
augmented reality in the abdominal ﬁeld. Surgery Today, 45(4):
397–406, Apr 2015. ISSN 1436-2813. doi: 10.1007/s00595-014-0
946-9. URL https://doi.org/10.1007/s00595-014-0946-9.

Edgar Rojas-Muñoz, Maria E Cabrera, Chengyuan Lin, Daniel An-
dersen, Voicu Popescu, Kathryn Anderson, Ben L Zarzaur, Brian
Mullis, and Juan P Wachs. The system for telementoring with aug-
mented reality (star): A head-mounted display to improve surgical
coaching and conﬁdence in remote areas. Surgery, 2020a.

Bill Papantoniou, M Soegaard, JR Lupton, M Goktürk, and
D Trepess. The glossary of human computer interaction. On-
line source: ht tp s: // ww w. in te ra ct io n-de si gn .o rg /l it er
at ur e/ bo ok /t he -g lo ss ar y-of -h um an -c om pu te r-in te ra
ct io n [2019-04-23], 2016.

Brian J Park, Nicholas R Perkons, Enri Profka, Omar Johnson,
Christopher Morley, Scott Appel, Gregory J Nadolski, Stephen J
Hunt, and Terence P Gade. Three-dimensional augmented real-
ity visualization informs locoregional therapy in a translational
model of hepatocellular carcinoma. Journal of Vascular and In-
terventional Radiology, 31(10):1612–1618, 2020.

Egidijus Pelanis, Rahul P Kumar, Davit L Aghayan, Rafael Palomar,
Åsmund A Fretland, Henrik Brun, Ole Jakob Elle, and Bjørn Ed-
win. Use of mixed reality for improved spatial understanding of
liver anatomy. Minimally Invasive Therapy & Allied Technologies,
29(3):154–160, 2020.

Edgar Rojas-Muñoz, Chengyuan Lin, Natalia Sanchez-Tamayo,
Maria Eugenia Cabrera, Daniel Andersen, Voicu Popescu,
Juan Antonio Barragan, Ben Zarzaur, Patrick Murphy, Kathryn
Anderson, et al. Evaluation of an augmented reality platform for
austere surgical telementoring: a randomized controlled crossover
study in cricothyroidotomies. NPJ Digital Medicine, 3(1):1–9,
2020b.

Jannick P Rolland, Richard L Holloway, and Henry Fuchs. Com-
parison of optical and video see-through, head-mounted displays.
In Telemanipulator and Telepresence Technologies, volume 2351,
pages 293–307. International Society for Optics and Photonics,
1995.

Paweł Rynio, Jan Witowski, Jakub Kamiński, Jakub Seraﬁn, Arka-
diusz Kazimierczak, and Piotr Gutowski. Holographically-guided
endovascular aneurysm repair. Journal of Endovascular Therapy,
26(4):544–547, 2019.

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

29

Igor M Sauer, Moritz Queisner, Peter Tang, Simon Moosburner, Ole
Hoepfner, Rosa Horner, Rudiger Lohmann, and Johann Pratschke.
Mixed reality in visceral surgery: development of a suitable work-
ﬂow and evaluation of intraoperative use-cases. Annals of surgery,
266(5):706–712, 2017.

Arnd Fredrik Viehöfer, Stephan Hermann Wirth, Stefan Michael
Zimmermann, Laurenz Jaberg, Cyrill Dennler, Philipp Fürnstahl,
and Mazda Farshad. Augmented reality guided osteotomy in hal-
lux valgus correction. BMC Musculoskeletal Disorders, 21(1):1–6,
2020.

Claudia Scherl, Johanna Stratemeier, Celine Karle, Nicole Rotter,
Jürgen Hesser, Lena Huber, Andre Dias, Oliver Hoﬀmann, Philipp
Riﬀel, Stefan O Schoenberg, et al. Augmented reality with hololens
in parotid surgery: how to assess and to improve accuracy. Euro-
pean Archives of Oto-Rhino-Laryngology, pages 1–11, 2020.

Huixiang Wang, Fang Wang, Anthony Peng Yew Leong, Lu Xu,
Xiaojun Chen, and Qiugen Wang. Precision insertion of percu-
taneous sacroiliac screws using a novel augmented reality-based
navigation system: a pilot study. International orthopaedics, 40
(9):1941–1947, 2016.

Paul D Schlosser, Tobias Grundgeiger, Penelope M Sanderson, and
Oliver Happel. An exploratory clinical evaluation of a head-worn
display based multiple-patient monitoring application: impact on
supervising anesthesiologistsâĂŹ situation awareness. Journal of
clinical monitoring and computing, 33(6):1119–1127, 2019.

DS Schoeb, J Schwarz, S Hein, D Schlager, PF Pohlmann, A Franken-
schmidt, C Gratzke, and A Miernik. Mixed reality for teaching
catheter placement to medical students: a randomized single-
blinded, prospective trial. BMC medical education, 20(1):1–8,
2020.

A Seginer. Rigid-body point-based registration: The distribution of
the target registration error when the ﬁducial registration errors
are given. Medical image analysis, 15(4):397–413, 2011.

T. Sielhorst, M. Feuerstein, and N. Navab. Advanced medical dis-
plays: A literature review of augmented reality. Journal of Display
Technology, 4(4):451–467, 2008.

Alina Solovjova, Benjamin Hatscher, and Christian Hansen. Inﬂuence
of augmented reality interaction on a primary task for the medical
domain. Mensch und Computer 2019-Workshopband, 2019.

Tianyu Song, Chenglin Yang, Omid Dianat, and Ehsan Azimi. En-
dodontic guided treatment using augmented reality on a head-
mounted display system. Healthcare Technology Letters, 5(5):201–
207, 2018.

Lianne M Wellens, Jene Meulstee, Cornelis P van de Ven, CEJ Ter-
wisscha van Scheltinga, Annemieke S Littooij, Marry M van den
Heuvel-Eibrink, Marta Fiocco, Anne C Rios, Thomas Maal, and
Marc HWA Wijnen. Comparison of 3-dimensional and augmented
reality kidney models with conventional imaging data in the preop-
erative assessment of children with wilms tumors. JAMA network
open, 2(4):e192633–e192633, 2019.

Ming-Long Wu, Jong-Chih Chien, Chieh-Tsai Wu, and Jiann-Der
Lee. An augmented reality system using improved-iterative closest
point algorithm for on-patient medical image visualization. Sen-
sors, 18(8):2505, 2018.

Medha V Wyawahare, Pradeep M Patil, Hemant K Abhyankar, et al.
Image registration techniques: an overview. International Journal
of Signal Processing, Image Processing and Pattern Recognition,
2(3):11–28, 2009.

Jian Yang, Jianjun Zhu, Daniel Y Sze, Li Cui, Xiaohui Li, Yanhua
Bai, Danni Ai, Jingfan Fan, Hong Song, and Feng Duan. Feasi-
bility of augmented reality–guided transjugular intrahepatic por-
tosystemic shunt. Journal of Vascular and Interventional Radiol-
ogy, 31(12):2098–2103, 2020.

Jang W Yoon, Robert E Chen, Phillip K Han, Phong Si, William D
Freeman, and Stephen M Pirris. Technical feasibility and safety
of an intraoperative head-up display device during spine instru-
mentation. The International Journal of Medical Robotics and
Computer Assisted Surgery, 13(3):e1770, 2017.

James Stewart and Mark Billinghurst. A wearable navigation display
can improve attentiveness to the surgical ﬁeld. International jour-
nal of computer assisted radiology and surgery, 11(6):1193–1200,
2016.

Sobia Zafar and Jessica Joanna Zachar. Evaluation of holohuman
augmented reality application as a novel educational tool in den-
tistry. European Journal of Dental Education, 24(2):259–265,
2020.

Qichang Sun, Yongfeng Mai, Rong Yang, Tong Ji, Xiaoyi Jiang, and
Xiaojun Chen. Fast and accurate online calibration of optical
see-through head-mounted display for ar-based surgical naviga-
tion using microsoft hololens. International Journal of Computer
Assisted Radiology and Surgery, 15(11):1907–1919, 2020a.

Zhen-yu Zhang, Wen-chao Duan, Ruo-kun Chen, Feng-jiang Zhang,
Bin Yu, Yun-bo Zhan, Ke Li, Hai-biao Zhao, Tao Sun, Yu-chen
Ji, et al. Preliminary application of mxed reality in neurosurgery:
Development and evaluation of a new intraoperative procedure.
Journal of Clinical Neuroscience, 67:234–238, 2019.

Xuetong Sun, Sarah B Murthi, Gary Schwartzbauer, and Amitabh
Varshney. High-precision 5 DoF tracking and visualization of
catheter placement in EVD of the brain using AR. ACM Trans-
actions on Computing for Healthcare, 1(2):1–18, 2020b.

Yaxuan Zhou, Paul Yoo, Yingru Feng, Aditya Sankar, Alireza Sadr,
and Eric J Seibel. Towards ar-assisted visualisation and guidance
for imaging of dental decay. Healthcare technology letters, 6(6):
243–248, 2019a.

Arthur Tang, Ji Zhou, and Charles Owen. Evaluation of calibra-
tion procedures for optical see-through head-mounted displays. In
The Second IEEE and ACM International Symposium on Mixed
and Augmented Reality, 2003. Proceedings., pages 161–168. IEEE,
2003.

Mathias Unberath, Javad Fotouhi, Jonas Hajek, Andreas Maier,
Greg Osgood, Russell Taylor, Mehran Armand, and Nassir Navab.
Augmented reality-based feedback for technician-in-the-loop c-arm
repositioning. Healthcare technology letters, 5(5):143–147, 2018.

Jens T Verhey, Jack M Haglin, Erik M Verhey, and David E Hartigan.
Virtual, augmented, and mixed reality applications in orthopedic
surgery. The International Journal of Medical Robotics and Com-
puter Assisted Surgery, 16(2):e2067, 2020.

Zeyang Zhou, Zhiyong Yang, Shan Jiang, Fujun Zhang, and Huzheng
Yan. Design and validation of a surgical navigation system for
brachytherapy based on mixed reality. Medical physics, 46(8):
3709–3718, 2019b.

Zeyang Zhou, Zhiyong Yang, Shan Jiang, Fujun Zhang, Huzheng
Yan, and Xiaodong Ma.
Surgical navigation system for low-
dose-rate brachytherapy based on mixed reality. IEEE Computer
Graphics and Applications, 2020.

Ezequiel Roberto Zorzal, José Miguel Campos Gomes, Maurício
Sousa, Pedro Belchior, Pedro Garcia da Silva, Nuno Figueiredo,
Daniel Simões Lopes, and Joaquim Jorge. Laparoscopy with aug-
mented reality adaptations. Journal of biomedical informatics,
107:103463, 2020.

30

Utility of Optical See-Through Head Mounted Displays in Augmented Reality-Assisted Surgery

Yi-bo Zou, Yi-min Chen, Ming-ke Gao, Quan Liu, Si-yu Jiang, Jia-
hui Lu, Chen Huang, Ze-yu Li, and Dian-hua Zhang. Coronary
heart disease preoperative gesture interactive diagnostic system
based on augmented reality. Journal of medical systems, 41(8):
126, 2017.

Yan Zuo, Taoran Jiang, Jiansheng Dou, Dewang Yu, Zaph-
lene Nyakuru Ndaro, Yunxiao Du, Qingfeng Li, Shuyi Wang, and
Gang Huang. A novel evaluation model for a mixed-reality surgical
navigation system: Where microsoft hololens meets the operating
room. Surgical Innovation, 27(2):193–202, 2020.

Appendix A Papers summary ta-

ble

Table A1
Description of AR visualization, Conducted experiments and accuracy of ﬁnal 91 articles used for quantitative synthesis. Acronyms: PM: Preoperative model; II: Intraoperative image. PI: Preoperative
image. IM: Intraoperative model. IV: Intraoperative live streaming video. PV: Preoperatively recorded video. DOC: Documents. COMM: 2D plane with video communication software application
(google hangouts etc.). IND: Intraoperative numerical data. SSE: System setup experiment without phanton, cadaver or patient involvement (may contain additional hardware). PE: Phantom
experiment. HCE: Human cadaver experiment. AE: Animal experiment. ACE: Animal cadaver experiment. SE: Simulator experiment. SCE: Simulated clinical environment experiment. PS: Patient
case study. Abbreviations: Quan: Quantitative study. Qual: Qualitative study.

Study

AR visualizations

Experiments

Chen et al. (2015)

PM: optimal bone drill trajectory, organs, bone struc-
tures

Quan: PE: 1.) registration accuracy, 2.) surgical nav-
igation. HCE: 3.) joint screw implantation

Reported Accuracy

1.) 0.809 ± 0.05 mm, 1.038◦ ± 0.05◦ .

Wang et al. (2016)

Deib et al. (2018)

PM: 3D pelvis model incl. vessels, optimal bone drill
trajectory
II: radiographic images

Andress et al. (2018)

Condino et al. (2018)

Stewart
Billinghurst (2016)
Gibby et al. (2019)

and

de Oliveira et al. (2019)

II: 2D X-ray images inkl. annotations, IM: guiding
lines, planes & spheres, C-arm source position (cylin-
der)
PM: Anatomical 3D models (incl. bones & muscles),
virtual menu with toggle buttons, preoperative plan.
IM: optimal tool trajectory
IM: pose of surgical tool (stack of cyan rings), naviga-
tion target (circle)
PM: virtual trajectories (pedicle screw guidance), lum-
bar spine 2D & 3D CT images
PM: 3D organs incl. ﬁducial or anatomical markers

Aaskov et al. (2019)

PI: anteroposterior lumbar X-ray 2D images

Liebmann et al. (2019)

Fotouhi et al. (2019a)

PM: targeted screw trajectory, drill entry points. IM:
drill angle between current and targeted screw trajec-
tory, 3D trajectory deviation triangle
II: interventional X-ray images, IM: view frustrum

Lin et al. (2018)

Meulstee et al. (2019)

IM: needle visualizations (needle position, orientation
& shape, tangential ray)
PM: 3D objects (cube)

Guo et al. (2019)
Brun et al. (2019)

3D calibration cubes
PM: 3D heart models

Li et al. (2017)
Zou et al. (2017)
Liu et al. (2019)

Kaneko et al. (2016)

Kuhlemann et al. (2017)

Karmonik et al. (2018)

Frantz et al. (2018)

PM: 3D coronay arteries models
PM: 3D cardio artery vascular models
PM: 3D heart, spine & cathether models, 3D catheter
path planning
II: ultrasound images

PM: 3D patient surface mesh, vascular tree, catheter
position, registration landmarks, canvasses (1.) 2D CT
slide, 2.) catheter point of view perspective inside vas-
cular tree)
PM: complex medical vascular & blood ﬂow 3D image
data
PM: 3D skull visualizations, localization markers

Yoon et al. (2017)

II: 2D neuronavigation images

Quan: HCE: joint screw implantation

2.7 ± 1.2 mm, 3.7 ± 1.1mm, 2.9◦ ± 1.1◦

Quan: & Qual: PE: Percutaneous vertebroplasty,
kyphoplasty and discectomy interventions
Qual: SSE: 1.) Calibration, 2.) HMD tracking,
3.)Landmark identiﬁcation, PE: 4.) K-wire guidance,
5.) Entry point localization (implantation of nails)
Quan: 1.) System accuracy estimation (perceived AR
target positions). Qual: 2.) Subjective workload as-
sessments (NASA Task Load Index)
Quan: PE: 1.) tracked tool positioning & orienting,
Qual: 2.) questionnaire
Quan: PE: 1.) Registration accuracy veriﬁcation, 2.)
Percutaneous placement
reliability assessment of virtual-physical
Quan: 1.)
mappings, Quan: & Qual: 2.) assessment of super-
imposed holograms in physical space
Qual: PS: 1.) Accuracy and 2.) repeatability valida-
tion
Quan: PE: guiding wire placement for pedicle screw

Quan: SSE: 1.) Hand-eye calibration experiment, PE:
2.) internal ﬁxation of pelvic ring fractures & percu-
taneous vertebroplasty
Quan & Qual: PE: needle insertion task

SSE: patient based heart model analysis

Quan: PE: 1.) Tight-Fit & Loose-Fit Accuracy Eval-
uation
Quan: SSE: calibration accuracy
Quan:
(anatomy identiﬁcation & diagnosis)
Quan: SSE: Dynamic and static gesture recognition
Quan: SSE: hand gesture recognition rate validation
Quan: PE: catheter navigation under C-arm ﬂuo-
roscopy guidance
Quan: SE: sonographic guided jugular vein catheteri-
zation
Quan: PE: 1.) calibration, 2.) catheter insertion &
navigation, 3.) Likert scale questionnaire evaluation

N/A

1.) 21.4 ± 11.4 mm, 2.) 16.2 ± 9.5 mm, 3.) 8.76 − 11.7 ±
(3.21 − 4.03) mm, 4.) 4.47 ± 2.91 $ 9.84 ± 3.97, 5.) 5.2
mm
1.) 0.6 mm

1.) 0.40 ± 0.78 mm, 2.07 ± 1.68◦

12.99 mm (12.3113.61 mm), 2.)

1.)
(12.3113.61 mm), 15.59 mm (12.0218.69 mm)
1.) 19.74 ± 2.38mm (x), 76.82 ± 3.83mm (y), 2.74 ±
1.96mm (z), 19.74 ± 2.38◦, 76.82 ± 3.83, 2.74◦ ± 1.96◦,
2.) 3.2 ± 1.6 mm (RMSE)
1.) 8.77 mm

12.99 mm

2.77 ± 1.46 mm, 3.38◦ ± 1.73◦

1.) 0.43 mm ± 0.34 mm, 0.43◦ ± 0.34◦

8.15 mm ± 0.4 mm, 6.54 mm ± 0.294 mm, 6.03 mm ±
0.291 mm
1.) 0.7 ± 0.2 mm, 2.) 2.3 ± 0.5 mm

below 6 mm, up to 5◦
N/A

N/A
N/A
0.425±0.021 mm (registration), 0.29±0.19 mm (catheter
position)
N/A

1.) 1.) 4.34 ± 0.709 mm (RMSE point-to-point corre-
spondence)

Quan: SSE: Evaluation of vascular & blood ﬂow image
data
Quan: PE: 1.) Manual registration, 2.) Maintaining
hologram registration via continuous camera tracking
Quan: PS: pedicle screw placement

N/A

1.) 4.39 ± 1.29 mm, 2.) 1.4 ± 0.67 mm (mean perceived
drift)
N/A

(continued on next page)

Table A1

(continued)

Study

AR visualizations

Experiments

Reported Accuracy

Pietruski et al. (2019)

Mitsuno et al. (2017)

Pratt et al. (2018)

Fotouhi et al. (2020)

Qian et al. (2018)

Song et al. (2018)

Hiranaka et al. (2017)
Katić et al. (2015)

Borgmann et al. (2016)

Unberath et al. (2018)
Sauer et al. (2017)

Armstrong et al. (2014)

Mahmood et al. (2018)

Rojas-Muñoz
(2019)
Li et al. (2019)

et

al.

Rojas-Muñoz
(2020a)
Pelanis et al. (2020)

et

al.

Pepe et al. (2019)

Nguyen et al. (2020)

II: 2D navigation monitor. PM: 3D mandible model,
3D osteotomy cutting guides (planes) & navigated sur-
gical saw, IND: cutting guide deviation coordinate sys-
tem
PM: Preoperative & ideal postoperative 3D facial sur-
face and facial bones
PM: 3D bony, vascular, skin & soft tissue structures,
vascular perforators, bounding box
PM: 3D virtual robot arm, 2D reﬂective AR display

PM: & II: 3D plane with endoscopy visualization, IM:
viewing frustrum, PM: endoscope, robotic & hand-held
instruments
PI: 2D radiographic images with guidance information,
IM: 3D drill guidance information
II: ﬂuoroscopic video
PM: & IM: position, depth & alignment of planed & ac-
tual dental drill, injury avoidance warnings, drill heads
II: preoperative CT scan

IM: Live 3D point cloud (C-arm pose)
PM: 3D hepatic artery, portal vein, hepatic veins, liver
tumor, liver capsule
COMM: google hangouts, DOC: articles from senior
author
PM: 3D anatomical models, 3D ultrasound streaming
plane
PM: 3D graphical annotations (incision lines, surgical
instruments)
PM: 3D liver structure (intraoperatively updated), tu-
mor, virtual needle, registration landmarks
PM: 3D graphical annotations (lines & models)

PM: 3D liver incl. parenchyma, portal, hepatic veins
& lesion
PM: 3D landmarks, 3D tumors, 3D axial facial CT
slice
PM: 3D patient head incl. skin, skull & spine

Zhou et al. (2019b)
Pietruski et al. (2020)

Chien et al. (2019)

PM: 3D organs, needle (actual & preoperative plan)
PM: 3D bones, surgical plan:
control points, os-
teotomy trajectories, navigated saw, 2D digital coor-
dinate system
PM: 3D patient skin surface

Zhang et al. (2019)
Heinrich et al. (2019)

PM: 3D intracranial structure, lesion
PM: 3D Needle insertion guidance visualization op-
tions: 1.) planes, 2.) lines, 3.) cone rings

Zhou et al. (2020)

Wellens et al. (2019)

3D anatomy (skin, bones, tumor tissue), virtual nee-
dles (planning & detected), seeds, 2D control panel
PM: 3D kidneys incl. tumor, arteries, veins, urinary
collecting structures

Quan: PE: osteotomies: 1.) augmented navigation
system monitor & 2.) superimposition of surgical plan

1.) 1.79 ± 0.94 mm, 3.67 ± 3.67◦, 2.) 2.41 ± 1.34 mm,
7.14 ± 5.19◦

Quan: PS: reconstructive surgeries (facial fractures or
deformities)
Quan: PS: ﬂap surgery

30 − 40 mm (display error)

N/A

Quan: PE: Registration with 1.) and without 2.) re-
ﬂective AR displays, 2.) Simulated robot-assisted tro-
car placement
Quan: SSE: 1.) Display calibration, 2.) Camera cali-
bration. PE: Visualization performance evaluation

Quan: PE: 1.) Accuracy evaluation, 2.) Tool naviga-
tion & guidance
Quan: PE: Guide wire insertion into femur
Quan: 1.) SSE: Calibration accuracy, 2.) ACE: Im-
plant placement
Quan: PS: diﬀerent urological procedures, Likert scale
questionnaire
Quan: PE: pelvic trauma surgery
Quan: PS: open hepatic surgery

1.) 16.5 ± 11.0 mm, 2.) 30.2 ± 23.9 mm (misalignment
error)

1.) 4.27 ± 3.09 mm

1.) Avg: 0.46 mm, Max: 0.86 mm, Avg: 1.17◦ , Max:
2.10◦
2.6 ± 0.02 mm
1.) 3.01 ± 3.01 mm, 2.) < 2.5 mm (implant deviation),

N/A

51.6 ± 19.2 mm, 1.54 ± 0.92◦
N/A

Quan: PS: reconstructive limb salvage procedure

Quan: SE: transesophageal echocardiography

N/A

N/A

Quan: SE: 1.) anatomical marker placement, 2.) mock
abdominal incision
Quan: 1.) PE: Registration accuracy validation, 2.)
AE: needle insertion operation
Quan: HCE: leg fasciotomy

Quan: SSE: Identiﬁcation of liver segments

1.) 11.37 ± 0.72 mm

1.) 2.24 mm (avg. target registration error)

N/A.

N/A

Quan: PE: Automatic registration after user calibra-
tion
Quan: PE: Registration accuracy. 3 registration meth-
ods: 1.) Keyboard, 2.) Tap to Place, 3.) 3-Point
correspondence matching
Quan: 1.) PE: & 2.) AE: needle insertion
Quan: PE: osteotomy

x: 3.3 ± 2.3 mm y: -4.5 ± 2.9 mm z: -9.3 ± 6.1 mm

1.) X Axis: 5 ± 5◦ Y Axis: -5.9 ± 5.9◦ Z Axis: 6.8 ±
5.9◦; displacement: XY Plane: 2.9 ± 1.8 mm ZY Plane:
1.8 ± 1.2 mm XZ Plane: 1.6 ± 0.9 mm
1.) 0.664 mm, 4.74◦, 2.) 1.617 mm, 5.574◦
4.1 ± 2.29 mm, 5.08 ± 3.64◦, 4.97 ± 2.91◦

Quan: PE: Alignment (diﬀerent data sparsity percent-
ages are tested but we refer only to 100 % of ﬂoating
data being used)
Quan: PS: Craniotomy
Quan: PE: 1.) Registration accuracy estimation: a)
Angle measurement of displayed lines I, b) Angle mea-
surement of displayed lines II, c) Tracked normal vec-
tor accuracy, d) Tracked normal vector accuracy, 2.)
Comparison study
Quan: 1.) PE: & 2.) AE: brachytherapy of tumors

5 reference points alignment error RMSE: Avg.: 0.932
mm, Min: 0.37 mm, Max: 1.49 mm

N/A.
1 a.) 0.76 ± 0.11◦, 1 b.) frontal viewing pos. 1.90 ±
1.82◦, 45◦ viewing pos. 4.28±4.09◦, lateral viewing pos.
7.94 ± 7.75◦, 1 c.) 0.72 ± 0.41◦, 1 d.) X $ Y marker:
0.27 ± 0.21◦, X $ Z marker: 0.31 ± 0.22◦, Y $ Z marker:
0.38 ± 0.36◦
Avg. needle location error: 1.) 0.957 mm, 2.) 2.416 mm

Quan: SSE: Assessment of anatomical structures

N/A

(continued on next page)

Table A1

(continued)

Study

AR visualizations

Experiments

Reported Accuracy

Fotouhi et al. (2019b)

IM: 3D anatomical structures, C-arm principle axis

Baum et al. (2020)

Liounakos et al. (2020)
Jalaliniya et al. (2017)

Rynio et al. (2019)

Boillat et al. (2019)
Zhou et al. (2019a)

Schlosser et al. (2019)

Ponce et al. (2014)

Dickey et al. (2016)

Al Janabi et al. (2020)
Wu et al. (2018)

El-Hariri et al. (2018)

Liebert et al. (2016)

Gnanasegaram et
(2020)
Sun et al. (2020b)

al.

PM: 3D patient skin surface, brain, intra-cortical le-
sion
II: live endoscopic camera image
COMM: videoconferencing application, PI: patient
records
PM: 3D arterial system, aneurysm, bones, PI: 2D im-
age with volume rendering, arterial diameters & plan-
ning notes
PM: 2D surgical safety checklist
PM: 3D tooth, cone (endoscope view frustrum), probe
alignment cyclinder & planes, II: 2D imaging
IND: 2D screen incl. patient heart rate, blood pres-
sure, blood oxygen saturation, alarm notiﬁcations
IV: hybrid image (surgical ﬁeld combined with hands
of remote surgeon)
IV: interactive video display incl.
supervising physician, PV: training guide
PI: CT images, IV: live ﬂuoroscopy, endoscopic view
PM: 3D patient anatomy (e.g. head, intracranial vas-
cular tissue)
PM: 3D bone structures, ﬁducial markers

cursor moved by

IND: 2D screen incl. patient arterial line blood pres-
sure, heart rate, heart rhythm, pulse oximetry, respi-
ratory rate
PM: 3D ear anatomy

PM: 3D catheter

Park et al. (2020)

PM: 3D volumes from MRI images

Mendes et al. (2020)

Laguna et al. (2020)

PM: 3D model of body simulator’s external surface
(upper torso) and 3D vascular structures
PM: 3D elbow fractures (bones)

Dallas-Orr et al. (2020)

PM: 3D spine model with a vascular model overlay

Zafar and Zachar (2020)

PM: 3D human skull

Fitski et al. (2020)

Schoeb et al. (2020)

Luzon et al. (2020)

PM: 3D intraparenchymal arteries and veins, kidney,
tumor
PV: 2D plane with catheter placement instruction
guidance
PM: 3D anatomy models (e.g. vascular model)

Matsukawa and Yato
(2020)
Yang et al. (2020)

Li et al. (2020b)

II: ﬂuoroscopic 2D image

PM: 3D portal vein and hepatic vein, liver

PM: 3D internal organs, PI: CT images, IM: progress
view of the virtual planned target, needle path, skin
entry point and needle end

Quan: SSE: 1.) calibration accuracy, PE: 2.) Target
augmentation error, 3.) Augmented surgical visualiza-
tion
Quan: PE: Target Localization

Quan: PS: Lumbar discectomy
Quan: PE: SCE: Mobile access to patient records,
telepresence
Quan: PS: Abdominal aortic aneurysm repair

Quan: SSE: time-out checklist execution
Quan: PE: 1.) Augmentation quality evaluation, 2.)
Dental decay localization
Quan: & Qual: PS: vital sign monitoring, Quan: situ-
ation awareness measurement
Quan: PS: shoulder replacement

Qual: & Quan: SSE: user survey

1.) 5.7 ± 0.26 mm, 2.) 10.8 ± 3.45 mm

N/A

N/A
N/A

N/A

N/A
31 ± 11 px (keypoint displacement)

N/A

N/A

N/A

Qual: & Quan: SE: mid-ureteric stone removal
Quan: PE: dummy head alignment test

N/A
< 3 mm (Avg. Target Registration Error)

Quan: PE: accuracy assessment

Quan: SE: Vital signs monitoring during bronchoscopy

Fiducual marker comparisons (RMSE): x: 3.22 mm, y:
22.46 mm, z: 28.30 mm
N/A

Quan: SSE: spatial exploration of holographic ear
model
Quan: SSE: 1.) Stability measurement of tracking al-
gorithm, 2.) testing of tracking accuracy 3.) latency
test using third-party tracker, 4.) HCE: EDV per-
formed on a cadaveric head
Quan: AE: transarterial embolization of hepatocellu-
lar carcinoma (HCC)
Quan: SE: tracked needle insertion

Quan: SSE: Orthopedic surgeons’ assessment of 3D
AR models for presurgical planning in complex pedi-
atric elbow fractures
Quan: SSE: 3D model measurement using circumfer-
ence and angle tools of standard-of-care PACS software
Quan: SSE: digital anatomy session with the HoloHu-
man virtual anatomy training software
Quan: PS: Preoperative planning of patients eligible
for nephron-sparing surgery (NSS)
Quan: SE: Bladder catheter placement using a male
catheterization-training model
Quan: PE: registration and needle placement

Quan: PS: single-segment posterior lumbar interbody
fusion (PLIF) at L5âĂŞS1
Quan: 1.) AE: dogs: simulated percutaneous puncture
of the portal vein and simulated TIPS, 2.) PE: liver
phantom experiment
Quan: PE: 1.) image overlay accuracy using 3D ab-
dominal phantom, 2.) needle placement performance
using tissue phantom

N/A

2.) avg. distance from catheter tip to corresponding grid
intersections (2D plane): 0.58 mm, overall avg. accuracy
on all 3 grid faces: 0.85 mm (3D space)

N/A

N/A

N/A

N/A

N/A

N/A

N/A

Target error distance: x-axis: 2.9757 ± 1.33396 mm, y-
axis: 2.2790 ± 1.44992 mm, z-axis: 2.7844 ± .91323mm
N/A

N/A

1.) Total target overlay error over 336 targets: 1.74 ±
0.86 mm. Needle overlay angle: 0.41 ± 0.23◦

(continued on next page)

Table A1

(continued)

Study

AR visualizations

Experiments

Reported Accuracy

Kumar et al. (2020)

PM: 3D liver and heart, slicing tool (plane)

Li et al. (2020a)

Gibby et al. (2020)

PM: 3D target organs and tumors (kidney, tumor,
renal vessels, renal collection system, skin, skeleton,
liver, spleen), PI: MR results, IV: laparoscopic video
stream
PI: 3D plane with axial CT image, PM: needle trajec-
tories in correct spatial orientation over patient

Gu et al. (2020)

PM: 3D glenoid and planed drilling path

Galati et al. (2020)
Viehöfer et al. (2020)

PM: 3D patient anatomy
PM: 3D foot

Dennler et al. (2020)

PM: vertebral body

Kriechling et al. (2020)

PM: planned drill trajectory, IM: current drill trajec-
tory, deviation in degrees and millimeters

Zorzal et al. (2020)

IV: 2D plane with laparoscopic video feed, PI: 2D
plane with MR image sclices

Cartucho et al. (2020)

Rojas-Muñoz
(2020b)

et

al.

Scherl et al. (2020)

PM: 3D organ models (brain and liver), PI: 2D planes
with volumetric MRI/CT imaging data (with scrolling
bar), 2D plane with intraoperative data (pCLE, iUS)
(with transparency adjustment up and down arrows))
PM: 3D annotations (incision lines) and 3D surgical
tools

PM: 3D mandible, parotid, tumor, head, grey circles,
operating menu (buttons), PI: 2D MRI images

Creighton et al. (2020)

PM: 3D skull and temporal bone

SSE: visualisation of patient-speciﬁc models and holo-
gram interaction (rotate, scale and move)
Quan: PS: Prospective review of patients with stage
T1N0M0 renal tumors who untervent laparoscopic par-
tial ephrectomy

N/A

N/A

Quan: 1.) PE: control data experiment (needle naviga-
tion) using skull with ballistic gelatin and radiopaque
balls (targets), 2.) PS: interventional spine procedures

Quan: PE: 1.) inside-out registration (via HoloLens
depth sensing camera), 2.)
accuracy evaluation of
inside-out registration using outside-in tracking with
optical tracker, 3.) registration with surface digitisa-
tion
Quan: PS: open abdomen surgeries
Quan: PE: distal osteotomy

Quan: PE: drilling pilot holes in lumbar vertebra saw-
bones models

Quan: PE: Using 3D printed scapula based on scans of
human cadavers: Guidewire positioning of the central
back of he
Qual: & Quan: SE: laparoscopic training simulator
incl. MR images and pre-recorded laparoscopic video
feed
Quan: SSE: interaction with the visualisation compo-
nents and exploration of holographic functionalities

Quan: SE: performing cricothyroidotomies in a simu-
lated austere scenario (smoke and loud noises of gun-
shots and explosions)
Quan: PS: live parotid surgery: study persons who did
not participate in the actual surgery performed manual
ologram
Quan: PE: Evaluation of manual target registration
error using skull model

Jiang et al. (2020)

PM: 3D vascular map, surrounding soft tissues, marker Quan: PE: Precision veriﬁcation of the vascular local-

Sun et al. (2020a)

PM: 3D planned mandibular reconstruction result

ization system
Quan: PE: 1.) Accuracy validation experiment for
OST-HMD calibration (3D printed skull with ﬁdu-
cials), 2.) Calibration method testing, 3.)
PS:
mandibular reconstruction

1.) 0.998±1.66 mm (mean error of needle tip to targeted
ball). Mean distance from model surface to targeted ball:
79.42 ± 15.33 mm, 2.) Mean error of needle to target:
1.73 ± 2.20 mm
1.) Inside-out registration accuracy compared with ex-
ternal tracking (optical trackser is used to verify inside-
out tracking): translation (max: 21.82 ± 2.33 mm), ro-
tation: max. 8.10 ± 2.89◦

N/A
Mean deviation between osteotomy plane and target
plane perpendicular to the second metatarsal (anterior
direction): 1.) Experienced surgeons: 4.9 ± 4.2◦, 2.) less
experienced surgeons: 6.4 ± 3.5◦
average minimal distance of the drill axis to the pedicle
wall (MAPW): 1.) Expert surgeons: 5.0±1.4 mm, novice
surgeons: 4.2 ± 1.8 mm
mean deviation of placed guidewires from the planned
trajectory: 2.7 ± 1.3◦, mean deviation to the planned
entry point of the placed guidewires: 2.3 ± 1.1 mm
N/A

N/A

N/A

Manual Registration accuracy using ﬁduical markers on
the head: outer borders of face: 10.09 ± 4.23mm, arotid:
13.39 ± 4.71 mm, Tumor: 13.29 ± 5.66 mm
Target registration error: 10.62 ± 5.90 mm10.62

mean errors (under diﬀerent conditions): min: 1.35 ±
0.43 mm , max: 3.18 ± 1.32
1.) Avg. root-mean-square error of control points be-
tween rendered object and skull model: 1.30 ± 0.39

