HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor
Space Using Wearable IMUs and LiDAR

Yudi Dai1 Yitai Lin1 Chenglu Wen1,* Siqi Shen1 Lan Xu2 Jingyi Yu2 Yuexin Ma2 Cheng Wang1

1Xiamen University, China

2ShanghaiTech University, China

Figure 1. 3D human motions with accurate localization in diverse, challenging scenes. The top line shows a person
walking from indoor to outdoor. The bottom row figures show the challenge cases, where the bottom right figure is the rock
climbing’s third-view from the camera.

Abstract

We propose Human-centered 4D Scene Capture
(HSC4D) to accurately and efficiently create a dynamic
digital world,
containing large-scale indoor-outdoor
scenes, diverse human motions, and rich interactions
between humans and environments. Using only body-
mounted IMUs and LiDAR, HSC4D is space-free without
any external devices’ constraints and map-free without
pre-built maps.
Considering that IMUs can capture
human poses but always drift for long-period use, while
LiDAR is stable for global localization but rough for local
positions and orientations, HSC4D makes both sensors
complement each other by a joint optimization and achieves
promising results for long-term capture. Relationships
between humans and environments are also explored to
make their interaction more realistic. To facilitate many
down-stream tasks,
like AR, VR, robots, autonomous
driving, etc., we propose a dataset containing three
large scenes (1k-5k m2) with accurate dynamic human

*Corresponding author.

motions and locations. Diverse scenarios (climbing gym,
multi-story building, slope, etc.) and challenging human
activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization
ability of HSC4D. The dataset and code is available at
http://www.lidarhumanmotion.net/hsc4d/.

1. Introduction

The development of digital society is overwhelming be-
cause it can enrich peoples’ life by serving Augmented Re-
ality, Virtual Reality, smart city, robots, autonomous driv-
ing, etc. Humans and environments are two main compo-
nents for creating the digital world. Current research works
always separate dynamic human motions and static envi-
ronments. Actually, taking account their interactions can
help improve both capture accuracy. It is a trend to directly
capture the whole scene with consecutive human activities.

To capture human motions, IMU sensors are widely used
and always be mounted on different parts of the human
body, like arms, legs, feet, head, etc. It can capture accu-
rate short-term motions but suffer from severe drift with the
acquisition time increasing. Some methods [6,13,39,47,48]
utilize extra external RGB or RGBD cameras as a remedy
to improve the accuracy, but result in limited capture space,
human activities, and interactions. HPS [9] uses a head-
mounted camera, which looks outwards like the human
eyes, to complement IMUs in global localization. Without
the constraint of external cameras, it can recover the full-
body pose and register the human in large 3D scan of real
scenes. However, HPS requires pre-built maps and a huge
image database for self-localization.

For accurate localization and mapping [51], LiDAR is
the most applicable sensor in current days, which is popu-
lar for mobile robots and autonomous vehicles. LiDAR is
also extensively used for large-scale scene scans. Although
there are many LiDAR-captured datasets, including indoor
scenes [4, 30] and large-scale outdoor scenes [7, 23], they
focus on scene understanding and 3D perception, ignoring
accurate human poses. PedX [16] provides 3D poses of
pedestrians by using SMPL [22] parameterization for joint
locations of instances on third-person-view images, which
is not accurate as IMUs. Furthermore, it focuses on traf-
fic scenes and is not applicable for generating diverse 3D
human motions.

Taking advantage of IMUs-based motion caption and
LiDAR-based localization and scene capture, we propose
Human-centered 4D Scene Capture (HSC4D) to accurately
and efficiently create a dynamic digital world with con-
secutive human motions in indoor-outdoor scenes. Using
only body-mounted sensors, HSC4D is space-free and pose-
free, and the interaction between humans and the environ-
ment inside is also free, which makes it possible to capture
most of the human-involved real-world scenes. Compared
with camera-based localization, LiDAR is more precise for
global localization, which dramatically reduces the drift of
IMUs, and does not need pre-built maps. IMUs can improve
the accuracy of LiDAR-captured local trajectories, where
the error is caused by the jitter of the body. Making use of
the complement of both sensors, we propose a joint opti-
mization to improve the performance of motion estimation
and human-scene mapping by considering several physical
constraints.

To facilitate further research and down-stream applica-
tions, we propose a dataset containing three large scenes
(1k-5k m2) with accurate dynamic human motions and lo-
cations. As Fig. 1 shows, the dataset contains diverse sce-
narios, like climbing gym, multi-story building, slope, etc.,
and challenging human activities, such as exercising, walk-
ing up/down stairs, climbing, etc. Accurate human poses
and natural interactions between human and the environ-

ment demonstrate the effectiveness and the generalization
ability of HSC4D.

Our contributions are summarized as follows:
• Based on body-mounted IMUs and LiDAR, we pro-
pose Human-centered 4D Scene Capture (HSC4D)
for creating a human-centered dynamic digital world,
which is space-free, pose-free, and interaction-free.
• We propose a joint optimization method by integrat-
ing LiDAR SLAM results and IMU poses with scene
constraints, resulting in natural human motions and ac-
curate global localization in large scenes.

• We provide a new dataset containing LiDAR point
cloud of large-scale scenes, IMU data of human poses,
and the results of poses and mapping by our optimiza-
tion, which also demonstrates the effectiveness and the
generalization ability of HSC4D.

2. Related work

2.1. IMU sensors for human pose estimation

IMU sensors have been widely used to capture hu-
man motions [11, 29, 38, 41]. However, IMU-based meth-
ods suffer from severe drift over time. To improve the
pose estimation accuracy, some methods [6, 13, 24, 39, 40]
utilize extra external RGB or RGBD cameras as a rem-
edy. Helten et al. [10] combined two RGB-D cameras
with IMUs to perform local pose optimization. Hybrid-
Fusion [54] has achieved more accurate motion tracking
performance by combining an RGBD camera with multi-
ple IMUs. 3DPW [39] uses a single hand-held RGB cam-
era and IMUs to optimize human pose for a certain period
of frames simultaneously. Constraints from external cam-
eras assist in recovering more accurate 3D poses but re-
sult in limited capture space, human activities, and inter-
actions. HPS [9] uses a first-view head-mounted camera to
self-localize the 3D pose from IMUs to the scene. However,
HPS requires pre-built maps and an image database for self-
localization. Instead, we use only body-mounted IMUs and
LiDAR. Without any external devices’ constraints and any
pre-built maps, we achieve promising results for long-term
human motion capture.

2.2. Human self-localization methods

Human self-localization aims at estimating the 6-DoF
of the human subject with carrying devices.
The re-
ceived signal strength (RSS) fingerprinting-based method-
ologies [1, 2, 18] are widely used for indoor human local-
ization. However, these methods need external receivers
and are limited to the indoor space. Some image-based
methods [15, 28, 42] regress locations directly from a single
image with a pre-built map. Still, the scene-specific prop-
erty makes them hard to generalize to unseen scenes. Some
methods integrate IMU as an aid sensor [25, 34] to improve

accuracy. With robustness and low drift, LiDAR-based lo-
calization has been successfully applied in indoor [27, 44]
and outdoor [19, 37, 49, 50] scenes. To localize the human
subject, LiDAR are designed as backpacked [14, 21, 46]
and hand-held [3]. LiDAR-based localization systems are
usually big pieces of equipment and would affect human
motion. We design a lightweight hip-mounted LiDAR to
rigidly connect with the human body, achieving human self-
localization in both large indoor and outdoor scenes.

2.3. LiDAR-based mapping methods

LiDAR is currently the most applicable sensor for 3D
mapping. As a pioneer, zhang et al. [51] proposed LOAM,
a real-time odometry and mapping method using a LiDAR,
greatly boosting the 3D mapping research. Some methods
[12, 20, 33, 43] further improve LOAM mapping for spe-
cific scenes and sensors. LeGo-LOAM [33] is a ground-
optimized version, which requires to keep the LiDAR hori-
zontal. LiDAR-based methods tend to fail when the Z-axis
jitters severely. To address the drift problem and improve
robustness, more sensors, such as visual sensors [32,36,52],
IMU [8, 26, 34], or both [5, 35, 55], have been integrated in
mapping task. To make the system lighter and able to work
wirelessly, we propose a LiDAR-only method for localiza-
tion and mapping in the scene. The joint optimization result
with scene and IMU poses will further improve the LiDAR
mapping result.

3. System setup

3.1. Notations and Task Description

The problem addressed in this paper is to estimate the
3D human motion with a 3D spinning LiDAR and IMUs in
a large unknown scene and build a map for it, where human
motion includes local 3D pose and global localization.
Notations. The N frames human motion is represented as
M = (T, θ, β), where T is the N × 3 translation parameter,
θ is the N × 24 × 3 pose parameter, and β is the N × 10
shape parameter. We assume that β is constant during a data
recording. The 3D point cloud scene is represented as S.
We use right subscript k, k ∈ Z + to indicate the index of
a frame. We use the Skinned Multi-Person Linear (SMPL)
body model [22] Φ(·) to map Mk to human mesh models
Vk, Vk ∈ R6890×3.

Let us define three coordinate systems: 1) IMU co-
ordinate system {I}: origin is at the hip joint of the
first SMPL model, and X/Y /Z axis is pointing to the
right/upward/forward of the human. 2) LiDAR Coordi-
nate system {L}: origin is at the center of the LiDAR, and
X/Y /Z axis is pointing to the right/forward/upward of the
LiDAR. 3) Global coordinate system {W }: the first LiDAR
frame’s coordinate.
Task definition.

Given a sequence of LiDAR sweep

k , k ∈ Z + in {L} and a sequence of 3D Human mo-
P L
tion M I
k in {I}, compute the human motion M W
in {W }
k
and build the 3D scene S with P I
k .

3.2. System design

Figure 2. Overview of our capturing system, which includes
17 body-attached IMU sensors, a LiDAR sensor, and a mini-
computer.

Hardware. We use a 64-beams Ouster LiDAR to acquire
3D point clouds P L
k , and Noitom’s inertial MoCap product
PN Studio to obtain human motion M I
k . The PN Studio uses
17 IMUs attached to the body limbs and a wireless receiver
to acquire data. To make the LiDAR work wirelessly, we
connect the LiDAR to a 9cm × 6cm × 3cm DJI Manifold2-
C mini-computer and use a 24V mobile power to charge the
LiDAR and the computer. To ensure a lightweight and pre-
cise capturing system, We modified all cables and designed
a 10cm × 10cm L-shape bracket to mount the LiDAR pack-
age. The battery and Manifold2 are stored in a small bag on
the human’s back. The LiDAR is worn tightly close to the
hip bone, making the origins of {I} and {L} as close as
possible. Thus, we assume that LiDAR and IMUs have a
rigid transformation.

The Ouster LiDAR has a 360° horizon view and a 45°
vertical view. However, due to the occlusion caused by back
and swing arms, the horizon field of view is reduced, rang-
ing from 150° to 200°. To avoid most laser points hitting
the nearby ground, we tilt up the LiDAR for 30° to get a
good vertical scanning view.

4. Approach

We first obtain the 3D human motion output by the in-
ertial MoCap system. Second, we estimate the ego-motion
of LiDAR and build a 3D scene map S through point cloud
data P I
k . Then we perform a data initialization to prepare
data for further optimization. Later we perform a graph-
based optimization to fuse LiDAR trajectory and IMU tra-
jectory. Finally, by combining the LiDAR data, IMUs data,
and 3D scene, a joint optimization is performed to give the
human motion M and an optimized scene.

30°IMU sensorsLiDARManifold2-CFigure 3. Overview of our method. The inputs are point cloud sequence and IMUs data. IMUs pose estimation and LiDAR mapping
are performed, respectively. Synchronization and calibration are applied to prepare data for further optimization. Finally, graph-based
optimization and joint optimization are performed to produce the global motion in the scene map.

4.1. IMUs Pose Estimation

This subsection aims to estimate the motion M I =
(T I , θI , β) in IMU coordinate {I}, where T I and θI are
provided by the commercial MoCap product. Pose parame-
ter θI is composed of the hip joint’s orientation RI relative
to the start frame and other joints’ rotation relative to their
parent joint. T I
k indicates the k-th frame translation relative
to the start. Since IMU is accurate in a short period, the
relative value of T I and RI can be used in the optimization.

4.2. LiDAR Localization and Mapping

Building a map using LiDAR data is challenging in this
scene because the LiDAR jitters as the human walking and
human body occludes the field of scanning view. By em-
ploying LiDAR-based SLAM methods [45,51], we estimate
the ego-motion of LiDAR and build the 3D scene map S
k , k ∈ Z + in {L}. We first exact planer and edge
with P L
feature points in every LiDAR scan P L
k and keep updating
the feature map. Similar to [45], we skip frame to frame
odometry and only perform frame to map registration be-
cause the mapping process can run offline. Finally, Lidar’s
ego-motion T W and RW , and the scene map S are com-
puted. The mapping function is denoted as:

T W , RW , S = F(P L

1:N )

(1)

4.3. Optimization initialization

Coordinate calibration. To obtain the rigid offset from
LiDAR to IMU and make all coordinate systems aligned,
we perform following steps: First, the human stands as an

A-pose before capture, and the human’s face direction is
regarded as scene’s Y -axis direction. After capturing, we
rotate the scene cloud Z-axis perpendicular to the starting
position’s ground. Last, we translate the scene to make its
origin to the first SMPL model’s origin on the ground. Li-
DAR’s ego motion T W and RW are translated and rotated
as the scene does. To now, LiDAR data are calibrated to
{W }. The pitch, roll, and translation of IMU are calibrated
to the world coordinate. The IMU’s yaw will be further re-
fined in Sec. 4.4.

Time synchronization. Firstly, we ask the subject to jump
at the starting place of every capture. Secondly, we auto-
matically locate the peaks in both T W and T I based on
their z value. Then, we can synchronize the LiDAR and
IMU according to the two peaks’ timestamps. Finally, we
resample the IMU data (100Hz) to the same frame rate as
LiDAR (20Hz).

4.4. Graph optimization data fusion

As seen from Fig. 4, IMUs drift severely over time and
fail when the scene’s height change, while the LiDAR lo-
calizes correctly but jitters at local movement. To estimate
a more smooth and stable trajectory, we utilize both data’s
advantages. Our strategy is presented as follows: 1) first
mark the frame in T W that exceeds x (1.2∼2.0) times of
IMU velocity and the local fitted value as the outliers, 2)
treat the remained (RW , T W ) as landmarks, and then seg-
ment T I every five seconds, 3) The nodes (IMU or land-
mark poses) and edges (a relative transformation between
two nodes) construct a graph, 4) finally perform a graph op-

LiDARPoint cloud sequenceHuman PosesIMUs Pose EstimationAttachedIMUsInitial TrajectoryScene MapIMU TrajectorySynchronizationCalibrationLiDAR MappingFoot Contact LossFoot sliding LossOrientation constraintSmoothnessconstraintInputEstimation and InitializationJoint optimizationOutputScene MapGlobal MotionGraph OptimizationHPS knowing the information about which foot is stepping
on the floor, we detect the foot state based on the move-
ments. First, we compare the left and right foot movement
for every successive foot vertices in V I
k, β)
from IMU. One foot is marked as a stable foot if its move-
ment is smaller than 2cm and smaller than another foot’s
movement. The k-th frame’s stable foot vertices index list
in Vj is denoted as Sk and the foot contact loss Lcont is
written as:

k = Φ(T I

k , θI

Lcont =

1
l

k+l
(cid:88)

(cid:88)

j=k

Sj
vc∈V
j

1
|Sj|

∥vc − (cid:101)vcpj∥2,

(3)

where (cid:101)vc is homogeneous coordinate of vc. V Sj
is denoted
as Sj foot vertices in Vj = Φ(Mj), which is from the mo-
tion to be optimized.

j

Foot sliding constraint. The foot sliding constraint re-
duces the motion’s sliding on the ground, making the mo-
tion more natural and smooth. The sliding loss is defined as
every two successive stable foot’s distance:

Lslid =

1
l

k+l−1
(cid:88)

j=k

∥E(V Sj+1

j+1 ) − E(V Sj+1

j

)∥2,

(4)

where E(·) is the average function.
Orientation constraint. This constraint encourages the
motion M to rotate as smooth as IMU and have the same
orientation with the landmarks A described in Sec. 4.4. The
orientation loss is written as follows:

Lorit =

1
|A|

(cid:88)

j∈A

∥(Rj)−1RW

j ∥2+

Figure 4. Comparison of trajectories of IMU and LiDAR.

timization [17] method to couple T I and T W .

4.5. Joint optimization

To obtain accurate and scene-natural human motion
M = (T, θ), and a higher quality scene cloud S, we per-
form a joint optimization method by using scene S and
physics constraints. Then we send T back to mapping func-
tion F as an initial value to create a new scene Sopt. We use
the following four constraints: the foot contact constraint
Lcont encouraging the human standing on the ground, the
sliding constraints Lsld eliminating the human walk slid-
ing, the orientation constraint Lort from RI making the ro-
tation smooth, and the smoothness constraint Lsmt making
the translation smooth. The optimization is expressed as:

L = λcontLcont + λsldLsld + λortLort + λsmtLsmt
L(M |T I , θI , RW , S)
M = arg min
M
Sopt = F(P L
1:N , M ),

(2)

1
l − 1

k+l−1
(cid:88)

j=k

max(0, ∥(Rj)−1Rj+1∥2 − ∥(RI

j )−1RI

j+1∥2).

(5)

where λcont, λsld, λort, λsmt are coefficients of loss terms.
L is minimized with a gradient descent algorithm to itera-
tively optimize M (i) = (T (i), θ(i)), where (i) indicates the
iteration. M (0) is set as (T W , θI ),
Plane detection. To improve validity of foot contact, we
detect the planes near the human. We first use Cloth Simula-
tion Filter (CSF) [53] to extract ground points Sg in S. And
then we search neighboring points of T W in Sg. Unlike the
dense mesh model, the discrete point cloud has empty ar-
eas, resulting in invalid foot contact constraints. To adress
this, we use RANSAC [31] to fit planes for the neighboring
points. We denote the plane fuction as pk.
Foot contact constraint. The foot contact loss is defined as
the distance from a stable foot to its nearest ground. Unlike

Smooth constraint. This constraint encourages the human
motion to move as smoothly as IMU motion, minimizing
the difference between LiDAR and IMU sensors’ transla-
tion distance. The smooth loss term is as follows:

Lsmt =

1
l

k+l−1
(cid:88)

j=k

max(0, ∥Tk − Tk+1∥2 − ∥T I

k+1 − T I

k ∥2).

(6)

5. Experiments

This section introduces our dataset and evaluates HSC4D
in large indoor-outdoor 3D scenes. The results demonstrate
the effectiveness and the generalization ability of HSC4D.

LiDAR trajectoryMoCap trajectoryInside the buildingFigure 5. The large indoor and outdoor scenes in our dataset. Left: a climbing gym (1200 m2). Right top: a lab building with an outside
courtyard 4000 m2. Right bottom: a loop road scene 4600 m2.

5.1. Dataset

5.2. Comparison

We propose an HSC4D dataset containing three large
scenes: a rock climbing gym, a multi-story building, and
an outdoor closed-loop road. The gym has a wall height of
20 meters, with a ground and climbing area size over 1200
m2. The building scene’s indoor and outdoor area size is up
to 5000 m2. The scenes have a diversity of heights and en-
vironments, including multi-story, slope, and staircase. The
outdoor closed-loop road is 70m×65m with slope. In these
scenes, captured human activities include walking, exercis-
ing, walking up/down stairs, rock climbing, speeching, etc.
Since the 3D map from LiDAR lacks color information,
we use a Terrestrial Laser Scanner (Trimble TX5) to scan
the color scenes for better visualization. In summary, The
HSC4D dataset provides 250K IMU frames (100Hz), 50k
time-synchronized LiDAR frames (20Hz), our SLAM re-
sults, and colored ground truth point clouds of the scenes.

HPS

HSC4D

Sensors

Input

IMU-based Mocap
Head-mounted camera(*)
LiDARs with cameras(+)

IMU motions
Video frames(*)
Pre-built 3D map(+)

IMU-based Mocap
Hip-mounted LiDAR(*)

IMU motions
LiDAR frames(*)

Table 1. Comparison between HSC4D and HPS.
object. (+): Additional object.

(*): Different

The most related work to ours is HPS [9], which uses
IMUs, a head-mounted camera, and NavVis M6 equipped
with 6 cameras and 4 LiDARs for human-scene modeling.
Besides, HPS also heavily records all images registered to
the captured 3D map for localization. In contrast, We re-
move the tedious reliance on the pre-built map in HPS ab-
tained using NavVis. Our approach is scene-prior-free and
contributes a novel body-worn setting of LiDAR and IMUs
for more practical human-scene modeling. (Tab. 1)
Baselines. Since HSC4D is the first method in such scenes,
there is no published baseline available to compare. For
effective comparison, we name the IMU result as Baseline1
and IMU + LiDAR without any optimization as Baseline2.
Global localization comparison. First, in the ground truth
point clouds provided by Trimble TX5, we mark some loca-
tions on the ground as checkpoints. Then the subject steps
on the checkpoint during capturing. At last, we measure the
distance from the SMPL model estimated by the method to
the checkpoint as global localization error.
Local pose comparison. To evaluate the local pose ac-
curacy and smoothness, we compare the foot contact loss
Lcont and the sliding loss Lsld described in Sec. 4.5. The
comparison is shown in Tab. 4.

Tab. 3 shows the localization error comparisons between
our method and baselines. As the distance increases in the

Sequences

Road
Gym01
Building01
Building04

r = 0.6m
l = 100

0.96/1.06
0.66/0.65
0.80/1.20
0.92/0.96

Loss term

Scene cropping radius

Length of optimization

w/o Lcont

w/o Lsld

w/o Lsmt

r = 0.4m

r = 0.8m

r = 1.0m

l = 50

l = 200

+4.44/-0.03
+7.03/-0.06
+5.47/-0.21
+5.88/-0.04

+0.01/+1.06
+0.02/+0.70
+0.04/+0.77
-0.04/+0.86

-0.06/+0.43
-0.31/-0.05
-0.10/-0.35
-0.12/-0.40

+0.02/+0.00
+0.02/+0.00
+0.04/-0.12
+0.03/+0.00

-0.01/+0.00
-0.01/+0.00
+0.02/-0.13
-0.01/+0.00

-0.01/+0.00
-0.01/-0.01
+0.01/-0.13
-0.02/+0.01

-0.03/-0.06
+0.00/+0.00
-0.03/-0.22
-0.02/+0.01

-0.02/-0.03
+0.01/+0.00
+0.02/-0.13
-0.01/+0.00

Table 2. Quantitative evaluation of our method: We record Lcont and Lsld in column two. Column 3-5: loss term ablating comparisons.
Column 6-8: analysis of neighborhood radius r for cropping the scene. Column 9-10: analysis of the optimization sequence length l.

CheckPoint

Building (P1)
Building(P2)
Building(P3)
Road(Pend)

Distance
to start

35m
350m
480m
250m

Baseline1

Balseline2 HSC4D

65.96
214.74
331.84
192.17

8.14
17.79
24.45
89.15

7.01
11.02
18.11
67.93

Table 3. Global localization error comparison between base-
lines and HSC4D. Baseline1: IMU result. Baseline2: IMU pose
+ LiDAR localization. Error is measured as distance (cm) from
the selected CheckPoint (CP) to the SMPL model’s foot.

Terrain

Sequences

Baseline1

Baseline2

HSC4D

Lcont/Lsld

Lcont/Lsld

Lcont/Lsld

Flat
Flat
Flat
Flat
Flat
Stairs
Stairs
Slope

Building01
Building03
Gym02
Gym03
Gym01
Building02
Building04
Road

2.35/1.00
3.54/0.80
4.20/0.43
44.156/1.14
187.65/0.56
389.70/1.05
394.46/0.89
445.39/1.03

6.22/2.57
4.13/2.26
5.33/1.08
3.21/2.08
7.88/1.72
7.58/2.67
6.80/3.09
8.59/3.70

0.80/1.20
0.64/0.91
0.78/0.20
0.79/0.26
0.66/0.65
0.81/0.87
0.92/0.96
0.96/1.06

Table 4. Local pose error comparison between baselines
and HSC4D. Building01/03: 1.5min/1min sequence on the sec-
ond/first floor’s corridor. Building02/04: 3min/1min outdoor se-
quences, including stairs and slope. Gym01/02/03:
three one-
minute walking and warming up sequences. Road: 3.5 minute
walking sequence.

Figure 6. Qualitative results of local pose projected to images.

scene, the error increases linearly in all methods. Base-
line1’s error is ten times compared to other methods because
IMU drifts severely over time. Baseline2 has a smaller
global localization error, but its accumulative errors still
vary from 8cm to 90cm. The last column shows that
HSC4D achieves the smallest global localization errors in
the multi-story building and the road with slopes. More
specifically, HSC4D improves 78.3% accuracy compared to
Baseline1 and 25.4% compared to Baseline2. In Fig. 7, we
show comparisons between baselines and our HSC4D.

Figure 7. An example of comparison between Baseline1, Base-
line2, and HSC4D. The colorful spheres represent the trajecto-
ries. HSC4D results are natural and accurate in both cases. Left:
Baseline1 and Baseline2 are floating in the air. Right: Baseline1 is
floating in the air, and Baseline2’s lower leg penetrates the ground.

Tab. 4 shows the comparison of local pose errors be-
tween our method and baselines. Baseline1’s foot contact
loss is much larger than other methods especially in scenes
where height change. Baseline2’s Lsld is the largest among
all methods. In the first three sequences where Baseline1
is not drift over height, Baseline2’s Lcont is much more
larger than Baseline1. These cases indicate that LiDAR
increases local errors. See from the last column, HSC4D
significantly decreases Lcont in all cases and achieve com-
parable smoothness on Lsld compared to Baseline1. These
comparisons reveal that HSC4D can achieve smooth results
in local pose and is robust in various height diversity scenes.

5.3. Evaluation

Quantitative evaluation. We evaluate our optimization
method with ablating different loss terms by analyzing
Lcont and Lsld. We also analyze two parameters of the
the neighborhood radius r, which is used
optimization:
to crop the scene for foot contact constraint, and the sub-
sequence length l of optimization. As shown in Tab. 2, with-
out the foot contact constraint or the foot sliding constraint,
Lcont or Lsld increases dramatically and has little impact
on another term. Both Lcont and Lsld decrease without the
smoothness constraint. However, the motion jitters more
severely in 3D visualization. Overall, all loss terms are nec-
essary to produce accurate and smooth human motion. We

HSC4DBaseline1Baseline2FloatingPenetrationFigure 8. Qualitative results of human activities in diverse scenes. The SMPL models in each figure are from the same human subject.

observe that both r and l have little impact on the result. In
practice, to balance the computational resource and running
time, we set l = 100. And to ensure we can detect points in
the scenes, we set r = 0.6m.

Qualitative evaluation. For local pose qualitative evalua-
tion, we use an extra camera registered to the scene and then
project the human motions to the images Fig. 6. We show
more qualitative examples in Fig. 8. There are various hu-
man activities in different indoor and outdoor scenes shown
in the figure. The sequence motions shown in figure repre-
sent the same human occurring at a successive time. Above
examples show that our method can estimate the natural and
challenging human motions with global localization.

6. Discussions

Limitations. First, HSC4D uses LiDAR SLAM results as
the initial localization. Consequently, it is limited when Li-
DAR mapping fails, such as crowded places, narrow areas,
etc. Second, to avoid a large occlusion during LiDAR data

capturing, some activities are limited in our system, such
as sitting in a chair with a backrest, standing back against
a wall. Besides, the optimization loss terms in our pipeline
are hand-created. Some cases like rock climbing do not al-
ways work. It is promising to propose a more general loss
term or a deep learning framework cooperating with the se-
mantic information.

Conclusions. We present a Human-centered 4D Scene cap-
ture method to accurately and efficiently create a dynamic
digital world using only body-mounted IMUs and LiDAR.
Our method is space-free, pose-free, and map-free. By in-
tegrating LiDARs and IMUs, our proposed joint optimiza-
tion algorithm can obtain accurate global localization and
smooth local poses in large scenes. Additionally, we pro-
vide a new dataset containing large scenes and diverse, chal-
lenging human motions. The experimental result demon-
strates the effectiveness of HSC4D. Our work contributes
to extending the motion capture to large dynamic scenes.
We hope this work will foster the creation and interaction
of the human-dynamic digital world in the future.

References

[1] Moustafa Abbas, Moustafa Elhamshary, Hamada Rizk, Mar-
wan Torki, and Moustafa Youssef. Wideep: Wifi-based
accurate and robust indoor localization system using deep
learning. In 2019 IEEE International Conference on Perva-
sive Computing and Communications (PerCom, pages 1–10.
IEEE, 2019. 2

[2] Abdulrahman Alarifi, AbdulMalik Al-Salman, Mansour Al-
saleh, Ahmad Alnafessah, Suheer Al-Hadhrami, Mai A Al-
Ammar, and Hend S Al-Khalifa. Ultra wideband indoor po-
sitioning technologies: Analysis and recent advances. Sen-
sors, 16(5):707, 2016. 2

[3] S´ebastien Bauwens, Harm Bartholomeus, Kim Calders, and
Philippe Lejeune. Forest inventory with terrestrial lidar: A
comparison of static and hand-held mobile laser scanning.
Forests, 7(6):127, 2016. 3

[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
In Proceedings of
modal dataset for autonomous driving.
the IEEE/CVF conference on computer vision and pattern
recognition, pages 11621–11631, 2020. 2

[5] Hanieh Deilamsalehy and Timothy C Havens. Sensor fused
three-dimensional localization using imu, camera and lidar.
In 2016 IEEE SENSORS, pages 1–3. IEEE, 2016. 3

[6] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip
Davidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts
Escolano, Christoph Rhemann, David Kim, Jonathan Taylor,
et al. Fusion4d: Real-time performance capture of challeng-
ing scenes. ACM Transactions on Graphics (ToG), 35(4):1–
13, 2016. 2

[7] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. The Inter-
national Journal of Robotics Research, 32(11):1231–1237,
2013. 2

[8] Patrick Geneva, Kevin Eckenhoff, Yulin Yang, and Guoquan
Huang. Lips: Lidar-inertial 3d plane slam. In 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems
(IROS), pages 123–130. IEEE, 2018. 3

[9] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard
Pons-Moll. Human poseitioning system (hps): 3d human
pose estimation and self-localization in large scenes from
In Proceedings of the IEEE/CVF
body-mounted sensors.
Conference on Computer Vision and Pattern Recognition,
pages 4318–4329, 2021. 2, 6

[10] Thomas Helten, Andreas Baak, Gaurav Bharaj, Meinard
Muller, Hans-Peter Seidel, and Christian Theobalt. Person-
alization and evaluation of a real-time depth-based full body
tracker. In International Conf. on 3D Vision, pages 279–286,
2013. 2

[11] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J.
Black, Otmar Hilliges, and Gerard Pons-Moll. Deep iner-
tial poser: Learning to reconstruct human pose from sparse
inertial measurements in real time. ACM Transactions on
Graphics, (Proc. SIGGRAPH Asia), 37(6):185:1–185:15,
nov 2018. 2

[12] Jianhao Jiao, Haoyang Ye, Yilong Zhu, and Ming Liu. Ro-
bust odometry and mapping for multi-lidar systems with on-
IEEE Transactions on Robotics,
line extrinsic calibration.
2021. 3

[13] Tomoya Kaichi, Tsubasa Maruyama, Mitsunori Tada, and
Hideo Saito. Resolving position ambiguity of imu-based hu-
man pose with a single rgb camera. Sensors, 20(19):5453,
2020. 2

[14] Samer Karam, George Vosselman, Michael Peter, Siavash
Hosseinyalamdary, and Ville V. Lehtola. Design, calibration,
and evaluation of a backpack indoor mobile mapping system.
Remote. Sens., 11:905, 2019. 3

[15] Alex Kendall, Matthew Grimes, and Roberto Cipolla.
Posenet: A convolutional network for real-time 6-dof cam-
era relocalization. In Proceedings of the IEEE international
conference on computer vision, pages 2938–2946, 2015. 2

[16] Wonhui Kim, Manikandasriram Srinivasan Ramanagopal,
Charles Barto, Ming-Yuan Yu, Karl Rosaen, Nick Goumas,
Ram Vasudevan, and Matthew Johnson-Roberson. Pedx:
Benchmark dataset for metric 3-d pose estimation of pedes-
IEEE Robotics and
trians in complex urban intersections.
Automation Letters, 4(2):1940–1947, 2019. 2

[17] Rainer K¨ummerle, Giorgio Grisetti, Hauke Strasdat, Kurt
Konolige, and Wolfram Burgard. g 2 o: A general frame-
In 2011 IEEE International
work for graph optimization.
Conference on Robotics and Automation, pages 3607–3613.
IEEE, 2011. 5
[18] Filip Lemic,

Jasper B¨usch, Mikolaj Chwalisz, Vlado
Handziski, and Adam Wolisz. Infrastructure for benchmark-
ing rf-based indoor localization under controlled interfer-
ence. In 2014 Ubiquitous Positioning Indoor Navigation and
Location Based Service (UPINLBS), pages 26–35. IEEE,
2014. 2

[19] Qing Li, Shaoyang Chen, Cheng Wang, Xin Li, Chenglu
Wen, Ming Cheng, and Jonathan Li. Lo-net: Deep real-time
lidar odometry. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 8473–
8482, 2019. 3

[20] Jiarong Lin and Fu Zhang. Loam livox: A fast, robust,
high-precision lidar odometry and mapping package for li-
dars of small fov. In 2020 IEEE International Conference on
Robotics and Automation (ICRA), pages 3126–3131. IEEE,
2020. 3

[21] Timothy Liu, Matthew Carlberg, George Chen, Jacky Chen,
John Kua, and Avideh Zakhor. Indoor localization and vi-
sualization using a human-operated backpack system. 2010
International Conference on Indoor Positioning and Indoor
Navigation, pages 1–10, 2010. 3

[22] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. SMPL: A skinned multi-
person linear model. ACM Transactions on Graphics, 2015.
2, 3

[23] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul
Newman. 1 year, 1000 km: The oxford robotcar dataset.
The International Journal of Robotics Research, 36(1):3–15,
2017. 2

[24] Charles Malleson, Marco Volino, Andrew Gilbert, Matthew
Trumble, John Collomosse, and Adrian Hilton. Real-time

full-body motion capture from video and imus. In 2017 Fifth
International Conference on 3D Vision (3DV), 2017. 2
[25] Helen Oleynikova, Michael Burri, Simon Lynen, and Roland
Siegwart. Real-time visual-inertial localization for aerial and
ground robots. In 2015 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), pages 3079–3085.
IEEE, 2015. 2

[26] Roberto Opromolla, Giancarmine Fasano, Giancarlo Rufino,
Michele Grassi, and Al Savvaris. Lidar-inertial integra-
tion for uav localization and mapping in complex environ-
ments. In 2016 International Conference on Unmanned Air-
craft Systems (ICUAS), pages 649–656. IEEE, 2016. 3
[27] Chao-Chung Peng, Yun-Ting Wang, and Chieh-Li Chen. Li-
In 2017
dar based scan matching for indoor localization.
IEEE/SICE International Symposium on System Integration
(SII), pages 139–144. IEEE, 2017. 3

[28] Noha Radwan, Abhinav Valada, and Wolfram Burgard.
Vlocnet++: Deep multitask learning for semantic visual lo-
IEEE Robotics and Automation
calization and odometry.
Letters, 3(4):4407–4414, 2018. 2

[29] Daniel Roetenberg, Henk Luinge, and Per Slycke. Moven:
Full 6dof human motion tracking using miniature inertial
sensors. Xsen Technologies, December, 2(3):8, 2007. 2

[30] Cristina Romero-Gonz´alez,

´Alvaro Villena, Daniel
Gonz´alez-Medina, Jesus Mart´ınez-G´omez, Luis Rodr´ıguez-
Ruiz, and Ismael Garc´ıa-Varea.
Inlida: A 3d lidar dataset
for people detection and tracking in indoor environments.
In International Conference on Computer Vision Theory
and Applications, volume 7, pages 484–491. SCITEPRESS,
2017. 2

[31] Ruwen Schnabel, Roland Wahl, and Reinhard Klein. Effi-
In Computer
cient ransac for point-cloud shape detection.
graphics forum, volume 26, pages 214–226. Wiley Online
Library, 2007. 5

[32] Youngwoo Seo and Chih-Chung Chou. A tight coupling of
vision-lidar measurements for an effective odometry. In 2019
IEEE Intelligent Vehicles Symposium (IV), pages 1118–1123.
IEEE, 2019. 3

[33] Tixiao Shan and Brendan Englot. Lego-loam: Lightweight
and ground-optimized lidar odometry and mapping on vari-
In 2018 IEEE/RSJ International Conference
able terrain.
on Intelligent Robots and Systems (IROS), pages 4758–4765.
IEEE, 2018. 3

[34] Tixiao Shan, Brendan Englot, Drew Meyers, Wei Wang,
Carlo Ratti, and Daniela Rus. Lio-sam: Tightly-coupled li-
dar inertial odometry via smoothing and mapping. In 2020
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pages 5135–5142. IEEE, 2020. 2, 3
[35] Tixiao Shan, Brendan Englot, Carlo Ratti, and Daniela Rus.
Lvi-sam: Tightly-coupled lidar-visual-inertial odometry via
smoothing and mapping. arXiv preprint arXiv:2104.10831,
2021. 3

[36] Young-Sik Shin, Yeong Sang Park, and Ayoung Kim. Dvl-
slam: sparse depth enhanced direct visual-lidar slam. Au-
tonomous Robots, 44(2):115–130, 2020. 3

[37] Mikaela Angelina Uy and Gim Hee Lee. Pointnetvlad: Deep
point cloud based retrieval for large-scale place recognition.

In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4470–4479, 2018. 3

[38] Daniel Vlasic, Rolf Adelsberger, Giovanni Vannucci, John
Barnwell, Markus Gross, Wojciech Matusik, and Jovan
Popovi´c. Practical motion capture in everyday surroundings.
ACM Transactions on Graphics (TOG), 26(3):35, 2007. 2

[39] Timo von Marcard, Roberto Henschel, Michael J. Black,
Bodo Rosenhahn, and Gerard Pons-Moll. Recovering ac-
curate 3d human pose in the wild using imus and a mov-
ing camera. In Proceedings of the European Conference on
Computer Vision (ECCV), September 2018. 2

[40] Timo von Marcard, Gerard Pons-Moll, and Bodo Rosen-
hahn. Human pose estimation from video and imus. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
38:1533–1547, 2016. 2

[41] Timo von Marcard, Bodo Rosenhahn, Michael Black, and
Gerard Pons-Moll. Sparse inertial poser: Automatic 3d hu-
man pose estimation from sparse imus. Computer Graph-
ics Forum 36(2), Proceedings of the 38th Annual Conference
of the European Association for Computer Graphics (Euro-
graphics), pages 349–360, 2017. 2

[42] Bing Wang, Changhao Chen, Chris Xiaoxuan Lu, Peijun
Zhao, Niki Trigoni, and Andrew Markham. Atloc: Attention
guided camera localization. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 34, pages 10393–
10401, 2020. 2

[43] H. Wang, C. Wang, C. Chen, and L. Xie. F-loam : Fast li-
dar odometry and mapping. In 2021 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2020.
3

[44] Yun-Ting Wang, Chao-Chung Peng, Ankit A Ravankar, and
Abhijeet Ravankar. A single lidar-based feature fusion in-
door localization algorithm. Sensors, 18(4):1294, 2018. 3

[45] Chenglu Wen, Yudi Dai, Yan Xia, Yuhan Lian, Jinbin Tan,
Cheng Wang, and Jonathan Li. Toward efficient 3-d col-
ored mapping in gps-/gnss-denied environments. IEEE Geo-
science and Remote Sensing Letters, 17(1):147–151, 2019.
4

[46] Chenglu Wen, Yudi Dai, Yan Xia, Yuhan Lian, Jinbin Tan,
Cheng Wang, and Jonathan Li. Toward efficient 3-d col-
ored mapping in gps-/gnss-denied environments. IEEE Geo-
science and Remote Sensing Letters, 17(1):147–151, 2020.
3

[47] Lan Xu, Wei Cheng, Kaiwen Guo, Lei Han, Yebin Liu, and
Lu Fang. Flyfusion: Realtime dynamic scene reconstruction
using a flying depth camera. IEEE transactions on visualiza-
tion and computer graphics, 27(1):68–82, 2019. 2

[48] Lan Xu, Yebin Liu, Wei Cheng, Kaiwen Guo, Guyue Zhou,
Qionghai Dai, and Lu Fang. Flycap: Markerless motion cap-
ture using multiple autonomous flying cameras. IEEE trans-
actions on visualization and computer graphics, 24(8):2284–
2297, 2017. 2

[49] Huan Yin, Li Tang, Xiaqing Ding, Yue Wang, and Rong
Xiong. Locnet: Global localization in 3d point clouds for
mobile vehicles. In 2018 IEEE Intelligent Vehicles Sympo-
sium (IV), pages 728–733. IEEE, 2018. 3

[50] Shangshu Yu, Cheng Wang, Zenglei Yu, Xin Li, Ming
Cheng, and Yu Zang. Deep regression for lidar-based local-
ization in dense urban areas. ISPRS Journal of Photogram-
metry and Remote Sensing, 172:240–252, 2021. 3

[51] Ji Zhang and Sanjiv Singh. Loam: Lidar odometry and map-
In Robotics: Science and Systems, vol-

ping in real-time.
ume 2, 2014. 2, 3, 4

[52] Ji Zhang and Sanjiv Singh. Visual-lidar odometry and map-
In 2015 IEEE Inter-
ping: Low-drift, robust, and fast.
national Conference on Robotics and Automation (ICRA),
pages 2174–2181. IEEE, 2015. 3

[53] Wuming Zhang, Jianbo Qi, Peng Wan, Hongtao Wang,
Donghui Xie, Xiaoyan Wang, and Guangjian Yan. An easy-
to-use airborne lidar data filtering method based on cloth
simulation. Remote Sensing, 8(6):501, 2016. 5

[54] Zerong Zheng, Tao Yu, Hao Li, Kaiwen Guo, Quionghai Dai,
Lu Fang, and Yebin Liu. Hybridfusion: Real-time perfor-
mance capture using a single depth sensor and sparse imus.
In European Conference on Computer Vision (ECCV), 2018.
2

[55] Xingxing Zuo, Patrick Geneva, Woosik Lee, Yong Liu, and
Guoquan Huang. Lic-fusion: Lidar-inertial-camera odome-
try. In 2019 IEEE/RSJ International Conference on Intelli-
gent Robots and Systems (IROS), pages 5848–5854. IEEE,
2019. 3

