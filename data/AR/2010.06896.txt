Efﬁcient Estimation of Material Property Curves and Surfaces via Active Learning

Yuan Tian,1 Dezhen Xue,1, ∗ Ruihao Yuan,1 Yumei Zhou,1, † Xiangdong Ding,1 Jun Sun,1 and Turab Lookman2, ‡
1State Key Laboratory for Mechanical Behavior of Materials, Xi’an Jiaotong University, Xi’an 710049, China
2Los Alamos National Laboratory, Los Alamos, NM 87545,USA
(Dated: October 15, 2020)

The relationship between material properties and independent variables such as temperature, external ﬁeld
or time, is usually represented by a curve or surface in a multi-dimensional space. Determining such a curve
or surface requires a series of experiments or calculations which are often time and cost consuming. A general
strategy uses an appropriate utility function to sample the space to recommend the next optimal experiment
or calculation within an active learning loop. However, knowing what the optimal sampling strategy to use to
minimize the number of experiments is an outstanding problem. We compare a number of strategies based on
directed exploration on several materials problems of varying complexity using a Kriging based model. These
include one dimensional curves such as the fatigue life curve for 304L stainless steel and the Liquidus line of the
Fe-C phase diagram, surfaces such as the Hartmann 3 function in 3D space and the ﬁtted intermolecular potential
for Ar-SH, and a four dimensional data set of experimental measurements for BaTiO3 based ceramics. We also
consider the effects of experimental noise on the Hartmann 3 function. We ﬁnd that directed exploration guided
by maximum variance provides better performance overall, converging faster across several data sets. However,
for certain problems, the trade-off methods incorporating exploitation can perform at least as well, if not better
than maximum variance. Thus, we discuss how the choice of the utility function depends on the distribution of
the data, the model performance and uncertainties, additive noise as well as the budget.

I.

INTRODUCTION

The accurate prediction of the properties of materials as
a function of independent variables is crucially important in
exploiting their use in different applications. Such a func-
tional relationship is usually described as a curve or surface
between a property and the independent variable/s in a multidi-
mensional diagram. [1] These properties can be mechanical,
thermal, electrical, magnetic, optical, and chemical ones, and
the independent variables usually include chemical compo-
sition, temperature, time and heat treatment conditions. [2]
The materials property curves and surfaces can determine the
variation behavior, critical states and property optima, and con-
sequently play a crucial role in the design of new materials,
the assessment of hazards and the optimization of processing
parameters. Familiar examples include phase boundaries and
surfaces in temperature versus composition space, fatigue life
cycle curves describing the relationship between mechanical
properties and loading cycles, and intermolecular potential
energy surfaces for molecules.

Determining a property curve or surface is often time and
cost consuming as a number of measurements or calculations
are required depending on the accuracy needed. An adequate
number of data needs to be accumulated as the independent
variable is varied in given steps. The data requirements are
sensitive to nonlinearities and sharp changes in the functional
form as well as the presence and number of multiple extrema,
including critical points. For example, establishing a phase di-
agram requires a series of experiments to determine the critical
temperature for different compositions or pressures. Similarly,
a number of parallel samples, each of which is used to obtain

∗ xuedezhen@xjtu.edu.cn
† zhouyumei@xjtu.edu.cn
‡ turablookman@gmail.com

the ultimate stress for a given number of loading cycles, are
required to obtain the fatigue curve of an alloy, This therefore
poses the challenge of accurately estimating the property curve
and surface with as few measurements or calculations as pos-
sible. Although regression algorithms have been employed to
model the functional form between the property and the inde-
pendent variables[3–6], the regressed results inevitably contain
large uncertainties if the number of initial data points is rela-
tively small, especially if the relationship between the property
and independent variable is complex. As the number of initial
data points increases, more experiments or calculations are
needed with concomitant increases in time and resources used.
Hence, there is a need for an approach that can predict general
property curves/surfaces and successively reﬁne them rapidly
using as few new measurements or calculations as possible.

Active learning or optimal experimental design allows an
algorithm to choose the data from which it learns so that it may
learn more efﬁciently with less training data than otherwise.[7–
12] This becomes particularly important in areas such as mate-
rials science where the size of a good quality labelled data set
for supervised learning is often limited because of the expense
associated with generating it. [7, 13–15] It then becomes de-
sirable to recommend one or more unlabelled instances from
a large pool of possibilities to be labelled by experiments or
calculations using active learning methods.[16–18] The goals
are to achieve higher accuracy of prediction or exploit the op-
timum, with minimization of the overall cost for obtaining
labeled data. [19–24] It is being increasingly applied to mate-
rials science data to efﬁciently guide and minimize the number
of experiments needed. [25–31]

By invoking concepts from decision theory, various utility
functions, which are akin to query strategies in active learn-
ing, can be deﬁned to decide which instances of the unlabeled
data would be most informative to be labeled. Sampling the
most important states is therefore a problem of considerable
importance to avoid excessive numbers of iterations or exper-

0
2
0
2

t
c
O
4
1

]
i
c
s
-
l
r
t

m

.
t
a
m
-
d
n
o
c
[

1
v
6
9
8
6
0
.

0
1
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

iments, especially when one may not know which states are
most important. This requires exploring the allowed space efﬁ-
ciently and well enough and the problem has been studied in
the context of reinforcement learning. Exploration techniques
are essentially of two kinds, undirected exploration and di-
rected exploration [32]. Undirected exploration is uninformed
and characterized by selecting actions randomly from a given
distribution. If the distribution is uniform, we have random
exploration in which costs or rewards are not taken into ac-
count. Whitehead [33] has proved that under certain conditions
random walk exploration leads to the result that the learning
time scales exponentially with the size of the state space. In
spite of this, undirected exploration is studied widely in the
literature via numerical experiments as often the conditions
leading to the analytical result are not fully satisﬁed.

Directed exploration, on the other hand, uses knowledge
to guide the exploration search so that the exploration rule
directly determines which action to take next. The goal is
to select actions which maximize the improvement over time,
which is impossible to determine as we do not know in advance
how a given decision will improve the performance. Thus, all
directed exploration techniques are heuristics designed to op-
timize knowledge gain. The exploration may be achieved by
choosing states based on frequency of occurrence (Counter
based exploration), or assumed to have a high prediction er-
ror (maximum variance), or those that include different de-
grees of exploitation functions based on using the best value
of the model predictions at the time. Examples of the latter
include trade-off methods such as the efﬁcient global optimiza-
tion (EGO) and knowledge gradient (KG) schemes based on
Bayesian optimization for ﬁnding maxima/minima of func-
tions. [34, 35] All have the aim to optimize both learning time
and learning costs. One of the few analytical results is that
for Counter based exploration. Thurn [32] has shown that the
worst-case complexity of learning, under given conditions, is
always polynomial in the size of the state space.

How much exploration needs to be performed depends on
the costs of collecting new information and the value associated
with that information. In the absence of analytical results for
realistic problems and strategies, what we are left with is a
study of different heuristics with different degrees of interplay
between exploration and exploitation. In practice, convergent
proofs are of little help and point to the need for studies that
compare different strategies on different sets of data of varying
sizes and distributions to evaluate their relative performance
in a ﬁnite number of iterations or experiments as a function of
the dimensionality of the problem, as well as the inﬂuence of
measurement noise. It has been shown in a number of studies
that trade-off strategies, such as EGO and KG perform well in
maximizing/minimizing material properties, even for complex
systems where the property behavior may include multiple
local maxima/minima.[16, 36–39] However, the performance
of these utility functions in the rapid and accurate estimation of
material property curves/surfaces has not been studied. Here
we propose an active learning loop to compare the efﬁciencies
of six utility functions to estimate material property curves in
terms of the number of new experiments required for each.

Since selection via maximum variance (Max-v.) is one of

our utilities, we introduce the utility, B.EGO, designed to mini-
mize/maximize the variability in the function over many boot-
strap samples. This is in contrast to ﬁnding the maximum /
minimum of the function, which is what EGO and KG have so
far been applied to. The uncertainties are given by the Krig-
ing model and used in evaluating Max-v, B.EGO, EGO, KG,
random exploration using a uniform distribution, and SKO,
Sequential Kriging Optimization, in recommending the next
candidate. The last utility considers the effects of experimental
noise on the data. We apply our approach to several problems
with increasing complexity to determine which utilities are
robust across all of these. As a common problem in materials
science is to predict property curves from limited data, we ex-
amine two applications, the fatigue life curve for 304L stainless
steel (SS304L) and estimating the Liquidus line of the Fe-C
phase diagram. We show that two or three new experiments or
calculations are all that is needed to complete the curve opti-
mization. Since these curves are 1D, we also consider surfaces
in the form of the 3D Hartmann 3 function used frequently in
optimization tests, to which we also add experimental noise to
study utility performance for noisy data, and the ﬁtted surface
for the intermolecular potential of Ar-SH. Finally, we apply
our tests to a data set of experimental measurements for the
Curie temperature of BaTiO3 ferroelectrics, which is modeled
in 4D.

Our principal conclusion is that for a range of materials data
and problems with varying complexities, directed exploration
via maximum variance generally performs better than other
utilities. The variability utility B.EGO based on bootstrap
samples is also a good performer, following Max-v. However,
for given problems, the trade-off methods that add various
degrees of exploitation can perform at least as well, if not
better, than maximum variance Max-v. Thus, the choice of the
utility function is sensitive to the distribution of the training
and subsequently acquired data, the model performance, the
noise as well as the budget, which determines the number of
iterations allowed.

II. ACTIVE LEARNING STRATEGY

Figure 1 illustrates our active learning loop for material
property curve estimation. We begin with a machine learning
model that uses regression to estimate a property curve from the
relatively small number of labeled data points available. The
uncertainties associated with this estimation will be large due
to limited data. We employ a Kriging model as the regressor
to obtain the predicted value, µ, for each point in the curve
as well as the variance of the prediction, s, at that point. The
utility functions are deﬁned in terms of µ and s and recommend
the next unlabeled point for the curve for which the label is
evaluated by the user via experiments or calculations. The
variance serves as input to Max-v as well as B.EGO. The latter
is deﬁned in terms of the bootstrap mean error ¯s and its standard
deviation se(s). The new point selected then augments the
training data so that the regressor can reﬁne and provide an
updated value for µ and s. The loop then continues so that the
curve can be improved step by step until an adequate estimate

3

FIG. 1. Flow chart of active learning strategy for efﬁcient estimation of material property curves. (1) A small, labeled training data serves as the
starting point for the targeted curve. (2) A surrogate model using machine learning methods provides an estimate of the ﬁt with uncertainties. (3)
Several criteria are used to check if the estimated ﬁt is adequate and fulﬁlls conditions for success. If not, (4) utility functions are evaluated and
ranked to recommend the next data point to use from the pool of possible points for measurement or calculation to determine the label. (5) The
recommended experiment/calculation is performed and the new labeled point augments the training data to obtain a revised estimate of the
curve. The loop continues until the criteria for success are met.

of the curve is obtained. The chief ingredients of our active
learning strategy includes the Kriging model, the evaluation
criterion of the estimated curve and different utility functions.

Assuming p training data, x∗, with unknown data points, x,

the universal Kriging (UK) equations are given by [43, 44]:

µ = m(x) + K(x, x∗)(K + ∆)−1( ˜y − m(x∗)),

(2)

A. Model: Gaussian Process via Kriging

Machine learning algorithms are efﬁcient at regressing data
points to estimate a relationship between a property y and in-
dependent variables x, i.e., y = f (x). They thus serve to build
a surrogate model for the estimation and in this work we will
use a Kriging model to perform the regression from the small
number of training data points available. Kriging is a spatial
interpolation method that provides a robust estimate of targets
and uncertainties associated with unknown points.[40–42] The
interpolated values are modeled by a Gaussian process gov-
erned by prior covariances. It is customary to consider noisy
observations of the targeted property y, where ˜y j = y(x j) + ε j
and ε j is a realization of a random variable so that ε j follows
an independent, identically distributed Gaussian distribution
N (0, τ 2

j )(1 ≤ j ≤ n) with homogeneous noise variance τ 2
j .
The property y is considered a realization of a Gaussian

process Y following Kriging. That is,

SK + (f(x)(cid:124) − K(x, x∗)(cid:124)(K + ∆)−1f(x∗))(cid:124)(f(x∗)(cid:124)
s2 = s2
(K + ∆)−1f(x∗))−1(f(x)(cid:124) − K(x, x∗)(cid:124)(K + ∆)−1f(x∗)),

(3)

where ˜y=( ˜y1, ..., ˜yp)T , K is covariance between training data
points, ∆ is a diagonal matrix with diagonal terms τ 2
1 , ..., τ 2
p.
The simple Kriging (SK) variance, s2

SK, is given by

SK = K(x, x) − K(x, x∗)(K + ∆)−1K(x∗, x),
s2

(4)

2 ( h

We use the covariance kernel g(h) = exp(− 1

θ )2), where h
and θ are hyper-parameters of the model and set characteristic
length-scales associated with the data. Note that the variance
value s2 at x depends on the distance from known point x∗.
If x is close to known point x∗, it is inﬂuenced by x∗ and the
variance at x will be small. If x is separated from known points,
the variance at x will be large.

Y = m + Z = ∑ β f + Z,

(1)

B. Evaluating goodness of ﬁt for the targeted curve

where m is a trend function, β is the coefﬁcient and the process
Z is assumed Gaussian.

We determine the quality of the model by tracking the devi-

ation of the regressed curve from the true curve.

In our testing case, as the true curve is known, we can use
the mean absolute error (MAE) and maximum absolute error
(Max.AE) deﬁned by

MAE =

1
n

n
∑
j=1

(|y j − µ j|),

Max.AE = max(|y j − µ j|),

(5)

(6)

where n is the total number of possible points in the function,
y j are the true values and µ j are the estimated values from
Kriging model. The error MAE is the average deviation of
the estimate value from the true value whereas Max.AE is the
largest error over the range of data points.

As the true curve is usually not known, we utilize the uncer-
tainty associated with the regressor prediction as an estimate of
the model quality. We thus use instead the mean standard devi-
ation (MSD) and the maximum standard deviation (Max.SD)
deﬁned as follows

MSD =

1
n

n
∑
j=1

(s j)

Max.SD = max(s j),

(7)

(8)

where s j is the standard deviation associated with each predic-
tion (µ j) in the curve. We will monitor the evolution of MAE,
Max.AE, MSD and Max.SD as we iterate the active learning
loop until the accuracy threshold is reached.

4

EGO. Efﬁcient Global Optimization (EGO) balances ex-
ploration and exploitation by evaluating the “Expected Im-
provement" (EI) and choosing the candidate with the largest
(EI). If ˜y∗
min is the minimum value in the training data, the
improvement at a point x j is I = max( ˜y∗
min − Yj, 0), where Yj
is distributed normally, N (µ j, s2
j ). As the tail of the density
function at point x j extends into ˜y∗
min, improvement is then
possible. Different amounts of improvement or distances from
˜y∗
min are associated with different density values. The EI is
obtained by weighting all these improvement values by the as-
sociated density values. The EI of each potential measurement
is the expectation of I at that point given by[46]

νEGO = E [max( ˜y∗

min −Yj, 0)] = sG (
˜y∗
min − µ
s

) + sφ (

)

˜y∗
min − µ
s
˜y∗
min − µ
s

(11)

)

= ( ˜y∗

min − µ)Φ(

min−µ
s

where G (z0) = z0Φ(z0) + φ (z0), z0 = ˜y∗
, s is the standard
deviation associated with the mean value µ of the model pre-
diction, Φ(·) and φ (·) are the standard normal density and
distribution functions, respectively. If the measurements are
noise-free, νEGO is zero at the sampled points (points that are
already measured) and is positive in elsewhere. In our dis-
cretized version of the problem here, EGO simply evaluates
EI at each unexplored point and recommends a point with the
largest EI to be measured next.

SKO. In the noise case, the current best estimate ˜y∗

min also
suffers from noise, and the actual minimum is indeed unknown.
We therefore utilize an extension of EGO, Sequential Kriging
Optimization (SKO) [47], in which ˜y∗
min in EGO is modiﬁed
through the model predictions µ ∗∗. A prefactor 1 − ε√
is

ε 2+s2

C. Utility functions

introduced to enhance the exploration.

Small data sets, characteristics of many materials science
problems, typically give rise to large uncertainties in prediction
and therefore additional statistical design criteria need to be
invoked. A utility function allows us to choose between experi-
ments or calculations by maximizing an expected utility, where
the utilities are deﬁned with respect to information theoretic
considerations. By ranking the expected value of the informa-
tion for possible alternatives for observation or calculation, the
utility function provides the means to prioritize the decision
making process based on the information gained or reduced by
observing a potential new data point. [13, 45] The utilities we
compare in this work are deﬁned below, in the noise case we
employ SKO instead of EGO.

Min-u. Min-u is a greedy choice in which the candidate
with the lowest predicted mean value from the model is chosen.
That is,

νMin−u = µ

(9)

Max-v. The variability in the predictions can be character-
ized by the variance at that point obtained from the Kriging
covariance Equation 3. Thus, Max-v is a risk-averse utility
function selecting the next candidate point based on the mag-
nitude of the variance in the property at a given point. That
is,

νmax−v = s

(10)

νSKO = (1 −

√

ε
ε 2 + s2

)sG (

µ ∗∗ − µ
s

)

(12)

where µ ∗∗ = µ(argmin [µ + λ s]), λ is the “risk avoidance”
parameter.

KG. Knowledge Gradient(KG) aims at maximizing the cur-

rent reward[48, 49]. KG can be calculated using

νKG = SG (−|

µ (cid:48) − µ
S

|)

(13)

where µ (cid:48) is the minimum of the predicted values in the virtual
unexplored space (µ j) without the next recommended sample
(µi), i.e., µ (cid:48) = minµ j, for j (cid:54)= i; If the measurements are noise-
free, S is the standard deviation s provided by the Kriging
model directly. KG is also available for the noise case and
the modiﬁed standard deviation S is given by S = s2/
τ 2 + s2,
where s2 is the variance of the prediction and τ 2 is the variance
associated with noise. [45, 48, 49]

√

B.EGO. In order to ﬁnd the point with the maximum vari-
ability in the property using EGO or KG, we introduce a new
utility that complements Max-v. Whereas the variability in
Max-v is directly obtained from the Kriging model for a given
data set, the variability in B.EGO is obtained by considering
many bootstrap samples. Let s∗
max be the largest so-far value

of the standard error of the bootstrap uncertainties in the train-
ing data, so that I = max(ERROR j − s∗
max, 0), where ERROR j
is distributed normally, N ( ¯s j, se(s)2
j ). The mean ¯s is given
by ∑B
, where B are the bootstrap replicates or samples,
and se(s) is the standard error of the bootstrap uncertainties
corresponding to ¯s. We use a value of B of 50. Then the ex-
pected improvement, EI of each potential measurement is the
expectation of I at that point given by[46]

b=1 sb
B

νB.EGO = E [max(ERROR j − s∗

max, 0)]

= se(s)G (

¯s − s∗
se(s)

max

)

(14)

= ( ¯s − s∗

max)Φ(

¯s − s∗
se(s)

max

) + se(s)φ (

¯s − s∗
s

max

)

−∞ Φ(z)dz = z0Φ(z0) + φ (z0), z0 = ¯s−s∗

where G (z0) = (cid:82) z0
se(s) .
B.EGO is designed to aim at searching for the point with the
maximum Expected Improvement, however, the improvement
is not for the current minimum function value but for the cur-
rent maximum standard deviation of all labeled observations.

max

Random. This involves a random choice of the unmeasured
candidate, such that if there are a total of N choices, xi is chosen
with probability 1/N.

III. RESULTS

j and x∗

We present results for three classes of problems with vary-
ing complexity. We consider 1D and multi-dimensional cases
with a ﬁnite number of measurements of relevance in materials
science. These include Case I: a) the fatigue life cycle curve
for 304L stainless steel (SS304L) and b) the Liquidus line in
the Fe-C phase diagram, Case II: a) a standard optimization
test function, the Hartmann 3 function in 3D, and b) the ﬁtted
intermolecular potential surface for Ar-SH, and Case III: Mea-
surements of the Curie temperature for ferroelectric samples
with 4 variables or features. In Case II a) we also vary the data
set size in the presence of “experimental" noise. In each case
a small subset ˜y∗
j of the data set is randomly chosen
as the initial training data and the remaining data ˜y j and x j
comprise the unexplored search space. We implemented the
feedback loop of Figure 1, monitoring the departures from the
true result using uncertainties given by MAE, Max.AE, MSD
and Max.SD deﬁned previously. We employed Kriging to per-
form regression and once the new measurement xi is selected
by the utility function, the new observed value ˜yi augments
the training data and the loop repeats itself, reﬁning successive
estimates. We monitor the number of iterations, (N), of the
loop, i.e., number of new measurements made with a given
utility that minimize (N). To garner adequate statistics, the
design process was repeated 100 times with different initial
training data randomly selected from all the discretized points
in the data set.

5

A. Case study I: 1D materials cases

a) Fatigue Life Curve for 304L stainless steel.

Fatigue properties of materials are often described using
the fatigue curve, which describes the relationship between
cyclic stress and number of cycles to failure.
It is critical
in assessing material failure and obtaining it experimentally
requires a series of tests to ﬁnd the ultimate stress for a given
number of loading cycles, quite time and cost consuming.

We choose a monotonic fatigue life curve for 304L stainless
steel from the simulation work of F.Mozafari et al.[50] as a
typical example to validate our design loop in Figure 1. The
dotted red line in Figure 2 shows this curve. We consider the
number of cycles to failure, N f , as the independent variable and
the stress amplitude σa as the output or property and discretize
the curve into 201 data points. We randomly choose 5 data
points as the initial training data with known x∗
j to
employ in the loop of Figure 1 to optimize the curve. The next
measurement is recommended from the remaining 196 data
points using different utility functions.

j and ˜y∗

Typical examples of the optimization process from one ini-
tial dataset are shown in Figure 2. The panels (# = 0) of
Figure 2 show the Kriging interpolation estimate of the curve
from the 5 initial data points for the different utility functions.
The predicted values µ j of all the unexplored/explored x j are
shown by the solid blue line with the shadows about the solid
blue line tracking the uncertainties s j associated with µ j. The
green curves in Figure 2 (for example a2 corresponding to the
curve a1) show the behavior of the utility function (ν) for each
point on the curve (x j). The xi with the largest value of the
utility function νi (indicated by the red arrow) is recommended
for the next measurement. The Kriging model is then reﬁned
based on 6 data points, and the updated curve is shown in
panel (# = 1) of Figure 2 . The process is repeated 5 times
for the different utility functions and the changes due to new
selections of points are quite apparent from panel to panel in
Figure 2. For each utility function in Figure 2, the optimiza-
tion begins with the same 5 points but is followed by different
sampling trajectories. Figure 2 shows a typical example of the
optimization process for Max-v, Random compared to EGO
and B.EGO. The function Max-v converges to the true objective
function in only three new measurements, outperforming the
other functions which need more measurements.

For a more robust comparison, we used an initial random
training data set of n = 5 training points and tracked the values
of MAE, Max.AE, MSD and Max.SD as a function of number of
new measurements for the different policies. By repeating over
a 100 trials, Figure 3 (a) - (d) shows the average values and
95% conﬁdence intervals for MAE, Max.AE, MSD and Max.SD
for the different utility functions. Both Max-v and our new
function B.EGO perform very well, converging in relatively
few iterations followed by Random which also converges but
with more iterations. The trade-off methods EGO and KG
decrease the error quickly in the ﬁrst three iterations, but then
relax slowly but nevertheless also converge. The greedy, pure
exploitation Min-u shows very little relaxation after a few
iterations.
b) Liquidus line in the Iron-carbon phase diagram.

6

FIG. 2. Optimization process for the Fatigue Life Curve for SS304L steel comparing the behavior of different utility functions. The solid blue
line is the Kriging mean estimate and to visualize the uncertainties, the shaded region denotes 10 times the standard deviation at each input
value. The red arrow denotes the next inﬁll point (maximum) in the utility functions. The optimizations are initialized with the same 5 points,
but quickly follow different sampling trajectories.

The Iron-carbon (Fe-C) phase diagram displays the phases,
compositions and transformations in iron-carbon alloys as a
result of heating and cooling, and therefore serves as the basis
for composition design and optimizing heat treatment of steels.
The liquidus line is the phase boundary in the phase diagram
limiting the bottom of the liquid ﬁeld, and the dotted red line
in Figure 4 shows the Liquidus line exhibiting a eutectic point
at C composition of 4.3% between γ and Fe3C. The curve is
irregular and the challenge is to obtain it with as few measure-
ments as possible. We discretize the Liquidus line into 118
data points, i.e., 118 composition-temperature data points and
randomly choose 5 initial points. The true Liquidus line of
the Fe-C phase diagram is shown by the dotted red line. The
estimated curves initially deviate signiﬁcantly from the true
curve, which gives rise to large values of MAE, Max.AE, MSD
and Max.SD. In Figure 4, the function Max-v only requires two
new measurements to match the true function and outperforms
all the the other functions which need more measurements.
The function B.EGO also does well in the optimization as it
works directly on the prediction errors. Both EGO and Random

predict a curve close to the true function in the second iteration,
but then get worse as the iteration number increases.

By repeating over a 100 trials, Figure 5 (a - d) shows the
average values and 95% conﬁdence intervals for MAE, Max.AE,
MSD and Max.SD for the different utilities. Figure 5 essentially
bears out our previous ﬁndings for the fatigue curve seen in
Figure 3, and the general features are very similar to those
discussed previously for the fatigue curve. The uncertainty
based Max-v and B. EGO perform well and converge readily
compared to Random and the trade-off methods, all of which
do converge although require more iterations. Max-v relaxes
more quickly than B. EGO if compared to fatigue, however,
other than pure exploitation Min-u, all the exploratory utilities
(including random) converge in the 1D materials data sets.

B. Case study II: Higher dimensional surfaces

7

To compare the efﬁciency of the utility function introduced
above in the optimization process, we used an initial randomly
training data set of n = 80 training points and tracked the values
of MAE, Max.AE, MSD and Max.SD as a function of number
of new measurements for the different policies. Figure 7 (a) -
(d) shows the average values and standard deviation for MAE,
Max.AE, MSD and Max.SD for 100 trials. Max-v and the trade-
off policies perform better than the rest, including B.EGO and
Random. This example also suggests that the actual variance
lends itself better to exploration of the space than the variability
across bootstrap samples. The performance of EGO, KG is
slightly better than Max-v for 30 or less iterations, which can be
explained through the optimization sequence shown in Figure 6
ii). We observe that the training data are further away from
the global minimum in Figure 6 ii), suggesting that the points
near the minimum will not be well predicted. EGO samples
some points near the global minimum in the ﬁrst few iterations
(shown by dark purple stars), whereas Max-v, B.EGO and
Random do not sample points in this area. Thus, it is not
surprising that EGO does better initially.

To study the number of iterations required for different poli-
cies or utility functions to match a targeted objective function
or curve, we set a threshold on MAE to stop our iteration loop.
The threshold is set to (ymax - ymin) × 3% and 1% respec-
tively, to show how these utility functions perform to meet
different demands of accuracy (shown in Figure 8(a,c) and
Figure 8(b,d)). The initial training data with sizes from 3%
to 15% times the number of total data for the ﬁrst threshold,
and 5% to 20% times the number of total data for the second
threshold was selected randomly. A total of 200 iterations is
set to stop the optimization loop. If after 200 iterations the loop
does not reach the threshold, the number of new measurements
needed is counted as 200. Figure 8(a,b) shows the number of
iterations required to meet the threshold as a function of initial
training data size. Each point with the error bar represents
the average value associated with 95% conﬁdence level over
100 random trials. As the size of training data increases, all
the utility functions perform better. Pure exploitation Min-u
performs much worse than the others, almost 2 or 3 times
slower, followed by B.EGO and Random. However, we notice
the differences between the trade-off methods and Max-v when
the MAE threshold differs. If the MAE threshold equals 3% of
y range, the trade-off methods EGO and KG are a little better
than Max-v. If the MAE threshold equals 1% of y range, a
higher requirement on the accuracy, then Max-v is the best
choice. Figure 8(c,d) show the probability density difference
(∆) in iterations needed to reach the threshold using EGO and
Max-v in 100 trials. The peak moves from negative to zero
and becomes narrower as the size of training data increases in
Figure 8(c), indicating that the advantages of EGO decrease
with the increasing size until they ﬁnally perform similarly.
For MAE threshold equals to 1% of y range, the opposite of
(Figure 8(d)) applies.

Effects of noise. Using the function above, we introduce
random errors to generate noisy data to simulate noisy measure-
ments in experiments. We assume noise ε j follows a normal
distribution N (0, τ 2), where τ is set to 5%, 10% and 15%
of y range, respectively. The observation values then can be

FIG. 3.
Comparison of the performance of utility functions in
optimizing the Fatigue Life Curve for SS304L steel. The initial data
size contains n = 5 training points and the mean values and error
bars showing the 95% conﬁdence levels of the points are evaluated
over 100 trials. Shown is the behavior of (a) mean standard deviation,
(b) maximum standard deviation, (c) mean absolute error, and (d)
maximum absolute error.

a) Hartmann 3 function
We utilize a well-known optimization test function, the Hart-
mann 3 function, to generate data for a 3-Dimensional mathe-
matical case with multiple local minima and 1 global minimum.
The function is deﬁned by:

y = −

4
3
∑
∑
n=1
m=1
where α = (1.0, 1.2, 3.0, 3.2)(cid:62)

αnexp(−

Anm(xm − Pnm)2),

A =






3.0 10 30
0.1 10 35
3.0 10 30
0.1 10 35






P = 10−4






3689 1170 2673
4699 4387 7470
1091 8732 5547
381 5743 8828






(15)

The whole space is discretized into 400 points (xm j, m =
1, 2, 3; j = 1, 2...400) using Latin Hypercube Sampling. Their
corresponding y j is the value evaluated by the function. The
surface as well as its projected contours on the two of these
variables planes is shown in Figure 6 i) for the 400 points. We
randomly select 80 data points (20% of the total search space)
as the initial training data, with known x∗
j , shown by
the gray points in Figure 6 ii). We ran 10 steps of different
utility functions and show the optimization process in Figure 6
ii). The stars with the numbers refer to the sequence obtained.
From the distribution of the new 10 points, we can see that
the points chosen by Max-v and B.EGO are widely distributed
on the whole surface and initially even points on the edge
of the contours are sampled. With EGO very few points are
distributed away from the local or the global minimum.

m j and y∗

01020304050048121601020304050010203040500102030405001020304001020304050050100150200(c)(d)(b)Mean Standard Deviations# Iterations B.EGO Max-v EGO KG Min-u Random(a)Maximum Standard Deviations# Iterations B.EGO Max-v EGO KG Min-u RandomMean Absolute Error# Iterations B.EGO Max-v EGO KG Min-u Random B.EGO Max-v EGO KG Min-u RandomMaximum Absolute Error# Iterations8

FIG. 4. Optimization process for the Liquidus line (red dotted) of the Fe-C phase diagram comparing the effects of different utility functions.
The solid blue line is the Kriging mean estimate and to visualize the uncertainties, the shaded region denotes 10 times the standard deviation at
each input value. The red arrow denotes the next inﬁll point (maximum) in the utility functions. The optimizations are initialized with the same
5 points, but quickly follow different sampling trajectories.

calculated via ˜y j = y(x j) + ε j. A second measurement of the
same candidate j, which has already been measured, is allowed.
The results for the different utility functions after 100 trials
are presented in Figure 9. Each row corresponds to one level
of noise, the noise level increases from top to bottom. Com-
pared to Figure 7, if the # Iterations equals to zero, MAE and
Max.AE increase. The increase in these values in the beginning
indicates that noise makes the prediction of the model deviate
much more from the real curve. That is, the prediction suffers
from both model uncertainties and measurement noise. SKO,
the modiﬁed version of EGO with noise, performs very well,
especially at noise levels of 5% and 10%. Max-v does sur-
prisingly well and does converge to SKO with more iterations.

b) Intermolecular potential energy surface for Ar-SH
The 3D intermolecular potential energy surface for Ar-SH
has been determined by a combination of spectroscopic mea-
surements and solutions to the Schrödinger equation [51]. The
ﬁtted surface and potential well reproduces all the known exper-

imental data and we utilize this example to test the utility func-
tions. Our database includes in total 1050 points with the cal-
culated potential energy in cm−1 and 3 variables, namely, the
distance between Ar and the center of mass of SH in Angstrom,
the angle theta, and the SH bond length in Angstroms. The
surface as well as its projected contours on the two of these
variables planes are shown in Figure 10 i) for the 1050 points.
We randomly selected 52 data points (5% of the total search
space) as the initial training data, shown by the gray solid ﬁlled
points in Figure 10 ii). We ran 10 iterations for each utility
function and the sequence of optimizations is shown in Fig-
ure 10 ii). The stars with the numbers refer to the sequence
obtained. The distribution of the newly acquired 10 starred
points by the utility functions is very similar to that for the
Hartmann 3 function. That is, those chosen by Max-v and
B.EGO are widely separated, whereas for EGO few points are
dispersed away from the local or global maxima.

To compare the efﬁciency of the utility function introduced
in the optimization process, we used an initial random training

9

30% of the data.

Figure 12 shows the results for 100 trials. There is a sig-
niﬁcant drop initially in the maximum error for Max-v and B.
EGO, suggesting that the bulk of the uncertainty is reduced
within a few iterations. Again, Max-v relaxes the most but the
others are not far from converging to it. Unlike the other cases,
Max.AE does not relax to zero and is indicative of the complex-
ity of the problem. We have essentially used only four features
to model this data from a sampling of 182 measurement points.
There are uncertainties in the model itself, hence the Kriging
needs to be accurate, and we are assuming that the sampling
of points is representative of the data in the whole space.

IV. DISCUSSION

Our objective has been to compare the inﬂuence of utility
functions for curves, such as phase boundaries, fatigue lines,
and other multi-dimensional cases important in materials sci-
ence. This is essential as in the absence of analytical results,
it is difﬁcult to predict a priori which utilities will be superior
in reducing the the costs of acquiring new information when
learning from data.

Except for the random case, all the utilities we compare are
based on directed exploration, which can incorporate different
degrees of exploitation. Our key ﬁnding is that maximum vari-
ance (Max-v) performs very well across a range of data sets
with varying complexities, including the addition of experimen-
tal noise. The function, B.EGO, which tracks the variability
over bootstrap samples, and uses EGO to minimize the vari-
ability across the whole function, also shows relatively good
performance, although it is not as robust as Max-v. Moreover,
we also ﬁnd that for each type of data set, there exists a utility
which performs as well, if not better than, or at least competes
with Max-v. The distributions of the property values in the
dataset y can also inﬂuence the behavior. If the distributions
depart from the uniform distribution, then typically there are
relatively few training data points located near global min-
ima/maxima, and these can be associated with large deviations
from the true result. We ﬁnd this to be the case for the Hart-
mann 3 function and intermolecular potential data sets, where
the trade-off methods EGO and KG perform as well, if not
better than Max-v. Also, for a given problem, several utilities
can converge but at varying iteration numbers. For the inter-
molecular potential, the convergence is far superior to Max-v
even after 50 iterations. In cases where Random selection does
converge, it requires a lot more iterations as the exploration is
unguided. In the presence of experimental noise, SKO, which
is essentially EGO with noise incorporated, is the superior
performer at noise levels of 5% and 10%, although Max-v also
does well, converging with more iterations.

Our results emphasize the importance of making the appro-
priate choice in ranking and selecting the next candidate for
measurements or calculations.

To gain an understanding of the behavior of these functions,
we plot the probability density functions (PDF) for the uncer-
tainties from the Kriging model estimates and for the deviation
of the estimate from the true curve, as a function of iterations.
Figure 13 and Figure 14 show the results for the Liquidus line
of Fe-C phase diagram and for the fatigue life curve for 304L

in

FIG. 5.
Comparison of the performance of utility functions in
optimizing the Liquidus line of the Fe-C phase diagram. The initial
data size contains n = 5 training points and the mean values and error
bars showing the 95% conﬁdence intervals of the points are evaluated
over 100 trials. Shown is the behavior of (a) mean standard deviation,
(b) maximum standard deviation, (c) mean absolute error, and (d)
maximum absolute error.

data set of n = 52 training points and monitored the values of
MAE, Max.AE, MSD and Max.SD as a function of number of
new measurements for the different policies. Figure 11 (a) -
(d) shows the average values and standard deviations for MAE,
Max.AE, MSD and Max.SD for 100 trials. The performance of
EGO, KG and Max-u is considerably better than Max-v in the
ﬁrst 50 iterations shown, and the optimization sequence in Fig-
ure 10 ii) shows the evolution. As the training data are further
away from the global maximum in Figure 10 ii), we expect the
predictions to have large uncertainties. EGO samples points
near the global maximum in the ﬁrst few iterations (shown by
dark purple stars), whereas Max-v, B.EGO and Random are
sampling points further away. Thus, it is not surprising that the
trade-off methods, such as EGO, as well as the greedy Max-u
perform well. Thus, as expected, the distribution of the data is
a factor in the relaxation and performance.

C. Case study III: Multi-dimensional Curie transition
temperature for BaTiO3 based ceramics

The Curie transition temperature in BaTiO3 based ceramics
is affected by several features or descriptors, and here we
test how the transition temperature behaves in terms of multi-
dimensional variables without a guiding functional form for
the surface. In total, we employ 182 pieces of data obtained in
our laboratory, and our objective is to decrease the difference
between the predicted values and the measurement values. The
variables are selected based on our previous work that use
methods such as gradient boosting with a Kriging based model.
It has been shown that four features adequately capture the
data with the smallest 10-fold CVerror using all combinations
of features. For the initial model training we randomly select

0102030405002040608010001020304050030609012015001020304050016324801020304050060120180240(c)(d)(b)Mean Standard Deviations# Iterations B.EGO Max-v EGO KG Min.u Random(a)Maximum Standard Deviations# Iterations B.EGO Max-v EGO KG Min.u RandomMean Absolute Error# Iterations B.EGO Max-v EGO KG Min.u Random B.EGO Max-v EGO KG Min.u RandomMaximum Absolute Error# Iterations10

i) The surface and contours for the Hartmann3 function constructed using 400 points with independent variables x1, x2, x3 = YY ZZ. A
FIG. 6.
global minimum and four local minima are shown. As the ﬁgures are constructed using Latin hypercube sampling, the minima deviate slightly
from the actual minimum of the function. ii) Optimization process comparing the behavior of different utility functions. Sequences of visited
points after 10 iterations for different utility functions are shown by stars with an iteration number. The gray ﬁlled circles represent the training
data.

uncertainties begin to narrow and the mean value tends towards
zero. All strategies are efﬁcient in this sense with Max-v lead-
ing to a desired narrower sharply peaked distribution (# = 5) of
Figure 13. For Random selection and EGO, points with very
large uncertainties always occur because of the long tails in the
distribution. The other utilities target such points with large
uncertainties to reduce the tail in the uncertainty distribution.
The Kriging estimate for the fatigue line behavior shown in
Figure 14 is better than for Fe-C as the function by comparison
is quite monotonic and so the initial distributions of “Deviation
from true curve” shown in panels (# = 0) of Figure 14 are
narrower. The evolution of the distributions for utility functions
(except for Random) is similar to that in Figure 13. Because
the objective function is simpler, all utility functions show
very similar performance in the ﬁrst two iterations. Thereafter
the PDF of “Deviation from true curve” employing Max-v
quickly converges to zero compared to B.EGO and EGO. Thus,
irrespective of how good initially the predictive model is, Max-
v shows superior performance in these two 1D cases.

We conclude with some general remarks on circumstances
that favor Max-v and trade-off methods such as EGO. If the
variance is large and the deviation from the true result also
large, then selection by Max-v will have a signiﬁcant affect in
decreasing the deviation further. However, if the deviation is
small (i.e. the model is good), then the reduction in deviation
will not be signiﬁcant. This is likely the case for both of the 1D
curve examples in Case 1 where Max-v is especially good. Sim-
ilarly, in situations where the uncertainty is not too large, and
the deviation from the true result is signiﬁcant, then trade-off
methods such as EGO will have a substantial effect in locating

FIG. 7.
Comparison of the performance of utility functions in
optimizing the the targeted objective function. The initial data size
contains n = 80 training points and the mean values and error bars
showing 95% conﬁdence intervals of the points are evaluated over
50 trials. Shown is the behavior of (a) mean standard deviation,
(b) maximum standard deviation, (c) mean absolute error, and (d)
maximum absolute error.

steel, respectively. The Fe-C curve is more complex and its
Kriging estimate would not be expected to be as good. Thus,
for all the four utilities being studied, we see wide distributions
in the uncertainty proﬁle for the estimates and the deviation
from true curve (panel # = 0) of Figure 13. With successive
iterations as the next point is added, the distributions of the

i)ii)010203040500.00.10.20.30.40.5010203040500.00.20.40.60.81.0010203040500.00.10.20.30.4010203040500.00.51.01.52.0(c)(d)(b)Mean Standard Deviations# Iterations B.EGO Max-v EGO KG Min-u Random(a)Maximum Standard Deviations# Iterations B.EGO Max-v EGO KG Min-u RandomMean Absolute Error# Iterations B.EGO Max-v EGO KG Min-u Random B.EGO Max-v EGO KG Min-u RandomMaximum Absolute Error# Iterations11

to the maximum in the distribution. As expected, EGO works
well near the maximum, whereas Max-v is better where the
uncertainties are large. For a more uniform distribution of y
data values as in Figure 15(d) for the 1D FeC example, the
solution proﬁle is more complex as in Figure 15(b), with Max-v
having a greater impact on driving the optimization towards
the solution. We hope our work will motivate further studies
on a variety of materials data to conﬁrm our general ﬁndings,
as well as provide a deeper understanding of why Max-v works
so well across data sets with varying complexities.

FIG. 8. The number of new measurements needed to achieve a
given accuracy (1% and 3% deviation in maximum and minimum
values in MAE) for the curve as a function of the training data
size(Figure 8(a,b)). Each optimization process is repeated 100 times.
The points with the error bar are the mean values associated with
95% conﬁdence levels over 100 trials. The probability density of the
difference in the number of iterations using EGO and Max-v. Plotted
along the x axis is NEGO − NMax−v.

the max/min of the curve, that is, decrease the deviation fur-
ther. We suggest this is the case for the Hartmann 3 function
and PES intermolecular potential. Trade-off methods depend
on balancing the uncertainty (exploration) with exploitation
(model performance), and in the limits where the uncertainties
are either very large or very small, for a given deviation, then
from expression (4), EGO behaves either as Max-v or chooses
the model prediction, respectively.

To illustrate graphically, we have plotted in Figure 15(c)
and Figure 15(d) the distribution of the y data values for the
intermolecular potential energy surface (PES) and FeC phase
boundary examples. We note that the former deviates strongly
from a uniform distribution of values, whereas the latter is
closer to uniform. Schematics of the solutions corresponding
to these two cases are shown in Figure 15(a) and Figure 15(b),
where the red line is the actual solution and the black line the
model prediction. The distribution of y values in Figure 15(c)
typically gives rise to the curves of Figure 15(a) with a max-
imum and there are relatively few training data points close

FIG. 9.
Comparison of the performance of utility functions in
optimizing the the targeted objective function subject to noise levels
of 5%, 10% and 15% corresponding to Figure 9(a1,a2), (b1,b2) and
(c1,c2). The initial data size contains n = 80 training points and the
mean values and error bars showing the 95% conﬁdence intervals of
the points are evaluated over 100 trials. Shown is the behavior of (a)
mean standard deviation, (b) maximum standard deviation, (c) mean
absolute error, and (d) maximum absolute error.

ACKNOWLEDGEMENTS

The authors gratefully acknowledge the support of the Na-
tional Key Research and Development Program of China
(2017YFB0702401), and National Natural Science Founda-
tion of China (Grant Nos. 51571156, 51671157, 51621063,
and 51931004).

REFERENCES

[1] Askeland, D. & Wright, W. Essentials of Materials Science and
Engineering (Cengage Learning, 2013). URL https://books.
google.com/books?id=sJk8zU0oKpAC.

[2] Newnham, R. Properties of Materials: Anisotropy, Symme-
try, Structure. Properties of Materials: Anisotropy, Symmetry,
Structure (OUP Oxford, 2005). URL https://books.google.
com/books?id=gSkTDAAAQBAJ.

[3] Shen, C. et al. Physical metallurgy-guided machine learning
and artiﬁcial intelligent design of ultrahigh-strength stain-
less steel. Acta Materialia 179, 201 – 214 (2019). URL
http://www.sciencedirect.com/science/article/
pii/S1359645419305452.

[4] Im, J. et al. Identifying Pb-free perovskites for solar cells by
machine learning. NPJ computational materials 5 (2019).

0.050.100.150204060800.050.100.150.2004080120160-100100.000.150.300.450.60-20-1001020300.000.150.300.450.60 B.EGO Max-v EGO KG Min-u RandomNumber of iterationsSize of traindata B.EGO Max-v EGO KG Min-u RandomNumber of iterationsSize of traindata(c)PDFNEGO-NMax-v size=0.03 size=0.05 size=0.1(a)MAE threshold = 1% of y range(b)PDFNEGO-NMax-v size=0.05 size=0.1 size=0.15 size=0.2(d)MAE threshold = 3% of y range0.000.150.300.450.600.00.51.01.52.02.50.000.150.300.450.600.00.51.01.52.02.5010203040500.150.300.450.60010203040501.02.03.0 B.EGO Max-v SKO KG Min-u Random(b2) noise level = 10% y range(b1) noise level = 10% y range(a2) noise level = 5% y rangeMean Absolute Error(a1) noise level = 5% y range B.EGO Max-v SKO KG Min-u RandomMaximum Absolute Error B.EGO Max-v SKO KG Min-u RandomMean Absolute Error B.EGO Max-v SKO KG Min-u RandomMaximum Absolute Error B.EGO Max-v SKO KG Min-u RandomMean Absolute Error# Iterations(c1) noise level = 15% y range B.EGO Max-v SKO KG Min-u RandomMaximum Absolute Error# Iterations(c2) noise level = 15% y range12

FIG. 10. i) The surface and contours of the Ar-SH potential energy surface with independent variables including the distance between Ar and
the center of mass of SH in Angstrom(R) and the angle theta(ANGLE). ii) Optimization process comparing the behavior of different utility
functions. Sequences of visited points after 10 iterations for different utility functions are shown by stars with an iteration number. The gray
ﬁlled circles represent the training data.

FIG. 11. Comparison of the performance of utility functions in op-
timizing the the targeted objective function. The initial data size
contains n = 52 training points and the mean values and error bars
showing 95% conﬁdence intervals of the points are evaluated over
100 trials. Shown is the behavior of (a) mean standard deviation,
(b) maximum standard deviation, (c) mean absolute error, and (d)
maximum absolute error.

FIG. 12. Comparison of the performance of utility functions in opti-
mizing the the targeted objective function for the Curie temperature
for BaTiO3 based ceramics. The initial data size contains n = 73
training points and the mean values and error bars showing the 95%
conﬁdence intervals of the points are evaluated over 100 trials. Shown
is the behavior of (a) mean standard deviation, (b) maximum standard
deviation, (c) mean absolute error, and (d) maximum absolute error.

[5] Stanev, V. et al. Machine learning modeling of superconducting
critical temperature. NPJ computational materials 4 (2018).
[6] Shin, D., Yamamoto, Y., Brady, M., Lee, S. & Haynes,
J. Modern data analytics approach to predict creep of
high-temperature alloys. Acta Materialia 168, 321 – 330
(2019). URL http://www.sciencedirect.com/science/
article/pii/S1359645419300953.

[7] Lookman, T., Balachandran, P. V., Xue, D. & Yuan, R. Active
learning in materials science with emphasis on adaptive sampling
using uncertainties for targeted design. NPJ computational
materials 5 (2019).

[8] Zhang, L., Lin, D.-Y., Wang, H., Car, R. & E, W.
Active learning of uniformly accurate interatomic poten-
Phys. Rev. Materials 3,
tials for materials simulation.

i)ii)01020304050040801201600102030405001002003004000102030405001530456001020304050040080012001600 B.EGO Max-v EGO KG Max-u Random(d)(c)(b)Mean Standard Deviations# Iterations(a) B.EGO Max-v EGO KG Max-u RandomMaximum Standard Deviations# Iterations B.EGO Max-v EGO KG Max-u RandomMean Absolute Error# Iterations B.EGO Max-v EGO KG Max-u RandomMaximum Absolute Error# Iterations0102030405004812010203040500102030405060700102030405003691201020304050060120180 B.EGO Max-v EGO KG Min-u Random(d)(c)(b)Mean Standard Deviations# Iterations(a) B.EGO Max-v EGO KG Min-u RandomMaximum Standard Deviations# Iterations B.EGO Max-v EGO KG Min-u RandomMean Absolute Error# Iterations B.EGO Max-v EGO KG Min-u RandomMaximum Absolute Error# Iterations13

FIG. 13. For the Liquidus line of the Fe-C phase diagram shown in Figure 4, the probability density functions (PDF) of the uncertainties
and deviation from true result associated with the Kriging model prediction for Random, Max-v, B.EGO and EGO are shown with successive
iterations.

023804 (2019).
1103/PhysRevMaterials.3.023804.

URL https://link.aps.org/doi/10.

[9] Fan, D. et al. A robotic intelligent towing tank for learn-
Science Robotics
URL https://robotics.sciencemag.org/

ing complex ﬂuid-structure dynamics.
4 (2019).
content/4/36/eaay5063.

[10] Brown, K. A., Brittman, S., Maccaferri, N., Jariwala, D. &
Celano, U. Machine learning in nanoscience: Big data at small
scales. Nano Letters 0, null (0). URL https://doi.org/10.
1021/acs.nanolett.9b04090. PMID: 31804080.

[11] Tran, K. & Ulissi, Z. W. Active learning across intermetallics
to guide discovery of electrocatalysts for CO2 reduction and
H-2 evolution. Nature Catalysis 1, 696–703 (2018). URL
https://doi.org/10.1038/s41929-018-0142-1.

[12] Gubernatis, J. E. & Lookman, T. Machine learning in ma-
terials design and discovery: Examples from the present
Phys. Rev. Materials 2,
and suggestions for the future.
120301 (2018).
URL https://link.aps.org/doi/10.
1103/PhysRevMaterials.2.120301.

[13] Casciato, M. J., Kim, S., Lu, J. C., Hess, D. W. & Grover,
M. A. Optimization of a Carbon Dioxide-Assisted Nanoparti-
cle Deposition Process Using Sequential Experimental Design
with Adaptive Design Space. INDUSTRIAL & ENGINEERING
CHEMISTRY RESEARCH 51, 4363–4370 (2012).

[14] Smith, J. S., Nebgen, B., Lubbers, N., Isayev, O. & Roitberg,
A. E. Less is more: Sampling chemical space with active learn-
ing. The journal of chemical physics 148, 241733 (2018). URL
https://doi.org/10.1063/1.5023802.

[15] Ramprasad, R., Batra, R., Pilania, G., Mannodi-Kanakkithodi,
A. & Kim, C. Machine learning in materials informatics:
recent applications and prospects. npj Computational Ma-
terials 3, 54 (2017). URL https://doi.org/10.1038/
s41524-017-0056-5.

[16] Fukazawa, T., Harashima, Y., Hou, Z. & Miyake, T. Bayesian
optimization of chemical composition: A comprehensive frame-
work and its application to RFe12-type magnet compounds.
Physical review materials 3 (2019).

[17] Talapatra, A. et al. Experiment design frameworks for accel-
erated discovery of targeted materials across scales. Frontiers
in Materials 6, 82 (2019). URL https://www.frontiersin.
org/article/10.3389/fmats.2019.00082.

[18] Vasudevan, R. K. et al. Materials science in the artiﬁcial intelli-
gence age: high-throughput library generation, machine learning,
and a pathway from correlations to the underpinning physics.
MRS Communications 9, 821–838 (2019).

[19] Reker, D. & Schneider, G. Active-learning strategies in
computer-assisted drug discovery. Drug discovery today 20,
458–465 (2015).

[20] Kapoor, A., Grauman, K., Urtasun, R. & Darrell, T. Gaussian
Processes for Object Categorization. International journal of
computer vision 88, 169–188 (2010).

[21] Kiyohara, S., Oda, H., Tsuda, K. & Mizoguchi, T. Acceleration
of stable interface structure searching using a kriging approach.
Japanese journal of applied physics 55 (2016).

[22] Cohn, D., Ghahramani, Z. & Jordan, M. Active learning with
mixture models. In Murray-Smith, R. & Johansen, T. (eds.)
Multiple model approaches to modelling and control, 167–83

14

FIG. 14. For the Fatigue life curve for SS304L steel shown in Figure 2, the probability density functions (PDF) of the uncertainties and deviation
from true result associated with the Kriging model prediction for Random, Max-v, B.EGO and EGO are shown with successive iterations.

[24] Seko, A. et al. Prediction of low-thermal-conductivity com-
pounds with ﬁrst-principles anharmonic lattice-dynamics cal-
culations and bayesian optimization. Phys. Rev. Lett. 115,
205901 (2015).
URL https://link.aps.org/doi/10.
1103/PhysRevLett.115.205901.

[25] Ling, J., Hutchinson, M., Antono, E., Paradiso, S. & Meredig,
B. High-Dimensional Materials and Process Optimization Us-
ing Data-Driven Experimental Design with Well-Calibrated Un-
certainty Estimates. Integrating materials and manufacturing
innovation 6, 207–217 (2017).

[26] Bassman, L. et al. Active learning for accelerated design of
layered materials. NPJ computational materials 4 (2018).
[27] Wen, C. et al. Machine learning assisted design of high entropy
alloys with desired property. ACTA materialia 170, 109–117
(2019).

[28] Yuan, R. et al. Accelerated search for batio3-based ceramics
with large energy storage at low ﬁelds using machine learn-
ing and experimental design. Advanced Science 6, 1901395
(2019). URL https://onlinelibrary.wiley.com/doi/
abs/10.1002/advs.201901395.

[29] Xue, D. et al. Accelerated search for materials with tar-
geted properties by adaptive design. Nature Communications
7, 11241 EP – (2016). URL https://doi.org/10.1038/
ncomms11241.

[30] Xue, D. et al. Accelerated search for batio<sub>3</sub>-
based piezoelectrics with vertical morphotropic phase boundary
using bayesian learning. Proceedings of the National Academy
of Sciences 113, 13301 (2016). URL http://www.pnas.org/
content/113/47/13301.abstract.

FIG. 15. Typical solutions and the probability density functions (PDF)
of the property values y for the intermolecular potential energy surface
(PES) and the FeC phase boundary datasets. The black line represents
the model prediction and the red line the true solution.

(1997).

[23] Schmidt, J., Marques, M. R. G., Botti, S. & Marques, M. A. L.
Recent advances and applications of machine learning in solid-
state materials science. NPJ comprtational materials 5 (2019).

EGOMax-vEGOMax-v(a)(b)ABCDExxyy[31] Terayama, K. et al. Efﬁcient construction method for phase
diagrams using uncertainty sampling. Phys. Rev. Materials
3, 033802 (2019). URL https://link.aps.org/doi/10.
1103/PhysRevMaterials.3.033802.

[32] Thrun, S. B. Efﬁcient exploration in reinforcement learning.

Tech. Rep., USA (1992).

[33] Whitehead, S. A study of cooperative mechanisms for faster

reinforcement learning (1991).

[34] Bisbo, M. & Hammer, B. Efﬁcient global structure optimization
with a machine learned surrogate model [arXiv]. arXiv 5 pp.,
supl. 8 pp. (2019).

[35] Theiler, J. & Zimmer, B. G. Selecting the selector: Comparison
of update rules for discrete global optimization. Stastical analy-
sis and data mining 10, 211–229 (2017). Conference on Data
Analysis (CoDA), Santa Fe, NM, MAR 02-04, 2016.

[36] Wang, Y., Reyes, K. G., Brown, K. A., Mirkin, C. A. & Powell,
W. B. Nested-batch-mode learning and stochastic optimization
with an application to sequential multistage testing in materials
science. Siam journal on scientiﬁc computing 37, B361–B381
(2015).

[37] Yuan, R. et al. Accelerated discovery of large electrostrains in
batio3-based piezoelectrics using active learning. Advanced Ma-
terials 30, 1702884 (2018). URL https://onlinelibrary.
wiley.com/doi/abs/10.1002/adma.201702884.

[38] Balachandran, P. V., Xue, D., Theiler, J., Hogden, J. & Lookman,
T. Adaptive Strategies for Materials Design using Uncertainties.
Scientiﬁc reports 6 (2016).

[39] Xue, D. et al. An informatics approach to transformation temper-
atures of niti-based shape memory alloys. Acta Materialia 125,
532–541 (2017). URL http://www.sciencedirect.com/
science/article/pii/S1359645416309454.

[40] CRESSIE, N. The origins of kriging. Mathematical geology 22,

239–252 (1990).

15

[41] JOURNEL, A. Kriging in terms of projections. Journal of the
international association for mathematical geology 9, 563–586
(1977).

[42] Ginsbourger, D., Le Riche, R. & Carraro, L. Kriging Is Well-
Suited to Parallelize Optimization, 131–162 (Springer Berlin
Heidelberg, Berlin, Heidelberg, 2010). URL https://doi.
org/10.1007/978-3-642-10701-6_6.

[43] Rasmussen, C. E. Gaussian processes for machine learning (MIT

Press, 2006).

[44] Roustant, O., Ginsbourger, D. & Deville, Y. DiceKriging,
DiceOptim: Two R Packages for the Analysis of Computer
Experiments by Kriging-Based Metamodeling and Optimization.
Journal of stastical software 51, 1–55 (2012).

[45] Powell, W. The Knowledge Gradient for Optimal Learning

(2011).

[46] Jones, D., Schonlau, M. & Welch, W. Efﬁcient global opti-
mization of expensive black-box functions. Journal of global
optimization 13, 455–492 (1998).

[47] Huang, D., Allen, T., Notz, W. & Zeng, N. Global optimization
of stochastic black-box systems via sequential kriging meta-
models. JOURNAL OF GLOBAL OPTIMIZATION 34, 441–466
(2006).

[48] Frazier, P. I., Powell, W. B. & Dayanik, S. A knowledge-gradient
policy for sequential information collection. Siam journal on
control and optimization 47, 2410–2439 (2008).

[49] Frazier, P., Powell, W. & Dayanik, S. The Knowledge-Gradient
Policy for Correlated Normal Beliefs. Informs journal of com-
puting 21, 599–613 (2009).

[50] Mozafari, F., Thamburaja, P., Srinivasa, A. R. & Moslemi, N. A
rate independent inelasticity model with smooth transition for
unifying low-cycle to high-cycle fatigue life prediction. Interna-
tional journal of mechanical sciences 159, 325–335 (2019).
[51] Sumiyoshi, Y. & Endo, Y. Spectroscopy of ar-sh and ar-sd. ii.
determination of the three-dimensional intermolecular potential-
energy surface. The Journal of Chemical Physics 123, 054325
(2005). URL https://doi.org/10.1063/1.1943968.

