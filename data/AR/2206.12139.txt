HARU: Haptic Augmented Reality-Assisted
User-Centric Industrial Network Planning

Qi Liao∗, Nikolaj Marchenko†, Tianlun Hu∗, Peter Kulics∗, Lutz Ewe∗
∗Nokia Bell Labs, Stuttgart, Germany
†Corporate Research and Advanced Engineering, Robert Bosch GmbH, Renningen, Germany

E-Mails:

@nokia-bell-labs.com,
qi.liao, peter.kulics, lutz.ewe
}
{

Nikolaj.Marchenko@de.bosch.com, tianlun.hu@nokia.com

2
2
0
2

t
c
O
3
1

]
I

N
.
s
c
[

2
v
9
3
1
2
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—To support Industry 4.0 applications with haptics
and human-machine interaction, the sixth generation (6G) re-
quires a new framework that is fully autonomous, visual, and in-
teractive. In this paper, we provide an end-to-end solution, HARU,
for private network planning services, especially industrial net-
works. The solution consists of the following functions: collecting
visual and sensory data from the user device, reconstructing 3D
radio propagation environment and conducting network planning
on a server, and visualizing network performance with augmented
reality (AR) on the user device with enabled haptic feedback. The
functions are empowered by three key technical components: 1)
vision- and sensor fusion-based 3D environment reconstruction, 2)
ray tracing-based radio map generation and network planning,
and 3) AR-assisted network visualization enabled by real-time
camera relocalization. We conducted the proof-of-concept in a
Bosch plant in Germany and showed good network coverage of
the optimized antenna location, as well as high accuracy in both
environment reconstruction and camera relocalization. We also
achieved real-time AR-supported network monitoring with an
end-to-end latency of about 32 ms per frame.

I.

INTRODUCTION

Impacted by the ever-increasing global spending on smart
manufacturing, the total addressable market for private net-
works is forecast to increase from $3.7 billion in 2021 to over
$109.4 billion in 2030, according to a report by ABI Research
[1]. However, the state-of-the-art network planning services
remain old-fashioned. For example, the network planning ser-
vice providers rely on either user-uploaded ﬂoor plan or on-site
measured site survey. Not only are such services costly in both
time and human resources, but they also provide a limited user
experience. Moreover, the modern ﬂexible and modular pro-
duction systems require industrial network planning solutions
to be quickly adapted to new network environments triggered
by dynamically changing batches of individual products.

Extended reality and digital twin can be promising tech-
nologies to facilitate autonomous and interactive network ser-
vices in the 6G era [2], [3]. However, most of the works
provided perspective on the challenges they raise to future
network systems, e.g., as emerging 5G use cases [4], while few
of the works considered them as a means to help build truly
user-centric, interactive cyber-physical network systems [5].
A few works proposed conceptual AR-based frameworks for
various small-scale network solutions. For example, WiART
[6] takes inputs from beam-steerable reconﬁgurable antennas
and enables users to select desired antenna radiation patterns in
the mobile app and observe their effects on link performance.
However, the AR visualization is limited to the antennas and
their radiation patterns within a short distance. In [5], the
authors proposed to scan an indoor apartment environment to

Fig. 1: Haptic AR-assisted network planning app

a vector facet model and use it for wireless signal propagation
prediction with ray tracing. Then, an AR application is applied
to visualize the prediction in the form of holograms with
an AR headset. However, the performance is limited by the
lack of information about the obstacles for radio propagation.
Also, the solution requires licensed AR applications and ex-
pensive commercial AR headsets. To provide autonomous,
visual, and interactive service for private network planning
with low-cost mobile devices, in [7] and [8], we proposed
HARU, a haptic augmented reality-assisted user-centric net-
work planning solution, enabled by vision- and sensor fusion-
based 3D radio environment reconstruction and AR-assisted
network visualization. The goal is to make the invisible radio
network visible and provide truly user-centric, gamiﬁed, haptic
interface to improve user experience. The service platform
receives video/image sequence and inertial measurement unit
(IMU) data from the user device via a mobile user interface
(UI), reconstructs the 3D radio environment based on the
received data, approximates the radio propagation model, and
demonstrates the optimized network planning solution based
on the user’s haptic feedback on UI with AR, as shown in
Fig. 1. A prerequisite for enabling AR is to estimate 6D
camera pose (including 3D position and 3D orientation) in real
time, known as the camera relocalization problem. With the
estimated camera pose, we can project the virtual objects from
the real-world coordinate system back to the camera coordinate
system. In our previous works [9], [10], we have provided
technical solutions and algorithms for individual functions,
3D radio environment reconstruction and real-time camera
relocalization, respectively. In this paper, we focus on the
system design of the end-to-end solution and its application
to a real industrial environment. Note that in the previous
works, we conducted the experiments in a small-scale ofﬁce
environment only, while in this paper we demonstrate the
most recent results of the proof-of-concept (PoC) in a Bosch

 
 
 
 
 
 
Fig. 2: Architecture of the end-to-end AR-empowered network planning solution

plant in Germany. To the best of the authors’ knowledge, we
provide the ﬁrst end-to-end interactive AR-assisted solution to
industrial network planning. Our contributions are summarized
as follows.

•

•

•

On the client side, we developed an Android app,
which sends visual and sensory data from the mobile
device to a server, receives model and localization
data from the server, and performs local rendering and
haptic function for the interactive interface.
On the server side, we developed and implemented
the following functions: 1) a 3D environment recon-
struction algorithm called simultaneous localization
and mapping with object recognition (SLAMORE)
[9], which can detect, track, localize, and reconstruct
the major obstacles for electromagnetic waves, 2)
a network planning policy assisted by ray-tracing
application, and 3) a real-time camera relocalization
algorithm [10] based on deep learning and sensor
fusion to enable the AR features.
We demonstrated the PoC of the end-to-end solution
with a standard Android mobile device and the edge
server in the real industrial environment – a Bosch
plant in Germany, and achieved an end-to-end de-
lay (including data compression, communication, and
computation) of about 32 ms per frame.

II. SYSTEM ARCHITECTURE

The proposed haptic AR-assisted network planning service
platform includes three main techniques: 1) mapping and
3D industrial environment reconstruction, 2) ray tracing-based
radio map generation and network planning, and 3) real-time
camera relocalization for AR features. Because the remote
service aims to provide real-time AR experience for monitoring
and interacting with the network, we need to allocate the dif-
ferent functions in either the mobile device or the server based
on their computational, transmission, and latency constraints.
After extensive experiments, we design the system architecture
of the end-to-end solution as shown in Fig. 2. We decide to
load the reconstructed 3D model and radio maps from the
server to the device, such that the AR rendering function can
be called locally. In general, local rendering function responds
much faster to user’s inputs for interaction, compared to remote

rendering. Moreover, because our SLAMORE algorithm only
extracts and reconstructs objects relevant to radio propagation,
the size of data is small (less than 5 MB) to be transferred
and locally stored. On the other hand, camera relocalization
is performed on the server because the deep learning-based
inference is
model
computationally inefﬁcient.

is relatively large and real-time local

In the ﬁrst step (mapping), the user runs our app on a mo-
bile device and the app sends the captured visual and sensory
data to the server. The server receives the data and calls the
mapping function – our developed SLAMORE algorithm [9]
– to detect, track, localize, and reconstruct the major obstacles
for electromagnetic waves. In this way, the server reconstructs
an “extracted” version of the environment customized for radio
propagation. Such a sparse reconstruction of the environment
signiﬁcantly eases the computation of ray tracing for further
steps of radio map generation and network planning.

In the second step (interactive network planning), with
the haptic function,
the user can specify a preferred area
(e.g., where a power supply is available) for access point
(AP) deployment and send the information to the server. The
server generates radio maps with ray tracing based on the
reconstructed 3D environment, and performs network planning
policy constrained by the user-speciﬁed area. Then, the server
sends the extracted environment model,
the proposed AP
deployment position(s), and the corresponding generated radio
map(s) to the user. The data is saved in the local storage and
can be retrieved locally to enable haptic interaction with the
virtual 3D model of the network environment.

In the ﬁnal step (AR-assisted network monitoring), the
server trains a multi-input deep neural network (DNN) for
camera relocalization [10], based on the previously collected
data and the generated environment in the mapping step. The
user sends the visual and sensory data to the server in real time,
and the relocalization model takes it as input and estimates the
6D camera pose as output. The server then sends the estimated
camera pose to the user device, and the app computes rendering
locally and projects the augmented radio map on the captured
camera view.

Mapping: SLAMORERadio map generationNetwork planningRelocalizationHaptic functionLocal renderingObject databaseLocal StorageCamera & IMU dataHaptic feedbackCamera poseOffload environment model, proposed antenna position(s), and generated radio map(s) from serverSecure customer storageClientServerRTPProtocol buffers(protobuf)CommunicationVirtual 3D modelAR viewer of networkReconstructed modelRadio mapsData collectionHow to efﬁciently reconstruct an “extracted” industrial
radio propagation environment that achieves good accuracy
yet eases the task of 3D radio propagation modeling?

To this end, we propose a feature- and object-based
monocular SLAM algorithm called SLAMORE [9], which can
efﬁciently detect, track, localize, and reconstruct the major
obstacles for electromagnetic waves. It extracts the industrial
network environment composed of reconstructed obstacles for
radio propagation and detected space boundaries. SLAMORE
is built on the basis of a state-of-the-art visual-inertial SLAM
algorithm ORB-SLAM3 [11]. However, ORB-SLAM3 recon-
structs the environment with sparse 3D point cloud and the
output point cloud does not provide the information needed
for radio propagation modeling, such as real-world coordi-
nates and scale, object segmentation, and surface material.
To overcome this challenge, we propose SLAMORE with
the following new features, as shown in Fig. 3: 1) We train
a customized convolutional object detector for a deﬁned set
of objects in industrial radio propagation environment. The
label includes the class of the object, 3D shape and size, and
surface material. 2) A series of new functions are embedded
in the local mapping thread for 3D object reconstruction
including object detection,
and called for every keyframe,
associating map points to objects, object tracking, new object
creation, and object culling and merging. 3) We solve the
major problem in ORB-SLAM3 when using low-cost and
low-frequency IMU sensors from mobile device – recovery
of the real-world coordinates and scale – by using the side
information contained in the class labels of the recognized
objects and by ﬁltering the IMU measurements.

With these new features, we can extract sufﬁcient but not
overwhelming information about the environment, to be further
used as the input for the efﬁcient radio propagation modeling.
Due to the limited space, we refer the interested readers to our
previous work [9] for the technical details.

C. 3D Radio Map Generation and Network Planning

Because ray tracing for radio propagation modeling is a
well-studied topic, we can use existing algorithms or software
development kits (SDKs) to compute the radio map such
as WinProp [14] and WISE [15]. Also,
to further reduce
complexity, various shooting-and-bouncing ray tracing algo-
rithms [16] can be applied. Moreover, because SLAMORE is
customized for reconstructing radio propagation environment,
it allows more efﬁcient computation of ray tracing, comparing
to the complex 3D scan composed of huge nonlinear trian-
gular meshes. With ray tracing, we can obtain the reference
signal received power (RSRP) values of any point in the 3D
space. For efﬁcient network planning, we can discretize the
bounded 3D space S := (cid:2)x(min), x(max)(cid:3)
×
(cid:2)z(min), z(max)(cid:3) into equally-spaced nonoverlapping voxels.
Thus, the 3D RSRP map can be computed for each antenna
S, denoted by P(a) := [pi,j,k(a)], where pi,j,k(a)
position a
∈
denotes the RSRP at the voxel with indices (i, j, k) in x, y, z
dimensions, respectively.

(cid:2)y(min), y(max)(cid:3)

×

The network planning aims to provide good coverage of
S, especially
the factory by optimizing the position of AP a
where the machines/devices equipped with transceivers are
located. We deﬁne the utility function as the weighted sum of
RSRP over all voxels, while assigning higher weights to the

∈

Fig. 3: SLAMORE architecture, where Sim3/SE3 stands for
3D afﬁne transform and BA means the bundle adjustment.
The white blocks are the original functions from ORB-SLAM3
[11], while the blue blocks are our developed and embedded
functions for 3D object reconstruction and plane detection.

III.END-TO-END SOLUTION

In this section, we describe the following technical compo-
nents of the end-to-end solution: 1) data collection and prepro-
cessing, 2) mapping and 3D radio environment reconstruction,
3) 3D radio map generation and network planning, and 4) real-
time camera relocalization.

A. Data Collection and Preprocessing

The app running on mobile device captures in real time
the RGB camera stream and the motion sensor measurements,
including timestamp, 3D angular rate, 3D linear acceleration,
and 4D quaternion from the device, and sends them to the
server via real-time transport protocol (RTP) (details of the
implementation will be given in Section IV-1). These mea-
surements are used for both 3D environment reconstruction
and camera relocalization algorithms.

On the server, to facilitate SLAMORE and reconstruct
an obstacle-aware radio propagation environment, a labelled
dataset of ﬁnite objects in the industrial environment needs to
be pre-collected. One may argue that in practice the pretrained
set of object classes can be too large for recognizing all
obstacles in the new environments. However, because we target
private networks, we can train multiple sets of classes for
different types of customers. For example, automotive man-
ufacturing plants from various manufacturers possibly have
deployed similar types of equipment, thus we can create a
large dataset for this speciﬁc type of customers.

B. Mapping and 3D Radio Environment Reconstruction

Most of the mapping and environment reconstruction meth-
ods, such as 3D scanning or simultaneous localization and
mapping (SLAM) algorithms, provide complex reconstructed
models by creating ultra-dense 3D point cloud (a collec-
tion of points deﬁned by a given coordinate system) and
recovering smooth nonlinear surfaces [12]. However, for radio
propagation environment reconstruction we cannot afford such
complexity: modeling radio propagation maps, e.g., by using
ray tracing [13], can be computationally extremely expensive
if it is based on a complex environment. Thus, to customize an
environment reconstructor for 3D radio propagation modeling,
we target the following problem:

Feature extractionIMU integrationFrameIMUInitial pose estimation, relocalization or map creationTrack local mapNew keyframe decisionKeyframePre-trainedobject detectorKeyframe insertionMap points cullingNew points creation Local BAIMU initializationIMU refinementLocal keyframes cullingObject detectionTrackingLocal mappingAssociate map pointsTrack local objectsCreate new objectsObjects culling/mergeCandidate detectionCompute Sim3/SE3Loop closing & map mergingLoop fusionOptimizing essential graphFull BAUpdate mapMerge mapsLocal BAOptimizing essential graphFull bundle adjustmentHorizontal plane and gravity vector detection with IMUMap pointsKeyframesObjectsAssociatemappointswithobjectsRecoverworldcoordinateweights of the pretrained CNN object detector for SLAMORE
as initial weights, since it is customized for the predeﬁned
object classes in the interested industrial environment. The
sensor input is then passed through a multi-layer perceptron
(MLP) composed of three dense (fully connected) layers with
ReLU activation. The output layers of the CNN and MLP
are concatenated and followed with 5 more dense layers
with ReLU activation. Finally, the output layer with linear
activation provides the camera pose estimation. The training
data can be obtained from the data previously collected for
SLAMORE, including the video sequence, the IMU data, and
the corresponding camera pose in the world coordinate system.
Due to the limited space, we omit the details here but refer
the interested readers to our previous work [10].

After deriving the estimated camera pose in the world
R3 and rotation
coordinate system, denoted by location lw
R3×3 (rotation matrix from the camera coordinate sys-
Rcw
tem to the world coordinate system, converted from orientation
o), we can project any 3D point of radio map pw in the world
coordinate system into the pixel system (u, v) as below:

∈

∈

(cid:34)u(cid:48)
v(cid:48)
α

(cid:35)

= KRT

cw [pw −
u = u(cid:48)/α and v = v(cid:48)/α,

lw] ,

(2)

(3)

where RT
world to the camera coordinate system, K
camera intrinsic matrix, and α
by SLAMORE.

cw = Rwc indicates the rotation matrix from the
R3×3 is the
∈
= 0 is the scaling factor learned

IV. PROOF-OF-CONCEPT

We conducted the experiment in a Bosch plant in Germany,
10m. The applied antenna model
and selected an area of 15m
is Nokia FW2HC integrated omni antenna with antenna gain
of 4.7 dBi. We use Nokia 6 as the mobile device and the
edge server is equipped with 4 Nvidia Tesla K80 GPUs for
SLAMORE and DNN training and inference.

×

1) Data Collection and Communication Protocols

Our developed Android app captures RGB camera stream
with a resolution of 480p and the motion sensor measurements
including timestamp, 3D angular rate, 3D linear acceleration
(from accelerometer), and 4D quaternion (from Android atti-
tude composite sensors which derive the rotation vector from
the physical sensors accelerometer, gyroscope, and magne-
tometer). Note that many mobile devices are equipped with
low-cost magnetometers and the reported quaternion can be
very noisy. A low-pass ﬁlter is applied to the sensor output
to reduce noise artifacts and smooth the signal reading. We
use RTP to carry the video stream compressed by codec. The
sensory data is also included in the RTP packets along with the
video stream since the RTP speciﬁcation allows for a custom
extension to the protocol.

2) 3D Radio Environment Reconstruction

To test SLAMORE, we collected 5 video sequences, which
provided 80596 frames in total. For training the object detector,
we manually labelled only 300 keyframes with 25 classes
of machines and objects in the industrial environment. The
object detector is initialized with the SSD MobileNetV2 [17]

Fig. 4: Multi-input DNN for real-time camera relocalization

following types of voxels: 1) those associated to the machine
objects which generate network trafﬁc, and 2) those associated
to the deﬁned trajectory of mobile objects such as automated
guided vehicles (AGVs). The optimization problem is then
deﬁned as

max
a∈S

U (a), s.t. U (a) =

(cid:88)

i,j,k

wi,j,kpi,j,k(a).

(1)

Because of the interactive interface and the haptic functions,
the users can specify their preferred deployment areas, e.g., a
limited area on the ceiling, denoted by S(cid:48). Thus, the searching
space S is further reduced to S(cid:48) and we can replace S with
S(cid:48) in (1). We consider the gradient-based search over S(cid:48). For
efﬁcient searching, we deﬁne the initial point as the geometric
center c of all detected machines/devices. Then, we generate N
searching instances, each starts from a distinct initial position
sampled near c. We perform gradient-based search for each of
the instances, and choose the one with the maximum converged
utility among all instances. Note that the gradient-based search
is an iterative process, i.e., we need to compute the radio
map for each iterative AP position, thus a fast yet accurate
ray tracing computation is crucial to reduce the complexity.
SLAMORE well fulﬁlls the task, because it extracts sufﬁcient
but not overwhelming information about
the environment,
which further enables faster ray tracing computation.

D. Real-Time Camera Relocalization to Enable AR

To enable the AR features, e.g., overlaying the virtual radio
map on the 2D images captured by camera, we need to estimate
the 6 degrees of freedom (DoF) camera pose p := [l, o]
consisting of camera location l and orientation o in real time.
Therefore, we are interested in solving the following problem:

∈

At each time frame t, given camera captured image array
Rw×h×3, where w and h are the image width and
I(t)
height in pixel respectively, and the extracted feature vector
Rn, what is the corresponding
from motion sensors m(t)
camera pose p(t) := [l(t), o(t)] composed of camera location
l := [x, y, z] and orientation o?

∈

Note that the orientation o can be written in the form of 3D
rotation vector r := [rx, ry, rz] or its equivalent 4D quaternion
q := [qx, qy, qz, qw]. The quaternion q needs to be normalized
to unit length, thus the camera pose is still 6-DoF when o = q.

To solve the problem, we propose a multi-input DNN
including both image and sensor data in the inputs. The
architecture of the DNN is shown in Fig. 4. The input image is
resized to (224, 224, 3) and fed into a pretrained convolutional
neural network (CNN) backbone for object detection, such as
MobileNetV2 [17]. To accelerate the learning, we load the

Convolutional neural networksMulti-layer perceptronConcatenateFully connected layersLinearactivation(cid:54)
Fig. 5: Object detection customized to industrial environment

backbone model from Tensorﬂow 2 Object Detection API and
ﬁnetuned with our labelled data collected from the industrial
environment. Although the number of labelled training data is
limited, we are able to achieve 87.2% mean average precision.
An example of the detected objects in a frame is given in Fig.
5.

Fig. 6: [Left] Unaligned coordinates with detected map points
with ORB-SLAM3. [Right] Reconstructed environment with
the recognized 3D obstacles using SLAMORE.

Then, for every keyframe, we project the detected map
points in the 3D space back into the 2D camera view and
adaptively associate them to the bounding boxes detected
by the object detector. By clustering the 3D map points
associated to the objects, and, with the shape information
contained in the object’s label, we can estimate the position
of the objects. We evaluate the performance with the av-
erage object-based root mean square error (RMSE) deﬁned
ˆpi)2/3, where K is the
as (1/K)
number of detected objects, pi and ˆpi are the ground-truth
and the estimated object’s position in i
axis. We
achieve the averaged RMSE of 0.2564 m over all detected
objects within the selected area of 15m × 10m.

i∈{x,y,z}(pi

(cid:113)(cid:80)

x, y, z

(cid:80)K

∈ {

k=1

−

}

·

In Fig. 6 we compare the output of our proposed
SLAMORE with the state-of-the-art output of the mapping
computed from ORB-SLAM3 [11]. On the left side of the
ﬁgure we show the raw outputs of ORB-SLAM3, including
the map points as the black points and the estimated camera
positions as the red trajectory, with inaccurate alignment with
the world coordinates and without object segmentation. The
poor alignment is caused by the noisy measurements from the
low-cost IMU sensors in the mobile device. On the right side
we show the output of SLAMORE, providing the reconstructed
environment and obstacles for radio propagation environment,
and the map points associated to distinct obstacles.

3) 3D Radio Map Generalization and Network Planning

In this experiment, we call the functions from WinProp
[14] to compute the radio map with ray tracing technique.
The heuristic searching algorithm described in Section III-C
converges between 20 and 40 steps for each randomly initial-
ized instance, which, in total costs less than 1 hour to perform
the network planning task for the 3D industrial space. The
generated radio maps of the optimized antenna position at
different heights 10 cm, 100 cm, and 150 cm are shown in
Fig. 7. In Fig. 8 we show that the optimized antenna position
signiﬁcantly improves the coverage at all three heights.

Fig. 7: Generated radio map of the optimized AP position

4) Real-Time Camera Relocalization

∈

∈

Either rotation vector (radians) r

R3 or quaternion
R4 can be used to represent the camera orientation. In
q
Table I we compare the output features r and q when using
different loss functions: 1) W Euc Euc: weighted sum of
location Euclidean distance and quaternion Euclidean distance,
and 2) W Euc Ang: weighted sum of location Euclidean
distance and quaternion angle error. Table I shows that using
quaternion q as output and including angle error in the loss
function provides the best performance. The average inference
time per frame is 3.87 ms.

TABLE I: Comparison of loss function and output features

Mean location error
Mean orientation error

o = r
W Euc Euc
9.2124 cm
8.8237◦

o = q
W Euc Euc
10.3212 cm
5.6327◦

o = q
W Euc Ang
8.2937 cm
3.5262◦

5) Performance of the End-to-End Solution

For the AR-empowered network visualization, we achieve
the end-to-end latency (including data processing, data trans-
mission, control message transmission, and deep learning-
based model inference) of 31.83 ms per frame, i.e., frame rate
of over 30 fps. We can also interact with the virtual 3D model
as shown in Fig. 9a or monitor the network with an AR view
in real time as shown in Fig. 9b.

V. CONCLUSION AND FUTURE WORK

We proposed a novel, interactive, and AR-assisted frame-
work for industrial network planning service. We provided an

ACKNOWLEDGMENT

This work was supported by the German Federal Min-
istry of Education and Research (BMBF) project KICK
[16KIS1102K]. We thank Csaba Szabo for his contribution
to this project. Partial contents of this paper appear in [9] and
[10].

REFERENCES

[1] ABI Research, “The role of 5G in ICT transformation,” Feb. 2022.
[2] H. Viswanathan and P. E. Mogensen, “Communications in the 6G era,”

IEEE Access, vol. 8, pp. 57 063–57 074, 2020.

[3] H. X. Nguyen, R. Trestian, D. To, and M. Tatipamula, “Digital twin
for 5G and beyond,” IEEE Communications Magazine, vol. 59, no. 2,
pp. 10–15, 2021.

[4] H. H. H. Mahmoud, A. A. Amer, and T. Ismail, “6G: A comprehensive
survey on technologies, applications, challenges, and research prob-
lems,” Transactions on Emerging Telecommunications Technologies,
vol. 32, no. 4, p. e4233, 2021.

[5] G. Koutitas, V. Kumar Siddaraju, and V. Metsis, “In Situ wireless
channel visualization using augmented reality and ray tracing,” Sensors,
vol. 20, no. 3, p. 690, 2020.

[6] D. H. Nguyen, J. Chacko, L. Henderson, A. Paatelma, H. Saarnisaari,
N. Kandasamy, and K. R. Dandekar, “WiART-visualize and interact
with wireless networks using augmented reality,” in Proceedings of
the 22nd Annual International Conference on Mobile Computing and
Networking, 2016, pp. 511–512.

[7] Q. Liao, I. Malanchini, V. Suryaprakash, and P. Baracca, “Method and
apparatus for a radio communication network,” May 2018, US Patent
App. 15/799,542.

[8] Q. Liao, S. Wesemann, I. Malanchini, and P. Baracca, “Haptic aug-
mented reality assisted self-service for wireless networks,” Sept. 2019,
US Patent App. 16/347,029.

[9] Q. Liao, “SLAMORE: SLAM with object recognition for 3D radio

environment reconstruction,” in IEEE ICC, 2020, pp. 1–7.

[10] T. Hu and Q. Liao, “Real-time camera localization with deep learning

and sensor fusion,” in IEEE ICC, 2021, pp. 1–7.

[11] C. Campos, R. Elvira, J. J. G. Rodr´ıguez, J. M. Montiel, and J. D.
Tard´os, “ORB-SLAM3: An accurate open-source library for visual,
visual–inertial, and multimap slam,” IEEE Transactions on Robotics,
vol. 37, no. 6, pp. 1874–1890, 2021.

[12] M. Berger, A. Tagliasacchi, L. M. Seversky, P. Alliez, G. Guennebaud,
J. A. Levine, A. Sharf, and C. T. Silva, “A survey of surface reconstruc-
tion from point clouds,” in Computer Graphics Forum, vol. 36, no. 1.
Wiley Online Library, 2017, pp. 301–329.

[13] Z. Yun and M. F. Iskander, “Ray tracing for radio propagation modeling:
Principles and applications,” IEEE Access, vol. 3, pp. 1089–1100, 2015.
[14] R. Hoppe, G. W¨olﬂe, and U. Jakobus, “Wave propagation and radio
network planning software WinProp added to the electromagnetic solver
package FEKO,” in IEEE ACES, 2017, pp. 1–2.

[15] R. Valenzuela, “A ray tracing approach to predicting indoor wireless
transmission,” in IEEE 43rd Vehicular Technology Conference, 1993,
pp. 214–218.

[16] S. Kasdorf, B. Troksa, C. Key, J. Harmon, and B. M. Notaroˇs,
“Advancing accuracy of shooting and bouncing rays method for ray-
tracing propagation modeling based on novel approaches to ray cone
angle calculation,” IEEE Transactions on Antennas and Propagation,
vol. 69, no. 8, pp. 4808–4815, 2021.

[17] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“MobileNetV2: Inverted residuals and linear bottlenecks,” in IEEE
CVPR, 2018, pp. 4510–4520.

Fig. 8: Comparison of the RSRP distribution between initial
and optimized antenna positions at different heights.

(a) Interactive birdview of the reconstructed 3D virtual
network. User can choose different height
layers and
antenna positions to switch the view of the network.

(b) Real-time AR-assisted network monitoring, with aug-
mented radio propagation map and detected objects.

Fig. 9: Developed UI of the Android app

end-to-end solution, which receives visual and sensory data
from the mobile device, reconstructs the 3D industrial network
environment, performs network planning in the server, and
visualizes the network with AR on the mobile device. Several
building blocks were developed: 1) SLAMORE algorithm to
reconstruct the customized radio propagation environment for
the factory, 2) ray tracing-based radio map generation and
network planning, and 3) deep learning- and sensor fusion-
based real-time camera relocalization to enable the AR fea-
tures. Finally, we conducted the PoC of the proposed end-to-
end solution in a Bosch factory in Germany and showed good
coverage of the optimized antenna location, as well as high
accuracy in both environment reconstruction and camera relo-
calization. We also achieved the real-time end-to-end latency
of less than 32 ms per frame for the AR-empowered network
monitoring. The future work includes further scaling up and
accelerating the algorithms for larger factory space.

−60−40−20RSRP0.00000.00250.00500.00750.01000.01250.0150DensityLayer=10cm−60−40−20RSRPLayer=100cm−60−40−20RSRPLayer=150cmInitialOptimized