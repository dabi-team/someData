1
2
0
2

r
a

M
3
1

]

V
C
.
s
c
[

3
v
7
5
7
2
0
.
8
0
0
2
:
v
i
X
r
a

Unsupervised Learning for Identifying Events in
Active Target Experiments

R. Solli

Expert Analytics AS, Tordenskiolds gate 6, 0160, Oslo, Norway

Department of Physics, University of Oslo, POB 1048 Oslo, N-0316 Oslo, Norway

D. Bazin

Department of Physics and Astronomy and Facility for Rare Ion Beams and National
Superconducting Cyclotron Facility, Michigan State University, East Lansing, MI 48824, USA

M. Hjorth-Jensen

Department of Physics and Astronomy and Facility for Rare Ion Beams and National
Superconducting Cyclotron Facility, Michigan State University, East Lansing, MI 48824, USA

Department of Physics and Center for Computing in Science Education, University of Oslo,
POB 1048 Oslo, N-0316 Oslo, Norway

M.P. Kuchera

Department of Physics, Davidson College, Davidson, North Carolina, USA

R.R. Strauss

Department of Mathematics and Computer Science, Davidson College, Davidson, North
Carolina, USA

Department of Computer Science, University of North Carolina, Chapel Hill, North Carolina,
USA

Abstract

This article presents novel applications of unsupervised machine learning
methods to the problem of event separation in an active target detector,
the Active-Target Time Projection Chamber (AT-TPC) [1]. The overarch-
ing goal is to group similar events in the early stages of the data analysis,

Email address: robert@xal.no (R. Solli)

Preprint submitted to Nuclear Instruments and Methods in Physics Research Section AMarch 16, 2021

 
 
 
 
 
 
thereby improving efﬁciency by limiting the computationally expensive
processing of unnecessary events. The application of unsupervised clus-
tering algorithms to the analysis of two-dimensional projections of particle
tracks from a resonant proton scattering experiment on 46Ar is introduced.
We explore the performance of autoencoder neural networks and a pre-
trained VGG16 [2] convolutional neural network. We study clustering per-
formance on both data from a simulated 46Ar experiment, and real events
from the AT-TPC detector. We ﬁnd that a k-means algorithm applied to
simulated data in the VGG16 latent space forms almost perfect clusters.
Additionally, the VGG16+k-means approach ﬁnds high purity clusters of
proton events for real experimental data. We also explore the application
of clustering the latent space of autoencoder neural networks for event
separation. While these networks show strong performance, they suffer
from high variability in their results.

Keywords: Active target experiments, Machine Learning, Unsupervised
Learning, Autoencoder Neural Networks

1. Introduction

The Active-Target Time Projection Chamber (AT-TPC) [1] is a novel
type of detector designed speciﬁcally for nuclear physics experiments where
the energies of the recoiling particles are very low compared to the energy
required to escape the target material. The luminosity of nuclear physics
experiments performed with ﬁxed targets is directly proportional to the
amount of material encountered by the beam. On the other hand, for sev-
eral classes of experiments the detection of recoil particles is paramount,
therefore limiting the target thickness. In addition, the properties of the
recoil particles are modiﬁed while traversing the target material, affecting
the resolutions that can be achieved. This necessary balance between lu-
minosity and resolution is particularly difﬁcult when performing experi-
ments with rare isotope beams, chieﬂy because of the low intensities avail-
able.

The concept of active targets aim at mitigating this compromise, by
turning the target itself into a detector [3]. Most active target detectors
such as the AT-TPC are composed of a time projection chamber (TPC)
where the detector gas is at the same time the target material. Recoil par-
ticles that originate from a nuclear reaction between a beam nucleus and

2

a gas nucleus can be tracked from the vertex of the reaction to their ﬁnal
position inside the active volume of the target. Their properties can there-
fore be measured without any loss of resolution regardless of the amount
of material traversed by the beam. At the same time, the detection efﬁ-
ciency is dramatically increased by the very large solid angle covered in
this geometry. A direct consequence of this concept is the inclusiveness of
the experimental data recorded by this type of detector: any nuclear reac-
tion happening within the target is recorded. Although this sounds like
an advantage from the scientiﬁc point of view, it poses great challenges
during the analysis phase that are reminiscent of bubble-chamber times
and on par with event classiﬁcation challenges in particle physics today,
see for example the recent review of Mehta et al. [4]. More often than not,
the reaction channel of interest has one of the lowest cross sections. When
analyzing the data one is therefore faced with the task of sorting out the
corresponding events from the background of other reaction channels.

Because TPCs produce three-dimensional images of charged particle
tracks, the event identiﬁcation task is often akin to a visual inspection
(comparable to the analyses made in the bubble-chamber era [5]), which
is not practical nowadays because of the large quantities of data. Ma-
chine learning techniques then appear as a promising prospect, in par-
ticular in the image recognition domain where much progress has been
made recently [4]. In addition, machine learning (ML) algorithms offer
new possibilities such as the potential discovery of unforeseen phenom-
ena that would have been missed by more traditional analysis methods.
Prior work has demonstrated the ability to apply supervised classiﬁcation
machine learning methods to AT-TPC data when a labeled training set is
available, whether through hand-labeled data or labeled simulated data (a
transfer learning application) [6]. In some experiments, a labeled data set
is unavailable. This could be due to the inability to hand-label events, or
the case where one does not know a priori the types and behaviors of the
reactions present in the detector in order to generate a labeled and sim-
ulated data set. In the latter case, there must also exist a validation data
set of real data, still requiring the ability to label a subset of the real data.
The unsupervised separation of event types, or clustering, based on a set of
ML algorithms is hereby examined, using experimental data recorded by
the AT-TPC during its commissioning experiment from a radioactive 46Ar
beam reacting on an isobutane target composed of proton and carbon nu-
clei.

3

Within the context of machine learning methods applied to the analysis
of nuclear physics experiments, the purpose of this work is thus to explore
the application of unsupervised learning algorithms to event identiﬁcation
from an active target detector. The necessity to identify events from raw
data prior to full processing is becoming a major issue in the data analy-
sis of detectors with complex responses such as the AT-TPC. After these
introductory words, we review the experimental information in the next
section. In section 3 we describe the set up of the data while sections 4
and 5 present the methods applied and our results and discussions, re-
spectively. Finally, our conclusions and perspectives for future work are
presented in section 6.

2. Experimental Details

The goal of the experiment was to measure the evolution of the elas-
tic and inelastic cross sections between 46Ar and protons as a function of
energy (the excitation function), and observe resonances in the composite
system 47K that correspond to analog states in the nucleus 47Ar. Spectro-
scopic information can then be obtained from the shape and amplitude of
the observed resonances [7]. The experiment was performed at the Na-
tional Superconducting Cyclotron Facility (NSCL) where a 46Ar beam was
produced via fragmentation of a 48Ca beam on a 9Be target at about 140
MeV/u. The 46Ar isotopes were then ﬁltered, thermalized, and ﬁnally re-
accelerated to 4.6 MeV/u by a linear accelerator. This scheme was used
to produce a low-emittance beam, which is necessary to guarantee a good
energy resolution in the excitation function. Because the 46Ar beam par-
ticles lose energy as they traverse the target gas volume, the position of
the reaction vertex along the beam axis is directly related to the energy at
which the reaction occurs. This allows the AT-TPC to measure the excita-
tion function over a wide range of energies from a single beam energy.

The detector was placed inside the bore of an MRI solenoid energized
to ∼ 2 Tesla. This axial magnetic ﬁeld served the purpose of bending
the trajectories of the recoil particles in order to i) increase their length
and ii) provide a measurement of their bending radius, directly related
to their magnetic rigidity. Because the recoil particles travel in gas, they
slow down and eventually stop, therefore their trajectories are described
by three-dimensional spirals (see [1]). One of the difﬁculties encountered
in the analysis is that the shape of these spirals does not have an analytical

4

form because it follows the energy-loss proﬁle of the particles. It there-
fore needs to be simulated via an integration, which is numerically costly.
Common integration methods such as Runge-Kutta have a computational
cost typically one of magnitude larger than an analytical calculation. Other
difﬁculties are related to several experimental effects that deteriorate the
quality of the data, namely saturation and cross-talk effects, as well as ran-
dom noise.

The method used in [7] to analyze the data followed a three-phase
sequence: cleaning, ﬁltering and ﬁtting. Traditional methods were used
to perform each of these tasks, and ultimately extract the scientiﬁc infor-
mation, but there were severe limitations and high computational costs
that become prohibitive in data sets larger by an order of magnitude. In
particular, as for the data presented and analyzed here, with data sets
in the terabyte region or larger, these more traditional methods become
prohibitively expensive from a computational stand. This is particularly
the case when using the Monte-Carlo ﬁtting procedure (explained later in
text), because of the thousands of calculated tracks that are required for
each individual event. For instance, the analysis of the 46Ar(p,p) data re-
quired several seconds per event on a CPU cluster composed of about 100
nodes.

The cleaning was performed using a combination of linear and circu-
lar Hough transforms on a 2-dimensional projection of the tracks [1]. The
following ﬁltering and ﬁtting phases were performed simultaneously, by
deﬁning the cost function to the ﬁtting algorithm as a sum of three χ2
components based on i) the position of the track in space, ii) the energy
deposited on each pixel of the sensor plane, and iii) the location of the ver-
tex of the reaction. While various ﬁtting algorithms were tested, the most
accurate was a Monte-Carlo algorithm that explored the six-dimensional
phase space of the particle’s kinematics parameters, reducing it progres-
sively at each iteration step until the desired accuracy was reached [1]. Al-
though this algorithm ended up being the most accurate, it is extremely
costly computationally because of the very large number of simulated
tracks needed for each event. The ﬁltering was performed by setting lim-
its to the χ2 distributions, below which the events were assigned as proton
scattering. This is a very inefﬁcient method because it requires performing
the ﬁtting for all events, including those that are not of interest. Pioneering
work on event identiﬁcation using machine learning methods, namely the
use of a pre-trained convolutional neural networks (CNN), later showed

5

that the ﬁltering phase would better be performed using this type of tech-
nique [6]. In addition, the authors of Ref. [6] demonstrated that the purity
and statistics of the data are improved with the use of machine learning
methods.

From the experimenter’s point of view, it is clear that the method used
in [7] to identify and ﬁlter events is not the most efﬁcient computationally.
The ML methods explored in [6] are a step forward, but they still rely on
supervised learning methods that require data labeling, a time-consuming
and error-prone process. The aim of the present study is to investigate
unsupervised learning methods that bypass the labeling step, and form
classes of events independently from the experimenter’s input. The task
of labeling the different classes is then much less time-consuming and can
potentially lead to the discovery of unforeseen types of events. Further-
more, it allows us to process larger amounts of data in a much shorter
time.

3. Data Preparation

In this section we give a brief overview of the data, for a more in-depth

consideration we refer the reader to Refs. [8, 9, 10].

The AT-TPC data we studied for this work was recorded as charge
time-series for each of the the ∼ 104 detector pads.
In this representa-
tion, an event is a record of 512 time-buckets for each of the 104 detector
pads. In our analysis, we represent each event as a down-sampled two-
dimensional projection. We chose to represent the data in two dimensions
to facilitate the use of advanced image-recognition machine learning mod-
els, and this data representation was shown to successfully classify the
events of this experiment in a supervised setting [6]. First, the time-series
data was represented as a three-dimensional point cloud, where each point
contains the maximum charge in the time-series trace. We log-transform
the data and perform a min-max scaling in order to map the data to the
interval [0, 1]. The data is projected onto a two-dimensional space by sum-
ming over the time-axis. The two-dimensional data is then down-sampled
into a 128 × 128 pixel image by discretizing the space in the x − y plane and
summing all charge values that fall within the bounds for each element lo-
cation.

One of the signiﬁcant considerations for the analysis of AT-TPC data
is to inject machine learning methods for track identiﬁcation at the best

6

point in the analysis pipeline. Using raw data is advantageous as it pro-
vides an unbiased view of the event, but the data volume and noise levels
might be prohibitive for the analysis. Therefore, we incrementally add
bias to the analysis by applying the algorithm further down the analy-
sis pipeline, with the beneﬁt being that more preprocessing improves the
signal-to-noise ratio, possibly improving model performance. To explore
this trade-off between model performance and preprocessing bias, we per-
formed our analysis on simulated, raw and cleaned events as discussed
below.

3.1. Simulated 46Ar events

A set of N = 8000 simulated AT-TPC events per class was generated
from the pytpc package developed by Bradt et al. [10] for the analysis of
the 46Ar(p, p) experiment.

For validation, we select a subset of the simulated data to be labeled
and treat the rest as unlabeled data. We chose this partition to consist of
15% of the data. We denote this subset and its associated labels as γL =
(X L, yL), while the entire data set is identiﬁed as X F. Note that X L ⊂ X F.
As this dataset is generated via simulation, the true labels are known for
this dataset.

3.2. Raw 46Ar events

The events analyzed in this section were retrieved from the 46Ar res-
onant proton scattering experiment recorded with the AT-TPC. While we
denote these events as raw, it is important to note that what we intend is
a raw two-dimensional projection of events without any post-processing
done to clean the data.

We display two different events from the 46Ar experiment in Fig. 1. The
top row illustrates a proton event with a distinctive spiral-shaped track,
while the bottom row shows a carbon event with a large fraction of noise.
A subset of this data was hand-labeled for validation, as discussed in [6].

3.3. Filtered 46Ar events

As we saw in the previous section, the detector picks up a signiﬁcant
amount of noise. We split the noise broadly in two categories, one being
randomly uncorrelated noise and the second one being structured noise.
The former can be quite trivially removed with a nearest-neighbor algo-
rithm, see for example [11], that checks if a point in the event is close to

7

any other. To remove the correlated noise, researchers at the National Su-
perconducting Cyclotron Laboratory of Michigan State University, devel-
oped an algorithm based on the Hough transform [12]. This transforma-
tion is a common technique in computer vision, used to identify common
geometric shapes like lines and circles, and has been used extensively in
high-energy particle physics since the bubble-chamber era [5].

We illustrate two ﬁltered events in Fig. 2. These are the same events
as shown in Fig. 1, but with the Hough transform and nearest-neighbor
ﬁltering applied. The same subset of hand-labeled data from Section 3.2
was used for validation.

8

Figure 1: Two- and three-dimensional representations of two events from the 46Ar exper-
iment. Each row is one event in two projections, where the color intensity of each point
indicates higher charge values recorded by the detector. The top row illustrates a proton
event with a distinctive spiral-shaped track, while the bottom row shows a carbon event
with a large fraction of noise. The beam direction is oriented along the z-axis (from posi-
tive to negative), close to the (x,y) origin. The magnetic ﬁeld is oriented on the same axis
as the beam, causing the bending of the charged particle’s trajectories. The spiral shape is
caused by the slowing down of the particles inside the gas volume. The bending radius
of the tracks depends not only on the energy, but also on the mass-to-charge ratio of the
particles, hence the large difference between proton and carbon events (see also section
2).

9

 x [mm]2000200  y [mm]2000200 z [mm]1002002000200 x [mm]2001000100200  y [mm] x [mm]2000200  y [mm]2000200 z [mm]01002002000200 x [mm]2001000100200  y [mm]Figure 2: Two- and three-dimensional representations of two events from the 46Ar ex-
periment. Each row is one event in two projections, where the lightness of each point
indicates higher charge values. These events have been ﬁltered with a nearest neighbors
algorithm and an algorithm based on the Hough transform [12].

10

 x [mm]2000200  y [mm]2000200 z [mm]2550751002000200 x [mm]2001000100200  y [mm] x [mm]2000200  y [mm]2000200 z [mm]7080901002000200 x [mm]2001000100200  y [mm]4. Methods

4.1. Classifying events

The traditional Monte Carlo event selection process, described in Sec-
tion 2, does not have a well-deﬁned method to quantify the effectiveness
of the event selection. In addition, the selection task produced a binary,
cut-based result: either a good or bad ﬁt to the event of interest. A bad ﬁt is
then assumed to be a different event type, and is removed from the anal-
ysis. In a broader perspective, an unsupervised classiﬁcation algorithm
would offer the possibility to discover rare events which may not be ex-
pected or are overlooked. These events would likely be ﬁltered out using
the traditional methods. From a practical point of view, compared to su-
pervised learning, it also avoids the necessary labeling task of the learning
set events, which is error prone and time consuming.

4.2. Why machine learning

The χ2 approach used in the traditional analysis performed on the 46Ar
data is extremely expensive computationally because it requires the simu-
lation of thousands of tracks for each recorded event. These events are in
turn simulated for each iteration of the Monte Carlo ﬁtting sequence. Even
though the reaction of interest in the 46Ar experiment had the largest cross
section (elastic scattering), the time spent on Monte Carlo ﬁtting of all of
the events produced in the experiment was the largest computational bot-
tleneck in the analysis. In the case of an experiment where the reaction of
interest would represent less than a few percent of the total cross section,
this procedure would become highly inefﬁcient and prohibitive. Adding
to this the large amount of data produced in this experiment (with even
larger data sets expected in future experiments), the analysis simply begs
for more efﬁcient analysis tools. The computationally expensive ﬁtting
procedure would be applied to every event, instead of the few percent of
the events that are of interest for the analysis. An unsupervised ML algo-
rithm able to separate the data without a priori knowledge of the differ-
ent types of events increases the efﬁciency of the analysis tremendously,
and allows the downstream analysis to concentrate on the ﬁtting efforts
only on events of interest. In addition, the clustering allows for more ex-
ploration of the data, potentially enabling new discovery of unexpected
reaction types.

11

4.3. Pre-trained neural networks

Training high-performing neural networks from scratch often requires
enormous data sets and computation time. However, it has been found
that models which are trained at large scale will learn general features
that are applicable to a variety of tasks. For example, large neural net-
works which are trained on the ImageNet data set [13] — a diverse image
classiﬁcation task — learn how to identify lines, edges, and other common
shapes that are useful for numerous problems. Thus, it is common prac-
tice to initialize the convolutional layers of a network with the pre-trained
weights learned from ImageNet (or some other large data set). The train-
ing process then only has to ﬁne-tune the network for the speciﬁc task.
Since we are building on prior knowledge in this case, learning becomes
far more efﬁcient, and better performance can often be achieved. Kuchera
et al. [6] used machine learning methods to classify the products of 46Ar re-
actions in the AT-TPC, and they found that a CNN initialized with weights
trained on ImageNet data resulted in the most successful classiﬁcation.

4.4. Clustering on latent spaces

In contrast with the classiﬁcation work of Kuchera et al. [6], we do
not assume access to ground truth labels and we are trying to solve a
fundamentally different learning problem. Thus, rather than ﬁne-tuning
a pre-trained network under the supervised learning regime, we extract
the output of the pre-trained network’s last convolutional layer as a latent
representation of the events, where each event is represented as a vector
in R8192. We then cluster events based on this representation using the
scikit-learn implementation of the k-means algorithm with default pa-
rameters [14].

4.5. Deep clustering: Mixture of autoencoders

As an alternative to relying on a pre-trained model, we also consider
the MIXAE algorithm [15], which is an end-to-end clustering model specif-
ically trained on the AT-TPC data.

The MIXAE model comprises several autoencoders, each of which cor-
responds to a cluster. Each autoencoder constructs a latent representation
of a given example. Those representations are used as inputs to an auxil-
iary network which assigns scores to the clusters, indicating the likelihood
that the given example belongs to each cluster. Examples are then assigned

12

to the cluster with the highest score. The number of autoencoders, and
thus the number of clusters, has to be determined beforehand.

The MIXAE algorithm relies on a few simple assumptions which are
necessary, but not sufﬁcient, for producing a high-quality model. The as-
sumptions can be stated as:

1. If an example is assigned to a particular cluster, the corresponding

autoencoder’s reconstruction should be accurate.

2. Within a batch of examples presented to the model, assignments

should spread across all clusters.

3. Each clustering prediction should be as strong as possible, i.e. as-

signing high probabilities is preferable to weak assignments.

The learning objective encourages these assumptions to be met. For
a more formal consideration of the model objective and the assumptions
made on the data by this model see Zhang et al. [15].

The architecture is portrayed in Fig. 3, wherein tapered boxes denote
a direction of compression in the network components. In the ﬁgure each
encoder and decoder pair makes up an autoencoder. The assignment of
the cluster for a sample x-y event image, ˆx ∈ R128×128, is taken to be the
index of the maximal element in the vector p as shown in the right-most
part of the ﬁgure. Finally, the model is trained end-to-end with back prop-
agation [16], as implemented in the machine learning package TensorFlow
[17].

4.6. Measuring performance

Unsupervised machine learning is often accompanied with a lack of
ground-truth labelled data. In the face of such missing data, a model is or-
dinarily assessed by measures that do not depend on knowing the ground
truth for any samples. However, as our work aims to explore the appli-
cation of unsupervised methods to track identiﬁcation, we chose the 46Ar
data since we have some ground truth labels to evaluate our models. The
measures we use to evaluate the models we then chose to be those which
measure the similarities between two arbitrary sets of clustering assign-
ments, while holding one to be the ground truth.

We measure the performance of the clustering algorithms by two func-
tions: the clustering accuracy and the adjusted rand index (ARI)[18]. Both

13

Figure 3: Schematic of a MIXAE model. A sample ˆx is compressed to set of lower-
dimensional representations {z(i)} by N autoencoders [15]. These samples are concate-
nated and passed through an auxiliary assignment network that predicts a conﬁdence of
cluster belonging for each autoencoder.

of these measurements fall in the range [0, 1], where 0 denotes a complete
disagreement and 1 a complete agreement between the ground truth and
predicted labels.

To compute the metrics we have to solve problems introduced by the
arbitrary labels of a clustering algorithm. That is, we do not know which
predicted assignment should correspond to a proton or carbon event. In
short, we want to ﬁnd the most reasonable correspondence between the
clusters and the ground truth classes. To solve this problem we ﬁrst deﬁne
two sets; the ground truth labels y = [yi, yi+1, . . .] as determined in sec-
ˆyi+1, . . .]. Furthermore,
tion 3, and the corresponding predictions ˆy = [ ˆyi,
we let both yi and ˆyi be integer representations of an event’s ground truth,
and predicted class, respectively.

To compute both the ARI and the clustering accuracy we also have
to construct the contingency table between two sets of clustering assign-
ments. A contingency matrix deﬁnes the overlaps between classes in these
sets, and its general form is shown in Table 1.

14

Encoder Decoder  Encoder Decoder 1Encoder Decoder ̂  2ConcatenateMixtureassignment[,,…,] 1 2  AssignmentTable 1: General form of a contingency table. Here yn and ˆyn are the ground truth labels
and clustering assignments respectively. The wij’s then describe how many samples are
in clusters yi and simultaneously in ˆyj.

y1

y2

w11 w12
w21 w22
...
...
f2
f1

· · ·

· · ·
· · ·
. . .
. . .

sums

e1
e2
...

ˆy1
ˆy2
...
sums

The algorithm for ﬁnding the clustering accuracy can then be described

in these steps:

• Compute the matrix W from the contingency table between y and ˆy

• Subtract W from its maximum value to ﬁnd the bipartite graph rep-

resentation of the assignment problem.

• Use an algorithm, like the Hungarian algorithm [19], to solve the
assignment problem, and take the average of the values in W this
solution indicates. This average is the clustering accuracy.

To compute the ARI we use the elements from the contingency table to

evaluate the function introduced by Hubert and Arabic [18]

ARI =

(cid:104)

(cid:105)

∑ (wij
) −
2
2) + ∑ ( fj

∑ (ei
(cid:105)

2) ∑ ( fj
2
(cid:104)
∑ (ei
−

/(n
2)
)
(cid:105)
2) ∑ ( fj

)

)

(cid:104)

1
2

∑ (ei

/(n
2)

.

(1)

2
The quantities fi and ei are deﬁned in Table 1. The important distinction
to make between the clustering accuracy and the ARI is that the clustering
accuracy is a simple comparison that does not account for chance assign-
ments, while the ARI does. In effect, this means that the accuracy is a good
heuristic for the performance, tempered by the ARI.

2

Lastly, we introduce two terms to describe cluster quality: purity and
quality. Purity is inferred by how much spread there is in the column be-
tween the ground truth labels in the matrix W. A high-quality cluster will,
in addition to being pure, also capture most entries the class represented
by the cluster.

The performance is measured by comparing model predictions on the

labelled subset of data (see Table C.7 in appendix B for more details).

15

5. Results and Discussions

The principal challenge in the AT-TPC experiments that we are trying
to address is the reliance on labelled samples in the analysis, as future
experiments may not have as visually dissimilar reaction products as we
observe in the 46Ar experiment. The ability to label data in the the 46Ar
experiment does, however, provide a useful example where we can then
explore unsupervised techniques.

We ﬁrst explore the results of applying a k-means approach on the la-
tent space of a pre-trained network. Subsequently, we investigate the per-
formance of the MIXAE algorithm as outlined in section 4.5.

5.1. k-means clustering on the VGG16 latent space

The results of the clustering runs are included in Table 3. We ran the k-
means algorithm N = 100 times with M = 10 initializations per run, of the
cluster centroids to assess the variability in the performance. The k-means
algorithm returns the best performing model of the M initializations on
the unsupervised objective. We report performance on the labelled subset
of data, using the labels to identify the top-performing model (Top 1). Ad-
ditionally, we report the mean and standard deviation of the algorithm on
the N trials, which indicate unsupervised performance.

We observe that the clustering on simulated data attains the highest
performance, and that there is a decline in performance as we add noise
by moving to the ﬁltered and raw data sets. The results are shown in Table
2.

Table 2: k-means clustering results on AT-TPC event data in the VGG-16 latent space, for
N = 100 runs of the k-means algorithm with M = 10 initializations. We observe that the
performance predictably decreases with the amount of noise in the data.

Accuracy

Top 1 µ ± σ

Simulated 0.97
0.75
Filtered
0.59
Raw

0.97 ± 0.0
0.75 ± 0.0
0.59 ± 0.0

ARI

Top 1 µ ± σ
0.89
0.40
0.17

0.89 ± 0.0
0.40 ± 0.0
0.17 ± 0.0

The lack of variability is explained by the number of initializations. As can
be seen from Table 3 where we run the k-means algorithm N = 1000 times
with M = 1 initializations of the centroids.

16

Table 3: k-means clustering results on AT-TPC event data in the VGG-16 latent space, for
N = 1000 runs of the k-means algorithm with M = 1 initializations. We observe that there
is signiﬁcant variability in the results, which is ordinarily masked by M re-initializations
that avoid local minima.

Accuracy

Top 1 µ ± σ

Simulated 0.97
0.75
Filtered
0.71
Raw

0.86 ± 0.18
0.75 ± 0.0
0.59 ± 0.019

ARI

Top 1 µ ± σ
0.89
0.40
0.29

0.63 ± 0.39
0.40 ± 0.0
0.18 ± 0.018

In addition to the performance measures reported in Table 3, it is in-
teresting to observe which samples are wrongly assigned. To investigate
this problem, we compute the matrices W as shown in Table 1. From these
tables, we can infer which classes are more or less entangled with others.
The results for each data set is shown in Figs. D.6, D.7 and D.8. We ob-
serve that the proton class is consistently assigned in a pure cluster. For
example, consider the row corresponding to the proton class in Fig. D.7.
The column corresponding to the largest entry in the proton row has zero
other predicted classes in it.

This high-quality cluster also appears in the clustering of raw data.
From Fig. D.8, we observe that there is a high purity proton cluster. In con-
trast to the ﬁltered data we observe that the deterioration in performance
can largely be ascribed to the algorithm creating a proton plus another
cluster and a carbon plus another cluster.

We repeat this analysis using a Principle Component Analysis (PCA)
dimensionality reduction1 on the latent space of the VGG16 model. This
is done to estimate to what degree the class separating information is en-
coded in the entirety of the latent space, or in some select regions. The
results from the PCA analysis, using the top 100 principal components,
were virtually identical to our previous results not containing the PCA
analysis. This in an interesting observation which indicates that the class-
encoding information is contained in a minority of the axes of variation in
the data.

1PCA is a common technique to ﬁnd the signiﬁcant variations in data by projecting

the data along a subset of its covariance matrix eigenvectors [20, 21]

17

Figure 4: A sample of proton events from different k-means clusters from the ﬁltered data
set. The bottom row shows proton samples that are intermingled with noise events in the
models’ predictions, and the top row belongs to a cluster that contains proton events
almost exclusively. Each row belongs to a single cluster corresponding to the ﬁltered
confusion matrix in Fig. D.7. While the events in the pure proton cluster are visually
distinct from the impure cluster, the difference is less pronounced than what we observe
for the full data-set in ﬁgure 5.
.

To further investigate the clusters presented in the matrix in Figs. D.7
and D.8, we visualize a random subset of examples from the proton events
belonging to different clusters for the ﬁltered and full data in Figs. 4 and 5,
respectively. We look at random samples of proton events in two clusters
to infer an intuition on the track properties that are considered similar. The
ﬁgures indicate that shorter tracks, therefore low-energy or small scatter-
ing angle protons, are more likely to appear similar to other event types.

5.2. MIXAE clustering results

In the previous section we demonstrated a powerful, but rather naive,
clustering technique for AT-TPC track identiﬁcation. To build on this re-
sult we will in this section explore the application of the mixture of au-
toencoders (MIXAE) algorithm introduced in section 4.5. For details on
hyper-parameter tuning and the experimental procedure for training the
MIXAE algorithm see the Appendix B.

18

Pure proton  clusterImpure mixed  clusterFiltered proton events from select K-means clustersFigure 5: A random sample of proton events from different k-means clusters from the
raw data set. Each row belongs to a single cluster corresponding to the full confusion
matrix in Fig. D.8. The proton events in the cluster corresponding to the ﬁrst row appear
more visually distinct from other reaction types while the proton events corresponding
to the cluster in the bottom row appear visually more similar to other reaction types in
our dataset.

19

Pure proton  clusterImpure mixed  clusterRaw proton events from select K-means clustersWith the best set of hyperparameters, each highest performing model
is thereafter run 10 times2. The results are listed in Table 4. We observe
that, while the algorithm can achieve a very strong performance, the per-
formance varies. In some cases the MIXAE model converges to a seem-
ingly good conﬁguration, based on its unsupervised training goals. How-
ever, when inspecting its clustering performance against labelled data, the
seemingly good model does no better than a model based on random se-
lection. This happens more frequently with the raw data, indicating an
interaction with the noise levels present in the events.

Table 4: MIXAE clustering performance on the 46 Ar experimental data with N = 10 runs
of the algorithm. To quantify the results we report the best performing model (Top 1),
and the mean and standard deviation for the result (µ ± σ). In contrast with the VGG-16
+ k-means approach we observe signiﬁcant variations in performance.

Accuracy

Top 1 µ ± σ

Simulated 0.96
0.75
Filtered
0.71
Raw

0.74 ± 0.16
0.71 ± 0.04
0.61 ± 0.07

ARI

Top 1 µ ± σ
0.84
0.52
0.32

0.33 ± 0.32
0.38 ± 0.14
0.09 ± 0.10

As with the VGG16 + K-Means approach we wish to further investi-
gate the clustering results. Taking the best performing MIXAE model on
ﬁltered and raw data we tabulate the clusters against their labels. These
tables are present in Figs. D.9 and D.10 for ﬁltered and raw data, respec-
tively.

Applied to raw data the MIXAE captures a proton cluster in a similar
vein to the k-means approach. The MIXAE forms a proton-majority clus-
ter, but with a signiﬁcant portion of the more noisy proton events being
clustered with the other-class. Additionally, we do not observe the carbon
events being separated from either the amorphous noise events or from
the proton cluster.

On ﬁltered data the highest performing MIXAE model achieves strong
separation of the other-class, but curiously creates two proton-majority
clusters.

2The MIXAE model is signiﬁcantly more computationally expensive to train than a

k-means model. Resulting in the skew in number of runs in the two cases.

20

The most striking result we present in this work is the success of the
k-means approach. As noted by [22], distance measures become less infor-
mative in higher dimensional spaces, but the k-means algorithm clusters
events well in our fairly high-dimensional VGG latent spaces. Another
surprise is the stability of the k-means algorithm. We attribute this stabil-
ity to the quality of the VGG16 latent space in creating class-separating
sub-spaces. While the separations are not perfect, the stability and qual-
ity of the proton track identiﬁcation create solid empirical grounding for
applying this approach to other active target experiments.

It is also interesting to compare and contrast the clustering results from
the MIXAE model with those of the VGG16 with the k-means approach.
In particular, the discrepancy in stability is worth noting. While the top
performing MIXAE runs outperform the k-means approach, its reliability
suffers. However, the high performance achieved indicates that this may
represent a valuable potential research path into more tailored models for
unsupervised track identiﬁcation.

5.3. Alternative approaches

In addition to the results presented in this section, we performed clus-
tering with a number of different algorithms included in the scikit-learn
package of Pedregosa et al. [14]. None of them provided any notable dif-
ferences from the k-means results or were signiﬁcantly worse. Notably, the
DBSCAN algorithm [23, 24] failed to provide any useful clustering results.
We ﬁnd this important as one of the signiﬁcant drawbacks of k-means,
and the deep clustering algorithm presented in section 4.5, is that they de-
pend on pre-determining the number of clusters. This is not the case for
DBSCAN.

Additionally, we considered the deep convolutional embedded clus-
tering (DCEC) approach by Guo et al. [25] as well as the MIXAE method
introduced by Zhang et al. [15]. While we were able to reproduce the au-
thors’ results on their data, the DCEC algorithm proved unable to cluster
AT-TPC data in our implementation. Further details on these experiments
are presented in the thesis by Solli [26].

However, this provides valuable insight as the seeds of the clusters
are constructed by a k-means algorithm. This insight contrasts with our
positive results from applying a pre-trained classiﬁcation model with k-
means and highlights potentially signiﬁcant differences in models trained

21

on a supervised or unsupervised objective for clustering tasks in nuclear
physics.

6. Conclusions and Perspectives

The purpose of this study has been to explore the application of unsu-
pervised learning algorithms to event identiﬁcation from an active target
detector. The necessity to identify events from raw data prior to full pro-
cessing is becoming a major issue in the data analysis of detectors with
complex responses such as the AT-TPC. As shown by both avenues ex-
plored in this work, it is clear that there is signiﬁcant potential to eventu-
ally achieve event classiﬁcation using fully automated unsupervised meth-
ods.

In particular, the ability of the k-means algorithm in picking out clear
proton clusters from the VGG16 latent space lends itself well to an ex-
ploratory phase of analysis, where clusters of events corresponding to
different reaction channels could be later identiﬁed by the experimenter.
Another interesting facet of the k-means clustering is its consistent perfor-
mance. As shown in Table 2, the variance is zero for the performance met-
rics. This result indicates that the clusters are very clearly deﬁned in the
VGG16 latent space. However, as can be seen from the non-proton clus-
ters in Figs. D.7 and D.8, this does not necessarily imply that the physical
signals are correspondingly clear. Furthermore, the unsupervised metric
that decides which of the M k-means initializations perform the best does
not necessarily coincide perfectly with separating the event classes. This
is evidenced by the highest performing model measured on labelled data
(Top 1) for the raw data showing up in Table 3, and being ﬁltered out from
Table 2.

A caveat to the k-means method is that the number of clusters has to be
speciﬁed in advance. Each experiment then has to be considered in light
of possible reaction channels to determine a sensible number of clusters
for this approach.

The same caveat is present in the MIXAE implementation. While it
shows better optimal performance than the k-means method, some in-
consistencies were observed that were notably not evident from the un-
supervised training-objectives of the model. These two factors currently
conspire to limit its immediate applicability, and more developments are
needed for this approach.

22

In summary, our study shows that unsupervised track classiﬁcation
with an implementation of the VGG16 and the k-means approach is a vi-
able solution. For future work, it is worth investigating whether the adap-
tation of models like the MIXAE algorithm will allow better performance
at no signiﬁcant cost to consistency. The two examples of unsupervised
machine learning methods studied in this work are a ﬁrst encouraging
step towards automated selection of similar events that could greatly re-
duce the resource cost of analysis. Selection and classiﬁcation algorithms
have the potential to eventually boost the efﬁciency of the experiment by
allowing post-trigger decisions based on such algorithm implemented in
hardware.

Acknowledgements

MHJ’s work is supported by the U.S. Department of Energy, Ofﬁce of
Science, ofﬁce of Nuclear Physics under grant No. DE-SC0021152 and U.S.
National Science Foundation Grants No. PHY-1404159 and PHY-2013047.
DB’s work is supported by the U.S. Department of Energy, Ofﬁce of Sci-
ence, Ofﬁce of Nuclear Physics, under Grant No. DE-SC0020451. This ma-
terial is based upon work supported by the National Science Foundation
under Grant No. PHY-2012865 (MPK). This project has also received sup-
port from the INTPART project of the Research Council of Norway (Grant
No. 288125) and the Davidson Research Initiative.

Appendix A. VGG16

The VGGNet models are a family of high-performing image classiﬁca-
tion, and object localization networks. In the VGGNet architecture a small
kernel size is leveraged to increase expressive power in a very deep con-
volutional network. A tabulated view of the VGG models can be seen in
[2].

The choice of the kernel size is based on the fact that a stacked 3 × 3
kernel is equivalent to larger kernels in terms of the receptive ﬁeld of the
output. Three 3 × 3 kernels with stride 1 have a 7 × 7 receptive ﬁeld, but
the larger kernel has 81% more parameters and only one non-linearity [2].
Stacking the smaller kernels then contributes to a lower computational
cost. Additionally, there is a regularizing effect from the lowered number
of parameters and increased explanatory power from the additional non-
linearities.

23

VGGnet models are distributed freely with weights trained on the Im-
ageNet [13] image classiﬁcation task. For the results in section 5.1 we used
a VGG16 model pre trained on ImageNet data.

Appendix B. MIXAE hyper-parameter tuning

In the MIXAE algorithm the hyper-parameters to adjust are all the or-
dinary parameters associated with a neural network. We chose to base
our neural network parameter choices on the VGG16 architecture. The
parameters chosen for the autoencoders are listed in full in Table B.6.

In addition to those parameters we have the weighting of the loss terms:
θ, α and γ. These weighting parameters are attached to the reconstruction
loss, sample entropy and batch-wise entropy respectively [15]. We focused
on the tuning of the clustering hyper-parameters, and deﬁned the autoen-
coder hyper-parameters to be a shallow 3 × 3 convolutional network as
detailed in the previous paragraph.

To train the MIXAE clustering algorithm, we use a large simulated
data set with M = 80000 points, evenly distributed between proton- and
carbon-events. The algorithm is trained on a subset of 60000 of these sam-
ples, and we track performance on the remaining 20000 events. On real
data the algorithm is trained on an unlabelled set of data, and evaluated
on a labelled subset3. Since there are then only three remaining hyperpa-
rameters we choose to perform a coarse grid-search for the optimal conﬁg-
uration. Finally, for the best parameters we re-ran the algorithm N = 10
times to investigate the stability of the algorithm.

The grids selected for the search are listed in Table B.5. The search

yielded an optimal conﬁguration with

3see Table C.7 for details

Table B.5: Hyperparameter grid for the MIXAE loss weighting terms. The grid is given
as exponents for logarithmic scales.

Parameter Grid

Scale

θ
α
γ

[−1, 5]
Logarithmic
[−5, −1] Logarithmic
[3, 5]
Logarithmic

24

θ = 10−1,
α = 10−2,
γ = 105.

(B.1)

(B.2)

(B.3)

For the full data set the MIXAE hyperparameters converge to the same

values as for the clean data:

θ = 101,
α = 10−1,
γ = 3.162 × 103.

(B.4)

(B.5)

(B.6)

Lastly we supply the conﬁguration used for the individual convolu-

tional autoencoder networks in Table B.6

Table B.6: Hyperparameters selected for the autoencoder components of the MIXAE al-
gorithm

Hyperparameter Value

Convolutional parameters:

Number of layers
Kernels
Strides
Filters

4
[3, 3, 3, 3]
[2, 2, 2, 2]
[64, 32, 16, 8, ]

Network parameters:

Activation
Latent dimension 20
Batchnorm

False

LReLu

Optimizer parameters:

η
β1
β2

10−3
0.9
0.99

25

Appendix C. Data

The data used for the analysis in this work were partitioned as shown

in Table C.7.

Table C.7: Descriptions of number of events in the data.

Simulated

Full

Filtered

Total
Labelled

8000
2400

51891
1774

49169
1582

Appendix D. Clustering confusion tables

To elucidate the results presented in Tables 4 and 3 we computed clus-
tering confusion-matrices. These matrices show a more detailed picture
of intermingled classes in a clustering or classiﬁcation task where ground
truth labels are available. In the ﬁgures below we tabulate the clusters, as
predicted by the algorithm, along the x-axis. Each cluster is decomposed
in its ground-truth members along the y-axis.

In this view, a perfect clustering algorithm will produce a confusion
matrix which only has nonzero elements in the primary diagonal under
free permutation of its columns.

26

Figure D.6: Confusion matrix for the k-means clustering of simulated AT-TPC events.
The true labels indicate samples belonging to the p (proton), or the carbon (C) class. We
observe very high quality separation between the proton and carbon classes.

27

01Predicted labelpCTrue label1206   0  691125Simulated02505007501000Event countFigure D.7: Confusion matrix for the k-means clustering of ﬁltered AT-TPC events. The
true labels indicate samples belonging to the p (proton), carbon (C), or other classes. Each
column denotes a cluster, with each cell in the column denoting the count of that rows’
class in the cluster. We observe that cluster 2 is a high quality proton event cluster

28

012Predicted labelpCOtherTrue label  45  96 298 168  22   0 254 698   1Filtered0200400600Event countFigure D.8: Confusion matrix for the k-means clustering of raw AT-TPC events. The true
labels indicate samples belonging to the p (proton), carbon (C), or other classes. Each
column denotes a cluster, with each cell in the column denoting the count of that rows’
class in the cluster. We observe that cluster 2 is a high quality proton event cluster

29

012Predicted labelpCOtherTrue label  55 196 197 158  54   0 428 686   0Raw0200400600Event countFigure D.9: Confusion matrix for the MIXAE clustering algorithm on ﬁltered AT-TPC
events. The true labels indicate samples belonging to the p (proton), C (Carbon), or
other classes. We observe that the algorithm forms two proton-majority clusters, and
one clearly deﬁned cluster of the other events.

30

012Predicted labelpCOtherTrue label 216 148  75  75  60  55  68  11 874Filtered200400600800Event countFigure D.10: Confusion matrices for the MIXAE clustering algorithm on raw AT-TPC
events. The true labels indicate samples belonging to the p (proton), carbon (C), or other
classes. We observe that the algorithm correctly captures a majority proton-event cluster
in cluster 0. However, in contrast with the k-means approach this cluster is contaminated
to some extent with both carbon events and other events.

31

012Predicted labelpCOtherTrue label 168  60 220  73  42  97  14 1001000Raw2004006008001000Event countReferences

[1] Joshua William Bradt. Measurement of isobaric analogue resonances of
47Ar with the active target time projection chamber. PhD thesis, Michigan
State University, 2017. URL http://publications.nscl.msu.edu/
thesis/%20Brandt_2017_5279.pdf.

[2] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional
Networks for Large-Scale Image Recognition. In International Confer-
ence on Learning Representations, 2015. doi: 10.1.1.740.6937.

[3] S. Beceiro-Novo, T. Ahn, D. Bazin, and W. Mittig. Active targets for
the study of nuclei far from stability. Progress in Particle and Nuclear
Physics, 84:124 – 165, 2015.
ISSN 0146-6410. doi: https://doi.org/
10.1016/j.ppnp.2015.06.003. URL http://www.sciencedirect.com/
science/article/pii/S0146641015000459.

[4] Pankaj Mehta, Marin Bukov, Ching Hao Wang, Alexandre G.R.
Day, Clint Richardson, Charles K. Fisher, and David J. Schwab.
A high-bias,
low-variance introduction to Machine Learning for
physicists. Physics Reports, 810:1, 2019. doi: 10.1016/j.physrep.
2019.03.001. URL https://linkinghub.elsevier.com/retrieve/
pii/S0370157319300766.

[5] P.V.C. Hough. Machine Analysis of Bubble Chamber Pictures. Conf.

Proc. C, 590914:554–558, 1959.

[6] Michelle. P. Kuchera, Raghu Ramanujan, Jack Z. Taylor, Ryan R.
Strauss, Daniel Bazin, Joshua W. Bradt, and Ruiming Chen. Machine
learning methods for track classiﬁcation in the AT-TPC. Nuclear In-
struments and Methods in Physics Research, Section A: Accelerators, Spec-
trometers, Detectors and Associated Equipment, 940:156, 2019. doi: 10.
1016/j.nima.2019.05.097. URL https://linkinghub.elsevier.com/
retrieve/pii/S0168900219308046.

[7] Study of spectroscopic factors at N = 29 using isobaric ana-
Physics Letters B, 778:
logue resonances in inverse kinematics.
155, 2018. URL https://www.sciencedirect.com/science/article/
pii/S0370269318300236?via{%}3Dihub.

32

[8] Wolfgang Mittig, Saul Beceiro-Novo, Adam Fritsch, Faisal Abu-
Nimeh, Daniel Bazin, Tan Ahn, William G. Lynch, Fernando Montes,
Amiee Shore, D. Suzuki, N. Usher, J. Yurkon, J.J. Kolata, A. Howard,
A.L. Roberts, X.D. Tang, and F.D. Becchetti. Active Target detectors
for studies with exotic beams: Present and next future. Nuclear In-
struments and Methods in Physics Research Section A: Accelerators, Spec-
trometers, Detectors and Associated Equipment, 784:494, 2015. doi: 10.
1016/J.NIMA.2014.10.048. URL https://www.sciencedirect.com/
science/article/pii/S0168900214012054.

[9] D. Suzuki, M. Ford, D. Bazin, W. Mittig, W.G. Lynch, T. Ahn, S. Aune,
E. Galyaev, A. Fritsch, J. Gilbert, F. Montes, A. Shore, J. Yurkon, J.J.
Kolata, J. Browne, A. Howard, A.L. Roberts, and X.D. Tang. Proto-
type AT-TPC: Toward a new generation active target time projection
chamber for radioactive beam experiments. Nuclear Instruments and
Methods in Physics Research Section A: Accelerators, Spectrometers, De-
tectors and Associated Equipment, 691:39, 2012. doi: 10.1016/J.NIMA.
2012.06.050.

[10] J. Bradt, D. Bazin, F. Abu-Nimeh, T. Ahn, Y. Ayyad, S. Beceiro
Novo, L. Carpenter, M. Cortesi, M.P. Kuchera, W.G. Lynch, W. Mittig,
S. Rost, N. Watwood, and J. Yurkon. Commissioning of the Active-
Target Time Projection Chamber. Nuclear Instruments and Methods
in Physics Research Section A: Accelerators, Spectrometers, Detectors and
Associated Equipment, 875:65, 2017. doi: 10.1016/J.NIMA.2017.09.
013. URL https://www.sciencedirect.com/science/article/pii/
S0168900217309683.

[11] T. Hasie, R. Tibshirani, and J. Friedman. The Elements of Statistical
Learning, Data Mining, Inference and Prediction. Springer, Berlin, sec-
ond edition, 2009.

[12] W Newman, Richard O Duda, and Peter E Hart. Graphics and Use
of the Hough Transformation To Detect Lines and Curves in Pictures.
Communications of the ACM, 15:11, 1972.

[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
In 2009 IEEE
Imagenet: A large-scale hierarchical image database.
conference on computer vision and pattern recognition, pages 248–255.
Ieee, 2009. doi: 10.1109/CVPR.2009.5206848.

33

[14] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent
Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Pret-
tenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and Édouard Duchesnay.
Scikit-learn: Machine Learning in
Journal of Machine Learning Research, 12:2825, 2011. URL
Python.
http://www.jmlr.org/papers/v12/pedregosa11a.html.

[15] Dejiao Zhang, Yifan Sunm, Brian Eriksson, and Laura Balzano. Deep
Unsupervised Clustering Using Mixture of Autoencoders. In Interna-
tional Conference on Neural Information Processing, page 373, 2017. URL
https://arxiv.org/pdf/1712.07788.pdf.

[16] Seppo Linnainmaa. Taylor expansion of the accumulated rounding
error. BIT, 16:146, jun 1976. doi: 10.1007/BF01931367. URL http:
//link.springer.com/10.1007/BF01931367.

[17] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz,
Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Ra-
jat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol
Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xi-
aoqiang Zheng, and Google Research. TensorFlow: Large-Scale Ma-
chine Learning on Heterogeneous Distributed Systems. In USENIX
Symposium on Operating Systems Design and Implementation, page 265,
2016. URL https://ai.google/research/pubs/pub45381.

[18] Lawrence Hubert and Phipps Arabic. Comparing Partitions.

Jour-
nal of Classiﬁcation, 2:193, 1985. URL https://link.springer.com/
content/pdf/10.1007{%}2FBF01908075.pdf.

[19] R.E. Burkard, M. Dell’Amico, and S. Martello. Assignment Problems.

SIAM, Philadelphia, USA, 2012.

[20] R. Vidal, Y. Ma, and S. Shankar Sastry. Generalized Principal Component

Analysis. Springer, Berlin, 2016.

34

[21] Stephen Marsland. Machine Learning: An Algorithmic Perspective,

2009. ISSN 00368075.

[22] Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Keim. On
the Surprising Behavior of Distance Metrics in High Dimensional
Space. pages 420–434. 2001. doi: 10.1007/3-540-44503-x_27. URL
https://bib.dbvis.de/uploadedFiles/155.pdf.

[23] Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu.
A density-based algorithm for discovering clusters in large spatial
databases with noise. pages 226–231. AAAI Press, 1996.

[24] James Bergstra and Yoshua Bengio. Random Search for Hyper-
Parameter Optimization Yoshua Bengio. Technical report, 2012. URL
http://scikit-learn.sourceforge.net.

[25] Xifeng Guo, Xinwang Liu, En Zhu, and Jianping Yin. Deep Cluster-
ing with Convolutional Autoencoders. In neural information processing
systems, page 373. 2017. doi: 10.1007/978-3-319-70096-0_39.

[26] Robert Solli. Latent Variable Machine Learning Algorithms: Applica-
tions in a Nuclear Physics Experiment. Master’s thesis, University of
Oslo, 2019. URL http://urn.nb.no/URN:NBN:no-75398.

35

