2
2
0
2

l
u
J

1
2

]

V
C
.
s
c
[

1
v
0
6
6
0
1
.
7
0
2
2
:
v
i
X
r
a

OMNI3D: A Large Benchmark and Model for 3D
Object Detection in the Wild

Garrick Brazil

Julian Straub Nikhila Ravi

Justin Johnson Georgia Gkioxari
Meta AI
https://github.com/facebookresearch/omni3d

Abstract

Recognizing scenes and objects in 3D from a single image is a longstanding goal
of computer vision with applications in robotics and AR/VR. For 2D recognition,
large datasets and scalable solutions have led to unprecedented advances. In 3D,
existing benchmarks are small in size and approaches specialize in few object
categories and specific domains, e.g. urban driving scenes. Motivated by the
success of 2D recognition, we revisit the task of 3D object detection by introducing
a large benchmark, called OMNI3D. OMNI3D re-purposes and combines existing
datasets resulting in 234k images annotated with more than 3 million instances
and 97 categories. 3D detection at such scale is challenging due to variations in
camera intrinsics and the rich diversity of scene and object types. We propose a
model, called Cube R-CNN, designed to generalize across camera and scene types
with a unified approach. We show that Cube R-CNN outperforms prior works on
the larger OMNI3D and existing benchmarks. Finally, we prove that OMNI3D is a
powerful dataset for 3D object recognition, show that it improves single-dataset
performance and can accelerate learning on new smaller datasets via pre-training.

Figure 1: Left: We introduce OMNI3D, a benchmark for visual 3D object detection which is larger
and more diverse than popular 3D benchmarks. Right: We propose Cube R-CNN, which generalizes
to unseen images (e.g. from COCO [42]) and outperforms prior works on popular 3D benchmarks.

1

Introduction

Understanding objects and their properties from single images is a longstanding problem in computer
vision with applications in robotics, assistive technology and AR/VR. In the last decade, 2D object
recognition [24, 37, 60, 61, 67] has made tremendous advances toward predicting and localizing
objects on the 2D image grid with the help of large-scale datasets [22, 42]. However, the world and
its objects are three dimensional laid out in 3D space. Perceiving objects in 3D from 2D visual inputs
poses new challenges framed by the task of 3D object detection. Here, the goal is to estimate a 3D
location and 3D extent of each object in an image in the form of a tight oriented 3D bounding box.

Preprint. Under review.

Dataset SizeCube R-CNN Predictions on COCOCube R-CNNGUPNetTotal3DSUN RGB-DKITTI32.019.630.921.4KITTIARKitScenesSUN RGB-DObjectronHypersimnuScenesOMNI3D10k234k7k 
 
 
 
 
 
Today 3D object detection is studied under two different lenses: for urban domains in the context
of self-driving vehicles [6, 12, 47, 49, 54] or indoor scenes [29, 34, 55, 68]. Despite the problem
formulation being shared, methods for urban and indoor scenes share little insights between domains.
Often approaches are tailored to work only for the domain in question. For instance, urban methods
make assumptions about objects resting on a ground plane and model only yaw angles for 3D rotation.
Indoor techniques may use a confined depth range (e.g. up to 6m in [55]). These assumptions are
generally not true for objects and scenes in the real-world. Moreover, the most popular benchmarks
for image-based 3D object detection are very small. Indoor SUN RBG-D [66] contains 10k images
and the urban KITTI [19] has 7k images; in contrast 2D benchmarks like COCO [42] are 20× larger.

We address the absence of a general large-scale dataset for 3D object detection by introducing a large
and diverse 3D benchmark called OMNI3D. OMNI3D is curated from publicly released datasets,
SUN RBG-D [66], ARKitScenes [5], Hypersim [62], Objectron [2], KITTI [19] and nuScenes [8],
and comprises 234k images with 3 million objects annotated with 3D boxes across 97 categories
including chair, sofa, laptop, table, cup, shoes, pillow, books, car, person, etc. As shown in Fig. 1(a),
OMNI3D is 20× larger than existing popular benchmarks used for 3D detection, SUN RGB-D and
KITTI. For efficient evaluation on our large dataset, we introduce a new, fast, batched and exact
algorithm for intersection-over-union for 3D boxes which is 450× faster than previous solutions [2].
We empirically prove the impact of OMNI3D as a large-scale dataset and show that it improves
single-dataset AP performance by up to 5.3% on urban and 3.8% on indoor benchmarks.

On this new dataset, we design a general and simple 3D object detector, called Cube R-CNN, inspired
by key research advances in 2D and 3D recognition of recent years [20, 49, 61, 64], which achieves
state-of-the-art results across domains. Cube R-CNN detects all objects and their 3D properties,
rotation, and depth end-to-end from a single image of any domain and for many object categories.
Attributed to OMNI3D’s diversity, our model shows strong generalization and outperforms prior
works for indoor and urban domains with one unified model, as shown in Fig. 1(b). Learning from
such diverse data comes with challenges as OMNI3D contains images of highly varying focal lengths
which exaggerate scale-depth ambiguity, exemplified in Fig. 4. We remedy this by operating on
virtual depth which transforms object depth with the same virtual camera intrinsics across the dataset.
An added benefit of virtual depth is that it allows the use of data augmentations (e.g. image rescaling)
during training, which is a critical feature for 2D detection [11, 73], and as we show, also for 3D.
Our approach with one unified design outperforms prior state-of-the-art approaches, Total3D [55] by
12.4% IoU3D on indoor SUN RGB-D and GUPNet [49] by 9.5% AP3D on urban KITTI.

2 Related Work

In this work we propose a new benchmark for general 3D object detection. We design a unified
approach to detect 3D objects from single RGB images by extending 2D object detection with the
ability to detect 3D cuboids in an intrinsics-invariant manner.

2D Object Detection. Here, methods include two-stage approaches [24, 61] which predict object
regions with a region proposal network (RPN) and then refine them via an MLP. Single-stage
detectors [41, 44, 60, 67, 78] omit the RPN and predict regions directly from the backbone.

3D Object Detection. Monocular 3D object detectors predict 3D cuboids from single input images.
There is extensive work in the urban self-driving domain where the car class is at the epicenter
[12, 17, 21, 23, 28, 43, 46, 51, 54, 59, 64, 65, 71, 77]. CenterNet [78] predicts 3D depth and size
from fully-convolutional center features, and is extended by [14, 39, 45, 47–49, 52, 76, 77, 80].
M3D-RPN [6] trains an RPN with 3D anchors, enhanced further by [7, 16, 35, 70, 81]. Others use
pseudo depth [3, 13, 50, 56, 72, 74] and explore depth and point-based LiDAR techniques [33, 57].
Similar to ours, [10, 64, 65] add a 3D head, specialized for urban scenes and objects, on two-stage
Faster R-CNN. [23, 65] augment their training by synthetically generating depth and box-fitted views,
coined as virtual views or depth. In our work, virtual depth aims at addressing varying focal lengths.

For indoor scenes, a vast line of work tackles room layout estimation [15, 26, 38, 53]. Huang et
al [29] predict 3D oriented bounding boxes for indoor objects. Factored3D [68] and 3D-RelNet [34]
jointly predict object voxel shapes. Total3D [55] predicts 3D boxes and meshes by additionally
training on datasets with annotated 3D shapes. We explore 3D object detection in its general form by
tackling both domains jointly and with a vocabulary of 5× more object categories than prior works.

2

OMNI3D SUN RGB-D

KITTI

COCO

LVIS

(a) Spatial statistics

(b) 2D Scale statistics

(c) Category statistics

Figure 2: OMNI3D (a) distribution of object centers in normalized image coordinates XY-plane (top)
and normalized depth XZ-plane (bottom), (b) relative 2D object scales, and (c) category frequency.

3D Datasets. KITTI [19] and SUN RGB-D [66] are popular datasets for 3D object detection on urban
and indoor scenes respectively. Since 2019, more datasets have emerged, both for indoor [2, 5, 62]
and urban [1, 8, 9, 18, 27, 30, 31]. In isolation, these datasets have unique properties and biases,
e.g. object and scene types, focal length, coordinate systems, etc. In this work, we unify existing
representative datasets [2, 5, 8, 19, 62, 66] to form a large-scale benchmark, called OMNI3D, which
is 20× larger than widely-used benchmarks and significantly more diverse. As such, new challenges
arise stemming from the increased variance in object rotation, size, layouts, and camera intrinsics.

3 The OMNI3D Benchmark

The primary benchmarks for 3D object detection are small (7k images in KITTI [19], 10k in SUN
RGB-D [66]), primarily focused on a few categories (e.g. 3 on KITTI), and dominated by a single
domain. The small size and lack of variance in 3D datasets is a stark difference to 2D counterparts,
such as COCO [42] and LVIS [22], which have pioneered progress in 2D recognition. We aim to
bridge the gap to 2D by introducing OMNI3D, a large-scale benchmark for image-based 3D object
detection consisting of 234k images, 3 million labeled 3D bounding boxes, and 97 object categories.
We re-purpose recently released 3D datasets of urban [8, 19], indoor [5, 62, 66], and general [2] scenes
to build a diverse large-scale dataset of varying camera intrinsics suitable for general image-based 3D
object detection. We analyze OMNI3D and show its rich spatial and semantic properties proving it is
visually diverse, similar to 2D data, and highly challenging for 3D as depicted in Fig. 2. We show the
value of OMNI3D for the task of 3D object detection with extensive quantitative analysis in Sec. 5.

3.1 Dataset Analysis

We curate OMNI3D from a collection of publicly available data sources, namely SUN RGB-D [66],
ARKitScenes [5], Hypersim [62], Objectron [2], KITTI [19] and nuScenes [8]. Each of these datasets
are either designed specifically for 3D object detection or contain 3D cuboid annotations which we
re-purpose accordingly. We semantically align object categories from each data source via manual
association which results in a set of 97 categories. Critically, all cuboids are converted to a unified 3D
camera coordinate system. There are a total of 234,152 images with more than 3 million annotations.
We split the dataset into 175k images for train, 19k for val and 39k for test, consistent with splits
from original sources when available, and otherwise free of video sequences overlapping in splits.
OMNI3D has images from indoor and outdoor domains. We denote respective subsets as OMNI3DIN
(SUN RGB-D, Hypersim, ARKit), and OMNI3DOUT (KITTI, nuScenes). Note that Objectron, which
primarily comprises close-up objects, is used only in the full OMNI3D setting.

Layout statistics. Fig. 2(a) shows in the top row the distribution of object centers onto the image
plane by projecting centroids on the XY-plane, and in the bottom row the distribution of object depths
by projecting centroids onto the XZ-plane. We compare OMNI3D to the most popular benchmarks for
3D object detection (SUN RGB-D and KITTI) and 2D object detection (COCO [42] and LVIS [22]).
We find that OMNI3D’s spatial distribution has a center bias, similar to 2D datasets. Fig. 2(b) depicts
the relative object size distribution, defined as the square root of object area divided by image area.
Objects are more likely to be small in size similar to LVIS (Fig. 6c in [22]) suggesting that OMNI3D
is also challenging for 2D detection, while objects in OMNI3DOUT are noticeably smaller.
The bottom row of Fig. 2(a) normalizes object depth in a [0, 20m] range, chosen for visualization
and satisfies 88% of object instances; OMNI3D depth ranges as far as 300m. We observe that the

3

Normalized widthNorm. heightRelative object sizePercent of instancesSorted category indexNumber of instancesNorm. depthN/AN/AOMNI3D depth distribution is far more diverse than SUN RGB-D and KITTI, which are biased toward
near or road-side objects respectively. See Appendix for each data source plot distribution. Fig. 2(a)
demonstrates OMNI3D’s rich diversity in spatial distribution and depth which suffers significantly
less bias than existing 3D benchmarks and is comparable in complexity to common 2D datasets.

2D and 3D correlation. To address the common assumption that urban objects appear on a mostly
planar ground, we compute the correlation between normalized 2D y and 3D z depth. We find
that such attributes are indeed fairly correlated in OMNI3DOUT at 0.524, but significantly less in
OMNI3DIN at 0.006 correlation. Similarly, the correlation between relative 2D object size (
h · w)
and z is 0.543 and 0.102 respectively. This confirms our intuition that common assumptions in urban
scenes do not hold well in other domains, thus making the problem more challenging overall.

√

Category statistics. Fig. 2(c) plots the distribution of instances across the 97 categories of OMNI3D.
The long-tail suggests that low-shot object detection in both 2D and 3D will be critical for performance.
In this work, we want to focus on large-scale and diverse 3D objects, which is comparably unexplored.
We therefore filter and focus on categories that have at least 1000 annotations in 3D. This leaves
50 diverse categories including chair, sofa, laptop, table, cup, shoes, books, window, car, truck,
pedestrian and many more, with 19 common categories having more than 10k unique instances.

4 Method

Our goal is to design a simple and effective model for general 3D object detection. Hence, our
approach is free of domain or object specific priors. We design our 3D object detection framework as
an extension of Faster R-CNN with a 3D object head to predict a cuboid per each detected 2D object.
We refer to our method as Cube R-CNN. Figure 3 shows an overview of our approach.

4.1 Cube R-CNN

Our model builds on Faster R-CNN [61], an end-to-end region-based object detection framework.
Faster-RCNN consists of a backbone network, commonly a CNN, which embeds the input image into
a higher-dimensional feature space. A region proposal network (RPN) predicts regions of interest
(RoIs) representing object candidates in the image. A 2D box head inputs the backbone feature map
and processes each RoI to predict a category and a more accurate 2D bounding box. Faster R-CNN
can be easily extended to tackle more tasks by adding task-specific heads e.g. Mask R-CNN [24]
adds a mask head to additionally output object silhouettes.

For the task of 3D object detection, we extend Faster R-CNN by introducing a 3D detection head
which predicts a 3D cuboid for each detected 2D object. Cube R-CNN extends Faster R-CNN in three
ways: (a) we replace the binary classifier in RPN which predicts region objectness with a regressor
that predicts IoUness, (b) we introduce a cube head which estimates the parameters to define a 3D
cuboid for each detected object (similar in concept to [64]), and (c) we define a new training objective
which incorporates a virtual depth for the task of 3D object detection.

IoUness. The role of RPN is two-fold: (a) it proposes RoIs by regressing 2D box coordinates from
pre-computed anchors and (b) it classifies regions as object or not (objectness). This is sensible in
exhaustively labeled datasets where it can be reliably assessed if a given region contains an object.
However, OMNI3D combines many data sources with no guarantee that all instances of all classes
are labeled. We overcome this by replacing objectness with IoUness, applied only to foreground.
Similar to [32], we replace objectness in the RPN with a regressor to predict IoU between an RoI and
a ground truth. Let o be the predicted IoU for a RoI and ˆo be the 2D IoU between the region and its
ground truth; we apply a binary cross-entropy (CE) loss LIoUness = ℓCE(o, ˆo). We train on regions
whose IoU exceeds 0.05 with a ground truth in order to learn IoUness from a wide range of region
overlaps. Thus, the RPN training objective becomes LRPN = ˆo · (LIoUness + Lreg), where Lreg is the
2D box regression loss from [61]. The loss is weighted by ˆo to prefer candidates close to true objects.

Cube Head. We extend Faster R-CNN with a new head, called cube head, to predict a 3D cuboid for
each detected 2D object. The cube head inputs 7 × 7 feature maps pooled from the backbone for
each predicted region and feeds them to 2 fully-connected (FC) layers with 1024 hidden dimensions.
Similar to Faster R-CNN, all 3D estimations in the cube head are category-specific.

4

Figure 3: Cube R-CNN takes as input an RGB image, detects all objects in 2D and predicts their 3D
cuboids B3D. During training, the 3D cuboid corners are compared against a 3D ground truth.

The cube head represents a 3D cuboid with 13 parameters each predicted by a final FC layer:

• [u, v] represent the projected 3D center on the image plane relative to the 2D RoI
• z ∈ R+ is the object’s center depth in meters transformed from virtual depth zv (see below)
• [ ¯w, ¯h, ¯l] ∈ R3
+ are the log-normalized physical box dimensions in meters
• p ∈ R6 represents a continuous 6D [79] allocentric rotation
• µ ∈ R is the predicted 3D uncertainty

The above parameters form the final 3D box in camera view coordinates for each detected object.
The object’s 3D center X is estimated from the predicted 2D projected center [u, v] and depth z via

X(u, v, z) = (cid:0) z
fx

(rx + urw − px) , z
fy

(ry + vrh − py), z(cid:1)

(1)

where [rx, ry, rw, rh] is the object’s 2D box, (fx, fy) are the camera’s known focal lengths and
(px, py) the principal point. The 3D box dimensions d are derived from [ ¯w, ¯h, ¯l] which are log-
normalized with category-specific pre-computed means (w0, h0, l0) for width, height and length
respectively, which are combined then arranged into a diagonal scaling matrix via

d( ¯w, ¯h, ¯l) = diag (cid:0)exp( ¯w)w0, exp(¯h)h0, exp(¯l)l0
Finally, we derive the rotation R(p) of the object as a 3 × 3 rotation matrix based on a 6D parameter-
ization (2 directional vectors) of p following [79] which is converted from allocentric to egocentric
rotation similar to [36], defined formally in Appendix. The final 3D cuboid, defined by 8 corners, is
B3D(u, v, z, ¯w, ¯h, ¯l, p) = R(p) d( ¯w, ¯h, ¯l) Bunit + X(u, v, z)
where Bunit are the 8 corners of an axis-aligned unit cube centered at (0, 0, 0). Lastly, µ denotes a
learned 3D uncertainty, which is mapped to a confidence at inference then joined with the classification
score s from the 2D box head to form the final score for the prediction, as (cid:112)s · exp(−µ).

(2)

(3)

(cid:1)

Training objective. Our training objective consists of 2D losses from the RPN and 2D box head
and 3D losses from the cube head. The 2D losses follow [61] with a deviation for the RPN loss
which replaces objectness with IoUness, as described above. The cube head introduces the 3D losses.
We begin by comparing each predicted 3D cuboid with its matched ground truth via a chamfer
loss, treating the 8 box corners of the 3D boxes as point clouds, namely Lall
3D).
Note that Lall
3D entangles all 3D variables via the box predictor B3D (Eq. 3), such that that er-
rors in variables may be ambiguous from one another. Thus, following [64], we isolate each
variable group with separate disentangled losses. The disentangled loss for each variable group
substitutes all but its variables with the ground truth from Eq. 3 to create a pseudo box prediction.
For example, the disentangled loss for the projected 3D center [u, v] would produce a 3D box with all
but (u, v) replaced with the true values and then compare against the ground truth box, via

3D = ℓchamfer(B3D, Bgt

We use an L1 loss for L(u,v)
3D to account for cuboid symmetry
such that rotation matrices of Euler angles modulo π produce the same non-canonical 3D cuboid.

and chamfer for Lp

3D

L(u,v)
3D = ∥B3D(u, v, zgt, ¯wgt, ¯hgt, ¯lgt, pgt) − Bgt
3D , L(z)

3D and L( ¯w,¯h,¯l)

3D∥1

(4)

5

IoUness0.75Anchor2D GT2D HeadCube Head[𝑢,𝑣]𝑧[ഥ𝑤,തℎ,ҧ𝑙]𝒑𝜇ቊClass2DBoxቊ2DBoxIoUness𝐵3𝐷𝓁chamferBackbone Feature MapInput Image𝑿(𝑢,𝑣,𝑧)𝒘𝒉𝒍𝑹(𝒑)↺3D Object Detection OutputTop DownSide View3D GTFigure 4: Varying camera intrinsics exaggerate scale-depth ambiguity as the same object at two
different depths can project to the same image, as shown in (a) and (b). We address this by introducing
a virtual camera system which is invariant to intrinsics and transforms the object’s depth to a virtual
depth zv such that the effective image size Hv and focal length fv are consistent across images.

Losses in box coordinates have natural advantages over losses which directly compare variables to
ground truths in that their gradients are appropriately weighted by the error as shown in [64].

The collective training objective of Cube R-CNN is given by,
√

2 · exp(−µ) · (Lall

L = LRPN + L2D +

(5)
where LRPN is the RPN loss, described above, and L2D is the loss from the 2D box head from [61].
The 3D losses are weighted by the predicted 3D uncertainty (inspired by [49]), such that the model
may trade a penalty to reduce the 3D loss when uncertain. In practice, µ is helpful for both improving
the 3D rating of a cuboids at inference and reducing the loss of hard samples during training.

3D) + µ

3D + L(u,v)

3D + L(z)

+ Lp

3D + L( ¯w,¯h,¯l)

3D

4.2 Virtual Depth

A critical part of 3D object detection is predicting an object’s depth from the camera in metric units.
Estimating depth from only visual cues requires an implicit mapping of 2D pixels to 3D distances,
which is more ambiguous if camera intrinsics vary. Prior works are able to ignore this as they
primarily train on images from a single sensor. OMNI3D contains images from many sensors and
thus demonstrates large variations in camera intrinsics. We design Cube R-CNN to be robust to
intrinsics by predicting the object’s virtual depth zv, which projects the metric depth z to a virtual
invariant camera space. As depicted in Fig. 4, virtual depth scales the depth using the (known) camera
intrinsics such that the effective image size and focal length are consistent across the dataset.

Definition: Virtual depth is designed to maintain an effective image height Hv and focal length fv,
which are both hyperparameters we refer to as virtual height and virtual focal length, respectively.
Let z be the true metric depth of an object from an image with focal length f and image height H.
We define the object’s virtual depth as zv = z · fv
. The derivation can be found in the Appendix.
f

H
Hv

Invariance to camera intrinsics using virtual depth also enables scale augmentations during training,
since H can vary without harming the consistency of zv. Data augmentations from image resizing
tend to be critical for 2D models but are not commonly used in 3D methods [6, 47, 64] since if
unaccounted for they may increase scale-depth ambiguity. With virtual depth, such ambiguities are
lessened which therefore enables powerful data augmentation strategies in training. We justify the
effects of virtual depth, dealing with varying intrinsics and enabling scale augmentations, in Sec. 5.

5 Experiments

We evaluate single-image 3D object detection on OMNI3D. We show our method’s superiority using
a single model design by comparing to prior best models on existing benchmarks and OMNI3D.
Finally, we prove the effectiveness of OMNI3D as a large-scale 3D object detection benchmark by
showing comprehensive cross-dataset generalization and its impact for pre-training.

Implementation details. We implement Cube R-CNN using Detectron2 [73] and PyTorch3D [58].
We use DLA-34 [75] pretrained on ImageNet [63] with a FPN [40] as our backbone. We train for
128 epochs with a batch size of 192 images distributed across 48 V100s. We use SGD with a learning
rate of 0.12 which decays by a factor of 10 after 60% and 80% of training. During training, we use
random data augmentation of horizontal flipping and scales ∈ [0.50, 1.25], enabled by virtual depth.
Virtual parameters are fv = Hv = 512. All source code and models will be made publicly available.

6

World Camera(s)Virtual Camera(c) Image height 𝐻𝑣, focal length 𝑓𝑣, virtual depth:𝑧𝑣=𝑠∙𝑧∙𝑓𝑣∙𝐻𝑠∙𝑓∙𝐻𝑣=𝑧∙𝑓𝑣∙𝐻𝑓∙𝐻𝑣(a) Image height 𝐻, focal length𝑓, object’s depth: 𝑧ImageCamera Center(b) Image height 𝐻, focal lengths∙𝑓, object’s depth: s∙𝑧Figure 5: Cube R-CNN predictions on OMNI3D test. For each example, we show the input image,
the 3D predictions overlaid on the image and a top view. More examples are given in the Appendix.

Metric. Following 2D object detection, we define average-precision (AP) as our core 3D object
detection metric. Predictions are matched to ground truth by measuring their overlap using IoU3D
which computes the intersection-over-union of 3D cuboids, replacing IoU2D in 2D. We contribute
a novel, fast, and exact algorithm to compute IoU3D, described below. We follow COCO [42] and
compute a mean AP3D across all 50 categories in OMNI3D and over a range of IoU3D thresholds
τ ∈ [0.05, 0.10, . . . 0.50]. The range of τ is more relaxed than in 2D to account for the new dimension.
We note that general 3D objects may be occluded by other objects or truncated by the image border,
and can be arbitrarily small. Following [19], during evaluation we treat objects with high occlusion
or truncation (> 66%), and tiny objects (< 6.25% image height) as ignored. We also denote AP3D at
varying levels of depth d as near: 0 < d ≤ 10m, medium: 10m < d ≤ 35m, far: 35m < d ≤ ∞.

Fast IoU3D. IoU3D compares two 3D cuboids by computing their intersection-over-union. Images
usually have many ground truths and produce several predictions so IoU3D computations need to be
fast and accurate. Prior implementations [19, 66] approximate IoU3D by projecting 3D boxes on a
ground plane and multiply the top-view 2D intersection with the box heights to compute a 3D volume.
Such approximations become notably inaccurate when objects are not on a planar ground or have
arbitrary orientation (e.g. nonzero pitch or roll). Objectron [2] provides an unbatched exact solution,
but is reliant on external libraries [4, 69] implemented only in C++. We implement a new, fast and
exact algorithm which directly finds the intersecting shape by representing cuboids as triangular
meshes and finds face intersections. Our algorithm is self-contained, batched with C++ and CUDA
support. Our implementation is 90× faster than Objectron in C++, and 450× faster in CUDA. As a
result, evaluation on the large OMNI3D takes only a few seconds instead of several hours.

5.1 Model Performance

We first ablate the design choices of our Cube R-CNN. Then, we compare our approach to prior
state-of-the-art methods on existing benchmarks and OMNI3D and show that it performs superior or
on par with prior works which are generally specialized for their respective domains. Our method,
Cube R-CNN, uses a single unified design to tackle general 3D object detection across domains.
Figure 5 shows qualitative predictions on the OMNI3D test set.

Ablations. Table 1 ablates the features of our proposed Cube R-CNN on OMNI3D. We report AP3D
and additional metrics at single IoU thresholds (0.25 and 0.50), as well as performance for near,
medium and far objects. We further train and evaluate on domain-specific subsets, OMNI3DIN and
OMNI3DOUT (APIN
3D , respectively) to show how ablation trends hold for single domain
scenarios. From Table 1 we draw a few standout observations:

3D and APOUT

Virtual depth is effective and improves AP3D by +6%, most noticable in the full OMNI3D which
has the largest variances in camera intrinsics. Enabled by virtual depth, scale augmentations during
training increase AP3D by +3.1% on OMNI3D, +2.7% on OMNI3DIN and +3.8% on OMNI3DOUT.
We find scale augmentation controlled without virtual depth to be harmful by −2.5% (rows 6 - 7).

7

Cube R-CNN
w/o disentangled
w/o IoUness
w/o Lall
3D
w/o scale aug.
w/o scale | virtual
w/o virtual depth
w/o uncertainty µ
ours

AP3D AP25
24.3
22.7
23.6
22.2
21.8
20.2
21.5
20.2
21.2
19.8
18.4
17.3
18.3
17.2
24.9
23.3

3D AP50
9.3
8.8
6.4
8.0
7.5
4.4
5.4
9.5

3D APnear
27.9
26.4
26.4
23.5
23.4
22.7
20.5
27.9

3D APmed
11.3
11.1
12.1
9.8
8.6
7.9
10.8
12.1

3D APfar
8.1
8.3
7.4
6.8
5.7
7.1
7.0
8.5

3D APIN
14.7
14.3
13.8
12.3
12.2
13.2
9.1
15.0

3D APOUT
3D
31.5
31.0
29.1
28.1
26.0
30.3
25.4
31.9

Table 1: Cube R-CNN ablations. We report AP3D on OMNI3D, at IoU thresholds 0.25 and 0.50, and
for near, medium and far objects. We further report AP3D on OMNI3DIN and OMNI3DOUT subsets.

f
Method
✗
M3D-RPN [6]
✓
M3D-RPN [6]++
✓
GUPNet [49]
Cube R-CNN (ours) ✓

APKIT
3D
18.4
19.0
21.4
30.9

KITTI
3D AP70
APNU
11.4
0.8
9.3
10.7
14.4
8.5
14.7
12.7

nuScenes
3D APNU
car APKIT
18.9
2.2
19.6
10.6
23.1
17.3
33.0
20.2

3D APKIT
10.4
16.2
24.5
36.0

OMNI3DOUT
3D APNU
17.9
20.4
20.5
32.7

3D APOUT
3D
13.7
17.0
19.9
31.9

Table 2: We compare to GUPNet [49], M3D-RPN [6] and its extension with virtual depth and 6D
allocentric pose, M3D-RPN++. We report AP3D, KITTI AP70
car of [19, 64] for val1 on cars with IoU
threshold of 0.7, and zero-shot generalization. f denotes if a method handles variance in focal length.

3D uncertainty boosts performance by about +6% on all OMNI3D subsets. Intuitively, it serves as a
measure of confidence for the model’s 3D predictions, and removing it means that the model would
rely only on its 2D classification to score cuboids. If uncertainty is used only to scale samples in Eq. 5,
but not at inference then AP3D still improves but by +1.6%. Table 1 also shows that the entangled
Lall
3D loss from Eq. 5 boosts AP3D by +3.1% while disentangled losses contribute less (+0.6%).
Replacing objectness with IoUness in the RPN head improves AP3D on OMNI3D by +1.1%.

Comparison with other methods. We compare Cube R-CNN with state-of-the-art approaches. We
choose representative methods designed for the urban and indoor domains, and evaluate on our
proposed OMNI3D benchmark and single-dataset benchmarks, KITTI and SUN RGB-D.

Urban baselines. Table 2 compares Cube R-CNN to baselines M3D-RPN [6], a single-shot 3D anchor
approach, and GUPNet [49], which uses 2D-3D geometry and camera focal length to derive depth.
We train M3D-RPN and GUPNet using their publicly available code and report the best performance
across 3 runs. For a stronger comparison with [6], we extend it using our virtual depth and the
6D [79] allocentric pose, referred to as M3D-RPN++. We train and evaluate on three datasets: KITTI
(columns 3-5), nuScenes (columns 6-7) and OMNI3DOUT (columns 8-10) using our defined data
splits for trainval and test. For each dataset, we report performance on the test set as well as zero-shot
generalization to the other dataset (shown with blue). For OMNI3DOUT, we report performance on
its test set, APOUT
3D. All methods are trained with the same backbone
and training recipe on the 5 KITTI categories, 9 nuScenes categories and 11 OMNI3DOUT categories.
On KITTI, we additionally report the commonly used, but approximate, KITTI val1 metric [19, 64]
on the car category with the strict 0.7 IoU threshold, denoted as AP70
car.

3D , and its subparts, APKIT

3D and APNU

From Table 2 we observe that our Cube R-CNN outperforms competitive methods across all datasets
on our robust AP3D metric by a large margin and performs comparably on the more volatile AP70
car. We
observe that our method shows the strongest zero-shot generalization performance on both KITTI and
nuScenes. M3D-RPN fails to generalize without virtual depth and succeeds with it in M3D-RPN++,
which increases its zero-shot generalization to about 10% for either direction of KITTI ↔ nuScenes.
This shows that virtual depth is effective across methods. Lastly, training Cube R-CNN on the much
larger OMNI3DOUT greatly impacts its 3D performance, e.g. from 30.9% to 36.0% for KITTI.

Indoor baselines. Table 3 compares Cube R-CNN to Total3D [55], a state-of-the-art method on SUN
RGB-D. We use Total3D’s public model which requires 2D object boxes and categories as input.
For a fair comparison, we use ground truth 2D detections as input to both Total3D and our Cube
R-CNN, each trained using a ResNet34 [25] backbone. We compare Cube R-CNN from two respects,
firstly trained only on SUN RGB-D which is identical to Total3D’s setting, and secondly trained on
OMNI3DIN which subsumes SUN RGB-D. We report the mean IoU3D for 12 categories in Table 3.
Our model outperforms Total3D by +12.4% when trained on the same training set and increases the
performance gap to +13.6% when trained on the large-scale OMNI3DIN dataset.

8

Trained on

SUN RGB-D 33.6

Method
6.9
Total3D [55]
Cube R-CNN SUN RGB-D 49.5 10.4
50.1 10.2
Cube R-CNN

avg.
19.6
32.0
33.2
Table 3: Comparison to Total3D [55] on SUN RGB-D test. We use oracle 2D detections fairly for all
methods and report IoU3D . Cube R-CNN outperforms Total3D by +12.4% with equivalent training.

bed books cabinet chair counter desk
23.1
35.7
35.6

lamp pillow fridge sofa sink table
30.1 18.8 27.7
10.5
46.0 31.9 39.2
15.6
50.0 31.8 40.7
17.3

19.1
34.7
34.6

11.7
20.4
20.4

24.2
39.9
41.6

16.9
34.4
37.5

13.1
26.6
29.0

OMNI3DIN

Trained on
Hypersim
SUN
ARKit
OMNI3D
OMNI3DIN

APHYP
15.2
5.8
5.9
19.0
17.8

3D APAR
3D APSUN
3D
7.5
9.5
13.1
34.7
38.6
14.2
38.2
32.6
41.2
35.4

Trained on
KITTI
nuScenes
OMNI3D
OMNI3DOUT

APKIT
37.1
20.2
37.8
42.4

3D APNU
3D
12.7
38.6
35.8
39.0

Table 4: Cross-dataset performance.

Figure 6: Pre-training on OMNI3D- vs. ImageNet.

5.2 The Impact of the OMNI3D Dataset

Sec. 5.1 analyzes the performance of our Cube R-CNN and proves that we outperform prior methods.
Now, we turn to OMNI3D and its impact as a large-scale dataset. We show two use cases of OMNI3D:
(a) a universal 3D dataset which integrates smaller ones, and (b) a pre-training dataset.

OMNI3D as a universal dataset. We treat OMNI3D as a dataset which integrates smaller single
ones and show the impact of OMNI3D on each one. To this end, we train Cube R-CNN on OMNI3D
and compare to single-dataset training. Table 4 shows the performance for the indoor (top) and
urban (bottom) domain reported on the category intersection, 14 for indoor and 3 for outdoor, to
ensure AP3D comparability. Training on OMNI3D and its domain-specific subsets, OMNI3DIN and
OMNI3DOUT, results in higher performance compared to single-dataset training, signifying that our
large-scale OMNI3D generalizes better and should be preferred over single dataset training. Notably,
ARKit sees a +2.6% boost and KITTI +5.3%. Except for Hypersim, the domain-specific subsets
tend to perform better on their domain, which is not surprising given their distinct properties (Fig. 2).

OMNI3D as a pre-training dataset. Next, we demonstrate the utility of OMNI3D for pre-training.
In this setting, an unseen dataset is finetuned using a Cube R-CNN model pre-trained from OMNI3D.
The motivation is to determine how a large-scale 3D dataset could accelerate low-shot learning with
minimum need for costly 3D annotations on a new dataset. We choose SUN RGB-D and KITTI
as our unseen target datasets given their popularity and small size. We pre-train Cube R-CNN on
OMNI3D-, which removes the target datasets from OMNI3D, and then subsequently finetune the
models using percentages of their available training data. The performance curves in Fig. 6 show the
model quickly gains its final performance even at a small fraction of the training data when pretrained
on OMNI3D- compared to ImageNet. When pre-trained on OMNI3D- a model finetuned from only
5% of either target achieves more than 70% of their upper-bound performance.

6 Conclusion

We revisit the task of image-based 3D object detection by proposing a large benchmark, OMNI3D,
with many categories from diverse domains and with varying camera intrinsics. For OMNI3D, we
propose a simple yet effective model, Cube R-CNN, and an exact implementation for IoU for fast
evaluation. Future work on this new and exciting benchmark includes few-shot learning on the 47
categories of OMNI3D with few annotations which we do not tackle. Our quantitative analysis shows
that 3D object detection is challenging, especially for far away objects. Ethics: We explore technical
advances for visual 3D recognition, which on the surface are neutral from an ethics perspective. We
acknowledge that the used public datasets have a pedestrian category which has ethical consideration,
but are otherwise far more dominated by general inanimate objects, chairs, lamps, cars, books etc.
However, we remain committed to be wary of any harmful consequences stemming from our work.

9

OMNI3D

OMNI3DIN OMNI3DOUT SUN RGB-D

ARKit

Hypersim

Objectron

KITTI

nuScenes

COCO

LVIS

Appendix

Figure 7: We show the distribution of object centers in normalized image coordinates projected onto
the XY-plane (top) and normalized depth projected onto the topview XZ-plane (bottom) across each
subset of OMNI3D, individual 3D datasets KITTI [19], SUN RGB-D [66]), nuScenes [8], ARKit [5],
and Hypersim [62], and large-scale 2D datasets COCO [42] and LVIS [22].

A1 Dataset Details

In this section, we give more details of OMNI3D, its sources and subsets. We will release the
OMNI3D annotations along with our models.

Sources. For each individual dataset used in OMNI3D, we use their official train, val, and test set for
any datasets which have all annotations released. If no public test set is available then we use their
validation set as our test set. Whenever necessary we further split the remaining training set in 10:1
ratio by sequences in order form train and val sets. The resultant image make up of OMNI3D is:

• KITTI [19] has 7481 images (3321 train, 391 val, 3769 test).
• SUN RGB-D [66] has 10335 images (4929 train, 356 val, 5050 test).
• nuScenes [8] has 34149 images (26215 train, 1915 val, 6019 test).
• Objectron [2] has 46644 images (33519 train, 3811 val, and 9314 test).
• ARKitScenes [5] has 60924 images (48046 train, 5268 val, and 7610 test).
• Hypersim [62] has 74619 images (59543 train, 7386 val, and 7690 test).

Coordinate system. We define our unified 3D coordinate system for all labels with the camera center
being the origin and +x facing right, +y facing down, +z inward [19]. Object pose is relative to an
initial object with its bottom-face normal aligned to +y and its front-face aligned to +x (e.g. upright
and facing to the right). All images have known camera calibration matrices with input resolutions
varying from 370 to 1920 and diverse focal lengths from 518 to 1708 in pixels. Each object label
contains a category label, a 2D bounding box, a 3D centroid in camera space meters, a 3 × 3 matrix
defining the object to camera rotation, and the physical dimensions (width, height, length) in meters.

Spatial Statistics Following Section 3, we provide the spatial statistics for the individual data
sources of KITTI [19], SUN RGB-D [66]), nuScenes [8], ARKit [5], and Hypersim [62], as well as
OMNI3DIN and OMNI3DOUT in Figure 7. As in the main paper we observe that the indoor domain
data, with the exception of Hypersim, have bias for close objects. Moreover, outdoor data tend to be
spatially biased with projected centroids along diagonal ground planes while indoor is more central.

A2 Model Details

In this section, we provide additional details on our Cube R-CNN model pertaining to its 3D bounding
box allocentric rotation (of Section 4.1) and the derivation of virtual depth (of Section 4.2).

A2.1

3D Box Rotation

Our 3D bounding box object rotation is predicted in the form of a 6D continuous parameter, which is
shown to be well suited for neural networks to regress compared to other forms of rotation in [79].

10

Normalized widthNorm. heightNorm. depthN/AN/AFigure 8: We show egocentric and allocentric representations under constant rotations. Despite being
identical rotations, the egocentric representation appears visually different as its spatial location
changes, unlike allocentric which maintains a consistent appearance at all spatial locations.

Let our predicted rotation p be split into two directional vectors p1−2 ∈ R3 and r1−3 ∈ R3
be the columns of a 3 × 3 rotational matrix Ra. Then p is mapped to Ra using r1 = norm(p1),
r2 = norm(p2 −(r1 ·p2)r1), and r3 = r1 ×r2, where ·, × denote dot and cross product respectively.

, v−py
fy

Ra is estimated in allocentric form similar to [36]. Let fx, fy, px, py be the known camera intrinsics,
u, v the predicted 2D projected center as in Section 4.1, and a = [0, 0, 1] be the camera’s principal axis.
Then o = norm([ u−px
, 1]) is a ray pointing from the camera to u, v with angle α = acos(o).
fx
Using standard practices of axis angle representations with an axis denoted as o × a and angle α, we
compute a matrix M ∈ R3×3, which helps form the final egocentric rotation matrix R = M · Ra.
We provide examples of 3D bounding boxes at constant egocentric or allocentric rotations in Figure 8.
The allocentric rotation is more aligned to the visual 2D evidence, whereas egocentric rotation
entangles relative position into the prediction. In other words, identical egocentric rotations may look
very different when viewed from varying spatial locations, which is not true for allocentric.

A2.2 Virtual Depth

In Section 4.2, we propose a virtual depth transformation in order to help our Cube R-CNN model
handle varying input image resolutions and camera intrinsics. The motivation of estimating a virtual
depth instead of metric depth is to keep the effective image size and focal length consistent in an
invariant camera space. Doing so enables two camera systems with nearly the same visual evidence
of an object to transform into the same virtual depth as shown in Figure 4 (Main paper). Next we
provide a simple proof for the conversion between virtual and metric depth.

Proof: Assume a 3D point (X, Y, Z) projected to (x, y) on an image with height H and focal length f .
The virtual 3D point (X, Y, Zv) is projected to (xv, yv) on the virtual image. The 2D points (x, y)
and (xv, yv) correspond to the same pixel location in both the original and virtual image. In other
words, yv = y · Hv
+pyv , where
py is the principal point and pyv = py · Hv
H
.
Hv

Z +py and yv = fv · Y
Zv
H ⇒ Zv = Z · fv
Hv

H . Recall the formula for projection as y = f · Y

H . By substitution fv · Y
Zv

= f · Y
Z

f

A2.3 Model Efficiency

When training on subsets smaller than OMNI3D, we adjust the learning rate, batch size, and number
of iterations linearly until we can train for 128 epochs between 96k to 116k iterations. Cube R-CNN
trains on V100 GPUs between 14 and 26 hours depending on the subset configuration when scaled to
multi-node distributed training, and while training uses approximately 1.6 GB memory per image.
Inference on a Cube R-CNN model processes image from KITTI [19] with a wide input resolution of
512×1696 at 52ms/image on average while taking up 1.3 GB memory on a Quadro GP100 GPU.
Computed with an identical environment, our model efficiency is favorable to M3D-RPN [6] and
GUPNet [49] which infer from KITTI images at 191ms and 66ms on average, respectively.

A3 Evaluation

In this section we give additional context and justification for our chosen τ which defines the range
of 3D IoU that AP3D is averaged over. We further provide details on our implementation of 3D IoU.

11

Constant 0°(aligned Z-axis)Constant 45°(about Y-axis)Figure 9: We slowly translate a 3D box (purple) backwards relative to its ground-truth (green), hence
simulating error in z up to 1 unit length (¯lgt
3D = 1). We plot 2D and 3D IoU vs z error in (a) and
visualize selected critical points showing their 2D projections, 3D front view and a novel top view.
Unsurprisingly, we find the drop in IoU3D is much steeper than 2D. This effect highlights both the
challenge of 3D object detection and helps justify the relaxed τ thresholds used for AP3D in Sec. 5.

Data: Two 3D boxes b1 and b2
Result: Intersecting shape S = []
Step 1: For each 3D triangular face e ∈ b1 we check wether e falls inside b2
Step 2: If e is not inside, then we discard it
Step 3: If e is inside, then S = S + [e]. If e is partially inside, then the part of e inside b2, call it
ˆe, is added to S, S = S + [ˆe]
Step 4: We repeat steps 1 - 3 for b2
Step 5: We check and remove duplicates in S (in the case of coplanar sides in b1 and b2)
Step 6: We compute the volume of S, which is guaranteed to be convex

Algorithm 1: An high-level overview of our fast and exact IoU3D algorithm.

A3.1 Thresholds τ for AP3D

We highlight the relationship between 2D and 3D IoU in Figure 9. We do so by contriving a general
example between a ground-truth and a predicted box, both of which share rotation and rectangular
cuboid dimensions (0.5 × 0.5 × 1.0). We then translate the 3D box along the z-axis up to 1 meter (its
unit length), simulating small to large errors in depth estimation. As shown in the left of Figure 9, the
3D IoU drops off significantly quicker than 2D IoU does between the projected 2D bounding boxes.
As visualized in right of Figure 9, a moderate score of 0.63 IoU2D may result in a low 0.01 IoU3D.
Despite visually appearing to be well localized in the front view, the top view helps reveal the error.
Since depth is a key error source for 3D, we find the relaxed settings of τ (Sec. 5) to be reasonable.

A3.2 IoU3D Details

We implement a fast IoU3D implementation. We provide more details for our algorithm here. Our
algorithm starts from the simple observation that the intersection of two oriented 3D boxes, b1 and b2,
is a convex polyhedron with n > 2 comprised of connected planar units. In 3D, these planar units are
3D triangular faces. Critically, each planar unit belongs strictly to either b1 or b2. Our algorithm finds
these units by iterating through the sides of each box, as described in Algorithm 1.

A4 Qualitative Examples

We provide more examples of 3D object detection from Cube R-CNN in Figure 10 from OMNI3D test.
In Figure 11, we demonstrate generalization for interesting scenes in the wild from COCO [42]
images. When projecting on images with unknown camera intrinsics, as is the case for COCO, we
visualize with intrinsics of f = 2 · H, px = 1
2 H, where H × W is the input image
resolution. As shown in Fig. 11, this appears to result in fairly stable generalization for indoor and
more common failure cases concerning unseen object or camera poses in outdoor.

2 W, py = 1

12

(a) IoUsvs Example Z Errors(b) IoUsat Critical PointsFigure 10: Cube R-CNN on OMNI3D test. We show the input image, the 3D predictions overlaid
on the image and a top view. Rows contains samples from datasets in order of SUN RGB-D [66],
ARKit [5], Hypersim [62] (Rows 3−4), Objectron [2], nuScenes [8], and KITTI [19] (Rows 7−8).

13

Figure 11: Cube R-CNN on COCO in the wild images. We select interesting scenes and observe that
generalization tends to perform well for indoor scenes (Rows 1-3) compared to outdoor (Rows 4-6),
which appear to fail when in the wild objects or cameras have high variation in unseen poses.

14

References

[1] Waymo open dataset: An autonomous driving dataset (2019)

[2] Ahmadyan, A., Zhang, L., Ablavatski, A., Wei, J., Grundmann, M.: Objectron: A large scale

dataset of object-centric videos in the wild with pose annotations. CVPR (2021)

[3] Bao, W., Xu, B., Chen, Z.: MonoFENet: Monocular 3D object detection with feature enhance-

ment networks. TIP 29, 2753–2765 (2019)

[4] Barber, C.B., Dobkin, D.P., Huhdanpaa, H.: The quickhull algorithm for convex hulls. ACM

Transactions on Mathematical Software (TOMS) 22(4), 469–483 (1996)

[5] Baruch, G., Chen, Z., Dehghan, A., Dimry, T., Feigin, Y., Fu, P., Gebauer, T., Joffe, B., Kurz,
D., Schwartz, A., Shulman, E.: ARKitscenes - a diverse real-world dataset for 3d indoor scene
understanding using mobile RGB-d data. In: NeurIPS Datasets and Benchmarks Track (Round
1) (2021)

[6] Brazil, G., Liu, X.: M3D-RPN: Monocular 3D region proposal network for object detection. In:

ICCV (2019)

[7] Brazil, G., Pons-Moll, G., Liu, X., Schiele, B.: Kinematic 3D object detection in monocular

video. In: ECCV. pp. 135–152. Springer (2020)

[8] Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A., Pan, Y., Baldan,
G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous driving. In: CVPR (2020)

[9] Chang, M.F., Lambert, J., Sangkloy, P., Singh, J., Bak, S., Hartnett, A., Wang, D., Carr, P.,
Lucey, S., Ramanan, D., et al.: Argoverse: 3d tracking and forecasting with rich maps. In:
CVPR. pp. 8748–8757 (2019)

[10] Chen, H., Huang, Y., Tian, W., Gao, Z., Xiong, L.: Monorun: Monocular 3D object detection

by reconstruction and uncertainty propagation. In: CVPR. pp. 10379–10388 (2021)

[11] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., Zhang,
Z., Cheng, D., Zhu, C., Cheng, T., Zhao, Q., Li, B., Lu, X., Zhu, R., Wu, Y., Dai, J., Wang, J.,
Shi, J., Ouyang, W., Loy, C.C., Lin, D.: MMDetection: Open mmlab detection toolbox and
benchmark. arXiv preprint arXiv:1906.07155 (2019)

[12] Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., Urtasun, R.: Monocular 3D object detection

for autonomous driving. In: CVPR (2016)

[13] Chen, Y.N., Dai, H., Ding, Y.: Pseudo-stereo for monocular 3D object detection in autonomous

driving. In: CVPR (2022)

[14] Chen, Y., Tai, L., Sun, K., Li, M.: Monopair: Monocular 3D object detection using pairwise
spatial relationships. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 12093–12102 (2020)

[15] Dasgupta, S., Fang, K., Chen, K., Savarese, S.: Delay: Robust spatial layout estimation for

cluttered indoor scenes. In: CVPR (2016)

[16] Ding, M., Huo, Y., Yi, H., Wang, Z., Shi, J., Lu, Z., Luo, P.: Learning depth-guided convolutions

for monocular 3D object detection. In: CVPRW. pp. 1000–1001 (2020)

[17] Fu, H., Gong, M., Wang, C., Batmanghelich, K., Tao, D.: Deep Ordinal Regression Network

for Monocular Depth Estimation. In: CVPR) (2018)

[18] Gählert, N., Jourdan, N., Cordts, M., Franke, U., Denzler, J.: Cityscapes 3D: Dataset and

benchmark for 9 DOF vehicle detection. In: CVPRW (2020)

[19] Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision

benchmark suite. In: CVPR (2012)

[20] Gkioxari, G., Malik, J., Johnson, J.: Mesh R-CNN. In: ICCV (2019)

15

[21] Gu, J., Wu, B., Fan, L., Huang, J., Cao, S., Xiang, Z., Hua, X.S.: Homography loss for

monocular 3d object detection. In: CVPR (2022)

[22] Gupta, A., Dollar, P., Girshick, R.: LVIS: A dataset for large vocabulary instance segmentation.

In: CVPR (2019)

[23] He, C., Huang, J., Hua, X.S., Zhang, L.: Aug3d-rpn: Improving monocular 3d object detection

by synthetic images with virtual depth. arXiv preprint arXiv:2107.13269 (2021)

[24] He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In: ICCV (2017)

[25] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR

(2016)

[26] Hedau, V., Hoiem, D., Forsyth, D.: Recovering the spatial layout of cluttered rooms. In: ICCV

(2009)

[27] Houston, J., Zuidhof, G., Bergamini, L., Ye, Y., Chen, L., Jain, A., Omari, S., Iglovikov, V.,
Ondruska, P.: One thousand and one hours: Self-driving motion prediction dataset. arXiv
preprint arXiv:2006.14480 (2020)

[28] Huang, K.C., Wu, T.H., Su, H.T., Hsu, W.H.: Monodtr: Monocular 3D object detection with

depth-aware transformer. In: CVPR (2022)

[29] Huang, S., Qi, S., Xiao, Y., Zhu, Y., Wu, Y.N., Zhu, S.C.: Cooperative holistic scene under-
standing: Unifying 3D object, layout, and camera pose estimation. In: NeurIPS (2018)

[30] Huang, X., Wang, P., Cheng, X., Zhou, D., Geng, Q., Yang, R.: The apolloscape open dataset

for autonomous driving and its application. PAMI 42(10), 2702–2719 (2019)

[31] Kesten, R., Usman, M., Houston, J., Pandya, T., Nadhamuni, K., Ferreira, A., Yuan, M., Low,
B., Jain, A., Ondruska, P., Omari, S., Shah, S., Kulkarni, A., Kazakova, A., Tao, C., Platinsky,
L., Jiang, W., Shet, V.: Level 5 perception dataset 2020. https://level-5.global/level5/
data/ (2019)

[32] Kim, D., Lin, T.Y., Angelova, A., Kweon, I.S., Kuo, W.: Learning open-world object proposals

without learning to classify. IEEE Robotics and Automation Letters 7(2), 5453–5460 (2022)

[33] Ku, J., Mozifian, M., Lee, J., Harakeh, A., Waslander, S.L.: Joint 3D proposal generation and

object detection from view aggregation. In: IROS. pp. 1–8. IEEE (2018)

[34] Kulkarni, N., Misra, I., Tulsiani, S., Gupta, A.: 3d-relnet: Joint object and relational network

for 3d prediction. In: ICCV (2019)

[35] Kumar, A., Brazil, G., Liu, X.: Groomed-nms: Grouped mathematically differentiable nms for

monocular 3D object detection. In: CVPR. pp. 8973–8983 (2021)

[36] Kundu, A., Li, Y., Rehg, J.M.: 3d-rcnn: Instance-level 3d object reconstruction via render-and-

compare. In: CVPR. pp. 3559–3568 (2018)

[37] Law, H., Deng, J.: Cornernet: Detecting objects as paired keypoints. In: ECCV. pp. 734–750

(2018)

[38] Lee, D.C., Hebert, M., Kanade, T.: Geometric reasoning for single image structure recovery. In:

CVPR (2009)

[39] Li, P., Zhao, H., Liu, P., Cao, F.: RTM3D: Real-time monocular 3D detection from object

keypoints for autonomous driving. In: ECCV. pp. 644–660. Springer (2020)

[40] Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks

for object detection. In: CVPR (2017)

[41] Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P.: Focal loss for dense object detection. In:

ICCV (2017)

16

[42] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.:

Microsoft COCO: Common objects in context. In: ECCV (2014)

[43] Liu, C., Gu, S., Van Gool, L., Timofte, R.: Deep line encoding for monocular 3D object

detection and depth prediction. In: BMVC 2021. p. 354 (2021)

[44] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: Ssd: Single shot

multibox detector. In: ECCV (2016)

[45] Liu, X., Xue, N., Wu, T.: Learning auxiliary monocular contexts helps monocular 3d object

detection. In: AAAI (2022)

[46] Liu, Y., Yixuan, Y., Liu, M.: Ground-aware monocular D object detection for autonomous

driving. RA-L 6(2), 919–926 (2021)

[47] Liu, Z., Wu, Z., Tóth, R.: Smoke: Single-stage monocular 3D object detection via keypoint

estimation. In: CVPR W (2020)

[48] Liu, Z., Zhou, D., Lu, F., Fang, J., Zhang, L.: Autoshape: Real-time shape-aware monocular 3D

object detection. In: ICCV. pp. 15641–15650 (2021)

[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty
projection network for monocular 3D object detection. In: ICCV. pp. 3111–3121 (2021)

[50] Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., Ouyang, W.: Rethinking pseudo-lidar representa-

tion. In: ECCV (2020)

[51] Ma, X., Wang, Z., Li, H., Zhang, P., Ouyang, W., Fan, X.: Accurate monocular 3D object
detection via color-embedded 3D reconstruction for autonomous driving. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 6851–6860 (2019)

[52] Ma, X., Zhang, Y., Xu, D., Zhou, D., Yi, S., Li, H., Ouyang, W.: Delving into localization errors

for monocular 3D object detection. In: CVPR (June 2021)

[53] Mallya, A., Lazebnik, S.: Learning informative edge maps for indoor scene layout prediction.

In: ICCV (2015)

[54] Mousavian, A., Anguelov, D., Flynn, J., Kosecka, J.: 3D bounding box estimation using deep

learning and geometry. In: CVPR (2017)

[55] Nie, Y., Han, X., Guo, S., Zheng, Y., Chang, J., Zhang, J.J.: Total3DUnderstanding: Joint layout,
object pose and mesh reconstruction for indoor scenes from a single image. In: CVPR (2020)

[56] Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is pseudo-lidar needed for monocular 3D

object detection? In: ICCV. pp. 3142–3152 (2021)

[57] Qi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J.: Frustum pointnets for 3D object detection from

rgb-d data. In: CVPRn (2018)

[58] Ravi, N., Reizenstein, J., Novotny, D., Gordon, T., Lo, W.Y., Johnson, J., Gkioxari, G.: Acceler-

ating 3D deep learning with PyTorch3D. arXiv:2007.08501 (2020)

[59] Reading, C., Harakeh, A., Chae, J., Waslander, S.L.: Categorical depth distribution network for

monocular 3D object detection. In: CVPR. pp. 8555–8564 (2021)

[60] Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time

object detection. In: CVPR (2016)

[61] Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with

region proposal networks. In: NeurIPS (2015)

[62] Roberts, M., Ramapuram, J., Ranjan, A., Kumar, A., Bautista, M.A., Paczan, N., Webb,
R., Susskind, J.M.: Hypersim: A photorealistic synthetic dataset for holistic indoor scene
understanding. In: ICCV (2021)

17

[63] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A.,
Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition
Challenge. IJCV (2015)

[64] Simonelli, A., Bulo, S.R., Porzi, L., López-Antequera, M., Kontschieder, P.: Disentangling

monocular 3D object detection. In: ICCV (2019)

[65] Simonelli, A., Bulo, S.R., Porzi, L., Ricci, E., Kontschieder, P.: Towards generalization across

depth for monocular 3D object detection. In: ECCV (2020)

[66] Song, S., Lichtenberg, S.P., Xiao, J.: Sun rgb-d: A rgb-d scene understanding benchmark suite.

In: CVPR (2015)

[67] Tian, Z., Shen, C., Chen, H., He, T.: FCOS: Fully convolutional one-stage object detection. In:

ICCV. pp. 9627–9636 (2019)

[68] Tulsiani, S., Gupta, S., Fouhey, D., Malik, A.A.E.J.: Factoring shape, pose, and layout from the

2d image of a 3d scene. In: CVPR (2018)

[69] Virtanen, P., Gommers, R., Oliphant, T.E., Haberland, M., Reddy, T., Cournapeau, D., Burovski,
E., Peterson, P., Weckesser, W., Bright, J., et al.: Scipy 1.0: fundamental algorithms for scientific
computing in python. Nature methods 17(3), 261–272 (2020)

[70] Wang, L., Du, L., Ye, X., Fu, Y., Guo, G., Xue, X., Feng, J., Zhang, L.: Depth-conditioned
dynamic message propagation for monocular 3d object detection. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 454–463 (2021)

[71] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate

transforms for monocular 3D object detection. In: NeurIPS. vol. 34 (2021)

[72] Wang, Y., Chao, W.L., Garg, D., Hariharan, B., Campbell, M., Weinberger, K.Q.: Pseudo-lidar
from visual depth estimation: Bridging the gap in 3D object detection for autonomous driving.
In: CVPR. pp. 8445–8453 (2019)

[73] Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https://github.com/

facebookresearch/detectron2 (2019)

[74] You, Y., Wang, Y., Chao, W.L., Garg, D., Pleiss, G., Hariharan, B., Campbell, M., Weinberger,
K.Q.: Pseudo-lidar++: Accurate depth for 3D object detection in autonomous driving. In: ICLR
(2020)

[75] Yu, F., Wang, D., Shelhamer, E., Darrell, T.: Deep layer aggregation. In: CVPR. pp. 2403–2412

(2018)

[76] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3D object detection. In:

CVPR. pp. 3289–3298 (2021)

[77] Zhou, D., Song, X., Dai, Y., Yin, J., Lu, F., Liao, M., Fang, J., Zhang, L.: Iafa: Instance-aware

feature aggregation for 3D object detection from a single image. In: ACCV (2020)

[78] Zhou, X., Wang, D., Krähenbühl, P.: Objects as points. In: arXiv preprint arXiv:1904.07850

(2019)

[79] Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H.: On the continuity of rotation representations in

neural networks. In: CVPR (2019)

[80] Zhou, Y., He, Y., Zhu, H., Wang, C., Li, H., Jiang, Q.: Monocular 3D object detection: An

extrinsic parameter free approach. In: CVPR. pp. 7556–7566 (2021)

[81] Zou, Z., Ye, X., Du, L., Cheng, X., Tan, X., Zhang, L., Feng, J., Xue, X., Ding, E.: The devil
is in the task: Exploiting reciprocal appearance-localization features for monocular 3D object
detection. In: ICCV. pp. 2713–2722 (2021)

18

