Object Detection in the Context of Mobile Augmented Reality

Xiang Li*

Yuan Tian‚Ä†

Fuyao Zhang‚Ä°

Shuxue Quan¬ß

Yi Xu¬∂

OPPO US Research Center

0
2
0
2

g
u
A
5
1

]

V
C
.
s
c
[

1
v
5
5
6
6
0
.
8
0
0
2
:
v
i
X
r
a

Figure 1: These images show the results of different object detection methods applied on the same image: (a) results of original
DNN (SSD Mobilenet model), (b) DNN with image orientation correction, (c) DNN with image correction and scale-based Ô¨Åltering, (d)
DNN with image correction and online semantic mapping, and (e) results of our proposed method.

ABSTRACT

In the past few years, numerous Deep Neural Network (DNN) mod-
els and frameworks have been developed to tackle the problem of
real-time object detection from RGB images. Ordinary object de-
tection approaches process information from the images only, and
they are oblivious to the camera pose with regard to the environ-
ment and the scale of the environment. On the other hand, mobile
Augmented Reality (AR) frameworks can continuously track a cam-
era‚Äôs pose within the scene and can estimate the correct scale of
the environment by using Visual-Inertial Odometry (VIO). In this
paper, we propose a novel approach that combines the geometric
information from VIO with semantic information from object de-
tectors to improve the performance of object detection on mobile
devices. Our approach includes three components: (1) an image
orientation correction method, (2) a scale-based Ô¨Åltering approach,
and (3) an online semantic map. Each component takes advantage
of the different characteristics of the VIO-based AR framework.
We implemented the AR-enhanced features using ARCore and the
SSD Mobilenet model [26] on Android phones. To validate our
approach, we manually labeled objects in image sequences taken
from 12 room-scale AR sessions. The results show that our approach
can improve on the accuracy of generic object detectors by 12% on
our dataset.

Index Terms:
Computer Graphics‚ÄîGraphics systems and
interfaces‚ÄîMixed / Augmented Reality; ArtiÔ¨Åcial intelligence‚Äî
Computer vision‚ÄîComputer vision problems‚ÄîObject detection;

1 INTRODUCTION

Object detection plays an important role in Augmented Reality (AR),
where virtual content can be associated with real-world objects to en-
able applications such as product recommendations for e-commerce,

*e-mail: xiang.li@oppo.com
‚Ä†e-mail: yuan.tian@oppo.com
‚Ä°email: fuyao.zhang@oppo.com
¬ßemail: shuxue.quan@oppo.com
¬∂e-mail: yi.xu@oppo.com

and textual and graphical informational displays for educational pur-
poses. In addition, the semantic information obtained by an object
detector can be used to help VIO. Recently, tremendous progress has
been made on object detection using Deep Neural Networks (DNNs)
including SSD [26], YOLO [30], faster R-CNN [32], etc. With
advancements in mobile computing power and lightweight DNN
frameworks designed for mobile platforms (e.g., TensorFlow Lite),
more and more object detection DNNs can run on smartphones in
real-time [16, 43]. However, most of these general-purpose object
detectors are not designed speciÔ¨Åcally for AR applications. First,
most of them are trained with the objects oriented upright in the
images, which presents challenges when the camera is rotated as
seen in Fig. 1(a). This type of camera rotation frequently occurs
when a user views virtual content during an AR session. For exam-
ple, if a virtual object is large and wide, the user has a tendency to
rotate the phone from a portrait to landscape orientation. Second,
object detectors are oblivious to scale information due to the scale
ambiguity of a single input image. Third, during an AR session,
an object may be viewed from many different viewpoints. Since
most object detectors do not memorize speciÔ¨Åc locations that the
device has visited before, this can potentially result in inconsistent
detection over a large range of viewpoints.

Although data augmentation and data synthesis can be used to
ease the above-mentioned problems of scale ambiguity and view-
point variance, it requires more training data and time. Another
approach is to use the additional geometry information provided by
a depth camera [42]. However, depth cameras on mobile devices
are not widely available. Video-based methods can also alleviate
the viewpoint variance problem (e.g. [12, 24]), but they are either
limited to temporally-close frames or rely on intricate network de-
signs. Our key insight is that we leverage the information generated
by an AR framework, especially by a VIO method, to solve the
aforementioned problems and improve the performance of object
detectors in the context of mobile AR applications.

Mobile AR frameworks have become widely available for appli-
cation developers, including Apple Inc.s ARKit and Google Inc.s
ARCore. These AR frameworks use VIO to track a six degrees of
freedom (6DoF) camera pose continuously during an AR session.
These frameworks also provide sparse 3D points with real-world
scales. In addition, AR frameworks have the ability to memorize
places by tracking the camera within a map. Even when tracking
is lost, re-localization can be used to re-estimate the camera pose

airplaneairplaneairplaneairplaneairplanelaptopchairchair(a)(b)(c)(d)(e)laptopchairchairairplanechairchairchairchair 
 
 
 
 
 
with respect to the map. Intermittent re-localization is typically
performed to detect revisited places in order to mitigate the drift
accumulated in the camera trajectory over time.

With recent advancements in mobile computing power, both AR
frameworks and object detection DNNs can run simultaneously on
a mobile device using a single camera as the input source. In this
paper, a new object detection pipeline is proposed in the context
of mobile AR. This pipeline takes advantage of the output and
features of AR frameworks to increase the overall object detection
accuracy of DNNs. SpeciÔ¨Åcally, we use camera pose information
from VIO to correct image orientation, use scale information to reject
false positives, and build an online semantic map to fuse detection
results from various viewpoints and distances on AR-capable mobile
devices. We show that our approach can improve the accuracy of
off-the-shelf object detection DNN models.

2 RELATED WORK

In this section, we discuss related work on object detection, including
object detection methods that handle scale and viewpoint variance,
as well as SLAM methods that utilize semantic information. Finally,
we show examples of how VIO output can beneÔ¨Åt other computer
vision algorithms.

2.1 Object Detection

Generic object detection is one of the most fundamental computer
vision problems. It provides a semantic understanding of the real
world and is related to many applications. DNN-based object detec-
tion can be separated into two main types. The Ô¨Årst type is region
proposal followed by classiÔ¨Åcation, such as Fast R-CNN [10], Faster
R-CNN [32], R-FCN [7], FPN [22] and Mask R-CNN [13]. The
second type uses DNNs as direct regressors to produce the Ô¨Ånal
categories and locations simultaneously. Examples of this second
method include MultiBox [8], YOLO [30], SSD [26], YOLOv2 [31],
DSSD [9], etc. R-CNN [11] was the Ô¨Årst work that applied CNN-
based feature extraction in the traditional object detection pipeline.
Later, it was improved by adding spatial pyramid matching [14],
by introducing multi-task loss on classiÔ¨Åcation and bounding box
regression [10], by adding a Region Proposal Network (RPN) [32],
and by adapting to a fully-convolutional architecture [7]. Mask
R-CNN [13] adds an additional branch to predict the segmentation
mask to enable instance segmentation. Many researchers have also
investigated end-to-end frameworks for object detection. Redmon
et al. proposed YOLO (you only look once) [30] which uses the
same feature map to predict both the bounding boxes and the conÔ¨Å-
dence scores for categories. Another important work is SSD [26],
which uses anchor boxes with different aspect ratios and scales
instead of Ô¨Åxed grids. YOLO and SSD are two very popular frame-
works and have continued to evolve in recent years [9, 21, 31, 43].
Recently, there has been research on 6D detection from RGB im-
ages [37, 39, 44]. This method can predict the pose of 3D objects
from images but requires a lot of training data to do so. On mobile
devices, Ahmadyan et al. [1] proposed a system that tracks the pose
of an object in real-time for AR applications. Similar to early at-
tempts at template-based object detection in AR [40] and map-based
relocalization [25], this 6D detection approach relies on a known
category and/or geometry. Based on the tracking results, AR effects
can be superimposed onto certain objects. Our work differs from
previous methods in that we combine generic object detection with
SLAM, so no prior knowledge of the scene or object is required.

2.2 Scale Invariance and Consistency

Although current object detection methods can achieve promising
results on public datasets, there are still many challenges such as
scale invariance, detection under large viewpoint changes, and de-
tection consistency over time. To achieve scale invariance, image
pyramids are used to extract multi-scale features [14]. However, this

requires a signiÔ¨Åcant amount of additional training time and higher
memory consumption. As a result, some DNNs only use pyramids
in the testing phase [10, 32]. Feature hierarchy inside the DNNs
has also been investigated for scale invariance [26]. Multi-scale
CNNs [5] generate multiple output layers so that receptive Ô¨Åelds
match objects of different scales. Similarly, TridentNet [20] gen-
erates scale-speciÔ¨Åc feature maps with a uniform representational
power. HyperNet [18] aggregates feature maps from different reso-
lutions into a uniform space. FPN [22] proposes a bottom-up and
top-down architecture for the feature hierarchy. YOLO v2 [31] and
SNIPER [35] also introduce multi-scale training. To improve object
detection, researchers have also utilized multi-task learning [3, 6] to
boost object detection. Contextual information is also introduced
to improve object detection performance using Markov Random
Field [48] and LSTMs [4]. Our work uses information from VIO to
improve accuracy using off-the-shelf models without any additional
training.

2.3 Semantic SLAM

With the development of deep learning, using semantic information
for SLAM has drawn a great deal of attention. Many researchers [27,
36, 46] have proposed methods to combine semantic segmentation
and SLAM together, where the semantic labels of objects are fused
into the SLAM map to create a semantic map. MaskFusion [33]
proposed segmenting different objects in the scene while tracking
and reconstructing them even as they move independently from the
camera. SLAM++ [34] reconstructed the object-level environment
with a 3D object database based on RGBD SLAM in real-time.
Bowman et al. [2] built an object-level map and also utilized object
association tightly coupled with SLAM to improve accuracy. Mu et
al. [28] treated all object landmarks with discrete distribution to be
coupled with SLAM. This was later improved by Zhang et al. [47]
by using a hierarchical Dirichlet process for data association. To the
best of our knowledge, our work is the Ô¨Årst to use VIO information
to improve DNN-based object detection and provide a semantic map
in real-time on AR capable mobile devices.

2.4 Improving Algorithms using VIO

AR capable mobile devices are often equipped with a camera and
Inertial Measurement Unit (IMU) and use VIO to estimate camera
pose. The additional sensor data and output of VIO can help solve
challenging computer vision problems. Kurz et. al. [19] proposed
gravity-aligned local feature descriptors for image classiÔ¨Åcation.
The idea is similar to our image orientation correction described in
Sect. 3.1. For 3D reconstruction, Chisel [17] utilized RGBD data
and camera pose information from VIO to reconstruct indoor scenes
using an implicit Truncated Signed Distance Function data structure.
For collision detection, Xu et. al. [45] proposed a real-time multi-
scale voxelization method using camera pose information and sparse
point clouds from mobile AR frameworks. Visual occlusion can also
be implemented by depth densiÔ¨Åcation [15, 38] or by depth from
motion [41]. Phalak et. al. [29] utilized a sequence of posed RGB
images as input to a DNN to estimate the boundaries of the indoor
environment. Our method takes advantage of VIO to improve object
detection.

3 METHODOLOGY

Fig. 2 shows an overview of our pipeline. During a live AR session,
a user moves around a scene to view virtual objects from different
viewing angles while holding the phone using different orientations.
The image sequence is consumed by the mobile AR framework (e.g.,
ARCore or ARKit), which uses VIO to track the pose of the mobile
device in real-time. The pose is used to correct the image orientation
so that the objects in the image appear to be in an upright orientation.
The corrected images are passed to a DNN for object detection.

Figure 2: Overview pipeline of our approach. The different modules in our method are shown as blue boxes.

Figure 3: Most real-world objects are in an upright orientation, such
as the desk, chair, and cup in this scene from the COCO dataset [23].

Most DNNs used for object detection, such as YOLO, SSD, and
Faster R-CNN, can output detection bounding boxes and correspond-
ing labels. Although our method works with any of these pre-trained
DNNs, we chose to focus on the efÔ¨Åcient models that can be inte-
grated with the AR framework on mobile devices. Moreover, since
we are interested in indoor environments, we chose to use the SSD-
mobilenetV1coco model from the TensorÔ¨Çow detection model zoo.
The model is pre-trained on the COCO dataset [23], which covers
popular object categories in indoor environments.

In addition to camera pose information, sparse point cloud data is
typically provided by the AR framework as well. The point clouds
and camera pose are combined with object detection output from
the DNN into our scale-based Ô¨Åltering module and semantic map
module. Our method improves object detection accuracy by taking
advantage of the geometric information with real-world scale. The
Ô¨Ånal detection result for the current frame is obtained by adjusting
the DNN output using information from the two modules.

3.1 Image Orientation Correction

If an out-of-the-box DNN-based object detection model is not trained
with rotation augmentation, its detection capability will be impaired
when the orientation of an object is not aligned with that of the
camera. In this case, even if a detector has a certain level of built-in
rotational invariance, the output bounding box of the object is still

Figure 4: Image orientation correction using gravity direction.

aligned with the camera axes and likely to include unwanted pixels
from the background.

We take advantage of the fact that most objects in real-world
scenes are placed on horizontal surfaces and are in their upright
orientations, as shown in Fig. 3. One important feature of mobile
devices is that the direction of gravity can be estimated by using the
IMU sensor. For example, the Android platform provides the gravity
sensor values that include the direction and magnitude of gravity.
For an input image, it is straightforward to compute the angle to
rotate the image around its center so that the x-axis of the image
is perpendicular to the direction of gravity. After this correction,
objects that are placed on horizontal surfaces will appear in their
upright orientations. In our pipeline, different from [19], orientation
correction works as a preprocessing step for the DNN model.

This orientation correction can greatly increase the detection
accuracy of certain object categories such as furniture and appliances.
Furthermore, the object bounding boxes can be used to estimate the
real-world vertical and horizontal size of an object in 3D space. This
information is used in our scale-based Ô¨Åltering in the next step.

3.2 Scale-based Filtering
The detection results from the object detector sometimes include
false positives, many of which can be easily Ô¨Åltered if scale infor-
mation is available (e.g., a book that is 5 m high is likely a false

AR FrameworkPer-Category Scale Databaseùëùùëôùëùùëöùëéùëù3.1 Image Orientation CorrectionObject DetectionDNNsùëùùë†ùëêùëéùëôùëíCorrected ImagesCamera ImagesBboxoutputCamera PosePoint CloudCategory Scale‚Ä¶+3.2 Scale-based FilteringSceneOur Output3.4 Final Resultsùëù=ùëùùë†ùëêùëéùëôùëí‚àóùëùùëöùëéùëù‚àóùëùùëô3.3 Online Semantic Mapping ImageCorrected ImageGravity DirectiongxTable 1: Example Data Entries in the Per-Category Scale Database

Category
chair
dining table
oven
refrigerator

minw(m) maxw(m) minh(m) maxh(m)
0.8
1.0
0.3
0.5

0.3
0.5
0.4
0.5

1.0
2.0
1.2
1.4

1.7
1.5
1.4
2.0

Scale-based Ô¨Åltering cannot distinguish between categories that
have a similar scale range. A more general approach, semantic
mapping, is proposed in the next subsection to take advantage of
the multi-viewpoint data of an object typically available in an AR
session.

3.3 Online Semantic Mapping

There are many challenges associated with object detection such
as variance in scale and viewpoint. An object detector must detect
objects with different scales on the images and from different view-
points. Moreover, the object detector might produce inconsistent
detection results over time. To overcome these problems, we build
a semantic map using output from both the AR framework and the
object detector. A probabilistic model is then used to maintain and
update object category, viewpoint, and scale information within the
semantic map. Finally, we extract information from the updated 3D
semantic map to adjust the object label probability from the object
detector for the current frame as shown in Fig. 2. The idea is if
an object can be correctly labeled by the detector most of the time
over a range of different viewpoints, our approach should predict
the correct label with higher conÔ¨Ådence.

Our semantic mapping and updating process consists of three
steps: (1) given the output of both the AR framework and the object
detector, we create object point representations from the detection
results on the current frame; (2) the new object points are fused with
the superpoints that were already stored in the semantic map; and
(3) the probability output from the object detector is updated. The
entire algorithm is shown in algorithm 1.

Algorithm 1: Online Semantic Mapping.

1: while object detection is active do

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

for each object detected on the current frame do

create one object point with the representation (locin, lin, vin, sin);
Ô¨Ånd all superpoints whose distance to the incoming object is within the
maximum scale of object category lin and insert them into a set Sin.

if Sin is not empty then

for each superpoint spi in Sin do
update the score E sp
list view ‚Üê vin if vdi f f ‚â• 45‚ó¶
list scale ‚Üê sin if sdi f f ‚â• 1

l using Equation 7

if no superpoints within the minimum scale distance of the incoming

object point then

Add a new superpoint with the representation
(loc ‚Üê locin, list score(E in

l ), list view(vin), list scale(sin))

update the probability of current object using Equation 9

Step 1: For each frame, the object detector outputs a list of N
object categories with associated bounding boxes and probabilities.
For each object, we create one Object Point with the representation
(loc, label, view, scale), where loc is the 3D coordinates of the me-
dian of all sparse points within the bounding box (same as in the
scale estimation step of Sect. 3.2), label is the object label, view
is the view direction from the camera to loc and is a normalized
unit vector, and scale is the scale information that depends on the
distance d from camera position to loc. The scale information s is

Figure 5: Scale estimation. The 3D sparse points are projected onto
the image shown as dots. Blue dots are points that are inside a
2D bounding box generated by the detector and the red dot is the
projection of the median of all blue points.

positive). Fortunately, most mobile AR frameworks provide a per-
frame sparse 3D point cloud in a coordinate system with a real-world
scale. In this section, we propose a method to estimate the scale of
the objects within the scene and Ô¨Ålter out false positives.

In each frame, the detector predicts bounding boxes around ob-
jects. For each box, we compute two values to represent an object‚Äôs
real scale in 3D using the following equation:

Dw = w/ fx ‚àó d
Dh = h/ fy ‚àó d

(1)

where Dw and Dh are the estimated horizontal and vertical lengths
of the object, fx and fy are focal lengths of the camera, w and h are
the width and height of the 2D detection bounding box, and d is the
distance from the camera to the object. To compute d, we compute
the median of the 3D sparse points produced by the AR framework
whose 2D projections are within the detection bounding box (e.g.,
shown as blue dots in Fig. 5). Then, d is computed as the Euclidean
distance between the camera position and the median.

To Ô¨Ålter out false positives based on the estimated scale, we intro-
duce a per-category scale database including 80 categories deÔ¨Åned
in COCO. For each category, we manually deÔ¨Åne a minimum and
maximum scale for the horizontal and vertical dimensions, result-
ing in four values for each category. Table 1 provides a number of
examples. Different object orientations may lead to varying widths
of the 2D bounding box on an image (e.g., the width of a thin slab
on an image will be very different from two different sides). There-
fore, we use both the smallest and the largest possible values for
the horizontal dimension when populating our database. Since the
main goal of scale-based Ô¨Åltering is to reject false positives, using
conservative values can ensure that true positives will be preserved
during Ô¨Åltering.

The Ô¨Ånal step of scale-based Ô¨Åltering is to determine if the cur-
rently estimated scale matches the corresponding scale data from
the database. Since both Dw and Dh are coarse estimations, we do
not use the estimated dimension value to adjust the Ô¨Ånal detection
probability. Instead, we only use these two values for Ô¨Åltering the
false-positive results pscale as follows:

pscale =

(cid:40)
1
0.5 otherwise

i f minw ‚â§ Dw ‚â§ maxw and minh ‚â§ Dh ‚â§ maxh

(2)

chairan integer deÔ¨Åned as:

s = (cid:98)log2d(cid:101)

(3)

Step 2: The second step is to fuse the estimated object with
the information already in the semantic map, which stores a set of
superpoints for objects in the scene. Different from the estimated
object point data structure, the object superpoints are represented
as (loc, list score, list view, list scale). The three lists encode infor-
mation from all previous frames in the same AR session.

‚Ä¢ list score(E1, E2, E3, ¬∑ ¬∑ ¬∑ , El, ¬∑ ¬∑ ¬∑ ) is a list of the scores El for
each label l that has been detected at this superpoint location.
The higher the score, the higher probability this superpoint is
of category l. The initial score values for all labels are 0.

‚Ä¢ list view(v1, v2, v3, ¬∑ ¬∑ ¬∑ ) is a list of historical view directions
from camera positions to the superpoint when an object of any
category is detected at the superpoint.

‚Ä¢ list scale(s1, s2, s3, ¬∑ ¬∑ ¬∑ ) is a list of historical scales when an

object of any category is detected at the superpoint.

It is worth noting that different median locations (loc) from dif-
ferent viewpoints during an AR session might be computed for the
same object because each image only shows a partial surface of an
object. Moreover, the same object might be given different labels
at different time instances. To fuse each incoming estimated object
point (locin, lin, vin, sin) with the superpoints in the semantic map,
we Ô¨Årst Ô¨Ånd a set of superpoints Sin whose loc are within a threshold
distance to the incoming object point locin. We use the maximum
scale of category lin from the scale database as the threshold when
selecting set Sin.

For the incoming object point of category l, we compute its score
Ein
l as follows:

Ein
l =

wv + ws
2

‚àó p
l

(4)

where pl is the probability of the category l generated by the

DNN and weights wv and ws are calculated as:

wv =

Ô£±
0
Ô£¥Ô£≤
(vdi f f ‚àí 45) / 45
Ô£¥Ô£≥
1

i f vdi f f < 45‚ó¶
i f 45‚ó¶ ‚â§ vdi f f ‚â§ 90‚ó¶
otherwise

ws =

(cid:40)

ks ‚àó sdi f f
1

i f sdi f f < 1/ks
otherwise

(5)

(6)

where vdi f f is the minimum absolute angular difference between
vin and all view directions in the list views of all points in Sin. The
higher the vdi f f , the higher the weight wv is. In this way, we value
information observed from different viewing angles. We also set the
weight wv to zero when the vdi f f is smaller than 45 degrees to only
update the semantic map intermittently. wv is capped at 1 when the
vdi f f is larger than 90 degrees.

Similarly, sdi f f is the minimum absolute scale difference between
sin and all scales in the list scales of all points in Sin. The higher
the sdi f f , the higher the weight ws is. ks is used to normalize sdi f f
based on a value range. In our method, this range is set empirically
to be [0, 5]. Therefore ks is set to be 0.2.

If the set Sin is not empty, for each superpoint sp ‚àà Sin, we update
l using the information of the new detection of
exists in list score. If not,

its detection score Esp
category l. We Ô¨Årst check if the score Esp
l

we initialize Esp
Esp
as follows:
l

l with 0 and add it into list score. Then we update

Esp
l =

(cid:40)

Esp
l + Ein
l
Esp
l + Ein
l + 1

i f maxScale ‚â• D > minScale
i f D ‚â§ minScale

(7)

where Dis the distance between the incoming point and sp. If sp is
within the minimum scale of the incoming object, we add 1 to the
score of the detected label as a reward.

We also add view direction of the current detection vin into
list view of sp if vdi f f ‚â• 45‚ó¶. We add scale of current detection
sin into list scale of sp if sdi f f ‚â• 1.

If there is no superpoint within the minimum distance of the
incoming object point, we initialize the three lists list score,
list view and list scale with the values of Ein
, vin and sin. Then we
l
add a new superpoint with these three lists into the map.

Step 3: For each incoming object point p with label l, we update
its label probability pl using the nearby superpoints stored in the
semantic map. We Ô¨Ånd a second set of superpoints whose loc are
within a distance (minimum scale of category lin) to the incoming
object point locin. We call this Supdate. We Ô¨Ånd the maximum score
among all the superpoints with label l in Supdate. We then Ô¨Ånd
Emax
l
the maximum score Emax
among all the superpoints with any other
label in Supdate. The probability pmap of the semantic map at point
p is deÔ¨Åned by a modiÔ¨Åed sigmoid function as follows:

¬Øl

pmap =

(cid:40)

1
1+eEmax
¬Øl
0.5

‚àíEmax
l

i f El ‚â• E¬Øl
i f El < E¬Øl

(8)

pmap is at least 0.5 to guarantee that it does not decrease the

output probability pl from the detector dramatically.

3.4 Final Detection Results
Combining scale-based Ô¨Åltering and adjustment using semantic map,
the Ô¨Ånal probability of an object is:

p = pscale ‚àó pmap ‚àó pl

(9)

For each frame, the Ô¨Ånal output is a list of bounding boxes, each
of which has the output (l, p, bbox), where the label and bounding
box are the same as the original output from the DNN.

4 EVALUATION
In this section, we show the evaluation of the proposed method
compared with detection results from the DNN only.

4.1 Datasets
Our approach does not need extra data for re-training the DNN. Un-
fortunately, none of the popular image datasets for object detection
provide the 3D sparse point cloud and camera pose information
acquired by an AR framework. For this paper, we collected a new
indoor dataset for evaluation.

Our indoor dataset includes about 2,000 images with 20 object
categories and 3,384 instances in total, primarily obtained from
scanning home and ofÔ¨Åce environments. It also includes per-frame
camera pose and sparse point cloud information from 12 room-
scale AR sessions. Fig. 6 shows the number of instances in the
top 10 categories of our dataset. The remaining 10 categories in
our dataset have a total of 167 instances. Chairs were the most
commonly scanned object, and the chair category is the largest
consisting of 945 instances. Some example images are shown in
Fig. 7. We use a mobile phone app developed using ARCore for
data acquisition. The app collects images at about 1 fps while

Figure 6: Number of instances for each of the top 10 categories of
our dataset.

ARCore is running continuously to track the camera pose of the
device. Each session is about 1 to 2 minutes long. We encouraged
the data collectors to capture objects from various viewpoints to
simulate real-world AR use cases where users tend to observe virtual
content from different viewing angles. The collected images are also
rotation corrected by using the method introduced in Sect. 3.1 and
used to compare our method with SSD (MobileNet) in an ablation
study where rotation correction is removed. We labeled objects in
the original and corrected images using the categories of COCO
dataset [23].

4.2 Results
We tested all modules of our approach on our dataset using the
SSD mobilenet V1 coco model from the TensorÔ¨Çow detection
model zoo. Fig. 1 shows the visualization of results on one un-
corrected example image using different methods. The original
DNN (SSD MobileNet) result in Fig. 1(a) has false positives be-
cause of the insufÔ¨Åcient data augmentation for training. Fig. 1(b)
shows that our image orientation correction process can ease this
problem. Fig. 1(c) shows the result after applying scale-based Ô¨Ålter-
ing: the ‚Äùairplane‚Äù label is Ô¨Åltered out as a false positive. Fig. 1(d)
shows the result of online semantic mapping: the ‚Äùlaptop‚Äù label is
also removed by the information from the semantic map. Fig. 1(e)
shows the result using our proposed method, which combines all of
the modules and generates a more accurate result.

To validate our online semantic mapping module, a semantic
map is visualized in Fig. 8. The semantic superpoints are shown
as spheres on two images from different viewpoints. The radius
of a sphere reÔ¨Çects the minimum scale of the corresponding object
category. Different colors represent different categories: white for
the couch, red for the chair, and green for the dining table. As shown
in Fig. 8, the superpoints maintain the correct semantic information.
The visualization only shows the class label with the highest score
for each superpoint, and the color saturation represents the score
value. A blue circle highlights one superpoint which has a relatively
low score for the current class label. It shows that using our semantic
mapping, a superpoint that is in proximity to other superpoints with
different labels tends to have a lower score. The semantic map is
updated online, and it provides a more accurate probability for object
detection.

Table 2 shows the evaluation results using metrics used by COCO.
In Table 2, SSD represents the results of the original SSD MobileNet
model, OC represents results using image orientation correction, SF
represents results using scale-based Ô¨Åltering, and OSM represents
results using online semantic mapping. As the results show, each
of our modules has generated higher average precision compared
with the original SSD MobileNet model. In particular, our entire
pipeline ALL (OSM+SF+OC+SSD) has increased the average pre-
cision from 8.0% to 20.4%. Table 2 also shows that scale-based
Ô¨Åltering and online semantic mapping improve detection results in
different ways, since the average precision of our overall method

Figure 7: Examples of our testing dataset

(20.4%) is signiÔ¨Åcantly higher than those of SF+OC+SSD (14.6%)
and OSM+OC+SSD (11.3%).

Table 3 shows that all individual modules improve the average
recall, as well, although the average recall of the entire pipeline has
decreased from the original model by 4%. This is because only the
bounding boxes that can survive both modules will be accepted by
our pipeline. This results in the missed detection of new objects
or truncated objects. The deÔ¨Ånitions of superscripts of average
precision and average recall metrics in both Table 2 and Table 3 are
the same as those in the COCO dataset [23].

We have also tested Google MediaPipe 6D object detection [1]
on our dataset. To compare our 2D detection bounding boxes with
MediaPipe‚Äôs 3D bounding boxes, we compute a 2D bounding box
based on each output cuboid of 6D detection. We set the IOU
threshold as 0.5 for evaluation purposes. Using a larger threshold
could be seen as unfair to 6D detection since the 2D box always
covers a larger area than the corresponding cuboid projected on the
image. We tested their chair model since the chair model is one
of the available models and accounts for almost one third of the
instances in our dataset. Our results show that MediaPipe has a
14.4% average precision on our dataset. The results also indicate
that our dataset, which is captured from real homes and ofÔ¨Åces, is
very challenging.

All of the results were tested on a Pixel 3 smartphone. Our
proposed method can run at ‚àº 7 fps while ARCore tracks the pose
of the camera in real-time. In comparison, the SSD inference alone
runs at ‚àº 7 fps while ARCore is running, conÔ¨Årming that our method
is very efÔ¨Åcient. We also expect that our method can build a semantic
map for large scale environments since we only store a small number
of superpoints for each detected object instead of a point cloud or
voxels. The selected SSD model takes 300 √ó 300 image as input and
outputs the top 10 bounding boxes with labels and properties.

5 CONCLUSION

There is a large amount of ongoing research on 2D object detection
DNNs, and some of them have achieved high efÔ¨Åciency and great
performance on mobile devices. Compared with 6D object detection
[42], it is much easier to collect and label 2D object boxes. Instead

94562638838434115313879787501002003004005006007008009001000chaircouchbowldiningtablecupbedovenbookbottlepersonNumber of Instances6 LIMITATIONS AND FUTURE WORK

One limitation of our approach is that a newly detected truncated
object might get rejected due to bad scale estimation and the fact
that our map has no prior information about the detection. This
can happen when the object is occluded by other objects or is only
partially imaged by the camera. However, our comprehensive ap-
proach may still work if the online semantic map is continuously
being updated using the output from the object detector. Another
limitation is that the 3D sparse point clouds generated by the AR
framework do not have correspondence between frames. Moreover,
the 3D points do not have temporal consistency due to optimization
during the SLAM process, which reduces the quality of our semantic
map. In the future, we would like implement our own VIO algorithm
so that we can further leverage the output of the VIO. Finally, our
work may fail when the object being scanned moves.

There are a few future work directions that we can pursue. In
this paper, we only use the sparse point cloud provided by the
AR framework and bounding boxes from the object detector. In
the future, we would like to explore how our pipeline can beneÔ¨Åt
from instance segmentation and dense depth map input. We would
also like to evaluate how the accuracy of the VIO system under
various conditions (e.g., scene depth, illumination, etc.) affects our
object detection method. Moreover, we would like to extend our
per-category scale database to make our approach more generic.
For example, we would like to explore how to handle dynamic
objects better by adding movement information to the database.
Another idea is to customize the database speciÔ¨Åcally for the known
environment in order to make the scale-based Ô¨Åltering more accurate.

ACKNOWLEDGMENTS

The authors wish to thank the reviewers for their detailed feedback
and suggestions. We would also like to thank our colleagues at the
OPPO US Research Center that helped us capture images of their
homes. Finally, we wold like to thank Chris Vick for helping with
paper writing.

REFERENCES

[1] A. Ahmadyan, T. Hou, J. Wei, L. Zhang, A. Ablavatski, and M. Grund-
mann. Instant 3D object tracking with applications in augmented reality.
In IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), Fourth Workshop on Computer Vision for AR/VR, 2020.
[2] S. L. Bowman, N. Atanasov, K. Daniilidis, and G. J. Pappas. Proba-
bilistic data association for semantic SLAM. In IEEE International
Conference on Robotics and Automation (ICRA), pp. 1722‚Äì1729, 2017.
[3] S. Brahmbhatt, H. I. Christensen, and J. Hays. StuffNet: Using stuffto
improve object detection. In IEEE Winter Conference on Applications
of Computer Vision (WACV), pp. 934‚Äì943, 2017.

[4] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene labeling with
LSTM recurrent neural networks. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 3547‚Äì3555, 2015.

[5] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A uniÔ¨Åed multi-
scale deep convolutional neural network for fast object detection. In
European conference on computer vision (ECCV), pp. 354‚Äì370, 2016.
[6] J. Dai, K. He, and J. Sun. Instance-aware semantic segmentation via
multi-task network cascades. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 3150‚Äì3158, 2016.

[7] J. Dai, Y. Li, K. He, and J. Sun. R-FCN: object detection via region-
based fully convolutional networks. In 30th International Conference
on Neural Information Processing Systems, pp. 379‚Äì387, 2016.
[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object de-
tection using deep neural networks. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 2147‚Äì2154, 2014.

[9] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg. DSSD: de-
convolutional single shot detector. arXiv preprint arXiv:1701.06659,
2017.

[10] R. Girshick. Fast R-CNN.

In IEEE International Conference on

Computer Vision (ICCV), pp. 1440‚Äì1448, 2015.

Figure 8: Visualization of a semantic map. Each sphere represents a
superpoint. White is for the couch (note that there is a black couch
behind the table), red is for the chairs, and green is for the dining table.
The bottom two images visualize superpoints on two different frames.

AP
Data
8.0
SSD
10.7
OC+SSD
14.6
SF+OC+SSD
OSM+OC+SSD 11.3
20.4
ALL

Table 2: AP Metrics
AP.75
7.4
10.0
13.5
10.6
18.9

AP.5
10.3
13.8
18.9
14.7
26.3

APs
0.28
0.75
0.60
0.63
0.78

APm
5.6
11.0
14.6
12.8
19.0

APl
14.7
17.6
19.1
20.3
26.0

of 6D object detection, we take a hybrid approach where we utilize
the 3D perception capabilities of the AR framework to improve
2D object detection. In this paper, we proposed three modules to
achieve this purpose: we introduced an image orientation correction
method to improve the input data to the object detector, we also
introduced a per-category scale database as a prior knowledge to
Ô¨Ålter 2D detection results based on their estimated real-world scales,
and Ô¨Ånally we proposed an online semantic mapping approach to
further improve detection accuracy. Our approach works with any
object detector as long as the output includes object bounding boxes,
labels, and probabilities. However, it is challenging to integrate other
DNN models with ARCore or ARkit while maintaining real-time
performance, with the exception of the SSD models maintained by
Google. We plan to keep working on such integration in the future.

Table 3: AR Metrics

AR10
Data
24.0
SSD
30.0
OC+SSD
SF+OC+SSD
24.6
OSM+OC+SSD 26.0
20.0
ALL

ARs
0.1
0.5
0.0
0.4
0.0

ARm
6.3
11.1
9.6
9.6
7.4

ARl
17.6
18.3
15.0
16.0
12.5

[11] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierar-
chies for accurate object detection and semantic segmentation. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp.
580‚Äì587, 2014.

[12] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh,
H. Shi, J. Li, S. Yan, and T. S. Huang. Seq-NMS for video object
detection. arXiv preprint arXiv:1602.08465, 2016.

[13] K. He, G. Gkioxari, P. Doll¬¥ar, and R. Girshick. Mask R-CNN. In IEEE
International Conference on Computer Vision (ICCV), pp. 2980‚Äì2988,
2017.

[14] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep
convolutional networks for visual recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 37(9):1904‚Äì1916, 2015.

[15] A. Holynski and J. Kopf. Fast depth densiÔ¨Åcation for occlusion-aware
augmented reality. ACM Transactions on Graphics (TOG), 37(6):1‚Äì11,
2018.

[16] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: EfÔ¨Åcient convo-
lutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861, 2017.

[17] M. Klingensmith, I. Dryanovski, S. Srinivasa, and J. Xiao. Chisel:
Real time large scale 3d reconstruction onboard a mobile device using
In Robotics: Science and
spatially hashed signed distance Ô¨Åelds.
Systems, vol. 4, p. 1, 2015.

[18] T. Kong, A. Yao, Y. Chen, and F. Sun. HyperNet: Towards accu-
rate region proposal generation and joint object detection. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp.
845‚Äì853, 2016.

[19] D. Kurz and S. B. Himane.

Inertial sensor-aligned visual feature
In IEEE Conference on Computer Vision and Pattern

descriptors.
Recognition (CVPR), pp. 161‚Äì166, 2011.

[20] Y. Li, Y. Chen, N. Wang, and Z. Zhang. Scale-aware trident networks
for object detection. In IEEE/CVF International Conference on Com-
puter Vision (ICCV), pp. 6053‚Äì6062, 2019.

[21] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollr. Focal loss for dense
object detection. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 42(2):318‚Äì327, 2020.

[22] T.-Y. Lin, P. Doll¬¥ar, R. Girshick, K. He, B. Hariharan, and S. Belongie.
Feature pyramid networks for object detection. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 936‚Äì944, 2017.
[23] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll¬¥ar, and C. L. Zitnick. Microsoft COCO: Common objects in
context. In European Conference on Computer Vision (ECCV), pp.
740‚Äì755. Springer, 2014.

[24] M. Liu and M. Zhu. Mobile video object detection with temporally-
aware feature maps. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5686‚Äì5695, 2018.

[25] R. Liu, J. Zhang, S. Chen, and C. Arth. Towards SLAM-based outdoor
localization using poor GPS and 2.5D building models. In 2019 IEEE
International Symposium on Mixed and Augmented Reality (ISMAR),
pp. 1‚Äì7, 2019.

[26] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
Berg. SSD: Single shot multibox detector. In European Conference on
Computer Vision (ECCV), pp. 21‚Äì37. Springer, 2016.

[27] J. McCormac, A. Handa, A. Davison, and S. Leutenegger. SemanticFu-
sion: Dense 3D semantic mapping with convolutional neural networks.
In International Conference on Robotics and Automation (ICRA), pp.
4628‚Äì4635, 2017.

[28] B. Mu, S.-Y. Liu, L. Paull, J. Leonard, and J. P. How. SLAM with
objects using a nonparametric pose graph. In IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pp. 4602‚Äì4609,
2016.

[29] A. Phalak, Z. Chen, D. Yi, K. Gupta, V. Badrinarayanan, and A. Ra-
binovich. DeepPerimeter: Indoor boundary estimation from posed
monocular sequences. arXiv preprint arXiv:1904.11595, 2019.
[30] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look
In IEEE Conference on
once: uniÔ¨Åed, real-time object detection.
Computer Vision and Pattern Recognition (CVPR), pp. 779‚Äì788, 2016.
[31] J. Redmon and A. Farhadi. YOLO9000: better, faster, stronger. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),

pp. 6517‚Äì6525, 2017.

[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-
time object detection with region proposal networks. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 39(6):1137‚Äì1149,
2017.

[33] M. Runz, M. BufÔ¨Åer, and L. Agapito. MaskFusion: Real-time recogni-
tion, tracking and reconstruction of multiple moving objects. In IEEE
International Symposium on Mixed and Augmented Reality (ISMAR),
pp. 10‚Äì20, 2018.

[34] R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H. Kelly, and
A. J. Davison. SLAM++: Simultaneous localisation and mapping at the
level of objects. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 1352‚Äì1359, 2013.

[35] B. Singh, M. Najibi, and L. S. Davis. SNIPER: EfÔ¨Åcient multi-scale
training. In 32nd Conference on Neural Information Processing Sys-
tems (NeurIPS), pp. 9310‚Äì9320, 2018.

[36] N. S¬®underhauf, T. T. Pham, Y. Latif, M. Milford, and I. Reid. Mean-
In IEEE/RSJ
ingful maps with object-oriented semantic mapping.
International Conference on Intelligent Robots and Systems (IROS), pp.
5079‚Äì5085, 2017.

[37] B. Tekin, S. N. Sinha, and P. Fua. Real-time seamless single shot 6D
object pose prediction. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 292‚Äì301, 2018.

[38] Y. Tian, Y. Ma, S. Quan, and Y. Xu. Occlusion and collision aware
smartphone AR using time-of-Ô¨Çight camera. In International Sympo-
sium on Visual Computing (ISVC), pp. 141‚Äì153. Springer, 2019.
[39] J. Tremblay, T. To, B. Sundaralingam, Y. Xiang, D. Fox, and S. Birch-
Ô¨Åeld. Deep object pose estimation for semantic robotic grasping of
household objects. In Conference on Robot Learning (CoRL), 2018.

[40] H. Uchiyama and E. Marchand. Object detection and pose tracking
for augmented reality: recent approaches. In 18th Korea-Japan Joint
Workshop on Frontiers of Computer Vision, 2012.

[41] J. Valentin, A. Kowdle, J. T. Barron, N. Wadhwa, M. Dzitsiuk,
M. Schoenberg, V. Verma, A. Csaszar, E. Turner, I. Dryanovski,
J. Afonso, J. Pascoal, K. Tsotsos, M. Leung, M. Schmidt, O. Guleryuz,
S. Khamis, V. Tankovitch, S. Fanello, S. Izadi, and C. Rhemann. Depth
from motion for smartphone AR. ACM Transactions on Graphics
(TOG), 37(6), Dec. 2018.

[42] C. Wang, D. Xu, Y. Zhu, R. Mart¬¥ƒ±n-Mart¬¥ƒ±n, C. Lu, L. Fei-Fei, and
S. Savarese. DenseFusion: 6D object pose estimation by iterative
In IEEE/CVF Conference on Computer Vision and
dense fusion.
Pattern Recognition (CVPR), pp. 3338‚Äì3347, 2019.

[43] A. Wong, M. Famuori, M. J. ShaÔ¨Åee, F. Li, B. Chwyl, and J. Chung.
YOLO Nano: a highly compact you only look once convolutional
neural network for object detection. arXiv preprint arXiv:1910.01271,
2019.

[44] Y. Xiang, W. Choi, Y. Lin, and S. Savarese. Data-driven 3D voxel pat-
terns for object category recognition. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 1903‚Äì1911, 2015.
[45] Y. Xu, Y. Wu, and H. Zhou. Multi-scale voxel hashing and efÔ¨Åcient 3D
representation for mobile augmented reality. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops (CVPRW), pp.
1618‚Äì1625, 2018.

[46] H. Yu, J. Moon, and B. Lee. A variational observation model of 3D
object for probabilistic semantic SLAM. In International Conference
on Robotics and Automation (ICRA), pp. 5866‚Äì5872, 2019.

[47] J. Zhang, M. Gui, Q. Wang, R. Liu, J. Xu, and S. Chen. Hierarchical
topic model based object association for semantic SLAM. IEEE Trans-
actions on Visualization and Computer Graphics (TVCG), 25(11):3052‚Äì
3062, 2019.

[48] Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler. segDeepM: Ex-
ploiting segmentation and context in deep neural networks for object
detection. In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pp. 4703‚Äì4711, 2015.

