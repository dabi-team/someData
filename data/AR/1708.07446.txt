8
1
0
2

n
a
J

9

]
T
S
.
h
t
a
m

[

2
v
6
4
4
7
0
.
8
0
7
1
:
v
i
X
r
a

Modern Stochastics: Theory and Applications 4 (4) (2017) 381–406
https://doi.org/10.15559/17-VMSTA91

On model ﬁtting and estimation of strictly stationary
processes

Marko Voutilainena,∗, Lauri Viitasaarib, Pauliina Ilmonena

aDepartment of Mathematics and Systems Analysis
Aalto University School of Science
P.O. Box 11100, FI-00076 Aalto, Finland
bDepartment of Mathematics and Statistics
University of Helsinki
P.O. Box 68, FI-00014 University of Helsinki, Finland

marko.voutilainen@aalto.ﬁ (M. Voutilainen), lauri.viitasaari@iki.ﬁ (L. Viitasaari),
pauliina.ilmonen@aalto.ﬁ (P. Ilmonen)

Received: 13 September 2017, Revised: 22 November 2017, Accepted: 25 November 2017,
Published online: 22 December 2017

Abstract Stationary processes have been extensively studied in the literature. Their applica-
tions include modeling and forecasting numerous real life phenomena such as natural disasters,
sales and market movements. When stationary processes are considered, modeling is tradi-
tionally based on ﬁtting an autoregressive moving average (ARMA) process. However, we
challenge this conventional approach. Instead of ﬁtting an ARMA model, we apply an AR(1)
characterization in modeling any strictly stationary processes. Moreover, we derive consistent
and asymptotically normal estimators of the corresponding model parameter.

Keywords AR(1) representation, asymptotic normality, consistency, estimation, strictly
stationary processes
2010 MSC 60G10, 62M09, 62M10, 60G18

1

Introduction

Stochastic processes are widely used in modeling and forecasting numerous real life
phenomena such as natural disasters, activity of the sun, sales of a company and mar-

∗Corresponding author.

www.i-journals.org/vmsta
Preprint submitted to VTeX / Modern Stochastics: Theory and
Applications
<June 18, 2021>

 
 
 
 
 
 
382

M. Voutilainen et al.

ket movements, to mention a few. When stationary processes are considered, model-
ing is traditionally based on ﬁtting an autoregressive moving average (ARMA) pro-
cess. However, in this paper, we challenge this conventional approach. Instead of ﬁt-
ting an ARMA model, we apply the AR(1) characterization in modeling any strictly
stationary processes. Moreover, we derive consistent and asymptotically normal esti-
mators of the corresponding model parameter.

One of the reasons why ARMA processes have been in a central role in modeling
) vanishing at inﬁnity
of time-series data is that for every autocovariance function γ(
·
N there exists an ARMA process X such that γ(k) = γX (k)
and for every n
for every k = 0, 1, .., n. For a general overview of the theory of stationary ARMA
processes and their estimation, the reader may consult for example [1] or [5].

∈

ARMA processes, and their extensions, have been studied extensively in the liter-
ature. A direct proof of consistency and asymptotic normality of Gaussian maximum
likelihood estimators for causal and invertible ARMA processes was given in [18].
The result was originally obtained, using asymptotic properties of the Whittle estima-
tor, in [7]. The estimation of the parameters of strictly stationary ARMA processes
with inﬁnite variances was studied in [16], again, by using Whittle estimators. Port-
manteau tests for ARMA models with stable Paretian errors with inﬁnite variance
were introduced in [12]. An efﬁcient method for evaluating the maximum likelihood
function of stationary vector ARMA models was presented in [14]. Fractionally inte-
grated ARMA models with a GARCH noise process, where the variance of the error
terms is also of ARMA form, was studied in [13]. Consistency and asymptotic nor-
mality of the quasi-maximum likelihood estimators of ARMA models with the noise
process driven by a GARCH model was shown in [3]. A least squares approach for
ARMA parameter estimation has been studied at least in [9] by contrasting its efﬁ-
ciency with the maximum likelihood estimation. Also estimators of autocovariance
and their limiting behavior have been addressed in numerous papers. See for example
[2, 8, 11] and [15].

Modeling an observed time-series with an ARMA process starts by ﬁxing the
orders of the model. This is often done by an educated guess, but there also exists
methods for estimating the orders, see e.g. [6]. After the orders are ﬁxed, the related
parameters can be estimated, for example, by using the maximum likelihood or least
squares estimators. These estimators are expressed in terms of optimization problems
and do not generally admit closed form representations. The ﬁnal step is to conduct
various diagnostic tests to determine whether the estimated model is sufﬁciently good
or not. These tests are often designed to recognize whether the residuals of the model
support the underlying assumptions about the error terms. Depending on whether one
considers strict or weak stationarity, the error process is usually assumed to be an IID
process or white noise, respectively. If the tests do not support the assumptions about
the noise process, then one has to start all over again. Tests for the goodness of ﬁt of
ARMA models have been suggested e.g. in [4].

The approach taken in this paper is based on the discrete version of the main
theorem of [17] leading to an AR(1) characterization for (any) strictly stationary pro-
cesses. Note that this approach covers, but is not limited to, strictly stationary ARMA
processes. It was stated in [17] that a process is strictly stationary if and only if for
every ﬁxed 0 < H < 1 it can be represented in the AR(1) form with φ = e−H

On model ﬁtting and estimation of strictly stationary processes

383

and a unique, possibly correlated, noise term. Although the representation is unique
only after H is ﬁxed, we show that in most of the cases, given just one value of the
autocovariance function of the noise, one is able to determine the AR(1) parameter
and, consequently, the entire autocovariance function of the noise process. It is worth
emphasizing that since the parameter–noise pair in the AR(1) characterization is not
unique, it is natural that some information about the noise has to be assumed. Note
that conventionally, when applying ARMA models, we have assumptions about the
noise process much stronger than being IID or white noise. That is, the autocovari-
ance function of the noise is assumed to be identically zero except at the origin. When
founding estimation on the AR(1) characterization, one does not have to select be-
tween different complicated models. In addition, there is only one parameter left to be
estimated. Yet another advantage over classical ARMA estimation is that we obtain
closed form expressions for the estimators.

The paper is organized as follows. We begin Section 2 by introducing some ter-
minology and notation. After that, we give a characterization of discrete time strictly
stationary processes as AR(1) processes with a possibly correlated noise term to-
gether with some illustrative examples. The AR(1) characterization leads to Yule–
Walker type equations for the AR(1) parameter φ. In this case, due to the correlated
noise process, the equations are of quadratic form in φ. For the rest of the section,
we study the quadratic equations and determine φ with as little information about the
noise process as possible. The approach taken in Section 2 leads to an estimator of the
AR(1) parameter. We consider estimation in detail in Section 3. The end of Section 3
is dedicated to testing the assumptions we make when constructing the estimators.
A simulation study to assess ﬁnite sample properties of the estimators is presented
in Section 4. Finally, we end the paper with three appendices containing a technical
proof, detailed discussion on some special cases and tabulated simulation results.

2 On AR(1) characterization in modeling strictly stationary processes

Throughout the paper we consider strictly stationary processes.

Deﬁnition 1. Assume that X = (Xt)t∈Z is a stochastic process. If

(Xt+n1 , Xt+n2, . . . , Xt+nk )

law
= (Xn1, Xn2 , . . . , Xnk )

for all k

N and t, n1, n2, . . . , nk

Z, then X is strictly stationary.

∈

∈

Deﬁnition 2. Assume that G = (Gt)t∈Z is a stochastic process and denote ∆tG =
Gt−1. If (∆tG)t∈Z is strictly stationary, then the process G is a strictly station-
Gt
ary increment process.

−

The following class of stochastic processes was originally introduced in [17].

Deﬁnition 3. Let H > 0 be ﬁxed and let G = (Gt)t∈Z be a stochastic process. If G
is a strictly stationary increment process with G0 = 0 and if the limit

lim
k→−∞

0

t=k
X

etH ∆tG

(1)

384

M. Voutilainen et al.

H .

exists in probability and deﬁnes an almost surely ﬁnite random variable, then G be-
longs to the class of converging strictly stationary increment processes, and we denote
G

∈ G
Next, we consider the AR(1) characterization of strictly stationary processes. The
continuous time analogy was proved in [17] together with a sketch of a proof for the
discrete case. For the reader’s convenience, a detailed proof of the discrete case is
presented in Appendix A.

Theorem 1. Let H > 0 be ﬁxed and let X = (Xt)t∈Z be a stochastic process. Then
X is strictly stationary if and only if limt→−∞ etHXt = 0 in probability and

∆tX =

e−H

1

Xt−1 + ∆tG

−

(cid:0)

(cid:1)

(2)

for a unique G

H .

∈ G

Corollary 1. Let H > 0 be ﬁxed. Then every discrete time strictly stationary process
(Xt)t∈Z can be represented as

φ(H)Xt−1 = Z (H)

t

,

Xt

−

(3)

where φ(H) = e−H and Z (H)

t = ∆tG is another strictly stationary process.

It is worth to note that the noise Z in Corollary 1 is unique only after the pa-
rameter H is ﬁxed. The message of this result is that every strictly stationary process
is an AR(1) process with a strictly stationary noise that may have a non-zero auto-
covariance function. The following examples show how some conventional ARMA
processes can be represented as an AR(1) process.

Example 1. Let X be a strictly stationary AR(1) process deﬁned by

Xt

−

ϕXt−1 = ǫt,

(ǫt)

∼

IID

0, σ2

with ϕ > 0. Then we may simply choose φ(H) = ϕ and Z (H)
Example 2. Let X be a strictly stationary ARMA(1, q) process deﬁned by

(cid:0)
(cid:1)
t = ǫt.

Xt

−

ϕXt−1 = ǫt + θ1ǫt−1 +

+ θqǫt−q,

· · ·

(ǫt)

∼

IID

0, σ2

with ϕ > 0. Then we may set φ(H) = ϕ, and Z (H)
Example 3. Consider a strictly stationary AR(1) process X with ϕ < 0. Then X
admits an MA(

then equals to the MA(q) process.

) representation

t

(cid:0)

(cid:1)

∞

∞

Xt =

ϕkǫt−k.

k=0
X

From this it follows that

∞

Z (H)

t = ǫt +

ϕk

ϕ

k=0
X

(cid:0)

φ(H)

ǫt−1−k

−

(cid:1)

On model ﬁtting and estimation of strictly stationary processes

385

and

cov

Z (H)
t

, Z (H)
0

= ϕt−2(ϕ

φ(H))σ2

ϕ +

ϕ

φ(H)

−

−

(cid:0)

(cid:1)

(cid:0)

∞

n

ϕ2

n=1
X
(cid:0)

(cid:1)

(cid:1)

.

!

Hence in the case of an AR(1) process with a negative parameter, the autocovariance
function of the noise Z of the representation (3) is non-zero everywhere.

Next we show how to determine the AR(1) parameter φ(H) in (3) provided that
the observed process X is known. In what follows, we omit the superindices in (3).
We assume that the second moments of the considered processes are ﬁnite and that the
processes are centered. That is, E(Xt) = E(Zt) = 0 for every t
Z. Throughout the
rest of the paper, we use the notation cov(Xt, Xt+n) = γ(n) and cov(Zt, Zt+n) =
r(n) for every t, n
Lemma 1. Let centered (Xt)t∈Z be of the form (3). Then

Z.

∈

∈

for every n

∈
Proof. Let n

φ2γ(n)

−

Z.

φ

γ(n + 1) + γ(n

1)

+ γ(n)

r(n) = 0

−

−

(4)

(cid:0)

(cid:1)

Z. By multiplying both sides of

∈

Xn

φXn−1 = Zn

−
φX−1 and taking expectations, we obtain

with Z0 = X0 −

E

Xn(X0 −
= φ2γ(n)
(cid:0)

φX−1)
φ

−

φE

−

γ(n + 1) + γ(n
(cid:1)
(cid:0)

Xn−1(X0 −
1)
−

φX−1)
+ γ(n) = r(n).
(cid:1)

(cid:0)
Corollary 2. Let centered (Xt)t∈Z be of the form (3) and let N

(cid:1)

N be ﬁxed.

∈

(1) If γ(N )

= 0, then either

γ(N + 1) + γ(N

−

φ =

1) +

(γ(N + 1) + γ(N

p

2γ(N )

1))2

−

−

4γ(N )(γ(N )

r(N ))

−

(5)

or

γ(N + 1) + γ(N

1)

−

−

φ =

(γ(N + 1) + γ(N

p

2γ(N )

1))2

−

−

4γ(N )(γ(N )

r(N ))

.

−

(6)

(2) If γ(N ) = 0 and r(N )

= 0, then

φ =

r(N )

−

γ(N + 1) + γ(N

.

1)

−

Note that if γ(N ) = r(N ) = 0, then Lemma 1 yields only γ(N + 1) + γ(N
1) = 0
providing no information about the parameter φ. As such, in order to determine the
parameter φ, we require that either γ(N )

= 0 or r(N )

= 0.

−

 
6
6
6
6
386

M. Voutilainen et al.

Remark 1. If the variance r(0) of the noise is known, then (5) and (6) reduces to

γ(1)

±

φ =

γ(1)2

p

γ(0)(γ(0)

−
γ(0)

r(0))

.

−

At ﬁrst glimpse it seems that Corollary 2 is not directly applicable. Indeed, in princi-
ple it seems like there could be complex-valued solutions although representation (3)
together with (4) implies that there exists a solution φ
(0, 1). Furthermore, it is not
clear whether the true value is given by (5) or (6). We next address these issues. We
start by proving that the solutions to (4) cannot be complex. At the same time we are
able to determine which one of the solutions one should choose.

∈

Lemma 2. The discriminants of (5) and (6) are always non-negative.

Proof. Let k
and applying (3) repeatedly we obtain

∈

Z. By multiplying both sides of (3) with Xt−k, taking expectations,

γ(k)

−

φγ(k

−

1) = E(ZtXt−k) = E

Zt(Zt−k + φXt−k−1)

= r(k) + φE(ZtXt−k−1)
= r(k) + φE
= r(k) + φr(k + 1) + φ2E(ZtXt−k−2).
(cid:1)

Zt(Zt−k−1 + φXt−k−2)

(cid:0)

(cid:0)

(cid:1)

Proceeding as above l times we get

γ(k)

−

φγ(k

−

1) =

l−1

i=0
X

φir(k + i) + φlE

Zt(φXt−k−l−2)

.

(cid:0)

(cid:1)

Letting l approach inﬁnity leads to

γ(k)

φγ(k

1) =

−

−

∞

i=0
X

φir(k + i),

(7)

where the series converges as r(k + i)
(7) that

≤

r(0) and 0 < φ < 1. It now follows from

γ(N ) = φγ(N

= φγ(N

= φγ(N

= φγ(N

∞

1) +

φir(N + i)

i=0
X

1) + r(N ) + φ

∞

i=1
X
∞

φi−1r(N + i)

1) + r(N ) + φ

φir(N + i + 1)

1) + φ

i=0
X
γ(N + 1)
−

φγ(N )

+ r(N ).

−

−

−

−

Denote the discrimant of (5) and (6) by D. That is,

(cid:0)

(cid:1)

D =

γ(N

−

1) + γ(N + 1)

2

−

4γ(N )

γ(N )

(cid:0)

(cid:1)

(cid:0)

r(N )

.

−

(cid:1)

On model ﬁtting and estimation of strictly stationary processes

387

By using the equation above we observe that

D =

(cid:18)

γ(N ) + φ2γ(N )

φ

−

r(N )

2

−

(cid:19)

4γ(N )

γ(N )

(cid:0)

r(N )

.

−

(cid:1)

Denoting aN = r(N )

γ(N ) , multiplying by

γ(N )2 , and using the identity

φ2

(a + b)2

−

4ab = (a

b)2

−

yields

φ2
γ(N )2

D =

1 + φ2

2

aN

−

−

4φ2(1

−

aN ) =

φ2

1 + aN

−

2

0.

≥

This concludes the proof.

(cid:0)

(cid:1)

(cid:0)

(cid:1)

Note that if r(N ) = 0, as φ < 1, the discriminant is always positive. Let aN =

r(N )
γ(N ) . The proof above now gives us the following identity

φ =

1
2φ

1 + φ2

(cid:18)

aN

−

±

|

γ(N )
|
γ(N )

φ2

−

1 + aN

.
(cid:19)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

−

1 + aN > 0, then φ is given by (5) (as φ

This enables us to consider the choice between (5) and (6). Assume that γ(N ) > 0. If
φ2
1 + aN < 0,
then φ is determined by (6). Finally, contrary conclusions hold in the case γ(N ) < 0.
In particular, we can always choose between (5) and (6) provided that either aN
0
or aN

1. Moreover, from (4) it follows that

(0, 1)). Similarly, if φ2

≤

−

∈

≥

r(N )
γ(N )

=

r(N + k)
γ(N + k)

if and only if

γ(N + 1) + γ(N

γ(N )

1)

−

=

γ(N + 1 + k) + γ(N
γ(N + k)

−

1 + k)

,

provided that the denominators differ from zero. Since (5) and (6) can be written as

φ =

γ(N + 1) + γ(N

2γ(N )

1)

−

1
2

±

sgn

γ(N )

s(cid:18)
(cid:1)

(cid:0)

γ(N + 1) + γ(N

γ(N )

1)

−

2

−

4

1
(cid:18)

−

(cid:19)

r(N )
γ(N )

,
(cid:19)

(8)

we observe that one can always rule out one of the solutions (5) and (6) provided that
= aN +k. Therefore, it always sufﬁces to know two values of the autocovariance
aN
r such that aN
(0, 1) for
= aN +k, except the worst case scenario where aj = a
Z. A detailed analysis of this particular case is given in Appendix B.
every j

∈

∈

6
6
388

M. Voutilainen et al.

Remark 2. Consider a ﬁxed strictly stationary process X. If we ﬁx one value of the
autocovariance function of the noise such that Corollary 2 yields an unambiguous
AR(1) parameter, then the quadratic equations (4) will unravel the entire autoco-
variance function of the noise process. In comparison, conventionally, the noise is
assumed to be white — meaning that the entire autocovariance function of the noise
is assumed to be known a priori.

We end this section by observing that in the case of vanishing autocovariance
function of the noise, we get the following simpliﬁed form for the AR(1) parameter.
N be ﬁxed. Assume

Theorem 2. Let centered (Xt)t∈Z be of the form (3) and let N
that r(m) = 0 for every m

= 0, then for every n

N . If γ(N

1)

∈

N , we have

≥

≥

φ =

−
γ(n)

.

γ(n

1)

−

In particular, γ admits an exponential decay for n

N .

≥
= 0. It follows directly from (7) and the assumptions that

Proof. Let γ(N

1)

−

The condition γ(N

1)

−

γ(n) = φγ(n

1)

for every n

−
= 0 now implies the claim.

N.

≥

Recall that the representation (2) is unique only after H is ﬁxed. As a simple
corollary for Theorem 2 we obtain the following result giving some new information
about the uniqueness of the representation (2).

Corollary 3. Let X be a strictly stationary process with a non-vanishing autocovari-
ance. Then there exists at most one pair (H, G) satisfying (2) such that the non-zero
part of the autocovariance function of the increment process (∆tG)t∈Z is ﬁnite.
Proof. Assume that there exists H1, H2 > 0, and G1 ∈ G
H2 such that
the pairs (H1, G1) and (H2, G2) satisfy (2) and the autocovariances of (∆tG1)t∈Z
and (∆tG2)t∈Z have cut-off points. From Theorem 2 it follows that H1 = H2 and
since for a ﬁxed H the process G in (2) is unique, we get G1 = G2.

H1 and G2 ∈ G

3 Estimation

Corollary 2 gives natural estimators for φ provided that we have been able to choose
between (5) and (6), and that a value of r(n) is known. We emphasize that in our
model it is sufﬁcient to know only one (or in some cases two) of the values r(n),
whereas in conventional ARMA modeling much stronger assumptions are required.
(In fact, in conventional ARMA modeling the noise process is assumed to be white
noise.) It is also worth to mention that, generally, estimators of the parameters of
stationary processes are not expressible in a closed form. For example, this is the
case with the maximum likelihood and least squares estimators of conventionally
modeled ARMA processes, see [1]. Within our method, the model ﬁtting is simpler.
Finally, it is worth to note that assumption of one known value of r(n) is a natural one
and cannot be avoided. Indeed, this is a direct consequence of the fact that the pair

6
6
6
On model ﬁtting and estimation of strictly stationary processes

389

(φ, Z) in representation (3) is not unique. In fact, for practitioner, it is not absolutely
necessary to know any values of r(n). The practitioner may make an educated guess
and proceed in estimation. If the obtained estimate then turns out to be feasible, the
practitioner can stop there. If the obtained estimate turns out to be unreasonable (not
on the interval (0, 1)), then the practitioner have to make another educated guess. The
process is similar to selecting p and q in traditional ARMA(p, q) modeling.

Throughout this section, we assume that (X1, . . . , XT ) is an observed series from
a centered strictly stationary process that is modeled using the representation (3).
We use ˆγT (n) to denote an estimator of the corresponding autocovariance γ(n). For
example, ˆγT (n) can be given by

ˆγT (n) =

1
T

T −n

t=1
X

XtXt+n,

or more generally

ˆγT (n) =

1
T

T −n

t=1
X

(Xt

−

¯X)(Xt+n

¯X),

−

n

where ¯X is the sample mean of the observations. For this estimator the correspond-
ing sample covariance (function) matrix is positive semideﬁnite. On the other hand,
the estimator is biased while it is asymptotically unbiased. Another option is to use
T
1 as a denominator. In this case one has an unbiased estimator, but the
sample covariance (function) matrix is no longer positive deﬁnite. Obviously, both
estimators have the same asymptotic properties. Furthermore, for our purposes it is
irrelevant how the estimators ˆγT (n) are deﬁned, as long as they are consistent, and
the asymptotic distribution is known.

−

−

We next consider estimators of the parameter φ arising from characterization (3).
In this context, we pose some assumptions related to the autocovariance function of
the observed process X. The justiﬁcation and testing of these assumptions are dis-
cussed in Section 3.1. From a priori knowledge that φ
(0, 1) we enforce also the
estimators to the corresponding closed interval. However, if one prefers to use un-
bounded versions of the estimators, one may very well do that. The asymptotic prop-
erties are the same in both cases. We begin by deﬁning an estimator corresponding to
the second part (2) of Corollary 2.
Deﬁnition 4. Assume that γ(N ) = 0. Then we deﬁne

∈

ˆφT =

r(N )

−

ˆγT (N + 1) + ˆγT (N

−

1

ˆγT (N +1)+ˆγT (N −1)6=0

1)

(9)

whenever the right-hand side lies on the interval [0, 1]. If the right-hand side is below
zero, we set ˆφT = 0 and if the right-hand side is above one, we set ˆφT = 1.
Theorem 3. Assume that γ(N ) = 0 and r(N )
[ˆγT (N + 1), ˆγT (N

1)]⊤ is consistent, then ˆφT is consistent.

= 0. If the vector-valued estimator

−

Proof. Since γ(N ) = 0 and r(N )
γ(N
1)
mapping theorem.

= 0, Equation (4) guarantees that γ(N + 1) +
= 0. Therefore consistency of ˆφT follows directly from the continuous

−

6
6
6
390

M. Voutilainen et al.

Theorem 4. Let ˆφT be given by (4), and assume that γ(N ) = 0 and r(N )
γγγ = [γ(N + 1), γ(N

1)]⊤ and ˆγγγT = [ˆγT (N + 1), ˆγT (N

1)]⊤. If

−

−

l(T )(ˆγγγT −
for some covariance matrix Σ and some rate function l(T ), then

(000, Σ)

γγγ)

law
−→ N

l(T )( ˆφT

φ)

law
−→ N

−

000,

f (γγγ)⊤Σ

∇

f (γγγ)

∇

,

where

f (γγγ) is given by

∇

f (γγγ) =

∇

(cid:0)

r(N )

−

(γ(N + 1) + γ(N

1))2 ·

−

(cid:1)

.

1
1
(cid:21)

(cid:20)

= 0. Set

(10)

Proof. For the simplicity of notation, in the proof we use the unbounded version of
the estimator ˆφT . Since the true value of φ lies strictly between 0 and 1, the very
same result holds also for the bounded estimator of Deﬁnition 2. Indeed, this is a
simple consequence of the Slutsky’s theorem. To begin with, let us deﬁne an auxiliary
function f by

f (xxx) = f (x1, x2) =

r(N )
x1 + x2
= 0, the function f is smooth in a neighborhood of xxx. Since γ(N ) = 0
= 0, we may apply the

= 0 implies that γ(N + 1) + γ(N

1x1+x26=0.

1)

−

If x1 + x2 6
together with r(N )
delta method at xxx = γγγ to obtain

l(T )( ˆφT

φ) =

−

−

l(T )

f (ˆγγγT )

f (γγγ)

−

law
−→ N

000,

f (γγγ)⊤Σ

∇

f (γγγ)

∇

,

where

(cid:0)
f (γγγ) is given by (10). This concludes the proof.

(cid:1)

(cid:0)

∇

(cid:1)

Remark 3. By writing

Σ =

σ2
X
σXY

(cid:20)

σXY
σ2
Y (cid:21)

the variance of the limiting random variable reads

r(N )2

(γ(N + 1) + γ(N

1))4

−

(cid:0)

X + 2σXY + σ2
σ2
Y

.

(cid:1)

Remark 4. In many cases the convergency rate is the best possible, that is l(T ) =
√T . However, our results are valid with any rate function. One might, for example
in the case of many long memory processes, have other convergency rates for the
estimators ˆγT (n).

We continue by deﬁning an estimator corresponding to the ﬁrst part (1) of the
Corollary 2. For this we assume that, for reasons discussed in Section 2, we have
chosen the solution (5) (cf. Remark 5 and Section 3.1). As above, we show that con-
sistency and asymptotic normality follow from the same properties of the autocovari-
ance estimators. In the sequel we use a short notation

g(xxx) = g(x1, x2, x3) = (x1 + x3)2

4x2

x2 −

−

r(N )

.

(cid:0)

(cid:1)

(11)

6
6
6
On model ﬁtting and estimation of strictly stationary processes

391

In addition, we denote

γγγ =

γ(N + 1), γ(N ), γ(N

⊤

1)

−

and

(cid:2)
ˆγT (N + 1), ˆγT (N ), ˆγT (N

ˆγγγT =

(cid:3)
1)

⊤

.

= 0. We deﬁne an estimator for φ associated to (5)

−

(cid:3)

Deﬁnition 5. Assume that γ(N )
by

(cid:2)

ˆφT =

ˆγT (N + 1) + ˆγT (N

1) +
−
2ˆγT (N )

g(ˆγγγT )1g(ˆγγγT )>0

p

1

ˆγT (N )6=0

(12)

whenever the right-hand side lies on the interval [0, 1]. If the right-hand side is below
zero, we set ˆφT = 0 and if the right-hand side is above one, we set ˆφT = 1.
Theorem 5. Assume that γ(N )
given by (5). If ˆγγγT is consistent, then ˆφT is consistent.
Proof. As g(γγγ) > 0, the result is again a simple consequence of the continuous
mapping theorem.

= 0 and g(γγγ) > 0. Furthermore, assume that φ is

Before proving the asymptotic normality, we present some short notation. We set

CN =

γ(N + 1) + γ(N

−
γ(N )

1) +

g(γγγ)

p

and

Σφ =

g(γγγ)

⊤

Σ

∇

g(γγγ) + 2

1
4γ(N )2 
(cid:0)


⊤

∇

p

(cid:1)

p

,

1
CN
1 







Σ



−



1
CN
1 


+



−



where

(13)

(14)

⊤

1
CN
1 




−



Σ

∇

g(γγγ)

p

g(γγγ) =

∇

p

1
g(γγγ) 

γ(N + 1) + γ(N

1)

2(r(N )

γ(N + 1) + γ(N

−

−
2γ(N ))

.

1)


−

Theorem 6. Let the assumptions of Theorem 5 prevail. If

p



l(T )(ˆγγγT −

γγγ)

law
−→ N

(000, Σ)

for some covariance matrix Σ and some rate function l(T ), then l(T )( ˆφT
asymptotically normal with zero mean and variance given by (14).

−

φ) is

Proof. The proof follows the same lines as the proof of Theorem 4 but for the reader’s
convenience, we present the details. Furthermore, as in the proof of Theorem 4, since
the true value of φ lies strictly between 0 and 1, for the notational simplicity, we may
and will use the unbounded version of the estimator. Indeed, the asymptotics for the
bounded version then follow directly from the Slutsky’s theorem. We have

6
6
(cid:18)

ˆγT (N + 1)1
ˆγT (N )
1
ˆγT (N )
1
ˆγT (N )

=

=

(cid:0)

(cid:18)

Similarly

1)1
−
ˆγT (N )
1
ˆγT (N )

(cid:18)

(cid:18)

=

and

392

M. Voutilainen et al.

ˆγT (N )6=0

γ(N + 1)
γ(N )

−

(cid:19)

ˆγT (N + 1)1

ˆγT (N )6=0 −

γ(N + 1)

+

γ(N + 1)

ˆγT (N ) −

γ(N + 1)
γ(N )

ˆγT (N + 1)1

ˆγT (N )6=0 −

(cid:1)
γ(N + 1)

−

(cid:18)
γ(N + 1)
γ(N )

ˆγT (N )

−

(cid:0)

(cid:19)
γ(N )

.
(cid:19)
(cid:1)

ˆγT (N

ˆγT (N )6=0

γ(N

1)

−
γ(N )

−

(cid:19)

ˆγT (N

1)1

ˆγT (N )6=0 −

−

γ(N

1)

−

−

γ(N

1)

−
γ(N )

ˆγT (N )

−

(cid:0)

γ(N )

(cid:19)
(cid:1)

g(ˆγγγT )1g(ˆγγγT )>0
ˆγT (N )

(cid:18) p

1

ˆγT (N )6=0

=

1
ˆγT (N )

g(ˆγγγT )1g(ˆγγγT )>0

(cid:18)
For CN given in (13) we have

p

g(γγγ)
γ(N )

(cid:19)

− p
1
ˆγT (N )6=0 −

g(γγγ)
γ(N )

− p

g(γγγ)

p

ˆγT (N )

−

(cid:0)

γ(N )

.
(cid:19)
(cid:1)

l(T )( ˆφT

φ) =

−

l(T )
2ˆγ(N )
(cid:0)
+ ˆγT (N

+

ˆγT (N + 1)1

ˆγT (N )6=0 −
γ(N

1)1
g(ˆγγγT )1g(ˆγγγT )>0

−

ˆγT (N )6=0 −
−
1
ˆγT (N )6=0 −

γ(N + 1)

1)

CN

−
g(γγγ)

ˆγT (N )

γ(N )

−

.

(cid:0)

(cid:1)

By deﬁning

p

p

(cid:1)

h(xxx) = h(x1, x2, x3) =

x1 + x3 +

g(xxx)1g(xxx)>0

CN x2

1x26=0 −
(cid:1)

we have

(cid:0)

p

l(T )( ˆφT

φ) =

−

l(T )
2ˆγT (N )

h(ˆγγγT )

−

h(γγγ)

.

(15)

= 0 and g(xxx) > 0, the function h is smooth in a neighborhood of xxx. Therefore

If x2 6
we may apply the delta method at xxx = γγγ to obtain

(cid:0)

(cid:1)

l(T )

h(ˆγγγT )

h(γγγ)

−

law
−→ N

000,

h(γγγ)⊤Σ

∇

h(γγγ)

∇

,

(cid:0)

(cid:1)

(cid:0)

(cid:1)

where

h(γγγ)⊤Σ

∇

h(γγγ) =

∇

1
CN
1 






−





⊤

+

∇

g(γγγ)

p





Σ





−





1
CN
1 


+

∇

g(γγγ)



p



On model ﬁtting and estimation of strictly stationary processes

393

=

g(γγγ)

⊤

Σ

∇

g(γγγ) + 2

∇
(cid:0)

p

+



−

(cid:1)
⊤

1
CN
1 


p
1
CN
1 


−

.

Σ




Hence (15) and Slutsky’s theorem imply that l(T )( ˆφT
with zero mean and variance given by (14).



⊤

1
CN
1 




−



Σ

∇

g(γγγ)

p

φ) is asymptotically normal

−

Remark 5. One straightforwardly observes the same limiting behavior as in Theo-
rems 5 and 6 for the estimator related to (6). This fact also can be used to determine
which one of Equations (5) and (6) gives the correct φ (cf. Section 3.1).

Remark 6. If γ(N )

= 0 and g(γγγ) = 0 we may deﬁne an estimator

ˆφT =

ˆγT (N + 1) + ˆγT (N

2ˆγT (N )

1)

−

1
ˆγT (N )6=0.

Assuming that

l(T )(ˆγγγT −
it can be shown similarly as in the proofs of Theorems 4 and 6 that

(000, Σ)

γγγ)

law
−→ N

l(T )( ˆφT

φ)

−

0,

law
−→ N 



1
4γ(N )2 

−

1
γ(N +1)+γ(N −1)
γ(N )
1



⊤

Σ







−



1
γ(N +1)+γ(N −1)
γ(N )
1










Remark 7. The estimator related to Theorem 2 reads

ˆφT =

ˆγT (n + 1)
ˆγT (n)

1
ˆγT (n)6=0,

where we assume that γ(n)
shown that if

= 0. By using the same techniques as earlier, it can be

l(T )

ˆγT (n + 1)

−

γ(n + 1), ˆγT (n)

γ(n)

−

law−
→ N

then

(cid:0)

(cid:1)

σ2
X
σXY

000,

(cid:18)

(cid:20)

σXY
σ2
Y

,

(cid:21)(cid:19)

l(T )( ˆφT

φ)

law
−→ N

−

0,

(cid:18)

σ2
X
γ(n)2 +

γ(n + 1)2
γ(n)4

σ2
Y −

2

γ(n + 1)
γ(n)3

σXY

.
(cid:19)

Note that the asymptotics given in Remarks 6 and 7 hold also if one forces the

corresponding estimators to the interval [0, 1] as we did in Deﬁnitions 4 and 5.

3.1 Testing the underlying assumptions
When choosing the estimator that corresponds the situation at hand, we have to make
assumptions related to the values of γ(N ) (for some N ) and g(γγγ). In addition, we
have to consider the question of the choice between (5) and (6).

6
6
394

M. Voutilainen et al.

Let us ﬁrst discuss how to test the null hypothesis that γ(N ) = 0. If the null
hypothesis holds, then by asymptotic normality of the autocovariances, we have that

l(T )ˆγT (N )

with some σ2. Hence we may use

law
−→ N

0, σ2

(cid:0)

(cid:1)

(16)

ˆγT (N )

0,

σ2
l(T )2

(cid:19)

a

∼

N

(cid:18)

as a test statistics. A similar approach can be applied also when testing the null hy-
pothesis that g(γγγ) = 0, where g is deﬁned by (11). The alternative hypothesis is of
the form g(γγγ) > 0. Assuming that the null hypothesis holds, we obtain by the delta
method that

l(T )

g(ˆγγγT )

for some ˜σ2 justifying the use of

(cid:0)

g(ˆγγγT )

g(γγγ)

−

law
−→ N

0, ˜σ2

(cid:0)

(cid:1)

(cid:1)

0,

˜σ2
l(T )2

a

∼

N

(cid:18)

(cid:19)
= 0 and g(γγγ) > 0, then the
as a test statistics. If the tests above suggest that γ(N )
choice of the sign can be based on the discussion in Section 2. Namely, if for the ratio
aN = r(N )
1, then the sign is unambiguous. The sign
≥
of γ(N ) can be deduced from the previous testing of the null hypothesis γ(N ) = 0.
By (16), if necessary, one can test the null hypothesis γ(N ) = r(N ) using the test
statistics

γ(N ) it holds that aN

0 or aN

≤

ˆγT (N )

r(N ),

a

∼

N

(cid:18)

σ2
l(T )2

,
(cid:19)

where the alternative hypothesis is of the form r(N )
γ(N ) < 1. Finally, assume that one
wants to test if the null hypothesis aN = ak holds. By the delta method we obtain
that

l(T )(ˆaN

for some ¯σ2 suggesting that

ˆak

−

−

aN + ak)

law
−→ N

0, ¯σ2

(cid:0)

(cid:1)

ˆaN

ˆak

−

a

∼

N

¯σ2
l(T )2

(cid:19)

0,

(cid:18)

could be utilized as a test statistics.

4 Simulations

We present a simulation study to assess the ﬁnite sample performance of the estima-
tors. In the simulations, we apply the estimator corresponding to the ﬁrst part (1) of
Corollary 2. We simulate data from AR(1) processes and ARMA(1, 2) processes with
θ1 = 0.8 and θ2 = 0.3 as the MA parameters. (Note that these processes correspond
to Examples 1 and 2.) We assess the effects of the sample size T , AR(1) parameter

6
On model ﬁtting and estimation of strictly stationary processes

395

ϕ, and the chosen lag N . We consider the sample sizes T = 50, 500, 5000, 50000,
lags N = 1, 2, 3, . . . , 10, and the true parameter values ϕ = 0.1, 0.2, 0.3, . . . , 0.9.
For each combination, we simulate 1000 draws. The sample means of the obtained
estimates are tabulated in Appendix C.

Histograms given in Figures 1, 2 and 3 reﬂect the effects of the sample size T ,
AR(1) parameter ϕ, and the chosen lag N , respectively. In Figure 1, the parameter
ϕ = 0.5 and the lag N = 3. In Figure 2, the sample size T = 5000 and the lag
N = 3. In Figure 3, the parameter ϕ = 0.5 and the sample size T = 5000. The
summary statistics corresponding to the data displayed in the histograms are given in
Appendix C.

Figure 1 exempliﬁes the rate of convergence of the estimator as the number of
observations grows. One can see that with the smallest sample size, the lower bound
is hit numerous times due to the large variance of the estimator. In the upper series
of the histograms, the standard deviation reduces from 0.326 to 0.019, whereas in the
lower series it reduces from 0.250 to 0.008. The faster convergence in the case of
ARMA(1, 2) can be explained with the larger value of γ(3) reducing the variance in
comparison to the AR(1) case. The same phenomenon recurs also in the other two
ﬁgures.

Figure 2 reﬂects the effect of the AR(1) parameter on the value of γ(3) and con-
sequently on the variance of the estimator. The standard deviation reduces from 0.322
to 0.020 in the case of AR(1) and from 0.067 to 0.009 in the case of ARMA(1, 2).

In Figure 3 one can see how an increase in the lag increases the variance of the
estimator. In the topmost sequence, the standard deviation increases from 0.014 to
0.326 and in the bottom sequence from 0.015 to 0.282.

We wish to emphasize that in general smaller lag does not imply smaller variance,
since the autocovariance function of the observed process is not necessarily decreas-
ing. In addition, although the autocovariance γ(N ) appears to be the dominant factor
when it comes to the speed of convergence, there are also other possibly signiﬁcant
terms involved in the limit distribution of Theorem 6.

A Proof of Theorem 1

We provide here a detailed proof of Theorem 1. The continuous time version of the
theorem was recently proved in [17] and we loosely follow the same lines in our proof
for the discrete time version.

Deﬁnition 6. Let H > 0. A discrete time stochastic process Y = (Yet )t∈Z with
limt→−∞ Yet = 0 is H-self-similar if

(Yet+s )t∈Z

law
=

esH Yet

t∈Z

for every s

Z in the sense of ﬁnite-dimensional distributions.

(cid:1)

(cid:0)

∈

Deﬁnition 7. Let H > 0. In addition, let X = (Xt)t∈Z and Y = (Yet )t∈Z be
stochastic processes. We deﬁne the discrete Lamperti transform by

H X)et = etH Xt

(
L

396

M. Voutilainen et al.

Fig. 1. The effect of the sample size T on the estimates ˆϕ = ˆφ. The true parameter value
ϕ = 0.5 and the lag N = 3. The number of iterations is 1000

On model ﬁtting and estimation of strictly stationary processes

397

Fig. 2. The effect of the true parameter value ϕ on the estimates ˆϕ = ˆφ. The sample size
T = 5000 and the lag N = 3. The number of iterations is 1000

398

M. Voutilainen et al.

Fig. 3. The effect of the lag N on the estimates ˆϕ = ˆφ. The sample size T = 5000 and the
true parameter value ϕ = 0.5. The number of iterations is 1000

On model ﬁtting and estimation of strictly stationary processes

399

and its inverse by

−1
H Y
Theorem 7 (Lamperti [10]). If X = (Xt)t∈Z is strictly stationary, then (
H-self-similar. Conversely, if Y = (Yet )t∈Z is H-self-similar, then (
stationary.

t = e−tHYet .
(cid:1)

L
(cid:0)

L

H X)et is
−1
H Y )t is strictly

L

Lemma 3. Let H > 0 and assume that (Yet )t∈Z is H-self-similar. Let us denote
∆tYet = Yet

Yet−1 . Then the process (Gt)t∈Z deﬁned by

−

t
k=1 e−kH ∆kYek ,

0
k=t+1 e−kH ∆kYek ,

0,
P
−

t
1
≥
t = 0
t

≤ −

1

P

Gt =






(17)

belongs to

H .

G

Proof. By studying the cases t
straightforward to see that

≥

2, t = 1, t = 0 and t

1 separately, it is

≤ −

∆tG = e−tH ∆tYet

for every t

Z.

∈

(18)

Now

0

lim
k→−∞

etH ∆tG = lim
k→−∞

t=k
X

and since Y is self-similar, we have

0

t=k
X

∆tYet = Ye0

lim
k→−∞

Yek

−

Thus

Yek

law
= ekH Ye0 .

lim
k→−∞

Yek = 0

in distribution, and hence also in probability. This implies that

etH ∆tG

0

t=−∞
X

is an almost surely ﬁnite random variable. Next we show that G has strictly stationary
increments. For this, assume that t, s, l

Z with t > s are arbitrary. Then

∈

Gt

−

Gs =

law
=

t

t

t+l

∆kG =

e−kH ∆kYek =

e−(j−l)H ∆j−lYej−l

k=s+1
X
t+l

j=s+l+1
X

k=s+1
X

j=s+l+1
X

e−jH ∆jYej = Gt+l

Gs+l,

−

where the equality in law follows from H-self-similarity of (Yet )t∈Z. Treating n-
dimensional vectors similarly concludes the proof.

400

M. Voutilainen et al.

Proof of Theorem 1. Assume ﬁrst that X is strictly stationary. In this case X clearly
satisﬁes the limit condition. In addition, there exists a H-self-similar Y such that

∆tX = e−tH Yet
=

e−H
e−H
(cid:0)

=

−

−

−
1

e−(t−1)HYet−1
e−(t−1)HYet−1 + e−tH (Yet
Xt−1 + e−tH ∆tYet .
(cid:1)

1

Yet−1 )

−

Deﬁning the process G as in Lemma 3 completes the proof of the ‘if’ part. For the
proof of the ‘only if’ part, assume that G

H . From (2) it follows that

(cid:1)

(cid:0)

∈ G
Xt = e−H Xt−1 + ∆tG = e−2HXt−2 + e−H∆t−1G + ∆tG

n

=

e−jH ∆t−jG + e−(n+1)H Xt−n−1

j=0
X

= e−tH

t

k=t−n
X

ekH ∆kG + e(t−n−1)H Xt−n−1

!

for every n
obtain that

∈

N. Since G

H and limm→−∞ emHXm = 0 in probability, we

∈ G

t

Xt = e−tH

ekH ∆kG

Z. Now, by strictly stationary increments of G, we have

k=−∞
X

for every t

∈

t

e−tH

ejH ∆j+sG law

= e−tH

t

j=−M
X

ejH ∆jG.

M

−

≤

t. Since the sums above converge as M tends to

j=−M
X
Z such that

for every t, M
inﬁnity, we obtain

∈

Xt+s = e−(t+s)H

t

j=−∞
X

e(j+s)H ∆j+sG law

= e−tH

ejH ∆jG = Xt.

t

j=−∞
X

Treating multidimensional distributions similarly we thus observe that X is strictly
H such that
stationary. Finally, to prove the uniqueness assume there exist G1, G2 ∈ G

t

t

etH Xt =

ekH ∆kG1 =

ekH ∆kG2

for every t

Z. Then

∈

k=−∞
X

k=−∞
X

etH Xt

−

e(t−1)H Xt−1 = etH∆tG1 = etH ∆tG2.

Hence ∆tG1 = ∆tG2 for every t
processes are zero at t = 0, it must hold that c = 0.

∈

Z implying that G1 = G2 + c. Since both

Remark 8. Corollary 1 is almost trivial. However, it is well motivated by Theorem
1. On the other hand, Theorem 1 is far away from trivial as it states both sufﬁcient

 
On model ﬁtting and estimation of strictly stationary processes

401

and necessary conditions. We prove Theorem 1 using discrete Lamperti transform.
In principle, one could consider proving Theorem 1 by starting from Corollary 1.
However, at this point, we have not assumed any moment conditions, and thus it is
not clear whether a process G constructed from Z (H) of Corollary 1 would satisfy
H . Indeed, a counter example is provided in [17, Proposition 2.1.]. See also
G
[17, Theorem 2.2.], where moment conditions are discussed.

∈ G

B Discussion on special cases

In this appendix we take a closer look at “worst case scenario” processes related to
the choice between (5) and (6). These are such processes that, for some 0 < a < 1,
aj = a for every j

Z. By (4) this is equivalent to

∈

γ(j + 1) + γ(j

γ(j)

1)

−

= b

(19)

for every j
consider formal power series.

Z, where φ < b < φ + 1

∈

φ . In order to study processes of this form, we

Deﬁnition 8. Let

∞

f (x) =

cnxn

be a formal power series in x. We now deﬁne the coefﬁcient extractor operator [
by

n=0
X

]

·

{∗}

= cm
Setting j = 0 in (19) we obtain that γ(1) = b

f (x)

(cid:2)

(cid:3)(cid:8)

(cid:9)

xm

2 γ(0). This leads to the following

recursion.

γ(n) = bγ(n

1)

γ(n

2)

for n

2.

(20)

−

−

−

≥

It follows immediately from the ﬁrst step of the recursion that b > 2 does not deﬁne
an autocovariance function of a stationary process. Note also that for b = 2 Equation
Z. This corresponds to the completely
(20) implies that γ(n) = γ(0) for every n
degenerate process Xn = X0. We next study the case 0 < b < 2. For this, we deﬁne
a generating function regarded as a formal power series by

∈

∞

f (x) =

γ(n)xn.

n=0
X

(21)

Then the coefﬁcients of f (x) satisfy

xn

f (x)

= b

(cid:2)

(cid:3)(cid:8)

(cid:9)

=

=

(cid:2)

f (x)

xn−1
xn
(cid:2)
xn

(cid:3)(cid:8)

bxf (x)
(cid:3)(cid:8)
bxf (x)

−

xn−2
xn
(cid:2)
(cid:9)
−
x2f (x)
(cid:3)(cid:8)
(cid:2)

(cid:9)
−

f (x)
x2f (x)
(cid:3)(cid:8)

(cid:9)

(cid:9)

for n
(cid:3)(cid:8)
ﬁrst order terms into account we obtain

≥

(cid:2)

2. For simplicity, we assume that γ(0) = 1. By taking the constant and the

(cid:9)

f (x) = bxf (x)

x2f (x)

−

−

bx + 1 +

b
2

x,

402

which implies

M. Voutilainen et al.

f (x) =

1

b
2 x
−
bx + 1
−

.

x2
Since the function above is analytic at x = 0, the corresponding power series expan-
sion is (21). Furthermore, since the recursion formula is linear, for a general γ(0) it
holds that

γ(n) = γ(0)

xn

(cid:2)

b
2

−

1
((cid:18)
(cid:3)

C Tables

∞

x

bx

(cid:19)

n=0
X
(cid:0)

n

x2

−

(cid:1)

.

)

The simulation results highlighted in Section 4 are chosen from a more extensive
set of simulations. All the simulation results are given in a tabulated form in this ap-
pendix. The two processes considered in the simulations are AR(1) and ARMA(1, 2).
The used MA parameters are θ1 = 0.8 and θ2 = 0.3. The tables represent the efﬁ-
ciency dependence of the estimator on the AR(1) parameter ϕ and the used lag N .
We have varied the column variable AR(1) parameter from 0.1 to 0.9 and the row
variable lag from 1 to 10. The tables display the sample means of the estimates from
1000 iterations with different sample sizes. At the end of this appendix, we provide
summary statistics tables corresponding to the histograms presented in Section 4.

N/ϕ
1
2
3
4
5
6
7
8
9
10

0.1
0.10
0.25
0.32
0.30
0.33
0.34
0.32
0.32
0.31
0.34

0.2
0.18
0.26
0.35
0.37
0.39
0.37
0.37
0.34
0.37
0.35

0.3
0.27
0.30
0.35
0.42
0.42
0.42
0.43
0.45
0.44
0.43

0.4
0.38
0.35
0.40
0.47
0.50
0.47
0.49
0.51
0.50
0.51

0.5
0.48
0.45
0.41
0.50
0.53
0.56
0.57
0.57
0.59
0.58

0.6
0.57
0.54
0.48
0.52
0.57
0.60
0.60
0.64
0.64
0.64

0.7
0.67
0.64
0.57
0.55
0.61
0.66
0.68
0.69
0.70
0.70

0.8
0.77
0.74
0.69
0.66
0.65
0.69
0.69
0.72
0.73
0.75

0.9
0.85
0.82
0.80
0.77
0.75
0.74
0.75
0.76
0.78
0.78

Table 1. The sample means of the parameter estimates ˆϕ = ˆφ for AR(1) processes with
different parameter values ϕ using lags N = 1, 2, 3, . . . , 10. The sample size is 50 and the
number of iterations is 1000

N/ϕ
1
2
3
4
5
6
7
8
9
10

0.1
0.10
0.23
0.29
0.32
0.30
0.30
0.30
0.32
0.30
0.32

0.2
0.20
0.24
0.31
0.37
0.37
0.36
0.37
0.39
0.38
0.39

0.3
0.30
0.30
0.34
0.40
0.42
0.44
0.44
0.44
0.45
0.46

0.4
0.40
0.40
0.40
0.40
0.48
0.47
0.49
0.51
0.51
0.52

0.5
0.50
0.51
0.50
0.49
0.50
0.53
0.53
0.57
0.59
0.58

0.6
0.60
0.60
0.61
0.61
0.58
0.58
0.57
0.61
0.63
0.64

0.7
0.70
0.70
0.71
0.70
0.70
0.68
0.65
0.68
0.68
0.70

0.8
0.80
0.80
0.81
0.81
0.81
0.80
0.79
0.76
0.75
0.75

0.9
0.90
0.91
0.91
0.90
0.90
0.90
0.90
0.90
0.89
0.89

Table 2. The sample means of the parameter estimates ˆϕ = ˆφ for AR(1) processes with
different parameter values ϕ using lags N = 1, 2, 3, . . . , 10. The sample size is 500 and the
number of iterations is 1000

On model ﬁtting and estimation of strictly stationary processes

403

N/ϕ
1
2
3
4
5
6
7
8
9
10

0.1
0.10
0.13
0.26
0.30
0.29
0.31
0.29
0.29
0.32
0.29

0.2
0.20
0.20
0.27
0.32
0.37
0.37
0.38
0.40
0.37
0.37

0.3
0.30
0.30
0.32
0.34
0.38
0.41
0.40
0.45
0.41
0.41

0.4
0.40
0.40
0.40
0.42
0.43
0.45
0.47
0.51
0.50
0.51

0.5
0.50
0.50
0.50
0.51
0.51
0.49
0.52
0.54
0.54
0.57

0.6
0.60
0.60
0.60
0.61
0.62
0.62
0.59
0.58
0.60
0.61

0.7
0.70
0.70
0.70
0.70
0.71
0.71
0.72
0.71
0.68
0.68

0.8
0.80
0.80
0.80
0.80
0.80
0.81
0.81
0.81
0.82
0.82

0.9
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.91
0.91
0.91

Table 3. The sample means of the parameter estimates ˆϕ = ˆφ for AR(1) processes with
different parameter values ϕ using lags N = 1, 2, 3, . . . , 10. The sample size is 5000 and the
number of iterations is 1000

N/ϕ
1
2
3
4
5
6
7
8
9
10

0.1
0.10
0.10
0.21
0.28
0.29
0.29
0.29
0.31
0.31
0.32

0.2
0.20
0.20
0.21
0.30
0.34
0.37
0.37
0.37
0.35
0.37

0.3
0.30
0.30
0.30
0.33
0.36
0.39
0.44
0.43
0.43
0.42

0.4
0.40
0.40
0.40
0.40
0.41
0.42
0.45
0.48
0.49
0.48

0.5
0.50
0.50
0.50
0.50
0.51
0.52
0.51
0.49
0.53
0.53

0.6
0.60
0.60
0.60
0.60
0.60
0.60
0.61
0.62
0.60
0.58

0.7
0.70
0.70
0.70
0.70
0.70
0.70
0.70
0.70
0.71
0.72

0.8
0.80
0.80
0.80
0.80
0.80
0.80
0.80
0.80
0.80
0.80

0.9
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90

Table 4. The sample means of the parameter estimates ˆϕ = ˆφ for AR(1) processes with
different parameter values ϕ using lags N = 1, 2, 3, . . . , 10. The sample size is 50000 and the
number of iterations is 1000

N/ϕ
1
2
3
4
5
6
7
8
9
10

0.1
0.08
0.09
0.32
0.65
0.64
0.64
0.64
0.65
0.63
0.65

0.2
0.14
0.13
0.33
0.66
0.67
0.68
0.67
0.67
0.68
0.68

0.3
0.22
0.20
0.32
0.62
0.69
0.69
0.70
0.71
0.72
0.70

0.4
0.32
0.30
0.34
0.60
0.70
0.72
0.72
0.72
0.74
0.74

0.5
0.41
0.39
0.40
0.60
0.69
0.74
0.77
0.76
0.78
0.78

0.6
0.52
0.50
0.46
0.56
0.69
0.75
0.78
0.79
0.80
0.80

0.7
0.61
0.60
0.58
0.60
0.69
0.76
0.79
0.81
0.82
0.83

0.8
0.72
0.72
0.71
0.68
0.70
0.75
0.79
0.80
0.83
0.85

0.9
0.81
0.82
0.81
0.78
0.77
0.78
0.80
0.83
0.84
0.85

Table 5. The sample means of the parameter estimates ˆϕ = ˆφ for ARMA(1, 2) processes with
different parameter values ϕ using lags N = 1, 2, 3, . . . , 10. The MA parameters θ1 = 0.8
and θ2 = 0.3, the sample size is 50 and the number of iterations is 1000

404

M. Voutilainen et al.

N/ϕ
1
2
3
4
5
6
7
8
9
10

0.1
0.09
0.09
0.12
0.58
0.64
0.65
0.66
0.66
0.66
0.65

0.2
0.19
0.19
0.18
0.49
0.65
0.68
0.68
0.68
0.68
0.68

0.3
0.29
0.28
0.26
0.38
0.62
0.67
0.69
0.71
0.71
0.71

0.4
0.39
0.39
0.37
0.37
0.57
0.68
0.71
0.72
0.74
0.74

0.5
0.49
0.49
0.48
0.45
0.52
0.66
0.72
0.75
0.76
0.77

0.6
0.59
0.59
0.59
0.57
0.56
0.61
0.69
0.73
0.75
0.78

0.7
0.69
0.69
0.69
0.69
0.68
0.67
0.69
0.72
0.76
0.78

0.8
0.80
0.79
0.79
0.79
0.79
0.79
0.78
0.78
0.77
0.78

0.9
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.89

Table 6. The sample means of the parameter estimates ˆϕ = ˆφ for ARMA(1, 2) processes with
different parameter values ϕ using lags N = 1, 2, 3, . . . , 10. The MA parameters θ1 = 0.8
and θ2 = 0.3, the sample size is 500 and the number of iterations is 1000

N/ϕ
1
2
3
4
5
6
7
8
9
10

0.1
0.10
0.10
0.10
0.34
0.61
0.65
0.63
0.64
0.65
0.64

0.2
0.20
0.20
0.19
0.21
0.55
0.65
0.68
0.68
0.69
0.67

0.3
0.30
0.30
0.30
0.27
0.40
0.62
0.67
0.68
0.69
0.71

0.4
0.40
0.40
0.40
0.39
0.37
0.50
0.63
0.71
0.73
0.75

0.5
0.50
0.50
0.50
0.50
0.48
0.48
0.58
0.69
0.71
0.74

0.6
0.60
0.60
0.60
0.60
0.60
0.59
0.57
0.60
0.67
0.74

0.7
0.70
0.70
0.70
0.70
0.70
0.70
0.70
0.69
0.68
0.69

0.8
0.80
0.80
0.80
0.80
0.80
0.80
0.80
0.80
0.80
0.80

0.9
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90

Table 7. The sample means of the parameter estimates ˆϕ = ˆφ for ARMA(1, 2) processes with
different parameter values ϕ using lags N = 1, 2, 3, . . . , 10. The MA parameters θ1 = 0.8
and θ2 = 0.3, the sample size is 5000 and the number of iterations is 1000

N/ϕ
1
2
3
4
5
6
7
8
9
10

0.1
0.10
0.10
0.10
0.13
0.56
0.62
0.63
0.64
0.62
0.65

0.2
0.20
0.20
0.20
0.19
0.30
0.60
0.65
0.66
0.67
0.67

0.3
0.30
0.30
0.30
0.30
0.28
0.41
0.63
0.68
0.69
0.71

0.4
0.40
0.40
0.40
0.40
0.39
0.37
0.46
0.63
0.71
0.73

0.5
0.50
0.50
0.50
0.50
0.50
0.50
0.47
0.49
0.60
0.70

0.6
0.60
0.60
0.60
0.60
0.60
0.60
0.60
0.59
0.58
0.59

0.7
0.70
0.70
0.70
0.70
0.70
0.70
0.70
0.70
0.70
0.70

0.8
0.80
0.80
0.80
0.80
0.80
0.80
0.80
0.80
0.80
0.80

0.9
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90
0.90

Table 8. The sample means of the parameter estimates ˆϕ = ˆφ for ARMA(1, 2) processes with
different parameter values ϕ using lags N = 1, 2, 3, . . . , 10. The MA parameters θ1 = 0.8
and θ2 = 0.3, the sample size is 50000 and the number of iterations is 1000

T
50
500
5000
50000

max
1.00
0.999
0.726
0.561

min
0.00
0.00
0.319
0.443

mean median
0.409
0.413
0.495
0.502
0.497
0.501
0.502
0.501

sd
0.326
0.218
0.058
0.019

mad
0.436
0.187
0.056
0.019

skewness
0.222
0.207
0.456
-0.058

Table 9. The effect of the sample size T on the estimates ˆϕ = ˆφ for an AR(1) process. The
true parameter value ϕ = 0.5 and the lag N = 3. The number of iterations is 1000

On model ﬁtting and estimation of strictly stationary processes

405

T
50
500
5000
50000

max
0.999
0.681
0.570
0.527

min
0.00
0.00
0.395
0.474

mean median
0.425
0.399
0.491
0.481
0.500
0.499
0.500
0.500

sd
0.250
0.086
0.024
0.008

mad
0.264
0.078
0.023
0.007

skewness
-0.062
-1.020
-0.201
0.036

Table 10. The effect of the sample size T on the estimates ˆϕ = ˆφ for an ARMA(1, 2) process.
The MA parameters θ1 = 0.8 and θ2 = 0.3, the true parameter value ϕ = 0.5 and the lag
N = 3. The number of iterations is 1000

ϕ
0.1
0.4
0.6
0.9

max
1.00
0.989
0.738
1.00

median
0.097
0.395
0.601
0.899
Table 11. The effect of the true parameter value ϕ on the estimates ˆϕ = ˆφ for AR(1) processes.
The sample size T = 5000 and the lag N = 3. The number of iterations is 1000

skewness
1.056
0.500
0.211
0.943

mean
0.257
0.396
0.602
0.901

mad
0.144
0.083
0.043
0.018

sd
0.322
0.096
0.041
0.020

min
0.00
0.111
0.476
0.852

ϕ
0.1
0.4
0.6
0.9

max
0.273
0.496
0.650
0.929

min
0.00
0.254
0.540
0.868

median
0.098
0.397
0.600
0.899
Table 12. The effect of the true parameter value ϕ on the estimates ˆϕ = ˆφ for ARMA(1, 2)
processes. The MA parameters θ1 = 0.8 and θ2 = 0.3, the sample size T = 5000 and the lag
N = 3. The number of iterations is 1000

skewness
0.144
-0.198
-0.061
-0.076

mean
0.096
0.396
0.600
0.899

mad
0.082
0.032
0.019
0.009

sd
0.067
0.032
0.018
0.009

N
1
3
5
7

max
0.550
0.726
1.00
1.00

median
0.501
0.497
0.493
0.558
Table 13. The effect of the lag N on the estimates ˆϕ = ˆφ for an AR(1) process. The sample
size T = 5000 and the true parameter value ϕ = 0.5. The number of iterations is 1000

skewness
0.017
0.456
0.098
-0.216

mean
0.501
0.501
0.513
0.525

mad
0.015
0.056
0.226
0.395

sd
0.014
0.058
0.246
0.326

min
0.457
0.319
0.00
0.00

N
1
3
5
7

max
0.548
0.570
0.710
1.00

min
0.455
0.395
0.00
0.00

median
0.500
0.500
0.499
0.613
Table 14. The effect of the lag N on the estimates ˆϕ = ˆφ for an ARMA(1, 2) process. The
MA parameters θ1 = 0.8 and θ2 = 0.3, the sample size T = 5000 and the true parameter
value ϕ = 0.5. The number of iterations is 1000

skewness
0.134
-0.201
-1.456
-0.488

mean
0.500
0.499
0.482
0.576

mad
0.016
0.023
0.092
0.275

sd
0.015
0.024
0.112
0.282

References

[1] Brockwell, P.J., Davis, R.A.: Time Series: Theory and Methods, 2nd edn. Springer, New

York (1991). MR1093459

406

M. Voutilainen et al.

[2] Davis, R., Resnick, S.: Limit theory for the sample covariance and correlation functions
of moving averages. The Annals of Statistics 14(2), 533–558 (1986). MR0840513
[3] Francq, C., Zakoïan, J.-M.: Maximum likelihood estimation of pure GARCH and

ARMA-GARCH processes. Bernoulli 10(4), 605–637 (2004)

[4] Francq, C., Roy, R., Zakoïan, J.-M.: Diagnostic checking in ARMA models with uncor-
related errors. Journal of the American Statistical Association 100(470), 532–544 (2005)
[5] Hamilton, J.D.: Time Series Analysis, 1st edn. Princeton university press, Princeton

(1994). MR1278033

[6] Hannan, E.J.: The estimation of the order of an ARMA process. The Annals of Statistics

8(5), 1071–1081 (1980). MR0585705

[7] Hannan, E.J.: The asymptotic theory of linear time-series models. Journal of Applied

Probability 10(1), 130–145 (1973)

[8] Horváth, L., Kokoszka, P.: Sample autocovariances of long-memory time series.

Bernoulli 14(2), 405–418 (2008). MR2544094

[9] Koreisha, S., Pukkila, T.: A generalized least-squares approach for estimation of autore-
gressive moving-average models. Journal of Time Series Analysis 11(2), 139–151 (1990)
[10] Lamperti, J.: Semi-stable stochastic processes. Transactions of the American mathemati-

cal Society 104(1), 62–78 (1962)

[11] Lévy-Leduc, C., Boistard, H., Moulines, E., Taqqu, M.S., Reisen, V.A.: Robust estima-
tion of the scale and of the autocovariance function of Gaussian short-and long-range
dependent processes. Journal of Time Series Analysis 32(2), 135–156 (2011)

[12] Lin, J.-W., McLeod, A.I.: Portmanteau tests for ARMA models with inﬁnite variance.

Journal of Time Series Analysis 29(3), 600–617 (2008)

[13] Ling, S., Li, W.: On fractionally integrated autoregressive moving-average time series
models with conditional heteroscedasticity. Journal of the American Statistical Associa-
tion 92(439), 1184–1194 (1997)

[14] Mauricio, J.A.: Exact maximum likelihood estimation of stationary vector ARMA mod-

els. Journal of the American Statistical Association 90(429), 282–291 (1995)

[15] McElroy, T., Jach, A.: Subsampling inference for the autocovariances and autocorrela-
tions of long-memory heavy-tailed linear time series. Journal of Time Series Analysis
33(6), 935–953 (2012)

[16] Mikosch, T., Gadrich, T., Kluppelberg, C., Adler, R.J.: Parameter estimation for ARMA
models with inﬁnite variance innovations. The Annals of Statistics 23(1), 305–326 (1995)
[17] Viitasaari, L.: Representation of stationary and stationary increment processes via
Langevin equation and self-similar processes. Statistics & Probability Letters 115, 45–
53 (2016)

[18] Yao, Q., Brockwell, P.J.: Gaussian maximum likelihood estimation for ARMA models. I.

time series. Journal of Time Series Analysis 27(6), 857–875 (2006)

