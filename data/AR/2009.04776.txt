0
2
0
2

p
e
S
3
1

]

V
C
.
s
c
[

2
v
6
7
7
4
0
.
9
0
0
2
:
v
i
X
r
a

Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D
sensors

Akhmedkhan Shabanov1

Ilya Krotov1

Sergei Kozlukov2

Igor Pasechnik3
Vadim Lebedev1

Nikolay Chinaev1
Bulat Yakupov1

Vsevolod Poletaev1

Artsiom Sanakoyeu4

Dmitry Ulyanov1

Abstract

Consumer-level depth cameras and depth sensors em-
bedded in mobile devices enable numerous applications,
such as AR games and face identiﬁcation. However, the
quality of the captured depth is sometimes insufﬁcient for
3D reconstruction,
tracking and other computer vision
tasks.
In this paper, we propose a self-supervised depth
denoising approach to denoise and reﬁne depth coming
from a low quality sensor. We record simultaneous RGB-D
sequences with unzynchronized lower- and higher-quality
cameras and solve a challenging problem of aligning se-
quences both temporally and spatially. We then learn a
deep neural network to denoise the lower-quality depth
using the matched higher-quality data as a source of su-
pervision signal. We experimentally validate our method
against state-of-the-art ﬁltering-based and deep denoising
techniques and show its application for 3D object recon-
struction tasks where our approach leads to more detailed
fused surfaces and better tracking.

(a)

(b) LQ depth

(c) Ours

Figure 1: We use lower- and higher-quality depth sensors
(a) to record simultaneous RGB-D sequences. Our auto-
matic data collection pipeline allows us to train a model
for denoising the data coming from the lower-quality (LQ)
sensor (b). In ﬁgure (c) we show a result produced by our
method for the input (b).

1. Introduction

A depth sensor is an extremely valuable tool for many
computer vision applications, and the latest generations of
smartphones often include one. These sensors provide an
important cue about the three-dimensional world, supple-
menting the output of a standard RGB camera. Depth
data is used to create a bokeh effect, to perform 3D recon-
struction, to enable a more immersive experience in aug-
mented/virtual reality and games. Unfortunately, the full
utilization of depth sensors is sometimes complicated by
their insufﬁcient accuracy. Figure 1 (b) shows an example
of a depth map captured using a TrueDepth camera of an
iPhone X.

Low signal-to-noise ratio of mobile depth sensors pre-

1In3D, 2Skoltech, 3Samsung AI Center, Moscow, 4Heidelberg Col-

laboratory for Image Processing.
Contact e-mail: shabanov.ae@phystech.edu

vents the use of the existing well-developed computer vi-
sion methods [27, 26, 6, 47] that are designed to work with
more accurate sensors, such as Microsoft Kinect V2 [33].
Therefore, there is a need to intelligently denoise the depth
coming from mobile sensors. Classic denoising methods
based on ﬁltering [38, 19, 49] are not able to work well
with noise of such magnitude and lead to over-smoothing
and loss of details. The development of data-driven and
supervised approaches is complicated as it is highly non-
trivial and sometimes even impossible to collect ground
truth data for supervision. Recently, [46, 16, 21] proposed
to synthesize ground truth by adopting depth fusion meth-
ods [27, 6, 47]. To overcome the lack of ground truth
data, Sterzentsenko et al. [36] used multiple sensors and
self-supervised photometric reprojection loss to train a deep
neural network.

In this paper we propose a learning-based method for
depth denoising of lower-quality (LQ) depth sensor’s out-

1

 
 
 
 
 
 
put using supervision of a higher-quality (HQ) depth sen-
sor. We use a very simple but ﬂexible method to acquire
ground truth data. We place LQ and HQ sensors nearby
and record a set of simultaneous RGB-D sequences. We
consider an in-the-wild scenario where the hardware clock
synchronization and prior extrinsic calibration for the sen-
sors is not possible. We perform automatic alignment of the
recorded sequences both in time and space by estimating the
time delta and an extrinsic transform between the devices.
We then use the frames shot around the same moment in
time to train a deep network to transform a depth image
from the LQ sensor to a corresponding depth image from
the HQ sensor reprojected to the LQ camera view. Note
that our self-supervised approach extracts supervision sig-
nal automatically from the data and does not require any
human labeling.

While the proposed method can be used with any pair
of RGB-D sensors, in this work, we aim to denoise lower-
quality Apple TrueDepth (TD) camera and use Microsoft
Kinect V2 (K2) as supervision. We focus on an application
of 3D body scanning challenging to do with a TD sensor. In
fact, designed with the task of face identiﬁcation in mind,
the TD camera is tuned for close objects, and quality of
the produced data quickly deteriorates at the distance of 1-2
meters that is required to ﬁt a standing person into the ﬁeld
of view. Therefore, we create a dataset with people captured
in a variety of poses and lightning conditions using a rig
shown in ﬁg. 1. To our knowledge, our dataset is the ﬁrst
to include RGB-D sequences recorded simultaneously with
different quality RGB-D sensors.

We extensively test the proposed method against state-
of-the-art ﬁltering and deep-learning based algorithms. We
demonstrate the effectiveness of our learned denoising in
3D surface reconstruction applications. Developed for sen-
sors such as K2, KinectFusion [27] and DoubleFusion [47]
fail to work with noisy sensors such as TD and our approach
enables usage of these reconstruction algorithms without
any modiﬁcations.

To summarize, our contributions are three-fold:

• Our main contribution is a pipeline for collecting
ground truth data for a task of depth denoising. Specif-
ically, we propose a method for temporal and spatial
alignment of RBG-D sequences, removing the need in
accurate checkerboard calibration and hardware clock
synchronization.

• We propose a data-efﬁcient training method based on
out-of-fold training paradigm. This approach enables
us to train a convolutional LSTM using very limited
amount of data.

2. Related work

Classic ﬁltration-based approaches such as bilateral ﬁl-
tering [38] are powerful sensor-independent single-image
approaches, yet are not applicable to high noise scenarios.
Several modiﬁcations [19, 49, 15] use the corresponding
color information to guide depth ﬁltering process, but suf-
fer from texture copying artifacts. Richardt et al. [31] per-
form depth upsampling for a depth stream using joint bilat-
eral ﬁltering for spatial ﬁltering and optical ﬂow for tempo-
ral ﬁltering. Yuan et al. [48] stack 240 FPS color camera
and 30 FPS depth camera and propose an algorithm to re-
construct intermediate depth maps and estimate scene ﬂow.
Similarly to us, they use a pair of RGB-D sensors to build
an aligned dataset, yet their application is completely dif-
ferent. A number of works [12, 44, 28, 10, 11] use shape-
from-shading techniques with shading extracted from an
RGB image to optimize for a detailed depth map. However,
these methods require precise alignment of depth and color
modalities and rely on strong assumptions about object’s
materials and albedo. Several papers [35, 9, 25, 1] tackle
a challenging problem of denoising Time-of-Flight (ToF)
data. Son et al. [35] use high-precision structured light sen-
sor to capture groundtruth depth along with ToF camera
measurements and learn a neural network to remove mul-
tipath distortions. In [34, 2, 13], authors explicitly model
the noise and explore parametric models for depth denois-
ing and camera undistortion. In [17], Kaiser et al. ﬁt primi-
tives such as planes to depth data to perform denoising and
hole ﬁlling.

Recently, [14, 37, 23, 51] used deep convolutional neural
networks to complete and upsample low-resolution depth
maps using a corresponding high-resolution intensity im-
age. Jeon et al. [16] use depth fusion [6] for scene recon-
struction and synthesize groundtruth depth using the esti-
mated surface and camera poses of each frame. They utilize
a large dataset of indoor scenes [5] and propose a methodol-
ogy for ﬁltering wrong groundtruth depth patches. Having
a dataset prepared, they learn a convolutional neural net-
work for denoising. In [21], the authors use another recon-
struction method [27] to train their multi-scale dictionary
of geometric primitives. Yan et al. [46] take a similar ap-
proach but focuses on human bodies. They use Double-
Fusion [47] to fuse depth sequences of non-rigidly moving
people. Although depth aggregation from multiple frames
indeed leads to less noisy surfaces, fusion methods fre-
quently fail to build consistent geometry and tracking errors
lead to over-smoothing and loss of details.

The method of Sterzentsenko et al. [36] is the most re-
lated to ours. Similarly to us, the authors use simultaneous
recording with multiple well-synchronized Intel RealSense
D415 sensors and train a deep network to perform denois-
ing. In this paper we tackle a more complicated case, when
the sensors can’t be precisely synchronized, have different

frame rate and intrinsic parameters. Moreover, rather than
using a complex color re-projection loss as in [36] we use
a simple L1 loss and re-projected depth frame as ground
truth.

Also there is a proposed Noise2Noise framework [22,
3, 20] reduces the need in clean groundtruth data yet relies
on strong assumptions about the noise model such as spa-
tial independence and zero mean value. Real-world sensors
exhibit a complex noise model[42] which limits the appli-
cability of the mentioned approaches.

3. Overview

Our

depth

L, Sk

self-supervised

approach
reﬁnement
We create a dataset
starts with data collection.
{(Sk
H): k∈[1 . . K]} of K paired sequences by si-
multaneously capturing streams Sk
H from lower-
and higher-quality sensors, respectively. In the following
text, we omit the sequence index k for notational clar-
ity. Each sequence is deﬁned by a set of measurements
endowed with timestamps:

L and Sk

SL = { (Ci
SH = { (Ci

L, Di
H, Di

L, ti
H, ti

L): i ∈ [1 . . NL]} ,
H): i ∈ [1 . . NH]} .

(1)

Here, Ci, Di stand for i’th color and depth images re-
spectively, ti’s are corresponding timestamps, and N is the
sequence length. Subscripts L and H stand for lower- and
higher-quality sequences. Note, that timestamps ti
H and ti
L
are not directly comparable as the devices are not synchro-
nized in time.

We then align the recorded sequences both in time and
space. We assume that the sensors’ clocks are out of sync
and deviate from each other by an additive constant. For
each sequence, we search for a shift ∆H→L, which would
align the clocks of the two sensors (section 4.2). For each
possible time shift ∆H→L, we extract indices of matching
frames {(i, Φ(i)) : i ∈ [1 . . . NL]} for Φ(i) deﬁned as

Φ(i; ∆H→L) = arg min

j

(cid:107)ti

L − (tj

H + ∆H→L)(cid:107) .

(2)

We then search for an extrinsic matrix TH→L to transform
point clouds captured with the HQ sensor to the LQ cam-
era coordinates by matching key points extracted from the
RGB images {CΦ(i)
L} (section 4.1). We project
the transformed point clouds using the LQ camera projec-
tion matrix to get the corresponding ground truth for the LQ
depth images. We choose the shift ∆H→L such that it deliv-
ers the best spatial alignment in the extrinsic matching step.
The pipeline is summarized in ﬁg. 2.

H } and {Ci

Having a dataset of spatially and temporally aligned
RGB-D frames, we interpret depth denoising as an image-
to-image translation task and train a deep convolutional

For each possible time shift ∆H→L

Match
frames (4.2)

Find
extrinsic
(4.1)

Evaluate
loss (eq. 4)

Figure 2: Summary of the data processing pipeline. See sec-
tion 4 for details.

neural network to map RGB-D images coming from the LQ
sensor to the reprojected HQ depth frames (section 5).

We train a simple UNet-like neural network for single
image denoising. To leverage the temporal information, we
employ an out-of-fold prediction strategy [43] and train a
ConvLSTM [45] on top of the UNet’s predictions. At test
time, it takes a single forward pass of the UNet network
to perform depth denoising. If sequential data is available,
we use the second-level ConvLSTM to postprocess UNet
predictions.

4. Dataset

To create a dataset with aligned data for self-supervised
learning, we ﬁx two RGB-D sensors as rigidly as possi-
In our setup (ﬁg. 1 (a)) we used Kinect V2 (K2)
ble.
and TrueDepth (TD) sensor embedded in iPhone X. We
recorded 51 simultaneous RGB-D sequences with differ-
ent human actors, each about 30 seconds long. We asked
the actors ﬁrst to make a turnaround in front of camera and
then move freely for the remaining time. We used six differ-
ent environments to maximize the diversity of the recorded
data. The resolution of K2 and TD depth is 512x424 and
480x640 respectively.

4.1. Spatial alignment

While in general it is possible to calibrate extrinsic pa-
rameters of the sensor pair using a checkerboard [50], we
did not ﬁnd such calibration robust enough for our scenario.
In fact, while the checkerboard calibration worked well for
the ﬁrst sequence recorded after the calibration, the esti-
mated extrinsic did not prove to be reliable for the follow-
ing sequences because of physical interaction with iPhone,
overheating, non-rigidity of K2 and our rig. Therefore, we
perform in-the-wild extrinsic matrix calibration separately
for each sequence pair using the available unsynchronized
RGB-D data.

Having an estimate of the time shift ∆H→L we extract
the set of matched frames {(Ci
i}.
We then optimize for an extrinsic matrix TH→L that would
transform HQ sensor coordinate system into the LQ sen-
sor system. For a point pH = (x, y) on the image plane
of HQ camera and the corresponding depth DΦ(i)
H (pH), the

H , DΦ(i)

L, CΦ(i)

H ) :

L, Di

location in 3D HQ camera space is deﬁned as PH(pH) =
H (pH, DΦ(i)
π−1
H (pH)) where π−1 is an unprojection opera-
tor. The 2D position on the LQ frame is then determined
as

ˆp(pH; TH→L) = πL(TH→LPH(pH)) ,

(3)

where πL is a projection function mapping 3D points in
LQ camera space onto the camera plane with z-buffer. We
compute RGB images ˆCΦ(i)
by using 3 to reproject color
frames CΦ(i)

according to the current estimate of TH→L.

H

H

For each frame pair (i, Φ(i)) we use the Superpoint de-
tector [7] to extract a set of 2D correspondences Ui from Ci
L
and ˆCΦ(i)
and minimize the distance on the image plane be-
tween them with respect to the transform TH→L:

H

L(TH→L) =

NL(cid:88)

(cid:88)

i=1

(pL,pH)∈Ui

(cid:107)ˆp(pH, TH→L) − pL(cid:107)2 .

(4)

We initialize the extrinsic matrix with identity transform
TH→L = I and parametrize the rotation component with
quaternions. We use gradient descent for simplicity and
ﬂexibility.

We found such iterative recomputation of the correspon-
dences and gradient-based optimization to be robust to in-
correct key points and occlusions.

4.2. Temporal alignment

As it is impossible to enable hardware synchronization
for the pair of the depth sensors we used, we implemented
software timestamp synchronization and frame ﬁltering as
postprocessing. Our alignment method builds a mapping Φ
which maps an index i of a LQ frame to an index Φ(i) of
the corresponding HQ frame shot closely in time and tackles
the following issues:

• Clocks of TD and K2 are not synchronized. That is, the
two devices assign different timestamp to any point in
time.

• The devices start recording at a different time.

• The devices have different frame rate. The frame rates

are not constant.

Importantly, we assume that the time coordinate system
for the two sensors differs only by a constant shift ∆H→L
and the timestamps do not ”drift” with time. That is, to
transform an arbitrary timestamp from HR sensor tH into
LR sensor timestamp tL we just need to add ∆H→L to the
former:

tL = tH + ∆H→L .

Note that we do not assume that the sensors have the
same frame rate. The frame rate can vary over time for both

P1

P2

Ptest

M 1

2 (P1) M 1

1 (P2) M 1(Ptest)

M 1

1 :

M 1

2 :

M 1 :

M 2 :

1 , M 1

Figure 3: Out-of-fold prediction scheme. We split all se-
quences into three groups {P1, P2, Ptest}. We then learn
2 , M 1 on the group shown in red and
three models M 1
get their predictions on the blue parts. For example, the ﬁrst
model is trained on the part P1 and evaluated on the part
P2. We then use the predictions M 1
2 (P1) to
train a second-level model M 2 while using M 1(Ptest) for
its validation.

1 (P2) and M 1

sensors. We only make assumption about the relation of
time coordinate systems.

L: i ∈ [1 . . NL]} and {tj

For each pair of sequences we seek for a shift ∆H→L
that would align the timestamps of the two sensors so that
a simple nearest neighbour search between the timestamps
{ti
H + ∆H→L: i ∈ [1 . . NH]} would
give us the best mapping Φ. We use binary search in the
[−60ms, 60ms] segment with 5ms step size to ﬁnd the best
∆H→L based on the correspondence loss deﬁned in eq. (4).
With both extrinsic TH→L and the frame mapping Φ
found, for each index i we compute depth and color im-
ages ˆDΦ(i)
by reprojecting the corresponding HQ
H
depth and color images DΦ(i)
to the LQ camera
using eq. (3). Finally, we construct a sequence ˆSH =
{ ˆCΦ(i)
i=1 that corresponds to LQ sequence SL.

H , ˆCΦ(i)

H , CΦ(i)

H , ˆDΦ(i)

H , ti

L}NL

H

5. Training

To exploit temporal information available in the consec-
utive frames it is tempting to use a recurrent model. How-
ever, we did not succeed to directly train one due to lim-
Instead, we utilize two-level train-
ited amount of data.
ing approach based on out-of-fold predictions approach [43]
widely used for model ensembling. First, we train ﬁrst-level
UNet [32]-like architecture to denoise depth on per-frame
basis. As a second-level model we train a convolutional
LSTM (ConvLSTM) to account for temporal correlations
in the data.

To train the ﬁrst-level model, we split all available se-
quences into three groups P1, P2, and Ptest. We use the
sequences from the group P1 to train a UNet M 1
1 and the
group P2 to train a UNet M 1
2 (ﬁg. 3). We use those models
to get the predictions for an unseen group of the sequences

(e.g. a model trained on the part P1 is used to predict depth
for the part P2 and vice versa). Additionally, we train a
UNet M 1 on both parts P1 and P2 and use it to get predic-
tions for the testing sequences Ptest.

merging ﬁrst- and second-level models in a single predictor.
Yet, we found it to be technically easier to apply models M 1
and M 2 one after another saving intermediate predictions to
a hard drive.

a

training

constructed

set
end, we
this
To
2 (P1)} and a testing set M 1(Ptest) for a
{M 1
1 (P2), M 1
second-level model
that consist of ﬁrst-level models’
predictions. Note, that by employing out-of-fold strategy
we create an unbiased dataset of the same size as the
original dataset, that we now can use to train a second-level
model.

We use a ConvLSTM M 2 as a second-level predictor.
We found it efﬁcient at capturing temporal correlations in
data and reﬁning single-image depth denoising result of
ﬁrst-level’s UNet M 1.

To account for synchronization errors, we ﬁrst use
frames aligned within 15ms to train both ﬁrst- and second-
level models and then ﬁnetune ConvLSTM using only 5ms
distant frames.

Losses. While we could employ photometric reprojection
losses as in [36], the availability of the groundtruth depth
images allows us to use a simple per-pixel loss between
the reprojected K2 depth frames ˆDΦ(i)
and model’s output
Di

H

pred:

L(Di

pred, ˆDΦ(i)

pred − ˆDΦ(i)

H ) = (cid:107)(Di

H ) · mΦ(i) · mi

seg(cid:107)1 .
(5)
Here, m is a binary mask that indicates the presence of
the depth information for each pixel of ˆDΦ(i)
seg is
person segmentation mask precomputed based on the color
image Ci
L using [8]. The model predicts a dense depth im-
age while the reprojected HQ depth frames ˆDΦ(i)
H are sparse
as the resolution of the K2 sensor is lower that the resolution
of TD camera.

and mi

H

6. Testing

At the testing time, given an input RGB-D frame com-
ing from the LQ sensor, we feed it to the ﬁrst-level UNet
M 1 to estimate a denoised depth image. Being trained on
humans, the network’s predictions are only reliable at the
pixels corresponding to human bodies. Therefore, we mul-
tiply the predicted depth image pixel-wise by person seg-
mentation mask. While the basic model M 1 can be used
both in single-image and multi-image scenarios, when se-
quential data is available, we can use the full model with
the second-level ConvLSTM M 2. We aggregate the predic-
tions of the ﬁrst-level model M 1 and feed them one by one
to M 2 to produce a ﬁnal denoised depth map for each frame
of the LQ sequence.

We note that even though our training pipeline is not end-
to-end, the testing pipeline can be implemented as such by

7. Experiments

In this section, we experimentally validate the proposed
method. We use four 30 second test sequences with actors
and environments never seen during training. For ﬁltering-
based methods Bilateral Filter (BF [38]), Rolling Guidance
Filter (RGF [49]), Joint Bilateral Filter (JBF [19]) we use
their OpenCV [4] implementation and pick the parameters,
that minimize mean squared error with respect to ground
truth K2 depth images. We used the code provided by au-
thors to evaluate DDRNet [46], DRR [16], DDD [36]. Qual-
itative comparison is provided in ﬁg. 4 and quantitative re-
sults are summarized in table 1.

Please note that DDD, DDRNet, DRR are trained on dif-
ferent datasets and it is not possible to retrain them using
our data for various reasons discussed below.

DDD. Sterzentsenko et al. explicitly claim DDD is
sensor-agnostic: [DDD] maintains its performance when
denoising depth maps captured from other sensors. We can-
not train DDD on our data as DDD requires the sensors
placed at a sufﬁcient distance (DDD uses reprojection loss).
DDRNet. DDRNet is traind on the data gathered with
K2 sensor. Note, that it is very challenging to retrain DDR-
Net on TD data as the TD depth is usually too noisy to per-
form 3D reconstruction reliably. DDRNet uses Double fu-
sion to create ground truth data. An example of DoubleFu-
sion run on TD data is presented at ﬁg. 6 (a). It is obvious
that the quality of the reconstructed mesh is insufﬁcient to
be used as ground truth.

DRR. DRR is trained on K2 data of static scenes. For
the experiments, we used a model provided by authors. Our
dataset consists of moving people and Kinect Fusion cannot
be used to reconstruct the scene. Even for static scenes, it
is still very challenging to run Kinect Fusion on TD data as
TD data is too noisy.

Out of all ﬁltering-based methods we ﬁnd Rolling Guid-
ance Filter to work best. Unfortunately, RGF is only efﬁ-
cient at removing high-frequency noise but not able to re-
cover missing details. DDRNet, being trained on Kinect
V2 data, also fails to adapt to the high noise magnitude.
DRR network of Jeon et al. is mostly trained on planar
surfaces and thus over-smooths the input depth. While
Sterzentsenko et al. claim that DDD is sensor-agnostic, we
did not manage to get good results with this method.

The proposed method achieves 21mm mean squared er-

ror which is lower than for the baselines.

Finally, we compare our basic single-image denoiser M 1
and the full model M 2 with ConvLSTM on top of the pre-
dictions of basic model. We found that the full model bene-

(a) Input (TD)

(b) RGF

(c) JBF

(d) BF

(e) DDD

(f) DDRNet

(g) DRR

(h) Ours (basic)

(j) Ours (full)

(k) K2

Figure 4: Qualitative comparison of depth denoising methods. Please see a video in supplementary materials.

(a) Color

(b) TrueDepth

(c) RGF

(d) DRR

(e) Ours(basic)

Figure 5: Our model learned in a self-supervised way takes color and depth data coming from the sensor of an iPhone X as
input (a, b) and produces a denoised and reﬁned depth (e). Our method works signiﬁcantly better than ﬁltering-based (c) and
other data-driven approaches (d). We intentionally visualize depth slightly rotated in 3D to highlight the differences between
the methods. For more examples please refer to supplementary materials.

Table 1: Quantitative comparison of depth denoising approaches. Our method reaches the lowest average MSE error between
predicted depth frames and groundtruth depth frames.

Raw

JBF
[19]

RGF
[49]

BF
[38]

DDD
[36]

DDRNet
[46]

DRR
[16]

Ours
(basic)

Ours
(LSTM)

MSE (mm) 57.22 49.69 49.49

56.39

84.07

57.08

53.32

31.61

21.02

ﬁts from aggregating depth measurements from subsequent
frames and outperforms the single-image M 1 by a huge
margin.

The resultant meshes are shown in ﬁg. 7. Our method pro-
vides more details in each frame resulting in a higher de-
tailed reconstructed geometry.

Dynamic body reconstruction. Next, we use the pro-
posed models for human body reconstruction and tracking
application. We run DoubleFusion [47] on sequences from
the validation set. We apply it to raw TD data, raw K2
data, and to the data obtained by applying the models M 1
and M 2 to the TD data. The qualitative comparison of the
reconstructed canonical models for one of the sequences
is given in ﬁg. 6. We compute average distance between
the surfaces created using TD depth and K2 depth and get
13mm for raw TD depth, 5.9mm for the depth denoised with
M 1 and 5.7mm for the depth denoised with M 2. Clearly,
denoising with our method signiﬁcantly improves Double-
Fusion reconstruction if compared to the result based on the
raw TD data and signiﬁcantly reduces the gap between TD
and K2.

Static body reconstruction. We use InﬁniTAM [30]
framework to assess an effect of our denoising procedure
on the quality of the resultant mesh in a KinectFusion re-
construction pipeline. In this experiment, we ﬁrst record a
frame sequence, denoise the depth and then run the depth
fusion algorithm with the denoised depth stream as input.
We observe that in the cases of slowly-moving camera with
high frame rate when a lot of frames can be accumulated,
an accurate mesh can be produced even with the low-quality
TD stream with objects at relatively large distances. In real-
ity, the number of frames may be limited both by the record-
ing conditions and by the available processing time. Thus,
we focus on the setting when only a small number of frames
available for reconstruction.

We ﬁxed an LQ sensor statically and recorded an RGB-
D stream of person rotating in the ofﬁce chair.1 We re-
moved the background using semantic segmentation mask
and depth thresholding. During the recording of the video
our test subject was rotated in the 90 degree arc, centered
in the front-faced position. We picked seven evenly-spaced
frames and run KinectFusion implemented via InﬁniTAM.

7.1. Technical details

In this section we provide technical details necessary to

reproduce the results of the proposed method.

Network architecture. We use a UNet with four down-
sampling operations. In addition to standard UNet architec-
ture, we used skip connections between the input layer and
each layer of the encoder.

We used a ConvLSTM model similar to [40, 41]. We
base it on UNet’s architecture and add ConvLSTM layer
after every convolutional layer in the encoder and decoder.
We train LSTM on the sequences of size four.

Training and testing. Both ﬁrst-level and second-level
models take color and depth images (Ci
L) along with
person segmentation mask precomputed using [8] and XY
“meshgrid” [24, 39] as input. We concatenate the men-
tioned maps along the feature dimension.

L, Di

We found that the pixel location information coming
with ”meshgrid” channels helps the network to better ﬁt the
data. Yet, we realize that this fact can also indicate about an
error in the extrinsic/intrinsic calibration. In fact, while we
use color information to ﬁnd the transformation that would
align sensor’s coordinate systems, we heavily rely on the
intrinsic and color-to-depth extrinsic calibration of each de-
vice which we precompute once. With insufﬁciently ac-
curate intra-sensor calibration, the photometric consensus
between LQ color image C i
L and the reprojected HQ color
image ˆC j
H does not lead to the alignment between the point-
clouds corresponding to the depth images Dj
H (e.g.
LQ point cloud is always slightly rotated with respect to the
HQ point cloud).

L and ˆDi

We train UNet for 5 hours with batch size of 48 using
four Titan V100 GPU’s. The training of the second-level
ConvLSTM model takes 2 days using the same hardware.
We use Adam optimizer [18] with a scheduler that lowers
the learning rate when the loss stop to decrease. We use
PyTorch [29] in all our experiments.

8. Conclusion and discussion

1This process is equivalent to moving camera around the person but

does not require disassembling of our installation.

In this paper we proposed a pipeline for self-supervised
depth denoising and reﬁnement. We overcome the lack of

(a) TD

(b) Ours (basic)

(c) Ours (full)

(e) K2

Figure 6: Canonical models built by DoubleFusion for different input data. DoubleFusion is designed to work with K2 input
(e) and cannot handle noisy TD depth (a). Our method greatly improves the result of DoubleFusion (b, c) enabling its usage
in mobile applications. We compare K2 mesh (e) with meshes (a, b c) by computing the distance between the surfaces and
visualize the result as heatmaps. Blue color corresponds to zero error while red corresponds to 44mm error.

Figure 7: 3D reconstruction using KinectFusion framework based on the raw depth stream from TD sensor (left) and based
on the output of our denoising algorithm (center). The visual outlook of the scene is shown on the right. With denoised input,
the quality of the produced model is higher and the level of noise is greatly reduced.

groundtruth data for data-driven depth denoising of low-
quality depth sensors by employing a higher quality sen-
sor and collecting simultaneous recordings. We automati-
cally extract frames captured close in time and do not re-
quire the sensor pair to be precalibrated. We formulate the
task of depth denoising as image-to-image translation and
use a simple L2 loss between the depth images. While our
method is easy to implement, it has a number of limitations.
Our method is limited by the applicability of the depth sen-
sors, e.g. Kinect V2 can’t be used in direct sunlight setting.

We also sometimes observe color leakage (ﬁg. 4 (j)) and
imperfect temporal consistency (cf. video in supplementary
material). Finally, being trained for a particular sensor types
our models cannot be applied to the depth images captured
with a another sensors.

9. Supplementary material

In this supplementary material we provide more exam-
ples for qualitative assessment of our method ﬁgs. 8 to 10 .
Here are video links with extensive comparison of our

method. There are 2 video recordings:

• Comparison of our methods - visualization of two pro-
posed depth enhancement methods along with Kinect
v2 and TrueDepth cameras recordings

• Comparison of ours vs other methods - comparison of
our basic method with ﬁltering-based (RGF [16]) and
data-driven (DRR [49]) approaches.

(a) Input (TD)

(b) RGF

(c) JBF

(d) BF

(e) DDD

(f) DDRNet

(g) DRR

(h) Ours (basic)

(j) Ours (full)

(k) K2

Figure 8: Qualitative comparison of depth denoising methods. Please see videos in addition to this PDF ﬁle in supplementary
materials.

(a) Color

(b) TD

(c) Ours (basic)

(d) Color

(e) TD

(f) Ours (basic)

Figure 9: More examples produced by our model for various people and environments.

(a) Color

(b) TD

(c) Ours (basic)

(d) RGF

(e) DDRNet

Figure 10: Qualitative comparison of depth denoising methods for lower-quality TD data. Here, we use our basic model as
the input data is represented by separate RGB-D frames rather than sequences, and our full model cannot be applied.

References

[1] G. Agresti and P. Zanuttigh. Deep learning for multi-path
error removal in ToF sensors. In ECCVW, pages 410–426,
2018. 2

[2] F. Basso, E. Menegatti, and A. Pretto. Robust intrinsic and
extrinsic calibration of RGB-D cameras. IEEE Transactions
on Robotics, (99):01–18, 2018. 2

[3] J. Batson and L. Royer. Noise2Self: Blind denoising by self-

supervision. In ICML, 2019. 3

[4] G. Bradski and A. Kaehler. Learning OpenCV: Computer
vision with the OpenCV library. ” O’Reilly Media, Inc.”,
2008. 5

[5] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Nießner. Scannet: Richly-annotated 3d reconstruc-
In Proceedings of the IEEE Con-
tions of indoor scenes.
ference on Computer Vision and Pattern Recognition, pages
5828–5839, 2017. 2

[6] A. Dai, M. Nießner, M. Zollh¨ofer, S. Izadi, and C. Theobalt.
BundleFusion. ACM Transactions on Graphics, 36(4):1,
2017. 1, 2

[7] D. DeTone, T. Malisiewicz, and A. Rabinovich. Superpoint:
Self-supervised interest point detection and description. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, pages 224–236, 2018.
4

[8] K. Gong, Y. Gao, X. Liang, X. Shen, M. Wang, and L. Lin.
Graphonomy: Universal human parsing via graph transfer
learning. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 7450–7459,
2019. 5, 7

[9] Q. Guo, I. Frosio, O. Gallo, T. Zickler, and J. Kautz. Tackling
3D ToF artifacts through learning and the FLAT dataset. In
ECCV, pages 368–383, 2018. 2

[10] B. Haefner, S. Peng, A. Verma, Y. Quau, and D. Cremers.
Photometric depth super-resolution. IEEE Transactions on
Pattern Analysis and Machine Intelligence (TPAMI), 2019.
2

[11] B. Haefner, Y. Qu´eau, T. M¨ollenhoff, and D. Cremers.
Fight ill-posedness with ill-posedness: Single-shot varia-
tional depth super-resolution from shading. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 164–174, 2018. 2

[12] Y. Han, J. Lee, and I. S. Kweon. High quality shape from
a single RGB-D image under uncalibrated natural illumina-
tion. In ICCV, pages 1617–1624, 2013. 2
[13] D. Herrera, J. Kannala, and J. Heikkil¨a.

Joint depth and
IEEE
color camera calibration with distortion correction.
Transactions on Pattern Analysis and Machine Intelligence,
34(10):2058–2064, 2012. 2

[14] T.-W. Hui, C. C. Loy, and X. Tang. Depth map super-
resolution by deep multi-scale guidance. In European con-
ference on computer vision, pages 353–369. Springer, 2016.
2

[15] T.-W. Hui and K. N. Ngan. Depth enhancement using rgb-d
guided ﬁltering. In 2014 IEEE International Conference on
Image Processing (ICIP), pages 3832–3836. IEEE, 2014. 2

[16] J. Jeon and S. Lee. Reconstruction-based pairwise depth
dataset for depth image enhancement using CNN. In ECCV,
pages 438–454, 2018. 1, 2, 5, 7, 9

[17] A. Kaiser, J. A. Y. Zepeda, and T. Boubekeur. Proxy clouds
for RGB-D stream processing. In SIGGRAPH. ACM, 2017.
2

[18] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015. 7

[19] J. Kopf, M. F. Cohen, D. Lischinski, and M. Uyttendaele.
Joint bilateral upsampling. ACM Transactions on Graphics,
26(3), 2007. 1, 2, 5, 7

[20] A. Krull, T. Buchholz, and F. Jug. Noise2Void - Learning
denoising from single noisy images. In CVPR, pages 2129–
2137, 2019. 3

[21] H. Kwon, Y. Tai, and S. Lin. Data-driven depth map reﬁne-
ment via multi-scale sparse representation. In CVPR, pages
159–167, 2015. 1, 2

[22] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Kar-
Noise2noise: Learning
arXiv preprint

ras, M. Aittala, and T. Aila.
image restoration without clean data.
arXiv:1803.04189, 2018. 3

[23] J. Li, W. Gao, and Y. Wu. High-quality 3d reconstruction
with depth super-resolution and completion. IEEE Access,
7:19370–19381, 2019. 2

[24] R. Liu, J. Lehman, P. Molino, F. P. Such, E. Frank,
A. Sergeev, and J. Yosinski. An intriguing failing of con-
In
volutional neural networks and the coordconv solution.
Advances in Neural Information Processing Systems, pages
9605–9616, 2018. 7

[25] J. Marco, Q. Hernandez, A. Mu˜noz, Y. Dong, A. Jarabo,
M. H. Kim, X. Tong, and D. Gutierrez. DeepToF: off-the-
shelf real-time correction of multipath interference in time-
of-ﬂight imaging. ACM Transactions on Graphics, 36(6):1–
12, 2017. 2

[26] R. A. Newcombe, D. Fox, and S. M. Seitz. Dynamicfusion:
Reconstruction and tracking of non-rigid scenes in real-time.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 343–352, 2015. 1

[27] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and
A. Fitzgibbon. KinectFusion: Real-time dense surface map-
ping and tracking. In ISMAR, pages 127–136, 2011. 1, 2
[28] R. Or - El, G. Rosman, A. Wetzler, R. Kimmel, and A. M.
Bruckstein. RGBD-fusion: Real-time high precision depth
recovery. In CVPR, pages 5407–5416, 2015. 2

[29] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in PyTorch. 2017. 7

[30] V. A. Prisacariu, O. K¨ahler, S. Golodetz, M. Sapienza,
T. Cavallari, P. H. Torr, and D. W. Murray.
Inﬁnitam v3:
a framework for large-scale 3d reconstruction with loop clo-
sure. arXiv preprint arXiv:1708.00783, 2017. 7

[31] C. Richardt, C. Stoll, N. A. Dodgson, H.-P. Seidel, and
C. Theobalt. Coherent spatiotemporal ﬁltering, upsampling
and rendering of rgbz videos. In Computer Graphics Forum,
volume 31, pages 247–256. Wiley Online Library, 2012. 2

[48] M.-Z. Yuan, L. Gao, H. Fu, and S. Xia. Temporal upsampling
IEEE transactions
of depth maps using a hybrid camera.
on visualization and computer graphics, 25(3):1591–1602,
2018. 2

[49] Q. Zhang, X. Shen, L. Xu, and J. Jia. Rolling guidance ﬁlter.

In ECCV, pages 815–830, 2014. 1, 2, 5, 7, 9

[50] Z. Zhang et al. Flexible camera calibration by viewing a
plane from unknown orientations. In Iccv, volume 99, pages
666–673, 1999. 3

[51] Y. Zuo, Y. Fang, Y. Yang, X. Shang, and B. Wang. Residual
dense network for intensity-guided depth map enhancement.
Information Sciences, 495:52–64, 2019. 2

[32] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, pages 234–241, 2015. 4

[33] J. Sell and P. O’Connor. The xbox one system on a chip and

Kinect sensor. IEEE Micro, 34(2):44–53, 2014. 1

[34] J. Shen and S. S. Cheung. Layer depth denoising and com-
pletion for structured-light RGB-D cameras. In CVPR, pages
1187–1194, 2013. 2

[35] K. Son, M.-Y. Liu, and Y. Taguchi. Learning to remove
multipath distortions in time-of-ﬂight range images for a
In 2016 IEEE International Confer-
robotic arm setup.
ence on Robotics and Automation (ICRA), pages 3390–3397.
IEEE, 2016. 2

[36] V. Sterzentsenko, L. Saroglou, A. Chatzitoﬁs, S. Thermos,
N. Zioulis, A. Doumanoglou, D. Zarpalas, and P. Daras. Self-
supervised deep depth denoising. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1242–
1251, 2019. 1, 2, 3, 5, 7

[37] J. Tang, F.-P. Tian, W. Feng, J. Li, and P. Tan. Learning
guided convolutional network for depth completion. arXiv
preprint arXiv:1908.01238, 2019. 2

[38] C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and
color images. In ICCV, pages 839–846, 1998. 1, 2, 5, 7
[39] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep image prior.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 9446–9454, 2018. 7

[40] R. Wang, J. Frahm, and S. M. Pizer. Recurrent neural net-
work for learning densedepth and ego-motion from video.
CoRR, abs/1805.06558, 2018. 7

[41] R. Wang, S. M. Pizer, and J. Frahm. Recurrent neural net-
work for (un-)supervised learning of monocular videovisual
odometry and depth. CoRR, abs/1904.07087, 2019. 7
[42] O. Wasenm¨uller and D. Stricker. Comparison of Kinect v1
and v2 depth images in terms of accuracy and precision. In
ACCV, pages 34–45, 2016. 3

[43] D. H. Wolpert. Stacked generalization. Neural networks,

5(2):241–259, 1992. 3, 4

[44] C. Wu, M. Zollh¨ofer, M. Nießner, M. Stamminger, S. Izadi,
and C. Theobalt. Real-time shading-based reﬁnement for
consumer depth cameras. ACM Transactions on Graphics,
33(6):1–10, 2014. 2

[45] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong,
and W.-c. Woo. Convolutional lstm network: A machine
learning approach for precipitation nowcasting. In Advances
in neural information processing systems, pages 802–810,
2015. 3

[46] S. Yan, C. Wu, L. Wang, F. Xu, L. An, K. Guo, and Y. Liu.
DDRNet: Depth map denoising and reﬁnement for consumer
depth cameras using cascaded CNNs. In ECCV, pages 155–
171, 2018. 1, 2, 5, 7

[47] T. Yu, Z. Zheng, K. Guo, J. Zhao, Q. Dai, H. Li, G. Pons-
Moll, and Y. Liu. Doublefusion: Real-time capture of human
performances with inner body shapes from a single depth
sensor. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 7287–7296, 2018. 1,
2, 7

