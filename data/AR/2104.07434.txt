Points as Queries: Weakly Semi-supervised Object Detection by Points

Liangyu Chen1,2 *

Tong Yang1 *

Xiangyu Zhang1

Wei Zhang2 ‚Ä†

Jian Sun1

1 MEGVII Technology

2 Fudan University

{chenliangyu,yangtong,zhangxiangyu,sunjian}@megvii.com

weizh@fudan.edu.cn

1
2
0
2

r
p
A
5
1

]

V
C
.
s
c
[

1
v
4
3
4
7
0
.
4
0
1
2
:
v
i
X
r
a

Abstract

We propose a novel point annotated setting for the
weakly semi-supervised object detection task, in which the
dataset comprises small fully annotated images and large
weakly annotated images by points. It achieves a balance
between tremendous annotation burden and detection per-
formance. Based on this setting, we analyze existing de-
tectors and Ô¨Ånd that these detectors have difÔ¨Åculty in fully
exploiting the power of the annotated points. To solve this,
we introduce a new detector, Point DETR, which extends
DETR by adding a point encoder. Extensive experiments
conducted on MS-COCO dataset in various data settings
show the effectiveness of our method. In particular, when
using 20% fully labeled data from COCO, our detector
achieves a promising performance, 33.3 AP, which outper-
forms a strong baseline (FCOS) by 2.0 AP, and we demon-
strate the point annotations bring over 10 points in various
AR metrics.

1. Introduction

Object detection is one of the fundamental problems in
computer vision. Modern object detectors [12, 14, 15, 22,
29] have achieved great success with the help of tremen-
dous annotated data. However, it is very costly to annotate
a large amount of detection data. SpeciÔ¨Åcally, for each ob-
ject instance, a precise bounding box needs to be labeled
manually and carefully, which is quite time-consuming: it
takes 10-35 seconds [27, 24, 1] for labeling an object.

To reduce the cost of data annotation, weakly super-
vised object detection (WSOD) and semi-supervised object
detection (SSOD) methods are proposed. Weakly super-
vised object detection methods [2, 11, 25, 36] utilize large
data with weak annotations, such as image labels, which
is far easier to collect than precisely annotated bound-
ing boxes. The semi-supervised object detection methods
[10, 17, 26, 28, 31] learn detectors with a small amount
of box-level labeled images and large unlabeled images,

*Equally contribution.
‚Ä†Corresponding author.

where the cost of image annotation is small. Although
these methods can reduce the cost of annotation signiÔ¨Å-
cantly, their performance is far inferior to their supervised
counterparts [14, 15, 29]. To make a trade-off between an-
notation cost and performance, weakly semi-supervised ob-
ject detection methods (WSSOD) [33] are studied, which
use small box-level labeled images as well as large weakly
labeled images to learn detectors. However, image-level
annotations in weakly annotated data are not optimal for
object detection task since image labels do not contain the
instance-level information of all objects. Motivated by [1],
we annotate each instance in the image by one point (as
shown in Figure 1d) instead of image-level annotation, for
two main reasons. Firstly, compared with image-level an-
notation, points bring much richer information, not only the
category of the object but also the strong prior of object
location. Secondly, there is no strict requirement on point
annotations, such as center points of objects. Thus, the in-
crease in the cost of labeling is marginal compared with
the image-level annotation [1]: 23.3 sec/image vs. 20.0
sec/image in VOC dataset [6].

Though the above new setting is better for weakly semi-
supervised object detection, most recent detectors [14, 15,
29] have difÔ¨Åculty in predicting object boxes based on point
annotations. In most detectors, FPN [14] is a basic compo-
nent, which utilizes multi-level feature maps to predict ob-
ject boxes. FPN can boost the performance of detectors, but
it is incompetent to predict object boxes using point anno-
tations since it is difÔ¨Åcult to select the optimal box predic-
tion from multi-level ones, predicted for a point annotation.
For the single-level feature detectors, they may suffer from
bad performance [20, 21, 22] or strict requirement on point
annotations [5, 12, 35] even though they avoid choosing
feature map levels.

Inspired by DETR [4] which achieves competitive per-
formance with a single-level feature map, we propose a
novel detector, Point DETR, by adding a point encoder to
DETR in this paper. It can predict object boxes precisely
from point annotations. SpeciÔ¨Åcally, it uses a single-level
feature map to predict object boxes, avoiding the multi-level
selection problem and can predict object boxes with loose

 
 
 
 
 
 
Figure 1. Different types of object detection settings to reduce the cost of data annotation

points, having no strict requirement on point annotations.
Besides, it inherits the strong representation of DETR, hav-
ing a good performance on object detection. But, different
from DETR, we encode position and category of annotated
points into object queries with the point encoder, which eas-
ily establishes one-to-one correspondences between points
and object queries, being Ô¨Åt for box prediction based on
points.
In addition, to boost detection performance and
make optimization easier, we do box predictions as offsets
w.r.t. point position rather than make box predictions di-
rectly like DETR.

To show the superiority of our detector, we mainly eval-
uate our proposed detector on the MS-COCO dataset [16].
To make a fair comparison, we take FCOS [29] as the de-
fault baseline, which is regarded as a point-based detector.
Following our proposed weakly semi-supervised object de-
tection setting, object instances of small image data fraction
(5% ‚àº 50%) are annotated fully and the rest are annotated
by points.
In these various settings with a different frac-
tion of fully-annotated image data, our proposed detector
outperforms other modern detectors, including multi-level
feature detectors and single-level feature detectors. In par-
ticular, when using 20% fully labeled data from COCO, our
detector outperforms FCOS and Faster R-CNN by 2.0 AP
and 1.9 AP, respectively.

Our main contributions can be summarized as follows:

‚Ä¢ We propose a potential and novel setting for the weakly
semi-supervised object detection task, which com-
prises small fully annotated images and large weakly
annotated images by points. Compared with the
image-level data setting [1], this setting introduces
weakly instance-level information with marginal an-
notation cost, which is Ô¨Åt for object detection. This
provides a new perspective to improve detection per-
formance with weakly annotated detection images.

‚Ä¢ Based on the above setting, we analyze the drawbacks

of existing modern object detectors and propose Point
DETR, which is simple and easily implemented. The
proposed detector takes object points as input, trans-
forms these points into object queries, and predicts ob-
ject box precisely for these queries, as shown in Fig-
ure 3.

‚Ä¢ Extensive experiments on COCO dataset [16] are con-
ducted to demonstrate the effectiveness of our pro-
posed detector. Our detector outperforms most modern
detectors in various data settings. We also do quantity
and quality experiments to show our detector solves
the problems suffered by most modern detectors.

2. Related Work

Supervised Object Detection: With the large-scale fully
annotated detection data, existing modern detectors [12, 14,
15, 22, 29] have obtained great improvements in the ob-
ject detection task. These detectors can be divided into
two categories: two-stage detectors and one-stage detectors.
FPN [14] is a popular two-stage detector, which predicts
object proposals Ô¨Årstly and reÔ¨Ånes these proposals Ô¨Ånally.
Unlike two-stage detectors, one-stage detectors [12, 15, 29]
directly outputs the classiÔ¨Åcation and location of each ob-
ject without reÔ¨Ånement. Though achieving great success,
these detectors are trained with a large amount of fully-
annotated data, which is costly to annotate. Thus, there are
many works proposed to reduce the annotation cost.

Semi-Supervised/Weakly Supervised Object Detection:
Semi-supervised object detection (SSOD) [10, 17, 26, 28,
31] and weakly-supervised object detection (WSOD) [2,
11, 25, 36] are introduced to reduce the large cost of data
annotation. The semi-supervised object detection methods
learn detectors with a small amount of box-level labeled
images and large unlabeled images. Jeong et al. [10] em-
ploy consistency constraints for object detection to exploit

(a) Weakly supervised object detection(b) Semi-supervised object detection(c) Weakly semi-supervised object detection with image-level label(d) Weakly semi-supervised object detection with points (ours)unlabeled data. While, weakly supervised object detection
methods utilize large data with weak annotations, such as
image labels. Bilen et al. [2] learn an object detector un-
der image-level supervision by combining region classiÔ¨Åca-
tion and selection. Furthermore, pursuing the performance
of supervised detection and keeping the low cost of an-
notation, weakly semi-supervised object detection methods
(WSSOD) [33] are studied, which use small box-level la-
beled images as well as large weakly labeled images to learn
detectors. Unlike these semi-/weakly-supervised object de-
tection, our proposed detector utilizes a new low-cost anno-
tation: points, which provide instance location. Recently,
UFO2 [23] also uses point supervision as weak labels, but
it does not explore the point information sufÔ¨Åciently as we
shown in Section 4.3.

Point based Semi-Supervised Segmentation: Point su-
pervision [1, 19, 34] has been employed by semantic seg-
mentation. Bearman et al. [1] incorporate point super-
vision along with objectness prior to boost segmentation
performance and alleviate annotation burden. Qian et
al. [19] leverage semantic relationships among several la-
beled points to address the semantic scene parsing task. Dif-
ferent from these works, we focus on object detection task,
where point-based detection has been explored little. Due
to a lack of exploitation, existing detectors do not Ô¨Åt point-
level annotation well.

DETR: Unlike existing detectors, DETR [4] removes the
need for many hand-designed components like a non-
maximum suppression procedure or anchor generation. By
virtue of Transformer [30], DETR takes an image as input
and directly outputs a Ô¨Åxed set of box predictions. For the
point-based detection task, DETR has a beneÔ¨Åcial charac-
teristic: a single-level feature map, avoiding the multi-level
selection problem. However, directly applied DETR into
point-based detection task is not practical. Object queries
in DETR are general embeddings and have no speciÔ¨Åc point
information. Conversely, our detector encodes the position
and category of annotated points into object queries with the
point encoder and establishes one-to-one correspondences
between point annotations and object queries.

3. Method

In this section, we Ô¨Årst introduce the task of weakly
semi-supervised object detection (WSSOD) with point an-
notations and discuss why existing object detectors can not
Ô¨Åt this task well. Next, in order to solve it, we illustrate our
novel detector, Point DETR, in detail.

WSSOD with point annotations: WSSOD generally uses
a small set of instance-level labeled images and tremendous
weakly image-level labeled images as training data (Fig-
ure 1c). However, for object detection, image-level labeled

Figure 2. Overall framework. The white arrows represent the
training stage, and the black arrows represent the inference stage.
The steps of the framework are represented by red, yellow, blue
rounded rectangles respectively. Best viewed in color.

images do not Ô¨Åt WSSOD well, since it can not provide in-
stance information. This raises a natural question: is there a
new data annotation for weakly labeled images, which has
instance information without a large annotation burden? In
this paper, we introduce point annotation for weakly labeled
images.

Point Annotations: It is introduced by Bearman et al. [1]
for weakly semantic segmentation, but it has not been ex-
plored well in object detection. In object detection, we de-
Ô¨Åne point annotation as follows: it locates on the object and
takes object class as its category. Thus, we represent an
object as (x, y, c), where (x, y) ‚àà [0, 1]2 and c represent
point location and object category, respectively. We must
note that our method is robust to point location, as shown
in Table 1e. Therefore, the point annotations can locate at
the anywhere of objects. In this way, we can alleviate the
annotation burden.

Overall Framework: With this new setting that a small
number of supervised images and a large number of
weakly supervised images, we adapt self-training as our
default training pipeline, which has made considerable
progress in semi-supervised learning (e.g. Lee [13],Noise-
Student [32],STAC [26]). The steps are summarized as fol-
lows:

1. Train a teacher model on available labeled images.

horsepersonstep2. generate pseudo labelsstep1. train a teacher modelTeachersupervised imagesweakly supervised imagessupervised imagespseudo-labeled imagesStudenthorsepersonhorsepersonhorsepersonstep3. train a student modelFigure 3. Point DETR takes the image and its corresponding object points as input. The object points are normalized to [0, 1]2, and are
encoded into object queries by the point encoder module. The transformer decoder takes the object queries and additionally attends to
the image features (extracting by backbone and encoder). The output of the transformer decoder is passed to the head, generating box
predictions. The box predictions are the relative offsets from the four sides of a bounding box to the point location. The components that
are different from DETR are highlighted by light yellow.

2. Generate pseudo-labels of weakly point annotated

images using the trained teacher model.

3. Train a student model with fully labeled images and

pseudo-labeled images.

The overall framework is shown in Figure 2. For most
self-training based detection methods, hyper-parameters are
selected carefully since they must keep true object boxes
and screen out false ones as much as possible. Instead, we
can directly predict the corresponding object box for each
point annotation without duplicate object boxes. Although
choosing hyper-parameters is no longer an obstacle to per-
formance, predicting object boxes from point-level annota-
tions with existing detectors remains a problem.

Discussion on Existing Detectors: Existing detectors can
be divided into two categories: multi-level feature detectors
and single-level feature detectors. For multi-level detec-
tors (e.g. FCOS[29]), it is difÔ¨Åcult for them to predict ob-
ject boxes with point annotations since point annotations do
not have feature-level information, which is used to select
one prediction from multi-level box predictions (Figure 8b).
On the other hand, single-level feature detectors (e.g. Faster
R-CNN[22]) suffer from the bad performance or strict re-
quirement on point annotations though avoiding choosing
feature map levels (Figure 8c). For more experiments see
Section 4.3.

3.1. Point DETR

To avoid the drawbacks of existing detectors in the WS-
SOD with point annotations task, we introduce a novel de-
tector, Point DETR: adding a point encoder to DETR. It
transforms point annotations into object queries, extracts
image features for each object query, and outputs the cor-
responding object box. Next, we introduce a key element of
Point DETR, point encoder, which is critical to the WSSOD
with point annotations task.

DETR: We begin by reviewing DETR [4], which is an end-
to-end set-based object detector. DETR consists of a CNN
backbone, an encoder-decoder transformer, and a predic-
tion head. DETR Ô¨Årst extracts a single-level 2D feature map
from the CNN backbone, Ô¨Çattens it, and supplements it with
a positional encoding. Then, the encoder-decoder trans-
former takes as input a Ô¨Åxed set of object queries (learned
positional embeddings) and attends to 1D image feature em-
beddings. Finally, the output embeddings of the transformer
are passed to the prediction head that predicts either a de-
tection (class and bounding box) or a ‚Äúno object‚Äù class.

Point DETR: Point DETR, as shown in Figure 3, adopts
most components of DETR. To Ô¨Åt the point annotated im-
ages, Point DETR has a special module, point encoder.
Point encoder can encode the point annotations into object
queries, which are taken as input by the transformer de-
coder. Unlike the object queries in DETR that are learned
positional embeddings, these object queries are speciÔ¨Åc in-
stance embeddings which contain position and category in-
formation of object instances. Thus, these object queries
have a one-to-one correspondence with object instances.
Moreover, the number of object queries varies with the
number of object instances in an image instead of a Ô¨Åxed
number(e.g. 100) like DETR.

During training, we simply deÔ¨Åne the loss of each ob-
ject query as L = Lbox, since we already have category
for each object query and only need to regress the object
box. The bounding-box loss Lbox is identical as it deÔ¨Åned
in DETR. But, for the box prediction ÀÜbi, it calculated by
ÀÜbi = ÀÜbinit
‚àà [0, 1]4 is (x, y, x, y), (x, y)
is the location of point annotation and ‚àÜÀÜbi ‚àà [0, 1]4 is
the relative offsets w.r.t. the point location (x, y) follow-
ing FCOS [29]. In our experiments, we show this way of
regression can alleviate the mismatch between point anno-
tation and object box, see Section 4.3.

i + ‚àÜÀÜbi, where ÀÜbinit

i

Point Encoder: In point DETR, how to encode point an-
notations into object queries is critical for point encoder.

Figure 4. Point encoder. For each point (x, y, c), it encodes the po-
sition (x, y) and category c separetely, and then takes the element-
wise addition as the point embedding.

As shown in Figure 4, a point annotation (x, y, c) is decom-
posed to a 2D coordinate (x, y) ‚àà [0, 1]2 and category index
c. Based on (x, y), the position embedding epos ‚àà R256 is
extracted from Ô¨Åxed spatial positional encodings [30, 18, 4],
which is the same as one used in the transformer encoder.
For category embedding ecat ‚àà R256, it is obtained from
predeÔ¨Åned learnable category embeddings by category in-
dex, i.e. c. In the end, we fuse these embedding to get the
object query by sum operation.

Though point encoder is simple and easily implemented,
it bridges the divisions between point annotations and ob-
ject queries. In the experiments, we show the essentials of
every component (positional encoder and category encoder)
in point encoder, see section 4.3.

4. Experiments

We evaluate our models on the COCO 2017 detection
dataset [16] with synthetic point annotations (details in sec-
tion 4.1). We report the standard COCO metrics including
AP (averaged over IoU thresholds), AP50, AP75. In addi-
tion, to show the quality of generated pseudo-boxes, we also
calculate the mIoU between the generated pseudo-boxes
and the ground truth bounding boxes.

With existing detectors that can not be directly applied to
our point annotated settings, we make some modiÔ¨Åcations
to existing detectors: FCOS and Faster R-CNN. These mod-
iÔ¨Åed detectors are denoted as FCOS‚Ä† and Faster R-CNN‚Ä†,
respectively. For FCOS‚Ä†, we separately extract point fea-
tures from multi-level feature maps by bilinear interpola-
tion [9] and predict the corresponding object box, Ô¨Ånally
use the box prediction with the highest point category score
as the pseudo-box. As for Faster R-CNN‚Ä†, we extract point
features from one-level feature map, and then predict boxes

Figure 5. Absolute vs. Relative Regression: Different colors to
distinguish instances and the color of the point annotation is con-
sistent with its corresponding box. Best viewed in color.

for different anchors, Ô¨Ånally use one with the highest point
category score as the pseudo-box.

4.1. Implementation Details

We use ResNet-50 [8] as the default backbone for differ-
ent detectors and set the hyper-parameters following these
detectors.

Dataset: We train the model with 118k training images and
evaluate the performance of the detectors on the remaining
5k val images. Specially, for our point annotated setting,
we randomly sample 5%, 10%, 20%, 30%, 40%, 50% of
training images as the fully labeled set and use the rest of
the images as a weakly labeled set. In this paper, we noted
them as different data settings for simplicity, e.g. 20% data
setting. For the weakly labeled set, we synthesize the point
annotations for each object as follows: (a) if the object has
instance segmentation, randomly sample a point from the
instance mask as the point annotation for the object; (b) if
not, simply randomly sample a point in its bounding box.

Training: In our framework, there are two models:
the
teacher model and student model. Our teacher model in-
cludes Point DETR, FCOS‚Ä†, and Faster R-CNN‚Ä†. While we
simply choose FCOS as the default student model since the
student model is only used to evaluate the effectiveness of
the teacher model. We show by experiments (in section 4.3)
that our method is robust to the architecture of the student
model.

decompose(ùë•,ùë¶,ùëê)ùëêcomposecategory encoder‚Ä¶256ùëê(ùë•,ùë¶)(ùë•,ùë¶)positional encoder256+ùë™(a) Ground truth.(b) Absolute regression.(c) Relative regression.Figure 6. Comparison in APs of the student model (i.e. FCOS)
for different methods on MS-COCO. ‚ÄúSupervised‚Äù refers to the
student models trained on labeled data only.

For the training of the teacher model, it is simple for
FCOS‚Ä† and Faster R-CNN‚Ä†. We train them with their de-
fault training settings. For a fair comparison, we also use
data augmentation as shown in [4]. For Point DETR, it fol-
lows most of the training settings used in [4] with several
differences: we train the model for 108 epochs on 8 GTX
1080Ti GPUs, with 2 images per GPU. To ensure training
stability, we use a warmup scheme [7] in the Ô¨Årst epoch.
The learning rate is reduced by a factor of 10 at epoch 72
and 96, respectively. In the training, we randomly sample a
point in each bounding box and transform points into point
annotations. With these point annotations, we train Point
DETR as shown in Figure 3.

For the default student model, we combine the fully la-
beled images and pseudo-labeled images generated by the
teacher model to train the student, as showed in Figure 2.

4.2. Main Results

We Ô¨Årst show the effectiveness of Point DETR on dif-
ferent data split settings, see Figure 6. We train the stu-
dent model (i.e. FCOS) only with the fully annotated im-
ages (noted as ‚ÄúSupervised‚Äù). By comparing ‚ÄúSupervised‚Äù
with the student model trained with pseudo-boxes, we can
evaluate the beneÔ¨Åts brought by the pseudo-boxes. Point
DETR and FCOS‚Ä† outperform ‚ÄúSupervised‚Äù by a large mar-
gin. This demonstrates that images with point annotations
can improve the performance of the detection task. Further-
more, Point DETR outperforms FCOS‚Ä† by a considerable
margin.

Next, we verify the factors that contribute to the great
performance of our method. We compare the accuracies of
FCOS and DETR as shown in Figure 7, DETR performs
worse than FCOS in most settings. Given that our method
based on DETR achieves greater performance, we can con-
clude that the high accuracy of our method does not mainly

Figure 7. Comparison in APs of FCOS and DETR to demonstrate
the improvement comes from our method rather than a stronger
teacher model. FCOS trained in DETR augmentation for a fair
comparison. In most cases (5% ‚àº 40%), FCOS has a better per-
formance than DETR.

beneÔ¨Åt from its strong representation. Moreover, we con-
duct quality and quantity experiments to show the superi-
ority of our method on pseudo-object boxes, see Figure 8.
FCOS‚Ä†, a multi-level feature detector, can not predict ob-
ject boxes well due to FPN, and Faster R-CNN‚Ä†, a single-
level feature detector, also has difÔ¨Åculty regressing box ow-
ing to poor representation. But, Point DETR can gener-
ate a more precise object box than other detectors. Specif-
ically, the mIoU of Point DETR is larger than FCOS‚Ä† and
Faster R-CNN‚Ä† by 6.3 and 5.3, respectively. Based on the
above experiments, our method achieves considerable per-
formance mainly by generating precise pseudo-object boxes
from point annotations.

4.3. Ablation Experiments

We conduct the ablation experiments at 20% data setting.

Results are shown in Table 1 and discussed in detail next.

Point Encoder: Table 1a shows the effectiveness of the
components in the point encoder module (as we shown in
Figure 4) . Point DETR with only positional embeddings
outperforms one with only category embeddings and point
DETR has a severe loss in AP (18.6 points) without posi-
tional embeddings. Based on that our method only regresses
the object boxes, this suggests that it is difÔ¨Åcult to learn the
relative offsets of a point with respect to the bounding box
without positional embeddings. We also Ô¨Ånd that adding
category embeddings to positional embeddings can boost
the performance by 2 points. We conjecture this improve-
ment is caused by the that category embeddings can provide
object prior, such as object shape.

Student Model: For student model, we use FCOS [29] as
the default detector. To exploit robustness of our approach,

17.221.12528.530.632.825.528.431.332.833.834.526.230.433.334.835.435.815202530350%10%20%30%40%50%APCOCOSupervisedFCOS‚Ä†Point DETR(ours)20.925.430.933.334.935.418.223.729.332.934.836.315202530350%10%20%30%40%50%APCOCOFCOSDETRPoint Encoder

pos? cate? AP AP50 AP75
(cid:88) 14.7 34.3
10.4
32.6
31.3 51.0
(cid:88) 33.3 53.5
34.8

(cid:88)
(cid:88)

(a) Point Encoder: The effectiveness of positional encoder
and category encoder.

Teacher
FCOS‚Ä†
Ours
FCOS‚Ä†
Ours

Student

RetinaNet

FCOS

AP AP50 AP75
31.6
30.4 49.9
33.7
32.5 52.8
32.6
31.3 50.7
34.8
33.3 53.5

(b) Student Model: RetinaNet [15] as the student model demonstrates
the effectiveness of our approach is not related to the student model.

Faster R-CNN‚Ä†
Ours

AP AP50 AP75
32.6
31.4 51.6
34.8
33.3 53.5

(c) Single-Level Detector: Point DETR
vs. Faster R-CNN‚Ä†.

points?

score?

(cid:88)

(cid:88)

AP
19.1
26.9
26.8
-0.1

DETR

Ours
‚àÜ

UFO2
Ours

Supervised AP AP50 AP75
30.1
33.5 53.8

29.1
28.1
(d) Comparison with UFO2 [23]:
‚ÄúSuper-
vised‚Äù refers to the model trained with fully la-
beled data only.

-
34.8

-

center? AP AP50 AP75
34.8
33.3 53.5
33.3 53.6
34.6

(cid:88)

Ours
Ours

(e) Point Location: The effectiveness
of the point location.

AP50 AP75 APs APm APl AR1 AR10 AR100
33.6
33.2
33.6
43.8
44.5
52.3
+10.9
+8.5

32.8
33.2
44.0
+10.8

20.2
27.7
29.5
+1.8

22.9
24.2
30.7
+6.5

5.6
9.2
12.6
+3.4

18.7
27.5
24.2
-3.3

31.3
39.9
38.9
-1.0

ARs
12.0
12.0
22.8
+10.8

ARm ARl
51.0
35.0
51.0
35.0
63.9
46.9
+12.9
+11.9

(f) Point Annotations: To conÔ¨Årm the beneÔ¨Åts of point annotations, we compare Point DETR (with points) vs. DETR (without points)
by analyzing the generated boxes with respect to ground truth boxes. With AR far exceeding DETR, our AP remains comparable.

Table 1. Ablations. All ablation experiments are conducted at 20% data setting except (d).

Figure 8. Visualized results of FCOS‚Ä†, Faster R-CNN‚Ä† and Point DETR ( ours). The mIoU between the ground truth boxes and pseudo-
boxes on the entire weakly labeled images are provided. Different colors to distinguish instances, and the color of the point annotation is
consistent with its corresponding box. Best viewed in color.

we replace FCOS with RetinaNet [15].
In Table 1b, we
Ô¨Ånd that our method has a 2.1 AP gain over baseline. This
demonstrates that our method is robust to the student model.

Single-Level Detector: We compare Point DETR with
single-level feature detectors and choose Faster R-CNN‚Ä†
as the default single-level feature detector. As shown in

Table 1c, Point DETR outperforms Faster R-CNN‚Ä† by 1.9
points. This highlights that effectiveness of Point DETR.

Comparison with UFO2 [23]: To show the effectiveness
of our method, we compare Point DETR with UFO2.
For fair comparison, we train Point DETR following the
dataset split in UFO2: COCO-35 (fully labeled images) and

(a) Ground truth.(b) FCOS‚Ä†. mIoU: 57.1.(c) Faster R-CNN‚Ä†. mIoU: 58.1.(d) Point DETR (ours). mIoU: 63.4.COCO-80 (point labeled images). As shown in Table 1d,
our method has inferior performance than UFO2 when is
only trained on COCO-35, but it outperforms UFO2 by 3.4
points adding COCO-80. This indicates that our method
can make better use of point annotation information.

Point Location: To validate that our method is robust to
the point location, we compare performances between two
point location schemes: center point and arbitrary point on
objects. As shown in Table 1e, our method has comparable
performance between these two point location scheme.

Absolute vs. Relative Regression: Our method use rela-
tive regression to predict object boxes. In Figure 5, we com-
pare our relative regression with absolute regression used in
DETR. Absolute regression incorrectly matches the point
with the bounding box that does not correspond (e.g. the
green clock in Figure 5b) in some cases. Compared with
absolute regression, relative regression has little mismatch
problem between point and object box, we attribute it to its
use of the prior knowledge: the point is in the bounding box.

Point Annotations: To evaluate the effectiveness of point
annotations, we compare our approach with the method
without point annotations. For a fair comparison, we use
DETR as the method without point annotations. We ap-
ply a self-training framework (following [26]) on DETR
directly. We train DETR with only fully labeled images
Ô¨Årst, and then generate pseudo-boxes for weakly labeled im-
ages without point annotations. To remove duplicate boxes,
we use a threshold œÑ = 0.7 which results in the best box
predictions on weakly labeled images. For the generated
pseudo-object boxes, they do not have the one-to-one cor-
respondence with point annotations. Thus, it is impractical
to calculate the mIoU between generated boxes and ground
truth boxes. To make comparison available, we use stan-
dard COCO metrics instead of mIoU , as shown in Table 1f.
Point DETR performs on par with DETR on mAP and out-
performs DETR by a large margin in the recall. SpeciÔ¨Åcally,
Point DETR achieves over 10 points of improvements in
various AR metrics (e.g. ARs, ARm, ARl, AR100) and its
AP is comparable with DETR (26.8 vs. 26.9). Though Point
DETR is 3.3 points AP75 lower than DETR, which is possi-
bly explained by that high œÑ screens out low-quality boxes
and remains high-quality boxes, the higher recall of Point
DETR can offset this bad inÔ¨Çuence.

Additionally, we set the classiÔ¨Åcation score of pseudo-
boxes generated by DETR to a constant value like 0.5,
In this setting, the
which is consistent with our method.
performance of DETR drops by a large margin and per-
forms much worse than our method. This highlights that
with point annotations, our method does not suffer from the
quality of classiÔ¨Åcation score.

We also analyze the errors of the generated boxes by
TIDE [3] in Figure 9. Missed ground truths is the largest

Figure 9. Diagnosing the errors of generated pseudo-boxes by
TIDE [3]. Different error types: Cls: localized correctly but classi-
Ô¨Åed incorrectly, Loc: classiÔ¨Åed correctly but localized incorrectly,
Both: both cls and loc error, Dupe: duplicate detection error, Bkg:
detected background as foreground, Miss: missed ground truth er-
ror.

issue for DETR, while it does not affect the performance of
Point DETR greatly. This is explained by that with point an-
notations, Point DETR does not miss objects like DETR. In
addition, unlike DETR, location error is the main challenge
of Point DETR. Also, Point DETR also has duplicate de-
tection errors. This is caused by those point annotations re-
siding in multiple bounding boxes that would predict object
boxes for wrong ground truths, which results in a ground
truth that has multiple box predictions.

5. Conclusion

In this work, we verify the effectiveness of point anno-
tations in the weakly semi-supervised detection task. We
also show that the power of point annotations is hindered
by existing detectors.
In order to solve this, we propose
the Point DETR which applies a point encoder to the point
annotations to establish the one-to-one correspondence be-
tween point annotations and objects. Our approach is sim-
ple and implemented easily. We demonstrate its efÔ¨Åcacy by
the extensive experimental analysis showing that it achieves
state-of-the-art performance.

Acknowledgments This work is supported by National
Key R&D Program of China (2020AAA0105200).

(a) DETR.(b) Point DETR (ours).References

[1] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li
Fei-Fei. What‚Äôs the point: Semantic segmentation with point
In European conference on computer vision,
supervision.
pages 549‚Äì565. Springer, 2016.

[2] Hakan Bilen and Andrea Vedaldi. Weakly supervised deep
detection networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2846‚Äì
2854, 2016.

[3] Daniel Bolya, Sean Foley, James Hays, and Judy Hoffman.
Tide: A general toolbox for identifying object detection er-
rors. arXiv preprint arXiv:2008.08115, 2020.

[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-
to-end object detection with transformers. arXiv preprint
arXiv:2005.12872, 2020.

[5] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qing-
ming Huang, and Qi Tian. Centernet: Keypoint triplets for
object detection. In Proceedings of the IEEE International
Conference on Computer Vision, pages 6569‚Äì6578, 2019.
[6] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
International journal of computer
classes (voc) challenge.
vision, 88(2):303‚Äì338, 2010.

[7] Priya Goyal, Piotr Doll¬¥ar, Ross Girshick, Pieter Noord-
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
large mini-
Yangqing Jia, and Kaiming He. Accurate,
arXiv preprint
batch sgd: Training imagenet in 1 hour.
arXiv:1706.02677, 2017.

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770‚Äì778, 2016.

[9] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Advances in neural infor-
mation processing systems, pages 2017‚Äì2025, 2015.
[10] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak.
Consistency-based semi-supervised learning for object de-
tection. In Advances in neural information processing sys-
tems, pages 10759‚Äì10768, 2019.

[11] Zequn Jie, Yunchao Wei, Xiaojie Jin, Jiashi Feng, and Wei
Liu. Deep self-taught learning for weakly supervised ob-
In Proceedings of the IEEE Conference
ject localization.
on Computer Vision and Pattern Recognition, pages 1377‚Äì
1385, 2017.

[12] Hei Law and Jia Deng. Cornernet: Detecting objects as
paired keypoints. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pages 734‚Äì750, 2018.

[13] Dong-Hyun Lee.

Pseudo-label: The simple and efÔ¨Å-
cient semi-supervised learning method for deep neural net-
works. In Workshop on challenges in representation learn-
ing, ICML, volume 3, 2013.

[14] Tsung-Yi Lin, Piotr Doll¬¥ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie.
Feature pyra-
In Proceedings of the
mid networks for object detection.
IEEE conference on computer vision and pattern recogni-
tion, pages 2117‚Äì2125, 2017.

[15] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll¬¥ar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE international conference on computer
vision, pages 2980‚Äì2988, 2017.

[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740‚Äì755.
Springer, 2014.

[17] Nhu-Van Nguyen, Christophe Rigaud, and Jean-Christophe
Burie. Semi-supervised object detection with unlabeled data.
In VISIGRAPP (5: VISAPP), pages 289‚Äì296, 2019.

[18] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, ≈Åukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-
age transformer. arXiv preprint arXiv:1802.05751, 2018.
[19] Rui Qian, Yunchao Wei, Honghui Shi, Jiachen Li, Jiaying
Liu, and Thomas Huang. Weakly supervised scene parsing
with point-based distance metric learning. In Proceedings of
the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 33,
pages 8843‚Äì8850, 2019.

[20] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: UniÔ¨Åed, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 779‚Äì788, 2016.
[21] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,
stronger. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7263‚Äì7271, 2017.
[22] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, pages 91‚Äì99, 2015.

[23] Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu
Liu, Alexander G Schwing, and Jan Kautz. Ufo 2: A uni-
Ô¨Åed framework towards omni-supervised object detection. In
European Conference on Computer Vision, pages 288‚Äì313.
Springer, 2020.

[24] Olga Russakovsky, Li-Jia Li, and Li Fei-Fei. Best of both
worlds: human-machine collaboration for object annotation.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 2121‚Äì2131, 2015.

[25] Miaojing Shi, Holger Caesar, and Vittorio Ferrari. Weakly
supervised object localization using things and stuff trans-
fer. In Proceedings of the IEEE International Conference on
Computer Vision, pages 3381‚Äì3390, 2017.

[26] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang,
Chen-Yu Lee, and Tomas PÔ¨Åster. A simple semi-supervised
arXiv preprint
learning framework for object detection.
arXiv:2005.04757, 2020.

[27] Hao Su, Jia Deng, and Li Fei-Fei. Crowdsourcing annota-
tions for visual object detection. In Workshops at the Twenty-
Sixth AAAI Conference on ArtiÔ¨Åcial Intelligence, 2012.
[28] Peng Tang, Chetan Ramaiah, Ran Xu, and Caiming Xiong.
learning for semi-supervised object detection.

Proposal
arXiv preprint arXiv:2001.05086, 2020.

[29] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos:
Fully convolutional one-stage object detection. In Proceed-
ings of the IEEE international conference on computer vi-
sion, pages 9627‚Äì9636, 2019.

[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998‚Äì6008, 2017.
[31] Keze Wang, Xiaopeng Yan, Dongyu Zhang, Lei Zhang, and
Liang Lin. Towards human-machine cooperation: Self-
supervised sample mining for object detection. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1605‚Äì1613, 2018.

[32] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V
Le. Self-training with noisy student improves imagenet clas-
siÔ¨Åcation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 10687‚Äì
10698, 2020.

[33] Ziang Yan, Jian Liang, Weishen Pan, Jin Li, and Chang-
shui Zhang. Weakly-and semi-supervised object detection
with expectation-maximization algorithm. arXiv preprint
arXiv:1702.08740, 2017.

[34] Shiyin Zhang, Jun Hao Liew, Yunchao Wei, Shikui Wei,
and Yao Zhao. Interactive object segmentation with inside-
In Proceedings of the IEEE/CVF Con-
outside guidance.
ference on Computer Vision and Pattern Recognition, pages
12234‚Äì12244, 2020.

[35] Xingyi Zhou, Dequan Wang, and Philipp Kr¬®ahenb¬®uhl. Ob-
jects as points. arXiv preprint arXiv:1904.07850, 2019.
[36] Yi Zhu, Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin
Jiao. Soft proposal networks for weakly supervised object
localization. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 1841‚Äì1850, 2017.

