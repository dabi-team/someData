Astronomy & Astrophysics manuscript no. mesa
June 18, 2021

©ESO 2021

Maximum Entropy Spectral Analysis: a case study

A. Martini1, S. Schmidt2, and W. Del Pozzo1

1 Dipartimento di Fisica, Università di Pisa, and INFN Sezione di Pisa, Pisa I-56127,Italy,

e-mail: martini.alessandr@gmail.com

2 Institute for Gravitational and Subatomic Physics (GRASP),Utrecht University, Princetonplein 1, 3584 CC, Utrecht, The Nether-

lands,
e-mail: s.schmidt@uu.nl

June 18, 2021

1
2
0
2

n
u
J

7
1

]
E
M

.
t
a
t
s
[

1
v
9
9
4
9
0
.
6
0
1
2
:
v
i
X
r
a

ABSTRACT

The Maximum Entropy Spectral Analysis (MESA) method, developed by Burg, provides a powerful tool to perform spectral estima-
tion of a time-series. The method relies on a Jaynes’ maximum entropy principle and provides the means of inferring the spectrum of a
stochastic process in terms of the coeﬃcients of some autoregressive process AR(p) of order p. A closed form recursive solution pro-
vides an estimate of the autoregressive coeﬃcients as well as of the order p of the process. We provide a ready-to-use implementation
of the algorithm in the form of a python package memspectrum. We characterize our implementation by performing a power spectral
density analysis on synthetic data (with known power spectral density) and we compare diﬀerent criteria for stopping the recursion.
Furthermore, we compare the performance of our code with the ubiquitous Welch algorithm, using synthetic data generated from
the released spectrum by the LIGO-Virgo collaboration. We ﬁnd that, when compared to Welch’s method, Burg’s method provides a
power spectral density (PSD) estimation with a systematically lower variance and bias. This is particularly manifest in the case of a
little number of data points, making Burg’s method most suitable to work in this regime.

1. Introduction

The study of the properties of stochastic processes is a cru-
cial task in many ﬁelds of physics, astronomy, quantitative bi-
ology, as well as engineering and ﬁnance. Among those, a spe-
cial place is occupied by the so-called wide-sense stationary pro-
cesses. These are stochastic processes that display an invariance
of their statistical properties, such as their two-point autoco-
variance function, with respect to time translation. If x(t) is a
wide-sense stationary process, it is completely determined by the
knowledge of the autocorrelation function

C(τ) = E[xt · xt+τ]

(1)

or, equivalently, by the knowledge of their power spectral den-
sity (PSD) S ( f ). Thanks to the Wiener-Khinchin theorem the
two are related by a Fourier transform:

S ( f ) =

(cid:90) ∞

−∞

dτC(τ)e−i2π f τ .

(2)

In some literature, especially in the context of gravitational
waves physics, e.g. (Finn 1992), the PSD is introduced as

E[ ˜x( f ) · ˜x( f (cid:48))] = S ( f )δ( f − f (cid:48))

(3)

without highlighting its connection with the time structure of the
process itself, thus masking some important properties that will
be explored further in what follows. The latter deﬁnition in (3)
gives, however, i) a straightforward interpretation of the PSD: it
measures how much signal “power" is located in each frequency;
ii) an operative way of estimating it for an unknown process.

An ubiquitous method for such computation is due to Welch
(1967) and it is based on Eqs.(2-3). The PSD is obtained by slic-
ing the observed realization x(t1), . . . , x(tn) of the process x(t)
into many window-corrected batches and averaging the squared

moduli of their Fourier transforms. This approach is equiva-
lent (Lomb 1976; Scargle 1982) to taking the Fourier Transform
of the windowed sample autocorrelation ρW , written as
ρW = {W0ρ0, W±1ρ±1, . . . , W±Mρ±M, 0, 0, . . . } ,

(4)

where ρ is the empirical autocorrelation and M is the maximum
time lag at which the autocorrelation is computed. The sequence
W is a window function that can be chosen in several diﬀerent
ways, each choice presenting advantages and disadvantages for
the ﬁnal estimate of the PSD.

The choice of a window function is arbitrary and typically is
made by trial and error, until a satisfactory compromise between
variance and resolution of the estimate of PSD is reached. A
high frequency resolution implies high variance and vice-versa.
Besides the window function, Welch’s method requires a number
of arbitrary choices to be made, such as the number of time slices
and the overlap between consecutive slices. All these knobs must
be tuned by hand and their choice can dramatically aﬀect the
PSD estimation, hence begging the question of what the “best"
PSD estimate is.

Another drawback of this approach is the requirement for the
window to be 0 outside the interval in which the autocorrelation
is computed. We are arbitrarily assuming ρ j = 0 for j > M
and modifying the estimate (i.e. the data) if a non-rectangular
window is chosen. Making assumptions on unobserved data and
modifying the ones we have at our disposal introduces “spuri-
ous" information about the process that we, in general, do not
really have.

A alternative approach providing a smooth PSD estimation,
is to adopt a parametric model for the PSD and to ﬁt its param-
eters to the data with a Markov Chain Monte Carlo (Cornish &
Littenberg 2015; Littenberg & Cornish 2015, e.g.). Despite be-
ing eﬀective, this method is problem dependent, since it needs to
make deﬁnite assumptions on the shape of the PSD. Moreover, it

Article number, page 1 of 16

 
 
 
 
 
 
A&A proofs: manuscript no. mesa

can be computationally expensive and it does not come with an
handy implementation available to the public. For all the above
reasons, we did not consider such method in our work.

An appealing alternative, based on the Maximum Entropy
principle (Jaynes 1957; Jaynes & Bretthorst 2003; Jaynes 1982),
has been put forward by Burg (1975). Being rooted on solid
theoretical foundations, we will see that Burg’s method, unlike
Welch’s, does not require any preprocessing of the data and re-
quires very little tuning of the algorithm parameters, since it pro-
vides an iterative closed form expression for the spectrum of
a stochastic stationary time series. Furthermore, it embeds the
PSD estimation problem into an elegant theoretical framework
and makes minimal assumptions on the nature of the data. Lastly
and most importantly, it provides a robust link between spectral
density estimation and the ﬁeld of autoregressive processes. This
provides a natural and simple machinery to forecast a time series,
thus predicting future observations based on previous ones.

In this paper, we discuss the details of the Maximum entropy
principle, its application to the problem of PSD estimation with
Burg’s algorithm and the link between Burg’s algorithm and au-
toregressive process. Our goal is to bring (again) to public atten-
tion Maximum Entropy Spectral analysis, in the hope that it will
be widely employed as a way out to the many undesired aspects
of the Welch’s algorithm (or other similar methods). To facilitate
this goal, we present and describe a new code, memspectrum,
that provides a robust and easy-to-use python implementation of
the algorithm1. We provide a thorough assessment of the per-
formance of our code and we validate our results performing a
number of tests on simulated and real data. We also compare our
results with those of spectral analysis carried out with the stan-
dard Welch’s method. In order to apply our model on a realistic
setting, we analyse some time series of broad interest in the sci-
entiﬁc community.

Our paper is organized as follows: we begin by brieﬂy re-
viewing the theoretical foundations of the maximum entropy
principle in Sec. 2. Sec. 3 presents the validation of Burg’s
method as well as of our implementation on simulated data.
In Sec. 4 we compare the results from memspectrum with the
Welch method; Sec. 5 presents a few applications to real time
series and, ﬁnally, we conclude with a discussion in Sec. 6.

2. Theoretical foundations

The Maximum Entropy principle (MAXENT) is among the most
important results in probability theory. It provides a way to
uniquely assign probabilities to a phenomenon in a way that best
represent our state of knowledge, while being non committal
with unavailable information. Its domain of application turned
out to be wider than expected. In fact, thanks to Burg (1975),
this method has also been applied to perform high quality com-
putation of power spectral densities of time series.

After a short introduction to Jaynes’ MAXENT (sec. 2.1),
we will develop in detail Burg’s technique of Maximum Entropy
Spectral Analysis (MESA) and show that the estimate can be
expressed in an analytical closed form (sec. 2.2). Next, we will
discuss an interesting link between Burg’s method and autore-
gressive processes (sec. 2.3) and in sec. 2.4 we will use such link
for straightforwardly forecasting a time series.

2.1. Maximum Entropy Principle

Before introducing MAXENT principle, we will deﬁne through
some simple examples the two core concepts of the problem and
the role they play: the ‘evidence’ and the ‘information’. Let us
start with the ‘information’ (or entropy): it is a measure of the
degree of uncertainty on the outcomes of some experiment and
speciﬁes the length of the message necessary to provide a full de-
scription of the system under study. For instance, consider a per-
fectly sinusoidal signal: knowledge of amplitude, frequency and
phase are suﬃcient to fully reproduce it: it has a low information
content. Even less information is required if we are studying a
system whose outcome is certain (has probability p = 1), as in
this case, a communication is not even needed. Shannon (1948)
proposed the quantity

I = log2

1
p(x)

(5)

to measure the quantity of information brought by an outcome x
with probability p(x). It is additive quantity as well as monoton-
ically decreasing as a function of p ∈ [0, 1]: the more uncertain
the outcome, the higher the information it brings.

We can generalize the deﬁnition of information in the case
where two diﬀerent outcomes E1, E2, with given probabilities P1
and P2, are possible. To gain some intuition on the problem, we
ask ourselves which are the probability assignments that make
the outcome more uncertain (i.e. maximize the information). If
P1 and P2 are largely diﬀerent, for instance P1 = 0.999 and P2 =
0.001, we are allowed to believe that event E1 will occur almost
certainly, considering E2 to be a very implausible outcome. The
information content will be very low. On the other hand, most
unpredictable experiment happens when
P1 = P2 = 1
2

:

this describes a situation of ‘maximum ignorance’ and the infor-
mation content of such system must be high. Any generalization
of eq. (5), must then have its maximum when P1 = P2. For N
events, the system with the highest possible information content
is when:
P1 = . . . = PN = 1
N

:

Shannon (1948) showed that the only functional form sat-
isfying continuity with respect to its parameters, additivity and
that has a maximum for equal probability events is:

H[p1, . . . , pN] = −

N(cid:88)

i=1

pi log pi,

(6)

The equation can be interpreted as the ‘expected information’
brought by an experiment with N possible outcomes each with
its own probability pi. In the continuous case:

(cid:90)

H[p(x)] = −

p(x) ln p(x)dx,

(7)

We call the functional H (information) entropy2.

We now turn to the core of our problem: how do we make
a probability assignment for a set of events, that keeps into ac-
count our knowledge of the system and, at the same time, it is

1 It
is
memspectrum/.

available

at

link:

https://pypi.org/project/

2 In deﬁning the information entropy as in Eq. (7) we are implicitly
assuming a uniform measure over the parameter space

Article number, page 2 of 16

Martini, Schmidt, Del Pozzo: MESA: a case study

non committal towards unavailable knowledge? The knowledge
at our disposal about the system under study is what we call ‘ev-
idence’ and any probability assignment must agree with it. In the
above cases, our knowledge on the system is only the total num-
ber N of diﬀerent outcomes – this is a minimal requirement. Of
course, more complex evidence constraints can be applied.

It is very common that the constraints provided by the evi-
dence are not enough for setting the probabilities for each event:
in this case, it is reasonable to assume that the probability assign-
ment should make the experiment as unpredictable as possible3.
In other words, the amount of ‘information’ introduced by the
probability assignment should be as high as possible.

MAXENT puts the aforementioned principle into a more
precise mathematical context by stating that probabilities should
be assigned by maximizing uncertainty (information entropy)
using evidence as constraint. This deﬁnes a variational prob-
lem, where the information entropy functional H (cid:2)p1, . . . , pN
(cid:3),
deﬁned in eq. (6), has to be maximized.

The maximisation of entropy, supplemented by evidence in
the form of constraints to which the sought-for probability dis-
tribution must obey, gives rise to several of the most common
probability distributions commonly employed in statistics. For
instance, whenever the only constraint available is the normal-
ization of the probability distribution (i.e. no evidence is avail-
able), the entropy is maximised by the uniform distribution. If
we have evidence to constraint the expected value, the informa-
tion entropy is maximised by the exponential distribution

Of particular importance is the case in which, in addition to
the mean, also the variance is known: MAXENT leads to the
Gaussian distribution. This derivation is particularly interesting
from the foundational point of view, since it provides a deeper
insight into the ubiquitous Gaussian distribution. Indeed, it is not
only the limit distribution provided by the central limit theorem
for ﬁnite variance processes but it is also the distribution that
maximizes the entropy. For this reason, appealing to MAXENT
principle, it is the correct assignment if mean and covariance are
the only quantities that fully deﬁne our process. In some sense,
we can interpret the central limit theorem as the natural ‘statisti-
cal’ evolution toward a conﬁguration that maximizes entropy.

For this work, we are particularly interested in the multi-
dimensional case. Suppose we have a vector of measurements
(x(t1), . . . , x(tn)) = (x1, . . . , xn) that we conveniently express as
a single realization of an unknown stochastic process x(t) and
we have information about the expectation value of the process
µ(t) and on the matrix of autocovariances Ci j ≡ C(ti, t j), then the
MAXENT distribution is the n-dimensional multivariate Gaus-
sian distribution (Gregory 2005):

ance matrix thus becomes a Toeplitz matrix4. Toeplitz matrices
are asymptotically equivalent to circulant matrices and thus di-
agonalized by the discrete Fourier transform base (Gray 2006).
Some simple algebra shows that the time-domain multivariate
Gaussian can be transformed into the equivalent frequency do-
main probability distribution:
p (cid:0)( ˜x1, . . . , ˜xn/2)|I(cid:1) =


−

1
(2π det S )n/2 exp

˜xiS −1

i j ˜x j

(cid:88)

1
2

(9)





,

i j

where the matrix S i j = S iδi j is an n×n diagonal matrix whose el-
ements are the PSD S ( f ) calculated at frequency fi. Many read-
ers will recognize the familiar form of the Whittle likelihood
that stands at the basis of the matched ﬁlter method(P. M. Wood-
ward & Higinbotham 1964) and of gravitational waves data anal-
ysis, (Finn 1992; Allen et al. 2012, e.g.). Thanks to MAXENT,
the problem of deﬁning the probability distribution describing a
wide-sense stationary process is thus entirely reduced to the esti-
mation of the PSD or, equivalently, the autocovariance function.

2.2. Maximum Entropy Spectral Analysis

In principle, if the autocorrelation was known exactly (i.e. at ev-
ery time τ ∈ (−∞, +∞)), the computation of the PSD would
reduce to a single Fourier transform. However, in any realistic
setting, we are dealing with a ﬁnite number of samples N from
the process, hence such computation is impossible. Moreover,
the error σk in the estimate of the autocorrelation after k steps
N − k5, so that only few values for the
increases as σ ∼ 1/
autocorrelation function can actually be computed reliably. This
bring us the core of the problem: how to give an estimate from
partial (and noisy) knowledge of the autocorrelation function?
MAXENT can guide us in this task without any a priori assump-
tions on the unavailable data6.

√

As in the previous examples, one needs to set up a varia-
tional problem where the entropy, Eq. (7), is maximized subject
to some problem-speciﬁc constraints. In our case, they are i) the
PSD estimate has to be non-negative; ii) its Fourier transform
has to match the sample autocorrelation (wherever an estimate
of this is available).

Before doing so, there is a technicality to solve: the deﬁnition
of entropy depends on a probability distribution, not on the PSD.
It can be shown, (Ables 1974; Bartlett 1968, e.g.), that the vari-
ational problem can be formulated in terms of the power spectral
density S ( f ) alone by considering our signal as the result of the

4 We remind the reader that a Toeplitz matrix is a matrix in the form:

p ((x1, . . . , xn)|I) =

1
(2π det C)k/2 exp


−



1
2

(cid:88)

i, j

(xi − µi)(x j − µ j)C−1
i j





.

(8)





a0
a−1
a−2
...
a−n+1
a−n

a1
a0
a−1
...
. . .
. . .

a2
a1
a0
...
. . .
. . .

. . .
. . .
. . .
...
. . .
. . .

. . .
. . .
. . .
...
a−1
a−2

. . .
. . .
. . .
...
a0
a−1





an
an−1
an−2
...
a1
a0

For a wide-sense stationary process the mean function is in-
dependent of time, hence it can be redeﬁned to be equal to zero
without loss of generality, and the auto-covariance function is
dependent only on the time lag τ ≡ ti − t j. One can thus choose
a sampling rate ∆t so that Ci j = C((i − j)∆t). The autocovari-

3 In Jaynes (1982) this statement is made more precise and justiﬁed
more thoroughly, with arguments based on combinatorial analysis.

5 This is clearly understood: when computing the autocorrelation at
order k, only N − k examples of the product xt xt+k are available and the
variance of the average value goes as the inverse of the square root of
the points considered.
6 Indeed this is the largest diﬀerence with the most common Welch
method. The latter assumes that the unknown values of the autocorre-
lation are 0. Clearly, this assumption is unjustiﬁed and MAXENT is a
good way to drop this assumption.

Article number, page 3 of 16

A&A proofs: manuscript no. mesa

ﬁltering a white noise process using a ﬁlter with transfer func-
tion T ( f ) equal to S ( f )7. The diﬀerence in entropy between the
input and the output time series (i.e. the entropy gain) obtained
by such ﬁlter applied on white noise is:

∆H =

(cid:90) Ny

−Ny

log S ( f )d f .

(10)

Thanks to Wold’s theorem (Wold 1939), every stationary
time series can be represented as an autoregressive process: this
ensures that maximum entropy estimation is faithful and gen-
eral; it turns out that the maximum entropy principle provides a
representation of the time series as an AR(p) process and Burg’s
algorithm computes the autoregressive coeﬃcients that are suit-
able to the available data.

where ∆t is sampling rate and Ny ≡ 1
Thus maximising Eq. (10) is equivalent to maximizing eq. (7).

2∆t is the Nyquist frequency.

Before maximizing the entropy gain, we need to include the
evidence available as a form of mathematical constraints for the
assignment of S ( f ). This is equivalent in imposing that the vari-
ational solution S ( f ) for the PSD matches the empirical auto-
correlation. Let us deﬁne a realization of a stochastic process
(x1, . . . , xN) with sample autocorrelations ¯rk, k = 0, . . . , N/2,
then the PSD must satisfy the following equation:
(cid:90) Ny

S ( f )eı2π f k∆td f = ¯rk .

−Ny

Thus, by maximizing Eq. (10) with constraints in Eq. (11),
we can give an estimate of the spectrum given an empirical time
series. This approach on PSD computation provides a result con-
sistent with the empirical autocorrelation function whenever this
is available and, at the same time, it does not make any assump-
tion for the unavailable estimates for the autocorelation at large
time lags.

(cid:16)(cid:80)N

s=0 a∗

S ( f ) =

sz−s(cid:17) ,

Most importantly, the variational problem admits a closed-
form analytical expression for S ( f ). The expression was ﬁrst
found by Burg (1975):
PN∆t
s=0 aszs(cid:17) (cid:16)(cid:80)N
is the sampling interval of the time series, z =
where ∆t
exp (2πi f ∆t), a0 = 1. The vector obtained as (1, a1, . . . , aN) is
also known as the prediction error ﬁlter. The coeﬃcients as(s >
0), together with an overall multiplicative scale factor PN, are to
be determined by an iterative process (called Burg’s algorithm).
The number N of such coeﬃcients is a choice that shall be made
by the user and indeed it is the only hyperparameter that needs
to be tuned. The details of the derivation and the actual form for
the coeﬃcients as can be found in appendix A.

(12)

2.3. Autoregressive Process Analogy

The application of MESA is not limited to spectral estimates, but
it also provides a link between spectral analysis and the study
of autoregressive processes (AR) (Ulrych & Bishop 1975). An
autoregressive stationary process of order p, AR(p), is a time
series whose values satisfy the following expression:
xt − b1xt−1 − b2 xt−2 . . . bpxt−p = νt
(13)
where b1, . . . , bp are real coeﬃcients and νt is white noise with
a given variance σ2. Thus, an AR(p) process models the depen-
dence of the value of the process at time t on every past p obser-
vations, thus being potentially able to model complex autocorre-
lation structures within observations.
7 A ﬁlter with transfer function T ( f ) takes in input a time series xt and
outputs a times series yt such that:

T ( f ) = ˜y( f )
˜x( f )

where ˜x( f ) denotes the Fourier transform of xt (and similarly for yt)

Article number, page 4 of 16

To show the analogy, we compute the PSD S AR(p) of an
AR(p) process and we show that it is formally equivalent to the
PSD obtained in Eq. (12). This will also provide a direct ex-
pression for the autoregressive coeﬃcients bi and for the noise
variance σ2. We start taking the z transform 8 of Eq. (13):
(cid:88)

(cid:88)

(cid:88)

bizi (cid:88)

xt−izt−i =

νtzt.

xtzt −

(14)

t

i

t

t

Calling ˜x(z) and ˜ν(z), the transformed quantities, in the z domain,
the process takes the form:

(11)

˜x(z) =

˜ν(z)
n=1 bnzn(cid:17)
(cid:16)
1 − (cid:80)p

(15)

Since we assumed a wide-sense stationary process, ˜x(z) is ana-
lytic both on and inside the unit circle. Taking its square value
and evaluating it on the unit circle z = e−ı2π f ∆t, from the deﬁni-
tion of spectral density one obtains:

S AR(p)( f ) = | ˜x(z)|2 =

|˜ν( f )|2
n=1 bneı2π f n∆t(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)1 − (cid:80)p

.

2

(16)

The numerator is the spectral density of white noise νt, i.e. its
(constant) variance σ2.

Eqs. (16) and (12) are equivalent, if we identify bi = −ai
and PN∆t = σ2. This shows that the MAXENT estimation of
the PSD models the observed times series as an AR process and
provides a ﬁt for the autoregressive coeﬃcients. Furthermore, as
a consequence of Wold’s theorem, there is the theoretical guar-
antee that every stationary time series can be modelled faithfully
by the MAXENT.

2.4. Forecasting

The link between MESA and AR processes is of particular in-
terest. Given the solution to Burg’s recursion to determine the
ak, we automatically obtain the coeﬃcients of the equivalent AR
process, hence we are able to exploit Eq. 13 to perform forecast-
ing, thus providing plausible future observations, conditioned on
the observed data. Indeed, for an AR(p) process the conditional
probability p(xt|xt−1, . . . , xt−p) of the observation at time t with
respect to the past p observation has the form:

p(xt|xt−1, . . . , xt−p)

=

1
√

σ

2π

exp








xt − (cid:80)p
i=1 bi xt−i
σ

−

1
2

2





.

(17)

The interpretation of Eq. (17) is straightforward: xt follows a
Gaussian distribution with a ﬁxed variance and a mean value
mt = (cid:80)p
i=1 bi xt−i computed from past observations. Eq. (17) pro-
vides then a well deﬁned probability framework for predicting
future observations: this is a very useful feature of MESA, that
does not have an equivalent in any other spectral analysis com-
putation methods.

8 The z transform is the discrete-time equivalent of the Laplace trans-
form, thus taking a discrete time-series and returning a complex fre-
quency series.

Martini, Schmidt, Del Pozzo: MESA: a case study

3. Validation of the model

It is now clear that MESA provides a recursive formula for com-
puting the coeﬃcients ak in Eq. (12). The number M of such
coeﬃcients is equivalent to the maximum order of the autocorre-
lation ¯rm considered. In an ideal scenario, this would be equal to
the number of points the autocorrelation is computed at (equiva-
lent to the length of data considered). However, the computation
of high order coeﬃcients of the autocorrelation is unstable and
for high enough m, as the estimation for ¯rm shows a very high
(cid:16) √
variance, broadly scaling as ∼

M − m

(cid:17)−1

.

It is then clear that the choice of the number of samples of
the discrete autocorrelation to consider is important: on the one
hand it is advisable to include as much knowledge of the auto-
correlation as possible, leading to include all the known ¯rm; on
the other hand, including values of the autocorrelation that are
not reliably estimated, can be counterproductive. The order M of
the autocorrelation to be considered (or, equivalently, the order
M of the underlying autoregressive process) is the only tuning
parameter of MESA and a careful balance between these two
necessities must be made when applying the algorithm.

The remainder of this section is devoted to an extensive study
on how to make such choice. In Section 3.1, we are going to
deﬁne three diﬀerent loss functions to measure how well the al-
gorithm is able to reproduce a known PSD. The basic idea is
to validate, as the autoregressive order considered increases, the
performance of the algorithm results by measuring the loss func-
tion and pick, among the orders the one that yields better results.
Second, the performance of diﬀerent loss functions will be as-
sessed by performing the spectral analysis on time series with an
analytical Gaussian PSD, sec. 3.2. In a last subsection 3.3, the
same analysis is performed on synthetic data generated from the
analytical spectrum released by the LIGO-Virgo collaboration.

3.1. Choice of the autoregressive order

Guided from numerical experiments, an indication on the upper
bound to the autoregressive order Mmax is (Berryman 1978):
Mmax = 2N/ ln (2N) ,
where N is the number of observed points in the time-series.
However, this is just a plausible upper limit on the order of the
AR process m and the optimal algorithm could employ fewer
points. We then need a more sophisticated method for comput-
ing the right value for m. Various loss functions to assess the al-
gorithm performance have been proposed in literature. We sum-
marise them below:

(18)

– Final prediction Error The ﬁrst criterion is due to Akaike
(1998). It was proposed that m should be chosen as the length
that minimizes the error when the ﬁlter is used as a predictor,
the ﬁnal prediction error (FPE):
FPE(m) = E (cid:104)(cid:16)
with ˆxt = (cid:80)M
mizing the quantity:

i=1 ai xt−i. Minimizing FPE is equivalent to mini-

(xt − ˆxt)2(cid:17)(cid:105)

(19)

LFPE(m) = Pm

N + m + 1
N − m − 1

(20)

with Pm being the estimated noise variance at order m, see
Eq. (A.9). In the N → ∞ limit, remembering mmax ∼
2N/ log(2N), Akaike’s loss function is equivalent to the min-
imization of the variance Pm of the white noise of the under-
lying AR(p) model.

– Criterion Autoregressive Transfer function (CAT) This
second loss function has been proposed by Parzen and stud-
ied in detail by Bhansali (1986). It is based on the assumption
that the observed process is an inﬁnite order autoregressive
process

xt =

∞(cid:88)

i=1

ai xt−i + νt

(21)

and tries to select the order m as the best ﬁnite-order ap-
proximation for the observed process. Being N the number
of samples it has the property that m → ∞ as N → ∞.
Since any real-valued stochastic process can be written as a
ARMA(p,q) process (Wold theorem) i.e. an AR(∞) process,
this is a physically signiﬁcant property. The loss function has
the functional form:

LCAT(m) = 1
N

m(cid:88)

k=1

N − k
NPk

−

N − m
NPm

,

(22)

and the so-chosen order is found to be asymptotically unbi-
ased for PN.

– Optimum Bayes Decision rule The last criterion we will
consider is the Optimum Bayes Decision rule (OBD)(Rao
et al. 1982). Let x(t) be the observed time series for the
process that has to be described as an AR(m) process, with
m to be determined. The OBD is obtained choosing be-
tween M diﬀerent hypothesis Hi, i = 1, . . . , M maximiz-
ing their posterior distribution P(Hi|x(t)). Each hypothesis
is uniquely determined from both the length and the val-
ues of the ﬁlter under study. Choosing a Likelihood of the
form Eq. (17) and uniform priors for both the coeﬃcients
and the hypotheses, Rao has shown that choosing the min-
imum for −log(P(Hi|x(t)), i.e. maximizing the posterior for
Hi, is asymptotically equivalent to the minimization of:

LOBD(m) = (N − m − 2) log(Pm)

+ m log(N) +

m−1(cid:88)

k=0

log(Pk) +

m(cid:88)

k=1

a2
k .

(23)

Once a loss function is selected, the choice of the best re-
cursion order is straightforward: we solve the Levinson recur-
sion (Levinson 1946) until Mmax, as given in Eq. (18), iterations
are reached. Then, the order m is selected to be the one that min-
imizes the speciﬁed loss function.

In a real implementation of the algorithm, computing all the
recursion up to Mmax can result in a signiﬁcant waste of compu-
tational power: the optimal value is often mopt << Mmax and, in
such cases, computing all the values of m until Mmax is not use-
ful. In practice, we can apply an early stop procedure: every few
iterations we look for the best order of mopt; if this value does
not change for a while, we assume that a good (local) minimum
of the loss function is found and the computation is stopped.

The following sections will be devoted to the study of the
statistical properties of the loss functions introduced above: we
need to understand which choice provides the best quality in the
reproduction of some known power spectral densities.

3.2. Choice of the loss function: Gaussian PSD

We test the performance of the three loss functions on a random
time series generated with a known power spectral density. In

Article number, page 5 of 16

A&A proofs: manuscript no. mesa

this ﬁrst experiment, we take the PSD to be a Gaussian distribu-
tion with mean µ = 2.5 and standard deviation σ = 0.5, where
the units are arbitrary. The time series are generated by sam-
pling a frequency vector from p(˜n( fi)) ∝ exp
and
performing an inverse Fast Fourier Transform on the frequency
series. For this investigation, we generate a dataset of N = 1000
time series of 3000 points each.

n2
i
S ( fi)

− 1
2

(cid:80)

(cid:26)

(cid:27)

i

We then apply Burg’s method choosing in turn each of the
loss functions on the ensemble of simulated time series, thus ob-
taining an ensemble of PSD estimates. Through these ensembles,
we characterize statistically the performance of each loss func-
tion. The disagreement between the PSD S i( f ) estimated from
the i-th simulated time series and the target PSD S ( f ) is mea-
sured via the frequency-averaged relative error ri:

ri = 1
N f

(cid:88)

f j={0,...,Ny}

|S i( f j) − S ( f j)|
S ( f j)

(24)

where Ny is the Nyquist frequency of the time series and N f is
the number of the discrete frequencies the PSD is evaluated at.

For each loss function, we compute the ensemble-averaged
PSD (as well as the 90% conﬁdence level) and the ensemble-
averaged relative error:

r( f ) = 1
N

(cid:88)

i

|S i( f ) − S ( f )|
S ( f )

.

(25)

For each loss function, Figs. 1 2 and 3 show the averaged PSD,
as well as the ensemble-averaged relative error from Eq. (25).
Furthermore, Fig. 4 displays the relation between ri and the au-
toregressive order mi chosen for each independently drawn time
series.

Final Prediction Error (FPE) The results of our investigation on
the performance of FPE are shown in Fig. 1. Qualitatively, there
is a good agreement between the reconstructed spectrum and the
true spectrum; however, we note that the reconstruction is not
very accurate in the low frequency region. Furthermore, the 90%
credible region is very small: this means that if we randomly take
two of the reconstructed spectra, we expect their diﬀerences to
be statistically small. These facts are evidence for FPE to be a
reliable loss function.

By looking at Fig. 4 (red series), we note that the AR orders
obtained with FPE are clustered in a very small region. This is
also a desirable property: FPE, in fact, provides a stable estima-
tion of the AR order, which does not aﬀect much the reconstruc-
tion error. We conclude that the FPE shows good quality recon-
struction for the spectrum and very desirable stability properties,
its estimate for ﬁlter’s length m is clustered in a region where
there is no dependence of error on m.

Optimum Bayes Decision Rule (OBD) The second loss func-
tion we consider is OBD and the results are summarized in
Fig. 2. As for the FPE case, they show a good agreement between
the average over the reconstructions and the true spectrum. Qual-
itatively the same behaviour of FPE is observed: a good quality
reconstruction at the intermediate and high frequencies with a
narrow 90% conﬁdence level as well as a degrading performance
at low frequencies. However, when looking at the error, the dis-
agreement of this method is found to be larger compared to FPE:
in the worst case, the error can be as large as a factor of 2 com-
pared to FPE.

Article number, page 6 of 16

Fig. 1. In the top panel, we show the average spectrum for LFPE(m)
with 90% conﬁdence regions (purple shaded region), together with the
median estimation (red line) and the target PSD (black dashed line).
In the bottom panel, we display the ensemble-averaged relative error
eq. (25). The average is computed with 1000 realization of a 3000 points
long time series.

The error as a function of the AR length (green series in
Fig. 4) clusters in a small region, indicating the stability of the
reconstructed process order m. We note that on average, OBD
tends to choose smaller values of m with respect to FPE.

Criterion Autoregressive Transfer function (CAT) The perfor-
mance of CAT is shown in Fig. 3. At a ﬁrst glance, CAT dif-
fers substantially from the other two loss functions considered.
The ensemble-average PSD matches very well the underlying
“true" PSD. This is also true even in the low frequency region,
where both OBD and FPE showed poor performance. However,
the variance of the reconstructed spectrum is quite large (much
larger than for FPE and OBD), and the relative error is quite high,
∼ 10% and it is approximately constant over all the frequencies.
The reason for this behaviour becomes clear from Fig. 4
(black series): CAT does not converge to any speciﬁc value for
the order of the AR process. The estimated m spans a large range
of values, hence the large variance observed. Fig. 4 also shows
a strong dependence of the error on the estimated length of the
ﬁlter. The good quality of the reconstruction from the average
spectrum can be explained as follows: long ﬁlters are able to
capture features that short ﬁlters cannot see, like outliers in dif-
ferent realizations of the time-series, but this is also responsible
for an increased variance in the estimate, by introducing spurious
peaks in the reproduction.

When averaging the diﬀerent PSD estimates, the noise in
each spectrum cancels, as expected from the Gaussian nature of
the AR process. This implies, in a sense, that each estimate of the
spectrum is independent of any other, as suggested by the huge
variance in the residuals. This lack of stability is not a good prop-
erty for the estimation of the PSD from a single realization of the

102101100105104103102101100PSD(a.u.)102101100f(Hz)104103102101100Percentage ErrorMartini, Schmidt, Del Pozzo: MESA: a case study

Fig. 2. In the top panel, we show the average spectrum for LOBD(m)
with 90% conﬁdence regions (purple shaded region), together with the
median estimation (red line) and the target PSD (black dashed line).
In the bottom panel, we display the ensemble-averaged relative error
eq. (25). The average is computed with 1000 realization of a 3000 points
long time series.

Fig. 3. In the top panel, we show the average spectrum for LCAT(m)
with 90% conﬁdence regions (purple shaded region), together with the
median estimation (red line) and the target PSD (black dashed line).
In the bottom panel, we display the ensemble-averaged relative error
eq. (25). The average is computed with 1000 realization of a 3000 points
long time series.

time-series, however, thanks to the averaging out of errors, this
estimator seems optimal in the case of repeatable experiments
and ensemble-averages.

Final remarks on the choice of the loss function In our anal-
ysis, the FPE and OBD loss functions are found to behave simi-
larly while CAT shows fairly diﬀerent properties. CAT provides
an accurate average spectrum over all the frequencies at the price
of a large variance; in turn OBD and FPE provide a poorer av-
erage in the low frequency tails, however they also display a
smaller variance, with FPE showing the lowest.

The poor low-frequency reconstruction from OBD and FPE
might be due to the fact that the ﬁrst tends to select shorter ﬁlters,
whereas long ﬁlters are required to model low frequency corre-
lation. This seems to be conﬁrmed by looking at ﬁg. 4. OBD,
which select the shortest ﬁlters, can provide an error as large as
300% at the extrema, as compared with 85% error of FPE. In
turn, CAT is 5% accurate in these regions.

However, we also note that the when inferring PSDs from a
single time-series realization, FPE provide the lowest averaged
error over all frequencies, while CAT can reach errors 5 times
larger (see again Fig. 4). Hence, while CAT is the loss function
that minimizes errors in the low-frequency end of the spectrum,
FPE obtains the best overall accuracy.

The conclusion is clear: even if in some cases CAT is more
accurate when taking the average over several realizations of the
underlying process, FPE guarantees that the single estimation is
more faithful. As in any common situation we cannot perform
such averaging over diﬀerent realizations of the same time se-
ries, we must prefer FPE over CAT (let alone OBD, which even
though qualitatively similar to FPE has worse performance).

However, if we indeed can measure the PSD by averaging over
diﬀerent time series, using CAT as a loss function is the best
choice. In this sense we retain CAT to provide the best, and most
similar in spirit, alternative to the commonly employed Welch
estimation method whenever ensemble-averages are needed and
justiﬁed.

3.3. Choice of the loss function: LIGO Spectrum

We continue our characterization of the various loss functions
considered in this work, by investigating the reconstruction of
a speciﬁc, known, power spectrum: that is the Advanced LIGO
(Aasi et al. 2015; Acernese et al. 2014; Somiya 2012; Aso et al.
2013) design sensitivity theoretical spectral curve (Abbott et al.
2020; LIGO/Virgo Collaboration 2020). For this analysis, we
generate N = 500 time series of 40960 points each, sampled
with a sampling rate of 2048 Hz, hence we ﬁx the duration of the
time-series to 20 s. The chosen length is convenient to capture
fairly accurately the low-frequency features of the LIGO PSD.
We report our ﬁndings in Figs. 5 and 6.

Fig. 5 shows the simulated spectrum (dashed line) and the
ensemble-averaged reconstructed PSD adopting the FPE (green
line), OBD (red line) and CAT (black line). In all cases, the spec-
trum is well reconstructed, but with a fairly distinct behaviour at
low frequency, where CAT – as in the Gaussian case – better
captures and resolves the distinct spectral feature at ∼ 17 Hz. In
Fig. 7, we report the reconstructed spectra around the two peaks
at ∼ 17 Hz and ∼ 438 Hz (left and right panel respectively).

Fig. 6 shows the distribution of recovered AR orders m
against the relative frequency-averaged error. The behaviour of
the three loss functions is very similar to what found in the Gaus-
sian PSD case (compare it with ﬁg. 4): ODB infers the smallest

Article number, page 7 of 16

102101100105104103102101100PSD(a.u.)102101100f(Hz)104103102101100Percentage Error102101100105103101PSD(a.u.)102101100f(Hz)1014×1026×102Percentage ErrorA&A proofs: manuscript no. mesa

Fig. 4. Frequency-averaged relative error r eq. (24) vs the length m of
autoregressive process, for each of the 1000 independent realizations of
the synthetic time series with a gaussian PSD. Diﬀerent colours refers to
diﬀerent choices for the loss function: in red LFPE(m), in green LOBD(m)
and in black LCAT(m). The top and right histograms show the marginal
distributions.

orders and gives average errors around 20%, FPE consistently
estimates orders of a few hundreds and shows the smallest errors
∼ 15 % while CAT does not show any preference towards any
AR order and displays wildly varying errors. Yet again, when
the PSD is averaged over multiple realization of the time, CAT is
able to capture the spectrum very precisely. In fact, even in pres-
ence of very sharp spectral features, CAT reconstruction seems
to be almost perfectly coincident with each of them. Hence, also
the study of simulated LIGO data seems to indicate that when-
ever and wherever ensemble-averaged PSDs are necessary, CAT
is the optimal choice of loss function. However, on a single time-
series realization, FPE is the more robust choice.

Let us summarize some key general conclusions:

– there are no reasons to prefer OBD over CAT or FPE;
– if we have one single realization for the process, we recom-
mend the use of FPE, that would get the best resolution pos-
sible. In this situation, CAT would provide spurious and un-
reliable results, with large error;

– in the case of several realizations of the same process, CAT
ensemble-average properties provide very a precise spectral
estimation.

Therefore, the choice of loss function, at least in between
CAT and FPE, depends on the problem one is attempting to
solve.

3.4. How well is the AR order recovered?

We now address the issue of how well the AR order (i.e. the
number of ak coeﬃcients employed) is estimated by each loss
function. This is useful to further characterize the properties of
the diﬀerent loss functions. Furthermore, as MESA is supposed

Article number, page 8 of 16

Fig. 5. Average spectrum for the three diﬀerent loss functions as com-
pared with the “true" PSD. The average is computed over 500 realiza-
tion of a 40960 points long time series.

to model data as an AR(p) process, it is interesting to quantify
its precision on some true autoregressive (and stationary) time
series.

We generate 100 autoregressive processes AR(p) with a ran-
dom value of p, drawn such that log(p) ∼ U[log(2),log(5000)]. Each
coeﬃcient ak is assigned according to a Dirichlet distribution
Dir([1, . . . , 1]). The sign of ak is assigned randomly according to
a binomial distribution. We report the result of this investigation
in Fig. 8.

It can be seen that OBD and FPE loss functions show very
similar behaviour. They are very reliable in capturing the cor-
rect AR order up to a certain threshold (p ∼ 100 for OBD and
p ∼ 200 for FPE). For any AR process of order higher than the
threshold, the optimization underestimates badly the actual value
of p. For this reason, FPE and OBD seem reliable only for rel-
atively simple AR processes. For more complicated processes,
they seem to “underﬁt” the problem (i.e. they output a model
simpler that those required).

On the other hand, CAT shows a diﬀerent behaviour. The
selected AR order p is always close to the maximum possible
value, regardless the actual value ptrue of the underlying process.
This produces always a model that is more complex than those
obtained with FPE and OBD and this can explain the origin of
the high variance in the estimation observed in the experiments
described in the previous sections. Of course, CAT does not per-
form a good job in reconstructing the AR process. However, for
high order AR processes, the error introduced by CAT is more
tolerable than the error introduced by FPE and OBD. This is a
good reason to prefer CAT in these situations (see also Sec. 5.1
for another example of this eﬀect).

102Filter Length estimate0.10.20.30.40.50.6Frequency averaged errorFPECATOBD02004000200101102103f(Hz)1047104610451044104310421041PSD(1Hz)FPECATOBDTrueMartini, Schmidt, Del Pozzo: MESA: a case study

Fig. 6. For each of the 500 independent realization of the time series,
we plot the relative error r (as in eq. (24)) against the length m of au-
toregressive process. The time series are randomly drawn with a the an-
alytical LIGO PSD in ﬁg.‘5. Diﬀerent colors refers to diﬀerent choices
for the loss function. Histograms for the distribution od the individual
quantities are also represented.

Fig. 7. Details of peaks of the spectrum and their reconstruction with
every optimizer

4. Comparison with Welch method

We perform a qualitative comparison between the performance
of the MESA and of the standard Welch algorithm. In this, we
cannot avoid to be only qualitative. Indeed, as the results of the
comparison are problem dependent, it is very hard to quantify

Fig. 8. Reconstructed value for the autoregressive order plotted against
the true value of the autoregressive order. The reconstructed autoregres-
sive orders are computed from a time series randomly drawn with an
AR(p) model, with the three diﬀerent loss functions under investigation.

this in a single metric. Although similar studies can be drawn
from any other PSD, in this section we focus on a single PSD and
we try to generalize some observations that we make. We decide
to use the analytical PSD computed for LIGO Handford inter-
ferometer, released together with the GWCT-1 catalog (Abbott
et al. 2019a,b), and computed with BayesLine package (Cornish
& Littenberg 2015; Littenberg & Cornish 2015; Cornish et al.
2021; Chatziioannou et al. 2019).

We simulate data9 from the PSD used for the analysis of
the event GW150914 and we employ both Welch’s method and
MESA to estimate the spectrum. We vary the length of the data
used for the estimation: this is also useful to assess how the com-
putation depends on the data available. We set the total observa-
tion time T = 1, 5, 10, 100, 1000s For the MESA algorithm, we
choose the FPE loss function. For the Welch algorithm, we em-
ploy a Tukey window with a parameter α equal to 0.4, an over-
lap fraction of 1/2 for the segments and a length of segments
L = 512, 1024, 2048, 8192, 32768 points, depending on the ob-
servation time. In all cases, the sampling rate is set to 4096 Hz.
For the Welch algorithm, we use the standard implementation
provided by the python library scipy (Harris et al. 2020; Virta-
nen et al. 2020). The results from both methods are summarized
in Figs. 9 and 10 respectively.

First of all, we note that using a longer time series results
in a better estimation of the PSD, especially at low frequencies.
This is somehow obvious: longer data streams probe lower fre-
quencies thanks to Nyquist’s theorem as well as providing better
estimates for the FFT, in the Welch case, and the sample auto-
correlation, for MESA.

We also note that MESA converges to the underlying spec-
trum much faster than Welch’s method, providing a better esti-

9 This is to ensure that we have a baseline PSD to compare the data
with

Article number, page 9 of 16

102103Filter Length estimate0.100.150.200.250.300.350.400.450.50Frequency averaged errorFPECATOBD0501001500501.4×1011.8×1012.2×101f(Hz)104510441043104210411040PSD(1Hz)4.34×1024.38×1024.42×102f(Hz)10471046104510441043FPECATOBD101102103104pMESA100101102103ptrueFPECATOBDA&A proofs: manuscript no. mesa

mate even in the case of short time series. Although observed
at every frequency, this behaviour is more evident in the low
frequency region. An accurate proﬁle reconstruction can be ob-
tained with MESA using a 5 seconds-strain only, while Welch
method requires at least 10 seconds of data to obtain a compara-
ble proﬁle. Furthermore, MESA is able to model all the details
of the peak at around ∼ 40 Hz (even with T = 100 s), while the
Welch’s algorithm fails to do so even with an observation time
of T = 1000 s.

Another important element is the noise of the spectral esti-
mation: we ﬁnd that the PSD estimation provided by the Welch’s
method is more noisy (i.e. has a large number of spurious peaks)
compared to the PSD measured with MESA and FPE loss func-
tion. This is especially true at high frequencies and for long ob-
servation times T .

Finally, as already discussed Welch’s method is very depen-
dent on the choice of window function. A Tukey window with
aforementioned parameters is what we found to be the best com-
promise between noise and accuracy for the reconstruction, but
diﬀerent choices can be made, possibly providing more accurate
results than the ones reported here. However, we want to stress
that this fact does not invalidate our discussion but reinforces it:
one of the most appealing advantages of MESA is the minimal
amount of ﬁne tuning required.

5. Applications

5.1. Temperature Time Series

As a further example of the breadth of applicability of MESA,
we applied our implementation to atmospheric temperature time
series. The reason for this choice is twofold: i) atmospheric tem-
perature time series present a variety of overlapping periodic-
ities most of which are known; ii) it provides a stress test for
the time series forecast analysis. As dataset, we used the his-
torical reanalysis data from the “ERA5-Land hourly data from
1981 to present" dataset, downloaded from the Climate Data
Store (Muñoz Sabater 2019). The data consist in temperatures
taken at coordinates N 45°5(cid:48) E 9°1(cid:48), corresponding about to
the city of Milan, Italy. The temperatures are given on an hourly
cadence for almost 31 years from 31st December 1989 to 30th
November 2020.

Fig. 11, shows the MESA spectrum inferred from the data.
As a comparison, we adopt both the FPE and the CAT loss func-
tions. In agreement with what we found in previous sections,
the FPE PSD is more regular compared to the one from CAT.
Both spectra show a peak at the frequency fD = 1 day−1 corre-
sponding to one day, corresponding to the day-night cycle: this
is expected. Higher order harmonics (corresponding to integer
multiples of fD) are also visible up to the Nyquist frequency
( fD = 0.5 hour−1): they correspond to signals that preserve the 1
day period while, at the same time, capturing the complex vari-
ability of the daily temperature cycle throughout the year.

Looking at low frequencies, FPE does not capture the yearly
variability at fyr = 1 yr−1. On the other hand, the peak is captured
well by the CAT loss function. In analogy as what observed for
the peak at fD, in the CAT spectrum we observe a higher order
harmonic at a frequency 2 · fyr. As in the previous case, this is
required to better model the temperature variation in the year.

The failure of FPE in capturing the low frequency trend can
be understood by looking at Fig. 8. It is shown that CAT sys-
tematically select very long ﬁlters (i.e. large pCAT values for
the autoregressive orders), whereas FPE tends to select smaller
values for the order pFPE of the autoregressive process, of-

Article number, page 10 of 16

ten underestimating the actual value. In this example, we have
pCAT ∼ 36500 ∼ 4 yr, whereas pFPE ∼ 1300 ∼ 2 months. The
autoregressive process selected by FPE, hence cannot produce a
meaningful prediction on the timescale longer than a few weeks
and for this reason it is unable to capture the low frequency peak
at fyr. In other words, the model is too simple to model both
the behaviour at high and low frequencies (separated by approx-
imately 3 orders of magnitude). On the other hand, the model
chosen by CAT is much more complex and is able to model also
the high frequency behaviour, at the expense of making a more
noisy estimation. The “actual" length of the autoregressive pro-
cess should be closer to the choice of CAT.

We now assess the accuracy of the forecasting of new obser-
vations of the time series. Based on actual data, we try to predict
future values as described in sec. 2.4 (of course the MESA is
performed with CAT loss function). We produce N = 100 inde-
pendent predictions and we compute the median as well as the
90% conﬁdence interval. This is compared with the actual mea-
sured temperature values. The predictions span a two years range
of time. We report our results in Fig. 11.

We note the observed diﬀerence is always well included in
the 90% conﬁdence interval: the forecasting predictions seem re-
liable. On the other hand, the conﬁdence interval is pretty large
(almost as large as 15 K), thus making “easy" for the actual data
to ﬁt the predictions. Indeed, the prediction model, while suitable
for spectral estimation, is nothing more than a linear regression
(plus noise term). Such a simple model hardly catches the com-
plex trend in the variability of the temperature daily trend during
a year. For more precise predictions, probably one should con-
sider nonlinear regression model, tapping into the wealth of non
linear predictors oﬀered by the ﬁeld of Deep Learning.

5.2. Forecasting the LIGO strain

As a last example of an application of MESA, we return to the
study of the strain produced by the LIGO-Virgo interferometers
and we forecast the future observations. A seminal work on the
application of autoregressive models to Virgo data is presented in
Cuoco et al. (2001), where an AR(p) model is trained on the data
for the purpose of estimating the PSD and to create a whitener
ﬁlter.

We focus on the public data released by the LIGO/Virgo col-
laboration (Abbott et al. 2021). We reconstruct the PSD both as-
suming the CAT and FPE loss functions on 1000 s of data from
the Livingston observatory starting from GPS time 1164603392.
The data are sampled at 4096 Hz. We then forecast the follow-
ing 100 s of observations with the model optimized with CAT10.
The results are shown in ﬁg. 13. The prediction is always in the
90% conﬁdence interval. The conﬁdence interval, however, is
very broad and the prediction is not very accurate (being the
same order of magnitude of the strain). By looking at the stan-
dard deviation σ of the predictions (bottom panel in Fig. 13),
we note that it increases very quickly in the ﬁrst 0.5 s. The order
of the autoregressive process selected is pCAT = 57766 ∼ 14 s,
which is much larger than the region in which σ is small ∼ 0.2 s.
The implication is that the series is very diﬃcult to predict: the
knowledge of past observations is not very helpful to predict fu-
ture observations. Although FPE selects an autoregressive order

10 The reason for this choice is that CAT, despite showing higher vari-
ance and a worse PSD estimation accuracy, estimates sistematically
high autoregressive orders. We believe that this feature provides a more
reliable forecast, as more points are used for predictions. In practice, we
found that CAT and FPE behave very similarly.

Martini, Schmidt, Del Pozzo: MESA: a case study

Fig. 9. Comparison between analytic (dashed line) and estimated (red
line) spectrum. The estimation is performed with Maximum Entropy
method on synthetic data, with an increasing observation time T =
1, 5, 10, 100, 1000 s.

Fig. 10. Comparison between analytic (dashed line) and estimated
(green line) spectrum. The estimation is performed with Welch’s
method on synthetic data with an increasing observation time T =
1, 5, 10, 100, 1000 s.

pFPE = 29924 ∼ 7 s smaller than CAT, the forecasted time series
behaves very similarly.

Despite poor predictions on long timescales, the AR process
obtained with MESA still can be useful on short timescales. In-
deed, a precise prediction of the strain time series can be beneﬁ-
cial in the detection of anomalies in the data and, eventually, their
removal. Indeed loud anomalies in the data, called glitches, pose
a major challenge to the ability of detecting signals and inten-
sive work has been done to mitigate the disruption in sensitivity
they cause (Nuttall et al. 2015; Abbott et al. 2016; Zevin et al.
2017) and to develop eﬀective subraction techniques (Pankow
et al. 2018; Zackay et al. 2019). The predictions can form an ex-
pected baseline for the strain; any anomaly (i.e. a glitch or even

a transient of physical origin) can show up as a large departure
from expected trend. Moreover, if a glitch is detected, its shape
can be estimated (as well as the conﬁdence level) by subtracting
the expected signal with the actual signal. This can work: a typ-
ical glitch can last as long as 0.2 s, close to the time scale over
which the forecasting is reliable. Future works will explore this
exicting opportunity.

As noted above, for pushing further the prediction perfor-
mance from our simple linear predictor, more sophisticated fore-
casting methods are available in the Machine Learning literature
(see e.g. (Hochreiter & Schmidhuber 1997; van den Oord et al.
2016; Lea et al. 2016)) and they can be trained for precision fore-
casting. However, a Maximum-Entropy trained AR process can

Article number, page 11 of 16

102103f(Hz)T = 1 sT = 5 sT = 10 sT = 100 sT = 1000 sPSD(a.u.)MESA102103f(Hz)T = 1 sT = 5 sT = 10 sT = 100 sT = 1000 sPSD(a.u.)WelchA&A proofs: manuscript no. mesa

Fig. 11. Spectrum for the historical temperature time series. The two
lines refers to diﬀerent loss functions (CAT and FPE); the inset shows
the harmonics of the fundamental frequency of a day. Two vertical lines
are drawn in correspondence to the frequency fD of a day and fyr of a
year.

Fig. 13. In the top panel, we show the diﬀerence between the forecasted
strain values (CAT model) and the actual values observed at Livingston
interferometer (blue). The red shadow denotes the 90% conﬁdence in-
terval subtracted with the actual time series values. The conﬁdence in-
terval is computed on 100 independent random realization of the pre-
dictions. The bottom panel displays the standard deviation of the pre-
dictions with both CAT and FPE models. The inset shows a detail of the
ﬁrst 0.5 s of plot in the top panel.

Fig. 12. Diﬀerence between the historical temperature T and the fore-
cast temperature T f orecast for two years of data. The red shadow denotes
the 90% conﬁdence interval of the predictions. The model is trained
with CAT loss function. The discrepancy between predicted and actual
values is always everywhere included in the 90% conﬁdence interval.

be a simple baseline for comparing such more advanced meth-
ods and might suﬃce for many purposes. This opens a promising
path in GW data analysis and other ﬁelds of physics might also
take advantage from this.

Article number, page 12 of 16

6. Final remarks and future prospects

We presented a case study of the application of Maximum En-
tropy principle to the realm of spectral estimation. Albeit the
methodology hereby presented is grounded on solid theoretical
foundations and its merits are widely recognised, Maximum En-
tropy methods have yet to be adopted routinely in the study of
problems related to time series. The superior nature of maxi-
mum entropy methods, and in particular of Burg’s method, is
exempliﬁed by the closed form estimate of the power spectral
density and by the theoretical bridge between spectral analysis
and AR processes. Moreover, the method presents, in our view,
two main advantages when compared with more traditional ones;
ﬁrst there is no need to choose an arbitrary window function to
correct the data and, second it provides as straightforward way
to compute predictions given past observations. Accompanying
this work, we provide a publicly available Python implementa-
tion, called memspectrum, that we used to perform the numeri-
cal studies presented in this work..

Since the order of the AR process is not yet determined by
the theory, we opted for an in-depth investigation of several pro-
posals in the literature and found that diﬀerent loss functions
are required for diﬀerent situations, with the FPE loss function
being the most indicated to deal with gravitational wave data.
Along these lines, we directly compared the PSDs computed
with MESA with the canonical Welch’s algorithm. As outlined
in Sec. 4, MESA provides PSD estimates with smaller variance
and better accuracy than Welch algorithm. The use of MESA is
particularly useful for short time series samples, where Welch’s
method is outperformed in both precision and conﬁdence.

This observation suggests a promising avenue to pursue in
future developments of gravitational waves data analysis: for

104103102101100101frequency (1/day)103106109101210151018PSD (1/Hz)123456781012CATFPE0100200300400500600700Time (days)15105051015[TforecastT](K)3210123hforecasth1e180246810Time (s)024681e190.00.20.4101hforecasth1e18CATFPEMartini, Schmidt, Del Pozzo: MESA: a case study

short time series, comparable with the length of binary black
hole systems as observed by LIGO, Virgo and KAGRA, the com-
putational cost of MESA is moderate and the inferred PSD is
an accurate representation of the true underlying PSD. Hence
MESA could be employed for online PSD estimation during a
Bayesian parameter estimation exercise.

This is possible whenever the log-likelihood of the model for

a time series takes the form

log L(dt|θ) ∝ −

1
2

| ˜d( f ) − ˜x( f ; θ)|2
S ( f )

−

1
2

(cid:34)(cid:90)

(cid:35)
d f S ( f )

log

(26)

where ˜ denotes the Fourier transform and the signal model
x(t; θ), dependent on some parameter θ, is a prediction for a de-
terministic signal buried in the observed time series d(t). In GW
data analysis, a typical approach is to estimate the PSD oﬄine on
a large batch of data and oﬀ-source: this scheme assumes station-
ary data on a long timescale (longer than the data under study)
and it might not reﬂect the structure of the noise in the analyzed
time slice. Some alternatives exist for dropping this assumption
and modifying the likelihood accordingly (Röver et al. 2010;
Röver 2011; Edwards et al. 2020; Chatziioannou et al. 2021).
MESA can add to those an elegant way out: at each evaluation
of the likelihood, a new spectral analysis is performed by com-
puting the PSD on the residual dt − xt. In this way, the PSD would
depend also on θ and would eﬀectively model the residuals. For
this method, we would only need to assume the stationariety of
the residual on the batch to analyse11, making a lighter assump-
tion on the nature of the data. Furthermore, this method would
get, as a bonus, a posterior distribution for the PSD of the ana-
lyzed data. Preliminary studies suggest that this is possible (Mar-
tini 2020). Other studies have been done in this direction, mostly
using a parametric model for the PSD (Littenberg et al. 2013;
Edwards et al. 2015; Veitch et al. 2015).

Furthermore, MESA provides a simple, but robust and quite
accurate, albeit for short times, predictor for the time series. This
fact is remarkable and can be used in time series analysis for
several purposes. As discussed in Sec. 5.2, an anomaly detection
pipeline could be built using the forecasts of MESA: the pre-
dictions can form a baseline to compare the actual observations
with. Whenever the observed data are outside the expectations,
an anomaly detection can be claimed. Of course such predictions
can be done with a more accurate (perhaps nonlinear) model;
however MESA has the advantage of being simple and fast to
construct, while providing decent predictions. At the same time,
several instruments present gaps in their data stream, for instance
LISA is expected to show such gaps (e.g. Baghi et al. (2019)
and references therein), MESA forecasting capabilities could be
used to ﬁll those gaps with predicted data from past observations.
In conclusion, we reiterate that MESA is a theoretically sound,
computationally feasible and reliable way of studying the prop-
erties of stochastic processes and we hope that the investigations
presented in this work will further stimulate developments and
applications of this method.

Acknowledgments We are grateful to S. Biscoveanu, D. Laghi,
M. Maugeri, C. Rossi and S. Shore for useful comments and dis-
cussions.
This research has made use of data, software and/or web

11 Unlike several works in GW data analysis (Biscoveanu et al. 2020;
Talbot & Thrane 2020), this approach does not address the issue of in-
cluding the PSD uncertainties in the posterior. Instead, it focuses on
making minimal assumption about the nature of the data under study.

tools obtained from the Gravitational Wave Open Science
Center (https://www.gw-openscience.org/), a service of
LIGO Laboratory, the LIGO Scientiﬁc Collaboration and the
Virgo Collaboration. LIGO Laboratory and Advanced LIGO are
funded by the United States National Science Foundation (NSF)
as well as the Science and Technology Facilities Council (STFC)
of the United Kingdom, the Max-Planck-Society (MPS), and the
State of Niedersachsen/Germany for support of the construc-
tion of Advanced LIGO and construction and operation of the
GEO600 detector. Additional support for Advanced LIGO was
provided by the Australian Research Council. Virgo is funded,
through the European Gravitational Observatory (EGO), by the
French Centre National de Recherche Scientiﬁque (CNRS), the
Italian Istituto Nazionale di Fisica Nucleare (INFN) and the
Dutch Nikhef, with contributions by institutions from Belgium,
Germany, Greece, Hungary, Ireland, Japan, Monaco, Poland,
Portugal, Spain.

References

Aasi, J., Abbott, B. P., Abbott, R., et al. 2015, Classical and Quantum Gravity,

32, 074001

Abbott, B., Abbott, R., Abbott, T., et al. 2019a, Physical Review X, 9
Abbott, B., Abbott, R., Abbott, T., et al. 2019b, LIGO Document P1900011-
Power Spectral Densities (PSD) release for GWTC-1, LIGO Document Ser-
vice: https://dcc.ligo.org/LIGO-P1900011/public

Abbott, B. P., Abbott, R., Abbott, T. D., et al. 2016, Classical and Quantum

Gravity, 33, 134001

Abbott, B. P., Abbott, R., Abbott, T. D., et al. 2020, Living Reviews in Relativity,

23

Abbott, R., Abbott, T., Abraham, S., et al. 2021, SoftwareX, 13, 100658
Ables, J. G. 1974, Astronomy and Astrophysics Supplement, 15, 383
Acernese, F., Agathos, M., Agatsuma, K., et al. 2014, Classical and Quantum

Gravity, 32, 024001

Akaike, H. 1998, Annals of the Institute of Statistical Mathematics, 137
Allen, B., Anderson, W. G., Brady, P. R., Brown, D. A., & Creighton, J. D. E.

2012, Phys. Rev. D, 85, 122006

Aso, Y., Michimura, Y., Somiya, K., et al. 2013, Physical Review D, 88
Baghi, Q., Thorpe, J. I., Slutsky, J., et al. 2019, Phys. Rev. D, 100, 022003
Barnard, T. E. 1975, The maximum entropy spectrum and the Burg technique.
Technical report no. 1: Advanced signal processing, NASA STI/Recon Tech-
nical Report N

Bartlett, M. 1968, Louvain Economic Review, 34, 227–227
Berryman, J. G. 1978, GEOPHYSICS, 43, 1384
Bhansali, R. J. 1986, Annals of Statistics., 14, 315
Biscoveanu, S., Haster, C.-J., Vitale, S., & Davies, J. 2020, Physical Review D,

102

Burg, J. 1975, Maximum Entropy Spectral Analysis, Stanford Exploration

Project (Stanford University)

Chatziioannou, K., Cornish, N., Wijngaarden, M., & Littenberg, T. B. 2021,

Physical Review D, 103

Chatziioannou, K., Haster, C.-J., Littenberg, T. B., et al. 2019, Physical Review

D, 100

Cornish, N. J. & Littenberg, T. B. 2015, Classical and Quantum Gravity, 32,

135012

Cornish, N. J., Littenberg, T. B., Bécsy, B., et al. 2021, Phys. Rev. D, 103, 044006
Cuoco, E., Calamai, G., Fabbroni, L., et al. 2001, Classical and Quantum Gravity,

18, 1727–1751

Edwards, M. C., Maturana-Russel, P., Meyer, R., et al. 2020, Physical Review D,

102

Edwards, M. C., Meyer, R., & Christensen, N. 2015, Physical Review D, 92
Finn, L. S. 1992, Physical Review D, 46, 5236–5249
Gray, R. M. 2006, Toeplitz and Circulant Matrices: A Review (Now Foundations

and Trends)

Gregory, P. 2005, Multivariate Gaussian from maximum entropy (Cambridge

University Press), 450–454

Harris, C. R., Millman, K. J., van der Walt, S. J., et al. 2020, Nature, 585,

357–362

Hochreiter, S. & Schmidhuber, J. 1997, Neural computation, 9, 1735
Jaynes, E. & Bretthorst, G. 2003, Probability Theory: The Logic of Science

(Cambridge University Press:)

Jaynes, E. T. 1957, Physical Review, 106, 620
Jaynes, E. T. 1982, Proceedings of the IEEE, 70, 939

Article number, page 13 of 16

A&A proofs: manuscript no. mesa

Lea, C., Flynn, M. D., Vidal, R., Reiter, A., & Hager, G. D. 2016, Temporal

Convolutional Networks for Action Segmentation and Detection

Levinson, N. 1946, Journal of Mathematics and Physics, 25, 261
LIGO/Virgo Collaboration. 2020, Noise curves used for Simulations in
the update of the Observing Scenarios Paper, https://dcc.ligo.org/
LIGO-T2000012/public

Littenberg, T. B. & Cornish, N. J. 2015, Physical Review D, 91
Littenberg, T. B., Coughlin, M., Farr, B., & Farr, W. M. 2013, Physical Review

D, 88

Lomb, N. R. 1976, Astrophysics and Space Science, 39, 447
Martini, A. 2020, Maximum Entropy Spectral Analysis: characterization and ap-
plications to on-source parameter estimation of time series, https://etd.
adm.unipi.it/theses/available/etd-11162020-182406/

Muñoz Sabater, J. 2019, ERA5-Land hourly data from 1981 to present,
Copernicus Climate Change Service (C3S) Climate Data Store (CDS):
https://cds.climate.copernicus.eu/cdsapp#!/dataset/
reanalysis-era5-land

Nuttall, L. K., Massinger, T. J., Areeda, J., et al. 2015, Classical and Quantum

Gravity, 32, 245005

P. M. Woodward, D. W. F. & Higinbotham, W. 1964, Probability and information

theory, with applications to radar, 2nd edn. (Pergamon Press)

Pankow, C., Chatziioannou, K., Chase, E. A., et al. 2018, Physical Review D, 98
Rao, A. R., Kashyap, R. L., & Mao, L. 1982, Water Resources Research, 18,

1097

Röver, C. 2011, Physical Review D, 84
Röver, C., Meyer, R., & Christensen, N. 2010, Classical and Quantum Gravity,

28, 015010

Scargle, J. D. 1982, The Astrophysical Journal, 263, 835
Shannon, C. E. 1948, Bell System Technical Journal, 27, 379
Somiya, K. 2012, Classical and Quantum Gravity, 29, 124007
Talbot, C. & Thrane, E. 2020, Gravitational-wave astronomy with an uncertain

noise power spectral density

Ulrych, T. J. & Bishop, T. N. 1975, Reviews of Geophysics, 13, 183
van den Oord, A., Dieleman, S., Zen, H., et al. 2016, WaveNet: A Generative

Model for Raw Audio

Veitch, J., Raymond, V., Farr, B., et al. 2015, Phys. Rev. D, 91, 042003
Virtanen, P., Gommers, R., Oliphant, T. E., et al. 2020, Nature Methods, 17, 261
Implementation of Burg’s Algorithm, https://
Vos, K. 2013, A Fast

opus-codec.org/docs/vos_fastburg.pdf

Welch, P. 1967, IEEE Transactions on audio and electroacoustics, 15, 70
Wold, H. 1939, Journal of the Institute of Actuaries, 70, 113–115
Zackay, B., Venumadhav, T., Roulet, J., Dai, L., & Zaldarriaga, M. 2019, Detect-

ing Gravitational Waves in Data with Non-Gaussian Noise

Zevin, M., Coughlin, S., Bahaadini, S., et al. 2017, Classical and Quantum Grav-

ity, 34, 064003

Article number, page 14 of 16

Martini, Schmidt, Del Pozzo: MESA: a case study

Appendix A: Details of PSD computation

Appendix A.1: MESA solution

We derive the expression for the MAXENT spectral estimator
following the approach proposed by Burg (1975). Unlike the
standard approach, we do not enforce the constraints in Eq. (11)
with the standard Lagrange Multipliers approach. We write in-
stead the PSD S ( f ) as the Fourier Transform of the sample au-
tocorrelation function:

S ( f ) = 1
2Ny

∞(cid:88)

n=−∞

¯rne−ı2πn∆t,

(A.1)

and, plugging it in the entropy gain expression eq. (10), we ob-
tain:

∆H =

(cid:90) Ny

log

−Ny





1
2Ny

∞(cid:88)

n=−∞

¯rne−ı2π f n∆t



 d f.

(A.2)

Note that this expression already takes into account the con-
straints in eq. (11).

We now introduce a set of coeﬃcients λs, deﬁned as the
derivative of ∆H with respect to the autocorrelation function rs.
Explicitly they are:

be another root inside of it and vice-versa. These properties allow
us to rewrite the Fourier expansion (A.5) as (Barnard 1975):

S ( f ) =

PN∆t
s=0 aszz(cid:17) (cid:16)(cid:80)N

(cid:16)(cid:80)N

sz−s(cid:17)

s=0 a∗

(A.6)

with a0 = 1 and ∆t the uniform sampling interval for the time se-
ries. The vector obtained as (1, a1, . . . , aN) is the prediction error
ﬁlter. The power spectral density S ( f ) is uniquely determined if
both the prediction error ﬁlter and PN coeﬃcients are computed.
To compute the as is convenient to plug into Eq. (11) the Lau-
rent Polynomial exansion for S ( f ) eq. (A.6) and then integrating
over z (taking values on S1). In this way the equation becomes:

PN
2πı

(cid:73)

S1

z−s−1
n=0 anzn (cid:80)N
(cid:80)N

n=0 a∗

nz−n

dz = ¯rs.

(A.7)

Substituting s → s − r, multiplying by a∗
the previous equation becomes

s and summing over s,

(cid:73)

N(cid:88)

s=0

as ¯rs−r = PN
2πı

zr−1
s=0 aszs

(cid:80)N

dz

(A.8)

λs (cid:66) δH
δ¯rs

= 1
2Ny

(cid:90) Ny

−Ny

S ( f )−1e−ı2π f s∆td f

(A.3)

For a wide-sense stationary processes, all the poles lay outside
the unit circle so that the previous integral can be easily com-
puted obtaining the following, well known, equations:

and we will show that S ( f )−1 can be written as a Fourier Expan-
sion in terms of such coeﬃcients. Then, the determination of the
values for the λs uniquely solves the problem of power-spectral
density estimation.

Some properties for the coeﬃcients can be worked out easily.

First, since S ( f ) is real, the λs show the property
λs = λ∗

−s.

The second property is obtained considering that the autocorre-
lation function rn can only be computed for a ﬁnite time interval
n ∈ [−N, N] and that the PSD estimation must not depend on
the unavailable values rn: this is part of the constraint in eq. (11)
This requirement can be implemented as:

N(cid:88)

s=0
N(cid:88)

s=0

as ¯rr−s = PN

if r = 0

as ¯rr−s = 0

if r (cid:44) 0.

(A.9)

(A.10)

Appendix A.2: Levinson recursion

The solution of the Eqs. (A.9-A.10) fully determines the func-
tional form of the power spectral density estimator (A.6). The
method for solving the equations is called the Levinson-Durbin
recursion (Levinson 1946) and it is described in the following.
For each order N of the iteration we deﬁne the quantities:

δH
δ¯rs

= 0 for |s| > N,

that means

λs = 0 for |s| > N.

From Eq. (A.3) and from the properties above, is easily seen
from the properties of the Fourier transform that S ( f ) can be
expressed via a Fourier Series

S ( f )−1 =

N(cid:88)

s=−N

λse−ı2π f s∆t.

∆N =

N(cid:88)

n=0

an ¯rN−n+1

cN = −

∆N
PN

,

(A.11)

(A.12)

The Levinson recursion computes the Nth order quantities

given the N − 1th order quantities:
1 − |cN−1|2(cid:17)

PN = PN−1

(cid:16)

(A.4)

and

Deﬁning z = e−ı2π f ∆t the previous Fourier expansion becomes a
Laurent Polynomial in z:

S ( f )−1 = λ0 +

N(cid:88)

s=1

λszs +

N(cid:88)

s=1

sz−s.
λ∗

(A.5)





1
a1
...
aN−1
aN





=





1
b1
...
bN−1
0





+ cN−1


0
b∗
N−1
...
b∗
1
1







.

(A.13)

(A.14)

0)−1 is
It is easy to show that if z0 is a root for the polynomial (z∗
also a root: for every root laying outside the unit circle there will

where b holds the value of the as coeﬃcients at order N − 1.
The 0-th order element can be easily initialized reminding that

Article number, page 15 of 16

A&A proofs: manuscript no. mesa

a0 = 1 (always) and that P0 can be determined from (A.9). Its
values turns out to be:

P0 = R(0),

(A.15)

∆0 and c0 are uniquely determined from their deﬁnitions and they
are:

∆0 = R(1);

c0 = −

R(1)
R(0)

.

(A.16)

These expressions allow us to compute a and PN to any or-
der by simply iterating (A.13) and (A.14). Substituting them in
equation (A.6) the problem of the estimation for the power spec-
tral density via maximum entropy principle is solved. Burg’s
method for spectral analysis is solved via Levinson is imple-
mented in the released memspectrum package. Another faster
recursion method is available in Vos (2013) and it is also avail-
able in memspectrum.

Article number, page 16 of 16

