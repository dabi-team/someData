JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

1

Quasi-Periodic Parallel WaveGAN: A
Non-autoregressive Raw Waveform Generative
Model with Pitch-dependent Dilated Convolution
Neural Network

Yi-Chiao Wu, Tomoki Hayashi, Takuma Okamoto, Member, IEEE, Hisashi Kawai, Member, IEEE,
and Tomoki Toda, Member, IEEE

1
2
0
2

b
e
F
9
1

]
S
A
.
s
s
e
e
[

3
v
5
5
9
2
1
.
7
0
0
2
:
v
i
X
r
a

Abstract—In this paper, we propose a quasi-periodic parallel
WaveGAN (QPPWG) waveform generative model, which applies
a quasi-periodic (QP) structure to a parallel WaveGAN (PWG)
model using pitch-dependent dilated convolution networks (PD-
CNNs). PWG is a small-footprint GAN-based raw waveform
generative model, whose generation time is much faster than
real time because of its compact model and non-autoregressive
(non-AR) and non-causal mechanisms. Although PWG achieves
high-ﬁdelity speech generation, the generic and simple network
architecture lacks pitch controllability for an unseen auxiliary
fundamental frequency (F0) feature such as a scaled F0. To
improve the pitch controllability and speech modeling capability,
we apply a QP structure with PDCNNs to PWG, which introduces
pitch information to the network by dynamically changing the
network architecture corresponding to the auxiliary F0 feature.
Both objective and subjective experimental results show that QP-
PWG outperforms PWG when the auxiliary F0 feature is scaled.
Moreover, analyses of the intermediate outputs of QPPWG also
show better tractability and interpretability of QPPWG, which
respectively models spectral and excitation-like signals using the
cascaded ﬁxed and adaptive blocks of the QP structure.

Index Terms—Neural vocoder, parallel WaveGAN, quasi-

periodic WaveNet, pitch-dependent dilated convolution

I. INTRODUCTION

S PEECH generation is a technique to generate speciﬁc

speech according to given inputs such as texts (text-to-
speech, TTS), the speech of a source speaker (speaker voice
conversion, VC), and noisy speech (speech enhancement,
SE). The core of speech generation is the controllability of
speech components, and the fundamental technique is called
a vocoder [1]–[3]. A vocoder encodes speech into acoustic

Manuscript received xxx xx, 2020; revised xxx xx, 2020. This work
was supported in part by the Japan Science and Technology Agency (JST),
Precursory Research for Embryonic Science and Technology (PRESTO) under
Grant JPMJPR1657, in part by the JST, CREST under Grant JPMJCR19A3,
and in part by the Japan Society for the Promotion of Science (JSPS) Grants-
in-Aid for Scientiﬁc Research (KAKENHI) under Grant 17H06101. The initial
investigation in this study was performed while Y.-C. Wu was interning at
NICT.

Y.-C. Wu is with Graduate School of Informatics, Nagoya University, Aichi,

Japan (e-mail: yichiao.wu@g.sp.m.is.nagoya-u.ac.jp).

T. Hayashi

is with Graduate School of Information Science, Nagoya
University, Aichi, Japan (e-mail: hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp).
T. Okamoto and H. Kawai are with National Institute of Information
and Communications Technology, kyoto, Japan (e-mail: okamoto@nict.go.jp,
hisashi.kawai@nict.go.jp).

T. Toda is with Information Technology Center, Nagoya University, Aichi,

Japan (e-mail: tomoki@icts.nagoya-u.ac.jp).

representations such as spectral and prosodic features and then
decodes speciﬁc speech on the basis of the manipulated acous-
tic features. Conventional vocoders such as STRAIGHT [4]
and WORLD [5] are based on a source-ﬁlter model [6], which
models speech with vocal fold movements (excitation) and
vocal tract resonances (spectral envelope). However, many
oversimpliﬁed designs such as a ﬁxed length of the analysis
window, a time-invariant linear ﬁlter, and a stationary Gaussian
process are imposed on the conventional vocoders. The losses
of phase information and temporal details caused by these
ad hoc designs result in speech quality degradation.

To tackle these problems, many neural network (NN)-based
speech generation models [7]–[34] have been proposed. In
contrast to the conventional source-ﬁlter-based vocoders, most
of these models directly model the relationships among speech
waveform samples. Speciﬁcally, autoregressive (AR) models
such as WaveNet (WN) [7] and SampleRNN [8] achieve
high-ﬁdelity speech generation by modeling the probability
distribution of each speech sample with the given auxiliary
features and previous samples. Taking conventional-vocoder-
extracted acoustic features as the auxiliary features for NN-
based speech generation models [35]–[39], which replace
the synthesizer of the conventional vocoders, also achieved
early success. However, the AR mechanism and huge network
architectures of WN and SampleRNN result in very slow
generations, making these models impractical for realistic
scenarios. To tackle these problems, many compact AR models
with speciﬁc knowledge [9]–[11] and non-AR models such
as ﬂow-based [12]–[16] and generative adversarial network
(GAN)-based [17]–[29] models have been proposed.

Although these NN-based models achieve high-ﬁdelity
speech generation without many ad hoc designs, the data-
driven nature, the generic network architecture, and the lack of
prior acoustic knowledge of these models make most of them
lose acoustic controllability and robustness to unseen auxiliary
features [40]–[44]. For instance, without explicitly modeling
the excitation signals as conventional source-ﬁlter models, it
is difﬁcult for WN to generate speech with accurate pitches
outside the fundamental frequency (F0) range of training data
when conditioned on the scaled F0 feature [33], [34]. However,
using carefully designed mixed periodic and aperiodic inputs
and source-ﬁlter-like architectures, the authors of [30]–[32]
proposed different NN-based models attaining pitch control-

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

2

Fig. 1. Comparison of waveform generative architectures.

lability. In our previous works [33], [34], we also proposed
a quasi-periodic WN (QPNet), which has a conventional-
vocoding-like framework while using a uniﬁed network with-
out the requirement of speciﬁc mixed inputs. QPNet advances
the dilated convolution neural networks (DCNNs) [45] of
WN with a pitch-dependent mechanism to improve the pitch
controllability of WN by dynamically changing the network
architecture according to the auxiliary F0 feature.

Although QPNet markedly improves the pitch accuracy of
the generated speech, the AR mechanism and the huge network
requirement of WN result in slow generations. To address this
problem, we extend the AR PDCNN of QPNet to a non-
AR PDCNN and apply the quasi-periodic (QP) structure to
parallel WaveGAN (PWG) [25], which is a compact non-
AR model with a WN-like network architecture consisting
of stacked DCNN layers. The proposed QPPWG speech
generation model [46] attains pitch controllability using a
simple pitch-dependent architecture without the requirement
of speciﬁc mixed periodic and aperiodic inputs as in [30]–[32].
Although QPPWG greatly improved the pitch controllability
of PWG, the behind mechanisms of the QP structure in the
non-AR model, the characteristic of each component, and the
effective architectures are remained to be explored.

Therefore, in this paper, we conduct more evaluations with
several hyperparameter settings and network architectures to
comprehensively explore the efﬁciency of model structures and
the internal behaviors and mechanisms of QPPWG. Speciﬁ-
cally, model details such as the order of the cascaded structure,
the numbers of dilation cycles and residual blocks, and the
balanced ratio of adaptive and ﬁxed modules are investigated.
Both objective and subjective evaluations are conducted, and
the experimental results show the effectiveness of the proposed
QP structure for PWG. Furthermore, we also investigate a new
parallel QP structure and show the reason why the stacked QP
structure is selected for QPPWG. In addition, comprehensive
analyses of intermediate outputs of QPPWG are presented to
make us know more about the internal behaviors of the QP
network. The discussions of the QP structure understanding
show the tractability and interpretability of QPPWG. The
analyses conﬁrm our assumption that QPPWG respectively
models harmonic components with long-term correlations and

non-harmonic components with short-term correlations using
the adaptive module with pitch-dependent DCNNs (PDCNNs)
and the ﬁxed module with DCNNs of QPPWG.

This paper is organized as follows. In Section II, we review
the recent GAN-based neural vocoders. In Section III, a
brief introduction to PWG is presented. In Section IV, we
describe the concepts and details of the proposed QPPWG. In
Section V, objective and subjective tests are presented to show
the effectiveness of QPPWG for generating speech with scaled
F0. Further discussion of QPPWG is presented in Section VI.
Finally, the conclusion is given in Section VII.

II. RELATED WORK

A. Source-ﬁlter and Data-driven Vocoders

Because of the high temporal resolution of speech signals,
directly modeling raw speech waveforms is challenging. One
of the standard speech modeling methods is source-ﬁlter
modeling [6]. Speciﬁcally, the speech generative process is
formulated as a convolution of an excitation (voice source)
signal and a spectral ﬁlter. The excitation signal models the
glottal waveform generated by vocal fold movements, and
the spectral ﬁlter models vocal tract resonances. As shown
the conventional parametric vocoders generate
in Fig. 1,
speech samples in an AR manner such as LPC vocoders [47],
[48] and mel-generalized cepstrum (MGC) vocoders [49],
[50] or in a non-AR manner such as STRAIGHT [4] and
WORLD [5]. Motivated by the development of deep NNs, NN-
based excitation generation models with the AR mechanism
such as LPCNet [11] and the non-AR mechanism such as
GlotGAN [17], [18] and GELP [19] have been proposed to
improve the generated speech quality. Moreover, the authors
of [31] and [32] also proposed a neural source-ﬁlter (NSF)
network to model the source-ﬁlter generative framework with
an advanced neural ﬁlter.

In addition to the source-ﬁlter-based vocoders, many uniﬁed
NN-based waveform generative models have been proposed to
directly generate high-ﬁdelity speech waveforms from acoustic
features in a purely data-driven manner as shown in Fig. 1.
For example, the WN [7] and WaveRNN [10] models au-
toregressively generate speech samples conditioned on acous-
tic/linguistic features and the previous samples, and the non-

Excitation adaptiveUnified modelsSource-filter modelsResonance IIR filteringExcitation generationAcoustic featuresAR pathWaveform generationAcoustic/linguistic featuresWaveform generation w/ QP structureAcoustic/linguistic featuresParametric-basedWaveform generationAcoustic/linguistic featuresWaveform generation w/ QP structureAcoustic/linguistic featuresResonance FIR filteringExcitation generationAcoustic featuresNN-basedResonance filteringExcitation generationAcoustic featuresResonance filteringExcitation generationAcoustic featuresResonance filteringExcitation generationAcoustic featuresProposedAR pathNN-basedE.g., LPC vocoder,MGC vocoder etc.E.g., STRAIGHT,WORLD etc.E.g., LPCNet etc.E.g., GlotGAN, GELP etc.E.g., NSF etc.E.g., WaveNet, WaveRNN etc.E.g., Parallel WN, Clarinet etc.E.g., QPNetE.g., QPPWGJOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

3

AR Parallel WN [12] and Clarinet [13] models simultaneously
generate all speech samples with acoustic/linguistic features
and white noise inputs. Although these models achieve high-
ﬁdelity speech generation without many ad hoc designs im-
posed on them, pitch controllability is degraded because of
the data-driven nature of not explicitly modeling excitation
signals as the source-ﬁlter-based models. To improve the
pitch controllability while keeping the uniﬁed and generic
network architectures, we proposed a QP structure [33], [34]
for WN. The proposed QPNet implemented a source-ﬁlter-
like mechanism into WN to simultaneously model the pe-
riodicity and aperiodicity of speech signals using a pitch-
adaptive network architecture. In this paper, to achieve real-
time generations, we extend the QP structure to the non-AR
PWG model [25] to markedly improve the generation speed
and show the generality of the proposed PDCNN, which can
be easily integrated into any CNN-based network.

B. GAN-based Vocoders

Recently, because of the successes of GAN [51] in image
and video generation, GAN-based neural vocoders [17]–[30]
have also been proposed. The two main categories of recent
GAN-based neural vocoders are models with prior speech
knowledge and models directly trained in a data-driven manner
as mentioned in the previous section.

Among the models with speech knowledge, GlotGAN [17],
[18] achieved early success in generating glottal excitation
signals, but
it suffered severe speech quality degradation
when directly applied to raw speech waveform generation.
GELP [19] has been proposed to improve the glottal generator
by using short-time Fourier transform (STFT)-based regression
loss and the adversarial loss of the ﬁnal generated waveforms.
For neural spectral ﬁltering,
the authors of [30] proposed
a GAN-based vocoder with tailored periodic and aperiodic
inputs, and the model was trained with the GAN loss of the
generated waveform and the Gaussian loss of its aperiodic
components. Inspired by the neural excitation generation of
differentiable digital signal processing (DDSP) [52] and the
neural spectral ﬁltering of NSF, completely differentiable
source-ﬁlter vocoders with a GAN structure such as neural
homomorphic vocoder (NHV) [20] and HooliGAN [21] also
have been proposed. Furthermore, the authors of HiNet [22]
also adopt a deep NN (DNN) model and an NSF model with
GAN structures to respectively predict amplitude spectrum and
phase for hierarchical speech generation.

Among the purely data-driven models,

teacher–student-
based parallel WN [12] conditioned on the mel-spectrogram
has been combined with a GAN structure of the waveform
domain for joint optimization [23] and speaker adaptation [24].
Furthermore, MelGAN [26] and GAN-TTS [27] have been
proposed to directly transform acoustic features to speech
waveforms using GAN structures with tailored generators
and discriminators. Speciﬁcally, both MelGAN and GAN-
TTS have an upsampling generator that gradually expands the
temporal resolution of the input acoustic features to match the
speech waveforms. MelGAN adopts a multi-scale discrimi-
nator with several different downsampling rates to enable its

Fig. 2. Architecture of parallel WaveGAN.

generator to capture the information of different levels. GAN-
TTS also adopts an ensemble of 10 similar discriminators with
different input window sizes with or without the conditional
acoustic features to guide its generator to learn different
aspects of speech information. Furthermore, the variants of
MelGAN such as VocGAN [28] adopted a multi-scale gen-
erator and a hierarchically-nested discriminator and multi-
band MelGAN [29] incorporated a multi-band technique into
MelGAN also achieved further speech quality or generative
efﬁciency improvements.

Another purely data-driven model called PWG [25], which
transforms white noise into speech with conditional mel-
spectrograms, has also been proposed. Instead of complex
discriminators, PWG adopts a simple one with stacked DCNN
layers. To achieve stable PWG training, STFT-based losses are
also utilized. In conclusion, most recent GAN-based neural
vocoders have adopted a convolutional feedforward network,
and the hierarchical information of speech waveforms such as
multi-resolution STFT-based losses is essential for training a
high-quality raw waveform generator.

In this paper, we focus on introducing prior pitch knowledge
to the data-driven PWG model, which is fast, compact, simple,
and easy to train, to improve its pitch controllability and
speech modeling capability and make it more consistent with
the deﬁnition of a vocoder.

III. PARALLEL WAVEGAN

As shown in Fig. 2, PWG includes a classical GAN module,
which consists of a discriminator (D) and a generator (G), with
fully convolutional feedforward networks and an additional
multi-resolution STFT loss module. The details are as follows.

A. GAN-based Waveform Generation

A WN-like architecture is adopted for the generator of
PWG. The main differences between the PWG generator and
WN are a Gaussian noise input instead of previous samples, a
raw waveform output instead of a probability distribution, and
a non-AR manner. Speciﬁcally, the inputs of the generator are
a Gaussian noise sequence z and auxiliary acoustic features,
and z is drawn from a Gaussian distribution with zero mean

Generator (G)Gaussian noiseAcoustic featuresDiscriminator (D)Generated speechNatural speechMulti-resolutionSTFT lossLDLadvLspLGλadv TrainingSynthesisJOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

4

and standard deviation, denoted as N (0, I). The output of the
generator is the waveform samples. The generator, which tries
to generate realistic speech samples, is trained in a manner
adversarial to the discriminator, which attempts to distinguish
natural (real) and generated (f ake) speech waveforms. The
adversarial loss of the generator (Ladv) is formulated as

Ladv(G, D) = Ez∈N (0,I)

(cid:2)(1 − D(G(z)))2(cid:3) .

(1)

Note that all auxiliary features of the generator are omitted
in this section for simplicity. Unlike some ﬂow-based mod-
els [14], [15], which adopt an invertible network to map the
real data into the Gaussian noise sequence, the generator of
PWG learns to transfer the input noise sequence to the output
waveforms via the feedback from the discriminator.

Furthermore, a simple architecture consisting of stacked
DCNN layers with LeakyReLU [53] activation functions is
adopted for the discriminator of PWG, and the dilation size of
each DCNN layer increases exponentially with a base of 2 and
the exponent of its layer index. The discriminator is trained to
minimize the adversarial loss (LD) formulated as

LD(G, D)
= Ex∈pdata

(cid:2)(1 − D(x))2(cid:3) + Ez∈N (0,I)

(cid:2)D(G(z))2(cid:3) ,

(2)

where x denotes the natural samples and pdata denotes the
data distribution of the natural samples.

B. Multi-resolution STFT Loss

Since training PWG with only adversarial losses is difﬁcult
and tends to be unstable, an additional STFT-based loss (Lsp)
is adopted to improve the stability and efﬁciency of the GAN
training. Speciﬁcally, a spectral convergence loss (Lsc) is
formulated as

Lsc(x, ˆx) =

(cid:107)|STFT(x)| − |STFT( ˆx)|(cid:107)F
(cid:107)|STFT(x)|(cid:107)F

,

and a log STFT magnitude loss (Lmag) is formulated as

Lmag(x, ˆx)
1
N

=

(cid:107)log |STFT(x)| − log |STFT( ˆx)|(cid:107)L1 ,

(3)

(4)

where ˆx denotes the samples generated from the generator,
(cid:107)·(cid:107)F is the Frobenius norm, (cid:107)·(cid:107)L1 is the L1 norm, |STFT (·)|
denotes the STFT magnitudes, and N is the number of
magnitude elements. The multi-resolution STFT-based loss
Lsp is formulated as

Lsp(G) =

1
M

M
(cid:88)

(L(m)

sc (G) + L(m)

mag(G)),

(5)

m=1

where M denotes the number of STFT setting groups, and
each group includes different FFT sizes, frame lengths, and
frame shifts. The losses L(m)
mag are calculated on the
basis of the STFT features extracted using the settings of the m
group. The multiple STFT losses prevent the generator from
a suboptimal problem and enhance the modeling capability
of the generator by making it capture speech structures with

and L(m)

sc

Fig. 3. Pitch-dependent dilated convolution.

different resolutions. In conclusion, the overall training loss of
the PWG generator (LG) is formulated as

LG(G, D) = Lsp(G) + λadvLadv(G, D),

(6)

which is a weighted sum of Ladv and Lsp with weight λadv.
The hyperparameter λadv is empirically set to 4.0 in this paper.

C. Problems in Using PWG as a Vocoder

Although PWG achieves high-ﬁdelity speech generation
with acoustic features, it is still vulnerable to unseen acoustic
features such as scaled F0. That is, the speech quality and pitch
accuracy of the PWG-generated speech will markedly degrade
when the F0 of the auxiliary acoustic features is scaled or is
outside the F0 range of training data [33], [34]. The possible
reasons for the degradation are the generic architecture, data-
driven nature, and lack of prior speech knowledge. Moreover,
since speech is a quasi-periodic signal, which includes both
periodic components with long-term correlations and aperi-
odic components with short-term correlations, modeling both
components with the ﬁxed network architecture of PWG is
inefﬁcient. For instance, the ﬁxed receptive ﬁeld size of the
network for both periodic and aperiodic components may
not be reasonable, and the receptive ﬁeld may include many
redundant samples when modeling the periodic structures of
speech.

IV. QUASI-PERIODIC PARALLEL WAVEGAN

Since pitch controllability is an essential feature of a
vocoder, we propose QPPWG [46] to improve the pitch con-
trollability and speech modeling efﬁciency of PWG. Specif-
ically, because the effectiveness of the GAN structure and
the multi-resolution STFT losses have been shown for PWG,
the proposed QPPWG only improves the generator of PWG
using the QP structure while keeping other components of
PWG the same. The QP structure of the proposed generator
introduces pitch information to the network via a non-AR
PDCNN module and a cascaded architecture. The details are
as follows.

A. Non-autoregressive Pitch-dependent Dilated Convolution

Inspired by pitch ﬁltering in code-excited linear prediction
(CELP) [54], [55], we proposed a PDCNN for causal AR

<=66)<=66>*.*08’3*/?’*1-@??*.8’3*/+*.*08’3*/?’*1-A$80$8&:‘@\;=B’--*:&:‘A\;=C:0$8)’8.9--*0*:-*:8/-’1%8*-/?%.8,+&/;=;4‘@222222A$80$8&‘@B’--*:&‘AC:0$8JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

5

Fig. 4.

(a) QPPWG generator with stacked QP strucutre; (b) QPPWG generator with parallel QP strucutre; (c) PWG generator (d) Macroblock.

models [33], [34]. In this paper, we further extend the PDCNN
in a non-causal manner for the non-AR PWG model. As shown
in Fig. 3, a DCNN is a convolution layer with gaps between
input samples, and the length of each gap is a predeﬁned
hyperparameter called the dilation size (rate). The non-causal
dilated convolution can be formulated as

t = W (c) × y(i)
y(o)

t + W (p) × y(i)

t−d + W (f) × y(i)

t+d,

(7)

t

where y(o)
is the DCNN output at sample t, y(i)
is the DCNN
t
input at sample t, and d is the dilation size. W (c), W (p),
and W (f) are the trainable 1 × 1 convolution ﬁlters of the
current, previous, and following samples, respectively. For the
vanilla DCNN, d is a predeﬁned time-invariant constant. As
an extension of a DCNN, the dilation size d(cid:48) of a PDCNN is
pitch-dependent and time-variant.

Speciﬁcally, the pitch-dependent dilated factor Et is multi-
plied by the dilation size d in each time step t to dynamically
set the dilation size d(cid:48) as

d(cid:48) = Et × d.

The dilated factor Et is derived from

Et = Fs/(F0,t × a),

(8)

(9)

where Fs is the sampling rate, F0,t is the fundamental fre-
quency of the input sample at time step t, and a is the dense
factor. The dense factor a is a hyperparameter that indicates
the number of samples in one cycle taken as the inputs of a
PDCNN. The higher the dense factor, the lower the sparsity
of the PDCNN. Using the pitch-dependent dilation size, the
architecture of QPPWG with PDCNNs is dynamically changed
according to the input F0 feature.

Furthermore, according to our previous work [33], [34],
calculating Et using the interpolated F0 values of the adjacent
voiced segments achieves higher speech quality than directly
setting Et to one for the unvoiced segments. Because our
internal evaluation results of QPPWG also show the same
tendency, all QPPWG models in this paper adopt the inter-
polated F0 values for calculating the Et values of the unvoice
segments. In conclusion, the adaptive architecture of QPPWG
introduces pitch knowledge to the network to improve the pitch
controllability, allows each sample to have a speciﬁc receptive
ﬁeld size, and efﬁciently extends the receptive ﬁelds.

B. QPPWG Generator with PDCNNs

As shown in Fig. 4, a QPPWG/PWG generator is com-
posed of input, macroblock, and output modules. The input
module includes a Gaussian noise input with 1 × 1 CNN
and upsampled acoustic features with the matched temporal
resolution to the output waveform samples. As shown in
Fig. 4 (d), a macroblock includes several stacked residual
blocks. The inputs of each residual block are the residual
connection output of the previous block and auxiliary features.
The outputs of each residual block are the residual connection
output for the next block input and the skip connection to the
output module. The architecture of each residual block consists
of a DCNN/PDCNN layer, a gate structure, and a residual
connection. Last, the summation of the skip connections from
all residual blocks is processed by two ReLU [56] activations
with 1 × 1 CNNs to directly output speech waveform samples.
The main difference between the QPPWG and PWG gen-
erators is the QP structure. Speciﬁcally, a QPPWG generator
includes a ﬁxed macroblock and an adaptive macroblock while
a PWG generator includes only one ﬁxed macroblock. The
ﬁxed macroblock consists of only ﬁxed (residual) blocks with
DCNN layers, and the adaptive macroblock consists of only
adaptive (residual) blocks with PDCNN layers. Each ﬁxed
block adopts a DCNN with a ﬁxed network architecture
to model the aperiodic speech components such as spectral
envelopes with short-term correlations. Each adaptive block
adopts a PDCNN layer to model the periodic speech compo-
nents such as excitation signals with long-term correlations,
and the PDCNN layer makes the architecture of the block
adaptive to auxiliary F0 values.

As shown in Fig. 4 (a), unlike PWG consisting of residual
blocks with only DCNNs, QPPWG adopts a cascaded architec-
ture composed of two different macroblocks. The cascaded ar-
chitecture simultaneously models both periodic and aperiodic
speech components in an efﬁcient manner by using prior pitch
knowledge, which also improves its pitch controllability. The
cascaded architecture with prior pitch knowledge is assumed
to have better tractability and interpretability than the original
PWG architecture since it models different speech compo-
nents with related speciﬁc network structures. Furthermore,
in this paper, since we assume that the ﬁxed and adaptive
macroblocks respectively focus on aperiodic and periodic
components, we also explore a new parallel QP structure as

QPPWG w/ parallel QPOutput layersMacroblock 01×1Gaussian noiseUpsampleAcoustic featuresAuxiliary featuresMacroblock 1Output layersSkip connectionInputsMacroblock 0InputsMacroblock 1ReLU1×1ReLU1×1MacroblockDCNN / PDCNNGatedAuxiliary featuresSkip connection1×11×11×1Fixed / AdaptiveResidual blockFixed / AdaptiveResidual blockQPPWG w/ stacked QP(a)(b)(d)MacroblockOutput layersPWG(c)InputsJOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

6

shown in Fig. 4 (b) to better understand the internal speech
production mechanisms.

V. EXPERIMENTS

A. Experimental Settings

All speech generation models in this paper were trained
in a multi-speaker manner. The training corpus consisted of
2200 utterances of the “slt” and “bdl” speakers of the CMU-
ARCTIC corpus [57] and 852 utterances of all speakers of
the Voice Conversion Challenge 2018 (VCC2018) corpus [58].
The total size of the training corpus was around 3000 utter-
ances and the data length was around 2.5 hours. The testing
corpus was the SPOKE set of the VCC2018 corpus. The
SPOKE set consists of two male and two female speakers,
and each speaker has 35 testing utterances. The sampling rate
of all speech data was set to 22,050 Hz, and the resolution of
the speech data was 16-bit.

features. Speciﬁcally,

The auxiliary features of these speech generation mod-
els were composed of one-dimensional continuous F0,
one-dimensional unvoiced/voiced binary code (U/V ), 35-
dimensional mel-cepstrum (mcep), and two-dimensional
coded aperiodicity (codeap)
the
WORLD (WD)1 vocoder was adopted to extract one-
dimensional F0 and 513-dimensional spectral (sp) and ape-
riodicity (ap) features with a frameshift of 5 ms. F0 was
interpolated to the continuous F0 and converted to U/V , ap
was coded into codeap, and sp was parameterized into mcep.
To simulate unseen data, the continuous F0 was scaled by
ratios of 0.5 and 2 while keeping the other features the same.
Moreover, the dilated factor Et of QPPWG was empirically
calculated on the basis of the continuous F0 because of the
higher speech quality [33], [34].

All PWG-like models were trained with the RAdam op-
timizer [59] ((cid:15) = 10−6) with 400 k iterations. Speciﬁcally,
the generators were trained with only multi-resolution STFT
losses for the ﬁrst 100 k iterations and then jointly trained
with the discriminators for the following 300 k iterations.
The multi-resolution STFT losses were calculated on the
basis of three STFT setting groups including different FFT
sizes (1024/2048/512), frame shifts (120/240/50), and frame
lengths (600/1200/240). The balanced weight λadv of Ladv
was set to 4.0. The generators’ learning rate was 10−4 and
the discriminators’ learning rate was 5 × 10−5. Both learning
rates decayed by 50 % every 200 k iterations. The minibatch
size was six and the batch length was 25,520 samples. Fur-
thermore, the baseline QPNet2 model was trained with the
Adam optimizer [60] with 200K iterations. The learning rate
of QPNet was 10−4 without decay, and the minibatch size was
one with a batch length of 20,000 samples.

B. Model Descriptions

In this paper, several variants of PWG and QPPWG models
and a baseline QPNet model were involved in the evaluations.
To describe the different architecture of each model, several

1https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder
2https://github.com/bigpon/QPNet

basic modules are introduced. Speciﬁcally, a macroblock mod-
ule consisting of stacked residual blocks was adopted, and each
macroblock was only composed of one type of residual block
namely, adaptive blocks (BAda) or ﬁxed blocks (BFix). The
PWG models only consisted of one macroblock (Macro 0)
with ﬁxed blocks. The proposed QPPWG and baseline QPNet
models were composed of two cascaded macroblocks (Macro
0 and 1) with different types of residual block.

Taking vanilla PWG as an example, the architecture com-
posed of 30 ﬁxed blocks with three cycles (repeats) of ex-
ponentially increasing dilation size, and each cycle contained
10 ﬁxed blocks. Therefore, the number of total blocks (Block
Num) of vanilla PWG was 30, and the vanilla PWG architec-
ture was 10 ﬁxed blocks × 3 cycles denoted as BFix10×3. For
the baseline QPNet, since Macro 0 consisted of 12 ﬁxed blocks
with 3 cycles (BFix4×3), and Macro 1 consisted of 4 adaptive
blocks with 1 cycles (BAda4 × 1), the order of macroblock
(Macro order) was denoted as BFix → BAda. The proposed
QPPWG models followed the same naming conventions.

Moreover, all PWG and QPPWG models had the same
discriminator architecture, which consisted of 10 non-causal
DCNN layers with 64 convolution channels, three kernels,
and LeakyReLU (α = 0.2) activation functions. For each
adaptive/ﬁxed block of the QPPWG/PWG generator, a gated
activation with tanh and sigmoid functions was adopted, and
the number of CNN channels of residual and skip connections
and auxiliary features was also 64. The QPNet structure
followed that in our previous works [34], and the number of
CNN channels of residual connections and auxiliary features
was 512 and that of skip connections was 256.

C. Objective Evaluations

As reported in this section, the quality of the vocoders
was evaluated by the mel-cepstral distortion (MCD), root
mean square error (RMSE) of log F0, and U/V decision
error. These measurements were calculated using the auxiliary
features and the acoustic features extracted from the generated
speech. Speciﬁcally, WD disentangles speech into a resonance
component, spectral envelope sp, and source components
including F0 and ap, and the designs of WD try to make
the extracted spectral envelope and source components highly
uncorrelated [61]. Therefore, the mcep and F0 features were
assumed as independent in this paper. In other words, for the
auxiliary features of the neural vocoders, we only manipulated
the F0 values and kept other acoustic features the same. Since
the topic of this paper is a neural vocoder, the ground truth
acoustic features of the objective evaluations were the auxiliary
features. That is, even for the scaled F0 scenarios, the ground
truth mcep was still the mcep extracted from natural speech.
The following objective evaluations were conducted to
explore different hyperparameter settings to ﬁnd the most
efﬁcient network architecture. Three design principles were
adopted to select the ﬁnal QPPWG architecture. First, because
of the more efﬁcient speech modeling of the QP structure, we
try to reduce the number of residual blocks while maintaining
a similar speech quality. Secondly, since the receptive ﬁeld
length is highly related to the speech modeling capacity, if the

JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

7

TABLE I
CNN CHANNELS OF PWG GENERATOR

TABLE III
RATIOS OF FIXED AND ADAPTIVE BLOCKS OF QPPWG GENERATORS
WITH 20 RESIDUAL BLOCKS, 16 CNN CHANNELS, AND DENSE FACTOR 4

Channels

MCD (dB)
F0RMSE
U/V (%)
Size (×106)

WD
-

2.58
0.10
10
-

64

3.69
0.12
14
1.16

PWG

32

4.15
0.14
16
0.34

16

4.23
0.15
16
0.11

8

4.89
0.20
15
0.04

TABLE II
BLOCKS AND CYCLES OF PWG GENERATORS WITH 16 CNN CHANNELS

Block Num
BFix

MCD (dB)
F0RMSE
U/V (%)
Size (×106)

30
10 × 3

20
10 × 2

10
10 × 1

4.23
0.15
16
0.11

4.61
0.17
17
0.08

5.95
0.31
33
0.04

20
5 × 4

4.59
0.35
44
0.08

20
20 × 1

5.98
0.30
27
0.08

performance differences are small, the model with the longest
receptive ﬁeld length will be selected. Last, the motivation of
this work is to improve the pitch controllability, so the pitch
accuracy is the ﬁrst priority.

1) Number of CNN Channels: To efﬁciently explore dif-
ferent network architectures and hyperparameter settings, we
ﬁrst explored the relationship between model capacities and
the number of CNN channels and tried to reduce the CNN
channels for fast model training while keeping reasonable
speech quality. The vanilla PWG generators with 8–64 CNN
channels were evaluated. Note that because this work focused
on improving the generator, all PWG/QPPWG models in this
section adopted the same discriminator, whose number of
CNN channels was 64 and whose model size was 0.1 M.
The results in Table I show that
the original setting (64
CNN channels) predictably achieves the best performance
characteristics of all objective measurements. However, even if
the number of CNN channels is reduced to 16, which greatly
reduces the training time because of the compact model size,
the speech quality and pitch accuracy are still acceptable.
Therefore, the objective evaluations in the following sections
were conducted based on the models with 16 CNN channels.
2) Numbers of Blocks and Cycles: Since one of the moti-
vations for adopting the QP structure is taking advantage of
the higher speech modeling capability to reduce the model
size, the importance of the numbers of residual blocks was
ﬁrst evaluated. As shown in Table II, we ﬁrst kept 10 residual
blocks in one cycle and reduced the number of cycles to cut
down the number of total blocks. The results show that the
model with 20 blocks still achieves acceptable performance
while the performance of the 10 blocks model signiﬁcantly
degrades. Moreover,
the importance of the dilation cycle
number was also evaluated. The results indicate that compared
to the model with two cycles, the four cycles model achieves
slightly higher spectral modeling accuracy but much lower
pitch accuracy, and both the spectral and pitch accuracies
of the one cycle model markedly degrades. In conclusion,
although fewer dilation cycles result in a longer receptive ﬁeld,

Macro 0 (BAda)
Macro 1 (BFix)

5 × 1
5 × 3

5 × 2
5 × 2

1 × F0
1/2 × F0
2 × F0

Average

1 × F0
1/2 × F0
2 × F0

Average

1 × F0
1/2 × F0
2 × F0

Average

4.79
5.22
5.66

5.22

0.13
0.22
0.10

0.15

23
26
18

23

5 × 3
5 × 1

5.58
6.03
7.13

6.24

MCD (dB)

4.79
5.29
6.03

5.37

RMSE of log F0
0.12
0.17
0.12

0.13
0.17
0.12

0.14

0.14

U/V decision error (%)

16
21
15

17

16
20
16

17

5 × 4
-

7.48
8.16
8.47

8.04

0.14
0.19
0.14

0.15

20
22
18

20

the network may not model the speech well. By contrast, the
larger the number of dilation cycles, the shorter the receptive
ﬁeld. Since a longer effective receptive ﬁeld can be achieved
by replacing ﬁxed blocks with adaptive blocks, we focus on
improving the PWG generators with 20 residual blocks and
two or four cycles using the QP structure in this paper.

3) Ratio of Fixed and Adaptive Blocks: Since speech is
a quasi-periodic signal, speech modeling is theoretically re-
quired both ﬁxed and adaptive blocks to respectively model
aperiodic and periodic components. To explore the efﬁcient
ratio of ﬁxed and adaptive blocks, four QPPWG models
with 20 residual blocks, four cycles, and dense factor 4
were evaluated. Speciﬁcally, because of the more possible
combinations of ﬁxed and adaptive blocks, the number of
cycles was set to four. The dense factor was empirically set
to 4, and more discussions of dense factor are presented in
the following subsection. As shown in Table III, although the
model with only adaptive blocks (BAda5 × 4) has the longest
receptive ﬁelds, the spectral modeling accuracy is markedly
low because of the limited modeling capability of the aperiodic
components. The same tendency can also be observed in the
spectral domain. The more adaptive blocks the model has, the
more harmonic components the generated speech has. How-
ever, overenhanced harmonic structures generate signiﬁcantly
robotic and unnatural sounds.

Since the model with balanced numbers of adaptive and
ﬁxed blocks achieves the highest pitch accuracy and lowest
U/V error while keeping acceptable spectral accuracy and
attaining longer receptive ﬁelds than the model with only ﬁve
adaptive blocks, the 20 residual blocks with balanced numbers
of adaptive and ﬁxed blocks was selected as the QPPWG
paradigm. To summarize, the ratio of adaptive and ﬁxed blocks
is crucial to the network for avoiding over/undermodeling
the harmonic structures. Moreover, since one dilation cycle
including 10 ﬁxed blocks showed effectiveness in the PWG
and WN models, and the receptive ﬁelds of 10 ﬁxed blocks

JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

8

TABLE IV
QP STRUCTURE OF QPPWG 20 GENERATOR WITH
16 CNN CHANNELS AND DENSE FACTOR 4

TABLE V
DENSE FACTOR OF QPPWGaf 20 GENERATOR WITH
16 CNN CHANNELS

QP structure

1 × F0
1/2 × F0
2 × F0

Average

stacked

RMSE
0.14
0.18
0.14

0.15

MCD
5.10
5.49
6.32

5.63

U/V
18
21
26

22

MCD
5.80
6.05
6.00

5.95

parallel

RMSE
0.32
0.48
0.45

U/V
28
43
52

Dense a

1 × F0
1/2 × F0
2 × F0

0.42

41

Average

16

5.26
5.64
5.92

5.60

0.13
0.21
0.14

0.16

17
25
24

22

8

4

2

1

MCD (dB)
5.10
5.49
6.32

5.63

RMSE of log F0
0.14
0.18
0.14

0.15

5.26
5.57
6.06

5.63

0.13
0.17
0.14

0.14

5.35
5.61
5.99

5.65

0.14
0.23
0.14

0.17

U/V decision error (%)
18
17
21
21
19
26

19

22

17
24
20

20

5.36
5.61
6.03

5.67

0.17
0.28
0.15

0.20

17
27
20

21

1 × F0
1/2 × F0
2 × F0

Average

1 × F0
1/2 × F0
2 × F0

Average

TABLE VI
MODEL SIZE (G: GENERATOR; D: DISCRIMINATOR)

Block Num
Macro 0 (BFix)
Macro 1 (BAda)
G (×106)
D (×106)

Macro order
Block Num
Macro 0
Macro 1

G (×106)
D (×106)

QPNet
16
4 × 3
4 × 1

24
-

30
10 × 3
-

1.16
0.10

PWG
20
10 × 2
-

0.78
0.10

16
4 × 4
-

0.63
0.10

QPPWGaf
BAda → BFix
16
20
4 × 2
5 × 2
4 × 2
10 × 1

QPPWGf a
BFix → BAda
16
20
4 × 2
10 × 1
4 × 2
5 × 2

0.79
0.10

0.63
0.10

0.79
0.10

0.63
0.10

of 1 and 2 achieve slightly worse performance. A similar
tendency was also observed by listening to the generated
speech. The generated utterances from the models with dense
factors of 1 and 2 were more unstable. Furthermore, PDCNN
degenerates to DCNN when Et is one, and a larger dense
factor makes Et closer to one for more F0 values. Therefore,
since a lower dense factor attains a longer receptive ﬁeld
expansion and a higher lower bound of F0, which makes PD-
CNN degenerate to DCNN, the dense factors of the following
QPPWG models were set to 4.

6) Overall Objective Evaluation: An overall objective eval-
uation was conducted including the WD, QPNet, PWG, and
QPPWG models. Speciﬁcally, since the AR QP structure has
shown effectiveness for the WN [33], [34] vocoder,
is
interesting to explore the generality of the QP structure for
non-AR models and the performance difference between the
QPNet and QPPWG models. Because the QPNet architecture
contained only 16 residual blocks with four cycles, the PWG
and QPPWG models with 16 residual blocks and four cy-
cles were also evaluated. Moreover, the effectiveness of the
different QPPWG macroblock orders was also explored. The

it

are longer than that of 5 × 2 ﬁxed blocks, the architecture of
the following QPPWG models was set to 20 residual blocks
including 5 × 2 adaptive blocks and 10 ﬁxed blocks. The
QPPWG architecture is denoted as QPPWG 20.

4) QP Structure: Since the ﬁxed and adaptive blocks are
assumed to respectively model aperiodic and periodic compo-
nents of speech signals, a new parallel QP structure (Fig. 4
(b)) was evaluated in this paper compared to the original
stacked QP structure (Fig. 4 (a)). However, the results in
Table IV show that the QPPWG 20 model with a parallel
QP structure achieves very low pitch accuracy and high U/V
errors, which indicate the very limited periodic component
modeling capability of the parallel model. Observing the
output waveforms of the skip connection summation from the
adaptive/ﬁxed blocks, we also ﬁnd that the output waveforms
are dominated by the ﬁxed blocks in the parallel QP model
while the outputs of the adaptive blocks are very small. In
other words, these results show that only the ﬁxed blocks
are well activated for speech modeling when the parallel QP
structure is adopted.

The possible reason is that

the difﬁculty of modeling
speech using a ﬁxed network architecture is lower than that
of the network adopting a more complicated pitch-adaptive
architecture in the very initial stage. Since the gradient paths
this
of the ﬁxed and adaptive macroblocks are separated,
difference of modeling difﬁculty may make the whole adaptive
macroblock inactive. On the other hand, because the adaptive
and ﬁxed macroblocks are cascaded in the stacked QP struc-
ture, these macroblocks are in the same gradient ﬂow, which
makes the entire network participates in the speech modeling.
Furthermore, since the aperiodic and periodic components
are not completely independent,
the stacked QP structure
takes advantage of the aperiodic and periodic information
propagations between the ﬁxed and adaptive macroblocks to
get better speech modeling capability. As a result, the stacked
QP structure was selected as the QPPWG paradigm. Further
discussion and more details about the outputs of the adap-
tive and ﬁxed macroblocks will be presented in Section VI.
Moreover, the cascaded adaptive to ﬁxed macroblock order is
denoted as af , and the reversed macroblock order is denoted
as f a. The effectiveness of the macroblock order will be
presented in the overall objective evaluation.

5) Dense Factor: The dense factor is inversely proportional
to the receptive ﬁeld size, and the QPPWGaf 20 models with
1–16 dense factors were evaluated. The results in Table V
show that while the models with dense factors of 4–16 achieve
similar generative performance, the models with dense factors

JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

9

TABLE VII
WORLD, QPNET, PWG, AND QPPWG VOCODERS

Male

16

4.28
5.02
4.39

QPPWGaf
16
20

QPPWGf a WD
16
20

-

QPNet
16

30

PWG
20

3.72
5.08
4.12

4.15
5.41
4.58

4.44
5.49
4.78

MCD (dB)
2.59
5.01
2.69
5.82
4.49
5.21

4.11
4.55
4.99

3.76
3.85
6.34

3.79
3.88
6.03

4.56

4.31

4.71

4.90

5.35

3.26

4.55

4.65

4.57

Gender

Vocoder
Block Num

WD
-

QPNet
16

30

PWG
20

1 × F0
1/2 × F0
2 × F0

2.57
5.09
3.10

4.29
5.29
4.23

3.61
5.09
4.13

Average

3.59

4.60

4.28

1 × F0
1/2 × F0
2 × F0

0.13
0.20
0.12

0.19
0.35
0.16

0.15
0.43
0.14

3.70
4.90
4.10

4.23

0.19
0.53
0.14

Average

0.15

0.24

0.24

0.29

0.49

0.19

1 × F0
1/2 × F0
2 × F0

Average

12
17
11

13

17
32
17

22

20
23
12

18

19
23
13

18

53
38
62

51

20
23
16

20

0.54
0.54
0.39

0.15
0.30
0.13

RMSE of log F0

0.14
0.30
0.12

0.16
0.29
0.12

0.07
0.08
0.09

0.09
0.11
0.20

0.08
0.11
0.15

0.10
0.11
0.16

0.07
0.09
0.10

0.19

0.19

0.08

0.13

0.12

0.12

0.55

0.09

U/V decision error (%)
11
19
20
23
28
14

22
22
13

8
12
11

19

19

10

20

9
19
13

14

10
20
21

17

57
52
69

60

13
22
23

19

0.13
0.21
0.11

0.15

21
22
18

20

0.08
0.09
0.09

0.08

14
23
11

16

0.07
0.09
0.11

0.09

11
22
12

15

QPPWGaf
16
20

QPPWGf a
16
20

3.87
3.95
5.73

4.20
4.36
6.26

4.65
4.88
6.44

4.52

4.94

5.32

Female

16

4.21
4.28
4.72

4.41

0.28
0.29
1.08

4.98
5.37
6.73

5.69

0.08
0.09
0.10

0.09

10
23
10

14

number of CNN channels of the PWG and QPPWG models
was set to 64 following the original setting. The model sizes
are shown in Table VI. Since the model size is proportional
to the square of the number of CNN channels, the model size
of vanilla PWG is only 5 % of that of QPNet because of the
greatly reduced number of CNN channels. The sizes of the
QPPWG models were reduced further by 30–50 % because of
the reduced number of residual blocks compared with that of
vanilla PWG (PWG 30).

To present

the correlations of the F0 distributions and
vocoder performances, the gender-dependent results are shown
in Table VII. Speciﬁcally, because of the multi-speaker training
manner, the F0 range of the training data covered both male
and female F0 values. Therefore, the most female 1/2 × F0
and male 2 × F0 values are still in the F0 range of the training
data while the most female 2×F0 and male 1/2×F0 values are
outside the F0 range. Since these gender-dependent differences
might cause different effects in the scaled F0 evaluations, the
gender-dependent results are more informative.

As the MCD results shown in Table VII, the female 2 × F0
and male 1/2 × F0 sets achieve much higher MCD than
the female 1/2 × F0 and male 2 × F0 sets as we expected.
However, the overall tendencies of the male and female sets
are similar. The QPPWG models with the af order outperform
the models with f a order in both sets and all scenarios
showing the superiority of QPPWGaf . The possible reason
is that modeling the long-term structure of speech signals ﬁrst
as QPPWGaf makes the generated speech more stable than
modeling the details ﬁrst as QPPWGf a. More details about
the comparison between QPPWGaf and QPPWGf a will be
presented in Section VI.

Furthermore, the QPPWGaf 20 model achieves a com-
parable spectral accuracy with the PWG 30 and PWG 20
models showing the QP structure keeping the similar spectral
prediction accuracy. Although the average MCD of PWG 16-

generated utterances is not very high, the very high RMSE of
log F0 and the very high U/V error indicate that the speech
quality of PWG 16 is low. Speciﬁcally, the similar MCDs of
PWG 16-generated utterances with different scaled F0 values
imply that the PWG 16 model tends to ignore the F0 scaled
ratio to generate similar speech waveforms. The very high
RMSE of log F0 and the very high U/V error also indicate that
the PWG 16-generated speech waveforms lack ﬁne harmonic
structures. On the other hand, compared to QPNet, although
the model size of QPPWGaf 16 is much smaller than that of
QPNet, the non-AR mechanism and GAN structure still make
QPPWG achieve comparable spectral prediction accuracy.

Because the GAN structure greatly improves the speech
modeling capability, the results of the F0 RMSE and U/V
error in Table VII show that
the non-AR PWG models
already achieve a comparable pitch accuracy with the AR
QPNet model. However, the QP structure still further im-
proves the pitch accuracy of the non-AR PWG models. The
QPPWGaf 16 model even attains a similar pitch accuracy
to the reference WD vocoder. Although the pitch and U/V
accuracies of PWG 16 markedly degrade because of the
short receptive ﬁeld, the QPPWGaf 16 model signiﬁcantly
improves them to an acceptable level showing the effectiveness
of the QP structure to enlarge receptive ﬁled. In conclusion,
the QP structure efﬁciently increases the effective receptive
ﬁeld size and introduces the pitch information to the network,
resulting in a comparable spectral accuracy, a much higher
pitch accuracy, and a smaller model size. The objective results
show the effectiveness of the proposed QP structure for the
PWG models.

On the other hand, since the WD-extracted mcep and F0
are not completely independent, taking mcep extracted from
natural speech as the ground truth of the scaled F0 scenar-
ios might cause some mismatches. However, the objective
the
evaluations still provide meaningful

information about

JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

10

Fig. 5. Speech quality MOS evaluations of male speakers with 95 % CI.

Fig. 6. Speech quality MOS evaluations of female speakers with 95 % CI.

performance of these vocoders, and we also conducted the
subjective evaluation in the following subsection to provide
convincing results from different aspects.

D. Subjective Evaluations

The set of samples used for subjective evaluation was
composed of 1680 synthesized and 80 natural utterances.
The synthesized utterances were generated by seven vocoders
conditioned on three F0 scaled ratios (unchanged, halved,
and doubled) and four speakers (the VCC2018 SPOKE set).
For each vocoder, speaker, and F0 scaled ratio, we randomly
selected 20 utterances from the 35 testing utterances for both
mean opinion score (MOS) and ABX evaluations. Speciﬁcally,
the speech quality of each utterance was evaluated by listeners
assigning MOSs of 1–5. The higher the MOS, the better the
speech quality. For each ABX, two testing utterances were
compared with one reference, and the listeners chose the one
whose pitch was more consistent with that of the reference.
Eight listeners evaluated part of the subjective evaluation set
in both MOS and ABX tests, and each utterance/pair was
evaluated by at least two listeners. Although the listeners were
not native English speakers, they worked on audio-related
research. The demo utterances can be found on the demo
page [62].

1) MOS Evaluation of Speech Quality: The MOS evalu-
ation included the vocoders of WD, QPNet, PWG of three
different sizes, and QPPWG of two different sizes. The MOS
results shown in Figs. 5 and 6 are presented for three different
F0 scaled ratios for male and female speakers, respectively.

The overall results show that the proposed QP structure im-
proves the speech modeling capacity of the PWG vocoders, es-
pecially when the PWG 16 vocoder has a very small receptive
ﬁeld. Because the QPPWG vocoders markedly outperform the
PWG vocoders of the same size for all scenarios in the MOS
evaluation, the following discussion focuses on comparisons
among QPPWGaf 20, PWG 30, and QPNet.

For the 1/2 × F0 scenario,

the QPPWGaf 20 vocoder
markedly outperforms the PWG 30 and WD vocoders and
attains a similar speech quality to the QPNet vocoder for the
male set. For the female set, the QPPWGaf 20 vocoder is
comparable to the PWG 30 and QPNet vocoders while still
outperforming the WD vocoder. The results indicate that the
models with the QP structure are more robust for an unseen
F0 outside the F0 range of the training data, such as most
of the 1/2 × F0 values in the male set. On the other hand,
although the combination of the 1/2 × F0 and other acoustic
features in the female set is still unseen, the scaled F0 values
are almost in the F0 range of the training data. Therefore, the
PWG 30 vocoder can still achieve a similar speech quality to
the QPPWGaf 20 vocoder.

For the 2×F0 scenario, because most of the scaled F0 values
of the male set are in the F0 range of the training data, the
performance of the QPPWGaf 20 vocoder is similar to that
of the PWG 30 vocoder for the male set. The QPPWGaf 20
vocoder outperforms the WD and QPNet vocoders in the male
set, while the QPNet vocoder achieves an inferior speech
modeling capacity for the 2 × F0 scenario [33], [34]. On the
other hand, although the QPPWGaf 20 vocoder predictably

> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 7 ratios to generate similar speech waveforms. The high RMSE of log F0 and U/V error also indicate that the PWG_16-generated speech waveforms lack fine harmonic structures. The results of the F0 RMSE and U/V error shown in Table VII also show that the non-AR PWG models already achieve a comparable pitch accuracy as the AR QPNet model. However, the QP structure still further improves the pitch accuracy of the non-AR PWG models. The QPPWGaf_16 model even attains a similar pitch accuracy as the reference WD vocoder. Moreover, although the generative performance of PWG_16 markedly degrades because of the short receptive field, the QPPWGaf_16 significantly improves its speech quality and pitch accuracy to an acceptable level. In conclusion, the QP structure efficiently extends the effective receptive field size and introduces the F0 information to the network conducing to the comparable spectral accuracy, much higher pitch accuracy, and smaller model size. The objective results show the effectiveness of the proposed QP structure for the PWG models.   D. Subjective Evaluations The subjective evaluation set was composed of 1680 synthesized utterances and 80 natural utterances. The synthesized utterances were generated by seven vocoders conditioned on three F0 scaled ratios (unchanged, halved, and doubled) and four speakers (VCC2018 SPOKE set). For each vocoder, speaker, and F0 scaled ratio, we randomly selected 20 TABLE VII COMPARISONS OF WORLD, QPNET, PWG-SERIOUS, AND QPPWG-SERIOUS MODELS WD QPNet PWG QPPWG - - 30 20 16 af_20 af_16 fa_20 fa_16 MCD (dB) 1×F0 2.58 4.20 3.69 3.74 4.25 3.80 4.18 4.54 4.99 ½×F0 3.89 4.92 4.47 4.39 4.65 4.52 4.89 5.18 5.60 ³∕₂×F0 3.09 4.32 4.23 4.12 4.36 4.21 4.73 5.00 5.44 2×F0 3.79 4.61 5.24 5.06 4.56 4.92 5.42 5.61 5.97 Average 3.34 4.51 4.41 4.33 4.45 4.36 4.80 5.08 5.50 RMSE of log F0 1×F0 0.10 0.14 0.12 0.15 0.41 0.11 0.10 0.11 0.12 ½×F0 0.14 0.23 0.27 0.32 0.42 0.19 0.15 0.20 0.19 ³∕₂×F0 0.10 0.16 0.12 0.12 0.40 0.11 0.10 0.10 0.11 2×F0 0.10 0.18 0.15 0.15 0.73 0.11 0.10 0.11 0.11 Average 0.11 0.18 0.16 0.18 0.49 0.13 0.11 0.13 0.13 U/V decision error (%) 1×F0 10 14 14 15 55 16 18 15 16 ½×F0 15 26 21 22 45 23 22 23 22 ³∕₂×F0 10 15 11 12 63 15 15 13 13 2×F0 11 22 12 17 66 19 14 13 11 Average 11 19 15 16 57 18 17 16 16 Fig. 4.  Speech quality MOS evaluations of male speakers with 95% CI. Fig. 5.  Speech quality MOS evaluations of female speakers with 95% CI. 4.83.11.72.84.32.92.64.12.23.13.72.12.71.31.31.34.12.83.13.31.72.5123451×𝐹₀½×𝐹₀2×𝐹₀MEAN OPINION SCORENaturalWDQPNetPWG_30PWG_20PWG_16QPPWG𝑎𝑓_20QPPWG𝑎𝑓_164.83.92.63.04.03.61.84.53.71.83.93.31.71.21.21.24.13.72.33.33.22.0123451×𝐹₀½×𝐹₀2×𝐹₀MEAN OPINION SCORENaturalWDQPNetPWG_30PWG_20PWG_16QPPWG𝑎𝑓_20QPPWG𝑎𝑓_16> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 7 ratios to generate similar speech waveforms. The high RMSE of log F0 and U/V error also indicate that the PWG_16-generated speech waveforms lack fine harmonic structures. The results of the F0 RMSE and U/V error shown in Table VII also show that the non-AR PWG models already achieve a comparable pitch accuracy as the AR QPNet model. However, the QP structure still further improves the pitch accuracy of the non-AR PWG models. The QPPWGaf_16 model even attains a similar pitch accuracy as the reference WD vocoder. Moreover, although the generative performance of PWG_16 markedly degrades because of the short receptive field, the QPPWGaf_16 significantly improves its speech quality and pitch accuracy to an acceptable level. In conclusion, the QP structure efficiently extends the effective receptive field size and introduces the F0 information to the network conducing to the comparable spectral accuracy, much higher pitch accuracy, and smaller model size. The objective results show the effectiveness of the proposed QP structure for the PWG models.   D. Subjective Evaluations The subjective evaluation set was composed of 1680 synthesized utterances and 80 natural utterances. The synthesized utterances were generated by seven vocoders conditioned on three F0 scaled ratios (unchanged, halved, and doubled) and four speakers (VCC2018 SPOKE set). For each vocoder, speaker, and F0 scaled ratio, we randomly selected 20 TABLE VII COMPARISONS OF WORLD, QPNET, PWG-SERIOUS, AND QPPWG-SERIOUS MODELS WD QPNet PWG QPPWG - - 30 20 16 af_20 af_16 fa_20 fa_16 MCD (dB) 1×F0 2.58 4.20 3.69 3.74 4.25 3.80 4.18 4.54 4.99 ½×F0 3.89 4.92 4.47 4.39 4.65 4.52 4.89 5.18 5.60 ³∕₂×F0 3.09 4.32 4.23 4.12 4.36 4.21 4.73 5.00 5.44 2×F0 3.79 4.61 5.24 5.06 4.56 4.92 5.42 5.61 5.97 Average 3.34 4.51 4.41 4.33 4.45 4.36 4.80 5.08 5.50 RMSE of log F0 1×F0 0.10 0.14 0.12 0.15 0.41 0.11 0.10 0.11 0.12 ½×F0 0.14 0.23 0.27 0.32 0.42 0.19 0.15 0.20 0.19 ³∕₂×F0 0.10 0.16 0.12 0.12 0.40 0.11 0.10 0.10 0.11 2×F0 0.10 0.18 0.15 0.15 0.73 0.11 0.10 0.11 0.11 Average 0.11 0.18 0.16 0.18 0.49 0.13 0.11 0.13 0.13 U/V decision error (%) 1×F0 10 14 14 15 55 16 18 15 16 ½×F0 15 26 21 22 45 23 22 23 22 ³∕₂×F0 10 15 11 12 63 15 15 13 13 2×F0 11 22 12 17 66 19 14 13 11 Average 11 19 15 16 57 18 17 16 16 Fig. 4.  Speech quality MOS evaluations of male speakers with 95% CI. Fig. 5.  Speech quality MOS evaluations of female speakers with 95% CI. 4.83.11.72.84.32.92.64.12.23.13.72.12.71.31.31.34.12.83.13.31.72.5123451×𝐹₀½×𝐹₀2×𝐹₀MEAN OPINION SCORENaturalWDQPNetPWG_30PWG_20PWG_16QPPWG𝑎𝑓_20QPPWG𝑎𝑓_164.83.92.63.04.03.61.84.53.71.83.93.31.71.21.21.24.13.72.33.33.22.0123451×𝐹₀½×𝐹₀2×𝐹₀MEAN OPINION SCORENaturalWDQPNetPWG_30PWG_20PWG_16QPPWG𝑎𝑓_20QPPWG𝑎𝑓_16JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

11

Fig. 7. Pitch accuracy ABX evaluations with 95 % CI.

outperforms the PWG 30 and QPNet vocoders in the female
2 × F0 scenario, the WD vocoder achieves a higher speech
quality than the QPPWGaf 20 vocoder. A possible reason
for this is that many PDCNNs of the QPPWGaf 20 model
might degenerate to DCNNs because of the values of Et close
to one due to the very high F0 values.

In conclusion,

the proposed QPPWG vocoder with 20
residual blocks attains speech quality competitive with the
PWG vocoder with 30 residual blocks for natural auxiliary
features even though the model size is only 70 % of that of
the PWG model. When conditioned on the auxiliary features
with the unseen F0 values, which are outside the F0 range
of the training data, the proposed QPPWG vocoders achieve
a higher speech quality than the PWG vocoders. The results
conﬁrm the effectiveness of the proposed QP structure for the
PWG model in efﬁciently modeling speech signals and dealing
with unseen F0 features.

2) ABX Evaluation of Pitch Accuracy: To evaluate the
perceptual pitch accuracy, we conducted ABX tests of the
QPPWGaf 20, PWG 30, and QPNet vocoders with the WD-
generated utterances taken as references. Note that because
there were no natural utterances with scaled F0 and the
conventional signal-processing-based vocoder usually attains
accurate pitch controllability,
the WD-generated utterances
were an alternative ground truth. Since the speech quality of
the WD-generated speech is usually worse than the neural-
vocoder-generated-speech, we asked the listeners to focus on
the pitch differences and ignore the speech quality differences.
Because the results of the female and male sets have the
same tendency, only the overall results are shown in Fig. 7.
We ﬁnd that the perceptual pitch accuracy of the proposed
QPPWGaf 20 vocoder is much better than that of the PWG
and QPNet vocoders for both halved and doubled F0 scenar-
ios. To summarize, the ABX results show perceptible pitch
differences between QPPWG- and PWG-/QPNet-generated
utterances, and the ABX experimental results are consistent
with the objective results of the RMSE of log F0

Fig. 8. Comparison of receptive ﬁeld lengths of PWG 30, PWG 20, and
QPPWG 20 for male (M) and female (F) sets.

TABLE VIII
COMPARISON OF RTF OF MODEL INFERENCE

PWG 20

PWG 30

QPPWG 20

Intel Xeon Gold 6142
Nvidia TITAN V

0.474
0.011

0.579
0.016

0.512
0.020

is 6139 (20 + · · · + 29 = 1023 with three cycles and two sides
plus one) and that of PWG 20 is 4093. For the QPPWG,
the effective receptive ﬁeld length is the summation of 2047
for BFix10 × 1 and 124×Et (20 + · · · + 24 = 31 with two
cycles and two sides) for BAda5 × 2. The male F0 range is
around 40–240 Hz and the female F0 range is around 100–
400 Hz, so the Et of the male set is around 20–140 and that
of the female set is around 10-–60 when the dense factor is
set to 4. As shown in Fig. 8, most of the effective receptive
ﬁled lengths of QPPWG 20 for the male set are longer than
the receptive ﬁled length of PWG 30, which may result in
the higher pitch accuracy and comparable speech quality of
QPPWG. The slightly lower speech quality of QPPWG 20
than of PWG 30 for the female set may result from the shorter
effective receptive ﬁelds of QPPWG 20. In conclusion, the
quality of the non-AR-vocoder-generated speech still strongly
depends on the length of the receptive ﬁeld, and QPPWG has
longer effective receptive ﬁelds by skipping some redundant
samples of the periodic components. Although the network
may also lose some details of the aperiodic components owing
to the skipping mechanism, the overall experimental results
still show the effectiveness of the QP structure.

VI. DISCUSSION

A. Effective Receptive Field

Our previous works [33], [34] showed that the capacity of
an AR vocoder is strongly related to the length of its receptive
ﬁeld, and we argue that a non-AR vocoder has a similar
tendency. Speciﬁcally, the receptive ﬁeld length of PWG 30

B. Deformable Dilated Convolution

The idea of a dynamically updated attention mechanism,
which makes a sequential network know “where to look”
at each time step, is not new. Generative models [63]–[65]
that utilize differentiable attention mechanisms to constrain
the read and write operations of the network to speciﬁc parts
of the scene have been proposed. To handle the limitation

> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 8 utterances from the 35 testing utterances for both mean opinion score (MOS) and XAB evaluations. Specifically, the speech quality of each utterance was evaluated by listeners assigning MOSs (1–5). The higher the MOS, the better the speech quality. For each XAB trail, two testing utterances were compared with one reference, and the listeners chose the one whose pitch was more consistent with that of the reference. Eight listeners respectively evaluated partial subjective evaluation set for both MOS and XAB tests, and each utterance/pair was evaluated by at least two listeners. Although the listeners were not native speakers, they worked on audio-related researches. The demo utterances can be found on the demo page [35]. 1)MOS Evaluation of Speech QualityThe MOS evaluation included the WD, QPNet, three different sized PWG, and two different sized QPPWG vocoders. The MOS results shown in Fig. 4 and 5 are presented with the scenarios of three different F0 scaled ratios and two genders. The overall results show that the proposed QP structure improves the speech modeling capacity of the PWG vocoders, especially when the PWG_16 vocoder has a very limited receptive field size. Because the QPPWG vocoders markedly outperform the same sized PWG vocoders for all scenarios in this section, the following discussions focus on comparisons among QPPWGaf_20, PWG_30, and QPNet. For the halved F0 scenario, the QPPWGaf_20 vocoder markedly outperforms the PWG_30 and WD vocoders and attains a similar speech quality as the QPNet vocoder in the male set. In the female set, the QPPWGaf_20 vocoder is comparable to the PWG_30 and QPNet vocoders while still outperforming the WD vocoder. The results indicate that the models with the QP structure are more robustness for the unseen F0, which is totally outside the F0 rage of the training data, such as the most half F0 values in the male set. On the other hand, although the combination of the half F0 and other acoustic features in the female set is still unseen, the scaled F0 values are almost in the F0 range of the training data. Therefore, the PWG_30 vocoder can still achieve a close speech quality as the QPPWGaf_20 vocoder. For the doubled F0 scenario, because the most scaled F0 values are in the F0 range of the training data, the performance of the QPPWGaf_20 vocoder is close to that of the PWG_30 vocoder in the male set. The QPPWGaf_20 also outperforms the WD and QPNet vocoder in the male set while the QPNet achieves a worse speech modeling capacity for the doubled F0 scenario. However, although the QPPWGaf_20 vocoder predictably outperforms the PWG_30 and QPNet vocoders in the doubled female F0 scenario, the WD vocoder achieves a higher speech quality than the QPPWGaf_20 vocoder. The possible reason is that many PDCNNs of the QPPWGaf_20 model might degenerate to DCNNs because of the close to one ET caused by the very high F0 values. In conclusion, the proposed QPPWG vocoder with 20 residual blocks attains a competitive speech quality as the PWG vocoder with 30 residual blocks for the natural auxiliary features while the model size is only 70% of that of the PWG model. When conditioned on the auxiliary features with unseen F0, the proposed QPPWG vocoders achieves a higher speech quality than the PWG vocoders. The results confirm the effectiveness of the proposed QP structure for the PWG model to efficiently model the speech signals and deal with the unseen F0 features.  2)XAB Evaluation of Pitch AccuracyTo evaluate the perceptual pitch accuracy, we conducted the XAB tests of the QPPWGaf_20, PWG_30, and QPNet vocoders while the WD-generated utterances taken as the references. Because the results of female and male sets have the same tendency, only the overall results are shown in Fig. 6. We can find that the perceptual pitch accuracy of the proposed QPPWGaf_20 vocoder is much better than that of the PWG and QPNet vocoders for both halved and doubled F0 scenarios. To sum up, the XAB results show the perceptible pitch differences between QPPWG- and PWG-/QPNet-generated utterances, and the XAB experimental results are consistent with the objective results of the RMSE of log F0.  VI.DISCUSSIONA. Effective Receptive Field Our previous works [18, 19] show that the capacity of AR vocoder is highly related to the length of the receptive field, and we argue that the No-AR vocoder has a similar tendency. Specifically, the receptive field length of PWG_30 is 6139 (20+...+29=1023 with three cycles and two sides plus one) and that of PWG_20 is 4093. For the QPPWG, the effective Fig. 6.  Pitch accuracy XAB evaluations with 95% CI. 27.120.372.979.735.412.064.688.0020406080100½×𝐹₀2×𝐹₀PREFERENCE SCORE(%)PWG_30QPPWG𝑎𝑓_20QPNetFig. 7.  Comparison of receptive field lengths of PWG_30, PWG_20, and QPPWG_20 with male (M) and female (F) sets.        5 H F H S W L Y H  I L H O G  O H Q J W K   î                                                 3 U R E D E L O L W \ 3 : * B          3 : * B          4 3 3 : * B     0  4 3 3 : * B     ) JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

12

PWG 20

(a) 5×BFix

(b) 10×BFix

(c) 15×BFix

(d) 20×BFix

1–5 blocks

1–10 blocks

1–15 blocks

1–20 blocks

QPPWGaf 20

(e) 5×BAda

(f) 10×BAda

(g) 10×BAda + 5×BFix

(h) 10×BAda + 10×BFix

QPPWGf a 20

(i) 5×BFix

(j) 10×BFix

(k) 10×BFix + 5×BAda

(l) 10×BFix + 10×BAda

Fig. 9. Comparison of intermediate cumulative outputs.

of the ﬁxed geometric structure of the CNNs, the authors
of [66] proposed a learnable spatial transformation of the input
feature maps of the CNNs to regularize the input of each CNN
layer. Moreover, the authors of [67] proposed a deformable
convolution to enable the freeform deformation of the CNN
sampling grid. The deformable convolution gives the network
an adaptive receptive ﬁled that focuses on different locations of
the input feature map corresponding to the current conditions.

Since the offsets of the grid sampling locations in PDCNN
are derived from the F0 values, the proposed PDCNN is a
special case of a deformable CNN. As a deformable CNN
with few additional parameters and computations, the PDCNN
is implemented with a simple indexing technique3 without a
large extra computational cost. As shown in Table VIII, the
average real-time factor (RTF) of the QPPWG 20 inferences
is similar to that of PWG 20 and less than that of PWG 30
when running on an Intel Xeon Gold 6142 CPU (2.60 GHz
and 32 threads). However, because of the different indexing
processes of each CNN kernel, the parallelization of the CNN
computation on a GPU is degraded. As shown in Table VIII,
although the model size of QPPWG 20 is only 70 % of that
of PWG 30, the QPPWG 20 model has 170 % of the training
time and 130 % of the inference time of the PWG 30 model
when using an Nvidia TITAN V GPU. However, since the RTF
of the PWG generation is much less than one, the additional
inference time of QPPWG is insigniﬁcant.

3https://github.com/bigpon/QPPWG

C. Understanding of QP Structure

Because of the direct waveform outputs of PWG/QPPWG,
we can easily dissect the models to explore the internal speech
modeling mechanisms. Speciﬁcally, the raw waveform outputs
of the PWG/QPPWG models are the cumulative results of the
skip connection outputs from the residual blocks. Therefore,
the speech modeling behavior of the residual blocks can be
explored via the visualized intermediate outputs of partial
residual blocks. Spectrograms of the intermediate outputs of
the cumulative residual blocks are presented in Fig. 9. For
the PWG vocoder results (Figs. 9 (a)–(d)), the spectrogram
contains more details and textures as the number of cumulative
residual blocks increases. In contrast to the PWG vocoder,
which gradually adds both harmonic and non-harmonic com-
ponents to the spectrogram, the ﬁrst 10 adaptive blocks of the
QPPWGaf vocoder mostly focus on modeling the harmonic
components as shown in Fig. 9 (f). By contrast, the ﬁrst ten
ﬁxed blocks of the QPPWGf a vocoder mostly generate the
non-harmonic part of the speech as shown in Fig. 9 (j). The
results conﬁrm our assumption that the adaptive blocks with
the PDCNNs primarily model the pitch-related speech compo-
nents with long-term correlations, while the ﬁxed blocks with
the DCNNs mainly focus on the spectrum-related components
with short-term correlations.

In addition, to explore the behaviors of the adaptive and
ﬁxed blocks for different scaled F0 features, comparisons
among the visualized cumulative outputs of the ﬁrst 10 residual
blocks from the QPPWGaf and QPPWGf a vocoders are
presented. The spectrograms of QPPWGaf shown in Figs. 10

                7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ] JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

13

QPPWGaf 20

QPPWGf a 20

(a) 1/2 × F0

(d) 1/2 × F0

(b) 1 × F0

(e) 1 × F0

(c) 2 × F0

(f) 2 × F0

Fig. 10. Comparison of intermediate cumulative outputs of 1–10 blocks with
different F0 scaled ratios.

(a)–(c) have similar structures along the time axis but in-
creasingly stretched harmonic structures along the frequency
axis as F0 increases. By contrast, despite the different F0
scaled ratios, both the frequency and temporal structures of
the spectrograms of QPPWGf a shown in Figs. 10 (d)–(f) are
similar. The results imply that the adaptive blocks primarily
model the pitch-dependent harmonic components and the ﬁxed
blocks mainly focus on the pitch-independent non-harmonic
components. Furthermore, although the QPPWG vocoder is a
uniﬁed NN-based waveform generative model, the generative
mechanism of its QP structure is similar to that of a source-
ﬁlter model. The cascaded adaptive (pitch-dependent) and
ﬁxed macroblocks of the QP structure are analogous to the
excitation generation and spectral ﬁltering of the source-
ﬁlter model. In conclusion, because a vocoder is assumed to
have the capability for independently controlling each speech
component, the QPPWG vocoder is more consistent with the
deﬁnition of a vocoder. The QPPWG vocoder with the QP
structure also attains a more tractable and interpretable archi-
tecture. More details of the visualized intermediate outputs can
be found on our demo page [62].

VII. CONCLUSION

To improve the pitch controllability of the PWG vocoder,
we propose a QPPWG vocoder to introduce the prior pitch
information to the network using the QP structure. Using

the proposed non-AR PDCNN, the network architecture is
dynamically adapted to the input F0 feature of each input
sample. Both objective and subjective experimental results
show the effectiveness of the QP structure for the PWG
vocoder. The QPPWG vocoder outperforms the PWG vocoder
in pitch accuracy and speech quality for unseen scaled F0
features while attaining a comparable speech quality to the
PWG vocoder for natural F0 features. Because of the more
efﬁcient receptive ﬁeld expansion by PDCNNs, the model
size of the QPPWG vocoder is only 70 % of that of the
PWG vocoder. Moreover, the visualized intermediate outputs
of QPPWG vocoders conﬁrm our assumption that adaptive
blocks mainly model long-term correlations and ﬁxed blocks
focus on short-term correlations. To summarize, the proposed
QPPWG vocoder is a fast and simple waveform generative
model with higher pitch controllability, smaller model size,
and better interpretability and tractability than vanilla PWG.
The effectiveness of the QPPWG vocoder also indicates the
generality of the QP structure for different CNN-based speech
generative models.

REFERENCES

[1] H. Dudley, “Remaking speech,” The Journal of the Acoustical Society

of America, vol. 11, no. 2, pp. 169–177, 1939.

[2] M. R. Schroeder, “Vocoders: Analysis and synthesis of speech,” Proc.

IEEE, vol. 54, no. 5, pp. 720–734, 1966.

[3] J. L. Flanagan and R. Golden, “Phase vocoder,” Bell System Technical

Journal, vol. 45, no. 9, pp. 1493–1509, 1966.

[4] H. Kawahara, I. Masuda-Katsuse, and A. De Cheveigne, “Restructuring
speech representations using a pitch-adaptive time–frequency smoothing
and an instantaneous-frequency-based F0 extraction: Possible role of a
repetitive structure in sounds,” Speech Communication, vol. 27, no. 3-4,
pp. 187–207, 1999.

[5] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: a vocoder-based
high-quality speech synthesis system for real-time applications,” IEICE
Transactions on Information and Systems, vol. 99, no. 7, pp. 1877–1884,
2016.

[6] R. McAulay and T. Quatieri, “Speech analysis/synthesis based on a
sinusoidal representation,” IEEE Transactions on Acoustics, Speech, and
Signal Processing, vol. 34, no. 4, pp. 744–754, 1986.

[7] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,
A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “WaveNet:
A generative model for raw audio,” in Proc. SSW9, Sept. 2016, p. 125.
[8] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo,
A. Courville, and Y. Bengio, “SampleRNN: An unconditional end-to-
end neural audio generation model,” in Proc. ICLR, Apr. 2017.

[9] Z. Jin, A. Finkelstein, G. J. Mysore, and J. Lu, “FFTNet: A real-time
speaker-dependent neural vocoder,” in Proc. ICASSP, Apr. 2018, pp.
2251–2255.

[10] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury, N. Casagrande,
E. Lockhart, F. Stimberg, A. van den Oord, S. Dieleman, and
K. Kavukcuoglu, “Efﬁcient neural audio synthesis,” in Proc. ICML, July
2018, pp. 2415–2424.

[11] J.-M. Valin and J. Skoglund, “LPCNet: Improving neural speech synthe-
sis through linear prediction,” in Proc. ICASSP, May 2019, pp. 5891–
5895.

[12] A. van den Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals,
K. Kavukcuoglu, G. van den Driessche, E. Lockhart, L. C. Cobo,
F. Stimberg, N. Casagrande, D. Grewe, S. Noury, S. Dieleman, E. Elsen,
N. Kalchbrenner, H. Zen, A. Graves, H. King, T. Walters, D. Belov, and
D. Hassabis, “Parallel WaveNet: Fast high-ﬁdelity speech synthesis,” in
Proc. ICML, July 2018, pp. 3915–3923.

[13] W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave generation in

end-to-end text-to-speech,” in Proc. ICLR, May 2019.

[14] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A ﬂow-based
generative network for speech synthesis,” in Proc. ICASSP, May 2019,
pp. 3617–3621.

[15] S. Kim, S.-G. Lee, J. Song, J. Kim, and S. Yoon, “FloWaveNet : A
generative ﬂow for raw audio,” in Proc. ICML, June 2019, pp. 3370–
3378.

                7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ] JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

14

[16] N.-Q. Wu and Z.-H. Ling, “WaveFFJORD: FFJORD-based vocoder for
statistical parametric speech synthesis,” in Proc. ICASSP, May 2020, pp.
7214–7218.

[17] B. Bollepalli, L. Juvela, and P. Alku, “Generative adversarial network-
based glottal waveform model for statistical parametric speech synthe-
sis,” in Proc. INTERSPEECH, 2017, pp. 3394–3398.

[18] L. Juvela, B. Bollepalli, J. Yamagishi, and P. Alku, “Waveform gen-
eration for text-to-speech synthesis using pitch-synchronous multi-scale
generative adversarial networks,” in Proc. ICASSP, May 2019, pp. 6915–
6919.

[19] L. Juvela, B. Bollepalli, J. Yamagishi, and P. Alku, “GELP: GAN-excited
linear prediction for speech synthesis from mel-spectrogram,” in Proc.
INTERSPEECH, Sept. 2019, pp. 694–698.

[20] Z. Liu, K. Chen, and K. Yu, “Neural homomorphic vocoder,” in Proc.

INTERSPEECH, Oct. 2020, pp. 240–244.

[21] O. McCarthy and Z. Ahmed, “HooliGAN: Robust, high quality neural

[40] K. Kobayashi, T. Hayashi, A. Tamamori, and T. Toda, “Statistical
voice conversion with WaveNet-based waveform generation,” in Proc.
INTERSPEECH, Aug. 2017, pp. 1138–1142.

[41] P. L. Tobing, Y.-C. Wu, T. Hayashi, K. Kobayashi, and T. Toda, “NU
voice conversion system for the Voice Conversion Challenge 2018,” in
Proc. Odyssey, June 2018, pp. 219–226.

[42] Y.-C. Wu, P. L. Tobing, T. Hayashi, K. Kobayashi, and T. Toda, “The
NU non-parallel voice conversion system for the Voice Conversion
Challenge 2018,” in Proc. Odyssey, June 2018, pp. 211–218.

[43] Y.-C. Wu, K. Kobayashi, T. Hayashi, P. L. Tobing, and T. Toda,
“Collapsed speech segment detection and suppression for WaveNet
vocoder,” in Proc. INTERSPEECH, Sept. 2018, pp. 1988–1992.
[44] Y.-C. Wu, P. L. Tobing, K. Kobayashi, T. Hayashi, and T. Toda, “Non-
parallel voice conversion system with WaveNet vocoder and collapsed
speech suppression,” IEEE Access, vol. 8, pp. 62 094–62 106, 2020.
[45] F. Yu and K. Vladlen, “Multi-scale context aggregation by dilated

vocoding,” arXiv preprint arXiv:2008.02493, 2020.

convolutions,” in Proc. ICLR, May 2016.

[22] Y. Ai and Z.-H. Ling, “A neural vocoder with hierarchical generation of
amplitude and phase spectra for statistical parametric speech synthesis,”
IEEE/ACM Transactions on Audio, Speech, and Language Processing,
vol. 28, pp. 839–851, 2020.

[23] R. Yamamoto, E. Song, and J.-M. Kim, “Probability density distillation
with generative adversarial networks for high-quality parallel waveform
generation,” in Proc. INTERSPEECH, Sept. 2019, pp. 699–703.
[24] Q. Tian, X. Wan, and S. Liu, “Generative Adversarial Network based
Speaker Adaptation for High Fidelity WaveNet Vocoder,” in Proc. 10th
ISCA Speech Synthesis Workshop, 2019, pp. 19–23. [Online]. Available:
http://dx.doi.org/10.21437/SSW.2019-4

[25] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A fast
waveform generation model based on generative adversarial networks
with multi-resolution spectrogram,” in Proc. ICASSP, May 2020, pp.
6199–6203.

[26] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh, J. Sotelo,
A. de Br´ebisson, Y. Bengio, and A. C. Courville, “MelGAN: Generative
adversarial networks for conditional waveform synthesis,” in Proc.
NeurIPS, Dec. 2019, pp. 14 910–14 921.

[27] M. Bi´nkowski,

J. Donahue, S. Dieleman, A. Clark, E. Elsen,
N. Casagrande, L. C. Cobo, and K. Simonyan, “High ﬁdelity speech
synthesis with adversarial networks,” in Proc. ICLR, Apr. 2020.
[28] J. Yang, J. Lee, Y. Kim, H. Cho, and I. Kim, “Vocgan: A high-ﬁdelity
real-time vocoder with a hierarchically-nested adversarial network,” in
Proc. INTERSPEECH, 2020, pp. 200–204.

[29] G. Yang, S. Yang, K. Liu, P. Fang, W. Chen, and L. Xie, “Multi-band
MelGAN: Faster waveform generation for high-quality text-to-speech,”
in Proc. SLT, Jan. 2021.

[30] K. Oura, K. Nakamura, K. Hashimoto, Y. Nankaku, and K. Tokuda,
“Deep neural network based real-time speech vocoder with periodic and
aperiodic inputs,” in Proc. SSW10, Sept. 2019, pp. 13–18.

[31] X. Wang, S. Takaki, and J. Yamagishi, “Neural source-ﬁlter-based
waveform model for statistical parametric speech synthesis,” in Proc.
ICASSP, May 2019, pp. 5916–5920.

[32] X. Wang, S. Takaki, and J. Yamagishi, “Neural source-ﬁlter waveform
models for statistical parametric speech synthesis,” IEEE/ACM Transac-
tions on Audio, Speech, and Language Processing, vol. 28, pp. 402–415,
2020.

[33] Y.-C. Wu, T. Hayashi, P. L. Tobing, K. Kobayashi, and T. Toda, “Quasi-
periodic WaveNet vocoder: A pitch dependent dilated convolution model
for parametric speech generation,” in Proc. INTERSPEECH, Sept. 2019,
pp. 196–200.

[34] Y.-C. Wu, T. Hayashi, P. L. Tobing, K. Kobayashi, and T. Toda, “Quasi-
periodic WaveNet: An autoregressive raw waveform generative model
with pitch-dependent dilated convolution neural network,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing, (submitted).
[35] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda,
“Speaker-dependent WaveNet vocoder,” in Proc. INTERSPEECH, Aug.
2017, pp. 1118–1122.

[36] T. Hayashi, A. Tamamori, K. Kobayashi, K. Takeda, and T. Toda, “An
investigation of multi-speaker training for WaveNet vocoder,” in Proc.
ASRU, Dec. 2017, pp. 712–718.

[37] K. Tachibana, T. Toda, Y. Shiga, and H. Kawai, “An investigation of
noise shaping with perceptual weighting for WaveNet-based speech
generation,” in Proc. ICASSP, Apr. 2018, pp. 5664–5668.

[38] N. Adiga, V. Tsiaras, and Y. Stylianou, “On the use of WaveNet as a
statistical vocoder,” in Proc. ICASSP, Apr. 2018, pp. 5674–5678.
[39] Y. Ai, H.-C. Wu, and Z.-H. Ling, “SampleRNN-based neural vocoder
for statistical parametric speech synthesis,” in Proc. ICASSP, Apr. 2018,
pp. 5659–5663.

[46] Y.-C. Wu, T. Hayashi, T. Okamoto, H. Kawai, and T. Toda, “Quasi-
Periodic parallel WaveGAN vocoder: a non-autoregressive pitch depen-
dent dilated convolution model for parametric speech generation,” in
Proc. INTERSPEECH, 2020, pp. 3535–3539.

[47] D. Wong, B.-H. Juang, and A. Gray, “An 800 bit/s vector quantization
LPC vocoder,” IEEE Transactions on Acoustics, Speech, and Signal
Processing, vol. 30, no. 5, pp. 770–780, 1982.

[48] A. V. McCree and T. P. Barnwell, “A mixed excitation LPC vocoder
model for low bit rate speech coding,” IEEE Transactions on Speech
and Audio Processing, vol. 3, no. 4, pp. 242–250, 1995.

[49] S. Imai, “Cepstral analysis synthesis on the mel frequency scale,” in
ICASSP’83. IEEE International Conference on Acoustics, Speech, and
Signal Processing, vol. 8.

IEEE, 1983, pp. 93–96.

[50] T. Fukada, K. Tokuda, T. Kobayashi, and S. Imai, “An adaptive algorithm
for mel-cepstral analysis of speech,” in Proc. ICASSP, vol. 1, 1992, pp.
137–140.

[51] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Proc. NIPS, Dec. 2014, pp. 2672–2680.

[52] J. Engel, L. Hantrakul, C. Gu, and A. Roberts, “DDSP: Differentiable

digital signal processing,” in Proc. ICLR, Apr. 2020.

[53] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectiﬁer nonlinearities
improve neural network acoustic models,” in Proc. ICML, June 2013,
pp. 3–11.

[54] S. Singhal and B. Atal, “Improving performance of multi-pulse LPC
coders at low bit rates,” in ICASSP’84. IEEE International Conference
on Acoustics, Speech, and Signal Processing, vol. 9.
IEEE, 1984, pp.
9–12.

[55] M. Schroeder and B. Atal, “Code-excited linear prediction (CELP):
High-quality speech at very low bit rates,” in Proc. ICASSP, vol. 10,
Apr. 1985, pp. 937–940.

[56] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted boltz-
mann machines,” in Proceedings of the 27th international conference on
machine learning (ICML-10), 2010, pp. 807–814.

[57] J. Kominek and A. W. Black, “The CMU ARCTIC speech databases
for speech synthesis research,” in Tech. Rep. CMU-LTI- 03-177, 2003.
[58] J. Lorenzo-Trueba, J. Yamagishi, T. Toda, D. Saito, F. Villavicencio,
T. Kinnunen, and Z. Ling, “The Voice Conversion Challenge 2018:
Promoting development of parallel and nonparallel methods,” in Proc.
Odyssey, June 2018, pp. 195–202.

[59] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, “On the
variance of the adaptive learning rate and beyond,” in Proc. ICLR, Apr.
2020.

[60] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimiza-

tion,” in Proc. ICLR, May 2015.

[61] M. Morise, “Cheaptrick, a spectral envelope estimator for high-quality
speech synthesis,” Speech Communication, vol. 67, pp. 1–7, 2015.

[62] Y.-C. Wu, QPPWG demo, Accessed: 2020.

[Online]. Available:

https://bigpon.github.io/QuasiPeriodicParallelWaveGAN demo/

[63] A. Graves, “Generating sequences with recurrent neural networks,” arXiv

preprint arXiv:1308.0850, 2013.

[64] A. Graves, G. Wayne, and I. Danihelka, “Neural turing machines,” arXiv

preprint arXiv:1410.5401, 2014.

[65] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra,
“DRAW: A recurrent neural network for image generation,” in Proc.
ICML, July 2015, pp. 1462–1471.

[66] M. Jaderberg, K. Simonyan, A. Zisserman, and K. kavukcuoglu, “Spatial
transformer networks,” in Proc. NIPS, Dec 2015, pp. 2017–2025.
[67] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable
convolutional networks,” in Proc. ICCV, Oct. 2017, pp. 764–773.

JOURNAL OF LATEX CLASS FILES, VOL. 0, NO. 0, JULY 2020

15

Yi-Chiao Wu received his B.S and M.S degrees
in engineering from the School of Communication
Engineering of National Chiao Tung University in
2009 and 2011, respectively. He worked at Realtek,
ASUS, and Academia Sinica for 5 years. Currently,
he is pursuing his Ph.D. degree at the Graduate
School of Informatics, Nagoya University. His re-
search topics focus on speech generation applica-
tions based on machine learning methods, such as
voice conversion and speech enhancement.

Tomoki Toda is a Professor of the Information
Technology Center at Nagoya University, Japan. He
received the B.E. degree from Nagoya University in
1999, and the D.E. degree from the Nara Institute of
Science and Technology (NAIST), Japan, in 2003.
He was a Research Fellow of the Japan Society for
the Promotion of Science from 2003 to 2005. He
was then an Assistant Professor (2005–2011) and
an Associate Professor (2011–2015) at NAIST. His
research interests include statistical approaches to
speech, music, and environmental sound processing.
He received the IEEE SPS 2009 Young Author Best Paper Award and the
2013 EURASIP-ISCA Best Paper Award (Speech Communication Journal).

Tomoki Hayashi received the B.E. degree in en-
gineering and the M.E. and Ph.D. degrees in in-
formation science from Nagoya University, Japan,
in 2014, 2016, and 2019, respectively. His research
interests include statistical speech and audio signal
processing. He is currently working as a postdoctoral
researcher at Nagoya University and the chief oper-
ating ofﬁcer of Human Dataware Lab. Co., Ltd. He
received the IEEE SPS Japan 2020 Young Author
Best Paper Award.

Takuma Okamoto received the B.E., the M.S., and
the Ph.D. degrees from Tohoku University, Japan,
in 2004, 2006, and 2009, respectively. From 2009,
he was a postdoctoral research fellow at Tohoku
University, Japan. During 2012 to 2020, he was a
researcher at the National Institute of Information
and Communications Technology, Japan, and he is
currently a senior researcher there. His main research
ﬁelds are sound ﬁeld synthesis based on acoustic
signal processing and speech synthesis based on
neural networks. He received the 32nd Awaya Prize
Young Researcher Award and the 57th Sato Prize Paper Award from the
Acoustical Society of Japan (ASJ) in 2012 and 2017, respectively. He is a
member of ASJ.

Hisashi Kawai received the B.E., M.E., and D. E.
degrees in electronic engineering from The Univer-
sity of Tokyo, Tokyo, Japan, in 1984, 1986, and
1989, respectively. He joined the Kokusai Denshin
Denwa Co. Ltd., Tokyo, Japan, in 1989. He was with
the ATR Spoken Language Translation Research
Laboratories, Brisbane, QLD, Australia, from 2000
to 2004, where he was engaged in the development
of text-to-speech synthesis system. From October
2004 to March 2009 and from April 2012 to Septem-
ber 2014, he was with the KDDI R&D Laboratories,
Fujimino, Japan, where he was engaged in the research and development of
speech information processing, speech quality control for telephone, speech
signal processing, acoustic signal processing, and communication robots.
From April 2009 to March 2012 and since October 2014, he has been with
the National Institute of Information and Communications Technology, Tokyo,
Japan, where he is engaged in the development of speech technology for
spoken language translation. He is a member of the Institute of Electronics,
Information and Communication Engineers, Tokyo, Japan, the Acoustical
Society of Japan, Tokyo, Japan, and the Institute of Electrical and Electronics
Engineers, Piscataway, NJ, USA. (Based on document published on 5 April
2018).

