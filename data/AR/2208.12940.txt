2
2
0
2

p
e
S
7

]

V
C
.
s
c
[

2
v
0
4
9
2
1
.
8
0
2
2
:
v
i
X
r
a

Actor-identiﬁed Spatiotemporal Action Detection —
Detecting Who Is Doing What in Videos⋆

Fan Yanga,∗, Norimichi Ukitab, Sakriani Saktia,c and Satoshi Nakamuraa,c

aNara Institute of Science and Technology, Japan
bToyota Technological Institute, Nagoya, Japan.
cRIKEN, Center for Advanced Intelligence Project AIP, Japan

A R T I C L E I N F O

Keywords:
Action Recognition
Multiple Object Tracking

A B S T R A C T

The success of deep learning on video Action Recognition (AR) has motivated researchers to progres-
sively promote related tasks from the coarse level to the ﬁne-grained level. Compared with conven-
tional AR that only predicts an action label for the entire video, Temporal Action Detection (TAD) has
been investigated for estimating the start and end time for each action in videos. Taking TAD a step
further, Spatiotemporal Action Detection (SAD) has been studied for localizing the action both spa-
tially and temporally in videos. However, who performs the action, is generally ignored in SAD, while
identifying the actor could also be important. To this end, we propose a novel task, Actor-identiﬁed
Spatiotemporal Action Detection (ASAD), to bridge the gap between SAD and actor identiﬁcation.

In ASAD, we not only detect the spatiotemporal boundary for instance-level action but also as-
sign the unique ID to each actor. To approach ASAD, Multiple Object Tracking (MOT) and Action
Classiﬁcation (AC) are two fundamental elements. By using MOT, the spatiotemporal boundary of
each actor is obtained and assigned to a unique actor identity. By using AC, the action class is esti-
mated within the corresponding spatiotemporal boundary. Since ASAD is a new task, it poses many
new challenges that cannot be addressed by existing methods: i) no dataset is speciﬁcally created for
ASAD, ii) no evaluation metrics are designed for ASAD, iii) current MOT performance is the bottle-
neck to obtain satisfactory ASAD results. To address those problems, we contribute to i) annotate a
new ASAD dataset, ii) propose ASAD evaluation metrics by considering multi-label actions and ac-
tor identiﬁcation, iii) improve the data association strategies in MOT to boost the MOT performance,
which leads to better ASAD results. We believe considering actor identiﬁcation with spatiotemporal
action detection could promote the research on video understanding and beyond. The code is available
at https://github.com/fandulu/ASAD.

1. Introduction

Vision-based Action Recognition (AR) aims to detect
human-deﬁned actions from a sequence of data (e.g., videos)
and has a wide range of applications in our daily life. For in-
stance, it has been applied for YouTube to recognize billions
of video tags before recommending a video to us, or for the
policemen to quickly retrieval a criminal from thousands-
hours surveillance videos, or for a virtual game machine to
interact with players, and many others ( [10, 33]).

In recent years, the success of deep learning on AR has
motivated researchers to progressively promote the AR task
from the coarse level to the ﬁne-grained level. Compared
with conventional AR that only predicts an action label for
the entire video, Temporal Action Detection (TAD) has been
investigated for estimating the start and end time for each
action in videos. Taking TAD a step further, Spatiotempo-
ral Action Detection (SAD) has been studied for localizing
the action both spatially and temporally in videos. However,
who performs the action is generally ignored in SAD studies.
We believe actor identiﬁcation should be considered to-
gether with SAD. When multiple actors are involved in the
target scenes (e.g., basketball/soccer games), it is preferred
to know “who is doing what”, and thus, identifying each

∗Corresponding author

hongheyangfan@gmail.com (F. Yang)
ORCID(s): 0000-0001-7185-5688 (F. Yang)

Figure 1: Actor-identiﬁed Spatiotemporal Action Detection
(ASAD) is Spatiotemporal Action Detection (SAD) pluses ac-
tor identiﬁcation.

actor with their actions is desired. Nonetheless, SAD and
actor identiﬁcation are treated as diﬀerent tasks for a long
time. To this end, we propose a novel task, Actor-identiﬁed
Spatiotemporal Action Detection (ASAD), to bridge the gap
between SAD and actor identiﬁcation (Figure 1).

To approach ASAD, Multiple Object Tracking (MOT) [49]

and Action Classiﬁcation (AC) [33] are two fundamental el-
ements (Figure 2). By using MOT, the spatiotemporal bound-
ary of each actor is obtained and assigned to a unique actor
identity. By using AC, the action class is estimated within
the corresponding spatiotemporal boundary. In general, they
may work as independent modules by considering the model

F.Yang et al.: Preprint submitted to Elsevier

Page 1 of 13

Example of Rough Action Recognition in Videosare playing basketballis dribblingis defendingExample of  Spatiotemporal Action Detection (SAD) in Videos is dribblingis defendingExample of Actor-identifiedSpatiotemporal Action Detection (ASAD) in Videos Actor ID 0Actor ID 1 
 
 
 
 
 
ASAD

training ﬂexibility.

Figure 2: The illustration of ASAD processing.

Since ASAD is a new task, it poses many new challenges
that cannot be addressed by existing methods: i) no dataset
is speciﬁcally created for ASAD, ii) no evaluation metrics
are designed for ASAD, iii) current MOT performance could
be the bottleneck to obtain satisfactory ASAD results. To
address those problems, we contribute to i) annotate a new
ASAD dataset, ii) propose ASAD evaluation metrics by con-
sidering multi-label actions and actor identiﬁcation, iii) im-
prove the data association strategies in MOT to boost the
MOT performance, which leads to better ASAD results 1.
We summarize the main contributions as follows.

• We raise a new study task of video action recognition
— Actor-identiﬁed Spatiotemporal Action Detection
(ASAD). As far as we are aware, it has great impor-
tance but has been historically overlooked. ASAD
bridges the gap between the existing Spatiotemporal
Action Detection (SAD) study and the new demand
for identifying actors.

• We speciﬁcally provided a novel dataset for the ASAD
study. It covers a rich action category and actor iden-
tities.

• We presented novel metrics for ASAD evaluation. To
the best of our knowledge, existing metrics cannot be
applied to ASAD, and we are the ﬁrst to introduce such
metrics.

2. Related Works
3. Video Action Recognition

In general, video action recognition research can be di-
vided into several categories (Figure 3). Normal Action Recog-
nition (AR) takes an entire video, or, a video clip, as the in-
put and generates a corresponding action class. It is used
to understand the overall video concept without specifying
the details in the spatial domain and temporal domain. Tem-
poral Action Detection (TAD) gives temporal details to AR,

1We have organized our proposed dataset and evaluation metrics scripts

at GitHub, it will be released.

by clarifying the start and end times of an action. Accord-
ingly, one video could be segmented into several temporal
components in TAD. Compared with TAD, Spatiotemporal
Action Detection (SAD) not only detects the action bound-
ary in the temporal domain but also locates the actor with
bounding boxes (or instance masks) in the spatial domain.
We generally call such a spatiotemporal boundary the action
tube. In this work, we propose Actor-identiﬁed Spatiotem-
poral Action Detection (ASAD) from SAD, by incorporating
the unique identity of each actor.

We summarize the related datasets and studies for AR,
TAD, SAD, and ASAD in Table 1. To link bounding boxes
to action tubes, Multiple Object Tracking (MOT) [82] is also
commonly applied in SAD. Some SAD works can also track
the actor and assign them with unique IDs. However, based
on the evaluation protocol of SAD, the annotation of actor
identity may not be provided and the actor identiﬁcation has
not been evaluated. That means, there is no clear bound-
ary between ASAD and SAD in terms of the method, their
diﬀerence lies more in the data annotation and evaluation
protocols. In detail, the action tube ID given in SAD may
not be consistent with actor ID. For example, after the same
actor changes his/her action, the corresponding action tube
ID changed but the actor ID should remain the same. Unfor-
tunately, such kind of actor ID is not available in most SAD
datasets.

As we suppose that MOT and AC are two important com-
ponents in ASAD, we take a look into the role of MOT and
AC in AR, TAD, SAD, and ASAD (see Table 3). The AC
could be a necessary module for all action recognition cate-
gories. In SAD, MOT might be used (e.g., on UCF101 +
ROAD dataset [63]), but not be necessary (e.g., on AVA
dataset [26]). However, both MOT and AC are needed in
ASAD.

In addition, previous studies [60, 79, 32] focus on only
identifying actors in videos, but without detecting their ac-
tions. In this manner, as a new task, ASAD has bridged the
gap between the SAD and the actor identiﬁcation (Table 2).

4. Multiple Object Tracking

Since Multiple Object Tracking (MOT) plays an impor-
tant role in Actor-identiﬁed Spatiotemporal Action Detec-
tion (ASAD), we further provide an overview of MOT-related
works.

Generally, MOT is applied to connect identical obser-
vations into tracklets based on the similarity of MOT fea-
tures. Speciﬁcally, MOT methods can be divided into two
categories — online MOT and oﬄine MOT (Table 4.). The
online data association is performed on observations that are
available up to the current frame. Diﬀerent from online ap-
proaches, oﬄine data association takes global observations
into consideration, which may not be applicable for real-time
applications but be ideal for assisting annotation works. Nu-
merous oﬄine approaches have been proposed in previous
studies [49]. Among them, formulating MOT data associa-
tion as a global clustering problem has achieved great suc-

F.Yang et al.: Preprint submitted to Elsevier

Page 2 of 13

is dribblingis defendingMultiple Object Tracking (MOT)Action Classification (AC)Actor ID 0Actor ID 1ASAD

Figure 3: A comparison of action recognition works, which could be roughly divided into
four categories: Action Recognition (AR), Temporal Action Detection (TAD), Spatiotem-
poral Action Detection (SAD), and our deﬁned Actor-identiﬁed Spatiotemporal Action
Detection (ASAD). Existing works (i.e., AR, TAD, and SAD) ignore to identify actors
while our ASAD addresses this issue. Parts of this graph credit to [33].

Action Recognition Category

Available Datasets

Related Works

AR

TAD

SAD

ASAD

HMDB [38], UCF101 [65],Sports-1M [37], Kinetics-700 [6]

[52, 47, 28, 62, 68, 18, 48, 87, 17, 27]

ActivityNet [16], YouTube-8M [1],THUMOS [34], HACS [91]

[64, 70, 78, 21, 9, 45]

UCF101+ROAD [63], DALY [74], Hollywood2Tubes [53] AVA [26],
AVA-Kinetics [39], ActEV [86]

[24, 73, 93, 31, 63, 36, 83]
[23, 85, 69, 55, 44, 43, 66]

Okutama [2] (available but not ideal)

Ours

Table 1
The related datasets and studies for AR, TAD, SAD, and ASAD. Note that, unlike other
SAD datasets, actor ID is given in annotations of Okutama [2] but the ASAD evaluation
has not been explored. Besides, the Okutama dataset consists of 4K-resolution drone
videos, which may only cover very limited scenarios of ASAD. In addition, some SAD
models, such as ROAD [63], AlphAction [66], and ACAM [69], may potentially generate
ASAD results but were evaluated by the SAD protocol in the original works. That is,
the consistency of actor identity is ignored in these works.

cesses [81]

5. Action Classiﬁcation

The Action Classiﬁcation (AC) model plays such a role
to map the spatiotemporal information to action categories.
There are numerous AC studies considering the approaches
of utilizing features and designing the model structure. In
detail, Action Classiﬁcation (AC) approaches could be di-
vided into 5 categories, including RGB AC, RGB + Flow
AC, Pose AC, RGB + Pose AC, and RGB + Flow + Pose
AC, as shown in Figure 4. Based on these 5 categories, we
list the corresponding studies in Table 5.

6. Proposed ASAD Dataset and Evaluation

Metrics
Given a video, Actor-identiﬁed Spatiotemporal Action
Detection (ASAD) aims to detect the spatiotemporal bound-
aries (i.e., tracklets/actor tubes) for each actor, assign each
actor a unique identity, and obtain the actions of actors at
each moment. Consequently, the ASAD dataset should in-
clude those factors and the ASAD metrics should verify the
performance on those factors.

6.1. Dataset for ASAD

By reviewing existing action recognition datasets (Sec-
tion 2), it can be noticed that a proper ASAD dataset may

F.Yang et al.: Preprint submitted to Elsevier

Page 3 of 13

Action Tube 0(Action Class 0)Action Tube 1(Action Class 1)Spatiotemporal Action Detection (SAD)Fame 1Fame 2Fame N-1Fame NHeight……Action Tube 2(Action Class 0)Actor-identified Spatiotemporal Action Detection (ASAD)Fame 1Fame 2Fame N-1Fame NHeight……Actor Tube 0(Actor ID 0)Actor Tube 1(Actor ID 1)Action Tube 0(Action Class 0)Action Tube 1(Action Class 1)Action Tube 2(Action Class 0)No DefinedAction (keep)Action Recognition (AR)Fame 1Fame 2Fame N-1Fame NHeight……Action Class 0Temporal Action Detection (TAD)Fame 1Fame 2Fame N-1Fame NHeight……Action Class 0Action Class 1Start 0Start 1End 0End 1No DefinedAction (remove)ASAD

Approaches

Identifying Actors Detecting Actions

Spatiotemporal Action Detection (SAD)
[24, 73, 93, 31, 63, 36, 83, 23, 55, 44, 43, 66]

Actor Identiﬁcation [60, 79, 32]

Actor-identiﬁed Spatiotemporal Action Detection (ASAD)

Table 2
A comparison of SAD, Actor Identiﬁcation, and ASAD.

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

Action Recognition Category

Using MOT Using AC

Action Recognition (AR)

Temporal Action Detection (TAD)

Not Need

Not Need

Need

Need

Spatiotemporal Action Detection (SAD)

May Not Need

Need

Actor-identiﬁed Spatiotemporal Action Detection (ASAD)

Need

Need

Table 3
The role of MOT and AC in AR, TAD, SAD, and ASAD. For some evaluation protocols
of SAD, there is no need to link detection to tubes and MOT may not be used.

not be available. Although the existing Spatiotemporal Ac-
tion Detection (SAD) dataset might be similar to our de-
sired ASAD dataset, the actor identity is not properly an-
notated in the SAD dataset. We illustrate the annotation dif-
ference between SAD and ASAD data annotation by using
UCF101+ROAD dataset [63] and AVA dataset [26] as ex-
amples (Figures 5 and 6). In the UCF101+ROAD dataset,
the spatiotemporal boundaries are incomplete. Since actor
identiﬁcation is not the concern in SAD, after the predeﬁned

action is ﬁnished, spatiotemporal annotation is not available.
In contrast, the annotation in ASAD should complete the
spatiotemporal boundary for each actor in the entire video,
no matter if the deﬁned action is ﬁnished or not. In the AVA
dataset, despite the actor IDs being given, multiple actor IDs
have been assigned to the same actor in a single video, which
is incorrect for actor identiﬁcation. For actor identiﬁcation
purposes, the unique actor ID should be assigned to each
actor in one piece of video. Besides, while some remote

Approaches

[57, 3, 77, 4, 13, 42, 20, 35, 50, 72, 41, 5, 76, 92, 90, 19, 8]

[88, 75, 89, 67, 59, 51, 29]

Online Data
Association

Oﬄine Data
Association

(cid:51)

(cid:55)

(cid:55)

(cid:51)

Table 4
Online and oﬄine MOT methods.

Figure 4: Categories of Action Classiﬁcation (AC) models.

F.Yang et al.: Preprint submitted to Elsevier

Page 4 of 13

RGB VideoPose/SkeletonOptical FlowOptical FlowEstimationPose EstimationRGB-basedAction ClassificationTwo-stream (RGB + Flow)Action ClassificationRGB + PoseAction ClassificationPose/Skeleton Based Action ClassificationThree-stream (RGB + Flow + Pose) Based Action ClassificationApproaches

RGB AC RGB + Flow AC Pose AC RGB + Pose AC RGB + Flow + Pose AC

ASAD

Large-scale AC [37]

Two-Stream [62]

C3D [68], I3D [7], ECO [95],
P3D [56], FastSlow [17]

HCN [40], 2s-AGCN [61],
DD-Net [84]

Potion [11], PA3D [80]

Chained AC [94]

(cid:51)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

Table 5
The properties of action classiﬁcation works

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

surveillance video datasets are equipped with spatiotempo-
ral boundaries, actor identities, and acting classes, they fo-
cus on the special scene (e.g., remote surveillance) and may
not be suitable for the general ASAD study. For example,
Okutama dataset [2] and PANDA [71] only record tiny scale
actors and cover a small group of human daily activities.

Figure 6: Comparison between SAD and ASAD actor ID an-
notation by using AVA [26] as an example. In a single video,
while the existing SAD dataset may assign multiple actor IDs
to the same actor, our ASAD assigns the unique actor ID the
actor.

More examples are illustrated in Figure 8.

We present the historical role of our A-AVA dataset in
Figure 9. As the ﬁrst dataset that is speciﬁcally designed for
the ASAD study, the A-AVA dataset covers a rich diversity
of video scenes, as indoor and outdoor, diﬀerent times of the
day, various actor scales, and more. Those properties are not
available in the previous dataset (i.e., Okutama dataset). A-
AVA dataset has bridged the gap between the SAD dataset
and actor identiﬁcation dataset.

6.2. Evaluation Metrics for ASAD

When considering the multi-label action, ASAD evalu-
ation could be a complicated task. Unlike single-label SAD
evaluation [25], it is challenging to simultaneously evaluate
multi-label action classiﬁcation and actor identiﬁcation with
spatial detection. To address this issue, we suggest evaluat-
ing ASAD from three aspects and then consider their overall
performance. The three aspects include Spatial Detection
Evaluation, Actor Identiﬁcation Evaluation, and Multi-label
Action Classiﬁcation Evaluation (Figure 10).

6.3. Spatial Detection Evaluation

We take the object detection metrics [15, 46, 30] to eval-
uate the spatial detection performance. First, we calculate

Figure 5: Comparison between SAD and ASAD spatiotempo-
ral annotation by using UCF101+ROAD [63] as an example.
The annotation in ASAD should complete the spatiotemporal
boundary for each actor in the entire video, no matter if the
deﬁned action is ﬁnished or not.

Due to the above reasons, we are motivated to anno-
tate a new ASAD dataset. Compared with the SAD dataset,
the ASAD dataset requires to add correct actor identities.
As the AVA dataset [26] is a canonical SAD dataset and
TAO dataset [14] oﬀers some actor identity annotations on
it, we select a part of the AVA dataset to make an ASAD
dataset (Figure 7). Note that, we mainly selected video clips
that have visible actors and multiple actors available. Mean-
while, due to the heavy annotation cost, only 77 video clips
are selected among 430 AVA video clips. We named our
ASAD dataset A-AVA, which represents the Actor-identiﬁed
AVA dataset. A-AVA dataset contains 47 videos for training
and 30 videos for testing. Be the same as the AVA dataset,
there are 80 action categories in the A-AVA dataset, and, ev-
ery 25 frames (i.e., around 1 second), the annotation is given
once. In the A-AVA dataset, the spatiotemporal boundaries,
actor identities, and corresponding actions are all annotated.

F.Yang et al.: Preprint submitted to Elsevier

Page 5 of 13

Original spatiotemporal annotations in UCF-ROAD dataset (SAD), e.g., fencing     .The spatiotemporal annotationsare incomplete.Expected spatiotemporal annotations for our ASAD, e.g., fencing     .The spatiotemporal annotations are complete.Actor 1Actor 2Actor 3Actor 4Actor 1Actor 2Actor 3Actor 4Actor ID annotation in the original AVA dataset (SAD). Actor IDs are fragmented in one video.Expected actor ID annotation in our ASAD. Actor IDs are consistent in one video.Actor 2Actor 1Actor 3Actor 2Actor 1Actor 3Actor 4Actor 5Actor 6Actor 7ASAD

Figure 7: We create a new ASAD dataset based on existing AVA dataset [25], by assigning
the unique actor identity to each actor.

Figure 8: Illustration of our Actor-identiﬁed AVA dataset.

Intersection over Union (IoU), which is deﬁned by

• False Positive (FP): A wrong detection with an IoU

𝐼𝑜𝑈 =

𝑏𝑏𝑜𝑥𝑝𝑟𝑒𝑑 ∩ 𝑏𝑏𝑜𝑥𝑡𝑟𝑢𝑒
𝑏𝑏𝑜𝑥𝑝𝑟𝑒𝑑 ∪ 𝑏𝑏𝑜𝑥𝑡𝑟𝑢𝑒

where 𝑏𝑏𝑜𝑥𝑝𝑟𝑒𝑑 and 𝑏𝑏𝑜𝑥𝑡𝑟𝑢𝑒 represent the predicted bound-
ing box and the ground-truth box, respectively.

Second, based on the IoU value, True Positive (TP), False

Positive (FP), and False Negative (FN) are deﬁned by

smaller than the threshold.

(1)

• False Negative (FN): A ground truth not detected.

and the corresponding Precision and Recall are

𝑃 𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =

𝑅𝑒𝑐𝑎𝑙𝑙 =

𝑇 𝑃
𝑇 𝑃 + 𝐹 𝑃
𝑇 𝑃
𝑇 𝑃 + 𝐹 𝑁

(2)

• True Positive (TP): A correct detection with an IoU

greater the threshold.

By traversing through all thresholds for detection con-
ﬁdence, diﬀerent pairs of precision and recall can generate

F.Yang et al.: Preprint submitted to Elsevier

Page 6 of 13

Training Set: 47 videos(part of AVA dataset)Testing Set: 30 videos (part of AVA dataset)ActionsCategory: 80 actions (the same as AVA dataset)Annotation Frequency: Every 25 frames (the same as AVA dataset)ASAD

Figure 9: A historical timeline overview of datasets intended for video action recognition
studies.

Figure 10: Overview of our ASAD metrics, which evaluate the performance of spatial
detection, action classiﬁcation, and actor identiﬁcation.

the precision-recall curve, which indicates the association
between precision and recall. To reduce the eﬀect of the
wiggles in the curve, the precision-recall curve is interpo-
lated as 𝑝𝑖𝑛𝑡𝑒𝑟𝑝
at recall score 𝑟 is assigned with
the highest precision for 𝑟 > 𝑟′ :

. The 𝑝𝑖𝑛𝑡𝑒𝑟𝑝

𝑝𝑖𝑛𝑡𝑒𝑟𝑝(𝑟) = max
𝑟>𝑟′

𝑝(𝑟

′

)

(3)

Since we only treat humans as the actor, there is only
one class for the detection, and therefore we utilize Average
Precision (AP), other than Mean Average Precision (mAP),
for the spatial detection evaluation. AP is the area under the
interpolated precision-recall curve, which can be calculated
using the following formula:

In this thesis, we assume any spatial detection with an
IoU value larger than 0.5 is True Positive, and the corre-
sponding metrics are represented as AP@0.5.

6.4. Actor Identiﬁcation Evaluation

While actor classiﬁcation has the pre-deﬁned actor iden-
tities, actor identiﬁcation assigns each actor a unique identity
and the number of actor identities is non-parametric. There-
fore, we utilized part of Multiple Object Tracking (MOT)
evaluation metrics for actor identiﬁcation evaluation, as IDF1
(ratio of correctly identiﬁed detections), MT (mostly tracked
targets), ML (mostly lost targets), and ID Switches. Those
identiﬁcation metrics were introduced by [58, 54] and have
been popularly utilized for a while. More speciﬁcally, the
IDF1, MT, and ML are respectively deﬁned by

𝐴𝑃 =

𝑛−1
∑

𝑖=1

(𝑟𝑖+1 − 𝑟𝑖)𝑝𝑖𝑛𝑡𝑒𝑟𝑝(𝑟𝑖+1)

(4)

𝐼𝐷𝐹 1 =

2𝐼𝐷𝑇 𝑃
2𝐼𝐷𝑇 𝑃 + 𝐼𝐷𝐹 𝑃 + 𝐼𝐷𝐹 𝑁

(5)

F.Yang et al.: Preprint submitted to Elsevier

Page 7 of 13

Action Recognition Related Data Release Time20122014201620182020Spatiotemporal Action Detection (SAD)Actor-identified Spatiotemporal Action Detection (ASAD)Action Recognition (AR)Temporal Action Detection (TAD)DALY [Weinzaepfelet al. ]UCF101+ROAD [Singh et al. ]AVA [Guet al. ]ActEV[Yooyounget al. ]Actor-identified AVA[Ours](originally designed for ASAD,small-to-largesize actor,>100 actors,movie videos,80 actions)AVA-Kinetics[Li et al.]Okutama[Barekatainet al. ](originally designed for SAD,small size actor,10 actors, drone videos,12 actions)MBDB[Kuehneet al.] UCF101[Soomroet al.] Sport-1M[Karpathyet al.] Kinetics-700[Carreiraet al.] ActivityNet[Heilbronet al.]YouTube-8M [Abu-El-Haijaet al.]THUMOS[Idreeset al.]HACS[Zhao et al.]Spatial Detection EvaluationDetections within the acceptable error rangespassMulti-label Action Classification EvaluationActor Identification EvaluationYesNoFailuresASAD

Figure 11: Illustration of matching pair between the ground-truth and the predicted sam-
ples.

where IDTP, IDFP, IDFN respectively represent the True
Positive ID, the False Positive ID, the False Negative ID.

𝑀𝑇 =

𝑀𝐿 =

∑

𝑖∈𝑁𝑡𝑟𝑢𝑒

∑

𝑖∈𝑁𝑡𝑟𝑢𝑒

𝟙{

𝟙{

)

𝑙𝑒𝑛( 𝑝𝑟𝑒𝑑
𝑖
𝑙𝑒𝑛( 𝑡𝑟𝑢𝑒
)
𝑖
𝑙𝑒𝑛( 𝑝𝑟𝑒𝑑
𝑖
𝑙𝑒𝑛( 𝑡𝑟𝑢𝑒
)
𝑖

)

⩾ 0.8}

⩽ 0.2}

In detail, we utilize matrix 

to represent the matching
distance between each ground-truth bounding box (denoted
by 𝑖) and predicted bounding box (denoted by 𝑖), and we ob-
tain 

by

𝑖,𝑗

𝑖,𝑗

(6)

{



𝑖,𝑗 =

1,
1 − 𝐼𝑜𝑈𝑖,𝑗,

if 𝐼𝑜𝑈𝑖,𝑗 < 0.5;
otherwise.

(7)

𝑖

and  𝑡𝑟𝑢𝑒
𝑖

where  𝑝𝑟𝑒𝑑
respectively denote the predicted and
the ground-truth Tracklet 𝑖, the number of  𝑡𝑟𝑢𝑒
. If
the prediction matches for the ground truth more than 80%
of its life span, it is regarded as mostly tacked (MT). If the
prediction only matches for the ground truth less than 20%
of total length, it is regarded as mostly lost (ML).

is 𝑁𝑡𝑟𝑢𝑒

𝑖

6.5. Multi-label Action Classiﬁcation Evaluation

It is intuitive to consider that each actor could take sev-
eral actions simultaneously, which are corresponding to multi-
label actions. For instance, an actor could be making a phone
call and walking at the same time. Due to the lack of evalu-
ation metrics, conventional Action Recognition studies have
been evaluated with only the single-label action for a while [2,
25]. Therefore, we provide metrics for multi-label ASAD
evaluation, which considers the evaluations of multi-label
multi-class action classiﬁcation and actor identiﬁcation.

The evaluation metrics for actor detection and multi-label
classiﬁcation have been well-studied separately [15, 22], but
the problem remains on how to associate them together for
multi-label ASAD evaluation.

A simple approach could be evaluating the “actor” ac-
tor detection performance for all detected samples and then
evaluating the multi-label action recognition performance
for positively detected samples. For instance, assuming that
a predicted sample is positive when IoU ≥ 0.5 for the pre-
dicted and ground-truth bounding boxes, we can apply 𝐻𝐿@0.5,
which corresponds to Hamming Loss associated with IoU
≥ 0.5, to measure its multi-label classiﬁcation performance.
Note that, due to the object occlusions, the IoU value
between multiple actors could be larger than 0.5. To re-
move such ambiguity, we apply the Hungarian Algorithm
for bipartite matching between the predicted bounding boxes
and the ground-truth bounding boxes before comparing their
classiﬁcation results. Meanwhile, a pair that has IoU < 0.5
will be excluded before calculating their Hamming Loss. We
illustrate these cases in Figure 11.

Next, we employ linear assignment [12] to obtain the op-

timal assignment ∗ with

∗ = arg min



∑

∑

𝑖

𝑗



𝑖,𝑗



𝑖,𝑗,

(8)

where  is a Boolean matrix. When the row 𝑖 (i.e., ground
truth box 𝑖) is assigned to column 𝑗 (i.e., predicted box 𝑗),
we have 
𝑖,𝑗 = 1. Each row can be assigned to at most one
column and each column to at most one row.

Since matching pairs that have IoU value less than 0.5,

we further process ∗ by

∗

𝑖,𝑗 =

{ 0,
∗

𝑖,𝑗,

if 
𝑖,𝑗 = 1;
otherwise.

(9)

Referring to ∗, we select matched pairs to evaluate the
corresponding action labels with Hamming Loss. The num-
(i.e.,
ber of matching pairs are represented by 𝑁𝑎𝑐𝑡𝑜𝑟𝑠@0.5
∗ = 1). Below, we show how the 𝐻𝐿@0.5 is extended
from the original Hamming Loss.

𝐻𝐿@0.5 =

1
𝑁𝑎𝑐𝑡𝑜𝑟𝑠@0.5

1
𝑁𝑙𝑎𝑏𝑒𝑙𝑠

𝑁𝑎𝑐𝑡𝑜𝑟@0.5∑

𝑁𝑙𝑎𝑏𝑒𝑙𝑠∑

𝑖=1

𝑙=1

𝑡𝑟𝑢𝑒 XOR 𝑌 𝑖,𝑙
𝑌 𝑖,𝑙

𝑝𝑟𝑒𝑑 ,

(10)

where XOR is an exclusive-or operation and 𝑁𝑙𝑎𝑏𝑒𝑙𝑠
for the number of action categories. 𝑌𝑡𝑟𝑢𝑒
arrays that denote the ground truth and predicted labels, re-
spectively.

stands
are boolean

and 𝑌𝑝𝑟𝑒𝑑

7. Experiment

We set up the ﬁrst benchmark for the ASAD study. Ex-
periments are conducted on our A-AVA dataset and evalu-
ated by our ASAD metrics.

7.1. ASAD Framework

As we have discussed in related works, some SAD mod-
els, such as ROAD [63], AlphAction [66], and ACAM [69],

F.Yang et al.: Preprint submitted to Elsevier

Page 8 of 13

IoU= 0.52IoU= 0.15IoU= 0IoU= 0Bounding Boxes:Matched Ground Truth [Green]Ignored Ground Truth [Grey]Matched Prediction [Red]Ignored Prediction [Red]IoU= 0.7IoU= 0.5IoU= 0.6IoU= 0.5ASAD

Figure 12: Overview of the basic ASAD framework.

Approaches

ASAD Baseline
w/ online MOT
ASAD Baseline
w/ oﬄine MOT

Actor Detection Evaluation Action Classiﬁcation Evaluation

Actor Identiﬁcation Evaluation

AP@0.5 (%)↑

HL@0.5 (0∼1)↓

IDF1 (%) ↑ MT (%)↑ ML (%)↓ # ID Sw.↓

72.4

72.4

0.06

0.06

60.4

71.4

67.3

88.4

10.5

5.2

413

273

Table 6
Comparison of using diﬀerent MOT modules in ADAD frameworks. We utilize our A-AVA
dataset and ASAD evaluation metrics, where ↑(↓) indicates that the larger(smaller) the
value is, the better the performance.

are consist of Multiple Object Tracking (MOT) and Action
Classiﬁcation (AC) modules. They could generate ASAD
results but were evaluated by the SAD protocol in the origi-
nal works. Based on the evaluation protocol of SAD, the an-
notation of actor identity may not be provided and the actor
identiﬁcation has not been evaluated. In other words, there
is no clear boundary between ASAD and SAD in terms of
the method, their diﬀerence more lies in the data annotation
and evaluation protocols.

Without changing the basic structure, letting the above
SAD methods to output actor identities with their original
outputs can make ASAD frameworks. In this study, we let
the oﬀ-the-shelf SAD methods to output actor identities that
are generated by their MOT module. In this manner, they can
perform as ASAD frameworks. In Figure 12, we summarize
the basic structure of ASAD framework that is adapted from
SAD models. Generally, an ASAD framework takes RGB
videos as the input and outputs the bounding boxes, unique
actor identity, and actions of each actor.

7.2. Experiment Results

By using our A-AVA dataset, the performance of spa-
tiotemporal detection, action classiﬁcation, and actor identi-
ﬁcation can be jointly evaluated. To generate a better actor
identiﬁcation result, we may need to focus on the data asso-
ciation strategy in MOT. We ﬁrst evaluated our ASAD base-
line with online MOT methods [77]. We showed the result
of using our ASAD evaluation metrics in Table 6. It can be
noticed that the action identiﬁcation performance is unsat-
isfactory. Using online MOT methods becomes the bottle-
neck to obtain satisfactory ASAD results. We then replaced
the online MOT module to be an oﬄine MOT method intro-
duced in [82]. Consequently, using an oﬄine MOT module
led to better actor identiﬁcation result: the oﬄine MOT mod-
ule gave a further gain in IDF1 and ML over the online MOT
module, and also reduced the ML and ID Switches.

7.3. Discussion

Why does oﬄine MOT have better performance in our
A-AVA dataset in terms of actor identiﬁcation? For static
camera recording, motion consistency is an important cue
for data association. In contrast, for non-static camera record-
ing, the motion consistency assumption could be failed. When-
ever the viewpoint suddenly changes in videos, it is chal-
lenging to track the correct actor identities. This issue fre-
quently happens in the movies and phone-recorded videos
(Figure 13). Most online MOT methods (e.g., [77]) may
have an over-reliance on the motion consistency and there-
fore cause failure cases in our A-AVA dataset. Employing
oﬄine MOT alleviates this issue by determining the corre-
spondence between observations more by their appearance
similarity. Moreover, applying an oﬄine MOT solution uti-
lizes the global information to further reduce ID switches
and generate robust actor identiﬁcation results.

8. Conclusion

In this paper, we introduced a novel task, Actor-identiﬁed
Spatiotemporal Action Detection (ASAD), which marks the
ﬁrst eﬀort in the computer vision community to jointly study
spatiotemporal boundaries, actor identities, and correspond-
ing actions. ASAD is ideal for action recognition applica-
tions when multiple actors are included, such as Human-
computer Interaction, basketball/soccer games, and grocery
operations monitoring, etc. To study ASAD, we are excited
to oﬀer a corresponding A-AVA dataset. As the ﬁrst dataset
that is speciﬁcally designed for the ASAD study, the A-AVA
dataset has bridged the gap between the SAD dataset and the
actor identiﬁcation dataset. We also proposed ASAD eval-
uation metrics by considering multi-label actions and actor
identiﬁcation. It is the ﬁrst evaluation metric in ASAD such
a complicated task.

Besides the above success, it is important to note that our

ASAD study also suﬀers some limitations:

F.Yang et al.: Preprint submitted to Elsevier

Page 9 of 13

Object DetectionData AssociationMultiple Object TrackingAction ClassificationVideosASAD Results(Spatiotemporal Boundaries,Actions,Actor IDs.)ASAD

Figure 13: The diﬀerence of motion consistency in static camera recording videos and
non-static camera recording videos.

• Considering the high annotation cost, the size of our
proposed ASAD dataset is still relatively small. Mean-
while, since the deﬁnition of action labels could be
ambiguous, the action annotation may not be accurate.
For instance, it is diﬃcult to judge the boundary be-
tween “walk” and ”running” in the continuous tempo-
ral domain. Or, without including the audio informa-
tion, it is challenging to decide who is speaking. Such
issues may impair the ASAD study. To cope with this
issue, it is necessary to perform high-quality annota-
tions with more annotators involved.

• Because evaluating the ASAD result is complicated,
we separately evaluated spatial detection, actor identi-
ﬁcation, and multi-label action classiﬁcation. Conse-
quently, the overall ASAD performance is represented
by multiple metric values. However, in an ideal case,
we hope to utilize a single metric value to represent the
overall ASAD performance. Considering that each of
our ASAD metrics (e.g., HL@0.5) is obtained from a
complex formula, it is challenging to integrate them
into a single metric value. To ﬁnd a solution, further
exploration is needed. Since we have raised this ques-
tion in the ASAD task, it might be solved in future
works.

And we are considering addressing those issues in future
works.

This paper is not the end, but rather the starting steps,
we are excited to engage with the research community to
explore ASAD deeper. We believe considering actor identi-
ﬁcation with spatiotemporal action detection could promote
the research on video understanding and beyond.

References
[1] Abu-El-Haija, S., Kothari, N., Lee,

J., Natsev, P., Toderici,
G., Varadarajan, B., Vijayanarasimhan, S., 2016. Youtube-8m:
A large-scale video classiﬁcation benchmark.
arXiv preprint
arXiv:1609.08675 URL: https://arxiv.org/abs/1609.08675.

[2] Barekatain, M., Martí, M., Shih, H.F., Murray, S., Nakayama, K.,
Matsuo, Y., Prendinger, H., 2017. Okutama-action: An aerial view
video dataset for concurrent human action detection, in: 1st Joint
BMTT-PETS Workshop on Tracking and Surveillance, CVPR, pp.
1–8.

[3] Bewley, A., Ge, Z., Ott, L., Ramos, F., Upcroft, B., 2016. Simple
online and realtime tracking, in: 2016 IEEE International Conference
on Image Processing (ICIP), pp. 3464–3468. doi:10.1109/ICIP.2016.
7533003.

[4] Bochinski, E., Eiselein, V., Sikora, T., 2017. High-speed tracking-by-
detection without using image information, in: International Work-
shop on Traﬃc and Street Surveillance for Safety and Security at
IEEE AVSS 2017, Lecce, Italy. URL: http://elvera.nue.tu-berlin.
de/files/Bochinski2017.pdf.

[5] Brasó, G., Leal-Taixé, L., 2020. Learning a neural solver for multi-
ple object tracking, in: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 6247–6257.

[6] Carreira, J., Noland, E., Hillier, C., Zisserman, A., 2019. A short
arXiv preprint

note on the kinetics-700 human action dataset.
arXiv:1907.06987 URL: https://arxiv.org/abs/1907.06987.

[7] Carreira, J., Zisserman, A., 2017. Quo vadis, action recognition? a
new model and the kinetics dataset, in: IEEE Conference on Com-
puter Vision and Pattern Recognition, IEEE. pp. 4724–4733.

[8] Chang, M.F., Lambert, J., Sangkloy, P., Singh, J., Bak, S., Hartnett,
A., Wang, D., Carr, P., Lucey, S., Ramanan, D., et al., 2019. Argov-
erse: 3d tracking and forecasting with rich maps, in: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 8748–8757.

[9] Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross, D.A., Deng,
J., Sukthankar, R., 2018. Rethinking the faster r-cnn architecture for
temporal action localization, in: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1130–1139.
[10] Chaquet, J.M., Carmona, E.J., Fernández-Caballero, A., 2013. A sur-
vey of video datasets for human action and activity recognition. Com-
puter Vision and Image Understanding 117, 633–659.

[11] Choutas, V., Weinzaepfel, P., Revaud, J., Schmid, C., 2018. Potion:
Pose motion representation for action recognition, in: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7024–7033.

[12] Crouse, D.F., 2016. On implementing 2d rectangular assignment al-
gorithms. IEEE Transactions on Aerospace and Electronic Systems
52, 1679–1696.

[13] Dai, P., Wang, X., Zhang, W., Chen, J., 2018. Instance segmentation
enabled hybrid data association and discriminative hashing for online
multi-object tracking. IEEE Transactions on Multimedia 21, 1709–
1723.

F.Yang et al.: Preprint submitted to Elsevier

Page 10 of 13

Staticcamerarecording:Non-staticcamerarecording:ASAD

[14] Dave, A., Khurana, T., Tokmakov, P., Schmid, C., Ramanan, D., 2020.
Tao: A large-scale benchmark for tracking any object. arXiv preprint
arXiv:2005.10356 URL: https://arxiv.org/abs/2005.10356.

[15] Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman,
A., 2010. The pascal visual object classes (voc) challenge. Interna-
tional journal of computer vision 88, 303–338.

[16] Fabian Caba Heilbron, Victor Escorcia, B.G., Niebles, J.C., 2015.
Activitynet: A large-scale video benchmark for human activity un-
derstanding, in: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 961–970.

[17] Feichtenhofer, C., Fan, H., Malik, J., He, K., 2019. Slowfast networks
for video recognition, in: Proceedings of the IEEE international con-
ference on computer vision, pp. 6202–6211.

[18] Feichtenhofer, C., Pinz, A., Zisserman, A., 2016. Convolutional two-
stream network fusion for video action recognition, in: Proceedings
of the IEEE conference on computer vision and pattern recognition,
pp. 1933–1941.

[19] Fernando, T., Denman, S., Sridharan, S., Fookes, C., 2018. Tracking
by prediction: A deep generative model for mutli-person localisation
and tracking, in: 2018 IEEE Winter Conference on Applications of
Computer Vision (WACV), IEEE. pp. 1122–1132.

[20] Fu, Z., Angelini, F., Chambers, J., Naqvi, S.M., 2019. Multi-level co-
operative fusion of gm-phd ﬁlters for online multiple human tracking.
IEEE Transactions on Multimedia 21, 2277–2291.

[21] Gao, J., Yang, Z., Nevatia, R., 2017. Cascaded boundary regression
for temporal action detection, in: British Machine Vision Conference
2017, BMVC 2017, London, UK, September 4-7, 2017, BMVA Press.
[22] Gibaja, E., Ventura, S., 2015. A tutorial on multilabel learning. ACM

Computing Surveys 47, 52.

[23] Girdhar, R., Carreira, J., Doersch, C., Zisserman, A., 2018. A bet-
ter baseline for ava. arXiv preprint arXiv:1807.10066 URL: https:
//arxiv.org/abs/1807.10066.

[24] Gkioxari, G., Malik, J., 2015. Finding action tubes, in: Proceedings
of the IEEE conference on computer vision and pattern recognition,
pp. 759–768.

[25] Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vi-
jayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., et al.,
2017. Ava: A video dataset of spatio-temporally localized atomic
visual actions. CoRR, abs/1705.08421 4.

[26] Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vi-
jayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., et al.,
2018. Ava: A video dataset of spatio-temporally localized atomic vi-
sual actions, in: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 6047–6056.

[27] Gu, Y., Ye, X., Sheng, W., Ou, Y., Li, Y., 2020. Multiple stream
deep learning model for human action recognition. Image and Vision
Computing 93, 103818.

[28] Han, L., Wu, X., Liang, W., Hou, G., Jia, Y., 2010. Discriminative
human action recognition in the learned hierarchical manifold space.
Image and Vision Computing 28, 836–849.

[29] Hornakova, A., Henschel, R., Rosenhahn, B., Swoboda, P., 2020.
Lifted disjoint paths with application in multiple object tracking, in:
The 37th International Conference on Machine Learning (ICML), pp.
1–12.

[30] Hosang, J., Benenson, R., Dollár, P., Schiele, B., 2015. What makes
for eﬀective detection proposals? IEEE transactions on pattern anal-
ysis and machine intelligence 38, 814–830.

[31] Hou, R., Chen, C., Shah, M., 2017. Tube convolutional neural net-
work (t-cnn) for action detection in videos, in: Proceedings of the
IEEE international conference on computer vision, pp. 5822–5831.

[32] Huang, Q., Xiong, Y., Lin, D., 2018. Unifying identiﬁcation and
context learning for person recognition, in: The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 2217–2225.
[33] Hutchinson, M., Gadepally, V., 2020. Video action understanding: A
tutorial. arXiv preprint arXiv:2010.06647 URL: https://arxiv.org/
abs/2010.06647.

[34] Idrees, H., Zamir, A.R., Jiang, Y., Gorban, A., Laptev, I., Sukthankar,
R., Shah, M., 2017. The thumos challenge on action recognition for

videos “in the wild”. Computer Vision and Image Understanding 155,
1–23.

[35] Jiang, X., Li, P., Li, Y., Zhen, X., 2019. Graph neural based end-to-
end data association framework for online multiple-object tracking.
arXiv preprint arXiv:1907.05315 URL: https://arxiv.org/abs/1907.
05315.

[36] Kalogeiton, V., Weinzaepfel, P., Ferrari, V., Schmid, C., 2017. Action
tubelet detector for spatio-temporal action localization, in: Proceed-
ings of the IEEE International Conference on Computer Vision, pp.
4405–4413.

[37] Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-
Fei, L., 2014. Large-scale video classiﬁcation with convolutional neu-
ral networks, in: Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition, pp. 1725–1732.

[38] Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T., 2011.
Hmdb: a large video database for human motion recognition, in: 2011
International Conference on Computer Vision, IEEE. pp. 2556–2563.
[39] Li, A., Thotakuri, M., Ross, D.A., Carreira, J., Vostrikov, A., Zis-
serman, A., 2020a. The ava-kinetics localized human actions video
dataset. arXiv preprint arXiv:2005.00214 URL: https://arxiv.org/
abs/2005.00214.

[40] Li, C., Zhong, Q., Xie, D., Pu, S., 2018. Co-occurrence feature learn-
ing from skeleton data for action recognition and detection with hier-
archical aggregation, in: Proceedings of the 27th International Joint
Conference on Artiﬁcial Intelligence, pp. 786–792.

[41] Li, J., Gao, X., Jiang, T., 2020b. Graph networks for multiple object
tracking, in: The IEEE Winter Conference on Applications of Com-
puter Vision, pp. 719–728.

[42] Li, J., Wei, L., Zhang, F., Yang, T., Lu, Z., 2019. Joint deep and depth
for object-level segmentation and stereo tracking in crowds.
IEEE
Transactions on Multimedia 21, 2531–2544.

[43] Li, Y., Lin, W., See, J., Xu, N., Xu, S., Yan, K., Yang, C., 2020c. Cfad:
Coarse-to-ﬁne action detector for spatiotemporal action localization,
in: European Conference on Computer Vision, Springer. pp. 510–527.
[44] Li, Y., Wang, Z., Wang, L., Wu, G., 2020d. Actions as moving points.

arXiv preprint arXiv:2001.04608 .

[45] Lin, T., Liu, X., Li, X., Ding, E., Wen, S., 2019. Bmn: Boundary-
matching network for temporal action proposal generation, in: Pro-
ceedings of the IEEE International Conference on Computer Vision,
pp. 3889–3898.

[46] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Dollár, P., Zitnick, C.L., 2014. Microsoft coco: Common objects in
context, in: European conference on computer vision, Springer. pp.
740–755.

[47] Liu, C., Yuen, P.C., 2010. Human action recognition using boosted

eigenactions. Image and vision computing 28, 825–835.

[48] Liu, Z., Zhang, C., Tian, Y., 2016. 3d-based deep convolutional neu-
ral network for action recognition with depth sequences. Image and
Vision Computing 55, 93–100.

[49] Luo, W., Xing, J., Milan, A., Zhang, X., Liu, W., Zhao, X., Kim, T.K.,
2014. Multiple object tracking: A literature review. arXiv preprint
arXiv:1409.7618 .

[50] Ma, C., Li, Y., Yang, F., Zhang, Z., Zhuang, Y., Jia, H., Xie, X., 2019.
Deep association: End-to-end graph-based learning for multiple ob-
ject tracking with conv-graph neural network, in: Proceedings of the
2019 on International Conference on Multimedia Retrieval, pp. 253–
261.

[51] Ma, L., Tang, S., Black, M.J., Gool, L.V., 2018. Customized multi-

person tracker, in: ACCV.

[52] Masoud, O., Papanikolopoulos, N., 2003. A method for human action

recognition. Image and Vision Computing 21, 729–743.

[53] Mettes, P., Van Gemert, J.C., Snoek, C.G., 2016. Spot on: Action
localization from pointly-supervised proposals, in: European confer-
ence on computer vision, Springer. pp. 437–453.

[54] Milan, A., Leal-Taixé, L., Reid, I., Roth, S., Schindler, K., 2016.
Mot16: A benchmark for multi-object tracking. arXiv:1603.00831
[cs] URL: http://arxiv.org/abs/1603.00831.

[55] Pan, J., Chen, S., Shou, Z., Shao, J., Li, H., 2020.

Actor-

F.Yang et al.: Preprint submitted to Elsevier

Page 11 of 13

ASAD

context-actor relation network for spatio-temporal action localization.
arXiv preprint arXiv:2006.07976 URL: https://arxiv.org/pdf/2006.
07976v2.pdf.

[56] Qiu, Z., Yao, T., Mei, T., 2017. Learning spatio-temporal representa-
tion with pseudo-3d residual networks, in: proceedings of the IEEE
International Conference on Computer Vision, pp. 5533–5541.

[57] Qu, W., Schonfeld, D., Mohamed, M., 2007.

Real-time dis-
tributed multi-object tracking using multiple interactive trackers and a
magnetic-inertia potential model. IEEE Transactions on Multimedia
9, 511–519.

[58] Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C., 2016. Per-
formance measures and a data set for multi-target, multi-camera track-
ing, in: European Conference on Computer Vision, Springer. pp. 17–
35.

[59] Ristani, E., Tomasi, C., 2018. Features for multi-target multi-camera
tracking and re-identiﬁcation, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 6036–6046.

[60] Satoh, S., 1999. Towards actor/actress identiﬁcation in drama videos,
in: Proceedings of the seventh ACM international conference on Mul-
timedia (Part 2), pp. 75–78.

[61] Shi, L., Zhang, Y., Cheng, J., Lu, H., 2019. Two-stream adaptive
graph convolutional networks for skeleton-based action recognition,
in: Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pp. 12026–12035.

[62] Simonyan, K., Zisserman, A., 2014. Two-stream convolutional net-
works for action recognition in videos, in: Advances in neural infor-
mation processing systems, pp. 568–576.

[63] Singh, G., Saha, S., Sapienza, M., Torr, P.H., Cuzzolin, F., 2017. On-
line real-time multiple spatiotemporal action localisation and predic-
tion, in: Proceedings of the IEEE International Conference on Com-
puter Vision, pp. 3637–3646.

[64] Soomro, K., Idrees, H., Shah, M., 2015. Action localization in videos
through context walk, in: Proceedings of the IEEE international con-
ference on computer vision, pp. 3280–3288.

[65] Soomro, K., Zamir, A.R., Shah, M., 2012. Ucf101: A dataset of
101 human actions classes from videos in the wild. arXiv preprint
arXiv:1212.0402 URL: https://arxiv.org/abs/1212.0402.

[66] Tang, J., Xia, J., Mu, X., Pang, B., Lu, C., 2020. Asynchronous inter-
action aggregation for action detection, in: Proceedings of the Euro-
pean conference on computer vision (ECCV).

[67] Tang, S., Andriluka, M., Andres, B., Schiele, B., 2017. Multiple peo-
ple tracking by lifted multicut and person re-identiﬁcation, in: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3539–3548.

[68] Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M., 2015.
Learning spatiotemporal features with 3d convolutional networks, in:
Proceedings of the IEEE international conference on computer vision,
pp. 4489–4497.

[69] Ulutan, O., Rallapalli, S., Srivatsa, M., Torres, C., Manjunath, B.,
2020. Actor conditioned attention maps for video action detection, in:
The IEEE Winter Conference on Applications of Computer Vision,
pp. 527–536.

[70] Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool,
L., 2016. Temporal segment networks: Towards good practices for
deep action recognition, in: European conference on computer vision,
Springer. pp. 20–36.

[71] Wang, X., Zhang, X., Zhu, Y., Guo, Y., Yuan, X., Xiang, L., Wang,
Z., Ding, G., Brady, D., Dai, Q., et al., 2020. Panda: A gigapixel-
level human-centric video dataset, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 3268–
3278.

[72] Wang, Z., Zheng, L., Liu, Y., Wang, S., 2019. Towards real-time

multi-object tracking. arXiv preprint arXiv:1909.12605 .

[73] Weinzaepfel, P., Harchaoui, Z., Schmid, C., 2015. Learning to track
for spatio-temporal action localization, in: Proceedings of the IEEE
international conference on computer vision, pp. 3164–3172.

[74] Weinzaepfel, P., Martin, X., Schmid, C., 2016.

tion localization with sparse spatial supervision.

Human ac-
arXiv preprint

arXiv:1605.05197 URL: https://arxiv.org/pdf/1605.05197.pdf.
[75] Wen, L., Li, W., Yan, J., Lei, Z., Yi, D., Li, S.Z., 2014. Multiple tar-
get tracking based on undirected hierarchical relation hypergraph, in:
Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1282–1289.

[76] Weng, X., Wang, Y., Man, Y., Kitani, K.M., 2020. Gnn3dmot: Graph
neural network for 3d multi-object tracking with 2d-3d multi-feature
learning, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR).

[77] Wojke, N., Bewley, A., Paulus, D., 2017. Simple online and realtime
tracking with a deep association metric. 2017 IEEE International Con-
ference on Image Processing (ICIP) , 3645–3649.

[78] Xu, H., Das, A., Saenko, K., 2017. R-c3d: Region convolutional 3d
network for temporal activity detection, in: Proceedings of the IEEE
international conference on computer vision, pp. 5783–5792.
[79] Xu, M., Yuan, X., Shen, J., Yan, S., 2010. Cast2face: Character iden-
tiﬁcation in movie with actor-character correspondence, in: Proceed-
ings of the 18th ACM international conference on Multimedia, pp.
831–834.

[80] Yan, A., Wang, Y., Li, Z., Qiao, Y., 2019. Pa3d: Pose-action 3d ma-
chine for video recognition, in: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 7922–7931.
[81] Yang, F., Chang, X., Dang, C., Zheng, Z., Sakti, S., Nakamura,
S., Wu, Y., 2020. Remots: Self-supervised reﬁning multi-object
tracking and segmentation. URL: https://arxiv.org/abs/2007.03200,
arXiv:2007.03200.

[82] Yang, F., Chang, X., Sakti, S., Wu, Y., Nakamura, S., 2021. Remot:
A model-agnostic reﬁnement for multiple object tracking. Image and
Vision Computing 106, 104091.

[83] Yang, F., Sakti, S., Wu, Y., Nakamura, S., 2019a. A framework for
knowing who is doing what in aerial surveillance videos. IEEE Ac-
cess 7, 93315–93325.

[84] Yang, F., Wu, Y., Sakti, S., Nakamura, S., 2019b. Make skeleton-
based action recognition model smaller, faster and better, in: Proceed-
ings of the ACM Multimedia Asia, pp. 1–6.

[85] Yang, X., Yang, X., Liu, M.Y., Xiao, F., Davis, L.S., Kautz, J., 2019c.
Step: Spatio-temporal progressive learning for video action detection,
in: Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pp. 264–272.

[86] Yooyoung, Y., Fiscus, J., Godil, A., Joy, D., Delgado, A., Golden,
J., 2019. Actev18: Human activity detection evaluation for ex-
tended videos, in: 2019 IEEE Winter Applications of Computer Vi-
sion Workshops (WACVW), IEEE. pp. 1–8.

[87] Yuan, Y., Qi, L., Lu, X., 2016. Action recognition by joint learning.

Image and Vision Computing 55, 77–85.

[88] Zhang, L., Li, Y., Nevatia, R., 2008. Global data association for multi-
object tracking using network ﬂows, in: Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pp. 1–8.

[89] Zhang, S., Wang, J., Wang, Z., Gong, Y., Liu, Y., 2015. Multi-target
tracking by learning local-to-global trajectory models. Pattern Recog-
nition 48, 580–590.

[90] Zhang, Y., Wang, C., Wang, X., Zeng, W., Liu, W., 2020. A simple
baseline for multi-object tracking. arXiv preprint arXiv:2004.01888
URL: https://arxiv.org/abs/2004.01888.

[91] Zhao, H., Torralba, A., Torresani, L., Yan, Z., 2019. Hacs: Human
action clips and segments dataset for recognition and temporal lo-
calization, in: Proceedings of the IEEE International Conference on
Computer Vision, pp. 8668–8678.

[92] Zhou, X., Koltun, V., Krähenbühl, P., 2020. Tracking objects as

points. ECCV .

[93] Zhu, H., Vial, R., Lu, S., 2017. Tornado: A spatio-temporal convolu-
tional regression network for video action proposal, in: Proceedings
of the IEEE International Conference on Computer Vision, pp. 5813–
5821.

[94] Zolfaghari, M., Oliveira, G.L., Sedaghat, N., Brox, T., 2017. Chained
multi-stream networks exploiting pose, motion, and appearance for
action classiﬁcation and detection, in: Proceedings of the IEEE Inter-
national Conference on Computer Vision, pp. 2904–2913.

F.Yang et al.: Preprint submitted to Elsevier

Page 12 of 13

[95] Zolfaghari, M., Singh, K., Brox, T., 2018. Eco: Eﬃcient convolu-
tional network for online video understanding, in: Proceedings of the
European conference on computer vision (ECCV), pp. 695–712.

ASAD

F.Yang et al.: Preprint submitted to Elsevier

Page 13 of 13

