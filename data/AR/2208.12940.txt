2
2
0
2

p
e
S
7

]

V
C
.
s
c
[

2
v
0
4
9
2
1
.
8
0
2
2
:
v
i
X
r
a

Actor-identiï¬ed Spatiotemporal Action Detection â€”
Detecting Who Is Doing What in Videosâ‹†

Fan Yanga,âˆ—, Norimichi Ukitab, Sakriani Saktia,c and Satoshi Nakamuraa,c

aNara Institute of Science and Technology, Japan
bToyota Technological Institute, Nagoya, Japan.
cRIKEN, Center for Advanced Intelligence Project AIP, Japan

A R T I C L E I N F O

Keywords:
Action Recognition
Multiple Object Tracking

A B S T R A C T

The success of deep learning on video Action Recognition (AR) has motivated researchers to progres-
sively promote related tasks from the coarse level to the ï¬ne-grained level. Compared with conven-
tional AR that only predicts an action label for the entire video, Temporal Action Detection (TAD) has
been investigated for estimating the start and end time for each action in videos. Taking TAD a step
further, Spatiotemporal Action Detection (SAD) has been studied for localizing the action both spa-
tially and temporally in videos. However, who performs the action, is generally ignored in SAD, while
identifying the actor could also be important. To this end, we propose a novel task, Actor-identiï¬ed
Spatiotemporal Action Detection (ASAD), to bridge the gap between SAD and actor identiï¬cation.

In ASAD, we not only detect the spatiotemporal boundary for instance-level action but also as-
sign the unique ID to each actor. To approach ASAD, Multiple Object Tracking (MOT) and Action
Classiï¬cation (AC) are two fundamental elements. By using MOT, the spatiotemporal boundary of
each actor is obtained and assigned to a unique actor identity. By using AC, the action class is esti-
mated within the corresponding spatiotemporal boundary. Since ASAD is a new task, it poses many
new challenges that cannot be addressed by existing methods: i) no dataset is speciï¬cally created for
ASAD, ii) no evaluation metrics are designed for ASAD, iii) current MOT performance is the bottle-
neck to obtain satisfactory ASAD results. To address those problems, we contribute to i) annotate a
new ASAD dataset, ii) propose ASAD evaluation metrics by considering multi-label actions and ac-
tor identiï¬cation, iii) improve the data association strategies in MOT to boost the MOT performance,
which leads to better ASAD results. We believe considering actor identiï¬cation with spatiotemporal
action detection could promote the research on video understanding and beyond. The code is available
at https://github.com/fandulu/ASAD.

1. Introduction

Vision-based Action Recognition (AR) aims to detect
human-deï¬ned actions from a sequence of data (e.g., videos)
and has a wide range of applications in our daily life. For in-
stance, it has been applied for YouTube to recognize billions
of video tags before recommending a video to us, or for the
policemen to quickly retrieval a criminal from thousands-
hours surveillance videos, or for a virtual game machine to
interact with players, and many others ( [10, 33]).

In recent years, the success of deep learning on AR has
motivated researchers to progressively promote the AR task
from the coarse level to the ï¬ne-grained level. Compared
with conventional AR that only predicts an action label for
the entire video, Temporal Action Detection (TAD) has been
investigated for estimating the start and end time for each
action in videos. Taking TAD a step further, Spatiotempo-
ral Action Detection (SAD) has been studied for localizing
the action both spatially and temporally in videos. However,
who performs the action is generally ignored in SAD studies.
We believe actor identiï¬cation should be considered to-
gether with SAD. When multiple actors are involved in the
target scenes (e.g., basketball/soccer games), it is preferred
to know â€œwho is doing whatâ€, and thus, identifying each

âˆ—Corresponding author

hongheyangfan@gmail.com (F. Yang)
ORCID(s): 0000-0001-7185-5688 (F. Yang)

Figure 1: Actor-identiï¬ed Spatiotemporal Action Detection
(ASAD) is Spatiotemporal Action Detection (SAD) pluses ac-
tor identiï¬cation.

actor with their actions is desired. Nonetheless, SAD and
actor identiï¬cation are treated as diï¬€erent tasks for a long
time. To this end, we propose a novel task, Actor-identiï¬ed
Spatiotemporal Action Detection (ASAD), to bridge the gap
between SAD and actor identiï¬cation (Figure 1).

To approach ASAD, Multiple Object Tracking (MOT) [49]

and Action Classiï¬cation (AC) [33] are two fundamental el-
ements (Figure 2). By using MOT, the spatiotemporal bound-
ary of each actor is obtained and assigned to a unique actor
identity. By using AC, the action class is estimated within
the corresponding spatiotemporal boundary. In general, they
may work as independent modules by considering the model

F.Yang et al.: Preprint submitted to Elsevier

Page 1 of 13

Example of Rough Action Recognition in Videosare playing basketballis dribblingis defendingExample of  Spatiotemporal Action Detection (SAD) in Videos is dribblingis defendingExample of Actor-identifiedSpatiotemporal Action Detection (ASAD) in Videos Actor ID 0Actor ID 1 
 
 
 
 
 
ASAD

training ï¬‚exibility.

Figure 2: The illustration of ASAD processing.

Since ASAD is a new task, it poses many new challenges
that cannot be addressed by existing methods: i) no dataset
is speciï¬cally created for ASAD, ii) no evaluation metrics
are designed for ASAD, iii) current MOT performance could
be the bottleneck to obtain satisfactory ASAD results. To
address those problems, we contribute to i) annotate a new
ASAD dataset, ii) propose ASAD evaluation metrics by con-
sidering multi-label actions and actor identiï¬cation, iii) im-
prove the data association strategies in MOT to boost the
MOT performance, which leads to better ASAD results 1.
We summarize the main contributions as follows.

â€¢ We raise a new study task of video action recognition
â€” Actor-identiï¬ed Spatiotemporal Action Detection
(ASAD). As far as we are aware, it has great impor-
tance but has been historically overlooked. ASAD
bridges the gap between the existing Spatiotemporal
Action Detection (SAD) study and the new demand
for identifying actors.

â€¢ We speciï¬cally provided a novel dataset for the ASAD
study. It covers a rich action category and actor iden-
tities.

â€¢ We presented novel metrics for ASAD evaluation. To
the best of our knowledge, existing metrics cannot be
applied to ASAD, and we are the ï¬rst to introduce such
metrics.

2. Related Works
3. Video Action Recognition

In general, video action recognition research can be di-
vided into several categories (Figure 3). Normal Action Recog-
nition (AR) takes an entire video, or, a video clip, as the in-
put and generates a corresponding action class. It is used
to understand the overall video concept without specifying
the details in the spatial domain and temporal domain. Tem-
poral Action Detection (TAD) gives temporal details to AR,

1We have organized our proposed dataset and evaluation metrics scripts

at GitHub, it will be released.

by clarifying the start and end times of an action. Accord-
ingly, one video could be segmented into several temporal
components in TAD. Compared with TAD, Spatiotemporal
Action Detection (SAD) not only detects the action bound-
ary in the temporal domain but also locates the actor with
bounding boxes (or instance masks) in the spatial domain.
We generally call such a spatiotemporal boundary the action
tube. In this work, we propose Actor-identiï¬ed Spatiotem-
poral Action Detection (ASAD) from SAD, by incorporating
the unique identity of each actor.

We summarize the related datasets and studies for AR,
TAD, SAD, and ASAD in Table 1. To link bounding boxes
to action tubes, Multiple Object Tracking (MOT) [82] is also
commonly applied in SAD. Some SAD works can also track
the actor and assign them with unique IDs. However, based
on the evaluation protocol of SAD, the annotation of actor
identity may not be provided and the actor identiï¬cation has
not been evaluated. That means, there is no clear bound-
ary between ASAD and SAD in terms of the method, their
diï¬€erence lies more in the data annotation and evaluation
protocols. In detail, the action tube ID given in SAD may
not be consistent with actor ID. For example, after the same
actor changes his/her action, the corresponding action tube
ID changed but the actor ID should remain the same. Unfor-
tunately, such kind of actor ID is not available in most SAD
datasets.

As we suppose that MOT and AC are two important com-
ponents in ASAD, we take a look into the role of MOT and
AC in AR, TAD, SAD, and ASAD (see Table 3). The AC
could be a necessary module for all action recognition cate-
gories. In SAD, MOT might be used (e.g., on UCF101 +
ROAD dataset [63]), but not be necessary (e.g., on AVA
dataset [26]). However, both MOT and AC are needed in
ASAD.

In addition, previous studies [60, 79, 32] focus on only
identifying actors in videos, but without detecting their ac-
tions. In this manner, as a new task, ASAD has bridged the
gap between the SAD and the actor identiï¬cation (Table 2).

4. Multiple Object Tracking

Since Multiple Object Tracking (MOT) plays an impor-
tant role in Actor-identiï¬ed Spatiotemporal Action Detec-
tion (ASAD), we further provide an overview of MOT-related
works.

Generally, MOT is applied to connect identical obser-
vations into tracklets based on the similarity of MOT fea-
tures. Speciï¬cally, MOT methods can be divided into two
categories â€” online MOT and oï¬„ine MOT (Table 4.). The
online data association is performed on observations that are
available up to the current frame. Diï¬€erent from online ap-
proaches, oï¬„ine data association takes global observations
into consideration, which may not be applicable for real-time
applications but be ideal for assisting annotation works. Nu-
merous oï¬„ine approaches have been proposed in previous
studies [49]. Among them, formulating MOT data associa-
tion as a global clustering problem has achieved great suc-

F.Yang et al.: Preprint submitted to Elsevier

Page 2 of 13

is dribblingis defendingMultiple Object Tracking (MOT)Action Classification (AC)Actor ID 0Actor ID 1ASAD

Figure 3: A comparison of action recognition works, which could be roughly divided into
four categories: Action Recognition (AR), Temporal Action Detection (TAD), Spatiotem-
poral Action Detection (SAD), and our deï¬ned Actor-identiï¬ed Spatiotemporal Action
Detection (ASAD). Existing works (i.e., AR, TAD, and SAD) ignore to identify actors
while our ASAD addresses this issue. Parts of this graph credit to [33].

Action Recognition Category

Available Datasets

Related Works

AR

TAD

SAD

ASAD

HMDB [38], UCF101 [65],Sports-1M [37], Kinetics-700 [6]

[52, 47, 28, 62, 68, 18, 48, 87, 17, 27]

ActivityNet [16], YouTube-8M [1],THUMOS [34], HACS [91]

[64, 70, 78, 21, 9, 45]

UCF101+ROAD [63], DALY [74], Hollywood2Tubes [53] AVA [26],
AVA-Kinetics [39], ActEV [86]

[24, 73, 93, 31, 63, 36, 83]
[23, 85, 69, 55, 44, 43, 66]

Okutama [2] (available but not ideal)

Ours

Table 1
The related datasets and studies for AR, TAD, SAD, and ASAD. Note that, unlike other
SAD datasets, actor ID is given in annotations of Okutama [2] but the ASAD evaluation
has not been explored. Besides, the Okutama dataset consists of 4K-resolution drone
videos, which may only cover very limited scenarios of ASAD. In addition, some SAD
models, such as ROAD [63], AlphAction [66], and ACAM [69], may potentially generate
ASAD results but were evaluated by the SAD protocol in the original works. That is,
the consistency of actor identity is ignored in these works.

cesses [81]

5. Action Classiï¬cation

The Action Classiï¬cation (AC) model plays such a role
to map the spatiotemporal information to action categories.
There are numerous AC studies considering the approaches
of utilizing features and designing the model structure. In
detail, Action Classiï¬cation (AC) approaches could be di-
vided into 5 categories, including RGB AC, RGB + Flow
AC, Pose AC, RGB + Pose AC, and RGB + Flow + Pose
AC, as shown in Figure 4. Based on these 5 categories, we
list the corresponding studies in Table 5.

6. Proposed ASAD Dataset and Evaluation

Metrics
Given a video, Actor-identiï¬ed Spatiotemporal Action
Detection (ASAD) aims to detect the spatiotemporal bound-
aries (i.e., tracklets/actor tubes) for each actor, assign each
actor a unique identity, and obtain the actions of actors at
each moment. Consequently, the ASAD dataset should in-
clude those factors and the ASAD metrics should verify the
performance on those factors.

6.1. Dataset for ASAD

By reviewing existing action recognition datasets (Sec-
tion 2), it can be noticed that a proper ASAD dataset may

F.Yang et al.: Preprint submitted to Elsevier

Page 3 of 13

Action Tube 0(Action Class 0)Action Tube 1(Action Class 1)Spatiotemporal Action Detection (SAD)Fame 1Fame 2Fame N-1Fame NHeightâ€¦â€¦Action Tube 2(Action Class 0)Actor-identified Spatiotemporal Action Detection (ASAD)Fame 1Fame 2Fame N-1Fame NHeightâ€¦â€¦Actor Tube 0(Actor ID 0)Actor Tube 1(Actor ID 1)Action Tube 0(Action Class 0)Action Tube 1(Action Class 1)Action Tube 2(Action Class 0)No DefinedAction (keep)Action Recognition (AR)Fame 1Fame 2Fame N-1Fame NHeightâ€¦â€¦Action Class 0Temporal Action Detection (TAD)Fame 1Fame 2Fame N-1Fame NHeightâ€¦â€¦Action Class 0Action Class 1Start 0Start 1End 0End 1No DefinedAction (remove)ASAD

Approaches

Identifying Actors Detecting Actions

Spatiotemporal Action Detection (SAD)
[24, 73, 93, 31, 63, 36, 83, 23, 55, 44, 43, 66]

Actor Identiï¬cation [60, 79, 32]

Actor-identiï¬ed Spatiotemporal Action Detection (ASAD)

Table 2
A comparison of SAD, Actor Identiï¬cation, and ASAD.

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

Action Recognition Category

Using MOT Using AC

Action Recognition (AR)

Temporal Action Detection (TAD)

Not Need

Not Need

Need

Need

Spatiotemporal Action Detection (SAD)

May Not Need

Need

Actor-identiï¬ed Spatiotemporal Action Detection (ASAD)

Need

Need

Table 3
The role of MOT and AC in AR, TAD, SAD, and ASAD. For some evaluation protocols
of SAD, there is no need to link detection to tubes and MOT may not be used.

not be available. Although the existing Spatiotemporal Ac-
tion Detection (SAD) dataset might be similar to our de-
sired ASAD dataset, the actor identity is not properly an-
notated in the SAD dataset. We illustrate the annotation dif-
ference between SAD and ASAD data annotation by using
UCF101+ROAD dataset [63] and AVA dataset [26] as ex-
amples (Figures 5 and 6). In the UCF101+ROAD dataset,
the spatiotemporal boundaries are incomplete. Since actor
identiï¬cation is not the concern in SAD, after the predeï¬ned

action is ï¬nished, spatiotemporal annotation is not available.
In contrast, the annotation in ASAD should complete the
spatiotemporal boundary for each actor in the entire video,
no matter if the deï¬ned action is ï¬nished or not. In the AVA
dataset, despite the actor IDs being given, multiple actor IDs
have been assigned to the same actor in a single video, which
is incorrect for actor identiï¬cation. For actor identiï¬cation
purposes, the unique actor ID should be assigned to each
actor in one piece of video. Besides, while some remote

Approaches

[57, 3, 77, 4, 13, 42, 20, 35, 50, 72, 41, 5, 76, 92, 90, 19, 8]

[88, 75, 89, 67, 59, 51, 29]

Online Data
Association

Oï¬„ine Data
Association

(cid:51)

(cid:55)

(cid:55)

(cid:51)

Table 4
Online and oï¬„ine MOT methods.

Figure 4: Categories of Action Classiï¬cation (AC) models.

F.Yang et al.: Preprint submitted to Elsevier

Page 4 of 13

RGB VideoPose/SkeletonOptical FlowOptical FlowEstimationPose EstimationRGB-basedAction ClassificationTwo-stream (RGB + Flow)Action ClassificationRGB + PoseAction ClassificationPose/Skeleton Based Action ClassificationThree-stream (RGB + Flow + Pose) Based Action ClassificationApproaches

RGB AC RGB + Flow AC Pose AC RGB + Pose AC RGB + Flow + Pose AC

ASAD

Large-scale AC [37]

Two-Stream [62]

C3D [68], I3D [7], ECO [95],
P3D [56], FastSlow [17]

HCN [40], 2s-AGCN [61],
DD-Net [84]

Potion [11], PA3D [80]

Chained AC [94]

(cid:51)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

Table 5
The properties of action classiï¬cation works

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

surveillance video datasets are equipped with spatiotempo-
ral boundaries, actor identities, and acting classes, they fo-
cus on the special scene (e.g., remote surveillance) and may
not be suitable for the general ASAD study. For example,
Okutama dataset [2] and PANDA [71] only record tiny scale
actors and cover a small group of human daily activities.

Figure 6: Comparison between SAD and ASAD actor ID an-
notation by using AVA [26] as an example. In a single video,
while the existing SAD dataset may assign multiple actor IDs
to the same actor, our ASAD assigns the unique actor ID the
actor.

More examples are illustrated in Figure 8.

We present the historical role of our A-AVA dataset in
Figure 9. As the ï¬rst dataset that is speciï¬cally designed for
the ASAD study, the A-AVA dataset covers a rich diversity
of video scenes, as indoor and outdoor, diï¬€erent times of the
day, various actor scales, and more. Those properties are not
available in the previous dataset (i.e., Okutama dataset). A-
AVA dataset has bridged the gap between the SAD dataset
and actor identiï¬cation dataset.

6.2. Evaluation Metrics for ASAD

When considering the multi-label action, ASAD evalu-
ation could be a complicated task. Unlike single-label SAD
evaluation [25], it is challenging to simultaneously evaluate
multi-label action classiï¬cation and actor identiï¬cation with
spatial detection. To address this issue, we suggest evaluat-
ing ASAD from three aspects and then consider their overall
performance. The three aspects include Spatial Detection
Evaluation, Actor Identiï¬cation Evaluation, and Multi-label
Action Classiï¬cation Evaluation (Figure 10).

6.3. Spatial Detection Evaluation

We take the object detection metrics [15, 46, 30] to eval-
uate the spatial detection performance. First, we calculate

Figure 5: Comparison between SAD and ASAD spatiotempo-
ral annotation by using UCF101+ROAD [63] as an example.
The annotation in ASAD should complete the spatiotemporal
boundary for each actor in the entire video, no matter if the
deï¬ned action is ï¬nished or not.

Due to the above reasons, we are motivated to anno-
tate a new ASAD dataset. Compared with the SAD dataset,
the ASAD dataset requires to add correct actor identities.
As the AVA dataset [26] is a canonical SAD dataset and
TAO dataset [14] oï¬€ers some actor identity annotations on
it, we select a part of the AVA dataset to make an ASAD
dataset (Figure 7). Note that, we mainly selected video clips
that have visible actors and multiple actors available. Mean-
while, due to the heavy annotation cost, only 77 video clips
are selected among 430 AVA video clips. We named our
ASAD dataset A-AVA, which represents the Actor-identiï¬ed
AVA dataset. A-AVA dataset contains 47 videos for training
and 30 videos for testing. Be the same as the AVA dataset,
there are 80 action categories in the A-AVA dataset, and, ev-
ery 25 frames (i.e., around 1 second), the annotation is given
once. In the A-AVA dataset, the spatiotemporal boundaries,
actor identities, and corresponding actions are all annotated.

F.Yang et al.: Preprint submitted to Elsevier

Page 5 of 13

Original spatiotemporal annotations in UCF-ROAD dataset (SAD), e.g., fencing     .The spatiotemporal annotationsare incomplete.Expected spatiotemporal annotations for our ASAD, e.g., fencing     .The spatiotemporal annotations are complete.Actor 1Actor 2Actor 3Actor 4Actor 1Actor 2Actor 3Actor 4Actor ID annotation in the original AVA dataset (SAD). Actor IDs are fragmented in one video.Expected actor ID annotation in our ASAD. Actor IDs are consistent in one video.Actor 2Actor 1Actor 3Actor 2Actor 1Actor 3Actor 4Actor 5Actor 6Actor 7ASAD

Figure 7: We create a new ASAD dataset based on existing AVA dataset [25], by assigning
the unique actor identity to each actor.

Figure 8: Illustration of our Actor-identiï¬ed AVA dataset.

Intersection over Union (IoU), which is deï¬ned by

â€¢ False Positive (FP): A wrong detection with an IoU

ğ¼ğ‘œğ‘ˆ =

ğ‘ğ‘ğ‘œğ‘¥ğ‘ğ‘Ÿğ‘’ğ‘‘ âˆ© ğ‘ğ‘ğ‘œğ‘¥ğ‘¡ğ‘Ÿğ‘¢ğ‘’
ğ‘ğ‘ğ‘œğ‘¥ğ‘ğ‘Ÿğ‘’ğ‘‘ âˆª ğ‘ğ‘ğ‘œğ‘¥ğ‘¡ğ‘Ÿğ‘¢ğ‘’

where ğ‘ğ‘ğ‘œğ‘¥ğ‘ğ‘Ÿğ‘’ğ‘‘ and ğ‘ğ‘ğ‘œğ‘¥ğ‘¡ğ‘Ÿğ‘¢ğ‘’ represent the predicted bound-
ing box and the ground-truth box, respectively.

Second, based on the IoU value, True Positive (TP), False

Positive (FP), and False Negative (FN) are deï¬ned by

smaller than the threshold.

(1)

â€¢ False Negative (FN): A ground truth not detected.

and the corresponding Precision and Recall are

ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =

ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =

ğ‘‡ ğ‘ƒ
ğ‘‡ ğ‘ƒ + ğ¹ ğ‘ƒ
ğ‘‡ ğ‘ƒ
ğ‘‡ ğ‘ƒ + ğ¹ ğ‘

(2)

â€¢ True Positive (TP): A correct detection with an IoU

greater the threshold.

By traversing through all thresholds for detection con-
ï¬dence, diï¬€erent pairs of precision and recall can generate

F.Yang et al.: Preprint submitted to Elsevier

Page 6 of 13

Training Set: 47 videos(part of AVA dataset)Testing Set: 30 videos (part of AVA dataset)ActionsCategory: 80 actions (the same as AVA dataset)Annotation Frequency: Every 25 frames (the same as AVA dataset)ASAD

Figure 9: A historical timeline overview of datasets intended for video action recognition
studies.

Figure 10: Overview of our ASAD metrics, which evaluate the performance of spatial
detection, action classiï¬cation, and actor identiï¬cation.

the precision-recall curve, which indicates the association
between precision and recall. To reduce the eï¬€ect of the
wiggles in the curve, the precision-recall curve is interpo-
lated as ğ‘ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘
at recall score ğ‘Ÿ is assigned with
the highest precision for ğ‘Ÿ > ğ‘Ÿâ€² :

. The ğ‘ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘

ğ‘ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘(ğ‘Ÿ) = max
ğ‘Ÿ>ğ‘Ÿâ€²

ğ‘(ğ‘Ÿ

â€²

)

(3)

Since we only treat humans as the actor, there is only
one class for the detection, and therefore we utilize Average
Precision (AP), other than Mean Average Precision (mAP),
for the spatial detection evaluation. AP is the area under the
interpolated precision-recall curve, which can be calculated
using the following formula:

In this thesis, we assume any spatial detection with an
IoU value larger than 0.5 is True Positive, and the corre-
sponding metrics are represented as AP@0.5.

6.4. Actor Identiï¬cation Evaluation

While actor classiï¬cation has the pre-deï¬ned actor iden-
tities, actor identiï¬cation assigns each actor a unique identity
and the number of actor identities is non-parametric. There-
fore, we utilized part of Multiple Object Tracking (MOT)
evaluation metrics for actor identiï¬cation evaluation, as IDF1
(ratio of correctly identiï¬ed detections), MT (mostly tracked
targets), ML (mostly lost targets), and ID Switches. Those
identiï¬cation metrics were introduced by [58, 54] and have
been popularly utilized for a while. More speciï¬cally, the
IDF1, MT, and ML are respectively deï¬ned by

ğ´ğ‘ƒ =

ğ‘›âˆ’1
âˆ‘

ğ‘–=1

(ğ‘Ÿğ‘–+1 âˆ’ ğ‘Ÿğ‘–)ğ‘ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘(ğ‘Ÿğ‘–+1)

(4)

ğ¼ğ·ğ¹ 1 =

2ğ¼ğ·ğ‘‡ ğ‘ƒ
2ğ¼ğ·ğ‘‡ ğ‘ƒ + ğ¼ğ·ğ¹ ğ‘ƒ + ğ¼ğ·ğ¹ ğ‘

(5)

F.Yang et al.: Preprint submitted to Elsevier

Page 7 of 13

Action Recognition Related Data Release Time20122014201620182020Spatiotemporal Action Detection (SAD)Actor-identified Spatiotemporal Action Detection (ASAD)Action Recognition (AR)Temporal Action Detection (TAD)DALY [Weinzaepfelet al. ]UCF101+ROAD [Singh et al. ]AVA [Guet al. ]ActEV[Yooyounget al. ]Actor-identified AVA[Ours](originally designed for ASAD,small-to-largesize actor,>100 actors,movie videos,80 actions)AVA-Kinetics[Li et al.]Okutama[Barekatainet al. ](originally designed for SAD,small size actor,10 actors, drone videos,12 actions)MBDB[Kuehneet al.] UCF101[Soomroet al.] Sport-1M[Karpathyet al.] Kinetics-700[Carreiraet al.] ActivityNet[Heilbronet al.]YouTube-8M [Abu-El-Haijaet al.]THUMOS[Idreeset al.]HACS[Zhao et al.]Spatial Detection EvaluationDetections within the acceptable error rangespassMulti-label Action Classification EvaluationActor Identification EvaluationYesNoFailuresASAD

Figure 11: Illustration of matching pair between the ground-truth and the predicted sam-
ples.

where IDTP, IDFP, IDFN respectively represent the True
Positive ID, the False Positive ID, the False Negative ID.

ğ‘€ğ‘‡ =

ğ‘€ğ¿ =

âˆ‘

ğ‘–âˆˆğ‘î‰€ğ‘¡ğ‘Ÿğ‘¢ğ‘’

âˆ‘

ğ‘–âˆˆğ‘î‰€ğ‘¡ğ‘Ÿğ‘¢ğ‘’

ğŸ™{

ğŸ™{

)

ğ‘™ğ‘’ğ‘›(î‰€ ğ‘ğ‘Ÿğ‘’ğ‘‘
ğ‘–
ğ‘™ğ‘’ğ‘›(î‰€ ğ‘¡ğ‘Ÿğ‘¢ğ‘’
)
ğ‘–
ğ‘™ğ‘’ğ‘›(î‰€ ğ‘ğ‘Ÿğ‘’ğ‘‘
ğ‘–
ğ‘™ğ‘’ğ‘›(î‰€ ğ‘¡ğ‘Ÿğ‘¢ğ‘’
)
ğ‘–

)

â©¾ 0.8}

â©½ 0.2}

In detail, we utilize matrix îˆ°

to represent the matching
distance between each ground-truth bounding box (denoted
by ğ‘–) and predicted bounding box (denoted by ğ‘–), and we ob-
tain îˆ°

by

ğ‘–,ğ‘—

ğ‘–,ğ‘—

(6)

{

îˆ°

ğ‘–,ğ‘— =

1,
1 âˆ’ ğ¼ğ‘œğ‘ˆğ‘–,ğ‘—,

if ğ¼ğ‘œğ‘ˆğ‘–,ğ‘— < 0.5;
otherwise.

(7)

ğ‘–

and î‰€ ğ‘¡ğ‘Ÿğ‘¢ğ‘’
ğ‘–

where î‰€ ğ‘ğ‘Ÿğ‘’ğ‘‘
respectively denote the predicted and
the ground-truth Tracklet ğ‘–, the number of î‰€ ğ‘¡ğ‘Ÿğ‘¢ğ‘’
. If
the prediction matches for the ground truth more than 80%
of its life span, it is regarded as mostly tacked (MT). If the
prediction only matches for the ground truth less than 20%
of total length, it is regarded as mostly lost (ML).

is ğ‘î‰€ğ‘¡ğ‘Ÿğ‘¢ğ‘’

ğ‘–

6.5. Multi-label Action Classiï¬cation Evaluation

It is intuitive to consider that each actor could take sev-
eral actions simultaneously, which are corresponding to multi-
label actions. For instance, an actor could be making a phone
call and walking at the same time. Due to the lack of evalu-
ation metrics, conventional Action Recognition studies have
been evaluated with only the single-label action for a while [2,
25]. Therefore, we provide metrics for multi-label ASAD
evaluation, which considers the evaluations of multi-label
multi-class action classiï¬cation and actor identiï¬cation.

The evaluation metrics for actor detection and multi-label
classiï¬cation have been well-studied separately [15, 22], but
the problem remains on how to associate them together for
multi-label ASAD evaluation.

A simple approach could be evaluating the â€œactorâ€ ac-
tor detection performance for all detected samples and then
evaluating the multi-label action recognition performance
for positively detected samples. For instance, assuming that
a predicted sample is positive when IoU â‰¥ 0.5 for the pre-
dicted and ground-truth bounding boxes, we can apply ğ»ğ¿@0.5,
which corresponds to Hamming Loss associated with IoU
â‰¥ 0.5, to measure its multi-label classiï¬cation performance.
Note that, due to the object occlusions, the IoU value
between multiple actors could be larger than 0.5. To re-
move such ambiguity, we apply the Hungarian Algorithm
for bipartite matching between the predicted bounding boxes
and the ground-truth bounding boxes before comparing their
classiï¬cation results. Meanwhile, a pair that has IoU < 0.5
will be excluded before calculating their Hamming Loss. We
illustrate these cases in Figure 11.

Next, we employ linear assignment [12] to obtain the op-

timal assignment îˆ¹âˆ— with

îˆ¹âˆ— = arg min

îˆ¹

âˆ‘

âˆ‘

ğ‘–

ğ‘—

îˆ°

ğ‘–,ğ‘—

îˆ¹

ğ‘–,ğ‘—,

(8)

where îˆ¹ is a Boolean matrix. When the row ğ‘– (i.e., ground
truth box ğ‘–) is assigned to column ğ‘— (i.e., predicted box ğ‘—),
we have îˆ¹
ğ‘–,ğ‘— = 1. Each row can be assigned to at most one
column and each column to at most one row.

Since matching pairs that have IoU value less than 0.5,

we further process îˆ¹âˆ— by

îˆ¹âˆ—

ğ‘–,ğ‘— =

{ 0,
îˆ¹âˆ—

ğ‘–,ğ‘—,

if îˆ°
ğ‘–,ğ‘— = 1;
otherwise.

(9)

Referring to îˆ¹âˆ—, we select matched pairs to evaluate the
corresponding action labels with Hamming Loss. The num-
(i.e.,
ber of matching pairs are represented by ğ‘ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘ @0.5
îˆ¹âˆ— = 1). Below, we show how the ğ»ğ¿@0.5 is extended
from the original Hamming Loss.

ğ»ğ¿@0.5 =

1
ğ‘ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘ @0.5

1
ğ‘ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘ 

ğ‘ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ@0.5âˆ‘

ğ‘ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘ âˆ‘

ğ‘–=1

ğ‘™=1

ğ‘¡ğ‘Ÿğ‘¢ğ‘’ XOR ğ‘Œ ğ‘–,ğ‘™
ğ‘Œ ğ‘–,ğ‘™

ğ‘ğ‘Ÿğ‘’ğ‘‘ ,

(10)

where XOR is an exclusive-or operation and ğ‘ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘ 
for the number of action categories. ğ‘Œğ‘¡ğ‘Ÿğ‘¢ğ‘’
arrays that denote the ground truth and predicted labels, re-
spectively.

stands
are boolean

and ğ‘Œğ‘ğ‘Ÿğ‘’ğ‘‘

7. Experiment

We set up the ï¬rst benchmark for the ASAD study. Ex-
periments are conducted on our A-AVA dataset and evalu-
ated by our ASAD metrics.

7.1. ASAD Framework

As we have discussed in related works, some SAD mod-
els, such as ROAD [63], AlphAction [66], and ACAM [69],

F.Yang et al.: Preprint submitted to Elsevier

Page 8 of 13

IoU= 0.52IoU= 0.15IoU= 0IoU= 0Bounding Boxes:Matched Ground Truth [Green]Ignored Ground Truth [Grey]Matched Prediction [Red]Ignored Prediction [Red]IoU= 0.7IoU= 0.5IoU= 0.6IoU= 0.5ASAD

Figure 12: Overview of the basic ASAD framework.

Approaches

ASAD Baseline
w/ online MOT
ASAD Baseline
w/ oï¬„ine MOT

Actor Detection Evaluation Action Classiï¬cation Evaluation

Actor Identiï¬cation Evaluation

AP@0.5 (%)â†‘

HL@0.5 (0âˆ¼1)â†“

IDF1 (%) â†‘ MT (%)â†‘ ML (%)â†“ # ID Sw.â†“

72.4

72.4

0.06

0.06

60.4

71.4

67.3

88.4

10.5

5.2

413

273

Table 6
Comparison of using diï¬€erent MOT modules in ADAD frameworks. We utilize our A-AVA
dataset and ASAD evaluation metrics, where â†‘(â†“) indicates that the larger(smaller) the
value is, the better the performance.

are consist of Multiple Object Tracking (MOT) and Action
Classiï¬cation (AC) modules. They could generate ASAD
results but were evaluated by the SAD protocol in the origi-
nal works. Based on the evaluation protocol of SAD, the an-
notation of actor identity may not be provided and the actor
identiï¬cation has not been evaluated. In other words, there
is no clear boundary between ASAD and SAD in terms of
the method, their diï¬€erence more lies in the data annotation
and evaluation protocols.

Without changing the basic structure, letting the above
SAD methods to output actor identities with their original
outputs can make ASAD frameworks. In this study, we let
the oï¬€-the-shelf SAD methods to output actor identities that
are generated by their MOT module. In this manner, they can
perform as ASAD frameworks. In Figure 12, we summarize
the basic structure of ASAD framework that is adapted from
SAD models. Generally, an ASAD framework takes RGB
videos as the input and outputs the bounding boxes, unique
actor identity, and actions of each actor.

7.2. Experiment Results

By using our A-AVA dataset, the performance of spa-
tiotemporal detection, action classiï¬cation, and actor identi-
ï¬cation can be jointly evaluated. To generate a better actor
identiï¬cation result, we may need to focus on the data asso-
ciation strategy in MOT. We ï¬rst evaluated our ASAD base-
line with online MOT methods [77]. We showed the result
of using our ASAD evaluation metrics in Table 6. It can be
noticed that the action identiï¬cation performance is unsat-
isfactory. Using online MOT methods becomes the bottle-
neck to obtain satisfactory ASAD results. We then replaced
the online MOT module to be an oï¬„ine MOT method intro-
duced in [82]. Consequently, using an oï¬„ine MOT module
led to better actor identiï¬cation result: the oï¬„ine MOT mod-
ule gave a further gain in IDF1 and ML over the online MOT
module, and also reduced the ML and ID Switches.

7.3. Discussion

Why does oï¬„ine MOT have better performance in our
A-AVA dataset in terms of actor identiï¬cation? For static
camera recording, motion consistency is an important cue
for data association. In contrast, for non-static camera record-
ing, the motion consistency assumption could be failed. When-
ever the viewpoint suddenly changes in videos, it is chal-
lenging to track the correct actor identities. This issue fre-
quently happens in the movies and phone-recorded videos
(Figure 13). Most online MOT methods (e.g., [77]) may
have an over-reliance on the motion consistency and there-
fore cause failure cases in our A-AVA dataset. Employing
oï¬„ine MOT alleviates this issue by determining the corre-
spondence between observations more by their appearance
similarity. Moreover, applying an oï¬„ine MOT solution uti-
lizes the global information to further reduce ID switches
and generate robust actor identiï¬cation results.

8. Conclusion

In this paper, we introduced a novel task, Actor-identiï¬ed
Spatiotemporal Action Detection (ASAD), which marks the
ï¬rst eï¬€ort in the computer vision community to jointly study
spatiotemporal boundaries, actor identities, and correspond-
ing actions. ASAD is ideal for action recognition applica-
tions when multiple actors are included, such as Human-
computer Interaction, basketball/soccer games, and grocery
operations monitoring, etc. To study ASAD, we are excited
to oï¬€er a corresponding A-AVA dataset. As the ï¬rst dataset
that is speciï¬cally designed for the ASAD study, the A-AVA
dataset has bridged the gap between the SAD dataset and the
actor identiï¬cation dataset. We also proposed ASAD eval-
uation metrics by considering multi-label actions and actor
identiï¬cation. It is the ï¬rst evaluation metric in ASAD such
a complicated task.

Besides the above success, it is important to note that our

ASAD study also suï¬€ers some limitations:

F.Yang et al.: Preprint submitted to Elsevier

Page 9 of 13

Object DetectionData AssociationMultiple Object TrackingAction ClassificationVideosASAD Results(Spatiotemporal Boundaries,Actions,Actor IDs.)ASAD

Figure 13: The diï¬€erence of motion consistency in static camera recording videos and
non-static camera recording videos.

â€¢ Considering the high annotation cost, the size of our
proposed ASAD dataset is still relatively small. Mean-
while, since the deï¬nition of action labels could be
ambiguous, the action annotation may not be accurate.
For instance, it is diï¬ƒcult to judge the boundary be-
tween â€œwalkâ€ and â€runningâ€ in the continuous tempo-
ral domain. Or, without including the audio informa-
tion, it is challenging to decide who is speaking. Such
issues may impair the ASAD study. To cope with this
issue, it is necessary to perform high-quality annota-
tions with more annotators involved.

â€¢ Because evaluating the ASAD result is complicated,
we separately evaluated spatial detection, actor identi-
ï¬cation, and multi-label action classiï¬cation. Conse-
quently, the overall ASAD performance is represented
by multiple metric values. However, in an ideal case,
we hope to utilize a single metric value to represent the
overall ASAD performance. Considering that each of
our ASAD metrics (e.g., HL@0.5) is obtained from a
complex formula, it is challenging to integrate them
into a single metric value. To ï¬nd a solution, further
exploration is needed. Since we have raised this ques-
tion in the ASAD task, it might be solved in future
works.

And we are considering addressing those issues in future
works.

This paper is not the end, but rather the starting steps,
we are excited to engage with the research community to
explore ASAD deeper. We believe considering actor identi-
ï¬cation with spatiotemporal action detection could promote
the research on video understanding and beyond.

References
[1] Abu-El-Haija, S., Kothari, N., Lee,

J., Natsev, P., Toderici,
G., Varadarajan, B., Vijayanarasimhan, S., 2016. Youtube-8m:
A large-scale video classiï¬cation benchmark.
arXiv preprint
arXiv:1609.08675 URL: https://arxiv.org/abs/1609.08675.

[2] Barekatain, M., MartÃ­, M., Shih, H.F., Murray, S., Nakayama, K.,
Matsuo, Y., Prendinger, H., 2017. Okutama-action: An aerial view
video dataset for concurrent human action detection, in: 1st Joint
BMTT-PETS Workshop on Tracking and Surveillance, CVPR, pp.
1â€“8.

[3] Bewley, A., Ge, Z., Ott, L., Ramos, F., Upcroft, B., 2016. Simple
online and realtime tracking, in: 2016 IEEE International Conference
on Image Processing (ICIP), pp. 3464â€“3468. doi:10.1109/ICIP.2016.
7533003.

[4] Bochinski, E., Eiselein, V., Sikora, T., 2017. High-speed tracking-by-
detection without using image information, in: International Work-
shop on Traï¬ƒc and Street Surveillance for Safety and Security at
IEEE AVSS 2017, Lecce, Italy. URL: http://elvera.nue.tu-berlin.
de/files/Bochinski2017.pdf.

[5] BrasÃ³, G., Leal-TaixÃ©, L., 2020. Learning a neural solver for multi-
ple object tracking, in: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 6247â€“6257.

[6] Carreira, J., Noland, E., Hillier, C., Zisserman, A., 2019. A short
arXiv preprint

note on the kinetics-700 human action dataset.
arXiv:1907.06987 URL: https://arxiv.org/abs/1907.06987.

[7] Carreira, J., Zisserman, A., 2017. Quo vadis, action recognition? a
new model and the kinetics dataset, in: IEEE Conference on Com-
puter Vision and Pattern Recognition, IEEE. pp. 4724â€“4733.

[8] Chang, M.F., Lambert, J., Sangkloy, P., Singh, J., Bak, S., Hartnett,
A., Wang, D., Carr, P., Lucey, S., Ramanan, D., et al., 2019. Argov-
erse: 3d tracking and forecasting with rich maps, in: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 8748â€“8757.

[9] Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross, D.A., Deng,
J., Sukthankar, R., 2018. Rethinking the faster r-cnn architecture for
temporal action localization, in: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1130â€“1139.
[10] Chaquet, J.M., Carmona, E.J., FernÃ¡ndez-Caballero, A., 2013. A sur-
vey of video datasets for human action and activity recognition. Com-
puter Vision and Image Understanding 117, 633â€“659.

[11] Choutas, V., Weinzaepfel, P., Revaud, J., Schmid, C., 2018. Potion:
Pose motion representation for action recognition, in: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7024â€“7033.

[12] Crouse, D.F., 2016. On implementing 2d rectangular assignment al-
gorithms. IEEE Transactions on Aerospace and Electronic Systems
52, 1679â€“1696.

[13] Dai, P., Wang, X., Zhang, W., Chen, J., 2018. Instance segmentation
enabled hybrid data association and discriminative hashing for online
multi-object tracking. IEEE Transactions on Multimedia 21, 1709â€“
1723.

F.Yang et al.: Preprint submitted to Elsevier

Page 10 of 13

Staticcamerarecording:Non-staticcamerarecording:ASAD

[14] Dave, A., Khurana, T., Tokmakov, P., Schmid, C., Ramanan, D., 2020.
Tao: A large-scale benchmark for tracking any object. arXiv preprint
arXiv:2005.10356 URL: https://arxiv.org/abs/2005.10356.

[15] Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman,
A., 2010. The pascal visual object classes (voc) challenge. Interna-
tional journal of computer vision 88, 303â€“338.

[16] Fabian Caba Heilbron, Victor Escorcia, B.G., Niebles, J.C., 2015.
Activitynet: A large-scale video benchmark for human activity un-
derstanding, in: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 961â€“970.

[17] Feichtenhofer, C., Fan, H., Malik, J., He, K., 2019. Slowfast networks
for video recognition, in: Proceedings of the IEEE international con-
ference on computer vision, pp. 6202â€“6211.

[18] Feichtenhofer, C., Pinz, A., Zisserman, A., 2016. Convolutional two-
stream network fusion for video action recognition, in: Proceedings
of the IEEE conference on computer vision and pattern recognition,
pp. 1933â€“1941.

[19] Fernando, T., Denman, S., Sridharan, S., Fookes, C., 2018. Tracking
by prediction: A deep generative model for mutli-person localisation
and tracking, in: 2018 IEEE Winter Conference on Applications of
Computer Vision (WACV), IEEE. pp. 1122â€“1132.

[20] Fu, Z., Angelini, F., Chambers, J., Naqvi, S.M., 2019. Multi-level co-
operative fusion of gm-phd ï¬lters for online multiple human tracking.
IEEE Transactions on Multimedia 21, 2277â€“2291.

[21] Gao, J., Yang, Z., Nevatia, R., 2017. Cascaded boundary regression
for temporal action detection, in: British Machine Vision Conference
2017, BMVC 2017, London, UK, September 4-7, 2017, BMVA Press.
[22] Gibaja, E., Ventura, S., 2015. A tutorial on multilabel learning. ACM

Computing Surveys 47, 52.

[23] Girdhar, R., Carreira, J., Doersch, C., Zisserman, A., 2018. A bet-
ter baseline for ava. arXiv preprint arXiv:1807.10066 URL: https:
//arxiv.org/abs/1807.10066.

[24] Gkioxari, G., Malik, J., 2015. Finding action tubes, in: Proceedings
of the IEEE conference on computer vision and pattern recognition,
pp. 759â€“768.

[25] Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vi-
jayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., et al.,
2017. Ava: A video dataset of spatio-temporally localized atomic
visual actions. CoRR, abs/1705.08421 4.

[26] Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vi-
jayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., et al.,
2018. Ava: A video dataset of spatio-temporally localized atomic vi-
sual actions, in: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 6047â€“6056.

[27] Gu, Y., Ye, X., Sheng, W., Ou, Y., Li, Y., 2020. Multiple stream
deep learning model for human action recognition. Image and Vision
Computing 93, 103818.

[28] Han, L., Wu, X., Liang, W., Hou, G., Jia, Y., 2010. Discriminative
human action recognition in the learned hierarchical manifold space.
Image and Vision Computing 28, 836â€“849.

[29] Hornakova, A., Henschel, R., Rosenhahn, B., Swoboda, P., 2020.
Lifted disjoint paths with application in multiple object tracking, in:
The 37th International Conference on Machine Learning (ICML), pp.
1â€“12.

[30] Hosang, J., Benenson, R., DollÃ¡r, P., Schiele, B., 2015. What makes
for eï¬€ective detection proposals? IEEE transactions on pattern anal-
ysis and machine intelligence 38, 814â€“830.

[31] Hou, R., Chen, C., Shah, M., 2017. Tube convolutional neural net-
work (t-cnn) for action detection in videos, in: Proceedings of the
IEEE international conference on computer vision, pp. 5822â€“5831.

[32] Huang, Q., Xiong, Y., Lin, D., 2018. Unifying identiï¬cation and
context learning for person recognition, in: The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 2217â€“2225.
[33] Hutchinson, M., Gadepally, V., 2020. Video action understanding: A
tutorial. arXiv preprint arXiv:2010.06647 URL: https://arxiv.org/
abs/2010.06647.

[34] Idrees, H., Zamir, A.R., Jiang, Y., Gorban, A., Laptev, I., Sukthankar,
R., Shah, M., 2017. The thumos challenge on action recognition for

videos â€œin the wildâ€. Computer Vision and Image Understanding 155,
1â€“23.

[35] Jiang, X., Li, P., Li, Y., Zhen, X., 2019. Graph neural based end-to-
end data association framework for online multiple-object tracking.
arXiv preprint arXiv:1907.05315 URL: https://arxiv.org/abs/1907.
05315.

[36] Kalogeiton, V., Weinzaepfel, P., Ferrari, V., Schmid, C., 2017. Action
tubelet detector for spatio-temporal action localization, in: Proceed-
ings of the IEEE International Conference on Computer Vision, pp.
4405â€“4413.

[37] Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-
Fei, L., 2014. Large-scale video classiï¬cation with convolutional neu-
ral networks, in: Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition, pp. 1725â€“1732.

[38] Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T., 2011.
Hmdb: a large video database for human motion recognition, in: 2011
International Conference on Computer Vision, IEEE. pp. 2556â€“2563.
[39] Li, A., Thotakuri, M., Ross, D.A., Carreira, J., Vostrikov, A., Zis-
serman, A., 2020a. The ava-kinetics localized human actions video
dataset. arXiv preprint arXiv:2005.00214 URL: https://arxiv.org/
abs/2005.00214.

[40] Li, C., Zhong, Q., Xie, D., Pu, S., 2018. Co-occurrence feature learn-
ing from skeleton data for action recognition and detection with hier-
archical aggregation, in: Proceedings of the 27th International Joint
Conference on Artiï¬cial Intelligence, pp. 786â€“792.

[41] Li, J., Gao, X., Jiang, T., 2020b. Graph networks for multiple object
tracking, in: The IEEE Winter Conference on Applications of Com-
puter Vision, pp. 719â€“728.

[42] Li, J., Wei, L., Zhang, F., Yang, T., Lu, Z., 2019. Joint deep and depth
for object-level segmentation and stereo tracking in crowds.
IEEE
Transactions on Multimedia 21, 2531â€“2544.

[43] Li, Y., Lin, W., See, J., Xu, N., Xu, S., Yan, K., Yang, C., 2020c. Cfad:
Coarse-to-ï¬ne action detector for spatiotemporal action localization,
in: European Conference on Computer Vision, Springer. pp. 510â€“527.
[44] Li, Y., Wang, Z., Wang, L., Wu, G., 2020d. Actions as moving points.

arXiv preprint arXiv:2001.04608 .

[45] Lin, T., Liu, X., Li, X., Ding, E., Wen, S., 2019. Bmn: Boundary-
matching network for temporal action proposal generation, in: Pro-
ceedings of the IEEE International Conference on Computer Vision,
pp. 3889â€“3898.

[46] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
DollÃ¡r, P., Zitnick, C.L., 2014. Microsoft coco: Common objects in
context, in: European conference on computer vision, Springer. pp.
740â€“755.

[47] Liu, C., Yuen, P.C., 2010. Human action recognition using boosted

eigenactions. Image and vision computing 28, 825â€“835.

[48] Liu, Z., Zhang, C., Tian, Y., 2016. 3d-based deep convolutional neu-
ral network for action recognition with depth sequences. Image and
Vision Computing 55, 93â€“100.

[49] Luo, W., Xing, J., Milan, A., Zhang, X., Liu, W., Zhao, X., Kim, T.K.,
2014. Multiple object tracking: A literature review. arXiv preprint
arXiv:1409.7618 .

[50] Ma, C., Li, Y., Yang, F., Zhang, Z., Zhuang, Y., Jia, H., Xie, X., 2019.
Deep association: End-to-end graph-based learning for multiple ob-
ject tracking with conv-graph neural network, in: Proceedings of the
2019 on International Conference on Multimedia Retrieval, pp. 253â€“
261.

[51] Ma, L., Tang, S., Black, M.J., Gool, L.V., 2018. Customized multi-

person tracker, in: ACCV.

[52] Masoud, O., Papanikolopoulos, N., 2003. A method for human action

recognition. Image and Vision Computing 21, 729â€“743.

[53] Mettes, P., Van Gemert, J.C., Snoek, C.G., 2016. Spot on: Action
localization from pointly-supervised proposals, in: European confer-
ence on computer vision, Springer. pp. 437â€“453.

[54] Milan, A., Leal-TaixÃ©, L., Reid, I., Roth, S., Schindler, K., 2016.
Mot16: A benchmark for multi-object tracking. arXiv:1603.00831
[cs] URL: http://arxiv.org/abs/1603.00831.

[55] Pan, J., Chen, S., Shou, Z., Shao, J., Li, H., 2020.

Actor-

F.Yang et al.: Preprint submitted to Elsevier

Page 11 of 13

ASAD

context-actor relation network for spatio-temporal action localization.
arXiv preprint arXiv:2006.07976 URL: https://arxiv.org/pdf/2006.
07976v2.pdf.

[56] Qiu, Z., Yao, T., Mei, T., 2017. Learning spatio-temporal representa-
tion with pseudo-3d residual networks, in: proceedings of the IEEE
International Conference on Computer Vision, pp. 5533â€“5541.

[57] Qu, W., Schonfeld, D., Mohamed, M., 2007.

Real-time dis-
tributed multi-object tracking using multiple interactive trackers and a
magnetic-inertia potential model. IEEE Transactions on Multimedia
9, 511â€“519.

[58] Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C., 2016. Per-
formance measures and a data set for multi-target, multi-camera track-
ing, in: European Conference on Computer Vision, Springer. pp. 17â€“
35.

[59] Ristani, E., Tomasi, C., 2018. Features for multi-target multi-camera
tracking and re-identiï¬cation, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 6036â€“6046.

[60] Satoh, S., 1999. Towards actor/actress identiï¬cation in drama videos,
in: Proceedings of the seventh ACM international conference on Mul-
timedia (Part 2), pp. 75â€“78.

[61] Shi, L., Zhang, Y., Cheng, J., Lu, H., 2019. Two-stream adaptive
graph convolutional networks for skeleton-based action recognition,
in: Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pp. 12026â€“12035.

[62] Simonyan, K., Zisserman, A., 2014. Two-stream convolutional net-
works for action recognition in videos, in: Advances in neural infor-
mation processing systems, pp. 568â€“576.

[63] Singh, G., Saha, S., Sapienza, M., Torr, P.H., Cuzzolin, F., 2017. On-
line real-time multiple spatiotemporal action localisation and predic-
tion, in: Proceedings of the IEEE International Conference on Com-
puter Vision, pp. 3637â€“3646.

[64] Soomro, K., Idrees, H., Shah, M., 2015. Action localization in videos
through context walk, in: Proceedings of the IEEE international con-
ference on computer vision, pp. 3280â€“3288.

[65] Soomro, K., Zamir, A.R., Shah, M., 2012. Ucf101: A dataset of
101 human actions classes from videos in the wild. arXiv preprint
arXiv:1212.0402 URL: https://arxiv.org/abs/1212.0402.

[66] Tang, J., Xia, J., Mu, X., Pang, B., Lu, C., 2020. Asynchronous inter-
action aggregation for action detection, in: Proceedings of the Euro-
pean conference on computer vision (ECCV).

[67] Tang, S., Andriluka, M., Andres, B., Schiele, B., 2017. Multiple peo-
ple tracking by lifted multicut and person re-identiï¬cation, in: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3539â€“3548.

[68] Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M., 2015.
Learning spatiotemporal features with 3d convolutional networks, in:
Proceedings of the IEEE international conference on computer vision,
pp. 4489â€“4497.

[69] Ulutan, O., Rallapalli, S., Srivatsa, M., Torres, C., Manjunath, B.,
2020. Actor conditioned attention maps for video action detection, in:
The IEEE Winter Conference on Applications of Computer Vision,
pp. 527â€“536.

[70] Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool,
L., 2016. Temporal segment networks: Towards good practices for
deep action recognition, in: European conference on computer vision,
Springer. pp. 20â€“36.

[71] Wang, X., Zhang, X., Zhu, Y., Guo, Y., Yuan, X., Xiang, L., Wang,
Z., Ding, G., Brady, D., Dai, Q., et al., 2020. Panda: A gigapixel-
level human-centric video dataset, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 3268â€“
3278.

[72] Wang, Z., Zheng, L., Liu, Y., Wang, S., 2019. Towards real-time

multi-object tracking. arXiv preprint arXiv:1909.12605 .

[73] Weinzaepfel, P., Harchaoui, Z., Schmid, C., 2015. Learning to track
for spatio-temporal action localization, in: Proceedings of the IEEE
international conference on computer vision, pp. 3164â€“3172.

[74] Weinzaepfel, P., Martin, X., Schmid, C., 2016.

tion localization with sparse spatial supervision.

Human ac-
arXiv preprint

arXiv:1605.05197 URL: https://arxiv.org/pdf/1605.05197.pdf.
[75] Wen, L., Li, W., Yan, J., Lei, Z., Yi, D., Li, S.Z., 2014. Multiple tar-
get tracking based on undirected hierarchical relation hypergraph, in:
Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1282â€“1289.

[76] Weng, X., Wang, Y., Man, Y., Kitani, K.M., 2020. Gnn3dmot: Graph
neural network for 3d multi-object tracking with 2d-3d multi-feature
learning, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR).

[77] Wojke, N., Bewley, A., Paulus, D., 2017. Simple online and realtime
tracking with a deep association metric. 2017 IEEE International Con-
ference on Image Processing (ICIP) , 3645â€“3649.

[78] Xu, H., Das, A., Saenko, K., 2017. R-c3d: Region convolutional 3d
network for temporal activity detection, in: Proceedings of the IEEE
international conference on computer vision, pp. 5783â€“5792.
[79] Xu, M., Yuan, X., Shen, J., Yan, S., 2010. Cast2face: Character iden-
tiï¬cation in movie with actor-character correspondence, in: Proceed-
ings of the 18th ACM international conference on Multimedia, pp.
831â€“834.

[80] Yan, A., Wang, Y., Li, Z., Qiao, Y., 2019. Pa3d: Pose-action 3d ma-
chine for video recognition, in: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 7922â€“7931.
[81] Yang, F., Chang, X., Dang, C., Zheng, Z., Sakti, S., Nakamura,
S., Wu, Y., 2020. Remots: Self-supervised reï¬ning multi-object
tracking and segmentation. URL: https://arxiv.org/abs/2007.03200,
arXiv:2007.03200.

[82] Yang, F., Chang, X., Sakti, S., Wu, Y., Nakamura, S., 2021. Remot:
A model-agnostic reï¬nement for multiple object tracking. Image and
Vision Computing 106, 104091.

[83] Yang, F., Sakti, S., Wu, Y., Nakamura, S., 2019a. A framework for
knowing who is doing what in aerial surveillance videos. IEEE Ac-
cess 7, 93315â€“93325.

[84] Yang, F., Wu, Y., Sakti, S., Nakamura, S., 2019b. Make skeleton-
based action recognition model smaller, faster and better, in: Proceed-
ings of the ACM Multimedia Asia, pp. 1â€“6.

[85] Yang, X., Yang, X., Liu, M.Y., Xiao, F., Davis, L.S., Kautz, J., 2019c.
Step: Spatio-temporal progressive learning for video action detection,
in: Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pp. 264â€“272.

[86] Yooyoung, Y., Fiscus, J., Godil, A., Joy, D., Delgado, A., Golden,
J., 2019. Actev18: Human activity detection evaluation for ex-
tended videos, in: 2019 IEEE Winter Applications of Computer Vi-
sion Workshops (WACVW), IEEE. pp. 1â€“8.

[87] Yuan, Y., Qi, L., Lu, X., 2016. Action recognition by joint learning.

Image and Vision Computing 55, 77â€“85.

[88] Zhang, L., Li, Y., Nevatia, R., 2008. Global data association for multi-
object tracking using network ï¬‚ows, in: Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pp. 1â€“8.

[89] Zhang, S., Wang, J., Wang, Z., Gong, Y., Liu, Y., 2015. Multi-target
tracking by learning local-to-global trajectory models. Pattern Recog-
nition 48, 580â€“590.

[90] Zhang, Y., Wang, C., Wang, X., Zeng, W., Liu, W., 2020. A simple
baseline for multi-object tracking. arXiv preprint arXiv:2004.01888
URL: https://arxiv.org/abs/2004.01888.

[91] Zhao, H., Torralba, A., Torresani, L., Yan, Z., 2019. Hacs: Human
action clips and segments dataset for recognition and temporal lo-
calization, in: Proceedings of the IEEE International Conference on
Computer Vision, pp. 8668â€“8678.

[92] Zhou, X., Koltun, V., KrÃ¤henbÃ¼hl, P., 2020. Tracking objects as

points. ECCV .

[93] Zhu, H., Vial, R., Lu, S., 2017. Tornado: A spatio-temporal convolu-
tional regression network for video action proposal, in: Proceedings
of the IEEE International Conference on Computer Vision, pp. 5813â€“
5821.

[94] Zolfaghari, M., Oliveira, G.L., Sedaghat, N., Brox, T., 2017. Chained
multi-stream networks exploiting pose, motion, and appearance for
action classiï¬cation and detection, in: Proceedings of the IEEE Inter-
national Conference on Computer Vision, pp. 2904â€“2913.

F.Yang et al.: Preprint submitted to Elsevier

Page 12 of 13

[95] Zolfaghari, M., Singh, K., Brox, T., 2018. Eco: Eï¬ƒcient convolu-
tional network for online video understanding, in: Proceedings of the
European conference on computer vision (ECCV), pp. 695â€“712.

ASAD

F.Yang et al.: Preprint submitted to Elsevier

Page 13 of 13

