A Novel Data Pre-processing Technique: Making Data Mining
Robust to Diﬀerent Units and Scales of Measurement

Arbind Agrahari Baniya1, Sunil Aryal1 and Santosh KC2

1Deakin University, Geelong, Victoria, Australia
{a.agraharibaniya,sunil.aryal}@deakin.edu.au
2University of South Dakota, Vermillion, SD, USA
santosh.kc@usd.edu

1
2
0
2

v
o
N
8

]

G
L
.
s
c
[

1
v
3
5
2
4
0
.
1
1
1
2
:
v
i
X
r
a

Abstract. Many existing data mining algorithms use feature values directly in their model, making them
sensitive to units/scales used to measure/represent data. Pre-processing of data based on rank transfor-
mation has been suggested as a potential solution to overcome this issue. However, the resulting data after
pre-processing with rank transformation is uniformly distributed, which may not be very useful in many
data mining applications. In this paper, we present a better and eﬀective alternative based on ranks over
multiple sub-samples of data. We call the proposed pre-processing technique as ARES — Average Rank
over an Ensemble of Sub-samples. Our empirical results of widely used data mining algorithms for classi-
ﬁcation and anomaly detection in a wide range of data sets suggest that ARES results in more consistent
task speciﬁc outcome across various algorithms and data sets. In addition to this, it results in better or
competitive outcome most of the time compared to the most widely used min-max normalisation and the
traditional rank transformation.

Keywords: Data measurement, Units and scales of measurement, Data pre-processing, Normalisation,
Rank Transformation

1 Introduction

In databases, data objects represent real-world entities deﬁned by a set of selected features or properties. For
example, people can be deﬁned by their name, age, salary etc. and cars can be represented by model, price,
fuel eﬃciency etc. Each data record/instance represents an instance of real-world entity deﬁned by their values
of the selected features. Features can have numeric or categorical values. In this paper, we consider the case
where features have numeric values. Let D be a collection of N data instances {x(i)}N
i=1. Each data instance x
is represented as a vector of its values of M features (cid:104)x1, x2, · · · , xM (cid:105) where ∀j xj ∈ R and R is a real domain.
In real-world applications, numeric features are often measured or recorded in some units or scales [1,2,3,4].
For example, annual income of people can be recorded in integer scale as x = 100, 000 or using a logarithmic
scale of base 10 like x(cid:48) = 5. Sample variability can be measured in terms of standard deviation (std) or variance
var or var = std2). Similarly, fuel eﬃciency of cars can be measured in km/ltr as x = 9.0 or
(var) (std =
ltr/100km as x(cid:48) = 11.11 where one is the inverse of the other.

√

Most of the existing data mining algorithms use feature values of data directly to build their models. They
assume that data are embedded in an M -dimensional Euclidean space and use the spatial positions of data in
the space. Many of them compute distances between data points and/or model density distribution of data.
Because feature values depend on how features are measured, existing algorithms are sensitive to units and
scales of measurement of features. They might give diﬀerent patterns if data are measured in diﬀerent units
and/or scales. For example, Fig 1 shows the distributions of annual incomes of 500 individuals in two scales
- normal integer scale and logarithmic scale of base 10. The two distributions look very diﬀerent from each
other. Similarly, distances between data may also be changed if they are expressed/measured diﬀerently. For
example, if x = 1, y = 2 and z = 3, y − x = z − y = 1. However, if the same data are provided as their squares
(x(cid:48) = 1, y(cid:48) = 4 and z(cid:48) = 9), y(cid:48) − x(cid:48) < z(cid:48) − y(cid:48).

The task-speciﬁc performances of many existing data mining algorithms depend on how data are measured.
They may perform poorly if data are not measured in appropriate units or scales. Unfortunately, information
regarding units and scales of feature values are often not available during data mining, instead only the mag-
nitudes of feature values are available. Even if known, the units and/or scales used for measurement may not
be appropriate for the task at hand. Data need to be pre-processed to transform into an appropriate scale
before using existing data mining algorithms. The simplest way to identify appropriate scale is to try diﬀerent
transformations and test which one produces the best task speciﬁc result. Because there is an inﬁnite number
of possible transformations, it is not feasible to ﬁnd best transformation from trial and error [5].

One simple solution is to use ranks of feature values. Rank transformation [6] is robust to units and scales of
measurement as it either preserves or reverses the ranks of data even though they are expressed diﬀerently. The
main issue with the rank transformation is that it makes the distribution of resulting data uniform because the

 
 
 
 
 
 
2

Agrahari Baniya et. al.

(a) Integer scale (×10, 000)

(b) Log scale of base 10

Fig. 1: Distributions of annual incomes (in dollars) of 500 individuals in integer scale and logarithmic scale.

rank diﬀerence between two consecutive points is always 1 regardless of diﬀerence in the magnitude. Since the
relative diﬀerences of data instances are lost, rank transformation may not be appropriate in some data mining
applications.

To overcome the above mentioned limitation of the rank transformation, we proposed a new variant of rank
transformation using an ensemble approach. Instead of computing rank of data point x among all N points in
D, we propose to compute the ranks of x in t sub-samples Dj ⊂ D (j = 1, 2, · · · , t) where |Dj| = ψ << N
and use the average rank as its ﬁnal transformed value. We call the proposed transformation technique Average
Rank over an Ensemble of Sub-samples (ARES). Similar to the rank transformation, ARES is also robust to
units and scales of measurement. However, it also preserves the relative diﬀerences between data points to some
extent.

Our empirical results in the classiﬁcation and anomaly detection tasks using diﬀerent algorithms in a wide
range of data sets show that pre-processing using ARES results in either better or competitive task speciﬁc
results compared to widely used min-max normalisation and rank transformation based pre-processing of data.
Furthermore, rank and ARES transformations are both robust to units and scales of measurement of raw data
whereas min-max normalisation is not.

The rest of the paper is organised as follows. Section 2 provides a brief review of previous works related to
this paper. The proposed method is discussed in Section 3 followed by experimental results in Section 4 and
conclusions in the last section.

2 Related work

Psychologist S. S. Stevens (1946) [1] discussed four types of scales of measurement and their statistical properties
which have been largely argued by many researchers. Since then, the scales of measurement has been a subject
of discussion mainly in the measurement theory and among psychology communities. Their argument is that
raw numbers can be misleading as they could have been measured/presented in diﬀerent ways. One should not
conclude anything from a given set of numbers without understanding where they come from and the underlying
process of generating them [7,8,9]. Although this issue is equally important if not more in automatic pattern
analysis, it has been hardly discussed in the data mining literature.

Velleman and Wilkinson (1993) [9] argued that good data analysis does not assume data types and scales
because data may not be what we see. Joiner (1981) [10] provided some examples of ‘lurking variables’ — data
appear to have one type whereas in fact hide other information. However, in data mining, numeric data are
assumed to be in ‘interval scale’ [2,5] — a unit diﬀerence has the same meaning irrespective of the values. This
assumption may not be true when data are represented as a non-linear transformations, such as logarithm and
inverse.

Diﬀerent features may have diﬀerent ranges of values. For example, annual income of people can be in the
range of tens of thousands to millions of dollars whereas age is in the range of 1 to 100 years. Since many
data mining algorithms use feature values directly to solve diﬀerent tasks, their analysis can be dominated by
features with larger range of values. To address such dominance, data values in each features are re-scaled to be
in unit range [0,1] [11] before passing them into data mining algorithms. This is referred to as min-max scaling
or normalisation in the literature. Though it resolves the diﬀerences in the ranges of values across features
resulted due to linear scaling (e.g., pounds vs kilograms or ◦C vs ◦F) where interval scale assumption holds, it
does not resolve the diﬀerences in ranges resulted due to non-linear scaling of data (e.g., km/ltr and ltr/100km)
where interval scale assumption is violated.

Recently researchers started to investigate the issue of units and scales of measurement in data mining [2,3].
One potential solution suggested in the literature is to do rank transformation [6] and use ranks instead of actual

ARES: Average Rank over an Ensemble of Sub-samples

3

values. It is robust even to non-linear scaling because the rank is either preserved or reversed. Some prior work
[5,12] show that rank transformation results in better task speciﬁc performances than using raw values with
some data mining algorithms particularly those using distance/similarity of data instances. The main issue with
the rank transformation is that it makes the distribution of resulting data uniform which can be detrimental in
some data mining tasks.

Another line of research related to this area is representation learning [13,14] where the task is to map data
from the given input space into a latent space which maximises the task speciﬁc performance of a given data
mining algorithm. Representation learning can be viewed as learning appropriate transformation that suits best
for the data set and algorithm at hand. However, this approach has some issues: (i) requires extensive learning
which can be computationally expensive in large and/or high-dimensional data sets; (ii) learns representation
appropriate for the given algorithm, representation learned for one algorithm may not be appropriate for others
in the same data set; and (iii) one cannot interpret the meaning of new features and what type of information
they capture.

Previous researches in this area are primarily focused on the algorithm level and developed new algorithms
which are robust to units and scales of measurement [2,3,5,12,4]. In this approach, diﬀerent set of algorithms
need to be developed for each data mining task. We believe a better option would be to work in the data
pre-processing level and develop a pre-processing technique robust to units and scales of measurement so that
existing data mining algorithms can be used as they are. In this paper we propose one such technique to
transform data such that the resulting distribution is similar even though raw data are given in diﬀerent units
and/or scales.

3 The proposed method

To address the issue of rank transformation resulting in uniform distribution of the transformed data, we propose
a new variant of rank transformation using an ensemble approach. In each feature or dimension i, instead of
computing rank of xi among all N values, we propose to aggregate ranks of xi in t sub-samples of values
in dimension i. We call the proposed method as ARES (Average Rank over an Ensemble of Sub-samples)
transformation.

To make the explanation simple, we assume D is a one-dimensional data set where |D| = N . We use t
sub-samples Dj ⊂ D (j = 1, 2, · · · , t) where |Dj| = ψ << N . The transformed value of x, ˜xARES, is the average
rank of x over all t sub-samples.

where r(x|Dj) is the rank of x in Dj:

˜xARES =

1
t

t
(cid:88)

j=1

r(x|Dj)

r(x|Dj) = |{y ∈ Dj : y < x}|

(1)

(2)

Based on the deﬁnition of r(x|Dj) given by Eqn 2, all x ∈ D have ranks in {0, 1, · · · , ψ} in Dj. If
, s(2)
j

are sorted samples in Dj:

, · · · , s(ψ)

j

s(1)
j

r(x|Dj) =






j

if x < s(1)
0
if s(k)
k
ψ if x ≥ s(ψ)

j

j ≤ x < s(k+1)

j

and 1 ≤ k ≤ ψ − 1

(3)

j

j ≤ x < s(k+1)

All x ∈ D such that s(k)

will get the same r(x|Dj) of k regardless of the their diﬀerences
in magnitudes. They can not be diﬀerentiated. However, some of them will have diﬀerent ranks in other sub-
samples. Thus, averaging over diﬀerent sub-samples will maintain their diﬀerences to some extent. For example,
even the case of |Dj| = ψ = 1 where all x ∈ D have ranks either 0 or 1 depending upon either they lie on the
left or right of the sample selected in Dj, doing it multiple times, say t = 10, maintains the diﬀerences between
data points to some extent. Fig 2(a-c) show the distribution of an example data set and its transformations
using traditional rank and ARES. It is clear that ARES (Sub-ﬁgure c) preserves the diﬀerences in data values
in the original distribution (Sub-ﬁgure a) better than the rank transformation using the entire D (Sub-ﬁgure
b).

Like the traditional rank transformation, ARES is also robust to the changes in the scales and units of
measurement. It is a variant of rank transformation using sub-samples in ensemble. Fig 2(d-i) show the same
data as in Sub-ﬁgure (a) in logarithmic1 and inverse scales and their transformations based on the traditional
rank and ARES. From Sub-ﬁgures c, f and i, it is clear that ARES results in the same distribution. Note that

1 The default base of logarithm in this paper is e, i.e., natural logarithm, unless speciﬁed otherwise

4

Agrahari Baniya et. al.

(a) x

(b) rank(x)

(c) ares(x)

(d) log(x)

(e) rank(log(x))

(f) ares(log(x))

(g) inv(x)

(h) rank(inv(x))

(i) ares(inv(x))

Fig. 2: Distributions of an example data set and its logarithmic and inverse transformations and their pre-
processing based on Rank and ARES. Histograms are plotted using the left y-axis and probability density
functions are plotted using the right y-axis

the resulting distribution in the case of inverse scale (Sub-ﬁgure i) is the reverse of that in the original and
logarithmic scales (Sub-ﬁgure c and f). There is some small diﬀerences in the ARES transformations of samples
selected in Dj (i.e., x = s(k)

) in the case of inverse scale because of the ‘<’ sign in Eqn 2.

j

In terms of run-time, ARES requires O(tψ log ψ) time to generate and sort ψ samples t times and O(N t log ψ)
to compute ranks of all N instances in D. The traditional rank transformation requires O(N log N ) time to sort
and rank N instances in D.

4 Empirical evaluation

In this section, we present the results of experiments conducted to evaluate the eﬀectiveness of ARES against
the widely used min-max normalisation and the traditional rank transformation. We evaluated them in terms
of their task speciﬁc performances and robustness to the changes in the units/scales of measurement in the
classiﬁcation and anomaly detection tasks using a wide range of publicly available data sets.

In order to mimic the real-world scenario of possible variations in units/scales in data, monotonic trans-
formations using logarithm, square, square root and inverse as log x, x2,
x were applied to data in
x are not deﬁned, all transformations were applied after
each dimension. To cater for x < 0 where log x and
rescaling feature values in the range of [0,1] in all dimensions. Further, to cater for x = 0 in the case of log x
and 1
x , all monotonic transformations were applied on x(cid:48) = b(x + a) where a = 0.0001 and b = 100. Scaling
using b = 100 was used to change the inter-point distances signiﬁcantly. We used the same settings of monotonic
transformation as discussed in [5].

x and 1

√

√

ARES: Average Rank over an Ensemble of Sub-samples

5

Table 1: Characteristics of data sets used in the classiﬁcation task.

Name
Churn
Corel
Diabetes
Glass
Heart
Miniboone
Occupancy
OpticalDigits
Pageblocks
Pendigits
RobotNavigation
SatImage
SocialNetworkAds
Steelfaults
USTaxPayer

#Features (M )
13
68
9
10
14
51
6
63
11
17
25
37
5
26
10

#Instances (N )
10000
10000
768
214
303
129596
9752
5620
5473
10992
5456
6435
400
1941
1004

#Classes (C)
2
100
2
6
2
2
2
10
5
10
4
6
2
7
3

(a) ANN

(b) KNN

(c) LR

Fig. 3: Average classiﬁcation accuracy over a 10-fold cross validation in the Corel data set.

The proposed method ARES and the traditional rank transformation were implemented in Python using
the Scikit Learn library [15] which has the implementation of the min-max normalisation. Two parameters ψ
and t in ARES were set empirically as default to 7 and 10, respectively.

We discuss the experimental setups and results in the classiﬁcation and anomaly detection tasks separately

in the following two subsections.

4.1 Classiﬁcation

We compared the classiﬁcation accuracies of the three contending pre-processing techniques (min-max nor-
malisation, rank transformation and ARES transformation) in three widely used classiﬁcation algorithms —
Artiﬁcial Neural Network (ANN) [16], K-Nearest Neighbours (KNN) [17] and Logistic Regression (LR) [18]. We
used 15 publicly available data sets whose characteristics are provided in Table 1.

We conducted all classiﬁcation experiments using a 10-fold cross-validation where 9 folds were used as the
training set and the remaining one fold was used as the test set. The same process was repeated 10 times using
each of the 10 folds as the test set. We reported the average classiﬁcation accuracy over 10 runs.

We used the Scikit Learn implementations of all three classiﬁers with default parameters settings except
solvers in ANN and LR which were set to ‘lbfgs’ and ‘saga’, respectively, and ‘multi-class’ in LR which was
set to multinomial, to suit the multi-class classiﬁcation problems. To make the results reproducible we set the
‘random state’ to 0 where applicable.

Because rank and ARES transformations produce the same results with all four monotonic transformations
(there will be some small variations with the inverse transformation due to the issue discussed in Section 3),
we present the results of rank and ARES with the original input scale only. As an example to justify this, the
detailed results of three contending pre-processing techniques with the original scale and the four monotonic
transformations in the Corel data set is provided in Fig 3. The average classiﬁcation accuracies of all three
classiﬁers using three contending pre-processing techniques - min-max (original scale and its four monotonic
transformations), rank (original scale) and ARES (original scale) are provided in Table 2.

It is evident from Fig 3 and Table 2 that the accuracies for each classiﬁer vary with diﬀerent monotonic
transformations when data are normalised with the min-max scaler and the original input data (x) did not always
produce the best accuracy (e.g., Corel, USTaxPayer). This justiﬁes the argument that classiﬁers’ accuracies
vary due to the change in scales of measurement with the traditional min-max normalisation. Also, diﬀerent
classiﬁers produced best accuracy with diﬀerent monotonic transformations. This results suggest that a scale

6

Agrahari Baniya et. al.

Table 2: Average classiﬁcation accuracy over a 10-fold cross validation. Overall best accuracy in each row is bold
faced and the best accuracy result among monotonic transformations in the case of min-max normalisation is
italicised.

Data set

x
85.48
Churn
40.31
Corel
76.57
Diabetes
70.53
Glass
79.59
Heart
92.38
Miniboone
99.27
Occupancy
97.67
OpticalDigits
96.80
Pageblocks
Pendigits
99.20
RobotNavigation 93.73
SatImage
89.40
SocialNetworkAds 89.75
72.34
Steelfaults
32.75
USTaxPayer
80.10
Churn
23.82
Corel
73.70
Diabetes
69.79
Glass
81.19
Heart
92.68
Miniboone
99.47
Occupancy
98.77
OpticalDigits
95.73
Pageblocks
Pendigits
99.25
RobotNavigation 86.14
SatImage
90.78
SocialNetworkAds 90.47
69.04
Steelfaults
32.08
USTaxPayer
80.74
Churn
31.68
Corel
76.70
Diabetes
57.04
Glass
82.54
Heart
91.11
Miniboone
99.24
Occupancy
97.06
OpticalDigits
93.31
Pageblocks
93.85
Pendigits
RobotNavigation 68.88
SatImage
84.46
SocialNetworkAds 82.21
65.85
Steelfaults
34.06
USTaxPayer

k
r
o
w
t
e
N

l
a
r
u
e
N

l
a
i
c
ﬁ
i
t
r
A

)
N
N
A
(

)
N
N
K
(

s
r
u
o
b
h
g
i
e
N
t
s
e
r
a
e
N
K

-

)
R
L
(

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

log(x)
85.80
46.49
74.74
66.83
74.38
90.20
99.04
95.18
97.30
93.71
89.96
82.52
89.74
69.52
36.17
82.27
33.72
66.15
70.61
81.27
92.27
99.54
94.39
96.97
95.73
93.97
90.32
88.75
68.83
33.67
81.49
44.36
66.15
63.60
81.60
89.91
97.98
94.02
96.04
84.40
86.14
81.27
79.23
59.94
34.45

min-max
sq(x)
85.62
32.38
74.62
70.57
77.95
91.89
99.34
97.31
94.35
99.07
88.25
90.04
87.26
70.41
34.76
79.81
13.68
74.09
69.59
81.18
91.10
99.45
98.31
94.03
98.81
82.24
90.66
88.48
65.18
34.25
80.45
19.32
77.09
55.99
81.84
91.11
98.56
96.62
91.52
91.36
65.87
84.52
83.71
65.74
34.95

sqrt(x)
85.81
45.82
76.44
70.65
79.57
92.22
99.27
97.49
97.24
99.02
95.33
86.08
89.25
72.97
32.87
80.79
34.04
70.32
67.55
82.20
92.91
99.50
98.35
96.84
98.57
90.25
90.77
90.72
71.52
33.86
81.23
42.36
75.52
58.34
84.57
90.51
99.05
96.76
95.34
93.51
77.31
84.30
82.73
64.26
33.56

inv(x)
85.61
26.06
67.19
68.46
72.99
84.34
96.20
91.21
96.44
66.68
75.13
64.15
88.79
58.39
33.88
82.20
13.15
63.81
70.22
79.28
88.16
99.33
92.63
96.53
88.90
95.20
82.30
87.00
58.68
31.36
82.02
19.55
65.88
58.84
79.99
79.14
88.59
90.71
94.10
64.58
55.02
52.86
80.49
49.34
34.66

rank
x

ares
x

76.98 84.57
00.92 45.30
59.50 74.09
58.14 70.80
67.58 79.57
71.84 94.35
72.74 99.18
12.94 97.01
89.77 97.13
10.58 99.01
40.32 96.70
41.34 90.07
64.26 88.49
34.11 73.17
32.96 33.47
76.79 82.27
34.68 35.01
73.58 73.69
68.25 69.36
69.29 81.56
92.60 92.91
99.40 99.33
94.11 96.92
96.89 97.00
98.81 98.85
93.31 93.42
90.75 90.46
86.96 88.48
68.01 71.41
33.37 32.79
73.01 82.36
34.80 48.59
62.51 76.44
59.44 63.69
72.95 84.22
90.62 90.93
85.42 98.80
95.52 96.07
96.44 96.71
90.88 93.97
80.46 87.72
81.93 85.41
68.02 80.24
57.60 69.36
34.05 33.67

appropriate for one classiﬁer may not be appropriate for another. With the mix-max normalisation, data need
to be transformed into an appropriate scale that suits the best for the classiﬁer at hand.

In contrast, rank and ARES transformations are robust to such changes in the scales of measurement. ARES
transformation almost always produced better accuracy than the traditional rank transformation (except in the
USTaxPayer data set with Logistic Regression). This could be because of the uniform distribution of the resulting
data in the case of rank transformation. Rank transformation performed really poorly in some data sets (e.g.,
Corel, OpticalDigits, Pendigits, RobotNavigation, SatImage, Steelfaults etc.) with ANN. ARES transformation
produced more consistent and better or competitive to the best classiﬁcation accuracies across diﬀerent classiﬁers
and data sets.

4.2 Anomaly detection

We compared the performance of the three contending pre-processing techniques in 8 publicly available data sets
listed in Table 3 using two widely used anomaly detectors - Isolation Forest (IF) [19] and Local-Outlier Factor
(LOF) [20]. After data pre-processing, instances in a data set where ranked based on their outlier scores using
IF and LOF and the Area Under the Receiver Operating Curve (AUC) is reported as a performance measure.
We used the implementations of IF and LOF provided in Scikit learn with the default parameter settings except
‘n neighbors’ in LOF which was set to (cid:100)
N (cid:101) as recommended by [20].
We observed the similar trend as in the classiﬁcation task. The AUC of both IF and LOF varied with
diﬀerent monotonic transformations in the case of min-max normalisation. Unlike min-max normalisation, both
rank and ARES transformations produced consistent AUC with diﬀerent monotonic transformations. Their
corresponding results in the Annthyroid data set is presented in Figure 4. AUCs of both IF and LOF in all data
sets across all the settings of pre-processings — min-max (original scale and its four monotonic transformations),
rank (original scale) and ARES (original scale), are listed in Table 4. With min-max normalisation, diﬀerent

√

ARES: Average Rank over an Ensemble of Sub-samples

7

Table 3: Characteristics of data sets used in the anomaly detection task.

Name
Annthyroid
Breastw
Ionosphere
Lymph
Mnist
Pima
Satellite
Shuttle

#Features (M )
22
10
33
39
97
9
37
10

#Instances (N )
7200
683
351
148
20444
768
6435
49097

#Anomalies
534
239
126
6
676
268
2036
3511

(a) IF

(b) LOF

Fig. 4: Anomaly detection AUC in the Annthyroid data set.

monotonic transformations produced the best AUCs in diﬀerent data sets for both IF and LOF. ARES produced
more consistent AUCs than any other contender across all data sets. It produced better results than the rank
transformation in many cases except for IF in Mnist and LOF in Annthyroid and Pima.

5 Conclusions and future work

Many existing data mining algorithms use feature values directly in their model making them sensitive to
units/scales used to measure data. Pre-processing of data based on rank transformation has been suggested
as a potential solution to overcome this issue. Because the resulting data after pre-processing is uniformly
distributed, it may not be very useful in many data mining tasks.

To address the above mentioned issue of the traditional rank transformation using the entire data set, we
propose to use ranks in multiple sub-samples of data and then aggregate them. The proposed approach preserves
the diﬀerences in data values in the given original distribution better than the traditional rank transformation.
Our results in classiﬁcation and anomaly detection tasks using widely used data mining algorithms show that
data pre-processing using the proposed approach produces better results than the pre-processing using the
traditional rank transformation. In comparison to the most widely used min-max normalisation based pre-
processing, it produces more consistent results with the change in units/scales of data. Its results are either
better or at least competitive to the min-max normalisation of the resulting data after the best scale mimicked
by the monotonic transformations considered in this paper.

Therefore, it is recommended to pre-process the given data using the proposed approach for a good and
consistent results of existing data mining algorithms particularly when there is uncertainty on how data are
measured or presented.

In future, we would like to test the eﬀectiveness of the proposed data pre-processing technique in: (i) other
data mining tasks such as clustering, (ii) application areas such as IoT, sensor networks and health care where
data could be in diﬀerent units/scales as they are measured by diﬀerent sensors/devices.

References

1. Stevens, S.S.: On the theory of scales of measurement. Science 103(2684) (1946) 677––680
2. Fernando, M.B.T.L.: Data Types and Measurement Scales in Data Analysis. PhD thesis, The faculty of Information

Technology, Monash University, Australia (2017)

3. Aryal, S.: A data-dependent dissimilarity measure: An eﬀective alternative to distance measures. PhD thesis, The

faculty of Information Technology, Monash University, Australia (2017)

4. Aryal, S.: Anomaly detection technique robust to units and scales of measurement. In: Proceedings of the 22nd

Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining. (2018) 589–601

8

Agrahari Baniya et. al.

Table 4: Area Under the ROC Curve (AUC) in the anomaly detection task. Overall best accuracy in each row is
bold faced and the best accuracy result among monotonic transformations in the case of min-max normalisation
is italicised.

Data set

x

Annthyroid 0.6049
0.9892
Breastw
Ionosphere 0.8338
1.0000
Lymph
0.8351
Mnist
0.6650
Pima
0.6585
Satellite
Shuttle
0.9971
Annthyroid 0.6152
Breastw
0.3868
Ionosphere 0.8658
0.9988
Lymph
0.8725
Mnist
0.6189
Pima
0.5685
Satellite
0.5085
Shuttle

t
s
e
r
o
F

n
o
i
t
a
l
o
s
I

)
F
I
(

r
e
i
l
t
u
O

-
l
a
c
o
L

)
F
O
L
(

r
o
t
c
a
F

log(x)
0.6450
0.6255
0.8711
1.0000
0.8111
0.4634
0.7574
0.9967
0.6976
0.3811
0.8662
0.9965
0.8096
0.4217
0.5956
0.5359

min-max
sq(x)
0.5673
0.9930
0.8278
1.0000
0.8225
0.7029
0.6909
0.9962
0.5307
0.4106
0.8518
0.9977
0.8580
0.6059
0.5606
0.5322

sqrt(x)
0.6462
0.9450
0.8590
1.0000
0.8295
0.6171
0.7232
0.9971
0.7284
0.3626
0.8796
0.9988
0.8705
0.5648
0.5864
0.5068

inv(x)
0.5541
0.4097
0.8710
0.9977
0.7748
0.3910
0.7506
0.9968
0.5179
0.4136
0.7913
0.9965
0.6046
0.3770
0.6532
0.5337

ares
rank
x
x
0.6299
0.6268
0.8306
0.6571
0.8587
0.8701
0.7441 1.0000
0.8421 0.8261
0.6109
0.5975
0.6505
0.6572
0.9848
0.9445
0.6040
0.6081
0.4780 0.5123
0.8841 0.8916
0.7758 0.9988
0.8466
0.8463
0.5412
0.5532
0.6219
0.6239
0.5163 0.5852

5. Fernando, T.L., Webb, G.I.: SimUSF: an eﬃcient and eﬀective similarity measure that is invariant to violations of

the interval scale assumption. Data Mining and Knowledge Discovery 31(1) (2017) 264–286

6. Conover, W.J., Iman, R.L.: Rank transformations as a bridge between parametric and nonparametric statistics. The

American Statistician 35(3) (1981) 124—-129

7. Lord, F.M.: On the statistical treatment of football numbers. American Psychologist 8(12) (1953) 750––751
8. Townsend, J.T., Ashby, F.G.: Measurement scales and statistics: The misconception misconceived. Psychological

Bulletin 96(2) (1984) 394––401

9. Velleman, P.F., Wilkinson, L.: Nominal, ordinal, interval, and ratio typologies are misleading. The American

Statistician 47(1) (1993) 65––72

10. Joiner, B.L.: Lurking variables: Some examples. The American Statistician 35(4) (1981) 227––233
11. Duda, R.O., Hart, P.E., Stork, D.G.: Pattern Classiﬁcation (2nd Edition). Wiley-Interscience (2000)
12. Aryal, S., Ting, K.M., Washio, T., Haﬀari, G.: Data-dependent dissimilarity measure: an eﬀective alternative to

geometric distance measures. Knowledge and Information Systems 35(2) (2017) 479—-506

13. Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new perspectives. IEEE Transactions

on Pattern Analysis and Machine Intelligence 35(8) (2013) 1798–1828

14. Zhong, G., Wang, L.N., Ling, X., Dong, J.: An overview on data representation learning: From traditional feature

learning to recent deep learning. The Journal of Finance and Data Science 2(4) (2016) 265 – 278

15. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss,
R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research 12 (2011) 2825–2830

16. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-propagating errors. In Anderson,
J.A., Rosenfeld, E., eds.: Neurocomputing: Foundations of Research. MIT Press, Cambridge, MA, USA (1988)
696–699

17. Cover, T., Hart, P.: Nearest neighbor pattern classiﬁcation. IEEE Transactions on Information Theory 13(1) (1967)

21–27

18. McCullagh, P., Nelder, J.: Generalized linear models. 2nd edn. Chapman and Hall, London (1989)
19. Liu, F., Ting, K.M., Zhou, Z.H.: Isolation forest. In: Proceedings of the Eighth IEEE International Conference on

Data Mining. (2008) 413–422

20. Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J.: LOF: Identifying Density-Based Local Outliers. In: Proceedings

of ACM SIGMOD Conference on Management of Data. (2000) 93–104

