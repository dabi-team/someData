1
2
0
2

r
p
A
0
2

]
P
A

.
t
a
t
s
[

1
v
9
7
8
9
0
.
4
0
1
2
:
v
i
X
r
a

GARCH-UGH: A bias-reduced approach for dynamic
extreme Value-at-Risk estimation in ﬁnancial time series

Hibiki Kaibuchia, Yoshinori Kawasakib & Gilles Stupﬂerc

a The Graduate University for Advanced Studies, Japan
b Institute of Statistical Mathematics, The Graduate University of Advanced Studies, Japan
c Univ Rennes, Ensai, CNRS, CREST - UMR 9194, F-35000 Rennes, France

Abstract

The Value-at-Risk (VaR) is a widely used instrument in ﬁnancial risk management.
The question of estimating the VaR of loss return distributions at extreme levels is an
important question in ﬁnancial applications, both from operational and regulatory per-
spectives; in particular, the dynamic estimation of extreme VaR given the recent past
has received substantial attention. We propose here a two-step bias-reduced estima-
tion methodology called GARCH-UGH (Unbiased Gomes-de Haan), whereby ﬁnancial
returns are ﬁrst ﬁltered using an AR-GARCH model, and then a bias-reduced estima-
tor of extreme quantiles is applied to the standardized residuals to estimate one-step
ahead dynamic extreme VaR. Our results indicate that the GARCH-UGH estimates
are more accurate than those obtained by combining conventional AR-GARCH ﬁlter-
ing and extreme value estimates from the perspective of in-sample and out-of-sample
backtestings of historical daily returns on several ﬁnancial time series.

Keywords: Bias correction · Extreme value theory (EVT) · Financial time series ·
GARCH model · Hill estimator · Value-at-Risk (VaR)

1

Introduction

A major concern in ﬁnancial risk management is to quantify the risk associated to high-
impact, low-probability extreme losses. The most widely known risk measure is Value-at-
Risk (VaR), deﬁned as an extreme quantile of the loss distribution. An estimation of the
unconditional VaR (that is, of the common distribution of returns over time, assumed to
be stationary) is appropriate for the estimation of potential large levels of loss over the
long term, for example with the goal of making long-term investment decisions. On the
other hand, the conditional VaR is more appropriate for day-to-day and short-term risk
management by capturing the dynamics and the key properties of ﬁnancial asset returns
such as volatility clustering and leptokurtosis. The estimation of the conditional VaR, on
which we focus in this paper, therefore gives a better understanding of the riskiness of the
portfolio because this riskiness varies with the changing volatility.

There are two main classes of methods to estimate the conditional VaR. Nonparametric
historical simulation relies on observed data directly, and uses the empirical distribution of
past losses without assuming any speciﬁc distribution, see [12], and [30]. Although histor-
ical simulation is easy to implement, the estimation of extreme quantiles is diﬃcult as the
extrapolation beyond observed returns is impossible because historical simulation essen-
tially assumes that one of the observed returns is expected to be the next period return.
By contrast, the parametric approach generally refers to the use of an econometric model

1

 
 
 
 
 
 
of volatility dynamics such as, among many others, the Generalized Autoregressive Con-
ditional heteroskedasticity (GARCH) model of [6]. These models estimate VaRs reﬂecting
the conditional heteroskedasticity of ﬁnancial data. However, GARCH-type models as-
suming a parametric distribution of the innovation variable tend to underestimate the risk
because this strong parametric assumption is not well-suited to the accurate estimation
of heavy-tailedness in the conditional returns of ﬁnancial time series.

To overcome the problems of purely nonparametric or parametric estimations of ex-
treme VaR, [29] proposes a two-step approach combining a GARCH-type model and Ex-
treme Value Theory (EVT), referred to as GARCH-EVT throughout. EVT focuses only
on the tails and allows the extrapolation beyond available data, while traditional econo-
metric models focus on the whole distribution at the expense of consideration of the tails.
The key idea of the GARCH-EVT method is to estimate the dynamic extreme VaR by ﬁrst
ﬁltering ﬁnancial time series with a GARCH-type model to estimate the current volatility,
and then by applying the EVT method to the standardized residuals for estimating the
tails of the residual distribution. This approach has been widely used to estimate the con-
ditional VaR. For instance [7, 19] ﬁnd GARCH-EVT to give accurate VaR estimates for
standard and extreme quantiles compared with GARCH-type models and unconditional
EVT methods on stock market data collected across the US, Latin America, Europe and
Asia. Being a two-step procedure based on GARCH-type ﬁltering, the accuracy of the
GARCH-EVT approach has been debated. [9] points out that estimates of the conditional
VaR via the GARCH-EVT approach are sensitive to the ﬁtting of a GARCH-type model
to the dataset in the ﬁrst step. On the other hand, [22, 27] have concluded that there is
no evidence of any diﬀerence in the conditional VaR estimates, regardless of the particular
GARCH model selected to ﬁlter ﬁnancial data.

Since the debate on ﬁltering, several modiﬁcations of the conventional GARCH-EVT
method have been suggested in the literature to provide a more accurate calculation of the
residuals before applying the EVT method in the second step. [34] proposes a semipara-
metric version of GARCH-EVT based on quantile regression. [35] adapts the FIGARCH,
HYGARCH and FIAPARCH models to evaluate conditional VaRs for crude oil and gaso-
line market. [4] proposes an approach called realized EVT where returns are pre-whitened
with a high-frequency based volatility model.
[36] develops hybrid time-varying long-
memory GARCH-EVT models by using a variety of fractional GARCH models. To the
best of our knowledge however, little work has been carried out on the EVT step; [18] uses
the skewed t-distribution that is ﬁtted to the standardized residuals from the GARCH step
in order to recover a fully parametric speciﬁcation. Meanwhile, a purely semiparametric
improvement of the EVT step using bias correction methods has not been investigated so
far, even though bias correction should be a key concern in extreme value estimation and
inference, and has an extensive history (see e.g. [8]).

This is the contribution of the present paper. More precisely, in the context of the
estimation of the one-step ahead dynamic extreme VaR, we develop a new methodology
called GARCH-UGH (standing for Unbiased Gomes-de Haan, after [16, 23]) which uses
an asymptotically unbiased estimator of the extreme quantile applied to the standardized
residuals from the GARCH step of [29], instead of the so-called Peaks-Over-Threshold
(POT) method in the GARCH-EVT approach. We analyze the performance of our ap-
proach on four ﬁnancial time series, which are the Dow Jones, Nasdaq and Nikkei stock
indices, and the Japanese Yen/British Pound exchange rate. As we shall illustrate, our
results indicate that GARCH-UGH provides substantially more accurate one-step ahead
conditional VaRs than the conventional GARCH-EVT and bias-reduced EVT without
ﬁltering, based on the performance of the in-sample and out-of-sample backtestings. In

2

addition, our bias-reduction procedure will be designed to be robust to departure from
the independence assumption, and as such will be able to handle residual dependence
present after ﬁltering in the ﬁrst step. Our ﬁnite-sample results will also illustrate that
the GARCH-UGH method leads to one-step ahead conditional VaR estimates that are less
sensitive to the choice of sample fraction, and hence mitigates the diﬃculty in selecting
the optimal number of observations for the estimations. Finally, the computational cost
of GARCH-UGH is lower than that of conventional GARCH-EVT: the extreme value step
in the GARCH-UGH method is semiparametric with an automatic and fast recipe for the
estimations of the one-step ahead conditional VaRs, while the GARCH-EVT method is
based on ﬁtting a Generalized Pareto Distribution (GPD) to the residuals using Maximum
Likelihood Estimation, which may be time-consuming since an appropriate starting value
for the estimates has to be found by searching the parameter space.

The rest of the paper is organized as follows. Section 2 presents our framework and
methodology. Section 3 ﬁrst describes the four ﬁnancial time series used in the empiri-
cal analysis, and discusses the performance of our proposed approach through in-sample
and out-of-sample backtestings of one-step ahead extreme VaRs compared to existing ap-
proaches. Section 4 discusses our ﬁndings and perspectives for future work.

2 The GARCH-UGH method and framework

Let pt be a daily-recorded price for a stock,
index or exchange rate, and let Xt =
− log(pt/pt−1) be the negative daily log-return on this price. We assume that the dy-
namics of Xt are governed by

Xt = µt + σtZt,
(1)
where µt ∈ R and σt > 0 denote the (conditional) mean and standard deviation, and the
innovations Zt form a strictly stationary white noise process, that is, they are i.i.d. with
zero mean, unit variance and common marginal distribution function FZ. We assume that
for each t, µt and σt are measurable with respect to the σ−algebra Ft−1 representing the
information about the return process available up to time t − 1.

We are concerned with estimating extreme conditional quantiles of these negative log-
returns. Recall that for a probability level τ ∈ (0, 1), the τ th unconditional quantile of
a distribution F is qτ = inf{x ∈ R : F (x) ≥ τ }. Here we focus on the one-step ahead
quantile, that is, the estimation of the conditional extreme quantile of Xt+1 given Ft.
In this case, by location equivariance and positive homogeneity of quantiles, the one-step
ahead conditional extreme quantile (or VaR) of Xt+1 can be written as

qτ (Xt+1 | Ft) = µt+1 + σt+1qτ (Z),

(2)

where qτ (Z) is the common τ th quantile of the marginal distribution of the innovations
Zt. The problem of estimating qτ (Xt+1 | Ft) can then be tackled by estimating the mean
and standard deviation components µt+1 and σt+1 and the unconditional quantile qτ (Z).
Given estimates ˆµt+1, ˆσt+1 and ˆqτ (Z) of these quantities, an estimate of qτ (Xt+1 | Ft) is
then

ˆqτ (Xt+1 | Ft) = ˆµt+1 + ˆσt+1 ˆqτ (Z).

In calculating this estimate, there are three main diﬃculties. First, one has to estimate
µt+1 and σt+1, which supposes that an appropriate model and estimation method have to
be chosen. Second, the innovations Zt are unobserved, which means that the estimation
of qτ (Z) has to be based on residuals following the estimation of µt+1 and σt+1. A third
diﬃculty is speciﬁc to our context: we wish here to estimate a dynamic extreme VaR,

3

that is, a conditional quantile qτ (Xt+1 | Ft) with τ very close to 1.
In such contexts,
it is well-known that traditional nonparametric estimators become inconsistent (see for
example the monographs by [5, 17]), and adapted extrapolation methodologies have to be
employed.

Our GARCH-UGH method combines estimation of the mean and standard deviation
in a GARCH-type model with a ﬂexible bias-reduced extrapolation methodology for the
estimation of qτ (Z) using the residuals obtained after estimation of the model structure.
We describe these two steps successively below.

2.1 GARCH step

In order to estimate µt+1 and σt+1, one should select a particular model in the class (1).
Many diﬀerent models for volatility dynamics have been used in the literature of GARCH-
EVT approach, as we highlighted in our literature review in Section 1. Here we use an
AR(1) model for the dynamics of the conditional mean, and a parsimonious but eﬀective
GARCH(1,1) model for the volatility, as in the original GARCH-EVT approach; this will
allow us to subsequently illustrate how improving the second, EVT-based step can result
in more accurate estimates.

We thus model the conditional mean of the series by µt = φXt−1, for some φ ∈ (−1, 1),
and the conditional variance of the mean-adjusted series (cid:15)t = Xt −µt by σ2
t−1 +
κ2σ2
t−1, where κ0, κ1, κ2 > 0. Necessary and suﬃcient conditions for the stationarity of
a model following GARCH(1,1) dynamics are given in Chapter 2 of [21]; the condition
κ1 + κ2 < 1 is a simple suﬃcient condition guaranteeing stationarity. The model is
therefore the AR(1)-GARCH(1,1) model

t = κ0 +κ1(cid:15)2

Xt = µt + σtZt, with µt = φXt−1 and σ2

t = κ0 + κ1(Xt−1 − µt−1)2 + κ2σ2

t−1.

(3)

In Equation (3), the innovations Zt are i.i.d. with zero mean, unit variance.

In order to make one-step ahead predictions at time t, we ﬁx a memory n so that at
the end of time t, the ﬁnancial data consist of the last n negative log-returns Xt−j, for 0 ≤
j ≤ n − 1. We then ﬁt the AR(1)-GARCH(1,1) model to the data (Xt−n+1, . . . , Xt−1, Xt)
using Gaussian Quasi-Maximum Likelihood Estimation (QMLE), that is, by maximizing
the likelihood constructed by assuming that the innovations Zt are i.i.d. Gaussian with zero
mean and unit variance. While of course the innovations Zt will not be Gaussian in general
(and indeed in our UGH step we shall assume that they are heavy-tailed), the QMLE
method yields a consistent and asymptotically normal estimator, see for example [20] for
a theoretical analysis. One may also put a strong heavy-tailed parametric speciﬁcation
on Zt, such as assuming that they are location-scale Student distributed; this was tried in
our analysis of ﬁnancial log-returns but did not improve results substantially.

Let ( ˆφ, ˆκ0, ˆκ1, ˆκ2) be the Gaussian QMLE estimates. Choosing sensible starting values
t−n and ˆσ2
for ˆ(cid:15)2
t−n (for example, constant values as in Section 7.1 of [21]), estimates of
the conditional mean and the conditional standard deviation, (ˆµt−n+1, . . . , ˆµt−1, ˆµt) and
(ˆσt−n+1, . . . , ˆσt−1, ˆσt) respectively, can be calculated from Equation (3) recursively. This
leads to the residuals

( ˆZt−n+1, . . . , ˆZt) =

(cid:18) Xt−n+1 − ˆµt−n+1
ˆσt−n+1

, . . . ,

Xt − ˆµt
ˆσt

(cid:19)
.

We end this step by calculating the estimates of the conditional mean and standard devi-

4

ation for time t + 1, which are the obvious one-step ahead forecasts, as follows:

ˆµt+1 = ˆφXt,

ˆσt+1 =

(cid:113)

ˆκ0 + ˆκ1ˆ(cid:15)2

t + ˆκ2ˆσ2
t ,

where ˆ(cid:15)t = Xt − ˆµt. In summary, this ﬁrst GARCH step of the method consists in ﬁtting
an AR(1)-GARCH(1,1) model to the negative log-returns at a certain past time horizon
n (not too small so that the method produces reasonable results, and not too large so
that the AR-GARCH model is believable over this time period), using a Gaussian QMLE,
leading to forecasts ˆµt+1 and ˆσt+1 and standardized residuals ˆZt−j, 0 ≤ j ≤ n − 1.

2.2 UGH step

With standardized residuals at our disposal, we can now discuss the estimation of qτ (Z).
The residuals ˆZt−j, 0 ≤ j ≤ n − 1, approximate the true unobservable Zt−j. Assume that
the underlying distribution of these Zt−j is heavy-tailed, that is

lim
t→∞

U (tz)
U (t)

= zγ, ∀z > 0, where U (t) = q1−t−1(Z).

(4)

In other words, we assume the tail of the innovations to be approximately Pareto, with
the index γ tuning how heavy the tail is. This assumption is ubiquitous in actuarial
and ﬁnancial risk management (see e.g. p.9 of [17] and p.1 of [32]). It makes it possible
to construct extrapolated extreme quantile estimators: the classical Weissman quantile
estimator (see [33]) of a quantile qτ (Z) = q1−p(Z) with p = 1 − τ close to 0 is then

q1−p(Z) =

(cid:19)γk

(cid:18) k
np

Zn−k,n

(5)

where Z1,n ≤ Z2,n ≤ · · · ≤ Zn,n are the order statistics from Zt−n+1, . . . , Zt and γk is a
consistent estimator of γ. The tuning parameter k denotes the eﬀective sample size for
the estimation: this parameter should be chosen not too small, so that the variance of the
estimator is reasonable, but also not too large so that the bias coming from the use of the
extrapolation relationship (4) does not dominate. The most common estimator γk of γ is
the Hill estimator (introduced in [24]):

γk = γH

k =

1
k

k
(cid:88)

i=1

log Zn−i+1,n − log Zn−k,n.

(6)

The Hill and Weissman estimators can be shown to be asymptotically Gaussian (see for
example Chapters 3 and 4 in [15]). A reasonable idea to deﬁne an estimator of q1−p(Z) in
our context is then to use the estimators deﬁned in Equations (5) and (6) with the order
statistics of the residuals, ˆZn−j,n, in place of the unobservable Zn−j,n.

The choice of the parameter k requires solving a bias-variance tradeoﬀ for which there
is no straightforward approach. Indeed, with a low k, the estimators use observations that
are very informative about the extremes, but their low number results in a high variance.
With a high k, the variance is reduced, but at the cost of taking into account observations
that are further into the bulk of the distribution and thus carry bias. One possible way
to make the choice of k easier is to work on correcting this bias. This can be done under
the following so-called second-order condition on U :

1
A(t)

(cid:18) U (tz)
U (t)

(cid:19)

− zγ

= zγ zρ − 1

ρ

lim
t→∞

, ∀z > 0,

(7)

5

where ρ ≤ 0 is called the second-order parameter and A is a positive or negative function
converging to 0 at inﬁnity, such that |A| is regularly varying with index ρ. The function
A therefore controls the rate of convergence in Equation (4): the larger |ρ| is, the faster
|A| converges to 0, and the smaller the error in the approximation of the right tail of U
by a Pareto tail is. This makes it possible to precisely quantify the bias of the Hill and
Weissman estimators, and to correct for this bias by estimating the function A and the
parameter ρ. This results in bias-corrected Hill and Weissman estimators for which the
selection of k is typically much easier because their performance is much more stable.

Our idea in this second, UGH step is to apply such bias-corrected estimators con-
structed in [16] (and built on second-order parameter estimators of [23], hence the name
UGH, for Unbiased Gomes-de Haan) to our residuals obtained from the GARCH step. Let
m be the number of positive residuals and, for any positive number α /∈ {1/2, 1}, set

1
k

i=1
M (α)

M (α)

k =

R(α)

k =

S(α)
k =

s(α)(ρ) =

k
(cid:88)

(log ˆZn−i+1,n − log ˆZn−k,n)α,

,

k )2

k )α

k − Γ(α + 1)(M (1)
k − 2(M (1)
M (2)
α(α + 1)2Γ2(α)
4 Γ(2α)

R(2α)
k
(R(α+1)
k
ρ2(1 − (1 − ρ)2α − 2αρ(1 − ρ)2α−1)
(1 − (1 − ρ)α+1 − (α + 1)ρ(1 − ρ)α)2 .

)2

,

Our estimator of ρ motivated by [23] is

k = (s(α))←(S(α)
ˆρ(α)
k ),
where α is a positive tuning parameter and ← denotes the generalized (left-continuous)
inverse. The version of ˆρ(α)
k with the true innovations Zk instead of the residuals is known
to be consistent under a so-called third-order condition which further strengthens (7). Here
we choose α = 2 since, on fully observed data, this appears to yield the smallest mean
squared error following the numerical simulations of [23]. This results in the estimator

ˆρ(2)
k =

provided 2/3 ≤ S(2)

k ≤ 3/4, where

−4 + 6S(2)

(cid:113)

3S(2)

k − 2

k +
4S(2)

k − 3

S(2)
k =

3
4

[M (4)

k − 24(M (1)

k )4][M (2)
k − 6(M (1)

k − 2(M (1)
k )3]2

k )2]

.

[M (3)

[This expression corrects a typo in p.375 of [16].] It is clear that ˆρ(2)
S(2)
k

/∈ [2/3, 3/4]. In practice, we select the value of k in this estimator by setting

k does not exist if

(cid:26)

(cid:18)

kρ = sup

k : k ≤ min

m − 1,

(cid:19)

2m
log log m

and ˆρ(2)
k

(cid:27)

exists

.

(8)

Here m is the number of positive observations in the sample. The intuition is that even
though this estimator of ρ requires a choice of k, this choice should be diﬀerent from

6

its counterpart used in the estimation of γ, and indeed intuitively the value of k in the
estimator of ρ should be rather high in order to allow the methodology to identify the
bias coming from including observations belonging to the bulk of the distribution (which
correspond to a high k). We then estimate ρ by ˆρkρ = ˆρ(2)
. If the set on the right-hand
kρ
side of (8) is empty, we deﬁne ˆρkρ = −1 as recommended in p.117 of Section 4.5.1 in [5].
The parameter γ is then estimated using the residual-based version of the bias-corrected
Hill estimator introduced in [16]:

ˆγk,kρ = ˆγH

k −

M (2)
k − 2(ˆγH
k ˆρkρ(1 − ˆρkρ)−1

k )2

2ˆγH

.

Our residual-based, bias-corrected estimator of unconditional extreme quantiles is then
given by

ˆq1−p(Z) = ˆZn−k,n

(cid:19)ˆγk,kρ

(cid:32)

×

1 −

(cid:18) k
np

(cid:2)M (2)

k − 2(ˆγH
2ˆγH

k )2(cid:3)[1 − ˆρkρ]2
k ˆρ2
kρ

(cid:34)

1 −

(cid:18) k
np

(cid:19)ˆρkρ

(cid:35)(cid:33)
.

(9)

This corresponds to a slightly diﬀerent version of the estimator in Section 4.3 of [16],
given later by [10], who pointed out a mistake in the analysis of [16]. The versions of these
estimators for fully observed data work when this data is weakly serially dependent, as
shown in [10]. As such, our proposed method will be robust to the presence of residual
dependence after ﬁltering and to model misspeciﬁcation in the sense of [25]. We shall also
show that the choice of k for this estimator is not as crucial in ﬁnite samples as for the
traditional Hill estimator, because this estimator has reasonably good performance across
a large range of values of k.

2.3 Summary and output of the GARCH-UGH method

The GARCH-UGH approach may be summarized by the following two successive steps:

1. GARCH step: based on n previous observations at time t, ﬁt an AR(1)-GARCH(1,1)
model to the negative daily log-returns data using a Gaussian QMLE. Obtain ˆµt+1
and ˆσt+1 using the ﬁtted model and compute standardized residuals.

2. UGH step: substitute the standardized residuals into the asymptotically unbiased

tail quantile estimator qτ (Z), resulting in the estimator ˆq1−p(Z) in (9).

Combining the two steps results in the ﬁnal GARCH-UGH estimator

ˆqτ (Xt+1 | Ft) = ˆµt+1 + ˆσt+1 ˆqτ (Z).

The goal of our real data analysis is to examine the ﬁnite-sample performance of this
estimator for low exceedance probabilities, that is, τ close to 1.

3 Empirical analysis of four ﬁnancial time series

We consider historical daily negative log-returns of three ﬁnancial indices and an exchange
rate, all made of n = 4000 observations:

• The Dow Jones Industrial Average (DJ) from 23 December 1993 to 9 November

2009;

7

• The Nasdaq Stock Market Index (NASDAQ) from 30 August 1993 to 16 July 2009;

• The Nikkei 225 (NIKKEI) from 14 May 1993 to 12 August 2009;

• The Japanese Yen-British Pound exchange rate (JPY/GBP) from 2 January 2000

to 14 December 2010.

The data have been taken from the R package qrmdata [26] and are represented in Figure 1.
The graphs show that these negative log-returns are extremely volatile around the 2007-
2008 ﬁnancial crisis, which created a succession of extreme positive and negative returns
over short time horizons. A noticeable degree of volatility clustering is also detected from
a visual inspection of Figure 1, revealing the presence of the heteroskedasticity.

Descriptive statistics and basic statistical tests applied to the negative log-returns on
the four ﬁnancial time series are reported in Table 1. According to the descriptive statistics,
the mean of the negative log-returns of all series are close to zero, and negative log-returns
are leptokurtic. The Jarque-Bera test statistics indicate that the Gaussian distribution
is not suitable for any of these series of negative log-returns. All four series pass the
augmented Dickey-Fuller (ADF) test, indicating that they can be considered stationary for
modeling purposes. The Ljung-Box test applied to the squared negative log-returns, with
orders 1 and 10, rejects the null hypothesis of no autocorrelation, indicating the presence
of substantial conditional heteroskedasticity in all series. This provides justiﬁcation for
our use of GARCH-type models with these data.

We compare the performance of our GARCH-UGH method with two benchmarks:

• The bias-reduced UGH method without ﬁltering: this method applies the UGH step

directly to the series Xt.

• The conventional GARCH-EVT method as described in [29]. This consists, ﬁrst, in
the same ﬁltering step as described in Section 2.1. Standardized residuals are then
recorded and a Generalized Pareto distribution is ﬁtted using a maximum likelihood
estimator, thus producing a VaR estimate ˜qτ (Z). This method therefore diﬀers from
ours as far as the extreme value step is concerned.

A comparison with the UGH method (without ﬁltering) allows us to see how eﬀective
ﬁltering is, and a comparison to the GARCH-EVT method (not featuring bias reduction)
will illustrate the beneﬁt of bias reduction at the extreme value step after ﬁltering. In
Section 3.1, we explain how we carry out backtesting of the performance of one-step ahead
conditional VaR estimators provided by each approach. This is then followed by in-sample
and out-of-sample evaluations of one-step ahead conditional VaR estimates at diﬀerent τ
levels and choices of k in Sections 3.2 and 3.3, respectively: in-sample estimation inves-
tigates the ﬁt of the approaches to high volatile returns, while out-of-sample estimation
tests how well the method predicts extreme VaR.

3.1 Statistical framework for VaR backtesting

Backtesting is carried out to examine the accuracy of the one-step ahead conditional VaR
It compares the ex-ante VaR estimates ˆqτ (Xt | Ft−1) with the ex-post re-
estimates.
alized negative log-returns in a time window WT , with a VaR violation at time t said
| Ft−1). Deﬁne a hit sequence of VaR violations as
to occur whenever xt > ˆqτ (Xt
| Ft−1)}.
It = 1{xt > ˆqτ (Xt
If a VaR estimation method is accurate, then the se-
quence (It) should approximately be an independent sequence of Bernoulli variables with
success probability p = 1 − τ . Both the distributional and independence properties are

8

equally important. A VaR estimation method with too few VaR violations will tend to
overestimate risk and therefore to be excessively conservative in ﬁnancial terms, while too
many VaR violations mean that risk is underestimated, leading to insuﬃcient provision
of capital and therefore potential insolvency in case of large losses. Besides, a violation of
the independence property typically arises when there is a clustering of VaR violations,
which indicates a model that does not represent volatility clustering well enough.

In order to test the distributional assumption, we use the unconditional coverage test
proposed by [28] (also known as Kupiec test or POF test, for Proportion Of Failures): ﬁx
a time window WT , let N = (cid:80)
It be the observed number of VaR violations over WT
and p be the theoretical violation rate. The Kupiec test statistic is the likelihood ratio
(LR) statistic given by

t∈WT

LRuc = −2 log{pN (1 − p)T −N } + 2 log

(cid:26)(cid:18) N
T

(cid:19)N (cid:18)

1 −

(cid:19)T −N (cid:27)

.

N
T

Under the null hypothesis that the It are independent and Bernoulli distributed with
success probability p, the test statistic LRuc is asymptotically χ2 distributed with 1 degree
of freedom. The Kupiec test rejects this null hypothesis with asymptotic type I error α
1,1−α is the (1 − α)−quantile of the χ2 distribution with 1
when LRuc > χ2
degree of freedom.

1,1−α, where χ2

One way to test the independence property is to use another likelihood ratio test
called the conditional coverage test, proposed by Christoﬀersen in [11]. This test is based
on testing for ﬁrst-order Markov dependence, with the test statistic being given by

LRcc = −2 log{pN (1 − p)T −N } + 2 log{ˆπN00

00 ˆπN01

01 ˆπN10

10 ˆπN11

11 }.

t∈WT

Here Nij = (cid:80)
1{It+1 = j, It = i} and ˆπij = Nij/(Ni0 + Ni1). Under the null
hypothesis that the sequence (It) is independent and identically distributed as Bernoulli
with parameter p, the test statistic LRcc is asymptotically χ2 distributed with 2 degrees of
freedom, and the conditional coverage test then rejects this null hypothesis with asymptotic
type I error α when LRcc > χ2
2,1−α (the (1 − α)−quantile of the χ2 distribution with 2
degrees of freedom). Strictly speaking the conditional coverage test only assesses departure
from either independence or stationarity, but in fact

LRcc = LRuc + LRind
(cid:26)(cid:18) N
T

with LRind = −2 log

(cid:19)N (cid:18)

1 −

(cid:19)T −N (cid:27)

N
T

+ 2 log{ˆπN00

00 ˆπN01

01 ˆπN10

10 ˆπN11

11 }.

The quantity LRind is nothing but a likelihood ratio test statistic of independence versus
nontrivial ﬁrst-order Markov dynamics of the sequence (It), which rejects independence
2,1−α = χ2
of (It) provided LRuc > χ2
1,1−α, checking stationarity and
independence via the pair of test statistics (LRuc, LRind) is exactly equivalent to checking
them via the unconditional and conditional coverage tests. We therefore use below both
the unconditional and conditional coverage tests to assess the performance of our dynamic
extreme VaR estimators. This constitutes a backtesting approach in the spirit of the one
suggested by the Basel Committee on Banking Supervision.

1,1−α. Since χ2

1,1−α + χ2

3.2 In-sample dynamic extreme VaR estimation and backtesting

We start by estimating in-sample one-step ahead conditional VaRs qτ (Xt+1 | Ft) for
τ ∈ {0.99, 0.995, 0.999}. For these in-sample evaluations, all methods (GARCH-UGH,

9

UGH without ﬁltering, and GARCH-EVT without bias reduction) are implemented on
a ﬁxed in-sample testing window WT , which consists of 3000 observations; this follows
advice by [12] which suggests that this testing window WT should cover at least 4 years of
data, or approximately 1000 observations, for a reliable statistical analysis. Speciﬁcally,
we use:

• The time period from 8 December 1997 to 9 November 2009 for the Dow Jones,

• The time period from 13 August 1997 to 16 July 2009 for the Nasdaq,

• The time period from 29 May 1997 to 12 August 2009 for the Nikkei,

• The time period from 28 September 2002 to 14 December 2010 for the JPY/GBP

exchange rate.

This allows us to focus on extreme VaR estimation around the 2007-2008 ﬁnancial crisis,
of which a consequence was a succession of extremely large negative log-returns in a very
short timeframe. This should be considered a challenging problem.

In each case, we implement the three methods on these 3000 observations. The
GARCH-UGH and GARCH-EVT methods ﬁlter the data using an AR(1)-GARCH(1,1)
model Xt = µt + σtZt, then estimate qτ (Z) on the basis of the residuals obtained from
this ﬁltering, before obtaining the ﬁnal extreme VaR estimate as ˆqτ (Xt+1 | Ft) = ˆµt+1 +
ˆσt+1 ˆqτ (Z) (for the GARCH-UGH method) and ˜qτ (Xt+1 | Ft) = ˆµt+1 + ˆσt+1 ˜qτ (Z) (for
the GARCH-EVT method) where the diﬀerence lies in how qτ (Z) is estimated. By com-
parison, the UGH method works directly on the series Xt, without ﬁltering, the estimate
then being ¯qτ (Xt+1 | Ft) = ˆqτ (X), where ˆqτ (X) is obtained as in Section 2.2 with the
Xt in place of the ˆZt. In addition, because the estimation of the constant ρ is known to
be a diﬃcult problem, we make the following adjustment to the GARCH-UGH method.
We calculate another version of the GARCH-UGH estimate where the estimator ˆρkρ is
replaced throughout by the constant −1. If this other version has a number of VaR viola-
tions closer to the expected number of violations (which is known and equal to 3000(1 − τ )
where τ is the VaR level), we retain this version. The use of the constant −1 has been
advocated in the literature before, see for example Section 4.5.1 in [5].

Results from Tables 2-5 indicate that, on the basis of in-sample validation, the proposed
GARCH-UGH approach is the most successful for estimating one-step ahead extreme VaRs
that satisfy both unconditional and conditional coverage properties. Across all samples and
in terms of number of VaR violations only, in 46 out of 60 cases our GARCH-UGH approach
is closest to the mark, with the conventional GARCH-EVT performing worst overall.
In addition, although the unﬁltered UGH estimate is somewhat reasonable in terms of
number of VaR violations, it is not appropriate because it lacks responsiveness to the
time-varying volatility and volatility clustering: Figure 2a illustrates that the non-dynamic
nature of the UGH estimate leaves it unable to respond immediately to high volatility, and
VaR violations tend to cluster. By contrast, the conditional VaR estimates obtained by
our GARCH-UGH approach (Figure 2b) clearly respond to the changing volatility with
no clustering of VaR violations, while bias reduction results in closer numbers of VaR
violations to the expected numbers than with the conventional GARCH-EVT. Numerically,
the GARCH-UGH method never fails either the Kupiec or Christoﬀersen tests, whereas the
GARCH-EVT method fails 7 and 5 times out of 60 cases, respectively. The bias correction
at the extreme value step appears to be very eﬀective for the accurate estimation of one-
step ahead dynamic extreme VaRs. It leads to results that seem less sensitive to the choice
of sample fraction k than the conventional GARCH-EVT method: see Tables 2-5, where
results appear to be consistently good across a large range of values of k.

10

3.3 Out-of-sample dynamic extreme VaR estimation and backtesting

We now focus on the out-of-sample estimation (that is, prediction) of one-step ahead VaR
via the same three approaches, again at level τ ∈ {0.99, 0.995, 0.999}. We consider the
following samples of data:

• The time period from 23 December 1993 to 9 November 2009 for the Dow Jones,

• The time period from 30 August 1993 to 16 July 2009 for the Nasdaq,

• The time period from 14 May 1993 to 12 August 2009 for the Nikkei,

• The time period from 2 January 2000 to 14 December 2010 for the JPY/GBP ex-

change rate.

In order to carry out this out-of-sample backtest, we adopt a rolling window estimation
approach. Speciﬁcally, we ﬁrst ﬁx a testing window WT in each case, which corresponds
to the periods of time considered in our in-sample evaluation (8 December 1997 to 9
November 2009 for the Dow Jones, 13 August 1997 to 16 July 2009 for the Nasdaq, 29
May 1997 to 12 August 2009 for the Nikkei, 28 September 2002 to 14 December 2010 for
the JPY/GBP exchange rate). At each time t in this testing window WT , we use a window
of length WE of prior information in order to predict the conditional VaR on time t + 1
(with parameter estimates updated when the estimation window changes), which is then
compared to the observed log-return on day t + 1. Various choices of WE have been made
in the literature: here we choose WE = 1000 as in [29], corresponding to approximately
four years of model calibration for each prediction with stock market data, and three years
with exchange rate data.

Tables 6-9 gather the numerical results.

It can be seen that again, the suggested
GARCH-UGH approach appears to be best overall. In 47 out of 60 cases, the GARCH-
UGH approach yields the closest number of VaR violations to the theoretically expected
numbers, while the unﬁltered UGH method fares worst. Based on the Kupiec test, the
GARCH-UGH approach fails twice, whereas the GARCH-EVT and UGH fail 6 and 49
times out of 60 cases, respectively. On one occasion GARCH-UGH fails the Christoﬀersen
test, while the GARCH-EVT and UGH methods fail 0 and 43 times out of 60 cases. The
GARCH-UGH typically performs better than other approaches except possibly when the
top 5% and 10% of observations are used (for the choice of k); this is because the bias is
not the dominating term in the bias-variance tradeoﬀ when k is small. The corresponding
plots of out-of-sample backtesting are shown in Figures 3-6, where it is clearly seen that
the GARCH-UGH and GARCH-EVT estimates have the same dynamics, with the bias
correction shifting the estimate upwards or downwards depending on the rolling estimation
window.

3.4 Conclusion from the empirical analysis

We conclude from this empirical analysis that the proposed GARCH-UGH approach com-
bining ﬁltering via the AR(1)-GARCH(1,1) model and a semiparametric bias-reduced
extreme value step provides better one-step ahead dynamic extreme VaR estimates for ﬁ-
nancial time series than the benchmark conventional GARCH-EVT approach of [29]. This
can be seen from both the in-sample and the out-of-sample estimations at several quantile
levels τ , including the very high τ = 0.999 corresponding to a 99.9% VaR, and a large
range of sample fractions k, due to the eﬀect of the bias correction. In addition, from a
computational standpoint, the GARCH-UGH method appears to be more reliable than the

11

conventional GARCH-EVT approach, because the extreme value step is carried out using
an automatic recipe for the estimation of the tail index and extreme quantile, whereas the
GARCH-EVT approach relies on maximum likelihood estimation, which crucially depends
on having a good starting value to ﬁt a Generalized Pareto distribution. This also makes
the GARCH-UGH method computationally cheaper than the GARCH-EVT competitor.

4 Discussion

In this paper we introduce an extension of the two-step GARCH-EVT approach from [29]
for extreme VaR estimation, based on a semiparametric bias-reduced extreme quantile
estimator from [10, 16]. This diﬀers from the other papers published in the econometric
literature by introducing a ﬁnite-sample improvement at the extreme value step, rather
than using a more complicated ﬁlter than the AR(1)-GARCH(1,1) ﬁlter.

Even though the Basel Committee on Banking Supervision recommends the use of
VaR at high levels (see for example [3]), the VaR itself has been criticized several times in
the ﬁnancial literature for two main reasons. First, the VaR only measures the frequency
of observations below or above the predictor and not their magnitude: this means that,
while it is known that 100(1 − τ )% of losses will be higher than the VaR qτ at level τ , the
VaR alone cannot give any further information about the size of these large losses. Second,
the VaR is not a coherent risk measure in the sense of [1], because it is not sub-additive
in general, meaning that it does not abide by the intuitive diversiﬁcation principle stating
that a portfolio built on several ﬁnancial assets carries less risk than a portfolio solely
consisting of one of these assets. These two weaknesses pushed the Basel Committee to
also recommend calculating the Expected Shortfall (or Conditional Value-at-Risk) as a
complement or alternative to the VaR. In practice, this is hampered by the fact that the
Expected Shortfall is not elicitable, and therefore the development of a simple backtesting
methodology for the Expected Shortfall is not clear. This is why we believe that the
accurate estimation of extreme VaR is still worth pursuing.

We highlight three possible directions for further investigations. The ﬁrst one is that
one could replace the AR(1)-GARCH(1,1) ﬁlter by a more sophisticated ﬁlter. Which
ﬁlter should be used is not obvious: one could think about replacing the AR(1) part by an
ARMA(p, q) part, or the GARCH(1,1) part by a GARCH(p, q) part (or a more complicated
asymmetric version), or both. This may make it possible to even better account or the
volatility dynamics, whose accurate estimation and prediction are key. The second one is
the extension of our GARCH-UGH approach to the estimation of the multiple-step ahead
conditional VaR. This is important, because certain regulations such as those advocated by
[2] require the estimation of the 10-day ahead VaR at the 99% conﬁdence level, rather than
merely the one-step ahead VaR. This is a challenging problem: [29] tackle this question
using a bootstrap methodology, but bootstrapping with heavy tails is known to be very
diﬃcult to calibrate. The development of an adaptation of the GARCH-UGH method to
the multiple-step ahead setup which stays computationally manageable is well beyond the
scope of the current paper. The third and ﬁnal perspective is the estimation of alternative
dynamic risk measures as a way of solving the two drawbacks of VaR that we highlighted
in the previous paragraph. One candidate will be the expectile risk measure (see [31]
for the original deﬁnition of expectiles in a regression context), which takes into account
both the frequency of extreme observations and their magnitude, and is also shown to
be a coherent and elicitable risk measure in [37]. The use of expectiles has recently
received substantial attention from the perspective of risk management as an alternative
tool for quantifying tail risk (see for example [13, 14]), but the case of dynamic estimation

12

of extreme expectiles in a ﬁnancial time series context has not been considered yet. Of
course, there exists no universally preferred risk measure: the expectile only has an implicit
formulation in general, and is more diﬃcult to interpret than the VaR. The development
of a GARCH-UGH-based method for the estimation of dynamic extreme expectiles will
thus be an interesting complement to the present paper.

Acknowledgments

The ﬁrst author gratefully acknowledges the ﬁnancial support from the SOKENDAI (Grad-
uate University for Advanced Studies) under SOKENDAI Student Dispatch Program
grant, and the Research Fellowships of Japan Society for the Promotion of Science for
Young Scientists (Project number: 20J15188). Y. Kawasaki is supported by JSPS Grant-
in-Aid for Scientiﬁc Research (18H00836, 19K01597, 20H01502) and by the ISM Coop-
erative Research Program 2020-ISMCRP-2038. G. Stupﬂer is supported by the French
National Research Agency under the grant ANR-19-CE40-0013/ExtremReg project, and
he also acknowledges support from an AXA Research Fund Award on “Mitigating risk in
the wake of the COVID-19 pandemic”.

References

[1] P. Artzner, F. Delbaen, J-M. Eber, and D. Heath. Coherent measures of risk. Math-

ematical Finance, 9:203–228, 1999.

[2] Basel Committee on Banking Supervision. Revisions to the Basel II market risk
framework. https://www.bis.org/publ/bcbs265.pdf, 2009. Accessed: 2020-10-14.

[3] Basel Committee on Banking Supervision. Fundamental review of the trading book:
A revised market risk framework. https://www.bis.org/publ/bcbs265.pdf, 2013.
Accessed: 2020-10-14.

[4] M. Bee, D.J. Dupuis, and L. Trapin. Realizing the extremes: Estimation of tail-risk
measures from a high-frequency perspective. Journal of Empirical Finance, 36:86–99,
2016.

[5] J. Beirlant, Y. Goegebeur, J. Segers, and J. Teugels. Statistics of Extremes: Theory

and Applications. Wiley, 2004.

[6] T. Bollerslev. Generalized autoregressive conditional heteroskedasticity. Journal of

Econometrics, 31:307–327, 1986.

[7] H.N.E. Bystr¨om. Managing extreme risks in tranquil and volatile markets using
conditional extreme value theory. International Review of Financial Analysis, 13:133–
152, 2004.

[8] J.-J. Cai, L. de Haan, and C. Zhou. Bias correction in extreme value statistics with

index around zero. Extremes, 16:173–201, 2013.

[9] V. Chavez-Demoulin, A.C. Davison, and A.J. McNeil. Estimating Value-at-Risk: A

point process approach. Quantitative Finance, 5:227–234, 2005.

[10] V. Chavez-Demoulin and A. Guillou. Extreme quantile estimator for β-mixing time
series and applications. Insurance: Mathematics and Economics, 83:59–74, 2018.

13

[11] P. Christoﬀersen. Evaluating interval forecasts.

International Economic Review,

39(4), 1998.

[12] J. Danielsson. Financial Risk Forecasting: The Theory and Practice of Forecasting

Market Risk with Implementation in R and Matlab. Wiley, 2011.

[13] A. Daouia, S. Girard, and G. Stupﬂer. Estimation of tail risk based on extreme
expectiles. Journal of the Royal Statistical Society: Series B, 80:263–292, 2018.

[14] A. Daouia, S. Girard, and G. Stupﬂer. Tail expectile process and risk assessment.

Bernoulli, 26(1):531–556, 2020.

[15] L. de Haan and A. Ferreira. Extreme Value Theory An Introduction. Springer Series
in Operations Research and Financial Engineering. New York: Springer, 2006.

[16] L. de Haan, C. Mercadier, and C. Zhou. Adapting extreme value statistics to ﬁnancial
time series: dealing with bias and serial dependence. Finance and Stochastics, 20:321–
354, 2016.

[17] P. Embrechts, C. Kl¨uppelberg, and T. Mikosch. Modelling Extremal Events for In-

surance and Finance. Springer, 1997.

[18] I. Ergen. Two-step methods in VaR prediction and the importance of fat tails. Quan-

titative Finance, 15(6):1013–1030, 2015.

[19] V. Fernandez. Risk management under extreme events.

International Review of

Financial Analysis, 14:113–148, 2005.

[20] C. Francq and J.-M. Zako¨ıan. Maximum likelihood estimation of pure GARCH and

ARMA-GARCH processes. Bernoulli, 10(4):605–637, 2004.

[21] C. Francq and J.-M. Zako¨ıan. GARCH Models: Structure, Statistical Inference and

Financial Applications. John Wiley & Sons, 2010.

[22] D. Furi´o and F.J. Climent. Extreme value theory versus traditional GARCH ap-
proaches applied to ﬁnancial data: a comparative evaluation. Quantitative Finance,
13(1):45–63, 2013.

[23] M.I. Gomes, L. de Haan, and L. Peng. Semi-parametric estimation of the second

order parameter in statistics of extremes. Extremes, 5:387–414, 2002.

[24] B. M. Hill. A simple general approach to inference about the tails of a distribution.

The Annals of Statistics, 3:1163–1174, 1975.

[25] J. B. Hill. Tail index estimation for a ﬁltered dependent time series. Statistica Sinica,

25(2):609–629, 2015.

[26] Marius Hofert and Kurt Hornik. qrmdata: Data Sets for Quantitative Risk Manage-

ment Practice, 2016. R package version 2016-01-03-1.

[27] A. Jalal and M. Rockinger. Predicting tail-related risk measures: The consequences
of using GARCH ﬁlters for non-GARCH data. Journal of Empirical Finance, 15:868–
877, 2008.

[28] P. Kupiec. Techniques for verifying the accuracy of risk management models. Journal

of Derivatives, 3:73–84, 1995.

14

[29] A.J. McNeil and R. Frey. Estimation of tail-related risk measures for heteroscedastic
ﬁnancial time series: an extreme value approach. Journal of Empirical Finance,
7:271–300, 2000.

[30] A.J. McNeil, R. Frey, and P. Embrechts. Quantitative Risk Management: Concepts,

Techniques and Tools - Revised Edition. Princeton University Press, 2015.

[31] W. K. Newey and J. L. Powell. Asymmetric least squares estimation and testing.

Econometrica, 55(4):819–847, 1987.

[32] S. Resnick. Heavy-Tail Phenomena: Probabilistic and Statistical Modeling. Springer,

2007.

[33] I. Weissman. Estimation of parameters and large quantiles based on the k largest
observations. Journal of the American Statistical Association, 73:812–815, 1978.

[34] Y. Yi, X. Feng, and Z. Huang. Estimation of extreme value-at-risk: An EVT approach

for quantile GARCH model. Econometric Letters, 124:378–381, 2014.

[35] M. Youssef, L. Belkacem, and K. Mokni. Value-at-risk estimation of energy com-
modities: A long-memory GARCH-EVT approach. Energy Economics, 51:99–110,
2015.

[36] L-T. Zhao, K. Liu, X-L. Duan, and M-F. Li. Oil price risk evaluation using a novel
hybrid model based on time-varying long memory. Energy Economics, 81:70–78, 2019.

[37] J.F. Ziegel. Coherence and elicitability. Mathematical Finance, 24(4):901–918, 2016.

15

Table 1: Summary of descriptive statistics and basic statistical tests for daily negative
log-returns on DJ, NASDAQ, NIKKEI and JPY/GBP.

Sample size
Mean
Median
Maximum
Minimum
Standard deviation
Skewness
Kurtosis
J-B test

Q(1)

Q(5)

Q(10)

Q2(1)

Q2(10)

DJ
4000

NASDAQ
4000

NIKKEI
4000

JPY/GBP
4000

−0.000250 −0.000355 −0.000169 −0.0000557
−0.000460 −0.00123

0.0820
−0.105
0.0119
0.117
8.096
10933∗
(0.0000)
13.159∗
(0.000)
37.723∗
(0.000)
50.388∗
(0.000)
131.54∗
(0.000)
2613.3∗
(0.000)

0.111
−0.172
0.0203
−0.110
4.469
3337.3∗
(0.0000)
12.098∗
(0.001)
37.62∗
(0.000)
42.192∗
(0.000)
207.21∗
(0.000)
1907.5∗
(0.000)

−0.0000177
0.121
−0.132
0.0155
0.175
5.579
5207.9∗
(0.0000)
6.680∗
(0.010)
14.429∗
(0.013)
23.023∗
(0.011)
248.32∗
(0.000)
3183.4∗
(0.000)

0
0.0600
−0.0640
0.00626
−0.586
10.931
2014.6∗
(0.0000)
128.68∗
(0.000)
146.47∗
(0.000)
150.37∗
(0.000)
275.36∗
(0.000)
1650.4∗
(0.000)

ADF test

−15.782∗∗ −14.794∗∗ −15.967∗∗ −16.415∗∗

Notes: A kurtosis greater than 3 indicates that the dataset has heavier tails than a normal distribution.
J-B stands for the Jarque-Bera test, Q(n) and Q2(n) are the Ljung-Box tests for autocorrelation at lags
n in the negative log-return series and squared negative log-returns, respectively. The ADF test is the
augmented Dickey-Fuller stationarity test statistic without trend. The p-values are given between brackets.
∗∗, ∗ denote signiﬁcance at 1% and 5% levels, respectively.

16

(a) DJ (23/12/1993 to 09/11/2009)

(b) NASDAQ (30/08/1993 to 16/07/2009)

(c) NIKKEI (14/05/1993 to 12/08/2009)

(d) JPY/GBP (02/01/2000 to 14/12/2010)

Figure 1: Daily negative log-returns of four ﬁnancial time series: DJ, NASDAQ, NIKKEI
and JPY/GBP.

17

Dec 231993Apr 031995Oct 011996Apr 011998Oct 011999Apr 022001Oct 012002Apr 012004Oct 032005Apr 022007Oct 0120081993-12-23 / 2009-11-09-0.10-0.05 0.00 0.05-0.10-0.05 0.00 0.05Aug 301993Jan 031995Jul 011996Jan 021998Jul 011999Jan 022001Jul 012002Jan 022004Jul 012005Jan 032007Jul 0120081993-08-30 / 2009-07-16-0.15-0.10-0.05 0.00 0.05 0.10-0.15-0.10-0.05 0.00 0.05 0.10May 141993Oct 031994Apr 011996Oct 011997Apr 011999Oct 022000Apr 012002Oct 012003Apr 012005Oct 022006Apr 012008Aug 1220091993-05-14 / 2009-08-12-0.10-0.05 0.00 0.05 0.10-0.10-0.05 0.00 0.05 0.10Jan 022000Jan 012001Jan 012002Jan 012003Jan 012004Jan 012005Jan 012006Jan 012007Jan 012008Jan 012009Jan 012010Dec 1420102000-01-02 / 2010-12-14-0.06-0.04-0.02 0.00 0.02 0.04-0.06-0.04-0.02 0.00 0.02 0.04Table 2: In-sample evaluations of one-step ahead conditional VaR estimates from 8 De-
cember 1997 to 9 November 2009 at diﬀerent quantile levels for the negative log-returns
of DJ index by means of the number of VaR violations, unconditional and conditional
coverage tests.

Testing window
% of top obs. used
DJ:
0.999 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.995 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.99 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

3000
5%

3
4
(0.583, 0.855)
2
(0.538, 0.826)
2
(0.538, 0.826)

15
18
(0.452, 0.012)
15
(1.000, 0.927)
13
(0.596, 0.821)

30
34
(0.472, 0.018)
27
(0.576, 0.669)
23
(0.180, 0.341)

10%

15%

20%

25%

3
5
(0.292, 0.569)
2
(0.538, 0.826)
2
(0.538, 0.826)

15
18
(0.452, 0.012)
14
(0.793, 0.905)
13
(0.596, 0.821)

30
34
(0.472, 0.018)
28
(0.711, 0.717)
23
(0.180, 0.341)

3
2
(0.538, 0.826)
2
(0.538, 0.826)
2
(0.538, 0.826)

15
16
(0.798, 0.009)
14
(0.793, 0.905)
13
(0.596, 0.821)

30
34
(0.472, 0.018)
29
(0.854, 0.741)
22
(0.123, 0.259)

3
2
(0.538, 0.826)
2
(0.538, 0.826)
2
(0.538, 0.826)

15
18
(0.452, 0.012)
15
(1.000, 0.927)
13
(0.596, 0.821)

30
36
(0.286, 0.018)
31
(0.855, 0.711)
22
(0.123, 0.259)

3
3
(1.000, 0.997)
2
(0.538, 0.826)
4
(0.583, 0.855)

15
20
(0.218, 0.011)
15
(1.000, 0.927)
13
(0.596, 0.821)

30
39
(0.114, 0.014)
33
(0.588, 0.598)
20
(0.050, 0.130)

Notes: The closest numbers of VaR violations to theoretically expected ones are highlighted in bold.
The p-values for the unconditional coverage test by [28] and conditional coverage test by [11] at the 5%
signiﬁcance level are given in brackets in order.

18

Table 3: In-sample evaluations of one-step ahead conditional VaR estimates from 13 Au-
gust 1997 to 16 July 2009 at diﬀerent quantile levels for the negative log-returns of NAS-
DAQ index by means of the number of VaR violations, unconditional and conditional
coverage tests.

Testing window
% of top obs. used
NASDAQ:
0.999 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.995 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.99 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

3000
5%

3
3
(1.000, 0.997)
4
(0.583, 0.855)
4
(0.583, 0.855)

15
21
(0.143, 0.295)
14
(0.793, 0.905)
13
(0.596, 0.821)

30
32
(0.717, 0.609)
23
(0.180, 0.341)
22
(0.123, 0.259)

10%

15%

20%

25%

3
1
(0.179, 0.406)
4
(0.583, 0.855)
4
(0.583, 0.855)

15
21
(0.143, 0.295)
14
(0.793, 0.905)
13
(0.596, 0.821)

30
33
(0.588, 0.135)
23
(0.180, 0.341)
17
(0.009, 0.031)

3
1
(0.179, 0.406)
4
(0.583, 0.855)
4
(0.583, 0.855)

15
21
(0.143, 0.295)
14
(0.793, 0.905)
10
(0.168, 0.374)

30
33
(0.588, 0.135)
23
(0.180, 0.341)
16
(0.005, 0.017)

3
1
(0.179, 0.406)
4
(0.583, 0.855)
4
(0.583, 0.855)

15
19
(0.320, 0.541)
14
(0.793, 0.905)
10
(0.168, 0.374)

30
35
(0.371, 0.127)
25
(0.345, 0.519)
16
(0.005, 0.017)

3
1
(0.179, 0.406)
2
(0.538, 0.826)
4
(0.583, 0.855)

15
21
(0.143, 0.295)
13
(0.596, 0.821)
10
(0.168, 0.374)

30
37
(0.215, 0.106)
25
(0.345, 0.519)
16
(0.005, 0.017)

Notes: The closest numbers of VaR violations to theoretically expected ones are highlighted in bold.
The p-values for the unconditional coverage test by [28] and conditional coverage test by [11] at the 5%
signiﬁcance level are given in brackets in order.

19

Table 4: In-sample evaluations of one-step ahead conditional VaR estimates from 29 May
1997 to 12 August 2009 at diﬀerent quantile levels for the negative log-returns of NIKKEI
index by means of the number of VaR violations, unconditional and conditional coverage
tests.

Testing window
% of top obs. used
NIKKEI:
0.999 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.995 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.99 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

3000
5%

3
4
(0.583, 0.855)
4
(0.583, 0.885)
5
(0.292, 0.569)

15
15
(1.000, 0.178)
13
(0.596, 0.821)
13
(0.596, 0.821)

30
32
(0.717, 0.609)
26
(0.453, 0.601)
25
(0.345, 0.287)

10%

15%

20%

25%

3
4
(0.583, 0.855)
2
(0.538, 0.826)
5
(0.292, 0.569)

15
15
(1.000, 0.178)
13
(0.596, 0.821)
12
(0.421, 0.689)

30
32
(0.717 0.609)
25
(0.345, 0.519)
24
(0.254, 0.430)

3
4
(0.583, 0.855)
4
(0.583, 0.885)
5
(0.292, 0.569)

15
17
(0.612, 0.199)
13
(0.596, 0.821)
12
(0.421, 0.689)

30
34
(0.472, 0.562)
26
(0.453, 0.601)
21
(0.081, 0.188)

3
4
(0.583, 0.855)
4
(0.583, 0.885)
5
(0.292, 0.569)

15
18
(0.452, 0.190)
13
(0.596, 0.821)
12
(0.421, 0.689)

30
36
(0.286 0.427)
31
(0.855, 0.711)
19
(0.030, 0.085)

3
1
(0.179, 0.406)
1
(0.179, 0.406)
5
(0.292, 0.569)

15
21
(0.143, 0.114)
12
(0.421, 0.689)
12
(0.421, 0.689)

30
38
(0.159, 0.297)
28
(0.711, 0.666)
18
(0.017, 0.049)

Notes: The closest numbers of VaR violations to theoretically expected ones are highlighted in bold.
The p-values for the unconditional coverage test by [28] and conditional coverage test by [11] at the 5%
signiﬁcance level are given in brackets in order.

20

In-sample evaluations of one-step ahead conditional VaR estimates from 28
Table 5:
September 2002 to 14 December 2010 at diﬀerent quantile levels for the negative log-
returns of JPY/GBP exchange rate by means of the number of VaR violations, uncondi-
tional and conditional coverage tests.

Testing window
% of top obs. used
JPY/GBP:
0.999 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.995 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.99 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

3000
5%

3
2
(0.538, 0.826)
3
(1.000, 0.997)
3
(1.000, 0.997)

15
16
(0.798, 0.195)
16
(0.798, 0.888)
11
(0.277, 0.532)

30
38
(0.159, 0.002)
31
(0.855, 0.612)
29
(0.854, 0.556)

10%

15%

20%

25%

3
2
(0.538, 0.826)
2
(0.538, 0.826)
3
(1.000, 0.997)

15
17
(0.612, 0.199)
14
(0.793, 0.905)
11
(0.277, 0.532)

30
40
(0.081, 0.002)
32
(0.717, 0.609)
29
(0.854, 0.556)

3
1
(0.179, 0.406)
3
(1.000, 0.997)
3
(1.000, 0.997)

15
16
(0.798 0.195)
14
(0.793, 0.905)
11
(0.277, 0.532)

30
41
(0.056, 0.001)
31
(0.855, 0.612)
28
(0.711, 0.501)

3
1
(0.179, 0.406)
2
(0.538, 0.826)
3
(1.000, 0.997)

15
18
(0.452, 0.190)
14
(0.793, 0.905)
11
(0.277, 0.532)

30
41
(0.056, 0.001)
29
(0.854, 0.556)
24
(0.254, 0.430)

3
1
(0.179, 0.406)
2
(0.538, 0.826)
3
(1.000, 0.997)

15
28
(0.003, 0.000)
16
(0.798, 0.888)
10
(0.168, 0.374)

30
46
(0.006, 0.001)
22
(0.123, 0.259)
22
(0.123, 0.259)

Notes: The closest numbers of VaR violations to theoretically expected ones are highlighted in bold.
The p-values for the unconditional coverage test by [28] and conditional coverage test by [11] at the 5%
signiﬁcance level are given in brackets in order.

21

(a) UGH approach

(b) GARCH-UGH approach

Figure 2: Twelve years (8 December 1997 to 9 November 2009) of in-sample backtesting of
the DJ index, and 99.5%-VaR violations by (a) the UGH approach and (b) the GARCH-
UGH approach when the top 15% of observations are used for the estimation. Red cross
marks denote the VaR violations.

22

050010001500200025003000-0.10-0.050.000.050.10Time (t)VaR050010001500200025003000-0.10-0.050.000.050.10Time (t)VaRTable 6: Out-of-sample evaluations of one-step ahead conditional VaR estimates from 8
December 1997 to 9 November 2009 at diﬀerent quantile levels for the negative log-returns
of DJ index by means of the number of VaR violations, unconditional and conditional
coverage tests.

Testing window
Estimation window
% of top obs. used
DJ:
0.999 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.995 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.99 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

3000
1000
5%

3
10
(0.001, 0.006)
3
(1.000, 0.997)
3
(1.000, 0.997)

15
40
(0.000, 0.000)
19
(0.320, 0.541)
19
(0.320, 0.541)

30
62
(0.000, 0.000)
33
(0.588, 0.598)
33
(0.588, 0.598)

10%

15%

20%

25%

3
9
(0.005, 0.020)
3
(1.000, 0.997)
4
(0.583, 0.885)

15
40
(0.000, 0.000)
18
(0.452, 0.676)
18
(0.452, 0.676)

30
64
(0.000, 0.000)
35
(0.371, 0.433)
30
(1.000, 0.738)

3
9
(0.005, 0.020)
3
(1.000, 0.997)
4
(0.583, 0.885)

15
40
(0.000, 0.000)
18
(0.452, 0.676)
18
(0.452, 0.676)

30
63
(0.000, 0.000)
32
(0.717, 0.663)
30
(1.000, 0.738)

3
7
(0.049, 0.142)
3
(1.000, 0.997)
4
(0.583, 0.885)

15
36
(0.000, 0.000)
16
(0.798, 0.888)
17
(0.612, 0.798)

30
63
(0.000, 0.000)
31
(0.855, 0.711)
28
(0.711, 0.717)

3
6
(0.128, 0.310)
3
(1.000, 0.997)
4
(0.583, 0.855)

15
29
(0.001, 0.001)
14
(0.793, 0.905)
17
(0.612, 0.798)

30
61
(0.000, 0.000)
28
(0.711, 0.717)
27
(0.576, 0.669)

Notes: The closest numbers of VaR violations to theoretically expected ones are highlighted in bold.
The p-values for the unconditional coverage test by [28] and conditional coverage test by [11] at the 5%
signiﬁcance level are given in brackets in order.

Figure 3: Out-of-sample backtesting of the DJ index from 8 December 1997 to 9 Novem-
ber 2009, and 99.9%-VaR estimates calculated using rolling estimation windows made of
1000 observations, with k corresponding to the top 15% observations from this window.
GARCH-UGH (blue line), GARCH-EVT (red line) and UGH (dark green line) estimates
are superimposed on the negative log-returns (black line).

23

1998200020022004200620082010-0.100.000.100.20TimeVaRTable 7: Out-of-sample evaluations of one-step ahead conditional VaR estimates from 13
August 1997 to 16 July 2009 at diﬀerent quantile levels for the negative log-returns of
NASDAQ index by means of the number of VaR violations, unconditional and conditional
coverage tests.

Testing window
Estimation window
% of top obs. used
NASDAQ:
0.999 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.995 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.99 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

3000
1000
5%

3
10
(0.001, 0.006)
6
(0.128, 0.370)
7
(0.049, 0.142)

15
39
(0.000, 0.000)
20
(0.218, 0.410)
16
(0.798, 0.888)

30
74
(0.000, 0.000)
34
(0.427, 0.544)
31
(0.855, 0.612)

10%

15%

20%

25%

3
8
(0.017, 0.057)
5
(0.292, 0.569)
7
(0.049, 0.142)

15
37
(0.000, 0.000)
17
(0.612, 0.798)
14
(0.793, 0.905)

30
74
(0.000, 0.000)
35
(0.371, 0.490)
28
(0.711, 0.501)

3
7
(0.049, 0.142)
5
(0.292, 0.569)
7
(0.049, 0.142)

15
35
(0.000, 0.000)
15
(1.000, 0.927)
13
(0.596, 0.821)

30
70
(0.000, 0.000)
31
(0.855, 0.612)
28
(0.711, 0.501)

3
4
(0.583, 0.855)
4
(0.583, 0.855)
7
(0.049, 0.142)

15
36
(0.000, 0.000)
16
(0.798, 0.888)
13
(0.596, 0.821)

30
65
(0.000, 0.000)
30
(1.000, 0.594)
24
(0.254, 0.430)

3
3
(1.000, 0.997)
3
(1.000, 0.997)
7
(0.049, 0.142)

15
40
(0.000, 0.000)
13
(0.596, 0.821)
13
(0.596, 0.821)

30
62
(0.000, 0.000)
25
(0.345, 0.287)
23
(0.180, 0.341)

Notes: The closest numbers of VaR violations to theoretically expected ones are highlighted in bold.
The p-values for the unconditional coverage test by [28] and conditional coverage test by [11] at the 5%
signiﬁcance level are given in brackets in order.

Figure 4: Out-of-sample backtesting of the NASDAQ index from 13 August 1997 to 16
July 2009, and 99.9%-VaR estimates calculated using rolling estimation windows made of
1000 observations, with k corresponding to the top 20% of observations from this window.
GARCH-UGH (blue line), GARCH-EVT (red line) and UGH (dark green line) estimates
are superimposed on the negative log-returns (black line).

24

1998200020022004200620082010-0.100.000.100.20TimeVaRTable 8: Out-of-sample evaluations of one-step ahead conditional VaR estimates from 29
May 1997 to 12 August 2009 at diﬀerent quantile levels for the negative log-returns of
NIKKEI index by means of the number of VaR violations, unconditional and conditional
coverage tests.

Testing window
Estimation window
% of top obs. used
NIKKEI:
0.999 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.995 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.99 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

3000
1000
5%

3
7
(0.049, 0.142)
4
(0.583, 0.855)
5
(0.292, 0.569)

15
34
(0.000, 0.000)
15
(1.000, 0.927)
13
(0.596, 0.821)

30
46
(0.006, 0.011)
33
(0.588, 0.598)
32
(0.717, 0.663)

10%

15%

20%

25%

3
6
(0.128, 0.310)
3
(1.000, 0.997)
4
(0.583, 0.855)

15
34
(0.000, 0.000)
15
(1.000, 0.927)
14
(0.793, 0.905)

30
47
(0.004, 0.007)
33
(0.588, 0.598)
29
(0.854, 0.741)

3
6
(0.128, 0.310)
2
(0.538, 0.826)
6
(0.128, 0.310)

15
34
(0.000, 0.000)
15
(1.000, 0.927)
13
(0.596, 0.821)

30
46
(0.004, 0.007)
33
(0.588, 0.598)
27
(0.576, 0.669)

3
5
(0.292, 0.569)
2
(1.000, 0.997)
6
(0.128, 0.310)

15
30
(0.000, 0.000)
15
(1.000, 0.927)
12
(0.421, 0.689)

30
45
(0.010, 0.015)
30
(1.000, 0.738)
27
(0.576, 0.669)

3
5
(0.292, 0.569)
1
(0.179, 0.406)
6
(0.128, 0.310)

15
23
(0.055, 0.062)
12
(0.421, 0.689)
12
(0.421, 0.689)

30
53
(0.000 0.000)
36
(0.286, 0.365)
26
(0.453, 0.601)

Notes: The closest numbers of VaR violations to theoretically expected ones are highlighted in bold.
The p-values for the unconditional coverage test by [28] and conditional coverage test by [11] at the 5%
signiﬁcance level are given in brackets in order.

Figure 5: Out-of-sample backtesting of the NIKKEI index from 29 May 1997 to 12 August
2009, and 99.9%-VaR estimates calculated using rolling estimation windows made of 1000
observations, with k corresponding to the top 10% of observations from this window.
GARCH-UGH (blue line), GARCH-EVT (red line) and UGH (dark green line) estimates
are superimposed on the negative log-returns (black line).

25

1998200020022004200620082010-0.100.000.100.20TimeVaRTable 9: Out-of-sample evaluations of one-step ahead conditional VaR estimates from 28
September 2002 to 14 December 2010 at diﬀerent quantile levels for the negative log-
returns of JPY/GBP exchange rate by means of the number of VaR violations, uncondi-
tional and conditional coverage tests.

Testing window
Estimation window
% of top obs. used
JPY/GBP:
0.999 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.995 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

0.99 Quantile
Expected
UGH

GARCH-UGH

GARCH-EVT

3000
1000
5%

3
7
(0.049, 0.142)
3
(1.000, 0.997)
6
(0.128, 0.310)

15
25
(0.018, 0.028)
21
(0.143, 0.295)
19
(0.320, 0.541)

30
47
(0.004, 0.000)
42
(0.038, 0.064)
38
(0.159, 0.227)

10%

15%

20%

25%

3
7
(0.049, 0.142)
2
(0.538, 0.826)
5
(0.292, 0.569)

15
27
(0.005, 0.010)
18
(0.452, 0.676)
19
(0.320, 0.541)

30
56
(0.000, 0.000)
46
(0.006, 0.012)
37
(0.215, 0.292)

3
6
(0.128, 0.310)
2
(0.538, 0.826)
5
(0.292, 0.569)

15
27
(0.005, 0.010)
15
(1.000, 0.927)
20
(0.219, 0.410)

30
55
(0.000, 0.000)
40
(0.081, 0.127)
38
(0.159, 0.227)

3
4
(0.583, 0.855)
2
(0.538, 0.826)
6
(0.128, 0.310)

15
34
(0.000, 0.000)
14
(0.793, 0.905)
20
(0.219, 0.410)

30
59
(0.000, 0.000)
38
(0.159, 0.227)
38
(0.159, 0.227)

3
4
(0.583, 0.855)
2
(0.538, 0.826)
7
(0.049, 0.142)

15
45
(0.000, 0.000)
12
(0.421, 0.689)
20
(0.219, 0.410)

30
67
(0.000, 0.000)
34
(0.472, 0.523)
36
(0.286, 0.365)

Notes: The closest numbers of VaR violations to theoretically expected ones are highlighted in bold.
The p-values for the unconditional coverage test by [28] and conditional coverage test by [11] at the 5%
signiﬁcance level are given in brackets in order.

Figure 6: Out-of-sample backtesting of the JPY/GBP exchange rate from 28 September
2002 to 14 December 2010, and 99.9%-VaR estimates calculated using rolling estimation
windows made of 1000 observations, with k corresponding to the top 10% of observations
from this window. GARCH-UGH (blue line), GARCH-EVT (red line) and UGH (dark
green line) estimates are superimposed on the negative log-returns (black line).

26

2004200620082010-0.050.000.050.10TimeVaR