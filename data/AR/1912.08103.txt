0
2
0
2

y
a
M
5
2

]
L
M

.
t
a
t
s
[

2
v
3
0
1
8
0
.
2
1
9
1
:
v
i
X
r
a

Proceedings of Machine Learning Research vol xxx:1–15, 2020

A Finite-Sample Deviation Bound for Stable Autoregressive Processes

Rodrigo A. Gonz´alez
Cristian R. Rojas
Division of Decision and Control Systems, KTH Royal Institute of Technology, Stockholm, Sweden

GRODRIGO@KTH.SE
CRRO@KTH.SE

Abstract
In this paper, we study non-asymptotic deviation bounds of the least squares estimator for Gaussian
AR(n) processes. By relying on martingale concentration inequalities and a tail-bound for χ2
distributed variables, we provide a concentration bound for the sample covariance matrix of the
process output. With this, we present a problem-dependent ﬁnite-time bound on the deviation
probability of any ﬁxed linear combination of the estimated parameters of the AR(n) process. We
discuss extensions and limitations of our approach.
Keywords: Autoregressive Processes, Non-Asymptotic Estimation, Least Squares, Finite Sample
Analysis.

1. Introduction

Autoregressive (AR) processes are ubiquitous in engineering sciences, as they are applied in econo-
metrics, time series analysis (Box et al., 2015), system identiﬁcation (Ljung, 1999), signal process-
ing (Kay, 1993), machine learning and control.

Given sampled data, the identiﬁcation of the parameters of an AR process is usually done
by ordinary least squares, which is known to have asymptotically optimal statistical performance
(Mann and Wald, 1943; Durbin, 1960) and is related to Maximum Likelihood in a Gaussian frame-
work. Despite its success in practical applications, most analyses of the least squares method are
asymptotic. Finite-time analyses of this method are still rare in the literature, despite being im-
portant for computing the number of samples needed for achieving a speciﬁed accuracy, deriving
ﬁnite-time conﬁdence sets, and designing robust control schemes. Non-asymptotic performance
bounds have been historically difﬁcult to derive since most of the classical statistical methods are
better suited for asymptotic results.

In recent years, new statistical tools from the theory of self-normalizing processes (De la Pe˜na et al.,

2008) and high dimensional probability (Wainwright, 2019) have shown to be useful for analyzing
a wide range of regression models. These tools have impulsed research on ﬁnite-time properties
of the least squares estimator, with unifying efforts from the system identiﬁcation, control and ma-
chine learning communities. Among topics of interest, we can ﬁnd sample complexity bounds
(Jedra and Proutiere, 2019), 1
δ probability bounds on parameter errors (Sarkar and Rakhlin,
2019), and conﬁdence bounds (Lattimore and Szepesv´ari, 2020, Chap. 20).

−

Even though autoregression is a key aspect in dynamical systems and regression models, ﬁnite-
time properties of AR(n) processes have not yet been studied deeply. AR(n) processes are of partic-
ular interest, as they build the foundations for studying general regression models such as ARX and
ARMAX models, which are widely used in linear system identiﬁcation (Ljung, 1999). Autoregres-
sive processes are also essential for two-stage ARMA estimation algorithms (Stoica and Moses,

2020 R.A. Gonz´alez & C.R. Rojas.

c
(cid:13)

 
 
 
 
 
 
A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

2005), and for speech production models (Makhoul, 1975). For a greater understanding on how
least squares performs on different autoregressive processes for ﬁnite-sample data, here we perform
a non-asymptotic analysis of the least squares estimator of the coefﬁcients of these processes. In
summary, the main results of this paper are:

•

•

via martingale concentration inequalities and bounds on χ2-distribution tails, we derive a
ﬁnite-time problem-dependent concentration bound for the sample data covariance matrix of
an n-th order autoregressive process;

using the previous result, we provide a bound on the deviation of any ﬁxed linear combination
of the parameters of an AR(n) process around its true value, such that larger deviations occur
only with probability at most δ.

The rest of this paper is organized as follows. Our work is put into context in Section 2, and we
deﬁne our notation in Section 3. In Section 4 the problem is explicitly formulated, and a preliminary
result is given. We state and prove our concentration inequalities for the sample covariance matrix
and deviation bound of the parameters of an AR(n) process in Section 5. Section 6 provides the
proof for a key result used in the previous section, and a discussion of the results is presented in
Section 7. We also include an Appendix with supplementary material of proofs and auxiliary results.

2. Relation to prior work

In a time-series context, Bercu et al. (1997) and Bercu (2001) studied large deviation rates of the
least squares estimator in an AR(1) process. These contributions provide problem independent
bounds, and do not generalize to AR(n) processes. A problem dependent ﬁnite-time deviation
and variance bound was provided by Gonz´alez and Rojas (2020) for stable and unstable AR(1)
processes. Unfortunately, the tools used in that work cannot be extended to a multivariate setting.
Asymptotic properties of AR(n) models were obtained by Lai and Wei (1983).

In a broader context, one of the ﬁrst non-asymptotic results in system identiﬁcation was pre-
sented in Campi and Weyer (2002), where a uniform bound for the difference between empirical
and theoretical identiﬁcation costs was obtained. More recently, among works that have analyzed
ﬁnite-time identiﬁcation for stochastic processes are Jedra and Proutiere (2020); Sarkar and Rakhlin
(2019); Simchowitz et al. (2018); Faradonbeh et al. (2018) and Zheng and Cheng (2018). These
contributions consider state-space formulations with ﬁrst order vector autoregressive models that,
contrary to the description of an AR(n) process in state-space, normally assume that the noise pro-
cess perturbs all states instead of only one. In particular, the performance bounds in Simchowitz et al.
(2018) consider the estimation of the full transition matrix instead of the parameters of interest for
AR(n) modeling. By leveraging the direct relationship between the coefﬁcients of interest and
the transition matrix of the underlying state-space in controller form, bounds for AR(n) processes
could possibly be obtained by ﬁnding the (ﬁnite-time) optimal projection of the A matrix whose
error in operator norm is bounded in Simchowitz et al. (2018), such that the resulting matrix is
exactly the one provided by the LS estimate of the underlying AR(n) process. After this, a concen-
tration inequality that bounds the error over ˆA(T ) and the transition matrix in controller form of the
AR(n) process would be needed. Although plausible, the results we seek do not seem direct from
(Simchowitz et al., 2018). Instead, our analysis resembles that of (Sarkar and Rakhlin, 2019, Theo-
rem 1) in the derivation of a matrix concentration bound, and similarly to the works cited above, a
Gramian matrix associated with the real process dictates the learning rate.

2

A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

3. Notation

n, ρ(A) denotes its spectral radius. If x is a vector and A is a ﬁxed positive
Given a matrix A
∈
deﬁnite matrix, then
kA is the weighted
k2 and
∞
kA := √x⊤Ax). If B is an event, Bc denotes its complement, and P(B) refers to
2-norm (i.e.,
x
k
its probability of occurrence. E

denotes the expected value of the random variable y.

-norm of x, while

denote the 2 and

Rn
×
x
k

x
k

x
k

k∞

y
{

}

4. Problem formulation and preliminary result

Consider the following AR(n) process described by

1θ0 + et,

1

−

−

−

−

−

θ1xn

2 . . . yt

1 := [yt

yt = Y ⊤t
−

Rn. Furthermore, assume that

(1)
n], et is a Gaussian white noise of variance σ2, and the pa-
where Y ⊤t
1 yt
−
rameter vector is θ0 := [θ1 θ2 . . . θn]⊤ ∈
is a stationary
process, and that θ0 is such that the AR(n) process is asymptotically stationary, which implies that
p(x) = xn
θn is a Schur polynomial. In this work, we are interested
θn
− · · · −
in how w⊤ ˆθN concentrates around its true value w⊤θ0, where w
k2 = 1 is ﬁxed and
w
k
ˆθN is the least squares estimator of θ0 given the data
N
t=1. By allowing w to be chosen freely,
we study deviation probabilities for single parameters, or linear combinations of them. Note that
this probability depends on the true parameters, an thus it gives information about how easily the
parameters can be identiﬁed through ordinary least squares for a particular system. In other words,
our interest is in interpretability; in particular, we are concerned on how the least squares estimator
performs under different AR processes.

Rn with

yt}
{

yt}
{

1x

−

∈

−

Unfortunately, an explicit expression of the deviation probability (or equivalently, the conﬁdence
region) of interest is elusive in the literature. Therefore, it is of our interest to ﬁnd an upper bound
of it instead.
1]⊤ and E := [en+1 . . . eN ]⊤, we can write
w⊤(ˆθN −

1Y ⊤E, and hence we pursue a bound of the form

If we deﬁne Y := [Yn . . . YN

θ0) as w⊤(Y ⊤Y )−

−

P(
w⊤(Y ⊤Y )−
|

1Y ⊤E

> ε)

δ,

≤

|

(2)

where ε can be expressed as a function of δ, N , and the true parameters. Note that the stochas-
tic quantity w⊤(ˆθN −
θ0) is a self-normalized process. That is, it is unit free and therefore not
affected by scale changes (De la Pe˜na et al., 2008). These processes are now ubiquitous in the ma-
chine learning community, as they arise naturally in, e.g., ﬁnite-time analysis of linear systems
(Simchowitz et al., 2018) and stochastic bandit problems (Krishnamurthy et al., 2018).

To derive a bound like (2), we make use of a martingale tail inequality introduced in Abbasi-Yadkori et al.

(2011), which is valid for sub-Gaussian stochastic processes.

Proposition 1 (Abbasi-Yadkori et al. (2011)) Let
valued stochastic process such that ηt is
some R > 0, i.e.

ηt}∞t=1 be a real-
{
Ft-measurable and ηt is conditionally R-sub-Gaussian for

{Ft}∞t=0 be a ﬁltration. Let

λ2R2
2
∈
Xt}∞t=1 be an Rd-valued stochastic process such that Xt is
Let
{
is a d
×

d positive deﬁnite matrix. For any t

R E[eληt

1, deﬁne

|Ft
−

exp

1]

≥

≤

(cid:18)

λ

∀

(cid:19)
Ft
−

.

t

t

1-measurable. Assume that V

V t = V +

XsX ⊤s ,

St =

ηsXs.

(3)

Xs=1

Xs=1

3

A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

Then, for any δ > 0, with probability at least 1

Stk
k

2
V t

−1

≤

2R2 log

δ, for all t

1,

≥

−
det(V t)1/2 det(V )−
δ

1/2

.

!

Although the result of Proposition 1 is also a deviation bound similar to (2), it relies on the fact that
the matrix V is positive deﬁnite, which is valid only for regularized least squares problems. Despite
t
s=1 XsX ⊤s , which plays an important
this, in V t we recognize the sample covariance matrix
role in our main result. The key idea behind the proposed approach is to ﬁrst obtain a ﬁnite-sample
N
s=n YsY ⊤s , and use this result together with Proposition
probability bound on the matrix Y ⊤Y =
1 to derive the novel deviation bound.

P

−

1

5. Main results

P

In this section, we present our ﬁnite-time bounds for AR processes. Firstly, in Theorem 2 we derive
δ concentration bound for the sample covariance matrix Y ⊤Y , and then use this result to obtain
a 1
a deviation bound for the least squares estimator for general AR(n) processes, which is presented
in Theorem 6. For the following, we express the AR(n) process in a state-space formulation

−

xt+1 =

θ0⊤
In

"

0
0n

1#

xt +

1
0n

(cid:20)

1(cid:21)

et+1,

Yt =

In 0n

xt,

1

(4)

×

×
where we denote from now on the transition matrix and input vector in (4) as A and B respectively.

×

(cid:3)

(cid:2)

Theorem 2 Consider the AR(n) process described in (1), and Y = [Yn . . . YN
ǫ > 0, deﬁne the following quantities:

−

1]⊤. Given

V : = σ2 ∞
Xi=0

AiBB⊤(A⊤)i,

Vdn : = (N

Vup : = (N

n)

In 0

(cid:2)

(cid:3)

n)

In 0

−

−

δ(ǫ, N ) : = 2

(

(cid:2)
√2 exp

+ exp

Ai(A⊤)i

Ai(A⊤)i

,

,

In
0

In
0

(cid:21)

(cid:21)

! (cid:20)

! (cid:20)
N

+ exp(

ǫ√N )

)

,

−

V

−

ǫσ2 ∞
Xi=0
V + ǫσ2 ∞
Xi=0
+ exp

(cid:3)
n)σ2ǫ
(N
−
24nE
y2
1} (cid:19)
{
n)ǫ
(N
−
k2 + 1)2 ˜β (cid:21)
θ
72(
k
θ1ejω(n
ejωn

−

(cid:18)

−

(cid:20)

MΦ : = max
ω |

−

(n + 1)N

˜β : =

1)

−

θn|
− · · · −
2MΦ(1 + ǫ−

2,

−

1/2)

N 1/4

.

#

E
y2
1}
ǫσ2 +
{

where

Then, for all ǫ > 0 such that Vdn

≻
P(Vdn

N

n "

−
0, we have
Y ⊤Y

(cid:22)

(cid:22)

Vup)

1

−

≥

δ(ǫ, N ).

4

n
−
2  

1 +

ǫ
3 − r

1 +

2ǫ
3 !#

"−

(5)

(6)

(7)

(8)

(9)

 
 
 
A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

Proof The AR(n) process can be rewritten as in (4), where xt is equal to
⊤. Note that
the eigenvalues of A are precisely the poles of the autoregressive process, with an extra eigenvalue
at 0. We are interested in bounding

Y ⊤t

yt

−

(cid:3)

(cid:2)

n

Y ⊤Y =

In 0

N

1

−

(cid:2)

Xi=n

(cid:3)

xix⊤i

In
0

.

(cid:21)

(cid:20)

The approach consists in ﬁrst determining a concentration bound for
it to a concentration bound for Y ⊤Y . In this spirit, we write

1

N
i=n xix⊤i , and then relating

−

P

xi+1x⊤i+1 = Axix⊤i A⊤ + Axiei+1B⊤ + Bei+1x⊤i A⊤ + e2

i+1BB⊤,

(10)

and denote VN as (N

1

n)−

−

1

N
i=n xix⊤i . If we sum over i = n

−

1, . . . , N

−

−

2 in (10), we obtain

VN = AVN A⊤+

1

−

N

n "

P
A(xn−1x⊤

n−1 −

xN −1x⊤

N −1)A⊤+

N −2

ei+1AxiB⊤+ ei+1Bx⊤

i A⊤+ e2

i+1BB⊤

i=n−1
X
(cid:0)
EN

.
#
(cid:1)

|

}
Since the AR(n) process is asymptotically stationary, the Lyapunov equation above has as solution
∞i=0 AiEN (A⊤)i. By construction, VN tends to V (deﬁned in (5)) with probability 1 as N
VN =
tends to inﬁnity (see, e.g. (S¨oderstr¨om, 2002, p. 64)). Thus, our goal is to obtain a ﬁnite-sample
concentration bound that relates VN with V . For this, we bound
i by its variance, and
bound the other terms of EN by a small matrix quantity ǫI, for all N > N (ǫ). Lemmas 3, 4 and 5
are needed for this purpose, which bound the probability of the following events:

N
2
−
i=n

1 e2

P

P

{z

−

E1 :=
E2 :=
E3 :=

ρ

h

n

ρ

A(xn

−
N
2
−
i=n

n

ρ

−
hP
N
2
−
i=n

1

−

1x⊤n
−
1 e2

xN

1x⊤N

1)A⊤

−

−
BB⊤

1 −
i+1BB⊤ −
ei+1AxiB⊤ + ei+1Bx⊤i A⊤

≤
i
ǫσ2(N

≤

i

ǫσ2(N

n)/3

−

−
n)/3

o

,

o
ǫσ2(N

−

≤

,

n)/3

.

o

Lemma 3 Consider the process described in (4), where
variance σ2, and

(cid:1)i
ei}
{
is a stationary random process. Then,

hP

n

(cid:0)

xt}
{

is a Gaussian zero-mean i.i.d. of

P (

E1)

1

≥

−

2√2 exp

−

n)σ2ǫ
(N
−
24nE
y2
1} (cid:19)
{

.

(cid:18)
be a Gaussian zero-mean i.i.d. sequence of variance σ2. Then,

Lemma 4 Let

ei}
{

P (

E2)

1

−

≥

2 exp

N

n
−
2  

"−

1 +

ǫ
3 − r

1 +

2ǫ
3 !#

.

Lemma 5 Consider the same assumptions as in Lemma 3. For any ǫ > 0, we have

P (

E3)

≥

1

−

2 exp

(N
θ0
72(
k

n)ǫ
−
k2 + 1)2 ˜β (cid:21)

−

(cid:20)

2 exp(

−

−

ǫ√N ),

where MΦ and ˜β are deﬁned in (7) and (8) respectively.

5

A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

In the main text, we provide proof of Lemma 5 only, which can be found in Section 6. With these
three lemmas, and by the subadditivity of the spectral radius of Hermitian matrices (Bernstein, 2009,
Fact 5.12.2), we have

E1 ∩ E2 ∩ E3 =
⇒

ρ(EN −

σ2BB⊤)

≤

which occurs with probability not less than 1

−

σ2ǫ =

⇒

σ2(BB⊤

EN (cid:22)
δ(ǫ, N ). This also implies (9).

ǫI)

(cid:22)

−

σ2(BB⊤ + ǫI),

Theorem 2 delivers a ﬁnite-sample bound on the sample covariance matrix. Naturally, this
matrix will deviate from its expected value by a small amount for large sample sizes. Note that this
bound depends on a ﬁxed value ǫ, which can be chosen arbitrarily small. As most self-normalized
process bounds, δ(ǫ, N ) does not depend on the variance of the process noise.

With this result, we are ready to state the desired deviation bound in Theorem 6.

Theorem 6 Consider the AR(n) process described in (1), where θ0 is assumed to yield an asymp-
totically stationary process,
yt}
{
is stationary. Then,

is an i.i.d. Gaussian random process with variance σ2, and

et}
{

P

w⊤(ˆθN −

|

θ0)
|

> 2σ

w⊤V −
dn
k

1/2

log

k2v
u
u
t

det(VupV −

dn + In)1/2

1

δ(ǫ, N )

≤

!


2δ(ǫ, N ),

(11)



where Vdn, Vup and δ(ǫ, N ) are as described in Theorem 2.

Proof We will follow the main ideas in (Sarkar and Rakhlin, 2019, Theorem 1). We start by writing
an upper bound using the Cauchy-Schwartz inequality

w⊤(ˆθN −
|

θ0)
|

=

w⊤(Y ⊤Y )−
|

1Y ⊤E

w⊤(Y ⊤Y )−

1/2

(Y ⊤Y )−

1/2Y ⊤E

k2k

k2.

| ≤ k
In Theorem 2 we have found deterministic matrices Vdn, Vup and a scalar δ(ǫ, N ) such that, for the
δ(ǫ, N ). The next step is to bound the
event
Vup
self-normalized norm. This can be done by ﬁrst deﬁning the event

, we have P(
E
}

Vdn
{

pm :=

Y ⊤Y

pm)

−

≥

(cid:22)

(cid:22)

E

1

sn :=

E




Y ⊤E
k

k(Y ⊤Y +Vdn)−1

≤ s

2σ2 log

(cid:18)

det(Y ⊤Y + Vdn)1/2 det(Vdn)−
δ(ǫ, N )

1/2

.




(cid:19)

It follows from Proposition 1 that P(
E
2Y ⊤Y , which implies (Y ⊤Y + Vdn)−



sn)
1

≥
(cid:23)

1
1
2 (Y ⊤Y )−

−

δ(ǫ, N ). Also, under

pm we have that Y ⊤Y +Vdn



1. So, considering the set

E

(cid:22)
sn, we obtain

pm

E

∩ E

pm

sn =

pm

E

∩ E

⇒ E

∩ 

Furthermore, observe that P(

E

(Y ⊤Y )−
k

1/2Y ⊤E

2σ

k2 ≤

log

det(VupV −

dn + In)1/2

1

δ(ǫ, N )

.

!


v
u
u
t

pm

sn)

1

−

≥

∩ E

2δ(ǫ, N ). So, if

pm

E

∩ E

sn holds, then



w⊤(Y ⊤Y )−
|

1Y ⊤E

2σ

w⊤V −
dn
k

| ≤

1/2

log

k2v
u
u
t

6

det(VupV −

dn + In)1/2

1

δ(ǫ, N )

,

!

(12)

 
 
 
A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

which means that the probability of the event in (12) is at least 1
complement event, we obtain the probability bound (11).

−

2δ(ǫ, N ). By considering the

Theorem 6 provides a ﬁnite-sample conﬁdence bound on the deviation of the weighted parame-
ter vector w⊤ ˆθN with respect to its asymptotic value w⊤θ0. This result delivers probability bounds
on the deviation each parameter θi individually, as well as any linear combination of them. Note
that ǫ can be considered a tightness variable, as by setting ǫ small, more samples are required to
guarantee a desired conﬁdence level, but the probability bound will be tighter.

To end this analysis, we derive the decay rate of our probability bound in Corollary 7.

Corollary 7 If ǫ is picked as λn −

N −

1/2, where λn is the smallest eigenvalue of

σ2

In
0

(cid:20)

⊤ ∞

Ai(A⊤)i

(cid:21)

Xi=0

1/2

−

In
0

(cid:20)

(cid:21)!

⊤

V

In
0

(cid:20)

(cid:21)

In
0

(cid:20)

σ2

In
0

(cid:20)

(cid:21)  

⊤ ∞

Ai(A⊤)i

(cid:21)

Xi=0

/2

−⊤

,

In
0

(cid:20)

(cid:21)!

then δ
shows that the rate of decay of the probability bound is at least exponential in √N.

λn√N for large N and the deviation in (11) is asymptotically a constant in N . This

Ce−

∼

6. Proof of Lemma 5

Here we present a sketch of the proof of Lemma 5, in which we use a martingale concentration
by applying a concen-
inequality from Simchowitz et al. (2018) and exploit the Gaussianity of
tration inequality for χ2 random variables found in Laurent and Massart (2000).
Proof of Lemma 5 For any vector q :=

Rn+1 of unit 2-norm, we have

et}
{

⊤

q1

˜q⊤

q⊤

N

1

−

n

N

1

−

Xi=n

ei(Axi

−

(cid:2)

1B⊤ + Bx⊤i
−

(cid:3)
1A⊤)

!

q =

2q1

N

n

−

(q1θ0⊤ + ˜q⊤)

N

1

−

Xi=n

eiYi

1.

−

(13)

Since (13) is symmetric around zero, it is sufﬁcient to bound its upper tail. Next, we denote
the vector z := 2q1(q1θ0 + ˜q). By using Lemma 4.2 of Simchowitz et al. (2018), with Zt =
2
2, we obtain the

, Wt = et, and β = ǫ ˜β(N

n)σ2 max

z, Yt

q
k

2=1 k
k

2q1θ0⊤ + ˜q⊤k

1i

−

−

1
√N
−
inequality

n h

∈

N −1

P

"(

i=n
X

z, Yi−1
h
N

ei
i
n ≥

−

σ2ǫ
3 ) ∩ (

N −1

i=n
X

Yi−1

k
σ2(N

k
−

2
2
n) ≤

ǫ ˜β

)# ≤

exp 

−

(N

72 ˜β max

kqk2=1 k

n)ǫ

−
q1(q1θ0 + ˜q)
k

.



2
2

Using the well-known inequality P(A
(n + 1)

i , we obtain

2
1 y2

N
i=

−
−

B)

∩

≥

P(A)

−




P(Bc), and the fact that



2
1k
2 ≤

N
1
−
i=n k

Yi

−

P

P

N −1

i=n
X

P
z, Yi−1
h
N

ei
i
n ≥

−

σ2ǫ
3 ! ≤

exp 

−




(N

−

n)ǫ
q1(q1θ0 + ˜q)
k

72 max

kqk2=1 k

n + 1

N −2

σ2(N

n)

−

i=−1
X

y2
i > ǫ ˜β

!

.

+ P

2
2

˜β 



To tackle the last probability, we note that
metric Toeplitz covariance matrix of eigenvalues
(cid:2)
χ2 random variable, whose distribution is equal to the distribution of v⊤RN v, where v

(0, RN ), where RN is a sym-
i is a generalized
(0, IN ).

yN
⊤
N
i=1. Hence, Z =
(cid:3)

· · ·
λi}
{

2
1 y2

∼ N

N
i=

−
−

y

−

−

1

2

P

∼ N

7

 
 
 
 
A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

By the singular value decomposition RN = UN DN U ⊤N where DN = diag
λi}
{
matrix, and the rotation invariance of v (Vershynin, 2018, Chap. 3), we see that

and UN is a unitary

where ˜v

∼ N

P

N

2

−

1
Xi=
−

E

y2
i −

N

2

−

(

1
Xi=
−

y2
i

)

> t

!

= P

N

Xi=1

λi(˜v2

i −

1) > t

,

!

(0, IN ). Then, by (Laurent and Massart, 2000, Lemma 1),

P

N

2

−

1
Xi=
−

E

y2
i −

N

2

−

(

1
Xi=
−

y2
i

)

λ
> 2
k

k2√t + 2
λ
k

k∞

t

exp(

−

t).

! ≤

(14)

It is known (see, e.g. (Gray, 2006, Section 4.2)) that the maximum eigenvalue of RN is bounded by
σMΦ, where MΦ is deﬁned as in (7). By letting t = ǫ√N in (14), and upper bounding
k2 and
λ
k

by √N MΦ and MΦ respectively, we deduce that

λ
k

k∞

n + 1

σ2(N

n)

−

N

2

−

1
Xi=
−

y2
i >

(n + 1)N

E

N

n

−

(cid:20)

y2
1}
σ2 +
{
ǫ ˜β

2MΦ(ǫ + √ǫ)
N 1/4

exp(

−

≤

ǫ√N ).

P 








(cid:21)






}

{z
θ0
Finally, note that max
(
2=1 k
k
k
complement event, we reach the bound in Lemma 5.

2
q1(q1θ0 + ˜q)
2 ≤
k

q
k

|

k2 + 1)2. With this, and considering the

7. Discussion and conclusions

P

In this paper, we have provided ﬁnite-sample guarantees for the least squares estimates of the co-
efﬁcients of general AR(n) processes. For this, a concentration bound for the sample covariance
∞i=0 Ai(A⊤)i in Vdn and Vup shows that
matrix was derived. In this bound, the Gramian matrix
faster processes need less samples to guarantee concentration of the covariance matrix, which is a
natural result. Regarding Theorem 6, we ﬁnd that the ﬁxed vector w impacts the conﬁdence bound
through the inverse of Vdn, which resembles the results obtained in (Lattimore and Szepesv´ari, 2020,
Eq. 20.2) for least squares estimates of linear bandit algorithms with deterministic actions. The
log det term is also unsurprising, as it also appears in ﬁnite-sample analysis of LTI systems (see,
e.g. (Sarkar and Rakhlin, 2019, Eq. 12)). The deterministic matrices Vdn and Vup in (11) capture
the correct behavior of the conﬁdence bound, since it is large when the uncertainty on the sample
covariance matrix is also large. Also, note that the proof of Theorem 6 heavily relies on bounding
the probability of the normal matrix Y ⊤Y , but it is easily decoupled from Theorem 2. That is, if
tighter bounds for
pm can be found, then Theorem 6 can be directly improved. Future work con-
cerns proving ﬁnite-time variance bounds for the estimated parameters, extending the analysis for
ARX models under sub-Gaussian noise, and deriving sharp lower bounds for AR(n) processes.

E

Acknowledgments

This work was supported by the Swedish Research Council under contract number 2016-06079
(NewLEADS).

8

 
 
 
A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

References

Y. Abbasi-Yadkori, D. P´al, and C. Szepesv´ari. Improved algorithms for linear stochastic bandits. In

Advances in Neural Information Processing Systems, pages 2312–2320, 2011.

B. Bercu. On large deviations in the gaussian autoregressive process: stable, unstable and explosive

cases. Bernoulli, 7(2):299–316, 2001.

B. Bercu, F. Gamboa, and A. Rouault. Large deviations for quadratic forms of stationary gaussian

processes. Stochastic Processes and their Applications, 71(1):75–90, 1997.

D. S. Bernstein. Matrix Mathematics: Theory, Facts, and Formulas. Princeton University Press,

2009.

G. E. P. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung. Time series analysis: forecasting and

control, 5th ed. John Wiley & Sons, 2015.

M. C. Campi and E. Weyer. Finite sample properties of system identiﬁcation methods.

IEEE

Transactions on Automatic Control, 47(8):1329–1334, 2002.

V. H. De la Pe˜na, T. L. Lai, and Q.-M. Shao. Self-normalized processes: Limit theory and Statistical

Applications. Springer, 2008.

J. Durbin. Estimation of parameters in time-series regression models. Journal of the Royal Statisti-

cal Society: Series B (Methodological), 22(1):139–153, 1960.

M. Faradonbeh, A. Tewari, and G. Michailidis. Finite time identiﬁcation in unstable linear systems.

Automatica, 96:342–353, 2018.

R. A. Gonz´alez and C. R. Rojas. Finite sample deviation and variance bounds for ﬁrst order au-
toregressive processes. In Proceedings of the 45th IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), accepted for publication, 2020.

R. M. Gray. Toeplitz and Circulant matrices: A review. Foundations and Trends in Communications

and Information Theory, 2(3):155–239, 2006.

Y. Jedra and A. Proutiere. Sample Complexity Lower Bounds for Linear System Identiﬁcation. In
58th IEEE Conference on Decision and Control (CDC), Nice, France, pages 2676–2681, 2019.

Y. Jedra and A. Proutiere. Finite-time identiﬁcation of stable linear systems: Optimality of the

least-squares estimator. arXiv preprint arXiv:2003.07937, 2020.

S. M. Kay. Fundamentals of Statistical Signal Processing: Estimation Theory. Prentice Hall, 1993.

A. Krishnamurthy, Z. S. Wu, and V. Syrgkanis. Semiparametric Contextual Bandits. In International

Conference on Machine Learning (ICML), pages 2776–2785, 2018.

T. L. Lai and C. Z. Wei. Asymptotic properties of general autoregressive models and strong con-
sistency of least-squares estimates of their parameters. Journal of multivariate analysis, 13(1):
1–23, 1983.

9

A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

T. Lattimore and C. Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020.

B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Annals

of Statistics, pages 1302–1338, 2000.

L Ljung. System Identiﬁcation: Theory for the User, 2nd Edition. Prentice-Hall, 1999.

J. Makhoul. Linear Prediction: A tutorial review. Proceedings of the IEEE, 63(4):561–580, 1975.

H. B. Mann and A. Wald. On the statistical treatment of linear stochastic difference equations.

Econometrica, Journal of the Econometric Society, pages 173–220, 1943.

D. S. Mitrinovic and P. M. Vasic. Analytic inequalities, volume 1. Springer, 1970.

T. Sarkar and A. Rakhlin. Near optimal ﬁnite time identiﬁcation of arbitrary linear dynamical

systems. In International Conference on Machine Learning, pages 5610–5618, 2019.

M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht. Learning without mixing: Towards
a sharp analysis of linear system identiﬁcation. In Conference On Learning Theory, pages 439–
473, 2018.

T. S¨oderstr¨om. Discrete-time Stochastic Systems: Estimation and Control. Springer, 2002.

P. Stoica and R. L. Moses. Spectral Analysis of Signals. Prentice Hall, 2005.

R. Vershynin. High-dimensional probability: An introduction with applications in data science.

Cambridge University Press, 2018.

M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Univer-

sity Press, 2019.

Y. Zheng and G. Cheng. Finite time analysis of vector autoregressive models under linear restric-

tions. arXiv preprint arXiv:1811.10197, 2018.

10

A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

Appendix A. Proofs of Lemmas 3 and 4

Proof of Lemma 3. For simplicity we consider ˜ǫ := σ2ǫ. We ﬁrst see that

˜ǫ
3

(cid:21)
1x⊤N

−

−

(cid:19)
1)A⊤

>

˜ǫ
3

(cid:19)

(cid:21)

P

ρ

(cid:18)

(cid:20)

N

1

−
P

A(xn

1x⊤n
−

−

1 −

xN

−

1x⊤N

−

n

1)A⊤

>

1

ρ

A(xn

1 + xN

≤

≤

≤

≤

P

P

P

(cid:18)

(cid:18)

(cid:18)

(cid:18)

N
(cid:20)
Axn
k
N
xn
k
N
xn
k
N

n
−
2
1k
2
−
n
−
2
1k
2
−
n
−
2
1k
2
−
n
−

+ k

+ k

>

xN
N
˜ǫ
6 ∪

1x⊤n
−

−
AxN
N

2
1k
2
−
n
−
2
1k
2
−
n
−
xN
k
N

˜ǫ
3
2
1k
2
n

>

−
−

>

˜ǫ
3

(cid:19)

(cid:19)

>

˜ǫ
6

.

(cid:19)

By stationarity, the probabilities of the events

En =
are equal. For some ﬁxed s > 0, we can bound P(
Chap. 2) and later compute the moment generating function of the generated χ2 distribution:

(cid:27)
En) by Chernoff’s inequality (Wainwright, 2019,

EN =

−
−

>

>

(cid:26)

(cid:26)

(cid:27)

,

,

xn
k
N

2
1k
2
−
n
−

˜ǫ
6

xN
k
N

2
1k
2
n

˜ǫ
6

P (

En)

≤

exp

˜ǫ
6

s

−

(cid:18)

E

exp

"

(cid:19)

s

−

n

N

n

1

−

1
t=
X
−

y2
t

!#

= exp

˜ǫ
6

s

−

(cid:18)

(cid:19)

n
i=1

1

1

−

2s

N

−

n λi(Rn)

,

n is the Toeplitz covariance matrix of [y

where Rn ∈ S
of the Rn, and 0 < s < (N
as Proposition 11 in the Auxiliary Results section), we obtain

1], λi(Rn) is the i-th eigenvalue
yn
n)/(2λmax(Rn)). By using the Weierstrass product inequality (stated

1 · · ·

−

−

−

Q

q

P (

En)

≤

exp

˜ǫ
6

s

−

(cid:18)

1

−

(cid:19) (cid:18)

2snE
N

y2
1}
{
n
−

(cid:19)

1/2

−

.

By noting that

we set s = (N

n)/(4nE

−

P

N

n

−
2λmax(Rn)
y2
1}
{
xn
k
N

2
1k
2
−
n
−

>

N
n
−
2tr(Rn)

>

N
4nE

n
−
y2
1}
{

> 0,

) to derive the exponential inequality

>

˜ǫ
6

√2 exp

≤

n)˜ǫ
(N
−
−
24nE
y2
1} (cid:19)
{

.

(cid:18)

(cid:18)
So, by the chain of inequalities above, we conclude that

(cid:19)

1
σ2(N

P

ρ

(cid:18)

(cid:20)

−

A(xn

1x⊤n
−

−

1 −

xN

−

1x⊤N

−

1)A⊤

n)

>

ǫ
3

(cid:21)

≤

(cid:19)

2√2 exp

−

n)σ2ǫ
(N
−
24nE
y2
1} (cid:19)
{

,

(cid:18)

which implies the statement we wanted to prove.

(cid:4)

11

 
A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

Proof of Lemma 4. Event

E2 is equivalent to the event

1

−

n

N

N

2

−

Xi=n
−

1

e2
i+1 −

σ2

σ2ǫ
3 )

.

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

((cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

To bound the probability of this event, we shall use a corollary of Lemma 1 of Laurent and Massart
(2000) (Corollary 10 in the Auxiliary Results section), which gives high probability bounds on the
tails of a χ2 statistic. Via this result and unnormalizing the χ2 statistic, we obtain that

P

N

1

−

n

N

1

−

Xi=n

σ2

e2
i −

≥

2σ2

x

−

n

N

r

+ 2σ2

x

−

N

x

e−

n ! ≤

and

P

1

−

n

N

Equations (15) and (16) imply that

N

1

−

Xi=n

σ2

e2
i −

≤ −

2σ2

x

−

N

r

e−

x.

n ! ≤

(15)

(16)

2σ2

P

 −

N

r

x

−

n ≤

N

1

−

n

N

1

−

Xi=n

σ2

e2
i −

≤

2σ2

x

−

n

N

r

+ 2σ2

x

−

N

1

−

n ! ≥

2e−

x.

(17)

Next, we are interested in solving the following quadratic equation for positive x:

2σ2

x

−

n

N

r

+ 2σ2

σ2ǫ
3

=

n

x

−

N

x

−
N

=

⇒

N

r

x =

=

⇒

= −

n

1 +

1 + 2ǫ
3

q
2

n
−
2  

1 +

ǫ
3 − r

1 +

2ǫ
3 !

.

By plugging this value of x in (17), we obtain the result.

(cid:4)

Appendix B. On the decay rate of the bound

Here we analyze the rate of decay of the deviation bound given in Theorem 6, which is stated
explicitly in Corollary 7. In particular, we study how fast δ decays to zero if the conﬁdence bound
is held constant.

Consider (11), and denote δ′ := 2δ. Also, denote ˜Vdn as

N

1

−

n Vdn, and

ε := 2

log(2)σ

1/2

w⊤V −
dn
k

log

1

dn + In)1/2
det(VupV −
δ′

.

!

p
First, we analyze ˜Vdn. We write ˜Vdn as ˜V

ǫ¯Γ, where ˜V := [In 0]V [In 0]⊤ and ¯Γ :=
∞i=0 Ai(A⊤)i[In 0]⊤. Next, let W W ⊤ be the Cholesky factorization of ¯Γ, and let

−

σ2[In 0]

P

k2v
u
u
t

12

 
 
 
A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

U DU ⊤ be the eigenvalue decomposition of W −
λ1 ≥ · · · ≥

λn > 0. Thus,

1 ˜V W −⊤, where D = diag

λ1, . . . , λn}
{

, and

1

˜V −
dn = W −⊤U 

1
λ1

−

ǫ

. . .

0

U ⊤W −

1.



1
λn

0



1/2. This choice leads to




−

ǫ

Next, we choose ǫ such that ǫ = λn −

N −

1/2

w⊤ ˜V −
dn
k

k2 =

w⊤V1w + N 1/2w⊤V2w,

where V1 and V2 are suitable matrices independent of N . In particular,

p

V2 = W −⊤U

k

0n
−
0

(cid:20)

0
Ik(cid:21)

U ⊤W −

1,

with k being the algebraic multiplicity of λn. Also, note that

det(V −

1
dn ) =

det(¯Γ)

N k/2
k
n
i=1 (λi −

−

.

ǫ)

(18)

(19)

Regarding δ′, with the choice of ǫ = λn −
(18) and (19) lead to the following computations:

N −

Q

1/2 we ﬁnd that δ′ ∼

Ce−

λn√N for large N . Hence,

w⊤
(N

˜V −
1
dn w

n) s

−

log

det(Vup + Vdn)1/2

det(Vdn)1/2δ′ (cid:19)

(cid:18)

ε = 2

log(2)σ

p

= 2

log(2)σ

p

s

s

w⊤V1w + N 1/2w⊤V2w

(N

n)

−

∼

=

2

log(2)σN −

1/4

p

(1) as N

O

.

→ ∞

1
2

w⊤V2wv
u
u
u
u
u
t

p

det

(cid:18)

V

In 0

In
0
(cid:3)
(cid:2)
det( ˜Vdn)1/2δ′

(cid:20)

1/2

(cid:21)(cid:19)








v
u
u
u
u
u
u
t

log 





det

In 0

V

In
0
(cid:20)
(cid:21)(cid:19)
(cid:3)
n
ǫ)
i=1 (λi −
−

k

log 

(cid:18)
C 2 det(¯Γ)

(cid:2)





Q

log(N ) + λn√N



+

k
4





Hence, the rate of decay is at least exponential in √N . That is, for an asymptotically constant
λn√N . Note that a byproduct of this
conﬁdence bound, the probability of interest decays as e−
derivation are the directions of w in which a faster learning can be achieved: Since V2 has rank k,
a subspace of dimension n
k of directions satisﬁes w⊤V2w = 0, for which the dominant term in
−
1/2 instead of N −
k2 becomes N −
w⊤V −
dn
k

1/4.

1/2

13

A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

Appendix C. Auxiliary results

Proposition 8 (Lemma 4.2 of Simchowitz et al. (2018)) Let
Zt}t
{
and
Ft and
1 be real-valued processes adapted to
Wt|Ft is mean zero and σ2-sub-Gaussian. Then, for any positive real numbers α, β we have

0 be a ﬁltration, and
{Ft}t
≥
Ft+1 respectively. Moreover, assume

Wt}t
{

≥

≥

1

T

P

"(

Xt=1

ZtWt ≥

α
) ∩ (

T

Xt=1

Z 2

t ≤

β

)# ≤

exp

α2
2σ2β

.

(cid:19)

−

(cid:18)

Proposition 9 (Lemma 1 of Laurent and Massart (2000)) Let (Y1, . . . , YD) be i.i.d. Gaussian
variables, with mean 0 and variance 1. Let a1, . . . , aD be nonnegative. We set

a
k

k∞

Let

= sup

i=1,...,D |

,
ai|

a
k

2
2 =
k

a2
i .

D

Xi=1

D

Z =

Xi=1

ai(Y 2

i −

1).

Then, the following inequalities hold for any positive x:

P(Z

k2√x + 2
a
2
a
x)
k
k
k∞
P(Z
k2√x)
a
2
k
≤ −
Corollary 10 (Corollary of Lemma 1 of Laurent and Massart (2000)) Let U be a χ2 statistic
with D degrees of freedom. For any positive x,

exp(

exp(

x).

x)

≤

−

−

≤

≥

P(U

D

−
P(U

2√Dx + 2x)
2√Dx)

≥
D

−

≤ −

exp(

−

exp(

−

x)

x).

≤

≤

Proposition 11 (Weierstrass Product Inequality (Mitrinovic and Vasic, 1970, p. 210)) Given
real numbers 0

1, the following inequality holds:

λ1, λ2, . . . , λn ≤

≤

(1

−

λ1)(1

λ2)

(1

λn)

1

−

≥

−

· · ·

−

λk.

n

Xk=1

(20)

Proof. We proceed by induction. For n = 1, the inequality is obvious. Next, assume that (20) holds
for some n. Then,

(1

−

λ1)(1

λ2)

(1

−

· · ·

−

λn)(1

−

λn+1)

1

≥  

= 1

−

1

−

≥

(1

−

λn+1)

!

n

λn+1 + λn+1

λk

Xk=1

−

n

λk

Xk=1
n
λk −

Xk=1
n+1

Xk=1

λk,

which shows that (20) is valid for n + 1 as well. By induction, the statement follows.

14

A FINITE-SAMPLE DEVIATION BOUND FOR STABLE AUTOREGRESSIVE PROCESSES

Proposition 12 (Subadditivity of the spectral radius of Hermitian matrices (Bernstein, 2009,
Fact 5.12.2)) Let A, B

n be Hermitian matrices. Then,

Rn

×

∈

Proof. For this we will ﬁrst prove the following chain of inequalities:

ρ(A + B)

≤

ρ(A) + ρ(B).

(21)

λmin(A) + λmin(B)

λmin(A + B)

≤

≤

λmax(A + B)

≤

λmax(A) + λmax(B).

(22)

By deﬁnition,

λmin(A)+λmin(B) = min
\{

Rn

∈

x

0

}

x⊤Ax
x⊤x

+ min
\{
∈

Rn

y

0

}

y⊤By
y⊤y ≤

x

min
Rn
\{
∈

0

}

x⊤(A + B)x
x⊤x

= λmin(A+B).

Similarly,

λmax(A+B) = max
\{

Rn

∈

x

0

}

x⊤(A + B)x
x⊤x

≤

x

max
Rn
\{
∈

0

}

x⊤Ax
x⊤x

+ max
Rn
\{
∈

y

0

}

y⊤By
y⊤y

= λmax(A)+λmax(B).

With (22) in mind, and using the fact that for any Hermitian matrix H we can write ρ(H) as
(Bernstein, 2009, Fact 5.11.5), we see that
max

{−

λmin(H), λmax(H)
}
λmin(A), λmax(A)
}

max

{−

≤

λmax(A + B)

+ max

λmin(B), λmax(B)
}

{−

= ρ(A) + ρ(B), (23)

and

λmin(A+B)

max

λmin(A), λmax(A)
}

{−

≤

+max

λmin(B), λmax(B)
}

{−

−

= ρ(A)+ρ(B), (24)

Combining (23) and (24), we reach

ρ(A + B) = max

{−

λmin(A + B), λmax(A + B)

} ≤

ρ(A) + ρ(B),

which is what we wanted to prove.

15

