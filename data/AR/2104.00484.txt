1
2
0
2

r
p
A
1

]

V
C
.
s
c
[

1
v
4
8
4
0
0
.
4
0
1
2
:
v
i
X
r
a

Neural Video Portrait Relighting in Real-time via Consistency Modeling

Longwen Zhang1,2 * Qixuan Zhang1,2 * Minye Wu1,3

Jingyi Yu1

Lan Xu1

1ShanghaiTech University

2Deemos Technology

3University of Chinese Academy of Sciences
{zhanglw2,zhangqx1,wumy,yujingyi,xulan1}@shanghaitech.edu.cn
{zhanglw,zhangqx}@deemos.com

Abstract

Video portraits relighting is critical in user-facing hu-
man photography, especially for immersive VR/AR experi-
ence. Recent advances still fail to recover consistent relit
result under dynamic illuminations from monocular RGB
stream, suffering from the lack of video consistency super-
vision. In this paper, we propose a neural approach for real-
time, high-quality and coherent video portrait relighting,
which jointly models the semantic, temporal and lighting
consistency using a new dynamic OLAT dataset. We pro-
pose a hybrid structure and lighting disentanglement in an
encoder-decoder architecture, which combines a multi-task
and adversarial training strategy for semantic-aware con-
sistency modeling. We adopt a temporal modeling scheme
via ﬂow-based supervision to encode the conjugated tempo-
ral consistency in a cross manner. We also propose a light-
ing sampling strategy to model the illumination consistency
and mutation for natural portrait light manipulation in real-
world. Extensive experiments demonstrate the effectiveness
of our approach for consistent video portrait light-editing
and relighting, even using mobile computing.

1. Introduction

The past ten years have witnessed a rapid development
of digital portrait photography with the rise of mobile cam-
eras. Relighting evolves as a cutting-edge technique in such
portrait photography for immersive visual effects of VR/AR
experience. How to further enable consistent relit video
results under challenging dynamic illumination conditions
conveniently remains unsolved and has received substantive
attention in both industry and academia.

For video portrait relighting, early solutions [14, 56] rely
on a sophisticated studio setup which is expensive and difﬁ-
cult to be deployed. Modern approaches [41, 44, 42] further

*Equal contribution

Figure 1. Our approach achieves high-quality and consistent video
portrait relighting under dynamic illuminations in real-time, using
only mobile computing and monocular RGB video input.

apply color or style transfer techniques to ease the hard-
ware requirements. However, they still require two to four
orders of magnitude more time than is available for inter-
active video application. The recent learning techniques
bring huge potential for human portrait modeling and re-
lighting [67, 65, 46, 31, 55] from only monocular RGB in-
put.
In particular, the methods [43, 39] perform explicit
neural inverse rendering but are limited to the low-quality
face and Spherical Harmonics (SH) lighting models. Re-
cent methods [65, 46] remove the explicit inverse rendering
by learning an efﬁcient end-to-end mapping between the in-
put headshots and relit ones, while the method [55] further
models reﬂectance attributes explicitly to handle lighting ef-
fects like specular or shadow. However, they still focus on
single image input without modeling the temporal consis-
tency for video portrait relighting, leading to severe jittery
artifacts, especially under the challenging dynamic illumi-
nations. Some recent mobile devices [2, 18] enables the
“Portrait Lighting” mode for video editing of lighting con-
ditions. Critically, they only modify existing illumination
rather than relight the captured video into various scenes.

In this paper, we attack the above challenges and present
a novel real-time and temporally coherent portrait relighting
approach from only a monocular RGB video input, as illus-

1

 
 
 
 
 
 
trated in Fig. 1. Our approach jointly models the semantic,
temporal and lighting consistency to enable realistic video
portrait light-editing and relighting into new scenes with
dynamic illuminations, whilst maintaining real-time perfor-
mance even on portable device.

Generating such realistic and consistent video relit re-
sult in a real-time and data-driven manner is non-trivial.
From the data aspect, existing face datasets [10, 59, 55] lack
the video ground-truth supervision for consistency model-
ing. Thus, we build up a high-quality dataset for video
portrait relighting, consisting of 603,288 temporal OLAT
(one light at a time) images at 25 frames per second (fps)
of 36 actors and 2,810 environment lighting maps. Our dy-
namic OLAT dataset is captured via a light stage setup with
114 LED light sources and a stationary 4K ultra-high-speed
camera at 1000 fps. From the algorithm side, we further
propose a novel neural scheme for consistent video por-
trait relighting under dynamic illuminations. To maintain
the real-time performance, we adopt the encoder-decoder
architecture to each input portrait image similar to previous
methods [46, 55]. Differently, we introduce a hybrid and
explicit disentanglement for semantic-aware consistency,
which self-supervises the portrait structure information and
fully supervises the lighting information simultaneously in
the bottleneck of the network. Such disentanglement is fur-
ther enhanced via multi-task training as well as an adver-
sarial strategy so as to encode the semantic supervision and
enable more realistic relighting. Then, to utilize the rich
temporal consistency in our dynamic OLAT dataset, a novel
temporal modeling scheme is adopted between two adjacent
input frames. Our temporal scheme encodes the conjugated
temporal consistency in a cross manner via ﬂow-based su-
pervision so as to model the dynamic relit effect. Finally,
a lighting sampling based on Beta distribution is adopted,
which augments the discrete environment lighting maps and
generates a triplet of lighting conditions for adjacent input
frames and the target output. Our sampling scheme models
the illumination consistency and mutation simultaneously
for natural video portrait light-editing and relighting in the
real-world. To summarize, our main contributions include:

• We present a real-time neural video portrait relighting
approach, which faithfully models the video consis-
tency for dynamic illuminations, achieving signiﬁcant
superiority to the existing state-of-the-art.

• We propose an explicit structure and lighting disen-
tanglement, a temporal modeling as well as a light-
ing sampling schemes to enable realistic video portrait
light-editing and relighting on-the-ﬂy.

• We make available our dataset of 36 performers with
603,288 temporal OLAT images to stimulate further
research of human portrait and lighting analysis.

2. Related Work

Portrait Relighting. Debevec et al. [14] invent Light Stage
to capture the reﬂectance ﬁeld of human faces, which has
enabled high-quality 3D face reconstruction and illumina-
tions rendering, advancing the ﬁlm’s special effects indus-
try. Some subsequent work has also achieved excellent
results by introducing deep learning[62, 29, 21, 30, 47].
Obviously, this is not a product for individual consumers;
thus various methods for single portrait relighting have
been proposed. Several methods [57, 6, 26, 58, 50] per-
form relighting on static objects.
Some work follows
the pipeline of color transfer to achieve the relighting ef-
fects [11, 41, 44, 42], which usually needs another portrait
image as the facial color distribution reference. Blanz et
al. [7] use a morphable model of faces that allows re-
lighting by changing the directional lighting model param-
eters. With the advent of deep neural networks, recent
approaches obtain various neural rendering tasks success-
fully [49]. Some methods [65, 28, 39] adapt Spherical
Harmonics (SH) lighting model to manipulate the illumi-
nation. Several work [1, 15, 54] jointly estimate the 3D
face and SH [5, 34] parameters and achieved relighting by
recovering the facial geometry and modify the parameters
of the SH lighting model. Sevastopolsky et al. [40] use
point cloud to generate relightable 3D head portraits, while
Tewari et al. [48] use GAN to manipulate the illumina-
tion. Explicitly modeling the shadow and specular [55, 31]
achieve excellent results in directional light source relight-
ing. Mallikarjunr et al. [4] take a single image portrait as
input to predict OLAT(one-light-at-a-time) as Reﬂectance
Fields, which can be relit to other lighting via image-based
rendering. Sun et al. [46] choose environment map as light-
ing model and use light stage captured OLAT data to gener-
ate realistic training data and train relighting networks in an
end-to-end fashion. We also use the OLAT images to gen-
erate training data for portrait relighting. Differently, our
approach enables real-time and consistent video portrait re-
lighting under dynamic illuminations, with the aid of a new
dynamic OLAT dataset.

Temporal Consistency. Previous image relighting methods
can be extended to videos if we directly treat every video
frames as independent images. However, those methods
will inevitably generate ﬂicker results on relit videos. To
suppress ﬂicker results, several approaches have been de-
veloped for video style transfer tasks [27, 23, 8, 60, 51, 9,
36, 53]. Speciﬁcally, Ruder et al. [36] employed a temporal
loss guided by optical ﬂow for video style transfer, but the
real-time computation of optical ﬂows makes this approach
slower. Vid2Vid [53] synthesised videos with temporal con-
sistency by training a network to estimate optical ﬂow and
apply it on previously generated frames. In this paper, we
show that temporal consistency and portrait relighting can

2

Figure 2. Illustration of the capturing system for building our dy-
namic OLAT dataset.

be simultaneously learned by a feed-forward CNN, which
avoids computing optical ﬂows in the inference stage.
Video Relighting. Some methods [52, 20, 37] use IR and
LEDs around the screen and web camera to provide ac-
ceptable lighting for video conferencing. Li et al. [25]
create free-viewpoint relighting video using multi-view re-
construction under general illumination, while Richardt et
al. [35] add video effects like relighting using RGBZ video
cameras. “The Relightables” [21] proposes a hybrid geo-
metric and machine learning reconstruction pipeline to gen-
erate high-quality relit video. In contrast, our method does
not require extra speciﬁc capturing equipment and enables
real-time video portrait relighting using mobile computing.
Face Dataset. Traditional face dataset usually takes 2D
images in various lighting conditions [19, 24, 16]. The
controlled lighting conditions are easy to build but lack re-
ﬂectance information for photo-realistic portrait relighting.
With the development of face scanning and reconstruction
techniques, 3D face datasets have been extended from only
geometric [61, 63, 38, 10, 12, 13, 64, 59] to include re-
ﬂectance channels [45, 55]. However, the existing rendering
scheme is difﬁcult to avoid the Valley of Terror effect with-
out manual modiﬁcation. 3D dataset still cannot achieve the
realism in 2D face dataset or those using image-based ren-
dering Thus, various face OLAT datasets [4, 46, 31] have
been proposed with light stage setup. In contrast, we con-
struct a new dynamic OLAT dataset through a a light stage
setup and a 4K ultra-high-speed camera. Our high-quality
dataset consists of 603,288 temporal OLAT imagesets at 25
fps of 36 subjects (18 females and 18 males) with various
expressions and hairstyles.

3. Dynamic OLAT Dataset Overview

Our goal is to naturally manipulate the lighting of a por-
trait RGB video captured in the wild into new environment
lighting conditions while preserving the consistent struc-
ture and content. To provide ground truth supervision for
video consistency modeling, we build up a high-quality dy-
namic OLAT dataset. As illustrated in Fig. 2, our capture
system consists of a light stage setup with 114 LED light
sources and Phantom Flex4K-GS camera (global shutter,
stationary 4K ultra-high-speed camera at 1000 fps), result-
ing in dynamic OLAT imageset recording at 25 fps using

Figure 3. Illustration of our video portrait relighting network based
on encoder-decoder architecture for real-time inference.

the overlapping method [56]. Our dataset includes 603,288
temporal OLAT imagesets of 36 actors (18 females and 18
males) with 2810 HDR environment lighting maps [22, 17],
to provide high-quality and diverse video portrait relighting
samples. Besides, we apply a pre-deﬁned light condition
to each OLAT imageset to obtained a fully-illuminated por-
trait image. Then, both the portrait parsing [66] and the
optical ﬂow [32] algorithms are applied to such a fully-
illuminated stream to obtained ground truth semantics and
correspondence supervisions. Our dynamic OLAT dataset
provides sufﬁcient semantic, temporal and lighting consis-
tency supervision to train our neural video portrait relight-
ing scheme, which can generalize to in-the-wild scenarios.

4. Neural Video Portrait Relighting

Our scheme obtains real-time and coherent video relit
results from only mobile computing and monocular RGB
steam, using our dynamic OLAT dataset, as shown in Fig. 4.
First, a hybrid disentanglement for portrait structured and
lighting is introduced for semantic-aware consistency mod-
eling (Sec. 4.1). Then, a temporal modeling is adopted to
utilize the ﬂow-based supervision for dynamic relit effects
(Sec. 4.2). We also introduce a lighting sampling strategy
to model the illumination consistency and mutation for nat-
ural portrait lighting manipulation in real-world scenarios
(Sec. 4.3).
Notations. To achieve the real-time performance, we adopt
the encoder-decoder architecture to the portrait stream se-
quentially similar to previous methods [46, 55], as illus-
trated in Fig. 3. Given an input image Ii
t at t-th frame and a
desired target lighting Lk (environment lighting map), our
network Φ predicts both a target portrait image ˜Ik
t lit by the
Lk and a corresponding semantic mask ˆPt:

t , ˆLi, ˆPt = Φ(Ii
˜Ik

t, Lk),

(1)

where ˆLi is the regressed lighting of the input image Ii
t.
Speciﬁcally, the encoder Φenc encodes the input image into
both the lighting Li and a portrait structure latent code ˆet:

ˆLi, Fi

t, ˆet = Φenc(Ii

t),

(2)

where Fi
corresponding decoder Φdec is formulated as:

t is the output of skip connections. Similarly, the

t, ˆPt = Φdec(ˆLi, Fi
ˆIi

t, ˆet),

(3)

3

Figure 4. The training pipeline of our approach. It consists of a structure and lighting disentanglement (Sec. 4.1), a temporal consistency
modeling (Sec. 4.2) and a lighting sampling (Sec. 4.3), so as to generate consistent video relit results from a RGB stream in real-time.

where we use predicted illumination ˆLi to relit itself. By
replacing the lighting with a known one Lk in the decoder,
we can obtain a relit portrait image ˜Ik
t corresponding to Lk.
Note that we use tilde and hat notations for the images relit
by known illumination or the predicted one, respectively.

4.1. Structure and Lighting Disentanglement

The core of realistic relighting is the reliable disentangle-
ment of the portrait structure information e and the lighting
condition L in the bottleneck of our network in Eqn. 1. To
this end, we adopt a hybrid disentanglement scheme comb-
ing with multi-task training and adversarial training strategy
to model the semantic consistency for realistic relighting.

Similar to the method [46], during training we optimize
the following basic loss for disentanglement, which mini-
mizes the photometric error and illumination distance be-
tween the prediction and the ground truth from our dataset:

Lbasic =

(cid:107) log(1 + Li) − log(1 + ˆLi)(cid:107)2
2

t − ˜Ik

t )(cid:107)1 + (cid:107)Mt (cid:12) (Ii

t − ˆIi

t)(cid:107)1,

(4)

1
2
+(cid:107)Mt (cid:12) (Ik

where Mt is a portrait foreground mask of the t correspond-
ing frame from face parsing and (cid:12) is element-wise multipli-
cation. However, only using this basic scheme fails to pro-
vide supervision on the structure latent code e and encode
the rich semantic consistency in our dynamic OLAT dataset.
Thus, we introduce the following strategies for more reli-
able disentanglement and more consistent relighting.
Structure Self-supervision. Recall that the method [46]
treats the latent space as lighting map and only relies on the
feature maps Fi
t for modeling the portrait structure informa-
tion. Differently, we utilize a separated structure-wise latent
code ˆet, which has larger receptive ﬁeld in the encoder to
represent the global context information of the portrait im-
age. Thus, we design a novel self-supervised scheme for

4

such structure-wise latent code by applying the encoder re-
currently to the relit output and enforces the consistency be-
tween the structure codes for further disentanglement. We
formulate it as follows:

Llatent = (cid:107)ˆet − ˜et(cid:107)2
2,

(5)

where ˜et is from the recurrent output of Φenc(˜Ik
t ). Here
we utilize encoder Φenc to encode the relit image ˜Ik
t with
the target light Lk and verify its global structure latent code
with the original one to enhance structure consistency.
Semantics-aware Multi-Task Learning. Human face skin
with scattering effects owns different reﬂectance distribu-
tion from other materials like hair. Treating all the portrait
pixels uniformly will cause strong artifacts in relighted re-
sults without maintaining the semantic consistency. To this
end, we design the multi-task decoder Φdec which aims to
restore both the relighted image ˆIo
t and the semantic por-
trait mask Pt under the supervision from our dataset. Such
parsing loss with binary cross-entropy metrics is formulated
as:

Lparsing = −

(cid:16)

(cid:17)
Pt (cid:12) log ˆPt + (1 − Pt) (cid:12) log(1 − ˆPt)

.

(6)

By predicting semantic portrait mask Pt, we enforce both
encoder and decoder networks to be aware of the semantic
information of human portrait image. Hence the networks
can implicitly models the semantic consistency for more re-
alistic relighting of various portrait regions.
Adversarial Training. To further reinforcing portrait im-
age details, we also introduce a discriminator network ΦD,
which has the same architecture as DCGAN’s [33]. We
adopt Wasserstein GAN [3] strategies to the proposed dis-
criminator for a stable training process. Speciﬁcally, we
remove the activation function of ΦD’s ﬁnal layer and ap-
ply the weight clipping method during training. Then, the

adversarial losses is formulated as:

LadvD = −ΦD(Ii
LadvG = −ΦD(Ii

t, Ik
t,˜Ik

t , Lk) + ΦD(Ii
t , Lk),

t,˜Ik

t , Lk)

(7)

where LadvD is only for updating the discriminator, and
LadvG is only for updating the decoder; Ladv = LadvD +
LadvG . Here, the discriminator takes a triplet as input, in-
cluding a source image, a relit image, and the correspond-
ing light condition Lk, which estimates the Wasserstein dis-
tance between the real image distribution and the relit image
distribution. Note that the structure of the source image and
Lk are essential cues for such distance measurement.

4.2. Temporal Consistency Enhancement

Previous single-image relighting approaches lack the ex-
plicit temporal modeling in terms of portrait motions or dy-
namic illuminations, leading to ﬂickering artifacts for video
applications. Thus, we propose a rescue by utilizing the rich
temporal consistency supervision in our dynamic OLAT
dataset. Speciﬁcally, for two adjacent training samples at
timestamps t and t + 1, we obtain the forward ﬂow ft,t+1(·)
and the backward ﬂow ft+1,t(·) from our continues OLAT
imagesets, where fa,b(·) warps images from time a to time
b. Note that our high-quality OLAT imagesets at 25 fps en-
sures the accuracy of such supervision based on optical ﬂow
for self-supervised veriﬁcation in a cross manner.

For balancing the lighting distribution between two
frames, we also introduce two conjugated illuminations in
our training scheme. Given two adjacent frames Ii
t and
Ij
t+1, we relit both of them using our network according
to the predicted lighting conditions ˆLi, ˆLj, as well as the
target illumination Lk to obtain corresponding relit images
at both frames. Thus, our temporal loss is formulated as:

Ltemporal = (cid:107)ft,t+1(˜Ik

t ) − ˜Ik

t+1(cid:107)1 + (cid:107)ft+1,t(˜Ik

(cid:0)(cid:107)ft,t+1(ˆIz

t ) − ˆIz

t+1(cid:107)1 + (cid:107)ft+1,t(ˆIz

t+1) − ˜Ik
t (cid:107)1

t+1) − ˆIz

t (cid:107)1
(cid:1),

(cid:88)

+

z∈{i,j}

(8)

which encodes the conjugated temporal consistency in a
cross manner via ﬂow-based supervision so as to model the
dynamic relit effect.

4.3. Lighting Conditions Sampling

Note that the discrete environment lighting maps in our
dynamic OLAT dataset still cannot models the illumina-
tion consistency and mutation for real-time video relight-
ing scenarios. Thus, we introduce a novel lighting sam-
pling scheme during training to generate a triplet of light-
ing conditions for adjacent input frames and the target out-
put, which enhances the illumination consistency for natural
portrait relighting and lighting manipulation.

5

Similar to previous work [46, 55], the lighting condition
L is represented as a ﬂattened latitude-longitude format of
size 16 × 16 with three color channels from the environ-
ment lighting map and we adopt the same environment map
re-rendering with random rotation to augment the lighting
conditions in our dataset, which forms a uniform lighting
condition sampling distribution G. To model illumination
mutation, we further design a lighting condition distribu-
tion H, where we randomly sample one to three point light
sources with uniformly random colors outside a unit sphere.
The maximum distance from the light source to the sphere is
limited by 1.5 in order to produce reasonable illumination.
Then, we project these light sources on a sphere according
to the Lambertian reﬂectance model [5] to form an environ-
ment map and corresponding lighting condition.

For each training sample, we generate three illumina-
tions using different sampling strategies, including Li and
Lj to generate the adjacent images Ii
t+1, as well as
the Lk for the target image Ik, which is formulated as:

t and Ij

Li = Xi
Lj = β1Li + (1 − β1)Xj
Lk = β2Lj + (1 − β2)Xk + Y,

(9)

where Xi, Xj, Xk ∼ G, Y ∼ H and β1, β2 are sampled
from a Beta distribution Beta(0.5, 0.5). Here, the Beta dis-
tribution drastically diversiﬁes the lighting combination for
modeling the illumination consistency and mutation simul-
taneously and enhancing the generalization ability of our
network. Conceptually, Li and Lj have similarity, con-
tributing to the coverage of temporal loss training. And Lk
provides challenging illumination examples to enhance the
lighting manipulation ability of our approach.

4.4. Implementation Details

We utilize our dynamic OLAT dataset to train our video
relighting network. Our training dataset consists of 536,256
temporal OLAT imagesets of 32 actors lasting for 188.16
seconds. The remaining OLAT imagesets of the other four
actors unseen during training are taken as the test dataset.
Note that we also augment the dataset using random crop-
ping and resizing to add more challenging incomplete por-
traits to enhance the generalization ability of our network.
During training our total loss is formulated as follows:

L = λ1Lbasic+λ2Llatent + λ3Lparsing

+ λ4Ltemporal + λ5Ladv,

(10)

where the weights for each term are set to be 1.0 in our ex-
periments. Since we utilize Wasserstein GAN in our ap-
proach, our network’s parameters are optimized by RM-
Sprop algorithm with a learning rate of 5e−5. Besides,
we also clamp parameters of discriminator into a range of
[−0.01, 0.01] and adopt the progressive training strategy.

Figure 5. Our relighting results under dynamic illuminations. Each triplet includes the input frame and two relit result examples.

Input

Ground Truth

Ours

SIPR

EMRCM

MTP

DPR

Figure 6. Qualitative comparisons of relit results on our dynamic OLAT dataset. Our approach achieves more realistic relighting.

5. Experiments

Here we evaluate our method in various challenging sce-
narios. We run our experiments on a PC with Intel i7-8700K
CPU, 32GB RAM, and Nvidia RTX 3080 GPU, where our
approach generates high-quality 512×512 relit results at
111 fps (15 fps on the iPhone 12 device). Fig. 5 demon-
strates several results of our approach, which can generate
consistent video relit results of both in-the-wild sequences
and the one from our dataset with challenging illuminations.

5.1. Comparison

We compare our approach against existing state-of-the-
art methods, including Single Image Portrait Relighting
(SIPR) [46],
the one via Explicit Multiple Reﬂectance
Channel Modeling (EMRCM) [55], the one based on Mass

Transport Approach (MTP) [42] and Deep Portrait Relight-
ing (DPR) [65]. Note that we re-implement SIPR [46] and
train it using our dataset for a fair comparison. Fig. 6 and
Fig. 7 provide the qualitative comparison on both our dy-
namic OLAT dataset and online or in-the-wild sequences,
respectively. Note that our approach achieves signiﬁcantly
more realistic relit results under challenging illuminations
by modeling the video consistency.

Then, we utilize our testing set with ground truth for
quantitative comparison. Similar to previous methods [46,
55], we adopt the RMSE, PSNR, and SSIM as metrics.
Note that the output values are normalized to [0, 1], and
only the valid portrait regions are considered. As shown in
Tab. 1, our approach consistently outperforms the baselines
in terms of these metrics above, illustrating the effective-

6

Input

Ours

SIPR

EMRCM

MTP

DPR

Figure 7. Qualitative comparisons of relit results on online or in-the-wild sequences. We obtain more natural results on unseen performers.

Figure 8. Quantitative comparison in terms of handling fast chang-
ing illuminations. Our approach consistently outperforms other
baselines under various lighting speed up factors.

Method

RMSE

PSNR

SSIM

SIPR

0.0974
EMRCM 0.0766
0.0902
0.1080
0.0349

MTP
DPR
Ours

20.6542
22.7197
21.9535
20.8042
30.6110

0.8901
0.8748
0.8775
0.8593
0.9584

Table 1. Quantitative comparison on our dynamic OLAT dataset.

ness of our approach for consistent video portrait relighting.
We further compare against baselines under dynamic illu-
minations. Thus, we synthesize 1000 frames with the static
performer and changing lighting conditions using various
speed-up factors from 1 to 10. Then, we relit the sequence
into static lighting and calculated the average RMSE of ad-
jacent output frames as the error metric for the jittery arti-
facts. As shown in Fig. 8, the error of our approach glows
much slower consistently compared to others, which illus-
trates our superiority to handle dynamic illuminations.

5.2. Evaluation

Hybrid disentanglement. Here we evaluate our hybrid
scheme for structure and lighting disentanglement. Let
w/o structure denote the variation of our approach with-
out the self-supervision of portrait structure in Eqn. 5, and
w/o enhance denote the variation without the disentangle

Input

w/o structure
Figure 9. Qualitative evaluation of structure self-supervison. Our
full pipeline achieve sharper relighting with ﬁne structured details.

Ours

Input

w/o enhance
Figure 10. Qualitative evaluation of disentangle enhancement. Our
scheme models the semantic consistency for realistic relighting.

Ours

enhancement of multi-task and adversarial strategy. As
shown in Fig. 9, our scheme with structure self-supervision
enables more accurate disentanglement for sharper realistic
results. The qualitative evaluation in Fig. 10 further illus-
trates that our multi-task and adversarial training strategy
encodes the semantic consistency for more realistic relight-
ing.
Temporal modeling. Here we compare against our vari-
ation without temporal consistency modeling, denoted as
w/o temporal. Similar to the comparison under dynamic il-
luminations, we relit the same synthesized sequence with

7

Input

w/o temporal

Ours

g
n
i
l
p
m
a
s

o
/
w

h
t
i

w

Figure 11. Evaluation of temporal modeling. Top: two relit exam-
ples of a static portrait from changing illuminations into a target
lighting condition. Down: the corresponding error curve.

Method

RMSE

PSNR

SSIM

w/o content
w/o temporal
w/o parsing
w/o sampling
Ours

0.0549
0.0404
0.0680
0.0616
0.0349

25.5496
28.6403
23.6189
24.5142
30.6110

0.9021
0.9510
0.9170
0.9223
0.9584

Table 2. Quantitative evaluation on synthesis sequences.

static performer and changing illuminations into a target
lighting condition for thorough evaluation. As shown in
Fig. 11, our approach with temporal modeling achieves
more temporal consistent results both qualitatively and
quantitatively. We also provide quantitative evaluation un-
der various lighting speed-up factors in Fig. 8. These results
illustrate the effectiveness of our scheme to utilize temporal
consistency.
Lighting Sampling We further evaluate our light sampling
strategy. Let w/o sampling denote our variation only using
the discrete environment lighting maps during training. As
shown in Fig. 12 and Fig. 13, our scheme models the illu-
mination consistency and mutation, enabling a more natural
portrait light-editing and relighting.

We further perform thorough quantitative analysis of the
individual components of our approach using our testing set
with ground truth. As shown in Tab. 2, our full pipeline con-
sistently outperforms other variations. This not only high-
lights the contribution of each algorithmic component but
also illustrates that our approach enables consistent video
portrait relighting.

6. Discussion and Conclusion

Limitations. As a trial to explore real-time and consistent
video portrait relighting under dynamic illuminations, our
approach still owns limitations as follows. First, our ap-
proach cannot handle the extreme illumination changes not
seen in training, like suddenly turn on/off all the lights. Be-
sides, the generated relit video results lose the facial details

Input

Recon

Modiﬁed

Modiﬁed

Figure 12. Evaluation of lighting sampling on real-world scenes.
Our scheme enables more natural lighting manipulation, where a
red light source is added into the environment lighting map.

Input

GT editing w/o samplingwith sampling

Figure 13. Evaluation of lighting sampling on synthetic scenes
where a red light is added from different directions. Top: the qual-
itative relit examples. Down: the corresponding error curve.

partially due to the light-weight encoder-decoder architec-
ture. We plan to utilize a parametric face model to recover
the facial details explicitly. Our current approach is also
limited to headshots only. It’s promising to include clothes
and garment material analysis for full-body portrait relight-
ing. It’s also interesting to enhance the OLAT dataset with
a generative model to handle high-frequency lighting.
Conclusion. We have presented a novel scheme for real-
time, high-quality and consist video portrait relighting un-
der dynamic illuminations from monocular RGB stream
and a new dynamic OLAT dataset. Our hybrid disentangles
scheme with a multi-task and adversarial training strategy
models the semantic consistency efﬁciently and generates
realistic relit results. Our temporal modeling scheme en-
codes the ﬂow-based supervision for temporally consistent
relighting, while our light sampling strategy enhances the
illumination consistency for natural portrait lighting manip-
ulation. Extensive results demonstrate the effectiveness of
our approach for consistent video portrait relighting in dy-
namic real-world scenarios. We believe that our approach
is a critical step for portrait lighting analysis, with many
potential applications in user-facing photography, VR/AR
visual effects or immersive telepresence.

8

References

[1] Oswald Aldrian and William AP Smith. Inverse rendering
of faces with a 3d morphable model. IEEE transactions on
pattern analysis and machine intelligence, 35(5):1080–1093,
2012.

[2] Apple. Use portrait mode on your iphone. https://

support.apple.com/en-us/HT208118.

[3] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
In Interna-
Wasserstein generative adversarial networks.
tional conference on machine learning, pages 214–223.
PMLR, 2017.

[4] Mallikarjun B R, Ayush Tewari, Tae-Hyun Oh, Tim Weyrich,
Bernd Bickel, Hans-Peter Seidel, Hanspeter Pﬁster, Woj-
ciech Matusik, Mohamed Elgharib, and Christian Theobalt.
Monocular reconstruction of neural face reﬂectance ﬁelds.
[5] Ronen Basri and David W Jacobs. Lambertian reﬂectance
and linear subspaces. IEEE transactions on pattern analysis
and machine intelligence, 25(2):218–233, 2003.

[6] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman,
and Ravi Ramamoorthi. Deep 3d capture: Geometry and re-
ﬂectance from sparse multi-view images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2020.

[7] Volker Blanz and Thomas Vetter. A morphable model for
In Proceedings of the 26th an-
the synthesis of 3d faces.
nual conference on Computer graphics and interactive tech-
niques, pages 187–194, 1999.

[8] Nicolas Bonneel, Kalyan Sunkavalli, Sylvain Paris, and
Hanspeter Pﬁster. Example-based video color grading. ACM
Transactions on Graphics, 32, 2013.

[9] Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, De-
qing Sun, Sylvain Paris, and Hanspeter Pﬁster. Blind video
temporal consistency. volume 34, 2015.

[10] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun
Zhou. Facewarehouse: A 3d facial expression database for
visual computing. IEEE Transactions on Visualization and
Computer Graphics, 20(3):413–425, 2013.

[11] Xiaowu Chen, Mengmeng Chen, Xin Jin, and Qinping Zhao.
Face illumination transfer through edge-preserving ﬁlters. In
CVPR 2011, pages 281–287. IEEE, 2011.

[12] Shiyang Cheng, Irene Kotsia, Maja Pantic, and Stefanos
Zafeiriou. 4dfab: A large scale 4d database for facial expres-
sion analysis and biometric applications. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, pages 5117–5126, 2018.

[13] Darren Cosker, Eva Krumhuber, and Adrian Hilton. A facs
valid 3d dynamic action unit database with applications to 3d
dynamic morphable facial modeling. In 2011 international
conference on computer vision, pages 2296–2303. IEEE,
2011.

[14] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter
Duiker, Westley Sarokin, and Mark Sagar. Acquiring the
In Proceedings of the
reﬂectance ﬁeld of a human face.
27th annual conference on Computer graphics and interac-
tive techniques, pages 145–156, 2000.

[15] Bernhard Egger, Sandro Sch¨onborn, Andreas Schnei-
der, Adam Kortylewski, Andreas Morel-Forster, Clemens

Blumer, and Thomas Vetter. Occlusion-aware 3d morphable
models and an illumination prior for face image analysis.
International Journal of Computer Vision, 126(12):1269–
1287, 2018.

[16] Wen Gao, Bo Cao, Shiguang Shan, Xilin Chen, Delong
Zhou, Xiaohua Zhang, and Debin Zhao. The cas-peal large-
scale chinese face database and baseline evaluations. IEEE
Transactions on Systems, Man, and Cybernetics-Part A: Sys-
tems and Humans, 38(1):149–161, 2007.

[17] Marc-Andr´e Gardner, Kalyan Sunkavalli, Ersin Yumer, Xi-
aohui Shen, Emiliano Gambaretto, Christian Gagn´e, and
Jean-Franc¸ois Lalonde. Learning to predict indoor illumi-
nation from a single image. ACM Transactions on Graphics
(TOG), 36(6):1–14, 2017.
Portrait
[18] Google.
lighting
with machine
//ai.googleblog.com/2020/12/
portrait-light-enhancing-portrait.html.

portrait
https:

Enhancing

learning.

light:

[19] Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade,
and Simon Baker. Multi-pie. Image and vision computing,
28(5):807–813, 2010.

[20] P. Gunawardane, T. Malzbender, R. Samadani, A.
McReynolds, D. Gelb, and J. Davis.
Invisible light: Us-
ing infrared for video conference relighting. In 2010 IEEE
International Conference on Image Processing, pages 4005–
4008, 2010.

[21] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,
Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-
Escolano, Rohit Pandey, Jason Dourgarian, Danhang Tang,
Anastasia Tkach, Adarsh Kowdle, Emily Cooper, Ming-
song Dou, Sean Fanello, Graham Fyffe, Christoph Rhemann,
Jonathan Taylor, Paul Debevec, and Shahram Izadi. The re-
lightables: Volumetric performance capture of humans with
realistic relighting. ACM Trans. Graph., 38(6), Nov. 2019.

[22] Yannick Hold-Geoffroy, Akshaya Athawale, and Jean-
Franc¸ois Lalonde. Deep sky modeling for single image out-
door lighting estimation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 6927–6935, 2019.

[23] Manuel Lang, Oliver Wang, Tunc Aydin, Aljoscha Smolic,
and Markus Gross. Practical temporal consistency for image-
based graphics applications. ACM Transactions on Graphics,
31, 2012.

[24] Kuang-Chih Lee, Jeffrey Ho, and David J Kriegman. Ac-
quiring linear subspaces for face recognition under variable
lighting. IEEE Transactions on pattern analysis and machine
intelligence, 27(5):684–698, 2005.

[25] Guannan Li, Yebin Liu, and Qionghai Dai. Free-viewpoint
video relighting from multi-view sequence under general il-
lumination. Machine vision and applications, 25(7):1737–
1746, 2014.

[26] Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan
Sunkavalli, and Manmohan Chandraker. Learning to recon-
struct shape and spatially-varying reﬂectance from a single
image. ACM Transactions on Graphics (TOG), 37(6):1–11,
2018.

[27] Peter Litwinowicz. Processing images and video for an im-

pressionist effect. 1997.

9

[28] Yang Liu, Alexandros Neophytou, Sunando Sengupta, and
Eric Sommerlade. Relighting images in the wild with a
self-supervised siamese auto-encoder. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision, pages 32–40, 2021.

[29] Abhimitra Meka, Christian Haene, Rohit Pandey, Michael
Zollh¨ofer, Sean Fanello, Graham Fyffe, Adarsh Kowdle,
Xueming Yu, Jay Busch, Jason Dourgarian, et al. Deep
reﬂectance ﬁelds: high-quality facial reﬂectance ﬁeld infer-
ence from color gradient illumination. ACM Transactions on
Graphics (TOG), 38(4):1–12, 2019.

[30] Abhimitra Meka, Rohit Pandey, Christian H¨ane, Sergio Orts-
Escolano, Peter Barnum, Philip David-Son, Daniel Erickson,
Yinda Zhang, Jonathan Taylor, Soﬁen Bouaziz, et al. Deep
relightable textures: volumetric performance capture with
neural rendering. ACM Transactions on Graphics (TOG),
39(6):1–21, 2020.

[31] Thomas Nestmeyer, Jean-Franc¸ois Lalonde, Iain Matthews,
and Andreas Lehrmann. Learning physics-guided face re-
In Proceedings of the
lighting under directional light.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5124–5133, 2020.

[32] Nvidia. Nvidia optical ﬂow sdks. https://developer.

nvidia.com/opticalflow-sdk.

[33] Alec Radford, Luke Metz, and Soumith Chintala. Un-
supervised representation learning with deep convolu-
arXiv preprint
tional generative adversarial networks.
arXiv:1511.06434, 2015.

[34] Ravi Ramamoorthi and Pat Hanrahan. On the relationship
between radiance and irradiance: determining the illumina-
tion from images of a convex lambertian object. JOSA A,
18(10):2448–2459, 2001.

[35] Christian Richardt, Carsten Stoll, Neil A Dodgson, Hans-
Peter Seidel, and Christian Theobalt. Coherent spatiotem-
poral ﬁltering, upsampling and rendering of rgbz videos. In
Computer graphics forum, volume 31, pages 247–256. Wi-
ley Online Library, 2012.

[36] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
Artistic style transfer for videos. volume 9796 LNCS, 2016.
[37] S. Ryu, S. H. Lee, S. C. Ahn, and J. Park. Tangible video tele-
conference system using real-time image-based relighting.
IEEE Transactions on Consumer Electronics, 55(3):1162–
1168, 2009.

[38] Arman Savran, Nes¸e Aly¨uz, Hamdi Dibeklio˘glu, Oya
C¸ eliktutan, Berk G¨okberk, B¨ulent Sankur, and Lale Akarun.
Bosphorus database for 3d face analysis. In European work-
shop on biometrics and identity management, pages 47–56.
Springer, 2008.

[39] Soumyadip Sengupta, Angjoo Kanazawa, Carlos D Castillo,
and David W Jacobs. Sfsnet: Learning shape, reﬂectance
and illuminance of facesin the wild’. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 6296–6305, 2018.

[40] Artem Sevastopolsky, Savva Ignatiev, Gonzalo Ferrer,
Evgeny Burnaev, and Victor Lempitsky. Relightable 3d
arXiv preprint
head portraits from a smartphone video.
arXiv:2012.09963, 2020.

[41] YiChang Shih, Sylvain Paris, Connelly Barnes, William T
Freeman, and Fr´edo Durand. Style transfer for headshot por-
traits. 2014.

[42] Zhixin Shu, Sunil Hadap, Eli Shechtman, Kalyan Sunkavalli,
Sylvain Paris, and Dimitris Samaras. Portrait lighting trans-
fer using a mass transport approach. ACM Transactions on
Graphics, 37(1), 2017.

[43] Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli,
Eli Shechtman, and Dimitris Samaras. Neural face editing
In Proceedings of the
with intrinsic image disentangling.
IEEE conference on computer vision and pattern recogni-
tion, pages 5541–5550, 2017.

[44] Yibing Song, Linchao Bao, Shengfeng He, Qingxiong Yang,
and Ming-Hsuan Yang. Stylizing face images via multi-
ple exemplars. Computer Vision and Image Understanding,
162:135–145, 2017.

[45] Giota Stratou, Abhijeet Ghosh, Paul Debevec, and Louis-
Philippe Morency. Effect of illumination on automatic ex-
pression recognition: a novel 3d relightable facial database.
In Face and Gesture 2011, pages 611–618. IEEE, 2011.
[46] Tiancheng Sun, Jonathan T. Barron, Yun Ta Tsai, Zexiang
Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay
Busch, Paul Debevec, and Ravi Ramamoorthi. Single image
portrait relighting. ACM Transactions on Graphics, 38(4),
jul 2019.

[47] Tiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello,
Christoph Rhemann, Paul Debevec, Yun-Ta Tsai, Jonathan T
Light stage super-
Barron, and Ravi Ramamoorthi.
ACM
resolution: continuous high-frequency relighting.
Transactions on Graphics (TOG), 39(6):1–12, 2020.
[48] Ayush Tewari, Mohamed Elgharib, Florian Bernard, Hans-
Peter Seidel, Patrick P´erez, Michael Zollh¨ofer, and Chris-
tian Theobalt. Pie: Portrait image embedding for semantic
control. ACM Transactions on Graphics (TOG), 39(6):1–14,
2020.

[49] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann,
Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-
Brualla, Tomas Simon, Jason Saragih, Matthias Nießner,
In Computer
et al. State of the art on neural rendering.
Graphics Forum, volume 39, pages 701–727. Wiley Online
Library, 2020.

[50] Jiandong Tian, Zachary Murez, Tong Cui, Zhen Zhang,
David Kriegman, and Ravi Ramamoorthi. Depth and image
restoration from light ﬁeld in a scattering medium. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 2401–2410, 2017.

[51] Peter V., Black Michael J Kong Naejin, and Gehler. Intrin-
sic video. pages 360–375. Springer International Publishing,
2014.

[52] Oliver Wang, James Davis, Erika Chuang, Ian Rickard,
Krystle De Mesa, and Chirag Dave. Video relighting using
In Computer Graphics Forum, vol-
infrared illumination.
ume 27, pages 271–279. Wiley Online Library, 2008.
[53] Ting Chun Wang, Ming Yu Liu, Jun Yan Zhu, Guilin Liu,
Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-
video synthesis. volume 2018-December, 2018.

[54] Yang Wang, Zicheng Liu, Gang Hua, Zhen Wen, Zhengyou
Zhang, and Dimitris Samaras. Face re-lighting from a single

10

[66] zllrunning.
tory, 2019.
face-parsing.PyTorch.

face-parsing.pytorch.

GitHub reposi-
https://github.com/zllrunning/

[67] M. Zollh¨ofer, J. Thies, P. Garrido, D. Bradley, T. Beeler, P.
P´erez, M. Stamminger, M. Nießner, and C. Theobalt. State
of the Art on Monocular 3D Face Reconstruction, Tracking,
and Applications. Computer Graphics Forum (Eurographics
State of the Art Reports 2018), 37(2), 2018.

image under harsh lighting conditions. In 2007 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1–8. IEEE, 2007.

[55] Zhibo Wang, Xin Yu, Ming Lu, Quan Wang, Chen Qian, and
Feng Xu. Single image portrait relighting via explicit mul-
tiple reﬂectance channel modeling. ACM Transactions on
Graphics, 39(6):1–13, nov 2020.

[56] Andreas Wenger, Andrew Gardner, Chris Tchou, Jonas
Unger, Tim Hawkins,
Perfor-
mance relighting and reﬂectance transformation with time-
multiplexed illumination. ACM Transactions on Graphics
(TOG), 24(3):756–764, 2005.

and Paul Debevec.

[57] Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao
Su, and Ravi Ramamoorthi. Deep view synthesis from sparse
photometric images. ACM Transactions on Graphics (TOG),
38(4):1–13, 2019.

[58] Zexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi
Ramamoorthi. Deep image-based relighting from optimal
sparse samples. ACM Transactions on Graphics (ToG),
37(4):1–13, 2018.

[59] Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu
Shen, Ruigang Yang, and Xun Cao. Facescape: a large-scale
high quality 3d face dataset and detailed riggable 3d face
prediction. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 601–610,
2020.

[60] Genzhi Ye, Elena Garces, Yebin Liu, Qionghai Dai, and
vol-

Intrinsic video and applications.

Diego Gutierrez.
ume 33, 2014.

[61] Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang, and Matthew J
Rosato. A 3d facial expression database for facial behavior
research. In 7th international conference on automatic face
and gesture recognition (FGR06), pages 211–216. IEEE,
2006.

[62] Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun,
Tianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip
Davidson, Christoph Rhemann, Paul Debevec, Jonathan T.
Barron, Ravi Ramamoorthi, and William T. Freeman. Neu-
ral light transport for relighting and view synthesis. ACM
Trans. Graph., 40(1), Jan. 2021.

[63] Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Cana-
van, Michael Reale, Andy Horowitz, and Peng Liu. A
high-resolution spontaneous 3d dynamic facial expression
database. In 2013 10th IEEE international conference and
workshops on automatic face and gesture recognition (FG),
pages 1–6. IEEE, 2013.

[64] Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Canavan,
Michael Reale, Andy Horowitz, Peng Liu, and Jeffrey M Gi-
rard. Bp4d-spontaneous: a high-resolution spontaneous 3d
dynamic facial expression database. Image and Vision Com-
puting, 32(10):692–706, 2014.

[65] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David Ja-
cobs. Deep single-image portrait relighting. In Proceedings
of the IEEE International Conference on Computer Vision,
volume 2019-Octob, pages 7193–7201. Institute of Electri-
cal and Electronics Engineers Inc., oct 2019.

11

