MSED: a multi-modal sleep event detection

model for clinical sleep analysis

Alexander Neergaard Olesen, Member, IEEE, Poul Jennum,

Emmanuel Mignot, and Helge B. D. Sorensen, Senior Member, IEEE

January 8, 2021

1
2
0
2

n
a
J

7

]

V
C
.
s
c
[

1
v
0
3
5
2
0
.
1
0
1
2
:
v
i
X
r
a

Abstract

Study objective:

Clinical sleep analysis require manual analysis of sleep patterns for correct diagnosis

of sleep disorders. Several studies show signiﬁcant variability in scoring discrete sleep events. We wished

to investigate, whether an automatic method could be used for detection of arousals (Ar), leg movements

(LM) and sleep disordered breathing (SDB) events, and if the joint detection of these events performed

better than having three separate models.

Methods: We designed a single deep neural network architecture to jointly detect sleep events in a

polysomnogram. We trained the model on 1653 recordings of individuals, and tested the optimized model

on 1000 separate recordings. The performance of the model was quantiﬁed by F1, precision, and recall

scores, and by correlating index values to clinical values using Pearson’s correlation coeﬃcient.

Results:

F1 scores for the optimized model was 0.70, 0.63, and 0.62 for Ar, LM, and SDB, respectively.

The performance was higher, when detecting events jointly compared to corresponding single-event models.

Index values computed from detected events correlated well with manual annotations (r2 = 0.73, r2 =

0.77, r2 = 0.78, respectively).

Conclusion:

Detecting arousals, leg movements and sleep disordered breathing events jointly is

possible, and the computed index values correlates well with human annotations.

1

Introduction

Clinical sleep analysis is currently performed manually by experts based on guidelines from the American

Academy of Sleep Medicine (AASM) detailed in the AASM Scoring Manual [1]. The guidelines detail

1

 
 
 
 
 
 
both technical and clinical best practices for setting up and recording polysomnographies (PSGs), which

are overnight recordings of various electrophysiological signals, such as electroencephalography (EEG),

electrooculography (EOG), chin and leg electromyography (EMG), electrocardiography (ECG), respiratory

inductance plethysmography from the thorax and abdomen, oronasal pressure, and blood oxygen levels.

Based on these signals, expert technicians analyse and score the PSG for sleep stages [wakefulness (W),

rapid eye movement (REM) sleep, non-REM stage 1 (N1), non-REM stage 2 (N2), and non-REM stage

3 (N3)], and sleep micro-events summarized in key metrics, such as the apnea-hypopnea index (AHI)

(number of apneas and hypopneas per hour of sleep), the periodic leg movement index (PLMI) (number

of period leg movements per hour of sleep), and the arousal index (ArI) (number of arousals per hour of

sleep).

Arousals are deﬁned as abrupt shifts in EEG frequencies towards alpha, theta, and beta rhythms

for at least 3 s with a preceding period of stable sleep of at least 10 s. During REM sleep, where the

background EEG shows similar rhythms, arousal scoring requires a concurrent increase in chin EMG

lasting at least 1 s. Limb movements (LMs) should be scored in the leg EMG channels, when there is

an increase in amplitude of at least 8 µV above baseline level with a duration between 0.5 s to 10 s. A

periodic leg movement (PLM) series is then deﬁned as a sequence of 4 LMs, where the time between

LM onsets is between 5 min to 90 min. Apneas are generally scored when there is a complete (≥90 %

of pre-event baseline) cessation of breathing activity either due to a physical obstruction (obstructive

apnea) or due to an underlying disruption in the central nervous system control (central apnea) for at

least 10 s. When the breathing is only partially reduced (≥30 % of pre-event baseline) and the duration

of the excursion is ≥10 s, the event is scored as a hypopnea if there is either a ≥4 % oxygen desaturation

or a ≥3 % oxygen desaturation coupled with an arousal (Ar).

However, several studies have shown signiﬁcant variability in the scoring of both sleep stages [2]–[8] and

sleep micro-events [9]–[16]. This has prompted extensive research into automatic methods for classifying

sleep stages in large-scale studies [17]–[24], while the research in automatic arousal [25]–[27] and LM [28]

detection on a similar scale is limited. Biswal et al. recently proposed a model based on a combination

of recurrent and convolutional neural networks, where the same architecture was used for sleep stage

classiﬁcation, AHI and limb movement index (LMI) prediction. They trained their model using 9000 PSG

recordings and evaluated the performance on the three tasks on a held out test set containing 1000 PSGs.

However, this model was trained in separate runs for each downstream task; furthermore, post-processing

was performed on the event predictions (apneas, limb movements).

In this study, we introduce the multi-modal sleep event detection (MSED) model for joint detection of

sleep micro-events, in this case Ars, sleep disordered breathing (SDB), and LMs. The model is based on

recent advances in machine learning and challenges current state of the art methods by directly classifying

and localizing sleep micro-events in the PSG signals at the same time.

2

Table 1: MrOS demographics by subset.

Dtrain

Deval

Dtest

p-value

n
Age, years
BMI, kg s−2

TST, min
SL, min
REML, min
WASO, min
SE, %
N1, %
N2, %
N3, %
REM, %

ArI, h−1
AHI, h−1
PLMI, h−1

1653
76.4 ± 5.6 [67.0, 90.0]
27.3 ± 3.9 [16.0, 47.0]

200
76.8 ± 5.4 [68.0, 90.0]
27.0 ± 3.6 [19.0, 40.0]

1000
76.4 ± 5.3 [67.0, 90.0]
27.0 ± 3.7 [17.0, 45.0]

357.3 ± 69.0 [54.0, 615.0]
22.9 ± 25.6 [1.0, 349.0]
109.5 ± 77.9 [0.0, 578.0]
116.7 ± 67.1 [11.0, 462.0]
75.9 ± 12.1 [17.0, 97.0]
6.8 ± 4.1 [0.0, 31.0]
62.7 ± 9.5 [28.0, 89.0]
11.4 ± 9.0 [0.0, 55.0]
19.2 ± 6.5 [0.0, 44.0]

354.0 ± 69.1 [108.0, 503.0]
21.6 ± 23.0 [1.0, 135.0]
103.5 ± 70.0 [10.0, 413.0]
119.0 ± 70.8 [15.0, 372.0]
75.5 ± 12.3 [37.0, 96.0]
7.0 ± 4.5 [0.0, 28.0]
62.0 ± 9.7 [30.0, 90.0]
11.8 ± 9.7 [0.0, 55.0]
19.4 ± 7.2 [0.0, 41.0]

353.6 ± 68.7 [62.0, 572.0]
25.1 ± 32.1 [1.0, 402.0]
107.2 ± 75.3 [3.0, 590.0]
112.9 ± 65.0 [6.0, 458.0]
76.4 ± 11.8 [26.0, 98.0]
6.9 ± 4.7 [1.0, 58.0]
62.8 ± 10.0 [21.0, 95.0]
11.1 ± 9.0 [0.0, 57.0]
19.3 ± 6.7 [0.0, 42.0]

23.5 ± 11.8 [3.0, 87.0]
13.5 ± 13.9 [0.0, 83.0]
35.4 ± 37.1 [0.0, 233.0]

23.4 ± 11.0 [4.0, 77.0]
13.6 ± 13.3 [0.0, 59.0]
36.6 ± 39.0 [0.0, 178.0]

23.8 ± 11.8 [4.0, 102.0]
14.2 ± 15.5 [0.0, 89.0]
36.0 ± 37.7 [0.0, 175.0]

-
0.404
0.247

0.312
0.284
0.466
0.471
0.690
0.968
0.451
0.638
0.894

0.661
0.907
0.993

Signiﬁcant p-values at signiﬁcance level α = 0.05 are highlighted in bold. BMI: body-mass index; TST: total
sleep time; SL: sleep latency; REML: REM sleep latency; WASO: wake after sleep onset; SE: sleep eﬃciency;
N1: non-REM stage 1; N2: non-REM stage 2; N3: non-REM stage 3; REM: rapid eye movement; ArI: arousal
index; AHI: apnea-hypopnea index; PLMI: periodic leg movement index.

2 Data

We collected PSGs from the MrOS Sleep Study, an ancillary part of the larger Osteoporotic Fractures in

Men Study. The main goal of the study is to research and discover connections between sleep disorders,

skeletal fractures, and cardiovascular disease and mortality in community-dwelling older (>65 years) [29]–

[31]. Of the original 5994 study participants, 3135 subjects were enrolled at one of six sites in the USA for

a comprehensive sleep assessment, while 2909 of these underwent a full-night in-home PSG recording, The

PSG studies were subsequently scored by certiﬁed sleep technicians. Sleep stages were scored into stages 1,

2, 3, 4 and REM, while stages 3 and 4 combined into slow wave sleep (SWS) according to R&K rules [32].

Ars were scored as abrupt increases in EEG frequencies lasting at least 3 s according to American Sleep

Disorders Association (ASDA) rules [33]. Apneas were deﬁned as complete or near complete cessation of

airﬂow lasting more than 10 s with an associated 3 % or greater SaO2 desaturation, while hypopneas were

based on a clear reduction in breathing of more than 30 % deviation from baseline breathing lasting more

than 10 s, and likewise assocated with a greater than 3 % SaO2 desaturation. While the scoring criteria

for scoring LMs are not explicitly available for the MrOS Sleep Study, the prevailing standard at the

time of the study was to score LMs following an increase in leg EMG amplitude of more than 8 µV above

resting baseline levels for at least 0.5 s, but shorter than 10 s [34].

3

2.1 Subset demographics and partitioning

We used a total of 2853 PSG studies downloaded from the National Sleep Research Resource (NSRR) [35],

[36], which we partitioned into a training set (Dtrain, ntrain = 1653), a validation set (Deval, neval = 200),

and a ﬁnal testing set (Dtest, ntest = 1000). Key demographics and PSG-related variables for each subset

are shown as mean ± standard deviation with range in parenthesis in Table 1.

2.1.1 Signal and events

For this study, we considered three PSG events: Ars, LMs, and SDB events, which includes all forms

of apneas (obstructive and central) and hypopneas. These event types are each based on a speciﬁc set

of electrophysiological channels from the PSG, and as such, we extracted left and right central EEG

(C3 and C4), left and right EOG, left and right chin EMG, left and right leg EMG, nasal pressure, and

respiratory inductance plethysmography from the thorax and abdomen. EEG and EOG channels were

referenced to the contralateral mastoid process, while a chin EMG was synthesized by subtracting the

right chin EMG from the left chin EMG.

Apart from the raw signal data, we also extracted onset time relative to the study start time and

duration times for each event type in each PSG.

3 Methods

Notation We denote by

for

1, N
(cid:74)

, and by n ∈

N
(cid:74)

the set of integers {n ∈ N | a ≤ n ≤ b} with

being shorthand

N

(cid:74)

(cid:75)

a, b
(cid:75)

(cid:74)
the nth sample in

N

. A segment of PSG data is denoted by x ∈ RC×T ,

(cid:75)

(cid:75)

(cid:75)
where C, T is the number of channels and the duration of the segment in samples, respectively. The
(cid:9), where
corresponding set of Nt true events for the segment is denoted by εt = (cid:8)(%t
%, δ are the center point and duration, respectively, of the ith event. By χ ∈ D∗ we denote a sample in

+ | i ∈

) ∈ R2

i, δt
i

Nt

(cid:74)

(cid:75)

(cid:74)

either one of the three subsets. In the description of the network architecture, we have omitted the batch

dimension from all calculations for brevity.

3.1 Model overview

Given an input set χ = {x, εt} ∈ RC×T × RNt×2

+

containing PSG data with C channels and T time steps,

and true events ε, the goal of the model is to detect any possible events in the segment, where, in this

context, detection covers both classiﬁcation and localization of any event in the segment space.

To accomplish this, the model generates a set of default event windows εd = (cid:8)(cid:0)%d

j , δd
j

(cid:1) ∈ R2

+ | j ∈

for the current segment, and match each true event to a default event window if their intersection-over-

(cid:9)

Nd

(cid:74)

(cid:75)

union (IoU) is at least 0.5.

4

At test time, we generate predictions over the default event windows and use a non-maximum

suppression procedure to select between the candidate predictions. For a given class k, the procedure is as

follows. First, the predictions are sorted according to probability of the event, which is above a threshold

θk. Then, using the most probable prediction as an anchor, we sequentially evaluate the IoU between the

anchor and the remaining candidate predictions, removing those with IoU >= 0.5.

The output of the model is thus the set {p, y} containing the predicted class probabilities along with

the corresponding onsets and durations.

3.2 Signal conditioning

We resampled all signals to a common sampling frequency of fs = 128 Hz using a poly-phase ﬁltering

approach (Kaiser window, β = 5.0). Based on recommended ﬁlter speciﬁcations from the AASM, we

designed Butterworth IIR ﬁlters for four sets of signals. EEG and EOG channels were ﬁltered with a

2nd order ﬁlter with a 0.3 Hz–35 Hz passband, while chin and leg EMG channels were ﬁltered with a 4th

order high-pass ﬁlter with a 10 Hz cut-oﬀ frequency. The nasal pressure channel was ﬁltered with a 4th

order high-pass ﬁlter with a 0.03 Hz cut-oﬀ frequency, while the thoracoabdominal channels were ﬁltered

with a 2nd order with a 0.1 Hz–15 Hz passband.

All ﬁlters were implemented using the zero-phase method, which sequentially applies the ﬁlter in the

forward direction, and then in the backwards direction. This accounts for the non-linear phase response

and subsequent frequency-dependent group delay inherent in IIR ﬁlters, but also eﬀectively squares the

magnitude response of the ﬁlter.

Filtered signals were subsequently standardized by

x(i) =

ˆx(i) − µ(i)
σ(i)

,

(1)

where ˆx(i) ∈ RC×T is the raw matrix containing C input channels and T samples, and µ(i), σ(i) ∈ RC

are the mean and standard deviation vectors for the ith PSG, respectively. This is a common approach

in computer vision tasks, and beneﬁcial to ensure a proper gradient propagation through a deep neural

network [37].

3.3 Target encoding

For each data segment, target event classes π ∈ RNm×K generated by one-hot encoding, while the target

detection variable containing the onset and duration times t ∈ RNm×2 was encoded as

ti =

i − %d
%m
j
δd
j

, log δm
i
δd
j

!
,

i ∈

Nm

(cid:74)

(cid:75)

, j ∈

Nd
(cid:74)

,
(cid:75)

(2)

5

 
where %m
i

is the center point of the true event matched to a default event window %d
j

, and δm
i

and δd
j

are

the corresponding durations of the true and default events.

3.4 Data sampling

As the total number of default event windows in a data segment Nd most likely will be much higher

than the number of event windows matched to a true event, i. e. Nd (cid:29) Nm, we implemented a similar

random data sampling strategy as in [25]. At training step t, a given PSG record r has a certain number

of associated number of Ar, LM, and SDB events (nAr, nLM, nSDB, respectively). We randomly sample

a class k with equal probability pk = 1
K

, whilst disregarding the negative class, since this class is most

likely over-represented in the data segment. Given the class k, we randomly sample an event εk with

probability p(εk) ∝ nk, and afterwards, we extract a segment of data of size C × T , where the start of

the segment is sampled from [¯εk − T, ¯εk + T ], where ¯εk is the sample midpoint of the event εk, thereby

ensuring at least 50 % overlap with at least one event associated with the data segment.

We found that this approach to sampling data segments with a large ratio of negative to positive

samples to be beneﬁcial in all our experiments, when monitoring the loss on the validation set.

3.5 Network architecture

Similar to the architecture described in [27], we designed a splitstream network architecture for the

diﬀerentiable function Φ, where each stream is responsible for the bulk feature extraction for a speciﬁc

event class. For the given problem of detecting Ars, LMs, and SDBs, the network contains three streams:

the Ar stream takes as input the EEGs, the EOGs, and the chin EOG signals for a total of CAr = 5

channels; the LM stream receives the CLM = 2 leg EMG signals; and the SDB stream receives the nasal

pressure and the thoracoabdominal signals for a total of CSDB = 3 channels. An overview of the network

architecture is shown graphically in Figure 1.

3.5.1 Stream speciﬁcs

Each stream is comprised of two components. First, a mixing module ϕmix : RC∗×T → RC∗×T computes

a non-linear mixing of the C channels using a set of C single-strided 1-dimensional ﬁlters w ∈ RC×C

and rectiﬁed linear unit (ReLU) activation [38], such that ϕmix(x) = max{0, w ⊗ x + b}, where the max

operation introduces the non-linearity, ⊗ is the conv operator over the C feature maps, and b ∈ RC

is a bias vector (in this case b = 0). Second, the output activations from ϕmix are used as input to
a deep neural network module ϕfeat : RC∗×T → Rf 0×T 0, which transforms the input feature maps to
T 0 . The feature extraction
module ϕfeat is realized using kmax successive conv operations with an increasing number of ﬁlters

a f 0 × T 0 feature space with a temporal dimension reduced by a factor of T

f 0 = f02k−1, k ∈

kmax
(cid:74)

(cid:75)

, where f0 is a tunable base ﬁlter number. Each conv feature map is normalized

6

Figure 1: MSED network architecture. The left column shows the output dimensions for each operation as
(number of ﬁlters[ x singleton] x time steps). Each stream on the right (green) processes a separate set of input
channels (blue, top), the results of which are concatenated before the bidirectional gated recurrent unit (bGRU)
(yellow). The outputs from the additive attention layer (purple) are convolved in the ﬁnal classiﬁcation and
localization layers (red) to output the probabilities for each event class, and the predicted onset and duration of
each event (blue, bottom). Convolution layers (orange, green, red) are detailed as [number of feature maps x
kernel size, stride]. Recurrent layer (yellow) shows the direction and number of hidden units. Additive attention
layer (purple) is described with the number of hidden and output units.

7

EEG / EOG / EMGleg EMG Nasal / Thor / AbdoAr streamLM streamSDB streamOutput sizeAr: LM: SDB: (1×5× )(1×2× )(1×3× )×(,1),1 Ar Ar(×1× ) ∗(2×1×) 0 2×(,1),1 LM LM×(,1),1 SDB SDB2×(1,3),2 04×(1,3),2 08×(1,3),2 016×(1,3),2 032×(1,3),2 064×(1,3),2 0128×(1,3),2 0(4×1×) 0 4(8×1×) 0 8(16×1×) 0 16(32×1×) 0 32(64×1×) 0 64(128×1×) 0 1282×(1,3),2 04×(1,3),2 08×(1,3),2 016×(1,3),2 032×(1,3),2 064×(1,3),2 0128×(1,3),2 02×(1,3),2 04×(1,3),2 08×(1,3),2 016×(1,3),2 032×(1,3),2 064×(1,3),2 0128×(1,3),2 0ConcatenationPast outputforward GRU, 128reverse GRU, 128Future outputConcatenation(384×) 0 128(128×) 128(256×) 128Additive attention, 128, K(4×) 128 ×256,1  2×256,1  (4×) 128p(event)(onset, duration)(× )×(×2)    using batch normalization (BN) [39], such that if ˜z ∈ Rf 0×T 0 denotes the output from a conv operation,

the subsequent normalized version is computed as

z = γ

˜z − E[˜z]
pVar[˜z] + (cid:15)

+ β,

(3)

where E[˜z] ∈ Rf 0

, Var[˜z] ∈ Rf 0
+

is the expectation and variance over the temporal dimension of each

feature map, (cid:15) is a small constant, and {γ, β} ∈ Rf 0

× Rf 0 are learnable parameters representing the

mean and bias for each feature map. Each normalized conv output is subsequently activated using ReLU.

3.5.2 Feature fusion for sequential processing

The outputs from the three feature extraction streams are subsequently fused by concenating each

output vector z∗ into a combined feature vector z = (zar, zlm, zsdb) ∈ R3f 0×T 0. We introduce sequential

modeling of the feature vectors using a bGRU [40], which has the advantage over other recurrent neural

network (RNN)-based models such as the long short-term memory (LSTM) of having fewer trainable

parameters while still being powerful enough to model complex, temporal relationships [41]. The output

of the gated recurrent unit (GRU) for timestep t is a vector ht = (cid:0)hf
concatenated outputs from the forward (f) and backward (b) directions. Each directional feature vector

(cid:1) ∈ R2nh containing the

t, hb
t

is calculated as a weighted combination of a gated new input nt and the feature vector from the previous

timestep ht−1

h∗
t

= (1 − ut) ⊗ nt + ut ⊗ ht−1.

The update gate ut and gated new input nt are computed as

ut = σ(cid:0)Wz

uzt + bz
u

+ Wh

(cid:1),

uht−1 + bh
u
+ rt ⊗ (cid:0)Wh

nt = tanh(cid:0)Wz

nzt + bz
n

nht−1 + bh
n

(cid:1)(cid:1),

where W∗

∗, b∗
∗

are weight matrices and bias vectors, respectively, and rt is a reset gate computed as

rt = σ(Wz

rzt + bz
r

+ Wr

hht−1 + br
h

).

(4)

(5)

(6)

(7)

3.5.3 Additive attention

The attention mechanism is a powerful technique to introduce a way for the network to focus on

relevant regions and disregard irrelevant regions of a data sample, and is a key part of the highly

successful Transformer model [42] and the subsequent state-of-the-art BERT model for natural language

processing [43]. In this work, we implemented a simple, but powerful, additive attention mechanism [44],

which computes context-vectors c ∈ R2nh for each event class as the weighted sum of the feature vector

8

outputs h ∈ R2nh×T 0 from the ϕh. Formally, attention is computed as

c = h · α =

T 0
X

t=1

htαt,

(8)

where T 0 is the reduced temporal dimension, ht is the feature vector for time step t, and αt ∈ RK is the

attention weight computed as

αt =

exp(tanh(htWu)Wa)

PT 0
τ

exp(tanh(hτ Wu)Wa)

.

(9)

Here, Wu ∈ R2nh×na and Wa ∈ Rna×K are linear mappings of the feature vectors, and tanh is the

hyperbolic tangent function.

3.5.4 Detection

The ﬁnal event classiﬁcation and localization is handled by two modules, ψclf : R2nh×K → RNd×K

and ψloc : R2nh×K → RNd×2, respectively. The classiﬁcation module ψclf : c 7→ p outputs a tensor

p ∈ [0, 1]Nd×K

+

containing predicted event class probabilities for each default event window. The

localization module ψloc : c 7→ y outputs a tensor y ∈ RNd×2 containing encoded relative onsets and

durations for a detected event for each default event window.

3.6 Loss function

Similar to [45], we optimized the network parameters according to a three-component loss function

consisting of: i) a localization loss ‘loc; ii) a positive classiﬁcation loss ‘+, and iii) a negative classiﬁcation

loss ‘−, such that the total loss ‘ was deﬁned by

‘ = ‘loc + ‘+ + ‘−.

The localization loss ‘loc was calculated using a Huber function

‘loc =

1

N+

X

f (i)
H

i∈π+

fH =






0.5(y − t)2,

if |y − t| < 1,

|y − t| − 0.5, otherwise,

(10)

(11)

(12)

where i ∈ π+ yields indices of event windows with positive targets, i. e. event windows matched to an

arousal, LM or SDB target, and N+ is the number of positive targets in the given data segment.

The positive classiﬁcation loss component ‘+ was calculated using a simple cross-entropy over the

9

event windows matched to an arousal, LM, or SDB event:

‘+ =

1

N+

X

X

π(i)
k

i∈π+

K

k∈
(cid:74)

(cid:75)

log p(i)

k , where p(i)

k

=

exp s(i)
k
exp s(i)
j

P
j

,

(13)

and π(i)
k

, p(i)
k

, and s(i)
k

are the true class probability, predicted class probability, and logit score for the

ith event window containing a positive sample.

Similar to [46], [47], the negative classiﬁcation loss ‘− was calculated using a hard negative mining

approach to balance the number of positive and negative samples in a data segment after matching default

event windows to true events [48]. Speciﬁcally, this is accomplished by calculating the probability for the

negative class (no event) for each unmatched default event window, and then calculating the cross entropy

loss using the Z most probable samples. In our experiments, we set the ratio of positive to negative

samples as 1:3, such that the calculation of ‘ involves Z = 3 times as many negative as positive samples.

We also explored a focal loss objective function for computing ‘+ and ‘− [49], however, we found that

this approach severely deteriorated the ability of the network to accurately detect LM and SDB events

compared to using worst negative mining.

3.7 Optimization

The network parameters were optimized using adaptive moment estimation (Adam) according to the

loss function described in Equation (10) [50]. This algorithms uses ﬁrst (m) and second (v) moment

estimations of gradients to update the model parameters θ of a diﬀerentiable function f at time t:

m(t) = β1m(t−1) + (1 − β1)∇θf (t)(cid:16)
θf (t)(cid:16)

v(t) = β2v(t−1) + (1 − β2)∇2

θ(t−1)(cid:17)
θ(t−1)(cid:17)

,

(14)

(15)

where β1, β2 are exponential decay rates for the ﬁrst and second moment, respectively, ∇ is the gradient

vector with respect to θ, and ∇2
θ

is the Hadamard product ∇θf (cid:12) ∇θf . The moment vectors are initialized

with 0’s, which induce a bias towards zero. This can be oﬀset by computing a bias-corrected estimate of

each moment vector as

which yields the ﬁnal update to θ as

ˆm(t) = m(t)
1 − βt
1
ˆv(t) = v(t)
1 − βt
2

,

θ(t) = θ(t−1) − η

√

ˆm(t)
ˆv(t) + (cid:15)

,

10

(16)

(17)

(18)

where η is the learning rate.

3.8 Experimental setups

In our experiments, we ﬁxed the exponential decay rates at (β1, β2) = (0.9, 0.999), the learning rate at

η = 10−3, and (cid:15) = 10−8. The learning rate was decayed in a step-wise manner by multiplying η with a

factor of 0.1 after 3 consecutive epochs with no improvement in loss value on the validation dataset.

Similarly, we employed an early stopping scheme by monitoring the loss on the validation dataset and

stopping the model training after 10 epochs of no improvement on Deval.

We tested four types of models in two categories: the ﬁrst is a default split-stream model as shown

in Figure 1 with and without weight decay (splitstream, splitstream-wd). The second is a variation of the

split-stream model, but where the ψclf and ψloc modules are realized using depth-wise convolutions, such

that each attention group is used only for that type of event. The second category is also tested with and

without weight decay (splitstream-dw, splitstream-dw-wd).

3.9 Performance evaluation

Performance was quantiﬁed using precision, recall and F1 scores. Statistical signiﬁcance in F1 score

between groups was assessed with Kruskall-Wallis H -tests. The performance of joint vs. single-event

detection models was tested with Wilcoxon signed rank tests for matched samples. The relationships

between true and predicted ArI, AHI, and LMI were assessed using linear models and Pearsons r2.

Signiﬁcance was set at α = 0.05.

4 Results and discussion

4.1 Model architecture evaluation

We found no signiﬁcant diﬀerences in F1 performance for either Ar (Kruskal-Wallis H = 0.961, p = 0.811),

LM (H = 0.230, p = 0.973), or SDB detection (H = 2.838, p = 0.417), when evaluating the model

architectures on Deval (see Figure 2). Based on this result, all further modeling was based on the default

splitstream architecture for simplicity.

4.2 Joint vs. single event detection

For each event type, we evaluated the F1 score as a function of classiﬁcation threshold θ on Deval for

both the joint detection model as well as the single-event models. It can be observed in Figure 3 that for

all three events, the joint detection model achieves higher F1 score, although the apparent increase is

not as large for LM detection. This was also observed when evaluating the joint and single detection

11

Figure 2: Architecture optimization.

Figure 3: Optimizing F1 performance on Deval as a function of θ). Full lines correspond to the joint model
and dashed lines are the corresponding single-event detection model. The blue and orange dots correspond to
optimized model performance on Dtest.

models with optimized thresholds on Dtest for both Ar (Wilcoxon W = 30440.0, p = 2.481 × 10−127), LM

(W = 101103.0, p = 6.454 × 10−60), and SDB detection (W = 93647.0, p = 2.378 × 10−64). Precision,

recall and F1 scores for optimized models evaluated on Dtest are shown in Table 2. These ﬁndings

are interesting, because they provide evidence that the presence of diﬀerent event types can module

the detection of others, and that this can be modeled using automatic methods. This is in line with

what previous studies have found e. g. on event-by-event scoring agreement in arousals, which improved

signiﬁcantly from 0.59 % to 0.91 %, when including respiratory signals in the analysis [13].

4.2.1 Detection vs. manual scorings

For each event type, we computed the correlation coeﬃcient between the predicted and true index

values (arousal index, ArI; apnea-hypopnea index, AHI; limb movement index, LMI), which is shown

12

arousallmsdb0.00.20.40.60.81.0F1n.s.n.s.n.s.splitstreamsplitstream-dwsplitstream-wdsplitstream-dw-wd0.50.60.70.80.9θ0.30.40.50.60.7F1Ar0.50.60.70.80.9θLM0.50.60.70.80.9θSDBFigure 4: Evaluating optimized joint and single-event detection models on Dtest. ∗∗∗∗: p < 10 × 10−4. Ar: arousal;
LM: limb movement; SDB: sleep disordered breathing.

Table 2: Performance scores for optimized models evaluated on
Dtest.

Event Model

Precision

Recall

F1

Ar

LM

SDB

Joint
Single
Joint
Single
Joint
Single

0.759 ± 0.114
0.777 ± 0.107
0.650 ± 0.169
0.661 ± 0.166
0.817 ± 0.142
0.765 ± 0.142

0.672 ± 0.125
0.571 ± 0.127
0.647 ± 0.120
0.607 ± 0.116
0.526 ± 0.146
0.486 ± 0.121

0.704 ± 0.106
0.649 ± 0.113
0.628 ± 0.123
0.613 ± 0.116
0.624 ± 0.115
0.578 ± 0.097

Metrics are shown aggregated across PSGs. Ar: arousal; LM: limb
movement; SDB: sleep disordered breathing.

in Figure 5. We found a large positive correlation between true and predicted values for ArI (r2 = 0.73,

p = 2.5 × 10−285), AHI (r2 = 0.77, p = 9.3 × 10−316), and LMI (r2 = 0.78, p = 3.1 × 10−321).

A similar study using an automatic method for automatic detection of SDB and LM events found

similar or higher correlations between automatic and manual scorings (r2 = 0.85, and r2 = 0.79,

respectively), although their ﬁndings were based on almost 5 times as much data [21]. Furthermore,

obstructive, central, mixed and hypopneas with an associated 4% desaturation were lumped together in

one single apnea class, which may have biased their ﬁndings towards obstructive apneas and hypopneas,

since these are in general more prevalent than central and mixed apneas.

13

ArLMSDB0.00.20.40.60.81.0F1************JointSingle(a)

(b)

(c)

Figure 5: Pearson correlation plots for each event type index between true and predicted values. The linear
relationship is indicated with solid blue with 95% conﬁdence intervals in light blue. Grey dashed lines indicate
perfect correlation lines. ArI: arousal index; AHI: apnea-hypopnea index; LMI: limb movement index.

4.2.2 Temporal characteristics

We compared the temporal precision between manual and automatic event scoring by looking at the

errors in onset (∆onset), oﬀsets (∆oﬀset), and durations (∆dur.) calculated as

∆ onset = onsetautomatic − onsetmanual

∆ oﬀset = oﬀsetautomatic − oﬀsetmanual

∆ dur = durautomatic − durmanual

(19)

(20)

(21)

so that positive values of ∆ onset, ∆ oﬀset corresponds to a positive shift to the right (delayed prediction),

and positive values of ∆ dur. meaning an overestimation of the event duration compared to manual

scoring. This is shown in Figure 6, where the blue distributions are the joint detection model for each

event type, and the orange distributions are the corresponding single-event models. The distributions are

shown as kernel density estimates superimposed on a histogram. For Ar events, the model overestimates

the duration on average by a couple of seconds, which is caused by an earlier prediction of onset and

delayed prediction of termination. For LM events, the model underestimates the duration by about

half a second on average, which is due to earlier prediction of termination. For SDB events, the model

overestimates the duration by about 25 seconds on average, which is caused by an earlier prediction of

onset and delayed prediction of termination. These errors in predicted durations reﬂects the temporal

characteristics of these events; LMs are shorter events (between 0.5 s to 10 s per deﬁnition), and it is thus

unlikely to be overestimated by several seconds, while SDBs are longer events by one to two orders of

magnitude, which also increases the size of the errors. Ars events are intermediate in length compared to

LMs and SDBs, which is reﬂected in the error distributions.

14

020406080100120True ArI020406080100120Pred. ArIr2 = 0.73020406080100120True AHI020406080100120Pred. AHIr2 = 0.770100200300400500True LMI0100200300400500Pred. LMIr2 = 0.78Figure 6: Temporal error metrics distributions across all events and PSGs. Positive values of ∆onset, ∆oﬀset
means delayed predictions, while positive values of ∆dur. means to an overestimation of event duration. Blue
distributions are joint detection models, while orange distributions are the corresponding single-event models.
Distributions are shown as kernel density estimates superimposed on a histogram. Ar: arousal; LM: limb movement;
SDB: sleep disordered breathing.

5 Conclusion

We have presented a novel method for detecting short and long events present in polysomnogram

recordings based on deep neural networks. Our method was able to distinguish between arousals, limb

movements, and sleep-disordered breathing events with F1 scores of 0.70, 0.63, and 0.62, respectively,

and we furthermore found that jointly optimizing a model for all three events performed better than the

respective models optimized for each speciﬁc event type.

Furthermore, clinically relevant event index values derived from the model outputs showed a high

positive correlation with manually computed index values indicating a high degree of agreement between

our model and humans.

15

Δ dur.ArLMSDBΔ onset−1001020Δ offset−2−101−500501000.00.20.40.60.81.0Time, s0.00.20.40.60.81.0References

[1] R. B. Berry, R. Brooks, C. E. Gamaldo, S. M. Harding, C. L. Marcus, and B. V. Vaughn, The

AASM Manual for the Scoring of Sleep and Associated Events: Rules, Terminology and Technical

Speciﬁcations, Version 2.6. Darien, IL, USA: American Academy of Sleep Medicine, 2020.

[2] R. G. Norman, I. Pal, C. Stewart, J. A. Walsleben, and D. M. Rapoport, “Interobserver Agreement

Among Sleep Scorers From Diﬀerent Centers in a Large Dataset,” Sleep, vol. 23, no. 7, pp. 1–8,

2000. doi: 10.1093/sleep/23.7.1e.

[3] H. Danker-Hopfe, D. Kunz, G. Gruber, G. Klösch, J. L. Lorenzo, S. L. Himanen, B. Kemp, T. Penzel,

J. Röschke, H. Dorn, A. Schlögl, E. Trenker, and G. Dorﬀner, “Interrater reliability between scorers

from eight European sleep laboratories in subjects with diﬀerent sleep disorders,” J. Sleep Res.,

vol. 13, pp. 63–69, 2004. doi: 10.1046/j.1365-2869.2003.00375.x.

[4] H. Danker-Hopfe, P. Anderer, J. Zeitlhofer, M. Boeck, H. Dorn, G. Gruber, E. Heller, E. Loretz,

D. Moser, S. Parapatics, B. Saletu, A. Schmidt, and G. Dorﬀner, “Interrater reliability for sleep

scoring according to the Rechtschaﬀen & Kales and the new AASM standard,” J. Sleep Res., vol. 18,

no. 1, pp. 74–84, 2009. doi: 10.1111/j.1365-2869.2008.00700.x.

[5] R. S. Rosenberg and S. Van Hout, “The American Academy of Sleep Medicine Inter-scorer Reliability

Program: Sleep Stage Scoring,” J. Clin. Sleep Med., vol. 9, pp. 81–87, 2013. doi: 10.5664/jcsm.2350.

[6] X. Zhang, X. Dong, J. W. Kantelhardt, J. Li, L. Zhao, C. Garcia, M. Glos, T. Penzel, and F. Han,

“Process and outcome for international reliability in sleep scoring,” Sleep Breath., vol. 19, no. 1,

pp. 191–195, 2015. doi: 10.1007/s11325-014-0990-0.

[7] M. Younes, J. Raneri, and P. Hanly, “Staging sleep in polysomnograms: Analysis of inter-scorer

variability,” J. Clin. Sleep Med., vol. 12, no. 6, pp. 885–894, 2016. doi: 10.5664/jcsm.5894.

[8] M. Younes, S. T. Kuna, A. I. Pack, J. K. Walsh, C. A. Kushida, B. Staley, and G. W. Pien,

“Reliability of the American Academy of Sleep Medicine Rules for Assessing Sleep Depth in Clinical

Practice,” J. Clin. Sleep Med., vol. 14, no. 2, pp. 205–213, 2018. doi: 10.5664/jcsm.6934.

[9] M. J. Drinnan, A. Murray, C. J. Griﬃths, and G. J. Gibson, “Interobserver Variability in Recognizing

Arousal in Respiratory Sleep Disorders,” Am. J. Respir. Crit. Care Med., vol. 158, pp. 358–362,

1998. doi: 10.1164/ajrccm.158.2.9705035.

[10] C. W. Whitney, D. J. Gottlieb, S. Redline, R. G. Norman, R. R. Dodge, E. Shahar, S. Surovec,

and F. J. Nieto, “Reliability of scoring respiratory disturbance indices and sleep staging,” Sleep,

vol. 21, no. 7, pp. 749–757, 1998. doi: 10.1093/sleep/21.7.749.

16

[11] J. S. Loredo, J. L. Clausen, S. Ancoli-Israel, and J. E. Dimsdale, “Night-to-Night Arousal Variability

and Interscorer Reliability of Arousal Measurements,” Sleep, vol. 22, no. 7, pp. 916–920, 1999. doi:

10.1093/sleep/22.7.916.

[12] M. Smurra, M. Dury, G. Aubert, D. Rodenstein, and G. Liistro, “Sleep fragmentation: comparison of

two deﬁnitions of short arousals during sleep in OSAS patients,” Eur. Respir. J., vol. 17, pp. 723–727,

2001. doi: 10.1183/09031936.01.17407230.

[13] R. J. Thomas, “Arousals in Sleep-disordered Breathing: Patterns and Implications,” Sleep, vol. 26,

no. 8, pp. 1042–1047, 2003. doi: 10.1093/sleep/26.8.1042.

[14] M. H. Bonnet, K. Doghramji, T. Roehrs, E. J. Stepanski, S. H. Sheldon, A. S. Walters, M. Wise,

and A. L. Chesson, “The scoring of arousal in sleep: Reliability, validity, and alternatives,” J. Clin.

Sleep Med., vol. 3, no. 2, pp. 133–145, 2007. doi: 10.5664/jcsm.26815.

[15] U. J. Magalang, N.-H. Chen, P. A. Cistulli, A. C. Fedson, T. Gíslason, D. Hillman, T. Penzel,

R. Tamisier, S. Tuﬁk, G. Phillips, and A. I. Pack, “Agreement in the Scoring of Respiratory

Events and Sleep Among International Sleep Centers,” Sleep, vol. 36, no. 4, pp. 591–596, 2013. doi:

10.5665/sleep.2552.

[16] R. S. Rosenberg and S. Van Hout, “The American Academy of Sleep Medicine Inter-scorer Reliability

Program: Respiratory Events,” J. Clin. Sleep Med., vol. 10, no. 4, pp. 447–454, 2014. doi: 10.5664/

jcsm.3630.

[17] H. Koch, J. A. Christensen, R. Frandsen, M. Zoetmulder, L. Arvastson, S. R. Christensen, P.

Jennum, and H. B. Sorensen, “Automatic sleep classiﬁcation using a data-driven topic model reveals

latent sleep states,” J. Neurosci. Methods, vol. 235, pp. 130–137, 2014. doi: 10.1016/j.jneumeth.

2014.07.002.

[18] A. Supratak, H. Dong, C. Wu, and Y. Guo, “DeepSleepNet: A Model for Automatic Sleep Stage

Scoring Based on Raw Single-Channel EEG,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 25,

no. 11, pp. 1998–2008, 2017. doi: 10.1109/TNSRE.2017.2721116.

[19] S. Chambon, M. N. Galtier, P. J. Arnal, G. Wainrib, and A. Gramfort, “A Deep Learning Architecture

for Temporal Sleep Stage Classiﬁcation Using Multivariate and Multimodal Time Series,” IEEE

Trans. Neural Syst. Rehabil. Eng., vol. 26, no. 4, pp. 758–769, 2018. doi: 10.1109/TNSRE.2018.

2813138.

[20] A. N. Olesen, P. Jennum, P. Peppard, E. Mignot, and H. B. D. Sorensen, “Deep residual networks

for automatic sleep stage classiﬁcation of raw polysomnographic waveforms,” in 2018 40th Annu.

Int. Conf. IEEE Eng. Med. Biol. Soc., Honolulu, HI, USA: IEEE, 2018, pp. 1–4. doi: 10.1109/

EMBC.2018.8513080.

17

[21] S. Biswal, H. Sun, B. Goparaju, M. B. Westover, J. Sun, and M. T. Bianchi, “Expert-level sleep

scoring with deep neural networks,” J. Am. Med. Informatics Assoc., vol. 25, no. 12, pp. 1643–1650,

2018. doi: 10.1093/jamia/ocy131.

[22] J. B. Stephansen, A. N. Olesen, M. Olsen, A. Ambati, E. B. Leary, H. E. Moore, O. Carrillo, L. Lin,

F. Han, H. Yan, Y. L. Sun, Y. Dauvilliers, S. Scholz, L. Barateau, B. Hogl, A. Stefani, S. C. Hong,

T. W. Kim, F. Pizza, G. Plazzi, S. Vandi, E. Antelmi, D. Perrin, S. T. Kuna, P. K. Schweitzer,

C. Kushida, P. E. Peppard, H. B. D. Sorensen, P. Jennum, and E. Mignot, “Neural network analysis

of sleep stages enables eﬃcient diagnosis of narcolepsy,” Nat. Commun., vol. 9, no. 1, p. 5229, 2018.

doi: 10.1038/s41467-018-07229-3.

[23] H. Phan, F. Andreotti, N. Cooray, O. Y. Chen, and M. De Vos, “Joint Classiﬁcation and Prediction

CNN Framework for Automatic Sleep Stage Classiﬁcation,” IEEE Trans. Biomed. Eng., vol. 66,

no. 5, pp. 1285–1296, 2019. doi: 10.1109/TBME.2018.2872652.

[24] ——, “SeqSleepNet: End-to-End Hierarchical Recurrent Neural Network for Sequence-to-Sequence

Automatic Sleep Staging,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 27, no. 3, pp. 400–410,

2019. doi: 10.1109/TNSRE.2019.2896659.

[25] A. N. Olesen, S. Chambon, V. Thorey, P. Jennum, E. Mignot, and H. B. D. Sorensen, “Towards

a Flexible Deep Learning Method for Automatic Detection of Clinically Relevant Multi-Modal

Events in the Polysomnogram,” in 2019 41st Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., Berlin,

Germany: IEEE, 2019, pp. 556–561. doi: 10.1109/EMBC.2019.8856570.

[26] D. Alvarez-Estevez and I. Fernández-Varela, “Large-scale validation of an automatic EEG arousal

detection algorithm using diﬀerent heterogeneous databases,” Sleep Med., vol. 57, pp. 6–14, 2019.

doi: 10.1016/j.sleep.2019.01.025.

[27] A. Brink-Kjaer, A. N. Olesen, P. E. Peppard, K. L. Stone, P. Jennum, E. Mignot, and H. B. Sorensen,

“Automatic Detection of Cortical Arousals in Sleep and their Contribution to Daytime Sleepiness,”

Clin. Neurophysiol., 2020. doi: 10.1016/j.clinph.2020.02.027. arXiv: 1906.01700 [q-bio.NC].

[28] L. Carvelli, A. N. Olesen, A. Brink-Kjær, E. B. Leary, P. E. Peppard, E. Mignot, H. B. Sørensen, and

P. Jennum, “Design of a deep learning model for automatic scoring of periodic and non-periodic leg

movements during sleep validated against multiple human experts,” Sleep Med., vol. 69, pp. 109–119,

2020. doi: 10.1016/j.sleep.2019.12.032.

[29] J. B. Blank, P. M. Cawthon, M. L. Carrion-Petersen, L. Harper, J. P. Johnson, E. Mitson, and R. R.

Delay, “Overview of recruitment for the osteoporotic fractures in men study (MrOS),” Contemp.

Clin. Trials, vol. 26, no. 5, pp. 557–568, 2005. doi: 10.1016/j.cct.2005.05.005.

18

[30] E. Orwoll, J. B. Blank, E. Barrett-Connor, J. Cauley, S. Cummings, K. Ensrud, C. Lewis, P. M.

Cawthon, R. Marcus, L. M. Marshall, J. McGowan, K. Phipps, S. Sherman, M. L. Stefanick, and

K. Stone, “Design and baseline characteristics of the osteoporotic fractures in men (MrOS) study —

A large observational study of the determinants of fracture in older men,” Contemp. Clin. Trials,

vol. 26, no. 5, pp. 569–585, 2005. doi: 10.1016/j.cct.2005.05.006.

[31] T. Blackwell, K. Yaﬀe, S. Ancoli-Israel, S. Redline, K. E. Ensrud, M. L. Stefanick, A. Laﬀan,

and K. L. Stone, “Associations Between Sleep Architecture and Sleep-Disordered Breathing and

Cognition in Older Community-Dwelling Men: The Osteoporotic Fractures in Men Sleep Study,” J.

Am. Geriatr. Soc., vol. 59, no. 12, pp. 2217–2225, 2011. doi: 10.1111/j.1532-5415.2011.03731.x.

[32] A. Rechtschaﬀen and A. Kales, Eds., A manual of standardized terminology, techniques and scoring

system for sleep stages of human subjects. Washington, DC: National Institute of Health, 1968.

[33] American Sleep Disorders Association, “EEG arousals: scoring rules and examples: a preliminary

report from the Sleep Disorders Atlas Task Force of the American Sleep Disorders Association,”

Sleep, vol. 15, no. 2, pp. 173–184, 1992, pmid: 11032543.

[34] M. Zucconi, R. Ferri, R. Allen, P. C. Baier, O. Bruni, S. Chokroverty, L. Ferini-Strambi, S. Fulda, D.

Garcia-Borreguero, W. A. Hening, M. Hirshkowitz, B. Högl, M. Hornyak, M. King, P. Montagna, L.

Parrino, G. Plazzi, and M. G. Terzano, “The oﬃcial World Association of Sleep Medicine (WASM)

standards for recording and scoring periodic leg movements in sleep (PLMS) and wakefulness

(PLMW) developed in collaboration with a task force from the International Restless Legs Syndrome

Study Grou,” Sleep Med., vol. 7, no. 2, pp. 175–183, 2006. doi: 10.1016/j.sleep.2006.01.001.

[35] D. A. Dean, A. L. Goldberger, R. Mueller, M. Kim, M. Rueschman, D. Mobley, S. S. Sahoo,

C. P. Jayapandian, L. Cui, M. G. Morrical, S. Surovec, G.-Q. Zhang, and S. Redline, “Scaling Up

Scientiﬁc Discovery in Sleep Medicine: The National Sleep Research Resource,” Sleep, vol. 39, no. 5,

pp. 1151–1164, 2016. doi: 10.5665/sleep.5774.

[36] G.-Q. Zhang, L. Cui, R. Mueller, S. Tao, M. Kim, M. Rueschman, S. Mariani, D. Mobley, and

S. Redline, “The National Sleep Research Resource: towards a sleep data commons,” J. Am. Med.

Informatics Assoc., vol. 25, no. 10, pp. 1351–1358, 2018. doi: 10.1093/jamia/ocy064.

[37] Y. A. LeCun, L. Bottou, G. B. Orr, and K. R. Müller, “Eﬃcient backprop,” in Neural Networks:

Tricks of the Trade, ser. Lect. Notes Comput. Sci. vol 7700, G. Montavon, G. B. Orr, and K.-R.

Müller, Eds., Springer Berlin Heidelberg, 2012. doi: 10.1007/978-3-642-35289-8-3.

[38] V. Nair and G. E. Hinton, “Rectiﬁed Linear Units Improve Restricted Boltzmann Machines,” in

Proc. 27th Int. Conf. Mach. Learn., Haifa, Israel, 2010.

19

[39] S. Ioﬀe and C. Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing

Internal Covariate Shift,” in Proc. 32nd Int. Conf. Mach. Learn., Lille, France: JMLR, 2015. arXiv:

1502.03167 [cs.LG].

[40] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, “On the Properties of Neural Machine

Translation: Encoder–Decoder Approaches,” in Proc. SSST-8, Eighth Work. Syntax. Semant. Struct.

Stat. Transl., Stroudsburg, PA, USA: Association for Computational Linguistics, 2014, pp. 103–111.

doi: 10.3115/v1/W14-4012.

[41] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical Evaluation of Gated Recurrent Neural

Networks on Sequence Modeling,” 2014. arXiv: 1412.3555 [cs.NE].

[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,

“Attention Is All You Need,” in 31st Conf. Neural Inf. Process. Syst. (NIPS 2017), Long Beach,

CA, USA, 2017. arXiv: 1706.03762 [cs.CL].

[43] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional

Transformers for Language Understanding,” 2019. arXiv: 1810.04805v2 [cs.CL].

[44] D. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning to Align

and Translate,” in 3rd Int. Conf. Learn. Represent. (ICLR 2015), San Diego, CA, USA, 2015. arXiv:

1409.0473 [cs.CL].

[45] A. N. Olesen, P. Jennum, E. Mignot, and H. B. D. Sorensen, “Deep transfer learning for improving

single-EEG arousal detection,” 2020.

[46] S. Chambon, V. Thorey, P. J. Arnal, E. Mignot, and A. Gramfort, “A Deep Learning Architecture

to Detect Events in EEG Signals During Sleep,” in 2018 IEEE 28th Int. Work. Mach. Learn. Signal

Process., Aalborg, Denmark: IEEE, 2018, pp. 1–6. doi: 10.1109/MLSP.2018.8517067.

[47] S. Chambon, V. Thorey, P. Arnal, E. Mignot, and A. Gramfort, “DOSED: A deep learning approach

to detect multiple sleep micro-events in EEG signal,” J. Neurosci. Methods, vol. 321, pp. 64–78,

2019. doi: 10.1016/j.jneumeth.2019.03.017.

[48] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, “SSD: Single Shot

MultiBox Detector,” in Comput. Vis. – ECCV 2016, B. Leibe, J. Matas, N. Sebe, and M. Welling,

Eds., Cham, Switzerland: Springer, 2016, pp. 21–37. doi: 10.1007/978-3-319-46448-0_2.

[49] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal Loss for Dense Object Detection,”

IEEE Trans. Pattern Anal. Mach. Intell., vol. 42, no. 2, pp. 318–327, 2020. doi: 10.1109/TPAMI.

2018.2858826.

[50] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,” 2014. arXiv: 1412.6980

[cs.LG].

20

