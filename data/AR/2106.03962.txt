Amortized Generation of Sequential
Algorithmic Recourses for Black-box Models

Sahil Verma,1,2 Keegan Hines,1 John P. Dickerson1,3
1 Arthur AI
2 University of Washington
3 University of Maryland
vsahil@cs.washington.edu, keegan@arthur.ai, john@arthur.ai

1
2
0
2

c
e
D
7
1

]

G
L
.
s
c
[

2
v
2
6
9
3
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

Explainable machine learning (ML) has gained traction in
recent years due to the increasing adoption of ML-based sys-
tems in many sectors. Algorithmic Recourses (ARs) provide
“what if” feedback of the form “if an input datapoint were x(cid:48)
instead of x, then an ML-based system’s output would be y(cid:48)
instead of y.” ARs are attractive due to their actionable feed-
back, amenability to existing legal frameworks, and ﬁdelity
to the underlying ML model. Yet, current AR approaches are
single shot—that is, they assume x can change to x(cid:48) in a single
time period. We propose a novel stochastic-control-based ap-
proach that generates sequential ARs, that is, ARs that allow
x to move stochastically and sequentially across intermediate
states to a ﬁnal state x(cid:48). Our approach is model agnostic and
black box. Furthermore, the calculation of ARs is amortized
such that once trained, it applies to multiple datapoints with-
out the need for re-optimization. In addition to these primary
characteristics, our approach admits optional desiderata such
as adherence to the data manifold, respect for causal relations,
and sparsity—identiﬁed by past research as desirable proper-
ties of ARs. We evaluate our approach using three real-world
datasets and show successful generation of sequential ARs
that respect other recourse desiderata.

1

Introduction

Machine learning (ML) models are increasingly used to
make predictions in systems that directly or indirectly im-
pact humans. This includes critical applications like health-
care (Faggella 2020), ﬁnance (Singla 2020), hiring (Sen-
naar 2019), and parole (Tashea 2017). To understand ML
models better and to promote their equitable impact in
society, it is necessary to assess stakeholders’—both ex-
pert (Holstein et al. 2019) and layperson (Saha et al. 2020)—
comprehension of and needs for general observability into
their systems (Poursabzi et al. 2021; Ehsan et al. 2021a). The
nascent Fairness, Accountability, Transparency, and Ethics
in machine learning (aka “FATE ML”) community conducts
research to develop methods to detect (and counteract) bias in
ML models, develop techniques that make complex models
explainable, and propose policies to advise and adhere to the
regulations of algorithmic decision-making (see Appendix A).
Here, we focus on ML model explainability.

Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Research in explainable ML is bifurcated. One high-level ap-
proach aims to develop inherently interpretable models such
as decision trees and linear models (Rudin 2019). Another
high-level approach aims to utilize existing complex clas-
siﬁcation techniques (such as deep neural networks) but to
bolster them with surrogate models that can render their pre-
dictions and/or internal processes understandable (Adadi and
Berrada 2018). This is achieved through explaining models
holistically (global explanation) or single predictions from
the model (local explanation).
Algorithmic Recourses (ARs). ARs ﬁnd the minimal
change in a datapoint such that the ML model ends up clas-
sifying the new datapoint in the desired class. Such new
datapoint(s) is termed as a counterfactual. (We provide an
in-depth discussion of terminology in Appendix D.) For ex-
ample, if an individual were denied a loan request, a recourse
might tell them that their request would be approved if they
were to increase their income by $2000. ARs provide a pre-
cise recommendation and are therefore more actionable than
other forms of local explainability like feature importance.
Recent research in this area has aimed to ensure ARs are
actionable and useful by incorporating additional desiderata
into the recourse generation problem. As described in Sec-
tion 2, these include notions of sparsity, causality, and real-
ism of ARs, among others. What is needed (see, e.g., Verma,
Dickerson, and Hines 2020; Chou et al. 2021; Karimi et al.
2020b) is a generalized approach that can accommodate such
varied constraints and can also be computed efﬁciently.
Operationalizing ARs. We propose a novel approach
(FASTAR) for generating ARs by translating a given recourse
generation problem into a Markov Decision Process (MDP).
FASTAR aims to learn a policy that can generate ARs for
given data distribution. Upon learning that policy once, it
can generate ARs for multiple datapoints (from that distribu-
tion) without the need to re-optimize (which is required by
most previous approaches; see Appendix B). Thus, FASTAR
amortizes the cost of repeatedly computing ARs. FASTAR
also allows enforcing desirable properties of ARs, such as
closeness to the training data distribution (data manifold), re-
spect of causal relations between the features, and mutability
and actionability of different features. FASTAR works for
black-box models and is therefore model agnostic.
Via the learned policy, FASTAR outputs ARs as a sequence

 
 
 
 
 
 
2 Desiderata of Practical ARs
The overarching goal of an AR is to provide practical guid-
ance to an individual seeking to change their treatment (e.g.,
class label) by a deployed ML model. Apart from the nec-
essary property of a AR having a desired class label, other
desiderata have been identiﬁed in the literature, enumerated
here:
• Actionability: ARs should only recommend changes to
the features that are actionable (Ustun, Spangher, and Liu
2019; Kanamori et al. 2020; Dandl et al. 2020). Actionable
features are dataset and preference dependent.

• Sparsity: Social studies have argued that smaller explana-
tions are more comprehensible to humans (Miller 2019).
Therefore ARs should make changes to a small set of fea-
tures (Van Looveren and Klaise 2020; Karimi et al. 2020a).
• Data manifold: To obey the correlations between features,
their input domain, and to be realistic, ARs should ad-
here to the training data manifold (Dhurandhar et al. 2019;
Kanamori et al. 2020; Dandl et al. 2020).

• Causal constraints: In order to adhere to real-world con-
straints in ARs, causal constraints between features must be
respected. They can encode facts like age cannot decrease
or increase in education level increases age (Mahajan, Tan,
and Sharma 2020).

• Model-agnostic: For wide-spread applicability, AR gener-
ating approaches should be model-agnostic (Laugel et al.
2018; Guidotti et al. 2018a).

• Black-box models: For applicability to proprietary ML mod-
els, AR generating approaches should work for black-box
models (Sharma, Henderson, and Ghosh 2019).

• Amortized: An amortized approach can generate ARs for
several datapoints without optimizing separately for each
of them. Such an approach is effective for deployment (Ma-
hajan, Tan, and Sharma 2020).

FASTAR satisﬁes all the above desiderata. To the best of our
knowledge, it is the ﬁrst approach to do so (see Table 1). The
choice of action space helps produce ARs that consider ac-
tionability among features and are sparse. It only modiﬁes the
actionable features. Its ARs are realistic as they adhere to the
training data manifold and respect causal relations between
features. FASTAR works for black-box models and, therefore,
is model-agnostic. It learns a policy that can produce ARs for
several input datapoints without the need to optimize again;
and, therefore, generates amortized ARs.

3 Examples of Translating ARs to MDPs
We now give two examples of translating an AR problem
into an MDP. Once modeled as an MDP, we can use various
off-the-shelf algorithms (from planning or RL) to learn a
policy to generate ARs.
Example 1: Consider two categorical features a, b ∈
{0, 1, 2}. The combinations of possible values for a and b
form the state space for the MDP (represented by S). The
directed edges in Figure 2a show that upon taking a speciﬁc
action, an agent can move from one state to another, e.g., it
transits from state (0, 1) to (0, 2) by taking the action b+1,
which increments the value of feature b by 1. Actions a+1
and a-1 respectively increase and decrease the value of fea-

Figure 1: Example of Stochastic Algorithmic Recourses. Starting
with a datapoint ((cid:56) denotes undesired class prediction), FASTAR
can stochastically generate ARs that lead to different counterfactual
states ((cid:52) denotes desired class prediction).

of steps that lead an individual to a counterfactual state. To
our knowledge, we are the ﬁrst to leverage techniques from
stochastic control to provide such sequential ARs (Ramakr-
ishnan, Lee, and Albarghouthi 2020; Naumann and Ntoutsi
2021). That sequence can also adhere to particular sparsity
constraints (e.g., only one feature changing per step).
Sequential and “rolled out” ARs have several advantages,
directly addressing gaps identiﬁed by recent survey pa-
pers (Verma, Dickerson, and Hines 2020; Chou et al. 2021;
Karimi et al. 2020b) and workshops (Ehsan et al. 2021b): 1)
action sparsity allows an individual to focus their effort on
changing a small number of features at a time; and 2) presen-
tation of ARs as a set of discrete and sequential steps is closer
to real-world actions, rather than one-step continuous change,
which most previous approaches do. Singh et al. (2021) re-
cently conducted a user-study with 54 participants, wherein
each of them was presented with 15 scenarios and asked if
they preferred one-shot or directed sequential AR in that sce-
nario. When overall results were pulled, the study concluded
a preference for sequential ARs with high conﬁdence.
Figure 1 shows an example of sequential ARs which are
generated for an individual whose loan request was denied
(shown by (cid:56)). Instead of a one-shot solution, FASTAR de-
lineates all intermediate steps to reach a counterfactual state
(shown by (cid:52)). FASTAR also models the stochastic factors
like the duration to complete a BSc degree, no or part-time
job during the course, and the salary variance in the new job
after graduation, which lead to different recourse paths and
hence different counterfactual states (as shown in Figure 1).
In summary, our contributions are:
1. A novel algorithm that translates an AR problem into a
Markov decision process (MDP). To the best of our knowl-
edge, our stochastic-control-based approach is the ﬁrst to
address several roadblocks to using ARs in practice that
have been identiﬁed by the community (Verma, Dickerson,
and Hines 2020; Chou et al. 2021; Karimi et al. 2020b).
2. The ﬁrst approach that generates sequential and amortized

ARs, and also works for black-box models.

3. An extensive evaluation with three real-world datasets and

nine baselines.

  20K    HS    25Cleaner  Income      Edu          Age          Job     0    HS    25  None     0   BSc    29  None   60K   BSc    28  Clerk     5K    HS    25  Sales     5K   BSc    28  Sales   80K   BSc    29  Tech     0   BSc    25  None     5K   BSc    25  Sales     0   BSc    28  None     5K   BSc    29  SalesTable 1: Desiderata comparison of various AR generating approaches. FASTAR is the ﬁrst and only one which satisﬁes all desiderata.

Approach

CFE Expl. (Wachter, Mittelstadt, and Russell 2017)
Recourse (Ustun, Spangher, and Liu 2019)
CEM (Dhurandhar et al. 2019)
MACE (Karimi et al. 2020a)
DACE (Kanamori et al. 2020)
DiCE (Mothilal, Sharma, and Tan 2020)
DiCE VAE (Mahajan, Tan, and Sharma 2020)
Spheres (Laugel et al. 2018)
LORE (Guidotti et al. 2018a)
Weighted (Grath et al. 2018)
CERTIFAI (Sharma, Henderson, and Ghosh 2019)
Prototypes (Van Looveren and Klaise 2020)
MOC (Dandl et al. 2020)
FASTAR

Actionability
(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:51)

Sparsity Agnostic Black-box Amortized Manifold Constraints

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

ing action costs something (negative reward), and reaching
desirable states generate a positive reward. In this MDP tak-
ing any action costs a constant amount of 1 and reaching
the terminal state (φ) gives a reward of +10. The terminal
state (φ) can only be reached via (2,2) (using any action), the
state in green color. All actions in the terminal state lead to
itself with 0 cost. This represents the situation in which a ML
model classiﬁes only (2,2) in the desired class.
The aim is to learn a policy that reaches a terminal state from
any state at the lowest cost (e.g., taking the fewest number of
steps). Cost (or reward) can be discounted in the traditional
way using a discount factor γ ∈ [0, 1). Formally, for this
example with a discrete state space and discrete action space,
our MDP is:
• States = {s ∈ S : {0, 0}, {0, 1}, {0, 2}, {1, 0}, . . . }.
• Actions = {a ∈ A : a+1, a-1, b+1, b-1}.
• Transition function T : S × A → S
• Reward function r : S × A → R.
• Discount factor γ ∈ [0, 1), capturing the tradeoff between

current and future reward.

Our goal is ﬁnding a policy π : S → A that, given a state s ∈
S (an input datapoint), returns an action a ∈ A that represents
the best ﬁrst step to take to reach a new state, hopefully closer
to the ML model’s decision boundary. FASTAR would then
call this precomputed policy repeatedly to ﬁnd an optimal
path to a counterfactual state.

Example 2: Now, consider a more realistic dataset having
3 features: age (denoted by a), education-level (denoted by
b), and race (denoted by r). This is accompanied by real-
world constraints like age and education-level cannot de-
crease, education-level affects age, and race is immutable.
When we increase the education-level (b) by 1, there is a
50% chance that age group (a) will remain the same and
a 50% chance that it will increase by 1. These interactions
between features can be captured by a structural causal model
(SCM), as we discuss in Section 4. The transition function
for the MDP representing the AR problem for this dataset is,
therefore, stochastic.
Deﬁned formally, here are the components for this MDP:
• States = {s ∈ S : {0, 0, 0}, {0, 1, 0}, {0, 2, 1}, . . . }.

(a)

(b)
Figure 2: Transition function for the two examples. Circles show all
the states, and edges show possible transitions. 1) Left-hand-side
shows the transition function for a dataset with two features a and
b, with no restrictions on the values both of them can take within
the input domain. The transition edges are therefore bidirectional.
2) Right-hand-side shows the transition function for a dataset with
three features: age (a), education-level (b), and race (r). The tran-
sition edges are unidirectional as both age and education cannot
decrease. Since race is immutable, there are no actions for r. Since
an increase in education stochastically affects age, the dashed edges
represent a 50% probability of transition.

ture a by 1 (similarly for feature b. These actions constitute
the action space for the MDP (represented by A). The third
component of the MDP is the transition function which is
represented by T : s × a → s (cid:48). This denotes that if an agent
takes action a in state s then it will move to state s(cid:48). This
transition function is deterministic because taking the action
a in state s will always land the agent in the state s(cid:48).
The ﬁnal component of the MDP is the reward function. Tak-

0, 00, 10, 21, 01, 11, 22, 02, 12, 2a±1a±1a±1a±1a±1a±1b±1b±1b±1b±1b±1b±1Φ0,0,ra+1a+1a+1a+1a+1a+1b+1b+1b+1b+1b+1b+1Φ0,1,r0,2,r1,0,r1,1,r1,2,r2,0,r2,1,r2,2,r• Actions = {a ∈ A : a+1, a-1, b+1, b-1}.
• Transition function T : S × A × S (cid:48) → {0,1} s.t. ∀s ∈
s(cid:48)∈S T (S, A, S(cid:48)) = 1.

S, ∀a ∈ A, (cid:80)

• Reward function r : S × A → R.
• Discount factor γ ∈ [0, 1).
Figure 2b shows the transition function for this problem. The
action that increases the education-level (b) now has a prob-
abilistic transition to two destination states, represented by
dashed unidirectional edges. Each transition edge has a 50%
probability of occurrence. Unidirectionality comes from the
fact that education-level cannot decrease. The edges change
feature a are also unidirectional as age cannot decrease. The
reward function is identical to the previous example; option-
ally, it can be changed to accommodate adherence to the data
manifold (Section 2) or having different costs for changing
different features, which we describe in Section 4. Additional
examples can be found in Appendix C.
4 An Algorithmic Approach for Generating

MDPs from AR Problems

We now present a general approach for translating an AR
problem setup into an MDP. Algorithm 1 generates all com-
ponents of an MDP: state space, action space, transition
function, reward function, and additional parameters such as
discount factor. We detail this process below.
State space. Features can be broadly categorized into numer-
ical (Num) and categorical (Cat) kinds. Numerical features
can take real number values within a speciﬁed domain, while
categorical features are mapped to a set of integers. Conse-
quently, the state space S of our MDP (line 1) consists of
the product of the continuous domains for numerical features
(a subset of R|Num|) and product of the integer domains for
categorical features (a subset of Z|Cat|).
Action space. To facilitate capturing actionability (Ustun,
Spangher, and Liu 2019) and causal relationships between
features (Karimi, Sch¨olkopf, and Valera 2020), we further
categorize features as follows:
• Actionable features can be directly changed by an individ-

ual, e.g., income, education level, age.

• Mutable but not actionable features are mutable but cannot
be modiﬁed directly by an individual, e.g., credit score
cannot be directly changed by a person, it changes due to
change in features like income and credit history.

• Immutable features cannot change, e.g., race, birthplace.
The agent is permitted to change only the actionable nu-
merical and categorical features (denoted by NumA and
CatA). Consequently, the action space A is a subset of
R|NumA| × Z|CatA| (line 2). Categorical features are changed
within their discrete domain, while numerical features are
changed within their continuous domain. Line 13 further
enforces the infeasibility of out-of-domain actions.
Transition function. The transition function (line 12) ﬁnds
the modiﬁed state when an action is taken. This function is
inﬂuenced by the structural causal model (SCM), which is an
optional input to Algorithm 1. An SCM consists of a triplet
M = (cid:104)U, V, F(cid:105). U is the set of exogenous features and V is
the set of endogenous features. In terms of a causal graph,

ALGORITHM 1: Generate MDP from an Algorithmic
Recourse Problem

Input

:Training Dataset (D), ML model (f),
Structural Causal Model (SCM), Numerical
actionable features (NumA), Categorical
actionable feature (CatA), Data Manifold
distance function (DistD), Data Manifold
adherence (λ), Desired Label (L), Distance
Function (DistF), Discount Factor (γ)

Output :MDP
// States consist of all numerical (Num) and

categorical (Cat) features.
1 State space S ⊆ R|Num| × Z|Cat|

// Actions change the actionable numerical and

categorical features.

2 Action space A ⊆ R|NumA| × Z|CatA|; denote actions A ∈ A
3 Function

Reward(f, L, CurrState, A, D, λ, DistD, SCM)

4

5

6

7

8

9

10

NextState ← Transition(CurrState, A, SCM)
if argmax(f(NextState)) = L then

CFReward ← Pos // High positive reward

else

CFReward ← f(NextState)[L]

// Probability of classification in the

desired class

return DistF(CurrState, A, D) // action cost
+λ ∗ DistD(NextState, D) // Manifold

distance cost

+CFReward // Counterfactual label reward

11
12 Function Transition(CurrState, A, SCM)

// Action does not violate feature domain and

13

14

15

16

17

18

19

unary constraints

if Allowed (A) & InDomain (A) then

NextState ← CurrState + A // Modify

features

else

return CurrState

// Modify the endogenous features
for V ∈ SCM do

if A ∈ Parent(V) then

NextState[V] ← F(U) // Stochastic or
deterministic update of endogenous

features
return NextState

20
21 MDP ← {S, A, Transition, Reward, γ}

the exogenous features U consist of features that have no
parents, i.e., they can change independently. The endogenous
features V consists of features that have parents in U and/or
other features in V. They change as an effect of change in
their parents. F is the set of functions that determine the
relationship between exogenous and endogenous features.
They are termed as structural equations.
Since knowing the exact SCM is often infeasible, Mahajan,
Tan, and Sharma (2020) overcome this limitation by utliz-
ing constraints from domain knowledge. Algorithm 1 also
accepts such contraints in unary (Un) and binary (Bin) forms.

Even if this does not provide us with the precise functional
form of the constraint, its nature can help the FASTAR’s re-
courses to be realistic. Unary constraints are derived from the
property of one feature, e.g., age and education level cannot
decrease. Binary constraints are derived from the relation
between two features, e.g., if the education level increases,
age increases. If an action does not violate the domain of the
feature it is changing, nor the constraints in the SCM, then
the feature is modiﬁed in NextState (line 14). If the modiﬁed
feature is an exogenous feature, we update its children using
the F functions (line 17-19).
Note that if no SCM is input to the algorithm, that will al-
low transitions from any state to any state (with intermediate
states), and FASTAR would generate ARs using this uncon-
strained transition function.
Reward function. Line 3 deﬁnes a reward function that,
given a state and an action, returns a reward based on three
components derived from the initial AR problem:
• Given the current state (CurrState), action (A), training
dataset (D), and distance function DistF, the ﬁrst part
returns the appropriate cost to take that action (line 9). The
distance function can either be the (cid:96)p norm of the change
produced by the action or a more complex function.

• The second part adds a cost if a datapoint is far from the
training data manifold (line 10) (which is computed using
the DistD function) A λ factor is used to control the
strictness of data manifold adherence.

• The third part rewards the agent with a large positive value
if a counterfactual state is reached (CFReward in line 11).
To avoid sparse rewards, we partially reward the agent with
a small reward equal to the probability of NextState being
classiﬁed in the desired class (line 8). However, the sparse
rewards can only be used if the underlying ML model
provides the class label probabilities instead of only the
class label, e.g., a neural network or random forest.

Other parameters. MDPs require additional parameters
such as the discount factor γ ∈ [0, 1]. At a high level, setting
γ < 1 penalizes longer paths; for additional intuition, see Sut-
ton and Barto (2018). We note that λ, DistD, and DistF are
user-speciﬁed and domain-speciﬁc parameters that directly
impact the reward function for the MDP. We instantiate them
in the evaluation section (see section 5).
5 Evaluation
We provide experimental validation of FASTAR using three
real-world datasets and comparison using nine baselines.
Our research questions (RQ) are motivated by the recourse
desiderata discussed in Section 2, and are enumerated here:
RQ1 Does FASTAR successfully generate ARs for various
input datapoints (validity)?
RQ2 How much change is required to reach a counterfactual
state (proximity)?
RQ3 How many features are changed to reach a counterfac-
tual state (sparsity)?
RQ4 Do the generated ARs adhere to the data manifold
(realisticness)?
RQ5 Do the generated ARs respect causal and feature im-
mutability constraints (feasibility)?

RQ6 How much time does FASTAR take to generate ARs
(amortizability)?
Datasets. Motivated by most previous AR generating ap-
proaches (Verma, Dickerson, and Hines 2020), we use three
datasets in our experiments: German Credit, Adult Income,
and Credit Default (Dua and Graff 2017). These datasets have
20, 13 (omitted education-num as it has one to one map-
ping with education), and 23 features respectively. We split
the datasets into 80%-10%-10% for training, validation, and
testing, respectively. Each dataset has two labels, ‘1’ and ‘0’,
where ‘1’ is the desired label. We trained a simple classiﬁer:
a neural network with two hidden layers (5 and 3 neurons)
with ReLU activations. The test accuracy of the classiﬁer
was 83.0% for German Credit, 83.7% for Adult Income, and
83.2% for Credit Default. Note that the classiﬁer’s accuracy
is relatively less important for FASTAR’s validation.
Implementation Algorithm. Any appropriate method for
computing an optimal policy π∗ : S → A, or any approxi-
mately optimal policy, to the MDP output of Algorithm 1 can
be used. Our MDPhas a continuous state and action space,
and therefore we use a policy gradient algorithm. Speciﬁcally,
we use proximal policy optimization (PPO) with generalized
advantage estimate (GAE) (Schulman et al. 2017; Mnih et al.
2016; Schulman et al. 2018) to train the agent. We justify
this choice and answer several other related questions in Ap-
pendix E. The features in all datasets are scaled between −1
and 1 before training both the ML model and the RL agent.
5.1 Baselines
Since, to our knowledge, FASTAR is the ﬁrst approach to
generate amortized ARs for black-box models, there exist no
previous approaches which we can directly compare against.
Nevertheless, we compare FASTAR to several previous pop-
ular AR generating approaches.
Baselines we developed. To compare FASTAR to ap-
proaches that generated ARs in an amortized manner for
black-box models, we developed two baselines:
• Random: This approach tries to reach a counterfactual
state by executing random actions from the action space.
• Greedy: At each step, this approach greedily chooses the
action (among all actions) which gives the highest reward.
Previous AR generating approaches. Based on the level
of required model access, AR generating approaches can be
categorized as: 1) access to complete model internals, i.e.,
weights of neurons or nodes of decision trees, 2) access to
model gradients (restricted to differentiable models like neu-
ral networks), and 3) access to only the predict function
(black-box). We choose popular methods from all categories:
internal
• Complete model
chose

access. We
MACE (Karimi et al. 2020a) from this category.

• Gradients access. Here we chose DiCE-Gradient
(Mothilal, Sharma, and Tan 2020) and DiCE-VAE (Ma-
hajan, Tan, and Sharma 2020). Notably, DiCE-VAE is the
only other amortized AR generation method, however, it
requires gradients and is restricted to differentiable models.
• Black-box. Open-source repository of the aforementioned
DiCE method also had three black-box and model-agnostic
approaches, namely: DiCE-Genetic, DiCE-KD-Tree, and

Table 2: Causal constraints and immutable features for the datasets.
We assume FASTAR is provided with them.

Dataset

Causal constraints

Immutable features

German
Credit

Adult
Income

Credit
Default

Age and Job cannot decrease

Age and Education cannot de-
crease, increasing Education
increases Age

Age and Education cannot de-
crease, increasing Education
increases Age

Foreign worker, Number of
liable people, Personal sta-
tus, Purpose

Marital-status,
Native-country, Sex

Race,

Sex, Marital status

DiCE-Random. We choose these three and Prototypes
(Van Looveren and Klaise 2020) for this category. We
did not compare with MOC (Dandl et al. 2020) as DiCE-
Genetic it also a genetic algorithm based approach and has
uses Python code.

5.2 Experimental Methodology
Here we describe the speciﬁc details of some approaches:
FASTAR speciﬁcs. As stated in Section 4, the recourses
generated by FASTAR are realistic if provided with the action-
ability of features and causal constraints. These constraints
can be provided using the complete/partial SCM of the data
generating process or using domain knowledge. We assume
these constraints are provided to FASTAR and are shown
in Table 2. As described in Algorithm 1, this directly im-
pacts the transition function. We use a particular instantiation
of Algorithm 1 in the experiments:
• Action space: To produce sequential ARs, actions modify
only one feature at a time. However, endogenous features
may simultaneously change due to change in their parent.
• Cost of action: We treat DistF function as a hyperparameter

and use several values for it in the experiments.

• Data manifold distance: Following previous work (Dandl
et al. 2020; Kanamori et al. 2020), we train k-Nearest
Neighbor (KNN) algorithm on the training dataset and
use it to ﬁnd the (cid:96)1 distance of a given datapoint from its
nearest neighbor (k = 1) in the dataset (DistD). We use
several values of the adherence factor λ in the experiments.
• Counterfactual state reward (CFReward): The agent re-
ceives a reward equal to the probability of its state be-
longing to the desired class (this ranges between 0 and 1).
However, when a counterfactual state is reached, the agent
is rewarded with 100 points.

• Discount Factor: We use a discount factor γ = 0.99. This
value encourages the agent to learn a policy that takes a
few steps to reach a counterfactual state.

We explore the impact of λ hyperparameter in Appendix G,
and give more implementation details in Appendix F.
MACE speciﬁcs. MACE requires as input the type of ML
classiﬁer to be used. We could not use a neural network
because of the MACE’s long runtime (see section 5.3), and
and therefore choose logistic regression (LR) and random
forest (RF), which had a reasonable runtime.
All approaches are requested to generate ARs for the test
datapoints that are predicted as ‘0’ by the classiﬁer. Due to
the small size of the German Credit dataset, we generate ARs
for datapoints that are predicted as ‘0’ both in the training and

test sets. Thus we request ARs for 257 datapoints in the Ger-
man credit, 7229 datapoints in the Adult Income, and 5363
datapoints in the Credit Default datasets. Since MACE uses
a different classiﬁer, the number of datapoints predicted as
‘0’ were slightly different. More details are provided in sec-
tion 5.3. FASTAR, random, and greedy approaches stop when
they reach a counterfactual state (predicted as ‘1’) or exhaust
50 actions. Other baselines have no such timeout.
5.3 Results
Table 3 shows the performance of FASTAR and all the base-
lines on the recourse desiderata. We report the average va-
lidity, average proximity (separately for the numerical and
categorical features), average sparsity, average data mani-
fold distance, average causal constraints adherence, and the
average time to generate the ARs per datapoint.
Answer to RQ1: As shown in Table 3, FASTAR has very
high validity for all datasets. For Adult Income, FASTAR
gets the highest validity at 100%, while for Credit Default
and German Credit, it achieves the second and third highest
validity, respectively. Random and greedy approaches have
low validity in general. DiCE-Genetic has validity in the high
range, but this comes at the cost of proximity, sparsity, and
data manifold distance. DiCE-KDTree is unable to generate
AR even for a single datapoint in all three datasets. DiCE-
Random achieves 100% validity for all datasets, and just like
DiCE-Genetic, this comes at the cost of proximity, sparsity,
and data manifold distance. The conclusion is similar for
DiCE-Gradient’s and Prototypes’ validity. DiCE-VAE’s va-
lidity is lower than 80% for all datasets. MACE also achieves
100% validity but is very expensive to run. Due to this, it
was impractical to run MACE for the larger datasets, Adult
Income and Credit Default (we show MACE run only for the
German Credit dataset). MACE was even more expensive
when the underlying classiﬁer was a neural network, and
we had to abandon that experiment. For the classiﬁers used
for MACE, ‘0’ was predicted for 210 datapoints by logistic
regression (LR) and 287 datapoints by random forest (RF).
MACE was supposed to generate ARs for these datapoints.
Answer to RQ2: We measure proximity for numerical and
categorical features separately (Prox-Num and Prox-Cat, re-
spectively). For numerical features, the distance is the sum
of the (cid:96)1 norm respectively divided by the median average
deviation for each numerical feature. For categorical features,
the distance is the number of categorical features changed
divided by the total number of categorical features. These
metrics were proposed and used in previous works (Mahajan,
Tan, and Sharma 2020). FASTAR’s ARs are most proximal
for Adult Income and Credit Default datasets, and second
best for German Credit. The random approach, Prototypes,
and the ﬁve variants of DiCE have large proximity values.
The greedy approach performs well on this metric, but its
validity is very low. MACE’s performance is about average.
Answer to RQ3: FASTAR achieves the lowest sparsity
among all approaches. Following previous works (Mothilal,
Sharma, and Tan 2020), we measure sparsity at the start
and endpoint of a recourse. Random, Prototypes, DiCE-VAE,
DiCE-Genetic, and DiCE-Gradient’s performance is abysmal.

Table 3: Comparison of FASTAR to all baselines for various AR evaluation metrics. Validity is the percentage an AR is actually classiﬁed
in the desired class. Prox-Num and Prox-Cat refers to the L1 distance of the datapoint and its AR for the numerical and categorical
features respectively. Sparsity is the number of features that were changed to produce the AR. Manifold dist. is the distance of the
AR as returned by the trained kNN algorithm. Constraints refer to the causal constraints adherence by the generated AR. Time is the
average time to generate ARs. For Validity and Constraints, a higher value is better, and for all other columns, a lower value is better.
MACE and DiCE-Gradient could not be run for larger datasets owing to their large computation time.

Dataset Approach

#DataPts. Validity

Prox-Num Prox-Cat

Sparsity Manifold dist. Constraints Time (s)

t
i
d
e
r
C
n
a
m
r
e
G

e
m
o
c
n
I

t
l
u
d
A

t
l
u
a
f
e
D

t
i
d
e
r
C

257
Random
257
Greedy
257
DiCE-Genetic
DiCE-KDTree
257
DiCE-Random 257
207
Prorotypes
257
DiCE-Gradient
257
DiCE-VAE
210
MACE (LR)
287
MACE (RF)
257
FASTAR

7229
Random
7229
Greedy
7229
DiCE-Genetic
7229
DiCE-KDTree
DiCE-Random 7229
500
Prototypes
500
DiCE-Gradient
7229
DiCE-VAE
7229
FASTAR

5363
Random
5363
Greedy
5363
DiCE-Genetic
5363
DiCE-KDTree
DiCE-Random 5363
500
Prototypes
100
DiCE-Gradient
5363
DiCE-VAE
5363
FASTAR

23.7
49.8
98.1
0.0
100.0
100.0
100.0
77.8
100.0
100.0
97.3

80.9
97.7
89.5
0.0
100.0
100.0
84.0
77.1
100.0

12.8
65.1
92.6
0.0
100.0
100.0
81.0
76.4
99.9

0.17
0.07
0.67
N/A
0.33
0.26
0.27
0.80
0.36
0.22
0.10

0.56
0.04
0.71
N/A
0.82
0.29
0.18
0.75
0.04

4.85
0.15
3.93
N/A
5.80
4.9
0.77
1.6
0.01

0.57
0.087
0.26
N/A
0.10
0.58
0.29
0.42
0.017
0.02
0.063

0.77
0.02
0.27
N/A
0.04
0.57
0.012
0.65
0.0

0.68
0.072
0.49
N/A
0.20
0.86
0.40
0.68
0.11

11.33
1.81
6.52
N/A
1.93
13.1
6.33
10.12
1.99
2.64
1.22

10.07
1.18
4.43
N/A
1.64
9.0
2.78
9.99
1.00

14.54
1.25
16.67
N/A
2.33
21.0
15.98
20.1
1.008

1.08
0.48
2.39
N/A
2.40
1.0
2.19
0.97
0.60
0.38
0.72

1.00
0.17
0.46
N/A
1.24
0.68
0.51
0.30
0.18

1.30
0.22
2.75
N/A
3.09
1.24
1.35
0.31
0.32

41.0
100.0
45.6
N/A
93.4
5.3
82.9
5.0
97.1
74.2
100.0

29.0
95.0
23.0
N/A
90.0
22.8
82.4
0.13
100.0

41.5
99.9
27.9
N/A
97.7
0.0
85.2
8.9
100.0

0.31
4.59
1.71
0.17
0.17
25.9
7.10
0.15
38.45
101.29
0.07

0.25
0.27
3.43
0.59
0.22
28.9
59.75
0.12
0.015

0.63
4.67
3.58
0.45
0.39
27.3
479.17
0.18
0.051

This is surprising because DiCE-Gradient has a post-hoc step
speciﬁcally for reducing sparsity. Greedy, MACE, and DiCE-
Random’s performance is about average.
Answer to RQ4: FASTAR achieves low average manifold
distance. It performs second best for Adult Income and Credit
Default and is in the middle for German Credit. The greedy
approach, MACE, and DiCE-VAE also perform well on this
metric. The random approach, Prototypes, and all variants of
DiCE (except DiCE-VAE) perform poorly on this metric.
Answer to RQ5: By construction, FASTAR always respects
causal constraints encoded in its transition function: it has
100% adherence in all datasets. The DiCE based approaches
(except DiCE-VAE), MACE, and random approach take as
input the immutable features, but not the other causal con-
straints and hence do not perform well. DiCE-VAE and Pro-
totypes do not accept immutable features and hence perform
the worst for this metric. The greedy approach performs well
on this metric, even though it does not have a knowledge of
the causal constraints.
Answer to RQ6: The ﬁnal column in Table 3 reports the
average computation time per AR. Owing to amortization,

FASTAR can generate ARs very quickly and takes the low-
est time among all approaches. The next best performers
are DiCE-VAE and DICE-Random. FASTAR is 2× faster
than DiCE-VAE on average (up to 8× faster), 8× faster than
DiCE-random on average (up to 15× faster). DiCE-random
and random approach perform similarly. The difference even
more staggering for DiCE-Genetic, Prototypes, and greedy
approach. MACE and Dice-Gradient were the slowest. FAS-
TAR is about 1000× faster than MACE on average (up to
1447× faster) and 4500× faster than DiCE-Gradient on aver-
age (up to 9400× faster). While amortization allows for the
rapid generation of new ARs, there exists a one-time training
cost. We give details about it in Appendix F.

6 Conclusion
We propose a novel RL-based approach, FASTAR, that gen-
erates amortized and sequential recourses for black-box ML
models. To the best of our knowledge, we are the ﬁrst to pro-
pose such an approach. The ARs generated by FASTAR pos-
sess desirable properties and when evaluated on the recourse
metrics, they perform better than several popular baselines.

References
Adadi, A.; and Berrada, M. 2018. Peeking inside the black-box: A
survey on Explainable Artiﬁcial Intelligence (XAI). IEEE Access,
PP.

Andrews, R.; Diederich, J.; and Tickle, A. B. 1995. Survey and
Critique of Techniques for Extracting Rules from Trained Artiﬁcial
Neural Networks. Know.-Based Syst., 8.

Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schul-
man, J.; Tang, J.; and Zaremba, W. 2016.
OpenAI Gym.
arXiv:arXiv:1606.01540.

Carvalho, D. V.; Pereira, E. M.; and Cardoso, J. S. 2019. Machine
learning interpretability: A survey on methods and metrics. Elec-
tronics, 8.

Chipman, H. A.; George, E. I.; and Mcculloch, R. E. 1998. Making
Sense of a Forest of Trees. In Proceedings of the 30th Symposium
on the Interface.

Chou, Y.-L.; Moreira, C.; Bruza, P.; Ouyang, C.; and Jorge, J.
2021. Counterfactuals and Causability in Explainable Artiﬁcial
Intelligence: Theory, Algorithms, and Applications. arXiv preprint
arXiv:2103.04244.

Craven, M. W.; and Shavlik, J. W. 1995. Extracting Tree-Structured
Representations of Trained Networks. In Conference on Neural
Information Processing Systems (NeurIPS). Cambridge, MA, USA:
MIT Press.

Dandl, S.; Molnar, C.; Binder, M.; and Bischl, B. 2020. Multi-
Objective Counterfactual Explanations. arXiv:2004.11165 [cs, stat].

Deng, H. 2014.
arXiv:1408.5456.

Interpreting Tree Ensembles with inTrees.

Dhurandhar, A.; Chen, P.-Y.; Luss, R.; Tu, C.-C.; Ting, P.; Shan-
mugam, K.; and Das, P. 2018. Explanations Based on the Missing:
Towards Contrastive Explanations with Pertinent Negatives.
In
Conference on Neural Information Processing Systems (NeurIPS).

Dhurandhar, A.; Pedapati, T.; Balakrishnan, A.; Chen, P.-Y.; Shan-
mugam, K.; and Puri, R. 2019. Model Agnostic Contrastive Expla-
nations for Structured Data. arXiv:1906.00117 [cs, stat].

Dhurandhar, A.; and Shanmugam, K. 2020.
tual vs Contrastive Explanations
https://towardsdatascience.com/counterfactual-vs-contrastive-
explanations-in-artiﬁcial-intelligence-e67a9cfc7e4e. Accessed:
2021-05-15.

Counterfac-
Intelligence.

in Artiﬁcial

Domingos, P. 1998. Knowledge Discovery Via Multiple Models.
Intell. Data Anal., 2.

Dua, D.; and Graff, C. 2017. UCI Machine Learning Repository.

Dunkelau, J.; and Leuschel, M. 2019. Fairness-Aware Machine
Learning.

Ehsan, U.; Liao, Q. V.; Muller, M.; Riedl, M. O.; and Weisz, J. D.
2021a. Expanding Explainability: Towards Social Transparency in
AI systems. In CHI.

Ehsan, U.; Wintersberger, P.; Liao, Q. V.; Mara, M.; Streit, M.;
Wachter, S.; Riener, A.; and Riedl, M. O. 2021b. Operationalizing
Human-Centered Perspectives in Explainable AI. In CHI.

Faggella, D. 2020. Machine Learning for Medical Diagnostics
– 4 Current Applications. https://emerj.com/ai-sector-overviews/
machine-learning-medical-diagnostics-4-current-applications/. Ac-
cessed: 2020-10-15.
Ghallab, M.; Nau, D.; and Traverso, P. 2016. Automated Planning
and Acting. USA: Cambridge University Press, 1st edition. ISBN
1107037271.

Grath, R. M.; Costabello, L.; Van, C. L.; Sweeney, P.; Kamiab, F.;
Shen, Z.; and Lecue, F. 2018.
Interpretable Credit Application
Predictions With Counterfactual Explanations. arXiv:1811.05245
[cs].
Guidotti, R.; Monreale, A.; Ruggieri, S.; Pedreschi, D.; Turini, F.;
and Giannotti, F. 2018a. Local Rule-Based Explanations of Black
Box Decision Systems.

Guidotti, R.; Monreale, A.; Ruggieri, S.; Turini, F.; Giannotti, F.;
and Pedreschi, D. 2018b. A Survey of Methods for Explaining
Black Box Models. ACM Comput. Surv., 51.
Henelius, A.; Puolam¨aki, K.; Bostr¨om, H.; Asker, L.; and Papa-
petrou, P. 2014. A Peek into the Black Box: Exploring Classiﬁers
by Randomization. Data Min. Knowl. Discov., 28.
Holstein, K.; Wortman Vaughan, J.; Daum´e III, H.; Dudik, M.; and
Wallach, H. 2019. Improving fairness in machine learning systems:
What do industry practitioners need? In Conference on Human
Factors in Computing Systems (CHI), 1–16.
Joshi, S.; Koyejo, O.; Vijitbenjaronk, W.; Kim, B.; and Ghosh,
J. 2019. Towards Realistic Individual Recourse and Actionable
Explanations in Black-Box Decision Making Systems.

Kanamori, K.; Takagi, T.; Kobayashi, K.; and Arimura, H. 2020.
DACE: Distribution-Aware Counterfactual Explanation by Mixed-
Integer Linear Optimization. In International Joint Conference on
Artiﬁcial Intelligence (IJCAI).
Karimi, A.-H.; Barthe, G.; Balle, B.; and Valera, I. 2020a. Model-
Agnostic Counterfactual Explanations for Consequential Decisions.
In Proceedings of the 23rd International Conference on Artiﬁcial
Intelligence and Statistics.
Karimi, A.-H.; Barthe, G.; Sch¨olkopf, B.; and Valera, I. 2020b. A
survey of algorithmic recourse: deﬁnitions, formulations, solutions,
and prospects. arXiv:2010.04050.
Karimi, A.-H.; Sch¨olkopf, B.; and Valera, I. 2020. Algorith-
mic Recourse: from Counterfactual Explanations to Interventions.
arXiv:2002.06278 [cs, stat].
Karimi, A.-H.; von K¨ugelgen, J.; Sch¨olkopf, B.; and Valera, I. 2020c.
Algorithmic recourse under imperfect causal knowledge: a proba-
bilistic approach.

Keane, M. T.; and Smyth, B. 2020. Good Counterfactuals and
Where to Find Them: A Case-Based Technique for Generating
Counterfactuals for Explainable AI (XAI). arXiv:2005.13997.

Kostrikov, I. 2018. PyTorch Implementations of Reinforcement
Learning Algorithms. https://github.com/ikostrikov/pytorch-a2c-
ppo-acktr-gail.

Krishnan, R.; Sivakumar, G.; and Bhattacharya, P. 1999. Extracting
decision trees from trained neural networks. Pattern Recognition,
32.

Krishnan, S.; and Wu, E. 2017. PALM: Machine Learning Explana-
tions For Iterative Debugging. In Proceedings of the 2nd Workshop
on Human-In-the-Loop Data Analytics.
Lash, M. T.; Lin, Q.; Street, W. N.; Robinson, J. G.; and Ohlmann,
J. W. 2017. Generalized Inverse Classiﬁcation. In SDM.
Laugel, T.; Lesot, M.-J.; Marsala, C.; Renard, X.; and Detyniecki, M.
2018. Comparison-Based Inverse Classiﬁcation for Interpretability
in Machine Learning. In Information Processing and Management
of Uncertainty in Knowledge-Based Systems, Theory and Founda-
tions (IPMU). Springer International Publishing.
Le, T.; Wang, S.; and Lee, D. 2019. GRACE: Generating Concise
and Informative Contrastive Sample to Explain Neural Network
Model’s Prediction. arXiv:1911.02042.

Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.;
and Batra, D. 2017. Grad-CAM: Visual Explanations from Deep
Networks via Gradient-Based Localization. In IEEE International
Conference on Computer Vision.
Sennaar, K. 2019. Machine Learning for Recruiting and Hiring
– 6 Current Applications. https://emerj.com/ai-sector-overviews/
machine-learning-for-recruiting-and-hiring/. Accessed: 2020-10-
15.
Sharma, S.; Henderson, J.; and Ghosh, J. 2019. CERTIFAI: Counter-
factual Explanations for Robustness, Transparency, Interpretability,
and Fairness of Artiﬁcial Intelligence models.
Singh, R.; Dourish, P.; Howe, P.; Miller, T.; Sonenberg, L.;
Velloso, E.; and Vetere, F. 2021. Directive Explanations for
Actionable Explainability in Machine Learning Applications.
arXiv:arXiv:2102.02671.
Singla, S. 2020. Machine Learning to Predict Credit Risk in
Lending Industry. https://www.aitimejournal.com/@saurav.singla/
machine-learning-to-predict-credit-risk-in-lending-industry. Ac-
cessed: 2020-10-15.
Sutton, R. S.; and Barto, A. G. 2018. Reinforcement Learning: An
Introduction. Cambridge, MA, USA: A Bradford Book. ISBN
0262039249.
Tashea, J. 2017. Courts Are Using AI to Sentence Criminals. That
Must Stop Now. https://www.wired.com/2017/04/courts-using-ai-
sentence-criminals-must-stop-now/. Accessed: 2020-10-15.
Tjoa, E.; and Guan, C. 2019. A Survey on Explainable Artiﬁcial
Intelligence (XAI): Towards Medical XAI. arXiv:1907.07374.
Turner, R. 2016. A Model Explanation System: Latest Updates and
Extensions.
Ustun, B.; Spangher, A.; and Liu, Y. 2019. Actionable recourse in
linear classiﬁcation. In Proceedings of the Conference on Fairness,
Accountability, and Transparency (FAccT).
Van Looveren, A.; and Klaise, J. 2020. Interpretable Counterfactual
Explanations Guided by Prototypes.
Verma, S.; Dickerson, J.; and Hines, K. 2020. Counterfactual Ex-
planations for Machine Learning: A Review. arXiv:2010.10596.
Verma, S.; and Rubin, J. 2018. Fairness Deﬁnitions Explained. In
Proceedings of the International Workshop on Software Fairness,
FairWare ’18. New York, NY, USA: Association for Computing
Machinery.
Wachter, S.; Mittelstadt, B.; and Russell, C. 2017. Counterfactual
Explanations Without Opening the Black Box: Automated Deci-
sions and the GDPR. SSRN Electronic Journal.
White, A.; and Garcez, A. d. 2019. Measurable Counterfactual
Local Explanations for Any Classiﬁer. arXiv:1908.03020 [cs].
Zhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; and Torralba, A.
2016. Learning Deep Features for Discriminative Localization. In
CVPR.
Zien, A.; Kr¨amer, N.; Sonnenburg, S.; and R¨atsch, G. 2009. The
Feature Importance Ranking Measure. In Machine Learning and
Knowledge Discovery in Databases. Berlin, Heidelberg: Springer
Berlin Heidelberg.

Liang, E.; Liaw, R.; Nishihara, R.; Moritz, P.; Fox, R.; Goldberg,
K.; Gonzalez, J. E.; Jordan, M. I.; and Stoica, I. 2018. RLlib:
Abstractions for Distributed Reinforcement Learning. In ICML.
Liang, E.; Nishihara, R.; Liaw, R.; and et al. 2020. Ray. https:
//github.com/ray-project/ray.
Mahajan, D.; Tan, C.; and Sharma, A. 2020. Preserving Causal
Constraints in Counterfactual Explanations for Machine Learning
Classiﬁers. arXiv:1912.03277 [cs, stat].
Miller, T. 2019. Explanation in artiﬁcial intelligence: Insights from
the social sciences. Artiﬁcial Intelligence, 267.
Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley,
T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous Methods
In Proceedings of The 33rd
for Deep Reinforcement Learning.
International Conference on Machine Learning. PMLR.
Mothilal, R. K.; Sharma, A.; and Tan, C. 2020. Explaining Machine
Learning Classiﬁers through Diverse Counterfactual Explanations.
In Proceedings of the Conference on Fairness, Accountability, and
Transparency (FAccT), FAT* ’20.
Naumann, P.; and Ntoutsi, E. 2021. Consequence-aware Sequential
Counterfactual Generation. arXiv:2104.05592.
Pawelczyk, M.; Broelemann, K.; and Kasneci, G. 2020. Learn-
ing Model-Agnostic Counterfactual Explanations for Tabular Data.
Proceedings of The Web Conference (WWW).
Pearl, J. 2000. Causality: Models, Reasoning, and Inference. USA:
Cambridge University Press. ISBN 0521773628.
Poulin, B.; Eisner, R.; Szafron, D.; Lu, P.; Greiner, R.; Wishart, D. S.;
Fyshe, A.; Pearcy, B.; MacDonell, C.; and Anvik, J. 2006. Visual
Explanation of Evidence in Additive Classiﬁers. In Conference on
Innovative Applications of Artiﬁcial Intelligence (IAAI).
Poursabzi, F. S.; Goldstein, D. G.; Hofman, J. M.; Wort-
man Vaughan, J. W.; and Wallach, H. 2021. Manipulating and
measuring model interpretability. In CHI.
Poyiadzi, R.; Sokol, K.; Santos-Rodriguez, R.; De Bie, T.; and
Flach, P. 2020. FACE: Feasible and Actionable Counterfactual
Explanations. Conference on Artiﬁcial Intelligence (AAAI).
Ramakrishnan, G.; Lee, Y. C.; and Albarghouthi, A. 2020. Synthesiz-
ing Action Sequences for Modifying Model Decisions. Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, 34.
Rathi, S. 2019. Generating Counterfactual and Contrastive Explana-
tions using SHAP.
Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ”Why Should
I Trust You?”: Explaining the Predictions of Any Classiﬁer.
In
Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’16. New York,
NY, USA: Association for Computing Machinery.
Rudin, C. 2019. Stop explaining black box machine learning mod-
els for high stakes decisions and use interpretable models instead.
Nature Machine Intelligence, 1(5).
Russell, C. 2019. Efﬁcient Search for Diverse Coherent Explana-
tions. In Proceedings of the Conference on Fairness, Accountability,
and Transparency (FAccT), FAT* ’19.
Saha, D.; Schumann, C.; Mcelfresh, D.; Dickerson, J.; Mazurek, M.;
and Tschantz, M. 2020. Measuring non-expert comprehension of
machine learning fairness metrics. In International Conference on
Machine Learning (ICML).
Schulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and Abbeel, P.
2018. High-Dimensional Continuous Control Using Generalized
Advantage Estimation. arXiv:1506.02438.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O.
2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347.

A Background
This section provides background about the social implications of
ML models and techniques to address concerns, along with a brief
introduction to Reinforcement Learning.
A.1 Fairness, Accountability, and Transparency

of AI and ML

Fairness and explainability of an ML model are two major themes
in the broad area of equitable ML learning research.
Fairness research mostly proposes algorithms that learn a model
that does not discriminate against individuals belonging to disad-
vantaged demographic groups. Other possibilities of intervention
lie in modifying the training data itself. Demographic groups are
determined by values of sensitive attributes prescribed by law, e.g.,
race, sex, religion, or the nation of origin. ML models can get biased
against certain demographic groups because of the bias in their
training data, speciﬁcally label bias and selection bias. Label bias
occurs due to manual biased labeling of datapoints belonging to a
demographic group, e.g., if individuals from the black community
were denied loans in the past irrespective of their ability to pay back,
this gets captured in the data from which the model can learn. Selec-
tion bias occurs when speciﬁc subsets of a demographic group are
selected, which captures potentially correlations between the pre-
diction target and a speciﬁc demographic group, e.g., selecting only
defaulters from a demographic group in the training data. More than
20 deﬁnitions of fairness of an ML model have been proposed in
literature (Verma and Rubin 2018). Dunkelau and Leuschel (2019)
summarize some of the signiﬁcant research advances that have been
made in fairness research, and is a comprehensive introductory text
for understanding the categorization and direction of research.
Explainability research can be broadly divided into model expla-
nation and outcome explanation research problems (Guidotti et al.
2018b). The model explanation problem seeks to search for an in-
herently interpretable and transparent model with high ﬁdelity to
the original complex model. Linear models, decision trees, and rule
sets are examples of inherently interpretable models. There exists
techniques to explain complex models like neural networks and tree
ensembles using interpretable surrogate like decision tree (Craven
and Shavlik 1995; Krishnan, Sivakumar, and Bhattacharya 1999;
Chipman, George, and Mcculloch 1998; Domingos 1998) and rule
sets (Deng 2014; Andrews, Diederich, and Tickle 1995). There also
exist approaches that can be applied to black-box models (Henelius
et al. 2014; Krishnan and Wu 2017; Zien et al. 2009).
The outcome explanation problem seeks to ﬁnd, for a single data-
point and prediction from a model, an explanation of why the model
made its prediction. The explanation is either provided in the form
of the importance of each feature in the datapoint, or the form of
example datapoints. The ﬁrst class of methods are called feature
attribution methods and are grouped into model-speciﬁc (Zhou et al.
2016; Selvaraju et al. 2017) and model-agnostic (Poulin et al. 2006;
Ribeiro, Singh, and Guestrin 2016; Turner 2016) kinds. Example-
based approaches return a few datapoints that either have the same
class label as the original datapoint or a different class label. The
motivation for the ﬁrst is to provide a set of datapoints that must
be similar in the input space. The motivation for the second is to
provide a set of datapoints that serves as a target to achieve in case
the individual wants to receive the alternative label. The second set
of datapoints can be referred to as counterfactual explanations.
Counterfactual explanations are applicable to supervised machine
learning where the desired label has not been obtained for a dat-
apoint. Most research in counterfactual explanations assumes a
classiﬁcation setting. Supervised ML setup consists of several la-
beled datapoints, which are inputs to the algorithm, and the aim is

to learn a function mapping from the input datapoints (with say m
features) to labels. In classiﬁcation, the labels are discrete values.
The input space is denoted by X m and the output space is denoted
by Y. The learned function is the mapping f : X m → Y is used to
make predictions. We expound on counterfactual explanations and
their desirable properties in Section 2.
Major beneﬁciaries of explainable machine learning include the
healthcare and ﬁnance sectors, which have a huge social im-
pact (Tjoa and Guan 2019). We point the readers to surveys in
the area of explainable machine learning (Adadi and Berrada 2018;
Carvalho, Pereira, and Cardoso 2019; Guidotti et al. 2018b).
A.2 Reinforcement Learning
Reinforcement Learning (RL) is one of the three broad classes of
machine learning, along with supervised and unsupervised learning.
In RL, the goal is to explore a given environment and to learn a
policy over time that dictates what action should be taken at a given
state. The exploration happens with the help of an agent. Therefore,
a policy is a mapping from a state to an action. When an action
is taken at a state, the environment returns with the new state and
a reward. A good policy aims to maximize the reward over time.
The calculation of the new state is facilitated through the transition
function, whereas the calculation of the reward is done using the
reward function. Naturally, the agent can either learn policies that
are greedy and only focus on immediate reward or learn policies
that focus on reward in the long-term. This trade-off is controlled
by a discount factor called γ, whose value lies between 0 and 1
(inclusive of 0 and 1). States can either be discrete or continuous.
Similarly, actions can also be either discrete or continuous. An RL
problem is expressed in terms of a Markov Decision Process (MDP),
which has ﬁve components. We illustrate each of them using the
game of chess.

• State space S, which are states an agent might explore. In chess,

these are the 64 squares that an agent can move to.

• Action space A, which are the possible actions an agent can
take. These might be restricted based on the current state. In
chess, the actions depend on the game pieces like a king, queen
or pawns, and the given position on the chessboard. The action
space is the union of all possible actions.

• Transition function T which given the current state and action,
ﬁnd the new state that the agent will transition to, e.g., moving
the pawn by 1 unit north end up putting the agent in the state
that is one unit north of its current state. Transition functions
can be deterministic or stochastic (see Section 3).

• Reward function R passes the reward to the agent given the
action, the current state, and the new state. This reward signal is
the main factor that the agent uses to learn a good policy, e.g.,
winning a game would pass a positive reward, and losing the
game would send a negative reward to the agent.

• Discount factor γ is associated with the nature of the problem
at hand. This is used to decide the trade-off between immediate
and long-term rewards.

Many algorithms have been developed to efﬁciently learn an agent,
given the environment like value iteration, policy iteration, policy
gradient, actor-critic methods (Sutton and Barto 2018).

B Related Works
Literature in counterfactual explanations for ML is relatively recent,
with the ﬁrst proposed algorithm in 2017. Wachter, Mittelstadt, and
Russell (2017) proposed ﬁnding counterfactuals as a constrained
optimization problem where the goal is to ﬁnd the minimum change
in the features such that the new datapoint has the desired label.

This approach was gradient-based, did not consider actionability
among features, did not adhere to data manifold or respect causal
relations, and the optimization problem needed to be solved for gen-
erating a CFE for each input datapoint. Other desiderata mentioned
in Section 2 were proposed by other papers: 1) approaches that
generate multiple, diverse counterfactuals for a single input data-
point (Mothilal, Sharma, and Tan 2020; Dandl et al. 2020; Mahajan,
Tan, and Sharma 2020; Karimi et al. 2020a; Sharma, Henderson, and
Ghosh 2019; Russell 2019), 2) approaches that generate counterfac-
tual for black-box models and are model-agnostic (Lash et al. 2017;
Laugel et al. 2018; Guidotti et al. 2018a; Grath et al. 2018; Sharma,
Henderson, and Ghosh 2019; Rathi 2019; White and Garcez 2019;
Poyiadzi et al. 2020; Keane and Smyth 2020; Dandl et al. 2020), 3)
approaches that generate CFEs adhering to data manifold (Dhurand-
har et al. 2018, 2019; Joshi et al. 2019; Van Looveren and Klaise
2020; Mahajan, Tan, and Sharma 2020; Pawelczyk, Broelemann,
and Kasneci 2020; Keane and Smyth 2020; Le, Wang, and Lee 2019;
Dandl et al. 2020; Kanamori et al. 2020), 4) approaches that gener-
ate CFEs that respect causal relations (Mahajan, Tan, and Sharma
2020; Karimi, Sch¨olkopf, and Valera 2020; Karimi et al. 2020c),
5) approaches that generate amortized CFEs (Mahajan, Tan, and
Sharma 2020).
Mahajan, Tan, and Sharma (2020) was the ﬁrst to propose an ap-
proach that can generate multiple CFEs for many datapoints, after
optimizing once, therefore amortized CFEs, but their approach is
gradient-based and therefore works only for differentiable models
and it not black-box. Our approach overcomes this limitation and
generates both amortized and model-agnostic CFEs, which adhere
to data manifold and respect causal relations. Out of the previous
approaches that respect causal relations, only Mahajan, Tan, and
Sharma (2020) works with partial SCM, while others require com-
plete causal graph or complete SCM (Karimi, Sch¨olkopf, and Valera
2020; Karimi et al. 2020c), which are mostly unavailable in the real
world. Our approach also works with a partial SCM.
All the previous works give a single-shot solution for getting to a
counterfactual state from an input datapoint. Our approach over-
comes this limitation by proposing a novel algorithm that generates
sequential CFEs.
Verma, Dickerson, and Hines (2020) and Karimi et al. (2020b)
have collected and summarized recent works in counterfactual ex-
plainability. We point the readers to these surveys for an excellent
in-depth review of the research landscape in this area.

C Illustrative examples
This section gives the remaining examples of translating a CFE
problem into an MDP.
Example 1: Let us now consider the example where one of the
two features is age (denoted by feature a). This adds a constraint
because age cannot decrease. This is captured by the transition
function. In Figure 3 we see that the edges which act on feature a
have now become unidirectional implying that the value of feature
a cannot decrease, action a-1 is not allowed.
Deﬁned formally, here are the components for this MDP:

• States S = {0,0}, {0,1}, {0,2}, {1,0}, . . . .
• Actions A = a+1, b+1, b-1.
• Transition function T : S × A → S
• Reward function R : S × A → R.
• Discount factor γ ∈ [0, 1).

Example 2: Let us now consider a dataset with three features, out
of which one is immutable, e.g., race (denoted by feature r). Feature
a still represents age and carries its non-decreasing constraint. Such

Figure 3: Transition function for a dataset with 2 features, out of
which one is age (denoted by a). Circles show the states and edges
show possible transitions. The edges which denote action on the
feature a are unidirectional as age cannot decrease. Action a-1
taken at any state would loop back to the same state. Each action
has a constant cost of 1.

Figure 4: Transition function for a dataset with 3 features, the ﬁrst
being age (denoted by a) and the third being race (denoted by r).
Circles show all the states and edges show possible transitions.
None of the actions can change the value of feature r as race is
immutable.

a feature cannot be changed using any action, and this is encoded in
the transition function by returning the same state if this action is
taken. The state space in this MDP will consist of 3 values, one for
each feature. Figure 4 shows the transition function for the MDP
representing the CFE problem using this dataset. As we already
saw, a which represents age is non-decreasing. Also, none of the
actions affect the value of feature r, it remains constant (shown by
the constant ‘r’ in the diagram). The reward function is similar to the
ﬁrst example: a constant cost to take any action and a high reward
for reaching the terminal state where the ﬁrst two features are (2,2).
This state follows into a dummy state where any action ends up in
the same dummy state.
Let r take values 0 and 1.
Deﬁned formally, here are the components for this MDP:

• States S = {0,0,0}, {0,1,0}, {0,2,1}, {1,0,0}, . . . .
• Actions A = a+1, b+1, b-1.
• Transition function T : S × A → S
• Reward function R : S × A → R.
• Discount factor γ ∈ [0, 1).

Example 3: In all the examples we visited, there was a constant
cost to taking any action, and all states but one gave a 0 reward on
reaching them. Consider the previous example where the dataset
consisted of 3 features: age, education-level, and race. Some of the

0, 00, 10, 21, 01, 11, 22, 02, 12, 2a+1a+1a+1a+1a+1a+1b±1b±1b±1b±1b±1b±1Φ0,0,ra+1a+1a+1a+1a+1a+1b±1b±1b±1b±1b±1b±1Φ0,1,r0,2,r1,0,r1,1,r1,2,r2,0,r2,1,r2,2,rstates do not appear in the training dataset used to train the classiﬁer
we are trying to generate CFEs for. Ideally, we would prefer to
generate CFEs that are similar to existing data; otherwise, we might
generate unrealistic and unactionable explanations. This is based on
the assumption that training data is a good representation of the true
distribution of features. Some of such states are:

• (0,2,0) and (0,2,1): intuitively this shows that it is unrealistic
for an individual to be in the lowest age group (0) and have the
highest education-level (1). This is true regardless of the person’s
race.

• (2,0,1): it is improbable for someone belonging to the race en-
coded by value 1 to be in the highest age group and have the
lowest possible education-level. Yet (2,0,0) is not an improbable
state, and this might be due to the differences in education level
across different races.

We encode this information in the MDP by modifying its reward
function. If we take an action that ends up in an unrealistic state,
it attracts a penalty of -5 points. The dummy state still carries the
+10 reward, other states reward 0, and there is a constant cost of 1
to take any action. The agent learning in this environment would
ideally learn to avoid the unrealistic states and take actions that go
to the terminal state. In this situation, the agent can learn not to take
a shorter path because it goes through an unrealistic state. We use
a k-Nearest Neighbour algorithm to ﬁnd the appropriate penalty
for landing in any state in our experiments. If a state is close to a
datapoint in the training dataset or occurs in the training dataset
itself, there is a low or no penalty.

Example 4: Reconsider the last example in which there are three
features. The reward function in the last example costed the same for
all features. It might be harder to change one feature than another in
real life, e.g., it might be easier for someone to wait to increase their
age rather than get a higher educational level. This can be accounted
for by posting higher costs to change features harder to change and
vice-versa for feature easier to change.

D Counterfactual vs. Contrastive

explanations

There is ongoing discussion on the exact deﬁnition of counterfactual
explanation, with some researchers advocating to call it contrastive
explanations. Dhurandhar and Shanmugam (2020) have captured
the precise difference in a recent article. They mention that the
counterfactual explanations as introduced by Wachter, Mittelstadt,
and Russell (2017) are almost the same as contrastive explanations.
These explanations seek to ﬁnd the minimal changes to the input
such that the prediction from the ML model changes. On the other
hand, counterfactuals are a function of the datapoint, its predic-
tion, the ML model, and the data generating process that created
that datapoint. Pearl (2000) describes three steps for generating
counterfactuals:

1. Abduction: This is the process of conditioning on the exogenous

variables in the data generation process.

2. Intervention: This is the process of making a sparse change on a

speciﬁc observable variable.

3. Prediction: This is the process of using the exogenous variables
identiﬁed in the ﬁrst step and propagating the intervention to
generate the counterfactual.

We agree with this framing. Therefore, counterfactual explanations
amount to much more perturbing the input datapoint—as in the case
of contrastive explanations, which are tied to the data generating pro-
cess. Indeed, it is our belief that our proposed framework captures
these concerns, if data regarding causal interactions is available.

We take note of this distinction and therefore have adherence to
causal relations as a desiderata of counterfactual explanations (Sec-
tion 2). Structural Causal Models (SCM) consists of the exogenous
and endogenous variables involved in the data generation process.
FASTAR takes as input the SCM (partial SCM is supported) of the
dataset and takes it into consideration while generating CFEs. If the
SCM is not provided, the explanations generated by FASTAR are
basically contrastive explanations.

E Justiﬁcation of the Choice for our

Implementation Algorithm.

In this section, using a set of questions and answers we attempt to
justify our choice of the algorithm used by FASTAR to generate
CFEs.
Ques 1. Why not use planning algorithms?
Ans. Classical planning approaches generate optimal plans of reach-
ing from the current state to the desired goal state. However, they
suffer from several disadvantages compared to learning algorithms,
which we enlist below:

1. Most planning algorithms deal with a ﬁnite and discrete state

and action space.

2. Most planning algorithms, e.g., random shooting, BFS, Dijk-
stra’s, A*, Greedy, etc., need to be run for each start state (data-
point in this case) separately and hence are not amortized (Ghal-
lab, Nau, and Traverso 2016). Our baselines, Random and
Greedy are example of two planning algorithms. Some plan-
ning algorithms like policy iteration are amortized though, and
we discuss that in Ques 3.

3. Most planning algorithms require a goal state to be speciﬁed in
order to ﬁnd an optimal path to reach it. In our case, we do not
know the goal state in advance.

Ques 2. Why not use Monte Carlo Tree Search (MCTS)?
Ans. MCTS takes actions at a state until a leaf node is reached,
and then uses heuristics to expand among a set of leaf nodes. A
value is assigned to each leaf node using a default policy and this
is back-propagated to earlier nodes in the path. This way one can
ﬁnd a path that maximizes the reward. Similar to other planning
algorithms, MCTS needs to be run separately for each datapoint and
is therefore not amortized.
Ques 3. Why not use Policy Iteration (PI)?
Ans. Policy Iteration is an iterative algorithm to ﬁnd the optimal
actions to be taken at each state. As with other planning algorithms,
PI can only work with a ﬁnite and discrete state and action space.
Nevertheless, we used PI to learn the optimal policy for the Ger-
man Credit dataset. Each numerical attribute was discretized into 4
values. Since there are 20 features in this dataset, even after sparse
discretization, it resulted in a total of 141557760000 states. When
the PI algorithm was run with the full state space, it requested more
than 500 GB of memory. When two features were dropped, and the
algorithm was run with the remaining 18 features, it still requested
over 500 GB of memory. Therefore, we performed an experiment
starting with 4 features and the summarized the results in Table 4.
As shown in the table, the time taken to learn the optimal policy
even with the subset of only 9 features is over 24 hours. We killed
the processes with 15 features and higher due to extreme memory
requirement, and the process with 11 features timed-out after 25
hours. Thus, PI is not able to scale for real-world datasets, both in
terms of time and memory requirement. For a smaller subset of 4,
6, and 7 features, the learned optimal policy has 100% validity in
generating CFEs for the datapoints classiﬁed in the negative class
by the ML classiﬁer. We used the same classﬁer as used in the main
experiments (see section 5).

of the best hyperparameters for each dataset. The one-time training
cost for the three datasets using this implementation was about 2
hours for the German Credit, and about 1 hour each for the Adult
Income and Credit default datasets.
We also implemented FASTAR using the recently released RLib
library (Liang et al. 2018), which is build on top of the Ray dis-
tributed framework (Liang et al. 2020). Due to the distributed nature
of this implementation, the training time of the RL agent in this
framework took about 15-20 minutes for all datasets on the same
CPU machine.
For training the PPO algorithm, we experimented with different
starting points for the episodes. In the ﬁrst case, we used a random
starting point sampled from the state space of the MDP. Note that
this starting point might not be in the data manifold as it is sampled
from the whole state space. In the second case, we used random
training datapoints from the dataset as the starting point. It turned
out that starting from the training datapoints led to better learning
by the RL agents, and we stick to this in the implementation. We
hypothesize that the RL agent is able to learn better actions when
it starts from the training data manifold because of the loss that an
agent incurs when it is far from the data manifold. So, for instance,
even if an agent took a correct action at a starting point which is
far off from the manifold, the next state is probably still going to
be far from the manifold incurring a large negative reward. Thus
when starting from off-manifold datapoints, the agent might most
get negative rewards for all actions, and this might lead to it not
learning much.
F.1 Consideration for fair comparison with

baselines.

The greedy baselines requires to have a ﬁnite number of actions to
evaluate and greedily choose the best action. Therefore, the action
space should be discretized. Therefore for a fair comparsion, we
compare FASTAR, random, and the greedy approaches using a
discrete action space.
DiCE-VAE requires several hyperparameters during training the
variantional auto-encoder (VAE), which is used to generate the ARs.
We ran a hyperparameter exploration for all the hyperparameters
and reported the results from the best hyperparameter per dataset
for DiCE-VAE.

G Effects of different hyperparameters on

evaluation metrics

Table 5 shows the effect of increasing the penalty for leaving the
training data manifold, which is enforced at each step of a coun-
terfactual path. With increasing λ, the manifold distance should
become smaller. Increasing λ also makes it harder for the agent
to learn a effective policy, and therefore the validity could also go
down. We observe both these expected trends in Table 5 above,
specially for the German Credit and Credit Default datasets.

Table 4: The time taken and memory consumption for the Policy
Iteration algorithm with increasing number of features of the Ger-
man Credit dataset. The original dataset has 20 features, and when
all the features are discretized it results in a large number of states
(last row in the table). Even when rerun with a subset of 9 features,
the time taken to train learn the optimal policy is prohibitively long.
This show that PI is not scalable for real-world datasets.

# Features

# States

Time Taken(s) Memory Validity

4
6
7
9
11
15
18
20 (original)

320
1280
5120
102400
1638400
393216000
35389440000
141557760000

67
473
2336
86566
–
–
–
–

0.6GB
0.7GB
0.7GB
1.0GB
7.5GB
315GB
500+GB
500+GB

100%
100%
100%
100%
–
–
–
–

Ques 4. Why not use Model Predictive Control (MPC)?

Ans. MPC simulates taking multiple paths upto a ﬁxed horizon. It
choses the path with the minimum cost but takes only one step in
that path and repeats the simulation process to take the next step.
Since it performs the path unrolling at each stage, it is an expensive
algorithm and not amortized.

Ques 5. Why not use LQR?

Ans. LQR, similar to other trajectory optimization methods, is not
amortized. Moreover, we do not know if the cost that is coming
from the black-box ML model is a quadratic function or not, and
hence LQR is not applicable.

Ques 6. Why not use model-based RL?

Ans. We don’t need to learn the model for FASTAR, we deﬁne
it. Most model-based RL methods involve MPC in the planning
step and therefore is not amortized. Model-based acceleration for
model-free RL could have been used for FASTAR’s implementation.
However, we choose model-free approaches for FASTAR owing
to availability of several standardized implementations on Github
and other libraries (Kostrikov 2018; Liang et al. 2020). We leave
model-based acceleration of our approach for future work.

Ques 7. Why not use Q-learning?

Ans. Q-learning is applicable for discrete action and state space
while Policy Gradients are also applicable in continuous state and
action domain. Talking about Deep Q-learning, Policy Gradients
methods have been shown to have better convergence than it.

Ques 8. Why not use vanilla Policy-Gradients?

Ans. Vanilla Policy Gradient algorithm like REINFORCE suffer
from high variance during the Monte Carlo update. Several ap-
proaches have been proposed to tackle this issue and we use one of
such popular approaches PPO+GAE.

F Implementation Details of FASTAR
In this section, we expound on some of the details of how the
PPO+GAE algorithm has been implemented in FASTAR. We used
the open-source implementation (Kostrikov 2018) for the base im-
plementation of the PPO algorithm. This implementation requires
the Reinforcement Learning (RL) environment to be available using
the OpenAI Gym library (Brockman et al. 2016). We, therefore,
created the Gym environment for the datasets used in the experi-
ments. We use the default approximator used in the implementation
for training an RL agent using the PPO algorithm. Both the actor
and critic are approximated by a fully connected neural network
with two hidden layers; each hidden layer has 64 neurons. There
are several hyperparameters in the PPO algorithm that can be tuned.
We ran a moderate-size hyperparameter exploration for each dataset
and selected the best one based on validity. Table 3 shows the results

Table 5: Comparison of the evaluation metrics of ARs for different value of λ hyper-parameter which determines the closeness to the training
data manifold.

Dataset

German Credit

Adult Income

Credit Default

λ

0
0.1
1
10
100

0
0.1
1
10
100

0
0.1
1
10
100

#DataPts. Validity

Prox-Num Prox-Cat

Sparsity Manifold dist. Causality Time (s)

257
257
257
257
257

7229
7229
7229
7229
7229

5363
5363
5363
5363
5363

94.6
97.3
95.3
40.5
42.4

100.0
100.0
100.0
100.0
100.0

99.96
99.96
79.38
47.29
5.74

0.09
0.10
0.11
0.0
0.0

0.04
0.04
0.04
0.04
0.04

0.007
0.015
0.001
0.44
1.22

0.060
0.063
0.059
0.077
0.079

0.0
0.0
0.0
0.0
0.0

0.11
0.11
0.14
0.0
0.0

1.17
1.22
1.20
1.00
1.03

1.00
1.00
1.00
1.00
1.00

1.00
1.00
1.28
1.00
1.00

0.70
0.72
0.71
0.62
0.64

0.18
0.18
0.18
0.18
0.18

0.32
0.32
0.37
0.24
0.60

100.0
100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0
100.0

0.12
0.07
0.07
0.15
0.22

0.028
0.016
0.016
0.016
0.028

0.08
0.05
0.08
0.18
0.32

