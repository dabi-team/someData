NEURAL SOURCE-FILTER-BASED WAVEFORM MODEL FOR STATISTICAL
PARAMETRIC SPEECH SYNTHESIS

Xin Wang1, Shinji Takaki1, Junichi Yamagishi1∗

1National Institute of Informatics, Japan
wangxin@nii.ac.jp, takaki@nii.ac.jp, jyamagis@nii.ac.jp

9
1
0
2

r
p
A
7
2

]
S
A
.
s
s
e
e
[

4
v
6
4
9
1
1
.
0
1
8
1
:
v
i
X
r
a

ABSTRACT

Neural waveform models such as the WaveNet are used in many
recent text-to-speech systems, but the original WaveNet is quite
slow in waveform generation because of its autoregressive (AR)
structure. Although faster non-AR models were recently reported,
they may be prohibitively complicated due to the use of a distilling
training method and the blend of other disparate training criteria.
This study proposes a non-AR neural source-ﬁlter waveform model
that can be directly trained using spectrum-based training criteria
and the stochastic gradient descent method. Given the input acoustic
features, the proposed model ﬁrst uses a source module to generate
a sine-based excitation signal and then uses a ﬁlter module to
transform the excitation signal into the output speech waveform.
Our experiments demonstrated that the proposed model generated
waveforms at least 100 times faster than the AR WaveNet and the
quality of its synthetic speech is close to that of speech generated by
the AR WaveNet. Ablation test results showed that both the sine-
wave excitation signal and the spectrum-based training criteria were
essential to the performance of the proposed model.

Index Terms— speech synthesis, neural network, waveform

modeling

1. INTRODUCTION

Text-to-speech (TTS) synthesis, a technology that converts texts
into speech waveforms, has been advanced by using end-to-end
architectures [1] and neural-network-based waveform models [2, 3,
4]. Among those waveform models, the WaveNet [2] directly models
the distributions of waveform sampling points and has demonstrated
outstanding performance. The vocoder version of WaveNet [5],
which converts the acoustic features into the waveform, also
outperformed other vocoders for the pipeline TTS systems [6].

As an autoregressive (AR) model, the WaveNet is quite slow
in waveform generation because it has to generate the waveform
sampling points one by one. To improve the generation speed,
the Parallel WaveNet [3] and the ClariNet [4] introduce a distilling
method to transfer ‘knowledge’ from a teacher AR WaveNet to
a student non-AR model
the
waveform sampling points. However, the concatenation of two large
models and the mix of distilling and other training criteria reduce the
model interpretability and raise the implementation cost.

that simultaneously generates all

In this paper, we propose a neural source-ﬁlter waveform model
that converts acoustic features into speech waveforms.
Inspired
by classical speech modeling methods [7, 8], we used a source

∗This work was partially supported by JST CREST Grant Number
JPMJCR18A6, Japan and by MEXT KAKENHI Grant Numbers (16H06302,
16K16096, 17H04687, 18H04120, 18H04112, 18KT0051), Japan.

module to generate a sine-based excitation signal with a speciﬁed
fundamental frequency (F0). We then used a dilated-convolution-
based ﬁlter module to transform the sine-based excitation into the
speech waveform. The proposed model was trained by minimizing
spectral amplitude and phase distances, which can be efﬁciently
implemented using discrete Fourier transforms (DFTs). Because the
proposed model is a non-AR model, it generates waveforms much
faster than the AR WaveNet. A large-scale listening test showed
that the proposed model was close to the AR WaveNet in terms of
the Mean opinion score (MOS) on the quality of synthetic speech.
An ablation test showed that both the sine-wave excitation and the
spectral amplitude distance were crucial to the proposed model.

The model structure and training criteria are explained in
Section 2, after which the experiments are described in Section 3.
Finally, this paper is summarized and concluded in Section 4.

2. PROPOSED MODEL AND TRAINING CRITERIA

2.1. Model structure

The proposed model (shown in Figure 1) converts an input acoustic
feature sequence c1:B of length B into a speech waveform (cid:98)o1:T of
length T . It includes a source module that generates an excitation
signal e1:T , a ﬁlter module that transforms e1:T into the speech
waveform, and a condition module that processes the acoustic
features for the source and ﬁlter modules. None of the modules
takes the previously generated waveform sample as the input. The
waveform is assumed to be real-valued, i.e., (cid:98)ot

R, 0 < t

T .

∈

≤

2.1.1. Condition module

{

}

· · ·

c1,

, cB

, where each cb = [fb, s(cid:62)

The condition module takes as input the acoustic feature sequence
b ](cid:62) contains
c1:B =
the F0 fb and the spectral features sb of the b-th speech frame.
The condition module upsamples the F0 by duplicating fb to
every time step within the b-th frame and feeds the upsampled F0
sequence f1:T to the source module. Meanwhile, it processes c1:B
using a bi-directional recurrent layer with long-short-term memory
(LSTM) units [9] and a convolutional (CONV) layer, after which
the processed features are upsampled and sent to the ﬁlter module.
The LSTM and CONV were used so that the condition module was
similar to that of the WaveNet-vocoder [10] in the experiment. They
can be replaced with a feedforward layer in practice.

2.1.2. Source module

Given the input F0 sequence f1:T , the source module generates a
sine-based excitation signal e1:T =
, T
R,

∈
· · ·
. Suppose the F0 value of the t-th time step

, where et

, eT

e1,

1,

{

}

t

∀

∈ {

· · ·

}

 
 
 
 
 
 
Fig. 1. Structure of proposed model. B and T denote lengths of input feature sequence and output waveform, respectively. FF, CONV, and
Bi-LSTM denote feedforward, convolutional, and bi-directional recurrent layers, respectively. DFT denotes discrete Fourier transform.

is ft
the instantaneous frequency [11], a signal e<0>

R≥0, and ft = 0 denotes being unvoiced. By treating ft as
1:T can be generated as

∈

e<0>
t

=





t
(cid:88)

α sin(

k=1

2π

fk
Ns

1
3σ

nt,

+ φ) + nt,

if ft > 0

,

(1)

if ft = 0

[

∈

∼ N

tricks.

π, π] is a random

First, a ‘best’ phase φ∗ for e<0>
1:T

Although we can directly set e1:T = e<0>

(0, σ2) is a Gaussian noise, φ
where nt
initial phase, and Ns is equal to the waveform sampling rate.

−
1:T , we tried two
additional
can be
determined in the training stage by maximizing the correlation
between e<0>
1:T and the natural waveform o1:T . During generation, φ
is randomly generated. The second method is to generate harmonics
by increasing fk in Equation (1) and use a feedforward (FF) layer
to merge the harmonics and e<0>
1:T into e1:T . In this paper we use 7
harmonics and set σ = 0.003 and α = 0.1.

2.1.3. Neural ﬁlter module

Given the excitation signal e1:T from the source module and the
processed acoustic features from the condition module, the ﬁlter
module modulates e1:T using multiple stages of dilated convolution
and afﬁne transformations similar to those in ClariNet [4]. For
the ﬁrst stage takes e1:T and the processed acoustic
example,
features as input and produces two signals a1:T and b1:T using
dilated convolution. The e1:T is then transformed using e1:T
(cid:12)
b1:T + a1:T , where
denotes element-wise multiplication. The
transformed signal is further processed in the following stages, and
the output of the ﬁnal stage is used as generated waveform (cid:98)o1:T .

The dilated convolution blocks are similar to those in Parallel
WaveNet [3]. Speciﬁcally, each block contains multiple dilated
convolution layers with a ﬁlter size of 3. The outputs of the
convolution layers are merged with the features from the condition
module through gated activation functions [3]. After that,
the
merged features are transformed into a1:T and ˜b1:T . To make sure
that b1:T is positive, b1:T is parameterized as b1:T = exp(˜b1:T ).

(cid:12)

Unlike ClariNet or Parallel WaveNet, the proposed model does
It is unnecessary to compute the
not use the distilling method.
mean and standard deviation of the transformed signal. Neither is
it necessary to form the convolution and transformation blocks as an
inverse autoregressive ﬂow [12].

2.2. Training criteria in frequency domain

Because speech perception heavily relies on acoustic cues in the
frequency domain, we deﬁne training criteria that minimize the
spectral amplitude and phase distances, which can be implemented
using DFTs. Given these criteria, the proposed model is trained
using the stochastic gradient descent (SGD) method.

2.2.1. Spectral amplitude distance

Following the convention of short-time Fourier analysis, we conduct
waveform framing and windowing before producing the spectrum
of each frame. For the generated waveform (cid:98)o1:T , we use (cid:98)x(n) =
[(cid:98)x(n)
M to denote the n-th waveform frame of
1 ,
∈
, (cid:98)y(n)
length M . We then use (cid:98)y(n) = [(cid:98)y(n)
K ](cid:62)
K to denote
∈
the spectrum of (cid:98)x(n) calculated using K-point DFT. We similarly
deﬁne x(n) and y(n) for the natural waveform o1:T .

, (cid:98)x(n)

M ](cid:62)

· · ·

· · ·

C

R

,

1

Suppose the waveform is sliced into N frames. Then the log

spectral amplitude distance is deﬁned as follows:

s =

L

1
2

N
(cid:88)

n=1

K
(cid:88)

k=1

(cid:104)

log

Re(y(n)
Re((cid:98)y(n)

k )2 + Im(y(n)
k )2 + Im((cid:98)y(n)
) denote the real and imaginary parts of a

k )2
k )2

(2)

(cid:105)2

,

where Re(
·
complex number, respectively.

) and Im(

·

(cid:98)

L

R

∂Ls

∂Im(

∂Re(

+ j

+ j

y(n))

(n)
(cid:98)
k )
y

Although

∂Ls
y(n)) ∈

s is deﬁned on complex-valued spectra, the gradient
∂Ls
T for SGD training can be efﬁciently calculated. Let
o1:T ∈
∂
us consider the n-th frame and compose a complex-valued vector
K , where the k-th element is
g(n) =
C
∂Re(
g(n)
k = ∂Ls
C. It can be shown that, as long
as g(n) is Hermitian symmetric, the inverse DFT of g(n) is equal
, ∂Ls
to ∂Ls
M 1. Using the same
∂
∂
x
, N
(cid:98)

method, ∂Ls
x(n) for n
can be computed in parallel.
(cid:98)
∂
∂Ls
Given
can be
x(1) ,
∂
(cid:98)
easily accumulated since the relationship between (cid:98)ot and each (cid:98)x(n)
has been determined by the framing and windowing operations.

(cid:98)
, ∂Ls
∂
x
1,
(cid:98)
, the value of each ∂Ls
ot

x(n) = [ ∂Ls

∂Ls
(n)
k ) ∈
(cid:98)
y

∈ {
x(N ) }

in ∂Ls
o1:T
∂

, ∂Ls
∂

(n)
M

(n)
1

· · ·

· · ·

· · ·

∂Im(

(n)
m

R

∈

{

}

m

x

∂

∂

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

,

]

1In the implementation using fast Fourier transform, (cid:98)x(n) of length M
is zero-padded to length K before DFT. Accordingly, the inverse DFT of
g(n) also gives the gradients w.r.t.
the zero-padded part, which should be
discarded (see https://arxiv.org/abs/1810.11946).

Neural	filter	moduleSource	modulebo1:To1:TDFTFraming/windowingDFTFraming/windowinginverseDFTDe-framing/windowingGenerated	waveformCondition	moduleNatural	waveforme1:THarmonicsNoise++FFf1:TUpsamplingDilated	CONVsTransformationUpsamplingBi-LSTMCONVF0Acoustic	featuresSpectral	features	&	F0…Dilated	CONVsTransformationGradientsTraining	criteriaModel	structureLbo1:Tby(n)a1:Tb1:Te1:T b1:T+a1:Tbx(n)y(n)x(n)@L@Re(by(n))+j@L@Im(by(n))Sine	generatorc1:Be<0>1:T@L@bo1:T@L@bx(n)Table 1. Three framing and DFT conﬁgurations for

DFT bins K

s1&
L
512

p1

L

Frame length M 320 (20 ms)
80 (5 ms)

Frame shift

p2

L

s2&
L
128
80 (5 ms)
40 (2.5 ms)

p

L
p3

s and
L
s3&
L
L
2048
1920 (120 ms)
640 (40 ms)

Note: all conﬁgurations use Hann window.

R

In fact,

∂Ls
o1:T ∈
∂

T can be calculated in the same manner
no matter how we set the framing and DFT conﬁguration, i.e., the
ss with different
values of N , M , and K. Furthermore, multiple
∂Ls
conﬁgurations can be computed, and the gradients
can be
o1:T
∂
simply summed up. For example, using the three
ss in Table 1
(cid:98)
was found to be essential to the proposed model (see Section 3.3).

L

L

(cid:98)

The Hermitian symmetry of g(n) is satisﬁed if

s is carefully
deﬁned. For example,
s can be the square error or Kullback-Leibler
divergence (KLD) of the spectral amplitudes [13, 14]. The phase
distance deﬁned below also satisﬁes the requirement.

L

L

2.2.2. Phase distance

Given the spectra, a phase distance [15] is computed as

p =

L

1
2

N
(cid:88)

K
(cid:88)

n=1

k=1

(cid:12)
(cid:12)
(cid:12)1

−

exp(j((cid:98)θ(n)

k −

θ(n)
k ))

(cid:12)
2
(cid:12)
(cid:12)

,

(3)

k

(cid:104)

(cid:105)

1

K
(cid:88)

N
(cid:88)

−

=

k=1

n=1

Re((cid:98)y(n)

k )Im(y(n)
k )

k ) + Im((cid:98)y(n)
k )Re(y(n)
y(n)
|(cid:98)y(n)
k |
k ||
where (cid:98)θ(n)
are the phases of (cid:98)y(n)
k , respectively.
The gradient
can be computed by the same procedure as
∂Ls
ss with different framing and DFT
ps and
o1:T
∂
conﬁgurations can be added up as the ultimate training criterion
∗s, additional DFT/iDFT and framing/windowing

(cid:98)
. For different
L
blocks should be added to the model in Figure 1.

and θ(n)
k
∂Lp
o1:T
∂

. Multiple
(cid:98)

and y(n)

L

L

L

k

3. EXPERIMENTS

3.1. Corpus and features

This study used the same Japanese speech corpus and data division
recipe as our previous study [16]. This corpus [17] contains neutral
reading speech uttered by a female speaker. Both validation and
test sets contain 480 randomly selected utterances. Among the 48-
hour training data, 9,000 randomly selected utterances (15 hours)
were used as the training set in this study. For the ablation test in
Section 3.3, the training set was further reduced to 3,000 utterances
(5 hours). Acoustic features, including 60 dimensions of Mel-
generalized cepstral coefﬁcients (MGCs) [18] and 1 dimension of
F0, were extracted from the 48 kHz waveforms at a frame shift
of 5 ms using WORLD [19]. The natural waveforms were then
downsampled to 16 kHz for model training and the listening test.

3.2. Comparison of proposed model, WaveNet, and WORLD

The ﬁrst experiment compared the four models listed in Table 22.
The WAD model, which was trained in our previous study [6],

2The models were implemented using a modiﬁed CURRENNT toolkit
[20] on a single P100 Nvidia GPU card. Codes, recipes, and generated speech
can be found on https://nii-yamagishilab.github.io.

Table 2. Models for comparison test in Section 3.2

WOR WORLD vocoder
WAD WaveNet-vocoder for 10-bit discrete µ-law waveform
WAC WaveNet-vocoder using Gaussian dist. for raw waveform
NSF

Proposed model for raw waveform

Fig. 2. MOS scores of natural speech, synthetic speech given natural
acoustic features (blue), and synthetic speech given acoustic features
generated from acoustic models (red). White dots are mean values.

Table 3. Average number of waveform points generated in 1 s

WAD
0.19k

NSF (memory-save mode)
20k

NSF (normal mode)
227k

contained a condition module, a post-processing module, and 40
dilated CONV blocks, where the k-th CONV block had a dilation
size of 2modulo(k,10). WAC was similar to WAD but used a Gaussian
distribution to model the raw waveform at the output layer [4].

The proposed NSF contained 5 stages of dilated CONV and
transformation, each stage including 10 convolutional layers with
a dilation size of 2modulo(k,10) and a ﬁlter size of 3.
Its condition
module was the same as that of WAD and WAC. NSF was trained
using
s∗ is
listed in Table 1. The phase distance

s3, and the conﬁguration of each

p∗ was not used in this test.

s2 +

s1 +

=

L

L

L

L

L

Each model generated waveforms using natural and generated
acoustic features, where the generated acoustic features were
produced by the acoustic models in our previous study [6]. The
generated and natural waveforms were then evaluated by paid native
Japanese speakers. In each evaluation round the evaluator listened
to one speech waveform in each screen and rated the speech quality
on a 1-to-5 MOS scale. The evaluator can take at most 10 evaluation
rounds and can replay the sample during evaluation. The waveforms
in an evaluation round were for the same text and were played in
a random order. Note that the waveforms generated from NSF and
WAC were converted to 16-bit PCM format before evaluation.

L

A total of 245 evaluators conducted 1444 valid evaluation
rounds in all, and the results are plotted in Figure 2. Two-sided
Mann-Whitney tests showed that the difference between any pair
of models is statistically signiﬁcant (p < 0.01) except NSF and
WAC when the two models used generated acoustic features.
In
general, NSF outperformed WOR and WAC but performed slightly
worse than WAD. The gap of the mean MOS scores between NSF
and WAD was about 0.12, given either natural or generated acoustic
features. A possible reason for this result may be the difference
between the non-AR and AR model structures, which is similar
to the difference between the ﬁnite and inﬁnite impulse response
ﬁlters. WAC performed worse than WAD because some syllables were
perceived to be trembling in pitch, which may be caused by the
random sampling generation method. WAD alleviated this artifact
by using a one-best generation method in voiced regions [6].

After the MOS test, we compared the waveform generation
speed of NSF and WAD. The implementation of NSF has a normal

NaturalWORWADWACNSF12345Quality(MOS)Fig. 3. Spectrogram (top) and instantaneous frequency (bottom) of natural waveform and waveforms generated from models in Table 4 given
natural acoustic features in test set (utterance AOZORAR 03372 T01). Figures are plotted using 5 ms frame length and 2.5 ms frame shift.

=
=

L
L
s3sssssssss (i.e.,

s3ssssssssss (i.e.,
s2ssssssssss (i.e.,
s2 nor

Table 4. Models for ablation test (Section 3.3)
NSF trained on 5-hour data
NSFs without using
L
NSFs without using
L
NSFs without using
L
NSFs using
s1 +
L
L
L
NSFs using KLD of spectral amplitudes
NSFs without harmonics
NSFs without harmonics or ‘best’ phase φ∗
NSFs only using noise as excitation
NSFs with b1:T = 1 in ﬁlter’s transformation layers
NSFs with b1:T = 0 in ﬁlter’s transformation layers

s1 +
s1 +
=
L
p2 +

L
s2 +

p1 +

s3 +

L
L

=

L

L

L

s2)
s3)
s1)
p3

L
L
L
L

NSFs
L1
L2
L3
L4
L5
S1
S2
S3
N1
N2

generation mode and a memory-save one.
The normal mode
allocates all the required GPU memory once but cannot generate
waveforms longer than 6 seconds because of the insufﬁcient memory
space in a single GPU card. The memory-save mode can generate
long waveforms because it releases and allocates the memory layer
by layer, but the repeated memory operations are time consuming.

We evaluated NSF using both modes on a smaller test set, in
which each of the 80 generated test utterances was around 5 seconds
long. As the results in Table 3 show, NSF is much faster than WAD.
Note that WAD allocates and re-uses a small size of GPU memory,
which needs no repeated memory operation. WAD is slow mainly
because of the AR generation process. Of course, both WAD and NSF
can be improved if our toolkit is further optimized. Particularly, if
the memory operation can be sped up, the memory-save mode of
NSF will be much faster.

3.3. Ablation test on proposed model

This experiment was an ablation test on NSF. Speciﬁcally, the 11
variants of NSF listed in Table 4 were trained using the 5-hour
training set. For a fair comparison, NSF was re-trained using the
5-hour data, and this variant is referred to as NSFs. The speech
waveforms were generated given the natural acoustic features and
rated in 1444 evaluation rounds by the same group of evaluators in
Section 3.2. This test excluded natural waveform for evaluation.

The results are plotted in Figure 4. The difference between
NSTs and any other model except S2 was statistically signiﬁcant
(p < 0.01). Comparison among NSTs, L1, L2, and L3 shows that
ss listed in Table 1 is beneﬁcial. For L3 that used
using multiple
only
s1, the generated waveform points clustered around one peak
in each frame, and the waveform suffered from a pulse-train noise.
This can be observed from L3 of Figure 3, whose spectrogram in the

L

L

Fig. 4. MOS scores of synthetic samples from NSFs and its variants
given natural acoustic features. White dots are mean MOS scores.

high frequency band shows more clearly vertical strips than other
models. Accordingly, this artifact can be alleviated by adding
s2
with a frame length of 5 ms for model training, which explained
the improvement in L1. Using phase distance (L4) didn’t improve
the speech quality even though the value of the phase distance was
consistently decreased on both training and validation data.

L

The good result of S2 indicates that a single sine-wave
excitation with a random initial phase also works. Without the
sine-wave excitation, S3 generated waveforms that were intelligible
but lacked stable harmonic structure. N1 slightly outperformed
NSFs while N2 produced unstable harmonic structures. Because the
transformation in N1 is equivalent to skip-connection [21], the result
indicates that the skip-connection may help the model training.

4. CONCLUSION

In this paper, we proposed a neural waveform model with separated
source and ﬁlter modules. The source module produces a sine-
wave excitation signal with a speciﬁed F0, and the ﬁlter module uses
dilated convolution to transform the excitation into a waveform. Our
experiment demonstrated that the sine-wave excitation was essential
for generating waveforms with harmonic structures. We also found
that multiple spectral-based training criteria and the transformation
in the ﬁlter module contributed to the performance of the proposed
model. Compared with the AR WaveNet,
the proposed model
generated speech with a similar quality at a much faster speed.

The proposed model can be improved in many aspects. For
example, it is possible to simplify the dilated convolution blocks. It
is also possible to try classical speech modeling methods, including
glottal waveform excitations [22, 23], two-bands or multi-bands
approaches [24, 25] on waveforms. When applying the model to
convert linguistic features into the waveform, we observed the over-
smoothing affect in the high-frequency band and will investigate the
issue in the future work.

2k4k8kFrequency(Hz)Natural2k4k8kFrequency(Hz)NSFsL3(NSFsw/oLs2norLs3)L4(NSFsw/Ls1,2,3,Lp1,2,3)S3(NSFsnoiseexcitation)N2(NSFswithb1:T=0)NSFsL1L2L3L4L5S1S2S3N1N212345Quality(MOS)[15] Shinji Takaki, Toru Nakashika, Xin Wang, and Junichi
Yamagishi, “STFT spectral loss for training a neural speech
waveform model,” in Proc. ICASSP, 2019, p. (accepted).

[16] Hieu-Thi Luong, Xin Wang, Junichi Yamagishi, and Nobuyuki
Nishizawa, “Investigating accuracy of pitch-accent annotations
in neural-network-based speech synthesis and denoising
effects,” in Proc. Interspeech, 2018, pp. 37–41.

[17] Hisashi Kawai, Tomoki Toda, Jinfu Ni, Minoru Tsuzaki, and
Keiichi Tokuda, “XIMERA: A new TTS from ATR based on
corpus-based technologies,” in Proc. SSW5, 2004, pp. 179–
184.

[18] Keiichi Tokuda, Takao Kobayashi, Takashi Masuko, and
“Mel-generalized cepstral analysis a uniﬁed

Satoshi Imai,
approach,” in Proc. ICSLP, 1994, pp. 1043–1046.

[19] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa,
“WORLD: A vocoder-based high-quality speech synthesis
IEICE Trans. on
system for
Information and Systems, vol. 99, no. 7, pp. 1877–1884, 2016.

real-time applications,”

[20] Felix Weninger, Johannes Bergmann, and Bj¨orn Schuller,
“Introducing CURRENT: The Munich open-source CUDA
recurrent neural network toolkit,” The Journal of Machine
Learning Research, vol. 16, no. 1, pp. 547–551, 2015.

[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,
in Proc.

“Deep residual learning for image recognition,”
CVPR, 2016, pp. 770–778.

[22] Gunnar Fant, Johan Liljencrants, and Qi-guang Lin, “A four-
parameter model of glottal ﬂow,” STL-QPSR, vol. 4, no. 1985,
pp. 1–13, 1985.

[23] Lauri Juvela, Bajibabu Bollepalli, Manu Airaksinen, and Paavo
Alku, “High-pitched excitation generation for glottal vocoding
in statistical parametric speech synthesis using a deep neural
network,” in Proc. ICASSP, 2016, pp. 5120–5124.

[24] John Makhoul, R Viswanathan, Richard Schwartz, and AWF
Huggins, “A mixed-source model for speech compression and
synthesis,” The Journal of the Acoustical Society of America,
vol. 64, no. 6, pp. 1577–1581, 1978.

[25] D. W. Grifﬁn and J. S. Lim,

“Multiband excitation
vocoder,” IEEE Transactions on Acoustics, Speech, and Signal
Processing, vol. 36, no. 8, pp. 1223–1235, Aug 1988.

5. REFERENCES

[1] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster,
Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,
Yuxuan Wang, Rj Skerrv-Ryan, et al., “Natural TTS synthesis
by conditioning WaveNet on Mel spectrogram predictions,” in
Proc. ICASSP, 2018, pp. 4779–4783.

[2] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen
Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,
Andrew Senior, and Koray Kavukcuoglu,
“WaveNet:
arXiv preprint
A generative model
arXiv:1609.03499, 2016.

raw audio,”

for

[3] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen
Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den
Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg,
Norman Casagrande, Dominik Grewe, Seb Noury, Sander
Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex
Graves, Helen King, Tom Walters, Dan Belov, and Demis
Hassabis,
“Parallel WaveNet: Fast high-ﬁdelity speech
synthesis,” in Proc. ICML, 2018, pp. 3918–3926.

[4] Wei Ping, Kainan Peng, and Jitong Chen, “Clarinet: Parallel
wave generation in end-to-end text-to-speech,” in Proc. ICLP,
2019.

[5] Akira Tamamori, Tomoki Hayashi, Kazuhiro Kobayashi,
Kazuya Takeda, and Tomoki Toda,
“Speaker-dependent
WaveNet vocoder,” in Proc. Interspeech, 2017, pp. 1118–1122.
[6] Xin Wang, Jaime Lorenzo-Trueba, Shinji Takaki, Lauri Juvela,
and Junichi Yamagishi, “A comparison of recent waveform
generation and acoustic modeling methods for neural-network-
based speech synthesis,” in Proc. ICASSP, 2018, pp. 4804–
4808.

[7] Per Hedelin, “A tone oriented voice excited vocoder,” in Proc.

ICASSP. IEEE, 1981, vol. 6, pp. 205–208.

[8] Robert

McAulay
and Thomas Quatieri, “Speech analysis/synthesis based on a
sinusoidal representation,” IEEE Trans. on Acoustics, Speech,
and Signal Processing, vol. 34, no. 4, pp. 744–754, 1986.
[9] Alex Graves, Supervised Sequence Labelling with Recurrent
thesis, Technische Universit¨at

Ph.D.

Neural Networks,
M¨unchen, 2008.

[10] Xin

Wang, Shinji Takaki, and Junichi Yamagishi, “Investigation
of WaveNet for text-to-speech synthesis,” Tech. Rep. 6, SIG
Technical Reports, feb 2018.

[11] John R Carson and Thornton C Fry,

“Variable frequency
electric circuit
theory with application to the theory of
frequency-modulation,” Bell System Technical Journal, vol.
16, no. 4, pp. 513–540, 1937.

[12] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen,
“Improved variational
in Proc. NIPS,

Ilya Sutskever, and Max Welling,
inference with inverse autoregressive ﬂow,”
2016, pp. 4743–4751.

[13] Daniel D Lee and H Sebastian Seung, “Algorithms for non-
negative matrix factorization,” in Proc. NIPS, 2001, pp. 556–
562.

[14] Shinji Takaki, Hirokazu Kameoka, and Junichi Yamagishi,
“Direct modeling of
frequency spectra and waveform
generation based on phase recovery for DNN-based speech
synthesis,” in Proc. Interspeech, 2017, pp. 1128–1132.

A. FORWARD COMPUTATION

Figure 5 plots the two steps to derive the spectrum from the generated waveform (cid:98)o1:T . We use (cid:98)x(n) = [(cid:98)x(n)
1 ,
n-th waveform frame of length M . We then use (cid:98)y(n) = [(cid:98)y(n)
DFT, i.e., (cid:98)y(n) = DFTK ((cid:98)x(n)). For fast Fourier transform, K is set to the power of 2, and (cid:98)x(n) is zero-padded to length K before DFT.

M to denote the
· · ·
K to denote the spectrum of (cid:98)x(n) calculated using K-point
C

, (cid:98)x(n)

, (cid:98)y(n)

K ](cid:62)

M ](cid:62)

· · ·

R

∈

∈

,

1

Fig. 5. Framing/windowing and DFT steps. T , M , K denotes waveform length, frame length, and number of DFT bins.

A.1. Framing and windowing
The framing and windowing operation is also parallelized over (cid:98)x(n)
let’s use the matrix operation in Figure 6. In other words, we compute

m using for each command in CUDA/Thrust. However, for explanation,

(cid:98)x(n)
m =

T
(cid:88)

t=1

(cid:98)otw(n,m)

t

,

(4)

where w(n,m)

t

is the element in the (cid:2)(n

1)

×

−

M + m(cid:3)-th row and the t-th column of the transformation matrix W .

Fig. 6. A matrix format of framing/windowing operation, where w1:M denote coefﬁcients of Hann window.

A.2. DFT

Our implementation uses cuFFT (cufftExecR2C and cufftPlan1d) 3 to compute

y(1),

{

· · ·

, y(N )

}

in parallel.

3https://docs.nvidia.com/cuda/cufft/index.html

bo1:To1:TDFTFraming/windowingDFTFraming/windowingGenerated	waveformNatural	waveformLby(n)bx(n)y(n)x(n)bo1bo2bo3boTbo1:T…bx(1)bx(2)bx(N)N	frames…K	DFT	binsK-pointsDFTby(N)by(1)by(2)Frame	Length	MPadding	K-M0000000000bx(1)1bx(1)2bx(1)MFraming/windowingby(1)1by(1)2by(1)Kby(1)3bx(2)1bx(2)2bx(2)Mbx(N)Mbx(N)1Complex-value	domainReal-value	domainbo1bo2bo3boTbx(1)1bx(1)2bx(1)Mbx(2)1bx(2)2bx(2)MX=bx(N)Mbx(N)1bx(N)2…1stFrame2ndFrameNthFrameT	rowsM	(frame	length)w1w2wM…w1w2wM…FrameshiftNM	columnsw1w2wM……W2RNM⇥TFor back-propagation, we need to compute the gradient

T following the steps plotted in Figure 7.

R

∂L
o1:T ∈

∂

(cid:98)

B. BACKWARD COMPUTATION

B.1. The 2nd step: from ∂L

∂

x

(n)
m

Fig. 7. Steps to compute gradients

to ∂L
ot
∂

Suppose we have

∂L
x(1) ,

,

(cid:98)
· · ·

∂L
(cid:98)
x(N ) }

∂

∂

{

(cid:98)

(cid:98)

, where each ∂L

∂

x(n) ∈

R

M and ∂L

∂

x

(n)
m ∈

R. Then, we can compute ∂L
ot

∂

on the basis Equation (4) as

(cid:98)

∂
L
∂(cid:98)ot

=

N
(cid:88)

M
(cid:88)

n=1

m=1

(cid:98)

∂
L
∂(cid:98)x(n)
m

w(n,m)
t

,

(cid:98)

(5)

where w(n,m)
relationship between (cid:98)ot and each (cid:98)x(n)

t

are the framing/windowing coefﬁcients. This equation explains what we mean by saying ‘ ∂L
ot

∂

can be easily accumulated the

m has been determined by the framing and windowing operations’.
Our implementation uses CUDA/Thrust for each command to launch T threads and compute ∂L
ot

∂

(cid:98)ot is only used in a few frames and there is only one w(n,m)

t

= 0 for each

n, t

{

}

· · ·
, Equation (5) can be optimized as

(cid:98)

, t

(cid:98)
∈ {

1,

in parallel. Because

, T

}

∂
L
∂(cid:98)ot

=

Nt,max
(cid:88)

n=Nt,min

∂
L
∂(cid:98)x(n)
mt,n

w(n,mt,n)

t

,

where [Nt,min, Nt,max] is the frame range that (cid:98)ot appears, and mt,n is the position of (cid:98)ot in the n-th frame.

B.2. The 1st step: compute

∂L
x

(n)
m

∂

Remember that (cid:98)y(n) = [(cid:98)y(n)

1

,

, (cid:98)y(n)

K ](cid:62)

(cid:98)
· · ·

∈

K is the K-points DFT spectrum of (cid:98)x(n) = [(cid:98)x(n)
1 ,

C

, (cid:98)x(n)

M ](cid:62)

R

∈

· · ·

M . Therefore we know

Re((cid:98)y(n)

k ) =

M
(cid:88)

m=1

(cid:98)x(n)
m cos(

2π
K

(k

−

1)(m

1)),

−

Im((cid:98)y(n)

k ) =

M
(cid:88)

−

m=1

(cid:98)x(n)
m sin(

2π
K

(k

−

1)(m

1)),

−

(6)

(7)

(8)

where k
1)) can be safely ignored 4.

∈

[1, K]. Note that, although the sum should be (cid:80)K

m=1, the summation over the zero-padded part (cid:80)K

m=M +1 0 cos( 2π

K (k

1)(m

−

−

4Although we can avoid zero-padding by setting K = M , in practice K is usually the power of 2 while the frame length M is not.

bo1:To1:TDFTFraming/windowingDFTFraming/windowingGenerated	waveformNatural	waveformLby(n)bx(n)y(n)x(n)…N	frames…K	DFT	binsK-pointsiDFTFrame	Length	MDe-framing/windowinginverseDFTDe-framing/windowingGradients@L@bo1:T@L@bx(n)@L@bx(N)@L@bx(2)@L@bx(1)Gradients	w.r.t.	zero-padded	partNot	used	in	de-framing/windowingPadding	K-M@L@bo1:Tg(n)k=@Ls@Re(by(n)k)+j@Ls@Im(by(n)k)2Cg(1)Kg(1)2g(1)1g(1)g(2)g(N)g(n)@L@bx(1)1@L@bx(1)2@L@bx(1)M@L@bx(2)M@L@bx(2)1@L@bx(N)1@L@bx(N)M@L@bo1@L@bo2@L@boComplex-value	domainReal-value	domain(cid:54)
Suppose we compute a log spectral amplitude distance

L
K
(cid:88)

N
(cid:88)

n=1

k=1

over the N frames as

(cid:104)

log

Re(y(n)
Re((cid:98)y(n)

k )2 + Im(y(n)
k )2 + Im((cid:98)y(n)

k )2
k )2

(cid:105)2

.

=

L

1
2

Because

L

, (cid:98)x(n)

m , Re((cid:98)y(n)

k ), and Im((cid:98)y(n)

k ) are real-valued numbers, we can compute the gradient

∂
L
∂(cid:98)x(n)
m

=

=

K
(cid:88)

k=1

K
(cid:88)

k=1

∂
L
∂Re((cid:98)y(n)
k )
∂
L
∂Re((cid:98)y(n)
k )

m

∂Re((cid:98)y(n)
k )
∂(cid:98)x(n)
2π
K

cos(

(k

+

K
(cid:88)

k=1

∂
L
∂Im((cid:98)y(n)
k )

1)(m

1))

−

−

−

m

∂Im((cid:98)y(n)
k )
∂(cid:98)x(n)
∂
L
∂Im((cid:98)y(n)
k )

K
(cid:88)

k=1

sin(

2π
K

(k

−

1)(m

1)).

−

using the chain rule:

∂L
x

(n)
m

∂

(cid:98)

(9)

(10)

(11)

Once we compute

∂L
x

(n)
m

∂

for each m and n, we can use Equation (6) to compute the gradient ∂L
ot

∂

.

B.3. Implementation of the 1st step using inverse DFT

(cid:98)

(cid:98)

Because

∂L
(n)
k )
y

∂Re(

and

∂L
(n)
k )
y

∂Im(

efﬁcient way is to use inverse DFT (iDFT).

are real numbers, we can directly implement Equation (11) using matrix multiplication. However, a more

Suppose a complex valued signal g = [g1, g2,

(cid:98)

(cid:98)

, gL]

∈

· · ·

K , we compute b = [b1,

C

· · ·

, bK ] as the K-point inverse DFT of g by 5

bm =

K
(cid:88)

k=1

gkej 2π

K (k−1)(m−1)

K
(cid:88)

=

[Re(gk) + jIm(gk)][cos(

2π
K

(k

−

1)(m

−

1)) + j sin(

2π
K

(k

−

1)(m

1))]

−

=

k=1

K
(cid:88)

k=1

+ j

Re(gk) cos(

2π
K

(k

−

1)(m

1))

−

−

K
(cid:88)

k=1

Im(gk) sin(

2π
K

(k

(cid:104) K
(cid:88)

k=1

Re(gk) sin(

2π
K

(k

−

1)(m

−

1)) +

K
(cid:88)

k=1

Im(gk) cos(

−

2π
K

1)(m

1))

−

(k

−

1)(m

−

(cid:105)
1))

.

For the ﬁrst term in Line 15, we can write

K
(cid:88)

k=1

Re(gk) sin(

2π
K

(k

−

1)(m

1))

−

=Re(g0) sin(

2π
K

(1

−

1)(m

−

1)) + Re(g K

2 +1) sin(

2π
K

(

K
2

+ 1

+

K

2(cid:88)

k=2

Re(gk) sin(

2π
K

(k

−

1)(m

−

1)) +

K
(cid:88)

k= K

2 +2

Re(gk) sin(

1)(m

1))

−

(k

−

1)(m

1))

−

−

2π
K

=

=

K

2(cid:88)

(cid:104)

k=2

K

2(cid:88)

(cid:104)

k=2

Re(gk) sin(

2π
K

(k

−

1)(m

−

1)) + Re(g(K+2−k)) sin(

2π
K

(K + 2

k

−

−

1)(m

−

(cid:105)

1))

Re(gk)

−

(cid:105)
Re(g(K+2−k))

sin(

2π
K

(k

−

1)(m

1))

−

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

1)(m

−

−

1)) =

1)(m

1)) = Re(g1) sin(0) = 0, and Re(g K

Note that in Line (17), Re(g1) sin( 2π
Re(g K

1)π) = 0.

K (1

−

−

−

2 +1) sin((m
It is easy to those that Line (20) is equal to 0 if Re(gk) = Re(g(K+2−k)), for any k
k=1 Im(gk) cos( 2π

Im(g(K+2−k)), k
terms are equal to 0, the imaginary part in Line (15) will be 0, and bm = (cid:80)K

1)) = 0 if Im(gk) =

[2, K
∈
k=1 gkej 2π

K (k

1)(m

(cid:80)K

−

−

−

[2, K

∈

2 ]. Similarly, it can be shown that
2 +1)) = 0. When these two

2 ] and Im(g1) = Im(g( K
K (k−1)(m−1) in Line (12) will be a real number.

2 +1) sin( 2π

K ( K

2 + 1

5 cuFFT performs un-normalized FFTs, i.e., the scaling factor 1

K is not used.

To summarize, if g satisﬁes the conditions below

inverse DFT of g will be real-valued:

Im(gk) =

−
0,

Re(gk) = Re(g(K+2−k)),
(cid:40)

Im(g(K+2−k)),

[2,

k

∈

K
2

]

k
∈
k =

[2, K
2 ]
1, K
2 + 1
{

}

K
(cid:88)

k=1

gkej 2π

K (k−1)(m−1) =

K
(cid:88)

k=1

Re(gk) cos(

2π
K

(k

−

1)(m

1))

−

−

K
(cid:88)

k=1

Im(gk) sin(

2π
K

(k

−

1)(m

1))

−

(21)

(22)

(23)

This is a basic concept in signal processing: the iDFT of a conjugate-symmetric (Hermitian)6 signal will be a real-valued signal.
∂L
y

We can observer from Equation (23) and (11) that, if

∂L
y

∂L
y

+ j

+ j

∂Im(

∂Re(

∂Im(

(cid:105)(cid:62)

(cid:104)

,

,

)

)

(n)
1

· · ·

∂L
(n)
K )
y

(n)
K )

is conjugate-symmetric,

the gradient vector

∂

∂L

x(n) = [ ∂L

∂

x

(n)
1

,

· · ·

(cid:98)

(cid:98)

, ∂L
x
∂

(n)
M


(cid:98)

∂Re(
](cid:62) be computed using iDFT:

(n)
1

(cid:98)















































= iDFT(






















∂L
x

(n)
1
∂L
(cid:98)
x

(n)
2

∂

∂

(cid:98)
· · ·
∂L
x

∂

(n)
M
∂L
(cid:98)
(n)
x
M +1

∂

(cid:98)
· · ·
∂L
x

∂

(n)
K

(cid:98)

+ j

+ j

)

)

∂L
y

(n)
1
∂L
(cid:98)
(n)
y
2

)

)

∂Im(

∂Im(

∂L
y

(n)
1
∂L
(cid:98)
(n)
y
2

∂Re(

∂Re(

(cid:98)

∂L
(n)
y
M )

∂Re(

∂L
(cid:98)
(n)
M +1)
y

∂Re(

(cid:98)

∂L
(n)
y
K )

∂Re(

· · ·

+ j

+ j

· · ·

+ j

(cid:98)

∂L
(n)
y
M )

∂Im(

∂L
(cid:98)
(n)
M +1)
y

∂Im(

(cid:98)

∂L
(n)
y
K )

∂Im(

(cid:98)

(cid:98)
























).

(24)

Note that

∂L
(n)
M +1

,

{

∂

x

· · ·

, ∂L
x
∂

(n)
K }

are the gradients w.r.t to the zero-padded part, which will not be used and can safely set to 0. The iDFT of
(cid:98)

(cid:98)

(cid:98)

a conjugate symmetric signal can be executed using cuFFT cufftExecC2R command. It is more efﬁcient than other implementations of
Equation (11) because

(cid:98)

(cid:98)

• there is no need to compute the imaginary part;
• there is no need to compute and allocate GPU memory for gk where k
• iDFT can be executed for all the N frames in parallel.

∈

[ K
2 + 2, K] because of the conjugate symmetry;

B.4. Conjugate symmetry complex-valued gradient vector

The conjugate symmetry of

)
common distance metrics can be used.

∂Re(

∂L
y

(n)
1

(cid:104)

+ j

∂L
y

(n)
1

∂Im(

,

)

· · ·

,

∂L
(n)
y
K )

∂Re(

+ j

∂L
(n)
y
K )

∂Im(

(cid:105)(cid:62)

is satisﬁed if

L

is carefully chosen. Luckily, most of the

(cid:98)

(cid:98)

(cid:98)

(cid:98)

B.4.1. Log spectral amplitude distance

Given the log spectral amplitude distance

s in Equation (9), we can compute

L

∂
s
L
∂Re((cid:98)y(n)
k )
∂
s
L
∂Im((cid:98)y(n)
k )

(cid:104)

(cid:104)

=

=

log[Re((cid:98)y(n)

k )2 + Im((cid:98)y(n)

k )2]

−

log[Re(y(n)

k )2 + Im(y(n)

k )2]

log[Re((cid:98)y(n)

k )2 + Im((cid:98)y(n)

k )2]

log[Re(y(n)

k )2 + Im(y(n)

k )2]

Re((cid:98)y(n)
Because (cid:98)y(n) is the DFT spectrum of the real-valued signal, (cid:98)y(n) is conjugate symmetric, and Re((cid:98)y(n)
in Equations (21) and (22), respectively. Because the amplitude Re((cid:98)y(n)

−

(cid:105)

(cid:105)

Re((cid:98)y(n)

2Re((cid:98)y(n)
k )
k )2 + Im((cid:98)y(n)
2Im((cid:98)y(n)
k )
k )2 + Im((cid:98)y(n)
k ) and Im((cid:98)y(n)

k )2

k )2

(25)

(26)

k )2 + Im((cid:98)y(n)
(cid:104)

k )2 does not change the symmetry,

∂Ls
y

(n)
1

+ j

)

∂Ls
y

(n)
1

∂Im(

,

)

· · ·

,

∂Ls
(n)
K )
y

∂Re(

+ j

∂Ls
(cid:98)
(n)
K )
y

∂Im(

∂Re(

k ) satisfy the condition
and

∂Ls
(n)
k )
y

∂Re(

(cid:105)(cid:62)

is

also satisfy the conditions in Equations (21) and (22), respectively, and

∂Ls
(n)
k )
y

∂Im(
conjugate-symmetric.

(cid:98)

6It should be called circular conjugate symmetry in strict sense

(cid:98)

(cid:98)

(cid:98)

(cid:98)

B.4.2. Phase distance
Let (cid:98)θ(n)

and θ(n)

k

k

to be the phases of (cid:98)y(n)

k

and y(n)

k , respectively. Then, the phase distance is deﬁned as

p =

L

=

=

=

=

=

=

N
(cid:88)

K
(cid:88)

(cid:16)

n=1

k=1

N
(cid:88)

K
(cid:88)

(cid:104)

n=1

k=1

N
(cid:88)

K
(cid:88)

(cid:104)

n=1

k=1

1
2

1
2

1
2

1
2

N
(cid:88)

K
(cid:88)

n=1

k=1

N
(cid:88)

K
(cid:88)

n=1

k=1

N
(cid:88)

K
(cid:88)

n=1

k=1

N
(cid:88)

K
(cid:88)

n=1

k=1

(cid:12)
(cid:12)
(cid:12)1

(cid:12)
(cid:12)
(cid:12)1

exp(j((cid:98)θ(n)

k −

−

θ(n)
k ))

(cid:12)
2
(cid:12)
(cid:12)

cos((cid:98)θ(n)

k −

−

θ(n)
k ))

−

j sin((cid:98)θ(n)

k −

2

(cid:12)
θ(n)
(cid:12)
k ))
(cid:12)

(cid:104)(cid:0)1

cos((cid:98)θ(n)

k −

−

k ))(cid:1)2 + sin((cid:98)θ(n)
θ(n)

k −

k ))2(cid:105)
θ(n)

(cid:104)
1 + cos((cid:98)θ(n)

k −

k ))2 + sin((cid:98)θ(n)
θ(n)

k −

θ(n)
k ))2

2 cos((cid:98)θ(n)

k −

−

(cid:105)
θ(n)
k ))

1

−

cos((cid:98)θ(n)

k −

θ(n)
k ))

(cid:17)

1

1

(cid:0) cos((cid:98)θ(n)

k ) cos(θ(n)

k )) + sin((cid:98)θ(n)

k ) sin(θ(n)

k ))(cid:1)(cid:105)

−

(cid:113)

−

Re((cid:98)y(n)
k )Re(y(n)
k )2 + Im((cid:98)y(n)

k ) + Im((cid:98)y(n)
(cid:113)
Re(y(n)
k )2

k )Im(y(n)
k )
k )2 + Im(y(n)

Re((cid:98)y(n)

k )2

(cid:105)
,

where

cos((cid:98)θ(n)

k ) =

sin((cid:98)θ(n)

k ) =

(cid:113)

(cid:113)

k )2

Re((cid:98)y(n)
k )
k )2 + Im((cid:98)y(n)
Im((cid:98)y(n)
k )
k )2 + Im((cid:98)y(n)

k )2

Re((cid:98)y(n)

Re((cid:98)y(n)

,

,

cos(θ(n)

k ) =

sin(θ(n)

k ) =

Re(y(n)
k )
k )2 + Im(y(n)
Im(y(n)
k )
k )2 + Im(y(n)

k )2

k )2

.

(cid:113)

Re(y(n)

(cid:113)

Re(y(n)

Therefore, we get

∂
p
L
∂Re((cid:98)y(n)
k )

cos(θ(n)
k )

=

−

cos(θ(n)
k )

=

−

cos(θ(n)
k )

=

−

sin(θ(n)
k )

∂cos((cid:98)θ(n)
k )
∂Re((cid:98)y(n)
k ) −
(cid:113)
k )2 + Im((cid:98)y(n)
Re((cid:98)y(n)

k )2

∂sin((cid:98)θ(n)
k )
∂Re((cid:98)y(n)
k )
Re((cid:98)y(n)
k )2 + Im((cid:98)y(n)

k ) 1

−

2

(cid:113)
k )2

Re((cid:98)y(n)

2Re(

(n)
k )
y
(n)
(n)
k )2
k )2+Im(
y
(cid:98)

Re(

y

(cid:98)

(cid:98)

sin(θ(n)
k )

−

Im((cid:98)y(n)
−

k ) 1

2

y

2Re(

(n)
k )
(n)
k )2+Im(
y
(cid:98)
k )2 + Im((cid:98)y(n)
k )2
(cid:98)

Re(

(cid:113)

(cid:98)

Re((cid:98)y(n)

(n)
k )2
y

sin(θ(n)
k )

Im((cid:98)y(n)
k )Re((cid:98)y(n)
k )
−
k )2(cid:1) 3
(cid:0)Re((cid:98)y(n)
k )2 + Im((cid:98)y(n)
k )Im((cid:98)y(n)
Im(y(n)
k )2
k )2(cid:1) 3
k )2 + Im((cid:98)y(n)

−

2

2

k )Re((cid:98)y(n)

Re(y(n)
2 (cid:0)Re((cid:98)y(n)

k )Re((cid:98)y(n)
k )

Re(y(n)

k )Re((cid:98)y(n)

−

Re((cid:98)y(n)

Re((cid:98)y(n)

k )2

2

k )2 + Im((cid:98)y(n)
(cid:0)Re((cid:98)y(n)
k )2 + Re(y(n)
(cid:0)Re(y(n)

k )2
−
k )2(cid:1) 3
k )2 + Im((cid:98)y(n)
k )Im((cid:98)y(n)
k )2
k )2 + Im(y(n)
k )Im((cid:98)y(n)
Im(y(n)
k )
−
k )2(cid:1) 1
2 (cid:0)Re((cid:98)y(n)

Re(y(n)
k )2 + Im(y(n)
∂cos((cid:98)θ(n)
k )
∂Im((cid:98)y(n)
k ) −
k )Re((cid:98)y(n)
Im(y(n)
Re(y(n)
k )
−
k )2(cid:1) 1
2 (cid:0)Re((cid:98)y(n)
k )2 + Im(y(n)

sin(θ(n)
k )

−
k )2(cid:1) 1
k )Re((cid:98)y(n)
k )
k )2 + Im((cid:98)y(n)
∂sin((cid:98)θ(n)
k )
∂Im((cid:98)y(n)
k )
k )Im((cid:98)y(n)
k )
k )2 + Im((cid:98)y(n)

(cid:0)Re(y(n)

cos(θ(n)
k )

(cid:0)Re(y(n)

k )2(cid:1) 3

2

k )2(cid:1) 3

2

=

−

=

−

=

−

=

−

∂
p
L
∂Im((cid:98)y(n)
k )

Im((cid:98)y(n)
k )

Re((cid:98)y(n)
k ).

Because both y(n) and (cid:98)y(n) are conjugate-symmetric, it can be easily observed that

Equations (21) and (22), respectively.

∂Lp
(n)
k )
y

∂Re(

and

∂Lp
y

∂Re(

(n)
k )

satisfy the condition in

(cid:98)

(cid:98)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

Different distance metrics can be merged easily. For example, we can deﬁne

C. MULTIPLE DISTANCE METRICS

=

L

s1 +

L

· · ·

+

sS +

L

p1 +

L

+

pP ,

L

· · ·

(43)

R and

where
s∗
vector ∂L∗
o1:T ∈
The gradients then will be simply merged together as

L
x(n) may be different, the gradient ∂L∗
∂

L

∈

∈

p∗

∂

R

R may use different numbers of DFT bins, frame length, or frame shift. Although the dimension of the gradient
T will always be a real-valued vector of dimension T after de-framing/windowing.

(cid:98)

(cid:98)

=

∂
L
∂ (cid:98)o1:T

s1

∂
L
∂ (cid:98)o1:T

+

+

· · ·

sS

∂
L
∂ (cid:98)o1:T

+

p1

∂
L
∂ (cid:98)o1:T

+

+

· · ·

pP

∂
L
∂ (cid:98)o1:T

.

(44)

Fig. 8. Using multiple distances

1,

{L

· · ·

,

L

L

}

bo1:TDFTconfig1Framing/windowingconfig1DFTconfig1Framing/windowingconfig1Natural	waveformiDFTconfig1De-framing/windowingconfig1@L1@bo1:TL1DFTconfig2Framing/windowingconfig2DFTconfig2Framing/windowingconfig2o1:TGenerated	waveformiDFTconfig2De-framing/windowingconfig2L2…DFTconfigLFraming/windowingconfigLDFT	configLFraming/windowingconfigLiDFTconfigLDe-framing/windowingconfigL@L2@bo1:TLL@L@bo1:T@LL@bo1:T+