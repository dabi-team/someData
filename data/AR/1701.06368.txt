An Upper Bound to Zero-Delay Rate Distortion via
Kalman Filtering for Vector Gaussian Sources

Photios A. Stavrou∗, Jan Østergaard∗, Charalambos D. Charalambous†, Milan Derpich‡
∗Department of Electronic Systems, Aalborg University, Denmark
†Department of Electrical and Computer Engineering, University of Cyprus, Cyprus
‡ Department of Electronic Engineering, Universidad T´ecnica Federico Santa Mar´ıa, Chile.
Emails:{fos, jo}@es.aau.dk, chadcha@uc.ac.cy, milan.derpich@usm.cl

7
1
0
2

g
u
A
1
2

]
T
I
.
s
c
[

3
v
8
6
3
6
0
.
1
0
7
1
:
v
i
X
r
a

to model

Abstract—We deal with zero-delay source coding of a vector
Gaussian autoregressive (AR) source subject to an average mean
squared error (MSE) ﬁdelity criterion. Toward this end, we con-
sider the nonanticipative rate distortion function (NRDF) which
is a lower bound to the causal and zero-delay rate distortion
function (RDF). We use the realization scheme with feedback
proposed in [1]
the corresponding optimal “test-
channel” of the NRDF, when considering vector Gaussian AR(1)
sources subject to an average MSE distortion. We give conditions
on the vector Gaussian AR(1) source to ensure asymptotic
stationarity of the realization scheme (bounded performance).
Then, we encode the vector innovations due to Kalman ﬁltering
via lattice quantization with subtractive dither and memoryless
entropy coding. This coding scheme provides a tight upper bound
to the zero-delay Gaussian RDF. We extend this result to vector
Gaussian AR sources of any ﬁnite order. Further, we show
that for inﬁnite dimensional vector Gaussian AR sources of any
ﬁnite order, the NRDF coincides with the zero-delay RDF. Our
theoretical framework is corroborated with a simulation example.

I. INTRODUCTION

Zero-delay source coding is desirable in various real-time
applications, such as, in signal processing [2] and networked
control systems [3]–[5]. Zero-delay codes form a subclass
of causal source codes (see [6]), namely, codes where the
reproduced source samples depends on the source samples in a
causal manner. However, zero-delay source coding compared
to causal source coding allow the reproduction of each source
sample at the same time instant that the source sample is
encoded. Unfortunately, causal source coding does not exclude
the possibility of long blocks of quantized samples, which may
cost arbitrary end-to-end delays.

Zero-delay codes (and causal codes) in constrast to non-
causal codes cannot achieve the classical rate distortion func-
tion (RDF). Indeed, an open problem in information theory
is quantifying the gap between the optimal performance the-
oretically attainable (OPTA) by non-causal codes, and the
OPTA by causal and zero-delay codes, hereinafter denoted
by Rop
ZD(D), respectively. Notable exceptions
where this gap is explicitly found are memoryless sources [6],
stationary sources in high rates [7], and zero mean stationary
scalar Gaussian sources with average mean squared error
(MSE) distortion [8].

c (D) and Rop

Throughout the years, the interest in zero-delay applications
is growing, thus, initiating further research on characterizing

it

turns out

that Rop

the fundamental limitations of the OPTA by zero-delay codes.
ZD(D) is very hard to
Unfortunately,
compute and for this reason there has been a turn in studying
variants of classical RDF that perform as tight as possible to
Rop

ZD(D).
In this paper, we derive a tight upper bound to zero-delay
source coding for vector Gaussian AR sources subject
to
an average MSE distortion. We consider nonanticipative rate
distortion function (NRDF) (see, e.g., [1], [8], [9]), which
gives a tighter lower bound to Rop
ZD(D) compared to the
classical RDF (see, e.g., [8, eq. (11)]). Then, we employ the
feedback realization scheme proposed in [1, Fig. IV.3], that
corresponds to the optimal ”test-channel” of NRDF for vector
Gaussian AR(1) sources and average MSE distortion. Further,
we give conditions to asymptotically stabilize the performance
of the speciﬁc scheme. By invoking standard techniques using
entropy coded dithered quantizer (ECDQ) [10], [11] on the
innovations’ encoder of the feedback realization scheme, we
derive the tight upper bound. In addition, we show how to
generalize our scheme to vector Gaussian AR sources of any
ﬁnite order. If the vector dimension of the Gauss AR source
tends to inﬁnity, we show that the Rop
ZD(D) coincides with the
NRDF. We demonstrate our results with a numerical example.
Notation: We let R = (−∞, ∞), N0 = {0, 1, . . .}. For
t ∈ N0. We denote a sequence of RVs by xn (cid:44) (x0, . . . , xn)
and its realization by xn = xn, xj ∈ Xj, j = 0, . . . , n. The
distribution of the RV x on X is denoted by Px(dx) ≡ P(dx).
The conditional distribution of RV y given x = x is denoted
by Py|x(dy|x = x) ≡ P(dy|x). The transpose of a matrix S
. For a square matrix S ∈ Rp×p with entries
is denoted by S
Sij on the ith row and jth column, we denote by diag{S}
the matrix having Sii, i = 1, . . . , p, on its diagonal and zero
elsewhere.

T

II. PROBLEM STATEMENT

In this paper we consider the zero-delay source coding
setting illustrated in Fig. 1. In this setting, the p−dimensional
(vector) Gaussian source is governed by the following discrete-
time linear time-invariant state-space model

xt+1 = Axt + Bwt, t ∈ N0,
(1)
where A ∈ Rp×p, and B ∈ Rp×q are known, x0 ∈ Rp ∼
N (0; Σx0) is the initial state, and the noise process wt ∈ Rq

 
 
 
 
 
 
is an i.i.d. Gaussian N (0; Iq×q) sequence, independent of x0.
We allow A to have eigenvalues outside the unit circle which
means that xt can be unstable.

The system operates as follows. At every time step t, the
encoder observes the source xt and produces a single binary
codeword zt from a predeﬁned set of codewords Zt of at most
a countable number of codewords. Since the source is random,
zt and its length lt are random variables. Upon receiving zt,
the decoder produces an estimate yt of the source sample. We
assume that both the encoder and decoder process information
without delay and they are allowed to have inﬁnite memory
of the past.

Fig. 1: A zero-delay source coding scenario.

The analysis of the noiseless digital channel is restricted
to the class of instantaneous variable-length binary codes zt.
The countable set of all codewords (codebook) Zt is time-
varying to allow the binary representation zt to be an arbi-
trarily long sequence. The encoding and decoding policies are
described by sequences of conditional probability distributions
as {P(dzt|zt−1, xt) :
t ∈
N0}, respectively. At t = 0, we assume P(dz0|z−1, x0) =
P(dz0|x0) and P(dy0|y−1, z0) = P(dy0|z0).

t ∈ N0} and {P(dyt|yt−1, zt) :

1
n+1

The design in Fig. 1 is required to yield an asymp-
E{d(xn, yn)} ≤
totic average distortion lim supn−→∞
D, where D > 0 is the pre-speciﬁed distortion level,
d(xn, yn) (cid:44) (cid:80)n
2. The objective is to min-
imize the expected average codeword length denoted by
E(lt), over all encoding-decoding
lim supn−→∞
policies. These design requirements are formally cast by the
following optimization problem:

t=0 ||xt − yt||2

1
n+1

(cid:80)n

t=0

Rop

ZD(D) (cid:44) inf lim sup

n−→∞

1
n + 1

n
(cid:88)

t=0

E(lt)

(2)

1
n + 1
i.e., the OPTA by zero-delay codes.

lim sup
n−→∞

s. t.

E{d(xn, yn)} ≤ D,

the

III. PRELIMINARIES
In this section, we give the deﬁnition of NRDF of vector
Gaussian AR sources subject to an average MSE distortion.
(cid:44)
distribution
source
Deﬁne
(cid:81)n
t=0 P(dxt|xt−1),
by
the
reconstruction
P(dyn||xn) (cid:44) (cid:81)n
t=0 P(dyt|yt−1, xt),
joint
distribution by P(dxn, dyn) (cid:44) P(dxn) ⊗ P(dyn||xn).
The marginal on yt ∈ Yt, P(dyt|yt−1), is induced by the
joint distribution P(dxn, dyn). We assume that at t = 0,
P(dy0|y−1, x0) = P(dy0|x0).

by P(dxn)
distribution
and the

Given the previous distributions, we introduce the mutual

information between xn and yn as follows

I(xn; yn) (cid:44)=

n
(cid:88)

t=0

E log(

P(yt|yt−1, xt)
P(yt|yt−1)

),

where E{·} is the expectation with respect to the joint distri-
bution P(dxn, dyn).

Deﬁnition 1. (NRDF with average MSE distortion)
(1) The ﬁnite-time NRDF is deﬁned by

Rna

0,n(D) (cid:44)

inf
P(dyt|yt−1,xt): t=0,...,n

1
n+1

E{d(xn,yn)}≤D

1
n + 1

I(xn; yn),

(3)

assuming the inﬁmum exists.
(2) The per unit time asymptotic limit of (3) is deﬁned by

Rna(D) = lim

n−→∞

Rna

0,n(D),

(4)

assuming the inﬁmum exists and the limit exists and it is ﬁnite.

If one replaces lim inf by inf lim in (4), then an upper bound

to Rna(D) is obtained, deﬁned as follows.

¯Rna(D) (cid:44)

inf
P∞(dy|z,x)

lim
n−→∞

1
n + 1

Rna

0,n(D),

(5)

where P∞(dy|z, x) is the stationary or time-invariant recon-
struction distribution.
It
is shown in [12, Theorem 6.6] that provided the limit
in (5) exists, and the source is stationary (or asymptotically
stationary) then Rna(D) = ¯Rna(D).

The optimization problem of Deﬁnition 1, in contrast to the
one given in (2) is convex (see e.g., [13]). In addition, for the
source model (1) and the average MSE distortion, then, by [1,
Theorems 1], the optimal “test channel” corresponding to (4)
is of the form

P∗(dyt|yt−1, xt) = P∗(dyt|yt−1, xt), t ∈ N0,

(6)

where at t = 0, P∗(dy0|y−1, x0) = P∗(dy0|x0), and the
t ∈ N0} is jointly
corresponding joint process {(xt, yt) :
Gaussian.

IV. ASYMPTOTICALLY STATIONARY FEEDBACK
REALIZATION SCHEME VIA KALMAN FILTERING

The authors in [1, Theorem 2] realized the optimal “test
channel” of (6) with the feedback realization scheme illus-
trated in [1, Fig. IV.3] that corresponds to a realization of the
form:

T

T

yt = E

t HtEt(xt − (cid:98)xt|t−1) + E

t Θtvt + (cid:98)xt|t−1,

(7)

where Ht (cid:44) ΦtΘt is a scaling matrix; vt is an independent
Gaussian noise process with N (0; Σvt), Σvt = diag{Vt}
independent of x0; the error xt − (cid:98)xt|t−1 is Gaussian with
N (0; Πt|t−1), and (cid:98)xt|t−1 (cid:44) E{xt|yt−1}; the error xt − (cid:98)xt|t is
Gaussian with N (0; Πt|t), and (cid:98)xt|t (cid:44) E{xt|yt}. Moreover,

Gaussian SourceEncoderDecoderNoiselessBinaryChannel0,1lttztyxtFig. 2: Asymptotically stationary feedback realization scheme corresponding to (7).

{(cid:98)xt|t−1, Πt|t−1} are given by the following Kalman ﬁlter
equations:

(8a)

(8b)

(8c)

(8d)

(8e)

T

t Et,

(cid:98)x0|−1 = E{x0},
+ BB

, Π0|−1 = Σx0 ,

x0|0 = E{x0},

T

T

Prediction:
(cid:98)xt|t−1 = A(cid:98)xt−1|t−1,
Πt|t−1 = AΠt−1|t−1A
Update:
(cid:98)xt|t = (cid:98)xt|t−1 + Gt
˜kt (cid:44) yt − (cid:98)xt|t−1 (innovation),
Πt|t = Πt|t−1 − GtStG
t HtEt)

˜kt,

T

T

T

T

,

St = (E

Gt = Πt|t−1(E

(Kalman Gain)
T

t , Π0|0 = Σx0 ,
S−1
t
T
t HtEt)Πt|t−1(E
t HtEt)
(cid:111)
(cid:110)
1 − ˜∆t
Λt
t Ht, ˜∆t (cid:44) diag{δt}, Λt (cid:44) EtΠt|t−1E

vt , Φt (cid:44)
where Ht (cid:44) diag
Θ−1
t = diag{λt},
and Et ∈ Rp×p is an orthogonal matrix. It is easy to verify
following that the following hold:

Ht ˜∆tΣ−1

t ΘtΣvtΘ

, Θt (cid:44)

+ E
(cid:113)

T

T

T
(cid:98)xt|t−1 = Ayt−1, (cid:98)xt|t = yt, Gt = I, Πt|t = E
t

(9)
where I ∈ Rp×p denotes the identity matrix. By substituting
(9) in (7) we can also deduce that P∗(dyt|yt−1, xt) =
P∗(dyt|yt−1, xt).

˜∆tEt,

The realization scheme of (7) becomes asymptotically sta-
tionary (stable) if one of the following two conditions hold:
(1) A is stable, i.e., its eigenvalues have magnitude less than
one; (2) the pair (A, B) is (completely) stabilizable (see e.g.,
[14, p. 342]). This means that Π (cid:44) limt−→∞ Πt|t−1 < ∞,
Π(cid:48) (cid:44) limt−→∞ Πt|t < ∞, Et ≡ E, and Σvt ≡ Σv.

Next, we brieﬂy discuss the resulting asymptotically sta-

tionary realization scheme depicted in Fig. 2.

Preprocessing at Encoder: Introduce the estimation error
t ∈ N0}, where kt (cid:44) xt − (cid:98)xt|t−1, t ∈ N0
{kt ∈ Rp :
with (error) covariance Πt|t−1, t ∈ N0. Under conditions
(1) or (2), we ensure that Π (cid:44) limt−→∞ Πt|t−1 and it is
unique. The error covariance matrix Π is diagonalized by
introducing an orthogonal matrix E (invertible matrix) such
= diag{λ} (cid:44) Λ. To facilitate the computation,
that EΠE
we introduce the scaling process {γt ∈ Rp : t ∈ Nn
0 }, where
γt (cid:44) Ekt, t ∈ N0, has independent Gaussian components.

T

Preprocessing at Decoder: Analogously, we introduce
the innovations process {˜kt :
t ∈ N0} deﬁned by (8d) and
t ∈ N0} deﬁned by ˜γt(cid:44)Θβt, with
the scaling process {˜γt :
βt (cid:44) (ΦEkt + vt) , vt ∼ N (0; Σv), and {Φ, Θ} are the
asymptotic limits of Φt and Θt, respectively.
The ﬁdelity criterion ||kt−˜kt||2
2 at each t is not affected by the
above processing of {(xt, yt) : t ∈ N0}, in the sense that the
preprocessing at both the encoder and decoder do not affect
the form of the squared error distortion function, that is,

||xt − yt||2

2 = ||kt − ˜kt||2
2,
Moreover, using basic properties of conditional entropy (see,
e.g., [15, Eq. (IV.35)]), it can be shown that

t ∈ N0.

(10)

Rna(D) = lim

1
n + 1

n
(cid:88)

t=0

I(xt; yt|yt−1)

(11)

s.t.

lim
n−→∞

1
n + 1

E {d(xn, yn)} ≤ D,

and

Rna,kn,˜kn

(D) (cid:44) lim

s.t.

lim
n−→∞

1
n + 1

E

n
(cid:88)

I(kt; ˜kt)

(12)

1
n + 1
(cid:110)

t=0
d(kn, ˜kn)

(cid:111)

≤ D,

are equivalent expressions.
In addition, the steady state values of (8b) is Π = AΠ(cid:48)A
BB
is

+
. The end-to-end MSE distortion of the scheme in Fig. 2

T

T

lim
t−→∞

= lim
t−→∞

T

E{(xt − yt)

(xt − yt)}
trace E{(xt − (cid:98)xt|t)(xt − (cid:98)xt|t)

T

} = trace(Π(cid:48)) ≤ D.

Hence, following [1, Theorem 2], the per unit time asymptotic
limit of Gaussian NRDF subject to the total MSE distortion
can be expressed as follows.

Rna(D) =

min
trace(Π(cid:48))≤D

1
2

(cid:32)

log max

1,

|AΠ(cid:48)A

T

+ BB

T

|

|Π(cid:48)|

(cid:33)

.

(13)

Clearly,

is a log-
the optimization problem in (13),
determinant minimization problem and can be solved using,

Vector Gaussian AR(1) Source DecoderptxEp-Parallel AWGN ChannelsptETt|1ˆttxttytkptInnovationsEncodertkvv(0;)tNfor instance, Karush-Kuhn-Tucker conditions [16, Chapter
5.5.3] or semideﬁnite programming (SDP). A way of solving
(13) is proposed in [17]. However, compared to that work, our
realization scheme is implemented with feedback to take into
account the effect of unstable sources in the dynamical system

V. UPPER BOUND TO ZERO-DELAY GAUSSIAN RDF

(a) p−parallel AWGN channels.

In this section, we derive an upper bound to the zero-delay
Gaussian RDF using a subtractively dithered uniform scalar
quantizer (SDUSQ) on the feedback realization scheme of
Fig. 2. The SDUSQ scheme was introduced in [10] and since
then it has been used in several papers (see, e.g., [4], [8],
[18]) under various realization setups. However, it has never
been documented for the realization scheme proposed in this
work. Here, we consider the vector Gaussian AR(1) source
of (1), and we quantize each time step t over p independently
operating SDUSQ, with their outputs being jointly entropy
coded conditioned to the dither. We extend our results when
using vector quantization showing that at inﬁnite dimensional
vectors, the space-ﬁlling loss due to compression and the en-
tropy coding extinguishes, i.e., Rna(D) and Rop
ZD(D) coincide.

A. Scalar quantization

Next, we use the asymptotically stationary feedback re-
alization scheme illustrated in Fig. 2 to design an efﬁcient
{encoder/quantizer,decoder} pair.

We select the quantizer step size ∆ so that the covariance of
the resulting quantization error meets Σv. The encoder does
not quantize the observed state xt directly. Instead, it quantizes
the deviation of xt from the linear estimate (cid:98)xt|t−1 of xt.
This method is known in least squares estimation theory as
innovations approach and, therefore, the encoder is named as
an innovations’ encoder. We consider the zero-delay source
coding setup illustrated in Fig. 2 with the additional change
of the p−parallel additive white Gaussian noise (AWGN)
channels with p independently operating SDUSQ. This is il-
lustrated in Fig. 3. Note that, all matrices and scalings adopted
in Fig. 2 still hold when the aforementioned replacement is
applied.

For each time step t, the input to the quantizer, is a scaled

estimation error deﬁned as follows

αt = Akt, A = ΦE.

(14)

Moreover, αt is an Rp−valued random process. The parallel
p−dimensional AWGN channel is replaced by p indepen-
dently operating SDUSQ, hence we can design the covariance
matrix Σv of the AWGN channels corresponding to the p-
parallel AWGN channels in such a way, that for each t, each
diagonal entry Vii, i = 1, . . . , p, i.e., Σv (cid:44) diag{V }, to
correspond to a quantization step size ∆i, i = 1, . . . , p, such
that

(b) Realization over p independently operating SDUSQ.

Fig. 3: Scalar quantization by replacing a p−dimensional
AWGN channel with p independently operating SDUSQ.

This results creates a multi-input multi-output (MIMO) trans-
mission of parallel and independent SDUSQ. We apply
SDUSQ to each component of αt, i.e.,

βt,i = QSD
∆i

(αt,i), i = 1, . . . , p

(16)

and we let rt be the Rp−valued random process of dither
signals whose individual components {rt,1, . . . , rt,p} are mu-
tually independent and uniformly distributed random variables
(cid:1) independent of the corresponding
rt,i ∼ Unif (cid:0)− ∆i
source input components αt,i, ∀t, i. The output of the quan-
tizer is given by

2 , ∆i

2

˜βt,i = Q∆i(αt,i + rt,i), i = 1, . . . , p.

(17)

Note that ˜βt = ( ˜βt,1, . . . , ˜βt,p) can take a countable number
of possible values. In addition, by construction (see Fig. 2), the
sequences {αt : t = 0, 1, . . .} and { ˜βt : t = 0, 1, . . .} are not
Gaussian any more since by applying the change illustrated
t = 0, 1, . . .} and { ˜βt :
in Fig. 3, {αt :
t = 0, 1, . . .}
contain samples of the uniformly distributed process {rt : t =
0, 1, . . .}. As a result, the Kalman ﬁlter in Fig. 2 is no longer
the least mean square estimator.

Entropy coding: In what follows, we apply joint entropy
coding across the vector dimension p and memoryless coding
across the time, that is, at each time step t the output of
the quantizer ˜βt is conditioned to the dither to generate a
codeword zt. The decoder reproduces βt by subtracting the
dithered signal rt from ˜βt. Speciﬁcally, at every time step
t, we require that a message ˜βt is mapped into a codeword
zt ∈ {0, 1}lt designed using Shannon codes [19, Chapter 5.4].
For a RV x, the codes constructed based on Shannon coding
scheme give an instantaneous (preﬁx-free) code with expected
code length that satisﬁes the bounds

Vii =

∆2
i
12

, i = 1, . . . , p.

(15)

H(x) ≤ E(l) ≤ H(x) + 1.

(18)

p-Parallel AWGN ChannelsptptdiagVvvv(0;),()tNLoseless Encoder()QtzLoseless Decoderttptptiitt,it,i:i1,...,p,Unif(,)22rrrIf x is a p−dimensional random vector then the normalized
version of (18) gives

H(x)
p

≤

E(l)
p

≤

H(x)
p

+

1
p

.

(19)

Fig. 4: An equivalent model to Fig. 3b based on scalar uniform
additive noise channel.

Since the SDUSQ operates using memoryless entropy cod-

ing over time, the following theorem holds.

Theorem 1. (Upper bound)
the zero-delay source-coding
Consider the realization of
scheme illustrated in Fig. 2 with the change of AWGN channel
with p−parallel independently operating SDUSQ illustrated
in Fig. 3. If the vector process { ˜βt :
t = 0, 1, . . .} of the
quantized output is jointly entropy coded conditioned to the
dither signal values in a memoryless fashion for each t, then
the operational Gaussian zero-delay rate, Rop
ZD(D), satisﬁes
(cid:17)

Rop

ZD(D) ≤ Rna(D) +

log2

p
2

(cid:16) πe
6

+ 1

(20)

where p is the dimension of the state-space representation
given in (1), while the average MSE distortion achieves the
end-to-end average distortion D of the system.

Proof. See Appendix A.

The previous main result combined with the lower bound
on Gaussian zero-delay RDF, leads to the following corollary.

Corollary 1. (Bounds on zero-delay RDF)
Consider the realization of
the zero-delay source-coding
scheme illustrated in Fig. 2 with the change of AWGN
channel with p−parallel independently operating SDUSQ as
illustrated in Fig. 3. Then, for vector (stable or unstable)
Gaussian AR(1) sources the following bounds hold

Rna(D) ≤ Rop

ZD(D) ≤ Rna(D) +

p
2

log

(cid:17)

(cid:16) πe
6

+ 1.

(21)

Proof. This is obtained using the fact
Rop

ZD(D), (13) and Theorem 1.
Theorem 2. (Generalization)
The bounds derived in Corollary 1 based on the realization
scheme of Fig. 2 hold for vector Gaussian sources of any order.

that Rna(D) ≤

Proof. See Appendix B.

In the next remark, we draw connections to existing results

in the literature.

Remark 1. (Relations to existing results)
(1) For stationary stable scalar-valued Gaussian AR sources,

our upper bound in Theorem 1 coincides with the bound
obtained in [8, Theorem 7]. However, the upper bound in [8]
is obtained using a realization scheme with four ﬁlters instead
of only one that we use in our scheme. In addition, our result
takes into account unstable Gaussian sources too.
(2) Compare to [18], we use ECDQ based on a different
realization setup that results into obtaining different lower and
upper bounds.

Next, we employ Theorem 1 to demonstrate a simulation

example.

Example 1. We consider a two-dimensional unstable Gaus-
sian AR(1) source as follows:

xt+1 =

(cid:20)−1.3
−0.3

(cid:124)

(cid:123)(cid:122)
A

0.4
0

(cid:21)

(cid:125)

xt +

(cid:20)1
0

(cid:21)
0
1

(cid:124) (cid:123)(cid:122) (cid:125)
B

wt,

(22)

where xt ∈ R2, the parameter matrix A is unstable be-
cause one of its eigenvalues, denoted by λi(A), has mag-
nitude greater than one, the pair (A, B) is stabilizable and
wt ∼ N (0; I2×2). By invoking SDPT3 [20] we plot
the
theoretical attainable lower and upper bounds to the zero-
delay RDF. This is illustrated in Fig. 5. As expected from
theory, Rna(D) ≥ (cid:80)
λi(A)>1 log |λi(A)| ≈ 0.263 bits/source
sample.

Fig. 5: Bounds on Rop

ZD(D) via the scheme of Fig. 2.

B. Vector Quantization

It is interesting to observe that if instead of uniform scalar
quantization we quantize over a lattice (vector) quantizer
followed by memoryless entropy coded conditioned to the
dither, then the upper bound in (20) becomes

Rop

ZD(D) ≤ Rna(D) +

p
2

log2 (2πeGp) + 1

(23)

where Gp is the normalized second moment of the lattice [11].
If we take the average rate per dimension and assume an inﬁ-
nite dimensional vector Gaussian source, then by [11, Lemma
1], Gp → 1
2πe , and the terms due to space-ﬁlling loss and the
loss due to entropy coding in (23) asymptotically goes to zero.
Utilizing the latter, and the fact that Rna(D) ≤ Rop
ZD(D), we
obtain

1
p

lim
p→∞

Rna(D) ≤ lim
p→∞
i.e., Rna(D) is the OPTA by zero-delay codes.

ZD(D) ≤ lim
p→∞

Rop

1
p

1
p

Rna(D).

(24)

p-Parallel Scalar Uniform Additive Noise Channelsptpt,,:1,...,,Unif(,)22ξξξiittitiip0123456789Distortion (D)0246810Rate [bits]VI. CONCLUSIONS AND FUTURE DIRECTIONS

Next, note that for n = 0, 1, . . ., the following inequality

We considered zero-delay source coding of a vector Gaus-
sian AR source under MSE distortion. Based on a feedback
realization scheme that quantizes the innovations of a Kalman
ﬁlter with a SDUSQ, we derived an upper bound to the zero-
delay RDF. We discussed the performance of this scheme
when using lattice quantization. For inﬁnite dimensions we
observed that the NRDF coincide with the zero-delay RDF.
An illustrative example is presented to support our ﬁndings.
As an ongoing research, we will apply the proposed coding
scheme based on SDUSQ to ﬁnd the actual operational rates
corresponding to the zero-delay RDF. Moreover, we will
examine similar coding schemes for ﬁxed-length coding rate.

APPENDIX A
PROOF OF THEOREM 1

In the realization scheme proposed in Fig. 2, with the change
of AWGN channel with p−parallel independently operating
SDUSQ,
the operational rate for each t is equal
to the
conditional entropy H( ˜βt|rt) where ˜βt = { ˜βt,1, . . . , ˜βt,p},
˜βt,i = Q∆i (αt,i + rt,i), i = 1, . . . , p, i.e., the entropy of the
quantized output ˜βt conditioned on the t−value of the dither
signal rt. This leads to the following analysis.

(a)
= I(αt; βt)

H( ˜βt|rt)
(b)
= I(αt; αt + ξt)
= h(αt + ξt) − h(ξt)
(c)
= h(αG
(d)
≤ I(αG
(e)
= I(αG

t ; αG
t ; αG

t + vt) − h(αG

= I(αG

t ; βG

t + vt) + D(ξt||vt)
p
log2
2
(cid:17)
(cid:16) πe
log2
6

t + vt) +
p
2

t ) +

,

(cid:16) πe
6

(cid:17)

t ) + D(ξt||vt) − D(αt + ξt||αG

t + vt)

holds in Fig. 2.

I(xn; yn)

(a)
=

(b)
=

(c)
=

n
(cid:88)

t=0
n
(cid:88)

t=0
n
(cid:88)

t=0

I(xt; yt|yt−1)

I(kt; ˜kt)

I(αt; βt),

(26)

where (a) follows from the structural properties of speciﬁc
extremum problem resulting in the realization of Fig. 2 (see,
e.g., the analysis in Section II and [15, Remark IV.5]); (b)
follows from the analysis in [15, Equation (35)]; (c) follows
from the fact that E, Φ, Θ are invertible matrices and as a
result the information from kt to ˜kt is the same as from αt
to βt (information lossless operation).

Since we are assuming joint memoryless entropy coding
of p independently operating scalar uniform quantizers with
subtractive dither, then by (18), for t = 0, 1, . . . , n, we obtain

n
(cid:88)

t=0

E(lt) ≤

n
(cid:88)

(cid:16)

H( ˜βt|rt) + 1

(cid:17)

t=0
n
(cid:88)

(cid:16)

(a)
≤

t=0

I(αG

t ; βG

t ) +

p
2

log2

(cid:17)

(cid:16) πe
6

(cid:17)

+ 1

(b)
≤ I(xn,G; yn,G) + (n + 1)

p
2

log2

(cid:17)

(cid:16) πe
6

+ (n + 1),

(27)

where (a) follows by (25) and (b) follows from (26).

Then, by ﬁrst taking the per unit time limiting expression

in (27) and then the inﬁmum, we obtain

(25)

inf lim sup
n−→∞

1
n + 1

n
(cid:88)

t=0

E(lt)

where (a) follows from [10, Theorem 1]; (b) follows from the
fact that the quantization noise is ξt = βt − αt (see Fig. 4);
(c) follows from the fact that the relative entropy D(x||x(cid:48)) =
h(x(cid:48)) − h(x), see, e.g., [19, Theorem 8.6.5]; (d) follows from
the fact that D(αt+ξt||αG
t +vt) ≥ 0, with equality if and only
if {ξt :
t = 0, 1, . . .} becomes a Gaussian distribution; (e)
from the fact that the differential entropy h(vt) of a Gaussian
random vector with covariance Σv (cid:44) diag{V } is

h(vt) =

1
2

log2(2πe)p|Σv| =

p
(cid:88)

i=1

1
2

log2(2πe)Vii,

and the entropy h(ξt) of the uniformly distributed random
vector ξt = {ξt,i : i = 1, 2, . . . , p}, ξt,i ∼ Unif (cid:0)− ∆i
(cid:1) is

2 , ∆i

2

h(ξt) =

p
(cid:88)

i=1

1
2

log2 ∆2
i .

Since we have that Vii = ∆2

i

12 , i = 1, . . . , p, the result follows.

≤ inf lim sup
n−→∞

1
n + 1

I(xn,G; yn,G) +

p
2

log2

(cid:17)

(cid:16) πe
6

+ 1

(a)
=⇒ Rop

ZD ≤ ¯Rna(D) +

p
2

log2

(cid:17)

(cid:16) πe
6

+ 1,

(28)

where (a) follows by (2) and (5) respectively, and ¯Rna(D) is
the upper bound expression of Rna(D) for the vector Gaussian
AR(1) source model given by (1).

Finally, by assumptions, (i.e., A is stable or (A, B) sta-
bilizable) the innovations’ Gaussian source is asymptotically
stationary. Since Rna(D) = Rna,kn,˜kn
(D), then at steady
state, we have ¯Rna(D) = Rna(D) and the result follows.

APPENDIX B
PROOF OF THEOREM 2

Assume the following vector Gaussian AR(s) process,

where s is a positive integer, in state space representation.

xt+1 =

s
(cid:88)

j=1

Ajxt−j+1 + Bwt,

(29)

REFERENCES

[1] P. A. Stavrou, T. Charalambous, and C. D. Charalambous, “Filtering
with ﬁdelity for time-varying Gauss-Markov processes,” in Proc. IEEE
Conf. Decision Control, December 2016, pp. 5465–5470.

[2] Y. Huang and J. Benesty, Audio Signal Processing for Next-Generation
Norwell, MA, USA: Kluwer

Multimedia Communication Systems.
Academic Publishers, 2004.

[3] T. Linder and S. Y¨uksel, “On optimal zero-delay coding of vector
Markov sources,” IEEE Trans. Inf. Theory, vol. 60, no. 10, pp. 5975–
5991, Oct 2014.
[4] T. Tanaka, K. H.

Johansson, T. Oechtering, H. Sandberg, and
M. Skoglund, “Rate of preﬁx-free codes in LQG control systems,” in
Proc. IEEE Int. Symp. Inf. Theory, July 2016, pp. 2399–2403.

[5] M. Barforooshan, J. Østergaard, and P. A. Stavrou, “Achievable perfor-
mance of zero-delay variable-rate coding in rate-constrained networked
control systems with channel delay,” in Proc. IEEE Conf. Decision
Control, December 2017, to appear.

[6] D. Neuhoff and R. Gilbert, “Causal source codes,” IEEE Trans. Inf.

Theory, vol. 28, no. 5, pp. 701–713, Sep. 1982.

[7] T. Linder and R. Zamir, “Causal coding of stationary sources and
individual sequences with high resolution,” IEEE Trans. Inf. Theory,
vol. 52, no. 2, pp. 662–680, Feb. 2006.

[8] M. S. Derpich and J. Østergaard, “Improved upper bounds to the causal
quadratic rate-distortion function for Gaussian stationary sources,” IEEE
Trans. Inf. Theory, vol. 58, no. 5, pp. 3131–3152, May 2012.

[9] A. K. Gorbunov and M. S. Pinsker, “Prognostic epsilon entropy of a
Gaussian message and a Gaussian source,” Problems Inf. Transmiss.,
vol. 10, no. 2, pp. 93–109, Apr.-June 1974.

[10] R. Zamir and M. Feder, “On universal quantization by randomized
uniform/lattice quantizers,” IEEE Trans. Inf. Theory, vol. 38, no. 2, pp.
428–436, March 1992.

[11] ——, “On lattice quantization noise,” IEEE Trans. Inf. Theory, vol. 42,

no. 4, pp. 1152–1159, Jul 1996.

[12] P. A. Stavrou, “Extremum problems of directed information,” Ph.D.

dissertation, University of Cyprus, 2016.

[13] C. D. Charalambous and P. A. Stavrou, “Directed information on abstract
spaces: Properties and variational equalities,” IEEE Trans. Inf. Theory,
vol. 62, no. 11, pp. 6019–6052, Nov 2016.

[14] B. Anderson and J. Moore, Optimal Filtering. Englewood Cliffs, NJ:

Prentice-Hall, 1979.

[15] C. D. Charalambous, P. A. Stavrou, and N. U. Ahmed, “Nonanticipative
rate distortion function and relations to ﬁltering theory,” IEEE Trans.
Autom. Control, vol. 59, no. 4, pp. 937–952, April 2014.

[16] S. Boyd and L. Vandenberghe, Convex Optimization. New York, NY,

USA: Cambridge University Press, 2004.

[17] T. Tanaka, K. K. K. Kim, P. A. Parrilo, and S. K. Mitter, “Semideﬁnite
programming approach to Gaussian sequential rate-distortion trade-offs,”
IEEE Trans. Autom. Control, vol. 62, no. 4, pp. 1896–1910, April 2017.
[18] E. I. Silva, M. S. Derpich, and J. Østergaard, “A framework for control
system design subject to average data-rate constraints,” IEEE Trans.
Autom. Control, vol. 56, no. 8, pp. 1886–1899, Aug 2011.

[19] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed.

John Wiley & Sons, Inc., Hoboken, New Jersey, 2006.

[20] R. H. T¨ut¨unc¨u, K. C. Toh, and M. J. Todd, “Solving semideﬁnite-
quadratic-linear programs using SDPT3,” Mathemat. Programm.,
vol. 95, no. 2, pp. 189–217, 2003.

where Aj ∈ Rp×p, and B ∈ Rp×q are deterministic matrices,
x0 ∈ Rp ∼ N (0; Σx0 ) is the initial state, and wt ∈ Rq ∼
N (0; Iq×q) is an i.i.d. Gaussian sequence, independent of x0.
Clearly, for s = 1, (29) gives as a special case the source
model described by (1).
Next, we show that (29) can be expressed as an augmented
vector Gaussian AR(1) process as follows

˜xt+1 = ˜A˜xt + ˜B ˜wt,

(30)

where ˜A ∈ Rsp×sp, and ˜B ∈ Rsp×sq are deterministic matri-
ces, ˜x0 ∈ Rsp ∼ N (0; Σ˜x0) is the initial state with Σ˜x0 being
the covariance of the initial state, and ˜wt ∈ Rsq ∼ N (0; Σ ˜wt)
is an i.i.d. Gaussian sequence, independent of ˜x0.

The proof employs a simple augmentation of the state
process. First, note that the state space model of (29) can be
modiﬁed as follows.








xt+1
xt
...
xt−s+2








=

+











A1 A2
0
I

0
...
0

I
...
0

. . . As−1 As
. . .
0
0
. . .
. . .
. . .

0
...
0








B 0
0
0
...
...
0
0

. . .
. . .

. . .
. . .


0
0


...


0








0
...
I
wt
0
...
0








.


















xt
xt−1
...
xt−s+1








(31)

Then, (31) can be written in an augmented state space form
as follows.

˜xt+1 = ˜A˜xt + B ˜wt,

(32)

























xt+1
xt
...
xt−s+2
A1 A2
0
I

0
...
0

I
...
0

where

˜xt+1 =

˜A =

˜B =

∈ Rsp, ˜xt =








∈ Rsp,








xt
xt−1
...
xt−s+1


. . . As−1 As
. . .
0
0
. . .
. . .
. . .

0
...
I

0
...
0

∈ Rsp×sp,
















B 0
0
0
...
...
0
0

. . .
. . .

. . .
. . .


0
0


...


0

∈ Rsp×sq, ˜wt =








wt
0
...
0








∈ Rsq.

The augmented state space representation of (32) is an aug-
mented vector Gaussian AR(1) process which can then be
applied to the feedback design of Fig. 2 obtaining the same
bound as in Theorem 1. This completes the proof.

