1
2
0
2

r
a

M
1
3

]

V
C
.
s
c
[

1
v
5
6
2
7
1
.
3
0
1
2
:
v
i
X
r
a

Human POSEitioning System (HPS): 3D Human Pose Estimation and
Self-localization in Large Scenes from Body-Mounted Sensors

Vladimir Guzov * 1,2

Aymen Mir * 1,2

Torsten Sattler 3

Gerard Pons-Moll1,2

1University of T¨ubingen, Germany,

2Max Planck Institute for Informatics, Saarland Informatics Campus, Germany

3CIIRC, Czech Technical University in Prague, Czech Republic

{vguzov, amir, gpons}@mpi-inf.mpg.de torsten.sattler@cvut.cz

Figure 1. HPS jointly estimates the full 3D human pose and location of a subject within large 3D scenes, using only
wearable sensors. Left: subject wearing IMUs and a head mounted camera. Right: using the camera, HPS localizes the hu-
man in a pre-built map of the scene (bottom left). The top row shows the split images of the real and estimated virtual camera.

Abstract

We introduce (HPS) Human POSEitioning System, a
method to recover the full 3D pose of a human registered
with a 3D scan of the surrounding environment using wear-
able sensors. Using IMUs attached at the body limbs and a
head mounted camera looking outwards, HPS fuses cam-
era based self-localization with IMU-based human body
tracking. The former provides drift-free but noisy position
and orientation estimates while the latter is accurate in the
short-term but subject to drift over longer periods of time.

We show that our optimization-based integration exploits
the beneﬁts of the two, resulting in pose accuracy free of
drift. Furthermore, we integrate 3D scene constraints into
our optimization, such as foot contact with the ground, re-
sulting in physically plausible motion. HPS complements
more common third-person-based 3D pose estimation meth-
ods.
It allows capturing larger recording volumes and
longer periods of motion, and could be used for VR/AR ap-

* Joint ﬁrst authors with equal contribution.

plications where humans interact with the scene without re-
quiring direct line of sight with an external camera, or to
train agents that navigate and interact with the environment
based on ﬁrst-person visual input, like real humans.

With HPS, we recorded a dataset of humans interact-
ing with large 3D scenes (300-1000 m2) consisting of 7
subjects and more than 3 hours of diverse motion. The
dataset, code and video will be available on the project
page: http://virtualhumans.mpi-inf.mpg.de/hps/.

1. Introduction

Capturing the full 3D pose of a human, while localizing
and registering it with a 3D reconstruction of the environ-
ment, using only wearable sensors, opens the door to many
applications and new research directions. For example, it
will allow Augmented / Mixed / Virtual Reality users to
move freely and interact with virtual objects in the scene,

IMU SensorsHead CameraSYNTHETICREALSYNTHETICREALREALSYNTHETIC 
 
 
 
 
 
without the need for external cameras. From the captured
data, we could train digital humans that plan and move like
real humans, based on visual data arriving at their eyes.
Moreover, by relying only on ego-centric data, we could
capture a wider variety of human motion, outside of a re-
stricted recording volume imposed by external cameras.

The dominant approach in vision has been to analyze hu-
mans from an external third-person camera, often without
considering scene context [4, 30, 39, 45, 51, 55]. A few re-
cent methods capture 3D scenes and humans [24], but again
using a third-person camera. Capturing with external cam-
eras is undoubtedly a central problem in vision, but it has
its limitations – occlusions are a problem, and interactions
across multiple rooms or beyond the viewing area cannot be
captured; consequently recordings are typically short.

We propose Human POSEitioning System (HPS), the
ﬁrst method to recover the full body 3D pose of a human
registered with a large 3D scan of the surrounding envi-
ronment relying only on wearable sensors – body-mounted
IMUs and a head mounted camera, approximating the vi-
sual ﬁeld of view of the human. Inspired by visual-inertial
odometry and localization [29, 40], as well as IMU-based
human pose estimation [50, 71, 73], HPS fuses information
coming from body-mounted IMUs with camera pose ob-
tained from camera self-localization [57,59,64] (see Fig. 1).
Instead of placing the camera towards the body [52, 67], we
place it towards the scene, which allows us to capture what
the human observes together with their 3D pose. In com-
parison to third-person pose methods, the body is not seen
by the camera, which poses new challenges.

Pure IMU-based tracking is known to drift over time and
camera localization produces many outliers. By jointly inte-
grating IMU tracking with camera self-localization, we are
able to remove drift [29, 40], and recover the human tra-
jectory when self-localization fails. Furthermore, since we
can approximately locate the person in the 3D scene, we in-
corporate scene constraints when foot contact is detected.
Overall, with HPS we recover natural human motions, reg-
istered with the 3D scene and free of drift, during long pe-
riods of time, and over large areas.

To demonstrate the capabilities of HPS, we capture a
dataset of real people moving in large scenes. Our HPS
dataset consists of 8 types of environments - some being
larger than 1000m2, and 7 subjects performing a variety of
activities such as walking, excercising, reading, eating, or
simply working in the ofﬁce. The dataset can be used as
a testbed for ego-centric tracking with scene constraints, to
learn how humans interact and move within large scenes
over long periods of time, and to learn how humans process
visual input arriving at their eyes.

We make the following contributions: 1) to the best of
our knowledge, HPS is the ﬁrst approach to estimate the full
3D human pose while localizing the person within a pre-

scanned large 3D scene using wearable sensors. 2) we intro-
duce a joint optimization which integrates camera localiza-
tion, IMU-based tracking and scene constraints, resulting in
smooth and accurate human motion estimates. 3) we pro-
vide the HPS dataset, a new dataset consisting of 3D scans
of large scenes (some larger than 1000 m2), ego-centric
video, IMU data, and our 3D reconstructed humans moving
and interacting with the scene. In contrast to existing 3D
pose datasets, which are captured from a third-person view,
ours is captured from an egocentric view. We believe both
HPS and HPS dataset will provide a step towards develop-
ing future algorithms to understand and model 3D human
motion and behavior within the 3D environment from an
egocentric (or third-person) perspective.

2. Related Work

IMU-based 3D Human Pose Estimation: Although
commercial solutions for IMU-based pose estimation have
improved the stability of earlier solutions [53], they still
suffer from severe drift, especially in the global orientation
and location of the body. Early work [70] developed a cus-
tom suit to capture 3D human pose during daily activities.
One line of work has focused on reducing the amount of
IMUs necessary to capture motion via space-time optimiza-
tion [73] or with deep learning [26]. In order to reduce drift
and improve accuracy, visual-inertial approaches combine
IMUs with multiple external cameras [42, 49, 50, 69, 72], a
depth-camera [25,85] or even a single hand-held RGB cam-
era [71]–which allowed collecting the 3DPW [71] dataset
with accurate 3D poses outdoors. However, they all re-
quire an external camera, which limits the ﬁeld of view to
be captured, or requires someone to follow the person being
tracked. Instead, we mount the camera (approximating the
person’s ﬁeld of view) on the head and use it to self-localize
the person in the scene.

Ego-centric capture and prediction: In contrast to our
method, most ego-centric body-capture approaches mount
the camera on the head looking towards the body. While
ego-centric capture has received considerable attention for
activity recognition [6, 12, 17, 41, 54, 80], methods at most
detect the upper body. For full body capture, a pioneering
method [52] relied on a helmet with sticks holding a camera
away from the body. More recent methods [67, 78] work
reasonably well even when the camera is close to the head.
However, the accuracy is still far from desired.

Another group of methods place the camera looking out-
wards (like humans), and aim at estimating 3D pose from
the ego-centric view alone, but 3D poses are inaccurate and
have high uncertainty [28, 81, 82]. These methods to in-
fer 3D pose from an ego-centric view [28, 81, 82] would
beneﬁt from our captured data, which contains ego-centric
video with corresponding accurate 3D pose registered with
the environment. An alternative approach places many cam-

2

Figure 2. Overview. We use IMU data, RGB video from a head mounted camera, and a pre-scanned scene as input. We obtain an
approximate 3D body pose using IMU data, and use head camera self-localization to localize the subject in the 3D scene. We then integrate
the approximate body pose, the camera position and orientation, along with the 3D scene in a joint optimization to obtain the ﬁnal location
and pose estimates. We urge readers to see the video at http://virtualhumans.mpi-inf.mpg.de/hps/.

eras on the body looking out and use multi-camera structure
from motion [61], but it can only recover slow motions.

Camera Localization: Most 6-DoF camera localization
algorithms can be split into three groups. The ﬁrst group is
structure-based [11,14,37,59,62,63,65,66], which matches
2D points in the query image with 3D scene keypoints to es-
timate the camera pose by minimizing the reprojection er-
ror. While they provide precise position in small scenes,
they do not scale to large scenes as matching becomes am-
biguous and computationally expensive.

The second group of methods is referred to as image-
based. The idea is to retrieve nearest neighbors in an image
database based on a global descriptor [5, 68, 76]. The cam-
era pose can then be approximated by the known poses of
the retrieved images. They are more robust and scalable
compared to structure based methods, but less precise, and
the quality depends on the size of the image database.

In the third group are hybrid approaches [10, 56, 57]
which combine the beneﬁts of the last two. First, a set of
relevant database images are found using an image-based
method, and then the precise camera pose is recovered using
structure-based methods. Another set of methods directly
regress the camera pose using a CNN [60, 74], but their ac-
curacy leaves a lot to be desired. Hybrid approaches have
been shown to be precise and to scale to large scenes, and
hence the self-localization part of HPS builds upon them.

Humans and Scenes: The relationship between hu-
mans, scenes, and objects is a recurrent subject of study
in vision. Examples are methods for 2D pose and ob-
ject detection [15, 21, 27, 31, 48, 79], 3D object detection
using human poses [20, 22], learning to insert people in

scenes [19,35,75,84], constraining pose [24,83], estimating
forces [36], or predicting long term motion [13] conditioned
on the scene. Most approaches predict only static poses in a
single room, and reasoning is done from a third-person per-
spective. In contrast, our analysis is from a ﬁrst-person per-
spective, and uses the scene to self-localize the human in it.
Furthermore, our method enables to capture humans in mo-
tion in multiple-room and outdoor environments. All afore-
mentioned methods would beneﬁt from the HPS dataset.

3. Method

Our goal is to recover the 3D body pose and location of a
subject in a known scene from egocentric measurements. To
this end, our method requires as input: 1) a head-mounted
camera, 2) body-mounted IMUs, and 3) a pre-built 3D scan
of a scene, along with a database of RGB scene images
with known camera parameters. Using camera data, our
method localizes the person within a pre-scanned 3D scene
(Sec. 3.2), estimates their 3D pose using IMUs (Sec. 3.3),
and in a joint optimization step (Sec. 3.4) integrates cam-
era localization, IMU pose estimates and scene constraints,
resulting in smooth and accurate human motion estimates.
For an overview of our method, see Fig. 2. For more details
on the 3D scene reconstruction, image database collection,
camera and IMU setup, we refer to the supplementary.

3.1. SMPL Body Model

We use the Skinned Multi-Person Linear (SMPL) body
model [38] to represent the human subject. SMPL is a dif-
ferentiable function M (θ, t, β) : R72×3×10 (cid:55)→ R6890×3
that maps pose θ, translation t and shape β parameters

3

Joint OptimizationIMU-based pose estimationCamera self-localization3D scan of a sceneResultSYNTHETICREALFigure 3. Camera self-localization. We match the head camera
image keypoints with the keypoints from the preﬁltered database
with known 2D-3D scene correspondences. We then localize the
camera in the scene by minimizing a reprojection error of the key-
points. From top to bottom: head camera image (query), top-3 re-
trieved images from a dataset, depthmaps rendered from the same
position to map 2D database keypoints to 3D, synthetic view of
the scene from the inferred camera position.

to the vertices of a watertight human mesh. The underly-
ing skeleton of SMPL has 24 joints. The pose parameters
θ ∈ R72 correspond to the relative orientation of each joint
in the SMPL skeleton, expressed in axis-angles. The shape
parameters β ∈ R10 are the PCA coefﬁcients of a shape
space learnt from a corpus of registered scans. We use the
notation Mn(θ, t, β) ∈ R3 to indicate the nth vertex of
SMPL. We obtain approximate shape parameters β of a per-
son from body measurements. We assume that β remains
constant during a sequence and aim to recover θ and t of
the subject registered with the 3D environment. Henceforth
we drop β for notational convenience.

3.2. Camera Self-localization

The camera self-localization stage aims to estimate the
position and orientation of the human head from a head-
mounted camera. To scale to large scenes, we use a hi-
erarchical structure-based localization algorithm [57, 58]
(Fig. 3).
It ﬁrst identiﬁes a set of potentially relevant
database images, i.e., images used to build the 3D scene
map, through image retrieval via NetVLAD [5] descrip-
tors. 2D-3D matches are established between local Super-
Point [16] features extracted in the query image and 3D
points visible in the top-40 retrieved images. These matches
are then used to estimate the camera pose by applying a
P3P solver [23, 32, 33] inside a RANSAC loop [18] with
local optimization [34]. Rather than building a separate
sparse Structure-from-Motion point cloud for localization,

4

Figure 4. Comparison of the trajectories of IMUs (in green) with
camera self-localization (in red). The yellow dot marks the start.
Notice the red trajectory is free of drift but has outliers.

as originally used in [57], we obtain 3D point positions
from our dense scene 3D model [64]. For each pixel in a
database image, we obtain the corresponding 3D point by
rendering the 3D model from the known pose of the image.
2D-2D matches between the query and the top-40 retrieved
database images thus yield the required 2D-3D matches.
From the camera self-localization step, we obtain estimates
for camera orientation RC and position tC.

3.3. IMU based Pose Estimation

We use a commercial inertial mo-cap system provided
by XSens [47], which uses 17 IMUs attached to the body
with velcro-straps or a suit. XSens IMUs provide 3D pose
estimates, denoted as θI and location estimates relative to
the starting position of a recording - denoted as tI , using a
proprietary algorithm based on a Kalman ﬁlter and a kine-
matic model of the human body to reduce drift. While it
provides accurate articulation, our experiments show that
the global orientation and position drift signiﬁcantly over
time, and consequently scene constraints are not satisﬁed
(Fig. 4, 6). Using acceleration information, IMUs also de-
tect feet contacts with the ground, which we integrate in our
joint optimization algorithm.

3.4. Joint Optimization

Our joint optimization algorithm ﬁnds the pose param-
eters of the SMPL body model in order to satisfy i) the
head camera self-localization, ii) scene, and iii) smoothness
constraints while remaining as close as possible to the IMU
pose estimate θI (excluding global orientation and position)
– while we could optimize SMPL to match the raw IMU
data directly [71, 73], we chose not to, because it contains a
lot of drift. Mathematically, we minimize the following ob-

2D–3D keypointmatchingLocalizationHead cameraSynthetic viewRetrieved database images3D scenejective over a batch of T frames (T is ﬁxed for all scenes)

E(θ1:T , t1:T ) = wsEself +wscEscene+wsmEsm+wpEIMU,
(1)
with respect to pose θ1:T and translation t1:T parameters.
θ1:T ∈ R72T and t1:T ∈ R3T are stacked model poses and
translations for each time step j = 1 . . . T . In the following,
we explain each of the terms in more detail.

Self-localization Term Eself : We use the estimated orien-
tation of the camera to constrain the orientation of SMPL.
Speciﬁcally, we minimize the geodesic distance [73] from
the head camera orientation as inferred from SMPL,
(θ), to the self-localization estimate RC over a batch
R
of frames T :

C

Eself =

1
T

T
(cid:88)

j=1

||(log((R

C

(θj))(cid:62)RC

j ))∨||2 ,

(2)

where the log operation recovers the skew-symmetric ma-
trix from the relative rotation matrix, and the ∨ operator
converts it to its axis-angle representation. The mapping
(θj) can be derived as follows. First, we obtain the head
R
bone orientation by traversing the kinematic chain of SMPL

C

RH (θ) =

(cid:89)

(cid:86)

exp(θi

) ,

(3)

(cid:86)

i∈PHead
where PHead is an ordered list of all the parents to the head
joint. The ∧ operator maps an axis-angle to its correspond-
ing skew-symmetric matrix and exp(θi
) are the relative
joint rotation matrices obtained from θi ∈ so(3) using the
Rodrigues formula. While RH : R72 (cid:55)→ SO(3) maps from
pose to head rotation, we need a mapping to camera orienta-
tion. Since the camera is rigidly attached to the head, there
is a constant camera to head offset that can be estimated at
frame 0 [50, 73]:

RHC = (RH (θI

0))(cid:62)RC

0 .

(4)

We ﬁnd the desired mapping from pose to camera at a sub-
sequent frame j as R

(θj) = RH (θj) RHC.

C

Scene Contact Term Escene: When the IMUs detect a foot
contact, we force it to be in contact with the ground by us-
ing an energy term consisting of two subterms Escene =
wcEcontact + wvEslide. Let Bk with k ∈ [1, 2, 3, 4] denote
4 sets of manually deﬁned vertex indices in the SMPL cor-
responding to the toe and heel regions for the left and right
foot (more details in supplementary), and let ck
j ∈ [0, 1] be
a binary variable indicating if part k is in contact with the
ground at frame j. We deﬁne the following contact term,
which snaps the foot vertices to the closest scene vertices

Econtact =

1
4T

T
(cid:88)

4
(cid:88)

(cid:88)

j=1

k=1

n∈Bk

1
|Bk|

ck
j ||Mn(θj, tj) − v(n)(cid:107)2 ,

(5)

5

where Mn(θj, tj) is the nth vertex of the SMPL mesh at
(||Mn(θj, tj) − vs||2) returns
frame j, and v(n) = argmin
vs∈Vs

the closest scene point vs ∈ Vs to Mn(θj, tj). To prevent
the foot from sliding when in contact with the scene, we also
constrain the distance between foot parts in contact with the
scene in two successive frames to be zero.

Eslide =

1
4(T − 1)

T −1
(cid:88)

4
(cid:88)

(cid:88)

j=1

k=1

n∈Bk

1
|Bk|

j ck
ck

j+1||Mn(θj, tj)−

Mn(θj+1, tj+1)||2 .

(6)

Smoothness Term Esm: This term ensures smooth chang-
ing of the global translation and orientation, as well as head
orientation

Esm = wT ET + wGEG + wH EH ,

(7)

where the translation term equals:

ET =

1
T − 1

T −1
(cid:88)

j=1

||(tj − tj+1)||2 .

(8)

Deﬁning RG : R72 (cid:55)→ SO(3) as RG(θ) = exp(θG
) where
θG is the axis-angle representation of the root (global) joint,
the global orientation smoothness term is

(cid:86)

EG =

1
T − 1

T −1
(cid:88)

j=1

||(log((RG(θj))(cid:62)RG(θj+1)))∨||2 (9)

Using Eq (9), the head orientation smoothness term is en-
forced with an equivalent term replacing RG by RH .

Pose Term EIMU: The pose recovered by IMUs captures
the articulation of the body well, but is inaccurate for global
orientation and translation. Hence, we constrain the pose
parameters corresponding to the body to remain close to the
IMUs estimate. Let B be an identity matrix with zeros at
the diagonal entries corresponding to the root joint. With
this, the pose is regularized with the following equation:

EIMU =

1
T

T
(cid:88)

j=1

(cid:113)

(θj − θI

j )(cid:62)B(θj − θI

j ) .

(10)

For implementation details of our joint optimization algo-
rithm, please see the supplementary.

3.5. Initialization

Since the objective function in Eq. (1) is highly non-
convex, convergence to a good minimum hinges on good
initialization. We initialize translation parameters tj using
camera localization estimates tC
j . Camera localization re-
sults are typically noisy, so instead of using raw results we
ﬁrst detect outliers by computing the velocity of translation

between each result and its inlier neighbours. We mark
a result as an outlier if its velocity exceeds the threshold
(cid:15) = 3m/s. We repeat this process until convergence and
replace all outliers by interpolation.

For poses θ, the simplest choice is to initialize with
the IMU pose estimate θj = θI
j . However, the global
body orientation often deviates from the more accurate self-
localization trajectory (see Fig. 4, 6). Observing that the
body orientation is often perpendicular to the trajectory,
our idea is to rotate the IMU pose to align it to the self-
localization trajectory. To this end, we ﬁrst estimate the
tangent direction of the self-localization and IMU trajecto-
ries

vC

j =

j+γ − tC
tC
j
||tC
j+γ − tC
j ||2

,

vI

j =

j+γ − tI
tI
j
||tI
j+γ − tI
j ||2

,

(γ = 10 in our case) and correct the root orientation
exp(θI,G

) of the IMU pose with the following formula

(cid:86)

j

θI,G∗
j

= (log(exp(vI

j × vC
j

)exp(θI,G

)))∨ ,

(11)

j

(cid:86)

(cid:86)

(cid:86)

j × vC
j

) is the planar rotation that aligns vI
where exp(vI
j
with vC
j . For stationary frames, we use the correction ma-
trix of the last frame with non-zero velocity. We ﬁnd that in
practise, for stationary frames, this a good approximation.

3.6. Coordinate Frame Alignment

While the camera estimates RC and tC are in the 3D
scene coordinates, IMU estimates θI and tI are not. Before
the initialization step (Sec. 3.5) of our joint optimization al-
gorithm (Sec. 3.4), we align the IMU coordinate frame with
the 3D scene frame by ﬁnding a planar rotation R∗
A that
orients the SMPL head at frame zero RH (θI
0) to match the
camera orientation RC
0 at the same frame. Mathematically,
this entails minimizing the following objective

R∗

A = argmin
RA∈R

||(log(RARH (θI

0))(cid:62)RC

0 ))∨||2 .

(12)

We use the axis-angle parameterization to deﬁne the set
of rotation matrices R = {exp(xα(cid:86)) : x ∈ R}. where α =
[0, 0, 1](cid:62) is the z-axis unit vector. The IMU pose θI
j and
position tI
j estimate of each subsequent frame are aligned
to the 3D scene reference frame by

(cid:86)

θI,G
j = (log(R∗

Aexp(θI,G

j

)))∨ , tI

j = R∗

AtI

j .

(13)

4. Dataset

HPS allows us to collect the HPS dataset - a dataset of
3D humans interacting with large 3D scenes (300-1000 m2,
up to 2500 m2). Our dataset contains images captured from
a head-mounted camera coupled with the reference 3D pose
and location of the person in a pre-scanned 3D scene. We
capture 7 people in 8 large scenes performing activities such

6

Distance
traveled

At start
70 m
200 m
380 m

IMU IMU + Cam

IMU + Cam
(ﬁltered)

HPS
w\o scene

6.85
54.49
69.02
108.44

9.24
742.32
136.81
32.17

10.48
6.93
5.93
6.15

7.21
6.48
5.80
5.69

HPS

5.20
4.60
4.26
4.53

Table 1. Drift and cam. outliers: 3D error (in cm) for the subject
standing in A-pose after moving freely around the scene.

Distance
traveled

At start
70 m
200 m
380 m

IMU IMU + Cam

IMU + Cam
(ﬁltered)

HPS
w\o scene

6.77
51.57
61.11
100.44

2189.75
569.71
719.44
261.72

10.05
21.75
7.34
12.59

9.19
20.68
6.67
11.96

HPS

6.44
15.96
4.76
10.07

Table 2. Drift and cam. outliers (dynamic): 3D error (in cm)
for the subject walking, standing and leaning on the table, after
moving around the scene. Error is measured from the dynamic
ground truth point cloud to the result (3D mesh in motion). Rows
indicate distance traveled before evaluation.

as exercising, reading, eating, lecturing, using a computer,
making coffee, dancing. All subjects have agreed to re-
lease their data for research purposes. In total, the dataset
provides more than 300K synchronized RGB images cou-
pled with the reference 3D pose and location. We plan to
keep updating the dataset by adding more long-term mo-
tion recordings with a variety of scene interactions. Figure
7 shows qualitative results from our dataset. For more ex-
amples, please see the video [1].

5. Experiments

This section shows that HPS does not drift with time and
distance traveled, is robust to non-persistent camera local-
ization outliers, and satisﬁes scene constraints (feet stay on
the ground during contact, and do not slide).

Since this is the ﬁrst method to track humans in large
scenes, there exist no published baselines to compare to,
and ground truth 3D human pose and localization cannot
be obtained for unbounded areas like ours. Hence, we use
depth cameras to obtain ground truth dynamic point clouds
of the human in a small sub-area of the scene. Subjects are
then asked to move freely in the large scene, and return to
the sub-area, where we can evaluate accuracy and drift.

5.1. Quantitative Evaluation

We evaluate the accuracy of our method by comparing
our output SMPL mesh (including translation) with a dy-
namic ground-truth point cloud of the person obtained from
three synchronized and calibrated external depth cameras
(Azure Kinect [2]). We register the point cloud to the scene
in three steps involving camera self-localization, ICP, and
manual correction. For an explanation of the Kinect setup
and point cloud registration we refer to the supplementary.
We report the bidirectional Chamfer distance between the

Metric

IMU IMU + Cam

IMU + Cam
(ﬁltered)

HPS
w\o scene

Dist. to Surf.
Foot Sliding

188.38
0.92

39.8
52.09

0.95
1.75

0.32
2.00

HPS

0.056
0.90

Table 3. Foot contact: For frames when foot contact is detected,
we report (in cm) Distance to surface: Average distance between
foot vertices and the scene, and Foot Sliding: Average distance on
the surface plane between foot vertices in two successive frames.
Numbers are computed for a 3 minute long walking sequence.

Figure 6. Global body orientation improvement. Combining
the IMU pose with position from camera localization (IMU+Cam
(ﬁltered)) results in unnatural motion–the global body orientation
does not face the direction of movement. By contrast, HPS cor-
rectly estimates the the global orientation. We refer to the video at
project page [1] for more visual examples.

ter. IMU+Cam (ﬁltered) mitigates this, but lacks precision
and suffers from global orientation errors (Fig. 6). HPS
w\o scene further improves results, but without knowledge
about foot-scene contacts, it is easily misled by incorrect
camera localization, and the subject penetrates or ﬂies over
the ground. HPS results satisfy these scene constraints, and
consistently achieve the best accuracy. HPS is inaccurate
when ﬁltered camera localization fails for a long period (see
2nd and 4th rows of Table 2), but it can recover once the
camera can be well localized in nearby frames (see 3rd row
of Table 2). Overall, the analysis reveals that HPS does not
drift (error does not increase with distance traveled or time),
and is robust to non-persistent camera localization outliers.
For scenes with with persistent camera localization fail-
ures (outdoor scenes, indoor scenes with repetitive pat-
terns), we implemented a slightly modiﬁed version of HPS,
described in the supplementary.

Foot contacts: We also report in Table 3 the average
foot-to-scene distance and foot-sliding-along-the-surface
distance during contacts detected with the IMUs. HPS bet-
ter preserves foot contact with the surface than the base-
lines, and has slightly lower foot-sliding compared to the
raw IMU tracker, which also integrates constraints with a
virtual imaginary ground. Foot contacts in HPS result in
stable and natural motion, see Fig. 5, and the video [1].

5.2. Qualitative evaluation

In Fig. 5 we show the effect of foot contact constraints.
As we encourage contact with the scene surface each time a
contact is detected, the human mesh does not ﬂy in the air or
penetrate the ground like the baseline. The motion is more
stable and physically correct. In Fig. 7 we show examples of

Figure 5. Effect of integrating predicted 3D scene contacts. As a
baseline we used camera localization results for localizing SMPL
model. Red regions mark closest surface to feet, heels and toes are
colored with light blue and blue when IMUs detect ground contact.

SMPL model (result) and ground truth point cloud from
depth sensors without Procrustes alignment.

Movements: For quantitative evaluation, we record us-
ing the following protocol: a subject starts within the
recording volume of the three RGB-D sensors and performs
different actions including standing in A-pose, leaning on a
table and walking. The subject then leaves the recording
volume and moves within the scene, returns back and re-
peats the same actions inside that volume again. This is
repeated several times, each time choosing a different path.
Baselines: There are no established baselines to com-
pare to, as no other method tackles the same problem.
Hence, to understand the inﬂuence of each component, we
use the following baselines: 1) IMU: pure IMU tracker,
2) IMU+Cam: pose from IMU, and translation from
camera self-localization, 3) IMU+Cam (ﬁltered): Like
IMU+Cam but with ﬁltered camera outliers (same as in
Sec. 3.5), 4) HPS w\o scene: Optimization without 3D
scene contact constraints.

Drift and Outliers: In Tables 1 and 2, we compare HPS
to the baselines. We observe that the IMU-only method
drifts over time, particularly the global body translation
and orientation. IMU+Cam corrects drift with camera lo-
calization, but produces translation noise and severe jit-

7

Frame 867Frame 885IMU + Cam (filtered)HPSIMU + Cam (filtered)HPSFigure 7. We show qualitative results of our method. Our method can localize and estimate the 3D pose of people performing activities
as diverse as exercising, dancing, reading, sitting, eating, talking in a range of indoor and outdoor scenes, all without external cameras.

humans performing different actions including sitting, lean-
ing on a table, dancing or performing push-ups. For more
examples, please see the video at our project page [1].

6. Conclusions and Future Work

We introduced HPS, to the best of our knowledge, the
ﬁrst method to estimate full body pose registered with a
pre-scanned 3D environment from only wearable sensors.
We demonstrate that HPS produces natural human mo-
tion, removes the typical drift of pure IMU based systems,
and is robust to non-persistent camera localization outliers.
HPS is able to continuously track humans in large scenes
(300 − 1000m2) including multiple rooms and outdoors.

The error of HPS does not accumulate with time or dis-
tance traveled. However, if camera localization is inaccu-
rate for long periods of time, HPS performance deteriorates.
This can be seen in the errors, which range from 4cm to
15cm. Two factors inﬂuence localization accuracy: 1) Lack
of features, 2) scene changes between the static 3D scan and
the real images, captured from the head camera.

While HPS achieves a remarkable accuracy and stabil-
ity, many applications will require errors in localization and
pose of less than 1cm. We envision many exciting research
directions to improve HPS. First, a local map could be built
on the ﬂy to update the large static scene with objects that
move, and adding new objects. This would improve lo-
It
calization and allow interaction with dynamic objects.

8

is not inconceivable that, in the future, a dynamic 3D re-
construction of the world will be stored on the cloud, and
will be continuously updated from cameras worn by peo-
ple [3]. Second, camera localization could incorporate se-
mantics [9, 86], e.g. detecting static and reliable objects.
Third, while HPS integrates foot contacts, scene constraints
with other body parts can further improve results. More
powerful would be to learn a model to anticipate human
intent to improve tracking. For example, we could detect
when the person is about to sit on a chair, or about to grab
an object. Conversely, HPS can be used to build models
of environment interaction and navigation [43, 77] from hu-
man captures consisting of several hours, as we believe nat-
ural behavior arises only during long recordings. Fourth,
we want to combine HPS with virtual humans of appear-
ance [7, 8, 44, 46] to generate realistic data for training and
evaluation of 3D human analysis methods.

HPS is the ﬁrst step in a new exciting research direc-
tion. We will release the HPS dataset and code for research
use [1], and hope it will foster new methods to perceive and
model scenes and humans from an ego-centric perspective.

for

their

Acknowledgments: We thank Bharat Bhatnagar, Verica Lazova, Anna
Kukleva and Garvita Tiwari
This work is
partly funded by the DFG - 409792180 (Emmy Noether Programme,
project: Real Virtual Humans),
the EU Horizon 2020 project RI-
CAIP (grant agreeement No.857306), and the European Regional De-
velopment Fund under project
IMPACT (No. CZ.02.1.01/0.0/0.0/15
003/0000468).

feedback.

References

[1] http://virtualhumans.mpi-inf.mpg.de/hps/. 6, 7, 8
[2] Microsoft Azure Kinect, accessed November 15, 2020.

https://en.wikipedia.org/wiki/Azure Kinect. 6

[3] Project

Aria,

accessed

November

15,

2020.

https://about.fb.com/realitylabs/projectaria/. 8

[4] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,
and Marcus Magnor.
Tex2shape: Detailed full human
body geometry from a single image. In IEEE International
Conference on Computer Vision (ICCV), pages 2293–2303.
IEEE, Oct 2019. 2

[5] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-
jdla, and Josef Sivic. Netvlad: Cnn architecture for weakly
In Proceedings of the IEEE
supervised place recognition.
conference on computer vision and pattern recognition,
pages 5297–5307, 2016. 3, 4

[6] Bharat Lal Bhatnagar, Suriya Singh, Chetan Arora, and C.V.
Jawahar. Unsupervised learning of deep feature representa-
tion for clustering egocentric actions. In Proceedings of the
Twenty-Sixth International Joint Conference on Artiﬁcial In-
telligence, IJCAI-17, pages 1447–1453, 2017. 2

[7] Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian
Theobalt, and Gerard Pons-Moll. Combining implicit func-
tion learning and parametric models for 3d human recon-
In European Conference on Computer Vision
struction.
(ECCV). Springer, August 2020. 8

[8] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt,
and Gerard Pons-Moll. Multi-garment net: Learning to dress
3d people from images. In IEEE International Conference on
Computer Vision (ICCV). IEEE, oct 2019. 8

[9] Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan
Leutenegger, and Andrew J Davison. Codeslam—learning
a compact, optimisable representation for dense visual slam.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 2560–2568, 2018. 8

[10] Eric Brachmann and Carsten Rother. Expert Sample Con-
In The IEEE
sensus Applied to Camera Re-Localization.
International Conference on Computer Vision (ICCV), 2019.
3

[11] Eric Brachmann and Carsten Rother. Visual camera re-
localization from RGB and RGB-D images using DSAC.
arXiv:2002.12324, 2020. 3

[12] Congqi Cao, Yifan Zhang, Yi Wu, Hanqing Lu, and Jian
Cheng. Egocentric gesture recognition using recurrent 3d
convolutional neural networks with spatiotemporal trans-
former modules. 2017 IEEE International Conference on
Computer Vision (ICCV), 2017. 2

[13] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qizhi Cai, Minh
Vo, and Jitendra Malik. Long-term human motion prediction
with scene context. In ECCV. 2020. 3

[14] Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord,
Julien Valentin, Victor A. Prisacariu, Luigi Di Stefano, and
Philip H. S. Torr. Real-time rgb-d camera pose estimation in
novel scenes using a relocalisation cascade. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
2019. 3

[15] Chaitanya Desai and Deva Ramanan. Detecting actions,
In European
poses, and objects with relational phraselets.
Conference on Computer Vision, pages 158–172. Springer,
2012. 3

[16] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-
novich. Superpoint: Self-supervised interest point detection
and description. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops, pages
224–236, 2018. 4

[17] Alireza Fathi, Ali Farhadi, and James M. Rehg. Understand-
ing egocentric activities. In Proceedings of the International
Conference on Computer Vision (ICCV), 2011. 2

[18] M. Fischler and R. Bolles. Random Sampling Consensus: A
Paradigm for Model Fitting with Application to Image Anal-
ysis and Automated Cartography. Communications of the
ACM (CACM), 24:381–395, 1981. 4

[19] David F Fouhey, Vincent Delaitre, Abhinav Gupta, Alexei A
Efros, Ivan Laptev, and Josef Sivic. People watching: Hu-
man actions as a cue for single view geometry. International
journal of computer vision, 110(3):259–274, 2014. 3
[20] Helmut Grabner, Juergen Gall, and Luc Van Gool. What
In CVPR 2011, pages 1529–1536.

makes a chair a chair?
IEEE, 2011. 3

[21] Abhinav Gupta and Larry S Davis. Objects in action: An ap-
proach for combining action understanding and object per-
ception. In 2007 IEEE Conference on Computer Vision and
Pattern Recognition, pages 1–8. IEEE, 2007. 3

[22] Abhinav Gupta, Scott Satkin, Alexei A Efros, and Martial
Hebert. From 3d scene geometry to human workspace. In
CVPR 2011, pages 1961–1968. IEEE, 2011. 3

[23] R.M. Haralick, C.-N. Lee, K. Ottenberg, and M. N¨olle. Re-
view and analysis of solutions of the three point perspective
pose estimation problem. International Journal of Computer
Vision (IJCV), 13(3):331–356, 1994. 4

[24] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas,
and Michael J. Black. Resolving 3D human pose ambigui-
ties with 3D scene constraints. In Proceedings International
Conference on Computer Vision, pages 2282–2292. IEEE,
Oct. 2019. 2, 3

[25] Thomas Helten, Andreas Baak, Gaurav Bharaj, Meinard
Muller, Hans-Peter Seidel, and Christian Theobalt. Person-
alization and evaluation of a real-time depth-based full body
tracker. In International Conf. on 3D Vision, pages 279–286,
2013. 2

[26] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J.
Black, Otmar Hilliges, and Gerard Pons-Moll. Deep iner-
tial poser: Learning to reconstruct human pose from sparse
inertial measurements in real time. ACM Transactions on
Graphics, (Proc. SIGGRAPH Asia), 37(6):185:1–185:15,
nov 2018. 2

[27] Umar Iqbal, Martin Garbade, and Juergen Gall. Pose for
In 2017 12th IEEE International
action-action for pose.
Conference on Automatic Face & Gesture Recognition (FG
2017), pages 438–445. IEEE, 2017. 3

[28] Hao Jiang and Kristen Grauman. Seeing invisible poses:
In 2017
Estimating 3d body pose from egocentric video.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 3501–3509. IEEE, 2017. 2

9

[29] Eagle S. Jones and Stefano Soatto. Visual-inertial naviga-
tion, mapping and localization: A scalable real-time causal
approach. The International Journal of Robotics Research,
30(4):407–430, 2011. 2

[30] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In IEEE Conf. on Computer Vision and Pattern Recog-
nition, 2018. 2

[31] Hedvig Kjellstr¨om, Javier Romero, and Danica Kragi´c. Vi-
sual object-action recognition: Inferring object affordances
from human demonstration. Computer Vision and Image Un-
derstanding, 115(1):81–90, 2011. 3

[32] Laurent Kneip, Davide Scaramuzza, and Roland Siegwart. A
Novel Parametrization of the Perspective-Three-Point Prob-
lem for a Direct Computation of Absolute Camera Position
In IEEE Conference on Computer Vision
and Orientation.
and Pattern Recognition (CVPR), 2011. 4

[33] Zuzana Kukelova, Martin Bujnak, and Tomas Pajdla.
Closed-Form Solutions to Minimal Absolute Pose Problems
In Asian Conference on
with Known Vertical Direction.
Computer Vision (ACCV), 2010. 4

[34] Karel Lebeda, Juan E. Sala Matas, and Ondˇrej Chum. Fixing
the Locally Optimized RANSAC. In British Machine Vision
Conference (BMVC), 2012. 4

[35] Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, Ming-
Hsuan Yang, and Jan Kautz. Putting humans in a scene:
Learning affordance in 3d indoor environments. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 12368–12376, 2019. 3

[36] Zongmian Li, Jiri Sedlar, Justin Carpentier, Ivan Laptev,
Nicolas Mansard, and Josef Sivic. Estimating 3d motion and
forces of person-object interactions from monocular video.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 8640–8649, 2019. 3

[37] Liu Liu, Hongdong Li, and Yuchao Dai. Efﬁcient global
2d-3d matching for camera localization in a large-scale 3d
map. In Proceedings of the IEEE International Conference
on Computer Vision, pages 2372–2381, 2017. 3

[38] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. SMPL: A skinned multi-
person linear model. ACM Transactions on Graphics, 2015.
3

[39] Zhengyi Luo, S Alireza Golestaneh, and Kris M Kitani. 3d
human motion estimation via motion compression and re-
ﬁnement. In Proceedings of the Asian Conference on Com-
puter Vision, 2020. 2

[40] Simon Lynen, Torsten Sattler, Michael Bosse, Joel A Hesch,
Marc Pollefeys, and Roland Siegwart. Get out of my
lab: Large-scale, real-time visual-inertial localization.
In
Robotics: Science and Systems, volume 1, page 1, 2015. 2

[41] Minghuang Ma, Haoqi Fan, and Kris M. Kitani. Going
2016 IEEE
deeper into ﬁrst-person activity recognition.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 1894–1903, 2016. 2

[42] Charles Malleson, Marco Volino, Andrew Gilbert, Matthew
Trumble, John Collomosse, and Adrian Hilton. Real-time
full-body motion capture from video and imus. In 2017 Fifth
International Conference on 3D Vision (3DV), 2017. 2

[43] Manolis

Savva*,

Abhishek Kadian*,

Oleksandr
Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi
Parikh, and Dhruv Batra. Habitat: A Platform for Embodied
AI Research. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), 2019. 8

[44] Aymen Mir, Thiemo Alldieck, and Gerard Pons-Moll. Learn-
ing to transfer texture from clothing images to 3d humans. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR). IEEE, June 2020. 8

[45] Mohamed Omran, Christop Lassner, Gerard Pons-Moll, Pe-
ter Gehler, and Bernt Schiele. Neural body ﬁtting: Unifying
deep learning and model based human pose and shape esti-
mation. In International Conf. on 3D Vision, 2018. 2
[46] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-
Moll. Tailornet: Predicting clothing in 3d as a function of
human pose, shape and garment style. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). IEEE,
jun 2020. 8

[47] Monique Paulich, Martin Schepers, Nina Rudigkeit, and G.
Bellusci. Xsens MTw Awinda: Miniature Wireless Inertial-
Magnetic Motion Tracker for Highly Accurate 3D Kinematic
Applications, 05 2018. 4

[48] Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, and
Bernt Schiele. Poselet conditioned pictorial structures.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 588–595, 2013. 3

[49] Gerard Pons-Moll, Andreas Baak, Juergen Gall, Laura Leal-
Taix´e, Meinard Muller, Hans-Peter Seidel, and Bodo Rosen-
hahn. Outdoor human motion capture using inverse kine-
matics and von mises-ﬁsher sampling. In Proceedings of the
2011 International Conference on Computer Vision (ICCV),
pages 1243–1250, 2011. 2

[50] Gerard Pons-Moll, Andreas Baak, Thomas Helten,
Meinard M¨uller, Hans-Peter Seidel, and Bodo Rosen-
hahn. Multisensor-fusion for 3d full-body human motion
capture. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 663–670, 2010. 2, 5
[51] Gerard Pons-Moll and Bodo Rosenhahn. Model-Based Pose
Estimation, chapter 9, pages 139–170. Springer, 2011. 2
[52] Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafut-
dinov, Mohammad Shaﬁei, Hans-Peter Seidel, Bernt Schiele,
and Christian Theobalt. Egocap: egocentric marker-less mo-
tion capture with two ﬁsheye cameras. ACM Transactions on
Graphics (TOG), 35(6):162, 2016. 2

[53] Daniel Roetenberg, Henk Luinge, and Per Slycke. Moven:
Full 6dof human motion tracking using miniature inertial
sensors. Xsen Technologies, December, 2007. 2

[54] Gr´egory Rogez, James S Supancic, and Deva Ramanan.
First-person pose recognition using egocentric workspaces.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 4325–4333, 2015. 2

[55] Istv´an S´ar´andi, Timm Linder, Kai O Arras, and Bastian
Leibe. Metrabs: Metric-scale truncation-robust heatmaps for
absolute 3d human pose estimation. IEEE Transactions on
Biometrics, Behavior, and Identity Science, 2020. 2

10

[56] P.E. Sarlin, F. Debraine, M. Dymczyk, R. Siegwart, and C.
Cadena. Leveraging deep visual descriptors for hierarchi-
cal efﬁcient localization. In Conference on Robot Learning,
Zurich, Switzerland, October 2018. 3

[57] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and
Marcin Dymczyk. From coarse to ﬁne: Robust hierarchical
localization at large scale. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
12716–12725, 2019. 2, 3, 4

[58] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
SuperGlue: Learning Feature

and Andrew Rabinovich.
Matching with Graph Neural Networks. In CVPR, 2020. 4

[59] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efﬁcient
& effective prioritized matching for large-scale image-based
localization. IEEE transactions on pattern analysis and ma-
chine intelligence, 39(9):1744–1756, 2016. 2, 3

[60] Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura
Leal-Taixe. Understanding the limitations of cnn-based
In Proceedings of the
absolute camera pose regression.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019. 3

[61] Takaaki Shiratori, Hyun Soo Park, Leonid Sigal, Yaser
Sheikh, and Jessica K Hodgins. Motion capture from body-
mounted cameras. In ACM Transactions on Graphics (TOG),
volume 30, page 31. ACM, 2011. 3

[62] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram
Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene
Coordinate Regression Forests for Camera Relocalization in
RGB-D Images. In 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2013. 3

[63] Linus Sv¨arm, Olof Enqvist, Fredrik Kahl, and Magnus Os-
karsson. City-scale localization for cameras with known ver-
IEEE transactions on pattern analysis and
tical direction.
machine intelligence, 39(7):1455–1461, 2016. 3

[64] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea
Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and
localization with
Akihiko Torii.
In Proceedings of
dense matching and view synthesis.
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 2, 4

Indoor visual

InLoc:

[65] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea
Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak-
Indoor visual localization with dense
ihiko Torii.
In Proceedings of the IEEE
matching and view synthesis.
Conference on Computer Vision and Pattern Recognition,
pages 7199–7209, 2018. 3

Inloc:

[66] Carl Toft, Erik Stenborg, Lars Hammarstrand, Lucas Brynte,
Marc Pollefeys, Torsten Sattler, and Fredrik Kahl. Semantic
match consistency for long-term visual localization. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 383–399, 2018. 3

[67] Denis Tome, Thiemo Alldeick, Patrick Peluse, Gerard Pons-
Moll, Lourdes Agapito, Hernan Badino, and Fernando de la
Torre. Selfpose: 3d egocentric pose estimation from a head-
set mounted camera. IEEE Transactions on Pattern Analysis
and Machine Intelligence, Oct 2020. 2

[68] Akihiko Torii, Relja Arandjelovic, Josef Sivic, Masatoshi
Okutomi, and Tomas Pajdla. 24/7 place recognition by view

11

synthesis. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1808–1817,
2015. 3

[69] Matthew Trumble, Andrew Gilbert, Charles Malleson,
Adrian Hilton, and John Collomosse. Total capture: 3d
human pose estimation fusing video and inertial sensors.
In Proceedings of 28th British Machine Vision Conference,
pages 1–13, 2017. 2

[70] Daniel Vlasic, Rolf Adelsberger, Giovanni Vannucci, John
Barnwell, Markus Gross, Wojciech Matusik, and Jovan
Popovi´c. Practical motion capture in everyday surroundings.
ACM Transactions on Graphics (TOG), 26(3):35, 2007. 2

[71] Timo von Marcard, Roberto Henschel, Michael Black, Bodo
Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d
human pose in the wild using imus and a moving camera. In
European Conf. on Computer Vision, sep 2018. 2, 4

[72] T von Marcard, G. Pons-Moll, and B. Rosenhahn. Hu-
man pose estimation from video and IMUs. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
38(8):1533–1547, 2016. 2

[73] Timo von Marcard, Bodo Rosenhahn, Michael Black, and
Gerard Pons-Moll. Sparse inertial poser: Automatic 3d hu-
man pose estimation from sparse imus. Computer Graph-
ics Forum 36(2), Proceedings of the 38th Annual Conference
of the European Association for Computer Graphics (Euro-
graphics), pages 349–360, 2017. 2, 4, 5

[74] Florian Walch, Caner Hazirbas, Laura Leal-Taix´e, Torsten
Sattler, Sebastian Hilsenbeck, and Daniel Cremers. Image-
Based Localization Using LSTMs for Structured Feature
Correlation. In The IEEE International Conference on Com-
puter Vision (ICCV), 2017. 3

[75] Xiaolong Wang, Rohit Girdhar, and Abhinav Gupta. Binge
watching: Scaling affordance learning from sitcoms. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2596–2605, 2017. 3

[76] Tobias Weyand, Ilya Kostrikov, and James Philbin. Planet-
In
photo geolocation with convolutional neural networks.
European Conference on Computer Vision, pages 37–55.
Springer, 2016. 3

[77] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra
Malik, and Silvio Savarese. Gibson env: Real-world percep-
tion for embodied agents. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
9068–9079, 2018. 8

[78] Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer, Helge
Rhodin, Pascal Fua, Hans-Peter Seidel, and Christian
Theobalt. Mo2Cap2 : Real-time mobile 3d motion capture
with a cap-mounted ﬁsheye camera. IEEE Transactions on
Visualization and Computer Graphics, pages 1–1, 2019. 2

[79] Bangpeng Yao and Li Fei-Fei. Modeling mutual context of
object and human pose in human-object interaction activi-
ties. In 2010 IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition, pages 17–24. IEEE,
2010. 3

[80] H. Yonemoto, K. Murasaki, T. Osawa, K. Sudo, J. Shima-
mura, and Y. Taniguchi. Egocentric articulated pose tracking
for action recognition. In International Conference on Ma-
chine Vision Applications (MVA), 2015. 2

[81] Ye Yuan and Kris Kitani. 3d ego-pose estimation via imita-
tion learning. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 735–750, 2018. 2

[82] Ye Yuan and Kris Kitani. Ego-pose estimation and forecast-
ing as real-time pd control. In The IEEE International Con-
ference on Computer Vision (ICCV), October 2019. 2
[83] Andrei Zanﬁr, Elisabeta Marinoiu, and Cristian Sminchis-
escu. Monocular 3d pose and shape estimation of mul-
tiple people in natural scenes-the importance of multiple
In Proceedings of the IEEE Conference
scene constraints.
on Computer Vision and Pattern Recognition, pages 2148–
2157, 2018. 3

[84] Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael J
Black, and Siyu Tang. Generating 3d people in scenes with-
In Proceedings of the IEEE/CVF Conference
out people.
on Computer Vision and Pattern Recognition, pages 6194–
6204, 2020. 3

[85] Zerong Zheng, Tao Yu, Hao Li, Kaiwen Guo, Quionghai Dai,
Lu Fang, and Yebin Liu. Hybridfusion: Real-time perfor-
mance capture using a single depth sensor and sparse imus.
In European Conference on Computer Vision (ECCV), 2018.
2

[86] Shuaifeng Zhi, Michael Bloesch, Stefan Leutenegger, and
Andrew J Davison. Scenecode: Monocular dense semantic
reconstruction using learned encoded scene representations.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 11776–11785, 2019. 8

12

