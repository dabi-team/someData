NON-AUTOREGRESSIVE SEQUENCE-TO-SEQUENCE VOICE CONVERSION

Tomoki Hayashi1,2, Wen-Chin Huang2, Kazuhiro Kobayashi1,2, Tomoki Toda2

1TARVO Inc., Japan, 2Nagoya University, Japan,
hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp

1
2
0
2

r
p
A
4
1

]

D
S
.
s
c
[

1
v
3
9
7
6
0
.
4
0
1
2
:
v
i
X
r
a

ABSTRACT

This paper proposes a novel voice conversion (VC) method based on
In-
non-autoregressive sequence-to-sequence (NAR-S2S) models.
spired by the great success of NAR-S2S models such as FastSpeech
in text-to-speech (TTS), we extend the FastSpeech2 model for the
VC problem. We introduce the convolution-augmented Transformer
(Conformer) instead of the Transformer, making it possible to cap-
ture both local and global context information from the input se-
quence. Furthermore, we extend variance predictors to variance
converters to explicitly convert the source speaker’s prosody com-
ponents such as pitch and energy into the target speaker. The experi-
mental evaluation with the Japanese speaker dataset, which consists
of male and female speakers of 1,000 utterances, demonstrates that
the proposed model enables us to perform more stable, faster, and
better conversion than autoregressive S2S (AR-S2S) models such as
Tacotron2 and Transformer.

Index Terms— Voice conversion, non-autoregressive, sequence-

to-sequence, Transformer, Conformer

1. INTRODUCTION

Voice conversion (VC) is a technique to make it possible to convert
speech of the source speaker into that of the target speaker while
keeping the linguistic content [1]. VC is an essential technique for
various applications such as a speaking aid system [2], a vocal effec-
tor for singing voice [3], [4], and a real-time voice changer for the
online communication [5]. Conventional VC methods [6]–[8] are
based on the analysis-synthesis framework [9], [10], where speech
is decomposed into a spectral feature, pitch, and aperiodicity com-
ponents. Since the conventional methods convert each component
frame-by-frame, it is difﬁcult to convert the duration or prosody of
speech, resulting in limited conversion quality.

With the improvement of deep learning techniques, attention-
based sequence-to-sequence (S2S) VC models have attracted atten-
tion [11]–[15]. Thanks to the attention mechanism [16], S2S-based
VC models can learn the alignment between the source sequence
and the target sequence in a data-driven manner, making it possi-
ble to convert the duration and prosody components. However, the
attention-based S2S models require relatively a large amount of the
training data to get a good alignment [14], causing conversion er-
rors such as the deletion and repetition [12]. To address this issue,
various approaches have been proposed, such as the attention reg-
ularization training [11], monotonic attention mechanism [15], and
the use of the automatic speech recognition (ASR) or text-to-speech
(TTS) pretraining model [14].

Recently, non-autoregressive S2S (NAR-S2S) end-to-end TTS
(E2E-TTS) models have achieve the great performances [17]–[20].
The NAR-S2S models enable us to generate the output sequence

much faster than autoregressive S2S (AR-S2S) and alleviate gen-
eration errors derived from the attention mechanism. Furthermore,
FastSpeech2 [19] and FastPitch [20] have achieved faster, better, and
more stable generation than AR-S2S models by using extra acoustic
features such as pitch and energy.

This paper proposes a novel voice conversion method based
on NAR S2S model for the one-to-one VC. Inspired by the great
successes of NAR-E2E-TTS models [18]–[20], we extend Fast-
Speech2 [19] for the VC problem. The contributions are summarized
as follows:

• We introduce the convolution-augmented Transformer (Con-
former) [21] architecture instead of Transformer in Fast-
Speech2 [19]. The Conformer architecture enables us to
capture both local and global context information from the
input sequence, making the conversion quality better.

• We extend variance predictors, which predict pitch and en-
ergy from the token embedding, into variance converters,
converting the source speaker’s pitch and energy into the
target speaker’s one. The variance converters enable us to
convert the prosody components more accurately.

• We perform an experimental evaluation using a Japanese
speaker dataset, which consists of two speakers of 1000
utterances. The experimental results demonstrate that the
proposed method can outperform the conventional AR-S2S
models in both objective and subjective evaluation.

2. PROPOSED METHOD

2.1. Overview

The overview of the proposed method is shown in Fig. 1. The
proposed method consists of a source encoder, a duration predic-
tor, a length regulator, variance converters, a target decoder, and a
neural vocoder. The source encoder consists of Conformer-blocks,
and it converts Mel-spectrogram of the source speaker into a hid-
den representation sequence. The input Mel-spectrogram of the
source speaker is normalized to be mean zero and variance one
over the source speaker’s training data. To reduce the length of
the Mel-spectrogram, we introduce a reduction factor scheme [22],
which concatenates the several frames into one frame. The duration
predictor predicts each frame’s duration from the encoder hidden
representation (See section 2.3). To solve the mismatch between the
length of the source and the target, the length regulator replicates
each frame of the source components with duration information [18].
The variance converters convert the source speaker’s pitch and en-
ergy into that of the target speaker (See section 2.4). Then, the target
decoder, which consists of Conformer blocks and Postnet [23],
predicts the Mel-spectrogram of the target speaker with replicated

 
 
 
 
 
 
Fig. 1. An overview of the proposed method.

encoder hidden representations, converted pitch, and converted en-
ergy, and then the Postnet [23] reﬁnes it. We use the ground-truth of
duration, pitch, and energy as the decoder inputs during the training.
The encoder and decoder are optimized to minimize the mean ab-
solute error between the predicted Mel-spectrogram and the target
Mel-spectrogram. We use the same reduction factor scheme and
normalization for the target Mel-spectrogram, but the normalization
is performed over the target speaker’s training data. Finally, the
neural vocoder generates the waveform with the converted Mel-
spectrogram (See section 2.5).

2.2. Conformer-block

The Conformer is a convolution-augmented Transformer, which
achieves state-of-the-art performance in the ﬁeld of ASR [21]. The
architecture of the Conformer block is illustrated in Fig. 2. The
Conformer block basically consists of feed-forward modules, a
multi-head attention layer, and a convolution module. The feed-
forward module consists of a LayerNorm layer, a linear layer with
Swish activation [24] and dropout, and a linear layer with dropout.
The ﬁrst linear layer expands the dimension four times, and the
second one projects back to the original input dimension. After both
linear layers, we multiply the outputs by 0.5 following the method
described in the original Conformer paper [21]. The multi-headed
attention uses the relative sinusoidal positional encoding introduced
in Transformer-XL [25], which makes the model more robust to the
various inputs of different lengths. The convolution module consists
of a LayerNorm layer, a 1 × 1 1D convolution layer with a gated
linear unit (GLU) activation [26], and a 1D depth-wise convolution
layer. The depth-wise convolution layer is followed by a 1D batch
normalization layer, a Swish activation, and a 1 × 1 1D convolution
layer with dropout.

2.3. Duration predictor

We follow the variance predictor architecture in [19], which consists
of several layers, including 1D convolution and ReLU activation fol-
lowed by LayerNorm and dropout. The duration predictor is trained
to minimize the mean square error between the predicted duration
and the ground-truth in the log domain. To obtain the ground-truth,
we train the AR-Transformer model [14] and then calculate the du-
ration from the attention weights of the source-target diagonal atten-
tion by following the manner in FastSpeech [18]. We use teacher

Fig. 2. An architecture of the Conformer-block.

forcing in calculating attention weights to obtain the duration, the
length of which is matched with the target Mel-spectrogram. We
can use any attention-based S2S model to obtain the ground-truth
duration, but in our preliminary experiments, Transformer gives us
more concrete alignment than the Tacotron2-based model.

2.4. Variance converter

The variance converters follow the same architecture of the duration
predictor except for the inputs. We extract continuous log pitch and
energy from the waveform of both the source speaker and the target
speaker and then normalize them to be mean zero and variance one
over each speaker’s training data. We use Dio and Stonemask al-
gorithm [27] with PyWorld1 to extract pitch. The extracted pitch or
energy is inputted into an additional 1D convolution layer to become
the same dimension of the encoder hidden representations, and we
use their summation as the inputs of the variance converters. Unlike
FastSpeech2 [19], we do not perform discretization in the variance
converters since we can use the source pitch and energy as the input.
We optimize the variance converters to minimize the mean square
errors and stop the gradient ﬂow from the pitch converter to the en-
coder to avoid overﬁtting.

2.5. Neural vocoder

The proposed model can use any neural vocoder which accepts Mel-
In this study, we use Parallel Wave-
spectrogram as the inputs.
GAN [28] as the neural vocoder since it can generate a waveform
faster than the real-time and can be trained with a limited amount of
the training data (e.g., less than one hour). We use an open-source
implementation available in Github [29].

1https://github.com/JeremyCCHsu/

Python-Wrapper-for-World-Vocoder

Source	EncoderVariance	AdaptorTarget	DecoderNeural	VocoderSourceMel-spectrogramConvertedMel-spectrogramConvertedWaveformDurationPredictorPitchConv1DEnergyConv1DSource	PitchSource	EnergyEnergyConverterPitchConverterConverted	EnergyConverted	PitchPitchConv1DEnergyConv1DDurationLength	RegulatorFeedForwardModuleConvolutionModuleFeedForwardModuleLayerNormLayerNormDropout1×1	Conv1DSwishBatchNormDepthwiseConv1DGLU1×1	Conv1DLinear×4	dim×1/4	dim×2	dim×1/2	dimLayerNormSwishDropoutLinearDropoutLayerNormRelative	MHADropoutTable 1. Objective evaluation results. The values in MCD and log F0 RMSE represent the mean and the standard deviation.

Male −→ Female

Female −→ Male

Method

MCD [dB]

log F0 RMSE CER [%] MCD [dB]

log F0 RMSE CER [%]

Ground-truth

N/A

N/A

Tacotron2
Transformer
Proposed

6.33 ± 0.36
6.11 ± 0.63
5.71 ± 0.32

0.16 ± 0.03
0.16 ± 0.03
0.15 ± 0.02

10.6

24.9
19.6
15.3

N/A

N/A

5.30 ± 0.20
5.28 ± 0.79
4.90 ± 0.22

0.22 ± 0.04
0.21 ± 0.05
0.21 ± 0.04

11.3

19.4
21.1
14.8

3. EXPERIMENTAL EVALUATION

3.1. Experimental condition

To demonstrate the proposed method’s performance, we conducted
experimental evaluations using the Japanese speech parallel dataset.
The dataset consists of one female speaker and one male speaker of
1,000 utterances. The dataset is recorded in a low reverberation and
quiet room, and the sampling rate is a 24 kHz sampling rate. We
used 950 utterances for the training, 25 utterances for the validation,
and 25 utterances for the evaluation.

The implementation was based on the open-source ESPnet
toolkit [30]. We extracted an 80-dimensional Mel-spectrogram with
2,048 FFT points, 300 shift points, and a hanning window of 1,200
points. The reduction factor was set to three for both the source
and the target. The number of encoder blocks and decoder blocks
was set to four, and heads in multi-headed attention were set to
two. The attention dimension was set to 384, and the kernel size
of the convolutional module in the Conformer block is seven. The
number of channels in the duration predictor and variance converters
was set to 256. The number of layers of the duration predictor and
the energy converter was two, and their kernel size was three. The
number of layers of the pitch converter was ﬁve, and the kernel size
in the pitch converter was ﬁve. We trained 100k iterations using
Noam optimizer [31], and the warmup steps were set to 4,000. The
neural vocoder was trained with the ground-truth Mel-spectrogram
of the target speaker based on parallel_wavegan.v1 conﬁgu-
ration available in [29]. As baseline methods, we used the following
models:

Tacotron2: Tacotron2-based attention S2S VC. We followed the
setting described in [11] with the guided attention loss and the
source and target reconstruction loss. The Mel-spectrogram
was used instead of the WORLD [10]-based acoustic features.

Transformer: Transformer-based attention S2S VC [14]. We fol-
lowed the original implementation in ESPnet, but we added
the LayerNorm layer in the front of each Transformer-block,
which makes the training more stable.

We use the same feature extraction setting, reduction factors, and
the neural vocoder for the baselines. The evaluation samples are
publicity available in [32].

3.2. Objective evaluation results

As the objective evaluation metrics, we used Mel-cepstrum distor-
tion (MCD), root mean square error (RMSE) of log F0, and char-
acter error rate (CER). To get the alignment between the prediction
and the reference, we used dynamic time warping (DTW). To calcu-
late the MCD, we calculated 0-34 order Mel-cepstrum with SPTK

Fig. 3. Examples of alignment failures in Transformer.

Table 2. Conversion speed comparison. The values represents the
mean of the standard deviation. The time of waveform generation
was not included.

Conversion speed [frames / sec]

Method

On CPU

On GPU

Tacotron2
Transformer
Proposed

1240.0 ± 47.4
225.6 ± 13.2
3640.1 ± 745.6

2147.3 ± 200.6
359.2 ± 18.0
16723.7 ± 3176.7

toolkit2. The CER was calculated by the Transformer-based ASR
model trained on the corpus of spontaneous Japanese (CSJ) [33],
which was provided by ESPnet [34].

The objective evaluation result is shown in Table 1. The re-
sults show that the proposed method outperformed the baselines in
both male to female and female to male conversions. The Trans-
former model usually gave better results than the Tacotron2 model,
but sometimes the decoding was failed due to the alignment error, as
shown in Fig. 3 This caused the repetition of speech, and it resulted
in a higher standard deviation of MCD or worse CER. On the other
hand, the proposed method could alleviate the above problem thanks
to the NAR architecture.

Table 2 shows the conversion speed comparison. The evaluation
was conducted with eight threads of CPUs (Xeon Gold, 3.00 GHz)
and a single GPU (NVIDIA TITAN V). The result shows that the
proposed method can perform the conversion mush faster than AR
S2S models, thanks to the NAR architecture. To realize the real-
time streaming voice conversion, we will consider the extension to
the streaming model in future work.

2http://sp-tk.sourceforge.net/

Table 3. Subjective evaluation results. The values represent the mean and 95 % conﬁdence interval.

Male −→ Female

Female −→ Male

Average

Method

Naturalness

Similarity

Naturalness

Similarity

Naturalness

Similarity

Groundtruth
Tacotron2
Transformer
Proposed

3.67 ± 0.14
2.41 ± 0.13
3.09 ± 0.14
3.26 ± 0.14

N/A

4.51 ± 0.14
58% ± 7% 3.01 ± 0.15
85% ± 5% 3.43 ± 0.15
85% ± 5% 3.64 ± 0.15

N/A

4.12 ± 0.09
52% ± 7% 2.71 ± 0.10
76% ± 6% 3.27 ± 0.11
78% ± 6% 3.47 ± 0.10

N/A
55% ± 5%
81% ± 4%
82% ± 4%

Table 4. Ablation study results on the male to female conversion.
Conf. and V.C. represents the use of Conformer and variance con-
verters. We use the Transformer-block instead of the Conformer-
block when no Conformer was used.

Conf. V.C.

Teacher

MCD

log F0 RMSE CER

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

Transformer
Transformer
Transformer
Tacotron2
Tacotron2

5.71 ± 0.32
5.82 ± 0.33
6.08 ± 0.63
5.84 ± 0.29
5.87 ± 0.33

0.15 ± 0.02
0.15 ± 0.02
0.16 ± 0.02
0.15 ± 0.02
0.15 ± 0.02

15.3
16.1
20.4
19.7
17.8

3.3. Subjective evaluation results

We conducted subjective evaluation tests on naturalness and speaker
similarity to check the perceptual quality. For naturalness, each sub-
ject evaluated 50 samples and rated the naturalness of each sample
on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor,
and 1 for bad. For speaker similarity, each subject evaluated 40 pairs
of the target sample and the converted sample to judge whether the
presented samples were produced by the same speaker with conﬁ-
dence (e.g., sure or not sure). The number of subjects was 36 of
Japanese native speakers.

Table 3 shows the subjective evaluation results. The result
demonstrated that the proposed method outperformed the conven-
tional AR-S2S models in both conversion conditions, showing the
proposed method’s effectiveness.

3.4. Ablation study

Next, we conducted an ablation study to check the effectiveness of
each extension. The results of the objective evaluation is shown in
The result showed that both the use of Conformer and
Table 4.
that of variance converters could improve the performance. If we do
not use the Conformer and variance converters (this case is similar
to FastSpeech architecture), the performance is almost the same as
the Transformer. This implied that the NAR model’s upper bound
without variance converters are the teacher model. The result also
showed that the teacher model’s duration affected the performance.
One of the reasons is that the Transformer can generate more con-
crete alignment between the output and the input, i.e., accurate du-
ration than the Tacotron2. The comparison of the attention weights
is shown in Fig.4. Since NAR S2S models do not need to learn data-
driven alignments, it is expected that the required training data is
much smaller than AR S2S models. To take the above advantage, we
will consider another method to obtain the duration in future work.

Fig. 4. Comparison of alignments Tacotron2 (left, the focus rate [18]
is 0.41) vs. Transformer (right, the focus rate is 0.93). Transformer’s
alignment was obtained from the most diagonal source-target atten-
tion.

4. CONCLUSION

In this paper, we proposed a novel NAR S2S VC method based on
FastSpeech2. The experimental evaluation results showed that the
proposed method outperformed the conventional AR-S2S models
and converted speech much faster than them.
In future work, we
will consider the extension to the streaming model with a causal ar-
chitecture, the obtainment of the ground-truth duration without AR-
S2S models, and extension to many-to-many voice conversion with
learnable embedding.

5. ACKNOWLEDGEMENT

This work was partly supported by JSPS KAKENHI Grant-in-Aid
for JSPS Research Fellow Number 19K20295, and by JST, CREST
Grant Number JPMJCR19A3.

6. REFERENCES

[1] T. Toda, “Augmented speech production based on real-
time statistical voice conversion,” in Proc. GlobalSIP, 2014,
pp. 592–596.

[2] A. B. Kain, J.-P. Hosom, X. Niu, J. P. Van Santen, M.
Fried-Oken, and J. Staehely, “Improving the intelligibility
of dysarthric speech,” Speech communication, vol. 49, no. 9,
pp. 743–759, 2007.

[3] F. Villavicencio and J. Bonada, “Applying voice conversion
to concatenative singing-voice synthesis,” in Proc. Inter-
speech, 2010, pp. 2162–2165.

[4] K. Kobayashi, T. Toda, H. Doi, T. Nakano, M. Goto, G. Neu-
big, S. Sakti, and S. Nakamura, “Voice timbre control based
on perceived age in singing voice conversion,” IEICE Trans-
actions on Information and Systems, vol. 97, no. 6, pp. 1419–
1428, 2014.

[5] T. Toda, T. Muramatsu, and H. Banno, “Implementation
of computationally efﬁcient real-time voice conversion,” in
Proc. Interspeech, 2012, pp. 94–97.

[6] T. Toda, A. W. Black, and K. Tokuda, “Voice conversion
based on maximum-likelihood estimation of spectral param-
eter trajectory,” IEEE Transactions on Audio, Speech, and
Language Processing, vol. 15, no. 8, pp. 2222–2235, 2007.

[7] K. Kobayashi, T. Toda, G. Neubig, S. Sakti, and S. Naka-
mura, “Statistical singing voice conversion with direct wave-
form modiﬁcation based on the spectrum differential,” in
Proc. Interspeech, 2014, pp. 2514–2518.

[8] L. Sun, S. Kang, K. Li, and H. Meng, “Voice conversion
using deep bidirectional long short-term memory based re-
current neural networks,” in Proc. ICASSP, 2015, pp. 4869–
4873.

[9] H. Kawahara, “STRAIGHT, exploitation of the other as-
pect of vocoder: Perceptually isomorphic decomposition of
speech sounds,” Acoustical science and technology, vol. 27,
no. 6, pp. 349–353, 2006.

[10] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A
vocoder-based high-quality speech synthesis system for
real-time applications,” IEICE Transactions on Information
and Systems, vol. 99, no. 7, pp. 1877–1884, 2016.

[11] K. Tanaka, H. Kameoka, T. Kaneko, and N. Hojo, “AttS2S-
VC: Sequence-to-sequence voice conversion with attention
and context preservation mechanisms,” in Proc. ICASSP,
2019, pp. 6805–6809.

[12] H. Kameoka, K. Tanaka, T. Kaneko, and N. Hojo, “ConvS2S-
VC: Fully convolutional sequence-to-sequence voice conver-
sion,” arXiv preprint arXiv:1811.01609, 2018.

[13]

J.-X. Zhang, Z.-H. Ling, Y. Jiang, L.-J. Liu, C. Liang, and
L.-R. Dai, “Improving sequence-to-sequence voice conver-
sion by adding text-supervision,” in Proc. ICASSP, 2019,
pp. 6785–6789.

[14] W.-C. Huang, T. Hayashi, Y.-C. Wu, H. Kameoka, and T.
Toda, “Voice transformer network: Sequence-to-sequence
voice conversion using transformer with text-to-speech pre-
training,” arXiv preprint arXiv:1912.06813, 2019.

[15] H. Kameoka, W.-C. Huang, K. Tanaka, T. Kaneko, N. Hojo,
and T. Toda, “Many-to-many voice transformer network,”
arXiv preprint arXiv:2005.08445, 2020.

[16] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine
translation by jointly learning to align and translate,” arXiv
preprint arXiv:1409.0473, 2014.

[17] K. Peng, W. Ping, Z. Song, and K. Zhao, “Parallel neural

text-to-speech,” arXiv preprint arXiv:1905.08459, 2019.

[18] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and
T.-Y. Liu, “Fastspeech: Fast, robust and controllable text to
speech,” in Proc. NeurIPS, 2019, pp. 3165–3174.

[19] Y. Ren, C. Hu, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, “Fast-
Speech 2: Fast and high-quality end-to-end text-to-speech,”
arXiv preprint arXiv:2006.04558, 2020.

[20] A. La´ncucki, “FastPitch: Parallel text-to-speech with pitch
prediction,” arXiv preprint arXiv:2006.06873, 2020.

[21] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu,
W. Han, S. Wang, Z. Zhang, Y. Wu, et al., “Conformer:
Convolution-augmented Transformer for speech recogni-
tion,” arXiv preprint arXiv:2005.08100, 2020.

[22] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N.
Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, et al., “Tacotron:
Towards end-to-end speech synthesis,” in Proc. Interspeech,
2017, pp. 4006–4010.

[23]

J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z.
Yang, Z. Chen, Y. Zhang, Y. Wang, R. J. Skerry-Ryan, R. A.
Saurous, Y. Agiomyrgiannakis, and Y. Wu, “Natural TTS
synthesis by conditioning WaveNet on Mel spectrogram
predictions,” in Proc. ICASSP, 2018, pp. 4779–4783.

[24] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for ac-
tivation functions,” arXiv preprint arXiv:1710.05941, 2017.

[25] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and
R. Salakhutdinov, “Transformer-XL: Attentive language
models beyond a ﬁxed-length context,” arXiv preprint
arXiv:1901.02860, 2019.

[26] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Lan-
guage modeling with gated convolutional networks,” in Proc.
ICML, 2017, pp. 933–941.

[27] M. Morise, H. Kawahara, and T. Nishiura, “Rapid F0 estima-
tion for high-SNR speech based on fundamental component
extraction,” IEICEJ Transactions on Information ans Sym-
stems, vol. 93, pp. 109–117, 2010.

[28] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN:
A fast waveform generation model based on generative ad-
versarial networks with multi-resolution spectrogram,” in
Proc. ICASSP, 2020, pp. 6199–6203.

[29] T. Hayashi, kan-bayashi/ParallelWaveGAN, https : / /
github . com / kan - bayashi / ParallelWaveGAN,
2019.

[30] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watan-
abe, T. Toda, K. Takeda, Y. Zhang, and X. Tan, “ESPnet-
TTS: Uniﬁed, reproducible, and integratable open source
end-to-end text-to-speech toolkit,” in Proc. ICASSP, 2020,
pp. 7654–7658.

[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all
you need,” in Proc. NeurIPS, 2017, pp. 5998–6008.

[32] T. Hayashi, kan-bayashi/NonARSeq2SeqVC, https : / /
kan - bayashi . github . io / NonARSeq2SeqVC,
2020.

[33] K. Maekawa, “Corpus of spontaneous Japanese: Its design
and evaluation,” in Proc. ISCA & IEEE Workshop on Spon-
taneous Speech Processing and Recognition, 2003.

[34] S. Watanabe, T. Hori, S. Karita, T. Hayashi, et al., “ESPnet:
End-to-end speech processing toolkit,” in Proc. Interspeech,
2018, pp. 2207–2211.

