Comparison-Based Convolutional Neural Networks for Cervical Cell/Clumps
Detection in the Limited Data Scenario

Yixiong Liang, Zhihong Tang, Meng Yan, Jialin Chen, Qing Liu, Yao Xiang‚àó

School of Computer Science and Engineering, Central South University, Changsha 410083, China

9
1
0
2
c
e
D
3
2

]

V
C
.
s
c
[

5
v
2
5
9
5
0
.
0
1
8
1
:
v
i
X
r
a

Abstract

Automated detection of cervical cancer cells or cell clumps has the potential to signiÔ¨Åcantly reduce error rate and
increase productivity in cervical cancer screening. However, most traditional methods rely on the success of accurate
cell segmentation and discriminative hand-crafted features extraction. Recently there are emerging deep learning-based
methods which train convolutional neural networks (CNN) to classify image patches, but they are computationally
expensive. In this paper we propose an eÔ¨Écient CNN-based object detection methods for cervical cancer cells/clumps
detection. SpeciÔ¨Åcally, we utilize the state-of-the-art two-stage object detection method, the Faster-RCNN with Feature
Pyramid Network (FPN) as the baseline and propose a novel comparison detector to deal with the limited data problem.
The key idea is that classify the proposals by comparing with the reference samples of each category in object detection.
In addition, we propose to learn the reference samples of the background from data instead of manually choosing them by
some heuristic rules. Experimental results show that the proposed Comparison Detector yields signiÔ¨Åcant improvement
on the small dataset, achieving a mean Average Precision (mAP) of 26.3% and an Average Recall (AR) of 35.7%, both
improving about 20 points compared to the baseline. Moreover, Comparison Detector improved AR by 4.6 points and
achieved marginally better performance in terms of mAP compared with baseline model when training on the medium
dataset. Our method is promising for the development of automation-assisted cervical cancer screening systems. Code
is available at https://github.com/kuku-sichuan/ComparisonDetector.

Keywords: Cervical cancer screening, object detection, prototype representations, few-shot learning

1. Introduction

Cervical cytology is the most common and eÔ¨Äective screening method for cervical cancer and premalignant cervical
lesions [1], which is performed by a visual examination of cytopathological analysis under the microscope of the collected
cells that have been smeared on a glass slide and stained and Ô¨Ånally giving a diagnosis report according to the descriptive
diagnosis method of the Bethesda system (TBS)[2]. Currently in developed countries, it has been widely used and has
signiÔ¨Åcantly reduced the number of deaths caused by related diseases, but it is still unavailable for population-wide
screening in the developing countries [3], partly due to the fact that it is labor-intensive, time-consuming and expensive
[4]. In addition, it is subjective and therefore has motivated lots of automated methods for the automation of cervical
screening based on the image analysis techniques.

Over the past 30 years extensive research has attempted to develop automation-assisted screening methods [5, 6]. Most
of them try to classify a single cell into various stages of carcinoma, which often consists three steps: cell (cytoplasm and
nuclei) segmentation, feature extraction and classiÔ¨Åcation. The performance of these methods, however, heavily depends
on the accuracy of the segmentation and the eÔ¨Äectiveness of the hand-crafted features.

With the overwhelming success in a broad range of applications such as image classiÔ¨Åcation [7, 8], semantic segmen-
tation [9], object detection [10, 11] and medical imaging analysis [12, 13], CNN has also been applied to the segmentation
and classiÔ¨Åcation of cervical cell [14, 15, 16, 17, 18]. The majority of them (e.g. [14, 15]) are trying to take advantage of
CNN to improve the segmentation accuracy of cytoplasm and nuclei, but they do not provide the needed segmentation
accuracy [16, 18], whereas once the segmentation error are taken into account, the classiÔ¨Åcation accuracy would drop [17].
To avoid the dependence on accurate segmentation, the patch-based methods (e.g. [18]) try to use CNN to classify the
image patches. However, the extraction of such patches still requires the segmentation of nuclei. The recent work [17]
also adopts the patch-based strategy but during the inference the random-view aggregation and multiple crop testing are
needed to produce the Ô¨Ånal prediction results and thereby is time-consuming.

‚àóCorresponding author.
Email addresses: yxliang@csu.edu.cn (Yixiong Liang), zhihongtang@csu.edu.cn (Zhihong Tang), bryant@csu.edu.cn (Meng Yan),

yao.xiang@csu.edu.cn (Yao Xiang)

Preprint submitted to Neurocomputing

December 24, 2019

 
 
 
 
 
 
Figure 1: The overall structure of Comparison detector. First, features of n √ó K reference samples are obtained by ResNet50 with FPN,
where K is number of category and n is number of each category. Then, it generates the prototype representations for each category by the
Generating Prototype Representation Block (GPRB). At the same time, it obtains the features of proposals from the whole image
through ResNet50 with FPN and RPN. It should be noted that the ResNet50 with FPN is shared. Finally, by comparing the features of each
proposal with all prototype representations, we can get the category of this proposal. Only the feature of proposals are used to Ô¨Åne-tune the
bounding box.

In this paper, we propose an eÔ¨Écient strategy to apply CNN for cervical cancer screening, without any pre-segmentation
step. SpeciÔ¨Åcally, we exploit the contemporary CNN-based object detection methods [10, 11] to detect the cervical
cytological abnormalities directly. It is straightforward and has been successfully applied for other medical image analysis
[19, 20], but very few works try to apply CNN-based object detection for automated cervical cytology. We attribute this
to the lack of the right cervical cancer microscopic image dataset for the detection task. CNN-based object detection
methods often need suÔ¨Écient annotated data to obtain good generalization, but for cervical cytological abnormalities
detection, collecting the large amounts of data with careful and accurate annotation is diÔ¨Écult partially due to the
limitation by laws, the scarcity of positive samples and especially the unanimous agreement between cytopathologists
[21].

To alleviate the limited data problem, we propose the named Comparison Detector, which migrate the idea of compar-
ison in one/few-shot learning for image classiÔ¨Åcation [22, 23, 24, 25] into CNN-based object detection, for cervical cancer
detection. SpeciÔ¨Åcally, we choose the state-of-the-art object detection method, Faster R-CNN [10] with FPN [11], as our
baseline model and replace the original parameter classiÔ¨Åer with a non-parametric one based on the idea of comparison
with the prototype representations of each category, which is generated from reference samples. Furthermore, instead of
manually choosing the reference images of the background category by some heuristic rules, we propose to learn them
from the data. We also investigate several important factors including the generation of prototype representations of
each category and the design of head model for cervical cell or cell clumps detection. Our algorithm directly operates on
the whole image rather than the extracted patches based on the nuclei and hereby only need one forward propagation for
each image, making the inference very eÔ¨Écient. In addition, the proposed method is Ô¨Çexible to be integrated into other
proposal-based methods.

We collect a small dataset Ds and a medium dataset Df which are directly dedicated to cervical cell/clumps detection,
on which we evaluate the performance of the proposed Comparison detector. When the model is learned from the small
dataset Ds, the performance of our method is signiÔ¨Åcantly better than the baseline model, i.e. Comparison detector has
a mAP 26.3% and an AR 35.7% but the baseline model only gains a mAP 6.6% and an AR 12.9%. When the model is
learned from the medium dataset Df , our Comparison detector achieves performance with a mAP of 45.9% compared to
45.2%, and improves nearly 5 points comparing to baseline model with AR.

We summarize our contributions as follows: 1) We propose an end-to-end object detection method called Comparison
detector to deal with the limited data problem in cervical cell/clumps detection; 2) We propose a strategy to directly
learn the prototype representations of background and 3) Our method performs much better than the baseline on both
small and medium dataset and has the potential applications to the real automation-assisted cervical cancer screening
systems.

2

Feature of proposalsPrototype representation of categoriesùê∂ùëÅClassificationBounding box fine-tuningReference samplesGPRBFPNWhole imageRPNFPN.......ùêπ1ùêπ2ùêπùêæ+1.......ùëÉ1ùëÉ2ùëÉùëÅ..CCC.......-d(ùêπùëò,ùëÉ1)Comparison operationsoftmaxùêπ1ùêπ2ùêπùêæ+1ùëÉ1ùëÉ1ùëÉ1.ùëù(ùëèùëúùë•1=ùëò|ùúÉ)(a)

(b)

Figure 2: Generating Prototype Representation Block (GPRB).K is number of categories exclude background. W, H and C are the
width, height and channels of prototype representation. We called the K categories as foreground category. (a): The process of generating
one foreground prototype representation from pyramid features of reference samples. (b): The operation of generating background prototype
representation through foreground prototype representations and concatenating with K foreground prototype representations.

2. Related work

2.1. cell segmentation and classiÔ¨Åcation

Traditional cytological criteria for classifying cervical cell abnormalities are based on the changes in nucleus to cyto-
plasm ratio, nuclear size, irregularity of nuclear shape and membrane, therefore there are numerous works focusing on the
segmentation of cell or cell components (nuclei, cytoplasm) [26, 27, 28]. Although signiÔ¨Åcant progress has been achieved
recently, the segmentation of cell or cell components remains an open problem due to the large shape and appearance
variation between cells, the poor contrast of cytoplasm boundaries and the overlap between cells [17, 28, 16].

On the other hand, cervical cell classiÔ¨Åcation methods try to diÔ¨Äerentiate the dysplastic cells from the norm cells
and classify them into various stages of carcinoma. According to TBS rules [2], a large number of hand-crafted features
are designed to describe the shape, texture and appearance characteristics of the nucleus and cytoplasm [29, 6]. The
resulting features are often further organized by feature selection or dimension reduction and then are fed into various
classiÔ¨Åers (e.g.
to perform the Ô¨Ånal classiÔ¨Åcation.
However, as mentioned above, the extraction of those engineered features depends on the accurate segmentation of cell
or cell components. Furthermore, it is also limited by the current understanding of cervical cytology [17]. To reduce
the dependency on the accurate segmentation, the CNN are used to learn the features from data recently [18], but an
approximate segmentation or region of interest (ROI) detection is still necessary. Although the DeepPap [17] is claimed
totally segmentation-free, it still needs the nucleus centroid information for training and the random-view aggregation
and multiple crops testing during the inference stage, which are very time-consuming.

random forests, SVM, softmax regression, neural network, etc.)

There are a handful public available microscopic image datasets dedicated to cervical cell segmentation such as ISBI-
141, ISBI-15 2, but to our best knowledge for cervical cell classiÔ¨Åcation the only public available microscopic image
dataset is the Herlev benchmark dataset [29], which consists of 917 single cell images corresponding to four categories of
abnormal cell with diÔ¨Äerent severity (namely light dysplastic, moderate dysplastic, severe dysplastic and carcinoma in
situ) and three categories of normal cells (normal columnar, normal intermediate and normal superÔ¨Åcial). The limited

1https://cs.adelaide.edu.au/~carneiro/isbi14_challenge/index.html
2https://cs.adelaide.edu.au/~zhi/isbi15_challenge/index.html

3

average across samplesùêπùëñadaptiveavgpoolaverage across pyramids‚Ä¶K x H x W x CTransposeReshape1 x H x W x CK1 x H x W x C(K+1) x H x W x CConvConcat...Concat(a)

(b)

(c)

Figure 3: The head for classiÔ¨Åcation and regression. (a): The head of baseline model. (b): The share module in our experiments. (c):
The independent module in the Comparison Detector.

annotated data prevents the applications of traditional object detection methods such as Viola-Jones detector [30] or
contemporary CNN-based detectors [31] to cervical cancer screening.

2.2. CNN-based object detection

The Overfeat [32] made the earliest eÔ¨Äorts to apply CNN for object detection and has achieved a signiÔ¨Åcant improve-
ment of more than 50% mAP when compared to the best methods at that time which were based on the hand-crafted
features. Since then, a lot of CNN-based methods [33, 10, 34, 35, 36, 37, 38, 39] have been proposed for high-quality
object detection, which can be roughly classiÔ¨Åed into two categories: object proposal-based and proposal-free. The
road-map of proposal-based methods starts from the notable R-CNN [34] and is improved by Fast-RCNN [35] in an
end-to-end manner and by Faster R-CNN [10] to quickly generate object regions, which has motivated a lot of follow-up
improvements [11, 36] in terms of accuracy and speed. The proposal-free methods [37, 38] directly predict the bounding
boxes without the proposal generation step. Generally, the proposal-free methods are conceptually simpler and much
faster than the proposal-based methods, but the detection accuracy is usually behind that of the proposal-based methods
[39]. Here we choose the Faster R-CNN [10] with FPN [11] as our baseline model but our method is compatible with
other proposal-based methods.

2.3. One/few-shot learning

One/few-shot learning is a task of learning from just one or a few training samples per class and has been extensively
discussed in the context of image recognition and classiÔ¨Åcation [22, 23, 24]. Recently signiÔ¨Åcant progress has been made
for one/few-shot learning tackled by meta-learning or learning-to-learn strategy, which can be roughly divided into three
categories: metric-based, memory-based and optimization-based. The metric-based methods [22, 23, 24, 25] learn to
compare the query image with support set images. The memory-based methods [40] exploited the memory-augmented
neural network to quickly store and retrieve suÔ¨Écient information for each classiÔ¨Åcation task, while the optimization-
based methods [41, 42] aim to learning a base-model which can be Ô¨Åne-tuned quickly for a new classiÔ¨Åcation task. All
these works only tackle image classiÔ¨Åcation tasks.

2.4. Object detection with limited data

Most prior works on object detection with limited labels use semi-/weakly-supervised methods or few-example learning
[43] to make use of abundant unlabeled data, whereas in limited data regime there are few works focus on using few-
shot learning to address this problem [44, 45]. Kang et al.
[45] decomposes the training into base-model learning
and meta-model learning and train a meta-model to reweight the features extracted by the base-model to assist novel
object detection. However, the training of base model still needs abundant annotated data for base classes. RepMet
[44] introduces a metric learning-based sub-network architecture to learn the embedding space and distribution of the
training categories without using external data. However, RepMat involves an alternating optimization between the
external class distribution module learning and net parameters updating, whereas our solution is a clean, single-step
training framework.

3. Comparison Detector

3.1. Basic Architecture

Our proposed Comparison detector is based on proposal-based detection framework consisting of a backbone network
for feature extraction, a region proposal network (RPN) for generating proposals and a head for the proposal classiÔ¨Åcation

4

NHWCN x CN x CN x (K+1) x 4N x (K+1)FCFCFCFCN x 1 x HWCN(K+1) x HWCN(K+1) x HWCN x (K+1) x 4N x (K+1)ConvFCFC1 x (K+1) x HWCN(K+1) x HWCConvN x 1 x HWCN(K+1) x HWCN x (K+1)N x (K+1) x 4ConvFC1 x (K+1) x HWCN x CFCFC(a)

(b)

Figure 4: t-SNE visualization for reference samples. (a):Visualization before learning. (b):Visualization after learning.

and bounding box regression. Here we choose the Faster R-CNN with FPN [11] as our baseline. Then we decouple the
regression and classiÔ¨Åcation in the head and replace the original parameter classiÔ¨Åer with our comparison classiÔ¨Åer. Our
comparison classiÔ¨Åer introduces an inductive bias, i.e. the within-class distance is less than the between-class in the
features space, into the model and henceforth reduces the complexity of the model and mitigates the generalization issue
with small datasets to some extent [46].

The framework of our Comparison detector is depicted in Fig.1, which is divided into three stages to describe. At the
Ô¨Årst stage, Comparison detector generates features for the reference samples and the whole images. As shown in Fig. 1,
both the features of them are computed by backbone network with FPN [11], without adding any extra models to encode
the reference samples. Assuming there are n reference samples for K category with L levels pyramid feature. Let F l
kj be
the j-th sample with k-th categories‚Äô prototype representation of the l-level pyramid features, which can be computed
by average operation as follows

kj = F l(Rkj)
F l
where F l(¬∑) and Rkj denote the l-th level feature extraction function and the j-th reference sample of class k, respectively.
At the same time, the feature Pm of the m-th object proposal xm is generated by

(1)

Pm = F l(xm).

(2)

It should be pointed out that categories in the training and test set are same in our settings unlike one/few-shot learning.
The second stage is to generate the prototype representations of each category from the reference samples‚Äô pyramid

features as shown in Fig. 2(a). The function of S(¬∑) is to compute the Ô¨Ånal prototype representation Fk for class k

F l

k =

1
n

n
(cid:88)

j=1

F l
kj

Fk = S({F l

k})

(3)

(4)

The third stage is the head model for classiÔ¨Åcation and bounding box regression (Fig. 3(c)), consisting of a few
convolutional (Conv) and fully connected (F C) layers. Let d(Fk, Pm) be a metric function to compute the distance
between the m-th proposal feature (Pm) and prototype representation of the k-th category (Fk). We will discuss this
function in section 4.5. Each proposal‚Äôs posterior probability pk and bounding box regression bk can be obtained by

pk =

e‚àíd(Fk,Pm)
i e‚àíd(Fi,Pm)

(cid:80)

bk = b(Fk, Pm)

(5)

(6)

where b(¬∑, ¬∑) denotes the bounding box regression function. We denote Eq. 5 as comparison classiÔ¨Åer. The rest of the
model is the same as Faster R-CNN with FPN model [11].

Finally, The objective function of Comparison detector is to minimize the total loss consisting of the RPN classiÔ¨Åcation

loss, RPN bounding box loss, head classiÔ¨Åcation loss, head bounding box loss:

min
w

Lrpnc + Lrpnb + ŒªLheadc + Lheadb

(7)

5

Table 1: All experiments train on Df , and the performance is evaluated on the test set. At the same time, the comparison classiÔ¨Åer of all
models directly adopts the (cid:96)2-distance. The reference samples are the same, produced by Ô¨Åxed mode before the experiment.

model

learning

background

independent mode

A

B

C

D

E

F

‚àö

‚àö

‚àö

‚àö

‚àö

‚àö

‚àö

‚àö

‚àö

using all

twice bounding box

pyramid features
‚àö

regression
‚àö

‚àö

‚àö

‚àö

‚àö

‚àö

‚àö

‚àö

mAP AR

31.4

34.1

32.7

41.0

38.9

37.7

49.3

53.3

50.8

51.3

49.8

51.1

w is the weight of Comparison detector. All classiÔ¨Åcation losses are cross-entropy loss and bounding box losses are
(cid:96)1-smooth loss [35].

3.2. Generating Prototype Representation Block (GPRB)

3.2.1. Generating prototype representations of categories

As shown in Eq. 3, we use n reference samples for each category to generate pyramid features, then obtains the
prototype representation like Eq. 4. We‚Äôll show a possible choice for S(¬∑). For simplicity, we directly resize the each
pyramid features which is generated by reference samples to a Ô¨Åxed size, and then calculate prototype representation by
averaging operation, i.e.

S({F l

k}) =

1
L

r(F l

k, s)

(cid:88)

l

(8)

where r(¬∑, ¬∑) is bilinear interpolation function and s is the size of Ô¨Ånal features.

3.2.2. Learning the prototype representations of background

There are many negative proposals generated by RPN, so the R-CNN [34] adds a background category to represent
them. In our Comparison detector, we need to select a number of reference samples for each category. Because of the
overwhelming diversity, selecting reference samples of background is very diÔ¨Écult. We notice that a background region is
considered to the proposal indicating that it has certain similarity with categories. Therefore, it can be inferred that its
prototype representations are a combination of diÔ¨Äerent categories in the most case. So we propose to learn its prototype
representations from other categories‚Äô prototype representations. First, we transpose the channels and reshape the tensor.
Then, we use a simple 1 √ó 1 convolution operation to generate the prototype representations of background; Ô¨Ånally, we
concat all prototype representations together, as shown in Fig. 2(b).

3.3. The head for classiÔ¨Åcation and regression

As shown in Fig. 3(a), the structure of the baseline model‚Äôs head is to transform the proposal feature Ô¨Årstly and then
one branch is used for classiÔ¨Åcation, and another is used to predict the oÔ¨Äset of the bounding box. For our Comparison
detector, due to the introduction of the reference samples, we need to re-organise the head. The are two choices according
to whether sharing the features between bounding box regression and classiÔ¨Åcation. One is that bounding box regression
branch and classiÔ¨Åcation branch are not shared, as shown in Fig. 3(c). Unlike the baseline model, the classiÔ¨Åer and
bounding box regressor in the head of Comparison detector are independent (independent module). And the bounding
box regressor only uses the features of proposal to predict the oÔ¨Äset of the bounding box. It is equivalent to

d(Fi, Pm) = F C(m(Fi, Pm)),
b(Fi, Pm) = F C(F C(F C(Pm))),

where m(Fi, Pm) = Conv3(Conv1(|Fi ‚àí Pm|2))3. Another choice is to share features for both classiÔ¨Åcation and regression,
as shown in Fig. 3(b), which means

b(Fi, Pm) = F C(m(Fi, Pm)).

We call this method as shared module.

3The subscript denotes kernel size of convolution

6

Figure 5: Object samples of entities seen in cervical cytology.

Figure 6: The distribution of categories on diÔ¨Äerent datasets.

3.4. Strategies for selecting reference samples

In our Comparison detector, we need to choose the reference samples for each category. A intuitive way is to select
them according to the Bethesda atlas [2]. However, there are very signiÔ¨Åcant diÔ¨Äerence between the given atlas and our
data due to the variations of the preparation and digitization of slide. Hence we resort to other feasible data-driven
alternatives. We randomly select about 150 instances of each category from the training sets. The shortest side of these
instances is greater than 16 pixels. Therefore we get a total of 1,560 instances as candidate reference samples from
training sets. we can select suitable instances in these objects as our reference samples.

There are two possible ways. The Ô¨Årst is to randomly choose several instances of each category as the reference
samples. The second is to Ô¨Årst map all 1,560 objects into the feature space through the ImageNet pre-trained model [8]
to get the features of each object and then use t-SNE [47] for feature dimension reduction (Fig. 4). Based on the results
of t-SNE, we empirically obtain the number of clusters each category. Then we use it as the parameter for K-means.
Finally, based on the result of K-means, we choose the instances which are the closest to the center of clusters as reference
samples.

7

ascusaschlsilhsilsccagctrichcandÔ¨Çoraherpsactinascusaschlsilhsilsccagctrichomonascandidafloraherpsactinomycescategories0200040006000800010000number of instances1754385613972602419634896493632412326212914114719538721367909164710291612221486161316837028916121914DfDstest setTable 2: Model with balanced bounding box regression loss and classiÔ¨Åcation loss. The metric in bracket is AR.

model
wo/blancing
w/blancing

B
34.1(53.3)
43.7(60.7)

D
41.0(51.3)
43.5(58.9)

F
37.7(51.1)
38.8(52.3)

Table 3: DiÔ¨Äerent comparison classiÔ¨Åer. The numbers in the brackets denote the result after balancing the loss.

comparator
mAP
AR

(cid:96)2-distance parameterized (cid:96)2-distance
34.1(43.7)
53.3(60.7)

38.2(44.5)
56.8(61.6)

concat
40.7(42.5)
49.1(58.1)

Table 4: Number of clusters for each category.

category

ascus

asch lsil hsil

scc

agc

trich cand Ô¨Çora herps

actin

number of clusters

3

4

2

4

2

3

1

2

2

4

1

4. Experiment and Result

4.1. Materials and experiments

There are no public available benchmarks for cervical cell object detection in the community, we Ô¨Årst establish
a database consisting of 7,086 cervical microscopical images which are cropped from the whole slide images (WSIs)
obtained by Pannoramic MIDI II digital slide scanner. The corresponding specimens are prepared by Thinprep methods
stained with Papanicolaou stain. Conforming to TBS categories [2], 48,587 object instance bounding boxes were labeled
by experienced pathologists which belong to 11 categories, namely ASC-US (ascus), ASC-H (asch), low-grade squamous
intraepithelial lesion (lsil), high-grade squamous intraepithelial lesion (hsil), squamous-cell carcinoma (scc), atypical
glandular cells (agc), trichomonas (trich), candida (cand), Ô¨Çora, herps, actinomyces (actin). Figure 5 shows some examples
of each category in our database. Then we randomly divide the dataset into training set Df which contains 6,667 images,
test set which contains 419 images for experiment. To verify the performance of Comparison detector on the small
dataset, we randomly choose 762 images from the training dataset to form a small dataset of Ds. The distribution of
categories in each dataset is shown in the Fig. 6.

In all experiments, we used ResNet50 as backbone network with ImageNet pre-trained model. For reference samples,
we re-scale them such that their side is w = h = 224 which is coincident with pre-trained model. The initial learning
rate is 0.001, and then decreased by a factor of 10 at 35-th and 50-th epoch. Training is stopped after 60 epochs and the
other parameters are the same as FPN [11]. The minibatch size is 2 images in 2 GPUs. The weight decay is 0.0001 and
the momentum is 0.9. The experiment is Ô¨Årstly trained on the Df to evaluate the performance of Comparison detector.
In our setting, the reference samples are Ô¨Åxed in each training iteration for the stability of the training model. And test
stage is the same.

For the cervical cell images, annotators are prone to take a higher threshold when label the objects due to the low
In addition, multiple nearby objects with the same category will be marked as one, so the
discrimination of them.
performance of the model can not be well reÔ¨Çected by mAP [34]. Therefore, the performance of the model is evaluated by
using mAP and AR as a complement on test set. If the mAP does not decrease and the AR improves, it surely signiÔ¨Åes
the performance of model is improved. Herein, the results are reported in both mAP and AR. A summary of results can
be found in Table 1 and some detection results on the test set are shown in Fig.7.

4.2. Learning the prototype representation of background

In order to compare the eÔ¨Äect of learning background, we randomly select some background samples from the proposals
to obtain the features of background(model A). As shown in Table 1,the result is 31.4% and the model B which is the model
of learning the prototype representation of background category has a mAP of 34.1%. It indicates that the proposed
method is better than random selection. It should be noted that because the prototype representation of the background
category learns from the prototype representation of other categories, the gradient propagation will also have some eÔ¨Äect
on the optimization of other prototype representation. In order to make sure whether this eÔ¨Äects is beneÔ¨Åcial, we stop
gradient propagation at the fork position in Fig. 2(b). The performance of the model has declined with a mAP of 33.0%
and an AR of 52.6%.

8

Table 5: DiÔ¨Äerent way of selecting the reference samples.

method Ô¨Åxed mode

mAP
AR

44.5
61.6

random mode
42.8
61.0

t-SNE
45.9
63.5

Table 6: The results of learning on diÔ¨Äerent size datasets. We regard Faster-RCNN with Feature Pyramid Network as baseline model. The
performance of Comparison detector and baseline model with training on diÔ¨Äerent datasets are shown in the following:

method

dataset AR mAP ascu

asch

lsil

hsil

scc

agc

trich

cand

Ô¨Çora

herps

actin

baseline

model

Comparison

detector

baseline

model

Comparison

detector

Ds

Ds

Df

Df

12.9

6.6

11.0

2.0

23.7

21.6

0.0

3.5

0.0

11.5

0.0

0.0

0.0

35.7

26.3

10.5

1.7

42.8

32.3

0.8

40.5

37.5

24.1

6.9

45.0

46.6

58.9

45.2

27.2

6.7

41.7

35.3

18.6

57.3

46.7

72.2

57.3

83.0

51.4

63.5

45.9

27.4

6.7

41.7

40.1

21.8

54.5

45.0

65.5

63.5

68.1

70.5

4.3. Prototype representations of categories

In our approach, as shown in Eq. 8, we use all pyramid features to generate prototype representation of categories.
Another choice is to only use the last level pyramid features as the category of prototype, i.e. S({F l
k . The model
B makes use of all pyramid features to learn the protopype representations, however the model C only use the last level
pyramid feature to learn the protopype representations. As shown in Table 1, the result of model B is 34.1% which is
better than model C (32.7%). The diÔ¨Äerence between models D and E is whether to use all pyramid features. The model
D which has an mAP of 41.0% is superior model E (38.9%). They show that using all pyramid features performs is best.
Because it can combine features of multiple scales, which not only have rich semantics but also take into account objects
of diÔ¨Äerent size.

k}) = F 5

4.4. Head model

As mentioned before, in independent module, the box regression function b(¬∑, ¬∑) is the same as baseline model because
experiment found that removing one layer will make the result worse. The model B is independent module and the
model model D is shared module. The result lists in Table 1. The model B has a mAP of 34.1% and model D is 41.0%.
The results show shared module performs much better than independent module. Furthermore, we drop the operation
of bounding box regression in the head (model F). It‚Äôs weird that it has a result 37.7% which is better than model B.
This phenomenon goes against common sense that twice bounding box regression are often better than just once. We
empirically conjecture that the importance of classiÔ¨Åcation should be more important than bounding box regression in
our model [20]. So we adjust the weight coeÔ¨Écient Œª in the Eq. 7 to balance the classiÔ¨Åcation loss and bounding box
regression loss. Here we select Œª = 5. The results are shown in Table 2. After balancing loss, the performance of the
model has been greatly improved. It conÔ¨Årms our guess. Moreover, by analyzing model B and model D, we Ô¨Ånd that the
diÔ¨Äerence between them is not only classiÔ¨Åcation and bbox regression is independent, but also the comparison classiÔ¨Åer
of model D is parameterized. After converting the comparison classiÔ¨Åer of model B into parameterized(Fig. 3 (c)), the
result shows that it is better than model D.

4.5. Optimizing comparison classiÔ¨Åer

We evaluate three distance metrics in the comparison classiÔ¨Åer. The Ô¨Årst is (cid:96)2-distance which means d(Pm, Fk) =
M (|Fk ‚àí Pm|2). M (¬∑) represents averaging function for tensor. The second is the parameterized (cid:96)2-distance, such as
d(Pm, Fk) = Conv7(|Fk ‚àí Pm|2). Similar to [25], we also try to make the model to learn the metric function instead of
the predeÔ¨Åned ones. According to the result of Table 3, parameterized (cid:96)2-distance shows the best performance which has
an mAP of 44.5%. So we ultimately adopt it. When Œª = 5, the result is shown in brackets. Combining with the results
shown in Table 2, we Ô¨Ånd that it is universal that the balance trick can improve performance in our model. So we adopt
this trick in all the next experiments.

4.6. Strategies for selecting reference samples

We Ô¨Årst evaluate the scheme of randomly choosing reference samples which includes two methods. The Ô¨Årst is to
randomly choose 3 instances of each category (this number is limited by GPU‚Äôs memory) as the reference samples (fixed
mode). The second one is to randomly select 5 candidates of each category in those objects. Then the model randomly

9

Figure 7: Comparison detector results on the test set. These results are based on ResNet-50 with Feature Pyramid Networks, achieving a
mAP of 45.9% and AR of 63.5%.

selected three of the Ô¨Åve candidates as templates during training, but Ô¨Åve in testing (random mode). In addition, we
also choose reference samples by applying t-SNE and K-means. During the training of t-SNE, we adopt the following
parameters setting, i.e. the hyper-parameters are 30 for perplexity, 1 for learning rate, and 10 for label supervision.
Throught t-SNE method, we can get the number of clusters for each category empirically, which as the n clusters
parameter for K-means. The number of clusters are shown in Table 4. Finally, according to the results of K-means, we
choose the instance objects which is close to the center of clusters as reference samples. As show in Table 5, the result of
t-SNE and K-means is 45.9%, however fixed mode is 44.5% and random mode is 42.8%. It indicates that the selection
reference samples via t-SNE and K-means perform the best.

4.7. Performance on training dataset Df and Ds

As shown in Table 6, Comparison detector has 0.7 mAP performance improvements when training on the Df dataset,
and improves the AR by 4.6 points. Due to the special annotating situation as described in Section 4.1, some correct
predictions may be identiÔ¨Åed as false positives. Therefore, there is a signiÔ¨Åcant increase in AR, but little improvement
in mAP. When training on the Ds, Comparison detector is completely superior to baseline model. It achieves a state-
of-the-art result on the test set with a mAP of 26.3% compared to 6.6%, which indicates our method alleviates the over
Ô¨Åtting problem to some extent. Prototype representation in this model is generated by reference samples, however, it can
be generated by other way, such as external memory. In the future work, we expect a better solution for the generation
of prototype representations.

5. Conclusion

In this work, we propose to apply contemporary CNN-based object detection methods for automated cervical cancer
detection. To deal with the limited training dataset, we develop the comparison classiÔ¨Åer into the state-of-the-art two-
stage object detection method based on the comparison with the reference samples of each category. Instead of manually
choosing the reference samples of the background by some heuristic rules, we present a scheme to learn them from the
data directly. We also investigate several important modules including the generation of prototype representations of
each category and the design of head model for cervical cell/clumps detection. Experimental results show that compared
with the baseline, our method improves the mAP by 19.7 points and the AR by 22.8 when trained on the small training
dataset, and achieves better mAP and improves the AR by 4.6 when trained on the medium training dataset. It should
be noticed that our algorithm directly operates on the whole image rather than the extracted patches based on the nuclei
and hereby only need one forward propagation for each image, making the inference extremely eÔ¨Écient. In addition, the
proposed method is Ô¨Çexible to be intergraded into other proposal-based methods.

10

candida 1.000actinomyces 1.000actinomyces 1.000actinomyces 0.995actinomyces 0.995actinomyces 0.993actinomyces 0.976actinomyces 0.976actinomyces 0.946agc 1.000agc 1.000agc 1.000agc 0.999agc 0.999agc 0.999agc 0.991agc 0.979agc 0.876hsil 0.998hsil 0.993hsil 0.991hsil 0.986hsil 0.983hsil 0.979hsil 0.969asch 0.965hsil 0.964hsil 0.949hsil 0.937hsil 0.933hsil 0.925hsil 0.884hsil 0.873hsil 0.860hsil 0.836hsil 0.828hsil 0.825hsil 0.798hsil 0.736trichomonas 0.993trichomonas 0.988trichomonas 0.988trichomonas 0.986trichomonas 0.985trichomonas 0.983trichomonas 0.971trichomonas 0.971trichomonas 0.913trichomonas 0.892trichomonas 0.889trichomonas 0.887trichomonas 0.870trichomonas 0.852trichomonas 0.842trichomonas 0.816trichomonas 0.803trichomonas 0.782trichomonas 0.756hsil 0.998hsil 0.997hsil 0.953scc 0.943lsil 0.703herps 1.000herps 1.000agc 0.980hsil 0.764flora 0.743hsil 0.725hsil 0.990hsil 0.989hsil 0.977asch 0.964hsil 0.956ascus 0.922asch 0.917asch 0.873hsil 0.819hsil 0.775hsil 0.762hsil 0.744hsil 0.734asch 0.723asch 0.706asch 1.000hsil 0.996hsil 0.995trichomonas 0.978hsil 0.938asch 0.935trichomonas 0.928hsil 0.866hsil 0.842hsil 0.830scc 1.000scc 0.999scc 0.999scc 0.999scc 0.998scc 0.995scc 0.994scc 0.994scc 0.992scc 0.992scc 0.985scc 0.981scc 0.980scc 0.972scc 0.971scc 0.968scc 0.966scc 0.960scc 0.953scc 0.940scc 0.925scc 0.924scc 0.914scc 0.876scc 0.856scc 0.812scc 0.785scc 0.767hsil 0.748scc 0.720lsil 1.000lsil 0.998ascus 0.957hsil 0.734ascus 0.704hsil 1.000hsil 0.999hsil 0.998hsil 0.994hsil 0.934lsil 0.908lsil 0.801hsil 0.785lsil 0.723Acknowledgements

This research was partially supported by the National Natural Science Foundation of China under Grant No. 61602522,
the Natural Science Foundation of Hunan Province, China under Grant No.14JJ2008 and the Fundamental Research
Funds of the Central Universities of Central South University under Grant No. 2018zzts577.

11

References

[1] E. Davey, A. Barratt, L. Irwig, S. F. Chan, P. Macaskill, P. Mannes, A. M. Saville, EÔ¨Äect of study design and quality
on unsatisfactory rates, cytology classiÔ¨Åcations, and accuracy in liquid-based versus conventional cervical cytology:
a systematic review, The Lancet 367 (9505) (2006) 122‚Äì132.

[2] R. Nayar, D. C. Wilbur, The Bethesda system for reporting cervical cytology: DeÔ¨Ånitions, criteria, and explanatory

notes, Springer, 2015.

[3] D. Saslow, D. Solomon, H. W. Lawson, M. Killackey, S. L. Kulasingam, J. Cain, F. A. Garcia, A. T. Moriarty, A. G.
Waxman, D. C. Wilbur, et al., American cancer society, american society for colposcopy and cervical pathology, and
american society for clinical pathology screening guidelines for the prevention and early detection of cervical cancer,
CA: A Cancer Journal for Clinicians 62 (3) (2012) 147‚Äì172.

[4] E. Bengtsson, P. Malm, Screening for cervical cancer using automated analysis of PAP-smears, Computational and

Mathematical Methods in Medicine 2014.

[5] L. Zhang, H. Kong, C. Ting Chin, S. Liu, X. Fan, T. Wang, S. Chen, Automation-assisted cervical cancer screening
in manual liquid-based cytology with hematoxylin and eosin staining, Cytometry Part A 85 (3) (2014) 214‚Äì230.

[6] H. A. Phoulady, M. Zhou, D. B. Goldgof, L. O. Hall, P. R. Mouton, Automatic quantiÔ¨Åcation and classiÔ¨Åcation
Image Processing (ICIP), 2016 IEEE International

of cervical cancer via adaptive nucleus shape modeling, in:
Conference on, IEEE, 2016, pp. 2658‚Äì2662.

[7] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiÔ¨Åcation with deep convolutional neural networks, in:

Advances in Neural Information Processing Systems, 2012, pp. 1097‚Äì1105.

[8] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE conference

on Computer Vision and Pattern Recognition, 2016, pp. 770‚Äì778.

[9] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic segmentation, in: Proceedings of the

IEEE conference on computer vision and pattern recognition, 2015, pp. 3431‚Äì3440.

[10] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time object detection with region proposal networks,

in: Advances in Neural Information Processing Systems, 2015, pp. 91‚Äì99.

[11] T.-Y. Lin, P. Doll¬¥ar, R. Girshick, K. He, B. Hariharan, S. Belongie, Feature pyramid networks for object detection,
in: Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, IEEE, 2017, pp. 936‚Äì944.

[12] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian, J. A. Van Der Laak, B. Van Ginneken,
C. I. S¬¥anchez, A survey on deep learning in medical image analysis, Medical Image Analysis 42 (2017) 60‚Äì88.

[13] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, S. Thrun, Dermatologist-level classiÔ¨Åcation

of skin cancer with deep neural networks, Nature 542 (7639) (2017) 115.

[14] A. Tareef, Y. Song, H. Huang, Y. Wang, D. Feng, M. Chen, W. Cai, Optimizing the cervix cytological examination

based on deep learning and dynamic shape modeling, Neurocomputing 248 (2017) 28‚Äì40.

[15] L. Zhang, M. Sonka, L. Lu, R. M. Summers, J. Yao, Combining fully convolutional networks and graph-based
approach for automated segmentation of cervical cell nuclei, in: Biomedical Imaging (ISBI 2017), 2017 IEEE 14th
International Symposium on, IEEE, 2017, pp. 406‚Äì409.

[16] Z. Lu, G. Carneiro, A. P. Bradley, D. Ushizima, M. S. Nosrati, A. G. Bianchi, C. M. Carneiro, G. Hamarneh,
Evaluation of three algorithms for the segmentation of overlapping cervical cells, IEEE journal of biomedical and
health informatics 21 (2) (2017) 441‚Äì450.

[17] L. Zhang, L. Lu, I. Nogues, R. M. Summers, S. Liu, J. Yao, DeepPap: Deep convolutional networks for cervical cell

classiÔ¨Åcation, IEEE Journal of Biomedical And Health Informatics 21 (6) (2017) 1633‚Äì1643.

[18] O. N. Jith, K. Harinarayanan, S. Gautam, A. Bhavsar, A. K. Sao, DeepCerv: Deep neural network for segmenta-
tion free robust cervical cell classiÔ¨Åcation, in: Computational Pathology and Ophthalmic Medical Image Analysis,
Springer, 2018, pp. 86‚Äì94.

[19] Y. Liang, R. Kang, C. Lian, Y. Mao, An end-to-end system for automatic urinary particle recognition with convo-

lutional neural network, Journal of medical systems 42 (9) (2018) 165.

12

[20] Y. Liang, Z. Tang, M. Yan, J. Liu, Object detection based on deep learning for urine sediment examination, Biocy-

bernetics and Biomedical Engineering 38 (4) (2018) 661‚Äì670.

[21] M. H. Stoler, M. SchiÔ¨Äman, et al., Interobserver reproducibility of cervical cytologic and histologic interpretations:

Realistic estimates from the ASCUS-LSIL triage study, JAMA 285 (11) (2001) 1500‚Äì1505.

[22] G. Koch, R. Zemel, R. Salakhutdinov, Siamese neural networks for one-shot image recognition, in: ICML Deep

Learning Workshop, Vol. 2, 2015.

[23] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., Matching networks for one shot learning, in: Advances in

Neural Information Processing Systems, 2016, pp. 3630‚Äì3638.

[24] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learning, in: Advances in Neural Information

Processing Systems, 2017, pp. 4077‚Äì4087.

[25] F. S. Y. Yang, L. Zhang, T. Xiang, P. H. Torr, T. M. Hospedales, Learning to compare: Relation network for
few-shot learning, in: Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt
Lake City, UT, USA, 2018.

[26] L. Zhang, H. Kong, C. T. Chin, S. Liu, Z. Chen, T. Wang, S. Chen, Segmentation of cytoplasm and nuclei of
abnormal cells in cervical cytology using global and local graph cuts, Computerized Medical Imaging and Graphics
38 (5) (2014) 369‚Äì380.

[27] L. Zhang, H. Kong, S. Liu, T. Wang, S. Chen, M. Sonka, Graph-based segmentation of abnormal nuclei in cervical

cytology, Computerized Medical Imaging and Graphics 56 (2017) 38‚Äì48.

[28] H. Lee, J. Kim, Segmentation of overlapping cervical cells in microscopic images with superpixel partitioning and
cell-wise contour reÔ¨Ånement, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, 2016, pp. 63‚Äì69.

[29] Y. Marinakis, G. Dounias, J. Jantzen, Pap smear diagnosis using a hybrid intelligent scheme focusing on genetic
algorithm based feature selection and nearest neighbor classiÔ¨Åcation, Computers in Biology and Medicine 39 (1)
(2009) 69‚Äì78.

[30] P. Viola, M. J. Jones, Robust real-time face detection, International Journal of Computer Vision 57 (2) (2004)

137‚Äì154.

[31] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, M. Pietik¬®ainen, Deep learning for generic object detection:

A survey, arXiv preprint arXiv:1809.02165.

[32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, Y. LeCun, Overfeat: Integrated recognition, localization

and detection using convolutional networks, arXiv preprint arXiv:1312.6229.

[33] W. Chu, D. Cai, Deep feature based contextual model for object detection, Neurocomputing 275 (2018) 1035‚Äì1042.

[34] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies for accurate object detection and semantic
segmentation, in: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2014, pp.
580‚Äì587.

[35] R. Girshick, Fast R-CNN, in: Proceedings of the IEEE International Conference on Computer Vision, 2015, pp.

1440‚Äì1448.

[36] K. He, G. Gkioxari, P. Doll¬¥ar, R. Girshick, Mask R-CNN, in: Computer Vision (ICCV), 2017 IEEE International

Conference on, IEEE, 2017, pp. 2980‚Äì2988.

[37] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: UniÔ¨Åed, real-time object detection, in: Pro-

ceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779‚Äì788.

[38] J. Redmon, A. Farhadi, Yolo9000: Better, faster, stronger, in: 2017 IEEE Conference on Computer Vision and

Pattern Recognition (CVPR), IEEE, 2017, pp. 6517‚Äì6525.

[39] S. Zhang, L. Wen, X. Bian, Z. Lei, S. Z. Li, Single-shot reÔ¨Ånement neural network for object detection, in: IEEE

CVPR, 2018.

[40] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap, Meta-learning with memory-augmented neural

networks, in: International conference on machine learning, 2016, pp. 1842‚Äì1850.

13

[41] S. Ravi, H. Larochelle, Optimization as a model for few-shot learning, in: International Conference for Learning

Representations (ICLR), 2017.

[42] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-learning for fast adaptation of deep networks, in: International

Conference on Machine Learning, 2017, pp. 1126‚Äì1135.

[43] X. Dong, L. Zheng, F. Ma, Y. Yang, D. Meng, Few-example object detection with model communication, IEEE

transactions on pattern analysis and machine intelligence 41 (7) (2018) 1641‚Äì1654.

[44] E. Schwartz, L. Karlinsky, J. Shtok, S. Harary, M. Marder, S. Pankanti, R. Feris, A. Kumar, R. Giries, A. M. Bron-
stein, RepMet: Representative-based metric learning for classiÔ¨Åcation and one-shot object detection, Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition.

[45] B. Kang, Z. Liu, X. Wang, F. Yu, J. Feng, T. Darrell, Few-shot object detection via feature reweighting, in:

Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 8420‚Äì8429.

[46] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Ra-
poso, A. Santoro, R. Faulkner, et al., Relational inductive biases, deep learning, and graph networks, arXiv preprint
arXiv:1806.01261.

[47] L. v. d. Maaten, G. Hinton, Visualizing data using t-SNE, Journal of Machine Learning Research 9 (Nov) (2008)

2579‚Äì2605.

[48] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Computation 9 (8) (1997) 1735‚Äì1780.

14

