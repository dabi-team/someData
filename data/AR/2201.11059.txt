2
2
0
2

t
c
O
2
1

]
L
M

.
t
a
t
s
[

4
v
9
5
0
1
1
.
1
0
2
2
:
v
i
X
r
a

Generalization Error Bounds on Deep Learning with
Markov Datasets

Lan V. Truong∗
Department of Engineering
University of Cambridge
Cambridge, CB2 1PZ
lt407@cam.ac.uk

Abstract

In this paper, we derive upper bounds on generalization errors for deep neural net-
works with Markov datasets. These bounds are developed based on Koltchinskii
and Panchenko’s approach for bounding the generalization error of combined clas-
siﬁers with i.i.d. datasets. The development of new symmetrization inequalities in
high-dimensional probability for Markov chains is a key element in our extension,
where the absolute spectral gap of the inﬁnitesimal generator of the Markov chain
plays a key parameter in these inequalities. We also propose a simple method to
convert these bounds and other similar ones in traditional deep learning and ma-
chine learning to Bayesian counterparts for both i.i.d. and Markov datasets. Ex-
tensions to m-order homogeneous Markov chains such as AR and ARMA models
and mixtures of several Markov data services are given.

1 Introduction

In statistical learning theory, understanding generalization for neural networks is among the most
challenging tasks. The standard approach to this problem was developed by Vapnik [1], and it is
based on bounding the difference between the prediction error and the training error. These bounds
are expressed in terms of the so called VC-dimension of the class. However, these bounds are very
loose when the VC-dimension of the class can be very large, or even inﬁnite. In 1998, several authors
[2, 3] suggested another class of upper bounds on generalization error that are expressed in terms
of the empirical distribution of the margin of the predictor (the classiﬁer). Later, Koltchinskii and
Panchenko [4] proposed new probabilistic upper bounds on generalization error of the combination
of many complex classiﬁers such as deep neural networks. These bounds were developed based
on the general results of the theory of Gaussian, Rademacher, and empirical processes in terms of
general functions of the margins, satisfying a Lipschitz condition. They improved previously known
bounds on generalization error of convex combination of classiﬁers.

In the context of supervised classiﬁcation, PAC-Bayesian bounds have proved to be the tightest [5–
7]. Several recent works have focused on gradient descent based PAC-Bayesian algorithms, aiming
to minimise a generalisation bound for stochastic classiﬁers [8–10]. Most of these studies use a
surrogate loss to avoid dealing with the zero-gradient of the misclassiﬁcation loss. Several authors
used other methods to estimate of the misclassiﬁcation error with a non-zero gradient by proposing
new training algorithms to evaluate the optimal output distribution in PAC-Bayesian bounds analyt-
ically [11–13]. Recently, there have been some interesting works which use information-theoretic
approach to ﬁnd PAC-bounds on generalization errors for machine learning [14, 15] and deep learn-
ing [16].

∗Use footnote for providing further information about author (webpage, alternative address)—not for ac-

knowledging funding agencies.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
All of the above-mentioned bounds are derived based on the assumption that the dataset is gener-
ated by an i.i.d. process with unknown distribution. However, in many applications in machine
learning such as speech, handwriting, gesture recognition, and bio-informatics, the samples of data
are usually correlated. Some of these datasets are time-series ones with stationary distributions
such as samples via MCMC, ﬁnite-state random walks, or random walks on graph. In this work,
we develop some upper bounds on generalization errors for deep neural networks with Markov or
hidden Markov datasets. Our bounds are derived based on the same approach as Koltchinskii and
Panchenko [4]. To deal with the Markov structure of the datasets, we need to develop some new
techniques in this work. The development of new symmetrization inequalities in high-dimensional
probability for Markov chains is a key element in our extension, where the absolute spectral gap of
the inﬁnitesimal generator of the Markov chain plays as a key parameter in these inequalities. Fur-
thermore, we also apply our results to m-order Markov chains such as AR and ARMA models and
mixtures of Markov chains. Finally, a simple method to convert all our bounds for traditional deep
learning to counterparts for Bayesian deep learning is given. Our method can be applied to convert
other similar bounds for i.i.d. datasets in the research literature as well. Bayesian deep learning was
introduced by [17, 18]. The key distinguishable property of a Bayesian approach is marginalization,
rather than using a single setting of weights in (traditional) deep learning [19].

Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neu-
ral networks, which are typically underspeciﬁed by the data, and can represent many compelling
but different solutions. Analysis of machine learning algorithms for Markov and Hidden Markov
datasets already appeared in research literature [20–22]. In practice, some real-world time-series
datasets are not stationary Markov chains. However, we can approximate time-series datasets by
stationary Markov chains in many applications. There are also some other methods of approximat-
ing non-stationary Markov chains by stationary ones via MA and ARMA models in the statistical
research literature. The i.i.d. dataset is a special case of the Markov dataset with stationary distribu-
tion.

2 Preliminaries

2.1 Mathematical Backgrounds

ν, where

∞n=1 on a state space

with transition kernel Q(x, dy) and the initial state
Let a Markov chain
Xn}
{
is a Polish space in R. In this paper, we consider the Markov chains which
X1 ∼
are irreducible and positive-recurrent, so the existence of a stationary distribution π is guaranteed.
An irreducible and recurrent Markov chain on an inﬁnite state-space is called Harris chain [23]. A
Markov chain is called reversible if the following detailed balance condition is satisﬁed:

S

S

π(dx)Q(x, dy) = π(dy)Q(y, dx),

x, y

∀

.

∈ S

d(t) = sup

x

∈S

dTV(Qt(x,

), π),

·

tmix(ε) := min

t : d(t)

{

ε

,

}

≤

Deﬁne

and

τmin := inf
ε
≤

≤

0

1

tmix(ε)

(cid:18)

2
1

ε
ε

−
−

2

,

(cid:19)

tmix := tmix(1/4).

(1)

(2)

(3)

Let L2(π) be the Hilbert space of complex valued measurable functions on
tegrable w.r.t. π. We endow L2(π) with inner product
f, f
h
and

S
f g∗dπ, and norm
1/2
π . Let Eπ be the associated averaging operator deﬁned by (Eπ)(x, y) = π(y),
i

that are square in-
k2,π :=
f
k
,
x, y
∈ S
∀

f, g
h

:=

R

i

k

B

Q

λ =

kL2(π)

L2(π) = maxv:

(4)
EπkL2(π)
where
k2,π. Q can be viewed as a linear operator (inﬁnites-
v
imal generator) on L2(π), denoted by Q, deﬁned as (Qf )(x) := EQ(x,
)(f ), and the reversibility
is equivalent to the self-adjointness of Q. The operator Q acts on measures on the left, creating a
measure µQ, that is, for every measurable subset A of
Q(x, A)µ(dx). For a
Markov chain with stationary distribution π, we deﬁne the spectrum of the chain as

k
k2,π =1 k

, µQ(A) :=

L2(π),

−
Bv

∈S

S

→

→

x

k

·

Q) is not invertible on L2(π)

.

(5)

R

S2 :=

ξ

C : (ξI

−

∈

(cid:8)

2

(cid:9)

It is known that λ = 1

γ∗ [24], where

−

γ∗ :=

1

−




0,

sup

{|

: ξ

ξ
= 1
|
if eigenvalue 1 has multiplicity 1,

∈ S2, ξ

}

,

otherwise

is the the absolute spectral gap of the Markov chain. The absolute spectral gap can be bounded by
the mixing time tmix of the Markov chain by the following expression:



1
γ∗ −

(cid:18)

1

log 2

(cid:19)

tmix ≤

≤

)

,

log(4/π
∗
γ

∗

(6)

where π
= minx
∗
wise positive) for some k
provided algorithms to estimate tmix and γ∗ from a single trajectory.
Deﬁne

πx is the minimum stationary probability, which is positive if Qk > 0 (entry-
1. See [25] for more detailed discussions. In [25, 26], the authors

≥

∈S

M2 :=

ν

∈ M

) :

(
S

dv
dπ

<

∞

,
(cid:27)

(cid:13)
(cid:13)
k · k2 is the standard L2 norm in the Hilbert space of complex valued measurable functions
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:26)

(7)

where
.
on

S

2.2 Problem settings

In this paper, we consider a uniformly bounded class of functions:
M for some ﬁnite constant M .
supf

f

∈F

∞ ≤

:=

f :

F

S →

R

such that

(cid:8)

(cid:9)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Deﬁne the probability measure P (A) :=
Pn be the empirical measure based on the sample (X1, X2,
S f dP and Pnf :=
also denote P f :=
R

A π(x)dx, for any measurable set A
R
S f dPn. Then, we have
R

∈ S
, Xn), i.e., Pn := 1
n

· · ·

n

P f =

f (x)π(x)dx and Pnf =

f (Xi).

. In addition, let
n
i=1 δXi . We

P

(8)

ZS

1
n

i=1
X

On the Banach space of uniformly bounded functions
supf

. Let

Y (f )
|

∈F |

, deﬁne an inﬁnity norm:

F

Y

k

kF

=

n

) := E

1

n−

giδXi

,

Gn(

F

(cid:20)(cid:13)
(cid:13)
(cid:13)
is a sequence of i.i.d. standard normal variables, independent of
(cid:13)
) the Gaussian complexity function of the class

(cid:13)
F (cid:21)
(cid:13)
(cid:13)
(cid:13)

i=1
X

.

F

where
n

gi}
{
Gn(
F
Similarly, we deﬁne

7→

and

Rn(

F

) := E

1

n−

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

εiδXi

n

i=1
X

,
(cid:13)
F (cid:21)
(cid:13)
(cid:13)
(cid:13)

n

P 0

n := n−

1

εiδXi ,

(9)

. We will call

Xi}

{

(10)

(11)

where
εi}
random variables, independent of
function of the class

is a sequence of i.i.d. Rademacher (taking values +1 and
Xi}

. We will call n

Rn(

7→

{

{

F

.

1 with probability 1/2 each)
−
) the Rademacher complexity

For times-series datasets in machine learning, we can assume that feature vectors are generated by
∞n=1 is the corresponding sequence
a Markov chain
. An i.i.d. sequence of
of labels. Furthermore, Q is irreducible and recurrent on some ﬁnite set

∞n=1 with stochastic matrix Q, and

Xn}

Yn}

{

{

F

S

i=1
X

3

6
feature vectors can be considered as a special Markov chain where Q(x, x′) only depends on x′. In
∞n=1 can be considered as being generated by
the supervised learning, the sequence of labels
Yn}
x) = g(x, y) for all
a Hidden Markov Model (HMM), where the emission probability PYn|
Xn (y
R+. It is easy to see that
∞n=1 is a Markov chain with the
1 and g :
n
transition probability

(Xn, Yn)
}

S × Y →

≥

{

{

|

PXn+1Yn+1|

XnYn (xn+1, yn+1|

xn, yn) = Q(xn, xn+1)g(xn+1, yn+1).

(12)

Let ˜Q(x1, y1, x2, y2) := Q(x1, x2)g(x2, y2) for all x1, x2 ∈ S
, which is the tran-
∞n=1 on ˜S :=
sition probability of the Markov chain
. Then, it is not hard to see
∞n=1 is irreducible and recurrent on ˜S, so it has a stationary distribution, say ˜π. The
that
associated following probability measure is deﬁned as

and y1, y2 ∈ Y
S × Y

(Xn, Yn)
}

(Xn, Yn)
}

{

{

P (A) :=

˜π(x, y)dxdy,

and the empirical distribution Pn based on the observations

ZS×Y

(Xk, Yk)
}

{

n
k=1 is

Pn :=

1
n

n

Xk=1

δXk,Yk .

(13)

(14)

2.3 Contributions

In this paper, we aim to develop a set of novel upper bounds on the generalization errors for deep
neural networks with Markov dataset. More specially, our target is to ﬁnd a relationship between
P f and Pnf which holds for all f
in terms of Gaussian and Rademacher complexities. Our
main contributions include:

∈ F

• We develop general bounds on generalization errors for machine learning (and deep learn-

ing) on Markov datasets.

• Since the dataset is non-i.i.d., the standard symmetrization inequalities in high-dimensional
probability can not be applied. In this work, we extend some symmetrization inequalities
for i.i.d. random processes to Markov ones.

• We propose a new method to convert all the bounds for machine learning (and deep learn-

ing) models to Bayesian settings.

• Extensions to m-order homogeneous Markov chains such as AR and ARMA models and

mixtures of several Markov services are given.

3 Main Results

3.1 Probabilistic Bounds for General Function Classes

In this section, we develop probabilistic bounds for general function classes in terms of Gaussian
and Rademacher complexities.

First, we prove the following key lemma, which is an extension of the symmetrization inequality
∞n=1 with the
for i.i.d. sequences (for example, [27]) to a new version for Markov sequences
stationary distribution π and the initial distribution ν
Lemma 1. Let

Xn}
be a class of functions which are uniformly bounded by M . For all n

Z+, deﬁne

∈ M2:

{

F

∈

An :=

˜An :=

Then, the following holds:

s
M
2n

(cid:20)

p

2M

n(1

λ)

−

+

64M 2

n2(1

λ)2

−

dv
dπ −

2τminn log n + √n + 4

1

,

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

(cid:21)

E

1
2

(cid:2)

P 0

nkF

k

˜An ≤

−

(cid:3)

P

E

Pn −
(cid:2)(cid:13)
(cid:13)

≤

F

(cid:3)

(cid:13)
(cid:13)

P 0

nkF

2E

k
(cid:2)

(cid:3)

+ An,

4

(15)

(16)

(17)

n2(1

λ)2

64

−
1(

dv
dπ −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−∞
f
δ

(cid:18)

+ Bn

Pnϕ
(cid:20)
τmin
n

8L(ϕ)
δ
π2
3

≤

+

(cid:19)

(cid:21)(cid:19)

where

.

(18)

P 0

nkF

k

:= sup
f

n

1
n

εif (Xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1
X

∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)
The proof of this lemma can be found in Appendix A of the supplement material. Compared with
the i.i.d. case, the symmetrization inequality for Markov chain in Lemma 1 are different in two
perspectives: (1) The expectation E
is now is under the joint distributions of Markov chain
and Rademacher random variables and (2) The term An appears in both lower and upper bounds
to compensate for the difference between the initial distribution ν and the stationary distribution π
of the Markov chain2. Later, we will see that An represents the effects of data structures on the
generalization errors in deep learning.

nkF

P 0

k

(cid:2)

(cid:3)

By applying Lemma 1, the following theorem can be proved. See a detailed proof in Appendix C in
the supplement material.
Theorem 2. Denote by

Bn :=

n(1

s

2

−

+

λ)

1

,

(19)

Let ϕ be a non-increasing function such that ϕ(x)

≥

,0](x) for all x

R. For any t > 0,

∈

P

f
∃

(cid:18)

: P

f

{

0

}

≤

∈ F

> inf
δ
∈

(0,1]

Rn(

F

)

and

+

t +

log log2 2δ−

1

(cid:0)

p

r
(cid:1)

exp(

2t2)

(20)

−

P

f
∃

(cid:18)

: P

f

{

0

}

≤

∈ F

> inf
δ
∈

(0,1]

+

t +

log log2 2δ−

1

p

r
(cid:1)

(cid:0)
1/√n

Pnϕ

f
δ

(cid:20)
τmin
n

(cid:18)

(cid:19)

+

2
√n

+

2L(ϕ)√2π
δ

Gn(

F

)

+ Bn

≤

(cid:21)(cid:19)

π2
3

exp(

−

2t2).

(21)

, Theorem 2 shows that with high probability, the generalization error can be
Since Bn = O
bounded by Rademacher or Gaussian complexity functions plus an O(1/√n) term, where n is the
length of the training set. This fact also happens in i.i.d. case [4]. However, because the dependency
among samples in Markov chain, the constant in O(1/√n) term is larger than the i.i.d. case.

(cid:0)

(cid:1)

It follows, in particular, in the example of the voting methods of combining classiﬁers [2], from
Theorem 2, we achieve the following PAC-bound:

P

˜f

{

≤

0

} ≤

δ

inf
(0,1]
∈

(cid:20)

˜f

Pn{

+

δ

}

≤

)

V (
H
n

8C
δ r
π2
3α

log

1
2

+ Bn +

+

log log2 2δ−

(cid:18)r
p
α (PAC-Bayes bound), where V (

1

(cid:19)r

τmin
n

(cid:21)

(22)

) is the VC-dimension of the class

H

with probability at least 1

and C is some positive constant.

−

H

3.2 Bounding the generalization error in deep neural networks

In this section, we consider the same example as [4, Section 6]. However, we assume that feature
be a the
vectors in the dataset are generated by a Markov chain instead of an i.i.d. process. Let
class of all uniformly bounded functions f :

is called the class of base functions.

R.

H

S →

H

2This difference causes a burn-in time [28] which is the time between the initial and ﬁrst time that the

Markov chain is stationary.

5

l
j=0 Vj where Vl =
Consider a feed-forward neural network with l layers of neurons V =
. The neurons vi and vo are called the input and the output neurons, respectively. To deﬁne the
vo}
{
network, we will assign the labels to the neurons in the following way. Each of the base neurons
. Each neuron of the j-th layer Vj , where j
is labelled by a function from the base class
1, is
Rm, where m is the number of inputs of the neuron.
labelled by a vector w := (w1, w2,
, wm)
Here, w will be called the vector of weights of the neuron.
Given a Borel function σ from R into [
(w1, w2,

1, 1] (for example, sigmoid function) and a vector w :=
+ 1, let

Rm where m =

vi} ∪

, wm)

H
∈

· · ·

S

≥

{

∈
Nσ,w : Rm

· · ·

≥

R, Nσ,w(u1, u2,

→

, um) := σ

· · ·

(cid:18)

wj uj

.
(cid:19)

(23)

Let σj : j

1 be functions from R into [

1, 1], satisfying the Lipschitz conditions

|

,

v

u

≤

−

−

u, v

∈ S

σj (u)

Lj|
The neural network works can be formed as the following. The input neuron inputs an instance
(cid:12)
(cid:12)
. A base neuron computes the value of the base function on this instance and outputs the value
x
1) computes and outputs through its output
through its output edges. A neuron in the j-th layer (j
, um are the values of the inputs of the
edges the value Nσj ,w(u1, u2,
neuron). The network outputs the value f (x) (of a function f it computes) through the output edge.
Nl the class of feed-forward neural networks
,
H

Nl the set of such networks. We call
and l layers of neurons (and with sigmoid σj). Let

≥
, um) (where u1, u2,

We denote by
with base
H
and then recursively

H0 :=

N∞

· · ·

· · ·

:=

∈

(cid:12)
(cid:12)

(24)

−
|H|

−
σj (v)

m

j=1
X

R.

∞j=1 Nj. Deﬁne
S
Rm

1.

∪ Hj

−

(cid:27)

, hm) : m

0, hi ∈ Hj

−

≥

1, w

∈

(25)

includes all the functions computable by feed-forward

Denote
neural networks with base

H∞

Hj :=
:=

Nσj ,w(h1, h2,

· · ·
(cid:26)
∞j=1 Hj. Clearly,
.
H
S

H∞

Let
putable by feed-forward neural networks with restrictions on the weights of neurons:

be a sequence of positive numbers. We also deﬁne recursively classes of functions com-

bj}

{

Hj(b1, b2,

· · ·

, bj) :=

Nσj ,w(h1, h2,

, hm) : m

0,

≥

· · ·

(cid:26)
1(b1, b2,

hi ∈ Hj

−

∈
k1 denotes the 1-norm of the vector w.

· · ·

−

, bj

1), w

Rm,

w

where

k
Clearly,

w

k

k1 ≤

bj

(cid:27) [

Hj

−

1(b1, b2,

, bj

1),

−

· · ·

(26)

=

∞

· · ·

· · ·

H∞

[ (cid:26)

, bj < +

, bj) : b1,

Hj(b1,

the class of measurable functions ˜f :

.
(cid:27)
Denote by ˜
is the alphabet of labels. ˜
H
H
is introduced for real machine learning applications where we need to work with a new Markov chain
∞n=1 instead of the feature-based
generated from both feature vectors and their labels
(Xn, Yn)
}
Markov chain
∞n=1. See Subsection 2.2 for detailed discussions. For binary classiﬁcation,
, where ˜f (x, y) = yf (x). Let ϕ be a function such that ϕ(x)
˜f : f
˜
,0](x) for
:=
{
H
R and ϕ satisﬁes the Lipschitz condition with constant L(ϕ). Then, the following is a direct
all x
∈
application of Theorem 2.
Theorem 3. For any t

Xn}
{
∈ H}

0 and for all l

R, where

S × Y →

−∞

I(

1,

≥

Y

{

(27)

≥

≥

P

f
∃

(cid:18)

(b1, b2,

· · ·

∈ H

, bl) : P

˜f

{

≤

0

}

> inf
δ
∈

(0,1]

Pnϕ
(cid:20)

(cid:18)

˜f
δ

(cid:19)

+

2√2πL(ϕ)
δ

l

j=1
Y

(2Ljbj + 1)Gn(

H

)

(28)

+

t +

log log2 2δ−

1

p
where Bn is deﬁned in (19).

(cid:0)

r
(cid:1)

τmin
n

+ Bn

≤

(cid:21)(cid:19)

π2
3

exp(

−

2t2),

6

Remark 4. P

˜f

{

0

}

≤

represents the probability of mis-classiﬁcation in the deep neural network.

Proof. Let

′l :=

H

H

(b1, b2,

· · ·

, bl). As the proof of [4, Theorem 13], it holds that

Gn(

′l)

H

l

≤

j=1
Y

(2Ljbj + 1)Gn(

H

).

Hence, (28) is a direct application of Theorem 2 and (29).

Now, given a neural network f

, let

∈ N∞

l(f ) := min
{

j

≥

1 : f

.

∈ Nj}

(29)

(30)

For any number k such that 1
network which is represented by f . Denote by

≤

≤

k

l(f ), let Vk(f ) be the set of all neurons of layer k in the neural

Wk(f ) := max

Vk(f ) k

u

∈

w(u)

bk,

k1 ∨

k = 1, 2,

, l(f )

· · ·

(31)

where w(u) is the coefﬁcient-vector associated with the neuron u in this layer. Deﬁne

l(f )

Λ(f ) :=

(4LkWk(f ) + 1),

Γα(f ) :=

Yk=1

l(f )

Xk=1 r

α
2

log(2 + log2 Wk(f )),

(32)

∞k=1 k−

α. Then, by using Theorem 3 with bk → ∞

where α > 0 is the number such that ζ(α) < 3/2, ζ being the Riemann zeta-function: ζ(α) :=
and the same arguments as [4, Proof of
Theorem 14], we obtain the following result. See Section 1.3 in the supplement material for more
P
detailed derivations.
Theorem 5. For any t

0 and for all l

1,

P

f
∃

(cid:18)

∈ H∞

: P

˜f

{

≤

0

}

> inf
δ
∈

(0,1]

(cid:20)

+

t + Γα(f ) +

log log2 2δ−

1

(cid:18)

where Bn is deﬁned in (19).

≥

p

≥

Pnϕ

˜f
δ

(cid:19)

(cid:18)
τmin
n

+

2√2πL(ϕ)
δ

Λ(f )Gn(

H

) +

2
√n

(cid:19)r

+ Bn

≤

(cid:21)(cid:19)

π2
3

(3

−

2ζ(α))−

1 exp

2t2

,

(33)

(cid:1)

−

(cid:0)

3.3 Generalization Error Bounds on Bayesian Deep Learning

and

For Bayesian machine learning and deep learning,
state space of the Markov chain and

is the
F
is the domain of (random) coefﬁcients. We assume that
are Polish spaces on R, which include both discrete sets and R. For example, in binary
S
classiﬁcation, f (X, W ) = sgn(W T X + b) where the feature X and the coefﬁcient W are random
vectors with speciﬁc prior distributions. In practice, the distribution of W is known which depends
on our design method, and the distribution of X is unknown. For example, W is assumed to be
Gaussian in Bayesian deep neural networks [19].

S × W →

, where

f :

:=

W

W

S

(cid:8)

(cid:9)

R

Since all the bounds on Subsections 3.1 and 3.2 hold for any function f in
at each ﬁxed vector
W = w, hence, they can be directly applied to Bayesian settings where W is random. However,
these bounds are not expected to be tight enough since we don’t use the prior distribution of W
when deriving them. In the following, we use another approach to derive new (and tighter) bounds
for Bayesian deep learning and machine learning from all the bounds in Subsections 3.1 and 3.2. For
illustration purposes, we only derive a new bound. Other bounds can be derived in a similar fashion.
, Wn are i.i.d. random variables as in [19].
We assume that W1, W2,
Let

· · ·

F

δXi,Wi,

(34)

˜Pn :=

1
n

n

i=1
X

7

and deﬁne a new probability measure ˜P on

such that

S × W

˜P (A) :=

˜π(x, w)dxdw,

(35)

for all (Borel) set A on
cess
(Xn, Wn)
}
two new (averaging) linear functionals:

∞n=1 with stochastic matrix ˜Q :=

S × W

{

. Here, ˜π is the stationary distribution of the irreducible Markov pro-
. In addition, deﬁne

Q(x, w)PW (w)

,w

∈S

∈W

}x

{

ZA

and

ˆPn(f ) =

1
n

n

i=1 ZW
X

f (Xi, w)dPW (w),

ˆP (f ) :=

f (x, w)˜π(x, w)dPW (w)dP (x).

(36)

(37)

ZS ZW

In practice, the prior distribution of W is known, so we can estimate ˆPn(f ) based on the training
set
(cf. Section 2.2). The
(X1, Y1), (X2, Y2),
following result can be proved.
Theorem 6. Let ϕ be a sequence of function such that ϕ(x)

, which is a Markov chain on

, (Xn, Yn)
}

,0](x). For any t > 0,

X × Y

· · ·

I(

{

P

f
∃

(cid:18)

: ˆP

f

{

0

}

≤

∈ F

> inf
δ
∈

(0,1]

and

+

t +

log log2 2δ−

1

(cid:0)

p

r
(cid:1)

≥

+

f
δ

(cid:19)

(cid:18)

+ Bn

(cid:21)(cid:19)

ˆPnϕ
(cid:20)
τmin
n

−∞
8L(ϕ)
δ
π2
3

≤

Rn(

F

)

exp(

2t2)

(38)

−

P

f
∃

(cid:18)

: ˆP

f

{

0

}

≤

∈ F

> inf
δ
∈

(0,1]

ˆPnϕ

f
δ

(cid:19)

(cid:18)

+

2L(ϕ)√2π
δ

Gn(

F

) +

2
√n

+

t +

log log2 2δ−

1

(cid:0)

Proof. Let W1, W2,
· · ·
coefﬁcients). For simplicity, we assume that
(Xn, Wn)
}

p
, Wn be an n samples of W
Wn}

{

{
∞n=1 forms a Markov chain with probability transition probability

(cid:20)
τmin
n

r
(cid:1)

+ Bn

≤

(cid:21)(cid:19)

π2
3

exp

2t2

.

(cid:1)

−

(cid:0)

(39)

PW on

(or samples of some set of random
∼
∞n=1 is an i.i.d. sequence. Then, it is obvious that

W

˜Q(xn, wn; xn+1, wn+1) = P(Xn+1 = xn+1, Wn+1 = wn+1|

= Q(xn, xn+1)PW (wn+1).

Xn = xn, Wn = wn)

(40)
(41)

+

t +

log log2 2δ−

1

+ Bn

exp(

2t2).

(42)

−

Rn(

F

)

8L(ϕ)
δ
π2
3

≤

(cid:21)(cid:19)
2t2), it holds that

(cid:20)
τmin
n

r
(cid:1)
π2
3 exp(

−

From Theorem 2, it holds that

P

f
∃

(cid:18)

: ˜P

f

{

0

}

≤

∈ F

> inf
δ
∈

(0,1]

˜Pnϕ

+

f
δ

(cid:19)

(cid:18)

This means that with probability at least 1

(cid:0)

p

˜P

f

{

0

≤

} ≤

δ

inf
(0,1]
∈

(cid:20)

˜Pnϕ

(cid:18)

+

f
δ

(cid:19)

−
8L(ϕ)
δ

From (43), it holds that with probability at least 1

2t2),

˜P

f

{

0

≤

} ≤

δ

1
n

inf
(0,1]
∈

(cid:20)

n

ϕ

i=1
X

(cid:18)

p

(cid:0)
π2
3 exp(

−
−
f (Xi, Wi)
δ

Rn(

F

) +

t +

log log2 2δ−

1

τmin
n

+ Bn

.
(cid:21)

(43)

r
(cid:1)

+

(cid:19)
τmin
n

8L(ϕ)
δ

Rn(

F

)

+ Bn

.

(cid:21)

(44)

+

t +

log log2 2δ−

1

(cid:0)

p

8

r
(cid:1)

f (Xi, Wi)
δ

+

8L(ϕ)
δ

Rn(

F

)

(cid:19)

ϕ

(cid:18)

From (44), with probability at least 1

−

2t2), it holds that
−
n

π2
3 exp(
1
n

inf
(0,1]
∈

δ

(cid:20)

EW

˜P

f

{

≤

0

}

≤

EW

(cid:2)

+

t +

(cid:3)
log log2 2δ−

1

(cid:0)

≤

δ

inf
(0,1]
∈

(cid:20)

1
n

p
n

i=1
X

EW

ϕ

(cid:20)

(cid:18)

+

t +

log log2 2δ−

1

r
(cid:1)
f (Xi, W )
δ

(cid:21)
8L(ϕ)
δ

+

Rn(

F

)

i=1
X

(cid:20)
τmin
n

+ Bn

(cid:19)(cid:21)

τmin
n

+ Bn

.

(cid:21)
From (46), we obtain (38). Similarly, we can achieve (39).

p

r
(cid:1)

(cid:0)

(45)

(46)

4 Extension to High-Order Markov Chains

In this subsection, we extend our results in previous sections to m-order homogeneous Markov chain.
The main idea is to convert m-order homogeneous Markov chains to 1-order homogeneous Markov
chain and use our results in previous sections to bound the generalization error. We start with the
following simple example.
Example 7. [m-order moving average process without noise] Consider the following m-order
Markov chain

m

Xk =

i=1
X

aiXk

i,

−

Z+.

k

∈

Let Yk := [Xk+m
where

−

1, Xk+m

2,

−

· · ·

, Xk]T . Then, from (119), we obtain Yk+1 = GYk,

(47)

Z+

k

∀

∈

(48)

a1 a2
0
1
1
0
...
...
0
0

G := 






· · ·
· · ·
· · ·
. . .

· · ·

am
−
0
0
...
1

1 am
0
0
...
0



.







Yn}
It is clear that
m-order Markov chain

{

∞n=1 is an order-1 Markov chain. Hence, instead of directly working with the

∞n=1, we can ﬁnd an upper bound for the Markov chain

Yn}

{

∞n=1.

Xn}

{

To derive generalization error bounds for the Markov chain
arguments. For all f
f (xk) where ˜f :
m

and (xk, xk+1,

, xk+m

· · ·

−

∈ F
→

R, we obtain
n

S

1), by setting ˜f (xk, xk+1,

∞n=1, we can use the following
1) =

, xk+m

Yn}

{

· · ·

−

1
n

1

f (Xi)

{

=

0

}

≤

1
n

n

1

˜f (Yi)

{

i=1
X
Hence, by applying all the results for 1-order Markov chain
Yn}
{
∞n=1.
upper bounds for the sequence of m-order Markov chain
Xn}

i=1
X

{

0

.

}

(49)

≤

∞n=1, we obtain corresponding

This approach can be extended to more general Markov chain. See Section 4 in the Supplementary
Material for details.

5 Conclusions

In this paper, we derive upper bounds on generalization errors for machine learning and deep neural
networks based on a new assumption that the dataset has Markov or hidden Markov structure. We
also propose a new method to convert all these bounds to Bayesian deep learning and machine
learning. Extension to m-order Markov chains and a mixture of Markov chains are also given. An
interesting future research topic is to develop some new algorithms to evaluate performance of these
bounds on real Markov datasets.

9

References

[1] V. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.

[2] Peter Bartlett, Yoav Freund, Wee Sun Lee, and Robert E. Schapire. Boosting the margin: a
new explanation for the effectiveness of voting methods. The Annals of Statistics, 26(5):1651
– 1686, 1998.

[3] Peter Bartlett and John Shawe-Taylor. Generalization Performance of Support Vector Ma-

chines and Other Pattern Classiﬁers, page 43–54. MIT Press, 1999.

[4] V. Koltchinskii and D. Panchenko. Empirical Margin Distributions and Bounding the General-

ization Error of Combined Classiﬁers. The Annals of Statistics, 30(1):1 – 50, 2002.

[5] J. Langford and J. Shawe-Taylor. PAC-Bayes and Margins. In Advances of Neural Information

Processing Systems (NIPS), 2003.

[6] D. A. McAllester. Pac-bayesian stochastic model selection. Machine Learning, 51, 2004.

[7] E. Parrado-Hern’’andez A. Ambroladze and J. ShaweTaylor. Tighter PAC-Bayes bounds. In

NIPS, 2007.

[8] G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep
(stochastic) neural networks with many more parameters than training data. In Uncertainty in
Artiﬁcial Intelligence (UAI), 2017.

[9] M. Austern R. P. Adams W. Zhou, V. Veitch and P. Orbanz. Non-vacuous generalization
bounds at the imagenet scale: a PAC-Bayesian compression approach. In The International
Conference on Learning Representations (ICLR), 2019.

[10] F. Biggs and B. Guedj. Differentiable PAC-Bayes objectives with partially aggregated neural

networks. Entropy, 23, 2021.

[11] A. McAllester. Some PAC-Bayesian theorems. In Conference on Learning Theory (COLT),

1998.

[12] Eugenio Clerico, George Deligiannidis, and Arnaud Doucet. Wide stochastic networks: Gaus-

sian limit and PACBayesian training. Arxiv: 2106.09798, 2021.

[13] Eugenio Clerico, George Deligiannidis, and Arnaud Doucet. Conditional Gaussian PAC-Bayes.

Arxiv: 2110.1188, 2021.

[14] A. Xu and M. Raginsky. Information-theoretic analysis of generalization capability of learning

algorithms. In Advances of Neural Information Processing Systems (NIPS), 2017.

[15] Amedeo Roberto Esposito, Michael Gastpar, and Ibrahim Issa. Generalization error bounds
via Rényi-f-divergences and maximal leakage. IEEE Transactions on Information Theory, 67
(8):4986–5004, 2021.

[16] D. Jakubovitz, R. Giryes, and M. R. D. Rodrigues. Generalization Error in Deep Learning.

Arxiv: 1808.01174, 30, 2018.

[17] David J.C. MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute

of Technology, United States, 1992.

[18] David J. C. Mackay. Probable networks and plausible predictions - a review of practical
bayesian methods for supervised neural networks. Network: Computation In Neural Systems,
6:469–505, 1995.

[19] A. G. Wilson and P. Izmailov. Bayesian deep learning and a probabilistic perspective of model

construction. In Proc. 37th ICML. Morgan Kaufmann, 2020.

[20] John C. Duchi, Alekh Agarwal, Mikael Johansson, and Michael I. Jordan. Ergodic mirror
descent. 2011 49th Annual Allerton Conference on Communication, Control, and Computing
(Allerton), pages 701–706, 2011.

10

[21] Gang Wang, Bingcong Li, and Georgios B. Giannakis. A multistep lyapunov approach for

ﬁnite-time analysis of biased stochastic approximation. ArXiv, abs/1909.04299, 2019.

[22] Lan V. Truong. On linear model with markov signal priors. In AISTATS, 2022.

[23] Pekka Tuominen and Richard L. Tweedie. Markov Chains with Continuous Components. Pro-

ceedings of the London Mathematical Society, s3-38(1):89–114, 01 1979.

[24] Daniel Paulin. Concentration inequalities for Markov chains by Marton couplings and spectral

methods. Electronic Journal of Probability, 20(79):1 – 32, 2015.

[25] G. Wolfer and A. Kontorovich. Estimating the mixing time of ergodic markov chains. In 32nd

Annual Conference on Learning Theory, 2019.

[26] R. Combes and M. Touati. Computationally efﬁcient estimation of the spectral gap of a markov
chain. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 3:1 – 21,
2019.

[27] A. W. Van Der Vaart and Wellner. Weak convergence and Empirical Processes. Springer, New

York, 1996.

[28] D. Rudolf. Explicit error bounds for Markov chain Monte Carlo. Arxiv: 1108.3201, 2011.

[29] Pascal Lezaud. Chernoff and Berry-Esséen inequalities for Markov processes. ESAIM: Prob-

ability and Statistics, EDP Sciences, 5:183–201, 2001.

[30] M. Ledoux and M. Talagrand. Probability in Banach Spaces. Springer, New York., 1991.

[31] Lan V. Truong. On rademacher complexity-based generalization bounds for deep learning.

ArXiv, abs/2208.04284, 2022.

[32] A. W. Van Der Vaart and J. A. Wellner. Weak convergence and Empirical Processes. Springer,

New York., 1996.

[33] P. Billingsley. Probability and Measure. Wiley-Interscience, 3rd edition, 1995.

[34] I. Kontoyiannis, L. A. Lastras-Montaño, and S. P. Meyn. Relative entropy and exponential
deviation bounds for general Markov chains. In Proc. of Intl. Symp. on Inform. Th., 2005.

11

6 Supplemental Materials

6.1 Probabilistic Bounds for General Function Classes

In this section, we develop probabilistic bounds for general function classes in terms of Gaussian
and Rademacher complexities. First, we prove the following key lemma, which is an extension of
the symmetrization inequality for i.i.d. sequences (for example, [27]) to a new version for Markov
sequences
∞n=1 with the stationary distribution π and the initial distribution ν
Xn}
Lemma 8. Let
be a class of functions which are uniformly bounded by M . For all n
F

∈ M2:
∈

Z+, deﬁne

{

An :=

˜An :=

Then, the following holds:

s
M
2n

(cid:20)

p

2M

n(1

λ)

−

+

64M 2

n2(1

λ)2

−

dv
dπ −

2τminn log n + √n + 4

1

,

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

(cid:21)

where

E

1
2

(cid:2)

P 0

nkF

k

˜An ≤

−

(cid:3)

P

E

Pn −
(cid:2)(cid:13)
(cid:13)

P 0

nkF

2E

k
(cid:2)

(cid:3)

+ An,

≤

F

(cid:3)

(cid:13)
(cid:13)
n

Proof. See Appendix A.

P 0

nkF

k

1
n

:= sup
f

∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1
X

.

εif (Xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(50)

(51)

(52)

(53)

Next, we prove the following theorem.
Theorem 9. Consider a countable family of Lipschitz function Φ =
R satisﬁes 1(
Deﬁne

ϕk(x) for all k. For each ϕ

,0](x)

−∞

≤

∈

1
→
Φ, denote by L(ϕ) its Lipschitz constant.

, where ϕk : R

ϕk : k

≥

}

{

Then, for any t > 0,

Bn :=

n(1

s

2

−

+

λ)

64

n2(1

λ)2

−

dv
dπ −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1

.

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

: P

f

{

0

}

> inf
k>0

≤

(cid:20)

Pnϕk(f ) + 4L(ϕk)Rn(

F

P

f
∃

(cid:18)

≤

and

∈ F
π2
3

exp(

2t2)

−

) +

t +

log k

(cid:0)

p

r
(cid:1)

τmin
n

+ Bn

P

f
∃

(cid:18)

: P

f

{

0

}

> inf
k>0

≤

∈ F

Pnϕk(f ) + √2πL(ϕk)Gn(

F

)

(cid:20)

+

2
√n

+

t +

log k

(cid:0)

p

r
(cid:1)

τmin
n

≤

(cid:21)(cid:19)

π2
3

exp(

−

2t2).

(54)

(cid:21)(cid:19)

(55)

(56)

Proof. See Appendix B.

Theorem 10. Let ϕ is a non-increasing function such that ϕ(x)
any t > 0,

≥

1(

,0](x) for all x

−∞

R. For

∈

P

f
∃

(cid:18)

∈ F

: P

{
π2
3

≤

f

≤

0

}

> inf
δ
∈

(0,1]

Pnϕ

(cid:20)

(cid:18)

+

8L(ϕ)
δ

Rn(

F

f
δ

(cid:19)

exp(

−

2t2)

) +

t +

log log2 2δ−

1

(cid:0)

p

τmin
n

+ Bn

(cid:21)(cid:19)

(57)

r
(cid:1)

12

and

P

f
∃

(cid:18)

where Bn is deﬁned in (54).

Proof. See Appendix C.

: P

f

{

≤

∈ F

0

}

> inf
δ
∈

(0,1]

Pnϕ

(cid:20)

(cid:18)

f
δ

(cid:19)

+

2L(ϕ)√2π
δ

Gn(

)

+

2
√n

+

t +

log log2 2δ−

1

(cid:0)

p

r
(cid:1)

τmin
n

+ Bn

≤

(cid:21)(cid:19)

F
π2
3

exp(

2t2),

(58)

−

In the next statements, we use Rademacher complexities, but Gaussian complexities can be used
R
similarly. Now, assume that ϕ is a function from R to R such that ϕ(x)
and ϕ satisﬁes the Lipschitz with constant L(ϕ). Then, the following theorems can be proved by
using similar arguments as the proof of Theorem 10.

,0](x) for all x

−∞

I(

≤

∈

Theorem 11. Let ϕ is a nonincreasing function such that ϕ(x)
any t > 0,

≤

1(

,0](x) for all x

−∞

R. For

∈

P

f
∃

(cid:18)

: P

f

{

0

}

≤

∈ F

< sup
δ

(0,1] (cid:20)

∈

Pnϕ

+

8L(ϕ)
δ

Rn(

F

)

f
δ

(cid:19)

+

t +

log log2 2δ−

1

and

(cid:0)

p

P

f
∃

(cid:18)

: P

f

{

0

}

≤

∈ F

< sup
δ

(0,1] (cid:20)

∈

+

t +

log log2 2δ−

1

where Bn is deﬁned in (54).

(cid:0)

p

r
(cid:1)

(cid:18)
τmin
n

r
(cid:1)

Pnϕ

f
δ

(cid:19)

(cid:18)
τmin
n

+ Bn

≤

(cid:21)(cid:19)

π2
3

2t2)

exp(

−

(59)

+

2√2πL(ϕ)
δ

Gn(

F

) +

2
√n

+ Bn

≤

(cid:21)(cid:19)

π2
3

exp(

−

2t2),

(cid:21)(cid:19)

(60)

By combining Theorem 10 and Theorem 11, we obtain the following result.

Theorem 12. Let

Then, for all t > 0,

∆n(

F

; δ) :=

8
δ

Rn(

F

) +

r

τmin log log2 2δ−
n

1

+ Bn.

(61)

P

f
∃

(cid:18)

:

∈ F

≤

and

Pn{
(cid:12)
2π2
(cid:12)
3

f

0

≤

} −

P

f

{

0

}

≤

2t2)

(cid:12)
(cid:12)

exp(

−

> inf
δ
∈

(0,1]

f

Pn{|
(cid:18)

δ

}

| ≤

+ ∆n(

F

; δ) + t

τmin
n

r

(cid:19)(cid:19)

(62)

P

f
∃

(cid:18)

:

∈ F

≤

Pn{
(cid:12)
2π2
(cid:12)
3

f

0

≤

} −

P

f

{

0

}

≤

2t2).

(cid:12)
(cid:12)

exp(

−

> inf
δ
∈

(0,1]

P

(cid:18)

f

{|

| ≤

δ

}

+ ∆n(

F

; δ) + t

τmin
n

(cid:19)

r

(63)

Proof. Equation (62) is drawn by setting ϕ(x) = 1
+ (1
10 and Theorem 11. Equation (63) is drawn by setting ϕ(x) = 1
these theorems.

≤

x

{

0

}

{

−
x

0

x)1
{
≤
1
} −

≤ −

x
x1

13

1
}
1
≤

≤
{−

in Theorem
in

x

0

≤

}

6.2 Conditions on Random Entropies and γ-Margins

As [4], given a metric space (T, d), we denote by Hd(T ; ε) the ε-entropy of T with respect to d, that
is

Hd(T ; ε) := log Nd(T ; ε),

(64)

where Nd(T ; ε) is the minimal number of balls of radius ε covering T . Let dPn,2 denote the metric
of the space L2(

; dPn):

S

For each γ

∈

(0, 1], deﬁne

dPn,2(f, g) :=

f

Pn|

−

g

2
|

1/2

.

(cid:0)

(cid:1)

and

δn(γ; f ) := sup

(0, 1) : δ

γ

2 P (f

δ)

≤

≤

n−

δ
(cid:26)

∈

ˆδn(γ; f ) := sup

(0, 1) : δ

γ

2 Pn(f

δ)

≤

≤

n−

δ
(cid:26)

∈

1

2 + γ

4

1

2 + γ

4

(cid:27)

.
(cid:27)

We call δn(γ; f ) and ˆδn(γ; f ), respectively, the γ-margin and empirical γ-margin of f .
Theorem 13. Suppose that for some α

(0, 2) and some constant D > 0,

∈
; u)

HdPn ,2(

F

Du−

α,

≤

u > 0

a.s.,

(65)

(66)

(67)

(68)

Then, for any γ

2α
2+α , there exists some constants ζ, υ > 0 such that when n is large enough,

≥
1 ˆδn(γ; f )

: ζ−

P

f
∀

(cid:20)

∈ F

δn(γ; f )

≤

ζ ˆδn(γ; f )
(cid:21)

≤

1

−

≥

2υ

log2 log2 n

exp

γ
2 /2

n

. (69)

−

(cid:0)

(cid:1)

(cid:8)

(cid:9)

Proof. See Appendix D for a detailed proof.

6.3 Convergence rates of empirical margin distributions

First, we prove the following lemmas.
Lemma 14. For any class
least 1
2 exp

2t2

F

, the following holds:

−

−

of bounded measurable functions from

R, with probability at

S →

(cid:0)

(cid:1)
sup
f

∈F

sup
R
y

∈

(cid:12)
(cid:12)

Pn(f

y)

−

≤

P (f

≤

y)

≤

Bn + t

τmin
n

,

r

(70)

p

(cid:12)
(cid:12)

where Bn is deﬁned in (54).
Remark 15. By setting t = √2 log n, (70) shows that supf
0 as n

.
→ ∞

Proof. See Appendix E.

Now, for each f

, deﬁne

∈ F

≤
{
Let L denote the Lévy distance between the distributions in R:

≤

}

Fn,f := Pn{

f

Ff (y) := P

f

y

,

supy

R

∈

∈F

Pn(f

y)

−

≤

P (f

≤

y)

→

(cid:12)
(cid:12)

y

,

}

R.

y

∈

(cid:12)
(cid:12)

(71)

L(F, G) := inf

δ > 0 : F (t)

G(t + δ) + δ,

G(t)

F (t + δ) + δ,

Lemma 16. Let M > 0 and
0, 0 for x
equal to 1 for x

F
≥

{

≤

≤
be a class of measurable functions from
1 and linear between them. Deﬁne

≤

into [

−

S

R

.

(72)

t
∀
}
M, M ]. Let ϕ be

∈

˜
Gϕ :=

ϕ

(cid:26)

◦

(cid:18)

f

y

−
δ

−

(cid:19)

1 : f

,

∈ F

y

[

−

∈

M, M ]

(cid:27)

(73)

14

for some δ > 0. Recall the deﬁnition of Bn in (54). Then, for all t > 0 and δ > 0, the following
holds:

P

sup
f
∈F
Especially, for all t > 0, we have

L(Ff , Ff,n)

(cid:26)

≥

δ + E

P 0
nk

k

˜
Gϕ

(cid:2)

(cid:3)

+ Bn + t

τmin
n

r

≤

(cid:27)

2 exp(

−

2t2).

(74)

P

sup
f

∈F

(cid:26)

L(Ff , Ff,n)

≥

4

E[

q

P 0

nkF

k

] + M/√n + Bn + t

τmin
n

r

≤

(cid:27)

2 exp(

−

2t2).

(75)

Proof. See Appendix F.

In what follows, for a function f from
equal to f if

f

into R and M > 0, we denote by fM the function that is
M if f <

M . We set

|

| ≤

S
M , is equal to M if f > M and is equal to
FM :=
∞

into [0,

fM : f

∈ F }
) is called an envelope of

from

−

F

S

(cid:8)

.

−

iff

|

F

f (x)

| ≤

F (x) for all

is a Glivenko-Cantelli class with respect to P (i.e.,
BCLT(P) and say that

0 as
satisﬁes the Bounded Central Limit Theorem

Pn −

kF →

P

k

F

F
F ∈

As always, a function
.
and all x
f

∈ F
We write
n
→ ∞
for P iff

∈ S
GC(P) iff

F ∈
a.s.). We write

Based on Lemma and Lemma 16, we prove the following theorems.
Theorem 17. Suppose that

(cid:13)
(cid:13)

(cid:3)

P

= O(n−

1/2).

F

E

Pn −
(cid:2)(cid:13)
(cid:13)

Then, the following two statements are equivalent:

sup
f

∈F

P

f

{|

| ≥

M

} →

0

as

M

.
→ ∞

• (i)

FM ∈

and

GC(P) for all M > 0

• (ii) supf

∈F

L(Fn,f , Ff )

0

a.s.

→

n

.
→ ∞

Proof. See Appendix G.

Next, the following theorems hold.
Theorem 18. [4, Theorem 7] The following two statements are equivalent:

• (i)

F ∈

and

GC(P) for all M > 0

• (ii) there exists a P -integrable envelope for the class
0

L(Fn,f , Ff )

and

n

supf

(c) =

F

f

{

−

P f : f

∈ F }

∈F

→ ∞
L(Fn,f , Ff )

→

sup
f

∈F

0

→

a.s.

n

.
→ ∞

Now, we prove the following theorem.
Theorem 19. Suppose that the class

F

Moreover, for some α

then

L(Fn,f , Ff ) = OP

sup
f

(cid:18)(cid:18)
∈F
(0, 2) and for some D > 0
∈
HdPn ,2(

Du−

; u)

α log n,

F

≤

is uniformly bounded. If

BCLT(P), then

F ∈

n

.
→ ∞

log n
n

1/4

(cid:19)

(cid:19)

u > 0,

a.s.,

sup
f

∈F

L(Fn,f , Ff ) = O

n−

1

2+α log n

n

,
→ ∞

a.s.,

(cid:0)

(cid:1)

Proof. Appendix H.

15

(76)

(77)

(78)

and

(79)

(80)

(81)

(82)

6.4 Bounding the generalization error of convex combinations of classiﬁers

We start with an application of the inequalities in Subsection 6.1 to bounding the generalization error
= K.
in general classiﬁcation problems. Assume that the labels take values in a ﬁnite set
Consider a class ˜
for an
F
iff
example x

into R. A function f

predicts a label y

of functions from

|Y|
∈ Y

S × Y

with

˜
F

Y

∈

∈ S

f (x, y) > max
=y

y′

f (x, y′).

(83)

x), so
In practice, f (x, y) can be set equal to P (y
margin of a labelled example (x, y) is deﬁned as

|

F

can be assumed to be uniformly bounded. The

(84)

(85)

(86)

(87)

(88)

(89)

(90)

mf,y(x) := f (x, y)

max
y′
=y

−

f (x, y′),

so f mis-classiﬁes the label example (x, y) if and only if mf,y ≤
˜
F

, y) : y

f (
·

∈ Y

:=

, f

F

∈

0. Let

.

Then, we can show the following theorem.

(cid:8)

Theorem 20. For all t > 0, it holds that

P

f
∃

(cid:18)

: P

mf,y ≤

{

0

}

∈ F

> inf
δ
∈

(0,1]

+

t +

log log2 2δ−

1

where Bn is deﬁned in (54). Here,

(cid:0)

p

Pn{
(cid:20)
τmin
n

r
(cid:1)

(cid:9)

mf,y ≤

δ

}

+ Bn

≤

(cid:21)(cid:19)

+

8
δ
π2
3

(2K

1)Rn(

F

)

−

exp(

−

2t2),

and

P

mf,y ≤

{

0

}

:=

Xx
∈S

π(x)1

{

mf,y(x)

0

,

}

≤

Pn{

mf,y ≤

δ

}

=

1
n

n

Xk=1

mf,y(Xk)

1

{

δ

}

≤

is the empirical distribution of the Markov process

mf,y(Xn)
}

{

∞n=1 given f .

Proof. First, we need to bound the Rademacher’s complexity for the class of functions
˜
F

. Observe that

mf,y : f

∈

(cid:8)

(cid:9)

E

(cid:20)

By [4, Proof of Theorem 11], we have

n−

sup
˜
f
F (cid:12)
(cid:12)
(cid:12)
(cid:12)

∈

n

1

j=1
X

.

(cid:21)

εjmf,y(Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

n−

n

j=1
X

E

(cid:20)

sup
˜
f
F (cid:12)
(cid:12)
(cid:12)
(cid:12)

∈

εjmf,y(Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(2K

1)Rn(

F

),

−

≤

(cid:21)

) is the Rademacher complexity function of the class

where Rn(
. Now, assume that this class
of function is uniformly bounded as in practice. Hence, by Theorem 10 for ϕ that is equal to 1 on
(
−∞

) and is linear in between, we obtain (86).

, 0], is equal to 0 on [1, +

∞

F

F

In addition, by using the fact that (X1, Y1)
(Xn, Yn) forms a Markov chain with
(X2, Y2)
stationary distribution ˜π (see the discussion on Section 2.2 in the main document), by applying
Theorem 10, we obtain the following result:

· · · −

−

16

6
6
Theorem 21. Let ϕ is a nonincreasing function such that ϕ(x)
any t > 0,

≥

1(

,0](x) for all x

−∞

R. For

∈

P

f
∃

(cid:18)

˜
F

∈

: P

˜f

{

≤

0

}

> inf
δ
∈

(0,1]

(cid:20)

+

t +

log log2 2δ−

1

˜f
δ

(cid:19)

Pnϕ

(cid:18)
τmin
n

+

8L(ϕ)
δ

Rn(

H

)

+ Bn

≤

(cid:21)(cid:19)

π2
3

exp(

−

2t2),

(91)

(cid:0)
where Bn is deﬁned in (54).

p

r
(cid:1)

As in [4], in the voting methods of combining classiﬁers, a classiﬁer produced at each iteration is
a convex combination ˜f of simple base classiﬁers from the class
. In addition, the Rademacher
complexity can be bounded above by

H

Rn(

H

C

)

≤

)

V (
H
n

r

) is the VC-dimesion of

H

for some constant C > 0, where V (

is equal to 0 on [1, +
with probability at least 1

∞

) and is linear in between. By setting tα =

α, it holds that

−

q

H

. Let ϕ be equal to 1 on (

, 0],
3α , from Theorem 21,

−∞

1

2 log π2

P

˜f

0

{

≤

} ≤

δ

inf
(0,1]
∈

Pn{
(cid:20)

˜f

+

δ

}

≤

8C
δ r

V (
H
n

r
(cid:1)
which extends the result of Bartlett et al. [2] to Markov dataset (PAC-bound).

p

(cid:0)

)

+

tα +

log log2 2δ−

1

τmin
n

+ Bn

, (93)

(cid:21)

6.5 Bounding the generalization error in neural network learning

In this section, we consider the same example as [4, Section 6]. However, we assume that feature
be a class
vectors in this dataset is generated by a Markov chain instead of an i.i.d. process. Let
be the set of function ˜f :
R.
of measurable functions from
The introduction of ˜
∞n=1 which is generated by
(Xn, Yn)
}
H
∞n=1.
both feature vectors and their labels instead of the feature-based Markov chain

R (base functions). Let ˜
H
is to deal with the new Markov chain
{

H
S × Y →

S →

Xn}

{

Consider a feed-forward neural network where the set V of all the neurons is divided into layers

V =

vi} ∪

{

l

Vj

j=0
[

(94)

{

vo}

where Vl =
. The neurons vi and vo are called the input and the output neurons, respectively.
To deﬁne the network, we will assign the labels to the neurons in the following way. Each of the
base neurons is labelled by a function from the base class
. Each neuron of the j-th layer Vj, where
H
Rm, where m is the number of inputs of
j
, wm)
∈
the neuron, i.e. m =
+ 1. Here, w will be called the vector of weights of the neuron.
Given a Borel function σ from R into [
(w1, w2,

1, 1] (for example, sigmoid function) and a vector w :=

1, is labelled by a vector w := (w1, w2,

Rm, let

, wm)

|H|

· · ·

−

≥

· · ·

∈

Nσ,w : Rm

R,

→

Nσ,w(u1, u2,

, um) := σ

· · ·

m

wj uj

.
(cid:19)

(cid:18)

j=1
X

For w

Rm,

∈

w

k1 :=

k

Let σj : j

≥

1 be functions from R into [

m

i=1
X

.

wi|

|

−
σj (v)

1, 1], satisfying the Lipschitz conditions

u

Lj|

v

,

|

−

≤

u, v

R.

∈

σj (u)

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

17

(92)

(95)

(96)

(97)

. A base neuron
The network works the following way. The input neuron inputs an instance x
computes the value of the base function on this instance and outputs the value through its output
1) computes and outputs through its output edges the value
edges. A neuron in the j-th layer (j
, um are the values of the inputs of the neuron). The
Nσj ,w(u1, u2,
network outputs the value f (x) (of a function f it computes) through the output edge.

≥
, um) (where u1, u2,

∈ S

· · ·

· · ·

We denote by
with base
H
and then recursively

Nl the set of such networks. We call
and l layers of neurons (and with sigmoid σj). Let

Nl the class of feed-forward neural networks
,
H

H0 :=

N∞

:=

∞j=1 Nj. Deﬁne
S
Rm

1.

∪ Hj

−

(cid:27)

, hm) : m

0, hi ∈ Hj

−

≥

1, w

∈

(98)

includes all the functions computable by feed-forward

Denote
neural networks with base

H∞

Hj :=
:=

Nσj ,w(h1, h2,

· · ·
(cid:26)
∞j=1 Hj. Clearly,
.
H
S

H∞

be a sequence of positive numbers. We also deﬁne recursively classes of functions com-

{

bj}

Let
putable by feed-forward neural networks with restrictions on the weights of neurons:
Hj(b1, b2,
:=

· · ·
Nσj ,w(h1, h2,

, hm) : n

1(b1, b2,

1), w

Rm,

, bj)

, bj

· · ·

−

∈

(cid:26)

Clearly,

[

· · ·

Hj

−

1(b1, b2,

, bj

−

· · ·

≥

0, hi ∈ Hj
1).

−

w

k

k1 ≤

(99)

bj

(cid:27)
(100)

(101)

=

H∞

Hj(b1, b2,

· · ·

, bj) : b1, b2,

, bj < +

· · ·

[ (cid:26)
R and ϕ satisﬁes
As in the previous section, let ϕ be a function such that ϕ(x)
the Lipschitz condition with constant L(ϕ). Then, the following is a direct application of Theorem
10.
Theorem 22. For any t

0 and for all l

−∞

I(

1,

≥

∈

.
(cid:27)

∞
,0] for all x

≥
(b1, b2,

· · ·

P

f
∃

(cid:18)

∈ H

> inf
δ
∈

(0,1]

Pnϕ

(cid:18)

(cid:20)
2
√n

≥

, bl) : P

˜f

{

0

}

≤

˜f
δ

(cid:19)

+

2√2πL(ϕ)
δ

l

j=1
Y

+

+

t +

log log2 2δ−

1

(cid:0)
where Bn is deﬁned in (54).

p

Proof. Let

(2Ljbj + 1)Gn(

H

)

τmin
n

+ Bn

≤

(cid:21)(cid:19)

π2
3

exp

2t2

,

(cid:1)

−

(cid:0)

r
(cid:1)

H
As the proof of [4, Theorem 13], it holds that

H

′l :=

(b1, b2,

, bl).

· · ·

Gn(

H

′l) := E
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n

1

n−

giδXi

i=1
X

l

≤

′
l

H

j=1
Y

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Hence, (102) is a direct application of Theorem 10 and (104).

(2Ljbj + 1)Gn(

H

).

(102)

(103)

(104)

Now, given a neural network f

, let

∈ N∞

For a number k, 1
≤
representing f . Denote

k

≤

l(f ) := min
{

.
l(f ), let Vk(f ) denote the set of all neurons of layer k in the graph

∈ Nj}

1 : f

(105)

≥

j

Wk(f ) := max

Vk(f ) k

u

∈

w(u)

bk,

k1 ∨

k = 1, 2,

, l(f )

· · ·

(106)

18

where w(u) is the coefﬁcient vector associated with the neuron u of the layer k, and let

l(f )

Λ(f ) :=

(4LkWk(f ) + 1),

Γα(f ) :=

Yk=1
l(f )

Xk=1 r

α
2

log(2 + log2 Wk(f )),

where α > 0 is the number such that ζ(α) < 3/2, ζ being the Riemann zeta-function:

ζ(α) :=

∞

k−

α.

Xk=1

(107)

(108)

(109)

Then, by using the same arguments as [4, Proof of Theorem 14], we obtain the following result.

Theorem 23. For any t

0 and for all l

1,

≥

≥

P

f
∃

(cid:18)

: P

˜f

{

≤

∈ H∞

0

}

> inf
δ
∈

(0,1]

Pnϕ
(cid:20)

(cid:18)

˜f
δ

(cid:19)

+

2√2πL(ϕ)
δ

Λ(f )Gn(

H

)

+

2
√n

+

t + Γα(f ) +

log log2 2δ−

1

(cid:0)
π2
3

≤

2ζ(α)

3

−

(cid:0)

(cid:1)

p
1
−

exp

2t2

,

(cid:1)

−

(cid:0)

where Bn is deﬁned in (54).

τmin
n

+ Bn

(cid:21)(cid:19)

r
(cid:1)

Proof. Let

∆k :=

1, 2k)

[2k
−
[1/2, 2)

for k
fork = 1.

(cid:26)

Z, k

= 0, 1

∈

Then, using the following partition:

f

{

∈ H∞}

∞

=

Z
[l=0 [k1∈
\{

Z
} [k2∈
0
\{

0

}

· · ·

Z
[kl∈
\{

0

}

f
(cid:26)

∈ H∞

: l(f ) = l, Wj(f )

∆kj ,

∈

j
∀

∈

On each subset

Γα(f ) by

f
(cid:26)

∈ H∞

: l(f ) = l, Wj(f )

∆kj ,

∈

j
∀

∈

[l]

, we can lower bound Λ(f ) and

(cid:27)

(110)

(111)

[l]

.
(cid:27)
(112)

(113)

(114)

l

j=1
Y
l

Λ(f )

Γα(f )

≥

≥

(2Lj2kj + 1),

α
2

log(
|

kj |

+ 1).

j=1 r
X

19

6
By replacing t by t +
event and then using the union bound, we can show that
p

P

l
j=1

α
2 log(kj + 1) and using Theorem 10 to bound the probability of each

P

f
∃

(cid:18)

∈ H∞

: P

˜f

{

≤

0

}

> inf
δ
∈

(0,1]

Pnϕ

(cid:20)

(cid:18)

˜f
δ

(cid:19)

+

2√2πL(ϕ)
δ

Λ(f )Gn(

H

)

+

2
√n

+

t + Γα(f ) +

log log2 2δ−

1

p

r
(cid:1)

τmin
n

+ Bn

(cid:21)(cid:19)

(115)

P

f
∃

(cid:18)

≤

(cid:0)

∈ H∞

: P

{

˜f

0

}

+

≤
˜f
δ

(cid:19)
α
2

> inf
δ
∈

(0,1]

(cid:20)

Pnϕ

(cid:18)
l

+

t +

(cid:18)

∞

j=1 r
X

2√2πL(ϕ)
δ

Λ(f )Gn(

H

)

log(kj + 1) +

log log2 2δ−

1

τmin
n

+

2
√n

(cid:19)r

+ Bn

(116)

(cid:21)(cid:19)

p

l

2 exp

2

t +

(cid:18)

−

(cid:18)

0

}

j=1 r
X

α
2

2

log(kj + 1)

(cid:19)

(cid:19)

(117)

(118)

≤

· · ·

0

}

Z
Xl=0 Xk1∈
\{
π2
3
3

2ζ(α)

Z
Xkl∈
\{
1

exp

−

2t2

,

≤

−
(cid:0)
where the last equation is followed by using some algebraic manipulations.

−

(cid:1)

(cid:0)

(cid:1)

7 Extension to High-Order Markov Chains

In this section, we extend our results in previous sections to m-order Markov chains and a mixture
of m independent Markov services.

7.1 Extend to m-order Markov chain

In this subsection, we extend our results in previous sections to m-order homogeneous Markov chain.
The main idea is to convert m-order homogeneous Markov chains to 1-order homogeneous Markov
chain and use our results in previous sections to bound the generalization error.

We start with the following simple example.
Example 24. [m-order moving average process without noise] Consider the following m-order
Markov chain

Xk =

m

i=1
X

aiXk

i,

−

Z+.

k

∈

Let Yk := [Xk+m

1, Xk+m

2,

−

· · ·

−

, Xk]T . Then, from (119), we obtain

Yk+1 = GYk,

Z+

k

∀

∈

where



G :=

a1 a2
0
1
1
0
...
...
0
0

· · ·
· · ·
· · ·
. . .

· · ·

am
−
0
0
...
1

1 am
0
0
...
0



.













(119)

(120)

(121)

∞n=1 is an order-1 Markov chain. Hence, instead of directly working with the

∞n=1, we can ﬁnd an upper bound for the Markov chain

Yn}

{

∞n=1.

Yn}
It is clear that
m-order Markov chain

{

Xn}
To derive generalization error bounds for the Markov chain
and (xk, xk+1,
ments. For all f

, xk+m

1)

{

∈ F

· · ·

−

∈ S

Yn}

m, by setting ˜f (xk, xk+1,

∞n=1, we can use the following argu-
1) =

, xk+m

{

· · ·

−

20

f (xk) where ˜f :

m

S

→

R, we obtain
n

1
n

1

f (Xi)

{

=

0

}

≤

n

1

1
n

˜f (Yi)

{

≤

}

0

.

(122)

i=1
X
Hence, by applying all the results for 1-order Markov chain
Yn}
{
upper bounds for the sequence of m-order Markov chain
∞n=1.
Xn}
extended to more general m-order Markov chain Xk

∞n=1, we obtain corresponding

i=1
X

{

R. More speciﬁcally,

=
for any tuple

This
g(Xk
−
(x1, x2,

approach can be
1, Xk
2 · · ·
−
, xm)

, Xk

−

· · ·

∈ S

m) where g

:

m, observe that

m

S

→

dg =

∂g
∂x1

dx1 +

∂g
∂x2

dx2 +

+

· · ·

∂g
∂xm

dxm.

(123)

Hence, if ∂g
∂xi

= αi for some constant αi and for each i

∈

[m], from (123), we have

m

m

g(x1, x2,

· · ·

, xm) = g(c1, c2,

· · ·

, cm) +

aixi +

aiνi,

(124)

where νi’s are constants. One speciﬁc example where the function g :
property is g(x1, x2,
amxm, where a1, a2,
, xm) = a1x1 + a2x2 +
in Example 24.

· · ·

· · ·

S
· · ·

m

R satisﬁes this
, am are constants as

→

Now, by choosing u = (g(c1, c2,

· · ·

, cm) +

m
i=1 αiνi

/(1

m
i=1 aiνi), from (124), we have

−

i=1
X

i=1
X

By setting Yk = [Xk+m

−

1 + u Xk+m

2 + u

g(x1, x2,

· · ·

P

, xm) + u =

m
(cid:1)

P
xi + u

.

ai

· · ·

· · ·
· · ·
· · ·
. . .

−
a1
1
0
...
0









a2
0
1
...
0

(cid:1)

(cid:0)

i=1
X
Xk + u]T , from (125), we have:
am
−
0
0
...
1

1 am
0
0
...
0


[] Yk.






Yk+1 =

(125)

(126)

In a more general setting, if Xk = g(Xk
independent of

Xk

{

i}

−

· · ·
, Xk
m
i=1 such as the Autoregressive model (AR), where
m

· · ·

1,

−

−

m, Vk) for some random variable Vk which is

we can use the following conversion procedure. First, by using Taylor’s approximation (to the ﬁrst-
order), we obtain

Xk = c +

aiXt

−

i + Vk,

(127)

i=1
X

g(x1, x2,

, xm, ξ)

≈

· · ·

g(c1, c2,

· · ·

m

, cm, ξ0) +

i=1
X
(ξ

−

,cm,ξ0)

(xi −

ci)

(c1,c2,

···

,cm,ζ0)

∂g
∂xi (cid:12)
(cid:12)
(cid:12)
(cid:12)

ξ0)

(128)

for some good choice of (c1, c2,
above trick with Yk = [Xk+m
we can replace the recursion Xk = g(Xk

−

× V

∈ S
2 + u
−
, Xk

V

, where

is the alphabet of Vk. Using the
,
,cm,ξ0)
m, Vk) by the following equivalent recursion:

Xk + u]T , ai = ∂g
∂xi

(c1,c2,

· · ·

···

Yk+1 =

a1 a2
0
1
1
0
...
...
0
0









· · ·
· · ·
· · ·
. . .

· · ·

−

1,

−
am
−
0
0
...
1

· · ·
1 am
0
0
...
0

Yk +


















(cid:12)
(cid:12)

,cm,ξ0)Vk

∂g
∂v

(c1,c2,

(cid:12)
(cid:12)

···
0
0
...
0

(129)

.










+

∂g
∂v

(c1,c2,

(cid:12)
(cid:12)
(cid:12)
, cm, ξ0)
(cid:12)
· · ·
1 + u Xk+m

···

m

21

m+1
i=2 or Yk, (129) models a new 1-order Markov chain
∞k=1. Then, by using the the same arguments to obtain (122), we can derive bounds on gen-

Xk+m

i}

Since Vk is independent of
{
Yk}
{
eralization error for this model.

−

For a general m-order homogeneous Markov chain, it holds that

PXk|

Xk−1=x1,Xk−2=x2,
···
m, where T(x1,x2,

,xm)
,Xk−m=xm ∼
,xm) is a random variable which depends only on

···

T(x1,x2,

for all (x1, x2,
x1, x2,

· · ·

, xm)

∈ S

· · ·

, xm and does not depend on k. Hence, we can represent the
2,

m, T(Xk−1,Xk−2,

Xk = ˜g(Xk

1, Xk

, Xk

···

· · ·
,Xk−m) = f (εk, Vk, Vk

−

−

−

where T(Xk−1,Xk−2,
m). Here, εk
1,
represents new noise at time k which is independent of the past. Hence, in a general m-order
homogeneous Markov chain, we can represent the m-order homogeneous Markov chain by the
following recursion:

q, Xk

, Xk

, Vk

· · ·

· · ·

···

−

−

−

−

−

,Xk−m)),
2,

···
1, Xk

(130)

(131)

(132)
, Xk
Xk = g(Xk
q
−
Z+. By using Taylor expansion to the ﬁrst
where εk represents new noise at time k and q
order, we can approximate the Markov chain in (132) by an Autoregressive Moving-Average Model
(ARMA(m, q)) model as following:

m, εk, Vk, Vk

1, Xk

, Vk

· · ·

· · ·

2,

1,

∈

(cid:1)

−

−

−

−

,

m

q

Xk = c + εk +

aiXk

i +

θiεk

i,

(133)

−

−

i=1
X

i=1
X
∞k=1 are i. i. d. Gaussian random variables

where c and ai’s are constants, and
this model, let

εk}

{

and

where

k

Vk :=

εi

i=1
X

Xk+m
Xk+m

−

1 + u
2 + u



Yk :=
















−

...
Xk + u
Vk+m
1
Vk+m
...
Vk+m
Vk+m

−
q

−

−

2

q

1

−

−

,


















Let Vk :=

k
i=1 εi for all k

≥

P

On the other hand, we have

u :=

1

−

c
m
i=1 ai

.

1. Observe that

P

Vk+m = εk+m + Vk+m

1.

−

Xk+m = c + εk+m +

= c + εk+m +

m

i=1
X
m

i=1
X
m

aiXk+m

i +

−

q

i=1
X
q

θiεk+m

i

−

aiXk+m

i +

−

θi

Vk+m

i=1
X

(cid:0)

= c + εk+m +

aiXk+m

i=1
X

i + θ1Vk+m

1 +

−

−

22

(0, σ2). For

N

(134)

(135)

(136)

(137)

(138)

(139)

i −

−

Vk+m

i

1

−

−

q

1

−

i=1
X

θi+1 −
(cid:0)

(cid:1)

θi

Vk+m

1

−

i −

−

θqVk+m

1.

q

−

−

(cid:1)

(140)

Then, we have

where

Here,

and

Yk+1 = GYk +

εk+m
0
...
0
εk+m
0
...
0
















,
















G :=

G11 G12
G21 G22

(cid:18)

.

(cid:19)

(141)

(142)

G11 :=

a1 a2
0
1
1
0
...
...
0
0









G12 :=

θ1

θ2 −
0
0
...
0

θ1
0
0
...
0









G21 :=

0 0
0 0
0 0
...
...
0 0









am
−
0
0
...
1

1 am
0
0
...
0









q+1

m

×

,

(143)

θq
θq −
0
0
...
0

−

θq
1 −
0
0
...
0

0 0
0 0
0 0
...
...
0 0









q+1

×

m









,

.

,

(144)

q+1

m

×

(145)

(146)

· · ·
· · ·
· · ·
. . .

· · ·

· · ·
· · ·
· · ·
. . .

· · ·

· · ·
· · ·
· · ·
. . .

· · ·

· · ·
· · ·
· · ·
. . .

G22 :=



0
0
1
...
0

1
1
0
...
0







0
0
0
...
1

0
0

0
...



0



· · ·
Since εk+m is independent of Yk, (141) models a 1-order Markov chain. Hence, we can use the
above arguments to derive new generalization error bounds for the m-order homogeneous Markov
chain where ARMA model is a special case.

×

q+1

q+1

7.2 Mixture of m Services

In this section, we consider the case that Yk =
∞k=1
[m]. This setting usually
are independent Markov chains on
happens in practice, for example, video is a mixture of voice, image, and text, where each service
can be modelled as a high-order Markov chain and the order of the Markov chain depends on the
type of service.

l=1 αlX (l)
with stationary distribution for all l

k for all k = 1, 2,

X (l)
k }

, where

· · ·

P

∈

S

{

m

23

Let

Zk := 






Then, it holds that

where

and

α1X (1)

k + α2X (2)
α2X (2)
k +

k +

· · ·
αmX (m)
k

αmX (m)
k

· · ·
...
αmX (m)
k









Zk = GXk

= 





α2X (2)

k +

αmX (m)
k

Yk

· · ·
...
αmX (m)
k

G := 

α1 α2
α2
0
...
...
0
0





· · ·
· · ·
...
· · ·

−

−

αm
αm
...
0

1 αm
1 αm
...
αm



,







.






(147)

(148)

(149)

(150)



Xk :=

.



X (1)
k
X (2)
k
...


X (m)

k


m
l=1 αl 6







m

S

× S

m, we have

It is obvious that G is non-singular since det(G) =

= 0. Therefore, for ﬁxed pair (x, y)

∈

P

Zk+1 = y

Zk = x

Q
= P(Xk+1 = G−

1y

Xk = G−

1x).

(151)

(cid:0)
Now, assume that G−
from (150) and (151), we have
(cid:16)

1x =

(cid:12)
(cid:1)
x , β(2)
β(1)
x ,
(cid:12)
· · ·

β(m)
x

)
(cid:17)

and G−

(cid:12)
1y =
(cid:12)

y , β(2)
β(1)
y ,
(cid:16)

· · ·

β(m)
y

)

. Then,

(cid:17)

m

m

P

Zk+1 = y

Zk = x

= P

(cid:0)

(cid:12)
(cid:12)

(cid:18)
m

Yl=1
m

(cid:1)

=

=

X (l)

k+1 = β(l)

y

X (l)

k = β(l)

x

\l=1
P

(cid:8)
X (l)
k+1 = β(l)

y

(cid:12)
\l=1
(cid:12)
(cid:9)
(cid:12)
(cid:12)
X (l)
k = β(l)

(cid:8)

x

(cid:0)
Ql(β(l)

x , β(l)

y ),

(cid:12)
(cid:12)

(cid:1)

(cid:19)

(cid:9)

(152)

(153)

(154)

Yl=1

{
m
l=1 have stationary distributions.

where Ql is the transition probability of the Markov chain l. It follows that
Markov chain. It is easy to see that
X (l)
k }
{
Now, as Subsection 7.1, to derive generalization error bounds for the Markov chain
can use the following arguments. For all f
˜f :

and by setting ˜f (z1, z2,

∞k=1 is a 1-order
∞k=1 has stationary distribution if all the Markov chains

∞n=1, we
Xn}
, zm) := f (z1) where

Zk}

Zk}

· · ·

{

{

m

S

→

∈ F
R, we obtain ˜f (GXk) = f (Yk) and
n

1
n

1

f (Yi)

{

=

0

}

≤

1
n

n

1

{

˜f (GXi)

0

.

}

(155)

≤

i=1
X
Hence, by applying all the results for 1-order Markov chain
corresponding upper bounds for the sequence of m-order Markov chain

Zn}

i=1
X

{

∞n=1 where Zn = GXn, we obtain

Xn}

{

∞n=1.

A Proof of Lemma 8

Before going to prove Lemma 8, we observe the following interesting fact.

24

Lemma 25. Let
independent copy (replica) of
Xn}
· · ·
R. Let ǫ := (ε1, ε2,
and
of i.i.d. Rademacher’s random variables. Then, the following holds:

{
a class of uniformly bounded functions from

∞n=1 be an arbitrary process on a Polish space

∞n=1. Denote by X = (X1, X2,

Xn}

S →

, and let
Yn}
, Xn), Y = (Y1, Y2,

∞n=1 be a
, Yn),
· · ·
, εn) be a vector

· · ·

F

S

{

{

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

Eǫ

EX,Y

n

εi(f (Xi)

= EX,Y

n

(f (Xi)

.

(156)

(cid:20)

(cid:20)

i=1
X

(cid:21)
Remark 26. Our lemma generalizes a similar fact for i.i.d. processes. In the case that
∞n=1
is an i.i.d. random process, (156) holds with equality since PX n,Y n (x1, x2,
, yn)
is invariant under permutation. However, for the Markov case, this fact does not hold in general.
Hence, in the following, we provide a new proof for (156), which works for any process
∞n=1
by making use of the properties of Rademacher’s process.

Xn}
· · ·
Xn}

{
, xn, y1, y2,

i=1
X

· · ·

(cid:21)(cid:21)

(cid:20)

{

−

f (Yi))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

f (Yi))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

Proof. Let g(x, y) := f (x)
f (y) for some f

f (y) and
. Then, it holds that

−

:=

g :

{

G

S × S →

R : g(x, y) := f (x)

−

∈ F }
n

Eǫ

EX,Y

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)

(cid:20)

i=1
X
Observe that g(Xi, Yi) =
[n], denote by
For all j

∈

and

Now, for each j

∈

−

εi(f (Xi)

f (Yi))
(cid:12)
(cid:12)
(cid:12)
g(Yi, Xi) for all i
(cid:12)

−

= Eǫ

EX,Y

(cid:20)

(cid:21)(cid:21)

[n].

∈

n

i=1
X

(cid:20)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:21)(cid:21)

εig(Xi, Yi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Nj := [n]

j

,

}

\ {

∈ Nj}
[n], observe that εj is independent of X n

{

ε

Nj :=

εi : i

.
1 , Y n

1 , ε

Nj . Hence, we have

n

Eεn

1 ,X n

1 ,Y n
1

sup
g
∈G (cid:12)
(cid:20)
(cid:12)
(cid:12)
EεNj ,X n
(cid:12)
1 ,Y n
1

i=1
X

εig(Xi, Yi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

=

1
2

(cid:20)

+

1
2

EεNj ,X n

1 ,Y n
1

=

1
2

EεNj ,X n

1 ,Y n
1

(cid:20)

+

1
2

EεNj ,X n

1 ,Y n
1

(cid:20)

sup
g
∈G (cid:12)
Xi
∈Nj
(cid:12)
(cid:12)
(cid:12)
sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
g
∈G (cid:12)
Xi
∈Nj
(cid:12)
(cid:12)
(cid:12)
sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)

(cid:21)

εig(Xi, Yi)

εig(Xi, Yi) + g(Xj, Yj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
g(Xj, Yj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈Nj
εig(Xi, Yi) + g(Xj, Yj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

εig(Xi, Yi) + g(Yj, Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈Nj

(cid:21)

(cid:21)

(cid:21)

where (161) follows from g(Xj, Yj) =
Now, by setting ˜ǫi :=

εi for all i

g(Yj, Xj).
n. Then, we obtain

−

−
EεNj ,X n

1 ,Y n
1

∈

(cid:20)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈Nj
= E˜εNj ,X n

1 ,Y n
1

= EεNj ,X n

1 ,Y n
1

= EεNj ,X n

1 ,Y n
1

(cid:20)

(cid:21)

Xi
∈Nj

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

εig(Xi, Yi) + g(Yj, Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜ǫig(Xi, Yi) + g(Yj, Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
εig(Yi, Xi) + g(Yj, Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
εig(Xi, Yi) + g(Xj, Yj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈Nj

Xi
∈Nj

(cid:20)

(cid:20)

(cid:21)

(cid:21)

,

(cid:21)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)
25

(157)

(158)

(159)

(160)

(161)

(162)

(163)

(164)

where (162) follows from (˜ǫi : i
from g(Xi, Yi) =
i

[n].

−

g(Yi, Xi) and ˜ǫi =

−

∈

From (161) and (164), we have

∈ Nj) has the same distribution as (εi : i

εi, and (164) follows from g(Xi, Yi) =

∈ Nj), (163) follows
g(Yi, Xi) for all

−

Eεn

1 ,X n

1 ,Y n
1

n

i=1
X

(cid:20)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

εig(Xi, Yi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

= EεNj ,X n

1 ,Y n
1

(cid:20)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈Nj

εig(Xi, Yi) + g(Xj, Yj )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

j
∀

∈

[n].

(165)

It follows that

n

Eεn

1 ,X n

1 ,Y n
1

(cid:20)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)
1 ,Y n
,X n
1

i=1
X

= E

εn−1
1

(cid:20)

1

2

n

−

(cid:21)

i=1
X
n
−

εig(Xi, Yi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
εig(Xi, Yi) + g(Xn, Yn)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1, Yn

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
g
∈G (cid:12)
(cid:12)
2
n
(cid:12)
(cid:12)
εig(Xi, Yi) + g(Xn

εig(Xi, Yi) + g(Xn

εig(Xi, Yi)

g(Xn

i=1
X

i=1
X

1 ,Y n
1

−

(cid:20)

(cid:20)

(cid:21)

−

−

−

−

n

2

1, Yn

−

−

=

1
2

E

εn−2
1

,X n

1 ,Y n
1

+

1
2

E

εn−2
1

,X n

=

1
2

E

εn−2
1

,X n

1 ,Y n
1

+

1
2

E

εn−2
1

,X n

1 ,Y n
1

εig(Xi, Yi) + g(Yn

−

1, Xn

−

where (166) follows from setting j = n in (165), and (168) follows from g(Yn

(cid:20)

i=1
X

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)

2

n

−

i=1
X

(166)

(167)

(cid:21)

−

1) + g(Xn, Yn)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

1, Yn

−

1) + g(Xn, Yn)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1) + g(Xn, Yn)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

,

1) + g(Xn, Yn)
(cid:12)
(cid:21)
(cid:12)
(cid:12)
1, Xn
(cid:12)

−

−

n

−

1, observe that

1, 1

}

g(Xn

1, Yn

1).

−

−
−
Now, for any ﬁxed tuple (xn
1

PXn,Yn|

X n−1
1

n

1

−

1

−

× {−
2
)
−

1

, yn
1

−

1

, εn
1

−

1

n

1

−

)
∈ S
(xn, yn|
xn
(xn|
1
xn
(xn|
1
Y n−1
1

,X n−1
1

−

−

1

−

−
× S
xn
, yn
1
1
1
)PYn|
)PXn|
,εn−2
1

1

Y n−1
1

X n−1
1
(xn, yn|

,εn−2
1

X n−1
1

,Y n−1
1
= PXn|
= PYn|
Y n−1
1
= PYn,Xn|

, εn
1
(yn|
(yn|
xn
1

yn
1
yn
1
1

−

1

)

−

1

−

)
, yn
1

1

, εn
1

−

2

).

−

On the other hand, we also have

,εn−2
1
(εn
1
(εn
1

PX n−1

1

,Y n−1
1
= Pεn−2
= Pεn−2
= PY n−1
Hence, from (171) and (174), we obtain
1 , yn
xn

1 , εn
−
1

PX n

2

1

1

1

1 ,Y n

1

1 ,εn−2
= PX n−1
(cid:0)

1

1

−

−

−

(xn
, yn
1
1
2
)PX n−1
)PY n−1
(xn
1

,εn−2
1

−

2

1

1

2

)

1

−

, εn
1
1

−

(xn
1
(xn
1

1

−

1

, yn
1

−

1

)PY n−1
)PX n−1
, εn
1
1

−

1

,X n−1
1

(yn
1
(yn
1

−

−

1

1

)

)

2

).

−

Y n−1
1

εn−2
1

(xn
(cid:1)
1

−

1

, yn
1

−

1

, εn
1

−

2

)PXnYn|

X n−1
1

Y n−1
1

εn−2
1

xn
1

−

1

, yn
1

−

1

, εn
1

−

2

(168)

1) =

(169)

(170)

(171)

(172)

(173)

(174)

= PY n−1

1

,X n−1
1

,εn−2
1

1

(xn
1

−

, yn
1

−

1

, εn
1

−

2

)PYn,Xn|

Y n−1
1

,X n−1
1

= PY n

1 ,X n

1 ,εn−2

1

1 , yn
xn

1 , εn
−
1

2

.

(cid:0)

(cid:1)

26

xn, yn|
(cid:0)
,εn−2
1

xn, yn|
(cid:0)

1

xn
1

−

, yn
1

−

1

(175)
(cid:1)
2
, εn
−
1
(176)
(cid:1)
(177)

(178)

(179)

(180)

(181)

(182)

(183)

(184)

(185)

Now, from (177), we also have

It follows from (178) that

PX n

1 (xn

1 , yn

1 ,Y n

1 ) = PY n

1 X n

1 (xn

1 , yn

1 ).

PXn−1,Yn−1(xn

−

1, yn

−

1) =

,yn−2
Xxn−2
1

1

,xn,yn

=

PX n

1 (xn

1 , yn
1 )

1 ,Y n

PY n

1 X n

1 (xn

1 , yn
1 )

Hence, from (177) and (181), we have

1

,yn−2
Xxn−2
,xn,yn
1
= PYn−1Xn−1 (xn

−

1, yn

−

1).

xn, yn, xn
−
1

2

, yn
1

−

2

2

, εn
1

−

xn

−

|

1, yn

−

1

(cid:1)

1

=

1 ,Y n

,εn−2
1

,Y n−2
1

PXn,Yn,X n−2
PX n
1 ,εn−2
PXn−1Yn−1(xn
−
1 , yn
xn
PY n

Xn−1,Yn−1
|
1 , εn
2
1 , yn
(xn
)
(cid:0)
−
1
1, yn
1)
−
1 , εn
2
−
1
1, yn

1 ,εn−2
1 ,X n
PYn−1Xn−1(xn

1)

=

1

1

(cid:0)

(cid:1)

= PXn,Yn,X n−2

1

−
,Y n−2
1

−
,εn−2
1

|

From (185), for each ﬁxed (xn

−

1, yn

−

1)

, we have

Xn−1,Yn−1

yn, xn, yn
−
1

2

, xn
1

−

2

2

, εn
1

−

yn

−

|

1, xn

−

.

1

(cid:1)

(cid:0)
∈ S × S
n

2

−

sup
g
∈G (cid:12)
i=1
X
(cid:12)
(cid:12)
(cid:12)
1 = xn
−

E

εn−2
1

,X n−2
1

,Y n−2
1

,Xn,Yn

(cid:20)

εig(Xi, Yi) + g(Yn

1, Xn

−

1)

−

+ g(Xn, Yn)
Xn
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
,Y n−2
1

,X n−2
1

εn−2
1

,Xn,Yn

= E

1, Yn

−

1 = yn

−

−

1

(cid:21)

εig(Xi, Yi) + g(yn

1 = xn

−

Xn
+ g(Xn, Yn)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
,Y n−2
1

,X n−2
1

εn−2
1

,Xn,Yn

= E

−

1 = yn

−

1

(cid:21)

εig(Yi, Xi) + g(yn

1 = xn

−

+ g(Yn, Xn)
Yn
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
,Y n−2
1

,X n−2
1

εn−2
1

,Xn,Yn

= E

−

1 = yn

−

1

(cid:21)

εig(Xi, Yi) + g(xn

(cid:20)

(cid:20)

(cid:20)

n

2

−

sup
g
∈G (cid:12)
i=1
X
(cid:12)
(cid:12)
(cid:12)
1, Yn
−

n

2

−

sup
g
∈G (cid:12)
i=1
X
(cid:12)
(cid:12)
(cid:12)
1, Xn

−

n

2

−

sup
g
∈G (cid:12)
i=1
X
(cid:12)
(cid:12)
(cid:12)
1, Xn

−

1, xn

−

1)

1, xn

−

1)

−

−

1, yn

−

1)

(186)

(187)

−

1 = xn

−

+ g(Xn, Yn)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Yn
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

1 = yn

−

1

(cid:21)

where (186) follows from (185), and (187) follows from the fact that g(x, y) =
x, y

.

g(y, x) for all

−

∈ S × S

From (187) and (181), we obtain

E

εn−2
1

,X n

1 ,Y n
1

(cid:20)

= E

εn−2
1

n

2

−

i=1
X

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)
1 ,Y n
1

,X n

εig(Xi, Yi) + g(Yn

−

1, Xn

−

εig(Xi, Yi) + g(Xn

2

n

−

i=1
X

(cid:20)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

27

(cid:21)

1) + g(Xn, Yn)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1) + g(Xn, Yn)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1, Yn

−

−

(188)

.

(cid:21)

From (168) and (188), we obtain

n

Eεn

1 ,X n

1 ,Y n
1

(cid:20)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)
εn−2
1

i=1
X

εig(Xi, Yi)
(cid:12)
(cid:21)
(cid:12)
n
(cid:12)
(cid:12)

2

−

= E

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)
By using induction, we ﬁnally obtain

1 ,Y n
1

,X n

(cid:20)

i=1
X

εig(Xi, Yi) + g(Xn

−

1, Yn

−

1) + g(Xn, Yn)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:21)

Eεn

1 ,X n

1 ,Y n
1

or equation (156) holds.

n

i=1
X

(cid:20)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

εig(Xi, Yi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

= EX n

1 ,Y n
1

n

i=1
X

(cid:20)

sup
g
∈G (cid:12)
(cid:12)
(cid:12)
(cid:12)

,
(cid:21)

g(Xi, Yi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(189)

(190)

Next, recall the following result which was developed base on the spectral method [29]:

Lemma 27. [28, Theorems 3.41] Let X1, X2,
, Xn be a stationary Markov chain on some Polish
space with L2 spectral gap λ deﬁned in Section 2.1 in the main document and the initial distribution
∈ M2. Let f
ν

and deﬁne

∈ F

· · ·

Sn,n0(f ) =

for all n0 ≥

0. Then, it holds that

f (Xj+n0 )

1
n

n

j=1
X

2M

E

Sn,n0(f )

−

2

Eπ[f (X)]

≤

n(1

λ)

n2(1

+

64M 2

λ)2 λn0

dv
dπ −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
In addition, we recall the following McDiarmid’s inequality for Markov chain:

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

−

(cid:21)

Lemma 28. [24, Cor. 2.10] Let X := (X1, X2,
values in a Polish state space Λ =

· · ·

, Xn) be a homogeneous Markov chain, taking
1).

, with mixing time tmix(ε) (for 0

ε

Recall the deﬁnition of τmin in Eq. (3) in the main document. Suppose that f : Λ

S × S × · · · × S
n times

|

{z

}

n

for every x, y

Λ, for some c

∈

∈

f (x)

f (y)

−

≤

i=1
X
+. Then, for any t

Rn

ci1

xi 6

= yi}

{

0,

≥

PX

f (X)

E[f (X)]

−

t

≥

≤

2 exp

Now, we return to the proof of Lemma 8.

(cid:2)(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:3)

2t2
.
2
2τmin (cid:19)

k

−

c

k

(cid:18)

Proof of Lemma 8. For each f

, observe that

∈ F

(193)

(194)

n

1
n

i=1
X
1
n

=

i=1
X

π(x)f (x)dx

f (Xi)

n

−

ZS

f (Xi)

−

E[f (Xi)] +

E[f (Xi)]

−

ZS

π(x)f (x)dx.

(195)

1
n

n

i=1
X

28

(191)

(192)

1

.

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤
≤
R satisﬁes

→

On the other hand, we have

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

n

i=1
X

E[f (Xi)]

−

ZS

= E

Sn,0(f )

−

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)
E

π(x)f (x)dx
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Eπ[f (X)]

(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)

Eπ[f (X)]

2

≤ s

Sn,0(f )

−

(cid:20)(cid:12)
(cid:12)
(cid:12)
2M
(cid:12)
n(1

−

≤ s
= An,

+

λ)

64M 2

n2(1

λ)2

−

(cid:21)
dv
dπ −

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(196)

(197)

(198)

(199)

where (199) follows from Lemma 27 with n0 = 0.
By using

b, from (195) and (199), we obtain

a + b

+

a

|

| ≤ |

|

|
E

On the other hand, let Y1, Y2,

· · ·

(cid:20)
, Yn is a replica of X1, X2,

i=1
X

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

, Xn. It holds that

· · ·

n

f (Xi)

−

E[f (Xi)]

+ An.

(200)

P

F

Pn −
(cid:2)(cid:13)
E
(cid:13)

≤

1
n

(cid:13)
(cid:3)
sup
(cid:13)
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

E

(cid:20)

1
n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)
= EX

i=1
X

n

f (Xi)

E[f (Xi)]

−

n

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
E[f (Yi)]
−

f (Xi)

(cid:20)

(cid:20)

1
n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1
X
1
n

(cid:20)

EY

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)

EY
(cid:20)

1
n

= EX

EX

≤

n

i=1
X
n

i=1
X

f (Xi)

f (Xi)

−

−

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
f (Yi)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

.

(cid:21)(cid:21)

f (Yi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Now, by Lemma 25 and the triangle inequality for inﬁnity norm, we have

E

(cid:20)

n

1
n

sup
f
∈F (cid:12)
i=1
X
(cid:12)
(cid:12)
(cid:12)
EεEX

f (Xi)

−

n

1
n

f (Yi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
f (Xi)

(cid:21)

εi

= EεEX,Y

1
n

n

i=1
X
1
n

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ EεEY

≤
= 2E

(cid:20)(cid:13)
(cid:13)
P 0
(cid:13)
nkF
(cid:13)
where (206) follows from the fact that Y is a replica of X.

(cid:13)
F (cid:21)
(cid:13)
(cid:1)
(cid:13)
(cid:13)

i=1
X
,

k

(cid:0)

(cid:2)

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

f (Yi)

εi

f (Xi)

−

(cid:0)
n

εi

f (Yi)

i=1
X

(cid:0)

F (cid:21)

(cid:13)
(cid:13)
(cid:1)
(cid:13)
(cid:13)

(cid:13)
F (cid:21)
(cid:13)
(cid:1)
(cid:13)
(cid:13)

From (200) and (206), we ﬁnally obtain

F
Now, by using the triangle inequality, we have
(cid:3)

P 0

nkF

≤

2E

k
(cid:2)

(cid:3)

+ An.

(cid:3)

E

P

Pn −
(cid:2)(cid:13)
(cid:13)
n

(cid:13)
(cid:13)

1
n

n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
1
(cid:12)
n

(cid:21)

εif (Xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
EY
−

f (Xi)

i=1
X

εi

i=1
X

(cid:0)

E[

P 0

nkF

k

] = EX,ε

(cid:20)

≤

E

(cid:20)

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

f (Yi)

(cid:12)
(cid:12)
(cid:3)(cid:1)
(cid:12)
(cid:12)

(cid:2)

29

+ E
(cid:20)

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n

i=1
X

εiEY[f (Yi)]

(209)

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(201)

(202)

(203)

(204)

(205)

(206)

(207)

(208)

= EX,ε

(cid:20)

EX,Y,ε

≤

= EX,Y

1
n

EY

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
f
∈F (cid:12)
(cid:12)
(cid:12)
1
(cid:12)
n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)

(cid:20)

1
n

(cid:20)

n

εi

f (Xi)

i=1
X

(cid:0)

−

f (Yi)

εi

f (Xi)

−

f (Yi)

n

i=1
X
n

(cid:0)
f (Xi)

−

f (Yi)

(cid:21)

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)

(cid:21)

(cid:21)(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)
+ E
(cid:20)

(cid:12)
(cid:21)
(cid:12)
(cid:1)
(cid:12)
(cid:12)
+ E
(cid:20)

+ E
(cid:20)

1
n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)
n

εiEY[f (Yi)]

n

i=1
X

1
n

n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
1
(cid:12)
n

i=1
X

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

εiEY[f (Yi)]

i=1
X
εiEY[f (Yi)]

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)
where (212) follows from Lemma 25.

i=1
X

Now, we have

EX,Y

1
n

(cid:20)

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)
= EX,Y

n

f (Xi)

−

f (Yi)

i=1
X

1
n

(cid:0)
sup
f
∈F (cid:12)
(cid:12)
(cid:12)
1
(cid:12)
n

(cid:20)

n

f (Xi)

i=1
X
n

(cid:0)
f (Xi)

−

(cid:21)

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)
−

EX,ε

≤

(cid:20)

= 2EX,ε

= 2E

k

i=1
X
n
1
n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
f
∈F (cid:12)
(cid:12)
(cid:12)
P
(cid:12)
kF

(cid:20)
Pn −

i=1
X
.

f (Xi)

−

(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)
P f

(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)

P f

f (Yi)

−

−

P f

(cid:0)
(cid:1)
+ EY,ε

P f

(cid:12)
(cid:21)
(cid:12)
(cid:1)(cid:1)
n
(cid:12)
(cid:12)

i=1
X

1
n

f (Yi)

P f

−

(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)

(cid:20)

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

On the other hand, by using the triangle inequality, we also have
(cid:3)

(cid:2)

E

(cid:20)

1
n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)
E

≤

(cid:20)

n

i=1
X

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

Now, observe that

εiEY[f (Yi)]

1
n

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
EY[f (Yi)]

n

εi

i=1
X

(cid:0)

Eπ[f (Y )]

−

1
n

n

i=1
X

E

(cid:20)

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

εiEπ[f (Y )]

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

εiEπ[f (Y )]

.
(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n

εi

(cid:21)

i=1
X

+ E
(cid:20)

1
n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)

n

i=1
X

n

Eπ[f (Y )]

Eε

1
n

1
n

(cid:20)

i=1
X

n

(cid:19)

(cid:12)
(cid:12)
εi

i=1
X
n

(cid:21)

2

εi

i=1
X

(cid:19)

(cid:21)

≤

(cid:18)

sup
f
∈F (cid:12)
(cid:12)

n

M Eε

≤

≤

=

(cid:20)

i=1
X

(cid:20)(cid:18)

Eε

M

v
u
u
t
.

M
√n

In addition, for each ﬁxed (ε1, ε2,

, εn)

∈ {−

1, +1

}

· · ·

n and f

∈ F

, we have

n

i=1
X

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

εi

EY[f (Yi)]

−

Eπ[f (Y )]

≤

=

(cid:0)
1
EY
n

EY

1
n

n

εi

f (Yi)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1
X
(cid:0)
gε(Y)

,

(cid:2)(cid:12)
(cid:12)

(cid:3)

(cid:12)
(cid:12)
30

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)
Eπ[f (Y )]

−

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)

(cid:12)
(cid:21)
(cid:12)
(210)
(cid:12)
(cid:12)
(211)

(212)

(213)

(214)

(215)

(216)

(217)

(218)

(219)

(220)

(221)

(222)

(223)

where

n

gε(y) :=

εi

f (yi)

Now, for all x, y

bn, we have

∈

i=1
X

(cid:0)

Eπ[f (Y )]

,

−

Rn.

y

∀

∈

(cid:1)

n

gε(y) =

εi

f (xi)

gε(x)

−

f (yi)

−

(cid:1)

f (yi)
|

−

(cid:0)
f (xi)

i=1
X
n

≤

|

i=1
X
n

Hence, by Lemma 28, we have

2M 1

xi 6

= yi}

.

{

≤

i=1
X

PY

gε(Y)

≥

M

2τminn log n

2
n

.

≤

It follows that

EY

gε(Y)

(cid:2)(cid:12)
(cid:12)

(cid:3)

(cid:12)
(cid:12)

(cid:2)(cid:12)
(cid:12)

≤

≤

(cid:3)

p

gε(Y)

(cid:12)
(cid:12)
gε(Y)
+ 2nM PY
(cid:2)(cid:12)
(cid:12)
(cid:12)
(cid:12)
≥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:2)(cid:12)
2τminn log n + 2nM
(cid:12)

gε(Y)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

< M

EY

M

2τminn log n

M

2τminn log n
(cid:3)

p
2
p
n

(cid:18)

(cid:19)

= M

p

2τminn log n + 4M,

(224)

(225)

(226)

(227)

(228)

(229)

(230)

(231)

(cid:3)

where (229) follows from the total law of expectation and
2nM for all y

Rn.

p

∈
From (223) and (231), we have

gε(y)

| ≤

|

n
i=1

f (yi)

−

Eπ[f (Y )]

P

(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)

εi

EY[f (Yi)]

Eπ[f (Y )]

−

n

1
n

(cid:12)
i=1
X
(cid:12)
(cid:12)
, (ε1, ε2,
(cid:12)

(cid:0)

for all f

∈ F
From (232), we obtain

· · ·

, εn)

1, +1

n.

}

∈ {−

1
n

≤

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)

M

2τminn log n + 4M

(232)

(cid:18)

p

(cid:19)

n

1
n

i=1
X

(cid:0)

Eε

(cid:20)

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
(cid:12)

From (217), (221), and (233), we have

εi

EY[f (Yi)]

Eπ[f (Y )]

−

1
n

≤

(cid:21)

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)

M

2τminn log n + 4M

(cid:18)

p

(233)

.
(cid:19)

E

(cid:20)

1
n

sup
f
∈F (cid:12)
(cid:12)
(cid:12)
1
(cid:12)
n

≤

n

i=1
X
M

εiEY[f (Yi)]

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
2τminn log n + 4M

(cid:18)

p

From (212), (216), and (234), we obtain

+

M
√n

.

(cid:19)

E[

P 0

nkF

k

]

≤

2E

Pn −

k

P

kF

+

1
n

and we ﬁnally have

(cid:2)

E

(cid:2)

(cid:3)

Pn −

k

P

kF

≥

(cid:3)

M
√n

,

M

2τminn log n + 4M

+

(cid:18)

1
2

p

(cid:19)

E[

P 0

nkF

k

]

−

˜An.

31

(234)

(235)

(236)

B Proof of Theorem 9

The proof of Theorem 9 is based on [4]. First, we prove (55). Without loss of generality, we can
assume that each ϕ
1). Then, it is
clear that ϕ(x) = 1 for x

Φ takes its values in [0, 1] (otherwise, it can be redeﬁned as ϕ
, we obtain

0. Hence, for each ﬁxed ϕ

Φ and f

∈

∧

≤

∈

∈ F

P

f

{

≤

0

P ϕ(f )
Pnϕ(f ) +

} ≤
≤

Pn −

k

P

kGϕ,

where

Now, let g(x) = supf

1
n

∈Gϕ

P

=

(cid:12)
(cid:12)

g(y)

g(x)

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Gϕ :=
n
i=1 f (xi)
−
n

ϕ

·

P f
(cid:8)

.

f : f

∈ F
. Then, for all x

(cid:9)

= y, we have

(cid:12)
(cid:12)
f (xi)

P f

−

f (xi)

−

P f

−

1
n

n

i=1
X

f (yi)

P f

−

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
f
∈Gϕ (cid:12)
(cid:12)
n
(cid:12)
1
(cid:12)
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

i=1
X
1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
P f

(cid:19)

(cid:18)

i=1
X

P f

f (yi)

−

n

f (yi)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
P f

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (xi)

−

i=1
X
n

i=1
X
n
1
n

i=1
X
n

1
n

sup
f
∈Gϕ (cid:12)
(cid:12)
(cid:12)
1
(cid:12)
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
f
∈Gϕ (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)
1
n

sup
f
∈Gϕ (cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
f
∈Gϕ (cid:12)
(cid:12)
(cid:12)
1
(cid:12)
n

sup
f
∈Gϕ
n
1
n

i=1
X

≤

≤

≤

≤

≤

i=1
X
n

f (xi)

f (xi)

i=1
X
(cid:12)
(cid:12)
= yi}
xi 6

{

.

1

f (yi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
f (yi)

−

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(237)
(238)

(239)

(240)

(241)

(242)

(243)

(244)

(245)

(246)

Hence, for t > 0, by Lemma 28 with we have

P

k
(cid:26)
Hence, with probability at least 1

kGϕ ≥

Pn −

P

Now, by Lemma 8 with M = 1 (since supf

P

f

{

≤

0

} ≤

P

E

(cid:20)

Pn −
(cid:13)
(cid:13)

(cid:21)

Gϕ

(cid:13)
(cid:13)

2 exp

≤

(cid:27)

2t2

.

(cid:1)

−

(cid:0)

E

Pn −

k

P

kGϕ

+ t

τmin
n

r
(cid:3)
for all f

2t2

(cid:2)
2 exp

−

−
(cid:1)
(cid:0)
Pnϕ(f ) + E[
k

Pn −

P

∈ F
kGϕ] + t
= 1), it holds that

r

τmin
n

.

(247)

+ An

M=1

(cid:12)
(cid:12)
εiδXi

+ Bn.

(248)

(249)

Since (ϕ
[30, 31], we obtain

−

1)/L(ϕ) is contractive and ϕ(0)

1 = 0, by using Talagrand’s contraction lemma

n

Eε

1

n−

εiδXi

2L(ϕ)Eε

1

n−

εiδXi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
From (246), (247), (249), and (251), with probability 1

(cid:13)
(cid:13)
= 2L(ϕ)Rn(
(cid:13)
(cid:13)
F
2 exp

i=1
X

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Gϕ

).

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
, we have for all f

(cid:1)
+ Bn.

(250)

(251)

,

∈ F

(252)

−

2t2

−
τmin
n

(cid:0)

) + t

r

P

f

{

≤

0

} ≤

Pnϕ(f ) + 4L(ϕ)Rn(

F

32

f

k∞

P 0
n

∈Gϕ k
2E

k

≤

(cid:2)
= 2E

(cid:13)
(cid:13)
1
n−

Gϕ
(cid:3)
n

i=1
X

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)
−

≤

(cid:13)
Gϕ(cid:21)
(cid:13)
(cid:13)
(cid:13)

n

i=1
X

6
) +

t +

log k

(cid:0)

p

r
(cid:1)

τmin
n

+ Bn

(cid:21)(cid:19)

(253)

(254)

(255)

(256)

Now, we use (252) with ϕ = ϕk and t is replaced by t + √log k to obtain

P

f
∃

(cid:18)

: P

f

{

0

}

> inf
k>0

≤

∈ F

(cid:20)

Pnϕk(f ) + 4L(ϕk)Rn(

F

2

t +

log k

−

∞

exp

Xk=1
∞

(cid:0)

(cid:0)
2 exp

k−

2

2

≤

≤

p
2t2

(cid:1)

−

,

Xk=1
π2
3
where (255) follows from

exp

(cid:0)
2t2

−

=

(cid:0)

(cid:1)

2

(cid:1)

(cid:1)

π2
6

n

E

1

n−

εiδXi

E

1

n−

giδXi

.

∞

=

k−

2.

Xk=1

π
2

≤

r

Gϕ

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n

i=1
X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
1

n

i=1
X

P

≤

(cid:21)

Gϕ

(cid:13)
(cid:13)

n−

√2πE
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n

giδXi

Gϕ

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Gϕ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ Bn.

i=1
X

Hence, from (249) and (257), we obtain

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Pn −
(cid:13)
Now, deﬁne Gaussian processes
(cid:13)

E

(cid:20)

and

Z1(f, σ) := σn−

1/2

gi(ϕ

◦

f )(Xi),

i=1
X

n

Z2(f, σ) := L(ϕ)n−

1/2

gif (Xi) + σg,

Next, we prove (56). By the equivalence of Rademacher and Gaussian complexity [32], we have

1 and g is standard normal independent of the sequence

where σ =
on the probability space (Ωg, Σg, Pg) on which the sequence
we have

gi}

{

±

. Let Eg be the expectation
and g are deﬁned, then by [4, 30],

gi}

{

i=1
X

Eg

sup
{
On the other hand, it holds that

Z1(f, σ) : f

(cid:20)

∈ F

, σ =

1

±

Eg

sup
{

(cid:20)

≤

}

(cid:21)

Z2(f, σ) : f

, σ =

∈ F

±

1

}

.
(cid:21)

n

i=1
X

Eg

1/2

n−

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Gϕ

(cid:13)
(cid:13)
(cid:13)
(cid:13)

giδXi

= Eg

n−

1/2 sup
¯
h
Gϕ

∈

n

i=1
X

gih(Xi)
(cid:21)

(cid:20)

= Eg

sup
{
, and similarly

(cid:20)

Z1(f, σ) : f

, σ =

∈ F

±

1

}

,
(cid:21)

+ E
|

g

| ≥

Eg

sup
{

(cid:20)

Z2(f, σ) : f

, σ =

∈ F

±

1

}

.
(cid:21)

where ¯

ϕ(f ),

ϕ(f ) : f

Gϕ :=
{
L(ϕ)Eg

From (261), (263), and (264), we have

−
1/2

n

i=1
X

n

1

n−

n−

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Eg

∈ F }

giδXi

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
giδXi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
√2π

(cid:13)
(cid:13)
By combining (258) and (265), we obtain
(cid:13)
(cid:13)

i=1
X

Gϕ

F

(cid:13)
(cid:13)
(cid:13)
(cid:13)

L(ϕ)Eg

1

n−

giδXi

+ n−

1/2E
|

g

.

|

≤

E

Pn −
(cid:2)(cid:13)
(cid:13)

P

Gϕ

≤

(cid:13)
(cid:13)

(cid:3)

L(ϕ)E

(cid:18)

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

Hence, from (247), (258), and (266), we ﬁnally obtain (56).

+ n−

1/2E
|

g

|

(cid:19)

+ Bn.

(266)

(cid:13)
F (cid:21)
(cid:13)
(cid:13)
(cid:13)

n

i=1
X

n

giδXi

i=1
X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
1

n−

33

(257)

(258)

(259)

(260)

(261)

(262)

(263)

(264)

(265)

C Proof of Theorem 10

We can assume, without loss of generality, that the range of ϕ is [0, 1] (otherwise, we can replace ϕ
by ϕ

0. In addition, set Φ =

k for all k

1). Let δk = 2−

, where

ϕk : k

1

∧

≥

ϕk(x) :=

{
0,
x
1), x < 0

≥

ϕ(x/δk),
ϕ(x/δk

−

(cid:26)

≥

}

.

(267)

Now, for any δ
f (Xi)/δk ≥

∈

f (Xi)/δ, so we have

(0, 1], there exists k such that δ

(δk, δk

−

∈

1]. Hence, if f (Xi)

≥

0, it holds that

ϕk(f (Xi)) = ϕ

(cid:18)

f (Xi)
δk (cid:19)
f (Xi)
,
δ
(cid:19)
) is non-increasing.

≤

(cid:18)

ϕ

where (269) follows from the fact that ϕ(
·

On the other hand, if f (Xi) < 0, then f (Xi)/δk

f (Xi)/δ. Hence, we have

1 ≥

−

ϕk(f (Xi)) = ϕ

f (Xi)
δk
−
f (Xi)
δ

(cid:18)

1 (cid:19)
,
(cid:19)
) is non-increasing.

≤

(cid:18)

ϕ

where (269) follows from the fact that ϕ(
·

From (269) and (271), we have

Pnϕk

f

=

(cid:0)

(cid:1)

≤

1
n

1
n

n

i=1
X
n

i=1
X

= Pnϕ

(cid:18)

ϕk(f (Xi))

f (Xi)
δ

ϕ

(cid:18)

(cid:19)

f
δ

.
(cid:19)

1
δk ≤

2
δ

,

log k = log log2

1
δk ≤

log log2 2δ−

1.

Moreover, we also have

and

Furthermore, observe that

L(ϕk) = sup
R
∈

x

dϕk(x)
dx
(cid:12)
(cid:12)
dϕ(x/δk)
(cid:12)
(cid:12)
dx
L(ϕ)
δk, δk

1}

−

= sup
R
x

∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
min
{
L(ϕ)
δk

≤

=

≤

2
δ

L(ϕ).

x

{

≥

+

0

}

1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dϕ(x/δk
dx

1)

−

x < 0

}

{

1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

By combining the above facts and using Theorem 9, we obtain (57) and (58).

34

(268)

(269)

(270)

(271)

(272)

(273)

(274)

(275)

(276)

(277)

(278)

(279)

(280)

(281)

D Proof of Theorem 13

Let ε > 0 and δ > 0. Deﬁne recursively

r0 := 1,

rk+1 = C√rkε

1,

∧

γk :=

ε
rk

r

(282)

some sufﬁciently large constant C > 1 (to be determined later) such that ε < C−

4. Denote by

δ0 := δ,
δk := δ(1

δk, 1

2

:=

1
2

γ0 − · · · −

−
δk + δk+1

,

γk

−
k

1),

1.

≥

(283)
(284)

(285)

(cid:1)
0, let ϕk be a continuous function from R into [0, 1] such that ϕk(u) = 1 for u

(cid:0)

δk. For k

≤
δk, ϕ′k(u) = 0 for u

≥

≤
1 let ϕ′k be a continuous function
, and linear for
1, 1
2

δk

≥

−

u

≤

2

≥

, ϕk(u) = 0 for u

δk, and linear δk, 1

For k
δk, 1
2 ≤
≥
from R into [0, 1] such that ϕ′k(u) = 1 for u
δk ≤
To begin with, we prove the following lemma.
F0 :=

Lemma 29. Deﬁne

1, 1
2

δk

≤

u

−

.

, and further recursively

F
Fk+1 :=

∈ Fk : P

{

f

≤

δk, 1

2 } ≤

f
(cid:26)

rk+1
2

.

(cid:27)

For all k

1, deﬁne

≥

and

Assume that

E(k) :=

k
(cid:26)

Gk :=

ϕk ◦

f : f

∈ Fk

,

k

0

≥

′k :=

G

(cid:8)
ϕ′k ◦
(cid:8)

f : f

∈ Fk

(cid:9)

,

(cid:9)

1.

k

≥

Pn −

P

kGk−1 ≤

E
k

Pn −

P

kGk−1 +

K2√rk

−

1ε + K3ε

√τmin

Pn −

k

P

′
k ≤

kG

E
k

Pn −

P

′
k

kG

+

∩

(cid:26)

and

(cid:0)
K2√rkε + K3ε

(cid:1)
√τmin

(cid:0)

(cid:1)

Then, it holds that

N

EN :=

E(k),

\k=1

N

1.

≥

4N exp

≤

nε2
2

.

(cid:19)

−

(cid:18)

P

Ec
N

(cid:2)

(cid:3)

Proof. The proof is based on [4, Proof of Theorem 5].

(cid:27)

,

(cid:27)

1,

(289)

k

≥

(290)

(291)

For the case C√ε
generality, we assume that C√ε < 1. For this case, we have

≥

1, by a simple induction argument, we have rk = 1. Now, without loss of

+2−(k−1)

···

rk = C1+2−1+
= C2(1
= (C√ε)2(1

−

2−k)ε1

−

2−k

2−k).

−

ε2−1+

+2−(k−1)

···

(292)

(293)

(294)

From (294), it is easy to see that rk+1 < rk for any k

0.

≥

35

(286)

(287)

(288)

Now, observe that

k

i=0
X

γi = C−

1

C√ε + (C√ε)2−1

+

+ (C√ε)2−k

· · ·

(cid:21)

= C−

1

(cid:20)

(cid:20)

≤

≤

(C√ε)2−k

+ ((C√ε)2−k

)2 + ((C√ε)2−k

)22

· · ·

+ ((C√ε)2−k

)2k

1

C−

(C√ε)2−k

+ ((C√ε)2−k

)2 + ((C√ε)2−k

)3

(cid:20)
1 ∞

C−

(C√ε)2−k

i

+ ((C√ε)2−k

)k

· · ·

(cid:21)

(cid:21)

i=1
X
(cid:0)
1(C√ε)2−k

C−

1

(cid:1)
(C√ε)2−k

1

−

1
2

,

≤
≤
(cid:1)
4, C > 2(2
log2 log2 ε−
1 and C√ε < 1. Hence, for small enough ε (note that our choice of ε

−
1 and k

(cid:0)
1)−

1, where (297) follows from i + 1

≤

−

1
4

2i
≤
4 implies

C−

≤

C−

for ε
≤
for all i
≥
C√ε < 1), we have

γ0 + γ1 +

+ γk ≤

· · ·

1
2

,

1.

k

≥

(300)

(δ/2, δ). Note also that below our choice of k will be such

1 for any ﬁxed ε > 0 will always be fulﬁlled.

Therefore, for all k
that the restriction k

≥
≤
From the deﬁnitions of (287) and (288), for k

1, we get δk ∈
log2 log2 ε−

P g2

sup
g
∈Gk

≤

sup
f
∈Fk

P

f

{

≤

≥
δk} ≤

1, we have

sup
f
∈Fk

P

f

{

≤

δk

1, 1

2 } ≤

−

rk
2 ≤

rk,

and

P g2

sup
′
g
k

∈G

≤

sup
f
∈Fk

P

f

{

≤

δk

1, 1

2 } ≤

−

rk
2 ≤

rk.

Since r0 = 1, it is easy to see that (301) and (302) trivially holds at k = 0.
Now, by the union bound, from (289), we have

P

E(k)

c

P

≤

(cid:2)(cid:0)

(cid:1)

(cid:3)

k
(cid:20)
+ P

Pn −

P

kGk−1 > E
k

Pn −

P

kGk−1 + K2√rk

−

1ε + K3ε

Pn −
In addition, by similar arguments which leads to (246), we have

Pn −

k
(cid:20)

′
k−1

′
k−1

kG

kG

P

P

> E
k

+ K2√rk

−

(cid:21)
1ε + K3ε

.

(cid:21)

Pn −

P

kGk−1 ≥

k
(cid:26)

E

(cid:2)

Pn −

k

P

kGk−1

+ u

(cid:3)

τmin
n

r

≤

(cid:27)

2 exp

2u2

.

(cid:1)

−

(cid:0)

(295)

(296)

(297)

(298)

(299)

(301)

(302)

(303)

(304)

k
(cid:26)

P

Pn −
K2√rk

kG

′

k−1 ≥

E

k

(cid:2)
1ε + K3ε

−

P

′
k−1

+ u

Pn −
√n to (304) and (305) for K2 > 0 and K3 > 0, we obtain

2 exp

r

kG

≤

−

(cid:27)

(cid:0)

(cid:1)

(cid:3)

.

(305)

2u2

τmin
n

(cid:0)
Pn −

P

P

k
(cid:26)

kGk−1 ≥

E

(cid:1)
Pn −
k

P

kGk−1

+

K2√rk

−

1ε + K3ε

√τmin

(cid:2)
2n(K2√rk

(cid:3)

1ε + K3)2

−

(cid:0)

(cid:19)

(cid:1)

2 exp

≤

−

(cid:18)

and

P

k
(cid:26)

Pn −

P

kGk−1 ≥

E

Pn −

k

P

kG

′
k−1

+

K2√rk

−

1ε + K3ε

√τmin

2 exp

≤

−

(cid:18)

(cid:2)
2n(K2√rk

−

(cid:3)

1ε + K3ε)2

(cid:0)

.
(cid:19)

(cid:1)

36

(cid:27)

(cid:27)

(306)

(307)

and

P

P

By replacing u =

Now, since 0 < C√ε

≤

1, by (294), we have

1 = (C√ε)2(1
−

2−(k−1))

rk

−

Hence, from (303), (306), (307), and (308), that

C2ε.

≥

P

c

E(k)

(cid:2)(cid:0)

(cid:1)

(cid:3)

if we choose K2 and K3 such that

4 exp

≤

−

(cid:18)

2n(K2√rk

−

1ε + K3ε)2

2n(K2C + K3)2ε2

4 exp

4 exp

≤

≤

−

(cid:18)

−

(cid:18)

nε2
2

,
(cid:19)

(cid:19)

(cid:19)

k

∀

≥

1

Then, by the union bound and (311), we have

K2C + K3 ≥

1
2

.

4N exp

≤

nε2
2

.

(cid:19)

−

(cid:18)

P

Ec
N

(cid:2)

(cid:3)

Lemma 30. Let ε > 0 and 0 < α < 2 such that

2
2+α

ε

≥

(cid:18)

1
nδα

(cid:19)

2 log n
n

Bn

∨

∨ r

(308)

(309)

(310)

(311)

(312)

(313)

(314)

for all large enough n, where Bn is deﬁned in (54). Denote by
Then, on the event EN ∩ L
, we have:
• (i) supf
f
∈Fk Pn{

δk} ≤

rk,

N

≤

≤

≤

k

0

:=

f

: Pn{

f

≤

δ

} ≤

ε

.

}

∈ F

L

(cid:8)

and

f

Pn{

δ

} ≤

ε

f

⇒

∈ FN

• (ii)

f
∀

≤

∈ L
for all positive integer N satisfying
1
η

N

≤

and η is some implicit positive constant.

log2 log2 ε−

1

and

rN ≥

ε,

(315)

Proof. We will use induction with respect to N . For N = 0, the statement is obvious. Suppose it
holds for some N
0, such that N + 1 still satisﬁes condition (315) of the lemma. Then, on the
event EN ∩ L

≥
we have

(i)

sup
f
∈Fk

f

Pn{

δk} ≤

≤

rk,

0

k

≤

≤

N

and

Suppose now that f
EN deﬁned in (290), we have f

∈ F

P

f

{

δN, 1

2 }

≤

(ii)
f
∀
∈ F
is such that Pn{

f

f

Pn{
δ
≤

δ

≤

} ≤

ε

f

∈ FN .

} ≤
ε. By the induction assumptions, on the event

⇒

(317)

f

f

f

≤

+ (P

δN, 1

∈ FN . Because of this, we obtain on the event EN +1
= Pn{
Pn)
{
f
Pn{
≤
f
Pn{
Pn{
f
ε + E
k

δN, 1
2 }
+ (P
δN }
δN }
≤
δN }
≤
P
Pn −

Pn)(ϕN (f ))
PnkGN
K2√rN ε + K3ε

+ (P
−
+
P
k
−
kGN +

−
Pn)
{

≤
δN, 1

√τmin.

≤
≤

2 }

2 }

≤

−

≤

≤

(cid:0)

(cid:1)

37

(316)

(318)

(319)

(320)
(321)

(322)

For the class

GN , deﬁne

ˆRn(

GN ) :=

1

n−

εiδXi

,

n

i=1
X

(cid:13)
(cid:13)
(cid:13)
(cid:13)

GN

(cid:13)
(cid:13)
(cid:13)
(cid:13)

where εi is a sequence of i.i.d. Rademacher random variables. By Lemma 8, it holds that

P

E

Pn −
(cid:2)(cid:13)
(cid:13)

GN

(cid:13)
(cid:13)

(cid:3)

2E
P 0
≤
k
ˆRn(
= 2E
(cid:2)

+ Bn

+ Bn.

nkGN
GN )
(cid:3)
(cid:3)

From (325), we have

(cid:2)

E

Pn −
P
GN
ˆRn(
2E
GN )
(cid:13)
(cid:2)(cid:13)
(cid:3)
(cid:13)
(cid:13)
≤
Eε
= 2E
1
EN }
(cid:3)
(cid:2)

{

+ Bn
ˆRn(

Next, by the well-known entropy inequalities for subgaussian process [32], we have

(cid:2)

(cid:2)

(cid:2)

(cid:2)

+ 2E

1

Ec

N }

{

Eε

ˆRn(

+ Bn.

GN )
(cid:3)(cid:3)

GN )
(cid:3)(cid:3)

n

Eε

1

n−

(cid:12)
(cid:12)
(cid:12)
η
(cid:12)
√n

0
Z

Eε

(cid:2)

ˆRn(

GN )
(cid:3)

≤

g

inf
∈GN

+

εjg(Xj)
(cid:12)
(cid:12)
(cid:12)
Png2)1/2
(cid:12)

j=1
X
(2 supg∈GN

for some implicit positive constant η > 0.
,
By the induction assumption, on the event EN ∩ L

H 1/2
dPn,2

GN ; u)du
(

(328)

Eε

inf
∈GN

g

n

j=1
X

1

n−
(cid:12)
(cid:12)
(cid:12)
(cid:12)

εjg(Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

≤

≤

n

1

j=1
X

n−
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Png2

2

εjg(Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p

f

Pn{

δN }

≤

p

Eε

g

g

inf
∈GN v
u
u
t
1
inf
√n
∈GN
1
√n
ε
n

inf
∈FN

f

≤

r
ε,
≤
δN } ≤

f

≤

inf f

∈FN Pn{

f

≤

δ

} ≤

f

Pn{

≤

δ

} ≤

ε by the

where (332) follows from inf f
∈FN Pn{
∈ FN .
induction assumption with f
We also have on the event EN ∩ L
sup
g
∈GN

Png2

, by (301), it holds that

≤

sup
f
∈FN

f

Pn{

δN } ≤

≤

rN .

The Lipschitz norm of ϕk

1 and ϕ′k is bounded by

−

L = 2(δk

1 −

−

δk)−

1 = 2δ−

1γ−
k
−

1
1 =

2
δ

r

1

rk
−
ε

which implies the following bound on the distance:

(323)

(324)

(325)

(326)

(327)

(329)

(330)

(331)

(332)

(333)

(334)

(335)

(336)

(337)

d2
Pn,2(ϕN ◦

f ; ϕN ◦

n

g) = n−

1

ϕN (f (Xj))

ϕN (g(Xj))

2

−

d2
Pn,2(f, g).

(cid:12)
(cid:12)

j=1
X
(cid:12)
(cid:12)
rN
ε

r

2
δ

2

(cid:19)

≤

(cid:18)

38

Therefore, on the event

,
EN ∩ L
1
√n

0
Z

du

GN ; u
(cid:0)

;

(cid:1)
δ√εu
2√rN (cid:19)

du

F
(cid:18)
α/4 r1/2
−
N
√nδα/2

α/4

(2 supg∈GN

Png2)1/2

H 1/2

dPn ,2

1
√n

≤

(2rN )1/2

H 1/2

dPn ,2

0
Z
2√D

≤

(cid:18)

(cid:18)

≤

=

(cid:18)

1

α/2

−

(cid:19)(cid:18)
21/2+α/4√D

α/2

−
21/2+α/4√D

1

1

α/2

−

rN
ε

(cid:19)

(cid:19)

(cid:19)
r1/2
N
εα/4 ε

2+α
4

√rN ε,

where (341) follows from the condition (314), which implies that

1

n1/2δα/2 ≤

2+α

4 .

ε

≤

GN )
(cid:3)
Eε

1

α/2

−

From (328) and (341), we obtain that on the event EN ∩ L
,
Eε

ˆRn(

ε +

21/2+α/4η√D

√rN ε.

(cid:2)
On the other hand, we also have

Hence, by combining with (343) and (344), from (327), we obtain
Ec

= 2E

E

P

1

1

Eε

Pn −
(cid:2)(cid:0)(cid:13)
(cid:13)

GN

(cid:13)
(cid:13)

(cid:1)(cid:3)

(cid:2)
≤

2

{

EN }
ε + η

(cid:20)

(cid:18)

−

ˆRn(

1.

≤

GN )
(cid:3)
+ 2E

(cid:2)
ˆRn(
GN )
21/2+α/4η√D
(cid:3)(cid:3)
(cid:2)
α/2

1

Eε

ˆRn(

{

N }

(cid:2)
√rN ε
(cid:21)

(cid:19)

(cid:2)

+ 8N exp

GN )
(cid:3)(cid:3)
−

(cid:18)

Now, by the condition (314), it holds that

and

Bn,

ε

≥

8ηN exp

nε2
2

≤

(cid:19)

−

(cid:18)

≤

≤

≤

≤

8ηN
n

1

8 log2 log2 ε−
n

8 log2 log2

n
2 log n

q

n
2 log n
n

η

r
ηε,

(338)

(339)

(340)

(341)

(342)

(343)

(344)

+ Bn

(345)

nε2
2

(cid:19)

+ Bn.

(346)

(347)

(348)

(349)

(350)

(351)

(352)

for n sufﬁciently large, where (348) and (350) follows from ε
and (351) holds for n sufﬁciently large.

≥

q

2 log n

n , (349) follows from (315),

From (346), (347), and (352), it holds that

P

E

Pn −
(cid:2)(cid:13)
(cid:13)

GN

≤

(cid:13)
(cid:13)

(cid:3)

2

ε + η

(cid:20)

= 4ε + 2η

21/2+α/4√D

1

α/2

−
21/2+α/4√D

α/2

1

−

(cid:18)

(cid:18)

√rN ε

+ 2ε

(cid:19)

(cid:21)

√rN ε.

(cid:19)

(353)

(354)

39

In addition, we have

or

rN = (C√ε)2(1
−

2−N )

C2ε,

≥

rN
C2 .

ε

≤

Hence, from (354) and (356), we conclude that with some constant ˜η > 0,

˜η√rN ε.

E

P

Pn −
(cid:2)(cid:13)
From (322) and (357), on the event EN +1 ∩ L
(cid:13)
ε + ˜η√rN ε +
f
P
{
1
2

C√rN ε

δN, 1

2 } ≤

≤

GN

≤
(cid:13)
(cid:3)
, we have
(cid:13)

≤
= rN +1/2,

K2√rN ε + K3ε

√τmin

(cid:0)

(cid:1)

by a proper choice of the constant C > 0, where (360) follows from (356). This means that f
FN +1 and the induction step for (ii) is proved.
Now, we prove (i). We have on the event EN +1,
δN +1} ≤

δN +1}

+ sup
f

Pn{

P )
{

≤

≤

≤

P

f

f

f

f

f

(361)

sup
∈FN +1

{

sup
∈FN +1
sup
{
f
∈FN +1
rN +1/2 + E
k

δN +1}
δN +1}
≤
P
Pn −

P

f

+

k

∈FN +1
Pn −
+

′
N +1

kG

≤

≤

(Pn −
P

kG

′
N +1

K2√rN +1ε + K3ε

(cid:0)

(cid:1)

(362)

√τmin.

(363)

By Lemma 8, we have
Pn −
(cid:13)
(cid:13)

E

P

′
N +1

G
2E
(cid:13)
1
(cid:13)
2E
(cid:2)

1

≤

≤

Eε
Eε

EN }
EN }

{

{

ˆRn(
G
ˆRn(
(cid:2)
G

′N +1)
′N +1)

(cid:3)(cid:3)

+ 2E
+ 2E
(cid:2)

1

1

Ec
Ec

N }
N }

{

{

Eε
Eε

ˆRn(
G
ˆRn(
(cid:2)
G

′N +1)
′N +1)

(cid:3)(cid:3)

+ Bn

+ ε,

(364)

(365)

where (365) follows from the condition (314).
(cid:2)

(cid:2)

(cid:3)(cid:3)

(cid:2)

(cid:2)

(cid:3)(cid:3)

As above, we have

Eε

ˆRn(
G

′N +1)

(cid:2)

(cid:3)

≤

g

inf
′
N +1
∈G

n

1

εjg(Xj)
(cid:12)
(cid:12)
(cid:12)
Png2)1/2
(cid:12)
N +1

j=1
X
(2 supg∈G′

Eε

+

n−
(cid:12)
(cid:12)
(cid:12)
η
(cid:12)
√n

0
Z

,
Since we already proved (i), it implies that on the event EN +1 ∩ L

H 1/2
dPn,2

G
(cid:0)

′N +1; u

du.

(366)

(cid:1)

2

Eε

inf
∈G

′
N +1

g

n

j=1
X

1

n−
(cid:12)
(cid:12)
(cid:12)
(cid:12)

εjg(Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(355)

(356)

(357)

(358)

(359)

(360)

∈

(367)

(368)

(369)

(370)

(371)

≤

g

inf
∈G

′

1
√n
1
√n
1
√n

≤

≤

≤

n

1

j=1
X

n−
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Png2

εjg(Xj)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Eε

N +1 v
u
u
t
inf
∈G

g

′
N +1

p

inf
∈FN +1

f

inf
∈FN

f

q

q
Pn{

Pn{

f

δN,1/2}

≤

f

δN,1/2}

≤

ε
n ≤

ε.

≤

r

40

,
By the induction assumption, we also have on the event EN +1 ∩ L
δN,1/2} ≤

Pn{

Png2

sup

≤

≤

f

rN +1

2 ≤

sup
∈FN +1

f

g

∈G

′
N +1

rN .

(372)

The bound for the Lipschitz norm of ϕ′k gives the following bound on the distance

d2
Pn,2

ϕ′N +1 ◦
(cid:0)

f ; ϕ′N +1 ◦

f

n

= n−

1

(cid:1)

j=1
X
(cid:12)
(cid:12)
rN
ε

r

2
δ

≤

(cid:18)

ϕ′N +1 ◦

f (Xj)

ϕ′N +1 ◦

−

2

g(Xj)

(373)

2

(cid:19)

d2
Pn,2(f, g).

(cid:12)
(cid:12)

(374)

Therefore, on the event EN +1 ∩ L

, we get quite similarly to (341),

(2 supg∈G′

N +1

Png2)1/2

1
√n

0
Z

(2rN )1/2

H 1/2
dPn ,2

1
√n

0
Z

(cid:18)

21/2+α/4√D

1

α/2

−
21/2+α/4√D

rN
ε

(cid:19)(cid:18)

(cid:19)

√rN ε.

≤

≤

=

α/2

1

−

(cid:18)

(cid:18)

H 1/2
dPn,2

′N +1; u

du

(cid:1)
du

G
(cid:0)
;

δ√εu
2√rN (cid:19)

F
1/2
α/4 r
N

−
√nδα/2

α
4

(cid:19)
,
EN +1 ∩ L
δN +1} ≤

rN +1
2

+ ¯η√rN ε

We collect all bounds to see that on the event

sup
∈FN +1

f

f

Pn{

≤

for some constant ¯η > 0.

(375)

(376)

(377)

(378)

Therefore, it follows that with a proper choice of constant C > 0 in the recurrence relationship
deﬁning the sequence

rk}

{

, we have on the event EN +1 ∩ L
sup
∈FN +1

δN +1} ≤

Pn{

≤

f

f

C√rN ε = rN +1,

(379)

which proves the induction step for (i) and, therefore, the lemma is proved. Finally, using Lemma
31 and the same arguments as [4, Proof of Theorem 5], we can prove Theorem 13.

Lemma 31. Suppose that for some α
holds. Then for any constant ξ > C2, for all δ

∈

0 and

≥

(0, 2) and for some D > 0 such that the condition (68)

2
2+α

ε

≥

(cid:18)

1
nδα

(cid:19)

2 log n
n

∨

Bn,

∨ r

(380)

and for all large enough n, the following:

P

f
∃

(cid:20)

and

P

f
∃

(cid:20)

: Pn{

f

≤

δ

} ≤

∈ F

ε and P

δ
2

f
(cid:26)

≤

ξε

≤

(cid:21)

≥

(cid:27)

4/η log2 log2 ε−

1 exp

nε2
2

(cid:27)(381)

−

(cid:26)

: P

f

{

≤

δ

} ≤

∈ F

ε

and Pn

δ
2

f
(cid:26)

≤

ξε

≤

(cid:21)

≥

(cid:27)

4/η log2 log2 ε−

1 exp

nε2
2

,
(cid:27)
(382)

−

(cid:26)

where η is some constant.

41

Proof. Observe that

P

f
∃

(cid:20)

: Pn{

f

≤

δ

} ≤

ε

P

f

{

∧

≤

δ/2

} ≥

ξε

∈ F

(cid:21)

:

f

Pn{

{

≤

δ

} ≤

ε

} ∧ {

P

f

{

≤

δ/2

} ≥

∈ F

ξε

EN

+ P[Ec

N ]

(383)

P

P

P

f
∃

(cid:20)(cid:26)

f
∃

(cid:20)(cid:26)

f
∃

(cid:20)(cid:26)

≤

≤

≤

P

f
∃
≤
(cid:20)(cid:26)
= P[Ec
N ]

}

∩

(cid:27)
+ P[Ec

N ]

(cid:21)
+ P[Ec

N ]

∈ FN ∩ L} ∧ {

P

f

{

≤

δ/2

} ≥

ξε

EN

∩

}

(cid:27)

∈ FN ∩

∈ FN ∩

L

L

P

f

{

≤

} ∧ {

δN } ≥

ξε

}

∩

EN

P

f

{

δN }

≤

} ∧ {

(cid:27)
> rN }

∩

(cid:27)

(cid:21)
EN

(cid:21)

+ P[Ec

N ]

(cid:21)

(384)

(385)

(386)

(387)

(388)

(389)

4N exp

≤

−

(cid:18)

nε2
2

(cid:19)
1

4/η

log2 log2 ε−

exp

≤

nε2
2

−

,
(cid:19)

(cid:0)

(cid:18)
(C√ε)2 < ξε for some
where (384) follows from (ii) in Lemma 30, (386) follows from rN ≤
constant ξ > C2, and (387) follows from (i) in Lemma 30, and (389) follows from the condition
(315) in Lemma 30, which holds for n sufﬁciently large.

(cid:1)

Now, we return to prove Theorem 13.

Proof of Theorem 13.

. Consider sequences δj := 2−

j 2
γ , and

εj :=

1
2+α′

,

1
nδα′

j (cid:19)

(cid:18)

0,

j

≥

α′ :=

2γ

2

−

α.

γ ≥

εj = n(γ

2)/42j.

−

where

Then, we have

Let

:=

j
{∃

≥

0

E

f
∃

: Pn{

f

δj} ∧

P

f

{

≤

≤

δj/2

.

ξεj}

} ≥

∈ F

By Lemma 31, the condition (68) implies that there exists ξ > 0 such that

P

E

≤

(cid:2)

(cid:3)

≤

∞

j=0
X
∞

4/η

4υ′

log2 log2 ε−
j

1

exp

(cid:0)
log2 log2 n

(cid:1)

nε2
j
2

γ
2

n
2

(cid:19)

22j

(cid:21)

−

(cid:18)

exp

−

(cid:20)

j=0
X

(cid:0)
log2 log2 n

exp

0

≥

(cid:1) Xj
n
2

−

γ
2

(cid:20)

(cid:21)

υ

≤

(cid:0)

(cid:1)

∈
ˆδn(γ; f )

(δj, δj

1].

−

∈

42

for some υ, υ′ > 0. Now, since ˆδn(γ; f )

(0, 1], there exists some j

1 such that

≥

(390)

(391)

(392)

(393)

(394)

(395)

(396)

(397)

Then, by the deﬁnition of ˆδn(γ; f ) in (67), we have
Pn{
≤
γ
j n−
δ−

δj} ≤

Pn{

≤

f

f

ˆδn(γ; f )
}
1+ γ
2

Suppose that for some f

∈ F

, the inequality ζ−

δn(γ; f ) fails, which leads to

Then, if ζ > 21+ 2

γ , from the deﬁnition of δn(γ; f ) in (66), it holds that

≤
q
= εj.
1 ˆδn(γ; f )

≤

1ˆδn(γ; f )

δn(γ; f ) < ζ−
δj
−
ζ

≤

1

.

P

f

{

≤

δj/2

} ≥

δj
−
ζ

P

f
(cid:26)

≤

1

(cid:27)
γ

1+γ/2

n−

1

δj
−
ζ

−

(cid:19)

22jζγn−

1+ γ
4

≥ s(cid:18)
1
2

=

r

= εj

ζγ
4

r

(407)
by choosing ζ sufﬁciently large, where ξ is deﬁned in Lemma 31. From (407), it holds that
ζ−

δn(γ; f ) fails for some f

1ˆδn(γ; f )

> ξεj

].

∈ F

to hold with probability at most P[
ζ ˆδn(γ; f ) fails for some f

E

with probability

∈ F

≤

E

Similarly, we can show that the event δn(γ; f )
at most P[

].

≤

By using the union bound, we ﬁnally obtain (69).

E Proof of Lemma 14

Proof. Observe that

Pn(f

y)

P (f

y)

≤

−
n

≤
1
n

=

(cid:12)
(cid:12)

Now, let

1

i=1
X

(cid:0)

for all x

and n

∈

∈ S

Now, let

Then, we have

(cid:12)
f (Xi)
(cid:12)
≤

{

y

} −

P(f (Xi)

≤

y)

+

1
n

(cid:1)

fn(x) := 1
{
Z+. It is clear that

f (x)

y

≤

} −

n

P(f (Xi)

i=1
X
P(f (Xn)

y),

≤

y)

−

≤

P (f

≤

y).

(408)

fnk∞ ≤

k

1.

g(x) :=

1
n

n

i=1
X

fi(xi).

(cid:12)
(cid:12)

g(x)

−

g(y)

=

(cid:12)
(cid:12)

≤

≤

1
n

1
n

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

n

i=1
X
n

i=1
X
n

(cid:0)
1

{

i=1
X

fi(xi)

−

fi(yi)

(cid:0)
1

f (xi)

{

y

≤

(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
} −

{

f (yi)

y

}

≤

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)

xi 6

= yi}

.

43

(398)

(399)

(400)

(401)

(402)

(403)

(404)

(405)

(406)

(409)

(410)

(411)

(412)

(413)

(414)

Then, by applying Lemma 28, it holds that

n

i=1
X
R, let

P

1
n

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)
∈

r

t

≥

fi(Xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜fy(x) := 1

τmin
n

≤

(cid:21)

2 exp

2t2

.

(cid:1)

−

(cid:0)

f (x)

y

.

{
= 1. Hence, by Lemma 8, it holds that

≤

}

On the other hand, for all y

R

k

˜fyk∞
P(f (Xi)

It is clear that supy

∈

n

i=1
X

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

y)

−

≤

P (f

≤

y)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

≤

n

i=1
X
Bn,

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
p

E[ ˜fy(Xi)]

−

Eπ

˜fy(X)

(cid:2)

(cid:12)
(cid:12)
(cid:3)
(cid:12)
(cid:12)

where (418) follows from Lemma 27 (with M = 1) and the Cauchy–Schwarz inequality.

From (408), (415), and (418), we have

sup
f

sup
R
y

Pn(f

∈F
with probability at least 1

∈
(cid:12)
(cid:12)
2 exp(

−

−

y)

−

≤

P (f

≤

y)

≤

Bn + t

2t2).

p

(cid:12)
(cid:12)

τmin
n

r

F Proof of Lemma 16

Let δ > 0. Let ϕ(x) be equal to 1 for x

0, 0 for x

≥

≤

1 and linear in between. Observe that

Ff (y) = P

f

{
P ϕ

≤
f

(cid:18)
Pnϕ

}
y

y

−
δ

f

−
δ

(cid:19)
y

(cid:18)

(cid:19)
Fn,f (y + δ) +

≤

≤

≤

and

+

Pn −
(cid:13)
P
Pn −
(cid:13)
(cid:13)
(cid:13)

P

˜
Gϕ
,

(cid:13)
(cid:13)
˜
Gϕ

(cid:13)
(cid:13)

Fn,f (y)

≤

≤

≤

≤

f

Pn{
Pnϕ

≤
f

(cid:18)
f

P ϕ

y

}
y
−
δ

y

−
δ

(cid:18)

(cid:19)
Ff (y + δ) +

(cid:19)

P

˜
Gϕ
(cid:13)
.
(cid:13)
˜
Gϕ

+

Pn −
(cid:13)
P
Pn −
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

Now, by applying Lemma 28 (see (246)), we have

P

Pn −

k

P

˜
Gϕ ≥

k

Pn −

P

k

˜
Gϕ

k

+ t

E

(cid:20)
(cid:2)
From (428), with probability at least 1

2 exp(

−

−

τmin
n

≤

2 exp

2t2

.

(cid:1)

−

(cid:0)

r
(cid:3)
2t2), it holds that

(cid:21)

Pn −

k

P

˜
Gϕ ≤

k

E

Pn −

k

P

˜
Gϕ

k

+ t

On the other hand, from Lemma 8, we have

(cid:2)

(cid:3)

τmin
n

.

r

E

Pn −

P

(cid:2)(cid:13)
(cid:13)

˜
Gϕ

≤

(cid:3)

(cid:13)
(cid:13)

2E

P 0
nk

k

˜
Gϕ

(cid:2)

(cid:3)

+ Bn.

44

(415)

(416)

(417)

(418)

(419)

(420)

(421)

(422)

(423)

(424)

(425)

(426)

(427)

(428)

(429)

(430)

From (429) and (430), with probability at least 1

2 exp(

−

−

2t2), it holds that

Pn −
(cid:3)
From (423), (427), and (431), with probability at least 1

˜
Gϕ ≤

k
(cid:2)

˜
Gϕ

P

k

k

P 0
nk

2E

+ Bn + t

τmin
n

.

r

(431)

2 exp(

−

−

2t2), we have

L(Ff , Ff,n)

δ + 2E

P 0
nk

˜
Gϕ

+ Bn + t

τmin
n

.

k
(cid:2)
Furthermore, by Talagrand’s contraction lemma [30, 31] for the class of function ˜ϕ(x) := ϕ(x)
we have

r

≤

(cid:3)

1,

−

(432)

E

P 0
nk

k

˜
Gϕ

(cid:2)

(cid:3)

≤

=

≤

2E
(cid:20)

f

∈F

sup
[
,y

∈

−

i=1
X

M,M] (cid:12)
(cid:12)
n
(cid:12)
(cid:12)
f (Xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1
X

2M
δ√n

.

1 sup
f
∈F (cid:12)
(cid:12)
(cid:12)
+
(cid:12)

P 0

nkF

E

n−

2
δ

2
δ

(cid:20)

E

k

(cid:2)

(cid:3)

n

εi

f (Xi)
δ

y

−

(cid:12)
(cid:21)
(cid:12)
n
(cid:12)
(cid:12)

i=1
X

εi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(433)

(434)

(435)

+

2M
δn

(cid:21)

E
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Hence, by setting δ :=
least 1

2 exp(

2t2) that
p

−

−

4E[

P 0

nkF

k

] + 4M/√n, from (432) and (435), it holds with probability at

L(Ff , Ff,n)

≤

4

E[

q

P 0

nkF

k

] + M/√n + Bn + t

τmin
n

.

r

G Proof of Theorem 17

Fix M > 0. Since

FM ∈

GC(P), we have

Pn −
which, by Lemma 8 with t = √log n,

k

(cid:2)

P

kFM

→

0 a.s. n

,
→ ∞

(cid:3)

E

E

P

Pn −
(cid:13)
(cid:2)(cid:13)
, from (438), we obtain
(cid:13)
(cid:13)
E

→ ∞

FM

E

1
2

(cid:2)

≥

(cid:3)

P 0

nkFM

k

˜An.

−

(cid:3)

P 0

nkFM

k

0,

→

By taking n

or

(cid:2)

εiδXi

n

i=1
X

E

1

n−

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:21)FM
(cid:13)
(cid:13)
(cid:13)

(cid:3)

→

0,

as n

.
→ ∞

Furthermore, with t = √log n, by Lemma 16, we have

P

sup
∈FM

f

(cid:26)

L(Fn,f , Ff )

≥

4

E[

q

P 0

nkF

k

] + M/√n + Bn + log n

τmin
n

(cid:27)

r

It follows from (441) that

2 exp

≤

−

(cid:0)

2 log n

=

2
n2 .

(cid:1)

∞

P

n=1
X

sup
∈FM

f

(cid:26)

L(Fn,f , Ff )

≥

4

E[

q

P 0

nkF

k

] + M/√n + Bn + log n

τmin
n

(cid:27)

r

1
n2 <

.
∞

2

≤

∞

n=1
X

45

(436)

(437)

(438)

(439)

(440)

(441)

(442)

(443)

(444)

(445)

(446)

(447)

(448)

(449)

(450)

Hence, by Borel-Cantelli’s lemma [33], Bn →

0, and (440), we obtain

sup
∈FM
L(Fn,fM , FfM ) = supf

f

Since supf

∈F

L(Fn,f , Ff )

0,

→

a.s..

∈FM L(Fn,f , Ff ), from (443), we have
a.s..
L(Fn,f , Ff )

0,

→

sup
f

∈F

L(Fn,fM , FfM ) = sup
∈FM

f

Now, by [4], the following facts about Levy’s distance holds:

and

L(Ff , FfM )

sup
f

∈F

≤

sup
f

∈F

P

f

{|

| ≥

M

}

L(Fn,f , Fn,fM )

sup
f

∈F

≤

sup
f

∈F

f

Pn{|

| ≥

M

.

}

Now, by the condition (78), we have

sup
f

∈F

sup
f

∈F

P

f

{|

| ≥

M

} →

0

a.s.

M

,
→ ∞

L(Ff , FfM )

0,

→

a.s.

M

.
→ ∞

so

To prove that

lim
→∞

M

lim sup
n

→∞

sup
f

∈F

L(Fn,f , Fn,fM ) = 0,

a.s.,

it is enough to show that

lim
→∞

M

lim sup
n

sup
f

f

Pn{|

| ≥

}

M

= 0,

a.s.

→∞
To this end, consider the function ϕ from R into [0, 1] that is equal to 0 for
1 for

> M and is linear in between. We have

∈F

u

|

|

u

|

| ≤

M

−

1, is equal to

sup
f

∈F

f

Pn{|

| ≥

M

}

f

≤

= sup
f
∈FM
sup
∈FM
sup
∈FM
sup
∈FM

≤

≤

f

f

M

}

f

Pn{|
Pnϕ(
|

| ≥

f

)

|

f

P ϕ(
|

|

) +

k

P

f

{|

| ≥

M

Pn −
1

−

}

where

:=

G

ϕ

f

|

◦ |

: f

∈ FM

.

P

+

kG
Pn −

k

P

,

kG

(451)

(452)

(453)

(454)

(455)

Then, by using the same arguments to obtain (246), it holds with probability 1

(cid:8)

(cid:9)

Pn −

k

P

kG ≤

E

Pn −

k

P

kG

+ t

(cid:2)
2E[

≤

P 0

nkF

k

r

(cid:3)
] + An + t

τmin
n
τmin
n

r

.

2 exp(

−

−

2t2) that

(456)

(457)

Then, by setting t = log n and using the Borel-Cantelli’s lemma [33], the following holds almost
surely:

Pn −

k

P

kG ≤

2E

k
(cid:2)

P 0

nkG

(cid:3)

46

+ An + log n

τmin
n

.

r

(458)

Now, since ϕ

f

◦

∈

ϕ

◦ FM , by (440) and Talagrand contraction lemma [31, 32], we have
as n

P 0

E

0

k

nkG

→

.
→ ∞

From (458) and (459), we obtain

(cid:2)

(cid:3)
Pn −

k

P

0 a.s..

kG →

Hence, we obtain (ii) from (i), the condition (78), and (460).

To prove that (ii) implies (i), we use the following bound [34]

which holds with some constant υ = υ(M ) for any two distribution functions on [
bound implies that

M

M

Z

−

td(F

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

G)(t)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

υL(F, G),

≤

Pn −

k

P

kFM = sup
∈FM |

f

Pnf

P f

|

−

M

M, M ]. This

−

(462)

≤

≤

≤

f

sup
∈FM (cid:12)
(cid:12)
(cid:12)
(cid:12)
υ sup
f
∈FM

Z

−

M

td(Fn,f −

Ff )(t)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
L(Fn,f , Ff ) + M sup
∈FM

f

υ sup
f

∈F

L(Fn,f , Ff ) + M sup
∈FM

f

+ M sup
f
∈FM

f

P (
|

| ≥

M )

f

Pn(
|

−

| ≥

M )

(cid:12)
(cid:12)
M )

f

P (
|

| ≥

f

Pn(
|

−

| ≥

M )

(cid:12)
P (
(cid:12)
|

f

M )

f

Pn(
|

−

| ≥

| ≥

M )

(cid:12)
(cid:12)

.

(cid:12)
(cid:12)
2 exp(

(cid:12)
(cid:12)
2t2), the following holds:

Now, by Lemma 14, with probability at least 1

−

Pn(f

y)

−

≤

P (f

≤

y)

t

≤

+

Bn.

(466)

τmin
n

r

p

By setting t = √log n and using the Borel-Cantelli’s lemma, from (466), we obtain

Pn(f

y)

−

≤

P (f

≤

y)

→

0,

a.s..

(467)

sup
R
y

∈

sup
∈FM

f

(cid:12)
(cid:12)

sup
R
y

∈

sup
∈FM

f

(cid:12)
(cid:12)

Finally, from (466), (467), and (ii), we obtain (i). This concludes our proof of Theorem 17.

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(459)

(460)

(461)

(cid:12)
(463)
(cid:12)
(464)

(465)

H Proof of Theorem 19

The proof is based on [4, Proof of Theorem 9]. Since
M > 0 such that

. To prove the ﬁrst statement, note that

F

FM =

F

is uniformly bounded, we can choose

BCLT(P) means that

F ∈

Now, from Lemma 8, we have

E

(cid:2)

Pn −

k

P

kF

= O(n−

1/2).

(cid:3)

1
2
for all t > 0. By applying (469), it easy to see that

Pn −
(cid:2)(cid:13)
(cid:13)

(cid:13)
(cid:13)

≥

E

P

(cid:3)

F

E

P 0

nkF

k

(cid:2)

−

(cid:3)

˜An,

εiδXi

n

i=1
X

E

1

n−

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

= E

P 0

k
(cid:2)

nkF
(cid:3)
2 ˜An + 2E

O

(cid:18)r

log n
(cid:2)(cid:13)
(cid:13)
n
(cid:19)

(cid:13)
F (cid:21)
(cid:13)
(cid:13)
(cid:13)

≤

≤

Pn −

P

(468)

(469)

(470)

(471)

(472)

F

(cid:13)
(cid:13)

(cid:3)

since ˜An = O

log n
n

by (50).

(cid:0)q

(cid:1)

47

Now, from Lemma 16, for t = √log n,

P

sup
f

L(Ff , Ff,n)

(cid:26)

q
From (472) and (473), it holds that

∈F

4

E[

≥

P 0

nkF

k

] + M/√n + Bn +

τmin log n
n

r

≤

(cid:27)

2
n2 .

n
log n

P

(cid:26)(cid:18)

1/4

sup
f

∈F

(cid:19)

L(Fn,f , Ff )

D

≥

→

(cid:27)

0

as n

→ ∞

for some constant D, or

L(Fn,f , Ff ) = OP

sup
f

∈F

log n
n

1/4

.
(cid:19)

(cid:19)

(cid:18)(cid:18)

Now, recall

˜
Gϕ :=

ϕ

(cid:26)

◦

(cid:18)

f

y

−
δ

−

(cid:19)

1 : f

, y

[

−

∈

∈ F

M, M ]

.
(cid:27)

To prove the second statement, we use the following fact [4, p.29]:

Eε

P 0
nk

k

˜
Gϕ

d
√n

≤

√2

0

H 1/2
dPn ,2

(
F

(cid:20) Z

; δu)du +

log

r

4M
δ

+ 1

(cid:21)

(cid:3)
for some constant d, which, under the condition (81), satisﬁes

(cid:2)

Eε

P 0
nk

˜
Gϕ

k
(cid:2)

(cid:3)

1
δα/2

≤

d

(cid:20)

r

1
n

+

1
√n

log

(cid:18)r

4M
δ

+ 1

.

(cid:19)(cid:21)

Now, by Lemma 16, it holds for all t > 0 and δ > 0 that

P

sup
f

∈F

(cid:26)

L(Ff , Ff,n)

δ + E

P 0
nk

k

˜
Gϕ

≥

+ Bn + t

τmin
n

r

≤

(cid:27)

2 exp(

−

2t2).

(cid:2)

= E[

P 0
nk

k

˜
Gϕ

]

≤

˜
Gϕ

(cid:3)

d
(cid:2)q

log n
n δ−

α/2 + 1
√n

log 4M

δ + 1

, from (479),

(cid:0)q

(cid:1)(cid:3)

δ + d

(cid:20)r

log n
n

δ−

α/2 +

1
√n

log

(cid:18)r

4M
δ

+ 1

+ Bn + t

(cid:19)(cid:21)

τmin
n

r

1

Since E
n−
for all t > 0, we have

n
i=1 εiδXi

(cid:13)
(cid:13)

P

P

sup
f

∈F

(cid:26)

L(Ff , Ff,n)

2 exp

≤

2t2

.

−

(cid:13)
(cid:13)

≥

(cid:1)
Now, by choosing t = √log n and δ = (log n)n−

(cid:0)

1
2+α , we have

P

L(Ff , Ff,n)

ν(log n)n−

1
2+α

≥

2
n2

≤

(cid:26)
for some constant ν and for n

(cid:27)
N0 for some ﬁnite N0 big enough.

sup
f

∈F

≥

From (481), we have

∞

P

n=1
X

sup
f

∈F

(cid:26)

L(Ff , Ff,n)

≥

ν(log n)n−

1
2+α

∞

N0 +

Xn=N0

≤

(cid:27)

2
n2 <

.
∞

Hence, by Borel-Cantelli’s lemma [33], it holds that

L(Ff , Ff,n) = OP

(log n)n−

1
2+α

,

a.s.

sup
f

∈F

This concludes our proof of Theorem 19.

(cid:1)

(cid:0)

48

(473)

(474)

(475)

(476)

(477)

(478)

(479)

(cid:27)
(480)

(481)

(482)

(483)

