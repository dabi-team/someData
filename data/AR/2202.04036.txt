Residual Aligned: Gradient Optimization for Non-Negative Image Synthesis

Flora Yu Shen1 Katie Luo2 Guandao Yang1,2 Harald Haraldsson1 Serge Belongie1,2

1Cornell Tech

2Cornell University

2
2
0
2

b
e
F
8

]

V
C
.
s
c
[

1
v
6
3
0
4
0
.
2
0
2
2
:
v
i
X
r
a

Abstract

In this work, we address an important problem of optical
see through (OST) augmented reality: non-negative image
synthesis. Most of the image generation methods fail under
this condition, since they assume full control over each pixel
and cannot create darker pixels by adding light. In order to
solve the non-negative image generation problem in AR im-
age synthesis, prior works have attempted to utilize optical
illusion to simulate human vision but fail to preserve light-
ness constancy well under situations such as high dynamic
range. In our paper, we instead propose a method that is
able to preserve lightness constancy at a local level, thus
capturing high frequency details. Compared with existing
work, our method shows strong performance in image-to-
image translation tasks, particularly in scenarios such as
large scale images, high resolution images, and high dy-
namic range image transfer.

1. Introduction

Non-negative image generation problem [9] remains a
difﬁcult challenge in Augmented Reality (AR). Since the
AR projector can only add light to the images, it is not pos-
sible to convert a lighter pixel to a darker pixel in an abso-
lute way, let alone preserving the lightness constancy [1].
Therefore, many works attempted to utilize human optical
illusion [9, 14, 12] in vision simulation. From AR projector
relighting [6] to OST content optimization [9], there have
been many attempts to improve the overlaid content dis-
played on real world views; nevertheless, there still exists
room for the visual quality to be improved.

Prior works in ﬁeld such as StayPositive formalized the
problem as optimizing the residual image from state-of-art
image generation methods. With an attempt to simulate hu-
man vision, their method assumes all parts of the image
contributes to the visual output equally, and they solve it
via a normalization over global maximum and minimum:

N (x) =

x − xmin
xmax − xmin

,

(1)

where x is each pixel, xmin and xmax are the global max-
imum and minimum. However, the global normalization
didn’t preserve lightness constancy [1] relative to surround-
ing neighborhoods, thus causing problems such as the in-
put leaking through, and over-exposure artifacts in high dy-
namic range images.

Inspired by lightness illusion [3], color constancy illu-
sions [10, 11] and Poisson Image Editing [13], we designed
a method that leverages gradient matching to preserve lo-
cal features and blend a target image with the input im-
age to realize image synthesis, with a soft constraint of
non-negativity. This method leverages neighborhood re-
gion constraints and performs optimization over every sin-
gle pixel through optimizing a gradients loss. After all, our
method achieves strong results in ensuring lightness con-
stancy on high dynamic range images, and preserving ﬁne
grained details on high resolution large scale images.

2. Method

Because we observe humans are more prone to seeing
changes in darkness and brightness [8], we are able to pro-
duce a better residual for human perception by focusing on
lightness constancy at a local level. In this section, we begin
by discussing our two-part loss function inspired by prior
work, then examine a local perceptual assumption — gra-
dient similarity — and propose a computationally efﬁcient
method to incorporate it into the framework. Lastly we will
go into some implementation details.

2.1. Framework

We formalize our problem as follow: we have input
I, desired proposal P generated from any state-of-the-art
image-to-image translation method [5, 4, 7], and we wish
to optimize a residual Rθ that we overlay on the input. By
optimizing a two-part loss function, our method generates
the ﬁnal residual.

We assume that our ﬁnal output image is a simple α-

blending between the input I and the residual image Rθ:

Oθ = clip{αI + (1 − α)Rθ},

(2)

 
 
 
 
 
 
ing image gradients in order to preserve lightness constancy
at local features.

2.2. Gradient Loss

Inspired by seamless cloning and Poisson Image Editing
[13], we decided to add constraint for pixels we want to ﬁne-
tune. This gives us the ability to match instance changes
in the proposal images, thus mimicking the way humans
perceptual systems detect changes in patterns and edges.
However, unlike in Poisson Image Editing, we replace the
hard constraint of value of the surrounded pixel, and use
a soft constraint such that the modiﬁed pixel value should
be non-negative. Also, in order to ﬁne-tune the whole pro-
posal images, we do not have a bounding area nor guidance
ﬁeld. Instead, we decided to use image gradients matching
to “blend” the input image and target image,

Lgrad(Oθ, P ) =

(cid:13)
(cid:13)
(cid:13)∇xOθ − ∇xP

(cid:13)
(cid:13)
(cid:13),
(5)
where Oθ = clip{αI + (1 − α)Rθ} is the output of our
optical combiner.

(cid:13)
(cid:13)
(cid:13)∇yOθ − ∇yP

(cid:13)
(cid:13)
(cid:13) +

By using gradient matching, we not only solve the prob-
lem with no target boundary, but we also allow the output
to preserve local details and neighborhood features. For
implementation, our gradient net uses the Sobel Operator
[15] to calculate image gradients. The Sobel Operator is
traditionally used as an edge-detector algorithm, allowing
us to match the high frequency details in our proposal im-
age. Furthermore, it is a discrete differentiation operator,
and gives us an efﬁcient approximation of the gradient of
the image intensity function. Thus, we choose Sobel Op-
erator because it emphasizes on high frequency details, is
easy to implement, and is fast to compute.

2.3. Implementation Details

We start by initializing a zero-valued residual image, Rθ,
as the optimizable parameters. We clip the residual image
into the feasible range, and minimize the total pixel value
amount that it is out of range using the constraint loss from
Equation 4. Then, we combine the input I and the residual
image Rθ to obtain the output for ﬁrst iteration. Lastly, we
apply our localized similarity loss from Equation 5.

For the gradient loss, we utilize the Sobel Operator with
convolution kernel size of 3-by-3 to approximate the image
intensity gradients. We calculate image gradients on both
horizontal x-direction and vertical y-direction. The gradi-
ent net processes image through RGB three channels for
two directions. We take the Euclidean norm by stacking the
6 gradients together as a complete gradient for one image.
This allows for efﬁcient batch optimization.

Figure 1. Our model uses gradient to simulate perceptual similar-
ity. We optimize every pixel in the residual image, by matching
gradients with the target image.

where α is a property of the device under our optical com-
biner assumption, and α controls the effect of the input im-
age. To ensure physical feasibility, we clip the residual im-
age into the desired range.

Our loss is comprised of the two desiderata for our im-
age residual: the physical feasibility constraint loss of the
residual Lconst, and the perceptual similarity loss between
the target image and the ﬁnal output Lgrad,

L(θ, I, P ) = Lconst(Rθ, 0, 1 − α) +

Lgrad(clip{αI + (1 − α)Rθ}, P ).

(3)

We employ a soft constraint loss to generate residuals

that are as close to being physically feasible as possible.

Lconst(R, a, b) =

(cid:88)

i,j

| max(min(Ri,j, b), a) − Ri,j|,

(4)
where R is the residual in our problem setting, and a = 0
and b = 1 are our minimum and maximum feasibility
bounds. This way, we allow our model to make some mis-
takes by making the residual Rθ not completely physically
realizable at all pixels, as long as the error between the ac-
tual output and the target output is relatively small.

Our primary contribution lies in our perceptual similarity
loss. Prior works [9] made the assumption that perceptual
similarity occurred at a global level, and introduce a similar-
ity loss that globally normalizes the images and then takes
the Euclidean distance between them. This simulates the
perceptual white-balancing. Furthermore, to simplify opti-
mization, the StayPositive method used two global param-
eters on the entire image. This limits the range of images
that can be expressed. In addition, human perceptual sys-
tems are not global across large regions, consisting of blind
spots and sharper foveal vision. Our method utilizes the
entire residual as individual optimizable parameters, thus
increasing the ﬂexibility of the ﬁnal output image.

In order to match localized perceptual similarity, our
method focuses on high-frequency detail losses by match-

CLIPβInputProposalαCLIPβOptical See-ThroughProposal GradientOutput GradientResidual3. Experiments

Methods

PSNR(↑)

SSIM(↑) LPIPS(↓)

We validate our optimization method on the Day2Night
[2] images to test the robustness to bright and darkness
changes; we also ran experiments on large scale, high reso-
lution style transfer images [2] to test the ﬁne-grained detail
performances. We compared our method to heuristic base-
line and the existing method StayPositive. However, since
the StayPositive model optimized the residual image on two
parameters (StayPositive-2parameters), while we optimized
our residual image on all pixels, we extended the StayPos-
itive method to an all pixels optimization (StayPositive-all
pixels) to fairly compare the methods. Our method is able to
out-perform both baselines, StayPositive-2parameters and
StayPositive-all pixels. It demonstrates better performance
in preserving dark and bright contrast in high dynamic range
images, and ﬁner details in large scale, high resolution im-
ages. We used Peak signal to noise ratio (PSNR), Structural
Index Similarity (SSIM) score [16], and Learned Perceptual
Image Patch Similarity (LPIPS) [17] to measure the perfor-
mance.

3.1. Performance on High Dynamic Range Images

We believe by optimizing the gradient loss, we would be
able to capture the relative pixels changes from the neigh-
borhood, therefore capturing the contrast of darkness and
brightness and project the gradient changes into pixel value
movement. During experiments, we used the Day2Night
image dataset to simulate the challenge of projecting light
to bright (high pixel-intensity value) images. We compare
our results with the heuristic baseline and the StayPositive-
allpixels method. By matching gradients, the model takes
advantages of human optical illusion of insensitivity to
global lightness changes. Therefore even minor changes
between dark and bright pixel values in very similar neigh-
borhoods will be enlarged by human perceptual systems,
regardless of the overall lightness. From 2 ﬁrst row, we can
see that even though our method has an overall brighter ef-
fect compared to the target image, but human eyes will be
deceived by the seemingly minor differences surrounded by
neighborhood, and will perceive that the sky is dark and the
scene is from nighttime. We compared the model perfor-
mances using three metrics over averages on small patches
of size 150 × 150. Our model has the highest PSNR score
and lowest LPIPS score (perceptual loss), indicating that
our method preserves the bright dark contrast well.

3.2. Performance on Large Scale Image Details

Contrasting the StayPositive method of optimizing only
two parameters for the residual images, we optimized each
single pixels of the residual images. Therefore our method
is able to preserve the local features and neighborhood gra-
dient changes, exhibiting high-deﬁnition details especially

Heuristic Baseline
StayPositive
Our Method

0.641
14.9
0.2704
17.6
0.133
18.6
Table 1. This is the table for the high dynamic range images.

0.542
0.809
0.6612

Methods

PSNR(↑)

SSIM(↑) LPIPS(↓)

Heuristic Baseline
StayPositive
Our Method

27.64
27.5
27.71

0.4662
0.7075
0.699

0.453
0.209
0.100

Table 2. This is the metrics table for the large scale last supper
image. Metrics are computed on patches of 100×100 pixels.

in large scale images. Furthermore, by optimizing the gra-
dient loss, our method is able to preserve the clear transition
of edges, therefore demonstrating high frequency details es-
pecially in large images. During experiments, we used large
scale, high resolution images such as Day2Night 2 images
and style transfer 3 images, which are over 4000×1000 pix-
els. We compare our results with the StayPositive-allpixels
method.

From Figure 3, we can see that our method shows ﬁner
and clearer details in the large scale image, with relative
brightness and contrast preserved as well. The result is
calculated by averaging metric on smaller patches because
we want to compare similarity between local patches. Our
method shows better results in all three metrics.

4. Conclusion

In this work, we explored the non-negative image gener-
ation problem and proposed an effective non-deep learning
image synthesis method, which can be combined with in-
ﬁeld image synthesis method and used in AR settings. It
has the advantages in presenting ﬁne details and preserv-
ing brightness contrast in large scale high resolution images
and high dynamic range images. It achieve consistently bet-
ter results than the baselines and existing methods. It can
be used in interactive settings, to extend the visual enjoy-
ment brought by augmented reality. However it also have
some limitations. On small images, our method doesn’t
have clearly better performance. Under this setting, it may
be more optimal to use a method that optimizes fewer pa-
rameters in a global way. Furthermore, our framework as-
sumes a highly simplistic optical combination framework.
Future studies would need to look into how to more accu-
rately model light combination from a projector with the
real world under the OST setting.

(f) Input

(g) Target

(h) Heuristic

(i) StayPositive

(j) Ours

Figure 2. Our model is able to preserve the brightness contrast on high dynamic images with gradient optimization.

Figure 3. Fine details comparison in large scale image (last supper).

[16] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing,
13(4):600–612, 2004.

[17] Richard Zhang, Phillip Isola, Alexei A. Efros, E. Shechtman,
and O. Wang. The unreasonable effectiveness of deep fea-
tures as a perceptual metric. 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 586–595,
2018.

References

[1] Edward H. Adelson. 24 lightness perception and lightness

illusions.

[2] Ivan Anokhin, Pavel Solovev, Denis Korzhenkov, Alexey
Kharlamov, Taras Khakhulin, Alexey Silvestrov, Sergey
Nikolenko, Victor Lempitsky, and Gleb Sterkin. High-
resolution daytime translation without domain labels. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2020.

[3] David Corney and R. Beau Lotto. What are lightness illu-
sions and why do we see them? PLOS Computational Biol-
ogy, 3(9):1–11, 09 2007.

[4] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-
In
age style transfer using convolutional neural networks.
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2414–2423, 2016.

[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
In Advances
Yoshua Bengio. Generative adversarial nets.
in neural information processing systems, pages 2672–2680,
2014.

[6] Bingyao Huang and Haibin Ling. Deprocams: Simultane-
ous relighting, compensation and shape reconstruction for
projector-camera systems. IEEE Transactions on Visualiza-
tion and Computer Graphics, 27(5):2725–2735, May 2021.
[7] Xun Huang and Serge Belongie. Arbitrary style transfer in
In ICCV,

real-time with adaptive instance normalization.
2017.

[8] Edwin H. Land and John J. McCann. Lightness and retinex

theory. J. Opt. Soc. Am., 61(1):1–11, Jan 1971.

[9] Katie Luo, Guandao Yang, Wenqi Xian, Harald Haralds-
son, Bharath Hariharan, and Serge Belongie. Stay positive:
Non-negative image synthesis for augmented reality. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 10050–10060, June
2021.

[10] Daniele Marini and Alessandro Rizzi. A computational ap-
proach to color illusions. In Alberto Del Bimbo, editor, Im-
age Analysis and Processing, pages 62–69, Berlin, Heidel-
berg, 1997. Springer Berlin Heidelberg.

[11] Daniele Marini, Alessandro Rizzi, and Maurizio Rossi.
Color constancy measurements for synthetic image genera-
tion. Journal of Electronic Imaging, 8(4):394 – 403, 1999.

[12] D. Marini, M. Rossi, L. Moltedo, and O. Salvetti. Virtual
reality and web tools to convey the visual information of an-
cient monuments. Computer Networks and ISDN Systems,
29(14):1655–1660, 1997. Visualization and Graphics on the
World Wide Web.

[13] P. P´erez, M. Gangnet, and A. Blake. Poisson image editing.

ACM SIGGRAPH 2003 Papers, 2003.

[14] Borja Jaume P´erez. Augmented illusionism. the inﬂuence
of optical illusions through artworks with augmented reality.
In 2020 IEEE International Symposium on Mixed and Aug-
mented Reality Adjunct (ISMAR-Adjunct), pages 158–164,
2020.

[15] Irwin Sobel. An isotropic 3x3 image gradient operator. Pre-

sentation at Stanford A.I. Project 1968, 02 2014.

