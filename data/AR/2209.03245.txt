THE HOLOLENS IN MEDICINE: A SYSTEMATIC REVIEW AND
TAXONOMY

2
2
0
2

p
e
S
6

]

C
H
.
s
c
[

1
v
5
4
2
3
0
.
9
0
2
2
:
v
i
X
r
a

Christina Gsaxner
Institute of Computer Graphics and Vision
Graz University of Technology
8010 Graz, Austria
gsaxner@tugraz.at

Jianning Li
Institute of AI in Medicine
University Medicine Essen
45131 Essen, Germany

Antonio Pepe
Institute of Computer Graphics and Vision
Graz University of Technology
8010 Graz, Austria

Yuan Jin
Institute of Computer Graphics and Vision
Graz University of Technology
8010 Graz, Austria

Jens Kleesiek
Institute of AI in Medicine
University Medicine Essen
45131 Essen, Germany

Dieter Schmalstieg
Institute of Computer Graphics and Vision
Graz University of Technology
8010 Graz, Austria

Jan Egger
Institute of AI in Medicine
University Medicine Essen
45131 Essen, Germany

September 8, 2022

ABSTRACT

The HoloLens (Microsoft Corp., Redmond, WA), a head-worn, optically see-through augmented
reality display, is the main player in the recent boost in medical augmented reality research. In
medical settings, the HoloLens enables the physician to obtain immediate insight into patient in-
formation, directly overlaid with their view of the clinical scenario, the medical student to gain a
better understanding of complex anatomies or procedures, and even the patient to execute therapeutic
tasks with improved, immersive guidance. In this systematic review, we provide a comprehensive
overview of the usage of the ﬁrst-generation HoloLens within the medical domain, from its release in
March 2016, until the year of 2021, were attention is shifting towards it’s successor, the HoloLens
2. We identiﬁed 171 relevant publications through a systematic search of the PubMed and Scopus
databases. We analyze these publications in regard to their intended use case, technical methodology
for registration and tracking, data sources, visualization as well as validation and evaluation. We ﬁnd
that, although the feasibility of using the HoloLens in various medical scenarios has been shown,
increased efforts in the areas of precision, reliability, usability, workﬂow and perception are necessary
to establish AR in clinical practice.

Keywords HoloLens · Medicine · Review · Taxonomy · Augmented Reality · Healthcare · Surgery

 
 
 
 
 
 
A PREPRINT - SEPTEMBER 8, 2022

1

Introduction

Augmented Reality (AR) enhances the real world with
virtual content by interposing computer graphics between
the human eye and its ﬁeld of vision. Recent consumer-
oriented developments made AR devices accessible to the
general public. As a result, the AR ﬁeld saw a strong
growth in various domains, such as industry and enter-
tainment. A main player in this new development was
the HoloLens (Microsoft Corp., Redmond, WA), released
in 2016. The HoloLens was originally marketed for ap-
plications in gaming, communication and 3D modeling;
nevertheless, it quickly drew the attention from the medical
domain. This development is unsurprising – after all, one
can hardly imagine a professional domain in which AR
could have a more signiﬁcant impact than in medicine. AR
has the potential to grant physicians "X-Ray vision" – the
ability to see critical structures within the patient, without
making a single incision. Wearable devices, such as the
HoloLens, can make critical patient information perma-
nently and readily available, and show them directly in the
vision of the physicians. This approach allows them to
keep their focus on the patient only and makes surgeries
cluttered with monitors obsolete. Immersing remote ex-
perts into the mixed reality environment would further per-
mit more and more patients to beneﬁt from their expertise.
Patients could be monitored and guided through various
treatment and rehabilitation stages using AR, be it within
the clinic or in their own homes, while medical students
could practice critical interventions in a safe, virtually en-
hanced setting or immerse themselves in 3D anatomy. The
HoloLens 1, as the ﬁrst wearable, fully untethered AR
device, was certainly an important step towards the future
of AR in medicine. But how much could it contribute, and
how far are we in making the aforementioned scenarios a
reality?

In this systematic review, we provide a comprehensive
overview of works that reported the usage of the ﬁrst-
generation HoloLens in the medical domain from 2016 to
2021. We identiﬁed 171 relevant publications through a
systematic search of the PubMed and Scopus databases and
analyzed them according to their intended use case, tech-
nical methodology concerning registration and tracking,
data sources and data visualization, as well as evaluation
and validation. Throughout our review, we highlight prin-
cipal ﬁndings, identify gaps and discuss challenges and
limitations. With the recent availability of its successor,
the HoloLens 2, this review outlines the impact the ﬁrst
generation HoloLens had during its lifetime in the medical
area.

2 Background

2.1 Augmented reality

One of the most common deﬁnitions of AR stems from
the virtuality continuum deﬁnition by Milgram et al. [137],
who describe AR as a mixed reality (MR), which con-

tains mainly real elements, enhanced with virtual content.
Azuma et al. [11] further characterize AR environments
as combining reality and virtuality by registration in 3D,
while being interactive in real-time. Although this deﬁ-
nition makes clear that AR can appeal to all senses, it is
mostly concerned with visual data. In the medical ﬁeld,
where digital imaging techniques provide rich information,
AR has huge potential. Unsurprisingly, once technology
was advanced enough to consider real-world AR appli-
cations, it quickly drew the attention from the medical
domain.

AR displays. The ﬁrst medical AR systems were intro-
duced as early as the late 1980s, with Roberts et al. [172]
describing the ﬁrst system, an operating microscope aug-
mented with segmented computed tomography (CT) im-
ages. A head-mounted displays (HMD) continued to be a
popular display choice in early medical AR systems, as,
for example, demonstrated by the works in the 1990s [195,
60] and in the early 2000 [182], who developed a video
see-through HMD for medical applications. An HMD is
a natural choice for medicine, as they intuitively align the
head gaze of the wearer with the viewpoint of the content,
and keep the hands of the wearer free at the same time.
However, early HMD designs could not easily fulﬁll the
high demands of medical AR systems in terms of perfor-
mance, latency and accuracy, which require powerful com-
putational infrastructure. Usually, this challenge resulted
in bulky form factors, with wired connections between
HMD and more capable computing and tracking infras-
tructure, making these systems difﬁcult to implement in
real clinical scenarios. Although head-worn microscopes,
optical and video see-through displays continued to be rel-
evant, in the years between 2011 and 2017, we see a shift
towards world-localized displays, such as stationary moni-
tors or projector systems [49]. The release of the HoloLens
1, which was the ﬁrst self-contained AR-HMD with a slim
form factor, subsequently caused research attention to shift
towards optical see-through (OST) displays again [75].

Registration and tracking. Alignment between reality
and virtuality is a fundamental concept of AR, which is
realized via registration. In a medical context, registration
is mostly desired between medical data, often volumetric
imaging such as CT or magnetic resonance imaging (MRI),
and the patient. To maintain registration and synchroniza-
tion of the viewpoint in the user’s perspective, the position
and orientation of the AR viewing camera with respect to
the environment need to be tracked.

For tracking and registration, two fundamental paradigms
can be distinguished: outside-in and inside-out approaches.
Outside-in (or extrinsic) tracking refers to strategies where
external sensors (e.g., cameras) are stationed around the
user and thus, observe the movement of the device from
the outside. Such methods can be very accurate, but re-
quire many components and only work in a limited space.
In inside-out (or intrinsic) tracking, the sensors are inte-
grated within the AR device itself and thus, the device

2

A PREPRINT - SEPTEMBER 8, 2022

can self-locate within an unprepared environment. Al-
though diverse types of sensors can be used for tracking,
vision-based methods, relying on visible light, infrared
(IR) cameras or depth sensors, have dominated the ﬁeld
for many years [223]. For vision-based tracking, observ-
able features need to be visible to the tracking cameras.
Typically, these features can be divided into artiﬁcial fea-
tures for marker-based tracking, and natural features for
marker-less tracking.

Marker-based tracking relies on indicators of pre-deﬁned
pattern and size, whose location in relation to the real
world is precisely known. These indicators can, for exam-
ple, be ﬁducial markers visible by standard RGB cameras,
or IR emitters (either active or passive), which are more
robust to variable lighting conditions. Medical technol-
ogy has appropriated this principle years ago: IR emitting
markers are well-established in surgical navigation sys-
tems, where they are anchored in rigid tissue, such as the
patients’ bones, and on surgical instruments, while being
tracked with stereo IR cameras. This approach allows a
computation of the relative position of tools in relation to
critical anatomy.

Marker-less systems do not require artiﬁcial objects and, in-
stead, rely on naturally observable features. Simultaneous
localization and mapping (SLAM) [48] and its variants are
the most common markerless tracking techniques, which
are capable of fusing information from various sensors
(e.g., visible light, depth, GPS) to build a map of the envi-
ronment and tracking the device within it. Virtual content
can then be placed manually or with the aid of markers
into the mapped world. Other marker-less tracking ap-
proaches involve models or templates of known, stationary
real-world objects, which are then ﬁtted to their real coun-
terparts, either through 2D-3D (in case of visible light
cameras) or 3D-3D (if 3D information of the scene is avail-
able) matching. Since 3D models of the patient’s skin
surface are typically available from medical imaging, such
methods are well-suited for medical applications.

2.2 The HoloLens

The ﬁrst generation HoloLens is wearable computer glass
(often also referred to as "smartglass"), which delivers
augmented reality experiences through a 3D optical see-
through head-mounted display (OST-HMD). It was de-
veloped by Microsoft and rolled out in 2016. Contrary
to other mixed or virtual reality headsets, the HoloLens
was the ﬁrst one to work fully untethered, requiring no
wired connections to stationary infrastructure or prepared
environments [222].

The HoloLens features a set of built-in sensors, including
an inertial measurement unit (IMU), four side-facing visi-
ble light cameras for capturing the environment, a time-of-
ﬂight (ToF) depth sensor, an ambient light sensor, four mi-
crophones and a front-facing, high deﬁnition photo/video
camera. Only microphone and photo/video camera were
accessible to developers in the beginning. In mid 2018,

however, the so-called Research Mode enabled access to
ToF and environmental understanding cameras for research
purposes [53]. Stereoscopic virtual content is displayed on
two semi-transparent combiner lenses in front of the user’s
eyes for 3D vision, combined with the real environment.
The equivalent of two 720p displays, one in front of each
eye, allows a diagonal ﬁeld of view (FOV) of 34 degrees,
with a resolution of 47 pixels per degree [73]. Sound is
delivered via built-in speakers. The HoloLens is equipped
with an Intel Atom x5 32-bit central processing unit (CPU)
with 1 GB of random access memory (RAM), and has 64
GB of storage. Its active battery life is speciﬁed at 2-3
hours.

A custom, dedicated hardware accelerator, the Holographic
Processing Unit with 1 GB of additional RAM, enables ef-
ﬁcient processing of the sensor data in parallel to processes
running on the HoloLens’ CPU. This custom chip facili-
tates a set of on-board capabilities to understand the users
actions, as well as the environment around the device. A
SLAM algorithm continuously constructs and reﬁnes a spa-
tial map of the environment, and locates the device within
it, resulting in on-board, marker-less inside-out tracking
of the HoloLens [102]. Gaze tracking is supported via
analyzing the user’s head movement. Users can interact
with virtual content via hand gestures or voice commands,
both of which are automatically recognized. Additional
input devices can be connected to the device via Bluetooth
4.1 LE, for example, the included clicker, a gamepad or an
external keyboard. Connections can further be established
wireless via Wi-Fi 802.11ac, or wired via Micro USB 2.0.

3 Methodology

3.1 Search strategy and selection process

We conducted a systematic review of existing research
about the HoloLens applied in medical scenarios. The
review followed the Preferred Reporting Items on System-
atic Reviews and Meta-Analysis (PRISMA) guidelines
by Moher et al. [140]. A systematic literature search
in the databases PubMed and Scopus was performed
for the keyword [hololens], together with any of the
terms [medicine], [surgery] or [healthcare] in March
2022. The publication period was restricted to the years
2016 to 2021. Duplicates were removed, then, an initial
screening of titles and abstracts was performed. After the
initial screening, full texts were retrieved and reviewed
for eligibility. Criteria for inclusion in both phases of
screening were: 1) studies with English full texts, 2)
studies describing full original research by the authors, 3)
studies which have been peer reviewed, and 4) studies
describing the application of the HoloLens primarily for a
human medical purpose. Consequently, exclusion criteria
were: 1) studies without English full texts, 2) studies not
describing full original research, such as reviews or book
chapters 3) studies which have not been peer-reviewed,
for example conference posters/abstracts or commentaries,
4) studies which do not use the HoloLens as main AR

3

Records identiﬁed through
database searching
(975)

Additional records identi-
ﬁed through other sources
(18)

Records screened
(993)

Records after du-
plicates removed
(949)

Records excluded, based
on exclusion criteria
(718)

A PREPRINT - SEPTEMBER 8, 2022

based on application areas. From every publication, we
also extracted information about applied registration and
tracking methodologies, if any (see section 7), where we
ﬁrst categorized studies based on their tracking paradigm
(manual vs. inside-out vs. outside-in), and further distin-
guished between marker-based and marker-less methods.
Data and visualization techniques are reviewed in section 8,
where we deﬁne categories based on data source (medical
vs. non-medical), data type (2D, 3D and other), as well
as acquisition time. Finally, we analyzed how medical
AR applications using the HoloLens have been evaluated,
grouping studies according to their evaluation scenarios,
and identiﬁed commonly used qualitative and quantitative
measures in section 9.

Full-text articles ac-
cessed for eligibility
(231)

Full-text articles excluded,
based on exclusion criteria
(60)

3.3 Related reviews

Studies included
(171)

Figure 1: Search strategy used in this systematic review.
Adapted from the PRISMA ﬂow diagram by Moher et
al. [140].

device, but only mention it, and 5) studies which are not
primarily focused on a human medical purpose, but on
other applications such as industry or gaming, and only
mention medicine as a possible ﬁeld of application.

The systematic electronic search resulted in a total of 975
records. 18 additional records previously known to the
authors were also considered. After removal of duplicates
and screening of titles, abstracts and full texts according
to our inclusion criteria, 171 studies were selected for the
ﬁnal analysis (see Figure 1).

3.2 Data extraction and taxonomy

Each study was reviewed by one author. We extracted infor-
mation about authors, year of publication and medical spe-
ciality from every publication. Medical specialities were
determined as stated by the authors, by publication venue
or targeted anatomy and grouped, were applicable, e.g.,
cranial and facial sub-specialities were combined as cranio-
maxillofacial. Then, we extracted information about every
publication according to our taxonomy, seen in Figure 2.
In section 6, we classiﬁed each study by the main intended
user of the HoloLens: 1) clinical systems, whose main
purpose is the support of physicians and healthcare pro-
fessionals in the clinical routine, 2) Educational works,
which aid medical and healthcare students in their school-
ing and training, and 3) applications focused on treatment
and rehabilitation, which aim at supporting patients during
different stages of therapy and disease management. Fur-
ther, we divided each main category into sub-categories,

According to our exclusion criteria, review publications
are not analyzed in this study. Still, we identiﬁed several
related reviews during our literature search, which might
be of interest for the reader.

Barsom et al. [15] provide a systematic review about
AR for medical training to the year of 2015, and found
that, although promising results were achieved, full vali-
dation of training systems was lacking. Chen et al. [34]
analyze trends and challenges in medical AR found in
over 1400 publications in the time period between 1995
and 2015. They identify powerful enabling technologies,
human-computer-interaction and validation as major re-
search challenges. Eckert et al. [49] review medical AR
applications described between the years of 2012 and 2017.
In these years, a trend towards display technology research
and medical treatment scenarios could be identiﬁed. Still,
a lack of evidence in clinical studies was noted.

Several reviews about AR, speciﬁcally for surgical applica-
tions, have been published. Vavra et al. [208] and Yoon et
al. [221] review articles published pre-HoloLens, between
2010 and 2016, as well as 1995 and 2017, respectively. In
this time period, live streaming from endoscopy, followed
by navigation and video recording, were the most popu-
lar applications. Rahman et al. [170] focus speciﬁcally
on HMD use in surgical scenarios up to the year of 2017.
More recent reviews about surgical AR using OST-HMD
come from Birlo et al. [17] and Doughty et al. [47] for
the years between 2013 and 2020, and 2021 to March
2022, respectively. They clearly show that the Microsoft
HoloLens was the major driving force in OST-HMD re-
search for surgery in the past years. Even more special-
ized surgical reviews have been published for orthopedic
surgery [98], oral and cranio-maxillofacial surgery [12, 75],
neurosurgery [134, 81, 126], laparoscopic surgery [16] and
robotic surgery [165].

In all these reviews, the lack of clinical validation is the
most re-occurring aspect, something we also identify in
this study. Other commonly mentioned challenges include
technical limitations in regards to device tracking and ren-
dering, and limited usability due to complicated workﬂows.

4

A PREPRINT - SEPTEMBER 8, 2022

HoloLens 1
in medicine

Registration
& Tracking

Data
& Visualization

Evaluation

Use case

Intended
users

Physicians

Students

Patients

Paradigm

Manual

Inside-out

Outside-in

Applications

Method

Data display

Marker-based

Marker-less

Image-guided inter-
ventions

Surgical navigation

Intervention training

Anatomy learning

Patient education & training

Assistance & monitoring

Diagnosis & treatment

Data
source

Medical

Scenario

Proof-of-concept

Non-medical

Laboratory

Relevant environ-
ment

Measures

Quantitative

Qualitative

Data type

3D

2D

Other

Acquisition
time

Pre-interventional

Intra-interventional

Figure 2: Taxonomy employed in this review. Each publication is analyzed with regard to intended use case, registration
and tracking principles, data sources and visualization, as well as evaluation and validation.

The HoloLens, with its self-tracking capabilities, good sup-
port for the development of user interfaces and interactions
and improved rendering capabilities, makes some of these
challenges obsolete. Therefore, in this review, we focus
exclusively on aspects and challenges coming with this
new generation of OST-HMD devices, which still bear sig-
niﬁcance for more recent hardware, such as the HoloLens
2 or Magic Leap 2 (Magic Leap, Plantation, FL). Thus, we
hope that it is interesting for not only looking back, but
in particular also for pointing future researchers towards
directions in which increased efforts are required.

4 Publications per year

Figure 3 provides an overview of the number of papers pub-
lished in each reviewed year, from 2016 to 2021. Although
the HoloLens was available from March 2016 in North

America and October 2016 worldwide, no publications
reporting it’s use in the medical domain were published
in this year. After that, the number of publications in all
categories shows a steady increase, with the highest num-
ber of research reported in 2020. In 2021, the number of
papers decreases again – likely caused by the release of
the HoloLens 2, which lead many researchers to shift their
attention towards the newer generation device.

5 Medical ﬁelds of applications

As shown in Figure 4, the HoloLens saw applications in
a large variety of medical areas, which we group into
21 ﬁelds. Surgical disciplines, in particular orthopedic
surgery (18) and neurosurgery (14), were most frequently
supported by AR applications, especially those targeted
at physicians. Interestingly, in these most frequent dis-

5

A PREPRINT - SEPTEMBER 8, 2022

Figure 4: Frequency of papers in each of the 21 identiﬁed
medical ﬁelds. "Nonspeciﬁc" refers to applications where
authors did not indicate a speciﬁc area, which means they
could be used in several disciplines.

Figure 3: Number of papers published per year in each use
case category between the years 2016 and 2021.

ciplines, image-guided and navigated interventions are
already particularly common, e.g., through surgical navi-
gation systems or ﬂuoroscopy. Hence, it can be assumed
that, from the perspective of user acceptance and recogni-
tion, the translation of AR technology into clinical practice
can be more successful in areas which already heavily
rely on such technological assistance. However, relevant
procedures have also highest demands in accuracy and
safety, which makes the implementation of AR much more
difﬁcult from a technical standpoint. 15 publications do
not indicate a speciﬁc medical ﬁeld, and eleven target sur-
gical procedures in general. These publications mostly
introduce more general concepts not targeted at speciﬁc
medical procedures – thus, they could be used in more
than one specialty. Patient-focused applications are rather
situated in speciality areas, where patient cooperation and
motivation has a large impact on treatment outcome, such
as neurology and kinesiology.

6 Use cases

We ﬁrst categorize publications by their intended users, and
further by the supported application. An overview of the
identiﬁed categories and number of associated publications
is given in Figure 5. Physicians and healthcare profession-
als working within the clinical routine have been, by far,
the most popular target audience of proposed HoloLens-
based AR systems. 128 out of 171 studies, almost 75%,
describe an application of the device for supporting health-
care professionals in tasks such as diagnosis, treatment
planning and treatment execution. Medical students come
second, with 24 works dedicated to anatomy learning or
training of interventional procedures. Lastly, 19 studies
targeted an application for patients, either for patient edu-
cation, monitoring and guidance, or diagnosis.

Figure 5: Overview of the number of papers identiﬁed
in each of our three main categories (deﬁned by targeted
users) and sub-categories (deﬁned by application area).

6.1 Physician-centered applications of the HoloLens

We group research within this category based on appli-
cation, ranked by technological complexity: 1) Data vi-
sualization applications, where the HoloLens primarily
serves as a display, are relatively simple to implement. 2)
Image-guided interventions demand either a registration
between virtual content and the patient or a way to display
intra-operative imaging in real-time and are, consequently,

6

more challenging. 3) Surgical navigation applications re-
quire tracking of medical tools in addition to the patient
and the HoloLens, and have the highest demands in accu-
racy and reliability, which makes them the most complex.
Table 1 shows all studies targeted at physicians and other
healthcare professionals, including their applications.

6.1.1 Data display

In its simplest form, the HoloLens can be used as an immer-
sive display for medical data, such as 2D/3D imaging or
healthcare records (see Figure 6 (a) and (b)). Pure data dis-
play applications do not need to establish a correspondence
between the physical space and the shown data – content
can simply be anchored to a ﬁxed position according to the
display itself, to be always visible for the wearer. Since
the HoloLens self-locates within its environment, virtual
objects can further be anchored to a stationary position
within the real world without additional expenditure, to
be naturally examined from different perspectives. This
ability can have several advantages for clinicians. Access
to medical data can be detached from stationary infrastruc-
ture and brought to treatment rooms, operating theaters and
the bedside of the patient. For 3D data, such as volumetric
medical imaging, stereoscopic visualization through the
HoloLens may lead to an improved perception of 3D rela-
tions. Furthermore, the possibility of touch-less interaction
with data is ideal for scenarios where sterility is important.
Finally, by synchronizing several headsets, visualizations
may be more easily shared between users. These factors
could make inspection of interaction with medical data dur-
ing diagnosis, intervention planning and procedures more
intuitive and less cumbersome. We identiﬁed 33 publica-
tions in this category. Most of them describe workﬂows
for visualizing pre-interventionally acquired, 3D volumet-
ric imaging data, such as CT, MRI or positron emission
tomography (PET), but also healthcare records and other
documents.

A smaller group of works explores telemedicine, where re-
mote monitoring and assistance are important concepts. A
remote expert can assist local staff in carrying out crit-
ical interventions, which is particularly useful in rural
or disadvantaged areas, with limited funding and staff.
The HoloLens features video conferencing capabilities,
which enable the real-time transmission and visualiza-
tion of the viewpoint of an interventionist to a remote ex-
pert/observers, and, vice versa, expert guidance via voice,
video or annotations, without having to look away from the
patient or using an external computer. Sirilak et al. [190]
developed an e-consulting platform to connect specialized
physicians with rural and remote hospitals. The feasibility
of video and voice communication during intervention or
surgery has further been explored by Mitsuno et al. [139]
and Glick et al. [71]. Proniewska et al. [161] developed a
strategy for digitizing the operating room, allowing tele-
monitoring from different perspectives with the HoloLens.

7

A PREPRINT - SEPTEMBER 8, 2022

6.1.2

Image-guided interventions

The majority of papers reviewed in this study describe an
application in image-guided intervention (IGI). AR for IGI
is mainly motivated by the desire to obtain X-Ray vision of
a patient, which can incorporate medical imaging data in-
tuitively into interventional workﬂows by aligning patient
anatomy, imaging data and the physician’s viewpoint. AR
technology can superimpose pre- or intra-operative images
and planning data directly with the patient, allowing the
physician to see target structures through skin or obstruc-
tive anatomy, as seen in Figure 6 (c). It can, thus, either
replace traditional image guidance, or provide guidance
for interventions usually performed without.

Especially minimally-invasive interventions, which are
performed without gaining direct access to the underlying
anatomy, can beneﬁt from X-ray visualization. Examples
include skull base surgery [132, 99, 39], arthroplasty [2,
213], percutaneous orthopedic screw placement [69, 120,
25, 45], ventricular drain insertion [118, 169, 89, 186] or
ablations [52, 38].

But X-ray visualization with the HoloLens has also been
applied for procedures where the target anatomy is sur-
gically exposed, such as tumor removal [158, 91, 175,
192, 88, 180, 94, 184, 185, 74], vessel surgery [160, 100,
215], or cranio-maxillofacial surgeries [105, 197, 133].
In these scenarios, the visualization of critical anatomical
structures, which are not directly or clearly visible on the
surgical site, such as blood vessels and nerves, or impor-
tant planning information; for example, tumor resection
margins or osteotomy lines, have the potential to make
interventions safer.

For a convincing X-Ray visualization, an accurate overlay
of imaging data with the patient is a prerequisite. Image-
to-patient-registration, relating virtual content with target
anatomy, is the key component for such a system, but
other factors, such as display calibration and stability of
the HoloLens self-tracking also play an important role.
While many of the aforementioned works rely on a man-
ual alignment of virtual content with the patient, several
publications within this category focus on addressing these
technical challenges. Mostly, they do not focus on speciﬁc
medical applications, but develop new concepts for system
calibration [6, 82, 57] or image-to-patient-registration [217,
35, 156, 201, 77], which could be applied in various medi-
cal scenarios. Other works evaluate and compare selected
technical aspects [58, 138, 205, 79, 157]. We will discuss
image-to-patient registration and calibration methods in
more detail in section 7

A third category of works does not target X-ray visual-
ization – instead, the HoloLens is used to enhance tradi-
tional image-guided interventions, such as laparoscopy,
endoscopy, ﬂuoroscopy or ultrasound. It has been shown
that monitor placement during image-guided procedures
plays an important role – a misalignment of the visual-
motor axis can increase fatigue and decrease orientation
and hand-eye coordination of the operator, and, conse-

A PREPRINT - SEPTEMBER 8, 2022

(a)

(b)

(c)

(d)

(e)

Figure 6: Examples for physician-centered applications of the HoloLens. Left: Data display examples, showing
immersive (a) 2D (Source: Galati et al. [62], CC-BY) and (b) 3D patient data (Source: Gehrsitz et al. [68], CC-BY),
without a reference to the physical space. Middle: Image-guided intervention via (c) X-ray vision (Source: Pratt
et al. [160], CC-BY) and (d) live imaging (Source: Al-Janabi et al. [3], CC-BY). Right: (e) example for a surgical
navigation application, were tools are tracked in relation to the target anatomy (Source: Kriechling et al. [107], CC-BY)

displayed in situ, fused with the target anatomy, as shown
in Figure 6 (e).

Surgical navigation with the HoloLens has been explored
as an alternative to commercial SNS in 27 publications.
Mostly, AR navigation was studied in procedures where
conventional SNS are already gold standard, such as neu-
rosurgery [29, 112, 206, 207], orthopedic (in particular,
spinal) surgery [50, 41, 119, 107, 147, 194], general
surgery [135] or cranio-maxillofacial surgery [63, 198,
70]. AR SN can also provide an X-ray free alternative to
interventions typically guided by intra-operative imaging,
such as endovascular procedures [109, 64, 121] or tis-
sue ablations [114, 117], or can be integrated into robotic
surgery [167, 164, 168].

These procedures have highest demands in accuracy and
reliability of registration and tracking, with a high ref-
erence precision in a millimeter or sub-millimeter range.
With the HoloLens hardware, it is difﬁcult to meet these
requirements. However, the HoloLens has a much slimmer
form factor than conventional image guidance systems,
which allows navigation for less critical procedures, usu-
ally performed without. Examples for such procedures
include brain stimulation treatment [115] and US examina-
tions [178, 150].

Instrument tracking methods with the HoloLens will be
reviewed in more detail in section 7.

quently, increase the risk of intervention-induced injuries
[51]. By anchoring the virtual 2D "monitor" to a con-
venient physical location or the head gaze of the user,
ergonomics and subjective workload may be improved.
These applications require methods to deliver live medical
data to the HoloLens in real-time. While most frameworks
could support a variety of imaging sources, studies speciﬁ-
cally evaluate intra-operative X-ray [43, 3], endoscopy [3],
ultrasound [30], electro-anatomic mapping [193] and MRI
[209]. An example is shown in Figure 6 (d).

6.1.3 Surgical navigation

Surgical navigation systems (SNS) have been shown to
make procedures more accurate, less invasive and faster,
resulting in improved outcomes for the patient [136]. Com-
pared to conventional image guidance using intra-operative
X-ray or CT, SNS do not burden operators and patients with
additional radiation exposure, and compared to ultrasound-
based guidance, they are more accurate and work for every
tissue type. Conventional SNS rely on visualizing navi-
gation information on separate monitors, which leads to a
switching focus problem for surgeons – they have to divide
their attention between the surgical site and the navigation
information. Such a division leads to issues of increased
workload, disorientation and deteriorated hand-eye coordi-
nation [84], which AR could alleviate by fusing navigation
information with the operating site. While IGI systems,
as described above, can already provide a basic guidance
based on images, precise surgical navigation requires real-
time tracking of medical instruments and tools in relation
to the patient anatomy, in addition to image-to-patient
registration. In AR, navigation information can then be

8

Table 1: Studies reporting an application of the HoloLens for physicians and other health care professionals.

Application

Focus

Studies

A PREPRINT - SEPTEMBER 8, 2022

Data display (33)

Medical Data
Visualization (29)

Morales Mojica et al. [143], Qian et al. [167], Sauer et al. [183], Bucioli
et al. [26], Fröhlich et al. [59], Jang et al. [95], Affolter et al. [1], Brun
et al. [23], Checcucci et al. [33], Kubben et al. [108], Moosburner et al.
[141], Soulami et al. [192], Talaat et al. [202], Witowski et al. [216],
Allison et al. [4], Bulliard et al. [27], Fitski et al. [55], Galati et al. [62],
Kumar et al. [111], Pelanis et al. [154], Perkins et al. [159], Cofano
et al. [36], Dennler et al. [44], Gehrsitz et al. [68], Iqbal et al. [93],
Morales Mojica et al. [142], Saito et al. [181], Velazco-Garcia et al.
[210], and Wake et al. [211]

Tele-medicine (5)

Sirilak et al. [190], Mitsuno et al. [139], Glick et al. [71], Proniewska
et al. [161], and Cofano et al. [36]

Image-guided
interventions (68)

X-ray vision:
clinical focus (38)

X-ray vision:
technical focus (24)

Perkins et al. [158], Agten et al. [2], Hanna et al. [83], Incekara et al.
[91], Li et al. [118], McJunkin et al. [132], Pratt et al. [160], Rae
et al. [169], Amini et al. [5], Gibby et al. [69], Huang et al. [89],
Kalavakonda et al. [99], Liu et al. [120], Lohou et al. [124], Rose
et al. [175], Wang et al. [213], Creighton et al. [39], Ferraguti et al.
[52], Huang et al. [88], Katayama et al. [100], Nuri et al. [151], Saito
et al. [180], Tian et al. [203], Buch et al. [25], Condino et al. [38],
Dennler et al. [44], Gouveia et al. [74], Iizuka et al. [90], Ivan et al.
[94], Koyachi et al. [105], Long et al. [125], Meng et al. [133], Qi et al.
[163], Scherl et al. [184, 185], Schneider et al. [186], Sugahara et al.
[197], and Wesselius et al. [215]

Xie et al. [218], Andress et al. [6], Frantz et al. [58], Hajek et al. [82],
Moreta-Martinez et al. [144], Pepe et al. [156], Wu et al. [217], Chien
et al. [35], Fotouhi et al. [57], Gsaxner et al. [77], Mitsuno et al. [138],
Pepe et al. [155], Sylos Labini et al. [201], Van Doormaal et al. [205],
Fischer et al. [54], Jiang et al. [97], Luzon et al. [128], Nguyen et al.
[149, 148], Zuo et al. [224], Castelan et al. [31], Gsaxner et al. [78],
Gu et al. [79], and Pérez-Pachón et al. [157]

Surgical navigation (27)

Live imaging (6)

Cui et al. [40], Deib et al. [43], Al Janabi et al. [3], Cartucho et al. [30],
Southworth et al. [193], and Velazco-Garcia et al. [209]

Kuhlemann et al. [109], Carbone et al. [29], El-Hariri et al. [50], García-
Vázquez et al. [64], Kuzhagaliyev et al. [114], Leuze et al. [115], Qian
et al. [164], De Oliveira et al. [41], Gao et al. [63], Li et al. [117],
Liebmann et al. [119], Liu et al. [121], Meulstee et al. [135], Qian et al.
[166], Kriechling et al. [107], Kunz et al. [112], Müller et al. [147],
Qian et al. [168], Rüger et al. [178], Sun et al. [198], Glas et al. [70],
Li et al. [116], Liu et al. [122], Nguyen et al. [150], Spirig et al. [194],
and Van Gestel et al. [206, 207]

6.2 Applications of the HoloLens for medical

students

While the HoloLens 1 was originally not intended as a
device for IGI or SN, its use as a tool for medical ed-
ucation was actively promoted. The CAE VimedixAR
(CAE Healthcare, Montreal, Canada) app, an AR ultra-
sound training simulator, was amongst the ﬁrst commer-
cial applications available for the HoloLens 1, and tools
for studying anatomy, such as HoloHuman by 3DMedi-
cal (Elsevier, Amsterdam, Netherlands) quickly followed.

9

Probably due to the availability of commercial solutions,
research in the area of medical education and training with
the HoloLens is not as numerous as one might expect. We
identiﬁed 24 publications in the area of HoloLens-based
medical and healthcare student support, which we further
categorize into: 1) Interventional and surgical training and
2) Anatomy learning. An overview is given in Table 2.

A PREPRINT - SEPTEMBER 8, 2022

(a)

(b)

Figure 7: Examples of HoloLens applications for medical students. (a): A hybrid simulator for training catheter
insertion with AR guidance (Source: Schoeb et al. [187], CC-BY). (b) Studying anatomy with the HoloLens (Source:
Ruthberg et al. [179]).

6.2.1 Interventional and surgical training

Simulation-based skill training has made its way into stan-
dard medical education, replacing or enhancing traditional
teaching and training methods [61]. Aside from traditional
simulators based on physical manikins, mixed reality tech-
nology has gained considerable popularity in this domain,
either by enabling fully virtual environments, or by enhanc-
ing manikin-based training through virtual guidance and
feedback [191]. 15 reviewed studies fall into this category.

The HoloLens 1 has been integrated into hybrid simulators,
where it can be used to display additional guidance or even
direct feedback to the user. Examples include the train-
ing of orthopedic surgery [32, 37], emergency medicine
interventions [103, 13, 86, 162], laparoscopic or US ex-
aminations [129, 171, 85] or urological procedures [146,
187]. Another possibility is to build fully simulated, virtual
training scenarios [32, 24] or to include remote experts
into the training sessions [214].

6.2.2 Anatomy learning

A meta-survey by Yammine et al. [220] has shown that
3D visualization techniques are preferable over traditional
methods for learning and teaching anatomy, both in terms
of factual and spatial knowledge. Contrary to such visual-
izations on conventional monitors or in virtual reality (VR),
AR could not only provide 3D visuals, but also annotate
real, physical models or cadavers with digital information.

Studies evaluating the use of the HoloLens to teach various
gross anatomy via 3D visualizations of anatomical models
have been reviewed [196, 130, 8, 72, 179, 145]. Robin-
son et al. [173] further tested the HoloLens as a learning
platform for studying microscopic anatomy.

6.3 Patient-focused applications of the HoloLens

19 publications describe HoloLens-based systems for as-
sisting patients during rehabilitation and treatment. De-

signing AR applications for patients is challenging due to
age demographics, varying afﬁnity to novel technologies
and general anxiety when it comes to medical treatments.
The novelty of AR technology also provides opportuni-
ties, since it can make otherwise repetitive or dull activi-
ties signiﬁcantly more engaging. We deduce three main
application areas in this domain: a) patient training and
education, b) assistance and monitoring and c) assessment
and diagnosis. An overview over all studies, grouped by
their application, is given in Table 3.

6.3.1 Patient training and education

It has been shown that immersive experiences can improve
patient engagement and satisfaction during training tasks
in rehabilitation [204] and pre-interventional patient edu-
cation [153]. Therefore, AR environments have the advan-
tage of being potentially more intriguing for patients than
conventional methods. At the same time, AR scenarios are
safe and easy to control.

A series of studies has investigated the usage of the
HoloLens to create virtual training environments for people
with cognitive disorders, such as Alzheimer’s disease [10,
66, 9]. Another training task, which has beneﬁted from AR
support through the HoloLens, is the control of functional
prostheses [188, 152]. In the context of patient education,
the HoloLens has been used to provide a more compre-
hensible and imaginable explanation to patients before
surgery [212, 87, 176].

6.3.2 Assistance and monitoring

AR, with its ability to enhance the reality around the users
in real-time, without insulating them, could be ideal for
compensating various impairments and overcoming difﬁ-
culties during the daily lives of patients. Mobile health
(mHealth) applications support such procedures through
mobile devices, such as smartphones, smartwatches, or,
in this case, the HoloLens, and are, consequently, ﬁtting

10

A PREPRINT - SEPTEMBER 8, 2022

Table 2: Studies reporting an application of the HoloLens for medical students and residents in an educational context.

Main application

Studies

Interventional and
surgical training (15)

Wang et al. [214], Cecil et al. [32], Condino et al. [37], Kobayashi et al. [103], Mahmood
et al. [129], Balian et al. [13], Hong et al. [86], Lu et al. [127], Muangpoon et al. [146],
Rewkowski et al. [171], Schoeb et al. [187], Brunzini et al. [24], Heinrich et al. [85],
Putnam et al. [162], and Suzuki et al. [200]

Anatomy learning (9)

Stojanovska et al. [196], Antoniou et al. [8], Gnanasegaram et al. [72], Maniam et al. [130],
Robinson et al. [173], Ruthberg et al. [179], Bogomolova et al. [19], and Kumar et al. [110]

(a)

(b)

(c)

Figure 8: Example applications for the patient. (a): The HoloLens is used during a training session for Alzheimer’s
patients (Source: Aruanno et al. [9]). (b) The HoloLens as an assistant and monitoring tool for medication adherence
(Source: Blusi et al. [18], CC-BY-NC). (c) A HoloLens-based system for functional mobility assessment (Source: Sun
et al. [199], CC-BY).

for scenarios outside of a clinical environment, e.g., in the
homes of patients.

The HoloLens has been explored for aiding patients with
vision impairments in navigating their surroundings [219,
7]. Other applications include assisting patients with cog-
nitive disorders in everyday activities [174, 96], helping
outpatients to adhere to their care plans [92, 18, 20] and
text editing for people with motor disabilities [80].

As mHealth applications are becoming more and more
pervasive in our everyday lives, integrating them into aug-
mented environments is a logical step, and the above-
mentioned studies suggest promising applications of head-
worn AR devices in mHealth. However, it should be noted
that the HoloLens is not yet suitable for operation during
everyday activities, as it is relatively expensive, and its
short battery live and bulky form factor make it unﬁt for
being worn and used for an extended period of time.

6.3.3 Assessment and diagnosis

The variety of built-in sensors, along with its self-tracking
capabilities, unfold the possibility to utilize the HoloLens
as a measurement device during patient assessments and
diagnosis. At the same time, instructions and demonstra-
tions, guiding patients through these tests, can be displayed
immersively and interactively.

Sun et al. [199] used the HoloLens for leading and track-
ing patient performance during functional mobility tests,
by evaluating the inertial measurement unit (IMU) data
recorded by the device. Geerse [67] and Koop [104] uti-
lize motion data collected by the HoloLens to assess gait
parameters (e.g., walking speed, step length, cadence) in
patients with movement disorders, in particular Parkinson’s
disease.

HoloLens-supported assessment and diagnosis is presum-
ably closest to real clinical applicability in the domain
of patient-oriented applications, as all reviewed studies
have shown reliability of the measurements derived from
the HoloLens sensors. At the same time, the ability to si-
multaneously monitor clinical parameters, while providing
instructions to the patient with a single device has obvious
beneﬁts in terms of ergonomics and economics. Further-
more, using the HoloLens during the conﬁned timespan of
such screenings is feasible without undue discomfort for
the patient.

7 Registration and object tracking with the

HoloLens

As already mentioned, the registration of virtual content to
the physical situation is one of the fundamental concepts

11

A PREPRINT - SEPTEMBER 8, 2022

Table 3: Studies reporting a patient-focused application of the HoloLens.

Main application

Studies

Patient training
and education (8)

Assistance
and monitoring (8)

Assessment
and diagnosis (3)

Aruanno et al. [10], Garzotto et al. [66], Sharma et al. [188], Aruanno et al. [9], Palermo
et al. [152], Wake et al. [212], House et al. [87], and Rositi et al. [176]

Yamashita et al. [219], Ingeson et al. [92], Angelopoulos et al. [7], Blusi et al. [18],
Rohrbach et al. [174], Boyd et al. [20], Guerrero et al. [80], and Janssen et al. [96]

Sun et al. [199], Geerse et al. [67], and Koop et al. [104]

display calibration, may be mitigated. 32 studies in this re-
view adopt such a manual registration technique, mostly by
using transformation of objects via on-board input methods
(hand gestures and voice commands) or additional input
devices, e.g., gamepads [25, 133] and keyboards [149].

Obviously, manual alignment of virtual content can be
time-consuming and ponderous, which affects applicabil-
ity in clinical settings, where time and personnel are usu-
ally scarce. Landmark-based methods can make manual
alignment faster and less cumbersome. They involve the
manual annotation of pre-deﬁned anatomical landmarks
in the spatial map of the real environment using gestures,
which are matched with their virtual counterparts in pre-
interventional imaging [138, 149, 148]. However, due
to the coarseness of the spatial map and the lack of hap-
tic feedback when selecting landmarks, these approaches
may not be reliable or accurate. All manual registration
methods have the disadvantage of being static – if the pa-
tient moves, the registration has to be manually adapted
accordingly.

7.2

Inside-out methods

The built-in sensors of the HoloLens offer several possi-
bilities for inside-out registration and tracking. The ad-
vantages of inside-out approaches in medical scenarios are
evident: They work in unprepared and unrestricted environ-
ments and do not rely on expensive, specialized hardware,
thus avoiding extra costs and further cluttering of already
densely occupied spaces, such as operation rooms. How-
ever, it is still difﬁcult to meet the high demands in accu-
racy and robustness of medical procedures using inside-out
approaches [189, 75]. With 52 occurrences, they are most
frequent in our reviewed studies.

Marker-based. Marker-based inside-out registration is
the most common registration technique identiﬁed in this
review, employed by 42 studies. Freely available AR li-
braries, such as Vuforia (PTC Inc, Boston, USA) or ArUco
library [65], facilitate optimized, close to real-time detec-
tion and tracking of image ﬁducials via the HoloLens’
front-facing RGB camera, which makes marker-based
inside-out strategies easy to implement. The most straight-
forward method for registration, also employed by com-
mercial SNS, is to anchor markers directly to rigid tissue

Figure 9: Frequency of registration and tracking methods
employed by the reviewed studies. Most works rely on
inside-out, marker-based tracking, followed by manual
alignment.

of medical AR. Registration permits X-ray visualization
for IGI or the localization of the patient’s anatomy for
surgical navigation, enables hybrid simulators or annotated
anatomical specimens in educational settings.

As previously mentioned, tracking for AR mostly fo-
cuses on the self-localization of the AR device. Since the
HoloLens already provides SLAM, applications in surgical
navigation or advanced medical simulators need only track
additional non-stationary objects (i.e., medical tools and
instruments) with high precision. SLAM is not suitable
for this type of dynamic tracking; therefore, other methods
need to be implemented. Registration and tracking are usu-
ally closely related, as the same paradigms and methods
can be applied to both tasks. In our analysis, we found
99 studies which establish a registration between virtual
and real content, which are listed in Table 4. 31 studies
further integrate methods for object tracking with their AR
systems, which are shown in Table 5. Figure 9 visualizes
the frequencies of identiﬁed paradigms and methods.

7.1 Manual registration

Due to the self-tracking capabilities of the HoloLens, reg-
istration between real and virtual content can be achieved
simply by manually aligning position, orientation and scale
of the virtual items to match their physical counterparts.
Since registration is performed for the perspective of the
user, factors hindering accurate perception, such as a poor

12

A PREPRINT - SEPTEMBER 8, 2022

Table 4: All studies applying registration between physical and virtual content, grouped by registration and tracking
paradigm and method.

Paradigm

Method

Studies

Manual (32)

Inside-out (52)

Agten et al. [2], Frantz et al. [58], Hanna et al. [83], Incekara et al. [91],
Kobayashi et al. [103], Li et al. [118], McJunkin et al. [132], Pratt et al. [160],
Rae et al. [169], Gibby et al. [69], Huang et al. [89], Liu et al. [120], Lohou et al.
[124], Mitsuno et al. [138], Creighton et al. [39], Fischer et al. [54], Katayama
et al. [100], Nguyen et al. [149, 148], Nuri et al. [151], Saito et al. [180], Scherl
et al. [184, 185], Tian et al. [203], Buch et al. [25], Gouveia et al. [74], Gu et al.
[79], Iizuka et al. [90], Ivan et al. [94], Meng et al. [133], Schneider et al. [186],
and Sugahara et al. [197]

Marker-based (42) Perkins et al. [158], Andress et al. [6], Carbone et al. [29], Frantz et al. [58],
Mahmood et al. [129], Moreta-Martinez et al. [144], Qian et al. [164], Amini
et al. [5], Gao et al. [63], Huang et al. [89], Kalavakonda et al. [99], Liebmann
et al. [119], Liu et al. [120], Qian et al. [166], Rose et al. [175], Van Doormaal
et al. [205], Ferraguti et al. [52], Huang et al. [88], Jiang et al. [97], Kriechling
et al. [107], Kunz et al. [112], Luzon et al. [128], Müller et al. [147], Qian et al.
[168], Rewkowski et al. [171], Wesselius et al. [215], Zuo et al. [224], Brunzini
et al. [24], Condino et al. [38], Dennler et al. [45], Gu et al. [79], Heinrich et al.
[85], Koyachi et al. [105], Li et al. [116], Long et al. [125], Nguyen et al. [150],
Pérez-Pachón et al. [157], Qi et al. [163], Schneider et al. [186], Spirig et al.
[194], Suzuki et al. [200], and Van Gestel et al. [206]

Marker-less (7)

Xie et al. [218], Pepe et al. [156], Gsaxner et al. [77], Pepe et al. [155], Sylos
Labini et al. [201], Gu et al. [79], and Gsaxner et al. [78]

Outside-in (25)

Marker-based (15) Condino et al. [37], El-Hariri et al. [50], Kuzhagaliyev et al. [114], Chien et al.
[35], De Oliveira et al. [41], Fotouhi et al. [57], Li et al. [117], Meulstee et al.
[135], Rewkowski et al. [171], Rüger et al. [178], Sun et al. [198], Glas et al.
[70], Gu et al. [79], Liu et al. [122], and Van Gestel et al. [207]

Marker-less (10)

Kuhlemann et al. [109], García-Vázquez et al. [64], Leuze et al. [115], Wu et al.
[217], Chien et al. [35], Wang et al. [213], Muangpoon et al. [146], Castelan
et al. [31], and Gu et al. [79]

of the patient, e.g., bones. For precisely relating the co-
ordinate frame of the marker to the target anatomy, it is
common practice to perform a pre-interventional scan, in-
cluding the marker. However, attaching markers to patients
is invasive and the additional imaging scan may lead to
increased radiation exposure of the patient. Additive man-
ufacturing offers an interesting alternative to this route,
which allows the creation of patient-speciﬁc bone guides
or occlusal splints for holding the markers [144, 63, 105].
In laboratory settings, 3D printing is also commonly used
to create custom, marker-embedded phantoms for testing
the registration method. Andress et al. [6] even devel-
oped a multi-modal marker, allowing intra-interventional
marker-based registration.

Alternatively, landmark-based approaches, where distinct
anatomical landmarks are digitized in the coordinate frame
of the HoloLens and matched to their virtual counterpart
using point based registration, can be used. A marker-
tracked pointing device is used for landmark selection in
these studies [205, 119, 107, 147, 215]. To adapt to
movements of the patient, a rigidly attached marker is
necessary, or the entire procedure has to be repeated.

It is straightforward to extend marker-based inside-out
methods for medical instrument tracking by simply attach-
ing markers to the tracked objects as well. 14 reviewed
studies apply such a combined strategy. Liu et al. [122]
and Condino et al. [37], combine marker-based, inside-out
patient registration with outside-in tracking, using stereo
cameras and electromagnetic sensors for tracking, respec-
tively.

A drawback of using ﬁducial markers is a general lack
of robustness and accuracy. It has been shown that the
tracking error using common libraries can range from sev-
eral millimeters to even centimeters [21, 28] and is highly
dependent on viewing angles, distance, lighting conditions
and movement patterns [115, 97, 128, 224]. These issues
make planar image targets not ideal for highly precise,
six degrees of freedom (6DoF) applications, as required
in most medical scenarios. In proof-of-concept studies,
Kunz et al. [112] and Van Gestel et al. [206] have explored
the possibility of tracking spherical, IR reﬂective mark-
ers inside-out using the IR sensor of the HoloLens, which
appears to be a promising direction.

13

A PREPRINT - SEPTEMBER 8, 2022

Table 5: All studies applying object tracking with the HoloLens, grouped by tracking paradigm and method.

Paradigm

Method

Studies

Inside-out (14)

Marker-based (14) Carbone et al. [29], Qian et al. [164], Gao et al. [63], Liebmann et al. [119],
Qian et al. [166], Kriechling et al. [107], Kunz et al. [112], Müller et al. [147],
Qian et al. [168], Rewkowski et al. [171], Li et al. [116], Nguyen et al. [150],
Spirig et al. [194], and Van Gestel et al. [206]

Outside-in (17)

Marker-based (12) El-Hariri et al. [50], Kuzhagaliyev et al. [114], Leuze et al. [115], De Oliveira
et al. [41], Li et al. [117], Meulstee et al. [135], Rewkowski et al. [171], Rüger
et al. [178], Sun et al. [198], Glas et al. [70], Liu et al. [122], and Van Gestel
et al. [207]

Marker-less (5)

Kuhlemann et al. [109], Condino et al. [37], García-Vázquez et al. [64], Liu
et al. [121], and Muangpoon et al. [146]

Marker-less. Ten studies explore the possibility of using
the various on-board sensors of the HoloLens for inside-
out, marker-less registration. An early work by Xie et
al. [218] explored the possibility of surface-based regis-
tration of a patient’s skin surface with the spatial map
created by the HoloLens SLAM. However, the spatial map
accessible to developers is very coarse, resulting in in-
sufﬁciently accurate natural features extractable from it.
Hajek et al. [82] also exploit the HoloLens SLAM by using
two devices in a master-worker conﬁguration, while Liu et
al. [121] use image-based matching to align intra-operative
X-ray with the patient anatomy.

Landmark-based registration approaches have been em-
ployed as well. For example, Pepe et al. [156, 155] use
automatically detected facial landmarks for registration.
From mid 2018 on, the Research Mode allowed access to
the HoloLens’ built in sensors aside from the RGB cam-
era, opening new possibilities for inside-out registration.
Sylos-Labini et al. [201] used automatically detected fa-
cial landmarks as well, but showed that, by combining
them with the ToF depth data, accuracy can be slightly
improved. Gsaxner et al. [77, 78] subsequently introduced
a pipeline for fully automatic registration via point cloud
matching, using 3D features from ToF depth alone. This
method was later also employed by Gu et al. [79], who
compared surface-based registration with marker-based
and outside-in methods.

7.3 Outside-in methods

25 reviewed publications use an outside-in paradigm for
registration and tracking. Outside-in approaches rely on ex-
ternal infrastructure for registration and tracking. External
infrastructure makes it possible to exploit highly precise,
specialized hardware, such as commercial SNS. The high
reference accuracy of such systems (usually ≤ 1 mm and
≤ 1 ◦) makes their integration into an AR environment
promising. However, the integration of external systems
requires the calibration of coordinate frames between the
HoloLens and the navigation device. This procedure usu-
ally involves manual and/or semi-automatic steps, which

can be cumbersome and disruptive to the clinical workﬂow,
as well as prone to errors and highly subjective [42].

Marker-based. Most commercially available SNS track
passively reﬂecting markers using stereoscopic IR cam-
eras [106]. By attaching those markers to the patient, their
relative localization in relation to pre-interventional imag-
ing can be determined. The HoloLens can be integrated
into such a setup, by afﬁxing markers to the headset as well.
Since SNS are designed not only for tracking patients, but,
in particular, medical instruments, object tracking can be
integrated easily with such systems, and most reviewed
studies in this category use this principle. Liu et al. [122]
use stereo cameras and LED markers instead.

Such marker-based SNS have a high reference precision,
often below one millimeter, however, in addition to poten-
tial complications resulting from system calibration, they
require a constant line-of-sight between IR camera, patient
and device, which may restrict movements.

Marker-less. Before the HoloLens Research Mode en-
abled access to the on-board ToF camera of the device,
some works integrated external depth sensors with the
HoloLens to enable a surface-based registration [115, 217,
35, 213]. As an alternative to capture the full surface of
patients, again, a sub-set of points in the form of anatom-
ical landmarks can be used, for example, digitized via
external electromagnetic trackers [109, 64, 146]. In these
scenarios, the electromagnetic sensors have been used for
instrument and tool tracking, as well. However, electro-
magnetic tracking is generally less popular than optical
tracking, as it suffers from interference with metallic mate-
rials, commonly found in clinical spaces [106].

8 Data and visualization

Various data were visualized in augmented environments
through the HoloLens. We distinguish data based on its
source (medical or non-medical) and type, according to
dimensionality (2D, 3D, other), and discuss how this data
is typically visualized. An overview of data source fre-

14

A PREPRINT - SEPTEMBER 8, 2022

Figure 10: Frequency of data sources used in the reviewed studies. 3D Medical imaging data is, by far, the most
common source of data for a visualization in AR.

quencies in the reviewed publications is given in Figure 10,
and a list of all papers in each category is provided in Ta-
ble 6. Note that most reviewed studies utilize more than
one source and type of data – therefore, multiple mentions
are possible.

8.1 Acquisition time

Regardless of the source and type, data can further be dis-
tinguished based on its acquisition time: Pre-interventional
data is acquired ofﬂine, processed and uploaded to the
HoloLens before the actual intervention. This method
allows more complex workﬂows, including manual ma-
nipulations of data. With overall 206 examples, pre-
interventional data makes up the majority of sources.

Intra-operative data is collected at run-time and streamed
to the device for visualization. Obviously, intra-operative
approaches are technically more complex, since they re-
quire a connection between the HoloLens and the raw
data source, and necessary processing steps need to be
performed automatically, in real-time. Overall, 58 intra-
interventional data sources have been identiﬁed for this
review.

8.2 Medical data

3D volumetric medical image data. For the majority
(110) of reviewed papers, 3D medical images, acquired
primarily through CT/CTA (89) and MRI (38), are a main
source of data. They are represented as volumetric grids,
where each voxel represents a speciﬁc value calculated
by the imaging device. For visualization, they have to be
rendered to present them on the HoloLens display.

Volumetric medical data is conventionally visualized in 2D
on monitors in clinical practice, in the form of orthogonal
slices through the image volume (mainly axial, sagittal
and coronal planes or, sometimes, oblique reformats, so
called multi-planar reformations). Since physicians are

accustomed to this type of visualization, slice rendering
of volumetric data has also been employed in 29 reviewed
medical HoloLens systems.

This technique has, of course, the drawback that data is
only shown in selected planes. Given a stereoscopic AR
display, true 3D visualization is becoming more widely
used, mostly in the form of 3D surface renderings, which
is computationally efﬁcient and natively supported by all
graphics engines compatible with the HoloLens. Further-
more, colors and opacities can easily be modiﬁed, enabling
visualization techniques such as wire frames or outline vi-
sualizations. However, for surface rendering, tissue has
to be segmented and converted to polygons prior to visu-
alization, leading to more time intensive workﬂows and
quantization inaccuracies. In contrast, direct volume ren-
dering offers superior image quality [113, 101] and does
not require surface extraction before visualization.
In-
stead, color and opacity are directly computed from the
underlying voxel values using specialized transfer func-
tions. Still, performance requirements of volume rendering
cannot be easily addressed with mobile hardware, such as
the HoloLens. Consequently, only six reviewed studies
attempt volume rendering [59, 216, 87, 94, 68, 4].

Since data acquisition and reconstruction of 3D volumet-
ric data is relatively costly, only few applications with
intra-operative acquisition times exist. Velazco-Garcia et
al. [209] describe a framework for live interactions with
MRI scanners. Qian et al. [166] stream 3D endoscopy data
to the HoloLens in real-time, while Southworth et al. [193]
display live 3D cardiac electrophysiology data with the
HoloLens.

2D medical image data. 20 reviewed studies use 2D
medical imaging as a data source. Common modalities
include X-Ray/ﬂuoroscopy scans (7), ultrasound (7) or
endoscopic video (3). Contrary to 3D imaging, 2D modali-
ties usually have short acquisition times (close to or even
meeting real-time requirements) and are comparably easy

15

A PREPRINT - SEPTEMBER 8, 2022

to deploy, and are therefore popular for intra-interventional
guidance of procedures. 15 publications in this category
support intra-interventional data acquisition during the run-
time of the HoloLens.

Analogous to the ordinary clinical practice, 2D imaging
data in AR is often visualized on virtual (AR) monitors,
which can be anchored to the head gaze of the HoloLens
wearer. Another possibility is to position 2D images on
3D planes in the environment, which allows an in-situ
visualization, if a registration between imaging data and
patient is available.

Other data from medical sources.
In many situations,
it is beneﬁcial to integrate other medical data not stem-
ming from medical imaging into the workﬂow. Medical
planning data is a particularly common example, with 29
publications integrating planning data into their workﬂows.
This data is usually created manually and pre-operatively
by medical professionals before an intervention on the
basis of medical imaging. It can include access points,
tool trajectories, cutting lines, resection margins and target
positions of implants, amongst others. This type of data
is usually translated into geometric primitives, which are
displayed in relation to the target anatomy.

For intra-interventional data, the positional coordinates of
medical tools (such as needles, wires, or screws) or other
tracked objects (parts of the anatomy, imaging systems)
obtained from outside-in or inside-out navigation systems
are the most common data source. Mostly, these objects are
represented by geometric primitives or 3D models, which
are transformed according to the positional information.
However, a simple numerical representation is also used in
some studies [119, 121, 107].

Other medical data sources, which have been captured both
pre- and intra-interventionally, include vital signs or other
biosignals and patient records, which can be displayed on
virtual monitors in AR.

8.3 Non-medical data

3D data. The inherent ability of the HoloLens for stereo-
scopic rendering make all sorts of 3D meshes an obvious
choice of data source for AR visualizations.

Eight studies use 3D scans of patients, captured with depth
or stereo cameras, instead of volumetric medical imaging,
mostly for the purpose of image-to-patient registration.

Contrary to medical 3D data, such scans can only capture
the surface of patients and do not inform about the un-
derlying anatomy. In particular in educational scenarios
(targeting both patients and students), the visualization
of anatomical models, created by medical artists, is com-
mon and used in 11 studies. Both of these data sources
have exclusively been deployed pre-interventionally to the
HoloLens.

Other non-medical 3D data include geometric primitives
and non-anatomical models, e.g., for creating serious

games [10, 66, 9] or procedure simulations [214, 32, 130,
174, 209].

2D data. A small number of eleven studies visualize non-
medical two-dimensional data in the form of pre-recorded
or live streamed videos or documents. As with 2D medical
data, it is usually displayed on virtual monitors anchored
to the display or environment.

Other data. Six works have explored the possibility of
integrating other data, in most cases coming from the
HoloLens itself, into their applications. Three publica-
tions track the user wearing the HoloLens, to measure
gait parameters [67, 104] or guide the user [219]. Two
publications utilize the gaze data from the HoloLens [199,
85]. Only Sharma et al. [188] use an external data source,
namely IMU data, for training limb prosthesis control.

9 Evaluation of medical HoloLens

applications

In general, an objective evaluation of AR applications is
challenging, because each user has a different perception of
augmented content, depending on individual anatomy (in-
terpupillary distance, eye sight), familiarity with the tech-
nology, familiarity with 3D visualizations in general [177]
and external inﬂuences, such as comfort while wearing
the AR device. In comparison to other areas of computer
science, no benchmarks, datasets or standard protocols ex-
ist to evaluate AR experiences and usefulness. Clinically,
comparative clinical trials, measuring and comparing pa-
rameters about treatment outcomes, such as treatment time,
number and severity of complications or survival rate, are
considered gold standard. However, each AR application
requires approval by a relevant agency or committee be-
fore it can be tested on cadavers, healthy human subjects or
even patients. Depending on the executive research institu-
tion and national regulations, obtaining such an approval
and the quantitative data that comes with it, can be very
difﬁcult for researchers. Therefore, in our reviewed studies,
a large variety of evaluation metrics have been collected
in distinct experimental scenarios, which are summarized
in Table 7.

9.1 Evaluation scenario

We ﬁrst analyze the reviewed publications with regard
to the evaluation scenario. Inspired by the Technology
Readiness Level [131], we group the studies according to
their evaluation settings, which is shown in Figure 11:

Proof of concept studies
focus on reporting a medical
problem, how AR could overcome it and describe their pro-
totype workﬂows and applications. Sometimes, anecdotal
or informal feedback from users or general observations
are reported, but, in general, these studies do not follow a
rigorous experimental protocol and do not collect quantita-
tive or qualitative measurements. Therefore, it is difﬁcult

16

to draw general conclusions from them. With 35 papers,
proof of concept studies are in the minority.

the possible advantages and implications of the HoloLens
in their domain.

A PREPRINT - SEPTEMBER 8, 2022

Laboratory studies
typically focus on the technical as-
pects of their applications and report quantitative mea-
surements. We identify 47 records in this category. The
study can be carried out using only hardware (e.g., the
HoloLens), or on cadavers, animals or humans (healthy or
patients). Most commonly, however, phantoms are used
to collect measurements. Specialized medical phantoms,
which include realistic anatomical structures and tissue
characteristics, are commercially available, however, they
are very expensive. Consequently, many researchers resort
to additive manufacturing (i.e., 3D printing) to replicate
the target anatomy or build more abstract phantoms.

Studies performed in a relevant environment
evalu-
ate their AR systems directly in the environment in which
it should be implemented. Such an approach involves the
usage of the system by one or more individuals of the in-
tended target group – either clinicians, patients or medical
students. Most of the time, qualitative feedback in the
form of questionnaires is collected from them, although
quantitative measurements, for example measuring task
performance, might also be taken. As seen in Figure 11,
most of the reviewed studies (89) fall into this category,
which indicates advanced research maturity. We further dis-
tinguish between non-comparative studies, where results
acquired through AR are not compared to another method
(for example, case series or uncontrolled cohort studies),
and comparative studies, which provide comparisons to
non-AR conditions. The latter are most conclusive about

Figure 11: Number of papers for each experimental setting
(inner circle) and experimental level (outer circle).

9.2 Quantitative metrics

Quantitative metrics are often focused on technical aspects
of the AR system. Therefore, acquiring them does not
require a large number of test subjects and, instead, can be
done by individuals. However, they can also characterize
the performance of individuals in carrying out certain tasks.
In this case, quantitative measures are usually closely re-
lated to the application scenario.

Technical performance metrics. Several works, in par-
ticular in the area of data display, measure performance
metrics of the HoloLens itself, such as hardware utilization,
frame rate, power usage, execution time and latency. The
studies come to the conclusion that the HoloLens is suit-
able for displaying pre- and intra-interventional medical
data given an appropriate software framework, also within
safety critical environments, such as the OR. A commonly
reported limiting factor is battery life, which restricts de-
vice usage to around two hours, which is too short for
many medical interventions.

The HoloLens was also compared to other OST-HMD de-
vices for medical usage. Qian et al. [167] evaluated the
HoloLens, Epson Moverio BT-200 and ODG R-7 for dis-
playing object-anchored 2D medical data, and concluded
that the HoloLens is the best choice in terms of contrast,
frame rate and perceived task load. Moosburner et al. [141]
compare the HoloLens to the Meta 2 (Meta Company,
San Mateo, California, USA) and found that, albeit the
HoloLens was criticized for having a comparably small
FoV and being more complicated and difﬁcult to operate,
medical students preferred it over the competitor, as it
does not rely on a wired connection to a powerful external
computer and presented virtual models more stably.

Accuracy metrics. For registration and tracking, accu-
racy metrics, measuring the spatial distance between the
virtual and real position of an object, are usually acquired.

While many different measures can be computed, the tar-
get registration error (TRE) is one of the most commonly
and consistently used metrics for evaluating registration
accuracy, and has been employed by 22 reviewed studies.

TRE measures the Euclidean distance between 3D target
points in the physical world and their virtual counterparts.
Studies evaluating the TRE report averages of just above 1
mm and up to 40 mm. E.g., for a registration using outside-
in tracking, El Hariri et al. [50] report a TRE of 36.9 mm,
while Kuhlemann et al. [109], Li et al. [117] and Sun et
al. [198] report much lower values of 4.3 mm, 2.2 mm and
1.3 mm, respectively. For manual registration, the reported
error spectrum is also large, ranging between 20 mm [25]
and 3 mm [203]. Registration using image ﬁducials seems
to be the most reliable in terms of TRE, with values in
the 2 mm region [37, 144], but several studies show that

17

A PREPRINT - SEPTEMBER 8, 2022

Table 6: All studies grouped by data source and type, data modality and acquisition time.

Source
& Type Modality

Acquisition
Time

Studies

Bucioli et al. [26], Kuhlemann et al. [109], Sauer et al. [183], Agten et al. [2],
Condino et al. [37], El-Hariri et al. [50], Frantz et al. [58], Fröhlich et al. [59],
García-Vázquez et al. [64], Hajek et al. [82], Kobayashi et al. [103], Li et al.
[118], Mahmood et al. [129], McJunkin et al. [132], Moreta-Martinez et al. [144],
Pratt et al. [160], Rae et al. [169], Wu et al. [217], Affolter et al. [1], Brun et al.
[23], Checcucci et al. [33], De Oliveira et al. [41], Gao et al. [63], Gibby et al.
[69], Gsaxner et al. [77], Huang et al. [89], Kalavakonda et al. [99], Kubben et al.
[108], Li et al. [117], Liebmann et al. [119], Liu et al. [121], Liu et al. [120],
Lohou et al. [124], Maniam et al. [130], Mitsuno et al. [138, 139], Moosburner
et al. [141], Rose et al. [175], Sylos Labini et al. [201], Van Doormaal et al.
[205], Wake et al. [212], Wang et al. [213], Witowski et al. [216], Al Janabi
et al. [3], Allison et al. [4], Bulliard et al. [27], Cartucho et al. [30], Creighton
et al. [39], Ferraguti et al. [52], Fischer et al. [54], Fitski et al. [55], Galati et al.
[62], Jiang et al. [97], Katayama et al. [100], Kriechling et al. [107], Kumar et al.
[111], Luzon et al. [128], Muangpoon et al. [146], Müller et al. [147], Nguyen
et al. [149, 148], Perkins et al. [159], Saito et al. [180], Sun et al. [198], Tian
et al. [203], Zuo et al. [224], Buch et al. [25], Cofano et al. [36], Condino et al.
[38], Dennler et al. [45, 44], Gehrsitz et al. [68], Glas et al. [70], Gsaxner et al.
[78], Gu et al. [79], Iizuka et al. [90], Koyachi et al. [105], Li et al. [116], Long
et al. [125], Meng et al. [133], Qi et al. [163], Saito et al. [181], Schneider et al.
[186], Spirig et al. [194], Sugahara et al. [197], Suzuki et al. [200], Van Gestel
et al. [206], Wake et al. [211], and Wesselius et al. [215]

Morales Mojica et al. [143], Perkins et al. [158], Xie et al. [218], Carbone et al.
[29], Fröhlich et al. [59], Incekara et al. [91], Jang et al. [95], Leuze et al. [115],
Gsaxner et al. [77], Kubben et al. [108], Soulami et al. [192], Van Doormaal
et al. [205], Wake et al. [212], Allison et al. [4], Cartucho et al. [30], Ferraguti
et al. [52], Galati et al. [62], Gnanasegaram et al. [72], House et al. [87], Kumar
et al. [111], Nguyen et al. [149, 148], Pelanis et al. [154], Tian et al. [203],
Condino et al. [38], Gehrsitz et al. [68], Gsaxner et al. [78], Iizuka et al. [90],
Ivan et al. [94], Morales Mojica et al. [142], Qi et al. [163], Scherl et al. [184,
185], Van Gestel et al. [206, 207], and Wake et al. [211]

CT &
CTA
(89)

Pre (89)

Pre (35)

MRI
(37)

Intra (2)

Velazco-Garcia et al. [210, 209]

PET (5)

Pre (5)

Fröhlich et al. [59], Pepe et al. [156, 155], Gsaxner et al. [77, 78], and Galati
et al. [62]

Other (2)

Intra (2)

Qian et al. [166] and Southworth et al. [193]

US (7)

Intra (7)

El-Hariri et al. [50], García-Vázquez et al. [64], Kuzhagaliyev et al. [114],
Mahmood et al. [129], Cartucho et al. [30], Rüger et al. [178], and Nguyen et al.
[150]

X-Ray
(7)

Pre (2)

Qian et al. [167] and Galati et al. [62]

Intra (5)

Andress et al. [6], Deib et al. [43], Fotouhi et al. [57], Liu et al. [121], and
Al Janabi et al. [3]

Video (3)

Intra (3)

Qian et al. [164], Al Janabi et al. [3], and Qian et al. [168]

Other (4)

Pre (3)

Hanna et al. [83], Huang et al. [88], and Robinson et al. [173]

Intra (1)

Cui et al. [40]

)
0
1
1
(

l
a
c
i
d
e
m
D
3

)
0
2
(

l
a
c
i
d
e
m
D
2

18

A PREPRINT - SEPTEMBER 8, 2022

Source
& Type Modality

Acquisition
time

Studies

Position
(26)

Intra (26)

Planning
(29)

Pre (29)

Kuhlemann et al. [109], Andress et al. [6], Carbone et al. [29], Condino et al.
[37], García-Vázquez et al. [64], Hajek et al. [82], Kuzhagaliyev et al. [114],
Qian et al. [164], Fotouhi et al. [57], Gao et al. [63], Li et al. [117], Liebmann
et al. [119], Liu et al. [120], Meulstee et al. [135], Qian et al. [166], Kriechling
et al. [107], Muangpoon et al. [146], Müller et al. [147], Qian et al. [168], Iqbal
et al. [93], Li et al. [116], Liu et al. [122], Spirig et al. [194], Van Gestel et al.
[206, 207], and Velazco-Garcia et al. [210]

Carbone et al. [29], Condino et al. [37], Kobayashi et al. [103], Li et al. [118],
Pratt et al. [160], Gibby et al. [69], Li et al. [117], Liebmann et al. [119], Liu
et al. [120], Lohou et al. [124], Wang et al. [213], Ferraguti et al. [52], Kriechling
et al. [107], Müller et al. [147], Maniam et al. [130], Tian et al. [203], Cofano
et al. [36], Condino et al. [38], Dennler et al. [44], Glas et al. [70], Li et al. [116],
Liu et al. [122], Long et al. [125], Meng et al. [133], Morales Mojica et al. [142],
Spirig et al. [194], Suzuki et al. [200], Van Gestel et al. [206], and Wesselius
et al. [215]

Vital
Signs (2)

Records
(2)

3D Scans
(8)

Pre (1)

Qian et al. [167]

Intra (1)

Sirilak et al. [190]

Pre (1)

Perkins et al. [159]

Intra (1)

Deib et al. [43]

Pre (8)

Hanna et al. [83], Amini et al. [5], Chien et al. [35], Talaat et al. [202], Nuri et al.
[151], Proniewska et al. [161], Kumar et al. [110], and Liu et al. [122]

)
4
3
(

Anatomical
model (11) Pre (11)

Pre (14)

Other (15)

Balian et al. [13], Stojanovska et al. [196], Antoniou et al. [8], Moro et al. [145],
Robinson et al. [173], Rositi et al. [176], Ruthberg et al. [179], Bogomolova
et al. [19], Brunzini et al. [24], Castelan et al. [31], and Putnam et al. [162]

Aruanno et al. [10], Wang et al. [214], Cecil et al. [32], Garzotto et al. [66],
Sharma et al. [188], Aruanno et al. [9], Maniam et al. [130], Meulstee et al.
[135], Palermo et al. [152], Rohrbach et al. [174], Geerse et al. [67], Janssen
et al. [96], Rewkowski et al. [171], and Velazco-Garcia et al. [210]

)
4
6
(

l
a
c
i
d
e
m

r
e
h
t
o

l
a
c
i
d
e
m
-
n
o
n
D
3

Intra (1)

Angelopoulos et al. [7]

Pre (3)

Sun et al. [199], Lu et al. [127], and Schoeb et al. [187]

Intra (4)

Sirilak et al. [190], Mitsuno et al. [139], Glick et al. [71], and Cofano et al. [36]

Video (7)

Docs (4)

Pre (4)

Hanna et al. [83], Sirilak et al. [190], Galati et al. [62], and Rositi et al. [176]

-

Intra (6)

Yamashita et al. [219], Sharma et al. [188], Sun et al. [199], Geerse et al. [67],
Koop et al. [104], and Heinrich et al. [85]

2D non-
medical
(11)

other
non-
medical
(6)

19

the achievable accuracy with image ﬁducials is highly de-
pendent on lumination, viewing angle and movement [97,
128, 224]. Whether the reported registration accuracies
are acceptable is, of course, contingent upon the clinical
scenario. However, most studies express the need to re-
duce the registration error before clinical usability. While
TRE provides some comparability between registration
methods, measuring it involves the selection or digitization
of matching landmark points, which is itself a subjective,
error-prone procedure, encumbered by a lack of haptic
feedback, ﬁne-grained input possibilities and depth percep-
tion. These problems explain the large variability reported
for this metric.

A variant of the TRE is the ﬁducial registration error (FRE),
which uses ﬁducials used in landmark-based registration
approaches as target points for error computation. Three
publications report FRE. For example, Van Doormaal et
al. [205] compared FRE achievable with marker-based
inside-out registration using landmarks with a conventional
SNS registration. They found that the AR system is less
accurate and not yet suitable for clinical application. How-
ever, it has been shown that FRE does not correlate with
the TRE and thus, does not inform much about the actual
registration accuracy [56].

Another common measure, evaluated in 18 studies, is the
target visualization error (TVE), which measures the re-
projection error between physical and virtual objects as
perceived by the user, e.g., using a ruler or a millimeter grid
or by marking the virtual projection directly on the real
counterpart. Most studies report TVE values in the millime-
ter region. Three studies compare the TVE achieved using
the HoloLens with a non-AR baseline: Ivan et al. [94]
found no signiﬁcant difference to a commercial SNS in
terms of TVE. Qi et al. [163] state that AR could reach
the reference precision in 80% of cases, while Incekara et
al. [91] determined that only in 38% of cases the reference
could be met, and the mean deviation of 4 mm between
HoloLens and SNS is too large for clinical applicability.
Still, the manual measurement of TVE is, again, subject to
operator bias.

Registration error, measured in three studies, calculates the
deviation between the source-to-target transformation com-
puted by the employed algorithm and a reference transfor-
mation obtained from a reference tracking system. Anal-
ogously, tracking error compares the pose of a tracked
object to a ground truth, ideally in six degrees of freedom.
Since the reference system and the HoloLens need to be
calibrated, such experiments are complicated to set up.
Therefore, many studies report a simpliﬁed tracking error,
e.g., in 2D [121] or positional only [112, 122].

In clinical interventions where pre-interventional planning
data is available, the target deviation error (TDE), which
measures the Euclidean distance between a pre-operatively
planned target point and the actual point after interven-
tion, can be determined. A typical scenario is the insertion
of objects, such as needles, wires or screws, into a phan-
tom, cadaver or patient under AR guidance. After insertion,

A PREPRINT - SEPTEMBER 8, 2022

post-operative imaging is acquired, which can be compared
to the planning. This type of clinically speciﬁc evaluation
is most objective and informative about how an AR system
can support the intervention in question. We identiﬁed 24
publications evaluating TDE. Several studies perform such
a clinically speciﬁc evaluation by comparing the outcome
of an AR-supported procedure to a non-AR control condi-
tion; however, results are inconclusive. Several studies [2,
6, 120, 147, 125] compare needle/wire placements under
AR guidance with a conventional, ﬂouroscopy-guided pro-
cedure. They found that placements in AR were slightly
less accurate than in the reference condition, although AR
guidance lead to faster task completion. Andress et al. [6]
and Long et al. [125] further point out that, with AR, less
radiation was required during image-guided procedures.
Li et al. [117], Ruger et al. [178] and Glas et al. [70], re-
port favorable needle insertion accuracies in AR-guided
procedures versus conventional image guided procedures.
Compared to freehand, non-guided procedures, AR could
improve both accuracy and number of successful task com-
pletions in placement tasks [52, 45, 207].

Task-speciﬁc scores. Studies using the HoloLens for
supporting speciﬁc medical tasks usually report some quan-
tiﬁcation of task completion. The task completion time
(TCT) is most commonly measured, namely in 32 reviewed
studies. Most comparative studies report that AR guidance
helped users in carrying out tasks faster [62, 2, 3, 6, 120,
147, 125, 85, 70, 188, 52, 168, 200], while others did
not report signiﬁcant differences [214, 43]. Only Qi et
al. [163] and Rohrbach et al. [174] report longer TCT
for the AR condition, however, the latter application is
targeted as Alzheimer’s patients, who may have more dif-
ﬁculties in adapting to novel technology, such as AR and
the HoloLens.

The number of successful task completions (NSC) is mea-
sured in 12 studies. Most studies report favorable outcomes
of HoloLens usage in terms of NSC [45, 186, 188, 168].
Only Agten et al. [2] found that AR actually leads to less
successful outcomes, compared to a conventional image-
guided procedure.

The effectiveness of AR for learning in an educational sce-
nario can be quantitatively measured by comparing exam
scores between AR-supported learners and a control group.
Seven studies perform such an evaluation. No statistically
signiﬁcant knowledge improvement was found between
students receiving AR lectures through the HoloLens ver-
sus students undergoing conventional anatomy courses
based on cadavers [196, 173, 179]. Robinson et al. [173],
however, highlight that students perceived the AR activity
more favorably. Similar ﬁndings are described in compar-
ison to other computerized learning methods by [8, 72,
145] – while student engagement, motivation and excite-
ment is typically higher for HoloLens-based education, the
outcomes in terms of learning effect are not signiﬁcantly
different.

20

A PREPRINT - SEPTEMBER 8, 2022

9.3 Qualitative metrics

We deﬁne qualitative metrics as parameters and data, which
reﬂect the personal opinion of individuals, and can, there-
fore, not be objectively and repeatably measured. Usually,
they are collected from application users by the means of
questionnaires or interviews. Since AR experiences are
highly individual, qualitative metrics can be considered
equally if not more important than quantitative measures.
After all, theoretical beneﬁts of medical AR are negligible
if the system that delivers them is deemed cumbersome or
fails to meet the user’s needs.

Commonly, questionnaires use a Likert scale, where re-
spondents express their level of agreement or disagreement
with certain statements. 42 reviewed publications use such
questionnaires for evaluating various system aspects. Ex-
amples include general comfort, image quality and audio
quality of the HoloLens and its suitability for medical
applications [37, 95, 190, 141, 62, 111, 3, 184, 45], the
effectiveness of certain types of visualization [23, 212, 87,
68] or, most commonly, how well the proposed application
can support a certain procedure.

Generally, the reported questionnaire outcomes are favor-
able towards the HoloLens and AR, and the common con-
sensus is that AR can have a large impact in the medical
domain. However, limitations of the device itself, such
as the small ﬁeld of view, short battery life and relative
discomfort while wearing it are frequently mentioned. For
IGI or navigation applications, users also frequently no-
ticed a lack of registration accuracy or a drift of virtual
content due to instabilities in the HoloLens SLAM, which
negatively inﬂuenced user ratings. In these scenarios, is-
sues of depth perception, where users perceived internal
anatomy to be on top of, not within, the patient, were also
frequently mentioned.

Some reviewed studies employed standardized question-
naires, with the NASA Task Load Index (NASA-TLX), a
tool to assess subjective workload, being the most com-
monly used one. A drawback of the NASA-TLX is that
it is only fully descriptive in comparative studies, where
it is measured for several conditions. A classiﬁcation or
interpretation of a single ﬁnal score is, generally, not sub-
stantial. Unfortunately, only a few comparative studies
measure the NASA-TLX – two of them report a reduced
task load for AR-supported procedures [52, 168], Rüger
et al. [178] found no signiﬁcant difference and Saito et
al. [180] found that the mental demand was higher for
participants using AR.

The System Usability Scale (SUS) [22] was applied in ﬁve
studies as a measure for application usability. Compared
to the NASA-TLX, the advantage of SUS is that it is a
fast way of classifying the ease of use of a system, even
without a comparison. Generally, overall scores greater
than 68 are considered above average; furthermore, an
adjective rating scale has been proposed [14]. Only two
reviewed studies compute the overall SUS, both reporting
above average usability with SUS values of 71.5 [5] and

74.8 [78], scoring a "Good" on the adjective rating scale.
While these ratings are encouraging, they suggest that there
is room for improvement.

10 Conclusion and outlook

With 171 original, peer reviewed works in the medical ﬁeld,
the HoloLens certainly had a large impact on medical AR
already. In this systematic review, we found that, while
various medical specialties and applications have been in-
vestigated, and a fair number of systems have been studied
clinically, only few works have clinically demonstrated
clear advantages of HoloLens-based systems over the cur-
rent state-of-the-art. The acceptance of new technologies,
such as AR, in the medical ﬁeld is an ongoing challenge for
researchers, medical professionals and patients alike. In
this review, we identify that increased efforts in the areas
of precision, reliability, usability, workﬂow and perception
are necessary to establish AR in clinical practice.

We found that applications targeted at physicians and
healthcare professionals are, by far, the most common.
While the potential beneﬁt for AR supported image guid-
ance and navigation is very high, those systems are also
difﬁcult to implement, mostly due to the high accuracy
and reliability demands. The reviewed studies suggest that,
for high precision applications, registration and tracking
errors achieved with the HoloLens are generally too high,
regardless of the employed technical paradigm and method.
However, for procedures carried out without image guid-
ance, for which sub-millimeter precision is not necessary
(e.g., ablations [52], ventriculostomy [118, 207, 186] or
certain orthopedic interventions [44]), the HoloLens is al-
ready a very promising tool. In these scenarios, the slim
form factor and low cost of the HoloLens in comparison to
traditional image guidance systems could make navigation
feasible for procedures which have not beneﬁted from it
before. For this purpose, however, automatic and accurate
inside-out registration and tracking is paramount to keep
the setup workﬂow.

The second most common intended user group were stu-
dents. In educational training scenarios, the HoloLens was
shown to be an effective enhancement for medical simula-
tors [13, 86, 146, 187, 200, 85], in particular for providing
visual feedback during training tasks. In anatomy learning,
the effects of HoloLens learning compared to conventional
learning using cadavers or other computerized methods
seem to be small, although several studies report improved
engagement and motivation of students, which could have
positive effects in the long term. Anatomy learning stud-
ies in this review also usually feature relatively simple,
conventional 3D models. More innovative visualizations,
including interactive, dynamic content, which can not be
easily delivered by regular computerized methods, have
not been explored in depth yet.

Finally, patients were the least frequent target user. Un-
fortunately, many interesting assistance and monitoring
applications are limited by the restricted possible usage

21

A PREPRINT - SEPTEMBER 8, 2022

Table 7: Summary of evaluation strategies for medical HoloLens applications. Studies are grouped according to their
evaluation setting and level. For each study, we report the acquired qualitative (Qual) and quantitative (Quant) measures.

Proof of Concept Studies (no measures reported)

Bucioli et al. [26], Cui et al. [40], Sauer et al. [183], Xie et al. [218], Carbone et al. [29], Cecil et al. [32], Garzotto
et al. [66], Hanna et al. [83], Kobayashi et al. [103], Kuzhagaliyev et al. [114], Mahmood et al. [129], Pratt et al. [160],
Affolter et al. [1], Kalavakonda et al. [99], Kubben et al. [108], Lohou et al. [124], Mitsuno et al. [139], Soulami et al.
[192], Witowski et al. [216], Allison et al. [4], Boyd et al. [20], Fitski et al. [55], Katayama et al. [100], Lu et al. [127],
Maniam et al. [130], Perkins et al. [159], Proniewska et al. [161], Castelan et al. [31], Gouveia et al. [74], Iizuka et al.
[90], Morales Mojica et al. [142], Saito et al. [181], Sugahara et al. [197], Wake et al. [211], and Wesselius et al. [215]

Level

Study

Measures

Laboratory Studies (Quantitative measures)

Technical

Morales Mojica et al. [143]
Fröhlich et al. [59]
Velazco-Garcia et al. [209]

CPU usage, memory consumption, FPS
CPU usage, GPU usage, memory consumption, FPS
FPS, Latency

m
o
t
n
a
h
P

Agten et al. [2]
El-Hariri et al. [50]
Frantz et al. [58]
García-Vázquez et al. [64]
Hajek et al. [82]

Task completion time, Number of successful completions
Target registration error
Registration time, Target visualization error, content drift
Latency
Calibration error (hand-eye), Target registration error, Number of suc-
cessful completions
Leuze et al. [115]
Tracking error, Registration error
Moreta-Martinez et al. [144] Target registration error, Target deviation error (surgical guide)
Qian et al. [164]
Rae et al. [169]
Wu et al. [217]
Chien et al. [35]
De Oliveira et al. [41]
Fotouhi et al. [57]
Gibby et al. [69]
Gsaxner et al. [77]
Huang et al. [89]
Liebmann et al. [119]

Target visualization error, Tracking error, FPS
Target visualization error, Registration time
Target registration error, Registration time
Target registration error, Registration time
SLAM accuracy, Latency, Target visualization error
Calibration error (hand-eye), Tracking accuracy
Target deviation error (needle), Task completion time
Target registration error, Registration error, Registration time
Target visualization error, Task completion time
Fiducial registration error, Registration time, Target deviation error
(screw)
Tracking error (2D)
Target deviation error (screw)
Calibration error (hand-eye), Target deviation error (model)
Registration time, Target visualization error
Target visualization error
Target registration error, Registration time
Target registration error
Registration error
Target visualization error
Target registration error
Target deviation error (k-wire)
Tracking error (relative, position)
Target registration error
Target visualization error
Calibration error, Latency
Target registration error, Registration time
Target visualization error
Latency

Liu et al. [121]
Liu et al. [120]
Meulstee et al. [135]
Mitsuno et al. [138]
Sylos Labini et al. [201]
Wang et al. [213]
Creighton et al. [39]
Gu et al. [79]
Huang et al. [88]
Jiang et al. [97]
Kriechling et al. [107]
Kunz et al. [112]
Luzon et al. [128]
Nguyen et al. [149]
Rewkowski et al. [171]
Sun et al. [198]
Pérez-Pachón et al. [157]
Van Gestel et al. [206]

22

A PREPRINT - SEPTEMBER 8, 2022

Level

Study

Measures

Animal

Li et al. [117]
Liu et al. [122]
Li et al. [116]

Target registration error, Target deviation error (needle)
Tracking accuracy
Target deviation error (marker), Task completion time, complications

Cadaver

McJunkin et al. [132]
Müller et al. [147]
Tian et al. [203]
Meng et al. [133]
Spirig et al. [194]

Target registration error
Target deviation error (k-wires)
Target registration error, Target deviation error
Target deviation error (osteotomy lines)
Target deviation error (k-wires)

Human

Buch et al. [25]

Quant: Target registration error

Studies in a Relevant Environment (Quantitative and qualitative measures)

s
e
i
d
u
t
s

e
s
a
C

Aruanno et al. [10]
Kuhlemann et al. [109]
Perkins et al. [158]
Qian et al. [167]
Yamashita et al. [219]
Condino et al. [37]
Ingeson et al. [92]
Jang et al. [95]

Pepe et al. [156]
Sirilak et al. [190]
Amini et al. [5]
Aruanno et al. [9]

Balian et al. [13]
Blusi et al. [18]
Brun et al. [23]
Checcucci et al. [33]
Gao et al. [63]
Moosburner et al. [141]
Pepe et al. [155]
Qian et al. [166]

Rose et al. [175]

Sun et al. [199]
Bulliard et al. [27]
Cartucho et al. [30]
Fischer et al. [54]
Guerrero et al. [80]
Hong et al. [86]
Kumar et al. [111]
Koop et al. [104]
Muangpoon et al. [146]
Nguyen et al. [148]
Nuri et al. [151]
Pelanis et al. [154]
Rositi et al. [176]
Scherl et al. [184]
Southworth et al. [193]
Zuo et al. [224]
Bogomolova et al. [19]
Brunzini et al. [24]
Cofano et al. [36]
Dennler et al. [45]
Gsaxner et al. [78]

Quant: Number of successful completions
Quant: Target registration error; Qual: Likert questionnaire
Quant: Target visualization error
Quant: Latency; Qual: Image quality, NASA-TLX
Quant: Number of successful completions
Quant: Target registration error; Qual: NASA TLX, Likert questionnaire
Qual: Likert questionnaire
Quant: CPU usage, GPU usage, power usage, FPS; Qual: Likert ques-
tionnaire
Quant: Target visualization error; Qual: Likert questionnaire
Qual: SUS, Likert questionnaire
Quant: Task completion time; Qual: Likert questionnaire, SUS
Quant: Number of successful completions, performance metrics; Qual:
Likert questionnaire
Quant: Performance metrics; Qual: Likert questionnaire
Qual: Likert questionnaire
Qual: Likert questionnaires, surgical plan
Qual: Likert questionnaires, surgical plan
Quant: Target deviation error (pointer), Task completion time
Qual: SUS, Likert questionnaire
Quant: Target visualization error; Qual: Likert questionnaire
Quant: Target visualization error, latency, Task completion time, Number
of successful completions
Quant: Target registration error, Task completion time; Qual: Likert
questionnaire
Quant: Measurement accuracy, Task completion time
Quant: Task completion time, number of interactions
Qual: Likert questionnaires
Quant: Target registration error, Registration time; Qual: questionnaire
Qual: Likert questionnaire
Quant: Performance metrics
Qual: Likert questionnaire
Quant: Measurement accuracy
Quant: Fiducial registration error; Qual: Likert questionnaire
Qual: preferences questionnaire
Quant: Target visualization error, Registration time
Quant: Task completion time, Number of successful completions
Qual: Likert questionnaire
Quant: Target registration error; Qual: Likert questionnaire
Quant: FPS, power usage, Latency; Qual: Image quality
Quant: Target registration error; Qual: NASA TLX
Qual: Likert questionnaire
Qual: Likert questionnaire, questionnaire;
Qual: SUS
Qual: Likert questionnaire
Quant: Usage times; Qual: Likert questionnaire, SUS

23

Level

Study

Measures

A PREPRINT - SEPTEMBER 8, 2022

y
d
u
t

S
e
v
i
t
a
r
a
p
m
o
C

Heinrich et al. [85]
Koyachi et al. [105]
Kumar et al. [110]
Schneider et al. [186]
Velazco-Garcia et al. [210]

Wang et al. [214]
Andress et al. [6]

Deib et al. [43]
Incekara et al. [91]
Sharma et al. [188]
Angelopoulos et al. [7]
Li et al. [117]
Palermo et al. [152]
Rohrbach et al. [174]
Stojanovska et al. [196]
Talaat et al. [202]
Van Doormaal et al. [205]
Wake et al. [212]
Al Janabi et al. [3]

Antoniou et al. [8]
Ferraguti et al. [52]

Galati et al. [62]
Geerse et al. [67]
Glick et al. [71]
Gnanasegaram et al. [72]
House et al. [87]
Janssen et al. [96]
Qian et al. [168]

Robinson et al. [173]
Ruthberg et al. [179]
Rüger et al. [178]

Saito et al. [180]
Schoeb et al. [187]
Condino et al. [38]
Gehrsitz et al. [68]
Glas et al. [70]

Dennler et al. [44]

Iqbal et al. [93]
Ivan et al. [94]
Long et al. [125]
Moro et al. [145]
Nguyen et al. [150]
Putnam et al. [162]
Qi et al. [163]

Scherl et al. [185]

Suzuki et al. [200]
Van Gestel et al. [207]

Quant: Task completion time, performance; Qual: Likert questionnaire
Quant: Target deviation error (osteotomy lines)
Qual: Likert questionnaire
Quant: Number of successful completions, Target deviation error (drain)
Quant: Task completion time, Target deviation error (needles), interac-
tions

Quant: Task completion time; Qual: Likert questionnaire, questionnaire
Quant: Marker tracking accuracy, Target registration error, Target devia-
tion error (k-wires), Number of X-Ray acquisitions
Quant: Task completion time, Dosimetry
Quant: Target visualization error
Quant: Task completion time, Number of successful completions
Quant: Task completion time, performance metrics
Quant: Registration time, Target deviation error (drain)
Quant: Task completion time, Number of successful completions
Quant: Task completion time, performance metrics
Quant: Exam scores, Learning time
Quant: Distance between landmarks
Quant: Fiducial registration error
Qual: Likert questionnaire
Quant: Task completion time; Qual: Performance evaluation, Likert
questionnaire
Quant: Biosignals
Quant: Registration accuracy, Task completion time, Target deviation
error (needles); Qual: NASA-TLX
Quant: Task completion time; Qual: Likert questionnaires
Quant: Measurement accuracy
Quant: Task completion time; Qual: Likert questionnaires
Quant: Exam scores, Qual: Likert questionnaire
Qual: Likert questionnaire
Quant: Performance metrics; Qual: Likert questionnaire
Quant: Task completion time, Number of successful completions; Qual:
NASA-TLX
Quant: Exam scores; Qual: Self assessment
Quant: Exam scores, Learning time
Quant: Task completion time, Target deviation error (needles); Qual:
NASA-TLX, questionnaire
Qual: NASA-TLX
Quant: Exam scores Qual: Self assessment, Likert questionnaires
Qual: Likert questionnaire, planning
Quant: Task completion time; Qual: Likert questionnaire
Quant: Task completion time, Target deviation error (pointer); Qual:
Questionnaire
Quant: Target deviation error (screws), Number of successful comple-
tions
Quant: Task completion time; Qual: Likert questionnaire
Quant: Target visualization error; Qual: Performance evaluation,
Quant: Target deviation error (needle), Task completion time, radiation
Quant: Exam scores; Qual: Likert questionnaire
Quant: FPS, Latency, marker tracking accuracy, Task completion time
Qual: Likert questionnaire
Quant: Registration time, Target visualization error, Task completion
time
Quant: Target registration error, Task completion time, number of com-
plicaitons
Quant: Task completion time, Performance metrics; Qual: Questionnaire
Quant: Target deviation error (drain); Qual: Performance evaluation,
Likert questionnaire

24

A PREPRINT - SEPTEMBER 8, 2022

time of the HoloLens, and it is not foreseeable that next-
gen OST-HMD devices will overcome these limitations
in the near future. However, for selected scenarios, such
as guidance through therapy and diagnosis sessions, the
HoloLens has already shown to be useful. Until now, only
a small number of applications have been explored in this
context – it will be interesting to see whether other disci-
plines can beneﬁt from such paradigms as well.

The majority of studies in this review seeks a registration
between real and virtual environment, and inside-out ap-
proaches, in particular using image ﬁducials, are the most
common methods to achieve the required registration. This
observation is unsurprising – after all, such approaches
are relatively easy to implement. Our analysis shows that
they deliver a reliable, acceptable accuracy in controlled
settings. Their described disadvantages, such as line-of-
sight constraints and susceptibility to different viewing
positions, movement patterns and lighting conditions, how-
ever, likely impede clinical adoption. Spherical markers
seem to be more robust and encouraging results have been
reported [112], more recently also for the HoloLens 2 [76].
Innovative, marker-less, inside-out strategies have been
reported for registration, but are still hampered by techni-
cal limitations. For instrument tracking, research in the
direction of marker-less, inside-out methods based on deep
learning is only recently gaining traction [46], but will
surely have a large impact in the ﬁeld.

When it comes to data and visualization, the majority
of studies display pre-interventionally acquired 3D med-
ical imaging data, primarily from CT or CTA, visualized
through surface rendering. We expect this trend to continue.
Compared to volume rendering, surface renderings are easy
to create, modify and efﬁcient to render, and no clear ad-
vantage of volume rendering through the HoloLens has
been shown so far. Perceptual issues, in particular depth
perception, are a known problem in AR [123], and several
works mention that incorrect depth perception negatively
inﬂuenced the perceived accuracy of their application and
impaired guidance through the HoloLens. Still, very few
reports concern themselves with visualization strategies
overcoming these limitations, and use very simple meth-
ods (e.g., wire frames [54]). While many strategies exist
to improve depth perception in medical AR [75], most
of them are difﬁcult to apply with OST displays, such as
the HoloLens, where only additive visual information is
possible and the view of reality cannot be altered. Novel,
innovative strategies will be necessary to overcome this
limitation in the future.

It is paramount that medical AR applications are validated
with the intended user in the loop, and it is encouraging to
see that the majority of studies in this review evaluate their
applications in a relevant setting. Still, the large variety in
experimental setups and acquired measures, together with
the lack of standardized protocols, makes it very difﬁcult
to clinically validate these methods. We believe that this
review can serve as a guideline to researchers, to help them
in picking appropriate experimental protocols and mea-

sures for their scenario. We think that it is time for medical
AR to step out of the comfort zone of controlled laboratory
settings, and ﬁnally ﬁnd its way into medical routine. To
this end, close collaborations between researchers, uni-
versities, clinicians and patients, as well as comparative
studies on a larger scale are necessary.

The HoloLens 1 has likely reached the end of its life cycle
in research, due to the release of its direct successor, but
it has caused a major boost in medical AR research. With
the availability of novel hardware, such as the HoloLens
2 or the Magic Leap 2, and the recent increased interest
of other leading tech companies in AR technologies, we
expect this trend to continue. Furthermore, specialized
medical OST-HMD devices, e.g., xvision (Augmedics Inc.,
Arlington Heights, IL) or VOSTARS (University of Pisa,
Pisa, IT), have the potential to address technical limita-
tions in current, commercial devices. Improved hardware
can also facilitate the use of deep learning models on the
HMD itself, opening up countless possibilities in terms
of recognition, tracking and scene understanding. In con-
clusion, we think that, although the feasibility of using
the HoloLens for various medical scenarios has been sug-
gested, research in medical AR is still in its early stages,
and abundant areas for future work remain.

Acknowledgments

This work received funding from the Austrian Science
Fund (FWF) KLI 678: ‘enFaced - Virtual and Augmented
Reality Training and Navigation Module for 3D-Printed
Facial Defect Reconstructions’, KLI 1044: ‘enFaced 2.0
- Instant AR Tool for Maxillofacial Surgery’ and ‘KITE’
(Plattform für KI-Translation Essen) from the REACT-EU
initiative.

References

[1] R. Affolter, S. Eggert, T. Sieberth, M. Thali, and
L. C. Ebert. “Applying augmented reality during
a forensic autopsy—Microsoft HoloLens as a DI-
COM viewer”. In: Journal of Forensic Radiology
and Imaging 16 (2019), pp. 5–8.

[2] C. A. Agten et al. “Augmented reality–guided lum-
bar facet joint injections”. In: Investigative radiol-
ogy 53.8 (2018), pp. 495–498.

[3] H. F. Al Janabi et al. “Effectiveness of the
HoloLens mixed-reality headset in minimally inva-
sive surgery: a simulation-based feasibility study”.
In: Surgical endoscopy 34.3 (2020), pp. 1143–
1149.

[4] B. Allison, X. Ye, and F. Janan. “Breast3D: An
Augmented Reality System for Breast CT and
MRI”. In: International Conference on Artiﬁcial
Intelligence and Virtual Reality (AIVR). IEEE.
2020, pp. 247–251.

25

A PREPRINT - SEPTEMBER 8, 2022

[18] M. Blusi and J. C. Nieves. “Feasibility and ac-
ceptability of smart augmented reality assisting pa-
tients with medication pillbox self-management”.
In: Studies in health technology and informatics
(2019), pp. 521–525.

[19] K. Bogomolova et al. “Development of a virtual
three-dimensional assessment scenario for anatom-
ical education”. In: Anatomical sciences education
14.3 (2021), pp. 385–393.

[20] N. Boyd, J. Hawkins, T. Yang, and S. A. Val-
court. “Augmented Reality: Telehealth Demonstra-
tion Application”. In: Practice and Experience in
Advanced Research Computing. 2020, pp. 452–
455.

[21] M. Brand, L. A. Wulff, Y. Hamdani, and T. Schüpp-
stuhl. “Accuracy of Marker Tracking on an Optical
See-Through Head Mounted Display”. In: Annals
of Scientiﬁc Society for Assembly, Handling and
Industrial Robotics. Springer, 2020, pp. 21–31.
J. Brooke et al. “SUS-A quick and dirty usability
scale”. In: Usability evaluation in industry 189.194
(1996), pp. 4–7.

[22]

[23] H. Brun et al. “Mixed reality holograms for heart
surgery planning: ﬁrst user experience in congen-
ital heart disease”. In: European Heart Journal-
Cardiovascular Imaging 20.8 (2019), pp. 883–888.
[24] A. Brunzini, A. Papetti, M. Germani, and E.
Adrario. “Mixed reality in medical simulation: a
comprehensive design methodology”. In: Proceed-
ings of the Design Society 1 (2021), pp. 2107–
2116.

[25] V. P. Buch et al. “Development of an intraoperative
pipeline for holographic mixed reality visualiza-
tion during spinal fusion surgery”. In: Surgical
innovation 28.4 (2021), pp. 427–437.

[26] A. A. Bucioli et al. “Holographic real time 3D
heart visualization from coronary tomography for
multi-place medical diagnostics”. In: Int Conf
on Dependable, Autonomic and Secure Comput-
ing, Int Conf on Pervasive Intelligence and Com-
puting, Int Conf on Big Data Intelligence and
Computing and Cyber Science and Technology
Congress (DASC/PiCom/DataCom/CyberSciTech).
IEEE. 2017, pp. 239–244.
J. Bulliard et al. “Preliminary testing of an aug-
mented reality headset as a DICOM viewer dur-
ing autopsy”. In: Forensic Imaging 23 (2020),
p. 200417.

[27]

[28] A. Cao, A. Dhanaliwala, J. Shi, T. P. Gade, and B. J.
Park. “Image-based marker tracking and registra-
tion for intraoperative 3D image-guided interven-
tions using augmented reality”. In: SPIE Medical
Imaging. Vol. 11318. 2020.

[29] M. Carbone et al. “Proof of concept: wearable
augmented reality video see-through display for
neuro-endoscopy”. In: International Conference

[5] S. Amini and M. Kersten-Oertel. “Augmented real-
ity mastectomy surgical planning prototype using
the HoloLens”. In: Healthcare technology letters
6.6 (2019), pp. 261–265.

[6] S. Andress et al. “On-the-ﬂy augmented reality
for orthopedic surgery using a multimodal ﬁdu-
cial”. In: Journal of Medical Imaging 5.2 (2018),
p. 021209.

[7] A. N. Angelopoulos, H. Ameri, D. Mitra, and M.
Humayun. “Enhanced depth navigation through
augmented reality depth mapping in patients with
low vision”. In: Scientiﬁc reports 9.1 (2019), pp. 1–
10.

[8] P. E. Antoniou et al. “Biosensor Real-Time Affec-
tive Analytics in Virtual and Mixed Reality Medi-
cal Education Serious Games: Cohort Study”. In:
JMIR Serious Games 8.3 (2020), e17823.

[9] B. Aruanno and F. Garzotto. “MemHolo: mixed
reality experiences for subjects with Alzheimer’s
disease”. In: Multimedia Tools and Applications
78.10 (2019), pp. 13517–13537.

[10] B. Aruanno, F. Garzotto, and M. C. Rodriguez.
“Hololens-based mixed reality experiences for sub-
jects with alzheimer’s disease”. In: Proceedings
of the Biannual Conference on Italian SIGCHI
Chapter. 2017, pp. 1–9.

[11] R. T. Azuma. “A survey of augmented reality”. In:
Presence: Teleoperators & Virtual Environments
6.4 (1997), pp. 355–385.

[12] G. Badiali et al. “Review on augmented reality
in oral and cranio-maxillofacial surgery: toward
“surgery-speciﬁc” head-up displays”. In: IEEE Ac-
cess 8 (2020), pp. 59015–59028.

[13] S. Balian, S. K. McGovern, B. S. Abella, A. L.
Blewer, and M. Leary. “Feasibility of an aug-
mented reality cardiopulmonary resuscitation train-
ing system for health care providers”. In: Heliyon
5.8 (2019), e02205.

[14] A. Bangor, P. Kortum, and J. Miller. “Determining
what individual SUS scores mean: Adding an ad-
jective rating scale”. In: Journal of usability studies
4.3 (2009), pp. 114–123.

[15] E. Z. Barsom, M. Graaﬂand, and M. P. Schijven.
“Systematic review on the effectiveness of aug-
mented reality applications in medical training”.
In: Surgical endoscopy 30.10 (2016), pp. 4174–
4183.

[16] S. Bernhardt, S. A. Nicolau, L. Soler, and C.
Doignon. “The status of augmented reality in la-
paroscopic surgery as of 2016”. In: Medical image
analysis 37 (2017), pp. 66–90.

[17] M. Birlo, P. E. Edwards, M. Clarkson, and D. Stoy-
anov. “Utility of optical see-through head mounted
displays in augmented reality-assisted surgery: a
systematic review”. In: Medical Image Analysis
(2022), p. 102361.

26

[30]

on Augmented Reality, Virtual Reality and Com-
puter Graphics (AVR). Springer. 2018, pp. 95–104.
J. Cartucho, D. Shapira, H. Ashraﬁan, and S. Gi-
annarou. “Multimodal mixed reality visualisation
for intraoperative surgical guidance”. In: Interna-
tional journal of computer assisted radiology and
surgery 15.5 (2020), pp. 819–826.

[31] E. Castelan, M. Vinnikov, and X. Alex Zhou.
“Augmented Reality Anatomy Visualization for
Surgery Assistance with HoloLens: AR Surgery
Assistance with HoloLens”. In: International Con-
ference on Interactive Media Experiences (IMX).
2021, pp. 329–331.
J. Cecil, A. Gupta, and M. Pirela-Cruz. “Design
of VR based orthopedic simulation environments
using emerging technologies”. In: International
Systems Conference (SysCon). IEEE. 2018, pp. 1–
7.

[32]

[33] E. Checcucci et al. “3D mixed reality holograms
for preoperative surgical planning of nephron-
sparing surgery: evaluation of surgeons’ percep-
tion.” In: Minerva urologica e nefrologica= The
Italian journal of urology and nephrology (2019).
[34] L. Chen, T. W. Day, W. Tang, and N. W. John. “Re-
cent developments and future challenges in med-
ical mixed reality”. In: International symposium
on mixed and augmented reality (ISMAR). IEEE.
2017, pp. 123–135.
J.-C. Chien, Y.-R. Tsai, C.-T. Wu, and J.-D. Lee.
“HoloLens-Based AR System with a Robust Point
Set Registration Algorithm”. In: Sensors 19.16
(2019), p. 3555.

[35]

[36] F. Cofano et al. “Augmented reality in medical
practice: from spine surgery to remote assistance”.
In: Frontiers in Surgery 8 (2021), p. 657901.
[37] S. Condino et al. “How to build a patient-speciﬁc
hybrid simulator for Orthopaedic open surgery:
beneﬁts and limits of mixed-reality using the Mi-
crosoft HoloLens”. In: Journal of healthcare engi-
neering 2018 (2018).

[38] S. Condino et al. “Hybrid simulation and planning
platform for cryosurgery with Microsoft Hololens”.
In: Sensors 21.13 (2021), p. 4450.

[39] F. X. Creighton et al. “Early feasibility studies of
augmented reality navigation for lateral skull base
surgery”. In: Otology & Neurotology 41.7 (2020),
pp. 883–888.

[40] N. Cui, P. Kharel, and V. Gruev. “Augmented real-
ity with Microsoft HoloLens holograms for near
infrared ﬂuorescence based image guided surgery”.
In: Molecular-Guided Surgery: Molecules, De-
vices, and Applications III. Vol. 10049. Interna-
tional Society for Optics and Photonics. 2017,
p. 100490I.

A PREPRINT - SEPTEMBER 8, 2022

[41] M. E. De Oliveira, H. G. Debarba, A. Lädermann,
S. Chagué, and C. Charbonnier. “A hand-eye cal-
ibration method for augmented reality applied to
computer-assisted orthopedic surgery”. In: The in-
ternational journal of medical robotics and com-
puter assisted surgery 15.2 (2019), e1969.
[42] M. E. De Oliveira et al. “An image-based method
to automatically propagate bony landmarks: appli-
cation to computational spine biomechanics”. In:
Computer methods in biomechanics and biomedi-
cal engineering 18.14 (2015), pp. 1535–1542.

[43] G. Deib et al. “Image guided percutaneous spine
procedures using an optical see-through head
mounted display: proof of concept and rationale”.
In: Journal of neurointerventional surgery 10.12
(2018), pp. 1187–1191.

[44] C. Dennler et al. “Augmented reality in the oper-
ating room: A clinical feasibility study”. In: BMC
musculoskeletal disorders 22.1 (2021), pp. 1–9.

[45] C. Dennler et al. “Augmented Reality Navigated
Sacral-Alar-Iliac Screw Insertion”. In: Interna-
tional Journal of Spine Surgery 15.1 (2021),
pp. 161–168.

[46] M. Doughty and N. R. Ghugre. “HMD-EgoPose:
Head-Mounted Display-Based Egocentric Marker-
Less Tool and Hand Pose Estimation for Aug-
mented Surgical Guidance”. In: arXiv preprint
arXiv:2202.11891 (2022).

[47] M. Doughty, N. R. Ghugre, and G. A. Wright.
“Augmenting Performance: A Systematic Review
of Optical See-Through Head-Mounted Displays
in Surgery”. In: Journal of Imaging 8.7 (2022),
p. 203.

[48] H. Durrant-Whyte and T. Bailey. “Simultaneous lo-
calization and mapping: part I”. In: IEEE robotics
& automation magazine 13.2 (2006), pp. 99–110.
[49] M. Eckert, J. S. Volmerg, and C. M. Friedrich.
“Augmented reality in medicine: systematic and
bibliographic review”. In: JMIR mHealth and
uHealth 7.4 (2019), e10967.

[50] H. El-Hariri, P. Pandey, A. J. Hodgson, and R.
Garbi. “Augmented reality visualisation for or-
thopaedic surgical guidance with pre-and intra-
operative multimodal
image data fusion”. In:
Healthcare Technology Letters 5.5 (2018), pp. 189–
193.

[51] G. El Shallaly and A. Cuschieri. “Optimum view
distance for laparoscopic surgery”. In: Surgical
Endoscopy And Other Interventional Techniques
20.12 (2006), pp. 1879–1882.

[52] F. Ferraguti et al. “Augmented reality and robotic-
assistance for percutaneous nephrolithotomy”. In:
IEEE robotics and automation letters 5.3 (2020),
pp. 4556–4563.

27

A PREPRINT - SEPTEMBER 8, 2022

[64] V. García-Vázquez et al. “Navigation and visual-
isation with HoloLens in endovascular aortic re-
pair”. In: Innovative surgical sciences 3.3 (2018),
pp. 167–177.

[65] S. Garrido-Jurado, R. Muñoz-Salinas, F. J. Madrid-
Cuevas, and M. J. Marín-Jiménez. “Automatic gen-
eration and detection of highly reliable ﬁducial
markers under occlusion”. In: Pattern Recognition
47.6 (2014), pp. 2280–2292.

[66] F. Garzotto, E. Torelli, F. Vona, and B. Aruanno.
“HoloLearn: Learning through mixed reality for
people with cognitive disability”. In: International
Conference on Artiﬁcial Intelligence and Virtual
Reality (AIVR). IEEE. 2018, pp. 189–190.
[67] D. J. Geerse, B. Coolen, and M. Roerdink. “Quanti-
fying spatiotemporal gait parameters with hololens
in healthy adults and people with Parkinson’s dis-
ease: Test-retest reliability, concurrent validity, and
face validity”. In: Sensors 20.11 (2020), p. 3216.
[68] P. Gehrsitz et al. “Cinematic rendering in mixed-
reality holograms: a new 3D preoperative planning
tool in pediatric heart surgery”. In: Frontiers in
Cardiovascular Medicine 8 (2021), p. 633611.
J. T. Gibby, S. A. Swenson, S. Cvetko, R. Rao,
and R. Javan. “Head-mounted display augmented
reality to guide pedicle screw placement utilizing
computed tomography”. In: International journal
of computer assisted radiology and surgery 14.3
(2019), pp. 525–535.

[69]

[70] H. H. Glas et al. “Augmented reality visualization
for image-guided surgery: a validation study using
a three-dimensional printed phantom”. In: Journal
of Oral and Maxillofacial Surgery 79.9 (2021),
1943–e1.

[71] Y. Glick et al. “Augmenting prehospital care”. In:

[72]

BMJ Mil Health (2020).
J. J. Gnanasegaram, R. Leung, and J. A. Beyea.
“Evaluating the effectiveness of learning ear
anatomy using holographic models”. In: Journal of
Otolaryngology-Head & Neck Surgery 49.1 (2020),
pp. 1–8.

[73] L. Goode. Microsoft’s HoloLens 2 Puts a Full-
Fledged Computer on Your Face. https://www.
wired.com/story/microsoft-hololens-2-
headset/. [Online; accessed April 2021]. 2019.

[74] P. F. Gouveia et al. “Breast cancer surgery with aug-
mented reality”. In: The Breast 56 (2021), pp. 14–
17.

[75] C. Gsaxner, U. Eck, D. Schmalstieg, N. Navab,
and J. Egger. “Augmented Reality in Oral and Max-
illofacial Surgery”. In: Computer-Aided Oral and
Maxillofacial Surgery - Developments, Applica-
tions, and Future Perspectives. Ed. by J. Egger
and X. Chen. First Edition. Academic Press, 2021,
pp. 1–282. ISBN: 978-0-12-823299-6.

[53] H. Ferrone and D. Coulter. HoloLens Research
Mode. https : / / docs . microsoft . com / en -
us / windows / mixed - reality / develop /
platform - capabilities - and - apis /
research-mode. [Online; accessed April 2021].
2020.

[54] M. Fischer et al. “Evaluation of different visual-
ization techniques for perception-based alignment
in medical AR”. In: International Symposium on
Mixed and Augmented Reality Adjunct (ISMAR-
Adjunct). IEEE. 2020, pp. 45–50.

[56]

[55] M. Fitski et al. “MRI-Based 3-Dimensional Vi-
sualization Workﬂow for the Preoperative Plan-
ning of Nephron-Sparing Surgery in Wilms’ Tu-
mor Surgery: A Pilot Study”. In: Journal of Health-
care Engineering 2020 (2020).
J. M. Fitzpatrick. “Fiducial registration error and
target registration error are uncorrelated”. In: Med-
ical Imaging 2009: Visualization, Image-Guided
Procedures, and Modeling. Vol. 7261. SPIE. 2009,
pp. 21–32.
J. Fotouhi et al. “Interactive ﬂying frustums (IFFs):
spatially aware surgical data visualization”. In: In-
ternational journal of computer assisted radiology
and surgery 14.6 (2019), pp. 913–922.

[57]

[58] T. Frantz, B.

Jansen,

J. Duerinck, and J.
Vandemeulebroucke. “Augmenting Microsoft’s
HoloLens with vuforia tracking for neuronaviga-
tion”. In: Healthcare technology letters 5.5 (2018),
pp. 221–225.

[59] M. Fröhlich et al. “Holographic visualisation and
interaction of fused CT, PET and MRI volumet-
ric medical imaging data using dedicated remote
GPGPU ray casting”. In: Simulation, Image Pro-
cessing, and Ultrasound Systems for Assisted Di-
agnosis and Navigation. Springer, 2018, pp. 102–
110.

[60] H. Fuchs et al. “Towards performing ultrasound-
guided needle biopsies from within a head-
mounted display”. In: International Conference on
Visualization in Biomedical Computing. Springer.
1996, pp. 591–600.

[61] D. M. Gaba. “The future vision of simulation in
health care”. In: BMJ Quality & Safety 13.suppl 1
(2004), pp. i2–i10.

[62] R. Galati et al. “Experimental setup employed in
the operating room based on virtual and mixed
reality: analysis of pros and cons in open abdomen
surgery”. In: Journal of healthcare engineering
2020 (2020).

[63] Y. Gao, L. Lin, G. Chai, and L. Xie. “A feasibility
study of a new method to enhance the augmented
reality navigation effect in mandibular angle split
osteotomy”. In: Journal of Cranio-Maxillofacial
Surgery 47.8 (2019), pp. 1242–1248.

28

A PREPRINT - SEPTEMBER 8, 2022

[88]

[87] P. M. House et al. “Use of the mixed reality tool
“VSI Patient Education” for more comprehensible
and imaginable patient educations before epilepsy
surgery and stereotactic implantation of DBS or
stereo-EEG electrodes”. In: Epilepsy research 159
(2020), p. 106247.
J. Huang, M. Halicek, M. Shahedi, and B. Fei.
“Augmented reality visualization of hyperspectral
imaging classiﬁcations for image-guided brain
tumor phantom resection”. In: Medical Imaging
2020: Image-Guided Procedures, Robotic Inter-
ventions, and Modeling. Vol. 11315. International
Society for Optics and Photonics. 2020, 113150U.
[89] L. Huang, S. Collins, L. Kobayashi, D. Merck,
and T. Sgouros. “Shared visualizations and guided
procedure simulation in augmented reality with
Microsoft HoloLens”. In: Medical Imaging 2019:
Image-Guided Procedures, Robotic Interventions,
and Modeling. Vol. 10951. SPIE. 2019, pp. 273–
278.

[90] K. Iizuka, Y. Sato, Y. Imaizumi, and T. Mizutani.
“Potential Efﬁcacy of Multimodal Mixed Reality
in Epilepsy Surgery”. In: Operative Neurosurgery
20.3 (2021), pp. 276–281.

[91] F. Incekara, M. Smits, C. Dirven, and A. Vincent.
“Clinical feasibility of a wearable mixed-reality
device in neurosurgery”. In: World neurosurgery
118 (2018), e422–e427.

[92] M. Ingeson, M. Blusi, and J. C. Nieves. “Microsoft
hololens-A Mhealth solution for medication ad-
herence”. In: International Workshop on Artiﬁcial
Intelligence in Health. Springer. 2018, pp. 99–115.
[93] H. Iqbal, F. Tatti, and F. R. y Baena. “Augmented
reality in robotic assisted orthopaedic surgery: A
pilot study”. In: Journal of Biomedical Informatics
120 (2021), p. 103841.

[95]

[94] M. E. Ivan et al. “Augmented reality head-mounted
display–based incision planning in cranial neuro-
surgery: a prospective pilot study”. In: Neurosurgi-
cal focus 51.2 (2021), E3.
J. Jang et al. “Three-dimensional holographic vi-
sualization of high-resolution myocardial scar on
HoloLens”. In: PloS one 13.10 (2018), e0205188.
[96] S. Janssen et al. “The effects of augmented reality
visual cues on turning in place in Parkinson’s dis-
ease patients with freezing of gait”. In: Frontiers
in neurology 11 (2020), p. 185.

[97] T. Jiang et al. “HoloLens-based vascular local-
ization system: precision evaluation study with
a three-dimensional printed model”. In: Journal of
medical Internet research 22.4 (2020), e16852.

[98] L. Jud et al. “Applicability of augmented reality in
orthopedic surgery–a systematic review”. In: BMC
musculoskeletal disorders 21.1 (2020), pp. 1–13.

[76] C. Gsaxner, J. Li, A. Pepe, D. Schmalstieg, and
J. Egger. “Inside-Out Instrument Tracking for Sur-
gical Navigation in Augmented Reality”. In: 27th
ACM Symposium on Virtual Reality Software and
Technology. 2021, pp. 1–11.

[77] C. Gsaxner, A. Pepe, J. Wallner, D. Schmalstieg,
and J. Egger. “Markerless image-to-face registra-
tion for untethered augmented reality in head and
neck surgery”. In: International Conference on
Medical Image Computing and Computer-Assisted
Intervention (MICCAI). Springer. 2019, pp. 236–
244.

[78] C. Gsaxner et al. “Augmented Reality for Head
and Neck Carcinoma Imaging: Description and
Feasibility of an Instant Calibration, Markerless
Approach”. In: Computer Methods and Programs
in Biomedicine 200 (2021), p. 105854.

[79] W. Gu, K. Shah, J. Knopf, N. Navab, and M. Un-
berath. “Feasibility of image-based augmented re-
ality guidance of total shoulder arthroplasty using
microsoft HoloLens 1”. In: Computer Methods in
Biomechanics and Biomedical Engineering: Imag-
ing & Visualization 9.3 (2021), pp. 261–270.
[80] G. Guerrero, L. Gómez, and J. Achig. “Holonote:
Text editor of augmented reality oriented to people
with motor disabilities.” In: 2020 15th Iberian Con-
ference on Information Systems and Technologies
(CISTI). IEEE. 2020, pp. 1–7.

[82]

[81] D. Guha et al. “Augmented reality in neurosurgery:
a review of current concepts and emerging appli-
cations”. In: Canadian Journal of Neurological
Sciences 44.3 (2017), pp. 235–245.
J. Hajek et al. “Closing the calibration loop:
an inside-out-tracking paradigm for augmented
Interna-
reality in orthopedic surgery”.
tional Conference on Medical Image Computing
and Computer-Assisted Intervention (MICCAI).
Springer. 2018, pp. 299–306.

In:

[83] M. G. Hanna, I. Ahmed, J. Nine, S. Prajapati, and
L. Pantanowitz. “Augmented reality technology
using Microsoft HoloLens in anatomic pathology”.
In: Archives of pathology & laboratory medicine
142.5 (2018), pp. 638–644.

[84] C. Hansen et al. “Auditory support for resection
guidance in navigated liver surgery”. In: Interna-
tional Journal of Medical Robotics and Computer
Assisted Surgery 9.1 (2013), pp. 36–43.

[85] F. Heinrich et al. “HoloPointer: a virtual aug-
mented reality pointer for laparoscopic surgery
training”. In: International journal of computer as-
sisted radiology and surgery 16.1 (2021), pp. 161–
168.

[86] K. Hong, Y. Sakamoto, G. Srinivasan, and P. Irani.
“Exploring technology for neonatal resuscitation
training”. In: Proceedings of the 11th Augmented
Human International Conference. 2020, pp. 1–9.

29

[99] N. Kalavakonda, L. Sekhar, and B. Hannaford.
“Augmented reality application for aiding tumor
resection in skull-base surgery”. In: 2019 Interna-
tional Symposium on Medical Robotics (ISMR).
IEEE. 2019, pp. 1–6.

[100] M. Katayama, K. Ueda, D. Mitsuno, and H. Kino.
“Intraoperative 3-dimensional Projection of Blood
Vessels on Body Surface Using an Augmented
Reality System”. In: Plastic and Reconstructive
Surgery Global Open 8.8 (2020).

[101] T. Kilgus et al. “Mobile markerless augmented re-
ality and its application in forensic medicine”. In:
International journal of computer assisted radiol-
ogy and surgery 10.5 (2015), pp. 573–586.
[102] G. Klein. Registration on HoloLens. Keynote talk

at ISMAR. 2017.

[103] L. Kobayashi, X. C. Zhang, S. A. Collins, N.
Karim, and D. L. Merck. “Exploratory applica-
tion of augmented reality/mixed reality devices for
acute care procedure training”. In: Western Journal
of Emergency Medicine 19.1 (2018), p. 158.
[104] M. M. Koop et al. “The HoloLens augmented real-
ity system provides valid measures of gait perfor-
mance in healthy adults”. In: IEEE Transactions
on Human-Machine Systems 50.6 (2020), pp. 584–
592.

[105] M. Koyachi et al. “Accuracy of Le Fort I osteotomy
with combined computer-aided design/computer-
aided manufacturing technology and mixed real-
ity”. In: International journal of oral and maxillo-
facial surgery 50.6 (2021), pp. 782–790.
[106] F. Kral, E. J. Puschban, H. Riechelmann, and
W. Freysinger. “Comparison of optical and elec-
tromagnetic tracking for navigated lateral skull
base surgery”. In: International Journal of Medi-
cal Robotics and Computer Assisted Surgery 9.2
(2013), pp. 247–252.

[107] P. Kriechling et al. “Augmented reality for base
plate component placement in reverse total shoul-
der arthroplasty: a feasibility study”. In: Archives
of orthopaedic and trauma surgery (2020), pp. 1–
7.

[109]

[108] P. L. Kubben and R. S. Sinlae. “Feasibility of using
a low-cost head-mounted augmented reality device
in the operating room”. In: Surgical neurology in-
ternational 10 (2019).
I. Kuhlemann, M. Kleemann, P. Jauer, A.
Schweikard, and F. Ernst. “Towards X-ray free
endovascular interventions–using HoloLens for
on-line holographic visualisation”. In: Healthcare
technology letters 4.5 (2017), pp. 184–187.
[110] N. Kumar, S. Pandey, and E. Rahman. “A novel
three-dimensional interactive virtual face to fa-
cilitate facial anatomy teaching using Microsoft
HoloLens”. In: Aesthetic Plastic Surgery 45.3
(2021), pp. 1005–1011.

A PREPRINT - SEPTEMBER 8, 2022

[111] R. P. Kumar et al. “Use of mixed reality for surgery
planning: Assessment and development workﬂow”.
In: Journal of Biomedical Informatics 112 (2020),
p. 100077.

[112] C. Kunz et al. “Infrared marker tracking with
the HoloLens for neurosurgical interventions”. In:
Current Directions in Biomedical Engineering 6.1
(2020).

[113] O. Kutter et al. “Real-time volume rendering for
high quality visualization in augmented reality”.
In: Proc. International Workshop on Augmented
environments for Medical Imaging including Aug-
mented Reality in Computer-aided Surgery (AMI-
ARCS). Springer, 2008.

[114] T. Kuzhagaliyev et al. “Augmented reality needle
ablation guidance tool for irreversible electropora-
tion in the pancreas”. In: Medical Imaging 2018:
Image-Guided Procedures, Robotic Interventions,
and Modeling. Vol. 10576. International Society
for Optics and Photonics. 2018, p. 1057613.

[115] C. Leuze, G. Yang, B. Hargreaves, B. Daniel, and
J. A. McNab. “Mixed-reality guidance for brain
stimulation treatment of depression”. In: Inter-
national Symposium on Mixed and Augmented
Reality Adjunct (ISMAR-Adjunct). IEEE. 2018,
pp. 377–380.

[116] C. Li, Y. Zheng, Y. Yuan, and H. Li. “Augmented
reality navigation-guided pulmonary nodule local-
ization in a canine model”. In: Translational Lung
Cancer Research 10.11 (2021), p. 4152.
[117] R. Li et al. “Mixed reality based respiratory liver
tumor puncture navigation”. In: Computational
Visual Media 5.4 (2019), pp. 363–374.

[118] Y. Li et al. “A wearable mixed-reality holographic
computer for guiding external ventricular drain in-
sertion at the bedside”. In: Journal of neurosurgery
131.5 (2018), pp. 1599–1606.

[119] F. Liebmann et al. “Pedicle screw navigation using
surface digitization on the Microsoft HoloLens”.
In: International journal of computer assisted ra-
diology and surgery 14.7 (2019), pp. 1157–1165.
[120] H. Liu et al. “Percutaneous placement of lumbar
pedicle screws via intraoperative CT image–based
augmented reality–guided technology”. In: Jour-
nal of Neurosurgery: Spine 32.4 (2019), pp. 542–
547.
J. Liu et al. “An augmented reality system for
image guidance of transcatheter procedures for
structural heart disease”. In: PloS one 14.7 (2019),
e0219174.

[121]

[122] P. Liu et al. “A wearable augmented reality nav-
igation system for surgical telementoring based
on microsoft HoloLens”. In: Annals of Biomedical
Engineering 49.1 (2021), pp. 287–298.

30

[123] M. A. Livingston, A. Dey, C. Sandor, and B. H.
Thomas. “Pursuit of “X-ray vision” for augmented
reality”. In: Human Factors in Augmented Reality
Environments. Springer, 2013, pp. 67–107.
[124] C. Lohou, B. Miguel, and K. Azarnoush. “Prelimi-
nary experiment of the interactive registration of
a trocar for thoracoscopy with HoloLens headset”.
In: International Conference on Image Analysis
and Processing (ICIAP). Springer. 2019, pp. 694–
703.

[125] D. J. Long et al. “Comparison of smartphone aug-
mented reality, smartglasses augmented reality,
and 3D CBCT-guided ﬂuoroscopy navigation for
percutaneous needle insertion: a phantom study”.
In: CardioVascular and Interventional Radiology
44.5 (2021), pp. 774–781.

[126] W. O. C. López, P. A. Navarro, and S. Crispin.
“Intraoperative clinical application of augmented
reality in neurosurgery: a systematic review”. In:
Clinical neurology and neurosurgery 177 (2019),
pp. 6–11.

[128]

[127] S. Lu, Y. P. S. Perdomo, X. Jiang, and B. Zheng.
“Integrating Eye-Tracking to Augmented Reality
System for Surgical Training”. In: Journal of Med-
ical Systems 44.11 (2020), pp. 1–7.
J. A. Luzon, B. V. Stimec, A. O. Bakka, B. Ed-
win, and D. Ignjatovic. “Value of the surgeon’s
sightline on hologram registration and targeting in
mixed reality”. In: International journal of com-
puter assisted radiology and surgery 15.12 (2020),
pp. 2027–2039.

[129] F. Mahmood et al. “Augmented reality and ultra-
sound education: initial experience”. In: Journal
of cardiothoracic and vascular anesthesia 32.3
(2018), pp. 1363–1367.

[131]

[130] P. Maniam et al. “Exploration of temporal bone
anatomy using mixed reality (HoloLens): develop-
ment of a mixed reality anatomy teaching resource
prototype”. In: Journal of visual communication
in medicine 43.1 (2020), pp. 17–26.
J. C. Mankins et al. “Technology readiness levels”.
In: White Paper, April 6.1995 (1995), p. 1995.
J. L. McJunkin et al. “Development of a mixed
reality platform for lateral skull base anatomy”. In:
Otology & Neurotology: Ofﬁcial Publication of the
American Otological Society, American Neurotol-
ogy Society [and] European Academy of Otology
and Neurotology 39.10 (2018), e1137.

[132]

[133] F.-H. Meng et al. “Feasibility of the applica-
tion of mixed reality in mandible reconstruction
with ﬁbula ﬂap: A cadaveric specimen study”. In:
Journal of Stomatology, Oral and Maxillofacial
Surgery 122.4 (2021), e45–e49.

[134] A. Meola et al. “Augmented reality in neuro-
surgery: a systematic review”. In: Neurosurgical
review 40.4 (2017), pp. 537–548.

A PREPRINT - SEPTEMBER 8, 2022

[135]

J. W. Meulstee et al. “Toward holographic-guided
surgery”. In: Surgical innovation 26.1 (2019),
pp. 86–94.

[136] U. Mezger, C. Jendrewski, and M. Bartels. “Nav-
igation in surgery”. In: Langenbeck’s archives of
surgery 398.4 (2013), pp. 501–514.

[137] P. Milgram and F. Kishino. “A taxonomy of mixed
reality visual displays”. In: IEICE TRANSAC-
TIONS on Information and Systems 77.12 (1994),
pp. 1321–1329.

[138] D. Mitsuno, K. Ueda, Y. Hirota, and M. Ogino.
“Effective application of mixed reality device
HoloLens: simple manual alignment of surgical
ﬁeld and holograms”. In: Plastic and reconstruc-
tive surgery 143.2 (2019), pp. 647–651.
[139] D. Mitsuno et al. “Telementoring demonstration
in craniofacial surgery with HoloLens, Skype, and
three-layer facial models”. In: Journal of Cranio-
facial Surgery 30.1 (2019), pp. 28–32.

[140] D. Moher, A. Liberati, J. Tetzlaff, D. G. Altman, P.
Group, et al. “Preferred reporting items for system-
atic reviews and meta-analyses: the PRISMA state-
ment”. In: PLoS medicine 6.7 (2009), e1000097.

[141] S. Moosburner et al. “Real world usability analy-
sis of two augmented reality headsets in visceral
surgery”. In: Artiﬁcial organs 43.7 (2019), pp. 694–
698.

[142] C. M. Morales Mojica et al. “A holographic aug-
mented reality interface for visualizing of MRI
data and planning of neurosurgical procedures”. In:
Journal of Digital Imaging 34.4 (2021), pp. 1014–
1025.

[143] C. M. Morales Mojica et al. “Holographic Inter-
face for three-dimensional visualization of MRI on
HoloLens: a prototype platform for MRI guided
neurosurgeries”. In: International Conference on
Bioinformatics and Bioengineering (BIBE). IEEE.
2017, pp. 21–27.

[144] R. Moreta-Martinez et al. “Augmented reality in
computer-assisted interventions based on patient-
speciﬁc 3D printed reference”. In: Healthcare tech-
nology letters 5.5 (2018), pp. 162–166.

[145] C. Moro, C. Phelps, P. Redmond, and Z.
Stromberga. “HoloLens and mobile augmented
reality in medical and health science education: A
randomised controlled trial”. In: British Journal of
Educational Technology 52.2 (2021), pp. 680–694.
[146] T. Muangpoon, R. H. Osgouei, D. Escobar-
Castillejos, C. Kontovounisios, and F. Bello. “Aug-
mented Reality System for Digital Rectal Exam-
ination Training and Assessment: System Valida-
tion”. In: Journal of Medical Internet Research
22.8 (2020), e18637.

[147] F. Müller et al. “Augmented reality navigation for
spinal pedicle screw instrumentation using intraop-
erative 3D imaging”. In: The Spine Journal 20.4
(2020), pp. 621–628.

31

[148] N. Q. Nguyen, J. Cardinell, J. M. Ramjist, D. An-
droutsos, and V. X. Yang. “Augmented reality and
human factors regarding the neurosurgical operat-
ing room workﬂow”. In: Optical Architectures for
Displays and Sensing in Augmented, Virtual, and
Mixed Reality (AR, VR, MR). Vol. 11310. SPIE.
2020, pp. 119–125.

[149] N. Q. Nguyen et al. “An augmented reality system
characterization of placement accuracy in neuro-
surgery”. In: Journal of Clinical Neuroscience 72
(2020), pp. 392–396.

[150] T. Nguyen, W. Plishker, A. Matisoff, K. Sharma,
and R. Shekhar. “HoloUS: Augmented reality visu-
alization of live ultrasound images using HoloLens
for ultrasound-guided procedures”. In: Interna-
tional Journal of Computer Assisted Radiology
and Surgery 17.2 (2022), pp. 385–391.

[151] T. Nuri, D. Mitsuno, Y. Otsuki, and K. Ueda. “Aug-
mented Reality Technology for the Positioning of
the Auricle in the Treatment of Microtia”. In: Plas-
tic and Reconstructive Surgery Global Open 8.2
(2020).

[152] F. Palermo, M. Cognolato, I. Eggel, M. Atzori, and
H. Müller. “An augmented reality environment
to provide visual feedback to amputees during
sEMG Data Acquisitions”. In: Annual Conference
Towards Autonomous Robotic Systems (TAROS).
Springer. 2019, pp. 3–14.

[153] V. C. Pandrangi et al. “The application of virtual
reality in patient education”. In: Annals of vascular
surgery 59 (2019), pp. 184–189.

[154] E. Pelanis et al. “Use of mixed reality for improved
spatial understanding of liver anatomy”. In: Mini-
mally Invasive Therapy & Allied Technologies 29.3
(2020), pp. 154–160.

[155] A. Pepe et al. “A marker-less registration approach
for mixed reality–aided maxillofacial surgery: a
pilot evaluation”. In: Journal of digital imaging
32.6 (2019), pp. 1008–1018.

[156] A. Pepe et al. “Pattern recognition and mixed real-
ity for computer-aided maxillofacial surgery and
oncological assessment”. In: Biomedical Engineer-
ing International Conference (BMEiCON). IEEE.
2018, pp. 1–5.

[157] L. Pérez-Pachón et al. “Effect of marker position
and size on the registration accuracy of HoloLens
in a non-clinical setting with implications for high-
precision surgical tasks”. In: International journal
of computer assisted radiology and surgery 16.6
(2021), pp. 955–966.

[158] S. L. Perkins et al. “A mixed-reality system for
breast surgical planning”. In: International Sym-
posium on Mixed and Augmented Reality Adjunct
(ISMAR-Adjunct). IEEE. 2017, pp. 269–274.
[159] S. L. Perkins et al. “A Patient-Speciﬁc Mixed-
Reality Visualization Tool for Thoracic Surgical

A PREPRINT - SEPTEMBER 8, 2022

Planning”. In: The Annals of thoracic surgery
110.1 (2020), pp. 290–295.

[160] P. Pratt et al. “Through the HoloLens™ looking
glass: augmented reality for extremity reconstruc-
tion surgery using 3D vascular models with perfo-
rating vessels”. In: European radiology experimen-
tal 2.1 (2018), pp. 1–7.

[161] K. Proniewska, A. Pr˛egowska, D. Dolega-
Dolegowski, J. Chmiel, and D. Dudek. “Three-
Dimensional Operating Room with Unlimited Per-
spective”. In: International Conference on Mul-
timedia Communications, Services and Security.
Springer. 2020, pp. 351–361.

[162] E. M. Putnam et al. “Virtual reality simulation for
critical pediatric airway management training”. In:
Journal of Clinical and Translational Research 7.1
(2021), p. 93.

[163] Z. Qi et al. “Holographic mixed-reality neuron-
avigation with a head-mounted device: technical
feasibility and clinical application”. In: Neurosur-
gical Focus 51.2 (2021), E22.

[164] L. Qian, A. Deguet, and P. Kazanzides. “ARssist:
augmented reality on a head-mounted display for
the ﬁrst assistant in robotic surgery”. In: Health-
care technology letters 5.5 (2018), pp. 194–200.

[165] L. Qian, J. Y. Wu, S. P. DiMaio, N. Navab, and
P. Kazanzides. “A review of augmented reality in
robotic-assisted surgery”. In: IEEE Transactions
on Medical Robotics and Bionics 2.1 (2019), pp. 1–
16.

[166] L. Qian, X. Zhang, A. Deguet, and P. Kazanzides.
“Aramis: Augmented reality assistance for mini-
mally invasive surgery using a head-mounted dis-
play”. In: International Conference on Medical
Image Computing and Computer-Assisted Inter-
vention (MICCAI). Springer. 2019, pp. 74–82.

[167] L. Qian et al. “Comparison of optical see-through
head-mounted displays for surgical interventions
with object-anchored 2D-display”. In: Interna-
tional journal of computer assisted radiology and
surgery 12.6 (2017), pp. 901–910.

[168] L. Qian et al. “FlexiVision: Teleporting the sur-
geon’s eyes via robotic ﬂexible endoscope and
head-mounted display”. In: International Confer-
ence on Intelligent Robots and Systems (IROS).
IEEE. 2020, pp. 3281–3287.

[169] E. Rae et al. “Neurosurgical burr hole placement
using the Microsoft HoloLens”. In: Medical Imag-
ing 2018: Image-Guided Procedures, Robotic In-
terventions, and Modeling. Vol. 10576. Interna-
tional Society for Optics and Photonics. 2018,
105760T.

[170] R. Rahman et al. “Head-mounted display use in
surgery: a systematic review”. In: Surgical innova-
tion 27.1 (2020), pp. 88–100.

32

[171] N. Rewkowski, A. State, and H. Fuchs. “Small
Marker Tracking with Low-Cost, Unsynchronized,
Movable Consumer Cameras For Augmented Re-
ality Surgical Training”. In: International Sympo-
sium on Mixed and Augmented Reality Adjunct
(ISMAR-Adjunct). IEEE. 2020, pp. 90–95.
[172] D. W. Roberts, J. W. Strohbehn, J. F. Hatch, W.
Murray, and H. Kettenberger. “A frameless stereo-
taxic integration of computerized tomographic
imaging and the operating microscope”. In: Jour-
nal of neurosurgery 65.4 (1986), pp. 545–549.

[173] B. L. Robinson, T. R. Mitchell, and B. M.
Brenseke. “Evaluating the Use of Mixed Real-
ity to Teach Gross and Microscopic Respiratory
Anatomy”. In: Medical Science Educator 30.4
(2020), pp. 1745–1748.

[174] N. Rohrbach et al. “An augmented reality ap-
proach for ADL support in Alzheimer’s disease: a
crossover trial”. In: Journal of neuroengineering
and rehabilitation 16.1 (2019), pp. 1–11.
[175] A. S. Rose, H. Kim, H. Fuchs, and J.-M. Frahm.
“Development of augmented-reality applications in
otolaryngology–head and neck surgery”. In: The
Laryngoscope 129 (2019), S1–S11.

[176] H. Rositi et al. “Presentation of a mixed reality
software with a HoloLens headset for a nutrition
workshop”. In: Multimedia tools and applications
80.2 (2021), pp. 1945–1967.
J. C. Rosser et al. “The impact of video games on
training surgeons in the 21st century”. In: Archives
of surgery 142.2 (2007), pp. 181–186.

[177]

[178] C. Rüger et al. “Ultrasound in augmented real-
ity: a mixed-methods evaluation of head-mounted
displays in image-guided interventions”. In: Inter-
national Journal of Computer Assisted Radiology
and Surgery 15.11 (2020), pp. 1895–1905.
J. S. Ruthberg et al. “Mixed reality as a time-
efﬁcient alternative to cadaveric dissection”. In:
Medical teacher 42.8 (2020), pp. 896–901.
[180] Y. Saito et al. “Intraoperative 3D hologram support
with mixed reality techniques in liver surgery”. In:
Annals of surgery 271.1 (2020), e4–e7.

[179]

[181] Y. Saito et al. “Intraoperative support with three-
dimensional holographic cholangiography in hep-
atobiliary surgery”. In: Langenbeck’s Archives of
Surgery 407.3 (2022), pp. 1285–1289.

[182] F. Sauer, A. Khamene, B. Bascle, and G. J. Rubino.
“A head-mounted display system for augmented re-
ality image guidance: Towards clinical evaluation
for imri-guided neurosurgery”. In: Lecture Notes
in Computer Science 2208 (2001), pp. 0707–0707.
I. M. Sauer et al. “Mixed reality in visceral surgery:
development of a suitable workﬂow and evaluation
of intraoperative use-cases”. In: Annals of surgery
266.5 (2017), pp. 706–712.

[183]

A PREPRINT - SEPTEMBER 8, 2022

[184] C. Scherl et al. “Augmented reality with HoloLens
in parotid surgery: how to assess and to improve
accuracy”. In: European Archives of Oto-Rhino-
Laryngology 278.7 (2021), pp. 2473–2483.
[185] C. Scherl et al. “Augmented reality with
HoloLens® in parotid tumor surgery: a prospective
feasibility study”. In: ORL 83.6 (2021), pp. 439–
448.

[186] M. Schneider et al. “Augmented reality–assisted
ventriculostomy”. In: Neurosurgical focus 50.1
(2021), E16.

[187] D. S. Schoeb et al. “Mixed reality for teaching
catheter placement to medical students: a random-
ized single-blinded, prospective trial”. In: BMC
medical education 20.1 (2020), pp. 1–8.
[188] A. Sharma et al. “A mixed-reality training environ-
ment for upper limb prosthesis control”. In: 2018
IEEE Biomedical Circuits and Systems Conference
(BioCAS). IEEE. 2018, pp. 1–4.

[189] T. Sielhorst, M. Feuerstein, and N. Navab. “Ad-
vanced medical displays: A literature review of
augmented reality”. In: Journal of Display Tech-
nology 4.4 (2008), pp. 451–467.

[190] S. Sirilak and P. Muneesawang. “A new procedure
for advancing telemedicine using the HoloLens”.
In: IEEE Access 6 (2018), pp. 60224–60233.

[191] H. Y. So, P. P. Chen, G. K. C. Wong, and T. T. N.
Chan. “Simulation in medical education”. In: Jour-
nal of the Royal College of Physicians of Edin-
burgh 49.1 (2019), pp. 52–57.

[192] K. B. Soulami et al. “Mixed-reality aided system
for glioblastoma resection surgery using microsoft
HoloLens”. In: International Conference on Elec-
tro Information Technology (EIT). IEEE. 2019,
pp. 079–084.

[193] M. K. Southworth et al. “Performance evaluation
of mixed reality display for guidance during tran-
scatheter cardiac mapping and ablation”. In: Jour-
nal of Translational Engineering in Health and
Medicine 8 (2020), pp. 1–10.
J. M. Spirig, S. Roner, F. Liebmann, P. Fürn-
stahl, and M. Farshad. “Augmented reality-
navigated pedicle screw placement: a cadaveric
pilot study”. In: European Spine Journal 30.12
(2021), pp. 3731–3737.

[194]

[195] A. State et al. “Technologies for augmented reality
systems: realizing ultrasound-guided needle biop-
sies”. In: Conference on computer graphics and
interactive techniques. 1996, pp. 439–446.
[196] M. Stojanovska et al. “Mixed reality anatomy us-
ing Microsoft HoloLens and cadaveric dissection:
a comparative effectiveness study”. In: Medical
Science Educator (2019), pp. 1–6.

[197] K. Sugahara et al. “Mixed reality and three dimen-
sional printed models for resection of maxillary
tumor: a case report”. In: Quantitative imaging in
medicine and surgery 11.5 (2021), p. 2187.

33

[198] Q. Sun et al. “Fast and accurate online calibra-
tion of optical see-through head-mounted display
for AR-based surgical navigation using Microsoft
HoloLens”. In: International Journal of Computer
Assisted Radiology and Surgery 15.11 (2020),
pp. 1907–1919.

[199] R. Sun, R. G. Aldunate, and J. J. Sosnoff. “The
validity of a mixed reality-based automated func-
tional mobility assessment”. In: Sensors 19.9
(2019), p. 2183.

[200] K. Suzuki et al. “Learning effectiveness of using
augmented reality technology in central venous ac-
cess procedure: an experiment using phantom and
head-mounted display”. In: International Journal
of Computer Assisted Radiology and Surgery 16.6
(2021), pp. 1069–1074.

[201] M. Sylos Labini et al. “Depth-awareness in a sys-
tem for mixed-reality aided surgical procedures”.
In: International conference on intelligent comput-
ing. Springer. 2019, pp. 716–726.

[202] S. Talaat et al. “Three-dimensional evaluation of
the holographic projection in digital dental model
superimposition using HoloLens device”. In: Or-
thodontics & craniofacial research 22 (2019),
pp. 62–68.

[203] X. Tian et al. “Validation and Precision of Mixed
Reality Technology in Baha Attract Implant
Surgery”. In: Otology & Neurotology 41.9 (2020),
pp. 1280–1287.

[204] G. Tieri, G. Morone, S. Paolucci, and M. Iosa. “Vir-
tual reality in cognitive and motor rehabilitation:
facts, ﬁction and fallacies”. In: Expert review of
medical devices 15.2 (2018), pp. 107–117.
[205] T. P. Van Doormaal, J. A. Van Doormaal,
and T. Mensink. “Clinical accuracy of holo-
graphic navigation using point-based registration
on augmented-reality glasses”. In: Operative Neu-
rosurgery 17.6 (2019), pp. 588–593.

[206] F. Van Gestel et al. “Augmented reality-assisted
neurosurgical drain placement (ARANED)”. In:
Intracranial Pressure and Neuromonitoring XVII.
Springer, 2021, pp. 267–273.

[207] F. Van Gestel et al. “The effect of augmented real-
ity on the accuracy and learning curve of external
ventricular drain placement”. In: Neurosurgical
focus 51.2 (2021), E8.

[209]

[208] P. Vávra et al. “Recent development of augmented
reality in surgery: a review”. In: Journal of health-
care engineering 2017 (2017).
J. D. Velazco-Garcia, D. J. Shah, E. L. Leiss, and
N. V. Tsekos. “A modular and scalable computa-
tional framework for interactive immersion into
imaging data with a holographic augmented reality
interface”. In: Computer Methods and Programs
in Biomedicine 198 (2021), p. 105779.

A PREPRINT - SEPTEMBER 8, 2022

[210]

J. D. Velazco-Garcia et al. “Evaluation of how
users interface with holographic augmented reality
surgical scenes: Interactive planning MR-Guided
prostate biopsies”. In: The International Journal of
Medical Robotics and Computer Assisted Surgery
17.5 (2021), e2290.

[211] N. Wake et al. “A workﬂow to generate patient-
speciﬁc three-dimensional augmented reality mod-
els from medical imaging data and example appli-
cations in urologic oncology”. In: 3D Printing in
Medicine 7.1 (2021), pp. 1–11.

[212] N. Wake et al. “Patient-speciﬁc 3D printed and
augmented reality kidney and prostate cancer mod-
els: impact on patient education”. In: 3D printing
in medicine 5.1 (2019), pp. 1–8.

[213] L. Wang, Z. Sun, X. Zhang, Z. Sun, and J. Wang.
“A HoloLens based augmented reality navigation
system for minimally invasive total knee arthro-
plasty”. In: International Conference on Intelli-
gent Robotics and Applications. Springer. 2019,
pp. 519–530.

[216]

[214] S. Wang et al. “Augmented reality as a
telemedicine platform for remote procedural train-
ing”. In: Sensors 17.10 (2017), p. 2294.
[215] T. S. Wesselius et al. “Holographic augmented
reality for DIEP ﬂap harvest”. In: Plastic and Re-
constructive Surgery 147.1 (2021), 25e–29e.
J. Witowski et al. “Augmented reality and three-
dimensional printing in percutaneous interventions
on pulmonary arteries”. In: Quantitative imaging
in medicine and surgery 9.1 (2019), p. 23.
[217] M.-L. Wu, J.-C. Chien, C.-T. Wu, and J.-D. Lee.
“An augmented reality system using improved-
iterative closest point algorithm for on-patient med-
ical image visualization”. In: Sensors 18.8 (2018),
p. 2505.

[218] T. Xie, M. M. Islam, A. B. Lumsden, and I. A.
Kakadiaris. “Holographic iRay: exploring augmen-
tation for medical applications”. In: International
Symposium on Mixed and Augmented Reality Ad-
junct (ISMAR-Adjunct). Institute of Electrical and
Electronics Engineers Inc. 2017, pp. 220–222.

[219] A. Yamashita, K. Sato, S. Sato, and K. Matsub-
ayashi. “Pedestrian navigation system for visually
impaired people using hololens and RFID”. In:
Conference on Technologies and Applications of
Artiﬁcial Intelligence (TAAI). IEEE. 2017, pp. 130–
135.

[220] K. Yammine and C. Violato. “A meta-analysis of
the educational effectiveness of three-dimensional
visualization technologies in teaching anatomy”.
In: Anatomical sciences education 8.6 (2015),
pp. 525–538.
J. W. Yoon et al. “Augmented reality for the sur-
geon: systematic review”. In: The International
Journal of Medical Robotics and Computer As-
sisted Surgery 14.4 (2018), e1914.

[221]

34

A PREPRINT - SEPTEMBER 8, 2022

[222] M. Zeller, E. Miller, and S. Paniagua. HoloLens
(1st gen) hardware. https://docs.microsoft.
com / en - us / hololens / hololens1 -
hardware/. [Online; accessed April 2021]. 2019.
[223] F. Zhou, H. B.-L. Duh, and M. Billinghurst.
“Trends in augmented reality tracking, interaction
and display: A review of ten years of ISMAR”.
In: International Symposium on Mixed and Aug-
mented Reality (ISMAR). IEEE. 2008, pp. 193–
202.

[224] Y. Zuo et al. “A novel evaluation model for a
mixed-reality surgical navigation system: where
microsoft hololens meets the operating room”. In:
Surgical innovation 27.2 (2020), pp. 193–202.

35

