6
1
0
2

r
p
A
6
2

]

G
L
.
s
c
[

1
v
4
8
4
7
0
.
4
0
6
1
:
v
i
X
r
a

DEEP MULTI-FIDELITY GAUSSIAN PROCESSES

MAZIAR RAISSI & GEORGE KARNIADAKIS

Abstract. We develop a novel multi-ﬁdelity framework that goes far beyond the classical AR(1)
Co-kriging scheme of Kennedy and O’Hagan (2000). Our method can handle general discontinuous
cross-correlations among systems with diﬀerent levels of ﬁdelity. A combination of multi-ﬁdelity
Gaussian Processes (AR(1) Co-kriging) and deep neural networks enables us to construct a method
that is immune to discontinuities. We demonstrate the eﬀectiveness of the new technology using
standard benchmark problems designed to resemble the outputs of complicated high- and low-ﬁdelity
codes.

Key words. Machine learning; deep nets; discontinuous correlations; Co-kriging; manifold

Gaussian processes;

1. Motivation. Multi-ﬁdelity modeling proves extremely useful while solving
inverse problems for instance. Inverse problems are ubiquitous in science. In general,
the response of a system is modeled as a function y = g(x). The goal of model
inversion is to ﬁnd a parameter setting x that matches a target response y∗ = g(x∗).
In other words, we are solving the following optimization problem:

||g(x) − y∗||,

min
x

for some suitable norm. In practice, x is often a high-dimensional vector and g is a
complex, non-linear, and expensive to compute map. These factors render the solution
of the optimization problem very challenging and motivate the use of surrogate models
as a remedy for obtaining inexpensive samples of g at unobserved locations. To this
end, a surrogate model acts as an intermediate agent that is trained on available
realizations of g, and then is able to perform accurate predictions for the response
at a new set of inputs. Multi-ﬁdelity framework can be employed to build eﬃcient
surrogate models of g. Our Deep Multi-ﬁdelity GP algorithm is most useful when
the function g is very complicated, involves discontinuities, and when the correlation
structures between diﬀerent levels of ﬁdelity have discontinuous nonfunctional forms.

2. Introduction. Using deep neural networks, we build a multi-ﬁdelity model
that is immune to discontinuities. We employ Gaussian Processes (GPs) (see [5])
which is a non-parametric Bayesian regression technique. Gaussian Processes is a
very popular and useful tool to approximate an objective function given some of its
observations. It corresponds to a particular class of surrogate models which makes
the assumption that the response of the complex system is a realization of a Gaussian
process. In particular, we are interested in Manifold Gaussian Processes [1] that are
capable of capturing discontinuities. Manifold GP is equivalent to jointly learning a
data transformation into a feature space followed by a regular GP regression. The
model proﬁts from standard GP properties. We show that the well-known classi-
cal multi-ﬁdelity Gaussian Processes (AR(1) Co-kriging) [4] is a special case of our
method. Multi-ﬁdelity modeling is most useful when low-ﬁdelity versions of a com-
plex system are available. They may be less accurate but are computationally cheaper.

For the sake of clarity of presentation, we focus only on two levels of ﬁdelity.
However, our method can be readily generalized to multiple levels of ﬁdelity.
In
the following, we assume that we have access to data with two levels of ﬁdelity
{{x1, f1}, {x2, f2}}, where f2 has a higher level of ﬁdelity. We use n1 to denote
1

 
 
 
 
 
 
the number of observations in x1 and n2 to denote the sample size of x2. The main
assumption is that n2 < n1. This is to reﬂect the fact that high-ﬁdelity data are
scarce since they are generated by an accurate but costly process. The low ﬁdelity
data, on the other hand, are less accurate, cheap to generate and hence are abundant.

As for the notation, we employ the following convention. A boldface letter such
as x is used to denote data. A non-boldface letter such as h is used to denote both a
vector or a scalar. This will be clear from the context.

3. Deep Multi-ﬁdelity Gaussian Processes. A simple way to explain the

main idea of this work is to consider the following structure:

(cid:21)

(cid:20) f1(h)
f2(h)

∼ GP

(cid:18)(cid:20) 0
0

(cid:21)

,

where

(cid:20) k1(h, h(cid:48))

ρk1(h, h(cid:48))

ρk1(h, h(cid:48)) ρ2k1(h, h(cid:48)) + k2(h, h(cid:48))

(cid:21)(cid:19)

,

(3.1)

x (cid:55)−→ h := h(x) (cid:55)−→

(cid:20) f1(h(x))
f2(h(x))

(cid:21)

.

The high ﬁdelity system is modeled by f2(h(x)) and the low ﬁdelity one by f1(h(x)).
We use GP to denote a Gaussian Process. This approach can use any determinis-
tic parametric data transformation h(x). However, we focus on multi-layer neural
networks

h(x) := (hL ◦ . . . ◦ h1)(x),

where each layer of the network performs the transformation

h(cid:96)(z) = σ(cid:96)(w(cid:96)z + b(cid:96)),

with σ(cid:96) being the transfer function, w(cid:96) the weights, and b(cid:96) the bias of the layer.
We use θh := [w1, b1, . . . , wL, bL] to denote the parameters of the neural network.
Moreover, θ1 and θ2 denote the hyper-parameters of the covariance functions k1 and
k2, respectively. The parameters of the model are therefore given by

θ := [ρ, θ1, θ2, θh].

It should be noted that the AR(1) Co-kriging model of [4] is a special case of our
model in the sense that for AR(1) Co-kriging h = h(x) = x.

3.1. AR(1) Co-kriging. In [4], the authors consider the following autoregres-

sive model

f2(x) = ρf1(x) + δ2(x),

where δ2(x) and f1(x) are two independent Gaussian Processes with

and

δ2(x) ∼ GP(0, k2(x, x(cid:48))),

f1(x) ∼ GP(0, k1(x, x(cid:48))).
2

Therefore,

and

f2(x) ∼ GP(0, ρ2k1(x, x(cid:48)) + k2(x, x(cid:48))),

(cid:21)

(cid:20) f1(x)
f2(x)

∼ GP

(cid:18)(cid:20) 0
0

(cid:21)

,

(cid:20) k1(x, x(cid:48))

ρk1(x, x(cid:48)) ρ2k1(x, x(cid:48)) + k2(x, x(cid:48))

ρk1(x, x(cid:48))

(cid:21)(cid:19)

,

(3.2)

which is a special case of (3.1) with h = h(x) = x. The importance of ρ is evident
from (3.2). If ρ = 0, the high ﬁdelity and low ﬁdelity models are fully decoupled and
by combining there will be no improvements of the prediction.

4. Prediction. The Deep Multi-ﬁdelity Gaussian Process structure (3.1) can be
equivalently written in the following compact form of a multivariate Gaussian Process

(cid:21)

(cid:20) f1(h)
f2(h)

∼ GP

(cid:18)(cid:20) 0
0

(cid:21)

(cid:20) k11(h, h(cid:48)) k12(h, h(cid:48))
k21(h, h(cid:48)) k22(h, h(cid:48))

,

(cid:21)(cid:19)

(4.1)

with k11 ≡ k1, k12 ≡ k21 ≡ ρk1, and k22 ≡ ρ2k1 + k2. This can be used to obtain the
predictive distribution

p (f2(h(x∗))|x∗, x1, f1, x2, f2)

of the surrogate model for the high ﬁdelity system at a new test point x∗ (see equation
(4.2)). Note that the terms k12(h(x), h(x(cid:48))) and k21(h(x), h(x(cid:48))) model the correlation
between the high-ﬁdelity and the low-ﬁdelity data and therefore are of paramount im-
portance. The key role played by ρ is already well-known in the literature [4]. Along
the same lines one can easily observe the eﬀectiveness of learning the transformation
function h(x) jointly from the low ﬁdelity and high ﬁdelity data.

We obtain the following joint density:





f2(h(x∗))
f1
f2







 ∼ N









 ,



0
0
0

k22(h∗, h∗)
k22(h∗, h2)
k21(h∗, h1)
k12(h1, h∗) k11(h1, h1) k12(h1, h2)
k22(h2, h∗) k21(h2, h1) k22(h2, h2)







 ,

where h∗ = h(x∗), h1 = h(x1), and h2 = h(x2). From this, we conclude that
(cid:1) ,
p (f2(h(x∗))|x∗, x1, f1, x2, f2) = N (cid:0)K∗K −1f , k22(h∗, h∗) − K∗K −1K T

∗

where

f :=

(cid:20) f1
f2

(cid:21)

,

K∗ := (cid:2) k21(h∗, h1) k22(h∗, h2) (cid:3) ,
(cid:20) k11(h1, h1) k12(h1, h2)
(cid:21)
k21(h2, h1) k22(h2, h2)

K :=

.

(4.2)

(4.3)

(4.4)

(4.5)

5. Training. The Negative Marginal Log Likelihood L(θ) := − log p (f |x) is

given by

L(θ) =

1
2

f T K −1f +

1
2

log |K| +

3

n1 + n2
2

log 2π,

(5.1)

where

x :=

(cid:20) x1
x2

(cid:21)

.

The Negative Marginal Log Likelihood along with its Gradient can be used to estimate
the parameters θ. Finding the gradient ∂L(θ)
is discussed in the following. First
∂θ
observe that

∂L(θ)
∂K

= −

1
2

K −1f f T K −1 +

1
2

K −1.

Therefore,

and

∂L(θ)
∂ρ
∂L(θ)
∂θ1
∂L(θ)
∂θ2

=

=

=

∂L(θ)
∂K
∂L(θ)
∂K
∂L(θ)
∂K

∂K
∂ρ
∂K
∂θ1
∂K
∂θ2

,

∂L(θ)
∂θh

=

∂L(θ)
∂K

∂K
∂h

∂h
∂θh

,

(5.2)

(5.3)

where h = h(x). We use backpropagation to ﬁnd ∂h
. Backpropagation is a popular
∂θh
method of training artiﬁcial neural networks. With this method one can calculate the
gradients of h with respect to all the parameters θh in the network.

6. Summary of the Algorithm. The following summarizes our Deep Multi-

ﬁdelity GP algorithm.

• First, we employ the Negative Marginal Log Likelihood (see eq. 5.1) to train
the parameters and hyper-parameters of the model using the low and high-
ﬁdelity data {x, f }. We are therefore jointly training the neural network h(x)
and the kernels k1 and k2 introduced in eq. 3.1.

• Then, use eq. 4.2 to predict the the output of the high-ﬁdelity function at a

new test point x∗.

7. Numerical Experiments. To demonstrate the eﬀectiveness of our proposed
method, we apply our Deep Multi-ﬁdelity Gaussian Processes algorithm to the fol-
lowing challenging benchmark problems.

7.1. Step Function. The high ﬁdelity data is generated by the following step

function

where

y2 = f2(x) + (cid:15)2,

(cid:15)2 ∼ N (0, 0.012)

f2(x) =

(cid:26) −1 if 0 ≤ x ≤ 1
if 1 < x ≤ 2

2

,

4

and the low ﬁdelity data are generated by

y1 = f1(x) + (cid:15)1,

(cid:15)1 ∼ N (0, 0.012)

where

f1(x) =

(cid:26) 0 if 0 ≤ x ≤ 1
1 if 1 < x ≤ 2

.

In order to generate the training data, we pick 50 + 100 + 50 uniformly distributed
random points from the interval [0, 2] = [0, 0.8] ∪ [0.8, 1.2] ∪ [1.2, 2]. Out of these
200 points, n1 = 45 are chosen at random to constitute x1 and n2 = 5 are picked
at random to create x2. We therefore obtain the dataset {{x1, y1}, {x2, y2}}. This
dataset is depicted in ﬁgure 7.1.

Fig. 7.1. Low-ﬁdelity and High-ﬁdelity dataset {{x1, y1}, {x2, y2}}

We use a multi-layer neural network of [3 − 2] neurons. This means that h : R → R2
is given by

h(x) = h2(h1(x)).

Moreover, h1 : R → R3 is given by h1(x) = σ(w1x + b1) with σ(z) = 1/(1 + e−z) being
the Sigmoid function. Furthermore, h2 : R3 → R2 is given by h2(z) = w2z + b2.

As for the kernels k1 and k2, we use the squared exponential covariance functions
with Automatic Relevance Determination (ARD) (see [5]) of the form

(cid:32)

k(x, x(cid:48)) = σ2

f exp

−

1
2

D
(cid:88)

d=1

(cid:18) xd − x(cid:48)
(cid:96)d

d

(cid:19)2(cid:33)

.

5

-1.5-1-0.5 0 0.5 1 1.5 2 2.5 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2yxLow and High Fidelity DataLow FidelityHigh FidelityLow Fidelity DataHigh Fidelity DataThe predictive mean and two standard deviation bounds for our Deep Multi-ﬁdelity
Gaussian Processes method is depicted in ﬁgure 7.2.

Fig. 7.2. Deep Multi-ﬁdelity Gaussian Processes predicive mean and two standard deviations

The 2D feature space discovered by the nonlinear mapping h is depicted in ﬁgure 7.3.
Recall that, for this example, we have h : R → R2.

The discontinuity of the model is captured by the non-linear mapping h. Therefore,
the mapping from the feature space to outputs is smooth and can be easily handled
by a regular AR(1) Co-kriging model. In order to see the importance of the mapping
h, let us compare our method with AR(1) Co-kriging. This is depicted in ﬁgure 7.4.

7.2. Forrester Function [3] with Jump. The low ﬁdelity data are generated

by

f1(x) =

(cid:26) 0.5(6x − 2)2 sin(12x − 4) + 10(x − 0.5) − 5

3 + 0.5(6x − 2)2 sin(12x − 4) + 10(x − 0.5) − 5

0 ≤ x ≤ 0.5
0.5 ≤ x ≤ 1

,

and the high ﬁdelity data are generated by

f1(x) =

(cid:26) 2f1(x) − 20x + 20

4 + 2f1(x) − 20x + 20

0 ≤ x ≤ 0.5
0.5 ≤ x ≤ 1

.

In order to generate the training data, we pick 50 + 100 + 50 uniformly distributed
random points from the interval [0, 1] = [0, 0.4] ∪ [0.4, 0.6] ∪ [0.6, 1]. Out of these
200 points, n1 = 50 are chosen at random to constitute x1 and n2 = 5 are picked
at random to create x2. We therefore obtain the dataset {{x1, f1}, {x2, f2}}. This
6

-1.5-1-0.5 0 0.5 1 1.5 2 2.5 0 0.5 1 1.5 2yxPredictionTwo Standard Deviation BandPosterior MeanHigh Fidelity DataLow Fidelity DataLow Fidelity - ExactHigh Fidelity - ExactFig. 7.3. The 2D feature space discovered by the nonlinear mapping h. Notice that h : R → R2.

Latent dimensions 1 and 2 correspond to the ﬁrst and second dimensions of h(R).

Fig. 7.4. AR(1) Co-kriging predicive mean and two standard deviations

dataset is depicted in ﬁgure 7.5.

7

-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1 1.2 0 0.5 1 1.5 2hxLearned mapping hLatent Dimension 1Latent Dimension 2-5-4-3-2-1 0 1 2 3 0 0.5 1 1.5 2yxPredictionTwo Standard Deviation BandPosterior MeanHigh Fidelity DataLow Fidelity DataLow Fidelity - ExactHigh Fidelity - ExactFig. 7.5. Low-ﬁdelity and High-ﬁdelity dataset {{x1, f1}, {x2, f2}}

Figure 7.6 depicts the relation between the low ﬁdelity and the high ﬁdelity data
generating processes. One should notice the discontinuous and non-functional form
of this relation.

Fig. 7.6. Relation between the Low-ﬁdelity and High-ﬁdelity data generating processes.

8

-10-5 0 5 10 15 20 25 30 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1yxLow and High Fidelity DataLow FidelityHigh FidelityLow Fidelity DataHigh Fidelity Data-5 0 5 10 15 20 25 30-10-5 0 5 10 15High FidelityLow FidelityLow versus High FidelityOur choice of the neural network and covariance functions is as before. The predic-
tive mean and two standard deviation bounds for our Deep Multi-ﬁdelity Gaussian
Processes method is depicted in ﬁgure 7.7.

Fig. 7.7. Deep Multi-ﬁdelity Gaussian Processes predicive mean and two standard deviations

The 2D feature space discovered by the nonlinear mapping h is depicted in ﬁgure 7.8.

Once again, the discontinuity of the model is captured by the non-linear mapping h.
In order to see the importance of the mapping h, let us compare our method with
AR(1) Co-kriging. This is depicted in ﬁgure 7.9.

7.3. A Sample Function. The main objective of this section is to demonstrate
the types of cross-correlation structures that our framework is capable of handling.
In the following, let the true mapping h : R → R2 be given by

h(x) =






(cid:20) x
x

(cid:21)

,

0 ≤ x ≤ 0.5

(cid:21)

(cid:20) x
2x

, 0.5 ≤ x ≤ 1

.

This is plotted in ﬁgure 7.10.

Given ρ = 1, we generate a sample of the joint prior distribution 3.1. This gives us
two sample functions f1(x) and f2(x), where f2(x) is the high-ﬁdelity one. In order
to generate the training data, we pick 50 + 100 + 50 uniformly distributed random
points from the interval [0, 1] = [0, 0.4] ∪ [0.4, 0.6] ∪ [0.6, 1]. Out of these 200 points,

9

-10-5 0 5 10 15 20 25 30 0 0.2 0.4 0.6 0.8 1yxPredictionTwo Standard Deviation BandPosterior MeanHigh Fidelity DataLow Fidelity DataLow Fidelity - ExactHigh Fidelity - ExactFig. 7.8. The 2D feature space discovered by the nonlinear mapping h. Notice that h : R → R2.

Latent dimensions 1 and 2 correspond to the ﬁrst and second dimensions of h(R).

Fig. 7.9. AR(1) Co-kriging predicive mean and two standard deviations

n1 = 50 are chosen at random to constitute x1 and n2 = 15 are picked at random
to create x2. We therefore obtain the dataset {{x1, f1}, {x2, f2}}. This dataset is
10

-1-0.5 0 0.5 1 1.5 2 2.5 0 0.2 0.4 0.6 0.8 1hxLearned mapping hLatent Dimension 1Latent Dimension 2-10-5 0 5 10 15 20 25 30 0 0.2 0.4 0.6 0.8 1yxPredictionTwo Standard Deviation BandPosterior MeanHigh Fidelity DataLow Fidelity DataLow Fidelity - ExactHigh Fidelity - ExactFig. 7.10. The true mapping h : R → R2.

depicted in ﬁgure 7.11.

Fig. 7.11. Low-ﬁdelity and High-ﬁdelity dataset {{x1, f1}, {x2, f2}}

Figure 7.12 depicts the relation between the low ﬁdelity and the high ﬁdelity data
generating processes. One should notice the discontinuous and non-functional form
of this relation.

Our choice of the neural network and covariance functions is as before. The predic-
tive mean and two standard deviation bounds for our Deep Multi-ﬁdelity Gaussian
Processes method is depicted in ﬁgure 7.13.

The 2D feature space discovered by the nonlinear mapping h is depicted in ﬁgure 7.14.
One should notice the discrepancy between the true mapping h and the one learned

11

Fig. 7.12. Relation between the Low-ﬁdelity and High-ﬁdelity data generating processes.

Fig. 7.13. Deep Multi-ﬁdelity Gaussian Processes predicive mean and two standard deviations

by our algorithm. This discrepancy reﬂects the fact that the mapping from x to the
feature space is not necessarily unique.

Once again, the discontinuity of the model is captured by the non-linear mapping h.
In order to see the importance of the mapping h, let us compare our method with
AR(1) Co-kriging. This is depicted in ﬁgure 7.15.

8. Conclusion. We devised a surrogate model that is capable of capturing gen-
eral discontinuous correlation structures between the low- and high-ﬁdelity data gen-
erating processes. The model’s eﬃciency in handling discontinuities was demonstrated
using benchmark problems. Essentially, the discontinuity is captured by the neural

12

Fig. 7.14. The 2D feature space discovered by the nonlinear mapping h. Notice that h : R → R2.

Latent dimensions 1 and 2 correspond to the ﬁrst and second dimensions of h(R).

Fig. 7.15. AR(1) Co-kriging predicive mean and two standard deviations

network. The abundance of low-ﬁdelity data allows us to train the network accurately.
We therefore need very few observations of the high-ﬁdelity data generating process.

A major drawback of our method could be its overconﬁdence which stems from
the fact that, unlike Gaussian Processes, neural networks are not capable of modeling
uncertainty. Modeling the data transformation function h as a Gaussian Process,
instead of a neural network, might be a more proper way of modeling uncertainty.
However, this becomes analytically intractable and more challenging. This could be
a promising subject of future research. A good reference in this direction is [2].

13

Acknowledgments. This work was supported by the DARPA project on Scal-
able Framework for Hierarchical Design and Planning under Uncertainty with Appli-
cation to Marine Vehicles (N66001-15-2-4055).

REFERENCES

[1] Roberto Calandra, Jan Peters, Carl Edward Rasmussen, and Marc Peter Deisenroth. Manifold

gaussian processes for regression. arXiv preprint arXiv:1402.5876, 2014.

[2] Andreas Damianou. Deep Gaussian processes and variational propagation of uncertainty. PhD

thesis, University of Sheﬃeld, 2015.

[3] Alexander IJ Forrester, Andr´as S´obester, and Andy J Keane. Multi-ﬁdelity optimization via
surrogate modelling. In Proceedings of the royal society of london a: mathematical, physical
and engineering sciences, volume 463, pages 3251–3269. The Royal Society, 2007.
[4] Kennedy, Marc C and O’Hagan, Anthony. Predicting the output from a complex computer code

when fast approximations are available. Biometrika, 87(1):1–13, 2000.

[5] Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning.

the MIT Press, 2(3):4, 2006.

14

