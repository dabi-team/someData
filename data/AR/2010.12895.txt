0
2
0
2

t
c
O
4
2

]
T
S
.
h
t
a
m

[

1
v
5
9
8
2
1
.
0
1
0
2
:
v
i
X
r
a

Some Theoretical Results Concerning Time-varying
Nonparametric Regression with Local Stationary
Regressors and Error

Jiyanglin Li and Tao Li∗

School of Statistics and management, Shanghai University of Finance and
Economics, Shanghai, China

Abstract

With regard to a three-step estimation procedure, proposed without theoretical
discussion by Li and You in Journal of Applied Statistics and Management, for a non-
parametric regression model with time-varying regression function, local stationary
regressors and time-varying AR(p) (tvAR(p)) error process , we established all nec-
essary asymptotic properties for each of estimator. We derive the convergence rate
and asymptotic normality of the preliminary estimation of nonparametric regression
function, establish the asymptotic distribution of time-varying coeﬃcient functions in
the error term, and present the asymptotic property of the reﬁned estimation of non-
parametric regression function. In addition, with regard to the ULASSO method for
variable selection and constant coeﬃcient detection for error term structure, we show
that the ULASSO estimator can identify the true error term structure consistently.
We conduct two simulation studies to illustrate the ﬁnite sample performance of the
estimators and validate our theoretical discussion on the properties of the estimators.

Keywords: time-varying autoregressive, locally stationary, local linear regression, ULASSO,
BIC

1

Introduction

The nonparametric regression model has played a prominent role in ﬁnance and economics
analysis due to its capability of capturing the nonlinear relationships between time series,

∗Corresponding author. Email: li.tao@mail.shufe.edu.cn

1

 
 
 
 
 
 
which are commonly encountered in practice. In some scenario, the factors omitted from
the regression model, like those included, are correlated across periods in an unknown way,
which will bring errors with autocorrelation. This leads to the regression model

Yt = g(Xt) + et,

t = 1, · · · , T,

where the error process {et} is autocorrelated but satisﬁes E(et|Xt) = 0.

By modeling {et} as some autocorrelated process, the autocorrelation in the data can
be removed so that the regression function can be estimated more eﬃcient. For instance,
Xiao et al. (2003) considered the case in which {et} is assumed to be an invertible linear
process, with the ﬁnite-order ARMA(p, q) process as the special case, and showed that the
autocorrelation function of the error process can improve the estimation of the regression
function. Su and Ullah (2006) modeled {et} as a ﬁnite order nonparametric AR process. Liu
et al. (2010) discussed the estimation of regression function based on the models with {et}
following both AR(p) and ARMA(p, q) process.

All aforementioned literatures share the same assumption that {Xt} and {et} are strictly
stationary. In practice, this assumption sometimes is hard to justify since the time series
are often observed with trends. One typical approach for applying the time series analysis
is to remove the deterministic trend and seasonality components. Various nonparametric
detrending procedures have been developed for the time series that contains the smooth
trend and ARMA or AR error term, including those of Qiu et al. (2013); Shao and Yang
(2017); Schr¨oder and Fryzlewicz (2013); Truong (1991), among others. For more detrending
methods, the reader is referred to Brockwell and Davis (2016). As a result of detrending,
the model used is hard to reveal the evolutionary nature of the original data.
In recent,
an alternative approach for modeling nonstationary time series, viz, the locally stationary
time series models come into researchers’ view. Locally stationary process is introduced
by Dahlhaus (1996), which can be used to model the nonstationary time series directly.
Intuitively speaking, a process is locally stationary if over short periods of time (i.e., locally
in time) it behaves in an approximately stationary way. Compared to the methods with the
(weak) stationary assumption, locally stationary time series model seems more attractive
since it can better describe the nonstationary behavior. Due to this fact, several models
for locally stationary processes have been proposed in the recent literature. Bellegem and
Dahlhaus (2006) discussed the time-varying AR(p) models with the selection of order p. Vogt
(2012) studied nonparametric regression, which includes a wide range of interesting nonlinear
time series such as nonparametric autoregressive models. Pei et al. (2018) developed methods
for inference in nonparametric time-varying ﬁxed eﬀects panel data models that allow for
locally stationary regressors.

In this paper, we focus on a class of nonparametric regression models with time-varying
regression function, local stationary regressors and time-varying AR(p) (tvAR(p)) error pro-
cess, which is studied by Li and You (2020). The model is given by

Yt = g(t/T, Xt) + et,

et −

p
(cid:88)

i=1

φi(t/T )et−i = (cid:15)t,

t = 1, · · · , T,

(1.1)

2

where g(·, ·) and φi(·) are unknown functions and allowed to change smoothly over time. The
d−dimension covariates Xt are assumed to be locally stationary. Without loss of generality,
set d = 1 in this paper. The error process {et} satisﬁes E(et|Xt, Xt−1, . . . ) = 0. The white
noise {(cid:15)t} is independent and identically distributed (i.i.d.) with mean zero and variance σ2,
and E((cid:15)t|et−1, et−2, . . . , Xt, Xt−1, . . . ) = 0. As discussed by Vogt (2012), the tvAR(p) error
process {et}, under some mild conditions, is locally stationary.

Obviously, model (1.1) is a quite general form.

If g(·, ·) and φi(·) are time-invariant,

model (1.1) is specialized as

Yt = g(Xt) + et,

et −

p
(cid:88)

i=1

φiet−i = (cid:15)t,

t = 1, · · · , T,

(1.2)

which are discussed in Su and Ullah (2006); Liu et al. (2010). Liu et al. (2010) proposed an
iterative estimation procedure for model (1.2) and showed that the iterative estimator is more
eﬃcient than the estimator by incorporating the correlation information of the error into the
local linear regression. Motivated by this, Li and You Li and You (2020) deﬁned a three-step
estimation procedure for model (1.1) and presented its applications in ﬁnance through two
real data. They ﬁrst obtained a preliminary estimation for g(·, ·) by local linear regression,
ignoring the time-varying autoregressive structure of the error term. Then the local linear
methods was employed on the residuals from the ﬁrst step to obtain the estimation of the
time-varying autoregressive coeﬃcient functions φi(·). At last, the autocorrelated error term
was eliminated by plugging in the estimations obtained from the ﬁrst two step and a reﬁned
estimation of g(·, ·) was obtained by using local linear methods again. Intuitively, the reﬁned
estimation should be more eﬃcient than the preliminary estimation, which is veriﬁed by the
real data in Li and You (2020), since the autocorrelated error term was estimated and
eliminated. Unfortunately, no any theoretically results on the estimation was provided by
Li and You (2020). Due to its valuable application, it is worthy to discuss the estimation
theoretically. In this paper, we present the asymptotical properties of preliminary estimation
of g(·, ·), the estimation of φi(·), and the reﬁned estimation of g(·, ·). Moreover, the fact that
the reﬁned estimation of g(·, ·) is more eﬃcient than the preliminary estimation is proved.
This is also illustrated by the simulation studies. For the real data application, the readers
are referred to Li and You (2020).

The rest of the paper is organized as follows. Section 2 introduces some notations and
presents the estimation method given by Li and You (2020). Section 3 is the main part of
the paper, in which all asymptotical properties of the estimations are provided. At last, the
simulation studies are conducted in Section 4.

2 Notations and estimation procedure

In this section, some notations are introduced and the estimation method given by Li and
You (2020) are presented brieﬂy for the readers’ convenience. The readers are referred to Li

3

and You (2020) for more details.

For model (1.1), we assume, without loss of generality, that the covariate Xt = Xt is

1-dimensional.

step 1. The preliminary estimator of g(·, ·).

Ignoring the time-varying autoregressive
structure of the error term, and by applying local linear ﬁtting method (Fan and Gijbels
(1996)) and locally weighted least square estimation method, the preliminary estimator of
g(·, ·) is derived as:


 = (cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)Y.

(2.1)





ˆg(u, x)
h ∂ˆg(u,x)
∂u
h ∂ˆg(u,x)
∂x

(cid:111)

where Y = (Y1, · · · , YT )(cid:62), 1T = (1, · · · , 1)T , Uu = ( 1/T −u
XT −x
h

)(cid:62), Z(u, x) = (1T , Uu, Xx). W(u, x) = diag

Kh(1/T − u)Kh(X1 − x), · · · , Kh(T /T −
, Kh(·) = h−1K(·/h) and K(·) : R (cid:55)→ R is a zero-symmetric kernel function
u)Kh(XT − x)
with compact support. It should be noted that the product kernel is applied here and the
bandwidths for both time and covariate Xt are assumed to be same. Following the similar
procedures in Ruppert and Wand (1994) and Pei et al. (2018), the results can be easily
modiﬁed to allow for non-product kernel and diﬀerent bandwidths.

, · · · , T /T −u

h

)(cid:62), Xx = ( X1−x

h

, · · · ,

(cid:110)

h

step 2. The estimator of φi(·). Let ˆet = Yt−ˆg(t/T, Xt) be the estimate of the unobservable
(cid:0)t/T (cid:1)ˆet−i ≈ (cid:15)t with the assumption that p is known,

error term et. For the model ˆet −(cid:80)p
by minimizing

i=1 φi

L(α(u)|ˆe, u) =

T
(cid:88)

(cid:110)

ˆet −

p
(cid:88)

(cid:16)

t=p+1

k=1

φk(u) + φ(cid:48)

k(u)(t/T − u)

(cid:111)2

(cid:17)

ˆet−k

Khe(t/T − u),

(2.2)

the estimation of α(u) = (φ(u)(cid:62), heφ(cid:48)(u)(cid:62))(cid:62) = (α1(u), · · · , α2p(u))(cid:62) can be obtained as

ˆα(u) =

(cid:19)

(cid:18) ˆφ(u)
ˆφ(cid:48)(u)
he

=

(cid:19)

(cid:18) Ip Op
Ip
Op

(ˆZ(cid:62)

u We

ˆZu)−1 ˆZ(cid:62)

u WeˆeT ,

(2.3)

where Ip is p × p identity matrix, Op is p × p zero matrix. Let ˆeT = (ˆep+1, · · · , ˆeT ). ˆZu =
(ˆe, Duˆe), and ˆe = (ˆep, · · · , ˆeT −1)(cid:62) with ˆet−1 = (ˆet−1, ˆet−2, · · · , ˆet−p)(cid:62) for t = p + 1, · · · , T ,
Khe((p + 1)/T −
and Du = diag

. Let We = diag

(cid:110) (p+1)/T −u
he

(p+2)/T −u
he

(cid:111)

(cid:110)

,

, · · · , T /T −u
(cid:111)

he

u), Khe((p + 2)/T − u), · · · , Khe(T /T − u)

, where Khe(·) = h−1

e K(·/he).

In Section 3, we show that ˆφ(u) is eﬃcient with the same convergence rate and has the
same asymptotic distribution as the estimator when et is observable, which is given by Kim
(2001).

4

Moreover, Li and You (2020) presented the solution to the problem that the order p
of the error series is unknown in the real study. Denote the true value of φk(·) as φ∗
k(·).
Without loss of generality, it is assumed with 0 ≤ p2 ≤ p1 ≤ p that the ﬁrst p2 component of
φ(·) are nonzero smooth functions, while the next p1 − p2 components are nonzero constant,
and the ﬁnal p − p1 components are zeros. Deﬁne S1 = {1, · · · , p1}, S2 = {1, · · · , p2}. It
is obvious that identifying the coeﬃcient functions and the constant coeﬃcients simultane-
ously is equivalent to identifying these two sets together. Applying the uniform adaptive
LASSO (ULASSO) method proposed by Wang and Kulasekera (2012), the ULASSO esti-
mator ˆαλ,γ(u) = ( ˆφλ(u)(cid:62), he
γ(u)(cid:62))(cid:62) for each u ∈ (0, 1) is deﬁned as the minimizer of the
convex function

ˆφ(cid:48)

Qλ,γ(α(u)|ˆe, u) = L(α(u)|ˆe, u) + λ

p
(cid:88)

k=1

|φk(u)|
wk

+ γ

p
(cid:88)

k=1

|φ(cid:48)

k(u)|
w(cid:48)
k

,

(2.4)

t=1

k = ((cid:80)T

˜φ2
k(t/T )/T )1/2 and w(cid:48)

where L(α(u)|ˆe, u) are deﬁned in (2.2), λ ≥ 0, γ ≥ 0 are the tuning parameters, wk =
((cid:80)T
˜φ(cid:48)2
k (t/T )/T )1/2 are the uniform adaptive weights for all
u ∈ (0, 1). Let ˆΦλ,γ = ( ˆαλ,γ(1/T ), · · · , ˆαλ,γ(T /T ))(cid:62), and denote ˆS1λ = {k : (cid:80)T
t=1 | ˆφλ,k(t/T )| >
0}, ˆS2γ = {k : | (cid:80)T
ˆφ(cid:48)
γ,k(t/T )| > 0} as the index set of the relative variables and the coef-
ﬁcient functions identiﬁed by ˆΦλ,γ. The two sets ˆS1λ and ˆS2γ are taken as the estimators of
S1 and S2 respectively.

t=1

t=1

To determine the tuning parameters, Li and You (2020) proposed a BIC selector:

BICλ,γ( ˆΦλ,γ|ˆe) = log{RSSλ,γ( ˆΦλ,γ|ˆe)} + (cid:98)df λ,γ ×

log (T he)
T he

,

(2.5)

where (cid:98)df λ,γ is the total number of nonzero elements in both ˆS1λ and ˆS2γ, and

RSSλ( ˆΦλ,γ|ˆe) =

1
T 2

T
(cid:88)

i=1

L( ˆαλ,γ(i/T )|e, i/T ).

Deﬁne (ˆλ, ˆγ) to be the minimizer of (2.5). Thus, ˆS1ˆλ and ˆS2ˆγ are taken as the estimators of
S1 and S2 respectively.

Once again, there is no any discussions on the property of ˆS1ˆλ and ˆS2ˆγ in Li and You
(2020), which is worthwhile and necessary to study. In Section 3, we show that (ˆλ, ˆγ) can
identify the true model consistently.

step 3. The reﬁned estimator of g(·, ·). By minimizing the locally weighted least square

loss function
(cid:40)

T
(cid:88)

Yt −

t= (cid:98)p0+1

p
(cid:88)

k=1

ˆφk(t/T )(cid:0)Yt−k − ˆg((t − k)/T, Xt−k) − g(u, x) −

∂g(u, x)
∂u

(t/T − u)

−

∂g(u, x)
∂x

(cid:27)2

(Xt − x)

Kh∗(t/T − u)Kh∗(Xt − x),

5

where h∗ is a new bandwidth, the reﬁned estimator of g(·, ·) is derived (seeLi and You (2020)).





ˆg∗(u, x)
∂ˆg∗(u,x)
he
∂u
∂ˆg∗(u,x)
he
∂x


 = (cid:0)Z∗(u, x)(cid:62)W∗(u, x)Z∗(u, x)(cid:1)−1Z∗(u, x)W∗(u, x)Y∗.

(cid:110)

u = ( ( (cid:98)p0+1)/T −u

t = Yt − (cid:80)p

(cid:98)p0+1, · · · , ˆY ∗
u, X∗
Kh∗( (cid:98)p0+1

T )(cid:62) with ˆY ∗
x), U∗
T − u)Kh∗(X

where ˆY∗ = ( ˆY ∗
Z∗(u, x) = (1T − (cid:98)p0, U∗
h
W∗(u, x) = diag
. Li and You
Li and You (2020) explain the reason that the reﬁned estimator is more eﬃcient than the
preliminary estimator intuitively. In Section 3, we present the asymptotic property of the
reﬁned estimator and show that the reﬁned estimator is more eﬃcient than the preliminary
estimator theoretically.

ˆφk(t/T )(cid:0)Yt−k − ˆg((t − k)/T, Xt−k).
(cid:99)p0+1−x
, · · · , XT −x
)(cid:62).
h
(cid:111)

k=1
, · · · , T /T −u
(cid:98)p0+1 − x), · · · , Kh∗( T

)(cid:62), X∗
x = (
T − u)Kh∗(XT − x)

X

h

h

3 Main results

Li and You (2020) presented the whole procedure of the estimation and the application
to the ﬁnance without any discussion on the property of the estimation. Due to its wide
range of applications, it is worthwhile and necessary to discuss the statistical property of
the estimation. In this article, we present the asymptotic properties of all estimators in Li
and You (2020). In this section, C > 0 denotes a generic constant that may vary from line
to line.

3.1 Asymptotic results on the preliminary estimator of g(·, ·)

For the preliminary estimator of g(·, ·), ˆg(u, x), we present its uniform convergence rate and
asymptotic normality. To do so, we need the following assumptions.

(C1) Both {Xt} and {et} are locally stationary, i.e., for each re-scaled time point u ∈
[0, 1], there exist two strictly stationary processes {Xt(u)} and {et(u)} such that |Xt −
t (u)
Xt(u)| ≤
almost surely, where Ut(u) is a process of positive variables satisfying E[(Ut(u))ρ] ≤ C
for some ρ > 0 and C < ∞, and U ∗
t (u) is a process of positive variables satisfying
E[(U ∗

t (u))ρ∗] ≤ C ∗ for some ρ∗ > 0 and C ∗ < ∞.

Ut(u) almost surely, and |et − et(u)| ≤

(cid:12)
(cid:12)
(cid:12) + 1
t
T − u

(cid:12)
(cid:12)
(cid:12) + 1

t
T − u

(cid:16)(cid:12)
(cid:12)
(cid:12)

(cid:16)(cid:12)
(cid:12)
(cid:12)

U ∗

(cid:17)

(cid:17)

T

T

(C2) {Xt, et} is α-mixing, and the mixing coeﬃcients α satisﬁes that α(k) ≤ Ak−β for some

A > 0 and β > 2s−2

s−2 with the same s in (C6).

(C3) The density f (u, x) of the variable Xt(u) is smooth in u and bounded away from zero.

In particular, f (u, x) is continuously diﬀerentiable.

6

(C4) g(u, x) is twice continuously partially diﬀerentiable and Lipschitz-continuous.

(C5) K(·) has compact support [−C1, C1] and is Lipschitz-continuous, K(x) = K(−x), and

(cid:107)K(cid:107)∞ = supx |K(x)| < ∞.

(C6) For some s > 2, E|es

t | < ∞.

(C7) With vT = log log T , β > 2s−2

that

T h8 → 0;

s−2 with the same s in (C6), and θ = β(1−2/s)−2/s−3
vT log T
T θh2 → 0;

T rhr+1 → 0;

T → ∞.

β+1

as

1

, it holds

Remark 3.1. (C1) is the basic assumption for a locally stationary process deﬁned as in Vogt
(2012), which is reasonable in modeling economic and ﬁnancial data. (C2) is the mixing
condition for each time series, which is reasonable and allows the notation in the proofs
as simple as possible.
(C3)-(C5) are the regularity conditions commonly used in locally
stationary ﬁelds and nonparametric settings. (C6) together with (C2) are useful conditions
proposed by Hansen (2008) to obtain the order of the stochastic part and the variance part.
(C7) involves the conditions to satisfy the optimal convergence rate.

Remark 3.2. The constant ρ can be regarded as a measure of how well Xt is approximated
by Xt(u): the larger ρ can be chosen, the less mass is contained in the tails of the distribution
of Ut(u). So the approximation of Xt by Xt(u) is getting better for larger ρ. This is also
true for ρ∗ with regard to et.

Remark 3.3. In general, it is not indispensable for the kernel function to have a bounded
support as long as its tails are thin (e.g., a density function that has a second moment).
However, as said in Fan and Yao (2008), ’when the kernel function K has a bounded support,
the integration above takes place only around a neighborhood of x. Hence, it suﬃces to
assume that the density f has a pth continuous derivative at the point x.’ For the sake of
simplicity, it is assumed in this article that the kernel function K has a bounded support.
This assumption can be removed at the cost of lengthier arguments.

The uniform convergence rate and the asymptotic normality of the preliminary estimator

are given by Theorem 3.1 and 3.2. The following lemma is needed to prove the theorems.

Lemma 3.1. Let S be a compact set of R, and the bandwidth h in (2.1) satisﬁes that

with vT = log log T , θ = β(1−2/s)−2/s−3
Then it holds under the conditions (C1)-(C7) that

β+1

s−2 with the same s in condition (C6).

vT log T
T θh2 = o(1),
, and β > 2s−2

sup
u∈[0,1],x∈S

(cid:12)
(cid:12)
(cid:12)

1
T

T
(cid:88)

t=1

Kh(t/T − u)Kh(Xt − x)(cid:0) t/T − u

h

(cid:1)i(

Xt − x
h

)jet

(cid:12)
(cid:12)
(cid:12) = Op

(cid:16)(cid:114)

log T
T h2

(cid:17)

.

7

Proof. Under condition (C2) that {et} is α-mixing, Lemma 3.1 follows immediately from
Theorem 4.1 of Vogt (2012).

Theorem 3.1. Assume that conditions (C1)-(C7) hold. r = min{ρ, 1}, where ρ is deﬁned
in condition (C1). Then we have

|ˆg(u, x) − g(u, x)| = Op

sup
x,u

(cid:16)(cid:114)

log T
T h2 +

1
T rh

+ h2(cid:17)

Proof. By (2.1), we have

ˆg(u, x) − g(u, x) = (1, 0, 0)(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)y − g(u, x)
= (1, 0, 0)(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)G − g(u, x)

+ (1, 0, 0)(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)e

≡ gB(u, x) + gV (u, x),

where

gB(u, x) = (1, 0, 0)(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)G − g(u, x),
gV (u, x) = (1, 0, 0)(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)e.

By Lemma 3.1, we have

sup
u∈[0,1],x∈S

|gV (u, x)| = Op

(cid:16)(cid:114)

log T
T h2

(cid:17)

.

Applying the arguments for Lemma 3.1 to gB(u, x), we have

sup
u∈[0,1],x∈S

|gB(u, x) − E(gB(u, x))| = Op

(cid:16)(cid:114)

log T
T h2

(cid:17)

.

We also claim that

sup
u∈[0,1],x∈S

|E(gB(u, x))| =

µ2h2
2

(cid:16)∂2g(u, x)

∂u2 +

(cid:17)

∂2g(u, x)
∂x2

+ O

(cid:16) 1

(cid:17)

T rh

+ op(h2),

(3.1)

where r = min{ρ, 1}. Note that supu∈[0,1],x∈S |E(gB(u, x))| is irrelative with et, thus (3.1)
can be proved by following the idea of Vogt (2012).

Note that

gB(u, x) = (1, 0, 0)(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)[G − Z(u, x)(g(u, x), 0, 0)(cid:62)]
= (1, 0, 0)(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)[G − g(u, x)1T ].

8

Firstly, using standard results from density estimation, we have

1
T

Z(u, x)(cid:62)W(u, x)Z(u, x) = f (u, x) ⊗ diag{1, µ2, µ2}(1 + op(h2)),

(3.2)

where ⊗ denotes the kronecker product. Then we consider the term Z(u, x)(cid:62)W(u, x)[G −
g(u, x)1T ]. Denote ¯K as a Lipschitz-continuous function with support [−qC1, qC1] for some
q > 1. Assume that ¯K = 1 for all x ∈ [−C1, C1]. Let ¯Kh(·) = ¯K(·/h), then it holds that

E(cid:0)Z(u, x)(cid:62)W(u, x)[G − g(u, x)1T ](cid:1) =

4
(cid:88)

i=1

Ξi(u, x)

Ξi(u, x) =

1
T

T
(cid:88)

t=1

Kh(t/T − u)ξi(u, x),

i = 1, · · · , 4,

with

and

ξ1(u, x) = E

ξ2(u, x) = E

ξ3(u, x) = E

(cid:105)
(cid:104) ¯Kh(Xt − x){Kh(Xt − x) − Kh(Xt(t/T ) − x)}{g(t/T, Xt) − g(u, x)}
(cid:104) ¯Kh(Xt − x)Kh(Xt(t/T ) − x){g(t/T, Xt) − g(t/T, Xt(t/T ))}
(cid:104)
{ ¯Kh(Xt − x) − ¯Kh(Xt(t/T ) − x)}Kh(Xt(t/T ) − x)

(cid:105)
,

,

(cid:105)
× {g(t/T, Xt(t/T )) − g(u, x)}

,

ξ4(u, x) = E

(cid:104)
Kh(Xt(t/T ) − x){g(t/T, Xt(t/T )) − g(u, x)}

(cid:105)

.

We ﬁrstly consider Ξ1(u, x). Since the kernel function K(·) is bounded, we can ﬁnd a constant
C ≤ ∞ such that |K(x) − K(x(cid:48))| ≤ C|K(x) − K(x(cid:48))|r for r = min{ρ, 1}. By the deﬁnition
of ¯K and the smoothness of g(u, x), ¯Kh(Xt − x)|g(t/T, Xt) − g(u, x)| can be bounded by Ch.
Thus, it follows with conditions (C1) and (C5) that

T
(cid:88)

Kh(t/T − u)E(|Kh(Xt − x) − Kh(Xt(t/T ) − x)|)

Kh(t/T − u)E

(cid:16)(cid:12)
(cid:12)
(cid:12)K

(cid:16) Xt − x
h

(cid:17)

− K

(cid:16)Xt(t/T ) − x
h

(cid:17)(cid:12)
(cid:12)
(cid:12)

r(cid:17)

Kh(t/T − u)E

(cid:16)(cid:12)
(cid:12)
(cid:12)

1
T h

(cid:12)
(cid:12)
Ut(t/T )
(cid:12)

r(cid:17)

Ch
T

t=1
T
(cid:88)

t=1
T
(cid:88)

t=1

C
T

C
T

Ξ1(u, x) ≤

≤

≤

≤

C
T rhr

9

uniformly in u and x. Using similarly arguments, we can ﬁnd that supu∈[0,1],x∈S |Ξ2(u, x)| ≤
T rh and supu∈[0,1],x∈S |Ξ3(u, x)| ≤ C
C
T rhr . Finally, applying Lemma 3.1 and conditions (C3)
and (C4), we obtain that

sup
u∈[0,1],x∈S

|Ξ4(u, x)| =

µ2h2
2

(cid:16) ∂2g(u, x)

∂u2 +

(cid:17)

∂2g(u, x)
∂x2

+ op(h2).

Thus (3.1) holds by combining the results of Ξ1(u, x), · · · , Ξ4(u, x) and (3.2). Then we have

sup
u∈[0,1],x∈S

|ˆg(u, x) − g(u, x)|

= sup

|ˆgV (u, x) + ˆgB(u, x) − E(ˆgB(u, x)) + E(ˆgB(u, x))|

u∈[0,1],x∈S

≤ sup

|ˆgV (u, x)| + sup

|ˆgB(u, x) − E(ˆgB(u, x))| + sup

|E(ˆgB(u, x))|

u∈[0,1],x∈S
(cid:114)

(cid:16)

= Op

log(T )
T h2 +

u∈[0,1],x∈S

+ h2(cid:17)

.

1
T rh

u∈[0,1],x∈S

Theorem 3.2. Under the conditions (C1)-(C7), let r = min{ρ, 1} > 1
2, which guarantees
that the bandwidth can be selected to obtain the optimal convergence rate. Then for any
u ∈ (0, 1),

√

T h2










ˆg(u, x)
h ∂ˆg(u,x)
∂u
h ∂ˆg(u,x)
∂x





 −



g(u, x)
h ∂g(u,x)
∂u
h ∂g(u,x)
∂x



 −






µ2h2
2

where Vu,x = (

(cid:82) 1
0 γ0(u)du
f (u,x)

(cid:1)Σ, and

(cid:16) ∂2g(u,x)

∂2u + ∂2g(u,x)

∂2x

0
0

(cid:17)







 + op(h2)


D−→ D N (0, Vu,x),

as T → ∞,

Σ =







ν2
0
ν0ν1
µ2
ν0ν1
µ2

ν0ν1
µ2
ν0ν2
µ2
2
ν2
1
µ2
2







.

ν0ν1
µ2
ν2
1
µ2
2
ν0ν2
µ2
2

(3.3)

Z∗(u, x) = (1T , Uu, X∗

To prove Theorem 3.2, we deﬁne the following notations. X∗
x), W∗(u, x) = diag
, R∗(u, x) = (Uu (cid:12) Uu, X∗

(cid:111)

(cid:110)

x = ( X1(1/T )−x
Kh(1/T − u)Kh(X1(1/T ) − x), · · · , Kh(T /T −

, · · · , XT (1/T )−x

h

h

)(cid:62),

u)Kh(XT (1/T ) − x)
t (u, x)
be the t-th row of Z∗(u, x) and R∗(u, x), respectively. Denote Hg(u, x) the Hessian matrix
of g at (u, x), and Qg(u, x) a T -dimensional vector whose t-th element is (t/T − u, Xt(t/T ) −

t (u, x) and R∗

x, Uu (cid:12) X∗

x). Let Z∗

x (cid:12) X∗

10

x)(cid:62)Hg(u, x)(t/T − u, Xt(t/T ) − x). Deﬁne

Γ(u) =








γ0(u)
γ1(u)
...

γ1(u)
γ0(u)
...
γp−1(u) γp−2(u) γp−3(u)

γ2(u)
γ1(u)
...

· · · γp−1(u)
· · · γp−2(u)
. . .
· · ·

...
γ0(u)








,

(3.4)

where γk(u) = (cid:80)∞
mate stationary process {et(u)} for the rescaled time point u ∈ (0, 1).

j=0 ϕj(u)ϕj+k(u) is the k-th order auto-covariance function of the approxi-

Proof. With ˆgV (u, x) and ˆgB(u, x) as in the proof of Theorem 3.1, we have

√





T h2







 −

ˆg(u, x)
h ∂ˆg(u,x)
∂u
h ∂ˆg(u,x)
∂x





g(u, x)
h ∂g(u,x)
∂u
h ∂g(u,x)
∂x









√

T h2

=






(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)G −

g(u, x)
h ∂g(u,x)
∂u
h ∂g(u,x)
∂x
T h2(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)e

√

+





√

(cid:77)
=

where

T h2B(u, x) + V (u, x),

B(u, x) = (cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)G −

V (u, x) =

T h2(cid:0)Z(u, x)(cid:62)W(u, x)Z(u, x)(cid:1)−1Z(u, x)(cid:62)W(u, x)e.

√





g(u, x)
h ∂g(u,x)
∂u
h ∂g(u,x)
∂x












 ,

Henceforth, we refer to B(u, x) and V (u, x) as the bias part and the variance part, respec-
tively.

By conditions (C1) and (C4), we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ C
(cid:12)g(t/T, Xt(t/T )) − g(t/T, Xt)

(cid:12)
(cid:12)
(cid:12)Xt(t/T ) − Xt

(cid:12)
(cid:12)
(cid:12) ≤

C
T

Ut(t/T ) = op(1).

Hence, g(t/T, Xt(t/T )) = g(t/T, Xt) + op(1). By Taylor Theorem, we have

G = Z∗(u, x)

= Z∗(u, x)









g(u, x)
h ∂g(u,x)
∂u
h ∂g(u,x)
∂x
g(u, x)
h ∂g(u,x)
∂u
h ∂g(u,x)
∂x



 +

1
2

Qg(u, x) + op(h21T )


 + R∗(u, x)






h2
2
h2
2
h2
2

∂2g(u,x)
∂2u
∂2g(u,x)
∂2x
∂2g(u,x)
∂u∂x


 + op(h21T ).


11

Thus the bias part B(u, x) can be written as

B(u, x) = (cid:0)Z∗(u, x)(cid:62)W∗(u, x)Z∗(u, x)(cid:1)−1Z∗(u, x)(cid:62)W∗(u, x)R∗(u, x)






h2
2
h2
2
h2
2

∂2g(u,x)
∂2u
∂2g(u,x)
∂2x
∂2g(u,x)
∂u∂x


 + op(h2).


(3.5)

Note that

1
T

Z∗(u, x)(cid:62)W∗(u, x)Z∗(u, x) =





M0,0 M1,0 M0,1
M1,0 M2,0 M1,1
M0,1 M1,1 M0,2



 ,

where

Mi,j = Mi,j(u, x) =

1
T

T
(cid:88)

t=1

Kh(t/T − u)Kh(Xt − x)(cid:0) t/T − u

h

(cid:1)i(

Xt(t/T ) − x
h

)j,

i, j = 0, 1, 2.

With the similar arguments as in Ruppert and Wand (1994), we have

and

1
T

1
T

Z∗(u, x)(cid:62)W∗(u, x)Z∗(u, x) = f (u, x) ⊗ diag{1, µ2, µ2}(1 + op(h2)),

(3.6)

Z∗(u, x)(cid:62)W∗(u, x)R∗(u, x) = f (u, x) ⊗






µ2 µ2 0
 (1 + op(h3)).
0
0
0
0
0
0

(3.7)

It follows from (3.5), (3.6), and (3.7) that the asymptotic bias

B(u, x) =

=












0
1
0 µ−1
2
0
0
(cid:16) ∂2g(u,x)

0
0
µ2−1
∂2u + ∂2g(u,x)

µ2h2
2

∂2x

 (1 + op(1)) ×

(cid:17)


 + op(h2).


0
0






µ2 µ2 0
 (1 + op(h3)) ×
0
0
0
0
0
0






h2
2
h2
2
h2
2

∂2g(u,x)
∂2u
∂2g(u,x)
∂2x
∂2g(u,x)
∂u∂x


 + op(h2)


(3.8)

Next we consider the variance part. The following proof is completed by a classical block-
ing technique Cai et al. (2000b). Partition {1, 2, · · · , T } into 2qT +1 subsets with small-block
of size s = sT and large-block of size l = lT . Let q = (cid:98) T
T Z∗(u, x)(cid:62)W∗(u, x)e.
For any non-zero vector ξ = (ξ1, ξ2, ξ3)(cid:62), let

r+s(cid:99), NT = 1

ξ1 + ξ2

(cid:0) t/T − u
h

(cid:1) + ξ3

(cid:0) Xt(t/T ) − x
h

(cid:1)(cid:111)

Kh(t/T − u)Kh(Xt(t/T ) − x)et

QT = ξ(cid:62)NT =

1
T

T
(cid:88)

(cid:110)

t=1

(cid:77)
=

1
T

T
(cid:88)

t=1

Zt,

12

where Zt = (ξ1, ξ2, ξ3)





1
t/T −u
h
Xt(t/T )−x
h



 Kh(t/T − u)Kh(Xt(t/T ) − x)et. Thus

√

T h2Qt) = V ar(

V ar(

h
√
T

T
(cid:88)

t=1

Zt) =

h2
T

T
(cid:88)

t=1

V ar(Zt) +

2h2
T

T −1
(cid:88)

T −l
(cid:88)

l=1

t=1

cov(Zt, Zt+l).

(3.9)

Let wt = K 2

h(t/T − u)K 2

h(Xt(t/T ) − x). It’s easy to show that E(Zt) = 0 and

V ar(Zt)
(cid:104)(cid:110)

=E

ξ1 + ξ2

(cid:104)(cid:110)

=E

ξ1 + ξ2


=E(e2

t )ξ(cid:62)




(cid:0) t/T − u
h
(cid:0) t/T − u
h
wt
(cid:0) t/T −u
h
(cid:0) Xt(t/T )−x
h


(cid:1) + ξ3

(cid:1) + ξ3

(cid:1)(cid:111)

(cid:0) Xt(t/T ) − x
h
(cid:0) Xt(t/T ) − x
h
(cid:0) t/T −u
h
(cid:0) t/T −u
h

(cid:1)wt
(cid:1)2wt
(cid:1)(cid:0) Xt(t/T )−x

h



(cid:1)wt
(cid:1)wt
ν2
0
ν0ν1 ν0ν2
ν2
ν0ν1
1

(cid:0) t/T −u
h
ν0ν1 ν0ν1
ν2
1
ν0ν2

=

E(e2
t )
h2 f (u, x)ξ(cid:62)



 ξ{1 + Op(h)}.

(cid:1)(cid:111)

Kh(t/T − u)Kh(Xt(t/T ) − x)et

(cid:105)2

(3.10)

Kh(t/T − u)Kh(Xt(t/T ) − x)

(cid:105)2

E(e2
t )

(cid:0) Xt(t/T )−x
h

(cid:1)wt
(cid:1)(cid:0) Xt(t/T )−x
(cid:1)2wt

(cid:0) t/T −u
h
h
(cid:0) Xt(t/T )−x
h

(cid:1)wt

(cid:1)wt




 ξ{1 + Op(h)}

(3.11)

(3.12)

As shown in Kim (2001),

E(e2

t ) = γ0(t/T ) + o(1),

Hence, it follows from that (3.11) and (3.12) that

1
T

T
(cid:88)

t=1

V ar(Zt) =

1
h2

(cid:90) 1

0

γ0(u)duf (u, x)ξ(cid:62)





ν2
0
ν0ν1 ν0ν2
ν2
ν0ν1
1

ν0ν1 ν0ν1
ν2
1
ν0ν2



 ξ{1 + Op(h)}.

Since {et} is α-mixing, it follows from Lemma (1) of Cai et al. (2000b) that h (cid:80)T −1
o(1), which implies that the second term of (3.9) is negligible. Hence,

t=1 |cov(Z1, Zt+1)| =

√

V ar(

T h2Qt) =

(cid:90) 1

0

γ0(u)duf (u, x)ξ(cid:62)

For 0 ≤ j ≤ q − 1, deﬁne





ν2
0
ν0ν1 ν0ν2
ν2
ν0ν1
1

ν0ν1 ν0ν1
ν2
1
ν0ν2



 ξ{1 + Op(h)}.

j(l+s)+l
(cid:88)

ηj =

Zt;

ξj =

(j+1)(l+s)
(cid:88)

Zt;

ζq =

T
(cid:88)

Zt.

t=j(l+s)+1

t=j(l+s)+l+1

t=q(l+s)+1

13

Then

√

T h2Qt =

h
√
T

q−1
(cid:88)
(

j=0

ηt +

q−1
(cid:88)

j=0

ξt + ζq) =

h
√
T

(Q1 + Q2 + Q3).

With the similar arguments of Theorem 2 in Cai et al. (2000b), it can be shown that small
block Q2 and the remainder Q3 are asymptotically negligible in probability, and each πj in
large block Q1 is asymptotically independent under condition (C2). Thus the asymptotic
normality of Q1 is derived by Lindeberg Theorem, and it holds that

√

(cid:32)

T h2NT

D→ N

0,

(cid:90) 1

0

γ0(u)duf (u, x)

It together with (3.6) follows that





ν2
0
ν0ν1 ν0ν2
ν2
ν0ν1
1

ν0ν1 ν0ν1
ν2
1
ν0ν2





(cid:33)
,

as T → ∞.

√

T h2(cid:0)Z∗(u, x)(cid:62)W∗(u, x)Z∗(u, x)(cid:1)−1Z∗(u, x)(cid:62)W∗(u, x)e D→ N (0, Vu,x),

as T → ∞,

(3.13)

where

Vu,x =

(cid:82) 1
0 γ0(u)du
f (u, x)







ν2
0
ν0ν1
µ2
ν0ν1
µ2

ν0ν1
µ2
ν0ν2
µ2
2
ν2
1
µ2
2







.

ν0ν1
µ2
ν2
1
µ2
2
ν0ν2
µ2
2

Combining the results of (3.8) and (3.13), the theorem is proved.

Theorem 3.1 and 3.2 show that the preliminary estimator ˆg(u, x) is consistent and asymp-
totic normal. However, it’s not eﬃcient due to the fact that the error structure is not taken
into account for estimation. On the other side, Li and You (2020) use it to estimate the
error structure and then reﬁne the estimator of g(u, x) with the ﬁtted error structure.

3.2 Asymptotic results on the estimator of the error term

In this section, we discuss the asymptotic property for the estimator of the error term, ˆφ(u),
which is given by Li and You (2020). To do so, we need some additional conditions except
for conditions (C1)-(C7).

(C8) The function φi(·) (i = 1, 2, · · · , p) is twice continuously diﬀerentiable in u with uni-
i=1 φi(u)zi

formly bounded second ordered derivative, and the root of Φ(u, z) = 1 − (cid:80)p
are bounded away from the unit circle for each u ∈ [0, 1].

(C9) As T → ∞, he = O(T − 1

5 ), h = o(T − 1

5 ), T − 2

5 h−2 log T → 0.

(C10) E((cid:15)4

t ) < ∞.

14

(C11) supt

(cid:80)∞

j=0 j 1

2 ϕj(t/T )2 < ∞, supt

(cid:80)∞

j=0 j 1

2 (ϕ(cid:48)

j(t/T ))2 < ∞.

Remark 3.4. Condition (C8) guarantees that the approximate stationary time series {et(u)}
of {et} around the neighborhood of u is causal and satisﬁes (cid:80)∞
k=−∞ |γk(u)| < ∞, where
γk(u), deﬁned in (3.4), is the k-th ordered auto-covariance function of {et(u)}. (C9) is the
condition to obtain the optimal convergence for the estimator of φk(·). Conditions (C10) and
(C11) are the same as the ones in Kim (2001) for the asymptotic normality of time-varying
autoregressive coeﬃcient functions.

Theorem 3.3 presents that the asymptotic normality of ˆα(u). To prove it, we ﬁrst show
a proposition for the asymptotic normality of ˜α(u), where ˜α(u) = ˜S−1
˜tT is the estimate of
T
α(u) with ˆet in (2.3) replaced by its true value et. Note that this proposition is the same
as the i.i.d case in Cai et al. (2000a). Its proof is similar to Theorem 5 of Kim (2001) and
therefore omitted here.

Proposition 3.1. Under conditions (C1)-(C11), for any u ∈ (0, 1),

(cid:16)

(cid:112)

T he

˜α(u) − α(u) −

h2
e
2

µ2

(cid:19)

(cid:18)φ(cid:48)(cid:48)(u)
0

(cid:17) D−→ N (0, Σ(u)),
+ o(h2
e)

as T → ∞,

where Σ(u) is deﬁned in (3.14).

Theorem 3.3. Assume that conditions (C1)-(C11) are satisﬁed, r = min{ρ, 1} > 1
u ∈ (0, 1), it holds that

2, for any

(cid:16)

(cid:112)

T he

ˆα(u) − α(u) −

h2
e
2

µ2

(cid:19)

(cid:18)φ(cid:48)(cid:48)(u)
0

(cid:17) D−→ N (0, Σ(u)),
+ o(h2
e)

as T → ∞,

where φ(cid:48)(cid:48)(u) = (φ(cid:48)(cid:48)

1(u), · · · , φ(cid:48)(cid:48)

p(u)), Γ(u) is given by (3.4), and

Σ(u) =

(cid:18) ν0
µ−1
1 ν1
µ−1
1 ν1 µ−2
1 ν2

(cid:19)

⊗ Γ−1(u).

(3.14)

Proof. With Slutsky Theorem, the asymptotic normality can be obtained immediately from
(3.15) and Proposition 3.1. Therefore we just prove that

ˆα(u) − ˜α(u) = op((T he)− 1

2 ).

Deﬁne a 2p × 2p matrix ˆST and a 2p-dimensional vector ˆtT as follows,

ˆST =

ˆtT =

1
T
1
T

ˆZ(cid:62)

u We

ˆZu =

(cid:18)ˆST,0(u) ˆST,1(u)
ˆST,1(u) ˆST,2(u)

(cid:19)

,

ˆZ(cid:62)
u WeˆeT = (ˆtT,0(u), ˆtT,1(u))(cid:62),

15

(3.15)

(3.16)

(3.17)

where

ˆST,i(u) =

ˆtT,i(u) =

1
T

1
T

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

Khe(t/T − u)(

Khe(t/T − u)(

t/T − u
he

t/T − u
he

)iˆet−1ˆe(cid:62)

t−1,

i = 0, 1, 2, 3,

(3.18)

)iˆet−1ˆet,

i = 0, 1.

(3.19)

So (2.3) can be represented as ˆα(u) = ˆS−1
T

ˆtT .
By (3.18), the (r, s)-th element of ˆST,i(u) is

ˆST,i(u, r, s) =

=

=

1
T

1
T

1
T

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1
(cid:16) t/T − u
he

×

(cid:17)i

Khe(t/T − u)ˆet−rˆet−s

(cid:17)i

(cid:16) t/T − u
he

Khe(t/T − u)

(cid:104)
Yt−r − ˆg

(cid:16) t − r
T

(cid:17)(cid:105)(cid:104)

Yt−s − ˆg

(cid:16)t − s
T

(cid:17)(cid:105)(cid:16) t/T − u

(cid:17)i

he

Khe(t/T − u)

(cid:104)
et−r + g

(cid:17)

(cid:16)t − r
T

− ˆg

(cid:16) t − r
T

(cid:17)(cid:105)(cid:104)

et−s + g

(cid:17)

(cid:16)t − s
T

− ˆg

(cid:16)t − s
T

(cid:17)(cid:105)

= ˜ST,i(u, r, s) + ˆs1(u, r, s) + ˆs2(u, r, s) + ˆs3(u, r, s),

where ˜ST,i(u, r, s) = 1
and

T

(cid:80)T

t=p+1 Khe(t/T −u)et−ret−s

(cid:17)i

(cid:16) t/T −u
he

is the (r, s)-th element of ˜ST,i(u)

ˆs1(u, r, s) =

ˆs2(u, r, s) =

ˆs3(u, r, s) =

1
T

1
T

1
T

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

(cid:104)
Khe(t/T − u)

g

(cid:17)

(cid:16)t − r
T

− ˆg

(cid:16) t − r
T

(cid:17)(cid:105)(cid:104)

(cid:17)

g

(cid:16)t − s
T

− ˆg

(cid:16)t − s
T

(cid:17)(cid:105)(cid:16)t/T − u

(cid:17)i

he

,

Khe(t/T − u)et−r

(cid:17)

(cid:104)
g

(cid:16) t − s
T

− ˆg

(cid:16) t − s
T

(cid:17)(cid:105)(cid:16) t/T − u

(cid:17)i

he

Khe(t/T − u)et−s

(cid:17)

(cid:104)

g

(cid:16)t − r
T

− ˆg

(cid:16) t − r
T

(cid:17)(cid:105)(cid:16)t/T − u

(cid:17)i

he

,

.

Similarly, the r-th element of ˆtT,i(u) is ˆtT,i(u, r) = ˜tT,i(u, r) + ˆt1(u, r) + ˆt2(u, r) + ˆt3(u, r),

16

where ˜tT,i(u, r) = 1
T

(cid:80)T

t=p+1 Khe(t/T − u)etet−r

(cid:17)i

(cid:16) t/T −u
he

is the r-th element of ˆtT,i(u) and

ˆt1(u, r) =

ˆt2(u, r) =

ˆt3(u, r) =

1
T

1
T

1
T

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

(cid:104)
Khe(t/T − u)

g

(cid:17)

(cid:16) t
T

− ˆg

(cid:16) t
T

(cid:17)(cid:105)(cid:104)

(cid:17)

g

(cid:16)t − r
T

− ˆg

(cid:16) t − r
T

(cid:17)(cid:105)(cid:16)t/T − u

(cid:17)i

he

,

Khe(t/T − u)et

(cid:17)

(cid:104)
g

(cid:16)t − r
T

− ˆg

(cid:16)t − r
T

(cid:17)(cid:105)(cid:16) t/T − u

(cid:17)i

he

,

Khe(t/T − u)et−r

(cid:17)

(cid:104)

g

(cid:16) t
T

− ˆg

(cid:16) t
T

(cid:17)(cid:105)(cid:16) t/T − u

(cid:17)i

he

.

With the decomposition of ˆST,i(u, r, s) and ˆtT,i(u, r), (3.15) is immediately obtained from
˜tT and the two statements

T (ˆtT − ˜tT ) + ˆS−1

T (˜ST − ˆST )˜S−1

T

the fact that ˆα(u) − ˜α(u) = ˆS−1
below:

(i) ˆs1(u, r, s) + ˆs2(u, r, s) + ˆs3(u, r, s) = op((T he)− 1

2 ),

i = 0, 1, 2;

(ii) ˆt1(u, r) + ˆt2(u, r) + ˆt3(u, r) = op((T he)− 1

2 ),

i = 0, 1.

Since the proof for (i) is similar as (ii), here we only demonstrate (ii).

Note that t/T −u
proof for the case i = 0.

he

≤ C1 on the compact support of Khe(t/T − u), thus we only present the

Firstly,

|ˆt1(u, r)| =

1
T

T
(cid:88)

t=p+1

(cid:12)
(cid:12)
(cid:12)g
Khe(t/T − u)

(cid:17)

(cid:16) t
T

− ˆg

(cid:16) t
T

(cid:17)(cid:12)
(cid:12)
(cid:12) ·

(cid:12)
(cid:12)
(cid:12)g

(cid:16)t − r
T

(cid:17)

− ˆg

(cid:16) t − r
T

(cid:17)(cid:12)
(cid:12)
(cid:12)

=

≤

1
T h

C
T h

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

K

(cid:16)t/T − u
he

(cid:17)(cid:12)
(cid:12)
(cid:12)g

(cid:16) t
T

(cid:17)

− ˆg

(cid:16) t
T

(cid:17)(cid:12)
(cid:12)
(cid:12) ·

(cid:12)
(cid:12)
(cid:12)g

(cid:16)t − r
T

(cid:17)

− ˆg

(cid:16) t − r
T

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12)g

(cid:16) t
T

− ˆg

(cid:16) t
T

(cid:17)(cid:12)
(cid:12)
(cid:12) ·

(cid:12)
(cid:12)
(cid:12)g

(cid:16) t − r
T

(cid:17)

− ˆg

(cid:16) t − r
T

(cid:17)(cid:12)
(cid:12)
(cid:12).

By Theorem 3.1 and condition (C9),

|ˆt1(u, r)| ≤

(cid:16)

·

C
he

max
1≤t≤T

(cid:17)

(cid:12)
(cid:12)
(cid:12)g

(cid:16) t
T

− ˆg

(cid:16) t
T

(cid:17)2

(cid:17)(cid:12)
(cid:12)
(cid:12)

=

C
he

· Op

(cid:16) log T

T h2 +

1

T 2rh2 + h4(cid:17)

= op((T he)− 1

2 ).

17

Next we prove that ˆt3(u, r) = op((T he)− 1

2 ).

ˆt3(u, r) =

1
T

T
(cid:88)

t=p+1

(cid:104)
Khe(t/T − u)(et−r − et−r(u))

g

(cid:17)

(cid:16) t
T

− ˆg

(cid:16) t
T

(cid:17)(cid:105)

+

1
T

T
(cid:88)

t=p+1

(cid:77)
= M1 + M2.

Khe(t/T − u)et−r(u)

(cid:17)

(cid:104)
g

(cid:16) t
T

− ˆg

(cid:16) t
T

(cid:17)(cid:105)

By (C1), it’s easy to show that

T
(cid:88)

t=p+1

K(

t/T − u
he

(cid:104)
)(et−r − et−r(u))

g

− ˆg

(cid:17)(cid:105)

(cid:16) t
T

(cid:17)

(cid:16) t
T
(cid:16)(cid:114)

(cid:88)

|t/T −u|≤C1he

(cid:12)
(cid:12)
C · Op(
(cid:12)

t
T

(cid:12)
(cid:12)
(cid:12) +
− u

(cid:17)

1
T

· Op

log T
T h2 +

1
T rh

+ h2(cid:17)

1
T he

1
T he

M1 =

≤

≤

(2T he + 1)C
T he
(cid:114)

(cid:16)

= Op

he

· Op(he) · Op

(cid:16)(cid:114)

log T

T h2 + h2(cid:17)

log T

T h2 + heh2(cid:17)

= op((T he)− 1

2 ).

On the other hand, notice that E(M2) = 0, we only need to show E(T he · M 2
M2 = op((T he)− 1

2 ). We have

2 ) → 0 for

E(T he · M 2

2 ) =

≤

≤

1
T he

C
T he

(cid:110) T
(cid:88)

E

t=p+1

K(

t/T − u
he

(cid:104)
)et−r(u)

g

(cid:17)

(cid:16) t
T

− ˆg

(cid:16) t
T

(cid:17)(cid:105)(cid:111)2

· ( sup
x∈[0,1]

|g(x) − ˆg(x)|)2 · E

(cid:16) (cid:88)

(cid:17)2

et−r(u)

|t/T −u|≤h

(2T he + 1)C
T he

· Op

(cid:16)log T

T h2 + h4(cid:17)

·

∞
(cid:88)

k=0

|γk(u)| −→ 0,

Thus ˆtr3(u) = op((T he)− 1
(ii) holds. The proof is completed.

2 ). Similarly it holds that ˆtr2(u) = op((T he)− 1

2 ). Then argument

Proposition 3.1 and Theorem 3.3 show that the estimator ˆφ(u) has the same convergence
rate and the asymptotic distribution as the estimator when et is observable as shown in Kim
(2001). Moreover, for the case that the order p of error series is unknown, Li and You (2020)
proposed the estimate of S1 and S2, viz., ˆS1ˆλ and ˆS2ˆγ respectively, which are used to identify
the coeﬃcient functions and the constant coeﬃcients. Theorem 3.4 shows that ˆS1ˆλ and ˆS2ˆγ
can identify the true model consistently. To prove it, we ﬁrst present some lemmas.

18

Lemma 3.2. Assume that conditions (C1)-(C11) are satisﬁed. Γ(u) deﬁned in (3.4) is non-
singular for all u ∈ (0, 1) and has uniformly bounded second derivatives. When (heλ)/
→ 0, and γ/(cid:112)T he log(T ) → 0 as T → ∞, we have

√

T he log T

(cid:107) ˆφλ(u) − φ∗(u)(cid:107) = Op(cT ),

sup
u

(cid:107)he( ˆφ(cid:48)

γ(u) − φ∗(cid:48)(u))(cid:107) = Op(cT ),

sup
u

where cT = (cid:0) log(T he)/T he

(cid:1)1/2 and (cid:107) · (cid:107) is the L2 norm.

Proof. The result of Lemma 3.2 can be directly derived from
(cid:107) ˆαλ,γ(u) − α∗(u)(cid:107) = Op(cT ).

sup
u

(3.20)

To prove (3.20), we ﬁrstly deﬁne a ball BC = {α(u) : α(u) = α∗(u) + cT r, (cid:107)r(cid:107) ≤ C}. By Fan
and Li (2001), we only need to show that for any (cid:15) > 0, there exists C > 0 which doesn’t
depend on u such that

(cid:16)

P

inf
(cid:107)r(cid:107)=C

Q(α∗(u) + cT r(cid:12)

(cid:12)ˆe, u) > Q(α∗(u)(cid:12)

(cid:12)ˆe, u)

(cid:17)

≥ 1 − (cid:15).

(3.21)

Let (cid:107)r(cid:107) = C. For any u ∈ (0, 1), by deﬁnition of Q(α(u)(cid:12)

R1 =

(cid:16)

he
log(1/he)

Q(α∗(u) + cT r(cid:12)

(cid:12)ˆe, u) − Q(α∗(u)(cid:12)

(cid:12)ˆe, u)

(cid:12) ˆX, u), we have
(cid:17)

=

he
log(1/he)

(cid:110) T
(cid:88)

(cid:16)

i=1

ˆeT − ˆZu(α∗(u) + cT r)

(cid:17)(cid:62)

(cid:17)
(cid:16)
ˆeT − ˆZu(α∗(u) + cT r)

We

−

+

+

i=1
p
(cid:88)

k=1
p
(cid:88)

T
(cid:88)

(cid:16)

ˆeT − ˆZuα∗(u)

(cid:17)(cid:62)

(cid:16)

We

ˆeT − ˆZuα∗(u)

(cid:17)(cid:111)

heλ
log(1/he)wk

(cid:16)(cid:12)
(cid:12)φ∗
(cid:12)

k(u) + cT rk

(cid:12)
(cid:12) − |φ∗
(cid:12)

k(u)|

(cid:17)

γ
log(1/he)w(cid:48)
k

(cid:16)(cid:12)
(cid:12)heφ∗(cid:48)
(cid:12)

k (u) + cT rp+k

(cid:12)
(cid:12) − |φ∗(cid:48)
(cid:12)

k (u)|

(cid:17)

.

k=1
By some simple calculation, we have

R1 ≥

(cid:114)

he
T

k(u) + cT rk

r(cid:62) ˆZ(cid:62)

u We
T
(cid:88)

+

+

k∈S1
(cid:88)

k∈S2

ˆZur

− 2

heλ
log(1/he)wk
γ
log(1/he)w(cid:48)
k

r(cid:62)
log(1/he)
(cid:16)(cid:12)
(cid:12)φ∗
(cid:12)
(cid:16)(cid:12)
(cid:12)heφ∗(cid:48)
(cid:12)

(cid:17)

u We(ˆeT − ˆZuα∗(u))
ˆZ(cid:62)
(cid:12)
(cid:12) − |φ∗
(cid:12)
(cid:12)
(cid:12) − |φ∗(cid:48)
(cid:12)

k (u)|

k(u)|

(cid:17)

.

k (u) + cT rp+k

19

Note that by Theorem 3.1, it holds that supt |ˆet − et| = supt |ˆg(t/T, Xt) − g(t/T, Xt)| =
Op

, which follows that

(cid:113) log T

T h2 + heh2(cid:17)

he

(cid:16)

ˆZur

r(cid:62) ˆZ(cid:62)

u We
T
u We(ˆeT − ˆZuα∗(u)) = Z(cid:62)
ˆZ(cid:62)

=

r(cid:62)Z(cid:62)

u WeZur

T

+ op(1),

u We(eT − Zuα∗(u)) + op(1).

(3.22)

(3.23)

Then we have

R1 ≥

r(cid:62)Z(cid:62)

u WeZur

− 2

T
(cid:88)

k∈S1
(cid:88)

k∈S2

+

+

heλ
log(1/he)wk
γ
log(1/he)w(cid:48)
k

r(cid:62)
log(1/he)
(cid:16)(cid:12)
(cid:12)φ∗
(cid:12)
(cid:16)(cid:12)
(cid:12)heφ∗(cid:48)
(cid:12)

(cid:114)

he
T

Z(cid:62)

u We(eT − Zuα∗(u))

k(u) + cT rk

(cid:17)

k(u)|

(cid:12)
(cid:12) − |φ∗
(cid:12)
(cid:12)
(cid:12) − |φ∗(cid:48)
(cid:12)

k (u) + cT rp+k

k (u)|

+ op(1),

(cid:17)

0 = lmin(cid:0)Λ(u)(cid:1) and lmin

Let lmin
eigenvalue of matrix A, and

T = inf u∈(0,1) lmin(cid:0) Z(cid:62)

u WeZu
T

(cid:1), where lmin(A) denotes the minimal

Λ(u) =

(cid:18)Γ(u)
Op

(cid:19)

Op
µ2Γ(u)

.

(cid:114)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

he
T

Z(cid:62)

u We

(cid:13)
(cid:13)
(cid:0)eT − Zuα∗(u)(cid:1)
(cid:13)
(cid:13)
(cid:13)
√
p
mink∈S2 w(cid:48)
k

·

γ
(cid:112)T he log(1/he)

(cid:107)r(cid:107) −

he
T

Z(cid:62)

u We

(cid:0)eT − Zuα∗(u)(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
√

(cid:107)r(cid:107) + op(1)

2p
mink∈S1 wk

−

γ
(cid:112)T he log(1/he)

·

2p
mink∈S2 w(cid:48)
k

(cid:41)

· C + op(1)

(cid:0)eT − Zuα∗(u)(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

hλ
(cid:112)T he log(1/he)

·

√

2p
mink∈S1 wk

Then

R1 ≥ (cid:107)r(cid:107)2lmin

T − 2(cid:107)r(cid:107) ·

√

sup
u

1
(cid:112)log(1/he)
p
mink∈S1 wk
(cid:13)
(cid:114)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

sup
u

heλ
(cid:112)T he log(1/he)
(cid:40)
2
(cid:112)log(1/he)
√

· C 2 −

·

−

= lmin
T

−

hλ
(cid:112)T he log(1/he)
· C 2 − A · C + op(1).

·

≡ lmin
T

where

A =

(cid:114)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

he
T

sup
u

2
(cid:112)log(1/he)
γ
(cid:112)T he log(1/he)

−

Z(cid:62)

u We
√

·

2p
mink∈S2 w(cid:48)
k

.

20

Similar with (3.2), we can show that

Z(cid:62)

u WeZu
T

p

−→ Λ(u).

(3.24)

Note that Γ(u) is nonsingular, it holds that lmin
lmin
−→ lmin
0

and (3.24), lmin

0 > 0.

p

T

0 > 0. Hence, by the deﬁnition of lmin

T

and

On the other side, it follows from condition (C1) and Lemma 6.1 of Fan and Yao (2008)

that

(cid:114)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

he
T

sup
u

Z(cid:62)

u We

(cid:13)
(cid:13)
(cid:0)eT − Zuα∗(u)(cid:1)
(cid:13)
(cid:13)
(cid:13)

= Op((cid:112)log(1/he)).

(3.25)

Note that mink∈S1 wk and mink∈S2 w(cid:48)
tively, which implies that

k converge in probability to a positive constant respec-

heλ
(cid:112)T he log(1/he)

·

√

2p
mink∈S1 wk

= op(1),

γ
(cid:112)T he log(1/he)

·

√

2p
mink∈S2 w(cid:48)
k

= op(1).

(3.26)

So it follows from (3.25) and (3.26) that A = Op(1). Therefore, as long as the constant C
· C 2 − A · C is positive, which implies that R1 > 0. Thus
is large enough, the value of lmin
(3.21) holds, which means that there exists a minimum in the ball BC for any u ∈ (0, 1)
with probability 1 − (cid:15). Therefore the minimizer ˆαλ,γ(u) of Q(α(u)(cid:12)
(cid:12)ˆe, u) must satisfy that
supu (cid:107) ˆαλ,γ(u) − α∗(u)(cid:107) = Op(cT ). The proof is completed.

T

Lemma 3.3. Assume that conditions (C1)-(C11) are satisﬁed. Γ(u) deﬁned in (3.4) is non-
T he →
singular for all u ∈ (0, 1) and has uniformly bounded second derivatives. When heλ/
T he → 0, T −1/5(log(T ))−1/2λ → ∞, and T −1/5(log(T ))−1/2γ → ∞ as T → ∞, we
0, γ/
have

√

√

(cid:16)

(i) P

supu | ˆφλ,Sc

1

(u)| = 0, supu | ˆφ(cid:48)

λ,Sc
2

(cid:17)

(u)| = 0

→ 1;

(ii) P ( ˆS1λ = S1, ˆS2γ = S2) → 1.

Proof. The proof for (i) is similar to Theorem 2 in Wang and Kulasekera (2012). One
diﬀerence is that the equations contain ˆet can be easily handled by using equation (3.22) and
(3.23), another is that Theorem 1 of Wang and Kulasekera (2012) are replaced by Lemma
3.2 in our case.

The consistency of ˆS1λ and ˆS2γ in (ii) is implied from (i).

Theorem 3.4. Assume conditions (C1)-(C11) are satisﬁed. Γ(u) deﬁned in (3.4) is nonsin-
gular for all u ∈ (0, 1) and has uniformly bounded second derivatives, then it holds that

P ( ˆS1ˆλ = S1, ˆS2ˆγ = S2) → 1,

as T → ∞.

21

Proof. The proof is similar with the i.i.d. case in Wang and Kulasekera (2012), we show the
procedure brieﬂy and omit the details. Denote ˆαλ,γ,k(u) as the kth component of ˆαλ,γ(u).
Let S = {1, · · · , p2, p2 + 1, · · · , p1} and ˆSλ,γ = {k : (cid:80)T
t=1 |ˆαλ,γ,k(t/T )| > 0}. Then Theorem
3.4 is equivalent to P ( ˆSˆλ,ˆγ = S) → 1. By (ii) of Lemma 3.3, we know that as T → ∞,
λT = γT = T 1/5 log(T ) satisﬁes that P ( ˆS1λT = S1, ˆS2γT = S2) → 1.

Then, we can divide R2 into three sets, i.e., R2

− = {(λ, γ) :
S (cid:54)⊂ Sλ,γ}, and R2
0 = {(λ, γ) : S = Sλ,γ}, corresponding to overﬁtted case, underﬁtted case,
and correctly ﬁtted case, respectively. The rest of the proof can be ﬁnished by mimicking
the proof of Theorem 4 in Wang and Kulasekera (2012), where Theorem 3 of in Wang and
Kulasekera (2012) are replaced by Lemma 3.3 in our case.

+ = {(λ, γ) : S (cid:40) Sλ,γ}, R2

In practice some data driven methods like grid search can be used to determine the
tuning parameters λ, γ. Actually, λ = γ is also allowed and it can save much computation
cost according to the condition of Lemma 3.3. Moreover, we can set γ = 0 in (2.4) if we only
focus on the purpose of identifying the nonzero coeﬃcients.

3.3 Asymptotic results on the reﬁned estimator of g(·, ·)

With the estimator of the error structure, ˆφ(u), Li and You Li and You (2020) derived a
reﬁned estimator of g(·, ·), which is denoted as ˆg∗(u, x). Theorem 3.5 presents the asymptotic
property of ˆg∗(u, x) and show that ˆg∗(u, x) is more eﬃcient than the preliminary estimator
ˆg(u, x).
Theorem 3.5. Let h∗ = Op(T − 1
holds for any u ∈ (0, 1) that

6 ). Under the conditions (C1)-(C11), r = min{ρ, 1} > 1

2, it

√

T h∗2










ˆg∗(u, x)
h∗ ∂ˆg∗(u,x)
∂u
h∗ ∂ˆg∗(u,x)
∂x



 −

D−→ N (0, V∗

u,x),





g(u, x)
h∗ ∂g(u,x)
∂u
h∗ ∂g(u,x)
∂x



 −






µ2h∗2
2

(cid:16) ∂2g(u,x)

∂2u + ∂2g(u,x)

∂2x

0
0

(cid:17)







 + op(h∗2)


as T → ∞.

where V∗

u,x = σ2

f (u,x) Σ and Σ is deﬁned in (3.3).

Proof. By model (1.1), it is easy to have

ˆY ∗
t = Yt −

p
(cid:88)

k=1

ˆφk(t/T )(cid:0)Yt−k − ˆg((t − k)/T, Xt−k)(cid:1)

= g(t/T, Xt) + (cid:15)t +

p
(cid:88)

(cid:0)φk(t/T ) − ˆφk(t/T )(cid:1)et−k +

− g((t − k)/T, Xt−k)(cid:9).

k=1

22

p
(cid:88)

k=1

ˆφk(t/T )(cid:8)ˆg((t − k)/T, Xt−k)

(3.27)

Let MT = 1

T Z∗(u, x)(cid:62)W∗(u, x)Z∗(u, x). It follows from (3.27) that





ˆg∗(u, x)
∂ˆg∗(u,x)
he
∂u
∂ˆg∗(u,x)
he
∂x


 = M−1

T






1
T
1
T







1
T
(cid:80)T
(cid:80)T

(cid:80)T

t=1 Kh∗( t

t=1 Kh∗( t

T , Xt)
T , Xt)





T − u)Kh∗(Xt − x)g( t
T , Xt)
T − u)Kh∗(Xt − x)(cid:0) t
(cid:1)g( t
T −u
h∗
T − u)Kh∗(Xt − x)( Xt−x
h∗ )g( t
T − u)Kh∗(Xt − x)(cid:15)t
T − u)Kh∗(Xt − x)(cid:0) t
(cid:1)(cid:15)t
T −u
h∗
T − u)Kh∗(Xt − x)( Xt−x
h∗ )(cid:15)t
T − u)Kh∗(Xt − x) (cid:80)p
k=1
T − u)Kh∗(Xt − x)(cid:0) t
(cid:1) (cid:80)p
T −u
h∗
h∗ ) (cid:80)p
T − u)Kh∗(Xt − x)( Xt−x
k=1

T − u)Kh∗(Xt − x)∆
T − u)Kh∗(Xt − x)(cid:0) t
(cid:1)∆
T −u
h∗
T − u)Kh∗(Xt − x)( Xt−x
h∗ )∆




k=1

t=1 Kh∗( t
t=1 Kh∗( t
(cid:80)T

1
T
(cid:80)T
(cid:80)T

1
T
(cid:80)T
(cid:80)T

1
T
(cid:80)T
(cid:80)T

t=1 Kh∗( t
t=1 Kh∗( t
(cid:80)T

t=1 Kh∗( t
t=1 Kh∗( t
(cid:80)T

t=1 Kh∗( t
t=1 Kh∗( t

t=1 Kh∗( t

t=1 Kh∗( t

+ M−1
T

+ M−1
T

+ M−1
T














1
T
1
T

1
T
1
T

1
T
1
T

(cid:0)φk( t

T )(cid:1)et−k

T ) − ˆφk( t
(cid:0)φk( t
(cid:0)φk( t

T ) − ˆφk( t
T ) − ˆφk( t

T )(cid:1)et−k
T )(cid:1)et−k






(cid:77)
= J1 + J2 + J3 + J4,

where ∆ = (cid:80)p

k=1

ˆφk(t/T )(cid:2)ˆg(cid:0) t−k

T , Xt−k

(cid:1) − g(cid:0) t−k

T , Xt−k

(cid:1)(cid:3).

(i) Following the same way as the proof of Theorem 3.2, we can easily show that

√





T h∗

J1 −









 =






µ2h2
e
2

g(u, x)
h ∂g(u,x)
∂u
h ∂g(u,x)
∂x

(cid:16) ∂2g(u,x)

∂2u + ∂2g(u,x)

∂2x

0
0

(cid:17)


 + op(h∗2).


(ii) By Theorem 3.3 and the proof of Theorem 3.2, it follows that

√

T h∗J2

D−→ N (0, V∗

u,x),

as T → ∞,

where V∗

u,x = σ2

f (u,x) Σ and Σ is deﬁned in (3.3).

(iii) Theorem 3.3 implies that (cid:107) ˆφ(u) − φ(u)(cid:107) = Op(h2

e + 1
T he

). So it holds that






(cid:13)
(cid:13)
M−1
(cid:13)
(cid:13)
T
(cid:13)






(cid:13)
(cid:13)
M−1
(cid:13)
(cid:13)
T
(cid:13)

≤

= Op(h2

e).

1
T
1
T

1
T
1
T

1
T
(cid:80)T
(cid:80)T

(cid:80)T

t=1 Kh∗( t

t=1 Kh∗( t
t=1 Kh∗( t
(cid:80)T

t=1 Kh∗( t

T − u)Kh∗(Xt − x) (cid:80)p
k=1
T − u)Kh∗(Xt − x)(cid:0) t
(cid:1) (cid:80)p
T −u
h∗
h∗ ) (cid:80)p
T − u)Kh∗(Xt − x)( Xt−x
k=1

T − u)Kh∗(Xt − x)
(cid:13)
(cid:13)
T − u)Kh∗(Xt − x)(cid:0) t
(cid:1)
T −u
(cid:13)
(cid:13)
h∗
(cid:13)
T − u)Kh∗(Xt − x)( Xt−x
h∗ )




k=1

t=1 Kh∗( t
t=1 Kh∗( t

1
T
(cid:80)T
(cid:80)T

(cid:0)φk( t

T )(cid:1)et−k

T ) − ˆφk( t
(cid:0)φk( t
(cid:0)φk( t

T ) − ˆφk( t
T ) − ˆφk( t

T )(cid:1)et−k
T )(cid:1)et−k






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
t∈{p+1,··· ,T }

(cid:107)

p
(cid:88)

k=1

(cid:0)φk(

t
T

) − ˆφk(

)(cid:1)et−k(cid:107)

t
T

23

Therefore, J3 = op

(cid:18)

h∗2 +

(cid:113) log T
T h∗2

(cid:19)
.

(iv) Condition (C8) implies that there exist some constant C such that |φk(t/T )| ≤ C < ∞
for all k = 1, · · · , p, then

∆ ≤ p × max

k∈{1,··· ,p}

{φk(t/T ) + op(1)} × sup
u,x

|ˆg(u, x) − g(u, x)|

(cid:16)(cid:114)

log T

T h2 + h2(cid:17)

= p[C + op(1)] × Op

(cid:16)(cid:114)

= Op

log T

T h2 + h2(cid:17)

.

By the standard results from density estimation, it holds that J4 = op(h∗2).

Combining the results of (i)-(iv), we have that

√

T h∗2












 −





ˆg∗(u, x)
h∗ ∂ˆg∗(u,x)
∂u
h∗ ∂ˆg∗(u,x)
∂x




√

T h∗2

=


J1 + J2 −



g(u, x)
h∗ ∂g(u,x)
∂u
h∗ ∂g(u,x)
∂x

g(u, x)
h∗ ∂g(u,x)
∂u
h∗ ∂g(u,x)
∂x




 −






 −

µ2h∗2
2






(cid:16) ∂2g(u,x)

∂2u + ∂2g(u,x)

∂2x

0
0

(cid:17)






 + op(h∗2)




µ2h∗2
2

(cid:16) ∂2g(u,x)

∂2u + ∂2g(u,x)

∂2x

0
0

(cid:17)







 + op(h∗2)


D−→ N (0, V∗

u,x),

as T → ∞.

Remark 3.5. There are two bandwidths h and h∗ in the procedure of estimating g(u, x).
Theorem 3.5 shows that the bandwidth h∗ in the reﬁned estimator should be of the standard
order of estimating a binary nonparametric function. However, the bandwidth h for the
preliminary estimator ˆg(u, x) should be of smaller order h = o(h∗) to control the bias in
the ﬁrst step of the estimation. Specially, to ensure the optimal convergence rate of the
estimators of the error autoregressive structure as shown in Theorem 3.3, the order of h
should be even smaller than the bandwidth he, i.e., h = o(T −1/5).
In practice, standard
bandwidth selection methods can be utilized for h∗ and he. Then h∗ can be multiplied by a
constant like 0.5 to obtain h as suggested by Liu et al. (2010).

4 Numerical studies

Li and You (2020) conducted two real data studies for their proposed method.
In order
to give an overall evaluation on the ﬁnite sample performance of their proposed method,
two simulation studies are conducted in this section. The Epanechnikov kernel K(t) =

24

0.75(1−t2)+ is used throughout this section. In each study, the leave-one-out cross-validation
is applied to select the optimal bandwidth for h∗ and he, and h = 0.5h∗ is for the preliminary
estimator. Since the results are not very sensitive to the bandwidth, only the case of the
optimal bandwidth is reported here.

The ﬁrst study is designed to compare the performance of the preliminary estimator and

the reﬁned estimator of the time-varying nonparametric function.

Example 1. Assume that the order of the error autoregressive structure is known. Yt is
generated by the model

Yt = g(t/T, Xt) + et,

t = 1, · · · , T,

where g(u, x) = 1.5 cos(2πu)x2, and the explanatory variable Xt is generated from a time-
varying AR(1) process that is locally stationary, i.e.,

Xt = 0.7t/T Xt−1 + 0.5ξt,

where ξt ∼ N (0, 1). The error term et is generated by the time-varying autoregressive
process et − (cid:80)5
i=1 φi(t/T )et−i = (cid:15)t, where (cid:15)t follows normal distribution N (0, σ2), and the
time-varying autoregressive coeﬃcient function φ(u) = (φ1(u), φ2(u), φ3(u), φ4(u), φ5(u))(cid:62)
are speciﬁed as follows.

Model (a) φ(u) = ((−0.1 + 0.6 sin(2πu), 0, 0, 0, 0)(cid:62);

Model (b) φ(u) = (3(u − 0.4)2 − 0.6, 0.3, 0, 0, 0)(cid:62);

Model (c) φ(u) = (5(u − 0.5)2 − 0.6, −1 + sin2(πu), 0, 0, 0)(cid:62).

Moreover, the square root of average squared error (RASE) criterion is used to evaluate

the performance of the estimators. For an estimator ˆg(u, x), its RASE is deﬁned as

RASE (cid:0)ˆg(u, x)(cid:1) =

(cid:20) 1
T

T
(cid:88)

(cid:16)

t=1

ˆg(cid:0) t
T

, Xt

(cid:1) − g(cid:0) t
T

(cid:1)(cid:17)2(cid:21)1/2

.

, Xt

With T = 200, 300, 400, σ = 0.5, 1, and φ(u) being model (a)-(c), the empirical mean
values and standard deviations (SD) of RASE based on 500 replications are presented in
Table 1. For the comparison purpose, the oracle estimator ˆgOR(u, x), which is the local
linear estimator of g(u, x) when the time-varying autoregressive error structure is completely
known, is also calculated and presented in Table 1.

From Table 1, we can draw the following conclusions:

• Under all of three error term models, the reﬁned estimator ˆg∗(u, x) has both smaller
mean values and standard deviations of RASE than the preliminary estimator ˆg(u, x).
The improvement of ˆg∗(u, x) is getting more signiﬁcant as the complexity of the error
term increases.

25

φ(u)

σ

T

ˆg(u, x)

ˆg∗(u, x)

ˆgOR(u, x)

Model (a)

Model (b)

Model (c)

0.5

1.0

0.5

1.0

0.5

1.0

Mean

SD Mean

SD Mean

SD

200
300
400

200
300
400

200
300
400

200
300
400

200
300
400

200
300
400

0.253
0.208
0.182

0.505
0.415
0.362

0.312
0.260
0.231

0.624
0.519
0.459

0.366
0.322
0.298

0.732
0.643
0.593

0.029
0.024
0.020

0.058
0.047
0.040

0.048
0.041
0.031

0.096
0.081
0.062

0.079
0.074
0.070

0.159
0.148
0.141

0.166
0.145
0.136

0.302
0.248
0.222

0.189
0.165
0.154

0.351
0.292
0.262

0.193
0.162
0.149

0.359
0.286
0.251

0.027
0.022
0.019

0.052
0.042
0.035

0.040
0.035
0.027

0.080
0.070
0.055

0.047
0.035
0.029

0.098
0.073
0.061

0.161
0.142
0.135

0.290
0.242
0.219

0.161
0.142
0.135

0.290
0.242
0.219

0.161
0.142
0.135

0.290
0.242
0.219

0.024
0.021
0.018

0.046
0.037
0.031

0.024
0.021
0.018

0.046
0.037
0.032

0.024
0.021
0.018

0.046
0.037
0.032

Table 1: Means and SDs of the RASEs of the estimators for g(u, x) = 1.5 cos(2πu)x2

26

• An increase in T results in a decrease in the mean values and standard deviations of
RASE for all estimators. An increase in σ results in an increase in the mean values
and standard deviations of RASE for all estimators.

• The performance of the reﬁned estimator ˆg∗(u, x) is very close to ˆgOR(u, x), which is

also consistent with the theoretical results.

Figure 1 depicts the boxplots of ˆg(u, x) and ˆg∗(u, x) at points (u, x) = (0.2, −0.5), (0.5, 0), (0.75, 0.4)

under Model (b). It shows that the standard error is decreasing with the increase of T , and
the bias is negligible when T is large, which is claimed by Theorem 3.5.

To further illustrate the performance of the reﬁned estimator, we plot the average reﬁned
estimated function, the true function, and the bias of the average estimated function in
Figure 2 for case (a) with T = 300 and σ = 0.5. The estimated surface is consistent with
the true surface, which validate the theoretical results. We omit the results for other cases,
which are similar.

Example 2. In this example, we aim to show the performance of the ULASSO estimator.
The model settings and simulation settings are the same as in Example 1 except that the
It’s obvious that Model (a) is a
order of the error autoregressive structure is unknown.
tvAR(1) with S1 = S2 = {1}; Model (b) is a tvAR(2) with S1 = {1, 2} and S2 = {1}; Model
(c) is a tvAR(2) with S1 = S2 = {1, 2}.

Under the assumption that the order of the error autoregressive structure is unknown,
the ULASSO estimator is adopted to estimate the autoregressive structure. ˆφk(u) is denoted
as the optimal ULASSO estimate of which the shrinkage parameters are determined by the
BIC criterion (2.5). To evaluate the performance of ULASSO estimate, the result of variable
selection (VS, i.e. nonzero coeﬃcient selection) is classiﬁed as Wang and Kulasekera (2012):
1. underﬁtted (at least one true nonzero variable is missing); 2. correctly ﬁtted; 3. overﬁtted
(all the signiﬁcant variables are identiﬁed while at least one spurious variable is included).
The percentages in each category are presented under the heading ‘VS’ in Table 2. Similarly,
we can partition the results for detecting the true coeﬃcient functions and constant coeﬃcient
into these categories. Therefore, a correct selection means that the procedure identiﬁed both
types correctly, an underﬁtted indicating missing at least one true function or a coeﬃcient
etc. These percentages are given under the heading ‘VS & CI’ in Table 2.

From Table 2, we can see that for every model, the percentage of correctly ﬁtted models
is satisfying and it increases steadily with the sample size. Furthermore, the percentage of
correct selection (VS & CI) is also acceptable, especially when the sample size is moderate
or large. On the other hand the percentage of VS & CI is slightly less than that of VS. This
is not surprising since the convergence speed of the derivative is a little slower than that of
the coeﬃcient function.

To further evaluate the ULASSO estimate, the mean values and standard deviations
of RASE of ˆφk(u) are calculated and presented in Table 3. Moreover, the performance of

27

Figure 1: Boxplots of ˆg(u, x) and ˆg∗(u, x) for Model (b). The left panel: σ = 0.5, the right
panel: σ = 1. The top panel: (u, x) = (0.2, −0.5), middle panel: (u, x) = (0.5, 0), bottom
panel: (u, x) = (0.75, 0.4)

28

φ(u)

σ

T

VS

VS & CI

Under Correct Over Under Correct Over

Model (a)

Model (b)

Model (c)

0.5

1.0

0.5

1.0

0.5

1.0

200
300
400

200
300
400

200
300
400

200
300
400

200
300
400

200
300
400

0.050
0.006
0.000

0.206
0.048
0.000

0.088
0.024
0.000

0.114
0.028
0.000

0.172
0.006
0.002

0.192
0.006
0.004

0.872
0.916
0.962

0.724
0.880
0.938

0.712
0.802
0.890

0.672
0.790
0.870

0.480
0.608
0.694

0.450
0.570
0.664

0.078
0.078
0.038

0.070
0.072
0.062

0.200
0.174
0.110

0.214
0.182
0.130

0.348
0.386
0.304

0.358
0.424
0.332

0.182
0.030
0.008

0.294
0.064
0.004

0.274
0.092
0.020

0.314
0.100
0.026

0.208
0.006
0.004

0.234
0.008
0.006

0.734
0.888
0.952

0.626
0.856
0.928

0.496
0.692
0.846

0.454
0.688
0.820

0.404
0.554
0.650

0.382
0.528
0.602

0.084
0.082
0.040

0.080
0.080
0.068

0.230
0.216
0.134

0.232
0.212
0.154

0.388
0.440
0.346

0.384
0.464
0.392

Table 2: The percentage of underﬁtted / correctly ﬁtted /overﬁtted ﬁtted models selection.
VS: variable selection; CI: constant coeﬃcient identiﬁcation

29

Figure 2: Averaged reﬁned estimated function, true function, and bias of the average esti-
mated function

constant coeﬃcient estimation for φ2(·) ≡ φ2 in Model (b) is also evaluated. If it is identiﬁed
as the constant, i.e., 2 /∈ ˆS2, the estimate of φ2 is taken as the mean of estimate at each time
point, i.e.,

ˆφ2 =

1
T

T
(cid:88)

t=1

ˆφ2(t/T ).

The bias and the standard error (SE) of ˆφ2 are included in the last two columns of Table 3.
It shows from Table 3 that under all of three error term models, an increase in T results in a
decrease in the mean values and standard deviations of RASE for all estimates. An increase
in σ results in an increase in the mean values and standard deviations of RASE. Moreover,
for the constant coeﬃcient estimation ˆφ2 for Model (b), the bias and the standard error of ˆφ2
also decreases with the increase of T . From Table 3, we can conclude that the performance of
the estimates for both time-varying coeﬃcient function and the non-zero constant coeﬃcient
are satisfying.

5 Conclusion

In this paper, we focus on the theoretical results of statistical inference for a class nonpara-
metric regression models with time-varying regression function, local stationary regressors
and time-varying AR(p) (tvAR(p)) error process (1.1), for which the estimation procedure is

30

φ(u)

σ

T

RASE

ˆφ1(·)

ˆφ2(·)

estimate
ˆφ2

Model (a)

Model (b)

Model (c)

0.5

1.0

0.5

1.0

0.5

1.0

mean

SD mean

SD

bias

SE

200
300
400

200
300
400

200
300
400

200
300
400

200
300
400

200
300
400

0.260
0.223
0.189

0.279
0.228
0.205

0.200
0.173
0.158

0.206
0.173
0.162

0.275
0.211
0.179

0.280
0.223
0.194

0.066
0.047
0.038

0.089
0.060
0.034

0.047
0.038
0.031

0.056
0.040
0.032

0.082
0.054
0.052

0.084
0.055
0.054

−
−
−

−
−
−

0.136
0.102
0.075

0.149
0.109
0.081

0.226
0.155
0.125

0.240
0.155
0.127

−
−
−

−
−
−

0.075
0.061
0.042

0.078
0.064
0.045

0.101
0.049
0.040

0.110
0.052
0.041

−
−
−

−
−
−

−
−
−

−
−
−

0.094
0.057
0.028

0.116
0.071
0.040

0.107
0.088
0.066

0.108
0.088
0.068

−
−
−

−
−
−

−
−
−

−
−
−

Table 3: Means and SDs of the RASEs of the estimator for φ(u) and bias and SE of the
constant coeﬃcient estimation

31

studied by Li and You (2020) with no theoretical discussion. With regard to their estimators,
we establish the convergence rate and asymptotic normality of the preliminary estimation
ˆg(u, x) of nonparametric regression function, show that their estimator of error term ˆφk(u)
has the same convergence rate and the asymptotic distribution as the estimator when et is ob-
servable, present the asymptotic property of the reﬁned estimation ˆg∗(u, x) of nonparametric
regression function, and show that ˆg∗(u, x) is more eﬃcient than the preliminary estimator
ˆg(u, x).
In addition, with regard to the ULASSO method for variable selection and con-
stant coeﬃcient detection for error term structure, we show that the ULASSO estimator can
identify the true error term structure consistently. Simulation results have been provided
to illustrate the ﬁnite sample performance of the estimators and validate our theoretical
discussion on the properties of the estimators.

The model introduced by Li and You (2020) is with time-varying AR(p) (tvAR(p)) error
process. Actually, it can be extended to the model with time-varying autoregressive moving
average model (tvARMA(p,q)), which is also local stationary. The essential thought on this
article can be also extended to the estimations under the model with tvARMA(p,q) error
term.

Acknowledgements

We would like to express our gratitude to Dr. Jinhong You for his heuristic discussion on
the article. Our thanks also go to the referees for their time and comments.

References

Bellegem, S. V. and Dahlhaus, R. (2006). Semiparametric estimation by model selection for
locally stationary processes. Journal of the Royal Statistical Society: Series B, 68:721–746.

Brockwell, P. J. and Davis, R. A. (2016).
Springer-Verlag, New York, 3 edition.

Introduction to time series and forecasting.

Cai, Z., Fan, J., and Li, R. (2000a). Eﬃcient estimation and inferences for varying-coeﬃcient

models. Journal of the American Statistical Association, 95(451):888–902.

Cai, Z., Fan, J., and Yao, Q. (2000b). Functional-coeﬃcient regression models for nonlinear

time series. Journal of the American Statistical Association, 95(451):941–956.

Dahlhaus, R. (1996). Asymptotic statistical inference for nonstationary processes with evo-

lutionary spectra. Lecture Notes in Statistics, 115:145–¨C159.

Fan, J. and Gijbels, I. (1996). Local polynomial modelling and its applications. Monographs

on Statistics and Applied Probability. Chapman & Hall/CRC.

32

Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its
oracle properties. Journal of the American statistical Association, 96(456):1348–1360.

Fan, J. and Yao, Q. (2008). Nonlinear time series: nonparametric and parametric methods.

Springer Science & Business Media.

Hansen, B. E. (2008). Uniform convergence rates for kernel estimation with dependent data.

Econometric Theory, 24(3):726–748.

Kim, W. (2001). Nonparametric kernel estimation of evolutionary autoregressive processes.
Technical report, Discussion Papers, Interdisciplinary Research Project 373: Quantiﬁca-
tion and Simulation of Economic Processes.

Li, J. and You, J. (2020). Time-varying nonparametric regression models with the locally
stationary error process and its applications in ﬁnance. Journal of Applied Statistics and
Management, to appear.

Liu, J. M., Chen, R., and Yao, Q. (2010). Nonparametric transfer function models. Journal

of econometrics, 157(1):151–164.

Pei, Y., Huang, T., and You, J. (2018). Nonparametric ﬁxed eﬀects model for panel data

with locally stationary regressors. Journal of Econometrics, 202(2):286–305.

Qiu, D., Shao, Q., and Yang, L. (2013). Eﬃcient inference for autoregressive coeﬃcients in

the presence of trends. Journal of Multivariate Analysis, 114:40–53.

Ruppert, D. and Wand, M. P. (1994). Multivariate locally weighted least squares regression.

The annals of statistics, pages 1346–1370.

Schr¨oder, A. L. and Fryzlewicz, P. (2013). Adaptive trend estimation in ﬁnancial time series

via multiscale change-point-induced basis recovery.

Shao, Q. and Yang, L. (2017). Oracally eﬃcient estimation and consistent model selection
for auto-regressive moving average time series with trend. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 79(2):507–524.

Su, L. and Ullah, A. (2006). More eﬃcient estimation in nonparametric regression with

nonparametricautocorrelated errors. Econometric Theory, 22:98–126.

Truong, Y. K. (1991). Nonparametric curve estimation with time series errors. Journal of

Statistical Planning and Inference, 28(2):167–183.

Vogt, M. (2012). Nonparametric regression for locally stationary time series. The Annals of

Statistics, 40(5):2601–2633.

Wang, D. and Kulasekera, K. (2012). Parametric component detection and variable selection
in varying-coeﬃcient partially linear models. Journal of Multivariate Analysis, 112:117–
129.

33

Xiao, Z., Linton, O. B., Carroll, R. J., and Mammen, E. (2003). More eﬃcient local poly-
nomial estimation in nonparametric regression with autocorrelated errors. Journal of the
American Statistical Association, 98:980–992.

34

