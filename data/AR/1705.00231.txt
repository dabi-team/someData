1
2
0
2

g
u
A
1
3

]
T
S
.
h
t
a
m

[

2
v
1
3
2
0
0
.
5
0
7
1
:
v
i
X
r
a

Optimal Invariant Tests in an Instrumental Variables
Regression With Heteroskedastic and Autocorrelated
Errors 1

Marcelo J. Moreira
FGV

Mahrad Sharifvagheﬁ
University of Pittsburgh

Geert Ridder
USC

September 2, 2021

1Preliminary results were presented at seminars organized by BU, Brown, Caltech, Harvard-
MIT, PUC-Rio, University of California (Berkeley, Davis, Irvine, Los Angeles, Santa Barbara,
and Santa Cruz campuses), UCL, USC, and Yale, at the FGV Data Science workshop, and at
conferences organized by CIREq (in honor of Jean-Marie Dufour), Harvard University (in honor of
Gary Chamberlain), Oxford University (New Approaches to the Identiﬁcation of Macroeconomic
Models), the Tinbergen Institute (Inference Issues in Econometrics), and Vanderbilt (Identiﬁcation
in Econometrics. This study was ﬁnanced in part by the Coordena¸c˜ao de Aperfei¸coamento de
Pessoal de N´ıvel Superior - Brazil (CAPES) - Finance Code 001. This research was also supported
in part by the University of Pittsburgh Center for Research Computing through the resources
provided.

 
 
 
 
 
 
Abstract

This paper uses model symmetries in the instrumental variable (IV) regression to derive an
invariant test for the causal structural parameter. Contrary to popular belief, we show that
there exist model symmetries when equation errors are heteroskedastic and autocorrelated
(HAC). Our theory is consistent with existing results for the homoskedastic model (Andrews,
Moreira, and Stock (2006) and Chamberlain (2007)). We use these symmetries to propose the
conditional integrated likelihood (CIL) test for the causality parameter in the over-identiﬁed
model. Theoretical and numerical ﬁndings show that the CIL test performs well compared
to other tests in terms of power and implementation. We recommend that practitioners
use the Anderson-Rubin (AR) test in the just-identiﬁed model, and the CIL test in the
over-identiﬁed model.

1

Introduction

In a regression model, the explanatory variable can be correlated with the error due to omit-
ted variables. To solve this endogeneity problem, practitioners often look for instrumental
variables (IVs). The instruments are valid if they are correlated with the endogenous variable
but uncorrelated with the error. The instruments are said to be weak when their correla-
tion with the endogenous explanatory variable is small. Under weak identiﬁcation, standard
estimators may be far from the true causality parameter, and commonly-used tests do not
have correct size. Searching for valid IVs can, unfortunately, narrow down the choices to
only weak instruments. Furthermore, techniques proposed to mitigate these problems can
themselves have limitations. Cruz and Moreira (2005) show that the second-order bias for
the two-stage least squares (2SLS) estimator is unreliable under weak identiﬁcation. Lee,
McCrary, Moreira, and Porter (2020) point out that the standard F¿10 rule for the t-ratio
leads to important size distortions in practice, even in the just-identiﬁed model. They pro-
pose a novel tF procedure if practitioners wish to use the F statistic combined with the
t-ratio.

With cross-sectional data, the errors in the IV model can be heteroskedastic. With
time-series or panel data, errors can also be autocorrelated. For these more complex data
generating processes (DGPs) for the errors, the asymptotic variance matrix of sample IV
moments can be quite diﬀerent from the one obtained from serially uncorrelated and ho-
moskedastic errors. Consistent estimators for this variance are readily available: see Newey
and West (1987) and Andrews (1991).

Andrews, Moreira, and Stock (2006) (abbreviated as AMS06 hereinafter) and Chamber-
lain (2007) show that symmetries exist in the IV model with homoskedastic errors when
the variance is ﬁxed. Up until the emergence of this project (Moreira and Ridder (2017)),
it was widely accepted that there are no symmetries in the IV model with heteroskedastic
and/or autocorrelated (HAC) errors. Indeed, at ﬁrst sight, invariance does not seem appli-
cable to the HAC-IV model. We argue that this view is incorrect. The HAC-IV has model
symmetries if the HAC variance matrix is assumed to be known but not ﬁxed, an important
distinction from the method used by AMS06, Chamberlain (2007), and related papers, for
homoskedastic and uncorrelated errors. We ﬁnd the largest aﬃne group which preserves the
null hypothesis for the causality parameter. This allows us to ﬁnd weights for the novel
conditional integrated likelihood (CIL) test. This test is invariant and can be interpreted
as the limit of a sequence of conditional weighted-average power (WAP) tests. Andrews
and Mikusheva (2020) provide a general framework for decision rules in GMM. They do not
propose a two-sided test, which is our main goal in this paper. Unlike the CIL test, other
(limits of) conditional WAP tests can be severely biased, as the critique by Moreira and
Moreira (2019) asserts.

In the just-identiﬁed model, the AR test is optimal within the classes of either unbiased
or invariant tests, assuming the reduced-form variance mentioned above is known; see Mor-
eira (2002, 2009), AMS06, and Moreira and Moreira (2019). In the over-identiﬁed model,
the AR test is not eﬃcient under the usual asymptotic theory. Several proposed tests are
asymptotically optimal under standard asymptotics. Moreira and Ridder (2020) show that
the Lagrange Multiplier/score (LM) and the conditional quasi-likelihood ratio (CQLR) tests
can suﬀer severe power deﬁciencies when distinguishing the null from the alternative hypoth-

1

esis should be easy. The weighted-average strongly unbiased (SU) tests are not invariant for
arbitrary weight choices. Furthermore, implementing the SU tests requires linear program-
ming. Although algorithms are readily available, it requires the calculation of a density
If not dealt with properly, the computation can exceed the numerical accuracy of
ratio.
computer packages. This leaves, as the main contender, the true conditional likelihood ra-
tio (CLR) test; see Andrews and Mikusheva (2016) and Moreira and Moreira (2019). The
CLR test does not have a closed-form expression with HAC errors, and requires numerical
optimization. We show some important limitations to the implementation of the CLR test.
We prove here that we can compactify the parameter space for the optimization. This is
important, and avoids some natural pitfalls if the parameter space is unbounded. However,
we ﬁnd that the number of initial points needed for the optimization algorithm depends
on the errors’ DGPs and on the instrument strength. In practice, we document the need
to include several initial points when errors are HAC. Worse yet, the optimization can be
even more troublesome when computing the conditional quantiles. This happens because
the model is misspeciﬁed for DGPs under the alternative hypothesis when we simulate these
null conditional quantiles.

We then compare power between the AR, CLR, and CIL tests. For homoskedastic errors,
the CLR test simpliﬁes to the CQLR test, which has a closed-form expression. Andrews,
Moreira, and Stock (2004) use these symmetries to choose weights for a conditional WAP
test. Power gains beyond the CQLR are small, as the latter performs near a two-sided
power envelope for invariant tests with homoskedastic errors. It is, however, reassuring that
the CIL test performs on an equal footing with the CQLR test. For HAC errors, the IV
model is much more complex than the simple homoskedastic model. Even reducing the data
using invariance, several parameters can aﬀect the performance of AR, LM, CQLR, CLR,
CIL, and any other invariant tests. We choose the same designs as Moreira and Moreira
(2019) and Moreira and Ridder (2020), to forestall any criticism that we may be selecting
parameter combinations which favor the CIL test. To bypass the aforementioned numerical
problems for the CLR test, we choose to implement an infeasible version of the CLR test,
in case better optimization methods are found in the future. This implementation selects
the unknown value of the structural parameter as one of the initial points in the likelihood
optimization. Overall, the CIL test outperforms the AR and CLR tests, with signiﬁcant
power gains for several of these designs.

This paper is organized as follows. Section 2 introduces the IV model, and describes
the family of similar tests robust to heteroskedastic-autocorrelated errors. Section 3 shows
model symmetries when the asymptotic variance can change with data transformations. We
present diﬀerent representations of the CIL test. One of them is important to show that
this test is invariant, as discussed later. The other expression is useful to derive conﬁdence
sets based on the CIL test. Section 4 shows that the CIL test can have very good power
(the supplement provides further evidence in favor of the CIL test). The more technical
details behind model invariance are left to the end of the paper. Section 5 shows that the
theory of AMS06 is a special case of ours when the variance has a Kronecker product form.
Section 6 derives the theory of conditional invariant tests. It shows that the AR, CLR, and
Lagrange Multiplier/score (LM) tests are also invariant. Section 7 discusses the next steps in
this research agenda and highlights the methodological importance of distinguishing between
parameters being known and being ﬁxed. The online appendix provides the proofs for our

2

theory.

2 The IV Model and Statistics

2.1 The HAC-IV model

Consider the following structural equation for the i-th observation of the variable y1:

y1i = y2iβ + x(cid:48)

iγ1 + ui, for i = 1, 2, · · · , n,

(2.1)

where y2i is an endogenous random variable with corresponding coeﬃcient β ∈ R, xi =
(x1i, x2i, · · · , xpi)(cid:48) ∈ RP is a ﬁxed vector of exogenous control variables with corresponding
p)(cid:48) ∈ Rp, and ui is an error term. We also consider
vector of coeﬃcients γ1 = (γ∗
the following reduced-form equation for the endogenous explanatory random variable:

2, · · · , γ∗

1, γ∗

y2i = ˜z(cid:48)

iπ + x(cid:48)

iξ1 + v2i, for i = 1, 2, · · · , n,

(2.2)

where ˜zi = (˜z1i, ˜z2i, · · · , ˜zki)(cid:48) ∈ Rk is a ﬁxed vector of instrumental variables (IVs) with
corresponding coeﬃcients π = (π1, π2, · · · , πk)(cid:48) ∈ Rk, ξ1 = (ξ1, ξ2, · · · , ξp) ∈ Rp, and an
error term v2i. It may be possible that E(ν2iui) (cid:54)= 0, so that y2 is an endogenous random
variable. Equations (2.1) and (2.2) can be presented in the following matrix format:

y1 = y2β + Xγ1 + u
y2 = (cid:101)Zπ + Xξ1 + v2,

(2.3)

where y1 = (y11, y12, · · · , y1n)(cid:48) ∈ Rn, y2 = (y21, y22, · · · , y2n)(cid:48) ∈ Rn, X = (x1, x2, · · · , xn)(cid:48) ∈
Rn×p, (cid:101)Z = (˜z1, ˜z2, · · · , ˜zn)(cid:48) ∈ Rn×k, u = (u1, u2, · · · , un)(cid:48) ∈ Rn, and v2 = (v21, v22, · · · , v2n)(cid:48) ∈
Rn. We assume that the matrix Z = [ (cid:101)Z : X] has full column rank k + p.

Our focus is on testing the null hypothesis H0 : β = β0 against the two-sided alternative
hypothesis H1 : β (cid:54)= β0.
It is convenient to transform the IV matrix (cid:101)Z to Z which is
orthogonal to matrix X, Z (cid:48)X = 0. For a conformable matrix A, we deﬁne NA = A(A(cid:48)A)−1A(cid:48)
and MA = I − NA. We write

y2 = Zπ + Xξ + v2,

(2.4)

where Z = MX (cid:101)Z and ξ = ξ1 + (X (cid:48)X)−1X (cid:48) (cid:101)Zπ. By substituting y2 from the reduced-form
equation (2.4) to the structural equation (2.3), we have

y1 = Zπβ + Xγ + v1,

(2.5)

where γ = γ1 + ξβ and v1 = u + v2β. The reduced-form equations (2.4) and (2.5) can be
written in the following matrix notation:

Y = Zπa(cid:48) + Xη + V,

(2.6)

where Y = [y1 : y2] ∈ Rn×2, V = [v1 : v2] ∈ Rn×2, a = (β, 1)(cid:48), and η = [γ : ξ] ∈ Rp×2.

3

Moreira and Moreira (2019) consider R ≡ (Z (cid:48)Z)−1/2 Z (cid:48)Y ∈ Rk×2. Because the trans-

formed IV matrix Z is orthogonal to X, we have

R = µa(cid:48) + (cid:101)V ,
where (cid:101)V = (Z (cid:48)Z)−1/2 Z (cid:48)V and µ = (Z (cid:48)Z)1/2 π. Commonly-used estimators and tests depend
on the data through R and estimators (cid:98)Σn for the variance Σn of vec( (cid:101)V ). For example,
consider the t-statistic based on the 2SLS estimator
2NZy2)−1 y(cid:48)
The estimator is clearly a ratio of quadratic forms of R. In our notation, the t-statistic (also
known as the Wald statistic) is

(cid:98)β = (y(cid:48)

2NZy1.

(2.7)

(2.8)

(cid:99)Wn =

(cid:98)β − β0
(cid:98)σβ,n

for (cid:98)σ2

β,n =

R(cid:48)

2((cid:98)b(cid:48) ⊗ Ik)(cid:98)Σn((cid:98)b ⊗ Ik)R2
2R2)2

(R(cid:48)

,

(2.9)

where (cid:98)b =

(cid:16)

1, −(cid:98)β

(cid:17)(cid:48)

and R2 is the second column of R.

The two-sided t-test rejects the null when |W | is larger than the 1 − α quantile of a
standard normal distribution. For this critical value to be reliable, the t-statistic needs
to be approximately normally distributed. This happens when the number of observations
n increases and the IVs are strong.
In applied work, however, it can be diﬃcult to ﬁnd
variables that are also uncorrelated with the error terms of the structural equation (2.1). In
practice, the search for valid IVs may lead to choices which are weakly correlated with the
endogenous explanatory variable y2. As a result, the null rejection probability for the t-test
can be sensitive to the quality of the instruments; see Nelson and Startz (1990), Dufour
(1997), and Staiger and Stock (1997). In particular, the null rejection probability can be
much larger than the usual nominal level. This problem spurs us to develop similar tests
which, by construction, have null rejection probability equal to nominal level α, no matter
how weak the IVs are.

2.2 Similar Tests

For simplicity, we start by assuming that vec( (cid:101)V ) is normally distributed with zero mean
and Σ is known. The online appendix relaxes this assumption, at the cost of asymptotic
approximations. For example, the t-statistic for known Σ (to streamline notation, we omit
the subscript n from Σn) would be

W =

(cid:98)β − β0
(cid:98)σβ

, where (cid:98)σ2

β =

R(cid:48)

2((cid:98)b(cid:48) ⊗ Ik)Σ((cid:98)b ⊗ Ik)R2
2R2)2

(R(cid:48)

.

(2.10)

For other test statistics, it is convenient to transform R into the pair of k × 1 statistics, S
being pivotal and independent of the statistic T . Moreira and Moreira (2019) and Moreira
and Ridder (2020) deﬁne

S = [(b(cid:48)
T = (cid:2)(a(cid:48)

0 ⊗ Ik) Σ (b0 ⊗ Ik)]−1/2 (b(cid:48)
0 ⊗ Ik) Σ−1 (a0 ⊗ Ik)(cid:3)−1/2 (a(cid:48)

0 ⊗ Ik) vec (R) and

0 ⊗ Ik) Σ−1vec (R) ,

(2.11)

4

for a0 = (β0, 1)(cid:48) and b0 = (1, −β0)(cid:48). Their marginal distributions are given by
(cid:1) and T ∼ N (Dβµ, Ik) , where

S ∼ N (cid:0)(β − β0) Cβ0µ, Ik

(2.12)

Cβ0 = [(b(cid:48)
Dβ = (cid:2)(a(cid:48)

0 ⊗ Ik) Σ (b0 ⊗ Ik)]−1/2 and
0 ⊗ Ik) Σ−1 (a0 ⊗ Ik)(cid:3)−1/2 (a(cid:48)

0 ⊗ Ik) Σ−1 (a ⊗ Ik) .

Examples of test statistics based on S and T are the Anderson-Rubin (AR), the score
or Lagrange multiplier (LM), and the quasi likelihood ratio (QLR) statistics. Anderson and
Rubin (1949) propose a pivotal test statistic. In our model, the Anderson-Rubin statistic is
given by

AR = S(cid:48)S.

(2.13)

Moreira and Moreira (2019) derive the LM statistic under the same distributional assump-
tion that we make here. The two-sided LM statistic is

LM = S(cid:48)NCβ0 D−1

β0

T S.

(2.14)

The AR and LM statistics have chi-square distributions with k and one degrees of freedom,
respectively. The AR and LM tests reject the null when their respective statistics are larger
than their 1 − α chi-square quantiles. By construction, both tests have correct size at level
α.

Kleibergen (2005), among others, adapts the likelihood ratio statistic for homoskedastic

errors to HAC errors. The quasi-likelihood ratio statistic is

QLR =

AR − r (T ) +

(cid:113)

(AR − r (T ))2 + 4LM · r (T )

2

,

(2.15)

where r (T ) = T (cid:48)T . Andrews (2016) proposes tests based on the following combination:

LC = m (T ) · AR + (1 − m (T )) · LM,

(2.16)

where 0 ≤ m (T ) ≤ 1. Unlike the AR and LM statistics, neither the QLR nor the LC
statistics are pivotal. We follow Moreira and Moreira (2019) and reject the null hypothesis
when the test statistic ψ is larger than κ (t, Σ), which is the null 1 − α quantile conditional
on T = t. Writing a test statistic as ψ (S, T, Σ) , we can compute the conditional rejection
probability under the null:

Pβ0,µ,Σ (ψ (S, T, Σ) ≥ x|T = t) = Pβ0,µ,Σ (ψ (S, t, Σ) ≥ x) .

(2.17)

This probability does not depend on µ because the distribution of S under the null is pivotal.
By construction, the conditional null quantile satisﬁes

Consequently, the unconditional null rejection probability is α,

Pβ0,µ,Σ (ψ (S, t, Σ) ≥ κ (t, Σ)) ≡ α.

Pβ0,µ,Σ (ψ (S, T, Σ) ≥ κ (T, Σ)) ≡ α.

5

(2.18)

(2.19)

For example, the conditional test based on the QLR statistic rejects the null when this
statistic is larger than its null conditional quantile. If the statistic is pivotal, like the AR and
LM statistics, the conditional quantile κ (t, Σ) collapses to the null unconditional quantile.
The QLR and LC statistics depend on S only through the AR and LM statistics. Moreira
and Ridder (2020) show that the statistic S has useful information beyond the Anderson-
Rubin and score statistics when the covariance matrix does not have a Kronecker product
structure. For that reason, we recommend the use of conditional tests based on either a
likelihood ratio statistic or a WAP statistic to be introduced here. These tests take advantage
of information beyond the Anderson-Rubin and score statistics.

The likelihood ratio statistic based on R is

LR = max

a

vec(R)(cid:48)Σ−1/2NΣ−1/2(a⊗Ik)Σ−1/2vec(R) − T (cid:48)T,

(2.20)

where LR can be written in terms of the pivotal statistic S and the complete statistic T ; see
Moreira and Moreira (2019). In the appendix, we show that this statistic can be written as

LR = b(cid:48)

0R(cid:48) [(b(cid:48)

0 ⊗ Ik) Σ (b0 ⊗ Ik)]−1 Rb0 − min

b

b(cid:48)R(cid:48) [(b(cid:48) ⊗ Ik) Σ (b ⊗ Ik)]−1 Rb,

(2.21)

where b = (1, −β)(cid:48). Hence, LR is associated to the GMM objective function based on the
moment E (Z (cid:48) (y1 − y2β)) = 0 and the continuously-updating weighting matrix; see Andrews
and Mikusheva (2016) for the general case. The LR statistic does not have a closed-form
solution and requires numerical searching methods. We instead use invariance to ﬁnd an
integrated likelihood test.

3

Invariance and the CIL Test

Contrary to popular belief, the IV model with HAC errors presents symmetries.1 The theory
developed for the IV model thus far assumes the variance matrix is ﬁxed. This assumption
prevents us from ﬁnding symmetries with more general error DGPs. In this paper, we instead
assume that the variance Σ is known, but not ﬁxed.

To explain the symmetries present in the IV model, ﬁrst consider a simple example, in
iid∼ N (τ , σ2), where σ2 is unknown. We want to test the null hypothesis H0 : τ =
which Yi
0 against H1 : τ (cid:54)= 0, treating σ2 as a nuisance parameter. For any scalar g (cid:54)= 0, the
iid∼ N (g · τ , g2σ2). This simple model is
transformed data Xi = g · Yi has distribution Xi
then symmetric (or said to be preserved) for the multiplicative group G. The transformation
preserves the null (and therefore, the alternative) because the mean of Xi is zero if and
only if the mean of Yi is zero. The suﬃcient statistic for (τ , σ2) is the sample mean Y n
and the variance estimator S2
. The transformation above induces a
change in the space of suﬃcient statistics: the pair Y n and S2
Y become X n = g · Y n and
X = g2S2
S2
Y , respectively. If these transformations preserve the hypothesis-testing problem

Y = n−1 (cid:80) (cid:0)Yi − Y n

(cid:1)2

1An econometric model is a (parametric, semi-parametric, or non-parametric) family P of probability
measures P for the data Y . Consider the transformations on the data g ◦ Y given by a group g ∈ G. This
action yields a transformation g ◦ P given by g ◦ P (Y ∈ B) ≡ P (g ◦ Y ∈ B) for any Borel set B. The model
is said to be symmetric when g ◦ P ∈ P for every g ∈ G and P ∈ P.

6

and the original data are supportive of a hypothesis, the transformed data should be equally
supportive of the same hypothesis. This is called the invariance principle. Therefore, the
test statistic should be the same whether computed from the original or from the transformed
data; in other words, the test has to be invariant. Any invariant test can be written as a
function of the largest invariant statistic. In this example, the maximal invariant is then
Y . Its distribution depends only on τ 2/σ2 and has a monotone likelihood
X
ratio property. As a result, the uniformly most powerful invariant (UMPI) test rejects the
Y is suﬃciently large. We refer interested readers to Eaton (1989) and
null when Y
Lehmann and Romano (2005) for the theory of optimal tests.

X = Y

2
n/S2

2
n/S2

2
n/S2

Now, consider instead the case in which σ2 is known. The multiplicative group does not
preserve the model if we assume σ2 to be ﬁxed. We would have to consider a much smaller
group in which g = ±1 only (this restriction is in perfect analogy to the sign group deﬁned
by AMS06, as we shall see in Section 5). However, this transformation only reduces the
2
n and S2
Y . How, then, can we use the model
suﬃcient statistic to the maximal invariant Y
symmetries to obtain a further reduction? One possibility is to distinguish the assumption
of a known variance from the assumption of a ﬁxed variance. The distinction hinges on
whether we actually know σ2 and treat it as ﬁxed, even after we transform the data.
If
an outsider tells us the value of σ2, this person would give a diﬀerent answer if we asked
what the variance is after multiplying the data by a nonzero scalar. The person reports a
known, but not ﬁxed, variance. We can still get an optimal test if we restrict ourselves to
unbiased tests. Because our simple model belongs to a one-parameter exponential family,
we automatically ﬁnd that the uniformly most powerful unbiased (UMPU) test rejects the
null hypothesis for large values of Y n

2/σ2.

Instead, we can take the variance σ2 as both part of the data and the parameter space.
The suﬃcient statistic is now the pair Y n and σ2, while the parameters are τ and also σ2.
The same multiplicative group transforms the suﬃcient statistic to X n = g·Y n and g2σ2, and
induces a change in the mean from τ to g · τ and the variance from σ2 to g2σ2. The maximal
2
n/σ2. This statistic has a noncentral chi-square distribution,
invariant is then X
where the noncentrality parameter τ 2/σ2 is zero if and only if the null hypothesis is true.
Because this distribution also has a monotonic likelihood ratio property, we again obtain a
UMPI test that rejects the null hypothesis if Y

2
n/σ2 = Y

2
n/σ2 is large.

In this simple canonical model, the UMPU and UMPI tests are the same. This is not
a coincidence:
if a UMPU test is unique (up to sets of measure zero) and there exists a
UMPI test with respect to some group of transformations, then both coincide (up to sets
of measure zero). For the IV model, however, there are no uniformly most powerful tests.
In perfect analogy to our canonical model, there are two lines of research in the IV model.
Moreira and Moreira (2019) seek optimal two-sided tests within a restricted class of tests
(the so-called SU tests) by ﬁxing a long-run reduced-form variance matrix, i.e., they consider
the known and ﬁxed case. In this paper, we instead explore model symmetries by taking
the reduced-form variance to be known, but not ﬁxed. As in the canonical model above, we
prefer not to take a stance on which thought experiment is more suitable. We consider both
approaches to be useful, leading to new insights in the IV model.

If the error variance matrix in the instrumental variable regression is considered known
–but not ﬁxed– then the model satisﬁes some natural symmetries. The main contribution

7

of this paper is that we propose a test that is invariant for the largest data transformation
that leaves the model and null hypothesis unchanged. The novel test, called the conditional
integrated likelihood (CIL) test, is invariant and the limit of WAP tests. The weights are
derived from relatively invariant measures on the parameter space. The weights of the
transformed parameters are then proportional to the weights of the original parameters.
The test statistic is the ratio of the integrated likelihoods of the parameter space under the
null and alternative hypotheses. As a result, the invariance of the model combined with the
proportional eﬀect of the transformation on the weights make the CIL test invariant to the
transformation, as required.

3.1 Model-Preserving Transformations in the HAC-IV Model

To understand model symmetries, it is convenient to transform the random matrix R into

R0 = RB0, where B0 =

(cid:18) 1

0
−β0 1

(cid:19)

,

(3.1)

so that the mean of the ﬁrst column of R0 = [R1 : R2] is zero under the null. The distribution
of R0 is

R0 ∼ N (µa(cid:48)

∆, Σ0) ,

where a(cid:48)

∆ = (∆, 1), ∆ = β − β0, and

Σ0 = (B(cid:48)

0 ⊗ Ik)Σ(B0 ⊗ Ik) =

(cid:18) Σ11 Σ12
Σ21 Σ22

(cid:19)

.

We partition the inverse variance as

(cid:21)

(cid:20) Σ11 Σ12
Σ21 Σ22

Σ−1

, where

0 =
Σ11 = (cid:0)Σ11 − Σ12Σ−1
Σ21 = (cid:0)Σ12(cid:1)(cid:48) = −Σ22Σ21Σ−1

22 Σ21

(cid:1)−1 , Σ22 = (cid:0)Σ22 − Σ21Σ−1
11 = −Σ−1

22 Σ21Σ11.

11 Σ12

(3.2)

(3.3)

(3.4)

(cid:1)−1 , and

For k = 1, the variance matrix Σ trivially has a Kronecker structure, as deﬁned in Section
5. Hence, AMS06 is directly applicable. In particular, the Anderson-Rubin test is the UMPI
test in the just-identiﬁed model (k = 1); see Comment 2 following Corollary 1 of AMS06.2
For k > 1, we recommend a novel WAP test. The weights are based on invariance
arguments. To show the model symmetries, we consider the aﬃne group of transformations
(A, G) ∈ R2k × R2k×2k of R0:

A + G · vec (R0) ∼ N (A + G (a∆ ⊗ µ) , GΣ0G(cid:48)) .

(3.5)

2AMS06’s optimality result for invariant tests when k = 1 can be seen from the perspective of unbi-
ased tests. Moreira (2002, 2009) shows that the Anderson-Rubin test is uniformly most powerful unbiased
(UMPU). If there is a UMPI test, then the Anderson-Rubin test must be the one; see Theorem 6.6.1 of
Lehmann and Romano (2005).

8

If we consider Σ0 to be ﬁxed, we have to impose restrictions on G and/or Σ0 for the trans-
formation to preserve the model, so that

GΣ0G(cid:48) = Σ0.

(3.6)

If the variance matrix Σ0 is known but not ﬁxed, it changes with the transformation. If Σ0
is a known variance matrix, so is GΣ0G(cid:48).

Partitioning A into k-dimensional vectors and G into k × k matrices:

A =

(cid:21)

(cid:20) A1
A2

and G =

(cid:20) G11 G21
G21 G22

(cid:21)

,

(3.7)

we ﬁnd that the expectation of the transformed R0 becomes

E[A + G · vec (R0)] = A +

(cid:20) G11 G12
G21 G22

(cid:21)

(cid:20) ∆µ
µ

·

(cid:21)

=

(cid:20) A1 + (G11∆ + G12) µ
A2 + (G21∆ + G22) µ

(cid:21)

.

(3.8)

To preserve the null hypothesis H0 : ∆ = 0, the ﬁrst sub-vector of the mean has to be zero
for all values of µ. This forces A1 = 0 and G12 = 0. In the original model, the mean of
R1 is proportional to the mean of R2. To preserve the model, the two subvectors of the
transformed mean must be proportional to each other, which forces A2 = 0 and

G11∆ · µ ∝ (G21∆ + G22) µ

(3.9)

for all µ. This implies that G11 = g11 · g1, G21 = g21 · g1, G22 = g22 · g1 with g11, g21, g22 being
constants of proportionality, and g1 a k × k matrix. As a result,

G =

(cid:20) g11 · g1

(cid:21)

0

g21 · g1 g22 · g1

= g2 ⊗ g1,

(3.10)

where g2 is a 2 × 2 lower triangular matrix. Therefore, g = (g1, g2) transforms the data to

g ◦ (R0, Σ0) = (g1R0g(cid:48)

2, (g2 ⊗ g1) Σ0 (g(cid:48)

2 ⊗ g(cid:48)

1)) .

(3.11)

We use the transpose of g2 so that the associated transformation is a left action. This
transformation leaves the model unchanged: it preserves the null and the proportionality of
the subvectors of the mean of R0. Speciﬁcally,

g ◦ (∆, µ, Σ0) =

(cid:18) ∆g11

∆g21 + g22

, g1µ (∆g21 + g22) , (g2 ⊗ g1) Σ0 (g(cid:48)

2 ⊗ g(cid:48)
1)

(cid:19)

.

(3.12)

If the matrix Σ0 is invertible, as we assume here, then g1 and g2 must be non-singular.3 This
means that g1 ∈ GL (k) and g2 ∈ GT (2), respectively, the groups of invertible k × k matrices
and of invertible lower-triangular 2 × 2 matrices (with matrix multiplication as the group
operator).

If the original data are supportive of the null hypothesis, then the transformed data
should be equally supportive of this hypothesis. The test should be the same whether it is

3Therefore, g11 and g22 are non-zero elements.

9

computed from the original or from the transformed data, i.e. the test should be invariant
to the transformation g. As we show later, the AR, LM, CQLR, and CLR tests are invariant
to the group of transformations g presented above. Without further restrictions on the
weight m (T ), the CLC test may be sensitive to this transformation. Likewise, the weighted-
average-power SU test proposed by Moreira and Moreira (2019) may also change with data
transformations. As a result, the CLC and SU tests may have power that changes with the
data transformation. Next, we propose a conditional integrated weighted likelihood test that
is invariant to g. This test does not have the undesirably low power of WAP tests based on
generic weights documented by Moreira and Moreira (2019).

3.2 The CIL Test

Consider an integrated likelihood (IL) statistic which is the ratio of two terms. The numer-
ator is the integrated likelihood over µ with respect to the Lebesgue measure and over ∆
with respect to |∆|k−2 d∆. The denominator is the density of the pivotal statistic S under
the null hypothesis. In Appendix A, we show the IL statistic is

IL =

(cid:90) ∞

(cid:20)

1
2

e

vec(R0)(cid:48)Σ−1/2

0

N

Σ

−1/2
0

(a∆⊗Ik )

Σ−1/2
0

vec(R0)−T (cid:48)T

(cid:21)

(3.13)

−∞
(cid:12)(a(cid:48)

× (cid:12)

∆ ⊗ Ik) Σ−1

0 (a∆ ⊗ Ik)(cid:12)
(cid:12)

−1/2 |∆|k−2 d∆,

up to a multiplication by |Σ0|1/2. We also prove this integral is ﬁnite in the over-identiﬁed
case k ≥ 2. The conditional (on T ) integrated likelihood (CIL) test based on (3.13) is
invariant to the transformation g because both the IL statistic and its conditional quantile
have the same proportionality multiplier χ (g) with respect to g. Furthermore, the CIL test
is the limit of a sequence of WAP tests. We relegate the theory and proofs to Section 6.
Here, we focus on the implementation of the CIL test.

The integral deﬁned in (3.13) is improper, which can create computational diﬃculties.
We circumvent this problem by changing variables, so that the integral is proper. This is
convenient for the numerical integration that we use to compute the IL statistic. First, we
standardize the vector a∆ to have norm one:
(cid:32)

(cid:33)

a∆ =

a∆
(1 + ∆2)1/2 =

∆
(1 + ∆2)1/2 ,
(a∆⊗Ik) and also that

1
(1 + ∆2)1/2

We note that NΣ−1/2

0

(a∆⊗Ik) = NΣ−1/2

0

.

(3.14)

(cid:12)
(cid:12)(a(cid:48)

∆ ⊗ Ik) Σ−1

0 (a∆ ⊗ Ik)(cid:12)
(cid:12)

−1/2 = (cid:0)1 + ∆2(cid:1)−k/2 (cid:12)

(cid:12)(a(cid:48)

∆ ⊗ Ik) Σ−1

0 (a∆ ⊗ Ik)(cid:12)
(cid:12)

−1/2 .

(3.15)

Therefore,

IL =

(cid:20)

1
2

e

(cid:90) ∞

−∞

vec(R0)(cid:48)Σ−1/2

0

N

Σ

−1/2
0

(a∆⊗Ik )

Σ−1/2
0

vec(R0)−T (cid:48)T

(cid:21)

(3.16)

× (cid:12)

(cid:12)(a(cid:48)

∆ ⊗ Ik) Σ−1

0 (a∆ ⊗ Ik)(cid:12)
(cid:12)

−1/2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∆
(1 + ∆2)1/2

k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
∆2 d∆.

10

By changing variables following

lη ≡ (sin η, cos η)(cid:48) = a∆,

the IL statistic becomes

IL =

(cid:90) pi/2

−pi/2

e

(cid:20)
vec(R0)(cid:48)Σ−1/2

0

1
2

N

Σ

−1/2
0

(lη ⊗Ik )

Σ−1/2
0

vec(R0)−T (cid:48)T

(cid:21)

× (cid:12)
(cid:12)

(cid:0)l(cid:48)

η ⊗ Ik

(cid:1) Σ−1

0 (lη ⊗ Ik)(cid:12)
(cid:12)

−1/2 |sin η|k (1 + tan2 η)

tan2 η

dη,

(3.17)

(3.18)

where pi = 3.14159.... Because

|sin η|k (1 + tan2 η)

tan2 η

= |sin η|k−2

sin η2
cos2 η · tan2 η

= |sin η|k−2 ,

(3.19)

the IL statistic simpliﬁes to

IL =

(cid:90) pi/2

−pi/2

e

(cid:20)
vec(R0)(cid:48)Σ−1/2

0

1
2

N

Σ

−1/2
0

(lη ⊗Ik )

Σ−1/2
0

vec(R0)−T (cid:48)T

(cid:21)

(3.20)

× (cid:12)
(cid:12)

(cid:0)l(cid:48)

η ⊗ Ik

(cid:1) Σ−1

0 (lη ⊗ Ik)(cid:12)
(cid:12)

−1/2 |sin η|k−2 dη,

which is easier to compute.

The IL statistic can be compared to the statistic that integrates the likelihood with

respect to the Lebesgue measure dµ × d∆ without the weights |∆|k−2:

IL0 =

(cid:90) pi/2

−pi/2

e

(cid:20)
vec(R0)(cid:48)Σ−1/2

0

1
2

N

Σ

−1/2
0

(lη ⊗Ik )

Σ−1/2
0

vec(R0)−T (cid:48)T

(cid:21)

(3.21)

× (cid:12)
(cid:12)

(cid:0)l(cid:48)

η ⊗ Ik

(cid:1) Σ−1

0 (lη ⊗ Ik)(cid:12)
(cid:12)

−1/2 |cos η|k−2 dη.

(The derivation of IL0 is analogous to the IL statistic, as shown in Appendix A.) Numerically,
the computation of IL or IL0 is equally diﬃcult. Without the weights |∆|k−2, the statistic
IL0 does not yield an invariant test when k > 2. Hence, the test suﬀers the power problems
documented by Moreira and Moreira (2019).

The representation of IL in terms of R0 and Σ0 is convenient to prove that the CIL
test is invariant to the transformation (3.11). However, the approach can be unnecessarily
challenging when testing for diﬀerent levels of β0. This pitfall can be important to derive
conﬁdence regions, which consist of all values of β0 which are not rejected by the CIL test.
For numerical stability, we instead recommend representing IL in terms of the original data,
R and Σ.

Algebraic manipulations show that

IL =

(cid:90) ∞

(cid:104)

1
2

e

−∞

vec(R)(cid:48)Σ−1/2N

Σ−1/2(a⊗Ik )

Σ−1/2vec(R)−T (cid:48)T

(cid:105)

(3.22)

× (cid:12)

(cid:12)(a(cid:48) ⊗ Ik) Σ−1 (a ⊗ Ik)(cid:12)
(cid:12)

−1/2 |β − β0|k−2 dβ.

11

By changing variables

lθ ≡ (sin θ, cos θ)(cid:48) =

a
(cid:107)a(cid:107)

≡ a,

(3.23)

and following steps analogous to the derivation of (3.20), we show in Appendix A that

IL = (cid:0)1 + β2

0

(cid:1)(k−2)/2 (cid:90) pi/2

(cid:104)

1
2

e

−pi/2

vec(R)(cid:48)Σ−1/2N

Σ−1/2(lθ ⊗Ik )

Σ−1/2vec(R)−T (cid:48)T

(cid:105)

(3.24)

× (cid:12)

(cid:12)(l(cid:48)

θ ⊗ Ik) Σ−1 (lθ ⊗ Ik)(cid:12)
(cid:12)

−1/2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
1 + β2
0

(cid:112)

sin θ −

β0
1 + β2
0

(cid:112)

cos θ

k−2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dθ.

(cid:1)(k−2)/2 can be ignored in the computation of the CIL test, as it is directly
The factor (cid:0)1 + β2
absorbed by the critical value function. Hence, we suggest implementing the conditional test
based on the (cid:0)1 + β2

(cid:1)−(k−2)/2 IL statistic.

0

0

There are also connections between the IL statistic and the LR statistic. The LR statistic

maximizes, with respect to ∆,

(cid:104)
vec (R0)(cid:48) Σ−1/2

0 NΣ−1/2

0

(a∆⊗Ik)Σ−1/2

0

vec (R0) − T (cid:48)T

(cid:105)

,

(3.25)

which is the term inside the brackets of (3.13). The IL statistic integrates the exponential of
this term after two corrections. The ﬁrst correction, (cid:12)
−1/2, arises from
(cid:12)
integration with respect to the Lebesgue measure dµ. The second correction |sin η|k−2 ensures
that the test is two-sided and invariant, so that we avoid the one-sided power behavior in
parts of the parameter space. In the next section, we show some advantages of the CIL test
over the AR and CLR tests.

0 (lη ⊗ Ik)(cid:12)
(cid:12)

(cid:1) Σ−1

η ⊗ Ik

(cid:0)l(cid:48)

4 Numerical Simulations

Here, we provide numerical simulations for the AR, CLR, CIL, and CIL0 tests. All results
reported here are for k = 5 and only one level of instrument strength based on 1,000 Monte
Carlo replications for power and 1,000 simulations to approximate the tests’ critical value
function. In the supplement, we provide two levels of identiﬁcation strength and consider
k = 2, 5, 10. For reasons explained below, a reliable implementation for the CLR test is
computationally intensive. Because of this, the supplemental power plots are limited to only
200 replications and 200 simulations for the conditional quantile.

We ﬁrst illustrate numerical problems with likelihood optimization and integration. Some
of these diﬃculties arise even in the simple case in which errors are homoskedastic. We focus
on tests with signiﬁcance level 5% for testing β0 = 0. We set the parameter µ =
1k
for k = 5 and set λ/k = 2, where 1k is a k-dimensional vector of ones and λ is a measure
of the IVs’ strength. The variance of structural-form errors is one and their correlation is
ρ = −0.9, 0.9. We present plots for the power envelope and power functions against various
alternative values of β. We plot power as a function of the rescaled alternative βλ1/2, which
reﬂects the diﬃculty of making inference on β for diﬀerent instruments’ strength.

λ1/2/

√

(cid:17)

(cid:16)

k

12

Figure 1: Power Curves for Homoskedastic Errors and k = 5

(Likelihood Optimization)

(a) ρ = 0.9

(b) ρ = −0.9

Figure 1 presents the one-sided and two-sided power envelope for invariant similar tests.
These power envelopes are derived analytically by Mills, Moreira, and Vilela (2014) and
Andrews, Moreira, and Stock (2006), respectively; see earlier theory by Andrews, Moreira,
and Stock (2004). This early work also shows these power bounds are valid for all invariant
tests which have correct size. We also plot power curves for the CQLR test as well as
two numerical optimization strategies to obtain the CLR test. The ﬁrst randomly draws
the initial point for the search optimization algorithm in (2.20) for β. Here, we consider
the uniform distribution over [−1000, 1000]. The second one relies on the fact that we can
maximize the likelihood over a compact set, without loss of generality. We can write the
likelihood ratio statistic as

LR = max

θ

vec(R)(cid:48)Σ−1/2NΣ−1/2(lθ⊗Ik)Σ−1/2vec(R) − T (cid:48)T,

(4.1)

where the maximization is over the compact set [−pi/2, pi/2]. The initial point is drawn
from a uniform distribution over that same set. Recall that the CQLR and CLR tests are
theoretically identical when errors are homoskedastic. Any power diﬀerence between the
CQLR test and these numerical implementations for the CLR test arises from failures in the
likelihood optimization.

The power upper bounds are useful to understand the diﬃculty in the likelihood opti-
mization behind the CLR test. Both CQLR and CLR tests based on optimization over the
compact set for θ perform alike. These two tests have power very close to the two-sided
power envelope. The CLR test based on a draw-and-search for the optimal β fails remark-
ably. In the ﬁrst graph, this implementation has power above the two-sided power envelope
and close to the one-sided power bound for parts of the parameter space. Furthermore, this
implementation must fail to deliver a test with correct size. Indeed, the implementation for
the CLR test over the whole real line has size close to 10% instead of the correct 5% level.
To make matters even worse, the second plot in Figure 1 shows bad behavior associated with
the sample implementation of the CLR test. The power can even be close to zero for parts
of the parameter space.

13

Of course, one could simply use the CQLR test for the homoskedastic case. The lesson
learned here is that likelihood optimization does matter for the power performance of the
CLR test in general. In more complex designs (i.e., non-Kronecker error variance), drawing
a unique initial point is far from suﬃcient. Our experience is that likelihood maximization
for the implementation of the CLR test can be very slow and unreliable. This is particularly
true when several initial points are required, as happens in some designs below.

Figure 2: Power Curves for Homoskedastic Errors and k = 5

(Integrated Likelihood)

(a) ρ = 0.9

(b) ρ = −0.9

Figure 2 presents power for the AR, CQLR, CIL, and CIL0 tests. The CQLR and CIL
tests have comparable power and outperform the AR test. These plots are a reassurance
that the CIL test performs well in scenarios more favorable to CQLR. The CIL0 test has
behavior quite diﬀerent from the CIL test. The dissimilar behavior of the CIL and CIL0 tests
illustrates that tests based on likelihood integration are sensitive to weight choices. While
the CIL and CIL0 tests perform comparably when λ increases, they have distinct properties
when IVs are weak. For one side of the alternative, the power of CIL0 is smaller than that of
the CQLR and CIL tests. For the other side of the alternative, it actually has larger power.
Therefore, the CIL0 behaves as a one-sided test. The CIL0 test being biased means the null
rejection probability is smaller for some alternatives than under the null. This undesirable
feature of the CIL0 test is not shared by the CQLR and CIL test. These two tests do
not suﬀer the same power deﬁciencies as the CIL0 test. They behave as two-sided tests by
construction, and have power close to the power upper bound.

We now move to the more complex case in which errors can be heteroskedastic, auto-
correlated, and/or clustered (HAC). We replicate four designs: the near-singular (NS), a
variation thereof (NS with perturbation), and growing alternative (GA) designs of Moreira
and Ridder (2020), and the non-Kronecker (NK) design of Moreira and Moreira (2019).
While these simulations are not exhaustive for all parameter combinations, none of these
designs is chosen to favor the CIL test over the CLR test. The main goal of these designs
is only to show that there exist invariant tests which depend on the statistic S beyond AR
and LM .

To conserve space, we focus here only on simulations based on the NS design for k = 5.

14

Table 1: Likelihood Optimization and Initial Values (percentage)

(1,No)
(0,Yes)
(51,No)
(50,Yes)

(1,No)
-
26.0
0.1
0.1

(0,Yes)
64.7
-
3.5
-

(51,No)
86.7
47.1
-
0.4

(50, Yes)
87.7
47.2
4.2
-

We set µ = λ1/2e1, with λ/k = 2. For the variance matrix, we deﬁne Jk to be the k × k
matrix with the anti-diagonal elements equal to one and the other components zero. We
have J 2

k = Ik. The k × k submatrices of Σ0 are

Σ11 = c11Ik, Σ12 = c12Jk, and Σ22 = c22Ik,

(4.2)

where c11, c12, and c22 are tuning parameters. The values for the NS design are c11 = 1,
c12 = 100, and c22 = c2
In this design, the power of both LM and CQLR tests
is essentially equal to size. The full set of results for k = 2, 5, 10 and λ/k = 2, 10 for all
four designs, as well as descriptions of the NS with perturbation, GA, and NK designs, are
presented in the supplement.

12 + c−3
12 .

Table 2: Likelihood Optimization and Initial Values (factor)

(1,No)
(0,Yes)
(51,No)
(50,Yes)

(1,No)
-
964.9
1,114.7
0.2

(0,Yes)
3,270.1
-
452.6
-

(51,No)
5,446.5
9,910.7
-
7.7

(50, Yes)
5,410.0
9,890.0
379.1
-

When the variance matrix has a Kronecker product form, the LR statistic has a closed-
form solution, and the CLR test reduces to the CQLR test. This sidesteps the daunting
task of numerically optimizing the likelihood. In the special case with homoskedastic errors,
choosing only one initial point after compactifying the search set is enough for our purposes.
Unfortunately, this conclusion is not valid for more complex variance matrices. Table 1
assesses improvements for the likelihood optimization under the null hypothesis. The values
inside the parentheses indicate the number of random initial points for θ and whether β0 is
included or not, respectively. We compute the LR statistic over 1,000 simulations for each
case. We then report the proportion of times in which one setup outperforms another setup
(relative improvement by an error margin of at least 0.1%).

Each row in Table 1 corresponds to a choice of the number of starting values and whether
β0 is among the starting values, as speciﬁed in the row header. The entries in a row report the
fraction of repetitions in which the initial values selection and the inclusion of β0, as speciﬁed
in the column header, give a higher maximum likelihood value. For example, if we choose β0
instead of only one random point as the initial value, we see improvements in the likelihood
optimization 64.7% of the time. Conversely, the likelihood optimization performs better
26.0% of the time if we choose a random point instead of β0. For both of these scenarios,

15

improvements are gained by adding about 50 random initial values. This can be seen in the
upper-right 2 × 2 block in Table 1, where the improvements range from 47.1% to 87.7%. On
the other hand, the improvements are negligible from starting with 50 random points and β0
as initial values –even when we include 51 other random points. What is perhaps interesting
is the improvement of 3.5% from adding β0 as an initial value in addition to the 50 random
points. These two ﬁndings suggest that running optimization algorithms after including 50
random points and β0 as initial values should suﬃce for our purposes. More worrisome, for
smaller values of λ or other combinations of µ and the variance matrix, we may need to
include even more initial points. This may happen, for example, if the likelihood can be ﬂat
for parts of the parameter space.

Table 2 presents the average percentage improvement (for the observations in which the
error margin is at least 0.1%). Even when we include 51 random initial points, meaningful
improvements can be gained by including the unknown parameter β. These gains are on
the order of 379.1% for 4.2% of the replications when we include another 50 random initial
points and β itself. On the other hand, when we include 51 other random initial points
beyond β and 50 points, the average improvement is on the order of 7.7% for only 0.4% of
the repetitions.

All simulations are for the null hypothesis. For the alternative, it is natural to use 50
random points, the null β0, and the alternative β as initial points. Of course, the parameter
β is unknown. However, we want to minimize the numerical issues associated with the CLR
test, in case better optimization methods are found in the future. The table shows that the
solution of using β in addition to 50 random points works well to compute the LR statistic.
A more complex problem happens when we ﬁnd the approximation for the critical value
function. Recall that this function is the conditional quantile under the null hypothesis. This
quantile is found by generating S from a standard multivariate normal distribution. That
means the model is misspeciﬁed when T is not generated under the null. One possibility is to
use the pseudo-parameter which minimizes the Kullback-Leibler divergence criterion. This
strategy follows from the fact that the maximum likelihood (ML) estimator converges to this
pseudo-parameter under strong instruments. This route seems complicated and unnecessary
for our purposes. Excluding this parameter, we get smaller values for the test statistic –not
larger. Hence, the 95% quantile used for the critical value function tends to underestimate
the true conditional quantile. The bottom line is that by including β, the LR statistic is
optimized properly while the conditional quantile can be smaller than it should be. This
means that, if anything, we may be overestimating the power of the CLR test.

Finally, we evaluate improvements over other numbers of random initial points for θ. For
example, unreported simulations show gains of about 5.3% obtained from adding 1 initial
random point beyond 20 random initial points. The choice of 50 seems the most sensible, in
terms of reliability and computational speed. Even then, the computation time for CLR is
about 35 times slower than that of the CIL test, on average (with the range between 4 to
100 times slower). For the aforementioned reasons, we include β0 and β as initial points as
well. At least for the designs considered here, unreported power comparisons for diﬀerent
choices of initial values indicate that including 50 random points, β0, and β oﬀers stable and
reliable power curves. There is, of course, no guarantee that this searching scheme would be
suﬃcient for other designs.

We now brieﬂy discuss power. More extensive power comparisons are reported in the

16

Figure 3: Power Curves for HAC Errors with k = 5 and λ/k = 2

(a) NS Design

(b) NS with perturbation

(c) GA Design

(d) NK Design

supplement (due to computational time for the CLR, these additional comparisons use only
200 Monte Carlo replications for power and 200 simulations for conditional quantiles). Figure
3 presents power for the AR, CLR, CIL, and CIL0 tests when k = 5 and λ/k = 2. We consider
all four sets of simulations: NS design, NS design with a perturbation, GA design, and NK
design. As before, the CIL0 test can be biased, while the CLR and CIL tests dominate the
AR test. In general, the CIL test outperforms the CLR test. The power diﬀerence can be
as large as 15% for these speciﬁc designs. For example, the CIL test can have power near
85% when the CLR test rejects the null about 70% of the time. This diﬀerence happens
even when we implement the infeasible version of the CLR test which includes β as one of
the initial points.

The more technical sections of the paper are next. Section 5 builds upon and connects

with the work of AMS06. Section 6 derives the CIL test.

17

5 Kronecker Variance Matrix

We ﬁrst consider the special case where Σ = Ω ⊗ Φ with Ω a 2 × 2 matrix and Φ a k × k
matrix. The Kronecker product framework is particularly interesting for two reasons. First,
we ﬁnd the maximal invariant, taking into consideration a transformation of Ω which is
known but not ﬁxed. This yields the same data reduction from S and T as that obtained
by AMS06 under the assumption that Ω is known and ﬁxed. This result is striking as the
AMS06 approach does not hold for general Σ, but ours does. Second, AMS06 do not rule out
the possibility that the test depends on Ω beyond the statistics S and T , because AMS06
treat Ω as being ﬁxed. Our framework instead shows that invariant tests should not depend
on Ω at all.

The S and T statistics in (2.11) simplify to the original statistics of Moreira (2002, 2009)
and AMS06 for the homoskedastic model. When Σ = Ω ⊗ Φ, the statistics S and T become

S = Φ−1/2(Z (cid:48)Z)−1/2Z (cid:48)Y b0 · (b(cid:48)
T = Φ−1/2(Z (cid:48)Z)−1/2Z (cid:48)Y Ω−1a0 · (a(cid:48)

0Ωb0)−1/2 and

0Ω−1a0)−1/2.

Their distribution is given by

S ∼ N (cid:0)cβΦ−1/2µ, Ik

(cid:1) and T ∼ N (cid:0)dβΦ−1/2µ, Ik

(cid:1)

(5.1)

(5.2)

0Ωb0)−1/2 and dβ = a(cid:48)Ω−1a0 · (a(cid:48)

with cβ = (β − β0) · (b(cid:48)
0Ω−1a0)1/2. AMS06 develop the theory
of invariant tests by treating Ω as known and ﬁxed. Even if Φ is known, the parameter
µΦ = Φ−1/2µ is unknown, because µ is unknown. Hence, AMS06’s invariance argument
applies to the new parameter µΦ = Φ−1/2µ. Speciﬁcally, let h1 ∈ O (k), the group of
orthogonal matrices with matrix multiplication as the group operator. The corresponding
transformation in the sample space is

The associated transformation in the parameter space is

h1 ◦ [S : T ] = h1 · [S : T ] .

h1 ◦ (β, µΦ) = (β, h1.µΦ) .

(5.3)

(5.4)

The transformation does not change β, so our testing problem is preserved. As argued before,
this means that the test statistic should be an invariant statistic (under the transformation
h1).

The maximal invariant statistic for the orthogonal transformation is
(cid:20) QS QST
QST QT

(cid:20) S(cid:48)S S(cid:48)T
S(cid:48)T T (cid:48)T

Q =

=

(cid:21)

(cid:21)

.

(5.5)

That is, any invariant test depends on the data only through Q. The density of Q at q for
the parameters β and λ = µ(cid:48)

ΦµΦ is given by

fβ,λ(qS, qST , qT ) = K0 exp(−λ(c2

β + d2

β)/2) |q|(k−3)/2

(5.6)

× exp(−(qS + qT )/2)(λξβ(q))−(k−2)/4I(k−2)/2(

(cid:113)

λξβ(q)),

18

where K −1
Bessel function of the ﬁrst kind, and

0 = 2(k+2)/2pi1/2Γ(k−1)/2, Γ(·) is the gamma function, I(k−2)/2(·) denotes the modiﬁed

ξβ(q) = c2

βqS + 2cβdβqST + d2

βqT .

(5.7)

AMS06 further shows that another group, given by sign transformations, preserves H0 :
β = β0 against H0 : β (cid:54)= β0. Consider the group O (1), which contains only two elements:
h2 ∈ {−1, 1}. For h2 = −1, the data transformation is given by

h2 ◦ [S : T ] = [−S : T ]

(5.8)

(by the deﬁnition of a group, the parameter remains unaltered at h2 = 1). This yields a
transformation in the maximal invariant space for h2:

h2 ◦ (QS, QST , QT ) = (QS, h2QST , QT ) .

(5.9)

The maximal invariant for the joint transformation h = (h1, h2) is the vector with compo-
nents QS, Q2
ST , and QT . In principle, the tests can depend on Ω with homoskedastic errors.
As we will see in Theorem 2, we are able to eliminate the dependence on the variance as
well, and show the triad QS, Q2
ST , and QT is the maximal invariant for g = (g1, g2) in the
case of known, but not ﬁxed, variance.

5.1 Instrument Transformation

The orthogonal transformation argument of AMS06 is originally designed for homoskedastic
errors. For the general Kronecker case, both Φ1/2S and Φ1/2T (which are equivalent to
the original statistics of AMS06) have variance Φ. Because their methodology assumes the
variance to be ﬁxed, their orthogonal transformation would not work, in general, because the
variance would change. We could manually standardize their statistics by Φ−1/2 to obtain
our statistics S and T , and apply the orthogonal group, as done earlier.4 An alternative
solution is to allow Φ to be known, but for it to change as we transform the data. For
example, take the special case in which Φ is a diagonal matrix. If we were to permute the
entries of S and T jointly, perhaps we should allow the permutation of the diagonal entries of
Φ as well. Formally, we will take the variance Σ = Ω ⊗ Φ as part of both data and parameter
spaces.

For the special case in which Σ = Ω ⊗ Φ, the distribution of R0 is given by

R0 ∼ N (µ (∆, 1) , Ω0 ⊗ Φ) ,

(5.10)

where ∆ = β − β0, Σ0 = Ω0 ⊗ Φ, and Ω0 = B(cid:48)
0ΩB0. The data are the realizations (R0, Ω0, Φ)
and the parameters are (∆, µ, Ω0, Φ). The matrices Ω0, Φ are assumed to be known, but not
ﬁxed. Thus, Ω0, Φ are both parameters and part of the data, simultaneously.

We introduced the g1 ∈ GL (k) transformation in Section 3.1. Its action on the sample

space is given by

g1 ◦ (R0, Ω0, Φ) = (g1R0, Ω0, g1Φg(cid:48)

1) .

(5.11)

4We could look instead at g1 ∈ GL (k) such that g1Φg(cid:48)

1 = Φ. This yields g1 = Φ1/2h1Φ−1/2. This is the
same as transforming the data to RΦ = Φ−1/2R, applying the orthogonal transformations, and transforming
the data back to R.

19

We note that

so the corresponding action on the parameter space is

g1R0 ∼ N (g1µ (∆, 1) , Ω0 ⊗ g1Φg(cid:48)

1) ,

g1 ◦ (∆, µ, Ω0, Φ) = (∆, g1µ, Ω0, g1Φg(cid:48)

1) .

We now show that the matrix

Q = [S : T ](cid:48) [S : T ] =

(cid:20) S(cid:48)S S(cid:48)T
S(cid:48)T T (cid:48)T

(cid:21)

,

(5.12)

(5.13)

(5.14)

together with Ω0 itself, is the maximal invariant statistic. That is, any other invariant
statistic can be written as a function of (Q, Ω0). The distribution of the maximal invariant
depends only on the concentration parameter λ, the parameter of interest β, and Ω0 itself.

Theorem 1. For the group actions in (5.11) and (5.13):
(i) The maximal invariant in the sample space is given by (Q, Ω0); and
(ii) The maximal invariant in the parameter space is given by (cid:0)c2

βλ, cβdβλ, d2

βλ, Ω0

(cid:1).

Comments: 1. The data ([S : T ] , Ω0, Φ) is a one-to-one transformation from the prim-
itive data (R, Ω, Φ). Hence, there is no loss of generality in using the pivotal statistic S and
the complete statistic T instead of using R (or R0).

2. There is a one-to-one mapping between Ω0 and Ω. Hence, (Q, Ω) is a maximal
invariant as well. We continue to use Ω0 because it is useful to ﬁnd a maximal invariant for
the two-sided transformations to be considered next.

3. The statistic Q is the maximal invariant based on the compact orthogonal group on
[S : T ], which is a straightforward application of AMS06. We instead allow the much larger,
noncompact group of nonsingular matrices with unitary determinant. The data also contain
the variance components given by Ω0 and Φ. Because the group GL (k) is not amenable, the
Hunt-Stein theorem is not applicable, and we do not necessarily obtain a minimax result.
This is in contrast to Chamberlain (2007), who builds on the fact that the orthogonal group
is compact.

4. The component Φ completely vanishes as the noncompact group GL (k) acts transitively

on Φ. Hence, the matrix Φ is not part of the maximal invariant.

5.2 Two-Sided Transformation

We now apply the g2 ∈ GT (2) transformation introduced in Section 3.1. The two-sided
transformation in the Kronecker model is given by

g2 ◦ (R0, Ω0, Φ) = (R0g(cid:48)

2, g2Ω0g(cid:48)

2, Φ) ,

(5.15)

where g2 ∈ GT (2), the group of nonsingular lower triangular 2 × 2 matrices. The transfor-
mation in the parameter space is
(cid:18) ∆g11

(cid:19)

, g1µ (∆g21 + g22) , (g2 ⊗ g1) Σ0 (g(cid:48)

2 ⊗ g(cid:48)
1)

.

(5.16)

g ◦ (∆, µ, Σ0) =

∆g21 + g22

Theorem 2 ﬁnds the maximal invariant based on g1 ∈ GL (k) and g2 ∈ GT (2).

20

Theorem 2. For the data group actions deﬁned in (5.11) and (5.15), and the parameter
actions in (5.13) and (5.16), we ﬁnd
(i) The induced group action of g2 on the space ([S : T ] , Ω0, Φ) is

g2 ◦ ([S : T ] , Ω0, Φ) = ([sgn (g11) S : sgn (g22) T ] , g2Ω0g(cid:48)

2, Φ) ;

(ii) The data maximal invariant to g = (g1, g2) is
(cid:0)QS, QT , Q2

ST

(cid:1) ;

(iii) The induced group action by g2 on the parameter functions (cβ, dβ, µ, Ω0, Φ) is given by

g2 ◦ (cβµ, dβµ, Ω0, Φ) = (sgn (g11) cβµ, sgn (g22) dβµ, g2Ω0g(cid:48)

2, Φ) ; and

(iv) The parameter maximal invariant to g = (g1, g2) is
βλ, |cβdβ| λ(cid:1) .

βλ, d2

(cid:0)c2

Comments: 1. The parameters β and Ω remain unchanged by the action (5.13).
Because the parameters cβ and dβ depend only on β and Ω, they are preserved as well. The
result now follows trivially because g1 ◦ (µ, Ω, Φ) = (g1µ, Ω, g1Φg(cid:48)

1).

2. We note that g21 may be diﬀerent from zero. Hence, the group of transformations is
larger than scale multiplication to each entry of the vector (∆, 1). A naive generalization
for the sign group of transformations by AMS06 to our setup is a diagonal matrix g2. In
the online appendix, we show that some invariant tests based on the associated maximal
invariant can behave as one-sided tests. Hence, we illustrate the importance of ﬁnding the
largest group of transformations before deriving invariant tests.

These actions are deﬁned using the reduced-form matrix Ω. For the homoskedastic model,

we could analyze the transformations in the structural-form matrix

Ψ =

(cid:20) σuu σu2
σu2 σ22

(cid:21)

.

(5.17)

One may wonder if there are actually symmetries in the original model. This turns out to be
true. In fact, the action in the structural-form variance matrix has a very simple structure.

Proposition 1. The group action on the reduced-form matrix Ω induces an action on the
structural-form matrix Ψ:

g2 ◦ (∆, λ, Ψ) =

(cid:18) ∆g11

∆g21 + g22

, (∆g21 + g22)2 λ, ΓΨΓ(cid:48)

(cid:19)

, where

Γ =

(cid:20) (∆g21 + g22)−1 g11g22
g21

0
∆g21 + g22

(cid:21)

.

21

Comment: Take β0 = 0. When g11 = −1, g21 = 0, and g22 = 1, we have g2 ◦ (v1, v2) =
(−v1, v2). Therefore, σ11 and σ22 are preserved while σ12 changes sign. Since σ12 = σu2+σ22β,
the new value for the structural-form covariance scalar, −σu2, and the new value of the
parameter, −β, comprise the only transformation that works for any value of σ22.

A corollary of our theory is that the AR test is UMPI when structural-form variance is
ﬁxed. This optimality result is novel and important. All optimality theorems for the AR
test, so far, assume the reduced-form variance to be ﬁxed (Moreira (2002, 2009), AMS06,
and Moreira and Moreira (2019)).

6

Invariant Tests

We now use the group of transformations by g = (g1, g2) to develop the CIL test. Recall
that the data consist of R0 and Σ0, where R0 has a normal distribution and the distribution
of Σ0 is degenerate. So, the density of the data is the product of two parts. The ﬁrst
part is the normal distribution of R0, which is absolutely continuous with respect to the
Lebesgue measure. The second part is the degenerate distribution of Σ0, which is absolutely
continuous with respect to the counting measure. Understanding how the density changes
with the data transformation is important for the development of the CIL test.

The density of R0 = [R1 : R2] evaluated at r0 = [r1 : r2] is given by

fR (r0; ∆, µ, Σ0) = (2pi)−k |Σ0|−1/2 exp

(cid:26)

−

(cid:20) r1 − µ∆
r2 − µ

1
2

(cid:21)(cid:48)

Σ−1
0

(cid:20) r1 − µ∆
r2 − µ

(cid:21)(cid:27)

.

(6.1)

As in Theorem 2, we consider the groups of instrument transformations g1 and two-
sided transformations g2 together, so that we have the joint transformation g = (g1, g2)
deﬁned in Section 3.1, where g1 ∈ GL (k) and g2 ∈ GT (2), and the associated transformation
g ◦ (∆, µ, Σ0) in the parameter space.

Basic algebraic manipulations show that

fR (g ◦ r0; g ◦ (∆, µ, Σ0)) = fR ([r1 : r2] ; ∆, µ, Σ0) |g2|−k |g1|−2

|(g2 ⊗ g1) Σ0 (g(cid:48)

2 ⊗ g(cid:48)

1)| = |g2|2k|g1|4|Σ0|.

(6.2)

(6.3)

because

Therefore,

fR (r0; ∆, µ, Σ0) = fR (g ◦ r0; g ◦ (∆, µ, Σ0)) χ0 (g) ,
(6.4)
where χ0 (g) = χ1 (g1) χ2 (g2) for the sub-group multipliers χ1 (g1) = |g1|2 and χ2 (g2) = |g2|k.
Hence, the density of R0 is relatively invariant with multiplier χ0(g).

Of course, the action g ∈ GL (k) × GT (2) is not proper.5 We can impose |g1| = 1 so that
χ1 (g1) = 1. In this case, g1 ∈ SL (k), the group of invertible matrices with determinant
equal to one. Alternatively, we can use another standardization such as g22 = 1. To develop

5See Deﬁnition 5.1 of Eaton (1989) for a formal statement on a group acting properly on the sample
space. In our case, it is trivial that the action by g is not proper, since we can multiply g1 and divide g2 by
the same constant.

22

the integrated likelihood invariant test, we use Haar measures to obtain invariant tests. It
is harder to work with the Haar measure for SL (k) than for GL (k); see Dedi´c (1990). On
the other hand, it is relatively simple to derive the Haar measure for 2 × 2 lower triangular
matrices with g22 = 1. For this reason, we prefer to impose a restriction on GT (2).

For the second part, the data Σ0 have a distribution that assigns probability one to the

value Σ0 itself. Therefore, the density at some arbitrary matrix value σ0 is

Using (6.5), we have

fΣ (σ0; Σ0) = PΣ0 (σ0 = Σ0) = I (σ0 = Σ0) .

fΣ(g ◦ σ0; g ◦ Σ0) = fΣ ((g2 ⊗ g1) σ0 (g(cid:48)

2 ⊗ g(cid:48)

1) ; (g2 ⊗ g1) Σ0 (g(cid:48)

2 ⊗ g(cid:48)

1)) = fΣ (σ0; Σ0)

(6.5)

(6.6)

so that this density is invariant with multiplier 1.

The joint likelihood is then given by

f (r0, σ0; ∆, µ, Σ0) = fR (r0; ∆, µ, Σ0) · fΣ (σ0; Σ0) ,

(6.7)

so that

f (r0, σ0; ∆, µ, Σ0) = f (g ◦ (r0, σ0) ; g ◦ (∆, µ, Σ0)) · χ0 (g) ,
i.e. the likelihood is relatively invariant with multiplier χ0(g). Because the Lebesgue measure
is relatively left invariant for the group g with multiplier χ0(g), the (relative) invariance of
the likelihood follows directly.

(6.8)

We use the invariance of the likelihood to propose a conditional weighted likelihood ratio

test. We also show that the AR, LM, CQLR, and CLR tests are also invariant.

6.1 Optimal Tests

Our goal in this section is to ﬁnd optimal tests. Speciﬁcally, a test is deﬁned to be a
measurable function φ (r0, σ0) that is bounded by 0 and 1. For a given outcome, the test
rejects the null with probability φ (r0, σ0) and accepts the null with probability 1 − φ (r0, σ0),
e.g., the Anderson-Rubin test is simply I (AR > c (k)) where I (·) is the indicator function.
The test is said to be nonrandomized if φ takes only values 0 and 1; otherwise, it is called a
randomized test. The rejection probability is given by

E∆,µ,Σ0φ (R0, Σ0) ≡

(cid:90)

φ (r0, σ0) f (r0, σ0; ∆, µ, Σ0) dr0 η (dσ0) ,

(6.9)

where η is the counting measure. The rejection probability (6.9) simpliﬁes to

E∆,µ,Σ0φ (R0, Σ0) =

=

(cid:90)

(cid:90)

φ (r0, σ0) fR (r0; ∆, µ, Σ0) fΣ (σ0; Σ0) dr0 η (dσ0)

φ (r0, Σ0) fR (r0; ∆, µ, Σ0) dr0.

(6.10)

The rejection probability E∆,µ,Σ0φ (R0, Σ0) taken as a function of ∆, µ, and Σ0 gives the
power curve for the test φ. In particular, E0,µ,Σ0φ (R0, Σ0) gives the null rejection probability.

23

Let the parameter space for ∆, µ, σ0 be denoted by Θ, with σ-ﬁeld the intersection of Θ
and sets in Bk+1 × {Σ0}. Let w be a measure on that σ-ﬁeld. We average the power curve
over the parameter space to obtain the weighted average power with weights that are given
by the measure w. By Tonelli’s theorem, the weighted average power is

Ewφ (R0, Σ0) =

(cid:90)

E∆,µ,Σ0φ (R0, Σ0) dw (∆, µ, Σ0) .

(6.11)

If the weights are such that for B × {Σ0},

w (B × {Σ0}) = wR (B) · wΣ ({σ0}) ,

(6.12)

where B ∈ Bk+1 and wΣ ({σ0}) has unitary mass on {Σ0}, then

Ewφ (R0, Σ0) =

(cid:90)

φ (r0, Σ0) fwR (r0, Σ0) dr0,

(6.13)

where fwR (r0, Σ0) is deﬁned as

(cid:90)

fwR (r0, Σ0) =

fR (r0; ∆, µ, Σ0) dwR (∆, µ) .

(6.14)

For a given weight w, we seek optimal similar tests

max
0≤φ≤1

Ewφ (R0, Σ0) , where E0,µ,Σ0φ (R0, Σ0) = α, ∀µ.

(6.15)

The next proposition ﬁnds the WAP test.

Proposition 2. The optimal test in (6.15) rejects the null when

fwR (r0, Σ0)
fS (s)

> κ (t, Σ0) ,

(6.16)

where fS (s) = (2pi)−k/2 e−s(cid:48)s/2 is the density of the statistic S under the null.

Comment: Because T is suﬃcient for µ under the null, we condition on T = t. The

dependence of the test statistic on t is absorbed in the critical value of the test.

For arbitrary weights, the WAP similar test is not guaranteed to have overall good power
in ﬁnite samples. In particular, the power can be near zero for parts of the parameter space
(as happens with the CIL0 test for k > 2). We circumvent this problem by carefully choosing
weights w so that the test given by (6.16) is invariant. The CIL test behaves as a two-sided
test, and so, it does not suﬀer the criticism by Moreira and Moreira (2019).

24

6.2 Similar Invariant Tests

Invariance of conditional tests follows from the relative invariance of test statistics.

Deﬁnition 1. A statistic ψ is relatively (left) invariant to g with multiplier χ if

ψ (g ◦ (s, t, σ0)) = χ (g) · ψ (s, t, σ0) ,

for any (s, t, σ0).

Proposition 3 establishes the invariance of the conditional test if the test statistic is

relatively invariant.

Proposition 3. Suppose that ψ (S, t, Σ0) is a continuous random variable under H0 : ∆ = 0
for every t. Deﬁne κψ (t, Σ0) to be the 1 − α quantile of the null distribution of ψ (S, t, Σ0).
Then the following hold:
(i) The conditional test φ (s, t, Σ0) that rejects the null when

ψ (s, t, Σ0) > κψ (t, Σ0)

is similar at level α;
(ii) If ψ (g ◦ (s, t, Σ0)) is relatively invariant under g ∈ GL (k) × GT (2) with multiplier χ,
then κψ (t, Σ0) is itself relatively invariant with multiplier χ; and
(iii) The conditional test φ (s, t, Σ0) is invariant.

Comments: 1. Careful examination of the proof shows that invariance of the conditional
quantile does not depend on the group transformation used. It is also applicable to other
models as long as there is a suﬃcient statistic, e.g. here under the null, that is boundedly
complete.

2. The comment above explains why the conditional quantile of the LR statistic depends
only on T (cid:48)T in the homoskedastic case. The LR statistic does not depend on Ω0 at all, and
T (cid:48)T is the maximal invariant to orthogonal transformations h1 ◦T = h1 ·T . This is consistent
with the results of Moreira (2003) and AMS06, but with no need to use pivotal statistics
and independence.

Before showing that the CIL test is invariant and is the limit of conditional WAP tests,

as given by (6.16), we establish that the AR, LM , LR, and QLR statistics are invariant.

Proposition 4. The AR, LM , LR, and QLR statistics are invariant to g = (g1, g2) ∈
GL (k) × GT (2).

Comment: Close inspection shows the proof of invariance of the LR statistic is very
general. It works for any model in the presence of symmetries which preserve the testing
problem.

25

6.3 An Invariant WAP Similar Test

The goal is to obtain a WAP invariant similar test in the over-identiﬁed model (k > 1). This
entails ﬁnding weights so that the ﬁnal test is relatively invariant.

Deﬁnition 2. A measure m is relatively (left) invariant with multiplier χ if

(cid:90)

F (cid:0)g−1 ◦ θ(cid:1) m (dθ) = χ (g)

(cid:90)

F (θ) m (dθ)

for any real-valued continuous function F with bounded support.

We could apply this result for θ = (∆, µ, Σ0). However, the parameter Σ0 is known, but
changes according to the data transformation. Therefore, it is enough to allow θ to be the
parameters (∆, µ) only.

Lemma 1. The product measure |∆|k−2 d∆× dµ is relatively (left) invariant to g = (g1, g2) ∈
GL (k) × GT (2) with multiplier |g1| · |g11|k−1.

The next proposition shows that the conditional test is invariant and can be evaluated

with a single (and not multiple) integral.

Theorem 3. The conditional test based on the test statistic

IL =

(cid:20)
vec(R0)(cid:48)Σ−1/2

0

(cid:90)

− 1
2
e

N

Σ

−1/2
0

(a∆⊗Ik )

Σ−1/2
0

vec(R0)−T (cid:48)T

(cid:21)

(6.17)

× (cid:12)

(cid:12)(a(cid:48)

∆ ⊗ Ik) Σ−1

0 (a∆ ⊗ Ik)(cid:12)
(cid:12)

−1/2 |∆|k−2 d∆

is invariant and is the limit of a sequence of WAP tests deﬁned in (6.16).

In separate work, we address admissibility of the CIL test. Showing admissibility based
on invariant weights for non-amenable groups is done on a case-by-case basis. This issue
is analogous to that encountered for the commonly-accepted and widely-used Hotelling T 2
statistic for testing means of diﬀerent populations. Stein (1955) addresses the admissibility of
the Hotelling T 2 statistic. This test relies on the same non-amenable GL (k) group considered
here for the HAC-IV model.

For the construction of the IL statistic, both priors for θ and µ are improper. In the spirit
of Theorem 3, we need to consider sequences of weights for the alternative hypothesis instead.
For the nuisance parameter µ, Moreira and Moreira (2019) and Andrews and Mikusheva
(2020) allow an “identiﬁcation” parameter go to inﬁnity, so that the prior converges weakly
to the Lebesgue measure. The completeness theorem shows that any admissible test is
the limit of Bayes tests (suﬃciency). However, is the limit of any sequence of Bayes tests
admissible (necessity)? Farrell (1968a,b) considers a more concrete version of Stein’s proof
of admissibility. For example, Moreira and Moreira (2013, 2019) rely on Farrell’s approach
by using subsequence arguments for admissibility of WAP similar tests.

26

7 Conclusion and Extensions

This paper shows the importance of distinguishing between parameters being known or
being ﬁxed when showing the presence of symmetries in the HAC-IV model. However, this
distinction is applicable to many other models. The existence of symmetries could be useful,
as they could simplify inference (e.g., data reduction by invariance) or enable us to ﬁnd
better estimators and tests.

Econometricians are often interested in some parameters in the presence of others. It is
well-understood that knowing the value of a nuisance parameter typically yields more eﬃcient
estimators and tests than estimating it.
In some cases, however, knowing or estimating
the nuisance parameter yields the same asymptotic eﬃciency. This feature can happen in
parametric models in cross-section or time-series data, as well as in semi-parametric models,
among others.

• Consider a linear regression when the error variance is unknown (up to a parameter of
ﬁxed dimension). The generalized least-squares (GLS) estimator is the optimal linear
unbiased estimator.
It enjoys asymptotic eﬃciency among regular estimators. The
feasible generalized least-squares (FGLS) estimator is asymptotically eﬃcient when we
consistently estimate the parametric error variance.

• Take the predictive regression model where the explanatory variable can be nearly
integrated of order one. The asymptotic behavior of several tests is the same whether
the long-run variance matrix of the errors is known or consistently estimated.

• In the GMM model, we can consistently estimate the optimal weighting matrix using
a HAC estimator. Assuming the variance is known or estimated, the GMM estimators
are asymptotically equivalent and eﬃcient.

These examples illustrate the caveats of estimating or testing by assuming some parame-
ters are known. This natural simpliﬁcation ironically leads to complications when parameters
are assumed to be ﬁxed. In particular, it leads to the incorrect folk theorem that many models
do not present natural symmetries. Once we distinguish between the assumptions of known
versus ﬁxed parameters, model symmetries can exist, contrary to popular belief. We hope
this new methodology will lead to new inferential methods to apply to important econometric
models.

References

Anderson, T. W., and H. Rubin (1949): “Estimation of the Parameters of a Single Equa-
tion in a Complete System of Stochastic Equations,” Annals of Mathematical Statistics,
20, 46–63.

Andrews, D. W. K. (1991): “Heteroskedasticity and Autocorrelation Consistent Covari-

ance Matrix Estimation,” Econometrica, 59, 817–858.

Andrews, D. W. K., M. J. Moreira, and J. H. Stock (2004): “Optimal Invariant

Similar Tests for Instrumental Variables Regression,” NBER Working Paper t0299.

27

(2006): “Optimal Two-Sided Invariant Similar Tests for Instrumental Variables

Regression,” Econometrica, 74, 715–752.

Andrews, I. (2016): “Conditional Linear Combination Tests for Weakly Identiﬁed Models,”

Econometrica, 84, 2155–2182.

Andrews, I., and A. Mikusheva (2016): “Conditional Inference with a Functional Nui-

sance Parameter,” Econometrica, 84, 1571–1612.

(2020): “Optimal Decision Rules for Weak GMM,” Unpublished manuscript, Har-

vard University.

Chamberlain, G. (2007): “Decision Theory Applied to an Instrumental Variables Model,”

Econometrica, 75, 609–652.

Cruz, L. M., and M. J. Moreira (2005): “On the Validity of Econometric Techniques
with Weak Instruments: Inference on Returns to Education Using Compulsory School
Attendance Laws,” Journal of Human Resources, 40, 393–410.

Dedi´c, L.

(1990):

“On Haar Measure on SL(N,R),” Publications de L’Institut

Math´ematique, 47, 56–60.

Dufour, J.-M. (1997): “Some Impossibility Theorems in Econometrics with Applications

to Structural and Dynamic Models,” Econometrica, 65, 1365–1388.

Eaton, M. L. (1989): Group Invariance Applications in Statistics. Regional Conference
Series in Probability and Statistics, Volume 1. Hayward, CA: Institute of Mathematical
Statistics.

Farrell, R. H. (1968a): “On Necessary and Suﬃcient Condition for Admissibility,” The

Annals of Mathematical Statistics, 38, 23–28.

(1968b): “Towards a Theory of Generalized Bayes Tests,” The Annals of Mathe-

matical Statistics, 38, 1–22.

Kleibergen, F. (2005): “Testing Parameters in GMM without Assuming that they are

Identiﬁed,” Econometrica, 73, 1103–1123.

Lee, D. S., J. McCrary, M. J. Moreira, and J. Porter (2020): “Valid t-ratio

Inference for IV,” arXiv:2010.05058v1.

Lehmann, E. L., and J. P. Romano (2005): Testing Statistical Hypotheses. Third edn.,

New York: Springer Series in Statistics.

Mills, B., M. J. Moreira, and L. P. Vilela (2014): “Tests Based on t-Statistics for

IV Regression with Weak Instruments,” Journal of Econometrics, 182, 351–363.

Moreira, H., and M. J. Moreira (2013): “Contributions to the Theory of Optimal

Tests,” Ensaios Economicos, 747, FGV/EPGE.

28

(2019): “Optimal Two-Sided Tests for Instrumental Variables Regression with

Heteroskedastic and Autocorrelated Errors,” Journal of Econometrics, 213, 398–433.

Moreira, M. J. (2002): “Tests with Correct Size in the Simultaneous Equations Model,”

Ph.D. thesis, UC Berkeley.

(2003): “A Conditional Likelihood Ratio Test for Structural Models,” Economet-

rica, 71, 1027–1048.

(2009): “Tests with Correct Size when Instruments Can Be Arbitrarily Weak,”

Journal of Econometrics, 152, 131–140.

Moreira, M. J., and G. Ridder (2017): “Optimal Invariant Tests in an Instrumental
Variables Regression with Heteroskedastic and Autocorrelated Errors,” arXiv:1705.00231.

(2020): “Eﬃciency Loss of Asymptotically Eﬃcient Tests in an Instrumental Vari-

ables Regression,” arXiv:2008.13042v1.

Nelson, C. R., and R. Startz (1990): “The Distribution of the Instrumental Variables
Estimator and its t-Ratio when the Instrument is a Poor One,” Journal of Business, 63,
5125–5140.

Newey, W. K., and K. D. West (1987): “A Simple, Positive Semi-Deﬁnite, Heteroskedas-
ticity and Autocorrelation Consistent Covariance Matrix,” Econometrica, 55, 703–708.

Staiger, D., and J. H. Stock (1997): “Instrumental Variables Regression with Weak

Instruments,” Econometrica, 65, 557–586.

Stein, C. (1955): “A Necessary and Suﬃcient Condition for Admissibility,” The Annals of

Mathematical Statistics, 26, 518–522.

29

