Weakly Aligned Feature Fusion for Multi-modal
Object Detection

Lu Zhang, Zhiyong Liu, Senior Member, IEEE, Xiangyu Zhu, Zhan Song, Xu Yang,
Zhen Lei, Senior Member, IEEE, and Hong Qiao, Fellow, IEEE

1

2
2
0
2

r
p
A
1
2

]

V
C
.
s
c
[

1
v
8
4
8
9
0
.
4
0
2
2
:
v
i
X
r
a

Abstract—To achieve accurate and robust object detection in
the real-world scenario, various forms of images are incorpo-
rated, such as color, thermal, depth, etc. However, multi-modal
data often suffer from the position shift problem, i.e. the image
pair is not strictly aligned, making one object has different
positions in different modalities. For the deep learning method,
this problem makes it difﬁcult to fuse multi-modal features
and puzzles the CNN training. In this paper, we propose a
general multi-modal detector named Aligned Region CNN (AR-
CNN) to tackle the position shift problem. Firstly, a region
feature alignment module with adjacent similarity constraint is
designed to consistently predict the position shift between two
modalities and adaptively align the cross-modal region features.
Secondly, we propose a novel RoI jitter strategy to improve
the robustness to unexpected shift patterns. Thirdly, we present
a new multi-modal feature fusion method, which selects the
more reliable feature and suppresses the less useful one via
feature re-weighting. Additionally, by locating bounding boxes
in both modalities and building their relationships, we provide
labelling named KAIST-Paired. Extensive
novel multi-modal
experiments on 2D and 3D object detection, RGB-T and RGB-
D datasets demonstrate the effectiveness and robustness of our
method.

Index Terms—multi-modal object detection, pedestrian detec-

tion, feature fusion, deep learning.

I. INTRODUCTION

R OBUST object detection is a crucial ingredient of many

computer vision applications in the real world, such as
robotics, surveillance, and autonomous driving. Nowadays,
smart devices are usually equipped with different sensors
(e.g. cameras, thermal cameras, time-of-ﬂight, structured light,

This work was supported by the National Key Research and Development
Plan of China under Grant 2020AAA0108902, the Strategic Priority Research
Program of Chinese Academy of Science under Grant XDB32050100, the
Beijing Science and Technology Plan Project under grant Z201100008320029,
the NSFC under Grants 61627808, 61876178, and 61806196, the Dongguan
Core Technology Research Frontier Project, China (2019622101001).

L. Zhang, Z. Liu, X. Yang and H. Qiao are with the State Key Laboratory
of Management and Control for Complex Systems, Institute of Automation,
Chinese Academy of Sciences, Beijing 100190, China, and also with Univer-
sity of Chinese Academy of Sciences, Beijing 100086, China. Z. Liu and X.
Yang are also with Center for Excellence in Brain Science and Intelligence
Technology, Chinese Academy of Sciences, Shanghai 200031, China. E-mail:
{zhanglu2016, zhiyong.liu, xu.yang, hong.qiao}@ia.ac.cn

X. Zhu and Z. Lei are with the Center for Biometrics and Security Research
& National Laboratory of Pattern Recognition, Institute of Automation, Chi-
nese Academy of Sciences, Beijing 100190, China, and also with University
of Chinese Academy of Sciences, Beijing 100086, China. Z. Lei is also with
the Centre for Artiﬁcial Intelligence and Robotics, Hong Kong Institute of
Science & Innovation, Chinese Academy of Sciences, Hong Kong, China.
Email: {xiangyu.zhu, zlei}@nlpr.ia.ac.cn

Z. Song is with the Shenzhen Institutes of Advanced Technology, Chi-
nese Academy of Sciences, Shenzhen 518055, Guangdong, China. E-mail:
zhan.song@siat.ac.cn

Fig. 1. Illustration of the weakly aligned image pair. Yellow boxes in the left
column denote original annotations shared by two modalities, which present
the position shift in multi-modal inputs. Red boxes in the right column denote
the proposed multi-modal labelling, which has pairs of annotations that match
each input modality.

LiDAR, Radars), which provide different image forms. Com-
pared to pure RGB data [1]–[3], integrating the extra modality
is proven to be effective for recognition: infrared camera pro-
vides biometric information for around-the-clock pedestrian
detection [4]–[6] and face recognition [7], [8], and depth
channel provides 3D information for object detection [9]–[11]
and pose estimation [12], [13], etc. Motivated by this, the
multi-modal object detection has attracted massive attention,
not only towards the improvement of accuracy but also for a
better robustness promise.

Most existing multi-modal methods [4], [11], [14] detect
objects under the alignment assumption, i.e. the image pairs
from different modalities are well aligned and have strong
pixel-to-pixel correspondence. However, this assumption does
not always hold in practice, as shown in Figure 1, which poses
great challenges to existing methods. To ﬁll the gap with the
alignment assumption and real-world applications, we discuss
three main reasons behind the challenges.

Weakly AlignedColorThermal112233 
 
 
 
 
 
First, the alignment quality of image pair is limited by
the physical properties of different sensors, e.g. geometrical
disparity due to relative displacement [15], mismatched resolu-
tions and ﬁeld-of-views. For example, the border areas of color
images need to be sacriﬁced since the color camera usually
has a higher resolution and larger ﬁeld view than the thermal
camera [4]. This will naturally lead to imperfect calibration
and result in alignment error. Second, the calibration and re-
calibration processes are important while tortuous, generally
require particular hardware such as a special heated cali-
bration board [16]–[18], making re-calibration difﬁcult when
the device is in operation. Moreover, the alignment is easily
impacted by external disturbance (e.g. mechanical vibrations
and temperature variation) and hardware aging, which is hard
to be avoided when a system starts to operate.

As a result,

the multi-modal

images are often weakly
aligned in practice, thus there is position shift between one
object in different modalities, i.e. the position shift problem.
The position shift problem degrades the CNN-based detectors
mainly in two aspects. First, multi-modal features in the
corresponding position are spatially mismatched, which will
puzzle the CNN training. Second, the position shift problem
makes it difﬁcult to match objects from both input modal-
ities by using a shared bounding box. As a workaround,
existing multi-modal datasets [4], [5] use larger bounding
boxes to encompass objects from both modalities or annotate
objects on a single input modality. Such label bias can lead
to the training
to bad supervision signals and is harmful
process, especially for modern detectors [19]–[22] since they
generally use the intersection over union (IoU) overlap for
foreground/background classiﬁcation.

Thus, how to robustly locate the object on weakly aligned
modalities remains to be a crucial issue for multi-modal object
detection, while it is barely touched or studied in the literature.
Besides, due to different devices, deployment environments,
and operation duration, the priorities of modality information
and the patterns of position shifts are ever-changing. Hence it
is a very important challenge to cope with the position shift
problem using a single model.

In this paper, a novel framework is proposed to tackle the
position shift problem in an end-to-end fashion. Speciﬁcally,
we present a novel Region Feature Alignment (RFA) module
to shift and align the to-be-fused region features from input
the region-based alignment
modalities. With this module,
process is inserted in the network and works in a learnable
way. To further enhance the robustness to different patterns
of position shift, we propose the RoI jitter training strategy,
which augments the RoIs of the sensed modality via random
jitter.

To validate the effectiveness of our method, we apply the
proposed framework on the typical RGB-T (multispectral)
pedestrian detection. To better meet
the around-the-clock
demands, we design a new conﬁdence-aware fusion module,
which selects the more reliable feature and suppresses the
less useful one via adaptive feature re-weighting. Besides,
we provide a novel KAIST-Paired1 annotation by locating

1available at https://github.com/luzhang16/AR-CNN

2

the bounding boxes in both modalities and building their
relationships. We also collect a drone-based RGB-T dataset,
which includes more object categories (e.g. car, bus, truck,
cyclist), to validate the generalization to RGB-T object de-
tection. Moreover, we extend the proposed method to the
RGB-D based object detection task. Experiments on 3D object
detection are conducted on the standard NYUv2 dataset. To
further validate the proposed framework, we build a dataset
containing different kinds of indoor objects, called SL-RGBD,
in which image pairs are labelled with multi-modal bounding
boxes. Extensive experimental results demonstrate that the
proposed method signiﬁcantly improves the robustness to
position shift problem and takes full advantage of multi-modal
inputs by effective feature fusion, thus achieving state-of-the-
art result on the challenging KAIST and CVC-14 datasets. For
the general 2D and 3D object detection tasks, our method also
achieves the best robustness performance on the NYUv2 and
SL-RGBD datasets.

This paper is an extension of our previous work [23]
mainly in the following aspects: 1) We improve our method
with the adjacent similarity constraint for the region shift
prediction and extend the approach to a general multi-modal
object detection task, including RGB-Thermal and RGB-Depth
detection, 2D and 3D object detection; 2) The experimental
upper bound of position shift for the weakly aligned image
pair are introduced to specify the deﬁnition of the weakly
aligned; 3) Additional experiments and analyses are conducted
to better show the effectiveness and generalization ability of
the proposed method.

The remainder of the paper is organized as follows: we
review the related work in Section II. Section III discusses
the position shift problem in weakly aligned image pairs and
presents our motivation. Section IV describes the proposed
framework in detail. Extensive experiment results of RGB-T
pedestrian detection are given in Section V. In Section VI, we
further report the results of the proposed method on RGB-D
based 2D and 3D object detection. We conclude the paper in
Section VII.

II. RELATED WORK

Recent intelligence systems have access to various forms
of images, such as RGB, depth, infrared, etc. Compared to
the RGB-based method, the adoption and fusion of novel
modality greatly improve the performance and robustness, thus
enabling many practical applications, e.g. around-the-clock
surveillance, autonomous driving and robot grasping.

A. Multispectral (RGB-T) Pedestrian Detection

Pedestrian detection is an essential step for many applica-
tions and recently gets increasing attention. In previous years,
many algorithms and features have been proposed, including
the traditional detectors [24]–[27] and the lately dominated
CNN-based detectors [28]–[32]. With the recent release of
large-scale multispectral pedestrian benchmarks, efﬁciently
exploiting the multispectral data has shown great advantages
on pedestrian detection, especially for the around-the-clock
operation [33]–[38]. In [4], Hwang et al. propose the KAIST

multispectral dataset and extend the ACF method to make full
use of the aligned color-thermal image pairs. With the success
of deep learning, several CNN-based methods [39]–[42] are
proposed in recent years. Liu et al. [43] build a two-stream
detector and experimentally analyse different fusion timings.
K¨onig et al. [44] fuse features of the Region Proposal Network
(RPN) and introduce the Boosted Forest (BF) framework as the
classiﬁer. In [45], Xu et al. propose a cross-modality learning
framework that can cope with missing thermal data at testing
time. Zhang et al. [40] utilize the cross-modality attention
mechanism to recalibrate the channel responses of halfway
feature maps. However, most previous approaches conduct
multi-modal fusion under the full alignment assumption. This
not only hinders the use of weakly aligned datasets (such
as CVC-14 [5]), but also limits the further development of
multispectral pedestrian detection, which is worthy of attention
but still lacks research.

B. RGB-D Object Detection

We brieﬂy introduce the 2.5D approaches in RGB-D images,
in which the depth images are generally treated in a similar
fashion as RGB images. In [46], the detector combines the
Histogram of Oriented Gradients (HOG) features of RGB data
and Histogram of Oriented Depths (HOD) of dense depth
data in a probabilistic way. With the recent dominance of
CNNs, Gupta et al. [9] build a CNN-based detector on the
top of pre-computed object segmentation candidates. They
extend the R-CNN [47] to utilize depth information with
HHA encoding (Horizontal Disparity, Height above ground,
and Angle of local surface normal w.r.t gravity direction). But
the outputs of detectors are still limited to 2D ones. In order
to infer the 3D bounding box, Deng et al. [11] propose the
Amodal3Det, which further explores the strong CNN to infer
3D dimensions in RGB-D data efﬁciently. Besides, the single-
shot 3D-SSD [48] is designed to achieve high-speed inference.
[49] downsamples the RoIs to get better 3D initialization.
Different from them, we consider the position shift problem
in RGB-D pairs for 3D object detection.

C. Weakly Aligned Image Pair

Though the multi-modal object detection is extensively
studied, few detection methods touch the position shift prob-
lem in a weakly aligned image pair. Since this problem is
unavoidable in the realistic scenarios due to both hardware
and environmental factors, before the detection methods, it
can be independently mitigated in two ways: pre-process and
post-process.

The pre-process aims to solve the camera calibration prob-
lem and elaborate a delicate modality-aligned system. How-
ever, this hardware promise is easily impacted by unavoidable
external disturbance. Also, the recalibration [16], [50] pro-
cess can be difﬁcult due to the characteristics of additional
modality. For example, the calibration of RGB and thermal
images generally need particular hardware and the special
heated calibration board [16]–[18].

A common paradigm of the post-process is to conduct image
registration (i.e. spatial alignment) [51]–[53]. As a image-level

3

(a)

(b)

Fig. 2. Some instances of original annotations in the KAIST (yellow boxes),
CVC-14 (orange box), and drone-based (white boxes) datasets. We crop the
image patches in the same position, but the objects pose poor alignment. Thus
such a shared bounding box for both modalities can not precisely locate the
object.

solution for the position shift problem, it geometrically aligns
the reference and sensed images. This task mainly consists
of four processes: feature detection, mapping function design,
feature matching, image transformation and re-sampling. Al-
though the image registration is well established, the low-level
transformation on the whole image is often time-consuming
and restricts end-to-end training of the CNN-based detector.
Moreover, since different modalities present different appear-
ances, the registration process faces great challenges because
the correspondence of key points is harder to determine.

III. MOTIVATION

In this section, we present our analysis of the position
shift problem in the weakly aligned image pair. To provide
insights into this problem, we ﬁrstly analyse the popular
KAIST [4] and CVC-14 [5] RGB-T pedestrian datasets, and
the structured-light-based RGB-D system. Then we experi-
mentally study how the position shift deteriorates the detection
performance.

A. The Position Shift Problem

The position shift is deﬁned as the spatial displacement
between two images of different cameras. The image pair is
taken in the same scene and time, generally including the same
objects. However, caused by the position shift problem, the
pixel-to-pixel relationship does not hold, two corresponding
image patches can be located in different positions.

RGB-T Pair From the image pairs in KAIST and CVC-14
RGB-T pedestrian datasets and the drone-based object dataset,
we can observe several issues.

• Misaligned Features. Figure 2(a) illustrates the position
shift between modalities, this problem makes it difﬁcult
to directly fuse such misaligned features. Besides, the
shift varies in different positions even in a single image.
In Figure 3, we present the statistics of position shift
in two datasets, which show that collection devices and
operation environments also inﬂuence the shift pattern.

Poor AlignmentUnpaired Objects4

(a) KAIST dataset

(b) CVC-14 dataset

Fig. 3. Statistics of the position shift for bounding boxes in KAIST and
CVC-14 datasets.

• Localization Bias. As shown in Figure 2(a), the annota-
tions need to be adjusted to match the weakly aligned
image pair. One way to remedy is using larger bounding
boxes to encompass objects from both modalities, but also
including unnecessary background information. Another
workaround is only focusing on one particular modality,
but this could introduce bias for another modality.

• Unpaired Objects. In practice, color and thermal cameras
often have different ﬁeld-of-views, the situation is even
worse when the synchronization and calibration are not
good. Therefore, as shown in Figure 2(b), the objects
appear in one modality but are truncated or lost in another.
Speciﬁcally, ∼ 12.5% (2, 245 of 18, 017) of pedestrians
are unpaired in CVC-14 [5].

RGB-D Pair Around different types of 3D sensing systems,
the projector-camera-based structured light methods have been
more and more important for 3D vision related applications.
A typical structured light system is composed of one pro-
jector and one industry camera. Since patterns used by the
structured light systems are usually monochrome or binary,
the monochrome cameras are preferred. In real applications,
to obtain high-quality color texture for the reconstructed 3D
models, an extra high-resolution RGB camera like a DSLR
(Digital Single-Lens Reﬂex) camera is usually added to the
system [54]. As a result, alignment of the high-resolution color
image and the low-resolution point cloud or depth image is
essential for any structured light systems which used extra
color texture camera.

To reveal the realistic position shift problem of RGB-D data,
we build such an RGB-D data collection system with a pair
of color and structural light cameras, as shown in Figure 4.
We can see that even the system is well calibrated, time and
external disturbance will degrade the quality of the calibration.
As demonstrated in Figure 4, the main problem is position
shift, along with milder deformation and rotation.

Fig. 4.
Illustration of the RGB-D system. The system consists of the color
camera and the projector-camera-based structured light. As the calibration
quality degrades, the RGB and depth image pair suffers from the position
shift problem.

(a) Baseline

(b) The proposed detector

Fig. 5. Surface plot of performances within the position shift experiments.
The x and y axis denote different step sizes by which color images are shifted
along. The vertical axis indicates the MR measured on the KAIST dataset,
lower is better.

dataset, this solid baseline detector achieves 15.2 log-average
miss rates (MR), about 11.0% better than the 26.2 MR
reported in [42], [43].

Robustness to Position Shift Without loss of generality, we
set thermal images as the reference, hence the color images are
spatially shifted along the x and y axis. We select 169 different
shift patterns in {(∆x, ∆y) | ∆x, ∆y ∈ [−6, 6]; ∆x, ∆y ∈
Z}. As illustrated in Figure 5(a), the detection performance
signiﬁcantly drops as the absolute shift value increases. Espe-
cially, the worst-case (∆x, ∆y) = (−6, 6) suffers about 65%
relative performance degradation (from 15.2 to 25.1 MR). The
interesting thing is, the origin without any shift does not get
the best performance. When we shift the color images to a
speciﬁc direction (∆x, ∆y) = (1, −1), the detector achieves
a better 14.7 MR. This indicates that the detection performance
can be further improved by appropriately tackling the position
shift problem.

IV. OUR METHOD

B. How the Position Shift Impacts?

To quantitatively analyse how the position shift problem
impacts the detector, we perform experiments on the relatively
well aligned KAIST dataset by manually simulating the posi-
tion shift.

Baseline Detector Following the setting in Adapted Faster
R-CNN [55], we build the baseline detector and utilize
RoIAlign [56] to extract region features. We use the halfway
fusion settings [43] for multispectral inputs. On the KAIST

In this section, we introduce the Aligned Region CNN
(AR-CNN) framework and KAIST-Paired annotation (Section
IV-C1). The architecture is illustrated in Figure 6, contain-
ing the region feature alignment module (Section IV-A), the
RoI jitter training strategy (Section IV-B). We also present
the conﬁdence-aware fusion step (Section IV-C1) for all-day
RGB-T pedestrian detection and the adaption to RGB-D based
3D object detection task (Section IV-D). Algorithm 1 describes
the whole pipeline.

Structured LightRGB-D systemColorFusion?Y/pixelsX/pixelslog-average Miss Ratelog-average Miss RateY/pixelsX/pixels15161718192021222324258.59.510.5910Y/pixelsX/pixelslog-average Miss Ratelog-average Miss RateY/pixelsX/pixels15161718192021222324258.59.510.59105

Fig. 6. The architecture of the proposed AR-CNN. We use color-thermal input as an example, after the two-stream feature extractor, numerous proposals are
generated by the fused Region Proposal Network (RPN). Then the RFA module is utilized to predict and align the shift of regions, and an extra shift loss
is calculated by utilizing the proposed KAIST-Paired annotation. Meanwhile, we perform the RoI Jitter strategy to enrich the shift patterns during training.
After that, we can pool the aligned region features from the feature map of each modality. For the all-day RGB-T pedestrian detection, we utilize the
conﬁdence-aware fusion (CAF) method to pay attention to the more reliable modality, Wr and Ws denote the conﬁdence weight of reference and sensed
modality respectively. For the 3D RGB-D object detection, we calculate the multi-task loss based on the 3D box initialization and regression process.

A. Region Feature Alignment

the position shift

To predict and align the shift between two modalities, we
present the Region Feature Alignment (RFA) module in this
is not a simple
subsection. In practice,
afﬁne transformation due to the different properties of camera
systems. As a result, the shift varies in different regions. It is
usually large in regions near the image edge and relatively
the RFA module
small near the image center. Therefore,
performs the shift prediction and feature alignment at the
regional level.

Reference and Sensed Modality In image registration
the reference and sensed images refer to the
[51], [52],
stationary and transformed images, respectively. We introduce
this concept into our multi-modal setting. In the training phase,
the reference modality is ﬁxed and the feature alignment and
RoI jitter strategy are performed on the sensed modality.

Proposals Generation To keep more potential objectness
regions, we aggregate both the reference and sensed feature
maps (Conv4 r and Conv4 s) to generate the proposals. We
utilize the Region Proposal Network (RPN) [19] for the
proposal generation process.

Feature Alignment Figure 7 illustrates the concrete connec-
tion scheme of the RFA module. Speciﬁcally, the RFA module
ﬁrst enlarges the proposals (RoIs) to encompass the contextual
information. Then we pool the contextual region features of
each modality to small H × W (such as 7 × 7) feature maps.
Then the pooled region features are concatenated to obtain
the cross-modal representation. Based on this representation,
we use two consecutive fully connected layers to predict the
shift (i.e. tx and ty) of each region. In this way, we obtain

new coordinates of the sensed region and re-pool on this new
region to get the aligned sensed features.

Since the proposed KAIST-Paired annotation (more details
in Section IV-C1) provides multi-modal bounding boxes, we
can calculate the targets of ground truth shift as follow:

t∗
x = (xs − xr)/wr

t∗
y = (ys − yr)/hr

(1)

where x and y are the center coordinates of the box, w and h
refer to the width and height, xr and xs denote the reference
and sensed ground truth box, t∗
x and t∗
y indicate the shift target
for x and y coordinates, respectively.

Adjacent Similarity Constraint For natural images, the
position shift is spatially smooth, i.e. the shift targets for
adjacent regions tend to be similar. Base on this, we add
the adjacent similarity constraint
to stabilize the training
process of the RFA module. Speciﬁcally, we ﬁrst randomly
sample one of four nearest pixels (with feature stride) to the
pooled features maps of the region of interest. Then, the same
alignment targets are assigned to the region of interest and
its neighbour, thus it encourages the RFA module to predict
similar for adjacent regions.

Position Shift Loss To measure the accuracy of predicted

shift targets, we calculate the position shift loss as follow:

Lshif t({p∗

i }, {ti}, {t∗

i }) =

1
Nshif t

n
(cid:88)

i=1

i smoothL1(ti − t∗
p∗
i )

(2)

The subscript i denotes the index of RoIs in minibatch, p∗
i is
the class label (1 for the pedestrian and 0 for the background)

RegionProposal NetworkConv3_s⊕Conv4_sConv5_sAggregationProposalsRoIJitter⊕Region ShiftRegion FeatureAlignmentRegionFeaturesShiftLoss*Ws*Wr⊕Confidence-Aware FusionFC LayersMulti-task LossAlgorithm 1 Framework of the multi-modal object detection
system.
Input:

The image of reference modality, Ir;
The image of sensed modality, Is;
The output threshold τ ;

Output:

A set of detection results Dr on reference images;

1: Extracting the feature maps Convr and Convs from the

reference and sensed modality;

2: Aggregating Convr and Convs to generate a set of 2D

proposals P;

3: Extracting region-wise features Fr and Fs of P by using

RoI pooling;

4: Predicting the position shift for P using the RFA module

and re-extracting aligned region-wise features F

(cid:48)

(cid:48)

s;
s with conﬁdence-

5: Fusing region-wise features Fr and F

aware module to obtain Ff ;

6: Conducting region-wise classiﬁcation and (3D) bounding
boxes regression to obtain conﬁdences C i and coordinats
f ∈ Ff of each proposal P i ∈ P;
Bi for the F i

7: Dr ← ∅;
8: for each P i ∈ P do
if C i > τ then
9:
10:
end if
11:
12: end for
13: return Dr;

Dr ← Dr ∪ {[Bi, C i]};

6

(a) Reference modality

(b) Sensed modality

Illustration of the proposed RoI jitter strategy. The red boxes are
Fig. 8.
reference and sensed ground truth boxes (GTR and GTS ). The blue boxes
stand for RoIs (proposals), which are shared by the two modalities. RoIj1,2,3
denote three potential proposals after jitter.

deﬁnition of RPN loss follows the literature [19].

B. RoI Jitter Strategy

During training, the position shift can be limited to a certain
kind of pattern, which is often related to the collection devices
and settings of datasets. However, the practical shift pattern is
unexpected. To ﬁll the gap between the ofﬂine training and
online testing, we present the RoI jitter strategy, as illustrated
in Figure 8. For the sensed modality, we introduce a random
disturbance to generated new jittered positions of the proposal.
Meanwhile, the shift targets of the RFA module are enriched
correspondingly. The random jitter target is derived from the
Gaussian distribution as follow:

x, tj
tj

y ∼ N (0, σ2

0;0, σ2

1; 0)

(4)

Fig. 7. The concrete structure of the proposed RFA module. Given the
contextual region feature (RF) of each modality, this module obtains the cross-
modal RF by conducting element-wise summation (⊕). Then we utilize two
FC layers to predict the position shift of each region of interest.

of the RoI, ti indicates the predicted shift, and t∗
i is the ground
truth shift target. Nshif t denotes the number of to-be-aligned
RoIs.

Multi-task Loss Then the multi-task loss function is deﬁned

as follow:

L({pi}, {ti}, {gi}, {p∗
+λ1Lshif t({p∗

i }, {ti}, {t∗

i }, {t∗

i }, {g∗

i }) = Lcls({pi}, {p∗
i }, {ˆti}, {t∗
i }, {gi}, {g∗

i }) + λ2Lasc({p∗
+Lreg({p∗

i })
i })
i })

(3)

Lcls and Lreg are similar to the loss in Fast R-CNN
[57]. Lasc is the loss of the adjacent similarity constraint,
ˆti is the predicted shift of the 4-neighbourhood. Variable pi
refers to the predicted class conﬁdence for the i-th RoI, gi
is the predicted reﬁnement, p∗
i are associated ground
truths. We balance the Lshif t and Lreg by setting the λ as
the weighting parameter. To weight the two terms, we use
λ1 = 0.75, λ2 = 0.25. Apart from this multi-task loss, the

i and g∗

Variable tj indicates jitter targets along the x and y axis,
σ denotes the radiation extent of jitter. Afterward, we use the
inverse process of Equation 1 to jitter the RoI to RoIj.

Minibatch Sampling To select

the set of samples for
minibatch training, we need to deﬁne the positive and negative
samples by calculating the IoU overlap. In our setting, we
consistently use the reference RoI for this calculation, since
we perform the random jitter on the sensed RoI.

C. All-day RGB-T Pedestrian Detection

1) KAIST-Paired Annotation: Existing RGB-T datasets ig-
nore the cross-modal correlation between pedestrians. To ﬁll
this gap, we provide the multi-modal labelling following these
rules:

• Multi-modal locating. We locate the pedestrian in both
color and thermal modality, thus providing clear position
shift patterns.

• Adding relationships. To indicate the cross-modal cor-
relations of objects, we assign unique indexes to the
pedestrians.

• Annotating unpaired pedestrians. If a pedestrian only
appears in a single modality, we annotate it as the
“unpaired” one.

Statistics of KAIST-Paired Since we have access to the
proposed KAIST-Paired annotation, the statistics of position
shift in the original KAIST dataset can be derived. As shown

Reference RFSensed RFCross-Modal RFFC LayersContextualRoI Pooling⊕Region ShiftRoIRGTRoIj1RoIj2RoIj3RoISGT7

Method

SVM [5]
DPM [5]
Random Forest [5]
ACF [58]
Faster R-CNN [58]
MACF [58]
Choi et al. [59]
Halfway Fusion [58]
Park et al. [58]
AR-CNN (Ours)

e
l
b
i
s
i
V

l
a
m
r
e
h
T
+
e
l
b
i
s
i
V

MR
Night
76.9
76.4
81.2
83.2
71.4
48.2
43.8
34.4
30.8
18.1

Day
37.6
25.2
26.6
65.0
43.2
61.3
49.3
38.1
31.8
24.3

All
-
-
-
71.3
51.9
60.1
47.3
37.0
31.4
22.0

TABLE I
DETECTION PERFORMANCES ON THE CVC-14 DATASET. THE FIRST
COLUMN DENOTES THE INPUT MODALITIES OF THE METHOD. FOR ACF,
MACF, FASTER R-CNN, AND HALFWAY FUSION, WE USE THE
RE-IMPLEMENTATION IN THE LITERATURE [58].

Method
SSD [20]
Faster R-CNN [19]
Halfway Fusion [43]
CIAN [40]
AR-CNN (Ours)

mAP
23.6
26.7
31.9
34.8
36.4

car
40.6
44.7
52.6
57.0
59.8

bus
31.7
30.7
37.7
40.2
40.6

truck
21.3
21.9
25.0
28.7
29.9

cyclist
6.8
9.5
12.4
13.3
15.1

TABLE II
OBJECT DETECTION RESULTS ON THE DRONE-BASED RGB-T DATASET.
ONLY RGB IMAGES ARE USED TO TRAIN THE SSD AND FASTER R-CNN.
RGB-T IMAGE PAIRS ARE USED TO TRAIN THE HALFWAY FUSION, CIAN,
AND AR-CNN MODEL. WE USE MAP TO EVALUATE THE PERFORMANCE
OF DETECTORS.

3D bounding box initialization and regression Given sev-
eral 2D proposals, we use some basic transformations to ini-
tialize the related 3D bounding boxes. Following the settings
in [11], each 3D bounding box is parametrized into one seven-
entry vector [xcam, ycam, zcam, l, w, h, θ]. [xcam, ycam, zcam]
is the centroid under camera coordinate system, and [l, w, h] is
the 3D dimension. The parameter θ ∈ [−π/2, π/2] is the angle
between the principal axis and its orientation vector under the
tilt coordinate system, in which the point clouds are aligned
with gravity direction. Hence this rotation is only around the
gravity direction.

For initialization, the median of depth value in each indi-
vidual region is used as the initialization of z axis, noted as
zini, then the xini and yini can be calculated as follow:

(cid:40)

xini = zini ∗ (cx − ox)/f
yini = zini ∗ (cy − oy)/f

(5)

where f is the focal length of RGB camera, (ox, oy) is the
principal point, (cx, cy) is the center coordinate of 2D box
proposal. The 3D size [l, w, h] is set as the class-wise average
dimension, which is inspired by the familiar size in human 3D

Fig. 9. The conﬁdence-aware fusion module. We illustrate three typical
situations in RGB-T pedestrian detection: (a) in the daytime, both modalities
present clear imaging; (b) when the illumination is bad, the pedestrian in
color modality is difﬁcult to distinguish, hence more attention will be paid to
the thermal modality; (c) the pedestrian is unpaired, only existing in a single
modality, hence the color feature will be depressed.

in Figure 3(a), over half of pedestrians suffer from the position
shift, which mostly ranges from 0 to 10 pixels.

In all-day operations,

2) Conﬁdence-Aware Fusion:

the
quality of information from color and thermal modality is
diverse: features from the color modality are distinguishable
in the daytime yet fade in the nighttime; and thermal images
provide the pedestrian silhouette around the clock but lacks
visual details like the clothing. To make full use of the
characteristics between two different modalities, we propose
the conﬁdence-aware fusion method to select the more reliable
feature while suppressing the less useful one via feature re-
weighting.

As illustrated in Figure 9,

the conﬁdence-aware fusion
module uses two conﬁdence weights Wr and Wf . To calculate
the two weights, we add a branch for each modality to obtain
classiﬁcation score p. Then the conﬁdence weights for each
s − p0
modality are calculated as: Wr = |p1
s|,
where p1 and p0 are the probability of foreground and back-
ground, r denotes the reference modality and s is the sensed
one. Then this module performs the multiplication to pay more
attention to the reliable modality.

r|, Ws = |p1

r − p0

Unpaired Objects To mitigate the ambiguous classiﬁcation
for unpaired objects, we use the disagreement weight, Wd =
1 − |p1
s|. If the sensed modality provides
a contradictory prediction with the reference, its feature will
be suppressed.

s| = 1 − |p0

r − p0

r − p1

D. 3D RGB-D Object Detection

To better understand the physical 3D world, object detection
also aims to predict the 3D location and its full extent in 3D
space. In this paper, we have RGB-D image pairs as inputs
and leverage the 2.5D representation to predict 3D bounding
boxes.

2D RoI proposals Since RGB and depth images contain
complementary information, we use both modalities to gener-
ate the proposals in 2D space. As in Section IV-A, the Region
Proposal Network (RPN) is adapted to generate 2D proposals
in an end-to-end fashion.

******ped: 0.94ped: 0.59Wr =0.88Ws =0.18⊕Wd =0.65Thermal (reference)Color (sensed)Wr =0.98Ws =0.96⊕Wd =0.99ped:0.99ped:0.98Wr =0.78Ws =0.96Wd =0.13ped: 0.02⊕ped: 0.89(a)(b)(c)perception [11], [60], [61]. The angle θ is set to 0 by default.

With the seven-entry targets prediction between initialized
vector and targeted vector, i.e. [vx, vy, vz, vl, vw, vh, vθ], the
3D regeression loss can be calculated as follow:
3d) = [p∗ >= 1] smoothL1(v∗

L3dreg(p∗, v3d, v∗

3d − v3d) (6)

where p∗ is the ground-truth class index, v∗
3d are the ground
truth regression offsets, and v3d are the predicted regression
targets.

8

dataset, we use an initial learning rate 0.0005 for 9 epochs
and decay it by 0.1 for another 3 epochs.

We utilize the log-average miss rate (MR) to measure the de-
tection performance, lower score indicates better performance.
We plot the miss rate averaged against the false positives per
image (FPPI) over the log-space range of [10−2, 100]. For the
test set, we use the widely adopted improved annotation, which
is proposed by Liu et al. [43]. Besides, based on our KAIST-
Paired annotation, we use the MRC and MRT to denote
the MR for color and thermal modality, respectively. For the
drone-based dataset, we use mean Average Precision (mAP)
to evaluate the performance of detectors.

V. EXPERIMENTS ON CASE 1: RGB-T OBJECT
DETECTION

C. Comparison Experiments

In this section, we ﬁrst conduct several experiments on the
challenging KAIST [4] and CVC-14 [5] RGB-T pedestrian
datasets. We evaluate all methods on the “reasonable” setup in
which pedestrians are not or partially occluded and the heights
are larger than 55 pixels. Then, to verify the generalization
of our method on object detection, we collect a drone-based
RGB-T dataset which includes 4 categories of objects: car,
truck, bus, cyclist.

A. Dataset

KAIST KAIST multispectral pedestrian dataset [4] includes
95, 328 color-thermal frame pairs with 103, 128 annotations.
To cover diverse illumination conditions, the dataset is cap-
tured in different scenes,
through day and night. For the
testing, we use the test set containing 2, 252 image pairs
sampled from the video with 20-frame skips.

CVC-14 CVC-14 dataset [5] uses both visible and FIR
cameras to record color-thermal video sequences during day
and night time. It contains 8, 518 frames, among which 7, 085
frames are for training and 1, 433 for testing. The two cameras
are not in a well-calibrated condition. In consequence, this
dataset suffers more from the position shift problem, as shown
in Figure 3(b). This problem makes it difﬁcult for state-of-the-
art multi-modal detectors to apply to the CVC-14 dataset [40],
[42], [45], [62], [63].

Drone-based Dataset The drone-based dataset is collected
from day to night using a DJI MATRICE 300 RTK drone
with pre-calibrated RGB-T cameras, covering different scenes
such as city, suburbs, and countryside. It contains 831 pairs
of all-day images with 6, 763 pairs of dense annotations for
4 object categories: car, truck, bus, cyclist. Though the RGB-
T cameras are pre-calibrated, there is still the position shift
problem due to different physical properties of cameras and
external disturbance in operation.

B. Implementation Details

KAIST The proposed AR-CNN is evaluated and compared
to other available competitors [4], [40], [42]–[44], [62]. As
shown in Figure 10, our method achieves 9.79, 8.07, and 9.03
MR on the reasonable day, night, and all-day subset respec-
tively, achieving the state-of-the-art performance. Besides, in
consideration of the position shift problem, the methods are
also evaluated with the KAIST-Paired annotation, i.e. MRC
and MRT . As shown in Figure 10, the proposed method has
signiﬁcant advantages, i.e. 8.52 v.s. 11.28 MRC and 8.01
v.s. 12.51 MRT , demonstrating the superiority of our method.
CVC-14 Our experiments on the CVC-14 dataset are fol-
lowing the protocol in [58]. As shown in Table I, compared
to other competitors, the proposed method achieves the best
performance. The greater advantage on the night subset (18.1
v.s. 30.8 MR) validates the contribution of thermal modality,
and shows that the detection accuracy can be signiﬁcantly
improved by appropriately handling the position shift problem.
Drone-based dataset As shown in Table II, compared
to the available competitors, our approach achieves the best
36.4 mAP, which demonstrates the generalization ability from
pedestrian detection to the wider object detection.

D. Robustness to Position Shift

Most multi-modal systems suffer from the position shift
problem, but degrees of the shift are varying with different
devices and settings. To clearly set
the experiments, we
introduce the empirical upper bound of position shift for the
weak aligned image pair.

Since the pedestrian detection and object detection tasks use
different evaluation metric, i.e. log average miss rate (MR,
lower is better) and mean average precision (mAP, higher is
better), we respectively deﬁne the performance degradation
rate Rd as follow:

Rd = (MRdegraded − MRoriginal)/MRoriginal

Rd = (mAPoriginal − mAPdegraded)/mAPoriginal

(7)

(8)

We use ImageNet [65] pre-trained VGG-16 [66] as the
backbone network. By default, the σ0 and σ1 in RoI jitter
is set to 0.05, which can be adjusted to tackle a wider or
narrower position shift. We horizontally ﬂip all the images for
data augmentation. We set the initial learning rate to 0.005 for
2 epochs and 0.0005 for 1 more epoch. For the drone-based

In this paper, we set the shift of 50% performance degradation
(Rd = 0.5 ) as the weak aligned bound. For instance, if
shifting 10 and −15 pixels along the x-axis leads to 50%
performance degradation, then the two upper bounds Bu1 and
Bu2 on this direction are 10 and −15. In experiments, we
calculate this bound based on the Halfway Fusion [43] model.

9

(a) Day, MR

(b) Night, MR

(c) All-day, MR

(d) Day, MRC

(e) Night, MRC

(f) All-day, MRC

(g) Day, MRT

(h) Night, MRT

(i) All-day, MRT

Fig. 10. Comparisons of the miss rate curves reported on the KAIST dataset. The performance is evaluated by the MR protocol, and also MRC and MRT
using the KAIST-Paired annotation.

Method

Halfway Fusion [43]
Fusion RPN [44]
Adapted Halfway Fusion
CIAN [40]
MSDS-RCNN [64]

AR-CNN

RFA RoIJ CAF

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

+ ASC

O
25.51
21.43
15.59
14.68
12.51

12.94
10.90
9.87
8.26
8.01

S0◦
µ
35.72
31.57
25.02
24.50
22.19

22.27
12.20
11.74
9.41
9.17

S45◦

S90◦

S135◦

σ
8.14
8.34
8.29
8.51
8.57

8.21
2.92
1.29
0.98
0.93

µ
35.19
30.42
25.85
22.97
22.32

14.76
11.85
11.01
9.52
8.92

σ
8.68
9.97
10.26
10.70
10.93

8.92
2.08
1.50
1.09
0.95

µ
34.07
29.96
22.85
20.01
20.10

19.34
16.87
15.92
9.53
9.66

σ
7.95
7.02
7.53
6.86
6.93

6.85
2.01
1.51
1.02
1.05

µ
34.59
33.14
26.37
23.03
23.36

15.62
11.07
10.65
9.46
8.99

σ
8.87
9.85
10.14
10.29
10.01

6.24
1.96
1.12
0.90
0.90

TABLE III
DETECTION PERFORMANCES AND ROBUSTNESS TO POSITION SHIFT ON THE KAIST DATASET. O REFERS TO MRT SCORES AT THE ORIGIN, µ AND σ
DENOTE THE MEAN AND STANDARD DEVIATION OF MRT . WE USE THE RE-IMPLEMENTED MODEL OF LITERATURE [43], [44], AND THE PROVIDED
MODEL OF LITERATURE [40], [64].

10-310-210-1100101False positives per image0.10.20.30.40.50.640.81miss ratelog-average miss rate9.79% AR-CNN (Ours)10.60% MSDS-RCNN14.77% CIAN14.67% IATDNN+IAMSS14.55% IAF-RCNN16.49% Fusion RPN+BF19.55% Fusion RPN24.88% Halfway Fusion29.59% ACF+T+THOG(optimized)10-310-210-1100101False positives per image0.10.20.30.40.50.640.81miss ratelog-average miss rate8.07% AR-CNN (Ours)13.73% MSDS-RCNN11.13% CIAN15.72% IATDNN+IAMSS18.26% IAF-RCNN15.15% Fusion RPN+BF22.12% Fusion RPN26.59% Halfway Fusion34.98% ACF+T+THOG(optimized)10-310-210-1100101False positives per image0.10.20.30.40.50.640.81miss ratelog-average miss rate9.03% AR-CNN (Ours)11.63% MSDS-RCNN14.12% CIAN14.95% IATDNN+IAMSS15.73% IAF-RCNN15.91% Fusion RPN+BF20.67% Fusion RPN25.75% Halfway Fusion31.34% ACF+T+THOG(optimized)10-310-210-1100101False positives per image0.10.20.30.40.50.640.81miss ratelog-average miss rate8.09% AR-CNN (Ours)9.91% MSDS-RCNN15.13% CIAN14.82% IATDNN+IAMSS14.95% IAF-RCNN16.60% Fusion RPN+BF19.69% Fusion RPN24.29% Halfway Fusion29.85% ACF+T+THOG(optimized)10-310-210-1100101False positives per image0.10.20.30.40.50.640.81miss ratelog-average miss rate9.19% AR-CNN (Ours)14.21% MSDS-RCNN12.43% CIAN15.87% IATDNN+IAMSS18.11% IAF-RCNN15.28% Fusion RPN+BF21.83% Fusion RPN26.12% Halfway Fusion36.77% ACF+T+THOG(optimized)10-310-210-1100101False positives per image0.10.20.30.40.50.640.81miss ratelog-average miss rate8.52% AR-CNN (Ours)11.28% MSDS-RCNN14.64% CIAN15.14% IATDNN+IAMSS15.65% IAF-RCNN15.98% Fusion RPN+BF20.52% Fusion RPN25.10% Halfway Fusion32.01% ACF+T+THOG(optimized)10-310-210-1100101False positives per image0.10.20.30.40.50.640.81miss ratelog-average miss rate8.88% AR-CNN (Ours)12.02% MSDS-RCNN16.21% CIAN15.02% IATDNN+IAMSS15.22% IAF-RCNN17.56% Fusion RPN+BF21.08% Fusion RPN25.20% Halfway Fusion30.40% ACF+T+THOG(optimized)10-310-210-1100101False positives per image0.10.20.30.40.50.640.81miss ratelog-average miss rate6.67% AR-CNN (Ours)13.01% MSDS-RCNN9.88% CIAN15.20% IATDNN+IAMSS17.56% IAF-RCNN14.48% Fusion RPN+BF20.88% Fusion RPN24.90% Halfway Fusion34.81% ACF+T+THOG(optimized)10-310-210-1100101False positives per image0.10.20.30.40.50.640.81miss ratelog-average miss rate8.01% AR-CNN (Ours)12.51% MSDS-RCNN14.68% CIAN15.08% IATDNN+IAMSS16.00% IAF-RCNN16.52% Fusion RPN+BF21.43% Fusion RPN25.51% Halfway Fusion31.90% ACF+T+THOG(optimized)10

Method

Halfway Fusion [43]
Fusion RPN [44]
Adapted Halfway Fusion
CIAN [40]
MSDS-RCNN [64]
Ours [23]
+ ASC

O

25.10
20.52
15.06
14.64
11.28
8.86
8.52

S0◦
µ

34.20
30.22
24.39
23.86
20.06
12.51
11.79

S45◦

S90◦

S135◦

σ

7.98
9.07
8.19
8.74
7.95
1.60
1.28

µ

34.80
29.15
25.91
24.87
20.81
11.52
10.58

σ

7.87
9.23
11.32
11.08
9.54
2.07
1.90

µ

32.79
25.12
21.34
19.16
16.61
11.27
11.31

σ

7.36
6.95
7.25
6.22
6.36
1.41
1.47

µ

33.85
29.97
26.58
25.20
21.73
11.83
11.03

σ

8.16
9.80
11.52
10.46
10.14
2.22
2.16

TABLE IV
THE DETECTION ROBUSTNESS TO thermal POSITION SHIFT (i.e. WE FIX THE COLOR IMAGE WHILE SHIFTING THE THERMAL IMAGE) ON THE KAIST
DATASET. MRC IS USED TO EVALUATE THE DETECTION PERFORMANCE.

σ0 and σ1
0.01
0.02
0.05
0.1
0.2

Day
9.97
9.90
9.79
9.96
10.07

Night All-day
9.29
8.13
8.01
9.10
9.03
8.07
9.21
8.17
9.36
8.22

TABLE V
OBJECT DETECTION RESULTS ON DIFFERENT HYPER-PARAMETER σ0 AND
σ1.

First, the thermal modality is set as the reference since it
usually provides consistent images throughout the day. We
use MRT to evaluate the detector. Following the settings in
Section III-B, the visual results in a surface plot are depicted
in Figure 5(b). As illustrated in Figure 5, compared to the
baseline method, the proposed detector signiﬁcantly enhances
the robustness to position shift, and the performance at the
origin is also improved. Besides, we design four metrics to
quantitatively evaluate the robustness, i.e. S0◦
, S90◦
,
S135◦
, where the 0◦-135◦ indicate the shift directions on
the image plane. We have 10 shift modes for each shift
direction, which are selected between the origin and weakly
aligned bound via equally spacing and rounding. Table III
shows the mean (µ) and standard deviation (σ) of those 10
results, our method achieves the best µ and smallest σ on
four metrics, demonstrating the robustness to diverse position
shift conditions.

, S45◦

Experiments on the color reference. Then, we ﬁx the color
image as the reference modality. Table IV shows that the pro-
posed method still achieves the best performance and smallest
standard deviation, further validating the effectiveness of the
AR-CNN. Additionally, compared to the thermal reference,
the color reference conﬁguration performs at a lower level in
experiments. This validates our intuition: the modality with
stable imagery is more suitable to serve as the reference one.

E. Ablation Study

To analyse our model in more detail, we conduct ablation

studies on the KAIST dataset.

Region Feature Alignment Table III shows detection re-
sults with and without the RFA module. It can be observed that
the miss rate and the σ under various position shift conditions
are remarkably reduced by the RFA module. Speciﬁcally, for
S45◦
, the RFA reduces the σ by a signiﬁcant 6.84 (i.e. from
8.92 to 2.08). For other three metrics, consistent reductions
are also observed. Moreover, some visualizations of proposals
and detection results are shown in Figure 11. For pedestrians
with the position shift problem, proposals of the sensed (color)
modality are adjusted to an aligned position. This phenomenon
demonstrates that the RFA module can predict the region-
wise position shift of two modalities and adaptively adjust the
sensed proposals, thus enabling the modality-aligned feature
fusion process for better classiﬁcation and localization.

RoI Jitter Strategy Then we demonstrate the contribution
of the RoI jitter. Table III shows that the strategy further
improves the detector’s robustness. Speciﬁcally, for S0◦
, the
µ is reduced from 12.20 to 11.74, and σ is decreased from
2.92 to 1.29. Meanwhile, it can be observed that this strategy
is more helpful for the σ than the miss rate at the origin,
which demonstrates that the main contribution of this strategy
is boosting the robustness to the position shift. Table V further
shows the detection results with varying hyper-parameter σ0
and σ1. When σ0 and σ1 are set to 0.05, we can obtain the best
overall score. When varying the hyper-parameter, the proposed
method exhibits stable performances which are better than the
baseline.

Conﬁdence-Aware Fusion We also compare performances
with and without the CAF module to validate the effectiveness
of our multi-modal fusion scheme. Table III shows that this
module signiﬁcantly reduces MRT at
the origin by 1.61,
yet slightly suppresses the σ. This illustrates that the main
contribution of the CAF module is improving the detection
performance, since it conducts adaptive fusion by paying more
attention to reliable features.

Besides, we also visualize some failed cases in Figure 12 to
bring insights for the error analysis and further improvement
of the detector. Generally, the far-scale tiny people are easier
to be missed since it is difﬁcult to obtain sufﬁcient features
after the CNN pooling. We also notice that the detection
is complex,
performance will degrade if the environment

11

Fig. 11. Qualitative results of the proposed approach. The ﬁrst row illustrates the reference proposals whose conﬁdence score (in the range [0, 1.0]) is greater
than 0.6, and the second row shows the corresponding sensed proposals. To demonstrate the effectiveness of the Region Feature Alignment module, some
proposal instances are shown in the third row: orange dotted boxes denote the reference proposals, which are good ones in the reference image but suffer the
position shift problem in the sensed modality; red bounding boxes refer to the adjusted sensed proposals after the region feature alignment process. In the
last two rows, green bounding boxes show the ﬁnal detection results whose conﬁdence score is greater than 0.6.

Fig. 12. Visualization of the failed cases on the KAIST test set. Red boxes represent the ground truth, yellow boxes denote ignored objects and green boxes
refer to detected objects whose conﬁdence scores are greater than 0.3.

e.g. with a cluttered background and a cluster of objects,
due to disturbed or incomplete features. In the future, we
would like to work on the occluded person by enhancing
the classiﬁcation and localization heads to further improve the
detection performance.

VI. EXPERIMENTS ON CASE 2: RGB-D OBJECT
DETECTION

In this section, we conduct experiments of 2D object detec-
tion on the SL-RGBD dataset and 3D object detection on the
NYUv2 [68] dataset.

12

Fig. 13. Data instances and detection results in the SL-RGBD dataset. The ﬁrst row shows RGB images and the second row depicts corresponding depth
images that suffer from different degrees of position shift problem.

Method

Amodal3Det (RGB only) [11]
DSS [67]
3D-SSD [48]
Rahman et al. [49]
Amodal3Det [11]
Halfway Fusion [43]
AR-CNN (ours)

O
30.0
36.3
39.7
43.1
40.9
41.0
41.2

S0◦
µ
-
-
-
30.2
29.2
29.0
33.5

S45◦

S90◦

σ
-
-
-
7.3
7.0
6.7
3.8

µ
-
-
-
31.3
30.7
30.1
34.3

σ
-
-
-
7.1
7.0
7.1
4.0

µ
-
-
-
31.5
30.1
29.8
34.2

σ
-
-
-
6.9
6.9
7.0
3.9

S135◦
µ
-
-
-
31.0
28.7
29.8
34.6

σ
-
-
-
7.1
7.5
7.2
4.0

TABLE VI
DETECTION PERFORMANCES AND ROBUSTNESS TO POSITION SHIFT ON THE NYUV2 DATASET. WE USE MAP FOR THE EVALUATION. WE
RE-IMPLEMENTED THE HALFWAY FUSION [43] AND AMODAL3DET [11] MODELS FOR COMPARISON.

A. Dataset

NYUv2 The NYU Depth V2 dataset [68] is a popular
yet challenging dataset for indoor scene understanding, which
contains 1,449 RGB-D pairs with 19 indoor object classes. To
achieve 3D object detection, [69] extended the NYUv2 dataset
with extra 3D bounding boxes, and reﬁne the depth map by
integrating multiple frames of raw video data. In [11], the
3D annotations are further reﬁned by addressing issues about
inconsistency and inaccuracy.

SL-RGBD The SL-RGBD dataset is built for indoor recog-
nition, which contains 214 RGB-D image pairs with 1, 297
object annotations, examples are shown in Figure 13. The ob-
ject classes are selected from a subset of the YCB benchmark,
consisting of lemon, banana, strawberries, orange, scissors,
plastic bolt and nut, keys. Various patterns of position shift are
included to show the characteristics of weakly aligned RGB-D
pairs. The details of the collection system are in Section III-A.

B. Implementation Details

The hyperparameter is set the same as in RGB-T pedestrian
detection. We use the ImageNet pre-trained model and ﬁne-
tune it on the NYUv2 and SL-RGBD datasets, respectively.
We use the same training schedule like that in the all-day
RGB-T pedestrian detection.

We use mAP (mean Average Precision) to evaluate the
performance of detectors. For 2D and 3D object detection, the
detected box is treated as true positive if the IoU is greater
than 0.5 and 0.25, respectively. Following previous works [11],
[48], [49], we use the improved 3D annotation provided by
[11] for training and testing.

C. Performance and Robutstness

2D Object Detection We conduct the 2D object detection
experiments on our SL-RGBD dataset. As shown in Table VII,
incorporating the depth modality signiﬁcantly improves the
detection performance (58.7 v.s. 52.9 mAP), and our AR-CNN
model achieves the best 66.3 mAP results since the position
shift problem is taken into consideration.

3D Object Detection To further validate the effectiveness
of our model, we test the performance and robustness of AR-
CNN on the RGB-D based 3D object detection task. Similar
to that for 2D pedestrians, we alternatively ﬁx the color and
depth image, then manually shift the other. Table VI shows
the results on the standard NYUv2 dataset, which uses color
images as the reference modality. Our AR-CNN achieves
comparative performance and best robustness (33.5 v.s. 30.2
mean performance and 3.8 v.s. 6.7 standard variance in S0◦
),
which demonstrates the validness and generalization ability of
the proposed method.

Input

RGB

RGB+D

Method
SSD [20]
Faster R-CNN [19]
Halfway Fusion [43]
CIAN [40]
AR-CNN (Ours)

mAP (%)
51.6
52.9
58.7
59.3
66.3

TABLE VII
OBJECT DETECTION RESULTS ON THE SL-RGBD DATASET. WE USE MAP
TO EVALUATE THE PERFORMANCE OF DETECTORS.

In Figure 14, we use some examples to further show the
model’s robustness to position shift problem. When the system
suffers from the position shift problem, as shown in the third
row, our model achieves better performances than its Halfway
Fusion counterpart. For example, the performances of our
model and Halfway fusion are 32.1 v.s. 23.3 mAP when the
shift pattern is (50, −50) on the image plane.

VII. CONCLUSION

To handle the practical position shift problem in multi-
modal object detection, we propose a novel framework to
improve the robustness of the detector when using weakly
aligned image pairs. First, we design a novel region feature
alignment module to predict the shift and aligns the multi-
modal features. Second, to further enrich the shift patterns,
we propose an RoI-level augmentation strategy named RoI
jitter. For the all-day RGB-T pedestrian detection, we present
new multi-modal labelling named KAIST-Paired and propose
the conﬁdence-aware fusion method to pay attention to the
more reliable modality. The proposed method achieves the best
performance and robustness on CVC-14 and KAIST datasets.
For the RGB-D based 2D and 3D detection, we extend our
method by aligning the features of 2D proposals and then
conducting a more reliable 3D bounding boxes regression.
When compared to the state-of-the-art, the proposed method
achieves comparative performance and better robustness on
NYUv2 and SL-RGBD datasets. In the real-world scenario,
the weakly aligned characteristic usually exists when using the
multi-modal system. The proposed method provides a generic
solution for multi-modal object detection, especially when it
faces the challenges of the position shift problem.

REFERENCES

[1] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft COCO: Common objects in
context,” in Proceedings of the European conference on Computer Vision
(ECCV), 2014, pp. 740–755.

[2] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu,
M. Pelillo, and L. Zhang, “Dota: A large-scale dataset for object
detection in aerial images,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018, pp. 3974–3983.
[3] Y. Pang, J. Cao, Y. Li, J. Xie, H. Sun, and J. Gong, “Tju-dhd: A
diverse high-resolution dataset for object detection,” IEEE Transactions
on Image Processing, vol. 30, pp. 207–219, 2020.

[4] S. Hwang, J. Park, N. Kim, Y. Choi, and I. S. Kweon, “Multispectral
pedestrian detection: Benchmark dataset and baseline,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2015, pp. 1037–1045.

13

[5] A. Gonz´alez, Z. Fang, Y. Socarras, J. Serrat, D. V´azquez, J. Xu, and
A. M. L´opez, “Pedestrian detection at day/night time with visible and
ﬁr cameras: A comparison,” Sensors, vol. 16, no. 6, p. 820, 2016.
[6] Y. Yuan, X. Lu, and X. Chen, “Multi-spectral pedestrian detection,”

Signal Processing, vol. 110, pp. 94–100, 2015.

[7] B. F. Klare and A. K. Jain, “Heterogeneous face recognition using
kernel prototype similarities,” IEEE transactions on pattern analysis and
machine intelligence, vol. 35, no. 6, pp. 1410–1422, 2012.

[8] M. Shao and Y. Fu, “Cross-modality feature learning through generic
hierarchical hyperlingual-words,” IEEE transactions on neural networks
and learning systems, vol. 28, no. 2, pp. 451–463, 2016.

[9] S. Gupta, R. Girshick, P. Arbel´aez, and J. Malik, “Learning rich
features from rgb-d images for object detection and segmentation,” in
Proceedings of the European conference on Computer Vision (ECCV),
2014, pp. 345–360.

[10] A. Eitel, J. T. Springenberg, L. Spinello, M. Riedmiller, and W. Burgard,
“Multimodal deep learning for robust rgb-d object recognition,” in 2015
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS).

IEEE, 2015, pp. 681–687.

[11] Z. Deng and L. J. Latecki, “Amodal detection of 3d objects: Inferring
3d bounding boxes from 2d ones in rgb-depth images,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2017, pp. 5762–5770.

[12] M. Schwarz, H. Schulz, and S. Behnke, “Rgb-d object recognition
and pose estimation based on pre-trained convolutional neural network
features,” in 2015 IEEE international conference on robotics and au-
tomation (ICRA).

IEEE, 2015, pp. 1329–1335.
[13] C. Choi and H. I. Christensen, “3d pose estimation of daily objects
using an rgb-d camera,” in 2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems.

IEEE, 2012, pp. 3342–3349.

[14] D. Feng, C. Haase-Schuetz, L. Rosenbaum, H. Hertlein, F. Duffhauss,
C. Glaeser, W. Wiesbeck, and K. Dietmayer, “Deep multi-modal object
detection and semantic segmentation for autonomous driving: Datasets,
methods, and challenges,” arXiv preprint arXiv:1902.07830, 2019.
[15] S. T. Barnard and W. B. Thompson, “Disparity analysis of images,”
IEEE Transactions on Pattern Analysis and Machine Intelligence, no. 4,
pp. 333–340, 1980.

[16] N. Kim, Y. Choi, S. Hwang, K. Park, J. S. Yoon, and I. S. Kweon,
“Geometrical calibration of multispectral calibration,” in 2015 12th In-
ternational Conference on Ubiquitous Robots and Ambient Intelligence
(URAI).

IEEE, 2015, pp. 384–385.

[17] W. Treible, P. Saponaro, S. Sorensen, A. Kolagunda, M. O’Neal,
B. Phelan, K. Sherbondy, and C. Kambhamettu, “Cats: A color and
thermal stereo benchmark,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2961–
2969.

[18] Y. Choi, N. Kim, S. Hwang, K. Park, J. S. Yoon, K. An, and I. S.
Kweon, “Kaist multi-spectral day/night data set for autonomous and as-
sisted driving,” IEEE Transactions on Intelligent Transportation Systems,
vol. 19, no. 3, pp. 934–948, 2018.

[19] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-
time object detection with region proposal networks,” in Advances in
Neural Information Processing Systems (NeurIPS), 2015, pp. 91–99.

[20] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and
A. C. Berg, “SSD:single shot multibox detector,” in Proceedings of the
European conference on Computer Vision (ECCV), 2016, pp. 21–37.

[21] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep
learning: A review,” IEEE transactions on neural networks and learning
systems, vol. 30, no. 11, pp. 3212–3232, 2019.

[22] Y. Li, Y. Pang, J. Cao, J. Shen, and L. Shao, “Improving single shot
object detection with feature scale unmixing,” IEEE Transactions on
Image Processing, vol. 30, pp. 2708–2721, 2021.

[23] L. Zhang, X. Zhu, X. Chen, X. Yang, Z. Lei, and Z. Liu, “Weakly
aligned cross-modal learning for multispectral pedestrian detection,” in
The IEEE International Conference on Computer Vision (ICCV), 2019,
pp. 5127–5137.

[24] P. Doll´ar, Z. Tu, P. Perona, and S. Belongie, “Integral channel features,”
in British Machine Vision Conference (BMVC), 2009, pp. 91.1–91.11.
[25] P. Doll´ar, R. Appel, S. Belongie, and P. Perona, “Fast feature pyramids
for object detection,” IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (PAMI), vol. 36, no. 8, pp. 1532–1545, 2014.
[26] W. Nam, P. Doll´ar, and J. H. Han, “Local decorrelation for improved
pedestrian detection,” in Advances in Neural Information Processing
Systems (NeurIPS), 2014, pp. 424–432.

[27] S. Zhang, R. Benenson, and B. Schiele, “Filtered channel features
for pedestrian detection,” in Proceedings of the IEEE Conference on

14

Fig. 14. Visualization of results in weakly aligned image pairs. The ﬁrst and second rows are aligned RGB-D image pairs in which the depth images are
reﬁned by integrating multiple frames from the raw video. The third row shows different patterns of the position shift problem. The last three rows depict 3D
bounding boxes from ground truth, Halfway Fusion, and AR-CNN, respectively. We show the boxes whose conﬁdence score (in the range [0, 1.0]) is greater
than 0.6. For better visualization, the point cloud is generated by aligned RGB and depth images.

Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1751–
1760.

[28] J. Mao, T. Xiao, Y. Jiang, and Z. Cao, “What can help pedestrian
detection?” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017, pp. 3127–3136.

[29] J. Hosang, M. Omran, R. Benenson, and B. Schiele, “Taking a deeper
look at pedestrians,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2015, pp. 4073–4082.

[30] X. Wang, T. Xiao, Y. Jiang, S. Shao, J. Sun, and C. Shen, “Repulsion
loss: Detecting pedestrians in a crowd,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2018,
pp. 7774–7783.

[31] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, “Occlusion-aware r-
cnn: detecting pedestrians in a crowd,” in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp. 637–653.

[32] J. Xie, Y. Pang, H. Cholakkal, R. Anwer, F. Khan, and L. Shao, “Psc-net:
learning part spatial co-occurrence for occluded pedestrian detection,”
Science China Information Sciences, vol. 64, no. 2, pp. 1–13, 2021.

[33] N. Kim, Y. Choi, S. Hwang, and I. S. Kweon, “Multispectral transfer

network: Unsupervised depth estimation for all-day vision,” in Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence, vol. 32, no. 1,
2018, pp. 6983–6991.

[34] Y. Choi, N. Kim, S. Hwang, and I. S. Kweon, “Thermal

image
enhancement using convolutional neural network,” in 2016 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
2016, pp. 223–230.

[35] Y. K. Choi, N. I. Kim, K. B. Park, S. M. Hwang, J. S. Yoon, and
I. S. Kweon, “All-day visual place recognition: Benchmark dataset and
baselines,” in CVPR2015 IEEE Conference on Computer Vision and
Pattern Recognition.
IEEE Computer Society and the Computer Vision
Foundation (CVF), 2015.

[36] K. Zhou, L. Chen, and X. Cao, “Improving multispectral pedestrian
detection by addressing modality imbalance problems,” in Proceedings
of the European conference on Computer Vision (ECCV), 2020, pp. 1–
19.

[37] A. Wolpert, M. Teutsch, M. S. Sarfraz, and R. Stiefelhagen, “Anchor-
free small-scale multispectral pedestrian detection,” in British Machine
Vision Conference (BMVC), 2020, pp. 1–14.

RGBdepthdepth(position shifted)ground truthHalfwayFusionOurschairscounterdoorsinktablelampbedpillow[38] H. Zhang, E. Fromont, S. Lef`evre, and B. Avignon, “Multispectral fusion
for object detection with cyclic fuse-and-reﬁne blocks,” in 2020 IEEE
International Conference on Image Processing (ICIP).
IEEE, 2020, pp.
276–280.

[39] J. Wagner, V. Fischer, M. Herman, and S. Behnke, “Multispectral
pedestrian detection using deep fusion convolutional neural networks,” in
24th European Symposium on Artiﬁcial Neural Networks, Computational
Intelligence and Machine Learning (ESANN), 2016, pp. 509–514.
[40] L. Zhang, Z. Liu, S. Zhang, X. Yang, H. Qiao, K. Huang, and A. Hus-
sain, “Cross-modality interactive attention network for multispectral
pedestrian detection,” Information Fusion, vol. 50, pp. 20–29, 2019.

[41] D. Guan, Y. Cao, J. Yang, Y. Cao, and C.-L. Tisse, “Exploiting fusion
architectures for multispectral pedestrian detection and segmentation,”
Applied Optics, vol. 57, no. 18, pp. D108–D116, 2018.

[42] C. Li, D. Song, R. Tong, and M. Tang, “Illumination-aware faster r-
cnn for robust multispectral pedestrian detection,” Pattern Recognition,
vol. 85, pp. 161–171, 2019.

[43] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, “Multispectral deep neural
networks for pedestrian detection,” in British Machine Vision Conference
(BMVC), 2016, pp. 73.1–73.13.

[44] D. K¨onig, M. Adam, C. Jarvers, G. Layher, H. Neumann, and
M. Teutsch, “Fully convolutional region proposal networks for multi-
spectral person detection,” in IEEE Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW), 2017, pp. 243–250.

[45] D. Xu, W. Ouyang, E. Ricci, X. Wang, and N. Sebe, “Learning
cross-modal deep representations for robust pedestrian detection,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017, pp. 5363–5371.

[46] L. Spinello and K. O. Arras, “People detection in rgb-d data,” in 2011
IEEE/RSJ International Conference on Intelligent Robots and Systems.
IEEE, 2011, pp. 3838–3843.

[47] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2014, pp. 580–587.

[48] Q. Luo, H. Ma, L. Tang, Y. Wang, and R. Xiong, “3d-ssd: Learning
hierarchical features from rgb-d images for amodal 3d object detection,”
Neurocomputing, vol. 378, pp. 364–374, 2020.

[49] M. M. Rahman, Y. Tan, J. Xue, L. Shao, and K. Lu, “3d object detection:
Learning 3d bounding boxes from scaled down 2d bounding boxes in
rgb-d images,” Information Sciences, vol. 476, pp. 147–158, 2019.
[50] S. Vidas, R. Lakemond, S. Denman, C. Fookes, S. Sridharan, and
T. Wark, “A mask-based approach for the geometric calibration of
thermal-infrared cameras,” IEEE Transactions on Instrumentation and
Measurement, vol. 61, no. 6, pp. 1625–1635, 2012.

[51] B. Zitova and J. Flusser, “Image registration methods: a survey,” Image

and Vision Computing, vol. 21, no. 11, pp. 977–1000, 2003.

[52] L. G. Brown, “A survey of image registration techniques,” ACM com-

puting surveys (CSUR), vol. 24, no. 4, pp. 325–376, 1992.

[53] S. Dawn, V. Saxena, and B. Sharma, “Remote sensing image registration
techniques: A survey,” in International Conference on Image and Signal
Processing, 2010, pp. 103–112.

[54] Z. Song, R. Chung, and X.-T. Zhang, “An accurate and robust strip-
edge-based structured light means for shiny surface micromeasurement
in 3-d,” IEEE Transactions on Industrial Electronics, vol. 60, no. 3, pp.
1023–1032, 2012.

[55] S. Zhang, R. Benenson, and B. Schiele, “Citypersons: A diverse dataset
for pedestrian detection,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2017, pp. 3213–
3221.

[56] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in
Proceedings of the IEEE International Conference on Computer Vision
(ICCV), 2017, pp. 2980–2988.

[57] R. Girshick, “Fast R-CNN,” in Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2015, pp. 1440–1448.
[58] K. Park, S. Kim, and K. Sohn, “Uniﬁed multi-spectral pedestrian
detection based on probabilistic fusion networks,” Pattern Recognition,
vol. 80, pp. 143–155, 2018.

[59] H. Choi, S. Kim, K. Park, and K. Sohn, “Multi-spectral pedestrian
detection based on accumulated object proposal with fully convolutional
networks,” in 2016 23rd IEEE International Conference on Pattern
Recognition (ICPR), 2016, pp. 621–626.

[60] J. Fredebon, “The role of instructions and familiar size in absolute
judgments of size and distance,” Perception & Psychophysics, vol. 51,
no. 4, pp. 344–354, 1992.

15

[61] A. Kar, S. Tulsiani, J. Carreira, and J. Malik, “Amodal completion
and size constancy in natural scenes,” in Proceedings of the IEEE
International Conference on Computer Vision, 2015, pp. 127–135.
[62] D. Guan, Y. Cao, J. Liang, Y. Cao, and M. Y. Yang, “Fusion of
multispectral data through illumination-aware deep neural networks for
pedestrian detection,” Information Fusion, vol. 50, pp. 148–157, 2019.
[63] Y. Cao, D. Guan, Y. Wu, J. Yang, Y. Cao, and M. Y. Yang, “Box-level
segmentation supervised deep neural networks for accurate and real-time
multispectral pedestrian detection,” ISPRS Journal of Photogrammetry
and Remote Sensing, vol. 150, pp. 70–79, 2019.

[64] C. Li, D. Song, R. Tong, and M. Tang, “Multispectral pedestrian detec-
tion via simultaneous detection and segmentation,” in British Machine
Vision Conference (BMVC), September 2018, pp. 1–12.

[65] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large
scale visual recognition challenge,” International Journal of Computer
Vision, vol. 115, no. 3, pp. 211–252, 2015.

[66] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[67] S. Song and J. Xiao, “Deep sliding shapes for amodal 3d object detection
in rgb-d images,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016, pp. 808–816.

[68] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
and support inference from rgbd images,” in European Conference on
Computer Vision. Springer, 2012, pp. 746–760.

[69] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene
understanding benchmark suite,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2015, pp. 567–576.

Lu Zhang received the B.E. degree in automation from Shandong Uni-
versity, in 2016, and the Ph.D. degree from the Institute of Automation,
Chinese Academy of Sciences, in 2021, where she is currently an Assistant
Professor. Her research interests include computer vision, pattern recognition,
and robotics.

in 1997,

Zhiyong Liu received the B.E. degree in electronic engineering from
Tianjin University, Tianjin, China,
the M.E. degree in control
engineering from the Institute of Automation, Chinese Academy of Sciences
(CASIA), Beijing, China, in 2000, and the Ph.D. degree in computer science
from the Chinese University of Hong Kong, Hong Kong, in 2003. He is
a Professor with the Institute of Automation, CASIA, Beijing. His current
research interests include machine learning, pattern recognition, computer
vision, and bio-informatics.

Xiangyu Zhu received the B.E. degree in Sichuan University (SCU),
in 2012, and the Ph.D. degree from the National Laboratory of Pattern
Recognition in the Institute of Automation, Chinese Academic of Sciences,
in 2017, where he is currently an Associate Professor. His research interests
include pattern recognition and computer vision, in particular, 3D morphable
model and face analysis.

Zhan Song received the Ph.D. degree in mechanical and automation
engineering from the Chinese University of Hong Kong in 2008. He is
currently with the Shenzhen Institutes of Advanced Technology (SIAT),
Chinese Academy of Sciences (CAS), as a Professor. His current research
interests include structured light-based sensing, image processing, 3D face
recognition, and human-computer interaction.

Xu Yang received the B.E. degree in electronic engineering from the Ocean
University of China, Qingdao, China, in 2009, and the Ph.D. degree from the
Institute of Automation, Chinese Academy of Sciences (CASIA), in 2014,
where he is currently an Associate Professor. His current research interests
include computer vision, graph algorithms, and robotics.

Zhen Lei received the B.S. degree in automation from the University of
Science and Technology of China, in 2005, and the Ph.D. degree from the
Institute of Automation, Chinese Academy of Sciences, in 2010, where he
is currently a Professor. He has published over 100 papers in international
journals and conferences. His research interests are in computer vision, pattern
recognition, image processing, and face recognition in particular.

Hong Qiao received the B.S. and M.S. degrees in engineering from Xi’an
Jiaotong University, Xi’an, China, in 1986 and 1989, respectively, the M.Phil.
degree from the University of Strathclyde, Glasgow, U.K. in 1997, and the
Ph.D. degree in robotics control from De Montfort University, Leicester,
U.K., in 1995. She held teaching and research positions with Universities
in the U.K. and Hong Kong, from 1990 to 2004. She is currently a “100-
Talents Project” Professor with Institute of Automation, Chinese Academy
of Sciences, Beijing, China. Her current research interests include robotic
manipulation, robotic vision, bio-inspired intelligent robot, and brain-like
intelligence.

