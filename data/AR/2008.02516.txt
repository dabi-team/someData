1
2
0
2

r
a

M
5
1

]
S
A
.
s
s
e
e
[

4
v
6
1
5
2
0
.
8
0
0
2
:
v
i
X
r
a

FastLR: Non-Autoregressive Lipreading Model
with Integrate-and-Fire

Jinglin Liuâˆ—
Zhejiang University
jinglinliu@zju.edu.cn

Chen Zhang
Zhejiang University
zc99@zju.edu.cn

Yi Renâˆ—
Zhejiang University
rayeren@zju.edu.cn

Zhou Zhaoâ€ 
Zhejiang University
zhaozhou@zju.edu.cn

Baoxing Huai
HUAWEI TECHNOLOGIES CO., LTD.
huaibaoxing@huawei.com

Nicholas Jing Yuan
Huawei Cloud BU
nicholas.yuan@huawei.com

ABSTRACT
Lipreading is an impressive technique and there has been a defi-
nite improvement of accuracy in recent years. However, existing
methods for lipreading mainly build on autoregressive (AR) model,
which generate target tokens one by one and suffer from high infer-
ence latency. To breakthrough this constraint, we propose FastLR,
a non-autoregressive (NAR) lipreading model which generates all
target tokens simultaneously. NAR lipreading is a challenging task
that has many difficulties: 1) the discrepancy of sequence lengths
between source and target makes it difficult to estimate the length
of the output sequence; 2) the conditionally independent behavior
of NAR generation lacks the correlation across time which leads
to a poor approximation of target distribution; 3) the feature rep-
resentation ability of encoder can be weak due to lack of effective
alignment mechanism; and 4) the removal of AR language model
exacerbates the inherent ambiguity problem of lipreading. Thus, in
this paper, we introduce three methods to reduce the gap between
FastLR and AR model: 1) to address challenges 1 and 2, we leverage
integrate-and-fire (I&F) module to model the correspondence be-
tween source video frames and output text sequence. 2) To tackle
challenge 3, we add an auxiliary connectionist temporal classifica-
tion (CTC) decoder to the top of the encoder and optimize it with
extra CTC loss. We also add an auxiliary autoregressive decoder to
help the feature extraction of encoder. 3) To overcome challenge
4, we propose a novel Noisy Parallel Decoding (NPD) for I&F and
bring Byte-Pair Encoding (BPE) into lipreading. Our experiments
exhibit that FastLR achieves the speedup up to 10.97Ã— comparing
with state-of-the-art lipreading model with slight WER absolute
increase of 1.5% and 5.5% on GRID and LRS2 lipreading datasets
respectively, which demonstrates the effectiveness of our proposed
method.1

CCS CONCEPTS
â€¢ Computing methodologies â†’ Computer vision; Speech recog-
nition.

1âˆ— Equal contribution. â€  Corresponding author.

MM â€™20, October 12â€“16, 2020, Seattle, WA, USA
Â© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
This is the authorâ€™s version of the work. It is posted here for your personal use. Not
for redistribution. The definitive Version of Record was published in Proceedings of
the 28th ACM International Conference on Multimedia (MM â€™20), October 12â€“16, 2020,
Seattle, WA, USA, https://doi.org/10.1145/3394171.3413740.

KEYWORDS
Lip Reading; Non-autoregressive generation; Deep Learning

ACM Reference Format:
Jinglin Liu, Yi Ren, Zhou Zhao, Chen Zhang, Baoxing Huai, and Jing Yuan.
2020. FastLR: Non-Autoregressive Lipreading Model with Integrate-and-
Fire. In Proceedings of the 28th ACM International Conference on Multimedia
(MMâ€™20), October 12â€“16, 2020, Seattle, WA, USA. ACM, New York, NY, USA,
9 pages. https://doi.org/10.1145/3394171.3413740

1 INTRODUCTION
Lipreading aims to recognize sentences being spoken by a talk-
ing face, which is widely used now in many scenarios including
dictating instructions or messages in a noisy environment, tran-
scribing archival silent films, resolving multi-talker speech [1] and
understanding dialogue from surveillance videos. However, it is
widely considered a challenging task and even experienced human
lipreaders cannot master it perfectly [3, 24]. Thanks to the rapid
development of deep learning in recent years, there has been a line
of works studying lipreading and salient achievements have been
made.

Existing state-of-the-art methods mainly adopt autoregressive
(AR) model, either based on RNN [33, 34], or Transformer [1, 2].
Those systems generate each target token conditioned on the se-
quence of tokens generated previously, which hinders the paralleliz-
ability. Thus, they all without exception suffer from high inference
latency, especially when dealing with the massive videos data con-
taining hundreds of hours (like long films and surveillance videos)
or real-time applications such as dictating messages in a noisy
environment.

To tackle the low parallelizability problem due to AR generation,
many non-autoregressive (NAR) models [13â€“17, 21, 31] have been
proposed in the machine translation field. The most typical one
is NAT-FT [13], which modifies the Transformer [29] by adding a
fertility module to predict the number of words in the target se-
quence aligned to each source word. Besides NAR translation, many
researchers bring NAR generation into other sequence-to-sequence
tasks, such as video caption [20, 22], speech recognition [5] and
speech synthesis[18, 22]. These works focus on generating the tar-
get sequence in parallel and mostly achieve more than an order of
magnitude lower inference latency than their corresponding AR
models.

However, it is very challenging to generate the whole target

sequence simultaneously in lipreading task in following aspects:

 
 
 
 
 
 
â€¢ The considerable discrepancy of sequence length between
the input video frames and the target text tokens makes it
difficult to estimate the length of the output sequence or to
define a proper decoder input during the inference stage.
This is different from machine translation model, which
can even simply adopt the way of uniformly mapping the
source word embedding as the decoder input [31] due to the
analogous text sequence length.

â€¢ The true target sequence distributions show a strong cor-
relation across time, but the NAR model usually generates
target tokens conditionally independent of each other. This
is a poor approximation and may generate repeated words.
Gu et al. [13] terms the problem as "multimodal-problem".
â€¢ The feature representation ability of encoder could be weak
when just training the raw NAR model due to lack of effective
alignment mechanism.

â€¢ The removal of the autoregressive decoder, which usually
acts as a language model, makes the model much more diffi-
cult to tackle the inherent ambiguity problem in lipreading.

In our work, we propose FastLR, a non-autoregressive lipreading
model based on Transformer. To handle the challenges mentioned
above and reduce the gap between FastLR and AR model, we intro-
duce three methods as follows:

â€¢ To estimate the length of the output sequence and allevi-
ates the problem of time correlation in target sequence, we
leverage integrate-and-fire (I&F) module to encoding the
continuous video signal into discrete token embeddings by
locating the acoustic boundary, which is inspired by Dong
and Xu [10]. These discrete embeddings retain the timing
information and correspond to the target tokens directly.
â€¢ To enhance the feature representation ability of encoder, we
add the connectionist temporal classification (CTC) decoder
on the top of encoder and optimize it with CTC loss, which
could force monotonic alignments. Besides, we add an aux-
iliary AR decoder during training to facilitate the feature
extraction ability of encoder.

â€¢ To tackle the inherent ambiguity problem and reduce the
spelling errors in NAR inference, we first propose a novel
Noisy Parallel Decoding (NPD) for I&F method. The rescor-
ing method in NPD takes advantages of the language model
in the well-trained AR lipreading teacher without harming
the parallelizability. Then we bring Byte-Pair Encoding (BPE)
into lipreading, which compresses the target sequence and
makes each token contain more language information to
reduce the dependency among tokens compared with char-
acter level encoding.

The core contribution of this work is that, we propose a non-
autoregressive lipreading system, and present several elaborate
methods metioned above to bridge the gap between FastLR and
state-of-the-art autoregressive lipreading models.

The experimental results show that FastLR achieves the speedup
up to 10.97Ã— comparing with state-of-the-art lipreading model with
slight WER increase of 1.5% and 5.5% on GRID and LRS2 lipreading
datasets respectively, which demonstrates the effectiveness of our
proposed method. We also conduct ablation experiments to verify
the significance of all proposed methods in FastLR.

2 RELATED WORKS
2.1 Deep Lipreading
Prior works utilize deep learning for lipreading. The first typical ap-
proach is LipNet [3] based on CTC [12], which takes the advantage
of the spatio-temporal convolutional front-end feature generator
and GRU [6]. Further, Stafylakis and Tzimiropoulos [25] propose a
network combining the modified 3D/2D-ResNet architecture with
LSTM. Afouras et al. [1] introduce the Transformer self-attention
architecture into lipreading, and build TM-seq2seq and TM-CTC.
The former surpasses the performance of all previous work on
LRS2-BBC dataset by a large margin. To boost the performance
of lipreading, Petridis et al. [19] present a hybrid CTC/Attention
architecture aiming to obtain the better alignment than attention-
only mechanism, Zhao et al. [34] provide the idea that transfer-
ring knowledge from audio-speech recognition model to lipreading
model by distillation.

However, state-of-the-art methods, either based on recurrent
neural network [33, 34] or Transformer [1, 2], take in the input
video sequence and generates the tokens of target sentence ğ‘¦ in
a recurrent structure during the inference process. And they all
suffer from the high latency.

2.2 Non-Autoregressive Decoding
An autoregressive model takes in a source sequence ğ‘¥ = (ğ‘¥1, ğ‘¥2, ...,
ğ‘¥ğ‘‡ğ‘¥ ) and then generates words in target sentence ğ‘¦ = (ğ‘¦1, ğ‘¦2, ..., ğ‘¦ğ‘‡ğ‘¦ )
one by one with the causal structure during the inference process
[26, 29]. To reduce the inference latency, Gu et al. [13] introduce
non-autoregressive model based on Transformer into the machine
translation field, which generates all target words in parallel. The
conditional probability can be defined as

ğ‘ƒ (ğ‘¦|ğ‘¥) = ğ‘ƒ (ğ‘‡ğ‘¦ |ğ‘¥)

ğ‘‡ğ‘¦
(cid:214)

ğ‘¡ =1

ğ‘ƒ (ğ‘¦ğ‘¡ |ğ‘¥),

(1)

where ğ‘‡ğ‘¦ is the length of the target sequence gained from the
fertility prediction function conditioned on the source sentence.
Due to the multimodality problem [13], the performance of NAR
model is usually inferior to AR model. Recently, a line of works
aiming to bridge the performance gap between NAR and AR model
for translation task has been presented [11, 14].

Besides the study of NAR translation, many works bring NAR
model into other sequence-to-sequence tasks, such as video caption
[32], speech recognition [5] and speech synthesis [18, 22].

2.3 Spike Neural Network
The integrate-and-fire neuron model describes the membrane po-
tential of a neuron according to the synaptic inputs and the injected
current [4]. It is bio-logical and widely used in spiking neural net-
works. Concretely, the neuron integrates the input signal forwardly
and increases the membrane potential. Once the membrane poten-
tial reaches a threshold, a spike signal is generated, which means
an event takes place. Henceforth, the membrane potential is reset
and then grows in response to the subsequent input signal again. It
enables the encoding from continuous signal sequences to discrete
signal sequences, while retaining the timing information.

Figure 1: The overview of the model architecture for FastLR.

Recently, Dong and Xu [10] introduce the integrate-and-fire
model into speech recognition task. They use continuous functions
that support back-propagation to simulate the process of integrate-
and-fire. In this work, the fired spike represents the event that
locates an acoustic boundary.

3 METHODS
In this section, we introduce FastLR and describe our methods
thoroughly. As shown in Figure 1, FastLR is composed of a spatio-
temporal convolutional neural network for video feature extrac-
tion (visual front-end) and a sequence processing model (main
model) based on Transformer with an enhenced encoder, a non-
autoregressive decoder and a I&F module. To further tackle the
challenges in non-autoregressive lipreading, we propose the NPD
method for I&F and bring byte-pair encoding into our method. The
details of our model and methods are described in the following
subsections2:

3.1 Enhenced Encoder
The encoder of FastLR is composed of stacked self-attention and
feed-forward layers, which are the same as those in Transformer [29]
and autoregressive lipreading model (TM-seq2seq[1]). Thus, we
add an auxiliary autoregressive decoder, shown in the left panel of
Figure 1, and by doing so, we can optimize the AR lipreading task
with FastLR together with one shared the encoder during training
stage. This transfers knowledge from the AR model to FastLR which
facilitates the optimization. Besides, we add the connectionist tem-
poral classification (CTC) decoder with CTC loss on the encoder for

2We introduce the visual front-end in section 4.2 as it varies from one dataset to
another.

forcing monotonic alignments, which is a widely used technique
in speech recognition field. Both adjustments improve the feature
representation ability of our encoder.

3.2 Integrate-and-fire module
To estimate the length of the output sequence and alleviate the
problem of time correlation in target sequence, we adopt continu-
ous integrate-and-Fire (I&F) [10] module for FastLR. This is a soft
and monotonic alignment which can be employed in the encoder-
decoder sequence processing model. First, the encoder output hid-
den sequence â„ = (â„1, â„2, . . . , â„ğ‘š) will be fed to a 1-dimensional
convolutional layer followed by a fully connected layer with sig-
moid activation function. Then we obtain the weight embedding
sequence ğ‘¤ = (ğ‘¤1, ğ‘¤2, . . . , ğ‘¤ğ‘š) which represents the weight of
information carried in â„. Second, the I&F module scans ğ‘¤ and accu-
mulates them from left to right until the sum reaches the threshold
(we set it to 1.0), which means an acoustic boundary is detected.
Third, I&F divides ğ‘¤ğ‘– at this point into two part: ğ‘¤ğ‘–,1 and ğ‘¤ğ‘–,2. ğ‘¤ğ‘–,1
is used for fulfilling the integration of current embedding ğ‘“ğ‘— to
be fired, while ğ‘¤ğ‘–,2 is used for the next integration of ğ‘“ğ‘—+1. Then,
I&F resets the accumulation and continues to scan the rest of ğ‘¤
which begins with ğ‘¤ğ‘–,2 for the next integration. This procedure is
noted as "accumulate and detect". Finally, I&F multiplies all ğ‘¤ğ‘˜ (or
ğ‘¤ğ‘˜,1, ğ‘¤ğ‘˜,2) in ğ‘¤ by corresponding â„ğ‘˜ and integrates them according
to detected boundaries. An example is shown in Figure 2.

3.3 Non-autoregressive Decoder
Different from Transformer decoder, the self-attention of FastLRâ€™s
decoder can attend to the entire sequence for the conditionally

Output Text(Shifted right) N Ã—Encoder BlockPositionalEncodingSource VideoDecoder BlockOutput TextI&F ModuleVisual Front-endSelf-AttentionSelf-AttentionAdd & NormFeed ForwardAdd & NormAdd & NormFeed ForwardAdd & NormEncoder Hidden1D conv & FCWeight EmbeddingWeight EmbeddingEncoderHiddenFiredEmbeddingFCCTC lossLength ControllerPositionalEncodingNÃ—Decoder BlockOutput TextInter-AttentionAdd & NormFeed ForwardAdd & NormPositionalEncodingSelf-AttentionAdd & NormAuxiliary auto-regressive decoderText embeddingpadN Ã—work, we tokenize the sentence with moses tokenizer 3 and then
use BPE algorithm to segment each target word into sub-words.

independent property of NAR model. And we remove the inter-
attention mechanism since FastLR already has an alignment mech-
anism (I&F) between source and target. The decoder takes in the
fired embedding sequence of I&F ğ‘“ = (ğ‘“1, ğ‘“2, . . . , ğ‘“ğ‘›) and generates
the text tokens ğ‘¦ = (ğ‘¦1, ğ‘¦2, . . .ğ‘¦ğ‘›) in parallel during either training
or inference stage.

3.4 Noisy parallel decoding (NPD) for I&F
The absence of AR decoding procedure makes the model much more
difficult to tackle the inherent ambiguity problem in lipreading. So,
we design a novel NPD for I&F method to leverage the language
information in well-trained AR lipreading model.

In section 3.2, it is not hard to find that, âŒŠğ‘†âŒ‹ represents the length
of predicted sequence ğ‘“ (or ğ‘¦), where ğ‘† is the total sum of ğ‘¤. And
Dong and Xu [10] propose a scaling strategy which multiplies ğ‘¤ by
(cid:101)ğ‘†
, . . . , ğ‘¤ â€²
ğ‘š), where (cid:101)ğ‘† is the
a scalar
(cid:205)ğ‘š
ğ‘–=1
ğ‘¦. By doing so, the total sum of ğ‘¤ â€² is equal to
length of target label (cid:101)
(cid:101)ğ‘† and this teacher-forces I&F to predict ğ‘“ with the true length of (cid:101)ğ‘†
which would benefit the cross-entropy training.

to generate ğ‘¤ â€² = (ğ‘¤ â€²
1

, ğ‘¤ â€²
2

ğ‘¤ğ‘–

However, we do not stop at this point. Besides training, we also
scale ğ‘¤ during the inference stage to generate multiple candidates
of weight embedding with different length bias (cid:101)ğ‘. When set the
beam size ğµ = 4,

ğ‘¤ â€²
(cid:101)ğ‘

=

(cid:205)ğ‘š
ğ‘¤ğ‘– + (cid:101)ğ‘
ğ‘–=1
(cid:205)ğ‘š
ğ‘¤ğ‘–
ğ‘–=1

Â· ğ‘¤, where (cid:101)ğ‘ âˆˆ [âˆ’4, 4] âˆ© Z,

(2)

where ğ‘¤ = (ğ‘¤1, ğ‘¤2, . . . , ğ‘¤ğ‘š) is the output of I&F module during
inference and length bias (cid:101)ğ‘ is provided in "Length Controller" mod-
ule in Figure 1. Then, we utilize the re-scoring method used in
Noisy Parallel Decoding (NPD), which is a common practice in
non-autoregressive neural machine translation, to select the best
sequence from these 2 âˆ— ğµ candidates via an AR lipreading teacher:

ğ‘¤ğ‘ ğ‘ƒğ· = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥

ğ‘¤â€²
(cid:101)ğ‘

ğ‘ğ´ğ‘… (ğº (ğ‘¥, ğ‘¤ â€²

(cid:101)ğ‘ ; ğœƒ )|ğ‘¥; ğœƒ ),

(3)

where ğ‘ğ´ğ‘… (ğ´) is the probability of the sequence ğ´ generated by
autoregressive model; The ğº (ğ‘¥, ğ‘¤; ğœƒ ) means the optimal generation
of FastLR given a source sentence ğ‘¥ and weight embedding ğ‘¤, ğœƒ
represents the parameters of model.

The selection process could leverage information in the lan-
guage model (decoder) of the well-trained autoregressive lipreading
teacher, which alleviates the ambiguity problem and gives a chance
to adjust the weight embedding generated by I&F module for pre-
dicting a better sequence length. Note that these candidates can
be computed independently, which wonâ€™t hurt the parallelizability
(only doubles the latency due to the selection process). The experi-
ments demonstrate that the re-scored sequence is more accurate.

3.5 Byte-Pair Encoding
Byte-Pair Encoding [23] is widely used in NMT [29] and ASR [10]
fields, but rare in lipreading tasks. BPE could make each token con-
tain more language information and reduce the dependency among
tokens compared with character level encoding, which alleviate the
problems of non-autoregressive generation discussed before. In this

Figure 2: An example to illustrate how I&F module works.
â„ respresents the encoder output hidden sequence. In this
case ğ‘“1 = ğ‘¤1 Â· â„1 + ğ‘¤2,1 Â· â„2, ğ‘“2 = ğ‘¤2,2 Â· â„2 + ğ‘¤3 Â· â„3 + ğ‘¤4 Â· â„4 + ğ‘¤5 Â·
â„5 + ğ‘¤6 Â· â„6 + ğ‘¤7,1 Â· â„7.

3.6 Training of FastLR
We optimize the CTC decoder with CTC loss. CTC introduces a
set of intermediate representation path ğœ™ (ğ‘¦) termed as CTC paths
for one target text sequence ğ‘¦. Each CTC path is composed of
scattered target text tokens and blanks which can reduce to the
target text sequence by removing the repeated words and blanks.
The likelihood of ğ‘¦ could be calculated as the sum of probabilities
of all CTC paths corresponding to it:
âˆ‘ï¸

ğ‘ƒğ‘ğ‘¡ğ‘ (ğ‘¦|ğ‘¥) =

ğ‘ƒğ‘ğ‘¡ğ‘ (ğ‘ |ğ‘¥)

(4)

Thus, CTC loss can be formulated as:

ğ‘ âˆˆğœ™ (ğ‘¦)

Lğ‘ğ‘¡ğ‘ = âˆ’

âˆ‘ï¸

âˆ‘ï¸

ğ‘ƒğ‘ğ‘¡ğ‘ (ğ‘ |ğ‘¥)

(5)

(ğ‘¥,ğ‘¦) âˆˆ( XÃ—Y)

ğ‘ âˆˆğœ™ (ğ‘¦)

where (X Ã— Y) denotes the set of source video and target text
sequence pairs in one batch.

We optimize the auxiliary autoregressive task with cross-entropy

loss, which can be formulated as:
âˆ‘ï¸

Lğ´ğ‘… = âˆ’

(ğ‘¥,ğ‘¦) âˆˆ( XÃ—Y)

ğ‘™ğ‘œğ‘”ğ‘ƒğ´ğ‘… (ğ‘¦|ğ‘¥)

(6)

And most importantly, we optimize the main task FastLR with

cross-entropy loss and sequence length loss:

Lğ¹ ğ¿ğ‘… = âˆ’

âˆ‘ï¸

ğ‘™ğ‘œğ‘”ğ‘ƒğ¹ ğ¿ğ‘… (ğ‘¦|ğ‘¥) + ( (cid:101)ğ‘†ğ‘¥ âˆ’ ğ‘†ğ‘¥ )2(cid:105)
(cid:104)

(7)

(ğ‘¥,ğ‘¦) âˆˆ( XÃ—Y)
where the (cid:101)ğ‘† and ğ‘† are defined in section 3.4.

Then, the total loss function for training our model is:

L = ğœ†1Lğ‘ğ‘¡ğ‘ + ğœ†2Lğ´ğ‘… + ğœ†3Lğ¹ ğ¿ğ‘…
(8)
where the ğœ†1, ğœ†2, ğœ†3 are hyperparameters to trade off the three
losses.
3https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer
/tokenizer.perl

Accumulate and detectw1w2,1w2,2w3w4w1w2w3w4w5w6w7w8w9w5w6w7,1w7,2w8w9h1h3h4h5h6h7h8h9h2f1f2Weight EmbeddingFired Embeddingâ€¦â€¦4 EXPERIMENTS AND RESULTS
4.1 Datasets

GRID. The GRID dataset [9] consists of 34 subjects, and each of
them utters 1,000 phrases. It is a clean dataset and easy to learn. We
adopt the split the same with Assael et al. [3], where 255 random
sentences from each speaker are selected for evaluation. In order to
better recognize lip movements, we transform the image into gray
scale, and crop the video images to a fixed 100 Ã— 50 size containing
the mouth region with Dlib face detector. Since the vocabulary size
of GRID datasets is quite small and most words are simple, we do
not apply Byte-Pair Encoding [23] on GRID, and just encode the
target sequence at the character level.

LRS2. The LRS2 dataset contains sentences of up to 100 charac-
ters from BBC videos [2], which have a range of viewpoints from
frontal to profile. We adopt the origin split of LRS2 for train/dev/test
sets, which contains 46k, 1,082 and 1,243 sentences respectively.
And we make use of the pre-train dataset provided by LRS2 which
contains 96k sentences for pretraining. Following previous works
[1, 2, 34], the input video frames are converted to grey scale and
centrally cropped into 114 Ã— 114 images. As for the text sentence,
we split each word token into subwords using BPE [23], and set the
vocabulary size to 1k considering the vocabulary size of LRS2.

The statistics of both datasets are listed in Table 1.

Table 1: The statistics on GRID and LRS2 lip reading datasets.
Utt: Utterance.

Dataset

Utt. Word inst. Vocab

hours

GRID

LRS2 (Train-dev)

33k

47k

165k

337k

51

18k

27.5

29

4.2 Visual feature extraction
For GRID datasets, we use spatio-temporal CNN to extract visual
features follow Torfi et al. [27]. The visual front-end network is
composed of four 3D convolution layers with 3D max pooling
and RELU, and two fully connected layers. The kernel size of 3D
convolution and pooling is 3 Ã— 3, the hidden sizes of fully connected
layer as well as output dense layer are both 256. We directly train
this visual front-end together with our main model end-to-end on
GRID on the implementation4 by Torfi et al. [27].

For LRS2 datasets, we adopt the same structure as Afouras et al.
[2], which uses a 3D convolution on the input frame sequence with
a filter width of 5 frames, and a 2D ResNet decreasing the spatial
dimensions progressively with depth. The network convert the ğ‘‡ Ã—
ğ» Ã—ğ‘Š frame sequence into ğ‘‡ Ã— ğ»
32 Ã— ğ‘Š
32 Ã—512 feature sequence, where
ğ‘‡ , ğ»,ğ‘Š is frame number, frame height, frame width respectively. It
is worth noting that, training the visual front-end together with the
main model could obtain poor results on LRS2, which is observed
in previous works [1]. Thus, as Zhao et al. [34] do, we utilize the
frozen visual front-end provided by Afouras et al. [1], which is pre-
trained on a non-public datasets MV-LRS [8], to exact the visual

4https://github.com/astorfi/lip-reading-deeplearning

features. And then, we train FastLR on these features end-to-end.
The pre-trained model can be found in http://www.robots.ox.ac.uk/
~vgg/research/deep_lip_reading/models/lrs2_lip_model.zip.

4.3 Model Configuration
We adopt the Transformer [29] as the basic model structure for
FastLR because it is parallelizable and achieves state-of-the-art ac-
curacy in lipreading [1]. The model hidden size, number of encoder-
layers, number of decoder-layers, and number of heads are set to
ğ‘‘â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘› = 512, ğ‘›ğ‘’ğ‘›ğ‘ = 6, ğ‘›ğ‘‘ğ‘’ğ‘ = 6, ğ‘›â„ğ‘’ğ‘ğ‘‘ = 8 for LRS2 dataset and
ğ‘‘â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘› = 256, ğ‘›ğ‘’ğ‘›ğ‘ = 4, ğ‘›ğ‘‘ğ‘’ğ‘ = 4, ğ‘›â„ğ‘’ğ‘ğ‘‘ = 8 for GRID dataset
respectively. We replace the fully-connected network in origin
Transformer with 2-layer 1D convolution network with ReLU acti-
vation which is commonly used in speech task and the same with
TM-seq2seq [1] for lipreading. The kernel size and filter size of 1D
convolution are set to 4 âˆ— ğ‘‘â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘› and 9 respectively. The CTC de-
coder consists of two fully-connected layers with ReLU activation
function and one fully-connected layer without activation function.
The hidden sizes of these fully-connected layers equal to ğ‘‘â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›.
The auxiliary decoder is an ordinary Transformer decoder with
the same configuration as FastLR, which takes in the target text
sequence shifted right one sequence step for teacher-forcing.

4.4 Training setup
As mentioned in section 3.1, to boost the feature representation
ability of encoder, we add an auxiliary connectionist temporal clas-
sification (CTC) decoder and an autoregressive decoder to FastLR
and optimize them together. We set ğœ†1 to 0.5, ğœ†2, ğœ†3 to 1, 0 during
warm-up training stage, and set ğœ†2, ğœ†3 to 0, 1 during main training
stage for simplicity. The training steps of each training stage are
listed in details in Table 2. Note that experiment on GRID dataset
needs more training steps, since it is trained with its visual front-
end together from scratch, different from experiments on LRS2
dataset. Moreover, the first 45k steps in warm-up stage for LRS2
are trained on LRS2-pretrain sub-dataset and all the left steps are
trained on LRS2-main sub-dataset [1, 2, 34].

We train our model FastLR using Adam following the optimizer
settings and learning rate schedule in Transformer [29]. The train-
ing procedure runs on 2 NVIDIA 1080Ti GPUs. Our code is based
on tensor2tensor [28].

Table 2: The training steps of FastLR for different datasets
for each training stage.

Stage

GRID LRS2

Warm-up
Main

300k
160k

55k
120k

4.5 Inference and Evaluation
During the inference stage, the auxiliary CTC decoder as well as
autoregressive decoder will be thrown away. Given the beam size
ğµ = 4, FastLR generates 2 âˆ— ğµ + 1 candidates of weight embedding
sequence which correspond to 2âˆ—ğµ +1 text sequences, and these text

sequences will be sent to the decoder of a well-trained autoregres-
sive lipreading model (TM-seq2seq) for selection as described in
section 3.4. The result of selected best text sequence is marked with
"NPD9". We conduct the experiments on both "NPD9" and "without
NPD". To be specific, the result of "without NPD" means directly us-
ing the candidate with zero-length bias without a selection process,
which has a lower latency.

The recognition quality is evaluated by Word Error Rate (WER)
and Character Error Rate (CER). Both error rate can be defined as:

ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘…ğ‘ğ‘¡ğ‘’ = (ğ‘† + ğ· + ğ¼ )/ğ‘ ,

(9)

where S, D, I and N are the number of substitutions, deletions,
insertions and reference tokens (word or character) respectively.

When evaluating the latency, we run FastLR on 1 NVIDIA 1080Ti

GPU in inference.

Table 3: The word error rate (WER) and character error rate
(CER) on GRID

works as the AR teacher for NPD selection. We also apply CTC loss
and BPE technique to TM-seq2seq for a fair comparison. 5

The results on two datasets are listed in Table 3 and 4. We can
see that 1) WAS [7] and TM-seq2seq [1, 2] obtain the best results
of autoregressive lipreading model on GRID and LRS2. Compared
with them, FastLR only has a slight WER absolute increase of 1.5%
and 5.5% respectively. 2) Moreover, on GRID dataset, FastLR out-
performs LipNet [3] for 0.3% WER, and exceeds LSTM [30] with
a notable margin; On LRS2 dataset, FastLR achieves better WER
scores than WAS and BLSTM+CTC [2] and keeps comparable per-
formance with LIBS [34]. In addition, compared with LIBS, we do
not introduce any distillation method in training stage, and com-
pared with WAS and TM-seq2seq, we do not leverage information
from other datasets beyond GRID and LRS2. We also propose a base-
line non-autoregressive lipreading model without Integrate-and-
Fire module termed as NAR-LR (base), and conduct experiments
for comparison. As the result shows, FastLR outperforms this NAR
baseline distinctly. The overview of the design for NAR-LR (base)
is shown in Figure 3.

GRID

Method

WER

CER

Autoregressive Models

LSTM [30]
LipNet [3]
WAS [7]

20.4%
4.8%
3.0%

/
1.9%
/

Non-Autoregressive Models

NAR-LR (base)
FastLR (Ours)

25.8% 13.6%
4.5%
2.4%

Table 4: The word error rate (WER) and character error rate
(CER) on LRS2. â€  denotes baselines from our reproduction.

LRS2

Method

WER

CER

Autoregressive Models

WAS [7]
BLSTM+CTC [2]
LIBS [34]
TM-seq2seq [1]

Non-Autoregressive Models

70.4%
76.5%
65.3%
61.7%â€ 

/
40.6%
45.5%
43.5%â€ 

NAR-LR (base)
FastLR (Ours)

81.3%
67.2%

57.9%
46.9%

4.6 Main Results
We conduct experiments of FastLR, and compare them with lipread-
ing baseline and some mainstream state-of-the-art of AR lipreading
models on the GRID and LRS2 datasets respectively. As for TM-
seq2seq [1], it has the same Transformer settings with FastLR and

Figure 3: The NAR-LR (base) model. It is also based
on Transformer [29], but generates outputs in the non-
autoregressive manner [13]. It sends a series of duplicated
trainable tensor into the decoder to generates target tokens.
The repeat count of this trainable tensor is denoted as "m".
For training, "m" is set to ground truth length, but for infer-
ence, we estimate it by a linear function of input length, and
the parameters are obtained using the least square method
on the train set. The auxiliary AR decoder is the same as
FastLRâ€™s. The CTC decoder contains FC layers and CTC loss.

4.7 Speedup
In this section, we compare the average inference latency of FastLR
with that of the autoregressive Transformer lipreading model. And

5Our reproduction has a weaker performance compared with the results reported in
[1, 2]. Because we do not have the resource of MV-LRS, a non-public dataset which
contains individual word excerpts of frequent words used by [1, 2]. Thus, we do not
adopt curriculum learning strategy as Afouras et al. [2].

PositionalEncodingSource VideoEncoderVisual Front-endNÃ—Decoder BlockOutput TextInter-AttentionAdd & NormFeed ForwardAdd & NormPositionalEncodingSelf-AttentionAdd & NormEncoder Hiddenmm Ã—â€¦: A trainable tensorm : Predicted lengthAuxiliary AR DecoderCTCDecoder: A trainable tensorm : Predicted lengththen, we analyze the relationship between speedup and the length
of the predicted sequence.

4.7.1 Average Latency Comparison. The average latency is mea-
sured in average time in seconds required to decode one sentence
on the test set of LRS2 dataset. We record the inference latency and
corresponding recognition accuracy of TM-seq2seq [1, 2], FastLR
without NPD and FastLR with NPD9, which is listed in Table 5.

The result shows that FastLR speeds up the inference by 11.94Ã—
without NPD, and by 5.81Ã— with NPD9 on average, compared with
the TM-seq2seq which has similar number of model parameters.
Note that the latency is calculated excluding the computation cost
of data pre-processing and the visual front-end.

Table 5: The comparison of average inference latency and
corresponding recognition accuracy. The evaluation is con-
ducted on a server with 1 NVIDIA 1080Ti GPU, 12 Intel Xeon
CPU. The batch size is set to 1. The average length of the gen-
erated sub-word sequence are all about 14.

Method

WER

Latency (s)

Speedup

TM-seq2seq [1]
FastLR (no NPD)
FastLR (NPD 9)

61.7%
73.2%
67.2%

0.215
0.018
0.037

1.00 Ã—
11.94 Ã—
5.81 Ã—

4.7.2 Relationship between Speedup and Length. During inference,
the autoregressive model generates the target tokens one by one,
but the non-autoregressive model speeds up the inference by in-
creasing parallelization in the generation process. Thus, the longer
the target sequence is, the more the speedup rate is. We visual-
ize the relationship between the length of the predicted sub-word
sequence in Figure 4. It can be seen that the inference latency in-
creases distinctly with the predicted text length for TM-seq2seq,
while nearly holds a small constant for FastLR.

Then, we bucket the test sequences of length within [30, 35],
and calculate their average inference latency for TM-seq2seq and
FastLR to obtain the maximum speedup on LRS2 test set. The results
are 0.494s and 0.045s for TM-seq2seq and FastLR (NPD9) respec-
tively, which shows that FastLR (NPD9) achieves the speedup up
to 10.97Ã— on LRS2 test set, thanks to the parallel generation which
is insensitive to sequence length.

5 ANALYSIS
In this section, we first conduct ablation experiments on LRS2
to verify the significance of all proposed methods in FastLR. The
experiments are listed in Table 6. Then we visualize the encoder-
decoder attention map of the well-trained AR model (TM-seq2seq)
and the acoustic boundary detected by the I&F module in FastLR
to check whether the I&F module works well.

5.1 The Effectiveness of Auxiliary AR Task
As shown in the table 6, the naive lipreading model with Integrate-
and-Fire is not able to converge well, due to the difficulty of learning
the weight embedding in I&F module from the meaningless encoder
hidden. Thus, the autoregressive lipreading model works as the

(a) TM-seq2seq [1]

(b) FastLR (NPD9)

Figure 4: Relationship between Inference time (second) and
Predicted Text Length for TM-seq2seq [1] and FastLR.

Table 6: The ablation studies on LRS2 dataset. Naive Model
with I&F is the naive lipreading model only with Integrate-
and-Fire. "+Aux" means adding the auxiliary autoregressive
task. We add our methods and evaluate their effectiveness
progressively.

Model

WER

CER

Naive Model with I&F

>1

+Aux
+Aux+BPE
+Aux+BPE+CTC

93.1%
75.7%
73.2%

75.2%

64.9%
52.7%
51.4%

+Aux+BPE+CTC+NPD
(FastLR)

67.2% 46.9%

auxiliary model to enhance the feature representation ability of
encoder, and guides the non-autoregressive model with Integrate-
and-Fire to learn the right alignments (weight embedding). From
this, the model with I&F begins to generate the target sequence
with meaning, and ğ¶ğ¸ğ‘… < 65% (Row 3).

5.2 The Effectiveness of Byte-Pair Encoding
BPE makes each token contain more language information and re-
duce the dependency among tokens compared with character level

5101520253035Predicted Text Length0.10.20.30.40.50.6Inference Time101520253035Predicted Text Length0.020.030.040.050.06Inference Timeencoding. In addition, from observation, the speech speed of BBC
video is a bit fast, which causes that one target token (character
if without BPE) corresponds to few video frames. While BPE com-
presses the target sequence and this will help the Integrate-and-Fire
module to find the acoustic level alignments easier.

From the table 6 (Row 4), it can be seen that BPE reduces the word
error rate and character error rate to 75.7% and 52.7% respectively,
which means BPE helps the model gains the ability to generates
understandable sentence.

5.3 The Effectiveness of CTC
The result shows that (Row 5), adding auxiliary connectionist tem-
poral classification(CTC) decoder with CTC loss will further boost
the feature representation ability of encoder, and cause 2.5% abso-
lute decrease in WER. At this point, the model gains considerable
recognition accuracy compared with the traditional autoregressive
method.

5.4 The Effectiveness of NPD for I&F
Table 6 (Row 6) shows that using NPD for I&F can boost the per-
formance effectively. We also study the effect of increasing the
candidates number for FastLR on LRS2 dataset, as shown in Figure
5. It can be seen that, when setting the candidates number to 9,
the accuracy peaks. Finally, FastLR achieves considerable accuracy
compared with state-of-the-art autoregressive lipreading model.

Figure 5: The effect of cadidates number on WER and CER
for FastLR model.

5.5 The Visualization of Boundary Detection
We visualize the encoder-decoder attention map in Figure 6, which
is obtained from the well-trained AR TM-seq2seq. The attention
map illustrates the alignment between source video frames and the
corresponding target sub-word sequence.

The figure shows that the video frames between two horizon-
tal red lines are roughly just what the corresponding target token
attends to. It means that the "accumulate and detect" part in I&F
module tells the acoustic boundary well and makes a right predic-
tion of sequence length.

Figure 6: An example of the visualization for encoder-
decoder attention map and the acoustic boundary. The hor-
izontal red lines represent the acoustic boundaries detected
by I&F module in FastLR, which split the video frames to
discrete segments.

6 CONCLUSION
In this work, we developed FastLR, a non-autoregressive lipread-
ing system with Integrate-and-Fire module, that recognizes source
silent video and generates all the target text tokens in parallel.
FastLR consists of a visual front-end, a visual feature encoder and
a text decoder for simultaneous generation. To bridge the accuracy
gap between FastLR and state-of-the-art autoregressive lipreading
model, we introduce I&F module to encode the continuous visual
features into discrete token embedding by locating the acoustic
boundary. In addition, we propose several methods including auxil-
iary AR task and CTC loss to boost the feature representation ability
of encoder. At last, we design NPD for I&F and bring Byte-Pair En-
coding into lipreading, and both methods alleviate the problem
caused by the removal of AR language model. Experiments on
GRID and LRS2 lipreading datasets show that FastLR outperforms
the NAR-LR baseline and has a slight WER increase compared with
state-of-the-art AR model, which demonstrates the effectiveness of
our method for NAR lipreading.

In the future, we will continue to work on how to make a better
approximation to the true target distribution for NAR lipreading
task, and design more flexible policies to bridge the gap between AR
and NAR model as well as keeping the fast speed of NAR generation.

ACKNOWLEDGMENTS
This work was supported in part by the National Key R&D Pro-
gram of China (Grant No.2018AAA0100603), Zhejiang Natural Sci-
ence Foundation (LR19F020006), National Natural Science Founda-
tion of China (Grant No.61836002, No.U1611461 and No.61751209)
and the Fundamental Research Funds for the Central Universities
(2020QNA5024). This work was also partially supported by the
Language and Speech Innovation Lab of HUAWEI Cloud.

1234567891011Candidates Number5055606570Error Rate %WERCERREFERENCES
[1] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, and An-
drew Zisserman. 2018. Deep audio-visual speech recognition. IEEE transactions
on pattern analysis and machine intelligence (2018).

[2] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. 2018. Deep
lip reading: a comparison of models and an online application. arXiv preprint
arXiv:1806.06053 (2018).

[3] Yannis M Assael, Brendan Shillingford, Shimon Whiteson, and Nando De Fre-
Lipnet: End-to-end sentence-level lipreading.
arXiv preprint

itas. 2016.
arXiv:1611.01599 (2016).

[4] Anthony N Burkitt. 2006. A review of the integrate-and-fire neuron model: I.

Homogeneous synaptic input. Biological cybernetics 95, 1 (2006), 1â€“19.

[5] Nanxin Chen, Shinji Watanabe, JesÃºs Villalba, and Najim Dehak. 2019. Non-
Autoregressive Transformer Automatic Speech Recognition. arXiv preprint
arXiv:1911.04908 (2019).

[6] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).

[7] Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. 2017.
Lip reading sentences in the wild. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). IEEE, 3444â€“3453.

[8] Joon Son Chung and AP Zisserman. 2017. Lip reading in profile. (2017).
[9] Martin Cooke, Jon Barker, Stuart Cunningham, and Xu Shao. 2006. An audio-
visual corpus for speech perception and automatic speech recognition. The
Journal of the Acoustical Society of America 120, 5 (2006), 2421â€“2424.

[10] Linhao Dong and Bo Xu. 2019. CIF: Continuous Integrate-and-Fire for End-to-End

Speech Recognition. arXiv preprint arXiv:1905.11235 (2019).

[11] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019. Mask-
predict: Parallel decoding of conditional masked language models. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP). 6114â€“6123.

[12] Alex Graves, Santiago FernÃ¡ndez, Faustino Gomez, and JÃ¼rgen Schmidhuber.
2006. Connectionist temporal classification: labelling unsegmented sequence
data with recurrent neural networks. In Proceedings of the 23rd international
conference on Machine learning. 369â€“376.

[13] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. 2017.
Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281
(2017).

[14] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. 2019. Non-
autoregressive neural machine translation with enhanced decoder input. In AAAI,
Vol. 33. 3723â€“3730.

[15] Jason Lee, Elman Mansimov, and Kyunghyun Cho. 2018. Deterministic Non-
Autoregressive Neural Sequence Modeling by Iterative Refinement. In EMNLP.
1173â€“1182.

[16] Jinglin Liu, Yi Ren, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, and Tie-Yan Liu.
2020. Task-Level Curriculum Learning for Non-Autoregressive Neural Machine
Translation. In Proceedings of the Twenty-Ninth International Joint Conference on
Artificial Intelligence, IJCAI-20. 3861â€“3867.

[17] Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy. 2019.
FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative
Flow. In EMNLP-IJCNLP. 4273â€“4283.

[18] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals,
Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo,
Florian Stimberg, et al. 2017. Parallel wavenet: Fast high-fidelity speech synthesis.
arXiv preprint arXiv:1711.10433 (2017).

[19] Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Georgios Tzimiropoulos, and
Maja Pantic. 2018. Audio-visual speech recognition with a hybrid ctc/attention
architecture. In 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE,
513â€“520.

[20] Yi Ren, Chenxu Hu, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 2020.
FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech. arXiv preprint
arXiv:2006.04558 (2020).

[21] Yi Ren, Jinglin Liu, Xu Tan, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 2020. A
Study of Non-autoregressive Model for Sequence Generation. arXiv preprint
arXiv:2004.10454 (2020).

[22] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan
Liu. 2019. Fastspeech: Fast, robust and controllable text to speech. In Advances
in Neural Information Processing Systems. 3165â€“3174.

[23] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine
translation of rare words with subword units. arXiv preprint arXiv:1508.07909
(2015).

[24] Brendan Shillingford, Yannis Assael, Matthew W Hoffman, Thomas Paine, CÃ­an
Hughes, Utsav Prabhu, Hank Liao, Hasim Sak, Kanishka Rao, Lorrayne Bennett,
et al. 2018. Large-scale visual speech recognition. arXiv preprint arXiv:1807.05162
(2018).

[25] Themos Stafylakis and Georgios Tzimiropoulos. 2017. Combining residual net-
works with LSTMs for lipreading. arXiv preprint arXiv:1703.04105 (2017).
[26] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning
with neural networks. In Advances in neural information processing systems. 3104â€“
3112.

[27] Amirsina Torfi, Seyed Mehdi Iranmanesh, Nasser Nasrabadi, and Jeremy Daw-
son. 2017. 3d convolutional neural networks for cross audio-visual matching
recognition. IEEE Access 5 (2017), 22081â€“22091.

[28] Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez,
Stephan Gouws, Llion Jones, Åukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan
Sepassi, Noam Shazeer, and Jakob Uszkoreit. 2018. Tensor2Tensor for Neural
Machine Translation. CoRR abs/1803.07416 (2018). http://arxiv.org/abs/1803.
07416

[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998â€“6008.
[30] Michael Wand, Jan KoutnÃ­k, and JÃ¼rgen Schmidhuber. 2016. Lipreading with long
short-term memory. In 2016 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP). IEEE, 6115â€“6119.

[31] Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu.
2019. Non-Autoregressive Machine Translation with Auxiliary Regularization.
In AAAI.

[32] Bang Yang, Fenglin Liu, and Yuexian Zou. 2019. Non-Autoregressive Video

Captioning with Iterative Refinement. arXiv preprint arXiv:1911.12018 (2019).

[33] Xingxuan Zhang, Feng Cheng, and Shilin Wang. 2019. Spatio-temporal fusion
based convolutional sequence learning for lip reading. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 713â€“722.

[34] Ya Zhao, Rui Xu, Xinchao Wang, Peng Hou, Haihong Tang, and Mingli Song.
2019. Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers.
arXiv preprint arXiv:1911.11502 (2019).

