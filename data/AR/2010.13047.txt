ORTHROS: NON-AUTOREGRESSIVE END-TO-END SPEECH TRANSLATION
WITH DUAL-DECODER

Hirofumi Inaguma1, Yosuke Higuchi2, Kevin Duh3, Tatsuya Kawahara1, Shinji Watanabe3

1Kyoto University, Japan 2Waseda University, Japan 3Johns Hopkins University, USA

1
2
0
2

b
e
F
8
1

]
L
C
.
s
c
[

3
v
7
4
0
3
1
.
0
1
0
2
:
v
i
X
r
a

ABSTRACT

Fast inference speed is an important goal towards real-world deploy-
ment of speech translation (ST) systems. End-to-end (E2E) mod-
els based on the encoder-decoder architecture are more suitable for
this goal than traditional cascaded systems, but their effectiveness
regarding decoding speed has not been explored so far. Inspired by
recent progress in non-autoregressive (NAR) methods in text-based
translation, which generates target tokens in parallel by eliminating
conditional dependencies, we study the problem of NAR decoding
for E2E-ST. We propose a novel NAR E2E-ST framework, Orthros,
in which both NAR and autoregressive (AR) decoders are jointly
trained on the shared speech encoder. The latter is used for selecting
better translation among various length candidates generated from
the former, which dramatically improves the effectiveness of a large
length beam with negligible overhead. We further investigate effec-
tive length prediction methods from speech inputs and the impact
of vocabulary sizes. Experiments on four benchmarks show the ef-
fectiveness of the proposed method in improving inference speed
while maintaining competitive translation quality compared to state-
of-the-art AR E2E-ST systems.

Index Terms— End-to-end speech translation, non-autoregressive

decoding, conditional masked language model

1. INTRODUCTION

There is a growing interest in speech translation [1] due to the in-
crease in demand for international communications. The goal is
to transform speech from one language to text in another language.
Traditionally, the dominant approach is to cascade automatic speech
recognition (ASR) and machine translation (MT) systems. Thanks to
recent progress in neural approaches, researchers are shifting to the
development of end-to-end speech translation (E2E-ST) systems [2],
aiming to optimize a direct mapping from source speech to target text
translation by bypassing ASR components. The potential advantages
of E2E modeling are (a) mitigation of error propagation from incor-
rect transcriptions, (b) low-latency inference, and (c) applications
in endangered language documentation [3]. However, most efforts
have been devoted to investigating methods to improve translation
quality by the use of additional data [4–12], better parameter initial-
ization [13–15], and improved training methods [16, 17].

For applications of ST systems in lectures and dialogues, infer-
ence speed is also an essential factor from the user’s perspective [18].
Although E2E models are more suitable for small latency inference
than cascaded models since the ASR decoder and the MT encoder
processing can be skipped, their effectiveness regarding inference
speed has not been well studied. Moreover, incremental left-to-right
token generation of autoregressive (AR) E2E models increases com-
putational complexity and, therefore, still suffer from slow inference.
To speed up inference while achieving comparable translation qual-

ity to AR models, non-autoregressive (NAR) decoding has been in-
vestigated in text-based translation [19–28]. The NAR inference en-
ables to generate tokens in parallel by eliminating conditional token
dependencies.

Motivated by this, we propose a novel NAR framework, Orthros,
for the E2E-ST task. Orthros has dual decoders on the shared speech
encoder: NAR and AR decoders. The NAR decoder is used for
the fast token generation, and the AR decoder selects better trans-
lation among multiple length candidates during inference. As the
AR decoder can rescore all tokens in parallel and reuse encoder
outputs, its overhead is minimal. This architecture design is moti-
vated by the difﬁculty of estimating a suitable target length given
a speech in advance. We adopt the conditional masked language
model (CMLM) [23] for the NAR decoder, in which a subset of
tokens are repeatedly updated by partial masking through constant
iterations. We also use semi-autoregressive training (SMART) to
alleviate mismatches between training and testing conditions [29].
However, any NAR decoder can be used in Orthros, conceptually.
Moreover, effective length prediction methods and the impact of vo-
cabulary sizes are also studied.

Experiments on four benchmark corpora show that the proposed
framework achieves comparable translation quality to state-of-the-
art AR E2E- and cascaded-ST models with approximately 2.31×
and 4.61× decoding speed-ups, respectively. Interestingly, the best
NAR model can even outperform AR models in terms of the BLEU
score in some cases. This work is the ﬁrst study of NAR models for
the E2E-ST task to the best of our knowledge.

2.1. End-to-end Speech Translation (E2E-ST)

2. BACKGROUND

E2E-ST is formulated as a direct mapping problem from input
speech X = (x1, . . . , xU ) in a source language to the target transla-
tion text Y = (y1, . . . , yN ). E2E models can be implemented with
any encoder-decoder architectures, and we adopt the state-of-the-art
Transformer model [30]. A conventional E2E-ST model is com-
posed of a speech encoder and an autoregressive (AR) translation
decoder, which decomposes a probability distribution of Y into a
chain of conditional probabilities from left to right as:

P (Y |X) =

N
(cid:89)

i=1

Par(yi|y<i, X).

(1)

Parameters are optimized with a single translation objective Lar =
− log Par(Y |X) after pre-training with the ASR and MT tasks [13].

2.2. Conditional masked language model (CMLM)

Since AR models generate target tokens incrementally during infer-
ence, they suffer from high inference latency and do not fully lever-
age the computational power of modern hardware such as GPU. In
order to tackle this problem, parallel sequence generation with non-
autoregressive (NAR) models have been investigated in a wide range

 
 
 
 
 
 
Table 1: An example of Mask-Predict decoding on the Fisher-dev set. Highlighted tokens are masked for the next iteration.

Reference
t = 4

t = 7
t = 10

but with that and and when we bought it i thought who’s gonna put a you know a walkman or something and that’s what i’m doing now
he came with that and when we bought

thought who going to put you know know ak or or something and now i’m doing it

it

i

he came with that and when we bought it i thought who he going to put you know akman or something and now i’m doing it
he came with that and when we bought it i thought who is going to put you know a walkman or something and now i’m doing it

of tasks such as text-to-speech synthesis [31, 32], machine transla-
tion [19], and speech recognition [33–38]. NAR models factorize
conditional probabilities in Eq. (1) by assuming conditional inde-
pendence for every target position. However, iterative reﬁnement
methods generally achieve better quality than pure NAR methods
at the cost of speed because of multiple forward passes [20, 23, 39].
Among them, the conditional masked language model (CMLM) [23]
is a natural choice because of its simplicity and good performance.
Moreover, we can ﬂexibly trade the speed during inference by chang-
ing the number of iterations T .

In CMLM, the partial decoder inputs are masked by replacing
with a unique token [MASK] based on conﬁdence. Intermediate dis-
crete variables at the t-th iteration Y (t) (1 ≤ t ≤ T ) are iteratively
reﬁned given the rest observed tokens Y (t)

obs ⊂ Y (t−1) as:

P (Y |X) =

T
(cid:89)

t=1

Pnar(Y (t)|Y (t)

obs, X).

(2)

The target length distribution for N is typically modeled by a linear
classiﬁer stacked on the encoder.

However, NAR models suffer from a multimodality problem,
where a distribution of multiple correct translations must be modeled
given the same source sentence. Recent studies reveal that sequence-
level knowledge distillation [40] makes training data deterministic
and mitigates this problem [19, 41, 42].

3. PROPOSED METHOD: ORTHROS

target sequence length, and ˆY (t)
obs be masked and observed
tokens in the prediction ˆY (t−1) (| ˆY (t−1)| = ˆN ) at the t-th iteration
(1 ≤ t ≤ T ), respectively. At the initial iteration t = 0, all tokens
are initialized with [MASK]. An example is shown in Table 1.

mask and ˆY (t)

3.2.1. Mask-Predict

The mask-predict algorithm performs two operations, mask and pre-
dict, at every iteration t. In the mask operation, given predicted to-
kens at the previous iteration, ˆY (t−1), we mask kt tokens having
the lowest conﬁdence scores, where kt is a linear decay function
kt = (cid:98) ˆN · T −t
t (cid:99). In the predict operation, we take the most prob-
able token from a posterior probability distribution Pcmlm at every
masked position i and update ˆy(t)

i ∈ ˆY (t)

mask as:

ˆy(t)
i = argmax

wi∈V

Pcmlm(wi| ˆY (t)

obs, X),

(3)

where V is the vocabulary. When using SMART, described in Sec-
i ∈ ˆY (t) are updated in Eq. (3) if they differ
tion 3.3.1, all tokens ˆy(t)
from those at the previous iteration. Furthermore, we generate l tar-
get sentences having different lengths in parallel and select the most
probable candidate by calculating the average log probability over
(cid:80)
all tokens at the last iteration: 1
ˆN

i log P (T )

i,cmlm.

For target length prediction, we sample top-l (l ≥ 1) length can-
didates from a linear classiﬁer conditioned on time-averaged encoder
outputs. We also study a simple scaling method using CTC outputs
used for the auxiliary ASR task.

3.1. Model architecture

3.2.2. Candidate selection with AR decoder

Typical text-based NAR models generate target sentences with mul-
tiple lengths in parallel to improve quality in a stochastic [19] or
deterministic [23] way, followed by an optional rescoring step with
a separate AR model [19]. However, a spoken utterance consists
of hundreds of acoustic frames even after downsampling, and its
length varies signiﬁcantly based on the speaking rate and silence
duration. Therefore, it is challenging to estimate the target length
given a speech in advance accurately. Moreover, extra computation
and memory consumption for feature encoding with the separate AR
model in rescoring are not negligible. This motivated us to pro-
pose Orthros, having dual decoders on top of the shared encoder:
an NAR decoder for fast token generation and an AR decoder for
candidate selection from the NAR decoder. This way, re-encoding
speech frames is unnecessary, and the AR decoder greatly improves
the effectiveness of using a large length beam. The speech encoder
is identical to that of AR models. A length predictor and a CTC ASR
layer for the auxiliary ASR task are also built on the same encoder.
Our NAR decoder is based on the conditional masked language
model (CMLM) [23]. One of the distinguished advantages over pure
NAR models [19] is that CMLM removes the necessity of a copy
of the source sentence to initialize decoder inputs. This could be
achieved by using a predicted transcription from the auxiliary ASR
sub-module, but this contradicts a motivation to avoid ASR errors.

3.2. Inference
The inference of the CMLM is based on the Mask-Predict algo-
rithm [23]. Let T be the number of iterations, ˆN be a predicted

Using multiple length candidates is effective for improving qual-
ity, but it is sub-optimal to directly use sequence-level scores from
P (T )
i,cmlm in Eq. (3) because they are stale [23]. Therefore, we propose
to select the most probable translation among l candidates after the
last iteration by using log probability scores from the AR decoder
averaged over all tokens. Note that we do not use scores from the
NAR decoder here. Since the AR decoder can rescore all tokens in
a candidate in parallel, it can still maintain the advantage of paral-
lelism in self-attention.

3.3. Training

The training objective of the CMLM can be formulated as follows:

Lcmlm = −

(cid:88)

y∈Ymask

log Pcmlm(y|Yobs, X),

(4)

where Ymask ⊂ Y and Yobs = Y \ Ymask. We sample the number of
masked tokens from a uniform distribution, U(1, N ), following [23].

3.3.1. Semi-autoregressive training (SMART)

To bridge the gap between training and test conditions in the CMLM,
we adopt semi-autoregressive training (SMART) [29]. SMART uses
two forward passes to calculate the cross-entropy (CE) loss. In the
ﬁrst pass, the CMLM generates predictions at all positions, ˆY , given
partially-observed ground-truth tokens Yobs as in the original train-
ing process. The gradient ﬂow is truncated in the ﬁrst pass [29].
Then, a subset of tokens in ˆY are masked again with a new mask.

Table 2: BLEU scores of AR and NAR methods on the tst-COMMON sets of Must-C (En→De and En→Fr), Fisher-test (Fsh-test) and
CallHome-evltest (CH-evltest) sets of Fisher-CallHome Spanish (Es→En), and the test set of Libri-trans (En→Fr). Seq-KD represents
sequence-level knowledge distillation. Latency is measured as average decoding time per sentence on Must-C En→De, with batch size 1.

BLEU

ID Model

Must-C

Fisher-CallHome

A1

A2

N1

N2

N3

N4

N5

A3

Transformer (b = 1)
Transformer (b = 4)
Transformer + Seq-KD (b = 1)
Transformer + Seq-KD (b = 4)
CTC (b = 1)
Orthros (CMLM, T = 4)
Orthros (CMLM, T = 4 +AR)
Orthros (CMLM, T = 10)
Orthros (CMLM, T = 10 +AR)
Orthros (SMART, T = 4)
Orthros (SMART, T = 4 +AR)
Orthros (SMART, T = 10)
Orthros (SMART, T = 10 +AR)
+ BPE8k→16k
+ large (SMART, T = 4 +AR, l = 7)
+ large (SMART, T = 10 +AR, l = 7)
AR ASR (b = 1) → AR MT (b = 1)
AR ASR (b = 4) → AR MT (b = 4)

De
21.54
23.12
23.88
24.43
19.40
18.78
19.62
20.89
21.79
20.03
21.08
21.25
22.27
22.88
22.54
23.92
22.20
23.30

Fr
32.26
33.84
33.92
34.57
27.38
25.99
27.77
28.74
30.31
27.22
29.30
29.31
31.07
32.20
31.24
33.05
31.67
33.40

Fsh-test
48.38
48.49
50.34
50.32
45.97
46.03
47.80
48.56
49.98
45.89
48.73
47.09
50.07
50.18
–
–
40.94
42.05

CH-evltest
18.07
18.90
19.09
19.81
15.91
16.71
18.28
18.60
19.71
17.39
19.25
18.25
20.10
19.88
–
–
19.15
19.77

Libri-
trans

16.52
16.84
15.91
16.44
12.10
12.90
13.69
14.68
15.43
14.17
14.99
15.11
16.08
16.22
–
–
16.44
16.52

Latency
(ms)

175
271
–
–
13
–
–
–
–
46
61
99
111
117
59
113
154→166
333→207

Speedup

1.54 ×
1.00 ×
–
–
20.84 ×
–
–
–
–
5.89 ×
4.44 ×
2.73 ×
2.44 ×
2.31 ×
4.59 ×
2.39 ×
0.84 ×
0.50 ×

AR

NAR

E2E

Cascade

AR

The resulting observed tokens ˆYobs are fed into the decoder as in-
puts in the second pass. The CE loss is calculated with predictions
at all positions in the second pass, unlike the original training.

3.3.2. Total training objective

The speech encoder and all four branches (NAR/AR decoders,
length predictor, and CTC ASR layer) are optimized jointly. The
total objective function is formulated as:

Ltotal = (1 − λasr)Lcmlm(Y |X) + λarLar(Y |X)

+ λlpLlp(N |X) + λasrLasr(Y src|X),

where Lar, Llp, and Lasr are losses in AR E2E-ST, length predic-
tion, and ASR tasks, Y src is the corresponding transcription, and λ∗
are the corresponding tunable hyperparameters. We set (λar, λlp,
λasr) to (0.3, 0.1, 0.3) throughout the experiments.

4. EXPERIMENTAL EVALUATION

4.1. Datasets

We used En-De (229k pairs, 408 hours) and En-Fr (275k pairs,
492 hours) language directions on Must-C [43], Fisher-CallHome
Spanish (Es-En, 138k pairs, 170 hours, hereafter Fisher-CH) [44],
and Libri-trans (En-Fr, 45k pairs, 100 hours) [45]. All corpora
contain a triplet of source speech and the corresponding transcrip-
tion and translation, and we used the same preprocessing as [46].
For Must-C, we report case-sensitive detokenized BLEU [47] on
the tst-COMMON set. Non-verbal speech labels such as ”(Ap-
plause)” were removed during evaluation. For Fisher-CH, we report
case-insensitive detokenized BLEU on the Fisher-test (four ref-
erences) and CallHome-evltest sets. For Libri-trans, we report
case-insensitive BLEU on the test set. We removed case infor-
mation and all punctuation marks except for apostrophe in both
transcriptions of all corpora and translations of Fisher-CH.

We extracted 80-channel log-mel ﬁlterbank coefﬁcients with 3-
dimensional pitch features using Kaldi [48] as input speech features,
which was augmented by a factor of 3 with speed perturbation [49]
and SpecAugment [50] to avoid overﬁtting. All sentences were tok-
enized with the tokenizer.perl script in Moses [51]. We built

vocabularies based on byte pair encoding (BPE) algorithm [52] im-
plemented with Sentencepiece [53]. The joint source and target vo-
cabularies were used in the ST/MT tasks, while the ASR vocabu-
laries were constructed with the transcriptions only. For Must-C,
we used 5k and 8k vocabularies for ASR and E2E-ST/MT models,
respectively. For Fisher-CH and Libri-trans, we used 8k and 1k vo-
cabularies for NAR E2E-ST models and the others, respectively.

4.2. Model conﬁgurations

We used the Transformer architecture implemented in ESPnet-
ST [46] for all tasks. All ASR and E2E-ST models consisted of
stacked 12 encoder layers and 6 decoder layers. Speech encoders
had 2 CNN layers before the self-attention layers, which performed
4-fold downsampling. The text encoders in the MT models consisted
of 6 layers. The dimension of self-attention layer dmodel and feed-
forward network dﬀ , and the number of heads H were set to 256,
2048, and 4, respectively. For the large model on Must-C, we set
dmodel = 512 and H = 8. The Adam optimizer [54] with β1 = 0.9,
β2 = 0.98, and (cid:15) = 10−9 was used for training with Noam learning
rate schedule [30]. Warmup steps and a learning rate constant were
set to 25000 and 5.0, respectively. A mini-batch was constructed
with 32 utterances, and gradients were accumulated for 8 steps in
NAR E2E-ST models. The last 5 best checkpoints based on the val-
idation performance were used for model averaging. Following the
standard practice in NAR models [19, 23], we used sequence-level
knowledge distillation (Seq-KD) [40] with the corresponding AR
Transformer MT model, except for Libri-trans. During inference,
we used a beam width b ∈ {1, 4} for AR ASR/ST/MT models, and a
length beam width l = 9 for NAR models. The language model was
used for the ASR model on Libri-trans only. Joint CTC/Attention
decoding [55] was performed for ASR models. Decoding time was
measured with a batch size 1 on a single NVIDIA TITAN RTX GPU
by averaging on ﬁve runs. We initialized encoder parameters of E2E-
ST models with those of the corresponding pre-trained ASR model
and AR decoder parameters with those of the corresponding pre-
trained AR MT model trained on the same triplets, respectively [13].
However, NAR decoder parameters were initialized based on the
weight initialization scheme in BERT [23, 56].

Fig. 1: Trade-off between decoding speed-up and BLEU scores on
the Must-C En-De tst-COMMON set. Parentheses, square brackets,
and curly brackets represent (T , l), [b], and {b (ASR), b (MT)},
respectively.

Table 3: Ablation study on the Fisher-dev set

Model

Orthros (N3)
- Seq-KD
- AR decoder
+ length prediction w/ CTC

BLEU

T = 4

T = 10

w/o AR
45.76
44.36
45.53
45.41

w/ AR
49.01
47.42
N/A
48.18

w/o AR
46.88
44.25
46.94
46.79

w/ AR
50.28
49.50
N/A
50.05

4.3. Main results

The main results are shown in Table 2. Iterative reﬁnement based on
CMLM (N2) signiﬁcantly outperformed the pure NAR CTC model
(N1) in translation quality.1 Increasing the number of iterations T
was effective for improving quality at the cost of latency. SMART
also boosted the BLEU scores with no extra latency during infer-
ence, except for Fisher-CH (N3). This is probably because sequence
lengths in Fisher-CH are relatively short. Candidate selection with
the AR decoder greatly improved the quality with a negligible in-
crease of latency, which corresponds to performing one more iter-
ation. We also found that NAR models prefer the large vocabu-
lary size for better quality. Using BPE16k, BLEU scores were im-
proved while keeping latency (N4). We note that the BPE size was
tuned separately for AR and NAR models. We will analyze this
phenomenon in Section 4.5. Increasing the model capacity also im-
proved the quality of NAR models while it did not hurt the speed so
much when using GPU. AR models did not beneﬁt from the larger
capacity on this corpus though not shown in the table (see Fig. 1).

For a comparison with AR models, Orthros achieved compara-
ble quality to both strong AR E2E (A1, A2) and cascaded systems
(A3) with smaller latency.
Interestingly, N4 and N5 even outper-
formed A1 in quality by a large margin on Fisher-CH and Must-C
En-De, respectively. Seq-KD was very effective for AR models as
well except for Libri-trans. Although relative speed-ups are smaller
than those in the MT literature [23, 39], this is probably because we
used the smaller vocabulary and the baseline AR models have much
smaller latency (e.g., 607ms in [39] vs. 271ms in ours).

Fig. 1 shows the trade-off between relative speed-ups and BLEU
scores on the Must-C En-De tst-COMMON set. Consistent with
text-based CMLM models [23], a large length beam width l was not
effective. However, the proposed candidate selection signiﬁcantly
improved the performances with a larger l. This way, a similar
BLEU score can be achieved with a smaller iteration. Moreover,

1CTC in the E2E-ST task has the speech encoder and the output classiﬁer.
It was optimized with a single CTC objective with a pair of (X, Y ). Since
input speech lengths are generally longer than target sequence lengths in the
E2E-ST task, we did not use the upsampling technique in [22].

Fig. 2: Impact of vocabulary size on the Fisher-dev set

Orthros (N4) can obtain the same BLEU as a baseline AR (A1) with
more than 3 times speed-up for greedy decoding and 1.5 times for
beam search. The large Orthros (N5) achieved better BLEU scores
than N4 with similar latency and outperformed the AR models with
beam search both in quality and latency. Although the cascaded
models showed reasonable BLEU scores, they were much slower
than the E2E models. We also compared AR models for candidate
selection: the AR decoder on the uniﬁed encoder (proposal) vs. the
separate AR encoder-decoder. The uniﬁed encoder showed smaller
latency with better quality. We suspect that this is because sharing
the encoder has a positive effect on candidate selection, or the AR
decoder in Orthros was trained with Seq-KD. We will analyze this in
future work. Although the overhead for additional speech encoding
was relatively small here, this would be enlarged when using a more
complicated encoder architecture. One more advantage of Orthros
is that the memory consumption for model parameters and encoder
output caching is much smaller.

4.4. Ablation study

To see individual contributions of the proposed techniques, we con-
ducted the ablation study on the Fisher-dev set in Table 3. Seq-KD
was beneﬁcial for boosting BLEU scores consistent with the NAR
MT task [23]. Joint training with the AR decoder did not hurt BLEU
scores when candidate selection was not used. For length predic-
tion, we also investigated a simple approach by scaling the transcrip-
tion length ˆNsrc, which was obtained from the CTC ASR layer with
greedy decoding, by a constant value α, i.e., ˆN = (cid:98)α ˆNsrc(cid:99). Al-
though this works as well, we needed to tune α on the dev set on
each corpus, and therefore we adopted the classiﬁcation approach.

4.5. Effect of vocabulary size

Finally, we investigated the impact of vocabulary size. Fig. 2 shows
BLEU scores of AR and NAR E2E models as a function of the vo-
cabulary size on the Fisher-dev set. We observed AR models have a
peak around 1k BPE because the data size is relatively small (170-
hours). However, the performance of NAR models continued to im-
prove according to the vocabulary size until 16k. The candidate se-
lection with the AR decoder was beneﬁcial for all the vocabulary
sizes, especially for 32k. This is probably because misspelling was
alleviated thanks to many complete words in the large vocabulary,
which had a complementary effect on the conditional independence
assumption made in the NAR models. We also observed similar
trends in other corpora and CTC models.

5. CONCLUSION

In this work, we proposed a uniﬁed NAR decoding framework to
speed-up inference in the E2E-ST task, Orthros, with NAR and AR
decoders on the shared encoder. Selecting the better candidate with
the AR decoder greatly improved the effectiveness of a large length
beam in the NAR decoder. We also presented that using a large vo-
cabulary and parameters is effective for NAR E2E-ST models. The
best NAR E2E model reached a level of state-of-the-art AR Trans-
former model in the BLEU score while reducing inference latency
more than twice.

6. REFERENCES

[1] Hermann Ney, “Speech translation: Coupling of recognition and

translation,” in Proc. of ICASSP, 1999, pp. 517–520.

[2] Alexandre B´erard et al.,

“Listen and translate: A proof of
in Proc. of
concept for end-to-end speech-to-text translation,”
NeurIPS 2016 End-to-end Learning for Speech and Audio Pro-
cessing Workshop, 2016.

[3] Marcely Zanon Boito et al., “Unwritten languages demand atten-
tion too! word discovery with encoder-decoder models,” in Proc.
of ASRU, 2017, pp. 458–465.

[4] Ye Jia et al., “Leveraging weakly supervised data to improve end-
to-end speech-to-text translation,” in Proc. of ICASSP, 2019, pp.
7180–7184.

[5] Juan Pino et al., “Harnessing indirect training data for end-to-
end automatic speech translation: Tricks of the trade,” arXiv, pp.
arXiv–1909, 2019.

[6] Chengyi Wang et al., “Bridging the gap between pre-training and
ﬁne-tuning for end-to-end speech translation,” in Proc. of AAAI,
2020, pp. 9161–9168.

[7] Chengyi Wang et al., “Curriculum pre-training for end-to-end
speech translation,” in Proc. of ACL, 2020, pp. 3728–3738.
[8] Hirofumi Inaguma et al., “Multilingual end-to-end speech trans-

lation,” in Proc. of ASRU, 2019, pp. 570–577.

[9] Mattia Antonino Di Gangi et al., “One-to-many multilingual end-
to-end speech translation,” in Proc. of ASRU, 2019, pp. 585–592.
[10] Juan Pino et al., “Self-training for end-to-end speech translation,”

in Proc. of Interspeech, 2020.

[11] Elizabeth Salesky et al., “Exploring phoneme-level speech rep-
resentations for end-to-end speech translation,” in Proc. of ACL,
2019, pp. 1835–1841.

[12] Elizabeth Salesky et al., “Phone features improve speech transla-

tion,” in Proc. of ACL, 2020, pp. 2388–2397.

[13] Alexandre B´erard et al., “End-to-end automatic speech translation
of audiobooks,” in Proc. of ICASSP, 2018, pp. 6224–6228.

[14] Sameer Bansal et al.,

“Pre-training on high-resource speech
recognition improves low-resource speech-to-text translation,” in
Proc. of NAACL-HLT, 2019, pp. 58–68.

[15] Sathish Indurthi et al., “End-end speech-to-text translation with
modality agnostic meta-learning,” in Proc. of ICASSP, 2020, pp.
7904–7908.

[16] Ron J Weiss et al., “Sequence-to-sequence models can directly
translate foreign speech,” in Proc. of Interspeech, 2017, pp. 2625–
2629.

[17] Yuchen Liu et al., “End-to-end speech translation with knowledge
distillation,” in Proc. of Interspeech, 2019, pp. 1128–1132.
[18] Jan Niehues et al., “Dynamic transcription for low-latency speech
translation.,” in Proc. of Interspeech, 2016, pp. 2513–2517.
[19] Jiatao Gu et al., “Non-autoregressive neural machine translation,”

in Proc. of ICLR, 2018.

[20] Jason Lee et al.,

“Deterministic non-autoregressive neural se-
quence modeling by iterative reﬁnement,” in Proc. of EMNLP,
2018, pp. 1173–1182.

[21] Lukasz Kaiser et al., “Fast decoding in sequence models using
discrete latent variables,” in Proc. of ICML, 2018, pp. 2390–2399.
[22] Jindˇrich Libovick`y et al., “End-to-end non-autoregressive neural
machine translation with connectionist temporal classiﬁcation,” in
Proc. of EMNLP, 2018, pp. 3016–3021.

[23] Marjan Ghazvininejad et al., “Mask-predict: Parallel decoding of
conditional masked language models,” in Proc. of EMNLP, 2019,
pp. 6114–6123.

[24] Jiatao Gu et al., “Levenshtein Transformer,” in Proc. of NeurIPS,

2019, pp. 11181–11191.

[25] Mitchell Stern et al., “Insertion Transformer: Flexible sequence
generation via insertion operations,” in Proc. of ICML, 2019, pp.
5976–5985.

[26] Xuezhe Ma et al., “Flowseq: Non-autoregressive conditional se-
quence generation with generative ﬂow,” in Proc. of EMNLP,
2019, pp. 4273–4283.
[27] Marjan Ghazvininejad et al.,

“Aligned cross entropy for non-

autoregressive machine translation,” in Proc. of ICML, 2020.
[28] Chitwan Saharia et al., “Non-autoregressive machine translation
with latent alignments,” arXiv preprint arXiv:2004.07437, 2020.

[29] Marjan Ghazvininejad et al., “Semi-autoregressive training im-
proves mask-predict decoding,” arXiv preprint arXiv:2001.08785,
2020.

[30] Ashish Vaswani et al., “Attention is all you need,” in Proc. of

NeurIPS, 2017, pp. 5998–6008.

[31] Aaron Oord et al., “Parallel wavenet: Fast high-ﬁdelity speech

synthesis,” in Proc. of ICML, 2018, pp. 3918–3926.

[32] Yi Ren et al., “FastSpeech: Fast, robust and controllable text to

speech,” in Proc. of NeurIPS, 2019, pp. 3171–3180.

[33] N Chen et al.,

autoregressive Transformer for speech recognition,”
preprint arXiv:1911.04908, 2019.

“Listen and ﬁll in the missing letters: Non-
arXiv

[34] William Chan et al.,

imputation and dynamic programming,”
arXiv:2002.08926, 2020.

“Imputer: Sequence modelling via
arXiv preprint

[35] Yosuke Higuchi et al., “Mask CTC: Non-autoregressive end-to-
end ASR with CTC and mask predict,” in Proc. of Interspeech,
2020, pp. 3655–3659.

[36] Yuya Fujita et al., “Insertion-based modeling for end-to-end au-
tomatic speech recognition,” in Proc. of Interspeech, 2020, pp.
3660–3664.

[37] Ye Bai et al., “Listen attentively, and spell once: Whole sentence
generation via a non-autoregressive architecture for low-latency
speech recognition,” in Proc. of Interspeech, 2020, pp. 3381–
3385.

[38] Zhengkun Tian et al., “Spike-triggered non-autoregressive Trans-
in Proc. of Inter-

former for end-to-end speech recognition,”
speech, 2020, pp. 5026–5030.

[39] Junliang Guo et al., “Jointly masked sequence-to-sequence model
for non-autoregressive neural machine translation,” in Proc. of
ACL, 2020, pp. 376–385.

[40] Yoon Kim et al., “Sequence-level knowledge distillation,” in Proc.

of EMNLP, 2016, pp. 1317–1327.

[41] Chunting Zhou et al., “Understanding knowledge distillation in
non-autoregressive machine translation,” in Proc. of ICLR, 2019.
[42] Yi Ren et al., “A study of non-autoregressive model for sequence

generation,” in Proc. of ACL, 2020, pp. 149–159.

[43] Mattia A. Di Gangi et al., “MuST-C: a Multilingual Speech Trans-

lation Corpus,” in Proc. of NAACL-HLT, 2019, pp. 2012–2017.

[44] Matt Post et al., “Improved speech-to-text translation with the
Fisher and Callhome Spanish–English speech translation corpus,”
in Proc. of IWSLT, 2013.
[45] Ali Can Kocabiyikoglu et al.,

“Augmenting Librispeech with
French translations: A multimodal corpus for direct speech trans-
lation evaluation,” in Proc. of LREC, 2018.

[46] Hirofumi Inaguma et al., “ESPnet-ST: All-in-one speech transla-
tion toolkit,” in Proc. of ACL: System Demonstrations, 2020, pp.
302–311.

[47] Kishore Papineni et al., “Bleu: a method for automatic evaluation
of machine translation,” in Proc. of ACL, 2002, pp. 311–318.
[48] Daniel Povey et al., “The kaldi speech recognition toolkit,” in

Proc. of ASRU, 2011.

[49] Tom Ko et al., “Audio augmentation for speech recognition,” in

Proc. of Interspeech, 2015, pp. 3586–3589.

[50] Daniel S Park et al., “SpecAugment: A simple data augmentation
method for automatic speech recognition,” in Proc. of Interspeech,
2019, pp. 2613–2617.

[51] Philipp Koehn et al., “Moses: Open source toolkit for statistical
machine translation,” in Proc. of ACL: Demo and Poster Sessions,
2007, pp. 177–180.

[52] Rico Sennrich et al., “Neural machine translation of rare words
with subword units,” in Proc. of ACL, 2016, pp. 1715–1725.
[53] Taku Kudo, “Subword regularization: Improving neural network
translation models with multiple subword candidates,” in Proc. of
ACL, 2018, pp. 66–75.

[54] Diederik Kingma et al., “Adam: A method for stochastic opti-

mization,” Proc. of ICLR, 2015.

[55] Shinji Watanabe et al., “Hybrid CTC/attention architecture for
end-to-end speech recognition,” IEEE Journal of Selected Topics
in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[56] Jacob Devlin et al., “BERT: Pre-training of deep bidirectional
Transformers for language understanding,” in Proc. of NAACL-
HLT, 2019, pp. 4171–4186.

