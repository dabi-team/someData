HiVLP: Hierarchical Vision-Language
Pre-Training for Fast Image-Text Retrieval

Feilong Chen, Xiuyi Chen, Jiaxin Shi,
Duzhen Zhang, Jianlong Chang, and Qi Tian

Huawei Cloud Computing
ivess.chan@gmail.com, {shijiaxin3, jianlong.chang, tian.qi1}@huawei.com

Abstract. In the past few years, the emergence of vision-language pre-
training (VLP) has brought cross-modal retrieval to a new era. However,
due to the latency and computation demand, it is commonly challenging
to apply VLP in a real-time online retrieval system. To alleviate the de-
fect, this paper proposes a Hierarchical Vision-Language Pre-Training
(HiVLP) for fast Image-Text Retrieval (ITR). Specifically, we design a
novel hierarchical retrieval objective, which uses the representation of dif-
ferent dimensions for coarse-to-fine ITR, i.e., using low-dimensional rep-
resentation for large-scale coarse retrieval and high-dimensional represen-
tation for small-scale fine retrieval. We evaluate our proposed HiVLP on
two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.
Extensive experiments demonstrate that our HiVLP not only has fast
inference speed but also can be easily scaled to large-scale ITR scenar-
ios. The detailed results show that HiVLP is 1, 427∼120, 649× faster
than the fusion-based model UNITER and 2∼5× faster than the fastest
embedding-based model LightingDot in different candidate scenarios. It
also achieves about +4.9 AR on COCO and +3.8 AR on Flickr30K than
LightingDot and achieves comparable performance with the state-of-the-
art (SOTA) fusion-based model METER.

Keywords: Image-Text Retrieval, Hierarchical Pre-training, Vision-Language
Pre-training

1

Introduction

With the explosive growth of multimodal data on the Internet, how to retrieve
the expected data of different modalities fast and well is becoming an urgent
problem. Image-text retrieval (ITR), which aims to retrieve the relevant images
and texts across each other, has been widely studied because of its broad ap-
plication prospects. Various benchmarks have been constructed to promote the
development of ITR, such as Flickr30k [34], COCO [3] and Multi30K [10]

In terms of the cross-modal interaction, existing methods of ITR can be di-
vided into two categories: embedding-based models and fusion-based models.
Embedding-based models [11,44,40] employ separate image encoder and text
encoder to represent the image and text simultaneously and utilize their dot

2

Chen et al.

Fig. 1. Two kinds of Image-text Retrieval models. (a) Embedding-based Models with-
out cross-modal attention and fusion, (b) Fusion-based Models with cross-modal at-
tention and fusion.

product for similarity matching as shown in Figure 1 (a). Fusion-based mod-
els [21,22,46,51,32,4] apply cross-modal attention between textual features and
visual features for multimodal fusion as shown in Figure 1 (b). Some work [21,22,46]
improve visual representation by utilizing advanced region-based visual encoder
such as a pre-trained Faster R-CNN [35].

With the recent remarkable success of pre-training technologies [7,8], the
pretraining-finetune paradigm is widely demonstrated in vision-language ar-
eas [39,25,38,5,41,29], which also promotes the performance of ITR to new state-
of-the-arts. Different from early models [19,11,44] that are train from scratch
with ranking loss, current approaches [40,26] pre-train the transformer-based [43]
image/text/joint encoders [7] with millions of weakly-supervised image-text pairs,
and then finetune them with specific image-text downstream benchmarks. The
pre-training facilitates both embedding-based models [40] and fusion-based ones [32,4,26,12]
and thus has become a standard technology in the ITR area.

Embedding-based models and fusion-based models have different advantages.
From the accuracy aspect, fusion-based models inherently perform better than
embedding-based models because of the adequate cross-modal interaction [1].
However, fusion-based models have to fuse the query and all candidates on-
line, which will cause an intolerable latency in realistic search scenarios. Instead,
embedding-based models enjoy the searching efficiency because these models dis-
entangle the processing of images and texts and encode the massive candidates
offline, which greatly accelerates the real-time searching [40]. However, consid-
ering the huge amount of candidates on the Internet, it is still unaffordable to
compute so many dot products for each query. For example, given 1e9 candi-
dates and 768 hidden dimensions, the retrieval needs about 1.5 trillion operations
(multiplication plus addition), which is too costly even with a distributed clus-
ter. If we increase the feature dimension for better capability (i.e., 2048 in [30]),
the computation overhead will become heavier.

To this end, we propose a novel embedding-based model, HiVLP, short for
Hierarchical Vision-Language Pre-Training, to enable fast and accurate real-
time ITR over massive candidates. We encode the query and candidates into
hierarchical features of different dimensions to obtain the coarse-to-fine repre-
sentation. Low-dimensional features are used for fast and coarse retrieval over
the large pool to narrow the candidate scope, and then high-dimensional features
are used for accurate retrieval over the resultant small pool. Besides, hierarchi-

Image EncoderText EncoderDot（a）Image EncoderText EncoderMultimodal FusionJoint Encoder（b）HiVLP

3

cal features are produced gradually by different encoder layers so that we can
parallel the encoding process with the retrieval process, to reduce the overall
latency further.

Specifically, we insert an Early Output Layer (EOL) into the intermediate
transformer layers, which converts the internal hidden states to external repre-
sentation features. Starting from the bottom of the transformer stack, we increase
the output dimension of EOL layer-by-layer to support a coarse-to-fine repre-
sentation capability. For model training, we propose three training objectives
to jointly train the visual transformer encoder and the linguistic transformer
encoder: (1) Linguistic Representation Modeling (LRM) to ensure the model
learns rich representations for both images and text, (2) Hierarchical Retrieval
Learning (HRL) to ensure cross-modal alignment of different granularities, and
(3) Vision-Language Matching (VLM) to obtain accurate matching.

We evaluate the proposed approach on two popular ITR benchmarks, and
experiments show that our approach HiVLP is 1,427/6,426 × faster than the
existing fusion-based model UNITER on Flick30k/COCO and achieves a higher
performance. Moreover, we evaluate our approach on a larger candidate pool,
HiVLP further shows its efficiency.

Our contributions are summarized as follows:

– We propose a novel approach HiVLP which utilizes hierarchical retrieval to

accelerate image-text retrieval with slight performance degradation.

– We conduct extensive experiments on two popular ITR benchmarks and
evaluate our approach under larger candidates’ ITR settings. All the experi-
ments show that our approach dramatically improves the speed of image-text
retrieval and makes real-time ITR possible with low latency and high accu-
racy. HiVLP achieves with comparable accuracy and higher speed than both
fusion-based models and embedding-based models. (1, 427∼120, 649× faster
than UNITER, 2∼5× faster than fastest embedding-based model Lighting-
Dot and comparable performance with METER).

2 Related Work

Image text retrieval aims to retrieve images with text or text with images. The
researchers’ exploration of this task is divided into two aspects: (1) improving
retrieval performance (2) accelerating retrieval speed.

2.1 Improving retrieval performance

One way to improve retrieval performance is to apply heavy cross-modal in-
teraction between vision and language. SCAN [21] utilize stacked cross-modal
transformer blocks to handle the interaction. CAMP [46] proposes cross-modal
adaptive message passing to improve retrieval performance, which adaptively
controls the information flow for message passing across modalities. CAAN [51]
obtains high retrieval performance via a unified context-aware attention net-
work, which selectively focuses on critical local fragments (regions and words)

4

Chen et al.

by aggregating the global context. Another way to improve retrieval performance
is to utilize pre-training technology [45,47]. Due to the success of BERT [7] in
NLP and ViT [8] in CV, more and more researchers begin to explore the visual-
language pre-training models [39,25,38,5,41,29,40]. Visual-language pre-training
models learn the general vision-language representation and achieve amazing
performance via finetuning themselves on image-text retrieval datasets.

2.2 Accelerating retrieval speed

There are some previous work [40,42,17] that aims to accelerate retrieval speed.
LightingDot [40] utilizes two independent encoders to represent images and text
and align them via dot product without cross-attention. Note that LightingDot
uses a faster RCNN pre-trained to extract images. Due to the slow feature ex-
traction speed of fast RCNN and the non-end-to-end training of LightingDot,
this may lead to the reduction of speed and accuracy. ALIGN [17] leverages two
transformers to learn visual and language representations via large-scale image-
text pairs (1.8B). HEI [42] learns to map the original data points into short
binary hash codes and coarsely preserve the heterologous matching relationship
in order to accelerate retrieval speed.

2.3 Hierarchical Retrieval

SHAN [16] proposes a step-wise hierarchical alignment network to learn the
alignment between images and text, which seems similar to our work. But in
fact, the difference between our proposed approach and SHAN is very obvious.
There are two main differences. First, SHAN learns hierarchical alignment after
the image and text are encoded by encoders, but our proposed method is to let
the model learn the alignment of different dimensional hierarchical features in
the process of encoding the image and text by encoders. Second, the purpose
of hierarchical retrieval is different. Hierarchical retrieval in SHAN is for full
multimodal interaction, and ours is to speed up retrieval.

3 Approach

In this section, we first introduce image-text retrieval and then propose our model
with hierarchical pre-training, which consists of the visual transformer encoder,
the linguistic transformer encoder and the cross-modal encoder as shown in
Figure 2. Third, we present the objectives to train our model and finally, we
describe the inference process by utilizing our hierarchical pre-training model.

3.1 Task Definition

Image-text retrieval can be divided into two sub-tasks: (1) image annotation for
a given image and (2) image search for a given text. All the two sub-tasks are
to rank the candidates (texts or images) for a given image or text.

HiVLP

5

Fig. 2. An overview of our proposed approach HiVLP. HiVLP inverts an Early Output
Layer (EOL) into the intermediate transformer layers, which converts the internal
hidden states to external representation features.

3.2 Hierarchical Pre-training Model

Following BERT [7] and ViT [8], we utilize the transformer encoder modified
from [43] as our visual encoder and linguistic encoder.

Visual Transformer Encoder For a given image from a image-text pair (I, T ),
we reshape the 2D image I ∈ RH×W ×C into a sequence of 2D patches xp ∈
RN V ×(P 2·C), where (H, W ) is the resolution of the image, C is the channel
number, (P, P ) is the resolution of each image patch, and N V = HW/P 2 is the
resulting number of patches. And then, we flatten the patches and map them to
dh dimensions with a trainable linear projection. We regard the output of this
projection as the patch embeddings. We add a special token [IMG] to obtain the
global representation of the input image. We use standard learnable 1D position
embeddings to retain positional information. We add position embeddings to
the patch embeddings to obtain visual input features xV. We encode xV into
multiple levels of visual representations hV = {hV 0, . . . , hV N V } (hV j ∈ Rd)
by using LV -stacked Transformer blocks, where the l-th Transformer block is
denoted as Hl = Transformer(Hl−1), l ∈ [1, LV ]. Specifically, the representations
Hl is calculated by using the multi-head self-attention [43] as follows:

Q = Hl−1WQ

l , K = Hl−1WK

l , V = Hl−1WV
l ,

Al = softmax(

QKT
√
dk

)V,

(1)

(2)

where WQ
l ∈ Rdh×dk are learnable weights for computing the queries,
keys, and values respectively. Then Al is passed into a feedforward layer to

l , WK

l , WV

Visual Trans. Enc. BlocksLinguistic Trans. Enc. BlocksVisual Trans. Enc. BlocksLinguistic Trans. Enc. BlocksVisual Trans. Enc. BlocksLinguistic Trans. Enc. BlocksEOL1_VL R MVisual FeaturesLingustic FeaturesGlobal Visual FeaturesGlobal Linguistic FeaturesHRL: Hierarchical Retrieval LearningVLM: Vision Language MatchingLRM: Linguistic Representation ModelingEOL1_LHRLEOL2_VEOL2_LHRLEOL3_VEOL3_LHRLCross-modal EncoderV L M6

Chen et al.

compute Hl for the next layer:

We utilize Hl to represent hV
sentation.

Hl = FFN(Al)

(3)

l. We use hV

l
0 to denote l-th global visual repre-

Linguistic Transformer Encoder For a given text from a image-text pair (I, T ),
we employ WordPiece tokenizer [48] to split it into a word sequence w, where
each word is embedded with an absolute positional code following [7], to obtain
high-dimensional feature vectors w = {w0, w1, ..., wN T } (wj ∈ Rdw , N T is the
number of tokens). Similarly, we add a special token [TXT] and use LT -stacked
Transformer blocks to obtain the global representation hT = {hT 0, . . . , hT N T }
l
(hT j ∈ Rd) of linguistic features. We use hT
0 to denote l-th global linguistic
representation.

Cross-modal Encoder To obtain the fused representation of vision and language,
our cross-modal encoder is applied a cross-attention [43] and takes the represen-
tation from the linguistic transformer encoder as query input and the represen-
tation from the visual transformer encoder as key and value inputs. We regard
the fused representation of [TXT] as multimodal representation.

3.3 Training Objectives

We utilize three objectives to train our model: (1) Linguistic Representation
Modeling (LRM) to ensure the model learns rich representations for both im-
ages and text, (2) Hierarchical Retrieval Learning (HRL) to ensure hierarchical
retrieval from rough retrieval to fine retrieval, (3) Vision-Language Matching
(VLM) to obtain accurate matching.

Linguistic Representation Modeling (LRM) Similar to [7,40], we mask
15% tokens and train the model to reconstruct the masked tokens. Formally, we
denote wm = {wm1, . . . , wmM } as masked regions, where m ∈ NM is the set of
masked indices of size M , randomly sampled from a natural number N. w\m are
the unmasked tokens. Linguistic Representation Modeling is supervised by:

LLRM(T, I) = E(I,T )∼DCE(Pθ(wm|w\m, I))

= −

1
M

M
(cid:88)

k=1

log Pθmlm (wmk |hT mk + hV

l
0) ,

(4)

where θmlm and the word probabilities Pθ are conditioned on the corresponding
l
image I via the global image representation hV
0. Notice that we only utilize
the final linguistic representation of the last layer to train the model with the
linguistic representation modeling objective.

HiVLP

7

Fig. 3. An overview of image retrieval by using our proposed approach HiVLP at
inference. Starting from the bottom of the transformer stack, we increase the output
dimension of EOL layer-by-layer, to support a coarse-to-fine representation capability,
which compute image-text similarity with hierarchical representation to reduce the
candidate images and accelerate inference speed.

Hierarchical Retrieval Learning (HRL) As shown in Figure 2, in order to
better carry out image-text retrieval, we leverage paired image-text samples to
learn the strong correlation between them at each early output layer. The model
is trained to optimize the similarity of paired samples (I, T ) and vice versa. The
l-th layer similarity of (I, T ) is calculated by dot product as follows:

Sl(t, i) = ˆhT

l

0 ⊙ ˆhV

l
0,

(5)

l

l
0 and ˆhT
where ⊙ denotes the inner product between two vectors, and ˆhV
0
are the output [IMG] and [TXT] representation from l-th early ouput layer of
l
visual encoder and linguistic encoder, respectively. We transform hV
0 ∈
RdEOLl via a MLP projection named Early Output Layer (EOL). Similarly, We
l
0 ∈ RdEOLl via another MLP projection. In order to utilize
transform hT
training samples efficiently, we propose a in-batch retrieval learning, which close
the matched sample and push far the unmatched sample. Given a batch samples
with n image-text pairs (Ii, Ti), when we treat text Ti as the query, there are
one positive image Ii and n − 1 negative images. The text-to-image similarity of
l-th early output layer is defined as follows:

0 into ˆhT

0 into ˆhV

l

l

pl
t2v,Ti

=

exp(Sl(Ti, Ii))
k=1 exp(Sl(Ti, Ik))

(cid:80)n

,

(6)

Similarly, we take image Ii as query and compute image-to-text similarity pl
The final retrieval loss at l-th layer is:

v2t,Ii

.

Ll

RL =

1
2

E(I,T )∼D[CE(yt2v, pl

t2v,Ti

) + CE(yv2t, pl

v2t,Ii

)],

(7)

where CE(·) denotes the cross-entropy, yt2v and yv2t denotes the ground-truth
label and D denotes the training set. For each early output layer, there is a

Linguistic Trans. Enc. BlocksLinguistic Trans. Enc. BlocksLinguistic Trans. Enc. BlocksEOL1_LEOL2_LEOL3_LGlobal Visual FeaturesLingustic FeaturesGlobal Linguistic FeaturesVisual EncoderImagesDot Product8

Chen et al.

retrieval loss Ll

RL and the final hierarchical retrieval learning loss is defined as:

LHRL =

L
(cid:88)

l=1

Ll

RL.

(8)

Vision-Language Matching (VLM) To further enhance the model’s per-
formance, we utilize the cross-modal encoder to fuse the visual representation
and linguistic representation. We regard the fused representation of [TXT] from
the cross-modal encoder as multimodal representation. We feed the fused multi-
modal representation of both modalities to an FC layer and a sigmoid function
to predict a score between 0 and 1, where 0 indicates the vision and language
are mismatched, and 1 indicates the vision and language are matched as follows:

LVLM = E(I,T )∼DCE(y, p(zLV

, hLT

)),

(9)

where D is the training dataset, p(·) denotes the transformer decoder blocks and
y denotes the ground-truth label.

3.4 Real-Time Inference

As shown in Figure 3, similar to most retrieval systems, our approach consists
of two phrases at real-time inference: (1) Offline Representation Extraction and
(2) Online Hierarchical Retrieval. We take text-to-image retrieval as an example
to introduce our hierarchical retrieval. The purpose of Offline Representation
Extraction is to extract the features of candidate images or candidate texts
extracted by visual encoder or text encoder offline before retrieval.

Online Hierarchical Retrieval During inference, given a text query T , we
encode it with our linguistic transformer encoder to obtain hierarchical global
representation and compute its similarity with small latency to reduce the candi-
date images as shown in Figure 3. For l-th early output layer, we propose top-Kl
retrieval to reduce image candidates and obtain a list of Kl images IT . At l+1-th
early output layer, the model only needs to retrieve top-Kl+1 images from Kl
images instead of all the image candidates KALL (Kl+1 ≪ Kl ≪ KALL), which
significantly reduces the retrieval time. In addition, the query representation ex-
traction and the similarity computing are processed in parallel approximatively,
which further reduces the retrieval time.

Retrieval Time Analysis The retrieval time consumption of embedding-based
models comes from the query encoding and the similarity calculation. We as-
sume that the encoding time of each layer is te, and the calculated time of
each dot product similarity is td
s for d-dimensional vectors. For a text-to-image

Model

COCO Test (5k images)

Flickr30K Test (1k images)

Text Retrieval

Image Retrieval

Text Retrieval

Image Retrieval

R@1 R@5 R@10 R@1 R@5 R@10 AR R@1 R@5 R@10 R@1 R@5 R@10 AR

Fusion-based Model

HiVLP

9

GXN [13]
SCAN-single [21]
R-SCAN [22]
BFAN
CAMP [46]
CAAN [51]
GSMN [28]
IMRAN [2]
CAMP-HEI [42]
GSMN-HEI [42]
IMRAN-HEI [42]
BFAN-HEI [42]
SHAN [16]
UNITER [4]
OSCAR [26]
ALBEF [24]
VinVL [50]
ViLT [18]
Pixel-BERT [15]
Visual Parsing [49]
METER [9]

VSE++ [11]
SCO [14]
LD [40]
LD+OSCAR [40]
ALIGN∗ [17]

-

-

-

-

-

-

56.8

80.0

74.6

89.6 41.5

84.7 31.7

42.0
46.4 77.4 87.2 34.4 63.7 75.7 64.1 67.9 89.0 94.4 43.9 74.2 82.8 75.4
45.4 77.9 87.9 36.2 65.6 76.7 65.0 66.3 90.6 96.0 51.4 77.8 84.9 77.8
49.9 79.5 88.8 36.9 65.7 77.2 66.3 69.2 91.4 96.2 50.0 77.2 84.8 78.1
50.1 82.1 89.7 39.0 68.9 80.2 68.3 68.1 89.7 95.2 51.5 77.1 85.3 77.8
52.5 83.3 90.9 41.2 70.3 82.9 70.2 70.1 91.6 97.2 52.8 79.0 87.9 79.8
49.4 79.3 88.8 35.9 65.5 76.9 66.0 73.3 91.8 96.4 52.4 79.2 86.3 79.9
52.5 81.2 89.8 39.1 68.4 79.5 68.4 71.0 92.0 96.3 53.1 79.9 86.2 79.7
43.4 75.0 86.2 32.3 63.2 75.1 62.5 67.6 90.9 95.0 52.6 78.2 84.4 78.1
49.3 79.3 88.8 35.9 65.4 76.7 65.9 73.4 91.9 96.7 52.4 79.0 86.0 79.9
52.5 81.3 89.8 35.9 65.4 76.7 66.9 71.0 92.0 96.4 53.2 79.7 85.8 79.7
49.6 79.1 88.4 36.9 65.7 77.2 66.2 69.2 91.2 96.2 49.9 77.2 84.6 78.1
74.6 93.5 96.9 55.3 81.3 88.4 81.7
64.4 87.4 93.1 50.3 78.5 87.2 76.8 85.9 97.1 98.8 72.5 92.3 95.9 90.4
70.0 91.1 95.5 54.0 80.8 88.5 79.8
73.1 91.4 96.0 56.8 81.5 89.2 81.3 94.3 99.4 99.8 82.8 96.7 98.4 95.2
74.6 92.6 96.3 58.1 83.2 90.1 82.5
61.5 86.3 92.7 42.7 72.9 83.1 73.2 83.5 96.7 98.6 64.4 88.7 93.8 87.6
63.6 87.5 93.6 50.1 77.6 86.2 76.4 87.0 98.9 99.5 71.5 92.1 95.8 90.8
87.0 98.4 99.5 73.5 93.1 96.4 91.3
76.2 93.2 96.8 57.1 82.7 90.1 82.6 94.3 99.6 99.9 82.2 96.3 98.4 95.1

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

Embedding-based Model

41.3 69.2 81.2 30.3 59.1 72.4 58.9 52.9 80.5 87.2 39.6 70.1 79.5 68.3
42.8 72.3 83.0 33.1 62.9 75.5 61.6 55.5 82.0 89.3 41.1 70.5 81.1 69.9
60.1 85.1 91.8 45.8 74.6 83.8 73.5 83.9 97.2 98.6 69.9 91.1 95.2 89.3
74.2 92.4 96.0 57.4 82.7 89.9 82.1
77.0 93.5 96.9 59.9 83.3 89.8 83.4 95.3 99.8 100.0 84.9 97.4 98.6 96.0

-

-

-

-

-

-

-

71.7 91.4 96.0 52.3 78.8 86.8 79.4 92.6 99.3 99.9 79.8 95.3 97.7 94.1
HiVLP (Ours)
HiVLP + CM (Ours) 76.6 94.2 97.1 60.0 83.8 90.4 83.7 95.5 99.7 100.0 84.0 96.9 98.4 95.8
Table 1. Comparision with both fusion-based models and embedding-based models.
Our approach achieves comparable performance with a faster inference speed. “LD”
denotes “LinghtDot”. * denotes that ALIGN utilizes 1.8B image-text data for training.

retrieval, given a query text T with N image candidates, the traditional infer-
ence time based on 12 layer encoders is 12te + N td
s. Considering our hierar-
chical retrieval encoders with 3 early output layers, the inference time of our
approach is 4te + max(4te, N1td1
s. When there are
N = 109 image candidates and we adopt N1 = 109, N2 = 105, N3 = 100 and
d = 768, d1 = 128, d2 = 300, under the assumption that each multiplication
consumes unit time 1 and the encoding time te = 103, the traditional inference
time is about 7 × 1011 and our inference time is about 1 × 1011. Our proposed
hierarchical retrieval is ∼ 7× faster than the traditional embedding-based model
settings based on the same model structure.

s ) + max(4te, N2td2

s ) + N3td

10

Chen et al.

Model

Text Retrieval

Image Retrieval

Text Retrieval

Image Retrieval

COCO-Full (123K Images)

Flickr30K-Full (31K Images)

R@5 R@10 R@20 R@5 R@10 R@20 AR R@5 R@10 R@20 R@5 R@10 R@20 AR

LightingDot
40.1 51.0
HiVLP (Ours) 54.3 65.6

62.0 28.2 37.4
69.5 42.8 55.4

47.8 44.4 69.6 78.9
60.6 58.3 78.5 94.1

86.1 51.8 62.3
97.4 60.5 84.2

72.3 70.2
90.7 84.2

Table 2. Results on the large retrieval setting of full Flickr30k and full COCO datasets.

4 Experiments

4.1 Datasets and Implement Details

Datasets Similar to most vision-language pre-training models [4,23,40], we
utilize four image-text pairs datasets with 4.1 million images and 9.5 million
associated captions for training, including COCO [27], Visual Genome [20],
Conceptual Captions [37], and SBU captions [33]. We use Flickr30k [34] and
COCO [27] datasets for evaluation, which include 31K images from Flickr30K
(29K/1k/1k images for train, validation and test, respectively) and 123K images
from COCO (114K/5K/5K for train, validation and test, respectively). Each
image from Flickr30k and COCO is associated with 5 human-written captions.

Implement Details We use the visual transformer encoder and linguistic trans-
former encoder, which has 12/6 layers, 768 hidden states and 12 heads, respec-
tively. The cross-modal encoder has 6 layers, 768 hidden states and 12 heads.
We utilize AdamW [31] with β1=0.9, β2=0.98 to optimize the model training.
Moreover, a learning rate warm-up strategy is adopted, where the learning rate
1e-4 is linearly increased during the first 10% of training steps, followed by a
linear decay to 1e-5 in pre-training and 1e-5 to 1e-6 in finetune. The L2 weight
decay is 0.01. We set the batch size to 32 per GPU. We conduct pre-training
experiments on 8× A100 GPUs with 6-step gradient accumulation. We train our
model for 10 epochs on Flickr30k, and 10 epochs on COCO. In our experiments,
we utilize 3 EOL layers. We set N2 = 1000 and N3 = 100 for all the evaluate
sets except for COCO FULL where we set N2 = 5000 and N3 = 1000. Notice
that N1 equals the number of candidates N of each evaluation set.

4.2 Evaluation Metrics

We measure image-text retrieval by recall@K (R@K) and the average R@K
(AR) for all K for both image-to-text retrieval and text-to-image retrieval tasks.
All metrics are the higher the better.

4.3 Results

We compare our approach HiVLP with the mainstream state-of-the-art mod-
els under the standard retrieval settings and the large candidate settings. The
inference speed comparision can be seen in Table 4 for details.

HiVLP

11

Model

Text Retrieval

Image Retrieval

R@1 R@5 R@10 R@1 R@5 R@10 AR

HiVLP w/ HRL1 56.3 81.1 87.3 36.9 67.0 78.1 66.8
HiVLP w/ HRL2 71.9 92.4 96.8 56.2 81.0 88.7 81.2
HiVLP w/ HRL3 93.7 99.1 99.7 79.5 95.0 97.3 94.1
92.6 99.3 99.9 79.8 95.3 97.7 94.1
HiVLP
95.5 99.7 100.0 84.0 96.9 98.4 95.8
HiVLP + CM

Table 3. Ablation studies on training strategy over Flickr30K. “HiVLP w/ HRL1”
denotes we evaluate the model by utilizing the outputs of first EOL layer. “HiVLP w/
HRL2” denotes we evaluate the model by utilizing the outputs of second EOL layer.
“HiVLP w/ HRL2” denotes we evaluate the model by utilizing the outputs of last EOL
layer.

Base Models The compared previous models are divided into two kinds.
Fusion-based models: GXN [13], SCAN [21], R-SCAN [22], CAMP [46], CAAN [51],
ViLBERT [32], Unicoder-VL [23], UNITER [4], OSCAR [26], VinVL [50], AL-
BEF [24], METER [9] and so on. Embedding-based models: VSE++ [11], SCO [14],
LightingDot [40], ALIGN [17]. Notice that ALIGN utilizes 1.8B image-text data
for training. For fair comparison, we do not compare with ALIGN.

Results under standard retrieval settings Performance on two benchmarks
COCO and Flickr30K is shown in Table 1. “+OSCAR” denotes LightingDot
uses OSCAR as a Re-Ranker. “+CM” denotes we utilize our cross-modal en-
coder to re-rank the results of the last EOL layer. From the results, we can
observe that: (1) Compared with embedding-based models. Our approach out-
performs non-pre-training approaches (VSE++, SCO) by large margins on all
metrics. Specifically, our HiVLP achieves 79.4% on AR on COCO and 94.1%
on Flickr30K on AR, beating SCO by +17.8% on AR on COCO Test and
+24.2% on Flickr30K Test. When comparing with pre-training models, our ap-
proach is better than LightingDot on performance, and our HiVLP is much faster
than LightingDot (2.23× faster on Flickr30K-test, 3.33× faster on COCO-test).
Our approach HiVLP achieves better performance compared with the existing
fastest pre-training model LightingDot (+5.9% AR on COCO and +4.8% AR
on Flickr30K) and is still much faster than it (see Table 4), which shows the
efficiency of the proposed hierarchical retrieval structure.

(2) Compared with models with fusion-based. HiVLP also outperforms the
non-pre-training models (CXN, CAAN and etc.) by a significant margin. Al-
though HiVLP achieves slightly lower performance on some metrics than pre-
training models with cross-attention, such as the SOTA model METER, HiVLP
is much faster than pre-training models with cross-attention. Notice that HiVLP
is ∼1,400/6,400× faster than UNITER during inference time and gains 3.7/2.6
points on Flickr30K/COCO simultaneously.

12

Chen et al.

Method
Flickr30K-test
COCO-test
Flickr30K-full
COCO-full

#images SCAN LightingDot HiVLP (Ours)
639×
1.8×
1,000
1,927×
1.9×
5,000
31,014
6,591×
1.8×
123,287 1.9× 23,869×

1,427×
6,426×
36,051×
120,649×

Table 4. Speedup w.r.t. UNITER-base. We compare HiVLP (Ours) and Lightningdot,
plus a lightweight cross-attention method SCAN [21].

Results under large candidate settings To further demonstrate the effi-
ciency of our approach HiVLP, we compare our approach with previous work
under large candidate settings following [40] as shown in Table 2. HiVLP out-
performs LignthingDot with a significant margin (+14.0% AR on Flickr30K
and +13.9% AR on COCO) with much faster inference speed (5.47× faster on
Flickr30K-Full, 5.05× faster on COCO-Full as shown in Table 4).

4.4 Retrieval Time Comparison

Following LightingDot [40], we use UNITER as the baseline to compare inference
speed in order to demonstrate the efficiency of our HiVLP as shown in Table 4.
In addition, SCAN [21] is also compared, which uses GRU [6] instead of a 12-
layer Transformer with a more lightweight cross-attention. We test HiVLP and
LightingDot on the same GPU and other results are from [40]. As shown in Table
4, SCAN is ∼1.9× faster than UNITER across both benchmarks because the
computational cost of GRU is much cheaper than that of Transformer, but SCAN
drops much performance. However, SCAN still utilizes cross-attention and the
speedup from SCAN is limited. In contrast, the embedding-based models without
cross-attention are much faster than UNITER. Lightningdot is 639× faster than
UNITER on Flickr30K while our HiVLP is 1,427× faster. HiVLP is still 2.23×
than Lightningdot. When tested with five times more images in COCO, the
speedup from HiVLP is 6426× faster than UNITER while LightingDot is 1927×.
HiVLP is 3.33× faster than Lightningdot.

Our purpose is to build a real-time image-text retrieval system. Thus we test
HiVLP in a simulated real-life scenario as shown in Table 2. The simulated real-
life scenario contains hundreds of thousands of images (text) where we combine
all images (text) from training, validation and test set to form a larger candidate
pool following [40]. We regard this setting on both benchmarks as Flickr30k-
full (31k) and COCO-full (123k), and models are still trained on the training
set following [40]. The number of candidate images (texts) scales up by >20×
in the simulated real-life scenario. With the same number of text quires, the
fusion-based models immediately become impractical. As shown in Table 4, our
HiVLP is 36,051× faster on Flickr30k-full and 120,649× faster on COCO-full
than UNITER. Moreover, HiVLP is 5.47× and 5.05× faster than LightingDot
on Flickr30k-Full and on COCO-Full, respectively.

HiVLP

13

Fig. 4. Visualization of GradCam for each word. Each image corresponds to a word.

4.5 Ablation Study

In order to comprehensively analyze our model, we conduct the ablation study
from the aspect of hierarchical retrieval strategy.

To verify the effectiveness of our hierarchical retrieval strategy, the ablation
study is shown in Table 3. We utilize the outputs of different EOL layers to
evaluate the models for image-to-text retrieval and text-to-image retrieval on
Flickr30K. From the results

4.6 Qualitative Examples

As shown in Figure 4, we visualize GradCam [36] for each word. The results
show that our model aligns words well with visual semantics and thus obtain
satisfactory retrieval performance. As shown in Figure 5, 6, 7 and 8, we show
examples of image retrieval results. All the 10 images are retrieved by our model.
The ground truth image is grounded in the green rectangle. Our model usually
retrieves the right images in the top 10 candidates, which is satisfies people’s
sense of experience in multimodal Internet search.

5 Conclusion

In this paper, we propose Hierarchical Vision-Language Pre-Training (HiVLP)
for fast image-text retrieval (ITR). Specifically, we propose a novel hierarchi-
cal retrieval objective, which uses the representation of different dimensions for
image-text retrieval from coarse-grained to fine-grained retrieval. Specifically,
HiVLP use low dimensional representation for large-scale rough retrieval and
high-dimensional representation for small-scale fine retrieval. Experiments on
two benchmarks show that HiVLP is 1, 427∼120, 649× faster than the fusion-
based models UNITER and 2∼5× faster than the fastest embedding-based model
LightingDot. Moreover, HiVLP achieves the higher performance (about +4.9 AR
on COCO and +3.8 AR on Flickr30K than LightingDot) simultaneously.

A young boy throws a big rock into a body of water.Ayoungboythrowsabigrockintoabodyofwarter14

Chen et al.

Fig. 5. Retrieved top 10 images for the query ”ice hockey player during the first period
of a hockey game against sports team”. The ground truth is in the green rectangle.

Fig. 6. Retrieved top 10 images for the query ”a group of boys in a crowd”. The ground
truth is in the green rectangle.

Fig. 7. Retrieved top 10 images for the query ”person on the summit in the snow”.
The ground truth is in the green rectangle.

Fig. 8. Retrieved top 10 images for the query ”silhouette of people walking on the beach
at sunset”. The ground truth is in the green rectangle.

HiVLP

15

References

1. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
L.: Bottom-up and top-down attention for image captioning and visual question
answering. In: CVPR (2018)

2. Chen, H., Ding, G., Liu, X., Lin, Z., Liu, J., Han, J.: Imram: Iterative matching with
recurrent attention memory for cross-modal image-text retrieval. In: Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition. pp.
12655–12663 (2020)

3. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll´ar, P., Zitnick, C.L.:
Microsoft coco captions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325 (2015)

4. Chen, Y.C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.:

Uniter: Universal image-text representation learning. In: ECCV (2020)

5. Cho, J., Lei, J., Tan, H., Bansal, M.: Unifying vision-and-language tasks via text

generation. In: ICML (2021)

6. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur-
rent neural networks on sequence modeling. In: Deep Learning and Representation
Learning Workshop. NeurIPS (2014)

7. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-

rectional transformers for language understanding. In: NAACL-HLT (2019)

8. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
An image is worth 16x16 words: Transformers for image recognition at scale (2020)
9. Dou, Z.Y., Xu, Y., et al.: An Empirical Study of Training End-to-End Vision-and-

Language Transformers. arXiv preprint arXiv:2111.02387 (2021)

10. Elliott, D., Frank, S., Sima’an, K., Specia, L.: Multi30k: Multilingual english-
german image descriptions. In: Proceedings of the 5th Workshop on Vision and
Language. Association for Computational Linguistics (2016)

11. Faghri, F., Fleet, D.J., Kiros, J.R., Fidler, S.: Vse++: Improving visual-semantic

embeddings with hard negatives. In: BMVC (2017)

12. Gan, Z., Chen, Y.C., Li, L., Zhu, C., Cheng, Y., Liu, J.: Large-scale adversarial
training for vision-and-language representation learning. In: NeurIPS (2020)
13. Gu, J., Cai, J., Joty, S.R., Niu, L., Wang, G.: Look, imagine and match: Improving
textual-visual cross-modal retrieval with generative models. In: CVPR (2018)
14. Huang, Y., Wu, Q., Song, C., Wang, L.: Learning semantic concepts and order for

image and sentence matching. In: CVPR (2018)

15. Huang, Z., Zeng, Z., Liu, B., Fu, D., Fu, J.: Pixel-bert: Aligning image pixels with
text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849 (2020)
16. Ji, Z., Chen, K., Wang, H.: Step-wise hierarchical alignment network for image-text

matching. arXiv preprint arXiv:2106.06509 (2021)

17. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.V., Sung, Y., Li,
Z., Duerig, T.: Scaling up visual and vision-language representation learning with
noisy text supervision. In: International Conference on Machine Learning (2021)

18. Kim, W., Son, B., et al.: ViLT: Vision-and-language transformer without convolu-

tion or region supervision. arXiv preprint arXiv:2102.03334 (2021)

19. Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings
with multimodal neural language models. In: Deep Learning and Representation
Learning Workshop. NeurIPS (2014)

16

Chen et al.

20. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language
and vision using crowdsourced dense image annotations. IJCV (2017)

21. Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.: Stacked cross attention for image-text

matching. In: ECCV (2018)

22. Lee, K.H., Palangi, H., Chen, X., Hu, H., Gao, J.: Learning visual relation priors
for image-text matching and image captioning with neural scene graph generators.
arXiv preprint arXiv:1909.09953 (2019)

23. Li, G., Duan, N., Fang, Y., Jiang, D., Zhou, M.: Unicoder-vl: A universal encoder

for vision and language by cross-modal pre-training. In: AAAI (2020)

24. Li, J., Selvaraju, R.R., Gotmare, A.D., Joty, S., Xiong, Caiming andHoi, S.: Align
before fuse: Vision and language representation learning with momentum distilla-
tion. Tech. Rep. arXiv:2107.07651 (2021)

25. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple
and performant baseline for vision and language. arXiv preprint arXiv:1908.03557
(2019)

26. Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang, L., Hu, H., Dong, L.,
Wei, F., et al.: Oscar: Object-semantics aligned pre-training for vision-language
tasks. In: ECCV (2020)

27. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)
28. Liu, C., Mao, Z., Zhang, T., Xie, H., Wang, B., Zhang, Y.: Graph structured
network for image-text matching. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 10921–10930 (2020)

29. Liu, J., Zhu, X., Liu, F., Guo, L., Zhao, Z., Sun, M., Wang, W., Lu, H., Zhou, S.,
Zhang, J., et al.: Opt: Omni-perception pre-trainer for cross-modal understanding
and generation. arXiv preprint arXiv:2107.00249 (2021)

30. Liu, W., Zhou, P., Zhao, Z., Wang, Z., Deng, H., Ju, Q.: Fastbert: a self-distilling

bert with adaptive inference time. arXiv preprint arXiv:2004.02178 (2020)

31. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2019)
32. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin-

guistic representations for vision-and-language tasks. In: NeurIPS (2019)

33. Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million

captioned photographs. In: NeurIPS (2011)

34. Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazeb-
nik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer
image-to-sentence models. In: ICCV (2015)

35. Ren, S., He, K., et al.: Faster R-CNN: towards real-time object detection with

region proposal networks. In: NeurIPS (2015)

36. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-
cam: Visual explanations from deep networks via gradient-based localization. In:
Proceedings of the IEEE international conference on computer vision. pp. 618–626
(2017)

37. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In: ACL
(2018)

38. Shen, S., Li, L.H., Tan, H., Bansal, M., Rohrbach, A., Chang, K.W., Yao, Z.,
Keutzer, K.: How much can clip benefit vision-and-language tasks? arXiv preprint
arXiv:2107.06383 (2021)

39. Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: Vl-bert: Pre-training of

generic visual-linguistic representations. In: ICLR (2020)

HiVLP

17

40. Sun, S., Chen, Y.C., Li, L., Wang, S., Fang, Y., Liu, J.: Lightningdot: Pre-training
visual-semantic embeddings for real-time image-text retrieval. In: NAACL-HLT
(2021)

41. Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations from

transformers. In: EMNLP (2019)

42. Tu, R.C., Ji, L., Luo, H., Shi, B., Huang, H.Y., Duan, N., Mao, X.L.: Hashing
based efficient inference for image-text matching. In: Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021. pp. 743–752 (2021)

43. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,

(cid:32)L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)

44. Wang, L., Li, Y., Huang, J., Lazebnik, S.: Learning two-branch neural networks for
image-text matching tasks. IEEE Transactions on Pattern Analysis and Machine
Intelligence (2018)

45. Wang, W., Bao, H., et al.: VLMo: Unified vision-language pre-training with

mixture-of-modality-experts. arXiv (2021)

46. Wang, Z., Liu, X., Li, H., Sheng, L., Yan, J., Wang, X., Shao, J.: Camp: Cross-

modal adaptive message passing for text-image retrieval. In: ICCV (2019)

47. Wang, Z., Yu, J., et al.: SimVLM: Simple visual language model pretraining with

weak supervision. arXiv preprint arXiv:2108.10904 (2021)

48. Wu, Y., Schuster, M., et al.: Google’s neural machine translation system: Bridging

the gap between human and machine translation. CoRR (2016)

49. Xue, H., Huang, Y.a.: Probing inter-modality: Visual parsing with self-attention

for vision-language pre-training. arXiv preprint arXiv:2106.13488 (2021)

50. Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., Gao, J.: Vinvl:
Revisiting visual representations in vision-language models. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5579–
5588 (2021)

51. Zhang, Q., Lei, Z., Zhang, Z., Li, S.Z.: Context-aware attention network for image-

text retrieval. In: CVPR (2020)

