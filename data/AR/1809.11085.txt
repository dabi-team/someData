SHIFTED CHOLESKYQR FOR COMPUTING THE QR
FACTORIZATION OF ILL-CONDITIONED MATRICES

TAKESHI FUKAYA∗, RAMASESHAN KANNAN† , YUJI NAKATSUKASA‡ , YUSAKU
YAMAMOTO§ , AND YUKA YANAGISAWA¶

Abstract. The Cholesky QR algorithm is an eﬃcient communication-minimizing algorithm for
computing the QR factorization of a tall-skinny matrix. Unfortunately it has the inherent numerical
instability and breakdown when the matrix is ill-conditioned. A recent work establishes that the
instability can be cured by repeating the algorithm twice (called CholeskyQR2). However, the
applicability of CholeskyQR2 is still limited by the requirement that the Cholesky factorization of the
Gram matrix runs to completion, which means it does not always work for matrices X with κ2(X) (cid:38)
u− 1
2 where u is the unit roundoﬀ. In this work we extend the applicability to κ2(X) = O(u−1)
by introducing a shift to the computed Gram matrix so as to guarantee the Cholesky factorization
RT R = AT A + sI succeeds numerically. We show that the computed AR−1 has reduced condition
number ≤ u− 1
2 , for which CholeskyQR2 safely computes the QR factorization, yielding a computed Q
of orthogonality (cid:107)QT Q−I(cid:107)2 and residual (cid:107)A−QR(cid:107)F /(cid:107)A(cid:107)F both O(u). Thus we obtain the required
QR factorization by essentially running Cholesky QR thrice. We extensively analyze the resulting
algorithm shiftedCholeskyQR3 to reveal its excellent numerical stability. shiftedCholeskyQR3 is also
highly parallelizable, and applicable and eﬀective also when working in an oblique inner product
space. We illustrate our ﬁndings through experiments, in which we achieve signiﬁcant (up to x40)
speedup over alternative methods.

Key words. QR factorization, Cholesky QR factorization, oblique inner product, roundoﬀ error

analysis, communication-avoiding algorithms,

AMS subject classiﬁcations. 65F30, 15A23, 65F15, 15A18, 65G50

1. Introduction. Computing the QR factorization X = QR is required in var-
ious applications in scientiﬁc computing. The Cholesky QR algorithm computes the
factorization by:

(1.1)

(1.2)

(1.3)

A = X (cid:62)X,
R = chol(A),
Q = XR−1,

where chol(A) denotes the Cholesky factor of A. Cholesky QR is a communication-
avoiding algorithm whose communication cost is equivalent to that of the TSQR
algorithm, which has been devised speciﬁcally to reduce communication for the QR
factorization of tall-skinny matrices [3]. Cholesky QR has the advantage over TSQR
that its arithmetic cost is about half and that its reduction operator is addition, while
that of TSQR is a QR factorization of a small matrix [4]. As a result, Cholesky
QR usually runs faster than TSQR. However, Cholesky QR is rarely used in prac-
tice because of its instability: the distance from orthogonality of its computed Q

8
1
0
2

p
e
S
8
2

]

A
N
.
h
t
a
m

[

1
v
5
8
0
1
1
.
9
0
8
1
:
v
i
X
r
a

∗ Hokkaido University, Hokkaido, Japan (fukaya@iic.hokudai.ac.jp)
†Arup,

3 Piccadilly Place, Manchester M1

3BN, United Kingdom.

(Ramase-

shan.Kannan@arup.com)

‡National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan (nakat-

sukasa@nii.ac.jp)

§The University of Electro-Communications, Tokyo, Japan / JST CREST, Tokyo, Japan

(yusaku.yamamoto@uec.ac.jp)

¶Waseda university, Waseda Research Institute for Science and Engineering, Tokyo, Japan

(yuuka@aoni.waseda.jp)

1

 
 
 
 
 
 
2

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

grows rapidly with the condition number of the input matrix. By contrast, TSQR is
unconditionally stable.

A recent work by the authors [16] establishes that the instability can be cured
signiﬁcantly by repeating the algorithm twice (called CholeskyQR2). However, the
applicability of CholeskyQR2 is still limited by the requirement that the Cholesky
factorization of the Gram matrix runs to completion, which means it does not always
work for matrices A with κ2(X) = O(u− 1
2 ) or larger, where u is the unit roundoﬀ.
In this work we extend the applicability of CholeskyQR-based algorithms to κ2(X) =
O(u−1).

The idea is to execute a preconditioning step so that the conditioning is improved
to a point where CholeskyQR2 is applicable. How do we ﬁnd an eﬀective precondi-
tioner? An inspiration to answer this is the fact that Cholesky QR and CholeskyQR2
belong to the category of triangular orthogonalization type algorithms [15, Lecture
10], in contrast to Householder type algorithms, which follows the principle of orthog-
onal triangularization. We summarize the classiﬁcation of algorithms in terms of their
principle, communication cost, and stability in Figure 1.2, which also clariﬁes where
our contribution (shifted CholeskyQR3) stands.

Fig. 1.1: Classiﬁcation of QR factorization algorithms.

In triangular orthogonalization, one right-multiplies an appropriate upper tri-
angular matrix R so that κ2(AR) = 1. Now, what if our goal is merely κ2(AR) =
O(u−1/2)? Once we have this, we can safely compute the QR factorization AR = QR1
using CholeskyQR2, to arrive at the overall QR factorization A = Q(R1R−1).

Clearly, such R is not unique, and we propose one way of ﬁnding such R. Namely,
as in Cholesky QR we compute the Gram matrix, but add a small shift sI so as
to guarantee the Cholesky factorization RT R = AT A + sI does not break down
numerically. We show that under the mild assumption κ2(A) ≤ u−1, the resulting
AR−1 (note that R−1 is also triangular) has reduced condition number ≤ u− 1
2 , for
which CholeskyQR2 safely computes the QR factorization, yielding computed Q, R
with excellent orthogonality (cid:107)QT Q − I(cid:107) = O(u) and residual (cid:107)A − QR(cid:107)F /(cid:107)A(cid:107)F =
O(u), overall a backward stable QR factorization. The algorithm is deceptively simple
(essentially the only new ingredient being the introduction of a shift); the analysis is
however not trivial. We give detailed analysis that gives the constants hidden in the
O(u) notation.

The main message of this paper is that for any matrix with condition number
well above u− 1
2 (but bounded by u−1), the QR factorization can be computed in a
backward stable manner by essentially running Cholesky QR thrice. We refer to

Orthogonal triangularization()Triangular orthogonalization()Householder QRTSQR (CAQR)Cholesky QRCholeskyQR2mixed-precisionCholesky QRshifted CholeskyQR3(this research)CGSMGSCGS2MGS2improvement of accuracy/stabilityCommunication expensive Communication avoidingSHIFTED CHOLESKYQR

3

this overall algorithm as shiftedCholeskyQR3; Figure 1.2 shows its diagram, and
how κ2(A) is reduced eventually to 1 through repeated multiplication by triangular
matrices.

Fig. 1.2: Diagram illustrating shiftedCholeskyQR3.

Let us comment on related studies in the literature. A recent work of Yamazaki et
al. [18] uses doubled precision arithmetic (e.g., quadruple precision when using double
precision) for the ﬁrst two steps (1.1), (1.2), and shows that the condition number gets
reduced by about O(u−1), and thus the QR factorization will be obtained by repeating
the process. This results in about 8.5 times as many arithmetic operations (per
iteration) as does the standard CholeskyQR without doubled precision. Moreover,
this approach clearly requires that higher-precision arithmetic is available, which may
not always be the case (and even when it is, it often comes with a signiﬁcant price
in speed). shiftedCholeskyQR3 developed in this paper does not require change of
arithmetic precision (in our experiments we use only IEEE double precision in which
u ≈ 1.1 × 10−16), and requires just one more CholeskyQR iteration than [18], thus is
usually much faster.

As a bonus, our development is straightforward to apply to computing the QR
factorization in a non-standard inner product space induced by a positive deﬁnite
matrix B (cid:31) 0, in which (x, y)B = xT By. Available algorithms for this task include
(modiﬁed) Gram-Schmidt [12], and Cholesky QR [10, 14, 9]. A recent work by Low-
ery and Langou [10] studies the numerical stability, with no method apparently being
(near) optimal in both orthogonality and backward error. We analyze the stability of
shiftedCholeskyQR3 in this case to show it has favorable stability properties. More-
over, shiftedCholeskyQR3 is signiﬁcantly faster than Gram-Schmidt type algorithms,
achieving up to 40-fold speedup in our experiments with sparse B.

This paper is organized as follows. In Section 2 we describe the shiftedCholeskyQR
algorithm. Section 3 analyzes shiftedCholeskyQR in detail and shows that it can be
used to reduce κ2(X). In Section 4 we combine shiftedCholeskyQR and CholeskyQR2
to derive shiftedCholeskyQR3 for ill-conditioned matrices with κ2(X) = O(u−1) and
prove its backward stability. Section 5 discusses the extension to the non-standard
inner product space. Numerical experiments are shown to illustrate the results in Sec-
tion 6 and Section 7 summarizes the performance of our software implementations.

We primarily focus on real matrices A ∈ Rm×n, but everything carries over to
complex matrices A ∈ Cm×n. We assume m ≥ n, and the algorithms developed here
are particularly useful in the tall-skinny case m (cid:29) n.

2. Shifted Cholesky QR. To overcome the numerical breakdown in the Cholesky

introduce a small shift AT A + sI
factorization (1.2), we propose a simple remedy:
to force the computed Gram matrix AT A to be numerically positive deﬁnite, so that
its Cholesky factorization runs without breakdown. The rest of the algorithm is the
same as Cholesky QR, and the algorithm shiftedCholeskyQR can be summarized in
pseudocode in Algorithm 2.1.

orthonormalshifted Cholesky QRCholesky QRCholesky QR(Cholesky QR2)4

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

Algorithm 2.1 shiftedCholeskyQR for X = QR

1: A = X (cid:62)X,
2: choose s > 0
3: R = chol(A + sI)
4: Q = XR−1

Introducing shifts in Cholesky QR was brieﬂy mentioned in [14], also as a remedy
for the breakdown. However, the focus there was on the algorithm called SVQB,
which computes the SVD of the Gram matrix instead of the Cholesky factorization.
SVQB computes a factorization of the form A = QB, where B is a full n × n matrix.
While Q is still a basis for the column space of A, an additional QR factorization of
B is needed to obtain a complete QR factorization of A. Furthermore, experiments
suggest that there are beneﬁts in working with a triangular matrix as in Cholesky QR
as opposed to full matrices as in SVQB, wirh respect to the row-wise stability of the
computed decomposition.

We shall discuss an appropriate choice of the shift s, which will be O(u), and
2 < κ2(X) < u−1
2 , for

prove that applying shiftedCholeskyQR to a matrix X with u− 1
results in a computed ˜Q with much reduced condition number κ2( ˜Q) < u− 1
which CholeskyQR2 suﬃces to compute the QR factorization.

3. Convergence and stability analysis of shiftedCholeskyQR. In this sec-
tion we present the main technical analysis of shiftedCholeskyQR. The goal is to show
that the algorithm improves the conditioning signiﬁcantly, that is, κ2( ˜Q) (cid:28) κ2(X).
A few assumptions need to be made on the matrix size and condition number. The
constants below are not of signiﬁcant importance but chosen so that the forthcoming
analysis runs smoothly. We shall assume the following hold:

(3.1)

(3.2)

(3.3)

(3.4)

6n2uκ2(X) < 1,
1
64

mnu ≤

,

n(n + 1)u ≤

1
64

,

11{mn + n(n + 1)}u(cid:107)X(cid:107)2

2 ≤ s ≤

1
100

(cid:107)X(cid:107)2
2.

Roughly speaking, the ﬁrst three assumptions require that the condition number
κ2(X) is safely bounded above by u−1, and the matrix dimensions m, n are small
compared with the precision u−1. We reiterate that κ2(X) > u− 1
2 is allowed, a
crucial diﬀerence from CholeskyQR2 treated in [16]. The assumption (3.4) imposes
that s is large enough for Cholesky to work, but small compared with (cid:107)X(cid:107)2
2.

Note that by (3.2), (3.3) we have

(3.5)

γm :=

mu
1 − mu

≤ 1.02mu,

γn+1 :=

(n + 1)u
1 − (n + 1)u

≤ 1.02(n + 1)u.

3.1. Preparations. We denote the computed quantities in shiftedCholeskyQR,

accounting for the numerical errors, by
ˆA = X (cid:62)X + E1,

(3.6)

(3.7)

(3.8)

ˆR(cid:62) ˆR = ˆA + sI + E2 = X (cid:62)X + sI + E1 + E2,

i = x(cid:62)
ˆq(cid:62)

i ( ˆR + ∆ ˆRi)−1

(i = 1, 2, . . . m).

SHIFTED CHOLESKYQR

5

i are the ith rows of X and ˆQ respectively. E1 is the matrix-matrix multiplica-
ˆq(cid:62)
i , x(cid:62)
tion error in the computation of the Gram matrix X (cid:62)X, and E2 is the ˆA backward
error incurred when computing the Cholesky factorization. ∆ ˆRi is the backward error
involved in the solution of the linear system q(cid:62)
i

ˆR = x(cid:62)
i .

We shall take s so that

(3.9)

(cid:107)E1(cid:107)2, (cid:107)E2(cid:107)2 = o(s),

which means s = c max((cid:107)E1(cid:107)2, (cid:107)E2(cid:107)2) for some c > 1.
(cid:107)E1(cid:107)2, (cid:107)E2(cid:107)2 = O(u)(cid:107)X(cid:107)2
u(cid:107)X(cid:107)2

In fact we shall see that
2, so (3.9) simply means s is chosen to be safely larger than

2 (qualitatively this is also assumed by (3.8)). In particular, we write

(3.10)

(3.8) gives

s := α(cid:107)X(cid:107)2
2,

0 < α < 1.

i ( ˆR + ∆ ˆRi)−1 = x(cid:62)
i = x(cid:62)
ˆq(cid:62)
Hence (I + ˆR−1∆ ˆRi)−1 = I + ˘Ri, ˘Ri := (cid:80)∞

i (I + ˆR−1∆ ˆRi)−1 ˆR−1.
k=1(− ˆR−1∆ ˆRi)k, so deﬁning

(3.11)

we have

(3.12)

Let ∆X =

(3.13)

∆x(cid:62)
i

:= x(cid:62)
i

˘Ri

i = (x(cid:62)
ˆq(cid:62)

i + ∆x(cid:62)

i ) ˆR−1

(i = 1, 2, . . . m).

(cid:21)

be the matrix obtained by stacking up the row vectors ∆x(cid:62)

i . Then

ˆQ = (X + ∆X) ˆR−1.

1

(cid:20) ∆x(cid:62)
...

∆x(cid:62)
m

∆x(cid:62)
i . From (3.8) we have ˆq(cid:62)
i

i and ∆ ˆRi. For later use, here we examine the relation between ∆ ˆRi and
ˆR =

i . We also have from (3.12) ˆq(cid:62)
i

i ∆ ˆRi = x(cid:62)

∆x(cid:62)
i + ∆x(cid:62)
x(cid:62)

ˆR + ˆq(cid:62)
i . Combining these we obtain

(3.14)

∆x(cid:62)

i = −ˆq(cid:62)

i ∆ ˆRi.

3.1.1. Error in computing X (cid:62)X. For general matrices A ∈ Rm×n, B ∈
Rn×m, the error in computing the matrix product C = AB can be bounded by [7,
Ch. 3]

(3.15)

|AB − f l(AB)| ≤ γn|A||B|

Here |A| is the matrix whose (i, j) element is |aij|.
bounded by

In [16] it is shown that E1 is

(3.16)

(cid:107)E1(cid:107)2 ≤ (cid:107)|E1|(cid:107)F ≤ γmn(cid:107)X(cid:107)2
2

Simplifying (3.16) using (3.5) yields (cid:107)E1(cid:107)2 ≤ 1.1mnu(cid:107)X(cid:107)2
2.

6

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

Backward error in the Cholesky factorization. Suppose that A ∈ Rn×n is a pos-
itive deﬁnite matrix and its Cholesky factorization computed in ﬂoating-point arith-
metic runs to completion and outputs ˆR. Then there exists ∆A ∈ Rn×n such that [7,
Thm. 10.3]

(3.17)

ˆR(cid:62) ˆR = A + ∆A,

|∆A| ≤ γn+1| ˆR(cid:62)|| ˆR|

Applying this to our situation gives

(cid:107)E2(cid:107)2 ≤ (cid:107)|E2|(cid:107)F ≤ γn+1n((cid:107) ˆA(cid:107)2 + (cid:107)E2(cid:107)2).

Using [16, eqn. (3.16)] in the right-hand side gives

(cid:107)E2(cid:107)2 ≤

γn+1n((1 + γmn + α)
1 − γn+1n

(cid:107)X(cid:107)2
2

≤

≤

(3.18)

1.02(n + 1)u · n(1 + 1.02mu · n + 0.01)
1 − 1.02(n + 1)u · n
1.02 · n(n + 1)u · (1 + 1.02 · 1

64 + 0.01)

(cid:107)X(cid:107)2
2

1 − 1.02
64

(cid:107)X(cid:107)2

2 ≤ 1.1n(n + 1)u(cid:107)X(cid:107)2
2,

where we have used α = s/(cid:107)X(cid:107)2

2 ≤ 1/100 and (3.2), (3.3).

Bounding (cid:107) ˆR−1(cid:107)2. Using Weyl’s theorem [6, Sec. 8.6.2] in (3.7) gives

(3.19)

σn( ˆR)2 ≥ (σn(X))2 + s − ((cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2).

By (cid:107)E1(cid:107)2 ≤ 1.1mnu(cid:107)X(cid:107)2

2 and (3.4) and (3.18) we obtain

(3.20)

(cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2 ≤ 1.1(mn + n(n + 1))u(cid:107)X(cid:107)2

2 ≤ 0.1s.

Substituting this into (3.19) we obtain

(3.21)

therefore

(3.22)

(σn( ˆR))2 ≥ (σn(X))2 + 0.9s,

(cid:107) ˆR−1(cid:107)2 = σn( ˆR)−1 ≤

1
(cid:112)(σn(X))2 + 0.9s

.

3.1.2. Bounding (cid:107)X ˆR−1(cid:107)2. We next bound (cid:107)X ˆR−1(cid:107)2. This can be done using

(3.20) and (3.22) as

(cid:107)X ˆR−1(cid:107)2 ≤

(cid:113)

(cid:115)

1 + (cid:107) ˆR−1(cid:107)2

2(s + (cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2)

1.1s
(σn(X))2 + 0.9s

(cid:114)

≤

1 +

1.1
0.9

≤ 1.5.

(3.23)

≤

1 +

3.1.3. Bounding (cid:107)∆ ˆRi(cid:107)2. Let R ∈ Rn×n be a nonsingular upper triangular
matrix. Generally, the computed solution ˆx obtained by solving an upper triangular
linear system Rx = b by back substitution in ﬂoating-point arithmetic satisﬁes [7,
Thm. 8.5]

(3.24)

(R + ∆R)ˆx = b,

|∆R| ≤ γn|R|.

SHIFTED CHOLESKYQR

7

We have for 1 ≤ i ≤ m

(3.25)

(cid:107)∆ ˆRi(cid:107)2 ≤ (cid:107)|∆ ˆRi|(cid:107)F ≤ γn

√

n(cid:107) ˆR(cid:107)2.

From (3.7), (3.4), and (3.20) we obtain

(3.26)

(cid:107) ˆR(cid:107)2

2 ≤ (cid:107)X(cid:107)2

2 + s + (cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2 ≤ (cid:107)X(cid:107)2

2 + 1.1s ≤ 1.1(cid:107)X(cid:107)2
2.

Substituting this into (3.25) gives

(3.27)

(cid:107)∆ ˆRi(cid:107)2 ≤ 1.02nu ·

√

√

n ·

1.1(cid:107)X(cid:107)2 ≤ 1.1n

√

nu(cid:107)X(cid:107)2.

3.1.4. Bounding (cid:107)∆X(cid:107)2 roughly. Here we give a rough bound for (cid:107)∆X(cid:107)F
s). This will be insuﬃcient for proving that
u), for which we will need (cid:107)∆X(cid:107)F = O(u(cid:107)X(cid:107)2), which we will prove

and prove that (cid:107)∆X(cid:107)F = O(u(cid:107)X(cid:107)2
2/
κ2( ˆQ) = O(1/
later after having obtained a bound for ˆQ. We shall proceed as follows.

√

√

√

1. Derive the “rough” bound (cid:107)∆X(cid:107)F = O(u(cid:107)X(cid:107)2
2/
2. Use above to show (cid:107) ˆQ(cid:107)2 = O(1).
3. Use above and (3.14) to prove the “tight” bound (cid:107)∆X(cid:107)F = O(u(cid:107)X(cid:107)2).
4. Use above to prove κ2( ˆQ) = O(u− 1
To establish the ﬁrst statement we recall (3.11), and bound (cid:107) ˘Ri(cid:107)2 as

2 ).

s).

(3.28)

(cid:107) ˘Ri(cid:107)2 ≤

∞
(cid:88)

k=1

((cid:107) ˆR−1(cid:107)2(cid:107)∆ ˆRi(cid:107)2)k =

(cid:107) ˆR−1(cid:107)2(cid:107)∆ ˆRi(cid:107)2
1 − (cid:107) ˆR−1(cid:107)2(cid:107)∆ ˆRi(cid:107)2

.

We bound the denominator from below as

1 − (cid:107) ˆR−1(cid:107)2(cid:107)∆ ˆRi(cid:107)2 ≥ 1 −

(3.29)

≥ 1 −

√

1.1n

nu(cid:107)X(cid:107)2

(cid:112)(σn(X))2 + 0.9s
√
nu(cid:107)X(cid:107)2

1.1n

(cid:112)0.9 · 11mnu(cid:107)X(cid:107)2

2

≥ 1 −

(cid:114)

1.21
9.9

·

n2u
m

≥ 0.95.

Substituting this into (3.28) yields

(3.30)

(cid:107) ˘Ri(cid:107)2 ≤

1
0.95

·

Together with the fact (cid:107)∆x(cid:62)

(3.31) (cid:107)∆X(cid:107)F =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

i=1

(cid:107)∆x(cid:62)

≤

√

1.1n

nu(cid:107)X(cid:107)2

√

1.2n

nu(cid:107)X(cid:107)2

(cid:112)(σn(X))2 + 0.9s

(cid:112)(σn(X))2 + 0.9s
i (cid:107) (cid:107) ˘Ri(cid:107)2, we can bound (cid:107)∆X(cid:107)F as
i (cid:107) ≤ (cid:107)x(cid:62)
(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:107) ˘Ri(cid:107)2 ≤

m
(cid:88)

(cid:107)x(cid:62)

i (cid:107)2 ≤

i (cid:107)2 · max
1≤i≤m

.

i=1

1.2n2u(cid:107)X(cid:107)2
2
(cid:112)(σn(X))2 + 0.9s

,

where we used

(cid:113)(cid:80)m

i=1 (cid:107)x(cid:62)

i (cid:107)2 = (cid:107)X(cid:107)F ≤

√

n(cid:107)X(cid:107)2 for the last inequality.

3.1.5. Bounding (cid:107) ˆQ(cid:107)2. We now proceed to bound (cid:107) ˆQ(cid:107)2.
Lemma 3.1. Suppose that X ∈ Rm×n with m ≥ n satisﬁes (3.2) and (3.3). Then,
the matrix ˆQ obtained by applying the shiftedCholeskyQR algorithm in ﬂoating-point
arithmetic to X satisﬁes

(cid:107) ˆQ(cid:62) ˆQ − I(cid:107)2 < 2,

and hence

(3.32)

(cid:107) ˆQ(cid:107)2 <

√

3.

8

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

Proof. We have

ˆQ(cid:62) ˆQ= ˆR−(cid:62)(X + ∆X)(cid:62)(X + ∆X) ˆR−1

= I − ˆR−(cid:62)(s + E1 + E2) ˆR−1 + (X ˆR−1)(cid:62)∆X ˆR−1 + ˆR−(cid:62)∆X (cid:62)(X ˆR−1) + ˆR−(cid:62)∆X (cid:62)∆X ˆR−1.

Thus we can bound (cid:107) ˆQ(cid:62) ˆQ − I(cid:107)2 as

(3.33)

(cid:107) ˆQ(cid:62) ˆQ − I(cid:107)2 ≤(cid:107) ˆR−1(cid:107)2

2(s + (cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2) + 2(cid:107) ˆR−1(cid:107)2(cid:107)X ˆR−1(cid:107)2(cid:107)∆X(cid:107)F

+ (cid:107) ˆR−1(cid:107)2

2(cid:107)∆X(cid:107)2
F .

The ﬁrst term of (3.33) can be bounded as

(3.34)

(cid:107) ˆR−1(cid:107)2

2(s + (cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2) ≤

1.1s
(σn(X))2 + 0.9s

≤

1.1
0.9

.

and for the second term in (3.33), using (3.22), (3.23) and (3.31) we obtain

2(cid:107) ˆR−1(cid:107)2(cid:107)X ˆR−1(cid:107)2(cid:107)∆X(cid:107)F ≤ 2 ·

(3.35)

≤

1
(cid:112)(σn(X))2 + 0.9s
4
11

2 · 1.5 · 1.2 · 1
0.9s

11 s

=

· 1.5 ·

1.2n2u(cid:107)X(cid:107)2
2
(cid:112)(σn(X))2 + 0.9s

.

For the third term in (3.33), from (3.22) and (3.31)

F ≤

(3.36)

(cid:107) ˆR−1(cid:107)2

2(cid:107)∆X(cid:107)2

1
(σn(X))2 + 0.9s
(1.2 · 1
11 s)2
(0.9s)2 =
Summarizing, we can bound the right-hand side of (3.33) as (cid:107) ˆQ(cid:62) ˆQ − I(cid:107)2 < 2, as
required.

(1.2n2u(cid:107)X(cid:107)2
2)2
(σn(X))2 + 0.9s

16
1089

≤

·

.

3.1.6. Bounding the residual in shiftedCholeskyQR. We now bound the

residual.

Lemma 3.2. Under the assumptions in Lemma 3.1, ˆQ ˆR computed by shiftedC-

holeskyQR satisﬁes

Proof. First note that

(cid:107)ˆq(cid:62)
i

ˆR − x(cid:62)

i (cid:107) = (cid:107)ˆq(cid:62)
i

(cid:107) ˆQ ˆR − X(cid:107)F
(cid:107)X(cid:107)2

≤ 2n2u.

ˆR − ˆq(cid:62)

i ( ˆR + ∆ ˆRi)(cid:107) ≤ (cid:107)ˆq(cid:62)

i ∆ ˆRi(cid:107) ≤ (cid:107)ˆq(cid:62)

i (cid:107)(cid:107)∆ ˆRi(cid:107).

Substituting (3.27) into this gives
ˆR − x(cid:62)

(3.37)

(cid:107)ˆq(cid:62)
i

i (cid:107) ≤ (cid:107)ˆq(cid:62)

i (cid:107) · 1.1n

√

nu(cid:107)X(cid:107)2.

On the other hand, from (3.32) we have

(3.38)

Hence it follows that

(cid:107) ˆQ(cid:107)F <

√

3n.

(cid:107) ˆQ ˆR − X(cid:107)F =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

(cid:107)ˆq(cid:62)
i

ˆR − x(cid:62)

i (cid:107)2 ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

i=1

(cid:107)ˆq(cid:62)

i (cid:107)2 · 1.1n

√

nu(cid:107)X(cid:107)2

(3.39)

i=1
= (cid:107) ˆQ(cid:107)F · 1.1n

√

nu(cid:107)X(cid:107)2 ≤ 2n2u(cid:107)X(cid:107)2.

SHIFTED CHOLESKYQR

9

Lemma 3.2 shows that shiftedCholeskyQR gives optimal residual up to a factor

involving a low-degree polynomial of m, n (recall that (cid:107)X(cid:107)F ≤

n(cid:107)X(cid:107)2).

√

Tighter bound for (cid:107)∆X(cid:107)F . By (3.13) we have ∆X = ˆQ ˆR − X, so (3.39) in

fact provides a bound for (cid:107)∆X(cid:107)F that is tighter than the previous bound (3.31):

(3.40)

(cid:107)∆X(cid:107)F ≤ 2n2u(cid:107)X(cid:107)2.

3.2. Main result. We are now ready to state the main result of the section,

which bounds the condition number of ˆQ.

Theorem 3.3. With one step of shiftedCholeskyQR in double precision arithmetic

applied to X satisfying (3.1)–(3.3) with shift s satisfying (3.4), we obtain ˆQ with

(3.41)

κ2( ˆQ) ≤ 2(cid:112)1 + α(κ2(X))2 ·

√

3,

where

√

α =

√

s
(cid:107)X(cid:107)2

.

Proof. Recall from (3.32) that σ1( ˆQ) <

σn( ˆQ) from below. Using Weyl’s theorem in (3.13) gives

√

3. The remaining task is to bound

(3.42)

σn( ˆQ) ≥ σn(X ˆR−1) − (cid:107)∆X ˆR−1(cid:107)2.

Using (3.22) and (3.40) we obtain

(3.43)

(cid:107)∆X ˆR−1(cid:107)2 ≤ (cid:107)∆X(cid:107)F (cid:107) ˆR−1(cid:107)2 ≤

2n2u(cid:107)X(cid:107)2
(cid:112)(σn(X))2 + 0.9s

.

Note that this is O(u 1
in m, n as constants.

2 ) when we take s = O(u) and regard low-degree polynomials

We next bound σn(X ˆR−1) from below. We proceed by examining the equation

(3.44)

ˆR−(cid:62)(X (cid:62)X + sI) ˆR−1 = I − ˆR−(cid:62)(E1 + E2) ˆR−1.

Let X = U ΣV (cid:62) by the SVD. Then for a diagonal matrix G, we can write

(3.45)

X (cid:62)X + sI = (U (Σ + G)V (cid:62))(cid:62)(U (Σ + G)V (cid:62)).

Indeed the left-hand side is V (Σ2 + sI)V (cid:62) and the right-hand side V (Σ + G)2V (cid:62), so
we can take

(3.46)

G = (Σ2 + sI)

1

2 − Σ = diag((cid:112)(σi(X))2 + s − σi(X))

Now setting T = U (Σ + G)V (cid:62) ˆR−1 we have

(3.47)

T (cid:62)T = I − ˆR−(cid:62)(E1 + E2) ˆR.

We next bound the singular values of T . By (3.20) and (3.22) we have

(3.48) (cid:107) ˆR−(cid:62)(E1 + E2) ˆR−1(cid:107)2 ≤ (cid:107) ˆR−1(cid:107)2

2((cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2) ≤

0.1s
(σn(X))2 + 0.9s

≤

1
9

,

(cid:113)

(cid:113)

1 − 1
so σi(T ) ∈ [
9 ,
SVD where E(cid:48) is diagonal, we have

1 + 1

9 ] ⊆ [0.9, 1.1]. Therefore, letting T = U (cid:48)(I + E(cid:48))V (cid:48) be the

(3.49)

T = U (cid:48)(I + E(cid:48))V (cid:48)(cid:62) = U (cid:48)V (cid:48)(cid:62)(I + V (cid:48)E(cid:48)V (cid:48)(cid:62)) = Q(I + E),

10

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

where Q = U (cid:48)V (cid:48)(cid:62) has orthonormal columns, and (cid:107)E(cid:107)2 = (cid:107)V (cid:48)E(cid:48)V (cid:48)(cid:62)(cid:107)2 = (cid:107)E(cid:48)(cid:107)2 ≤ 0.1.

Now plugging into (3.49) the deﬁnition of T gives

(3.50)

Q(I + E) = U (Σ + G)V (cid:62) ˆR−1.

Recalling that X = U ΣV (cid:62), we have σi(X ˆR−1) = σi(ΣV (cid:62) ˆR−1) and

(3.51)

= diag

ΣV (cid:62) ˆR−1 = Σ(Σ + G)−1U (cid:62)Q(I + E)
(cid:32)
(cid:33)

σi(X)
(cid:112)(σi(X))2 + s

U (cid:62)Q(I + E).

Using the general inequality for singular values of matrix products σmin(AB) ≥
σmin(A)σmin(B) (which holds when A or B is square) along with (3.21), we obtain

(3.52)

σn(X ˆR−1) ≥

σn(X)
(cid:112)(σn(X))2 + s

· 0.9.

Using this and (3.43), from (3.42) we obtain

σn( ˆQ) ≥

≥

≥

≥

≥

(3.53)

0.9σn(X)
(cid:112)(σn(X))2 + s
0.9σn(X)
(cid:112)(σn(X))2 + s
0.9
(cid:112)(σn(X))2 + s
0.9
(cid:112)(σn(X))2 + s
σn(X)
2(cid:112)(σn(X))2 + s

−

−

(cid:18)

2n2u(cid:107)X(cid:107)2
(cid:112)(σn(X))2 + 0.9s
2n2u(cid:107)X(cid:107)2
0.9(cid:112)(σn(X))2 + s
2
√

√

σn(X) −

0.9

0.9

(σn(X) − 0.4σn(X))

=

1
2(cid:112)1 + α(κ2(X))2

.

(cid:19)

· n2u(cid:107)X(cid:107)2

We have used the assumption (3.1) for the fourth inequality. Together with (3.32) we
obtain

(3.54)

κ2( ˆQ) =

(cid:107) ˆQ(cid:107)2
σn( ˆQ)

≤ 2(cid:112)1 + α(κ2(X))2 ·

√

3.

Theorem 3.3 implies that, provided that α(κ2(X))2 (cid:29) 1,

(3.55)

√

κ2( ˆQ) (cid:46) 2

√

3 ·

ακ2(X).

Thus applying one step of shiftedCholeskyQR results in the condition number being
reduced by about a factor

α =

√

√

.

s
(cid:107)X(cid:107)2

4. shiftedCholeskyQR3: shiftedCholeskyQR + CholeskyQR2. We now
discuss an algorithm for the QR factorization of an ill-conditioned matrix that ﬁrst
uses shiftedCholeskyQR, then runs CholeskyQR2. We refer to this algorithm as shift-
edCholeskyQR3, since it runs Cholesky QR (or its shifted variant) three times. The
initial shiftedCholeskyQR can thus be regarded as a preconditioning step that re-
duces the condition number so that CholeskyQR2 becomes applicable. We continue
to assume (3.1)–(3.3); a particular case of interest is u− 1

2 < κ2(X) < u−1.

SHIFTED CHOLESKYQR

11

4.1. Choice of shift s. We ﬁrst discuss the choice of the shift s for shiftedC-

holeskyQR, which balances two requirements:

• s should be as small as possible to maximize the condition number improve-

ment (3.41).

• s should be large enough so that the Cholesky factorization chol(A + sI) runs

to completion without numerically breaking down.

In addition to these, s must satisfy Eq. (3.4) for the error analysis in the previous
section to be valid.

To address the issue of breakdown we review Rump and Ogita’s [13] error analysis

for Cholesky factorization, which builds upon Demmel’s early work [2].

Let A be a symmetric positive deﬁnite matrix. It is known [13, Thm. 2.3] that

chol(A) succeeds numerically if the following holds:

(4.1)

λn(A) ≥

n
(cid:88)

i=1

γi+1
1 − γi+1

(cid:107)A(cid:107)2.

It has been shown in [19] that a suﬃcient condition for (4.1) to hold is

(4.2)

˜s ≥ cn+2utr(A),

cn+2 :=

(n + 2)
1 − (n + 1)(n + 3)u

< 2.2(n + 1).

In our context of the Cholesky factorization (3.7), we need to take into account
the error term E1 in computing the matrix multiplication A = X (cid:62)X. By (3.6) and
Weyl’s theorem we obtain a lower bound

(4.3)

λn( ˆA) ≥ λn(X (cid:62)X) − (cid:107)E1(cid:107)2.

This means that ˆA may not be positive deﬁnite if λn(X (cid:62)X) ≤ γmn(cid:107)X(cid:107)2
to apply formula (4.2), we must ﬁrst shift ˆA by γmn(cid:107)X(cid:107)2
safe choice of s to avoid numerical breakdown is

2. Accordingly,
2. Thus we conclude that a

(4.4)

˜s := γm+1n(cid:107)X(cid:107)2

2 + cn+2utr( ˆA + γm+1n(cid:107)X(cid:107)2

2I).

By further taking into account (3.4), we have

(4.5)

s := max (cid:0)11{mn + n(n + 1)}u(cid:107)X(cid:107)2

2, ˜s(cid:1) .

This expression can be simpliﬁed by evaluating ˜s using the results in the previous
section. First, we note that ˆA = X (cid:62)X + E1 from (3.6). Denoting the jth column
vector of X by ˜xj, we can evaluate tr(E1) as

(4.6) tr(E1) ≤

n
(cid:88)

i=1

|E1|ii ≤

n
(cid:88)

i=1

γm|˜xi|(cid:62)|˜xi| = γm

n
(cid:88)

i=1

(cid:107)˜xi(cid:107)2 = γm(cid:107)X(cid:107)2

F ≤ γmn(cid:107)X(cid:107)2
2.

On the other hand,

(4.7)

tr(X (cid:62)X) = (cid:107)X(cid:107)2

F ≤ n(cid:107)X(cid:107)2
2.

Plugging these into the right-hand side of (4.4), we can bound ˜s as

˜s ≤ γm+1n(cid:107)X(cid:107)2

2 + 2.2(n + 1)u(n(cid:107)X(cid:107)2

2 + γmn(cid:107)X(cid:107)2

2 + γm+1n(cid:107)X(cid:107)2
2)

(4.8)

≤ 2.4{mn + n(n + 1)}u(cid:107)X(cid:107)2
2,

12

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

which shows that the maximum in (4.5) is attained by the ﬁrst argument. Hence, in
what follows we shall assume

(4.9)

s := 11{mn + n(n + 1)}u(cid:107)X(cid:107)2
2.

In practice, since (cid:107)X(cid:107)2 is expensive to compute we can estimate it reliably using a
norm estimator e.g. MATLAB’s function normest, or alternatively just replace it
with (cid:107)X(cid:107)F , which results in a larger (more conservative) shift.

We now summarize our algorithm in pseudocode. Algorithm 4.1 blends shiftedC-
holeskyQR and CholeskyQR2 in an adaptive manner, initially attempting to reduce
the condition number using shiftedCholeskyQR as in (3.55) so that CholeskyQR(2)
becomes applicable. The shifts are introduced only when necessary, judged by whether
or not the Cholesky factorization chol(A(k)) breaks down. Our experiments suggest
that Algorithm 4.1 may well be applicable even to extremely ill-conditioned matrices
with possibly κ2(X) > u−1.

Algorithm 4.1 Iterated CholeskyQR for X = QR with shifts when necessary.

1: Let Q := X, R := I
2: repeat
3: A := QT Q
4:
5:
6:
7:
8:
9: Q := Q ˜R−1, R := ˜RR
√
10: until (cid:107)QT Q − I(cid:107)F ≤

end if

nu

˜R := chol(A) //chol: Cholesky factorization
if chol(A) breaks down then

s := 11{mn + n(n + 1)}u(cid:107)X(cid:107)2
2
˜R := chol(A + sI)

//introduce shift

In the analysis below, we focus on the case κ2(X) < u−1; to be precise, when
u− 1
2 < κ2(X) (so that CholeskyQR2 is inapplicable) and κ2(X) is bounded from
above by (4.12) given below. For such matrices, the algorithm provenly runs one shift-
edCholeskyQR, then CholeskyQR2 (thus executing three Cholesky factorizations).
This computes a stable QR factorization, and we refer to this algorithm as shiftedC-
holeskyQR3. For completeness we present its pseudocode in Algorithm 4.2. The last
two lines represent CholeskyQR2 for the Q obtained by the ﬁrst shiftedCholeskyQR.

Algorithm 4.2 shiftedCholeskyQR3 for X = QR.

1: Let Q := X
2: A := QT Q
3: s := 11{mn + n(n + 1)}u(cid:107)X(cid:107)2
2
4: R := chol(A + sI)
5: Q := Q ˜R−1
6: ˜R := chol(QT Q), Q := Q ˜R−1, R := ˜RR // Cholesky QR
7: ˜R := chol(QT Q), Q := Q ˜R−1, R := ˜RR // CholeskyQR2

// shiftedCholeskyQR

//introduce shift

4.2. When is thrice enough?. Here we derive a condition on κ2(X) that
guarantees that shiftedCholeskyQR3 gives a numerically stable QR factorization of
X.

SHIFTED CHOLESKYQR

13

Recall that (3.54) gives a bound for the condition number of ˆQ obtained by shift-
√
edCholeskyQR: κ2( ˆQ) ≤ 2(cid:112)1 + α(κ2(X))2 ·
3. As in (4.9), to guarantee avoidance
of breakdown we take α = 11{mn + n(n + 1)}u, so the condition number of ˆQ is
bounded as

(4.10)

√

κ2( ˆQ) ≤ 2

3(cid:112)1 + 11{mn + n(n + 1)}u(κ2(X))2

On the other hand, as shown in [16], a suﬃcient condition for CholeskyQR2 to com-
pute a stable QR factorization of ˆQ is

(4.11)

κ2( ˆQ) ≤

1
8(cid:112){mn + n(n + 1)}u

.

Combining these facts, we obtain the following condition under which shiftedCholeskyQR3
is guaranteed to compute a numerically stable QR factorization:

√

2

3(cid:112)1 + 11{mn + n(n + 1)}u(κ2(X))2 ≤

1
8(cid:112){mn + n(n + 1)}u

.

If κ2(X) > u− 1
1)}u(κ2(X))2 and the condition can be simpliﬁed as

2 , we have 1 + 11{mn + n(n + 1)}u(κ2(X))2 (cid:39) 11{mn + n(n +

(4.12)

κ(X) ≤

u−1
96{mn + n(n + 1)}

.

Note that this ensures that the condition (3.1) for the error analysis in Section 3 is
automatically satisﬁed.

In practice, it often happens that shiftedCholeskyQR3 (or more often the iterated
Algorithm 4.1) computes the QR factorization for matrices with even larger condition
numbers than indicated by (4.12). One explanation is that in the absence of roundoﬀ
errors, one iteration of shiftedCholeskyQR reduces the condition number by a factor
≈ u 1
2 . However, we think that a rigorous convergence analysis in ﬁnite precision
arithmetic would be possible only under some assumption on κ2(X), and that (4.12)
provides a sharp bound up to (at most) a low-degree polynomial in m, n.

We now examine the numerical stability of shiftedCholeskyQR3 and show that it
enjoys excellent stability both in orthogonality and backward error. Roughly, the
result follows by combining the facts that (i) shiftedCholeskyQR gives a ˆQ with
κ2( ˆQ) < u− 1
2 with small backward error, and (ii) for matrices with condition num-
2 , CholeskyQR2 computes a stable QR factorization of ˆQ as shown in [16].
ber < u− 1
Below we make this statement precise.

4.3. Numerical stability of shiftedCholeskyQR3.
Theorem 4.1. Let X ∈ Rm×n be a matrix satisfying (3.1)–(3.3) and (4.12).
Then shiftedCholeskyQR3 computes a QR factorization X ≈ ˆQ ˆR satisfying the or-
thogonality measure

(4.13)

(cid:107) ˆQ(cid:62) ˆQ − I(cid:107)F ≤ 6{mn + n(n + 1)}u,

and backward error

(4.14)

(cid:107) ˆQ ˆR − X(cid:107)F
(cid:107)X(cid:107)2

≤ 15n2u.

14

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

Proof. The orthogonality measure of the output ˆQ of shiftedCholeskyQR3 is es-
sentially exactly the same as that of CholeskyQR2, which is analyzed in detail in [16].
This is because the bound there applies to any matrix with condition number (cid:46) u− 1
2 .
We next establish (4.14). By (3.13), with the ﬁrst shiftedCholeskyQR executed

in ﬁnite precision arithmetic we have

(4.15)

X + ∆X = ˆQ ˆR,

where (cid:107)∆X(cid:107)F is bounded as in (3.40). We then apply CholeskyQR2 to ˆQ to obtain
the QR factorization ˆQ = ZU . In ﬁnite precision we have

(4.16)

ˆQ + ∆ ˆQ = ˆZ ˆU ,

where (see Appendix; this bound slightly improves [16])

(4.17)

(cid:107)∆ ˆQ(cid:107)F ≤ 5n2u(cid:107) ˆQ(cid:107)2.

The upper triangular factor S in the QR factorization of the original matrix X is
computed as

(4.18)

ˆS = ﬂ ( ˆU ˆR) = ˆU ˆR + ∆S.

Here ∆S represents the forward error incurred in the matrix multiplication.

Summarizing, we can bound the overall backward error as

(cid:107) ˆZ ˆS − X(cid:107)F = (cid:107) ˆZ( ˆU ˆR + ∆S) − ˆQ ˆR + ∆X(cid:107)F

= (cid:107)∆ ˆQ ˆR + ˆZ∆S + ∆X(cid:107)F
≤ (cid:107)∆ ˆQ(cid:107)F (cid:107) ˆR(cid:107)2 + (cid:107) ˆZ(cid:107)2(cid:107)∆S(cid:107)F + (cid:107)∆X(cid:107)F .

(4.19)

We now bound the terms in the right-hand side. For the ﬁrst term, using [16, Thm 3.5]
and (3.32) we can bound (cid:107)∆ ˆQ(cid:107)F as

(4.20)

(cid:107)∆ ˆQ(cid:107)F ≤ 5n2u(cid:107) ˆQ(cid:107)2 ≤ 5

√

3n2u.

To bound (cid:107) ˆR(cid:107)2, we use (3.26) to obtain

(4.21)

√

(cid:107) ˆR(cid:107)2 ≤

1.1(cid:107)X(cid:107)2.

We next bound the second term in (4.19). By [16, Thm. 3.3], (cid:107) ˆZ(cid:107)2 can be bounded
as

(4.22)

(cid:107) ˆZ(cid:107)2 ≤ (cid:112)1 + 6(mnu + n(n + 1)u) ≤

(cid:115)

1 + 6

(cid:19)

(cid:18) 1
64

+

1
64

=

√

76
8

.

Regarding (cid:107)∆S(cid:107)F , using the general error bound for matrix multiplications |∆S| ≤
γn| ˆU | | ˆR| we obtain

(4.23)

(cid:107)∆S(cid:107)F ≤ γn(cid:107) | ˆU | | ˆR| (cid:107) ≤ γn(cid:107) ˆU (cid:107)F (cid:107) ˆR(cid:107)F ≤ nγn(cid:107) ˆU (cid:107)2(cid:107) ˆR(cid:107)2.

Here (cid:107) ˆR(cid:107)2 can be bounded as in (4.21). To bound (cid:107) ˆU (cid:107)2, we recall (4.16) and left-
multiply ˆZ (cid:62) to obtain

(4.24)

ˆZ (cid:62)( ˆQ + ∆ ˆQ) = ˆZ (cid:62) ˆZ ˆU .

SHIFTED CHOLESKYQR

15

Now, by [16, Thm. 3.3], the eigenvalues of ˆZ (cid:62) ˆZ lie in the interval [1 − 6(mnu + n(n +
1)u), 1 + 6(mnu + n(n + 1)u)], so it follows that

(cid:107) ˆU (cid:107)2 ≤ (cid:107)( ˆZ (cid:62) ˆZ)−1(cid:107)2(cid:107) ˆZ (cid:62)(cid:107)2((cid:107) ˆQ(cid:107)2 + (cid:107)∆ ˆQ(cid:107)2)
1
1 − 6(mnu + n(n + 1)u)

76
8

√
(

√

≤

·

√

3n2u)

3 + 5

(4.25)

≤

1
1 − 6 (cid:0) 1
64 + 1

64

(cid:1) ·

√

76
8

(cid:18)√

√

3 ·

3 + 5

(cid:19)

1
64

≤ 2.6.

Finally, we can bound (cid:107)∆X(cid:107)F as in (3.40).

Combining the above bounds and substituting into (4.19) yields

(cid:107) ˆZ ˆS − X(cid:107)F ≤ 5

√

3n2u ·

√

1.1(cid:107)X(cid:107)2 +

(4.26)

≤ 15n2u(cid:107)X(cid:107)2,

√

76
8

as required.

· n · 1.02nu · 2.6 ·

√

1.1(cid:107)X(cid:107)2 + 2n2u(cid:107)X(cid:107)2

Comparison with CGS2. As we saw above, (4.12) is a suﬃcient condition for
shiftedCholeskyQR3 to work in ﬁnite precision arithmetic. This condition roughly
requires that κ2(X)(mn + n2)u = O(1). Let us compare this with the analysis in [5]
for the CGS2 algorithm, which shows that

(4.27)

κ2(X)m2n3u = O(1)

is a suﬃcient condition for CGS2 to compute the QR factorization in a stable manner.
Observe that (4.27) is much more stringent than (4.12); indeed in large-scale
computing in which m, n (cid:29) 1000, with double precision (4.27) is unlikely to be
satisﬁed even with well-conditioned X.

This diﬀerence might appear to suggest shiftedCholeskyQR3 is superior to CGS2
in terms of robustness, but we have not observed this in practice. We suspect that
the diﬀerence is an artifact of the analysis, and the practical robustness of CGS2 and
shiftedCholeskyQR3 seem comparable. An advantage of shiftedCholeskyQR3 is that
it is rich in BLAS-3 operations, and oﬀers ample opportunity for parallelization.

5. Oblique inner product. The (shifted) Cholesky QR algorithm is readily
applicable to the QR decomposition in a non-standard inner product space (x, y)B =
xT By deﬁned via a symmetric positive deﬁnite matrix B ∈ Rm×m. The resulting
Algorithm 5.1 is almost identical to Algorithm 2.1, except that A is computed as
A = X (cid:62)BX and the shift s is chosen in a manner to be described below.

Algorithm 5.1 shiftedCholeskyQR for X = QR, QT BQ = In
1: A = X (cid:62)BX
2: choose s > 0
3: R = chol(A + sI)
4: Q = XR−1

In this section, we examine the stability of Algorithm 5.1. The argument closely
parallels that in Sections 3 and 4, but new features arise that aﬀect the bounds, in
particular involving (cid:112)κ2(B).

16

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

5.1. Assumptions. We make the following assumptions on m, n, X and B.
As in the case of standard inner product, the constants below are not of signiﬁcant
importance but chosen so that the analysis goes through.

(5.1)

(5.2)

(5.3)

(5.4)

· (cid:112)κ2(B) < 1,

6n2u ·

√

m

mnu ≤

(cid:112)(cid:107)B(cid:107)2
(cid:107)X(cid:107)2
(cid:112)σn(X (cid:62)BX)
1
64
1
64

,

,

n(n + 1)u ≤

√

11{2m

mn + n(n + 1)}u(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2 ≤ s ≤

1
100

(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2.

The assumption (5.1) roughly demands that X is not too ill-conditioned relative to u.
As before, (5.2) and (5.3) require that the matrix dimensions m, n are small compared
with the precision u−1. In (5.1), we can use a simpler assumption

(5.5)

6n2uκ2(X)κ2(B) < 1,

but (5.1) is less stringent.

5.2. Preparations. We denote the computed results of shiftedCholeskyQR in

an oblique inner product, accounting for the numerical errors, as

(5.6)

(5.7)

(5.8)

ˆA = X (cid:62)BX + E1,

ˆR(cid:62) ˆR = ˆA + sI + E2 = X (cid:62)BX + sI + E1 + E2,

i = x(cid:62)
ˆq(cid:62)

i ( ˆR + ∆ ˆRi)−1

(i = 1, 2, . . . m).

i are the ith rows of Q and ˆX, respectively. E1 is the matrix-matrix multiplica-
ˆq(cid:62)
i , x(cid:62)
tion error in the computation of the Gram matrix X (cid:62)BX, and E2 is the ˆA backward
error incurred when computing the Cholesky factorization. ∆ ˆRi is the backward er-
ror involved in the solution of the linear system q(cid:62)
i . Equation (5.8) can be
i
rewritten as

ˆR = x(cid:62)

(5.9)

where

(5.10)

Let ∆X =

(5.11)

ˆq(cid:62)
i

ˆR = x(cid:62)

i + ∆x(cid:62)
i ,

∆x(cid:62)

i = −ˆq(cid:62)

i ∆ ˆRi.

(cid:21)

be the matrix obtained by stacking up the row vectors ∆x(cid:62)

i . Then

X + ∆X = ˆQ ˆR,

1

(cid:20) ∆x(cid:62)
...

∆x(cid:62)
m

showing that ∆X is the residual.

Now we give bounds on (cid:107)E1(cid:107)2, (cid:107)E2(cid:107)2, (cid:107) ˆR−1(cid:107)2, (cid:107)B 1

2 X ˆR−1(cid:107)2, (cid:107)∆ ˆRi(cid:107)2, (cid:107) ˆQ(cid:107)2 and

(cid:107)∆X(cid:107)F as in the case of standard inner product.

SHIFTED CHOLESKYQR

17

5.2.1. Bounding (cid:107)E1(cid:107)2, (cid:107)E2(cid:107)2, (cid:107) ˆR−1(cid:107)2, (cid:107)B 1

2 X ˆR−1(cid:107)2 and (cid:107)∆ ˆRi(cid:107)2. Using the
standard error analysis of matrix-matrix multiplication and Cholesky factorization [7]
and the assumptions (5.2) and (5.3), we can bound (cid:107)E1(cid:107)2 and (cid:107)E2(cid:107)2 as

(5.12)

(5.13)

√

mnu(cid:107)X(cid:107)2
(cid:107)E1(cid:107)2 ≤ 2.2m
(cid:107)E2(cid:107)2 ≤ 1.1n(n + 1)u(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2,
2(cid:107)B(cid:107)2.

See [17] for details. From the assumption (5.4) on s, these bounds ensure that
√

(5.14)

(cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2 ≤ (2.2m

mn + 1.1n(n + 1))u(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2 ≤ 0.1s.

Combining this with (5.7) and using Weyl’s theorem [6, Sec. 8.6.2], we obtain a bound
on (cid:107) ˆR−1(cid:107)2:

(5.15)

(cid:107) ˆR−1(cid:107)2 ≤

1
(cid:112)σn(X (cid:62)BX) + 0.9s

.

The bound on (cid:107)B 1

2 X ˆR−1(cid:107)2 can be derived as follows. First, note that from (5.7),

(5.16)

(B

1

2 X ˆR−1)(cid:62)(B

1

2 X ˆR−1) = I − ˆR−(cid:62)(sI + E1 + E2) ˆR−1.

Using (5.14) and (5.15), we have

(cid:107)B

1

2 X ˆR−1(cid:107)2 ≤

(cid:113)

1 + (cid:107) ˆR−1(cid:107)2

2(s + (cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2)

(5.17)

(cid:115)

≤

1 +

(cid:114)

≤

1 +

1.1s
σn(X (cid:62)BX) + 0.9s

1.1
0.9

≤ 1.5.

The bound on (cid:107)∆ ˆRi(cid:107)2 can be obtained from the standard error analysis of backward
substitution [7, Thm. 8.5] as

(5.18)

(cid:107)∆ ˆRi(cid:107)2 ≤ (cid:107)|∆ ˆRi|(cid:107)F ≤ γn

√

n(cid:107) ˆR(cid:107)2.

From (5.7), (5.4), and (5.14) we obtain

(5.19) (cid:107) ˆR(cid:107)2

2 ≤ (cid:107)X(cid:107)2

2(cid:107)B(cid:107)2 + s + (cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2 ≤ (cid:107)X(cid:107)2

2(cid:107)B(cid:107)2 + 1.1s ≤ 1.1(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2.

Substituting this into (5.18) gives

(5.20)

(cid:107)∆ ˆRi(cid:107)2 ≤ 1.02nu ·

√

√

n ·

1.1(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2 ≤ 1.1n

√

nu(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2.

5.2.2. Bounding (cid:107)∆X(cid:107)2 roughly. Here we give a rough bound for (cid:107)∆X(cid:107)F
√
and prove that (cid:107)∆X(cid:107)F = O(u(cid:107)X(cid:107)2
s). This will be insuﬃcient for proving
2
(cid:112)κ2(B)),
our main result, Theorem 5.3, for which we will need (cid:107)∆X(cid:107)F = O(u(cid:107)X(cid:107)2
which we will prove later after having obtained a bound for (cid:107) ˆQ(cid:107)2. We shall proceed
in the following steps.

(cid:112)(cid:107)B(cid:107)2/

1. Derive the “rough” bound (cid:107)∆X(cid:107)F = O(u(cid:107)X(cid:107)2
2
2. Use above to show (cid:107)B 1
3. Use above and (5.10) to prove the “tight” bound (cid:107)∆X(cid:107)F = O(u(cid:107)X(cid:107)2

2 ˆQ(cid:107)2 = O(1) and (cid:107) ˆQ(cid:107)2 = O(1/(cid:112)σn(B)).

s).

(cid:112)(cid:107)B(cid:107)2/

√

(cid:112)κ2(B)).

18

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

4. Use above to prove Theorem 5.3.
To establish the ﬁrst statement, we express ∆x(cid:62)
i

Substituting (5.8) into (5.10), we have

in terms of x(cid:62)

i , ˆR and ∆ ˆRi.

(5.21)

where

(5.22)

∆x(cid:62)

i = x(cid:62)

i ( ˆR + ∆ ˆRi)−1∆ ˆRi = x(cid:62)

i (I + ˆR−1∆ ˆRi)−1 ˆR−1∆ ˆRi = x(cid:62)

i ∆ ˘Ri,

∆ ˘Ri = (I + ˆR−1∆ ˆRi)−1 ˆR−1∆ ˆRi.

Here, (cid:107) ˆR−1∆ ˆRi(cid:107)2 can be bounded using (5.15) and (5.20) as

(cid:107) ˆR−1∆ ˆRi(cid:107)2 ≤ (cid:107) ˆR−1(cid:107)2(cid:107)∆ ˆRi(cid:107)2 ≤

(5.23)

≤

≤

√

(cid:112)(cid:107)B(cid:107)2
nu(cid:107)X(cid:107)2
1.1n
(cid:112)σn(X (cid:62)BX) + 0.9s

√

1.1n

nu(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2
(cid:112)0.9 · 11n(n + 1)u(cid:107)X(cid:107)2
(cid:114) 1.21
9.9

· nu ≤ 0.05.

2(cid:107)B(cid:107)2

Thus, we can rewrite ∆ ˘Ri using the Neumann expansion as ∆ ˘Ri = (cid:80)∞
Hence,

k=1( ˆR−1∆ ˆRi)k.

(cid:107)∆ ˘Ri(cid:107)2 ≤

∞
(cid:88)

((cid:107) ˆR−1(cid:107)2(cid:107)∆ ˆRi(cid:107)2)k

k=1

=

≤

(5.24)

(cid:107) ˆR−1(cid:107)2(cid:107)∆ ˆRi(cid:107)2
1 − (cid:107) ˆR−1(cid:107)2(cid:107)∆ ˆRi(cid:107)2

1
1 − 0.05

·

√

(cid:112)(cid:107)B(cid:107)2
1.1n
nu(cid:107)X(cid:107)2
(cid:112)σn(X (cid:62)BX) + 0.9s

√

(cid:112)(cid:107)B(cid:107)2
1.2n
nu(cid:107)X(cid:107)2
(cid:112)σn(X (cid:62)BX) + 0.9s

.

≤

Together with the fact (cid:107)∆x(cid:62)
(5.25)

(cid:107)∆X(cid:107)F =

(cid:107)∆x(cid:62)

i (cid:107)2 ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

i=1

i (cid:107) ≤ (cid:107)x(cid:62)

i (cid:107) (cid:107)∆ ˘Ri(cid:107)2, we can bound (cid:107)∆X(cid:107)F as

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

i=1

(cid:107)x(cid:62)

i (cid:107)2 · max
1≤i≤m

(cid:107)∆ ˘Ri(cid:107)2 ≤

(cid:112)(cid:107)B(cid:107)2
1.2n2u(cid:107)X(cid:107)2
2
(cid:112)σn(X (cid:62)BX) + 0.9s

,

where we used

(cid:113)(cid:80)m

i=1 (cid:107)x(cid:62)

i (cid:107)2 = (cid:107)X(cid:107)F ≤

√

n(cid:107)X(cid:107)2 for the last inequality.

5.2.3. Bounding (cid:107) ˆQ(cid:107)2. We now proceed to bound (cid:107) ˆQ(cid:107)2.
Lemma 5.1. Suppose that X ∈ Rm×n with m ≥ n satisﬁes (5.2) and (5.3). Then,
the matrix ˆQ obtained by applying the shiftedCholeskyQR algorithm in ﬂoating-point
arithmetic to X satisﬁes

(cid:107) ˆQ(cid:62)B ˆQ − I(cid:107)2 < 2,

hence

(5.26)

Moreover,

(5.27)

(cid:107)B

1

2 ˆQ(cid:107)2 <

√

3.

(cid:107) ˆQ(cid:107)2 ≤

√

3
(cid:112)σn(B)

.

SHIFTED CHOLESKYQR

19

Proof. We have

ˆQ(cid:62)B ˆQ = ˆR−(cid:62)(X + ∆X)(cid:62)B(X + ∆X) ˆR−1

= I − ˆR−(cid:62)(sI + E1 + E2) ˆR−1 + (X ˆR−1)(cid:62)B∆X ˆR−1
+ ˆR−(cid:62)∆X (cid:62)B(X ˆR−1) + ˆR−(cid:62)∆X (cid:62)B∆X ˆR−1.

Thus we can bound (cid:107) ˆQ(cid:62)B ˆQ − I(cid:107)2 as

(cid:107) ˆQ(cid:62)B ˆQ − I(cid:107)2 ≤ (cid:107) ˆR−1(cid:107)2

2(s + (cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2)
(cid:112)(cid:107)B(cid:107)2 (cid:107)B
F (cid:107)B(cid:107)2.

2(cid:107)∆X(cid:107)2

+2(cid:107) ˆR−1(cid:107)2
+(cid:107) ˆR−1(cid:107)2

1

2 X ˆR−1(cid:107)2(cid:107)∆X(cid:107)F

(5.28)

The ﬁrst term of (5.28) can be bounded as

(5.29)

(cid:107) ˆR−1(cid:107)2

2(sI + (cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2) ≤

1.1s
σn(X (cid:62)BX) + 0.9s

≤

1.1
0.9

.

and for the second term in (5.28), using (5.15), (5.17) and (5.25) we obtain

2(cid:107) ˆR−1(cid:107)2

(cid:112)(cid:107)B(cid:107)2 (cid:107)B

1

2 X ˆR−1(cid:107)2(cid:107)∆X(cid:107)F

≤ 2 ·

1
(cid:112)σn(X (cid:62)BX) + 0.9s
11 s

2 · 1.5 · 1.2 · 1
0.9s

4
11

.

=

(5.30)

≤

· (cid:112)(cid:107)B(cid:107)2 · 1.5 ·

(cid:112)(cid:107)B(cid:107)2
1.2n2u(cid:107)X(cid:107)2
2
(cid:112)σn(X (cid:62)BX) + 0.9s

For the third term in (5.28), from (5.15) and (5.25)

(cid:107) ˆR−1(cid:107)2

2(cid:107)∆X(cid:107)2

F (cid:107)B(cid:107)2 ≤

(5.31)

≤

(1.2n2u(cid:107)X(cid:107)2
2
σn(X (cid:62)BX) + 0.9s

(cid:112)(cid:107)B(cid:107)2)2

· (cid:107)B(cid:107)2

1
σn(X (cid:62)BX) + 0.9s
(1.2 · 1
11 s)2
16
(0.9s)2 =
1089

·

.

Summarizing, we can bound the right-hand side of (5.28) as (cid:107) ˆQ(cid:62)B ˆQ − I(cid:107)2 < 2, as
required.

To derive (5.27), let B = V DV (cid:62) and ˆQ(cid:62)B ˆQ = U ΛU (cid:62) be the symmetric eigen-
value decompositions of B and ˆQ(cid:62)B ˆQ, respectively. Then, from ˆQ(cid:62)V DV (cid:62) ˆQ =
U ΛU (cid:62), we have

(5.32)

Λ− 1

2 U (cid:62) ˆQ(cid:62)V DV (cid:62) ˆQU Λ− 1

2 = I.

Hence there exists an orthogonal matrix W such that

(5.33)

D

1

2 V (cid:62) ˆQU Λ− 1

2 = W.

Noting that (cid:107)Λ(cid:107)2 < 3 and (cid:107)D−1(cid:107)2 = (σn(B))−1, we can bound (cid:107) ˆQ(cid:107)2 as

(5.34)

(cid:107) ˆQ(cid:107)2 ≤ (cid:107)D− 1

2 (cid:107)2(cid:107)Λ

1

2 (cid:107)2 ≤

√

3/(cid:112)σn(B).

20

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

5.3. Bounding the residual. We now bound the residual.
Lemma 5.2. Suppose that X ∈ Rm×n with m ≥ n satisﬁes (5.2) and (5.3). Then,
the matrices ˆQ, ˆR obtained by Algorithm 5.1 in ﬂoating-point arithmetic to X satisﬁes

(5.35)

and

(5.36)

(cid:107) ˆQ ˆR − X(cid:107)F
(cid:107)X(cid:107)2

≤ 2n2u(cid:112)κ2(B),

(cid:107) ˆQ ˆR − X(cid:107)F ≤ γn

√

n(cid:107) ˆQ(cid:107)F (cid:107) ˆR(cid:107)2.

Proof. First note that

(5.37)

(cid:107)ˆq(cid:62)
i

ˆR − x(cid:62)

i (cid:107) = (cid:107)ˆq(cid:62)
i

ˆR − ˆq(cid:62)

i ( ˆR + ∆ ˆRi)(cid:107) ≤ (cid:107)ˆq(cid:62)

i ∆ ˆRi(cid:107) ≤ (cid:107)ˆq(cid:62)

i (cid:107)(cid:107)∆ ˆRi(cid:107)2.

Substituting (5.20) into this gives

(5.38)

(cid:107)ˆq(cid:62)
i

ˆR − x(cid:62)

i (cid:107) ≤ (cid:107)ˆq(cid:62)

i (cid:107) · 1.1n

√

nu(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2.

On the other hand, from (5.27) we have

(5.39)

so it follows that

(cid:107) ˆQ(cid:107)F <

√

3n/(cid:112)σn(B),

(cid:107)∆X(cid:107)F = (cid:107) ˆQ ˆR − X(cid:107)F =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

i=1

(cid:107)ˆq(cid:62)
i

ˆR − x(cid:62)

i (cid:107)2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

≤

(cid:107)ˆq(cid:62)

i (cid:107)2 · 1.1n

√

nu(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2 = (cid:107) ˆQ(cid:107)F · 1.1n

√

nu(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2

(5.40)

i=1
≤ 2n2u(cid:107)X(cid:107)2

(cid:112)κ2(B).

To obtain the second bound in the statement, we use (5.18) in (5.37) to obtain (cid:107)ˆq(cid:62)
i
x(cid:62)
i (cid:107) ≤ γn

i (cid:107)(cid:107) ˆR(cid:107)2, hence

n(cid:107)ˆq(cid:62)

√

ˆR−

(cid:107) ˆQ ˆR − X(cid:107)2

F ≤ γ2

nn

m
(cid:88)

i=1

(cid:107)ˆq(cid:62)

i (cid:107)2(cid:107) ˆR(cid:107)2

2 = γ2

nn(cid:107) ˆQ(cid:107)2

F (cid:107) ˆR(cid:107)2
2,

as required.

Lemma 5.2, in particular (5.36), shows that shiftedCholeskyQR gives optimal

residual up to a factor bounded by a low-degree polynomial of m, n.

5.4. Main result. We are now ready to state the main result of the section,

which bounds the quantity (cid:107) ˆQ(cid:107)2
measure for ˆQ, reducing to κ2( ˆQ) when B = I.

(cid:112)(cid:107)B(cid:107)2/

(cid:113)

σn( ˆQ(cid:62)B ˆQ). Note that this is a B-orthonormality

Theorem 5.3. With one step of shiftedCholeskyQR applied in double precision
arithmetic to X satisfying (5.1)–(5.3) with shift s satisfying (5.4), we obtain ˆQ with
(deﬁning α =

)

s
(cid:107)X(cid:107)2
2(cid:107)B(cid:107)2

(5.41)

(cid:107) ˆQ(cid:107)2
(cid:113)

(cid:112)(cid:107)B(cid:107)2
σn( ˆQ(cid:62)B ˆQ)

√

≤ 2

(cid:115)

3 ·

1 + α ·

(cid:107)X(cid:107)2
2(cid:107)B(cid:107)2
σn(X (cid:62)BX)

· (cid:112)κ2(B)

SHIFTED CHOLESKYQR

21

Proof. Recall from (5.27) that (cid:107) ˆQ(cid:107)2 <
2 ˆQ) from below. Note that B 1

bound σn(B 1
Using Weyl’s theorem gives

√

3/(cid:112)σn(B). The remaining task is to
2 ∆X ˆR−1 from (5.11).

2 X ˆR−1 + B 1

2 ˆQ = B 1

(5.42)

σn(B

1

2 ˆQ) ≥ σn(B

1

2 X ˆR−1) − (cid:107)B

1

2 ∆X ˆR−1(cid:107)2.

Using (5.15) and (5.40) we obtain

(5.43)

(cid:107)B

1

2 ∆X ˆR−1(cid:107)2 ≤ (cid:112)(cid:107)B(cid:107)2(cid:107)∆X(cid:107)F (cid:107) ˆR−1(cid:107)2 ≤

2n2u(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2
(cid:112)σn(X (cid:62)BX) + 0.9s

(cid:112)κ2(B)

.

Note that this is O(u 1
constants.

2 (cid:112)κ2(B)) when we regard low-degree polynomials in m, n as

We next bound σn(B 1

2 X ˆR−1) from below. We start with the equation

(5.44)

ˆR−(cid:62)(X (cid:62)BX + sI) ˆR−1 = I − ˆR−(cid:62)(E1 + E2) ˆR−1.

Let B 1

2 X = U ΣV (cid:62) by the SVD. Then for a diagonal matrix G, we can write

(5.45)

X (cid:62)BX + sI = (U (Σ + G)V (cid:62))(cid:62)(U (Σ + G)V (cid:62)).

Indeed the left-hand side is V (Σ2 + sI)V (cid:62) and the right-hand side V (Σ + G)2V (cid:62), so
we can take

(5.46)

G = (Σ2 + sI)

1
2 − Σ = diag

(cid:18)(cid:113)

σi(X (cid:62)BX) + s −

(cid:113)

σi(X (cid:62)BX)

(cid:19)

Now setting T = U (Σ + G)V (cid:62) ˆR−1 we have
(5.47)

T (cid:62)T = ˆR−(cid:62)V (Σ + G)2V (cid:62) ˆR−1 = ˆR−(cid:62)(X (cid:62)BX + sI) ˆR−1 = I − ˆR−(cid:62)(E1 + E2) ˆR.

We next bound the singular values of T . By (5.14) and (5.15) we have

(5.48) (cid:107) ˆR−(cid:62)(E1 + E2) ˆR−1(cid:107)2 ≤ (cid:107) ˆR−1(cid:107)2

2((cid:107)E1(cid:107)2 + (cid:107)E2(cid:107)2) ≤

0.1s
σn(X (cid:62)BX) + 0.9s

≤

1
9

,

(cid:113)

(cid:113)

1 − 1
so σi(T ) ∈ [
9 ,
SVD where E(cid:48) is diagonal, we have

1 + 1

9 ] ⊆ [0.9, 1.1]. Therefore, letting T = U (cid:48)(I + E(cid:48))V (cid:48) be the

(5.49)

T = U (cid:48)(I + E(cid:48))V (cid:48)(cid:62) = U (cid:48)V (cid:48)(cid:62)(I + V (cid:48)E(cid:48)V (cid:48)(cid:62)) = W (I + E),

where W = U (cid:48)V (cid:48)(cid:62) has orthonormal columns, and (cid:107)E(cid:107)2 = (cid:107)V (cid:48)E(cid:48)V (cid:48)(cid:62)(cid:107)2 = (cid:107)E(cid:48)(cid:107)2 ≤
0.1.

Now plugging into (5.49) the deﬁnition of T gives

(5.50)

W (I + E) = U (Σ + G)V (cid:62) ˆR−1.

Recalling that B 1

2 X = U ΣV (cid:62), we have σi(B 1

2 X ˆR−1) = σi(ΣV (cid:62) ˆR−1) and

(5.51)

= diag

ΣV (cid:62) ˆR−1 = Σ(Σ + G)−1U (cid:62)W (I + E)
(cid:33)

(cid:32) (cid:112)σi(X (cid:62)BX)
(cid:112)σi(X (cid:62)BX) + s

U (cid:62)W (I + E).

22

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

Using the general inequality for singular values of matrix products σmin(AB) ≥
σmin(A)σmin(B) (applicable if A or B is square) we obtain

(5.52)

σn(B

1

2 X ˆR−1) ≥

(cid:112)σn(X (cid:62)BX)
(cid:112)σn(X (cid:62)BX) + s

· 0.9.

By inserting this and (5.43) into (5.42), we obtain

σn(B

1

2 ˆQ) ≥

≥

=

≥

(5.53)

≥

0.9(cid:112)σn(X (cid:62)BX)
(cid:112)σn(X (cid:62)BX) + s
0.9(cid:112)σn(X (cid:62)BX)
(cid:112)σn(X (cid:62)BX) + s
0.9
(cid:112)σn(X (cid:62)BX) + s
0.9
(cid:112)σn(X (cid:62)BX) + s
(cid:112)σn(X (cid:62)BX)
2(cid:112)σn(X (cid:62)BX) + s

−

2n2u(cid:107)X(cid:107)2

(cid:112)κ2(B)

(cid:112)(cid:107)B(cid:107)2
(cid:112)σn(X (cid:62)BX) + 0.9s
(cid:112)κ2(B)
(cid:112)(cid:107)B(cid:107)2
0.9(cid:112)σn(X (cid:62)BX) + s

2n2u(cid:107)X(cid:107)2

−

√

(cid:18)(cid:113)

(cid:18)(cid:113)

σn(X (cid:62)BX) −

2
√

0.9

0.9

· n2u(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2

(cid:19)
(cid:112)κ2(B)

σn(X (cid:62)BX) − 0.4

(cid:113)

(cid:19)

σn(X (cid:62)BX)

=

2

(cid:113)

1
1 + α · (cid:107)X(cid:107)2

2(cid:107)B(cid:107)2
σn(X (cid:62)BX)

,

where we used (5.1) in the fourth inequality and the deﬁnition of α in the last equality.

Together with (5.27) we obtain

(cid:107) ˆQ(cid:107)2
(cid:113)

(cid:112)(cid:107)B(cid:107)2
σn( ˆQ(cid:62)B ˆQ)

(cid:115)

≤ 2

1 + α ·

(cid:107)X(cid:107)2
2(cid:107)B(cid:107)2
σn(X (cid:62)BX)

·

√

3
(cid:112)σn(B)

· (cid:112)(cid:107)B(cid:107)2

√

= 2

(cid:115)

3 ·

1 + α ·

(cid:107)X(cid:107)2
2(cid:107)B(cid:107)2
σn(X (cid:62)BX)

· (cid:112)κ2(B).

(5.54)

Thus we conclude that, provided that α(cid:107)X(cid:107)2

(5.55)

(cid:107) ˆQ(cid:107)2
(cid:113)

(cid:112)(cid:107)B(cid:107)2
σn( ˆQ(cid:62)B ˆQ)

(cid:46) 2(cid:112)3ακ2(B) ·

2(cid:107)B(cid:107)2/σn(X (cid:62)BX) (cid:29) 1,
(cid:112)(cid:107)B(cid:107)2
(cid:107)X(cid:107)2
(cid:112)σn(X (cid:62)BX)

.

Thus applying one step of shiftedCholeskyQR results in the condition number being
√
reduced by about a factor (cid:112)ακ2(B) =

sκ2(B)
√

.

(cid:107)X(cid:107)2

(cid:107)B(cid:107)2

5.5. shiftedCholeskyQR3 in oblique inner product. We now describe the
analogue of shiftedCholeskyQR3 for an oblique inner product space. We continue to
σn( ˆQ(cid:62)B ˆQ) <
assume (5.1)–(5.4); a particular case of interest is u− 1
u−1.

(cid:112)(cid:107)B(cid:107)2/

2 < (cid:107) ˆQ(cid:107)2

(cid:113)

Choice of shift s. As in Section 4.1, we need to take into account the error term
E1 in (5.6), incurred in the computation of A = X T BX. From (5.6), we obtain a
lower bound

(5.56)

λn( ˆA) ≥ λn(X (cid:62)BX) − (cid:107)E1(cid:107)2.

SHIFTED CHOLESKYQR

23

Accordingly to apply (4.2), we ﬁrst shift ˆA by the upper bound in (5.12). Thus a
choice of s that avoids numerical breakdown is

(5.57)

√

˜s := 2.2m

mnu(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2 + cn+2utr( ˆA + 2.2m

√

mnu(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2I).

Together with the assumption (5.4), we obtain

(5.58)
First we note that ˆA = X T BX + E1 from (5.6). As in (4.6) and (4.7), we have

s := max(11{2m

mn + n(n + 1)}u(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2, ˜s).

√

tr(E1) ≤ 2.2mn

√

mnu(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2, tr(X (cid:62)BX) ≤ n(cid:107)X (cid:62)BX(cid:107)2 ≤ n(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2.

Thus, we can bound ˜s as

√

√

˜s := 2.2m

mnu(cid:107)X(cid:107)2
2(cid:107)B(cid:107)2
+2.2(n + 1)u (cid:0)n(cid:107)X(cid:107)2

≤ 2.4{m

mn + n(n + 1)}u(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2 + 2.2mn
2(cid:107)B(cid:107)2,

√

mnu(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2 + 2.2mn

√

mnu(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2

(cid:1)

which shows that the maximum in (5.58) is

(5.59)

s := 11{2m

√

mn + n(n + 1)}u(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2.

We now summarize the algorithm in pseudocode in Algorithm 5.2, which is the B-
orthogonal analogue of Algorithm 4.2. We still call the algorithm shiftedCholeskyQR3
(supressing B as the two algorithms are essentially equivalent when B = I, aside
from a slight diﬀerence in the shift strategy). The iterated Algorithm 4.1 can also
be extended to B (cid:54)= I similarly; we omit its pseudocode for brevity. In the analysis
below, we focus on the case u− 1

(cid:112)(cid:107)B(cid:107)2/(cid:112)σn(X (cid:62)BX) < u−1.

2 < (cid:107)X(cid:107)2

Algorithm 5.2 shiftedCholeskyQR3 for X = QR, QT BQ = In.
1: Let Q := X
2: A := QT BQ
√
3: s := 11{2m
4: R := chol(A + sI)
5: Q := QR−1
6: ˜R := chol(QT Q), Q := Q ˜R−1, R := ˜RR
7: ˜R := chol(QT Q), Q := Q ˜R−1, R := ˜RR

mn + n(n + 1)}u(cid:107)X(cid:107)2

2(cid:107)B(cid:107)2

When is thrice enough?. Here we derive a condition on (cid:107) ˆQ(cid:107)2

σn( ˆQ(cid:62)B ˆQ)
that guarantees that shiftedCholeskyQR3 gives a numerically stable QR factorization
of X.

(cid:112)(cid:107)B(cid:107)2/

(cid:113)

Recall that (5.54) gives a bound by shiftedCholeskyQR:

(5.60)

(cid:107) ˆQ(cid:107)2
(cid:113)

(cid:112)(cid:107)B(cid:107)2
σn( ˆQ(cid:62)B ˆQ)

√

≤ 2

(cid:115)

3 ·

1 + α ·

(cid:107)X(cid:107)2
2(cid:107)B(cid:107)2
σn(X (cid:62)BX)

· (cid:112)κ2(B).

As in (5.59), to guarantee avoidance of breakdown we take α = 11{2m
1)}u, so we have

√

mn + n(n +

(5.61)

(cid:107) ˆQ(cid:107)2
(cid:113)

(cid:112)(cid:107)B(cid:107)2
σn( ˆQ(cid:62)B ˆQ)

√

≤ 2

(cid:115)

3·

1 + 11{2m

√

mn + n(n + 1)}u

(cid:107)X(cid:107)2
2(cid:107)B(cid:107)2
σn(X (cid:62)BX)

·(cid:112)κ2(B).

24

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

On the other hand, as shown in [17], a suﬃcient condition for CholeskyQR2 in an
oblique inner product to compute a stable QR factorization of ˆQ is

(5.62)

(cid:107) ˆQ(cid:107)2
(cid:113)

(cid:112)(cid:107)B(cid:107)2
σn( ˆQ(cid:62)B ˆQ)

≤

√

8(cid:112)(m

1

mn + n(n + 1))u

.

Combining these facts, we obtain the following condition under which shiftedCholeskyQR3
is guaranteed to compute a numerically stable QR factoriziation:

(cid:115)

3·

√
2

√

1 + 11{2m

mn + n(n + 1)}u

(cid:107)X(cid:107)2
2(cid:107)B(cid:107)2
σn(X (cid:62)BX)

·(cid:112)κ2(B) ≤

√

8(cid:112)(m

1

mn + n(n + 1))u

.

√

(cid:107)X(cid:107)2
√

(cid:107)B(cid:107)2
σn(X (cid:62)BX)

If

> u− 1

2 , we have

√

1+11{2m

mn+n(n+1)}u

(cid:112)(cid:107)B(cid:107)2
(cid:107)X(cid:107)2
(cid:112)σn(X (cid:62)BX)

√

(cid:39) 11{2m

mn+n(n+1)}u

(cid:112)(cid:107)B(cid:107)2
(cid:107)X(cid:107)2
(cid:112)σn(X (cid:62)BX)

and the condition can be simpliﬁed as

(5.63)

(cid:112)(cid:107)B(cid:107)2
(cid:107)X(cid:107)2
(cid:112)σn(X (cid:62)BX)

≤

√

96{2m

u−1

mn + n(n + 1)}(cid:112)κ2(B)

.

Note that this ensures that the condition (5.1) for the error analysis in Subsection 5.1
is automatically satisﬁed.

Numerical stability. We now examine the numerical stability of shiftedCholeskyQR3

and show that it enjoys excellent stability both in orthogonality and backward error.
Theorem 5.4. Let B ∈ Rm×m (cid:31) 0 and X ∈ Rm×n be a matrix satisfying (5.63),

and 80κ2(B)(m
computes a QR factorization X = QR satisfying the B-orthogonality measure

mnu+n(n+1)u) ≤ 1. Then shiftedCholeskyQR followed by CholeskyQR2

√

(5.64)

(cid:107) ˆQ(cid:62)B ˆQ − I(cid:107)F ≤ 8[m

√

mnu + n(n + 1)u]κ2(B),

and backward error

(5.65)

(cid:107) ˆQ ˆR − X(cid:107)F
(cid:107) ˆX(cid:107)2

≤ 16n2u(κ2(B))3/2.

√

Proof. When both (5.64) and 80κ2(B)(m

mnu + n(n + 1)u) ≤ 1 hold, the as-
sumptions (5.1), (5.2) and (5.3) for the application of the shifted Cholesky QR al-
gorithm are automatically satisﬁed and the computed orthogonal factor ˆQ satisﬁes
(5.62). Then the B-orthogonal version of CholeskyQR2 can be safely applied to ˆQ
and the resulting orthogonal factor ˆZ satisﬁes (5.64), as shown in Theorem 2 of [17].
We next establish (5.65). By (5.11), after the ﬁrst shiftedCholeskyQR we have

(5.66)

X + ∆X = ˆQ ˆR,

where (cid:107)∆X(cid:107)F is bounded as in (5.35). We apply CholsekyQR2 to ˆQ to obtain the
QR factorization ˆQ = ZU . We have

(5.67)

ˆQ + ∆ ˆQ = ˆZ ˆU

SHIFTED CHOLESKYQR

25

where

(5.68)

(cid:107)∆ ˆQ(cid:107)F ≤ 5n2u(cid:107) ˆQ(cid:107)2κ2(B) ≤

√
5
3
(cid:112)σn(B)

κ2(B)n2u

from (5.27) and [17, Thm. 3]. The upper triangular factor S in the QR factorization
of X is obtained as

(5.69)

ˆS = ˆU ˆR + ∆S

where ∆S is the forward error in the matrix multiplication. Summarizing, we can
bound the overall backward error as

(cid:107) ˆZ ˆS − X(cid:107)F = (cid:107) ˆZ( ˆU ˆR + ∆S) − ˆQ ˆR + ∆X(cid:107)F

= (cid:107)∆ ˆQ ˆR + ˆZ∆S + ∆X(cid:107)F
≤ (cid:107)∆ ˆQ(cid:107)F (cid:107) ˆR(cid:107)2 + (cid:107) ˆZ(cid:107)2(cid:107)∆S(cid:107)F + (cid:107)∆X(cid:107)F .

(5.70)

We now bound the terms in the right-hand side. For the ﬁrst term, we can bound
(cid:107)∆ ˆQ(cid:107)F as (5.68). To bound (cid:107) ˆR(cid:107)2, we use (5.4), (5.7) and (5.14) to obtain
√

(cid:107) ˆR(cid:107)2 ≤

1.1(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2.

(5.71)

We next bound the second term in (5.70). By [17], (cid:107) ˆZ(cid:107)2 can be bounded as

(5.72)

(cid:107) ˆZ(cid:107)2 ≤

√

2
(cid:112)σn(B)

.

Regarding (cid:107)∆S(cid:107)F , using the general error bound for matrix multiplications |∆S| ≤
γn| ˆU | | ˆR| we obtain

(5.73)

(cid:107)∆S(cid:107)F ≤ γn(cid:107) | ˆU | | ˆR| (cid:107)F ≤ γn(cid:107) ˆU (cid:107)F (cid:107) ˆR(cid:107)F ≤ nγn(cid:107) ˆU (cid:107)2(cid:107) ˆR(cid:107)2.

Here (cid:107) ˆR(cid:107)2 can be bounded as in (5.71). To bound (cid:107) ˆU (cid:107)2, we recall (5.67) and left-
multiply ˆZ (cid:62)B to obtain

(5.74)

ˆZ (cid:62)B( ˆQ + ∆ ˆQ) = ˆZ (cid:62)B ˆZ ˆU .

Now, by [17, Thm. 2], the eigenvalues of ˆZ (cid:62)B ˆZ lie in the interval [1−8κ2(B)(m
n(n + 1)u), 1 + 8κ2(B)(m

mnu + n(n + 1)u)], so it follows that

√

√

mnu+

(cid:107) ˆU (cid:107)2 ≤ (cid:107)( ˆZ (cid:62)B ˆZ)−1(cid:107)2(cid:107) ˆZ (cid:62)(cid:107)2(cid:107)B(cid:107)2((cid:107) ˆQ(cid:107)2 + (cid:107)∆ ˆQ(cid:107)2)
1
√
mn + n(n + 1))u

1 − 8κ2(B)(m

2
(cid:112)σn(B)

√

≤

(cid:107)B(cid:107)2

(cid:32) √

3
(cid:112)σn(B)

√

5
3
(cid:112)σn(B)

+

(cid:33)

κ2(B)n2u

Using the assumption 80κ2(B)(m
we have

√

mnu + n(n + 1)u) ≤ 1 (which is [17, eqn. (1)]),

(5.75)

(cid:107) ˆU (cid:107)2 ≤ 2.9κ2(B).

Finally, we can bound (cid:107)∆X(cid:107)F as in (5.35).

√
5
3
(cid:112)σn(B)
√
2
(cid:112)σn(B)

+

26

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

Combining the above bounds and substituting into (5.70) yields

(cid:107) ˆZ ˆS − X(cid:107)F ≤

κ2(B)n2u ·

√

1.1(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2

· 1.02n2u · 2.9κ2(B)

√

1.1(cid:107)X(cid:107)2

(cid:112)(cid:107)B(cid:107)2 + 2n2u(cid:112)κ2(B)(cid:107)X(cid:107)2

≤ 16n2u(cid:107)X(cid:107)2(κ2(B))3/2,

as required.

Experiments indicate that the bounds in Theorem 5.4 are overestimates, in par-

ticular the dependence on κ2(B) appears to be much weaker.

6. Numerical experiments. In this section we present some numerical experi-
ments to illustrate our results. All computations were carried out on MATLAB 2017b
and IEEE standard 754 binary64 (double precision) in Mac OS X version 10.13 with
2 GHz Intel Core i7 Duo processor, so that u = 2−53 ≈ 1.11 × 10−16.

6.1. Convergence with CholeskyQR iterates. First, we take B = I and
examine how κ2( ˆQ(k)) and (cid:107) ˆQ(k)(cid:62) ˆQ(k) − I(cid:107)2 are reduced after k (shifted)Cholesky
QR steps. We also compare shiftedCholeskyQR3 with the mixed-precision Cholesky
QR (mixedCholQR) [18], which uses doubled precision for the ﬁrst two steps (1.1),
(1.2) and repeats the process in double precision. To run mixedCholQR we used
the Multiprecision Computing Toolbox [11], which enables computation in MATLAB
with arbitrary precision. We generate test matrices with a speciﬁed condition number
by forming

(6.1)

X := U ΣV T ∈ Rm×n,

where U is an m × n random orthogonal matrix obtained by taking the QR factor-
ization of a random matrix, V is an n × n random orthogonal matrix and

Σ = diag(1, σ

1
n−1 , · · · , σ

n−2
n−1 , σ).

Here, 0 < σ < 1 is some constant. This is essentially MATLAB’s randsvd construc-
tion. Thus (cid:107)X(cid:107)2 = 1 and the 2-norm condition number of X is κ2(X) = 1/σ. Let k
denote the number of iterations. In Table 6.1, κ2(X) = 1012, m = 1000, n = 30. In
Table 6.2, κ2(X) = 1013, m = 100, n = 100.

Tables 6.1 and 6.2 illustrate that the conditioning κ2( ˆQ(1)) is improved by shifted-
2 . Then (cid:107) ˆQ(1)(cid:62) ˆQ(1) − I(cid:107)2 = O(1)
α)κ2(X) (cid:46) u− 1
CholeskyQR to approximately O(
and CholeskyQR2 safely computes the QR factorization of of ˆQ(1); these are all consis-
tent with Theorem 3.3 and Lemma 3.1. We also see that κ2( ˆQ(1)) ≈ uκ2(X) = O(1)
in mixedCholQR, which is consistent with Theorem 3.4 in [18]. Here shiftedC-
holeskyQR3 requires one more iteration than mixedCholQR, which is a typical be-
havior and reﬂects the theory for κ2(X) ∈ (u−1/2, u−1). shiftedCholeskyQR3 has the
advantage over mixedCholQR that no high-precision arithmetic is needed, thereby
being much faster in practice; indeed here it was faster by orders of magnitude.

√

We next turn to B (cid:54)= I. We set B to be a SPD matrix as above, with U = V . We

illustrate Theorem 5.3 by examining how
(cid:112)ακ2(B) where α =

√
s
√
(cid:107)B(cid:107)2

(cid:107)X(cid:107)2

√

(cid:107)X(cid:107)2
√

(cid:107)B(cid:107)2
σn(X (cid:62)BX)

is reduced as compared with

. We set X to be a random matrix with a speciﬁed

SHIFTED CHOLESKYQR

27

condition number as in (6.1) and form B (cid:23) 0 as in (6.1), now taking V = U T . In
Table 6.3, we took κ2(X) = 1012, κ2(B) = 108, and m = 300, n = 30.
√
Table 6.3 also conﬁrms that
is improved by shiftedCholeskyQR3
2 (experiments suggest that the (cid:112)κ2(B) dependence
by a factor O((cid:112)α/κ2(B)) (cid:46) u− 1
is often a signiﬁcant overestimate). CholeskyQR2 then safely completes the QR fac-
torization, as predicted by Theorem 5.3.

(cid:107) ˆQ(1)(cid:107)2
σn( ˆQ(1)T B ˆQ(1))

(cid:107)B(cid:107)2

√

Table 6.1: Results for test matrices with κ2(X) = 1012, m = 1000, n = 30, B = I.

k
1
2
3

κ2( ˆQ(k))
6.14 · 106
1.01
1.00

Algorithm 4.1
(cid:107) ˆQ(k)(cid:62) ˆQ(k) − I(cid:107)2
1.00
1.70 · 10−4
5.66 · 10−16

√

α
1.80 · 10−5
-
-

mixedCholQR

κ2( ˆQ(k))
1.00
1.00
-

(cid:107) ˆQ(k)(cid:62) ˆQ(k) − I(cid:107)2
9.88 · 10−6
9.04 · 10−16
-

Table 6.2: Results for test matrices with κ2(X) = 1013, m = 100, n = 100, B = I.

k
1
2
3

κ2( ˆQ(k))
4.95 · 107
1.02
1.00

Algorithm 4.1
(cid:107) ˆQ(k)(cid:62) ˆQ(k) − I(cid:107)2
1.00
1.93 · 10−2
1.07 · 10−15

√

α
1.48 · 10−5
-
-

mixedCholQR

κ2( ˆQ(k))
1.00
1.00
-

(cid:107) ˆQ(k)(cid:62) ˆQ(k) − I(cid:107)2
3.62 · 10−4
1.15 · 10−15
-

Table 6.3: Results for test matrices with B (cid:54)= I, (cid:107)X(cid:107)2
1.38 × 1010, m = 300, n = 30.

(cid:112)(cid:107)B(cid:107)2/(cid:112)σn(X (cid:62)BX) =

√

√

(cid:107)B(cid:107)2

(cid:107) ˆQ(k)(cid:107)2
σn( ˆQ(k)T B ˆQ(k))
4.11 · 108
13.50
13.50

k

1
2
3

Algorithm 5.2
(cid:107) ˆQ(k)(cid:62)B ˆQ(k) − I(cid:107)2

1.00
8.31 · 10−3
3.49 · 10−15

mixedCholQR

(cid:112)ακ2(B)
8.41 · 10−2
-
-

√

√

(cid:107)B(cid:107)2

(cid:107) ˆQ(k)(cid:107)2
σn( ˆQ(k)T B ˆQ(k))
13.50
13.50

k

1
2

(cid:107) ˆQ(k)(cid:62)B ˆQ(k) − I(cid:107)2

4.82 · 10−5
3.34 · 10−15

Remark 1. The choice of s in (5.59) tends to be a conservative overestimate, and
in most cases, a successful Cholesky factorization can be computed with a smaller shift,
such as A + (u(cid:107)X(cid:107)2
2(cid:107)B(cid:107)2)I. It can be seen (if Cholesky still does not break down) that
√
the reduction factor of κ2( ˆQ(k)) improves to about (
u)(k) after k shiftedCholeskyQR
steps. To illustrate this, in Figure 6.1 we show the values of κ2( ˆQ(1)) as we vary
the shift in shiftedCholeskyQR taking B = I, m = 1000, n = 50, κ2(X) = 1015.
2 ≈ 6.1 · 10−11 is shown in Fig-
Our “safe” choice s := 11{mn + n(n + 1)}u(cid:107)X(cid:107)2
ure 6.1 by a blue asterisk. In this case, κ2( ˆQ(1)) is larger than required by (4.11),

28

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

and more shiftedCholeskyQR iterations would be needed. On the other hand, if we
set s := u(cid:107)X(cid:107)2
2 for
CholeskyQR2 to work. However, there is no guarantee that the initial Cholesky fac-
torization chol(A + u(cid:107)X(cid:107)2

2, then ˆQ(1) will satisfy the suﬃcient condition κ2( ˆQ(1)) ≤ u− 1

2I) does not break down.

Fig. 6.1: κ2( ˆQ(1)) for test matrix with κ2(X) = 1015, m = 1000, n = 50, varying the
shift of shiftedCholeskyQR.

6.2. Orthogonality and residual. Next we examine the numerical stability of
shiftedCholeskyQR3 (shown in the ﬁgures as sCholQR3) and compare it with other
popular QR decomposition algorithms, namely, Householder QR, classical and modi-
ﬁed Gram-Schmidt (CGS and MGS; we also run them twice, CGS2 and MGS2). We
ﬁrst take B = I and vary κ2(X), m and n and investigate the dependence of the
orthogonality and residual on them. We set X as in (6.1). We examine the orthog-
onality and residual measured by the Frobenius norm under various conditions in
Figures 6.2 through 6.4. Figure 6.2 shows the orthogonality (cid:107) ˆQT ˆQ − I(cid:107)F and residual
(cid:107) ˆQ ˆR − X(cid:107)F , where we take m = 300, n = 10 and κ2(X) was varied from 108 to 1015.
In Figure 6.3, κ2(X) = 1012, n = 50 and m was varied from 1000 to 10000. In Figure
6.4, κ2(X) = 1012, m = 1000 and n was varied from 10 to 500.

We see in Figure 6.2 that with shiftedCholeskyQR, the orthogonality and the
residual are independent of κ2(X) and are of O(u), as long as κ2(X) is at most
O(u−1). This is in good agreement with Theorem 4.1. Figures 6.3 and 6.4 indicate
that the orthogonality and residual increase only mildly with m and n. Although they
are inevitably overestimates, these also reﬂect our results (4.13) and (4.14). Compared
with Householder QR, we observe that shiftedCholeskyQR3 usually produces slightly
better orthogonality and residual. With MGS, the deviation from orthogonality in-
creases proportionally to κ2(X). As is well known, Gram-Schmidt type algorithms
perform well when repeated twice, and we can verify this here. As mentioned in
Section 4.3, an advantage of shiftedCholeskyQR3 is that it is rich in BLAS-3 opera-
tions and easily parallelized (it ran more than ten times faster than Gram-Schmidt
algorithms in the experiments here). Overall, we see that shiftedCholeskyQR3 is an
eﬃcient and reliable method for matrices with condition number at most O(u−1).

Next, we again take B (cid:54)= I and test Algorithm 5.2, comparing it with the stability
of other popular QR decomposition algorithms, namely, MGS, CGS2 and MGS2.
We varied κ2(B), m and n and investigated the orthogonality (cid:107) ˆQT B ˆQ − I(cid:107)F and
residual (cid:107) ˆQ ˆR − X(cid:107)F . Figure 6.5 shows the results for the case m = 500, n = 20,
κ2(B) = 1010 and κ2(B) was varied from 101 to 1010. Figure 6.6 takes m = 300,
n = 50, κ2(X) = 1010 and κ2(B) was varied from 108 to 1015. In Figure 6.7 we took

10-1610-1410-1010-710-210810101014SHIFTED CHOLESKYQR

29

κ2(X) = 108, κ2(B) = 1010, n = 50 and m was varied from 500 to 2000. Figure 6.8
takes κ2(X) = 108, κ2(B) = 1010, m = 1000 and n was varied from 50 to 500.

From Figure 6.5 we see that the orthogonality and residual of shiftedCholeskyQR3
(cid:46) O(u−1), reﬂecting

are independent of κ2(X) and are of O(u), as long as

(cid:107)X(cid:107)2
√

√

(cid:107)B(cid:107)2
σn(X (cid:62)BX)

Theorem 5.4. Figure 6.6 shows that the orthogonality increase rather mildly with
κ2(B), indicating the dependence on κ2(B) suggested by 5.64 is perhaps improvable;
we leave this for future work. Figures 6.7 and 6.8 illustrate the orthogonality and
residual increase with m and n, again mildly. These are also in agreement with (5.64)
and (5.65) in Theorem 5.4; again, its m, n-dependence may be improvable. Compared
with CGS2 and MGS2, the orthogonality and residual of CGS2 and MGS2 and that
of shiftedCholeskyQR3 are of the same magnitude. Again, shiftedCholeskyQR3 has
the advantage of being parallelization-friendly. All our experiments corroborate that
shiftedCholeskyQR3 is a reliable method whether B = I or B (cid:54)= I, for matrices with
(cid:107)X(cid:107)2
√

< O(u−1).

√

(cid:107)B(cid:107)2
σn(X (cid:62)BX)

Fig. 6.2: Orthogonality (cid:107) ˆQ(cid:62) ˆQ − I(cid:107)F and residual (cid:107) ˆQ ˆR − X(cid:107)F for test matrices with
m = 300, n = 10, varying κ2(X).

Fig. 6.3: Orthogonality (cid:107) ˆQ(cid:62) ˆQ − I(cid:107)F and residual (cid:107) ˆQ ˆR − X(cid:107)F for test matrices with
κ2(X) = 1012, n = 50, varying m.

1010101510-1510-1010-5100CGS2MGS2MGSHouseholderQRsCholQR31010101510-1510-1010-5100CGS2MGS2MGSHouseholderQRsCholQR320004000600080001000010-1510-1010-5100CGS2MGS2MGSHouseholderQRsCholQR320004000600080001000010-1510-1010-5100CGS2MGS2MGSHouseholderQRsCholQR330

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

Fig. 6.4: Orthogonality (cid:107) ˆQ(cid:62) ˆQ − I(cid:107)F and residual (cid:107) ˆQ ˆR − X(cid:107)F for test matrices with
κ2(X) = 1012, m = 1000, varying n.

Fig. 6.5: Orthogonality (cid:107) ˆQ(cid:62)B ˆQ − I(cid:107)F and Residual (cid:107) ˆQ ˆR − X(cid:107)F for test matrices
with m = 500, n = 20, κ2(B) = 1010, varying κ2(X).

7. Runtime Performance. We next evaluate the runtime performance of shift-
edCholeskyQR3 in multi-core CPU environments for both the standard and the
oblique case involving a sparse symmetric, positive deﬁnite matrix.

7.1. Standard inner product. We used a compute node of the Laurel 2 super-
computer system installed at the Academic Center for Computing and Media Studies,
Kyoto University, whose speciﬁcations are listed in Table 7.1. Here we focus on the
standard B = I case, to facilitate comparison with available algorithms. Test matrices
are generated as in the previous experiments, using (6.1). Here, random orthogonal
matrices are obtained by applying the LAPACK Householder QR routines (dgeqrf
and dorgqr) to a random matrix. The code is written in Fortran90 and uses LAPACK
and BLAS routines.

Table 7.2 presents the computational time of several methods, where the QR
factorization of matrices whose condition number is 1011 is computed. Here, we com-
pare shiftedCholeskyQR3 (dgemm and dsyrk versions), Householder QR, and CGS2
(including its blocked version). It is worth noting that dgeqr is a novel LAPACK rou-
tine that appropriately uses the TSQR algorithm. The block width in Block CGS2
was empirically tuned. Table 7.2 clearly shows that shiftedCholeskyQR3 (both dgemm
and dsyrk versions) outperforms other methods for all cases. Among methods besides

2040608010010-1510-1010-5100CGS2MGS2MGSHouseholder QRsCholQR32040608010010-1510-1010-5100CGS2MGS2MGSHouseholder QRsCholQR3105101010-1510-1010-5100CGS2MGS2MGSsCholQR105101010-1510-1010-5100CGS2MGS2MGSsCholQRSHIFTED CHOLESKYQR

31

Fig. 6.6: Orthogonality (cid:107) ˆQ(cid:62)B ˆQ − I(cid:107)F and Residual (cid:107) ˆQ ˆR − X(cid:107)F for test matrices
with m = 300, n = 50, κ2(X) = 1010, varying κ2(B).

Fig. 6.7: Orthogonality (cid:107) ˆQ(cid:62)B ˆQ − I(cid:107)F and Residual (cid:107) ˆQ ˆR − X(cid:107)F for test matrices
with κ(X) = 108, κ2(B) = 1010, n = 50, varying m.

shiftedCholeskyQR3, dgeqr is fastest, probably due to employing TSQR, but shifted-
CholeskyQR3 (dsyrk) is more than 1.7 times faster in every case. These results also
indicate that, even if four iterations is required for ill-conditioned problems, iterated
CholeskyQR (shown in Algorithm 4.1) would still be faster than other methods.

It is of interest to compare shiftedCholeskyQR3 with mixedCholQR [18] from the
viewpoint of computational time, but implementing and highly tuning double-double
gemm or syrk routines (whose input matrices are in double precision) is generally
diﬃcult and requires signiﬁcant eﬀort. We thus estimate the computational cost of
mixedCholQR routine; we only discuss the case where gemm is used, but almost the
same discussion is applicable when we use syrk. Table 7.3 presents the results of the
benchmark for dgemm, which computes X (cid:62)X, where X is an m × n matrix. From
this table, we can assume that 700 GFLOPS is a rough upper bound of the achieved
performance. According to the paper on mixed precision CholeskyQR [18], the number
of double precision operations required in ddgemm (a double-double precision gemm
routine) is 8 times (Cray-style) or 12.5 times (IEEE-style) that required in dgemm.
Therefore, assuming that double precision operations in ddgemm are performed at 700
GFLOPS, we can estimate the time (sec.) of ddgemm as 8·2
7 · mn2 · 10−11 (Cray-style)
or 12.5·2

· mn2 · 10−11 (IEEE-style).

7

1010101510-1510-1010-5100CGS2MGS2MGSsCholQR31010101510-1510-1010-5100CGS2MGS2MGSsCholQR350010001500200010-1510-1010-5100CGS2MGS2MGSsCholQR50010001500200010-1510-1010-5100CGS2MGS2MGSsCholQR32

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

Fig. 6.8: Orthogonality (cid:107) ˆQ(cid:62)B ˆQ − I(cid:107)F and Residual (cid:107) ˆQ ˆR − X(cid:107)F for test matrices
with κ2(X) = 108, κ2(B) = 1010, m = 1000, varying n.

Table 7.1: Speciﬁcations of the Laurel 2 system.

Item
CPU
Number of CPUs / node
Memory size / node
Peak FLOPS / node
Compiler
Compile options

BLAS, LAPACK

Speciﬁcation
Intel Xeon E5-2695 v4 (Broadwell, 2.1 GHz, 18 cores)
2
128 GB
1.21 TFLOPS (in double precision)
Intel ifort ver. 17.0.6
-mcmodel=medium, -shared-intel, -qopenmp
-O3, -ipo, -xHost
Intel MKL ver. 2017.0.6 (-mkl=parallel)

Based on the above estimation and the breakdown of timing results of shifted-
CholeskyQR3, we compare shiftedCholeskyQR3 and mixed precision CholeskyQR in
Table 7.4. Here, we ignore the increasing cost for ddpotrf because it is small relative
to the total time. From the table, we can expect that shiftedCholeskyQR3 is faster
than mixed precision CholeskyQR in this computational environment. Considering
this estimation and the fact that a well-tuned ddgemm is currently rarely available,
shiftedCholeskyQR3 seems to be more practical than mixed precision CholeskyQR.

Finally we brieﬂy mention the performance of shiftedCholeskyQR3 on large-
scale distributed parallel systems. Based on our previous performance evaluation
of CholeskyQR2 on the K computer (for details, see [4]), we give a rough estimation
of the computational time of shiftedCholeskyQR3 in Table 7.5, where we estimate
the computational time of shiftedCholeskyQR3 as 1.5 times that of CholeskyQR2,
simply based on the number of iterations. From this table, shiftedCholeskyQR3 is ex-
pected to be still signiﬁcantly faster than Householder QR methods (both TSQR and
ScaLAPACK routines) in large-scale parallel computation for matrices κ2(X) < u−1.

7.2. Oblique inner product. In the case of the inner product deﬁned by a pos-
itive deﬁnite matrix B, we focus on large-sparse B, as arises commonly in applications.
Owing to the sparsity of B, orthogonalizing X such that X T BX = I depends heavily
on the performance of sparse matrix-vector multiplication (SpMV). Both CGS2 and
shiftedCholeskyQR3 would therefore achieve only a fraction of the machine’s peak
performance compared with the case when B is dense, where performance would be

10020030040050010-1510-1010-5100CGS2MGS2MGSsCholQR310020030040050010-1510-1010-5100CGS2MGS2MGSsCholQR3SHIFTED CHOLESKYQR

33

Table 7.2: Computational time on Laurel 2: κ2(X) = 1011, m = 100, 000 and the
number of threads is 36.

Time (sec.)

Method
sCholQR3 (dgemm ver.)
sCholQR3 (dsyrk ver.)
dgeqrf + dorgqr
dgeqr + dgemqr
CGS2
Block CGS2

n = 32
2.62 × 10−3
2.39 × 10−3
6.31 × 10−2
4.39 × 10−3
9.60 × 10−3
9.62 × 10−3

n = 64
9.16 × 10−3
7.44 × 10−3
8.94 × 10−2
1.30 × 10−2
2.31 × 10−2
2.49 × 10−2

n = 128
3.32 × 10−2
2.45 × 10−2
1.19 × 10−1
4.39 × 10−2
1.40 × 10−1
7.88 × 10−2

n = 256
1.16 × 10−1
8.20 × 10−2
1.96 × 10−1
1.68 × 10−1
1.24
2.05 × 10−1

Table 7.3: Achieved performance of dgemm: m = 100, 000 and the number of threads
is 36.

n
GFLOPS

32
476

64
559

128
539

256
589

512
600

1, 024
624

4, 096
654

16, 384
628

dictated by dgemm. Optimizing the performance of shiftedCholeskyQR3 for a sparse B
involves extracting performance from the computation of the inner product X T BX,
which in turn depends on

• sparse matrix multiple-vector multiplication kernels [8], and
• the choice of whether X T BX is computed by forming Y := BX explicitly

followed by computing X T Y or in blocks.

By blocking we refer to forming the matrix Yj := BX(:, jk : (j + 1)k) for some block
size k ∈ [1, n] and j ∈ (1, n/k) in succession, and subsequently calculating the matrix
X T Yj. The choice of strategy is highly sensitive to both the sparsity structure of B,
and the cache hierarchy of the architecture on which we execute. For an unbounded
cache size, computing Y explicitly will be the fastest strategy since the subsequent
operation Y T X has excellent computational intensity. Realistically however, for a
small L1 cache or indeed a large B, forming Y will cause the entries of X to be
evicted from cache, leading to unnecessary cache misses and a slower performance.
For a more detailed discussion and performance analysis of the diﬀerent strategies,
we refer the reader to [9, sec. 6.4]; in this section we present the overall speed-up
obtained over CGS2 implementations.

Our experiments for a sparse B were carried out on a shared memory system
with Intel Xeon E5-2670 (Sandy Bridge) symmetric multiprocessors. There are 2
processors with 8 cores per processor, each with 2 hyperthreads that share a large
20 MB L3 cache. Each core also has access to 32 KB of L1 cache and 256 KB of
L2 cache and runs at 2.6 GHz. They support the AVX instruction set with 256-
bit wide SIMD registers, resulting in 10.4 GFlop/s of double precision performance
per core or 83.2 GFlop/s per processor and a dgemm performance of 59 GFlop/sec.
The implementation was parallelized using OpenMP, and compiled using Intel C++
Compiler version 14.0 with -O3 optimization level, with autovectorization turned on
for both CPU. Tests are run by scheduling all threads on a single processor with 16
threads with OpenMP ‘compact’ thread aﬃnity, which is set using KMP SET AFFINITY.
We use as test problems symmetric positive deﬁnite matrices from the University
of Florida [1] collection, as listed in Table 7.6. The matrices are stored and operated
upon B using the Compressed Sparse Row (CSR) format and we avoid changing the

34

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

Table 7.4: Comparison of shiftedCholeskyQR3 with Mixed Precision CholeskyQR
based on the performance estimation for ddgemm: m = 100, 000 and n = 64.

shiftedCholeskyQR3

Mixed Precision CholeskyQR

Cray-style
Time (sec.)
–
–
–

IEEE-style
Time (sec.)
–
–
–

sCholQR

CholQR

CholQR

Misc.
Total

–
–
–

dgemm
dpotrf
dtrsm
dgemm
dpotrf
dtrsm
dtrmm
dgemm
dpotrf
dtrsm
dtrmm

Routine Time (sec.) Routine
1.90 × 10−3
3.00 × 10−5
1.11 × 10−3
1.68 × 10−3
2.91 × 10−5
1.31 × 10−3
3.10 × 10−5
1.81 × 10−3
3.79 × 10−5
1.24 × 10−3
2.91 × 10−5
1.36 × 10−4
9.34 × 10−3

ddgemm ≥ 9.36 × 10−3 ≥ 1.46 × 10−2
ddpotrf ≥ 2.91 × 10−5 ≥ 2.91 × 10−5
1.31 × 10−3
dtrsm
–
–
1.81 × 10−3
dgemm
3.79 × 10−5
dpotrf
1.24 × 10−3
dtrsm
2.91 × 10−5
dtrmm
≥ 0

1.31 × 10−3
–
1.81 × 10−3
3.79 × 10−5
1.24 × 10−3
2.91 × 10−5
≥ 0

≥ 1.38 × 10−2 ≥ 1.91 × 10−2

Table 7.5: Estimation of the computational time of shiftedCholeskyQR3 on the K
computer: the number of nodes (= MPI processes) is 16, 384.

m
4,194,304

16,777,216

n
16
64
256
16
64
256

TSQR
1.64 × 10−3
7.42 × 10−3
2.32 × 10−1
1.84 × 10−3
8.82 × 10−3
2.40 × 10−1

Time (sec.)

Measured
pdgeqrf + pdorgqr
1.04 × 10−2
4.14 × 10−2
1.84 × 10−1
1.13 × 10−2
5.65 × 10−2
3.92 × 10−1

CholQR2
8.02 × 10−4
2.52 × 10−3
3.05 × 10−2
9.06 × 10−4
3.13 × 10−3
3.38 × 10−2

Estimated
sCholQR3
1.20 × 10−3
3.79 × 10−3
4.57 × 10−2
1.36 × 10−3
4.70 × 10−3
5.07 × 10−2

format to favour performance although the results in [8] strongly suggest that this
is beneﬁcial. The reason for this is that oblique QR factorization is usually part of
a “larger” program, for example, a sparse generalized eigensolver [9, chap. 4], and
hence the storage format needs may be governed by other operations in the parent
algorithm, for example, a sparse direct solution, and such software may not exist for
the new format. Changing the sparse matrix format to accelerate the factorization
may also slow down other parts of the calling program that are not optimized to work
with a diﬀerent format.

We present the performance of shiftedCholeskyQR3 on CPU by varying n, i.e.,
the size of X and compare the results with those from CGS2 in Figure 7.1. On
the CPU, shiftedCholeskyQR3 was faster than CGS2 by a minimum of 3.7 times for
n = 16 and a maximum of 40 times for n = 256 vectors. The large speedup obtained
is only a reﬂection of the reliance of CGS2 on matrix-vector products, which in the
case of sparse matrices, has a particularly poor CPU utilization.

SHIFTED CHOLESKYQR

35

Table 7.6: Test matrices used for benchmarking chol borth.

Application
ﬁnite diﬀerence
ﬁnite element
model reduction
circuit simulation
ﬁnite element

Name
apache2
bairport
bone010
G3 circuit
Geo 1438
parabolic fem CFD
serena
shipsec8
watercube

ﬁnite element
ﬁnite element
ﬁnite element

Size
715,176
67,537
986,703
1,585,478
1,437,960
525,825
1,391,349
114,919
68,598

Nonzeros
2,766,523
774,378
47,851,783
7,660,826
60,236,322
2,100,225
64,131,971
3,303,553
1,439,940

Nonzeros/row
3.86
11.46
48.50
4.83
41.89
3.99
46.09
28.74
20.99

Fig. 7.1: Performance speed-ups of shiftedCholeskyQR3 over CGS2 for sparse prob-
lems for varying sizes of X.

8. Conclusion and discussion. Our algorithm shiftedCholeskyQR3 combines
speed, stability, and versatility (applicable to B (cid:54)= I). We believe it oﬀers an attractive
alternative in high-performance computing to the conventional Householder-based QR
factorziation algorithms when B = I, and can be the clear algorithm of choice when
B (cid:54)= I.

shiftedCholeskyQR3 as presented could beneﬁt from further tuning, and this work
suggests a few future directions. First, the choice of shift s introduced in this paper
is conservative, and severely so when m, n are large. Our experiments suggest that a
much smaller shift, such as s = O(u(cid:107)A(cid:107)2), is usually suﬃcient to avoid breakdown in
chol(A + sI), and as illustrated in Figure 6.1, a smaller shift results in improved con-
ditioning, and hence smaller number of shiftedCholeskyQR iterations. Introduction
of a shift strategy that is both stable and eﬃcient is an important remaining task.

Our performance results hold promise for the competitiveness of shiftedCholeskyQR3,

and further work will focus on comparing it with other state-of-the-art implementa-
tions for QR factorziations such as TSQR in an HPC setting. In the case where B (cid:54)= I
for a sparse B, the performance beneﬁts over CGS2 are remarkable and our algorithm
is the clear choice for applications.

Finally, for rank-deﬁcient matrices, shiftedCholeskyQR3 is inapplicable, because

36

FUKAYA, KANNAN, NAKATSUKASA, YAMAMOTO AND YANAGISAWA

AR is rank-deﬁcient for any R1. This issue is not present in Householder-type meth-
ods, and a workaround for shiftedCholeskyQR3 is much desired.

Appendix A. A sharper bound on the residual of CholeskyQR2. In [16],
nu(cid:107)X(cid:107)2. In this

the residual of CholeskyQR2 is bounded by (cid:107) ˆZ ˆU − X(cid:107)F ≤ 5n2√
appendix, we derive a sharper bound 5n2u(cid:107)X(cid:107)2.

In the CholeskyQR2 algorithm in ﬂoating-point arithmetic, the QR decomposition

X = ZU is computed as follows.

(A.1)

(A.2)

ˆA = f l(X (cid:62)X),
ˆC = f l( ˆY (cid:62) ˆY ),

ˆR = f l(chol( ˆA)),
ˆS = f l(chol( ˆC)),

ˆY = f l(X ˆR−1),
ˆZ = f l( ˆY ˆS−1),

ˆU = f l( ˆS ˆR).

Let us denote the residuals in the ﬁrst and the second step by ∆X and ∆Y , respec-
tively, and the forward error in the computation of ˆU by ∆U . Then,

(A.3)
(A.4)

(A.5)

X + ∆X = ˆY ˆR,
ˆY + ∆ ˆY = ˆZ ˆS,

ˆU = ˆS ˆR + ∆U.

Using these quantities, the residual of CholeskyQR2 can be evaluated as
(cid:107) ˆZ ˆU − X(cid:107)F = (cid:107) ˆZ( ˆS ˆR + ∆U ) − ˆY ˆR + ∆X(cid:107)F

(A.6)

= (cid:107)∆ ˆY ˆR + ˆZ∆U + ∆X(cid:107)F
≤ (cid:107)∆ ˆY (cid:107)F (cid:107) ˆR(cid:107)2 + (cid:107) ˆZ(cid:107)2(cid:107)∆U (cid:107)F + (cid:107)∆X(cid:107)F .

In [16] the residual was bounded row-wise, then summed to bound (cid:107) ˆZ ˆU − X(cid:107)F .
n by directly bounding the
The analysis below improves the bound by a factor
matrix norm and using the reﬁned analysis employed in Section 3. Now we bound
each term in (A.6). In [16], (cid:107) ˆR(cid:107)2 and (cid:107)∆U (cid:107)F (which is denoted as (cid:107)E5(cid:107)F in [16]) are
evaluated as

√

(A.7)

(cid:107) ˆR(cid:107)2 ≤ 1.1(cid:107)X(cid:107)2,
(cid:107)∆U (cid:107)F ≤ 1.2n2u(cid:107)X(cid:107)2.
(A.8)
(cid:107) ˆZ(cid:107)2 can be bounded as in Eq. (4.22) of this paper. To bound (cid:107)∆X(cid:107)F , we recall that
the ith row of ˆY , which we denote by ˆy(cid:62)
i , is computed from the ith row of X, which
we denote by x(cid:62)

i , by triangular solution and therefore it holds that

i = x(cid:62)
ˆy(cid:62)

i ( ˆR + ∆ ˆRi)−1

(A.9)
where ∆ ˆRi is the backward error of the triangular solution. According to [16], (cid:107)∆ ˆRi(cid:107)2
is bounded as

(i = 1, 2, . . . , m),

(A.10)

(cid:107)∆ ˆRi(cid:107)2 ≤ 1.2n

√

nu(cid:107)X(cid:107)2.

i and noting the relationship ∆x(cid:62)

i =

Hence, by denoting the ith row of ∆X by ∆x(cid:62)
−ˆy(cid:62)
(A.11)

i ∆ ˆRi, we obtain

(cid:107)∆X(cid:107)F =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

i=1

(cid:107)ˆy(cid:62)

i ∆ ˆRi(cid:107)2 =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

i=1

(cid:107)ˆy(cid:62)

i (cid:107)2 max
1≤i≤m

(cid:107)∆ ˆRi(cid:107)2 ≤ (cid:107) ˆY (cid:107)F · 1.2n

√

nu(cid:107)X(cid:107)2.

1However, roundoﬀ errors often map the zero singular values to O(u), and so a few iterations of

shiftedCholeskyQR usually result in κ2(Q) ≤ u−1/2.

SHIFTED CHOLESKYQR

37

Since the singular values of ˆY are bounded by
[16]), (cid:107) ˆY (cid:107)F ≤

n(cid:107) ˆY (cid:107)2 ≤

√

√

√

69
8

n. Inserting this into (A.11), we have

√

69
8

(see the proof of Corollary 3.2 in

(cid:107)∆X(cid:107)F ≤ 1.3n2u(cid:107)X(cid:107)2.
(A.12)
A bound on (cid:107)∆Y (cid:107)F can be obtained by replacing X and ˆY in (A.11) with ˆY and ˆZ, re-
n(cid:112)1 + 6(mnu + n(n + 1)u) ≤
spectively, and noting that (cid:107) ˆY (cid:107)2 ≤
8 and (cid:107) ˆZ(cid:107)F ≤
1.1

n (see Theorem 3.3 in [16]). The result is

√

√

69

√

(A.13)

(cid:107)∆ ˆY (cid:107)F ≤ 1.4n2u.

Putting all these together and inserting into (A.6), we ﬁnally have

(cid:107) ˆZ ˆU − X(cid:107)F ≤ 1.4n2u · 1.1(cid:107)X(cid:107)2 +

(A.14)

≤ 5n2u(cid:107)X(cid:107)2.

√

76
8

· 1.2n2u(cid:107)X(cid:107)2 + 1.3n2u(cid:107)X(cid:107)2

REFERENCES

[1] T. A. Davis and Y. Hu, The University of Florida Sparse Matrix Collection, ACM Trans.
Math. Softw., 38 (2011), pp. 1:1–1:25, https://doi.org/10.1145/2049662.2049663, http://
doi.acm.org/10.1145/2049662.2049663.

[2] J. Demmel, On ﬂoating point errors in Cholesky, Tech. Report 14, LAPACK Working Note,

1989.

[3] J. Demmel, L. Grigori, and M. Hoemmen, Implementing communication-optimal parallel

and sequential QR factorizations, arXiv:0809.2407, (2008).

[4] T. Fukaya, Y. Nakatsukasa, Y. Yanagisawa, and Y. Yamamoto, CholeskyQR2: a simple
and communication-avoiding algorithm for computing a tall-skinny QR factorization on
a large-scale parallel system, in Proceedings of the 5th Workshop on Latest Advances in
Scalable Algorithms for Large-Scale Systems, IEEE Press, 2014, pp. 31–38.

[5] L. Giraud, J. Langou, M. Rozloˇzn´ık, and J. Eshof, Rounding error analysis of the classical
gram-schmidt orthogonalization process, Numer. Math., 101 (2005), pp. 87–100.
[6] G. H. Golub, V. Loan, and C. F., Matrix Computations, The Johns Hopkins University

Press, 4th ed., 2013.

[7] N. J. Higham, Accuracy and Stability of Numerical Algorithms, SIAM, Philadelphia, PA, USA,

second ed., 2002.

[8] R. Kannan, Eﬃcient sparse matrix multiple-vector multiplication using a bitmapped format,
in 20th IEEE International Conference on High Performance Computing (HiPC’13), De-
cember 2013, pp. 286–294, https://doi.org/10.1109/HiPC.2013.6799135.

[9] R. Kannan, Numerical Linear Algebra problems in Structural Analysis, PhD thesis, School of

Mathematics, The University of Manchester, 2014.

[10] B. R. Lowery and J. Langou, Stability analysis of QR factorization in an oblique inner

product, arXiv:1401.5171, (2014).

[11] Multiprecision Computing Toolbox. Advanpix, Tokyo. http://www.advanpix.com.
[12] M. Rozloˇzn´ık, M. T˘uma, A. Smoktunowicz, and J. Kopal, Numerical stability of orthogo-
nalization methods with a non-standard inner product, BIT, (2012), pp. 1–24.

[13] S. M. Rump and T. Ogita, Super-fast validated solution of linear systems, J. Comput. Appl.

Math., 199 (2007), pp. 199–206.

[14] A. Stathopoulos and K. Wu, A block orthogonalization procedure with constant synchroniza-

tion requirements, SIAM J. Sci. Comp, 23 (2002), pp. 2165–2182.

[15] L. N. Trefethen and D. Bau, Numerical Linear Algebra, SIAM, Philadelphia, 1997.
[16] Y. Yamamoto, Y. Nakatsukasa, Y. Yanagisawa, and T. Fukaya, Roundoﬀ error analysis

of the CholeskyQR2 algorihm, Electron. Trans. Numer. Anal, 44 (2015), pp. 306–326.

[17] Y. Yamamoto, Y. Nakatsukasa, Y. Yanagisawa, and T. Fukaya, Roundoﬀ error analysis of
the CholeskyQR2 algorithm in an oblique inner product, JSIAM Letters, 8 (2016), pp. 5–8.
[18] I. Yamazaki, S. Tomov, and J. Dongarra, Mixed-precision Cholesky QR factorization and
its case studies on Multicore CPU with Multiple GPUs, SIAM J. Sci. Comp, 37 (2015),
pp. C307–C330.

[19] Y. Yanagisawa, T. Ogita, and S. Oishi, A modiﬁed algorithm for accurate inverse Cholesky
factorization, Nonlinear Theory and Its Applications, IEICE, 5 (2014), pp. 35–46.

