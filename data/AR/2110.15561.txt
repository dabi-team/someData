Exposing Deepfake with Pixel-wise Autoregressive
and PPG Correlation from Faint Signals

Maoyu Mao∗, Jun Yang∗,
∗Dept. of Electronic and Information Engineering, Tongji University, Shanghai, China. Email: junyang@tongji.edu.cn

1
2
0
2

t
c
O
9
2

]

V
C
.
s
c
[

1
v
1
6
5
5
1
.
0
1
1
2
:
v
i
X
r
a

Abstract—Deepfake poses a serious threat to the reliability of
judicial evidence and intellectual property protection. In spite of
an urgent need for Deepfake identiﬁcation, existing pixel-level
detection methods are increasingly unable to resist the growing
realism of fake videos and lack generalization. In this paper,
we propose a scheme to expose Deepfake through faint signals
hidden in face videos. This scheme extracts two types of minute
information hidden between face pixels – photoplethysmography
(PPG) features and auto-regressive (AR) features, which are used
as the basis for forensics in the temporal and spatial domains,
respectively. According to the principle of PPG, tracking the
absorption of light by blood cells allows remote estimation
of the temporal domains heart rate (HR) of face video, and
irregular HR ﬂuctuations can be seen as traces of tampering.
On the other hand, AR coefﬁcients are able to reﬂect the inter-
pixel correlation, and can also reﬂect the traces of smoothing
caused by up-sampling in the process of generating fake faces.
Furthermore,
the scheme combines asymmetric convolution
block (ACBlock)-based improved densely connected networks
(DenseNets) to achieve face video authenticity forensics. Its
asymmetric convolutional structure enhances the robustness of
network to the input feature image upside-down and left-right
ﬂipping, so that the sequence of feature stitching does not affect
detection results. Simulation results show that our proposed
scheme provides more accurate authenticity detection results
on multiple deep forgery datasets and has better generalization
compared to the benchmark strategy.

Index Terms—Deepfake detection, photoplethysmography,
auto-regressive, densely connected networks, asymmetric con-
volution networks.

I. INTRODUCTION

Fake face video generated by advanced computer vision
technology and deep learning technology poses a serious
threat to the reliability of judicial evidence and intellectual
property protection. Therefore, it is imperative to develop a
universal Deepfake detection algorithm. Methods of Deepfake
detection are divided into fake image detection and fake
video detection [1]. In 2018, Pavel K et al. ﬁrst conducted a
fairly comprehensive evaluation of facial recognition methods
for detecting Deepfake videos [2]. They observed that, even
though Visual Geometry Group (VGG) and FaceNet neural
networks, the state of the art image classiﬁcation models, are
helpless for Deepfake video recognition. The compression of
video results in the degradation of frame data, and the timing
characteristics between frames may change, so image detec-
tion algorithms cannot be directly used for video detection.
in order to resist an endless stream of
Deepfake videos, researchers begin to develop datasets and

Until recently,

Corresponding author is Jun Yang. Email: junyang@tongji.edu.cn

countermeasure for such problems. In view of temporal
structure of fake videos, the detection strategies for forgery
videos can be divided into methods using temporal features
across frames and visual artifacts within frame. Methods
based on artifacts within frame usually combines image frame
features combined with deep or shallow classiﬁers to achieve
detection. A compact facial video forgery detection network
named Mesonet proposed in [3] utilizes deep neural network
with a low number of layers to focus on the mesoscopic
properties of images, as noise analysis cannot be used in
compressed video detection with strong noise reduction while
it is not easy to distinguish the authenticity of human face
at semantic level. Five different detection systems evaluated
thoroughly in [4] reveals that a linear stack of depthwise
separable convolution layers with residual connections named
XceptionNet providing the best results in both DeepFakes and
FaceSwap manipulation methods. The authors also focus on
Deepfakes, Face2Face and FaceSwap as outstanding of facial
manipulations to build a general large-scale database. Addi-
tionally, background and foreground images containing fake
faces are also statistically inconsistent during fusion, which is
caused by inconsistent camera parameters, inconsistent lens
illumination, compression in the generation algorithm and
so on. When the synthetic face area is synthesized into the
original image to generate Deepfake image, artifacts will be
generated at the splicing place. Landmarks extracted in [5]
display the mixed boundary information of the Deepfake im-
ages and describe the syncretic mask for showing face x-ray.
Further, they adopt advanced high-resoultion nets (HRNets)
to achieve two-class detection. Instead of making blind as-
sumptions around speciﬁc facial manipulation algorithm and
content, the methods relying on prior hidden information of
videos can be widely effective. The basis of temporal features
across frames methods is that the reconstruction process of
Deepfake videos mostly relies on manipulation of frame-by-
frame, therefore the temporal features between video frames
cannot be thoroughly preserved. Intuitively,
this type of
methods can adopt deep recurrent neural networks (RNNs)
for end-to-end detection [6], [7], while there is no strong
theoretical support for directly inputting the original fake face
video into network and pure pixel-level methods cannot resist
the increasingly realistic fake videos. On the other hands,
extracting temporal features by using the inconsistency of
biological attributes on face provides a good solution. Errors
caused by inconsistent head poses utilized in [8] combines
with shallow classiﬁer support vector machine (SVM) to

 
 
 
 
 
 
achieve convenient classiﬁcation. The abnormal blinking state
found in [9] is modeled in combination with long-term
recurrent convolutional neural networks (LRCNs) model to
achieve Deepfake detection. Fidelity of lip movement in face
videos evaluated in [10] combines sound synchronization
analysis to judge the authenticity of video.

Fake face videos realized by deep algorithms such as GAN-
based entire face synthesis or face indentify swap can learn
pixel-based features to complete visual deception, but it can-
not imitate the change rules of subtle signal, such as biological
signal contained in all organisms. Non-electrical biological
signals, including heartbeat, pulse, etc., ﬂuctuate regularly,
and can be used as ﬁngerprints for authenticity detection as
they will not be completely retained in the spatio-temporal
domain of fake videos. As early as 2008, W. Verkruysse ﬁrst
proposes the technology that PPG signal related to heart rate
can be extracted from face videos collected from camera
so as to realize remote heart rate monitoring [11]. This
technology is named remote PPG (rPPG). Blood volume pulse
from the facial regions extracted in [12] applies blind source
separation (BBS) algorithm used in RGB color space and is
subsequently utilized to quantify HR, respiratory rate, and
HR variability. Signiﬁcant motion renders vulnerability of
previous algorithm. Difference in intensity of PPG signals
in RGB color channel found in [11] is used to calculate
the linear combination and ratio of chrominance signals,
and ﬁnally obtains chrominance-based rPPG, with root mean
square error (RMSE) and standard deviation both a factor of
2 better than BSS-based algorithms. AR modelling and pole
cancellation utilized in [19] cancels out aliased frequency
components caused by artiﬁcial light ﬂicker and constucts
accurate HR spectrum from AR model. The rPPG technology
is widely concerned and applied in the ﬁelds of medical
imaging research [13], emotion recognition [14] and face anti-
spooﬁng [15]. Lately, experimental veriﬁcation in [16] shows
that spatial coherence and temporal consistency of biological
signals can be used to achieve Deepfake detection, with the
video accuracy up to 94.65%, which is the ﬁrst time HR
signals is mentioned in the ﬁeld of deep forgery. On the basis
of this conﬁrmation, this team further proposes that spatio-
temporal patterns of biological signals can be considered as
representative projections of residuals [17]. This novel scheme
can realize deep forgery source detection according to the
characteristics of generative models.

In this paper, we investigate the modeling of biological
signal and its AR coefﬁcient as implicit prior ﬁngerprints
and propose a deep forgery forensics scheme with the help of
weak biological signals to address known or unknown facial
manipulation detection. Since HR-related signal cannot be
accurately reproduced by Deepfake generation network, and
biological signals hidden in portrait video have continuous,
stable and complex ﬂuctuations, its spatio-temporal features
can be extracted and applied to neural network to detect
falseness in face videos. More speciﬁcally, in this scheme,
the cheek part of the face image frame is selected as the ROI
and is divided into sub-regions of same size. The PPG signal

and its AR coefﬁcient of the corresponding area of portrait
videos are extracted one by one and modeled as two class of
intuitive maps. We build two DenseNets that accept different
feature representations (PPG map and AR map) of input and
demonstrate the advantages of integrating our model detec-
tion. Simulation results show that the proposed deep forgery
forensics model fusion can improve the detection accuracy,
reduce the detection delay and enhance wide universality.

The main contributions of this paper can be summarized

as:

• We explore novel remote biological signal and its vari-

ants as priori ﬁngerprints for Deepfake forensics.

• We formulate a bio-signal-based deep forgery forensics
scheme to detect Deepfake face in portrait videos. Two
types of maps are obtained by modeling the PPG signal
and its AR coefﬁcients, and then combined with deep
neural networks to achieve two-class detection.

• We utilize feature fusion technology to input the two
types of feature representations into the network model
and evaluate the performance. The proposed scheme
improves the detection accuracy, reduces the detection
delay and enhances wide universality.

The rest of this paper is structured as follows. We review
photoelectric plethysmography and AR coefﬁcients in Section
II and describe the feature map of proposed Deepfake forensic
scheme in Section III. We propose a PPG-based Deepfake
forensic scheme with fusion model IV and provide simulation
results in Section V. Conclusion is drawn in Section VI.

II. RELATED WORK

Despite the CNNs models are capable of generating fake
images, these generated synthetic images are still detectable.
What has greatly contributed to the popularity of Deepfake
detection methods is the fact that the retained ﬁngerprints
generated by the CNNs model can be distinguished from real
counterparts [18]. Creating images and videos are available
today as it only rely on several real images or short videos
as references. The Deepfake detection methods include fake
iamge detection and fake video detection. Characteristics in
videos are varied among different frames. Based on a obvious
observation that the feature distribution of the real faces is
crowded while that of the fake ones is uncrowded among
domains but crowded in single domain, Jia et al [19]. develops
a single-side domain generalization framework for judging the
feature space for real and fake faces. In particular, the gener-
alization of the model is available by using the normalization
function of feature and weight. Durall et al. [20] claim that
one common limitation of the ordinary up-sampling models,
e.g. well-known GAN models, is the incapability of recon-
structing the same distribution of spectrum like that in true
data. Therefore, they formulate a special spectrum regularizer
for optimization both the fake images quality and training
stability. Liu et al. [21] believe that the texture information
between fake and genuine faces is virtually different and there
is a greater difference in global texture information between
real faces and real ones. Inspired by this view, the Gram-Net

they proposed is shown to outperform the existing methods.
Dang et al. [22] manifest that the attention module they
proposed achieves good performance not only in learning the
feature maps but also in distinguishing the genuine and fake
faces. Meanwhile, the manipulated regions can be visualized.
From a statistical point of view, due to the common systematic
drawbacks on CNN-generated images, only utilize a single
well-trained CNN, fake images generated by eleven different
architectures can be picked out easily [18].

However, despite many potentials, these image detection
methods are not suitable for detecting videos. Sabir et al.
[7] observe that the temporal coherence within the synthetic
videos are weak, which can be calculated between two
pictures before and after in a same video. The inconsistencies
both on intra-frame and temporality has become an essential
basis for judging true and false videos [23]. As far as the
method in general goes, the carefully conﬁgured combination
of CNNs and long short term memory (LSTM) are widely
used for detecting the spatial and temporal features, respec-
tively [6], [24], [25].

The amount of light absorbed by blood varies naturally due
to the ﬂow of blood in the arteries, and vice versa. Therefore,
we use the rPPG to detect such signals to identify genuine
and fake faces, according to our method. Another observation
is that even the generated Deepfake images are used by
the ”state-of-the-art” models, like GAN, the blurring process
during upsampling still leaves traces in the images [26]. Apart
from this, the research on Deepfake detection using smooth-
ing operations during up-sampling is still lacking. Kang et
al. [27] propose to use an AR model after converting the
image to a one-dimensional signal and achieved robustness
in image median ﬁltering detection. Subsequently, inspired
by Kang’s work, Yang et al. [28] extend the AR model to
two-dimensional space, directly enhancing its applicability
in two-dimensional signal processing such as images. It is
natural, to give us an inspiration that we can tell the difference
between fake and true data from the correlation between
pixels. Therefore, in this paper, AR is used to calculate the
continuity between two adjacent pixels. When the correlation
between pixels is low, it is very likely that the picture has
been tampered with.

III. SYSTEM MODEL

The system model of the proposed Deepfake detection
scheme is shown in Fig. 1. First, the face video is divided
into multiple face image frame segments. The cheek region
is selected as the ROI to generate PPG and AR ﬁngerprint
images. Two types of ﬁngerprints are put into the DenseNet
modiﬁed by ACBlock for training. Finally, model fusion is
performed to achieve segment-level and video-level authen-
ticity detection.

A. Biological Signal-PPG signal for HR

Photoplethysmography is an optical technique that mon-
itors various vital signs, such as heart rate, which exploits
photoelectric sensors to detect and record the light absorption

TABLE. I. SUMMARY OF SYMBOLS AND NOTATIONS

Symbols
I(x, y, t)
s
ρDC
ρAC
Vi(x, y, t)
i
Xs(t), Ys(t)
C(t)
L(t)
p
ϕi
c
ǫi
L(x, y)
C(n)
ROI
φ
n

Notations
Illumination intensity
Specular reﬂection
Direct current part
Alternating current part
Intensity of reﬂected light
Color channel
The orthogonal chrominance signal
Chrominance-PPG signal
AR model
Order of AR
The autocorrelation coefﬁcient
Constant
White noise
Pixel coordinate
PPG signal
Region of interest
AR coefﬁcient matrix
Frame with a ﬁxed length

or reﬂection intensity variations of the human skin due to
the blood volume variations during the cardiac cycle, thereby
achieve the remote extraction of biological signals.

Extracting frame sequence of video t, t = 1, 2, 3... recorded
by the light sensitive sensor in the camera, the intensity of
the reﬂected light from the skin surface recorded by the video
can be expressed as

V (x, y, t) = I(x, y, t)R ,

(1)

where I(x, y, t) is the illumination intensity of the light source
at the pixel coordinates (x, y) of the video frames and R is the
reﬂectance of the skin surface. Light produces specular and
diffuse reﬂections on the facial skin. Therefore, the reﬂectance
R can be further decomposed as

R = s + ρ,

(2)

where s and ρ denote specular reﬂection and diffuse reﬂec-
tion, respectively. Analyzing the interaction of the structural
level of the skin with light, the diffuse reﬂection ρ is divided
into a direct current part ρDC and an alternating current part
ρAC , i.e.

ρ(t) = ρDC + ρAC .

(3)

For the selected region of interest (ROI), the intensity of re-
ﬂected light Vi(x, y, t) for each color channel i ∈ {R, G, B}
can be modeled as

Vi(x, y, t) = Ii(x, y, t)(s + ρiDC + ρiAC ) ,

(4)

where s is is identical for all the color channels, ρiDC is the
stationary part of the reﬂection coefﬁcient of the skin in the
color channel i, while ρiAC is the zero-mean time-varying
physiological waveform attributed to cardiac synchronous
changes in the blood volume with heart rate.

To reduce motion artifacts and other noise effects parallel
to the imaging plane, a group of pixels M × N in ROI is

(cid:215)(cid:215)(cid:215)

(cid:215)(cid:215)(cid:215)

PPG fingerprints

Long video

Face video clips

(cid:215)(cid:215)(cid:215)

Original

Deepfake

Original

Deepfake

Fake

Original

AR fingerprints

ACBlock-DenseNet

Probability classifier

Fig. 1. System model of proposed Deepfake detection scheme.

selected for averaging pooling. Then the chrominance signal
of each color channel is

Ci(t) = P

N −1
y=0 P

M−1
x=0 Vi(x, y, t)
M × N

,

i ∈ {R, G, B} .

(5)

In a white-light illumination environment, due to differ-
ences in the mapping of skin tones of different individuals,
we correct the chrominance signal Ci(t) with the help of
normalization of the differences as

Cin (t) =

Ci(t)
σ

,

(6)

where σ is deﬁned as pCR(t)2 + CG(t)2 + CB(t)2. In a
white lighting environment, the standard skin colour vector
σ is noted as [0.7682, 0.5121, 0.3841]. Under white light
illumination, the angles between different skin colors and the
standard skin color vectors are approximately equally dis-
tributed in colour space. Therefore, the corrected chrominance
signal can be expressed as

[CRn (t), CGn (t), CBn (t)]

=[0.7682CR(t), 0.5121CG(t), 0.3841CB(t)].

(7)

Two orthogonal chrominance signals are utilized to elimi-

nate specular reﬂection component in (4), i.e.,

Xs(t) =

Ys(t) =

= 3CR(t) − 2CG(t)

CRn (t) − CGn(t)
0.7682 − 0.5121
0.5CRn (t) + 0.5CGn(t) − CBn (t)
0.7682 + 0.5121 − 0.7682
= 1.5CR(t) + CG(t) − 1.5CB(t) .

(8)

When the skin surface moves with respect to the light
the illumination intensity in (4) will change and
source,
affect
the chrominance intensity. However, such intensity
modulations are equal for all channels, hence the inﬂuence
of this motion can be eliminated by calculating the ratio of

two ﬁltered orthogonal signals. This ratio can be represented
as

S(t) =

− 1 .

(9)

Xs(t)
Ys(t)

According to Taylor expansion, the approximate ratio ˆS(t) is

ˆS(t) ≈ Xs(t) − Ys(t)

= 1.5CRf (t) − 3CGf (t) + 1.5CBf (t) ,

(10)

where Cif (t), i ∈ {R, G, B} is the bandpassed ﬁltered
version of Ci(t).

Finally, the difference between the two frames of signals is
used to reduce the inﬂuence of pigment absorption on diffuse
reﬂection, and a PPG signal based on chromaticity changes
is obtained as

C(t) = ˆS(t + 1) − ˆS(t) .

(11)

C(t) is the chrominance-PPG (C-PPG) signal containing
blood volume variation information. If needed, based on the
multiple signal classiﬁcation (MUSIC) algorithm, an estimate
value of heart rate is obtained by calculating the pseudo-
spectral peak in signal subspace.

B. Autoregressive Model

For a stationary non-white noise sequence, a linear model is
usually established to ﬁt the trend of the sequence in statistics,
and the useful information in the sequence is extracted by
this. The autoregressive(AR) model is a representation of
a type of random process, which speciﬁes that the output
variable depends linearly on its own previous values and on
a stochastic term, i.e.,

Lt = c +

p

X
i=1

ϕiLt−i + ǫi .

(12)

Face recognition

Selet ROI

N

N

(cid:182)

B

G

R

N

Aligned facial skin 
sub-area in RGB 
space

Calculate PPG signal  

PPG fingerprint of 
single channel

PPG fingerprint

(a) Process of generating PPG ﬁngerprints.

(cid:182)

N

(cid:182)

B

G

R

N

Face recognition

Selet ROI

Connect the pixels 
in a(cid:254)Z(cid:255)shape

AR coefficient of 
order (cid:182)

AR fingerprint of 
single channel

AR fingerprint

(b) Process of generating AR ﬁngerprints.

Fig. 2. Process of generating ﬁngerprints. With φ = 36 and n = 128, ﬁngerprints of size 36 × 128 are obtained.

where p is the order of AR model, ϕi is the autocorrelation
coefﬁcient, c is a constant, and ǫi is white noise.

AR model can be used as a linear prediction model, which
predicts the current sample based on evaluating the corre-
lation between current sample and previous sample. In the
established AR model, the calculated correlation coefﬁcient
is used to evaluate the correlation between current sample
and previous sample. Based on camera imaging principles,
we assume that pixel points are generated one by one. Pixels
in real facial image have a strong correlation, which can be
modeled as a stable AR processing model

or

L(x, y) = c +

L(x, y) = c +

p

X
i=1

p

X
i=1

ϕiL(x − i, y) + ǫi ,

(13)

ϕiL(x, y − i) + ǫi ,

(14)

where L(x, y) is the pixel coordinate of facial images.

In the established model, the calculated correlation coef-
ﬁcient is used to evaluate the correlation between current
sample and previous sample.

C. Fingerprints from PPG and AR

Time and space information hidden in the fake face video
will change. PPG is good at detecting changes in time domain,
and AR has the ability to detect the relationship in spatial
domain. Therefore, this article innovatively combines the two
signals to obtain fusion ﬁngerprint information based on the
time-spatial domain.

Pure PPG signals have steady, periodic ﬂuctuations in the
heart rate range. Fake face video generated on a frame-by-
frame basis disrupts the quasi-periodicity of the PPG signal,
resulting in a jump in its instantaneous phase. Therefore,
the abnormal changes in the PPG signal can be seen as

ﬁngerprints of a Deepfake face. The process of generating
a PPG ﬁngerprint is shown in Fig. 2 (a). In particular:

1) Extract a collection of face images from a video of frame

length t using face detector.

2) Select the pixels enclosed by four pixel coordinates of
the cheek region as the region of interest (ROI) Ro for feature
extraction. The capillaries in the cheek area are abundant and
generally not obscured by hair, accessories, etc.

3) Construct point-line relationship by Triangulation
rectangle Rn by means
each frame of Rn
non-overlapping sub-

and stretch Ro into a regular
afﬁne
of
uniformly
regions:Rn(1), Rn(2), . . . , Rn(i), . . . , Rn(36).

transformation. Divide
area
into

equal

36

4) Align facial skin ROI Rn(i) in color space RGB.
Separate three dimensional chromaticity signals of RGB for
each subregion. According to Equation (11), calculate the
PPG signal C(n) of each sub-region over a range of frames
of ﬁxed length n(n < t), and normalize it to the range of
[0 − 255]. Replace B channel that contains the least obvious
HR information with the PPG signal C(n).

5) Calculate PPG signal value of length n from each sub-
region. Construct a matrix with 36 rows and n columns of
signal values of length n in 36 sub-regions. In the same way,
the R and G channels are reconstructed to obtain three 36 × n
grayscale images. Finally, merge them into a color image as
a PPG ﬁngerprint of a video clip.

Fig. 3 shows sample facial images generated by original
and each deepfake method (top), and an example of an
original PPG ﬁngerprint and a deep forged PPG ﬁngerprint
generated from the same window (middle).

Each row or column of pixels on the picture has a certain
correlation, so it can be modeled as an AR model. The order
of the AR can be used as a one-dimensional AR signal to
describe the relevant information between pixel points. The
process of generating fake face images, such as GAN-based

Original

Deepfake

Face2Face

FaceSwap

NeuralTextures

Face image

PPG fingerprint

AR fingerprint

Fig. 3. Examples of face images and their PPG ﬁngerprints, AR ﬁngerprints. Real faces and faces generated by Deepfake,
Face2Face, FaceSwap, NeuralTextures models (top) are extracted to PPG ﬁngerprints (middle) and AR ﬁngerprints (bottom)
with our scheme.

face generation, will include changes in frequency features
caused by smoothing operations such as up-sampling, and the
correlation between pixel points also changes. The process
of generating an AR ﬁngerprint is shown in Fig. 2 (b). In
particular:

1) Extract a collection of face images from a video of frame

length t using face detector.

2) Select the pixels enclosed by four pixel coordinates of
the cheek region as the region of interest (ROI) Ro for feature
extraction. The capillaries in the cheek area are abundant and
generally not obscured by hair, accessories, etc.

3) Construct point-line relationship by Triangulation and
stretch Ro into a regular rectangle Rn by means of afﬁne
transformation.

4) Traverse the pixel values of rows 1, 3, 5, . . . , 2n − 1 in
the order from left to right, and traverse the pixel values of
rows 2, 4, . . . , 2n in the order from right to left when building
an AR model. They are connected row by row. According
to camera imaging principles, pixel dots are generated one
by one in ⇄ sequence, i.e. the pixels at the end of an odd
numbered row are more correlated with the pixels at the end
of the next even numbered row. Same thing with column
traversal. Finally, the grey values in both directions will be
joined to give a one-dimensional sequence of pixel values.

5) Set the order of AR to the number of blocks in PPG sub-
region, i.e. 36 orders. Calculate the one-dimensional model
coefﬁcient matrix φ of the AR model, according to Equation
(14).

6) Calculate the AR coefﬁcient matrix φ of each frame
within the range of the number of frames with a ﬁxed length
of n(n < t), and construct a 36 × n gray-scale image.
Similarly, three 36×n grey-scale images based on the RGB
colour space are obtained.Finally, merge the three into a color
image as an AR ﬁngerprint of a video clip.

Fig. 3 shows an example of an original AR ﬁngerprint and a
deep forged AR ﬁngerprint generated from the same window
(bottom).

D. Authenticity discrimination with ACNet

Instead of the traditional neural network classiﬁer, a novel
convolutional layer structure, derived from ACNet, is utilized
on top of DenseNet to identify deep face forgery. In general
networks, 3 × 3 convolution kernels are widely used. ACNet
splits the 3 × 3 convolution into asymmetric convolution in
order to improve the efﬁciency and robustness of feature
extraction.

In training phase, we replace each 3 × 3 convolutional
kernel with an AC block (ACB) containing 3 × 3, 1 × 3
and 3 × 1 convolutional kernels, perform the convolutional
operations separately and summed up their ﬁnal results. Once
the convolutional kernels have been fused, the same 3 × 3
structure is used as the normal convolutional kernels in the
testing phase, without adding additional computation.

The asymmetric convolution structure improves the robust-
ness of the network to the up-and-down and left-right ﬂipping
of the input feature images, which has the advantage that the
detection results are not affected regardless of the order in
which our input ﬁngerprint images are stitched together.

An overview of the network architecture for Deepfake
detection is shown in Fig. 1. The structure of DenseNet121
model
includes 1 large scale convolution layer ﬁrstly, 1
pooling layers, 4 Dense blocks, 3 transition layers, 1 full
connection layer, input layer and output layer. Replacing the
3 × 3 convolution kernels in the network with the structure of
the ACB enhances the robustness of image inversion up and
down and left and right, thereby eliminating the effect of the
different stitching order of PPG or AR features.

IV. SIMULATION RESULTS

The system is implemented in python, using the dlib library
for face detection, OpenCV for image processing and Keras
for neural network implementation. Most of the networks are
trained and tested on 4 Tesla V100 GPUs, with short training
times. The most computationally intensive part of the system
is extracting PPG and AR ﬁngerprints from large datasets,
requiring two processes per video.

FaceForensics++ is a facial forgery dataset that enables re-
searchers to train deep learning-based methods in a supervised

TABLE. II. Qualitative comparison: authenticity detection accuracy (%) of different depth forgery detection schemes.

Manipulations

Xception MesoNet

[16]

Ours

FaceForensics++

Deepfakes
Face2Face
FaceSwap
NeuralTextures
All

FaceForensics

video acc.
94.28
91.56
93.70
82.11
-
87.81

video acc.
89.52
84.44
83.56
75.74
-
82.13

segment acc.
90.76
93.12
94.22
-
91.62
88.97

video acc.
94.87
96
95.75
-
94.65
90.66

segment acc.
93.71
87.26
95.45
77.66
92.93
91.49

video acc.
96.44
90.50
95.95
82.97
96.13
94.01

TABLE. III. Ablation experiments: detection accuracy (%) of
original RGB signal with ACNet, removing AR ﬁngerprint
(i.e., PPG ﬁngerprint with ACNet), removing PPG ﬁngerprint,
removing ACNet (i.e., AR ﬁngerprint and PPG ﬁngerprint
with CNN), and our proposed scheme.

Manipulations

RGB+ACNet

-AR

-PPG -ACNet Ours

Deepfakes
Face2Face
FaceSwap
NeuralTextures
All

85.32
76.61
80.01
72.57
78.75

96.01
88.93
92.29
90.04
93.38

93.20
82.17
94.58
82.26
93.65

92.50
89.40
92.70
79.59
93.15

96.44
90.50
95.95
82.97
96.13

Fig. 4. ROC curves and AUC values with different AR orders.

manner, which includes 4000 fake face videos from 1000 real
vdieos, separately generated by four submethods: DeepFake
(DF), Face2Face (F2F), FaceSwap (FS) and NeuralTextures
(NT). Celeb-DF dataset contains 590 real and 5639 DeepFake
synthesized videos with subjects of different ages, ethic
groups and genders.

Simulations are performed to evaluate the performance of
our methods with ﬁxed frame length n = 128, each ﬁngerprint
image is 128 in length and 36 in width. In the simulations,
70% of the original ﬁngerprints ﬁgures and Deepfake-based
ﬁgnerprints ﬁgures are randomly chosen to train the network,
and 30% are randomly selected to be the testing set. In order
to calculate the video detection accuracy, when randomly

selecting, it is necessary to ensure that the PPG and AR
ﬁngerprint images extracted from the same video are either
all in training set or all in test set.

A. Order analysis of AR

ROC (receiver operating characteristic) curves remain cred-
ibly evaluated on the unbalanced data set because they are not
affected by the proportion of the category distribution. On the
full dataset of FaceForensics++, ROC curves with AR orders
of 1, 5, 10, 20, 30, and 36 are shown in Fig. 4. We can see that
the AUC rises until the 20th order, after which the increase in
order no longer plays a role in the AUC (area under curve) of
the model. Even when the order goes to 30 or 36, the AUC
is stable and saturated at around 0.98. For example, when the
order is 5, the AUC is only 0.8841; when the order is 20,
the AUC reaches 0.98, and when the order is 36, the AUC
is still stable at 0.9835. It should be noted that the AUC rate
has been stabilized, and increasing the order will not improve
the performance of our model. In order to generate an image
with the same size as the PPG ﬁngerprint, the AR order is
selected as 36 to calculate the AR ﬁngerprint.

B. Quantitative comparison

Quantitative comparison between the proposed method and
the method based on basic neural network and other weak
signal methods is shown in Table. II. Our method calculates
video-level accuracy, while calculating segment accuracy us-
ing the number of segment frames n=128. This is similar to
the original PPG-based method proposed by [16].

It can be seen from the table that when the FaceForensic++
dataset is selected as the experimental data, the detection
accuracy is improved from 70.47% to 96.13% compared
with the basic algorithm based on neural network. Compared
with the method [16], which is also based on weak hidden
information, the segment detection accuracy has increased
from 91.61% to 92.93%, and the video detection accuracy
has increased from 94.65% to 96.13%. Similarly, when the
original FaceForensic is selected as the experimental data,
compared with the basic network algorithm, the video de-
tection accuracy is improved by 11.88%. Compared with the
comparison algorithm [16], the segment accuracy is increased
by 2.52%, and the video accuracy is increased by 3.35%.
Because we not only improved the method of generating
PPG ﬁngerprints, but also used the AR model to consider
the correlation between pixels in the frame. Furthermore, we

choose ACNet as the classiﬁer, which is more robust in feature
arrangement, for experiments.

For different generative models, the four models in Face-
Forensic++ are quantitatively compared. Video detection ac-
curacy is higher with the proposed methods than with the
basic network model algorithm. For example, on the fake
face video data based on the Deepfakes generation model,
the video detection accuracy has increased from 89.52% to
96.44% using MesoNet. In addition, using Xception to detect
fake faces based on the FaceSwap generative model, the
video detection accuracy increased from 93.70% to 95.95%.
Compared with the comparison algorithm [16], in addition to
the lower accuracy of fake face video detection based on the
Face2Face generative model, the proposed solutions all show
better detection performance.

C. Ablation experiments

In order to demonstrate the effectiveness of the various
components of the proposed scheme, ablation experiments
with different component combinations are shown in Ta-
ble. III. In summary, compared with the ablation experiment
that removes any components, the proposed scheme shows
higher video detection accuracy in different fake face genera-
tion models and overall authenticity detection. Speciﬁcally,
comparing the last two columns in the table, the overall
accuracy is slightly improved. This reﬂects the improvement
of the robustness of the ACNet asymmetric convolution
structure to the selected feature ﬁngerprints upside-down
and left-right upside down, eliminates the inﬂuence of the
feature arrangement order, and enhances the accuracy of
model detection. Comparing the second and third columns
in the table, the overall accuracy rate has increased from
81.89% to 93.39%, conﬁrming the main contribution of the
reference model [16]. That is, the heart rate signal in the
real video has not been fully reproduced in the deep fake
video, so it can be used for authenticity detection. Taking
the last column as a reference,
the overall accuracy of
the third and fourth columns has increased by 2.74% and
2.48% respectively. This respectively reﬂects the gain effect of
ﬁngerprint features based on AR coefﬁcients on the detection
of the proposed authenticity detection scheme in the space
domain, and ﬁngerprint features based on PPG signals on the
detection of the proposed authenticity detection scheme in
the time domain. Neither of the ﬁrst two columns in the table
can well judge the authenticity of the face video. For example,
in an experiment based only on RGB channels without any
feature preprocessing, the video detection accuracy can only
reach about 80%;
that detects directly
in an experiment
through a complete frame, the authenticity detection method
is almost ineffective. The experimental results re-proven two
points of view: (1) The feature preprocessing based on the
sparse AR sparseness of the heart rate signal is the key
factor for authenticity detection. (2) In the fake face detection
experiment, it is necessary to extract the face in advance
instead of the complete video frame to improve the detection
performance.

TABLE. IV. Detection accuracy (%) of cross Deepfake gen-
eration model and cross fake face video data domain.

Train set

Test set

Video acc.

FF++ - Deepfakes
FF++ - Face2Face
FF++ - FaceSwap
FF++ - NeuralTextures
FF++
FF++

Deepfakes
Face2Face
FaceSwap
NeuralTextures
FF
Celeb-DF

95.75
92.47
96.15
79.25
90.66
86.57

D. Cross-model and cross-domain evaluation

The cross-depth fake face generation model and the cross-
fake face video data domain authenticity detection perfor-
mance are shown in Table. IV. In the cross-method evaluation
of the four generative models in FaceForensic++, except
for NeuralTextures, all have obtained high video detection
accuracy. For example, when the model is trained with the re-
maining categories except FaceSwap, and then the ﬁngerprint
data of the FaceSwap generated model is used for detection,
the video detection accuracy can be achieved by the accu-
racy of 96.15%. The cross-model evaluation and detection
performance of the NeuralTextures generative model in the
table is only 79.25%, because it is a generative model that is
fundamentally different from other generative models.

In addition,

the cross-domain evaluation results of the
FaceForensic++ dataset and its predecessor, FaceForensic,
and the more authentic Celeb-DF are shown in the last two
rows of Table. IV. Detected on the more streamlined original
FaceForensic, the performance is very good, indicating that
as long as the training data is sufﬁciently representative, the
model can make correct predictions. Detecting on a more
realistic Celeb-DF also obtains better detection performance,
which veriﬁes that the proposed scheme has generalization
and can be widely used in various real-world scenarios.

V. CONCLUSION

In this paper, we propose a Deepfake detection scheme
based on PPG signals and AR coefﬁcients. In this scheme, the
PPG signal is used to reﬂect the remote HR, and its ﬂuctuation
is regarded as temporal-domain features; the AR model is
used to reﬂect the inter-pixel correlation, and the changes
in its coefﬁcients are treated as spatial-domain features.
Furthermore, this scheme uses ACBlock-based DenseNet to
evaluate the temporal-spatial ﬁngerprints generated by two
features, thereby realizing automatic authenticity detection.
According to the simulation results, the proposed forensic
scheme improves the accuracy and generalization of face
video authenticity detection. For example,
the simulation
results show that on the FaceForensic++ dataset, the scheme
improves the total accuracy by 25.66% compared to baseline
network, and 1.48% compared to the baseline strategy in [16].
In addition, in cross-domain evaluation, the model trained on
the FaceForensic++ dataset by the proposed scheme is able
to achieve an accuracy of 86.57% when tested on Celeb-

[19] Y. Jia, J. Zhang, S. Shan, and X. Chen, “Single-side domain gen-
eralization for face anti-spooﬁng,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 8484–8493, 2020.

[20] R. Durall, M. Keuper, and J. Keuper, “Watch your up-convolution:
CNN based generative deep neural networks are failing to reproduce
spectral distributions,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 7890–7899,
2020.

[21] Z. Liu, X. Qi, and P. H. Torr, “Global texture enhancement for fake
face detection in the wild,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 8060–8069,
2020.

[22] H. Dang, F. Liu, J. Stehouwer, X. Liu, and A. K. Jain, “On the
detection of digital face manipulation,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern recognition (CVPR),
pp. 5781–5790, 2020.

[23] Z. Sun, L. Sun, and Q. Li, “Investigation in spatial-temporal domain
for face spoof detection,” in 2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 1538–1542,
2018.

[24] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venu-
gopalan, K. Saenko, and T. Darrell, “Long-term recurrent convolutional
networks for visual recognition and description,” in Proceedings of the
IEEE conference on computer vision and pattern recognition (CVPR),
pp. 2625–2634, 2015.

[25] A. Chintha, B. Thai, S. J. Sohrawardi, K. Bhatt, A. Hickerson,
M. Wright, and R. Ptucha, “Recurrent convolutional structures for audio
spoof and video deepfake detection,” IEEE Journal of Selected Topics
in Signal Processing, vol. 14, no. 5, pp. 1024–1037, 2020.

[26] N. Yu, L. S. Davis, and M. Fritz, “Attributing fake images to GANs:
the
Learning and analyzing GAN ﬁngerprints,” in Proceedings of
IEEE/CVF International Conference on Computer Vision (CVPR),
pp. 7556–7566, 2019.

[27] X. Kang, M. C. Stamm, A. Peng, and K. R. Liu, “Robust median
ﬁltering forensics using an autoregressive model,” IEEE Transactions
on Information Forensics and Security, vol. 8, no. 9, pp. 1456–1468,
2013.

[28] J. Yang, H. Ren, G. Zhu, J. Huang, and Y.-Q. Shi, “Detecting median
ﬁltering via two-dimensional ar models of multiple ﬁltered residuals,”
Multimedia Tools and Applications, vol. 77, no. 7, pp. 7931–7953,
2018.

DF dataset. The proposed false face forensics scheme can
effectively improve the authenticity detection performance to
achieve the reliability of judicial forensics and intellectual
property protection. Future research will be devoted to explore
the possibility of more faint information for face authenticity
detection.

REFERENCES

[1] T. T. Nguyen, C. M. Nguyen, D. T. Nguyen, D. T. Nguyen, and
S. Nahavandi, “Deep learning for deepfakes creation and detection:
A survey,” arXiv preprint arXiv:1909.11573, 2019.

[2] P. Korshunov and S. Marcel, “Deepfakes: a new threat to face recog-
nition? assessment and detection,” arXiv preprint arXiv:1812.08685,
2018.

[3] D. Afchar, V. Nozick, J. Yamagishi, and I. Echizen, “Mesonet: a
compact facial video forgery detection network,” in 2018 IEEE In-
ternational Workshop on Information Forensics and Security (WIFS),
pp. 1–7, 2018.

[4] A. R¨ossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and
M. Niessner, “Faceforensics++: Learning to detect manipulated facial
images,” in 2019 IEEE/CVF International Conference on Computer
Vision (ICCV), pp. 1–11, 2019.

[5] L. Li, J. Bao, T. Zhang, H. Yang, and B. Guo, “Face x-ray for more
general face forgery detection,” in 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2020.

[6] D. G¨uera and E. J. Delp, “Deepfake video detection using recurrent
neural networks,” in 2018 15th IEEE International Conference on
Advanced Video and Signal Based Surveillance (AVSS), pp. 1–6, 2018.
[7] E. Sabir, J. Cheng, A. Jaiswal, W. AbdAlmageed, I. Masi, and P. Natara-
jan, “Recurrent convolutional strategies for face manipulation detection
in videos,” Interfaces (GUI), vol. 3, no. 1, pp. 80–87, 2019.

[8] X. Yang, Y. Li, and S. Lyu, “Exposing deep fakes using inconsistent
head poses,” in ICASSP 2019 - 2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pp. 8261–8265,
2019.

[9] Y. Li, M. Chang, and S. Lyu, “In Ictu Oculi: Exposing AI Created
Fake Videos by Detecting Eye Blinking,” in 2018 IEEE International
Workshop on Information Forensics and Security (WIFS), pp. 1–7,
2018.

[10] S. Agarwal, H. Farid, Y. Gu, M. He, K. Nagano, and H. Li, “Protecting
world leaders against deep fakes,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)
Workshops, June 2019.

[11] W. Verkruysse, L. O. Svaasand, and J. S. Nelson, “Remote plethysmo-
graphic imaging using ambient light.,” Optics express, vol. 16, no. 26,
pp. 21434–21445, 2008.

[12] M.-Z. Poh, D. J. McDuff, and R. W. Picard, “Advancements in non-
contact, multiparameter physiological measurements using a webcam,”
IEEE transactions on biomedical engineering, vol. 58, no. 1, pp. 7–11,
2010.

[13] C. Br¨user, C. H. Antink, T. Wartzek, M. Walter, and S. Leonhardt,
“Ambient and unobtrusive cardiorespiratory monitoring techniques,”
IEEE Reviews in Biomedical Engineering, vol. 8, pp. 30–43, 2015.

[14] V. Markova, T. Ganchev, and K. Kalinkov, “CLAS: A database for
cognitive load, affect and stress recognition,” in 2019 International
Conference on Biomedical Innovations and Applications (BIA), pp. 1–4,
2019.

[15] Y. Liu, A. Jourabloo, and X. Liu, “Learning deep models for face
anti-spooﬁng: Binary or auxiliary supervision,” in Proceedings of the
IEEE conference on computer vision and pattern recognition (CVPR),
pp. 389–398, 2018.

[16] U. A. Ciftci, I. Demir, and L. Yin, “Fakecatcher: Detection of synthetic
portrait videos using biological signals,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, pp. 1–1, 2020.

[17] U. A. Ciftci, I. Demir, and L. Yin, “How do the hearts of deep
fakes beat? deep fake source detection via interpreting residuals with
biological signals,” in 2020 IEEE International Joint Conference on
Biometrics (IJCB), pp. 1–10, 2020.

[18] S.-Y. Wang, O. Wang, R. Zhang, A. Owens, and A. A. Efros, “CNN-
generated images are surprisingly easy to spot... for now,” in Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 8695–8704, 2020.

