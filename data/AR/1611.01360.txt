PORTMANTEAU TESTS FOR ARMA
MODELS WITH INFINITE VARIANCE

By J.-W. Lin AND A.I. McLeod
The University of Western Ontario

Jen-Wen Lin and A. Ian McLeod (2008). Portmanteau Tests for ARMA
Models with Inﬁnite Variance. Journal of Time Series Analysis, 29, 600-617

6
1
0
2

v
o
N
4

]
T
S
.
h
t
a
m

[

1
v
0
6
3
1
0
.
1
1
6
1
:
v
i
X
r
a

1

 
 
 
 
 
 
Abstract.

Autoregressive and moving-average (ARMA) models with stable

Paretian errors is one of the most studied models for time series with

inﬁnite variance. Estimation methods for these models have been studied

by many researchers but the problem of diagnostic checking ﬁtted models

has not been addressed. In this paper, we develop portmanteau tests for

checking randomness of a time series with inﬁnite variance and as a

diagnostic tool for checking model adequacy of ﬁtted ARMA models. It is

assumed that least-squares or an asymptotically equivalent estimation

method, such as Gaussian maximum likelihood in the case of AR models, is

used. And it is assumed that the distribution of the innovations is IID

stable Paretian. It is seen via simulation that the proposed portmanteau

tests do not converge well to the corresponding limiting distributions for

practical series length so a Monte-Carlo test is suggested. Simulation

experiments show that the proposed test procedure works eﬀectively. Two

illustrative applications to actual data are provided to demonstrate that an

incorrect conclusion may result if the usual portmanteau test based on the

ﬁnite variance assumption is used.

Keywords. ARMA models, Inﬁnite variance, Least squares method,

Portmanteau test, Residual autocorrelation function, Stable Paretian

distribution

2

1. INTRODUCTION

Time series models with stable Paretian errors have been studied by

many researchers. Adler et al. (1998) discussed many aspects of how to

apply standard Box-Jenkins techniques to stable ARMA processes. Adler

et al. (1998) concluded that, in principle, the standard Box-Jenkins

techniques do carry over to the stable setting but a great deal of care needs

to be exercised. In

2 we brieﬂy review the stable Paretian distribution and
§

in

3 we develop portmanteau tests for whiteness or randomness for an IID
§

series. The whiteness test is illustrated with a brief application to exchange

rate data. In

4 we develop portmanteau diagnostic checks for residuals of
§

an AR model ﬁtted by least-squares assuming the true innovations are IID

stable Paretian distributed. This is extended to the ARMA model in

Appendix C. An illustrative example shows the diﬀerences in inferences

that may result between the ﬁnite variance and inﬁnite variance

portmanteau tests.

2. THE STABLE PARETIAN DISTRIBUTION

A stable distribution is usually deﬁned through its characteristic

function. A random variable Z, or Zα(σ, β, µ), is said to have a stable

distribution if its characteristic function has the following form:

E (eitZ ) =

exp
exp

{

σ
σ

t
|
|
t
|
|

−
−

n

n

α

1
−
1 + iβ 2
(cid:16)
(cid:16)

iβ sgn(t) tan πα
2
t
π sgn(t) log
|

|

(cid:17)

+ iµt
(cid:17)
+ iµt

o

if α
= 1
if α = 1,

o

(1)

where i2 =

1, t is the parameter of the characteristic function, α is the

−

index of stability, or the characteristic exponent, satisfying 0 < α

2,

≤

3

6
σ > 0 is the scale parameter, β is the skewness satisfying

1
−

≤

β

≤

1,

R1 is the location parameter, and

µ

∈

sgn(t) =

1
0

{

if t > 0
if t = 0
1 if t < 0.

−
In this paper, we restrict our attention to processes generated by

application of a linear ﬁlter to an independently and identically distributed

(IID) sequence,

Zt : t = 0,

1, . . . ,

±

}

{

, of random variables whose

distribution F has Pareto-like tails, i.e.,

{

xα (1
−
xα F (
−
p = 1

F (x)) = xα P (Zt > x)
x) = xα P (Zt <
x)

→

p C
q C,

−

→

q

1, and C is a ﬁnite positive constant, or

(2)

as x

, where 0

→ ∞

≤
the dispersion of the random variable Zt.

−

≤

3. PORTMANTEAU TESTS FOR RANDOMNESS OF STABLE

PARETIAN TIME SERIES

In this section, we shall derive the asymptotic distributions of

portmanteau tests for checking randomness of a sequence of stable Paretian

random variables. We consider the stable analogues of portmanteau tests of

Box and Pierce (1970) as well as Peˇna and Rodriguez (2002), denoted by
QBP and ˆD, respectively. To do so, we require some important properties of

sample autocorrelation functions (ACF) and sample partial autocorrelation

functions (PACF) of stable Paretian ARMA processes (Brockwell and

Davis, 1991, Ch. 13; Samorodnitsky and Taqqu, 1994; Adler et al., 1998).

3.1 Asymptotic Distribution of Autocorrelation Function

4

Let

{

Zt : t = 0,

1,

±

2, . . .
}

±

be an IID sequence of stable Paretian

random variables and Xt be the strictly stationary process deﬁned by

∞

Xt =

ψjZt−j,

t = 1, . . . , n,

Xj=−∞

where

∞

j
Xj=−∞ |

ψj|

| |

δ <

∞

,

for some δ

(0, α)

∩

∈

[0, 1] .

(3)

(4)

The stable analogue of the autocorrelation function at lag k is deﬁned as

ρk =

ψjψj+k/

ψ2

j , k = 1, 2, . . . .

(5)

Xj
Eqn (5) can be estimated by the sample autocorrelation function as follows:

Xj

rk =

n−k

(

Xt=1

XtXt+k

/

)

n

Xt=1

X 2

t , k = 1, 2, . . . ,

(6)

for α > 0. According to Davis and Resnick (1986), for any positive integer

k, the limiting distribution of sample autocorrelation functions is given by

1
α

n
log(n) #

"

(r1 −

ρ1, . . . , rk −

ρk)T

→

(Y1, . . . , Yk)T ,

(7)

where

→

denotes convergence in distribution and

Yh =

∞

Xj=1

(ρk+j + ρk−j −

2ρj ρk)

Sj
S0

, h = 1, . . . , k,

(8)

where S0, S1, . . . are independent stable variables; S0 is positive with
S0 ∼

α/2 , 1, 0), and the Sj are Zα(C −1/α

Zα/2(C −2/α

, 0, 0), where

α

and

Cα =

Γ(2

α
1
−
α) cos( πα
2 )

−

if α

= 1,

Cα =

2
π

if α = 1.

5

6
Under the null hypothesis that Xt are a sequence of IID stable

Paretian random variables, we have ρ0 = 1 and ρk = 0 for k

1 so the

≥

limiting distribution of sample ACFs can be further simpliﬁed as follows:

(9)

(10)

(11)

1
α

n
log(n) #

"

(r1, . . . , rk)T

→

(W1, . . . , Wk)T ,

where Wh are given by

Wh =

Sh
S0

, h = 1, . . . , k.

Note that, for α > 1, we may also use the mean-corrected sample

autocorrelation function at lag k, denoted as ˜rk, which is given by

˜rk =

n−k

Xt=1

(Xt −

¯X)(Xt+k −

¯X)/

n

Xt=1

¯X)2,

(Xt −

k = 1, 2, . . . . Davis and Resnick (1986) indicated that the limiting

distribution of ˜rk is the same as that of rk.

3.2 Asymptotic Distribution of Partial Autocorrelation Function

Consider an AR (p) process,

Xt −

φ1Xt−1 −

. . .

−

φpXt−p = Zt,

{
φ1z

2, . . .
}

±
φpzp

±
= 0,

where

Zt : t = 0,

1,

are a sequence of IID stable Paretian errors,

1

−

−

−

. . .

z
|
| ≤
R(p) = (ρ|i−j|)p×p be the p
matrix, and φ(p) = (φ1, . . . , φp)T . The Yule-Walker equations are deﬁned as

1. Let ρ(p) = (ρ1, . . . , ρp)T be a vector of

autocorrelation functions,

p autocorrelation

×

R(p)φ(p) = ρ(p).

6

(12)

6
The PACF at lag p is simply the p-th element of the solution of the

Yule-walker equations,

φY W
(p) = Ψ

=

ρ(p)
(cid:16)

(cid:17)

−1
(p)ρ(p).

R

Likewise, the sample partial autocorrelation function at lag p is deﬁned as

the p-th element of the sample estimate of the Yule-walker solution,

ˆφY W
(p) = Ψ(r(p)) = R−1

(p)r(p),

where R(p) = (r|i−j|)p×p and r(p) = (r1, . . . , rp)T are the p

p sample

×

autocorrelation matrix and the p

×

1 vector of sample autocorrelation

functions, respectively. It is apparent that the sample partial

autocorrelations is a function of sample autocorrelations. Their relationship

is clearly described in the Durbin-Levison algorithm.

Let πk be the sample PACF at lag k, and π(m) = (π1, . . . , πm)T . By the

Durbin-Levison algorithm, the vector π(m) can be expressed as a function of

r(m), π(m) = ψ(r(m)), with the k-th element given by

πk = ψ(r(k)) =

(k−1)R−1
rT
(k−1)R−1
rT

(k−1)r∗
(k−1)
(k−1)r(k−1)

,

rk −
1
−

(13)

where R(k) and r(k) are as deﬁned above and r∗

(k) = (rk, . . . , r1)T .

Following the proof in Monti (1994), we can derive the asymptotic

distribution of sample partial autocorrelation functions. Under the null

hypothesis that Xt are independent, the autocorrelation functions are all

zero, and according to Brockwell and Davis (1991, ch. 13),

rh = Op 
"


−1/α

n
log(n) #





7

, h = 1, 2, . . . .

Therefore,

where 1k is a k

×

R(k) = 1k + Op 


−1/α

n
log(n) #

"

,





k identity matrix. By eqn. (13),

π(m) = r(m) + Op 


"

−2/α

n
log(n) #

.





Using eqn. (9), we have

1
α

n
log(n) #

"

(π1, . . . , πm)T

→

(W1, . . . , Wm)T .

(14)

(15)

3.3 Asymptotic Distributions of QBP and ˆD Tests

We can now derive the limiting distributions of the QBP and ˆD tests for

checking randomness of a sequence of stable Paretian random variables.

Under the assumption that 1 < α < 2, Runde (1997) derived the limiting

distribution of QBP, based on the mean corrected sample autocorrelation

functions. His result is given by

2/α m

n
log(n) !

˜r2
j →

W 2

1 +

· · ·

+ W 2
m,

Xj=1

where

Wk : k = 1, . . . , m
}

{

are deﬁned in eqn. (10). Note that if 0 < α

(16)

1,

≤

the limiting distribution of eqn. (16) remains the same if ˜rk are replaced by

rk.

Consider next the ˆD test of Peˇna and Rodriguez (2002). The test

statistic may be given by

ˆD =

2/α

n
log(n) !

1
(cid:16)

1/m

R(m)|

− |

.

(cid:17)

(17)

8

 
 
Following the proof of Theorem 1 in Peˇna and Rodriguez (2002), we may

have the asymptotic distribution of eqn. (17) in the following Theorem.

The proof is given in Appendix A.

THEOREM 1 ˆD in eqn. (17) is asymptotically distributed as

m + 1
m

i

−

W 2
i ,

m

Xi=1

where

Wi : i = 1, . . . , m
}

{

are as deﬁned in eqn. (10).

Remark 1: It is possible to compute the limiting distributions of the QBP
and ˆD tests by making use of the change variable technique and some

numerical algorithms of calculating the probability density function of

stable random variables, such as Mittnik et al. (1999). This approach

requires, however, intensive numerical computations.

Remark 2: Another approach to obtaining the asymptotic distributions of
the QBP and ˆD tests is to simulate the aforementioned tests based on their
asymptotic distributions. For example, ˆD is simulated as deﬁned in

Theorem 1. This approach also requires a large scale of computation but is

much less intensive computationally than the approach mentioned in

Remark 1. This approach will be adopted in the subsequent analysis based

on 104 simulations.

3.4 Simulation Experiments

The ﬁnite sample performance of QBP and ˆD tests for randomness will

be investigated in this section. Based on 250 simulations, the 5, 10, 30, 50,

70, 90, 95, 97.5, 99 (%) empirical quantiles of both tests with lag m = 5

9

were calculated and plotted against the corresponding asymptotic

distributions. It is seen in Figure 1 and Figure 2 that the empirical and

asymptotic quantiles do not agree very well unless n is very large.

It is seen in Figures 1 to 2 that the speed of convergence of both tests

to the corresponding asymptotic distributions is very slow. A solution to

this problem is to use the Monte-Carlo test or parametric bootstrap

(Appendix B).

[Figures 1 and 2 about here]

Consider the simulation experiments. IID random sequence of

Zα(1, 0, 0) with series length n = 250 and α = 1.9, 1.7, 1.5, 1.3, 1.1 were

simulated. The empirical sizes of both tests were calculated based on

N = 104 simulations and each Monte-Carlo test was simulated based on 103

simulations. The results are tabulated in Table 1. It is seen that the

empirical sizes of both tests are very close to the 5% nominal level even

with n = 250.

[Table 1 about here]

3.5 Illustrative Example

Consider the daily Canada/U.S. exchange rates dated from September

06, 1996 to September 05, 2006. The data was retrieved from the website of

the Federal Reserve Bank of St. Louis and the returns, et = log(zt+1/zt),

were computed and tested for randomness. The consistent estimators of

McCulloch (1986) were used to estimate α and β for the returns. We

10

obtained ˆαM = 1.5644 and ˆβM =

0.0472. It is seen that ˆβM is close to

−

zero so the series is not highly skewed. Since ˆαM is much less than 2, the

usage of the portmanteau tests in

3 are more reasonable than that of the
§

ordinary portmanteau tests in this data. The P-values for QLB(m) test were

determined using the asymptotic χ2(m) distribution and the Monte-Carlo

method in Appendix B. The results are compared in Table 2. Note that

when m = 5 the ﬁnite-variance portmanteau test suggested possible

evidence of non-randomness but this is not the case when the

inﬁnite-variance Monte Carlo test is used.

[Table 2 about here]

Remark 3: Portmanteau tests based on the nonparametric bootstrap

procedure could also be used but it would be expected that they would be

less powerful since less information is used.

4. DIAGNOSTIC CHECK FOR MODEL ADEQUACY OF AR(p)

MODELS WITH STABLE PARETIAN ERRORS

4.1 Some Asymptotic Results

In this section, we shall derive the asymptotic distributions of QBP and

ˆD tests for diagnostic check in model adequacy of AR (p) models with

stable Paretian errors. Consider the general AR (p) process as follows:

φ(B)Xt = Zt,

(18)

11

where

Zt : t = 0,

2, . . .
}
random variables, B denotes the backward operator, and

is an IID sequence of stable Paretian

±

±

1,

{

φ(B) = 1

φ1B

−

− · · · −

φpBp. Let ˆφ(p) = ( ˆφ1, . . . , ˆφp)T denote the estimates

of autoregressive coeﬃcients. The residuals of the ﬁtted model are given as

follows:

ˆZt = Zt( ˆφ(p)) = Xt −

ˆφ1Xt−1 −

. . .

−

ˆφpXt−p = ˆφ(B)Xt,

(19)

and the corresponding residual autocorrelation at lag k is given by

ˆrk =

ˆZt ˆZt−k
ˆZ 2
t

.

P

Consider the estimators of ˆφ(p) satisfying

P

ˆφ(p) = φ(p) + Op

[n/ log(n)]−1/α

.

(cid:16)

(cid:17)

From Appendix C, the residual autocorrelation at lag k, ˆrk, can be

approximated by the ﬁrst order Taylor expansion about error

autocorrelation functions, rk. Speciﬁcally, the approximation is

p

ˆrk = rk +

(φj −
where ψj is the impulse response coeﬃcient at lag j and

[n/ log(n)]−2/α
(cid:16)

ˆφj) ψk−j + Op

Xj=1

,

(cid:17)

(20)

rk =

ZtZt−k/

Z 2

t is the error autocorrelation at lag k. Eqn. (20) can

also be written in matrix form, to order Op

P

P

[n/ log(n)]−2/α
(cid:16)

ˆr(p) = r(p) + X

φ(p) −

(cid:16)

ˆφ(p)

,

(cid:17)

where

X =













1

ψ1
...
...

0

1
...
...

ψm−1 ψm−2

12

· · ·
. . .
. . .
. . .

· · ·

0

0

0

0
ψm−p

.













,

(cid:17)

(21)

(22)

By making use of eqn. (20) or eqn. (21) as well as following the proof

in Theorem 1, we may derive the asymptotic distributions of the

aforementioned portmanteau tests for diagnostic check in AR (p) models.

This distribution, however, is usually very complicated and may not be

traceable unless the AR (p) models of interest are ﬁtted by least squares

(LS). For simplicity, we only consider the case that eqn. (18) is estimated

using least squares in the subsequent analysis.

According to

4 in Davis (1996), if the ARMA parameters, β, are
§

estimated using least squares , we have [n/log(n)]1/α
distribution, where ˆβLS denotes the LS estimates of β. Hence, in terms of
our notation, we have ˆφ(p) −
Pierce (1970),
{

in eqn. (19) satisfy the orthogonality conditions and, to

. Then, by Box and

[n/log(n)]−1/α

converges in

φ(p) = Op

ˆZt}

β

(cid:17)

(cid:17)

(cid:16)

(cid:16)

ˆβLS −

order Op

1/√n [n/log(n)]−1/α

,

(cid:16)

(cid:17)

ˆrT
(p) X = 0.

If we now multiply eqn. (21) on both sizes by

Q = X(XT X)−1XT ,

then using eqn. (23) we have

ˆr(p) = (1m −

Q) r(p)

approximately, where 1m is an m
×
Q = X(XT X)−1XT . It was shown by Box and Pierce (1970) that 1m −
idempotent of rank m

p. Hence, the asymptotic distribution of the QBP

m identity matrix and

−

test is given by

(

n
log n

)2/α

m

X1

ˆr2
k →

WT

m(1m −

Q)Wm,

(25)

13

(23)

(24)

Q is

where Wm = (W1, . . . , Wm)T and

Wi : i = 1, . . . , m
}

{

are deﬁned in eqn.

(10).

Consider next the asymptotic distributions of residual partial

autocorrelations. Let ˆπ(m) be the vector of the ﬁrst m residual partial

autocorrelations and π(m) is the vector of error partial autocorrelations.

The Taylor expansion of ψ(ˆr(m)) around r(m) yields

ˆπ(m) = π(m) +

∂π(m)
∂r(m) (cid:16)

ˆr(m) −

r(m)

+ Op 
"


(cid:17)

−2/α

n
log n #

.





By eqn. (13) and (14), eqn. (26) becomes

ˆπ(m) = ˆr(m) + Op 


−2/α

n
log n #

"

.




Consider the Peˇna-Rodriguez test as the form of

ˆD = (

n
log n

)2/α

1

(cid:16)

ˆR(m)|

− |

1/m

,

(cid:17)

(26)

(27)

(28)

where ˆR(m) = (ˆr|i−j|)m,m is the m

×

m residual autocorrelation matrix. By

eqn. (27) and following the proof in Theorem 1, the limiting distribution of

eqn. (28) is WT
Wm,m is a m
(m

Wm,m (1m −
m Am Wm, where Am = (1m −
m diagonal matrix with (i, i)-th element equal to
×
i + 1)/m for i = 1,

Q)T

, m.

−

· · ·

Q) and

Remark 4: It is shown in Appendix C.4 that the residuals in a ﬁtted

ARMA model are asymptotically equivalent to those in a particular AR

model. Hence the asympotic results for the AR may be extended to the

ARMA case.

4.2 Some Size and Power Calculations

14

As in

3.4, the slow convergence of QBP and ˆD tests to their asymptotic
§

distributions is also present at the residual autocorrelations. The ﬁrst order
autoregressive process Xt = 0.5Xt−1 + Zt with Zt ∼
simulated and AR (1) models were ﬁtted to the data. Then the 5, 10, 30,

Z1.2(1, 0, 0) was

50, 70, 90, 95, 97.5, 99 (%) empirical quantiles of ˆr1 were plotted against its

theoretical asymptotic distribution based on 103 simulations. The

asymptotic distribution of the error autocorrelation at lag one, r1, was also

plotted in Figure 3. It is seen that empirical quantiles of ˆr1 get closer to its

asymptotic distribution as the series length n increases. However, this is

not the case for the empirical quantiles of ˆr1 to the asymptotic distribution

of r1. Therefore, serious size distortion may be present in this case if one

uses error autocorrelations as a diagnostic tool for checking model

adequacy. The slow convergence of residual autocorrelations to its

asymptotic distribution may cause diﬃculties in using portmanteau tests in

practice. Therefore, as in

3.4, we suggested using the Monte-Carlo test to
§

improve the eﬀectiveness of portmanteau tests.

[Figure 3]

We now investigate the eﬀectiveness of QBP and ˆD tests for diagnostic

check in ﬁtted AR models with stable Paretian errors. The empirical sizes
of ˆD and QBP tests for a 5% signiﬁcance test were ﬁrst calculated via

simulation. In this experiment, AR (1) models, Xt = φ1Xt−1 + Zt, were
simulated, where Zt ∼
±
and AR (1) models were ﬁtted to the simulated data by the Burg

Z1.5(1, 0, 0) and φ1 = 0,

0.7,

0.3,

0.1,

0.5,

±

±

±

±

0.9

algorithm. The empirical size for each test was calculated based on N = 104

15

simulations and each Monte Carlo test used 103 simulations. Series length

n = 100 and lags m = 5, 10, 20 were investigated. It is seen in Table 3 that

the empirical sizes of both tests are very close to their nominal level.

[Table 3]

The empirical powers of ˆD and QBP tests as diagnostic tools were also

investigated via simulation. Twelve ARMA (2, 2) models of series length

n = 100 in Table 4 of Peˇna and Rodriguez (2002) were simulated and

AR (1) models were ﬁtted to the simulated data using the Burg algorithm.

Both tests with lags m = 5, 10, 20 were calculated using the parametric

bootstrap procedure. The empirical powers were calculated based on

N = 103 simulations and each Monte Carlo test used 103 simulations. It is

seen in Table 4 that the empirical powers of both tests are reasonably good

for most models. Some of them are even better than the powers listed in

Peˇna and Rodriguez (2002). In addition, increasing the series length can

also improve the eﬀectiveness of the proposed test procedure. For example,

with model 3 in Table 2, if the series length was increased to n = 250, the
empirical powers of the ˆD test at lags m = 5, 10, 20 were increased

signiﬁcantly from 23.37%, 20.10% and 17.61% to 58.27%, 43.71% and

35.52%, respectively. Similar improvement was also found in the QBP test.

Finally, as in Peˇna and Rodriguez (2002), our simulation experiments show
that ˆD is more powerful than QBP as a diagnostic tool.

[Table 4]

Remark 5: It is well known that the Burg estimate of φ1 is close to the LS

16

estimate. The advantage of using Burg estimate is that it is always in the

stationary region and this is needed for the Monte-Carlo test.

4.3 Illustrative Application

Tsay (2002, Ch. 2) tentatively identiﬁed an AR(3) or AR(5) model for

the monthly simple returns of CRSP value-weighted index from January

1926 to December 1997 using the partial autocorrelation function. Here

n = 864 and the usual Box-Pierce portmanteau test at lags m = 5, 10, 20

does not suggest model inadequacy of either model at the 5% level. By
applying our Monte-Carlo test procedure, however, both the ˆD and QBP

tests in

4 reject both models. The P-values are displayed in Table 5. The
§

inﬁnite variance hypothesis is plausible since the estimates for α of residuals

in the ﬁtted AR(3) and AR(5) models are 1.696 and 1.635, respectively. We

may conclude from this example that using the ordinary portmanteau tests

may lead to a wrong decision if innovations have inﬁnite variance.

[Table 5]

5. CONCLUDING REMARK

We will provide an R package implementing the portmanteau tests

described in this paper on CRAN.

17

APPENDIX A: PROOF OF THEOREM 1

First, by decomposing the determinant of the sample autocorrelation

matrix R(m), Pena and Rodriguez (2002) showed that

R(m)|

|

1/m is a

weighted function of the ﬁrst m partial autocorrelations. Speciﬁcally,

m

1/m =

(1

R(m)|

|

i )(m+1−i)/m.
π2

−

Yi=1
Suppose that under the null hypothesis, ˆD is asymptotic distributed as

(29)

.

X

By applying the δ-method to g(x) = log(1

x), it follows that

−

(n/ log(n))2/α log

−
|
eqn. (29), we can have

(cid:16)

R(m)|

1/m

is asymptotically distributed as

(cid:17)

. From

X

n
log(n) !

n
log(n) !

−  

−  

2/α

log

2/α m

Xi=1

1/m

=

|

Rm|
(cid:16)
m

−

(cid:17)
i + 1
m

log(1

π2
i ).

−

Next suppose that

2/α

n
log(n) !

(cid:16)

1, π2
π2

2, . . . , π2
m

T

(cid:17)

Y,

−→

and apply the multivariate δ-method to

(30)

(31)

g(π2

1, π2

2, . . . , π2

m) =

m

m

−

Xi=1

−

i + 1
m

log(1

π2
i ),

−

it follows that

m

m

−

Xi=1

−

i + 1
m

log(1

π2
i )

−

1,

→ (cid:18)

m

−
m

1

, . . . ,

1
m (cid:19)

Y.

(32)

18

 
From the Cramer-Wold theorem, it follows that

m

1

,

−
m

· · ·

,

1
m (cid:19)

1,

(cid:18)





2/α

n
log(n) !

π2
1, . . . ,

1,

−→ (cid:18)

2/α

n
log(n) !
m

1

, . . . ,

−
m

T

π2
m


1
m (cid:19)

Y

By eqn. (15), it follows that

1

m

−
m

1,

(cid:18)

, . . . ,

1
m (cid:19)





2/α

n
log(n) !

π2
1, . . . ,

n
log(n) !

W 2

1 +

−→

1

m

−
m

W 2

2 + . . . +

2/α

T

π2
m


W 2
m,

1
m

(33)

(34)

Finally, from eqn. (33) and eqn. (34),

m

−
m

1,

(cid:18)

1

, . . . ,

1
m (cid:19)

Y

m

→

Xi=1

m + 1
m

i

−

W 2
i ,

and from (31), we have the

ˆD

m

→

Xi=1

m + 1
m

i

−

W 2
i .

✷

19

 
 
 
 
APPENDIX B: MONTE-CARLO TEST PROCEDURE

The Monte-Carlo test procedure for diagnostic checking of AR and

ARMA models with stable Paretian errors can be summarized below. Note

that, to check randomness of a time series, we skip Step 1 and in Step 4 we

simulate data from an IID sequence of

Z ˆα}

{

rather than from the ﬁtted

model.

Step 1 Fit an AR model to data using least-squares or the Burg algorithm

or for ARMA, an approximate Gaussian maximum likelihood

algorithm is used. Calculate residuals
of interest , say ˆDm.

ˆZt}

{

and the portmanteau test

Step 2 Estimate α from residuals

{
McCulloch (1986) may be used.

ˆZt}

in Step 1. The estimator given by

Step 3 Select the number of Monte-Carlo simulations, B. Typically

100

B

≤

≤

1000.

Step 4 Simulate the ﬁtted model using the estimated AR or ARMA

parameters in Step 1 and ˆα in Step 2. Obtain ˆDm after estimating the

parameters in the simulated series.

Step 5 Repeat Step 4 B times counting the number of times k that a value
of ˆDm greater than or equal to that in Step 1 has been obtained.

Step 6 The P -value for the test is (k + 1)/(B + 1).

Step 7 Reject the null hypothesis if the P -value is smaller than a

predetermined signiﬁcance level.

20

APPENDIX C: THE GENERALIZATION OF LINEAR EXPANSION OF

RESIDUAL AUTOCORRELATION

C.1 Introduction

Residual autocorrelations are an important tool for diagnostic checking

of autoregressive and moving average ( ARMA ) models. Their asymptotic

distributions from univariate ARMA models were ﬁrst derived by Box and

Pierce (1970). McLeod (1978) reﬁned the derivation and extended it to the

multiplicative seasonal ARMA models. Their results were established

under the assumption that error sequences have ﬁnite variance and the

parameters are estimated using least squares, or equivalently, using

maximum likelihood estimation (MLE) for Gaussian ARMA processes.

Their result may not be valid if the parameters of interest are estimated

using other estimation methods or linear processes with inﬁnite variance.

This section demonstrates how the linear expansion of residual

autocorrelations in Box and Pierce (1970) also holds for other estimation

methods and for AR models with stable Paretian errors. The expansion

may be used to derive the limiting distribution of residual autocorrelations.

C.2 The Autoregressive Process

Consider an AR (p) process as follows:

φ(B)yt = at,

(35)

where B denotes the backward operator, φ(B) = 1
at}

{

is a sequence of independent and identical random variables with mean

φ1B

−

− · · · −

φpBp, and

21

zero and ﬁnite variance σ2

a. For given values ˙Φ =

parameters, we can deﬁne

˙φ1,

· · ·

(cid:16)

, ˙φp

T

of

(cid:17)

˙at = at( ˙Φ) = yt −

˙φ1yt−1 − · · · −

˙φpyt−p = ˙Φ(B)yt

and the corresponding autocorrelation function at lag k as

˙rk = rk( ˙Φ) =

˙at ˙at−k
˙a2
t

.

P

P

(36)

(37)

C.3 Linear Expansion of Residual Autocorrelation Function about Error

Autocorrelation Functions

Consider approximating the residual autocorrelation ˆrk by a ﬁrst order

Taylor expansion about ˆΦ = Φ. Let ˙ck and ˙rk denote

˙at ˙at−k and ˙ck/ ˙c0

respectively, where k

∈

integer. Consider the estimators of Φ satisfying

P

ˆφj = φj + Op

,

1/√n
(cid:17)

(cid:16)

j.

∀

ˆrk = rk +

p

φj −

Xj=1 (cid:16)

ˆφj

ˆδjk + Op (1/n) ,

(cid:17)

We have

where

ˆδjk =

−

˙Φ= ˆΦ

∂ ˙rk
∂ ˙φj |
∂
˙ck
∂ ˙φj (cid:18)
˙c0 (cid:19) |
−
ij + ˆδ(2)
= ˆδ(1)
ij ,

=

˙Φ= ˆΦ

ˆδ(1)
ij =

˙ck

−

∂
∂ ˙φj (cid:18)

1
˙c0 (cid:19) |

˙Φ= ˆΦ

22

(38)

(39)

(40)

and

ˆδ(2)
ij =

1
˙c0

∂ ˙ck
∂ ˙φj |

−

˙Φ= ˆΦ.

For LS estimates, we have that

∂
∂ ˙φj hX
so it is straightforward that ˆδ(1)
(1970) showed that ˆδjk = ψk−j to order Op

˙Φ= ˆΦ =

˙a2
t

i

|

∂c0
∂ ˙φj |

ij = 0. Using this result, Box and Pierce

n−1/2

, where ψj’s are the

˙Φ= ˆΦ = 0

(41)

(cid:16)
impulse response coeﬃcients of the MA (
) representation of eqn. (35).
∞
For other estimation methods, however, ˆδ(1)
ij may not be zero since eqn. (41)
does not hold. To obtain a general result for ˆδij, therefore, we will calculate
ˆδ(1)
ij explicitly.

(cid:17)

Note that ˆδ(1)

ij can be written as follows:

˙ck ·

˙a2
t

−2 ∂ ˙c0
∂ ˙φj |

˙Φ= ˆΦ.

(42)

i
By eqn. (2.15) of Box and Pierce (1970) and letting k = 0, eqn. (42) can be

hX

expressed as follows:

P

P

P

=

p

y2
t
ˆa2
t ·
Xi=0
ˆφi

p
i=0

p
i=0

ˆφi

−i+j + r(y)
r(y)
h

i−j

ˆck
ˆc0

·

i

r(y)
−i+j + r(y)
h
ˆφjr(y)
p
j=0

ˆφi

i−j

i−j

ˆrk,

i

·

(43)

where

Let ˆζj denote

P

P

r(y)
ν =

ytyt−ν
y2
t

.

P

P

p

Xi=0

ˆφi

−i+j + r(y)
r(y)
h

i−j

!
i

/

23

p

p





Xi=0

Xj=0

ˆφi ˆφjr(y)

i−j 

,



 
and approximate ˆζj by replacing ˆφ’s and r(y)’s with φ’s and ρ’s, the

theoretical parameters and the autocorrelations of the autoregressive

process

yt}

{

. By the Barteltt’s formula,

r(y)
k = ρk + Op

1/√n
(cid:16)
(cid:17)

as well as eqn. (38) and (43), we have

ˆζj = ζj + Op

1/√n
(cid:17)

(cid:16)

.

(44)

Then by making use of the recursive relation which is satisﬁed by the

autocorrelations of an autoregressive process, eqn. (2.19) of Box and Pierce

(1970), or

ρν −

φ1ρν−1 − · · · −

φpρν−p = φ(B)ρν = 0,

1,

ν

≥

(45)

ζj can be simpliﬁed to yield

ζj =

p
i=0 φiρ−j+i
p
i=0 φiρi

P

.

(46)

P
Note that eqn. (46) has the same form of eqn. (2.20) of Box and Pierce

(1970). Speciﬁcally, it can be seen as δ−j. Moreover, Box and Pierce

indicated that δν = 0, ν < 0 so ζj = 0. Plugging this result into eqn. (43),
we have ˆδ(1)

ij = 0. Consequently, eqn. (2.20) of Box and Pierce (1970) for

the linear expansion of residual autocorrelations still holds for other
estimators with order ˆφi −
Remark 6 : Many estimators of φ(p) for an AR model with Paretian

φ = Op(1/√n).

stable errors have order Op([n/ log(n)]−1/α), such as Whittle’s, Yule-Walker

and LS estimtors. Using the result that r(p) = ρ(p) + Op([n/ log(n)]−1/α),

24

and following the proofs in this section as well as in Box and Pierce (1970),

we may obtain the linear expansion of residual autocorrelation functions for

AR models with stable Paretian errors as in eqn. (20)

C.4 The Equality of Residuals in AR and ARIMA Models

The result in

C.3 may be extended to ARIMA models using
§

technique in

5.1 of Box and Pierce (1970). If two time series (a) an
§

ARMA (p, q) process

φ(B)wt = θ(B)at,

(47)

and (b) an autoregressive series

π(B)xt =

π1B

1
(cid:16)

−

− · · · −

are both generated from the same set of errors

πp+qBp+q

xt = at,

(48)

(cid:17)
, where

at}

{

φ(B) = 1

φB

−

−

φB2

− · · · −

φBp,

θ(B) = 1

θB

−

−

θB2

− · · · −

θBq.

and

If

π(B) = φ(B)θ(B),

(49)

then when the models are ﬁtted by least squares, their residuals, and hence

also their autocorrelations, will be very nearly the same. In this section, we

consider whether the equality of residuals between AR and ARIMA

models is still valid when the parameters are estimated by other approaches.

As in eqn. (36), deﬁne

t = aAR
˙aAR

t

( ˙π) = ˙π(B)xt =

p+q

−

Xj=0

25

˙πjxt−j,

(50)

q

−1

˙θjBj



wt,

(51)

# 
Xj=0




1. Using eqn. (5.12) and eqn. (5.13) of Box and Pierce

where ˙π0 =

1, and now also

−
t ( ˙φ, ˙θ) = ˙φ(B) ˙θ(B)−1wt =

t = a⋆
˙a⋆

˙φiBi

p

"
Xi=0

where ˙φ0 = ˙θ0 =
(1970), we can approximate aAR

−

and a⋆

t as follows:

t

˙aAR = a + X (π

˙π)

−

and

˙a⋆ = a + X

β
(cid:16)

−

˙β

.

(cid:17)

(52)

(53)

Note that eqn. (52) and eqn. (53) can be seen as a linear regression

model. We can estimate regression coeﬃcients, π

˙π and β

−

−

˙β using any

suitable method. Let g(X, ˙a•) denote the corresponding estimator. Since

both eqn. (52) and eqn. (53) have the same form, their estimators should

agree with each other. For example, least squares estimates are given by

and

˙π = g(X, ˙aAR) = (XT X)−1XT ˙aAR

ˆπ

−

˙β = g(X, ˙a⋆) = (XT X)−1XT ˙a⋆.

ˆβ

−

Then by setting ˙a = a and estimating the regression coeﬃcients of eqn.

(52) and eqn. (53), we have

π = g(X, a) = ˆβ

ˆπ

−

β.

−

(54)

(55)

(56)

Finally, by setting ˙aAR = ˆaAR and ˙a⋆ = ˆa⋆ in eqn. (52) and eqn. (53), it

follows from eqn. (56) that to order Op

ˆβ

β

|
ˆaAR = g(X, a) = ˆa⋆,

−

(cid:16)

|

2

(cid:17)

(57)

and thus (to the same order) ˆrAR = ˆr⋆.

26

REFERENCES

Adler, R.J. Feldman, R.E. and Gallagher, C. (1998), “Analysing

Stable Time Series,” A Practical Guide to Heavy Tails: Statistical

Techniques and Applications, Birkh¨auser, Boston.

Box, G.E.P. and Pierce, D.A. (1970), “Distribution of Residual

Autocorrelation in Autoregressive-Integrated Moving Average Time

Series Models,” Journal of American Statistical Association 65,

1509-1526.

Brockwell, P.J. and Davis, R.A. (1991), Time Series: Theory and

Methods, Springer, New York.

Davis, R.A. (1996), “Gauss-Newton and M-estimation for ARMA

processes,” Stochastic Processes and their Applications 63, 75–95.

Davis, R.A. and Resnick, S. (1986), “Limit Theory for the Sample

Covariance and Correlation Functions of Moving Averages,” The

Annals of Statistics 14, 533–558.

McCulloch, J.H. (1986), “Simple Consistent Estimators of Stable

Distribution Parameters,” Communication in Statistics–Computation

and Simulation, 15, 1109–1136.

Mittnik, S., Rachev, S.T., Doganoglu, T. and Chenyao, D.

(1999), “Maximum Likelihood Estimation of Stable Paretian Models,”

Mathematical and Computer Modelling, 29, 275–293.

Monti, A.C. (1994), “A Proposal for Residual Autocorrelation Test in

Linear Models,” Biometrika 81, 776–780.

Peˇna, D. and Rodriguez, J. (2002), “A Powerful Portmanteau Test of

Lack of Fit For Time Series,” Journal of American Statistical

27

Association 97, 601-610.

Runde, R. (1997), “The Asymptotic Null Distribution of the Box-Pierce

Q-Statistic for Random Variable with Inﬁnite Variance: An

Application to German Stock Returns,” Journal of Econometrics 78,

205-216.

Samorodnitsky, G. and Taqqu, M. (1994), Stable-Non-Gaussian

Random Processes, Chapman-Hall, New York.

Tsay, R.S. (2002), Analysis of Financial Time Series, New York: Wiley.

28

Table I. Empirical sizes (%) of ˆD and QBP for a 5% significance
test based on the parametric bootstrap procedure. The empir-
ical size for each test was calculated based on N = 104 simu-
lations. Each Monte Carlo test also used B = 103 simulations.
Series length n = 250 and lags m = 5, 10, 15 were investigated.

ˆD(5)
5.30
5.18
4.82
4.80
5.26

ˆD(10)
4.66
4.44
4.99
5.03
5.33

α = 1.9
α = 1.7
α = 1.5
α = 1.3
α = 1.1

ˆD(15) QBP(5) QBP(10) QBP(15)
4.78
4.44
5.13
5.18
5.12

4.96
4.82
5.07
5.04
5.33

4.71
4.43
5.27
5.00
5.25

4.87
4.41
5.30
5.27
5.15

29

Table II. P-values for QLB statistic using Monte-Carlo test and
χ2-method for testing randomness of exchange-rate returns.

Monte-Carlo Test χ2(m) Test

m = 5
m = 10
m = 20

0.500
0.582
0.828

0.042
0.228
0.404

30

Table III. Empirical sizes (%) of ˆD and QBP for a 5% significance
test. ˆD and QBP tests for checking model adequacy of AR (1)
models fitted by the Burg algorithm. Both tests were imple-
mented by the parametric bootstrap procedure. The empirical
size for each test was calculated based on N = 104 simulations.
Each Monte Carlo test also used B = 103 simulations. Series
length n = 100 and lags m = 5, 10, 20 were investigated.

φ1
0.9
0.7
0.5
0.3
0.1
0.1
0.3
0.5
0.7
0.9

−
−
−
−
−

ˆD(5)
4.90
4.97
5.37
5.11
4.92
5.30
5.00
5.00
5.62
5.21

ˆD(10)
4.75
5.20
5.32
4.90
5.01
5.45
5.20
4.93
5.73
5.02

ˆD(20) QBP(5) QBP(10) QBP(20)
4.88
5.16
5.14
4.82
5.20
5.29
5.33
5.10
5.65
5.07

4.60
4.95
5.55
5.13
5.14
5.25
4.79
5.00
5.20
5.01

4.71
4.94
5.12
4.80
4.75
5.08
5.30
4.93
5.45
5.00

4.96
5.42
5.16
5.26
4.86
4.90
5.45
5.26
5.41
5.30

31

Table IV. Empirical powers (%) of ˆD and QBP for a 5% signifi-
cance test. ˆD and QBP tests for checking model adequacy of
twelve ARMA (2, 2) models in Table 3 of Peˇna and Rodriguez
(2002) fitted by AR (1) using the Burg algorithm. Both tests
were implemented based on the parametric bootstrap proce-
dure. The empirical power for each test was calculated based
on N = 104 simulations. Each Monte Carlo test also used
B = 103 simulations. Series length n = 100 and lags m = 5, 10, 20
were investigated.

Model
1
2
3
4
5
6
7
8
9
10
11
12

ˆD(5)
53.32
99.01
23.37
77.13
93.22
13.74
26.51
33.92
99.44
76.71
99.01
99.89

ˆD(10)
38.31
98.56
20.10
59.38
87.62
11.17
26.25
26.68
99.27
58.06
98.46
99.87

ˆD(20) QBP(5) QBP(10) QBP(20)
19.25
32.77
59.61
98.01
15.17
17.61
35.15
48.12
58.46
79.84
8.61
10.05
13.05
24.92
19.25
23.57
78.88
99.16
25.94
48.50
57.11
97.87
99.48
99.48

29.59
94.53
21.62
60.82
84.66
10.68
17.56
27.36
98.71
40.62
94.02
99.86

21.76
70.46
16.71
40.29
66.68
9.13
13.80
20.60
93.17
28.39
67.04
99.63

32

Table V. An illustrated example using the monthly simple re-
turn of CRSP value-weighted index data from Tsay (2002). The
data were fitted by an AR(3) model and an AR(5) model. The
entries in the first two columns are the P-values of ˆD and
QBP in
4 based on the Monte-Carlo test; those in the third
§
column are the P-value of the portmanteau test of Box and
Pierce (1970) assuming a normal distribution, denoted by QN
BP.

AR (3)
QBP QN
BP
0.197
0.026
0.107
0.021
0.247
0.012

AR (5)
QBP QN
BP
0.998
0.055
0.345
0.045
0.438
0.024

ˆD
0.050
0.030
0.019

ˆD
0.064
0.052
0.024

m = 5
m = 10
m = 20

m = 5
m = 10
m = 20

33

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

9
.
0
=
a
h
p
a

l

9
.
0
=
a
h
p
a

l

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

9
.
0
=
a
h
p
a

l

0

200

400

600

800

1000

0

200

400

600

800

1000

Dhat of lag= 5 and n= 1000

Dhat of lag= 5 and n= 2000

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

9
.
0
=
a
h
p
a

l

0

500

1500

2500

0

1000

3000

5000

Dhat of lag= 5 and n= 5000

Dhat of lag= 5 and n= 10000

Figure 1: The slow convergence of the ˆD test to its asymptotic distribution.
Random sequences of series length n = 103, 2000, 5000, 104 were simulated
from S1.5(1, 0, 0). 250 simulations were used to retrieve empirical percentiles
of the ˆD test with m = 5. The 5, 10, 30, 50, 70, 90, 95, 97.5, 99 (%) empirical
quantiles were plotted as black circles and the corresponding asymptotic
distribution was also plotted as the dot line.

34

 
 
 
 
0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

9
.
0
=
a
h
p
a

l

9
.
0
=
a
h
p
a

l

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

9
.
0
=
a
h
p
a

l

0

500

1000

1500

0

200 400 600 800

1200

Box−Pierce test of lag= 5 and n= 1000

Box−Pierce test of lag= 5 and n= 2000

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

9
.
0
=
a
h
p
a

l

0

1000

2000

3000

4000

0

2000

6000

10000

Box−Pierce test of lag= 5 and n= 5000

Box−Pierce test of lag= 5 and n= 10000

Figure 2: The slow convergence of the QBP test to its asymptotic distri-
bution. Random sequences of series length n = 103, 2000, 5000, 104 were
simulated from S1.5(1, 0, 0). 250 simulations were used to retrieve empiri-
cal percentiles of the QBP test with m = 5. The 5, 10, 30, 50, 70, 90, 95,
97.5, 99 (%) empirical quantiles were plotted as circles and the corresponding
asymptotic distribution was also plotted as the dot line.

35

 
 
 
 
0
1

.

8
0

.

6
.
0

4
.
0

2
.
0

F
D
C
E

0
1

.

8
0

.

6
.
0

4
.
0

2
.
0

F
D
C
E

0
1

.

8
0

.

6
.
0

4
.
0

2
.
0

F
D
C
E

−0.5

0.0

0.5

1.0

−1.0

0.0

1.0

2.0

−1

0

1

2

3

Racf at lag 1 with alpha= 1.2 and n= 100

Racf at lag 1 with alpha= 1.2 and n= 500

Racf at lag 1 with alpha= 1.2 and n= 10000

Figure 3: The slow convergence of residual autocorrelation to its asymptotic
distribution. AR (1) process, Xt = 0.5Xt−1 +Zt, of series length n = 100, 500,
104 were simulated respectively, where
is distributed as Z1.2(1, 0, 0). The
{
number of simulation NSIM = 104 were used. AR (1) models were then ﬁt-
ted to simulated data and residual autocorrelation at lag one was calculated.
The 5, 10, 30, 50, 70, 90, 95, 97.5, 99 (%) empirical quantiles of residual
autocorrelation at lag one were plotted as circles. The corresponding asymp-
totic distribution was plotted as the dot line. The asymptotic distribution of
sample autocorrelation was plotted as the real line.

Zt}

36

