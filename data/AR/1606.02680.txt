First Result on Arabic Neural Machine Translation

Amjad Almahairi
MILA, Universit´e de Montr´eal
amjad.almahairi@umontreal.ca

Kyunghyun Cho
New York University
kyunghyun.cho@nyu.edu

Nizar Habash
New York University
nizar.habash@nyu.edu

Aaron Courville
MILA, Universit´e de Montr´eal
aaron.courville@umontreal.ca

6
1
0
2

n
u
J

8

]
L
C
.
s
c
[

1
v
0
8
6
2
0
.
6
0
6
1
:
v
i
X
r
a

Abstract

Neural machine translation has become a ma-
jor alternative to widely used phrase-based
statistical machine translation. We notice
however that much of research on neural ma-
chine translation has focused on European
languages despite its language agnostic na-
ture.
In this paper, we apply neural ma-
chine translation to the task of Arabic transla-
tion (Ar↔En) and compare it against a stan-
dard phrase-based translation system. We run
extensive comparison using various conﬁg-
urations in preprocessing Arabic script and
show that the phrase-based and neural trans-
lation systems perform comparably to each
other and that proper preprocessing of Arabic
script has a similar effect on both of the sys-
tems. We however observe that the neural ma-
chine translation signiﬁcantly outperform the
phrase-based system on an out-of-domain test
set, making it attractive for real-world deploy-
ment.

1

Introduction

Neural machine translation (Kalchbrenner and Blun-
som, 2013; Sutskever et al., 2014; Cho et al., 2014)
has become a major alternative to the widely used
statistical phrase-based translation system (Koehn et
al., 2003), evidenced by the successful entries in
WMT’15 and WMT’16.

Previous work on using neural networks for Ara-
bic translation has mainly focused on using neural
networks to induce an additional feature for phrase-
based statistical machine translation systems (see,
e.g., (Devlin et al., 2014; Setiawan et al., 2015)).
This hybrid approach has resulted in impressive im-
provement over other systems without any neural

network, which raises a hope that a fully neural
translation system may achieve a even higher trans-
lation quality. We however found no prior work on
applying a fully neural translation system (i.e., neu-
ral machine translation) to Arabic translation.

In this paper, our aim is therefore to present
the ﬁrst result on the Arabic translation using
neural machine translation. On both directions
(Ar→En and En→Ar), we extensively compare a
vanilla attention-based neural machine translation
system (Bahdanau et al., 2015) against a vanilla
phrase-based system (Moses, (Koehn et al., 2003)),
while varying pre-/post-processing routines,
in-
cluding morphology-aware tokenization and ortho-
graphic normalization, which were found to be cru-
cial in Arabic translation (Habash and Sadat, 2006;
Badr et al., 2008; El Kholy and Habash, 2012).

The experiment reveals that neural machine trans-
lation performs comparably to the standard phrase-
based system. We further observe that the tokeniza-
tion and normalization routines, initially proposed
for phrase-based systems, equally improve the trans-
lation quality of neural machine translation. Finally,
on the En→Ar task, we ﬁnd the neural translation
system to be more robust to the domain shift com-
pared to the phrase-based system.

2 Neural Machine Translation

is

an

A major workforce behind neural machine trans-
lation
encoder-decoder
attention-based
model (Bahdanau et al., 2015; Cho et al., 2015).
encoder-decoder model
This
consists of an encoder, decoder and attention mech-
anism. The encoder, which is often implemented
as a bidirectional recurrent network, reads a source

attention-based

 
 
 
 
 
 
sentence X = (x1, . . . , xTx) and returns a set of
context vectors C = (h1, . . . , hTx).

The decoder is a recurrent language model. At

each time t(cid:48), it computes the new hidden state by

zt(cid:48) = φ(zt(cid:48)−1, ˜yt(cid:48)−1, ct(cid:48)),

where φ is a recurrent activation function, and zt(cid:48)−1
and ˜yt(cid:48)−1 are the previous hidden state and previ-
ously decoded target word respectively. ct(cid:48)
is a
time-dependent context vector and is a weighted
sum of the context vectors returned by the en-
coder: ct(cid:48) = (cid:80)Tx
t=1 αtht, where the attention weight
αt is computed by the attention mechanism fatt:
αt ∝ exp(fatt(zt(cid:48)−1, ˜yt(cid:48)−1, ht)). In this paper, we
use a feedforward network with a single tanh hid-
den layers to implement fatt.

Given a new decoder state zt(cid:48), the conditional dis-
tribution over the next target symbol is computed as

p(yt = w|˜y<t, X) ∝ exp(gw(zt(cid:48))),

where gw returns a score for the word w, and V is a
target vocabulary.

The entire model, including the encoder, decoder
and attention mechanism, is jointly tuned to max-
imize the conditional log-probability of a ground-
truth translation given a source sentence using a
training corpus of parallel sentence pairs. This learn-
ing process is efﬁciently done by stochastic gradient
descent with backpropagation.

Subword Symbols Sennrich et al. (2015), Chung
et al. (2016) and Luong and Manning (2016) showed
that the attention-based neural translation model can
perform well when source and target sentences are
represented as sequences of subword symbols such
as characters or frequent character n-grams. This
use of subword symbols elegantly addresses the is-
sue of large target vocabulary in neural networks
(Jean et al., 2014), and has become a de facto stan-
dard in neural machine translation. Therefore, in our
experiments, we use character n-grams selected by
byte pair encoding (Sennrich et al., 2015).

3 Processing of Arabic for Translation

3.1 Characteristics of Arabic Language

Arabic exhibits a rich morphology. This makes Ara-
bic challenging for natural language processing and

machine translation. For instance, a single Arabic

token ‘ (cid:233)(cid:16)(cid:74)(cid:74)(cid:46) (cid:187)(cid:81)(cid:214)(cid:207)(cid:240)’ (‘and to his vehicle’ in English) is
formed by prepending ‘(cid:240)’ (‘and’) and ‘(cid:5)(cid:203)’ (‘to’) to
(cid:16)(cid:233)(cid:74)(cid:46) (cid:187)(cid:81)(cid:211)’ (‘vehicle’), appending ‘ (cid:232)’
the base lexeme ‘
(cid:16)(cid:232) ’ (ta
(cid:16)(cid:72) ’. This fea-

(‘his’) and replacing the feminine sufﬁx ‘
marbuta) of the base lexeme to ‘
ture of Arabic is challenging, as (1) it increases the
number of out-of-vocabulary tokens, (2) it conse-
quently worsens the issue of data sparsity 1, and
(3) it complicates the word-level correspondence be-
tween Arabic and another language in translation.
This is often worsened by the orthographic ambigu-
ity found in Arabic scripts, such as the inconsistency
in spelling certain letters.

Previous work has thus proposed morphology-
aware tokenization and orthographic normalization
as two crucial components for building a high qual-
ity phrase-based machine translation system (or its
variants) for Arabic (Habash and Sadat, 2006; Badr
et al., 2008; El Kholy and Habash, 2012). These
techniques have been found very effective in alle-
viating the issue of data sparsity and improving the
generalization to tokens not included in a training
corpus (in their original forms.)

3.2 Morphology-Aware Tokenization

The goal of morphology-aware tokenization, or mor-
pheme segmentation (Creutz and Lagus, 2005) is
to split a word in its surface form into a sequence
of linguistically sound sub-units. Contrary to sim-
ple string-based tokenization methods, morphology-
aware tokenization relies on linguistic knowledge of
a target language (Arabic in our case) and applies,
for instance, various morphological or orthographic
adjustments to the resulting sub-units.

In this paper, we investigate the tokenization
scheme used in the Penn Arabic Treebank (ATB,
(Maamouri et al., 2004)) which was found to
work well with phrase-based translation system in
(El Kholy and Habash, 2012). This tokenization
separates all clitics other than deﬁnite articles.

When translating to Arabic, the decoded sequence
of tokenized symbols must be de-tokenized. This de-
tokenization step is not trivial, as it needs to undo
any adjustment (implicitly) made by the tokeniza-
tion algorithm. In this work, we follow the approach

1see Sec. 5.2.1 of (Cho, 2015) for detailed discussion.

proposed in (Badr et al., 2008; Salameh et al., 2015).
This approach builds a lookup table from a train-
ing corpus and uses it for mapping a tokenized form
back to its original form. When the tokenized form
is missing in the lookup table, we back off to a num-
ber of hand-crafted de-tokenization rules.

3.3 Orthographic Normalization

Since the sources of major orthographic ambiguity
are in the letters ‘alif’ and ‘ya’, we normalize these
letters (and their inconsistent replacements.) Fur-
thermore, we replace parentheses ‘(’ and ‘)’ with
special tokens ‘–LRB–’ and ‘–RRB–’, and remove
diacritics.

4 Experimental Settings

4.1 Data Preparation

Training Corpus We combine LDC2004T18,
LDC2004T17 and LDC2007T08 to form a training
parallel corpus. The combined corpus contains ap-
proximately 1.2M sentence pairs, with 33m tokens
on the Arabic side. Most of the sentences are from
news articles. We ignore sentence pairs which either
side has more than 100 tokens.

In-Domain Evaluation Sets We use the eval-
uation sets from NIST 2004 (MT04) and 2005
(MT05) as development and test sets respectively.
In Ar→En, we use all four English reference trans-
lations to measure the translation quality. We use
only the ﬁrst English sentence out of four as a source
during En→Ar. Both of these sets are derived from
news articles, just as the training corpus is.

Out-of-Domain Evaluation Set
In the case of
En→Ar, we evaluate both phrase-based and neural
translation systems on MEDAR evaluation set (Ha-
mon and Choukri, 2011). This set has four Ara-
It is derived
bic references per English sentence.
from web pages discussing climate changes, signif-
icantly differing from the training corpus. This set
was selected to highlight the robustness to domain
mismatch between training and test sets.

We verify the domain mismatches of the evalua-
tion sets relative to the training corpus by ﬁtting a
5-gram language model on the training corpus and
computing the likelihoods of the evaluation sets, on
the Arabic side. As can be seen in Table 1, the do-
main of the MEDAR is signiﬁcantly further away

MT04 MT05 MEDAR

Avg. Log-Prob.

-59.74
Table 1: Language model scores of the Arabic side. The lan-
guage model was tuned on the training corpus.

-55.97

-75.03

from the training corpus than the others are.

Note on MT04 and MT05 We noticed that a sig-
niﬁcant portion of Arabic sentences in MT04 and
MT05 are found verbatim in the training corpus (172
on MT04 and 26 on MT05).
In order to accu-
rately measure the generalization performance, we
removed those duplicates from the evaluation sets.

4.2 Machine Translation Systems

Phrase-based Machine Translation We use
Moses (Koehn et al., 2007) to build a standard
phrase-based statistical machine translation system.
Word alignment was extracted by GIZA++ (Och
and Ney, 2003), and we used phrases up to 8 words
to build a phrase table. We use the following op-
tions for alignment symmetrization and reordering
model: grow-diag-ﬁnal-and and msd-bidirectional-
fe. KenLM (Heaﬁeld et al., 2013) is used as a
language model and trained on the target side of the
training corpus.

Neural machine translation We use a publicly
available implementation of attention-based neural
machine translation.2 For both directions–En→Ar
and Ar→En–, the encoder is a bidirectional recur-
rent network with two layers of 512×2 gated recur-
rent units (GRU, (Cho et al., 2014)), and the decoder
a unidirectional recurrent network with 512 GRU’s.
Each model is trained for approximately seven days
using Adadelta (Zeiler, 2012) until the cost on the
development set stops improving. We regularize
each model by applying dropout (Srivastava et al.,
2014) to the output layer and penalizing the L2 norm
of the parameters (coefﬁcient 10−4). We use beam
search with width set to 12 for decoding.

4.3 Normalization and Tokenization

Arabic We test simple tokenization (Tok) based
on the script from Moses, and orthographic normal-
ization (Norm), and morphology-aware tokeniza-
tion (ATB) using MADAMIRA (Pasha et al., 2014),
. In the latter scenario, we reverse the tokenization
before computing BLEU. Note that ATB includes

2 https://github.com/nyu-dl/dl4mt-tutorial

Arabic

English

En→Ar

Tok. Norm. ATB Tok. Lower
√
√
√
√
√
√

√
√
√
√
√
√

√
√
√
√

√
√

√

√

√

MT05

MEDAR

31.52
33.03
34.98
35.63
35.7
35.98

–
(1.51)
(3.46)
(4.11)
(4.18)
(4.46)

8.69
9.78
17.34
17.75
18.67
19.27

–
(1.09)
(8.65)
(9.06)
(9.98)
(10.58)

Ar→En
MT05

48.59
49.44
49.51
49.91
50.67
51.19

–
(0.85)
(0.92)
(1.32)
(2.08)
(2.60)

√
√
√
√
√
√

√

28.64
29.77
32.53
32.95
33.53
33.62
Table 2: BLEU scores with the improvement over the tokenization-only models in the parentheses.

–
(-0.94)
(11.27)
(11.70)
(12.02)
(13.37)

–
(1.13)
(3.89)
(4.31)
(4.89)
(4.98)

47.12
47.63
48.53
47.53
49.21
49.7

11.09
10.15
22.36
22.79
23.11
24.46

√
√
√
√

√
√

√

√

–
(0.51)
(1.41)
(0.41)
(2.09)
(2.58)

√
√
√
√
√
√

T
M
S
-
B
P

T
M

l
a
r
u
e
N

Norm, and both of them include simple tokeniza-
tion performed by MADAMIRA.

English We test simple tokenization (Tok), lower-
casing (Lower) for En→Ar and truecasing (True,
(Lita et al., 2003)) for Ar→En.

Byte pair encoding As mentioned earlier
in
Sec. 2, we use byte pair encoding (BPE) for neural
machine translation. We apply BPE to the already-
tokenized training corpus to extract a vocabulary of
up to 20k subword symbols. We use the publicly
available script released by Sennrich et al. (2015).

5 Result and Analysis

En→Ar From Table 2, we observe that the transla-
tion quality improves as a better preprocessing rou-
tine is used. By using the normalization as well as
morphology-aware tokenization (Tok+Norm+ATB),
the phrase-based and neural systems each achieve as
much as +4.46 and +4.98 BLEU over the baselines,
on MT05. The improvement is even more appar-
ent on the MEDAR whose domain deviates from the
training corpus, conﬁrming that proper preprocess-
ing of Arabic script indeed helps in handling word
tokens that are not present in a training corpus.

We notice that the tested tokenization strategies
have nearly identical effect on both the phrase-
based and neural translation systems. The transla-
tion quality of either system is mostly effective by
the tokenization strategy employed for Arabic, and
is largely insensitive to whether source sentences
in English were lowercased. This reﬂects well the
complexity of Arabic scripts, compared to English,
discussed earlier in Sec. 3.1.

Another important observation is that the neu-
ral translation system signiﬁcantly outperforms the
phrase-based one on the out-of-domain evaluation
set (MEDAR), while they perform comparably to
each other in the case of the in-domain evaluation set
(MT05). We conjecture that this is due to the neural
translation system’s superior generalization capabil-
ity based on its use of continuous space representa-
tions.
Ar→En In the last column of Table 2, we ob-
serve a trend similar to that from En→Ar. First,
both phrase-based and neural machine translation
beneﬁt quite signiﬁcantly from properly normal-
izing and tokenizing Arabic, while they are both
less sensitive to truecasing English.
The best
translation quality using either model was achieved
when all the tokenization methods were applied
(Ar: Tok+Norm+ATB and En:Tok+True), improv-
ing upon the baseline by more than 2+ BLEU. Fur-
thermore, we again observe that the phrase-based
and neural translation systems perform comparably
to each other.

6 Conclusion

We have provided ﬁrst results on Arabic neural MT,
and performed extensive experiments comparing it
with a standard phrase-based system. We have con-
cluded that neural MT beneﬁts from morphology-
based tokenization and is robust to domain change.

References

[Badr et al.2008] Ibrahim Badr, Rabih Zbib, and James
Glass. 2008. Segmentation for english-to-arabic sta-

In Proceedings of the
tistical machine translation.
46th Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technologies:
Short Papers, pages 153–156. Association for Compu-
tational Linguistics.

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural machine
translation by jointly learning to align and translate.
In ICLR 2015.

[Cho et al.2014] Kyunghyun Cho, Bart Van Merri¨enboer,
Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. arXiv:1406.1078.
[Cho et al.2015] Kyunghyun Cho, Aaron Courville, and
Yoshua Bengio. 2015. Describing multimedia con-
tent using attention-based encoder-decoder networks.
IEEE Transactions on, 17(11):1875–
Multimedia,
1886.

[Cho2015] Kyunghyun Cho.

lan-
guage understanding with distributed representation.
arXiv:1511.07916.

Natural

2015.

[Chung et al.2016] Junyoung Chung, Kyunghyun Cho,
and Yoshua Bengio.
2016. A character-level de-
coder without explicit segmentation for neural ma-
chine translation. In ACL.

[Creutz and Lagus2005] Mathias Creutz and Krista La-
gus. 2005. Unsupervised morpheme segmentation
and morphology induction from text corpora using
Morfessor 1.0. Helsinki University of Technology.

[Devlin et al.2014] Jacob

Devlin,

Zbib,
Zhongqiang Huang, Thomas Lamar, Richard M
Schwartz, and John Makhoul. 2014. Fast and robust
neural network joint models for statistical machine
translation. In ACL.

Rabih

[El Kholy and Habash2012] Ahmed El Kholy and Nizar
Habash. 2012. Orthographic and morphological pro-
cessing for english–arabic statistical machine transla-
tion. Machine Translation, 26(1-2):25–45.

[Habash and Sadat2006] Nizar Habash and Fatiha Sadat.
2006. Arabic preprocessing schemes for statistical
machine translation.

[Hamon and Choukri2011] Olivier Hamon and Khalid
Choukri. 2011. Evaluation methodology and results
for english-to-arabic mt. Proceedings of MT Summit
XIII, pages 480–487.
[Heaﬁeld et al.2013] Kenneth

Ivan
Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn.
2013. Scalable modiﬁed Kneser-Ney language model
estimation. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics,
pages 690–696, Soﬁa, Bulgaria, August.

Heaﬁeld,

[Jean et al.2014] S´ebastien

Jean, Kyunghyun Cho,
Roland Memisevic, and Yoshua Bengio. 2014. On

using very large target vocabulary for neural machine
translation. In ACL 2015.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner and
Phil Blunsom. 2013. Recurrent continuous translation
models. In EMNLP, pages 1700–1709.

In Proceedings of

[Koehn et al.2003] Philipp Koehn, Franz Josef Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
the 2003 Conference
lation.
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 48–54. Association for
Computational Linguistics.

[Koehn et al.2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, et al. 2007. Moses: Open
source toolkit for statistical machine translation.
In
Proceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguistics.
[Lita et al.2003] Lucian Vlad Lita, Abe Ittycheriah, Salim
Roukos, and Nanda Kambhatla. 2003. Truecasing.
In Proceedings of the 41st Annual Meeting on Associ-
ation for Computational Linguistics-Volume 1, pages
152–159. Association for Computational Linguistics.
and
Christopher D Manning.
2016. Achieving open
vocabulary neural machine translation with hybrid
word-character models. arXiv:1604.00788.

[Luong and Manning2016] Minh-Thang

Luong

[Maamouri et al.2004] Mohamed Maamouri, Ann Bies,
Tim Buckwalter, and Wigdan Mekki. 2004. The penn
arabic treebank: Building a large-scale annotated ara-
In NEMLAR conference on Arabic lan-
bic corpus.
guage resources and tools, volume 27, pages 466–467.
[Och and Ney2003] Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statis-
tical alignment models. Computational linguistics,
29(1):19–51.

[Pasha et al.2014] Arfath

Pasha,

Mohamed

Al-
Badrashiny, Mona T Diab, Ahmed El Kholy,
Ramy Eskander, Nizar Habash, Manoj Pooleery,
Owen Rambow, and Ryan Roth. 2014. Madamira: A
fast, comprehensive tool for morphological analysis
In LREC, pages
and disambiguation of arabic.
1094–1101.

[Salameh et al.2015] Mohammad Salameh, Colin Cherry,
and Grzegorz Kondrak. 2015. What matters most
in morphologically segmented smt models. Syntax,
Semantics and Structure in Statistical Translation,
page 65.

[Sennrich et al.2015] Rico Sennrich, Barry Haddow, and
Alexandra Birch. 2015. Neural machine translation
arXiv preprint
of rare words with subword units.
arXiv:1508.07909.

[Setiawan et al.2015] Hendra

Setiawan,

Zhongqiang
Huang, Jacob Devlin, Thomas Lamar, Rabih Zbib,
Richard Schwartz, and John Makhoul. 2015. Statisti-
cal machine translation features with multitask tensor
networks. arXiv:1506.00698.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-
ton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to pre-
vent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958.
[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In NIPS, pages 3104–3112.

[Zeiler2012] Matthew D Zeiler.

2012.

an adaptive learning rate method.
arXiv:1212.5701.

Adadelta:
arXiv preprint

