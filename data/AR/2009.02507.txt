Learning AR factor models

Francesca Crescente, Lucia Falconi, Federica Rozzi, Augusto Ferrante and Mattia Zorzi

0
2
0
2

p
e
S
5

]

C
O
.
h
t
a
m

[

1
v
7
0
5
2
0
.
9
0
0
2
:
v
i
X
r
a

Abstract— We face the factor analysis problem using a
particular class of auto-regressive processes. We propose an ap-
proximate moment matching approach to estimate the number
of factors as well as the parameters of the model. This algorithm
alternates a step of factor analysis and a step of AR dynamics
estimation. Some simulation studies show the effectiveness of
the proposed estimator.

I. INTRODUCTION

Factor models are among the ﬁrst instances where sta-
tistical tools have proved their power in providing sensible
representations of a data collection: the ﬁrst contributions in
this ﬁeld date back to more than a century ago, [6], [22]. In
its simplest form the problem, aimed at extracting statistical
commonalities in multivariate data, may be reformulated as
that of decomposing a positive deﬁnite covariance matrix Σ
as the sum

Σ = L + D

(1)

where both L and D are positive semideﬁnite, D is a
diagonal matrix, and L has the lowest possibile rank. It turns
out that, in general, this is a formidable problem for which
a rich stream of literature has been produced. We refer the
reader to the recent papers [3], [4], [7], [10], [20] where
different principles for ﬁnding the decomposition in (1) have
been proposed and the papers [5], [12] where generalized
formulations of the problem have been considered. From the
basic problem (1), countless variations have been considered
and studied. In particular, considerable effort has been de-
voted to the dynamic case, see [1], [2], [18], [19], [21] and
the references therein.

To practically compute a decomposition of type (1) where
the rank of L is small, the typical strategy is to minimize a
proxy of the rank of L, i.e. the trace norm of L, [14], [16],
[17]. Trace norm regularization is also used in the so called
latent-variable auto-regressive (AR) graphical models, [8],
[11], [24], [26] where we learn the spectral density of the
model such that its inverse admits a “sparse plus low-rank
decomposition”. It is worth noting that data enters in these
estimators through an approximate moments matching, in a
similar spirit of [15]. This is a wise way to use moments.
Indeed, in practice they are estimated from data and thus an
estimation error is inevitable and must be taken into account.

Falconi,

the Department

F. Rozzi, A.
of

F. Crescente, L.
are with

and M.
Engineering,
Italy.
francesca.crescente@studenti.unipd.it,

Zorzi
University of Padova, Via Gradenigo 6/B, 35131 Padova,
Emails:
lucia.falconi@studenti.unipd.it,
federica.rozzi@studenti.unipd.it,
augusto@dei.unipd.it, zorzimat@dei.unipd.it

Information

Ferrante

The natural dynamic extension to (1) is to consider Σ, L
and D as spectral densities of stationary stochastic processes,
[13]. In [25], factor analysis for moving average processes
has been considered. The proposed estimator, however,
matches exactly the “noisy” moments. As a consequence, the
estimated decomposition is good provided that the number
of data points is sufﬁciently large.

In this paper, leveraging on the results in [9], [10], we
consider the problem of identiﬁcation of the parameters of
an AR model driven by a white noise whose covariance
matrix admits a decomposition of the form (1) with the
rank of L being much smaller than the dimension of Σ.
This is an interesting situation because it corresponds to
the case when independent observation noises affect each
channel of the observed AR process while a small num-
ber of common factors account for the information shared
among the observations. Our attention to AR processes is
motivated by the fact that they can approximate arbitrary
well any purely non deterministic process as long as the
order is sufﬁciently high. Our contribution is to propose an
approximate moments matching method for the identiﬁcation
of the parameters of these AR factor models. This method
is based on alternating a step of factor analysis (solved by
resorting to the minimum trace proxy [9]) and a step of
AR dynamics estimation by means of moments matching.
While this method mostly hinges on heuristic arguments, it
provides accurate estimations in high-dimensional instances:
some simulations are described at the end of the paper that
indeed show the performances of the method for AR process
with 40 and 100 channels.

The rest of the paper is organized as follows: Section II
describes the problem formulation. Section III introduces the
proposed algorithm. Numerical simulations and results are
presented in Section IV. Finally, Section V concludes the
paper.

Notation: In this section we summarize and describe both
the syntax and the semantics that will be used in the sequel.
Given a matrix M , M (cid:62) denotes the transpose; |M | and
tr(M ) denote its determinant and trace (for a square M ),
respectively. The symbol Qm denotes the space of real
symmetric matrices of size m. If M ∈ Qm is positive
deﬁnite or positive semi-deﬁnite, then we write M (cid:31) 0
or M (cid:23) 0, respectively. Moreover, we denote by Dm
the space of diagonal matrices of size m. The symbols
(cid:107) · (cid:107)F and (cid:107) · (cid:107) stand for the Frobenius norm and Euclidean
norm, respectively. With f (·) we denote the probability
density function of a given random variable. The shorthand
notation x ⊥ y means that the random vectors x and y are
independent. We deal with Gaussian multivariate processes

 
 
 
 
 
 
deﬁned over the integers Z.

II. PROBLEM FORMULATION
Consider, with an abuse of notation 1 , the auto-regressive

factor model:

y(t) = a(z)−1[WLv(t) + WDw(t)]

where

a(z) =

p
(cid:88)

k=0

akz−k,

ak ∈ R,

(2)

(3)

WL ∈ Rm×r, WD ∈ Dm is diagonal, and p is the order
t ∈ Z} and
of the model. The processes v = {v(t),
w = {w(t), t ∈ Z} are normalized white Gaussian noises
of dimension r and m respectively; moreover, for all t1, t2,
v(t1) ⊥ w(t2). The aforementioned model has the following
interpretation: v is the process which describes the r factors,
with r (cid:28) m, not accessible to observation; a−1WL is the
factor loading transfer matrix and a−1WLv(t) represents
the latent variable. a−1WDw(t) is the idiosyncratic noise
describing the independent noises affecting each channel.

Notice that

u(t) := a(z)y(t) = WLv(t) + WDw(t),

(4)

L + WDW (cid:62)

D = L + D, where L := WLW (cid:62)

is white Gaussian noise with covariance matrix given by
Σ := WLW (cid:62)
L and
D := WDW (cid:62)
D . We make the reasonable assumption that
Σ (cid:31) 0 (since in most practical cases the noise affects the
model in all the directions) so that there exists LΣ (cid:31) 0 such
that Σ = LΣL(cid:62)
Σ . Then, (4) may be written as

u(t) = a(z)y(t) = LΣe(t)

(5)

where e = { e(t), t ∈ Z } is an m-dimensional normalized
white noise. It follows that

¯y(t) := L−1

Σ y(t) = a−1(z)e(t)

(6)

is still an AR process of order p. The process ¯y(t) is obtained
by stacking together the output of m identical (scalar) ﬁlters
driven by independent (scalar) white noises. Therefore ¯y(t)
may be viewed as a multivariate process with m independent
channels ¯yi(t) all of which feature the same probability
description. This will be a key feature in what follows.

Assume now to collect a ﬁnite length realization of y,
say yN = { y1, . . . , yN }. Our aim is to estimate the
corresponding factor model (2) as well as the number of
factors r. The idea is to iteratively estimate L, D and a(z) by
pre-processing yN through a and L−1
Σ , respectively. Finally,
it is crucial to observe that there is an identiﬁability issue in
this problem. Indeed, if we multiply a(z) by an arbitrary non-
zero real number k and L and D by k2, the model remains
the same. We can easily eliminate this uninteresting degree
of freedom by normalizing the polynomial a(z) so that from
now on we assume that a0 = 1.

1 For the sake of simplicity, we mix both the time and the z-domain

representation.

III. PROBLEM’S SOLUTION

Our solution approach is based on an iterative algorithm
that recursively estimates a(z), L, D and r, until a certain
tolerance is achieved. To easily explain our method, we
ﬁrstly suppose r is ﬁxed. With such hypothesis, the proposed
solution is presented in Algorithm 1. It receives as input the
data yN , the order p of a(z), the number r of factors, and
the error tolerance ε. Given these inputs, it alternates the two
following steps:

1) the static factor analysis, estimating the matrices L

and D;

2) the AR dynamics estimation, estimating the vector a :=
[a1 a2 . . . ap](cid:62) of the p parameters of the polynomial
a(z) (recall that we have set a0 = 1).

These quantities are updated until the difference between two
consecutive estimated values of L, D and a becomes smaller
than a chosen threshold; more precisely, we impose that the
mean square difference between the identiﬁcation parameters
in two successive steps is smaller than a given constant ε:
(cid:107)D − Dold(cid:107)2
F
m
where the subscript old denotes the estimates of the previous
iteration. In addition, to ensure termination of the algorithm,
we impose a maximum number lmax of iterations.

(cid:107)L − Lold(cid:107)2
F
m2

(cid:107)a − aold(cid:107)
p

e :=

≤ ε

(7)

+

+

The two steps are explained hereafter, while the estimation

of r is addressed in Section III-C.

Algorithm 1 Dynamic AR Factor Analysis
Input yN , p, r, ε, lmax
r , ˆΣr
r, L◦
Output: a◦

r, D◦

1: Initialize a, L, D, l = 1
2: repeat
3:
4:
5:

aold = a, Lold = L, Dold = D
Data ﬁltering uN = ayN
Compute the sample covariance ˆΣ from uN
Compute L, D s.t. rank(L) ≤ r, D ∈ Dm and
(cid:107) ˆΣ − (L + D)(cid:107)F is minimized
Compute LΣ s.t. L + D = LΣL(cid:62)
Σ
Data “disentangle” ¯yN = L−1
Σ yN
Estimate the AR coefﬁcients in a from ¯yN
Compute e := (cid:107)L−Lold(cid:107)2
l = l + 1

+ (cid:107)D−Dold(cid:107)2

m2

m

F

F

6:

7:
8:
9:

10:
11:
12: until e ≤ ε or l ≥ lmax
r = L, D◦
r = a, L◦
a◦
13:

r = D, ˆΣr = ˆΣ.

+ (cid:107)a−aold(cid:107)
p

Some comments are in order.
1) Formula uN = ayN in line 4 of Algorithm 1 has to be
understood as follows: consider the moving average
ﬁlter a(z) := 1 + [z−1 . . . z−p]a, uN is the ﬁnite
length trajectory obtained by passing through the ﬁlter
a(z) the ﬁnite length trajectory yN with zero initial
conditions. Similarly for formula in line 8: ¯yN is
the ﬁnite length trajectory obtained by multiplying on
the left side by L−1
Σ each vector of the ﬁnite length
trajectory yN .

2) The aforementioned pre-processing steps are adaptive,
indeed, at each iteration these operations changes ac-
cording to the current a and LΣ, respectively.

3) Lines 6 and 9 of Algorithm 1 correspond to other al-
gorithms that are detailed in the following subsections.

A. Static factor analysis

The static factor analysis problem is stated in [9] as
follows: for a given rank r and a given matrix Σ we want
to ﬁnd a positive semideﬁnite matrix L with rank at most r
and a positive semideﬁnite diagonal matrix D such that their
sum is as close as possible to Σ. This can be formalized as:

(L∗, D∗) := arg min

L∈Lm,r,D∈Dm

(cid:107)Σ − L − D(cid:107)2
F

(8)

where Lm,r := {X ∈ Qm : X (cid:23) 0, rank(X) ≤ r} and
Dm := {X ∈ Dm : X (cid:23) 0}. To efﬁciently solve this
problem we resort to Algorithm 2 that was ﬁrst proposed and
analyzed in [9]. It receives as input the current matrix Σ to be
decomposed and the current value of the rank r, together with
the error threshold εs. The estimation procedure is based on
a coordinate descent type iterative algorithm. Such algorithm
iterates between solving a minimization problem with respect
to L and a minimization problem with respect to D:

where the negative log-likelihood (cid:96)(¯y, a) is deﬁned as

(cid:96)(¯yN ; a) := − log f

(cid:16)

¯y(N ), .., ¯y(p + 1)|¯y(p), .., ¯y(1)

(cid:17)
.

In other words, we estimate the parameters vector a in such
a way that the model

¯y(t) = a(z)−1e(t),

(10)

maximizes the likelihood of producing the ﬁnite trajectory
¯yN .

Firstly, we consider the scalar case m = 1. Since we are
dealing with an AR model the solution can be obtained by
standard arguments in closed form. In fact, by taking (3) into
account, we can rewrite (10) as (cid:80)p
k=0 ak ¯y(t − k) = e(t),
so that ¯y(t) = − (cid:80)p

k=1 ak ¯y(t − k) + e(t). Therefore,
(cid:19)
(cid:18)

f (cid:0)¯y(t)|¯y(t − 1) . . . ¯y(t − p)(cid:1) ∼ N

− [ 0 a(cid:62) ]Y (t), 1

(11)

and
f (cid:0)¯y(N ) . . . ¯y(p + 1)|¯y(p) . . . ¯y(1)(cid:1) =

N
(cid:89)

t=p+1

f (cid:0)¯y(t)|¯y(t − 1) . . . ¯y(t − p)(cid:1)

(12)

L = arg min
L∈Lm,r

(cid:107)Σ−L−Dold(cid:107)2

F , D = arg min
D∈Dm

(cid:107)Σ−L−D(cid:107)2
F

where Y (t) = [ ¯y(t) ¯y(t − 1) . . . y(t − p) ](cid:62). Then, the
negative log-likelihood (up to constant terms) results

where Dold denotes the value of the diagonal matrix at the
previous iteration. Notice that PLm,r and PDm in Algorithm
2 are the projector onto the sets Lm,r and Dm respectively.
These projectors can be implemented very efﬁciently and
robustly even for matrices Σ with several hundreds of rows
and columns.
The terminating condition is reached when (cid:107)Σ − L −
D(cid:107)2

F ≤ εs is satisﬁed.

F /(cid:107)Σ(cid:107)2

(cid:96)(¯yN ; a) =

=

=

1
2

1
2

1
2

N
(cid:88)

t=p+1

(cid:0)[ 1 a(cid:62) ]Y (t)(cid:1)2

N
(cid:88)

(cid:18)

tr

t=p+1

[1 a(cid:62)]Y (t)Y (t)(cid:62)

[1 a(cid:62)]

(cid:34) N
(cid:88)

t=p+1

Y (t)Y (t)(cid:62)

(cid:21) (cid:19)

(cid:20)1
a
(cid:35) (cid:20)1
(cid:21)
a

Algorithm 2 Static Factor Analysis
Input: Σ, r, εs, imax
Output: L∗, D∗

F > εs and i < imax do

1: Initialize D randomly, i = 0
F /(cid:107)Σ(cid:107)2
2: while (cid:107)Σ − L − D(cid:107)2
L = PLm,r (Σ − D)
3:
4: D = PDm(Σ − L)
5:
6: end while
7: L∗ = L, D∗ = D

i = i + 1

B. AR dynamics estimation

The second step is the AR dynamics estimation. Given
a ﬁnite-length realization ¯yN = {¯y(1) . . . ¯y(N )} of the
AR process (6), the aim is to estimate the coefﬁcients of the
ﬁlter a(z), namely a = [a1 · · · ap](cid:62) ∈ Rp (as we have ﬁxed
a0 = 1). To this aim we resort to the maximum-likelihood
(ML) principle and compute the estimate as

We now deﬁne the matrix

ˆT :=

1
N − p

N
(cid:88)

t=p+1

Y (t)Y (t)(cid:62) =

(cid:20)˜τ0
z

(cid:21)

z(cid:62)
ˆT22

where the partition is such that ˜τ0 is a scalar and z is a
column vector. In this way, we have

(cid:96)(¯yN ; a) =

N − p
2

[1 a(cid:62)] ˆT

(cid:21)
(cid:20)1
a

=

1
2

(cid:104)
˜τ0 + 2z(cid:62)a + a(cid:62) ˆT22a

(cid:105)

Since (cid:96)(¯yN ; a) is clearly convex in a, Problem (9) is solved
by annihilating the gradient of (cid:96)(¯yN ; a) with respect to a, i.e.
by imposing that

∂(cid:96)(¯yN ; a)
∂a

= 2zT + 2 ˆT22a = 0.

(13)

Since (cid:96)(¯yN ; a) is a quadratic form in a, (13) provides a
closed form formula which ﬁnally yields

aM L = − ˆT −1

22 z.

(14)

aM L := arg min

a

(cid:96)(¯yN ; a)

(9)

Of course, the interesting case is the multivariate one i.e.
m > 1. To address this case, we recall that the m components

¯yi(t) of the vector process ¯y(t) are independent scalar
processes i.e. ¯yk(t) ⊥ ¯yl(s),
∀k (cid:54)= l, ∀s, t, and they all
have the same probabilistic description i.e. all the ¯yk(t)’s
have the same spectral density

Φ¯yk (z) =

1
a(z)a(z−1)

,

∀i = 1, . . . , m.

(15)

The multivariate case can therefore be addressed as that
of a scalar process with m-times as many data. In fact, in
view of the independence of the components of ¯y(t), the
likelihood is

f (¯y(N ) . . . ¯y(p + 1)|¯y(p) . . . ¯y(1)) =

m
(cid:89)

k=1

f (¯yk(N ) . . . ¯yk(p + 1)|¯yk(p) . . . ¯yk(1)) .

in view of (15), we can repeat for each k
Moreover,
the argument that led to (12) to obtain an expression for
f (¯yk(N ) . . . ¯yk(p + 1)|¯yk(p) . . . ¯yk(1)). This yields

f (¯y(N ) . . . ¯y(p + 1)|¯y(p) . . . ¯y(1)) =

m
(cid:89)

N
(cid:89)

k=1

t=p+1

f (cid:0)¯yk(t)|¯yk(t − 1) . . . ¯yk(t − p)(cid:1).

We can now repeat the previous computation and obtain

(cid:96)(¯yN ; a) =

1
2

m
(cid:88)

k=1

[1 a(cid:62)]

(cid:34) N
(cid:88)

t=p+1

Yk(t)Yk(t)(cid:62)

(cid:21)

(cid:35) (cid:20)1
a

where Yk(t) := [ ¯yk(t) ¯yk(t − 1) . . . ¯yk(t − p) ]T . We now
deﬁne the matrix

ˆT :=

1
m(N − p)

m
(cid:88)

N
(cid:88)

k=1

t=p+1

Yk(t)Yk(t)(cid:62) =

(cid:20)˜τ0
z

(cid:21)

z(cid:62)
ˆT22

(16)

where, as for the scalar case, the partition is such that τ is a
scalar and z is a column vector. In this way, we are exactly in
the situation discussed for the scalar case and the solution is
thus given again by (14) with T and z now provided by (16).
Such a solution, however, is not guaranteed to correspond to
a stable model (i.e. a model such that all the zeros of a(z)
are inside the unit circle). Notice that ˆT is an estimate of the
Toeplitz matrix T = E[Yk(t)Yk(t)(cid:62)] (cid:31) 0. Although ˆT → T
almost surely as N → ∞, ˆT (cid:31) 0 it is not Toeplitz for ﬁnite
values of N . To address such an issue, we consider the biased
estimate

ˆTb =









τ0

τ1

τp

τ1

τ0
. . .

. . .
. . .
. . .
τ1









τp

τ1
τ0

=

(cid:20) τ0
zb

(cid:21)

z(cid:62)
b
ˆTb,22

(17)

(cid:80)m

(cid:80)N

where τl = 1
t=k+1 yk(t)yk(t − l), l = 0 . . . p.
mN
It is not difﬁcult to see that ˆTb (cid:31) 0 generically. Accordingly,
we can choose as estimate of a:

k=1

aM E = − ˆT −1

b,22zb.

(18)

It is worth noting that (18) is the solution to a Yule-Walker
equation [23]. Accordingly, aM E(z) = 1+[z−1 . . . z−p]aM E

is a stable polynomial. Hence, the estimated spectral density
of each ¯yk(t) is ΦM E(z) = (aM E(z)aM E(z−1))−1.

Proposition 3.1: Let ˇτl = τlv(cid:62) ˆT −1

b v with v =
[ 1 0 . . . 0 ](cid:62). Then, ΦM E is the unique solution to the
following maximum entropy problem:

ΦM E =argmax

(cid:90) π

log det Φ(ejϑ)dϑ

(19)

Φ

−π

(cid:90) π

ejϑlΦ(ejϑ)dϑ = ˇτl, l = 0 . . . p.

s.t.
Proof: Let ˇTb
b v ˆTb.
is well known (see
the solution to (19) is Φ(z) =
for example [23]) that
σ2(a(z)a(z−1))−1 with a(z) = 1 + [z−1 . . . z−p]a such that

−π
:= v(cid:62) ˆT −1

(20)

It

ˇTb

(cid:20) 1
a

(cid:21)

=

(cid:20) σ2
0

(cid:21)

.

(21)

Notice that in (21) is a system of p + 1 equations. Consider
the subsystem composed by the second equation up to the
last equation: since ˇTb is invertible, its solution is (18). It
remains to show that σ2 = 1. Substituting (18) in the ﬁrst
equation, we have

σ2 = v(cid:62) ˇTb[ 1 a(cid:62)
= (τ0 − z(cid:62)
b

M E ](cid:62) = (τ0 + z(cid:62)
b,22zb)(v(cid:62) ˆT −1
ˆT −1

b v) = 1

b aM E)(v(cid:62) ˆT −1

b v)

where the last equality is due by the fact that τ0 − z(cid:62)
b
is the Schur complement of the block ˆTb,22 of ˆTb. (cid:4)

ˆT −1
b,22zb

Algorithm 3 summarizes the AR estimation procedure.

Algorithm 3 AR dynamics Estimation
Input: p, ¯yN
Output: aM E
1: Let Yk(t) := [¯yk(t) . . . ¯yk(t − p)](cid:62), k = 1 . . . m.
2: Compute ˆTb and thus ˆTb,22, zb from Yk(t)
3: aM E = − ˆT −1

b,22zb

Remark 3.1: It is clear that a◦
r in Algorithm 1 matches
the rescaled moments of ¯yN = L−1
r + D◦
r
ˆΣr
ryN .
approximately matches the zeroth moment of uN = a◦
Accordingly, Algorithm 1 is an approximate moment match-
ing estimator for an AR factor model of type (2).

yN . Moreover, L◦

C. Estimation of r

r + D◦

r and D◦

r (where ˆΣr, L◦

As regards the estimation of the number r of factors in (2),
we propose the following procedure. We start from the most
parsimonious model with only a single factor and increase
the number of factors until the difference between ˆΣr and
the sum L◦
r are outputs of
Algorithm (1)) is sufﬁciently small to be explained by the
estimation error (due to ﬁniteness number of data) of the
sample covariance ˆΣr. More precisely, starting from r =
1, Algorithm 1 is iteratively applied, increasing r at each
step. The stopping criterion is deﬁned in the sequel. Let ˆΣr,
L◦
r be the achieved values using r. Consider the
Kullback-Leibler divergence between ˆΣr and Σ◦
r+D◦
r ,
deﬁned as:

r and D◦

r := L◦

DKL(Σ◦

r(cid:107) ˆΣr) :=

1
2

(− log |Σ◦

r|+log | ˆΣr|+tr(Σ◦

r

ˆΣ−1

r )−m).

(a)

(c)

(b)

(d)

Fig. 1: Case m = 40, r = 10 and p = 5 with N = 200, N = 500 and N = 800. Figure (a), (b) and (c) show the box-plot of the error
ea, eL and eD respectively; (d) is the bar-plot of estimated r◦.

Then, the value of r is increased until DKL(Σ◦
smaller than a given tolerance δ, i.e.

r(cid:107) ˆΣr) becomes

DKL(Σ◦

r(cid:107) ˆΣr) ≤ δ

(22)

and such value of r, denoted by r◦, is the estimate of the
rank. Clearly, the optimal model is given by a◦
r◦ and
D◦

r◦ .
It is worth noting that DKL(Σ◦

r(cid:107) ˆΣr) measures how well
the model with r factors explains the data. Accordingly, the
stop criterium in (22) selects the model with the best trade-
off, according to δ, between data adherence and complexity.
The latter is deﬁned as the number of factors.

r◦ , L◦

As regards the choice of δ, our solution hinges on the
following scale-invariance property of the Kullback-Leibler
divergence (see [10]):

Proposition 3.2: Let x(t) ∼ N (0, Σ) , t = 1, . . . , N be
i.i.d. random vectors taking values in Rm and let x(t) be a
realization of x(t). Deﬁne the sample covariance estimator as
ˆΣ := 1
t=1 x(t)x(cid:62)(t). The Kullback-Leibler divergence
N
between Σ and ˆΣ is a random variable whose distribution
depends only on the number N of random variables and on
the dimension m of each random variable. Namely

(cid:80)N

d := DKL(Σ(cid:107) ˆΣ) =

1
2

(log |QN | + tr(Q−1

N ) − m)

(23)

where QN is the random matrix deﬁned by QN :=
(cid:80)N
1
t=1 ˜x(t)˜x(t)(cid:62) with ˜x(t) being i.i.d. normalized Gaus-
N
sian random vectors: ˜x(t) ∼ N (0, Im).

In view of this result, we can empirically approximate the
distribution of the random variable d = DKL(Σ(cid:107) ˆΣr) by a
standard Monte Carlo method. In particular, after choosing a

probability α ∈ (0, 1) and N , we can ﬁnd the neighborhood
of radius δα (in the Kullback-Leibler topology) for which
Pr(d ≤ δα) = α .

IV. NUMERICAL SIMULATIONS

To provide empirical evidence of the estimation per-
formance of the algorithm, simulations studies have been
performed by using the software Matlab-R2019b.

We considered the case of a covariance matrix Σ, com-
puted as the sum of a randomly generated positive semi-
deﬁnite low-rank matrix L of dimension m and rank r, and
a randomly generated positive deﬁnite diagonal matrix D
such that (cid:107)D(cid:107)F ≈ (cid:107)L(cid:107)F , i.e. the idiosyncratic noise is
not negligible. Furthermore, we generated a(z) by randomly
choosing p stable poles; without loss of generality we ﬁxed
a0 = 1. Regarding the parameters of our procedure, we
set α = 0.99, ε = 0.03, lmax = 200, εs = 10−6 and
imax = 200. In what follows, we analyze the following
quantities:

• the relative error on a, ea := (cid:107)a − a◦
• the relative error on L, eL := (cid:107)L − L◦
• the relative error on D, eD := (cid:107)D − D◦

r◦ (cid:107)/(cid:107)a(cid:107).

r◦ (cid:107)F /(cid:107)L(cid:107)F ;

r◦ (cid:107)F /(cid:107)D(cid:107)F ;

First study: We performed 200 Monte Carlo runs with
m = 40, r = 10 and p = 5. Figure 1a, 1b and 1c show that
the proposed algorithm reaches good results since the errors
are on the order of 10−1 for L and D, and on the order of
10−3 for a. The original low-rank and diagonal matrices are
recovered with negligible numerical errors. The bar-plot of
Figure 1d shows good performances also in the estimation of
r. The worst case is achieved when N = 200: the efﬁciency

N = 200N = 500N = 80000.511.522.53x 10−3eaN = 200N = 500N = 80000.050.10.150.20.25eLN = 200N = 500N = 80000.10.20.30.40.50.60.70.80.91eD89101112131417020406080100120140160180200Fig. 2: Case m = 100, r = 15, p = 4 with N = 500. (a) Box-plot of the errors ea, eL and eD; (b) bar-plot of estimated r◦.

(a)

(b)

is just the 53% since in the 44% of the cases the rank is
underestimated as r◦ = 9. On the other hand, with N = 500
and N = 800 the efﬁciency reaches the 93% and 95.5%
respectively, with just some outliers.

Second study: We performed 200 Monte Carlo runs with
m = 100, r = 15 and p = 4. Hence, we considered a
high dimensional case. The errors are plotted in Figure 2a
and in Figure 2b the bar-plot of the estimated rank. The
estimates are very good. In particular the rank in the majority
of the cases is correctly estimated. A similar study with
essentially the same results has been conducted with the same
parameters except for the order p = 5 of the AR dynamics.
It is remarkable that in all the simulations the algorithm
stops before reaching the maximum number of iterations
lmax when ro approximates quite well the true value, as
in these cases condition (7) is satisﬁed.

V. CONCLUSIONS

In this paper we have considered the problem to estimate
an AR factor model. More precisely, we have proposed
an approximate moment matching procedure which alter-
nates a static factor analysis step and an AR identiﬁcation
step. Empirical results showed that the algorithm estimates
accurately the number of factors. This paradigm can be
generalized to the case in which the order of the AR process
is unknown. In such a scenario one could choose a criterium
with complexity term, such as BIC. It is clear this requires to
compare the candidate models over a two dimensional grid
(one dimension is r and the other one is p), as a consequence
the computational burden will be increased.

REFERENCES

[1] B. D. O. Anderson and M. Deistler. Identiﬁability in dynamic errors-
in-variables models. Journal of Time Series Analysis, 5(1):1–13, 1984.
Identiﬁcation of scalar errors-in-variables

[2] Brian D.O. Anderson.

models with dynamics. Automatica, 21(6):709 – 716, 1985.

[3] J. Bai, S. Ng, et al. Large dimensional factor analysis. Foundations

and Trends® in Econometrics, 3(2):89–163, 2008.

[4] D. Bertsimas, M. S Copenhaver, and R. Mazumder. Certiﬁably optimal
Journal of Machine Learning Research,

low rank factor analysis.
18(29):1–53, 2017.

[5] G. Bottegal and G. Picci. Modeling complex systems by generalized
factor analysis. IEEE Transactions on Automatic Control, 60(3):759–
774, March 2015.

[6] C. Burt. Experimental tests of general intelligence. British Journal of

Psychology, 1904-1920, 3(1/2):94–177, 1909.

[7] V. Ciccone, A. Ferrante, and M. Zorzi. Factor analysis with ﬁnite
data. In 2017 IEEE 56th Annual Conference on Decision and Control
(CDC), pages 4046–4051, Dec 2017.

[8] V. Ciccone, A. Ferrante, and M. Zorzi. Robust identiﬁcation of ‘sparse
plus low-rank’ graphical models: An optimization approach. In 2018
IEEE Conference on Decision and Control (CDC), pages 2241–2246,
2018.

[9] V. Ciccone, A. Ferrante, and M. Zorzi. An alternating minimization

algorithm for factor analysis. Kybernetika, (4):740–754, 2019.
[10] V. Ciccone, A. Ferrante, and M. Zorzi. Factor models with real data:
A robust estimation of the number of factors. IEEE Transactions on
Automatic Control, 64(6):2412–2425, June 2019.

[11] V. Ciccone, A. Ferrante, and M. Zorzi. Learning latent variable
dynamic graphical models by conﬁdence sets selection. IEEE Trans.
Autom. Control (accepted), 2020.

[12] M. Deistler, W. Scherer, and B. Anderson. The structure of generalized
linear dynamic factor models. In Empirical Economic and Financial
Research, pages 379–400. Springer, 2015.

[13] M. Deistler and C. Zinner. Modelling high-dimensional time series
by generalized linear dynamic factor models: An introductory survey.
Communications in Information & Systems, 7(2):153–166, 2007.
[14] G. Della Riccia and A. Shapiro. Minimum rank and minimum trace

of covariance matrices. Psychometrika, 47:443–448, 1982.

[15] P. Enqvist and E. Avventi. Approximative covariance interpolation
with a quadratic penalty. In 46th IEEE Conference on Decision and
Control, pages 4275–4280, 2007.

[16] M. Fazel. Matrix rank minimization with applications. Elec. Eng.

Dept. Stanford University, 54:1–130, 2002.

[17] M. Fazel, H. Hindi, and S. Boyd. Rank minimization and applications
in system theory. In Proceedings of the American Control Conference,
volume 4, pages 3273–3278, Jun. 2004.

[18] J. Geweke. The dynamic factor analysis of economic time series mod-
els. In Latent Variables in Socio-Economic Models, SSRI workshop
series, pages 365–383. 1977.

[19] C. Heij, W. Scherrer, and M. Deistler.

System identiﬁcation by
dynamic factor models. SIAM Journal on Control and Optimization,
35(6):1924–1951, 1997.

[20] L. Ning, T. T Georgiou, A. Tannenbaum, and S. P. Boyd. Linear
models based on noisy data and the frisch scheme. SIAM Review,
57(2):167–197, 2015.

[21] G. Picci and S. Pinzoni. Dynamic factor-analysis models for stationary
IMA Journal of Mathematical Control and Information,

processes.
3(2-3):185–210, 1986.

[22] C. Spearman.

”General Intelligence,” Objectively Determined and

Measured. American Journal of Psychology, 15:201–293, 1904.
[23] Petre Stoica, Randolph L Moses, et al. Spectral analysis of signals.

Prentice Hall, NJ, 2005.

[24] M. Zorzi. Empirical Bayesian learning in AR graphical models.

Automatica, 109:108516, 2019.

[25] M. Zorzi and R. Sepulchre.

Factor analysis of moving average
processes. In 2015 European Control Conference (ECC), pages 3579–
3584, 2015.

[26] M. Zorzi and R. Sepulchre. AR identiﬁcation of latent-variable graph-
IEEE Transactions on Automatic Control, 61(9):2327–

ical models.
2340, Sept 2016.

N = 50000.511.522.5x 10−3eaN = 5000.050.10.150.20.25eLN = 50000.10.20.30.40.50.60.70.8eD152538020406080100120140160180200