Rollage: Eﬃcient Rolling Average Algorithm
to Estimate ARMA Models for Big Time Series Data

Ali Eshragh∗†

Glen Livingston∗

Thomas McCarthy McCann∗

Luke Yerbury∗

Abstract

We develop a new method to estimate the order of an AR model in the presence of big
time series data. Using the concept of a rolling average, we develop a new eﬃcient algorithm,
called Rollage, to estimate the order of an AR model and subsequently ﬁt the model. The
worst case time complexity of our proposed algorithm is O (cid:0)np2 + p(p − 1)/2(cid:1). When used in
conjunction with an existing methodology, speciﬁcally Durbin’s algorithm, we show that our
proposed method can be used as a criterion to optimally ﬁt ARMA models. Empirical results on
large-scale synthetic time series data support the theoretical results and reveal the eﬃcacy of
this new approach, especially when compared to existing methodology.

1

Introduction

Stochastic processes have proved their eﬀectiveness and advantages in modelling and analysing
stochastic dynamic systems evolving in time. They continue to gain in popularity for modelling
a wide range of applications spanning from supply chains (e.g., [1, 22, 23, 24]) and energy systems
(e.g., [15,21]) to epidemiology (e.g., [5,6,18,20]) and computational complexity (e.g., [4,16,17,19]).
One common application of stochastic processes is in time series analysis.

In time series analysis, one objective is to develop a statistical model to forecast the future
behaviour of the system. The autoregressive moving average (ARMA) model is a widely applied model
to achieve this objective. The model was popularised by [7] for analyzing stationary time series
data. Extensions of this model were subsequently introduced, such as the autoregressive integrated
moving average (ARIMA) model for analyzing non-stationary time series data which posses a trend
in mean, and the seasonal ARIMA (SARIMA) model to deal with time series data displaying seasonal
eﬀects [30]. All in all, each SARIMA model is, at its core, an ARMA model for a linearly transformed
time series constructed by diﬀerencing the original time series at proper lags.

The development of ARMA models was accompanied by [7] revered modelling methodology, ap-
propriately known as the Box-Jenkins method. Broadly speaking, the Box-Jenkins method involves
three steps, namely, identiﬁcation, estimation and diagnosis of an ARMA model. The ﬁrst step, model
identiﬁcation, involves making an initial guess for an appropriate order of the model. Throughout
the literature, there have been numerous procedures proposed to estimate the orders of an ARMA

1
2
0
2

c
e
D
0
3

]
E
M

.
t
a
t
s
[

3
v
5
7
1
9
0
.
3
0
1
2
:
v
i
X
r
a

∗School of

Information and Physical Sciences, University of Newcastle, NSW, Australia.

Emails:
ali.eshragh@newcastle.edu.au, glen.livingstonjr@newcastle.edu.au, thomas.mccarthymccann@uon.edu.au,
Luke.Yerbury@uon.edu.au

†International Computer Science Institute, University of California at Berkeley, CA, USA.

1

 
 
 
 
 
 
model [11]. Commonly the orders are chosen through use of the autocorrelation function (ACF) in
conjunction with the partial autocorrelation function (PACF), and observing their respective plots.
This method is not without its drawbacks as modellers require a high level of expertise to in-
terpret the ACF and PACF plots manually [31]. In addition, the PACF only uses the last over-ﬁtted
coeﬃcient to estimate the order of the autoregressive (AR) component which may not truly incor-
porate all information from the sample. The asymptotic distribution of the estimated coeﬃcients
of an over-ﬁtted AR model has been derived by [8]. Using the results of this derivation, the main
goal of this thesis is to establish a new method to estimate the order of an AR model that utilises
all over-ﬁtted coeﬃcients.

The second step of the Box-Jenkins method involves estimating the parameters for the chosen
ARMA model. For pure AR models, an analytical estimation can be achieved by using a variant of max-
imum likelihood estimation (MLE) known as conditional maximum likelihood estimation (CMLE).
However, for pure moving average (MA) and combined ARMA models, both the likelihood and condi-
tional likelihood functions are complex, non-linear functions. Hence, ﬁnding a solution to either the
MLE or CMLE is intractable. Consequentially, numerical optimization methods are often employed
to approximately estimate the parameters of an ARMA model.

The second step of the Box-Jenkins method involves estimating the parameters for the cho-
sen ARMA model. For pure AR models, this can be achieved analytically by performing a variant
of maximum likelihood estimation (MLE) known as conditional maximum likelihood estimation
(CMLE) [26]. However, for pure MA and combined ARMA models, both the likelihood and conditional
likelihood are complex non-linear functions. Hence,

ﬁnding a solution to either the MLE or CMLE is intractable. Consequently, numerical optimiza-

tion methods are often employed to estimate the parameters of MA and ARMA models.

Durbin [12] worked to overcome the intractability of the maximum likelihood equations of MA
models by exploiting the asymptotic equivalence between AR models of inﬁnite order and MA models
of ﬁnite order. The solution was to ﬁt a large order AR model to represent the inﬁnite order AR
model using CMLE. The residuals of that model can then be used as estimates of the unobservable
white noise of the MA process. Standard linear regression techniques could then be used to estimate
the parameters of the MA model. Durbin [13] then extended this initial work to estimate the
parameters for a full ARMA model using a similar approach. Durbin’s method is promising as it
replaces a non-linear estimation problem with two stages of linear estimation [9]. However, a good
question to ask is, how large should the order of the AR model be in Durbin’s methodology to
provide both accurate estimates and optimal eﬃciency?

Through practice and simulation, [9, 10] identiﬁed that increasing the order of the large AR
model inﬁnitely does not necessarily result in more accurate estimates and that the optimal order
is in fact ﬁnite. As a result, an appropriately large order is essential in Durbin’s algorithm to ensure
parameter accuracy and optimal forecasts, as well as improving computational complexity. The use
of model selection criteria, such as the Akaike Information Criteria (AIC) [2], has been suggested
to aide in the selection of an appropriately large order AR model by [27] and [9].

With these developments serving as motivation, this paper will look to develop a new algorithm
to appropriately estimate the order of an AR model, and then use this algorithm as a criterion
in Durbin’s methodology to optimally ﬁt an ARMA model. In particular, our contributions can be
summarized as follows:
1. We introduce the concept of a rolling average.

2. Using the rolling average concept, we develop a highly-eﬃcient algorithm, called Rollage, for

2

ﬁtting AR models.

3. We use the Rollage algorithm as a model selection criterion in Durbin’s methodology to opti-
mally ﬁt ARMA models.

4. On large-scale, synthetic time series data, we empirically demonstrate the eﬀectiveness of the
Rollage algorithm to estimate ARMA models when compared to existing criteria, namely the BIC
and GIC.

The structure of this paper is as follows: Section 2 introduces three times series models utilised
in this paper, namely AR, MA and ARMA models, and covers their important properties and estimation
techniques. Motivated by Theorem 1, Section 3 introduces the concept of a rolling average and
subsequently develops a new methodology to estimate ARMA models, appropriately called Rollage
algorithm. Section 4 illustrates the eﬃcacy of the new methodology by implementing it on several
synthetic big time series data and comparing it to existing methodology. Section 5 concludes the
paper and addresses future work.

Notation

Throughout the paper, vectors and matrices are denoted by bold lower-case, and upper-case letters
respectively (e.g., v and M ). All vectors are assumed to be column vectors. We use regular lower-
case to denote scalar constants (e.g., c). Random variables are denoted by regular upper-case letters
(e.g., X). For a real vector, v, its transpose is denoted by v
. For a vector v and a matrix M ,
(cid:107)v(cid:107) and (cid:107)M (cid:107) denote vector (cid:96)2 norm and matrix spectral norm, respectively. The determinant and
adjugate of a square matrix M are denoted by det(M ) (or |M |, used interchangeably) and adj(M ),
respectively. Adopting Matlab notation, we use A(i, :) and A(:, j) to refer to the ith row and jth
column of the matrix A, respectively, and consider them as a column vector.

(cid:124)

2 Background

In this section, we present a brief overview of the three time series models considered in this paper,
namely autoregressive models (Section 2.1), moving average models (Section 2.1), and autoregres-
sive moving average models (Section 2.3).

2.1 Autoregressive Models

A time series {Yt; t = 0, ±1, ±2, . . .} is called (weakly) stationary, if the mean E[Yt] is independent
of time t, and the auto-covariance Cov(Yt, Yt+h), denoted by γh, depends only on the lag h for any
integer values t and h. A stationary time series {Yt; t = 0, ±1, ±2, . . .} with the constant mean
E[Yt] = 0 is an AR model with the order p, denoted by AR(p), if we have

Yt = φ(p)

1 Yt−1 + · · · + φ(p)

p Yt−p + Wt,

(1)

where φ(p)
(cid:54)= 0 and the time series {Wt; t = 0, ±1, ±2, . . .} is a Gaussian white noise with the mean
p
E[Wt] = 0 and variance Var(Wt) = σ2
W . Gaussian white noise is a stationary time series in which
each individual random variable Wt has a normal distribution and any pair of random variables
Wt1 and Wt2 for distinct values of t1, t2 ∈ Z are uncorrelated. For the sake of simplicity, we assume
that E[Yt] = 0

3

It is readily seen that each AR(p) model has p + 2 unknown parameters consisting of the order
W . Following is a brief explanation of a

p, the coeﬃcients φ(p)
common method in the literature for estimating the unknown order p.

and the variance of white noises σ2

i

Estimating the order p. A common method to estimate the order of an AR(p) model is to use
the partial autocorrelation function (PACF) [30, Chapter 3]. The PACF of a stationary time series
{Yt; t = 0, ±1, ±2, . . .} at lag m is deﬁned by

PACFm :=




ρ(Yt, Yt+1)

ρ(Yt+m − (cid:98)Yt+m,−m, Yt − (cid:98)Yt,m)



for m = 1,

for m ≥ 2,

(2)

where ρ denotes the correlation function, and where (cid:98)Yt,m and (cid:98)Yt+m,−m denote the linear regression,
in the population sense, of Yt and Yt+m on {Yt+1, . . . , Yt+m−1}, respectively.
In order to apply
the PACF values to estimate the order of an AR model, we ﬁrst need to introduce the concept of
“causality”, as given in Deﬁnition 1.

Deﬁnition 1 (Causal AR Model). An AR(p) model is said to be “causal”, if the time series
{Yt; t = 0, ±1, ±2, . . .} can be written as

Yt =

∞
(cid:88)

i=0

ψiWt−i,

where ψ0 = 1 and the constant coeﬃcients ψi satisfy (cid:80)∞

i=0 |ψi| < ∞.

It can be shown that for a causal AR(p) model, while the theoretical PACF (2) at lags m =
1, . . . , p − 1 may be non-zero and at lag m = p is strictly non-zero, at lag m = p + 1 it drops to zero
and then remains at zero henceforth [30, Chapter 3]. Theorem 1 indicates the statistical properties
of the parameter estimates using PACF and the PACF estimates. These can be used to select the
model order in practice by plotting the sample PACF versus lag m along with a 95% zero-conﬁdence
√
boundary, that is two horizontal lines at ±1.96/
n, are plotted. Then, the largest lag m in which
the sample PACF lies out of the zero-conﬁdence boundary for PACF is used as an estimation of the
order p.

Theorem 1 plays a crucial role in developing theoretical results in Section 3.

Theorem 1 (Asymptotic Distribution of Estimated Coeﬃcients [8]). Suppose the time series
{Y1, · · · , Yn} be a stationary casual AR(p) model as given in (1) and ﬁt an AR(m) model (m > p)
to the time series data, that is

Yt = φ(m)

1 Yt−1 + · · · + φ(m)

m Yt−m + Wt.

The maximum likelihood estimate of the coeﬃcient vector, denoted by

4

ˆφp,m =

(cid:104)
ˆφ(m)
1

(cid:105)(cid:124)

· · ·

ˆφ(m)
m

, asymptotically, has a multivariate normal distribution

√

n( ˆφp,m − φp,m) ∼ MN(0, Σp,m),

where φp,m :=

(cid:104)
φ(p)
1

· · · φ(p)
p

0 · · · 0

(cid:105)(cid:124)

, the covariance matrix Σp,m = σ2

W Γ−1

p,m, and

Γp,m =








γ0
γ1
...

γ1
γ0
...

γm−1 γm−2








,

· · · γm−1
· · · γm−2
. . .
· · ·

...
γ0

is the autocovariance matrix of the given time series. Furthermore, ˆφ(m)
for PACFm with the limit distribution

m is an unbiased estimate

√

n ˆφ(m)

m ∼ N[0, 1].

2.2 Moving Average Models

A stationary time series {Yt; t = 0, ±1, ±2, . . .} with the constant mean E[Yt] = 0 is an MA model
with the order q, denoted by MA(q), if we have

Yt = θ1Wt−1 + · · · + θqWt−q + Wt,

(3)

where θq (cid:54)= 0 and the time series {Wt; t = 0, ±1, ±2, . . .} is a Gaussian white noise with the mean
E[Wt] = 0 and variance V ar(Wt) = σ2

W .

Similar to an AR(p) model, each MA(q) model has q + 2 unknown parameters consisting of the
W . Here, we brieﬂy explain the common

order q, the coeﬃcients θi and the variance of white noises σ2
methods in the literature to estimate each of these unknown parameters.

Estimating the order q. A common method to estimate the order of an MA(q) model is to use
the autocorrelation function (ACF) [30, Chapter 3]. The ACF of a stationary time series {Yt; t =
0, ±1, ±2, . . .} at lag m is deﬁned by

ACFm :=




γm
γ0

for m = 0, 1, . . . ,



ACF−m for m = −1, −2, . . . .

(4)

In order to apply the ACF values to estimate the order of an MA model, we ﬁrst need to introduce
the concept of “ invertibility”, as given in Deﬁnition 2.

Deﬁnition 2 (Invertible MA Model). An MA(q) model is said to be “invertible”, if embedded

5

white noises of the time series {Yt; t = 0, ±1, ±2, . . .} can be written as

Wt =

∞
(cid:88)

i=0

πiYt−i,

where π0 = 1 and the constant coeﬃcients πi satisfy (cid:80)∞

i=0 |πi| < ∞.

It can be shown that for an invertible MA(q) model, while the theoretical ACF (4) at lags
m = 1, . . . , q − 1 may be non-zero and at lag m = q is strictly non-zero, at lag m = q + 1 it drops
to zero and then remains at zero henceforth [30, Chapter 3]. Furthermore, it can be shown that
if y1, . . . , yn is a time series realization of an invertible MA(q) model, the (estimated) sample ACF
values, scaled by
n, at lags greater than q has a standard normal distribution, in limit. Thus,
in practice, the (estimated) sample ACF versus lag m along with a 95% zero-conﬁdence boundary,
that is two horizontal lines at ±1.96/
n, are plotted. Then, the largest lag m in which the sample
ACF lies out of the zero-conﬁdence boundary for ACF is used as an estimation of the order q.

√

√

Maximum likelihood estimation of the coeﬃcients θi and variance σ2
W . Unlike an AR
model, for an MA(q) model both the log-likelihood function and the conditional log-likelihood func-
tion are complicated non-convex functions and cannot be maximized analytically [26, Chapter 5].
So, one approach in estimating the parameters of an MA model is maximizing the corresponding
(log-)likelihood function approximately by applying some numerical optimization algorithms, such
as the gradient descent method.

The main diﬃculty in dealing with the likelihood function of an MA model is that the lagged
values of white noises are not known before ﬁtting a model to the data. [12] overcame this problem
by developing a new method for MA model ﬁtting. Motivated from Deﬁnition 2, it is readily seen
that an “invertible” MA(q) model can be represented by

∞
(cid:88)

(−πi)Yt−1 + Wt,

Yt =

i=1

implying that an invertible MA(q) model is equivalent to an AR(∞) model. Durbin exploited this
equivalence to estimate the parameters of an MA model and later extended this for ARMA models.
As a result, Durbin’s Algorithm ﬁrst ﬁts an AR model with a suﬃciently large order, say ˜p, to
the data and approximates the values of white noises Wt by ﬁnding the residuals of the ﬁtted AR
model, that is,

(cid:101)wt = yt −

˜p
(cid:88)

(−ˆπi)yt−i.

(5)

i=1
In the next step, the algorithm approximately estimates the coeﬃcients θi by regressing the time
series over the q lagged values of the estimated residuals in (5), that is

Yt = ˜θ1 ˜wt−1 + · · · + ˜θq ˜wt−q + Wt.
The steps of Durbin’s Algorithm are depicted in Algorithm 1. [27] extended this algorithm with
trimming steps to improve the initial parameter estimates, as well as suggesting the use the Bayesian
Information Criterion (BIC) as a model selection criterion. Details regarding the BIC can be seen
in Deﬁnition 3. Both algorithms are readily applicable to ARMA models with the inclusion of the
appropriate lags of the primary time series in Step 3.

(6)

6

Deﬁnition 3 (Bayesian Information Criterion [30]). Consider a regression model with k coef-
ﬁcients and denote the maximum likelihood estimator for the variance as

ˆσ2
k =

SSE(k)
n

where SSE(k) is the residual sum of squares under the model with k regression coeﬃcients.
The Bayesian Information Criterion, denoted BIC, is

BIC = log( ˆσ2

k) +

k log(n)
n

,

with the value of k yielding the minimum BIC specifying the best regression model.

Algorithm 1 Durbin’s Algorithm: An Algorithm for MA Fitting

Input:

- Time series data {y1, . . . , yn} ;

- The estimated order q ;

Step 1. Choose a suﬃciently high order ˜p > q;

Step 2. Find the CMLE of the coeﬃcient vector, (ˆπ1, . . . , ˆπ˜p) and compute the white noise
approximations (cid:101)wt as in (5);
Step 3. Regress the primary time series over q lagged values of (cid:101)wt to estimate the coeﬃcient
vector, (˜θ1, . . . , ˜θq) as in (6);

Output: Estimated coeﬃcients (

ˆ˜θ1, . . . ,

ˆ˜θq).

[9, 10] identiﬁed, through practice and simulation, that simply increasing the size of ˜p in the
intermediate AR model does not necessarily improve the quality of estimates of the associated MA
model. Broerson [9, 10] also identiﬁed that the optimal ˜p value is in fact ﬁnite and advocated the
use of model selection criteria, such as the AIC [2], in order to choose an appropriate ˜p. Instead,
Broerson [9, 10] developed a new selection criterion for optimally choosing ˜p by generalizing the
AIC (GIC), details of which can be seen in Deﬁnition 4.

Deﬁnition 4 (Generalized Information Criterion [9]). Consider an MA(q) process as deﬁned
in (3) with variance

(cid:32)

Y = σ2
σ2
W

1 +

(cid:33)

θ2
i

=

q
(cid:88)

i=1

σ2
W
i=1(1 − k2
i )

(cid:81)∞

where θi are the coeﬃcients of an MA model and ki are the reﬂection coeﬃcients of a long AR
model. Then the residual sum of squares for N observations of an MA(q) process, denoted RSSp,

7

is given by

RSSp =

N
(cid:88)

σ2
Y

p
(cid:89)

(1 − k2

i ) = RSSp−1(1 − k2

p).

The Generalized Information Criterion, denoted GIC(p, α), with penalty factor 1 for α, is

n=1

i=1

GIC(p, α) = log

(cid:19)

(cid:18) RSSp
N

+

αp
N

where the optimal order k is the order with minimum GIC(p, 1), p = 0, 1, . . . , ∞.

2.3 Autoregressive Moving Average Models

A stationary time series {Yt; t = 0, ±1, ±2, . . .} with the constant mean E[Yt] = 0 is an ARMA model
with orders p and q, denoted by ARMA(p, q), if we have

Yt = φ1Yt−1 + · · · + φpYt−p + Wt + θ1Wt−1 + · · · + θqWt−q,

(7)

where φp (cid:54)= 0, θq (cid:54)= 0 and Wt is Gaussian white noise with the mean E[Wt] = 0 and variance
V ar(Wt) = σ2
W . It is readily seen that an ARMA model is a combination of the AR and MA model
seen previously in Section 2.1 and Section 2.2 respectively. Consequently, each ARMA(p, q) model
has p + q + 3 unknown parameters consisting of orders p and q, the coeﬃcients φi and θi and the
variance of the white noises σ2
W . Here, we brieﬂy explain the common methods in the literature to
estimate each of these unknown parameters.

Estimating the orders p and q Since each ARMA model contains both AR and MA components,
the order of an ARMA(p, q) model is commonly estimated by using the PACF seen in Section 2.1
in conjunction with the ACF seen in Section 2.2. Table 1 [30] summarises the behaviour of the
ACF and PACF for ARMA models and its derivatives. For pure AR and MA models, the behaviour of
the PACF and ACF is clear and estimation of p or q is straight forward as previously discussed in
Section 2.1 and Section 2.2 respectively. However, the behaviour of the PACF and ACF for an ARMA
model is ambiguous with no clear cut oﬀ after a particular lag to estimate the orders p and q. As
a result, a more precise inspection of the PACF and ACF is required, and often multiple models,
consisting of diﬀerent combinations of orders p and q, are made as an initial guess of what the
true orders may be. Steps 2 and 3 of the Box-Jenkins method, estimation and diagnosis, are then
performed to help distinguish the best modelling combination of p and q.

AR(p)
Tails oﬀ

ACF
PACF Cuts oﬀ after lag p

MA(q)
Cuts oﬀ after lag q
Tails oﬀ

ARMA(p, q)
Tails oﬀ
Tails oﬀ

Table 1: This table highlights the behaviour of the ACF and PACF for AR(p), MA(q) and ARMA(p, q)
processes.

8

Maximum likelihood estimation of the coeﬃcients φi and θi and variance σ2
W : Due to
the MA component, both the log-likelihood and conditional log-likelihood function of an ARMA(p, q)
model are complicated non-convex functions and cannot be maximized analytically. Numerical
optimization techniques are commonly employed to deal with estimating the coeﬃcients of ARMA
models. Durbin’s methodology, Algorithm 1, of exploiting the asymptotic equivalence between
AR(∞) and MA(q) models can similarly be used to estimate the coeﬃcients of ARMA models. The
optimal order ˜p can also be chosen using model selection criteria such as the BIC utilised by Hannan
and Rissanen [27] and the GIC utilised by [9, 10] as seen in Section 2.2. Durbin’s methodology,
Algorithm 1, along with the BIC and GIC criteria will be used as the estimation method for ARMA
models for the remainder of this paper, and will be compared empirically in Section 4 to the new
algorithm, called Rollage, developed in Section 3.

3 Theoretical Results

In this section, we introduce the concept of rolling average and develop its theoretical properties.
These results are utilized to construct an eﬃcient algorithm to estimate an appropriate AR model
for big time series data.
It should be noted that all proofs of this section as well as technical
lemmas/propositions/theorems used in the proofs are presented in the Supplementary Materials
document.

3.1 Rolling Average

We start this section by introducing the rolling average which is plays a central role in this work.

Deﬁnition 5 (Rolling Average). Suppose the time series {Y1, · · · , Yn} is a causal AR(p) process.
Fit an AR(m) model (m > p) to the data and ﬁnd the MLE of the coeﬃcient vector, ˆφp,m =
( ˆφ(m)
. The “rolling average” of this estimation is denoted by ¯φp,m and deﬁned as
1
follows:

, · · · , ˆφ(m)
m )

(cid:124)

¯φp,m :=

1
m − p

m
(cid:88)

j=p+1

ˆφ(m)
j

.

The main motivation of introducing rolling averages is the convergence result on the asymptotic
distribution of the MLEs of an AR model coeﬃcients ﬁtted to (large enough) time series data, as
expressed in Theorem 1. More precisely, Theorem 1 states that if the underlying model of the time
series data is truly an AR(p) model, but an AR(m) model (m > p) is (over)ﬁtted to the data, the
MLE of the (over-ﬁtting) coeﬃcient vector, asymptotically, has a multivariate normal distribution,
that is,

√

(cid:104)
ˆφ(m)
p+1

n

(cid:105)(cid:124)

· · ·

ˆφ(m)
m

∼ MN(0, Σp,m(p + 1, m : p + 1 : m)).

Accordingly, the rolling average ¯φp,m which is the sample mean of the estimated (over-ﬁtting)
coeﬃcients ˆφ(m)
m should also have a Normal distribution with a mean zero, in limit. This
implies that, similar to the PACF values, the rolling averages can be also considered as a tool to

p+1, · · · , ˆφ(m)

9

estimate the order of an AR model. Furthermore, while the former approach looks only at the last
(over-ﬁtting) coeﬃcient ˆφ(m)
m to estimate the order of the model, rolling averages consider all (over-
ﬁtting) coeﬃcients together. Hence, one could expect that the latter exploits more information
from the sample, and consequently may provide more eﬃcient and accurate estimates. This is
the motivation behind developing a new algorithm for estimating an AR model based on rolling
averages.

Hence, we should derive the variance of the rolling averages to be able to utilize them in an
algorithmic way to estimate the order of an AR model. For this purpose, we ﬁrst deﬁne nested lower
right corner matrix in Deﬁnition 6 and then obtain its structure in Theorem 2. Finally, Theorem 3
applies all these results to establish the asymptotic distribution of the rolling average ¯φp,m.

Deﬁnition 6 (Nested Lower Right Corner Matrix (NLRC)). Suppose the time series {Y1, · · · , Yn}
is a causal AR(p) process. Fit an AR(m) model (m > p) to the data and ﬁnd the covariance
matrix Σp,m. The (m − p) × (m − p) square matrix Σp,m(p + 1 : m, p + 1 : m) extracted from the
lower right corner of the covariance matrix is called “nested lower right corner” matrix and
denoted by NLRCp,m.

Theorem 2 (Closed Form for NLRCp,m). Let the time series {Y1, · · · , Yn} be a causal AR(p)
model. The nested lower right corner matrix NLRCp,m for a ﬁxed p ≥ 1 and m = p+1, p+2, . . .,
is a symmetric positive deﬁnite matrix with the lower triangular coordinates satisfying the
following “nested” equations for m = p + 2, p + 3, . . .,

NLRCp,m(i, j) =

m−p−1 + NLRCp,m−1(i, 1)

if m ≤ 2p + 1 :
m−p−iφ(p)
φ(p)
0 φ(p)
φ(p)

m−p−1

if m ≥ 2p + 2 :

NLRCp,m−1(i, 1)






0

for all m ≥ p + 2 :

NLRCp,m−1(i − 1, j − 1)

for j = 1, i = 1, . . . , m − p − 1

for j = 1, i = m − p

for j = 1, i = 1, . . . , m − p − 1

for j = 1, i = m − p

for 2 ≤ j ≤ i ≤ m − p,

where φ(p)
0

:= −1 and the initial condition is NLRCp,p+1(1, 1) = (φ(p)

0 )2. Moreover, the lower

10

triangular coordinates equal

NLRCp,m(i, j) =

min{m−p−i,p−(cid:96)}
(cid:88)






min{m−p−i,p}
(cid:88)

(φ(p)

k )2

k=0

min{m−p−i,p−1}
(cid:88)

for j = i, i = 1, . . . , m − p

k φ(p)
φ(p)

k+1

for j = i − 1, i = 2, . . . , m − p

k=0

...

...

k φ(p)
φ(p)

k+(cid:96)

for j = i − (cid:96), i = (cid:96) + 1, . . . , m − p

...

for j = i − p, i = p + 1, . . . , m − p

elsewhere,

k=0

...
φ(p)
0 φ(p)
p

0

and above the diagonal coordinates are calculated through

NLRCp,m(i, j) = NLRCp,m(j, i)

for 1 ≤ i < j ≤ m − p.

Note that those coordinates where the corresponding range for i has a lower bound greater than
the upper bound should be disregarded.

Example 1. For an AR(2) model, the lower triangular coordinates of the symmetric matrix
NLRC2,7 are illustrated below (for sake of simplicity, the superscript (2) has been omitted):

NLRC2,7 =














0 + φ2
1 + φ2
φ2
2
φ0φ1 + φ1φ2 φ2

0 + φ2
1 + φ2
2
φ0φ1 + φ1φ2 φ2

1 + φ2
0 + φ2
2
φ0φ1 + φ1φ2 φ2

φ0φ2

0 + φ2
1
φ0φ1

φ0φ2

0

φ0φ2

0

0














.

φ2
0

Theorem 3 (Asymptotic Distribution of Rolling Average). Let the time series {Y1, · · · , Yn}
be a causal AR(p) process. If an AR(m) model (m > p) is ﬁtted to the data, asymptotically, we
have

√

n ¯φp,m ∼ N [0, σ2

p,m],

11

where σ2

p,m satisﬁes the recursion,

(m − p)2σ2

p,m =






(m − p − 1)2σ2
(m − p − 1)2σ2
(φ(p)

0 )2

p,m−1 + (φ(p)
p,m−1 + (φ(p)

0 + · · · + φ(p)
0 + · · · + φ(p)

p )2
m−p−1)2

for m = 2p + 1, 2p + 2, . . . ,

for m = p + 2, . . . , 2p,

for m = p + 1,

with the general solution for σ2

p,m,






(φ(p)
0 )2
(cid:16)
1
22

(φ(p)

0 )2 + (φ(p)

0 + φ(p)

1 )2(cid:17)

...

(cid:16)

1
(cid:96)2

(φ(p)

0 )2 + (φ(p)

0 + φ(p)

1 )2 + · · · + (φ(p)

0 + · · · + φ(p)

(cid:96)−1)2(cid:17)

...

(cid:16)

1
p2

(φ(p)

0 )2 + (φ(p)

0 + φ(p)

(cid:16)

1
(p + k)2

p2σ2

p,2p + k(φ(p)

1 )2 + · · · + (φ(p)
0 + · · · + φ(p)
p )2(cid:17)

0 + · · · + φ(p)

p−1)2(cid:17)

for m = p + 1,

for m = p + 2,

...
for m = p + (cid:96),

...
for m = 2p,

for m = 2p + k, k = 1, 2, . . . .

3.2 Rollage Algorithm for Fitting AR Models

Based on theoretical results developed in Section 3.1, we introduce the Rollage algorithm, depicted
in Algorithm 2 to ﬁt an appropriate AR model to a given time series data.

Typical procedure for calculating rolling average.
averages are calculated in the Rollage algorithm.

Tables 2 and 3 illustrate how the rolling

Model Parameters
ˆφ(1)
AR(1)
1
1 , ˆφ(2)
ˆφ(2)
AR(2)
ˆφ(3)
1 , ˆφ(3)
AR(3)
...
...
1 , · · · , ˆφ(¯p)
ˆφ(¯p)
AR(¯p)

2 , ˆφ(3)

¯p

3

2

Table 2: Fitting ¯p models AR(1), . . . , AR(¯p) to the data and ﬁnding the MLE of their parameters

3.3 Rollage Algorithm for Fitting ARMA Models

In order to ﬁt ARMA (and MA) models using the theory developed in Section 3.2, we must use
the Rollage algorithm in conjunction with Durbin’s Algorithm 1. To achieve this, we must ﬁrst

12

Algorithm 2 Rollage: A Novel Algorithm for AR Fitting

Input:

- Time series data {y1, . . . , yn} ;

- A relatively large value ¯p (cid:28) n ;

Step 0. Set (cid:96) = 1 ;

while (cid:96) < ¯p do

Step 1. (cid:96) ← (cid:96) + 1 ;
Step 2. Find the CMLE of the coeﬃcient vector, ( ˆφ((cid:96))
Step 3. Compute the values of rolling averages, ¯φh,(cid:96), for h = 1, . . . , (cid:96) − 1 as in Deﬁnition 5 ;
Step 4. Compute the variance of rolling averages, ˆσ2

(cid:96),m, for m = (cid:96) + 1, . . . , ¯p as in Theorem 3 ;

1 , . . . , ˆφ((cid:96))

(cid:96) ) ;

end while
Step 5. Estimate p as the largest (cid:96) such that at least 5% of inequalities | ¯φ(cid:96),m| ≥ 1.96σ(cid:96),m/
for m = (cid:96) + 1, . . . , ¯p are held ;
Output: Estimated order p and coeﬃcients ( ˆφ(p)

1 , . . . , ˆφ(p)
p ).

√

n − ¯p

Model Rolling average
AR(1) −
AR(2)
AR(3)
...
AR(¯p)

¯φ1,2 = ˆφ(2)
2
¯φ1,3 = 1
...
¯φ1,¯p = 1

2 ( ˆφ(3)

¯p−1 ( ˆφ(¯p)

2 + ˆφ(3)

3 ), ¯φ2,3 = ˆφ(3)

3

2 + · · · + ˆφ(¯p)

¯p ), . . ., ¯φ¯p−1,¯p = ˆφ(¯p)

¯p

Table 3: Calculating the rolling averages

introduce a threshold hyper-parameter to Rollage so that an appropriately large ˜p value is chosen
for the AR model, in the intermediate step of Algorithm 1. In this scenario, Rollage will act as a
stopping criterion to choose an appropriately large ˜p value.

We deﬁne the threshold hyper-parameter, denoted δ, as the following inequality:

(cid:18)

δ < max

| ¯φ(cid:96),m|
√

1.96σ(cid:96),m/

n − ¯p

(cid:19)

,

(8)

whereby the estimate of the large ˜p value for the intermediate AR model is the ﬁrst value of l such
that the inequality is violated. This extension to Durbin’s Algorithm 1 can be seen in Algorithm 3,
and is called Rollage*.

13

Algorithm 3 Rollage*: An Algorithm for ARMA (and MA) Fitting

Input:

- Time series data {y1, . . . , yn} ;

- The estimated order q ;

- Threshold parameter δ

Step 1. Using Rollage (2) choose the suﬃciently high order ˜p > q to be the ﬁrst value of l such
that the inequality δ < max

is violated in the Rollage algorithm;

(cid:17)

(cid:16)

| ¯φ(cid:96),m|
√
1.96σ(cid:96),m/

n−¯p

Step 2. Find the CMLE of the coeﬃcient vector, (ˆπ1, . . . , ˆπ˜p) and compute the white noise
approximations (cid:101)wt as in (5);
Step 3. Regress the primary time series over q lagged values of (cid:101)wt to estimate the coeﬃcient
vector, (˜θ1, . . . , ˜θq) as in (6);

Output: Estimated coeﬃcients (

ˆ˜θ1, . . . ,

ˆ˜θq).

4 Empirical Results

In this section, we present the performance of the Rollage algorithm on several synthetic time series
data. The data were simulated using models with randomly generated sets of coeﬃcient parameters.
The variance of the error parameter was 1 for each case. The resulting models were conﬁrmed to
be causal and invertible using MATLAB’s arima() function. Parameters were randomly generated
for AR(p) and MA(q) models with p = 5, 10, . . . , 100 and q = 5, 10, . . . , 100. All possible parameter
combinations were then utilised to produce 400 ARMA(p, q) models.
Numerical analysis is presented in three subsequent sections.

In Section 4.1, the Rollage
algorithm is applied to the AR synthetic data where its overall eﬃcacy is analysed. In Section 4.2, the
Rollage algorithm is applied to the MA data where it is used as a criterion in Durbin’s Algorithm 1
and compared to the BIC and GIC criteria for estimating an optimal ˜p value then subsequently
ﬁtting an MA model. The relative errors of parameter estimation are also analysed amongst the
Rollage, BIC and GIC criteria. Analogously, Section 4.3 compares the Rollage, BIC and GIC
criteria in estimating ARMA models.

4.1 Autoregressive Models

From each of the 20 AR(p) models, 500,000 synthetic time series realizations were generated. The
Rollage algorithm was applied to each with results being the average of 20 replications.

For each of the 20 models, the Rollage algorithm correctly identiﬁes the order p, showing its
eﬃcacy in estimating this parameter. Figure 1 displays the rolling average graphs produced by the
Rollage algorithm at various lags for the data generated from the AR(20) model. In general, we
notice that for lags less than or equal to the true order p, at least one rolling average lies signiﬁcantly
outside the 95% conﬁdence boundary. However, for all subsequent lags greater than the true order
p all rolling averages lie within the 95% conﬁdence boundary. Speciﬁcally, Figure 1(a-e) displays
lags less than or equal to p = 20, where it can be seen that the rolling averages lie outside the 95%
conﬁdence bounds. Figure 1(f-i) then shows lags greater than p = 20 where the rolling averages

14

lie within the 95% conﬁdence bounds. This feature is dictated by the inequalities in Step 5 of
the Rollage algorithm. It should be noted that the conﬁdence boundaries in Figure 1(a-f) display
similar curvature to those in Figure 1(g-i), also dictated by Step 5, however can not be distinguished
due to the larger scale in their respective graphs. In this sense, the Rollage graphs can be used as
a tool, analogous to the PACF, in choosing the order of an AR model.

(a) p = 5

(b) p = 10

(c) p = 15

(d) p = 19

(e) p = 20

(f) p = 21

(g) p = 25

(h) p = 30

(i) p = 35

Figure 1: This ﬁgure illustrates the rolling averages at diﬀerent lags for nine AR(p) models where the
underlying data is generated from an AR(20) model. It is readily seen that up to p = 20, there exists
at least one rolling average which is signiﬁcantly out of the 95% conﬁdence boundaries. However,
from p = 21 onwards, all rolling averages at all lags lie within the 95% conﬁdence bounds.

4.2 Moving Average Models

For each of the 20 MA(q) models, synthetic time series data was generated for sample sizes n = 10,000,
20,000, 50,000, 100,000, 200,000, 500,000 and 1,000,000. For each dataset, the Rollage, BIC and
GIC criteria are used to estimate an optimal ˜p value for ﬁtting an AR model before subsequently
ﬁtting an MA model. The threshold hyper-parameter for the Rollage algorithm was varied from 2.5

15

to 4 in 0.25 increments throughout the experiments with threshold equal to 3 providing the best
trade oﬀ between estimating an optimal ˜p and producing the lowest relative error of parameter
estimates, on average. As a result, all subsequent empirical results utilise a threshold parameter of
3 in the Rollage algorithm.

Figure 2 displays the estimated ˜p values for each q (i.e. ˜p vs q) from the 20 MA models at the
various sample sizes n (excluding n = 10,000). More precisely, the blue, red and magenta graphs
are associated with the Rollage, BIC and GIC criteria, respectively. It can be observed that for
all sample sizes, the Rollage algorithm produces a smaller than or equal to ˜p value compared to
both the BIC and GIC criteria, with this becoming more noticeable as q and n increases. Table 4
highlights this, comparing the average ˜p and associated relative error produced by the Rollage,
BIC and GIC criteria for each of the respective sample sizes n. On average, the Rollage algorithm
provides smaller estimates for ˜p than both the BIC and GIC criteria, with the diﬀerence between
Rollage and the other criteria increasing as n increases. The bottom 2 rows of Table 4 quantify
this relative diﬀerence between Rollage and the BIC and GIC criteria. The relative diﬀerence of
˜p is calculated as

|˜pA − ˜pR|
˜pR
where ˜pR is given by the Rollage algorithm and ˜pA is suggested by the mentioned alternative.
The total average column of Table 4 summarises this trend for all sample sizes, showing that the
Rollage criterion provides ˜p values that are 18.06% smaller than the BIC and 32.26% smaller than
the GIC criteria, on average. The results of Figure 2 and Table 4 provide empirical evidence that
the Rollage algorithm is computationally less expensive than both the BIC and GIC criteria, on
average.

(9)

Similarly, Figure 3 displays the estimated ˜p values for each sample size n (i.e. ˜p vs n) for various
MA(q) models where a similar conclusion, that the Rollage criterion provides smaller estimates for ˜p
than the BIC and GIC criteria, is reached. Additionally, Figure 2 suggests that ˜p increases linearly
as q increases, and becomes more evident as n increases, whereas Figure 3 suggests that ˜p increases
logarithmically as n increases. We have ﬁtted linear models to predict the optimal ˜p, which ﬁnd q,
log(n) and q log(n) to be statistically signiﬁcant predictors for ˜p across all three criteria. Table 5
provides the coeﬃcients for linear models to predict the optimal ˜p for each of the Rollage, BIC
and GIC criteria. For the Rollage algorithm, the rearranged linear model is

ˆ˜p = q(0.81 log(n) − 7.12) + 3.19 log(n)

(10)

highlighting that ˜p is heavily dependent on q, however the sample size n is less impactful as log(n)
stunts this impact. The linear models can’t be applied generally to prescribe a ˜p, even within the
range of q and n considered, as the set of 20 MA model coeﬃcients may not be representative of
the entire population. The models do however provide a good initial approximation for ˜p and oﬀer
insight into variable relationships.

Analogous to Figure 2, Figure 4 shows the corresponding relative error associated with estimat-
ing the parameters of an MA(q) model, in percentage. Here we deﬁne the relative error of parameter
estimates by

|| ˆθ − θ||
||θ||

(11)

where ˆθ is the parameter estimates and θ is the true parameter values for an MA(q) model.
It
is observed that for all sample sizes, the Rollage criterion produces similar relative errors to the

16

(a) n = 20, 000

(b) n = 50, 000

(c) n = 100, 000

(d) n = 200, 000

(e) n = 500, 000

(f) n = 1, 000, 000

Figure 2: Each graph illustrates the ˜p values estimated by the Rollage, BIC and GIC criteria to
initially ﬁt an AR(p) model and then subsequently ﬁt an associated MA(q) model. The Rollage, BIC
and GIC criteria are represented by the blue, red and magenta graphs respectively. The six graphs
represent the diﬀerent sample sizes n that were used to ﬁt an MA(q) model. It is readily seen that
the Rollage criteria regularly produces the smallest estimate for ˜p.

BIC and GIC criteria over the whole range of MA(q) models. Table 4 summarises this by showing
the average relative errors below the average estimate of ˜p. For all criteria, the relative error of
parameter estimates becomes smaller as n increases and for big data regimes (i.e. n = 1,000,000) all
criteria produce relative errors of approximately 1.5%. As a result, the Rollage algorithm provides
a great trade oﬀ between computational runtime and algorithmic accuracy for ﬁtting an MA model,
when compared to the BIC and GIC criteria.

In addition, Figure 4 suggests that the relative error increases linearly as a function of q,
however as n increases the linear relationship appears to become more constant across all values
of q (particularly n = 1,000,000). In contrast, Figure 5 displays the relative error of parameter
estimates as a function of the sample size n for various MA(q) models, where we see the relative errors
decrease exponentially. This is further evidence that the Rollage algorithm may be appropriate
for ﬁtting MA models to big time series data when compared to current alternatives.

4.3 Autoregressive Moving Average Models

For each of the 400 ARMA(p, q) models, synthetic time series data was generated for sample sizes n =
10,000, 20,000, 50,000, 100,000, 200,000, 500,000 and 1,000,000. For each dataset, the Rollage,
BIC and GIC criteria produced estimates of the optimal ˜p value for ﬁtting a large AR model before
subsequently ﬁtting the full ARMA model. The threshold hyper-parameter for the Rollage algorithm
was varied from 2.5 to 4 in 0.25 increments throughout the experiments and 3 was found to provide

17

10k

20k

50k

100k

200k

500k

1M Total Average

Rollage

71
(14.09%)

86
(10.26%)

115
(7.14%)

134
(5.76%)

168
(3.93%)

216
(2.99%)

297
(1.78%)

BIC

GIC

76
(13.69%)

100
(12.79%)

94
(9.75%)

113
(9.30%)

129
(6.66%)

155
(5.22%)

205
(3.47%)

263
(2.55%)

363
(1.48%)

151
(6.34%)

174
(5.07%)

217
(3.40%)

276
(2.53%)

403
(1.42%)

Relative to BIC

7.04%

9.30%

12.17% 15.67% 22.02% 21.76% 22.22%

Relative GIC

40.85%

31.40% 31.30% 29.85% 29.17% 27.78% 35.69%

155
(6.56%)

183
(6.12%)

205
(5.84%)

18.06%

32.26%

Table 4: Each cell provides the average ˜p (rounded to the nearest integer) and average associated
relative error (rounded to 2 d.p.) of the subsequently ﬁtted MA models for the given sample size
and estimation method.

Rollage
BIC
GIC

q
-7.12
-9.39
-9.43

log(n)
3.19
3.27
5.76

q log(n) R2
a
0.95
0.95
0.94

0.81
1.06
1.05

Table 5: Coeﬃcients for the corresponding terms of linear models ﬁtted to predict the optimal ˜p
chosen by each criterion based on all MA simulated time series data.

the lowest average relative errors for the parameter estimates over the various n < 1,000,000 and
3.5 for n = 1,000,000.

Similar to the case with MA models, Figure 6 provides examples of the Rollage algorithm
consistently providing smaller values of ˜p than the other criteria. The total average column of
Table 6 summarises this trend for all considered sample sizes, showing also that GIC consistently
provides the largest values. Rollage begins suggesting signiﬁcantly smaller ˜p than BIC for n >
50,000, and for all considered sample sizes for GIC. The bottom 2 rows of Table 6 quantify this
relative diﬀerence, which is calculated as per (9). The relative diﬀerence becomes signiﬁcant as
n gets large. However, the Rollage algorithm provides relative errors that closely matches those
for BIC and GIC for all considered sample sizes. When n = 1,000,000, we see that BIC provides
a relative error only 1.5% lower than Rollage, but the ˜p prescribed by BIC is almost 20% larger
than that for Rollage. This is evidence that the Rollage algorithm could provide signiﬁcant
computational savings within the context of big data with a limited reduction in parameter accuracy.
Figures 6 and 7 suggest that ˜p grows linearly as a function of q and logarithmically as a function
of n. The relationship with p is less obvious, but linear models ﬁnd q, p, p log(n), q log(n) and
log(n) to be signiﬁcant predictors for ˜p across all three criterion. The coeﬃcients of the three linear
models are provided in Table 7. The dependence of ˜p suggested by Rollage on p and q is clariﬁed
if we consider the rearranged model

ˆ˜p = p(2.08 − 0.14 log(n)) + q(0.76 log(n) − 6.29) + 4.19 log(n).

(12)

Here we can see that for the range of sample sizes considered for this study, the coeﬃcients of
p and q are both positive, but as n increases the coeﬃcient of p decreases while the coeﬃcient of q

18

(a) MA(5)

(b) MA(10)

(c) MA(25)

(d) MA(45)

(e) MA(50)

(f) MA(55)

(g) MA(75)

(h) MA(90)

(i) MA(100)

Figure 3: This ﬁgure illustrates ˜p as a function of the sample size n for various MA(q) models.
The Rollage criteria is shown to consistently produce the smallest estimate for ˜p. A logarithmic
relationship can be observed between ˜p and the sample size n.

increases. This is more evidence that for larger n, the value of q is more critical for determining the
optimal ˜p than p. This is likely due to q determining the number of white noise columns included in
the design matrix. These regression models can’t be applied generally to prescribe a ˜p, even within
the range of p, q and n considered, as the 400 ARMA models may not be representative of the entire
population of ARMA models. The linear model does however provide a good initial approximation
for ˜p and oﬀer insight into variable relationships.

5 Conclusion

In this paper, we have developed a new eﬃcient algorithm to estimate an AR model. Motivated
by Theorem 1, we utilise the concept of a rolling average to develop an algorithm, called Rollage,
to estimate the order of an AR model and subsequently ﬁt the model. When used in conjunction

19

(a) n = 20, 000

(b) n = 50, 000

(c) n = 100, 000

(d) n = 200, 000

(e) n = 500, 000

(f) n = 1, 000, 000

Figure 4: Each graph illustrates the Relative Error % of estimating the parameters of an MA(q)
model for each of the Rollage, BIC and GIC criteria. The Rollage, BIC and GIC criteria are
represented by the blue, red and magenta graphs respectively. The six graphs represent the diﬀerent
sample sizes n that were used to ﬁt an MA(q) model. All three criteria produce similar relative errors
in estimating the parameters of an associated MA(q) model. Relative errors reduce as sample size n
increases for all criteria.

with existing methodology, speciﬁcally Durbin’s, the Rollage algorithm can be used as a criterion
to optimally ﬁt ARMA models to big time series data. Empirical results on large-scale synthetic
time series data show the eﬃcacy of Rollage and when compared to existing criteria, the Rollage
algorithm provides a great trade oﬀ between computational complexity and algorithmic accuracy.

20

(a) MA(5)

(b) MA(10)

(c) MA(25)

(d) MA(45)

(e) MA(50)

(f) MA(55)

(g) MA(75)

(h) MA(90)

(i) MA(100)

Figure 5: This ﬁgure illustrates Relative Error % of estimating the parameters of an MA(q) model
as a function of the sample size n. The Rollage criterion is shown to produce similar relative errors
when compared to the BIC and GIC criteria. A negative exponential relationship can be observed
between Relative Error % and the sample size n.

21

10k

20k

50k

100k

200k

500k

1M

Total Average

Rollage

130
(35.94%)

142
(32.59%)

168
(27.33%)

172
(25.32%)

205
(20.59%)

269
(14.99%)

301
(11.09%)

BIC

GIC

131
(35.24%)

143
(31.90%)

169
(26.97%)

182
(24.12%)

222
(19.06%)

292
(13.51%)

170
(35.00%)

186
(31.53%)

220
(26.76%)

219
(23.74%)

263
(18.86%)

344
(13.72%)

358
(9.68%)

425
(9.83%)

Relative to BIC

0.77%

0.70%

0.60%

5.81%

8.29%

8.55%

18.94%

Relative GIC

30.77%

30.99%

30.95%

27.33%

28.29%

27.88%

41.20%

195
(23.88%)

211
(22.84%)

257
(22.69%)

8.21%

31.79%

Table 6: Each cell provides the average ˜p (rounded to the nearest integer) and average associated
relative error (rounded to 2 d.p.) of the subsequently ﬁtted ARMA models for the given sample size
and estimation method.

Rollage
BIC
GIC

p
2.08
1.87
2.54

q
-6.29
-8.12
-8.86

log(n)
4.19
3.55
7.09

p log(n)
-0.14
-0.11
-0.19

q log(n) R2
a
0.92
0.94
0.90

0.76
0.94
1.03

Table 7: Coeﬃcients for the corresponding terms of linear models ﬁtted to predict the optimal ˜p
chosen by each criterion based on all ARMA simulated time series data.

22

(a) ARMA(25, 75)

(b) ARMA(75, 25)

(c) ARMA(25, 75)

(d) ARMA(75, 25)

Figure 6: The ﬁrst two plots (left to right) show the values of ˜p suggested by the 3 criterion plotted
against n for two ARMA models. The last two plots show the Relative Error % of the full ARMA
parameter estimates for the same models when the optimal ˜p according to each criterion was used
in Durbin’s Algorithm (1).

23

(a) ARMA(p, 25)

(b) ARMA(p, 50)

(c) ARMA(p, 75)

(d) ARMA(p, 100)

(e) ARMA(25, q)

(f) ARMA(50, q)

(g) ARMA(75, q)

(h) ARMA(100, q)

Figure 7: The values of ˜p suggested by the three criterion are plotted against p and q for n =
1, 000, 000. The dependence of ˜p linearly on q is clearly more signiﬁcant than the dependence on p.

24

A Technical Lemmas/Propositions/Theorems and Proofs

A.1 Proof of Theorem 2

We ﬁrst present Lemma 1 [25], Theorem 4 [8], and Theorem 5 [25] and then prove Propositions 1
and 2 which are used in the proof of Theorem 2.

Lemma 1 (Block Matrix Inversion [25]). Consider the 2 × 2 block matrix





(cid:124)



c b

 ,

b A

M =

where A, b, and c are an m × m matrix, an m × 1 vector and a scalar, respectively. If A is
invariable, the inverse of matrix M exists an can be calculated as follows

M −1 =





1
k

1

(cid:124)

−b

A−1



−A−1b kA−1 + A−1bb

(cid:124)

A−1

 ,

where k = c − b

(cid:124)

A−1b.

Theorem 4 (Recursion for Autocovariance Fucntion [8]). For a causal AR(p) model with an
induced Gaussian white noise series with mean 0 and variance σ2
W , the autocovariance function
at lag j is given by

γj =






1 γ1 + · · · φ(p)
φ(p)
φ(p)
1 γj−1 + · · · + φ(p)
γ−j

p γp + σ2
W
p γj−p

for j = 0,

for j = 1, 2, . . . ,
for j = −1, −2, . . . .

Theorem 5 (Inverse and Adjugate of a Square Matrix [25]). If A is an invertible square
matrix, its inverse can be represented by

A−1 =

1
det(A)

adj(A),

where adj(A) is the adjugate of matrix A, that is, the transpose of its cofactor matrix.

Proposition 1 (Recursion For Matrix Σp,m). Let the time series {Y1, · · · , Yn} be a causal
AR(p) model. The covariance matrix Σp,m for a ﬁxed p ≥ 1 and m = p + 2, p + 3, . . ., satisﬁes

25

the following recursion

Σp,m =



1

(cid:124)
p,m−1

v



vp,m−1 Σp,m−1 + Vp,m−1



 ,

where v is an (m − 1) × 1 vector given by

(cid:124)
p,m−1 :=
v

(cid:104)
φ(p)
1

· · · φ(p)
p

0 · · · 0

(cid:105)

and Vp,m−1 is a symmetric (m − 1) × (m − 1) matrix deﬁned by

Vp,m−1 := vp,m−1v

(cid:124)
p,m−1.

Proof. From Theorem 1, we know that Σp,m = σ2

W Γ−1
p,m. Furthermore, we have


Γp,m =








γ0
γ1
...

γ1
γ0
...

γm−1 γm−2

· · · γm−1
· · · γm−2
. . .
· · ·

...
γ0
γm−1






=








γ0
γ1
...
γm−1

γ1

· · ·

Γp,m−1








=

(cid:18) γ0

(cid:124)
p,m−1
γp,m−1 Γp,m−1

γ

(cid:19)

,

where γ

(cid:124)

p,m−1 := (cid:2)γ1

· · · γm−1

Γ−1

p,m =

1
kp,m−1





(cid:3). So, Lemma 1 implies that
(cid:124)
p,m−1Γ−1
p,m−1γp,m−1γ

−γ
p,m−1 + Γ−1

p,m−1γp,m−1 kp,m−1Γ−1

p,m−1

1

−Γ−1



 ,

(cid:124)
p,m−1Γ−1

p,m−1

where kp,m−1 = γ0 − γ
of the inverse matrix Γ−1
Γp,m−1vp,m−1 = γp,m−1. More precisely,

(cid:124)
p,m−1Γ−1

p,m−1γp,m−1. Firstly, we simplify Γ−1
p,m. For this purpose, let deﬁne vp,m−1 := Γ−1

p,m−1γp,m−1 appearing in all blocks
p,m−1γp,m−1 or equivalently,








γ0
γ1
...

γ1
γ0
...

γm−2 γm−3








· · · γm−2
· · · γm−3
. . .
· · ·

...
γ0








v1
v2
...
vm−1








=








.








γ1
γ2
...
γm−1

Following 4 as well as the uniqueness of the solution of the above system of linear equations (due
to the invertibility of matrix Γp,m−1), we can conclude that for any m ≥ p + 2, we have

Therefore,

(cid:124)
p,m−1 =
v

(cid:104)

φ(p)
1

· · · φ(p)
p

(cid:105)
0 · · · 0

.

kp,m−1 = γ0 − γ

(cid:124)
p,m−1Γ−1

p,m−1γp,m−1

26

(cid:124)
p,m−1vp,m−1
1 γ1 − · · · − φ(p)

1 γp

= γ0 − γ
= γ0 − φ(p)
W .

(Theorem 4) = σ2

Thus, all these results conclude that

and, equivalently,

Γ−1

p,m =





1
σ2
W

Σp,m =

1

−v

(cid:124)
p,m−1

−vp,m−1 σ2

W Γ−1

p,m−1 + vp,m−1vp,m−1



 ,



1

−v

(cid:124)
p,m−1



−vp,m−1 Σp,m−1 + Vp,m−1,



 .

Proposition 2 (Recursion for Determinant of Autocovariance Matrix). Let the time series
{Y1, · · · , Yn} be a causal AR(p) model with an induced Gaussian white noise series with mean 0
and variance σ2
W . We have the following recursion for the determinant of the autocovariance
matrix:

det(Γp,m) = σ2

W det(Γp,m−1),

for p ≥ 1, m ≥ p + 1.

Proof. The autocovariance matrix Γp,m for some m > p is given by

Γp,m =








γ0
...
γm−2
γm−1

· · · γm−2 γm−1
...
. . .
γ0
· · ·
γ1
· · ·

...
γ1
γ0















=

Γp,m−1

γm−1

· · ·

γ1








.

γm−1
...
γ1
γ0

Let Γp,m(:, j) for j = 1, . . . , m denote the jth column of the autocovariance matrix Γp,m. Motivated
from Theorem 4, performing the following column operation on the last column of Γp,m does not
change the determinant:

Γp,m(:, m) = Γp,m(:, m) − φ1Γp,m(:, m − 1) − · · · − φpΓp,m(:, m − p).

This results in

det(Γp,m) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γm−1
(cid:12)

Γp,m−1

· · ·

γ1

γm−1 − φ1γm−2 − · · · − φpγm−p−1
...
γ1 − φ1γ0 − · · · − φpγp−1
γ0 − φ1γ1 − · · · − φpγp

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

27

Applying Theorem 4 to column m implies that,

det(Γp,m) =

Γp,m−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γm−1
· · ·
(cid:12)
= (−1)m+mσ2
= σ2

0
...
0
γ1 σ2
W
W det(Γp,m−1)

W det(Γp,m−1).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

By considering all these results together, we can now prove Theorem 2.

Proof of Theorem 2

Proof. We prove the nested equation for the matrix NLRCp,m by ﬁxing p and varying m = p +
1, p + 2, . . .. For the initial condition at m = p + 1, we have

NLRCp,p+1(1, 1) = Var(

√

n ˆφ(p+1)

p+1 ) = Var(

√

n(cid:91)PACF1),

where (cid:91)PACF1 is the sample PACF at lag 1. Since the variance of the sample PACF, scaled by
any lag is equal to 1 (cf. Theorem 1), it results in

√

n, at

Now, consider a ﬁxed value of m > p + 1. According to Deﬁnition 6 and Proposition 1, we have

NLRCp,p+1(1, 1) = 1 = (φ(p)

0 )2.

NLRCp,m(2 : m − p, 2 : m − p) = Σp,m(p + 2 : m, p + 2 : m)

(Proposition 1) = Σp,m−1(p + 1 : m − 1, p + 1 : m − 1)

+ Vp,m−1(p + 1 : m − 1, p + 1 : m − 1)

= Σp,m−1(p + 1 : m − 1, p + 1 : m − 1)
= NLRCp,m−1.

Note that the second last equality is due to the structure of matrix Vp,m−1 as deﬁned in Proposi-
tion 1. More precisely, except the p × p block on the top left corner of this matrix, all the other
coordinates are equal to zero, including the (m − p − 1) × (m − p − 1) block on the lower right
corner, that is Vp,m−1(p + 1 : m − p − 1, p + 1 : m − p − 1).
To complete this proof, we only need to show that the nested equations provided for the ﬁrst
column of NLRCp,m hold. For this purpose, we utilize Theorems 1 and 5 in the proof. For a ﬁxed
i ∈ {1, . . . , m − p}, we have

NLRCp,m(i, 1) = Σp,m(p + i, p + 1)
(Theorem 1) = σ2

W Γ−1

p,m(p + i, p + 1)
adj(Γp,m)(p + i, p + 1)
det(Γp,m)

(Theorem 5) = σ2
W

.

(13)

28

The value of adj(Γp,m)(p + i, p + 1) is equal to the coordinate (p + 1, p + i) of the cofactor matrix of
Γp,m. However, as the latter matrix is symmetric, its cofactor matrix is symmetric as well, implying
that both coordinates (p + 1, p + i) and (p + i, p + 1) of the cofactor matrix are identical. This
results in

adj(Γp,m)(p + i, p + 1) = (−1)2p+i+1

(cid:12)
γ0
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γp+i−2
(cid:12)
(cid:12)
γp+i
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γm−1
(cid:12)

γp−1
...
γi−1
γi+1
...

· · ·
. . .
· · ·
· · ·
. . .
· · · γm−p γm−p−2

γp+1
...
γi−3
γi−1
...

γm−1
...

· · ·
. . .
· · · γm−p−i+1
· · · γm−p−i−1
. . .
· · ·

...
γ0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

We ﬁnd this determinant in four cases and consequently derive NLRCp,m(i, 1) for each case:
• Case 1: 1 ≤ i ≤ m − p − 1 and m ≤ 2p + 1. Similar to the proof of Proposition 2, we can add a
weighted sum of some columns to the last column and then utilize Theorem 4 to simplify the result
while keeping the determinant unchanged. However, since m ≤ 2p + 1 and the (p + 1)st column of
the matrix Σp,m has been already removed, implementing such operations on the last column will
result in the coordinates of the removed column multiplied by its corresponding coeﬃcient as in
Theorem 4. More precisely, by adding to the last column the p − 1 preceding columns multiplied
by −φ(p)

p , respectively, and applying Theorem 4, we yield

1 , · · · , −φ(p)

m−p−2, −φ(p)

adj(Γp,m)(p + i, p + 1) = (−1)i+1

m−p, · · · , −φ(p)
(cid:12)
(cid:12)
γ0
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γp+i−2
(cid:12)
(cid:12)
(cid:12)
γp+i
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γm−1
(cid:12)

· · ·
. . .
· · ·

· · ·
. . .
· · ·

γp−1
...
γi−1
γi+1
...

γp+1
...
γi−3
γi−1
...

· · ·
. . .
· · · γm−p γm−p−2

· · ·
. . .
· · · φ(p)

= (−1)m−p+i−1φ(p)

m−p−1

(cid:12)
γ0
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γp+i−2
(cid:12)
(cid:12)
γp+i
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γm−1
(cid:12)

γp−1
...
γi−1
γi+1
...

· · ·
γp
...
. . .
· · ·
γi−2
· · ·
γi
...
. . .
· · · γm−p γm−p−1 + σ2

W /φ(p)

m−p−1 γm−p−2

(cid:12)
φ(p)
(cid:12)
m−p−1γp
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
(cid:12)
φ(p)
m−p−1γi−2
(cid:12)
(cid:12)
φ(p)
(cid:12)
m−p−1γi
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
(cid:12)
m−p−1γm−p−1 + σ2
(cid:12)
W
γm−2

γp+1
. . .
γi−3
γi−1
. . .

· · ·
...
· · ·
γm−p−i
· · · γm−p−i−2
...
· · ·

γ1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

In the last equality, the determinant is multiplied by (−1)m−p−2 due to the m − p − 2 column
exchange operations to bring the last column to the (p + 1)st column. Now, by implementing a
similar operation on the last row we obtain

adj(Γp,m)(p + i, p + 1) = (−1)m−p+i−1φ(p)

m−p−1

29

×

(cid:12)
γ0
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γp+i−2
(cid:12)
(cid:12)
γp+i
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
φ(p)
m−p−iγp+i−1
(cid:12)
= (−1)(m−p+i−1)+(m−p−i−1)φ(p)

· · ·
. . .
· · ·
· · ·
. . .
· · · φ(p)

γp−1
...
γi−1
γi+1
...

m−p−iγi φ(p)

γp
...
γi−2
γi
...

γm−2

γm−p−i
γm−p−i−2

γp+1
. . .
γi−3
γi−1
. . .
m−p−iγi−2

· · ·
...
· · ·
· · ·
...
· · · φ(p)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m−p−iγi−1 + σ2

W /φ(p)

h−p φ(p)

m−p−iγm−p−i−1

×

(cid:12)
γ0
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γp+i−2
(cid:12)
(cid:12)
γp+i−1
(cid:12)
(cid:12)
(cid:12)
γp+i
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γm−2

= φ(p)

m−p−1φ(p)

m−p−i

m−p−1φ(p)

m−p−i

γi−1 + σ2

m−p−i)

γp
...
γi−2
W /(φ(p)
m−p−1φ(p)
γi
...
γm−p−2
γp
...
γi−2
γi−1
γi
...

γp−1
...
γi−1
γi
γi+1
...

· · ·
. . .
· · ·
· · ·
· · ·
. . .
· · · γm−p−1 γm−p−2 γm−p−3

γp+1
. . .
γi−3
γi−2
γi−1
. . .

γm−2

· · ·
...
γm−p−i
· · ·
· · · γm−p−i−1
· · · γm−p−i−2
...
· · ·

γ0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

γp+1
. . .
γi−3
γi−2
γi−1
. . .
γm−p−3
· · ·
...
· · ·
γm−p−i
· · · γm−p−i−1
· · · γm−p−i−2
...
· · ·

γm−2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

γp−1
...
γi−1
γi
γi+1
...

· · ·
. . .
· · ·
· · ·
· · ·
. . .
· · · γm−p−1

W /(φ(p)
σ2

m−p−i)

0
...
0
m−p−1φ(p)
0
...
0

γ0
γp+1
. . .
γi−3
γi−2
γi−1
. . .
γm−p−3

γm−2

· · ·
...
· · ·
γm−p−i
· · · γm−p−i−1
· · · γm−p−i−2
...
· · ·

γ0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

γp−1
...
γi−1
γi
γi+1
...

· · ·
. . .
· · ·
· · ·
· · ·
. . .
· · · γm−p−1
(cid:12)
γ0
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γp+i−2
(cid:12)
(cid:12)
γp+i−1
(cid:12)
(cid:12)
γp+i
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γm−2
(cid:12)
(cid:12)
γ0
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γp+i−2
(cid:12)
(cid:12)
γp+i−1
(cid:12)
(cid:12)
(cid:12)
γp+i
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γm−2

+ φ(p)

m−p−1φ(p)

m−p−i

= φ(p)

m−p−1φ(p)

m−p−idet(Γp,m−1) + σ2

W adj(Γp,m−1)(p + i, p + 1).

This result along with (13) and Proposition 2 imply that,

NLRCp,m(i, 1) = σ2
W

m−p−1φ(p)
φ(p)

m−p−idet(Γp,m−1) + σ2
det(Γp,m)

W adj(Γp,m−1)(p + i, p + 1)

(Proposition 2) = φ(p)

m−p−1φ(p)

m−p−i + σ2
W

adj(Γp,m−1)(p + i, p + 1)
det(Γp,m−1)

= φ(p)

m−p−1φ(p)

m−p−i + NLRCp,m−1(i, 1)

for i = 1, . . . , m − p − 1.

30

• Case 2: i = m − p and m ≤ 2p + 1. Analogous to Case 1, we have

adj(Γp,m)(m, p + 1) = (−1)m−p+1

(cid:12)
γ0
(cid:12)
(cid:12)
γ1
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γm−2
(cid:12)

· · ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

· · ·

γp−1
γp−2
...

γp+1
γp
...

γp+1
γp
...

· · · γm−1
· · · γm−2
. . .
· · ·

· · ·
· · ·
. . .
· · · γm−p−1 γm−p−3
φ(p)
m−p−1γp
φ(p)
m−p−1γp−1
...
m−p−1γm−p−2
γp
γp−1
...

· · ·
. . .
· · · φ(p)

· · ·
· · ·
. . .
· · · γm−p−1 γm−p−2 γm−p−3

...
γ1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

γp−1
γp−2
...

γp+1
γp
. . .

γp−1
γp−2
...

· · ·
. . .
· · · γm−p−1 γm−p−3
(cid:12)
γ0
(cid:12)
(cid:12)
γ1
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γm−2
(cid:12)

m−p−1

= (−1)m−p+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

γ0
γ1
...
γm−2

= (−1)(m−p+1)+(m−p−2)φ(p)

· · · γm−2
· · · γm−3
...
· · ·

γ0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= φ(p)

0 φ(p)

m−p−1det(Γp,m−1).

This result along with (13) and Proposition 2 imply that,

NLRCp,m(m − p, 1) = σ2
W

0 φ(p)
φ(p)

m−p−1det(Γp,m−1)

det(Γp,m)

(Proposition 2) = φ(p)

0 φ(p)

m−p−1.

• Case 3: 1 ≤ i ≤ m − p − 1 and m > 2p + 1. Unlike Case 1, in this case all preceding p columns
of the last column exist in the determinant. So, analogous to the proof of Proposition 2, we add
to the last column the p preceding columns multiplied by −φ1, · · · , −φp, respectively, and apply
Theorem 4 to yield

adj(Γp,m)(p + i, p + 1) = (−1)i+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

γ0
...
γp+i−2
γp+i
...
γm−1

γp−1
...
γi−1
γi+1
...

· · ·
. . .
· · ·
· · ·
. . .
· · · γm−p γm−p−2

γp+1
...
γi−3
γi−1
...

· · ·
0
...
. . .
· · ·
0
· · ·
0
...
. . .
· · · σ2
W

γp−1
...
γi−1
γi+1
...

· · ·
. . .
· · ·
· · ·
. . .
· · · γm−p−1 γm−p−3

γm−2
· · ·
...
. . .
· · ·
γm−p−i
· · · γm−p−i−2
. . .
· · ·

...
γ0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (−1)(i+1)+(2(m−1))σ2
W

γ0
...
γp+i−2
γp+i
...
γm−2
W adj(Γp,m−1)(p + i, p + 1).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= σ2

31

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γp+1
...
γi−3
γi−1
...

This result along with (13), Proposition 2, and Theorem 1 imply that,

NLRCp,m(i, 1) = σ2
W

(Proposition 2) = σ2
W

σ2
W adj(Γp,m−1)(p + i, p + 1)
det(Γp,m)

adj(Γp,m−1)(p + i, p + 1)
det(Γp,m)

(Theorem 1) = NLRCp,m−1(i, 1)

for i = p + 2, . . . , m − p − 1.

• Case 4: i = m − p and m > 2p + 1. Analogous to Cases 2 and 3, we obtain

adj(Γp,m)(m, p + 1) = (−1)m−p+1

= (−1)m−p+1

= 0.

(cid:12)
γ0
(cid:12)
(cid:12)
γ1
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γm−2
(cid:12)
(cid:12)
γ0
(cid:12)
(cid:12)
γ1
(cid:12)
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
γm−2
(cid:12)

γp+1
γp
...

γp−1
γp−2
...

· · ·
· · ·
. . .
· · · γm−p−1 γm−p−3
· · ·
· · ·
. . .
· · · γm−p−1 γm−p−3

γp−1
γp−2
...

γp+1
γp
...

· · · γm−1
· · · γm−2
. . .
· · ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

...
γ1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

· · · 0
· · · 0
...
. . .
· · · 0

This result along with (13) imply that,

NLRCp,m(m − p, 1) = 0.

Now, by starting with the scaler NLRCp,p+1(1, 1) = 1 and implementing the nested equations
iteratively on m, the coordinates of the matrix NLRCp,m are accordingly derived.

A.2 Proof of Theorem 3

Proof. We prove this theorem by ﬁxing p and utilizing Deﬁnition 5 and Theorem 2 in three cases:
• Case 1: m = p + 1.

σ2
p,p+1 = Var(

√

√

= Var(

n ¯φp,p+1)
n ˆφ(p+1)
p+1 )
= NLRCp,p+1(1, 1)
= (φ(p)

0 )2.

• Case 2: p + 1 < m ≤ 2p.

(m − p)2σ2

p,m = (m − p)2Var(

(Deﬁnition 5) = (m − p)2Var(

√

n ¯φp,m)
1
m − p

m
(cid:88)

√

= Var(

n ˆφ(m)

p+1) + Var(

√

n ˆφ(m)
i

)

√

n ˆφ(m)
i

) + 2Cov(

√

n ˆφ(m)
p+1,

m
(cid:88)

√

n ˆφ(m)
i

)

i=p+1
m
(cid:88)

i=p+2

i=p+2

32

√

= Var(

n ˆφ(m)

p+1) +





m
(cid:88)

√

Var(

n ˆφ(m)
i

) + 2

m
(cid:88)

i−1
(cid:88)

Cov(

√

n ˆφ(m)
i

,

√

n ˆφ(m)
j

i=p+2

i=p+3

j=p+2



)


+ 2

m
(cid:88)

i=p+2

√

Cov(

n ˆφ(m)
p+1,

√

n ˆφ(m)
i

)

(Theorem 2) = NLRCp,m(1, 1) +





m−p
(cid:88)

NLRCp,m(i, i) + 2



NLRCp,m(i, j)



m−p
(cid:88)

i−1
(cid:88)

i=3

j=2

i=2

NLRCp,m(i, 1)

+ 2

m−p
(cid:88)

i=2

NLRCp,m−1(i − 1, j − 1)





m−p
(cid:88)

i−1
(cid:88)

i=3

j=2

(Theorem 2) =

=





m−p
(cid:88)





m−p−1
(cid:88)

i=1
(cid:32)

NLRCp,m−1(i − 1, i − 1) + 2

i=2
(cid:32)

+

NLRCp,m(1, 1) + 2

(cid:33)

NLRCp,m(i, 1)

m−p
(cid:88)

i=2

NLRCp,m−1(i, i) + 2

m−p−1
(cid:88)

i−1
(cid:88)

i=2

j=1



NLRCp,m−1(i, j)



+

NLRCp,m(1, 1) + 2

(cid:33)

NLRCp,m(i, 1)

m−p
(cid:88)

i=2

(Theorem 2) =





m−1
(cid:88)

√

Var(

i=p+1

n ˆφ(m−1)
i

) + 2

m−1
(cid:88)

i−1
(cid:88)

i=p+2

j=p+1

√

Cov(

n ˆφ(m−1)
i

,

√



n ˆφ(m−1)
j

)


(cid:32)m−p−1
(cid:88)

(φ(p)

k )2 + 2

m−p
(cid:88)

m−p−i
(cid:88)

(cid:33)

k φ(p)
φ(p)

k+i−1

k=0

m−1
(cid:88)

√

i=2



n ˆφ(m−1)
i

 +
)

k=0
(cid:32)m−p−1
(cid:88)

(φ(p)

k )2 + 2

m−p−1
(cid:88)

m−p−1−i
(cid:88)

(cid:33)

k φ(p)
φ(p)

k+i

+



=

Var(

i=p+1

k=0

i=1

k=0

= (m − p − 1)2σ2

p,m−1 + (φ(p)

0 + · · · + φ(p)

m−p−1)2.

• Case 3: m ≥ 2p + 1. The proof of this case is analogous to Case 2. To avoid duplication, it is
omitted.
Now, it is readily seen that by starting from Case 1 and implementing the recursion iteratively, the
provided general solution is derived.

References

[1] M. Abolghasemi, A. Eshragh, J. Hurley, and B. Fahimnia, Demand Forecasting in the Pres-
ence of Systematic Events: Cases in Capturing Sales Promotions, International Journal of
Production Economics, 230:107892, 2020.

33

[2] H. Akaike, A New Look at the Statistical Model Identiﬁcation, IEEE Transactions on Auto-

matic Control, 19(6):716–723, 1974.

[3] K. Avrachenkov, A. Eshragh and J. Filar, Markov Chains and Hamiltonian Transition Matri-
ces, Proceedings of the 5th International ICST Conference on Performance Evaluation Method-
ologies and Tools, Paris, France, 2011.

[4] K. Avrachenkov, A. Eshragh and J. Filar, On Transition Matrices of Markov Chains Corre-

sponding to Hamiltonian Cycles, Annals of Operations Research, 243(1):19-35, 2016.

[5] N.G. Bean, R. Elliott, A. Eshragh and J.V. Ross, On Binomial Observation of Continuous-

Time Markovian Population Models, Journal of Applied Probability, 52:457-472, 2015.

[6] N.G. Bean, A. Eshragh and J.V. Ross, Fisher Information for a Partially-Observable Simple
Birth Process, Communications in Statistics: Theory and Methods, 45(24):7161-7183, 2016.

[7] G.E.P. Box and G.M. Jenkins, Time Series Analysis, Forecasting and Control, Holden-Day,

San Francisco, 1976.

[8] P.J. Brockwell and R.A. Davis, Time Series: Theory and Methods, Springer Series in Statistics,

Springer, 2009.

[9] P.M.T. Broersen, The Best Order of Long Autoregressive Models For Moving Average Esti-

mation, Technical Report, 1996.

[10] P.M.T. Broersen, Autoregressive Model Orders for Durbin’s MA and ARMA Estimators, IEEE

Transactions on Signal Processing, 48(8):2454–2457, 2000.

[11] B. Choi, ARMA Model Identiﬁcation, Springer Series in Statistics, Springer, 1992.

[12] J. Durbin, Estimation of Parameters in Moving-Average Models, Biometrika, 46(3/4), 306–

316, 1959.

[13] J. Durbin, The Fitting of Time Series Models, Review of the International Statistical Institute,

28(3):233–244, 1960.

[14] A. Eshragh and M. Modarres, A New Approach to Distribution Fitting: Decision on Beliefs,

Journal of Industrial and Systems Engineering, 3(1):56-71, 2009.

[15] A. Eshragh, J. Filar and A. Nazari, A Projection-Adapted Cross Entropy (PACE) Method for

Transmission Network Planning, Energy Systems, 2(2):189-208, 2011.

[16] A. Eshragh and J. Filar, Hamiltonian Cycles, Random Walks and the Geometry of the Space
of Discounted Occupational Measures, Mathematics of Operations Research, 36(2):258-270,
2011.

[17] A. Eshragh, J. Filar and M. Haythorpe, A Hybrid Simulation-Optimization Algorithm for the

Hamiltonian Cycle Problem, Annals of Operations Research, 189:103-125, 2011.

[18] A. Eshragh, Fisher Information, Stochastic Processes and Generating Functions, Proceedings of
the 21st International Congress on Modeling and Simulation, Gold Coast, Australia, December
2015.

34

[19] A. Eshragh, F. Roosta, A. Nazari and M. Mahoney, LSAR: Eﬃcient Leverage Score Sampling
Algorithm for the Analysis of Big Time Series Data, (arXiv preprint arXiv:1911.12321).

[20] A. Eshragh, S. Alizamir, P. Howley and E. Stojanovski, Modeling the Dynamics of the COVID-
19 Population in Australia: A Probabilistic Analysis, PLOS-One, 15(10):e0240153, 2020.

[21] A. Eshragh, B. Ganim, T. Perkins, and K. Bandara, The Importance of Environmental Fac-
tors in Forecasting Australian Power Demand, Environmental Modeling & Assessment (To
Appear).

[22] B. Fahimnia, J. Sarkis, A. Choudhary and A. Eshragh, Tactical Supply Chain Planning Under
a Carbon Tax Policy Scheme: A Case Study, International Journal of Production Economics,
164:206-215, 2015.

[23] B. Fahimnia, J. Sarkis and A. Eshragh, A Trade-oﬀ Model for Green Supply Chain Planning:

A Leanness-Versus-Greenness Analysis, OMEGA, 54:173-190, 2015.

[24] B. Fahimnia, H. Davarzani and A. Eshragh, Performance Comparison of Three Meta-Heuristic
Algorithms for Planning of a Complex Supply Chain, Computers and Operations Research,
89:241-252, 2018.

[25] G.H. Golub and C.F. Van Loan, Matrix Computations, Johns Hopkins University Press, 1983.

[26] D.J. Hamilton, Time Series Analysis, Princeton University Press, New Jersey, 1994.

[27] E.J. Hannan and J. Rissanen, Recursive Estimation of Mixed Autoregressive-Moving Average

Order, Biometrika, 69(1):81–94, 1982.

[28] F. Iravani, S. Alizamir, A. Eshragh and K. Bandara, An Interpretable Machine Learning
Approach to Predicting Customer Behavior on JD.Com, Technical Report (Available at SSRN).

[29] H. Mahlooji, A. Eshragh, H. Abouee Mehrizi and N. Izady, Uniform Fractional Part: A Simple
Fast Method for Generating Continuous Random Variates, Scientia Iranica, 15(5):613-622,
2008.

[30] R.H. Shumway and D.S. Stoﬀer, Time Series Analysis and Its Applications, Springer, London,

2017.

[31] Q. Song and A.O. Esbogue, A New Algorithm for Automated Box-Jenkins ARMA Time Se-
ries Modelling Using Residual Autocorrelation/Partial Autocorrelation Functions, Industrial
Engineering and Management Systems, 5(2):00–00, 2006.

35

