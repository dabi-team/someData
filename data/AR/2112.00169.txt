3D Photo Stylization:
Learning to Generate Stylized Novel Views from a Single Image

Fangzhou Mu1* Jian Wang2‚Ä† Yicheng Wu2‚Ä† Yin Li1‚Ä†
1University of Wisconsin-Madison 2Snap Research

1{fmu2, yin.li}@wisc.edu

2{jwang4, yicheng.wu}@snap.com

1
2
0
2
c
e
D
4

]

V
C
.
s
c
[

2
v
9
6
1
0
0
.
2
1
1
2
:
v
i
X
r
a

Figure 1. 3D photo stylization. Given a single content image, our method synthesizes novel views of the scene in an arbitrary style. In
doing so, our method delivers immersive viewing experience of a memorable moment within existing photos.

Abstract

1. Introduction

Visual content creation has spurred a soaring interest
given its applications in mobile photography and AR / VR.
Style transfer and single-image 3D photography as two rep-
resentative tasks have so far evolved independently. In this
paper, we make a connection between the two, and address
the challenging task of 3D photo stylization ‚Äî generating
stylized novel views from a single image given an arbitrary
style. Our key intuition is that style transfer and view syn-
thesis have to be jointly modeled for this task. To this end,
we propose a deep model that learns geometry-aware con-
tent features for stylization from a point cloud representa-
tion of the scene, resulting in high-quality stylized images
that are consistent across views. Further, we introduce a
novel training protocol to enable the learning using only
2D images. We demonstrate the superiority of our method
via extensive qualitative and quantitative studies, and show-
case key applications of our method in light of the growing
demand for 3D content creation from 2D image assets.1

*Work partially done when Fangzhou was an intern at Snap Research
‚Ä†co-corresponding authors
1Project page: http://pages.cs.wisc.edu/Àúfmu/style3d

Given an input content image and a reference style im-
age, neural style transfer [4, 13, 14, 16, 22, 25, 33, 36, 41, 49]
creates a novel image that ‚Äúpaints‚Äù the content with the
style. Despite a high quality stylized image, the result is
limited to the same viewpoint of the content image. What
if we can render stylized images from different views? See
Fig. 1 for examples. When displayed with parallax, this ca-
pacity will provide drastically more immersive visual expe-
rience for 2D images, and support the application of interac-
tive browsing of 3D photos on mobile and AR/VR devices.
In this paper, we address the new task of generating styl-
ized images of novel views from a single input image and
an arbitrary reference style image, as illustrated in Fig 1.
We refer to this task as 3D photo stylization ‚Äî a marriage
between style transfer and novel view synthesis.

3D photo stylization has several major technical barriers.
As observed in [21], directly combining existing methods of
style transfer and novel view synthesis yields blurry or in-
consistent stylized images, even with dense 3D geometry
obtained from structure from motion and multi-view stereo.
This challenge is further manifested with a single content

1

Single content image + styleStylized view 1Stylized view 2 
 
 
 
 
 
image as the input, where a method must resort to monocu-
lar depth estimation with incomplete and noisy 3D geome-
try, leading to holes and artifacts when synthesizing stylized
images of novel views. In addition, training deep models for
this task requires a large-scale dataset of diverse scenes with
dense geometry annotation that is currently lacking.

To bridge this gap, we draw inspiration from one-shot
3D photography [29, 40, 50], and adopt a point cloud based
scene representation [21, 40, 57]. Our key innovation is
a deep model that learns 3D geometry-aware features on
the point cloud without using 2D image features from the
content image for rendering novel views with a consistent
style. Our method accounts for the input noise from depth
maps, and jointly models style transfer and view synthe-
sis. Moreover, we propose a novel training scheme that
enables learning our model using standard image datasets
(e.g., MS-COCO [34]), without the need of multi-view im-
ages or ground-truth depth maps.

Our contributions are summarized into three folds. (1)
We present the Ô¨Årst method to address the new task of 3D
photo stylization ‚Äî synthesizing stylized novel views from
a single content image with arbitrary styles. (2) Unlike pre-
vious methods, our method learns geometry-aware features
on a point cloud without using 2D content image features
and from only 2D image datasets. (3) Our method demon-
strates superior qualitative and quantitative results, and sup-
ports several interesting applications.

2. Related work

Neural Style Transfer. Neural style transfer has received
considerable attention. Image style transfer [12, 13] renders
the content of one image in the style of another. Video style
transfer [48] injects a style to a sequence of video frames
to produce temporally consistent stylization, often by en-
forcing smoothness constraint on optical Ô¨Çow [3, 20, 48, 55]
or in the feature space [9, 36]. Our method faces the same
challenge as video style transfer; that the style must be con-
sistent across views. However, our task of 3D photo styl-
ization is more challenging, as it requires the synthesis of
novel views and a consistent style among all views.

Technically, early methods formulate style transfer as
a slow iterative optimization process [12, 13]. Fast feed-
forward models later perform stylization in a single for-
ward pass, but can only accommodate one [25, 54] or a
few styles [4, 10]. Most relevant to our work are methods
that allow for the transfer of arbitrary styles while retain-
ing the efÔ¨Åciency of a feed-forward model [6, 22, 33]. Our
style transfer module builds on Liu et al. [36], extending an
attention-based method to support arbitrary 3D stylization.
Novel View Synthesis from a Single Image. Novel view
synthesis from a single image, also known as one-shot 3D
photography, has seen recent progress thanks to deep learn-
ing. Existing approach can be broadly classiÔ¨Åed as end-

to-end models [7, 18, 31, 46, 52, 53, 57, 60] and modular
systems [24, 29, 40, 50]. End-to-end methods often fail to
recover accurate scene geometry and have difÔ¨Åculty gen-
eralizing beyond the scene categories present in training.
Hence, our method builds on modular systems.

Modular systems for one-shot 3D photography combine
depth estimation [44,45,59] and inpainting models [35], and
have demonstrated strong results for in-the-wild images.
Niklaus et al. [40] maintains and rasterizes a point cloud
representation of the scene to synthesize 3D Ken Burns ef-
fect. Later methods [29, 50] improve on synthesis quality
via local content and depth inpainting on a layered depth
image (LDI) of the scene. Jampani et al. [24] further intro-
duces soft scene layering to better preserve appearance de-
tails. Our work is closely related to Shih et al. [50]. We ex-
tend their LDI inpainting method for point cloud, and lever-
age their system to generate ‚Äúpseudo‚Äù views during training.
Our method also uses the differentiable rasterizer from [40].
3D Stylization. There has been a growing interest in the
stylization of 3D content for creative shape editing [2, 58],
visual effect simulation [17], stereoscopic image editing [5,
15] and novel view synthesis [8, 21]. Our method falls in
this category and is most relevant to stylized novel view
synthesis [8, 21]. The key difference is that our method
generates stylized novel views from a single image, while
previous methods need hundreds of calibrated views as in-
put. Another difference is that our model learns 3D geom-
etry aware features on a point cloud. In contrast, Huang et
al. [21] back-projects 2D image features to 3D space with-
out accounting for scene geometry. While their point ag-
gregation module enables post hoc processing of image-
derived features, the point features remain 2D, leading to vi-
sual artifacts and inadequate stylization in renderings. Our
work is also related to point cloud stylization e.g., PSNet [2]
and 3DStyleNet [58]. Both our method and [2,58] use point
cloud as the representation. The difference is that point
cloud is an enabling device for stylization and view synthe-
sis in our method, and not as the end product as in [2, 58].
Deep Models for Point Cloud Processing. Many deep
models have been developed for point cloud processing.
Among the popular architectures are models of set based
[42, 43], graph convolution based [30, 56] and point con-
volution based [19, 51]. Our model extends a graph based
model [56] to handle dense point clouds (one million points)
for high quality stylization.

3. 3D Photo Stylization

Given a single input content image and an arbitrary style
image, the goal of 3D photo stylization is to generate styl-
ized novel views of the content image. The key of our
method is the learning of 3D geometry aware content fea-
tures directly from a point cloud representation of the scene
for high-quality stylization that is consistent across views.

2

Figure 2. Method overview. Central to our method is a point cloud based scene representation that enables geometry-aware feature
learning, attention-based feature stylization and consistent stylized renderings across views. SpeciÔ¨Åcally, we Ô¨Årst construct an RGB point
cloud from the content image and its estimated depth map. Content features are then extracted directly from the point cloud and stylized
given an image of the reference style. Finally, the stylized point features are rendered to novel views and decoded into stylized images.

In this section, we describe our workÔ¨Çow at inference time.
Method Overview. Fig. 2 presents an overview of our
method. Our method starts by back-projecting the input
content image into an RGB point cloud using its estimated
depth map. The point cloud is further ‚Äúinpainted‚Äù to cover
dissoccluded parts of the scene and then ‚Äúnormalized‚Äù (Sec-
tion 3.1). An efÔ¨Åcient graph convolutional network is de-
signed to process the point cloud and extract 3D geom-
etry aware features on the point cloud, leading to point-
wise features tailored for 3D stylization (Section 3.2). A
style transfer module is subsequently adapted to modulate
those point-wise features using the input style image (Sec-
tion 3.3). Finally, a differentiable rasterizer projects the fea-
turized points to novel views for the synthesis of stylized
images that are consistent across views (Section 3.4).

3.1. Point Cloud Construction

Our method starts by lifting the content image into an
RGB point cloud, and further normalizes the point cloud to
account for scale ambiguity and uneven point density.
Depth Estimation and Synthesis of Hidden Geometry.
Our method Ô¨Årst estimates a dense depth map using an
off-the-shelf deep model for monocular depth estimation
(LeReS [59]). A key challenge for single-image novel view
synthesis is the occlusion in the scene. A dense depth map
might expose many ‚Äúholes‚Äù when projected to a different
view. Inpainting the occluded geometry is thus critical for
view synthesis. To this end, we further employs the method
of Shih et al. [50] for the synthesis of occluded geometry
on a layered depth image (LDI). Thanks to the duality be-
tween point cloud and LDI, we map the LDI pixels to an
RGB point cloud via perspective back-projection.
Point Cloud Normalization. In light of scale ambiguity
and uneven point density characteristic of image-derived
point clouds, we transform them into Normalized Device
Coordinate (NDC) [38] before further processing. The re-
sulting points fall within the [‚àí1, 1] cube with density ad-
justed accordingly to account for perspectivity. As shown

Figure 3. Effect of point cloud normalization. Model without
normalization (-) performs poorly due to scale ambiguity in depth
estimation and non-uniformity in point distribution. In contrast,
model with normalization (+) captures Ô¨Åne appearance detail and
produces strong stylization irrespective of depth estimator in use.

in Fig 3, this simple procedure is crucial for our method to
generalize across scene categories, and allows us to switch
to different depth estimators without re-training our model.

3.2. Encoding Features on Point Cloud

Our next step is to learn features amenable to styliza-
tion. While virtually all existing style transfer algorithms
make use of ImageNet pre-trained VGG features, we found
that associating 3D points with back-projected VGG fea-
tures (such as in Huang et al. [21]) is sub-optimal for styl-
ized novel view synthesis, leading to geometric distortion
and structural artifacts as shown in our ablation. We argue
that features from a network pre-trained on 2D images are
incompetent to describe the intricacy of 3D geometry. This
leads us to design an efÔ¨Åcient graph convolutional network
(GCN) that learns geometry aware features directly from an
RGB point cloud, as opposed to using 2D image features.
EfÔ¨Åcient GCN. One common drawback for GCN architec-
tures lies in their scalability. Existing GCNs are designed
for points clouds with a few thousand points [30], whereas
an image at 1K resolution results in one million points af-
ter inpainting. To bridge this gap, we propose a highly
efÔ¨Åcient GCN encoder by drawing strength from multiple
point-based network architectures.

3

ContentDepthPoint Cloud Construction(Section 3.1)Geometry-aware Feature Encoding(Section 3.2)3DPhotoEncodeStyleStylizePoint Feature Stylization(Section 3.3)RenderView 1View NStylized Neural Rendering(Section 3.4)Point cloud based scene representation‚Ä¶Content + styleLeReSDPT++--Figure 4. Components of our deep model. Our model includes three modules ‚Äî a point cloud encoder, a stylizer and a neural renderer.
The encoder applies MRConvs [30] along with farthest point sampling to embed and sub-sample the input RGB point cloud. The stylizer
computes attention between the embedded content and style features, and uses attention-weighted afÔ¨Åne transformation to modulate the
content features for stylization. The neural render consists of a rasterizer that anti-aliases the modulated point features and projects them
to novel views, and a U-Net [47] that reÔ¨Ånes the resulting 2D feature maps and decodes them into stylized images.

Our GCN encoder adopts the max-relative convolu-
tion [30] for its computational and memory efÔ¨Åciency. To
further improve the efÔ¨Åciency, we replace the expensive dy-
namic k-NN graphs with radius-based ball queries [43] for
point aggregation. Moreover, we follow the hierarchical de-
sign of VGG network by repeatedly sub-sampling the point
cloud via farthest point sampling, as opposed to maintain-
ing the full set of points throughout the model [30]. We
illustrate our encoder design in Fig. 4. The output of our
encoder is a sub-sampled, featurized point cloud.

3.3. Stylizing the Point Cloud

Going further, our model injects style into the content
features. The technical barrier here is the misalignment of
content and style features, as the former are deÔ¨Åned on a
3D point cloud while the latter (from a pre-trained VGG
network) lie in a 2D plane. To address this discrepancy, we
make use of learned feature mappings and Adaptive Atten-
tion Normalization (AdaAttN) [36] to match and combine
the content and style features. Let Fc be the point-wise con-
tent features and Fs the style features on a 2D grid. Our
style transfer operation is given by

Fcs = œà(AdaAttN(œÜ(Fc), Fs)),

(1)

where œÜ and œà, implemented as point-wise multi-layer per-
ceptrons (MLPs), are learned mappings between the con-
tent and style feature spaces, and AdaAttN is the attention-
weighted adaptive instance normalization from [36].
AdaAttN computes attention between every content feature
(a point) and each style feature (a pixel), and uses the at-
tention map to modulate the afÔ¨Åne parameters within the
instance normalization applied on content features. As a re-
sult, Fcs incorporates both content and style, and will be
further used to render stylized images.

decoding the projected features into an image using a 2D
convolutional network.
Feature Rasterization. Our rasterizer follows Niklaus et
al. [40], and projects the point cloud features Fcs into a
single-view 2D feature map F2d. There is one important dif-
ference: we up-sample Fcs using inverse distance weighted
interpolation [43] before rasterization. This is reminiscent
of super-sampling ‚Äî a classical anti-aliasing technique in
graphics. In doing so, we grant more Ô¨Çexibility for decod-
ing the projected features into stylized images.
Image Decoding. Our decoder further maps the 2D fea-
ture map F2d to a stylized RGB image at input resolution.
The decoder is realized using a 2D convolutional network,
following the architecture of U-Net [47], with transposed
convolutions at the entry of each stage for up-sampling.

4. Learning from 2D Images

We now present our training scheme. Our model is

trained using 2D images following a two-stage approach.
Generating Multi-view Images for Training. Training
our model requires images from multiple views of the
same scene. Unfortunately, a large-scale multi-view im-
age dataset with a diverse set of scenes is lacking. To
bridge this gap, we propose to learn from the results of ex-
isting one-shot 3D photography methods. Concretely, we
use 3DPhoto [50] to convert images from a standard dataset
(MS-COCO) into high-quality 3D meshes, from which we
synthesize arbitrary pseudo target views to train our model.
In doing so, our model learns from a diverse collection of
scenes present in MS-COCO. Learning from synthesized
images leads to an inevitable bias residing in 3DPhoto re-
sults in trade of dataset diversity. Through our experiments,
we show that our model generalizes well across a large set
of in-the-wild images at inference time.

3.4. Stylized Neural Rendering

4.1. Two-Stage Training

Our Ô¨Ånal step is to render stylized point features Fcs into
stylized images with speciÔ¨Åed viewpoints. As illustrated in
Fig 4, this is accomplished by (1) projecting point features
to an image plane given camera pose and intrinsics; and (2)

The training of our model is divided into a view synthesis
stage where the model learns 3D geometry aware features
for novel view synthesis, and a stylization stage where the
model is further trained for novel view stylization.

4

dim = 64dim = 128dim = 256dim = 3MRConv√ó1MRConv√ó2MRConv√ó2UpProjectdim = 25625625625612812864641√ó1 Conv1√ó1 Conv64rStylefeaturesPoint cloud encoder (Sec 3.2)Stylizer(Sec 3.3)Neural renderer (Sec 3.4)AdaAttNRGBpoint cloud2D feature mapsFeaturized point cloudFeaturized point cloudOutputFigure 5. Depth estimation fails on stylized images. One alternative to 3D photo stylization is to combine stylized content image
and its depth estimate. Unfortunately, strong depth estimators such as DPT [44] and LeReS [59] fail on image style transfer output from
AdaIN [20], LST [32] and AdaAttN [36] because stylized images do not follow natural image statistics.

Figure 6. 3D photo of a stylized content image manifests ubiquitous visual artifacts. Another alternative to stylizing 3D photos is to
combine stylized content image with depth estimate from the original content image. While depth estimation is unaffected, the style effect
bleeds through depth discontinuities. 3D photo inpainting thus fails, with ubiquitous visual artifacts (red arrows) in novel view renderings.

Enforcing Multi-view Consistency. A key technical con-
tribution of our work is a multi-view consistency loss.
Building a point cloud representation of the input content
image allows us to impose additional constraint on pixel
values of the rendered images.2 The key idea is that a scene
point p in the point cloud P should produce the same pixel
color in the views to which it is visible. To this end, we
deÔ¨Åne our consistency loss as

Lcns =

(cid:88)

(cid:88)

p‚ààP

i,j‚ààV

V(p; i, j) ¬∑ (cid:107)Ii(œÄi(p)) ‚àí Ij(œÄj(p))(cid:107)1,

(2)

where V is the set of sampled views, Ii the rendered image
from view i, œÄi(¬∑) the projection to view i, and V(p; ¬∑, ¬∑) a
visibility function which evaluates to 1 if p is visible to both
views and 0 otherwise. Computing the loss incurs minimal
overhead since the evaluation of œÄ and V is part of rasteriza-
tion. As evidenced by our ablation study, our proposed loss
signiÔ¨Åcantly improves consistency of stylized renderings.
View Synthesis Stage. We Ô¨Årst train our model for view
synthesis, a surrogate task that drives the learning of geom-
etry aware content features. Given an input image, we ran-
domly sample novel views of the scene and ask the model
to reconstruct them. To train our model, we make use of
an L1 loss Lrgb deÔ¨Åned on pixel values, a VGG perceptual
loss Lf eat deÔ¨Åned on network features, and our multi-view

2While the sharing of a featurized point cloud entails multi-view con-
sistency of rasterized feature maps, the features are subject to a learnable
decoding process, through which inconsistency will be introduced.

consistency loss Lcns. The overall loss function is

Lview = Lrgb + Lf eat + Lcns,

(3)

Stylization Stage. Our model learns to stylize novel views
in the second stage. We freeze the encoder for content fea-
ture extraction, train the stylizer, and Ô¨Åne-tune the neural
renderer. This is done by randomly sampling novel views
of the scene and style images from WikiArt [39], and train-
ing our model using

Lstyle = Ladaattn + Lcns,

(4)

where Ladaattn is the same AdaAttN loss from [36] and
Lcns is again our multi-view consistency loss.
Training Details. For view synthesis, we train for 20K iter-
ations (2 epochs) on MS-COCO with a batch size of 8 using
Adam [26] and set the learning rate to 1e-4. We apply the
same training schedule for stylization.

5. Experiments

We now present the main results of our paper and leave

additional results to the supplementary material.

5.1. Qualitative results

By permuting the steps of (1) depth estimation, (2) in-
painting, (3) rendering and (4) style transfer, one could
imagine two alternative workÔ¨Çows that combine existing
models for 3D photo stylization. To compare them with
our method, we instantiate these baselines by combining six

5

Content + styleAdaINLSTAdaAttNDPTLeReSDPTDPTDPTLeReSDepthLeReSLeReSInput viewAdaINLSTAdaAttNContent + styleOursInput viewNovel viewFigure 7. Stylizing rendered images from a 3D photo introduces inconsistency in stylization. A third baseline is to na¬®ƒ±vely build
a 3D photo from the raw content image, then stylize its renderings either one view at a time (e.g., using LST [32] or AdaAttN [36]) or
collectively as a video (e.g., using ReReVST [55] or the video variant of AdaAttN). Despite stronger results than the other two baselines,
the stylization is agnostic to the scene geometry shared by all views and thus produces inconsistent results (yellow arrows).

different style transfer methods (AdaIN [22], LST [32] and
AdaAttN [36] for image style transfer, and ReReVST [55],
MCC [9] and the video variant of AdaAttN for video
style transfer) with DPT [44] for depth estimation and
3DPhoto [50] for inpainting and rendering. Results are
created using images from Unsplash [1], a free-licensed,
professional-grade dataset of in-the-wild images.

(1) Style ‚Üí Depth ‚Üí Inpainting ‚Üí Rendering: While
geometric consistency is granted, depth estimation fails
catastrophically on stylized images (Figure 5). One may
alternatively back-project a stylized image using depth es-
timation from the raw input. Despite better geometry, in-
painting remains error-prone due to color bleed-through and
shift in color distribution caused by stylization (Figure 6).

(2) Depth ‚Üí Inpainting ‚Üí Rendering ‚Üí Style: This
baseline often produces inconsistent stylization across
views (Figure 7), as each view‚Äôs style is independent and
agnostic to the underlying scene geometry.

In contrast, our method manages to generate high-quality
stylized renderings free of visual artifacts and inconsistency.

The second baseline produces gentle inconsistency under
small viewpoint change typical to 3D photo browsing. This
is more benign than the visual artifacts produced by the Ô¨Årst
baseline. We further compare our method with the second
baseline via quantitative experiments and a user study.

5.2. Quantitative results

Given that evaluation of style quality is a very subjec-
tive matter, we defer it to the user study and focus on the
evaluation of consistency in our quantitative experiments.
Evaluation Protocol and Metrics. We run our method and
the baseline on ten diverse content images from the web and
40 styles sampled from the compilation of Gao et al. [11].
The baseline, as discussed before, runs 3DPhoto to synthe-
size plain novel-view images, then stylizes them using one
of the six style transfer algorithms. Ultimately, this results
in 400 stylized 3D photos from each of the seven candidate
methods. To quantify inconsistency between a pair of styl-
ized views, we warp one view to the other according to the
point cloud based scene geometry, and compute RMSE and

6

Content + styleLSTAdaAttN (image)ReReVSTAdaAttN (video)OursInput view(1)Novel view(2)Warp(1‚Üí2)(2)StylizationConsistencyüó∏‚®Øüó∏‚®Øüó∏‚®Ø‚®Øüó∏üó∏üó∏Zoom-inMethod

RMSE LPIPS

AdaIN [22]
LST [32]
AdaAttN (image) [36]
ReReVST [55]
MCC [9]
AdaAttN (video) [36]

0.222
0.195
0.187
0.115
0.092
0.135
0.086

0.304
0.287
0.329
0.213
0.200
0.209
0.133

3DPhoto [50] ‚Üí

Ours

Table 1. Results on consistency. We compare our model against
baselines that sequentially combine 3DPhoto and image/video
style transfer on consistency using RMSE (‚Üì) and LPIPS (‚Üì).

Figure 8. User study. We conduct a user study to compare
our method against baselines that sequentially combine 3DPhoto
and image/video style transfer. Methods are evaluated on (a) style
quality, (b) multi-view consistency and (c) overall synthesis qual-
ity. Results show percentage of users voting for an algorithm.

the masked LPIPS metric as deÔ¨Åned in Huang et al. [21].
We average the result over 400 pairs of views for each styl-
ized 3D photo and report the mean over all available photos.
Results. Our results are summarized in Table 1. Our
method outperforms all six instantiations of the baseline
by a signiÔ¨Åcant margin in terms of both RMSE and LPIPS.
Not surprisingly, video style transfer methods produce more
consistent results than image style transfer methods ow-
ing to their extra smoothness constraint. The fact that
our method performs even better without such a constraint
shows the effectiveness of maintaining a central featurized
point cloud for 3D photo stylization.

5.3. User study

Going further, we conduct a user study to better under-
stand the perceptual quality of stylized images produced by
our method and the baselines. Our study includes three sec-
tions for the assessment of style quality, multi-view consis-
tency and overall synthesis quality. Our analysis is based on
5,400 votes from 30 participants. We elaborate on our study

7

Figure 9. Effect of geometry-aware feature learning. 3D photo
stylization with back-projected 2D VGG features suffers from ge-
ometric distortion (yellow arrows) and visual artifacts (red boxes).
In contrast, our geometry-aware learning scheme better maintains
content structure and produces more pleasant texture.

Training stage

ViewSyn
‚àí
+
‚àí
+

Stylize
‚àí
‚àí
+
+

RMSE LPIPS

0.113
0.109
0.081
0.086

0.199
0.190
0.132
0.128

Table 2. Effect of consistency loss. We compare models trained
with (+) or without (-) the loss using RMSE (‚Üì) and LPIPS (‚Üì).

design in the supplementary material.
Results. We visualize the results in Figure 8. For style
quality, our method is consistently rated better than the al-
ternatives, with the only exception being LST, which our
method is on par with. Not coincidentally, our method ex-
cels at multi-view consistency, harvesting an overwhelm-
ing 95 percent of the votes in four of the six tests. Finally,
our method remains the most preferred for overall synthe-
sis quality, beating all alternatives by a large gap. Putting
things together, our results provide solid validation on the
strength of our approach in producing high-quality styliza-
tion that is consistent across views.

5.4. Ablation studies

Effect of Geometry-aware Feature Learning. We study
the strength of geometry-aware feature learning. SpeciÔ¨Å-
cally, we construct a variant of our model with the only dif-
ference that content features are not learned on the point
cloud, but rather come from a pre-trained VGG network
as in 2D style transfer methods. In particular, we sidestep
our proposed GCN encoding scheme by projecting an RGB
point cloud to eight extreme views deÔ¨Åned by a bound-
ing volume, running the VGG encoder for feature extrac-
tion, and back-projecting the 2D features to a point cloud
from which stylization and rendering proceed as before. As
shown in Fig 9, this VGG-based variant produces geometric
distortion and visual artifacts in stylized images, as opposed
to our model using geometry-aware feature learning.
Effect of Consistency Loss. We evaluate the contribu-
tion of our consistency loss in Table 2. Despite a shared
point cloud, model trained without the consistency loss pro-

28.0%51.3%47.4%39.7%37.9%45.7%72.0%48.7%52.6%60.3%62.1%54.3%(a) Stylization3.9%3.0%3.0%16.4%4.3%17.2%96.1%97.0%97.0%83.6%95.7%82.8%(b) Consistency8.2%15.5%7.3%35.8%10.3%22.4%91.8%84.5%92.7%64.2%89.7%77.6%(c) OverallAdaINLSTAdaAttN(img)ReReVSTMCCAdaAttN(vid)OursAdaINLSTAdaAttN(img)ReReVSTMCCAdaAttN(vid)AdaINLSTAdaAttN(img)ReReVSTMCCAdaAttN(vid)OursOursContent + styleGCNVGGFigure 10. Extension to multi-view input. Compared with StyleScene [21], our method more closely resembles the reference style,
better preserves the content geometry (red boxes), and is more robust to change in viewpoint distribution (second row).

Method

Truck

Short-range consistency
Train

Playground

M60

Truck

Long-range consistency
Train

Playground

M60

StyleScene (global)
StyleScene (local)
Ours (local)

RMSE LPIPS RMSE LPIPS RMSE LPIPS RMSE LPIPS
0.143
0.124
N/A
0.119
0.112
0.099

0.143
0.168
0.107

0.121
0.161
0.104

0.142
0.169
0.111

0.108
0.127
0.093

0.157
0.169
0.112

0.120
N/A
0.117

RMSE LPIPS RMSE LPIPS RMSE LPIPS RMSE LPIPS
0.192
0.163
N/A
0.152
0.136
0.113

0.188
0.203
0.128

0.189
0.205
0.127

0.146
0.166
0.110

0.213
0.220
0.145

0.160
N/A
0.136

0.159
0.204
0.120

Table 3. Consistency in the multi-view scenario. On the Tanks and Temples dataset [28], we compare our method with StyleScene on
short- and long-range consistency as deÔ¨Åned in [21] using RMSE (‚Üì) and LPIPS (‚Üì).

duces less consistent renderings measured in RMSE and
LPIPS. We attribute this to the learnable feature decoding
step, which is too Ô¨Çexible to preserve consistency in output
images in the absence of a constraint. In this respect, our
consistency loss, especially when applied in the stylization
stage of training, acts as a strong regularizer on the decoder.

5.5. Extension to Multi-view Inputs

Our method can be easily extended for stylized novel
view synthesis given multi-view inputs. We compare our
extension with StyleScene [21], which similarly operates
on point cloud but requires multiple input views. We per-
form experiments on the Tanks and Temples dataset [28]
under two protocols. The global protocol uses all available
views (up to 300) as in [21] for point cloud reconstruction,
whereas the more challenging local protocol uses a sparse
set of 6-8 views on the camera trajectory for novel view syn-
thesis. In Fig 10 and Table 3, we show that our method is
better in terms of style quality, short- and long-range con-
sistency, and robustness to the distribution of input views.

5.6. Applications

Layered Stylization for AR applications. Human cen-
tered photography is of central interest in mobile AR appli-
cations. As a proof-of-concept experiment to demonstrate
our method‚Äôs potential in AR, we apply PointRend [27] to
segment foreground human subjects in images from Un-
splash [1], and stylize the background scene using our
method while leaving the foreground human untouched
(Fig 11a). The Ô¨Ånal stylized 3D photo upon rendering initi-
ates a virtual tour into a 3D environment in an artistic style.
3D Exploration of Stylized Historical Photos. Historical
photos represent a large fraction of existing image assets

Figure 11. Demonstration of Applications.
Layered styl-
ization for AR (upper) and 3D browsing of a stylized historical
photo3(lower)‚Äî‚ÄúA small arch welcomes the President to Met-
lakatla, Alaska, created by D. L. Hollandy 1923.‚Äù

and remain under-explored in computer vision and graphics.
As we demonstrate on the Keystone dataset [37] (Fig 11b),
our method can be readily applied for the 3D browsing of
historical photos in an artistic style, bringing past moments
back alive in an unexpected way.

6. Discussion

In this paper, we connected neural style transfer and
one-shot 3D photography for the Ô¨Årst time, and introduced
the novel task of 3D photo stylization ‚Äì generating styl-
ized novel views from a single image given an arbitrary
style. We showed that a na¬®ƒ±ve combination of solutions
from the two worlds do not work well, and proposed a deep
model that jointly models style transfer and view synthe-
sis for high-quality 3D photo stylization. We demonstrated
the strength of our approach using extensive qualitative and
quantitative studies, and presented interesting applications
of our method for 3D content creation. We hope our method
will open an exciting avenue of applications in 3D content
creation from 2D photos.

8

TruckM60Content + styleStyleScene (global)StyleScene (local)Ours (local)(a) Layered stylization(b) Historical photosReferences

[1] Unsplash dataset. https://unsplash.com/data,

2020. 6, 8

[2] Xu Cao, Weimin Wang, Katashi Nagao, and Ryosuke Naka-
mura. Psnet: A style transfer network for point cloud styl-
ization on geometry and color. In WACV, 2020. 2

[3] Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang
Hua. Coherent online video style transfer. In ICCV, pages
1105‚Äì1114, 2017. 2

[4] Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang
Hua. Stylebank: An explicit representation for neural image
style transfer. In CVPR, 2017. 1, 2

[5] Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang
Hua. Stereoscopic neural style transfer. In CVPR, 2018. 2
[6] Tian Qi Chen and Mark Schmidt. Fast patch-based style
transfer of arbitrary style. Workshop in Constructive Ma-
chine Learning, NeurIPS, 2016. 2

[7] Xu Chen, Jie Song, and Otmar Hilliges. Monocular neu-
ral image based rendering with continuous view control. In
ICCV, 2019. 2

[8] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-
sheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via im-
plicit representation and hypernetwork. arXiv, 2021. 2
[9] Yingying Deng, Fan Tang, Weiming Dong, Haibin Huang,
Chongyang Ma, and Changsheng Xu. Arbitrary video style
transfer via multi-channel correlation. In AAAI, 2021. 2, 6,
7, 11

[10] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
A learned representation for artistic style. In ICLR, 2017. 2
[11] Wei Gao, Yijun Li, Yihang Yin, and Ming-Hsuan Yang. Fast

video multi-style transfer. In WACV, 2020. 6

[12] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A

neural algorithm of artistic style. arXiv, 2015. 2

[13] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-
In
age style transfer using convolutional neural networks.
CVPR, 2016. 1, 2

[14] Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent
Dumoulin, and Jonathon Shlens. Exploring the structure of
a real-time, arbitrary neural artistic stylization network. In
BMVC, 2017. 1

[15] Xinyu Gong, Haozhi Huang, Lin Ma, Fumin Shen, Wei Liu,
and Tong Zhang. Neural stereoscopic image style transfer.
In ECCV, 2018. 2

[16] Shuyang Gu, Congliang Chen, Jing Liao, and Lu Yuan. Ar-
bitrary style transfer with deep feature reshufÔ¨Çe. In CVPR,
2018. 1

[17] Jie Guo, Mengtian Li, Zijing Zong, Yuntao Liu, Jingwu He,
Yanwen Guo, and Ling-Qi Yan. Volumetric appearance styl-
ization with stylizing kernel prediction network. TOG, 2021.
2

[18] Ronghang Hu, Nikhila Ravi, Alexander C Berg, and Deepak
Pathak. Worldsheet: Wrapping the world in a 3d sheet for
view synthesis from a single image. In ICCV, 2021. 2
[19] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point-
wise convolutional neural networks. In CVPR, 2018. 2

[20] Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao
Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu. Real-time
neural style transfer for videos. In CVPR, 2017. 2, 5
[21] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh
Singh, and Ming-Hsuan Yang. Learning to stylize novel
views. In ICCV, 2021. 1, 2, 3, 7, 8, 11, 12

[22] Xun Huang and Serge Belongie. Arbitrary style transfer in
In ICCV,

real-time with adaptive instance normalization.
2017. 1, 2, 6, 7, 11

[23] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, 2015. 11

[24] Varun Jampani, Huiwen Chang, Kyle Sargent, Abhishek
Kar, Richard Tucker, Michael Krainin, Dominik Kaeser,
William T Freeman, David Salesin, Brian Curless, et al.
Slide: Single image 3d photography with soft layering and
depth-aware inpainting. In ICCV, 2021. 2

[25] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In
losses for real-time style transfer and super-resolution.
ECCV, 2016. 1, 2

[26] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015. 5

[27] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-
In

Image segmentation as rendering.

shick. Pointrend:
CVPR, 2020. 8

[28] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen
Koltun. Tanks and temples: Benchmarking large-scale scene
reconstruction. TOG, 2017. 8

[29] Johannes Kopf, Kevin Matzen, Suhib Alsisan, Ocean
Quigley, Francis Ge, Yangming Chong, Josh Patterson, Jan-
Michael Frahm, Shu Wu, Matthew Yu, et al. One shot 3d
photography. TOG, 2020. 2

[30] Guohao Li, Matthias M¬®uller, Guocheng Qian, Itzel Car-
olina Delgadillo Perez, Abdulellah Abualshour, Ali Kassem
Thabet, and Bernard Ghanem. Deepgcns: Making gcns go
as deep as cnns. TPAMI, 2021. 2, 3, 4, 11

[31] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu
Wang, and Gim Hee Lee. Mine: Towards continuous depth
mpi with nerf for novel view synthesis. In ICCV, 2021. 2

[32] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang.
Learning linear transformations for fast image and video
style transfer. In CVPR, 2019. 5, 6, 7, 11

[33] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu,
and Ming-Hsuan Yang. Universal style transfer via feature
transforms. In NeurIPS, 2017. 1, 2

[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, 2014. 2

[35] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,
Andrew Tao, and Bryan Catanzaro. Image inpainting for ir-
regular holes using partial convolutions. In ECCV, 2018. 2
[36] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling
Wang, Xin Li, Zhengxing Sun, Qian Li, and Errui Ding.
Adaattn: Revisit attention mechanism in arbitrary neural
style transfer. In ICCV, 2021. 1, 2, 4, 5, 6, 7, 11

9

[37] Xuan Luo, Yanmeng Kong, Jason Lawrence, Ricardo
Martin-Brualla, and Steven M. Seitz. Keystonedepth: His-
tory in 3d. In 3DV, 2020. 8

[57] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin
Johnson. Synsin: End-to-end view synthesis from a single
image. In CVPR, 2020. 2

[58] Kangxue Yin, Jun Gao, Maria Shugrina, Sameh Khamis, and
Sanja Fidler. 3dstylenet: Creating 3d shapes with geometric
and texture style variations. In ICCV, 2021. 2

[59] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus,
Long Mai, Simon Chen, and Chunhua Shen. Learning to
recover 3d scene shape from a single image. In CVPR, 2021.
2, 3, 5

[60] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance Ô¨Åelds from one or few images. In
CVPR, 2021. 2

[38] Steve Marschner and Peter Shirley. Fundamentals of com-

puter graphics. 2021. 3

[39] Kiri Nichol. Painters by numbers, wikiart. https://www.
kaggle.com/c/painter-by-numbers, 2016. 5
[40] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d ken
burns effect from a single image. TOG, 2019. 2, 4, 11
[41] Dae Young Park and Kwang Hee Lee. Arbitrary style trans-
fer with style-attentional networks. In CVPR, 2019. 1
[42] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiÔ¨Åcation
and segmentation. In CVPR, 2017. 2

[43] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-
net++: Deep hierarchical feature learning on point sets in a
metric space. In NeurIPS, 2017. 2, 4, 11

[44] Ren¬¥e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. 2021. 2, 5, 6
[45] Ren¬¥e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. TPAMI, 2020. 2

[46] Chris Rockwell, David F Fouhey, and Justin Johnson. Pixel-
synth: Generating a 3d-consistent experience from a single
image. In ICCV, 2021. 2

[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 4, 11

[48] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
Artistic style transfer for videos and spherical images. IJCV,
2018. 2

[49] Lu Sheng, Ziyi Lin, Jing Shao, and Xiaogang Wang. Avatar-
net: Multi-scale zero-shot style transfer by feature decora-
tion. In CVPR, 2018. 1

[50] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin
Huang. 3d photography using context-aware layered depth
inpainting. In CVPR, 2020. 2, 3, 4, 6, 7, 11

[51] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud,
Beatriz Marcotegui, Franc¬∏ois Goulette, and Leonidas J.
Guibas. Kpconv: Flexible and deformable convolution for
point clouds. In ICCV, 2019. 2

[52] Richard Tucker and Noah Snavely. Single-view view syn-

thesis with multiplane images. In CVPR, 2020. 2

[53] Shubham Tulsiani, Richard Tucker, and Noah Snavely.
Layer-structured 3d scene inference via view synthesis. In
ECCV, 2018. 2

[54] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Vic-
tor S Lempitsky. Texture networks: Feed-forward synthesis
of textures and stylized images. In ICML, 2016. 2

[55] Wenjing Wang, Jizheng Xu, Li Zhang, Yue Wang, and Jiay-
ing Liu. Consistent video style transfer via compound regu-
larization. In AAAI, 2020. 2, 6, 7, 11

[56] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma,
Michael M. Bronstein, and Justin M. Solomon. Dynamic
graph cnn for learning on point clouds. TOG, 2019. 2

10

Supplementary Material

We refer to our supplementary video for an overview of
our results and comparisons with the baselines. This docu-
ment describes the technical details, the design of our user
study, the details of extending our method to multi-view in-
puts, as well as a discussion on the limitation of our method.

A. Implementation Details

Our model architecture is illustrated in Fig 12. We now

present our implementation details.

Point Cloud Encoder Architecture. Our GCN encoder
adopts a hierarchical design for computational and memory
efÔ¨Åciency. It takes an input RGB point cloud and processes
it in three stages with 1, 2 and 2 MRConv layers [30] respec-
tively. The point features are 64, 128 and 256 dimensional
after each stage. Contrary to [30], our MRConv variant per-
forms point aggregation using ball queries, and we progres-
sively increase the ball radius throughout the encoder to en-
large its receptive Ô¨Åeld. At the entry of each stage, we apply
farthest point sampling to sub-sample the point cloud by a
factor of 4. A residual connection is introduced every two
layers to facilitate gradient Ô¨Çow during training. We apply
batch normalization [23] after each layer and use ReLU as
the non-linearity.

Stylizer Architecture. Our stylizer follows AdaAttN [36].
As discussed in the main paper, we apply a multi-layer per-
ceptron (MLP) with two fully-connected layers of 256 units
to map content features to the style feature space before
stylization. A symmetric MLP is applied after stylization
to bring the modulated features back to the content feature
space. The MLPs use ReLU as the non-linearity.

Neural Renderer Architecture. Our neural renderer Ô¨Årst
up-samples the 256-dimensional encoder output via inverse
distance weighted interpolation [43] until the output reso-
lution is the same as the encoder input. The rasterizer [40]
projects the up-sampled point features to the image plane of
a novel view given camera pose and intrinsics. The resulting
2D feature maps have 256 dimensions and are further pro-
cessed by a U-Net [47] with three levels. The encoder part
of the U-Net downsamples the feature maps without inÔ¨Çat-
ing the channel dimension. We interpret this as a learnable
anti-aliasing step in the same spirit as widely used super-
sampling in computer graphics. The decoder part subse-
quently up-samples the feature maps via transposed convo-
lution and meanwhile halves the channel dimension. The
skip connections, implemented as 1 √ó 1 convs, pass along
feature from the encoder to the decoder to facilitate gradi-
ent Ô¨Çow. All layers in the U-Net except the skip convs have
a kernel size of 3 √ó 3. We apply leaky ReLU with a slope
of 0.2 in the encoder and ReLU in the decoder as the non-
linearity.

Figure 12. Model architecture.
Architecture of our point
cloud encoder and neural renderer. The layer speciÔ¨Åcations are
as follows: Conv1/2d (input channel, output channel, kernel size,
stride); MRConv (input channel, output channel, maximum num-
ber of neighboring points, ball radius).

B. User Study Design

We conduct a user study to compare our method with
baselines that sequentially combine 3DPhoto [50] and one
of the six image [22, 32, 36] or video style transfer meth-
ods [9, 36, 55]. The study includes three sections for the as-
sessment of style quality, multi-view consistency and over-
all synthesis quality. Each section consists of 60 random
binary choice questions that compare our method with one
of the baselines. For convenience, a stylized 3D photo is
displayed as a 90-frame snippet following a random camera
trajectory. For fair evaluation of style quality, we only dis-
play stylized image of the input view so as not to bias par-
ticipants toward more consistent renderings. Similarly, we
hide the content and style images when consistency is eval-
uated to minimize the impact of style quality. Our analysis
is based on a total of 5,400 votes collected from 30 volun-
teers. We show a screenshot of our user study in Fig 13. Our
user study is anonymous and does not involve the collection
of personally identiÔ¨Åable data.

C. Details on Extension to Multi-view Inputs

Extending our method to the multi-view setting is im-
mediate after a small modiÔ¨Åcation on point cloud normal-
ization. Now that more than one input views are available,
we back-project all views to a point cloud and transform it
into the NDC space anchored to the center view. Everything
else stays exactly the same, and importantly, the model need
not be re-trained thanks to the normalization step. In our
experiments, we use the same depth maps from [21] for a
fair comparison with StyleScene [21]. Those results were
shown in Table 3 and Figure 10 of our main paper.

11

Conv2d (256, 256, 3, 2)Conv2d (256, 256, 3, 1)Conv2d (256, 256, 3, 2)Conv2d (256, 256, 3, 1)Conv2d (256, 256, 3, 2)Conv2d (256, 256, 3, 1)ConvTranspose2d (256, 128, 3, 2)Conv2d (256, 128, 3, 1)ConvTranspose2d (128, 64, 3, 2)Conv2d (128, 64, 3, 1)Conv2d (64, 3, 3, 1)Projected feature mapsStylized RGB imageMRConv (3, 64, 16, 0.015)MRConv (64, 128, 16, 0.015)MRConv (128, 128, 16, 0.025)MRConv (128, 256, 16, 0.025)MRConv (256, 256, 16, 0.05)4x down4x downRGB point cloudConv1d (256, 256, 1, 1)Featurized point cloud++(a) Point cloud encoder(b) Neural rendererFigure 13. Screenshot of our user study. A randomly picked question in our user study.

Societal impacts: We anticipate that our research would
facilitate new applications of 3D content creation from 2D
photos. Similar to other image manipulation methods like
neural style transfer, our method might face potential copy-
right infringement, when copyright-protected content im-
ages are modiÔ¨Åed and improperly used.

D. Limitations

Despite steady progress in monocular depth estimation,
current state of the arts do not always produce reliable depth
maps for complex scenes, and in particular for those pixels
near depth discontinuities. Our method relies on monoc-
ular depth estimation on the input image and thus inherits
the failure mode of the underlying depth estimators. As
a partial remedy, we have demonstrated an extension of
our method to use mutli-view inputs with more reliable
depth estimations. Another limitation our method shares
with StyleScene [21] lies in the run-time speed. While our
method renders stylized images of 1K resolution at interac-
tive rate on a TITAN Xp GPU, the current implementation
may not support interactive exploration of a high-resolution
stylized 3D photo on mobile devices. Future work may fo-
cus on improving rendering speed for 3D photo stylization.

12

