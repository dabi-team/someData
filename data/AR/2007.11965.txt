CAD-DEFORM: DEFORMABLE FITTING OF CAD MODELS TO
3D SCANS

Vladislav Ishimtsev∗
Skolkovo Institute of Science and Technology
Russia

Alexey Bokhovkin*
Skolkovo Institute of Science and Technology
Russia

Alexey Artemov
Skolkovo Institute of Science and Technology
Russia

Savva Ignatyev
Skolkovo Institute of Science and Technology
Russia

Matthias Niessner
Technical University of Munich, Germany

Denis Zorin
New York University, USA
Skolkovo Institute of Science and Technology
Russia

Evgeny Burnaev
Skolkovo Institute of Science and Technology
Russia

ABSTRACT

Shape retrieval and alignment are a promising avenue towards turning 3D scans into lightweight CAD
representations that can be used for content creation such as mobile or AR/VR gaming scenarios.
Unfortunately, CAD model retrieval is limited by the availability of models in standard 3D shape
collections (e.g., ShapeNet). In this work, we address this shortcoming by introducing CAD-Deform1,
a method which obtains more accurate CAD-to-scan ﬁts by non-rigidly deforming retrieved CAD
models. Our key contribution is a new non-rigid deformation model incorporating smooth transfor-
mations and preservation of sharp features, that simultaneously achieves very tight ﬁts from CAD
models to the 3D scan and maintains the clean, high-quality surface properties of hand-modeled CAD
objects. A series of thorough experiments demonstrate that our method achieves signiﬁcantly tighter
scan-to-CAD ﬁts, allowing a more accurate digital replica of the scanned real-world environment
while preserving important geometric features present in synthetic CAD environments.

0
2
0
2

l
u
J

3
2

]

V
C
.
s
c
[

1
v
5
6
9
1
1
.
7
0
0
2
:
v
i
X
r
a

Keywords Scene reconstruction · Mesh deformation

1

Introduction

A wide range of sensors such as the Intel RealSense, Google Tango, or Microsoft Kinect can acquire point cloud data
for indoor environments. These data can be subsequently used for reconstructing 3D scenes for augmented and virtual
reality, indoor navigation and other applications [1, 2, 3, 4, 5, 6]. However, available 3D reconstruction algorithms are
not sufﬁcient for many applied scenarios as the quality of the result may be signiﬁcantly affected by noise, missing data,
and other artifacts such as motion blur found in real scans, disabling reconstruction of ﬁne-scale and sharp geometric
features of objects. In most instances, reconstructions are still very distant from the clean, 3D models created manually.

∗Equal contribution
1The code for the project: https://github.com/alexeybokhovkin/CAD-Deform

 
 
 
 
 
 
CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 1: CAD-Deform takes as input a set of 3D CAD models aligned on a RGB-D scan (left). In order to achieve
tight ﬁts (middle), we propose a novel part-based deformation formulation that maintains the desired CAD properties
such as sharp features.

An approach to overcome these problems has been proposed in [7, 8] and more recently developed using modern ML
methods in [9, 10]. Building on the availability of parametric (CAD) models [11, 12] they perform reconstruction by
ﬁnding and aligning similar CAD models from a database to each object in a noisy scan. To realize this approach, the
authors of [9] introduced the Scan2CAD dataset comprising of pairwise keypoint correspondences and 9 DoF (degrees
of freedom) alignments between instances of unique synthetic models from ShapeNet [11] and reconstructed scans from
ScanNet [13]; in order to ﬁnd and align CAD models to an input scan, they developed a deep neural model to predict
correspondences, with a further optimization over potential matching correspondences for each candidate CAD model.
The difference of [10] compared to the approach from [9] is an end-to-end procedure, combining initially decoupled
steps to take into account additional feedback through the pipeline by learning the correspondences speciﬁcally tailored
for the ﬁnal alignment task.

However, geometric ﬁdelity achieved between scans and CAD objects remains limited. CAD model geometry (clean
and complete) differs signiﬁcantly from scan geometry in low-level geometric features. As these methods focus on
ﬁnding alignments by optimizing a small number of parameters (9 DoF), resulting alignments only roughly approximate
scans, not capturing geometric details such as variation in 3D shapes of parts of individual objects.

In contrast, to improve geometric ﬁdelity while keeping the beneﬁt of mesh-based representations, we propose to
increase the number of degrees of freedom by allowing the CAD objects to deform rather than stay rigid. In this work,
we introduce a deformation framework CAD-Deform, which signiﬁcantly increases the geometric quality of object
alignment regardless of the alignment scheme. For an input scan, given object location and 9 DoF alignment of a
potentially matching CAD model, we apply a specially designed mesh deformation procedure resulting in a more
accurate shape representation of the underlying object. The deformation matches each semantic part of a 3D shape
extracted from the PartNet [14] to the corresponding data in the scans and keeps sufﬁcient rigidity and smoothness to
produce perceptually acceptable results while minimizing the distance to scanned points. Thus, even if the initial location
and alignment are not entirely accurate, the deformation can compensate to a signiﬁcant extent for the discrepancy.

Our approach builds highly detailed scene descriptions with a high level of semantic accuracy for applications in
3D graphics. The approach outperforms state-of-the-art methods for CAD model alignment and mesh deformation
by 2.1–6.3% for real-world 3D scans. To the best of our knowledge, the approach we propose is the ﬁrst to use
mesh deformation for scan-to-CAD alignment and real-world scene reconstruction. In summary, our work has several
contributions:

• We developed a mesh deformation approach that 1) is computationally efﬁcient, 2) does not require exact
correspondences between each candidate CAD model and input scan, and 3) provides perceptually plausible

2

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

deformation thanks to a specially introduced smoothness term and inclusion of geometric features of a CAD
model in the optimization functional.

• We developed a methodology to assess the ﬁtting accuracy and the perceptual quality of the scan-to-CAD
reconstruction. The methodology includes standard data ﬁtting criteria similar to Chamfer distance to
evaluate alignment accuracy, complimentary local and global criteria for visual quality assessment of resulting
deformations, and a user study.

• We performed an ablation study to assess the inﬂuence of inaccuracies in the initial object location and
alignment on the ﬁnal reconstruction results. For that we used both ground-truth alignments from Scan2CAD
dataset [9] along with predictions of their method, and alignments trained in the end-to-end fashion [10]. We
compared results with the state-of-the-art methods for mesh deformation to highlight the advantages of our
approach.

2 Related work

RGB-D scanning and reconstruction. RGB-D scanning and reconstruction are increasingly widely used, due to the
availability of commodity range sensors and can be done both in real-time and ofﬂine modes. There are many methods
for RGB-D-based real-time reconstruction such as KinectFusion [1], Voxel Hashing [3] or Bundle Fusion [6] that use
the well-known volumetric fusion approach from [15]. ElasticFusion [4] is a representative ofﬂine approach to the
reconstruction. These methods can produce remarkable results for large 3D environments. However, due to occlusions
and imperfections of existing sensors, the reconstructed scenes contain many artifacts such as noise, missing surface
parts, or over-smooth surfaces. Some methods aim to predict unobserved or corrupted parts of 3D scans from depth
data. In [16], the world is modeled as a grid of voxels representing the signed distance to the nearest surface. Then
structured Random Forest is used to predict the value of the signed distance function for each of the voxels computed
to form a ﬁnal occupancy estimate for unobserved voxels. [17] is to encode a depth map as a 3D volume and then
aggregate both local geometric and contextual information via a 3D CNN to produce the probability distribution of
voxel occupancy and object categories for all voxels inside the camera view frustum. Another approach is [18], where a
3D CNN architecture predicts a distance ﬁeld from a partially-scanned input and ﬁnds the closest 3D CAD model from
a shape database. By copying voxels from the nearest shape neighbors, they construct a high-resolution output from
the low-resolution predictions, hierarchically synthesize a higher-resolution voxel output, and extracts the mesh from
the implicit distance ﬁeld. However, although all these methods can complete partial scans and improve 3D geometry
reconstruction, the quality of the results still is far from artist-created 3D content.

CAD model alignment. Instead of reconstructing 3D geometry in a bottom-up manner, one can perform reconstruction
by retrieving CAD models from a dataset and aligning them to the noisy scans. Matching CAD models to scans requires
extracting 3D feature descriptors; thus, approaches have been proposed for 3D feature extraction. Hand-crafted features
are often based on various histograms of local characteristics (e.g., [19]). Such approaches do not generalize well to
inherent variability and artifacts in real-world scans. Deep learning approaches lead to further improvements: e.g., [20]
propose a view consistency loss for a 3D keypoint prediction network based on RGB-D data; [21] develop 3D local
learnable features for pairwise registration. After the descriptors are extracted, one can use a standard pipeline for
CAD-to-scan alignment: ﬁrst, matches between points based on 3D descriptors are identiﬁed, and then variational
alignment is used to compute 6- or 9-DoF CAD model alignments. Typical examples, realizing this two-step approach,
are described in [7, 8, 22, 9, 10]. The most recent ones [9, 10] use learnable descriptors and differ in that the latter
considers an end-to-end scan-to-CAD alignment, reconstructing a scene in a single forward pass. An obvious limitation
of these two-step pipelines is that the resulting alignments approximate scans only coarsely due to a pre-deﬁned set of
available CAD models and a highly constrained range of transformations. Other approaches in this category [23, 24, 25],
although relying on the same two-step strategy, use only a single RGB or RGB-D input.

Mesh deformation. To improve surface reconstruction accuracy and obtain a more faithful description of geometric
details (e.g., preserve distinctive geometric features), it is desirable to consider a more general space of deformations
than a simple non-uniform scaling. To this end, a mesh deformation approach based on Laplacian surface editing
is presented in [26]. Another iterative mesh deformation scheme [27] imposes rigidity on the local transformations.
Despite their conceptual simplicity, both methods require specifying correspondences between mesh vertices and scan
data points. The same is true for [28, 29, 30] and unsuitable in our setting, due to extremely low (2–8) number of
correspondences available per part, that additionally may not even be well deﬁned for noisy and incomplete real-world
scans. Many methods [29, 31, 32, 33] focus on automatic posing of human models, dense surface tracking and similar
applications. However, while producing compelling results, the methods implicitly use the assumption that a template
mesh and its target shape are very similar; as a result, the semantic parts of individual 3D objects are not changed either
in shape or relative scale. In our work, we are comparing to ARAP (as-rigid-as-possible) [27] and Harmonic [34, 35]
mesh deformation methods, however, with an added Laplacian smoothing term to leverage second-order information

3

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 2: Data acquisition for CAD-Deform: we project PartNet labels onto aligned ShapeNet CAD models (left),
register these models to a 3D scene scan (middle), and extract points on the scene within ε-neighborhood of aligned
mesh surface (we set ε = 10 cm), copying labels corresponding to nearest part of CAD model (right).

about the surface. This modiﬁcation makes ARAP/Harmonic similar to [29, 28] as far as non-data-dependent energy
terms are concerned. Other methods exist that propose non-linear constraints [36, 37, 38]. The energy terms of our
framework were designed as a natural match for our problem: we deﬁne local 3D transformations on the CAD model,
mapping each subpart to the 3D scene volume, and require smooth changes of these transformations over the model.
In contrast to most deformation methods, we do not aim to keep the surface close to isometric: e.g., a table reference
model can be stretched highly anisotropically to match a different table in the data. Our energy is deﬁned using local 3D
afﬁne transform primitives, penalizing sharp changes, without penalizing anisotropic deformations. These primitives
also allow us to express 1D feature preservation simply, and the non-data terms are quadratic which is critical for our
efﬁcient preconditioned optimization. Methods in [36, 37, 38] propose non-linear energies, focusing on large rotations,
but implicitly assuming quasi-isometry; adapting these methods is nontrivial.

3 Overview of CAD-Deform framework

Our approach is built on top of the framework from [9, 10] for CAD model retrieval and 9 DoF alignment in 3D scans.
By running any of the approaches from [9, 10] for an input scan, we obtain initial object locations and 9 DoF alignments
of CAD models potentially matching speciﬁc parts of the scan. Next, we apply our proposed mesh deformation
procedure (see Section 4), resulting in a more accurate shape representation of the aligned objects:

1. We segment the CAD models into semantic 3D parts following the labelling from the PartNet dataset [14].

2. For each aligned object, we select points in the scan that are the nearest (within some ﬁxed radius) to each
vertex of the CAD model. We assign a label of the nearest part of the aligned CAD model to each such point.

3. As an input to the proposed mesh deformation procedure, we use the mesh model with semantic part labels

and labelled segment of the 3D scene.

4. We deform the mesh by optimizing the energy depending on the relative positions of mesh vertices and labelled

points of the scene, see Section 4.

4 Data-driven shape deformation

In this section, we describe our method for ﬁtting a closest-match mesh from the shape dataset to the scanned data. Our
algorithm assumes two inputs:

4

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

• an initial mesh M from the dataset, with part labels assigned to vertices and 9 degrees-of-freedom (9 DoF)

transformation assigned to mesh;

• a subset of the scanned data, segmented with the same part labels as M.

Fig. 2 shows an example of the input data, see also Section 3.
Notations. A mesh M = (V0, E, F) consists of a set of initial 3D vertex coordinates V0 of size nv, the edge set
E of size ne, and the triangle face set F of size nf . We compute the deformed vertex positions V by minimizing a
deformation energy. All vertices are assigned with part labels, which is a map Q : V0 → C, where C is the set of
labels ci, i = 1, . . . , nc, and nc is the number of parts in all objects in our dataset. For a mesh Mm, Cm ⊂ C is the set
of labels of its parts. The set of nP points P has the same labels as the mesh Mm we ﬁt, i.e., CP = Cm. In addition,
we assume that for every mesh Mm on the scan we have a scaling transformation, represented by a 4 × 4 matrix T 0
m,
that aligns it with voxelized points P. In our optimization, we use per-edge transformations Te, discussed below, which
we use to measure the deviation from a scaled version of the original shape and deformation smoothness.

4.1 Deformation energy

Our goal is to deﬁne a deformation energy to match the scanned data as closely as possible, while maintaining continuity
and deformation smoothness, and penalizing deviation from the original shape. In addition, we include a term that
preserves perceptually important linear geometric features of the mesh.

Conceptually, we follow common mesh deformation methods such as ARAP [27] which estimate a local transformation
from vertex positions and penalize the deviation of this transformation from the target (e.g., closest rotation). An
important distinction is that the transformation we need to estimate locally is a 3D, rather than 2D transformation, and
cannot be estimated from the deformed positions of vertices of a single triangle.

Edge transformations. We associate local 3D afﬁne transformations with mesh edges. Each transformation is deﬁned
in a standard way by a 4 × 4 matrix in projective coordinates, with the last row (0, 0, 0, c), c (cid:54)= 0. The vertices of
two faces (i1, i2, i3) and (i2, i1, i4), incident at an edge e = (i1, i2), form a (possibly degenerate) tetrahedron. If it is
not degenerate, then there is a unique linear transformation Te mapping the undeformed positions (v0
, v0
)
i4
i1
to (vi1, vi2, vi3, vi4). Matrix T 0 of the afﬁne transformation has 12 coefﬁcients that are uniquely determined by the
equations Tev0
i = vi, i = 1 . . . 4; moreover, these are linear functions of the deformed positions vi, as these are only
present on the right-hand side of the equations. Handling of degenerate tetrahedra is discussed in Section 4.2.

, v0
i3

, v0
i2

Energy. We deﬁne the following non-linear objective for the unknown deformed vertex positions V and (a ﬁxed) point
set P:

E(V, P) = Eshape + αsmoothEsmooth + αsharpEsharp
(cid:125)

(cid:124)

+αdataEdata,

(cid:123)(cid:122)
quadratic problem
e (cid:107)2
2

;

(cid:107)Te(V) − T 0

(cid:123)(cid:122)
deviation

(cid:125)

Eshape =

Esharp =

(cid:88)

e∈E
(cid:124)

np
(cid:88)

Esmooth =

(cid:88)

(cid:88)

f ∈F

ei,ej ∈f

(cid:107)Tei(V) − Tej (V)(cid:107)2
2;

(cid:88)

(cid:107)Tes (V) − Tes+1(V)(cid:107)2
2;

Edata = fdata(V, P).

(1)

k=1

es∈Ek

sharp

The ﬁrst term penalizes deviations of the 3D afﬁne transformations deﬁned by the deformed vertex positions from the
transformation (a non-uniform scale) that aligns mesh with the data. This term directly attempts to preserve the shape
of the object, modulo rescaling.

, v0
i2

As explained above, the transformations Te(V) are deﬁned for non-degenerate input mesh conﬁgurations. Suppose the
four initial vertex positions (v0
) form a degenerate tetrahedron W, i.e., two faces incident at the edge are
i1
close to co-planar. In this case, we use an energy term consisting of two terms deﬁned per triangle. Instead of 4 × 4
matrix for each non-degenerate tetrahedron, there is a 3 × 2 matrix for each triangular face of degenerate tetrahedron
that represents a transformation restricted to the plane of this face. Note that in this case, the deformation in the direction
perpendicular to the common plane of the triangles does not affect the energy, as it does not have an impact on the local
shape of the deformed surface. We explain the remaining energy terms in the next sections.

, v0
i4

, v0
i3

5

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

4.2 Quadratic terms

Smoothness term. This term can be thought of as a discrete edge-based Laplacian, applied to the transforms associated
with edges: the difference of any pair of transforms for edges belonging to the same triangle is penalized.

Sharp features term. We have observed that a simple way to preserve some of the perceptually critical geometric aspects
of the input meshes is to penalize the change of deformations Te along sharp geometric features, effectively preserving
their overall shape (although still allowing, possibly non-uniform, scaling, rotations and translations). We detect sharp
edges based on a simple dihedral angle thresholding, as this is adequate for the classes of CAD models we use. Detected
sharp edges are concatenated in sequences, each consisting of vertices with exactly two incident sharp edges each,
except the ﬁrst and the last. Those sequences are deﬁned for each part of the mesh. Sequences of different parts has no
common sharp edges or vertices belonging to them. Effectively, the sharpness term increases the weights for some
edges in the smoothness term.

4.3 Data term

We use two approaches to deﬁning the data term, one based on screened attraction between close points in the mesh
Mm and P, and the other one based on attraction between a priori chosen corresponding points of the mesh Mm and
data points in the set P. We found that the former method works better globally, when the deformed mesh is still far
from the target point cloud, while the latter is able to achieve a better match once a closer approximation is obtained.

Part-to-part mapping. We deﬁne a data-ﬁtting term based on point proximity: we set an energy proportional to the
distance between sufﬁciently close points. To avoid clustering of mesh points, we add a screening term that disables
attraction for mesh vertices close to a given point.
We denote by H(x) the Heaviside function, i.e., a function with value 1 if x (cid:62) 0 and zero otherwise. We set

f p2p
data(V, P) =

(cid:88)

(cid:88)

(cid:88)

ξσ(p, Vc)(cid:0)dε(v − p)(cid:1)2

,

c∈C

v∈Vc

p∈Pc

(2)

where dε(v −p) = (v −p)·H(ε−(cid:107)v −p(cid:107)2
value for σ is chosen to be the mean edge length, the value of ε is about 10 edge lengths. To make f p2p
instead of the Heaviside function and min, we can use their smoothed approximations.

2), and ξσ(p, V) = H(min{v∈V} (cid:107)v −p(cid:107)−σ) is a “screening” function. The
data differentiable,

Nearest-neighbor mapping. Recall that the vertices {vi} of the mesh m and the points {pi} of the set P have the same
part label sets Cm = CP. For each label c, we consider V0
c and Pc, the set of mesh vertices in initial positions and the
set of points with the same label c in P. Let BVc , BPc be the bounding boxes of these sets, and consider the afﬁne
transform T c
B that maps BVc to BPc. Among all possible correspondences between corners of the boxes, we choose
the one that produces the afﬁne transform closest to identity. Then the index i = ιT (p) of the vertex vi, corresponding
to a point p ∈ Pc, is determined as ιc(p) = arg min{i,vi∈Vc} (cid:107)T c

2. Then the data term is deﬁned as

Bvi − p(cid:107)2

f nn
data(V, P) =

(cid:88)

(cid:88)

c∈C

p∈Pc

(cid:107)p − vιc(p)(cid:107)2
2.

(3)

4.4 Optimization

The optimization of (1) is highly nonlinear due to the data term. However, all other terms are quadratic functions of
vertex coordinates, so minimizing

Equad = Eshape + αsmoothEsmooth + αsharpEsharp

(cid:62)

= V

AshapeV + αsmoothV

(cid:62)

AsmoothV + αsharpV

(cid:62)

AsharpV + b(cid:62)V

is equivalent to solving a system of linear equations. Denoting the sum of the matrices in the equation by Aquad, we
obtain the optimum by solving AquadV = 0 where the vector V is a ﬂattening of the vector V of 3D vertex positions to
a vector of length 3nV.

6

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

The data term is highly nonlinear, but solving the complete optimization problem can be done efﬁciently using A−1
quad
as the preconditioner. For our problem, we use the preconditioned L-BFGS optimizer summarized in Algorithm 1.

0

Algorithm 1: Preconditioned L-BFGS mesh optimization (PL-BFGS)
Mprecond = A−1
quad // stored as LU decomposition
V = T 0
)
m(V
for i ← 0 to Niter do
gtot = αdata
V = L-BFGS-step(V, gtot, Mprecond)

dEdata/dp + AquadV + b

end

5 Datasets

Our method relies on a number of publicly available datasets to compute mesh annotations for our deformations and
assess ﬁtting performance. To assess ﬁtting performance, we use Scan2CAD dataset [9] that consists of 14225 ground-
truth 9 DoF transformations between objects in 1506 reconstructions of indoor scenes from Scannet [13] and 3049 unique
CAD models from ShapeNet [11].

Our deformation framework requires high-quality watertight meshes to support numerically stable optimization, which
does not hold for ShapeNet CAD models. Thus, we remesh these to around 10k–15k vertices using [39], obtaining more
uniform discretizations. To annotate these remeshed CAD models with semantic part labels required by our deformation
procedure, we register them with the corresponding part-labeled meshes from the PartNet dataset [14] by ﬁrst choosing
an appropriate 90° rotation around each axis and then optimizing for a ﬁner alignment using a point-to-point ICP
algorithm [40] between vertices of the two meshes. We annotate the semantic parts for vertices in each mesh by
projecting it from the respective closest vertices of the registered PartNet mesh.

The original ShapeNet meshes, however, are used to extract sharp geometric features, as these have easily detectable
sharp angles between adjacent faces. We label as sharp all edges adjacent to faces with a dihedral angle smaller than
the threshold αsharp = 120°. We further project vertex-wise sharpness labels from the original to the remeshed CAD
models and select a sequence of edges forming the shortest paths between each pair of vertices as sharp.

6 Results

6.1 Evaluation setup

Our performance evaluation of obtained deformations is multifaceted and addresses the following quality-related
questions:

• Scan ﬁtting performance: How well do CAD deformations ﬁt?

• Perceptual performance for deformations: How CAD-like are deformations?

• Contributions of individual energy terms: Which energy terms are essential?

• Deformation ﬂexibility: Can better shapes be achieved by approximating clean meshes rather than noisy

scans?

Fitting and perceptual metrics. We quantify the deformation performance in terms of ﬁtting quality between the scene
scans and 3D CAD models using a family of related measures computed on a per-instance basis. For vertices V = (vi)
of the deformed mesh M, we compute distances to their respective nearest neighbors (NN(vi, S)) in the scan S. We
compute per-instance Accuracy = |Vclose|/|V|, reﬂecting the fraction of closely located vertices, and trimmed minimum
matching distance tMMD = (cid:80)
min(τ, (cid:107)vi −NN(vi, S)(cid:107)1)/|V|, where Vclose = {vi ∈ V : (cid:107)vi −NN(vi, S)(cid:107)1 < τ }
vi∈V

is the set of vertices falling within L1-distance τ to their nearest neighbor in the scan, and τ controls robustness w.r.t.
incomplete scans. We set τ = 0.2 (see supplementary) in our experiments and report Accuracy and tMMD values
averaged over classes and instances.

There is no universally agreed perceptual quality measure for meshes; thus, we opted for a tripartite evaluation for
our resulting deformations. First, we measure dihedral angle mesh error (DAME) [41], revealing differences in local

7

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 3: Deformations obtained using our method and the baselines, with mesh colored according to the Euclidean
distance to its nearest point in the scan. We note that high accuracy scores for Harmonic and ARAP deformations are
achieved at the cost of destroying the initial structure of the mesh, particularly in regions where scan is missing (note
that back side and armrests are gone for chairs in the ﬁrst and second rows). In contrast, our method is better able to
preserve smooth surfaces, sharp features, and overall mesh integrity, while keeping accurate local alignment.

surface quality between the original and the distorted meshes:

DAME(M, Mdef) =

1
|E|

(cid:88)

(cid:12)
(cid:12)Df1,f2 − Df1,f2

(cid:12)
(cid:12) · exp (cid:8)(ZDAMEDf1,f2)2(cid:9),

adjacentf1,f2

where Df1,f2 and Df1,f2 represent oriented dihedral angles between faces f1 and f2 in the original and deformed
meshes, respectively, and ZDAME = (cid:112)log(100/π)/π is a parameter scaling DAME values to [0, 100].
Second, we assess abnormality of deformed shapes with respect to the distribution of the undeformed shapes, building
on the idea of employing deep autoencoders for anomaly detection in structured high-dimensional data. By replicating
the training instances, autoencoders learn features that minimize reconstruction error; for novel instances similar to those
in the training set, reconstruction error is low compared to that of strong outliers. We train six autoencoders [42, 43] for
point clouds using vertices of undeformed meshes separately for the top six classes present in Scan2CAD annotation:
table, chair, display, trashbin, cabinet, and bookshelf. Passing vertices Vdef of a deformed shape to the respective
autoencoder, one can assess how accurately deformed meshes can be approximated using features of undeformed meshes.
This property can be evaluated with Earth Mover’s Distance (EMD) dEMD(Vdef, V(cid:48)
(cid:107)v − φ(v)(cid:107)2,

def) = min

φ:Vdef→V(cid:48)
def

(cid:80)
v∈Vdef

where φ is a bijection, obtained as a solution to the optimal transportation problem involving Vdef and V(cid:48)
intuitively be viewed as the least amount of work needed to transport Vdef vertices to positions of V(cid:48)
Lastly, we assess real human perception of deformations in a user study, detailed in Section 6.3.

def.

def, that can

Optimization details. To perform quantitative comparisons, we use 299 scenes in ScanNet constituting Scan2CAD
validation set [9], but with 697 shapes present in PartNet dataset, amounting to 1410 object instances. Our full
experimental pipeline is a sequence of deformation stages with different optimization parameters, and Hessian being
recomputed before each stage. Speciﬁcally, we perform one part-to-part optimization with parameters αshape =
1, αsmooth = 0, αsharp = 0, αdata = 5 × 104 for 100 iterations, then we perform 5 runs of nearest-neighbor deformation
for 50 iterations with parameters αshape = 1, αsmooth = 10, αsharp = 10, αdata = 103. More details about optimization
and timings are provided in the supplementary.

6.2 Fitting Accuracy: How well do CAD Deformations ﬁt?

We ﬁrst demonstrate how deformation affects scan ﬁtting performance for meshes aligned using different methods,
speciﬁcally, we use true-positive shape alignments computed using Scan2CAD (S2C) [9], End-to-End (E2E) [9], as
well as ground-truth alignments. We start with an aligned mesh, copy the 9 DoF transformation to each of the mesh
vertices, and optimize using our deformation method with parameters described in Section 6.1. We report Accuracy

8

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Method

# TPs
TP undeformed

Ours: NN only
Ours: p2p only
Ours: w/o smooth
Ours: w/o sharp
CAD-Deform

GT

1410
89.2

89.7
90.3
90.6
90.3
91.7

Class avg.
S2C [9] E2E [10] GT

Instance avg.
S2C [9] E2E [10]

499
83.7

84.3
88.3
90.0
86.9
89.8

882
88.5

89.0
89.4
89.6
90.6
90.3

1410
90.6

91.4
91.6
92.3
92.3
93.1

499
79.4

84.7
90.3
90.3
89.4
92.8

882
93.9

94.4
94.9
95.0
95.2
94.6

Table 1: Comparative evaluation of our deformations to true positive (TP) alignments by non-deformable approaches in
terms of Accuracy (%). Note that deformations improve performance for all considered alignment approaches.

Method
# instances

bookshelf
142

cabinet
162

chair
322

display
86

table
332

trashbin
169

other
197

class avg.
201.4

avg.
1410

88.0
90.5

Ground-truth
Ours

98.9
99.1
Table 2: Comparative evaluation of our approach to non-deformable ground-truth and baselines in terms of scan
approximation Accuracy (%). We conclude that our deformations improve ﬁtting accuracy across all object classes
by 2.5 % on average.

75.2
82.2

94.8
95.4

96.6
98.6

89.6
91.0

81.4
84.8

89.2
91.7

90.6
93.1

scores in terms of fraction of well-approximated points in the scan for aligned shapes pre- and post-optimization in
Table 2, achieving improved performance across all considered alignment procedures.

Surprisingly, we improve even over ground-truth alignments by as much as 2.5 %. Thus, we compute per-class scores in
Table 1 (comparison across alignments), reporting improvements of up to 7 % in average scan approximation accuracy
that are consistent across all object classes. We visualize example deformations obtained using our approach and
baselines in Figure 3.

We have discovered our deformation framework to be robust w.r.t. level of detail in the data term in (2) and provide
more detail in the supplementary.

6.3 CAD Quality: How CAD-like are deformed models?

Having obtained a collection of deformed meshes, we aim to assess their visual quality in comparison to two baseline
deformation methods: as-rigid-as-possible (ARAP) [27] and Harmonic deformation [34, 35], using a set of perceptual
quality measures. To bring second-order information about mesh surface in energy formulation of ARAP/Harmonic, we
add the Laplacian smoothness term ELap(V, V(cid:48)) = (cid:80)|V|
i)(cid:107)2, where L(vi) = 1
vj and N (i)
Ni
is a set of one-ring neighbors of vertex vi ∈ V (v(cid:48)
i ∈ V(cid:48)). The details of our user study design and visual assessment
are provided in the supplementary.

i=1 (cid:107)L(vi) − L(v(cid:48)

(cid:80)|N (i)|
j=1

Method

DAME

cls.

inst.

EMD ×10−3 User Accuracy
inst.
cls.

study

inst.

cls.

No deformation

0

0

77

77

8.6

89.2

90.6

ARAP [27]
47.1 45.7
Harmonic [34, 35] 65.1 65.2
20.5 17.2
CAD-Deform

88
104
84
Table 3: Quantitative evaluation of visual quality of deformations obtained using ARAP [27], Harmonic deformation [34,
35], and our CAD-Deform, using a variety of local surface-based (DAME [41]), neural (EMD [42, 43]), and human
measures.

90.8
96.2
91.7

91.8
96.6
93.1

87
102
84

4.0
2.6
7.7

9

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

6.4 Ablation study

To evaluate the impact of individual energy terms in (1) on both scan ﬁtting performance and perceptual quality, we
exclude each term from the energy and compute deformations by optimizing the remaining ones. First, we exclude
sharpness or smoothness terms, optimizing for deformations using the original two-stage method; second, to better
understand the inﬂuence of each stage, we perform experiments with only the ﬁrst or the second stage (a single run
only). We aggregate results into Table 1 and display them visually in Figure 4, concluding that our CAD-Deform
maintains the right balance between ﬁt to the scan and perceptual quality of resulting deformations.

Figure 4: Qualitative results of ablation study usind our deformation framework, with mesh coloured according to the
value of the tMMD measure. Note that including smoothness term is crucial to prevent surface self-intersections, while
keeping sharpness allows to ensure consistency in parallel planes and edges.

Figure 5: Qualitative shape translation results, interpolating between the original mesh (left) and the target mesh (right).

10

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

6.5 Shape morphing results

To demonstrate the ability of our mesh deformation framework to perform shape interpolation, we choose two different
meshes in the same ShapeNet category and optimize our energy (1) to approximate one with the other, see Fig. 5.

7 Conclusion

In this work, we have presented CAD-Deform, a shape deformation-based scene reconstruction method leveraging
CAD collections that is capable of improving over existing alignment methods. More speciﬁcally, we introduce a
composite deformation energy formulation that achieves regularization from semantic part structures, enforces smooth
transformations, and preserves sharp geometric features. As a result we obtain signiﬁcantly improved perceptual quality
of ﬁnal 3D CAD models compared to state-of-the-art deformation formulations, such as ARAP and polyharmonic
deformation frameworks. Overall, we believe that our method is an important step towards obtaining lightweight digital
replica from the real world that are both of high-quality and accurate ﬁts at the same time.

Acknowledgements

The authors acknowledge the usage of the Skoltech CDISE HPC cluster Zhores for obtaining the results presented in
this paper. The work was partially supported by the Russian Science Foundation under Grant 19-41-04109.

11

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

References

[1] Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie
Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, and Andrew Fitzgibbon. Kinectfusion: Real-time 3d
reconstruction and interaction using a moving depth camera. In UIST ’11 Proceedings of the 24th annual ACM
symposium on User interface software and technology, pages 559–568. ACM, 2011.

[2] Richard A. Newcombe, Shahram Izadi, Otmar Hilliges, David Kim, Andrew J. Davison, Pushmeet Kohli, Jamie
Shotton, Steve Hodges, and Andrew Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. In
IEEE ISMAR. IEEE, October 2011.

[3] M. Nießner, M. Zollhöfer, S. Izadi, and M. Stamminger. Real-time 3d reconstruction at scale using voxel hashing.

ACM Transactions on Graphics (TOG), 2013.

[4] T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker, and A. J. Davison. ElasticFusion: Dense SLAM

without a pose graph. In Robotics: Science and Systems (RSS), Rome, Italy, July 2015.

[5] Sungjoon Choi, Q. Zhou, and V. Koltun. Robust reconstruction of indoor scenes. In 2015 IEEE Conference on

Computer Vision and Pattern Recognition (CVPR), pages 5556–5565, June 2015.

[6] Angela Dai, Matthias Nießner, Michael Zollöfer, Shahram Izadi, and Christian Theobalt. Bundlefusion: Real-time
globally consistent 3d reconstruction using on-the-ﬂy surface re-integration. ACM Transactions on Graphics 2017
(TOG), 2017.

[7] Renato F. Salas-Moreno, Richard A. Newcombe, Hauke Strasdat, Paul H. J. Kelly, and Andrew J. Davison.
In CVPR, page 1352–1359. IEEE

Slam++: Simultaneous localisation and mapping at the level of objects.
Computer Society, 2013.

[8] Oliver Mattausch, Daniele Panozzo, Claudio Mura, Olga Sorkine-Hornung, and Renato Pajarola. Object detection
and classiﬁcation from large-scale cluttered indoor scans. In Computer Graphics Forum, volume 33, pages 11–21.
Wiley Online Library, 2014.

[9] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X Chang, and Matthias Nießner. Scan2cad:
Learning cad model alignment in rgb-d scans. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2614–2623, 2019.

[10] Armen Avetisyan, Angela Dai, and Matthias Nießner. End-to-end cad model retrieval and 9dof alignment in 3d
scans. In Proceedings of the IEEE International Conference on Computer Vision, pages 2551–2560, 2019.
[11] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012, 2015.

[12] Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc
Alexa, Denis Zorin, and Daniele Panozzo. Abc: A big cad model dataset for geometric deep learning. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.

[13] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5828–5839, 2017.

[14] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A
large-scale benchmark for ﬁne-grained and hierarchical part-level 3d object understanding. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages 909–918, 2019.

[15] Brian Curless and Marc Levoy. A volumetric method for building complex models from range images. In
Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’96,
page 303–312, New York, NY, USA, 1996. Association for Computing Machinery.

[16] Michael Firman, Oisin Mac Aodha, Simon Julier, and Gabriel J Brostow. Structured prediction of unobserved
voxels from a single depth image. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5431–5440, 2016.

[17] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene
completion from a single depth image. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1746–1754, 2017.

[18] Angela Dai, Charles Ruizhongtai Qi, and Matthias Nießner. Shape completion using 3d-encoder-predictor cnns
and shape synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
5868–5877, 2017.

12

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

[19] Bertram Drost and Slobodan Ilic. 3d object detection and localization using multimodal point pair features. In

3DIMPVT, pages 9–16. IEEE Computer Society, 2012.

[20] Xingyi Zhou, Arjun Karpur, Chuang Gan, Linjie Luo, and Qixing Huang. Unsupervised domain adaptation for 3d
keypoint estimation via view consistency. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair
Weiss, editors, Computer Vision – ECCV 2018, pages 141–157, Cham, 2018. Springer International Publishing.
[21] Haowen Deng, Tolga Birdal, and Slobodan Ilic. 3d local features for direct pairwise registration. In The IEEE

Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.

[22] Yangyan Li, Angela Dai, Leonidas Guibas, and Matthias Nießner. Database-assisted object retrieval for real-time

3d reconstruction. Computer Graphics Forum, 34(2):435–446, 2015.

[23] Mathieu Aubry, Daniel Maturana, Alexei Efros, Bryan Russell, and Josef Sivic. Seeing 3d chairs: exemplar

part-based 2d-3d alignment using a large dataset of cad models. In CVPR, 2014.

[24] Ruiqi Guo, Chuhang Zou, and Derek Hoiem. Predicting complete 3d models of indoor scenes. arXiv preprint

arXiv:1504.02437, 2015.

[25] Saurabh Gupta, Pablo Arbeláez, Ross Girshick, and Jitendra Malik. Aligning 3d models to rgb-d images of
cluttered scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
4731–4740, 2015.

[26] Carsten Stoll, Zachi Karni, Christian Rössl, Hitoshi Yamauchi, and Hans-Peter Seidel. Template deformation for

point cloud ﬁtting. In SPBG, pages 27–35, 2006.

[27] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry processing,

volume 4, pages 109–116, 2007.

[28] Miao Liao, Qing Zhang, Huamin Wang, Ruigang Yang, and Minglun Gong. Modeling deformable objects from a

single depth camera. pages 167 – 174, 11 2009.

[29] Jascha Achenbach, Eduard Zell, and Mario Botsch. Accurate face reconstruction through anisotropic ﬁtting and

eye correction. In VMV, pages 1–8, 2015.

[30] B. Amberg, S. Romdhani, and T. Vetter. Optimal step nonrigid icp algorithms for surface registration. In 2007

IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8, 2007.

[31] Cedric Cagniart, Edmond Boyer, and Slobdodan Ilic. Iterative mesh deformation for dense surface tracking. In
2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, pages 1465–1472.
IEEE, 2009.

[32] Sang Il Park and Seong-Jae Lim. Template-based reconstruction of surface mesh animation from point cloud

animation. ETRI journal, 36(6):1008–1015, 2014.

[33] Tamal K Dey, Bo Fu, Huamin Wang, and Lei Wang. Automatic posing of a meshed human model using point

clouds. Computers & Graphics, 46:14–24, 2015.

[34] Mario Botsch and Leif Kobbelt. An intuitive framework for real-time freeform modeling. ACM Transactions on

Graphics (TOG), 23(3):630–634, 2004.

[35] Alec Jacobson, Elif Tosun, Olga Sorkine, and Denis Zorin. Mixed ﬁnite elements for variational surface modeling.

In Computer graphics forum, volume 29, pages 1565–1574. Wiley Online Library, 2010.

[36] L. He and S. Schaefer. Mesh denoising via l0 minimization. Proc. ACM SIGGRAPH, pages 64:1–64:8, 01 2013.
[37] Eitan Grinspun, Anil N. Hirani, Mathieu Desbrun, and Peter Schröder. Discrete shells. In Proceedings of the 2003
ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA ’03, page 62–67, Goslar, DEU, 2003.
Eurographics Association.

[38] Stefan Fröhlich and Mario Botsch. Example-driven deformations based on discrete shells. Comput. Graph. Forum,

30:2246–2257, 12 2011.

[39] Jingwei Huang, Hao Su, and Leonidas Guibas. Robust watertight manifold surface generation method for shapenet

models. arXiv preprint arXiv:1802.01698, 2018.

[40] Szymon Rusinkiewicz and Marc Levoy. Efﬁcient variants of the icp algorithm. In Proceedings Third International

Conference on 3-D Digital Imaging and Modeling, pages 145–152. IEEE, 2001.

[41] Libor Váša and Jan Rus. Dihedral angle mesh error: a fast perception correlated distortion measure for ﬁxed

connectivity triangle meshes. Computer Graphics Forum, 31(5):1715–1724, 2012.

[42] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and

generative models for 3d point clouds. In International Conference on Machine Learning, pages 40–49, 2018.

13

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

[43] Vage Egiazarian, Savva Ignatyev, Alexey Artemov, Oleg Voynov, Andrey Kravchenko, Youyi Zheng, Luiz Velho,
and Evgeny Burnaev. Latent-space laplacian pyramids for adversarial representation learning with 3d point clouds.
12 2019.

14

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

A Statistics on the used datasets

In Tables 4 & 5, we summarize statistical information on the number of instances and categories considered in our
evaluation. As we require parts annotations as an important ingredient in our deformation, we only select instances
in Scan2CAD [9] where the associated parts annotation in PartNet [14] is available, resulting in total in 9 categories
(25%), 572 instances (18%), and 1979 annotated correspondences (14%). Note that the vast majority of cases remain
within our consideration, keeping our evaluation comprehensive.

Collection

Categories

Instances Corresp.

Scan2CAD [9]

35
24
Table 4: Overall statistics on the numbers of categories, instances, and correspondences present in our datasets.

w/parts annotations

14,225
12,246

3,049
2,477

Name

Scan2CAD

corresp.

shapes

PartNet ∩ Scan2CAD
corresp.

shapes

Shape categories used in our evaluation:
4351
chair
2594
table
1258
cabinet
1042
trash bin
812
bookshelf
762
display

4677
2616
1401
1044
824
770

652
830
310
89
150
165

632
822
294
88
145
161

Shape categories NOT used in our evaluation:
bed
ﬁle cabinet
bag
lamp
bathtub
microwave
sofa
laptop
keyboard

50
70
9
55
96
37
247
24
11
Table 5: The top 15 most frequent ShapeNet categories in Scan2CAD dataset including a detailed information on those
with the availability of the corresponding parts annotations.

342
290
165
135
129
98
60
51
48

355
294
165
135
474
99
577
51
62

47
68
9
55
25
36
20
24
9

We further select the most well-presented six shape categories as our core evaluation set, outlined in Table 5. Note that
as our method is non-learnable, we can just as easily experiment with the remaining categories, at the cost of somewhat
reduced statistical power.

B Optimization details

Our full experimental pipeline is a sequence of deformation stages with different optimization parameters, and
Hessian being recomputed before each stage. Speciﬁcally, we perform one part-to-part optimization with parameters
αshape = 1, αsmooth = 0, αsharp = 0, αdata = 5 × 104 for 100 iterations, then we perform 5 runs of nearest-neighbor

15

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

deformation for 50 iterations with parameters αshape = 1, αsmooth = 10, αsharp = 10, αdata = 103. Such number of
iterations was sufﬁcient to achieve convergence with energy changes less than 10−1 in our experiments. Runtime of
our method breaks into cost computation (∼0.3 s), backward (∼0.2 s), and optimization steps containing the main
bottleneck (sparse matrix-vector multiplication) (∼1.2 s) for a typical 104 vertices mesh. All operations can be easily
further optimized.

C Qualitative ﬁtting results

Method

ARAP
Harmonic

bookshelf

cabinet

chair

display

table

trash bin

other

class avg.

avg.

52.48
64.77

41.77
58.74

45.52
68.06

51.30
64.22

41.77
58.26

57.00
80.13

39.75
61.70

47.08
65.12

45.67
65.18

21.54
22.44
27.15
26.43
24.8

Ours: NN only
Ours: p2p only
Ours: w/o smooth
Ours: w/o sharp
CAD-Deform

18.37
21.12
27.05
24.87
24.4
Table 6: Quantitative results of local surface quality evaluation using DAME measure [41] (the smaller, the better,
normalized to a maximum score of 100), where our CAD-Deform compares favourably to the baselines across
all considered categories. Note, however, how surface quality signiﬁcantly decreases when smoothness and sharp
feature-related terms are dropped.

16.07
18.34
23.26
21.18
17.6

17.64
18.54
24.30
22.19
20.5

14.14
15.57
20.95
19.10
17.2

23.39
24.28
29.27
25.98
24.1

18.69
18.76
24.48
22.47
21.6

7.31
9.51
14.50
13.34
11.4

18.13
15.30
24.39
21.04
19.4

EMD ×10−3

Ground-truth

ARAP [27]
Harmonic [34, 35]
CAD-Deform

bookshelf

cabinet

chair

display

table

trash bin

class avg.

77.8

80.3
94.0
79.0

78.9

86.5
110.6
81.1

76.1

88.5
95.8
80.3

77.5

85.4
95.3
91.7

77.3

86.8
103.2
87.0

73.1

98.1
122.6
87.4

76.8

87.6
103.6
84.4

avg.

77.0

87.3
101.7
83.8

Table 7: Results of LSLP-GAN reconstruction in terms of Earth-Mover’s Distance between reconstructed and original
point clouds of mesh vertices.

In Figure 6, we display a series of qualitative results with a variety of shape deformations with different classes of
instances. Comparing to baselines, our framework achieves accurate ﬁt while preserving sufﬁcient perceptual quality.

Table 6 reports the results of surface quality evaluation using deformations obtained with our CAD-Deform vs. the
baselines, category-wise. While outperforming the baseline methods across all categories, we discover the smoothness
and sharpness energy terms to be the crucial ingredients in keeping high-quality meshes.

Figure 7 displays visually the deformation results using the three distinct classes, highlighting differences in surfaces
obtained using the three methods.

Table 7 reports shape abnormality evaluation results across the six considered categories. Baselines show (Fig. 8) low
reconstruction quality as evidenced by a larger number of black points. In other words, comparing to CAD-Deform, the
distance from these meshes to undeformed ones is mush larger.

In Figure 9, we show a series of examples for CAD-Deform ablation study. Perceptual quality degrades when excluding
every term from the energy.

D Morphing

In this section, we present an additional series of examples of morphing properties (Fig. 10). Every iteration of
optimization process gradually increases the quality of ﬁt. With CAD-Deform we can morph each part to imitate the
structure of the target shape.

16

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Accuracy, % class avg.

avg.

Ground-truth
Level 1 (object)
Level 2
Level 3
Level 4 (parts)

89.22
89.25
89.16
89.40
91.65

90.56
90.79
91.21
91.05
93.12

Table 8: Comparative evaluation of our approach in terms of Accuracy on different levels of detail.

E PartNet annotation

This set of experiments shows how quality of ﬁtting depends on mesh vertices labelling. We can provide labels for
mesh in difﬁrent ways depending on the level in PartNet hierarchy [14]. We observe the increase of ﬁtting quality with
greater level of detail (Table 8). Examples presented in Figure 11 are selected as the most distinguishable deformations
on different levels. There are minor visual differences in deformation performance of part labeling level.

F Fitting Accuracy analysis

CAD-Deform deformation framework is sensitive to Accuracy threshold τ for the distance between mesh vertices and
close scan points. In Figure 12 variation of τ threshold is presented and we selected τ = 0.2 m for ﬁtting Accuracy
metric.

G Perceptual assessment and user study details

Having obtained a collection of deformed meshes, we aim to assess their visual quality in comparison to two baseline
deformation methods: as-rigid-as-possible (ARAP) [27] and Harmonic deformation [34, 35], using a set of perceptual
quality measures. The details of our user study design and visual assessment are provided in the supplementary. To this
end, we use original and deformed meshes to compute DAME and reconstruction errors, as outlined in Section 6.1,
and complement these with visual quality scores obtained with a user study (see below). These scores, presented in
Table 3, demonstrate that shapes obtained using CAD-Deform have 2× higher surface quality, only slightly deviate
from undeformed shapes as viewed by neural autoencoders, and receive 2× higher ratings in human assessment, while
sacriﬁcing only 1.1–4.5 % accuracy compared to other deformation methods.

Design of our user study. The users were requested to examine renders of shapes from four different categories: the
original undeformed shapes as well as shapes deformed using ARAP, Harmonic, and CAD-Deform methods, and give a
score to each shape according to the following perceptual aspects: surface quality and smoothness, mesh symmetry,
visual similarity to real-world objects, and overall consistency. Ten random shapes from each of the four categories
have been rendered from eight different views and scored by 100 unique users on a scale from 1 (bad) to 10 (good).
The resulting visual quality scores are computed by averaging over users and shapes in each category.

In Figure 13, we present a distribution of user scores over different deformation methods and shapes. It can be
clearly seen that users prefer our deformation results to baselines for all of the cases, which is obvious from the
gap between histogram of CAD-Deform and ARAP/Harmonic histograms. At the same time, shapes deformed by
CAD-Deform are close to undeformed ShapeNet shapes in terms of surface quality and smoothness, mesh symmetry,
visual similarity to real-world objects, and overall consistency. Besides, in Tables 9, 10 we provide numbers for
evaluation of ARAP/Harmonic deformations w.r.t. the change of Laplacian term weight.

17

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Lap. term
weight
αLap = 10−2
αLap = 10−1
αLap = 1
αLap = 5
αLap = 20

GT

90.9
91.0
91.0
90.9
90.9

Class avg.
S2C [9] E2E [10] GT

Instance avg.
S2C [9] E2E [10]

81.3
81.3
81.3
81.2
81.2

90.0
90.0
89.9
89.9
89.9

92.0
92.0
91.9
91.9
91.8

80.9
80.9
80.9
80.9
80.8

90.8
90.7
90.7
90.7
90.6

Table 9: Comparative evaluation of ARAP deformations w.r.t. the change of Laplacian term weight in terms of Accuracy
(%).

Lap. term
weight
αLap = 10−2
αLap = 10−1
αLap = 1
αLap = 5
αLap = 20

GT

96.3
96.3
96.3
96.2
96.2

Class avg.
S2C [9] E2E [10] GT

Instance avg.
S2C [9] E2E [10]

94.3
94.2
94.2
94.0
93.8

96.6
96.6
96.6
96.6
96.5

96.7
96.7
96.7
96.6
96.6

94.5
94.4
94.2
94.0
93.8

96.9
96.9
96.9
96.8
96.7

Table 10: Comparative evaluation of Harmonic deformations w.r.t. the change of Laplacian term weight in terms of
Accuracy (%).

18

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 6: Qualitative shape deformation results using obtained using ARAP [27], Harmonic deformation [34, 35], and
our CAD-Deform. Mesh surface is colored according to the value of tMMD measure, with darker values corresponding
to the larger distance values.

19

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 7: Qualitative comparison of deformations obtained using ARAP [27], Harmonic deformation [34, 35], and our
CAD-Deform, with shapes coloured according to the value of DAME measure [41]. Our approach results in drastic
improvements in local surface quality, producing higher-quality surfaces compared to other deformations.

20

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 8: Qualitative comparison of reconstruction of point clouds extracted from mesh vertices. These meshes are
obtained using ARAP [27], Harmonic deformation [34, 35], and our CAD-Deform, the ﬁrst column corresponds to
original undeformed meshes. The color of reconstructed point clouds is related to Earth-Mover’s Distance between
reconstructed and original point clouds of mesh vertices.

21

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 9: Qualitative results of ablation study usind our deformation framework, with mesh coloured according to the
value of the tMMD measure.

22

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 10: Qualitative shape translation results, interpolating between the original mesh (left) and the target mesh
(right).

23

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 11: Deformation performance depending on different level of labelling from the PartNet dataset [14]. Deformed
mesh surfaces are colored according to the value of tMMD measure, with darker values corresponding to the larger
distance values.

24

CAD-Deform: Deformable Fitting of CAD Models to 3D Scans

Figure 12: Fitting Accuracy vs. varying τ threshold for the distance between mesh vertices and close scan points.

Figure 13: Distribution of user scores averaged by ten shapes from original ShapeNet [11], meshes deformed with
ARAP [27], Harmonic [34, 35] and CAD-Deform.

25

