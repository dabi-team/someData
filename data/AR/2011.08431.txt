Association Rules Enhanced
Knowledge Graph Attention Network

Zhenghao Zhang, Jianbin Huang* and Qinglin Tan

School of Computer Science and Technology, Xidian University, Xi’an, 710071, China
Email: zhangzhenghao1108@126.com

Knowledge graphs enable a wide variety of applications, including information
retrieval, question answering, and hypothesis generation. Despite the great eﬀort
invested in their creation and maintenance, most existing knowledge graphs suﬀer
from incompleteness. Embedding knowledge graphs into continuous vector spaces
has recently attracted increasing interest in knowledge base completion. However,
in most existing embedding methods, only fact triplets are utilized, and logical
rules have not been thoroughly studied for the knowledge base completion task. To
overcome the problem, we propose an association rules enhanced knowledge graph
attention network (AR-KGAT). The AR-KGAT captures both entity and relation
features for high-order neighborhoods of any given entity in an end-to-end manner
under the graph attention network framework. The major component of AR-
KGAT is an encoder of an eﬀective neighborhood aggregator, which addresses the
problems by aggregating neighbors with both association-rules-based and graph-
based attention weights. Additionally, the proposed model also encapsulates the
representations from multi-hop neighbors of nodes to reﬁne their embeddings. The
decoder enables AR-KGAT to be translational between entities and relations while
keeping the superior link prediction performance. A logic-like inference pattern
is utilized as constraints for knowledge graph embedding. Then, the global loss
is minimized over both atomic and complex formulas to achieve the embedding
task. In this manner, we learn embeddings compatible with triplets and rules,
which are certainly more predictive for knowledge acquisition and inference. We
conduct extensive experiments on two benchmark datasets: WN18RR and FB15k-
237, for two knowledge graph completion tasks: the link prediction and triplet
classiﬁcation to evaluate the proposed AR-KGAT model. The results show that
the proposed AR-KGAT model achieves signiﬁcant and consistent improvements
over state-of-the-art methods.

Keywords: Knowledge graphs, graph attention network, association rules, knowledge
inference, high-order neighborhood, embedding propagation.

0
2
0
2

v
o
N
4
1

]

R

I
.
s
c
[

1
v
1
3
4
8
0
.
1
1
0
2
:
v
i
X
r
a

1.

INTRODUCTION

Over recent years,
large-scale knowledge base, such
as Freebase [1], DBpedia [2], and YAGO, have been
developed to store structured information of common
knowledge. KBs are represented as directed multi-
relational graphs, Knowledge Graphs (KGs), where
entities and relations are represented as nodes and as
edges of diﬀerent types, respectively. They usually
consist of numerous facts structured in the triplets:
<head entity, relation, tail entity>, e.g. <Melbourne,
city-of, Australia> and <Paris, Capital-of, France>.

Through rich factual knowledge to be organized and
stored, KGs play a critical role in various natural
language processing (NLP) applications in recent years,
including question answering ([3], [4]), machine reading
[5], information retrieval, and semantic searching ([6],
[7]), and personalized recommendation [4]. Many
large-scale KGs have been constructed with millions

of entities and relations. However, the existing facts
and newly created knowledge are too vast, and thus
a lot of valid triplets are missed in the real world [8].
For example, nationalities or birthplaces are missed in
more than 70% of the person entries in Freebase. As
shown in Figure 1, the nationality of Kobe Bryant (a
famous basketball star) is missed. Thus, the target
triplet (Kobe Bryant, nationality, ?) does not exist as
a knowledge graph to be retrieved.

relation prediction,

KGs are inherently discrete and incomplete. To
also
alleviate this drawback,
referred to as knowledge base completion,
infers the
missing facts based on the given facts. Enormous eﬀorts
have been devoted to knowledge base completion to
estimate missing relations between entities under the
supervision of the existing knowledge graph.

Knowledge graph embedding (KGE) successfully
handled the problems of symbolic nature in various

The Computer Journal, Vol. ??, No. ??,

????

 
 
 
 
 
 
2

Jianbin Huang

KGs.
In KGE, the components of KG: entities
and relations were embedded into a low dimensional
continuous vector space, while speciﬁc properties of
the original graph were preserved. Accordingly, the
inherent structure of the KG is preserved, while the
manipulation is simpliﬁed. KGE has recently attracted
in knowledge base completion
increasing interest
and inference, with progressive advancement from
the translational models (TransE[9], TransH[10], and
DistMult[11]) to the recent deep CNN models (e.g.,
ConvE[12] and ConvKB[13]).
in these
embedding models, each triplet was independently
the potential
processed,
information of knowledge base.
Rich semantical
and latent relationships between them have not been
exploited, which are inherently implicit in the local
neighborhood surrounding a triplet.

resulting in the loss of

However,

Recently, the Graph Neural Network (GNN) was
proposed to utilize the graph connectivity structure
as another way of learning graph node embedding.
However, in most of the existing researches on GNNs,
node representations were learned in simple undirected
graphs. Since KGs are multi-relationally structured,
unlike homogeneous graphs[14], na¨ıve application of
the existing GNN models in handling relational graphs
induces over-parameterization. Also, it is limited to
only node representations learning. Therefore,
it is
required to advance the GNN framework to jointly
learn node and relation representations by utilizing KG
embedding techniques.

Sub-
FIGURE 1: Example of a knowledge graph.
graph consists of the relationships between entities
(solid lines) and inferred relationships (dashed lines).
Inferred relationships are initially hidden.

Moreover, many existing methods focused only on
fact-triplets from the given knowledge graph. The
models have the following limitations: (1) the logical
rules obtained from KB do not lead to other scientiﬁc
basis or the reduced sparseness of knowledge graphs.
(2) joint embedding of the rules and knowledge graph
is challenging since the traditional algebraic operations
and rules of logic symbols cannot be naively used in
triplets. A ﬂexible and declarative language is provided

Thus,

by the logical rules for expressing rich background
knowledge.
to transfer human knowledge
to entity and relation embedding, the logical rules
can be integrated into knowledge graph embedding,
strengthening the learning process.

In order to overcome the limitations, we propose
an end-to-end GAT framework for multi-relational
knowledge graphs, Association Rules Enhanced Knowl-
edge Graph Attention Network (AR-KGAT). The pro-
posed AR-KGAT generalizes and extends the atten-
tion mechanism by taking the beneﬁts of both logic-
based and graph-based attention simultaneously. Also,
entity-relation composition operations are systemati-
cally leveraged from knowledge graph embedding tech-
niques for relation prediction. Speciﬁcally, the proposed
AR-KGAT framework consists of three main designs to
correspondingly address the challenges above in knowl-
edge graph embedding. First, our uniform framework
considers two types of association rules: one-to-one and
n-to-one rules to fully utilize rich information of rules
to augment knowledge inference. Then, through the
mining algorithm, only rules with conﬁdences greater
than a threshold are selected in our embedding model.
As a result, the sparseness of knowledge graphs is re-
duced. Second, in the proposed novel aggregator, the
neural attention mechanism and association rules-based
weighting mechanism are simultaneously used to learn
the weights of the neighborhood of any given entity.
Also, we propose the recursive embedding propagation
to update an entity’s embedding based on its neighbor-
hood. Through the recursive embedding propagation,
high-order relations are captured in linear time com-
plexity. Last, the AR-KGAT is learned by minimizing
a global loss consisting of triplets and rules terms to ob-
tain entity and relation embeddings. The learned em-
beddings can better predict knowledge acquisition and
inference since it is compatible with both triplets and
logical rules.

Extensive comparisons are conducted to evaluate the
proposed rule-enhanced method under two benchmark
link predic-
datasets on several KG completion tasks:
tion and triplet classiﬁcation. The experimental anal-
ysis shows that our model provides remarkable perfor-
mance gains compared to state-of-the-art methods.
The contributions of this paper are as follows:
(1)Triplets and logical rules are jointly modeled in the
proposed uniﬁed framework to achieve more predictive
entity and relation embeddings.

(2) Two types of association rules are introduced, and
the association rules and the corresponding promotion
degree are automatically mined.

(3) KG-speciﬁc attention based neighborhood aggre-
gator is proposed. It recursively propagates represen-
tations along with high-order connectivity from an en-
tity’s neighbors to update its representation. Weights of
cascaded propagations are generated through the inte-
gration of logic- and graph-based attention mechanisms
to reveal the importance of such high-order connectiv-

The Computer Journal, Vol. ??, No. ??,

????

Association Rules Enhanced Knowledge Graph Attention Network

3

ity.

2.2. Convolution Based Models

(4) The representations for entity and relation are
learned by a global loss function compatible with both
triplets and high-order logical rules.

The remainder of this paper is organized as follows.
Section 2 brieﬂy reviews the related works. Section 3
presents the preliminaries of our proposed model. The
details of our approach are described in Sections 4 and
5. The experimental results of the proposed model are
analyzed in section 6. Section 7 concludes this paper
with a future research direction.

2. RELATED WORK

Recently, several variants of knowledge graph embed-
ding methods have been proposed for relation predic-
tion. These methods can be broadly classiﬁed as: trans-
lational based, convolutional neural network based, and
graph neural network based models. Moreover, existing
research of jointly embedding KG and logical rules are
also introduced.

2.1. Translational Models

there have been multiple
Starting with TransE,
proposed approaches that use simple operations like dot
products and matrix multiplications to compute a score
function. Other transition-based models extend TransE
to additionally use projection vectors or matrices to
translate head and tail embeddings into the relation
vector space, such as: TransH [9], TransR [10], and
TransD [15], STransE [16]. TransH allows entities
to have multiple representations. To obtain multiple
representations of an entity, TransH projects an entity
vector into relation-speciﬁc hyperplanes.
TransR
also handles the problem of TransE by introducing
relation spaces.
It allows an entity to have various
vector representations by mapping an entity vector
into relation-speciﬁc spaces.
To handle multiple
types of relations, TransD constructs relation mapping
matrices dynamically by considering entities and a
relation simultaneously. However, these models lose
the simplicity and eﬃciency of TransE. Furthermore,
DISTMULT [11] and ComplEx [17] use a tri-linear
dot product to compute the score for each triple.
Recent research has shown that using relation paths
between entities in the KBs could help to get contextual
information for improving KB completion performance
([7], [18, 19, 20]). These translational models are faster,
require fewer parameters and are relatively easier to
train, but result in less expressive KG embeddings.
Translational approaches on KG embedding work in
a transductive manner. They require that all entities
should be seen during training. Such limitation hinders
them from eﬃciently generalizing to emerging entities.

Translation based embedding models are a popular
form of representation model. While translational
models learn representations using simple operations
and limited parameters,
they produce low quality
representations.
Shortcomings of translation based
models however, limits their practicability as knowledge
completion algorithm. In contrast, Convolution based
models
learn more expressive representations due
to their parameter eﬃciency and consideration of
complex relations. Several recent works suggest that
convolutional neural network (CNN) based models
generate richer and more expressive feature embeddings
and hence also perform well on relation prediction.
[12] proposed ConvE—the ﬁrst model
Dettmers et al.
applying CNN for the KB completion task. ConvE
uses stacked 2D convolutional ﬁlters on reshaping of
entity and relation representations, thus increasing their
expressive power, while remaining parameter eﬃcient
at the same time. ConvKB [13] is another convolution
based method which applies convolutional ﬁlters of
width l on the stacked subject, relation and object
embeddings for computing score.

However, both translational and CNN based models
still have not fully exploited the potential of the knowl-
edge base since they suﬀer from the following limita-
tions: these methods process each triple independently
and hence fail to encapsulate the semantically rich and
latent relations that are inherently implicit in the local
neighborhood surrounding a triple.

2.3. Graph Neural Network Based Models

this

In [21], the authors propose applying Graph Neural
Networks (GNNs) on the KG, which generates the
embedding of a new entity by aggregating all its known
neighbors. In addition, two popular variants of GNN,
including Graph Convolutional Networks (GCN) and
Graph Attention Networks (GAT), have been proposed
shortcoming in modeling graph-
to address
structured data which aggregate local neighborhood
information for each node, and successfully applied to
various domains. However, their model aggregates the
neighbors via simple pooling functions, which neglects
the diﬀerence among the neighbors. Other works like
[22] aim at embedding nodes for node classiﬁcation
given the entire graph and thus are inapplicable for
inductive KG-speciﬁc tasks. A graph based neural
network model called R-GCN [23] is an extension of
applying graph convolutional networks (GCNs) [24] to
relational data.
It applies a convolution operation to
the neighborhood of each entity and assigns them equal
weights. This graph based model does not outperform
the CNN based models. Existing methods either learn
KG embeddings by solely focusing on entity features
or by taking into account the features of entities and
relations in a disjoint manner.

The Computer Journal, Vol. ??, No. ??,

????

4

Jianbin Huang

2.4. Jointly embedding KG and logic rules

Most existing methods embed the knowledge graph
based solely on triples contained in the knowledge
graph. Several recent works try to incorporate other
types of available knowledge, e.g., relation paths ([25],
[18]), relation type-constraints [26], entity types [15],
and entity descriptions [27], to learn better embeddings.
Logic rules have been widely used in knowledge
inference and acquisition [28, ?], usually on the basis
of Markov logic networks ([29, 30, 31]). Recent works
have shown that the inclusion of background knowledge,
such as logical rules, can improve the performance
of embeddings in downstream machine learning tasks.
There has been growing interest in combining logical
[32] tried to
rules and embedding models. Wei et al.
leverage both embedding methods and logical rules into
the knowledge graph embedding for KG completion. In
their work, however, rules are modeled separately from
embedding methods, serving as post-processing steps,
and thus will not help to obtain better embeddings.
[16] proposed a joint model which
Rockt¨aschel et al.
jointly encodes the rules into the embedding.
In [33]
a new method named LR-KGE is proposed to jointly
embed the knowledge graph and logic rules. However,
their work focuses on relation extraction task and
creates embeddings for entity pairs, and hence fails to
discover relations between unpaired entities.

3. PRELIMINARIES

We begin this section by introducing the notations and
deﬁnitions of Knowledge Graph (KG) used in the rest
of the paper, followed by a brief overview of Graph
Attention Network (GATs) for undirected graphs and
its extension to knowledge graphs.

3.1. Knowledge Graph

Knowledge Graphs (KGs) can be represented by a
collection of valid factual triples in the form of (head
entity, relation, tail entity) denoted as (h, r, t). Each
triple consists of two entities h, t ∈ E and a relation
r ∈ R, h denotes the head entity of a triple and t
denotes the tail entity of a triple. Moreover, for each
entity e, we denote by NK(e) its neighborhood in K,
i.e., all related entities with the involved relations.
Formally, NK(e) = {(r, e(cid:48)) | (e, r, e(cid:48)) ∈ K} . We denote
the projection of NK(e) on E and R by NE(e) and NR(e),
respectively. Here NE(e) are neighbors and NR(e) are
neighboring relations.

a scoring function f, such that for a given input
triple (h, r, t), f gives the likelihood of (h, r, t) being
a valid triple by optimizing the translation principle
er
h + er ≈ er
t . Finally, to learn the entity and relation
representations, an optimization problem is solved for
maximizing the plausibility of the triple in the KG.

Let us

take a widely used knowledge graph
embedding model TransE (Bordes et al., 2013) as an
example to illustrate it explicitly: TransE employs
a transitional characteristic to model relationships
between entities, in which it assumes that if (h, r, t)
the embedding of head entity h
is a valid fact,
plus the embedding of relation r should be close
to the embedding of tail entity t,
g(h, r, t) =
(cid:107)vh + vr − vt(cid:107)p
p of the positive triple (h, r, t) should be
close to 0 and smaller than score of negative triples
(h(cid:48), r, t(cid:48)) . Herein, vh, vt ∈ Rd and vr ∈ Rk are the
embedding for h, t and r respectively. Moreover, the
negative triple (h(cid:48), r, t(cid:48)) is obtained from (h, r, t) by
replacing h by h ’ or t by t(cid:48). A lower score of g(h, r, t)
suggests that the triplet is more likely to be valid, and
vice versa. The training of TransE aims to optimize the
discrimination between positive triplets and negative
ones.

i.e.

3.3. Graph Attention Networks

Graph convolutional networks gather information from
the entity’s neighborhood and all neighbors contribute
equally in the information passing. To address the
shortcomings of GCNs,
introduced Graph Attention
Networks (GATs). GATs learn to assign changing levels
of importance to nodes in every node’s neighborhood,
rather than treating all neighboring nodes with equal
importance, as is done in GCN. The input feature
set of nodes to a layer is x = {(cid:126)x1, (cid:126)x2, . . . , (cid:126)xN }. A
layer produces a transformed set of node feature vectors
x(cid:48) = {(cid:126)x(cid:48)
i are input and
output embeddings of the entity ei, and N is number of
entities, a single GAT can be described as:

N } , where (cid:126)xi and (cid:126)x(cid:48)

2, . . . , (cid:126)x(cid:48)

1, (cid:126)x(cid:48)

eij = a (W (cid:126)xi, W (cid:126)xj) ,

where eij is the attention value of the edge in G, W is
a parameterized linear transformation matrix mapping
the input features to a higher dimensional output
feature space, and a is any attention function of our
choosing. Attention values for each edge are the
importance of the edge’s features for a source node.
Here, the relative attention value can be computed
using a softmax function over all the values in the
neighborhood.

3.2. Knowledge Graph Embedding

4. ASSOCIATION RULES MINING

Knowledge graph embedding is an eﬀective way
to parameterize
entities and relations as vector
representations, while preserving the graph structure.
Embedding models try to learn an eﬀective low-
dimensional representations of entities, relations, and

In the proposed framework, the input and output are
triplets of the knowledge graph G and the association
rules of the two types with corresponding scores. The
main steps of the proposed framework are described in
this section as follows.

The Computer Journal, Vol. ??, No. ??,

????

Association Rules Enhanced Knowledge Graph Attention Network

5

FIGURE 2: The overall framework of association rules mining from a knowledge graph.

4.1. Rules Modeling

linked by relation rt.

A knowledge base (KB) is represented by a set of
triples T = {(h, r, t)} with each triple consists of two
entities h, t ∈ E and a relation r ∈ R.E stands for the
set of entities while R stands for the set of relations.
For any triple (ei, rk, ej) is taken as a ground atom
which applies a relation rk to a pair of entities (ei, ej)
and can be modeled by the translation assumption,
i.e., relations act as translations between head and tail
entities. Moreover, triples are represented as atomic
formulae and modeled by the translation assumption,
while n-hop logical rules can then be interpreted as
a complex formula f , constructed by combining a set
of ground atoms(triples) with logical connectives, and
modeled by t-norm fuzzy logics.

A rule can present an abstract pattern mined from
the data.
It is found that rules in a KG are simply
not independent of each other. For an entity e, one
neighboring relation r1 may imply the existence of
another neighboring relation r2. Taken Figure 1 as
example, the fact that one “plays for” Los-Angeles
Lakers always implies that “he is a Basketball Player
of Lakers”. Additionally, a neighbor relation “play
for” Los Angeles could help to imply the relation of
”live in” since there are other athletes playing for
Lakers while living in Los-Angeles city. Hence, it is
signiﬁcantly beneﬁcial to exploit such association rules
in an entity’s neighborhood to make the aggregation
more informative.

In this paper, we consider two types of association

rules, which will be used in our proposed model:

Deﬁnition 1.

(One-to-One Association
Rules): One-to-One Association rules can be divided
into two categories:

Type 1 (Inference Rule): inference rule discovers
correlation of one-hop relations with the same head and
tail entities. This Association rule can be represented
in the form of ∀x, y : (x, rs, y) ⇒ (x, rt, y), stating that
any two entities linked by relation rs should also be

For instance, a universally quantiﬁed rule ∀x, y :
(x, capital − of, y) ⇒ (x, located − in, y) might be
instantiated with the concrete entities of Beijing and
China, giving the ground rule:

(Beijing, Capital−of, China) ⇒ (Beijing, located−in, China)

Type 2 (Anti-symmetry Rule):

An anti-
symmetry rule can be denoted in the form of ∀x, y :
(x, ra, y) ⇒ (y, rb, x) . The relation ra can imply the
anti-symmetry relation rb, which denotes that two
relations ra and rb are anti-symmetrical. For example,
(apple, hypernym, fruit) ⇔ (fruit, hyponym, apple).
Speciﬁcally, an anti-symmetry rule is undirected.

Deﬁnition 2. (N-to-One Association Rules):
N-to-One association rule is quite similar in nature
from one-to-one association rules we introduce above.
When n=2, a speciﬁc two-to-one transitivity rule can
be represented by:

∀x, y, z : (x, rs1, y) ∧ (x, rs2, z) ⇒ (x, rt, z) .

This transitivity rule above denotes that if x and y are
linked by relation s1 and y and z are linked by s2, x and
z will be linked by relation t.

For instance, a KB may contain the fact that a
child has a mother, then the mother’s husband is most
likely the father.
In Figure 1, we deﬁne an instance
for the entities Kobe, Vanessa and Natalia, and they
are connected through the triples including (Vanessa,
is mother, Natalia), (Kobe, is father, Natalia), (Kobe,
marry with, Natalia).
In the example, we can mine
the two-to-one transitivity rule: (x1, hasamother, x2) ∧
(x2, marrywith, x3) ⇒ (x1, hasaf ather, x3)

It is well known that rules are important for reasoning
and they are also useful for KG completion, which is the
process to automatically extract new facts from existing
ones (e.g., link prediction). In this paper, both one-to-
one and n-to-one association rules are used to discover
correlations in Knowledge Base (KB). The goal of our

The Computer Journal, Vol. ??, No. ??,

????

6

Jianbin Huang

paper is to incorporate such two types of association
rules from KBs in graph neural network framework
and evaluate whether these rules can indeed improve
the quality of embedding. Moreover, it is easy to see
that besides these two types of rules, our framework
is general enough to handle other types of rules. The
investigation of other types of association rules will be
left for future work.

fa and fb to all entities respectively.

Deﬁnition 3 (Support Degree):

In a KG,
psupport (fa ⇒ fb) indicates the proportion of associa-
tion rule (fa ⇒ fb) , which denotes that entities with
formula fa also have fb as a neighboring formula to all
entities, and can be denoted as follows:

psupport (fa ⇒ fb) =

ne (fa ∧ fb)
N

.

4.2. Rules Extraction

LR-KGE [33] was proposed to manually ﬁlter the top-
ranked wrong rules since the results inferred by wrong
candidate rules are not reasonable. However, LR-KGE
cannot be applied to a large-scale knowledge graph
In this paper, a
having a large number of relations.
novel approach is proposed to choose proper rules from
the candidate pool.

where N denotes as the total number of entities,
and ne (fa ∧ fb) represents the number of entities with
neighboring formula fa also have fb. As an empirical
statistic over the entire KG, psupport (fa ⇒ fb) is larger
if more entities with neighboring formula fa also have
fb as an association rule. For example, there are 10000
entities in a KB Dataset, including 6000 entities with
neighbor formula f1, 7500 with f2, and 4000 with both.
Then we can obtain the support degree:

4.2.1. Extraction of Rule Sample
Samples of the rule are extracted from given triplets
in this step. The sample rule is referred to as the
triplet combinations that satisfy speciﬁc conditions. For
example, a one-to-one rule sample comprises the triplet
t1 and triplet t2 in the “One-to-One Rule Samples”
table of Figure 2. The rule sample satisﬁes the one-to-
one association rule that the two triplets have the same
head and tail entities. On the other hand, the n-to-
one rule sample comprises the triplet t1, triplet t3, and
triplet t4 in the N-to-One Rule Samples table of Figure
??. The rule sample satisﬁes the n-to-one association
rule that the entities are sequential. Through this
method, the rule samples that belong to the above rule
types can be mined.

4.2.2. Extraction of Rule Candidate
Statistics are derived from the extracted rule candi-
dates. For instance, we extract (r1 and r2) in the
“Candidates of One-to-One Rule” from each one-to-one
association rule sample. Also, (r1, r3 and r4 ) in the
“Candidates of N-to-One Rule” are extracted from each
n-to-one association rule sample. Those candidates for
one-to-one and n-to-one association rules are extracted
in such a way. Then, the candidates are ranked in terms
of the frequencies in descending order.

4.3. Calculation of Promotion Degrees

Here, fa and fb are deﬁned as two constituent formulas,
either atomic or complex, which consist of a single atom
or a set of atoms (triples) with logical connectives.
Following notations in logics, we denote potential
dependency between fa and fb by an “association
rule” (fa ⇒ fb).
To measure the extent of such
dependency, we will deﬁne three metrics, which are
denoted as psupport (fa ⇒ fb) , pconf idence (fa ⇒ fb) and
ppromotion (fa ⇒ fb) of (fa ⇒ fb) respectively. And
p (fa) , p (fb) are proportions of entities with formulae

psupport (f1 ⇒ f2) =

4000
10000

= 0.4.

Deﬁnition 4 (Conﬁdence Degree): It indicates
the proportion of entities with neighboring formula both
fa and fb at the same time to entities that have fa as
a neighboring formula, and can be denoted as follows:

pConf idence (fa ⇒ fb) =

ne (fa ∧ fb)
ne (fa)

.

We continue to take the previous example as

4000
6000

illustration, pConf idence (f1 ⇒ f2) =
= 0.67,
which denotes that 67% of entities with f1 have f2 as a
neighboring formula. As we can see above example, the
conﬁdence degree of f1 to f2 is 0.67, which seems to be
quite high, but in fact, it is misleading. Why do you
say that? Because in the absence of any conditions, the
proportion of f2 is 0.75, while conﬁdence degree of both
f1 to f2 is 0.67. That is to say, if the condition is set, the
proportion of f2 will decrease. This shows that formulae
f1 and f2 is exclusive. Therefore, Promotion Degree is
proposed in this paper to tackle this shortcoming.

Deﬁnition 5 (Promotion Degree): It represents
the ratio of conﬁdence degree pConf idence (fa ⇒ fb) to
p (fb) , and can be shown as follows:

ppromotion (fa ⇒ fb) =

pConf idence (fa ⇒ fb)
p (fb)

The promotion degree reﬂects the correlation between
fa and fb in association rule (fa ⇒ fb). As an empirical
statistic over the entire KG, ppromotion (fa ⇒ fb) is
larger if more entities with fa also have fb as a
neighboring formula. In example above, we regard the
ratio of 0.67/0.75 as the promotion degree of f1 to
f2. The higher the promotion degree is, the higher
the positive correlation is. The lower the promotion
degree is, the higher the negative correlation is. We set
If promotion degree
a threshold value denoted as λ.

The Computer Journal, Vol. ??, No. ??,

????

Association Rules Enhanced Knowledge Graph Attention Network

7

is larger than λ, it can be shown that fa and fb are
association. If smaller than λ, fa and fb is exclusive.

In this paper, we apply the promotion degree of a
logic rule to measure the extent of such dependency. As
shown in Figure x, the input of this framework is triples
of the knowledge graph, and the output is the ground
rules of diﬀerent types with corresponding promotion
degrees. Moreover, only rules with promotion degree
greater than a threshold are used in our association
rules enhanced knowledge graph embedding.

5. AR-KGAT: ASSOCIATION RULES EN-
HANCED KNOWLEDGE GRAPH AT-
TENTION NETWORK

In the attention-based embedding propagation layer,
the ﬁrst-order connectivity information is explicitly
leveraged to relate representations of entity and
relation. Although the performance of GATs was
proved, they cannot be used for KGs because relation
(edge) information and logical rules in the neighborhood
they
are neglected. As an integral part of KGs.
could promote more eﬀective aggregation of
the
transformed representations. Unlike the traditional
neighborhood aggregation where only transformed
node representations are used,
in the proposed AR-
KGAT, node and relation features are jointly embedded
by exploiting various composition operations from
knowledge graph embedding techniques, incorporating
multi-relational information.

Figure ?? shows an overview of our AR-KGAT
framework, where the encoder-decoder structure is
adopted. With entity embeddings learned from the
encoder module as input, the decoder aims to measure
the plausibility of triplets and logical rules. The
parameters of the aggressor are adjusted according to
the feedback provided by the decoder. The proposed
network is trained with the loss function in an end-
to-end learning manner. Accordingly, the entity and
relation embeddings are learned from ground rules and
original triplets.

In the following sub-sections, components of AR-
KGAT are described in detail. Specially, we present
our injection techniques for the association logical rules
into attention aggregator in Section 5.2. Then, the
generalization to high-order propagation is described in
Section 5.3.
In Section 5.4, we introduce our global
loss-functions for joint optimization.

5.1. Model Inputs

AR-KGAT treats KB as a graph with multi-typed nodes
and relations, denoted as KGs. In a given KG denoted
by G = (E, R), E and R are the entities (nodes) and
relations (links), respectively. The KB contains a set of
triplets K = {(ei, rk, ej)} , and each triplet consists of
two entities ei, ej ∈ E and the corresponding relation
rj ∈ R. In the embedding model, representations of

entities, relations, and scoring function f are learned.
For a given input triplet t = (es, r, eo) , f (t) provides the
likelihood of t being a valid triplet. Logical rules L are
given besides these triplets, including One-to-One and
N-to-One association rules introduced in section 4.

The proposed AR-KGAT learns d -dimensional
relation representations hr ∈ Rd(∀r ∈ R) along with
entity representations hv ∈ Rd(∀v ∈ V ), describing
their latent semantics. These two types of embedding
matrics are used as input. The matrix H ∈ RNe×T
represents entity embeddings, where the ith row is the
embedding of the entity. T and Ne are the dimension
of each entity embedding and the total number of
entities, respectively. Similarly, the matrix G ∈ RNr×P
represents the relation embeddings. The outputs of
those matrices are H (cid:48) ∈ RNe×T and G(cid:48) ∈ RNr×P .

5.2.

Incorporating Neighborhood Attention

In order to determine the relative weights of neigh-
bors in the attention model, the following two problems
are considered.
(1) The types of neighborhood rela-
tions that lead to potentially critical neighborhoods.
(2) The important neighbors in transformed embed-
ding according to those relations. Considering these
two requirements, a novel association rules-enhanced
attention-based aggregator is proposed, incorporating
both logic-based and graph-based attention mecha-
nisms.

Let N (i) = {(ei, rk, ej) | (ei, rk, ej) ∈ G} denote the
set of neighboring triplets where ei is the head entity.
The contribution of the neighboring node ej to eO
i
is determined according to its relative importance.
Traditional methods for aggregating neighborhoods
ignored useful
information in the neighbor node ej
and high-order relation rk since only the collections of
transformed embeddings were considered into account.
In order to facilitate more eﬀective aggregation of the
transformed embeddings, relation-representations are
incorporated into the GAT formulation. The encoder
learns the representation of each triplet associated with
ei by a linear transformation over the concatenated
feature vectors of entity and relation, formulated as
follows:
(cid:104)(cid:126)hi

(cid:105) (cid:104)(cid:126)hj

c→
ijk = W1

[(cid:126)gk]

(cid:105)

ijk is the vector form of a triplet tk

ij.(cid:126)hi and (cid:126)hj are
where c→
the corresponding representations of entities ei and ej,
respectively. (cid:126)gk is the representation of relation rk · W1
indicates the linear transformation matrix. This low
complexity transformation matrix consists of matrix
product operations.

5.2.1. Association Rule Weighing Mechanism
With the promotion degree between association
rules (fa ⇒ fb) at hand, the neighboring formula is
characterized that it leads to important neighbors.
For (fa ⇒ fb) , if the logical formula fa has a large

The Computer Journal, Vol. ??, No. ??,

????

8

Jianbin Huang

FIGURE 3: Illustration of the proposed AR-KGAT model.

Ppromotion (fa ⇒ fb) , it is statistically relevant to fb.
In the proposed model, association relations can be
recognized as positive only when a promotion degree
is larger than the threshold value λ.

Under the above intuitions, the logical rule mecha-
nism for measuring the correlation of association rules
is implemented as follow:

αL1

(i,k,j) =

(cid:89)

ro(cid:54)=rk&ro∈NR(i)

logλ Ppromotion (rulen(i, j)) ,

where Ppromotion ( rule n(i, j)) ≥ λ and n = {1, 2, 3}.
Speciﬁcally,

rule1(i, j) = f1(i, o, j) ⇒ f1(i, k, j),

rule2(i, j) = f1(i, o, j) ⇔ f1(j, k, i),

rule3 = f2(i, m, n, j) = r(i, m, k)∧r(k, n, j) ⇒ f1(i, o, j),

where f1(i, k, j) is a one-hop logical formula with a
head entity ei, a tail entity ej, and body relation
rk · f2(i, m, n, j) is a two-hop logical formula with a
head entity ei, a tail entity ej, and body relation
r(i, m, k) ∧ r(k, n, j).
Note that αL1

(i,k,j) promotes one-hop neighboring
formula strongly implying f1(i, k, j) and demotes those
implied by some other irrelevant formula in the same
neighborhood. Speciﬁcally, for any neighboring logical
formula f1(i, o, j) that has the larger promotion degree
it is statistically relevant to f1(i, k, j).
to f1(i, k, j),
The lower the promotion degree is, the higher the
negative correlation is. In this paper, only neighboring
logical formulae with greater promotion degree than
the threshold λ are selected in the knowledge graph
embedding.

5.2.2. Neural Network Mechanism
In our model, which is an extension of classic GAT,
diﬀerent types of relations have diﬀerent weights for
aggregation, and the weights are learned during the
training. The attention weights are guided to be
distributed at a coarse granularity of relations with
global relations statistics. However, such statistics
is not enough to consult ﬁner-grained information
hidden in the transformed neighbor representations to
determine the relative importance of neighborhoods.
To address this issue, we adopt an attention-based
mechanism shown in Figure 4:

FIGURE 4: The attentive embedding propagation layer
of the proposed model.

Similar to GAT, the proposed attention mechanism
is applied to learn the importance of each triplet b(i,j,k)
automatically. The linear transformation with a weight
matrix W2 is conducted, followed by the LeakyRelu
activation function. LeakyRelu is employed to obtain

The Computer Journal, Vol. ??, No. ??,

????

Association Rules Enhanced Knowledge Graph Attention Network

9

the absolute attention value of the triplet:

b(i,j,k) = Leaky Re LU (W2(cid:126)cijk)

The relative attention values αN 1

for a single
triplet are computed via the softmax layer applied over
b(i,j,k), as shown in the following equation.

(i,k,j)

αN 1

(i,k,j) = sof t max
jk

(cid:0)b(i,k,j)

(cid:1) =

(cid:80)

j∈Ni

exp (cid:0)b(i,k,j)
(cid:80)

(cid:1)

ro∈Rij

exp (cid:0)b(i,o,j)

where Ni represents the neighborhood of entity ei.Rij
indicates the set of relations of entities ei and ej.

Through the softmax function, the coeﬃcients are
normalized across all triplets connected with ei. Unlike
the mechanism of the association rule at the relation-
level, the calculation of αN 1
(i,k,j) concentrates more on
the neighbor feature itself.
It also demonstrates that
the features of both neighbor entities and relations are
helpful in training current triplets. Formally, these
two weighting mechanisms are used together in the
computation of the importance of neighbors, and the
new embedding of the entity ei
is the sum of each
triplet representation weighted by both logic-based and
network-based weights as shown below:

(cid:126)ei =

(cid:88)

(cid:16)

αL1
(i,k,j) + αN 1

(i,k,j)

(cid:17)

C 1

(i,k,j)

f1(i,k,j)∈NL1 (i)

(i,k,j) and αN 1

Here αL1
(i,k,j) are logic-based and network-
based attention weights for ej given ei via relation
rk.
This transformation can signiﬁcantly improve
the representation vectors through the accumulated
and encoded features
from local and structured
neighborhoods.

5.3. High-order Propagation

the notion of

In the proposed architecture,
the
link is extended to a directed formula to consider
more neighborhood information into the aggregation
in sparse graphs. An auxiliary high-order relation
introduced for n-hop neighbors between two
is
entities.
in
the logical
formula are summarized to compute
the auxiliary relation. Motivated by the above
aggregator architecture, an eﬃcient model for high-
order propagation is constructed and used to update
forward-pass any entity. The model
is deﬁned as
follows:

Representations of all

the relations

Figure 5 shows the ﬁrst layer of the proposed
model, where all entities extract information from
their direct in-ﬂowing neighborhoods. The aggregation
process learns new representations for entities, and an
auxiliary edge among two-hop neighbors is introduced.
The USA combines the possessed information about
their neighbors in the previous layers from entities
Kobe Bryant, California, Boeing.co, and Donald
In the proposed model, knowledge can be
Trump.

(cid:1)

FIGURE 5: The aggregation process of the graph
attention layer. The dashed lines represent an auxiliary
link from n-hop neighbors (n=2, here).

iteratively accumulated from distant neighbors guided
by a speciﬁc logic formula. The propagation layers
can be further stacked to obtain more higher-order
connectivity, collecting the information about higher-
hop neighborhoods. Note that the inﬂuence of distant
entities is exponentially decreased according to the
increased model depth.

Furthermore, the initially embedded information of
entities gets lose over the learning new embeddings.
To resolve this problem, H t is obtained by a linear
transformation with a weight matrix W E ∈ RT i×T f
from the input entity embeddings H i.H t represents the
transformed entity embeddings. T i and T f are the
dimensions of the initial and ﬁnal entity embeddings,
respectively. The initial entity embedding is added to
the entity embeddings obtained in the ﬁnal attention
layer, as follows:

H (cid:48)(cid:48) = W EH t + H f

Considering the computational complexity, we use
one-hop to three-hop neighborhood information for the
propagation in this paper. The ﬁnal attention values
indicate the relative signiﬁcance of neighboring nodes to
obtain collaborative signals. The attention parameters
in the encoder are learned in the training process using
label data. Unlike the information propagation in GCN
and GAT, our method exploits the association rules of
KB and speciﬁes the diﬀerent signiﬁcance of neighbors.

5.4. Global Objective Function

The decoder measures the plausibility of the trained
triplet and logical rules, given the entity and relation
representations output from the encoder. Triplets and
logical rules are uniﬁed as an atomic and complex
formula. Then, representations for entity and relation
are learned by minimizing the global loss, optimizing
the objective function.

5.4.1. Truth Function
A training set F contains all positive samples, including:
(i) observed triplets; (ii) ground rules. Further, a truth
function I : F → [0, 1] assigns a soft truth value to each

The Computer Journal, Vol. ??, No. ??,

????

10

Jianbin Huang

formula, indicating the degree that ground-rule is met
or triplet holds.

Also, t-norm fuzzy logic is used to model the rules. It
decomposes a complex formula into its constituents via
speciﬁc t-norm based logical connectives. Moreover, we
deﬁne the compositions associated with logical negation
(¬), conjunction (∧), and disjunction (∨):

I (f1 ∧ f2) = I (f1) · I (f2)

I (f1 ∨ f2) = I (f1) + I (f2) − I (f1) · I (f2)

I (¬f1) = 1 − I (f1)

where f1 and f2 are two constituent formula.

Then, the truth values of the constituent triples can
be used to determine the truth values of two types
of association rules using the corresponding logical
connectives:

(1) Given a ground one-to-one association rule:
For the inference rule L1→1( inf er) (cid:44) (em, rs, en) ⇒

(em, rt, en) , the truth value is calculated as:
I (cid:0)L1→1( inf er)

(cid:1) = I (em, rs, en)·I (em, rt, en)−I (em, rs, en)+1

Also,

for the anti-symmetry rule L1→1(anti) (cid:44)
(em, ra, en) ⇔ (en, rb, em), the truth value is calculated
as:
I (cid:0)L1→1(anti)

(cid:1) = I (em, ra, en)·I (en, rb, em)−I (em, ra, en)+1

where the truth value of a constituent triple I(·, ·, ·) is
deﬁned as:

I (ei, rk, ej) = 1 −

1
√

3

d

(cid:107)(cid:126)ei + (cid:126)rk − (cid:126)ej(cid:107)1

(cid:126)ei, (cid:126)ej and (cid:126)rk are vector representations of corresponding
entities ei, ej, and relation rk.

(2) Given a ground two-to-one association rule:

L2→1 (cid:44) (el, rs1, em) ∧ (em, rs2, en) ⇒ (el, rt, en) ,

The truth value is calculated by:

I (L2→1) =I (el, rs1 , em) · I (em, rs2, en) · I (el, rt, en)

− I (el, rs1, em) · I (em, rs2 , en) + 1

A larger truth value indicates that ground rules are
more-satisﬁed. In our proposed AR-KGAT framework,
any rules represented as the ﬁrst-order logic formula can
be handled besides these two types of rules.

5.4.2. Loss Function
The training dataset consists of the original triplets
and two types of ground rules. The dataset is used
to train the proposed model, where the global loss is
minimized to learn the entity and relation embedding
representation. The positive formula is forced to have
larger truth values than negative ones:

min
{e},{r}

(cid:88)

(cid:88)

f +∈F

f −∈Nf +

(cid:2)γ − I (cid:0)f +(cid:1) + I (cid:0)f −(cid:1)(cid:3)

+

f + ∈ F represents positive samples, including triplets
f − ∈ Nf + represents negative
and ground rules.
samples constructed by corrupting f +.γ > 0 is a
separate-parameter of positive and negative samples.
The constraints that ∀e ∈ E, (cid:107)e(cid:107)2 ≤ 1, ∀r ∈ R, (cid:107)r(cid:107)2 ≤ 1
are employed during the training process.

(1) For positive triplet f +, γ > 0 being a triplet
sample (em, rk, en) , the f − is generated by replacing
one of the head and tail entities by a random entity.
The procedure is deﬁned as follows:

Nf + = {(e(cid:48)

m, rk, en | e(cid:48)

m ∈ E) ∪ (em, rk, e(cid:48)

n | e(cid:48)

n ∈ E)}

The triplets from the knowledge graph ∆ are considered
as positive triplets. For each (s, q, o) ∈ ∆, the negative
triplets ∆(cid:48)
(s,q,o) are generated by randomly corrupting
one of the object and subject entities by another entity
in E. For instance, a negative instance (Berlin, Capital-
Of, France) can be generated from the positive triplet
(Berlin, Capital-Of, Germany).

(2) For positive rule f + : ground-rule sample of one-

to-one association rule

For inference rule L1→1( inf er) (cid:44) (em, rs, en) ⇒
(em, rt, en) , one of the head and tail entities is replaced
by a random one to generate the negative rule f −, as
described in the following:

Nf + = {[(e(cid:48)

m, rs, en) ⇒ (e(cid:48)

m, rt, en) | e(cid:48)
n) | e(cid:48)

m ∈ E] ∪ [(em, rs, e(cid:48)
n ∈ E]}

⇒ (em, rt, e(cid:48)

n)

For

anti-symmetry

(cid:44)
rule L1→1(anti)
an
(em, ra, en) ⇔ (en,
rb, em), one of the head and
tail entities is replaced by a random one to generate
the negative rule f −, as described in the following:

Nf + = {[(e(cid:48)

m, ra, en) ⇒ (e(cid:48)

m, rb, en) | e(cid:48)
n) | e(cid:48)

m ∈ E] ∪ [(em, rb, e(cid:48)
n ∈ E]}

⇒ (em, ra, e(cid:48)

n)

(3) For positive rule f + : ground-rule sample of n-to-

one association rule

L2→1 (cid:44) (el, r1, em) + (em, r2, en) ⇒ (el, r3, en)

One of the el and en is replaced by a random entity to
generate the negative rule f − according, as described
in the following:

Nf + = {[(e(cid:48)

l, r1, em) + (em, r2, en) ⇒ (e(cid:48)

[(el, r1, em) + (em, r2, e(cid:48)

n) ⇒ (el, r3, e(cid:48)

l, r3, en) | e(cid:48)
n) | e(cid:48)

l ∈ E] ∪
n ∈ E]}

The generated negative samples, including corrupted
triplets and rules, f − are diﬀerent from the original
triplets and ground rules. The generated negative
rules are used as negative samples during the training
process, leading to lower values for the negative rules.
Note that the embedding representation remains the
same for an entity that appears in the head position
and tail position of a triple in this paper.

The Computer Journal, Vol. ??, No. ??,

????

Association Rules Enhanced Knowledge Graph Attention Network

11

5.4.3. Optimization
The training of the proposed model is conducted as a
two-step procedure. First, the specially designed GAT
is trained to encode the graph entities and relations
information. Then, a decoder model
is trained to
perform speciﬁc tasks, such as relation prediction task.
The Stochastic Gradient Descent (SGD) algorithm is
used with a mini-batch to optimize the minimization
problem.
In the proposed AR-KGAT framework, the
global loss is minimized over the training formula F .
Accordingly, the embedding representations for entity
and relation are learned compatible with both triplets
and rules. The objective function provides lower values
for positive samples than for negative samples. The set
of positive triplets and formula are randomly traversed
multiple times. The gradient is back-propagated to
update the corresponding model parameters after each
mini-batch process.

6. EXPERIMENTAL EVALUATION

The performance of the proposed model AR-KGAT, es-
pecially the embedding propagation layer, is evaluated
on three real-world datasets in typical tasks of knowl-
edge graph completion: the link prediction and triplet
classiﬁcation.
In this section, ﬁrst, the performance
of the proposed AR-KGAT is compared with state-of-
the-art knowledge graph embedding methods. Second,
the contribution analysis of each component of the pro-
posed AR-KGAT, including association rules, attention
mechanism, and aggregator selection, are also analyzed.
Lastly, the eﬀects of the hyper-parameters of the pro-
posed AR-KGAT are analyzed.

6.1. Benchmark Datasets

In the experiments, the performance of the proposed
model is evaluated on two widely used public datasets,
WN18RR [34] and FB15k-237 [35], for link prediction
and triplet classiﬁcation tasks.

FB15K [36]

FB15k-237.

includes the subset
of knowledge base triplets, originally derived from
Freebase. The dataset consists of 310,116 triplets with
14,541 entities, 237 relations. As shown in Table 1, the
dataset is randomly split. The FB15k-237 is consists of
textual mentions of Freebase entity pairs and knowledge
base relation triplets [35].

WN18RR. WN18RR is generated from WN18RR
[36]. As a subset of WordNet, WN18RR includes
40,943 entities and 18 relations. The text triplets
generated by inverting triplets in WN18RR are removed
to construct the WN18RR dataset to ensure no inverse
relation test leakage exists in the evaluation dataset. In
summary, 93,003 triplets composed of 40,943 entities
and 11 relation types are used in the WN18RR dataset.
Table 1 summarizes the detailed information on
WN18RR and FB15k-237. The triplets samples of
each dataset are divided into training, validation, and

test sub-datasets. They are utilized in the model
training, hyper-parameter tuning for model selection,
and evaluating the model. WN18RR is split following
the original data split, while only triplets associated
with the 237 relations are employed from each training,
validation, and test datasets of FB15K.

the

First,

two types of association rules are
constructed for each dataset, in the form of Rule 1 (one-
to-one inference rule ∀x, y : (x, rs, y) ⇒ (x, rt, y)), Rule
2 (one-to-one anti-symmetric rule ∀x, y : (x, rs, y) ⇔
(y, rt, x)), and Rule 3 ( n-to-one transitivity rule
∀x, y, z : (x, rs1, y) ∧ (x, rs2, z) ⇒ (x, rt, z)) following
the schemes described in Section 4. The entity and
relation embeddings are obtained by using the proposed
aggregator. Then, the truth values for each rule are
computed.

Table 3 summarizes the details of the mined rules.
For the FB15K-166 and FB15K-237 datasets,
the
threshold τ is set 0.5 for rule 1, rule 2 and rule 3. For
the WN18RR dataset, the threshold τ is also set as
0.5 for rule 2. The threshold values were determined
using the validation datasets. The method illustrated
in Section 3 instantiates the mined rules with concrete
entities (grounding) after the association rules mining.

Table 4 and Table 2 present the candidates of the
ground rules and some examples of the rules in both
datasets, respectively.

6.2. Training Details

Two sets of negative samples of triplets are generated,
where the head entity is randomly replaced in the
ﬁrst set, while the tail entity is randomly replaced in
the second set. Equal numbers of negative triplets
from two sets are generated to achieve more robust
performance in detecting both head and tail. The
embeddings for entity and relation are initialized
with TransE. The default Xavier initializer is applied
The hyper-
to initialize the model parameters.
learning
parameter ranges are speciﬁed as follows:
rate {0.01,0.005,0.003,0.001}, dropout-rate {0, 0.1, 0.2,
0.3, 0.4, 0.5}, embedding dimension 50, 100, 150,
200, 250, and number of neighbors {3, 4, 5, 6, 7, 8,
9, 10, 11}. We use the adaptive moment (Adam)
algorithm to optimize the model weights. A grid
search is used to determine the hyper-parameters in our
models during the training. Also, an early training-
i.e., the training
termination strategy is employed,
process is prematurely terminated when Hit@k on the
validation set is not increased during 100 successive
epochs. Diﬀerent hyper-parameters are evaluated on
the validation dataset to select the optimal model. The
optimal hyper-parameters are described in Section 6.7.
For each dataset, the following settings are used in
this experiment. For the FB15k-237, the dropout rate
is 0.2, the maximum number of neighbor relations is
8, the learning rate is 0.003, and the embedding size

The Computer Journal, Vol. ??, No. ??,

????

12

Jianbin Huang

TABLE 1: Statistics of the datasets.

Dataset Entities Relations Training Validation Test Total
20466 310116
93003
3134

FB15k-237
14541
WN18RR 40943

272115
86835

17535
3034

237
11

Dataset

Rule examples
∀x, y : / sports / athlete / team (x, y) ⇒ /sports / sports team / player (y, x)

TABLE 2: Examples of rules created

FB15K-237

WN18RR

∀x, y, z : / people / person / nationality (x, y)∧ /location / country / oﬃcal

language (y, z)

⇒ / people / person / language (x, z)
∀x, y, z : country / ad min istrative divisions (x, y)∧ /ad min istrative division / capital (y, z)
⇒ / people / person / language (x, z)
∀x, y : llocation / country / capital (x, y) ⇒ location / location / contains (x, y)
∀x, y : /location / country / ﬁrst level − divisions (x, y) ⇒ location / location / contains (x, y)
∀x, y : people / family / members (x, y) ⇒ / people / person / place − of − birth (y, x)
∀x, y :− member − of − domain −usage(x, y) ⇔− synset domain usage of (y, x)
∀x, y :− hyponym (x, y) ⇔− hupernym (y, x)
∀x, y :− part − of (x, y) ⇔− has − part(y, x)
∀x, y :−ins tan ce hyponym(x, y) ⇔−ins tan ce hupernym(y, x)

TABLE 3: The rule statistics.

Dataset #Rule 1 #Rule 2 #Rule 3 Total
3334
11

FB15K-237
WN18RR

2357
11

160
0

637
0

TABLE 4: The candidates of ground rule.

Dataset #Rule 1 #Rule 2 #Rule 3 Total
274971
9352

FB15K-237
WN18RR

113476
0

152987
9352

8508
0

is 100; for the WN18RR dataset, the dropout rate is
0.2, the maximum number of neighbor relations is 6,
the learning rate is 0.003, and the embedding size is
150. Moreover, each dataset is split into three sets for
training, validation, and testing, which is the same as
the setting of the original models. The proposed model
was implemented in PyTorch, and the experiments were
conducted on two NVIDIA GTX 2080Ti GPU. The
computational times for one epoch are 141 seconds and
15 seconds for the FB15k-237 and WN18RR datasets,
respectively.

6.3. Baselines

The proposed model is evaluated with comparisons to
the state-of-the-art methods categorized as follows:

• Translation-based methods: Relatively simple
For example, TransE[9],

vector-based methods.
TransR[10], DistMult[11] and ComplEx[17].

• Deep Neural network-based methods: A se-
ries of deep non-linear neural network-based meth-
ods,
including convolution based models (including
ConvE[12], ConvKB[13] and ConvR[28]), and graph
neural network based models (including R-GCN[23],
CompGCN[37] and SACN[38]).

• Logical rules powered methods:

Several

methods which jointly embed knowledge graphs and
logical rules, including LR-KGE[33] and KALE[39].

• Variant of our model: AR-KGCN is referred
to as a variant of our proposed AR-KGAT, which
contains GCN based aggregator without consideration
of attention mechanism.

Among these compared methods, except for LR-
KGE, KALE, AR-KGCN, and AR-KGAT, other
baselines use only triplets.
In the experiments,
the eﬀectiveness of the proposed model is evaluated
with state-of-the-art methods on two typical tasks for
knowledge graph completion:
link prediction (Section
6.4) and triplet classiﬁcation (Section 6.5).

6.4. Link Prediction

i.e.

The task of link prediction is formulated to predict
missing facts using the given facts in Knowledge
Graphs. A directed labeled graph G = (V, E, R) is used
to represent the knowledge base. However, in the link
prediction task, an incomplete subset ˆE is given rather
than the full set of edges E. Thus, the link prediction
is stated as completing a triplet (ei, rk, ej) when ei or
ej missing,
the prediction of ej given (ei, rk, ?)
or the prediction of ei given (?, rk, ej) . The task is to
assign scores f (ei, rk, ej) to possible edges (ei, rk, ej) to
determine the degree to which those edges belong to E.
is replaced by
every other entity e(cid:48)
i ∈ E\ei to construct a set of (N −1)
corrupt triplets for each test triple (ei, rk, ej) . Then, a
score is assigned to each corrupted triple (e(cid:48)
i, rk, ej) .
The correct entity ei is ranked in descending order of
scores (or in ascending order of the distances). The
rank for the corrupted entity of the tail entity ej is
obtained in a similar way. All the models are evaluated
in a ﬁltered setting, where the corrupted triplets already
presented in other datasets are removed during the
ranking process.

Implementation: Each entity ei

The Computer Journal, Vol. ??, No. ??,

????

Association Rules Enhanced Knowledge Graph Attention Network

13

TABLE 5: Results of link prediction on the testing dataset of FB15k-237 and WN18RR.

FB15K-237

WN18RR

Methods

TransE
ComplEX
DisMult
ConvE
ConvKB
ConvR
SACN

MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10
0.224
0.247
0.240
0.317
0.396
0.335
0.354
CompGCN 0.355
0.419
0.282
0.338
AR-KGCN 0.360
0.442
AR-KGAT

0.441
0.463
0.440
0.440
0.441
0.494
0.486
0.494
0.497
0.348
0.522
0.516
0.540

0.532
0.510
0.496
0.525
0.558
0.529
0.548
0.546
0.567
0.489
0.528
0.571
0.626

0.138
0.158
0.163
0.237
0.198
0.242
0.269
0.264
0.311
0.271
0.241
0.287
0.361

0.427
0.410
0.391
0.400
0.058
0.432
0.435
0.443
0.438
0.222
0.430
0.428
0.465

0.395
0.428
0.422
0.493
0.517
0.511
0.547
0.535
0.558
0.464
0.533
0.555
0.581

0.246
0.287
0.267
0.352
0.324
0.401
0.393
0.397
0.434
0.289
0.346
0.404
0.483

0.236
0.392
0.403
0.458
0.465
0.466
0.470
0.479
0.483
0.376
0.463
0.476
0.518

R-GCN
KALE
LR-KGE

Evaluation protocol: We employ the widely used
two metrics for evaluation: mean reciprocal rank
(MRR) and Hits@k (the proportion of ground truth
entities ranked top- k ∈ {1, 3, 10} ). Higher values
indicate better performance for both MRR and Hits@k.
Also, the ﬁltered setting (Bordes et al., 2013) is used
because the knowledge graphs contain some corrupted
triplets. In other words, all valid triplets are ﬁltered out
before ranking.

The results of

link prediction on FB15K and
WN18RR datasets are shown in Table 5. The results
clearly indicate that our proposed method outperforms
state-of-the-art methods in terms of
four metrics,
especially for the Hit@k metrics. Our logical rule-
enhanced model greatly improves the performance
over the triplets-only-based methods. On average,
our AR-KGAT obtains an improvement of at least
9% and 7.5% on FB15k-237 and WN18RR in terms
of MRR over other baselines. Also, the Hits@1 is
increased by 0.282 with our AR-KGAT model on the
WN18RR dataset, showing the promising potential of
the proposed method. Note that, among diﬀerent
metrics, the ﬁrst rank metric is the most essential in
practical knowledge inference applications. Low values
of Hits@1 usually refer that the corrected triplet is not
likely to be ranked in the ﬁrst place. From this point
of view, the proposed AR-KGAT model can provide
knowledge graph embedding for practical applications
of knowledge inference.

Furthermore, the proposed AR-KGAT model

im-
proves the performance for the FB15k-237 test dataset,
Hits@10 by a margin of 4.1%, and Hits@3 by a margin
of 5.7%. Also, the performance of the proposed model
is improved by 8.3% and 9.3%, respectively, in terms
of Hits@10 and Hits@3 on the WN18RR test dataset.
The logical rule-enhanced models, including LR-KGE
and AR-KGAT, mostly outperform the original models,
where triplets are used alone on both datasets. This im-
plies that integrating logical rules can indeed infer new
and corrected triplets. Speciﬁcally, compared to GNN-

based methods, our AR-KGAT model outperforms on
both benchmark datasets in terms of all the evaluation
metrics.
It validates the superiority of our model to
other aggregators and the necessities to integrate the as-
sociation rule-based weighting mechanism into the over-
all framework. Moreover, AR-KGCN consistently un-
derperforms AR-KGAT, which illustrates that remov-
ing attention components degrades the performance of
the model. On the whole, our rule-enhanced GAT
framework outperforms other compared methods con-
It shows that the entities and relations are
sistently.
eﬃciently represented in a low-dimensional space, lever-
aging the logical rules and attention mechanism.

6.5. Triple Classiﬁcation

The discriminative ability to distinguish between true
and false facts is evaluated in a classical triplet
classiﬁcation task in knowledge graph embedding.
Triplet classiﬁcation is formulated to determine if
a given triplet (h, r, t) is correct or wrong.
The
validation and testing datasets include both positive
and negative triplets in the benchmark datasets, while
the training set contains only positive triplets. We
use the thresholds δr for each relation to address this
task. It is optimized by maximizing the classiﬁcation
accuracy in all triplets with the corresponding relation
on the validation set. A triplet (h, r, t) is classiﬁed
as positive if the dissimilarity score φ(h, r, t) ≥ δr;
otherwise, it is classiﬁed as negative.

Implementation: The same datasets used in the
link prediction task are used in triplet classiﬁcation.
Accordingly, the ranges of hyper-parameters are also
the same as the link prediction. Thus, the optimal
hyper-parameters are also used without any change for
the triplet classiﬁcation. The estimated truth values
(or distances) are used to classify the triplets. In this
way, a triplet having a large truth value (equivalently
small distance) is likely to be determined as positive.
The triplets association with each speciﬁc relation are

The Computer Journal, Vol. ??, No. ??,

????

14

Jianbin Huang

TABLE 6:
triplet
classiﬁcation for the FB15k-237 and WN18RR test sets.

The accuracy comparison of

FB15K-237 WN18RR

Dataset
TransE
TransR
ConvE
ConvKB
ConvR
CompGCN
R-GCN
SACN
KALE
LR-KGE
AR-KGCN
AR-KGAT

0.8258
0.8196
0.8986
0.8890
0.9094
0.8991
0.9068
0.9011
0.8856
0.8993
0.9104
0.9254

0.9470
0.9513
0.9686
0.9601
0.9780
0.9523
0.9726
0.9679
0.9573
0.9646
0.9527
0.9733

ranked in descending order of the truth values (in
ascending order of the distances) in the evaluation.
The precisions are averaged for each relation. Then,
the mean average precision (MAP) is computed over
diﬀerent relations in test sets.

results of

The analytical

Results: Table 6 compares the accuracy of the
triplet classiﬁcation for the FB15k-237 and WN18RR
datasets.
the triplet
classiﬁcation show consistent observation as the results
obtained in the link prediction task. The CNN and
including CompGCN, R-GCN,
GNN based models,
and ConvKB, slightly outperform the original TransE
and TransKB models on both datasets in most cases.
For these results, we conclude that the translational
characteristic between relation and entity can be
eﬀectively kept in CNN or GNN based models using
the neural network, achieving promising performance
in the triplet classiﬁcation. Also, adopting the logical
rule into the models shows signiﬁcant improvements
over the na¨ıve methods that utilize triplets only. This
shows that the knowledge representation, such as entity
and relation embedding, can be learned better in the
translation-based knowledge graph embedding models
by the proposed rule-enhanced approach. Further, more
rule-related information is exploited in the proposed
rule-powered model compared to adding an inferred
triplet into training data. The best average accuracy
is achieved by the proposed AR-KGAT, conﬁrming
the advantages of the proposed model over compared
methods in the triplet classiﬁcation. This observation
jointly validates with the observation in Section 6.4
that more predictive embeddings and extended ability
beyond the capability of pure logical inference can be
learned in a joint embedding scenario. The importance
of the attention mechanism is proved by the consistently
superior performance of the AR-KGAT over the AR-
KGCN, showing it is one of the key components of
the proposed model. Equal weighting for all neighbors
regardless of their importance might lead to worse
embedding propagation process.

In the following subsections, the proposed AR-KGAT

is further analyzed in terms of generalization ability to
other conﬁgurations. Also, whether it outperforms the
others for expected reasons is analyzed.

6.6. Study of AR-KGAT Model

To get deep insights into the AR-KGAT, we study
the inﬂuences of logical rules-based mechanisms and
diﬀerent types of
logical rules on the performance.
Then, the convergence speed of our proposed AR-
KGAT is analyzed.

6.6.1. The analysis of
mechanism

the association rule-based

In this experiment, the contribution of the logical
rules is analyzed on the improvement of the graph
attention network in our model. To this end, we
conduct further experiments that a single model of
the only logical rule mechanism (so-called Logical rules
Speciﬁcally, the proposed AR-
Only) is evaluated.
KGAT is compared with three degenerated methods,
including AR-KGAT-OptOnly, AR-KGAT-AggOnly,
and AR-KGAT-TriOnly.
In AR-KGAT-OptOnly and
AR-KGAT-AggOnly, the logical rule mechanism is
removed from the aggregator and objective function,
respectively. As a basic graph attention network
framework, AR-KGAT-TriOnly uses only triplets in
the optimization function and aggregator to conduct
the embedding task. Moreover, AR-KGAT further
incorporates both training triplets and ground rules
before and during embedding in a joint learning
framework.

Tables 8 and 9 show the experimental results on
the test sets. The results indicate that both AR-
KGAT-OptOnly and AR-KGAT-AggOnly outperform
AR-KGAT-TriOnly by signiﬁcant margins,
implying
the superiority of incorporating association logical rules.
Compared to other variants, AR-KGAT-TriOnly shows
the relatively poor result (both in WN18RR and FB15-
237), denoting that the association rule mechanism
In
is the critical element of the proposed model.
AR-KGAT-TriOnly, the representation relatedness is
not explicitly modeled on the granularity of logical
rules. A shallow model could be more suitable for the
WN18RR dataset since it has only one type of rule.
Thus, the performance of the proposed model can be
lower for this dataset. Moreover, AR-KGAT-AggOnly
always outperforms AR-KGAT-OptOnly, showing the
capability of the graph attention network scenario to
learn more predictive embeddings.
In speciﬁc, the
incorporation of the logical rules into the training of
the aggregator is crucial to generate more accurate
representations for entities, instead of only relying on
a neural network to learn from the data. The proposed
AR-KGAT combines the graph attention networks
and the logical rules, thus providing outperforming
performance over all the compared models. As shown
in Table 7, the proposed AR-KGAT provides the

The Computer Journal, Vol. ??, No. ??,

????

Association Rules Enhanced Knowledge Graph Attention Network

15

TABLE 7: Link prediction results by relation category for LR-KGE and AR-KGAT on FB15k-237. The complex
relations are more eﬀectively captured in the proposed AR-KGAT model than the rule-enhanced LR-KGE model.

Method

LR-KGE

AR-KGAT

LR-KGE

AR-KGAT

Rule Category

none
one-to-one
(inference rule)
one-to-one
(anti-symmetry rule)
n-to-one (n=2)
overall

Predicting head entities

Predicting tail entities

MRR Hits@10 MRR Hits@10 MRR Hits@10 MRR Hits@10
0.147

0.237

0.270

0.189

0.202

0.155

0.211

0.284

0.425

0.515

0.491

0.570

0.258

0.484

0.314

0.428

0.172

0.242
0.388

0.215

0.474
0.554

0.228

0.343
0.501

0.258

0.466
0.562

0.158

0.398
0.313

0.282

0.494
0.505

0.217

0.483
0.437

0.298

0.597
0.483

TABLE 8: Eﬀect of association rules on FB15k-237 and WN18RR for link prediction.

Model

AR-KGAT-TriOnly
AR-KGAT-OptOnly
AR-KGAT-AggOnly
AR-KGAT

FB15K-237

WN18RR

MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10
0.331
0.355
0.394
0.442

0.497
0.519
0.566
0.581

0.425
0.456
0.484
0.518

0.244
0.273
0.302
0.361

0.448
0.446
0.487
0.540

0.535
0.517
0.582
0.626

0.377
0.395
0.446
0.483

0.364
0.392
0.427
0.465

TABLE 9: Eﬀect of association rules on FB15k-237 and
WN18RR for triple classiﬁcation.

Model

AR-KGAT-TriOnly
AR-KGAT-OptOnly
AR-KGAT-AggOnly
AR-KGAT

FB15K-237 WN18RR

MAP
0.9091
0.9011
0.9192
0.9254

MAP
0.9523
0.9779
0.9933
0.9926

performance improvement of the MRR by 23.3%, and
the HITS@10 by 28.2% on the FB15k-237 dataset.
Also, the proposed AR-KGAT shows a relatively non-
obvious improvement in the WN18RR dataset: MRR
and HITS@10 increase by 3.1% and 0.3%, respectively.
The results show that more predictive embeddings are
learned through the joint embedding of triplets and
rules, especially for the dataset that contains multiple
types of logical rules.

6.6.2. Evaluation on Diﬀerent Type of Association

Rules

In this section, the performance of the proposed model
is analyzed for diﬀerent types of rules. In this analysis,
the FB15k-237 dataset is employed due to its diversity
of logical rules set. As described in Section 4, the
logical rules are classiﬁed into three types: one-to-one
inference rules, one-to-one anti-symmetry rules, and
n-to-one transitivity rules (n=2). We chose a logical
rule powered knowledge graph embedding method, LR-
KGE, for comparison.

The results shown in Table 7 can be used to evaluate
the performance and describe the behavior of diﬀerent
rules. As deﬁned in Section 4, we divide rules into three
types, for which the promotion percentage in FB15K-

237 is 34.2%, 7.9%, and 18.0% respectively over the
variant with only triplets considered, demonstrating the
superiority of incorporating logical rules. Moreover,
it is shown that the proposed model
is eﬀective
due to the modeling of multiple association rules.
This also demonstrates that integrating rules into
GNN allows the model to capture more complex
knowledge information. However, the performance
improvement
is not always guaranteed, and the
performance ﬂuctuates since some association rules
may contain noisy or conﬂict information with existing
ones. From the results shown in Table 7, we can
observe that the one-to-one inference rule can get
the better performance than other individual rules
in predicting head entities task because this type of
association rule contains more important correlations
which promote the prediction performance. Similar
to the previous results,
in predicting tail entities,
experimental results demonstrate that the two-to-
one transitivity rule can get superior performance
over other individual rules. By integrating all three
types of logical rules, AR-KGAT achieves signiﬁcant
improvement across 4 out of 5 metrics compared to
other variants.
It also indicates that more ability to
capture complicated correlations can be obtained by
leveraging more information on multi-type logical rules,
which is essential to learn more precise representations
for entities and relations on knowledge graphs. The
results also show that the performance can be improved
by facilitating the alignment of diﬀerent views in the
proposed collaborative framework.

6.6.3. The analysis of the convergence
As shown in Figure 6, in FB15K-237, our proposed AR-
KGAT (the red line) provides better performance over

The Computer Journal, Vol. ??, No. ??,

????

16

Jianbin Huang

(a) MRR in FB-237

(b) MRR in WN18RR

FIGURE 6: The convergence study of the three
models in FB15K-237 and WN18RR datasets using the
validation set. Due to the page limitation, only the
results of MRR are reported here.

ConvKB (the green line) and LR-KGE (the yellow line)
after several epochs. Speciﬁcally, it can be seen that
our proposed AR-KGAT shows continuously increasing
performance until around 800 epochs.
In contrast,
the performances of the LR-KGE and ConvKB have
converged around 1200 and 1,250 epochs in FB15K-237
respectively. However, in WN18RR, which has only one
single type of rules, ConvKB achieves the fast coverage
speed. The performances of ConvKB have converged
about 250 epochs in WN18RR, and our model until
nearly 500 epochs. Compared with relatively simple
CNN based model, in our AR-KGAT model, not only a
lot of signiﬁcant parameters of graph attention network,
but also a number of hyper-parameters of diﬀerent
types of association rules are required to be accurately
reﬁned according to lager epochs. Moreover, though
diﬀerent trends were observed with the FB15k-237 and
WN18RR datasets, the metrics of our proposed AR-
KGAT outperforms the other two models ﬁnally. The
gap between these three models also shows that graph
attention network and logical rules mechanism are two
signiﬁcant components in the proposed model.

6.7. The Analysis of Parameters Sensitivity

the AR-KGAT framework to
sensitivity of
The
parameters was analyzed,
involving (1) number of
neighbor relations, (2) model depth, (3) dimension
of the learned node vectors, and (4) the proportion
In the following experiments, the
of unseen entities.
diﬀerent parameters are analyzed in terms of embedding
performance.

6.7.1. Number of Neighbor relations
In this subsection, the eﬀect of the number of neighbor
relations to the performance is evaluated on three
datasets. The node with a larger number means that it
can receive more information from neighboring relations
than other nodes with a smaller number. Speciﬁcally,
the relation prediction task is analyzed with the varied
number of neighbors from 3 to 10.

As shown in Figure 7, diﬀerent sets of nodes analyzed
with a diﬀerent number of neighbor relations. The

FIGURE 7: Number of neighbor relations study using
FB15k-237 and WN18RR datasets.

average MRR, Hits@10, and Hits@3 scores are used
in the analysis. For the diﬀerent number of neighbor
relations, the red line represents the performance for
WN18RR, and the black line indicates the values for
FB15k-237. The performance of the proposed model
is increased according to the increased number of
neighbors in terms of all the metrics, MRR, Hits@10,
and Hits@3, showing that the information of neighbors
can eﬀectively enhance the representations of nodes.
It is also shown that the personalized features of the
entities can be modeled more eﬀectively by using more
neighbor information. The performance improvement,
according to the increasing neighbors, tends to be
steady for WN18RR. However, such an increment of
the number of neighbors leads to the increment of
the computational complexity, i.e., they are the trade-
oﬀ. Therefore, to prevent impractical computation
complexity due to the limited computational resources,
the maximum numbers of neighbors are set as 8 for the
FB15k-237 dataset and 6 for the WN18RR dataset.

6.7.2. Eﬀect of Model Depth
We vary the depth of KGAT (e.g., L) to investigate the
eﬃciency of usage of multiple embedding propagation
layers.
In particular, the layer number is searched in
the range of {1, 2, 3}. We use KGAT-1 to indicate the
model with one layer and similar notations for others.
We summarize the results in Table 10 and have the
following observations:

Increasing the depth of KGAT is capable of
boosting the performance substantially.
Clearly,
KGAT-2 achieves consistent improvement over KGAT-
1 across all the board.
The results demonstrate
that appropriate depth is essential for training graph
attention networks to characterize rich high-order
semantic relations between entities. We attribute
the improvements to the eﬀective modeling of high-
order relation between entities, carried by the ﬁrst-
and second-order connectivities, respectively. Further

The Computer Journal, Vol. ??, No. ??,

????

Association Rules Enhanced Knowledge Graph Attention Network

17

TABLE 10: Eﬀect of association rules on FB15k-237 and WN18RR for link prediction.

Model

AR-KGAT-1
AR-KGAT-2
AR-KGAT-3

FB15K-237

WN18RR

MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10
0.370
0.442
0.448

0.492
0.550
0.568

0.579
0.626
0.630

0.303
0.361
0.361

0.467
0.518
0.527

0.412
0.483
0.497

0.419
0.465
0.493

0.517
0.581
0.602

stacking one more layer over KGAT-2, we observe
that KGAT-3 only achieve marginal
improvements.
It suggests that taking into account ﬁrst-order and
second-order relations among entities simultaneously
could be suﬃcient to the demands of most downstream
applications.

6.7.3. Eﬀect of Embedding Dimension
Relation prediction performance was analyzed when
entity embedding dimension (denoted by d and learned
by AR-KGAT) varies on both datasets of FB15k-237
and WN18RR. The overall results are summarized in
Figure 8.

the unseen entities over

Inﬂuence of the proportion of unseen entities

6.7.4.
When the ratio of
the
training entities increases (i.e., the observed knowledge
graph becomes sparser), the performance of the model
generally deteriorated.
The eﬀect of the sparse
knowledge graph in the proposed model is analyzed
in the link prediction task for diﬀerent sample rates of
datasets.

FIGURE 9: Eﬀect of the proportion of unseen entities
on FB15k-237 dataset.

The results are displayed in Figure 9. We observe
that
the increasing proportion of unseen entities
certainly has a negative impact on all models. However,
the performance of our model does not decrease as
drastically as that of R-GCN, TransE and ConvKB
(three SOTA baselines), indicating that AR-KGAT is
more robust on sparse KGs.

7. CONCLUSION AND FUTURE WORK

In our proposed aggregator,

This paper proposes a joint embedding framework, AR-
KGAT, for knowledge graphs and logical rules. The
key idea is to integrate triplets and association rules
in the knowledge graph attention network framework
to generate eﬀective representations.
Speciﬁcally,
the graph attention mechanisms are generalized and
extended so that both entity and relation features
are captured in a multi-hop neighborhood of a given
entity.
the weights
of a coarse relation level and a ﬁne neighbor level
are estimated by logical rules and neural attention
networks, respectively. Moreover, the embedding task
is then conducted by minimizing the loss function
In this way,
on both complex and atomic formulas.
the learned embeddings are certainly more useful
for knowledge acquisition and inference, which are
compatible with triplets and rules. The proposed AR-
KGAT is evaluated for two datasets. Each dataset
is used for both the triplet classiﬁcation and link

FIGURE 8: Eﬀect of the varying embedding dimension
(d).

We observe that on increasing the dimension of
embeddings,
the value of MRR and Hit@k grows
ﬁrst. Such a phenomenon occurs because this model
requires a proper dimension to preserve useful semantic
information. However, it then remains unchanged and
degrades as the dimension of entity representation is
increased beyond a certain limit. We hypothesize that
this is due to the over parameterization of the model.
Moreover, if the dimension is excessively large, noisy
information may be added, which consequently leads to
worse performances and brings extra storage burden.
Based on the experimental ﬁndings above, our proposed
AR-KGAT needs a proper dimension to encode rich
semantic information, and too large a dimension may
introduce additional redundancies. Hence, we set the
dimension of embeddings as 150 on WN18RR, and 100
on FB15k-237.

The Computer Journal, Vol. ??, No. ??,

????

18

Jianbin Huang

prediction tasks. Experimental results indicate that
signiﬁcant and consistent performance improvement is
achieved through the joint embedding over state-of-
the-art models on two typical KG completion tasks.
The detailed and exhaustive empirical analysis gives
insight into the superiority of the proposed method for
relation prediction on KGs. Future works will include
extending the proposed method to hierarchical graphs,
capturing higher-order relations between entities in our
graph attention model. Further, more types of eﬀective
rules that exist in data will be investigated. The
embedding of multiple-source knowledge graphs will
also be explored, e.g., jointly embedding Freebase and
YAGO.

ACKNOWLEDGMENTS

The work was supported by the National Natural
Science Foundation of China [grant numbers: 61876138,
61602354]. Any opinions, ﬁndings and conclusions
expressed here are those of the authors and do not
necessarily reﬂect the views of the funding agencies.

REFERENCES

[1] K. Bollacker, C. Evans, P. Paritosh, T. Sturge,
and J. Taylor, “Freebase: a collaboratively created
graph database for structuring human knowledge,” in
Proceedings of the 2008 ACM SIGMOD international
conference on Management of data, 2008, pp. 1247–
1250.

[2] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann,
R. Cyganiak, and Z. Ives, “Dbpedia: A nucleus for a
Springer,
web of open data,” in The semantic web.
2007, pp. 722–735.

[3] Y. Hao, Y. Zhang, K. Liu, S. He, Z. Liu, H. Wu,
and J. Zhao, “An end-to-end model
for question
answering over knowledge base with cross-attention
combining global knowledge,” in Proceedings of the 55th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 2017, pp. 221–
231.

[4] Y. Zhang, K. Liu, S. He, G. Ji, Z. Liu, H. Wu,
and J. Zhao, “Question answering over knowledge
base with neural attention combining global knowledge
information,” arXiv preprint arXiv:1606.00979, 2016.

[5] W. Xiong, T. Hoang, and W. Y. Wang, “Deeppath:
A reinforcement learning method for knowledge graph
reasoning,” arXiv preprint arXiv:1707.06690, 2017.
[6] F. M. Suchanek, G. Kasneci, and G. Weikum, “Yago: a
core of semantic knowledge,” in Proceedings of the 16th
international conference on World Wide Web, 2007, pp.
697–706.

[7] X. V. Lin, R. Socher, and C. Xiong, “Multi-hop
knowledge graph reasoning with reward shaping,”
arXiv preprint arXiv:1808.10568, 2018.

[8] R. West, E. Gabrilovich, K. Murphy, S. Sun, R. Gupta,
and D. Lin, “Knowledge base completion via search-
based question answering,” in Proceedings of the 23rd
international conference on World wide web, 2014, pp.
515–526.

[9] Z. Wang, J. Zhang, J. Feng, and Z. Chen, “Knowledge
graph embedding by translating on hyperplanes.” in
Aaai, vol. 14, no. 2014. Citeseer, 2014, pp. 1112–1119.
[10] Y. Lin, Z. Liu, H. Luan, M. Sun, S. Rao, and S. Liu,
“Modeling relation paths for representation learning
of knowledge bases,” arXiv preprint arXiv:1506.00379,
2015.

[11] B. Yang, W.-t. Yih, X. He, J. Gao, and L. Deng, “Em-
bedding entities and relations for learning and inference
in knowledge bases,” arXiv preprint arXiv:1412.6575,
2014.

[12] T. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel,
“Convolutional 2d knowledge graph embeddings,”
arXiv preprint arXiv:1707.01476, 2017.

[13] D. Q. Nguyen, T. D. Nguyen, D. Q. Nguyen,
and D. Phung, “A novel embedding model
for
knowledge base completion based on convolutional
neural network,” arXiv preprint arXiv:1712.02121,
2017.

[14] C. Shi, Y. Li, J. Zhang, Y. Sun, and S. Y.
Philip, “A survey of heterogeneous information network
analysis,” IEEE Transactions on Knowledge and Data
Engineering, vol. 29, no. 1, pp. 17–37, 2016.

[15] G. Ji, S. He, L. Xu, K. Liu,

and J. Zhao,
“Knowledge graph embedding via dynamic mapping
matrix,” in Proceedings of the 53rd annual meeting of
the association for computational
linguistics and the
7th international joint conference on natural language
processing (volume 1: Long papers), 2015, pp. 687–696.
[16] T. Rockt¨aschel, S. Singh, and S. Riedel, “Injecting
logical background knowledge into embeddings for
the 2015
relation extraction,”
Conference of
the
Association for Computational Linguistics: Human
Language Technologies, 2015, pp. 1119–1129.

in Proceedings of
the North American Chapter of

[17] T. Trouillon, J. Welbl, S. Riedel, ´E. Gaussier, and
G. Bouchard, “Complex embeddings for simple link
prediction.”
International Conference on Machine
Learning (ICML), 2016.

[18] Y. Luo, Q. Wang, B. Wang, and L. Guo, “Context-
dependent knowledge graph embedding,” in Proceed-
ings of the 2015 Conference on Empirical Methods in
Natural Language Processing, 2015, pp. 1656–1661.
[19] K. Toutanova, D. Chen, P. Pantel, H. Poon,
P. Choudhury, and M. Gamon, “Representing text
for joint embedding of text and knowledge bases,”
in Proceedings of the 2015 conference on empirical
methods in natural language processing, 2015, pp. 1499–
1509.

[20] D. Q. Nguyen, T. D. Nguyen, D. Q. Nguyen,
and D. Phung, “A novel embedding model
for
knowledge base completion based on convolutional
neural network,” arXiv preprint arXiv:1712.02121,
2017.

[21] T. Hamaguchi, H. Oiwa, M. Shimbo, and Y. Mat-
sumoto, “Knowledge transfer for out-of-knowledge-base
entities: A graph neural network approach,” arXiv
preprint arXiv:1706.05674, 2017.

[22] J. Tang, M. Qu, and Q. Mei, “Pte: Predictive
text embedding through large-scale heterogeneous text
networks,” in Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 2015, pp. 1165–1174.

The Computer Journal, Vol. ??, No. ??,

????

Association Rules Enhanced Knowledge Graph Attention Network

19

[38] C. Shang, Y. Tang, J. Huang, J. Bi, X. He, and B. Zhou,
“End-to-end structure-aware convolutional networks
for knowledge base completion,” in Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, vol. 33,
2019, pp. 3060–3067.

[39] S. Guo, Q. Wang, L. Wang, B. Wang, and L. Guo,
“Jointly embedding knowledge graphs and logical
the 2016 Conference on
rules,” in Proceedings of
Empirical Methods in Natural Language Processing,
2016, pp. 192–202.

[23] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van
Den Berg,
I. Titov, and M. Welling, “Modeling
relational data with graph convolutional networks,” in
European Semantic Web Conference. Springer, 2018,
pp. 593–607.

[24] T. N. Kipf and M. Welling,

“Semi-supervised
classiﬁcation with graph convolutional networks,”
arXiv preprint arXiv:1609.02907, 2016.

[25] A. Neelakantan, B. Roth, and A. McCallum, “Compo-
sitional vector space models for knowledge base com-
pletion,” arXiv preprint arXiv:1504.06662, 2015.
[26] D. Krompaß, S. Baier, and V. Tresp, “Type-constrained
representation learning in knowledge graphs,” in
International semantic web conference. Springer, 2015,
pp. 640–655.

[27] H. Zhong, J. Zhang, Z. Wang, H. Wan, and Z. Chen,
“Aligning knowledge and text embeddings by entity
descriptions,” in Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing,
2015, pp. 267–272.

[28] X. Jiang, Q. Wang, and B. Wang, “Adaptive convo-
lution for multi-relational learning,” in Proceedings of
the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Pa-
pers), 2019, pp. 978–987.

[29] M. Richardson and P. Domingos, “Markov logic
networks,” Machine learning, vol. 62, no. 1-2, pp. 107–
136, 2006.

[30] J. Pujara, H. Miao, L. Getoor, and W. Cohen,
“Knowledge graph identiﬁcation,” in International
Semantic Web Conference.
Springer, 2013, pp. 542–
557.

[31] I. Beltagy and R. J. Mooney, “Eﬃcient markov
language semantics,” in
logic inference for natural
Workshops at the Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence. Citeseer, 2014.

[32] Z. Wei, J. Zhao, K. Liu, Z. Qi, Z. Sun, and G. Tian,
“Large-scale knowledge base completion: Inferring via
grounding network sampling over selected instances,”
in Proceedings of the 24th ACM International on Con-
ference on Information and Knowledge Management,
2015, pp. 1331–1340.

[33] P. Wang, D. Dou, F. Wu, N. de Silva, and L. Jin,
“Logic rules powered knowledge graph embedding,”
arXiv preprint arXiv:1903.03772, 2019.

[34] T. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel,
“Convolutional 2d knowledge graph embeddings,”
arXiv preprint arXiv:1707.01476, 2017.

[35] K. Toutanova and D. Chen, “Observed versus latent
features for knowledge base and text inference,” in
Proceedings of the 3rd Workshop on Continuous Vector
Space Models and their Compositionality, 2015, pp. 57–
66.

[36] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston,
and O. Yakhnenko, “Translating embeddings
for
modeling multi-relational data,” in Advances in neural
information processing systems, 2013, pp. 2787–2795.

[37] S. Vashishth, S. Sanyal, V. Nitin, and P. Taluk-
dar, “Composition-based multi-relational graph convo-
lutional networks,” arXiv preprint arXiv:1911.03082,
2019.

The Computer Journal, Vol. ??, No. ??,

????

