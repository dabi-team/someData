2
2
0
2

l
u
J

9

]

G
L
.
s
c
[

1
v
8
0
3
4
0
.
7
0
2
2
:
v
i
X
r
a

1

Dynamic Time Warping based Adversarial
Framework for Time-Series Domain

Taha Belkhouja, Yan Yan, Member, IEEE and Janardhan Rao Doppa, Member, IEEE

Abstract—Despite the rapid progress on research in adversarial robustness of deep neural networks (DNNs), there is little principled
work for the time-series domain. Since time-series data arises in diverse applications including mobile health, ﬁnance, and smart grid, it is
important to verify and improve the robustness of DNNs for the time-series domain. In this paper, we propose a novel framework for the
time-series domain referred as Dynamic Time Warping for Adversarial Robustness (DTW-AR) using the dynamic time warping measure.
Theoretical and empirical evidence is provided to demonstrate the effectiveness of DTW over the standard Euclidean distance metric
employed in prior methods for the image domain. We develop a principled algorithm justiﬁed by theoretical analysis to efﬁciently create
diverse adversarial examples using random alignment paths. Experiments on diverse real-world benchmarks show the effectiveness of
DTW-AR to fool DNNs for time-series data and to improve their robustness using adversarial training. The source code of DTW-AR
algorithms is available at https://github.com/tahabelkhouja/DTW-AR

Index Terms—Time Series, Robustness, Deep Neural Networks, Adversarial Examples, Dynamic Time Warping.

(cid:70)

1 INTRODUCTION

T O deploy deep neural network (DNN) based systems

in important real-world applications such as healthcare,
we need them to be robust. Adversarial methods expose
the brittleness of DNNs and motivate methods to improve
their robustness. There is little principled work for the
time-series domain even though this type of data arises
in many real-world applications including mobile health
[1], ﬁnance [2], and smart grid analytics [3]. The time-series
modality poses unique challenges for studying adversarial
robustness that are not seen in images [4] and text [5]. The
standard approach of imposing an lp-norm bound to create
worst possible scenarios from a learning agent’s perspective
doesn’t capture the true similarity between time-series
instances. Consequently, lp-norm constrained perturbations
can potentially create adversarial examples that correspond
to a completely different class label. There is no prior work
on ﬁltering methods in the signal processing literature to
automatically identify such adversarial candidates. Hence,
adversarial examples from prior methods based on lp-norm
will confuse the learner when they are used to improve the
robustness of DNNs. In other words, the accuracy of DNNs
will degrade on real-world data after adversarial training.

This paper proposes a novel adversarial framework
for time-series domain referred as Dynamic Time Warping
for Adversarial Robustness (DTW-AR) to address the above-
mentioned challenges. DTW-AR employs the dynamic time
warping measure [6], [7] as it can be used to measure a realis-
tic distance between two time-series signals (e.g., invariance
to shift and scaling operations) [7], [8]. For example, a signal
that has its frequency changed due to Doppler effect would
output a small DTW measure to the original signal. However,
if Euclidean distance is used, both signals would look very
dissimilar, unlike the reality. We theoretically analyze the

• The authors are with the School of Electrical Engineering and Computer
Science, Washington State University, Pullman, WA, 99164. E-mail:
{taha.belkhouja,yan.yan1,jana.doppa}@wsu.edu

suitability of DTW measure over the Euclidean distance.
Speciﬁcally, the space of candidate adversarial examples in
the DTW space is a superset of those in Euclidean space for
the same distance bound. Therefore, DTW measure provides
a more appropriate bias than the Euclidean space for the time
series domain and our experiments demonstrate practical
beneﬁts of DTW-based adversarial examples.

To create targeted adversarial examples, we formulate
an optimization problem with the DTW measure bound
constraint and propose to solve it using an iterative gradient-
based approach. However, this simple method has two
drawbacks. First, this method allows us to only ﬁnd one
valid adversarial example out of multiple solution candidates
from the search space because it operates on a single optimal
alignment. Second, we need to compute DTW measure in
each iteration as the optimal DTW alignment path changes
over iterations. Since the number of iterations are typically
large and DTW computation is expensive, the overall algo-
rithm becomes prohibitively slow. To successfully overcome
these two drawbacks, our key insight is to employ stochastic
alignments to create adversarial examples. We theoretically and
experimentally show that a simpler distance measure based
on random alignment path upper-bounds the DTW measure
measure and that this bound is tight. This algorithm allows
us to efﬁciently create many diverse adversarial examples
using different alignment paths to improve the robustness
of DNN models via adversarial training. Our experiments
on real-world time-series datasets show that the DTW-AR
creates more effective adversarial attacks to fool DNNs
when compared to prior methods and enables improved
robustness.

Contributions. The key contribution of this paper is the
development and evaluation of the DTW-AR framework
for studying adversarial robustness of DNNs for time-series
domain. Speciﬁc list includes:

• Theoretical and empirical analysis to demonstrate the

 
 
 
 
 
 
effectiveness of DTW over the standard l2 distance
metric for adversarial robustness studies.

• Principled algorithm using DTW measure to efﬁ-
ciently create diverse adversarial examples via ran-
dom alignment paths justiﬁed by theoretical analysis.
• Experimental evaluation of DTW-AR on diverse real-
world benchmarks and comparison with state-of-the-
art baselines. The code and data are available on a
GitHub repository and will be made public.

• The source code of DTW-AR algorithms is available
at https://github.com/tahabelkhouja/DTW-AR

2 BACKGROUND AND PROBLEM SETUP
Let X ∈ Rn×T be a multi-variate time-series signal, where n
is the number of channels and T is the window-size of the
signal. We consider a DNN classiﬁer Fθ : Rn×T → Y, where
θ stands for parameters and Y is the set of classiﬁcation labels.
Table 1 summarizes the different mathematical notations
used in this paper.

TABLE 1
Mathematical notations used in this paper.

Variable

Deﬁnition

Fθ
Rn×T

Xadv

DNN classiﬁer with parameters θ

Time-series input space, where n is the number

of channels and T is the window-size

Adversarial example generated from time-series
input X ∈ Rn×T

Y

Set of output class labels

DT W (·, ·) Dynamic time warping distance

P

C

δ

Alignment path: a sequence of cost matrix cells

{(i, j)}i≤T,j≤T

Alignment cost matrix generated by dynamic

programming with elements Ci,j

Distance bound constraint

Xadv is called an adversarial example of X if:
(cid:8)Xadv

(cid:12)
(cid:12) DIST (Xadv, X) ≤ δ and Fθ(X) (cid:54)= Fθ(Xadv)(cid:9)

where δ deﬁnes the neighborhood of highly-similar examples
for input X using a distance metric DIST to create worst-
possible outcomes from the learning agent’s perspective.
Note that adversarial examples depend on the target concept
because it deﬁnes the notion of invariance we care about.
Challenges for time-series data. The standard lp-norm
distance doesn’t capture the unique characteristics (e.g., fast-
pace oscillations, sharp peaks) and the appropriate notion
of invariance for time-series signals. Hence, lp-norm based
perturbations can lead to a time-series signal that seman-
tically belongs to a different class-label. Our experiments
show that small perturbations result in adversarial examples
whose l2 distance from the original time-series signal is
greater than the distance between time-series signals from
two different class labels (see Section 5.2). Therefore, we need
to study new methods by exploiting the structure and unique
characteristics of time-series signals.

2

DTW measure. The DTW measure between two uni-variate
signals X and Z ∈ RT is computed via a cost matrix
C ∈ RT ×T using a dynamic programming (DP) algorithm
with time-complexity O(T 2). The cost matrix is computed
recursively using the following equation:

Ci,j = d(Xi, Zj) + min (cid:8)Ci−1,j, Ci,j−1, Ci−1,j−1

(cid:9)

(1)

where d(·, ·) is any given distance metric (e.g., (cid:107) · (cid:107)p norm).
The DTW measure between signals X and Z is DT W (X, Z)
= CT,T . The sequence of cells P = {ci,j = (i, j)} contributing
to CT,T is the optimal alignment path between X and Z. Figure
1 provides illustration for an optimal alignment path. We
note that the diagonal path corresponds to the Euclidean
distance metric.

Fig. 1. Illustration of DTW alignment between two uni-variate signals X
and Z of length 4. The optimal alignment path (shown in green color) is
P = {(1, 1), (2, 1), (3, 2), (4, 2), (4, 3), (4, 4)}.

For the multi-variate case, where X and Z ∈ Rn×T ,
to measure the DTW measure using Equation 1, we have
d(Xi, Zj) with Xi, Zj ∈ Rn [9]. We deﬁne the distance
function distP (X, Z) between time-series inputs X and
Z according to an alignment path P using the following
equations:

distP (X, Z) =

(cid:88)

(i,j)∈P

d(Xi, Zj)

(2)

Hence, the DTW measure between X and Z is given by:

DT W (X, Z) = min

P

distP (X, Z)

(3)

3 DYNAMIC TIME WARPING BASED ADVERSARIAL
ROBUSTNESS FRAMEWORK
The DTW-AR framework creates targeted adversarial ex-
amples for time-series domain using the DTW measure
as illustrated in Figure 2. For any given time-series input
X, DNN classiﬁer Fθ, and distance bound δ, we solve an
optimization problem to identify an adversarial example
Xadv which is within DTW measure δ to the original time-
series signal X. In what follows, we ﬁrst provide empirical
and theoretical results to demonstrate the suitability of DTW
measure over Euclidean distance for adversarial robustness
studies in the time-series domain (Section 3.1). Next, we
introduce the optimization formulation based on the DTW
measure to create adversarial examples and describe its main
drawbacks (Section 3.2). Finally, we explain our key insight
of using stochastic alignment paths to successfully overcome
those drawbacks to efﬁciently create diverse adversarial
examples and provide theoretical justiﬁcation (Section 3.3).

3

Fig. 2. Overview of the DTW-AR framework to create targeted adversarial examples. Given an input X, a target class-label ytarget and a distance
bound δ, DTW-AR aims to identify an adversarial example Xadv using a random alignment path Prand. DTW-AR solves an optimization problem
involving a DTW-similarity loss and a classiﬁcation loss using a random alignment path Prand and a DNN classiﬁer Fθ. Using different random
alignment paths, DTW-AR will be able to create diverse adversarial examples which meet the DTW measure bound δ.

3.1 Effectiveness of DTW measure measure

Empirical justiﬁcation. As we argued before, the standard
l2 distance is impractical for adversarial learning in time-
series domain. Perturbations based on Euclidean distance can
result in adversarial time-series signals which semantically
belong to a different class-label. Figure 3 provides an intuitive
illustration of suitability of DTW over l2 distance and is based
on the real-world data representation that is provided in
Figure 4. It shows the difference in the true data distribution

between data points in the original space. Figure 4 shows
MDS results of SC dataset. We can clearly see how the data
from different class labels are better clustered in the DTW
space compared to the Euclidean space, as provided in Figure
4. An adversarial example for an SC data point in the green-
labeled class is more likely to semantically belong to the red-
labeled distribution in the Euclidean space. However, in the
DTW space, the adversarial example is more likely to remain
in the green-labeled space, while only being misclassiﬁed by
the DNN classiﬁer due the adversarial problem.

Euclidean Space

DTW Space

Euclidean Space

DTW Space

Fig. 3. Illustration of the suitability of DTW over Euclidean distance using
the true data distribution from two classes shown in red and green colors.
The concentric circles represent the close-similarity area of each input
instance (i.e., center) using the corresponding distance measure.

Fig. 4. Multi-dimensional scaling results showing the labeled data
distribution in Euclidean space (left column) and DTW space (right
column) for the SC dataset. DTW space exhibits better clustering for
same-class data than Euclidean space.

in Euclidean space (i.e., l2 is used as the similarity measure)
and in DTW space (i.e., DTW is used as the similarity
measure) for two classes shown in red and green colors. The
concentric circles represent the close-similarity area around
each input instance (i.e., center) where adversarial examples
are considered. We can observe that in the Euclidean space,
adversarial example of an input instance can belong to
another class label, which is not the case in the DTW space.
This simple illustration shows how DTW-AR can generate
effective adversarial examples due to the appropriate bias of
DTW for time-series domain.

We also show that this abstraction is true for real-world
data. We employ multi-dimensional scaling (MDS), a visual
representation of dissimilarities between sets of data points
[10], to compare DTW and Euclidean spaces. MDS is a
dimensionality reduction method that preserves the distances

Theoretical justiﬁcation. We prove that the DTW measure
allows DTW-AR to explore a larger space of candidate
adversarial examples when compared to perturbations based
on the Euclidean distance, i.e., identiﬁes blind spots of prior
methods. This result is based on the fact that the point-to-
point alignment (i.e., Euclidean distance) between two time-
series signals is not always the optimal alignment. Hence, the
existence of adversarial examples which are similar based
on DTW and may not be similar based on the Euclidean
distance. To formalize this intuition, we provide Observation
1. We characterize the effectiveness of DTW-AR based attack
as better for their ability to extend the space of attacks based
on the Euclidean distance and their potential to fool DNN
classiﬁers that rely on Euclidean distance for adversarial
training. Our experimental results demonstrate that DTW-
AR generates effective adversarial examples to fool the target

DNN classiﬁers by leveraging the appropriate bias of DTW
for time-series data.

The ﬁnal loss function L we want to minimize to create
optimized adversarial example Xadv is:

4

Observation 1. Let l2 be the equivalent of Euclidean distance
using the cost matrix in the DTW space. ∀X ∈ Rn×T (n >
1, T > 1), there exists (cid:15) ∈ Rn×T and an alignment path P such
that distP (X, X + (cid:15)) ≤ δ and l2(X, X + (cid:15)) > δ.
Theorem 1. For a given input space Rn×T , a constrained DTW
space for adversarial examples is a strict superset of a constrained
euclidean space for adversarial examples. If X ∈ Rn×T :

(cid:26)

Xadv

(cid:12)
(cid:12)DT W (X, Xadv) ≤ δ

(cid:27)

(cid:26)

⊃

Xadv

(cid:12)
(cid:12)(cid:107)X − Xadv(cid:107)2

2 ≤ δ

(cid:27)

(4)

As an extension of Observation 1, the above theorem
states that in the space where adversarial examples are
constrained using a DTW measure bound, there exists
more adversarial examples that are not part of the space
of adversarial examples based on the Euclidean distance for
the same bound (i.e., blind spots). This result implies that
DTW measure has an appropriate bias for the time-series
domain. We present the proofs of both Observation 1 and
Theorem 1 in the Appendix. Hence, our DTW-AR framework
is potentially capable of creating more effective adversarial
examples than prior methods based on l2 distance for the
same distance bound constraint. These adversarial examples
are potentially more effective as they are able to break deep
models by leveraging the appropriate bias of DTW measure.
However, to convert this potential to reality, we need
an algorithm that can efﬁciently search this larger space of
attacks to identify most or all adversarial examples which
meet the DTW measure bound. Indeed, developing such an
algorithm is one of the key contributions of this paper.

3.2 Naive optimization based formulation and chal-
lenges to create adversarial examples
To create adversarial examples to fool the given DNN Fθ,
we need to ﬁnd an optimized perturbation of the input time-
series X to get Xadv. Our approach is based on minimizing
a loss function L using gradient descent that achieves two
goals. 1) Misclassiﬁcation goal: Adversarial example Xadv to
be mis-classiﬁed by Fθ as a target class-label ytarget; and 2)
DTW similarity goal: close DTW-based similarity between
time-series X and adversarial example Xadv.

To achieve the mis-classiﬁcation goal, we employ the

formulation of [11] to deﬁne a loss function:

Llabel(Xadv) = max

(cid:104)

max
y(cid:54)=ytarget

(Sy (Xadv))

− Sytarget (Xadv) , ρ

(5)

(cid:105)

where ρ < 0. It ensures that the adversarial example will be
classiﬁed by the DNN as class-label ytarget with a conﬁdence
|ρ| using the output of the pre-softmax layer {Sy}y∈Y .

To achieve the DTW similarity goal, we need to
create Xadv for a given time-series input X such that
DT W (X, Xadv) ≤ δ. We start by a naive optimization
over the DTW measure using the Soft-DTW measure
SDTW(X, Xadv) [12]. Hence, the DTW similarity loss func-
tion is:

LDT W (Xadv) = SDTW(X, Xadv)

(6)

L(Xadv) = Llabel(Xadv) + LDT W (Xadv)

((cid:63))

We operate under white-box setting and can employ
gradient descent to minimize the loss function in Equation (cid:63)
over Xadv. This approach works for black-box setting also.
In this work, we consider the general case where we do
not query the black-box target DNN classiﬁer. We show
through experiments that the created adversarial examples
can generalize to fool other black-box DNNs.

Challenges of Naive approach. Recall that our overall goal
is to identify most or all targeted adversarial time-series
examples that meet the DTW measure bound. This will
allow us to improve the robustness of DNN model using
adversarial training. This naive approach has two main
drawbacks.

(a)

(b)

Fig. 5. Illustration of the close-similarity space around a given time-series
signal (black center) in the Euclidean and DTW space. Using l2 norm is
sufﬁcient to explore the entire Euclidean space around the input. However,
in the DTW space, each colored section corresponds to one adversarial
example that meets the DTW measure bound constraint. Each of them
can be found using only a subset of candidate alignment paths.

• Single adversarial example. The method allows us to
only ﬁnd one valid adversarial example out of multiple
solution candidates from the search space because it op-
erates on a single optimal alignment path. Using a single
alignment path (whether the diagonal path for Euclidean
distance or the optimal alignment path generated by DTW),
the algorithm will be limited to the adversarial examples
which use that single alignment. In Figure 5, we provide a
conceptual illustration of SADV (X), the set of all adversarial
examples Xadv which meet the distance bound constraint
DT W (X, Xadv) ≤ δ. In the Euclidean space, using l2 norm
is sufﬁcient to explore the entire search space around the
original input to create adversarial examples. However, in
the DTW space, each colored section in SADV (X) can only
be found using a subset of candidate alignment paths.

• High computational cost. DTW is non-differentiable
and approximation methods are often used in practice.
These methods require O(n.T 2) to ﬁll the cost matrix and
O(T ) to backtrack the optimal alignment path. These steps
are computationally-expensive. Gradient-based optimization
iteratively updates the adversarial example Xadv to achieve
the DTW similarity goal, i.e., DT W (X, Xadv) ≤ δ, and
the mis-classiﬁcation goal, i.e., Fθ(Xadv)=ytarget. Standard
algorithms such as projected gradient descent (PGD) [13]
and Carloni & Wagner (CW) [11] require a large number

of iterations to generate valid adversarial examples. This is
also true for the recent computer vision speciﬁc adversarial
algorithms [14], [15]. For time-series signals arising in many
real-world applications, the required number of iterations
to create successful attacks can grow even larger. We need
to compute DTW measure in each iteration as the optimal
DTW alignment path changes over iterations. Therefore, it is
impractical to use the exact DTW computation algorithm to
create adversarial examples. We also show that the existing
optimized approaches to estimate the DTW measure remain
computationally expensive for an adversarial framework.
We provide results to quantify the runtime cost in our
experimental evaluation.

Algorithm 1 DTW-AR based Adversarial Algorithm
Input: time-series X; DNN classiﬁer Fθ; target class-label
ytarget; learning rate η; maximum iterations MAX
Output: adversarial example Xadv
1: Prand ← random alignment path
2: Initialization: Xadv ← X
3: for i=1 to MAX do
4:
5:
6:

L(Xadv) ← Llabel(Xadv) + LDT W (Xadv, Prand)
Compute gradient ∇Xadv L(Xadv)
Perform gradient descent step:
Xadv ← Xadv − η × ∇Xadv L(Xadv)

7: end for
8: return optimized adversarial example Xadv

3.3 Stochastic alignment paths for the DTW similarity
goal and theoretical justiﬁcation

In this section, we describe the key insight of DTW-AR to
overcome the above-mentioned two challenges and provide
theoretical justiﬁcation.

To overcome the above-mentioned two challenges of the
naive approach, we propose the use of a random alignment
path to create adversarial attacks on DNNs for time-series
domain. The key idea is to select a random alignment path P
and to execute our adversarial algorithm while constraining
over distP (X, Xadv) instead of DT W (X, Xadv). This choice
is justiﬁed from a theoretical point-of-view due to the special
structure in the problem to create DTW based adversarial
examples. Using the distance function distP (X, Xadv), we
redeﬁne Equation 6 as follows:

LDT W (Xadv, P ) = α1 × distP (X, Xadv)

− α2 × distPdiag (X, Xadv)

(7)

where α1 > 0, α2 ≥ 0, Pdiag is the diagonal alignment
path equivalent to the Euclidean distance, and P is a given
alignment path (P (cid:54)= Pdiag). The ﬁrst term of Equation 7 is
deﬁned to bound the DTW similarity of adversarial example
Xadv to a threshold δ as stated in Observation 2. The second
term represents a penalty term to account for adversarial
example with close Euclidean distance to the original input
X and pushes the algorithm to look beyond adversarial
examples in the Euclidean space. The coefﬁcients α1 and
α2 contribute in deﬁning the position of the adversarial
output in the DTW and/or Euclidean space. If α2 → 0, the
adversarial example Xadv will be highly similarity to the

5

original input X in the DTW space with no consideration to
the Euclidean space. Hence, the adversarial example may be
potentially adversarial in the Euclidean space also. However,
if α2 > 0, the adversarial output will be highly similar to
the original input in the DTW space but out of the scope
of adversarial attacks in the Euclidean space (i.e., a blind
spot). Recall from Theorem 1 that DTW space allows more
candidate adversarial examples than Euclidean space. Hence,
this setting allows us to ﬁnd blind spots of Euclidean space
based attacks.

The DTW-AR approach to create adversarial examples is
shown in Algorithm 1. We note that the naive approach that
uses Soft-DTW with the Carlini & Wagner loss function is a
sub-case of DTW-AR as shown below:

SDTW(X, Xadv) = LDT W (Xadv, PDT W ) =
1 × distPDT W (X, Xadv) − 0 × distPdiag (X, Xadv)

(8)

where PDT W is the optimal DTW alignment path.

Observation 2. Given any alignment path P and two multivari-
ate time-series signals X, Z ∈ Rn×T . If we have distP (X, Z) ≤
δ, then DT W (X, Z) ≤ δ.

Observation 2 states that distP (X, Z) deﬁned with re-
spect to a path P is always an upper bound for DT W (X, Z),
since DTW uses the optimal alignment path. Hence, when
the alignment path is ﬁxed, the time-complexity is reduced
to a simpler similarity measure that requires only O(n.T ),
which results in signiﬁcant computational savings due to
repeated calls within the adversarial algorithm.

Our stochastic alignment method also improves the
search strategy for ﬁnding multiple desired adversarial
examples. Suppose SADV (X) is the set of all adversarial
examples Xadv which meet the distance bound constraint
DT W (X, Xadv) ≤ δ. Each adversarial example in SADV (X)
can be found using only a subset of candidate alignment
paths. By using a stochastic alignment path, we can leverage
the large pool of different alignment paths to uncover more
than one adversarial example from SADV (X). On the other
hand, if the exact DTW computation based algorithm was
feasible, we would only ﬁnd a single Xadv, as DTW based
algorithm operates on a single optimal alignment path.

Theoretical tightness of bound. While Observation 2 pro-
vides an upper bound for the DTW measure, it does not
provide any information about the tightness of the bound. To
analyze this gap, we need to ﬁrst deﬁne a similarity measure
between two alignment paths to quantify their closeness.
We deﬁne PathSim as a similarity measure between two
alignment paths P1 and P2 in the DTW cost matrix of
time-series signals X, Z ∈ Rn×T . Let P1 = {c1
k} and
P2 = {c2
1, ..., c2
l } represent the sequence of cells for paths P1
and P2 respectively.

1, ..., c1

PathSim (P1, P2) =

1
2T



(cid:88)



c1
i

min
c2
j

(cid:107)c1

i − c2

j (cid:107)1 +

(cid:88)

c2
i

min
c1
j

(cid:107)c2

i − c1

j (cid:107)1





(9)

As PathSim(P1, P2) approaches 0, P1 and P2 are
very similar, and they will be the exact same path if
PathSim(P1, P2) = 0. For X, Z ∈ Rn×T , two very similar

alignment paths corresponds to a similar feature alignment
between X and Z. Theorem 2 shows the tightness of the
bound given in Observation 2 using the path similarity
measure deﬁned above.
Theorem 2. For a given input X ∈ Rn×T and a random
alignment path Prand, the resulting adversarial example Xadv
from the minimization over distPrand(X, Xadv) is equivalent to
minimizing over DT W (X, Xadv). For any Xadv generated by
DTW-AR using Prand, we have:




PathSim(Prand, PDT W ) = 0
&
distPrand(X, Xadv) = DT W (X, Xadv)
where PDT W is the optimal alignment path found using DTW
computation between X and Xadv.

(10)



Similarity measure PathSim deﬁnition. For DTW-AR, we
rely on a stochastic alignment path to compute distP deﬁned
in Equation 2. To improve our understanding of the behavior
of DTW-AR framework based on stochastic alignment paths,
we propose to deﬁne a similarity measure that we call
PathSim. This measure quantiﬁes the similarities between
two alignment paths P1 and P2 in the DTW cost matrix
for two time-series signals X, Z ∈ Rn×T . If we denote
the alignment path sequence P1 = {c1
k} and P2 =
{c2
1, · · · , c2
l }, then we can measure their similarity as deﬁned
in Equation 9.

1, · · · , c1

This deﬁnition is a valid similarity measure as it satisﬁes

all the distance axioms [16]:

Non-negativity: By deﬁnition, PathSim(P1, P2) is a
l1 distances, which are all positives. Hence,

sum of
PathSim(P1, P2) ≥ 0.

Unicity: PathSim(P1, P2) = 0

⇐⇒

1
2T

(cid:88)
(

min
c2
j

(cid:107)c1

i − c2

j (cid:107)1

c1
i
(cid:88)

+

⇐⇒

c2
i
min
c2
j

(cid:88)

c1
i

(cid:107)c2

i − c1

j (cid:107)1) = 0

min
c1
j

(cid:107)c1

i − c2

j (cid:107)1 +

(cid:88)

c2
i

min
c1
j

(cid:107)c2

i − c1

j (cid:107)1 = 0

As we have a sum equal to 0 of all positive terms, we
can conclude that each term (min (cid:107) · (cid:107)1) is equal to 0:
PathSim(P1, P2) = 0

⇐⇒ ∀i : (cid:107)c1
⇐⇒ ∀i : c1

i − c2
i = c2
i

i (cid:107)1 = 0

As both paths have the same sequence of cells, we can safely
conclude that PathSim(P1, P2) = 0 ⇐⇒ P1 = P2:

Symmetric Property:

PathSim(P1, P2) =





(cid:88)

c1
i


1
2T

=

(cid:107)c1

i − c2

j (cid:107)1 +

min
c2
j

(cid:88)

c2
i

min
c1
j

1
2T

(cid:88)



min
c1
j

c2
i
= PathSim(P2, P1)

(cid:107)c2

i − c1

j (cid:107)1 +

(cid:88)

c1
i

min
c2
j

(cid:107)c2

i − c1

j (cid:107)1





(cid:107)c1

i − c2

j (cid:107)1





6

Note that the triangle inequality is not applicable as the
alignment path spaces does not support additive operations.
This similarity measure quantiﬁes the similarity between
two alignment paths as it measures the l1 distance between
the different cells of each path. The multiplication factor
1/2T is introduced to prevent scaling of the measure for
large T values for a given time-series input space Rn×T .

In Figure 6, we visually show the relation between
PathSim measure and the alignment path for a given cost
matrix. We observe that when PathSim(P1, P2) → 0, P1
and P2 are very similar, and they will be the exact same
path if PathSim(P1, P2) = 0. For PathSim(P1, P2) (cid:29) 0,
the alignment path will go through different cells which are
far-placed from each other in the cost matrix.

Empirical tightness of bound. Figure 7 shows that over the
iterations of the DTW-AR algorithm, the updated adversarial
example yields to an optimal alignment path that is more
similar to the input random path. This result strongly
demonstrate that Theorem 2 holds empirically.

Corollary 1. Let P1 and P2 be two alignment paths such
that PathSim(P1, P2) > 0. If X 1
adv are the ad-
versarial examples generated using DTW-AR from any given
time-series X using paths P1 and P2 respectively such that
DT W (X, X 1
and X 2

adv) = δ and DT W (X, X 2

adv) = δ, then X 1

adv are not necessarily the same.

adv and X 2

adv

Theorem 2 shows that the adversarial example generation
using DTW-AR is equivalent to the ideal setting where it is
possible to optimize DT W (X, Xadv). The above corollary
extends Theorem 2 to show that if we employ different
alignment paths within Algorithm 1, we will be able to ﬁnd
more adversarial examples which meet the distance bound
in contrast to the naive approach.

4 RELATED WORK

Adversarial methods. Prior work on adversarial examples
mostly focus on image and text domains [4], [5]. Such
methods include Carlini & Wagner attack [11], boundary
attack [17], and universal attacks [18]. Recent work focuses
on regularizing adversarial example generation methods to
obey intrinsic properties of images [19], [20], [21]. In NLP
domain, methods to fool text classiﬁers employ the saliency
map of input words to generate adversarial examples while
preserving meaning to a human reader in white-box setting
[22]. DeepWordBug [23] employs a black-box strategy to fool
classiﬁers with simple character-level transformations. Since
characteristics of time-series (e.g., fast-pace oscillations, sharp
peaks) are different from images and text, prior methods are
not suitable to capture the appropriate notion of invariance
for time-series domain.

Adversarial robustness. Adversarial training is one of the
strongest empirical defense methods against adversarial
attacks [13], [24]. This involves employing attack methods
to create adversarial examples to augment the training
data for improving robustness. Stability training [25] is an
alternative method that explicitly optimizes for robustness
by deﬁning a loss function that evaluates the classiﬁer on
small perturbations of clean examples. This method yield to
a deep network that is stable against natural and adversarial

7

PathSim = 0

PathSim = 0.25

PathSim = 0.5

PathSim = 1.25

PathSim = 2.5

PathSim = 0.5

PathSim = 2.5

PathSim = 5

PathSim = 15

PathSim = 25

Fig. 6. Visualization of PathSim values along different example alignment paths in Rn×10 (First row) and Rn×100 (Second row) spaces.

Time-series pre-processing methods. A possible solution to
overcome the Euclidean distance concerns is to introduce
pre-processing steps that are likely to improve the existing
frameworks. Simple pre-processing steps such as MinMax-
normalization or z-normalization only solves problems such
as scaling problem. However, they do not address any con-
cern about signal-warping or time-shifts. Other approaches
rely on learning feature-preserving representations. A well-
known example is the GRAIL [33] framework. This frame-
work aims to learn compact time-series representations that
preserve the properties of a pre-deﬁned comparison function
such as DTW. The main concern about feature-preserving
pre-processing steps is that the representation learnt is not
reversible. In other words, a real-world time-series signal
cannot be generated from the estimated representation. The
goal of adversarial attacks is to create real-world time-series
that can be used to fool any DNN. Such challenges would
limit the usability and the generality of methods based on
pre-processing steps to study the robustness of DNNs for
time-series data.

In summary, existing methods for time-series domain
are lacking in the following ways: 1) they do not create
targeted adversarial attacks; and 2) they employ lp-norm
based perturbations which do not take into account the
unique characteristics of time-series data.

5 EXPERIMENTS AND RESULTS
We empirically evaluate the DTW-AR framework and discuss
the results along different dimensions.

5.1 Experimental setup

Datasets. We employ the UCR datasets benchmark [34].
We present the results on ﬁve representative datasets
(AtrialFibrillation, Epilepsy, ERing, Heartbeat,
RacketSports) from diverse domains noting that our
ﬁndings are general as shown by the results on remaining
UCR datasets in the Appendix. We employ the standard
training/validation/testing splits from these benchmarks.

(a)

(b)

Fig. 7. (a) Example of the convergence of the optimal alignment path
between the adversarial example and the original example at the start
of the algorithm (dotted red path) and at the end (red path) to the
given random alignment path (blue path). (b) PathSim score of the
optimal alignment path between the adversarial example and the original
example and the given random path for the ECG200 dataset averaged
over multiple random alignment paths.

distortions in the visual input. There are other defense
methods which try to overcome injection of adversarial
examples [26], [27], [28]. However, for time-series domain, as
lp-norm based perturbations may not guarantee preserving
the semantics of true class label, adversarial examples may
mislead DNNs during adversarial training resulting in
accuracy degradation.

Adversarial attacks for time-series domain. There is little
to no principled prior work on adversarial methods for time-
series. Fawaz et al., [29] employed the standard Fast Gradient
Sign method [30] to create adversarial noise with the goal
of reducing the conﬁdence of deep convolutional models
for classifying uni-variate time-series. Network distillation is
employed to train a student model for creating adversarial
attacks [31]. However, this method is severely limited: it can
generate adversarial examples for only a small number of
target labels and cannot guarantee generation of adversarial
example for every input. [32] tried to address adversarial
examples with elastic similarity measures, but does not
propose any elastic-measure based attack algorithm.

Conﬁguration of algorithms. We employ a 1D-CNN archi-
tecture for the target DNNs. We operate under a white-box
(WB) setting for creating adversarial examples to fool W B
model. To assess the effectiveness of attacks, we evaluate
the attacks under the black-box (BB) setting and to fool BB
model. Neural architectures of both W B and BB models
are in the Appendix. The adversarial algorithm has no prior
knowledge/querying ability of target DNN classiﬁers. Target
DNNs include: 1) DNN model with a different architec-
ture trained on clean data (BB); 2) DNNs trained using
augmented data from baselines attacks that are not speciﬁc
to image domain: Fast Gradient Sign method (F GS) [30],
Carlini & Wagner (CW ) attack [11], and Projected Gradient
Descent (P GD) [13]; and 3) DNN models trained using
stability training [25] (ST N ) for learning robust classiﬁers.

Evaluation metrics. We evaluate attacks using the efﬁciency
metric αEf f ∈ [0, 1] over the created adversarial examples.
αEf f (higher means better attacks) measures the capability
of adversarial examples to fool a given DNN Fθ to output
the target class-label. αEf f is calculated as the fraction of
adversarial examples that are predicted correctly by the
classiﬁer: αEf f = # Adv. examples s.t.F (X)==ytarget
. We evaluate
adversarial training by measuring the accuracy of the model
to predict ground-truth labels of adversarial examples. A
DNN classiﬁer is robust if it is successful in predicting the
true label of any given adversarial example.

# Adv. examples

5.2 Results and Discussion

Spatial data distribution with DTW. We have shown in
Figure 4 how the data from different class labels are better
clustered in the DTW space compared to the Euclidean
space. These results demonstrate that DTW suits better the
time-series domain as generated adversarial examples lack
true-label guarantees. Moreover, Euclidean distance based
attacks can potentially create adversarial examples that are
inconsistent for adversarial training. Our analysis showed
that for datasets such as WISDM, there are time-series signals
from different classes with l2-distances ≤ 2, while PGD or
FGS require (cid:15) ≥ 2 to create successful adversarial examples
for more than 70% time-series instances. We provide in
the Appendix an additional visualization of the adversarial
examples using DTW.

Admissible alignment paths. The main property of DTW
alignment is the one-to-many match between time-steps to
identify similar warped pattern. Intuitively, if an alignment
path matches few time-steps from the ﬁrst signal with
too many steps in the second signal, both signals are not
considered similar. Consequently, the optimal path would be
close to the corners of the cost matrix. Figure 8 provides a
comparison between two adversarial signals generated using
a green colored path closer to the diagonal vs. a red colored
path that is close to the corners. We can see that the red path
produces an adversarial example that is not similar to the
original input. Hence, we limit the range of the random path
Prand used to a safe range omitting the cells at the top and
bottom halves of the top-left and bottom-right corners.

Multiple diverse adversarial examples using DTW-AR.
In section 3.2, we argued that using stochastic alignment
paths, we can create multiple diverse adversarial time-series

8

Fig. 8. Effect of alignment path on adversarial example.

TABLE 2
Average percentage of dissimilar adversarial examples created by
DTW-AR using stochastic alignment paths for a given time-series. The
threshold (cid:15)sim determines whether two adversarial examples are
dissimilar or not based on l2 and DTW measures.

(cid:15)sim l2 norm

(cid:15)sim DTW

0.01

0.05

0.1

0.01

0.05

0.1

Atrial Fibrillation

98% 90% 87% 100% 100% 98%

Epilepsy

ERing

Heartbeat

99% 96% 93% 100% 100% 97%

99% 95% 93% 100% 100% 98%

99% 94% 92% 100%

99%

98%

RacketSports

99% 94% 92% 100% 100% 96%

examples within the same DTW measure bound. DTW-AR
method leverages the large pool of candidate alignment
paths to uncover more than one adversarial example as
illustrated in Figure 5. To further test this hypothesis, we
perform the following experiment. We sample a subset of
different (using PathSim) alignment paths {Prand}i and
execute DTW-AR algorithm to create adversarial examples
for the same time-series X. Let Xadv,i be the adversarial
example generated from X using Prand,i. We measure the
similarities between the generated {Xadv}i using DTW and
l2 distance. If the distance between two adversarial examples
is less than a threshold (cid:15)sim, then they are considered the
same adversarial example. Table 2 shows the percentage of
adversarial examples generated using different alignment
paths from a given time-series signal that are not similar
to any other adversarial example. We conclude that DTW-
AR algorithm indeed creates multiple different adversarial
examples from a single time-series signal for the same DTW
measure bound.

Empirical justiﬁcation for Theorem 2. We provided a
proof for the gap between creating an adversarial example
using the proposed DTW-AR algorithm and an ideal DTW
algorithm. In Figure 7(a), we provide an illustration of
the optimal alignment path update using DTW-AR. This
experiment was performed on the ECG200 dataset as an
example (noting that we observed similar patterns for other
datasets as well): the blue path represents the selected
random path to be used by DTW-AR and the red path
represents the optimal alignment path computed by DTW. At
the beginning, the optimal alignment path (dotted path) and
the random path are dissimilar. However, as the execution of
DTW-AR progresses, the updated adversarial example yields
to an optimal alignment path similar to the random path. In
Figure 7(b), we show the progress of the PathSim score as a
function of the iteration numbers of Algorithm 1. This ﬁgure
shows the convergence of the PathSim score to 0. These

9

Fig. 10. The progress of loss function values over the ﬁrst 100 iterations of
DTW-AR on different examples from ATRIALFIBRILLATION datasets. We
observe that empirically, the Equation (cid:63) plateaus at ρ before minimizing
LDT W .

DTW-AR Setting: α2 (cid:54)= 0

strong results conﬁrm the main claim of Theorem 2 that the
resulting adversarial example Xadv from the minimization
over distPrand(X, Xadv) is equivalent to minimizing over
DT W (X, Xadv)!

Loss function scaling.As the ﬁnal loss function is using two
different terms to create adversarial attacks, the absence of
a scaling parameter can affect the optimization process. In
Figure 10, we demonstrate that empirically, the ﬁrst term
of the Equation (cid:63) plateaus at ρ before minimizing LDT W .
The ﬁgure shows the progress of both Llabel and LDT W =
α1 × distP (X, Xadv) over the ﬁrst 100 iterations of DTW-AR
algorithm. We conclude that there is no need to scale the
loss function noting that our ﬁndings were similar for other
time-series datasets. In the general case, if a given application
requires attention to scaling both terms (LDT W and Llabel),
the learning rate can be adjusted to two different values:
Instead of having η ∗ ∇L = η ∗ ∇Llabel + η ∗ ∇LDT W , we
can use a learning rate pair η = (η1, η2) and gradient descent
step becomes η ∗ ∇L = η1 ∗ ∇Llabel + η2 ∗ ∇LDT W .

Effectiveness of adversarial attacks. Results of the fooling
rate of DTW-AR generated attacks for different models are
shown in Figure 9. We observe that under the white-box
setting (WB model), we have αEf f =1. This shows that for
any ytarget, DTW-AR successfully generates an adversarial
example for every input in the dataset. For black-box setting
(BB model) and other models using baseline attacks for
adversarial training, we see that DTW-AR attack is highly
effective for most cases. We conclude that these results
support the theoretical claim made in Theorem 1 by showing
that standard l2-norm based attacks have blind spots and
the DTW bias is appropriate for time-series. The importance
of α2 in Equation 7 is shown to improve the fooling rate of
adversarial examples. To implement DTW-AR adversarial
attacks, we have ﬁxed α1 = 0.5 and α2 = 0.5 for Equation
7. The importance of α2 is to push the algorithm to create
adversarial examples out of the scope of the Euclidean space
as shown in Figure 11. The adversarial examples with α2 (cid:54)= 0
evade DNNs with adversarial training baselines better than
the examples with α2 = 0.

DTW-AR Setting: α2 = 0

Fig. 11. Results for the effectiveness of adversarial examples from DTW-
AR on different DNNs under white-box (WB) and black-box (BB) settings,
and using adversarial training baselines (PGD, FGS, CW and STN) on
different datasets under two attack settings:α2 (cid:54)= 0 and α2 = 0.

DTW-AR based adversarial training. Our hypothesis is
that l2-based perturbations lack true-label guarantees and
can degrade the overall performance of DNNs. Figure 12
shows the accuracy of different DNNs after adversarial
training on clean data. This performance is relative to the
clean testing set of each dataset. We observe that all l2-
based methods degrade the performance using adversarial
training for at least one dataset. However, for many datasets,

Fig. 9. Results for the effectiveness of adversarial examples from DTW-
AR on different DNNs under white-box (WB) and black-box (BB) settings,
and using adversarial training baselines (PGD, FGS, CW and STN) on
different datasets.

the performance is visibly improved using DTW-AR based
adversarial training. Compared to standard training (i.e.,
no augmented adversarial examples), the performance on
AtrialFibrillation improved using DTW-AR while it
declined with other methods; and on HeartBeat, DTW-AR
based training improves from 70% to 75%. To evaluate the

10

Fig. 12. Results of adversarial training using baseline attacks and DTW-
AR, and comparison with standard training without adversarial examples
(No Attack) to classify clean data.

accuracy of DTW-AR in predicting the ground-truth label of
adversarial examples, we create adversarial examples using
a given attack algorithm and label each example with the
true class-label of the corresponding clean time-series input.
Figure 13 shows the results of DTW-AR based adversarial
training using W B architecture. In this experiment, we con-

DTW-AR Setting: α2 ∈ [0, 1]

DTW-AR Setting: α2 = 0

Fig. 13. Results of DTW-AR based adversarial training to predict the true
labels of adversarial examples generated by DTW-AR and the baseline
attack methods. The adversarial examples considered are those which
successfully fooled DNNs that do not use adversarial training.

sider the adversarial examples that have successfully fooled
the original DNN (i.e., no adversarial training). We observe
that DNNs using DTW-AR for adversarial training are able
to predict the original label of adversarial examples with
high accuracy. We can see how FGS and PGD attacks cannot
evade the DTW-AR based trained deep model for almost
any dataset. These results show that DTW-AR signiﬁcantly
improves the robustness of deep models for time-series data

DTW-AR Setting: α2 (cid:54)= 0

Fig. 14. Results of DTW-AR based adversarial training to predict the true
labels of adversarial examples generated by DTW-AR and the baseline
attack methods. The adversarial examples considered are those that
successfully fooled DNNs that do not use adversarial training.

to evade attacks generated by DTW-AR and other baseline

time-series as it is able to evade attacks generated by CW-
SDTW. Both these experiments demonstrate that CW-SDTW
is neither able to create stronger attacks nor a more robust
deep model when compared to DTW-AR.

11

Fig. 16. Results for the effectiveness of adversarial training using DTW-
AR based examples against adversarial attacks from CW-SDTW on
different datasets.

Comparison with Karim et al., [31]. The approach from
Karim et al., [31] employs network distillation to train a
student model for creating adversarial attacks. However, this
method is severely limited: only a small number of target
classes yield adversarial examples and the method does not
guarantee the generation of an adversarial example for every
input. Karim et al., have shown that for many datasets, this
method creates a limited number of adversarial examples in
the white-box setting. To test the effectiveness of this attack
against DTW-AR, Figure 29 shows the success rate of deep
model from DTW-AR based adversarial training to predict
the true labels of the attacks generated by the method from
Karim et al., on different datasets.

attacks. For the adversarial training, we employ several
values to create adversarial examples to be used in the
training phase. We have set α1 ∈ [0.1, 1] and α2 ∈ [0, 1]. In
Figure 14, we show the role of the term α2 of Equation 7 in
the robustness of the DNN. α2 ∈ [0, 1] ensures diverse DTW-
AR examples to increase the robustness of a given DNN.
When set to 0, we see that there is no signiﬁcant difference
in the performance against baseline attacks. However, the
DNN cannot defend against all DTW-AR attacks. We can also
observe that the setting where α2 is strictly different than 0
is the worst, as the DNN does not learn from the adversarial
examples that are found in the Euclidean space by DTW-AR
or the given baselines.

Naive approach: Carlini & Wagner with Soft-DTW. Recall
that naive approach uses DTW measure within the Carlini
& Wagner loss function. SoftDTW [12] allows us to create a
differentiable version of DTW measure. Hence, we provide
results for this naive approach to verify if the the use of Soft-
DTW with existing Euclidean distance based methods can
solve the challenges for the time-series domain mentioned
in this paper. We compare the DTW-AR algorithm with the
CW-SDTW that plugs Soft-DTW within the Carlini & Wagner
algorithm instead of the standard l2 distance. CW-SDTW has
the following limitations when comapred against DTW-AR:

• The time-complexity of Soft-DTW is quadratic in the
dimensionality of time-series input space, whereas
the distance computation in DTW-AR is linear.
• The CW-SDTW attack method is a sub-case of the
DTW-AR algorithm. If DTW-AR algorithm uses the
optimal alignment path instead of a random path, the
result will be equivalent to a CW-SDTW attack.
For a given time-series signal, CW-SDTW will output
one single adversarial example and cannot uncover
multiple adversarial examples which meet the DTW
measure bound. However, DTW-AR algorithm gives
the user control over the alignment path and can
create multiple diverse adversarial examples.

•

Fig. 17. Results of the success rate of deep model from DTW-AR
based adversarial training to predict the true label of adversarial attacks
generated using method in [31].

DTW-AR outperforms [31] due to following reasons:

• DTW-AR generates at least one adversarial example
for every input X ∈ Rn×T as shown in our experi-
ments.

• Adversarial examples created by DTW-AR are highly
effective against deep models relying on [31] for
adversarial training as this baseline fails to create
adversarial examples for many inputs and target
classes (shown in [31]).

• Adversarial examples created by the method from [31]
does not evade deep models from DTW-AR based
adversarial training.

Computational runtime of DTW-AR vs. DTW. As ex-
plained in the technical section, optimization based attack
algorithm requires a large number of iterations to create
a highly-similar adversarial example. For example, 103

Fig. 15. Results for the effectiveness of adversarial examples from DTW-
AR against adversarial training using examples created by CW-SDTW
on different datasets.

In conclusion, both challenges that were explained in the
Challenges of Naive approach in Section 3.1 cannot be solved
using CW-SDTW. As a consequence, the robustness goal
aimed by this paper cannot be achieved using solely CW-
SDTW. Indeed, our experiments support this hypothesis.
Figure 15 shows that DTW-AR is successful to fool a DNN
that uses adversarial examples from CW-SDTW for adver-
sarial training. This shows that our proposed framework is
better than this naive baseline. Figure 16 shows that DTW-
AR signiﬁcantly improves the robustness of deep models for

12

Fig. 18. Average runtime per iteration for standard DTW, FastDTW,
cDTW, and DTW-AR (on NVIDIA Titan Xp GPU).

5.3 Summary of Experimental Results

Fig. 19. Results for the effectiveness of adversarial examples from DTW-
AR using DTWAdaptive [9] (DTWD top row, DTWI bottom row) on
different DNNs under different settings.

iterations is the required default choice for CW to create
successful attacks, especially, for large time-series in our
experiments. The exact DTW method is non-differentiable,
thus, it is not possible to perform experiments to compare
DTW-AR method to the exact DTW method. Hence, we
assume that each iteration will compute the optimal DTW
path and use it instead of the random path. To assess the
runtime of computing the DTW measure, we employ three
different approaches: 1) The standard DTW algorithm, 2)
FastDTW [35] that is introduced as a very fast and accurate
DTW variant that works on lower resolutions of the data,
and 3) cDTW [36] that is a different variant of FastDTW. We
provide the runtime of performing each iteration using the
different algorithms in Figure 18. We can clearly observe that
DTW-AR is orders of magnitude faster than the standard
DTW and the accelerated DTW algorithms. The overall
computational cost will be signiﬁcantly reduced using DTW-
AR compared to exact DTW or soft-DTW [12] for large-size
time-series signals.

DTW-AR extension to other multivariate DTW mea-
sures.The DTW-AR framework relies on the distance func-
tion distP (X, Z) = (cid:80)
(i,j)∈P d(Xi, Zj) between two time-
series signals X and Z according to an alignment path
P to measure their similarity. Extending the DTW no-
tion from univariate to multivariate is a known prob-
lem, where depending on the application, researchers’
suggest to change the deﬁnition of distP (X, Z) to bet-
ter ﬁt the characteristics of the application at hand. In
all cases, DTW-AR relies on using the ﬁnal cost matrix
distP (X, Z) using dynamic program-
DT W (X, Z) = min
ming Ci,j = d(Xi, Zj) + min (cid:8)Ci−1,j, Ci,j−1, Ci−1,j−1
(cid:9)
.
Therefore, the use of different variants of distP (X, Z) (e.g.,
DT WI or DT WD [9]) will only affect the cost matrix values,
but will not change the assumptions and applicability of
DTW-AR. Therefore, DTW-AR is general and can work with
any variant of DTW. In Figure 19, we demonstrate that using
a different family of DTW (DT WI ) does not have a major
impact on DTW-AR’s performance and effectiveness. The
performance of DTW-AR framework using both alternative
measures of multi-variate DTW does not affect the overall
performance. Therefore, for a given speciﬁc application, the
practitioner can conﬁgure DTW-AR appropriately.

P

Our experimental results supported all the claims made in
Section 3. The summary list includes:

•

Figure 4 showed that DTW space is more suitable
for adversarial studies in the time-series domain than
Euclidean distance to support Theorem 1.

•

•

•

• Using stochastic alignment paths, DTW-AR creates
multiple diverse adversarial examples to support
Corollary 1 (Table 2), which is impossible using the
optimal alignment path.
Figure 7 provides empirical justiﬁcation for Theorem
2 showing that minimizing over a given alignment
path is equivalent to minimizing using exact DTW
method (bound is tight).
Figure 9 shows that adversarial examples created by
DTW-AR have higher potential to break time-series
DNN classiﬁers.
Figures 12 and 13 show that DTW-AR based adver-
sarial training is able to improve the robustness of
DNNs against baseline adversarial attacks.
Figure 15 and 16 shows that DTW-AR outperforms
the naive approach CW-SDTW that uses SoftDTW
with the Carlini & Wagner loss function. We also
demonstrated several limitations of CW-SDTW to
achieve the robustness goal aimed by this paper.
Figure 18 clearly demonstrates that DTW-AR signif-
icantly reduces the computational cost compared to
existing approaches of computing the DTW measure
for creating adversarial examples.
Figure 19 demonstrates that DTW-AR can generalize
to any multivariate DTW measure (such as DTWAdap-
tive [9]) without impacting on its performance and
effectiveness.

•

•

•

6 CONCLUSIONS
We introduced the DTW-AR framework to study adversarial
robustness of deep models for the time-series domain using
dynamic time warping measure. This framework creates ef-
fective adversarial examples by overcoming the limitations of
prior methods based on Euclidean distance. We theoretically
and empirically demonstrate the effectiveness of DTW-AR
to fool deep models for time-series data and to improve
their robustness. We conclude that the time-series domain
needs focused investigation for studying robustness of deep
models by shedding light on the unique challenges.

13

[23] J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi, “Black-box generation
of adversarial text sequences to evade deep learning classiﬁers,” in
IEEE Security and Privacy Workshops (SPW), 2018, pp. 50–56.
[24] F. Tramer, N. Carlini, W. Brendel, and A. Madry, “On adap-
tive attacks to adversarial example defenses,” arXiv preprint
arXiv:2002.08347, 2020.

[25] S. Zheng, Y. Song, T. Leung, and I. J. Goodfellow, “Improving the
robustness of deep neural networks via stability training,” in 2016
IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2016, Las Vegas, NV, USA, June 27-30, 2016.
IEEE Computer Society,
2016, pp. 4480–4488.

[26] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradients give
a false sense of security: Circumventing defenses to adversarial
examples,” ser. Proceedings of Machine Learning Research, J. Dy
Stockholmsm¨assan, Stockholm
and A. Krause, Eds., vol. 80.
Sweden: PMLR, 10–15 Jul 2018, pp. 274–283.

[27] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural
networks,” in IEEE Symposium on Security and Privacy (SP), 2016,
pp. 582–597.

[28] F. Tram`er, A. Kurakin, N. Papernot, I. J. Goodfellow, D. Boneh,
and P. D. McDaniel, “Ensemble adversarial training: Attacks and
defenses,” in 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings. OpenReview.net, 2018.

[29] H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P. Muller,
“Adversarial attacks on deep neural networks for time series
classiﬁcation,” in International Joint Conference on Neural Networks,
IJCNN 2019 Budapest, Hungary, July 14-19, 2019.
IEEE, 2019, pp.
1–8.

[30] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples
in the physical world,” in 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Work-
shop Track Proceedings. OpenReview.net, 2017.

[31] F. Karim, S. Majumdar, and H. Darabi, “Adversarial attacks on time
series,” IEEE Transactions on pattern analysis and machine intelligence,
2020.

[32] I. Oregi, J. D. Ser, A. P´erez, and J. A. Lozano, “Adversarial
sample crafting for time series classiﬁcation with elastic similarity
measures,” in Intelligent Distributed Computing XII, 12th International
Symposium on Intelligent Distributed Computing, IDC 2018, Bilbao,
Spain, 15-17 October 2018, ser. Studies in Computational Intelligence,
vol. 798. Springer, 2018, pp. 26–39.

[33] J. Paparrizos and M. J. Franklin, “Grail: Efﬁcient time-series

representation learning,” Proc. VLDB Endowment, 2019.

[34] A. Bagnall, J. Lines, W. Vickers, and E. Keogh, “The UEA & UCR
time series classiﬁcation rep.” www.timeseriesclassiﬁcation.com,
2020.

[35] S. Salvador and P. Chan, “Toward accurate dynamic time warping

in linear time and space,” Intelligent Data Analalysis, 2007.

[36] R. Wu and E. J. Keogh, “Fastdtw is approximate and generally
slower than the algorithm it approximates,” 2020. [Online].
Available: https://arxiv.org/abs/2003.11246

[37] M. Abadi and et al., “TensorFlow: Large-scale machine learning
on heterogeneous systems,” 2015, software available from
tensorﬂow.org. [Online]. Available: https://www.tensorﬂow.org/
[38] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,”
in Proceedings of Asia Conference on Computer and Communications
Security (ASIACCS). ACM, 2017.

[39] N. Papernot and et al., “Technical report on the cleverhans v2.1.0
adversarial examples library,” arXiv preprint arXiv:1610.00768, 2018.

REFERENCES

[1] A. Ignatov, “Real-time human activity recognition from accelerom-
eter data using convolutional neural networks,” Applied Soft
Computing, vol. 62, pp. 915–922, 2018.

[2] A. M. ¨Ozbayoglu, M. U. Gudelek, and O. B. Sezer, “Deep learning
for ﬁnancial applications : A survey,” Appl. Soft Comput., vol. 93, p.
106384, 2020.

[3] Z. Zheng, Y. Yang, X. Niu, H.-N. Dai, and Y. Zhou, “Wide and
deep convolutional neural networks for electricity-theft detection
to secure smart grids,” IEEE Transactions on Industrial Informatics,
2017.

[4] Z. Kolter and A. Madry, “Tutorial adversarial robustness: Theory

and practice,” NeurIPS, 2018.

[5] W. Y. Wang, S. Singh, and J. Li, “Deep adversarial learning for NLP,”
in Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2, 2019,
Tutorial Abstracts, A. Sarkar and M. Strube, Eds. Association for
Computational Linguistics, 2019, pp. 1–5.

[6] H. Sakoe, “Dynamic-programming approach to continuous speech
recognition,” in 1971 Proc. the International Congress of Acoustics,
Budapest, 1971.

[7] M. M ¨uller, “Dynamic time warping,” Information retrieval for music

and motion, pp. 69–84, 2007.

[8] D. J. Berndt and J. Clifford, “Using dynamic time warping to ﬁnd
patterns in time series.” in KDD workshop, vol. 10, no. 16. Seattle,
WA, USA:, 1994, pp. 359–370.

[9] M. Shokoohi-Yekta, B. Hu, H. Jin, J. Wang, and E. Keogh, “Gen-
eralizing dtw to the multi-dimensional case requires an adaptive
approach,” Data mining and knowledge discovery, vol. 31, no. 1, pp.
1–31, 2017.

[10] A. Buja, D. F. Swayne, M. L. Littman, N. Dean, H. Hofmann,
and L. Chen, “Data visualization with multidimensional scaling,”
Journal of computational and graphical statistics, vol. 17, no. 2, pp.
444–472, 2008.

[11] N. Carlini and D. A. Wagner, “Towards evaluating the robustness of
neural networks,” in 2017 IEEE Symposium on Security and Privacy,
SP 2017, San Jose, CA, USA, May 22-26, 2017.
IEEE Computer
Society, 2017, pp. 39–57.

[12] M. Cuturi and M. Blondel, “Soft-dtw: a differentiable loss function
for time-series,” in International Conference on Machine Learning.
PMLR, 2017, pp. 894–903.

[13] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
“Towards deep learning models resistant to adversarial attacks,” in
6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings. OpenReview.net, 2018.

[14] C. Laidlaw and S. Feizi, “Functional adversarial attacks,” in
Advances in Neural Information Processing Systems (Neur’IPS), 2019.
[15] A. Shafahi, M. Najibi, Z. Xu, J. Dickerson, L. S. Davis, and
T. Goldstein, “Universal adversarial training,” Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, vol. 34, 2020.

[16] M. J. Cullinane, “Metric axioms and distance,” The Mathematical

Gazette, vol. 95, no. 534, pp. 414–419, 2011.

[17] W. Brendel, J. Rauber, and M. Bethge, “Decision-based adversarial
attacks: Reliable attacks against black-box machine learning mod-
els,” in 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings. OpenReview.net, 2018.

[18] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Univer-
sal adversarial perturbations,” in 2017 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July
21-26, 2017.
IEEE Computer Society, 2017, pp. 86–94.

[19] C. Laidlaw and S. Feizi, “Functional adversarial attacks,” in
Advances in Neural Information Processing Systems (Neur’IPS), 2019,
pp. 10 408–10 418.

[20] C. Xiao, J. Zhu, B. Li, W. He, M. Liu, and D. Song, “Spatially
transformed adversarial examples,” in 6th International Conference
on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018.

[21] H. Hosseini, B. Xiao, M. Jaiswal, and R. Poovendran, “On the
limitation of convolutional neural networks in recognizing negative
images,” in 16th International Conference on Machine Learning and
Applications (ICMLA).

IEEE, 2017.

[22] S. Samanta and S. Mehta, “Towards crafting text adversarial

samples,” arXiv preprint arXiv:1707.02812, 2017.

APPENDIX A
EXPERIMENTAL AND IMPLEMENTATION DETAILS

14

Datasets. We have employed the standard benchmark train-
ing, validation, and testing split on the datasets. All datasets
are publicly available from the UCR repository [34]. We
employ the datasets in the main paper on which the classiﬁer
is able to have a performance better than random guessing.
Experiments on the effectiveness of adversarial attack that
aim to fool poor classiﬁers would not exhibit trust-worthy
results, as the classiﬁer originally is unable to predict clean
data.

DNN architectures. To evaluate the DTW-AR framework,
we employ two different 1D-CNN architectures — A0 and
A1 — to create two DNNs: W B uses A0 to evaluate the
adversarial attack under a white-box setting, and is trained
using clean training examples. BB uses the architecture A1
to evaluate the black-box setting for a model trained using
clean examples. The architecture details of the deep learning
models are presented in Table 3.

TABLE 3
Details of DNN architectures. C: Convolutional layers, K: kernel size, P:
max-pooling kernel size, and R: rectiﬁed linear layer.

C

x

100

K

x

5

C

66

50

K

12

5

P

12

4

R

1024

R

x

200

100

A0

A1

DTW-AR implementation. We implemented the DTW-AR
framework using TensorFlow 2 [37]. The parameter ρ that
was introduced in Equation 5 plays an important role in the
algorithm.

Llabel(Xadv) = max[ max

y(cid:54)=ytarget

(Sy (Xadv))−Sytarget (Xadv) , ρ]

(5)
ρ will push gradient descent to minimize mainly the second
term (LDT W ) when the ﬁrst term plateaus at ρ. Otherwise,
the gradient can minimize the general loss function by
pushing Llabel to −∞, which is counter-productive for our
goal. In all our experiments, we employ ρ = −5 for Llabel in
Equation 5 for a good conﬁdence in the classiﬁcation score.
A good conﬁdence score is important for the attack’s effec-
tiveness in a black-box setting. Black-box setting assumes
that information about the target deep model including its
parameters θ are not accessible. In general, the attacker
will create a proxy deep model to mimic the behavior of
the target model using regular queries. This technique can
be more effective when a target scenario is well-deﬁned
[24], [38]. However, in this work, we consider the general
case where we do not query the black-box target DNN
classiﬁer for a better assessment of the proposed framework.
Figure 20 shows the role of ρ value in enhancing DTW-AR
attacks in a black-box setting on ECG200 dataset noting
that we see similar patterns for other datasets as well.
Adversarial examples are generated using a maximum of
5 × 103 iterations of gradient descent with the ﬁxed learning
rate η=0.01. After all the iterations, the ﬁnal adversarial

15

not suitable for time-series domain since DTW-AR based
adversarial training is able to defend against these attacks.

Results and Discussion on l1 and l∞. Figure 26 shows the
MDS results of SC and Plane in the space using l1 as a
similarity measure (left) and in the space using l∞ as a
similarity measure (right). We can observe that similar to
the Euclidean space, there is a substantial entanglement
between different classes. Thus, using l1 and l∞ comes
with similar drawbacks to using the Euclidean space for
adversarial studies.

Figure 27 and 28 show the results of DTW-AR based
adversarial training against adversarial attacks generated
using l1 and l∞ as a metric.

We conclude that DTW-AR is able to generalize against
attacks in other spaces than the Euclidean one. Since the
Manhattan distance is similar to the Euclidean distance
in the point-to-point matching, and ∞-norm describes a
signal by solely its maximum value, DTW measure is still
considered a better similarity measure. The empirical success
of the DTW-AR suggests that the framework can be further
analyzed theoretically and empirically for future research
into adversarially robust classiﬁcation when compared to
different alternative similarity measures.

Comparison with [31] on the full MV UCR dataset. Figure
29 shows that the observations made within the main paper
are still valid over the different datasets.

DTW-AR transferability to RNNs. Prior work has shown
that RNN models are competitive with 1D-CNNs for time-
series data. Therefore, we evaluate the transferability of
adversarial examples from W B (1D-CNN) to an RNN model.
Figure 30 shows the success rate of DTW-AR adversarial
attacks to fool a Long short-term memory (LSTM) model. We
can observe that DTW-AR attacks have the transfer potential
to fool other non-CNN-based models. Since this paper only
studies the setting of no queries to the target deep model,
the attacks would have an increased efﬁciency if the target
deep model is available for label queries [38].
Runtime comparison of DTW-AR vs. Carlini & Wagner.
As explained in Section 3, one main advantage of DTW-AR
is reducing the time complexity of using DTW to create
adversarial examples. We provide in Figure 31 a comparison
of the average runtime per iteration to create one targeted
adversarial example by iterative baseline methods. We note
that we only compare to CW because FGSM and PGD are
not considered targeted attacks, and Karim et al. [31], fails
to create adversarial examples for every input. While we
observe that CW is faster, we note that we have already
demonstrated empirically (Figure 21 and 24) that DTW-AR
always outperforms CW in both effectiveness of adversarial
examples and adversarial training. We also observe differ-
ences in the DTW-AR’s runtime across datasets. DTW-AR
is relatively quick for small-size data such as RacketSports
(30 × 6) and slower for large-size data such as HeartBeat
(405 × 61). The additional runtime cost is explained by the
proposed loss function in Equation 7 that guarantees a highly-
similar adversarial example. For future work, we aim to
optimize the implementation of DTW-AR to further reduce
the runtime on large time-series datasets.

Results on the UCR univariate datasets. To show the effec-

Fig. 20. Results for the fooling rate on ECG200 dataset w.r.t different ρ
values for a black-box attack setting.

output is chosen from the iteration with the lowest DTW loss
provided from Equation 7.

LDT W (Xadv, P ) =α1 × distP (X, Xadv)

− α2 × distPdiag (X, Xadv)

(7)

Experimentally, we notice that for d(·, ·) in Equation 1
of the main paper, there is no inﬂuence on the performance
between choosing p = 1, 2 or ∞ for d(·, ·) = (cid:107) · (cid:107)p. However,
for datasets in Rn×T with n ≥ 2, we do not use p = ∞
as the dimensions are not normalized and data points will
be compared only along the dimension with the greater
magnitude.

Implementation of baselines. The baseline methods for CW,
PGD ,and FGS were implemented using the CleverHans
library [39] with updates to TensorFlow 2. For FGS and PGD
algorithms, we employed a minimal perturbation factors
((cid:15) < 1 ) for two main reasons. First, larger perturbations
signiﬁcantly degrade the overall performance of the adver-
sarial training and potentially creates adversarial signals
that are semantically different than the original time-series
input. Second, we want to avoid the risk of leaking label
information [13]. STN was implemented using the code
provided with the paper [25].

APPENDIX B
ADDITIONAL EXPERIMENTAL RESULTS

Results on the full UCR multivariate dataset. First, we
provide in Figures 21, 22 and 23 the experiments conducted
in the main paper on the Effectiveness of adversarial attacks
and the DTW-AR based adversarial training on all the UCR
multivariate dataset to our DTW-AR framework is general
and highly-effective for all datasets.

In Figure 21, we can observe that DTW-AR performs
lower (αEf f ≤ 0.5) for some cases. We explain below how
the other baseline attacks fail to outperform the proposed
DTW-AR method on the same datasets. In Figure 24 and
25, we show results to evaluate the effectiveness of baseline
attacks against the models shown in Figure 21. These results
show that DTW-AR is more effective in fooling DNNs
created using baseline attacks-based adversarial training.
For datasets where DTW-AR did not succeed in fooling the
deep models with a high score, Figure 24 and 25 show that
baselines fail to outperform our proposed DTW-AR attack.
We also demonstrate in Figure 23 that the baselines are

16

Fig. 21. Results for the effectiveness of adversarial examples from DTW-AR on different DNNs under white-box (WB) and black-box (BB) settings,
and using adversarial training baselines (PGD, FGS, CW and STN) on all the UCR multivariate datasets

Fig. 22. Results of adversarial training using baseline attacks and DTW-AR on all the UCR multivariate datasets, and comparison with standard
training without adversarial examples (No Attack) to classify clean data.

Fig. 23. Results of DTW-AR based adversarial training to predict the true labels of adversarial examples generated by DTW-AR and the baseline
attack methods on all the UCR multivariate datasets. The adversarial examples considered are those that successfully fooled DNNs that do not use
adversarial training.

Fig. 24. Results for the effectiveness of adversarial examples from CW on different deep models using adversarial training baselines (PGD, FGS,
CW).

17

Fig. 25. Results for the effectiveness of adversarial examples from PGD and FGS on different deep models using adversarial training baselines (PGD,
FGS, CW).

l1 Space

l∞ Space

Fig. 26. Multi-dimensional scaling results showing the labeled data
distribution in spaces using l1 as a similarity measure (left column) and
l∞ (right column) for two datasets: SC (top row) and Plane (bottom row).

tiveness of our proposed method, we additionally evaluate
DTW-AR on univariate datasets from the UCR time-series
benchmarks repository [34]. We show the results of DTW-AR
based adversarial training to predict the ground-truth labels
of adversarial attacks generated by DTW-AR and the baseline
attack methods on the univariate datasets in Figure 32. We
observe similar results as the datasets shown in the main
paper. DTW-AR is successful in identifying attacks from the
baseline methods by creating robust deep models. These
strong results show that DTW-AR outperforms baselines for
creating more effective adversarial attacks and yields to more
robust DNN classiﬁers. We conclude that the proposed DTW-
AR framework is generic, and more suitable for time-series
domain to create robust deep models.

18

Fig. 27. Results for the effectiveness of adversarial examples from DTW-AR on different deep models using adversarial training baselines (PGD,
FGS, CW) with l1-norm.

Fig. 28. Results for the effectiveness of adversarial examples from DTW-AR on different deep models using adversarial training baselines (PGD,
FGS, CW) with l∞-norm.

Fig. 29. Results of the success rate of DTW-AR adversarial trained model
to predict the true label of adversarial attack generated from [31].

Fig. 30. Results for transferability of DTW-AR attacks across an LSTM
model.

Fig. 31. Average runtime for CW and DTW-AR to create one targeted
adversarial example (run on NVIDIA Titan Xp GPU).

APPENDIX C
THEORETICAL PROOFS

C.1 Proof of Observation 1

Let l2 be the equivalent of Euclidean distance using the cost matrix
in the DTW space. ∀X ∈ Rn×T , there exists (cid:15) ∈ Rn×T and an
alignment path P such that distP (X, X + (cid:15)) ≤ δ and l2(X, X +
(cid:15)) > δ.

The existence of (cid:15) is guaranteed as follows: We know
from the nature of the DTW algorithm and the alignment
paths that for two time-series signals X and X (cid:48), the optimal
alignment path is not always the diagonal path. If (cid:15) does not
exist, it means that for all signals X (cid:48) that are different from

X, the diagonal path is an optimal alignment path, which
is absurd. Thus, (cid:15) = X (cid:48) − X and it always exists for any
time-series signal .

Let Pdiag be the diagonal alignment path in the cost

matrix C.

For X ∈ Rn×T , let (cid:15) ∈ Rn×T such that the optimal
alignment path P ∗ between X and X + (cid:15) is different than
Pdiag.

Let us suppose that there is no alignment path P between
X and X + (cid:15) such that distP (X, X + (cid:15)) ≤ δ and l2(X, X +
(cid:15)) > δ. The last statement is equivalent to: distP (X, X +(cid:15)) <
distPdiag (X, X + (cid:15)).

Since we assumed that there is no alignment path P that

satisﬁes this statement, this implies:

∀P , distP (X, X + (cid:15)) ≥ distPdiag (X, X + (cid:15))
⇒ distP ∗ (X, X + (cid:15)) ≥ distPdiag (X, X + (cid:15))
⇒ DT W (X, X + (cid:15)) ≥ distPdiag (X, X + (cid:15))

Therefore, from the deﬁnition of DT W (·, ·) as a min

operation during backtracing of the DP process, we get:

⇒ DT W (X, X + (cid:15)) = distPdiag (X, X + (cid:15))
Hence, Pdiag = P ∗, which contradicts our main assumption
in constructing (cid:15) such that Pdiag (cid:54)= P ∗.

Therefore, we conclude that:

∃P s.t. distP (X, X + (cid:15)) ≤ δ and l2(X, X + (cid:15)) > δ

C.2 Proof of Theorem 1
For a given input space Rn×T , a constrained DTW space for
adversarial examples is a strict superset of a constrained euclidean
space for adversarial examples. If X ∈ Rn×T :

(cid:26)

Xadv

(cid:12)
(cid:12)DT W (X, Xadv) ≤ δ

(cid:27)

(cid:26)

⊃

Xadv

(cid:12)
(cid:12)(cid:107)X − Xadv(cid:107)2

2 ≤ δ

(cid:27)

(11)

We want to prove that a constrained DTW space allows
more candidates adversarial examples than a constrained
Euclidean space. Let X ∈ Rn×T be an input time-series and
Xadv denote a candidate adversarial example generated from
X. In the DTW-space, this requires that DT W (X, Xadv) ≤ δ.
In the Euclidean space, this requires that (cid:107)X − Xadv(cid:107)2
2 ≤ δ,
which is equivalent to distPdiag (X, Xadv) ≤ δ.

Suppose A be the space of all candidate adversarial
examples in DTW space
and
B be the space of all candidate adversarial examples in
(cid:14)(cid:107)X − Xadv(cid:107)2
(cid:8)Xadv
Euclidean space
To prove A (cid:41) B, we need to prove:

(cid:8)Xadv/DT W (X, Xadv) ≤ δ(cid:9)

2 ≤ δ(cid:9)

.

1)
2)

∀Xadv ∈ B/Xadv ∈ A
∃Xadv/Xadv ∈ A and Xadv /∈ B

Statement 1: Let Xadv ∈ B. For the optimal alignment

path P ∗ between X and Xadv, if:

⇒ DT W (X, Xadv)

=

• P ∗

Pdiag
=
distPdiag (X, Xadv)
⇒ Xadv ∈ A

• P ∗ (cid:54)= Pdiag ⇒ According to Observation 1:
DT W (X, X + (cid:15)) < distPdiag (X, X + (cid:15))
⇒ Xadv ∈ A

Hence, we have ∀Xadv ∈ B/Xadv ∈ A.

19

Statement 2: Let Xadv ∈ A such that P ∗ (cid:54)= Pdiag. Conse-
quently, according to Observation 1, distPdiag (X, X + (cid:15)) >
DT W (X, X + (cid:15)).

⇒ distPdiag (X, X + (cid:15)) > δ.
As the diagonal path corresponds to the Euclidean

distance, we conclude that Xadv /∈ B.

Hence, ∃Xadv/Xadv ∈ A and Xadv /∈ B.

C.3 Proof of Observation 2
Given any alignment path P and two multivariate time-series
signals X, Z ∈ Rn×T . If we have distP (X, Z) ≤ δ, then
DT W (X, Z) ≤ δ.

Let P any given alignment path and P ∗ be the optimal
alignment path used for DTW measure along with the
DTW cost matrix C. Let us suppose that distP (X, Z) >
DT W (X, Z).

We

denote

P ={(1, 1), · · · , (i, j), · · · , (T, T )}

and
P ∗={(1, 1), · · · , (i∗, j∗), · · · , (T, T )}. Let us denote by k
the index at which, P and P ∗ are not using the same
cells anymore, and by l, the index where P and P ∗ meet
again using the same cells until (T, T ) in a continuous way.
By deﬁnition, k > 1 and l < min(len(P ), len(P ∗)). For
example, if P ={(1, 1), (1, 2), (2, 2), (3, 3), (3, 4), (4, 5), (5, 5)}
and P ∗={(1, 1), (1, 2), (2, 3), (3, 4), (4, 4), (5, 5)}, then k=3
and l=6.

•

•

k+1

k+1

, Zj∗

k+1,j∗

) + C(i∗

k+1) = d(Xi∗

then P =P ∗. Therefore, distP (X, Z) >

If k=l,
DT W (X, Z) is absurd.
If k (cid:54)= l: To provide the (k + 1)th element of P ∗,
we have C(i∗
k ).
k,j∗
To provide the (k + 1)th element of P , we have
C(ik+1,jk+1) = d(Xik+1, Zjk+1 ) + C(ik,jk). Using the
deﬁnition of the optimal alignment path provided in
k+1) ≤ C(ik+1,jk+1).
Equation 1, we have C(i∗
k+1,j∗
If we suppose that
the remaining elements of
P would lead to distP (X, Z) < distP ∗ (X, Z),
then this would lead to CT,T < DT W (X, Z),
which contradicts the deﬁnition of DTW. Hence,
we have distP (X, Z) ≤ distP ∗ (X, Z) implying that
distP (X, Z) > DT W (X, Z) is absurd.

Therefore, if we upper-bound distP (X, Z) by δ for any given
P , then we guarantee that DT W (X, Z) ≤ δ.

C.4 Proof of Theorem 2
For a given input space Rn×T and a random alignment path
Prand, the resulting adversarial example Xadv from the minimiza-
tion over distPrand(X, Xadv) is equivalent to minimizing over
DT W (X, Xadv). For any Xadv generated by DTW-AR using
Prand, we have:

(cid:40)

PathSim(Prand, PDT W ) = 0 &
distPrand (X, Xadv) = DT W (X, Xadv)

(12)

where PDT W is the optimal alignment path found using DTW
computation between X and Xadv.

Let Prand be the random alignment path over which the
algorithm would minimize distPrand(X, Xadv). For the ease
of notation, within this proof, we will refer to Xadv by X (cid:48).
j). As
j) ≥ 0, then minimizing distPrand(X, X (cid:48)) trans-

We have distPrand(X, X (cid:48)) = (cid:80)

∀i, j, d(Xi, X (cid:48)
lates to minimizing each d(Xi, X (cid:48)

d(Xi, X (cid:48)

(i,j)∈Prand

j).

20

j), then

Let us denote min d(Xi, X (cid:48)

min distPrand(X, X (cid:48)) = (cid:80)

j) by dmin(Xi, X (cid:48)
dmin(Xi, X (cid:48)
j).

(i,j)∈Prand
Using the back-tracing approach of DTW to de-
ﬁne the optimal alignment path, we want to verify if
PathSim(Prand, PDT W ) = 0. Let Prand be the sequence
of cells {ck,l} and PDT W be the sequence {ck(cid:48),l(cid:48)}. Every cell
{ck(cid:48),l(cid:48)} in PDT W is deﬁned to be the successor of one of
the cells {ck(cid:48)−1,l(cid:48)}, {ck(cid:48),l(cid:48)−1}, {ck(cid:48)−1,l(cid:48)−1} which will make
the distance sum along PDT W until the cell {ck(cid:48),l(cid:48)} be the
minimum distance. As we have minimized the distance over
the path Prand to be dmin(Xi, X (cid:48)
j), the cells of PDT W and
Prand will overlap. This is due to the recursive nature of
DTW computation and the fact that the last cells in both
sequences PDT W and Prand is the same (by the deﬁnition of
DTW alignment algorithm).

Hence, ∀(k, l) ∈ Prand, (k(cid:48), l(cid:48)) ∈ PDT W , we have k = k(cid:48)

and l = l(cid:48).

Therefore, the optimal alignment between between
X and Xadv will overlap with Prand and we obtain
PathSim(Prand, PDT W ) = 0.

be

C.5 Proof of Corollary 1
Let P1
and P2
such that
two
PathSim(P1, P2) > 0. If X 1
adv are the adversarial
examples generated using DTW-AR from any given time-series X
using paths P1 and P2 respectively such that DT W (X, X 1
adv) =
δ and DT W (X, X 2
adv are not
necessarily the same.

adv) = δ, then X 1

adv and X 2

alignment paths

adv and X 2

Let P1 and P2 be two alignment paths such that
PathSim(P1, P2) > 0. We want to create an adversarial
example from time-series signal X using one given alignment
path. When using P1, we will obtain Xadv,1 such that
DT W (X, Xadv,1) = δ, and when using P2, we will obtain
Xadv,2 such that DT W (X, Xadv,2) = δ.

To show that Xadv,1 and Xadv,2 are more likely to be
different, let us suppose that given P1 and P2, we always
have Xadv,1 = Xadv,2.

(i,j)∈P2

(i,j)∈P1

Again,

d(Xi, Z (cid:48)

d(Xi, Z (cid:48)

d(Xi, Zj) = (cid:80)

to simplify notations,
for

let us notate Xadv,1
by Z and Xadv,2 by Z (cid:48)
this proof. As we
have DT W (X, Z) = DT W (X, Z (cid:48)) = δ,
then
(cid:80)
j). If we suppose
(i,j)∈P2
that by construction Z is always equal to Z (cid:48), this means
that for Z (cid:54)= Z (cid:48), the statement (cid:80)
d(Xi, Zj) =
(cid:80)
j) does not hold. The last claim is clearly
incorrect. Let us suppose that Z is pre-deﬁned and we
assume Z (cid:54)= Z (cid:48). Let the ensemble of indices {k} refer
to the indices where Zk (cid:54)= Z (cid:48)
k. This means that we have
k − 1 degrees of freedom to modify Z (cid:48)
k to ﬁx the equality
(cid:80)
d(Xi, Z (cid:48)
j)).

(i,j)∈P1
Therefore, considering PathSim(P1, P2) > 0, we can
construct Xadv,1 (cid:54)= Xadv,2 such that DT W (X, Xadv,1) = δ
and DT W (X, Xadv,2).

d(Xi, Zj) = (cid:80)

(i,j)∈P2

(i,j)∈P1

21

Fig. 32. Results of DTW-AR based adversarial training to predict the true labels of adversarial examples generated by DTW-AR and the baseline
attack methods on the Univariate dataset. The adversarial examples considered are those that successfully fooled DNNs that do not use adversarial
training.

