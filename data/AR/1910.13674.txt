Efﬁcient Robust Parameter Identiﬁcation in
Generalized Kalman Smoothing Models

Jonathan Jonker∗ Peng Zheng† Aleksandr Y. Aravkin†

1

9
1
0
2

t
c
O
0
3

]

C
O
.
h
t
a
m

[

1
v
4
7
6
3
1
.
0
1
9
1
:
v
i
X
r
a

Abstract—Dynamic

inference problems

in autoregressive
(AR/ARMA/ARIMA), exponential smoothing, and navigation are
often formulated and solved using state-space models (SSM),
which allow a range of statistical distributions to inform inno-
vations and errors. In many applications the main goal is to
identify not only the hidden state, but also additional unknown
model parameters (e.g. AR coefﬁcients or unknown dynamics).
We show how to efﬁciently optimize over model parameters
in SSM that use smooth process and measurement losses. Our
approach is to project out state variables, obtaining a value
function that only depends on the parameters of interest, and
derive analytical formulas for ﬁrst and second derivatives that
can be used by many types of optimization methods.

The approach can be used with smooth robust penalties such
as Hybrid and the Student’s t, in addition to classic least squares.
We use the approach to estimate robust AR models and long-run
unemployment rates with sudden changes.

I. INTRODUCTION.

The linear state space model is widely used in tracking and
navigation [4], control [1], signal processing [2], and other
time series [6], [9]. The model assumes linear relationships
between latent states with noisy observations:

xk = Gkxk−1 + (cid:15)p
k,
zk = Hkxk + (cid:15)m
k ,

k = 1, . . . , N,

k = 1, . . . , N,

(1)

where x0 is a given initial state estimate, x1, . . . , xN are
unknown latent states with known linear process models
Gk, and z1, . . . , zN are observations obtained using known
linear models Hk. The errors (cid:15)p
k are assumed to be
mutually independent random variables with covariances Qk
and Rk. These covariances may be singular to capture standard
autoregressive structures.

k and (cid:15)m

In many applications the models Gk, Hk, Qk, Rk are spec-
iﬁed up to model parameters θ. We restrict out attention to
formulations where variances Qk, Rk are known, while Gk(θ)
and Hk(θ) are C2 mappings of θ. This captures smoothing
parameters in Holt-Winters c.f. [6], autoregressive and moving
average parameters in ARMA c.f. [9], and unknown dynamic
parameters in navigation models. In most of these models, G
and H are afﬁne functions of unknown parameters θ.

Standard models assume the errors (cid:15)p

k are Gaussian,
which gives rise to the least squares penalty in the inference
problem, see the red solid curve in Figure 1. Changing the

k and (cid:15)m

∗Department of Mathematics, University of Washington, Seattle, WA

(jonkerjo@uw.edu)

†Department of Applied Mathematics, University of Washington, Seattle,

WA (zhengp@uw.edu)

†Department of Applied Mathematics, University of Washington, Seattle,

WA (saravkin@uw.edu)

Fig. 1: Common smooth loss functions: least-squares (red
solid), Hybrid (blue dashed), and student’s T (violet dash dot).

observation model to the Hybrid (blue dash) or Student’s t
loss (violet dash dot) robustiﬁes model estimates in the face of
measurement outliers. Analogous changes to the innovations
model allows the framework to track sudden changes.

Motivating Application: Structural Unemployment Rate.
We are interested in ﬁtting parameters within structural unem-
ployment rate models, see e.g. [8]. The state vector

xk = (cid:2)uk−1 uc

k−1 uk uc
k

(cid:3)T

(2)

tracks total (u) and ‘cyclic’ (uc) unemployment using the auto-
regressive model

Gk =







0
0
−l1
0

Hk =

0
0
0
1/2 − l2
(cid:20)0
0 γ/2

0

1
0
1 + l1
0







0
1
0
l2

,

(cid:15)p
k =







0
0
(cid:15)1
k
(cid:15)2
k







,

(3)

(cid:21)

1

1
0 γ/2

,

(cid:15)m
k =

(cid:21)

.

(cid:20)(cid:15)3
k
(cid:15)4
k

(4)

Here, (cid:96)1 and (cid:96)2 are auto-regressive parameters while γ is an
unknown measurement parameter. Unemployment rates can
experience fast changes, so we need a heavy tailed model for
innovations. To solve the full problem, we must

1) Estimate states {xk} as well as parameters (cid:96)1, (cid:96)2, γ.
2) Account for the singular process covariance Q.
3) Use non-Gaussian losses (e.g. Hybrid and Student’s t)

for (cid:15)p

k to track fast rate changes.

The paper proceeds as follows. In Section III, we review
optimization formulations for state and model parameter infer-
ence using singular and nonsingular covariance models, and
introduce the value function which depends only on the model
parameters, as e.g. (cid:96)1, (cid:96)2, γ above. In Sections IV and V, we
look in detail at nonsingular and singular Kalman smoothing

 
 
 
 
 
 
models, and obtain existence results and formulas for ﬁrst and
second derivatives of the value function. Finally, in Section VI
we present use cases that show how to efﬁciently obtain
structural parameters when using general losses and singular
covariance structure.

II. NOTATIONS AND PRELIMINARIES

We ﬁrst introduce notation and key deﬁnitions.

a) Superscript and subscript: We use superscripts to
distinguish process (p) and measurement (m) model variables
and subscripts to represent partial derivatives (θ, y, r, . . .) and
the index in the Kalman model (k). When taking derivatives,
subscripts indicate the position rather than actual variable. For
example, if we denote v(θ) = f (θ, Y (θ)), then

∇v(θ) (cid:54)= fθ(θ, Y (θ)) = ∂θf (θ, y)|y=Y (θ).

b) Loss functions: We use the following loss functions:

• Least squares: (cid:96)(r) = 1

• Hybrid: (cid:96)(r; ν) = (cid:80)
i

i + ν2 − ν.

2 (cid:107)r(cid:107)2.
(cid:112)r2

• Student’s T: (cid:96)(r; ν) = (cid:80)

i ln(1 + r2

i /ν).

III. DIFFERENTIATING IMPLICIT FUNCTIONS

In this section, we introduce a general theoretical result
for calculating the derivatives for implicit functions in an
optimization context. We then specialize this general theorem
to nonsingular and singular state space models (SSM) in the
next section.

Consider a C2-smooth function, f : Rn × Rm → R, where
in SSM models we denote parameters by θ in f (θ, y) and
the states together with any auxiliary variables (such as dual
variables) by y. The appropriate stationary condition is given
by

H(θ, y) := fy(θ, y) = 0.

(5)

For any given θ, the optimal estimate y(θ) is obtained by
solving the equation H(θ, y) = 0; in particular y(θ) depends
on θ implicitly. We introduce a variant of the implicit function
theorem presented by [5] to characterize the structure of this
implicit dependence.

Theorem 1 (Implicit Functions and Derivatives). Suppose that
U ⊂ Rn and V ⊂ Rm are open, H : U × V → Rm is
continuously differentiable. If there exists θ ∈ U and y ∈
V , such that H(θ, y) = 0 and Hy(θ, y) is invertible. Then
there exists (if necessary we choose U and V to be small
neighborhood of θ and y to guarantee the existence) a C1
mapping Y : U → V satisfying Y (θ) = y, and H(θ, Y (θ)) =
0 for all θ in V .

Moreover, we have the formula

Yθ(θ) = −Hy(θ, Y (θ))−1Hθ(θ, Y (θ)).

When the function Y (θ) as above exists, we can deﬁne the

value function

2

Corollary 1 (Derivatives of the Value Function (6)). Under
the assumptions of Theorem 1, and using y to represent the y
obtained by evaluating Y (θ), we have,

vθ(θ) =fθ(θ, y)
vθθ(θ) =fθθ(θ, y) − Hθ(θ, y)(cid:62)Hy(θ, y)−1Hθ(θ, y).

(7)

These derivations are along the lines of those presented
by Bell and Burke [5] and are mainly given here for a self-
contained exposition.

We now compute analytic expressions of derivatives with
respect to model parameters for both nonsingular and singular
Kalman smoothing systems.

IV. NONSINGULAR SSM

Consider the case where the covariance matrices Qk and
k , the

Rk in SSM are nonsingular. Pre-whitening (cid:15)p
objective function of interest is given by

k and (cid:15)m

f (θ, y) =

N
(cid:88)

k=1

(cid:16)

(cid:110)

(cid:96)p
k

Q−1/2
k

(xk − Gk(θ)xk−1)

+ (cid:96)m
k

(cid:16)

R−1/2
k

(zk − Hk(θ)xk)

(cid:17)(cid:111)

(cid:17)

,

(8)

where y = x = [x1; . . . ; xN ], (cid:96)p
are the loss
function corresponding to the distributions of (cid:15)p
k and (cid:15)m
k .
Here we assume (cid:96)p
k are smooth; three key examples are
least squares, Hybrid, and Student’s t losses introduced in
Section II. Objective (8) can be written compactly as,

k and (cid:96)m
k

k, (cid:96)m

f (θ, y) = (cid:96)p (cid:16)
(cid:96)m (cid:16)

Q−1/2(G(θ)x − ζ)

R−1/2(H(θ)x − z)

(cid:17)

+
(cid:17)

,

(9)

where,

G(θ) =

H(θ) =



I

−G2(θ)









H1(θ)

0







0

I
. . .

0

H2(θ)
. . .

. . .
. . .
−GN (θ)









,

0
I



. . .
. . .
0 HN (θ)

0









Q1

Q =




. . .



R1

, R =




. . .




 ,




 ,

QN

RN

and ζ = [x0; 0; . . . ; 0] and z = [z1; . . . ; zN ].

The stationary condition in this case is given by

H(θ, y) = fy(θ, y)

= G(θ)(cid:62)Q−(cid:62)/2(cid:96)p

r(rp) + H(θ)(cid:62)R−(cid:62)/2(cid:96)m

r (rm)

(10)
where rp = Q−1/2(G(θ)y − ζ) and rm = R−1/2(H(θ)y − z).
In the least squares case, (10) is a linear equation solved by

inverting the block-tridiagonal system

v(θ) = f (θ, Y (θ)).

(6)

G(θ)(cid:62)Q−1G(θ) + H(θ)(cid:62)R−1H(θ).

Our goal is to compute ﬁrst and second derivatives of v, which
are summarized in the following corollary.

for more general smooth penalties (cid:96)p, (cid:96)m a Newton method is
needed to compute y(θ).

By Theorem 1, existence and differentiability of Y (θ) is
guaranteed by the existence of the pair (θ, y) such that,
H(θ, y) = 0 and the partial Hessian below is invertible:

Hy(θ, y) =G(θ)(cid:62)Q−(cid:62)/2(cid:96)p
H(θ)(cid:62)R−(cid:62)/2(cid:96)m

rr(rp)Q−1/2G(θ)+
rr(rm)R−1/2H(θ).

When (cid:96)p, (cid:96)m are least squares or Hybrid, Hy(θ, y) is invertible
for any (θ, y), and for every θ there exist a y such that
H(θ, y) = 0, since both penalties are strictly convex. In the
case of the Student’s t, the Hessian may fail to be positive
deﬁnite at some pairs (θ, y) [3] and there is no absolute
guarantee that the methodology will hold, as expected for a po-
tentially nonconvex formulation. Practical behavior is another
matter, and in numerical experience we see a positive deﬁnite
Hessian at the minimizer, and so the derivative formulas hold.
A safeguard can be added to any practical implementation,
that can trigger a failsafe and use a less efﬁcient method.

We now compute remaining terms in Corollary 1, assuming

for simplicity that G(θ) and H(θ) are afﬁne functions of θ.

3

Here we deﬁne the value function as a mini-max problem

using the Lagrangian:

v(θ) := max
λp,λm

min
x,rp,rm

L(θ, x, rp, rm, λp, λm).

(13)

The system of equations we are interested in is now

0 = H(θ, y) := fy(θ, y),

y := {x, rp, rm, λp, λm};

that is, the system of equations that deﬁnes a saddle point of
the Lagrangian. Explicitly, fy(θ, y) = 0 is given by

fy(θ, y) =









G(θ)(cid:62)λp + H(θ)(cid:62)λm
(cid:96)p
r(rp) − Q(cid:62)/2λp
r (rm) − R(cid:62)/2λm
(cid:96)m
G(θ)x − ζ − Q1/2rp
H(θ)x − z − R1/2rm









= 0

Y (θ) is differentiable when Hy(θ, y) = fyy(θ, y) is invertible:

rp := Q−(cid:62)/2(cid:96)p

r(rp),

rm := R−(cid:62)/2(cid:96)m

r (rm)



θ R−(cid:62)/2(cid:96)m

r (rm)

fyy(θ, y) =

fθ(θ, y) = (Gx)(cid:62)
fθθ(θ, y) = (Gx)(cid:62)
+ (Hx)(cid:62)

θ Q−(cid:62)/2(cid:96)p
θ Q−(cid:62)/2(cid:96)p
θ R−(cid:62)/2(cid:96)m
Hθ(θ, y) = (G(θ)(cid:62)rp)θ + G(θ)(cid:62)Q−(cid:62)/2(cid:96)p

r(rp) + (Hx)(cid:62)
rr(rp)Q−1/2(Gx)θ
rr(rm)R−1/2(Hx)θ

+ (H(θ)(cid:62)rm)θ + H(θ)(cid:62)R−(cid:62)/2(cid:96)m

rr(rp)Q−1/2(Gx)θ
rr(rm)R−1/2(Hx)θ

0
0
0

0
rr(rp)
(cid:96)p
0

G(θ) −Q1/2
H(θ)

0







0
0
rr(rm)
(cid:96)m
0
−R1/2

G(θ)(cid:62) H(θ)(cid:62)
−Q(cid:62)/2
0
0
0

0
−R(cid:62)/2
0
0









(14)

We now have, fully and explicitly, ﬁrst and second derivatives
of the value function v(θ) in (7) for the nonsingular case.
Though these results are straightforward, they do not appear
in any smoothing literature we are aware of in this compact
form, even for least squares losses.

We state the following theorem.

Theorem 2. Invertibility of fyy(θ, y) is equivalent to invert-
ibility of the so called Hessian of the Lagrangian

H(θ)G(θ)−1Q1/2((cid:96)p
+ R1/2((cid:96)m

rr(rm))−1R(cid:62)/2.

rr(rp))−1Q(cid:62)/2G(θ)−(cid:62)H(θ)(cid:62)

(15)

V. SINGULAR SSM

When (cid:96)p, (cid:96)m are the least squares or Hybrid penalties, (15) is
invertible if and only if

When covariances Qk and Rk are singular, we rewrite (8)
to include null-space constraints. A singular covariance matrix
precludes any errors and innovations that are not in its range.
We follow [7] in formulating this problem:

N (R) ∩ N (QG−T H (cid:62)) = {0}.

(16)

When Theorem 2 holds, we use Corollary (1) to get deriva-

tives of v(θ) in (13):

min
θ,x,rp,rm

(cid:96)p(rp) + (cid:96)m(rm)

s.t. Q1/2rp = G(θ)x − ζ,
R1/2rm = H(θ)x − z,

and introduce the Lagrangian

f (θ, y) =L(θ, x, rp, rm, λp, λm)
=(cid:96)p(rp) + (cid:96)m(rm)

(cid:68)

λp, Q1/2rp − G(θ)x + ζ

(cid:69)

(12)

(cid:68)

λm, R1/2rm − H(θ)x + z

(cid:69)

.

−

−

(11)

v(θ) =f (θ, y)

vθ(θ) =fθ(θ, y)
vθθ(θ) =fθθ(θ, y) − Hθ(θ, y)(cid:62)Hy(θ, y)−1Hθ(θ, y).

(17)

It remains only to compute fθ, fθθ, Hθ, and Hy.

fθ(θ, y) := ((cid:10)λp, G(θ)x(cid:11))θ + ((cid:10)λm, H(θ)x(cid:11))θ

When G, H are afﬁne functions of θ, we have fθθ = 0. Finally,

When Qk and Rk are invertible, we can solve for rp, rm
in (11) and reduce the problem to (8), so nonsingular systems
are a special case of (11). Formulation (11) can be solved for
a variety of loss functions (cid:96)p and (cid:96)m (see [7]).

Hθ(θ, y) = fyθ(θ, y) =









(G(θ)(cid:62)λp)θ + (H(θ)(cid:62)λm)θ
0
0
(G(θ)x)θ
(H(θ)x)θ









.

4

(19)

A. Special case: Invertible R.

A. Robust AR Fitting

The structural unemployment model in the introduction has
a singular Q but an invertible R. In such cases, the derivative
formulas can be written using only primal quantities, which
signiﬁcantly decreases the notational burdern. In particular,
using the optimality conditions we have

λm = R−(cid:62)/2(cid:96)m

r (rm)

(cid:16)

R−1/2(H(θ)y − z)

(cid:17)

λp = −G(θ)−(cid:62)H(θ)(cid:62)λm

and so we get the explicit primal-only formula for vθ(θ) by
plugging these expressions into

vθ(θ) = ((cid:10)λp, G(θ)y(cid:11))θ + ((cid:10)λm, H(θ)y(cid:11))θ.

B. Special case: Least-squares.

If (cid:96)p(·) and (cid:96)m(·) are both given by 1

2 (cid:107) · (cid:107)2, the optimality

conditions simplify substantially, and we have

An AR-1 model begins with equations
xk = c + ϕxk−1 + (cid:15)p
k
yk = Hkxk + (cid:15)m
k

k, (cid:15)m

Where (cid:15)p
k have covariances Qk, Rk respectivly, c is an
unknown constant, and ϕ is a parameter to be estimated. To
make this take the form of (1) we create an augmented state

ˆxk =

(cid:21)

(cid:20)xk
c

and use state equations

ˆxk =

yk =

(cid:20)ϕ 1
(cid:21)
1
0
(cid:20)Hk
0

ˆxk−1 + ˆ(cid:15)p
k
(cid:21)
0
0

ˆxk + ˆ(cid:15)m
k

(20)

(21)

rp = QT /2λp,

rm = RT /2λm

Then

Q1/2rp = Qλp = G(θ)x − ζ,
R1/2rm = Rλm = H(θ)x − z

0 = G(θ)(cid:62)λp + H(θ)(cid:62)λm.

Plugging these conditions back into the Lagrangian, we get
the dual objective
1
2

f (λp, λm) = −

(λm)(cid:62)Rλm − (λp)(cid:62)ζ − (λm)(cid:62)z

(λp)(cid:62)Qλp −

1
2

s.t. G(θ)(cid:62)λp + H(θ)(cid:62)λm = 0.
Using invertibility of G, we eliminate λp:

f (λm) = −

1
2

(λm)(cid:62) (cid:0)HG−1QG−T H (cid:62) + R(cid:1) λm

− (λm)(cid:62)(z − HG−1ζ).

In the least squares case, the dual solution λm is unique exactly
when the linear system

HG−1QG−T H (cid:62) + R

(18)

is invertible, and then we have

λm = (cid:0)HG−1QG−T H (cid:62) + R(cid:1)−1

(z − HG−1ζ),

a closed form solution. A simple sufﬁcient condition for the
invertibility of (18) is to have R itself invertible, as in the
special case previously discussed.

VI. NUMERICAL EXAMPLES
We now apply the results of the previous sections to analyze
two simple singular models with unknown states and parame-
ters. In Section VI-A we present a state-space formulation for
the AR-1 model, show how to robustify it to outliers in the
data, and present explicit derivatives for the value function.
We use these derivatives to design an efﬁcient solver for
both standard and robust AR models. In Section VI-B, we
apply the methods in this paper to ﬁt a structural model for
unemployment rates that can track fast changes. While the
structural unemployment model is currently used in the EU,
in this paper we only show results on simulated synthetic data
where we know ground truth, and leave any more detailed
work to future collaborations.

ˆQ0 =

(cid:20)Q0
0

(cid:21)

0
1

,

ˆQk =

(cid:20)Qk
0

(cid:21)

0
0

k > 0

ˆRk =

(cid:20)Rk
0

(cid:21)

0
0

This choice of ˆQ0 will allow us to ﬁt the constant c as
part of the state while ˆQk, k > 0 will act as equality
constraints holding it constant through all time points. In order
to compute the derivatives of the value functions we ﬁrst note
the following derivative formula in this case for any vector η.
(cid:21) (cid:20)η1
η2

(cid:16) (cid:20)ϕ 1
1
0

(cid:20)η1
0

Gi(θ)η

= ˜Dη

(cid:21) (cid:17)

(22)

=

=

(cid:16)

(cid:17)

(cid:21)

θ

Where ˜D =

θ
(cid:20)1
0

(cid:21)
0
0

. Deﬁne









0
0
˜D 0
. . .

D =









. . .
. . .
0
˜D 0

Then using the above, for the AR-1 model
(cid:16)

(cid:17)

G(θ)x

= −Dx

And similarly

(cid:16)

G(θ)T λ

(cid:17)

We now have the expressions

θ

θ

= −D(cid:62)λ

fθ(θ, y) := − (cid:10)λp, Dx(cid:11)

fθθ = 0










D(cid:62)λp
0
0
Dx
0







Hθ(θ, y) = −

When combined with the general results of section V, we get

v(θ) =f (θ, y)
vθ(θ) =fθ(θ, y) = − (cid:10)λp, Dx(cid:11)
vθθ(θ) = − Hθ(θ, y)(cid:62)Hy(θ, y)−1Hθ(θ, y).

(23)

B. Fast Tracking of Unemployment Rates

5

(cid:96)2

In this section, we apply the proposed approach to esti-
mate parameters ((cid:96)1, (cid:96)2, γ) for the structural unemployment
model (2)—(4). To test the approach, we generate ground truth
parameters and then create synthetic data in order to compare
model performance and speed using different formulations
and algorithms. The data is generated by ﬁxing parameters
to reasonable values similar to those observed in practice,
γ(cid:3) = (cid:2)0.68 1.41 −0.68(cid:3), and applying
namely at (cid:2)(cid:96)1
the unemployment rate state space model (2)—(4) to generate
the state as well as noisy observations. We then consider
three cases: nominal errors, outliers in the observations, and
jumps in the unemployment process. In the nominal cases we
use variance parameters known to the smoother. To generate
outliers we randomly select 10% of measurements and add
additional noise drawn from a N (0, 1) Gaussian distribution.
To generate large jumps we add large deviations .4, −.2 at
indices corresponding to 25 and 65. To Examples of the
generated data are in Figure 2. An example of the estimated
using this data are in Figure 3.

All algorithms are initialized at (cid:2)0 0

0(cid:3), except for LM-
Newton on T/ls in the nominal case, which is initialized at
(cid:2)0 0 0.5(cid:3), as the standard zero initialization leads to bad
results for this (nonconvex) case.

In the ﬁrst iteration the state is initialized by propagating
the initial x0 through the dynamics for all time. In subsequent
iterations the state is always initialized using the previous state
solution. In all methods, the full state at each iteration is
computed using Newton’s method to ﬁnd a saddle point of
the augmented Lagrangian

AL(y, θ) =(cid:96)p(rp) + (cid:96)m(rm)
(cid:68)

−

λp, Q1/2rp − G(θ)x + ζ

(cid:69)

−

+

+

(cid:68)

λm, R1/2rm − H(θ)x + z

(cid:69)

(24)

1
2
1
2

(cid:107)Q1/2rp − G(θ)x + ζ(cid:107)2

(cid:107)R1/2rm − H(θ)x + z(cid:107)2

using the Hessian of the Lagrangian provided in the Appendix.
The value at an optimal point of (24) is the same as at
an optimal point of (12). (24) is better conditioned which
leads to faster convergence in practice. For the outer iterations
on the parameter space we compare a Newton method, L-
BFGS, and a LM-Newton solver. The standard Newton and
L-BFGS are from a standard python library. The LM-Newton
solver is a quasi-Newton method where the Hessian is boosted
by a parameter that is updated adaptively based on model
performance. The results are summarized in Table I.

All methods work well for convex models. In the nonconvex
case, the algorithms become more sensitive. In particular when
(cid:96)p is student’s T, we have to boost (cid:96)p
rr by a constant in order

Fig. 2: Example of generated data with outliers (top) and large jumps
(bottom). Observations are shown in gray.

to make Newtons method converge. In practice this constant
must be tuned depending on the parameter ν in the student’s
T function. Convergence is therefore sensitive to the choice
of ν and boosting constant but a good rule of thumb is to
choose 1 ≤ ν ≤ 20 and boost just enough to make the Hessian
positive semideﬁnite.

VII. DISCUSSION

We presented a general approach for parameter estimation
in singular and non-singular Kalman smoothing models. In
particular we showed how to compute ﬁrst and second deriva-
tives of the value function (optimizing over state) with respect
to the hidden parameters for both singular and nonsingular
cases, which captures a wide variety of models, including the
motivating example. A simple numerical illustration shows
how the computed quantities can be used by a variety of
optimization methods. The examples also show that when
working with structural parameters, it pays off to have convex
subproblems within each iteration of the value function. While
non-convex losses such as Student’s t are always appealing
from a modeling perspective, when the problem is to ﬁnd both
the state and parameters, the resulting models are more fragile
than those that use convex losses. This observation opens the
way to future research in both theory and algorithm design.

0204060801000.00.51.01.52.02.50204060801000.00.51.01.52.02.56

Nominal
T/ls
0.143
250
5
36.5
0.137
228
18
33.9
0.0691
385
15
51.6

H/ls
0.1884
206
10
29.7
0.184
207
10
26.5
0.184
180
20
26.4

ls/ls
0.158
19
9
11.5
0.158
19
10
6.2
0.158
16
12
7.2

Outliers
ls/T
0.115
71
15
20.2
0.115
86
17
19.4
0.12
70
28
20.0

ls/H
0.142
102
13
19.3
0.141
88
12
14.0
0.142
114
19
21.0

ls/ls
1.72
14
6
6.3
1.72
19
9
6.3
1.04
32
25
14.4

||ˆθ − θ||2
Inner Iter
Outer Iter
Time
||ˆθ − θ||2
Inner Iter
Outer Iter
Time
||ˆθ − θ||2
Inner Iter
Outer Iter
Time

Newton

L-BFGS

LM-Newton

Large Jumps
T/ls
0.032
329
9
48.1
0.034
375
14
51.4
0.028
302
15
43.2

H/ls
0.099
373
14
49.5
0.099
519
15
67.5
0.099
357
21
48.1

ls/ls
0.296
21
13
13.3
0.296
32
16
10.6
0.296
24
18
11.3

TABLE I: Table of results when run on generated data. The second row indicates the loss functions that were used where ls
stands for least squares ( 1

2 ||˙||2), T stands for Students T with ν = 10, and H stands for Hybrid with (cid:15) = .7.

was supported by the Boeing Data Science Research Grant.

REFERENCES

[1] B. D. Anderson and J. B. Moore. Optimal control: linear quadratic

methods. Courier Corporation, 2007.

[2] B. D. O. Anderson and J. B. Moore. Optimal Filtering. Prentice Hall,

1979.

[3] A. Y. Aravkin, J. V. Burke, and G. Pillonetto. Robust and trend-following
student’s t kalman smoothers. SIAM Journal on Control and Optimization,
52(5):2891–2916, 2014.

[4] Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan.

Estimation with
applications to tracking and navigation. John Wiley & Sons, Inc., New
York, 2001.

[5] B. M. Bell and J. V. Burke. Algorithmic differentiation of implicit
functions and optimal values. In Advances in Automatic Differentiation,
pages 67–77. Springer, 2008.

[6] R. J. Hyndman, A. B. Koehler, R. D. Snyder, and S. Grose. A state
space framework for automatic forecasting using exponential smoothing
methods. International Journal of Forecasting, 18(3):439–454, 2002.
[7] J. Jonker, A. Aravkin, J. V. Burke, G. Pillonetto, and S. Webster. Fast
robust methods for singular state-space models. Automatica, 105:399–
405, 2019.

[8] H. N. Mocan. Structural unemployment, cyclical unemployment, and
income inequality. Review of Economics and Statistics, 81(1):122–134,
1999.

[9] R. S. Tsay. Analysis of ﬁnancial time series, volume 543. John Wiley &

Sons, 2005.

Fig. 3: Example of estimated solution run on data in Figure 2. Top is
ls/H run on data with outliers. Bottom panel shows T/ls run on data
with large jumps added. Both are computed using Newton’s method.

Acknowledgment. We are very grateful to Jon Nielsen (Eco-
nomic Council of the Labour Movement (ECLM)) for pointing
us to the use of Kalman smoothers in structural unemployment
models, and for teaching us about these models. Research of
A. Aravkin and P. Zheng was supported by the Washington
Research Foundation Data Science Professorship. J. Jonker

0204060801000.00.51.01.52.02.50204060801000.00.51.01.52.02.5a) Proof of Theorem 2: We reduce HX (θ, X ) in (14) to block upper triangular form using invertible block row operations

APPENDIX

7

rr(rp))−1R1
rr(rm))−1R2

R1 = ((cid:96)p
R2 = ((cid:96)m
R3 = G(θ)−T R3
R4 = −G(θ)−1 (cid:16)
R5 = R5 − R1/2R2 − HR4

R4 − Q1/2R1 − Q1/2((cid:96)p

rr(rp))−1QT /2R3

(cid:17)

The resulting system is given by


I
0


0


0


0

rr)−1QT /2

0 −((cid:96)p
I
0
0
0

0
I
0
0

0
0
0
I
0

0

−((cid:96)m

rr(rm))−1RT /2
−G−1H (cid:62)
rr(rp))−1QT /2G−T H (cid:62)

rr(rp))−1QT /2G−T H (cid:62)
rr(rm))−1RT /2(cid:1)

G−1Q1/2((cid:96)p
(cid:0)HG−1Q1/2((cid:96)p
+R1/2((cid:96)m










The invertibility of HX is thus equivalent to the invertibility of the symmetric positive semideﬁnite system (15).

b) Hessian of the Lagrangian (24):

ALyy(y, θ) =









G(θ)T G(θ) + H(θ)T H(θ)
−QT /2G(θ)
−RT /2H(θ)
G(θ)
H(θ)

−G(θ)T Q1/2
rr(rp) + QT /2Q1/2
(cid:96)p
0
−Q1/2
0

−H(θ)T R1/2
0
rr(rm) + RT /2R1/2
(cid:96)m
0
−R1/2

G(θ)T H(θ)T
−QT /2
0
0
0

0
−RT /2
0
0









(25)

