Draft version July 19, 2021
Typeset using LATEX default style in AASTeX63

Tracing Hα Fibrils through Bayesian Deep Learning

Haodi Jiang,1, 2 Ju Jing,1, 3, 4 Jiasheng Wang,1, 3, 4 Chang Liu,1, 3, 4 Qin Li,1, 3, 4 Yan Xu,1, 3, 4 Jason T. L. Wang,1, 2 and
Haimin Wang1, 3, 4

1Institute for Space Weather Sciences, New Jersey Institute of Technology, University Heights, Newark, NJ 07102-1982, USA;
hj78@njit.edu, wangj@njit.edu, haimin.wang@njit.edu
2Department of Computer Science, New Jersey Institute of Technology, University Heights, Newark, NJ 07102-1982, USA
3Center for Solar-Terrestrial Research, New Jersey Institute of Technology, University Heights, Newark, NJ 07102-1982, USA
4Big Bear Solar Observatory, New Jersey Institute of Technology, 40386 North Shore Lane, Big Bear City, CA 92314-9672, USA

ABSTRACT

We present a new deep learning method, dubbed FibrilNet, for tracing chromospheric ﬁbrils in Hα
images of solar observations. Our method consists of a data pre-processing component that prepares
training data from a threshold-based tool, a deep learning model implemented as a Bayesian convolu-
tional neural network for probabilistic image segmentation with uncertainty quantiﬁcation to predict
ﬁbrils, and a post-processing component containing a ﬁbril-ﬁtting algorithm to determine ﬁbril orien-
tations. The FibrilNet tool is applied to high-resolution Hα images from an active region (AR 12665)
collected by the 1.6 m Goode Solar Telescope (GST) equipped with high-order adaptive optics at the
Big Bear Solar Observatory (BBSO). We quantitatively assess the FibrilNet tool, comparing its image
segmentation algorithm and ﬁbril-ﬁtting algorithm with those employed by the threshold-based tool.
Our experimental results and major ﬁndings are summarized as follows. First, the image segmentation
results (i.e., detected ﬁbrils) of the two tools are quite similar, demonstrating the good learning capa-
bility of FibrilNet. Second, FibrilNet ﬁnds more accurate and smoother ﬁbril orientation angles than
the threshold-based tool. Third, FibrilNet is faster than the threshold-based tool and the uncertainty
maps produced by FibrilNet not only provide a quantitative way to measure the conﬁdence on each
detected ﬁbril, but also help identify ﬁbril structures that are not detected by the threshold-based
tool but are inferred through machine learning. Finally, we apply FibrilNet to full-disk Hα images
from other solar observatories and additional high-resolution Hα images collected by BBSO/GST,
demonstrating the tool’s usability in diverse datasets.

Keywords: Solar atmosphere; Solar chromosphere; Convolutional neural networks

1. INTRODUCTION

Fibrils are thin threadlike absorption features ubiquitously observed in the solar chromosphere. Depending on their
location and dynamic behavior, they may have diﬀerent names, e.g., threads of ﬁlaments (Martin 1998; Wang et al.
2000), the superpenumbra of sunspots (Loughhead 1968; Jing et al. 2019), mottles in quiet-Sun rosette structures
(Heinzel & Schmieder 1994), etc. Fibrils are often observed with narrowband solar ﬁltergrams in the chromospheric
spectral lines such as Hα, where they are denser than their surroundings (Mooroogen et al. 2017). Physically speaking,
ﬁbrils represent the cool gas “frozen” in magnetic ﬁeld lines and protected by magnetic ﬁelds from diﬀusing out
(Pikel’ner 1971; Langangen et al. 2008; Rouppe van der Voort et al. 2009). For this reason, ﬁbrils have been traditionally
assumed to be aligned with the direction of the chromospheric magnetic ﬁeld (Foukal 1971a,b).

Tracing chromospheric ﬁbrils in Hα is an important subject in heliophysics research (Jing et al. 2011; Leenaarts
et al. 2015), and has attracted much attention in the heliophysics community. The comparison between ﬁbrils and the
potential magnetic ﬁeld may provide a quick way to examine the nonpotentiality of active regions (ARs) (Jing et al.
2011). The orientation of ﬁbrils could be used as a constraint to improve the non-linear force-free modeling of coronal
ﬁelds (Wiegelmann et al. 2008; Aschwanden et al. 2016; Fleishman et al. 2019). Tracing ﬁbrils also helps estimate
the amount of energy in acoustic waves (Fossum & Carlsson 2006) and the free magnetic energy in the chromosphere
(Aschwanden et al. 2016).

1
2
0
2

l
u
J

6
1

]

R
S
.
h
p
-
o
r
t
s
a
[

1
v
6
8
8
7
0
.
7
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Jiang et al.

Many ﬁbril tracing methods have been developed in recent years. Leenaarts et al. (2015) conducted 3-dimensional
magnetohydrodynamic simulations to investigate the relation between chromospheric ﬁbrils and magnetic ﬁeld lines.
Aschwanden et al. (2016) performed nonpotential ﬁeld modeling of chromospheric structures and coronal loops with
the VCA-NLFFF code. Jafarzadeh et al. (2017) adopted the CRISPEX tool for visual inspection and identiﬁcation of
isolated slender ﬁbrils. Gafeira et al. (2017) used image processing and contrast enhancement techniques to identify
these ﬁbrils. Asensio Ramos et al. (2017) employed the rolling Hough transform (RHT) for ﬁbril detection and a
Bayesian hierarchical model to analyze the pixels in spectro-polarimetric chromospheric images of penumbrae and
ﬁbrils. The authors concluded that ﬁbrils are often well aligned with magnetic azimuth. This RHT technique has also
been used by Schad (2017) to analyze ﬁbrils and coronal rain. Jing et al. (2011) developed a threshold-based algorithm
to automatically segment chromospheric ﬁbrils from Hα observations and extracted direction information along the
ﬁbrils with a ﬁbril-ﬁtting algorithm. The authors further quantitatively measured the nonpotentiality of the ﬁbrils by
the magnetic shear angle. In contrast to the above methods, our deep learning-based tool (FibrilNet) presented here
can automatically predict ﬁbrils and measure the uncertainties in the predicted results simultaneously.

Deep learning is a branch of machine learning where neural networks are designed to learn from large amounts of
data (LeCun et al. 2015). It has been used extensively in computer vision and natural language processing, and more
recently in astronomy and astrophysics for ﬂare prediction, spectroscopic analysis, solar image segmentation, among
others (Huertas-Company et al. 2018; Leung & Bovy 2018; Kim et al. 2019; Lieu et al. 2019; Liu et al. 2019; Wu
& Boada 2019; Jiang et al. 2020). Diﬀerent from the previous solar image segmentation techniques, which focus on
predicting a value for each pixel, our FibrilNet employs a probabilistic segmentation model, speciﬁcally a Bayesian
convolutional network, that predicts a value for each pixel accompanied with reliable uncertainty quantiﬁcation. Such
a model leads to a more informed decision, and improves the quality of prediction.

In general, there are two types of uncertainty in Bayesian modeling: aleatoric uncertainty and epistemic uncer-
tainty (Kendall & Gal 2017). Aleatoric uncertainty, also known as data uncertainty, measures the noise inherent in
observations. Epistemic uncertainty, on the other hand, measures the uncertainty in the parameters of a model; this
uncertainty is also known as model uncertainty. Quantifying uncertainties with machine learning ﬁnds many applica-
tions ranging from computer vision (Kendall & Gal 2017), natural language processing (Xiao & Wang 2019), medical
image analysis (Kwon et al. 2020) to geomagnetic storm forecasting (Gruet et al. 2018; Xu et al. 2020). Here we
present a new application of uncertainty quantiﬁcation with machine learning in ﬁbril tracing.

The rest of this paper is organized as follows. Section 2 describes solar observations and data used in this study.
These data are from an active region (AR 12665) collected by the Big Bear Solar Observatory (BBSO). Section 3
presents details of our FibrilNet method and algorithms used by the method. FibrilNet employs a Bayesian convo-
lutional network for probabilistic image segmentation with uncertainty quantiﬁcation to predict ﬁbrils. It then uses
a ﬁbril-ﬁtting algorithm with a polynomial regression function of varying degrees to model the predicted ﬁbrils and
determine their orientations. Section 4 reports experimental results, showing traced ﬁbrils in the Hα images of the
solar observations in AR 12665 collected by BBSO. Furthermore, we apply FibrilNet to other types of observations,
demonstrating the tool’s usability in diverse datasets. Section 5 presents a discussion and concludes the paper.

2. OBSERVATIONS AND DATA PREPARATION

The Goode Solar Telescope (GST) is a 1.6 m clear aperture, oﬀ-axis telescope at BBSO, which is located in Big
Bear Lake, California (Cao et al. 2010; Goode et al. 2010; Goode & Cao 2012; Varsik et al. 2014). GST is equipped
with a high-order adaptive optics system, AO-308, which provides high-order correction of atmospheric seeing within
an isoplanatic patch (about 6(cid:48)(cid:48) at 500 nm in summer), with a gradual roll-oﬀ of correction at larger distances (Shumko
et al. 2014). Under a stable seeing condition, BBSO/GST observed AR 12665 at (W27◦, S4◦) on July 13, 2017, in
which the data taken during ∼20:16-22:42 UT were used in the study presented here.

The Visible Imaging Spectrometer (VIS; Cao et al. 2010) of GST utilizes a telecentric mount of the Fabry-P´erot
etalon. This imaging system was used for observing the Hα line. It scanned the target area at ±0.6, ±0.4 and 0.0
˚A (0.08 ˚A bandpass) from the Hα line center 6563 ˚A with a 70(cid:48)(cid:48) circular ﬁeld of view (FOV). At each wavelength step,
the 25 frames, out of 60 frames taken in succession, with the best contrast were saved. These frames, with exposure
time ranging from 7 to 20 ms and an image scale of 0(cid:48)(cid:48).03 per pixel, were processed by the high-order AO system and
post-facto speckle image reconstruction algorithms (W¨oger et al. 2008), which improved the quality of the images by
correcting the wavefront deformation caused by atmospheric distortion. An Hα line scan was performed over the FOV,
and the position with the minimum intensity was deﬁned as the Hα line center. It should be pointed out that the GST

Tracing Hα Fibrils through Bayesian Deep Learning

3

Figure 1. Five test images at (A) 0.0 ˚A, (B) +0.4 ˚A, (C) +0.6 ˚A, (D) −0.4 ˚A, (E) −0.6 ˚A, respectively, from the Hα line
center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV collected in AR 12665 on 2017 July 13 20:15:58 UT. Enormous amounts of ﬁbrils exist
in these Hα images.

narrowband Hα data do not contain the full spectral information, which restricts the full characterization of ﬁbrils in
three dimensions. Therefore, our study of ﬁbrils is all based on their projected morphology on the observational image
plane.

Our dataset contained the GST Hα observations in AR 12665 from 20:16:32 UT to 22:41:30 UT on July 13, 2017
where the observed region was located at (W27◦, S4◦). During this period of time, 241 Hα line center images (i.e.,
those at 0.0 ˚A from the Hα line center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV) were used as training data since features in
these images were abundant. The test set contained ﬁve Hα images taken from AR 12665 at 20:15:58 UT on the same
day (see Figure 1). Thus, there were 241 training Hα images and 5 test Hα images where the size of each image was
720 × 720 pixels. The training and test sets were disjoint, as the training observations and test observations were taken
at diﬀerent time points. Please note that the ﬁve test images were chosen in such a way that they were on ﬁve diﬀerent
wavelength positions rather than at ﬁve diﬀerent time points on the same wavelength position. The reason why we
did not choose the test images equally distributed over the time series on the same wavelength position was because
the features in the images on the same wavelength position did not change much across the images. By contrast, the
features in the images on the ﬁve diﬀerent wavelength positions appeared quite diﬀerently as shown in Figure 1.

3. METHODOLOGY

3.1. Overview of FibrilNet

Figure 2 explains how FibrilNet works. Training Hα images are pre-processed in steps 1 and 2, and then used to
train the Bayesian deep learning model (step 3). The trained model takes as input a test Hα image (step 4) and
produces as output a predicted mask accompanied with results for quantifying aleatoric uncertainty and epistemic
uncertainty (step 5). In the post-processing phase (step 6), based on the predicted mask, ﬁbrils on the test Hα image

4

Jiang et al.

Figure 2. Illustration of the proposed method (FibrilNet) for ﬁbril tracing. FibrilNet employs a Bayesian deep learning model
for probabilistic image segmentation with uncertainty quantiﬁcation to predict ﬁbrils and a ﬁbril-ﬁtting algorithm to determine
ﬁbril orientations. The training data used to train the Bayesian deep learning model are highlighted in the dashed box. The
tracing results for the test Hα image include predicted/detected ﬁbrils, their orientations, aleatoric uncertainty and epistemic
uncertainty.

are detected and highlighted by thin red curves. Furthermore, the orientations of the detected ﬁbrils are determined
based on a ﬁbril-ﬁtting algorithm where the orientations are shown by diﬀerent colors.

Speciﬁcally, in step 1, we apply the threshold-based tool developed by Jing et al. (2011) to each training Hα image
described in Section 2 to obtain a corresponding ﬁbril mask. Fibril patterns on this mask are very thick, which contains
a lot of noise. In step 2, we reﬁne the ﬁbril mask via a skeletonization procedure to obtain a ﬁbril skeleton in which
ﬁbrils are marked by black and regions without ﬁbrils are marked by white. The skeletonization procedure works by
extracting a region-based shape feature representing the general form of ﬁbrils. This skeletonization procedure results
in better and cleaner images suitable for model training (Umbaugh 2010).

The training Hα images and ﬁbril skeletons are then used to train the Bayesian deep learning model for probabilistic
image segmentation and uncertainty quantiﬁcation (step 3). During training, in order to obtain a robust model, we
use the data augmentation technique described in Jiang et al. (2020) to expand the training set by shifting, rotating,
ﬂipping and scaling the training images. In step 4, a test Hα image is fed to the trained Bayesian deep learning model.
During testing, we use the Monte Carlo (MC) dropout sampling technique described in Section 3.2 to produce the
predicted mask of the test Hα image accompanied with aleatoric uncertainty and epistemic uncertainty results (step
5). In step 6, by using the ﬁbril-ﬁtting algorithm based on the polynomial regression model described in Section 3.3,
our FibrilNet tool outputs detected ﬁbrils marked by red color on the test Hα image and their orientations represented
by diﬀerent colors.

3.2. Implementation of the Bayesian Deep Learning Model in FibrilNet

The Bayesian deep learning model used by FibrilNet is similar to the model used in SolarUnet (Jiang et al. 2020)
for tracking magnetic ﬂux elements. Both models have 4 encoder blocks (E1, E2, E3, E4), 4 decoder blocks (D1, D2,
D3, D4), mediated by a bottleneck (Bot). See Figure 3 and Jiang et al. (2020) for the conﬁguration and parameter
settings of the models. While both models are based on an encoder-decoder convolutional neural network, they diﬀer
in three ways. First, in performing 2×2 max pooling, represented by a red arrow in Figure 3, the corresponding
max pooling indices are stored. During decoding, the max pooling indices at the corresponding encoder layer are
recalled, represented by an orange arrow, to upsample, represented by a green arrow, as done in Badrinarayanan
et al. (2017). This upsampling technique used by FibrilNet, designed to reduce the number of trainable parameters in

Tracing Hα Fibrils through Bayesian Deep Learning

5

Figure 3. Architecture of the encoder-decoder convolutional neural network (i.e., the Bayesian deep learning model) used in
FibrilNet. This network is similar to the one presented in Jiang et al. (2020). See text for their diﬀerences.

the model (network) and hence save memory, is diﬀerent from the up-convolution layers used in SolarUnet. Second,
since ﬁbril patterns are relatively vague and harder to identify than magnetic ﬂux elements, FibrilNet uses twice as
many kernels as those in SolarUnet in all of the blocks in the encoder and decoder, as well as the bottleneck. Finally,
during testing, instead of using the trained model (network) directly to produce segmentation results as done in
SolarUnet, FibrilNet employs a Monte Carlo (MC) dropout sampling technique, detailed below, to produce, for a test
Hα image, a predicted mask accompanied with aleatoric uncertainty and epistemic uncertainty results (see Figure 2).
This MC dropout sampling technique allows FibrilNet to perform probabilistic image segmentation with uncertainty
quantiﬁcation, which is lacking in SolarUnet.

Speciﬁcally, to quantify uncertainty with the convolutional neural network, we use a prior probability, P (W), over
the network’s weights, W. During training, pairs of Hα images and their corresponding ﬁbril skeletons, collectively
referred to as D, are used to train the network. According to Bayes’ theorem,

P (W | D) =

P (D | W)P (W)
P (D)

.

(1)

Computing the exact posterior probability, P (W | D), is intractable (Denker & LeCun 1990). Nevertheless, we can use
variational inference (Graves 2011) to learn the variational distribution over the network’s weights parameterized by
θ, qθ(W), by minimizing the Kullback–Leibler (KL) divergence of qθ(W) and P (W | D) (Blei et al. 2017). It is known
that training a network with dropout is equivalent to a variational approximation on the network (Gal & Ghahramani
2016). Furthermore, minimizing the cross-entropy loss of the network is equivalent to the minimization of the KL
divergence (Goodfellow et al. 2016). Therefore, we use a binary cross-entropy loss function and the adaptive moment
estimation (Adam) optimizer (Goodfellow et al. 2016) with a learning rate of 0.0001 to train our model (network). Let
ˆθ denote the optimized variational parameter obtained by training the model (network); we use qˆθ(W) to represent
the optimized weight distribution.

In deep learning, dropout is mainly used to prevent over-ﬁtting, where a trained model overﬁts training data and
hence can not be generalized to make predictions on unseen test data. During training, dropout refers to ignoring or
dropping out units (i.e., neurons) of certain set of neurons which is chosen randomly. During testing, dropout can be
used to retrieve T Monte Carlo (MC) samples by processing the input test Hα image T times (Gal & Ghahramani
2016). (In the study presented here, T is set to 50.) Each time a set of weights is randomly drawn from qˆθ(W). Each
pixel in the predicted mask, shown in step 5 of Figure 2, gets a mean and variance over the T samples. If the mean is
greater than or equal to a threshold, the pixel is marked by black indicating that the pixel is part of a ﬁbril; otherwise
the pixel is marked by white indicating that the pixel is not part of a ﬁbril. (In the study presented here, the threshold
is set to 0.5.) Following Kwon et al. (2020), we decompose the variance into the aleatoric uncertainty and epistemic
uncertainty at the pixel. The aleatoric uncertainty captures the inherent randomness of the predicted result, which
comes from the input test Hα image, while the epistemic uncertainty comes from the variability of W, which accounts
for the uncertainty in the model parameters (weights).

In the post-processing phase, we use a connected-component labeling algorithm (He et al. 2009) to group all adjacent
black segments if their pixels in edges or corners touch each other. For each resulting group, which represents a ﬁbril,

6

Jiang et al.

we locate its pixels in the predicted mask and highlight their corresponding pixels in the test Hα image by red.
(Resulting groups containing less than 10 pixels are considered as noise and ﬁltered out.) We then output the detected
ﬁbrils highlighted by red color in the test Hα image, as shown in step 6 of Figure 2.

3.3. Implementation of the Fibril-Fitting Algorithm in FibrilNet

Most of the detected ﬁbrils are lines or curves. In contrast to Jing et al. (2011), which used a quadratic function to
ﬁt the detected ﬁbrils, we adopt a polynomial regression model here. Speciﬁcally, our regression model is a polynomial
function with varying degrees capable of ﬁtting the detected ﬁbrils with diﬀerent curvatures. In general, regression
analysis investigates the relationship between a dependent variable and an independent variable (Bishop 2006). We
model a detected ﬁbril as an nth degree polynomial function as follows:

y = γ0 + γ1x + γ2x2 + . . . + γnxn + (cid:15),

(2)

where γi are coeﬃcients and (cid:15) is a random error term. In Equation (2), the independent variable x represents the x
coordinate of a pixel in the detected ﬁbril and the dependent variable y represents the y coordinate of the same pixel,
where the x-axis represents the E-W direction and the y-axis represents the S-N direction (see Figure 1). When the
degree n equals 1, Equation (2) represents a linear regression model, meaning that the detected ﬁbril is represented
by a straight line. In our work, n ranges from 1 to 10.

We then use the least squares method (Ostertagov´a 2012) to ﬁnd the optimal γi values. There are 10 candidate
polynomial functions for representing the detected ﬁbril. We use the R-squared score (Ostertagov´a 2012) to assess
the feasibility of these 10 candidate polynomial functions. Speciﬁcally, we choose the candidate polynomial function
yielding the largest R-squared score, and use this polynomial function to represent the detected ﬁbril. To determine
the orientation of the detected ﬁbril, we calculate the derivative of the chosen polynomial function. For each pixel on
the detected ﬁbril, we thus obtain the slope of the tangent at the pixel, leading to the orientation angle of the pixel,
denoted θf , with respect to the x-axis. Notice that the orientation angle θf is in the 0◦ − 180◦ range, as two directions
diﬀering by 180◦ are indistinguishable here because the detected ﬁbril in Hα does not carry information on the vertical
dimension. Thus, θf represents the direction of the detected ﬁbril with a 180◦ ambiguity (Jing et al. 2011).

4.1. Tracing Results of FibrilNet Based on Data from AR 12665

4. RESULTS

In this series of experiments, we used the 241 Hα line center images from 20:16:32 UT to 22:41:30 UT on 2017 July
13 mentioned in Section 2 along with their corresponding ﬁbril skeletons to train the FibrilNet tool as described in
Section 3. We then used the trained tool to predict and trace ﬁbrils on the ﬁve test images at 0.0 ˚A, +0.4 ˚A, +0.6 ˚A,
−0.4 ˚A, −0.6 ˚A, respectively, from the Hα line center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV collected in AR 12665 on 2017
July 13 20:15:58 UT (see Figure 1). Figure 4 presents tracing results on the test image at 0.0 ˚A; tracing results on the
other four test images can be found in the Appendix. In all of the tracing results, ﬁbrils containing 10 or fewer pixels
were treated as noise and excluded.

Figure 4(A) shows the original test Hα image. Figure 4(B) shows the enlarged FOV of the region highlighted by the
white box 1 in Figure 4(A). It can be seen from Figure 4(B) that there are salt-and-pepper noise pixels in the region
highlighted by the white box 2, where the noise pixels are caused by image reconstruction limitations. Figure 4(C)
shows the ﬁbrils (red curves) on the test Hα image detected by the tool (after skeletonization) presented in Jing et al.
(2011). Figure 4(D) shows the ﬁbrils (red curves) predicted by FibrilNet. FibrilNet uses the images processed by the
tool in Jing et al. (2011) as training data. The results in Figures 4(C) and 4(D) are quite similar, demonstrating the
good learning capability of FibrilNet.

Figures 4(E) and 4(F) show the aleatoric uncertainty (data uncertainty) and epistemic uncertainty (model uncer-
tainty) maps, respectively, produced by FibrilNet. Regions predicted with less uncertainty and higher conﬁdence are
colored by blue. Regions predicted with more uncertainty and lower conﬁdence are colored by red. We can see that the
main source of uncertainty comes from the data rather than the model. Speciﬁcally, the values in the data uncertainty
map in Figure 4(E) range from 0 to 0.246 while the values in the model uncertainty map in Figure 4(F) range from 0
to 0.086. Furthermore, we observe that the ends of a detected ﬁbril are often associated with higher uncertainty. This
happens because there is ambiguity surrounding the transition from the ﬁbril body to the non-ﬁbril background area,
a ﬁnding consistent with that in object detection with uncertainty quantiﬁcation reported in the literature (Kendall
& Gal 2017; Kwon et al. 2020).

Tracing Hα Fibrils through Bayesian Deep Learning

7

Figure 4. Fibril tracing results on the test image at 0.0 ˚A from the Hα line center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV collected in
AR 12665 on 2017 July 13 20:15:58 UT where training data were 241 Hα line center images taken from the same AR between
20:16:32 UT and 22:41:30 UT on the same day. (A) The original test Hα image. (B) The enlarged FOV of the region highlighted
by the white box 1 in (A). (C) Fibrils (red curves) on the test Hα image detected by the tool in Jing et al. (2011). (D) Fibrils
(red curves) on the test Hα image predicted by FibrilNet. (E) The aleatoric uncertainty (data uncertainty) map produced by
FibrilNet. (F) The epistemic uncertainty (model uncertainty) map produced by FibrilNet.

Notice also that the map in Figure 4(E) shows higher uncertainty in the noisy region inside the white box 2 compared
to the region outside the white box 2. Speciﬁcally, in the noisy region inside the white box 2, 90% of the values are
contained in the range [0.00014 (5%), 0.20528 (95%)]. By contrast, in the region outside the white box 2, 90% of the
values are contained in the range [0 (5%), 0.18969 (95%)]. Furthermore, it is observed in Figure 4(C) that the tool in
Jing et al. (2011) misses some ﬁbril structures with at least 15 pixels inside the white box 3. These ﬁbril structures are
not present in the mask predicted by FibrilNet either, as shown inside the white box 3 in Figure 4(D). Nevertheless, the
uncertainty maps of FibrilNet are able to catch and display these missed ﬁbril structures with higher uncertainty by
Bayesian inference, as shown inside the white box 3 in Figures 4(E) and 4(F) respectively. This ﬁnding demonstrates
the usefulness of the uncertainty maps, as they not only provide a quantitative way to measure the conﬁdence on each
predicted ﬁbril, but also help identify ﬁbril structures that are not detected by the tool in Jing et al. (2011) but are
inferred through machine learning. It should be pointed out that the previous ﬁbril tracing tool in Jing et al. (2011)
does not have the capability of producing these uncertainty maps as described here.

Figure 5 compares the orientation angles of the ﬁbrils found by the tool in Jing et al. (2011) and by FibrilNet
respectively. The colors of angles between 0◦ and 90◦ range from dark blue to green. The colors of angles between 90◦
and 180◦ range from green to dark red. It can be seen from Figure 5 that the orientation angles found by the two tools

8

Jiang et al.

Figure 5. Orientation angles (colored curves) of the detected ﬁbrils on the test image at 0.0 ˚A from the Hα line center 6563
˚A with a 70(cid:48)(cid:48) circular FOV collected in AR 12665 on 2017 July 13 20:15:58 UT. (A) Fibril orientation angles calculated by
the tool in Jing et al. (2011). (B) Fibril orientation angles determined by FibrilNet. Orientation angles of a number of ﬁbrils,
some of which are highlighted by small red circles here, are calculated wrongly by the tool in Jing et al. (2011), but correctly
by FibrilNet.

mostly agree with each other, though the angles detected by FibrilNet tend to be smoother. This happens because
FibrilNet uses polynomial functions of varying degrees, as opposed to the quadratic function employed by the tool in
Jing et al. (2011), to better ﬁt the detected ﬁbrils with diﬀerent curvatures. Notice also that the quadratic function
used by the tool in Jing et al. (2011) may produce wrong angles, which are calculated correctly by the polynomial
regression model of FibrilNet. For example, the orientation angle of the ﬁbril at E-W = 425(cid:48)(cid:48) and S-N = −190(cid:48)(cid:48), which
is highlighted by a small red circle in Figure 5, is roughly 90◦. It is calculated incorrectly by the tool in Jing et al.
(2011), as shown in Figure 5(A). On the other hand, FibrilNet calculates the orientation angle of this ﬁbril correctly,
as shown in Figure 5(B). It should be pointed out that the smoother and more accurate orientation angles detected by
FibrilNet are due to the better ﬁbril-ﬁtting algorithm used by the tool, as explained above. They are not caused by
FibrilNet’s Bayesian deep learning model, whose purpose is mainly for image segmentation (i.e., marking each pixel by
black indicating the pixel is part of a ﬁbril or white indicating the pixel is not part of a ﬁbril as shown in the predicted
mask in Figure 2) with uncertainty quantiﬁcation (i.e., producing the uncertainty maps as shown in Figure 4).

4.2. Quantitative Assessment of FibrilNet Based on Data from AR 12665

As mentioned above, FibrilNet has two parts: (i) the Bayesian deep learning model for predicting ﬁbrils with
probabilistic image segmentation, and (ii) the ﬁbril-ﬁtting algorithm for determining orientations of the predicted
ﬁbrils based on the polynomial regression function in Equation (2). Here, we adopt four measures, deﬁned below,
to quantitatively assess the ﬁrst part, comparing the image segmentation algorithms employed by FibrilNet and the
tool (after skeletonization) in Jing et al. (2011), based on the same data from AR 12665 used in Section 4.1. Unlike
FibrilNet, which employs deep learning for image segmentation, the tool in Jing et al. (2011) used a threshold-based
algorithm rather than machine learning for image segmentation.

Let A (B, respectively) denote the set of 720 × 720 = 518, 400 pixels in the mask (skeleton, respectively) predicted
by FibrilNet (calculated by the tool in Jing et al. (2011), respectively) for a test image. Let p ∈ A be a pixel in A and
let q ∈ B be p’s corresponding pixel in B, i.e., q is at the same position as p. We use A ∩A B to represent the subset of
pixels in A such that for each pixel p in A ∩A B and p’s corresponding pixel q in B, p and q are marked by the same
color. That is, p, q are both marked by black indicating p (q, respectively) is part of a ﬁbril in A (B, respectively), or
p, q are both marked by white indicating p (q, respectively) is not part of a ﬁbril in A (B, respectively). Similarly, we

Tracing Hα Fibrils through Bayesian Deep Learning

9

use A ∩B B to represent the subset of pixels in B such that for each pixel q in A ∩B B and q’s corresponding pixel p
in A, q and p are marked by the same color. The ﬁrst quantitative measure is the pixel similarity (PS), also known as
global accuracy (Badrinarayanan et al. 2017), which is deﬁned as

PS =

|A ∩A B| + |A ∩B B|
|A| + |B|

,

(3)

where |.| is the cardinality of the indicated set. PS is used to assess the pixel-level similarity between the mask A
predicted by FibrilNet and the skeleton B calculated by the tool in Jing et al. (2011) for the test image. The value
of PS ranges from 0 to 1. The larger (i.e., closer to 1) the PS value, the higher the pixel-level similarity between the
mask A and the skeleton B.

Let AF (BF , respectively) denote the set of pixels on the ﬁbrils in A (B, respectively). Thus, in A, the pixels in AF
are marked by black while the pixels not in AF are marked by white. Similarly, in B, the pixels in BF are marked by
black while the pixels not in BF are marked by white. We use AF ∩AF BF to represent the subset of pixels in AF such
that for each black pixel p in AF ∩AF BF , p’s corresponding pixel q is also black, i.e., q is in BF . Similarly, we use
AF ∩BF BF to represent the subset of pixels in BF such that for each black pixel q in AF ∩BF BF , q’s corresponding
pixel p is also black, i.e., p is in AF . The second quantitative measure is the fraction of common ﬁbril pixels (FCFP),
deﬁned as

FCFP =

|AF ∩AF BF | + |AF ∩BF BF |
|AF | + |BF |

.

(4)

FCFP is used to measure the pixel-level similarity between the ﬁbrils predicted by FibrilNet and those found by the
tool in Jing et al. (2011). The value of FCFP ranges from 0 to 1. The larger (i.e., closer to 1) the FCFP value, the
higher the pixel-level similarity between the ﬁbrils predicted by FibrilNet and those found by the tool in Jing et al.
(2011).

The third quantitative measure is the fraction of disjunct ﬁbril pixels (FDFP), deﬁned as

FDFP = 1 − FCFP .

(5)

FDFP is used to measure the pixel-level dissimilarity (distance) between the ﬁbrils predicted by FibrilNet and those
found by the tool in Jing et al. (2011). The value of FDFP ranges from 0 to 1. The smaller (i.e., closer to 0) the FDFP
value, the higher the pixel-level similarity between the ﬁbrils predicted by FibrilNet and those found by the tool in
Jing et al. (2011).

The fourth quantitative measure is the Rand Index (RI; Rand 1971; Unnikrishnan et al. 2005), which calculates the
ratio of pairs of pixels whose colors (black or white) are consistent between the mask A predicted by FibrilNet and the
skeleton B calculated by the tool in Jing et al. (2011) for the test image. RI accommodates the inherent ambiguity
in image segmentation, and provides region sensitivity and compensation for coloring errors near the ends of detected
ﬁbrils. For example, consider a wider ﬁbril. FibrilNet may detect the portion to the left of the center of the ﬁbril and
highlight this portion by red. The tool in Jing et al. (2011) may detect the portion to the right of the center of the
ﬁbril and highlight that portion by red. Under this circumstance, FCFP does not consider there are common pixels
between the two red curves, though RI treats the two red curves as consistent curves. Visually the ﬁbril is indeed
found by both tools. As a consequence, RI is often used in comparing image segmentation algorithms. The value of
RI also ranges from 0 to 1. The larger (i.e., closer to 1) the RI value, the higher the visual similarity between the
ﬁbrils predicted by FibrilNet and those found by the tool in Jing et al. (2011).

Table 1 presents the quantitative measure values of FibrilNet based on the ﬁve test images in Figure 1. It can be
seen from the table that the mask predicted by FibrilNet and the skeleton calculated by the tool in Jing et al. (2011)
are very similar at pixel level, with PS ≥ 95% on the test images. The fraction of common ﬁbril pixels (FCFP) is
about 80%. However, visually, the similarity/consistency between the ﬁbrils predicted by FibrilNet and those found
by the tool in Jing et al. (2011) is much higher, where the similarity/consistency is quantitatively assessed with RI
≥ 91% on the test images. This ﬁnding is consistent with the results presented in Figures 4(C) and 4(D).

Next, we quantitatively assess the second part of FibrilNet, comparing the ﬁbril-ﬁtting algorithms employed by
FibrilNet and the tool (after skeletonization) in Jing et al. (2011), based on the same data from AR 12665 described
in Section 4.1. The ﬁbril-ﬁtting algorithms are used to determine orientations of detected ﬁbrils. Let θf represent the
ﬁbril orientation angle of a pixel calculated by the polynomial regression function in FibrilNet, and let θj represent

10

Jiang et al.

Table 1. Comparison of the Image Segmentation Algorithms Used in FibrilNet and the Tool of Jing et al. (2011) Based on
Four Quantitative Measures and Five Test Images

Test Image
Hα 0.0 ˚A
Hα +0.4 ˚A
Hα +0.6 ˚A
Hα −0.4 ˚A
Hα −0.6 ˚A

PS
0.9576
0.9571
0.9659
0.9546
0.9536

FCFP
0.8038
0.8097
0.8079
0.7922
0.8022

FDFP
0.1962
0.1903
0.1921
0.2078
0.1978

RI
0.9188
0.9178
0.9340
0.9134
0.9115

the ﬁbril orientation angle of the same pixel calculated by the quadratic function in the tool of Jing et al. (2011). The
acute angle diﬀerence between θf and θj, denoted δ(θf , θj), is deﬁned as

(cid:40)

δ(θf , θj) =

|θf − θj|
180◦ − |θf − θj| otherwise

if |θf − θj| ≤ 90◦

.

(6)

The angle diﬀerence is decided in favor of an acute or right angle, i.e., 0◦ ≤ δ(θf , θj) ≤ 90◦.

Figure 6 quantitatively compares the orientation angles of common ﬁbril pixels calculated by the ﬁbril-ﬁtting al-
gorithms used in FibrilNet and the tool of Jing et al. (2011) based on the test image at 0.0 ˚A from the Hα line
center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV collected in AR 12665 on 2017 July 13 20:15:58 UT. Figure 6(A) shows the
2D histogram of the orientation angles of common ﬁbril pixels produced by the two tools where the x-axis (y-axis,
respectively) represents the orientation angles calculated by FibrilNet (the tool of Jing et al. (2011), respectively). The
2D histogram is computed by grouping common ﬁbril pixels whose orientation angles are speciﬁed by their x and y
coordinates into bins, and counting the common ﬁbril pixels in a bin to compute the color of the tile representing the
bin. The width of each bin equals 2 degrees. It can be seen from Figure 6(A) that the orientation angles of common
ﬁbril pixels calculated by the two tools mostly agree with each other, which is consistent with the ﬁndings shown in
Figure 5. Figure 6(B) shows diﬀerences of the orientation angles of common ﬁbril pixels produced by the two tools.
It can be seen from Figure 6(B) that most of the common ﬁbril pixels have very small orientation angle diﬀerences,
displayed by purple color. For the common ﬁbril pixels with large orientation angle diﬀerences, the orientation angles
calculated by the quadratic function used in the tool of Jing et al. (2011) are often incorrect (see, for example, the
ﬁbrils highlighted by the small red circles in Figure 6(B) and Figure 5).

4.3. Application of FibrilNet to Other Data

In this series of experiments, we applied FibrilNet to other types of test images, including (i) a full-disk image from
the Global Oscillation Network Group (GONG; Harvey et al. 1996; Plowman & Berger 2020) at the National Solar
Observatory (NSO), (ii) a full-disk image from the Kanzelh¨ohe Solar Observatory (KSO; Otruba 1999; Otruba et al.
2008), (iii) high-resolution superpenumbral ﬁbrils from BBSO (Jing et al. 2019), and (iv) two high-resolution quiet
Sun regions from BBSO. The GONG full-disk Hα LH (Learmonth Reduced Hα) data in (i) was collected on 2015
September 28 00:01:34 UT. The KSO full-disk Hα Fi (Full-disk raw image) data in (ii) was collected on 2015 September
14 09:14:20 UT. The GONG and KSO full-disk images have relatively low resolution. The BBSO superpenumbra of
sunspots in (iii) was collected at Hα −0.6 ˚A from AR 12661 (501E, 95N) on 2017 June 4 19:08:44 UT. The two BBSO
quiet-Sun regions in (iv) were collected on 2018 July 29 16:33:12 UT and 2020 June 10 16:10:25 UT at Hα −0.6 ˚A from
(604E, 125S) and Hα 0.0 ˚A from (283E, 789N), respectively. The FibrilNet tool was trained using the same 241 Hα
line center images described in Section 2. Here we present results without uncertainty maps. Results with uncertainty
maps can be generated similarly as done in Section 4.1.

Figure 7 shows ﬁbrils (red curves) predicted by FibrilNet on the GONG and KSO test images. Figure 7(A) presents
the GONG full-disk Hα image. Figure 7(B) shows the enlarged view of the region highlighted by the white box in
Figure 7(A). In Figure 7(C), we see that FibrilNet detects many ﬁbrils on the GONG image. Figure 7(D) presents the
KSO full-disk Hα image. Figure 7(E) shows the enlarged view of the region highlighted by the white box in Figure
7(D). Figure 7(F) clearly demonstrates that FibrilNet detects the threads of ﬁlaments and ﬁbrils on the KSO image.
Figure 8 presents ﬁbril prediction results on the BBSO high-resolution test Hα images. Figure 8(A) shows the BBSO
superpenumbra of sunspots image used in the study. It can be seen that there are superpernumbral ﬁbrils around the
sunspot in the center of the image. Figure 8(D) shows the predicted superpenumbral ﬁbrils (red curves) produced

Tracing Hα Fibrils through Bayesian Deep Learning

11

Figure 6. Quantitative comparison of the orientation angles of common ﬁbril pixels calculated by the ﬁbril-ﬁtting algorithms
used in FibrilNet and the tool of Jing et al. (2011) based on the test image at 0.0 ˚A from the Hα line center 6563 ˚A with a
70(cid:48)(cid:48) circular FOV collected in AR 12665 on 2017 July 13 20:15:58 UT. (A) 2D histogram of the orientation angles of common
ﬁbril pixels produced by the two tools. (B) Diﬀerences of the orientation angles of common ﬁbril pixels produced by the two
tools. The orientation angles of the ﬁbrils highlighted by small red circles are calculated wrongly by the tool of Jing et al.
(2011), but correctly by FibrilNet as indicated in Figure 5.

by FibrilNet on the image in Figure 8(A). We see in Figure 8(D) that FibrilNet can distinguish the superpenumbral
ﬁbrils from the clusters of spicules nearby. Figures 8(B) and 8(C) present the two BBSO quiet-Sun regions. Figures
8(E) and 8(F) show the predicted mottles in the quiet-Sun rosette structures in Figures 8(B) and 8(C), respectively.
These high-resolution Hα images clearly demonstrate the good ﬁbril prediction capability of our tool.

5. DISCUSSION AND CONCLUSIONS

We develop a Bayesian deep learning method, FibrilNet, for tracing chromospheric ﬁbrils in Hα images of solar
observations. We apply the FibrilNet tool to high-resolution Hα images from an active region (AR 12665) collected
by BBSO/GST on July 13, 2017. The tool performs well on these high-resolution Hα images, predicting ﬁbrils with
uncertainty quantiﬁcation and determining the orientations of the predicted ﬁbrils. We further apply FibrilNet to
full-disk Hα images from other solar observatories and additional high-resolution Hα images collected by BBSO/GST,
demonstrating the tool’s usability in diverse datasets.

Our main results are summarized as follows:

1. The encoder-decoder convolutional neural network (i.e., the Bayesian deep learning model) used in
FibrilNet, as illustrated in Figure 3, is an enhancement of two deep learning models, namely U-Net (Falk
et al. 2019), based on which our SolarUnet (Jiang et al. 2020) for magnetic tracking was developed,
and SegNet (Badrinarayanan et al. 2017). FibrilNet predicts ﬁbrils on a test Hα image through image
segmentation (i.e., predicting each pixel in the test Hα image to be black indicating the pixel is part of a
ﬁbril or white indicating the pixel is not part of a ﬁbril). In computer vision and image processing, U-Net
and SegNet are two of the best image segmentation models. By combining these two models, FibrilNet
produces good image segmentation (i.e., ﬁbril prediction) results, as described in Section 4.

2. The training dataset used in this study comprises 241 high-resolution Hα line center images in AR
12665 collected by BBSO/GST from 20:16:32 UT to 22:41:30 UT on 2017 July 13. After FibrilNet is
trained on this dataset, we apply the trained model to predict ﬁbrils on ﬁve high-resolution test Hα
images from the same active region (AR 12665) collected by BBSO/GST on 2017 July 13 20:15:58 UT as
described in Section 4.1, as well as an additional ﬁve test Hα images including two full-disk Hα images from
GONG/KSO and three other high-resolution Hα images collected by BBSO/GST as described in Section

12

Jiang et al.

Figure 7. Fibrils (red curves) predicted by FibrilNet on the GONG and KSO full-disk Hα images collected on 2015 September
28 00:01:34 UT and 2015 September 14 09:14:20 UT, respectively. (A) The GONG full-disk Hα image. (B) The enlarged view
of the region highlighted by the white box in (A). (C) Fibrils predicted by FibrilNet on the image in (B). (D) The KSO full-disk
Hα image. (E) The enlarged view of the region highlighted by the white box in (D). (F) Fibrils predicted by FibrilNet on the
image in (E).

4.3. Our experimental results show that the Bayesian deep learning model employed by FibrilNet performs
well not only on the ﬁve high-resolution test Hα images from AR 12665 that are not seen during training,
but also on the additional ﬁve test Hα images. No further training is needed for FibrilNet to predict ﬁbrils
in the additional ﬁve test Hα images. This is achieved by the generalization and inference capabilities of the
deep learning model used by FibrilNet. On the other hand, the threshold-based tool in Jing et al. (2011)
is tailored for the high-resolution Hα images collected by BBSO/GST. When applying the threshold-based
tool in Jing et al. (2011) to the GONG full-disk Hα image in Figure 7, the threshold-based tool performs
poorly, missing many ﬁbrils on the GONG Hα image.

3. FibrilNet obtains training data from the threshold-based tool in Jing et al. (2011) where the training
dataset contains 241 high-resolution Hα line center images from AR 12665 collected by BBSO/GST as
described in item 2 above. When applying FibrilNet and the threshold-based tool to the ﬁve high-resolution
test Hα images from the same active region (AR 12665) collected by BBSO/GST, the two tools agree well
on the detected ﬁbrils as described in Sections 4.1 and 4.2. This demonstrates the good learning capability
of FibrilNet. When predicting ﬁbrils on a test Hα image, FibrilNet uses an uncertainty quantiﬁcation
technique (more precisely a Monte Carlo sampling technique) to process the test Hα image T times where
T = 50 as described in Section 3.2. Unlike FibrilNet, which employs deep learning, the tool in Jing et al.
(2011) used a threshold-based algorithm, rather than machine learning, for image segmentation to detect

Tracing Hα Fibrils through Bayesian Deep Learning

13

Figure 8. Fibrils (red curves) predicted by FibrilNet on additional high-resolution BBSO test Hα images. (A) The BBSO
superpenumbra of sunspots image collected at Hα −0.6 ˚A from AR 12661 (501E, 95N) on 2017 June 4 19:08:44 UT. (B) The
BBSO quiet-Sun image collected at Hα −0.6 ˚A from (604E, 125S) on 2018 July 29 16:33:12 UT. (C) The BBSO quiet-Sun
image collected at Hα 0.0 ˚A from (283E, 789N) on 2020 June 10 16:10:25 UT. (D) Fibrils predicted by FibrilNet on the image
in (A). (E) Fibrils predicted by FibrilNet on the image in (B). (F) Fibrils predicted by FibrilNet on the image in (C).

ﬁbrils on the test Hα image.
It takes several seconds for the threshold-based tool to process the test
Hα image. When the uncertainty quantiﬁcation technique is turned oﬀ (i.e., T is set to 1), FibrilNet is
ten times faster than the threshold-based tool in Jing et al. (2011) due to the fact that FibrilNet detects
ﬁbrils through making predictions, while the two tools produce similar results. When the uncertainty
quantiﬁcation technique is turned on (i.e., T is set to 50), FibrilNet is as fast as the threshold-based tool
while producing uncertainty maps that not only provide a quantitative way to measure the conﬁdence on
each detected ﬁbril, but also help identify ﬁbril structures that are not detected by the threshold-based
tool (i.e., that do not exist in the training data) but are inferred through machine learning as described in
Section 4.1. It is worth noting that the main source of uncertainty comes from the data rather than our deep
learning model. Uncertainty values are higher in the noisy regions of the test Hα image. Furthermore, the
ends of a predicted ﬁbril are often associated with higher uncertainty, due to the ambiguity surrounding the
transition from the ﬁbril body to the non-ﬁbril background area. To the best of our knowledge, FibrilNet
is the ﬁrst tool capable of predicting ﬁbrils with uncertainty quantiﬁcation.

4. We conducted additional experiments to evaluate the eﬀectiveness of the data augmentation technique
used for training FibrilNet as described in Section 3.1. Our experimental results show that, without the
data augmentation technique, the performance of FibrilNet degrades, particularly when the tool is applied
to the GONG and KSO full-disk Hα images in Figure 7. This happens because the data augmentation

14

Jiang et al.

technique can increase the generalization and inference capabilities of the Bayesian deep learning model
used by FibrilNet. Our training dataset comprises 241 Hα line center images from AR 12665 collected on
July 13, 2017 as described in Section 2. We also performed experiments where we split the training dataset
into two parts based on image quality. The ﬁrst part contained 12 Hα line center images with slightly lower
quality. The second part contained the remaining 229 Hα line center images with higher quality. Since the
ﬁrst part contained too few Hα images, we expanded it by including 12 lower-quality images from the other
four wavelength positions in AR 12665 studied here, yielding a total of 60 lower-quality Hα images. Our
experimental results show that the deep learning models trained by all 241 Hα line center images and by
the 229 higher-quality Hα line center images produce similar results. On the other hand, the performance
of the deep learning model trained by the 60 lower-quality Hα images degrades, and becomes even worse in
the absence of data augmentation, particularly when the model is applied to the GONG and KSO full-disk
Hα images.

5. To further understand the behavior of FibrilNet, we trained the tool using all 241 × 5 = 1205 high-
resolution Hα images from all ﬁve wavelength positions in AR 12665 studied here and applied the trained
tool to the same test images described in Section 4. The results obtained are similar to those presented
here, indicating our tool works equally well even with fewer training images. When the tool is trained by
a much smaller dataset such as one with less than 100 Hα line center images from AR 12665 collected on
July 13, 2017, the tool still performs well on the high-resolution test Hα images described in Section 4,
but ﬁnds fragmented ﬁlaments and ﬁbrils, rather than long, complete ﬁlaments and ﬁbrils, on the KSO
full-disk Hα image in Figure 7, even when the tool is trained by the data augmentation technique with
higher-quality training images.

6. As mentioned above, the Bayesian deep learning model in FibrilNet performs image segmentation to
predict ﬁbrils with uncertainty quantiﬁcation. On the other hand, the ﬁbril-ﬁtting algorithm in FibrilNet
uses a polynomial regression function with varying degrees to calculate the orientation angles of the pre-
dicted ﬁbrils. This polynomial regression model produces more accurate and smoother ﬁbril orientation
angles than the quadratic function used by the tool in Jing et al. (2011) as described in Sections 4.1 and
4.2. However, if we replace the polynomial regression model by the quadratic function in FibrilNet, the
two tools would produce the same orientation angles on common ﬁbril pixels detected by the tools.

We conclude that FibrilNet is an eﬀective and alternative method for ﬁbril tracing. It is expected that this tool will
be a useful utility for processing observations from diverse instruments including BBSO/GST and the new DKIST
(Daniel K. Inouye Solar Telescope).

We thank the referee and Scientiﬁc Editor for very helpful and thoughtful comments. We also thank the BBSO/GST
team for providing the data used in this study. The BBSO operation is supported by the New Jersey Institute of
Technology and U.S. NSF grant AGS-1821294. The GST operation is partly supported by the Korea Astronomy
and Space Science Institute, the Seoul National University, and the Key Laboratory of Solar Activities of the Chinese
Academy of Sciences (CAS) and the Operation, Maintenance and Upgrading Fund of CAS for Astronomical Telescopes
and Facility Instruments. This work was supported by U.S. NSF grants AGS-1927578 and AGS-1954737. C.L. and
H.W. acknowledge the support of NASA under grants 80NSSC20K1282, 80NSSC18K0673, and 80NSSC18K1705.

Facilities: Big Bear Solar Observatory, National Solar Observatory, Kanzelh¨ohe Solar Observatory.

APPENDIX

Figure A1 (Figure A2, Figure A3, Figure A4, respectively) compares ﬁbril tracing results and ﬁbril orientations
obtained by FibrilNet and the tool in Jing et al. (2011) on the test image at +0.4 ˚A (+0.6 ˚A, −0.4 ˚A, −0.6 ˚A,
respectively) from the Hα line center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV collected in AR 12665 on 2017 July 13 20:15:58
UT where training data were 241 Hα line center images taken from the same AR between 20:16:32 UT and 22:41:30
UT on the same day.

Aschwanden, M. J., Reardon, K., & Jess, D. B. 2016, ApJ,

Asensio Ramos, A., de la Cruz Rodr´ıguez, J., Mart´ınez

826, 61, doi: 10.3847/0004-637X/826/1/61

Gonz´alez, M. J., & Socas-Navarro, H. 2017, A&A, 599,
A133, doi: 10.1051/0004-6361/201629755

REFERENCES

Tracing Hα Fibrils through Bayesian Deep Learning

15

Figure A1. Fibril tracing results on the test image at +0.4 ˚A from the Hα line center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV
collected in AR 12665 on 2017 July 13 20:15:58 UT where training data were 241 Hα line center images taken from the same
AR between 20:16:32 UT and 22:41:30 UT on the same day. (A) Fibrils on the test Hα image detected by the tool in Jing
et al. (2011). (B) Fibrils on the test Hα image predicted by FibrilNet. (C) The aleatoric uncertainty (data uncertainty) map
produced by FibrilNet. (D) The epistemic uncertainty (model uncertainty) map produced by FibrilNet. (E) Fibril orientation
angles calculated by the tool in Jing et al. (2011). (F) Fibril orientation angles determined by FibrilNet. Orientation angles
of a number of ﬁbrils, some of which are highlighted by small red circles here, are calculated wrongly by the tool in Jing et al.
(2011), but correctly by FibrilNet.

16

Jiang et al.

Figure A2. Fibril tracing results on the test image at +0.6 ˚A from the Hα line center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV
collected in AR 12665 on 2017 July 13 20:15:58 UT where training data were 241 Hα line center images taken from the same
AR between 20:16:32 UT and 22:41:30 UT on the same day. (A) Fibrils on the test Hα image detected by the tool in Jing
et al. (2011). (B) Fibrils on the test Hα image predicted by FibrilNet. (C) The aleatoric uncertainty (data uncertainty) map
produced by FibrilNet. (D) The epistemic uncertainty (model uncertainty) map produced by FibrilNet. (E) Fibril orientation
angles calculated by the tool in Jing et al. (2011). (F) Fibril orientation angles determined by FibrilNet. Orientation angles
of a number of ﬁbrils, some of which are highlighted by small red circles here, are calculated wrongly by the tool in Jing et al.
(2011), but correctly by FibrilNet.

Tracing Hα Fibrils through Bayesian Deep Learning

17

Figure A3. Fibril tracing results on the test image at −0.4 ˚A from the Hα line center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV
collected in AR 12665 on 2017 July 13 20:15:58 UT where training data were 241 Hα line center images taken from the same
AR between 20:16:32 UT and 22:41:30 UT on the same day. (A) Fibrils on the test Hα image detected by the tool in Jing
et al. (2011). (B) Fibrils on the test Hα image predicted by FibrilNet. (C) The aleatoric uncertainty (data uncertainty) map
produced by FibrilNet. (D) The epistemic uncertainty (model uncertainty) map produced by FibrilNet. (E) Fibril orientation
angles calculated by the tool in Jing et al. (2011). (F) Fibril orientation angles determined by FibrilNet. Orientation angles
of a number of ﬁbrils, some of which are highlighted by small red circles here, are calculated wrongly by the tool in Jing et al.
(2011), but correctly by FibrilNet.

18

Jiang et al.

Figure A4. Fibril tracing results on the test image at −0.6 ˚A from the Hα line center 6563 ˚A with a 70(cid:48)(cid:48) circular FOV
collected in AR 12665 on 2017 July 13 20:15:58 UT where training data were 241 Hα line center images taken from the same
AR between 20:16:32 UT and 22:41:30 UT on the same day. (A) Fibrils on the test Hα image detected by the tool in Jing
et al. (2011). (B) Fibrils on the test Hα image predicted by FibrilNet. (C) The aleatoric uncertainty (data uncertainty) map
produced by FibrilNet. (D) The epistemic uncertainty (model uncertainty) map produced by FibrilNet. (E) Fibril orientation
angles calculated by the tool in Jing et al. (2011). (F) Fibril orientation angles determined by FibrilNet. Orientation angles
of a number of ﬁbrils, some of which are highlighted by small red circles here, are calculated wrongly by the tool in Jing et al.
(2011), but correctly by FibrilNet.

Tracing Hα Fibrils through Bayesian Deep Learning

19

Badrinarayanan, V., Kendall, A., & Cipolla, R. 2017, IEEE

Graves, A. 2011, in Advances in Neural Information

Transactions on Pattern Analysis and Machine

Intelligence, 39, 2481, doi: 10.1109/TPAMI.2016.2644615

Bishop, C. M. 2006, Pattern Recognition and Machine

Learning (Information Science and Statistics) (Berlin,

Heidelberg: Springer-Verlag).

https://dl.acm.org/doi/book/10.5555/1162264

Blei, D. M., Kucukelbir, A., & McAuliﬀe, J. D. 2017,

Journal of the American Statistical Association, 112, 859,

doi: 10.1080/01621459.2017.1285773

Cao, W., Gorceix, N., Coulter, R., et al. 2010,

Astronomische Nachrichten, 331, 636,

doi: 10.1002/asna.201011390

Denker, J. S., & LeCun, Y. 1990, in Proceedings of the 3rd

International Conference on Neural Information

Processing Systems, NIPS’90 (San Francisco, CA, USA:

Morgan Kaufmann Publishers Inc.), 853–859.

https://dl.acm.org/doi/10.5555/2986766.2986882

Processing Systems 24: 25th Annual Conference on
Neural Information Processing Systems 2011.
Proceedings of a meeting held 12-14 December 2011,
Granada, Spain, ed. J. Shawe-Taylor, R. S. Zemel, P. L.
Bartlett, F. C. N. Pereira, & K. Q. Weinberger,
2348–2356. http://papers.nips.cc/paper/
4329-practical-variational-inference-for-neural-networks
Gruet, M. A., Chandorkar, M., Sicard, A., & Camporeale,

E. 2018, Space Weather, 16, 1882,
doi: 10.1029/2018SW001898

Harvey, J. W., Hill, F., Hubbard, R. P., et al. 1996,

Science, 272, 1284, doi: 10.1126/science.272.5266.1284

He, L., Chao, Y., Suzuki, K., & Wu, K. 2009, Pattern
Recogn., 42, 1977, doi: 10.1016/j.patcog.2008.10.013

Heinzel, P., & Schmieder, B. 1994, A&A, 282, 939
Huertas-Company, M., Primack, J. R., Dekel, A., et al.
2018, ApJ, 858, 114, doi: 10.3847/1538-4357/aabfed
Jafarzadeh, S., Solanki, S. K., Gafeira, R., et al. 2017,

ApJS, 229, 9, doi: 10.3847/1538-4365/229/1/9

Jiang, H., Wang, J., Liu, C., et al. 2020, ApJS, 250, 5,

Falk, T., Mai, D., Bensch, R., et al. 2019, Nature Methods,

doi: 10.3847/1538-4365/aba4aa

16, 67, doi: 10.1038/s41592-018-0261-2

Jing, J., Li, Q., Liu, C., et al. 2019, ApJ, 880, 143,

Fleishman, G., Mysh’yakov, I., Stupishin, A., Loukitcheva,

doi: 10.3847/1538-4357/ab2b44

M., & Anﬁnogentov, S. 2019, ApJ, 870, 101,

doi: 10.3847/1538-4357/aaf384

Fossum, A., & Carlsson, M. 2006, ApJ, 646, 579,

doi: 10.1086/504887

Foukal, P. 1971a, SoPh, 20, 298, doi: 10.1007/BF00159759

—. 1971b, SoPh, 19, 59, doi: 10.1007/BF00148824

Gafeira, R., Lagg, A., Solanki, S. K., et al. 2017, ApJS, 229,

6, doi: 10.3847/1538-4365/229/1/6

Gal, Y., & Ghahramani, Z. 2016, in Proceedings of the 33rd

International Conference on International Conference on

Machine Learning - Volume 48, ICML’16 (JMLR.org),

1050–1059.

https://dl.acm.org/doi/10.5555/3045390.3045502

Goode, P. R., & Cao, W. 2012, in Ground-based and

Airborne Telescopes IV, ed. L. M. Stepp, R. Gilmozzi, &

Jing, J., Yuan, Y., Reardon, K., et al. 2011, ApJ, 739, 67,

doi: 10.1088/0004-637x/739/2/67

Kendall, A., & Gal, Y. 2017, in Proceedings of the 31st

International Conference on Neural Information
Processing Systems, NIPS’17 (Red Hook, NY, USA:
Curran Associates Inc.), 5580–5590.
https://dl.acm.org/doi/10.5555/3295222.3295309

Kim, T., Park, E., Lee, H., et al. 2019, Nature Astronomy,

3, 397, doi: 10.1038/s41550-019-0711-5

Kwon, Y., Won, J.-H., Kim, B. J., & Paik, M. C. 2020,
Computational Statistics and Data Analysis, 142,
106816, doi: https://doi.org/10.1016/j.csda.2019.106816
Langangen, Ø., De Pontieu, B., Carlsson, M., et al. 2008,

ApJL, 679, L167, doi: 10.1086/589442

LeCun, Y., Bengio, Y., & Hinton, G. 2015, Nature, 521,

436, doi: 10.1038/nature14539

Leenaarts, J., Carlsson, M., & Rouppe van der Voort, L.

2015, ApJ, 802, 136, doi: 10.1088/0004-637x/802/2/136

H. J. Hall, Vol. 8444, International Society for Optics and

Leung, H. W., & Bovy, J. 2018, Monthly Notices of the

Photonics (SPIE), 1–8, doi: 10.1117/12.925494

Goode, P. R., Yurchyshyn, V., Cao, W., et al. 2010, ApJL,

714, L31, doi: 10.1088/2041-8205/714/1/L31

Goodfellow, I., Bengio, Y., & Courville, A. 2016, Deep

Learning (MIT Press).

http://www.deeplearningbook.org

Royal Astronomical Society, 483, 3255,
doi: 10.1093/mnras/sty3217

Lieu, M., Conversi, L., Altieri, B., & Carry, B. 2019,

Monthly Notices of the Royal Astronomical Society, 485,
5831, doi: 10.1093/mnras/stz761

Liu, H., Liu, C., Wang, J. T. L., & Wang, H. 2019, ApJ,

877, 121, doi: 10.3847/1538-4357/ab1b3c

20

Jiang et al.

Loughhead, R. E. 1968, SoPh, 5, 489,

Shumko, S., Gorceix, N., Choi, S., et al. 2014, in Society of

doi: 10.1007/BF00147015

Martin, S. F. 1998, SoPh, 182, 107,

doi: 10.1023/A:1005026814076

Mooroogen, K., Morton, R. J., & Henriques, V. 2017, A&A,

607, A46, doi: 10.1051/0004-6361/201730926

Ostertagov´a, E. 2012, Procedia Engineering, 48, 500 ,

doi: https://doi.org/10.1016/j.proeng.2012.09.545

Otruba, W. 1999, in Astronomical Society of the Paciﬁc

Conference Series, Vol. 184, Third Advances in Solar

Physics Euroconference: Magnetic Fields and

Oscillations, ed. B. Schmieder, A. Hofmann, & J. Staude,

314–318.

https://ui.adsabs.harvard.edu/abs/1999ASPC..184..314O

Otruba, W., Freislich, H., & Hanslmeier, A. 2008, Central

European Astrophysical Bulletin, 32, 1.

https://ui.adsabs.harvard.edu/abs/2008CEAB...32....1O

Pikel’ner, S. B. 1971, SoPh, 20, 286,

doi: 10.1007/BF00159757

Plowman, J. E., & Berger, T. E. 2020, Solar Physics, 295,

143, doi: 10.1007/s11207-020-01682-4

Rand, W. M. 1971, Journal of the American Statistical

Association, 66, 846.

http://www.jstor.org/stable/2284239

Rouppe van der Voort, L., Leenaarts, J., de Pontieu, B.,

Carlsson, M., & Vissers, G. 2009, ApJ, 705, 272,

doi: 10.1088/0004-637X/705/1/272

Schad, T. 2017, SoPh, 292, 132,

doi: 10.1007/s11207-017-1153-9

Photo-Optical Instrumentation Engineers (SPIE)
Conference Series, Vol. 9148, Adaptive Optics Systems
IV, ed. E. Marchetti, L. M. Close, & J.-P. Vran, 914835,
doi: 10.1117/12.2056731

Umbaugh, S. E. 2010, Digital Image Processing and

Analysis: Human and Computer Vision Applications
with CVIPtools, Second Edition, 2nd edn. (Boca Raton,
FL, USA: CRC Press, Inc.).
https://dl.acm.org/citation.cfm?id=1951634

Unnikrishnan, R., Pantofaru, C., & Hebert, M. 2005, in

2005 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR’05) - Workshops,
34–34, doi: 10.1109/CVPR.2005.390

Varsik, J., Plymate, C., Goode, P., et al. 2014, in

Proc. SPIE, Vol. 9147, Ground-based and Airborne
Instrumentation for Astronomy V, 91475D,
doi: 10.1117/12.2056688

Wang, J., Li, W., Denker, C., et al. 2000, The

Astrophysical Journal, 530, 1071, doi: 10.1086/308377

Wiegelmann, T., Thalmann, J. K., Schrijver, C. J., De
Rosa, M. L., & Metcalf, T. R. 2008, SoPh, 247, 249,
doi: 10.1007/s11207-008-9130-y

W¨oger, F., von der L¨uhe, O., & Reardon, K. 2008, A&A,

488, 375, doi: 10.1051/0004-6361:200809894

Wu, J. F., & Boada, S. 2019, Monthly Notices of the Royal

Astronomical Society, 484, 4683,
doi: 10.1093/mnras/stz333

Xiao, Y., & Wang, W. Y. 2019, in The Thirty-Third AAAI
Conference on Artiﬁcial Intelligence, AAAI 2019, The
Thirty-First Innovative Applications of Artiﬁcial
Intelligence Conference, IAAI 2019, The Ninth AAAI
Symposium on Educational Advances in Artiﬁcial
Intelligence, EAAI 2019, Honolulu, Hawaii, USA,
January 27 - February 1, 2019 (AAAI Press), 7322–7329,
doi: 10.1609/aaai.v33i01.33017322

Xu, S. B., Huang, S. Y., Yuan, Z. G., Deng, X. H., & Jiang,
K. 2020, ApJS, 248, 14, doi: 10.3847/1538-4365/ab880e

