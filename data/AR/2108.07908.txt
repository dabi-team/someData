m-ar-K-FastICA: Reliable Feature Extraction in scikit-learn

m-ar-K-Fast Independent Component Analysis

Luca Parisi
Coventry, United Kingdom
PhD in Machine Learning for Clinical Decision Support Systems
MBA Candidate with Artiﬁcial Intelligence Specialism

luca.parisi@ieee.org

Abstract
This study presents the m-arcsinh Kernel (’m-ar-K’) Fast Independent Component Anal-
ysis (’FastICA’) method (’m-ar-K-FastICA’) for feature extraction. The kernel trick has
enabled dimensionality reduction techniques to capture a higher extent of non-linearity
in the data; however, reproducible, open-source kernels to aid with feature extraction are
still limited and may not be reliable when projecting features from entropic data. The
m-ar-K function, freely available in Python and compatible with its open-source library
’scikit-learn’, is hereby coupled with FastICA to achieve more reliable feature extraction in
presence of a high extent of randomness in the data, reducing the need for pre-whitening.
Diﬀerent classiﬁcation tasks were considered, as related to ﬁve (N = 5) open access datasets
of various degrees of information entropy, available from scikit-learn and the University
California Irvine (UCI) Machine Learning repository. Experimental results demonstrate
improvements in the classiﬁcation performance brought by the proposed feature extrac-
tion. The novel m-ar-K-FastICA dimensionality reduction approach is compared to the
’FastICA’ gold standard method, supporting its higher reliability and computational eﬃ-
ciency, regardless of the underlying uncertainty in the data.

Keywords: m-arcsinh, Kernel, m-ar-K-FastICA, Independent Component Analysis, ICA,
FastICA, scikit-learn

1
2
0
2

g
u
A
7
1

]

G
L
.
s
c
[

1
v
8
0
9
7
0
.
8
0
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
Luca Parisi, PhD, MBA Candidate

1. Introduction

Despite theoretical advances in the Independent Component Analysis (ICA) feature ex-
traction method, suitable for dimensionality reduction in presence of non-Gaussian/non-
normal data distributions, such as the FastICA (Hyv¨arinen and Oja, 2000) (Javidi et al.,
2011), the Kernel ICA (KICA) (Bach and Jordan, 2002), the Fast Kernel-based ICA (Fast-
KICA) (Shen et al., 2009), and the Adaptive Mixture ICA (AMICA) (Hsu et al., 2018),
usable, explainable, reproducible and replicable variations of the ICA method that im-
prove feature extraction are still limited. In fact, the open-source Python library named
’scikit-learn’ (Pedregosa et al., 2011) leverages the FastICA algorithm as the only ICA-
related variation available, which uses the logcosh as the G function to approximate to
neg-entropy (Nguyen et al., 2018).

However, such variants of ICA, even with appropriate data pre-whitening (James and Hesse,

2004) (Hsu et al., 2015), may not be reliable when performing dimensionality reduction of
entropic data (McKeown et al., 2003) (Li and Adali, 2010), characterised by a high extent
of randomness or uncertainty. When applied prior to classiﬁcation, they can lead to slow or
lack of convergence (Vert and Vert, 2006) (Jacot et al., 2018), due to trapping at local min-
ima (Parisi, 2014a) (Parisi and Manaog, 2016) (Parisi and Manaog, 2017a) (Parisi and Manaog,
2017b). Furthermore, despite recent advancements in kernel and activation functions across
both shallow and deep learning (Parisi et al., 2020a) (Parisi et al., 2021c) (Parisi et al.,
2021b) (Parisi et al., 2021a), it is still challenging to derive a kernel function that can
aid feature extraction methods to generalise in presence of non-Gaussian and entropic
data (Parisi et al., 2018a) (Parisi et al., 2020b) (Parisi and RaviChandran, 2020).

To address these challenges, over the past two decades, there has been an increasing
trend in the Artiﬁcial Intelligence (AI) community to generate deep models that incor-
porate feature extraction within their architectures, starting from the Convolutional Neu-
ral Network (CNN) (LeCun et al., 1995) to deep CNN-like variations (Krizhevsky et al.,
2012) (Simonyan and Zisserman, 2014) (He et al., 2016), and, more recently, even deeper
models, such as Transformer-type of neural networks
(Vaswani et al., 2017). Neverthe-
less, this school of thought attempts to handle entropy in the data intrinsically, by training
deep models that are capable of automated feature extraction on expensive hardware, e.g.,
Graphics and Tensor Processing Units.

This trendy reliance on deep models and costly hardware has transformed AI into an
elitist ﬁeld, albeit with a very few exceptions (Chen et al., 2019) (Liu et al., 2021), as it
demands expensive computing resources for predictive analytics. Furthermore, it has jeop-
ardised the explainability of classiﬁcations (Gaur et al., 2021), as features are extracted and
transformed within deep model architectures, which, per se, are already not straightforward
to describe to end users. In fact, these limitations have hindered a wide and transparent
application of deep learning models in regulated industries, such as ﬁnance and health-
care (Samek and M¨uller, 2019).

2

m-ar-K-FastICA: Reliable Feature Extraction in scikit-learn

To contribute to the democratisation of AI (Garvey, 2018) (Kobayashi et al., 2019) (Ahmed and Wahed,

2020), thus enabling resource-constrained environments to leverage extrinsic and explain-
able feature extraction methods for AI to have a wider positive impact in society, in this
study, a novel kernel-based dimensionality reduction method is proposed to reduce the un-
certainty in the data conveyed to improve classiﬁcation performance. Written in Python
and made freely available in ’scikit-learn’ (Pedregosa et al., 2011) for the ’FastICA’ class,
the proposed feature extraction method is presented as a more reliable approach than the
gold standard FastICA method. By virtue of its liberal license, this kernel-based dimen-
sionality reduction method is broadly disseminated within the free software Python library
’scikit-learn’ (Pedregosa et al., 2011), and it can be used for both academic research and
commercial purposes.

3

Luca Parisi, PhD, MBA Candidate

2. Methods

2.1 Datasets leveraged from scikit-learn and the UCI ML repository

In this study, the following datasets from The University California Irvine (UCI) ML repos-
itory were used:

• ‘Parkinson’s’ dataset (Little et al., 2007), which has 23 features corresponding to 195
biomedical voice measurements from 31 people, 23 with Parkinson’s disease (PD), to
help in detecting PD from speech signals.

• ‘Haberman survival’ dataset (Lim, 1999), with three features (age of patient at time
of operation; patient’s year of operation; number of positive axillary nodes detected)
to predict whether 306 patients who had undergone surgery for breast cancer would
have died within 5 years of follow up or survived for longer.

• ‘Heart failure clinical records’ dataset (Chicco and Jurman, 2020), to predict whether
a patient was deceased during the follow-up period, based on 13 clinical features from
medical records of 299 patients who had heart failure.

• ‘SPECTF’ dataset (Cios et al., 2001), which has 267 images collected via a cardiac
Single Proton Emission Computed Tomography (SPECT), describing whether each
patient has a physiological or pathophysiological heart based on 44 features, as per
the following two data splits or partitions:

– training data partition (’SPECTF.train’ ﬁle), with 80 images.

– testing data partition (’SPECTF.test’ ﬁle), which has 187 images.

Moreover, the following dataset from scikit-learn was leveraged for a further evaluation:

• ’Breast cancer Wisconsin (diagnostic)’ dataset (Wolberg et al., 1995), having 30 char-
acteristics of cell nuclei from 569 digitised images of a ﬁne needle aspirate of breast
masses, to detect whether they correspond to either malignant or benign breast cancer.

2.2 Baseline MLP model, activation functions, and hyperparameters

The purpose of this study is not to devise the most optimised, best-performing classi-
ﬁer for any of the classiﬁcation tasks involved in section 2.1, but to develop a novel re-
liable kernel-based feature extraction method. Thus, whilst applying early stopping to
avoid overﬁtting, a baseline Multi-Layer Perceptron (MLP) (Rumelhart et al., 1986) (Parisi,
2014b) (Parisi et al., 2015) (Parisi et al., 2018b) (Parisi, 2019) model was leveraged using
various benchmark activation functions (m-arcsinh (Parisi, 2020) (Parisi et al., 2021d), iden-
tity, tanh (Jacot et al., 2018), and ReLU) and with the following hyperparameters for all
classiﬁcation tasks in section 2.1:

4

m-ar-K-FastICA: Reliable Feature Extraction in scikit-learn

• MLP-related hyperparameters:

– ’random state’ = 1.

– ’max iter’ = 250, where ’max iter’ is the maximum number of iterations.

Listing 1 provides the snippet of code in Python to use an MLP with diﬀerent activa-
tion functions available in ’scikit-learn’ (Pedregosa et al., 2011), including the ’m-arcsinh’,
following feature extraction.

Listing 1: MLP

with

diﬀerent

activation

functions

available

in

’scikit-

learn’ (Pedregosa et al., 2011), including the ’m-arcsinh’.

from s k l e a r n . n e u r a l n e t w o r k import M L P C l a s s i f i e r

f o r a c t i v a t i o n in ( ’m−a r c s i n h ’ ,

’ i d e n t i t y ’ ,

’ tanh ’ ,

’ r e l u ’ ) :

c l a s s i f i e r = M L P C l a s s i f i e r ( a c t i v a t i o n =a c t i v a t i o n ,
r a n d o m s t a t e =1 , m a x i t e r =1000)

Except for the ‘SPECTF’ dataset (Cios et al., 2001), which is already provided in two
separate partitions for training and testing (see section 2.1), the other four datasets were
split into 80% for training and 20% for testing via ’train test split’ in ’scikit-learn’ (Pedregosa et al.,
2011) from ’sklearn.model selection’, without randomisation (’shuﬄe’=False) to enable re-
producibility of the results presented in this study.

2.3 m-arK-FastICA: A novel reliable feature extraction approach

Coupling FastICA with the modiﬁed (’m-’) inverse hyperbolic sine function (’arcsinh’) or
’m-arcsinh’ function that was found to generalise as a kernel and activation for optimal sepa-
rating hyperplane- and neural network-based classiﬁers (Parisi, 2020) (Parisi et al., 2021d),
the proposed feature extraction method seeks to 1) handle the non-linearity in the data fur-
ther via an improved transformation in the G function that is leveraged in the approximation
to neg-entropy, 2) cope with entropic data by coupling the m-arcsinh Kernel (’m-ar-K’) with
FastICA, thus achieving the hybrid method ’m-ar-K-FastICA’, and 3) improve classiﬁcation
of non-linearly separable and entropic input data into target classes by fulﬁlling points 1)
and 2).

Via a weighted interaction eﬀect between the arcsinh and the slightly non-linear char-
acteristic of the squared root function, the m-ar-K (Parisi, 2020) (Parisi et al., 2021d) can
be represented as follows:

y = arcsinh(x)× 1

3 × 1

4 ×p|x| = arcsinh(x)× 1

12 ×p|x|

(1)

5

Luca Parisi, PhD, MBA Candidate

0.1

5 · 10−

2

0

−5 · 10−

2

−0.1

−1

−0.5

0

0.5

1

The derivative of m-ar-K (Parisi, 2020) (Parisi et al., 2021d) is described as follows:

arcsinh(x)
3
2

x

×
24

×|

|

(2)

dy
dx = p|x|×

1
√x2+1

+ x

12

×

(2)

0.1

8 · 10−

2

6 · 10−

2

4 · 10−

2

2 · 10−

2

0
−1

−0.5

0

0.5

1

To enable reproducibility of the results obtained in this study, considering that the
number of components ( n components) were set based on the dimensionality and the level
of entropy of the data, the following parameters were used in both the FastICA and the
proposed m-ar-K-FastICA method to extract features from:

• All datasets:

– ’random state’ = 42.

6

m-ar-K-FastICA: Reliable Feature Extraction in scikit-learn

• The ‘Haberman survival’ (Lim, 1999) and the ‘Heart failure clinical records’ datasets (Chicco and Jurman,

2020):

– ’n components’ = 2.

• The ‘Parkinson’s’ dataset (Little et al., 2007):

– ’n components’ = 8.

• The ‘Breast cancer Wisconsin (diagnostic)’ dataset (Wolberg et al., 1995):

– ’n components’ = 16.

• The ‘SPECTF’ dataset (Cios et al., 2001):

– ’n components’ = 43.

Listing 2 provides the snippet of code in Python that implements the proposed m-arcsinh
Kernel (m-ar-K) (1) and its derivative (2) coupled in the ’FastICA’ class as a G function
leveraged to approximate to neg-entropy in ’scikit-learn’ (Pedregosa et al., 2011).

Listing 2: Coupling the m-arcsinh kernel (’m-ar-K’) and its derivative as a G function in

the approximation to neg-entropy in ’scikit-learn’ (Pedregosa et al., 2011).

import numpy a s np

def m a r c s i n h ( x ,

f u n a r g s ) :

””” Compute t h e m−a r c s i n h k e r n e l
i n p l a c e .

f u n c t i o n and i t s d e r i v a t i v e

e x p l o i t s

I t
t h e d e r i v a t i v e
o u t p u t v a l u e from t h e m−a r c s i n h k e r n e l .

t h e f a c t

t h a t

i s a s i m p l e f u n c t i o n o f

t h e

Parameters
−−−−−−−−−−
x : { array −l i k e ,

s p a r s e m a t r i x } , s h a p e ( n s a m p l e s , n f e a t u r e s )

The i n p u t d a t a .

Returns
−−−−−−−
A t u p l e c o n t a i n i n g t h e v a l u e o f
”””

t h e f u n c t i o n , and t h a t o f

i t s d e r i v a t i v e .

return ( 1 / 3 ∗ np . a r c s i n h ( x ) ) ∗ ( 1 / 4 ∗ np . s q r t ( np . abs ( x ) ) ) ,

( np . s q r t ( np . abs ( x ) ) / ( 1 2 ∗ np . s q r t ( x ∗∗2+1))
+ ( x∗np . a r c s i n h ( x ) ) / ( 2 4 ∗ np . abs ( x ) ∗ ∗ ( 3 / 2 ) ) ) . mean ( a x i s =−1)

7

Luca Parisi, PhD, MBA Candidate

# Usi ng t h e m−a r c s i n h k e r n e l as a G f u n c t i o n ( ’ f u n ’ )
# f o r
# f o r

f e a t u r e
r e p r o d u c i b i l i t y i n t h e example b e l o w )

e x t r a c t i o n ( u s i n g 16 components and f i x e d random s t a t e

s e t

t o 42

i n s i d e t h e c l a s s

’ FastICA ’

i f

s e l f . fun == ’ l o g c o s h ’ :
g = l o g c o s h

e l i f

s e l f . fun == ’ exp ’ :

g = e x p

e l i f

s e l f . fun == ’ cube ’ :

g = c u b e

e l i f

s e l f . fun == ’ m a r c s i n h ’ :

g = m a r c s i n h

e l i f c a l l a b l e ( s e l f . fun ) :

def g ( x ,

f u n a r g s ) :

return s e l f . fun ( x , ∗∗ f u n a r g s )

e l s e :

ex c = V a l u e E r r o r
r a i s e ex c (

i f

i s i n s t a n c e ( s e l f . fun , s t r ) e l s e TypeError

”Unknown f u n c t i o n %r ; ”
” s h o u l d be one o f
% s e l f . fun

’ l o g c o s h ’ ,

)

’ exp ’ ,

’ cube ’ ,

’ m a r c s i n h ’ , o r c a l l a b l e ”

from s k l e a r n . d e c o m p o s i t i o n import FastICA
t r a n s f o r m e r = FastICA ( n components=16 , r a n d o m s t a t e =42 ,
t r a n s f o r m e d i n p u t s = t r a n s f o r m e r . f i t t r a n s f o r m ( i n p u t s )

fun= ’ m a r c s i n h ’ )

2.4 Performance evaluation

Further to feature extraction as per section 2.3, the accuracy of the MLP classiﬁer in sec-
tion 2.2 using diﬀerent activation functions (m-arcsinh (Parisi, 2020) (Parisi et al., 2021d),
identity, tanh (Jacot et al., 2018), and ReLU) on the datasets described in section 2.1, was
evaluated via the ’accuracy score’ available in ’scikit-learn’ (Pedregosa et al., 2011) from
’sklearn.metrics’. The reliability of such classiﬁers was assessed via the weighted average of
the precision, recall and F1-score computed via the ’classiﬁcation report’, also available in
’scikit-learn’ (Pedregosa et al., 2011) from ’sklearn.metrics’.
To understand what classiﬁcation accuracy and reliability are, and how they can be evalu-
ated, please refer to the following studies: (Parisi et al., 2018a), (Parisi et al., 2018b), (Parisi et al.,
2020b), (Parisi and RaviChandran, 2020).

Moreover, the computational cost of the classiﬁer, to quantify the impact of using the
proposed feature extraction method (m-ar-K-FastICA) as opposed to the current gold stan-
dard one (FastICA), was assessed via the training time in seconds. Experiments were run
on an AMD E2-9000 Radeon R2 processor, 1.8 GHz and 4 GB DDR4 RAM.

8

m-ar-K-FastICA: Reliable Feature Extraction in scikit-learn

3. Results

Experimental results demonstrate the overall higher reliability achieved via the proposed m-
ar-K-FastICA kernel-based dimensionality reduction method for MLP-based classiﬁcation,
including a shorter training time, as follows:

• Higher reliability on 2 of 5 datasets evaluated (Tables 1 and 3).

• The same classiﬁcation performance on 3 of 5 datasets assessed (Tables 2, 4, and 5).

• Shorter training time for 3 of 5 datasets evaluated (Tables 2-4).

Table 1 Results on performance evaluation of baseline (non-optimised) Multi-Layer
Perceptron (MLP) in scikit-learn with diﬀerent kernel functions, to compare the impact of
using either the gold standard FastICA feature extraction algorithm or the proposed m-ar-
K-FastICA method. The performance of this classiﬁer was assessed on the ‘Breast cancer
Wisconsin (diagnostic)’ dataset (Wolberg et al., 1995) available in scikit-learn.

Training time

Accuracy

Precision

Recall

F1-score

Classiﬁer Kernel

Feature extraction

MLP
MLP
MLP
MLP
MLP
MLP
MLP
MLP

m-arcsinh FastICA
FastICA
Identity
FastICA
tanh
ReLU
FastICA
m-arcsinh m-ar-K-FastICA
m-ar-K-FastICA
Identity
m-ar-K-FastICA
tanh
m-ar-K-FastICA
ReLU

(s)
4.71
1.64
2.97
1.87
5.47
1.12
3.40
2.00

(0-1)
0.95
0.95
0.97
0.95
0.95
0.96
0.97
0.96

(0-1)
0.95
0.95
0.97
0.95
0.95
0.97
0.97
0.97

(0-1)
0.95
0.95
0.97
0.95
0.95
0.96
0.97
0.96

(0-1)
0.95
0.95
0.97
0.95
0.95
0.96
0.97
0.96

Table 2. Results on performance evaluation of baseline (non-optimised) Multi-Layer
Perceptron (MLP) in scikit-learn with diﬀerent kernel functions, to compare the impact of
using either the gold standard FastICA feature extraction algorithm or the proposed m-ar-
K-FastICA method. The performance of this classiﬁer was assessed on the ‘Heart failure
clinical records’ dataset (Chicco and Jurman, 2020) available in the University California
Irvine (UCI) Machine Learning repository.

Training time

Accuracy

Precision

Recall

F1-score

Classiﬁer Kernel

Feature extraction

MLP
MLP
MLP
MLP
MLP
MLP
MLP
MLP

m-arcsinh FastICA
FastICA
Identity
FastICA
tanh
ReLU
FastICA
m-arcsinh m-ar-K-FastICA
m-ar-K-FastICA
Identity
m-ar-K-FastICA
tanh
m-ar-K-FastICA
ReLU

(s)
0.29
0.26
0.34
0.30
0.29
0.47
0.34
0.38

(0-1)
0.78
0.78
0.78
0.78
0.78
0.78
0.78
0.78

(0-1)
0.61
0.61
0.61
0.61
0.61
0.61
0.61
0.61

(0-1)
0.78
0.78
0.78
0.78
0.78
0.78
0.78
0.78

(0-1)
0.69
0.69
0.69
0.69
0.69
0.69
0.69
0.69

9

Luca Parisi, PhD, MBA Candidate

Table 3. Results on performance evaluation of baseline (non-optimised) Multi-Layer
Perceptron (MLP) in scikit-learn with diﬀerent kernel functions, to compare the impact
of using either the gold standard FastICA feature extraction algorithm or the proposed
m-ar-K-FastICA method. The performance of this classiﬁer was assessed on the ‘Haberman
survival’ dataset (Lim, 1999) available in the University California Irvine (UCI) Machine
Learning repository.

Training time

Accuracy

Precision

Recall

F1-score

Classiﬁer Kernel

Feature extraction

MLP
MLP
MLP
MLP
MLP
MLP
MLP
MLP

m-arcsinh FastICA
FastICA
Identity
FastICA
tanh
ReLU
FastICA
m-arcsinh m-ar-K-FastICA
m-ar-K-FastICA
Identity
m-ar-K-FastICA
tanh
m-ar-K-FastICA
ReLU

(s)
0.93
0.95
0.96
1.04
0.80
1.10
0.92
1.00

(0-1)
0.82
0.82
0.82
0.82
0.82
0.82
0.82
0.82

(0-1)
0.68
0.68
0.68
0.68
0.78
0.78
0.78
0.78

(0-1)
0.82
0.82
0.82
0.82
0.82
0.82
0.82
0.82

(0-1)
0.74
0.74
0.74
0.74
0.79
0.79
0.79
0.79

Table 4. Results on performance evaluation of baseline (non-optimised) Multi-Layer
Perceptron (MLP) in scikit-learn with diﬀerent kernel functions, to compare the impact of
using either the gold standard FastICA feature extraction algorithm or the proposed m-ar-
K-FastICA method. The performance of this classiﬁer was assessed on the ‘Parkinson’s’
dataset (Little et al., 2007) available in the University California Irvine (UCI) Machine
Learning repository.

Training time

Accuracy

Precision

Recall

F1-score

Classiﬁer Kernel

Feature extraction

MLP
MLP
MLP
MLP
MLP
MLP
MLP
MLP

m-arcsinh FastICA
FastICA
Identity
FastICA
tanh
FastICA
ReLU
m-arcsinh m-ar-K-FastICA
m-ar-K-FastICA
Identity
m-ar-K-FastICA
tanh
m-ar-K-FastICA
ReLU

(s)
0.80
0.78
0.80
0.68
0.67
0.62
0.63
0.70

(0-1)
0.77
0.77
0.77
0.77
0.77
0.77
0.77
0.77

(0-1)
0.59
0.59
0.59
0.59
0.59
0.59
0.59
0.59

(0-1)
0.77
0.77
0.77
0.77
0.77
0.77
0.77
0.77

(0-1)
0.67
0.67
0.67
0.67
0.67
0.67
0.67
0.67

10

m-ar-K-FastICA: Reliable Feature Extraction in scikit-learn

Table 5. Results on performance evaluation of baseline (non-optimised) Multi-Layer
Perceptron (MLP) in scikit-learn with diﬀerent kernel functions, to compare the impact
of using either the gold standard FastICA feature extraction algorithm or the proposed
m-ar-K-FastICA method. The performance of this classiﬁer was assessed on the ‘SPECTF’
dataset (Cios et al., 2001) available in the University California Irvine (UCI) Machine Learn-
ing repository.

Training time

Accuracy

Precision

Recall

F1-score

Classiﬁer Kernel

Feature extraction

MLP
MLP
MLP
MLP
MLP
MLP
MLP
MLP

m-arcsinh FastICA
FastICA
Identity
FastICA
tanh
ReLU
FastICA
m-arcsinh m-ar-K-FastICA
m-ar-K-FastICA
Identity
m-ar-K-FastICA
tanh
m-ar-K-FastICA
ReLU

(s)
0.53
0.60
0.62
0.65
0.68
0.53
0.65
0.81

(0-1)
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

(0-1)
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

(0-1)
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

(0-1)
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

11

Luca Parisi, PhD, MBA Candidate

4. Discussion

As demonstrated by the highest reliability achieved via the proposed feature extraction
method ’m-ar-K-FastICA’, quantiﬁed in section 2.4, and achieved on 2 datasets assessed
(Tables 1 and 3), whilst the same reliability was obtained on the other 3 datasets evalu-
ated, it yielded the best classiﬁcation performance when considering the same classiﬁer as
opposed to the current gold standard FastICA method. For the dataset considered in Table
1, when using the MLP with the identity and ReLU activation functions, leveraging the
novel m-ar-K-FastICA method led to 1% improvement in classiﬁcation accuracy, recall/sen-
sitivity, and F1-score, whilst the precision increased by 2%. When considering the dataset
in Table 3, precision and F1-score increased by 10% and 5% respectively, using the MLP
classiﬁer for all activation functions used (m-arcsinh, identity, tanh, ReLU).

Thus, overall, the m-ar-K-FastICA algorithm led to less false positives and increased re-
liability of the predictions. These improvements in classiﬁcation performance achieved via
the proposed dimensionality reduction method is important to consider for medical applica-
tions to reduce psychosocial consequences due to inaccurate screenings (Rasmussen et al.,
2020), such as those to aid early diagnosis of various tumours, including lung, prostate, and
breast cancers.

Besides its higher reliability, leveraging the proposed m-ar-K-FastICA feature extrac-
tion method led to reductions in training time for 3 of 5 datasets evaluated, i.e., 14.38%,
3.27%, and 1.55% shorter training times for datasets in Tables 4, 2, and 3 respectively.
Thus, also considering the computational eﬃciency brought by the novel dimensionality
reduction method m-ar-K-FastICA, it shows potential to be applied in resource-constrained
environments for maximising its positive impact.

Therefore, the m-ar-K-FastICA demonstrates that the m-arcsinh kernel is not only a
generalisable function for classiﬁcation, but also when coupled with feature extraction meth-
ods, such as FastICA, to improve the reliability and computational eﬃciency of classiﬁers.
As a reliable and computationally eﬃcient approach, the m-ar-K-FastICA is thus deemed
a new gold standard dimensionality reduction method, freely available in scikit-learn.

12

m-ar-K-FastICA: Reliable Feature Extraction in scikit-learn

5. Conclusion

The m-ar-K-FastICA feature extraction algorithm in scikit-learn provides a new method
to achieve dimensionality reduction, even in presence of entropic data. In this study, the
proposed approach was shown to lead to higher reliability and computationally eﬃciency in
supervised learning, with regards to MLP-aided classiﬁcation. As it is made freely available,
open source, on the Python and scikit-learn ecosystems, it adds to the choices that both
academia and industry can have when selecting a dimensionality reduction method, also in
resource-constrained environments.

Importantly, the proposed m-ar-K-FastICA method was found faster and more reliable
for feature extraction than the current gold standard FastICA method. Written in a high-
level programming language (Python), it can be used within ML-based pipelines in speciﬁc
use cases, wherein high precision and reliability need to be obtained, whilst expensive com-
putational hardware may not be available, such as to aid screenings of tumours, including
prostate, lung, and breast cancers. Future work involves extending the coupling of the
m-arcsinh kernel to beneﬁt other dimensionality reduction methods and provide further
choices that the AI community worldwide, including in resource-constrained environments,
can have when selecting an extrinsic feature extraction algorithm to aid predictive analytics.

Acknowledgments

This research did not receive any speciﬁc grant from funding agencies in the public, com-
mercial, or not-for-proﬁt sectors.

13

Luca Parisi, PhD, MBA Candidate

References

N. Ahmed and M. Wahed. The de-democratization of ai: Deep learning and the compute

divide in artiﬁcial intelligence research. arXiv preprint arXiv:2010.15581, 2020.

F. R. Bach and M. I. Jordan. Kernel independent component analysis. Journal of Machine

Learning Research, 3(Jul):1–48, 2002.

B. Chen, T. Medini, J. Farwell, S. Gobriel, C. Tai, and A. Shrivastava. Slide: In defense of
smart algorithms over hardware acceleration for large-scale deep learning systems. arXiv
preprint arXiv:1903.03129, 2019.

D. Chicco and G. Jurman. Machine learning can predict survival of patients with heart
failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and
Decision Making, 20(1):16, 2020.

K. J. Cios, L. A. Kurgan, and L. S. Goodenday. Spectf heart data set - UCI machine learning
repository, 2001. URL https://archive.ics.uci.edu/ml/datasets/SPECTF+Heart.

C. Garvey. A framework for evaluating barriers to the democratization of artiﬁcial intelli-

gence. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.

M. Gaur, K. Faldu, and A. Sheth. Semantics of the black-box: Can knowledge graphs
IEEE Internet

help make deep learning systems more interpretable and explainable?
Computing, 25(1):51–59, 2021.

K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
770–778, 2016.

S.-H. Hsu, T. R. Mullen, T.-P. Jung, and G. Cauwenberghs. Real-time adaptive eeg source
separation using online recursive independent component analysis. IEEE Transactions
on Neural Systems and Rehabilitation Engineering, 24(3):309–319, 2015.

S.-H. Hsu, L. Pion-Tonachini, J. Palmer, M. Miyakoshi, S. Makeig, and T.-P. Jung. Model-
ing brain dynamic state changes with adaptive mixture independent component analysis.
NeuroImage, 183:47–61, 2018.

A. Hyv¨arinen and E. Oja. Independent component analysis: algorithms and applications.

Neural Networks, 13(4-5):411–430, 2000.

A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In Advances in Neural Information Processing Systems, pages 8571–
8580, 2018.

C. J. James and C. W. Hesse.

Independent component analysis for biomedical signals.

Physiological Measurement, 26(1):R15, 2004.

S. Javidi, C. C. Took, and D. P. Mandic. Fast independent component analysis algorithm
for quaternion valued signals. IEEE Transactions on Neural Networks, 22(12):1967–1978,
2011.

14

m-ar-K-FastICA: Reliable Feature Extraction in scikit-learn

Y. Kobayashi, M. Ishibashi, and H. Kobayashi. How will “democratization of artiﬁcial
intelligence” change the future of radiologists? Japanese Journal of Radiology, 37(1):
9–14, 2019.

A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in Neural Information Processing Systems, 25:
1097–1105, 2012.

Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series.

The Handbook of Brain Theory and Neural Networks, 3361(10):1995, 1995.

X.-L. Li and T. Adali. Complex independent component analysis by entropy bound min-
imization. IEEE Transactions on Circuits and Systems I: Regular Papers, 57(7):1417–
1430, 2010.

T.-S. Lim. Haberman’s survival data set - UCI machine learning repository, 1999. URL

https://archive.ics.uci.edu/ml/datasets/Haberman’s+Survival.

M. A. Little, P. E. McSharry, S. J. Roberts, D. A. E. Costello, and I. M. Moroz. Exploiting
nonlinear recurrence and fractal scaling properties for voice disorder detection. Biomedical
Engineering Online, 6(1):23, 2007.

S. Liu, D. C. Mocanu, A. R. R. Matavalam, Y. Pei, and M. Pechenizkiy. Sparse evolutionary
deep learning with over one million artiﬁcial neurons on commodity hardware. Neural
Computing and Applications, 33(7):2589–2604, 2021.

M. J. McKeown, L. K. Hansen, and T. J. Sejnowsk. Independent component analysis of
functional mri: what is signal and what is noise? Current Opinion in Neurobiology, 13
(5):620–629, 2003.

H. M. Nguyen, G. Kalra, T. J. Jun, and D. Kim. A novel echo state network model
In International

using bayesian ridge regression and independent component analysis.
Conference on Artiﬁcial Neural Networks, pages 24–34. Springer, 2018.

L. Parisi. Exploiting kinetic and kinematic data to plot cyclograms for managing the
International Journal of

rehabilitation process of bkas by applying neural networks.
Medical, Health, Pharmaceutical and Biomedical Engineering, 2014a.

L. Parisi. Neural networks for distinguishing the performance of two hip joint implants on
the basis of hip implant side and ground reaction force. International Journal of Medical,
Health, Pharmaceutical and Biomedical Engineering, 2014b.

L. Parisi. Machine Learning-based Feature Selection and Optimisation for Clinical Decision
Support Systems. Optimal Data-driven Feature Selection Methods for Binary and Multi-
class Classiﬁcation Problems: Towards a Minimum Viable Solution for Predicting Early
Diagnosis and Prognosis. PhD thesis, University of Bradford, 2019.

L. Parisi. m-arcsinh: An eﬃcient and reliable function for svm and mlp in scikit-learn.

arXiv preprint arXiv:2009.07530, 2020.

15

Luca Parisi, PhD, MBA Candidate

L. Parisi and M. L. Manaog. Preliminary validation of the lagrangian support vector ma-
chine learning classiﬁer as clinical decision-making support tool to aid prediction of prog-
In The 16th International Conference on Biomedical
nosis in patients with hepatitis.
Engineering, National University of Singapore (NUS), 2016.

L. Parisi and M. L. Manaog. The importance of selecting appropriate k-fold cross-validation
and training algorithms in improving postoperative discharge decision-making via arti-
ﬁcial intelligence. In 2017 AUT mathematical Sciences Symposium, volume 1, page 16,
2017a.

L. Parisi and M. L. Manaog. A minimum viable machine learning-based speech processing
solution for facilitating early diagnosis of parkinson’s disease. In MATLAB Conference
2017, Auckland, New Zealand, 2017b.

L. Parisi and N. RaviChandran. Evolutionary feature transformation to improve prognostic

prediction of hepatitis. Knowledge-Based Systems, 200:106012, 2020.

L. Parisi, P. R. Biggs, G. M. Whatling, and C. A. Holt. A novel comparison of artiﬁcial intel-
ligence methods for diagnosing knee osteoarthritis. In XXV Congress of the International
Society of Biomechanics, 2015.

L. Parisi, N. RaviChandran, and M. L. Manaog. Decision support system to improve
postoperative discharge: A novel multi-class classiﬁcation approach. Knowledge-Based
Systems, 152:1–10, 2018a.

L. Parisi, N. RaviChandran, and M. L. Manaog. Feature-driven machine learning to improve
early diagnosis of parkinson’s disease. Expert Systems with Applications, 110:182–190,
2018b.

L. Parisi, D. Neagu, R. Ma, and F. Campean. Qrelu and m-qrelu: Two novel quantum
activation functions to aid medical diagnostics. arXiv preprint arXiv:2010.08031, 2020a.

L. Parisi, N. RaviChandran, and M. L. Manaog. A novel hybrid algorithm for aiding
prediction of prognosis in patients with hepatitis. Neural Computing and Applications,
32(8):3839–3852, 2020b.

L. Parisi, R. Ma, N. RaviChandran, and M. Lanzillotta. hyper-sinh: An accurate and
reliable function from shallow to deep learning in tensorﬂow and keras. Machine Learning
with Applications, page 100112, 2021a.

L. Parisi, R. Ma, N. RaviChandran, and M. Lanzillotta. Machine learning with applications.

Machine Learning with Applications, 2021b.

L. Parisi, R. Ma, A. Zaernia, and M. Youseﬃ. Hyper-sinh-convolutional neural network
for early detection of parkinson’s disease from spiral drawings. WSEAS Transactions on
Computer Research, 2021c.

L. Parisi, R. Ma, A. Zaernia, and M. Youseﬃ. m-ark-support vector machine for early
detection of parkinson’s disease from speech signals. International Journal of Mathematics
and Computers in Simulation, 2021d.

16

m-ar-K-FastICA: Reliable Feature Extraction in scikit-learn

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011.

J. F. Rasmussen, V. Siersma, J. Malmqvist, and J. Brodersen. Psychosocial consequences
of false positives in the danish lung cancer ct screening trial: a nested matched cohort
study. BMJ Open, 10(6):e034682, 2020.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-

propagating errors. Nature, 323(6088):533–536, 1986.

W. Samek and K.-R. M¨uller. Towards explainable artiﬁcial intelligence. In Explainable AI:

Interpreting, explaining and visualizing deep learning, pages 5–22. Springer, 2019.

H. Shen, S. Jegelka, and A. Gretton. Fast kernel-based independent component analysis.

IEEE Transactions on Signal Processing, 57(9):3498–3511, 2009.

K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556, 2014.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, Aidan N. G.,  L. Kaiser, and
I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing
Systems, pages 5998–6008, 2017.

R. Vert and J.-P. Vert. Consistency and convergence rates of one-class svms and related

algorithms. Journal of Machine Learning Research, 7(May):817–854, 2006.

W. H. Wolberg, W. N. Street, and O. L. Mangasarian.

consin (diagnostic) data set
https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic).

- UCI machine learning repository, 1995.

Breast cancer wis-
URL

17

