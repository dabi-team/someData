0
2
0
2

l
u
J

6
2

]
E
M

.
t
a
t
s
[

1
v
2
8
2
3
0
.
8
0
0
2
:
v
i
X
r
a

ORIGINAL RESEARCH ARTICLE

Empirical Likelihood Estimation for Linear Regression Models with
AR(p) Error Terms

S¸enay ¨Ozdemir a, Ye¸sim G¨uney b, Yetkin Tua¸cb and Olcay Arslanb
aDepartment of Statistics, Afyon Kocatepe University, 03200, Afyonkarahisar, Turkey ;
bDepartment of Statistics, Ankara University, 06100, Ankara, Turkey

ARTICLE HISTORY
Compiled August 10, 2020

ABSTRACT
Linear regression models are useful statistical tools to analyze data sets in several
diﬀerent ﬁelds. There are several methods to estimate the parameters of a linear
regression model. These methods usually perform under normally distributed and
uncorrelated errors with zero mean and constant variance. However, for some data
sets error terms may not satisfy these or some of these assumptions. If error terms are
correlated, such as the regression models with autoregressive (AR(p)) error terms,
the Conditional Maximum Likelihood (CML) under normality assumption or the
Least Square (LS) methods are often used to estimate the parameters of interest.
For CML estimation a distributional assumption on error terms is needed to carry
on estimation, but, in practice, such distributional assumptions on error terms may
not be plausible. Therefore, in such cases some alternative distribution free methods
are needed to conduct the parameter estimation. In this paper, we propose to esti-
mate the parameters of a linear regression model with AR(p) error term using the
Empirical Likelihood (EL) method, which is one of the distribution free estimation
methods. A small simulation study and a numerical example are provided to eval-
uate the performance of the proposed estimation method over the CML method.
The results of simulation study show that the proposed estimators based on EL
method are remarkably better than the estimators obtained from the CML method
in terms of mean squared errors (MSE) and bias in almost all the simulation conﬁg-
urations. These ﬁndings are also conﬁrmed by the results of the numerical and real
data examples.

KEYWORDS
AR(p) error terms; dependent error; empirical likelihood; linear regression

1. Introduction

Consider the following linear regression model

yt = xT

t β + εt

for

t = 1, 2, . . . , N

(1)

where yt are the t − th response variable, xt ∈ RM are the design vector, β ∈ RM is
the unknown M-dimensional parameter vector and εt are the uncorrelated errors with
E(εt) = 0 and V ar(εt) = σ2.

CONTACT S¸. ¨Ozdemir. Email: senayozdemir@aku.edu.tr

 
 
 
 
 
 
It is known that the LS estimators of the regression parameters are obtained by
minimizing the sum of the squared residuals or solving the following estimating equa-
tion

1
N

N
(cid:88)

t=1

(cid:0)yt − xT

t β(cid:1) xt = 0.

(2)

The LS estimators are the minimum variance unbiased estimators of β if εt are nor-
mally distributed. However, in real data applications the normality assumption may
not be completely satisﬁed. If the normally distributed error term is not a reasonable
assumption for a regression model, some alternative distributions can be used as the
error distribution and the maximum likelihood estimation method can be applied to
estimate the regression and other model parameters. On the other hand, if an error
distribution is not be easy to specify some alternative distribution free estimation
methods should be preferred to obtain estimators for the parameters of interest. One
of such methods is the EL method introduced by Owen [14, 15]. A noticeable advan-
tage of EL method is that it creates likelihood-type inference without specifying any
distributional model for the data. The EL method supposes that there are unknown
πt, t = 1, 2, . . . , N , probability weights for each observation and it tries to estimate
these probability weights by maximizing an EL function deﬁned as the production of
πts under some constraints related with πts, and the regression parameters. The EL
method can be mathematically deﬁned as follows. Maximize the following EL function

under the constraints

L(β) =

N
(cid:89)

t=1

πt

N
(cid:88)

t=1

πt = 1

N
(cid:88)

t=1

πtxt

(cid:0)yt − xT

t β(cid:1) = 0.

(3)

(4)

(5)

Note that the constraint given in equation (5) is similar to the estimating equation
given in equation (2). The only diﬀerence is that in equation (2) we use the equal
known weight (1/n) for each observation, but in equation (5) we use the unknown
probability weights πts by considering that each observation has diﬀerent contribution
to the estimation procedure. Further, if we also want to estimate the error variance
along with the regression parameters we add the following constraint to the constraints
given in equations (4)-(5).

N
(cid:88)

t=1

πt

(cid:2)(yt − xT

t β)2 − σ2(cid:3) = 0.

2

(6)

Since the EL method can be used for estimating parameters, constructing conﬁdence
regions and testing statistical hypothesis, it is a useful tool for making statistical in-
ference when it is not too easy to assign a distribution to data. There are several
remarkable studies on the EL method after it was ﬁrst introduced by Owen [14–16].
Some of these papers can be summarized as follows. Hall and La Scala[10] studied
on main features of the EL, Kolaczyk [11] adapted it in generalized linear regression
model,Qin and Lawless [23] combined general estimating equations and the EL, Chen
et al. [5–9] considered this method for constructing conﬁdence regions and parame-
ter estimation with additional constraints,Newey and Smith[13] studied higher-order
properties generalized methods of moments and generalized empirical likelihood es-
timators, Shi and Lau [22] considered for robustifying the constraint in equation (5)
using median constraint, and Bondell and Stefanski [4] suggested a robust estimator
by maximizing a generalized EL function instead of the EL function given in equation
(3). Recently, ¨Ozdemir and Arslan [20] have considered using constraints based on
robust M estimation in EL estimation method. Also, ¨Ozdemir and Arslan [19] have
proposed an alternative algorithm to compute EL estimators.

The estimation of πts and hence the model parameters β and σ2 can be done
by maximizing the EL function given in equation (3) under the constraints (4)-(6).
In general, Lagrange multipliers method can be used for these types of constrained
optimization problems. For our problem the Lagrange function will be as follows

L(π, β, λ0, λT

1 , λ2) =

N
(cid:88)

t=1

log(nπt) + λ0

(cid:33)

πt − 1

(cid:32) N
(cid:88)

t=1

N
(cid:88)

+ λT
1

πtxt

(cid:0)yt − xT

t β(cid:1)

t=1
N
(cid:88)

πt

t=1

+ λ2

(cid:2)(yt − xT

t β)2 − σ2(cid:3)

where π = [π1, π2, . . . , πN ]T , λ0, λ2 ∈ R1 and λT
1 ∈ RM are Lagrange multipliers.
Taking the derivatives of Lagrange function with respect to each πt and setting to
zero we get

πt =

N + λT

1 xt

(cid:0)yt − xT

1
t β(cid:1) + λ2

(cid:2)(yt − xT

t β)2 − σ2(cid:3) .

(7)

Substituting πt given in equation (7) in the EL function and constrains, the opti-
mization problem is reduced to the problem of ﬁnding β, σ and Lagrange multipliers.
However, this problem is still not easy to handle to obtain the estimators for β and σ.
The solution of this problem are considered by several author using several diﬀerent
approaches. For details of the algorithms they are suggesting, one can see the papers
[14–18, 20, 27].

It should be noted that in all of the mentioned papers researchers consider the EL
estimation method to estimate the parameters of a regression model with uncorrelated
error terms. However, in practice, uncorrelated error assumption may not be plausible
for some data sets. For these data sets regression analysis should be carried on with
an autoregressive error terms regression model (regression model with AR(p) error

3

terms). An autoregressive error terms regression model is deﬁned as follows.

yt =

M
(cid:88)

i=1

xt,iβi + et, t = 1, 2, . . . , N,

(8)

where yt is the response variable, xt,i is the predictor variable, βi is the unknown
regression parameter and et is the AR(p) error term with

et = φ1et−1 + · · · + φpet−p + at.

(9)

Here φj, j = 1, 2, 3, ..., p are the unknown autoregressive parameters and at, t =
1, 2, 3, ..., N are i.i.d. random variables with E(at) = 0 and V ar(at) = σ2. Note that
this regression equation is diﬀerent from the regression equation given in (1). How-
ever, using the back shift operator B, this equation can be transformed to the usual
regression equation as follows. Let

at = Φ(B)et = et − φ1et−1 − · · · − φpet−p,

Φ(B)yt = yt − φ1yt−1 − · · · − φpyt−p,

(10)

and

Φ(B)xt = xt − φ1xt−1,i − · · · − φpxt−p,i.

(11)

Then, the regression model given in (8) can be rewritten as

Φ(B)yt =

M
(cid:88)

i=1

βiΦ(B)xt,i + at, t = 1, 2, . . . , N.

(12)

In literature, parameters of an autoregressive error term regression model are es-
timated using LS, ML or CML estimation methods. Some of the related papers are
[1], [3], [24], [25] and [26]. In all of the mentioned papers some known distributions,
such as normal or t, are assumed as the error distribution to carry on estimation of
the parameters of interest in this model. However, since imposing appropriate distri-
butional assumptions on the error term of a regression model may not be easy some
other distribution free estimation methods may be preferred to carry on regression
analysis of a data set. In this study, unlike the papers in literature, we will not assume
any distribution for the error terms and propose to use the EL estimation method
to estimate the parameters of the linear regression model described in the previous
paragraph.

The rest of the paper is organized as follows. In Section 2, the CML and the EL
estimation methods for the linear models with AR(p) error terms are given. In Section

4

3, A small simulation study and a numerical and a real data examples are provided to
assess the performance of the EL based estimators over the estimators obtained from
the classical CML method. Finally, we draw some conclusions in Section 4.

2. Parameter Estimation for Linear Regression Models with AR(p) Error

Terms

In this section, we describe in detail how the EL method is used to estimate the
parameters of an autoregressive error term regression model. We will ﬁrst recall the
CML method under normality assumption of at. Note that since the exact likelihood
function could be well approximated by the conditional likelihood function [2] CML
estimation method are often used in cases where ML estimation method is not feasible
to carry on.

2.1. Conditional Maximum Likelihood Estimation

In general, a system of nonlinear equations of the parameters have to be solved to
obtain ML estimators. However, since in most of the cases ML estimators cannot be
analytically obtained numerical procedures should be used to get the estimates for the
parameters of interest. An alternative way for numerical maximization of the exact
likelihood function is to regard the value of the ﬁrst p observations as known and to
maximize the likelihood function conditioned on the ﬁrst observations. In this part,
the CML estimation method will be considered for the regression model given in (8).
Let the error terms at in the regression model given in equation (12) have normal
distribution with zero mean and σ2 variance. Then, the conditional log-likelihood
function will be as

ln L = c −

N − p
2

ln σ2 −

1
2σ2

N
(cid:88)

t=p+1

(cid:32)

Φ (B) yt −

M
(cid:88)

i=1

(cid:33)2

βiΦ (B) xt,i

(13)

[1]. Taking the derivatives of the conditional log-likelihood function with respect to
the unknown parameters, setting them to zero and rearranging the resulting equations
yields the following estimating equations for the unknown parameters of the regression
model under consideration

(cid:104)

(cid:101)β =

(cid:101)Φ (B) X T (cid:101)Φ (B) X

(cid:105)−1 (cid:104)

(cid:101)Φ (B) X T (cid:101)Φ (B) Y

(cid:105)

(cid:101)σ2 =

(cid:104)

1
N − p

(cid:101)Φ (B) Y − (cid:101)Φ (B) X T (cid:101)β

(cid:105)T (cid:104)

(cid:101)Φ (B) Y − (cid:101)Φ (B) X T (cid:101)β

(cid:105)

(cid:101)Φ = R−1((cid:101)β)R0((cid:101)β)

where (cid:101)Φ (B) X =

(cid:104)

(cid:101)Φ (B) xt,i

(cid:105)

, (cid:101)Φ (B) Y =

(cid:104)

(cid:101)Φ (B) yt

(cid:105)

,

5

(14)

(15)

(16)

(cid:17)

(cid:16)

(cid:101)β

R

=








(cid:80)N

t=p+1 e2

t−1

...

and R0

(cid:17)

(cid:16)

(cid:101)β

=









(cid:80)N
(cid:80)N

(cid:80)N

t=p+1 etet−1
t=p+1 etet−2
...
t=p+1 etet−p

(cid:80)N

t=p+1 et−1et−2
(cid:80)N
t=p+1 e2
...

t−2

· · · (cid:80)N
· · · (cid:80)N
. . .
· · · (cid:80)N

t=p+1 et−1et−p
t=p+1 et−2et−p
...
t=p+1 e2

t−p
















.

However, since these estimating equations cannot be explicitly solved to get estima-
tors for the unknown parameters some numerical methods should be used to compute
the estimates. Among all numerical methods the estimating equations suggest a sim-
ple iteratively reweighting algorithm (IRA) to compute estimates for the unknown
parameters [1, 25].

2.2. Empirical Likelihood Estimation

In this section we consider the EL method to estimate the unknown parameters of
the regression model given in equation (12). The required constraints related to the
parameters will be formed similar to the EL estimation used in classical regression
case (uncorrelated error term regression model). Since we will use CML estimation
approach we will again assume that ﬁrst p observations are known and will form the
conditional empirical likelihood (CEL) function using the unknown probability weights
πt for the observations t = p + 1, ..., N . It should be noted that in CML estimation
case at are assumed to have normal distribution, however, in CEL estimation case we
do not need to assume any speciﬁc distribution for at. Now we can formulate the CEL
estimation procedure as follows.

Let πt for t = p + 1, ..., N be the unknown probabilities for the observations i =

p + 1, ..., N . Then, maximize the following CEL function

under the constraints

maxπt∈(0,1)

N
(cid:88)

t=p+1

log(πt)

N
(cid:88)

t=p+1

πt = 1

(17)

(18)

(cid:32)

πt

Φ(B)yt −

N
(cid:88)

t=p+1

M
(cid:88)

i=1

(cid:33)

βiΦ(B)xt,i

Φ(B)xt = 0

(19)

6

(cid:32)

πt

Φ(B)yt −

N
(cid:88)

t=p+1

M
(cid:88)

i=1

(cid:33)

βiΦ(B)xt,i

et,p = 0

(20)

N
(cid:88)

t=p+1



(cid:32)

πt



Φ(B)yt −

M
(cid:88)

i=1

(cid:33)2



βiΦ(B)xt,i

− σ2

 = 0

(21)

to obtain CEL estimators for the parameters of the regression model given in equations
(12). Here,
Φ (B) xt = [Φ (B) xt,1, Φ (B) xt,2, ..., Φ (B) xt,M ]T and et,p = [et−1, et−2, ..., et−p]T .

Since this is a constraint optimization problem Lagrange multiplier method can be
used to solve it. To this extend, let λ0, λ = (λT
2 , λ3), φ and π denote the Lagrange
multipliers, vector of φi, i = 1, 2, ..., p and vector of πt, t = p + 1, ..., N , respectively.
Then, the Lagrange function for this optimization problem can be written as

1 , λT

L(β, φ, σ2, π, λ, λ0) = −

N
(cid:88)

log(πt) + λ0





N
(cid:88)

t=p+1



πt − 1



t=p+1


N
(cid:88)

+ λT
1



t=p+1





N
(cid:88)

t=p+1

+ λT
2

(cid:32)

πt

Φ(B)yt −

(cid:32)

πt

Φ(B)yt −

M
(cid:88)

i=1

M
(cid:88)

i=1

(cid:33)



βiΦ(B)xt,i

Φ(B)xt



(cid:33)



βiΦ(B)xt,i

et,p





+ λ3



N
(cid:88)

t=p+1



(cid:32)

πt



Φ(B)yt −

M
(cid:88)

i=1

(cid:33)2





βiΦ(B)xt,i

− σ2





Taking the derivatives of L(β, φ, σ2, π, λ, λ0) with respect to πt and the Lagrangian
multipliers, and setting the resulting equations to zero we get ﬁrst order conditions of
this optimization problem. Solving the ﬁrst order conditions with respect to πt yields

πt =

where

(N − p) + λT

1
1 Ψ1,t + λT

2 Ψ2,t + λ3Ψ3,t

, t = p + 1, . . . , N

(22)

7

(cid:32)

Ψ1,t =

Φ(B)yt −

(cid:32)

Ψ2,t =

Φ(B)yt −



(cid:32)

M
(cid:88)

i=1
M
(cid:88)

i=1

Ψ3,t =



Φ(B)yt −

(cid:33)

βiΦ(B)xt,i

Φ(B)xt

(cid:33)

βiΦ(B)xt,i

et,p

(cid:33)2



βiΦ(B)xt,i

− σ2

 .

M
(cid:88)

i=1

Substituting these values of πt into equation (17) we ﬁnd that

l(λ, β, φ, σ2) =

N
(cid:88)

t=p+1

log πt

= −

N
(cid:88)

t=p+1

log (cid:0)(N − p) + λT

1 Ψ1,t + λT

2 Ψ2,t + λ3Ψ3,t

(cid:1)

(23)

Since 0 < πt < 1, the EL method maximizes this function over the set (N − p) +
λT
1 Ψ1,t + λT
2 Ψ2,t + λ3Ψ3,t > 1. The rest of this optimization problem will be carried
on as follows.

For given β, φ and σ2 minimize the function l(λ, β, φ, σ2) given in equation (23) with
respect to the Lagrange multipliers λ = (λ1, λ2, λ3). That is, for given β, φ and σ2
solve the following minimization problem to get the values of Lagrange multipliers

λ(β, φ, σ2) = argminλl(λ, β, φ, σ2).

Since the solution of this minimization problem cannot be obtained explicitly numeri-
cal methods should be used to get solutions. Substituting this solution in l(λ, β, φ, σ2)
yields the function l(λ(β, φ, σ2), β, φ, σ2) that is only depend on the model param-
eters β, φ and σ2. This function can be regarded as a proﬁle conditional empirical
log-likelihood function. The CEL estimators ˆβ, ˆφ and ˆσ2 will be obtained by maxi-
mizing l(λ(β, φ, σ2), β, φ, σ2) function with respect to the model parameters β, φ and
σ2. That is, the CEL estimators will be the solutions of the following maximization
problem

(ˆβ, ˆφ, ˆσ2) = argmaxβ,φ,σ2l(λ(β, φ, σ2), β, φ, σ2).

Since, there is no explicit solution of this maximization problem to explicitly obtain
the CEL estimators ˆβ, ˆφ and ˆσ2 numerical methods should be used to obtain CEL
estimators. In this paper, we use a Newton type algorithm to carry on this optimization
problem. The steps of our algorithm are as follows.

Step 0. Set initial values λ(0), β(0), φ(0) and σ2(0). Fix stopping rule (cid:15). The starting

8

value of λ(0) can be set to be the zero vector, but setting it −n for each lagrange
multiplier yields faster convergence.

Step 1. The function l(λ, β(0), φ(0), σ2(0)) given in equation (23) is minimized with
respect to λ. This will be done by iterating

λj+1 = λj +

1
2

(l(cid:48)(cid:48)−1
λ l(cid:48)
λ)

until convergence is satisﬁed. Here, l(cid:48)
λ is the second
order derivative of l(λ, β(0), φ(0), σ2(0)) with respect to λ. By doing so we calculate
λ(m) for m = 1, 2, 3, ... value for the lagrange multipliers computed at m − th step.

λ is the ﬁrst order derivative and l(cid:48)(cid:48)

Step 2. After ﬁnding λ(m) at Step 1, the function l(λ(m), β, φ,σ2) is maximized with
respect to φ, β and σ2 using following updating equations.

φj+1 = φj −

1
2

(l(cid:48)(cid:48)−1
φ

l(cid:48)
φ)

βj+1 = βj −

1
2

(l(cid:48)(cid:48)−1
β

l(cid:48)
β).

j+1 = σ2
σ2

j −

1
2

(l(cid:48)(cid:48)−1
σ2

l(cid:48)
σ2).

β, l(cid:48)

φ and l(cid:48)

Here l(cid:48)
β, φ and σ2 while l(cid:48)(cid:48)
β, l(cid:48)(cid:48)
obtain β(m), φ(m) and σ2(m), for m = 1, 2, 3, ...

σ2 are the ﬁrst order derivatives of l(λ(m), β, φ,σ2) with respect to
σ2 denote the second order derivatives. At this step we

φ and l(cid:48)(cid:48)

Step 3. Together, steps 1 and 2 accomplish one iteration. Therefore, after steps 1 and
2 we obtain the values λ(m), β(m), φ(m) and σ2(m) computed at m−th iteration. These
are current estimates for the parameters and are used as initial values to obtain the
estimates in (m + 1) − th iteration. Therefore, repeat steps 1 and 2 until

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

β(m+1) − β(m)
φ(m+1) − φ(m)
σ2(m+1) − σ2(m)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

< (cid:15)

is satisﬁed.

The performance of the proposed algorithm evaluated by the help of simulation study
and a numerical example given in the next section.

9

3. Simulation Study

In this section, we give a small simulation study and a numerical example to illustrate
the performance of the empirical likelihood estimators for the autoregressive error
term regression model over the estimators obtained from the normally distributed
error terms. We use R version 3.4.0 (2017-04-21) [21] to carry on our simulation study
and numerical example.

3.1. Simulation Design

We consider the second order (AR(2)) autoregressive error term model with M=3
regression parameters. The sample sizes are taken as n = 25, 50 and 100. For each
case the explanatory variables xt are generated from standard normal distribution
(xt ∼ N (0, 1)). The values of the parameters are taken as β = (β1, β2, β3)(cid:48) = (1, 3, 5)(cid:48)
,σ2 = 1 and φ = (φ1, φ2)(cid:48) = (0.8, −0.2)(cid:48). Note that the values of φ are taken as
above to guarantee the stationarity assumption of the error terms. We consider three
diﬀerent distributions for the error term at: N (0, 1), 0.9N (0, 1) + 0.1N (30, 1) and
0.9N (0, 1) + 0.1N (0, 10). Note that last two distributions are used to generate outliers
in y-direction. After setting the values of the model parameters β, φ and σ2 and
deciding the distribution of the error term, the values of the response variable are
generated using φ(B)yt = (cid:80)M

i=1 βiφ(B)xt,i + at.

We compare the results of the CEL estimators with the results of the CML es-
timators. Mean squared error (MSE) and bias values are calculated as performance
measures to compare the estimators. These values are calculated using the following
equations for 1000 replications:

M SE(ˆβ) = ( 1

i=1 (ˆβi − β)2, bias(ˆβ) = β − β,

M SE( ˆφ) = ( 1

i=1 ( ˆφi − φ)2, bias( ˆφ) = φ − φ,

M SE( ˆσ2) = ( 1

i=1 (ˆσ2

i − σ2)2, bias( ˆσ2) = σ2 − σ2

1000 ) (cid:80)1000
1000 ) (cid:80)1000
1000 ) (cid:80)1000
1000 ) (cid:80)1000

i=1

where β = ( 1

ˆβi , φ = ( 1

1000 ) (cid:80)1000

i=1

ˆφi,and σ2 = ( 1

1000 ) (cid:80)1000

i=1 ˆσ2
i .

3.2. Simulation Results

In Table 1, mean estimates, MSE and bias values of the CEL and CML estimators
computed over 1000 replications for the normally distributed error case (for the case
without outliers). We observe from these results that the both estimators have similar
behavior for the large sample sizes when outliers are not the case. On the other hand,
for smaller sample sizes the CEL estimators have better performance than the CML
estimators in terms of the MSE values. Thus, for small sample sizes the estimator
based on EL method may be a good alternative to the CML method.

The simulation results for the outlier cases are reported in Table 2 and 3. From these
tables we observe that when outliers are introduced in the data the CML estimator is
noticeably inﬂuenced by the outliers with higher MSE values. On the other hand, the
estimators based on empirical likelihood still retain their good performance in terms
of having smaller MSE and bias values. To sum up, the performance of the estimators

10

obtained from the empirical likelihood is superior to the performance of the estimators
obtained from CML method when some outliers are present in the data and/or the
sample size is smaller.

3.3. Numerical Example

Montgomery, Peck, and Vining [12] give soft drink data example relating annual re-
gional advertising expenses to annual regional concentrate sales for a soft drink com-
pany for 20 years. After calculating LS residuals for the linear regression model, the
assumption of uncorrelated errors can be tested by using the Durbin–Watson statistic,
which is a test statistics detecting the positive autocorrelation. The critical values of
the Durbin–Watson statistic are dL = 1.20 and dU = 1.41 for one explanatory variable
and 20 observations. The calculated value of the Durbin–Watson statistic d = 1.08
and this value is less than the critical value dL = 1.20. Therefore it can be said that
the errors in the regression model are positively autocorrelated.

We consider a linear regression model with AR(p) error terms and ﬁnd the LS
estimators for the model parameters. Then, we use the LS estimators as the initial
values to run the algorithm to compute CEL and CML estimators. The estimates of
the parameters are given in Table 4. We observe from the results given in Table 4
that the CEL and the CML methods give similar estimates for the regression and the
autoregressive parameters. But, the CEL estimate of error variance is smaller than
the estimate obtained from the CML method. Figure 1(a) shows the scatter plot of
the data set with the ﬁtted regression lines obtained from CEL and CML methods.
We observe that the ﬁtted lines are coinciding. To further explore the behavior of the
estimators against the outliers we create one outlier by replacing the last observation
with an outlier. For the new data set (the same data set with one outlier) we again
ﬁnd the CEL and the CML estimates for all the parameters. We also provide these
estimates in Table 4. Figure 1(b) depicts the scatter plot of the data and the ﬁtted
lines obtained from the CEL and the CML estimates. We observe that, unlike the
ﬁtted line obtained from the CEL method, the ﬁtted line obtained from the CML
method is badly inﬂuenced by the outlier.

3.4. Real Data Example

We use a real data set consisting of CO2 emission (metric kgs per capita) and en-
ergy usage (kg of oil equivalent per capita) of Turkey between the years 1974-2014.
The data set can be obtained from “https://data.worldbank. org”. The scatter plot
in Figure 2(a) show that a linear relationship between logarithms of CO2 emission
and energy usage. If the error terms are calculated using the LS estimates (-15.06489,
1.16258), it can be seen that the data has AR(1) structure with ˆφ = 0.65 autocorrela-
tion coeﬃcient, since the Durbin-Watson test statistic and its p-value are 0.662571 and
1.235e-07,respectively. The ACF (autocorrelation function) and PACF (Partial ACF)
plots given in Figure 3 also support the same results. Further, Q-Q plot of residuals
in Figure 3 yield that the assumption of normally distributed error terms is achieved.
For this data set,we use the CML and CEL estimates to estimate the parameters
of regression model and autocorrelation structure and also error variance. Note that
the LS estimates of original data are used as initial values to compute the CML and
the CEL estimates given in Table 5. The ﬁtted regression lines along with the scatter
plot of original data are also given in Figure 2(a),and we can see that both methods

11

give similar results.

To evaluate the performance of CEL method when there are outliers in the data or
departure from normality, we create an artiﬁcial outlier multiplying last observation
by ﬁve in the data, and use the both estimation methods to estimate the model
parameters. The estimates are provided in Table 5 and the ﬁtted lines obtained from
both methods can be seen in Figure 2(b). We can easily see from Figure 2(b), the
CEL method have better ﬁt than the CML method to the data with outlier. Although
real autocorrelation is positive, the CML estimate of autocorrelation is calculated
as negative because of the outlier eﬀect. In this case the LS estimates of regression
parameters are (-27.4952, 2.0717) and estimated autocorrelation is -0.0107.

4. Conclusion

In literature, the CML or the LS methods are often adapted to estimate the
parameters of an autoregressive error terms regression model. The CML method
are applied to carry on the estimation under some distributional assumption on the
error therm. However, for some data sets it may not be plausible to make some
distributional assumption on the error term due to the lack of information about data
sets. For those data sets, some alternative distributional assumption free methods
should be preferred to continue regression analysis. One of those distributional free
estimation methods is the EL method, which can also be used for small sample sizes.
In this paper, we have used the EL estimation method to estimate the parameters
of an autoregressive error terms regression model. We have deﬁned a CEL function
and constructed the necessary constraints using probability weights and the normal
equations borrowed from the classical LS estimation method to conduct parameter
estimation. To evaluate and compare the performance of the proposed method with
the CML method we have provided a simulation study and an example. We have
compared the results using the MSE and the bias. We have designed two diﬀerent
simulation scenarios, data with outliers and data without outliers. The results of
simulation study, numerical and real data examples have demonstrated that the CEL
method can perform better than the CML method when there are outliers in the data
set, which symbolizes the deviation from normality assumption. On the other hand,
they have similar performance if there are no outliers in the data sets.

12

References

[1] T. Alpuim and A. El-Shaarawi, On the eﬃciency of regression analysis with AR(p)

errors, Journal of Applied Statistics, 35:7(2008),pp.717–737.

[2] C.F. Ansley, An algorithm for the exact likelihood of a mixed autoregressive-moving

average process, Biometrika, 66:1 (1979),pp. 59–65.

[3] C.M. Beach and J.G. McKinnon, A maximum likelihood procedure for regression with

autocorrelated errors, Econometrica 46:1(1978),pp.51–58.

[4] H.D. Bondell and L.A. Stefanski, Eﬃcient Robust Regression via Two-Stage Generalized
Empirical Likelihood, Journal of the American Statistical Association, 108:502 (2013),pp.
644–655,.

[5] S.X. Chen,On the accuracy of empirical likelihood conﬁdence regions for linear regression

model, Ann. Inst. Statist. Math., 45 (1993),pp. 621–637.

[6] S.X. Chen,Empirical likelihood conﬁdence intervals for linear regression coeﬃcients, J.

Multiv. Anal., 49 (1994), pp. 24–40.

[7] S.X. Chen, Empirical likelihood conﬁdence intervals for nonparametric density estimation,

Biometrika, 83 (1996),pp. 329–341.

[8] S.X. Chen and H. Cui, An extended empirical likelihood for generalized linear models,

Statist. Sinica, 13 (2003), pp. 69–81.

[9] S.X. Chen, and I.V. Keilegom, A review on empirical likelihood methods for regression,

TEST, 19:3 (2009),pp.415–447.

[10] P. Hall and B. La Scala, Methodology and algorithms of empirical likelihood, Internat.

Statist. Review, 58(1990),pp. 109–127.

[11] E. D. Kolaczyk, Empirical

likelihood for generalized linear model, Statist. Sinica, 4

(1994),pp. 199–218.

[12] D.C. Montgomery, E.A. Peck and G.G. Vining, Introduction to linear regression analysis

(Vol. 821). John Wiley & Sons, 2012.

[13] W. Newey and R.J. Smith, Higher-order properties of GMM and generalized empirical

likelihood estimators, Econometrica, 72 (2004),pp. 219–255.

[14] A.B. Owen, Empirical

likelihood ratio conﬁdence intervals for a single functional,

Biometrika, 75(1988),pp. 237-249.

[15] A.B. Owen, Empirical likelihood conﬁdence regions, Ann. Statist., 18(1990),pp. 90–120.

[16] A.B. Owen, Empirical likelihood for linear models, Ann. Statist., 19(1991),pp. 1725–1747.

[17] A.B. Owen, Empirical Likelihood, Chapman and Hall, New York,2001.

[18] A.B. Owen, Self-concordance for empirical

likelihood, Canadian Journal of Statistics,

41:3 (2013),pp. 387-397.

13

[19] S¸. ¨Ozdemir and O. Arslan,An alternative algorithm of the empirical likelihood estimation
for the parameter of a linear regression model, Communications in Statistics - Simulation
and Computation,48:7 (2019), 1913-1921.

[20] S¸.

¨Ozdemir

and O. Arslan,
for
methods
bust
tions
Simulation
-
https://www.tandfonline.com/doi/full/10.1080/03610918.2019.1659968

Combining
linear

estimation
in

regression

Statistics

empirical

models,

likelihood

and Computation,(2019). Available

and

ro-
Communica-
at

[21] R Core Team, R: A Language and Environment for Statistical Computing, (2017).

Available at https://www.R-project.org/

[22] J. Shi and T.S. Lau, Robust empirical likelihood for linear regression models under median
constraints, Communications in Statistics: Theory and Methods,28:10 (1999), pp. 2465–
2476.

[23] J. Qin and J. Lawless, Empirical
Statist., 22(1994), pp. 300–325.

likelihood and general estimating equations, Ann.

[24] M.L. Tiku, W. Wong and G. Bian, Estimating parameters in autoregressive models in
nonnormal situations: Symmetric innovations, Communications in Statistics: Theory
and Methods, 28:2(1999),pp. 315–341.

[25] Y. Tua¸c, Y. G¨uney, B. S¸eno˘glu and O. Arslan, Robust parameterestimation of regression
model with AR(p) error terms, Communications in Statistics - Simulation and Compu-
tation, 47:8(2018), pp. 2343–2359.

[26] Y. Tua¸c, Y. G¨uney and O. Arslan,Parameter estimation of regression model with
AR(p) error terms based on skew distributions with EM algorithm, Soft Computing,
(2019).Available at https://doi.org/10.1007/s00500-019-04089-x

[27] D. Yang and D.S. Small, An R package and a study of methods for computing empirical
likelihood, Journal of Statistical Computation and Simulation, 83:7 (2013),pp. 1363–1372.

14

Table 1. Estimates, MSE and Bias values of the estimates from at ∼ N (0, 1)
for diﬀerent sample sizes without outliers. True values are (β1, β2, β3)(cid:48) = (1, 3, 5)(cid:48),
(φ1, φ2)(cid:48) = (0.8, −0.2) and σ2 = 1

n=25

n=50

n=100

Normal

Empirical

Normal

Empirical

Normal

Empirical

ˆβ1

β1 MSE
BIAS
ˆβ2
MSE
BIAS
ˆβ3

β2

β3 MSE
BIAS
ˆφ1
MSE
BIAS
ˆφ2

φ1

φ2 MSE
BIAS
ˆσ2
MSE
BIAS

σ2

0.98796
0.03276
0.14255
3.00136
0.03247
0.14406
5.00803
0.03481
0.14491
0.76906
0.0457
0.16869
-0.2095
0.04266
0.16559
0.88229
0.03511
0.15368

1.00138
0.0058
0.05276
3.00697
0.00613
0.05512
5.00139
0.00657
0.05574
0.80372
0.00028
0.01328
-0.19714
0.00029
0.01394
1.15221
0.03062
0.15458

0.99845
0.01445
0.09522
2.99808
0.01346
0.09181
4.99681
0.01421
0.09389
0.77997
0.02118
0.11537
-0.20945
0.01949
0.11067
0.94378
0.01351
0.09334

1.00178
0.00228
0.0319
3.00009
0.00225
0.03214
4.99966
0.00202
0.03042
0.8013
0.0001
0.00784
-0.199
0.0001
0.00824
1.1022
0.01454
0.1022

0.99932
0.00584
0.0605
2.9978
0.00664
0.06437
4.99913
0.00583
0.06045
0.79587
0.01042
0.08196
-0.21079
0.00951
0.07779
0.96983
0.00633
0.06461

1.00068
0.00074
0.01832
2.99753
0.00071
0.01797
5.00266
0.00069
0.01789
0.80081
0.00004
0.00465
-0.20002
0.00004
0.00467
1.07259
0.00784
0.07326

Table 2. Estimates, MSE and Bias values of the estimates for diﬀerent sample sizes
from at ∼ 0.9N (0, 1) + 0.1N (30, 1). True values are (β1, β2, β3)(cid:48) = (1, 3, 5)(cid:48), (φ1, φ2)(cid:48) =
(0.8, −0.2) and σ2 = 1

n=25

n=50

n=100

Normal

Empirical

Normal

Empirical

Normal

Empirical

ˆβ1

β1 MSE
BIAS
ˆβ2
MSE
BIAS
ˆβ3

β2

β3 MSE
BIAS
ˆφ1
MSE
BIAS
ˆφ2

φ1

φ2 MSE
BIAS
ˆσ2
MSE
BIAS

σ2

0.99758
0.53329
0.56527
2.97227
0.51638
0.5623
4.97749
0.50668
0.55812
1.77218
0.9494
0.97218
-0.94992
0.58956
0.74992
5.95214
24.68756
4.95214

1.0001
0.00588
0.05219
3.00938
0.00659
0.05636
5.00772
0.00559
0.05204
0.80245
0.00029
0.0136
-0.19518
0.00033
0.01471
1.13923
0.02581
0.1419

1.01599
0.11102
0.26139
2.98684
0.10983
0.26129
5.00388
0.10949
0.2574
1.7009
0.81299
0.9009
-0.75483
0.31046
0.55483
4.42534
11.7763
3.42534

1.0008
0.00189
0.02957
2.9988
0.00222
0.03185
5.00511
0.00218
0.03174
0.80143
0.00009
0.00746
-0.19879
0.00011
0.0083
1.10108
0.0142
0.10183

0.9947
0.02944
0.13598
2.99961
0.02613
0.12822
4.99569
0.0282
0.13337
1.63445
0.69691
0.83445
-0.64632
0.19993
0.44632
3.30909
5.34667
2.30909

1.00067
0.00071
0.01776
3.00186
0.00068
0.01766
5.00152
0.00072
0.01798
0.80104
0.00003
0.00446
-0.20002
0.00004
0.00462
1.06915
0.00688
0.06921

15

Table 3. Estimates, MSE and Bias values of the estimates for diﬀerent sample sizes
from at ∼ 0.9N (0, 1) + 0.1N (0, 10). True values are (β1, β2, β3)(cid:48) = (1, 3, 5)(cid:48), (φ1, φ2)(cid:48) =
(0.8, −0.2) and σ2 = 1

n=25

n=50

n=100

Normal

Empirical

Normal

Empirical

Normal

Empirical

ˆβ1

β1 MSE
BIAS
ˆβ2
MSE
BIAS
ˆβ3

β2

β3 MSE
BIAS
ˆφ1
MSE
BIAS
ˆφ2

φ1

φ2 MSE
BIAS
ˆσ2
MSE
BIAS

σ2

1.00012
0.21859
0.33632
2.99806
0.23989
0.34908
5.01594
0.22748
0.3443
0.79014
0.43476
0.51506
-0.46927
0.7917
0.70074
2.65719
3.79346
1.65951

1.00766
0.00777
0.05886
2.9978
0.0086
0.06087
5.00257
0.0079
0.05964
0.80161
0.00022
0.01213
-0.19645
0.00031
0.01421
1.14735
0.02846
0.14943

0.9878
0.11943
0.26183
2.98896
0.13169
0.2692
4.99975
0.12487
0.26638
0.73106
0.18784
0.34467
-0.26362
0.23481
0.37436
2.70776
3.59029
1.70791

0.99887
0.00276
0.034
3.00449
0.00265
0.03391
5.00495
0.00273
0.03409
0.80076
0.00009
0.00757
-0.19819
0.0001
0.00786
1.09725
0.01368
0.09889

0.99299
0.06829
0.20502
2.98302
0.06363
0.19512
4.99407
0.06541
0.19757
0.7298
0.08888
0.23919
-0.21955
0.08613
0.23868
2.94881
4.16932
1.94881

1.00153
0.00076
0.01859
3.00347
0.00083
0.01922
4.99616
0.00083
0.01894
0.80066
0.00004
0.00471
-0.19966
0.00004
0.00473
1.06939
0.00702
0.0697

Table 4. The parameter estimates for soft drink data

without outlier

with one outlier

Empirical

Normal

Empirical

Normal

ˆβ0
ˆβ1
ˆφ1
ˆσ2

1593.95911
20.07335
0.89579
6.43127

1645.37912
19.80988
0.56856
16.96544

1617.57068
19.93477
0.89814
2.44785

560.9601
30.7569
-2.5689
2025.174

Table 5. The parameter estimates for CO2 emission-Energy
usage data

without outlier

with one outlier

Empirical

Normal

Empirical

Normal

ˆβ0
ˆβ1
ˆφ1
ˆσ2

-15.389316
1.187502
0.655439
0.824727

-13.827491
1.074341
0.861551
0.021447

-13.891223
1.076084
0.511249
1.148087

-25.869536
1.952316
-0.201119
0.900177

16

(a) without outlier

(b) with one outlier

Figure 1. Scatter plot of soft drink data and, CEL and CML ﬁts with outlier

(a) without outlier

(b) with one outlier

Figure 2. Scatter plot of CO2 emission- Energy usage data,and CEL and CML ﬁts

17

Figure 3. ACF, PACF and Q-Q of plots of residuals CO2 emission- Energy usage data

18

