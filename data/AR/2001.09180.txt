0
2
0
2

n
a
J

4
2

]
T
S
.
h
t
a
m

[

1
v
0
8
1
9
0
.
1
0
0
2
:
v
i
X
r
a

Imputation for High-Dimensional Linear Regression

Kabir Aladin Chandrasekher∗

Ahmed El Alaoui∗†

Andrea Montanari∗†

Abstract

We study high-dimensional regression with missing entries in the covariates. A common
strategy in practice is to impute the missing entries with an appropriate substitute and then
implement a standard statistical procedure acting as if the covariates were fully observed. Recent
literature on this subject proposes instead to design a speciﬁc, often complicated or non-convex,
algorithm tailored to the case of missing covariates. We investigate a simpler approach where we
ﬁll-in the missing entries with their conditional mean given the observed covariates. We show
that this imputation scheme coupled with standard oﬀ-the-shelf procedures such as the LASSO
and square-root LASSO retains the minimax estimation rate in the random-design setting where
the covariates are i.i.d. sub-Gaussian. We further show that the square-root LASSO remains
pivotal in this setting.

It is often the case that the conditional expectation cannot be computed exactly and must be
approximated from data. We study two cases where the covariates either follow an autoregres-
sive (AR) process, or are jointly Gaussian with sparse precision matrix. We propose tractable
estimators for the conditional expectation and then perform linear regression via LASSO, and
show similar estimation rates in both cases. We complement our theoretical results with sim-
ulations on synthetic and semi-synthetic examples, illustrating not only the sharpness of our
bounds, but also the broader utility of this strategy beyond our theoretical assumptions.

1

Introduction

Statistical estimation procedures are usually designed under the assumption that data is fully
observed. It is however common that some portion of the data is missing or observed through a
noisy channel. A natural strategy to address this problem is to replace the missing data with a
sensible proxy. Such a strategy, known as imputation, is widely used in practice.

We observe a response vector y ∈ Rn from a linear model with design matrix X ∈ Rn×p in

following manner:

y = Xβ0 + (cid:15),

(1.1)

where (cid:15) is zero-mean sub-Gaussian noise. We are interested in recovering the unknown regression
vector β0 ∈ Rp. We are speciﬁcally interested in the high dimensional regime, when p (cid:29) n. In this
underdetermined setting, it is necessary to assume some structure on the regression vector β0. It
is of particular interest when β0 has a few non-zero entries. In this case we say the vector β0 is
sparse and denote its number of non-zeros by s. Sparse regression in the high-dimensional regime
has been widely studied in the last decade and has witnessed a beautiful line of results showing

∗Department of Electrical Engineering, Stanford University
†Department of Statistics, Stanford University

1

 
 
 
 
 
 
that convex programs such as the Lasso [Tib96] and the Dantzig Selector [CT07] give rate-optimal
statistical guarantees [RWY11, CD13, BRT+09].

These estimation procedures typically require full knowledge of the design matrix X. We
consider a setting where a corrupted version of the data, Z is observed instead of X. This models
the scenario where some covariates have missing entries. We will be mainly interested in two types
of patterns of missingness, outlined for instance in the book [LR14]:

1. Missing Completely at Random (MCAR): The most benign mechanism in which each

entry is missing independently of everything with probability 1 − α:

Zia =

(cid:40)

Xia with probability α,
(cid:63)

with probability 1 − α.

(1.2)

(The symbol (cid:63) indicates that an entry is missing.)

2. Missing Not at Random (MNAR): The general case where the missingness pattern

(1{Zia=(cid:63)})1≤i≤n,1≤a≤p is composed of i.i.d. arbitrarily distributed rows.

Given the response vector y and the observed data Z, how can we estimate β0?

An example: standard Gaussian design. We illustrate our approach in the simple case where
the design matrix X is i.i.d. normal Xij ∼ N(0, 1), and assume MCAR with parameter α. Let us
additionally consider the case that there is no additive noise. We thus observe the pair (y, Z) given
by

y = Xβ0,

Zia =

(cid:40)

Xia with probability α,
(cid:63)

with probability 1 − α.

One natural strategy is to then impute the missing entries with some reasonable proxy. Since we
know the data is standard normal, a ﬁrst attempt could be to replace each missing entry with its
mean; that is, whenever an entry is missing, give it the value 0. We thus construct the imputed
matrix

(cid:98)Xia =

(cid:40)

Zia
0

if Zia (cid:54)= (cid:63),
otherwise.

We can now run the LASSO with data (y, (cid:99)X) and regularization parameter λ =
choice is justiﬁed in Corollary 3.1):

(cid:98)β ∈ arg min

β∈Rp

(cid:26) 1
2n

(cid:13)
(cid:13)
(cid:13)y − (cid:99)Xβ

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:27)

+ λ (cid:107)β(cid:107)1

.

(cid:113) (1−α) log p
αn

(this

(1.3)

We plot the error (cid:107)(cid:98)β −β0(cid:107)2 in Figure 1 as a function of α, compared to the scaling of the theoretical
error bound of Corollary 3.1 in solid line (letting the universal constant pre-factor be one). The

ﬁgure shows that this strategy recovers the regression vector β0 at the optimal rate
with a α-dependent term accounting for the missingness.

2

(cid:113) s log p
n

adjusted

MSE Imputation for Identity Covariance Gaussian

2
(cid:13)(cid:13)(cid:13)
0
β
−
(cid:98)β
(cid:13)(cid:13)(cid:13)

10−0.5

10−1

Empirical Error
(cid:113) (1−α)s log p
αn

α (cid:55)→

0.5

0.6

0.7

0.8

0.9

1

Density of observed entries α

p(cid:101) = 35 (square root
Figure 1. MSE as function of α. We used n = 1000, p = 1200, s = (cid:100)
sparsity). Each data point is an average of 100 trials. Shaded area: errors obtained over the 50 trials.
The solid line shows the scaling of the bound shown in Corollary 3.1. Note that this bound holds for
α > 0, and we appeal to classical results on sparse linear regression when α = 0.

√

How should we generalize the above success? Consider re-writing

y = Xβ0 + (cid:15) = (cid:99)Xβ0 +

(cid:16)

X − (cid:99)X

(cid:17)

β0 + (cid:15) ≡ (cid:99)Xβ0 + (cid:101)(cid:15).

(1.4)

Now, notice that if we take (cid:99)X = E{X | Z}, by the orthogonality of conditional expectation, (cid:99)Xβ0
and (X − (cid:99)X)β0 are uncorrelated and we can intuitively think of the problem as a linear model with
data (cid:99)X and noise (cid:101)(cid:15). Using this intuition, we analyze the strategy of imputation by conditional
expectation and present the following contributions:

Rate-optimal estimation via imputation and the LASSO. Assuming the rows of X are
i.i.d. sub-Gaussian, we show that imputation with conditional expectation followed by LASSO,
Eq. (1.3), retains rate-optimal statistical guarantees, regardless of the model of missingness.

Rate-optimal and pivotal estimation via imputation and the square-root LASSO.
Using the same imputation strategy, we show that the minimizers of square-root LASSO pro-
gram (1.5) introduced by Belloni et al. [BCW11]

(cid:98)β ∈ arg min

β∈Rp

(cid:26) 1
√
n

(cid:13)
(cid:13)
(cid:13)y − (cid:99)Xβ

(cid:13)
(cid:13)
(cid:13)2

(cid:27)

+ λ (cid:107)β(cid:107)1

,

(1.5)

retain rate-optimal guarantees and are moreover pivotal with respect to noise variance σ2 and the
radius of the problem (cid:107)β0(cid:107)2. That is, the appropriate choice of the regularization parameter λ in
Eq. (1.5) does not require the knowledge of these quantities, nor do they need to be estimated in
order to optimally implement the square-root LASSO. We emphasize that both (1.3) and (1.5) can
be implemented using existing packages for the LASSO and square-root LASSO; the statistician
needs only to operate on the observed data matrix Z.

Imputation with estimated conditional expectation. We provide two examples of data
from Gaussian graphical models and leverage this structure to approximately compute the con-
ditional expectation given the observed data. We provide statistical guarantees for both of these

3

cases, noting that in our results we use the entire dataset to estimate the conditional expectation,
and then run the LASSO on this data. The structure of the graphical model is used in a crucial way
to design tractable estimators; we expect similar results to hold for more general models retaining
such structure.

We organize our results as follows:

• Section 3 contains our main results in the case in which we give rates of statistical error of the
convex program (1.3) under MCAR and the assumption that a conditional expectation can be
computed exactly. In particular, we show that our estimator (cid:98)β has the optimal dependence
on each on the parameters R, σ, p, n, s for a design with i.i.d. sub-Gaussian rows. Moreover,
when the covariance is the identity, our upper bound is optimal in the parameter α as well.
We additionally state our result regarding rates when the data is MNAR. The proofs for the
results of this section can be found in appendix A.

• Section 4 states our results on pivotal estimation; the proof for the main result of this section

can be found in appendix C.

• Section 5 provides two examples of cases where the conditional expectation may not be avail-
able, but an approximate conditional expectation may be computed. In particular, our ﬁrst
example of autoregressive–AR(1)–model provides a case in which an approximate conditional
expectation yields optimal results. We then provide rates for Gaussian design with sparse
inverse covariance matrix. The proofs of the results of this section can be found in appendix B.

• Section 6 provides simulation results, giving numerical evidence of our results; both on syn-

thetic and semi-synthetic data.

2 Related literature

Estimation in the presence of missing data has been studied for decades. An in-depth overview
of techniques can be found in [LR14]. The problem of high-dimensional linear regression with
missing data was ﬁrst studied theoretically in [RT10]. The ﬁrst rate optimal theoretical results in
this direction were given in a sequence of papers by Loh and Wainwright [LW12b, LW12a]. The
main idea in these papers is to recast the LASSO optimization as a quadratic program taking the
covariance matrix of the (fully observed) covariates as input. In the missing data case, the authors
follow a plug-in principle and construct an unbiased estimator of this covariance to be used in
lieu of the full covariance. The fact that the some covariates are not observed is accounted for
by subtracting a diagonal term in their estimator which causes it to be non-positive semideﬁnite.
Consequently the resulting optimization problem is no longer convex. Remarkably, the authors are
able to show that a simple projected gradient descent procedure reaches a near-optimal point with
high probability, thereby producing a good estimate of β0. Following this result, many authors
have proposed using this plug-in principle for various sparse recovery algorithms such as orthogonal
matching pursuit (OMP) [CC13] and the Dantzig Selector [WWBS17]. Other convex surrogates
for this problem have been given by [RT13, BRT17, DZ+17] and the more complicated case of
dependent measurements has been tackled by Rudelson and Zhou [RZ17]. Notably, Belloni et
al. [BCK+17] proposed a pivotal estimator for this problem based on the idea of self-normalization.
In particular, we do not design any
Our approach diﬀers from these in its use of imputation.
new algorithms for linear regression; rather, we use existing algorithms (LASSO and square-root

4

LASSO) in their most ‘vanilla’ version and analyze their statistical guarantees under a particular
imputation strategy.

More recently Agarwal et al. [ASSS19] consider a diﬀerent setting where no sparsity is assumed
on β0 but the covariates have a low-dimensional structure. They propose a matrix estimation
approach for imputation followed by a simple least squares method. We ﬁnally mention that
beyond linear regression, other models of high-dimensional statistical problems have been studied
in the setting of missing data, such as covariance estimation [Lou14], sparse principal component
analysis [EvdG18, Lou13], and precision matrix estimation [KX12, FJSZ19].

Notation and basic notions. We use bold-face lower-case letters to denote vectors (w, v, . . . )
and bold-face upper-case letters to denote matrices (X, Z, . . . ). Rows of matrices will be denoted
by the letters (i, j, . . . ) and columns by (a, b, . . . ). Accordingly we will refer to rows of the matrix
X as (X i, X j, . . . ) and its columns as (X a, X b, . . . ). Additionally, for a vector x ∈ Rd, and a set
T = {i1, i2, . . . , i|T |} ⊆ [d], we will use the notation xT to denote the vector (xi1, xi2, . . . , xi|T |) of
length |T |.

A mean zero random variable x ∈ R is called σ2

X -sub-Gaussian if

E{eθX } ≤ e

X θ2
σ2
2

∀θ ∈ R.

It is instead called (σ2
X , b)-subexponential if the above holds for all θ ≤ 1/b. By extension, a random
vector x is sub-Gaussian (sub-exponential) if (cid:104)u, x(cid:105) is sub-Gaussian (sub-exponential) for all unit
norm vectors u.

Throughout the paper, we will use C to denote a universal constant that may change from line
to line, and C(·) as a constant depending only on its arguments. We will write f (cid:46) g to denote
f ≤ Cg (the symbol (cid:38) is similarly deﬁned), and we write f (cid:16) g if f (cid:46) g and f (cid:38) g.

We will write MCAR(α) to indicate that the data is MCAR with probability of observing an

entry equal to α. We will assume α to be a constant throughout the paper.

3

Imputation by conditional expectation

In this section, we state our results on the statistical error of our estimator (cid:98)β; the proofs of
each statement can be found in Appendix A. Recall that (cid:99)X = E {X | Z}. We will denote its
(cid:99)X ∈ Rp×p. Throughout this section, we will make the following
covariance matrix by Σ
assumptions:

(cid:99)X ≡ 1

E (cid:99)X

(cid:62)

n

A1. The rows of X are i.i.d. zero-mean and σ2

X sub-Gaussian with covariance matrix ΣX .

A2. The regression vector β0 is s-sparse: |supp(β0)| = s. Additionally, (cid:107)β0(cid:107)2 ≤ R.

A3. There exists a constant cmin > 0 such that the minimum eigenvalue of Σ

(cid:99)X satisﬁes λmin(Σ

(cid:99)X ) ≥

cmin.

A4. There exists a constant c0 > 0 such that n ≥ c0 max

(cid:16)

σ4
X
λmin(Σ

(cid:99)X )2 ,

σ2
X
λmin(Σ

(cid:99)X )

(cid:17)

s log p.

Theorem 1. Assume A1–A4 and that the data is MCAR(α) for α < 1. Assume additionally that
(cid:113) log p

1 − α for a positive constant c1. Then, there exist positive constants c2, c3, c4 such

√

n ≤ c1

5

that with probability at least 1 − c2p−c3, (cid:98)β as deﬁned in (1.3) with regularization parameter

λ = c4

(cid:0)σX σ + σ2

X

(cid:114)

√

1 − αR(cid:1)

log p
n

,

satisﬁes

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

√

(cid:46) σX σ + σ2

X
λmin(Σ

1 − αR
(cid:99)X )

(cid:114)

s log p
n

.

(3.1)

Specializing the analysis for the special case of Gaussian design with identity covariance, X i ∼
N(0, I p×p), we have the following corollary which establishes that for this special case, our imputa-
tion estimator is rate optimal, in a minimax sense, with respect to all the parameters σ, α, n, p, s.

(cid:113) log p

Corollary 3.1. Assume the data is MCAR(α) for α < 1 and Xij ∼iid N(0, 1). Assume additionally
(cid:112)α(1 − α) for a positive constant c1. Then, there exist positive constants c2, c3, c4
that
such that with probability at least 1 − c2p−c3, (cid:98)β as deﬁned in (1.3) with regularization parameter

n ≤ c1

satisﬁes

√

α (cid:0)σ +

√

1 − αR(cid:1)

λ = c4

(cid:114)

log p
n

,

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

(cid:46) σ +

(cid:114)

√
√

1 − αR
α

s log p
n

.

(3.2)

Indeed, the bound displayed in Corollary 3.1 is matched by a lower bound Wang et al. [WWBS17]:

Theorem 2. [WWBS17] Suppose 4 ≤ s < 4p/5, s log p/s

αn → 0 and Xij ∼iid N(0, 1). Then,

inf
(cid:98)β

sup
β0∈B2(R)∩B0(s)

E

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:38) φ(R, σ, α) min

(cid:40)(cid:115)

s log p/s
(1 − α)2n

,

s log p/s
αn

(cid:41)

,

where φ(R, σ, α) = min (cid:8)σ2 + (1 − α)R2, e(1−α)sσ2(cid:9).

and σ2 + (1 − α)R2 ≤
Remark 3.1. We observe that in the regime where
e(1−α)sσ2, Corollary 3.1 matches the lower bound in dependence on all parameters. Indeed, the
lower bound simpliﬁes to

(1−α)2n ≥ s log p/s

αn

(cid:113) s log p/s

σ2 + (1 − α)R2
α

s log p/s
n

.

Finally, we give analogous rates when the missing data mechanism is MNAR.

n ≤ c1
Theorem 3. Assume that the data is MNAR and A1–A4. Assume additionally that
for a positive constant c1. Then, there exist positive constants c2, c3, c4 such that with probability
at least 1 − c2p−c3, (cid:98)β as deﬁned in Eq. (1.3) with (cid:99)X = E {X | Z} and regularization parameter

(cid:113) log p

satisﬁes

λ = c4

(cid:0)σX σ + σ2

X R(cid:1)

(cid:114)

log p
n

,

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

(cid:46) σX σ + σ2
X R
(cid:99)X )
λmin(Σ

(cid:114)

s log p
n

.

6

(3.3)

Remark 3.2. Consider the scenario in which Xij ∼ N(0, 1) and the data is MCAR(α). The
bound one gets from Theorem 3 is σ+R
n , whereas the tighter result from Corollary 3.1 gives
α

(cid:113) s log p

σ+

√
1−αR√
α

(cid:113) s log p
n .

Two remarks are in order:

1. Our results dictate a speciﬁc choice of the regularization parameter λ which depends on
σX , σ, R and α (in the MCAR case). Although α, σX may be estimated, it is diﬃcult to
estimate the noise variance σ, and in many situations it is diﬃcult to determine R, which is
a function of the regression vector we are aim to recover.

2. We assume that the conditional expectation (cid:99)X can be computed exactly. This may not be
realistic in practice; even with Gaussian designs, this quantity requires the knowledge of the
covariance matrix.

We address these two questions in the following two sections. Regarding the ﬁrst question, we show
that the square-root LASSO with conditional expectation imputation is pivotal with respect to σ, R
and as such these parameters need not be estimated to set the regularization λ. Regarding the
second question, we show that if the covariates come from a graphical model, we can approximate
conditional expectation eﬃciently to an accuracy suﬃcient for consistency of LASSO.

4 Pivotal estimation: the square-root LASSO

When the data matrix X is known, the insight of [BCW11, Ant10] was that the procedure

(cid:98)βS ∈ arg min

β∈Rp

(cid:26) 1
√
n

(cid:107)y − Xβ(cid:107)2 + λ (cid:107)β(cid:107)1

,

(cid:27)

known as the square-root LASSO, retains the statistical guarantees of the LASSO, but does not
require knowledge of the noise standard deviation σ to choose λ. The intuition that guided us in
the decomposition (1.4) leads to the insight that in the imputed linear model, the eﬀective noise
(cid:112)(1 − α)R. The following theorem formalizes this intuition and
standard deviation will be σ + σX
shows that the square-root LASSO with conditional expectation imputation,

(cid:98)β ∈ arg min

β∈Rp

1
√
n

(cid:13)
(cid:13)
(cid:13)y − (cid:99)Xβ

(cid:13)
(cid:13)
(cid:13)2

+ λ (cid:107)β(cid:107)1 ,

(4.1)

retains such a guarantee, showing that when using the square-root LASSO, the statistician need
not know σ nor R when picking the regularization constant λ.

Theorem 4. Assume A1–A4 and that the data is MCAR(α). Assume additionally that there exists
X + σ2(cid:1)2. Then, there exist positive constants
a constant c1 < 1 such that n ≥ 1
c1
c2, c3, c4 such that with probability at least 1 − δ − c2p−c3, (cid:98)β as deﬁned in (4.1) with regularization
parameter

(cid:0)ln δ−1(cid:1) (cid:0)R2σ2

λ = c4σX

(cid:114)

log p
n

,

satisﬁes

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

(cid:46) σX (σ + σX R)
λmin(Σ

(cid:99)X )

(cid:114)

s log p
n

.

7

5

Imputation by approximate conditional expectation

A strong assumption in Section 3 is the exact knowledge of a conditional expectation E {X i,Sc | X i,S}
for any S ⊆ [p]. In this section, we give two examples in which the exact conditional expectation is
not available, but an approximate version can be computed. Both examples are Gaussian graphical
models and our results exploit the graphical structure in a crucial way, allowing us to compute
conditional expectations using only a small subset of the variables.

In both examples, we deal with the uncertainty in the same way. First, we use all of the data
to estimate the covariance, and use this estimate to compute the conditional expectation. We then
run the LASSO based on the imputed matrix in order to recover the regression vector β0. The
Markov structure of these models is exploited algorithmically: whenever an entry is missing, we
need only to consider a small number of observed nodes to estimate the missing entry.

Let us note that in both cases, similar results can be shown through sample splitting. For
instance, if the statistician reserves n/2 samples for learning the covariance and uses the remaining
n/2 samples for regression, rate-optimality can be shown in a manner similar to that of Section 3.
We analyze the more challenging case in which samples are re-used in order to keep higher ﬁdelity
to statistical practice.

5.1 Example #1: AR(1) model

We consider the autoregressive real-valued stationary process Xt = φXt−1 + Zt where Zt ∼i.i.d.
N(0, 1) with unknown coeﬃcient φ satisfying |φ| < 1. We form the rows of the data matrix X by
sampling p consecutive points X0, . . . , Xp−1 from the stationary chain (Xt), independently for each
row. The covariance matrix of each row is (ΣX )ij = 1

1−φ2 φ|i−j|.

Let Mik be the indicator of whether entry (i, k) is observed or not. We ﬁnd an estimate ˆφ of

the true parameter φ from the observed data:

1
α2np

ˆφ =

(cid:80)n

(cid:80)p−1

i=1
1
αnp

a=1 XiaXi(a+1)MiaMi(a+1)
(cid:80)n

(cid:80)p−1

a=1 X 2

iaMia

i=1

.

(5.1)

Suppose the entry (i, k) is missing. By the Markov property satisﬁed by this model (see Figure 2)
the conditional expectation E {Xik | X iS} is a function on the closest observed entries on either side
of node k. Using the formula for the conditional expectation of a multivariate Gaussian random
variable, we have

E {Xik | X iS} = (cid:98)Xik =

φd1+d2
1 − φ2(d1+d2)

(cid:16)

Xi,L(k)

(cid:16)

φ−d2 − φd2

(cid:17)

+ Xi,R(k)

(cid:16)

φ−d1 − φd1

(cid:17)(cid:17)

,

(5.2)

where d1 = k − L(k) and d2 = R(k) − k with L(k) and R(k) the positions of the closed observed
entries to the left and right of k, respectively. We plug in the estimate ˆφ in lieu of φ to approximate
conditional expectation:

(cid:101)Xik =

ˆφd1+d2
1 − ˆφ2(d1+d2)

(cid:16)

Xi,L(k)

(cid:16) ˆφ−d2 − ˆφd2

(cid:17)

+ Xi,R(k)

(cid:16) ˆφ−d1 − ˆφd1

(cid:17)(cid:17)

.

(5.3)

We repeat this process for every missing entry to create the matrix (cid:102)X and proceed as in the previous
section. The following result shows that this procedure indeed leads to rate optimal estimation.

8

Xi,1

Xi,2

Xi,3

Xi,k−2

Xi,k−1

Xi,k

Xi,k+1

Xi,d

· · ·

· · ·

Figure 2. The graphical model for row i. The gray nodes represent missing entries whereas the
black nodes represent observed entries. The observed entries at the end of the dashed path are
used to estimate the missing entries Xi,k−1, Xi,k.
In this case, L(k) = L(k − 1) = k − 2 and
R(k) = R(k − 1) = k + 1.

1
Theorem 5. Assume A1–A4, the data is MCAR(α), that the sample size n ≥ c1
α8 s log p for
a positive constant c1, and that the rows of X are generated from the stationary auto-regressive
process described above with |φ| < 1. Then, there exist positive constants c2, c3, c4 such that with
probability at least 1 − c2p−c3. (cid:98)β as deﬁned in (1.3), using (cid:102)X in place of (cid:99)X, with regularization
parameter

λ = c4

(cid:18) σX σ

α2 +

σ2
X
α4 R

(cid:19) (cid:114)

log p
n

,

satisﬁes

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

(cid:46)

(cid:16) σX σ

X

α2 + σ2
α4 R
(cid:99)X )
λmin(Σ

(cid:17)

(cid:114)

s log p
n

.

(5.4)

(5.5)

Remark 5.1. Note that in this case the model is fully explicit and ΣX is Toeplitz. One can thus
compute for instance σ2
(cid:99)X ) is more diﬃcult; we provide
in Appendix F.9 a lower bound for a restricted range of α, φ.

X ≤ (1 − φ2)−1(1 − φ)−1. Computing λmin(Σ

The proof of this theorem can be found in appendix B.

5.2 Example #2: Gaussian design with sparse precision matrix

We now generalize the strategy of the previous section to the case of Gaussian rows with sparse
precision matrices. We consider a zero mean normal distribution with covariance Σ and precision
matrix Ω = Σ−1. We make the following assumptions:

C1. There exist positive constants 0 < c < c such that the eigenvalues of Σ satisfy c ≤ λmin(Σ) ≤

λmax(Σ) ≤ c. Additionally, rows of X are drawn i.i.d. from the distribution N(0, Σ).

C2. Each row of Ω as at most dmax non-zero entries, where dmax satisﬁes (1 − α)(dmax − 1) < 1

and the sparsity pattern of Ω is known to the statistitian.

C3. There exists a constant C(α, dmax) > 0 depending only on α, dmax such that the sample size

n satisﬁes n ≥ C(α, dmax) max

(cid:16)

σ4
X
λmin(Σ

(cid:99)X )2 ,

σ2
X
λmin(Σ

(cid:99)X )

(cid:17)

max (cid:0)s2 log p, log7 p(cid:1).

Remark 5.2. Although the sparsity pattern is assumed to be known to the statistician, under
the additional technical assumption of irrepresentability of the graphical model [RWRY11], the
sparsity pattern can be found with high probability using for example the graphical LASSO [KX12,
RWRY11].

9

Xi,k

S(i, k)

Figure 3. The Markov blanket used to estimate the missing entry Xi,k. Each of the gray nodes
in the neighborhood of Xi,k denotes a missing entry whereas each of the black nodes denotes an
observed entry. Each of the values of the observed entries on the boundary are used to estimate Xi,k.

We are interested in computing the conditional expectation of the missing entries E{X iSc |
(cid:1)−1X iS. We require some

X iS}, which in this Gaussian setting is given by the formula ΣSc,S
deﬁnitions.

(cid:0)ΣS,S

Deﬁnition 5.1. Let the matrix A with Aij = 1 {Ωij (cid:54)= 0} denote the adjacency matrix of the graph
G = (V = [p], E). Let (Gi)i∈[n] be n copies of G, where for each a ∈ [p], vertex a of Gi is “closed”
if Zia = (cid:63) and “open” otherwise.

Deﬁnition 5.2. The Markov blanket S(i,a) of the vertex a in graph Gi is the set of ﬁrst open nodes
encountered by all walks in Gi starting at vertex a.

Remark 5.3. The assumption (1 − α)(dmax − 1) < 1 in C2 corresponds to the threshold of the
Bernoulli site percolation process on the inﬁnite dmax-regular tree, and is used to control the size
of the Markov blanket of a given vertex.

Now to compute E{X iSc | X iS}, we exploit the graphical structure of the model. If an entry
(i, k) is missing, Xik is conditionally independent of all observed entries not in its Markov blanket,
given the latter. That is, consider a node Xi,k and let S(i, k) ⊆ S be the subsets of observed nodes
connected to Xi,k by paths which contain only missing nodes; see Figure 3. It follows from this
observation and the conditional independence structure of Gaussian graphical models that

(cid:98)Xik = E{Xik | X iS} = Σk,S(i,k)

(cid:0)ΣS(i,k),S(i,k)

(cid:1)−1X S(i,k).

Now we simply use a plug-in estimator for Σ in order to estimate the conditional expectations:

(cid:101)Xik = (cid:101)Σk,S(i,k)

(cid:0)

(cid:101)ΣS(i,k),S(i,k)

(cid:1)−1X S(i,k).

In order to deﬁne our estimator (cid:101)Σ of the covariance, we let the zero-imputed design matrix X 0

10

be such that

X0,(i,a) =

(cid:40)

Xia
0

if Zia observed,
otherwise.

We then take our estimator (cid:101)Σ of the covariance to be the empirical covariance matrix of the
zero-imputed design matrix X 0:

(cid:101)Σ =

1
α2n

n
(cid:88)

i=1

X 0,iX (cid:62)

0,i −

1 − α
α2n

n
(cid:88)

i=1

(cid:16)

diag

X 0,iX (cid:62)
0,i

(cid:17)

.

This is an unbiased estimator of the covariance matrix and is constructed by modifying the usual
empirical covariance to account for the missing data. Before stating the main theorem of this
section, we summarize the algorithm above:

Algorithm 1: Imputation and regression for sparse gaussian graphical models

Data: Matrix Z, vector y, adjacency matrix for graphical model A, probability of observed

entry α, parameter λ

Result: Estimated regression vector (cid:98)β
Initialize (cid:102)X = 0;
Compute (cid:101)Σ = 1
α2n
for i ∈ [n] do

i=1 X 0,iX (cid:62)

0,i − 1−α
α2n

(cid:80)n

(cid:80)n

i=1 diag (cid:0)X 0,iX (cid:62)

0,i

(cid:1);

for a ∈ [p] do

if M ia = 1 then
Let (cid:101)Xia = Zia;

end
else

Let Gi ← A ;
Color each node in Gi green if observed, red if missing ;
Let S(i, k) ← boundary of breadth-ﬁrst search in Gi started from node k and
terminated when each path reaches a green node;
(cid:101)ΣS(i,k),S(i,k)
Let (cid:101)Xia = (cid:101)Σk,S(i,k)

(cid:1)−1X S(i,k);

(cid:0)

end

end

end

return (cid:98)β ∈ arg minβ∈Rp

1
2n

(cid:13)
(cid:13)
(cid:13)y − (cid:102)Xβ

(cid:13)
2
(cid:13)
(cid:13)
2

+ λ (cid:107)β(cid:107)1

The strategy outlined above leads to the following theorem.

n ≤ C(α, dmax, c, c)
Theorem 6. Assume A2–A3, C1–C3, the data is MCAR(α), and
for C(α, dmax, c, c) ¿0. Then, there exist positive constants c1, c2, c3, C(α, dmax), such that with
probability at least 1 − c1n−1 − c2pc3, (cid:98)β as deﬁned in (1.3) with regularization parameter

(cid:113) log 2np

λ = C(α, dmax) (σ + R)

(cid:114)

s log np
n

,

satisﬁes

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

≤

C(α, dmax) (σ + R)
λmin(Σ ˆX )

s

(cid:114)

log 2np
n

.

(5.6)

11

s in the last display of the theorem. This
Remark 5.4. Observe the excess multiplicative factor
can be avoided using sample-splitting: using the ﬁrst n/2 samples to estimate the covariance and
the remaining n/2 for regression. This extra factor is a byproduct of the analysis of our procedure
which reuses in regression the data already used for covariance estimation.

√

6 Simulations

We now provide ﬁve numerical examples to support our theoretical ﬁndings:

1. Standard Gaussian design: LASSO. Here, we elaborate on the experiment to produce

Figure 1.

2. AR(1) Approximate conditional expectation. We generate data according to an AR(1)

model and then use our imputed LASSO after estimating the covariance of the model.

3. Banded inverse covariance approximate conditional expectation. We generate data
with a banded inverse covariance and then use the imputed LASSO after estimating the
covariance of the model.

4. Semi-synthetic data: gene expression. We use the gene expression cancer RNA-Seq
data from [DG19]. We artiﬁcially induce MCAR data, generate a synthetic regression vector
β0, and then perform our imputed LASSO.

5. Real data: communities and crime. We use the communities and crime data from [DG19].
We compare the prediction error of the LASSO with the prediction error of the imputed
LASSO. Although we impose MCAR data artiﬁcially, we no longer create a linear model;
rather, the dataset contains responses.

Our simulations make use of the package scikit-learn to compute the LASSO estimate.

6.1 Standard Gaussian design: LASSO

We ﬁrst simulate the simple setting in which each entry of the data matrix Xij ∼i.i.d N(0, 1). We
then form (cid:99)X = E{X | Z} by 0 imputation. That is,

(cid:98)Xij =

(cid:40)

Xij
0

if Zij = Xij,
if Zij = (cid:63).

We isolate the eﬀective noise caused by missing data by considering the noiseless setting (i.e.,
σ = 0). Additionally, we generate β0 with square root sparsity, setting each of the ﬁrst (cid:100)
p(cid:101)
entries of β0 to be 1 and the remaining ones to be 0. We set the regularization parameter to be

√

The results of the simulation, compared with the value λ

√

s are shown in Figure 1.

(cid:114)

λ =

α (1 − α) log p
n

.

12

6.2 AR(1) Approximate conditional expectation

We consider the AR(1) model of Section 5.1. Two cases are simulated: when the parameter φ is
known, and when it needs to be estimated. We consider the noiseless additive regime and plot our
results in Figure 4. In our simulations, we take

λ =

(cid:114)

1
α4 R

log p
n

.

We simulate three cases of the parameter φ = {0.05, 0.1, 0.15}.

6.3 Banded Inverse Covariance

Here, we simulate two cases: when the exact conditional expectation can be computed (the co-
variance is known), and when we need to estimate the covariance from the data (but the sparsity
pattern of the inverse covariance matrix is known). Using Σ = Ω−1, we take:

Ωij =

(cid:40)

φ|i−j|,
0,

if |i − j| ≤ 3
otherwise,

Again, we isolate the eﬀective noise caused by missing data by considering the noiseless setting.
We generate β0 in the same way as above. For these simulations, we take φ = 0.25 and we set the
regularization parameter

λ = λmax(ΣX )

(cid:114)

(1 − α) log p
n

We plot the empirical error as a function of α, using the known covariance and the empirical error
using the approximated covariance on the same plot. The results of the simulation are shown in
Figure 5, where we simulate n = 1000 and vary p = {600, 900, 1200}. The two curves show that
even without sample splitting, the quality of estimation is similar between the setting where ΣX is
known exactly and where it must be approximated.

6.4 Semi-synthetic data: gene expression

We simulate our imputation procedure on the gene expression cancer RNA-Seq data set from the
UCI repository [DG19]. The original data set contains 801 samples and has dimension 20532. Our
experiments randomly subsample the columns of the data and discard columns with small weight:
we are left with a random sample of size 936 in our simulations. We then center and normalize the
remaining data matrix and set this to be X. We generate a regression vector β0 which is one on
the ﬁrst

p coordinates and zero everywhere else. That is, we let

√

(β0)i =

(cid:40)
1
0

√

if i ≤
p,
otherwise,

and generate responses y according to y = Xβ0. Additionally, on the data matrix X, we use the
graphical LASSO, using the skggm package [LN17], to ﬁnd the precision matrix Ω and its associated
graphical model. Then, for each α, we generate Z by deleting each entry of X independently
with probability 1 − α. Accordingly, for each α, we ﬁnd (cid:99)X using the imputation described in
n . The results of
Section 5.2 and use the LASSO with regularization parameter λ = 0.1

(1 − α) log p

(cid:113)

13

Exact vs. Approximate Conditional Expectation, AR(1), φ = 0.05

2
(cid:13)(cid:13)(cid:13)
0
β
−
(cid:98)β
(cid:13)(cid:13)(cid:13)

100

10−0.2

10−0.4

0.7

100

2
(cid:13)(cid:13)(cid:13)
0
β
−
(cid:98)β
(cid:13)(cid:13)(cid:13)

10−0.2

10−0.4

Exact
Approx.

Exact
Approx.

0.8

0.9

1

AR(1), φ = 0.1

0.7

0.8

0.9

AR(1), φ = 0.15

2
(cid:13)(cid:13)(cid:13)
0
β
−
(cid:98)β
(cid:13)(cid:13)(cid:13)

100

10−0.2

10−0.4

Exact
Approx

0.7

0.8

0.9

α

1

1

Figure 4. Estimation error as a function of the density of observed entries α. Each plot shows the
performance of imputation with exact conditional expectation in black compared to the approximated
conditional expectation (where the parameter φ is estimated from data) in gray. This example used
n = 1000, p = 1200, s = (cid:100)
p(cid:101) = 35 (square root sparsity). Each data point is an average of 10
trials. The error of the known covariance example is shaded in solid between the best and worst error
values over 10 trials whereas the error of the approximated covariance is crosshatched between the
best and worst error values. Each plot corresponds to a diﬀerent value of the parameter φ.

√

the simulation are found in Figure 6. Notice that even using the approximated graphical model and
the imputed matrix, the LASSO is able to recover β0 reasonably well. For context, we note that
in this simulation, using the “oracle” LASSO with the data matrix X achieves error
=
0.124.

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

14

Exact vs. Approximate Conditional Expectation: Banded, p = 600

100

2
(cid:13)(cid:13)(cid:13)
0
β
−
(cid:98)β
(cid:13)(cid:13)(cid:13)

10−1

0.5

100

2
(cid:13)(cid:13)(cid:13)
0
β
−
(cid:98)β
(cid:13)(cid:13)(cid:13)

2
(cid:13)(cid:13)(cid:13)
0
β
−
(cid:98)β
(cid:13)(cid:13)(cid:13)

10−1

0.5

100

10−1

0.5

Exact
Approx.

0.6

0.7

0.8

0.9

1

Banded Covariance, p = 900

Exact
Approx.

0.6

0.7

0.8

0.9

1

Banded Covariance, p = 1200

Exact
Approx.

0.6

0.7

0.8

0.9

1

α

Figure 5. Estimation error as a function of the density of observed entries α. Each plot shows
the performance with exact conditional expectation imputation (in black) versus the performance
of approximate conditional expectation imputation (in gray) for varying values of the dimension p.
This example used n = 1000, s = (cid:100)
p(cid:101) (square root sparsity) for p = 600 in the top ﬁgure, p = 900
in the middle ﬁgure and p = 1200 in the bottom. Each data point is an average of 10 trials. The
error of the known covariance example is shaded in solid between the best and worst error values
over 10 trials whereas the error of the approximated covariance is crosshatched between the best and
worst error values.

√

6.5 Real data: communities and crime

Whereas in the last section we generated responses y according to a linear model, we will now
simulate our procedure on a dataset “Communities and crime” from the UCI repository [DG19]
that contains response variables. The original dataset contains 2215 samples and has dimension
147. In order to isolate the eﬀect of missing data, we will ﬁrst remove the samples which have
any missing entries. We additionally remove linearly dependent columns and are left with a data
matrix X with n = 342, p = 123. We now perform two simulations, shown in ﬁgure 7:

15

Approximate Imputation Gene Expression Data

10−0.2

2
(cid:13)(cid:13)(cid:13)
0
β
−
(cid:98)β
(cid:13)(cid:13)(cid:13)

10−0.4

10−0.6

10−0.8

0.8

Empirical Error

0.9
Probability of Observing an Entry (α)

1

Figure 6. Error as a function of the density of observed entries α. This example used the gene
expression cancer RNA-Seq data set from the UCI repository [DG19]. n = 801, p = 936, s = (cid:100)
p(cid:101)
(square root sparsity). Each data point is an average of 10 trials. The error is shaded between the
best and worst error values over the 10 trials for each point.

√

1. We ﬁrst perform an “oracle” simulation in which there is no missing data. We vary the
regularization parameter λ over the interval (0, 0.9) and perform the following 200 times. We
randomly take 80% of the data for training and leave 20% of the data as holdout. We plot
the prediction error on the 20% test set and plot the average value as well as the standard
error.

2. For various values of α we perfrom a simulation with missing data. We again vary the
regularization parameter over the same interval and perform the following 50 times. We
randomly perform an 80/20 split in the same manner as before. This time on the training
data, we run the graphical LASSO to ﬁnd the sparsity pattern. We then erase 1 − α fraction
of the training data and run the approximate conditional expectation imputed LASSO. We
plot the average prediction error on the 20 percent test set as well as the standard error.

As shown in ﬁgure 7, the performance does not degrade much as a function of α.

7 Conclusion

We have studied high-dimensional linear regression in the presence of missing data. In contrast to
previous theoretical work in this setting, we focus on the imputation strategy, followed by simple
oﬀ-the-shelf estimation procedures. Imputation by conditional expectation is shown to retain the
following properties:

1. Rate-optimality. Obtained broadly with respect to the dimension, and with respect to

every parameter when the covariance of the data is the identity matrix.

2. Pivotal. The square-root LASSO retains rate-optimal statistical guarantees and is pivotal

with respect to the radius of the problem R and the noise variance σ.

16

Approximate Imputation Communities and Crime Data

2 2
(cid:13)(cid:13)(cid:13)
y
−
(cid:98)β
X
(cid:13)(cid:13)(cid:13)
1n

r
o
r
r
e

n
o
i
t
c
i
d
e
r
P

1

0.8

0.6

0.4

0.2

0

0.1

0.2

α = 0.5
α = 0.75
α = 1

0.8

0.9

1

0.3

0.4

0.5
Regularization parameter (λ)

0.7

0.6

Figure 7. Prediction error for induced missing data as well as the oracle model with full data for the
“communities and crime” dataset. For each λ, the gray closed circles represent the average prediction
error for the “oracle” model across 200 random splits of the data. The open gray circles represent
the average prediction error for approximate conditional expectation imputation across 50 random
splits of the data when 0.75-fraction of the data is kept, and the black circles when 0.5-fraction of
the data is kept. The error bars give the standard error.

3. Robust to unknown covariance. An approximated covariance suﬃces for the purposes of

the imputed LASSO when the data comes from a sparse gaussian graphical model.

Several potential future directions remain. For instance, it is unclear what a theoretically
principled way to handle missing data is in, say, the generalized linear model. Additionally, we
have given coarse, non-asymptotic bounds for the linear model in the high-dimensional regime. It
would be interesting to characterize the exact asymptotic performance of our procedures in the
proportional regime.

References

[Ant10]

Anestis Antoniadis, Comments on: l1-penalization for mixture regression models. 7

[ASSS19] Anish Agarwal, Devavrat Shah, Dennis Shen, and Dogyoon Song, Model agnostic high-

dimensional error-in-variable regression, arXiv preprint arXiv:1902.10920 (2019). 5

[BCK+17] Alexandre Belloni, Victor Chernozhukov, Abhishek Kaul, Mathieu Rosenbaum, and
Alexandre B Tsybakov, Pivotal estimation via self-normalization for high-dimensional
linear models with error in variables, arXiv preprint arXiv:1708.08353 (2017). 4

[BCW11] Alexandre Belloni, Victor Chernozhukov, and Lie Wang, Square-root lasso: pivotal
recovery of sparse signals via conic programming, Biometrika 98 (2011), no. 4, 791–
806. 3, 7, 30

[BLM13]

St´ephane Boucheron, G´abor Lugosi, and Pascal Massart, Concentration inequalities:
A nonasymptotic theory of independence, Oxford university press, 2013. 37

17

[BRT+09] Peter J Bickel, Yaacov Ritov, Alexandre B Tsybakov, et al., Simultaneous analysis of
lasso and dantzig selector, The Annals of Statistics 37 (2009), no. 4, 1705–1732. 2, 19,
20

[BRT17]

Alexandre Belloni, Mathieu Rosenbaum, and Alexandre B Tsybakov, Linear and conic
programming estimators in high dimensional errors-in-variables models, Journal of the
Royal Statistical Society: Series B (Statistical Methodology) 79 (2017), no. 3, 939–956.
4

[CC13]

[CD13]

[CT07]

Yudong Chen and Constantine Caramanis, Noisy and missing data regression:
Distribution-oblivious support recovery, International Conference on Machine Learn-
ing, 2013, pp. 383–391. 4

Emmanuel J Candes and Mark A Davenport, How well can we estimate a sparse
vector?, Applied and Computational Harmonic Analysis 34 (2013), no. 2, 317–323. 2

Emmanuel Candes and Terence Tao, The dantzig selector: Statistical estimation when
p is much larger than n, The Annals of Statistics 35 (2007), no. 6, 2313–2351. 2

[DG19]

Dheeru Dua and Casey Graﬀ, UCI machine learning repository, 2019. 12, 13, 15, 16

[DZ+17]

Abhirup Datta, Hui Zou, et al., Cocolasso for high-dimensional error-in-variables re-
gression, The Annals of Statistics 45 (2017), no. 6, 2400–2426. 4

[EvdG18] Andreas Elsener and Sara van de Geer, Sparse spectral estimation with missing and

corrupted measurements, arXiv preprint arXiv:1811.10443 (2018). 5

[FJSZ19]

Roger Fan, Byoungwook Jang, Yuekai Sun, and Shuheng Zhou, Precision matrix esti-
mation with noisy and missing data, arXiv preprint arXiv:1904.03548 (2019). 5

[God81]

Christopher David Godsil, Matchings and walks in graphs, Journal of Graph Theory 5
(1981), no. 3, 285–297. 54

[Gra06]

[KX12]

[LN17]

[Lou13]

Robert M Gray, Toeplitz and circulant matrices: A review, Foundations and Trends R(cid:13)
in Communications and Information Theory 2 (2006), no. 3, 155–239. 45

Mladen Kolar and Eric P Xing, Estimating sparse precision matrices from data with
missing values, Proceedings of the 29th International Conference on Machine Learning,
Edinburgh, Scotland, UK, 2012. 5, 9

Jason Laska and Manjari Narayan, skggm 0.2.7: A scikit-learn compatible package for
Gaussian and related Graphical Models, July 2017. 13

Karim Lounici, Sparse principal component analysis with missing observations, High
dimensional probability VI, Springer, 2013, pp. 327–356. 5

[Lou14]

, High-dimensional covariance matrix estimation with missing observations,

Bernoulli 20 (2014), no. 3, 1029–1058. 5

[LR14]

Roderick JA Little and Donald B Rubin, Statistical analysis with missing data, vol.
333, John Wiley & Sons, 2014. 2, 4

18

[LW12a]

Po-Ling Loh and Martin J Wainwright, Corrupted and missing predictors: Minimax
bounds for high-dimensional linear regression, Information Theory Proceedings (ISIT),
2012 IEEE International Symposium on, IEEE, 2012, pp. 2601–2605. 4

[LW12b]

, High-dimensional regression with noisy and missing data: Provable guarantees

with nonconvexity, The Annals of Statistics 40 (2012), no. 3, 1637. 4, 34

[RT10]

[RT13]

Mathieu Rosenbaum and Alexandre B Tsybakov, Sparse recovery under matrix uncer-
tainty, The Annals of Statistics 38 (2010), no. 5, 2620–2651. 4

, Improved matrix uncertainty selector, From Probability to Statistics and Back:
High-Dimensional Models and Processes–A Festschrift in Honor of Jon A. Wellner,
Institute of Mathematical Statistics, 2013, pp. 276–290. 4

[RWRY11] Pradeep Ravikumar, Martin J Wainwright, Garvesh Raskutti, and Bin Yu, High-
dimensional covariance estimation by minimizing (cid:96)1-penalized log-determinant diver-
gence, Electronic Journal of Statistics 5 (2011), 935–980. 9

[RWY11] Garvesh Raskutti, Martin J Wainwright, and Bin Yu, Minimax rates of estimation
for high-dimensional linear regression over (cid:96)q-balls, IEEE transactions on information
theory 57 (2011), no. 10, 6976–6994. 2

[RZ17]

[Tib96]

[Ver18]

Mark Rudelson and Shuheng Zhou, Errors-in-variables models with dependent mea-
surements, Electronic Journal of Statistics 11 (2017), no. 1, 1699–1797. 4

Robert Tibshirani, Regression shrinkage and selection via the lasso, Journal of the
Royal Statistical Society. Series B (Methodological) (1996), 267–288. 2

Roman Vershynin, High-dimensional probability: An introduction with applications in
data science, vol. 47, Cambridge University Press, 2018. 30, 35, 36, 44, 45, 51

[WWBS17] Yining Wang, Jialei Wang, Sivaraman Balakrishnan, and Aarti Singh, Rate optimal
estimation and conﬁdence intervals for high-dimensional regression with missing co-
variates, arXiv preprint arXiv:1702.02686 (2017). 4, 6

A Proofs for imputation by conditional expectation

We now prove the main results of section 3. We begin by overviewing the proof technique. We
note will explicitly write constants in this section; in subsequent, more complicated sections, we
will drop this convention for readability. The remainder of this section is organized as follows:

1. Proof of theorem 1. This is provided in subsection A.1.

2. Proof of corollary 3.1. This is provided in subsection A.2.

3. Proof of theorem 3. This is provided in subsection A.3

Our proofs extend the technique ﬁrst used in [BRT+09]. We recall brieﬂy the set-up. We assume
the linear model: y = Xβ0 + (cid:15). We observe Z, an in-exact version of X and the response vector

19

y. The set-up of [BRT+09] assumes exact knowledge of X and the response vector y and analyzes
the properties of the LASSO estimator:

(cid:98)βLas ∈ arg min

β∈Rp

(cid:26) 1
2n

(cid:107)y − Xβ(cid:107)2

2 + λ (cid:107)β(cid:107)1

(cid:27)

We will consider a matrix (cid:102)X which is “close” to X and analyze the properties of:

(cid:98)β ∈ arg min

β∈Rp

(cid:26) 1
2n

(cid:13)
(cid:13)
(cid:13)y − (cid:102)Xβ

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:27)

+ λ (cid:107)β(cid:107)1

In particular, we have the following proposition, whose proof we provide in Appendix D:

Proposition A.1. Given y = Xβ0 + (cid:15) and (cid:102)X ∈ Rn×p and λ > 0, consider the solution to the
convex program

Further, assume

(cid:101)β ∈ arg min

β∈Rp

(cid:26) 1
2n

(cid:13)
(cid:13)
(cid:13)y − (cid:102)Xβ

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:27)

+ λ (cid:107)β(cid:107)1

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:102)X

(cid:102)X − X

and

inf
w∈C∩Sp−1

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

λ
4

,

1
n

(cid:13)
(cid:13)
(cid:13)(cid:102)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

≥ κ,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:15)

(cid:102)X

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

λ
4

,

where C is the cone C = {w ∈ Rp : (cid:107)wT c(cid:107)1 ≤ 3 (cid:107)wT (cid:107)1}. Then

(cid:13)
(cid:13)
(cid:13)(cid:101)β − β0

(cid:13)
(cid:13)
(cid:13)2

√

≤ 12λ
κ

s

.

Remark A.1. Observe that taking (cid:102)X = X recovers the setting of [BRT+09]. Of course, X is not
available in the context of this paper.

We are now prepared to proceed with the proofs of our main results of section 3.

A.1 Proof of theorem 1

We recall the statement for the reader’s convenience:

Theorem 1. Assume A1–A4 and that the data is MCAR(α) for α < 1. Assume additionally that
(cid:113) log p

1 − α for a positive constant c1. Then, there exist positive constants c2, c3, c4 such

√

n ≤ c1

that with probability at least 1 − c2p−c3, (cid:98)β as deﬁned in (1.3) with regularization parameter

λ = c4

(cid:0)σX σ + σ2

X

(cid:114)

√

1 − αR(cid:1)

log p
n

,

satisﬁes

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

√

(cid:46) σX σ + σ2

X
λmin(Σ

1 − αR
(cid:99)X )

(cid:114)

s log p
n

.

(A.1)

20

Proof. We will take a constant A >

√

2 and deﬁne

(cid:16)

λ = 4A

8eσX σ + 16

√

2eσ2
X

√

1 − αR

(cid:17) (cid:114)

log p
n

.

(A.2)

Using this choice, we establish the following lemmas, each of which controls one of the two (cid:96)∞
terms required by proposition A.1. The proofs of these lemmas are provided in appendix E.

Lemma A.2. Assuming λ as deﬁned in equation (A.2), and the assumptions of Theorem 1, we
have

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:17)

(cid:99)X

(cid:99)X − X

β0

≥

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:27)

λ
4

≤ 2p1− A2
2 .

Lemma A.3. Assuming λ as deﬁned in equation (A.2), and the assumptions of Theorem 1, we
have

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:99)X

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

≤ 2p1− A2
2 .

We now need to control the so-called restricted eigenvalue, that is inf w∈C∩Sp−1

this end, we have the following fact that we will use repeatedly:

1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

. To

Fact A.4. Assume that the rows of a matrix X are σ2
the random vector (cid:99)X i = E {X i | X S} is sub-Gaussian with parameter σ2
X .
Proof of Fact A.4.

X sub-Gaussian. Then, for any set S ⊆ [p],

eλ(cid:104)u,(cid:99)X i(cid:105)(cid:111)
(cid:110)

E

≤ E

(cid:110)

E

(cid:110)

eλ(cid:104)u,X i(cid:105) | X S

(cid:111)(cid:111)

= E

eλ(cid:104)u,X i(cid:105)(cid:111)
(cid:110)

≤ eλ2σ2
X .

We have the following proposition, whose proof is provided in appendix D.

Proposition A.5. Let X ∈ Rn×p be a matrix with i.i.d. rows, each of which is sub-Gaussian with
parameter σ2
X and has covariance matrix ΣX . Additionally, let S ⊆ [p] with |S| = s. Then X
satisﬁes the restricted eigenvalue condition

inf
w∈C∩Sp−1

1
n

(cid:107)Xw(cid:107)2

2 ≥

λmin (ΣX )
2

,

with probability at least

(cid:40)

(cid:32)

(cid:32)

1 − 2 exp

−cn

min

λmin (ΣX )2
σ4
X

,

λmin (ΣX )
σ2
X

(cid:33)

(cid:33)(cid:41)

,

−

s log p
n

where C is the cone C = {w ∈ Rp : (cid:107)wSc(cid:107)1 ≤ 3 (cid:107)wS(cid:107)1}.

Let us now conclude the proof of theorem 1. Let ARE denote the event that inf w∈C∩Sp−1

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)(cid:99)Xw
(cid:13)
2
λmin(ΣX )
where we take the set S in the assumptions of proposition A.5 to be T . Then the assump-
2
tions of the theorem, fact A.4 and proposition A.5 imply that P{ARE} ≥ 1 − 2p−c3, where c3 is a
constant. Additionally, let A∞ denote the event that the results of lemmas A.2, A.3 hold. Then,
on the event A∞ ∩ ARE, which holds with probability at least 1 − 2pc1 with c1 = (2 − A2) ∨ c3,
proposition A.1 with λ as in (A.2), κ =

implies the result.

λmin(Σ

(cid:99)X )

1
n

2

≥

21

A.2 Proof of corollary 3.1

We recall the statement for the reader’s convenience:

(cid:113) log p

Corollary 3.1. Assume the data is MCAR(α) for α < 1 and Xij ∼iid N(0, 1). Assume additionally
(cid:112)α(1 − α) for a positive constant c1. Then, there exist positive constants c2, c3, c4
that
such that with probability at least 1 − c2p−c3, (cid:98)β as deﬁned in (1.3) with regularization parameter

n ≤ c1

√

α (cid:0)σ +

√

1 − αR(cid:1)

λ = c4

(cid:114)

log p
n

,

satisﬁes

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

(cid:46) σ +

(cid:114)

√
√

1 − αR
α

s log p
n

.

(A.3)

Before we embark on the proof we note some key diﬀerences between corollary 3.1 and theorem 1.

First, since the data is i.i.d. N(0, 1), the imputation matrix (cid:99)X is drastically simpliﬁed:

(cid:98)Xia =

(cid:40)

Xia
0

if Zia observed,
otherwise .

Immediately, we are able to see that in this model, σX = 1, Σ
(cid:99)X = αI p×p. Finally, the indepen-
dence between each entry in the data matrix allows us to get better control in the concentration
inequalities we use to prove the (cid:96)∞ bounds needed for proposition A.1. We now proceed to the
proof.

Proof. We set

λ = 4A

√

α

(cid:16)

√
8

√

2eσ + 32e

1 − αR

(cid:17) (cid:114)

log p
n

.

(A.4)

2. The proof will use two supplementary lemmas whose proofs are provided in ap-

√

with A >
pendix E:

Lemma A.6. Assuming λ as deﬁned in equation (A.4), and the assumptions of corollary 3.1, we
have

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:17)

(cid:99)X

(cid:99)X − X

β0

≥

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:27)

λ
4

≤ 2p1− A2
2 .

Lemma A.7. Assuming λ as deﬁned in equation (A.4), and the assumptions of corollary 3.1, we
have

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:99)X

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

≥ 2p1− A2
2 .

Exactly as in the proof of theorem 1, we let ARE denote the event that inf w∈C∩Sp−1

λmin(Σ

(cid:99)X )

2

= α

2 . Then the assumptions of the corollary, fact A.4 and proposition A.5 imply P {ARE} ≥
1 − 2p−c3. Letting A∞ denote the event that the results of lemmas A.6, A.7 hold, we immediately
see that proposition A.1 holds with probability at least 1 − 2p−c1, taking λ as in (A.4), κ = α
2 and
the result follows immediately.

1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

≥

22

A.3 Proof of theorem 3

We re-state the theorem for the reader’s convenience:

Theorem 3. Assume that the data is MNAR and A1–A4. Assume additionally that
n ≤ c1
for a positive constant c1. Then, there exist positive constants c2, c3, c4 such that with probability
at least 1 − c2p−c3, (cid:98)β as deﬁned in Eq. (1.3) with (cid:99)X = E {X | Z} and regularization parameter

(cid:113) log p

satisﬁes

λ = c4

(cid:0)σX σ + σ2

X R(cid:1)

(cid:114)

log p
n

,

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

(cid:46) σX σ + σ2
X R
λmin(Σ
(cid:99)X )

(cid:114)

s log p
n

.

Proof. Again, noting the freedom of the universal constant C, we take (for A >

λ = 4A (cid:0)8eσX σ + 480eσ2

X R(cid:1)

(cid:114)

log p
n

.

We then use the following two lemmas:

√

2):

(A.5)

(A.6)

Lemma A.8. Assuming λ as deﬁned in equation (A.6), and the assumptions of corollary 3, we
have:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:17)

(cid:99)X

(cid:99)X − X

β0

≥

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:27)

λ
4

≤ 6p1− A2
2 .

Lemma A.9. Assuming λ as deﬁned in equation (A.6), and the assumptions of corollary 3, we
have:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:99)X

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

≤ 2p1− A2
2 .

Using these two lemmas, the theorem statement follows exactly as the proof of theorem 1.

B Proofs for imputation by approximate conditional expectation

We now prove the main results of section 5. We will ﬁrst describe the proof technique; the remainder
of the section contains:

1. Proof of theorem 5. This is provided in subsection B.1.

2. Proof of theorem 6. This is provided in subsection B.2

Proof strategy. Recall that we observe the pair (y, Z) where y = Xβ0 + (cid:15). The proofs of
appendix A used (cid:99)X = E {X | Z} as a proxy for X in the LASSO estimator given in (1.3). The proof
then followed by validating the assumptions of proposition A.1 (repeated here for convenience):

Proposition A.1. Given y = Xβ0 + (cid:15) and (cid:102)X ∈ Rn×p and λ > 0, consider the solution to the
convex program

(cid:101)β ∈ arg min

β∈Rp

(cid:26) 1
2n

(cid:13)
(cid:13)
(cid:13)y − (cid:102)Xβ

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:27)

+ λ (cid:107)β(cid:107)1

.

23

Further, assume

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:102)X

(cid:102)X − X

and

inf
w∈C∩Sp−1

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

λ
4

,

1
n

(cid:13)
(cid:13)
(cid:13)(cid:102)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

≥ κ,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:15)

(cid:102)X

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

λ
4

,

where C is the cone C = {w ∈ Rp : (cid:107)wSc(cid:107)1 ≤ 3 (cid:107)wS(cid:107)1}. Then

(cid:13)
(cid:13)
(cid:13)(cid:101)β − β0

(cid:13)
(cid:13)
(cid:13)2

√

≤ 12λ
κ

s

.

We will henceforth use (cid:102)X to denote the approximate conditional expectation and (cid:99)X to denote
the true conditional expectation. There are two key diﬀerences in carrying out the steps required
for proposition A.1:

1. Since (cid:102)X is not the true conditional expectation, it loses important properties of (cid:99)X such as

orthogonality and sub-Gaussianity

2. (cid:102)X is computed using all of the data, and thus its rows are not independent! This lies in stark

contrast to (cid:99)X which retains independence of the rows.

The reader may recall that in appendix A, we used heavily the independence of the rows of (cid:99)X to
invoke standard concentration inequalities. The main technical challenge of this section is working
around the fact that the rows of (cid:102)X are no longer independent. We work around this in a simple
way, by writing (cid:102)X = ((cid:102)X − (cid:99)X)
. The strategy is then to decouple these two terms, argue
(cid:125)

(cid:124)

+ (cid:99)X
(cid:124)(cid:123)(cid:122)(cid:125)
B.

that term A. is “small” in an appropriate sense and inherit the analysis of appendix A for term B.

(cid:123)(cid:122)
A.

B.1 Proof of theorem 5

We re-state the theorem here for convenience:

1
Theorem 5. Assume A1–A4, the data is MCAR(α), that the sample size n ≥ c1
α8 s log p for
a positive constant c1, and that the rows of X are generated from the stationary auto-regressive
process described above with |φ| < 1. Then, there exist positive constants c2, c3, c4 such that with
probability at least 1 − c2p−c3. (cid:98)β as deﬁned in (1.3), using (cid:102)X in place of (cid:99)X, with regularization
parameter

λ = c4

(cid:18) σX σ

α2 +

σ2
X
α4 R

(cid:19) (cid:114)

log p
n

,

satisﬁes:

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

(cid:46)

(cid:114)

s log p
n

.

(cid:16) σX σ

(cid:17)

X

α2 + σ2
α4 R
λmin(Σ
(cid:99)X )
(cid:13)
(cid:17)
≤ λ
(cid:13)
(cid:13)∞

β0

T (cid:16)

(cid:13)
(cid:13)
(cid:13)

1
n (cid:102)X

(cid:102)X − X

Proof. We begin by showing
inequality implies:

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:102)X

(cid:102)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

4 with high probability. The triangular

≤

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n
1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:99)X − X

(cid:17)

24

(cid:16)

1
n
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

β0

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:17)

(cid:102)X − (cid:99)X

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:99)X − X

(cid:17)

(cid:13)
(cid:13)
β0
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

β0

.

(B.1)

(B.2)

Recall that our strategy is to prove that terms involving (cid:102)X − (cid:99)X are small. It is thus useful to recall
the construction of (cid:102)X. The true covariance matrix is (ΣX )ij = 1
1−φ2 φ|i−j|. We then take (cid:98)Xik as in
equation (5.2) and (cid:101)Xik as in equation (5.3):

(cid:98)Xik =

(cid:101)Xik =

φd1+d2
1 − φ2(d1+d2)
ˆφd1+d2
1 − ˆφ2(d1+d2)

(cid:16)

(cid:16)

Xi,L(k)

(cid:16)

φ−d2 − φd2

(cid:17)

+ Xi,R(k)

(cid:16)

φ−d1 − φd1

(cid:17)(cid:17)

Xi,L(k)

(cid:16) ˆφ−d2 − ˆφd2

(cid:17)

+ Xi,R(k)

(cid:16) ˆφ−d1 − ˆφd1

(cid:17)(cid:17)

,

where d1 = k − L(k) and d2 = R(k) − k with L(k) and R(k) the positions of the closed observed
entries to the left and right of k, respectively, and we have deﬁned (cid:98)φ as in equation 5.1:

1
α2np

(cid:98)φ =

(cid:80)n

(cid:80)p−1

i=1
1
αnp

a=1 XiaXi(a+1)MiaMi(a+1)
(cid:80)n

(cid:80)p−1

a=1 X 2

iaMia

i=1

.

The following lemma, whose proof can be found in F.2, is the key ingredient to showing (cid:102)X − (cid:99)X
is “small”.
Lemma B.1. Deﬁne ˆφ as in (5.1). Then under the assumptions of theorem 5,

(cid:40)
(cid:12)
(cid:12)
(cid:12)

P

ˆφ − φ

(cid:12)
(cid:12)
(cid:12) ≥

4
α2

(cid:115)

(cid:41)

log p
np

≤ cp−C.

where c, C > 0 are universal constants.

T (cid:16)

Step 1: Control

(cid:13)
(cid:13)
(cid:13)
defer to appendix F.
Lemma B.2. Under the assumptions of theorem 5, for any u ∈ Rp, we have:

(cid:102)X − X

(cid:13)
(cid:13)
(cid:13)∞

1
n (cid:102)X

β0

(cid:17)

. We require the following four lemmas, whose proofs we

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥

Cσ2
X
α2 (cid:107)u(cid:107)1

(cid:115)

(cid:41)

log p
np

≤ c0p−c1,

where C, c0, c1 denote universal constants.

The proof of the above lemma yields the identitical corollary:

Corollary B.3. Under the assumptions of theorem 5, for any u ∈ Rp, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T

(cid:13)
(cid:13)
(cid:99)Xu
(cid:13)
(cid:13)∞

≥

Cσ2
X
α2 (cid:107)u(cid:107)1

(cid:115)

(cid:41)

log p
np

≤ c0p−c1

where C, c0, c1 denote universal constants.

Lemma B.4. Under the assumptions of theorem 5, we have:

(cid:16)

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C

σ2
X
α4 (cid:107)u(cid:107)1

log p
np

(cid:27)

≤ c0p−c1,

where C, c0, c1 denote universal constants.

25

Lemma B.5. Under the assumptions of theorem 5, for any u ∈ Rp, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:99)X − X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C

σ2
X
α2 (cid:107)u(cid:107)1

(cid:115)

(cid:41)

log p
np

≤ c0p−c1,

where C, c0, c1 denote universal constants.

Lemma B.6. Assuming λ as deﬁned in equation (B.1), and the assumptions of theorem 5, we
have, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

T (cid:16)

1
n

(cid:99)X

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ Cσ2

X R

(cid:114)

(cid:41)

log p
n

≤ c0p−c1,

where c0, c1 denote universal constants.

In lemmas B.2 - B.5, let u = β0 and notice that (cid:107)β0(cid:107)1 ≤

and the fact that s ≤ p then imply:

√

sR by Cauchy Schwarz. Lemma B.2

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥

Cσ2
X
α2 R

(cid:114)

(cid:41)

log p
n

≤ c0p−c1,

Similarly, lemma B.5 implies

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C

(cid:41)

(cid:114)

σ2
X
α2 R

log p
n

≤ c0p−c1,

lemma B.4 and the assumption that
is small enough) yields:

(cid:113) log p

n < 1 (this happens as long as c1 in the theorem statement

(cid:16)

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C

(cid:27)

σ2
X
α4 R

log p
n

≤ c0p−c1.

Picking the universal constant C in (B.1) large enough implies that each of the preceeding events, as
well as that of lemma B.6 hold with probability at least 1−c0p−c1 (where we remind the reader that
n ≥
universal constants c0, c1 may change line to line). This and noting that C
C σ2

imply immediately that P

(cid:17) (cid:113) log p

(cid:16) σX σ

α4 R

T (cid:16)

(cid:111)

(cid:17)

X

(cid:102)X − X

β0

(cid:113) log p
n

X

α4 R

(cid:110)(cid:13)
(cid:13)
(cid:13)

1
n (cid:102)X

α2 + σ2
≥ 1 − c0p−c1.
(cid:13)
(cid:13)
(cid:13)∞
T

≤ λ

(cid:13)
(cid:13)
(cid:13)

1
n (cid:99)X

(cid:15)

(cid:13)
(cid:13)
(cid:13)∞

≤ λ
4

(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
1
(cid:13)
n (cid:102)X
(cid:13)
(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

+

T

(cid:15)

4 . To this end, the

. We will use the

Step 2: Control

(cid:13)
(cid:13)
(cid:13)

T

(cid:15)

1
n (cid:102)X

triangular inequality implies

following two lemmas:

(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)

≤ λ

1
n (cid:102)X

4 We now tackle showing
(cid:16)

(cid:17)T

T

(cid:15)

(cid:13)
(cid:13)
(cid:13)∞

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

Lemma B.7. Assuming λ as deﬁned in equation (B.1), and the assumptions of theorem 5, we
have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:102)X − (cid:99)X

(cid:17)T

(cid:15)

(cid:16)

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:114)

(cid:41)

log p
n

≤ c0p−c1,

where C, c0, c1 are universal constants.

≥ C

σX σ
α2

26

Lemma B.8. Assuming λ as deﬁned in equation (B.1), and the assumptions of theorem 5, we
have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

T

(cid:15)

(cid:99)X

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ CσX σ

≥ c0p−c1,

(cid:114)

(cid:41)

log p
n

where C, c0, c1 are universal constants.

Picking the universal constant C in the deﬁnition of λ (B.1), noting that C

(cid:113) log p

C σX σ
α2
1 − c0p−c1.

n , and lemmas B.7, B.8 imply immediately

(cid:13)
(cid:13)
(cid:13)

T

1
n (cid:102)X

(cid:13)
(cid:13)
(cid:15)
(cid:13)∞

(cid:16) σX σ

α2 + σ2

α4 R

X

(cid:17) (cid:113) log p

n ≥

≤ λ

4 with probability at least

Step 3: Control inf w∈C∩Sp−1

1
n

(cid:13)
(cid:13)
(cid:13)(cid:102)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

We would now like to analyze inf w∈C∩Sp−1
(cid:69)

(cid:68)

(cid:17)

T

(cid:16)

(cid:102)X − (cid:99)X

+ (cid:99)X and 1
n

=

w, 1

n (cid:102)X

(cid:102)Xw

(cid:13)
(cid:13)
(cid:13)(cid:102)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

1
n

(cid:13)
(cid:13)
(cid:13)(cid:102)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

. Recalling the useful relation (cid:102)X =

, we note that:

(cid:28)

w,

1
n

T

(cid:102)X

(cid:102)Xw

(cid:29)

(cid:28)

=

w,

(cid:28)

+

w,

1
n
1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

(cid:29)

(cid:28)

w

+

w,

T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

(cid:29)

(cid:28)

w

+

w,

T

(cid:99)X

1
n

(cid:99)X

(cid:99)Xw

.

(cid:102)X − (cid:99)X

(cid:17)T

(cid:29)

(cid:99)Xw

(cid:16)

1
n

(cid:29)

Notice that by H¨older’s inequality, for any matrix A, (cid:104)w, Aw(cid:105) ≥ − (cid:107)w(cid:107)1 (cid:107)Aw(cid:107)∞. This then
implies that:

(cid:28)

w,

1
n

(cid:29)

T

(cid:102)X

(cid:102)Xw

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:16)

1
n

(cid:17)

w

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

− (cid:107)w(cid:107)1

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T

(cid:99)Xw

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ − (cid:107)w(cid:107)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

− (cid:107)w(cid:107)1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
n

(cid:99)X

T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

w

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

+

1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

.

We will lower bound each of these four terms, starting with the last (and recalling that w ∈ C∩S p−1).

≥ inf w∈C∩Sp−1

(cid:13)
(cid:13)
2
1. 1
(cid:13)
(cid:13)
(cid:13)(cid:99)Xw
(cid:13)
n
2
λmin(Σ
(cid:99)X )
. Then the assumptions of the theorem, fact A.4, and proposition A.5 imply that
P {ARE} ≥ 1 − c0p−c1.

. Let ARE denote the event that inf w∈C∩Sp−1

(cid:13)
(cid:13)
(cid:13)(cid:99)Xw

(cid:13)
(cid:13)
(cid:13)(cid:99)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
2
(cid:13)
(cid:13)
2

≥

1
n

1
n

2

2. − (cid:107)w(cid:107)1

(cid:16)

(cid:13)
(cid:13)
1
(cid:13)
n
(cid:13)
implies (cid:107)w(cid:107)1 ≤ 4

(cid:102)X − (cid:99)X
√

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

w

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

. Recall that since w ∈ C ∩ S p−1, Cauchy Schwarz

Noting that s ≤ p and using the fact that
1 − c0p−c1,

n < 1, we see that with probability at least

s. Additionally, taking u as w in lemma B.5 gives:

(cid:16)

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:102)X − (cid:99)X

≥ C

(cid:27)

σ2
X
α4

s log p
np

≤ c0p−c1.

(cid:17)

w

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:113) log p

− (cid:107)w(cid:107)1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:16)

1
n

27

(cid:17)

w

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ −C

(cid:114)

σ2
X
α4

s log p
n

.

3. − (cid:107)w(cid:107)1

(cid:16)

1
n

(cid:102)X − (cid:99)X

(cid:17)T

(cid:99)Xw

. Taking u as w in cor B.3, using (cid:107)w(cid:107)1 ≤ 4

√

s and s ≤ p, we

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

see that with probability at least 1 − c0p−c1,

− (cid:107)w(cid:107)1

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T

(cid:99)Xw

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ −C

(cid:114)

σ2
X
α2

s log p
n

.

4. − (cid:107)w(cid:107)1

(cid:13)
(cid:13)
(cid:13)

T (cid:16)

1
n (cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

w

(cid:13)
(cid:13)
(cid:13)∞

. Taking u as w in lemma B.2, using (cid:107)w(cid:107)1 ≤ 4

√

s and s ≤ p,

we see that with probability at least 1 − c0p−c1,

− (cid:107)w(cid:107)1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

w

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ −C

(cid:114)

σ2
X
α2

s log p
n

.

Now, under the assumption that n > C σ4
X
α8

(cid:16)

1

λmin(Σ

(cid:99)X )

that with probability at least 1 − c0p−c1, inf w∈C∩Sp−1

(cid:17)2

s log p for suﬃciently large C, we have shown

1
n

(cid:13)
(cid:13)
(cid:13)(cid:102)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

≥

λmin(Σ

(cid:99)X )

4

. To conclude, combine

the results of steps 1-3 and invoke proposition A.1 with λ as in (B.1) and κ =

λmin(Σ

(cid:99)X )

.

4

B.2 Proof of theorem 6

We re-state the theorem here for convenience:

n ≤ C(α, dmax, c, c)
Theorem 6. Assume C1–C3, A2–A3, the data is MCAR(α), and that
for positive constant C(α, dmax, c, c). Then, there exist positive constants c1, c2, c3, C(α, dmax), such
that with probability at least 1 − c1n−1 − c2pc3, (cid:98)β as deﬁned in (1.3) with regularization parameter

(cid:113) log 2np

λ = C(α, dmax) (σ + R)

(cid:114)

s log np
n

,

satisﬁes

(cid:114)

≤

(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

C(α, dmax) (σ + R)
λmin(Σ ˆX )
Proof. The strategy is then largely the same as Theorem 5. We will rely on the simple equality
(cid:102)X = (cid:99)X +
. We will be able to control the diﬀerence (cid:101)Xia − (cid:98)Xia. Recall the Markov
blanket S(i,a) (5.2). Since each row X i is multivariate Gaussian, we have:
(cid:17)

log 2np
n

(cid:102)X − (cid:99)X

(B.3)

(cid:16)

(cid:16)

(cid:17)

s

.

(cid:101)Xia − (cid:98)Xia =

Σa,S(i,a)Σ−1

S(i,a),S(i,a)

− (cid:101)Σb,S(i,a) (cid:101)Σ

−1
S(i,a),S(i,a)

X S(i,a).

Step 1: Control

T (cid:16)

(cid:13)
(cid:13)
(cid:13)

1
n (cid:102)X

(cid:102)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)∞

. Recall that by the triangular inequality:

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:102)X

(cid:102)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n
1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:99)X − X

(cid:17)

β0

(cid:16)

1
n
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:17)

(cid:102)X − (cid:99)X

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:99)X − X

(cid:17)

(cid:13)
(cid:13)
β0
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

β0

.

We require the following four lemmas, whose proofs we defer to appendix G.

28

Lemma B.9. Under the assumptions of theorem 6, for any u ∈ Rp, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C(α, dmax) (cid:107)u(cid:107)1

(cid:41)

(cid:114)

log np
n

≤ n−1 + c0p−c1,

where c0, c1 denote universal constants and C(α, dmax) a constant depending only on α, dmax.

Lemma B.10. Under the assumptions of theorem 6, we have:

(cid:16)

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C(α, dmax) (cid:107)u(cid:107)1

(cid:27)

log np
n

≤ n−1 + c0p−c1,

where c0, c1 denote universal constants and C(α, dmax) a constant depending only on α, dmax.

Lemma B.11. Under the assumptions of theorem 6, for any u ∈ Rp, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:99)X − X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C(α, dmax) (cid:107)u(cid:107)1

(cid:41)

(cid:114)

log np
n

≤ n−1 + c0p−1,

where c0, c1 denote universal constants and C(α, dmax) a constant depending only on α, dmax.

The proof of the above lemma yields the identitical corollary:

Corollary B.12. Under the assumptions of theorem 6, for any u ∈ Rp, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T

(cid:13)
(cid:13)
(cid:99)Xu
(cid:13)
(cid:13)∞

≥ C(α, dmax) (cid:107)u(cid:107)1

(cid:41)

(cid:114)

log np
n

≤ n−1 + c0p−1,

where c0, c1 denote universal constants and C(α, dmax) a constant depending only on α, dmax.

The following lemma follows by lemma B.6.

Lemma B.13. Under the assumptions of theorem 6, we have, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ Cσ2

X R

(cid:114)

(cid:41)

log p
n

≤ c0p−c1,

where c0, c1 denote universal constants.

Taking u = β0 and noting (cid:107)β0(cid:107)1 ≤ R

√

s, for C(α, dmax) large enough, we have:

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:102)X

(cid:102)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ C(α, dmax)R

(cid:114)

s log np
n

,

Step 2: Control

with probability at least 1 − c0p−c1.
(cid:13)
1
(cid:13)
n (cid:102)X
(cid:13)
(cid:13)
(cid:13)
(cid:13)

inequality implies

(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)∞

1
n (cid:102)X

≤

1
n

(cid:15)

(cid:15)

T

T

(cid:13)
(cid:13)
(cid:13)
(cid:13)

lemmas:

. We now tackle showing

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

+

29

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T

1
n (cid:102)X

T

1
n (cid:99)X

(cid:15)

(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:15)
(cid:13)∞

≤ λ

4 . To this end, the triangular

. We will use the following two

Lemma B.14. Under the assumptions of theorem 6, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

≥ C(α, dmax)σ

(cid:41)

(cid:114)

log np
n

≤ c0n−1 + c1p−c2,

where c0, c1 denote universal constants and C(α, dmax) a constant depending only on α, dmax.

The following lemma follows by lemma B.8.

Lemma B.15. Under the assumptions of theorem 6, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:15)

(cid:99)X

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ Cσ

(cid:114)

(cid:41)

log p
n

≥ c0p−c1,

where C, c0, c1 denote universal constants.

1
Thus, with probability at least 1 − c0p−c1,
n (cid:102)X
The restricted eigenvalue follows by the assumption and the exact same steps as in the proof of

≤ C(α, dmax)σ

(cid:15)

(cid:13)
(cid:13)
(cid:13)

T

(cid:13)
(cid:13)
(cid:13)∞

(cid:113) log np
n .

theorem 5.

C Proofs for square-root LASSO

This section contains the proof of theorem 4. The proof largely follows the recipe of the main result
of [BCW11]. To recall the set-up, we have the linear model y = Xβ0 + (cid:15) and observe the response
vector y as well as Z. We use the square root LASSO

(cid:98)β ∈ arg min

β∈Rp

1
√
n

(cid:13)
(cid:13)
(cid:13)y − (cid:99)Xβ

(cid:13)
(cid:13)
(cid:13)2

+ λ (cid:107)β(cid:107)1 ,

(C.1)

to recover β0. We will use the notation f (β) = 1√
n
We require the following lemma:

(cid:13)
(cid:13)
(cid:13)y − (cid:99)Xβ

(cid:13)
(cid:13)
(cid:13)2

where ∇f (β0) = −

T ((X−(cid:99)X)β0+(cid:15))
1
n (cid:99)X
1√
n (cid:107)(X−(cid:99)X)β0+(cid:15)(cid:107)2

.

Lemma C.1. Under the assumptions of Theorem 4 and taking λ = cσX
with probability at least 1 − δ − 2p−2.

(cid:113) log p

n , then λ ≥ c (cid:107)∇f (β0)(cid:107)∞

Proof. We ﬁrst note that
theorem 3.1.1 implies:

(cid:16)

X − (cid:99)X

(cid:17)

β0+(cid:15) is 8R2σ2

X +2σ2 sub-gaussian. Note that Veryshynin [Ver18]

P

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√
n

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:17)

X − (cid:99)X

β0 + (cid:15)

√

γ

−

(cid:13)
(cid:13)
(cid:13)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

(cid:27)

√
t
√

γ
n

(cid:40)

≤ exp

−c

t2
X + 2σ2(cid:1)2

(cid:0)8R2σ2

(cid:41)

,

(C.2)

(cid:13)
2
where we let γ = E 1
(cid:13)
β0 + (cid:15)
(cid:13)
n
2
implies that with probability at least 1 − δ:

X − (cid:99)X

(cid:13)
(cid:13)
(cid:13)

(cid:17)

(cid:16)

. Under the assumption

(cid:113)

(8R2σ2

X +2σ2)
√
n

ln δ−1
c

≤ c0, this

1
√
n

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:17)

X − (cid:99)X

β0 + (cid:15)

√

≥

(cid:13)
(cid:13)
(cid:13)2

γ(1 − c0) ≥

30

(cid:113)

8R2σ2

X + 2σ2(1 − c0).

Notice now that:

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:99)X

T (cid:16)(cid:16)

(cid:17)

X − (cid:99)X

β0 + (cid:15)

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:99)X

T (cid:16)(cid:16)

X − (cid:99)X

(cid:17)

β0

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:15)

(cid:99)X

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

.

By lemmas A.2 and A.3, the right hand side is upper bounded by CσX (σ + σX R)
n with
probability at least 1 − 2p−2. Combining these implies that with probability at least 1 − 2p−2 − δ:

(cid:113) log p

(cid:13)
(cid:13)
(cid:13)

This completes the proof.

(cid:17)

T (cid:16)(cid:16)

1
n (cid:99)X

1√
n

(cid:16)

(cid:13)
(cid:13)
(cid:13)

X − (cid:99)X
(cid:17)

X − (cid:99)X

β0 + (cid:15)
(cid:13)
(cid:13)
(cid:13)2

β0 + (cid:15)

(cid:17)(cid:13)
(cid:13)
(cid:13)∞

≤ CσX

(cid:114)

log p
n

.

Lemma C.2. Assume β0 satisﬁes A3 and let (cid:98)w = (cid:98)β − β0, where (cid:98)β is deﬁned as in (C.1). Then,
(cid:107) (cid:98)wT c(cid:107)1 ≤ c+1

c−1 (cid:107) (cid:98)wT (cid:107)1.

Proof. Noting that (cid:98)β is a minimizer yields the following simple inequality:

1
√
n

(cid:13)
(cid:13)
(cid:13)(cid:99)X (cid:98)β − y

(cid:13)
(cid:13)
(cid:13)2

−

1
√
n

(cid:13)
(cid:13)
(cid:13)(cid:99)Xβ0 − y

(cid:13)
(cid:13)
(cid:13)2

≤ λ (cid:107)β0(cid:107)1 − λ

(cid:13)
(cid:13)
(cid:13)(cid:98)β

(cid:13)
(cid:13)
(cid:13)1

.

(C.3)

Let f (β) = 1√
n

(cid:13)
(cid:13)
(cid:13)y − (cid:99)Xβ

(cid:13)
(cid:13)
(cid:13)2

and notice that by convexity,

1
√
n

(cid:13)
(cid:13)
(cid:13)(cid:99)X (cid:98)β − y

(cid:13)
(cid:13)
(cid:13)2

−

1
√
n

(cid:13)
(cid:13)
(cid:13)(cid:99)Xβ0 − y

(cid:13)
(cid:13)
(cid:13)2

≥ (∇f (β0))T (cid:16)

(cid:98)β − β0

(cid:17)

,

The preceding two inequalities then imply:

λ ((cid:107)β0(cid:107)1 − (cid:107) (cid:98)w + β0(cid:107)1) ≥ ∇f (β0)T

(cid:98)w ≥ − (cid:107)∇f (β0)(cid:107)∞ (cid:107) (cid:98)w(cid:107)1 .

Noting that by lemma C.1, λ ≥ c (cid:107)∇f (β0)(cid:107)∞, we see that − 1
by the triangular inequality we see that:

c (cid:107) (cid:98)w(cid:107)1 ≤ (cid:107)β0(cid:107)1 − (cid:107) (cid:98)w + β0(cid:107)1. Thus,

−c−1 (cid:107) (cid:98)wT (cid:107)1 − c−1 (cid:107) (cid:98)wT C (cid:107)1 ≤ (cid:107) (cid:98)wT (cid:107)1 − (cid:107) (cid:98)wT C (cid:107)1 .

Re-arranging yields the result.

It is useful to recall the theorem statement:

Theorem 4. Assume A1–A4 and that the data is MCAR(α). Assume additionally that there exists
X + σ2(cid:1)2. Then, there exist positive constants
a constant c1 < 1 such that n ≥ 1
c1
c2, c3, c4 such that with probability at least 1 − δ − c2p−c3, (cid:98)β as deﬁned in (4.1) with regularization
parameter

(cid:0)ln δ−1(cid:1) (cid:0)R2σ2

λ = c4σX

(cid:114)

log p
n

,

satisﬁes

(cid:13)
(cid:13)
(cid:13)(cid:98)β − β0

(cid:13)
(cid:13)
(cid:13)2

(cid:46) σX (σ + σX R)
λmin(Σ

(cid:99)X )

(cid:114)

s log p
n

.

31

Proof. We begin by writing the diﬀerence 1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)X (cid:98)β − y

(cid:13)
2
(cid:13)
(cid:13)
2

− 1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)Xβ0 − y

(cid:13)
2
(cid:13)
(cid:13)
2

in two diﬀerent ways:

1. 1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)X (cid:98)w

(cid:13)
2
(cid:13)
(cid:13)
2

− 2
n

(cid:68)
(cid:98)w, (cid:99)X

T (cid:104)(cid:16)

(cid:17)

X − (cid:99)X

(cid:105)(cid:69)

β0 + (cid:15)

,

2.

(cid:16) 1√

n

(cid:13)
(cid:13)
(cid:13)(cid:99)X (cid:98)β − y

(cid:13)
(cid:13)
(cid:13)2

− 1√
n

(cid:13)
(cid:13)
(cid:13)(cid:99)Xβ0 − y

(cid:13)
(cid:13)
(cid:13)2

(cid:17) (cid:16) 1√

n

(cid:13)
(cid:13)
(cid:13)(cid:99)X (cid:98)β − y

(cid:13)
(cid:13)
(cid:13)2

+ 1√
n

(cid:13)
(cid:13)
(cid:13)(cid:99)Xβ0 − y

(cid:17)

.

(cid:13)
(cid:13)
(cid:13)2

Combining these yields:

1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)X (cid:98)w

(cid:13)
2
(cid:13)
(cid:13)
2

=

≤

T (cid:104)(cid:16)

T (cid:104)(cid:16)

(cid:68)

(cid:68)

2
n
2
n

(cid:98)w, (cid:99)X

(cid:98)w, (cid:99)X

X − (cid:99)X

X − (cid:99)X

(cid:17)

(cid:17)

β0 + (cid:15)

β0 + (cid:15)

(cid:105)(cid:69)

(cid:105)(cid:69)

(cid:16)

(cid:16)

+

+

f ((cid:98)β) − f (β0)

f ((cid:98)β) + f (β0)

(cid:17) (cid:16)

(cid:17)

f ((cid:98)β) + f (β0)

(cid:17)

λ ((cid:107) (cid:98)wT (cid:107)1 − (cid:107) (cid:98)wT C (cid:107)1) ,

where the inequality follows by (C.3). Recalling the explicit calculation of ∇f (β0) and using
H¨older’s inequality implies:

(cid:68)

2
n

(cid:98)w, (cid:99)X

T (cid:104)(cid:16)

X − (cid:99)X

(cid:17)

β0 + (cid:15)

(cid:105)(cid:69)

≤ 2 (cid:107) (cid:98)w(cid:107)1 (cid:107)∇f (β0)(cid:107)∞
(cid:13)
(cid:13)
(cid:13)

(cid:107)w(cid:107)2

(c − 1)n

1
√
n

4λ

√

≤

s

1
√
n

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:17)

X − (cid:99)X

β0 + (cid:15)

(cid:13)
(cid:13)
(cid:13)2

(cid:16)

(cid:17)

X − (cid:99)X

β0 + (cid:15)

(cid:13)
(cid:13)
(cid:13)2

,

where the second inequality follows by lemma C.1, lemma C.2, and Cauchy-Schwarz. Note addi-
tionally that (C.3) implies f ((cid:98)β) ≤ f (β0) + λ
n ((cid:107) (cid:98)wT (cid:107)1 − (cid:107) (cid:98)wT C (cid:107)1) and (cid:107) (cid:98)wT (cid:107)1 − (cid:107) (cid:98)wT C (cid:107)1 ≤ (cid:107) (cid:98)w(cid:107)1 so
that

(cid:16)

f ((cid:98)β) + f (β0)

(cid:17)

λ ((cid:107) (cid:98)wT (cid:107)1 − (cid:107) (cid:98)wT C (cid:107)1) ≤ 2f (β0)λ (cid:107) (cid:98)w(cid:107)1 + λ2 (cid:107) (cid:98)w(cid:107)2
1 .
√

Combining and using (cid:107) (cid:98)w(cid:107)1 ≤ 2c

c−1

s (cid:107) (cid:98)w(cid:107)2, we see that:

1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)X (cid:98)w

(cid:13)
2
(cid:13)
(cid:13)
2

≤

√

s
4λ
(c − 1)

(cid:107) (cid:98)w(cid:107)2 f (β0) +

4c
c − 1

λ

√

sf (β0) (cid:107) (cid:98)w(cid:107)2 + λ2

4c2

(c − 1)2 s (cid:107) (cid:98)w(cid:107)2
2 .

(cid:13)
(cid:13)
(cid:13)(cid:99)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

λmin(Σ

(cid:99)X)

Let ARE denote the event that inf w∈C∩Sp−1

1
where we take the set S
n
in the assumptions of proposition A.5 to be T . Then the assumptions of the theorem, fact A.4
and proposition A.5 imply that P{ARE} ≥ 1 − 2p−c3, where c3 is a constant. Now, plugging in
λ = cσ2
−
n and taking the constant c0 in assumption A4 large enough implies that
X

(cid:113) log p

λmin(Σ

(cid:99)X)

≥

2

2

4c4
(c−1)2

s log p

n ≥

λmin(Σ

(cid:99)X)

4

. This implies:

λmin

(cid:1)

(cid:99)X

(cid:0)Σ
4

(cid:107) (cid:98)w(cid:107)2

2 ≤ CσX f (β0)

(cid:114)

s log p
n

.

Finally, noting that inequality (C.2) and the assumptions of the theorem imply f (β0) ≤ C (σ + σX R)
with probability at least 1 − δ. The result follows by re-arranging the above inequality and plugging
in this upper bound for f (β0).

32

D General results for the LASSO

This appendix provides proofs for the outline of the general technique to prove error bounds for
the Lasso.
Proposition A.1. Given a design matrix (cid:102)X ∈ Rn×p and λ > 0, consider the solution to the convex
program

Further, assume

(cid:101)β ∈ arg min

β∈Rp

(cid:26) 1
2n

(cid:13)
(cid:13)
(cid:13)y − (cid:102)Xβ

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:27)

+ λ (cid:107)β(cid:107)1

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:102)X

(cid:102)X − X

and

sup
w∈C∩Sp−1

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

λ
4

,

1
n

(cid:13)
(cid:13)
(cid:13)(cid:102)Xw

(cid:13)
2
(cid:13)
(cid:13)
2

≥ κ,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:15)

(cid:102)X

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

λ
4

,

where C is the cone C = {w ∈ Rp : (cid:107)wSc(cid:107)1 ≤ 3 (cid:107)wS(cid:107)1}. Then

(cid:16)

(cid:17)

=

+ λ (cid:107)β(cid:107)1 =

(cid:102)X − X
(cid:17)

(cid:13)
(cid:13)
(cid:13)y − (cid:102)Xβ

Proof of Proposition A.1. We ﬁrst re-write the objective 1
2n
the variable w = β − β0:
(cid:13)
1
2
(cid:13)
(cid:13)
2n
2

(cid:13)
(cid:13)
(cid:13)(cid:102)X (β − β0) +
(cid:13)
(cid:16)
(cid:13)
(cid:13)(cid:102)Xw +
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)(cid:102)Xw
(cid:13)
2
+ λ (cid:107)β0 + w(cid:107)1
(cid:13)
(cid:13)
(cid:104)
2
Writing L(w) = 1
(cid:13)
(cid:13)
(cid:102)X
(cid:13)
(cid:13)
2n
2
and letting (cid:101)w = arg minw L(w), we see that L( (cid:101)w) ≤ L(0). Thus:
T (cid:16)

β0 − (cid:15)
(cid:13)
2
(cid:13)
(cid:13)
2

1
2n
1
2n
1
2n

(cid:13)
(cid:13)
(cid:13)(cid:102)Xw

(cid:102)X − X

(cid:102)X − X

(cid:102)X − X

β0 − (cid:15)

β0 − (cid:15)

β0 − (cid:15)

w, 1
n

1
2n

+ 1
2n

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:105)(cid:29)

(cid:13)
(cid:13)
(cid:13)

=

+

+

(cid:28)

(cid:68)

(cid:17)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:13)
2
(cid:13)
(cid:13)
2

1
2n

(cid:13)
(cid:13)
(cid:13)(cid:102)X (cid:101)w

(cid:13)
2
(cid:13)
(cid:13)
2

≤ −

w,

(cid:104)
(cid:102)X

(cid:102)X − X

β0 − (cid:15)

+ λ (cid:107)β(cid:107)1

+ λ (cid:107)β0 + w(cid:107)1
1
n

(cid:13)
2
(cid:13)
(cid:13)
2

w,

+

(cid:28)

√

≤ 12λ
κ

s

.

(cid:13)
(cid:13)
(cid:13)(cid:101)β − β0
(cid:13)
(cid:13)
(cid:13)y − (cid:102)Xβ

(cid:13)
(cid:13)
(cid:13)2
(cid:13)
2
(cid:13)
(cid:13)
2

+λ (cid:107)β(cid:107)1 by introducing

T (cid:16)

(cid:104)

(cid:102)X

(cid:102)X − X

(cid:17)

β0 − (cid:15)

(cid:105)(cid:29)

T (cid:16)

(cid:17)

(cid:102)X − X

(cid:105)(cid:69)

β0 − (cid:15)

+λ (cid:107)β0 + w(cid:107)1

1
n


(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:124)

≤ (cid:107) (cid:101)w(cid:107)1

T (cid:16)

1
n

(cid:102)X

(cid:102)X − X

(cid:17)

β0

(cid:123)(cid:122)
I.

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:125)

+ λ ((cid:107)β0(cid:107)1 − (cid:107)β0 + (cid:101)w(cid:107)1)




(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞
(cid:125)

1
n

(cid:102)X

T

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

(cid:123)(cid:122)
II.

+ λ ((cid:107)β0(cid:107)1 − (cid:107)β0 + (cid:101)w(cid:107)1)

≤

λ
2

(cid:107) (cid:101)w(cid:107)1 + λ ((cid:107)β0(cid:107)1 − (cid:107)β0 + (cid:101)w(cid:107)1)

where the second inequality follows by H¨older’s inequality and the triangle inequality and the upper
bounds on terms I. and II. are by assumption. Letting S denote the set of entries on which β0 is
supported and noting that 1
2n

≥ 0, we have:

(cid:13)
ˆX (cid:101)w
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

λ (cid:13)

(cid:13)β0,S + (cid:101)wS

(cid:13)
(cid:13)1

+ λ (cid:107) (cid:101)wSc(cid:107)1 ≤

λ
2

(cid:107) (cid:101)wS(cid:107)1 +

λ
2

(cid:107) (cid:101)wSc(cid:107)1 + λ (cid:13)

(cid:13)β0,S

(cid:13)
(cid:13)1

33

Thus, by the triangle inequality, we see that (cid:107) (cid:101)wSc(cid:107)1 ≤ 3 (cid:107) (cid:101)wS(cid:107)1. Thus, we see that the solution (cid:101)w
belongs to the cone C. Now, by the third assumption, we see that 1
2 . Thus, we have:
2n

≥ κ

(cid:13)
(cid:13)
(cid:13)(cid:102)X (cid:101)w

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:107) (cid:101)w(cid:107)1 + λ ((cid:107)β0(cid:107)1 − (cid:107) (cid:101)w + β0(cid:107)1)

κ
2

(cid:107) (cid:101)w(cid:107)2

2 ≤

λ
2
3λ
2
√
≤ 6

≤

(cid:107) (cid:101)wS + (cid:101)wSc(cid:107)1
sλ (cid:107) (cid:101)wS(cid:107)2

Re-arranging the inequality gives the result.

D.1 Restricted Eigenvalue Condition

We restate the result for convenience.

Proposition A.5. Let X be a matrix with i.i.d. rows, each of which is sub-Gaussian with parameter
σ2
X and has covariance matrix ΣX . Then X satisﬁes a restricted eigenvalue condition:

with probability at least

sup
w∈C∩Sp−1

1
n

(cid:107)Xw(cid:107)2

2 ≥

λmin (ΣX )
2

(cid:26)

(cid:18)

1 − 2 exp

−cn

min

(cid:19)

(cid:18) t2
σ4
X

,

t
σ2
X

+

s log p
n

(cid:19)(cid:27)

where C is the cone C = {w ∈ Rp : (cid:107) ˆwSc(cid:107)1 ≤ 3 (cid:107) ˆwS(cid:107)1}.

Proof of Proposition A.5. The proof can be found in Loh and Wainwright [LW12b], however
we repeat much of the argument here for clarity. The proof will follow three main steps:

1: Noting that C∩B2(1) ⊆ B1(

√

16s)∩B2(1), we show B1(

√

s)∩B2(1) ⊆ 3cl {conv {B0(s) ∩ B2(1)}}

2: Argue that on the simpler set w ∈ B0(s) ∩ B2(1), the behavior of the desired quadratic form

concentrates around its expectation

3: Show that any w ∈ 3cl {conv {B0(s) ∩ B2(1)}} maintains the same bounds from Step 2

We now prove each step.

1: This can be found in Loh and Wainwright [LW12b] Lemma 11.

2: We claim:

(cid:40)

P

sup
w∈B0(s)∩B2(1)

1
n

(cid:12)
(cid:12)(cid:107)Xw(cid:107)2
(cid:12)

2 − E (cid:107)Xw(cid:107)2

2

(cid:41)

(cid:12)
(cid:12)
(cid:12) ≥ t

≤ 2ps9s exp

(cid:26)

−cn min

(cid:19)(cid:27)

(cid:18) t2
σ4
X

,

t
σ2
X

To do this, we ﬁrst consider sets SU = {w ∈ Rp : (cid:107)w(cid:107)2 ≤ 1, supp(w) ⊆ U } and notice that
B0(s) ∩ B2(1) = (cid:83)
let N(cid:15) be a (cid:15)-cover of SU and note that there ex-
E X T X, we re-write
n X T X − 1
ists such a set with |N(cid:15)| ≤ ( 3

(cid:15) )s [Ver18]. Taking X ≡ 1

|U |=s SU . Now,

n

34

(cid:12)
(cid:12)(cid:107)Xw(cid:107)2
1
(cid:12)
n
Thus, we have:

2 − E (cid:107)Xw(cid:107)2

2

(cid:12)
(cid:12) = (cid:12)
(cid:10)w, Xw(cid:11)(cid:12)
(cid:12)
(cid:12)

(cid:12) and see supw∈SU

(cid:10)w, Xw(cid:11)(cid:12)
(cid:12)
(cid:12)

(cid:12) ≤ 1

1−2(cid:15) supw∈N(cid:15)

(cid:12)
(cid:12)

(cid:10)w, Xw(cid:11)(cid:12)
(cid:12).

(cid:40)

P

sup
w∈B0(s)∩B2(1)

1
n

(cid:12)
(cid:12)(cid:107)Xw(cid:107)2
(cid:12)

2 − E (cid:107)Xw(cid:107)2

2

(cid:41)

(cid:12)
(cid:12)
(cid:12) ≥ t

≤

≤

≤

(cid:19)

(cid:40)

P

sup
w∈SU

(cid:41)

(cid:12)
(cid:10)w, Xw(cid:11)(cid:12)
(cid:12)

(cid:12) ≥ t

(cid:19)

(cid:26)

(cid:12)
(cid:12)

P

(cid:10)w, Xw(cid:11)(cid:12)

sup
w∈N(cid:15)
(cid:10)w, Xw(cid:11)(cid:12)
|N(cid:15)| P (cid:8)(cid:12)
(cid:12)

(cid:19)

(cid:27)

(cid:12) ≥ (1 − 2(cid:15))t

(cid:12) ≥ (1 − 2(cid:15))t(cid:9)

(cid:18)p
s
(cid:18)p
s
(cid:18)p
s

Noting that the square of a sub-Gaussian random variable is sub-Exponential, we see that if
t ≤ 4eσ2

1−2(cid:15) , then, by Bernstein’s Inequality (see e.g. Vershynin [Ver18] Theorem 2.8.1):

X

P (cid:8)(cid:12)
(cid:12)

(cid:10)w, Xw(cid:11)(cid:12)

(cid:12) ≥ (1 − 2(cid:15))t(cid:9) ≤ 2 exp

(cid:26)

−cn min

(cid:18) t2(1 − 2(cid:15))2
σ4
X

,

t(1 − 2(cid:15))
σ2
X

(cid:19)(cid:27)

Taking (cid:15) = 1

3 and noting that (cid:0)p

s

(cid:1) ≤ ps implies the claim.

(cid:88)

|(cid:104)w, Xw(cid:105)| =

i=1 αiwi where αi are non-negative weights satisfying (cid:80)k

3: Suppose that for a ﬁxed matrix X, |(cid:104)w, Xw(cid:105)| ≤ δ for all w ∈ B0(16s) ∩ B2(1). We claim
that for all w ∈ C that |(cid:104)w, Xw(cid:105)| ≤ 27δ. To see this, let w ∈ 3conv {B0(16s) ∩ B2(1)}. Then
we can write w = (cid:80)k
i=1 αi = 1 and
wi ∈ B0(16s) ∩ B2(3). Then, we can see that:
(cid:12)
(cid:42) k
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j
(cid:88)

(cid:12)
(cid:12)
(cid:12)
((cid:104)wi + wj, X(wi + wj)(cid:105) − (cid:104)wi, Xwi(cid:105) − (cid:104)wj, Xwj(cid:105))
(cid:12)
(cid:12)
(cid:12)

αiαj (cid:104)wi, Xwj(cid:105)

αiαj
2

(cid:43)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

αiwi, X

αiwi

k
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

i=1

=

=

i,j

· |36δ + 9δ + 9δ| = 27δ

αiαj
2

≤

i,j

Now since |(cid:104)w, Xw(cid:105)| is a continuous function of w and by step 1, we see that if for all
v ∈ B0(16s) ∩ B2(1), |(cid:104)v, Xv(cid:105)| ≤ δ, then for all w ∈ B1(
16s) ∩ B2(1), |(cid:104)w, Xw(cid:105)| ≤ 27δ.
This implies the claim since C ∩ B2(1) ⊆ B1(
16s) ∩ B2(1)
(cid:110)
(cid:10)w, Xw(cid:11)(cid:12)
X : (cid:12)
(cid:12)

(cid:12) ≤ λmin(ΣX )

∀w ∈ C ∩ B2(1)

. Con-

√

√

(cid:111)

2

To conclude, let us work on the event A =
ditioned on this event, we see that

(cid:12)
(cid:28)
(cid:12)
(cid:12)
(cid:12)

w,

1
n

X T Xw

(cid:29)

(cid:28)

−

w,

1
n

E X T Xw

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

λmin(ΣX )
2

An application of the triangle inequality yields:
(cid:12)
(cid:28)
(cid:28)
(cid:12)
(cid:12)
(cid:12)

E X T Xw

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

w,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

w,

1
n

X T Xw

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

λmin(ΣX )
2

35

and now noting that (cid:12)
(cid:10)w, 1
(cid:12)
n
λmin(ΣX )
2

n X T Xw(cid:11) ≥
∀w ∈ C ∩ B2(1). We now control P{Ac}. By step 3, it suﬃces to use the bound of step

(cid:12) ≥ λmin(ΣX ) and re-arranging implies (cid:10)w, 1

E X T Xw(cid:11)(cid:12)

2:

(cid:40)

P

sup
w∈B0(s)∩B2(1)

1
n

(cid:12)
(cid:12)(cid:107)Xw(cid:107)2
(cid:12)

2 − E (cid:107)Xw(cid:107)2

2

(cid:41)

(cid:12)
(cid:12)
(cid:12) ≥ t

≤ 2ps9s exp

(cid:26)

−cn min

(cid:19)(cid:27)

(cid:18) t2
σ4
X

,

t
σ2
X

Taking t = λmin(ΣX )

54

, we have the result:

P {A} ≥ 1 − 2 exp

(cid:26)

(cid:18)

−cn

min

(cid:19)

(cid:18) t2
σ4
X

,

t
σ2
X

+

s log p
n

(cid:19)(cid:27)

and we are done.

E Proofs of technical lemmas from appendix A

T (cid:16)

This appendix is dedicated to the proofs of the (cid:96)∞ terms needed in theorem 1, corollary 3.1, and
theorem 3.
In particular, we devote one subsection to each of lemmas A.2, A.3, A.6, A.7, A.8,
and A.9. The strategy for each of the proofs is essentially the same, although the techniques vary
(cid:15) or
signiﬁcantly. Each of these lemmas is concerned with one of the two random vectors (cid:99)X
n (cid:107)·(cid:107)∞ for each of these random
(cid:99)X
vectors.
In particular, we will aim to show in each of these lemmas that the random vector
is the empirical average of a sum of sub-exponential with as tight a sub-exponential parameter as
possible and we will conclude using a standard concentration inequality for sub-exponential random
variables, such as Bernstein’s inequality [Ver18].

β0. We will be interested in the concentration of 1

(cid:99)X − X

(cid:17)

T

E.1 Proof of lemma A.2

Recall the deﬁnition of λ in equation (A.2):

(cid:16)

λ = 4A

8eσX σ + 16

√

2eσ2
X

√

1 − αR

(cid:17) (cid:114)

log p
n

.

We re-state the lemma for the reader’s convenience:

Lemma A.2. Assuming λ as deﬁned in equation (A.2), and the assumptions of theorem 1, we
have:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:99)X

T (cid:16)

(cid:17)

(cid:99)X − X

β0

(cid:27)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥

λ
4

≤ 2p1− A2
2 .

Proof. To clarify our strategy, let us write:

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= (cid:107)β0(cid:107)2 max

a=1,2,...,p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:68)

(cid:98)Xia

X i − (cid:99)X i, (cid:101)β

(cid:69)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

36

where we have taken (cid:102)β0 = β0/ (cid:107)β0(cid:107)2. We will now show that the random variable (cid:98)Xia
is sub-exponential. Take θ such that |θ| ≤ 1
generating function (using the notation Sc = [p] S for S ⊆ [p]):

X i − (cid:99)X i, (cid:101)β
X . We now control the moment

and let cX = 16eσ2

16eσ2
X

(cid:68)

(cid:69)

E exp

(cid:110)

θ (cid:98)Xia

(cid:68)

X i − (cid:99)X i, (cid:101)β

(cid:69)(cid:111)

=

(cid:88)

α|S|(1 − α)p−|S| E exp

(cid:110)

θ (cid:98)Xia

(cid:68)

(cid:99)X i,Sc − X i,Sc, (cid:101)β0,Sc

(cid:69)(cid:111)

.

S⊆[p]

Notice now that by the orthogonality property of the conditional expectation, E (cid:98)Xia
0. Additionally, note that (cid:98)Xia is σ2

X sub-gaussian and by fact H.5,

(cid:68)

(cid:99)X i,Sc − X i,Sc, (cid:101)β0,Sc

is

(cid:69)

(cid:68)

(cid:99)X i,Sc − X i,Sc, (cid:101)β0,Sc

(cid:69)

=

4σ2
X

(cid:13)
(cid:13)
(cid:13)(cid:101)β0,Sc

(cid:13)
2
(cid:13)
(cid:13)
2

sub-gaussian. Thus, by lemma H.1,

(cid:88)

S⊆[p]

α|S|(1 − α)p−|S| E exp

(cid:110)

θ (cid:98)Xia

(cid:68)

(cid:99)X i,Sc − X i,Sc, (cid:101)β0,Sc

(cid:69)(cid:111)

≤

(cid:88)

α|S|(1 − α)p−|S|e

1
2 θ2c2

X(cid:107)(cid:101)β0,Sc(cid:107)2

2

S⊆[p]

= ES exp

(cid:26) 1
2

θ2c2
X

(cid:13)
(cid:13)
(cid:13)(cid:101)β0,Sc

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:27)

.

We now control the term ES exp

(cid:26)

ment [BLM13]. Deﬁne φ(θ) ≡ log ES exp
Thus, we have φ(θ) = (cid:82) θ
0

1
2 θ2c2
X
(cid:82) t1
0 φ(cid:48)(cid:48)(t2)dt2dt1. Now,
(cid:32)

(cid:13)
1
2 θ2c2
(cid:13)
(cid:13)(cid:101)β0,Sc
X
(cid:26)

(cid:27)

(cid:13)
2
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)(cid:101)β0,Sc

using a technique similar to Herbst’s argu-

(cid:27)

(cid:13)
2
(cid:13)
(cid:13)
2

. Notice that φ(0) = 0 and φ(cid:48)(0) = 0.

φ(cid:48)(cid:48)(θ) = c2
X

ES

(cid:18)(cid:13)
(cid:13)
(cid:13)(cid:101)β0,Sc

(cid:13)
2
(cid:13)
(cid:13)
2

≤ c2
X

ES

(cid:18)(cid:13)
(cid:13)
(cid:13)(cid:101)β0,Sc

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:19)

zS

zS

(cid:19)

+ θ2c4
X

ES

(cid:18)(cid:13)
(cid:13)
(cid:13)(cid:101)β0,Sc

+ θ2c4
X

(cid:18)

ES

(cid:18)(cid:13)
(cid:13)
(cid:13)(cid:101)β0,Sc

(cid:19)

zS

−

(cid:20)
ES

(cid:18)(cid:13)
(cid:13)
(cid:13)(cid:101)β0,Sc

(cid:13)
2
(cid:13)
(cid:13)
2

zS

(cid:19)(cid:21)2(cid:33)

(cid:19)(cid:19)

zS

(cid:13)
4
(cid:13)
(cid:13)
2

(cid:13)
4
(cid:13)
(cid:13)
2

X(cid:107)(cid:101)β0,Sc(cid:107)2
where zS = e
X(cid:107)(cid:101)β0,Sc(cid:107)2
ES e
2
(cid:13)
= 1 − α and ES
(cid:13)
(cid:13)(cid:101)β0,Sc

(cid:13)
(cid:13)
(cid:13)(cid:101)β0,Sc

2 θ2c2
1
1
2 θ2c2

ES

2

(cid:13)
2
(cid:13)
(cid:13)
2

. Notice that e− 1

2 c2

X θ2

≤ zS ≤ e

1
2 c2

X θ2

. Additionally, notice that

(cid:13)
4
(cid:13)
(cid:13)
2

≤ 1 − α (since (cid:101)β0 is a unit norm vector). We thus see that:

φ(cid:48)(cid:48)(θ)
c2
X (1 − α)

1
2 c2

X θ2

≤ e

+ θ2c2

X e

1
2 c2

X θ2

.

Now, take θ such that |θ| ≤ 1√

2cX

thus giving

φ(cid:48)(cid:48)(θ)
X (1−α) ≤ 2. Therefore:
c2

φ(θ) =

(cid:90) θ

(cid:90) t1

0

0

φ(cid:48)(cid:48)(t2)dt2dt1 ≤ θ2(1 − α)c2
X .

This implies that for |θ| ≤ 1√

2cX

E exp

(cid:110)

θ (cid:98)Xia

(cid:68)

X i − (cid:99)X i, (cid:101)β

(cid:69)(cid:111)

≤ eθ2(1−α)c2
X ,

(E.1)

37

so the desired term is sub-exponential. To conclude, notice that:

P

(cid:26) 1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)X

T (cid:16)

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

≤ P

(cid:40)

(cid:40)

T (cid:16)

(cid:13)
(cid:13)
(cid:13)(cid:99)X

1
n

(cid:99)X − X

√

(cid:17)

(cid:101)β0

(cid:13)
(cid:13)
(cid:13)∞

≥

2AcX

(cid:114)

√

1 − α

(cid:41)

log p
n

= P

max
a=1,2,...,p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:68)

(cid:98)Xia

X i − (cid:99)X i, (cid:101)β

(cid:12)
(cid:12)
(cid:69)
(cid:12)
(cid:12)
(cid:12)

√

≥

2AcX

√

1 − α

(cid:114)

(cid:41)

.

log p
n

Under the assumption that

(cid:113) log p

√

invoking lemma I.1 with Zi = (cid:98)Xia
√
(cid:113) log p
n .

2AcX

1 − α

√

n ≤
(cid:68)

1−α
A , we conclude by a union bound over a ∈ [p] and
X , and t =

Z = 2(1 − α)c2

, θ = 1√

and σ2

X i − (cid:99)X i, (cid:101)β

(cid:69)

2cX

E.2 Proof of lemma A.3

We repeat the lemma for the reader’s convenience:

Lemma A.3. Assuming λ as deﬁned in equation (A.2), and the assumptions of theorem 1, we
have:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:99)X

≥

(cid:27)

λ
4

≤ 2p1− A2
2 .

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞
(cid:12)
(cid:12)
(cid:12)

1
n

(cid:80)n

Proof. We write
(cid:98)Xia(cid:15)i is sub-exponential: for all θ ≤ (8eσX σ)−1, E eθ (cid:98)Xia(cid:15)i ≤ e32e2θ2σ2

= maxa=1,2,...,p

i=1 (cid:98)Xia(cid:15)i

(cid:15)

(cid:12)
(cid:12)
(cid:12). By lemma H.1, the random variable
X σ2. We are thus interested in:

(cid:13)
(cid:13)
(cid:13)

T

1
n (cid:99)X

(cid:13)
(cid:13)
(cid:13)∞

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:15)

(cid:99)X

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

(cid:40)

≤ P

max
a=1,2,...,p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:98)Xia(cid:15)i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ 8eAσX σ

(cid:114)

(cid:41)

log p
n

Thus, under the assumption that

(cid:113) log p

a ∈ [p] and lemma I.1 with Zi = (cid:98)Xia(cid:15)i, θ = (8eσX σ)−1, σ2

n ≤ (A)−1, we conclude by invoking a union bound over
(cid:113) log p
n .

X σ2, and t = 8eAσX σ

Z = 64e2σ2

E.3 Proof of lemma A.6

Let us ﬁrst copy the regularization parameter λ given in (A.4):

λ = 4A

√

(cid:16)

√
8

α

√

2eσ + 32e

1 − αR

(cid:17) (cid:114)

log p
n

.

We repeat the lemma for the reader’s convenience:

Lemma A.6. Assuming λ as deﬁned in equation (A.4), and the assumptions of corollary 3.1, we
have:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:99)X − X

(cid:17)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

≤ 2p1− A2
2 .

β0

38

Proof. As in the proof of lemma A.2, we take

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= (cid:107)β0(cid:107)2 max

a=1,2,...,p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:68)

(cid:98)Xia

X i − (cid:99)X i, (cid:101)β0

(cid:69)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

We again would like to show that (cid:98)Xia
is sub-exponential. Of course, we have done
this already in the proof of lemma A.2; however, in this simpler case we will be able to get tighter
control on the sub-exponential constant. We have:

X i − (cid:99)X i, (cid:101)β0

(cid:68)

(cid:69)

E exp

(cid:110)

θ (cid:98)Xia

(cid:68)

X i − (cid:99)X i, (cid:101)β0

(cid:69)(cid:111)

= (1 − α) + α E exp

Now, by inequality (E.1), for |θ| ≤ (256e2

√

2)−1,






θXia

(cid:88)

b(cid:54)=a

(Xib − (cid:98)Xib)(cid:101)β0,b






.

E exp






θXia

(cid:88)

(Xib − (cid:98)Xib)(cid:101)β0,b

b(cid:54)=a






≤ exp (cid:8)256e2θ2(1 − α)(cid:9) .

and we are interested in (1 − α) + α exp (cid:8)256e2θ2(1 − α)(cid:9). Note that for any |θ| ≤ (256
2e2)−1,
256e2θ2(1 − α) ≤ 1. We thus employ the numeric inequality 1 + α(ex − 1) ≤ 1 + 2αx ≤ e2αx for
x ≤ 1 to see that:

√

(1 − α) + α E exp






θXia

(cid:88)

(Xib − (cid:98)Xib)(cid:101)β0,b

b(cid:54)=a






≤ (1 − α) + α exp (cid:8)256e2θ2(1 − α)(cid:9)

≤ exp (cid:8)512e2θ2α(1 − α)(cid:9) .

We are ready to conclude. Notice that:

P

(cid:26) 1
n

(cid:13)
(cid:13)
(cid:13)(cid:99)X

T (cid:16)

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

≤ P

(cid:40)

(cid:40)

T (cid:16)

(cid:13)
(cid:13)
(cid:13)(cid:99)X

1
n

(cid:99)X − X

(cid:17)

(cid:101)β0

(cid:13)
(cid:13)
(cid:13)∞

√

≥ 32

2eA(cid:112)α(1 − α)

(cid:114)

(cid:41)

log p
n

= P

max
a=1,2,...,p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:68)

(cid:98)Xia

X i − (cid:99)X i, (cid:101)β

(cid:12)
(cid:12)
(cid:69)
(cid:12)
(cid:12)
(cid:12)

√

≥ 32

2eA(cid:112)α(1 − α)

(cid:114)

(cid:41)

.

log p
n

√

(cid:113) log p
n ≤
(cid:68)

α(1−α)
√
8
2eA
X i − (cid:99)X i, (cid:101)β

Under the assumption that

using lemma I.1 with Zi = (cid:98)Xia

√

32

2eA(cid:112)α(1 − α)

(cid:113) log p
n .

E.4 Proof of lemma A.7

We repeat this lemma here:

, the desired result follows by a union bound and
(cid:69)
Z = 1024e2α(1 − α), and t =

, θ =

, σ2

√

1
256e2

2

Lemma A.7. Assuming λ as deﬁned in equation (A.4), and the assumptions of corollary 3.1, we
have:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:99)X

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

(cid:27)

λ
4

≥ 2p1− A2
2 .

≥

39

Proof. We write

(cid:13)
(cid:13)
(cid:13)

T

1
n (cid:99)X

(cid:13)
(cid:13)
(cid:15)
(cid:13)∞

= maxa=1,2,...,p

(cid:12)
(cid:12)
(cid:12)

1
n

(cid:80)n

i=1 (cid:98)Xia(cid:15)i

(cid:12)
(cid:12)
(cid:12). Notice that

E eθ (cid:98)Xia(cid:15)i = (1 − α) + α E eθXia(cid:15)i ≤ (1 − α) + αe32e2θ2σ2

.

Where the last inequality holds for all θ ≤ (8eσ)−1 by lemma H.1. Thus by the numeric inequality
1 + α(ex − 1) ≤ 1 + 2αx ≤ e2αx for x ≤ 1, we have E eθ (cid:98)Xia(cid:15)i ≤ e64e2θ2ασ2. We are thus interested in:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T

(cid:99)X

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

(cid:40)

≤ P

max
a=1,2,...,p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:98)Xia(cid:15)i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

≥ 8

√

2eA

ασ

(cid:114)

(cid:41)

log p
n

Thus, under the assumption that

(cid:113) log p

n ≤ 2

√
α√
2A

and lemma I.1 with Zi = (cid:98)Xia(cid:15)i, θ = (8eσ)−1, σ2

Z = 128e2ασ2, and t = 8

2eA

ασ

, we conclude by invoking a union bound over a ∈ [p]
√

√

(cid:113) log p
n .

E.5 Proof of lemma A.8

We repeat the regularization parameter λ from (A.6):

λ = 4A (cid:0)8eσX σ + 480eσ2

X R(cid:1)

(cid:114)

log p
n

We now repeat the lemma for convenience:

Lemma A.8. Assuming λ as deﬁned in equation (A.6), and the assumptions of corollary 3, we
have:

P

Proof. We ﬁrst re-write

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:17)

(cid:99)X

(cid:99)X − X

β0

≥

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:27)

λ
4

≤ 6p1− A2
2 .

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= (cid:107)β0(cid:107)2 max

a=1,2,...,p

(cid:68)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

(cid:16)

(cid:99)Xea,

X − (cid:99)X

(cid:17)

(cid:101)β0

(cid:69)(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:16)

(cid:17)

Note that (cid:99)Xea and
X − (cid:99)X
0 by the orthogonality property of the conditional expectation. We will thus re-write the inner
product as:

(cid:101)β0 are both sub-gaussian random vectors and that E

X − (cid:99)X

(cid:99)Xei,

(cid:68)

(cid:16)

(cid:17)

(cid:69)

(cid:101)β0

=

(cid:107)β0(cid:107)2
2

max
a=1,2,...,p

(cid:12)
(cid:18) 1
(cid:12)
(cid:12)
(cid:12)
n
(cid:12)
(cid:124)

(cid:13)
(cid:13)
(cid:13)(cid:99)Xea +

(cid:16)

X − (cid:99)X

(cid:17)

(cid:101)β0

(cid:13)
2
(cid:13)
(cid:13)
2

−

1
n
(cid:123)(cid:122)
I.

E

(cid:13)
(cid:13)
(cid:13)(cid:99)Xea +

(cid:16)

X − (cid:99)X

(cid:17)

(cid:101)β0

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:19)

(cid:125)

−

(cid:18) 1
n

(cid:124)

(cid:13)
(cid:13)
(cid:13)(cid:99)Xea

(cid:13)
2
(cid:13)
(cid:13)
2

−

1
n

E

(cid:13)
(cid:13)
(cid:13)(cid:99)Xei

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:19)

(cid:18) 1
n

(cid:16)

(cid:13)
(cid:13)
(cid:13)

−

X − (cid:99)X

(cid:17)

(cid:101)β0

(cid:13)
2
(cid:13)
(cid:13)
2

− E

(cid:16)

(cid:13)
(cid:13)
(cid:13)

X − (cid:99)X

(cid:17)

(cid:101)β0

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:123)(cid:122)
II.

(cid:125)

(cid:124)

(cid:123)(cid:122)
III.

We thus have: A union bound as well as noting that 8eσX σ ≥ 0 gives:

(cid:12)
(cid:19)
(cid:12)
(cid:12)
.
(cid:12)
(cid:12)
(cid:125)

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

≤ p P

|I. − II. − III.| ≥ 432

√

2eAσ2
X

(cid:114)

(cid:41)

.

log p
n

(cid:40)

40

We thus reduce ourselves to examining P {|I. − II. − III.| ≥ t}. Notice then that P {|I. − II. − III.| ≥ t} ≤
(cid:80)

(cid:9). We go through this sum term by term. First, we examine P{|I.| ≥ t

P (cid:8)|I| ≥ t
3

3 }.

I∈{I.,II.,III.}

We re-write:

(cid:13)
(cid:13)
(cid:13)(cid:99)Xea +

(cid:16)

1
n

X − (cid:99)X

(cid:17)

(cid:101)β0

(cid:13)
2
(cid:13)
(cid:13)
2

=

1
n

n
(cid:88)

i=1

(cid:16)(cid:68)

(cid:99)X i, ea

(cid:69)

(cid:68)

+

X i − (cid:99)X i, (cid:101)β0

(cid:69)(cid:17)2

.

Note that by two applications of fact H.5,
by lemma H.2, we have that for |θ| ≤ (160eσ2

(cid:99)X i, ea
X )−1:

(cid:68)

(cid:69)

(cid:68)

+

X i − (cid:99)X i, (cid:101)β0

(cid:69)

is 10σ2

X sub-gaussian. Thus,

E exp

(cid:26)(cid:16)(cid:68)

(cid:99)X i, ea

(cid:68)

(cid:69)

+

X i − (cid:99)X i, (cid:101)β0

(cid:69)(cid:17)2

(cid:16)(cid:68)

− E

(cid:99)X i, ea

(cid:69)

(cid:68)

+

X i − (cid:99)X i, (cid:101)β0

(cid:69)(cid:17)2(cid:27)

≤ e128(10e)2θ2σ4
X .

This implies that by lemma I.1, taking θ = (160eσ2
X )−1, σ2
(cid:27)
(cid:26)
≤ 2 exp{− A2

that for

(cid:113) log p

|I.| ≥ 160eAσ2
X

(cid:113) log p
n

n ≤ 1

A , P

Z = (160e)2σ4

X and t = 160
(cid:68)

2 log p}. Similarly,

√

2eAσ2
X
(cid:69)

(cid:99)X i, ea

(cid:113) log p
n ,

is σ2
X

sub-gaussian, and lemma H.2 implies that for |θ| ≤ (8eσ2

X )−1:

(cid:26)(cid:16)(cid:68)

E exp

(cid:99)X i, ea

(cid:69)(cid:17)2

(cid:16)(cid:68)

− E

(cid:99)X i, ea

(cid:69)(cid:17)2(cid:27)

≤ e128e2θ2σ4
X .

(cid:113) log p

(cid:113) log p

Thus, taking θ = (16eσ2

1

A , lemma I.1 yields P

(cid:26)

X )−1, σ2

Z = 256e2σ4
(cid:113) log p
n

|II.| ≥ 160eAσ2
X

X , and t = 16eAσ2
X
(cid:26)

(cid:27)

≤ P

|II.| ≥ 16eAσ2
X

n , and assuming
(cid:27)
≤ 2e− A2

(cid:113) log p
n

n ≤
2 log p. We

additionally see that by fact H.5,
that for |θ| ≤ (64eσ2

X )−1,

(cid:68)

X i − (cid:99)X i, (cid:101)β0

(cid:69)

is 4σ2

X sub-gaussian and thus lemma H.2 implies

(cid:26)(cid:16)(cid:68)

E exp

X i − (cid:99)X i, (cid:101)β0

(cid:69)(cid:17)2

(cid:16)(cid:68)

− E

X i − (cid:99)X i, (cid:101)β0

(cid:69)(cid:17)2(cid:27)

≤ e128(4e)2θ2σ4
X .

Thus, taking θ = (64eσ2

X )−1, σ2

lemma I.1 implies P

(cid:26)

Z = (64e)2σ4
(cid:113) log p
n

|III.| ≥ 160eAσ2
X

X and t = 64eAσ2
X
(cid:26)

(cid:27)

≤ P

|III.| ≥ 64eAσ2
X

(cid:113) log p

(cid:113) log p

n , and assuming
(cid:113) log p
n

(cid:27)

≤ 2e− A2

n ≤ 1
A ,
2 log p. We

are ready to conclude. Under the most stringent assumption

(cid:113) log p

n ≤ 1
A ,

(cid:88)

I∈{I.,II.,III.}

(cid:40)

P

|I| ≥ 160eAσ2
X

(cid:114)

(cid:41)

log p
n

≤ 6p− A2
2 .

We thus have:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥

(cid:27)

λ
4

≤ 6p1− A2
2 ,

and our assumption that A >

√

2 yields the result.

41

E.6 Proof of lemma A.9

We repeat the lemma here for the reader’s convenience:

Lemma A.9. Assuming λ as deﬁned in equation (A.6), and the assumptions of corollary 3, we
have:

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞
Proof. This is exactly the same as lemma A.3.

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:99)X

P

T

≥

(cid:27)

λ
4

≤ 2p1− A2
2 .

F Proofs of technical lemmas from appendix B.1

This appendix is dedicated to the proofs needed for theorem 5. It is organized as follows: subsec-
tion F.1 collects facts that will be used in each proof in lemma F.1. Then, subsection F.2 contains
the proof of lemma B.1 which controls the term | (cid:98)φ − φ|. Subsections F.3 - F.8 contain the auxiliary
lemmas necessary for each of the terms in theorem 5. Finally, the assumptions of the theorem are
addressed in subsection F.9.

F.1 Useful facts for AR proofs

Throughout this section, we will write the explicit forms (cid:98)Xik, (cid:101)Xik more succinctly, taking fL(φ) =
(cid:0)φ−d1 − φd1(cid:1). By the mean value theorem, we

(cid:0)φ−d2 − φd2(cid:1) and fR(φ) = φd1+d2

1−φ2(d1+d2)

φd1+d2
1−φ2(d1+d2)
write

(cid:101)Xik − (cid:98)Xik =

(cid:16)

(cid:98)φ − φ

(cid:17) (cid:16)

Xi,L(k)f (cid:48)

L(k)(ξL(k)) + Xi,R(k)f (cid:48)

R(k)(ξR(k))

(cid:17)

,

(F.1)

for some ξL(k), ξR(k) ∈ (min(φ, (cid:98)φ), max(φ, (cid:98)φ)). We will make use many times of the following lemma,
which collects various facts which will be useful for the proof:

Lemma F.1. Under the assumptions of theorem 5:

i. For all b ∈ [p], |f (cid:48)

L(b)(ξL(b)))|, |f (cid:48)

R(b)(ξR(b))| ≤ C, where C is a universal constant.

ii. (cid:98)Xia, Xi,L(a), Xi,R(a) are all σ2

X sub-gaussian.

iii. P

(cid:110)

maxa,b∈[p]×[p]

1
n

(cid:80)n

i=1| (cid:98)XiaXi,L(b)| ≥ 18eσ2
X

(cid:111)

≤ 2p2− A2
2 .

Proof.

i. This follows from a straightforward, but tedious calculation and using the fact that

|φ| < 1.

ii. We see that (cid:98)Xia is sub-gaussian by using fact A.4 for vector (cid:99)X i and then using the fact
. The second two statements follow by noticing that E eθXi,L(a) =

that (cid:98)Xia =
EL(a) E (cid:8)eθXi,L(a) | L(a)(cid:9) and using sub-gaussianity of each entry of X.

(cid:99)X i, ea

(cid:69)

(cid:68)

iii. Part ii.

in combination with lemma H.3 imply that for any |θ| ≤ (16eσ2

(cid:16)

(cid:110)
θ

satisﬁes E exp
inequality I.2 with θ = (16eσ2

| (cid:98)XiaXi,L(b)| − E| (cid:98)XiaXi,L(b)|

(cid:17)(cid:111)

≤ E exp (cid:8)128e2θ2σ4
X

X )−1 and σ2

Z = 256e2σ4

X )−1, | (cid:98)XiaXi,L(b)|
(cid:9). Thus, Bernstein’s

X implies:
(cid:26) n
2

≤ 2 exp

min

(cid:18)

t2
(16eσ2

X )2 ,

t
16eσ2
X

(cid:19)(cid:27)

.

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

(cid:16)

i=1

| (cid:98)XiaXi,L(b)| − E| (cid:98)XiaXi,L(b)|

(cid:41)

≥ t

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

42

Now, a union bound over [p] × [p] as well as taking t = 16eAσ2
X

(cid:113) log p
n

and recalling the

n < 1 (which is satisﬁed for c(cid:96) in assumption B2.

large enough)

assumption that A
yields:

(cid:113) log p

(cid:40)

P

max
a,b∈[p]×[p]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:16)

(cid:17)
| (cid:98)XiaXi,L(b)| − E| (cid:98)XiaXi,L(b)|

≥ 16eAσ2
X

(cid:114)

(cid:41)

log p
n

≤ 2p2− A2
2 .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Noting that by fact H.4, E| (cid:98)XiaXi,L(b)| ≤ σ2

X , the result follows.

F.2 Proof of lemma B.1

We re-state the lemma here:

Lemma B.1. Deﬁne ˆφ as in (5.1). Then under the assumptions of theorem 5,

(cid:40)
(cid:12)
(cid:12)
(cid:12)

P

ˆφ − φ

(cid:12)
(cid:12)
(cid:12) ≥

4
α2

(cid:115)

(cid:41)

log p
np

≤ cp−C.

where c, C > 0 are universal constants.

Proof. It is helpful to recall the deﬁnition of (cid:98)φ from (5.1):

1
α2np

ˆφ =

(cid:80)n

(cid:80)p−1

i=1
1
αnp

a=1 XiaXi(a+1)MiaMi(a+1)
(cid:80)n

(cid:80)p−1

a=1 X 2

iaMia

i=1

.

Let us ﬁrst examine the numerator:

1
α2np

(cid:80)n

i=1

(cid:80)p−1

a=1 XiaXi(a+1)MiaMi(a+1). We have:

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

=

M

1
n(p − 1)
(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

n
(cid:88)

p−1
(cid:88)

i=1

a=1

XiaXi(a+1)MiaMi(a−1) −

α2φ
1 − φ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

≥ t

1
n(p − 1)

n
(cid:88)

p−1
(cid:88)

i=1

a=1

XiaXi(a+1)MiaMi(a+1) −

α2φ
1 − φ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

M

P {M }

Let NM ≡ (cid:80)

i,a MiaMi(a+1) and notice that E

(cid:40)

1
n(p−1)

(cid:80)n

i=1

(cid:80)p−1

a=1 XiaXi(a+1)MiaMi(a+1)

(cid:41)

M

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

NM φ

n(p−1)(1−φ2) . By an application of the triangle inequality, we see that:

n
(cid:88)

p−1
(cid:88)

i=1

a=1

XiaXi(a+1)MiaMi(a+1) −

α2φ
1 − φ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ t

(cid:41)

M

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ P

1
n(p + 1)
(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(p − 1)

n
(cid:88)

p−1
(cid:88)

i=1

a=1

XiaXi(a+1)MiaMi(a+1) −

NM φ
n(p − 1)(1 − φ2)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ t −

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

φ
1 − φ2

α2 −

NM
n(p − 1)

(cid:41)

M

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

43

Now, consider ﬂattening X into the vector X which has distribution N (0, Σ), where we deﬁne:

Σ ≡








Σ 0
. . .
0 Σ . . .
...
...
0
0

0
0
...
. . .
. . . Σ








This implies that:

1
n(p − 1)

(cid:88)

i,a

XiaXi(a+1)MiaMi(a+1)

d=

1
n(p − 1)

(cid:68)

g, Σ

1
2 AΣ

1
2 g

(cid:69)

where g ∼ N (0, I np), A ∈ Rnp×np and

A =








A1
0
0 A2
...
...
0
0








. . .
. . .

0
0
...
. . .
. . . An

and (Ai)jk = 1 {j = k − 1, M ijM ik = 1}. We will make use of the following Hanson-Wright
inequality (see [Ver18] Theorem 6.2.1):

tn(p − 1)
(cid:13)
(cid:13)
(cid:13)op

1
2 AΣ

1
2

(cid:13)
(cid:13)
(cid:13)Σ











(cid:26)

P

1
n(p − 1)

(cid:12)
(cid:68)
(cid:12)
(cid:12)

g, Σ

1
2 AΣ

1
2 g

(cid:69)

(cid:68)

− E

g, Σ

1
2 AΣ

1
2 g

(cid:27)

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥ t

≤ 2 exp




−c min








(tn(p − 1))2
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)Σ
(cid:13)
F

1
2 AΣ

1
2

,

We now upper bound

(cid:13)
(cid:13)
(cid:13)Σ

1
2 AΣ

1
2

(cid:13)
2
(cid:13)
(cid:13)
F

and

(cid:13)
(cid:13)
(cid:13)Σ

1
2 AΣ

1
2

(cid:13)
(cid:13)
(cid:13)op

. We begin with the former:

(cid:13)
(cid:13)
(cid:13)Σ

1
2 AΣ

1
2

(cid:13)
2
(cid:13)
(cid:13)
F

=

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)Σ

1

2 AiΣ

1
2

(cid:13)
2
(cid:13)
(cid:13)
F

≤ n

(cid:13)
(cid:13)
(cid:13)Σ

1
2 AΣ

1
2

(cid:13)
2
(cid:13)
(cid:13)
F

,

where (A)jk = 1 {j = k − 1}. Noting the covariance structure of Σ, we compute:

(cid:13)
(cid:13)
(cid:13)Σ

n

1
2 AΣ

1
2

(cid:13)
2
(cid:13)
(cid:13)
F

(cid:16)

= nTr

1

2 AT Σ

1
2 Σ

1
2 AΣ

1
2

Σ

(cid:17)

= nTr (cid:0)AΣAT Σ(cid:1)

Computing this trace gives:

nTr (cid:0)AΣAT Σ(cid:1) = n

p−1
(cid:88)

a=2

φ2
(1 − φ2)2

(cid:32)

2 +

2 − φ2(a−1) − φ2(p−a)
1 − φ2

(cid:33)

≤ Cn(p − 1)

where the inequality is by using the assumption φ < 1. Now we tackle

(cid:13)
(cid:13)
(cid:13)Σ
≤ (cid:107)Σ(cid:107)op (cid:107)A(cid:107)op. Note that (cid:107)A(cid:107)op = 1.
Additionally, since Σ is block diagonal with Σ as each of the n blocks, (cid:107)Σ(cid:107)op = (cid:107)Σ(cid:107)op. Now, since

multiplicativity of the operator norm yields

. Sub-

(cid:13)
(cid:13)
(cid:13)op

(cid:13)
(cid:13)
(cid:13)op

(cid:13)
(cid:13)
(cid:13)Σ

1
2 AΣ

1
2 AΣ

1
2

1
2

44

Σ is Toeplitz, [Gra06] Lemma 4.1 implies that (cid:107)Σ(cid:107)op ≤ (1 − φ2)−1(1 − φ)−1 and the concentration
inequality becomes:

(cid:26)

P

1
n(p − 1)

(cid:12)
(cid:68)
(cid:12)
(cid:12)

g, Σ

1
2 AΣ

1
2 g

(cid:69)

(cid:68)

− E

g, Σ

1
2 AΣ

1
2 g

(cid:27)

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥ t

≤ 2 exp (cid:8)−cn(p − 1) min (cid:8)t2, t(cid:9)(cid:9) .

; that is, M(cid:15) denotes the event that NM
(cid:16)

(cid:12)
(cid:12)
(cid:12)

φ
1−φ2

α2 − NM

n(p−1)

n(p−1) estimates α2 with
1−φ2 (cid:15). Thus, by the

(cid:17)(cid:12)
(cid:12) ≥ t − |φ|
(cid:12)

n(p−1)

(cid:12)
(cid:111)
(cid:12)
(cid:12) ≤ (cid:15)

Now, let M(cid:15) =

(cid:110)(cid:12)
(cid:12)α2 − NM
(cid:12)
at most (cid:15) error. Notice that on this event, t −
Hanson-Wright inequality, we have:
(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

p−1
(cid:88)

n
(cid:88)

(cid:88)

P

i=1

a=1

M

1
n(p − 1)
(cid:40)

(cid:40)(cid:18)

XiaXi(a+1)MiaMi(a+1) −

≤ 2 exp

−cn(p − 1) min

|φ|
1 − φ2 (cid:15)
a=2 MiaMi(a−1) − α2(cid:12)
(cid:111)
We now tackle P (cid:8)MC
1
(cid:12)
(cid:12) ≥ (cid:15)
(cid:15)
n(p−1)
bounded diﬀerences inequality (e.g. Theorem 2.9.1 in [Ver18]), we have:

|φ|
1 − φ2 (cid:15)

(cid:9) = P

(cid:110)(cid:12)
(cid:12)
(cid:12)

, t −

(cid:80)n

(cid:80)p

t −

i=1

(cid:19)2

α2φ
1 − φ2

≥ t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:41)(cid:41)

(cid:41)

M

P {M }

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ P {Mc

(cid:15)} .

. Notice that by the

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(p − 1)

n
(cid:88)

p
(cid:88)

i=1

a=2

MiaMi(a−1) − α2

(cid:41)

(cid:26)

≥ (cid:15)

≤ 2 exp

−

(cid:15)2n(p − 1)
2

(cid:27)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Thus,

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(p − 1)
(cid:40)

n
(cid:88)

p−1
(cid:88)

i=1

a=1

≤ 2 exp

−cn(p − 1) min

t −

Taking t (cid:16)

(cid:113) log p

np , (cid:15) (cid:16) t gives:

XiaXi(a+1)MiaMi(a−1) −

α2φ
1 − φ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

≥ t

(cid:40)(cid:18)

(cid:19)2

|φ|
1 − φ2 (cid:15)

, t −

(cid:41)(cid:41)

|φ|
1 − φ2 (cid:15)

(cid:26)

+ 2 exp

−

(cid:15)2n(p − 1)
2

(cid:27)

.

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(p − 1)

n
(cid:88)

p−1
(cid:88)

i=1

a=1

XiaXi(a+1)MiaMi(a−1) −

α2φ
1 − φ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

(cid:115)

(cid:41)

log p
np

≤ 4p−c1,

where c1 > 0 is a universal constant. We now tackle the denominator
We are interested in:

1
n(p−1)

(cid:80)n

i=1

(cid:80)p−1

a=1 X 2

iaMia.

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(p − 1)

n
(cid:88)

p−1
(cid:88)

X 2

iaMia −

a=1
To this end, we deﬁne the matrix A ∈ Rnp×np:

i=1

α
1 − φ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

≥ t



Σ

1
2









A ≡

a M 1a

(cid:17)

1
2

Σ

1
2

Σ

(cid:16)(cid:80)p−1

a=1 eaeT
0
...
0

(cid:16)(cid:80)p−1

0
a=1 eaeT
...
0

a M 2a

(cid:17)

1
2

Σ

. . .

. . .

· · ·

. . . Σ

1
2

0

0
...
a=1 eaeT

(cid:16)(cid:80)p−1

a M na











(cid:17)

1
2

Σ

45

1
We thus see that we can write our estimator
n(p−1)
(cid:13)
1
(cid:13)
(cid:13)Σ

F ≤ n

g ∼ N (0, I np). We can calculate (cid:107)A(cid:107)2
≤ Cnp and
(cid:107)A(cid:107)op ≤ (1 − φ2)−1(1 − φ)−1. We thus combine again the Hanson-Wright inequality and bounded
diﬀerences inequality in the same manner as above to see:

iaMia = 1
a=1 X 2
(cid:13)
2
(cid:1) Σ
(cid:13)
(cid:13)
F

= np

n(p−1) (cid:104)g, Ag(cid:105). Where
(cid:16) 1−φ2p
(1−φ2)3

i=1
a=1 eaeT
a

2 (cid:0)(cid:80)p

(cid:17)

1
2

(cid:80)n

(cid:80)p−1

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(p − 1)
(cid:110)

n
(cid:88)

p−1
(cid:88)

i=1

a=1

≤ 2 exp

−cn(p − 1) min

X 2

iaMia −

(cid:41)

α
1 − φ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(t − C(φ)(cid:15))2 , t − C(φ)(cid:15)

≥ t

(cid:110)

(cid:111)(cid:111)

+ 2 exp (cid:8)−2(cid:15)2n(p − 1)(cid:9) .

Taking t (cid:16)

(cid:113) log p

np , (cid:15) (cid:16) t gives:

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(p − 1)

n
(cid:88)

p−1
(cid:88)

i=1

a=1

X 2

iaMia −

α
1 − φ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

(cid:115)

(cid:41)

log p
np

≤ 4p−c2.

where c2 > 0 is a universal constant. To conclude, we analyze

(cid:113) log p

(cid:12)
noting that with probability at least 1 − cp−C,
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ 1
(cid:12)
2 (which holds for c(cid:96) large enough) implies the result.

(cid:12)
1
(cid:12)
(cid:12)
αnp
(cid:113) log p

iaMia − 1
1−φ2

(cid:80)n
i=1
(cid:113) log p

assumption

np ≤ α

np and

a=1 X 2

1
α2np

(cid:80)p−1

(cid:80)n

1
α2

i=1

α

(cid:80)n

(cid:80)p−1

1
α2np

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:80)p−1

(cid:80)p−1

i=1
1
αnp

a=1 XiaXi(a+1)MiaMi(a+1)
(cid:80)n

− φ
(cid:12)
a=1 XiaXi(a+1)MiaMi(a+1) − φ
(cid:12)
(cid:12) ≤
1−φ2
np . Using these two facts as well as the

a=1 X 2

iaMia

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

F.3 Proof of lemma B.2

We re-state the lemma for the reader’s convenience:

Lemma B.2. Under the assumptions of theorem 5, for any u ∈ Rp, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥

Cσ2
X
α2 (cid:107)u(cid:107)1

(cid:115)

(cid:41)

log p
np

≤ c0p−c1,

where C, c0, c1 denote universal constants.

Proof. Let us begin by writing:

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= max
a∈[p]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

p
(cid:88)

i=1

b=1

(cid:16)

(cid:98)Xia

(cid:101)Xib − (cid:98)Xib

(cid:17)

ub

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Using (F.1), we write:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

p
(cid:88)

i=1

b=1

max
a∈[p]

(cid:16)

(cid:98)Xia

(cid:101)Xib − (cid:98)Xib

(cid:17)

ub

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= max
a∈[p]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

(cid:88)

i,b

(cid:98)Xia( (cid:98)φ − φ)

(cid:16)

Xi,L(b)f (cid:48)

L(b)(ξL(b)) + Xi,R(b)f (cid:48)

R(b)(ξR(b))

(cid:17)

ub

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

The triangular inequality then implies that the RHS is upper bounded by:

| (cid:98)φ − φ| max
a∈[p]

p
(cid:88)

b=1

|ub|

1
n

n
(cid:88)

(cid:16)

i=1

| (cid:98)Xi,aXi,L(b)f (cid:48)

L(b)(ξL(b))| + | (cid:98)XiaXi,R(b)f (cid:48)

(cid:17)
R(b)(ξR(b))|

.

(F.2)

46

Lemma F.1 i. implies that (F.2) is upper bounded by:

C| (cid:98)φ − φ| (cid:107)u(cid:107)1 max

a,b∈[p]×[p]

1
n

n
(cid:88)

(cid:16)

i=1

| (cid:98)XiaXi,L(b)| + | (cid:98)XiaXi,R(b)|

(cid:17)

.

We are thus led to study:

(cid:40)

P

max
a,b∈[p]×[p]

1
n

n
(cid:88)

(cid:16)

i=1

| (cid:98)XiaXi,L(b)| + | (cid:98)XiaXi,R(b)|

(cid:17)

(cid:41)

(cid:40)

≥ t

≤ P

max
a,b∈[p]×[p]

1
n

n
(cid:88)

i=1

| (cid:98)XiaXi,L(b)| ≥

(cid:41)

,

t
2

where the inequality follows by an application of the union bound, and inequality P {A + B ≥ t} ≤
2 P (cid:8)A ≥ t
(cid:9) for identitically distributed (but not necessarily independent) non-negative random
2
variables A and B. Let A1 denote the event
and
notice that lemma F.1 iii. implies P{A1} ≥ 1 − 2p2− A2
let A2 denote the event that | (cid:98)φ − φ| ≤ 4
α2
The result follows immediately.

X ). Additionally,
np and notice that by lemma B.1, P{A2} ≥ 1 − cp−C.

2 (that is we take t = 36eσ2

i=1| (cid:98)XiaXi,L(b)| ≥ 18eσ2
X

maxa,b∈[p]×[p]

(cid:113) log p

(cid:80)n

1
n

(cid:110)

(cid:111)

F.4 Proof of lemma B.4

We re-state the lemma for the reader’s convenience:

Lemma B.4. Under the assumptions of theorem 5, we have:

(cid:16)

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C

σ2
X
α4 (cid:107)u(cid:107)1

log p
np

(cid:27)

≤ c0p−c1,

where C, c0, c1 denote universal constants.

Proof. We begin by writing:

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= max
a∈[p]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

p
(cid:88)

(cid:16)

i=1

b=1

(cid:101)Xia − (cid:98)Xia

(cid:17) (cid:16)

(cid:101)Xib − (cid:98)Xib

Using (F.1), this can be written as:

(cid:12)
(cid:12)
(cid:12) (cid:98)φ − φ

(cid:12)
2
(cid:12)
(cid:12)

max
a∈[p]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

p
(cid:88)

(cid:89)

i=1

b=1

k∈{a,b}

(cid:0)Xi,L(k)f (cid:48)

L(ξL(k)) + Xi,R(k)f (cid:48)

R(ξR(k))(cid:1) ub

(cid:17)

ub

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

The triangular inequality, expanding the product and using lemma F.1 i., we can upper bound this
quantity by:

C max
a∈[p]

(cid:12)
(cid:12)
(cid:12) (cid:98)φ − φ

(cid:12)
2
(cid:12)
(cid:12)

p
(cid:88)

|ub|

b=1

1
n

n
(cid:88)

i=1

≤ C

(cid:12)
(cid:12)
(cid:12) (cid:98)φ − φ

(cid:12)
2
(cid:12)
(cid:12)

(cid:107)u(cid:107)1 max

a,b∈[p]×[p]

(cid:0)|Xi,L(a)Xi,L(b)| + |Xi,L(a)Xi,R(b)| + |Xi,R(a)Xi,L(b)| + |Xi,R(a)Xi,R(b)|(cid:1)

1
n

n
(cid:88)

i=1

(cid:0)|Xi,L(a)Xi,L(b)| + |Xi,L(a)Xi,R(b)| + |Xi,R(a)Xi,L(b)| + |Xi,R(a)Xi,R(b)|(cid:1) .

47

Just as in the proof of lemma B.2, we are led to study:

(cid:40)

P

max
a,b∈[p]×[p]

1
n

n
(cid:88)

| (cid:98)XiaXi,L(b)| ≥

i=1

(cid:41)

.

t
4

Lemma F.1 iii. with t = 72eσ2
n
(cid:88)

max
a,b∈[p]×[p]

1
n

i=1

X implies that :

(cid:0)|Xi,L(a)Xi,L(b)| + |Xi,L(a)Xi,R(b)| + |Xi,R(a)Xi,L(b)| + |Xi,R(a)Xi,R(b)|(cid:1) ≤ 72eσ2
X ,

with probability at least 1 − p2− A2
probability at least 1 − cp−C. The result follows immediately.

2 . Additionally, lemma B.1 implies that

(cid:12)
(cid:12)
(cid:12) (cid:98)φ − φ

(cid:12)
2
(cid:12)
(cid:12)

≤ 16
α4

log p
np with

F.5 Proof of lemma B.5

We re-state the lemma here for ease of reading:
Lemma B.5. Under the assumptions of theorem 5, for any u ∈ Rp, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:99)X − X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C

σ2
X
α2 (cid:107)u(cid:107)1

(cid:115)

(cid:41)

log p
np

≤ c0p−c1,

where C, c0, c1 denote universal constants.

Proof. We begin by writing:

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:99)X − X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= max
a∈[p]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

p
(cid:88)

(cid:16)

i=1

b=1

(cid:101)Xia − (cid:98)Xia

(cid:17) (cid:16)

(cid:98)Xib − Xib

(cid:17)

ub

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

The triangular inequality, (F.1), and lemma F.1 i. imply this is upper bounded by:

C

(cid:12)
(cid:12)
(cid:12) (cid:98)φ − φ

(cid:12)
(cid:12)
(cid:12) (cid:107)u(cid:107)1 max

a,b∈[p]×[p]

1
n

Lemma F.1 iii. with t = 72eσ2

i=1
X implies

n
(cid:88)

(cid:16)

|Xi,L(a) (cid:98)Xib| + |Xi,L(a)Xib| + |Xi,R(a) (cid:98)Xib| + |Xi,R(a)Xib|

(cid:17)

.

max
a,b∈[p]×[p]

1
n

n
(cid:88)

(cid:16)

i=1

|Xi,L(a) (cid:98)Xib| + |Xi,L(a)Xib| + |Xi,R(a) (cid:98)Xib| + |Xi,R(a)Xib|

(cid:17)

≤ 72eσ2
X ,

with probability at least 1 − p2− A2
at least 1 − cp−C. The result follows immediately.

2 and lemma B.1 implies that | (cid:98)φ − φ| ≤ 4
α2

(cid:113) log p

np with probability

F.6 Proof of lemma B.6

We re-state the lemma here for ease of reading:

Lemma B.5. Assuming λ as deﬁned in equation (B.1), and the assumptions of theorem 5, we
have, we have:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

T (cid:16)

1
n

(cid:99)X

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:27)

≤

λ
16

≥ 1 − c0p−c1,

where c0, c1 denote universal constants.

Proof. This follows immediately from lemma A.2.

48

F.7 Proof of lemma B.7

We repeat the lemma here:

Lemma B.7. Assuming λ as deﬁned in equation (B.1), and the assumptions of theorem 5, we
have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:102)X − (cid:99)X

(cid:17)T

(cid:15)

(cid:16)

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C

σX σ
α2

(cid:114)

(cid:41)

log p
n

≤ c0p−c1,

where C, c0, c1 are universal constants.

Proof. We will write:

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

= max
a∈[p]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

(cid:16)

i=1

(cid:101)Xia − (cid:98)Xia

(cid:17)

(cid:15)i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Using equation (F.1), the triangular inequality, and lemma F.1 i., this quantity is upper bounded
by:

C

(cid:12)
(cid:12)
(cid:12) (cid:98)φ − φ

(cid:12)
(cid:12)
(cid:12) max
a∈[p]

1
n

n
(cid:88)

i=1

(cid:0)|Xi,L(a)(cid:15)i| + |Xi,R(a)(cid:15)i|(cid:1) .

We are interested in

(cid:40)

P

max
a∈[p]

1
n

n
(cid:88)

i=1

(cid:0)|Xi,L(a)(cid:15)i| + |Xi,R(a)(cid:15)i|(cid:1) ≥ t

(cid:41)

(cid:40)

≤ 2 P

max
a∈[p]

1
n

n
(cid:88)

i=1

|Xi,L(a)(cid:15)i| ≥

(cid:41)

.

t
2

We will require a concentration inequality similar to lemma F.1 iii. Lemma F.1 ii.
implies
that Xi,L(a) is σ2
lemma H.3
implies that for any |θ| ≤ (16eσX σ)−1, |Xi,L(a)(cid:15)i| satisﬁes E exp (cid:8)θ (cid:0)|Xi,L(a)(cid:15)i| − E|Xi,L(a)(cid:15)i|(cid:1)(cid:9) ≤
X σ2(cid:9). Thus, Bernstein’s inequality I.2 with θ = (16eσX σ)−1 and σ2
exp (cid:8)128eθ2σ2
X σ2
implies:

X sub-gaussian and by assumption (cid:15)i

is σ2 sub-gaussian. Thus,

Z = 256e2σ2

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:0)|Xi,L(a)(cid:15)i| − E|Xi,L(a)(cid:15)i|(cid:1)

(cid:41)

(cid:26)

≥ t

≤ 2 exp

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

min

n
2

t2
(16eσX σ)2 ,

t
16eσX σ

(cid:19)(cid:27)

.

Taking t = 16eAσX σ
yields:

(cid:113) log p

n , a union bound over [p] and recalling the assumption that A

n < 1

(cid:113) log p

(cid:40)

P

max
a∈[p]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:0)|Xi,L(a)(cid:15)i| − E|Xi,L(a)(cid:15)i|(cid:1)

≥ 16eAσX σ

(cid:114)

(cid:41)

log p
n

≤ 2p1− A2
2 .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Noting the assumption that A

(cid:113) log p

n < 1 and fact H.4, this implies:

(cid:40)

P

max
a∈[p]

1
n

n
(cid:88)

i=1

|Xi,L(a)(cid:15)i| ≥ 18eAσX σ

(cid:41)

≤ 2p1− A2
2 .

(cid:113) log p

Therefore, we take t = 36eAσX σ in (F.7). Additionally, we recall that lemma B.1 implies | (cid:98)φ − φ| ≤
4
np with probability at least 1 − cp−C. Putting these together implies that with probability
α2
at least 1 − cp−C:

C

(cid:12)
(cid:12)
(cid:12) (cid:98)φ − φ

(cid:12)
(cid:12)
(cid:12) max
a∈[p]

1
n

n
(cid:88)

i=1

(cid:0)|Xi,L(a)(cid:15)i| + |Xi,R(a)(cid:15)i

(cid:1) ≤ C

(cid:115)

σX σ
α2

log p
np

.

49

The result is immediate.

F.8 Proof of lemma B.8

Lemma B.6. Assuming λ as deﬁned in equation (B.1), and the assumptions of theorem 5, we
have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

T

(cid:15)

(cid:99)X

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ CσX σ

(cid:114)

(cid:41)

log p
n

≥ c0p−c1,

where C, c0, c1 are universal constants.

Proof. This is immediate from lemma A.3.

F.9 Proof of λmin(Σ

(cid:99)X) > 0

As mentioned after the assumptions of the main theorem, we need to verify a lower bound on the
minimum eigenvalue of the covariance. We remark that this is included to show a speciﬁc case
of when the minimum eigenvalue can be lower bounded by a constant. For this reason, we have
carried out the analysis in far from the tightest way possible and with more work, we believe the
assumption α > 0.844 need not be so restrictive. We have the following lemma:

Lemma F.2. Under the assumptions of theorem 5, Σ

(cid:99)X satisﬁes λmin(Σ

(cid:99)X ) > 0.

Proof. Consider a row of X, let it be x and a row of (cid:99)X which we call (cid:98)x. Note that ΣX = E xxT
and Σ

(cid:99)X = E

(cid:98)x(cid:98)xT . Notice:

E xxT = E (x − (cid:98)x + (cid:98)x) (x − (cid:98)x + (cid:98)x)T = E

(cid:16)

(cid:98)x(cid:98)xT − E (x − (cid:98)x) (x − (cid:98)x)T .
E (x − (cid:98)x) (x − (cid:98)x)T (cid:17)
(cid:98)x(cid:98)xT (cid:17)

E

(cid:0)E xxT (cid:1) − λmax
This implies that λmin
. Let x0 denote
the zero-imputed estimator for the same observation z for which (cid:98)x = E x | z. Then by the
(cid:0)E xxT (cid:1) −
orthogonality property of the condtional expectation, we have λmin

≥ λmin

≥ λmin

E

(cid:16)

(cid:16)

(cid:98)x(cid:98)xT (cid:17)

(cid:16)

E (x − x0) (x − x0)T (cid:17)

. Notice now that E (x − x0) (x − x0)T = (1−α)2ΣX −α(1−α) diag (ΣX ),

λmax
which has maximum eigenvalue (1 − α)2λmax(ΣX ) − α(1−α)
plies that λmin(ΣX ) ≥ 1
0.

4 and we know λmax(ΣX ) ≤ 3. Thus, assuming α > 0.844, λmin(Σ

1−φ2 . Now, Gershgorin’s circle theorem im-
(cid:99)X ) >

G Proofs of technical lemmas from appendix B.2

This appendix is dedicated to the proofs needed for theorem 6. It is organized as follows: sub-
section G.1 provides a useful lemma which allows us to control the empirical covariance, sub-
section G.2 provides a series of lemmas controlling the Markov blankets, subsection G.3 provides
a series of technical lemmas which will be used multiple times, and subsections G.4- G.7 provide
proofs for lemmas B.9 - B.14.

50

G.1 Control of the empirical covariance

We begin with a lemma controlling the empirical covariance.

Lemma G.1. Consider the empirical covariance matrix

(cid:101)Σ =

1
α2n

n
(cid:88)

i=1

X 0,iX (cid:62)

0,i −

1 − α
α2n

n
(cid:88)

i=1

(cid:16)

diag

X 0,iX (cid:62)
0,i

(cid:17)

.

We have the following two results:

(i.) For any submatrices ΣSS, (cid:101)ΣSS and u ≥ 0, if

(cid:113) |S|+u

n ≤ 1
(cid:114)

2 , we have:

(cid:13)
(cid:13)
(cid:13) (cid:101)ΣSS − ΣSS

(cid:13)
(cid:13)
(cid:13)op

≤

C(1 − α) (cid:107)Σ(cid:107)op
α2

|S| + u
n

with probability at least 1 − e−u.
√

(ii.) Assume that for some A ≥

3, A
α

(cid:113) log p

n ≤ 1. Then,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:101)Σ − Σ
(cid:13)(cid:96)1→(cid:96)∞

≤

CA (cid:107)Σ(cid:107)op
α2

(cid:114)

log p
n

with probability at least 1 − p3−A2.

[Ver18],
Proof. i. The proof follows by an (cid:15)-net argument and is a straightforward extension of
Exercise 4.7.3. First, let N1/4 be a 1
4 -net of the unit sphere S|S|−1. For convenience, we will drop
the S dependence of ˜Σ and Σ for the remainder of the proof, noting that both matrices are square
(cid:69)(cid:12)
(cid:12)
with dimensions |S| × |S|. Note now that
(cid:12). Thus, we

(cid:12)
(cid:68)(cid:16) ˜Σ − Σ
(cid:17)
(cid:12)
(cid:12)

≤ 2 supu∈N1/4

(cid:13)
(cid:13)
˜Σ − Σ
(cid:13)
(cid:13)
(cid:13)op
(cid:13)

u, u

have:

where

P

(cid:26)(cid:13)
(cid:13)
˜Σ − Σ
(cid:13)
(cid:13)
(cid:13)op
(cid:13)

(cid:27)

≥ t

≤

(cid:88)

u∈N1/4

P

(cid:26)(cid:12)
(cid:12)
(cid:12)

(cid:68)(cid:16) ˜Σ − Σ
(cid:17)

u, u

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥

(cid:27)

t
2

(cid:88)

≤

(cid:26)

P

|U1(u)| ≥

u∈N1/4

(cid:27)

t
4

+ P

(cid:26)

|U2(u)| ≥

(cid:27)

t
4

(cid:42)

(cid:32)

U1(u) =

u,

(cid:42)

(cid:32)

U2(u) =

u,

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

1
α2 ZiZT

i − E 1

α2 Z1ZT

1

(cid:33)

(cid:43)

u

1 − α

α2 diag (cid:0)ZiZT

i

(cid:1) − E 1 − α

α2 diag (cid:0)Z1ZT

1

(cid:33)

(cid:43)

u

(cid:1)

Let us ﬁrst tackle the term U1(u). To this end, we re-write:

(cid:26)

P

|U1(u)| ≥

(cid:27)

t
4

= P

(cid:40)

1
α2n

(cid:12)
n
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:104)Zi, u(cid:105)2 − E (cid:104)Zi, u(cid:105)2

(cid:41)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

t
4

51

Notice that E eλ(cid:104)Zi,u(cid:105) ≤ e

1

2 λ2(cid:107)Σ(cid:107)op(cid:107)u(cid:107)2

E exp

(cid:16)

(cid:110)

λ

Thus, if t ≤

32e(cid:107)Σ(cid:107)op
α2

, we have:

2. Thus, by Lemma H.2, we have ∀λ ≤
(cid:104)Zi, u(cid:105)2 − E (cid:104)Zi, u(cid:105)2(cid:17)(cid:111)

≤ eλ232e2(cid:107)Σ(cid:107)2

op

1
8e(cid:107)Σ(cid:107)op

:

(cid:40)

P

1
α2n

(cid:12)
n
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:104)Zi, u(cid:105)2 − E (cid:104)Zi, u(cid:105)2

(cid:41)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

t
4

(cid:40)

≤ 2exp

−

(cid:41)

nα4t2
1024e2 (cid:107)Σ(cid:107)2
op

which implies by the union bound:

(cid:88)

(cid:26)

P

|U1(u)| ≥

u∈N1/4

(cid:27)

t
4

≤ 2 (cid:12)

(cid:12)N1/4

(cid:12)
(cid:12) exp

(cid:40)

−

nα4t2
1024e2 (cid:107)Σ(cid:107)2
op

(cid:41)

We now tackle U2(u). First, we see that:

U2(u) =

1 − α
α2n

n
(cid:88)

i=1

We are interested in

(cid:10)u, (cid:0)diag (cid:0)ZiZT

i

(cid:1) − E diag (cid:0)ZiZT

i

(cid:1)(cid:1) u(cid:11) =

1 − α
α2n

n
(cid:88)

|S|
(cid:88)

i=1

j=1

u2
j

(cid:0)Z2

ij − E Z2
ij

(cid:1)

E exp




λ

|S|
(cid:88)



j=1

u2
j

(cid:0)Z2

ij − E Z2
ij

(cid:1)






= 1 +

≤ 1 +

∞
(cid:88)

k=2

∞
(cid:88)

k=2

λk E

(cid:26)(cid:16)(cid:80)|S|

j=1 u2
j

(cid:0)Z2

ij − E Z2
ij

(cid:1)(cid:17)k(cid:27)

k!

λk E

(cid:26)(cid:16)(cid:80)|S|

j=1 u2

j Z2
ij

(cid:17)k(cid:27)

k!

Where the inequality follows by the non-negativity of Z2
1, u2

ij. Notice now that since u lies on the
lies on the simplex, and thus Jensen’s inequality (with

unit sphere, the vector

2, . . . , u2
|S|

u2

(cid:16)

(cid:17)

respect to the discrete distribution formed by the squared elements of u) gives
(cid:80)|S|

j=1 u2

j Z2k

ij . Thus, we have:

(cid:16)(cid:80)|S|

j=1 u2

j Z2
ij

(cid:17)k

≤

λk E

1 +

∞
(cid:88)

k=2

(cid:26)(cid:16)(cid:80)|S|

j=1 u2

j Z2
ij

k!

(cid:17)k(cid:27)

≤ 1 +

≤ 1 +

∞
(cid:88)

k=2

∞
(cid:88)

k=2

λk (cid:80)|S|

j=1 u2
j
k!

E Z2k
ij

(cid:16)

λk2k

(cid:17)k

2 (cid:107)Σ(cid:107)op
k!

Γ(k) (cid:107)u(cid:107)2
2

where the ﬁrst inequality follows by the calculations of Equation H.1 and noticing that each random
variable Zij has sub-Gaussian constant at most (cid:107)Σ(cid:107)op. Now, under the assumption λ <
,
we have:

1
8e(cid:107)Σ(cid:107)op

(cid:16)

λk2k

(cid:17)k

2 (cid:107)Σ(cid:107)op
k!

1 +

∞
(cid:88)

k=2

Γ(k) (cid:107)u(cid:107)2
2

≤ exp

(cid:110)

32e2λ2 (cid:107)Σ(cid:107)2
op

(cid:111)

52

Thus, if t ≤

32(1−α)e(cid:107)Σ(cid:107)op
α2

, we have

(cid:88)

(cid:26)

P

|U1(u)| ≥

u∈N1/4

≤ 2 (cid:12)

(cid:12)N1/4

(cid:12)
(cid:12) exp

(cid:40)

−

(cid:27)

t
4

nα4t2
1024(1 − α)2e2 (cid:107)Σ(cid:107)2
op

(cid:41)

√

(cid:12)N1/4

Noting that (cid:12)

(cid:12)
(cid:12) ≤ 9|S| and taking t =

n ≤ 1
2 .
ii. Notice that the oﬀ-diagonal entries of (cid:101)ΣSS are given by 1
i=1 Zi(cid:96)Zik and the diagonal en-
α2n
tries are given by 1
i(cid:96). We notice that repeating the argument of Lemma ?? using a sym-
αn
metrization argument to account for products with non-zero mean implies E eλ(Zi(cid:96)Zik−E Zi(cid:96)Zik) ≤
e128e2λ2(cid:107)Σ(cid:107)2
op for all

gives the result as long as

i(cid:96)) ≤ e32e2λ2(cid:107)Σ(cid:107)2

op for all λ ≤

64e(1−α)(cid:107)Σ(cid:107)op
n

i=1 Z2

i(cid:96)−E Z2

(cid:80)n

(cid:80)n

|S|+u

α2

√

(cid:113) |S|+u

λ ≤

1
8e(cid:107)Σ(cid:107)op

. This implies that for t ≤

1
16e(cid:107)Σ(cid:107)op

. Similarly, by Lemma H.2, E eλ(Z2
8e(cid:107)Σ(cid:107)op
α

16e(cid:107)Σ(cid:107)op
α2

∧

:

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

1
α2n
(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

Zi(cid:96)Zik − Σ(cid:96)k

i=1

1
αn

n
(cid:88)

i=1

Z2

i(cid:96) − Σ(cid:96)(cid:96)

(cid:41)

(cid:40)

≥ t

≤ 2exp

−

(cid:41)

(cid:40)

≥ t

≤ 2exp

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

α4nt2
256e2 (cid:107)Σ(cid:107)2
op
(cid:41)

α2nt2
64e2 (cid:107)Σ(cid:107)2
op

Thus for p ≥ 2 a union bound gives:

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:101)Σ − Σ
(cid:13)(cid:96)1→(cid:96)∞

(cid:27)

(cid:40)

≥ t

≤ exp

−

α4nt2
256e2 (cid:107)Σ(cid:107)2
op

(cid:41)

+ 3 log p

Thus, taking t =
least 1 − p3−A2

A·16e(cid:107)Σ(cid:107)op
α2

(cid:113) log p

n and assuming A

α

(cid:113) log p

n ≤ 1 yields the result with probability at

G.2 Control of Markov blankets

Lemma G.2. Under the assumptions of theorem 6, for any u ∈ Rp, we have the following:

i. E

ii. E

(cid:26)(cid:16)(cid:80)p

b=1 |ub| (cid:112)S(i,b)

(cid:80)

j∈S(i,b)

(cid:17)2(cid:27)

|Xij|

≤ (cid:107)u(cid:107)2

1 (cid:107)Σ(cid:107)op

E S3

(i,b), and

(cid:26)(cid:16)(cid:80)p

b=1 |ub| (cid:112)S(i,b)

(cid:80)

j∈S(i,b)

(cid:17)4(cid:27)

|Xij|

≤ 3 (cid:107)u(cid:107)4

1 (cid:107)Σ(cid:107)2

op

E S6

(i,b).

Proof.

i. We ﬁrst expand:

p
(cid:88)

E










(cid:113)

S(i,b)

|ub|

(cid:88)

b=1

j∈S(i,b)



|Xij|



2







= E

(cid:88)

(cid:89)



b,c∈[p]×[p]

I∈{b,c}

(cid:113)

S(i,I)

|uI |

(cid:88)

j∈S(i,I)

|X ij|






We ﬁrst look at the terms b = c and the quantity of interest is thus E
(cid:26)

Noticing that E X 2

ij ≤ (cid:107)Σ(cid:107)op and E X ijXik ≤ (cid:107)Σ(cid:107)op shows E

(cid:16)(cid:80)

j∈S(i,b)

(cid:26)

S(i,b)
(cid:16)(cid:80)

(cid:17)2(cid:27)

.

|Xij|
(cid:17)2(cid:27)

S(i,b)

j∈S(i,b)

|Xij|

≤

53

(cid:107)Σ(cid:107)op



E

E S3

(i,b). Similarly noting that by Cauchy-Schwarz, E (cid:112)S(i,b)S(i,c) ≤ E Si,b, we see that:









(cid:113)

(cid:113)

(cid:88)

(cid:89)

(cid:88)

(cid:88)

(cid:88)

(cid:89)

|uI |

S(i,I)

|X ij|

=

|uI | E

S(i,I)



|X ij|



j∈S(i,I)



b(cid:54)=c∈[p]×[p]

I∈{b,c}

j∈S(i,I)



≤

b(cid:54)=c∈[p]×[p]
(cid:88)

I∈{b,c}
(cid:89)

b(cid:54)=c∈[p]×[p]

I∈{b,c}

|uI | (cid:107)Σ(cid:107)op

E S3

(i,b)

Summing the two pieces gives the desired result.

ii. First notice that 4 identically distributed, positive random variables Y1, Y2, Y3, Y4 are such that
(cid:0)(Y1Y2)2 + (Y3Y4)2(cid:1) and
1 where the second inequality is by Cauchy-Schwarz. Now, expanding

1 . One can see this by noticing that Y1Y2Y3Y4 ≤ 1
2

E Y1Y2Y3Y4 ≤ EY 4
that E(Y1Y2)2 ≤ E Y 4
and noting that E |Xij|4 ≤ 3 (cid:107)Σ(cid:107)2

op gives the result (following the same recipe as above).

In the remaining of this subsection, we bound the moment generating function (MGF) of the

Markov blanket.

Let G = (V = [d], E) be the sparsity graph of the precision matrix Ω; i.e., an edge is present if
and only if the corresponding entry in Ω is not zero. Each vertex is independently declared ‘open’
with probability α and ‘closed’ otherwise. We let ωv ∈ {0, 1} indicate the state of a vertex v (open
if ωv = 1 and closed otherwise). The Markov blanket of the random variable Xi,u (at vertex u ∈ V )
is the set of ﬁrst open vertices encountered by all walks starting at vertex u on the graph. We
denote its size by SG(v). Let ST (v) denote the size of the Markov blanket of v in the dmax-regular
tree T rooted at v, which we deﬁne in the same way. We ﬁrst compare SG(v) with ST (v), and then
perform a recursive argument to bound the MGF of latter.

Lemma G.3. Let v ∈ V . There exists a coupling of SG(v) and ST (v) such that P(SG(v) ≤
ST (v)) = 1.

Proof. The construction of the coupling relies on the notion of the path tree of a graph [God81]. The
path tree of G rooted at v is a ﬁnite tree rooted at v whose vertices are simple paths (v, v1, · · · , vl)
in G starting at v (a path is simple if no vertex appears more than once in it). Two paths are
adjacent if one can be obtained by appending one new vertex to the other, i.e., edges of the path
tree are of the form (v, v1, · · · , vl) ∼ (v, v1, · · · , vl+1).

Let ˜T be the path tree rooted at v obtained from G. We ﬁrst associate to every path (v, v1, · · · , vl)
the state ωvl of its endpoint in G. We then consider all paths staring at v and having the same
endpoint u. If there is more than one such path (and this will be the case if the graph G contains
cycles) then we independently resample the state (open/closed) of all but one path. (We do this for
all u (cid:54)= v.) The path whose state is not resampled is chosen arbitrarily, e.g., uniformly at random.
In this way we have constructed a Bernoulli site percolation process on the tree ˜T such that the
size of the Markov blanket of v in G is almost surely upper-bounded by that of v in ˜T .

Lastly, the tree ˜T has maximal degree dmax but may not be regular (it is not if G is not
regular). We extend it to an inﬁnite dmax-regular tree T rooted at v and associate to the extra
vertices independent states sampled with probability α. The size of the Markov blanket of v can
only grow with this operation.

Now we bound the moment generating function of ST (v). Let u be an oﬀspring of v. Let S↓

be the number of ﬁrst open vertices which are descendants of u, and S↓≤l

T (u)
T (u) the number of ﬁrst

54

open vertices up to distance l from u. Deﬁne χ(θ) ≡ E eθS↓
T (u) , θ ∈ R.
These functions do not depend on the speciﬁc choice of u. It is clear by monotone convergence that
χ≤l converges pointwise to χ as l → ∞. Also note that χ is so far only deﬁned formally: contrarily
to S↓≤l

T (u) and χ≤l(θ) ≡ E eθS↓≤l

T (u) which is ﬁnite almost surely, S↓

T (u) may be inﬁnite.

Lemma G.4. We have χ≤0 = αeθ + 1 − α, and the functions χ≤l, l ≥ 1 satisfy the recursion:
χ≤l(θ) = αeθ + (1 − α)(χ≤l−1(θ))dmax−1.

Proof. Since S↓≤0
exploit the recursive structure of the tree. The vertex u has dmax − 1 oﬀsprings and

(u) = 1 with probability α and 0 otherwise, the ﬁrst statement follows. Next, we

T

S↓≤l
T (u) =

(cid:40)
1
(cid:80)

with probability α

w∼u Z(w) with probability 1 − α,

where Zw is the number of ﬁrst open vertices which are descendants of w and are within distance
l − 1 from w. Since (Z(w))w∼u are independent and have the same distribution as S↓≤l−1
(u), we
have

T

E eθS↓≤l

T (u) = αeθ + (1 − α) E eθ (cid:80)

w∼u S↓≤l−1

T

(w)

= αeθ + (1 − α)

(cid:16)

E eθS↓≤l−1

T

(u)(cid:17)dmax−1

.

Next, Lemma G.6 follows from Lemma G.3 and the following result.

Lemma G.5. If (1 − α)(dmax − 1) < 1 then the random variable ST (v) has sub-exponential tail.
That is, there exists a constant c = c(α, dmax) > 0 depending only on α, dmax such that:

P {ST (v) ≥ t} ≤ 2e−ct.

Proof. It suﬃces to prove that there exists θ > 0 such that χ(θ) < +∞ when (1 − α)(dmax − 1) < 1.
Indeed, by the argument used in Lemma G.4, the MGF of v (the parent of u and root of the tree)
is given by E eθST (v) = αeθ + (1 − α)χ(θ)dmax. Therefore the argument boils down to analyzing
the convergence of χ≤l to a ﬁxed point of fθ(x) ≡ αeθ + (1 − α)xdmax−1. We have f (cid:48)
θ(x) =
θ(x) = 1 is x∗ = ((dmax − 1)(1 − α))−1/(dmax−2).
(dmax − 1)(1 − α)xdmax−2. The point at which f (cid:48)
The equation fθ(x) = x has two roots if fθ(x∗) ≤ x∗ (which merge when fθ(x∗) = x∗) and no roots
otherwise. After rearranging the inequality fθ(x∗) ≤ x∗, this provides a bound on θ:

αeθ ≤

dmax − 2
dmax − 1

1
((dmax − 1)(1 − α))−1/(dmax−2)

.

Let ¯θ = ¯θ(α, dmax) the maximal value of θ such that the above bound holds, and let θ < ¯θ. It
remains to show that the ﬁxed point iteration xl = fθ(xl−1), x0 = 1 converges to one of the two
roots when (dmax − 1)(1 − α) < 1. This will be true if x0 is smaller than the largest ﬁxed point,
call it x+. We observe that fθ has derivative larger than 1 the second time it crosses the diagonal:
f (cid:48)
θ(x+) > 1. Since f (cid:48)
θ(x0) < 1 to ensure that x0 < x+. And
θ(x0) = (1 − α)(dmax − 1). To sum up, if (1 − α)(dmax − 1) < 1 and θ < ¯θ then (χ≤l(θ))l≥0
we have f (cid:48)
converges to a ﬁnite limit, hence S↓
T (u) has a ﬁnite MGF on [0, ¯θ] (and is the smallest of the two
ﬁxed points of fθ.)

θ is increasing it suﬃces to check that f (cid:48)

55

The next lemma follows immediately from Lemma G.3 and the preceding result.

Lemma G.6. The random variables S(i, b) have sub-exponential tails. That is, there exists a
constant c = c(α, dmax) > 0 depending only on α, dmax such that for all t ≥ 0,

P {S(i, b) ≥ t} ≤ c0e−ct,

where c0 is a universal constant.

The same strategy implies the following result:

Lemma G.7. Suppose that (1 − α)(dmax − 1)k < 1. Then, the Markov blankets S(i,b) satisfy:

E Sk

(i,b) ≤ α + (1 − α)dk

max ·

α
1 − (1 − α)(dmax − 1)k

G.3 Useful lemmas

We will use many times the lemmas in this sub-section.

Lemma G.8. For all b ∈ [p] and i ∈ [n], we have:

(cid:16)

(cid:13)
(cid:13)
(cid:13)

Σb,S(i,b) − (cid:101)Σb,S(i,b)

(cid:17)

Σ−1

S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)∞

≤ C(α, dmax)

(cid:113)(cid:12)

(cid:12)S(i,b)

(cid:114)

(cid:12)
(cid:12)

log 2np
n

,

and

(cid:13)
(cid:13)
(cid:13) (cid:101)Σb,S(i,b)

(cid:16)

Σ−1

S(i,b),S(i,b)

− (cid:101)Σ

−1
S(i,b),S(i,b)

(cid:17)(cid:13)
(cid:13)
(cid:13)∞

≤ C(α)

(cid:113)(cid:12)

(cid:12)S(i,b)

(cid:114)

(cid:12)
(cid:12)

log p
n

,

with probability at least 1 − c
only on α.

p , where c is a universal constant and C(α) is a constant depending

Proof. We examine the second term ﬁrst. Note that for some vector x ∈ Rd and square matrix
A ∈ Rd×d, we have the following inequality (letting A(i) denote the ith column of A):

(cid:13)xT A(cid:13)
(cid:13)

(cid:13)∞ = max

i=1,2,...,d

(cid:12)
(cid:12)
(cid:12)

x, A(i)(cid:69)(cid:12)
(cid:68)
(cid:12)
(cid:12) ≤ max

i=1,2,...,d

(cid:107)x(cid:107)∞

√

(cid:13)
(cid:13)

(cid:13)A(i)(cid:13)
(cid:13)
(cid:13)1

≤

d max
i=1,...,d

(cid:107)x(cid:107)∞

√

(cid:13)
(cid:13)

(cid:13)A(i)(cid:13)
(cid:13)
(cid:13)2

≤

d (cid:107)x(cid:107)∞ (cid:107)A(cid:107)op ,

which implies immediately:

(cid:13)
(cid:13)
(cid:13) (cid:101)Σb,S(i,b)

(cid:16)

Σ−1

S(i,b),S(i,b)

− (cid:101)Σ

−1
S(i,b),S(i,b)

(cid:17)(cid:13)
(cid:13)
(cid:13)∞

≤

(cid:13)
(cid:13)
(cid:13) (cid:101)Σb,S(i,b)

(cid:13)
(cid:13)
(cid:13)∞

(cid:113)

S(i,b)

(cid:13)
(cid:13)Σ−1
(cid:13)

S(i,b),S(i,b)

− (cid:101)Σ

−1
S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)op

.

−1
S(i,b),S(i,b)

(cid:16)

Our strategy will be to take the power series expansion of (cid:101)Σ
where for convenience we have used W S(i,b),S(i,b) = (cid:101)ΣS(i,b),S(i,b) − ΣS(i,b),S(i,b). Recall that by
(cid:13)
Lemma G.1 i., with probability at least 1 − e−u,
(cid:13)
(cid:13) (cid:101)ΣSS − ΣSS
(cid:113) |S|+u

ΣS(i,b),S(i,b) + W S(i,b),S(i,b)

(cid:113) |S|+u
n

as long as

(cid:13)
(cid:13)
(cid:13)op

64e(1−α)(cid:107)Σ(cid:107)op
≤
α2
(cid:27)
(cid:113) |S(i,b)|+2 log 2np
n

. We will additionally

(cid:26)(cid:13)
(cid:13)
(cid:13)W S(i,b),S(i,b)

64e(1−α)(cid:107)Σ(cid:107)op
α2

2 . Let AO =

n ≤ 1

≤

=

(cid:13)
(cid:13)
(cid:13)op

need control on |S(i, b)|. Let event AB denote the event that the size of the Markov blankets behave

56

(cid:17)−1

,

“nicely”, i.e. AB =
implies P {AB} ≥ 1 − 1

(cid:110)

(cid:111)
CB(α,dmax) log 2np ∀i ∈ [n], b ∈ [p]
|S(i, b)| ≤
2np . Now, on the event AO ∩ AB, we have:

2

and notice that Lemma G.6

(cid:13)
(cid:13)
(cid:13)W S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)op

≤

(cid:16)

64e(1 − α) (cid:107)Σ(cid:107)op
α2

1 +

2
CB(α,dmax)

(cid:17)

(cid:114)

log 2np
n

.

Noticing that by assumption

series expansion of

(cid:16)

α2

(cid:17)−1

(cid:101)ΣS(i,b),S(i,b) − ΣS(i,b),S(i,b)

and we see thus that:

64e(1−α)(cid:107)Σ(cid:107)op

(cid:16)

1+

2
CB (α,dmax)

(cid:17)

(cid:113) log 2np

n < 1, so we can take the power

(cid:13)
(cid:13)Σ−1
(cid:13)

S(i,b),S(i,b)

− (cid:101)Σ

−1
S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)op

≤

(cid:13)
(cid:13)Σ−1
(cid:13)

S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)op

∞
(cid:88)

k=1

(cid:13)
(cid:13)Σ−1
(cid:13)
(cid:16)

1 +

S(i,b),S(i,b)

(cid:13)
k
(cid:13)
(cid:13)

op

(cid:13)
(cid:13)
(cid:13)W S(i,b),S(i,b)
(cid:17)

(cid:13)
k
(cid:13)
(cid:13)

op

128e(1 − α) (cid:107)Σ(cid:107)op

2
CB(α,dmax)

(cid:114)

≤

α2λmin(Σ)2

log 2np
n

,

α2λmin(Σ)
(cid:113) log p

64e(cid:107)Σ(cid:107)op
α2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:101)Σ − Σ
(cid:13)(cid:96)1→(cid:96)∞
(cid:113) log p
n ≤ 1, we upper

≤

where in the ﬁrst inequality we have used the power series expansion, the triangle inequality, and
the sub-multiplicativity of the operator norm and the second inequality used the assumption that
64e(1−α)(cid:107)Σ(cid:107)op

1+

(cid:17)

(cid:16)

(cid:113) log 2np

2
CB (α,dmax)

n < 1

2 . Now, let AM denote the event that

n . Then on the event AM ∩ AO ∩ AB and assuming that 64e
α2

bound the overall term by:

(cid:13)
(cid:13)
(cid:13) (cid:101)Σb,S(i,b)

(cid:16)

Σ−1

S(i,b),S(i,b)

− (cid:101)Σ

−1
S(i,b),S(i,b)

(cid:17)(cid:13)
(cid:13)
(cid:13)∞

≤

≤

(cid:113)

(cid:13)
(cid:13)Σ−1
(cid:13)
S(i,b)
(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:101)Σb,S(i,b)
(cid:13)∞
128e(1 − α) (cid:107)Σ(cid:107)2
op
α2λmin(Σ)2

1 +

−1
S(i,b),S(i,b)

− (cid:101)Σ
(cid:17)

(cid:13)
(cid:13)
(cid:13)op
(cid:114)

S(i,b),S(i,b)

2
CB(α,dmax)

(cid:113)

|S(i,b)|

log 2np
n

.

Similarly, we see that (working on the same event):

(cid:16)

(cid:13)
(cid:13)
(cid:13)

Σb,S(i,b) − (cid:101)Σb,S(i,b)

(cid:17)

Σ−1

S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)∞

≤

≤

(cid:13)
(cid:13)
(cid:13)Σb,S(i,b) − (cid:101)Σb,S(i,b)
64e (cid:107)Σ(cid:107)op
α2λmin(Σ)

log p
n

(cid:114)

(cid:13)
(cid:13)
(cid:13)∞
(cid:113)

S(i,b).

(cid:112)S(i, b)

(cid:13)
(cid:13)Σ−1
(cid:13)

S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)op

The result follows immediately by recalling the assumption that c(cid:96) ≤ λmin(Σ) ≤ (cid:107)Σ(cid:107)op ≤ cσ, noting
that P {AO} ≥ 1 − 1
p by
lemma G.1 ii.

2np by lemma G.1 i. and a union bound, and noting that P {AM } ≥ 1 − 1

Lemma G.9. Under the assumptions of theorem 6 and for any vector u ∈ Rp,

n
(cid:88)





p
(cid:88)

i=1

b=1

1
n

(cid:113)

S(i,b)

|ub|

(cid:88)

j∈S(i,b)


2

|Xij|



≤ C(α, dmax) (cid:107)u(cid:107)2
1 ,

with probability at least 1 − 1

n , where C(α, dmax) is a constant depending only on α, dmax.

57

Proof. This involves a simple application of Chebyshev’s inequality. First, we let Let





p
(cid:88)

(cid:113)

|ub|

S(i,b)

(cid:88)


2

|Xij|



.

Zi =

Chebyshev’s inequality then implies:

b=1

j∈S(i,b)

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

Zi − E Zi

(cid:41)

(cid:113)

E Z2
i

≥

≤

1
n

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Now, by lemma G.2 and the assumption that c(cid:96) ≤ λmin(Σ) ≤ (cid:107)Σ(cid:107)op ≤ cσ imply the result.

Lemma G.10. Under the assumptions of theorem 6,

max
a∈1,2,...,p





1
n

n
(cid:88)

i=1

(cid:113)

S(i,a)

(cid:88)

j∈S(i,a)


2

|Xij|



≤ C(α, dmax),

with probability at least 1 − c0p−c1, where c0, c1 are absolute constants.
(cid:16)(cid:112)S(i,a)

and using
Proof. We proceed by obtaining tail bounds on the quantity
this to ﬁnd an exponential tail bound on the empirical average from which point we can conclude
with a union bound. Notice that

j∈S(i,a)

|Xij|

(cid:80)

(cid:17)2

P






S(i,a)

(cid:88)

(cid:96),j∈S(i,a)×S(i,a)

|XijXi(cid:96)| ≥ t









p
(cid:88)

P

=

(cid:88)



(cid:96),j∈S(i,a)×S(i,a)

|XijXi(cid:96)| ≥






t
k

P (cid:8)S(i,a) = k(cid:9)

k=1

∞
(cid:88)

k=1

≤

(cid:40)

4exp

−

t

16e (cid:107)Σ(cid:107)op k3 − CB(α, dmax)k

(cid:41)

,

where the last inequality follows by lemma G.6 and noting that (cid:80)

(cid:96),j∈S(i,a)×S(i,a)

|XijXi(cid:96)| is a sub-

exponential random variable. Now, we balance terms in the inﬁnite sum around t

1
4 to see:

(cid:40)

4 exp

−

∞
(cid:88)

k=1

t

16e (cid:107)Σ(cid:107)op k3 − CB(α, dmax)k

(cid:41)

= 4e−at

∞
(cid:88)

1
4

(cid:40)

exp

at

1
4 −

t + 16e (cid:107)Σ(cid:107)op CB(α, dmax)k4
16e (cid:107)Σ(cid:107)op k3

(cid:41)

k=1
(cid:32)

(cid:88)

= 4e−at

1
4

(cid:40)

exp

at

1
4 −

t + 16e (cid:107)Σ(cid:107)op CB(α, dmax)k4
16e (cid:107)Σ(cid:107)op k3

(cid:41)

k≤t1/4
(cid:40)

exp

at

1
4 −

t + 16e (cid:107)Σ(cid:107)op CB(α, dmax)k4
16e (cid:107)Σ(cid:107)op k3

(cid:41)(cid:33)

(cid:88)

k>t1/4

+

≤

4
1 − e−CB(α,dmax)/2

1
4

e−at

where we have taken a such that a <

1
16e(cid:107)Σ(cid:107)op

∧ CB(α,dmax)
2

. Now, assuming

t <

128e
a − ae−CB(α,dmax)/2

a − ae−CB(α,dmax)/2
1024e2

(cid:33)3

(cid:32)

n

58

by Lemma I.3, we have:

P






1
n

n
(cid:88)





(cid:113)

S(i,a)


2

|Xij|



≥ 2


t + E

(cid:88)

j∈S(i,a)





(cid:113)

S(i,a)



|Xij|



(cid:88)

j∈S(i,a)

2








i=1





≤ 2exp

−a

(cid:32)
nt2 a − ae−CB(α,dmax)/2
1024e2

(cid:33) 1
7

+ log n

(cid:18)

4
1 − e−CB(α,dmax)/2




(cid:19)



Thus, let AT be the event that

max
a∈1,2,...,p





1
n

n
(cid:88)

i=1

(cid:113)

S(i,a)

(cid:88)

j∈S(i,a)



2



(cid:114)

|Xij|



≤ 2

A

(log c1np)7
n

+ E





(cid:113)

S(i,a)

(cid:88)



2

|Xij|





j∈S(i,a)

where c1 = 2 +

(cid:16) a−ae−CB (α,dmax)/2
1024e2

c2 = a
we have by Lemma G.2:

4

1−e−CB (α,dmax)/2 and note that P {AT } ≥ 1 − exp (cid:8)(cid:0)1 − c2A2/7(cid:1) log c1np(cid:9) where
(i,a),

. Thus, on the event AT , and assuming A

(cid:113) (log c1np)7
n

≤ (cid:107)Σ(cid:107)op

E S3

(cid:17)1/7

max
a∈1,2,...,p





1
n

n
(cid:88)

i=1

(cid:113)

S(i,a)

(cid:88)

j∈S(i,a)



2

|X ij|



≤ 4 (cid:107)Σ(cid:107)op

E S3

(i,a).

The result follows immediately taking A large enough and using lemma G.7.

G.4 Proof of lemma B.9

We re-state the lemma for convenience.

Lemma B.9. Under the assumptions of theorem 6, for any u ∈ Rp, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C(α, dmax) (cid:107)u(cid:107)1

(cid:41)

(cid:114)

log np
n

≤ n−1 + c0p−c1,

where c0, c1 denote universal constants and C(α, dmax) a constant depending only on α, dmax.

Proof. We write:

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

T (cid:16)

(cid:99)X

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= max

a=1,2,...,p

≤ max

a∈1,2,...,p

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:98)Xia

p
(cid:88)

(cid:16)

b=1

(cid:101)Xib − (cid:98)Xib

(cid:17)

ub

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12) (cid:98)Xia

(cid:12)
2
(cid:12)
(cid:12)

·

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

(cid:32) p

(cid:88)

(cid:16)

i=1

b=1

(cid:101)Xib − (cid:98)Xib

(cid:33)2

,

(cid:17)

ub

where the inequality is by Cauchy-Schwarz over the indices i. Our strategy is then to upper bound
each of the square root terms separately. The more diﬃcult term is the second, so we begin there,
ﬁrst by re-writing the approximate conditional expectation explicitly:
(cid:118)
(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:117)
(cid:116)
(cid:116)

n
(cid:88)

n
(cid:88)

(cid:32) p

(cid:32) p

(cid:88)

(cid:88)

(cid:33)2

=

(cid:17)

(cid:16)

(cid:16)

(cid:17)

(cid:101)Xib − (cid:98)Xib

ub

Σb,S(i,b)Σ−1

− (cid:101)Σb,S(i,b) (cid:101)Σ

−1
S(i,b),S(i,b)

S(i,b),S(i,b)

(cid:12)
(cid:12)
(cid:12)ub

X S(i,b)

(cid:33)2
(cid:12)
(cid:12)
(cid:12)

.

1
n

1
n

i=1

b=1

i=1

b=1

59

An application of H¨older’s inequality (noting that
row vector) yields the following upper bound:

(cid:16)

Σb,S(i,b)Σ−1

S(i,b),S(i,b)

− (cid:101)Σb,S(i,b) (cid:101)Σ

−1
S(i,b),S(i,b)

(cid:17)

is a

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)





p
(cid:88)

i=1

b=1

|ub|

(cid:13)
(cid:13)Σb,S(i,b)Σ−1
(cid:13)

S(i,b),S(i,b)

− (cid:101)Σb,S(i,b) (cid:101)Σ

−1
S(i,b),S(i,b)


2

|Xij|



.

(G.1)

(cid:13)
(cid:13)
(cid:13)∞

(cid:88)

j∈S(i,b)

The key diﬃculty is that each of the terms is correlated across indices i in a complicated manner,
so we are unable to immediately use standard concentration inequalities on the sum. To deal with
this, we will uniformly upper-bound each of these terms which correlate the terms in the sum. To
this end, we have:

(cid:13)
(cid:13)Σb,S(i,b)Σ−1
(cid:13)

S(i,b),S(i,b)

− (cid:101)Σb,S(i,b) (cid:101)Σ

−1
S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)∞

(cid:16)

(cid:13)
(cid:13)
(cid:13)

≤
(cid:13)
(cid:13)
(cid:13) (cid:101)Σb,S(i,b)

Σb,S(i,b) − (cid:101)Σb,S(i,b)

(cid:16)

Σ−1

S(i,b),S(i,b)

− (cid:101)Σ

(cid:17)

Σ−1

S(i,b),S(i,b)
−1
S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)∞
(cid:17)(cid:13)
(cid:13)
(cid:13)∞

+

.

Now, by lemma G.8, we have:

(cid:13)
(cid:13)Σb,S(i,b)Σ−1
(cid:13)

S(i,b),S(i,b)

− (cid:101)Σb,S(i,b) (cid:101)Σ

−1
S(i,b),S(i,b)

(cid:13)
(cid:13)
(cid:13)∞

≤ C(α, dmax)

(cid:113)

(cid:114)

|S(i,b)|

log np
n

,

with probability at least 1 − c

p . We thus upper bound (G.1) with:

(cid:114)

C(α, dmax)

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)





p
(cid:88)

i=1

b=1

log np
n

(cid:113)

S(i,b)

|ub|

(cid:88)

j∈S(i,b)


2

|Xij|



.

By lemma G.9, with probability at least 1 − 1
n ,

(cid:114)

C(α, dmax)

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

1
n

log np
n

n
(cid:88)





p
(cid:88)

(cid:113)

S(i,b)

|ub|

(cid:88)

i=1

b=1

j∈S(i,b)


2

|Xij|



≤ C(α, dmax) (cid:107)u(cid:107)1

(cid:114)

log np
n

.

Now, note that maxa∈[p]
argument as lemma F.1 iii. Combining these pieces implies the result.

i=1| (cid:98)Xia|2 ≤ 18e (cid:107)Σ(cid:107)op with probability at least 1 − c0p−c1 by the same

1
n

(cid:80)n

G.5 Proof of lemma B.10

We re-state the lemma for convenience:

Lemma B.11. Under the assumptions of theorem 6, we have:

(cid:16)

P

(cid:26)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C(α, dmax) (cid:107)u(cid:107)1

(cid:27)

log np
n

≤ n−1 + c0p−c1,

where c0, c1 denote universal constants and C(α, dmax) a constant depending only on α, dmax.

60

Proof. We write:

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:102)X − (cid:99)X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

= max

a=1,2,...,p

≤ max

a∈1,2,...,p

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:101)Xia − (cid:98)Xia

(cid:17)

p
(cid:88)

(cid:16)

(cid:101)Xib − (cid:98)Xib

n
(cid:88)

(cid:16)

i=1

(cid:17)

ub

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

(cid:16)

i=1

(cid:101)Xia − (cid:98)Xia

b=1

(cid:17)2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

·

1
n

n
(cid:88)

(cid:32) p

(cid:88)

(cid:16)

i=1

b=1

(cid:101)Xib − (cid:98)Xib

(cid:33)2

.

(cid:17)

ub

We tackle the ﬁrst term ﬁrst, noticing that with probability at least 1 − c0p−c1,

max
a∈1,2,...,p

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

(cid:16)

i=1

(cid:17)2

(cid:101)Xia − (cid:98)Xia

≤ max
a∈[p]

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1





(cid:13)
(cid:13)Σa,S(i,a)Σ−1
(cid:13)

S(i,a),S(i,a)

− (cid:101)Σa,S(i,a) (cid:101)Σ

−1
S(i,a),S(i,a)



2

|Xij|



(cid:13)
(cid:13)
(cid:13)∞

(cid:88)

j∈S(i,a)

(cid:114)

(cid:114)

≤ C(α, dmax)

≤ C(α, dmax)

log np
n

max
a∈[p]

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)





1
n

n
(cid:88)

i=1

(cid:113)

S(i,a)

(cid:88)

j∈S(i,a)



2

|Xij|



log np
n

,

where the second inequality follows by lemma G.8 and the last inequality follows by lemma G.10.
The second term we analyzed in lemma B.9, and thus with probability at least 1 − c0p−1 − 1
n ,

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

(cid:32) p

(cid:88)

(cid:16)

i=1

b=1

(cid:101)Xib − (cid:98)Xib

(cid:33)2

(cid:17)

ub

≤ C(α, dmax) (cid:107)u(cid:107)1

(cid:114)

log np
n

.

The result follows immediately.

G.6 Proof of lemma B.11

We re-state the lemma:

Lemma B.12. Under the assumptions of theorem 6, for any u ∈ Rp, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:99)X − X

(cid:17)

u

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≥ C(α, dmax) (cid:107)u(cid:107)1

(cid:41)

(cid:114)

log np
n

≤ n−1 + c0p−1,

where c0, c1 denote universal constants and C(α, dmax) a constant depending only on α, dmax.

Proof. We begin by writing

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T (cid:16)

(cid:99)X − X

(cid:17)

β0

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ max

a∈1,2,...,p

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

(cid:16)

i=1

(cid:101)Xia − (cid:98)Xia

(cid:17)2

· (cid:107)u(cid:107)2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(cid:68)
(cid:99)X i − X i, (cid:101)u

(cid:69)2

,

61

where (cid:101)u = u
(cid:107)u(cid:107)2
4 (cid:107)Σ(cid:107)op. Notice also that:

. Notice that by fact H.5, the vector (cid:99)X i − X i is sub-Gaussian with parameter

(cid:68)

E

(cid:99)X i − X i, (cid:101)u

(cid:69)2

(cid:88)

≤

|(cid:101)ua(cid:101)ub| E

(cid:12)
(cid:12)
(cid:12) (cid:98)Xia − Xia

(cid:12)
(cid:12)
(cid:12)

2

a,b
(cid:88)

≤ 4

a,b

|(cid:101)ua(cid:101)ub| E X 2

ia

≤ 4 (cid:107)Σ(cid:107)op (cid:107)(cid:101)u(cid:107)2
1 ,

where the ﬁrst inequality is by Cauchy Schwarz and the second by Jensen’s inequality. Applying
Lemma H.2 and lemma I.2, we obtain:

(cid:40)

P

1
n

n
(cid:88)

(cid:68)

i=1

(cid:69)2

(cid:99)X i − X i, (cid:101)u

(cid:68)

− E

(cid:69)2

(cid:99)X i − X i, (cid:101)u

(cid:41)

(cid:40)

≥ t

≤ exp

−c

(cid:41)

nt2
(cid:107)Σ(cid:107)2
op

for all t < C (cid:107)Σ(cid:107)op. Setting t = (cid:107)Σ(cid:107)op implies

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(cid:68)
(cid:99)X i − X i, (cid:101)u

(cid:69)2

≤ C (cid:107)u(cid:107)2
1 .

with probability at least 1 − e−cn. Note that by the analysis of lemma B.10, with probability at
least 1 − c0p−1 − n−1

max
a∈1,2,...,p

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

(cid:16)

i=1

(cid:17)2

(cid:101)Xia − (cid:98)Xia

(cid:114)

≤ C(α, dmax)

log np
n

.

The result follows immediately.

G.7 Proof of lemma B.14

We re-state the lemma for convenience.

Lemma B.14. Under the assumptions of theorem 6, we have:

P

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:16)

(cid:102)X − (cid:99)X

(cid:17)T

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

≥ C(α, dmax)σ

(cid:41)

(cid:114)

log np
n

≤ c0n−1 + c1p−c2,

where c0, c1 denote universal constants and C(α, dmax) a constant depending only on α, dmax.

Proof. We write:

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(cid:102)X − (cid:99)X

(cid:17)T

(cid:13)
(cid:13)
(cid:15)
(cid:13)
(cid:13)∞

= max

a=1,2,...,p

≤ max

a=1,2,...,p

(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
n
(cid:12)
(cid:118)
(cid:117)
(cid:117)
(cid:116)

62

n
(cid:88)

(cid:16)

i=1

(cid:101)Xia − (cid:98)Xia

(cid:17)

(cid:15)i

1
n

n
(cid:88)

(cid:16)

i=1

(cid:101)Xia − (cid:98)Xia

(cid:17)2

1
n

n
(cid:88)

i=1

(cid:15)2
i .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:118)
(cid:117)
(cid:117)
(cid:116)

Note that by the analysis of lemma B.10, with probability at least 1 − c0p−1 − n−1

max
a∈1,2,...,p

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

(cid:16)

i=1

(cid:17)2

(cid:101)Xia − (cid:98)Xia

(cid:114)

≤ C(α, dmax)

log np
n

.

Now deﬁne the event AE such that 1
n
we have P (cid:8)AC
E

(cid:9) ≤ exp {−Cn}. The result follows from these obvservations.

(cid:80)n

i=1 (cid:15)2

i ≤ 2σ2 and notice that by lemma H.2 and lemma I.2,

H Facts about sub-gaussian and sub-exponential random vari-

ables

This appendix contains a collection of facts about sub-gaussian random variables.

Lemma H.1. Assume X is σ2
1
Then, ∀θ ≤
8eσX σY

:

X sub-gaussian and Y is σ2

Y sub-gaussian and that XY is zero-mean.

E eθXY ≤ e32e2θ2σ2

X σ2

Y .

Proof. Taylor expanding eθXY gives:

E eθXY ≤ 1 +

∞
(cid:88)

k=2

θk E |XY |k
k!

≤ 1 +

θk(cid:112)E |X|2k E |Y |2k
k!

,

∞
(cid:88)

k=2

where the ﬁrst inequality follows since E XY = 0 and XY ≤ |XY | and the second inequality follows
by Cauchy-Schwarz. Now,

E |X|2k =

(cid:90) ∞

0

2kt2k−1 Pr {|X| ≥ t} dt ≤ 2k(2σ2

X )kΓ(k),

(H.1)

where Γ(k) = (cid:82) ∞
0
implies E |X|2k ≤ (4σ2

tk−1e−tdt. Stirling’s inequality gives Γ(k) ≤ kk and noting that 2k ≤ 2k for k ≥ 2

X )kkk. Proceeding similarly for E |Y |2k yields

1 +

∞
(cid:88)

k=2

θk(cid:112)E |X|2k E |Y |2k
k!

≤ 1 +

∞
(cid:88)

k=2

θk(4σX σY )kkk
k!

≤ 1 +

∞
(cid:88)

(4θeσX σY )k = 1 +

k=2

(4θeσX σY )2
1 − 4θeσX σY

,

where the third inequality used the fact that k! ≥ kk
geometric series. Finally, noting that θ ≤

1
8eσX σY

, we can upper bound

ek and the last equality invoked the sum of a

1 +

(4θeσX σY )2
1 − 4θeσX σY

≤ 1 + 32e2θ2σ2

X σ2

Y ≤ e32e2θ2σ2

X σ2

Y .

We will repeatedly make use of the following related lemma whose proof is nearly identical.

Lemma H.2. Assume X is σ2

X sub-gaussian. Then, ∀θ ≤ 1
16eσ2
X

:

E eθ(X 2−E X 2) ≤ e128e2θ2σ4
X .

63

Proof. As before, we begin by Taylor expanding eλXY gives:

E eλ(X 2−E X 2) ≤ 1 +

λk E (cid:12)

(cid:12)X 2 − E X 2(cid:12)
(cid:12)
k!

k

∞
(cid:88)

k=2

≤ 1 +

∞
(cid:88)

k=2

(2λ)k E |X|2k
k!

where the last inequality follows by writing X 2 − E X 2 = X 2 − EX1 X 2
1 where X1 is an i.i.d.
copy of X and using Jensen’s inequality. The remainder of the proof follows exactly as that of
Lemma H.1.

We will use another similar lemma, whose proof we omit as it can be easily derived using the ideas
of the previous two lemmas.

Lemma H.3. Assume X is σ2

X sub-gaussian and Y is σ2

Y sub-gaussian. Then, for all |θ| ≤

1
16eσX σY

:

E exp {θ (|XY | − E|XY |)} ≤ exp (cid:8)128e2θ2σ2

X σ2
Y

(cid:9) .

We collect a few simple facts about sub-gaussian random variables, which we state without proof.

Fact H.4. Assume X is σ2

X sub-Gaussian and Y is σ2

Fact H.5. Assume X is σ2
sub-gaussian.

X sub-gaussian and Y is σ2

Y sub-gaussian, then E XY ≤ σX σY .
Y sub-gaussian. Then X + Y is 2 (cid:0)σ2

X + σ2
Y

(cid:1)

I Concentration inequalities

In this appendix, we provide some useful concentration inequalities.

I.1 Sub-exponential concentration

We will often be interested in the sub-gaussian portion of the tail of sub-exponential random
variables. The following well-known lemma formalizes this concentration.

Lemma I.1. Assume Z1, Z2, . . . , Zn are i.i.d.
eθ2σ2
Z /2 for all |θ| ≤ θ. Then, for any t ≤ σ2
Zθ:
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

1
n

Zi

P

≥ t

i=1

zero-mean random variables such that E eθZi ≤

(cid:41)

(cid:26)

≤ 2 exp

−

(cid:27)

nt2
2σ2
Z

Proof. Note that P (cid:8)(cid:12)
(cid:12) 1
n
ﬁrst term:

(cid:80)n

i=1 Zi

(cid:12)
(cid:12) ≥ t(cid:9) = P (cid:8) 1

n

(cid:80)n

i=1 Zi ≥ t(cid:9) + P (cid:8) 1

n

(cid:80)n

i=1 Zi ≤ −t(cid:9). We analyze the

(cid:40)

P

1
n

n
(cid:88)

i=1

(cid:41)

Zi ≥ t

≤ e−θte

θ2σ2
Z
2n = e

− nt2
2σ2
Z

(I.1)

where the ﬁrst step has used Markov’s inequality and the second step is by taking θ = nt
σ2
Z

which is

by assumption ≤ θ. The second term follows by considering −Zi which have the same property of
the moment generating function, and we are done.

64

The following lemma (Bernstein’s inequality) is a generalization of this lemma, whose proof
follows by optimizing the RHS of the inequality in (I.1) over all θ ∈ R. The proof is straightforward
and we omit it here:

Lemma I.2. (Bernstein Inequality) Assume Z1, Z2, . . . , Zn are i.i.d. zero-mean random variables
such that E eθZi ≤ eθ2σ2

Z /2 for all |θ| ≤ θ. Then for any t ≥ 0:

P

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Zi

(cid:41)

(cid:26)

≥ t

≤ 2 exp

−

n
2

min

(cid:18) t2
σ2
Z

(cid:19)(cid:27)

, tθ

I.2 Tail Bounds for Sums of Random Variables

Lemma I.3. Consider iid non-negative random variables (Zi)i∈[n] and assume the tail bound:

P {Zi ≥ t} ≤ Ce−ctγ

where γ ∈ (0, 1) and C, c are constants. Then, assuming t < 8eC
γc

(cid:0)n γc
64e2C

(cid:1) 1−γ
γ :

(cid:40)

P

1
n

n
(cid:88)

i=1

(cid:41)

Zi ≥ 2(t + E Z1)

≤ 2exp

(cid:110)

−C

(cid:16)(cid:0)nt2(cid:1) γ

2−γ + log n

(cid:17)(cid:111)

where C is a universal constant.

Proof. We proceed by a truncation argument. Let Z↓
We then see that:
(cid:40)

(cid:40)

(cid:41)

P

n
(cid:88)

1
n

Zi ≥ 2(t + E Z1)

≤ P

i=1

(cid:124)

(cid:41)

(cid:40)

n
(cid:88)

1
n

Z↓
i ≥ t + E Z1

+ P

i=1

(cid:123)(cid:122)
A.

(cid:125)

1
n

n
(cid:88)

i=1

Z↑
i ≥ t + E Z1

(cid:123)(cid:122)
B.

(cid:41)

(cid:125)

i := Zi1 {Zi ≤ M } and Z↑

i := Zi1 {Zi ≥ M }.

We examine term B. ﬁrst. Notice that P
tionally, we have:

(cid:110) 1
n

(cid:80)n

i=1 Z↑

i ≥ t + E Z1

≤ n P

(cid:110)

Z↑
1 ≥ t + E Z1

(cid:111)

. Addi-

(cid:124)

(cid:111)

P

(cid:110)

Z↑
1 ≥ t + E Z1

(cid:111)

(cid:110)

= P
(cid:110)

Z↑
1 ≥ t + E Z1 | Z1 < M
(cid:111)

Z↑
1 ≥ t + E Z1 | Z1 ≥ M

P

(cid:111)

P {Z1 < M } +

P {Z1 ≥ M }

≤ P {Z1 ≥ M }

Thus:

(cid:40)

P

1
n

n
(cid:88)

i=1

(cid:41)

Zi ≥ 2(t + E Z1)

≤ P

(cid:40)

1
n
(cid:40)

≤ 2 P

n
(cid:88)

(cid:41)

Z↓
i ≥ t + E Z1

+ n P {Z1 ≥ M }

Z↓
i ≥ t + E Z1

(cid:41)

(cid:41)

∨ n P {Z1 ≥ M }

i − E Z↓
Z↓

i ≥ t

∨ n P {Z1 ≥ M }

≤ 2 P

(cid:40)

1
n

i=1
n
(cid:88)

1
n

i=1
n
(cid:88)

i=1

65

where the last inequality follows because E Z1 ≥ E Z↓
generating function that exists, so we aim to compute P

1 . The truncated portion now has a moment
i − E Z↓
by bounding

i=1 Z↓

i ≥ t

(cid:110) 1
n

(cid:80)n

(cid:111)

(cid:26)

(cid:16)

λ
e

E

1 −E Z↓
Z↓

1

(cid:17)(cid:27)

. We have:

(cid:26)
e

E

(cid:16)
1 −E Z↓
Z↓

1

λ

(cid:17)(cid:27)

=

(cid:16)

λ(cid:96) E

(cid:17)(cid:96)

1 − E Z↓
Z↓
(cid:96)!

1

∞
(cid:88)

(cid:96)=0

≤ 1 +

∞
(cid:88)

(cid:96)=2

λ(cid:96)2(cid:96) E

(cid:12)
(cid:12)Z↓
(cid:12)
(cid:96)!

1

(cid:12)
(cid:96)
(cid:12)
(cid:12)

where the inequality is by the same trick used in the proof of Lemma H.2. Now, notice that

E

(cid:12)
(cid:12)Z↓
(cid:12)

1

(cid:12)
(cid:96)
(cid:12)
(cid:12)

=

=

≤

dy

(cid:111)

(cid:12)
(cid:12)
(cid:12) ≥ y
(cid:110)(cid:12)
(cid:12)Z↓
(cid:12)

1

(cid:111)

(cid:12)
(cid:12)
(cid:12) ≥ y

dy

(cid:96)y(cid:96)−1 P

(cid:110)(cid:12)
(cid:12)Z↓
(cid:12)

1

(cid:90) ∞

0
(cid:90) M

0

(cid:96)y(cid:96)−(cid:96)0y(cid:96)0−1 P
(cid:96)M (cid:96)−(cid:96)0Γ (cid:0)(cid:96)0γ−1(cid:1)
γc(cid:96)0γ−1

Here, we have chosen (cid:96)0 arbitrarily, and we set it such that (cid:96)0γ−1 = (cid:96). Thus, we have:

λ(cid:96)2(cid:96) E

(cid:12)
(cid:12)Z↓
(cid:12)
(cid:96)!

1

1 +

∞
(cid:88)

(cid:96)=2

(cid:96)

(cid:12)
(cid:12)
(cid:12)

≤ 1 +

∞
(cid:88)

(cid:96)=2

(2λ)(cid:96)
(cid:96)!

(cid:96)M (cid:96)−(cid:96)0Γ (cid:0)(cid:96)0γ−1(cid:1)
γc(cid:96)0γ−1

≤ 1 +

C
γ

∞
(cid:88)

(cid:96)=2

(cid:19)(cid:96)

(cid:18) 4eλM 1−γ
c

Now, under the assumption λ < cM 1−γ

8e

, we get the upper bound:

Thus, we can see that under the assumption t < 8eC

(cid:26)

λ
e

E

(cid:16)
Z↓
1 −E Z↓

1

(cid:17)(cid:27)

≤ exp

(cid:27)

(cid:26) 32e2C

γc2 M 2−2γλ2
γc M 1−γ, we have:

(cid:40)

P

1
n

n
(cid:88)

i=1

Z↓
i ≥ t + E Z1

(cid:41)

(cid:26)

≤ 2exp

−

nt2γc2
64e2CM 2−2γ

(cid:27)

Putting these pieces together, we see that under the same assumption on t:

(cid:40)

P

1
n

n
(cid:88)

i=1

Zi ≥ 2(t + E Z1)

(cid:41)

(cid:26)

≤ 2exp

−

nt2γc2
64e2CM 2−2γ

(cid:27)

∨ exp {−cM γ + log Cn}

Balancing terms, we set the truncation M = (cid:0)nt2
have:

γc
64e2C

(cid:1) 1
2−γ . Thus, if t < 8eC
γc

(cid:0)n γc
64e2C

(cid:1)1−γ γ, we

(cid:40)

P

1
n

n
(cid:88)

i=1

Zi ≥ 2(t + E Z1)

(cid:41)

(cid:26)

≤ 2exp

−c

(cid:16)

nt2 γc
64e2C

66

(cid:17) γ

2−γ + log Cn

(cid:27)

