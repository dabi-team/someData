MANUSCRIPT

1

Neural source-ﬁlter waveform models for statistical
parametric speech synthesis

Xin Wang, Member, IEEE, Shinji Takaki, Member, IEEE, Junichi Yamagishi, Senior Member, IEEE

9
1
0
2

v
o
N
7
1

]
S
A
.
s
s
e
e
[

2
v
8
8
0
2
1
.
4
0
9
1
:
v
i
X
r
a

Abstract—Neural waveform models have demonstrated better
performance than conventional vocoders for statistical paramet-
ric speech synthesis. One of the best models, called WaveNet,
uses an autoregressive (AR) approach to model the distribution
of waveform sampling points, but it has to generate a waveform in
a time-consuming sequential manner. Some new models that use
inverse-autoregressive ﬂow (IAF) can generate a whole waveform
in a one-shot manner but require either a larger amount of
training time or a complicated model architecture plus a blend
of training criteria.

As an alternative to AR and IAF-based frameworks, we pro-
pose a neural source-ﬁlter (NSF) waveform modeling framework
that is straightforward to train and fast to generate waveforms.
This framework requires three components to generate wave-
forms: a source module that generates a sine-based signal as
excitation, a non-AR dilated-convolution-based ﬁlter module that
transforms the excitation into a waveform, and a conditional
module that pre-processes the input acoustic features for the
source and ﬁlter modules. This framework minimizes spectral-
amplitude distances for model training, which can be efﬁciently
implemented using short-time Fourier transform routines. As
an initial NSF study, we designed three NSF models under the
proposed framework and compared them with WaveNet using
our deep learning toolkit. It was demonstrated that the NSF
models generated waveforms at least 100 times faster than our
WaveNet-vocoder, and the quality of the synthetic speech from
the best NSF model was comparable to that from WaveNet on a
single-speaker Japanese speech corpus.

Index Terms—speech synthesis, neural network, waveform

model, short-time Fourier transform

I. INTRODUCTION

Text-to-speech synthesis (TTS), a technology that converts
a text string into a speech waveform [1], has rapidly advanced
with the help of deep learning. In TTS systems based on a
statistical parametric speech synthesis (SPSS) framework [2],
[3], deep neural networks have been used to derive linguistic
features from text [4]. More breakthroughs have been reported
for the acoustic modeling part, for which various types of
neural networks have been proposed that convert linguistic
features into acoustic features such as the fundamental fre-

Xin Wang is with National Institute of Informatics, Tokyo, 101-8340, Japan.

e-mail:wangxin@nii.ac.jp

Shinji Takaki is with the Nagoya Institute of Technology, Japan. e-mail:

takaki@sp.nitech.ac.jp

Junichi Yamagishi is with the National Institute of Informatics, Tokyo, 101-
8340, Japan and also with Centre for Speech Technology Research, University
of Edinburgh, UK. e-mail: jyamagis@nii.ac.jp.

The guest editor coordinating the review of this manuscript was Dr.
Heiga Zen. This work was partially supported by JST CREST Grant
Number JPMJCR18A6, Japan and by MEXT KAKENHI Grant Num-
bers (19K24371,16H06302, 16K16096, 17H04687, 18H04120, 18H04112,
18KT0051), Japan.

quency (F0) [5], [6], cepstral or spectral features [7], [8], [9],
and duration [10], [11].

Deep learning has recently been used to design a vocoder,
a component that converts acoustic features into a speech
waveform. The pioneering model called WaveNet [12] directly
generates a waveform in a sample-by-sample manner given the
input features. It has shown better performance than classical
signal-processing-based vocoders for speaker-dependent SPSS
systems [13]. Similar AR models such as SampleRNN [14]
have also performed reasonably well for SPSS [15]. The
key idea of these neural waveform models is to implement
an autoregressive (AR) probabilistic model that describes the
distribution of a current waveform sample conditioned on
previous samples. Although AR models can generate high-
quality waveforms, their generation speed is slow because they
have to generate waveform samples one by one.

Inverse-AR is another approach to neural waveform mod-
eling. For example, inverse-AR-ﬂow (IAF) [16] can be used
to transform a noise sequence into a waveform without the
sequential generation process. However, each waveform must
be sequentially transformed into a noise-like signal during
model training [17], which signiﬁcantly increases the amount
of training time. Rather than the direct
training method,
Parallel WaveNet [18] and ClariNet [19] use a pre-trained AR
model as a teacher to evaluate the waveforms generated by
an IAF student model. Although this training method avoids
sequential transformation, it requires a pre-trained WaveNet
as the teacher model. Furthermore, additional training criteria
have to be incorporated, without which the student model
only produces mumbling sounds [18], [19]. The blend of
disparate training criteria and the complicated knowledge-
distilling approach make IAF-based frameworks even less
accessible to the TTS community.

In this paper, we propose a neural source-ﬁlter (NSF)
waveform modeling framework, which is straightforward to
implement, fast in generation, and effective in producing high-
quality speech waveforms for TTS. NSF models designed
under this framework have three components: a source module
that produces a sine-based excitation signal, a ﬁlter module
that uses a dilated-convolution-based network to convert the
excitation into a waveform, and a conditional module that pro-
cesses the acoustic features for the source and ﬁlter modules.
The NSF models do not rely on AR or IAF approaches and are
directly trained by minimizing the spectral amplitude distances
between the generated and natural waveforms.

We describe the three speciﬁc NSF models designed under
our proposed framework: a baseline NSF (b-NSF) model that
adopts a WaveNet-like ﬁlter module, simpliﬁed NSF (s-NSF)

 
 
 
 
 
 
MANUSCRIPT

2

model that simpliﬁes the ﬁlter module of the baseline NSF
model, and harmonic-plus-noise NSF (hn-NSF) model that
uses separate source-ﬁlter pairs for the harmonic and noise
components of waveforms. While we previously introduced
the b-NSF model [20], the s-NSF and hn-NSF models are
newly introduced in this paper. Among the three NSF models,
the hn-NSF model outperformed the other two in a large-scale
listening test. Compared with WaveNet, the hn-NSF model
generated speech waveforms with comparably good quality
but much faster.

We review recent neural waveform models in Section II and
describe the proposed NSF framework and three NSF models
in Section III. After explaining the experiments in Section IV,
we draw a conclusion in Section V.

II. REVIEW OF RECENT NEURAL WAVEFORM MODELS

We consider neural waveform modeling as the task of
mapping input acoustic features into an output waveform.
Suppose one utterance has B frames and its acoustic features
are encoded as a sequence c1:B =
, where
RD is the feature vector with D dimensions for the
cb ∈
b-th frame. Without loss of generality, we deﬁne the target
R, and T is the
waveform as o1:T =
, where ot ∈
, oT }
waveform length1. We then use
o1:T to denote the waveform
generated by a neural waveform model.

, cB}

o1,
{

c1,

· · ·

· · ·

{

(cid:98)
A. Naive neural waveform model

Let us ﬁrst consider a naive neural waveform model. After
upsampling c1:B to ˜c1:T by replicating each cb multiple times,
we may use a recurrent or convolution network to learn the
mapping from ˜c1:T to o1:T . Such a model can be trained by
minimizing the mean square error (MSE):

Θ∗ = arg min
Θ

1
T

T

t=1
(cid:88)

(ot −

ot)2,

(1)

,

}

(cid:98)

(cid:98)

· · ·

∈ {

t
∀

· · ·
(cid:98)

ot =

by setting

o1: ˜T =
, ˜T

where
FΘ(c1:T , t) is the output of the neural network
at the t-th time step for the training utterance, and Θ is the
network’s parameter set. During generation, the model can
generate a waveform
o ˜T }
o1,
ot =
{
given the input c1: ˜T .
1,
FΘ∗ (c1: ˜T , t),
However, such a naive model did not produce intelligible
(cid:98)
(cid:98)
waveforms in our pilot test. We found that the generated wave-
form was over-smoothed and lacked the variations that evoke
the perception of speech formant. This over-smoothing effect
may be reasonable because the way we train the naive model
is equivalent to maximizing the likelihood of a waveform o1:T
over the distribution
p(o1:T |
=

c1:T ; Θ)
T

c1:T ; Θ) =

(ot;

(2)

(cid:98)

T

FΘ(c1:T , t), σ2),

p(ot|

N

t=1
(cid:89)

, σ2) is a Gaussian distribution with an unknown
where
·
standard deviation σ. It is assumed that the waveform values
are independently distributed with the naive model, which may

N

t=1
(cid:89)
;
(
·

1Some waveform models require discretized waveform values, i.e., ot ∈ N.

be incompatible with the strong temporal correlation of natural
waveforms. This mismatch between model assumption and
natural data distribution may cause the over-smoothing effect.

B. Neural AR waveform models

AR neural waveform models have recently been proposed
for better waveform modeling [12]. Contrary to the naive
model assumption in Equation (2), it is assumed that

T

p(o1:T |

c1:B; Θ) =

p(ot|

o<t, c1:B; Θ)

(3)

t=1
(cid:89)
o<t, c1:B; Θ) depends on the
with an AR model, where p(ot|
previous waveform values o<t. Such a model can potentially
temporal correlation among waveform
describe the causal
samples.

An AR model can be implemented by feeding the waveform
sample of the previous time step as the input to a convolution
network (CNN) or recurrent network (RNN) at the current
step, which is illustrated in Figure 1. The model is trained by
maximizing the likelihood over natural data while using the
data for feedback, i.e., teacher forcing [21]. In the generation
stage, the model has to sequentially generate a waveform and
use the previously generated sample as the feedback datum.

Pioneering neural AR waveform models include WaveNet
[12] and SampleRNN [14]. There are also models that com-
bine WaveNet with classical speech-modeling methods such
as glottal waveform modeling [22] and linear-prediction cod-
ing [23], [24]. Although these models can generate high-
quality waveforms, their sequential generation process is time-
consuming. More speciﬁcally, these AR models’ time com-
plexity to generate a single waveform of length T is theo-
retically equal to
(T ). Note that the time complexity only
takes the waveform length into account, ignoring the number
of hidden layers and implementation tricks such as subscale
dependency [25] and squeezing [17]. Other models, such as
WaveRNN [25], FFTNet [26], and subband WaveNet [27],
use different network architectures to reduce or parallelize the
computation load, but their generation time is still linearly
proportional to the waveform length.

O

Note that in the teacher-forcing-based training stage, the
CNN-based (e.g., WaveNet) and RNN-based AR models (e.g.,
WaveRNN) have time complexity of
(T ), re-
spectively. The time complexity of the RNN-based models
is limited by the computation in the recurrent layers. These
theoretical interpretations are summarized in Table I.

(1) and

O

O

C. IAF-ﬂow-based models

Rather than the AR-generation process, an IAF-ﬂow-based
model such as WaveGlow [17] uses an invertible and non-AR
) to convert a random Gaussian noise signal
function H −
z1:T , c1:B). In the training
z1:T into a waveform
stage, the model likelihood has to be evaluated as

o1:T = H −

1
Θ (
·

1
Θ (

(cid:98)

(cid:98)
c1:B; Θ)

pO(o1:T |
=pZ(z1:T = HΘ(o1:T , c1:B))

(cid:98)

det

∂HΘ(o1:T , c1:B)
∂o1:T

(cid:12)
(cid:12)
(cid:12)

(4)

(cid:12)
(cid:12)
(cid:12)

MANUSCRIPT

3

TABLE I
THEORETICAL TIME COMPLEXITY TO PROCESS WAVEFORM OF LENGTH T

Time complexity
train
Model
O(1)
AR (CNN-based, e.g., WaveNet)
AR (RNN-based, e.g., WaveRNN) O(T )
O(T )
IAF ﬂow (e.g., WaveGlow)
O(1)
AR+IAF (e.g., Parallel WaveNet)
O(1)
NSF

generation
O(T )
O(T )
O(1)
O(1)
O(1)

Knowledge
distilling
×
×
×
(cid:88)
×

modules: a condition module that processes input acoustic
features, source module that produces a sine-based excitation
signal given the F0, and neural ﬁlter module that converts the
excitation into a waveform using dilated convolution (CONV)
and feedforward transformation. Rather than the MSE or
the likelihood over waveform sampling points, the proposed
framework uses spectral-domain distances for model training.
Because our proposed framework explicitly uses a source-ﬁlter
structure, we refer to all of the models based on the framework
as NSF models.

Our proposed NSF framework does not rely on AR or IAF
approaches. An NSF model converts an excitation signal into
an output waveform without sequential transformation. It can
be simply trained using the stochastic gradient descent (SGD)
method under a spectral domain criterion. Therefore, its time
complexity is theoretically irrelevant to the waveform length,
(1) in both the model training and waveform generation
i.e.,
stages. Neither does an NSF model use knowledge distilling,
which makes it straightforward in training and implementation.
An NSF model can be implemented in various architectures.

O

We gives details on the three speciﬁc NSF models:

1) The b-NSF model has a network structure partially

similar to ClariNet and Parallel WaveNet;

2) The s-NSF model inherits b-NSF’s structure but uses

much simpler neural ﬁlter modules;

3) The hn-NSF model extends the s-NSF and explicitly
generates the harmonic and noise components of a
waveform.

Although the three NSF models use different neural ﬁlter
modules, they use the same spectral-domain training criterion.
We therefore ﬁrst explain the training criterion in Section III-A
then the three NSF models in Sections III-B to III-D.

A. Training criterion based on spectral amplitude distances

R

≥

(cid:98)

(cid:98)

∈

L

∂
L∂ (cid:98)o1:T

o1:T , o1:T )
(

A good metric

0 should measure the
distance between the perceptual quality of a generated wave-
o1:T and that of a natural waveform o1:T . Additionally, a
form
based on the metric should be easily calculated
gradient
so that a model can be trained using the SGD method. For the
NSF models, we use spectral amplitude distances calculated
on the basis of short-time Fourier transform (STFT). Although
spectral amplitude distance has been used in classical speech
coding methods [30], [31], [32], we further compute multiple
distances with different short-time analysis conﬁgurations, as
illustrated in Figure 2.

Fig. 1. Three types of neural waveform models in training stage. (cid:98)o1:T and
o1:T denote generated and natural waveforms, respectively. c1:B denotes
input acoustic features. Red arrows denote gradients for back propagation.

1
Θ (
·

where HΘ(o1:T , c1:B) sequentially inverts a training wave-
form o1:T into a noise signal z1:T for likelihood evaluation.
c1:B; Θ) can
When H −
) is sufﬁciently complex, pO(o1:T |
approximate the true distribution of o1:T [28].
In terms of the theoretical time complexity w.r.t T , the
ﬂow-based models are dual to the CNN-based AR models,
as Table I summaries. Although the time complexity of the
ﬂow-based models in waveform generation is irrelevant to T ,
(T ). Some models
their time complexity in model training is
such as WaveGlow reduce T by squeezing multiple waveform
samples into one vector [17] but still require a huge amount of
training time. For example, in one study using Japanese speech
data [29], a WaveGlow model was trained on four V100 GPU
cards for one month to produce high-quality waveforms2.

O

D. AR plus IAF

Some models, such as Parallel WaveNet [18] and Clar-
iNet [19], combine the advantages of AR and IAF-ﬂow-
based models under a distilling framework. In this frame-
work, a ﬂow-based student model generates waveform samples
1
z1:T , c1:B) and queries an AR teacher model
Θ (
o1:T = H −
o1:T . The student model learns by minimizing the
to evaluate
o1:T ) and that given by the teacher model.
distance between p(
(cid:98)
Therefore, neither training nor generation requires sequential
transformation.

(cid:98)

(cid:98)

However, it was reported that knowledge distilling is insufﬁ-
cient and additional spectral-domain criteria must be used [18],
[19]. Among the blend of disparate loss functions, it remains
unclear which one is essential. Furthermore, knowledge distill-
ing with two large models is complicated in implementation.

(cid:98)

III. NEURAL SOURCE-FILTER MODELS

We propose a framework of neural waveform modeling that
is fast in generation and straightforward in implementation. As
Figure 1 illustrates, our proposed framework contains three

2The original WaveGlow paper used eight GV100 GPU cards for model

training [17]. However, it did not report the amount of training time.

RNN/CNNotCondition moduleLikelihood Delayc1:BInverse autoregressive flowCondition moduleLikelihood c1:Bo1:TN(z;0,I)Neural filter moduleCondition modulec1:BSourceF0o1:TSpectral distancebo1:TNeural autoregressive modelot 1Flow-based modelNeural source-filter modelz1:T=H⇥(o1:T,c1:B)<latexit sha1_base64="25xgvimD2W+g06h1zOWtEYKEpW4=">AAACR3icbVDLahsxFNW4j7hO0zrtshtRU3AgmJFbaDEUTLvJMgU7CXiGQaO5jkX0GCRNijPoX/o1gazabb+iu9BlNc4s2rgXhM7jXnR18lJw6+L4Z9R58PDR453uk97u071nz/v7L06srgyDOdNCm7OcWhBcwdxxJ+CsNEBlLuA0v/jc+KeXYCzXaubWJaSSniu+5Iy6IGX9SZLb+spnNZnMPP6Ij7K6UZLZChz1ftgQ3dqHuGFswz75g6w/iEfxpvA2IC0YoLaOs/5tUmhWSVCOCWrtgsSlS2tqHGcCfC+pLJSUXdBzWASoqASb1ps/evwmKAVeahOOcnij/j1RU2ntWuahU1K3sve9Rvyft6jc8kNac1VWDhS7e2hZCew0bgLDBTfAnFgHQJnhYVfMVtRQ5kKsvUTBV6alpKpogvMLkoZbi6LZRYt6QLwPQZH7sWyDk/GIvB2Nv7wbTEkbWRe9Qq/REBH0Hk3RETpGc8TQN3SNvqMf0U30K7qNft+1dqJ25iX6pzrRH0J5slQ=</latexit>MANUSCRIPT

4

x(n)
1 ,

1) Evaluation: Given the natural and generated waveforms,
we follow the STFT convention and conduct framing and win-
dowing on the waveforms, after which the spectrum of each
frame is computed using the discrete Fourier transform (DFT).
x(n)
RM to denote the n-th
x(n) = [
Let us use
M ](cid:62)
frame of the generated waveform
o1:T , where M is the frame
x(n)
length, and
m is the value of a windowed waveform sample.
(cid:98)
y(n)
CK to denote the
y(n) = [
We then use
K ](cid:62)
(cid:98)
· · ·
x(n) calculated using K-point DFT. We similarly
spectrum of
(cid:98)
deﬁne x(n) and y(n) for the natural waveform o1:T . Suppose
o1:T and o1:T have been sliced into N frames. A log spectral
can be calculated as
amplitude distance

y(n)
1

· · ·

∈

∈

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

,

,

,

(cid:98)

=

L

1
2N K

log

k )2 + Im(y(n)
Re(y(n)
k )2 + η
y(n)
y(n)
k )2 + Im(
Re(
k )2 + η

2

, (5)

(cid:105)

n=1
(cid:88)k=1 (cid:104)
(cid:88)
where Re(
) and Im(
·
·
(cid:98)
a complex number, respectively, and η = 1e
−
number to ensure numerical stability.

) denote the real and imaginary parts of
5 is a constant

(cid:98)

to calculate the gradient vector
back to the neural ﬁlter module that produces

2) Backward propagation: For model training, we need
RT and propagate it
o1:T . Although
is calculated given complex-valued spectra, it turns out that
L
∂
L∂ (cid:98)o1:T

can be computed efﬁciently.

∂
L∂ (cid:98)o1:T ∈

(cid:98)

L

N

K

First, because

spectrum of

x(n) = [
(cid:98)

y(n) = [
x(n)
1 ,

y(n)
1
,
· · ·
(cid:98)
x(n)
(cid:98)
m cos

∈
(cid:98)

2π
K

(k

,
,
· · ·
x(n)
M ](cid:62)

y(n)
K ](cid:62)
RM , we know that

CK is the

∈

1)(m

1)

,

−

−

(6)

(cid:0)
x(n)
m sin

2π
K

(k

−

1)(m

−

(cid:1)
1)

,

(7)

Re(

y(n)
(cid:98)
k ) =

Im(

(cid:98)
y(n)
k ) =

M
(cid:98)
m=1
(cid:88)
M
(cid:98)

−

m=1
(cid:88)

1,

(cid:98)
, and

where k
· · ·
∂
∂
L
L
∂Im((cid:98)y(n)
∂Re((cid:98)y(n)
k )
k )
compute the gradient by using the chain rule:
(cid:98)

y(n)
(cid:98)
3. Because Re(
k ),
, K
}
are real-valued numbers, we can

∈ {

(cid:1)
y(n)
k ), Im(

(cid:0)

K

(cid:88)k=1 (cid:104)
K

∂
L
x(n)
m

∂

(cid:98)

=

=

∂
∂Re(

L
y(n)
k )

∂
∂Re(

L
(cid:98)
y(n)
k )

∂Re(

∂

y(n)
k )
x(n)
m
(cid:98)
2π
(k
(cid:98)
K

cos(

+

∂
∂Im(

L
y(n)
k )

∂Im(

(cid:98)
y(n)
k )
x(n)
m
(cid:98)

∂

(cid:105)

1)(m

1))
(cid:98)

−

−

(cid:98)

−

(cid:88)k=1
K

(cid:88)k=1

∂
∂Im(

L
(cid:98)
y(n)
k )

sin(

2π
K

(k

−

1)(m

1)).

−

(cid:98)

1,

for t

L∂(cid:98)ot
L
∂(cid:98)x(n)
m

∂
As long as we can compute
for each m and n, the
L
∂(cid:98)x(n)
m
gradient ∂
can be easily accumulated
}
x(n)
from ∂
given the relationship between ot and each
m
that is determined by the framing and windowing operations.
∂
then is sent to the output layer of the neural ﬁlter module
L∂(cid:98)ot
for back-propagation and SGD training.

∈ {

· · ·

, T

(cid:98)

m=M +1 0 cos( 2π
K

3The summation in Equations (6)-(7) should be (cid:80)K

m=1, but the zero-
padded part (cid:80)K
(k − 1)(m − 1)) can be safely ignored.
Although we can avoid zero-padding by setting K = M , in practice, K is
usually the power of 2 to take advantage of the fast Fourier transform (FFT).
A waveform frame of length M can be zero-padded to length K = 2(cid:100)log2 M (cid:101)
or longer to increase the frequency resolution.

Fig. 2. Illustration of calculating three spectral distances for forward (black)
and backward (red) propagation. DFT denotes discrete Fourier transform.
Vectors (cid:98)x(n), (cid:98)y(n), and (cid:98)g(n) denote windowed waveform, spectrum, and
composed gradient vector for n-th frame.

Although we can use matrix multiplication to implement
Equation (8) and compute
](cid:62) for
each frame n, a more efﬁcient method is to use the inverse-
CK can be
DFT. Suppose a complex-valued signal g(n)
∈
, g(n)
composed for the n-th frame as g(n) = [g(n)
K ], where
,
· · ·

L∂ (cid:98)x(n) = [ ∂

, ∂
L
∂(cid:98)x(n)
M

L
∂(cid:98)x(n)
1

· · ·

∂

1

,

g(n)
k =

∂
∂Re(

L
y(n)
k )

+ j

∂
∂Im(

L
y(n)
k ) ∈

C.

If g(n) is conjugate symmetric, i.e.,

(cid:98)

(cid:98)

Re(g(n)

k ) = Re(g(n)

Im(g(n)

k ) =

−
0,

(cid:40)

[2,

k

∈

K
2

]

k)),
(K+2
−
Im(g(n)

(K+2

k)),
−

k

∈
k =

[2, K
2 ]
1, K
2 + 1
{

,

}

(9)

(10)

(11)

the inverse-DFT of g(n) will be a real-valued signal b(n) =
[b(n)
1 ,

RK, where

, b(n)
K ]

k ej 2π
g(n)

K (k

−

1)(m

−

1)

· · ·

∈
K

b(n)
m =

(cid:88)k=1
K

(cid:88)k=1
K

(cid:88)k=1

(8)

=

Re(gk) cos(

2π
K

(k

−

1)(m

1))

−

−

(12)

Im(gk) sin(

2π
K

(k

−

1)(m

1)).

−

∂
L
∂(cid:98)x(n)
m

By combining Equations (9) and (12), we can see that b(n)
m
in Equation (8). In other words, if g(n) is
is equal to
L∂ (cid:98)x(n) by constructing
conjugate symmetric, we can calculate
g(n) and taking its inverse-DFT. It can be shown that g(n)
is conjugate symmetric when
is deﬁned as the log spectral
L
amplitude distance in Equation (5). Other common distances
such as the Kullback-Leibler divergence (KLD) between spec-
tra [33], [34] are also applicable.

∂

DFTFramingconfig1DFTFramingconfig1iDFTDe-framingconfig1L1DFTFramingconfig2DFTFramingconfig2iDFTDe-framingconfig2+L2bx(n)by(n)g(n)DFTFramingconfig3DFTFramingconfig3iDFTDe-framingconfig3L3@L1@bx(n)<latexit sha1_base64="ew/U2ZaMbjybWL0lzZSm4hSXlo0=">AAACSXicdZBNa9wwEIblTT/S7Uc27bEX0aWQXozlZLMp9BDoJYceUugmgbW7jGU5KyJLRpKbLEJ/pr8mkFN7y8/opZSeKm+29IN2QOjlnRlm5ikawY1Nkuuot3br9p276/f69x88fLQx2Hx8ZFSrKZtQJZQ+KcAwwSWbWG4FO2k0g7oQ7Lg4e93ljz8wbbiS7+yiYXkNp5JXnIIN1mzwKqs0UJc1oC0HkdVg5xSEe+NnxP+yz3nJ5mBdVhh34f17tyVfeD8bDJN4nI7T0R5O4uTlbjpKgyC7I7K9g0mcLGOIVnE4G3zNSkXbmklLBRgzJUljc9cNoYL5ftYa1gA9g1M2DVJCzUzulld6/Dw4Ja6UDk9avHR/73BQG7Ooi1DZXWH+znXmv3LT1lZ7ueOyaS2T9GZQ1QpsFe6Q4ZJrRq1YBAFU87ArpnMI2GwA288kO6eqrkGWHR4/JXn4lSi7XZRwQ7IE9ZMG/r84SmOyHadvd4b7ZIVsHT1Fz9AWImiM9tEBOkQTRNFHdIk+oc/RVfQl+hZ9vyntRaueJ+iP6K39AOE6tfw=</latexit>@L1@bo1:T<latexit sha1_base64="n1fGSFFpsYT3JJcfnwpFI5hKGLk=">AAACTXicdVBNixNBEO2JH7vGr6wevTQGwdMwPVkS8RTw4sHDCpvdhcwQanp6Ns32x9Dd4xKa/jv7awRPCv4QPYnYk42wihYU9Xiviqp6VSu4dVn2NRncun3n7t7+veH9Bw8fPR4dPDmxujOULagW2pxVYJngii0cd4KdtYaBrAQ7rS7e9PrpB2Ys1+rYbVpWSjhXvOEUXKRWo3nRGKC+aME4DsIXEtyaRvAurEgIN4RLXrM1OF9U1usQVp68Pg6xjsZZOp0dTmY5ztKcTKdk2oNJlmczTNJsG2O0i6PV6HtRa9pJphwVYO2SZK0rfb+GChaGRWdZC/QCztkyQgWS2dJvPw34RWRq3GgTUzm8ZW9OeJDWbmQVO/tH7N9aT/5LW3aueVV6rtrOMUWvFzWdwE7j3jZcc8OoE5sIgBoeb8V0DdE6F80dFopdUi0lqLr3JyxJGasWdX+LFn5Mtkb9dgP/H5zkKZmk+fvD8ZzsLNtHz9Bz9BIRNENz9BYdoQWi6Ap9RJ/Rl+RT8i35kfy8bh0ku5mn6I8Y7P0C/CW4Ag==</latexit>@L2@bo1:T<latexit sha1_base64="yz7wULO2C5cqv54tmhYugzB1rIQ=">AAACTXicdVBNixNBEO2JH7vGr6wevTQGwdMwPVkS8RTw4sHDCpvdhcwQenpqNs32x9Dd4xKa/jv7awRPCv4QPYnYk42wihYU9Xiviqp6VSu4dVn2NRncun3n7t7+veH9Bw8fPR4dPDmxujMMFkwLbc4qakFwBQvHnYCz1gCVlYDT6uJNr59+AGO5Vsdu00Ip6bniDWfURWo1mheNocwXLTWOU+ELSd2aRfAurPIQbgiXvIY1db6orNchrDx5fRxiHY2zdDo7nMxynKU5mU7JtAeTLM9mmKTZNsZoF0er0fei1qyToBwT1NolyVpX+n4NExCGRWehpeyCnsMyQkUl2NJvPw34RWRq3GgTUzm8ZW9OeCqt3cgqdvaP2L+1nvyXtuxc86r0XLWdA8WuFzWdwE7j3jZccwPMiU0ElBkeb8VsTaN1Lpo7LBRcMi0lVXXvT1iSMlYt6v4WLfyYbI367Qb+PzjJUzJJ8/eH4znZWbaPnqHn6CUiaIbm6C06QgvE0BX6iD6jL8mn5FvyI/l53TpIdjNP0R8x2PsF/fG4Aw==</latexit>@L3@bo1:T<latexit sha1_base64="pH35U14iWr9Tv9M+NM5yYNJnMjs=">AAACTXicdVBNixNBEO2JH7vGr6wevTQGwdMwPVkS8RTw4sHDCpvdhcwQanp6Ns32x9Dd4xKa/jv7awRPCv4QPYnYk42wihYU9Xiviqp6VSu4dVn2NRncun3n7t7+veH9Bw8fPR4dPDmxujOULagW2pxVYJngii0cd4KdtYaBrAQ7rS7e9PrpB2Ys1+rYbVpWSjhXvOEUXKRWo3nRGKC+aME4DsIXEtyaRvAurCYh3BAuec3W4HxRWa9DWHny+jjEOhpn6XR2OJnlOEtzMp2SaQ8mWZ7NMEmzbYzRLo5Wo+9FrWknmXJUgLVLkrWu9P0aKlgYFp1lLdALOGfLCBVIZku//TTgF5GpcaNNTOXwlr054UFau5FV7OwfsX9rPfkvbdm55lXpuWo7xxS9XtR0AjuNe9twzQ2jTmwiAGp4vBXTNUTrXDR3WCh2SbWUoOren7AkZaxa1P0tWvgx2Rr12w38f3CSp2SS5u8Px3Oys2wfPUPP0UtE0AzN0Vt0hBaIoiv0EX1GX5JPybfkR/LzunWQ7Gaeoj9isPcL/724BA==</latexit>@L@bo1:T<latexit sha1_base64="1GNg75a6OFSZy8EZkul/co3oJUs=">AAACS3icdZDPahRBEMZ7VmPimpjVHL00LoKnYXo27AYPEvCSg4cI2SSwMyw9PTXZJv1n6O4xLM28jU8jeNJLnkPIQTzYs1khhljQ9MdXVVTVr6gFty5JrqPeo8cbTza3nvafbe883x28eHlqdWMYTJkW2pwX1ILgCqaOOwHntQEqCwFnxeWHLn/2GYzlWp24ZQ25pBeKV5xRF6z54H1WGcp8VlPjOBU+k9QtWBAf27a9Y1/xEhbU+aywXrft3JN3J6FiPhgm8XiyP5qkOIlTMh6TcSdGSZpMMImTVQzROo7ng5us1KyRoBwT1NoZSWqX+24ME9D2s8ZCTdklvYBZkIpKsLlf3dniN8EpcaVNeMrhlXu3w1Np7VIWobI7w97PdeZDuVnjqoPcc1U3DhS7HVQ1AjuNO2i45AaYE8sgKDM87IrZggZwLqDtZwqumJaSqrLj085IHn4tym4XLfyQrED9pYH/L07TmIzi9NP+8JCskW2hV+g1eosImqBDdISO0RQx9AV9Rd/Rj+hb9DP6Ff2+Le1F65499E/0Nv4AmFG3Xg==</latexit>bo1:To1:TMANUSCRIPT

5

}

∀

1,

m

· · ·

· · ·

, M

∈ {

b(n)
m ,

R and

Note that, if

, b(n)
K }

(cid:98)
∂
L
∂(cid:98)x(n)
m ←
that correspond to the zero-padded part.

x(n) is zero-padded from length M to length
K before DFT, the inverse-DFT of g(n) will contain gra-
dients w.r.t the zero-padded part. In such a case, we can
simply assign
and ignore
b(n)
M +1,
{
3) Multi-resolution spectral amplitude distance: The pre-
RT
∂
vious explanation shows that
L∂ (cid:98)o1:T ∈
can be computed no matter how we conﬁgure the window
length, frame shift, and DFT bins,
the values of N ,
M , K. It is thus straightforward to merge multiple distances
with different windowing and framing conﬁgu-
{L1,
· · ·
rations. In such a case, the ultimate distance can be deﬁned as
LS. Accordingly, the gradients can be merged
=
L1 +
∂
+
L∂ (cid:98)o1:T
Using multiple spectral distances is expected to help the
model learn the spectral details of natural waveforms in differ-
ent spatial and temporal resolutions. We used three distances
in this study, which are explained in Section IV-B.

, as illustrated in Figure 2.

+
1
L
∂ (cid:98)o1:T

· · ·
= ∂

S
L
∂ (cid:98)o1:T

LS}

+ ∂

L ∈

i.e.,

L
as

· · ·

,

(cid:12)

y∗

4) Remark on spectral amplitude distance: The short-
time spectral amplitude distances may be more appropriate
than the waveform MSE in Equation (1) for the NSF models.
For a single speech frame, it is assumed with an NSF model
using a spectral amplitude distance that the spectral amplitude
RK follows a multivariate log-
vector z = y
normal distribution with a diagonal covariance matrix, where
CK is the K-point DFT of a waveform
y = DFT(x)
∈
denotes element-wise multiplication. Although
frame x and
we cannot derive an analytical form of p(x) from p(z), we
can at least infer that the distribution of x or the original
waveform o1:T assumed with the model is at least not an
isotropic Gaussian distribution. An NSF model with a spec-
tral amplitude distance can potentially model the waveform
temporal correlation within an analysis window.

(cid:12)

∈

Using the spectral amplitude distance is reasonable also
because the perception of speech sounds are affected by the
spectral acoustic cues such as formants and their transition
[35], [36], [37]. Although the spectral amplitude distance
ignores other acoustic cues, such as phase [38] and timing
[39], we only considered the spectral amplitude distance in this
study because we have not found a phase or timing distance
that is differentiable and effective.

B. Baseline NSF model

(cid:98)

We now give the details on the b-NSF model. As Figure 3
illustrates, the b-NSF model uses three modules to convert an
input acoustic feature sequence c1:B of length B into a speech
waveform
o1:T of length T : a source module that generates an
excitation signal e1:T , a ﬁlter module that transforms e1:T into
an output waveform, and a condition module that processes
c1:B for the source and ﬁlter modules. The model is trained
using the spectral distance explained in the previous section.
1) Condition module: The input of the condition module
, where cb = [ ˜fb, s(cid:62)b ](cid:62) includes
is c1:B =
the F0 value ˜fb and the spectral features sb for the b-th
˜f1,
is upsampled to
frame. The F0 sub-sequence
{
f1:T by replicating each ˜fb for
times, after which
f1:T is fed to the source module. The spectral features are

, ˜fB}
(cid:101)

· · ·
T /B
(cid:100)

, cB}

c1,

· · ·

{

processed by a bi-directional recurrent layer with long-short-
term memory (LSTM) units [40] and a 1-dimensional CONV
layer with a window size of 3. The processed spectral features
are then concatenated with the F0 and upsampled as ˜c1:T .
The layer size of the Bi-LSTM and CONV layers is 64 and
63, respectively. The dimension of ˜ct is 64. Note that there
is no golden network structure for the condition module. We
used the structure in Figure 3 because it has been used in our
WaveNet-vocoder [41] 4.

2) Source module: Given the F0, the source module con-
structs an excitation signal on the basis of sine waveforms
and random noise. In voiced segments, the excitation signal
is a mixture of sine waveforms whose frequency values are
determined by F0 and its harmonics. In unvoiced regions, the
excitation signal is a sequence of Gaussian noise.
R
The input F0 sequence is f1:T , where ft ∈

0 is the F0
value of the t-th time step, and ft > 0 and ft = 0 denote being
voiced and unvoiced, respectively. A sine waveform e<0>
1:T with
the fundamental frequency can be generated as

≥

t

α sin(

2π

fk
Ns

+ φ) + nt,

if ft > 0

e<0>
t

,

[

∈

−

(13)

nt,

(cid:88)k=1

if ft = 0

α
3σ
(0, σ2) is Gaussian noise, φ

= 


π, π] is a ran-
where nt ∼ N
dom initial phase, and Ns is the waveform sampling rate. The
hyper-parameter α adjusts the amplitude of source waveforms,
while σ is the standard deviation of the Gaussian noise5. We
set σ = 0.003 and α = 0.1 in this study. Equation (13) treats
ft as an instantaneous frequency [42]. Thus, the phase of the
e<0>
1:T becomes continuous even if ft changes. Figure 5 plots
an example e<0>
and the corresponding f1:T .
1:T
The source module also generates harmonic overtones. For
the h-th harmonic overtone, which corresponds to the (h + 1)-
th harmonic frequency, an excitation signal e<h>
is generated
from Equation (13) with (h + 1)ft. The source module then
uses a trainable feedforward (FF) layer with a tanh activation
function to combine e<0>
into the ﬁnal excitation
1:T
signal e1:T =
.
, eT }
e1,
}
{
This combination can be written as

and e<h>
, where et ∈

t
∀

∈ {

R,

· · ·

· · ·

, T

1,

1:T

1:T

H

e1:T = tanh(

whe1:T

<h> + wb),

(14)

where
the total number of overtones.

wH , wb}

w0,

· · ·

{

(cid:88)h=0
are the FF layer’s weights, and H is

The value of H is not critical to the model’s performance
because the model can re-create higher harmonic tones, as the
experiments discussed in Section IV-D demonstrated. We set
fmax < Ns/4,
H = 7 based on a tentative rule (H + 1)
where Ns = 16 kHz is the sampling rate, and fmax ≈
500
Hz is the largest F0 value observed in our data corpus. We

∗

4Our previous NSF model [20] had no “concatenate” block in the condition
module. In this study we added the “concatenate” block so that our NSF
models use exactly the same condition module as our WaveNet-vocoder.

5In our previous NSF paper [20], we used 1
3σ

In this study, we used α
3σ
segments is comparable to that of the sine waveforms in voiced segments.

nt for the noise excitation.
nt so that the amplitude of the noise in unvoiced

MANUSCRIPT

6

Fig. 3. Structure of baseline NSF (b-NSF) model. B and T denote lengths of input feature sequence and output waveform, respectively. FF, CONV, and
Bi-LSTM denote feedforward, convolutional, and bi-directional recurrent layers, respectively. Structure of dilated-CONV ﬁlter block is plotted in Figure 4.

Fig. 4. Structure of dilated-CONV-based ﬁlter blocks for b-NSF model (top) and simpliﬁed NSF (s-NSF) model (bottom). vin
1:T denote input and
output of one ﬁlter block, respectively. (cid:12) denotes element-wise product. Every ﬁlter block contains 10 dilated-convolution layers, and every CONV and FF
layer use tanh activation function.

1:T and vout

multiplication. The output vout
1:T is further processed in the
following ﬁlter block, and the output of the last ﬁlter block is
the generated waveform

o1:T .

(cid:98)

Our implementation used a kernel size of 3 for the dilated-
CONV layers, which is supposed to be necessary for non-
AR waveform models [18]. Both the input and output feature
vectors of the dilated-CONV layer have 64 dimensions. Ac-
cordingly, the residual connection that connects two adjacent
dilated-CONV layers also has 64 dimensions. The feature
vectors to the FF layer that produces a1:T and b1:T have 128
dimensions, i.e., skip-connection of 128 dimensions. The b1:T
is parameterized as b1:T = exp(˜b1:T ) to be positive [19].

The baseline dilated-CONV ﬁlter block is similar to the
student models in ClariNet and Parallel WaveNet because
all use the stack of so-called “dilated residual blocks” in
AR WaveNet [12]. However, because the b-NSF model does
not use knowledge distilling, it is unnecessary to compute
the distribution of the signal during forward propagation as
ClariNet and Parallel WaveNet do. Neither is it necessary to
make the ﬁlter blocks invertible as IAF does. Accordingly, the
dilated convolution layers can be non-causal, even though we
used causal ones to keep conﬁgurations of our NSF models
consistent with our WaveNet-vocoder in the experiments.

C. Simpliﬁed NSF model

The network structure of the b-NSF model, especially the
ﬁlter module, was designed on the basis of our experience
with implementing WaveNet. However, we found that the ﬁlter
module can be simpliﬁed, as shown in Figure 4. Such a ﬁlter
block keeps only the dilated-CONV layers, skip-connections,
and FF layers for dimension change. The output of a ﬁlter

Fig. 5. Example of F0 sequence f1:T and fundamental component e<0>
1:T

used Ns/4 as the upper-bound so that there is at least four
sampling points in each period of the sine waveform.

3) Neural ﬁlter module: The ﬁlter module of the b-NSF
model transforms the excitation e1:T into an output waveform
o1:T by using ﬁve baseline dilated-CONV ﬁlter blocks. The
structure of a baseline ﬁlter block is illustrated in Figure 4.

∈

∈

1,

1,

, T

· · ·

∈ {

R64

∈ {

t ∈

R64,

· · ·
∈

1 is the transformation matrix and b

Suppose the input to the block is vin

R,
1:T , where vin
(cid:98)
. This input vin
1:T is expanded in dimension
t
∀
}
through an FF layer as tanh(wvin
,
, T
t
t +b)
∀
}
R64
where w
×
is then processed by a
is the bias. The expanded signal
dilated-CONV layer, summed with the condition feature ˜c1:T ,
processed by the gated activation unit based on tanh and
sigmoid [12], and transformed by two additional FF layers.
This procedure is repeated ten times within this ﬁlter block,
and the dilation size of the dilated convolution layer in the
k-th stage is set to 2k
1. The outputs from the ten stages
are summed and transformed into two signals a1:T and b1:T .
After that, vin
1:T by
1:T = vin
vout
denotes element-wise

1:T is converted into an output signal vout

b1:T + a1:T , where

−

1:T (cid:12)

(cid:12)

Neural	filter	moduleSource	moduleCondition	modulee1:TNoise++FFf1:TUpsamplingDilated-CONV-based	filter	block	1UpsamplingBi-LSTMF0Acoustic	featuresSpectral	features…bo1:TSine	generatorc1:BDilated-CONV-based	filter	block	2Dilated-CONV-based	filter	block	5˜c1:TConcatenateCONVBaseline	dilated-CONV-based	filter	block	for	b-NSFDilated	CONV	layer+TanhSigmoid*FFFFFF+Dilated	CONV	layer+TanhSigmoid*FFFF+FF+a1:Tb1:T˜c1:T˜c1:Tp1:Tp b+aq1:Te<h>1:T<latexit sha1_base64="2bceStuTENk0NHmOVBCyyIGlfHQ=">AAACH3icbVDLSgMxFM34tr6qLt0MFsFVmaigiEjBjcsKVoV2LJnMrQ3mMSQZtYT5FcGV/ok7cdsfcW2mduHrQMjhnHu5h5NknBkbRcNgYnJqemZ2br6ysLi0vFJdXbswKtcUWlRxpa8SYoAzCS3LLIerTAMRCYfL5Pak9C/vQBum5LkdZBALciNZj1FivdStrnUS46DoOnx4Xly7o/5x0a3Wono0QviX4DGpoTGa3epHJ1U0FyAt5cSYNo4yGzuiLaMcikonN5ARektuoO2pJAJM7EbZi3DLK2nYU9o/acOR+n3DEWHMQCR+UhDbN7+9UvzPa+e2dxA7JrPcgqRfh3o5D60KyyLClGmglg88IVQznzWkfaIJtb6uSkfCPVVCEJk6X1HRxrH/FU/LLIq7Gi7KovDvWv6Si5063q3vnO3VGnhc2RzaQJtoG2G0jxroFDVRC1H0gB7RM3oJnoLX4C14/xqdCMY76+gHguEni22jdA==</latexit>Simplified	dilated-CONV-based	filter	block	for	s-NSF	and	h-NSFDilated	CONV	layer+FFFFa1:T˜c1:TDilated	CONV	layer+˜c1:Tp1:Tp+aq1:TDilated	CONV	layerDilated	CONV	layer……+e<0>1:T<latexit sha1_base64="oBopXlJiXuZzfT1lqdegZgTCSQU=">AAACH3icbVDLSgMxFM34tr6qLt0MFsFVmaigiIjgxqWCbYV2LJnMrQbzGJKMWsL8iuBK/8SduPVHXJtpu9DqgZDDOfdyDyfJODM2ij6Dicmp6ZnZufnKwuLS8kp1da1pVK4pNKjiSl8lxABnEhqWWQ5XmQYiEg6t5O609Fv3oA1T8tL2M4gFuZGsxyixXupW1zqJcVB0HT68LK7dUXRcdKu1qB4NEP4leERqaITzbvWrkyqaC5CWcmJMG0eZjR3RllEORaWTG8gIvSM30PZUEgEmdoPsRbjllTTsKe2ftOFA/bnhiDCmLxI/KYi9NeNeKf7ntXPbO4gdk1luQdLhoV7OQ6vCsogwZRqo5X1PCNXMZw3pLdGEWl9XpSPhgSohiEydr6ho49j/iqdlFsVdDRdlUXi8lr+kuVPHu/Wdi73aCR5VNoc20CbaRhjtoxN0hs5RA1H0iJ7QC3oNnoO34D34GI5OBKOddfQLwec3Lj2jPA==</latexit>Neural filter moduleSource moduleCondition modulee1:TNoise++FFf1:TUpsamplingDilated-CONV-based filter block 1UpsamplingBi-LSTMF0Acoustic featuresSpectral features…bo1:TSine generatorc1:BDilated-CONV-based filter block 2Dilated-CONV-based filter block 5˜c1:TConcatenateCONVBaseline dilated-CONV-based filter block for b-NSFDilated CONV layer+TanhSigmoid.FFFFFF+Dilated CONV layer+TanhSigmoid.FFFF++a1:Tb1:T˜c1:T˜c1:Te<h>1:T<latexit sha1_base64="2bceStuTENk0NHmOVBCyyIGlfHQ=">AAACH3icbVDLSgMxFM34tr6qLt0MFsFVmaigiEjBjcsKVoV2LJnMrQ3mMSQZtYT5FcGV/ok7cdsfcW2mduHrQMjhnHu5h5NknBkbRcNgYnJqemZ2br6ysLi0vFJdXbswKtcUWlRxpa8SYoAzCS3LLIerTAMRCYfL5Pak9C/vQBum5LkdZBALciNZj1FivdStrnUS46DoOnx4Xly7o/5x0a3Wono0QviX4DGpoTGa3epHJ1U0FyAt5cSYNo4yGzuiLaMcikonN5ARektuoO2pJAJM7EbZi3DLK2nYU9o/acOR+n3DEWHMQCR+UhDbN7+9UvzPa+e2dxA7JrPcgqRfh3o5D60KyyLClGmglg88IVQznzWkfaIJtb6uSkfCPVVCEJk6X1HRxrH/FU/LLIq7Gi7KovDvWv6Si5063q3vnO3VGnhc2RzaQJtoG2G0jxroFDVRC1H0gB7RM3oJnoLX4C14/xqdCMY76+gHguEni22jdA==</latexit>Simplified dilated-CONV-based filter block for s-NSF and hn-NSFDilated CONV layer+FFFFa1:T˜c1:TDilated CONV layer+˜c1:TDilated CONV layerDilated CONV layer……+e<0>1:T<latexit sha1_base64="oBopXlJiXuZzfT1lqdegZgTCSQU=">AAACH3icbVDLSgMxFM34tr6qLt0MFsFVmaigiIjgxqWCbYV2LJnMrQbzGJKMWsL8iuBK/8SduPVHXJtpu9DqgZDDOfdyDyfJODM2ij6Dicmp6ZnZufnKwuLS8kp1da1pVK4pNKjiSl8lxABnEhqWWQ5XmQYiEg6t5O609Fv3oA1T8tL2M4gFuZGsxyixXupW1zqJcVB0HT68LK7dUXRcdKu1qB4NEP4leERqaITzbvWrkyqaC5CWcmJMG0eZjR3RllEORaWTG8gIvSM30PZUEgEmdoPsRbjllTTsKe2ftOFA/bnhiDCmLxI/KYi9NeNeKf7ntXPbO4gdk1luQdLhoV7OQ6vCsogwZRqo5X1PCNXMZw3pLdGEWl9XpSPhgSohiEydr6ho49j/iqdlFsVdDRdlUXi8lr+kuVPHu/Wdi73aCR5VNoc20CbaRhjtoxN0hs5RA1H0iJ7QC3oNnoO34D34GI5OBKOddfQLwec3Lj2jPA==</latexit>vin1:T<latexit sha1_base64="4u34VCm51wVmBZufpjrban8f/DM=">AAACJXicbVBNSxxBEO0xJm42Jq7xEshlyBLwtEyrkJDTQi45KrgfsDNZenpqtbE/hu4azdKMvyaQk/4TbxLIyX/hOb0fh2TXB00/3quiql5eSuEwSf5EG882n7/Yarxsvtp+/Wantfu270xlOfS4kcYOc+ZACg09FChhWFpgKpcwyC++zvzBJVgnjD7FaQmZYmdaTARnGKRx612aO39Zjz39clp/9ynCD/RC1/W41U46yRzxOqFL0iZLHI9bj2lheKVAI5fMuRFNSsw8syi4hLqZVg5Kxi/YGYwC1UyBy/z8gjr+GJQinhgbnsZ4rv7b4ZlybqryUKkYnrtVbyY+5Y0qnHzOwkFlhaD5YtCkkjGaeBZHXAgLHOU0EMatCLvG/JxZxjGE1kw1XHGjFNOFD0HVI5qF38hitouRvk3nQdHVWNZJ/6BDDzsHJ0ftLl1G1iDvyQeyTyj5RLrkGzkmPcLJNflJbsht9Cu6i+6j34vSjWjZs0f+Q/TwF8qVps8=</latexit>vout1:T<latexit sha1_base64="My3ezJWgyyfYt7O9/wcd4p7SKQ8=">AAACJnicbZDLahsxFIY1SdO4btO66apkM8QUujKjpJDQlSGbLFOIL+CZGI3mOBHRZZDOODVi6NMEsmrfpLtSuutTdB35smjtHhD6+f9z0NGXl1I4TJJf0db2k52nu41nzecv9l6+ar3e7ztTWQ49bqSxw5w5kEJDDwVKGJYWmMolDPLbs3k+mIJ1wuhLnJWQKXatxURwhsEat96mufPTeuzpx8v6yqcIn9GbCut63GonnWRR8aagK9Emq7oYt/6kheGVAo1cMudGNCkx88yi4BLqZlo5KBm/ZdcwClIzBS7ziy/U8bvgFPHE2HA0xgv37wnPlHMzlYdOxfDGrWdz83/ZqMLJaeaFLisEzZcPTSoZo4nnPOJCWOAoZ0EwbkXYNeY3zDKOgVoz1XDHjVJMFz6Qqkc0C7eRxXwXI32bLkDRdSybon/Uocedo08f2l26QtYgB+SQvCeUnJAuOScXpEc4+ULuyVfyLXqIvkc/op/L1q1oNfOG/FPR70fPt6da</latexit>vin1:T<latexit sha1_base64="4u34VCm51wVmBZufpjrban8f/DM=">AAACJXicbVBNSxxBEO0xJm42Jq7xEshlyBLwtEyrkJDTQi45KrgfsDNZenpqtbE/hu4azdKMvyaQk/4TbxLIyX/hOb0fh2TXB00/3quiql5eSuEwSf5EG882n7/Yarxsvtp+/Wantfu270xlOfS4kcYOc+ZACg09FChhWFpgKpcwyC++zvzBJVgnjD7FaQmZYmdaTARnGKRx612aO39Zjz39clp/9ynCD/RC1/W41U46yRzxOqFL0iZLHI9bj2lheKVAI5fMuRFNSsw8syi4hLqZVg5Kxi/YGYwC1UyBy/z8gjr+GJQinhgbnsZ4rv7b4ZlybqryUKkYnrtVbyY+5Y0qnHzOwkFlhaD5YtCkkjGaeBZHXAgLHOU0EMatCLvG/JxZxjGE1kw1XHGjFNOFD0HVI5qF38hitouRvk3nQdHVWNZJ/6BDDzsHJ0ftLl1G1iDvyQeyTyj5RLrkGzkmPcLJNflJbsht9Cu6i+6j34vSjWjZs0f+Q/TwF8qVps8=</latexit>vout1:T<latexit sha1_base64="My3ezJWgyyfYt7O9/wcd4p7SKQ8=">AAACJnicbZDLahsxFIY1SdO4btO66apkM8QUujKjpJDQlSGbLFOIL+CZGI3mOBHRZZDOODVi6NMEsmrfpLtSuutTdB35smjtHhD6+f9z0NGXl1I4TJJf0db2k52nu41nzecv9l6+ar3e7ztTWQ49bqSxw5w5kEJDDwVKGJYWmMolDPLbs3k+mIJ1wuhLnJWQKXatxURwhsEat96mufPTeuzpx8v6yqcIn9GbCut63GonnWRR8aagK9Emq7oYt/6kheGVAo1cMudGNCkx88yi4BLqZlo5KBm/ZdcwClIzBS7ziy/U8bvgFPHE2HA0xgv37wnPlHMzlYdOxfDGrWdz83/ZqMLJaeaFLisEzZcPTSoZo4nnPOJCWOAoZ0EwbkXYNeY3zDKOgVoz1XDHjVJMFz6Qqkc0C7eRxXwXI32bLkDRdSybon/Uocedo08f2l26QtYgB+SQvCeUnJAuOScXpEc4+ULuyVfyLXqIvkc/op/L1q1oNfOG/FPR70fPt6da</latexit>vin b+a<latexit sha1_base64="Mj1xEzGoXJZA+jSl9pNm+0ocXJA=">AAACMXicbVDLSiNBFK32bXxFXbppjIIghG4VdCnMxmUGjArpnlBdfaOF9WiqbquhqR/wawZc6Z9kN7h1P+upjln4mAPFPZxzL3XvyQrBLUbRKJianpmdm19YbCwtr6yuNdc3LqwuDYMu00Kbq4xaEFxBFzkKuCoMUJkJuMxuf9T+5R0Yy7U6x2EBqaTXig84o+ilfnMnyWx1535VCcIDVlw5l+hcYy1nbr8u1PWbragdjRF+J/GEtMgEnX7zb5JrVkpQyAS1thdHBaYVNciZANdISgsFZbf0GnqeKirBptX4GhfueiUPB9r4pzAcqx8nKiqtHcrMd0qKN/arV4v/83olDk5Sf2JRIij2/tGgFCHqsI4mzLkBhmLoCWWG+11DdkMNZegDbCQK7pmWkqq88rG4Xpz6qkVe76JF1YpdHVT8NZbv5OKgHR+2D34etU7jSWQLZItskz0Sk2NySs5Ih3QJI4/kN3kmL8FTMAr+BK/vrVPBZGaTfELw9g/VBqyD</latexit>FFvin+a<latexit sha1_base64="1Jh196wpjPNf1XqydSJUavvE30w=">AAACJnicbVDLSsRAEJz4dn2tehIvwUUQhCVZBT0KXjwquCps4jKZ9OrgPMJMR11C8GsET/on3kS8+RWenax78FUwTFHVTXdXkgluMQjevJHRsfGJyanp2szs3PxCfXHpxOrcMGgzLbQ5S6gFwRW0kaOAs8wAlYmA0+Rqv/JPr8FYrtUx9jOIJb1QvMcZRSd16ytRYovr8ryIEG6x4KosNyuJlt16I2gGA/h/STgkDTLEYbf+EaWa5RIUMkGt7YRBhnFBDXImoKxFuYWMsit6AR1HFZVg42JwQumvOyX1e9q4p9AfqN87Ciqt7cvEVUqKl/a3V4n/eZ0ce7uxuyvLERT7GtTLhY/ar/LwU26Aoeg7QpnhblefXVJDGbrUapGCG6alpCotXCxlJ4zdr0Va7aJF0QjLKqjwdyx/yUmrGW41W0fbjb1wGNkUWSVrZIOEZIfskQNySNqEkTtyTx7Jk/fgPXsv3utX6Yg37FkmP+C9fwIHxad4</latexit>++++++++●●0200F0valueF0valuef1:T4400045000460004700048000490005000051000Timeindex−0.10.00.1Amplitudee<0>1:TMANUSCRIPT

7

Fig. 6. Diagram of harmonic-plus-noise NSF (hn-NSF) model and frequency responses of low-pass (LP) and high-pass (HP) ﬁlters for voiced and unvoiced
regions.

block is the sum of a residual signal a1:T and the input signal
1:T . Using the sum vout
vin
1:T + a1:T rather than the afﬁne
transformation vout
1:T = vin
b1:T +a1:T was motivated by the
result of our ablation test on the b-NSF model (Section IV-C:
see the results of N1). Note that each dilated-CONV layer uses
the tanh activation function.

1:T = vin
1:T (cid:12)

On the basis of the simpliﬁed ﬁlter block, we constructed
the s-NSF model. Compared with the b-NSF model, the s-NSF
model has the same network structure except the simpliﬁed
ﬁlter blocks. Accordingly, the s-NSF model has fewer param-
eters and a faster generation speed. The cascade of simpliﬁed
ﬁlter blocks also turns the neural ﬁlter module into a deep
residual network [43].

D. Harmonic-plus-noise NSF model

Although the b-NSF and s-NSF models perform well in
many cases, we found that both may produce low-quality
unvoiced sounds or sometimes silence for fricative consonants.
Analysis on the neural ﬁlter modules of a well-trained model
suggests that the hidden features for the voiced sounds have a
waveform-like temporal structure, while those for the unvoiced
sounds are noisy. We thus hypothesize that voiced and un-
voiced sounds may require different non-linear transformations
in ﬁlter modules.

A better strategy may be to generate a periodic and noise
component separately and merge them with different ratios
into voiced and unvoiced sounds, an idea similar to the
harmonic-plus-noise model [44], [45], [46]. Following the
literature, we also refer to the periodic component as the
harmonic component.

As an implementation, we constructed the hn-NSF model,
which is illustrated in Figure 6. While the hn-NSF model uses
the same modules as the s-NSF model to generate a waveform
for the harmonic component, it uses only a noise excitation
and simpliﬁed ﬁlter block for the noise component. The noise
and harmonic components are ﬁltered by a high-pass and low-
pass digital ﬁlter, respectively, and are summed as the output
waveform. Digital ﬁlters have been used to merge the noise
and periodic signals in the classical speech modeling methods
[47], [48]. The difference is that the hn-NSF model uses ﬁlters
to directly merge the waveforms rather than the source signals.
Because the voiced sounds are usually dominated by the
harmonic part, while the unvoiced sounds are dominated by the

TABLE II
PARAMETERS OF EQUIRIPPLE LOW- AND HIGH-PASS FIR FILTERS FOR
HN-NSF. PASSBAND RIPPLE AMPLITUDE IS LESS THAN 5 DB, AND
STOPBAND ATTENUATION IS -40 DB. FILTER COEFFICIENTS ARE
CALCULATED USING PARKS-MCCLELLAN ALGORITHM [49].

Voiced sound
Unvoiced sound

Low-pass FIR ﬁlter

High-pass FIR ﬁlter

passband
0 − 5 kHz
0 − 1 kHz

stopband
7 − 8 kHz
3 − 8 kHz

passband
7 − 8 kHz
3 − 8 kHz

stopband
0 − 5 kHz
0 − 1 kHz

noise part, we use two pairs of low- and high-pass ﬁlters in the
hn-NSF model to merge the harmonic and noise components,
one pair for voiced sounds and the other for unvoiced sounds.
The conﬁgurations of the ﬁlters are listed in Table II, and
their frequency responses are plotted on the right side of
Figure 6. We implemented the ﬁlters as equiripple ﬁnite
impulse response (FIR) ﬁlters and computed their coefﬁcients
by using the Parks-McClellan algorithm [49]. Note that the
order of the FIR ﬁlter is determined by the Parks-McClellan
algorithm. For the ﬁlters speciﬁed in Figure 6, the ﬁlter order is
around 10. After the ﬁlter coefﬁcients are calculated using the
algorithm, they are stored and ﬁxed in the model. The voicing
ﬂag for selecting the ﬁlter can be easily extracted from the
input F0 sequence.

We determined the passband and stopband of the FIR
ﬁlters after analyzing the spectrogram of the speech data in
our corpus. Although the passband and stopband are ﬁxed,
the neural ﬁlter blocks can learn to compensate and ﬁne-
tune the energy of the generated signals in certain frequency
bands. Figure 7 plots the spectral amplitudes of the harmonic
and noise components of one voiced frame generated by
the hn-NSF model. For the harmonic component, although
the passband of the low-pass ﬁlter is only up to 5kHz, the
neural ﬁlter module generates a harmonic component with a
high energy above between 5 and 7 kHz, which compensates
for the attenuation of the low-pass ﬁlter. As the last row
of Figure 7 shows, the harmonic component dominates the
generated waveform frame from 0 to 7 kHz, and its spectral
amplitude is similar to that of natural speech.

IV. EXPERIMENTS
Following the explanation on our NSF models, we now
discuss the experiments. After describing the corpus and data

Voiced/unvoicedSource	modulee1:Tf1:TSimplified	dilated-CONV	filter	block	1…Simplified	dilated-CONV	filter	block	2Simplified	dilated-CONV	filter	block	5Condition	moduleGaussian	noise	Simplified	dilated-CONV	filter	block+bo1:TVoicing	flagVoiced/unvoicedc1:BAcoustic	featuresSource	modulef1:TSimplified	dilated-CONV	filter	block	1…Simplified	dilated-CONV	filter	block	5Condition	moduleGaussian	noise	Simplified	dilated-CONV	filter	block+bo1:TVoicing	flagc1:BAcoustic	featuresHPLP−100−500Filtersforvoicedregionslow-passhigh-pass0.01.02.03.04.05.06.07.08.0Frequency(kHz)−100−500FiltersforunvoicedregionsAmplitude(dB)MANUSCRIPT

8

TABLE III
SHORT-TIME ANALYSIS CONFIGURATIONS FOR SPECTRAL AMPLITUDE
DISTANCE OF NSF MODELS

DFT bins K

L1
512

Frame length M 320 (20 ms)
80 (5 ms)

Frame shift

L2
128
80 (5 ms)
40 (2.5 ms)

L3
2048
1920 (120 ms)
640 (40 ms)

Note: all conﬁgurations used Hann window.

used the F0 and either the MGCs or the Mel-spectrogram as
the input features to the waveform models.

To evaluate the waveform models in SPSS TTS systems,
we also trained acoustic models to predict acoustic features
from linguistic features. To train acoustic models, we extracted
linguistic features from text, including quin-phone identity,
phrase accent
information [54].
These features were force-aligned with the acoustic features
by using hidden Markov models.

type, and other structural

B. Model conﬁgurations

Four waveform models were evaluated in the experiments:
our three NSF models b-NSF, s-NSF, and hn-NSF and AR
WaveNet (WaveNet). We chose WaveNet as the benchmark
because of its excellent performance reported in both the
original paper [12] and our previous study [13][55]. We did
not include IAF-ﬂow-based models due to their high demand
on training time and GPU resources. Neither did we consider
Parallel WaveNet or ClariNet due to the lack of authentic
implementation.

−

The network conﬁguration of b-NSF was described in
Section III-B. It used ﬁve baseline dilated-CONV ﬁlter blocks,
and each block contained ten dilated CONV and other hidden
layers, as illustrated in Figure 4. The dilation size of the k
layer was 2k
1. s-NSF was the same as b-NSF except that
each baseline dilated-CONV ﬁlter block was replaced with a
simpliﬁed version. hn-NSF used the same network as s-NSF
for the harmonic component and a single simpliﬁed block for
the noise component. All three NSF models used the spectral
L3, and the conﬁguration of
amplitude distance
L2 +
L1 used the same frame
each
shift and frame length as those for extracting the acoustic
features from the waveforms.
L3 were decided so that
one has a higher temporal resolution while the other has a
higher frequency resolution than
L1. The conﬁgurations in
Table III may be inappropriate for a different corpus, and a
good conﬁguration may be found through trial and error.

=
is listed in Table III. Note that

L2 and

L1 +

L∗

L

Our WaveNet used the same network structure as that in
our previous study [13]. It used 40 dilated-CONV layers in
total, where the k-th one had a dilation size of 2modulo(k
1,10).
Between two adjacent CONV layers, WaveNet used a net-
work structure similar to that in the baseline ﬁlter block of
b-NSF. However, each dilated-CONV layer used a kernel size
of 2 and 128 output channels, and the condition feature ˜ct was
transformed by an additional FF layer into 128 dimensions
before being summed with the dilated-CONV layer’s output.
The skip-connection had 256 dimensions, and the output
vectors of the 40 dilated-CONV stages were propagated by

−

Fig. 7. Spectral amplitude and ﬁlter frequency response of one voiced speech
frame generated by hn-NSF (Section IV-D). Note that line of harmonic
component overlaps with harmonic+noise and natural one up to 7 kHz.

conﬁguration in Section IV-A, we describe an ablation test
conducted on the b-NSF model in Section IV-C. We then
compare the three NSF models with WaveNet in Section IV-D.
In Section IV-E, we investigate the controllability of the input
F0 on the NSF models, and in Section IV-F, we examine the
internal behaviors of the NSF models.

A. Corpus and data conﬁguration

Our experiments used a data set of neural-style reading
speech by a Japanese female speaker (F009), which is part of
the XIMERA speech corpus [50]. This data set was recorded
at a sampling rate of 48 kHz and segmented into 30,016
utterances. The total duration is around 50 hours.

We prepared three training subsets for the experiments on
waveform modeling: the ﬁrst subset contained 9,000 randomly
selected utterances (15 hours),
the second included 3,000
utterances (5 hours) randomly selected from the ﬁrst subset,
and the third contained 1,000 utterances (1.6 hours) randomly
selected from the second subset. The ﬁrst and third subsets
were used to evaluate the performance of the NSF models
and WaveNet in Section IV-D. The second subset was used
in the ablation test on the NSF models in Section IV-C. We
also prepared a validation set with 500 utterances and a test set
with another 480 utterances. All utterances were downsampled
to 16 kHz for waveform model training.

The acoustic features were extracted from the natural wave-
forms with a frame shift of 5 ms (200 Hz). The F0 values were
extracted by an ensemble of pitch trackers [51]. Two types
of spectral features were prepared: Mel-generalized cepstral
coefﬁcients (MGCs) [52] of order 60 extracted using the
WORLD vocoder [53] and Mel-spectrogram of order 80. We

−100−500Amplitude(dB)Spectralamplitudeofharmoniccomponentbeforeﬁlteringafterﬁlteringﬁlterfreq.response−100−500Amplitude(dB)Spectralamplitudeofnoisecomponentbeforeﬁlteringafterﬁlteringﬁlterfreq.response0k1k2k3k4k5k6k7k8kFrequency(Hz)−100−500Amplitude(dB)Spectralamplitudeofharmoic+noiseHarmoniccomponentNoisecomponentHarmonic+noiseNaturalMANUSCRIPT

9

TABLE IV
MODELS FOR ABLATION TEST (SECTION IV-C)

Model
b-NSF
L1
L2
L3
S1
S2
N1
N2

Description
trained on 5-hr. training set
b-NSF without using L3sssssssssssssssssss (i.e., L = L1 + L2)
b-NSF without using L2sssssssssssssssssss (i.e., L = L1 + L3)
b-NSF without using L2 or L3sssssssssssssssss (i.e., L = L1)
b-NSF without harmonics overtones (i.e., H=0 in Equation (14))
b-NSF using noise as source signal (i.e., e1:T = α
3σ
b-NSF with b1:T = 1 in ﬁlter blocks
b-NSF with b1:T = 0 in ﬁlter blocks

n1:T )

the skip-connection and summed together. The summed vector
was transformed into a 1024-dimensional activation vector to a
softmax function, which calculates the categorical distribution
for the 10-bit quantized µ-law waveform value. The condition
module of WaveNet was the same as the NSF models. During
generation, WaveNet selected the most probable waveform
value from the distribution for 25% of the voiced time steps.
Otherwise, it randomly drew a sample as the output6.

All the neural waveform models were trained on a single-
GPU card (Nvidia Tesla P100) using the Adam optimizer [56]
with a learning rate= 0.0003, β1 = 0.9, β2 = 0.999, and
8. The training process was terminated when the error
(cid:15) = 10−
on the validation set continually increases for ﬁve epochs.
The batch size was 1, and each utterance was truncated into
segments of at most 3 seconds to ﬁt the GPU memory.

All the neural waveform models were implemented using
a modiﬁed CURRENNT toolkit [57]. Using the same toolkit
allows us to fairly compare the models since they use the
same set of low-level CUDA/THUST functionalities [58].
Note that our WaveNet implementation is sufﬁciently good
as the benchmark. As another study on the same corpus
demonstrated [55], our WaveNet implementation can generate
speech waveforms that are similar to the original natural
waveforms in terms of perceived quality, given natural acoustic
features 7. The code, scripts, and samples are publicly available
at https://nii-yamagishilab.github.io/samples-nsf/nsf-v2.html.

For the acoustic models that predict acoustic features from
the linguistic features, we used shallow and deep neural AR
models [9], [5] to generate the MGCs and F0, respectively.
The recipes for training these acoustic models were the same
as those in another of our previous study [54]. We trained
another deep AR model to generate the Mel-spectrogram using
a similar recipe. The number of training utterances for acoustic
models was around 28,000 (47 hours of data). The acoustic
feature sequences were generated given the duration force-
aligned on the test set.

C. Ablation test on b-NSF

Although we claimed that an NSF model can be imple-
mented in varied network architectures, some of the compo-

6This new strategy slightly improved the aperiodicity of voiced sounds,
compared with the WaveNet in our previous study, which selected the most
probable waveform value at all voiced steps [13]

7Unlike naive open-source implementations, our WaveNet-vocoder avoids

redundant CONV operations as the so-called Fast-WaveNet did [59]

Fig. 8. Mean opinion scores (MOSs) of synthetic samples from b-NSF and
its variants given natural acoustic features. White dots denote mean MOSs.

Fig. 9. Natural and generated waveforms from NSF models in ablation test
(Section IV-C).

nents may be essential to model performance. This ablation
test was conducted to identify those essential components.

We used b-NSF as the reference model and prepared a few
variants, as listed in Table IV. All the models including b-NSF
were trained using the MGCs, F0, and waveforms from the 5-
hr. training set. The trained models then generated waveforms
given the natural acoustic features in the test set, and these
generated waveforms were evaluated in a subjective evaluation
test. In one evaluation round, an evaluator listened to one
speech waveform on one screen, rated the speech quality on
a 1-to-5 mean-opinion-score (MOS) scale, and repeated the
process for multiple screens. The waveforms in one evaluation
round were for the same text and were played in a random
order. All the waveforms were converted to 16-bit PCM format
in advance.

A total of 245 paid Japanese native speakers participated
in the test, and 1444 valid evaluation rounds were conducted.
The results are plotted in Figure 8, and the difference between
b-NSF and the other models was statistically signiﬁcant
(p < 0.01), as two-sided Mann-Whitney tests demonstrated.
First, a comparison made among b-NSF, L1, L2, and L3
showed that using spectral amplitude distances with different
to the
windowing and framing conﬁgurations is essential
model’s performance. The waveforms generated from L1, L2,

b-NSFL1L2L3S1S2N1N212345Quality(MOS)0Natural0b-NSF0L10L20L30S244000460004800050000520000N2WaveformamplitudeMANUSCRIPT

10

Fig. 10. MOSs of experimental systems under different training and test conditions. Error bars at conﬁdence level of 95% are plotted.

and L3 were perceptually worse because of a pulse-train-like
sound in both unvoiced and voiced sounds. This type of artifact
could be easily observed in the unvoiced segments plotted in
Figure 9. We hypothesize that using spectral distances with
different temporal-spatial resolutions could mitigate the pulse-
train-like artifacts in the spectrogram.

By comparing b-NSF, S1, and S2, we found that the sine-
based excitation is essential to the NSF models. In the case of
S2, the generated waveforms were intelligible but unnatural
because the perceived pitch was unstable. As Figure 9 shows,
the waveform generated from S2 lacked the periodic structure
that should be observed in voiced sounds. With a sine-based
excitation, the b-NSF model may have a better starting point to
generate waveforms with a periodic structure. This hypothesis
is supported by the results of the investigation discussed in
Section IV-F.

Interestingly, N1 outperformed b-NSF even though it used
a simpler transformation in the ﬁlter blocks. In comparison,
the waveforms generated from N2 were unnatural because they
lacked the stable periodic structures in voiced segments. One
possible reason is that the skip-connection enables the sine
excitation to be propagated to the later ﬁlter blocks without
being attenuated by the non-linear transformations.

In summary, the results of the ablation test suggest that both
the sine excitation and multi-resolution spectral distances are
crucial to NSF models. It is also important to keep the skip-
connections inside the ﬁlter modules.

D. Comparison between WaveNet and NSF models

1) Quality of generated waveforms: This experiment com-
pared b-NSF, s-NSF, hn-NSF, and WaveNet under four
training conditions. Each model was trained using either the
15-hr. or 1.6-hr. training set and conditioned on F0 and either
Mel-spectrogram or MGCs. In the testing stage, each model
generated speech waveforms given natural acoustic features
or generated features produced by the acoustic models. Ac-
cordingly, each model was trained and tested under eight
conditions. The generated speech waveforms were evaluated
in a subjective evaluation test, which was organized in the
same manner as that mentioned in Section IV-C.

The results are plotted in Figure 10. Among the three
NSF models, hn-NSF performed better than or comparably
the Mel-
well with the other two versions. Interestingly,
spectrogram-based s-NSF performed poorly when it was

TABLE V
MODEL COMPARISON IN TERMS OF TRAINING SPEED, GENERATION
SPEED, AND NUMBER OF MODEL PARAMETERS. TRAINING SPEED WAS
MEASURED ON 15-HR AND 1.6-HR TRAINING DATA SETS. TRAINING AND
GENERATION WERE CONDUCTED ON SINGLE NVIDIA P100 GPU CARD.

Training speed
(hr/epoch)

1.6-hr set 15-hr set mem-save
3.01
3.40
1.81
1.92

-
20 k
78 k
71 k

0.46
0.37
0.25
0.27

points in 1-s GPU time

No. of generated waveforms No. of
model
parameters
2.96e+6
1.83e+6
1.07e+6
1.20e+6

normal
0.19 k
227 k
343 k
335 k

Model
WaveNet
b-NSF
s-NSF
hn-NSF

trained using the 15-hr. set. One reason was that s-NSF pro-
duced low-quality unvoiced sounds. For example, the unvoiced
segments generated by s-NSF had a very small amplitude,
as shown in Figure 11. b-NSF performed well when it was
trained using MGCs and F0 from the 15-hr. training set, but its
performance was worse than hn-NSF and b-NSF when the
amount of training data was less than 2 hours. One hypothesis
is that b-NSF requires more training data since it has more
parameters, as Table V shows.

A comparison made between hn-NSF and WaveNet
shows that hn-NSF was comparable to WaveNet in terms
of the generated speech quality. Speciﬁcally,
in the TTS
application, hn-NSF trained on 15 hours of Mel-spectrogram
data slightly outperformed WaveNet and b-NSF trained on
15 hours of MGC data, which were the best performing models
in our previous study [20].

2) Training and generation speed: After the MOS test,
we compared the waveform training and generation speed
of the experimental models. Although the theoretical time
complexity is described in Table I, we measured the actual
speed of each model in training and generation stages.

Table V lists the time cost to train the experimental models
for one epoch. The training time cost on the 15-hr training set
was larger than that on the 1.6-hr set because the number of
training utterance increased. Nevertheless, the results indicate
that the NSF models are comparable to WaveNet in terms
of training speed. This is expected because all three NSF
models and WaveNet require no sequential transformation
of waveforms during model training.

For waveform generation, our implementation of the NSF
models has normal and memory-saving generation modes. The

NaturalWaveNethn-NSFb-NSFs-NSFWaveNethn-NSFb-NSFs-NSFWaveNethn-NSFb-NSFs-NSFWaveNethn-NSFb-NSFs-NSF12345Quality(MOS)Mel-spectrogram+F01.6-hr.trainingset15-hr.trainingsetMGC+F01.6-hr.trainingset15-hr.trainingsetCopy-synthesisTTSMANUSCRIPT

11

Fig. 11. Natural and generated waveforms and their spectrograms. Models were trained using 15 hours of natural Mel-spectrogram and F0.

normal mode allocates all of the required GPU memory once
but cannot generate very long waveforms due to the limited
memory space on a single GPU card. The memory-saving
mode supports the generation of long waveforms because
it releases and allocates the GPU memory layer by layer.
However, these memory operations cost processing time.

We evaluated the NSF models in both modes by using
a test subset with 80 test utterances, each of which was
around 5 seconds. As the results in Table V indicate, the NSF
models were much faster than WaveNet even in the memory-
save mode. Note that WaveNet requires no repeated memory
operation or memory-save mode. It is slow because of the
AR-generation process. Compared with b-NSF, s-NSF was
faster in generation because of its simpliﬁed network structure.
Although hn-NSF slightly lagged behind s-NSF in terms of
generation speed, it outperformed b-NSF.

In summary, the results of the MOS and speed tests indicate
that hn-NSF performed no worse than WaveNet in terms of
the quality of the generated waveforms. Furthermore, hn-NSF
outperformed WaveNet by a large margin in term of wave-
form generation speed8.

E. Consistency between input F0 and F0 in generated wave-
forms

The ablation test discussed in Section IV-C demonstrated
that sine excitation with the input F0 is essential to the NSF
models. In this experiment, we investigated the consistency
between the input F0 and the F0 of the waveforms generated
from the NSF models, especially when the F0 input to the
source module is not identical to the F0 in the input Mel-
spectrogram.

This experiment was conducted on hn-NSF and WaveNet,
which were trained using the natural Mel-spectrogram and F0
from the 15-hr. training set. The WaveNet was included as

8The generation speed in the memory-save mode may be further increased

if the GPU memory allocation can be accelerated.

TABLE VI
F0 CORRELATION BETWEEN F0 INPUT TO WAVEFORM MODELS AND F0
EXTRACTED FROM WAVEFORMS GENERATED BY H N-NSF AND WA V ENE T.
NOTE THAT H N-NSF AND WA V ENE T USED MEL-SPECTROGRAM AND F0
IN F0R1, F0R2, OR F0R3.

F0r1
F0r2
F0r3
F0 in Mel-spect.

hn-NSF
F0r2
0.930
0.986
0.929
0.907

F0r1
0.992
0.926
0.926
0.901

F0r3
0.921
0.919
0.988
0.900

WaveNet
F0r2
0.916
0.919
0.921
0.971

F0r3
0.914
0.917
0.920
0.973

F0r1
0.917
0.918
0.922
0.975

a reference model. Before generating the waveforms, we used
the deep AR F0 model to randomly generate three F0 contours
for each of the test set utterances [5]. In this random generation
mode, the three F0 contours for the same test utterance were
slightly different from each other [5]. Let F0r1, F0r2, and
F0r3 denote the three sets of F0 contours. We then used
the three F0 sets and the generated Mel-spectrogram as the
input to hn-NSF and WaveNet, which resulted in six sets of
generated waveforms.

For each of the six sets, we calculated the correlation
between the input F0 and the F0 extracted from the generated
waveforms. For reference, we also extracted the F0 from the
input Mel-spectrograms using a neural-network-based method
[60]. The results listed in Table VI indicate that the F0 contours
of the waveforms generated from the NSF models were highly
consistent with the F0 input to the source module. However,
the waveforms generated from WaveNet correlated with the
F0 information buried in the input Mel-spectrogram.

The results indicate that we can easily control the F0 of the
generated waveforms from the NSF models through directly
manipulating the input F0. In contrast, it is less straightforward
in the case of WaveNet because we have to manipulate the F0
contained in the input Mel-spectrogram.

Natural4k8kFrequency(Hz)WaveNethn-NSFb-NSFs-NSFWaveNethn-NSFb-NSFs-NSF(a)Usingnaturalacousticfeaturesforwaveformgeneration(b)UsingpredictedacousticfeaturesforwaveformgenerationMANUSCRIPT

12

Input, output, and hidden signals from hn-NSF given natural Mel-spectrogram and F0. From left to right: input excitation e1:T , outputs from 5

Fig. 12.
neural ﬁlter blocks for harmonic waveform component, ﬁltered harmonic and noise waveform components, and output waveform (cid:98)o1:T .

F. Investigation of hidden features of hn-NSF

We argued in Section III-C that the simpliﬁed neural ﬁlter
module in the s-NSF and hn-NSF models is similar to a
deep residual network. It is thus interesting to look inside
the neural ﬁlter module. From hn-NSF trained on 15 hours
of Mel-spectrogram and F0, we generated one test utterance
given natural condition data and extracted the one-dimensional
output of each simpliﬁed ﬁlter block (i.e., the vout
1:T in the
bottom panel of Figure 4) in the sub-network to generate the
harmonic waveform component. We also extracted the sine-
based excitation e1:T , ﬁltered harmonic and noise waveform
o1:T . These signals
components, and ﬁnal output waveform
and their spectrograms are plotted in Figure 12.

(cid:98)

We can observe that the dilated-CONV ﬁlter blocks mor-
phed the sine excitation into the waveform. The spectrogram
of the sine excitation had no formant structure but only the
fundamental frequency and harmonics. From blocks 1 to 5,
the spectrogram of the signal was gradually enriched with the
formant structure. The results also suggest that hn-NSF kept
the F0 of the sine excitation in the output waveform. This
explains why the F0 of the waveform generated from hn-NSF
was highly consistent with the frequency of the sine excitation,
or the input F0, the experiments of Section IV-E.

Similar results were observed when we analyzed s-NSF
and b-NSF. For b-NSF, the results are consistent with the
ablation test where we found that b-NSF without the skip-
connections in the ﬁlter module performed poorly (N2 in
Section IV-C). The skip-connections make the ﬁlter module
a deep residual network based on which the excitation signal
can be gradually transformed into the output waveform.

These results also indicate how the sine excitation eases the
task of waveform modeling because the neural ﬁlter modules
do not need to reproduce the periodic structure that evokes the
perception of F0 in voiced sounds. Without the sine excitation,
it may be difﬁcult for the neural ﬁlter modules to generate the
periodic structure, which explains the poor performance of the
b-NSF model without sine excitation (S2 in Section IV-C).

V. CONCLUSION

We proposed a framework called “neural source-ﬁlter mod-
eling” for the waveform models in TTS systems. A model
implemented in this framework, which is called an “NSF
model”, can convert input acoustic features into a high-quality
speech waveform. Compared with other neural waveform
models such as WaveNet, an NSF model does not use an AR
network structure and avoids the slow sequential waveform

generation process. Neither does an NSF model use ﬂow-based
approaches nor knowledge distilling. Instead, an NSF model
uses three modules that can be easily implemented: a source
module that produces a sine-based excitation signal, ﬁlter
module that transforms the excitation into an output waveform,
and condition module that processes the input features for
the source and ﬁlter modules. Such an NSF model can be
efﬁciently trained using a merged spectral amplitude distance.
Even though this distance is calculated using multiple short
time analysis conﬁgurations, it can be efﬁciently implemented
on the basis of STFT. Therefore, the proposed NSF framework
to be built and trained
allows a neural waveform model
straightforwardly.

Experimental results indicated that

the speciﬁc hn-NSF
model, which uses separate modules to model the harmonic
and noise components of waveforms, performed comparably
well to our WaveNet on a large single-speaker Japanese speech
corpus. Furthermore, this NSF model can generate speech
waveforms at a much faster speed. Another advantage of this
NSF model is that the F0 input to the source module allows
easy control on the pitch of the generated waveform.

In this primary study, we mainly described the NSF frame-
work in detail and compared several NSF models with our
veriﬁed WaveNet implementation. To further understand the
NSF models’ performance, we need a thorough comparison
between NSF models and other types of neural waveform
models on multi-speaker corpora. We leave this task for future
work because of the time required to implement and train other
models.

REFERENCES

[1] P. Taylor, Text-to-Speech Synthesis. Cambridge University Press, 2009.
[2] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, and K. Oura,
“Speech synthesis based on hidden Markov models,” Proceedings of the
IEEE, vol. 101, no. 5, pp. 1234–1252, 2013.

[3] H. Zen, K. Tokuda, and A. W. Black, “Statistical parametric speech
synthesis,” Speech Communication, vol. 51, pp. 1039–1064, 2009.
[4] K. Yao and G. Zweig, “Sequence-to-sequence neural net models for
grapheme-to-phoneme conversion,” in Proc. Interspeech, 2015, pp.
3330–3334.

[5] X. Wang, S. Takaki, and J. Yamagishi, “Autoregressive neural F0 model
for statistical parametric speech synthesis,” IEEE/ACM Transactions on
Audio, Speech, and Language Processing, vol. 26, no. 8, pp. 1406–1419,
2018.

[6] M. S. Ribeiro, O. Watts, and J. Yamagishi12, “Parallel and cascaded
deep neural networks for text-to-speech synthesis,” in Proc. SSW, 2016,
pp. 100–105.

[7] H. Zen, A. Senior, and M. Schuster, “Statistical parametric speech
synthesis using deep neural networks,” in Proc. ICASSP, 2013, pp. 7962–
7966.

e1:T4k8kFrequency(Hz)block1block2block3block4block5harmoniccomp.noisecomp.bo1:TMANUSCRIPT

13

[8] Y. Fan, Y. Qian, F. Xie, and F. K. Soong, “TTS synthesis with bidi-
rectional LSTM based recurrent neural networks,” in Proc. Interspeech,
2014, pp. 1964–1968.

[9] X. Wang, S. Takaki, and J. Yamagishi, “An autoregressive recurrent
mixture density network for parametric speech synthesis,” in Proc.
ICASSP, 2017, pp. 4895–4899.

[10] G. E. Henter, S. Ronanki, O. Watts, M. Wester, Z. Wu, and S. King,
“Robust TTS duration modelling using DNNs,” in Proc. ICASSP, 2016,
pp. 5130–5134.

[11] B. Chen, T. Bian, and K. Yu, “Discrete duration model for speech

synthesis,” in Proc. Interspeech, 2017, pp. 789–793.

[12] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,
A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “WaveNet:
A generative model for raw audio,” arXiv preprint arXiv:1609.03499,
2016.

[13] X. Wang, J. Lorenzo-Trueba, S. Takaki, L. Juvela, and J. Yamagishi,
“A comparison of recent waveform generation and acoustic modeling
methods for neural-network-based speech synthesis,” in Proc. ICASSP,
2018, pp. 4804–4808.

[14] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo,
A. Courville, and Y. Bengio, “SampleRNN: An unconditional end-to-
end neural audio generation model,” in Proc. ICLR, 2017.

[15] Y. Ai, H.-C. Wu, and Z.-H. Ling, “SampleRNN-based neural vocoder
IEEE,

for statistical parametric speech synthesis,” in Proc. ICASSP.
2018, pp. 5659–5663.

[16] D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and
M. Welling, “Improved variational inference with inverse autoregressive
ﬂow,” in Proc. NIPS, 2016, pp. 4743–4751.

[17] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A ﬂow-based
generative network for speech synthesis,” in Proc. ICASSP, 2019, pp.
3617–3621.

[18] A. van den Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals,
K. Kavukcuoglu, G. van den Driessche, E. Lockhart, L. Cobo, F. Stim-
berg, N. Casagrande, D. Grewe, S. Noury, S. Dieleman, E. Elsen,
N. Kalchbrenner, H. Zen, A. Graves, H. King, T. Walters, D. Belov, and
D. Hassabis, “Parallel WaveNet: Fast high-ﬁdelity speech synthesis,” in
Proc. ICML, 2018, pp. 3918–3926.

[19] W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave generation in

end-to-end text-to-speech,” in Proc. ICLP, 2019.

[20] X. Wang, S. Takaki, and J. Yamagishi, “Neural source-ﬁlter-based
waveform model for statistical parametric speech synthesis,” in Proc.
ICASSP, 2019, pp. 5916–5920.

[21] R. J. Williams and D. Zipser, “A learning algorithm for continually
running fully recurrent neural networks,” Neural computation, vol. 1,
no. 2, pp. 270–280, 1989.

[22] L. Juvela, V. Tsiaras, B. Bollepalli, M. Airaksinen, J. Yamagishi,
and P. Alku, “Speaker-independent raw waveform model for glottal
excitation,” in Proc. Interspeech 2018, 2018, pp. 2012–2016.

[23] J.-M. Valin and J. Skoglund, “LPCNet: Improving neural speech synthe-
sis through linear prediction,” in Proc. ICASSP, 2019, pp. 5891–5895.
[24] M.-J. Hwang, F. Soong, F. Xie, X. Wang, and H.-G. Kang, “LP-
WaveNet: Linear prediction-based WaveNet speech synthesis,” arXiv
preprint arXiv:1811.11913, 2018.

[25] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury, N. Casagrande,
E. Lockhart, F. Stimberg, A. van den Oord, S. Dieleman, and
K. Kavukcuoglu, “Efﬁcient neural audio synthesis,” in Proc. ICML,
vol. 80, 10–15 Jul 2018, pp. 2410–2419.

[26] Z. Jin, A. Finkelstein, G. J. Mysore, and J. Lu, “FFTNet: A real-time
IEEE, 2018, pp.

speaker-dependent neural vocoder,” in Proc. ICASSP.
2251–2255.

[27] T. Okamoto, K. Tachibana, T. Toda, Y. Shiga, and H. Kawai, “An
investigation of subband WaveNet vocoder covering entire audible
frequency range with limited acoustic features,” in Proc. ICASSP. IEEE,
2018, pp. 5654–5658.

[28] D. Rezende and S. Mohamed, “Variational inference with normalizing

ﬂows,” in Proc. ICML, 2015, pp. 1530–1538.

[29] T. Okamoto, T. Toda, Y. Shiga, and H. Kawai, “Investigations of real-
time neural vocoders with fundamental frequency and Mel-cepstra,” in
Proc. ASJ spring meeting, 2019, p. (in Japanese).

[30] D. Grifﬁn and J. Lim, “A new model-based speech analysis/synthesis

system,” in Proc. ICASSP, vol. 10.

IEEE, 1985, pp. 513–516.

[31] D. W. Grifﬁn and J. S. Lim, “Multiband excitation vocoder,” IEEE
Transactions on Acoustics, Speech, and Signal Processing, vol. 36, no. 8,
pp. 1223–1235, Aug 1988.

[32] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-time
Fourier transform,” IEEE Trans. ASSP, vol. 32, no. 2, pp. 236–243,
1984.

[33] D. D. Lee and H. S. Seung, “Algorithms for non-negative matrix

factorization,” in Proc. NIPS, 2001, pp. 556–562.

[34] S. Takaki, H. Kameoka, and J. Yamagishi, “Direct modeling of fre-
quency spectra and waveform generation based on phase recovery for
DNN-based speech synthesis,” in Proc. Interspeech, 2017, pp. 1128–
1132.

[35] D. B. Fry, A. S. Abramson, P. D. Eimas, and A. M. Liberman, “The
identiﬁcation and discrimination of synthetic vowels,” Language and
speech, vol. 5, no. 4, pp. 171–189, 1962.

[36] A. M. Liberman, F. S. Cooper, D. P. Shankweiler, and M. Studdert-
Kennedy, “Perception of the speech code,” Psychological review, vol. 74,
no. 6, p. 431, 1967.

[37] W. Strange, “Evolving theories of vowel perception,” The Journal of the

Acoustical Society of America, vol. 85, no. 5, pp. 2081–2087, 1989.

[38] I. Saratxaga, I. Hernaez, M. Pucher, E. Navas, and I. Sainz, “Percep-
tual importance of the phase related information in speech,” in Proc.
Interspeech, 2012.

[39] J. M. Hillenbrand and T. M. Nearey, “Identiﬁcation of resynthe-
sized/hvd/utterances: Effects of formant contour,” The Journal of the
Acoustical Society of America, vol. 105, no. 6, pp. 3509–3523, 1999.

[40] A. Graves, “Supervised Sequence Labelling with Recurrent Neural
Networks,” Ph.D. dissertation, Technische Universit¨at M¨unchen, 2008.
[41] X. Wang, S. Takaki, and J. Yamagishi, “Investigation of WaveNet for
text-to-speech synthesis,” SIG Technical Reports, Tech. Rep. 6, feb 2018.
[42] J. R. Carson and T. C. Fry, “Variable frequency electric circuit theory
with application to the theory of frequency-modulation,” Bell System
Technical Journal, vol. 16, no. 4, pp. 513–540, 1937.

[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image

recognition,” in Proc. CVPR, 2016, pp. 770–778.

[44] A. J. Abrantes, J. S. Marques, and I. M. Trancoso, “Hybrid sinusoidal
modeling of speech without voicing decision,” in Proc. Eurospeech,
1991, pp. 231–234.

[45] J. Laroche, Y. Stylianou, and E. Moulines, “HNS: Speech modiﬁcation
IEEE,

based on a harmonic+ noise model,” in Proc. ICASSP, vol. 2.
1993, pp. 550–553.

[46] Y. Stylianou, “Harmonic plus noise models for speech, combined
with statistical methods, for speech and speaker modiﬁcation,” Ph.D.
dissertation, Ecole Nationale Superieure des Telecommunications, 1996.
[47] J. Makhoul, R. Viswanathan, R. Schwartz, and A. Huggins, “A mixed-
source model for speech compression and synthesis,” The Journal of the
Acoustical Society of America, vol. 64, no. 6, pp. 1577–1581, 1978.

[48] A. V. McCree and T. P. Barnwell, “A mixed excitation LPC vocoder
model for low bit rate speech coding,” IEEE Transactions on Speech
and audio Processing, vol. 3, no. 4, pp. 242–250, 1995.

[49] T. Parks and J. McClellan, “Chebyshev approximation for nonrecursive
digital ﬁlters with linear phase,” IEEE Transactions on Circuit Theory,
vol. 19, no. 2, pp. 189–194, 1972.

[50] H. Kawai, T. Toda, J. Ni, M. Tsuzaki, and K. Tokuda, “XIMERA: A new
TTS from ATR based on corpus-based technologies,” in Proc. SSW5,
2004, pp. 179–184.

[51] L. Juvela, X. Wang, S. Takaki, S. Kim, M. Airaksinen, and J. Yamagishi,
“The NII speech synthesis entry for Blizzard Challenge 2016,” in Proc.
Blizzard Challenge Workshop, 2016.

[52] K. Tokuda, T. Kobayashi, T. Masuko, and S. Imai, “Mel-generalized
cepstral analysis a uniﬁed approach,” in Proc. ICSLP, 1994, pp. 1043–
1046.

[53] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A vocoder-based
high-quality speech synthesis system for real-time applications,” IEICE
Trans. on Information and Systems, vol. 99, no. 7, pp. 1877–1884, 2016.
[54] H.-T. Luong, X. Wang, J. Yamagishi, and N. Nishizawa, “Investigating
accuracy of pitch-accent annotations in neural-network-based speech
synthesis and denoising effects,” in Proc. Interspeech, 2018, pp. 37–
41.

[55] Y. Yasuda, X. Wang, S. Takaki, and J. Yamagishi, “Investigation of
enhanced Tacotron text-to-speech synthesis systems with self-attention
for pitch accent language,” in Proc. ICASSP.
IEEE, 2019, pp. 6905–
6909.

[56] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

in Proc. ICLR, 2014, p. unknown.

[57] F. Weninger, J. Bergmann, and B. Schuller, “Introducing CURRENT:
The Munich open-source CUDA recurrent neural network toolkit,” The
Journal of Machine Learning Research, vol. 16, no. 1, pp. 547–551,
2015.

[58] N. Bell and J. Hoberock, “Thrust: A productivity-oriented library for
Elsevier, 2011, pp.

CUDA,” in GPU computing gems Jade edition.
359–371.

MANUSCRIPT

14

[59] T. L. Paine, P. Khorrami, S. Chang, Y. Zhang, P. Ramachandran,
M. A. Hasegawa-Johnson, and T. S. Huang, “Fast WaveNet generation
algorithm,” arXiv preprint arXiv:1611.09482, 2016.

[60] L. Juvela, B. Bollepalli, X. Wang, H. Kameoka, M. Airaksinen, J. Yam-
agishi, and P. Alku, “Speech waveform synthesis from MFCC sequences
with generative adversarial networks,” in Proc. ICASSP.
IEEE, 2018,
pp. 5679–5683.

