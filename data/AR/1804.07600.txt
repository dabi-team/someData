Conditional Maximum Lq-Likelihood Estimation for Regression Model with Autoregressive Error 

Terms 

Y. Güney, Y. Tuaç, Ş. Özdemir and O. Arslan 

Ankara University, Faculty of Science, Department of Statistics, 06100 Ankara/Turkey 

Afyon Kocatepe University, Faculty of Science, Department of Statistics, Afyonkarahisar, Turkey 

ydone@ankara.edu.tr, ytuac@ankara.edu.tr, senayozdemir@aku.edu.tr, oarslan@ankara.edu.tr 

Abstract 

In  this  article,  we  consider  the  parameter  estimation  of  regression  model  with  pth  order  autoregressive 

(AR(p))  error  term.  We  use  the  Maximum  Lq-likelihood  (MLq)  estimation  method  that  is  proposed  by 

Ferrari  and  Yang  (2010a),  as  a  robust  alternative  to  the  classical  maximum  likelihood  (ML)  estimation 

method to handle the outliers in the data. After exploring the MLq estimators for the parameters of interest, 

we provide some asymptotic properties of the resulting MLq estimators. We give a simulation study and a 

real data example to illustrate the performance of the new estimators over the ML estimators and observe 

that the MLq estimators have superiority over the ML estimators when outliers are present in the data. 

Keywords: autoregressive stationary process; conditional maximum Lq-likelihood; linear regression.  

1. 

Introduction 

Incorporating the autoregressive error terms into the linear regression models is a useful way for analyzing 

the  relationships  between  economic  indicators  and  has  also  been  attracted  a  good  deal  interest  in  both 

statistical theory and applications. In literature, classical parameter estimation methods are generally used 

1 

 
 
 
 
to estimate the parameters of the regression model with autoregressive error terms (AR(p)). For instance, 

Cochrane  and  Orcutt  (1949)  have  considered  some  modification  of  the  ordinary  least  squares  (OLS) 

estimation method for autoregressive error terms regression model. Beach and Mackinnon (1978) have used 

maximum likelihood (ML) estimation method to estimate the parameters of AR(1) error term regression 

model. Alpuim and El-Shaarawi (2008) have estimated the parameters of the regression model with AR(p) 

error  term  using  the  OLS  estimation  method.  They  have  also  used  the  ML  estimation  and  conditional 

maximum  likelihood  (CML)  estimation  method  under  the  assumption  of  normality  and  studied  the 

asymptotic properties of the resulting estimators. Tuac et al. (2017) have considered linear regression model 

with AR(p) errors with Student’s t-distribution as a heavy tailed alternative to the normal distribution and 

used CML estimation method to obtain the model parameters. 

Under normality assumption, CML and OLS estimation methods are commonly used estimation procedures 

for  the  regression  model  with  autoregressive  error  terms.  Although  they  are  appropriate  choices  in 

estimating the parameters of regression with autoregressive errors, they are highly sensitive to outliers. To 

handle  this problem,  we  will  use  the  MLq  estimation  method  proposed by  Ferrari and  Yang  (2010a)  to 

estimate the parameters of the AR(p) error term regression model.  

MLq estimation method have recently proposed and have gained considerable attention in the past decade. 

For example, Ferrari and Paterlini (2009) have investigated the behavior of the MLq estimation on both 

simulated data and on real-world time series for extreme quantile estimation. Ferrari and Paterlini (2010b) 

have applied the MLq estimation to expected return and volatility estimation of financial asset returns under 

multivariate  normality.  Huang,  Lin  and  Ren  (2013)  have  proposed  a  generalized  form  of  the  classical 

likelihood ratio statistic by using the Lq -likelihood ratio (LqR) statistic based on the MLq estimation for 

hypothesis testing problem for the shape parameter of the GEV distribution and showed that the asymptotic 

behavior of proposed statistic characterize by the degree of tuning parameter q. Qin and Priebe (2013)  have 

proposed a new EM algorithm namely an expectation-maximization algorithm with Lq-likelihood (EMLq) 

which addresses MLq estimation within the EM framework for mixture models. Qin and Priepe (2016) have 

2 

 
introduced a robust hypothesis testing procedure: the Lq-likelihood-ratio-type test (LqRT). By deriving the 

asymptotic distribution of this test statistic, the authors have demonstrated its robustness both analytically 

and numerically, and they investigated the properties of both its influence function and its breakdown point. 

Also Ozdemir et al. (2019) use the MLq estimation method to estimate the parameters of Marshall-Olkin 

extended Burr XII distribution and show that MLq estimation method outperform the ML. Recently, Dogru 

et  al.  (2018)  propose  parameter  estimation  of  the  multivariate  t  distribution  using  the  MLq  estimation, 

provide that unlike the ML estimation the degrees of freedom parameters can be estimated along with the 

other parameters, and still gain the robustness. 

In  this  paper,  we  consider  the  conditional  maximum  Lq-likelihood  (CMLq)  estimation  method  for  the 

autoregressive  error  terms  regression  models  under  normality  assumption.  We  obtain  the  parameter 

estimation for all the parameters. We give an extensive simulation study to compare the performances of 

the CML and the CMLq estimation methods. The performance of the two sets of estimators is evaluated for 

different data structure including outlier cases. The simulation results show that the CMLq estimators can 

reduce the effects of the outliers if the tuning parameter 𝑞 is less than one. Note that q is a key parameter to 

maintain the robustness in MLq estimation method. It is considered as a robustness tuning parameter and 

choosing it very important. In this paper we choose q that minimizes robust AIC. Further we provide the 

asymptotic  distribution  of the  proposed  estimator  and  use  the  asymptotic covariance  matrix  to form  the 

asymptotic confidence intervals for the parameters. 

The rest of the paper is organized as follows. In the next section, we briefly describe the regression models 

with AR(p) error terms and the CML estimation method for the parameters of interest. A brief description 

of the MLq estimation method is summarized in Section 3. Then we obtain the CMLq estimators for the 

parameters of the regression models with AR(p) error terms in Section 4. Since the estimators cannot be 

obtained in explicit form the iteratively reweighted algorithm steps given in Section 5. Section 6 presents 

the asymptotic normality of the proposed estimators. In Section 7, Monte Carlo simulation study and a real 

data example are presented to compare the performance of CMLq and the CML estimation methods in terms 

3 

 
of RMSE at different data structure scenarios including outliers. Finally, some concluding remarks are given 

in Section 8. 

2. Regression Models with Autoregressive Error 

Consider the linear regression model with the error terms follow a stationary AR(p) process 

𝑀
𝑦𝑡 = ∑ 𝑥𝑡,𝑖
𝑖=1

𝛽𝑖 + 𝑒𝑡   ,

                                                                                                                           (1) 

𝑒𝑡 = 𝜙1𝑒𝑡−1 + ⋯ + 𝜙𝑝𝑒𝑡−𝑝 + 𝑎𝑡,        𝑡 = 1,2, … , 𝑁                                                                     (2) 

where, 𝑦𝑡  is the response variable,  𝑥𝑡,𝑖 are explanatory variables,  𝛽𝑖 are unknown regression parameters 

and 𝜙𝑗 are unknown autoregressive parameters.  The 𝑎𝑡 is normally and independently distributed such that 

𝐸(𝑎𝑡) = 0 and  𝑉𝑎𝑟(𝑎𝑡) = 𝜎2. 

To simplify the notation, we denote 𝑎𝑡 = Φ(𝐵)𝑒𝑡  and here, 𝐵 is called the Backshift operator.  By doing 

so, an alternative form of the model (1) is  

Φ(𝐵)𝑦𝑡 = ∑ 𝛽𝑖

𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖 + 𝑎𝑡,    𝑡 = 𝑝 + 1, 𝑝 + 2, … , 𝑁,                                                         (3) 

where  

Φ(𝐵)𝑦𝑡 = 𝑦𝑡 − 𝜙1𝑦𝑡−1 − ⋯ − 𝜙𝑝𝑦𝑡−𝑝,                                                                                                    (4)

Φ(𝐵)𝑥𝑡,𝑖 = 𝑥𝑡,𝑖 − 𝜙1𝑥𝑡−1,𝑖 − ⋯ − 𝜙𝑝𝑥𝑡−𝑝,𝑖                                                                                             (5)                                                                      

𝜙𝑗 are the AR(p) model parameters for 𝑗 = 1,2, … , 𝑝.  

Since  the  exact  likelihood  function  could  be  well  approximated  by  the  conditional  likelihood  function 

(Ansley, 1979) we will first give the CML estimation method which are used mainly in cases where ML 

estimates are difficult to compute.  

4 

 
 
 
 
2.1 Parameter Estimation of AR(p) Error Terms Regression Model with CML    

Under  the  assumptions  associated  with  equation  (3)  the  conditional  log-likelihood  function  will  be  as 

follows (Alpuim and El-Shaarawi, 2008). 

𝑙𝑛𝐿 = 𝑐 −

𝑁
2

𝑙𝑛𝜎2 −

1
2𝜎2 ∑  

𝑡=𝑝+1 

𝑁

𝑀

2

(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

Φ(𝐵)𝑥𝑡,𝑖)

                                                       (6) 

𝑖=1

To obtain the estimating equations, the derivatives of the conditional log-likelihood function with respect 

to unknown parameters are taken and set to zero. The derivatives of the conditional log-likelihood function 

are given in Appendix A. 

Rearranging the estimating equations, we get the following forms of estimators. 

𝑁

−1

𝑁

𝛽̂ = [ ∑ Φ̂ (𝐵)𝑥𝑡Φ̂ (𝐵)𝑥𝑡
𝑇

]

𝑡=𝑝+1

[ ∑ Φ̂ (𝐵)𝑦𝑡Φ̂ (𝐵)𝑥𝑡
𝑡=𝑝+1

]                                                                   (7) 

𝜙̂ = 𝑹−1 (𝛽̂) 𝑅0 (𝛽̂)                                                                                                                                   (8) 

𝜎̂2 =

1
𝑁 − 𝑝

𝑁
2
∑ (Φ̂ (𝐵)𝑦𝑡 − 𝛽̂Φ̂ (𝐵)𝑥𝑡)
𝑡=𝑝+1

                                                                                           (9)

where 

𝑅0 (𝛽) =

𝑁
∑ 𝑒𝑡𝑒𝑡−1
𝑡=𝑝+1
𝑁
∑ 𝑒𝑡𝑒𝑡−2
𝑡=𝑝+1

⋮

𝑁
∑ 𝑒𝑡𝑒𝑡−𝑝
[
𝑡=𝑝+1

]

, 𝑹 (𝛽) =

2

𝑁
∑ 𝑒𝑡−1
𝑡=𝑝+1
𝑁
∑ 𝑒𝑡−2𝑒𝑡−1
𝑡=𝑝+1

⋮

𝑁
∑ 𝑒𝑡−𝑝𝑒𝑡−1
[
𝑡=𝑝+1

5 

𝑁
∑ 𝑒𝑡−1𝑒𝑡−2
𝑡=𝑝+1
𝑁
∑ 𝑒𝑡−2
𝑡=𝑝+1
⋮

2

𝑁
∑ 𝑒𝑡−𝑝𝑒𝑡−2
𝑡=𝑝+1

…
…

𝑁
∑ 𝑒𝑡−1𝑒𝑡−𝑝
𝑡=𝑝+1
𝑁
∑ 𝑒𝑡−2𝑒𝑡−𝑝
𝑡=𝑝+1

⋱

…

⋮

𝑁
∑ 𝑒𝑡−𝑝
𝑡=𝑝+1

2

      (10) 

]

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
and Φ̂ (𝐵) is the backshift operator with the estimates of  𝜙𝑗. 

Equations (7) and (9) can be written in vector form as follows 

𝛽̂ = [𝚽̂ (𝑩)𝑿𝑻𝚽̂ (𝑩)𝑿]

−1

[𝚽̂ (𝑩)𝑿𝑻Φ̂ (𝐵)𝑌]                                                                                           (11) 

[Φ̂ (𝐵)𝑌 − 𝚽̂ (𝑩)𝑿𝛽̂]𝑇[Φ̂ (𝐵)𝑌 − 𝚽̂ (𝑩)𝑿𝛽̂]                                                                   (12) 

𝜎̂2 =

1
𝑁 − 𝑝

where 

𝚽̂ (𝑩)𝑿 = [𝚽̂ (𝑩)𝑥𝑡,𝑖], 

𝚽̂ (𝑩)𝒀 = [𝚽̂ (𝑩)𝑦𝑡]. 

These vector forms are important to implement the IRA algorithm to compute the estimates and will be our 

updating equations.  

3.  Maximum Lq Likelihood Estimation Method    

Ferrari and Yang (2010a) introduced the MLq estimation method based on q entropy and defined as follows. 

Suppose  𝑥  = (𝑥1, 𝑥2, … , 𝑥𝑛)  be  a  random  sample  from  a  distribution  with  probability  density  function 

𝑓(𝑥; 𝜃) with 𝜃 ∈ Θ.  The MLq estimator of 𝜃 is defined as  

𝜃̃𝑀𝐿𝑞𝐸 = argmax

𝜃∈Θ

𝑛
∑
𝑖=1 

𝐿𝑞(𝑓(𝑥𝑖; 𝜃)) 

 , 𝑞 > 0                                                                 (13) 

where 𝐿𝑞 ∶   (0, ∞) → ℝ  called non-extensive entropy or q-order entropy is defined by 

𝐿𝑞(𝑢) = {

𝑙𝑜𝑔𝑢
𝑢1−𝑞 − 1
⁄
1 − 𝑞

,

𝑞 = 1

, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒

(Havrda and Charvát, 1967 and Tsallis, 1988). 

6 

 
 
 
Define  

𝑈∗(𝑥; 𝜃, 𝑞) = ∇𝜃 𝐿𝑞(𝑓(𝑥; 𝜃)) = 𝑈(𝑥; 𝜃)𝑓(𝑥; 𝜃)1−𝑞. 

where  𝑈(𝑥; 𝜃) = ∇𝜃 log 𝑓(𝑥; 𝜃).  ∇  denotes derivative operator. Then Lq-likelihood equations have the 

form  

𝑛
∑ 𝑈∗(𝑥𝑖; 𝜃, 𝑞)
𝑖=1

𝑛

= ∑ 𝑈(𝑥𝑖; 𝜃)𝑓(𝑥𝑖; 𝜃)1−𝑞

= 0.                                                                                  (14) 

𝑖=1

The MLq estimation method can be regarded as a generalization of the ML estimation method. In particular, 

when  𝑞 is equal to 1, MLq and ML estimation methods are equal. It is also easy to see that when → 1, 

𝐿𝑞(𝑢) → log 𝑢. In this sense the MLq approaches the ML. Specially, MLq estimator belongs to the class of 

M-estimators  (Hampel,  et  al.  1986,  Huber  et  al.  2009,  Maronna  et.  al.  2006),  as  q  is  fixed  (Ferrari  and 

Paterlini, 2009).  MLq estimator also minimizes power divergences. The family of power divergences has 

various special cases for specific values of q (For more details, see Ferrari and Paterlini, 2009).  For instance, 

for q = 1/2 the MLq estimator is minimum Hellinger distance estimator. 

Classical weighted likelihood approach has been developed to deal with the disadvantages of maximum 

likelihood approach by modifying the role of observations by means of the weights. The MLq likelihood 

score functions are the same as the weighted likelihood score functions. That is why the MLq estimation 

can be seen as weighted likelihood estimators. However, it is important to note that the MLq estimator isn’t 

obtained with the idea of weighting the likelihood function. The difference from most weighted likelihood 

estimators is that the weights are a function of pdf.  The weight function is also based on tuning parameter 

q.  Therefore,  choices  q  is  the  key  for  this  approach.  MLq  estimation  method  give  low  or  high  weights 

depending on q < 1 or q > 1 to the extreme observations, by doing so it reduces the effects of outliers. In 

particular, all the observations have the same weight when q=1. Taking the q value slightly different makes 

it possible to obtain better estimates in terms of balance between bias and variance when the sample size is 

small. MLq estimation method balances two apparently contrasting needs: efficiency and robustness by a 

7 

 
proper choice of q. It provides strong robustness at expense of a slightly reduced efficiency in presence of 

outliers. Specially, the effect of extreme observations is reduced by taking q<1. On the contrary, when q>1, 

the effect of the observations corresponding to density values close to zero is accentuated (Ferrari and Yang, 

2007). A detailed discussion of the role of the tuning parameter q see Ferrari and Yang (2007). 

4. 

Parameter Estimation of AR(p) Error Terms Regression Model with CMLq  

Let’s (𝑥𝑖, 𝑦𝑖), 𝑖 =  1, 2, … , 𝑛, be a random sample from model given in (3) with the assumption that 𝑎𝑡 is 

normal and 𝜃′ = ( 𝛽1, 𝛽2, … , 𝛽𝑀, 𝜙1, 𝜙2, … , 𝜙𝑝, 𝜎2) be the parameter vector of the model. Then the CMLq 

estimator of 𝜃 is defined as 

𝜃̃𝑀𝐿𝑞𝐸 = argmax

𝜃∈Θ

𝑁
∑ 𝐿𝑞(𝑓(𝑎𝑡|𝑎1, 𝑎2, … , 𝑎𝑝, 𝜃).
𝑡=𝑝+1 

                                                                     (15) 

Define  

𝑈(𝑎𝑡; 𝜃) = ∇𝜃 log 𝑓(𝑎𝑡; 𝜃) ,  

𝑈∗(𝑎𝑡; 𝜃, 𝑞) = 𝑈(𝑎𝑡; 𝜃)𝑓(𝑎𝑡; 𝜃)

1−𝑞

 , 𝑡 = 𝑝 + 1, … , 𝑁. 

Then Lq-likelihood equations are 

∑

𝑁
𝑡=𝑝+1

𝑈∗(𝑎𝑡; 𝜃, 𝑞)

= ∑

𝑁
𝑡=𝑝+1

𝑈(𝑎𝑡; 𝜃)𝑓(𝑎𝑡; 𝜃)

1−𝑞

= 0.                                                 (16)                        

The elements of 𝑈(𝑎𝑡; 𝜃) and 𝑈∗(𝑎𝑡; 𝜃, 𝑞) are given in Appendix A and Appendix B, respectively.  

The CMLq estimators of the parameters are the solutions of (16). From these equations we get the following 

forms of estimators. Provided that [∑

𝑁
𝑡=𝑝+1

𝜔𝑡(𝜈)Φ̂ (𝐵)𝑥𝑡Φ̂ (𝐵)𝑥𝑡
𝑇

]

−1

 and 𝑹𝝎

−1(𝛽̂) exist. 

8 

 
 
 
𝑁

−1

𝑁

𝛽̃ = [ ∑ 𝜔𝑡Φ̃ (𝐵)𝑥𝑡Φ̃ (𝐵)𝑥𝑡
𝑇

]

𝑡=𝑝+1

[ ∑ 𝜔𝑡Φ̃ (𝐵)𝑦𝑡Φ̃ (𝐵)𝑥𝑡
𝑡=𝑝+1

],                                                        (17) 

𝜙̃ = 𝑹𝝎

−1 (𝛽̃) 𝑅𝜔0 (𝛽̃),                                                                                                                             (18) 

𝜎̃ 2 =

1

∑

𝑁
𝑡=𝑝+1

𝜔𝑡

𝑁
∑ 𝜔𝑡 (Φ̃ (𝐵)𝑦𝑡 − Φ̃ (𝐵)𝑥𝑡
𝑡=𝑝+1

2
𝑇𝛽̃)

                                                                             (19)

where 

𝑅𝜔0 (𝛽) =

𝑁
∑ 𝜔𝑡𝑒𝑡𝑒𝑡−1
𝑡=𝑝+1
𝑁
∑ 𝜔𝑡𝑒𝑡𝑒𝑡−2
𝑡=𝑝+1

⋮

𝑁
∑ 𝜔𝑡𝑒𝑡𝑒𝑡−𝑝
[
𝑡=𝑝+1

, 

]

𝑹𝝎 (𝛽) =

2

𝑁
∑ 𝜔𝑡𝑒𝑡−1
𝑡=𝑝+1
𝑁
∑ 𝜔𝑡𝑒𝑡−2𝑒𝑡−1
𝑡=𝑝+1

⋮

𝑁
∑ 𝜔𝑡𝑒𝑡−𝑝𝑒𝑡−1
[
𝑡=𝑝+1

𝑁
∑ 𝜔𝑡𝑒𝑡−1𝑒𝑡−2
𝑡=𝑝+1
𝑁
∑ 𝜔𝑡𝑒𝑡−2
𝑡=𝑝+1

2

⋮

𝑁
∑ 𝜔𝑡𝑒𝑡−𝑝𝑒𝑡−2
𝑡=𝑝+1

…
…

𝑁
∑ 𝜔𝑡𝑒𝑡−1𝑒𝑡−𝑝
𝑡=𝑝+1
𝑁
∑ 𝜔𝑡𝑒𝑡−2𝑒𝑡−𝑝
𝑡=𝑝+1

⋱

…

⋮

𝑁
∑ 𝜔𝑡𝑒𝑡−𝑝
𝑡=𝑝+1

2

]

We can obtain the following vector forms related the estimators 𝛽̂ and 𝜎̂2  

𝛽̃ = [𝚽̃ (𝑩)𝑿𝑻𝑾𝚽̃ (𝑩)𝑿]

−1

[𝚽̃ (𝑩)𝑿𝑻𝑾Φ̃ (𝐵)𝑌],                                                                                (20) 

𝜎̃ 2 =

1
𝑡𝑟𝑎𝑐𝑒(𝑾) 

[Φ̃ (𝐵)𝑌 − 𝚽̃ (𝑩)𝑿𝛽̃]

𝑇

𝑾 [Φ̃ (𝐵)𝑌 − 𝚽̃ (𝑩)𝑿𝛽̃],                                                  (21) 

9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
where 

𝚽̃ (𝑩)𝑿 = [Φ̃ (𝐵)𝑥𝑡,𝑖]𝑡=𝑝+1,…,𝑁
𝑖=1,…,𝑀

, 

Φ̃ (𝐵)𝑌 = [Φ̃ (𝐵)𝑦𝑡]

𝑡=𝑝+1,…,𝑁

, 

𝑾 = 𝑑𝑖𝑎𝑔{𝜔𝑡}𝑡=𝑝+1,…,𝑁. 

From equations (17)-(19), CMLq estimators can be viewed as a weighted version of the CML estimators 

given in (7)-(9). Here the weights are proportional to the (1- q)th power of the normal distribution density. 

If 𝜔𝑡 = 1 then (17)-(19) gives the CML estimates of the parameters.  Since the weight function 𝜔𝑡 is a 

decreasing function of  (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

2
𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)

/𝜎2 as q<1, the observations with larger residuals 

receive small weights. When q approaches to zero, the weights get smaller. Thus, the weight function down-

weights the effect of the outliers on the estimation procedure.  The tuning parameter q balances the efficiency 

and the robustness of the estimator. When q gets closer to one the resulting estimators get closer to the CML 

estimators. On the other hand, smaller values of q produce estimators that less sensitive to the outliers but 

not as efficient as CML. 

Concerning the explicit forms of the estimators, we observe that the estimators given in (17) - (19) depend 

on the weights and the weights are also function of the estimators. Therefore, explicit solutions cannot be 

obtained from this system of equations. It is also clear from equations (17) - (19) that for the fixed value of 

q, the estimation problem can be solved in terms of a weighting process. In this study, we use the iteratively 

re-weighted algorithm (IRA) to solve this problem. 

10 

 
 
 
 
 
5.  Iteratively Reweighted Algorithm to Compute the Estimates 

Let 𝑚 ∈ {0,1,2, … } denotes the iteration step. 

(i)  Set the initial values 𝛽(0), 𝜙(0) 𝑎𝑛𝑑 𝜎2(0)

  and fix a stopping rule (𝜀).  

(ii)  Calculate the following weight function for  𝑚 = 0,1,2 …      

(𝑚) = (

𝜔𝑡

1
√2𝜋𝜎2(𝑚)

𝑒𝑥𝑝 {−

(Φ(𝑚)(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

(𝑚)

𝑀
𝑖=1
2𝜎2(𝑚)

2
Φ(𝑚)(𝐵)𝑥𝑡,𝑖)

1−𝑞

} )

(iii) 

 Calculate 𝑹𝝎 (𝛽(𝑚)) and 𝑅𝜔0 (𝛽(𝑚))  

𝜙(𝑚+1) = 𝑅𝜔

−1 (𝛽(𝑚)) 𝑅𝜔0 (𝛽(𝑚))                                                                                                         

(iv)  Using 𝜔𝑡

(𝑚) and 𝜙(𝑚+1) calculate   

𝛽(𝑚+1) = [𝚽(𝒎+𝟏)(𝑩)𝑿𝑻𝑾(𝒎)𝚽(𝒎+𝟏)(𝑩)𝑿]

−1

[𝚽(𝒎+𝟏)(𝑩)𝑿𝑻𝑾(𝒎)Φ(𝑚+1)(𝐵)𝑌]           

where 𝑾(𝒎) = diag{𝜔𝑡

𝒏
(𝑚)}

. 

𝒕=𝒑+𝟏

(v)  Using  𝑾(𝒎) , 𝜙(𝑚+1) and 𝛽(𝑚+1) calculate  

(𝜎2)(𝑚+1) = 𝑞

1
𝑡𝑟𝑎𝑐𝑒(𝑾(𝒎))

[Φ(𝑚+1)(𝐵)𝑌 − 𝚽(𝒎+𝟏)(𝑩)𝑿𝛽(𝑚+1)]

𝑇

𝑾(𝒎) [Φ(𝑚+1)(𝐵)𝑌 − 𝚽(𝒎+𝟏)(𝑩)𝑿𝛽(𝑚+1)] 

(vi) 

If    ‖𝛽(𝑚+1) − 𝛽(𝑚)‖ < 𝜀,  ‖𝜙(𝑚+1) − 𝜙(𝑚)‖ < 𝜀  and  |(𝜎2)(𝑚+1) − (𝜎2)(𝑚)| < 𝜀  then 

stop, else repeat the steps (ii-vi) until the convergence condition is satisfied. 

11 

 
 
  
                        
 
 
 
 
 
6.  Asymptotic Distribution of CMLq Estimator and Asymptotic Confidence Interval 

In  this  section,  the  asymptotic  covariance  matrix  of  the  CMLq  estimator  of  the  parameters  of  the 

autoregressive  error  terms  regression  model  under  the  assumptions  given  in  Section  2  is  obtained  to 

construct the asymptotic confidence intervals for the parameters of interest. 

Ferrari and Yang (2010a), provide the asymptotic distribution of MLq estimator for the parameters of any 

∗ 
exponential family of distributions under some assumptions. We will briefly describe their results. Let 𝜃𝑛

be the value such that 

𝐸𝜃0(𝑈∗(𝜃𝑛

∗; 𝑋, 𝑞𝑛)) = 0. 

They state that  𝜃𝑛

∗ =

𝜃
𝑞𝑛

  where 𝜃 is the true parameter. They call 𝜃𝑛

∗ the surrogate parameter of 𝜃. As they 

point out, since the actual target of 𝜃̃𝑛 given in (13) is 𝜃𝑛

∗, 𝑞𝑛 must converge to one to obtain the asymptotic 

unbiasedness of 𝜃̃𝑛. Therefore, under the following conditions they show that Lq-likelihood equation has a 

solution, it is unique and maximizes the Lq-likelihood function in the parameter space. 

A1. 𝑞𝑛 > 0 is a monotone sequence such that 𝑞𝑛 → 1 as 𝑛 → ∞. 

A2. The parameter space Θ is compact and the parameter 𝜃 is an interior point in Θ. 

Further, with these assumptions the asymptotic distribution of MLq estimator is   

√n𝐕q

−1/2(𝜃̃𝑛 − 𝜃𝑛

∗) → Np(0, 𝐈p), 

where 𝐈p is the (𝑝 × 𝑝) identity matrix and  

𝑽𝑞(𝜃𝑛

∗) = 𝑱(𝜃𝑛

∗)−1𝑲(𝜃𝑛

∗)𝑱(𝜃𝑛

∗)−1 

𝑱(𝜃𝑛

∗) = 𝐸 (∇𝜃𝑈∗(𝜃𝑛

∗; 𝑋, 𝑞𝑛)) 

12 

 
 
 
 
 
𝑲(𝜃𝑛

∗) = 𝐸 (𝑈∗(𝜃𝑛

∗; 𝑋, 𝑞𝑛)′𝑈∗(𝜃𝑛

∗; 𝑋, 𝑞𝑛)). 

Here 

𝑈∗(𝜃𝑛

∗; 𝑋, 𝑞𝑛) = 𝑈(𝜃𝑛

∗; 𝑋)𝑓(𝑋, 𝜃𝑛

∗)1−𝑞𝑛 

is 

the 

first 

and 

∇𝜃𝑈∗(𝜃𝑛

∗; 𝑋, 𝑞𝑛) =

∇𝜃(𝑈(𝜃𝑛

∗; 𝑋)𝑓(𝑋, 𝜃𝑛

∗)1−𝑞𝑛) is the second partial derivatives of Lq-likelihood function. 

Concerning the autoregressive error terms regression model under the assumptions given in Section 2 let 

𝜃(0) = ( 𝛽1

(0), 𝛽2

(0), … , 𝛽𝑀

(0), 𝜙1

(0), 𝜙2

(0), … , 𝜙𝑝

(0), 𝜎(0)2

) be the real parameter vector.  Further, let 𝑈(𝜃; 𝑎𝑡) 

be the score vector. It is known that for all 𝜃 = ( 𝛽1, 𝛽2, … , 𝛽𝑀, 𝜙1, 𝜙2, … , 𝜙𝑝, 𝜎2) ∈ Θ ,  𝐸𝜃[𝑈(𝜃; 𝑎𝑡)] = 0, 

namely, 

∫ 𝑈(𝜃; 𝑎𝑡)𝑓(𝑎𝑡, 𝜃)𝑑(𝑎𝑡) = 0.                                                               (22) 

The modified score vector is such that 𝑈∗(𝜃, 𝑎𝑡, 𝑞𝑛) = 𝑈(𝜃; 𝑎𝑡)𝑓(𝑎𝑡, 𝜃)

1−𝑞𝑛 and so 

𝐸𝜃(0)[𝑈∗(𝜃; 𝑎𝑡, 𝑞𝑛)] = ∫ 𝑈(𝜃; 𝑎𝑡)𝑓(𝑎𝑡, 𝜃)

1−𝑞𝑛𝑓(𝑎𝑡, 𝜃(0))𝑑(𝑎𝑡). 

𝑒𝑥𝑝(

(0)2

−1

2𝜎(0)2𝑎𝑡
−𝑞𝑛
2)
2𝜎2 𝑎𝑡

𝑒𝑥𝑝(

)

𝑑(𝑎𝑡)                     (23) 

By rearranging the function to be integrate, we obtain 

𝐸𝜃(0)[𝑈∗(𝜃; 𝑎𝑡, 𝑞𝑛)] =

𝑞𝑛−1

2 𝜎𝑞

(2𝜋)

𝜎(0)

∫ 𝑈(𝜃; 𝑎𝑡)𝑓(𝑎𝑡, 𝜃)

where 𝑎𝑡

(0) = 𝑎𝑡|𝜃=𝜃(0).  

13 

 
 
 
 
 
 
 
 
 
 
Define 𝜃𝑛

∗ = (𝛽1

(0), 𝛽2

(0), … , 𝛽𝑀

(0), 𝜙1

(0), 𝜙2

(0), … , 𝜙𝑝

(0), 𝑞𝑛𝜎(0)2

 ) and substitute  𝜃𝑛

∗ with  𝜃 in equation (23), 

then we obtain  

𝐸𝜃(0)[𝑈∗(𝜃𝑛

∗; 𝑎𝑡, 𝑞𝑛)] =

𝑞𝑛−1

(2𝜋)

2 (𝑞𝑛𝜎(0))𝑞𝑛

𝜎(0)

∫ 𝑈(𝜃𝑛

∗)
∗; 𝑎𝑡)𝑓(𝑎𝑡, 𝜃𝑛

(0)2

)

−1

𝑒𝑥𝑝(

2𝜎(0)2𝑎𝑡
−𝑞𝑛
2(𝑞𝑛𝜎(0)2

(𝑎𝑡|𝜃=𝜃∗)
)

𝑒𝑥𝑝(

𝑑(𝑎𝑡).               (24) 

2

)

Since the parameters which are related to the location in 𝜃(0)  and 𝜃𝑛

∗ are equal to each other, 𝑎𝑡|𝜃=𝜃∗ =

𝑎𝑡|𝜃=𝜃(0) = 𝑎𝑡

(0).     Therefore, following equation is provided 

(0)2

)

𝑒𝑥𝑝 (

−1
2𝜎(0)2 𝑎𝑡
−𝑞𝑛
2(𝑞𝑛𝜎(0)2

)

𝑒𝑥𝑝 (

2
(𝑎𝑡|𝜃=𝜃∗)

=

)

𝑒𝑥𝑝 (

(0)2

)

𝑒𝑥𝑝 (

−1
2𝜎(0)2 𝑎𝑡
−𝑞𝑛
2(𝑞𝑛𝜎(0)2

)

= 1. 

(0)2
𝑎𝑡

)

Then the equation (24) can rewritten as follows. 

𝐸𝜃(0)[𝑈∗(𝜃𝑛

∗; 𝑎𝑡, 𝑞𝑛)] =

(2𝜋)

𝑞𝑛−1

2 (𝑞𝑛𝜎(0))𝑞𝑛
𝜎(0)

∫ 𝑈(𝜃𝑛

∗; 𝑎𝑡)𝑓(𝑎𝑡, 𝜃𝑛

∗)𝑑(𝑎𝑡) 

Knowing that the equation (22) is valid for all 𝜃 ∈ Θ, we obtain 

∫ 𝑈(𝜃𝑛

∗; 𝑎𝑡)𝑓(𝑎𝑡, 𝜃𝑛

∗)𝑑(𝑎𝑡) = 0 

and 

Finally, 

for 

our 

problem 

the 

surrogate 

parameter 

will 

be 

∗ =

𝜃𝑛

𝐸𝜃(0)[𝑈∗(𝜃𝑛

∗; 𝑎𝑡, 𝑞𝑛)] = 0. 

( 𝛽1

(0), 𝛽2

(0), … , 𝛽𝑀

(0), 𝜙1

(0), 𝜙2

(0), … , 𝜙𝑝

(0), 𝑞𝑛𝜎(0)2

) .  Notice that only the scale parameter 𝜎 is different from 

the true parameter value  𝜎(0) , it is depend on the tuning parameter 𝑞𝑛. For this reason, CMLq estimator is 

14 

 
 
 
 
 
expected to make substantial improvements in the prediction of 𝜎 without much improvement on location 

parameters.  

Also note that, Cavalieri (2002) has shown the same procedure for measurement error  models  to determine 

the surrogate parameter.  

Remark.  The considered model contains m+p+1 parameter which are 𝛽𝑖, 𝑖 = 1,2, … , 𝑀 and 𝜙𝑗,

𝑗 = 1,2, … , 𝑝  are  the  model  parameter  which  are  related  to  the  location  parameter  of  the 

distribution.  On the other  hand  𝜎 is  the scale parameter and unlike the location parameter,  it 

controls the shape of the distribution (the kurtosis of the distribution). Since the MLq method is 

based on the 𝑞𝑛

𝑡ℎ power of the pdf, the tuning parameter 𝑞𝑛 only affects the shape of the distribution 

not the location. Therefore it is observed from the asymptotic properties that only scale parameter 

𝜎 is affected from q.  For this reason in simulation study, we have redefined 𝜎 by multiplying the 

resulting estimates with q. 

By maximizing the Lq-likelihood function we actually target to  get  𝜃𝑛

∗. Therefore, to get 𝜃(0), 𝑞𝑛 

should tends to one as n tends to infinity. Therefore, similar to the Ferrari and Yang (2010a) the 

assumptions A1 and A2 should be hold to get asymptotic unbiasedness of 𝜃̃𝑛 .  These assumptions 

also guarantee that there is a solution of the Lq-likelihood equation, it is a maximizer of the Lq-

likelihood function and it is unique. 

Similar to Ferrari and Yang (2010a) the asymptotic distribution of the MLq estimator for the autoregressive 

error terms regression model with the tuning parameter 𝑞𝑛 is obtained as 

√n𝐕q

−1/2(θ̃ q − θ) → Np(0, 𝐈p), 

where 𝐈p is the (𝑝 × 𝑝) identity matrix and  

15 

 
 
 
 
𝑽𝑞(𝜃) = 𝑱(𝜃∗)

−1

𝑲(𝜃∗)𝑱(𝜃∗)

−1

𝑱(𝜃) = 𝐸 (∇𝜃𝑈∗(𝜃𝑛

∗; 𝑎𝑡, 𝑞𝑛)) 

𝑲(𝜃) = 𝐸 (𝑈∗(𝜃𝑛

∗; 𝑎𝑡, 𝑞𝑛)′𝑈∗(𝜃𝑛

∗; 𝑎𝑡, 𝑞𝑛)). 

Here 𝑈∗(𝜃𝑛

∗; 𝑎𝑡, 𝑞𝑛) = 𝑈(𝜃∗; 𝑎𝑡)𝑓(𝑎𝑡, 𝜃∗)

1−𝑞𝑛 is the score vector and  

∇𝜃𝑈∗(𝜃𝑛

∗; 𝑎𝑡, 𝑞𝑛) = ∇𝜃 (𝑈(𝜃∗; 𝑎𝑡)𝑓(𝑎𝑡, 𝜃∗)

1−𝑞𝑛)    

                               = {

𝑓(𝑎𝑡, 𝜃∗)

1−𝑞𝑛 {(1 − 𝑞𝑛)𝑈(𝜃∗; 𝑎𝑡)

′

𝑈(𝜃∗; 𝑎𝑡) + ∇𝜃𝑈(𝜃∗; 𝑎𝑡)}

∇𝜃 (𝑈(𝜃∗; 𝑎𝑡))

,

𝑞𝑛 = 1
, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒

.   (22) 

The  first  and  second  derivatives,  which  are  required  to  obtain  𝑲  and  𝑱,  are  given  in  Appendix  A  and 

Appendix B. 

A necessary and sufficient condition for asymptotic normality of CMLq estimator is 𝑞𝑛 → 1 when 𝑛 → ∞. 

It is important to note that when q is fixed, CMLq estimator is equal to M-estimator, and so the asymptotic 

covariance  matrix  of  CMLq  estimator  will  be  the  same  as  the  asymptotic  covariance  matrix  of  the  M- 

estimator (Hampel, et al. 1986, Huber et al. 2009, Maronna et. al. 2006).   

7. Numerical studies 

In this section, to examine the performances of the CMLq method over the CML estimation method in the 

cases both with and without outliers in the data we carry on a simulation study and analyze a real-data. All 

computations are carried out in R-3.1.2 (R Development Core Team, 2017)  

16 

 
 
 
 
 
 
 
 
7.1 Sampling Design 

We  generate  𝑦𝑡  from  model  given  in  (3)  included  5  and  10  covariates  with  same  autoregressive 

structure 𝜙 = (𝜙1, 𝜙2)′ = (0.8, −0.2)′  and  𝑥𝑡,𝑖 s  are  generated  standard  normal  distribution.  Here  the 

values of 𝜙 are taken to guarantee the stationarity assumption for the model of the error terms.  Regression 

coefficients  are  considered  as  𝛽 = (𝛽1, 𝛽2, 𝛽3, 𝛽4, 𝛽5)′  = (1, 3,  5,  2, 1)′  and    𝛽 = (𝛽1, 𝛽2, … , 𝛽10)′  =

(3, 3, … , 3)′. The simulation study is repeated 100 times with sample sizes 50. 

The simulation study is reported under the following cases. 

Case I. In the first part of our simulation study, data without contamination is considered. The results are 

presented in Table 1 and Table 4 for p=5 and p=10, respectively. 

Case II.  10% of 𝑦𝑖  observations are replaced by the values generated from N(10,1), which are referred as 

outliers in y-direction. Table 2 and Table 5 show the simulation results for this scenario for p=5 and p=10, 

respectively. 

Case III. In this case, both x and y variables will include outliers. In the y direction the outliers will be 

created using procedure described in Case II. In the x direction similar procedure will be used to create the 

outliers. Table 3 and Table 6 show the simulation results for this case for p=5 and p=10, respectively. 

Selection of the tuning parameter q 

The  performance  of  the  CMLq  estimator  depends  on  the  tuning  parameter  that  controls  the  weights. 

Therefore, it is important to select q appropriately.  

17 

 
 
 
 
 
 
 
In this paper we will use the robust AIC criterion proposed by Ronchetti (1985) to choose the appropriate 

tuning constant q. That is, we select the tuning parameter (q) which minimizes the following formula 

𝑅𝐴𝐼𝐶 = −

1
𝑛

𝑛
𝑡=𝑝+1 + 𝑡𝑟(−𝑀2
∑

𝐿𝑞

−1𝑀1), 

where 𝑀1 is the first derivative and 𝑀2 is the second derivative of 𝐿𝑞 function with respect to the parameters 

of interest. All of these derivatives are provided in Appendix A and Appendix B. The search is carried on 

using on the interval (0,1). 

Performance Measures  

To  evaluate  the  performances,  the  bias,  the  root  mean  squared  error  (RMSE),  the  standard  errors  (SE) 

obtained from the asymptotic covariance matrix and the asymptotic confidence intervals (CIL − CIU) are 

calculated for each model parameter (𝛾 = 𝛽𝑖, 𝜙𝑙, 𝜎 ) . The bias and RMSE are calculated using  

𝐵𝑖𝑎𝑠(𝛾̂) = 𝛾̅ − 𝛾,   𝛾̅ =

1
100

∑

100
𝑖=1

𝛾̂𝑖

, 

𝑅𝑀𝑆𝐸(𝛾̂) = √

1
100

∑ (𝛾̂𝑖 − 𝛾)2

100
𝑖=1

. 

The asymptotic intervals for the CMLq estimates of the parameters of autoregressive error terms regression 

model are calculated by using the asymptotic covariance matrix given in Section 6. 

Figures 1-6 are the boxplots of the model parameter estimates from 100 simulated datasets for the sample 

size 50 in all three cases.  

18 

 
 
 
                                         
 
 
 
7.2 Simulation Results 

In Tables, root mean squared error (MSE), bias values, standard errors and the confidence intervals of the 

estimated parameters are given to compare the performance of the estimators. We also give the chosen q 

values with respect to the minimum RAIC values.  

The simulation results for Case I are summarized in Tables 1 for p=5 and in Tables 4 for p=10. Both Bias 

and RMSE values indicated that when there is no contamination in the data, the CMLq estimation is close 

to  CML  estimation  method  in every  sample  sizes.  By  examining  the results  came  from  different tuning 

parameter q, the best result for CMLq estimation is obtained from q is close to 1 as expected.  In Figure 1, 

Figure  4  we  observe that  the  variability  of the  estimates obtained from  the  CML  and  CMLq  estimation 

methods are very similar. Also, in those figures, boxplots show that both methods are accurately estimate 

the regression parameters. 

Table 2 and Table 5 show the simulation results for Case II for p=5 and p=10, respectively. We observed 

from these results that the Bias and the RMSE values of the CMLq estimates are drastically better than those 

of the CML estimates. These results confirm that CML estimators are badly affected from the outliers in y 

direction.  

The best results for CMLq estimator which are summarized in Table 5  are corresponding to the chosen q 

that makes the RAIC minimum. We observe that when the data contain outliers, the chosen q is not very 

close to one which makes the corresponding estimators robust against the outliers. Figure 2 and Figure 5 

also show that if there are some outliers in the data the CML estimation method fails to correctly estimate 

the  parameters.  However,  CMLq  estimation  method  is  not  affected  by  the  outliers  in  y  direction  and 

produces estimates that are very close to true parameter values. These results show that CMLq estimators 

are resistant to the vertical outliers in the data. 

19 

 
 
 
 
In the case of outliers in x-y direction, Table 3 and Table 6 show that CML estimation has a considerably 

higher RMSE than CMLq estimation for all the parameters except for the autoregressive model parameters. 

Figure 3 and Figure 6 can support the same results.  

To sum up the simulation results confirm that the CMLq estimation method produces comparable results 

when there are no outliers in the data and it has definite superiority over the CML when there are outliers in 

the y and x and y direction. It should be noted that in case of outliers in x direction, the performance of the 

CMLq estimation method is not very premising. This is due to the fact that the CMLq estimation method is 

a M estimation method for fixed value of q and it is known that M-estimators are not robust against the 

outliers in x direction. 

7.3 Real-data analysis 

We consider a data set on the proportion of the number of ten million international phone calls from Belgium 

in the years 1950-1973. Rousseeuw and Leroy (1987) modeled this dataset with a robust regression method 

the least median of squares (LMS). Rousseeuw and Leroy (1987), point out that: “...it turned out that from 

1964 to 1969 another recording system was used, giving the total number of minutes of these calls. The 

years 1963 and 1970 are also partially affected because the transitions did not happen exactly on New Year’s 

Day...”.  This different measurement system in 1964-1969 causes a heavy contamination.  As it can be seen 

in Figure 7, there are several outliers in the y-direction in 1964-1969. 

Tuaç et al. (2017) have also used a linear regression model with AR(1) error terms with the assumption that 

the error terms have a t distribution as a heavy-tailed alternative to the normal distribution. For these reasons, 

this data set is modeled with autoregressive error terms regression model and the parameters of this model 

are estimated by using the CMLq method. 

The number of phone calls made from Belgium is the dependent variable (y) and the explanatory variable 

(x) is the year. We observe from the autocorrelation function and the partial autocorrelation function graphs 

20 

 
 
 
of the OLS residuals that the residuals show an autocorrelated structure with type AR(1). That is why we 

consider a regression model with AR(1) error term to model this data set. The summary of the results is 

reported in Table 7. 

The linear regression model and the error structure are as follows 

𝑦𝑡 = 𝛽0 + 𝛽1𝑥𝑡 + 𝑒𝑡 

𝑒𝑡 = 𝜙1 + 𝑎𝑡 

We also calculated RAIC for the real data example to compare the performance of the CMLq and CML 

estimation methods. The results are shown in Table 7. Figure 7 show the scatter plot of the data with the 

fitted regression lines obtained from CML and CMLq estimates. We observe from this figure that the CMLq 

estimate provides considerable better fit than the CML estimate. 

Figure 7. Number of international phone calls from Belgium with the CML and CMLq fit 

21 

 
 
 
 
 
Table 7. Summary of the estimated parameters for real data example 

  CML 

SE 

CIL 

CIU 

CMLq 

SE 

CIL 

CIU 

q* 

𝛽̂
0 

𝛽̂
1 

𝜙̂ 

-45.36150 

8.32749 

-48.96257 

-41.76042 

-5.0884 

7.03634 

-8.13120 

-2.04571 

0.917 

0.84219 

0,14078 

0.78131 

0.90307 

0.10687  0.12175  0.05421    0.15952 

0.81672 

0.05827 

0.79290 

0.84053 

0.70260  0.08389  0.66831    0.73688 

𝜎̂ 

4.1424 

0.08345 

3.27576    

5.99481 

1.20222  0.04520  0.95068    1.73980 

RAIC  61.3656 

38.68 

Also, according to results in Table 7, CMLq estimation method shows better performance in terms of the 

RAIC and the confidence intervals. We can observe that the length of the confidence intervals obtained from 

the CMLq are shorter than the length of the confidence intervals obtained from the CML estimators except 

the parameter 𝜙.  

8  Discussion 

In this paper, we have applied the CMLq estimation method to estimate the parameters of the regression 

model with autoregressive error term model when the data set is contaminated. The simulation results have 

revealed  that  CMLq  estimation  produces  results  that  are  close  to  the  results  obtained  from  the  CML 

estimation method when there are no outliers in the data. However, the CMLq estimation outperforms when 

the dataset contains outliers in y and x and y direction. These simulation results have also displayed that the 

smaller values of q correspond to the CMLq estimators that are less sensitive to the outliers.  To sum up, 

the CMLq estimation method can provide robust estimation method alternative to the CML and therefore 

this method can be used to estimate the parameters of the AR(p) error term regression model when outliers 

are present in the data. 

22 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
References 

Alpuim, T. and El-Shaarawi, A. 2008. On the efficiency of regression analysis with AR(p) errors. Journal 

of Applied Statistics, 35(7): 717-737. 

Ansley,  C.  F.  1979.  An  algorithm  for  the  exact  likelihood  of  a  mixed  autoregressive-moving  average         

process. Biometrika. 66 (1): 59-65.doi: 10.1093/biomet/66.1.59. 

Beach,  C.M.  and  Mackinnon,  J.  G.  1978.  A  Maximum  Likelihood  Procedure  for  Regression  with 

Autocorrelated Errors. Econometrica, 46(1): 51- 58. 

Cavalieri, J. 2002. O método de máxima Lq-verossimilhança em modelos com erros de medição (Doctoral 

thesis,  Federal  University  of  São  Carlos,  Department  of  Statistics).  Retrieved 

from 

https://repositorio.ufscar.br/bitstream/handle/ufscar/4554/4180.pdf?sequence=1  

Cochrane, D. and Orcutt, G. H. 1949.  Application of least square to relationship containing autocorrelated 

error terms. Journal of American Statistical Association, 44: 32–61. 

Dogru, F. Z., Bulut, Y. M. and Arslan, O. 2018. Doubly Reweighted Estimators for the Parameters of the 

Multivariate t Distribution. (Accepted) 

Ferrari  D.  and  Yang  Y.  2007.  Estimation  of  tail  probability  via  the  maximum  Lq-likelihood  method. 

Technical report 659, School of statistics, University of Minnesota 

23 

 
 
 
 
 
 
 
 
 
Ferrari D. and Paterlini S. 2009. The maximum Lq-likelihood method: an application to extreme quantile 

estimation in finance. Methodol Comput Appl Probab, 11(1): 3–19 

Ferrari, D. and Yang, Y. 2010a. Maximum Lq-likelihood estimation. Annals of Statistics, 38(2): 753-   

       783. 

Ferrari, D., Paterlini, S. 2010b. Efficient and robust estimation for financial returns: an approach based on  

q-entropy. 

Available 

at 

SSRN: 

http://ssrn.com/abstract=1906819 

or 

http://dx.doi.org/10.2139/ssrn.1906819. 

Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J. and Stahel, W. A. 1986. Robust Statistics. The Approach 

Based on Influence Functions. New York: John Wiley and Sons. Chap. 4.2. pp. 230-231. 

Havrda,  J.  and  Charvát,  F.  1967.  Quantication  method  of  classication  processes:  Concept  of  structural 

entropy. Kibernetika, 3: 30-35. 

Huang, C., Lin, J. and Ren, Y.Y. 2013. Testing for the shape parameter of generalized extreme value  

distribution based on the Lq-likelihood ratio statistic. Metrika, 76: 641-671. 

Huber, P. J. And Ronchetti, E. M. 2009. Robust Statistics. New Jersey: John Wiley and Sons. 

Maronna, R. A.,  Martin, R. D. and Yohai, V. J. 2006. Robust Statistics: Theory and Methods. Chichester : 

John Wiley and Sons. 

24 

 
 
 
 
 
 
 
 
 
Ozdemir,  S.,  Guney,  Y.  Tuac,  Y.  and  Arslan,  O.  2019.  Maximum  Lq-Likelihood  Estimation  for  The 

Parameters of Marshall-Olkin Extended Burr XII Distribution. Commun. Fac. Sci. Univ. Ank. Ser. 

A1 Math. Stat, 68(1): 17-34.  

Qin,  Y.  and  Priebe,  E.C.  2013.  Maximum  Lq-Likelihood  Estimation  via  the  Expectation  Maximization 

Algorithm: A Robust Estimation of Mixture Models. Journal of the American Statistical Association, 

108: 914-928. 

Qin, Y. and Priebe, E.C. 2016. Robust Hypothesis Testing via Lq-Likelihood. Statistica Sinica: Preprint, 

doi:10.5705/ss.202015.0441. 

R Core Team (2017). R: A language and environment for statistical computing. R Foundation for Statistical 

Computing, Vienna, Austria. URL http://www.R-project.org 

Ronchetti, E. 1985. Robust model selection in regression. Statist. Probab. Lett, 3: 21–23. 

Rousseeuw, P.J. and  Leroy, A.M. 1987. Robust Regression and Outlier Detection. Wiley Series, USA.  

Tsallis, C. 1988. Possible generalization of Boltzmann-Gibbs statistics. Journal of Statistical Physics, 52, 

479-487. 

Tuac, Y., Guney, Y., Senoglu, B. and Arslan, O. 2017. Robust Parameter Estimation of Regression Model 

with  AR(p)  Error  Terms.    Communications  in  Statistics  -  Simulation  and  Computation. 

https://doi.org/10.1080/03610918.2017.1343839 . 

25 

 
 
 
 
 
 
 
 
 
 
APPENDIX A 

Let’s 𝜃 = ( 𝛽1, 𝛽2, … , 𝛽𝑀, 𝜙1, 𝜙2, … , 𝜙𝑝, 𝜎2). The elements of 𝑈(𝑎𝑡; 𝜃) are 

𝜕𝑙𝑛𝐿
𝜕𝛽𝑘

=

𝜕𝑙𝑛𝐿
𝜕𝜙𝑙

=

1
𝜎2 (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

1
𝜎2 (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑀

𝑖=1

𝑀

𝑖=1

Φ(𝐵)𝑥𝑡,𝑖) Φ(𝐵)𝑥𝑡,𝑘 ,   𝑘 = 1,2, … , 𝑀 

𝑀

Φ(𝐵)𝑥𝑡,𝑖) (𝑦𝑡−𝑙 − ∑ 𝛽𝑖

𝑥𝑡−𝑙,𝑖) ,

𝑙 = 1,2, … , 𝑞, 

𝜕𝑙𝑛𝐿
𝜕𝜎2 = −

1
2𝜎2 +

1
2𝜎4 (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑀

𝑖=1

𝑖=1

2

Φ(𝐵)𝑥𝑡,𝑖)

 , 

where 𝑡 = 𝑝 + 1, … , 𝑁. 

The second partial derivatives of conditional log-likelihood function are 

𝜕2𝑙𝑛𝐿
𝜕𝛽𝑗𝜕𝛽𝑘

= −

1
𝜎2 Φ(𝐵)𝑥𝑡,𝑗Φ(𝐵)𝑥𝑡,𝑘 , 

𝜕2𝑙𝑛𝐿
𝜕𝛽𝑗𝜕𝜙𝑖

= −

1
𝜎2 [𝑒𝑡−𝑖Φ(𝐵)𝑥𝑡,𝑗 + 𝑎𝑡𝑥𝑡−𝑖,𝑗], 

𝜕2𝑙𝑛𝐿
𝜕𝛽𝑗𝜕𝜎2 = −

1
𝜎4 𝑎𝑡Φ(𝐵)𝑥𝑡,𝑗  , 

𝜕2𝑙𝑛𝐿
𝜕𝜙𝑖𝜕𝜙𝑟

= −

1
𝜎2 𝑒𝑡−𝑖𝑒𝑡−𝑟  , 

𝜕2𝑙𝑛𝐿
𝜕𝜙𝑖𝜕𝜎2 = −

2
𝜎4 𝑎𝑡𝑒𝑡−𝑖  , 

𝜕2𝑙𝑛𝐿
𝜕(𝜎2)2 =

1
2𝜎4 −

1
2 , 
𝜎6 𝑎𝑡

26 

 
 
 
where 𝑡 = 𝑝 + 1, … , 𝑁, 𝑗, 𝑘 = 1,2, … , 𝑀 and 𝑖, 𝑟 = 1,2, … , 𝑝. 

APPENDIX B 

The first partial derivatives of Lq- likelihood function given in (), namely the elements of 𝑈∗(𝑎𝑡; 𝜃, 𝑞) are 

𝜕𝐿𝑞
𝜕𝛽𝑘

=

𝜕𝐿𝑞
𝜕𝜙𝑙

=

1
𝜎2 𝜔𝑡 (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

1
𝜎2 𝜔𝑡 (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑀

𝑖=1

𝑀

𝑖=1

Φ(𝐵)𝑥𝑡,𝑖) Φ(𝐵)𝑥𝑡,𝑘,   𝑘 = 1,2, … , 𝑀 

𝑀

Φ(𝐵)𝑥𝑡,𝑖) (𝑦𝑡−𝑙 − ∑ 𝛽𝑖

𝑥𝑡−𝑙,𝑖) ,

𝑙 = 1,2, … , 𝑞,      

𝑖=1

2

Φ(𝐵)𝑥𝑡,𝑖)

],  

1
2𝜎4 (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑀

𝑖=1

𝜕𝐿𝑞
𝜕𝜎2 = 𝜔𝑡 [−

1
2𝜎2 +

where  

𝜔𝑡 = (

1
√2𝜋𝜎2

𝑒𝑥𝑝 {−

𝑀
(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)
2𝜎2

2

1−𝑞

} )

, 𝑡 = 𝑝 + 1, … 𝑁. 

The second partial derivatives of conditional Lq-likelihood function can be obtained by substituting the 

derivatives given in Appendix A in the equation (22). 

27 

 
 
 
  
 
 
 
 
 
Table 1. Simulation results for Case I, p=5  
q 

𝛽̃1 
𝛽̃2 
𝛽̃3 
0.99  𝛽̃4 
𝛽̃5 
𝜙̃1 
𝜙̃2 
𝜎̃ 
𝛽̂1 
𝛽̂2 
𝛽̂3 
1.00  𝛽̂4 
𝛽̂5 
𝜙̂1 
𝜙̂2 
𝜎̂ 

Estimates 
0.95568 
3.04248 
5.11161 
1.98414 
0.91404 
0.85362 
-0.23565 
0.95734 
0.94579 
3.08820 
5.1055 
1.97433 
0.95916 
0.83571 
-0.22147 
0.96519 

Bias 
-0.04431 
0.04247 
0.11161 
-0.01585 
-0.08596 
0.05362 
-0.03565 
-0.04266 
-0.0542 
0.0882 
0.10549 
-0.02566 
-0.04083 
0.0357 
-0.02146 
-0.0348 

RMSE 
0.03271 
0.1836 
0.48159 
0.14032 
0.33923 
0.11338 
0.14057 
0.11086 
0.04062 
0.33399 
0.43553 
0.14209 
0.33732 
0.17236 
0.13431 
0.10839 

Table 2. Simulation results for Case II, p=5  
q 

𝛽̃1 
𝛽̃2 
𝛽̃3 
0.60  𝛽̃4 
𝛽̃5 
𝜙̃1 
𝜙̃2 
𝜎̃ 
𝛽̂1 
𝛽̂2 
𝛽̂3 
1.00  𝛽̂4 
𝛽̂5 
𝜙̂1 
𝜙̂2 
𝜎̂ 

Estimates 
1.04486 
2.89744 
5.00725 
2.08757 
1.0461 
0.76276 
-0.04947 
1.01113 
1.43832 
4.06013 
6.75896 
2.48166 
1.70155 
-0.11389 
0.05842 
10.23524 

Bias 
0.04486 
-0.10255 
0.00725 
0.08757 
0.04610 
-0.03723 
0.15052 
0.01113 
0.43832 
1.06014 
1.75897 
0.48166 
0.70155 
-0.91389 
0.25843 
9.23524 

RMSE 
0.29874 
0.47066 
0.04656 
0.37546 
0.31289 
0.18873 
0.64675 
0.51718 
1.98659 
3.845018 
4.256039 
1.209248 
1.452112 
1.156451 
0.981569 
10.06469 

SE 
0.032535 
0.186175 
0.48565 
0.137329 
0.340653 
0.19614 
0.18256 
0.16592 
0.04011 
0.33698 
0.4364 
0.1403 
0.33845 
0.17432 
0.17758 
0.12213 

SE 
0.29647 
0.47394 
0.04251 
0.38353 
0.30953 
0.18428 
0.75916 
0.52521 
1.93437 
4.45256 
6.91579 
2.58094 
2.67861 
3.48156 
1.05701 
9.28857 

CIL 
0.94666 
2.99087 
4.97699 
1.94607 
0.81961 
0.79925 
-0.28625 
0.82682 
0.934672 
2.994794 
4.984536 
1.935441 
0.865346 
0.787391 
-0.27069 
0.82009 

CIL 
0.96268 
2.76607 
4.99546 
1.98126 
0.96030 
0.71168 
-0.25990 
0.86618 
0.90214 
2.82594 
4.84200 
1.76626 
0.95907 
-1.07893 
-0.23457 
0.76790 

CIU 
0.96469 
3.09408 
5.24622 
2.02220 
1.00846 
0.90798 
-0.18505 
1.25555 
0.95690 
3.18160 
5.22646 
2.01321 
1.05297 
0.88402 
-0.17225 
1.24533 

CIU 
1.12703 
3.02880 
5.01903 
2.19387 
1.13189 
0.81384 
0.16095 
1.31531 
1.97450 
5.29431 
8.67591 
3.19706 
2.44402 
0.85114 
0.35140 
13.31430 

28 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3. Simulation results for Case III, p=5  
q 

𝛽̃1 
𝛽̃2 
𝛽̃3 
0.44  𝛽̃4 
𝛽̃5 
𝜙̃1 
𝜙̃2 
𝜎̃ 
𝛽̂1 
𝛽̂2 
𝛽̂3 
1.00  𝛽̂4 
𝛽̂5 
𝜙̂1 
𝜙̂2 
𝜎̂ 

Estimates 
0.97228 
3.10851 
4.93869 
2.08666 
0.89745 
0.79471 
-0.04648 
1.11441 
0.55661 
1.8251 
2.81999 
1.26058 
0.45004 
0.06648 
-0.18028 
9.52381 

Bias 
-0.02771 
0.10851 
-0.0613 
0.08666 
-0.10254 
-0.00528 
0.15351 
0.11441 
-0.44338 
-1.1749 
-2.18001 
-0.73941 
-0.54996 
-0.73352 
0.01972 
8.52382 

RMSE 
0.36524 
0.39046 
0.50891 
0.31819 
0.52575 
0.09552 
0.64144 
0.6555 
1.77338 
2.00342 
5.43887 
1.69692 
1.81343 
1.77549 
0.6972 
8.99814 

SE 
0.35617 
0.48200 
0.49178 
0.40511 
0.48494 
0.09315 
0.64641 
0.6941 
1.93625 
4.26081 
8.40224 
2.9974 
2.94925 
2.81078 
0.65814 
1.75666 

CIL 
0.87355 
2.97490 
4.80237 
1.97436 
0.76303 
0.76889 
-0.22566 
0.95465 
0.01990 
0.64406 
0.49100 
0.42974 
-0.36745 
-0.71263 
-0.36271 
0.62334 

CIU 
1.07100 
3.24211 
5.07500 
2.19895 
1.03186 
0.82053 
0.13269 
1.44966 
1.09331 
3.00613 
5.14897 
2.09141 
1.26753 
0.84558 
0.00214 
12.38886 

29 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 4. Simulation results for Case I, p=10  
q 

0.99 

1.00 

𝛽̃1 
𝛽̃2 
𝛽̃3 
𝛽̃4 
𝛽̃5 
𝛽̃6 
𝛽̃7 
𝛽̃8 
𝛽̃9 
𝛽̃10 
𝜙̃1 
𝜙̃2 
𝜎̃ 
𝛽̂1 
𝛽̂2 
𝛽̂3 
𝛽̂4 
𝛽̂5 
𝛽̂6 
𝛽̂7 
𝛽̂8 
𝛽̂9 
𝛽̂10 
𝜙̂1 
𝜙̂2 
𝜎̂ 

Estimates 
2.99046 
2.94312 
2.96989 
2.99039 
2.98233 
2.98382 
2.99666 
2.99154 
2.98169 
3.06193 
0.78136 
-0.20134 
1.00999 
2.99532 
2.94274 
2.97302 
3.00258 
2.98221 
2.98584 
2.99757 
2.99822 
2.98818 
3.05098 
0.77994 
-0.21621 
1.01461 

Bias 
-0.00954 
-0.05688 
-0.03011 
-0.00961 
-0.01767 
-0.01618 
-0.00334 
-0.00846 
-0.01831 
0.06193 
-0.01864 
-0.00134 
0.00999 
-0.00468 
-0.05726 
-0.02698 
0.00258 
-0.01779 
-0.01416 
-0.00243 
-0.00178 
-0.01182 
0.05098 
-0.02006 
-0.01621 
0.01461 

RMSE 
0.09088 
0.19870 
0.18171 
0.08300 
0.12804 
0.10177 
0.07957 
0.08836 
0.12315 
0.31800 
0.12322 
0.04165 
0.00876 
0.08804 
0.25417 
0.09807 
0.07348 
0.13158 
0.09284 
0.07611 
0.07573 
0.11985 
0.2166 
0.12453 
0.04228 
0.00913 

SE 
0.08861 
0.30373 
0.19418 
0.07931 
0.12564 
0.10730 
0.07999 
0.08791 
0.12893 
0.31167 
0.12308 
0.0423 
0.00881 
0.08732 
0.25364 
0.10266 
0.07215 
0.12087 
0.09489 
0.07328 
0.07837 
0.12292 
0.23053 
0.12090 
0.04599 
0.08417 

CIL 
2.96589 
2.85893 
2.91606 
2.96840 
2.94750 
2.95407 
2.97448 
2.96717 
2.94595 
2.97553 
0.74724 
-0.21306 
0.90836 
2.97111 
2.87243 
2.94456 
2.98258 
2.94870 
2.95953 
2.97725 
2.97649 
2.95410 
2.98708 
0.74642 
-0.22896 
0.91251 

CIU 
3.01502 
3.02731 
3.02371 
3.01237 
3.01715 
3.01356 
3.01883 
3.01590 
3.01742 
3.14832 
0.81547 
-0.18962 
1.41563 
3.01952 
3.01304 
3.00147 
3.02257 
3.01571 
3.01214 
3.01788 
3.01994 
3.02225 
3.11488 
0.81345 
-0.20346 
1.42210 

30 

 
 
 
 
 
 
 
 
 
Table 5. Simulation results for Case II, p=10  
q 

0.63 

1.00 

𝛽̃1 
𝛽̃2 
𝛽̃3 
𝛽̃4 
𝛽̃5 
𝛽̃6 
𝛽̃7 
𝛽̃8 
𝛽̃9 
𝛽̃10 
𝜙̃1 
𝜙̃2 
𝜎̃ 
𝛽̂1 
𝛽̂2 
𝛽̂3 
𝛽̂4 
𝛽̂5 
𝛽̂6 
𝛽̂7 
𝛽̂8 
𝛽̂9 
𝛽̂10 
𝜙̂1 
𝜙̂2 
𝜎̂ 

Estimates 
3.10437 
2.99613 
3.17695 
2.99576 
2.89541 
2.89955 
3.21677 
2.95599 
2.80923 
3.35013 
0.76617 
0.06148 
1.19711 
3.32414 
3.41139 
4.17681 
4.31203 
3.94000 
4.44877 
3.69657 
3.97170 
3.88432 
4.04412 
0.00740 
-0.38703 
13.22522 

Bias 
0.10437 
-0.00386 
0.17695 
-0.00423 
-0.10458 
-0.10044 
0.21677 
-0.04400 
-0.19076 
0.45013 
-0.03382 
0.26148 
0.19711 
0.32415 
0.41140 
1.17682 
1.31203 
0.94001 
1.44877 
0.69657 
0.97171 
0.88433 
1.04413 
-0.79260 
-0.18703 
12.22523 

RMSE 
0.46401 
0.11980 
0.93522 
0.06135 
0.45475 
0.62792 
0.78716 
0.31494 
0.88541 
0.97399 
0.23361 
0.80275 
0.82447 
1.34633 
1.92714 
1.47198 
3.25816 
1.23295 
3.40305 
2.94512 
2.89445 
2.92778 
2.06263 
1.02508 
0.81849 
13.27970 

SE 
0.46874 
0.12620 
0.98024 
0.06784 
0.44097 
0.61124 
0.92172 
0.34858 
0.98674 
1.26457 
0.54672 
0.97569 
0.73086 
1.21082 
2.01287 
4.64121 
5.26659 
3.71485 
5.80092 
3.18861 
3.62755 
3.83186 
3.97276 
2.99382 
0.79606 
12.03498 

CIL 
2.97444 
2.96115 
2.90524 
2.97696 
2.77318 
2.73012 
2.96128 
2.85937 
2.53572 
2.99961 
0.61463 
-0.20897 
0.97665 
2.98852 
2.85345 
2.89033 
2.85221 
2.91030 
2.84084 
2.81273 
2.96619 
2.82218 
2.94293 
-0.82244 
-0.60769 
0.28287 

CIU 
3.23430 
3.03111 
3.44866 
3.01456 
3.01764 
3.06898 
3.47226 
3.05261 
3.08274 
3.70065 
0.91771 
0.33193 
2.67790 
3.65976 
3.96933 
5.46329 
5.77185 
4.96970 
6.05670 
4.58041 
4.97721 
4.94646 
5.14531 
0.83724 
-0.16637 
18.53680 

31 

 
 
 
 
 
 
 
 
 
Table 6. Simulation results for Case III, p=10  
q 

0.41 

1.00 

𝛽̃1 
𝛽̃2 
𝛽̃3 
𝛽̃4 
𝛽̃5 
𝛽̃6 
𝛽̃7 
𝛽̃8 
𝛽̃9 
𝛽̃10 
𝜙̃1 
𝜙̃2 
𝜎̃ 
𝛽̂1 
𝛽̂2 
𝛽̂3 
𝛽̂4 
𝛽̂5 
𝛽̂6 
𝛽̂7 
𝛽̂8 
𝛽̂9 
𝛽̂10 
𝜙̂1 
𝜙̂2 
𝜎̂ 

Estimates 
2.87524 
2.65257 
2.81819 
2.77023 
2.80791 
2.76750 
2.98588 
2.82776 
2.74666 
2.85908 
0.67642 
0.03722 
2.35923 
2.31931 
2.23790 
2.28866 
2.24767 
2.34417 
2.29419 
2.43387 
2.05707 
2.16312 
2.29770 
0.09797 
-0.15374 
7.09458 

Bias 
-0.12475 
-0.34742 
-0.18181 
-0.22977 
-0.19209 
-0.23249 
-0.01412 
-0.17223 
-0.25333 
-0.14092 
-0.12358 
0.23722 
1.35923 
-0.68069 
-0.76210 
-0.71134 
-0.75233 
-0.65583 
-0.70581 
-0.56613 
-0.94293 
-0.83688 
-0.70230 
-0.70203 
0.04626 
6.09458 

RMSE 
0.34051 
0.91942 
0.6982 
0.642591 
0.93393 
0.850986 
0.23973 
0.647294 
0.96672 
0.650834 
0.50757 
0.961066 
1.62963 
1.37979 
1.17922 
2.34043 
2.24115 
1.15104 
1.29847 
1.14423 
1.39418 
1.39916 
1.10200 
0.91911 
0.58027 
6.62821 

SE 
0.48154 
1.30643 
0.81535 
0.84573 
0.93361 
0.87468 
0.21890 
0.68577 
0.97839 
0.70445 
0.48179 
0.96167 
1.96340 
3.07655 
2.85772 
3.09404 
2.82917 
2.75168 
2.62589 
2.29822 
3.63473 
3.18660 
2.85404 
2.78527 
0.61885 
6.35406 

CIL 
2.74176 
2.29045 
2.59219 
2.53581 
2.54913 
2.52505 
2.92520 
2.63767 
2.47546 
2.66382 
0.54287 
-0.22934 
0.52184 
1.46653 
1.44578 
1.43104 
1.46346 
1.58144 
1.56633 
1.79684 
1.04957 
1.27984 
1.50660 
-0.67407 
-0.32528 
0.38068 

CIU 
3.00872 
3.01469 
3.04419 
3.00465 
3.06669 
3.00995 
3.04656 
3.01785 
3.01786 
3.05434 
0.80997 
0.30378 
3.30676 
3.17209 
3.03002 
3.14628 
3.03188 
3.10690 
3.02205 
3.07090 
3.06457 
3.04640 
3.08880 
0.87001 
0.01780 
9.94393 

32 

 
 
 
 
 
 
 
 
 
Figure 1. Boxplots of the estimates of the parameters for Case I, p=5. True parameter values are  β =
(1, 3,  5,  2, 1)′, ϕ = (0.8, −0.2)′, σ = 1. 

Figure 2. Boxplots of the estimates of the parameters for Case II, p=5.  True parameter values are  β =
(1, 3,  5,  2, 1)′, ϕ = (0.8, −0.2)′, σ = 1 

33 

 
 
 
 
Figure 3. Boxplots of the estimates of the parameters for Case III, p=5. True parameter values are  β =
(1, 3,  5,  2, 1)′, ϕ = (0.8, −0.2)′, σ = 1. 

Figure 4. Boxplots of the estimates of the parameters for Case I, p=10. True parameter values are  β =
(β1, β2, … , β10)′  = (3, 3, … , 3)′, ϕ = (0.8, −0.2)′, σ = 1. 

34 

 
 
 
 
 
Figure 5. Boxplots of the estimates of the parameters for Case II, p=10. True parameter values are  β =
(β1, β2, … , β10)′  = (3, 3, … , 3)′, ϕ = (0.8, −0.2)′, σ = 1. 

Figure 6. Boxplots of the estimates of the parameters for Case III, p=10. True parameter values are  β =
(β1, β2, … , β10)′  = (3, 3, … , 3)′, ϕ = (0.8, −0.2)′, σ = 1. 

35 

 
 
 
 
 
 
