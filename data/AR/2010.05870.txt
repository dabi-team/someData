0
2
0
2

t
c
O
2
1

]
E
M

.
t
a
t
s
[

1
v
0
7
8
5
0
.
0
1
0
2
:
v
i
X
r
a

Model-based bias correction for short AR(1) and AR(2)
processes

Sigrunn H. Sørbye*1, Pedro G. Nicolau1 and H˚avard Rue2

1UiT The Arctic University of Norway, Tromsø, Norway

2King Abdullah University of Science and Technology, Thuwal, Saudi Arabia

October 13, 2020

Abstract

The class of autoregressive (AR) processes is extensively used to model temporal depen-
dence in observed time series. Such models are easily available and routinely ﬁtted using
freely available statistical software like R. A potential caveat in analyzing short time series is
that commonly applied estimators for the coefﬁcients of AR processes are severely biased.
This paper suggests a model-based approach for bias correction of well-known estimators
for the coefﬁcients of ﬁrst and second-order stationary AR processes, taking the sampling
distribution of the original estimator into account. This is achieved by modeling the relation-
ship between the true and estimated AR coefﬁcients using weighted orthogonal polynomial
regression, ﬁtted to a huge number of simulations. The ﬁnite-sample distributions of the
new estimators are approximated using transformations of skew-normal densities and their
properties are demonstrated by simulations and in the analysis of a real ecological data set.
The new estimators are easily available in our accompanying R-package ARbiascorrect
for time series of length n = 10, 11, . . . , 50, where original estimates are found using exact
or conditional maximum likelihood, Burg’s method or the Yule-Walker equations.

Keywords: autoregressive processes, density dependence, ﬁnite-sample properties, Gaussian
copula, Monte Carlo simulation, orthogonal polynomial regression, skew-normal approximation

1 Introduction

The class of autoregressive (AR) processes is one of the most central and widely applied time
series models. These processes are simple and intuitive, expressing the current value of a time
series as a linear combination of previous values and additional random noise. Speciﬁcally, a
pth order autoregressive process (AR(p)) can be deﬁned by

p

xt − µ =

φj(xt−j − µ) + wt

(1)

j=1
X

*Address for correspondence: Sigrunn Holbek Sørbye, Department of Mathematics and Statistics, Faculty of

Science, UiT The Arctic University of Norway, 9037 Tromsø, Norway. E-mail: sigrunn.sorbye@uit.no

1

 
 
 
 
 
 
where µ = E(xt), the set {φj}p
j=1 are ﬁxed coefﬁcients and {wt} is a Gaussian white noise
process, wt ∼ N (0, σ2). Originated by the famous Yule-Walker equations (Yule, 1927; Walker,
1931), the statistical properties of AR models have been thoroughly studied and established,
see e.g. Brockwell and Davis (2002), Box et al. (2008) and Shumway and Stoffer (2017) for
comprehensive introductions. However, one problem still remains. Commonly used estima-
tors for the AR coefﬁcients are severely biased for small sample sizes, i.e. having less than 50
observations (Shaman and Stine, 1988; Huitema and McKean, 1991; DeCarlo and Tryon, 1993;
Cheang and Reinsel, 2000). This is problematic in several ﬁelds of applications in which re-
alistic time series lengths are inherently short, e.g. in behavioral (Arnau and Bono, 2001) and
ecological research studies (Bissonette, 1999; Ives et al., 2010).

This paper studies the ﬁnite-sample properties of commonly applied estimators for the coefﬁ-
cients of stationary AR(1) and AR(2) processes and provides bias-corrected versions of such es-
timators. The AR(1) and AR(2) processes do have considerable practical importance (Box et al.,
2008, p.53) and various estimators for the AR coefﬁcients are easily available. The popular
stats:::ar-function in R (R Core Team, 2020) implements the closed form solution given
by the Yule-Walker equations as its default. The ar-function also provides estimates using
Burg’s method (Burg, 1967) and a conditional maximum likelihood estimator (MLE), maximiz-
ing the likelihood given initial values of x0, . . . , xp−1 in (1). The R-package FitAR:::FitAR
(McLeod and Zhang, 2008) provides the exact maximum likelihood estimator, where initial val-
ues are set using Burg’s method. All of the mentioned estimators give very similar results for
large sample sizes while the exact MLE has been claimed to usually perform better than alterna-
tives for short time series (McLeod and Zhang, 2006; Box and Luceno, 1997).

The upper panels of Figure 1 illustrate the average coefﬁcient estimates of AR(2) processes
of length n = 15 (left) and n = 30 (right), using the exact MLE, Burg’s method and the
Yule-Walker estimator. The results are based on generating 10000 time series for each selected
combination of (φ1, φ2) within the triangular stationary area of such processes. The AR(2)
process has pseudo-periodic behavior for pairs of coefﬁcients below the given parabolic curve.
In the lower part of this region, the bias of the exact MLE and Burg’s method is not too severe
but it increases with increasing values of φ2. The Yule-Walker approach clearly gives the most
biased results for both sample sizes. The average estimates using the conditional MLE are not
illustrated as these were not visually distinguishable from the exact MLE.

A number of methods to provide bias-corrected estimators for the AR coefﬁcients have been
proposed in literature. These range from asymptotic-based formulas for the bias (Marriott and Pope,
1954; Kendall, 1954; Shaman and Stine, 1988; Tanaka, 1984; Cordeiro and Klein, 1994) to meth-
ods using restricted maximum likelihood (Cheang and Reinsel, 2000) and bootstrapping
(Thombs and Schucany, 1990; Kim, 2003). Andrews (1993) introduced a median-unbiased cor-
rection for the least squares estimator of the AR(1) coefﬁcient which was generalized to give
an approximate median-unbiased estimator of AR(p) processes in Andrews and Chen (1994).
This estimator is implemented in the R-package BootPR (Kim, 2014), also including the esti-
mators of Shaman and Stine (1988) and Roy and Fuller (2001). The resulting estimates are not
constrained to fall within the stationary area of the processes.

Our bias-correcting approach differs from previous suggestions in the sense that we model
the true AR coefﬁcients as a function of original estimates, accounting for the sampling distri-
bution of the original estimator. This is achieved by a brute-force simulation approach where
we generate AR(1) and AR(2) processes for a ﬁne grid of underlying true values of the coef-
ﬁcients. The relationship between the true and estimated coefﬁcients is then described using a
weighted orthogonal polynomial regression model. In the AR(2) case, the regression model is

2

2

2

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

2

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

−2

−1

0
f 1

1

2

−2

−1

1

2

0
f 1

2

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

−2

−1

0
f 1

1

2

−2

−1

1

2

0
f 1

Figure 1: Upper panels: Mean estimates of selected pairs (φ1, φ2) based on 10000 simulations,
using the exact MLE (black), Burg’s algorithm (red) and the Yule-Walker solution (blue) when
n = 15 (left) and n = 30 (right). Lower panels: The corresponding corrected mean estimates
found by our proposed simulation-based approach.

ﬁtted based on a total of more than 59 million time series for a given sample size n. The result-
ing bias-corrected average estimates are illustrated in the lower panels of Figure 1, again using
the exact MLE, Burg’s method and the Yule-Walker solution as original estimators. We do see a
clear improvement in the bias properties for all of the methods. Admittedly, the bias-corrected
estimators will not be completely unbiased as the relationship between the true and originally
estimated coefﬁcients cannot be modeled perfectly. Especially this is the case for coefﬁcient
combinations along the borders of the triangular area.

The given brute-force simulation approach makes it possible to also derive sampling distri-
butions for both the original and bias-corrected estimators. Speciﬁcally, we have ﬁtted skew-
normal distributions to transformations of the originally estimated coefﬁcients of AR(1) and
AR(2) processes. The parameters of the skew-normal distribution are then modeled in terms of
the true underlying AR coefﬁcients, again using orthogonal polynomial regression. In the AR(1)
case, it is straightforward to obtain conﬁdence intervals for the original and bias-corrected es-
timators using Monte Carlo sampling.
In the AR(2) case, conﬁdence intervals are found by
combining Monte Carlo sampling and a Gaussian copula representation to preserve correlation

3

f
f
f
f
between the estimated AR coefﬁcients.

This paper is structured as follows. Section 2 outlines our modeling approach giving bias-
corrected estimators of the ﬁrst-lag autocorrelation coefﬁcient of AR(1) processes and derive
their sampling distributions. In Section 3, the suggested approach is extended to give bias cor-
rection and approximate sampling distributions in estimating the coefﬁcients of AR(2) processes.
Section 4 illustrates bias correction for a real ecological data set, where the autoregressive co-
efﬁcients are used to characterize density dependence and population dynamics. Concluding
remarks are given in Section 5. The appendix 6 describes our accompanying R-package which
can be used to obtain the bias-corrected estimates and 95% conﬁdence intervals for time series
of length n = 10, 11, . . . , 50, using the four mentioned original estimators.

2 Bias correction and ﬁnite-sample properties for AR(1) models

The AR(1) model has a simple one-step dependence as deﬁned by (1) where p = 1. The bias in
estimating the ﬁrst-order autocorrelation coefﬁcient φ1 = φ has been studied by several authors,
see e.g Krone et al. (2017) for a recent comparative simulation study of AR(1) estimators in
short time series. The bias of the exact MLE is illustrated for different sample sizes in Figure 2,
giving empirical averages for 10000 simulations for a ﬁne grid of φ-values in the stationary area
(−1, 1).

1
.
0

0
.
0

1
.
0
−

2
.
0
−

3
.
0
−

4
.
0
−

s
a
b

i

l

a
c
i
r
i
p
m
E

−1.0

−0.5

0.0

0.5

1.0

Figure 2: The estimated empirical bias ˆφ − φ of the exact MLE for AR(1) processes with length
n = 10 (black), n = 15 (red), n = 20 (green), n = 30 (blue), n = 40 (light blue) and n = 50
(pink).

A common way to explicitly construct an unbiased estimate for a parameter φ is simply to
subtract an estimate of the bias from the original estimator, providing a bias-corrected estimator
of the form

ˆφc = ˆφ − ˆE( ˆφ − φ).

For example, the exact MLE can be corrected using the asymptotic bias −(1 + 3φ)/n (Tanaka,
1984; Cordeiro and Klein, 1994). Naturally, such a linear correction would not be accurate
enough for small sample sizes. In Arnau and Bono (2001), the ordinary autocorrelation estima-
tor for φ is bias-corrected by adding the absolute value of a polynomial ﬁt to the empirical bias.
This has a slight resemblance to our approach, but an important difference is that we do not try

4

f
 
to estimate or model the bias. We model the true parameter value φ directly as a function of
original estimates based on a huge number of simulations. Similarly, we provide approximate
sampling distributions by modelling parameters of skew-normal approximations as functions of
the true values of φ.

2.1 Deriving bias-corrected estimators by simulation

Let ˆφ denote an original estimator for the ﬁrst-lag autocorrelation coefﬁcient of AR(1) processes.
Our goal is to construct a bias-corrected estimator for φ such that E( ˆφc) = φ for all values of
φ. To achieve this we model the relationship between the true and estimated parameter values
using a weighted orthogonal polynomial regression model. The coefﬁcients of the regression
model are derived by minimizing the weighted squared error between the corrected estimate and
true parameter values. The speciﬁc steps in constructing the bias-corrected estimator can be
summarized as follows:

1. To avoid constraints on the support of φ, we ﬁrst introduce a monotonic transformation

g(φ) = logit

φ + 1
2

,

(cid:19)

(cid:18)

(2)

which has inﬁnite support. This facilitates optimization and implies that our inverse trans-
formed bias-corrected estimate will always be within the stationary area of the AR(1)
process.

2. Let ˆφ denote an original estimator of φ. We model the true AR coefﬁcient using an

orthogonal polynomial model

φ = f ( ˆφ, β) = g

−1

K

k=0
X

βkhk(g( ˆφ))

!

,

ˆφ ∈ (−1, 1)

(3)

k=0 denotes a ﬁxed set of regression coefﬁcients while {hk(.)}K

where β = {βk}K
k=0 repre-
sents a set of orthogonal polynomials of order k. Here, we choose to use the probabilists’
Hermite polynomials which are orthogonal with respect to the standard normal density.
These polynomials are deﬁned by

h0(x) = 1,

h1(x) = x,

hk+1(x) = xhk(x) − khk−1(x),

k ≥ 1.

(4)

3. To estimate β for a given sample size n, we generate a total of m = 10000 time series
for a ﬁne grid of φ-values. Note that the suggested estimator is a non-linear function of ˆφ
implying

E(f ( ˆφ, β)) 6= f (E( ˆφ), β).

This means that the optimization takes the estimated value for each time series into ac-
count not just the average estimate of the m simulations for each φ. The regression coef-
ﬁcients are thus found by solving the optimization problem

ˆβ = arg min
β

= arg min

β

l

r=1
X
l

r=1
X

1
s2
r 

1
m



1
s2
r 

1
m



m

j=1
X
m

j=1
X

5

−1

g

K

Xk=0

βkhk(g( ˆφrj ))

!

− φr

f ( ˆφrj, β) − φr

2

.





2





(5)

 
 
The quantity ˆφrj represents the estimate of φr in simulation j. Speciﬁcally, we choose
the grid φr ∈ (−0.95, −0.94, . . . , 0.95), implying that l = 191. The ordinary sample
variances s2
r are used as weights. This gives a unique set of regression coefﬁcients ˆβ for
each sample size n and for each original estimator ˆφ, implying that ˆβ accounts for the
sampling distribution of the estimator ˆφ. The bias-corrected estimator is then given by
ˆφc = f ( ˆφ, ˆβ) which is used to predict the true value φ.

In minimizing (5), we have chosen to exclude values of φ close to the edges of the stationary
interval. This is to avoid a severe inﬂation of the variance caused by forcing the estimator to
give unbiased estimates at the edges of the interval. In practice, the given approach can be used
to ﬁnd corrected estimates for any estimator ˆφ giving values within the stationary range. The
corrected estimates might be equal to ±1, but will never fall outside of the interval [−1, 1].
Our implementation includes the exact and conditional MLEs, Burg’s algorithm and the Yule-
Walker solution. We have stored all the sets of regression coefﬁcients for these four estimators
for AR(1) series of length n = 10, 11, . . . , 50, and the resulting bias-correction is available from
our R-package, see Section 6.

2.2 Bias-correcting curves

Figure 3 illustrates how the correction works for AR(1) processes of length n = 15 and n = 30
for a ﬁne grid of estimated φ values in the interval (-1,1). The given curves correspond to using
the exact MLE, Burg’s algorithm and the Yule-Walker solution where the corrected estimators
are calculated using up to cubic Hermite polynomials (K = 3). For an original estimate ˆφ given
at the horizontal axis, we calculate the corresponding corrected estimate ˆφc at the vertical axis.
Naturally, this gives a quite large bias-correction when n = 15, where original estimates above
0.5 are corrected to a value close to 1. In the case of n = 30, we notice that the correction curves
are close to linear for an internal subset of the interval.

e
t
a
m

i
t
s
e
d
e
t
c
e
r
r
o
C

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

e
t
a
m

i
t
s
e
d
e
t
c
e
r
r
o
C

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

−1.0

−0.5

0.0

0.5

1.0

−1.0

−0.5

0.0

0.5

1.0

Original estimate

Original estimate

Figure 3: The computed correction curves when n = 15 (left) and n = 30 (right) for the exact
MLE (black), Burg’s method (red) and the Yule-Walker solution (blue).

The original and the corresponding corrected average estimates for m = 10000 simulations
are displayed in Figure 4. Using the correction, we do get close to unbiased results both when
n = 15 and n = 30. The overall average bias and sample variance for the estimators are given

6

 
 
in Table 1. We also compute the overall root mean squared error (RMSE), which in the case of
the corrected estimator is deﬁned by

RMSE( ˆφc) =

l

m

2

ˆφc,rj − φr

1
ml

v
u
u
t

(cid:17)

j=1 (cid:16)
X

r=1
X
where ˆφc,rj is the corrected estimate of φr in simulation j. The RMSE for the original estimator
and the bias and variances are computed correspondingly. The results illustrate the well-known
bias-variance trade-off showing that a decrease in the bias of an estimator will inherently cause
an increase in the variance. For time series of length n = 15, the choices we have made imply
that RMSE is slightly larger for the corrected estimator versus the originals. When n = 30, we
get approximately unbiased results only causing a negligible increase in RMSE. In minimizing
(5), we could have chosen to exclude more of the φ-values at the ends of the unit interval, e.g.
using φ ∈ (−0.9, 0.9). This would reduce the variance but naturally also increase the bias close
to the limits of the interval (−1, 1).

e
t
a
m

i
t
s
e

e
g
a
r
e
v
A

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

e
t
a
m

i
t
s
e

e
g
a
r
e
v
A

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

−1.0

−0.5

0.0

0.5

1.0

−1.0

−0.5

0.0

0.5

1.0

Figure 4: The average original (dashed) and resulting corrected average estimates (solid) for
10000 simulations, generating time series of length n = 15 (left) and n = 30 (right). The esti-
mators used include the exact MLE (black), Burg’s method (red) and the Yule-Walker solution
(blue).

2.3 Finite-sample distributions for the original and corrected estimators

Commonly applied estimators for the coefﬁcients of AR(p) processes are asymptotically normal
(Hannan, 1970) but the ﬁnite-sample distributions of these estimators have not been thoroughly
studied. Figure 5 illustrates the sampling distribution for the logit-transformation g( ˆφ) where ˆφ
is the exact MLE for AR(1) processes of length n = 30 and where the underlying true values
are φ ∈ (−0.9, −0.3, 0, 3, 0.9). The ﬁtted curves are skew-normal densities which are seen to
give very good approximations to the given sampling distributions. These have been ﬁtted using
the function fGarch:::snormFit in R (Wuertz et al., 2020), implementing the skew-normal
density as deﬁned by Fernandez and Steel (1998), i.e.

πsn(x) =

2
ξ + 1
ξ

(πG(x/ξ)H(x) + πG(xξ)H(−x)) .

(6)

7

f
 
f
 
Method

n=15 Exact MLE

-0.080
Conditional MLE -0.080
Burg’s method
-0.081
-0.079
Yule-Walker
-0.037
n=30 Exact MLE
Conditional MLE -0.037
Burg’s method
-0.038
-0.038
Yule-Walker

Original estimator
Bias Variance RMSE
0.263
0.055
0.263
0.055
0.263
0.052
0.265
0.046
0.171
0.026
0.171
0.026
0.172
0.025
0.175
0.024

Bias-corrected estimator
Bias Variance RMSE
0.286
0.084
0.286
0.084
0.288
0.085
0.296
0.089
0.174
0.031
0.174
0.031
0.175
0.031
0.178
0.032

-0.0015
-0.0015
-0.0015
-0.0013
-0.0004
-0.0004
-0.0005
-0.0005

Table 1: The average bias, variance and root mean square error for the original and bias-corrected
estimators of φ when n = 15 and n = 30. The averages are computed based on m = 10000
simulations for each value of φ ∈ (−0.95, −0.94, . . . , 0.95).

The function πG(.) denotes the standard normal density while H(x) is the ordinary Heaviside
or unit step function. In addition to the skewness parameter ξ > 0, the skew-normal density is
parameterized in terms of a mean µ and standard deviation σ, using an input argument (x−µ)/σ.

y
t
i
s
n
e
D

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

y
t
i
s
n
e
D

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

y
t
i
s
n
e
D

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

y
t
i
s
n
e
D

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

−4

−2

0
g(f^)

2

4

−4

−2

0
g(f^)

2

4

−4

−2

0
g(f^)

2

4

−4

−2

2

4

0
g(f^)

Figure 5: Sampling distributions for g( ˆφ) where ˆφ is the exact MLE. The dashed lines represent
the true values of g(φ) where φ ∈ (−0.9, −0.6, 0.6, 0.9).

Applying the skew-normal approximation, we can derive ﬁnite-sample distributions for the
estimators ˆφ and the corresponding bias-corrected estimators ˆφc. Let ˜πsn(.) denote the skew-
normal approximation for the logit transformation g( ˆφ). The approximate sampling distribution
for the original estimator is easily expressed analytically by the ordinary change of variable
transformation

˜π( ˆφ) = ˜πsn(g( ˆφ))

(1 + ˆφ)

−1 + (1 − ˆφ)

−1

.

Likewise, an approximation to the sampling distribution for the corrected estimator can be de-
rived numerically as

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

˜π( ˆφc) = ˜πsn(s( ˆφc))

ds( ˆφc)
d ˆφc

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where s( ˆφc) represents a spline function approximating the monotonic relationship between ˆφc
and g( ˆφ). The resulting sampling distributions for the original and corrected estimators are
shown in Figure 6, where n = 30 and φ ∈ (−0.9, −0.6, 0, 6, 0.9).

For each of the estimators and each n, we can now model the parameters of the skew-normal
approximation for g( ˆφ) as a function of φ. This is done using a third order orthogonal polyno-

8

y
t
i
s
n
e
D

y
t
i
s
n
e
D

0
1

8

6

4

2

0

0
1

8

6

4

2

0

y
t
i
s
n
e
D

0
1

8

6

4

2

0

y
t
i
s
n
e
D

0
1

8

6

4

2

0

y
t
i
s
n
e
D

0
1

8

6

4

2

0

−1.0

−0.5

0.0

f^

0.5

1.0

−1.0

−0.5

y
t
i
s
n
e
D

0
1

8

6

4

2

0

−1.0

−0.5

0.0

f^

c

0.5

1.0

−1.0

−0.5

0.0

f^

0.0

f^

c

0.5

1.0

−1.0

−0.5

y
t
i
s
n
e
D

0
1

8

6

4

2

0

0.5

1.0

−1.0

−0.5

0.0

f^

0.0

f^

c

0.5

1.0

−1.0

−0.5

0.5

1.0

0.0

f^

y
t
i
s
n
e
D

0
1

8

6

4

2

0

0.5

1.0

−1.0

−0.5

0.5

1.0

0.0

f^

c

Figure 6: Sampling distributions for ˆφ (upper panels) and ˆφc (lower panels) where ˆφ is the exact
MLE. The dashed lines give the true values of φ ∈ (−0.9, −0.6, 0.6, 0.9).

mial regression model similar to (3), now using the parameters of the skew-normal approxima-
tion as the response variables. Speciﬁcally, let θ = (µ, σ, ξ). The predicted value of each of the
parameters is then deﬁned by

K

ˆθs =

ˆbk,shk(g(φ)),

s = 1, 2, 3,

Xk=0

(7)

where the coefﬁcients {ˆbk,s} are found by the ordinary least squares approach using a polyno-
mial order of K = 3. Figure 7 illustrates the relationships between each of the skew-normal
parameters and φ using the exact MLE when n = 30. The red curves illustrate the ﬁtted curves
deﬁned by (7). Especially, we notice that the skewness parameter is quite close to 1 for the
interior of the given interval, implying that the given distributions for g( ˆφ) are not very far from
being Gaussian. The skewness increases as φ increases towards the upper limit of the stationary
area.

n
a
e
M

2

0

2
−

4
−

 s
n
o
i
t

i

a
v
e
d
d
r
a
d
n
a
S

t

2
.
1

0
.
1

8

.

0

6
0

.

4
0

.

0
2
.
1

5
1
.
1

0
1

.

1

5
0
1

.

0
0
1

.

5
9
0

.

 x
s
s
e
n
w
e
k
S

−1.0

−0.5

0.0

0.5

1.0

−1.0

−0.5

0.0

0.5

1.0

−1.0

−0.5

0.0

0.5

1.0

Figure 7: The mean, standard deviation and skewness parameter of the skew-normal approxi-
mation for g(φ) as smoothed functions (red) of φ using the exact MLE when n = 30.

To predict the skew-normal parameters for a new AR(1) series we use the estimates ˆφ as
plug-in estimates for φ in (7). We then obtain conﬁdence intervals for the estimates ˆφ and ˆφc

9

f
 
m
f
 
f
by Monte Carlo simulations. To investigate the coverage probability of the resulting conﬁdence
intervals, we have performed a simulation study in which we generated 10000 AR(1) process
with a uniformly drawn coefﬁcient, i.e. φ ∼ Uniform(−1, 1). The simulation study was per-
formed for sample sizes n = 10, 15, 20, 30, 40, 50, in which the coefﬁcient φ was estimated
using each of the four original methods. We then calculated the bias-corrected estimates and
found 95% equi-tailed conﬁdence intervals for all cases by Monte Carlo simulation. The results
demonstrate that the coverage probabilities for the original estimators are below the nominal
level of 0.95 for all estimators and all sample sizes, see Table 2. Especially, the nominal level
using the Yule-Walker solution is below 0.90 for all sample sizes. Using the bias-corrected esti-
mators, the coverage properties are clearly better being quite close to the nominal level of 0.95
in all cases. For the smaller sample sizes, this can partly be explained by the larger variance of
the bias-corrected estimators giving wider conﬁdence intervals. For the large sample sizes, the
conﬁdence lengths are approximately the same.

Exact MLE
ˆφc
ˆφ
0.9771
0.8092
0.9741
0.8326
0.9644
0.8463
0.9527
0.8617
0.9506
0.8728
0.9474
0.8813

Conditional MLE

ˆφ
0.8097
0.8330
0.8468
0.8608
0.8773
0.8878

ˆφc
0.9768
0.9736
0.9635
0.9532
0.9502
0.9457

Burg’s method
ˆφc
0.9829
0.9754
0.9620
0.9508
0.9488
0.9456

ˆφ
0.7798
0.8012
0.8233
0.8460
0.8621
0.8722

Yule-Walker
ˆφ
ˆφc
0.9942
0.6692
0.9791
0.7198
0.9624
0.7415
0.9476
0.7814
0.9437
0.8088
0.9396
0.8234

n
10
15
20
30
40
50

Table 2: Coverage probabilities of 95% conﬁdence intervals using the original estimators ˆφ and
the respective bias-corrected versions ˆφc for a total of 10000 simulations where the true value
φ is randomly generated from (−1, 1). The conﬁdence intervals are found by Monte Carlo
sampling after ﬁtting a skew-normal approximation to g( ˆφ).

3 Bias correction and sampling distribution in the AR(2) case

In this section we extend the given model-based approach to construct bias-corrected estimators
for the pair of coefﬁcients (φ1, φ2) of an AR(2) process. This is far more computationally expen-
sive than in the AR(1) case as we need to generate time series for a ﬁne two-dimensional grid of
the coefﬁcients in the triangular stationary area. In total we have ﬁtted a weighted polynomial
regression model to more than 2.43 billion time series, providing regression coefﬁcients which
can be used to bias-correct estimators for sample sizes from n = 10, 11, . . . , 50. As previously,
the original estimators used include the exact and conditional MLE, Burg’s algorithm and the
Yule-Walker solution. In addition, we obtain approximate sampling distributions by construct-
ing a Gaussian copula where the marginals are generated as transformations of skew-normal
densities and combine this with Monte Carlo simulations.

3.1 Modeling approach in two dimensions

The AR(2) process is deﬁned by (1) where p = 2 and it is stationary within the triangular area
constrained by φ2 +|φ1| < 1 where |φ2| < 1. A more appealing parameterization of this process

10

is given by the partial autocorrelations,

ψ1 =

φ1
1 − φ2

, ψ2 = φ2,

(8)

as the stationary area of the AR(2) process is then deﬁned by the square ψi ∈ (−1, 1), i = 1, 2.
1(1 − ψ2)2 +
The area in which the process has pseudo-periodic behavior is characterized by ψ2
4ψ2 < 0.

We now extend the algorithm in Section 2 to construct bias-corrected estimators ( ˆφc,1, ˆφc,2),
again taking the sampling distribution of the original pair of estimators ( ˆφ1, ˆφ2) into account.
In this case we estimate the parameters of the regression model by minimizing the squared
error between the corrected and true partial autocorrelations. This is computationally beneﬁcial
to avoid the triangular constraints on φ1 and φ2. Also, the correlation between ˆψ1 and ˆψ2 is
much smaller than the corresponding correlation between the φ-coefﬁcients. Naturally, this only
makes a difference for the ﬁrst coefﬁcient as φ2 = ψ2. The algorithm extending the weighted
polynomial regression model to two dimensions can be summarized as follows:

1. Using the logit-transformation in (2), the underlying true partial autocorrelations are mod-

eled by

ψi = f ( ˆψ1, ˆψ2, βi) = g

−1



K

K−k

Xk=0

q=0
X



βk,q,ihk,q(g( ˆψ1), g( ˆψ2))



,

i = 1, 2

(9)

where hk,q(g( ˆψ1), g( ˆψ2)) = hk(g( ˆψ1))hq(g( ˆψ2)) denotes the product of Hermite polyno-
mials of order k and q. Notice that the two partial autocorrelations are modeled separately,
giving separate sets of regression coefﬁcients βi = {βk,q,i} for i = 1, 2. However, each of
the true partial autocorrelations need to be modeled in terms of the estimated pair ( ˆψ1, ˆψ2)
as these parameters are not independent.



2. Due to the dependence, the regression coefﬁcients β = {β1, β2} of the given predic-
tors for ψ1 and ψ2 are found simultaneously. This is achieved by solving the following
optimization problem:

ˆβ = arg min
β

= arg min

β

l

2

r=1
X
l

i=1
X
2

r=1
X

i=1
X

1
m

1
m

m

j=1
X
m

j=1
X

1
s2
ri 


1
s2
ri 


−1

g

K

K−k





Xk=0

q=0
X

f ( ˆψrj1, ˆψrj2, βi) − ψri

2

.







βk,q,ihk,q(g( ˆψrj1), g( ˆψrj2))



− ψri

The values ( ˆψrj1, ˆψrj2) denote the original estimates for the rth pair of the partial auto-
correlations in simulation j while s2
ri denotes the sample variances for the m simulations
in each case. The value l denotes the total number of pairs of the partial autocorrelations
that are included in the minimization.

2





(10)

In solving (10), we needed to generate time series for a ﬁne two-dimensional grid of the pa-
rameter values (ψ1, ψ2) within the square deﬁning the stationary area. Speciﬁcally, our estimate
ˆβ is based on using the grid ψi ∈ (−0.95, −0.925, . . . , 0.95), i = 1, 2. This gives a total of
l = 772 = 5929 different combinations of the partial autocorrelations. For each pair (ψ1, ψ2),

11

we generated m = 10000 times series of a speciﬁc length n implying that the regression coef-
ﬁcients are estimated based on approximately 59 million time series. This was repeated for all
sample sizes n = 10, 11, . . . , 50, such that the total number of generated time series is equal to
772 · 41 · 10000 = 2430890000 or approximately 2.43 billion time series for a given value of
K. We then saved the regression coefﬁcients for each sample size and for each of the original
estimation methods providing bias-corrected estimators

ˆφc,i = g

−1

f ( ˆψ1, ˆψ2, ˆβi)
(cid:17)
(cid:16)

,

i = 1, 2.

As in the AR(1) case, the estimated coefﬁcients might fall at the border of the stationary area
but not outside.

If run sequentially on an ordinary single-core laptop, the given brute-force simulation ap-
proach to compute the bias correction would be computationally infeasible as it would take
approximately 5 years of CPU time. The computations were therefore done on the Ibex cluster
at KAUST (https://www.hpc.kaust.edu.sa/ibex), which reduced the time down to about 2 days.

3.2 Properties of the bias-corrected estimators for AR(2)

By using the partial autocorrelations in (10), the resulting corrected estimators are not con-
structed to be completely unbiased. However, in addition to the numerical advantages, we have
noticed that this gives a smaller variance and RMSE compared to performing the optimization
with respect to the φ-coefﬁcients. In constructing the bias-corrected estimator it is important
that we also consider the ordinary bias-variance trade-off. This is also relevant in determining
the maximum order K of the Hermite polynomials in (10) as increasing values of K naturally
give decreased bias but higher variance.

The estimated partial autocorrelations using the exact MLE and the corresponding bias-
corrected estimates when K = 3 and K = 7 are shown for sample sizes n = 15 and n = 30 in
Figure 8 – 9, respectively. Visually, the bias properties using K = 3 and K = 7 are quite sim-
ilar. Both of these corrected estimators are very accurate in estimating ψ2 but show some bias
in estimating ψ1, especially for the upper part of the square. When K = 7, the predictor in (10)
will have a total of 36 terms for each of the parameters ψ1 and ψ2. This reduces the overall bias
slightly compared with using K = 3, but it also creates some instability in the estimates and the
variance increases. Using K = 3, the number of regression coefﬁcients for each parameter is
reduced to 10.

To further study the properties of the given estimators, we have calculated the overall average
bias, variance and the RMSE of these estimators when K = 3 and K = 7. The calculations
are based on taking the averages for the m simulations for each of the l = 5929 combinations
of (ψ1, ψ2), which are easily transformed to give estimates for the AR coefﬁcients by (8). In
calculating the overall averages we have added the results for both parameters. For example,
RMSE for the bias-corrected estimator is given by

RMSE(( ˆφc,1, ˆφc,2)) =

1
2l

l

2

r=1
X

i=1
X

1
m

m

j=1
X

( ˆφc,rji − φri)2.

v
u
u
t

The bias and the variance are calculated correspondingly. Table 3 summarizes the overall aver-
age bias, variance and the RMSE for the original and bias-corrected estimators using K = 3.
For comparison, we have also included RMSE when K = 7. The bias-corrected estimators do
have a smaller bias and larger variance than the original estimators, but the improved bias prop-
erties do not seem to appear at the expense of any signiﬁcant increase in RMSE. When n = 30,

12

8
.
0
−

5
.
0

2

0
.
0

  − 0 . 6  

 −0.4 

  − 0 . 2  

 0 

  0 . 2  

 0.4 

0

0
.
2

0
.
4

0
.
6

0
.
8

 0.6 

 0.8 

5
.
0
−

−
0
.
8

−
0
.
6

−
0
.
4

−
0
.
2

−0.5

0.5

0.0
y 1

 0.2 

 −0.4 

 −0.8 

−0.5

 0.4 

 0 

 −0.2 

 −0.6 

0.5

0.0
y 1

  0 . 6  

 0.8 

 0.6 

 0.4 

 0.2 

 0 

 −0.2 

 −0.4 

 −0.6 

 −0.8 

5
.
0

2

0
.
0

5
.
0
−

1.0

0.5

0.0

−0.5

−1.0

1.0

0.5

0.0

−0.5

−1.0

5
.
0

2

0
.
0

−

0

.

8

−
0

.

6

  − 0 . 4  

 −0.2 

 0.2 

 0 

  0 . 4  

  0 . 6  

0

0
.
2

0
.
4

0
.
6

0
.
8

8
.
0

5
.
0
−

−
0
.
8

−
0
.
6

−
0
.
4

−
0
.
2

−0.5

0.5

0.0
y 1

 0.8 

 0.6 

5
.
0

2

0
.
0

5
.
0
−

 0.4 

 0.4 

 0.2 

 0.2 

 0 

 0 

 −0.2 

 −0.4 

 −0.4 

 −0.6 

 −0.8 

 0.8 

 0.6 

 −0.2 

 −0.6 

 −0.8 

0.5

−0.5

0.0
y 1

1.0

0.5

0.0

−0.5

−1.0

1.0

0.5

0.0

−0.5

−1.0

 −0.6 

  − 0 . 4  

 −0.2 

 0 

 0.2 

  0 . 4  

 0.6 

0

0
.
2

0
.
4

0
.
6

0
.
8

 0.8 

5
.
0

−

0

.

8

2

0
.
0

5
.
0
−

−
0
.
8

−
0
.
6

−
0
.
4

−
0
.
2

−0.5

0.5

0.0
y 1

5
.
0

2

0
.
0

5
.
0
−

 0.8 

 0.6 

 0.4 

 0.2 

 0 

 −0.2 

 −0.4 

 −0.6 

 −0.8 

 0.8 

 0.6 

 0.2 

 −0.6 

 −0.8 

0.0
y 1

 0.4 

 −0.2 

−0.5

 0 

 −0.4 

0.5

1.0

0.5

0.0

−0.5

−1.0

1.0

0.5

0.0

−0.5

−1.0

Figure 8: Results for n = 15. The mean estimated values of ψ1 (upper) and ψ2 (lower) using
the exact MLE (left), the bias-corrected estimator with K = 3 (middle) and the bias-corrected
estimator with K = 7 (right).

the RMSE is smaller for the bias-corrected versus the original estimators. When n = 15, the
increase in RMSE for the bias-corrected estimators is very small for the MLEs. When K = 7,
the RMSE of the corrected estimators increase in all cases. We have also investigated properties
of the bias-corrected estimator when K = 5 but our results conﬁrm that K = 3 is a reasonable
choice.

3.3 Sampling distributions using a Gaussian copula representation

Similar to the AR(1) case, we move on to study sampling distributions for the estimators of the
parameters of the AR(2) model. For each method and each sample size, we have ﬁtted skew-
normal densities to all of the r generated samples of g( ˆψr,1) and g( ˆψr,2) where r = 1, . . . , l.
Figure 10 illustrates the skew-normal approximation for a few selected pairs of the transformed
original estimators where the true underlying partial autocorrelations were set equal to ψi =
±0.6. The skew-normal densities seem to give quite accurate approximations of the sampling
distributions also in the AR(2) case.

The next step is to ﬁnd approximations of the sampling distributions for the original non-
transformed estimators ( ˆφ1, ˆφ2) and the bias-corrected estimators ( ˆφc,1, ˆφc,2). This implies
that for any pair (g( ˆψ1), g( ˆψ2)), we need to predict the corresponding values for the param-
eters of the skew-normal densities. We also need a predicted value of the correlation ˆρ =
Cor(g( ˆψ1), g( ˆψ2)). We choose to do this using an orthogonal polynomial regression model
similar to (9) where each of the seven parameters is modeled in terms of (ψ1, ψ2), in separate re-
gression models. Denote the parameters of the ﬁtted skew-normal densities by θi = (µi, σi, ξi),

13

y
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
y
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
y
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
y
y
y
  − 0 . 6  

 −0.4 

  0  

 −0.2 

  0 . 2  

  0 . 4  

 0.6 

0

0
.
2

0
.
4

0
.
6

0
.
8

8
.
0

5
.
0

−
0

.

8

2

0
.
0

5
.
0
−

−
0
.
8

−
0
.
6

−
0
.
4

−
0
.
2

−0.5

0.5

0.0
y 1

5
.
0

2

0
.
0

5
.
0
−

 0.8 

 0.6 

 0.4 

 0.2 

 0 

 −0.2 

 −0.4 

 −0.6 

 −0.8 

 0.6 

 0.4 

 0.2 

 0 

 −0.2 

 −0.4 

 −0.6 

 −0.8 

−0.5

0.5

0.0
y 1

1.0

0.5

0.0

−0.5

−1.0

1.0

0.5

0.0

−0.5

−1.0

−

0

.

4

5
.
0

  − 0 . 2  

 0 

 0.2 

  0 . 4  

  0 . 6  

 0.8 

2

0
.
0

−
0

.

8

−
0

.

6

0

0
.
2

0
.
4

0
.
6

0
.
8

5
.
0
−

−
0
.
8

−
0
.
6

−
0
.
4

−
0
.
2

−0.5

0.5

0.0
y 1

 0.8 

 0.8 

5
.
0

2

0
.
0

5
.
0
−

 0.6 

 0.4 

 0.2 

 0 

 −0.2 

 −0.4 
 −0.4 

 −0.6 

 −0.8 

 0.6 

 0.4 

 0.2 

 0 

 −0.2 

 −0.6 

 −0.8 

0.0
y 1

0.5

−0.5

1.0

0.5

0.0

−0.5

−1.0

1.0

0.5

0.0

−0.5

−1.0

  − 0 . 4  

 −0.2 

6
.
0
−

 0 

 0.2 
  0 . 4  

 0.6 

 0.8 

8
.
0
−

0

0
.
2

0
.
4

0
.
6

0
.
8

5
.
0

2

0
.
0

5
.
0
−

−
0
.
8

−
0
.
6

−
0
.
4

−
0
.
2

−0.5

0.5

0.0
y 1

5
.
0

2

0
.
0

5
.
0
−

 0.8 
 0.8 

 0.6 

 0.4 

 0.2 

 0 

 −0.2 

 −0.4 

 −0.6 

 −0.8 

 0.6 

 0.4 

 0.2 

 0 

 −0.8 

−0.5

 −0.4 

 −0.6 

0.0
y 1

 −0.2 

0.5

1.0

0.5

0.0

−0.5

−1.0

1.0

0.5

0.0

−0.5

−1.0

Figure 9: Results for n = 30. The mean estimated values of ψ1 (upper) and ψ2 (lower) using
the exact MLE (left), the bias-corrected estimator with K = 3 (middle) and the bias-corrected
estimator with K = 7 (right).

i = 1, 2. Speciﬁcally, the predicted value of a parameter θs ∈ (θ1, θ2, ρ) is

K

K−k

ˆθs =

ˆbk,q,shk,q(g(ψ1), g(ψ2))

k=0
X

q=0
X

(11)

where the coefﬁcients {ˆbk,q,s} are found by the ordinary least squares approach. Using K = 3,
this gives a set of 10 regression coefﬁcients {bk,q,s} for each of the seven parameters, stored for
all methods and sample sizes.

In generating samples for the original and bias-corrected estimators, we need to preserve the
correlation between the transformed partial autocorrelations. This can be done by constructing
a two-dimensional Gaussian copula by

C(u1, u2) = Φˆρ(F

−1
sn (u1; ˆθ1), F

−1
sn (u2; ˆθ2)).

The function Φˆρ(.) denotes the joint cumulative distribution of a bivariate standard normal vector
with correlation between the components being equal to ˆρ. The functions Fsn(.) represent the
skew-normal cumulative distribution functions (cdf) of g( ˆψ1) and g( ˆψ2). By the probability
integral transform,

x = (F

−1
sn (u1; ˆθ1), F

−1
sn (u2; ˆθ2))

represents samples from the given skew-normal densities where the uniformly distributed vari-
ables u1 and u2 are generated from the inverse cdf of the standard normal marginals of the
bivariate distribution.

14

y
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
y
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
y
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
y
y
y
Method

Original estimator
Bias Variance RMSE

n=15 Exact MLE

-0.127
Conditional MLE -0.126
-0.119
Burg’s method
Yule-Walker
-0.124
-0.061
n=30 Exact MLE
Conditional MLE -0.061
-0.061
Burg’s method
Yule-Walker
-0.064

0.068
0.068
0.064
0.054
0.029
0.029
0.029
0.028

0.311
0.312
0.306
0.334
0.186
0.186
0.188
0.207

Bias-corrected estimator

Bias Variance RMSE RMSE
K = 3 K = 3 K = 7
0.332
0.317
0.319
0.318
0.339
0.323
0.401
0.352
0.188
0.182
0.190
0.183
0.194
0.184
0.208
0.199

0.099
0.099
0.102
0.121
0.034
0.034
0.035
0.040

K = 3
-0.017
-0.017
-0.018
-0.019
-0.005
-0.005
-0.005
-0.006

Table 3: Bias, variance and root mean square error for the original estimators of (φ1, φ2) and
the corrected estimator for n = 15 and n = 30. These results are calculated using all the 10000
simulations for each of the selected pairs in (φ1, φ2) within a ﬁne grid of the stationary area.

The resulting samples for ( ˆφ1, ˆφ2) and ( ˆφc,1, ˆφc,2) can be used to ﬁnd conﬁdence intervals
for (φ1, φ2). We performed a similar simulation study as in the AR(1) case, generating 10000
AR(2) processes where the partial autocorrelation coefﬁcients are drawn randomly from (-1,1).
The coverage probabilities of the estimated 95% conﬁdence intervals are given in Table 4 for
sample sizes n = 15 and n = 30. We do notice that the coverage probabilities using the original
estimators are smaller than the nominal level in all cases. In particular, the coverage is very low
for φ2 giving values below 0.80 also when n = 30. The conﬁdence intervals using the bias-
corrected estimators have coverage probabilities quite close to the nominal levels being within
the interval 0.95 ± 0.03.

Original estimator Bias-corrected estimator

n Method
15 Exact MLE

φ1
0.8965
Conditional MLE 0.8967
Burg’s method
0.8940
0.7896
Yule-Walker
0.9138
30 Exact MLE
Conditional MLE 0.9139
Burg’s method
0.9100
0.8335
Yule-Walker

φ2
0.7454
0.7474
0.7268
0.5485
0.7815
0.7819
0.7609
0.6133

φ1
0.9398
0.9413
0.9485
0.9507
0.9459
0.9459
0.9500
0.9360

φ2
0.9637
0.9656
0.9687
0.9778
0.9443
0.9459
0.9438
0.9217

Table 4: Coverage for 95% conﬁdence intervals for φ1 and φ2 found by sampling using a Gaus-
sian copula and transformations of skew-normal densities. The results are based on 10000 sim-
ulations where ψ1 and ψ2 are randomly generated from (−1, 1).

4 Application in ecology

Wildlife ecological research studies are often characterized by small sample sizes (Bissonette,
1999). This can be due to sparse distributions of the animal species of interest and also the

15

y
t
i
s
n
e
D

y
t
i
s
n
e
D

5
.
1

0
.
1

5
.
0

0
.
0

5
.
1

0
.
1

5
.
0

0
.
0

y
t
i
s
n
e
D

5
.
1

0
.
1

5
.
0

0
.
0

−4

−2

0
g(y ^

)
1

2

4

−4

−2

2

4

0
g(y ^

)

2

y
t
i
s
n
e
D

5
.
1

0
.
1

5
.
0

0
.
0

y
t
i
s
n
e
D

y
t
i
s
n
e
D

5
.
1

0
.
1

5
.
0

0
.
0

5
.
1

0
.
1

5
.
0

0
.
0

y
t
i
s
n
e
D

5
.
1

0
.
1

5
.
0

0
.
0

−4

−2

0
g(y ^

)

1

2

4

−4

−2

2

4

0
g(y ^

)
2

y
t
i
s
n
e
D

5
.
1

0
.
1

5
.
0

0
.
0

−4

−2

0
g(y ^

)
1

2

4

−4

−2

0
g(y ^

)

2

2

4

−4

−2

0
g(y ^

)

1

2

4

−4

−2

2

4

0
g(y ^

)
2

Figure 10: The marginal sampling distributions for g( ˆψ1) and g( ˆψ2) for different combinations
of (ψ1, ψ2). These combinations include the values (−0.6, −0.6) (upper left pair), (−0.6, −0.6)
(upper right pair), (−0.6, 0.6) (bottom left pair) and (0.6, 0.6) (bottom right pair). The dotted
vertical lines give the true values for g(ψ1) and g(ψ2).

research design in which data are collected by ﬁeldwork. As expressed by Ives et al. (2010):
“ While a time series covering 40 years might represent an ecologist’s entire career, such time
series are short for statistical purposes”. Both AR(1) and AR(2) processes have commonly been
used as models for ecological time series, for example modeling population cycles of different
animal species like small rodents (Bjørnstad et al., 1995; Hansen et al., 1999). In such studies,
the ﬁrst and second-order coefﬁcients are interpreted in terms of direct and delayed intra-speciﬁc
dependence, respectively.

To demonstrate the effect of bias correction for a real world example, we consider a dataset
on gray-sided voles (Myodes rufocanus), collected by the Japanese Forestry Agency at 85 dif-
ferent sites in Hokkaido, Japan (Saitoh et al., 1998). This dataset has been extensively studied
and AR(2) model approximations have been used to assess density dependence, periodicities
and synchrony of the vole populations (Stenseth et al., 2003; Hugueny, 2006; Cohen and Saitoh,
2016). The dataset includes time series for the raw counts of the voles at each site, collected dur-
ing both spring and fall for a total of 31 years (1962–1992). Here, we ﬁt the AR(2) model to the
log of the density estimates for the fall time series derived by Cohen and Saitoh (2016) which
account for differences in sampling effort.

The left panel of Figure 11 illustrates the estimated AR(2) coefﬁcients for the 85 time series
using the exact MLE (black) and the corresponding bias corrected estimates (red). Two-third
of the original estimates are within the pseudo-periodic area which implies cyclic population
dynamics where shorter periods imply stronger density dependence (Stenseth et al., 2003). As
expected, the difference between the original and bias-corrected estimates are not very large for
this dataset as the time series length is n = 31. However, the bias correction is systematic in the
sense that the estimates of φ1 and φ2 are mainly shifted right and upwards, respectively. This is
further illustrated in the middle and left panels of Figure 11 showing scatterplots of the corrected
versus the original estimates. In correspondence with our simulation results, the bias correction
of φ1 is quite small for estimates that are not too close to the borders of the triangle, while

16

0
.
1

5
.
0

2

0
.
0

5
.
0
−

0
.
1
−

2

1

1
,
c

^

0

1
−

2
−

0
.
1

5
.
0

2
,
c

^

0
.
0

5
.
0
−

0
.
1
−

−2

−1

0
f 1

1

2

−2

−1

0
f^

1

1

2

−1.0

−0.5

0.0
f^

2

0.5

1.0

Figure 11: Coefﬁcient estimates ﬁtting AR(2) processes to log-density estimates of Hokkaido
vole populations observed at 85 different sites. The left panel shows the estimates of (φ1, φ2)
using the exact MLE (black) and the bias-corrected estimates (red). The middle and left panels
show the individual scatter plots of the bias-corrected versus original estimates.

φ2 is increasingly underestimated for larger values of the parameter. The given bias-correction
implies slightly weaker estimated density dependence and some of the series can no longer be
considered to be cyclic as the pairs of autoregressive coefﬁcients fall outside of the pseudo-
periodic area. Overestimation of the strength of density dependence have been associated with
ignoring sampling variance (Stenseth et al., 2003) and ignoring the estimation bias could thus
add to this overestimation.

5 Concluding remarks

The simplicity and parsimonious parameterization of ﬁrst and second-order AR processes make
them attractive as plausible models for short time series. The AR(1) model reﬂects the Markov
property, providing an important extension to a temporal independence assumption. The AR(2)
process is more ﬂexible and is particularly useful in modelling pseudo-periodic dynamics. The
bias involved in estimating the coefﬁcients of short AR processes has been well-known for
decades but this remains a problem, even for the simple ﬁrst and second-order cases. The choice
of estimator does make a difference for small sample sizes and incautious use of commonly
applied estimators might give misleading results. The default choice using the ar-function in R
gives the Yule-Walker estimates which are clearly not optimal, neither for short nor long time
series. As stated by Tjøstheim and Paulsen (1983), “uncritical use of Yule-Walker estimates may
be hazardous”. The ar-function also provides implementation of the popular least squares ap-
proach. We have not considered this method here as the coefﬁcient estimates often fall outside
the stationary area of AR processes.

The main goal of this paper was to provide a quick and easily available option to compute
bias-corrected versions of original coefﬁcient estimates for AR(1) and AR(2) processes and
assess estimation uncertainty by conﬁdence intervals. This is achieved by modeling the true
parameter values in terms of estimated values for a huge number of simulations, accounting for
the sampling distribution of the chosen original estimators. The model ﬁtting step is computa-
tionally expensive but needs to be done only once for each original estimator and each sample
size. Similarly, we model the parameters of skew-normal distributions in terms of the esti-
mated AR coefﬁcients providing approximate ﬁnite sampling distributions for the original and
bias-corrected estimators. The resulting approximate conﬁdence intervals reﬂect the estimation
variance. This is important as improved bias properties do come at the cost of increased variance

17

f
f
f
and in practical use this inherent bias-variance trade-off has to be considered.

Data availability statement

The data on the Hokkaido vole population in Section 4 were downloaded from the Supporting
Information of Cohen and Saitoh (2016), available at https://doi.org/10.1002/ecy.1575. We use
the log of the density estimates given in the ﬁle App3BayesCountsParameterEstimates.csv in
their Zip archive.

References

Andrews, D. W. K. (1993). Exactly median-unbiased estimation of ﬁrst order autoregressive/unit

root models. Econometrica, 61:139–165.

Andrews, D. W. K. and Chen, H.-Y. (1994). Approximately median-unbiased estimation of

autoregressive models. Journal of Business and Economic Statistics, 12:187–204.

Arnau, J. and Bono, R. (2001). Autocorrelation and bias in short time series: An alternative

estimator. Quality and Quantity, 35:365–387.

Bissonette, J. A. (1999). Small sample size problems in wildlife ecology: a contingent analytical

approach. Wildlife Biology, 5:65–71.

Bjørnstad, O. N., Falck, W., and Stenseth, N. C. (1995). A geographic gradient in small ro-
dent density ﬂuctuations: a statistical modelling approach. Proceedings of the Royal Society
London B, 262:127–133.

Box, G. E. P., Jenkins, G. M., and Reinsel, G. C. (2008). Time Series Analysis: Forecasting and

Control. John Wiley and Sons, Inc. Hoboken, New Jersey.

Box, G. E. P. and Luceno, A. (1997). Time Series Analysis: Forecasting and Control. Wiley,

New York.

Brockwell, P. J. and Davis, R. A. (2002). Introduction to Time Series and Forecasting. Springer-

Verlag, New Work, 2nd edition.

Burg, J. P. (1967). Maximum entropy spectral analysis. Proceedings of 37th Meeting of the

Society of Exploration Geophysics, Oklahoma City.

Cheang, W. K. and Reinsel, G. C. (2000). Bias reduction of autoregressive estimates in time
series regression model through restricted maximum likelihood. Journal of the American
Statistical Association, 95:1173–1184.

Cohen, J. E. and Saitoh, T. (2016). Population dynamics, synchrony, and environmental quality

of Hokkaido voles lead to temporal and spatial Taylor’s laws. Ecology, 97:3402–3413.

Cordeiro, G. M. and Klein, R. (1994). Bias correction in ARMA models. Statistics and Proba-

bility Letters, 19:169–176.

DeCarlo, L. T. and Tryon, W. W. (1993). Estimating and testing autocorrelation with small
samples: A comparison of the c-statistic to a modiﬁed estimator. Behaviour Research and
Therapy, 31:781–788.

18

Fernandez, C. and Steel, M. F. J. (1998). On Bayesian modeling of fat tails and skewness.

Journal of the American Statistical Association, 93:359–371.

Hannan, E. J. (1970). Multiple Time Series. Wiley, New York.

Hansen, T. F., Stenseth, N. C., Henttonen, H., and Tast, J. (1999). Interspeciﬁc and intraspeciﬁc
competition as causes of direct and delayed density dependence in a ﬂuctuating vole popu-
lation. Proceedings of the National Academy of Sciences of the United States of America,
96:986–991.

Hugueny, B. (2006). Spatial synchrony in population ﬂuctuations: extending the Moran theorem

to cope with spatially heterogeneous dynamics. OIKOS, 115:3–14.

Huitema, B. E. and McKean, J. W. (1991). Autocorrelation estimation and inference with small

samples. Psycological Bulletin, 110:291–304.

Ives, A. R., Abbott, K. C., and Ziebarth, N. L. (2010). Analysis of ecological time series with

ARMA(p,q) models. Ecology, 91:858–871.

Kendall, M. G. (1954). Note on bias in the estimation of autocorrelation. Biometrika, 41:403–

404.

Kim, J. H. (2003). Forecasting autoregressive time series with bias-corrected parameter estima-

tors. International Journal of Forecasting, 19:493–502.

Kim, J. H. (2014). BootPR: Bootstrap Prediction Intervals and Bias-Corrected Forecasting. R

package version 0.60.

Krone, T., Albers, C. J., and Timmerman, M. E. (2017). A comparative simulation study of

AR(1) estimators in short time series. Quality and Quantity, 51:1–21.

Marriott, F. H. C. and Pope, J. A. (1954). Bias in the estimation of autocorrelations. Biometrika,

41:390–402.

McLeod, A. I. and Zhang, Y. (2006). Partial autocorrelation parameterization for subset autore-

gression. Journal of Time Series Analysis, 27:599–612.

McLeod, A. I. and Zhang, Y. (2008). Improved subset autoregression: With R package. Journal

of Statistical Software, 28:1–28.

R Core Team (2020). R: A Language and Environment for Statistical Computing. R Foundation

for Statistical Computing, Vienna, Austria.

Roy, A. and Fuller, W. A. (2001). Estimation for autoregressive time series with a root near one.

Journal of Business and Economic Statistics, 19:482–493.

Saitoh, T., Stenseth, N. C., and Bjørnstad, O. N. (1998). The population dynamics of the vole
clethrionomys rufocanus in Hokkaido, Japan. Researches on Population Ecology, 40:61–76.

Shaman, P. and Stine, R. A. (1988). The bias of autoregressive coefﬁcient estimators. Journal

of the American Statistical Association, 83:842–848.

Shumway, R. H. and Stoffer, D. S. (2017). Time Series Analysis and its Applications with R

Examples. Springer, New York.

19

Stenseth, N. C., Viljugrein, H., Saitoh, T., Hansen, T. F., Kittilsen, M. O., Bølviken, E., and
Glockner, F. (2003). Seasonality, density dependence, and population cycles in Hokkaido
voles. Proceedings of the National Academy of Sciences, 100:11478–11483.

Tanaka, K. (1984). An asymptotic expansion associated with the maximum likelihood estimators

in ARMA models. Journal of the Royal Statistical Society, Series B, 46:58–67.

Thombs, L. A. and Schucany, W. R. (1990). Bootstrap prediction intervals for autoregression.

Journal of the American Statistical Association, 85:486–492.

Tjøstheim, D. and Paulsen, J. (1983). Bias of some commonly-used time series estimates.

Biometrika, 70:389–399.

Walker, G. (1931). On periodicity in series of related terms. Proceedings of the Royal Society,

Series A, 131:518–532.

Wuertz, D., Setz, T., Chalabi, Y., Boudt, C., Chausse, P., and Miklovac, M. (2020).

fGarch:
Rmetrics - Autoregressive Conditional Heteroskedastic Modelling. R package version
3042.83.2.

Yule, G. U. (1927). On a method of investigating periodicities in disturbed series, with special
reference to Wolfer’s sunspot numbers. Philosophical Transactions of the Royal Society A:
Mathematical, Physical and Engineering Sciences, 226:267–298.

6 Appendix: R-package

To make the presented methodology easily available, we include the R-package ARbiascorrect
which can be used to obtain the bias-corrected estimates and their approximate 95% conﬁdence
intervals, in addition to 95% conﬁdence intervals for the original estimates. The package con-
sists of only one function as deﬁned below. The user can either use the relevant time series as
input or the parameter estimates found by one of the original methods including the exact and
conditional MLE, Burg’s algorithm or the Yule-Walker solution. The R-package can be down-
loaded and installed from https://github.com/pedrognicolau/ARbiascorrect,
or installed directly in R through the devtools package by running:

devtools::install_packages("pedrognicolau/ARbiascorrect")

The speciﬁcations of this package are as follows:

Description

Gives bias-corrected estimates and 95% conﬁdence intervals for autoregressive coefﬁ-
cients of AR(1) and AR(2) processes, for sample sizes n = 10, 11, . . . , 50.

Usage

biascorrect(phi = NULL, n = NULL,

method = c("mle", "cmle", "burg", "yw"),
order = NULL, x = NULL)

Arguments

20

phi Single value (AR1) or two-dimensional vector (AR2) containing the coefﬁcient es-
timates subject to bias correction. Not required if the time series x is used as input.

n Integer for the length of the time series. Needs to be between 10 and 50. Not required

if the time series x is used as input.

method Character string specifying the method used to estimate the autoregressive coefﬁ-
cients. Needs to be either mle, cmle, burg or yw, specifying the exact MLE, the
conditional MLE, Burg’s method and the Yule-Walker solution, respectively.

order Order of the estimated autoregressive process. Needs to be provided if x is used as

input.

x The time series to be ﬁtted by AR(1) or AR(2) in which order needs to be speciﬁed.

The default estimation method is the exact MLE if none is speciﬁed.

Value

phi.hat The original estimates of the AR coefﬁcients

phi.correct The bias-corrected estimates

ci.hat The 95% conﬁdence interval for the original estimates

ci.correct The 95% conﬁdence interval for the bias-corrected estimates

21

