Robust Parameter Estimation of Regression Model with AR(p) Error Terms 

Y. Tuaç, Y. Güney B. Şenoğlu and O. Arslan  

Ankara University, Faculty of Science, Department of Statistics, 06100 Ankara/Turkey 

ytuac@ankara.edu.tr, ydone@ankara.edu.tr, senoglu@science.ankara.edu.tr, oarslan@ankara.edu.tr 

Abstract 

In this paper, we consider a linear regression model with AR(p) error terms with the assumption that the 
error  terms  have  a  t  distribution  as  a  heavy  tailed  alternative  to  the  normal  distribution.  We  obtain  the 
estimators  for  the  model  parameters  by  using  the  conditional  maximum  likelihood  (CML)  method.  We 
conduct an iteratively reweighting algorithm (IRA) to find the estimates for the parameters of interest. We 
provide  a  simulation  study  and  three  real  data  examples  to  illustrate  the  performance  of  the  proposed 
robust estimators based on t distribution.  

Keywords:  autoregressive  stationary  process;  conditional  maximum  likelihood;  linear  regression;  non 
normal distributions; robust estimation.  

1. Introduction 

Consider the following linear regression model 

𝑀
𝑦𝑡 = ∑ 𝑥𝑡,𝑖
𝑖=1

𝛽𝑖 + 𝑒𝑡   ,

 𝑡 = 1,2, … , 𝑁                                                                                                 (1) 

where, 𝑦𝑡  is  the  response  variable,   𝑥𝑡,𝑖  are  the  explanatory  variables,   𝛽𝑖  are  the  unknown  regression 
parameters and 𝑒𝑡 is the error term. In classical regression analysis, the general assumptions on the error 
term are zero mean, constant variance and not correlated with each other. It is well known that under these 
assumptions  the  ordinary  least  squares  (OLS)  estimator  is  the  best.  However,  one  of  the  problems  in 
application  is  that  the  error  term  may  be  correlated  with  each  other.  In  this  case,  although,  the  OLS 
estimators are unbiased and consistence, they may be no longer efficient even in large sample cases, and 
hence this may cause large estimated standard errors for the estimators of the regression parameters (see 
Olaomi and Ifederu [13]). There are many ways to deal with autocorrelated structures in the disturbances; 
the most common way is to assume autoregressive error terms in regression model. 

 We assume that 𝑒𝑡 is a stationary autoregressive error process of order p (AR(p)) given as  

𝑒𝑡 = 𝜙1𝑒𝑡−1 + ⋯ + 𝜙𝑝𝑒𝑡−𝑝 + 𝑎𝑡,                                                                                                     (2) 

1 

 
 
 
 
 
 
where 𝜙𝑗 , for 𝑗 = 1,2, … , 𝑝, are unknown autoregressive parameters.  

For simplicity we use   𝑎𝑡 = 𝑒𝑡 − 𝜙1𝑒𝑡−1 − ⋯ − 𝜙𝑝𝑒𝑡−𝑝 = Φ(𝐵)𝑒𝑡, where 𝐸(𝑎𝑡) = 0, 𝑉𝑎𝑟(𝑎𝑡) = 𝜎2 and 
𝑎𝑡’s are uncorrelated random variables with constant variance. Here 𝐵 is called the backshift operator and 
where Φ(∙) is the function  defining  the  autoregression. Then  using  the  backshift  operator the regression 
model given in (1) can be rewritten as  

Φ(𝐵)𝑦𝑡 = ∑ 𝛽𝑖

𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖 + 𝑎𝑡,    𝑡 = 𝑝 + 1,2, … , 𝑁,                                                                        

(3) 

where  

Φ(𝐵)𝑦𝑡 = 𝑦𝑡 − 𝜙1𝑦𝑡−1 − ⋯ − 𝜙𝑝𝑦𝑡−𝑝,                                                                                                    (4)
Φ(𝐵)𝑥𝑡,𝑖 = 𝑥𝑡,𝑖 − 𝜙1𝑥𝑡−1,𝑖 − ⋯ − 𝜙𝑝𝑥𝑡−𝑝,𝑖                                                                                             (5)                                                                      

In general, it is assumed that 𝑎𝑡 is normally distributed. For instance, Alpuim and El-Shaarawi [2] 
estimated the parameters of the regression model with AR(p) error term using the OLS estimation method. 
They  also  used  the  maximum  likelihood  (ML)  estimation  and  CML  estimation  method  under  the 
assumption  of  normality  and  studied  the  asymptotic  properties  of  the  resulting  estimators.  Beach  and 
Mackinnon  [5]  used  ML  estimation  method  to  estimate  the  parameters  of  AR(1)  error  term  regression 
models. Tiku [17] estimated the parameters by using the modified maximum likelihood (MML) method 
for  the  regression  model  with  AR(1)  error terms  under  the assumption that the error term  has  the  Long 
Tailed Symmetric (LTS) distribution. There are some other studies used heavy tailed distributions in time 
series. For instance, Hill [7] used tail-trimming and/or weighting to show how robust to any type of light 
or  heavy  tailed  distribution  in  infinite  variance  autoregressions  case.  Also,  heavy  tailed  asymmetrically 
distributed errors in GARCH model were discussed with tail-trimmed QML estimator in Hill [8,9].  

Another challenging problem in a regression analysis is the presence of outliers in data. Since the 
parameter  estimators  based  on  normal  distribution  are  very  sensitive  to  the  outliers,  the  corresponding 
estimators  will  be  no  longer  efficient.  One  way  to  combat  with  the  outliers  is  to  use  heavy  tailed 
distributions as alternatives to the normal distribution. Thus, the t distribution provides a useful alternative 
to  the  normal  distribution  for  statistical  modelling  of  data  sets  that  have  heavier  tailed  empirical 
distribution.  The  motivation  of  this  paper  is  to  propose  conditional  maximum  likelihood  estimators  for 
unknown parameters of a linear regression model with autoregressive errors under the assumption that the 
independent identically distributed (iid) error term 𝑎𝑡 given in equation (2) has a t distribution with known 
degrees of freedom. The estimators for the parameters of interest obtained under this assumption will be 
robust in terms of the influence function. It is known that if the degrees of freedom is estimated along with 
the other parameters the influence function of the resulting estimators will be unbounded and hence they 
are not going to be robust (Lucas [10]). Therefore, the degrees of freedom is usually taken as fixed and 
treated as a robustness tuning parameter in robustness studies, for example see Lange et al. [11].  

 The rest of the paper is organized as follows. In section 2, we first summarize the CML estimation 
method. Then, we move on the CML estimation for the parameters of regression model with AR(p) error 
terms  under  the  assumption  that 𝑎𝑡’s  have  t  distribution.  We  also  give  the  observed  Fisher  information 
matrix  for the  estimators. Note that  the  observed  Fisher information  matrix  will  be  used  in  section 4  to 
form confidence intervals and to compute the standard errors of the estimators. In section 3, we give an 

2 

 
 
 
IRA to compute the estimates. A simulation study and three real data examples are given in section 4 to 
illustrate  the  performance  of  the  proposed  estimators.  Finally  we  conclude  the  paper  with  a  discussion 
section. 

2. Parameters estimation of the AR(p) error term regression model 

In this section, since the exact likelihood function could be well approximated by the conditional 
likelihood function (Ansley [1]) we will first give the CML estimation method. CML estimators are used 
mainly  in  cases  where  ML  estimators  are  difficult  to  compute.  We  will  briefly  give  the  conditional 
likelihood estimators under the normality assumption and move on the t distribution case. We also provide 
observed Fisher matrices for both cases.  

2.1 Conditional likelihood under normality    

Let  𝑎𝑡’s  have the probability density function 𝑓(𝑎𝑡, 𝜽). If we condition on 𝑎1, 𝑎2, … , 𝑎𝑝, the conditional 
log-likelihood function will be 

𝑁

𝑙𝑛𝐿 = ∑ 𝑙𝑛𝑓(𝑎𝑡|𝑎1, 𝑎2, … , 𝑎𝑡−𝑝, 𝜽).

                                                                                                   (6) 

𝑡=𝑝+1

Consider the regression model given in (3). If it is assumed that 𝑎𝑡’s are normally distributed the 

conditional log-likelihood function will be as follows (Alpuim and El-Shaarawi [2]). 

𝑙𝑛𝐿 = 𝑐 −

𝑁 − 𝑝
2

𝑙𝑛𝜎2 −

1
2𝜎2 ∑  

𝑁

𝑡=𝑝+1 

𝑀

2

(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

Φ(𝐵)𝑥𝑡,𝑖)

                                               (7) 

𝑖=1

Taking the derivatives of the conditional log-likelihood function with respect to unknown parameters and 
setting to zero yield the following estimating equations.  

𝜕𝑙𝑛𝐿
𝜕𝛽𝑘

=

𝜕𝑙𝑛𝐿
𝜕𝜙𝑙

=

𝑁

𝑁

1
𝜎2 ∑ (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑡=𝑝+1

1
𝜎2 ∑ (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑡=𝑝+1

𝑀

𝑖=1

𝑀

𝑖=1

Φ(𝐵)𝑥𝑡,𝑖)

Φ(𝐵)𝑥𝑡,𝑘 = 0                                                             (8) 

𝑀

Φ(𝐵)𝑥𝑡,𝑖)

 (𝑦𝑡−𝑙 − ∑ 𝛽𝑖

𝑥𝑡−𝑙,𝑖) = 0                                     (9) 

𝑖=1

2

Φ(𝐵)𝑥𝑡,𝑖)

  = 0                                                  (10) 

𝜕𝑙𝑛𝐿
𝜕𝜎2 = −

𝑁 − 𝑝
2𝜎2 +

1
2𝜎4 ∑ (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑁

𝑡=𝑝+1

𝑀

𝑖=1

Rearranging these equations we get the following estimators. 

3 

 
 
𝑁

−1

𝑁

𝛽̂ = [ ∑ Φ̂ (𝐵)𝑥𝑡Φ̂ (𝐵)𝑥𝑡
𝑇

]

𝑡=𝑝+1

[ ∑ Φ̂ (𝐵)𝑦𝑡Φ̂ (𝐵)𝑥𝑡
𝑡=𝑝+1

]                                                                   (11) 

𝜙̂ = 𝑅−1(𝛽̂)𝑅0(𝛽̂)                                                                                                                                       (12) 

𝜎̂2 =

1
𝑁 − 𝑝

𝑁
2
∑ (Φ̂ (𝐵)𝑦𝑡 − 𝛽̂Φ̂ (𝐵)𝑥𝑡)
𝑡=𝑝+1

                                                                                           (13)

where 

𝑅0(𝛽) =

𝑁
∑ 𝑒𝑡𝑒𝑡−1
𝑡=𝑝+1
𝑁
∑ 𝑒𝑡𝑒𝑡−2
𝑡=𝑝+1

⋮

𝑁
∑ 𝑒𝑡𝑒𝑡−𝑝
[
𝑡=𝑝+1

]

, 𝑅(𝛽) =

2

𝑁
∑ 𝑒𝑡−1
𝑡=𝑝+1
𝑁
∑ 𝑒𝑡−2𝑒𝑡−1
𝑡=𝑝+1

⋮

𝑁
∑ 𝑒𝑡−𝑝𝑒𝑡−1
[
𝑡=𝑝+1

𝑁
∑ 𝑒𝑡−1𝑒𝑡−2
𝑡=𝑝+1
𝑁
∑ 𝑒𝑡−2
𝑡=𝑝+1
⋮

2

𝑁
∑ 𝑒𝑡−𝑝𝑒𝑡−2
𝑡=𝑝+1

𝑁

⋯ ∑ 𝑒𝑡−1𝑒𝑡−𝑝

𝑡=𝑝+1
𝑁

⋯ ∑ 𝑒𝑡−2𝑒𝑡−𝑝

𝑡=𝑝+1

     (14) 

⋱

⋮

𝑁
⋯ ∑ 𝑒𝑡−𝑝
𝑡=𝑝+1

2

]

and Φ̂ (𝐵) is the backshift operator with the estimates of  𝜙𝑗. 

Using these equations we can rewrite 𝛽̂ and 𝜎̂2, 

𝛽̂ = [𝚽̂ (𝑩)𝑿𝑻𝚽̂ (𝑩)𝑿]

−1

[𝚽̂ (𝑩)𝑿𝑻Φ̂ (𝐵)𝑌]                                                                                           (15) 

𝜎̂2 =

1
𝑁 − 𝑝

where 

[Φ̂ (𝐵)𝑌 − 𝚽̂ (𝑩)𝑿𝛽̂]𝑇[Φ̂ (𝐵)𝑌 − 𝚽̂ (𝑩)𝑿𝛽̂]                                                                   (16) 

𝚽̂ (𝑩)𝑿 = [Φ̂ (𝐵)𝑥𝑡,𝑖], 

Φ̂ (𝐵)𝑌 = [Φ̂ (𝐵)𝑦𝑡]. 

These estimators depend on the estimators of the other parameters. Therefore, the values of the estimators 
should be computed using numerical methods. We use IRA to compute these estimators to guarantee the 
convergence (see Lange et al. [11], Arslan and Genç [4]).  These estimators correspond also to the OLS 
estimators  obtained  through  the  minimization  of  the  sum  of  squares  of  the 𝑎𝑡 .  These  estimators  are 
sensitive to the outliers in the data. Therefore, an alternative error distribution should be considered to deal 

4 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
with  this  problem.  In  the  following  section  we  will  assume  that 𝑎𝑡’s  have  a  t  distribution  with  known 
degrees of freedom and carry out the estimation under this assumption. 

Further,  we  also  give  the  observed  Fisher  information  matrix  for  the  unknown  parameters  of  the 
regression  model  defined  in  equation  (3)  with  normally  distributed  error  terms.  Note  that  the  observed 
Fisher  information  matrices  will  be  used  to compute  the  standard  errors  and  the  confidence intervals  in 
simulation study and the real data examples. 

After  some  straightforward  algebra  the  observed  Fisher  information  matrix  for  the  normal  distribution 
case can be obtained as follows. 

                                             (17)  

2.2 Parameters estimation under t distribution  

Consider  the  regression  model  given  in  equation  (3)  and  assume  that 𝑎𝑡’s    have  t  distribution  with  the 
density function 

𝑓(𝑎𝑡) =

𝑐𝑣
𝜎

(𝜈 +

2
𝑎𝑡
𝜎2)

−

𝑣+1
2

,                                                                                                                        (18) 

where 𝑐𝑣 =

Γ(

ν+1
2
√𝜋Γ(

)𝜈𝜈/2
𝑣
)
2

 , 𝜈 > 0 degrees  of  freedom  and 𝜎 > 0 scale  parameter.  Under  this  assumption  the 

conditional log-likelihood function will be obtained as 

𝑙𝑛𝐿 = 𝑙𝑛𝑐𝑣 − (𝑁 − 𝑝)𝑙𝑛𝜎 −

𝑣 + 1
2

𝑁

∑ ln

[𝑣 +

𝑡=𝑝+1 

(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

2
𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖 )
𝜎2

].                 (19) 

Taking the derivatives of log-likelihood function with respect to the unknown parameters and setting them 
to zero yield the following estimating equations. 

𝜕𝑙𝑛𝐿
𝜕𝛽𝑘

=

(𝑣 + 1)

𝑁

𝜎2 ∑

𝑡=𝑝+1

(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)Φ(𝐵)𝑥𝑡,𝑘
2
𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)
𝜎2

]

[𝑣 +

= 0,                                               (20) 

5 

2241ˆˆ()00ˆ1ˆˆˆ(,,)00ˆˆ()00ˆTBXBXFRNp 
 
 
  
𝜕𝑙𝑛𝐿
𝜕𝜙𝑙

=

𝑁

(𝑣 + 1)

𝜎2 ∑

𝑡=𝑝+1

(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)(𝑦𝑡−𝑙 − ∑ 𝛽𝑖
(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑀
𝑖=1
2
𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)
𝜎2

]

[𝑣 +

𝑥𝑡−𝑙,𝑖)

= 0,                                 (21) 

𝜕𝑙𝑛𝐿
𝜕𝜎

= −

(𝑁 − 𝑝)
𝜎

+ (

𝑁

𝑣 + 1
𝜎3 ) ∑

𝑡=𝑝+1

(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

2
𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)

[𝑣 +

(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

2
𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)
𝜎2

]

= 0.                        (22) 

Rearranging these equations we get 

𝑁

𝑁

𝜕𝑙𝑛𝐿
𝜕𝛽𝑘

=

𝜕𝑙𝑛𝐿
𝜕𝜙𝑙

=

1
𝜎2 ∑ 𝑤𝑡(𝜈) (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑡=𝑝+1

1
𝜎2 ∑ 𝑤𝑡(𝜈) (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑡=𝑝+1

𝑀

𝑖=1

𝑀

𝑖=1

Φ(𝐵)𝑥𝑡,𝑖) Φ(𝐵)𝑥𝑡,𝑘

= 0,                                             (23) 

𝑀

Φ(𝐵)𝑥𝑡,𝑖) (𝑦𝑡−𝑙 − ∑ 𝛽𝑖

𝑥𝑡−𝑙,𝑖) =

0,                       (24) 

𝑖=1

2

Φ(𝐵)𝑥𝑡,𝑖)

= 0                                      (25) 

𝜕𝑙𝑛𝐿
𝜕𝜎

= −

(𝑁 − 𝑝)
𝜎

+

where  

𝑁

1
𝜎3 ∑ 𝑤𝑡(𝜈) (Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

𝑡=𝑝+1

𝑀

𝑖=1

𝑤𝑡(𝜈) =

𝑣 + 1

[𝑣 +

(Φ(𝐵)𝑦𝑡 − ∑ 𝛽𝑖

2
𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)
𝜎2

]

 .                                                                                (26) 

These equations yield the following estimators provided that [∑

𝑁
𝑡=𝑝+1

𝑤𝑡(𝜈)Φ̂ (𝐵)𝑥𝑡Φ̂ (𝐵)𝑥𝑡
𝑇

]

−1

and 𝑅𝑤

−1(𝛽̂) exist. 

𝑁

−1

𝑁

𝛽̂ = [ ∑ 𝑤𝑡(𝜈)Φ̂ (𝐵)𝑥𝑡Φ̂ (𝐵)𝑥𝑡
𝑇

]

𝑡=𝑝+1

[ ∑ 𝑤𝑡(𝜈)Φ̂ (𝐵)𝑦𝑡Φ̂ (𝐵)𝑥𝑡
𝑡=𝑝+1

],                                           (27) 

𝜙̂ = 𝑅𝑤

−1(𝛽̂)𝑅𝑤0(𝛽̂),                                                                                                                                 (28) 

6 

 
  
  
 
  
 
 
 
𝜎̂2 =

1
𝑁 − 𝑝

𝑁
∑ 𝑤𝑡(𝜈) (Φ̂ (𝐵)𝑦𝑡 − Φ̂ (𝐵)𝑥𝑡
𝑡=𝑝+1

2
𝑇𝛽̂)

                                                                               (29)

where 

𝑅𝑤0(𝛽) =

𝑁
∑ 𝑤𝑡𝑒𝑡𝑒𝑡−1
𝑡=𝑝+1
𝑁
∑ 𝑤𝑡𝑒𝑡𝑒𝑡−2
𝑡=𝑝+1

⋮

𝑁
∑ 𝑤𝑡𝑒𝑡𝑒𝑡−𝑝
𝑡=𝑝+1

[

]

,

𝑅𝑤(𝛽) =

2

𝑁
∑ 𝑤𝑡𝑒𝑡−1
𝑡=𝑝+1
𝑁
∑ 𝑤𝑡𝑒𝑡−2𝑒𝑡−1
𝑡=𝑝+1

⋮

𝑁
∑ 𝑤𝑡𝑒𝑡−1𝑒𝑡−2
𝑡=𝑝+1
𝑁
∑ 𝑤𝑡𝑒𝑡−2
𝑡=𝑝+1

2

𝑁

⋯ ∑ 𝑤𝑡𝑒𝑡−1𝑒𝑡−𝑝

𝑡=𝑝+1
𝑁

⋯ ∑ 𝑤𝑡𝑒𝑡−2𝑒𝑡−𝑝

𝑡=𝑝+1

.      (30) 

⋮

⋱

⋮

𝑁
∑ 𝑤𝑡𝑒𝑡−𝑝𝑒𝑡−1
[
𝑡=𝑝+1

𝑁
∑ 𝑤𝑡𝑒𝑡−𝑝𝑒𝑡−2
𝑡=𝑝+1

𝑁
⋯ ∑ 𝑤𝑡𝑒𝑡−𝑝
𝑡=𝑝+1

2

]

Further, these equations can be rewritten as  

𝛽̂ = [𝚽̂ (𝑩)𝑿𝑻𝑾𝚽̂ (𝑩)𝑿]

−1

[𝚽̂ (𝑩)𝑿𝑻𝑾Φ̂ (𝐵)𝑌],                                                                                 (31) 

𝜎̂2 =

1
𝑁 − 𝑝

[Φ̂ (𝐵)𝑌 − 𝚽̂ (𝑩)𝑿𝛽̂]𝑇𝑾 [Φ̂ (𝐵)𝑌 − 𝚽̂ (𝑩)𝑿𝛽̂],                                                            (32) 

by using vector notation. Here 

𝚽̂ (𝑩)𝑿 = [Φ̂ (𝐵)𝑥𝑡,𝑖]𝑡=𝑝+1,…,𝑁
𝑖=1,…,𝑀
, 

Φ̂ (𝐵)𝑌 = [Φ̂ (𝐵)𝑦𝑡]
𝑾 = 𝑑𝑖𝑎𝑔{𝑤𝑡}𝑡=𝑝+1,…,𝑁. 

𝑡=𝑝+1,…,𝑁

, 

Since  the  weight  function 𝑤𝑡 is  a  decreasing  function  of   
larger  residuals  receive  small  weights. Therefore,  the  effect  of  the corresponding  point  on  the estimator 
will  be  downweighted.  This  behavior  of  the  t  distribution  guarantees  the  robustness  of  the  resulting 
estimators (Lucas [10], Arslan and Genç [3, 4]). Note that as 𝜈 tends to infinity 𝑤𝑡(𝜈) → 1 and this case 
gives the estimators given in equations (11)-(13).  

   the  observations  with 

𝜎2

(Φ(𝐵)𝑦𝑡−∑

2
𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖)

𝛽𝑖

Similarly the observed Fisher information matrix for the unknown parameters of the regression model 
defined in equation (3) with the t distributed error terms can be obtained as follows.  

7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
                                        (33)  

We should also note that since the estimators given in (27)-(29) are dependent on the weights and 
since  the  weights  are  also  functions  of  the  estimators  these  equations  cannot  be  solved  explicitly. 
Therefore, the numerical methods are also needed to solve these equations to get the estimates. Because of 
the form of the equations the IRA can be easily implemented to get the estimates as it is done for all the 
procedures  based  on  t  distribution.  Note  that  in  the  t  distribution  case  the  IRA  is  an  expectation-
maximization (EM) algorithm so that its convergence is guaranteed (see Lange et al. [11], McLachlan and 
Krishnan, [12] Arslan and Genç [4]). The following section is devoted to the IRA.   

3. Iteratively reweighted algorithm  

Using  the  updating  equations  (28,  31,  32)  and  the  weight  function  given in  equation  (26)  the  following 
iteratively reweighted algorithm can be formed to calculate the estimates for, 𝛽, 𝜙 and 𝜎2. Note that the 
degrees of freedom of the t distribution will be taken as known and fixed. 

(i) Set the initial values 𝛽(0), 𝜙(0) 𝑎𝑛𝑑 𝜎2(0)

  and fix a stopping rule 𝛿. 

(ii) Calculate the following weight function for  𝑚 = 0,1,2 …      

(𝑚) =

𝑤𝑡

𝑣 + 1

[𝑣 +

𝑀
(Φ(𝑚)(𝐵)𝑦𝑡 − ∑ 𝛽𝑖
𝑖=1
𝜎2(𝑚)

(𝑚)

2
Φ(𝑚)(𝐵)𝑥𝑡,𝑖)

]

 .                                                                (34) 

 (iii) Using these values calculate  

𝜙(𝑚+1) = 𝑅𝑤

−1(𝑚)(𝛽̂ (𝑚))𝑅𝑤0

(𝑚)(𝛽̂ (𝑚)).                                                                                                  (35) 

(iv) Using 𝑤𝑡

(𝑚) and 𝜙(𝑚+1) calculate   

𝛽(𝑚+1) = [𝚽(𝑚+1)(𝑩)𝑿𝑻𝑾(𝑚)𝚽(𝑚+1)(𝑩)𝑿]

−1

[𝚽(𝑚+1)(𝑩)𝑿𝑻𝑾(𝑚)Φ(𝑚+1)(𝐵)𝑌].                (36) 

8 

2241ˆˆ()00ˆ1ˆˆˆ(,,)00ˆˆ()00ˆTwBXWBXFRNp 
 
  
  
 
 
  
(v) Using  𝑤𝑡

(𝑚) , 𝜙(𝑚+1) and 𝛽(𝑚+1) calculate  

(𝜎2)(𝑚+1) =

1
𝑁 − 𝑝

𝑇
[Φ(𝑚+1)(𝐵)𝑌 − 𝚽(𝑚+1)(𝑩)𝑿𝛽(𝑚+1)]

𝑾(𝑚) [Φ(𝑚+1)(𝐵)𝑌 − 𝚽(𝑚+1)(𝑩)𝑿𝛽(𝑚+1)]                  (37) 

(vi)  Repeat the  steps (ii)-(v)  until  the  convergence  condition 𝑚𝑎𝑥(‖𝛽(𝑚+1) − 𝛽(𝑚)‖, ‖𝜙(𝑚+1) − 𝜙(𝑚)‖, 
‖(𝜎2)(𝑚+1) − (𝜎2)(𝑚)‖) < 𝛿 is satisfied. 

In  section  4,  we  will  use  this  algorithm  to  compute  the  CML  estimates  in  simulation  and  the  real  data 
examples. 

4. Numerical study 

In this section, we give a small simulation study and three real data examples to illustrate the performance 
of  the  regression  estimators  obtained  from  the  t  distribution  (with  AR  (2)  error  terms  for  finite  sample 
case)  with  and  without  outliers  in  the  data.    The  CML  estimates  are  computed  using  the  IRA  given  in 
section 3. Note that throughout the simulation study and the real data examples the degrees of freedom (𝜈) 
of the t distribution is taken as 3 since the small values such as 3 are suggested for the sake of robustness 
in literature (e.g see Lange et.al, 1989).  

4.1 A simulation study 

Simulation design.  Firstly we generate three independent variables  𝑥𝑡 from standard normal distribution 
( 𝑥𝑡,𝑖 ∼ 𝑁(0,1) ).  The  values  of  the  parameters  are  𝛽 = (𝛽1, 𝛽2, 𝛽3)′  = (0.1,  0.5,  0.9)′  and   𝜙 =
(𝜙1, 𝜙2)′ = (−0.7, 0.12)′.  Note that the AR (2) model values of 𝜙 are taken to guarantee the stationarity 
assumption for the model of the error terms.  Then the values of the response variable are generated using 
Φ(𝐵)𝑦𝑡 = ∑ 𝛽𝑖

𝑀
𝑖=1 Φ(𝐵)𝑥𝑡,𝑖 + 𝑎𝑡.  

Simulation  cases.  In  first  case  the  𝑎𝑡 ’s  in  (3)  are  generated  from  standard  normal  distribution 
(𝑎𝑡~𝑁(0,1)) and  parameters  are  estimated  by  using  normal  distribution’s  conditional  likelihood  and  t 
distribution’s conditional likelihood. In the second case the 𝑎𝑡’s are generated from t distribution with 𝜈 =
3 degrees  of  freedom (𝑎𝑡~𝑡𝜈(0,1)) and  parameters  are  estimated  by  using  normal  and  t  assumptions 
again.  The  last  case  is  for  the  symmetric  Pareto  distributed  error  terms,  where  we  use  the  Pareto 
distribution  symmetric  by  zero  as  given  in  [6]  with  the  tail  index  (shape)  parameter   𝜅 = 1.25.  For  the 
Pareto error case we compute the values of the estimators obtained from the normal and the t distributions.  

Outlier case. To add some  outliers  to  the  data  10 percent  of 𝑌 is  replaced  by  the points  generated from 
𝑁(0, 100).  

Performance  measures.  Mean  squared  error  (MSE)  and  bias  values  are  calculated  to  compare  the 
estimators.    These  values  are  calculated  by  using 𝑅 = 100 replications  for  the  sample  sizes 𝑛 = 25, 50 
and 100. Also, using the observed Fisher information matrix given in section 2, the standard errors (SE) 
and the confidence intervals (CIL - CIU) are calculated. 

9 

 
  
 
 The MSE values and the biases are calculated using 

𝑀𝑆𝐸(𝛽̂) =

𝑀𝑆𝐸(𝜙̂) =

1
𝑅

1
𝑅

2
∑ (𝛽̂𝑖 − 𝛽)

𝑅
𝑖=1

, 𝑏𝑖𝑎𝑠(𝛽̂) = 𝛽̅ − 𝛽,  

2
∑ (𝜙̂𝑖 − 𝜙)

𝑅
𝑖=1

, 𝑏𝑖𝑎𝑠(𝜙̂) = 𝜙̅ − 𝜙,  

𝑀𝑆𝐸(𝜎̂2) =

𝑅
∑ (𝜎̂𝑖
𝑖=1

2
2 − 𝜎2)

1
𝑅

, 𝑏𝑖𝑎𝑠(𝜎̂2) = 𝜎2̅̅̅̅ − 𝜎2,  

where  𝛽̅ =

1
𝑅

∑ 𝛽̂𝑖
𝑅
𝑖=1

 , 𝜙̅ =

1
𝑅

∑ 𝜙̂𝑖
𝑅
𝑖=1

 ,𝜎2̅̅̅̅ =

1
𝑅

∑ 𝜎𝑖̂ 2
𝑅
𝑖=1

. 

Simulation results. Simulation results are given in Tables 1 and 2.  

Simulation results without outliers 

Table 1. Bias, MSE, SE, CIL and CIU values of the estimates without outliers 
True Values (𝛽1, 𝛽2, 𝛽3)′  = (0.1,  0.5,  0.9)′ , (𝜙1, 𝜙2)′ = (−0.7, 0.12) and 𝜎 = 1 

n 
25 

𝛽1 

𝛽2 

𝛽3 

𝜙1 

𝜙2 

𝛽̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂3 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂2 
Bias 
MSE 
SE 
CIL 
CIU 

   St-t 

      Normal Error 
   Norm 
0.0854 
0.0230 
0.0453 
0.1598 
0.0501 
0.1754 
0.4817 
0.0182 
0.0372 
0.2119 
0.4044 
0.6008 
0.8959 
0.0128 
0.0468 
0.1813 
0.8266 
0.9687 
-0.7034 
-0.0326 
0.0715 
0.0261 
-0.7136 
-0.6931 
0.0276 
-0.1222 
0.0832 
0.1403 
-0.0644 
0.1196 

0.0749 
0.0302 
0.0578 
0.3965 
-0.0366 
0.2484 
0.4717 
-0.0293 
0.0656 
0.2997 
0.3513 
0.6417 
0.9039 
0.0130 
0.0537 
0.3596 
0.7264 
1.0499 
-0.5273 
0.0923 
0.0810 
0.1414 
-0.6208 
-0.4338 
0.0626 
-0.1412 
0.0854 
0.1397 
-0.0111 
0.1364 

t Error 

Paretian Error (𝜅 = 1.25) 

   Norm 

   St-t 

  Norm 

0.1360 
-0.0075 
0.0708 
0.5131 
0.0272 
0.2449 
0.4600 
0.0086 
0.1150 
0.4930 
0.3677 
0.5522 
0.9008 
-0.0125 
0.1110 
0.4130 
0.7370 
1.0645 
-0.5611 
0.0818 
0.0618 
0.2476 
-0.6007 
-0.5214 
0.0413 
-0.1295 
0.0669 
0.1835 
-0.0027 
0.0852 

25.3813 
25.2813 
26940.5 
5.0918 
23.4241 
27.3385 
24.2895 
23.6061 
61730.1 
5.4498 
22.1946 
26.3843 
-20.515 
-21.415 
17290.3 
2.9110 
-21.634 
-19.396 
-0.5226 
0.1774 
0.0973 
0.1617 
-0.5845 
-0.4606 
0.2662 
0.1462 
0.0857 
0.9441 
-0.0987 
0.6310 

0.1135 
-0.0136 
0.0789 
0.2678 
0.0082 
0.2189 
0.4945 
-0.0263 
0.0945 
0.2235 
0.4045 
0.5846 
0.8800 
0.0112 
0.0901 
0.3182 
0.7574 
1.0025 
-0.7484 
-0.0240 
0.0699 
0.0826 
-0.7833 
-0.7136 
0.0283 
-0.1013 
0.0825 
0.1227 
-0.0213 
0.0778 

10 

  St-t 
-0.2909 
-0.3909 
20.0117 
0.7380 
-0.5746 
-0.0073 
-0.0754 
-0.5754 
56.5089 
1.8644 
-0.7921 
0.6412 
0.6152 
-0.2848 
10.3356 
0.4780 
0.4315 
0.7989 
-0.5219 
0.2995 
0.1307 
0.1971 
-0.4763 
-0.3247 
0.2888 
0.1251 
0.0404 
0.1710 
0.1794 
0.3108 

 
 
 
 
 
 
𝜎̂ 
Bias 
MSE 
SE 
CIL 
CIU 

𝛽̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂3 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝜎̂ 
Bias 
MSE 
SE 
CIL 
CIU 

𝛽̂1 
Bias 
MSE 

50 

𝜎 

𝛽1 

𝛽2 

𝛽3 

𝜙1 

𝜙2 

𝜎 

100 

𝛽1 

0.8711 
-0.1101 
0.0349 
0.1241 
0.7894 
0.9300 

0.6923 
0.3134 
0.1105 
0.0963 
0.6490 
0.7264 

1.3911 
0.4137 
0.3948 
0.2948 
1.2774 
1.5048 

Normal Error 

t Error 

0.8149 
0.2423 
0.1188 
0.1381 
0.7606 
0.8692 

169.849 
168.849 
652360 
34.675 
156.522 
183.176 

0.9941 
-0.0059 
  0.2501 
0.2017 
0.9166 
1.0717 
Paretian Error (𝜅 = 1.25) 

     Norm 

  St-t 

     Norm 

    St-t 

     Norm 

0.0863 
-0.0137 
0.0198 
0.3551 
-0.0121 
0.1847 
0.5004 
0.0004 
0.0118 
0.1746 
0.4520 
0.5488 
0.8986 
-0.0014 
0.0186 
0.2209 
0.8363 
0.9610 
-0.7449 
-0.0449 
 0.0264 
0.2443 
-0.8122 
-0.6775 
0.0543 
-0.0657 
0.0309 
0.1643 
0.0076 
0.1010 
0.9334 
-0.0666 
0.0170 
0.1347 
0.8960 
0.9707 

0.0792 
-0.0208 
0.0258 
0.2667 
0.0229 
0.1555 
0.4949 
-0.0051 
0.0163 
0.2404 
0.4787 
0.5112 
0.9083 
0.0083 
0.0196 
0.1215 
0.8831 
0.9531 
-0.6906 
 0.1968 
0.0431 
0.0962 
-0.8290 
-0.5575 
0.0803 
-0.0441 
0.0206 
0.1237 
0.0390 
0.1127 
0.7397 
-0.2603 
0.0742 
0.0836 
0.7178 
0.7616 

0.0677 
-0.0323 
0.0557 
0.2473 
0.0069 
0.1346 
0.5349 
-0.0349 
0.0401 
0.2325 
0.4687 
0.6010 
0.9122 
-0.0122 
0.0496 
0.1092 
0.8832 
0.9411 
-0.6860 
0.0140 
0.0273 
0.0974 
-0.7127 
-0.6593 
0.0788 
-0.0412 
0.0279 
0.1146 
0.0464 
0.1112 
1.5421 
0.5421 
0.4703 
0.2265 
1.4804 
1.6038 

0.0883 
-0.0117 
0.0399 
0.3190 
0.0252 
0.1513 
0.5310 
0.0310 
0.0307 
0.2985 
0.4448 
0.6172 
0.9088 
0.0088 
0.0310 
0.2660 
0.8802 
0.9374 
-0.5060 
0.1932 
0.0567 
0.1196 
-0.5345 
-0.4747 
0.0945 
-0.0896 
0.0199 
0.1161 
0.0559 
0.1250 
0.9104 
-0.0896 
0.0199 
0.1157 
0.8772 
0.9435 

Normal Error 

t Error 

     Norm 

   St-t 

     Norm 

   St-t 

-2.9119 
-3.0119 
499.264 
1.8167 
-3.4155 
-2.4083 
-1.0789 
-1.5789 
353.475 
0.9547 
-1.3435 
-0.8142 
1.7127 
-2.6127 
792.217 
2.0044 
-2.2683 
-1.1572 
-0.5743 
0.1257 
0.0426 
0.0725 
-0.6029 
-0.5627 
0.1740 
0.1078 
0.0347 
0.0825 
0.1512 
0.1969 
60.540 
59.540 
25203 
8.7382 
58.1179 
65.9622 

Paretian Error (𝜅 = 1.25) 
     Norm 

0.1001 
0.0001 
0.0072 

0.0964 
-0.0036 
0.0106 

0.1288 
0.0288 
0.0186 

0.1085 
0.0085 
0.0103 

4.9025 
4.8025 
2028.2 

11 

  St-t 
0.3695 
-0.2695 
4.5549 
0.3675 
0.2693 
0.4697 
0.3599 
-0.1401 
2.4658 
0.9719 
0.0898 
0.6300 
1.1110 
0.2110 
1.4357 
1.3471 
0.7256 
1.4965 
-0.4243 
0.2757 
0.1184 
0.3794 
-0.5284 
-0.3202 
0.1980 
0.0780 
0.0210 
0.3210 
0.1090 
0.2871 
1.1750 
0.1750 
0.3459 
0.1975 
1.1198 
1.2303 

  St-t 
0.1551 
0.0551 
0.7551 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
SE 
CIL 
CIU 
𝛽̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂3 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝜎̂ 
Bias 
MSE 
SE 
CIL 
CIU 

𝛽2 

𝛽3 

𝜙1 

𝜙2 

𝜎 

0.1271 
0.0805 
0.1197 
0.5123 
0.0123 
0.0078 
0.1415 
0.4969 
0.5277 
0.8996 
-0.0004 
0.0062 
0.1340 
0.8745 
0.9248 
-0.7217 
-0.0217 
0.0148 
0.0881 
-07408 
-07026 
0.0912 
-0.0288 
0.0161 
0.0882 
0.0693 
0.1131 
0.9748 
-0.0252 
0.0064 
0.0891 
0.9555 
0.9941 

0.1666 
0.0769 
0.1159 
0.5129 
0.0129 
0.0101 
0.1827 
0.4885 
0.5372 
0.9047 
0.0047 
0.0083 
0.1736 
0.8754 
0.9341 
-0.5309 
0.1691 
0.0389 
0.0795 
-0.5467 
-0.5150 
0.0980 
-0.0220 
0.0101 
0.0772 
0.0797 
0.1162 
0.7721 
-0.2279 
0.0550 
0.0612 
0.7603 
0.7839 

0.2197 
0.0929 
0.1647 
0.5054 
0.0054 
0.0186 
0.2314 
0.4890 
0.5217 
0.9211 
0.0211 
0.0174 
0.2144 
0.8888 
0.9534 
-0.6996 
0.0004 
0.0091 
0.1357 
-0.7180 
-0.6812 
0.1072 
-0.0128 
0.0110 
0.1359 
0.0868 
0.1276 
1.5719 
0.5719 
0.4113 
0.1660 
1.5407 
1.6030 

0.1995 
0.0653 
0.1516 
0.4990 
-0.0010 
0.0135 
0.2146 
0.4632 
0.5348 
0.9197 
0.0197 
0.0146 
0.1940 
0.8853 
0.9541 
-0.5160 
0.1840 
0.0425 
0.0807 
-0.5330 
-0.4990 
0.0973 
-0.0227 
0.0070 
0.0724 
0.0807 
0.1139 
0.9178 
-0.0822 
0.0126 
0.0843 
0.9012 
0.9345 

3.0280 
4.2516 
5.5535 
19.0216 
18.5216 
9080.24 
3.0746 
18.3705 
19.6726 
20.3116 
19.4116 
3555.75 
3.1505 
19.8952 
20.7280 
-0.6227 
0.0773 
0.0188 
0.6589 
-0.6344 
-0.6110 
0.1961 
0.0761 
0.0161 
1.0997 
0.1826 
0.2096 
175.337 
174.332 
444540 
16.5886 
191.887 
159.144 

0.3802 
0.0596 
0.3095 
0.3903 
-0.1097 
1.5062 
0.4928 
0.1974 
0.5832 
0.7991 
-0.1009 
0.6635 
0.2957 
0.7643 
0.8339 
-0.3619 
0.3381 
0.1393 
0.7619 
-0.3768 
-0.3469 
0.2485 
0.1285 
0.0276 
0.7410 
0.2337 
0.2632 
1.2211 
0.2211 
0.3115 
0.1521 
1.1916 
1.2507 

Table 1 displays the simulation results for the case without outlier in the data with different sample sizes. 
From  the  table  we  can  say  that  error  terms  based  on  normal  distribution  and  t  distribution  cases  have 
similar  performance.  When  the  error  distribution  is  normal  the  CML  estimators  based  on  the  normal 
distribution  perform  the  best  and  the  performance  of  the  CML  estimator  based  on  the  t  distribution  is 
comparable  with  the  estimators  based  on  the  normal  distribution.  On  the  other  hand,  if  the  error 
distribution is the t distribution the estimators based on t distribution are the best, and it is followed by the 
normal  distribution.  Finally,  for  the  Pareto  distributed  error  case  the  estimators  based  on  the  normal 
distribution drastically affected and give the worst results with the larger MSE and the bias values. But, 
for  the  Pareto  distributed  error  case  the  estimators  obtained  from  the  t  distribution  behave  much  better 
than the normal case. 

12 

 
 
 
 
 
 
 
 
 
 
 
 
 
Simulation results with 10% outliers 

Table 2. Bias, MSE, SE, CIL and CIU values of the estimates with outliers  
True Values (𝛽1, 𝛽2, 𝛽3)′  = (0.1,  0.5,  0.9)′ , (𝜙1, 𝜙2)′ = (−0.7, 0.12) and 𝜎 = 1 

n 

25 

50 

Parameter 

𝛽1 

𝛽2 

𝛽3 

𝜙1 

𝜙2 

𝜎 

𝛽1 

𝛽̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂3 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝜎̂ 
Bias 
MSE 
SE 
CIL 
CIU 

𝛽̂1 
Bias 
MSE 
SE 
CIL 
CIU 

Normal Error 

Normal 
-0.1805 
-0.2805 
14.395 
0.7206 
-0.3517 
-0.0094 
1.1081 
0.6081 
19.419 
2.2292 
0.9673 
1.2490 
0.2261 
-0.6739 
15.333 
2.2848 
0.0442 
0.4079 
-0.0335 
0.6665 
3.0020 
5.2471 
-0.2352 
0.1683 
-2.0472 
-2.1672 
19.1661 
2.2688 
-2.1668 
-1.9275 
17.8423 
16.8423 
417.615 
87.2684 
16.3839 
19.3007 

          t 

0.1277 
0.0277 
0.0666 
0.6763 
-0.2601 
0.5154 
0.5161 
0.0161 
0.1170 
0.7016 
0.1159 
0.9163 
0.9344 
0.0344 
0.0984 
0.8055 
0.7948 
1.0740 
-0.2303 
0.4697 
0.3129 
0.0421 
-0.3086 
-0.1521 
0.1866 
0.0666 
0.0970 
0.0995 
0.0904 
0.2829 
0.9916 
-0.0084 
0.0277 
0.1956 
0.9112 
1.0720 

t Error 
Normal                    t 
-0.7210 
-0.8210 
24.5021 
0.6216 
-0.8925 
-0.5494 
0.4523 
-0.0477 
14.6355 
0.6484 
0.2394 
0.6652 
0.6559 
-0.2441 
18.1168 
0.9610 
0.5443 
0.7674 
0.0294 
0.7294 
2.1621 
0.3258 
-0.1113 
0.1701 
-1.7184 
-1.8384 
13.2772 
0.3081 
-2.2067 
-1.2301 
19.3621 
18.3621 
452.471 
4.2045 
17.7795 
20.9447 

-0.0001 
-0.1001 
0.1529 
0.6371 
-0.2184 
0.2182 
0.5571 
0.0571 
0.2807 
0.5687 
0.2805 
0.8336 
0.9139 
0.0139 
0.1785 
0.9656 
0.7959 
1.0320 
-0.2678 
0.4322 
0.3226 
0.0030 
-0.3572 
-0.1784 
0.2123 
0.1005 
0.0923 
0.1152 
0.1214 
0.3031 
1.1540 
0.1540 
0.0759 
0.2637 
1.0451 
1.2628 

Normal Error 

t Error 

                 t 

0.1139 
0.0139 
0.0363 
0.4740 
-0.0874 
0.3152 

Normal                     t 
-0.2276 
-0.3276 
9.6600 
2.9891 
-0.6726 
0.2173 

0.0913 
-0.0087 
0.0531 
0.7061 
-0.0615 
0.2440 

Normal 
-0.2296 
-0.3296 
11.890 
30.410 
-0.8091 
0.3500 

13 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
𝛽2 

𝛽3 

𝜙1 

𝜙2 

𝜎 

𝛽1 

𝛽2 

𝛽3 

𝛽̂
2 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂3 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝜎̂ 
Bias 
MSE 
SE 
CIL 
CIU 

𝛽̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝛽̂3 
Bias 
MSE 
SE 
CIL 
CIU 

0.9483 
0.4483 
14.957 
25.740 
0.3299 
1.5667 
0.4665 
-0.4335 
16.155 
31.805 
0.1967 
0.7363 
0.0013 
0.7013 
  0.9785 
3.8399 
-0.0391 
0.0417 
-0.3809 
-0.7013 
0.9788 
0.3777 
-0.5184 
-0.2434 
23.1810 
22.1810 
557.262 
3.3555 
22.2536 
24.1085 

0.4933 
-0.0067 
0.0228 
0.3973 
0.3797 
0.6069 
0.8961 
-0.0039 
0.0307 
0.2983 
0.7335 
1.0587 
-0.2284 
0.4716 
0.2699 
0.1041 
-0.2299 
-0.2269 
0.2138 
0.0938 
0.0449 
0.1455 
0.1850 
0.2427 
1.0546 
0.0546 
0.0189 
0.1666 
1.0101 
1.0991 

0.3304 
-0.1696 
13.637 
2.8076 
-0.2893 
0.9501 
0.9969 
0.0969 
10.6562 
1.4603 
0.5512 
1.4426 
0.0789 
0.7789 
1.0454 
0.3321 
0.0054 
0.1524 
-0.3230 
-0.4430 
1.4744 
0.9165 
-0.5298 
-0.1163 
22.1079 
21.1079 
592.552 
3.3084 
21.2234 
22.9924 

0.5128 
0.0128 
0.0557 
0.4466 
0.3497 
0.6758 
0.9010 
0.0010 
0.0540 
0.7257 
0.8402 
0.9619 
-0.2730 
0.4270 
0.2211 
0.1633 
-0.2899 
-0.2560 
0.1971 
0.0771 
0.0450 
0.1320 
0.1826 
0.2116 
1.2178 
0.2178 
0.0812 
0.2140 
1.1584 
1.2771 

Normal Error 

                 t 

0.0977 
-0.0023 
0.0164 
0.3981 
0.0287 
0.1668 
0.5049 
0.0049 
0.0164 
0.4066 
0.4306 
0.5793 
0.9085 
0.0085 
0.0162 
0.2197 
0.8551 
0.9619 

Normal 
0.3444 
0.2444 
6.6409 
5.4338 
-0.2259 
0.9146 
0.5080 
0.0080 
8.6819 
8.2289 
-0.2487 
1.2647 
0.9028 
0.0028 
5.9380 
9.1633 
-0.0449 
1.8505 

14 

t Error 
Normal                    t 
0.5861 
0.4861 
7.8438 
4.7214 
0.3555 
0.8168 
0.9318 
0.4318 
8.2327 
4.1940 
-0.6039 
2.4675 
0.6209 
-0.2791 
11.6861 
5.5664 
-1.4063 
2.6481 

0.1064 
0.0064 
0.0199 
0.4410 
0.0258 
0.1869 
0.4921 
-0.0079 
0.0310 
0.3476 
0.4320 
0.5521 
0.9054 
0.0054 
0.0278 
0.1939 
0.8350 
0.9759 

100 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
𝜙1 

𝜙2 

𝜎 

𝜙̂1 
Bias 
MSE 
SE 
CIL 
CIU 
𝜙̂2 
Bias 
MSE 
SE 
CIL 
CIU 
𝜎̂ 
Bias 
MSE 
SE 
CIL 
CIU 

0.0245 
0.7245 
0.6194 
0.7856 
0.0101 
0.0389 
-0.1095 
-0.2295 
0.1745 
0.3773 
-0.1226 
-0.0965 
27.4424 
26.4424 
770.938 
2.9298 
26.8991 
27.9858 

-0.1722 
0.5278 
0.3110 
0.0835 
-0.1792 
-0.1653 
0.2172 
0.5278 
0.0259 
0.0513 
0.2068 
0.2275 
1.1533 
0.1533 
0.0422 
0.1340 
1.1269 
1.1796 

-0.0047 
0.6953 
0.5554 
0.0976 
-0.0167 
0.0073 
-0.0639 
-0.1839 
0.1300 
0.0244 
-0.0763 
-0.0515 
28.8648 
27.8648 
755.250 
2.7141 
28.2933 
29.4363 

-0.2039 
0.4961 
0.2652 
0.0734 
-0.2183 
-0.1894 
0.2186 
0.0986 
0.0281 
0.0555 
0.1989 
0.2383 
1.3182 
0.3182 
0.1214 
0.1875 
1.2838 
1.3526 

Table 2 shows the simulation results with 10 percent outlier in the data. When outliers are introduced in 
the data the estimators based on normal distribution are drastically worsen which is reflected to the higher 
MSE  values.  However,  the  estimators  based  on  t  distribution  still  have  excellent  performance  with 
outliers.  The  estimators  based  on  t  distribution  superior  to  the  estimators  of  the  normal  distribution  in 
terms of MSE and bias values.  

4.2 Real data examples 

Example  1.  In  this  example  we  will  analyze  the  data  set  given  by  Sheather  [16].  The  data  shows  that 
Australian Film Commission’s (ACF) yearly gross box office receipts from movies screened in Australia. 
Table 3 shows the data set. In that book two different scenarios have been applied to this data set. The first 
one is the ordinary regression model and the second one is the regression model with autoregressive error 
terms. In this paper we also consider two models and estimate the parameters of interest using the normal 
and t distributions. Table 4 shows the summary of the estimates, standard errors and the 95% confidence 
intervals  for 𝛽̂  and 𝜙̂ .  We  calculated  the  standard  errors  and  the  95%  confidence  intervals  using  the 
observed Fisher information.  

      Table 3. Australian Gross Box Office Results from 1976 to 2007  

Gross box office 
($M) 
95.3 
86.4 
119.4 
124.4 
154.2 
174.3 

Year 
1976 
1977 
1978 
1979 
1980 
1981 

Gross box office 
($M) 
334.3 
388.7 
476.4 
501.4 
536.8 
583.9 

Year 
1992 
1993 
1994 
1995 
1996 
1997 

15 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                        
210 
208 
156 
160.6 
188.6 
182.1 
223.8 
257.6 
284.6 
325 

1982 
1983 
1984 
1985 
1986 
1987 
1988 
1989 
1990 
1991 

629.3 
704.1 
689.5 
812.4 
844.8 
865.8 
907.2 
817.5 
866.6 
895.4 

1998 
1999 
2000 
2001 
2002 
2003 
2004 
2005 
2006 
2007 

We consider the following model 

𝐺𝑟𝑜𝑠𝑠𝐵𝑜𝑥𝑂𝑓𝑓𝑖𝑐𝑒 = 𝛽0 + 𝛽1𝑌𝑒𝑎𝑟𝑠 + 𝑒𝑡                                                                          

and we assume that the error terms have AR(1) model. The LS estimates show that 𝛽0 is insignificant so 
that 𝛽0 is not included in the model. The LS estimates are used as the initial values for the algorithms.  

Table 4. Parameter estimates for Australian Gross Box Office Results 

𝛽̂1 
SE 
95% CI 
𝜙̂ 
SE 
95% CI 

𝛽1 

𝜙 

AIC 
BIC 

Normal 
27.1927 
0.4263 
(27.0449) – (27.3404) 
0.8816 
2.9578 
(-0.1436) – (1.9067) 
344.2041 
347.1356 

t 
26.7505 
0.7030 
(26.5069) – (26.9941) 
0.2558 
0.0066 
(0.2358) – (0.2817) 
  344.2835 
  347.2149 

Table 4 gives a summary of estimation with both cases. The table shows the estimates standard errors and 
95% confidence intervals for two different estimation methods.  

16 

 
 
 
 
 
 
 
 
 
 
Figure 1.  Australian Gross Box Office Results data set estimated by t and normal distributions methods 

Figure 1 shows the fitted regression lines on the data. The solid line shows the fitted regression obtained 
from the t distribution and the dashed line corresponds to the normal case. When data has no outlier the 
fitted lines obtained from the normal and the t distributions have similar behavior. 

Example 2. In this example we will analyze the data set given by Rousseeuw and Leroy  [15]. The data 
shows  the  proportion  of  the  number  of  ten  million  international  phone  calls  from  Belgium  in  the  years 
1950-1973. From Figure 2 we can observe that there are outliers in the data.  Rousseeuw and Leroy [15] 
modeled this data set with a linear regression model to illustrate the performance of the robust regression 
method the least median of squares (LMS). The following table displays the data.  

Table 5. Number of International Calls from Belgium  
Number of Callsa 
Number of Callsa 
Year 
(𝑦𝑖) 
(𝑥𝑖) 
(𝑦𝑖) 
1.61 
50 
0.44 
2.12 
51 
0.47 
11.90 
52 
0.47 
12.40 
53 
0.59 
14.20 
54 
0.66 
15.90 
55 
0.73 
18.20 
56 
0.81 
21.20 
57 
0.88 
4.30 
58 
1.06 
2.40 
59 
1.20 
2.70 
60 
1.35 
2.90 
61 
1.49 

Year 
(𝑥𝑖) 
62 
63 
64 
65 
66 
67 
68 
69 
70 
71 
72 
73 

17 

 
 
   
a In tens of millions.  

We  observe  that  the  OLS  residuals  show  an  autocorrelated  structure  with  type AR(1). This  observation 
based  on  autocorrelation  function  and  the  partial  autocorrelation  function  graphs  of  the  OLS  residuals. 
Therefore,  we use a regression model with autoregressive error term with AR(1)  to model this data set 
and use normal and the t distribution to obtain estimates for the parameters.  The following table gives the 
summary  of  the  estimates  along  with  the  standard  errors  and  the  95%  confidence  intervals.  We  also 
provide the values of the AIC and BIC criteria. The values of AIC and BIC show that the  t distribution 
gives  the  better  fit  then  the  normal  distribution.  It  should  be  noticed  that  the  estimates  obtain  from  t 
distribution are much closed to the values obtained from the LMS. 

Table 6. Parameter Estimates for International Calls From Belgium 

𝛽̂0 
SE 
95% CI 
𝛽̂1 
SE 
95% CI 
𝜙̂ 
SE 
95% CI 

𝛽0 

𝛽1 

𝜙 

AIC 
BIC 

Normal 
-13.8142 
10.9242 
(-18.1848) – (-9.4436) 
0.2980 
0.1796 
(0.2262) – (0.3699) 
0.7366 
50.0647 
(-19.2934) – (20.7667) 
61.3656 
120.2654 

t 
-5.3724 
11.6686 
(-10.0408) – (-0.7040) 
0.1131 
0.1918 
(0.0363) – (0.1898) 
0.1627 
0.2422 
(0.0658) – (0.2596) 
37.4305 
72.3951 

Figure 2.  Number of International Phone Calls from Belgium Data Set Estimated by t and Normal 
Distributions 

18 

 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2 depicts the scatter plot of the data with the fitted regression lines obtain from normal and the  t 
distributions. From this figure we observe that unlike the fitted line obtain from the normal distribution, 
the fitted line from the t distribution is not affected from the outliers. 

Example  3.  We  use  the  data  set  has  been  previously  analyzed  by  Ramanathan  [14]  to  show  the 
performance of autoregressive error terms regression model. The data provides consumption of electricity 
by  residential  customers  served  by  San  Diego  Gas  and  Electric  Company.  This  data  set  consists  of  87 
quarterly  observations  for  each  4  covariates  from  the  second  quarter  of  1972  through  fourth  quarter  of 
1993. The response variable is electricity consumption as measured by the logarithm of the kwh (LKWH) 
sales  per  residential  customer.  The  explanatory  variables  are  the  per-capita  income  (LY),  the  price  of 
electricity (LPRICE), cooling degree days (CCD) and heating degree days (HDD). The linear regression 
model and the expected signs of the 𝛽’s considered in Ramanathan [14] are as follows:  

𝐿𝐾𝑊𝐻 = 𝛽0 + 𝛽1𝐿𝑌 + 𝛽2𝐿𝑃𝑅𝐼𝐶𝐸 + 𝛽3𝐶𝐷𝐷 + 𝛽4𝐻𝐷𝐷 + εt 

𝛽1 > 0,  𝛽2 < 0, 𝛽3 > 0, 𝛽4 > 0. 

It is pointed out by Ramanathan [14] that when the OLS method is used to obtain the estimates the signs 
of LPRICE, CDD and HDD are consistent with the expected ones, but estimation of LY has the reverse 
sign. They note that this unexpected result may happen due to ignoring the autocorrelation structure of the 
error term, hence they suggest using autoregressive error term regression model to model the data. They 
select the AR order 4 which minimizes the BIC. Then, they used the OLS method to find the estimators 
for the parameters, and they observed that the sign of the LY is changed towards the expectations. 

Here we use the normal and the t distributions as the error distribution and obtain the estimators for the 
parameters of interest. In Table 7 we give the estimates and the AIC values obtained from the normal and 
the  t  cases.  Note  that  the  estimates  obtained  from  the  normal  distribution  are  the  same  with  the  OLS 
estimates for the AR(4) model given in Ramanathan [14].  

Table 7. The estimated coefficients without outlier 
OLS 
-0.00234 
-0.01856 
0.06365 
0.08564 
- 

LS 
0.18625 
-0.09354 
0.00029 
0.00022 
4 

LY 
LPRICE 
CDD 
HDD 
AR order 

𝜷̂𝟏 
𝛽̂2 
𝛽̂3 
𝛽̂4 

Table 7. The estimated coefficients with outlier 

LY 
LPRICE 
CDD 
HDD 
AR order 

𝜷̂𝟏 
𝛽̂2 
𝛽̂3 
𝛽̂4 

OLS 
-2.69756 
-0.96123 
0.57743 
0.96874 
- 

LS 
1.69845 
-0.00154 
0.07264 
0.04622 
4 

Bridge 
- 
-0.08563 
0.00028 
0.00023 
4 

MMLASSO  MMBridge 

0.14879 
-0.06455 
0.00028 
0.00022 
4 

- 
-0.08563 
0.00028 
0.00023 
4 

Bridge 
- 
-0.64786 
0.91231 
0.84521 
4 

MMLASSO  MMBridge 

0.25654 
-0.00547 
0.00072 
0.00095 
4 

- 
-0.02645 
0.00036 
0.00041 
4 

LASSO 
0.05879 
-0.08455 
0.00028 
0.00022 
4 

LASSO 
3.65769 
-0.98555 
0.64135 
0.95344 
4 

19 

 
 
 
 
 
 
 
 
 
 
We notice from this table that unlike the OLS estimate without AR(4) structure the sign of 𝛽1 is positive 
when the autoregressive errors are assumed as Ramanathan [14] reported. Further, when the estimation is 
carried  out  using  the  t  distribution,  the  AIC  value  is  smaller  than  the  AIC  obtained  from  the  normal 
distribution.  Thus,  it  can  be  concluded  that  the  t  distribution  may  provide  better  fit  than  the  normal 
distribution for this data. 

5. Discussion 

In this paper, we have proposed to use the t distribution as an alternative to the normal distribution as the 
error distribution in linear regression model with autoregressive error terms. The simulation results and the 
real data examples have shown that the t and the normal distributions give similar results when there is no 
outlier  in  the  data.  On  the  other  hand,  when  the  data  have  some  outliers  the  t  distribution  has  better 
performance than the normal distribution for all the settings. Further, Example 3 has shown that the OLS 
method may fail to accurately estimate the unknown parameters when the error terms have autocorrelation 
structure.  However,  when  the  autoregressive  error  form  is  introduced  into  the  model  the  estimates  are 
correctly obtained in terms of sign from the OLS method, the normal and the t distributions. For the same 
example  we  have  also  noticed  that  the  result  obtained  from  the  t  distribution  is  better  than  the  result 
obtained from the normal according to AIC values, which may show that the t distribution can be better 
model than the normal distribution. To sum up, all of these results show that the t distribution can be used 
as  an  alternative  to  the  normal  distribution  for  parameter  estimation  in  a  linear  regression  model  with 
autoregressive error terms when the data sets have outliers and/or heavy tailed error distributions.  

Acknowledgements 

The  authors  thank  the  anonymous  referee,  the  editor  and  the  associate  editor  whose  comments, 
suggestions, and corrections have led to a considerably improvement of this paper. 

References 

[1]      Ansley,  C.  F.  (1979).  An  algorithm  for  the  exact  likelihood  of  a  mixed  autoregressive-moving 

average process. Biometrika, 66 (1): 59-65. 

[2]    Alpuim,  T.  and  El-Shaarawi,  A.  (2008).  On  the  efficiency  of  regression  analysis  with  AR(p)              

errors. Journal of Applied Statistics, 35:7, 717-737. 

[3]    Arslan,  O.  and  Genç,  A.  İ.  (2003).  Robust  location  and  scale  estimation  based  on  the  univariate 

generalized  t  (GT)  distribution.  Communications  in  Statistics  –  Theory  and  Methods,  32:8,  1505-

1525.  

20 

 
 
 
 
[4]  Arslan, O. and Genç, A. İ. (2009). The skew generalized t distribution as the scale mixture of a skew 

exponential  power  distribution  and  its  applications  in  robust  estimation.  Statistics:  A  Journal  of 

Theoretical and Applied Statistics, 43:5, 481-498.  

[5]     Beach,  C.M.  and  McKinnon,  J.  G.  (1978).  A  maximum  likelihood  procedure  for  regression  with 

autocorrelated errors. Econometrica, 46:1, 51- 58. 

[6]  Huang, X., Zhou, Y. and Zhang R. (2004). A multiscale model for MPEG-4 varied bit rate video  

       traffic. IEEE Transactions on Broadcasting, 50:3, 323-334.   

[7]  Hill, J. B. (2013). Least tail-trimmed squares for infinite variance autoregressions. Journal of 

       Time Series Analysis, 34, 168-186.  

[8]  Hill, J. B. (2015a). Robust estimation and inference for heavy tailed GARCH. Bernoulli, 21, 1629- 

1669. 

[9]  Hill, J. B. (2015b). Robust generalized empirical likelihood for heavy tailed autoregressions with 

conditionally heteroscedastic errors. Journal of Multivariate Analysis, 135, 131-152. 

[10]    Lucas,  A.  (1997).  Robustness  of  the  student  t  based  M-estimator.  Communications  in  Statistics  – 

Theory and Methods, 26(5), 1165-1182. 

[11]  Lange,  K.L.,  Little,  R.J.A.  and  Taylor,  J.M.G.  (1989).    Robust  statistical  modeling  using  the  t-

distribution. J. Am. Stat. Assoc., 84, 881–896. 

[12] McLachlan, G., Krishnan, T. (1997). The EM Algorithm  and Extensions. Wiley Series in Probability 

and Statistics, USA. 

[13] Olaomi, J.O. and Ifederu, A. (2008). Understanding estimators of linear regression model with AR(1) 

error  which  are  correlated with exponential regressor.  Asian Journal  of Mathematics and  Statistics, 

1(1), 14-23.   

[14] Ramanathan, R. (1998). Introductory Econometrics with Applications. Fort Worth: Dryden: Har- 

        court Brace College Publishers. 

[15]  Rousseeuw, P.J. and  Leroy, A.M. (1987). Robust Regression and Outlier Detection. Wiley Series, 

USA.  

[16]  Sheather S.J. (2009). A Modern Approach to Regression with R. Springer-Verlag.  

[17]  Tiku,  M.L.,  Wong,  W.,  Bian,  G.  (1999).    Estimating  parameters  in  autoregressive  models  in  non-

normal situations: symmetric innovations. Communications in Statistics Theory and Methods, 28(2), 

315-341. 

21 

 
