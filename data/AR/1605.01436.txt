Sampling Requirements for Stable Autoregressive
Estimation

Abbas Kazemipour, Student Member, IEEE, Sina Miran, Student Member, IEEE, Piya Pal, Member, IEEE,

Behtash Babadi, Member, IEEE, and Min Wu, Fellow, IEEE

1

7
1
0
2

n
a
J

7
1

]
T
I
.
s
c
[

2
v
6
3
4
1
0
.
5
0
6
1
:
v
i
X
r
a

Abstract—We consider the problem of estimating the pa-
rameters of a linear univariate autoregressive model with sub-
Gaussian innovations from a limited sequence of consecutive
observations. Assuming that the parameters are compressible,
we analyze the performance of the ℓ1-regularized least squares
as well as a greedy estimator of the parameters and characterize
the sampling trade-offs required for stable recovery in the non-
asymptotic regime. In particular, we show that for a ﬁxed sparsity
level, stable recovery of AR parameters is possible when the
number of samples scale sub-linearly with the AR order. Our
results improve over existing sampling complexity requirements
in AR estimation using the LASSO, when the sparsity level scales
faster than the square root of the model order. We further derive
sufﬁcient conditions on the sparsity level that guarantee the
minimax optimality of the ℓ1-regularized least squares estimate.
Applying these techniques to simulated data as well as real-world
datasets from crude oil prices and trafﬁc speed data conﬁrm our
predicted theoretical performance gains in terms of estimation
accuracy and model selection.

Index Terms—linear autoregressive processes, sparse estima-

tion, compressive sensing, sampling.

I. INTRODUCTION

Autoregressive (AR) models are among the most funda-
mental tools in analyzing time series. Applications include
ﬁnancial time series analysis [2] and trafﬁc modeling [3]–
[8]. Due to their well-known approximation property, these
models are commonly used to represent stationary processes
in a parametric fashion and thereby preserve the underlying
structure of these processes [9]. In order to leverage the
approximation property of AR models, often times parameter
sets of very large order are required [10]. For instance,
any autoregressive moving average (ARMA) process can be
represented by an AR process of inﬁnite order. Statistical
inference using these models is usually performed through
ﬁtting a long-order AR model to the data, which can be viewed
as a truncation of the inﬁnite-order representation [11]–[14]. In
general, the ubiquitous long-range dependencies in real-world
time series, such as ﬁnancial data, results in AR model ﬁts
with large orders [2].

A. Kazemipour, S. Miran, B. Babadi and M. Wu are with the Department
of Electrical and Computer Engineering (ECE), University of Maryland, Col-
lege Park, MD 20742 USA (e-mails: kaazemi@umd.edu; smiran@umd.edu;
behtash@umd.edu; minwu@umd.edu). P. Pal
is with the Department of
ECE, University of California, San Diego, La Jolla, CA 92093 (e-mail:
pipal@ucsd.edu).

This work has been presented in part at the 50th Annual Conference on

Information Sciences and Systems, 2016 [1].

Corresponding author: Behtash Babadi (e-mail: behtash@umd.edu).
Copyright (c) 2017 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

In various applications of interest, the AR parameters ﬁt
to the data exhibit sparsity, that is, only a small number of
the parameters are non-zero. Examples include autoregressive
communication channel models, quasi-oscillatory data tuned
around speciﬁc frequencies and ﬁnancial time series [8], [15],
[16]. The non-zero AR parameters in these models correspond
to signiﬁcant time lags at which the underlying dynamics
operate. Traditional AR order selection criteria such as the Fi-
nal Prediction Error (FPE) [17], Akaike Information Criterion
(AIC) [18] and Bayesian Information Criterion (BIC) [19], are
based on asymptotic lower bounds on the mean squared pre-
diction error. Although there exist several improvements over
these traditional results aiming at exploiting sparsity [11], [14],
[20], the resulting criteria pertain to the asymptotic regimes
and their ﬁnite sample behavior is not well understood [21].
Non-asymptotic results for AR estimation, such as [21], [22],
do not fully exploit the sparsity of the underlying parameters
in favor of reducing the sample complexity. In particular, for
an AR process of order p, sufﬁcient sampling requirements of
(p5)
p are established in [21]
p and n
n
and [22], respectively.

(p4)

∼ O

∼ O

≫

≫

A relatively recent line of research employs the theory
of compressed sensing (CS) for studying non-asymptotic
sampling-complexity trade-offs for regularized M-estimators.
the CS theory has become the standard
In recent years,
framework for measuring and estimating sparse statistical
models [23]–[25]. The theoretical guarantees of CS imply that
when the number of incoherent measurements are roughly
proportional to the sparsity level, then stable recovery of the
model parameters is possible. A key underlying assumption
in many existing theoretical analyses of linear models is the
independence and identical distribution (i.i.d.) of the covari-
ates’ structure. The matrix of covariates is either formed by
fully i.i.d. elements [26], [27], is based on row-i.i.d. correlated
designs [28], [29], is Toeplitz-i.i.d. [30], or circulant i.i.d.
[31], where the design is extrinsic, ﬁxed in advance and is
independent of the underlying sparse signal. The matrix of
covariates formed from the observations of an AR process does
not ﬁt into any of these categories, as the intrinsic history of
the process plays the role of the covariates. Hence the under-
lying interdependence in the model hinders a straightforward
application of existing CS results to AR estimation. Recent
non-asymptotic results on the estimation of multi-variate AR
(MVAR) processes have been relatively successful in utilizing
sparsity for such dependent structures. For Gaussian and
low-rank MVAR models, respectively, sub-linear sampling
requirements have been established in [32], [33] and [34],

 
 
 
 
 
 
using regularized LS estimators, under bounded operator norm
assumptions on the transition matrix. These assumptions are
shown to be restrictive for MVAR processes with lags larger
than 1 [35]. By relaxing these boundedness assumptions for
Gaussian, sub-Gaussian and heavy-tailed MVAR processes,
(s log p) and
respectively, sampling requirements of n
((s log p)2) have been established in [36] and [35], [37].
O
However, the quadratic scaling requirement in the sparsity
level for the case of sub-Gaussian and heavy-tailed innovations
incurs a signiﬁcant gap with respect to the optimal guarantees
of CS (with linear scaling in sparsity), particularly when the
sparsity level s is allowed to scale with p.

∼ O

In this paper, we consider two of the widely-used esti-
mators in CS, namely the ℓ1-regularized Least Squares (LS)
or the LASSO and the Orthogonal Matching Pursuit (OMP)
estimator, and extend the non-asymptotic recovery guarantees
of the CS theory to the estimation of univariate AR pro-
cesses with compressible parameters using these estimators.
In particular, we improve the aforementioned gap between
non-asymptotic sampling requirements for AR estimation and
those promised by compressed sensing by providing sharper
sampling-complexity trade-offs which improve over existing
results when the sparsity grows faster than the square root of
p. Our focus on the analysis of univariate AR processes is mo-
tivated by the application areas of interest in this paper which
correspond to one-dimensional time series. Existing results in
the literature [32]–[37], however, consider the MVAR case
and thus are broader in scope. We will therefore compare our
results to the univariate specialization of the aforementioned
results. Our main contributions can be summarized as follows:
First, we establish that for a univariate AR process with
sub-Gaussian innovations when the number of measurements
scales sub-linearly with the product of the ambient dimension
p and the sparsity level s, i.e., n
p, then
stable recovery of the underlying AR parameters is possible
using the LASSO and the OMP estimators, even though the
covariates are highly interdependent and solely based on the
2 +δ for some
history of the process. In particular, when s
0 and the LASSO is used, our results improve upon
δ
those of [35], [37], when specialized to the univariate AR case,
by a factor of pδ(log p)3/2. For the special case of Gaussian
AR processes, stronger results are available which require a
scaling of n
(s log p) [36]. Moreover, our results provide
a theory-driven choice of the number of iterations for stable
estimation using the OMP algorithm, which has a signiﬁcantly
lower computational complexity than the LASSO.

(s(p log p)1/2)

∼ O

∼ O

≪

∝

≥

p

1

×

∼ O

(s(p log p)1/2)

Second, in the course of our analysis, we establish the
Restricted Eigenvalue (RE) condition [38] for n
p design
matrices formed from a realization of an AR process in
a Toeplitz fashion, when n
p. To
this end, we invoke appropriate concentration inequalities for
sums of dependent random variables in order to capture and
control the high interdependence of the design matrix. In the
special case of a white noise sub-Gaussian process, i.e., a
sub-Gaussian i.i.d. Toeplitz measurement matrix, we show that
(s(p log p)1/2) to
our result can be strengthened from n
(s(log p)2), which improves by a factor of s/log p over
n
the results of [30] requiring n

∼ O
(s2 log p).

∼ O

≪

∼ O

2

Third, we establish sufﬁcient conditions on the sparsity level
which result in the minimax optimality of the ℓ1-regularized
LS estimator. Finally, we provide simulation results as well
as application to oil price and trafﬁc data which reveal
that the sparse estimates signiﬁcantly outperform traditional
techniques such as the Yule-Walker based estimators [39]. We
have employed statistical tests in time and frequency domains
to compare the performance of these estimators.

The rest of the paper is organized as follows. In Section
II, we will introduce the notations and problem formulation.
In Section III, we will describe several methods for the
estimation of the parameters of an AR process, present the
main theoretical results of this paper on robust estimation of
AR parameters, and establish the minimax optimality of the ℓ1-
regularized LS estimator. Section IV includes our simulation
results on simulated data as well as the real-world ﬁnancial
and trafﬁc data, followed by concluding remarks in Section V.

II. NOTATIONS AND PROBLEM FORMULATION

i to denote the vector [xi,

Throughout the paper we will use the following notations.
We will use the notation xj
, xj ]T .
· · ·
(.) and the biased
We will denote the estimated values by
estimates with the superscript (.)b. Throughout the proofs, ci’s
express absolute constants which may change from line to
line where there is no ambiguity. By cη we mean an absolute
constant which only depends on a positive constant η.
Consider a univariate AR(p) process deﬁned by

c

−

−

· · ·

2 +

+ θpxk

1 + θ2xk

xk = θ1xk

p + wk = θT xk
k

1
p + wk,
(1)
is an i.i.d sub-Gaussian innovation se-
where
quence with zero mean and variance σ2
w. This process can
be considered as the output of an LTI system with transfer
function

wk}

−∞

∞k=

−
−

{

−

H(z) =

1

−

σ2
w
p
ℓ=1 θℓz−

ℓ .

(2)

θ

k

1

θ

−

P

k1≤

Throughout the paper we will assume

η < 1 to
enforce the stability of the ﬁlter. We will refer to this assump-
tion as the sufﬁcient stability assumption, since an AR process
with poles within the unit circle does not necessarily satisfy
k1< 1. However, beyond second-order AR processes, it
k
is not straightforward to state the stability of the process in
terms of its parameters in a closed algebraic form, which
in turn makes both the analysis and optimization procedures
intractable. As we will show later, the only major requirement
of our results is the boundedness of the spectral spread (also
referred to as condition number) of the AR process. Although
the sufﬁcient stability condition is more restrictive, it will
signiﬁcantly simplify the spectral constants appearing in the
analysis and clariﬁes the various trade-offs in the sampling
bounds (See, for example, Corollary 1).

The AR(p) process given by

in (1) is stationary
{
in the strict sense. Also by (2) the power spectral density of
the process equals

xk}

−∞

∞k=

S(ω) =

1

|

−

P

σ2
w
p
ℓ=1 θℓe−

2 .

jℓω

|

(3)

The sufﬁcient stability assumption implies boundedness of

the spectral spread of the process deﬁned as

ρ = sup

S(ω)

ω

S(ω).

inf
ω

.

We will discuss how this assumption can be further relaxed in
Appendix A-B. The spectral spread of stationary processes in
general is a measure of how quickly the process reaches its
ergodic state [21]. An important property that we will use later
in this paper is that the spectral spread is an upper bound on
the eigenvalue spread of the covariance matrix of the process
of arbitrary size [40].

We will also assume that the parameter vector θ is com-
pressible (to be deﬁned more precisely later), and can be well
p. We observe
approximated by an s-sparse vector where s
n consecutive snapshots of length p (a total of n + p
1
samples) from this process given by
p+1 and aim
to estimate θ by exploiting its sparsity; to this end, we aim
at addressing the following questions in the non-asymptotic
regime:

xk}

n
k=

≪

−

{

−

•

•

•

Are the conventional LASSO-type and greedy techniques
suitable for estimating θ?
What are the sufﬁcient conditions on n in terms of p and
s, to guarantee stable recovery?
Given these sufﬁcient conditions, how do these estimators
perform compared to conventional AR estimation tech-
niques?
Traditionally,

the Yule-Walker (YW) equations or least
squares formulations are used to ﬁt AR models. Since these
methods do not utilize the sparse structure of the parameters,
they usually require n
p samples in order to achieve
satisfactory performance. The YW equations can be expressed
as

≫

×

1
p,

(4)

1
p + σ2
w,

Rθ = r−
−

r0 = θT r−
−
p = E[xp
xpT
where R := Rp
p covariance
1 ] is the p
1
×
matrix of the process and rk = E[xixi+k] is the autocor-
relation of the process at lag k. The covariance matrix R
and autocorrelation vector r−
1
p are typically replaced by their
−
sample counterparts. Estimation of the AR(p) parameters from
the YW equations can be efﬁciently carried out using the
Burg’s method [41]. Other estimation techniques include LS
regression and maximum likelihood (ML) estimation. In this
paper, we will consider the Burg’s method and LS solutions
as comparison benchmarks. When n is comparable to p, these
two methods are known to exhibit substantial performance
differences [42].

When ﬁtted to the real-world data, the parameter vector θ
usually exhibits a degree of sparsity. That is, only certain lags
in the history have a signiﬁcant contribution in determining
the statistics of the process. These lags can be thought of
as the intrinsic delays in the underlying dynamics. To be
more precise, for a sparsity level s < p, we denote by
the support of the s largest elements of
S
θ in absolute value, and by θs the best s-term approximation
to θ. We also deﬁne

⊂ {

1, 2,

· · ·

, p

}

σs(θ) :=

θ

k

θsk1 and ςs(θ) :=

k

θ

−

θsk2,

−

(5)

3

≤

which capture the compressibility of the parameter vector
θ in the ℓ1 and ℓ2 sense, respectively. Note that by def-
inition ςs(θ)
σs(θ). For a ﬁxed ξ
(0, 1), we say
∈
that θ is (s, ξ)-compressible if σs(θ) =
(s1
ξ ) [43]
O
and (s, ξ, 2)-compressible if ςs(θ) =
(s1
ξ ). Note that
−
(s, ξ, 2)-compressibility is a weaker condition than (s, ξ)-
compressibility and when ξ = 0, the parameter vector θ is
exactly s-sparse.

O

−

1

1

≪

Finally, in this paper, we are concerned with the compressed
sensing regime where n
p, i.e., the observed data has a
much smaller length than the ambient dimension of the param-
eter vector. The main estimation problem of this paper can be
summarized as follows: given observations xn
p+1 from an AR
−
process with sub-Gaussian innovations and bounded spectral
spread, the goal is to estimate the unknown p-dimensional
(s, ξ, 2)-compressible AR parameters θ in a stable fashion
(where the estimation error is controlled) when n

p.

≪

III. THEORETICAL RESULTS

In this section, we will describe the estimation procedures

and present the main theoretical results of this paper.

(6)

(7)

A. ℓ1-regularized least squares estimation
Given the sequence of observations xn
−

θ, the normalized estimation error can be expressed as:
1
n

xn
1 −

:=

X

L

θ

θ

b

2

,

p+1 and an estimate

where

(cid:13)
(cid:13)
(cid:13)
2
−
3

−

(cid:16)
(cid:17)
1 xn
xn
b
−
2 xn
xn
...
...
x0

x

−

1

−

· · ·
· · ·
. . .

· · ·

X = 





2
(cid:13)
(cid:13)
xn
b
(cid:13)
xn

p

−
p

−

−

...

x

−

p+1

1



.






Note that the matrix of covariates X is Toeplitz with highly

interdependent elements. The LS solution is thus given by:

θLS = arg min

L(θ),

(8)

θ

Θ

∈

where

b
θ
Θ :=
{

Rp

θ

k1< 1

η

}

∈

−

−

k1< 1

| k
is the convex feasible region for which the stability of the
process is guaranteed. Note that the sufﬁcient constraint of
θ
η is by no means necessary for stability. However,
k
the set of all θ resulting in stability is in general not convex.
We have thus chosen to cast the LS estimator of Eq. (8) –as
well as its ℓ1-regularized version that follows– over a convex
subset Θ, for which fast solvers exist. In addition, as we will
show later, this assumption signiﬁcantly clariﬁes the various
constants appearing in our theoretical analysis. In practice, the
Yule-Walker estimate is obtained without this constraint, and
is guaranteed to result in a stable AR process. Similarly, for
the LS estimate, this condition is relaxed by obtaining the
unconstrained LS estimate and checking post hoc for stability
[44].

Consistency of the LS estimator given by (8) was shown
for Gaussian innovations. In the
in [2] when n
case of Gaussian innovations the LS estimates correspond to

→ ∞

conditional ML estimation and are asymptotically unbiased
under mild conditions, and with p ﬁxed, the solution converges
. For ﬁxed p, the
to the true parameter vector as n
estimation error is of the order
p/n) in general [30].
However, when p is allowed to scale with n, the convergence
rate of the estimation error is not known in general.
In the regime of interest in this paper, where n

p, the
LS estimator is ill-posed and is typically regularized with a
smooth norm. In order to capture the compressibility of the
parameters, we consider the ℓ1-regularized LS estimator:

→ ∞
(

≪

p

O

θℓ1 := arg min

θ

Θ

∈

L(θ) + γnk

θ

k1,

(9)

b

where γn > 0 is a regularization parameter. This estimator,
deemed as the Lagrangian form of the LASSO [45], has been
comprehensively studied in the sparse recovery literature [46]–
[48] as well as AR estimation [20], [22], [28], [47]. A general
asymptotic consistency result for LASSO-type estimators was
established in [47]. Asymptotic consistency of LASSO-type
estimators for AR estimation was shown in [20], [28]. For
sparse models, non-asymptotic analysis of the LASSO with
covariate matrices from row-i.i.d. correlated design has been
established in [28], [46].

≫

∼ O

In many applications of interest, the data correlations are
exponentially decaying and negligible beyond a certain lag,
and hence for large enough p, autoregressive models ﬁt the
data very well in the prediction error sense. An important
question is thus how many measurements are required for
estimation stability? In the overdetermined regime of n
p,
the non-asymptotic properties of LASSO for model selection
of AR processes has been studied in [22], where a sampling
(p5) is established. Recovery guarantees
requirement of n
for LASSO-type estimators of multivariate AR parameters in
the compressive regime of n
p are studied in [32]–[37]. In
≪
particular, sub-linear scaling of n with respect to the ambient
dimension is established in [32], [33] for Gaussian MVAR
processes and in [34] for low-rank MVAR processes, respec-
tively, under the assumption of bounded operator norm of the
transition matrix. In [36] and [35], [37], the latter assumption
is relaxed for Gaussian, sub-Gaussian, and heavy-tailed MVAR
processes, respectively. These results have signiﬁcant practical
implications as they will reveal sufﬁcient conditions on n with
respect to p as well as a criterion to choose γn, which result in
stable estimation of θ from a considerably short sequence of
observations. The latter is indeed the setting that we consider
in this paper, where the ambient dimension p is ﬁxed and the
goal is to derive sufﬁcient conditions on n
p resulting in
stable estimation.

≪

It is easy to verify that the objective function and constraints
in Eq. (9) are convex in θ and hence
θℓ1 can be obtained using
standard numerical solvers. Note that the solution to (9) might
not be unique. However, we will provide error bounds that hold
for all possible solutions of (9), with high probability.
Recall that, the Yule-Walker solution is given by

b

θyw := arg min

J(θ) =

R−

1

1
p,

r−
−

(10)

where J(θ) :=
1
pk2. We further consider two
b
other sparse estimators for θ by penalizing the Yule-Walker

r−
−

−

b

b

k

Θ

θ
Rθ

∈

b

b

4

equations. The ℓ1-regularized Yule-Walker estimator is deﬁned
as:

θyw,ℓ2,1 := arg min

θ

Θ

∈

J(θ) + γnk

θ

k1,

(11)

b

where γn > 0 is a regularization parameter. Similarly, using
the robust statistics instead of the Gaussian statistics,
the
estimation error can be re-deﬁned as:

Rθ

J1(θ) :=

pk1,
we deﬁne the ℓ1-regularized estimates as
b
θyw,ℓ1,1 := arg min

−

r−
−

b

k

1

J1(θ) + γnk

θ

k1.

(12)

θ

Θ

∈

B. Greedy estimation

b

Although there exist fast solvers for the convex problems
of the type given by (9), (11) and (12), these algorithms
are polynomial time in n and p, and may not scale well
with the dimension of data. This motivates us to consider
greedy solutions for the estimation of θ. In particular, we
will consider and study the performance of a generalized
Orthogonal Matching Pursuit (OMP) algorithm [49], [50]. A
ﬂowchart of this algorithm is given in Table I for completeness.
At each iteration, a new component of θ for which the
gradient of the error metric f(θ) is the largest in absolute value
is chosen and added to the current support. The algorithm
proceeds for a total of s⋆ =
(s log s) steps, resulting in an
O
estimate with s⋆ components. When the error metric L(θ)
is chosen, the generalized OMP corresponds to the original
OMP algorithm. For the choice of the YW error metric J(θ),
we denote the resulting greedy algorithm by ywOMP.

θ(s⋆)
OMP
Start with the index set S(0) = ∅
b
θ(0)
and the initial estimate
OMP = 0
b

Input: f(θ), s⋆
Output:

θOMP =
b

Initialization:n
for k = 1, 2, · · · , s⋆
j = arg max
(cid:16)∇f (cid:16)
S(k) = S(k−1) ∪ {j}
θ(k)
OMP = arg min
b
end

supp(θ)⊂S(k)

(cid:12)
(cid:12)
(cid:12)

i

θ(k−1)
OMP (cid:17)(cid:17)i(cid:12)
(cid:12)
b
(cid:12)

f(θ)

TABLE I: Generalized Orthogonal Matching Pursuit (OMP)

C. Estimation performance guarantees

The main theoretical result regarding the estimation per-
formance of the ℓ1-regularized LS estimator is given by the
following theorem:

(√s),

If σs(θ) =

Theorem 1.
tive constants d0, d1, d2, d3 and d4 such that
s max
{
tion parameter γn = d2
the bound

there exist posi-
for n >
and a choice of regulariza-
θℓ1 to (9) satisﬁes

d0(log p)2, d1(p log p)1/2

log p
n , any solution

O

}

q
s log p
n

r

+

b
d3σs(θ) 4
r

p

log p
n

,

(13)

d3

θ

θℓ1 −
(cid:13)
(cid:13)
(cid:13) b

2 ≤

(cid:13)
(cid:13)
(cid:13)

( 1
with probability greater than 1
nd4 ). The constants
depend on the spectral spread of the process and are explicitly
given in the proof.

− O

Similarly, the following theorem characterizes the estima-

tion performance bounds for the OMP algorithm:
Theorem 2. If θ is (s, ξ, 2)-compressible for some ξ < 1/2,
there exist positive constants d′0, d′1, d′2, d′3 and d′4 such that
for n > s log s max
the OMP
{
estimate satisﬁes the bound

d′0(log p)2, d′1(p log p)1/2

}

,

θOMP
(cid:13)
(cid:13)
(cid:13) b

θ

−

2 ≤

d′2

r

s log s log p
n

+ d′3

log s
2

1
ξ −

s

(14)

(cid:13)
(cid:13)
after s⋆ = 4ρs log 20ρs iterations with probability greater
(cid:13)
. The constants depend on the spectral spread
than 1
of the process and are explicitly given in the proof.

−O

1
nd

′
4

(cid:16)

(cid:17)

1

k

θ

k1≤

The results of Theorems 1 and 2 suggest that under suitable
compressibility assumptions on the AR parameters, one can
estimate the parameters reliably using the ℓ1-regularized LS
and OMP estimators with much fewer measurements com-
pared to those required by the Yule-Walker/LS based methods.
To illustrate the signiﬁcance of these results further, several
remarks are in order:
Remark 1. The sufﬁcient stability assumption of
−
η < 1 is restrictive compared to the class of stable AR
models. In general, the set of parameters θ which admit a
stable AR process is not necessarily convex. This condition
ensures that
the resulting estimates of (9)-(12) pertain to
stable AR processes and at the same time can be obtained by
convex optimization techniques, for which fast solvers exist.
A common practice in AR estimation, however, is to solve
for the unconstrained problem and check for the stability of
the resulting AR process post hoc. In our numerical studies in
Section IV, this procedure resulted in a stable AR process in
all cases. Nevertheless, the stability guarantees of Theorems 1
and 2 hold for the larger class of stable AR processes, even
though they may not necessarily be obtained using convex
optimization techniques. We further discuss this generalization
in Appendix A-B.
Remark 2. When θ = 0, i.e., the process is a sub-Gaussian
white noise and hence the matrix X is i.i.d. Toeplitz with
sub-Gaussian elements, the constants d1 and d′1 in Theorems
1 and 2 vanish, and the measurement requirements strengthen
to n > d0s(log p)2 and n > d′0s log s(log p)2, respectively.
Comparing this sufﬁcient condition with that of [30] given by
1
(s2 log p) reveals an improvement of order s(log p)−
n
by our results.
Remark 3. When θ
= 0, the dominant measurement require-
ments are n > d1s(p log p)1/2 and n > d′1s log s(p log p)1/2.
(s(p log p)1/2) of
Comparing the sufﬁcient condition n
Theorem 1 with those of [23]–[25], [46] for linear models
with i.i.d. measurement matrices or row-i.i.d. correlated de-
(s log p) a loss of order
signs [28], [29] given by n
these conditions
O
require n
p. However, the loss seems to be natural as it
stems from a major difference of our setting as compared to
traditional CS: each row of the measurement matrix X highly

((p/log p)1/2) is incurred, although all

∼ O

∼ O

∼ O

≪

5

1

∈

≥

O

p

≪

∼ O

0, our results imply n

depends on the entire observation sequence xn
1 , whereas in
traditional CS, each row of the measurement matrix is only
related to the corresponding measurement. Hence, the afore-
mentioned loss can be viewed as the price of self-averaging of
the process accounting for the low-dimensional nature of the
covariate sample space and the high inter-dependence of the
covariates to the observation sequence. Recent results on M-
estimation of sparse MVAR processes with sub-Gaussian and
(s2(log p)2)
heavy-tailed innovations [35], [37] require n
∼ O
when specialized to the univariate case, which compared to our
((p/log p)1/2) to (log p)2 with
results improve the loss of
the additional cost of quadratic requirement in the sparsity s.
2 +δ for some
However, in the over-determined regime of s
p
∝
(p1+δ(log p)1/2), providing a
δ
saving of order pδ(log p)3/2 over those of [35], [37].
Remark 4. It can be shown that the estimation error for the LS
p/n [30] which is not desirable
method in general scales as
p. Our result, however, guarantees a much smaller
when n
p
the sufﬁciency
s log p/n. Also,
error rate of the order
conditions of Theorem 2 require high compressibility of the
parameter vector θ (ξ < 1/2), whereas Theorem 1 does
not impose any extra restrictions on ξ
(0, 1). Intuitively
speaking, these two comparisons reveal the trade-off between
computational complexity and measurement/compressibility
requirements for convex optimization vs. greedy techniques,
which are well-known for linear models [51].
Remark 5. The condition σs(θ) =
(√s) in Theorem 1 is not
restricting for the processes of interest in this paper. This is
due to the fact that the boundedness assumption on the spectral
spread implies an exponential decay of the parameters (See
Lemma 1 of [21]). Finally, the constants d1, d′1 are increasing
with respect to the spectral spread of the process ρ. Intuitively
speaking, the closer the roots of the ﬁlter given by (2) get to the
unit circle (corresponding to larger ρ and smaller η), the slower
the convergence of the process will be to its ergodic state, and
hence more measurements are required. A similar dependence
to the spectral spread has appeared in the results of [21] for
ℓ2-regularized least squares estimation of AR processes.
Remark 6. The main ingredient in the proofs of Theorems 1
and 2 is to establish the restricted eigenvalue (RE) condition
introduced in [38] for the covariates matrix X. Establishing
the RE condition for the covariates matrix X is a nontrivial
problem due to the high interdependence of the matrix entries.
We will indeed show that if the sufﬁcient stability assumption
holds, then with n
s max
}
{
the sample covariance matrix is sharply concentrated around
(cid:1)
the true covariance matrix and hence the RE condition can
be guaranteed. All constants appearing in Theorems 1 and 2
are explicitly given in Appendix A-B. As a typical numerical
example, for η = 0.9 and σ2
w = 0.1, the constants of Theorem
108, d2 ≈
1 can be chosen as d0 ≈
0.15, d3 ≈
1000, d1 ≈
140, and d4 = 1. The full proofs are given in Appendix A-B.

d0(log p)2, d1(p log p)1/2

∼ O

O

×

3

(cid:0)

D. Minimax optimality

In this section, we establish the minimax optimality of
the ℓ1-regularized LS estimator for AR processes with sparse
of
parameters. To this end, we will focus on the class

H

6
1/2

H
.

(15)

2
pre(
R

θℓ1)

≤

d5

s log p
n

+ σ2
w.

(19)

θ

stationary processes which admit an AR(p) representation with
s-sparse parameter θ such that
η < 1. The
theoretical results of this section are inspired by the results
of [21] on non-asymptotic order selection via ℓ2-regularized
LS estimation in the absence of sparsity, and extend them by
studying the ℓ1-regularized LS estimator of (9).
We deﬁne the maximal estimation risk over

k1≤

to be

−

1

k

R

est(

θ) := sup

E

H (cid:16)

k
h

θ

θ

2
2

k

−

i(cid:17)

The minimax estimator is the one minimizing the maximal
b
estimation risk, i.e.,

b

θminimax := arg min

θ

Θ

∈

est(

θ).

R

(16)

b

θminimax, in general, cannot be constructed
Minimax estimator
explicitly [21], and the common practice in non-parametric
θ which is order optimal
estimation is to construct an estimator
b
as compared to the minimax estimator:
b
θminimax).

(17)

est(

est(

θ)

L

b

R

≤

R

with L
1 being a constant. One can also deﬁne the
minimax prediction risk by the maximal prediction error over
all possible realizations of the process:

≥

b

b

2
pre(
R

θ) := sup
H

E

xk −

θ′xk
k

−
−

2

.

1
p

(18)

(cid:21)

b

b

(cid:17)

O

≫

(cid:20)(cid:16)
In [21], it is shown that an ℓ2-regularized LS estimator with
an order p⋆ =
(log n) is minimax optimal. This order
pertains to the denoising regime where n
p. Hence, in
order to capture long order lags of the process, one requires
a sample size exponentially large in p, which may make the
estimation problem computationally infeasible. For instance,
consider a 2-sparse parameter with only θ1 and θp being non-
(2p)
zero. Then, in order to achieve minimax optimality, n
measurements are required. In contrast, in the compressive
p, the goal, instead of selecting p, is to
regime where s, n
ﬁnd conditions on the sparsity level s, so that for a given n
and large enough p, the ℓ1-regularized estimator is minimax
optimal without explicit knowledge of the value of s (See for
example, [52]).

∼ O

≪

In the following proposition, we establish the minimax
optimality of the ℓ1-regularized estimator over the class of
sparse AR processes with θ
Proposition 1. Let xn
s-sparse parameters satisfying
n
n
d1(p log p)1/2 ,
log p ,
min

1 be samples of an AR process with
1

η and s
−
≤
. Then, we have:

k1≤
k
n
d0(log p)2

1
η
−
√8πη

Θ:

∈

θ

n

q

est(

θℓ1)

L

est(

o
θminimax).

R

≤
where L is a constant and is only a function of η and σ2
is explicitly given in the proof.

R

b

b

w and

Remark 5. Proposition 1 implies that ℓ1-regularized LS is
minimax optimal in estimating the s-sparse parameter vector
θ, for small enough s. The proof of the Proposition 1 is given
in Appendix A-D. This result can be extended to compressible
θ in a natural way with a bit more work, but we only present
the proof for the case of s-sparse θ for brevity. We also state

6

the following proposition on the prediction performance of the
ℓ1-regularized LS estimator:
Proposition 2. Let xn
p+1 be samples of an AR process with s-
−
sparse parameters and Gaussian innovations, then there exists
a positive constant d5 such that for large enough n, p and s
satisfying n > d1s(p log p)1/2, we have:

It can be readily observed that for n

s log p the prediction
error variance is very close to the variance of the innovations.
The proof is similar to Theorem 3 of [21] and is skipped in
this paper for brevity.

≫

b

IV. APPLICATION TO SIMULATED AND REAL DATA

In this section, we study and compare the performance of
Yule-Walker based estimation methods with those of the ℓ1-
regularized and greedy estimators given in Section III. These
methods are applied to simulated data as well as real data from
crude oil price and trafﬁc speed.

A. Simulation studies

θ

− k

In order to simulate an AR process, we ﬁltered a Gaussian
white noise process using an IIR ﬁlter with sparse parameters.
Figure 1 shows a typical sample path of the simulated AR
process used in our analysis. For the parameter vector θ,
we chose a length of p = 300, and employed n = 1500
generated samples of the corresponding process for estimation.
The parameter vector θ is of sparsity level s = 3 and
k1= 0.5. A value of γn = 0.1 is used, which
η = 1
is slightly tuned around the theoretical estimate given by
Theorem 1. The order of the process is assumed to be known.
θLS using
We compare the performance of seven estimators: 1)
θℓ1 from ℓ1-
LS, 2)
θyw,ℓ2,1 using Eq.
b
regularized LS, 4)
θywOMP using the cost
θyw,ℓ1,1 using Eq. (12), and 7)
b
b
(11), 6)
function J(θ) in the generalized OMP. Note that for the LS
and Yule-Walker estimates, we have relaxed the condition of
k1< 1, to be consistent with the common usage of these
k
methods. The Yule-Walker estimate is guaranteed to result in
a stable AR process, whereas the LS estimate is not [44].
Figure 2 shows the estimated parameter vectors using these
algorithms. It can be visually observed that ℓ1-regularized and
greedy estimators (shown in purple) signiﬁcantly outperform
the Yule-Walker-based estimates (shown in orange).

θyw using the Yule-Walker equations, 3)

θOMP using OMP, 5)

θ

b

b

b

b

2

0

-2

0 

         40 

        80 

      120 

     160 

  200

Fig. 1: Samples of the simulated AR process.

In order to quantify the latter observation precisely, we
repeated the same experiment for p = 300, s = 3 and
105. A comparison of the normalized MSE of the
10
estimators vs. n is shown in Figure 3. As it can be inferred
from Figure 3, in the region where n is comparable to or

≤

≤

n

 
 
 
 
 
0.2

0

-0.2

0.1

0

-0.1

0.1

0

-0.1

0.1

0

-0.1

0.1

0

-0.1

0.1

0

-0.1

0.2

0.1

0

-0.1

0.2

0

-0.2

50

100 150 200 250 300

50

100 150 200 250 300

Fig. 2: Estimates of θ for n = 1500, p = 300, and s = 3
(These results are best viewed in the color version).

less than p (shaded in light purple), the sparse estimators have
a systematic performance gain over the Yule-Walker based
estimates, with the ℓ1-regularized LS and ywOMP estimates
outperforming the rest.

101

100

10-1

10-2

10-3

10 

       102   

      103 

     104 

     105

Fig. 3: MSE comparison of the estimators vs. the number
of measurements n. The shaded region corresponds to the
compressive regime of n < p.

The MSE comparison in Figure 3 requires one to know
the true parameters. In practice, the true parameters are not
available for comparison purposes. In order to quantify the
performance gain of these methods, we use statistical tests
to assess the goodness-of-ﬁt of the estimates. The common
chi-square type statistical tests, such as the F-test, are useful
when the hypothesized distribution to be tested against is
discrete or categorical. For our problem setup with sub-
Gaussian innovations, we will use a number of statistical
tests appropriate for AR processes, namely, the Kolmogorov-
Smirnov (KS) test, the Cram´er-von Mises (CvM) criterion, the
spectral Cram´er-von Mises (SCvM) test and the Anderson-
Darling (AD) [53]–[55]. A summary of these tests is given
in Appendix B. Table II summarizes the test statistics for
different estimation methods. Cells colored in orange (darker

7

shade in grayscale) correspond to traditional AR estimation
methods and those colored in blue (lighter shade in grayscale)
correspond to the sparse estimator with the best performance
among those considered in this work. These tests are based on
the known results on limiting distributions of error residuals.
As noted from Table II, our simulations suggest that the OMP
estimate achieves the best test statistics for the CvM, AD and
KS tests, whereas the ℓ1-regularized estimate achieves the best
SCvM statistic.

TABLE II: Goodness-of-ﬁt tests for the simulated data

Test

❳❳❳❳❳❳❳❳
Estimate
θ
bθLS
bθyw
bθℓ1
bθOMP
bθyw,ℓ2,1
bθyw,ℓ1,1
bθywOMP

CvM AD

KS

SCvM

0.31
0.68
0.65
0.34
0.29
0.35
0.42
0.29

1.54
5.12
4.87
1.72
1.45
1.80
2.33
1.46

0.031
0.037
0.034
0.030
0.028
0.032
0.040
0.030

0.009
0.017
0.025
0.009
0.009
0.009
0.008
0.009

B. Application to the analysis of crude oil prices

In this and the following subsection, we consider applica-
tions with real-world data. As for the ﬁrst application, we
apply the sparse AR estimation techniques to analyze the crude
oil price of the Cushing, OK WTI Spot Price FOB dataset
[56]. This dataset consists of 7429 daily values of oil prices in
dollars per barrel. In order to avoid outliers, usually the dataset
is ﬁltered with a moving average ﬁlter of high order. We have
skipped this procedure by visual inspection of the data and
selecting n = 4000 samples free of outliers. Such ﬁnancial
data sets are known for their non-stationarity and long order
history dependence. In order to remove the deterministic trends
in the data, one-step or two-step time differencing is typically
used. We refer to [8] for a full discussion of this detrending
method. We have used a ﬁrst-order time differencing which
resulted in a sufﬁcient detrending of the data. Figure 4 shows
the data used in our analysis. We have chosen p = 150 by
inspection. The histogram of ﬁrst-order differences as well the
estimates are shown in Figure 5.

)
l
e
r
r
a
b
/
s
r
a
l
l
o
d
(

e
c
i
r
P

60

20

1990   

        1994 

      1998 

              2002         

Year

Fig. 4: A sample segment of the Cushing, OK WTI Spot Price
FOB data.

θℓ1) and OMP (

A visual inspection of the estimates in Figure 5 shows
θOMP) estimates
that the ℓ1-regularized LS (
consistently select speciﬁc time lags in the AR parameters,
whereas the Yule-Walker and LS estimates seemingly overﬁt
the data by populating the entire parameter space. In order
to perform goodness-of-ﬁt tests, we use an even/odd two-
fold cross-validation. Table III shows the corresponding test

b

b

 
 
 
 
 
 
 
 
 
100

50

0

0.1

0

-0.1

0.1
0.05
0
-0.05

0.02
0
-0.02
-0.04
-0.06

Histogram of first-order differences

-4

-2

0

2

4

0.06

0

-0.06

0.08
0.04
0
-0.04

0.05
0
-0.05
-0.1

0.05

0

-0.05

50

100

150

50

100

150

Fig. 5: Estimates of θ for the second-order differences of the
oil price data.

statistics, which reveal that indeed the ℓ1-regularized and OMP
estimates outperform the traditional estimation techniques.

TABLE III: Goodness-of-ﬁt tests for the crude oil price data

Test

❳❳❳❳❳❳❳❳
Estimate
bθLS
bθyw
bθℓ1
bθOMP
bθyw,ℓ2,1
bθyw,ℓ1,1
bθywOMP

CvM AD

KS

SCvM

0.88
0.58
0.27
0.22
0.28
0.24
0.23

5.55
3.60
1.33
1.18
1.40
1.26
1.18

0.055
0.043
0.031
0.025
0.027
0.027
0.026

0.046
0.037
0.020
0.022
0.021
0.022
0.022

C. Application to the analysis of trafﬁc data

Our second real data application concerns trafﬁc speed data.
The data used in our simulations is the INRIX R
speed data
(cid:13)
for I-495 Maryland inner loop freeway (clockwise) between
US-1/Baltimore Ave/Exit 25 and Greenbelt Metro Dr/Exit 24
from 1 Jul, 2015 to 31 Oct, 2015 [57], [58]. The reference
speed of 65 mph is reported. Our aim is to analyze the long-
term,
large-scale periodicities manifested in these data by
ﬁtting high-order sparse AR models. Given the huge length of
the data and its high variability, the following pre-processing
was made on the original data:

1) The data was downsampled by a factor of 4 and averaged
by the hour in order to reduce its daily variability, that
is each lag corresponds to one hour.

2) The logarithm of speed was used for analysis and the
mean was subtracted. This reduces the high variability
of speed due to rush hours and lower trafﬁc during
weekends and holidays.

8

80

70

60

50

40

30

20

10

11

9

7

5

3

)
h
p
m

(

d
e
e
p
S

)
n
i
m

(

e
m
T

i

l
e
v
a
r
T

1
12 AM  

   8 AM 

             4 PM 

     11 PM

Fig. 6: A sample of the speed and travel time data for I-495.

half of the data (n = 1500) for ﬁtting, from which the AR
parameters and the distribution and variance of the innovations
were estimated. The statistical tests were designed based on
the estimated distributions, and the statistics were computed
accordingly using the second half of the data. We selected an
order of p = 200 by inspection and noting that the data seems
to have a periodicity of order 170 samples.

A segment of the log-centered data

0

-1

-2

0.6

0.4

0.2

0

0.6

0.4

0.2

0

0.3

0.2

0.1

0

0.4

0.2

0

0.4

0.2

0

0.6

0.4

0.2

0

0.4

0.2

0

50

100

150

200

50

100

150

200

Fig. 7: Estimates of θ for the trafﬁc speed data.

b

Figure 7 shows part of the data used in our analysis as well
θℓ1) and
as the estimated parameters. The ℓ1-regularized LS (
θOMP) are consistent in selecting the same components
OMP (
of θ. These estimators pick up two major lags around which θ
has its largest components. The ﬁrst lag corresponds to about
24 hours which is mainly due to the rush hour periodicity
on a daily basis. The second lag is around 150
170 hours
which corresponds to weekly changes in the speed due to
lower trafﬁc over the weekend. In contrast, the Yule-Walker
and LS estimates do not recover these signiﬁcant time lags.

−

b

Figure 6 shows a typical average weekly speed and travel
time in this dataset and the corresponding 25-75-th percentiles.
As can be seen the data shows high variability around the rush
hours of 8 am and 4 pm. In our analysis, we used the ﬁrst

Statistical tests for a selected subset of the estimators are
shown in Table IV. Interestingly, the ℓ1-regularized LS esti-
mator signiﬁcantly outperforms the other estimators in three
of the tests. The Yule-Walker estimator, however, achieves the

 
 
 
 
 
 
  
 
   
 
 
TABLE IV: Goodness-of-ﬁt tests for the trafﬁc speed data

Test

❳❳❳❳❳❳❳❳
Estimate
bθyw
bθℓ1
bθOMP
bθywOMP

CvM

AD

KS

SCvM

0.012
1.4×10−7
0.017
0.025

0.066
2.1×10−6
0.082
0.122

0.220
6.7×10−4
0.220
0.270

0.05
0.25
1.49
0.14

best SCvM test statistic.

V. CONCLUSIONS

1

p

∼

≥

O

2 +δ for some δ

In this paper, we have investigated sufﬁcient sampling
requirements for stable estimation of AR models in the non-
asymptotic regime using the ℓ1-regularized LS and greedy
estimation (OMP) techniques. We have further established
the minimax optimality of the ℓ1-regularized LS estimator.
Compared to the existing literature, our results provide several
major contributions. First, when s
0,
(pδ(log p)3/2)
our results suggest an improvement of order
in the sampling requirements for the estimation of univariate
AR models with sub-Gaussian innovations using the LASSO,
(p2(log p)2)
over those of [35] and [37] which require n
for stable AR estimation. When specialized to a sub-Gaussian
white noise process, i.e., establishing the RE condition of
i.i.d. Toeplitz matrices, our results provide an improvement of
order
(s/log p) over those of [30]. Second, although OMP
is widely used in practice, the choice of the number of greedy
iterations is often ad-hoc. In contrast, our theoretical results
prescribe an analytical choices of the number of iterations
required for stable estimation, thereby promoting the usage of
OMP as a low-complexity algorithm for AR estimation. Third,
we established the minimax optimality of the ℓ1-regularized
LS estimator for the estimation of sparse AR parameters.

∼ O

O

We further veriﬁed the validity of our theoretical results
through simulation studies as well as application to real
ﬁnancial and trafﬁc data. These results show that the sparse
estimation methods signiﬁcantly outperform the widely-used
Yule-Walker based estimators in ﬁtting AR models to the data.
Although we did not theoretically analyze the performance of
sparse Yule-Walker based estimators, they seem to perform on
par with the ℓ1-regularized LS and OMP estimators based on
our numerical studies. Finally, our results provide a striking
connection to our recent work [59], [60] in estimating sparse
self-exciting discrete point process models. These models
regress an observed binary spike train with respect to its
history via Bernoulli or Poisson statistics, and are often used
in describing spontaneous activity of sensory neurons. Our
results have shown that in order to estimate a sparse history-
dependence parameter vector of length p and sparsity s in a
(s2/3p2/3 log p)
stable fashion, a spike train of length n
is required. This leads us to conjecture that these sub-linear
sampling requirements are sufﬁcient for a larger class of
autoregressive processes, beyond those characterized by linear
models. Finally, our minimax optimality result requires the
(n/(p log p)1/2).
sparsity level s to grow at most as fast as
We consider further relaxation of this condition, as well as
the generalization of our results to sparse MVAR processes as
future work.

∼ O

O

9

APPENDIX A
PROOFS OF THEOREMS 1 AND 2

A. The Restricted Strong Convexity of the matrix of covariates

The ﬁrst element of the proofs of both Theorems 1 and
2 is to establish the Restricted Strong Convexity (RSC) for
the matrix X of covariates formed from the observed data.
First, we investigate the closely related Restricted Eigenvalue
(RE) condition. Let [λmin(s), λmax(s)] be the smallest interval
XS), where XS is a
containing the singular values of 1
sub-matrix X over an index set S of size s.
Deﬁnition 1 (Restricted Eigenvalue Condition). A matrix X
is said to satisfy the RE condition of order s if λmin(s) > 0.

n (XT

S

Although the RE condition only restricts λmin(s), in the
following analysis we also keep track of λmax(s), which
appears in some of the bounds. Establishing the RSC for X
proceeds in a sequence of lemmas (Lemmas 1–5 culminating
in Lemma 6). We ﬁrst show that the RE condition holds for
the true covariance of an AR process:
Lemma 1 (from [61]). Let R
k covariance
∈
matrix of a stationary process with power spectral density
S(ω), and denote its maximum and minimum eigenvalues by
φmax(k) and φmin(k), respectively. Then, φmax(k) is increas-
ing in k, φmin(k) is decreasing in k, and we have

k be the k

Rk

×

×

φmin(k)

inf
ω

↓

S(ω),

and φmax(k)

sup
ω

↑

S(ω).

(20)

This result gives us the following corollary:
Corollary 1 (Singular Value Spread of R). Under the sufﬁ-
cient stability assumption, the singular values of the covari-
ance R of an AR process lie in the interval

σ2
w

.

8π , σ2

w
2πη2

Proof: For an AR(p) process

h

i

S(ω) =

1
2π

σ2
w
p
ℓ=1 θℓe−

2 .

jℓω

|

1

|

−

Combining

θ

k

k1≤

1

−

η < 1 with Lemma 1 proves the claim.

P

Note that by Lemma 1, the result of Corollary 1 not only
holds for AR processes, but also for any stationary process
satisfying inf ω S(ω) > 0 and supω S(ω) <
, i.e., a process
with ﬁnite spectral spread.

∞

We next establish conditions for the RE condition to hold

R:
for the empirical covariance
Lemma 2. If the singular values of R lie in the interval
b
[λmin, λmax], then X satisﬁes the RE condition of order s⋆ with
parameters λmin(s⋆) = λmin
ts⋆ and λmax(s⋆) = λmax + ts⋆,
−
.
where t = maxi,j|
Rij |
Proof: Let

n (XT X). For every s⋆-sparse θ we have

θT

Rθ

θ

t

b
≥

−
θT Rθ + t

k
θ

2
1≥
k
2
1≤

k

(λmin

ts⋆)
−
k
(λmax + ts⋆)
k

θ

k
θ

2
2,

2
2,

k

k

θT

Rθ
b
which proves the claim.
b

≤

We will next show that t can be suitably controlled with high
probability. Before doing so, we state a key result of Rudzkis

Rij −
R = 1
b
θT Rθ

[62] regarding the concentration of second-order empirical
sums from stationary processes:
Lemma 3. Let xn
−
which satisﬁes

p+1 be samples of a stationary process

∞

xk =

kwj ,

bj

−

j=
X
−∞

where wk’s are i.i.d random variables with
k)

(˜cσw)kk! , k = 2, 3,

E(
|

wj |

|≤
|
for some constant ˜c and

∞

<

bj|

|

.
∞

,

· · ·

(21)

(22)

(23)

j=
X
−∞
Then, the biased sample autocorrelation given by
1
n + k

rb
k =

xixj

n+k

Xi,j=1,j
−

i=k

satisﬁes
P(
rb
rb
k|
k−
|

b

≤

> t)

c1(n+k) exp

c2
σw

−

t2(n + k)
w + t3/2√n + k

c3σ3

,

b

(cid:18)

(cid:19)
(24)
for
positive absolute constants c1, c2 and c3 which are
independent of the dimensions of the problem. In particular,
if xk = wk, i.e., a sub-Gaussian white noise process, c3
vanishes.

Proof: The lemma is a special case of Theorem 4 under
Condition 2 of Remark 3 in [62]. For the special case of xk =
wk, the constant H in Lemma 7 of [62] and hence c3 vanish.

Using the result of Lemma 3, we can control t and establish

R as follows:

the RE condition for
Lemma 4. Let m be a positive integer. Then, X satisﬁes the
RE condition of order (m + 1)s with a constant λmin/2 with
probability at least

b

c1p2(n + p) exp

1

−



−

c4
1 + c5

n
s
n+p

,

(25)

s )3/2 
p
( n

2(m+1) and

λmin



where c1 is the same as in Lemma 3, c4 = c2
σw
c5 =

3/2 .

q

c3σ3
w
λmin
2(m+1) (cid:17)

(cid:16)
Proof: First, note that for the given AR process, condition
(21) is veriﬁed by the Wold decomposition of the process,
condition (22) results from the sub-Gaussian assumption on
the innovations, and condition (23) results from the stability
of the process. Noting that
1
n

n + k
n

xixi+k =

Ri,i+k =

xixj =

rb
k,

1
n

n+k

n

i=k

Xi,j=1,j
−
, p

· · ·

−

i=1
X

b

· · ·
Ri,i+k −
b

P

|
(cid:16)

for i = 1,

, n and k = 0,

1, Eq. (24) implies:

Ri,i+k|

> τ

(cid:17)

c1(n + k) exp

≤

c2√τ n
w(n+k)

.
c3σ4
τ 3/2n3/2 + σw !

 −

By the union bound and k

P

max
i,j |

(cid:18)

Rij −
b

> τ

Rij |

≤

(cid:19)

p, we get:

≤
c1p2(n + p) exp

 −

c2√τ n
w(n+p)

.
c3σ4
τ 3/2n3/2 + σw !
(27)

10

Choosing τ = λmin
establishes the result of the lemma.

2(m+1)s and invoking the result of Lemma 2

We next deﬁne the closely related notion of the Restricted

Strong Convexity (RSC):

Deﬁnition 2 (Restricted Strong Convexity [63]). Let

V :=

h

{

∈

Rp

hSc

k1≤

3

hSk1+4

k

k

θSc

.

k1}

|k

(28)

Then, X is said to satisfy the RSC condition of order s if there
exists a positive κ > 0 such that

hT XT Xh =

1
n

1
n k

Xh

2
2≥

k

κ

k

h

k

2
2,

h

∀

∈

V.

(29)

The RSC condition can be deduced from the RE condition

according to the following result:

Lemma 5 (Lemma 4.1 of [38]). If X satisﬁes the RE condition
of order s⋆ = (m + 1)s with a constant λmin((m + 1)s), then
the RSC condition of order s holds with

κ = λmin((m + 1)s)

1

3

−

s

λmax(ms)
mλmin ((m + 1)s) !

2

.

(30)

We can now establish the RSC condition of order s for X:

Lemma 6. The matrix of covariates X satisﬁes the RSC
condition of order s with a constant κ = σ2
16π with probability
at least

w

c1p2(n + p) exp

1

−

cη
1 + c′η

n
s
n+p

−



s )3/2 
p
( n

and c′η = c3(16π(72+η2))3/2



η3

,

(31)

.

where cη =

c2η
√16π(72+η2)

Proof: Choosing m =

72
, and using Lemmas 2, 4, and
η2 ⌉
5 establishes the result. Note that if xk = wk, i.e., a sub-
Gaussian white noise process, then c3 and hence c′η vanish.

⌈

We are now ready prove Theorems 1 and 2.

B. Proof of Theorem 1

We ﬁrst establish the so-called vase (cone) condition for the

(26)
b

error vector h =

θ:

θℓ1 −
b
XT (xn

k∞

= 2

L(θ)

Lemma 7. For a choice of the regularization parameter γn ≥
Xθ)
, the optimal error h =
k∇
θℓ1 −
b

1 −
n k
θ belongs to the vase

k∞

Rp

h

hSc

k1≤

3

hSk1+4

k

k

θSc

.

k1}

{

∈

|k

V :=

(32)

Proof: Using several instances of the triangle inequality

 
xn
1 −

Xθ

2
2

k

+

(cid:1)

k1+
k1−k

θ

k1)

hS + θSc

k1−k

θ

k1)

we have:

0

≥

θ

h

k∞k

1 −

k1−k

2
2−k

xn
1 −
θ + h

X(θ + h)
k
k1)
Xθ)

1
n
k
γn (
(cid:0)
k
1
XT (xn
n k
θS + hSc + hS + θSc
γn (
k
γn
hSc
k1+
(
2
k
θS + hSc
γn (
k
γn
hSc
(
k1+
2
−
k
k
θSk1+
hSc
γn(
k
k
γn
hSc
(
k1−
2
k

hSk1)+
k
k1−k
hSk1)+
k1−k
hSk1−

3

k

≥ −

≥ −

=

=

k1−k

θSc

k1−k

θSk1)

P

θSc
hSk1−k
θSc
k1).
4
k

The following result of Negahban et al. [63] allows us to

characterize the desired error bound:
Lemma 8 (Theorem 1 of [63]). If X satisﬁes the RSC condi-
,
tion of order s with a constant κ > 0 and γn ≥ k∇
then any optimal solution

θℓ1 satisﬁes

L(θ)

k∞

k

θℓ1 −
b

θ

k2≤

2√sγn
b
κ

+

r

2γnσs(θ)
κ

.

(⋆)

In order to use Lemma 8, we need to control γn =
L(θ)

. We have:

k∇

k∞

L(θ) =

∇

2
n

XT (xn

1 −

Xθ),

(33)

It is easy to check that by the uncorrelatedness of the innova-
tions wk’s, we have

2
n

2
n

∇

1 −

E [

L(θ)] =

E

XT (xn

Xθ)

=

E

XT wn
1

= 0.

(cid:3)

(cid:2)

(cid:2)

(cid:3)

∇

(34)
Eq. (34) is known as the orthogonality principle. We next show
that

L(θ) is concentrated around its mean. We can write
2
n
and observe that the jth element in this expansion is of the
j+1. It is easy to check that the
form yj = xn
sequence yn
1 is a martingale with respect to the ﬁltration given
by

L(θ))i =

j+1wn

xn
−

wn
1 ,

iT
−
i+1

(
∇

i
−

−

−

Fj = σ

j+1

−
p+1

xn
−

(cid:16)

,

(cid:17)

) denote the sigma-ﬁeld generated by the random
where σ(
·
variables x
j+1. We use the following
, xn
concentration result for sums of dependent random variables
[64]:

p+1, x

p+2,

· · ·

−

−

−

Lemma 9. Fix n
Fj-
measurable random variables, satisfying for each j =
1, 2,

1. Let Zj’s be sub-Gaussian

, n,

≥

· · ·

11

Proof: This is a special case of Theorem 3.2 of [64]
or Lemma 3.2 of [65], for sub-Gaussian-weighted sums of
random variables. The constant c depends on the sub-Gaussian
constant of Zi’s.

Since yj’s are a product of two independent sub-Gaussian
random variables, they are sub-Gaussian as well. Lemma 9
implies that

P (

L(θ)i|≥

|∇

exp

t)

≤

nt2
c2
0σ4

w (cid:19)

.

−

(cid:18)

(35)

where c2
we get:

0 := c2

σ4
w

is an absolute constant. By the union bound,

L(θ)

k∇

k∞ ≥

exp

≤

t

(cid:17)

(cid:16)
be
d4
w√1 + d4

Let
c0σ2

any
positive
log p
n , we get:

t2n
c2
0σ4
w

−

+ log p

.

(36)

(cid:19)
(cid:18)
integer. Choosing

t

=

q
L(θ)

P

 k∇

k∞ ≥

c0σ2
w

1 + d4

p

log p

r

n ! ≤

2
nd4

.

log p

4c

c2
η

n with d2 := c0σ2
with probability at least 1
q

w√1 + d4,
2
nd4 .
. Using Lemma 6, the

L(θ)
k∞
′
η (3+d4)
and d1 =
cη
d0(log p)2, d1(p log p)1/2

Hence, a choice of γn = d2
satisﬁes γn ≥ k∇
Let d0 := (3+d4)2
fact that n > s max
by hypothesis,
{
and p > n we have that the RSC of order s hold for κ =
σ2
1
pd4 . Combining
16π with a probability at least 1
w
these two assertions, the claim of Theorem 1 follows for d3 =
(cid:4)
32πc0√1 + d4.

2c1
pd4 −

−

−

}

C. Proof of Theorem 2

The proof is mainly based on the following lemma, adopted
from Theorem 2.1 of [50], stating that the greedy procedure is
successful in obtaining a reasonable s⋆-sparse approximation,
if the cost function satisﬁes the RSC:

Lemma 10. Let s⋆ be a constant such that

s⋆

≥

4ρs log 20ρs,

(37)

and suppose that L(θ) satisﬁes RSC of order s⋆ with a
constant κ > 0. Then, we have

θ(s⋆)
OMP −

θS

2 ≤

√6εs⋆
κ

,

(cid:13)
where ηs⋆ satisﬁes
(cid:13)
(cid:13) b
εs⋆

(cid:13)
(cid:13)
(cid:13)
√s⋆ + s

L(θS)

.

(38)

≤

k∇
Proof: The proof is a specialization of the proof of
Theorem 2.1 in [50] to our setting with the spectral spread
ρ = 1/4η2.

k∞

E [Zj|Fj

−

1] = 0, almost surely,

In order to use Lemma 10, we need to bound

We have:

L(θS)

.

k∞

k∇

then there exists a constant c such that for all t > 0,

1
n

n

j=1
X

P





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E[Zj]

Zj −

t

≥



≤

exp

nt2
c2

.

(cid:19)

−

(cid:18)



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E [

L(θS)] =

∇

XT (xn

1 −

E

1
n
(cid:2)
= R(θ
−

θS)

≤

=

1
XθS)
n
σ2
(cid:3)
2πη2 ςs(θ)1,
w

E

XT X(θ

θS)

−

(cid:2)

(cid:3)

where in the second inequality we have used (34), and the
last inequality results from Corollary 1. Let d′4 be any positive
integer. Using the result of Lemma 9 together with the union
bound yields:

with sparse parameters θ for which the minimax risk is optimal
modulo constants. In our construction, we assume that the
innovations are Gaussian. The key element of the proof is the
Fano’s inequality:

12

P

L(θS)

c0σ2
w

1 + d′4

k∞≥

 k∇

! ≤
Hence, we get the following concentration result for εs⋆ :

p

r

log p
n

+

wςs(θ)
σ2
2πη2

2
nd′

4

.

P

εs⋆

(cid:18)

≥

√s⋆ + s

c0σ2
w

1 + d′4

(cid:18)

p

q

log p

n + σ2

wςs(θ)
2πη2

≤

(cid:19)(cid:19)

.

′
4

2
nd
(39)

′

Noting that by (37) we have s⋆ + s
4(3+d
η2c2
η
1
ξ

and d′1 =
≤
for some constant A, and invoking the results of

. By the hypothesis of ςs(θ)

4s log s
η2

. Let d′0 =

′
η (3+d4)

4)2

16c

≤

cη

−

As1
Lemmas 6 and 10, we get:

θ(s⋆)
OMP −

(cid:13)
(cid:13)
(cid:13) b

θS

(cid:13)
(cid:13)
(cid:13)

d′2

2 ≤

r

d′2

≤

r
16πc0√24(1+d′
4)
η
1
′
pd
4 −

4 −

θ

=

2c1
′
pd
−
θ(s⋆)
OMP −

where d′2 =
at least 1

s log s log p
n
s log s log p
n

+ d′′2

s log sςs(θ)

+ d′′2

p
√log s

1
ξ −

3
2

s

,

and d′′2 = A
. Finally, we have:

πη3 , with probability

′
4

2
nd
θ(s⋆)
OMP −
(cid:13)
θ(s⋆)
(cid:13)
(cid:13) b
OMP −
(cid:13)
(cid:13)
(cid:13) b

(cid:13)
(cid:13)
(cid:13) b

2
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
Choosing d′3 = 2d′′2 completes the proof.
(cid:13)
(cid:13)

≤

θ

θS + θS −
θS
+

2
(cid:13)
θ
θS −
(cid:13)
(cid:13)

k

k2.

(cid:4)

D. Proof of Proposition 1

Consider the event deﬁned by

Eq. (27) in the proof of Lemma 4 implies that:

:=

A

max
i,j |

(cid:26)

Rij|≤

τ

.

(cid:27)

Rij −
b

P(

c)

c1p2(n + p) exp

≤

A

c3σ4
τ 3/2n3/2 + σw !
By choosing τ as in the proof of Theorem 1, we have

 −

.

c2√τ n
w (n+p)

θminimax)

2
est(
R

b

2
est(
≤ R

θℓ1) = sup

E

P(

)d2
b
3

A

s log p
n

d2
3

s log p

n + 8(1

−

≤

≤

H (cid:16)

k
h
+ sup

H
η)2c1 exp

2
2

θ

θℓ1 −
EAc
b

k
i(cid:17)
θ
θℓ1 −
k
h
c2√τ n
b
c3σ4
w (n+p)
τ 3/2n3/2 + σw

 −

2
2

k

i
+ 3 log p

,

!

where the second inequality follows from Theorem 1, and
the third inequality follows from the fact that
4(1
−
s max
{
dominant, and thus we get
large enough n.

2
2≤
η)2 by the sufﬁcient stability assumption. For n >
d0(log p)2, d1(p log p)1/2
, the ﬁrst term will be the
}
θminimax)
n , for
est(

q
θminimax), we take the
approach of [21] by constructing a family of AR processes

As for a lower bound on

θℓ1 −
b

b
est(
R

2d3

s log p

R

≤

θ

k

k

b

Lemma 11 (Fano’s Inequality). Let
with a subclass
, 2M
0,
i
· · ·
KL(fθ1k
D

be a class of densities
Z
⋆ of densities fθi, parameterized by θi, for
Z
. Suppose that for any two distinct θ1, θ2 ∈
}
θ be an
β for some constant β. Let
fθ2)
Z
estimate of the parameters. Then

∈ {
⋆,

≤

sup
j

θ
P(

= θj|

Hj)

1

−

≥

β + log 2
M

,

b

(40)

where Hj denotes the hypothesis that θj is the true parameter,
and induces the probability measure P(.

Hj).

b

Consider a class
Z
ters over any subset S
parameters given by

|
of AR processes with s-sparse parame-
= s, with

satisfying

1, 2,

, p

S

⊂ {

· · ·

}

|

|

θℓ =

±

m

e−

1S(ℓ),

(41)

Z

where m remains to be chosen. We also add the all zero
vector θ to
. For a ﬁxed S, we have 2s + 1 such parameters
ZS. Consider the maximal collection of
forming a subfamily
p
subsets S for which any two subsets differ in at least
s
s/4 indices. The size of this collection can be identiﬁed by
(cid:0)
(cid:1)
A(p, s
4 , s) in coding theory, where A(n, d, w) represents the
maximum size of a binary code of length n with minimum
distance d and constant weight w [66]. We have

A(p, s

4 , s)

p

1

7
8 s

−
s!

,

≥

for large enough p (See Theorem 6 in [67]). Also, by the
Gilbert-Varshamov bound [66], there exists a subfamily
ZS, of cardinality
2⌊
θ1, θ2 ∈ Z
θ1, θ2 ∈ Z

⋆
S ⊂
⌋ + 1, such that any two distinct
⋆
S differ at least in s/16 components. Thus for
⋆ :=

⋆
S|≥
⋆
S, we have

|Z

s/8

Z

[S Z
θ1 −

k

θ2k2≥

1
4

√se−

m =: α,

(42)

7
8

p

s−1

⋆

s/8

and
s! 2⌊
|Z
|≥
the testing problem between the p
θ = θj ∈ Z
Using Markov’s inequality we have

7
8

⌋. For an arbitrary estimate
s−1

θ, consider

⌋ hypotheses Hj :
s! 2⌊
b
⋆, using the minimum distance decoding strategy.

s/8

sup

E

Z

k

h

θ

θ

k2

−

b

i

≥

≥

=

sup

E

⋆

k
h
P
sup

⋆

Z
sup
j

P

Z
α
2
α
2

θ

θ

−
θ

k2
i
θ
k2≥

−
= θj|
b

Hj

α
2

.

(cid:17)

(43)

b
k
(cid:16)
θ

(cid:16)

b

Let fθj denote joint probability distribution of
ditioned on

n
k=1 con-
p+1 under the hypothesis Hj. Using the

0
k=

{

xk}

{

−

(cid:17)
xk}

6
6
Gaussian assumption on the innovations, for i

= j, we have

θ′j

xk
k

−

xk −
(cid:16)

KL(fθik
D

fθj )

≤

E

sup
=j
i

≤

1
2σ2
w

sup
=j
i
n

E

log

(cid:20)
xk −

fθi
fθj |

Hi

θ′i

xk
k

1
p

−
−

(cid:21)
2

"−
n
2σ2
w

2

1
p

E

(cid:17)
Hi

θj)′xk
k

(cid:20)(cid:16)
(θi −

Xk=1 (cid:18)(cid:16)
(θi −
θj)′R(θi −
θjk
Using Lemma 11, (42), (43) and (44) yield:

sup
=j
i
n
sup
2σ2
=j
i
w
nλmax
2σ2
w

nse−
64πη2 =: β.

−
−
(cid:17)
θj)

θi −

sup
=j k
i

2
2≤

2m

(cid:12)
(cid:12)
(cid:12)

(cid:21)

≤

=

≤

1
p

−
−

2

Hi

(cid:17)

(cid:19) (cid:12)
(cid:12)
(cid:12)

#

(44)

13

Fn(t)

realization of a known distribution F0 which is most likely
absolutely continuous . Let us denote the empirical distribution
Fn. If the samples are generated from F0
of the n-samples by
the Glivenko-Cantelli theorem suggests that:
b
sup
|
t
Fn is uniformly
That is, for large n the empirical distribution
close to F0. The Kolmogorov-Smirnov (KS) test, Cram´er-
von Mises (CvM) criterion and the Anderson-Darling (AD)
test are three measures of discrepancy between
Fn and F0
which are easy to compute and are sufﬁciently discriminant
against alternative distributions. More speciﬁcally, the limiting
distribution of the following three random variables are known:
The KS test statistic

F0(t)
|

a.s.
−→

0.

−

b

b

b

sup

E

Z

k
h

θ

θ

k2

−

≥

i

√se−

m

2

1
8 

−

(cid:16)

−2m

nse
64πη2 + log 2
s log p

.

(cid:17)



Kn := sup

t

Fn(t)

|

F0(t)
|

,

−

the CvM statistic

b

b

9
8


for p large enough so that log p

log s
3
8 −
gives us the claim of Proposition 1 with L =
log(256).
n
log p guarantees that for all
(cid:4)

n
8πη2 log p
for large enough s and p such that s log p
(cid:16)
1
η
−
√8πη
1

1
2 log
d3
η√2π
The hypothesis of s
θ
θ

. Choosing m =

⋆, we have



−
1
s

≥

≥

(cid:17)

≤
k1≤

k

η.
q

−

∈ Z

E. Generalization to stable AR processes

1

−

We consider relaxing the sufﬁcient stability assumption of
η < 1 to θ being in the set of stable AR processes.
θ
k1≤
k
Given that the set of all stable AR processes is not necessarily
convex, the LASSO and OMP estimates cannot be obtained
by convex optimization techniques. Nevertheless, the results
of Theorems 1 and 2 can be generalized to the case of stable
AR models:
Corollary 2. The claims of Theorems 1 and 2 hold when Θ is
replaced by the set of stable AR processes, except for possibly
slightly different constants.

Proof: Note that

the stability of the process guaran-
tees boundedness of the power spectral density. The result
on the
follows by simply replacing the bounds
singular values of the covariance matrix R in Corollary 1 by
[inf ω S(ω), supω S(ω)].

8π , σ2

w
2πη2

σ2
w

i

h

APPENDIX B
STATISTICAL TESTS FOR GOODNESS-OF-FIT

In this appendix, we will give an overview of the statistical
goodness-of-ﬁt tests for assessing the accuracy of the AR
model estimates. A detailed treatment can be found in [68].

A. Residue-based tests

Let

θ be an estimate of the parameters of the process. The
θ are

residues (estimated innovations) of the process based on
given by
b

ek = xk −

θxk
k

1
p,

−
−

i = 1, 2,

, n.

· · ·

b

The main idea behind most of the available statistical tests
n
i=1 is to an i.i.d.
is to quantify how close the sequence

b

ei}

{

Cn :=

Fn(t)
(

F0(t))2dF0(t),

−

and the AD statistic

Z

An :=

b
(
Fn(t)
−
F0(t) (1

F0(t))2
F0(t))

−

Z

b

dF0(t).

For large values of n, the Glivenko-Cantelli theorem also sug-
gests that these statistics should be small. A simple calculation
leads to the following equivalent for the statistics:

Kn = max
1
n
i
≤

≤

max

nCn =

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)
+

1
12n

i
n −

n

,

F0(ei)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
F0(ei)

i=1 (cid:18)
X

i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

1
−
n −

1

2i

−
2n

,

(cid:27)

F0(ei)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

,

(cid:19)

and

nAn =

n

−

−

1
n

n

i=1
X

(2i

−

(cid:16)

1)

log F0(ei)+log

1

F0(ei)

.

−

(cid:16)

(cid:17)(cid:17)

B. Spectral domain tests for Gaussian AR processes

The aforementioned KS, CvM and AD tests all depend on
the distribution of the innovations. For Gaussian AR processes,
the spectral versions of these tests are introduced in [55]. These
tests are based on the similarities of the periodogram of the
data and the estimated power-spectral density of the process.
The key idea is summarized in the following lemma:

√n

2
(cid:18)

0
Z

(cid:16)

b

Lemma 12. Let S(ω) be the (normalized) power-spectral den-
sity of stationary process with bounded spectral spread, and
Sn(ω) be the periodogram of the n samples of a realization
of such a process, then for all ω we have:
b

ω

Sn(λ)

−

S(λ)

dλ

d.
−→ Z

(ω),

(45)

where

Z

(ω) is a zero-mean Gaussian process.

The explicit formula for the covariance function of

(.) is
Z
calculated in [55]. Lemma 12 suggests that for a good estimate
θ), one should
θ which admits a power spectral density S(ω;
θ)
get a (close to) Gaussian process replacing S(ω) with S(ω;
in (45). The spectral form of the CvM, KS and AD statistics
b
can thus be characterized given an estimate
b

θ.

b

b

(cid:17)

(cid:19)

6
6
6
6
6
6
ACKNOWLEDGMENT

This material is based upon work supported in part by the

National Science Foundation under Grant No. 1552946.

REFERENCES

[1] A. Kazemipour, B. Babadi, and M. Wu, “Sufﬁcient conditions for stable
recovery of sparse autoregressive models,” in 50th Annual Conference
on Information Sciences and Systems (CISS), March 16–18, Princeton,
NJ, 2016.

[2] H. Sang and Y. Sun, “Simultaneous sparse model selection and coef-
ﬁcient estimation for heavy-tailed autoregressive processes,” Statistics,
vol. 49, no. 1, pp. 187–208, 2015.

[3] K. Farokhi Sadabadi, “Vehicular trafﬁc modelling, data assimilation,
estimation and short term travel time prediction,” Ph.D. dissertation,
University of Maryland, College Park, 2014.
[4] S. A. Ahmed and A. R. Cook, Application of

time-series analysis

techniques to freeway incident detection, 1982, no. 841.

[5] M. S. Ahmed and A. R. Cook, Analysis of freeway trafﬁc time-series

data by using Box-Jenkins techniques, 1979, no. 722.

[6] J. Barcel´o, L. Montero, L. Marqu´es, and C. Carmona, “Travel time fore-
casting and dynamic origin-destination estimation for freeways based on
bluetooth trafﬁc monitoring,” Transportation Research Record: Journal
of the Transportation Research Board, no. 2175, pp. 19–27, 2010.
[7] S. Clark, “Trafﬁc prediction using multivariate nonparametric regres-
sion,” Journal of transportation engineering, vol. 129, no. 2, pp. 161–
168, 2003.

[8] P. M. Robinson, Time series with long memory. Oxford University

Press, 2003.

[9] H. Akaike, “Fitting autoregressive models for prediction,” Annals of the
institute of Statistical Mathematics, vol. 21, no. 1, pp. 243–247, 1969.
[10] D. S. Poskitt, “Autoregressive approximation in nonstandard situations:
the fractionally integrated and non-invertible cases,” Annals of
the
Institute of Statistical Mathematics, vol. 59, no. 4, pp. 697–725, 2007.
[11] R. Shibata, “Asymptotically efﬁcient selection of the order of the model
for estimating parameters of a linear process,” The Annals of Statistics,
pp. 147–164, 1980.

[12] J. W. Galbraith and V. Zinde-Walsh, “On some simple, autoregression-
based estimation and identiﬁcation techniques for arma models,”
Biometrika, vol. 84, no. 3, pp. 685–696, 1997.

[13] J. Galbraith and V. Zinde-Walsh, “Autoregression-based estimators for

arﬁma models,” CIRANO, Tech. Rep., 2001.

[14] C.-K. Ing and C.-Z. Wei, “Order selection for same-realization predic-
tions in autoregressive processes,” The Annals of Statistics, vol. 33, no. 5,
pp. 2423–2474, 2005.

[15] K. E. Baddour and N. C. Beaulieu, “Autoregressive modeling for fading
channel simulation,” IEEE Transactions on Wireless Communications,
vol. 4, no. 4, pp. 1650–1662, 2005.

[16] M. E. Mann and J. Park, “Oscillatory spatiotemporal signal detection in
climate studies: A multiple-taper spectral domain approach,” Advances
in geophysics, vol. 41, pp. 1–132, 1999.

[17] H. Akaike, “Maximum likelihood identiﬁcation of gaussian autoregres-
sive moving average models,” Biometrika, vol. 60, no. 2, pp. 255–265,
1973.

[18] ——, “Statistical predictor identiﬁcation,” Annals of the Institute of

Statistical Mathematics, vol. 22, no. 1, pp. 203–217, 1970.

[19] G. Schwarz, “Estimating the dimension of a model,” The annals of

statistics, vol. 6, no. 2, pp. 461–464, 1978.

[20] H. Wang, G. Li, and C.-L. Tsai, “Regression coefﬁcient and autoregres-
sive order shrinkage and selection via the lasso,” Journal of the Royal
Statistical Society: Series B (Statistical Methodology), vol. 69, no. 1,
pp. 63–78, 2007.

[21] A. Goldenshluger and A. Zeevi, “Nonasymptotic bounds for autoregres-
sive time series modeling,” Annals of statistics, pp. 417–444, 2001.
[22] Y. Nardi and A. Rinaldo, “Autoregressive process modeling via the lasso
procedure,” Journal of Multivariate Analysis, vol. 102, no. 3, pp. 528–
549, 2011.

[23] D. L. Donoho, “Compressed sensing,” IEEE Transactions on Informa-

tion Theory, vol. 52, no. 4, pp. 1289–1306, 2006.

[24] E. J. Cand`es, “Compressive sampling,” in Proceedings of the Interna-
tional Congress of Mathematicians Madrid, August 22–30, 2006, pp.
1433–1452.

[25] E. J. Cand`es and M. B. Wakin, “An introduction to compressive
sampling,” IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21–30,
2008.

14

[26] M. Rudelson and R. Vershynin, “On sparse reconstruction from fourier
and gaussian measurements,” Communications on Pure and Applied
Mathematics, vol. 61, no. 8, pp. 1025–1045, 2008.

[27] R. Baraniuk, M. Davenport, R. DeVore, and M. Wakin, “A simple proof
of the restricted isometry property for random matrices,” Constructive
Approximation, vol. 28, no. 3, pp. 253–263, 2008.

[28] P. Zhao and B. Yu, “On model selection consistency of lasso,” Journal
of Machine Learning Research, vol. 7, no. Nov, pp. 2541–2563, 2006.
[29] G. Raskutti, M. J. Wainwright, and B. Yu, “Restricted eigenvalue
properties for correlated gaussian designs,” The Journal of Machine
Learning Research, vol. 11, pp. 2241–2259, 2010.

[30] J. Haupt, W. U. Bajwa, G. Raz, and R. Nowak, “Toeplitz compressed
sensing matrices with applications to sparse channel estimation,” IEEE
Trans. on Information Theory, vol. 56, no. 11, pp. 5862–5875, 2010.

[31] H. Rauhut, J. Romberg, and J. A. Tropp, “Restricted isometries for par-
tial random circulant matrices,” Applied and Computational Harmonic
Analysis, vol. 32, no. 2, pp. 242–254, 2012.

[32] P.-L. Loh and M. J. Wainwright, “High-dimensional regression with
noisy and missing data: Provable guarantees with non-convexity,” The
Annals of Statistics, vol. 40, no. 3, pp. 1637–1664, 2012.

[33] F. Han and H. Liu, “Transition matrix estimation in high dimensional

time series.” in ICML (2), 2013, pp. 172–180.

[34] S. Negahban and M. J. Wainwright, “Estimation of (near) low-rank
matrices with noise and high-dimensional scaling,” The Annals of
Statistics, pp. 1069–1097, 2011.

[35] K. C. Wong, A. Tewari, and Z. Li, “Regularized estimation in high
time series under mixing conditions,” arXiv preprint

dimensional
arXiv:1602.04265, 2016.

[36] S. Basu and G. Michailidis, “Regularized estimation in sparse high-
dimensional time series models,” The Annals of Statistics, vol. 43, no. 4,
pp. 1535–1567, 2015.

[37] W.-B. Wu and Y. N. Wu, “Performance bounds for parameter estimates
of high-dimensional linear models with correlated errors,” Electronic
Journal of Statistics, vol. 10, no. 1, pp. 352–379, 2016.

[38] P. J. Bickel, Y. Ritov, and A. B. Tsybakov, “Simultaneous analysis of
lasso and dantzig selector,” The Annals of Statistics, pp. 1705–1732,
2009.

[39] P. Stoica and R. L. Moses, Introduction to spectral analysis. Prentice

hall Upper Saddle River, 1997, vol. 1.

[40] S. S. Haykin, Adaptive ﬁlter theory. Pearson Education India, 2008.
[41] J. P. Burg, “Maximum entropy spectral analysis.” in 37th Annual
International Meeting. Society of Exploration Geophysics, 1967.
[42] S. L. Marple Jr, “Digital spectral analysis with applications,” Englewood

Cliffs, NJ, Prentice-Hall, Inc., 1987, 512 p., vol. 1, 1987.

[43] D. Needell and J. A. Tropp, “CoSaMP: Iterative signal recovery from
incomplete and inaccurate samples,” Applied and Computational Har-
monic Analysis, vol. 26, no. 3, pp. 301–321, 2009.

[44] D. B. Percival and A. T. Walden, Spectral analysis for physical appli-

cations. Cambridge University Press, 1993.

[45] R. Tibshirani, “Regression shrinkage and selection via the lasso,”
Journal of the Royal Statistical Society. Series B (Methodological), pp.
267–288, 1996.

[46] M. J. Wainwright, “Sharp thresholds for high-dimensional and noisy
sparsity recovery using-constrained quadratic programming (lasso),”
IEEE transactions on information theory, vol. 55, no. 5, pp. 2183–2202,
2009.

[47] K. Knight and W. Fu, “Asymptotics for lasso-type estimators,” Annals

of statistics, pp. 1356–1378, 2000.

[48] N. Meinshausen and P. B¨uhlmann, “High-dimensional graphs and vari-
able selection with the lasso,” The annals of statistics, pp. 1436–1462,
2006.

[49] Y. C. Pati, R. Rezaiifar, and P. Krishnaprasad, “Orthogonal matching
pursuit: Recursive function approximation with applications to wavelet
decomposition,” in Conference Record of The Twenty-Seventh Asilomar
IEEE, 1993, pp. 40–
Conference on Signals, Systems and Computers.
44.

[50] T. Zhang, “Sparse recovery with orthogonal matching pursuit under
RIP,” IEEE Transactions on Information Theory, vol. 57, no. 9, pp.
6215–6221, 2011.

[51] A. M. Bruckstein, D. L. Donoho, and M. Elad, “From sparse solutions of
systems of equations to sparse modeling of signals and images,” SIAM
review, vol. 51, no. 1, pp. 34–81, 2009.

[52] E. J. Candes, “Modern statistical estimation via oracle inequalities,” Acta

numerica, vol. 15, pp. 257–325, 2006.
[53] R. B. D’Agostino, Goodness-of-ﬁt-techniques.
[54] S. Johansen, “Likelihood-based inference in cointegrated vector autore-

gressive models,” OUP Catalogue, 1995.

[55] T. W. Anderson, “Goodness-of-ﬁt tests for autoregressive processes,”
Journal of time series analysis, vol. 18, no. 4, pp. 321–339, 1997.

15

[56] “Cushing,
last
http://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=RWTC&f=D

price
wti
14-December-2015).

(Date
Available:

ok
accessed

[Online].

dataset,”

spot

fob

[57] “Regional integrated transportation information system (ritis),” (Date

last accessed 27-December-2015). [Online]. Available: https://ritis.org

[58] “Regional
(Date
http://i95coalition.org/projects/regional- integrated- transportation- information- system- ritis

system (ritis),”
[Online]. Available:

integrated
accessed

27-December-2015).

transportation

information

last

[59] A. Kazemipour, B. Babadi, and M. Wu, “Sparse estimation of self-
exciting point processes with application to LGN neural modeling,” in
2014 IEEE Global Conference on Signal and Information Processing
(GlobalSIP).

IEEE, 2014, pp. 478–482.

[60] A. Kazemipour, M. Wu, and B. Babadi, “Robust estimation of self-
exciting point process models with application to neuronal modeling,”
arXiv preprint arXiv:1507.03955, 2015.

[61] U. Grenander and G. Szeg¨o, Toeplitz forms and their applications. Univ

of California Press, 1958, vol. 321.

[62] R. Rudzkis, “Large deviations for estimates of spectrum of stationary
series,” Lithuanian Mathematical Journal, vol. 18, no. 2, pp. 214–226,
1978.

[63] S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu, “A
uniﬁed framework for high-dimensional analysis of M-estimators with
decomposable regularizers,” Statistical Science, vol. 27, no. 4, pp. 538–
557, 2012.

[64] S. A. van de Geer, “On Hoeffding’s inequality for dependent ran-
dom variables,” in Empirical Process Techniques for Dependent Data,
H. Dehling and W. Philipp, Eds. Springer, 2001.

[65] ——, Empirical Processes in M-estimation.

Cambridge university

press, 2000.

[66] F. J. MacWilliams and N. J. A. Sloane, The theory of error correcting

codes. Elsevier, 1977, vol. 16.

[67] R. L. Graham and N. Sloane, “Lower bounds for constant weight codes,”
Information Theory, IEEE Transactions on, vol. 26, no. 1, pp. 37–43,
1980.

[68] E. L. Lehmann, J. P. Romano, and G. Casella, Testing statistical

hypotheses. Wiley New York et al, 1986, vol. 150.

