9
1
0
2

r
p
A
9
2

]

C
H
.
s
c
[

3
v
0
0
0
8
0
.
2
1
8
1
:
v
i
X
r
a

Privacy-Aware Eye Tracking Using Differential Privacy

Julian Steil
Max Planck Institute for Informatics
Saarland Informatics Campus
jsteil@mpi-inf.mpg.de

Michael Xuelin Huang
Max Planck Institute for Informatics
Saarland Informatics Campus
mhuang@mpi-inf.mpg.de

Inken Hagestedt
CISPA Helmholtz Center for Information Security
Saarland Informatics Campus
inken.hagestedt@uni-saarland.de

Andreas Bulling
University of Stuttgart
Institute for Visualisation and Interactive Systems
andreas.bulling@vis.uni-stuttgart.de

ABSTRACT
With eye tracking being increasingly integrated into virtual and
augmented reality (VR/AR) head-mounted displays, preserving
users’ privacy is an ever more important, yet under-explored, topic
in the eye tracking community. We report a large-scale online
survey (N=124) on privacy aspects of eye tracking that provides
the first comprehensive account of with whom, for which services,
and to what extent users are willing to share their gaze data. Using
these insights, we design a privacy-aware VR interface that uses
differential privacy, which we evaluate on a new 20-participant
dataset for two privacy sensitive tasks: We show that our method
can prevent user re-identification and protect gender information
while maintaining high performance for gaze-based document type
classification. Our results highlight the privacy challenges particular
to gaze data and demonstrate that differential privacy is a potential
means to address them. Thus, this paper lays important foundations
for future research on privacy-aware gaze interfaces.

Human and societal aspects of se-
Human

CCS CONCEPTS
• Security and privacy
→
curity and privacy; • Human-centered computing
computer interaction (HCI);
KEYWORDS
Online Survey; Data Sharing; Privacy Protection; Gaze Behaviour;
Eye Movements; User Modeling

→

ACM Reference Format:
Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling.
2019. Privacy-Aware Eye Tracking Using Differential Privacy. In 2019 Sym-
posium on Eye Tracking Research and Applications (ETRA ’19), June 25–
28, 2019, Denver, CO, USA. ACM, New York, NY, USA, 26 pages. https:
//doi.org/10.1145/3314111.3319915

1 INTRODUCTION
With eye tracking becoming pervasive [Bulling and Gellersen 2010;
Tonsen et al. 2017], preserving users’ privacy has emerged as an

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ETRA ’19, June 25–28, 2019, Denver, CO, USA
© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6709-7/19/06. . . $15.00
https://doi.org/10.1145/3314111.3319915

Figure 1: Using differential privacy prevents third parties,
like companies or hackers, from deriving private attributes
from a user’s eye movement behaviour while maintaining
the data utility for non-private information.

important topic in the eye tracking, eye movement analysis, and
gaze interaction research communities. Privacy is particularly im-
portant in this context given the rich information content available
in human eye movements [Bulling et al. 2011a], on one hand, and
the rapidly increasing capabilities of interactive systems to sense,
analyse, and exploit this information in everyday life [Hansen et al.
2003; Stellmach and Dachselt 2012; Vertegaal et al. 2003] on the
other. The eyes are more privacy-sensitive than other input modal-
ities: They are typically not consciously controlled; they can reveal
unique private information, such as personal preferences, goals, or
intentions. Moreover, eye movements are difficult to remember, let
alone reconstruct in detail, in retrospect, and hence do not easily
allow users to “learn from their mistakes”, i.e. to reflect on their
past and change their future privacy-related behaviour.

These unique properties and rapid technological advances
call for new research on next-generation eye tracking systems
that are privacy-aware, i.e. that preserve users’ privacy in all
interactions they perform with other humans or computing systems
in everyday life. However, privacy-aware eye tracking remains
under-investigated as of yet [Liebling and Preibusch 2014].

The lack of research on privacy-aware eye tracking results in two
major limitations: First, there is a lack of even basic understanding

Eye Movement Behaviour  Differential Privacy  TimeAR/VR UserThird PartiesAR/VR CompaniesUnauthorised Third PartiesGenderSexual PreferenceHealthAgeDocument Type Classification    Activity RecognitionPersonality Traits 
 
 
 
 
 
ETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

of users’ privacy concerns with eye tracking in general and eye
movement analysis in particular. Second, there is a lack of eye track-
ing methods to preserve users’ privacy, corresponding systems, and
user interfaces that implement (and hence permit the evaluation
of) these methods with end users. Our work aims to address both
limitations and, as such, make the first crucial step towards a
new generation of eye tracking systems that respect and actively
protect private information that can be inferred from the eyes.

Our work first contributes a large-scale online survey on privacy as-
pects of eye tracking and eye movement analysis. The survey provides
the first comprehensive account of with whom, for which services,
and to what extent users are willing to share their eye movement
data. The survey data is available at https://www.mpi-inf.mpg.de/
MPIIDPEye/. Informed by the survey, we further contribute the first
method to protect users’ privacy in eye tracking based on differential
privacy (DP), a well-studied framework in the privacy research com-
munity. In a nutshell, DP adds noise to the data so as to minimise
chances to infer privacy-sensitive information or to (re-)identify a
user while, at the same time, still allow use of the data for desired
applications (the so-called utility task), such as activity recognition
or document type classification (see Figure 1). We illustrate the use
of differential privacy for a sample virtual reality (VR) gaze interface.
We opted for a VR interface given that eye tracking will be readily
integrated into upcoming VR head-mounted displays, and hence,
given the significant and imminent threat potential [Adams et al.
2018]: Eye movement data may soon be collected at scale on these
devices, recorded in the background without the user noticing, or
even transferred to hardware manufacturers.

2 RELATED WORK
We discuss previous works on 1) information available in eye move-
ments, 2) eye movements as a biometric, and 3) differential privacy.

2.1 Information Available in Eye Movements
A large body of work across different research fields has demon-
strated the rich information content available in human eye move-
ments. Pupil size is related to a person’s interest in a scene [Hess
and Polt 1960] and can be used to measure cognitive load [Matthews
et al. 1991]. Other works have shown that eye movements are closely
linked to mental disorders, such as Alzheimer’s [Hutton et al. 1984],
Parkinson’s [Kuechenmeister et al. 1977], or schizophrenia [Holz-
man et al. 1974]. More recent work in HCI has demonstrated the use
of eye movement analysis for human activity recognition [Bulling
et al. 2013; Steil and Bulling 2015] as well as to infer a user’s cogni-
tive state [Bulling and Zander 2014; Faber et al. 2017] or personality
traits [Hoppe et al. 2018]. More closely related to our work, several
researchers have shown that gender and age can be inferred from
eye movements, e.g. by analysing the spatial distribution of gaze
on images like faces [Cantoni et al. 2015; Sammaknejad et al. 2017].
All of these works underline the significant potential of eye
movement analysis for a range of future applications, some of
which may soon become a reality, for example, with the advent of
eye tracking-equipped virtual and augmented reality head-mounted
displays. Despite the benefits of these future applications, the wide
availability of eye tracking will also pose significant privacy risks
that remain under-explored in the eye tracking community.

2.2 Eye Movements as a Biometric
Eye movement biometrics has emerged as a promising approach to
user authentication [Kasprowski and Ober 2003]. While first works
required a point stimulus that users were instructed to follow with
their eyes [Kasprowski 2004; Kasprowski and Ober 2005], later ones
explored static points [Bednarik et al. 2005] or images [Maeder and
Fookes 2003]. Kinnunen et al. presented the first method for “task-
independent” person authentication using eye movements [Kin-
nunen et al. 2010]. Komogortsev et al. proposed the first attempt
to model eye movements for authentication using an Oculomotor
Plant Mathematical Model [Komogortsev and Holland 2013; Ko-
mogortsev et al. 2010]. Eberz et al. presented a biometric based on
eye movement patterns. They used 20 features that allowed them
to reliably distinguish and authenticate users across a variety of
real-world tasks, including reading, writing, web browsing, and
watching videos on a desktop screen [Eberz et al. 2016]. Zhang et al.
used eye movements to continuously authenticate the wearer of a
VR headset by showing different visual stimuli [Zhang et al. 2018].
While an ever-growing body of research explores eye move-
ments as a promising modality for privacy applications and user au-
thentication, we are the first to practically explore eye movements
recorded using eye tracking as a potential threat to users’ privacy.

2.3 Differential Privacy
Differential privacy has been studied in privacy research for more
than a decade in terms of its theoretical foundations and its practi-
cal applications to different data types, such as location [Pyrgelis
et al. 2017], biomedical data [Saleheen et al. 2016], or continuous
time series data [Fan and Xiong 2012]. We refer the reader to [Zhu
et al. 2017] for a survey. A key challenge in differential privacy is to
find the right trade-off between privacy and utility, that is, the right
amount of random noise to “hide” an individual without hampering
data utility. Fredrikson et al. demonstrated how important it is to
balance privacy and utility [Fredrikson et al. 2014]. They observed
that either privacy was not preserved or that utility suffered, lead-
ing to increased health risks for the patients from unsuitable drug
dosage. A good privacy-utility trade-off is possible if privacy mech-
anisms are tailored towards a specific use case [Fan and Xiong 2012;
Pyrgelis et al. 2017]. While differential privacy has a long history
in privacy research, to the best of our knowledge, we are the first
to apply this framework to eye tracking data.

3 PRIVACY CONCERNS IN EYE TRACKING
We conducted a large-scale online survey to shed light on users’
privacy concerns related to eye tracking technology and the infor-
mation that can be inferred from eye movement data. We adver-
tised our survey on social platforms (Facebook, WeChat) and local
mailing lists for study announcements. The survey opened with
general questions about eye tracking and VR technologies; contin-
ued with questions about future use and applications, data sharing
and privacy (especially regarding with whom users are willing to
share their data); and concluded with questions about the partici-
pants’ willingness to share different eye movement representations.
Participants answered each question on a 7-point Likert scale (1:
Strongly disagree to 7: Strongly agree). To simplify the analysis, we
merged scores 1 to 3 to “Disagree” and 5 to 7 to “Agree”.

Privacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

Figure 2: Survey results (Services and Attributes): With which services would you agree to share your eye tracking data (Ser-
vices)?; Would you agree to private attributes being inferred by these services (Private Attributes)?

Figure 3: Survey results (Whom and Where): Would you agree to share your eye tracking data in general (Sharing); with whom
(Owner); where (Environment); in exchange for benefits or for VR/AR usage (Application)?

The survey took about 20 minutes to complete, was set up as
a Google Form, and was split into the parts described above. Our
design ensured that participants without pre-knowledge of eye
tracking and VR technology could participate as well: We provided
a slide show containing information about eye tracking in general,
and in VR devices specifically, and introduced the different forms of
data representation, showing example images or explanatory texts.
In our survey, 124 people (81 male, 39 female, 4 did not tick the
gender box) participated, aged 21 to 66 (mean = 28.07, std = 5.89).
The participants were from all over the world, coming from 29 dif-
ferent countries (Germany: 39%, India: 12%, Pakistan: 6%, Italy: 6%,
China: 5%, USA: 3%). Sixty-seven percent of them had a graduate
university degree (master’s or PhD), and 22% had an undergradu-
ate university degree (bachelor’s). Fifty-one percent were students
of a variety of subjects (law, language science, computer science,
psychology, etc.); 34% were scientists and researchers, IT profes-
sionals (7%), or had business administration jobs (2%). Since the
topic of the survey was in the title of posts and emails, most likely
people inherently interested in the topic participated. The majority
were young, educated people with a technical background the exact
group of people most likely to experience AR/VR technology (73%)
in contrast to, for example, older generations.

Given the breadth of results, we highlight key insights most
relevant for the current paper. We found nearly all answers for the
provided questions to be significantly different from an equal dis-
tribution tested with Pearson’s chi-squared test (p < 0.001, dof = 6).
Additionally, we calculated the skewness and observed that the
majority of questions show a significant difference to the corre-
sponding normal distribution (p < 0.1). Detailed numbers, plots,
significance and skewness test results can be found in the supple-
mentary material (see https://www.mpi-inf.mpg.de/MPIIDPEye/).

Services and Attributes: In the first part of our survey, we asked
participants for which services they would share their eye tracking
data and presented both currently available and potential future
services as answer options. As we can see from Figure 2, more
than 80% of all participants agreed to share their eye tracking data
for (early) detection of diseases like Alzheimer’s or Parkinson’s.
Likewise, the majority agreed to share their data for hands-free

VR and user interface interaction. Similar results can be observed
for learning and reading skill detection as well as for stress level
monitoring. However, for improved gaze target recognition, website
content, and activity recognition, we observe two peaks. A clear
majority is unwilling to share data with shopping assistance and
interest detection services.

Our next set of questions indicated the fact that services could
be able to infer private attributes from their data, and we asked
whether participants would still want to share their eye tracking
data. We clearly observed that if the attributes of sexual preference,
gender, race, and identity can be inferred, a majority do not want
to share their data. It was only for age and emotion detection that
we identified two different interest groups that either agree with or
object to sharing their data.

×

10−

Whom and Where: In the second part, of our survey we asked
participants whether they would share eye tracking data in general,
and with whom. Moreover, we were interested in whether the envi-
ronment has an influence on their sharing behaviour (see Figure 3).
Finally, we wanted to know whether the sharing behaviour is dif-
ferent if participants get benefits (not specified) in exchange for
their data or if the data is collected during VR/AR usage in general.
The answers as to whether participants would share their eye
tracking data in general do not show a clear tendency; the partic-
ipants’ opinions are split in two groups (χ 2(dof = 6) = 32.25, p =
6). Next, we asked more specifically whether participants
1.46
would share their data if it were later owned and operated by one of
the given “owner” options in Figure 3. According to their answers,
participants would only share their data if the co-owner is a gov-
ernmental health-agency; they do not trust local and international
companies, or company internal use. However, participants would
also share their data for research purposes, which is not surprising
given that 67% of participants have a graduate university degree
and trust in research institutes. Participants would not agree to
share their data in public, nor in private environments, but they
would agree to constrained environments. Furthermore, the par-
ticipants object to sharing their data for any kind of benefit, but
would agree when their eye tracking data was collected in VR/AR
(χ 2(dof = 6) = 26.72, p = 0.00016).

DiseasesDetectionNatural VRInteractionVisual SearchTarget DetectionUser InterfaceInteractionUnderstandableWebsite ContentReading SkillImprovementLearning SkillImprovementStress LevelMonitoringInterestIdentificationActivityRecognitionShoppingAssistanceRecognized Private Company(in foreign country)SexualPreferenceGenderAgeMood andEmotionsRaceIdentity1-3 - Disagree:4 - Neither agree nor disagree:5-7 - Agree:13.715.6580.6524.194.8470.9741.948.8749.1926.615.6567.7450.8111.2937.9020.169.6870.1616.138.0675.8119.3511.2969.3573.398.0618.5550.8112.1037.1079.034.8416.1374.196.4519.3551.617.2641.1341.1312.1046.7744.3512.1043.5565.328.8725.8178.234.0317.74                                             Services                                                                                                         Private Attributes             Eye TrackingDataPrivate Company(in foreign country)GovernmentalAgency(non-health)Governmental Health AuthorityLocal CompanyInternationalCompanyPrivate Company(user's country)Private Company(foreign country)User Himself(home cloud)Company Internal Use (intranet)ResearchInstitutePublicPrivateConstrainedIn Exchange for BenefitsVR/AR1-3 - Disagree:4 - Neither agree nor disagree:5-7 - Agree:41.1312.9045.9762.905.6531.4537.108.0654.8461.2916.1322.5863.7112.9023.3960.4817.7421.7773.3917.748.8714.525.6579.8456.4516.1327.428.0611.2980.6563.719.6826.6158.0616.9425.0032.2613.7154.0363.7111.2925.0032.2616.1351.61Sharing                                                              Owner                                                                     Environment                         Application   ETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

Figure 4: (Left) Our method assumes that AR/VR users share their eye tracking data and privacy-sensitive information with a
third party, which is able to train classifiers with or without differentially private data to infer private attributes of an unknown
(without prior knowledge) or a known (with prior knowledge) person; (Right) Applying differential privacy to test data pre-
vents private information inference (gender, user (re-)identification) but maintains data utility (document type classification).

Data Representation: In the final part of the survey, we asked
participants in what form they would agree to share their data.
We discriminate 12 different representations, ranging from raw
eye tracking, to heatmaps, to aggregated features (see Figure 3
in the supplementary material). Additionally, we were interested
in whether their sharing behaviour changes if the data is first
anonymised. Information which provides gaze information, like
fixations, or scan path information on a surface would mostly not
be shared. Participants largely agree to share their eye tracking data
as statistical features, and especially aggregated features. This is
why we focus in our study on the aggregated feature representation
to apply differential privacy. Our survey shows a clear increase in
participants willing to share their data in anonymised form.

4 PRIVACY-PRESERVING EYE TRACKING
The findings from our survey underline the urgent need to develop
privacy-aware eye tracking systems – systems that provide a formal
guarantee to protect the privacy of their users. Additionally, it is
important not to forget that eye movement data typically also serves
a desired task – a so-called utility. For example, eye movement data
may be used in a reading assistant to detect the documents a user is
reading [Kunze et al. 2013b] or to automatically estimate how many
words a user reads per day [Kunze et al. 2013a, 2015]. Therefore, it
is important to ensure that any privacy-preserving method does
not render the utility dysfunctional, i.e. that the performance on
the utility task will not drop too far. The key challenge can thus be
described as ensuring privacy without impeding utility.

We assume in the following that multiple users share their eye
tracking data in the form of aggregated features. The resulting eye
tracking database is visualised in the left part of Figure 4. This
database can be downloaded both for legitimate use cases as well
as for infringing on users’ privacy, for example, to train classifiers
for various tasks. Therefore, our proposed privacy mechanism is
applied prior to the release by a trusted curator.

4.1 Threat Models
We have identified two attack vectors on users’ privacy in the
context of eye tracking that we formalise in two threat models.
They differ in their assumption about the attackers’ prior knowledge
about their target (see the right part of Figure 4).

Without Prior Knowledge. In the first threat model, we assume
that an attacker has no prior knowledge about the target and wants

to infer a private attribute; we focus on gender in our example study.
The attacker can only rely on a training data set from multiple par-
ticipants different from the target. This data can be gathered by
companies or game developers we share our data with in exchange
for a specific service. Some users might opt in to share their data
with a third party to receive personalised advertisements, or they
might create a user account to remove advertisements. These com-
panies with eye tracking data can misuse the data, forward it to
third parties or get hacked by external attackers. Another source for
attackers to get eye tracking datasets is publicly available datasets
generated for research purposes. Concretely, VR glasses are offered
in gaming centres and used by multiple visitors, which we refer to
as the one-device-multiple-users scenario. An attacker with access
to the eye tracking data might be interested in inferring the gender
of the current user to show gender-specific advertisements.

With Prior Knowledge. The second threat model assumes that the
attacker has already gathered prior knowledge about the target. Ob-
serving further eye tracking data, the attacker wants to re-identify
the target to inspect the target’s habits. Concretely, the target might
be using different user accounts or even different devices for work
and leisure time (a one-user-multiple-devices scenario). We assume
the attacker is able to link the target’s work data to the target’s
identity and now wants to identify the target’s data from his/her
leisure activities. Again, the attacker could be a VR/AR company
exploiting their data to check whether a device is only used by one
person, or re-identify a user automatically to adapt device settings.
Moreover, data could be released intentionally to a third party for
money or unintentionally through a hack.

4.2 Differential Privacy for Eye Tracking
We propose to mitigate the privacy threats emerging from our two
threat models using differential privacy, a well-known framework
from privacy research [Dwork et al. 2014]. Differential privacy
guarantees that the answer of the privacy-preserving mechanism
does not depend on whether a single user contributed his/her data
or not; hence, there is no way to infer further information about this
user. Concretely, the answer to the question “What is the average
fixation rate when reading a text?” should be almost the same,
whether or not a specific user, say, Alice, has contributed her data
to our database of fixation rates. We denote a differentially private
and refer to Alice’s data as a single data element
mechanism by
in the database D. Typically,
adds random noise to “hide” each
data element, which we will formalise in the following.

M

M

AR/VR UserShared Eye Movement Data  UnknownPersonKnown PersonUser (Re-)Identiﬁcation  Document TypeClassiﬁcation      Eye TrackingDatabaseApplying Diﬀerential PrivacyClassiﬁerTraining  GenderPrediction With Diﬀerential PrivacyWithoutDiﬀerential PrivacyWithout Prior Knowledge WithPrior Knowledge  Privacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

D

Pr

M

Definition 1 (ϵ-Differential Privacy [Dwork et al. 2006]).
provides ϵ-differential privacy if for all databases
,
(M)
(1)

A mechanism
D, D ′ that differ in at most one element and for every S
we have

·
Differential privacy allows computing an arbitrary function д
d , where d denotes the dimension-
over the database, i.e. д :
ality of the output of д. For our running example, д would compute
the average and output one number, hence d = 1. Similarly, we
could define д to average over 30-second windows of fixation data
and then output a vector of length d.

R∗ (cid:55)→ R

D ′) ∈

Range

[M(

[M(

] ≤

) ∈

eϵ

Pr

⊆

S

S

]

.

How much noise we have to add depends on the variance of the

data between two arbitrary elements. Formally:

Definition 2 (L1 Sensitivity [Dwork et al. 2006]). For all
d , the L1 sensitivity is the smallest number ∆д

functions д :
s.th. for all databases D, D ′ differing in one element, we have
д

R∗ (cid:55)→ R

(2)

∆д .

д
||

D
(

) −

D ′)||L1 ≤
(

Intuitively, the sensitivity captures the maximal influence Alice’s
data could have on the answer to our query. In the worst case, for
her privacy, Alice’s data is an outlier, e.g. Alice is a very slow reader
compared to all other participants. Even in this case, the difference
between Alice’s data and any other entry in the database must be
smaller than or equal to the sensitivity. The noise to “hide” Alice’s
contribution is scaled to this worst case, ensuring Alice’s privacy.
Next, we formalise the exponential mechanism that is one way

to generate differentially private data:

Definition 3 (Exponential Mechanism [Dwork et al. 2014]).
The exponential mechanism selects and outputs an element r
in
the range of permissible output elements with probability equal to
(written: r
ϵ

∈ R

)

∼

r

exp

∼

(

·

x, r
u
(
2∆u

)

)

(3)

where u is a utility function judging the quality of r with respect to
the original data element x.

In order to apply the exponential mechanism to our example data-
base of fixation durations, we would first need to define a utility
function u and the set of permissible outputs. Valid answers to the
query “What are the average fixation rates when reading a text,
sampled at 30 second windows?” are vectors of length d containing
0. The utility function u is a
real-numbered entries; thus,
measure of quality for the output r with respect to the original data
entry x. The exponential mechanism ensures that high-quality out-
puts r are generated exponentially more often than low-quality r .
Finally, we state one theorem that allows combining several

= Rd
≥

R

differentially private mechanisms into one.

D
Mk (

Theorem 1 (Composition Theorem [Dwork et al. 2006]). Let
M1, ...,
Mk be a fixed sequence of mechanisms, where each mecha-
=
i is ϵi -differentially private. Then, their joint output
nism
M
is ϵ-differentially private for ϵ = (cid:205)k
, ...,
D
(M1(
)
4.3 Implementing Differential Privacy
Our dataset contains data from n participants, which we refer to
as p1, ..., pn . For each participant, we measure m features, f1, ..., fm
at different points in time. In summary, p1, f7,t5 denotes the value
of the 7th feature at time point 5 of participant 1, and the vector

M(
i=1 ϵi .

D

))

)

(4)

(5)

p1, f7,t0 , ..., p1,f7,tmax, 1 )
contains all measurements of feature 7 for
(
participant 1. Notice that the data entries available may have differ-
ent lengths, i.e. tmax,1, the last time point of participant 1, may be
different from another participant’s last time point, e.g. tmax,2.

The sensitivity for our mechanism then depends on the range of
the features, which is different across our m features. For example,
feature f15 is the fixation duration in our dataset, and it has an esti-
mated range of [0.11, 2.75] seconds, while f22, which describes the
pupil diameter size, has an estimated range of [21.9, 133.9] pixels.
Therefore, we derive one privacy mechanism
Mfi for each feature
separately and use the composition theorem (Theorem 1) to com-
bine the m mechanisms into our final mechanism. The exponential
mechanism requires a utility function u. We choose the L1 distance
for simplicity of the derivation:

u

pfi , r
(

)

=

pfi, j −
|

rj

|

tmax,p
(cid:213)

j=1

According to Definition 2, the sensitivity ∆u, fi is
∆u, fi

pfi,t0 , ..., pfi,tmax,p ) − (

= max

pfi ,qfi ||(

qfi,t0 , ..., qfi,tmax,q )||L1

i.e. the maximal difference between the data vectors of two arbitrary
participants p and q for the i-th feature. Next, we unify the length
by padding the data vector with the shorter length. Let tmax be the
maximal length: tmax = max
. Using this and the
)
definition of the L1 norm:

tmax,p , tmax,q
(

∆u, fi ≤

max
pfi ,qfi

tmax
(cid:213)

j=1

pfi,tj −
|

qfi,tj |

= tmax

δi

·

(6)

In the last step, we used the fact that we can derive the range δi of
feature fi , either estimated from the data or by theoretic constraints.
We rely on the exponential mechanism (see Definition 3) to
obtain a vector r that is differentially private for each participant p
and feature fi :

ϵi

(cid:205)tmax,p
j=1
2

pfi, j −
|
δi
tmax
·

rj

|

(7)

r

exp

ϵiu

pfi , r
(
2∆u, fi

)

Eq. 4= exp

)

(

∼

(
To increase readability, we define λi =
, which is con-
stant once i and ϵi are fixed. We generate such a vector r from the
exponential distribution by first sampling a random scalar y from
the exponential distribution with location 0 and scale parameter
1
. We derive our differentially private vector r from y as follows:
λi

·
ϵi
tmax
·

δi
·

)

2

y = exp

λi
(

·

tmax,p
(cid:213)

j=1

pfi, j −
|

rj

|) ⇔

loдe
y
(
λi

)

=

tmax,p
(cid:213)

j=1

pfi, j −
|

rj

|

(8)

y
loдe (
Selecting rj =
)
tmax
λi ×
randomly sampled sign.

±

+ pfi, j fulfils the above constraint with

The privacy guarantee of the combined mechanism

composition theorem (Theorem 1), (cid:205)m

i=1 ϵi .

is, by the

M

w , ..., pk,i,
·

Subsampling. In order to achieve a higher privacy guarantee, we
propose to subsample the data. Given a window size w, we draw
for each participant k
pk,i,n
one sample from
w )
(
)·
N, such that the sampling
and feature i independently where n
∈
windows are non-overlapping. Notice that this subsampling ap-
proach and the corresponding window size are independent of the
feature generation process. This method decreases the sensitivity
further by a factor of w: ∆u,fi,w ≤

n+1
(

w ·

tmax

δi .

ETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

5 DATA COLLECTION
Given the lack of a suitable dataset for evaluating privacy-
preserving eye tracking using differential privacy, we recorded
our own dataset. As a utility task, we opted to detect different doc-
ument types the users read, similar to a reading assistant [Kunze
et al. 2013b]. Instead of printed documents, participants read in
VR, wearing a corresponding headset. The recording of a single
participant consists of three separate recording sessions, in which
a participant reads one out of three different documents: a comic,
online newspaper, or textbook (see Figure 5). All documents include
a varying proportion of text and images. Each of these documents
was about a 10-minute read, depending on a user’s reading skill
(about 30 minutes in total).

Participants. We recruited 20 participants (10 male, 10 female)
aged 21 to 45 years through university mailing lists and adverts in
different university buildings on campus. Most participants were
BSc and MSc students from a large range of subjects (e.g. language
science, psychology, business administration, computer science)
and different countries (e.g. India, Pakistan, Germany, Italy). All
participants had little or no experience, with eye tracking studies
and had normal or corrected-to-normal vision (contact lenses).

Apparatus. The recording system consisted of a desktop com-
puter running Windows 10, a 24" computer screen, and an Oculus
DK2 virtual reality headset connected to the computer via USB. We
fitted the headset with a Pupil eye tracking add-on [Kassner et al.
2014] that provides state-of-the-art eye tracking capabilities. To
have more flexibility in the applications used by the participants in
the study, we opted for the Oculus “Virtual Desktop” that shows ar-
bitrary application windows in the virtual environment. To record
a user’s eye movement data, we used the capture software provided
by Pupil. We recorded a separate video from each eye and each doc-
ument. Participants used the mouse to start and stop the document
interaction and were free to read the documents in arbitrary order.
We encouraged participants to read at their usual speed and did
not tell them what exactly we were measuring.

Recording Procedure. After arriving at the lab, participants were
given time to familiarise themselves with the VR system. We showed
each participant how to behave in the VR environment, given that
most of them had never worn a VR headset before. We did not cali-
brate the eye tracker but only analysed users’ eye movements from
the eye videos post hoc. This was so as not to make participants feel
observed, and to be able to record natural eye movement behaviour.
Before starting the actual recording, we asked participants to sign a
consent form. Participants then started to interact with the VR inter-
face, in which they were asked to read three documents floating in
front of them (see Figure 5). After finishing reading a document, the
experimental assistant stopped and saved the recording and asked
participants questions on their current level of fatigue, whether they
liked and understood the document, and whether they found the
document difficult using a 5-point Likert scale (1: Strongly disagree
to 5: Strongly agree). Participants were further asked five questions
about each document to measure their text understanding. The VR
headset was kept on throughout the recording.

After the recording, we asked participants to complete a ques-
tionnaire on demographics and any vision impairments. We also

(a) Comic

(b) Newspaper
Figure 5: Each participant read three different documents:
(a) comic, (b) online newspaper, and (c) textbook.

(c) Textbook

assessed their Big Five personality traits [John and Srivastava 1999]
using established questionnaires from psychology. In this work we
only use the given ground truth information of a user’s gender
from all collected (private) information, the document type, and
IDs we assigned to each participant, respectively.

Eye Movement Feature Extraction. We extracted a total of 52
eye movement features, covering fixations, saccades, blinks, and
pupil diameter (see Table 1 in the supplementary material). Similar
to [Bulling et al. 2011b], we also computed wordbook features that
encode sequences of n saccades. We extracted these features using
a sliding window of 30 seconds (step size of 0.5 seconds).

6 EVALUATION
The overall goal of our evaluations was to study the effectiveness
of the proposed differential privacy method and its potential as
a building block for privacy-aware eye tracking. In these evalua-
tions, gaze-based document type classification served as the utility
task, while gender prediction exemplified an attacker without prior
knowledge about the target, and user re-identification an attacker
with prior knowledge.

6.1 Classifier Training
For each task, we trained a support vector machine (SVM) classifier
with radial basis function (RBF) kernel and bias parameter C = 1
on the extracted eye movement features. We opted for an SVM due
to the good performance demonstrated in a large body of work for
eye-based activity recognition [Bulling et al. 2011b; Steil and Bulling
2015]. As the first paper of its kind, one goal was to enable readers
to compare our results to the state of the art. We standardised
the training data (zero mean, unit variance) before training the
classifiers; the test data was standardised with the same parameters.
Majority voting was used to summarise all classifications from
different time points for the respective participant. We randomly
sampled training and test sets with an equal distribution of samples
for each of the respective classes, i.e. for the three document classes,
two gender classes and 20 classes for user identification.

Document Type Classification. We trained a multi-class SVM
for document type classification and used leave-one-person-out
cross-validation, i.e. we trained on the data of 19 participants and
tested on the remaining one – iteratively over all combinations
– and averaged the performance results in the end. We envision
that in the future, only differentially private data will be available;
therefore, we applied our privacy-preserving mechanism to the
training and test sets. However, currently there is non-noised data
available as well: thus, we set up an additional experiment using
clean data for training and noised data for testing.

Privacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

Figure 6: Performance for the threat model without prior
knowledge trained on differentially private data.

Figure 7: Performance for the threat model without prior
knowledge trained on clean data.

Gender Prediction. We trained a binary SVM for gender predic-
tion, using reported demographics as ground truth, and applied it
again with a person-independent (leave-one-person-out) cross-vali-
dation. Since we are in the without prior knowledge threat model, we
trained on differentially private and non-noised data to model both
the future and current situation, as for document type classification.

User (Re-)Identification. We trained a multi-class SVM for user
(re-)identification but without a leave-one-person-out evaluation
scheme. Instead, we used the first half of the extracted aggregated
feature vectors from each document and each participant for train-
ing. We tested on the remaining half, since here we are in the with
prior knowledge threat model. In this scenario, we assumed a pow-
erful attacker that was able to obtain training data from multiple
people without noise and was able to map their samples to their
identities. The attacker’s goal was to re-identify these people when
given noised samples without identity labels.

Implementing the Differential Privacy Mechanism. We applied the
exponential mechanism for each of our n = 20 participants and for
each of the m = 52 features, using a subsampling window size w =
10 to reduce sensitivity. In preliminary evaluations, we observed
that subsampling alone had no negative effect on the performance
of the SVM. The sensitivity for our differentially private mechanism
was generated by data-driven constraints: For each feature i, we
estimated δi by calculating the global minimum mini and maximum
maxi over all participants and time points and set δi = maxi
mini .
This way, the sensitivity ensures privacy protection even of outliers.
The noise we added in our study can be understood as reading-task-
specific noise. For all fi , we used the same ϵi so that the released
data of the whole dataset is (cid:205)52

−

i=1 ϵi -private.

We repeated our experiments five times each and report aver-
aged results to account for random subsampling and noise gen-
eration effects. As a performance metric, we report Accuracy =
T P +F P +T N +F N , where TP, FP, TN, and FN represent sample-based
true positive, false positive, true negative, and false negative counts.

T P +T N

6.2 Without Prior Knowledge
In Figure 6, we first evaluated the gender prediction task, our exam-
ple for the attacker without prior knowledge, trained on differentially
private (noised) data (Gender DP) for decreasing ϵ values. As one
might expect, decreasing ϵ, and thereby increasing the noise, nega-
tively influences the testing performance when trained on differen-
tially private data with ϵ < 30. For ϵ = 15, the performance almost

Figure 8: Performance for the threat model with prior
knowledge trained on clean data.

≤

drops to the chance level of 54% (random guessing in a slightly
imbalanced case due to the leave-one-person-out cross-validation).
We conclude that on our dataset, privacy of the participants’ gender
information is preserved for ϵ

15.
We then evaluated the impact of the noise level for this ϵ-value
on utility (see Figure 6) using the SVMs trained for document type
classification on noised data. As expected, noise negatively influ-
ences document type classification as well, but to a lesser extent
compared to gender prediction. For privacy preservation, it is suffi-
cient to set ϵ = 15, resulting in an accuracy of about 55% for docu-
ment type classification, which is still about 22% over chance level.
So far, we have assumed the SVMs were trained on noised data
(Document DP). At present, to the best of our knowledge, all avail-
able eye movement datasets are not noised. To study this current
situation, we trained both the gender prediction SVM and the doc-
ument type classification SVM without noise and tested at various
noise levels. Figure 7 shows the results of this evaluation. As can be
seen, also in this scenario, privacy can be preserved: For ϵ = 20, the
accuracy of the gender prediction has dropped below chance level,
while document type classification is still around 70%. We observed
that even ϵ = 30 would already preserve privacy, since training
with noise seems to balance out some negative noise effects. Thus,
we conclude that for both current and future situations, privacy
preservation is possible while preserving most of the utility.

6.3 With Prior Knowledge
Finally, we evaluated in Figure 8 the with prior knowledge threat
model, in which we assumed the attacker trained a SVM on the data
of multiple users without noise and wanted to re-identify which

nonoise7065605550454035302520151054321†-Value0102030405060708090100Accuracy in %Document DPGender DPDocument BaselineGender Baselinenonoise7065605550454035302520151054321†-Value0102030405060708090100Accuracy in %DocumentGenderDocument BaselineGender Baselinenonoise7065605550454035302520151054321†-Value0102030405060708090100Accuracy in %DocumentUIDDocument BaselineUID BaselineETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

person a set of noised samples belongs to. We again added the docu-
ment type classification performance to be able to judge the effects
on utility. As expected, the noise on the test data disturbed the
attacker’s classification ability: for ϵ = 40, the attacker’s accuracy
dropped to 50%. For ϵ = 15, it dropped down almost to chance level
(6.4%) while the utility preserved an accuracy of about 70%. We
conclude that, in this scenario as well, it is possible to preserve a
user’s privacy with acceptable costs on utility.

7 DISCUSSION
7.1 Privacy Concerns in Eye Tracking
The ever-increasing availability of eye tracking to end users, e.g. in
recent VR/AR headsets, in combination with the rich and sensitive
information available in the eyes (e.g. on personality [Hoppe et al.
2018]), creates significant challenges for protecting users’ privacy.
Our large-scale online survey on privacy implications of pervasive
eye tracking, the first of its kind, yielded a number of interesting
insights on this important, yet so far largely unexplored, topic (see
the supplementary material for the full results). For example, we
found that users are willing to share their eye tracking data for med-
ical applications, such as (early) disease detection or stress level
monitoring (see Figure 2), or for services, if these improve user
experience, e.g. in VR or AR (see Figure 3). On the other hand, par-
ticipants refused services that use eye movement data for interest
identification or shopping assistance, and a majority did not like the
idea of services inferring their identity, gender, sexual preference,
or race. These findings are interesting, as they suggest that users
are indeed willing to relinquish privacy in return for service use.
They also suggest, however, that users may not be fully aware of
the fact that, and to what extent, these services could also infer
privacy-sensitive information from their eyes. Our proposed dif-
ferential privacy approach addresses this challenge by allowing
sharing of eye movement data while protecting individual privacy.
To prevent inference of users’ private attributes from eye track-
ing data, not every data representation is suitable. Nonetheless, we
identified a clear information gap on the user side, since a majority
of participants agreed to share their eye tracking data in almost
every data representation (see Figure 3 in the supplementary ma-
terial). Participants seemed unaware of the fact that, in particular,
raw eye movement data representation is inappropriate to protect
their privacy. Adding noise to this data representation would not
protect their private attributes either: the added noise could easily
be removed by smoothing. Instead, we recommend using statistical
or aggregated feature representations that summarise temporal
and appearance statistics of a variety of eye movements, such as
fixation, saccades, and blinks. We are the first to propose a prac-
tical solution to this challenge by using differential privacy that
effectively protects private information, while at the same time
maintaining data utility.

7.2 Privacy-Preserving Eye Tracking
Informed by our survey results, we presented a privacy-aware
eye tracking method in a VR setting. This is the first of its kind
to quantitatively evaluate the practicability and effectiveness of
privacy-aware eye tracking. For that purpose, we study 1) two

realistic threat models (with and without prior knowledge about the
target user), and 2) different scenarios in training with and without
clean/non-noised data. We conducted an extensive evaluation on a
novel 20-participant dataset and 3) demonstrated the effectiveness
of the trained threat models on two example privacy-infringing
tasks, namely gender inference and user identification.

Applying differential privacy mitigates these privacy threats. The
fundamental principle of differential privacy is to apply appropriate
noise on the data to deteriorate the accuracy of a privacy-infringing
task while maintaining that of a utility task. As such, the level of
noise should be smaller than the inter-class difference in the utility
task but larger than that of the privacy-infringing task.

We showed in our practical evaluations that users’ privacy can be
preserved with acceptable accuracy of the utility task by applying
differential privacy. This conclusion was consistent across different
evaluation paradigms in our example study, which aimed to per-
form gaze-based document type classification while preserving the
privacy of users’ gender and identity.

Our mechanism can be used to sanitise data not only before
releasing it to the public, but also in VR/AR devices themselves,
since it sanitises one user at a time. Although our example study
focuses only on reading, we expect our method to generalise to
any other activity involving eye tracking. Due to our data-driven
approach, sensitivity can be adapted so that a similar trade-off
can be found. Depending on sensitivity and data vector length,
the privacy level ϵ of this trade-off may differ from the presented
results. Similarly, our study was evaluated on a typical HCI dataset
size, and we expect our approach to generalise to larger datasets
that will be available in the future, given the rapid emergence of
VR and eye tracking technology.

To conclude, the proposed method is an effective and low-cost
solution to preserve users’ privacy while maintaining the utility
task performance.

8 CONCLUSION
In this work we reported the first large-scale online survey to under-
stand users’ privacy concerns about eye tracking and eye movement
analysis. Motivated by the findings from this survey, we also pre-
sented the first privacy-aware gaze interface that uses differential
privacy. We opted for a virtual reality gaze interface, given the
significant and imminent threat potential created by upcoming
eye tracking technology equipped VR headsets. Our experimen-
tal evaluations on a new 20-participant dataset demonstrated the
effectiveness of the proposed approach to preserve private infor-
mation while maintaining performance on a utility task – hence,
implementing the principle ensure privacy without impeding utility.

ACKNOWLEDGMENTS
This work was funded, in part, by the Cluster of Excellence on
Multimodal Computing and Interaction (MMCI) at Saarland Uni-
versity, Germany, by a JST CREST research grant under Grant
No.: JPMJCR14E1, Japan, as well as by the German Federal Ministry
of Education and Research (BMBF) for the Center for IT-Security,
Privacy and Accountability (CISPA) (FKZ: 16KIS0656).

Privacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

REFERENCES
Devon Adams, Alseny Bah, Catherine Barwulor, Nureli Musaby, Kadeem Pitkin, and
Elissa M Redmiles. 2018. Ethics emerging: the story of privacy and security per-
ceptions in virtual reality. In Fourteenth Symposium on Usable Privacy and Security
(

2018). 427–442. https://doi.org/10.13016/M2B853K5P

SOUPS

Roman Bednarik, Tomi Kinnunen, Andrei Mihaila, and Pasi Fränti. 2005. Eye-
movements as a biometric. In Scandinavian conference on image analysis. Springer,
780–789. https://doi.org/10.1007/11499145_79

{

}

Andreas Bulling and Hans Gellersen. 2010. Toward Mobile Eye-Based Human-
https:

IEEE Pervasive Computing 9, 4 (2010), 8–12.

Computer Interaction.
//doi.org/10.1109/MPRV.2010.86

Andreas Bulling, Daniel Roggen, and Gerhard Tröster. 2011a. What’s in the Eyes
for Context-Awareness? IEEE Pervasive Computing 10, 2 (2011), 48–57. https:
//doi.org/10.1109/MPRV.2010.49

Andreas Bulling, Jamie A. Ward, Hans Gellersen, and Gerhard Tröster. 2011b. Eye
Movement Analysis for Activity Recognition Using Electrooculography.
IEEE
Transactions on Pattern Analysis and Machine Intelligence 33, 4 (2011), 741–753.
https://doi.org/10.1109/TPAMI.2010.86

Andreas Bulling, Christian Weichel, and Hans Gellersen. 2013. EyeContext: Recognition
of High-level Contextual Cues from Human Visual Behaviour. In Proc. ACM SIGCHI
Conference on Human Factors in Computing Systems (CHI). 305–308. https://doi.
org/10.1145/2470654.2470697

Andreas Bulling and Thorsten O. Zander. 2014. Cognition-Aware Computing. IEEE
Pervasive Computing 13, 3 (2014), 80–83. https://doi.org/10.1109/MPRV.2014.42
Virginio Cantoni, Chiara Galdi, Michele Nappi, Marco Porta, and Daniel Riccio. 2015.
GANT: Gaze analysis technique for human identification. Pattern Recognition 48, 4
(2015), 1027–1038. https://doi.org/10.1016/j.patcog.2014.02.017

Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating
noise to sensitivity in private data analysis. In Theory of cryptography conference.
Springer, 265–284. https://doi.org/10.1007/978-3-540-32732-5_32

Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differential
privacy. Foundations and Trends® in Theoretical Computer Science 9, 3–4 (2014),
211–407. https://doi.org/10.1561/0400000042

Simon Eberz, Kasper B Rasmussen, Vincent Lenders, and Ivan Martinovic. 2016. Looks
like eve: Exposing insider threats using eye movement biometrics. ACM Transactions
on Privacy and Security (TOPS) 19, 1 (2016), 1. https://doi.org/10.1145/2904018
Myrthe Faber, Robert Bixler, and Sidney K D’Mello. 2017. An automated behavioral
measure of mind wandering during computerized reading. Behavior Research
Methods (2017), 1–17. https://doi.org/10.3758/s13428-017-0857-y.

Liyue Fan and Li Xiong. 2012. Adaptively sharing time-series with differential privacy.

arXiv preprint arXiv:1202.3461 (2012).

Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas
Ristenpart. 2014. Privacy in Pharmacogenetics: An End-to-End Case Study of
https:
Personalized Warfarin Dosing. In USENIX Security Symposium. 17–32.
//doi.org/10.1.1.469.4356

John Paulin Hansen, Anders Sewerin Johansen, Dan Witzner Hansen, Kenji Itoh, and
Satoru Mashino. 2003. Command without a click: Dwell time typing by mouse
and gaze selections. In Proceedings of Human-Computer Interaction–INTERACT.
121–128. https://doi.org/10.1.1.535.168

Eckhard H Hess and James M Polt. 1960. Pupil size as related to interest value of visual
stimuli. Science 132, 3423 (1960), 349–350. https://doi.org/10.1126/science.132.3423.
349

Philip S Holzman, Leonard R Proctor, Deborah L Levy, Nicholas J Yasillo, Herbert Y
Meltzer, and Stephen W Hurt. 1974. Eye-tracking dysfunctions in schizophrenic
patients and their relatives. Archives of general psychiatry 31, 2 (1974), 143–151.
https://doi.org/10.1001/archpsyc.1974.01760140005001

Sabrina Hoppe, Tobias Loetscher, Stephanie A Morey, and Andreas Bulling. 2018. Eye
movements during everyday behavior predict personality traits. Frontiers in Human
Neuroscience 12 (2018), 105. https://doi.org/10.3389/fnhum.2018.00105

J Thomas Hutton, JA Nagel, and Ruth B Loewenson. 1984. Eye tracking dysfunction in
Alzheimer-type dementia. Neurology 34, 1 (1984), 99–99. https://doi.org/10.1212/
WNL.34.1.99

Oliver P John and Sanjay Srivastava. 1999. The Big Five trait taxonomy: History,
measurement, and theoretical perspectives. Handbook of personality: Theory and
research 2, 1999 (1999), 102–138.

Paweł Kasprowski. 2004. Human identification using eye movements. Praca doktorska,

Politechnika Œląska (2004). https://doi.org/10.13140/RG.2.1.3466.3924

Pawel Kasprowski and J Ober. 2003. Eye movement tracking for human identification.

In 6th World Conference BIOMETRICS.

Pawel Kasprowski and Józef Ober. 2005. Enhancing eye-movement-based biometric
identification method by using voting classifiers. In Biometric Technology for Human
Identification II, Vol. 5779. International Society for Optics and Photonics, 314–324.
https://doi.org/10.1117/12.603321

Moritz Kassner, William Patera, and Andreas Bulling. 2014. Pupil: an open source
platform for pervasive eye tracking and mobile gaze-based interaction. In Adj.
Proc. ACM International Joint Conference on Pervasive and Ubiquitous Computing
(UbiComp). 1151–1160. https://doi.org/10.1145/2638728.2641695

Tomi Kinnunen, Filip Sedlak, and Roman Bednarik. 2010. Towards task-independent
person authentication using eye movement signals. In Proceedings of the 2010
Symposium on Eye-Tracking Research & Applications. ACM, 187–190. https://doi.
org/10.1145/1743666.1743712

Oleg V Komogortsev and Corey D Holland. 2013. Biometric authentication via complex
oculomotor behavior. In Biometrics: Theory, Applications and Systems (BTAS), 2013
IEEE Sixth International Conference on. IEEE, 1–8. https://doi.org/10.1109/BTAS.
2013.6712725

Oleg V Komogortsev, Sampath Jayarathna, Cecilia R Aragon, and Mechehoul Mahmoud.
2010. Biometric identification via an oculomotor plant mathematical model. In
Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications. ACM,
57–60. https://doi.org/10.1145/1743666.1743679

Craig A Kuechenmeister, Patrick H Linton, Thelma V Mueller, and Hilton B White.
1977. Eye tracking in relation to age, sex, and illness. Archives of General Psychiatry
34, 5 (1977), 578–579. https://doi.org/10.1001/archpsyc.1977.01770170088008
Kai Kunze, Hitoshi Kawaichi, Kazuyo Yoshimura, and Koichi Kise. 2013a. The
Wordometer–Estimating the Number of Words Read Using Document Image Re-
trieval and Mobile Eye Tracking. In 12th International Conference on Document
Analysis and Recognition (ICDAR). 25–29. https://doi.org/10.1109/ICDAR.2013.14
Kai Kunze, Katsutoshi Masai, Masahiko Inami, Ömer Sacakli, Marcus Liwicki, Andreas
Dengel, Shoya Ishimaru, and Koichi Kise. 2015. Quantifying reading habits: counting
how many words you read. In Proceedings of the 2015 ACM International Joint
Conference on Pervasive and Ubiquitous Computing. ACM, 87–96. https://doi.org/
10.1145/2750858.2804278

Kai Kunze, Yuzuko Utsumi, Yuki Shiga, Koichi Kise, and Andreas Bulling. 2013b. I know
what you are reading: recognition of document types using mobile eye tracking.
In Proceedings of the 2013 International Symposium on Wearable Computers. ACM,
113–116. https://doi.org/10.1145/2493988.2494354

Daniel J Liebling and Sören Preibusch. 2014. Privacy considerations for a pervasive
eye tracking world. In Proceedings of the 2014 ACM International Joint Conference
on Pervasive and Ubiquitous Computing: Adjunct Publication. ACM, 1169–1177.
https://doi.org/10.1145/2638728.2641688

Anthony J Maeder and Clinton B Fookes. 2003. A visual attention approach to personal

identification. (2003).

G Matthews, W Middleton, B Gilmartin, and MA Bullimore. 1991. Pupillary diameter

and cognitive load. Journal of Psychophysiology (1991).

Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. 2017. Knock Knock,
Who’s There? Membership Inference on Aggregate Location Data. arXiv preprint
arXiv:1708.06145 (2017).

Nazir Saleheen, Supriyo Chakraborty, Nasir Ali, Md Mahbubur Rahman, Syed Monowar
Hossain, Rummana Bari, Eugene Buder, Mani Srivastava, and Santosh Kumar.
2016. mSieve: differential behavioral privacy in time series of mobile sensor data.
In Proceedings of the 2016 ACM International Joint Conference on Pervasive and
Ubiquitous Computing. ACM, 706–717. https://doi.org/10.1145/2971648.2971753
Negar Sammaknejad, Hamidreza Pouretemad, Changiz Eslahchi, Alireza Salahirad,
and Ashkan Alinejad. 2017. Gender classification based on eye movements: A
processing effect during passive face viewing. Advances in cognitive psychology 13,
3 (2017), 232. https://doi.org/10.5709/acp-0223-1

Julian Steil and Andreas Bulling. 2015. Discovery of everyday human activities from
long-term visual behaviour using topic models. In Proceedings of the 2015 ACM
International Joint Conference on Pervasive and Ubiquitous Computing. ACM, 75–85.
https://doi.org/10.1145/2750858.2807520

Sophie Stellmach and Raimund Dachselt. 2012. Look & touch: gaze-supported target
acquisition. In Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems. ACM, 2981–2990. https://doi.org/10.1145/2208636.2208709

Marc Tonsen, Julian Steil, Yusuke Sugano, and Andreas Bulling. 2017. InvisibleEye:
Mobile Eye Tracking Using Multiple Low-Resolution Cameras and Learning-Based
Gaze Estimation. Proceedings of the ACM on Interactive, Mobile, Wearable and
Ubiquitous Technologies 1, 3 (2017), 106. https://doi.org/10.1145/3130971

Roel Vertegaal et al. 2003. Attentive user interfaces. Commun. ACM 46, 3 (2003), 30–33.

https://doi.org/10.1145/636772.636794

Yongtuo Zhang, Wen Hu, Weitao Xu, Chun Tung Chou, and Jiankun Hu. 2018. Con-
tinuous Authentication Using Eye Movement Response of Implicit Visual Stimuli.
Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
1, 4 (2018), 177. https://doi.org/10.1145/3161410

Tianqing Zhu, Gang Li, Wanlei Zhou, and S Yu Philip. 2017. Differentially private
data publishing and analysis: a survey. IEEE Transactions on Knowledge and Data
Engineering 29, 8 (2017), 1619–1638. https://doi.org/10.1109/TKDE.2017.2697856

ETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

Privacy-AwareEyeTrackingUsingDifferentialPrivacy[SupplementaryMaterial]JULIANSTEIL,MaxPlanckInstituteforInformatics,SaarlandInformaticsCampusINKENHAGESTEDT,CISPAHelmholtzCenterforInformationSecurity,SaarlandInformaticsCampusMICHAELXUELINHUANG,MaxPlanckInstituteforInformatics,SaarlandInformaticsCampusANDREASBULLING,UniversityofStuttgart,InstituteforVisualisationandInteractiveSystemsWitheyetrackingbeingincreasinglyintegratedintovirtualandaugmentedreality(VR/AR)head-mounteddisplays,preservingusers’privacyisanevermoreimportant,yetunder-explored,topicintheeyetrackingcommunity.Wereportalarge-scaleonlinesurvey(N=124)onprivacyaspectsofeyetrackingthatprovidesthefirstcomprehensiveaccountofwithwhom,forwhichservices,andtowhatextentusersarewillingtosharetheirgazedata.Usingtheseinsights,wedesignaprivacy-awareVRinterfacethatusesdifferentialprivacy,whichweevaluateonanew20-participantdatasetfortwoprivacysensitivetasks:Weshowthatourmethodcanpreventuserre-identificationandprotectgenderinformationwhilemaintaininghighperformanceforgaze-baseddocumenttypeclassification.Ourresultshighlighttheprivacychallengesparticulartogazedataanddemonstratethatdifferentialprivacyisapotentialmeanstoaddressthem.Thus,thispaperlaysimportantfoundationsforfutureresearchonprivacy-awaregazeinterfaces.Thissupplementarydocumentprovidessurveyresultsofusers’privacyconcernsineyetracking,correspondingsignificanceandskewnesstests,thefulllistofquestionsandresultsofouronlinesurveyaswellasalistofallextractedeyemovementfeatures.CCSConcepts:•Securityandprivacy→Humanandsocietalaspectsofsecurityandprivacy;•Human-centeredcomputing→Humancomputerinteraction(HCI);Authors’addresses:JulianSteil,MaxPlanckInstituteforInformatics,SaarlandInformaticsCampus,jsteil@mpi-inf.mpg.de;InkenHagestedt,CISPAHelmholtzCenterforInformationSecurity,SaarlandInformaticsCampus,inken.hagestedt@uni-saarland.de;MichaelXuelinHuang,MaxPlanckInstituteforInformatics,SaarlandInformaticsCampus,mhuang@mpi-inf.mpg.de;AndreasBulling,UniversityofStuttgart,InstituteforVisualisationandInteractiveSystems,andreas.bulling@vis.uni-stuttgart.de.2019.ManuscriptsubmittedtoACMManuscriptsubmittedtoACMPrivacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

1SURVEYRESULTSWeconductedalarge-scaleonlinesurveytoshedlightonusers’privacyconcernsrelatedtoeyetrackingtechnologyandtheinformationthatcanbeinferredfromeyemovementdata.Weadvertisedoursurveyonsocialplatforms(Facebook,WeChat)andlocalmailinglistsforstudyannouncements.ThesurveyopenedwithgeneralquestionsabouteyetrackingandVRtechnologies;continuedwithquestionsaboutfutureuseandapplications,datasharingandprivacy(especiallyregardingwithwhomusersarewillingtosharetheirdata);andconcludedwithquestionsabouttheparticipants’willingnesstosharedifferenteyemovementrepresentations.Participantsansweredeachquestionona7-pointLikertscale(1:Stronglydisagreeto7:Stronglyagree).Tosimplifytheanalysis,wemergedscores1to3to“Disagree”and5to7to“Agree”.Attheendweaskedfordemographicinformationandofferedaraffle.1.1ServicesandAttributesDiseasesDetectionNatural VRInteractionVisual SearchTarget DetectionUser InterfaceInteractionUnderstandableWebsite ContentReading SkillImprovementLearning SkillImprovementStress LevelMonitoringInterestIdentificationActivityRecognitionShoppingAssistanceRecognized Private Company(in foreign country)SexualPreferenceGenderAgeMood andEmotionsRaceIdentity1-3 - Disagree:4 - Neither agree nor disagree:5-7 - Agree:13.715.6580.6524.194.8470.9741.948.8749.1926.615.6567.7450.8111.2937.9020.169.6870.1616.138.0675.8119.3511.2969.3573.398.0618.5550.8112.1037.1079.034.8416.1374.196.4519.3551.617.2641.1341.1312.1046.7744.3512.1043.5565.328.8725.8178.234.0317.74                                             Services                                                                                                         Private Attributes             Fig.1.Surveyresults(ServicesandAttributes):Withwhichserviceswouldyouagreetoshareyoureyetrackingdata(Services)?;Wouldyouagreetoprivateattributesbeinginferredbytheseservices(PrivateAttributes)?1.2WhomandWhereEye TrackingDataPrivate Company(in foreign country)GovernmentalAgency(non-health)Governmental Health AuthorityLocal CompanyInternationalCompanyPrivate Company(user's country)Private Company(foreign country)User Himself(home cloud)Company Internal Use (intranet)ResearchInstitutePublicPrivateConstrainedIn Exchange for BenefitsVR/AR1-3 - Disagree:4 - Neither agree nor disagree:5-7 - Agree:41.1312.9045.9762.905.6531.4537.108.0654.8461.2916.1322.5863.7112.9023.3960.4817.7421.7773.3917.748.8714.525.6579.8456.4516.1327.428.0611.2980.6563.719.6826.6158.0616.9425.0032.2613.7154.0363.7111.2925.0032.2616.1351.61Sharing                                                              Owner                                                                     Environment                         Application   Fig.2.Surveyresults(WhomandWhere):Wouldyouagreetoshareyoureyetrackingdataingeneral(Sharing);withwhom(Owner);where(Environment);inexchangeforbenefitsorforVR/ARusage(Application)?1.3DataRepresentationRawTemporalStatisticsAppearanceStatisticsFixationStatisticsFixation Pointson SurfaceScan Pathon SurfaceSaccadeStatisticsGaze PlotScan PathStatisticsHeatmapsAreas ofInterestAggregatedFeaturesRecognized Private Company(in foreign country)RawTemporalStatisticsAppearanceStatisticsFixationStatisticsFixation Pointson SurfaceScan Pathon SurfaceSaccadeStatisticsGaze PlotScan PathStatisticsHeatmapsAreas ofInterestAggregatedFeatures1-3 - Disagree:4 - Neither agree nor disagree:5-7 - Agree:16.9416.1366.9413.7114.5271.7714.529.6875.8125.0016.1358.8740.3215.3244.3539.5215.3245.1620.9713.7165.3231.4514.5254.0317.7419.3562.9032.2612.9054.8429.8412.1058.0617.7416.1366.138.875.6585.488.874.8486.297.268.0684.6811.298.0680.6516.948.8774.1916.137.2676.6113.715.6580.6515.327.2677.4213.717.2679.0313.717.2679.0313.718.8777.4212.906.4580.65                     No Modification                                                                            Modified Representation (Anonymised)  Fig.3.Surveyresults(DataRepresentations):Whatkindofdatarepresentationwouldyouagreetoshare(NoModification),anddoesthisbehaviourchangeifthedataisanonymisedpriortosharing(Anonymised)?ETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

2STATISTICALTESTSThefollowingtablescorrespondtotheFigures1,2,and3.WefoundnearlyallanswersfortheprovidedquestionstobesignificantlydifferentfromequaldistributiontestedwithPearson’schi-squaredtest(p<0.001,dof=6).Additionally,wecalculatedtheskewnessandobservedthatthemajorityofquestionsshowasignificantdifferencetothecorrespondingnormaldistribution(p<0.1).2.1StatisticaltestscorrespondingtoFigure1DiseasesDetectionNatural VRInteractionVisual SearchTarget DetectionUser InterfaceInteractionUnderstandableWebsite ContentReading SkillImprovementLearning SkillImprovementStress LevelMonitoringInterestIdentificationActivityRecognitionShoppingAssistanceRecognized Private Company(in foreign country)SexualPreferenceGenderAgeMood andEmotionsRaceIdentitychi-squaredp-valuez-score skewp-value skew91.191.7e-17-4.643.5e-654.845.0e-10-3.072.1e-325.482.8e-4-0.940.3543.11.1e-7-3.151.6e-324.923.5e-40.540.5955.633.5e-10-3.397.0e-468.847.1e-13-4.055.0e-540.733.3e-7-3.032.5e-373.587.5e-143.455.5e-419.53.4e-30.790.43114.91.9e-224.281.9e-5138.392.2e-274.311.6e-551.03.0e-90.710.4832.481.3e-5-1.040.3028.876.4e-5-0.350.7274.155.8e-142.992.8e-3189.313.6e-384.791.7e-6                                                 Services                                                                                                       Private Attributes          2.2StatisticaltestscorrespondingtoFigure2Eye TrackingDataPrivate Company(in foreign country)GovernmentalAgency(non-health)Governmental Health AuthorityLocal CompanyInternationalCompanyPrivate Company(user's country)Private Company(foreign country)User Himself(home cloud)Company Internal Use (intranet)ResearchInstitutePublicPrivateConstrainedIn Exchange for BenefitsVR/ARchi-squaredp-valuez-score skewp-value skew26.731.6e-4-0.970.3344.795.2e-81.590.1122.898.4e-4-1.530.1343.11.1e-71.50.1355.973.0e-101.80.0761.392.4e-111.760.0877.421.2e-142.060.0472.01.6e-13-4.584.7e-626.162.1e-41.550.1288.945.0e-17-4.957.5e-739.037.1e-72.10.0430.04.0e-51.70.0931.811.8e-5-1.970.0540.53.6e-72.550.0132.261.5e-5-1.990.05Sharing                                                               Owner                                                                       Environment                          Application   2.3StatisticaltestscorrespondingtoFigure3RawTemporalStatisticsAppearanceStatisticsFixationStatisticsFixation Pointson SurfaceScan Pathon SurfaceSaccadeStatisticsGaze PlotScan PathStatisticsHeatmapsAreas ofInterestAggregatedFeaturesRecognized Private Company(in foreign country)RawTemporalStatisticsAppearanceStatisticsFixationStatisticsFixation Pointson SurfaceScan Pathon SurfaceSaccadeStatisticsGaze PlotScan PathStatisticsHeatmapsAreas ofInterestAggregatedFeatureschi-squaredp-valuez-score skewp-value skew41.742.1e-7-3.251.1e-357.11.8e-10-3.781.6e-463.767.7e-12-3.987.0e-529.774.3e-5-2.130.0313.853.1e-2-0.780.449.791.3e-1-0.90.3740.054.5e-7-2.982.9e-321.761.3e-3-1.870.0648.529.3e-9-2.93.7e-319.613.2e-3-1.840.0726.162.1e-4-2.120.0349.316.5e-9-3.54.7e-495.262.4e-18-4.81.6e-696.611.3e-18-4.458.6e-691.081.8e-17-4.281.9e-574.714.4e-14-4.449.1e-651.452.4e-9-3.821.3e-458.688.4e-11-3.919.4e-575.842.6e-14-3.771.6e-461.392.4e-11-4.06.4e-568.58.3e-13-4.242.2e-568.169.7e-13-4.143.4e-562.741.3e-11-3.986.9e-573.248.8e-14-4.56.6e-6                    No Modification                                                                                Modified Representation (Anonymised)  Privacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

3SURVEYEVALUATION3.1EyeTrackingandVirtualReality(VR)Technologies3.1.1Iamfamiliarwitheyetrackingtechnology.12345670%10%20%30%40%4.0312.14.846.4533.0620.9718.551 - Strongly disagree2 - Disagree3 - Somewhat disagree4 - Neither agree nor disagree5 - Somewhat agree6 - Agree7 - Strongly agree3.1.2Howmanyeyetrackingapplicationsorexperimentshaveyouusedorparticipated?12345670%10%20%30%40%33.0634.6812.98.873.230.816.4501-23-45-67-89-10>103.1.3Iamconcernedabouteyetrackingtechnologyintermsof...1)2)3)4)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:6.4516.9414.5216.1321.7717.746.456.4520.9710.4815.3224.1915.327.266.4512.908.8713.7121.7726.619.680.815.654.848.8720.9725.8133.061)socialacceptability(e.g.:HowIamperceivedbyotherpeople?)2)mentalcomfortability(e.g.:increase/decreasementalworkload)3)physicalcomfortability(e.g.:increase/decreasephysicalworkload)4)privacyETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

3.1.4Iamfamiliarwithvirtualreality(VR)andaugmentedreality(AR)technology.12345670%10%20%30%0.813.237.268.8729.8425.025.01 - Strongly disagree2 - Disagree3 - Somewhat disagree4 - Neither agree nor disagree5 - Somewhat agree6 - Agree7 - Strongly agree3.1.5HowmanyVRapplicationsorexperimentshaveyouusedorparticipated?12345670%10%20%30%40%20.9737.117.748.066.450.818.8701-23-45-67-89-10>103.1.6IamconcernedaboutVRtechnologyintermsof...1)2)3)4)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:8.8720.1612.9017.7421.7714.524.038.068.877.2612.9026.6125.0011.294.033.235.6510.4828.2334.6813.714.0316.1312.1014.5217.7418.5516.941)socialacceptability(e.g.:HowIamperceivedbyotherpeople?)2)mentalcomfortability(e.g.:increase/decreasementalworkload)3)physicalcomfortability(e.g.:increase/decreasephysicalworkload)4)privacyPrivacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

3.2FutureUseofEyeTrackingData3.2.1Wouldyouagreetoshareeyetrackingdata...1)2)3)4)5)6)7)8)9)10)11)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:1.614.847.265.6515.3232.2633.063.2311.299.684.8429.0327.4214.528.8715.3217.748.8720.1624.194.847.268.0611.295.6521.7730.6515.3211.2924.1915.3211.2916.1319.352.423.2310.486.459.6826.6130.6512.903.235.657.268.0626.6133.0616.133.239.686.4511.2926.6122.5820.1635.4824.1913.718.0612.105.650.8116.1322.5812.1012.1020.9711.294.8444.3520.9713.714.8410.484.840.811)forearlydetectionofmentalandpsychologicaldiseaselikedementiaorParkinson?2)toenablehands-freeinteractionwithdisplaysandinvirtualrealty?YoucouldtypelettersorselectabuttonbygazeinteractionorinteractwithitemsorpersonsinVRmorenaturally.3)toidentifythetargetofyourvisualsearchtoprovideinformationaboutthetargetyouarelookingat(e.g.thenameofaperson,informationaboutaproduct,etc.)?4)toimproveinteractionswithuserinterfacesanddevices,e.g.tomakethemmoreintuitiveorfaster?5)toallowappsandwebsitestoprovidecontenteasytounderstand?6)toanalysereadingabilityandproposemethodstoimproveyourreadingskillsortochangereadingmaterialintermsofappearance(enlargingtext,highlightingcurrentline)?7)toanalyseandimproveyourlearningskills?8)monitoryourstresslevelandtoprovideearly-stagehealthcareintervention?9)toidentifyyourinterests,e.g.whatyoulikeordislike,andguideyoulikeashoppingassistance,ortosteeradvertisement?10)toidentifyactivityspecificpatternswhichcouldbeusedforactivitytracking,lifeloggingorself-quantifying,(e.g.reading,watchingTV,playingavideogame,computerwork,etc.)?11)toanalyseyourshoppingbehaviouronwebsitesorwithinshoppingmallstoimproveproductplacement?3.2.2Wouldyouagreetoshareeyetrackingdatatoidentifyyour...forabetterservice(e.g.entertainment,news,business,education,etc.)?1)2)3)4)5)6)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:49.1918.556.456.454.8412.102.4231.4515.324.847.2616.1320.974.0320.9712.108.0612.1016.9426.613.2325.8110.488.0612.1020.1618.554.8437.9020.976.458.878.0614.523.2355.6517.744.844.038.069.680.001)sexualpreferences2)gender3)age4)moodandemotions5)race6)identityETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

3.3SharingEyeTrackingData3.3.1Wouldyoushareyoureyetrackingdata?12345670%10%20%30%12.116.1312.912.927.4215.323.231 - Strongly disagree2 - Disagree3 - Somewhat disagree4 - Neither agree nor disagree5 - Somewhat agree6 - Agree7 - Strongly agree3.3.2Ingeneral,IwouldtrustamanufacturerandIamwillingtosharemyeyetrackingdataifitisoperated/ownedby...1)2)3)4)5)6)7)8)9)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:25.0021.7716.135.6521.778.870.8112.1014.5210.488.0624.1922.588.0621.7725.0014.5216.1318.553.230.8131.4518.5513.7112.9019.353.230.8125.8127.427.2617.7418.552.420.8133.0623.3916.9417.748.870.000.004.036.454.035.6522.5829.8427.4226.6116.9412.9016.1313.7110.483.233.230.814.0311.2918.5534.6827.421)agovernmentalagency(non-healthrelated).2)agovernmentalhealthauthority(e.g.,city,state/province,federal/national).3)arecognizedlocalcompany.4)arecognizedinternationalcompany.5)arecognizedprivatecompanyinuser’scountry.6)arecognizedprivatecompanyinforeigncountry.7)theuserhimself(homecloud).8)companyinternaluse(intranet).9)researchinstitute.3.3.3Wouldyoushareyoureyetrackingdatainexchangeforbenefitslikeshoppingassistance,activitylogging,etc.?12345670%10%20%30%25.8126.6111.2911.2912.99.682.421 - Strongly disagree2 - Disagree3 - Somewhat disagree4 - Neither agree nor disagree5 - Somewhat agree6 - Agree7 - Strongly agreePrivacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

3.3.4Wouldyoushareyoureyetrackingdataifthedatawascollectedin...1)2)3)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:25.8124.1913.719.6815.3210.480.8122.5823.3912.1016.9413.719.681.617.2612.1012.9013.7126.6122.584.841)public,e.g.trainstationorpark?2)aprivateenvironment,e.g.officeorhome?3)constrainedenvironments(e.g.aspecificroomorplace)?3.3.5Wouldyoushareeyetrackingdataifthedatawascollectedinoneofthefollowingplaces?1)2)3)4)5)6)7)8)9)10)11)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:29.0325.009.685.6520.168.062.4225.8125.0020.978.0611.298.060.8125.0022.5814.529.6814.5212.101.6123.3920.978.0614.5217.7410.484.8416.1320.977.2613.7120.1615.326.4560.4819.359.684.841.613.230.8129.8426.6110.488.0612.9010.481.6112.1015.3212.1011.2918.5519.3511.2932.2619.3510.4810.4814.5211.291.6133.0620.9710.489.6812.9010.482.4234.6820.978.876.4512.1014.522.421)departmentstore2)friend’shome3)publictransport4)home5)library6)restroom7)workplace8)car9)lobby(e.g.hotel)10)cafe11)street3.3.6WouldyoushareyoureyetrackingdataifthedatawascollectedinVRorAR?.12345670%10%20%30%8.8711.2912.116.1328.2319.354.031 - Strongly disagree2 - Disagree3 - Somewhat disagree4 - Neither agree nor disagree5 - Somewhat agree6 - Agree7 - Strongly agreeETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

3.3.7Wouldyoushareyoureyetrackingdataifthedatawascollected...1)2)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:12.9020.169.6825.0016.1312.104.0313.7117.7416.1323.3916.1311.291.611)indoor?2)outdoor?3.3.8Wouldyoushareeyetrackingdataifthecollecteddatawasrecordedinthe...1)2)3)4)5)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:9.6823.397.2631.4512.9011.294.038.0621.775.6530.6517.7412.104.038.0621.774.0333.8716.9411.294.0310.4824.196.4534.6810.4810.483.2314.5226.6111.2929.0312.904.840.811)morning?2)noon?3)afternoon?4)evening?5)night?3.3.9Wouldyoushareeyetrackingdataiftherecordingdurationwasrestrictedtooneofthefollowingoptions?1)2)3)4)5)6)7)8)9)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:1.617.265.659.6829.0333.8712.9010.4817.7423.397.2629.0310.481.6114.5220.9712.9016.9418.5510.485.6522.5830.6512.9010.4812.908.062.428.068.0612.1016.1321.7720.1613.7117.7423.3920.1612.1011.2911.294.0318.5523.3912.9020.1614.527.263.2320.1629.8412.1018.558.874.845.6544.3525.0016.135.655.652.420.811)foraspecificapplicationwithuserallowance2)automaticdatarecordingifeyetrackingdataisnec-essaryforusage3)selectedhoursperday4)duringworktime(atwork)5)forpersonaluse6)duringfreetime7)duringworkdays8)duringweekend9)wholedayrecordingPrivacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

3.3.10Wouldyoushareeyetrackingdataifthecollecteddatawassavedfor...1)2)3)4)5)6)7)8)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:4.846.4510.4812.1035.4820.1610.4846.7723.398.069.688.873.230.0013.7113.7113.7120.1620.1614.524.0318.5518.5512.9020.9714.5210.484.0329.0320.1613.7112.9015.327.261.6137.9024.198.0612.9010.485.650.8143.5522.5812.109.687.264.030.8162.9016.138.067.265.650.000.001)applicationspecificpurpose(e.g.directapplicationfeedback,VRinteractionandgaming,etc)?2)anunspecifiedamountoftime?3)aday?4)aweek?5)amonth?6)ahalfyear?7)ayear?8)forever?3.3.11Wouldyoushareeyetrackingdataifthedatawascollectedduringoneofthefollowingemotions?1)2)3)4)5)6)7)8)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:14.5221.778.8722.5814.5216.131.6115.3220.976.4525.0013.7116.941.6115.3223.3912.1023.3912.1012.101.6111.2914.525.6521.7723.3917.745.6511.2912.904.0324.1924.1917.745.6516.1320.166.4525.8115.3215.320.8113.7116.943.2325.8118.5516.944.8414.5213.714.0325.8119.3516.945.651)fear2)anger3)sadness4)joy5)surprise6)disgust7)trust8)anticipationETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

3.3.12Wouldyoushareeyetrackingdataifitwascollectedtorunanapplicationofthefollowingcategories?1)2)3)4)5)6)7)8)9)10)11)12)13)14)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:22.5818.5517.7413.7120.167.260.0017.7419.3520.9712.1016.9411.291.618.0611.2911.2910.4825.8122.5810.4820.9721.7720.1613.7116.135.651.6112.9014.5211.2912.9025.8118.554.0321.7720.9714.5213.7117.748.872.4224.1929.8418.558.8710.486.451.6125.0025.0015.3215.3211.297.260.8115.3212.108.0612.9032.2614.524.8419.3520.1614.5215.3216.1311.293.2313.7113.7112.1017.7417.7420.974.039.685.656.458.8722.5833.0613.7121.7722.5811.2917.7412.9011.292.423.231.613.235.6515.3234.6836.291)utilities(e.g.taxiapp,bankapp,etc.)2)entertainment(e.g.streaming,chatting,watchingvideos,etc.)3)games(e.g.VR)4)news5)productivity6)lifestyle7)socialnetworking8)business9)education/parenting10)travel11)book12)health/medicalandfitness13)foodanddrink14)research(anonymiseddatastorage)3.3.13Wouldyoushareeyetrackingdataifitwascollectedduringoneofthefollowingactivities?1)2)3)4)5)6)7)8)9)10)11)12)13)14)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:20.9726.6114.5211.2915.329.681.6120.1619.3511.299.6822.5813.713.2311.299.689.6812.9029.8419.357.2612.9013.718.8712.1027.4218.556.4520.1621.7716.1311.2916.1311.293.2325.8114.5221.7715.3212.908.061.619.687.267.2616.1329.8424.195.6517.7419.3511.2919.3516.9412.103.238.877.269.6815.3225.8125.008.0628.2313.718.8732.268.067.261.6119.3516.949.6816.1318.5513.715.6520.9715.3213.7116.1318.5511.294.0322.5817.749.6815.3216.1314.524.0322.5815.3212.9013.7119.359.686.451)officework2)computerwork3)reading4)writing5)browsing6)socialinteraction7)gaming8)eating,drinking9)driving10)smoking11)walking12)mobilephoneinteraction13)watchingTV14)concentratedworkPrivacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

3.3.14Wouldyoushareeyetrackingdataifitwascollectedwhileyouareinteractingwithoneofthefollowingdevices?1)2)3)4)5)6)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:12.9016.138.8712.9025.8116.137.2612.9016.138.8714.5225.0016.136.4516.1316.138.8715.3225.0014.524.0312.9017.748.0616.9424.1915.324.849.688.068.0612.1029.8424.198.0616.9415.328.0618.5519.3516.944.841)desktopcomputer2)laptop3)mobilephone4)tablet5)book6)TV3.3.15Wouldyoushareeyetrackingdataifthedataiscollectedwhileyouareinteractingwithoneofthefollowingpersons?1)2)3)4)5)6)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:27.4220.1610.487.2618.5512.104.0328.2320.9712.1011.2913.7110.483.2314.528.8710.4812.1029.8416.138.0623.3916.1310.4819.3518.558.873.2323.3922.588.0615.3216.139.684.8432.2620.9710.4815.328.877.264.841)friends2)relatives3)pets4)foreigners/strangers5)workingcolleagues6)bossETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

3.3.16Wouldyoushareeyetrackingdataifthekindofrecordedwasrestrictedto...1)2)3)4)5)6)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:6.4510.4810.4812.1026.6128.235.6519.3512.9016.9416.1320.9711.292.4216.1312.9012.1017.7421.7715.324.0319.3516.1313.7116.1318.5511.294.8417.7416.139.6816.1317.7418.554.0322.5816.1312.1017.7416.9411.293.231)gazeorpupilbehaviour?2)scenevideocontent?3)eyevideocontent?4)gazeorpupilbehaviour+scenevideocontent?5)gazeorpupilbehaviour+eyevideocontent?6)gazeorpupilbehaviour+scenevideocontent+eyevideocontent?3.3.17Supposeyouwanttocreateananonymousonlineidentityinordertoshareyoureyetrackingdata.Wouldyou“hide”thefollowingADDITIONALpersonalinformation?1)2)3)4)5)6)7)8)9)10)11)12)13)14)15)16)17)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:3.25.75.75.78.127.444.44.02.42.43.29.724.254.04.00.82.43.26.527.455.64.00.82.42.43.226.660.53.28.96.510.510.522.637.94.07.38.115.312.118.634.74.89.716.918.613.712.124.24.811.319.416.916.19.721.86.58.916.112.910.518.626.65.75.714.513.78.918.633.13.25.74.08.96.520.251.63.23.23.22.44.023.460.55.78.113.713.710.520.228.26.520.214.515.39.714.519.48.914.512.916.17.312.927.413.720.219.414.58.16.517.76.513.715.312.96.512.932.31)firstname2)lastname3)identifiableprofilepicture4)residentialaddress5)citywhereIlive6)occupationandemploymentinformation7)hobbies8)interests9)currentlocationinformation(e.g.,kitchen,publictransport,office)10)myhealthcondition(s)11)emailaddress12)phonenumber13)ageanddateofbirth14)gender15)race16)eyecolor17)irisimagePrivacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

3.4EyeTrackingDataRepresentations3.4.1Wouldyouagreetoshareeyetrackingdatawhichconsistsof...1)2)3)4)5)6)7)8)9)10)11)12)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:4.037.265.6516.1322.5827.4216.943.237.263.2314.5225.0029.8416.943.238.063.239.6829.0328.2318.553.2312.109.6816.1320.1626.6112.1011.2914.5214.5215.3220.1619.354.8415.3210.4813.7115.3217.7420.167.264.849.686.4513.7127.4225.0012.909.6813.718.0614.5222.5823.398.065.658.064.0319.3531.4518.5512.908.8712.1011.2912.9022.5823.398.878.0613.718.0612.1025.0023.399.685.658.873.2316.1322.5830.6512.901)therawxandygazeorpupilpositionovertime?2)statisticsofsteady(fixations)anddynamic(saccades)stateoftheeyes(whenfixationsandsaccadestakeplace)?3)statisticsofsteady(fixations)anddynamic(saccades)stateoftheeyes(howoftenfixationsandsaccadesappearinagiventimerange)?4)statisticsofeyetrackingdatawhichdescribethenumberoffixations,fixationdurationaswellastheirspatialdistributiononpublicdisplays,computermonitors,orinVRenvironment?5)fixationpointsonpublicdisplays,computermonitors,orinVRenvironment?6)scanpathinformation,theconcatenationofgazemovementsonpublicdisplays,computermonitors,orinVRenvironment?7)scanpathstatistics,e.g.whetherafteragazemovementtoleftisfollowedbyamovementupwards,onpublicdisplays,computermonitors,orinVRenvironment?8)fixationswithdurationandscanpathinformation(GazePlot),theconcatenationofgazemovementsandfixationalbehaviour,onpublicdisplays,computermonitors,orinVRenvironment?9)fixationswithdurationandscanpathinformationstatistics,e.g.whetherafteragazemovementtoleftisfollowedbyamovementupwardsandhowlongthefollowingfixationlasts,onpublicdisplays,computermonitors,orinVRenvironment?10)heatmaps,user’sgazedistributiononpublicdisplays,computermonitors,orinVRenvironment?11)statisticsofgazedistributiononareasofinterests(AOIs)onpublicdisplays,computermonitors,orinVRenvironment?12)aggregatedfeatures,givenasso-calledfeaturevectors,whereeachentryofsuchavectordescribeafeatureofuser’sbehaviourlikeaverageblinkingrate,fixationduration,ratioofsaccadicmovements,etc.withinagiventimewindow?ETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

3.4.2Imagineyoureyetrackingdatacouldbemodifiedsothatitisanonymous,i.e.indistinguishablefromthatofanotheruser.Wouldyouagreetoshare...1)2)3)4)5)6)7)8)9)10)11)12)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:1.612.424.845.6525.0032.2628.230.813.234.844.8428.2329.8428.230.813.233.238.0627.4229.0328.232.424.034.848.0624.1928.2328.234.035.657.268.8722.5825.8125.813.237.265.657.2625.8125.0025.810.816.456.455.6529.0323.3928.233.235.656.457.2625.8125.0026.612.425.655.657.2623.3928.2327.423.234.845.657.2627.4223.3928.232.426.454.848.8725.0025.0027.423.234.844.846.4525.0027.4228.231)therawxandygazeorpupilpositionovertime?2)statisticsofsteady(fixations)anddynamic(saccades)stateoftheeyes(whenfixationsandsaccadestakeplace)?3)statisticsofsteady(fixations)anddynamic(saccades)stateoftheeyes(howoftenfixationsandsaccadesappearinagiventimerange)?4)statisticsofeyetrackingdatawhichdescribethenumberoffixations,fixationdurationaswellastheirspatialdistributiononpublicdisplays,computermonitors,orinVRenvironment?5)fixationpointsonpublicdisplays,computermonitors,orinVRenvironment?6)scanpathinformation,theconcatenationofgazemovementsonpublicdisplays,computermonitors,orinVRenvironment?7)scanpathstatistics,e.g.whetherafteragazemovementtoleftisfollowedbyamovementupwards,onpublicdisplays,computermonitors,orinVRenvironment?8)fixationswithdurationandscanpathinformation(GazePlot),theconcatenationofgazemovementsandfixationalbehaviour,onpublicdisplays,computermonitors,orinVRenvironment?9)fixationswithdurationandscanpathinformationstatistics,e.g.whetherafteragazemovementtoleftisfollowedbyamovementupwardsandhowlongthefollowingfixationlasts,onpublicdisplays,computermonitors,orinVRenvironment?10)heatmaps,user’sgazedistributiononpublicdisplays,computermonitors,orinVRenvironment?11)statisticsofgazedistributiononareasofinterests(AOIs)onpublicdisplays,computermonitors,orinVRenvironment?12)aggregatedfeatures,givenasso-calledfeaturevectors,whereeachentryofsuchavectordescribeafeatureofuser’sbehaviourlikeaverageblinkingrate,fixationduration,ratioofsaccadicmovements,etc.withinagiventimewindow?Privacy-Aware Eye Tracking Using Differential Privacy

ETRA ’19, June 25–28, 2019, Denver, CO, USA

3.4.3Wouldyouagreetosharescenevideoinformationofpublicdisplays,computermonitors,orinVRenvironmentwhichconsistsof...1)2)3)4)5)6)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:10.4816.9421.7716.9419.3512.102.428.8715.3213.7120.1623.3915.323.238.0614.5214.5216.9425.8116.943.234.8416.1313.7117.7425.8116.944.844.8416.1312.9017.7425.8119.353.238.0616.1311.2914.5228.2317.744.031)thescenecontentduringawholeeyetrackingrecording?2)singleframefromeachfixation?3)gazestripes,sequenceofimageframefromeachfixation?4)tinyimagepatchesaroundthegazepositionduringawholeeyetrackingrecording?5)tinyimagepatchesaroundthegazepositionfromeachfixation?6)thewholescenevideobutwithblurredoutsurroundingandclearobjectofinterest?3.4.4Wouldyouagreetoshareeyevideoinformationrecordedfromeyetrackingcamerawhichconsistsof...1)2)3)1 - Strongly disagree:2 - Disagree:3 - Somewhat disagree:4 - Neither agree nor disagree:5 - Somewhat agree:6 - Agree:7 - Strongly agree:17.7420.1619.3511.2913.7113.714.0314.5218.5511.2910.4820.9716.138.066.4515.328.0610.4822.5827.429.681)thewholeeyevideowithvisibleirisandsurroundingfacialexpressions?2)sharethewholeeyevideowithvisibleirisbutblurredsurroundingfacialexpressions?3)thewholeeyevideobutonlyshowingthepupilcentrewithoutirisorfacialexpressions?ETRA ’19, June 25–28, 2019, Denver, CO, USA

Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling

4EYEMOVEMENTFEATUREEXTRACTIONTable1summarisesthefeaturesthatweextractedfromfixations,saccades,blinks,pupildiameter,andauser’sscanpaths.Similarto[Bullingetal.2011],eachsaccadeisencodedasacharacterformingwordsoflengthn(wordbook).Weextractedthesefeaturesonaslidingwindowof30seconds(stepsizeof0.5seconds).Fixation(8)rate,mean,max,varofdurations,mean/varofvarpupilpositionwithinonefixationSaccades(12)rate/ratioof(small/large/right/left)saccades,mean,max,varianceofamplitudesCombined(1)ratiosaccadestofixationsWordbooks(24)numberofnon-zeroentries,maxandminentries,andtheirdifferenceforn-gramswithn<=4Blinks(3)rate,mean/varblinkdurationPupilDiameter(4)mean/varofmean/varduringfixationsTable1.Weextracted52eyemovementfeaturestodescribeauser’seyemovementbehaviour.Thenumberoffeaturespercategoryisgiveninparentheses.REFERENCESAndreasBulling,JamieA.Ward,HansGellersen,andGerhardTröster.2011.EyeMovementAnalysisforActivityRecognitionUsingElectrooculography.IEEETransactionsonPatternAnalysisandMachineIntelligence33,4(2011),741–753.https://doi.org/10.1109/TPAMI.2010.86