The expectation-maximization algorithm for autoregressive models
with normal inverse Gaussian innovations

Monika S. Dhulla,∗, Arun Kumara, Agnieszka Wy(cid:32)loma´nskab

aDepartment of Mathematics, Indian Institute of Technology Ropar, Rupnagar, Punjab - 140001, India
bFaculty of Pure and Applied Mathematics, Hugo Steinhaus Center, Wroclaw University of Science and
Technology, Wyspianskiego 27, 50-370 Wroclaw, Poland

Abstract

The autoregressive (AR) models are used to represent the time-varying random process in
which output depends linearly on previous terms and a stochastic term (the innovation). In
the classical version, the AR models are based on normal distribution. However, this distri-
bution does not allow describing data with outliers and asymmetric behavior. In this paper,
we study the AR models with normal inverse Gaussian (NIG) innovations. The NIG distri-
bution belongs to the class of semi heavy-tailed distributions with wide range of shapes and
thus allows for describing real-life data with possible jumps. The expectation-maximization
(EM) algorithm is used to estimate the parameters of the considered model. The eﬃcacy of
the estimation procedure is shown on the simulated data. A comparative study is presented,
where the classical estimation algorithms are also incorporated, namely, Yule-Walker and
conditional least squares methods along with EM method for model parameters estimation.
The applications of the introduced model are demonstrated on the real-life ﬁnancial data.

Keywords: Normal inverse Gaussian distribution, autoregressive model, EM algorithm,
Monte Carlo simulations, ﬁnancial data

1. Introduction

In time series modeling, the autoregressive (AR) models are used to represent the time-
varying random process in which output depends linearly on previous terms and a stochastic
term also known as error term or innovation term. In the classical approach, the marginal
distribution of the innovation terms is assumed to be normal. However, the application
of non-Gaussian distributions in time series allows modeling the outliers and asymmetric
behavior visible in many real data. In a study conducted by Nelson and Granger [1], it was
shown that out of 21 real time series data, only 6 were found to be normally distributed. In
addition, in ﬁnancial markets the distribution of the observed time series (log-returns) are
mostly non-Gaussian and have tails heavier than the normal distribution, however lighter

∗Corresponding author.
Email address: 2018maz0005@iitrpr.ac.in (Monika S. Dhull)

1
2
0
2

v
o
N
2
1

]
E
M

.
t
a
t
s
[

1
v
5
6
5
6
0
.
1
1
1
2
:
v
i
X
r
a

 
 
 
 
 
 
than the power law. These kind of distributions are also called semi heavy-tailed, see, e.g.,
[2–4]. The AR models with non-Gaussian innovations are very well studied in the literature.
Sim [5] considered AR model of order 1 with Gamma process as the innovation term. For AR
models with innovations following a Student’s t-distribution, see, e.g., [6–9] and references
therein. Note that Student’s t-distribution is used in modeling of asset returns [10].

One of the semi heavy-tailed distributions with wide range of shapes is normal inverse
Gaussian (NIG), which was introduced by Barndorﬀ-Nielsen [11]. NIG distributions were
used to model the returns from the ﬁnancial time-series [12–18]. In practical situations, we
come across the data with skewness, extreme values or with missing values which can be
easily assimilated by the NIG distribution. The distribution has stochastic representation,
i.e., it can be written as the normal variance-mean mixture where the mixing distribution
is the inverse Gaussian distribution and a more general distribution known as generalised
hyperbolic distribution is obtained by using generalised inverse Gaussian as the mixing
distribution [19].
In recent years, new methods dedicated to analyze and estimate the
parameters related to NIG distributions based models were introduced which is a testimony
of their popularity, see, e.g. [20–23].

It is worth mentioning, apart from applications in economics and ﬁnance, the NIG dis-
tributions have found interesting applications in many other areas, such as computer science
[24], energy markets [25], commodity markets [26] and image analysis [27]. The multivariate
NIG distributions, counterparts of univariate NIG distributions, were considered in [28, 29]
and were applied in various disciplines, see e.g. [30].

In the literature, there are also various models and stochastic processes that are based
on NIG distribution. The very ﬁrst example is the NIG L´evy process, see e.g, [31] which was
also applied to ﬁnancial data modeling [32]. There are also numerous time series models with
NIG distributed innovations and their various applications. We mention here the interesting
analysis of heteroscedastic models [33–37] and autoregressive models [38], see also [39].

In this paper, we study the autoregressive model of order p (an AR(p)) with NIG inno-
vations. Because the NIG distribution tails are heavier than the normal one, the introduced
model can capture large jumps in the real-life time-series data which are quite ubiquitous.
Moreover, by applying the autoregressive ﬁlter, the considered model can capture the possi-
ble short dependence in the data that is characteristic for ﬁnancial time series returns. We
introduce a new estimation algorithm for the analyzed model’s parameters. A step-by-step
procedure for model parameters’ estimation based on expectation-maximization (EM) al-
gorithm is proposed. According to our knowledge, the EM algorithm was not used for the
time series models with NIG distributed innovations. For EM-based approach for the NIG
distributions, see, e.g. [22]. Thus, in this sense we extend this research. The eﬃcacy of the
proposed estimation procedure is shown for the simulated data. For the comparison, the
model parameters are also estimated using Yule-Walker (YW) and conditional least square
(CLS) methods, the most known algorithms for the AR models with ﬁnite-variance residu-
als. The comparative study clearly indicates that the EM-based algorithm outperforms the
other considered methods. Finally, the introduced model applications are demonstrated on
NASDAQ stock market index data. The introduced model explains very well the NASDAQ
index data which can not be modeled using AR model with normally distributed innovations.

2

The rest of the paper is organized as follows: In Section 2, ﬁrst we provide the main
properties of the NIG distribution and then the AR(p) model with NIG innovation term is
introduced. A step-by-step procedure to estimate the parameters of the introduced model
based on EM algorithm is discussed in Section 3. In Section 4, the eﬃcacy of the estimation
In this section we also present the comparative
procedure is proved for simulated data.
study, where the new technique is compared with the YW and CLS algorithm. Finally, the
applications to real ﬁnancial data are demonstrated. The last section concludes the paper.

2. NIG autoregressive model

In this section, we introduce the AR(p) model having independent identically distributed
(i.i.d.) NIG innovations. However, ﬁrst we remind the deﬁnition and main properties of the
NIG distribution.
NIG distribution: A random variable X is said to have a NIG distribution which is
denoted by X
NIG (α, β, µ, δ), if its probability density function (pdf) has the following
form

∼

f (x; α, β, µ, δ) =

(cid:16)

δ

(cid:112)

α2

exp

α
π

(cid:17)

βµ

β2

−

−

φ(x)−1/2K1(δαφ(x)1/2) exp(βx), x

R,

(2.1)

∈

where φ(x) = 1 + [(x
the modiﬁed Bessel function of the third kind of order ν evaluated at x and is deﬁned by

R, δ > 0 and Kν(x) denotes

γ2 + β2, 0

α, µ

≤ |

| ≤

−

∈

β

µ)/δ]2, α = (cid:112)

Kν(x) =

1
2

(cid:90) ∞

0

yν−1e− 1

2 x(y+y−1)dy.

Using the asymptotic properties of the modiﬁed Bessel function, Kν(x)
and the fact that φ(x)

, we have the following expression

(x/δ)2 as x

∼

(cid:112) π

2 e−xx−1/2 [40]

f (x; α, β, µ, δ)

∼

∼

∼

∼

α
π
α
π
α
π
(cid:114) α
2π

∼

→ ∞
e(δ√α2−β2−βµ)φ(x)−1/2

(cid:114) π
2

e−δα√φ(x) (cid:16)

δα

(cid:112)

φ(x)

(cid:17)−1/2

eβx

e(δ√α2−β2−βµ)

e(δ√α2−β2−βµ)

(cid:114) π
2
(cid:114) π
2

(δα)−1/2φ(x)−3/4e−δα√φ(x)eβx

(δα)−1/2δ3/4x−3/4eµαe−(α−β)x, α > β,

e(δ√α2−β2−βµ)δx−3/2e−(α−β)x, α > β, as x

.
→ ∞

From the above, one can conclude that the tail probability for NIG distributed random
variable X satisﬁes the following

P(X > x)

∼

cx−3/2e−(α−β)x, as x

,
→ ∞

δ

where c = (cid:112) α
(α−β) e(δ√α2−β2−βµ), which shows that NIG is a semi-heavy tailed distribution
[2–4]. It is worth mentioning that the NIG distributed random variable X can be represented
3

2π

in the following form

X = µ + βG + √GZ,

(2.2)

where Z is a standard normal random variable i.e. Z
Gaussian (IG) distribution with parameters γ and δ denoted by G
of the following form

∼

∼

N (0, 1) and G has an inverse
IG(γ, δ), having pdf

g(x; γ, δ) =

δ
√2π

exp(δγ)x−3/2 exp





1
2

−

(cid:32)

δ2
x

(cid:33)

+ γ2x

 , x > 0.

(2.3)

The representation given in Eq.
(2.2) is useful when we generate the NIG distributed
random numbers. It is also suitable to apply the EM algorithm for the maximum likelihood
(ML) estimation of the considered model’s parameters. The representation (2.2) makes it
convenient to ﬁnd the main characteristics of a NIG distributed random variable X, namely,
we have

EX = µ + δ

and Var(X) = δ

β
γ

α2
γ3 .

Moreover, the skewness and kurtosis of X is given by

Skewness =

3β
α√δγ

Kurtosis =

3(1 + 4β2/α2)
δγ

.

For β = 0, the NIG distribution is symmetric. Moreover, it is leptokurtic if δγ < 1 while
it is platykurtic in case δγ > 1. Note that a leptokurtic NIG distribution is characterized
by larger number of outliers than we have for normal distribution and thus, it is a common
tool for ﬁnancial data description.
NIG autoregressive model of order p: Now, we can deﬁne the AR(p) univariate sta-
tionary time-series

Z with NIG innovations

Yt

, t

{

}

∈

Yt =

p
(cid:88)

i=1

ρiYt−i + εt = ρT Yt−1 + εt,

(2.4)

(cid:15)t

· · ·

, ρp)T is a p-dimensional column vector, Yt−1 = (Yt−1, Yt−2,

, Yt−p)T
where ρ = (ρ1, ρ2,
Z are i.i.d. innovations distributed as NIG(α, β, µ, δ).
is a vector of p lag terms and
is a stationary one if and only if the modulus of all the roots of the
The process
Yt
{
ρpzp) are greater than one. In this article, we
characteristic polynomial (1
assume that the error term follows a symmetric NIG distribution with mean 0 i.e. µ = β = 0.
Using properties of NIG distribution (see Appendix A), the conditional distribution of Yt
given ρ, α, β, µ, δ and the preceding data

, Y1)T is given by

t−1 = (Yt−1, Yt−2,

∈
ρ2z2

−· · ·−

, t
}

ρ1z

· · ·

−

−

{

}

F

· · ·

p(Yt

ρ, α, β, µ, δ,
|

t−1) = f (yt; α, β, µ + ρT yt−1, δ),

F
(2.1) and yt−1 is the realization of Yt−1. We have
ε = Var(εt) = δα2/γ3 and γj =

j=1 ρjγj, where σ2

1 (see Appendix B).

where f (
) is the pdf given in Eq.
·
E[Yt] = E[εt] = 0 and Var[Yt] = σ2
E[YtYt−j] = ρ1γj−1 + ρ2γj−2 +

ε + (cid:80)p
+ ρpγj−p, j

· · ·

≥
4

3. Parameter estimation using EM algorithm

In this section, we provide a step-by-step procedure to estimate the parameters of the
model proposed in Eq. (2.4). The procedure is based on EM algorithm. In this paper, we
provide estimates of all parameters of the introduced AR(p) with NIG distributed innova-
tions. It is worth to mention that EM is a general iterative algorithm for model parameter
estimation by maximizing the likelihood function in the presence of missing or hidden data.
The EM algorithm was introduced in [41] and it is considered as an alternative to numerical
It is popularly used in estimating the parame-
optimization of the likelihood function.
ters of Gaussian mixture models (GMMs), estimating hidden Markov models (HMMs) and
model-based data clustering algorithms. Some extensions of EM include the expectation
conditional maximization (ECM) algorithm [42] and expectation conditional maximization
either (ECME) algorithm [43]. For a detailed discussion on the theory of EM algorithm and
its extensions we refer the readers to [44]. The EM algorithm iterates between two steps,
namely the expectation step (E-step) and the maximization step (M-step). In our case, the
observed data X is assumed to be from NIG(α, β, µ, δ) and the unobserved data G follows
IG((cid:112)
β2, δ). The E-step computes the expectation of the complete data log-likelihood
with respect to the conditional distribution of the unobserved or hidden data, given the
observations and the current estimates of the parameters. Further, in the M-step, a new
estimate for the parameters is computed which maximize the complete data log-likelihood
computed in the E-step. We ﬁnd the conditional expectation of log-likelihood of complete
data (X, G) with respect to the conditional distribution of G given X. For θ = (α, β, µ, δ, ρT )
we ﬁnd

α2

−

Q(θ

X, θ(k)],
θ)
G|X,θ(k)[log f (X, G
|
|
in the E-step where θ(k) represents the estimates of the parameter vector at k-th itera-
tion. Further, in the M-step, we compute the parameters by maximizing the expected
log-likelihood of complete data found in the E-step such that

θ(k)) = E
|

θ(k+1) = argmax

Q(θ

θ

θ(k)).
|

The algorithm is proven to be numerically stable [44]. Also, as a consequence of Jensen’s
inequality, log-likelihood function at the updated parameters θ(k+1) will not be less than that
at the current values θ(k). Although there is always a concern that the algorithm might get
stuck at local extrema, but it can be handled by starting from diﬀerent initial values and
comparing the solutions. In next proposition, we provide the estimates of the parameters of
the model deﬁned in eq. (2.4) using EM algorithm.

Proposition 3.1. Consider the AR(p) time-series model given in Eq. (2.4) where error
terms follow NIG(α, β, µ, δ). The maximum likelihood estimates of the model parameters
using EM algorithm are as follows

ˆρ =

(cid:32) n

(cid:88)

t=1

(cid:33)−1 n

(cid:88)

wtYtYT

t−1

t=1

5

(wtyt

µwt

−

−

β) Yt−1,

(3.5)

n
(cid:88)

t=1

(cid:15)twt

nβ

−

,

ˆµ =

ˆβ =

ˆδ =

n ¯wt

n
(cid:88)

(wt(cid:15)t)

t=1

n(1
−
(cid:114) ¯s
(¯s ¯w

−
ρT Yt−1, ¯(cid:15)t = 1
n
(cid:80)n
t=1 wt.

(cid:80)n

where (cid:15)t = yt
E
G|ε,θ(k)(g−1

t

(cid:15)t, θ(k)) and ¯w = 1
n
|
Proof. See Appendix C.

−

n ¯wt ¯(cid:15)t

−

¯st ¯wt)

, ˆγ =

1)

(3.6)

,

δ
¯s

, and ˆα = (γ2 + β2)1/2,

t=1 (cid:15)t, st = E

G|ε,θ(k)(gt

(cid:15)t, θ(k)), ¯s = 1
n
|

(cid:80)n

t=1 st, wt =

4. Simulation study and applications

In this section, we illustrate the performance of the proposed model and the introduced
estimation technique using simulated data sets and real time series of NASDAQ stock ex-
change data.

4.1. Simulation study

We discuss the estimation procedure for AR(2) and AR(1) models. The model (2.4)
is simulated in two steps. In the ﬁrst step, the NIG innovations are simulated using the
normal variance-mean mixture form (2.2). For NIG random numbers, standard normal and
IG random numbers are required. The algorithm mentioned in [45] is used to generate iid
, N using the following steps:
IG distributed random numbers Gi

IG(µ1, λ1), i = 1, 2,

∼
Step 1: Generate standard normal variate Z and set Y = Z 2.

· · ·

Step 2: Set X1 = µ1 + µ2
1Y
2λ1 −

µ1
2λ1

(cid:112)4µ1λ1Y + µ2

1Y 2.

Step 3: Generate uniform random variate U [0, 1].

Step 4: If U <= µ1

µ1+X1

, then G = X1; else G = µ2
1
X1

.

Note that the substitutions for parameters as µ1 = δ/γ and λ1 = δ2 are required in the
above algorithm because the pdf taken in [45] is diﬀerent from the form given in (2.3).
Again we simulate a standard normal vector of size N and use (2.2) with simulated IG
random numbers to obtain the NIG random numbers of size N . In step 2, the simulated
NIG innovations and the relation given in (2.4) are used to generate AR(p) simulated series.
Case 1: In the simulation study, ﬁrst we analyze the AR(2) model with NIG innovations.
In the analysis we used 1000 trajectories of length N = 1000 each. The used parameters
6

(a) The time series data plot.

(b) Scatter plot of innovation term.

Figure 1: The exemplary time series of length N = 1000 (left panel) and the corresponding innovation term
(right panel) of the AR(2) model with NIG distribution. The parameters of the model are: ρ1 = 0.5, ρ2 =
0.3, α = 1, β = 0, µ = 0, and δ = 2.

of the model are: ρ1 = 0.5 and ρ2 = 0.3 while the residuals were generated from NIG
distribution with α = 1, β = 0, µ = 0 and δ = 2. The exemplary time series data
plot and scatter plot of innovation terms are shown in Fig. 1. Now, for each simulated
trajectory, we apply the estimation algorithm presented in the previous section. For EM
algorithm several stopping criteria could be used. One of the examples is the criterion based
on change in the log-likelihood function which utilizes the relative change in the parameters’
values. We terminate the algorithm when the following criterion for the relative change in
the parameters’ values is satisﬁed (this criterion is commonly used in literature)

max


(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)


α(k)

α(k+1)

−
α(k)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
,
(cid:12)
(cid:12)

δ(k)

δ(k+1)

−
δ(k)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

ρ(k)

ρ(k+1)

−
ρ(k)






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 10−4.

(4.7)

The parameters’ estimates obtained from the simulated data are shown in the boxplot in Fig.
2. Moreover, we compared the estimation results with the classical YW algorithm and CLS
method. We remind that the YW algorithm is based on the YW equations calculated for the
considered model, and utilizes the empirical autocovariance function for the analyzed data.
More details of YW algorithm for autoregressive models can be found, for instance in [46].
We remind, the CLS method estimates the model parameters for dependent observations by
minimizing the sum of squares of deviations about the conditional expectation.

Fig. 2(a) and Fig. 2(b) represent the estimates of the model parameters ρ1 and ρ2
using YW, CLS and EM methods, respectively. Furthermore, using the estimated ρ1 and
ρ2 parameters with YW and CLS methods the residuals or innovation terms are obtained
and then again EM algorithm is used to estimate the remaining α and δ parameters which
are plotted in Fig. 2(c) and 2(d). Moreover, the estimates for α and δ using directly EM
algorithm given in (3.6) are also plotted in Fig. 2(c) and 2(d). From boxplots presented
in Fig. 2 we observe that the estimates of ρ1 and ρ2 parameters using the EM algorithm
7

02004006008001000t−8−6−4−20246Yt02004006008001000t−4−2024ǫthave less variance in comparison to the YW and CLS algorithms. Moreover, for α and δ
parameters, we see that the means of the estimates for the three presented methods are
close to the true values, but the range of outliers for the EM algorithm is comparatively
less. Therefore, we can infer that EM algorithm performs better (in comparison of other
considered algorithms) in the parameter estimation of AR(p) model with NIG innovations.

(a) Boxplots for ρ1 estimates.

(b) Boxplots for ρ2 estimates.

(c) Boxplots for δ estimates.

(d) Boxplots for α estimates.

Figure 2: Boxplots of the estimates of the AR(2) model’s parameters with theoretical values: ρ1 = 0.5, ρ2 =
0.3, δ = 2 and α = 1 represented with blue dotted lines. The boxplots are created using 1000 trajectories
each of length 1000.

Case 2: As the second example, we analyze the AR(1) model with NIG innovations. Here we
examine the trajectories of 579 data points. The same number of data points are examined
in the real data analysis demonstrated in the next subsection. This exemplary model is
discussed to verify the results for the real data. The simulated errors follow from NIG
distribution with parameter α = 0.0087, β = 0, µ = 0 and δ = 70.3882 while the model’s
In Fig. 3, we present the exemplary simulated trajectory and
parameter is ρ = 0.9610.
the corresponding innovation terms. Similarly as in Case 1, the introduced EM algorithm
was applied to the simulated data with the stopping criteria based on the relative change
in the parameter values deﬁned in Eq. (4.7). The boxplots of the estimated parameters for

8

YWCLSEM0.4000.4250.4500.4750.5000.5250.5500.5750.600YWCLSEM0.2000.2250.2500.2750.3000.3250.3500.3750.400YWCLSEM1.52.02.53.03.5YWCLSEM0.81.01.21.41.61.82.0(a) Time series data plot

(b) Scatter plot of innovation terms

Figure 3: The exemplary time series plot of the ﬁrst trajectory of length N = 579 from AR(1) model
(left panel) with the corresponding scatter plot of innovation terms NIG distribution (right panel). The
parameters of the model are ρ = 0.961, α = 0.0087, β = 0, µ = 0 and δ = 70.3882.

1000 trajectories each of length 579 are shown in Fig. 4. Similar as in the previous case, we
compare the results for EM, YW and CLS algorithms.

From Fig. 4 one can observe that although the estimate of ρ has more variance compared
to YW and CLS methods, but the estimates δ and α have less variance and the spread of
outliers is also slightly less. The means of the estimated parameters from 1000 trajectories
of length N = 579 using EM algorithm are ˆρ = 0.9572, ˆδ = 71.8647 and ˆα = 0.0091. We
can conclude that the EM algorithm, also in this case, gives the better parameters’ estimates
for the considered model.

4.2. Real data applications

In this part, the considered AR(p) model with NIG distribution is applied to the NAS-
It covers the
DAQ stock market index data, which is available on Yahoo ﬁnance [47].
historical prices and volume of all stocks listed on NASDAQ stock exchange from the period
March 04, 2010 to March 03, 2020. The data consists of 2517 data points with features
having open price, closing price, highest value, lowest value, adjusted closing price and vol-
ume of stocks for each working day end-of-the-day values. We choose the end-of-the-day
adjusted closing price as a univariate time series for the analysis purpose. The innovation
terms of time series data is assumed to follow NIG distribution as the general one. In Fig.
5 we represent the adjusted closing price of NASDAQ index. Observe that the original time
series data has an increasing trend. Moreover, one can easily observe that the data exhibit
non-homogeneous behavior. Thus, before further analysis the analyzed time series should be
segmented in order to obtain the homogeneous parts. To divide the vector of observations
into homogeneous parts, we applied the segmentation algorithm presented in [48], where
authors proposed to use the statistics deﬁned as the cumulative sum of squares of the data.
Finally, the segmentation algorithm is based on the speciﬁc behavior of the used statistics
when the structure change point exists in the analyzed time series. More precisely, in [48]

9

0100200300400500t−600−400−2000200400600800Yt0100200300400500t−2000200400600ǫt(a) Boxplot for ρ estimate.

(b) Boxplot for δ estimate.

(c) Boxplot for α estimate.

Figure 4: Boxplots of the estimates of the AR(1) model’s parameters with theoretical values: ρ = 0.9610, δ =
70.3883 and α = 0.00872 represented with blue dotted lines. The boxplots are created using 1000 trajectories
each of length 579.

it was shown that the cumulative sum of squares is a piece-wise linear function when the
variance of the data changes. Because in the considered time series we observe the non-
stationary behavior resulting from the existence of the deterministic trend, thus, to ﬁnd the
structure break point, we applied the segmentation algorithm for their logarithmic returns.
Finally, the algorithm indicates that the data needs to be divided into two segments, the
ﬁrst 1937 observations are considered as data 1 and rest all observations as - data 2.

The trends for both data sets were removed by using the degree 6 polynomial detrending.
The trend was ﬁtted by using the least squares method. The original data sets with the ﬁtted
polynomials are shown in Fig. 6. Next, for data 1 and data 2 we analyze the detrending time
series and for each of them we use the partial autocorrelation function (PACF) to recognize
the proper order of AR model. It is worth mentioning the PACF is a common tool to ﬁnd
the optimal order of the autoregressive models [46]. We select the best order that is equal
to the lag corresponding to the largest PACF value (except a lag equal to zero). We use the

10

YWCLSEM0.880.900.920.940.960.98YWCLSEM60708090100YWCLSEM0.0060.0080.0100.0120.0140.0160.018Figure 5: The adjusted closing price (in$) of NASDAQ index from the period March 04, 2010 to March 03,
2020 with 2517 data points.

(a) Data 1

(b) Data 2

Figure 6: The segmented data 1 (left panel) and data 2 (right panel) together with the ﬁtted polynomials.

PACF plots to determine the components of AR(p) model. Fig. 7 shows the stationary data
(after removing the trend) and corresponding PACF plots indicating the optimal model -
AR(1). After above-described pre-processing steps, the EM algorithm is used to estimate
the model’s parameters. In the proposed model, µ = 0 and β = 0 parameters are kept ﬁxed.
The estimated values of parameters for data 1 and data 2 are summarised in Table 1.

ˆρ
0.9809
0.9610

ˆδ
34.5837
70.3883

ˆα
0.0226
0.0087

Data 1
Data 2

Table 1: Estimated parameters of data 1 and data 2 using EM algorithm.

11

20102011201220132014201520162017201820192020Year2000300040005000600070008000900010000Adjustedclosingprice02505007501000125015001750t200030004000500060007000YtData1Polynomialtrendofdegree60100200300400500t6500700075008000850090009500YtData2Polynomialtrendofdegree6(a) Stationary data 1.

(b) PACF of data 1.

(c) Stationary data 2.

(d) PACF of data 2.

Figure 7: The time-series plot of stationary data 1 and data 2 (after removing the trend) - left panel and
corresponding PACF plot - right panel.

As the ﬁnal step, we analyze the innovation terms corresponding to data 1 and data 2 to
conﬁrm they can be modeled by using NIG distribution. The Kolmogorov-Smirnov (KS)
test is used to check the normality of the residuals corresponding to data 1 and data 2.
The KS test is a non-parametric test which is used as goodness-of-ﬁt test by comparing
the distribution of samples with the given probability distribution (one-sample KS test) or
by comparing the empirical distribution function of two samples (2-sample KS test) [49].
First, we use the one-sample KS test to reject the hypothesis of normal distribution of the
residuals. The p-value of the KS test is 0 for both cases, indicating that the null hypothesis
(normal distribution) is rejected for both series. Thus, we applied two-sample KS test
for both innovation terms with the null hypothesis of NIG distribution. The tested NIG
distributions have the following parameters: µ = 0, β = 0, δ = 34.5 and α = 0.02 - for
residuals corresponding to data 1; and µ = 0, β = 0, δ = 70.5 and α = 0.008 - for the
residuals corresponding to data 2. The p-values for 2-sample KS test are 0.565 and 0.378 for
data 1 and data 2, respectively, which indicates that there is no evidence to reject the null

12

02505007501000125015001750t−800−600−400−2000200400DetrendeddataYt051015202530Lags0.00.20.40.60.81.0PACF0100200300400500t−1250−1000−750−500−2500250500DetrendeddataYt051015202530Lags0.00.20.40.60.81.0PACFhypothesis. Therefore, we assume that both the residual series follow the same distribution,
implying that data 1 follows NIG(α = 0.02, β = 0, µ = 0, δ = 34.5) and data 2 has
NIG(α = 0.008, β = 0, µ = 0, δ = 70.5).

(a) QQ plot between innovations of data 1 and normal
distribution.

(b) QQ plot between innovations of data 1 and NIG
distribution.

Figure 8: QQ plots of innovation terms of data 1 compared with (a) normal distribution and (b) NIG(α =
0.02, β = 0, µ = 0, δ = 34.5) distribution.

(a) QQ plot between innovations of data 2 and normal
distribution.

(b) QQ plot between innovations of data 2 and NIG
distribution.

Figure 9: QQ plots of innovation terms of data 2 compared with (a) normal distribution and (b) NIG(α =
0.008, β = 0, µ = 0, δ = 70.5) distribution.

To conﬁrm that NIG distributions (with ﬁtted parameters) are acceptable for the residual
series, in Fig. 8 and Fig. 9 we demonstrate the QQ plot for the residuals of AR(1) models for
data 1 and data 2 and the simulated data from normal (left panels) and corresponding NIG
distributions (right panels). Observe that the tail of both data 1 and data 2 deviates from
the red line on the left panels, which indicates that the data does not follow the distribution.

13

−4−20246Normal quantiles−4−20246Sample quantiles−200−1000100200NIG quantiles−200−1000100200Sample quantiles−4−202Normal quantiles−4−202Sample quantiles−400−2000200400NIG quantiles−400−2000200400Sample quantiles(a) KDE plot for data 1

(b) KDE plot for data 2

Figure 10: Kernel density estimation plots for comparing the innovation terms of data 1 with NIG(α =
0.1170, σ2 = 1513.0754) (left
0.02, β = 0, µ = 0, δ = 34.5) distribution and normal distribution N (µ =
1.6795, σ2 =
panel) and data 2 with NIG(α = 0.008, β = 0, µ = 0, δ = 70.5) and normal distribution N (µ =
89.2669) (right panel).

−

−

(a) Data 1

(b) Data 2

Figure 11: The adjusted closing price of NASDAQ index for both segments (blue lines) along with the
quantile lines of 10%, 20%, ..., 90% constructed base on the ﬁtted AR(1) models with NIG distribution with
added trends.

The correspondence with NIG distribution is also demonstrated in Fig. 10, where the
kernel density estimation (KDE) plot is presented for the residual series and compared with
the pdf of the normal and corresponding NIG distributions. A brief description of KDE
method is given in Appendix D. As one can see, the KDE plots clearly indicate the NIG
distribution is the appropriate one for the residual series.

Finally, to conﬁrm that the ﬁtted models are appropriate for data 1 and data 2, we
constructed the quantile plots of the data simulated from the models for which the removed

14

−300−200−1000100200Residuals0.0000.0010.0020.0030.004DensityResiduals-data1NormalNIG−400−2000200400Residuals0.000000.000250.000500.000750.001000.001250.001500.001750.00200DensityResiduals-data2NormalNIG02505007501000125015001750t200030004000500060007000AdjustedclosingpriceYt0100200300400500t6000650070007500800085009000950010000AdjustedclosingpriceYtpolynomials were added. The quantile lines are constructed based on 1000 simulated tra-
jectories with the same lengths as data 1 and data 2. In Fig. 11 we present the constructed
quantile lines on the levels 10%, 20%, ..., 90% and the adjusted closing price of the NASDAQ
index.

The presented results for the real data indicate that the AR(1) model with innovation
terms corresponding to NIG distribution can be useful for the ﬁnancial data with visible
extreme values.

5. Conclusions

The heavy-tailed and semi-heavy-tailed distributions are at the heart of the ﬁnancial
time-series modeling. NIG is a semi-heavy tailed distribution which has tails heavier than
the normal and lighter than the power law tails. In this article, the AR models with NIG
distributed innovations are discussed. We have demonstrated the main properties of the
analyzed systems. The main part is devoted to the new estimation algorithm for the con-
sidered models’ parameters. The technique incorporates the EM algorithm widely used in
the time series analysis. The eﬀectiveness of the proposed algorithm is demonstrated for the
simulated data. It is shown that the new technique outperforms the classical approaches
based on the YW and CLS algorithms. Finally, we have demonstrated that an AR(1) model
with NIG innovations explain well the NASDAQ stock market index data for the period
March 04, 2010 to March 03, 2020. We believe that the discussed model is universal and
can be used to describe various real-life time series ranging from ﬁnance and economics to
natural hazards, ecology, and environmental data.

Acknowledgements: Monika S. Dhull would like to thank the Ministry of Education
(MoE), Government of India, for supporting her PhD research work.
The work of A.W. was supported by National Center of Science under Opus Grant No.
2020/37/B/HS4/00120 “Market risk model identiﬁcation and validation using novel statis-
tical, probabilistic, and machine learning tools”. A.K. would like to express his gratitude to
Science and Engineering Research Board (SERB), India, for the ﬁnancial support under the
MATRICS research grant MTR/2019/000286.

References

[1] H. L. Nelson(Jr.) and C. Granger, “Experience with using the Box-Cox transformation when forecasting

economic time series,” J. Econom., vol. 10, no. 1, pp. 57–69, 1979.

[2] S. T. Rachev, Handbook of Heavy Tailed Distributions in Finance: Hand-books in Finance. Amsterdam,

The Netherlands: Elsevier, 2003.

[3] R. Cont and P. Tankov, Financial Modeling With Jump Processes. Chapman & Hall, CRC Press,

London, UK, 2004.

[4] E. Omeya, S. V. Gulcka, and R. Vesilo, “Semi-heavy tails,” Lith. Math. J., vol. 58, pp. 480–499, 2018.
[5] C. H. Sim, “First-order autoregressive models for gamma and exponential processes,” J. Appl. Prob.,

vol. 27, pp. 325–332, 1990.

[6] M. L. Tiku, W. K. Wong, D. C. Vaughan, and G. Bian, “Time series models in non-normal situations:

Symmetric innovations,” J. Time Ser. Anal., vol. 21, pp. 571–596, 2000.

15

[7] B. Tarami and M. Pourahmadi, “Multi-variate t autoregressions: Innovations, prediction variances and

exact likelihood equations,” J. Time Ser. Anal., vol. 24, pp. 739–754, 2003.

[8] J. Christmas and R. Everson, “Robust autoregression: Student-t innovations using variational Bayes,”

IEEE Trans. Signal Process., vol. 59, pp. 48–57, 2011.

[9] U. C. Nduka, “EM-based algorithms for autoregressive models with t-distributed innovations,” Com-

mun. Statist. Simul. Comput., vol. 47, pp. 206–228, 2018.

[10] C. C. Heyde and N. N. Leonenko, “Student processes,” Adv. Appl. Probab., vol. 37, pp. 342–365, 2005.
[11] O. Barndorﬀ-Nielsen, “Exponentially decreasing distributions for the logarithm of particle size,” Proc.

Roy. Soc. London Ser. A., vol. 353, pp. 401–409, 1977.

[12] L. Alfonso, R. Mansilla, and C. A. Terrero-Escalante, “On the scaling of the distribution of daily price
ﬂuctuations in the Mexican ﬁnancial market index,” Physica A, vol. 391, no. 10, pp. 2990–2996, 2012.
[13] K. Burnecki, J. Gajda, and G. Sikora, “Stability and lack of memory of the returns of the Hang Seng

index,” Physica A, vol. 390, no. 18–19, pp. 3136–3146, 2011.

[14] O. Barndorﬀ-Nielsen, “Normal inverse Gaussian distributions and stochastic volatility modelling,”

Scand. J. Stat., vol. 24, pp. 1–13, 1977.

[15] R. Weron, Computationally intensive value at risk calculations. Springer, Berlin, 2004.
[16] R. Weron, K. Weron, and A. Weron, “A conditionally exponential decay approach to scaling in ﬁnance,”

Physica A, vol. 264, no. 3-4, pp. 551–561, 1999.

[17] A. Kalemanova, B. Schmid, and R. Werner, “The normal inverse Gaussian distribution for synthetic

CDO pricing,” J. Deriv., vol. 14, no. 3, pp. 80–94, 2007.

[18] F. E. Benth, M. Groth, and P. C. Kettler, “A quasi-Monte Carlo algorithm for the normal inverse
Gaussian distribution and valuation of ﬁnancial derivatives,” Int. J. Theor. Appl. Finance, vol. 9,
no. 6, pp. 843–867, 2006.

[19] O. Barndorﬀ-Nielsen, T. Mikosch, and S. I. Resnick, “L´evy processes: Theory and applications,” Proc.

Roy. Soc. London Ser. A., 2013.

[20] Y.-P. Chang, M.-C. Hung, H. Liu, and J.-F. Jan, “Testing symmetry of a NIG distribution,” Commun.

Stat. - Simul. Comput.®, vol. 34, no. 4, pp. 851–862, 2005.

[21] A. Hanssen and T. A. Oigard, “The normal inverse Gaussian distribution: A versatile model for heavy-
tailed stochastic processes,” in 2001 Proc. - ICASSP IEEE Int. Conf. Acoust. Speech Signal Process.
(Cat. No. 01CH37221), vol. 6, pp. 3985–3988, IEEE, 2001.

[22] D. Karlis, “An EM type algorithm for maximum likelihood estimation of the normal-inverse Gaussian

distribution,” Statist. Probab. Lett., vol. 57, pp. 43–52, 2002.

[23] D. Karlis and J. Lillest¨ol, “Bayesian estimation of NIG models via Markov chain Monte Carlo methods,”

Appl. Stoch. Models Bus. Ind., vol. 20, no. 4, pp. 323–338, 2004.

[24] H. Sadreazami, M. O. Ahmad, and M. N. S. Swamy, “Multiplicative watermark decoder in contourlet
domain using the normal inverse Gaussian distribution,” IEEE Trans. Multimed., vol. 18, no. 2, pp. 196–
207, 2016.

[25] F. E. Benth and J. urat˙e ˆSaltyt˙e Benth, “The normal inverse Gaussian distribution and spot price

modelling in energy markets,” Int. J. Theor. Appl. Finance, vol. 7, no. 2, pp. 177–192, 2004.

[26] Q. Z. Zou, W. Y. Dou, and J. L. Li, “Study on distribution of oil price return based on NIG distribution,”

in 2010 International Conference on Management and Service Science, pp. 1–5, 2010.

[27] X. Zhang and X. Jing, “Image denoising in contourlet domain based on a normal inverse Gaussian

prior,” Digit. Signal Process., vol. 20, no. 5, pp. 1439–1446, 2010.

[28] Y.-P. Chang, M.-C. Hung, S.-F. Wang, and C.-T. Yu, “An EM algorithm for multivariate NIG distri-

bution and its application to value-at-risk,” Int. J. of Inf., vol. 21, no. 3, pp. 265–283, 2010.

[29] T. A. Oigard, A. Hanssen, R. E. Hansen, and F. Godtliebsen, “EM-estimation and modeling of heavy-
tailed processes with the multivariate normal inverse Gaussian distribution,” Signal Process., vol. 85,
pp. 1655–1673, 2005.

[30] K. Aas, I. H. Haﬀ, and X. K. Dimakos, “Risk estimation using the multivariate normal inverse Gaussian

distribution,” J. Risk, vol. 8, no. 2, pp. 39–60, 2005/2006.

[31] T. H. Rydberg, “The normal inverse Gaussian L´evy process: simulation and approximation,” Commun.

16

Stat. Stoch. Model., vol. 13, no. 4, pp. 887–910, 1997.

[32] H. Albrecher and M. Predota, “On Asian option pricing for NIG L´evy processes,” J. Comput. Appl.

Math., vol. 172, no. 1, pp. 153–168, 2004.

[33] A. Wy(cid:32)loma´nska, “How to identify the proper model,” Acta Phys. Pol. B, vol. 43, no. 5, pp. 1241–1253,

2012.

[34] J. Gajda, G. Bartnicki, and K. Burnecki, “Modeling of water usage by means of ARFIMA–GARCH

processes,” Physica A, vol. 512, pp. 644–657, 2018.

[35] R. Kili¸c, “Conditional volatility and distribution of exchange rates: GARCH and FIGARCH models

with NIG distribution,” Stud. Nonlinear Dyn. Econom., vol. 11, no. 3, 2007.

[36] L. Forsberg and T. Bollerslev, “Bridging the gap between the distribution of realized (ECU) volatility
and ARCH modelling (of the Euro): the GARCH-NIG model,” J. Appl. Econom., vol. 17, no. 5,
pp. 535–548, 2002.

[37] M. B. Jensen and A. Lunde, “The NIG-S&ARCH model: a fat-tailed, stochastic, and autoregressive

conditional heteroskedastic volatility model,” Econom. J., vol. 4, no. 2, p. 319–342, 2001.

[38] J. Weiss, P. Bernardara, M. Andreewsky, and M. Benoit, “Seasonal autoregressive modeling of a skew

storm surge series,” Ocean Model(Oxf ), vol. 47, pp. 41–54, 2012.

[39] A. Wilhelmsson, “Value at risk with time varying variance, skewness and kurtosis—the NIG-ACD

model,” Econom. J., vol. 12, no. 1, pp. 82–104, 2009.

[40] B. Jorgensen, “Statistical properties of the generalized inverse Gaussian distribution. Lecture Notes in

Statistics,” Springer-Verlag, New York, vol. 9, 1982.

[41] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood from incomplete data via EM

algorithm,” J. R. Stat. Soc. Ser. B 39, vol. 39, pp. 1–38, 1977.

[42] X. L. Meng and D. B. Rubin, “Maximum likelihood estimation via the ECM algorithm: A general

framework,” Biometrika, vol. 80, pp. 267–278, 1993.

[43] C. H. Liu and D. B. Rubin, “ML estimation of t-distribution using EM and its extensions, ECM and

ECME,” Statist. Sinica., vol. 5, pp. 19–39, 1995.

[44] G. J. McLachlan and T. Krishnan, The EM Algorithm and Extensions, 2ed. John Wiley & Sons, 2007.
[45] L. Devroye, Non-Uniform Random Variate Generation, 1ed. Springer, New York, 1986.
[46] P. J. Brockwell and R. A. Davis, Introduction to Time Series and Forecasting. Springer, 2016.
[47] “Historical data of NASDAQ stock exchange available on Yahoo Finance.” https://finance.yahoo.

com/quote/%5EIXIC/history?period1=1267660800&period2=1583193600&interval=1d&filter=
history&frequency=1d&includeAdjustedClose=true.

[48] J. Gajda, G. Sikora, and A. Wy(cid:32)loma´nska, “Regime variance testing- A quantile approach,” Acta Phys.

Pol. B, vol. 44, no. 5, pp. 1015–1035, 2013.

[49] F. J. Massey(Jr.), “The Kolmogorov-Smirnov test for goodness of ﬁt,” J. Am. Stat. Assoc., vol. 46,

pp. 68–78, 1951.

[50] J. Lillestol, “Risk analysis and the NIG distribution,” J. Risk, vol. 4, p. 41–55, 2000.
[51] A. Elgammal, R. Duraiswami, D. Harwood, and L. Davis, “Background and foreground modeling using
non-parametric kernel density estimation for visual surveillance,” Proceedings of the IEEE, vol. 90,
no. 7, pp. 1151–1163, 2002.

Appendix A. Additional properties of NIG distribution

The following properties of NIG distribution are presented in [50]. Let X

(α, β, µ, δ), then following holds.

(a) The moment generating function of X is

NIG

∼

MX(u) = eµu+δ(√α2−β2−√α2−(β+u)2

.

(b) If X

∼

NIG (α, β, µ, δ), then X + c

∼

NIG (α, β, µ + c, δ),

c

17

R

∈

(c) If X
(d) If X1

∼

NIG (α, β, µ, δ), then cX

∼
NIG (α, β, µ1, δ1) and X2

∼
X1 + X2
NIG (α, β, µ, δ), then X−µ

NIG (α, β, µ1 + µ2, δ1 + δ2).

∼

∼

(e) If X

∼

NIG (αδ, βδ, 0, 1).

δ ∼

NIG (α/c, β/c, cµ, cδ), c > 0.

NIG (α, β, µ2, δ2) are independent then the sum

Appendix B. Mean and variance of AR(p) model with NIG innovations

We have Yt = ρ1Yt−1 + ρ2Yt−2 +

+ ρpYt−p + (cid:15)t. Assuming stationarity, it follows that

EYt =

1

ρ1

· · ·
E((cid:15)t)
ρ2

=

1

ρp

µ + δβ/γ
ρ2

−

− · · · −

ρ1

.

ρp

−
−
Further, for µ = β = 0, we have E[Yt] = E[(cid:15)t] = 0 and

− · · · −

−

Y 2
t = ρ1Yt−1Yt + ρ2Yt−2Yt +
t ] = ρ1E[Yt−1Yt] + ρ2E[Yt−2Yt] +
E[Y 2
= ρ1E[Yt−1Yt] + ρ2E[Yt−2Yt] +

· · ·

+ ρpYt−pYt + (cid:15)tYt.

+ ρpE[Yt−pYt] + E[(cid:15)tYt]
+ ρpE[Yt−pYt] + E[(cid:15)2
t ].

· · ·
· · ·

Var[Yt] = σ2

(cid:15) +

p
(cid:88)

j=1

ρjγj,

where σ2

(cid:15) = Var[(cid:15)t] and γj = E[YtYt−j]. Moreover,

γj = ρ1γj−1 + ρ2γj−2 +

+ ρpγj−p, j

1.

≥

· · ·

For p = 2, using (B.1) and (B.2), it is easy to show that

(B.1)

(B.2)

Var[Yt] =

1

ρ2

(1
−
ρ2
1 −

ρ2)σ2
(cid:15)
ρ2
2 −

,

1ρ2 + ρ3
ρ2
2

where σ2

−
(cid:15) = δα2/γ3. Again for p = 3, usiigarg (B.1) and (B.2), it follows

−

Var[Yt] =

ρ2

1

−

−

ρ1ρ3

ρ2
1 −

ρ2
2 −

−

2ρ2

3 −

ρ2
1ρ2

−

ρ2

(1
−
2ρ2
ρ2
3 −

ρ1ρ3

−
1ρ2
ρ2

3 −

3)σ2
ρ2
(cid:15)
−
4ρ1ρ2ρ3 + ρ2ρ2
ρ3
1ρ3

−

3 + ρ1ρ4

3 + ρ3

2 + ρ4

3 + ρ1ρ2

2ρ3

.

Appendix C. Proof of Proposition 3.1

Proof. For AR(p) model, let (εt, Gt), for t = 1, 2, ..., n denote the complete data. The observed data εt is
assumed to be from NIG(α, β, µ, δ) and the unobserved data Gt follows IG(γ, δ). We can write the innovation
terms as follows

εt = Yt

−

ρT Yt−1, for t = 1, 2, ..., n.

Note that ε

G = g
|

∼

N (µ + βg, g) and the conditional pdf is

f (ε = (cid:15)t

G = gt) =
|

1
√2πgt

exp

1
2gt

(yt

−

ρT yt−1

µ

−

−

(cid:19)

.

βgt)2

(cid:18)

−

18

Now, we need to estimate the unknown parameters θ = (α, β, µ, δ, ρT ). We ﬁnd the conditional expectation
of log-likelihood of unobserved/complete data (ε, G) with respect to the conditional distribution of G given ε.
Since the unobserved data is assumed to be from IG(γ, δ) therefore, the posterior distribution is generalised
inverse Gaussian (GIG) distribution i.e.,

The conditional ﬁrst moment and inverse ﬁrst moment are as follows:

ε, θ
G
|

GIG(

−

∼

(cid:112)

1, δ

φ((cid:15)), α).

E(G
(cid:15)) =
|

E(G−1

(cid:15)) =
|

δφ((cid:15))1/2
α

α
δφ((cid:15))1/2

,

K0(αδφ((cid:15))1/2)
K1(αδφ((cid:15))1/2)
K−2(αδφ((cid:15))1/2)
K−1(αδφ((cid:15))1/2)

.

These ﬁrst order moments will be used in calculating the conditional expectation of the log-likelihood
function. The complete data likelihood is given by

L(θ) =

=

n
(cid:89)

t=1
n
(cid:89)

t=1

f ((cid:15)t, gt) =

n
(cid:89)

t=1

fε|G((cid:15)t

gt)fG(gt)
|

(cid:32)

exp(δγ) exp

δ
2πg2
t

δ2
2gt −

−

γ2gt

2 −

g−1
t
2

µ)2

((cid:15)t

−

−

β2gt
2

+ β((cid:15)t

−

(cid:33)

µ)

.

The log likelihood function will be

l(θ) = n log(δ)

n log(2π) + nδγ

nβµ

−

−

−

γ2
2

n
(cid:88)

t=1

−

gt

−

1
2

n
(cid:88)

t=1

g−1
t

((cid:15)t

µ)2

−

β2
2

−

n
(cid:88)

2

log(gt)

δ2
2

n
(cid:88)

t=1

−

g−1
t

t=1
n
(cid:88)

gt + β

t=1

n
(cid:88)

t=1

(cid:15)t.

Now in E-step of EM algorithm, we need to compute the expected value of complete data log likelihood
known as Q(θ

θk), which is expressed as

|

Q(θ

θ(k)) = E
|

G|ε,θ(k)[log f (ε, G
|

θ)

ε, θ(k)] = E
|

= n log δ + nδγ

nβµ

−

−

n log(2π)

−

θ(k))]

G|ε,θ(k) [L(θ
n
(cid:88)

|
E(log gt

2

t=1

(cid:15)t, θ(k))
|

−

δ2
2

n
(cid:88)

t=1

wt

γ2
2

n
(cid:88)

t=1

−

st

−

β2
2

n
(cid:88)

t=1

st + β

n
(cid:88)

t=1

(yt

−

ρT Yt−1)

1
2

n
(cid:88)

t=1

−

(yt

−

ρT Yt−1

µ)2wt,

−

where, st = E

G|ε,θ(k)(gt

|

(cid:15)t, θ(k)) and wt = E

G|ε,θ(k) (g−1

t

(cid:15)t, θ(k)). Update the parameters by maximizing the

|

19

Q function using the following equations

∂Q
∂ρ

∂Q
∂µ

∂Q
∂β

∂Q
∂δ

∂Q
∂γ

=

n
(cid:88)

t=1

wt(yt

−

ρT Yt−1

µ)Y T

t−1 −

−

β

n
(cid:88)

t=1

Y T

t−1,

nβ +

=

−

nµ +

=

−

n
(cid:88)

t=1
n
(cid:88)

t=1

wt((cid:15)t

µ),

−

yt

β

−

n
(cid:88)

t=1

st

−

n
(cid:88)

t=1

ρT Yt−1,

= nγ +

n
δ −

δ

n
(cid:88)

t=1

wt,

= nδ

γ

−

n
(cid:88)

t=1

st.

Solving the above equations, we obtain the following estimates of the parameters

ˆρ =





n
(cid:88)

t=1



−1

wtYt−1Y T

t−1



n
(cid:88)

t=1

(wtyt

µwt

−

−

β)Yt−1

n
(cid:88)

(cid:15)twt

nβ +

t=1
n ¯wt

;

−

ˆµ =

;

(C.1)

n
(cid:88)

t=1

wt(cid:15)t

−

n ¯wt¯(cid:15)t

¯st ¯wt)

ˆβ =

ˆδ =

n(1
−
(cid:114) ¯s
(¯s ¯w

−

, ˆγ =

1)

δ
¯s

, and ˆα = (γ2 + β2)1/2,

where ¯s = 1
n

(cid:80)n

t=1 st, ¯w = 1
n

(cid:80)n

t=1 wt.

Appendix D. KDE method

The KDE method also known as Parzen–Rosenblatt window method, is a non-parametric approach to
ﬁnd the underlying probability distribution of data. It is a technique that lets one to create a smooth curve
given a set of data and one of the most famous method for density estimation. For a sample S =
having distribution function f (x) has the kernel density estimate (cid:98)f (x) deﬁned as [51]

i=1,2,...,N
}

xi
{

(cid:98)f (x) =

1
N

N
(cid:88)

n=1

Kσ(x

xi),

−

where Kσ is kernel function with bandwidth σ such that Kσ(t) = ( 1

σ )K( t

σ ).

20

