9
1
0
2

g
u
A
1
2

]

G
L
.
s
c
[

4
v
1
4
7
9
0
.
7
0
8
1
:
v
i
X
r
a

PADME: A Deep Learning-based Framework for Drug-Target
Interaction Prediction

Qingyuan Feng1, Evgenia Dueva2, Artem Cherkasov2,3,*, Martin Ester1,2,*

1 School of Computing Science, Simon Fraser University, 8888 University
Drive, Burnaby, BC, V5A1S6, Canada
2 Vancouver Prostate Centre, Vancouver, BC V6H3Z6, Canada
3 Department of Urologic Sciences, University of British Columbia,
Vancouver, BC V5Z1M9, Canada

* ester@sfu.ca; artc@interchange.ubc.ca

Abstract

In silico drug-target interaction (DTI) prediction is an important and challenging problem
in biomedical research with a huge potential benefit to the pharmaceutical industry
and patients. Most existing methods for DTI prediction including deep learning models
generally have binary endpoints, which could be an oversimplification of the problem,
and those methods are typically unable to handle cold-target problems, i.e., problems
involving target protein that never appeared in the training set. Towards this, we contrived
PADME (Protein And Drug Molecule interaction prEdiction), a framework based on
Deep Neural Networks, to predict real-valued interaction strength between compounds
and proteins without requiring feature engineering. PADME takes both compound and
protein information as inputs, so it is capable of solving cold-target (and cold-drug)
problems. To our knowledge, we are the first to combine Molecular Graph Convolution
(MGC) for compound featurization with protein descriptors for DTI prediction. We used
multiple cross-validation split schemes and evaluation metrics to measure the performance
of PADME on multiple datasets, including the ToxCast dataset, and PADME consistently
dominates baseline methods. The results of a case study, which predicts the binding affinity
between various compounds and androgen receptor (AR), suggest PADME’s potential in
drug development. The scalability of PADME is another advantage in the age of Big Data.

1 Introduction

Finding out the interaction strengths between compounds (candidate drugs) and target
proteins is of crucial importance in the drug development process. However, it is both
expensive and time-consuming to be done in wet lab experiments, while virtual screening
using computational (also called “in silico”) methods to predict the interactions between
compounds and target proteins can greatly accelerate the drug development process at a
significantly reduced cost. Indeed, machine learning models for drug-target interaction
(DTI) prediction are often used in computer-aided drug design [9].

1/29

 
 
 
 
 
 
Datasets used for training and evaluating machine learning models for DTI prediction of-
ten include compounds’ interaction strengths with enzymes, ion channels, nuclear receptors,
etc [54]. Traditionally, these datasets contain binary labels for the interaction of certain
drug-target pairs, with 1 indicating a known interaction. Recently, the community has also
explored the usage of datasets with real-valued interaction strength measurements [13, 33],
which include the Davis dataset [8] that uses the inhibition constant (Ki), the Metz
dataset [27] that uses the dissociation constant (Kd) and the KIBA dataset [45] whose
authors devised their own measurement index.

Existing traditional machine learning methods for predicting DTI can be roughly divided
into similarity-based and feature-based approaches, and most of them formulate the problem
as a classification problem. Similarity-based methods depend on the assumption that
compounds with similar structures should have similar effects. Feature-based methods
construct feature vectors as input, which are generated by combining descriptors of
compounds with descriptors of targets, and the feature vectors serve as inputs for algorithms
such as support vector machine (SVM) [13].

SimBoost [13] and KronRLS [33] are two state-of-the-art methods for DTI prediction.
Both of them have single outputs. KronRLS is based on Regularized Least Squares and
utilizes the similarity matrices for drugs and targets to get the parameter values. SimBoost
is a feature-based method, but in its feature construction, similarity matrices of the drugs
and those of targets are also involved. These methods can both predict continuous values
and binarized values. However, these methods either simply rely on similarities, or require
expert knowledge to define the relevant features of proteins and compounds, called “feature
engineering”. Additionally, they are often unable to model highly complex interactions
within compound molecules [25] and between the compounds and their target proteins.

Deep Neural Networks (DNN) promise to address these challenges.
Deep learning, the machine learning method based on DNN, has been enjoying ever-
rising popularity in the past few years. It has seen wide and exciting applications in
computer vision, speech recognition, natural language processing, reinforcement learning,
and drug-target interaction prediction. DNNs can automatically extract important features
from the input data, synthesize and integrate low-level features into high-level features,
and capture complicated nonlinear relationships in a dataset [19, 38]. Deep learning-based
DTI prediction has been shown to consistently outperform the existing methods and has
become the new “golden standard” [7, 22, 46].

The current deep learning approaches to drug-target interaction prediction can be
roughly categorized based on their neural network types and prediction endpoints. Simple
feedforward neural networks, Convolutional Neural Networks (CNN) and Recurrent Neural
Networks (RNN) have been adopted in various papers [20, 56]. To our knowledge, almost
all existing deep learning methods, except those that have 3D structural information as
input, treat the problem as a classification problem, most of which are binary, namely
active/inactive. Though there are deep learning models using 3D structural information
that yield good results in regression problems [11, 50], the requirement of 3D structural
information limits the applicability of a model since such information is not always available,
so we do not consider them in this paper. As deep learning for DTI is still in its infancy,
the current models have several disadvantages.

First, formulating the problem as a classification problem has some inadequacies:
obviously, the classification result depends on a predefined binarization threshold, which

2/29

introduces some arbitrariness into the data; some useful information is lost, for instance,
true-negative and missing values may not be discriminated in some chemical datasets [13,33].
On the other hand, if we formulate it as a regression problem, not only can we avoid the
problems above, but given the regression results, the real-valued outputs can be easily
converted to produce a ranking or classification. Some existing non-DNN methods formulate
the problem as a regression problem, in which the interaction strength between the drug
molecule and the target protein is a real number, serving as the regression target [13].
Common real-valued interaction strength metrics include Ki,Kd, etc.

The second problem is that most of the existing deep learning methods do not
incorporate the target protein information into the network, except very few recent works,
like Wen et al. [51] and Lenselink et al. [20]. As a result, the models are unable to solve
the “cold target” problem, i.e. to predict the drug-target interactions for target proteins
absent in the training dataset. In the field of proteochemometrics, this idea of combining
protein and compound information as the input to the model was widely used, though
most of them do not use DNN models [6, 34, 49, 52].

A recent model, DeepDTI [51], addressed the second problem by combining the protein
information with the compound feature vector. It uses the classical Extended-Connectivity
Fingerprint (ECFP) [36] for describing compounds, which relies on a fixed hashing function
and cannot adjust to specific problems at hand. DeepDTI concatenates ECFP and Protein
Sequence Composition (PSC) descriptors [4] (describing the target proteins’ sequence
information) to construct a feature vector, which is fed into a Deep Belief Network
(DBN) to predict a binary endpoint. DeepDTI outperformed the state-of-the-art methods
on a dataset extracted from DrugBank. Another slightly later work used feedforward
networks with the combined compound and protein feature vector as the input and binary
classification result as the output [20]. Their compound and protein feature vectors also
required lots of feature engineering and expert knowledge.

In this paper, we propose PADME (Protein And Drug Molecule interaction prEdiction),
a deep learning-based framework for predicting DTI, which can be roughly categorized into
the feature-based methods. PADME overcomes the limitations of the existing methods by
predicting real-valued interaction strengths instead of binary class labels, and, to address
the cold-start problems (drugs or targets that are absent from the training set but appear in
the test set), PADME utilizes a combination of drug and target protein features/fingerprints
as the input vector, where no feature engineering is required. The drug and target vectors
can be generated from SMILES representation and Amino Acid sequence, respectively,
without loss of information. Unlike DeepDTI which uses DBN, PADME uses a feedforward
network, mainly composed of ReLU layers, to connect the input vector to the output layer.
PADME adopts Molecular Graph Convolution (MGC) which is more flexible than ECFP,
because it learns the mapping function from molecular graph representations to feature
vectors [2, 10, 15] . Similar to DeepDTI, we used Protein Sequence Composition (PSC)
descriptor to represent the protein. To the best of our knowledge, this work is the first to
integrate MGC with protein descriptors for the DTI problem. In addition to the kinase
inhibitor datasets used by previous researchers, we also used the ToxCast dataset [47],
which has a much larger variety of proteins and could be another useful benchmarking
dataset for future researches of the same type.

We conducted computational experiments with multiple cross-validation settings and
evaluation metrics. The results demonstrated the superiority of PADME over baseline

3/29

methods across all experimental settings. Besides, PADME is more scalable than SimBoost
and KronRLS since it does not rely on computationally expensive similarity matrices and
can accommodate multiple outputs.As a case study, we also applied PADME to predict the
binding affinity between some compounds and the androgen receptor (AR). We examined
the top compounds among them and confirmed this prediction through literature research,
suggesting that the predictions of PADME have practical implications.

The subsequent sections of the paper are organized as follows. The Method section
will introduce the methods for compound featurization, protein featurization, and network
structure. The Experiments section will present the experiments conducted, introducing
the baseline methods, datasets used, experimental design, and the experimental results.
The Discussion section clarifies some implementation and design choices, and outlines
possible future directions to further this work. The last section concludes the whole paper.

2 Method

PADME is a deep learning-based DTI prediction model which uses the combined small-
molecule compound (candidate drug) and target protein feature vectors. We consider two
variants of PADME with either Molecular Graph Convolution (MGC) or ECFP [36] as the
compound featurization method. For the protein, we use Protein Sequence Composition
(PSC) descriptor [4]. In fact, PADME is compatible with all kinds of protein descriptors
and molecular featurization methods. The compound vector is concatenated with a target
protein vector to form the Combined Input Vector (CIV) for the neural network. PADME
predicts a real-valued interaction strength, i.e., it solves a DTI regression problem. The
structure of the network is shown in Figure 1: if we use the MGC network to get the
molecular vector, that network will be trained together with the feedforward network
connecting the CIV to the prediction endpoint in an end-to-end fashion.

Figure 1. a) PADME-ECFP architecture. The Extended-Connectivity Fingerprint
was used as the molecular input to the model. b) PADME-GraphConv architecture.
Note that the graph convolutional network generating the latent molecular vector is trained
together with the rest of the network, while the protein descriptor generation process is
independent of the training of the network. The black dots represent omitted neurons and
layers.

4/29

2.1 Compound Featurization

There has been a lot of research on representing small molecules (compounds) as a descriptor
or fingerprint. Among the traditional molecular descriptors and fingerprints, ECFP [36] is
widely adopted as the state-of-the-art method for compound featurization [10], and was
also used in DeepDTI [51]. However, it has a fixed set of mapping and hashing functions,
unable to be tailored for the specific task at hand.

DNN, especially MGC, can be used to generate more flexible feature vectors. Instead
of depending only on the molecule, compound feature vectors generated using DNN
depend on both the molecule and the prediction task (Boolean or continuous). DNN-
derived feature vectors can outperform the ECFP baseline and at times offer some good
interpretability [2, 10, 56].

MGC [2, 10, 15] is an extension of Convolutional Neural Network which learns a vector
representing the compound from the graph-based representation of the molecule. In the
graph representation of molecules, the atoms are denoted by nodes, while the bonds are
denoted by edges. MGC takes into account the neighbors of a node when computing
the intermediate feature vector for a specific node, and the same operation is applied to
the neighborhood of each node (atom), hence it is analogous to ordinary convolutional
networks typically used in Computer Vision [10, 12]. Due to the GraphConv model [53]
among MGC models being more recent and popular with an easier implementation, we use
the GraphConv model as a representative of MGC under the time and resource constraints.
We applied both types of compound featurization methods: ECFP and GraphConv, and
compared their performances. They can both be converted from SMILES representation
without loss of information.

2.2 Target Protein Featurization

There exist many schemes to represent the target protein as a feature vector based on its
amino acid sequence information. DeepDTI [51] used Protein Sequence Composition (PSC)
descriptor, which has 8420 entries for each protein, consisting of amino acid composition
(AAC), dipeptide composition (DC), and tripeptide composition (TC) [4]. It captures rich
information and does not transform the protein as much as some other protein descriptors
(which implies less human knowledge required and less information loss), which we think
could be a desirable attribute as the input to a neural network. In addition to the 8420
entries for each protein sequence, we added an additional binary entry signaling the
phosphorylation status so that the Davis dataset (see below) can be represented more
accurately, with ‘1’ denoting phosphorylated, resulting in 8421 entries in total.

Mousavian et al. [29] used PSSM (Position Specific Scoring Matrix) descriptor to
represent the protein, which focuses on dipeptide sequences and is related to the evolutionary
history of proteins [40]. It is observed that PSSM performed pretty well. Other popular
protein sequence descriptors include Autocorrelation, CTD (Composition, Transition and
Distribution) descriptor, Quasi-sequence order, etc [4].

As PSC contains rich information (like tri-peptide sequence occurrence) with high
dimensionality, and has already shown promising performance in deep learning-based
models for DTI prediction [51], we use PSC in this research. There could be future
comparisons of the performance of PSC and other protein featurization methods as an
extension to this work.

5/29

2.3 Architecture of the Deep Neural Network

PADME uses a feedforward neural network taking the CIV as the input. The PADME
architecture has one output neuron per prediction endpoint, i.e., one output neuron for
most datasets, and 61 output neurons for the ToxCast dataset (see below). DNNs with
single output neuron are called single-task networks, and those with multiple output
neurons are called multi-task networks. Although we only consider the DTI regression
problem in this paper, PADME can also be used for constructing classification models with
minimal changes, either by binarizing the continuous prediction results or by directly using
a softmax/sigmoid layer as the output layer, of which the latter could be more preferable.
For regularization, we use Early Stopping, Dropout and Batch Normalization techniques
[12]. Hyperparameters like dropout rates are automatically searched to find the best set of
them before running cross-validation, as elaborated in the “Experimental Design” section.
Adam optimizer [17] was used to train the network. The activation functions used for fully
connected layers are all Rectified Linear Units (ReLU).

2.4 Time Complexity

PADME does not require drug-drug or target-target similarity matrices or matrix factor-
ization, so it is much more scalable than KronRLS and SimBoost, to be introduced in
the next section. Suppose there are n compounds and m proteins, since KronRLS and
SimBoost need the similarity matrices, both of the methods have at least O(n2 + m2) time
and space complexity, SimBoost involves matrix factorization so it is even more expensive.
But in each epoch of PADME’s training process, the time complexity only depends on the
number of drug-target pairs in the training set, which, in the best case, is O(max(n, m)),
and O(nm) in the worst case. There is no closed-form relationship between the number
of epochs until convergence and n or m, so the number of epochs cannot be analyzed
theoretically. However, we observe that, in practice, this number is possibly sub-linear
in n and m or even independent from n and m, we can fix the number of epochs to a
small constant if we want to get some crude results, while KronRLS and SimBoost strictly
require at least O(n2 + m2) time to get any results.

3 Experiments

3.1 Baseline methods

There are two baseline methods used as comparisons: SimBoost [13] and KronRLS [33],
which are state-of-the-art methods for the DTI regression task.

SimBoost Simboost predicts continuous DTI values using gradient boosting regression
trees. Each drug-target pair corresponds to a continuous DTI value, and the authors defined
3 types of features to characterize the drug-target pairs: type 1 features for individual
entities (drugs or targets); type 2 features, derived from the drug similarity networks and
target similarity networks; type 3 features, which are derived from drug-target interaction
network. The 3 types of features are concatenated to form a feature vector.

Let xi ∈ Rd denote the vector of features for the i -th drug-target pair, while xi ∈ R is

its binding affinity. The score ˆyi predicted for input xi is computed as follows:

6/29

ˆyi = φ(xi) =

K
(cid:88)

k=1

fk(xi), fk ∈ F

where K is the number of regression trees and F is the space of possible trees.The

algorithm learns the set of trees fk.

SimBoost cannot handle cold-start problems, which means it does not work for pairs in

the test set with a drug or target that is absent from the training set.

KronRLS KronRLS stands for Kronecker Regularized Least Squares. It learns a pre-
diction function f (x) for drug-target pairs. It could use some similarity measure between
two drug-target pairs x and xi, and f (x) is constructed as a linear combination of the
similarity values. The algorithm learns the coefficients of this linear combination from the
training data.

Unlike SimBoost, KronRLS is applicable to cold-start problems.

3.2 Non-proteochemometric (Compound-Only) DNN methods

To investigate the usefulness of including protein feature vector (PSC in this paper) in
PADME, we implemented a version of PADME with only compound information as input.
Different from the full PADME, this version has one output unit for each specific target
protein, resulting in a network structure similar to that of [22]. Similar to PADME, we
considered ECFP and GraphConv variants of this DNN model. We call these PADME
versions Compound-Only DNNs later in this paper.

3.3 Datasets and tools

Similar to He et al. [13], we used kinase inhibitor datasets. Following its naming convention,
we call them Davis dataset [8], Metz dataset [27] and KIBA dataset [45], respectively.
However, the versions of these datasets curated by Pahikkala et al. [33] that He et al. [13]
used was slightly different from the original dataset, and Pahikkala et al. did not give the
corresponding justifications. We thus used the data provided by the respective original
authors, then preprocessed them ourselves as described in the Supporting Information.
We assume the observations within each dataset are under the same experimental settings.
Metz dataset contained lots of imprecise values, which we discarded in the preprocessing
step.

Because of the limitations of SimBoost and KronRLS, we filtered the datasets. The
original KIBA dataset contains 52498 compounds, a large proportion of which only have
the interaction values with very few proteins. Considering the huge compound similarity
matrix required and the time-consuming matrix factorization used in SimBoost, it would
be infeasible to work directly on the original KIBA dataset. Thus, we had to filter it
rather aggressively so that the size becomes more manageable. We chose a threshold of 6
(drugs and targets with no more than 6 observations are removed), more lenient than the
threshold of 10 used in He et al. [13], aiming at a reduction of the unfair advantages that
SimBoost can gain by keeping only the denser submatrix of the interaction matrix.

7/29

For the Metz and Davis datasets, as SimBoost cannot handle cold drug/target problem,
we had to ensure that in creating Cross-Validation folds, each drug or target appear in at
least 2 folds, thus those drugs/targets with no more than 1 observation are discarded.

We also used the ToxCast dataset [47], containing a much larger variety of proteins [48].
It contains toxicology data obtained from high-throughput in vitro screening of chemicals,
mainly measured in AC50, which means the concentration at half of the maximum activity.
The prepared dataset (see Supporting Information) contains observations for 530605
drug-target pairs. Its large size and coverage of diverse protein types allow us to test
the robustness and scalability of computational models for DTI prediction. After the
preprocessing, it still contains a total of 672 assays, compared to single assay/interaction
strength measurement of the other 3 datasets. Some of those assays are closely related, but
most of them are different from each other. Because it contains so many heterogeneous
endpoints, we manually grouped those assays into 61 different measurements for interaction
strength based on assay type, such that observations in each measurement are reasonably
homogeneous, also increasing the number of observations for each measurement endpoint.
The number of observations in each measurement range from ∼290 to ∼160,000. For
the ToxCast dataset, we constructed multi-task networks, in which each measurement
corresponds to a neuron in the output layer. As KronRLS and SimBoost are both single-
task models, to evaluate the performance of those two models on the ToxCast dataset, one
must train 61 models for each of them, which would be an extraordinarily tedious job, so
we did not run the SimBoost and KronRLS models on ToxCast. This indicates PADME is
not only more scalable in the number of drugs/targets, but also much more scalable in the
number of endpoints. As ToxCast does not have the bottlenecks imposed by KronRLS
and SimBoost, we did not filter it.

Please refer to table 1 for the sizes of the datasets after filtering.

Table 1. Dataset sizes after filtering.

Dataset

Number of drugs
(compounds)

Number of target
proteins

Total number of
drug-target pairs used

Davis
Metz
KIBA
ToxCast
(No filtering)

72
1423
3807

7657

442
170
408

335

31824
35259
160296

530605

We applied the same numerical transformation as He et al. [13] to the datasets:
transf ormed = 4 − log10(original). For the ToxCast dataset, we changed the inac-
tive value from 1,000,000 to 1,000, so that there would be no large gaps in the distribution
after transforming the data.

The model was constructed based on the implementation of the DeepChem python
package [35], in which RDKit [18] was used; the networks were constructed using TensorFlow
1.3 [1]. In the practical application, PADME takes SMILES representations of the candidate
drug as part of the input, which are transformed into graph representations or ECFP by the
program. PSC was obtained independently from this process: we used the propy python
package [4] to generate PSC descriptors, and manually added a binary entry indicating
phosphorylation. Afterwards, PSC was saved in a standalone file, which the program reads

8/29

into the memory in the runtime.

The experiments were conducted on a Linux server with 8 Nvidia Geforce GTX 1080Ti
graphics cards, among which 4 were used. The server has 40 logical CPU cores and 256
GB of RAM. A computer with less than 110 GB RAM might not be able to perform
cross-validation for the ToxCast dataset using GraphConv-based PADME.

3.4 Experimental Design

To examine PADME’s prediction power, we used cross-validation (CV), which is the
convention of the prior researches, also because we believe the comprehensive coverage
of the whole dataset will offer a more thorough evaluation of the model’s performance,
rather than only using 1 hold-out test set. To measure the performance of the model under
different settings, multiple CV splitting schemes were employed to evaluate the predictions
of the models trained from the training sets against the known interaction strengths in the
test sets. The performances of PADME-ECFP and PADME-GraphConv were compared
against each other under identical settings.

We performed 5-fold CV. For SimBoost to work, every compound (candidate drug) or
target must be present in at least 2 folds, this splitting scheme is called “warm split” in
this paper. There are no such restrictions for KronRLS, since it can handle cold-start data.
Since we did not run SimBoost on ToxCast data, there is no need to perform warm-split
on it, we then used random split in that case. If we force a warm split on the ToxCast
dataset, a filter threshold of 1 must be used to reduce the size of the dataset, which is
undesirable. As cold-start prediction is an important objective in DTI prediction (and an
advantage of PADME), we also included cold-splitting in constructing the cross-validation
folds, such that all compounds (candidate drugs) in the test fold are absent from the
training fold (cold-drug split), or all targets in the test fold are absent from the training
fold (cold-target split). Also, similar to [25], we also implemented a cold-drug cluster
split, using single-linkage clustering with Tanimoto similarity to create compound clusters.
Compounds whose ECFP4 fingerprint had higher similarity than 0.7 were assigned to
the same cluster. Compounds belonging to the same cluster were assigned to same folds,
so that compounds in the validation fold would not be similar to those in the training
fold. The cold-drug cluster split can prevent the performance estimation from being overly
optimistic. Though [33] suggested another splitting scheme which results in simultaneous
cold-drug and cold-target in each validation fold, as it greatly decreases the size of the
training set in each fold (4/9 of the original data instead of 4/5 in other splitting schemes),
we decided that it would cause unfair comparison and did not use it.

For every dataset, we performed four types of CV splitting (warm, cold-target, cold-
drug, cold-drug cluster), and for every CV splitting scheme, we evaluated the prediction
performances of the applicable models (KronRLS and PADME for all splitting schemes,
SimBoost for warm splits only). To reduce the random effects, we repeated the splitting
several times for each splitting scheme on all the datasets and calculated the average values
of the evaluation metrics of the prediction results across the splits. The Compound-Only
DNNs (as mentioned in subsection 3.2) take only compound information as input and
predict the response for multiple proteins simultaneously. Therefore, they cannot handle
cold-target scenarios, and it is unnatural to test them in a warm-split scenario. We only
use them to compare against PADME in cold-drug and cold-drug cluster splits.

Not only do we have multiple splitting methods, we also used multiple model settings

9/29

Figure 2. The
histogram
of
the distribution
of the negative
log transformed
ToxCast mea-
surement results.
The
majority
(over 94%) are
concentrated
at one inactive
value.

and evaluation metrics. For each of PADME-ECFP and PADME-GraphConv, a single-task
network was trained for every splitting scheme of every dataset, except ToxCast, for which
we constructed a multi-task network with 61 output neurons to avoid the complexity caused
by 61 separate single-task networks.

We also wanted to investigate whether PADME can predict the ordering of the interaction
strengths correctly, so in addition to metrics focusing on value correctness (RMSE (Root
Mean Squared Error) and R2), we also used metrics focusing on order correctness, like
concordance index (CI). Using CI as a metric in cheminformatics setting was proposed by
Pahikkala et al. [33]. It measures the probability of correctly ordering the non-equal pairs
in the dataset, ranging across [0, 1], with bigger values indicating better results. If you use
the same value (e.g. mean value of the training set) as the predicted results across the
test set, the CI would be 0.5. We note that the CI neglects the magnitude of values while
focusing on the pairwise comparison, and it does not consider the prediction correctness for
data points that truly have values equal to each other. Thus, CI should be used alongside
other metrics like RMSE. However, in virtual screening, we are typically only interested in
the top predictions, so that the drawback of neglecting the magnitude is not a big concern.
To improve the readability of the re-
ported results for the ToxCast dataset,
the performance metrics are averaged
across the 61 different measurements,
weighted by the number of records for
each of the measurements, so the results
reported for the ToxCast dataset look
the same as other datasets with single
endpoints.

As an exploratory analysis of the
datasets, we found the ToxCast to be
special. As shown in Figure 2, the trans-
formed ToxCast dataset is extremely
concentrated at a value of 1 which cor-
responds to no interaction. This led us
to ignore the R2 values for this dataset:
because R2 is sensitive to the overall departure of the predicted values from the true values,
we argue that the huge concentration of values has rendered R2 uninformative in measuring
the performance of the model on the ToxCast dataset. This concentration of values also
makes RMSE less informative than it otherwise would be (since one can blindly guess
inactive values for all and still get pretty good RMSE), so we argue that CI is the most
useful metric in the ToxCast dataset prediction evaluation. This pronounced imbalance
in the dataset caused us to consider balancing it through oversampling (see supporting
information).

Following the principle of parsimony, we wanted to use a minimal number of hyper-
parameter sets wherever possible, to keep the time and computational costs manageable.
If, instead, we do one hyperparameter tuning to get the hyperparameters for each CV
iteration, to ensure a reasonable coverage of parameter space, it would have taken well over
a month to run a CV for a dataset due to the intrinsic complexity of DNN models involving
protein information, which would have been unrealistic, both for us and future users. So

10/29

we cannot use the nested/double cross-validation as used in [3] and [26]. Also, since the
datasets are not very large for deep learning, we wanted to maximize the training set size
in each iteration. Thus, in our hyperparameter tuning process, we randomly selected 90%
of the dataset to be the training set, the remaining 10% to be the validation set, and the
validation set was used for determining the best set of hyperparameters. Then in the 5-fold
CV splits, we excluded the aforementioned validation set elements from each validation
fold, but the validation set elements are retained in the training folds, thus in each CV
iteration, the training folds are 80% the size of the dataset, while the validation fold is 18%
the size of the dataset. This simultaneously makes the training folds as large as possible,
and avoids bias in evaluation.

To efficiently tune the hyperparameters (like dropout rates, batch size, learning rate,
number of layers, nodes per layer, etc.) for both PADME models and Compound-Only
DNN models, we used Bayesian Optimization [39] implemented by the Python package
pygpgo [14]. We also used early stopping to determine the number of training epochs needed.
To guide early stopping, we used mean(RM SE) − mean(CI) calculated on the validation
set as the composite score to be minimized. The training and validation sets used for
hyperparameter searching are split randomly from the original dataset without any concern
for warm or cold drug/target splits. We only store one optimal set of hyperparameters
per (dataset, PADME variant) pair, which were then used for all CV settings for that
(dataset, PADME variant) pair. Note that, for simplicity and to examine the robustness of
PADME, the set of hyperparameters found in the random splitting was used in all CV
settings, including those with cold-drug split and cold-target split, though we believe better
CV results could be achieved if the hyperparameter searching processes are specifically
designed for that CV fold split scheme, e.g., for cold-target CV folds, we could use the
hyperparameters found by running the Bayesian Optimization on cold-target splitted
datasets.

The resulting networks typically have 2 or 3 fully-connected layers connecting the CIV
to the output unit, with thousands of neurons in each of the layers. Each fully-connected
layer is batch-normalized.

3.5 Experimental Results

3.5.1 Quantitative Results

Based on the experimental design in subsection 3.4, we obtained the quantitative results
for PADME. In Tables 2 to 4, the bold numbers indicate the best values attained for each
setting. The sample standard deviations of CV mean results are also calculated, based
on which we performed two-sample t-tests with unequal variances. For each t-test, the
null hypothesis was that the mean of the CV results of the model is not worse than the
best PADME result (boldfaced ones), while the alternative was that the model was worse
than the best PADME model. For RMSE, worse means larger, while for CI or R2, worse
means smaller. The p-values are reported. We observe that the two versions of PADME
dominate the other methods1, including the Compound-Only DNN models though to a
lesser degree, across all datasets and splits for all evaluation metrics.

1Note that the SimBoost results reported here are considerably worse than the results reported in their
original paper. It is because we have examined their source code and found they calculated MSE but
reported it as RMSE.

11/29

We note the following exceptions. SimBoost outperforms PADME-GraphConv on the
Metz dataset, which could be due to the small dataset size: PADME-GraphConv could be
overfitting for Metz data, while SimBoost uses gradient boosting trees, a machine learning
model better suited for small datasets than Deep Neural Networks. Because it does not
use MGC, PADME-ECFP has a much smaller network than PADME-GraphConv, which
may explain why the former performs slightly better on the Metz dataset. However, we do
not observe the same phenomenon on the Davis dataset, which has a similar size and even
fewer entities. Comparing PADME-ECFP against Compound-Only ECFP and PADME-
GraphConv against Compound-Only GraphConv, we observe that PADME consistently
performs better in Concordance Index, for example, they outperform Compound-Only
DNNs by around 10% or even more in Concordance Index on the ToxCast dataset. But
in RMSE and R2, Compound-Only DNNs sometimes perform similarly to the PADME
models in Davis and KIBA datasets, or even insignificantly outperform PADME models.
In general, it seems that PADME models are better than others, but more so in predicting
the order, as reflected in CI. Besides, Compound-Only DNNs outperform KronRLS in
general, except the Compound-Only GraphConv on the Davis dataset.

It is somewhat surprising that PADME-ECFP is not outperformed by PADME-
GraphConv; instead, it slightly outperforms PADME-GraphConv in many cases, though in
general their performances are very close to each other. The Compound-Only ECFP models
usually outperforms Compound-Only GraphConv. PADME-ECFP only takes about 23%
of the time and 45% the space (RAM) of PADME-GraphConv in the training process and
yields similar (and sometimes better) results, so PADME-ECFP is a more reasonable choice.
Nonetheless, we cannot be certain that PADME-GraphConv and PADME-ECFP truly have
similar performances, as there might be a better set of hyperparameters for each model that
would differentiate their performances significantly. We think that the higher complexity
of PADME-GraphConv introduced by the MGC network makes it harder to find a good
set of hyperparameters, while it is relatively easier to find a good set of hyperparameters
for PADME-ECFP which has a simpler network. This could be a possible reason why
MGC cannot beat ECFP in our experiments. Thus, future researchers should continue
investigating MGC and find better sets of hyperparameters in PADME-GraphConv, and
perhaps propose better MGC models.

From Tables 2 to 4 we can observe an interesting phenomenon: when there are many
compounds and few targets in the training set, the cold-drug predictions tend to outperform
the cold-target predictions; on the other hand, when there are many targets and few
compounds, the cold-target predictions tend to be better than the cold-drug ones. We
hypothesize that it is because the models can be more robust in entities (drugs or targets)
with more information in the training set, thus performing better in the corresponding
scenario. This trend is not only present in the PADME models, but in KronRLS as well.
It seems that the models also require much more types of compounds than proteins for
learning their chemical features, as can be seen from the KIBA dataset, whose cold-drug
and cold-target performances are very similar, though it has 3807 compounds and only
408 proteins. And, as expected, there is a universal trend that the performance of warm
splits is always better than that of cold-drug, cold-drug cluster, or cold-target splits.

The use of cold-drug clusters prevents us from overestimating the performance of the
models: in the Metz and KIBA datasets, the performances for cold-drug cluster CV are
noticeably worse than those for cold-drug CVs, while the performances on the Davis and

12/29

Table 2. The regression performance across the datasets measured in RMSE (smaller is
better), averaged across independent repetitions of CV. The mean RMSE are enclosed
in square brackets; sample standard deviations are also reported. The best results in the
PADME models are boldfaced, and one-sided two-sample t-tests are conducted against
them. The blue values are insignificantly bigger (worse) (p > 0.05) than the boldfaced
values, while the orange ones are insignificantly smaller (better) (p < 0.95) than them.
The uncolored ones are significantly worse than the boldfaced values.

Dataset

Cross Validation
Splitting type

Davis

Metz

KIBA

ToxCast

Warm

Cold Drug

Cold Drug
Cluster

Cold Target

Warm

Cold Drug

Cold Drug
Cluster

Cold Target

Warm

Cold Drug

Cold Drug
Cluster

Cold Target

Warm

Cold Drug

Cold Drug
Cluster

Cold Target

Value
Type
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val

PADME-
ECFP
[0.4287]
0.0029
—
[0.8054]
0.0118
—
[0.7671]
0.0171
—
[0.5639]
0.0065
—
[0.5556]
0.0022
—
[0.7119]
0.0016
—
[0.7770]
0.0115
—
[0.7905]
0.0127
—
[0.4334]
0.0069
0.0405
[0.6007]
0.0036
—
[0.7132]
0.0270
—
[0.6226]
0.0035
0.4867
[0.4049]
0.0011
—
[0.4447]
6.37E-4
—
[0.4480]
0.0006
0.3264
[0.4794]
0.0089
—

PADME-
GraphConv
[0.4313]
0.0029
0.0969
[0.8280]
0.0192
0.0312
[0.7922]
0.0228
0.0435
[0.5748]
0.0080
0.0229
[0.6100]
0.0111
1.4E-4
[0.7533]
0.0086
1.6E-4
[0.8099]
0.0089
5.8E-4
[0.8239]
0.0107
0.0011
[0.4247]
0.0027
—
[0.6444]
0.0149
0.0040
[0.7263]
0.0224
0.2409
[0.6225]
0.0058
—
[0.4092]
0.0013
0.0012
[0.4448]
3.3E-4
0.4343
[0.4476]
0.0014
—
[0.4896]
0.0120
0.1133

RMSE

SimBoost KronRLS

[0.4820]
0.0019
4.25E-9
—
—
—
—
—
—
—
—
—
[0.5813]
0.0016
4.29E-8
—
—
—
—
—
—
—
—
—
[0.4689]
0.0010
4.93E-6
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—

[0.5729]
0.0051
2.97E-12
[0.8405]
0.0210
0.0041
[0.8368]
0.0382
0.0024
[0.6596]
0.0020
6.5E-7
[0.7813]
3.87E-4
4.42E-10
[0.7843]
0.0052
2.41E-8
[0.8315]
0.0054
5.81E-5
[0.8989]
0.0101
2.56E-7
[0.6566]
1.74E-4
2.01E-7
[0.7024]
0.0024
1.85E-8
[0.7536]
0.0039
0.0285
[0.6811]
0.0082
2.37E-5
—
—
—
—
—
—
—
—
—
—
—
—

Compound-Only
ECFP
—
—
—
[0.8038]
0.0148
0.5733
[0.8040]
0.0435
0.0675
—
—
—
—
—
—
[0.7738]
0.0146
3.10E-4
[0.8250]
0.0075
5.56E-5
—
—
—
—
—
—
[0.6319]
0.0045
3.93E-6
[0.7029]
0.0064
0.7459
—
—
—
—
—
—
[0.4550]
0.0012
1.03E-6
[0.4509]
9.53E-4
0.0048
—
—
—

Compound-Only
GraphConv
—
—
—
[0.8505]
0.0238
0.0047
[0.8351]
0.0144
7.86E-5
—
—
—
—
—
—
[0.7775]
0.0045
3.16E-7
[0.8386]
0.0051
2.94E-5
—
—
—
—
—
—
[0.6421]
0.0024
2.42E-6
[0.7196]
4.71E-4
0.3329
—
—
—
—
—
—
[0.4682]
0.0036
4.42E-5
[0.4566]
0.0025
1.82E-4
—
—
—

13/29

Table 3. The regression performance across the datasets measured in Concordance Index
(larger is better), averaged across independent repetitions of CV. Similar to Table 2, the
mean CI are enclosed in square brackets; sample standard deviations are also reported.
One-sided two-sample t-tests are conducted against the best PADME models. The blue
values are insignificantly smaller (worse) (p > 0.05) than the boldfaced values, while
the orange ones are insignificantly larger (better) (p < 0.95). The uncolored ones are
significantly worse than the boldfaced values.

Concordance Index

Dataset

Cross Validation
Splitting type

Davis

Metz

KIBA

ToxCast

Warm

Cold Drug

Cold Drug
Cluster

Cold Target

Warm

Cold Drug

Cold Drug
Cluster

Cold Target

Warm

Cold Drug

Cold Drug
Cluster

Cold Target

Warm

Cold Drug

Cold Drug
Cluster

Cold Target

Value
Type
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val

PADME-
ECFP
[0.9034]
0.0020
0.2780
[0.7120]
0.0026
—
[0.7238]
0.0094
—
[0.8538]
0.0034
—
[0.8065]
0.0012
—
[0.7432]
0.0021
—
[0.7158]
0.0048
—
[0.6961]
0.0076
0.0058
[0.8577]
0.0011
0.0028
[0.7742]
0.0011
—
[0.7465]
0.0019
—
[0.7684]
0.0020
0.4210
[0.7908]
0.0041
0.0380
[0.7196]
0.0062
0.0082
[0.7161]
0.0012
0.0010
[0.6752]
0.0108
0.0144

PADME-
GraphConv
[0.9040]
0.0012
—
[0.7099]
0.0139
0.3801
[0.7190]
0.0096
0.2208
[0.8428]
0.0031
3.70E-4
[0.7931]
0.0016
3.49E-7
[0.7384]
0.0013
0.0018
[0.7132]
0.0014
0.1485
[0.7099]
0.0031
—
[0.8616]
0.0014
—
[0.7524]
0.0032
1.64E-4
[0.7190]
0.0030
9.77E-6
[0.7687]
0.0029
—
[0.7963]
8.62E-4
—
[0.7329]
0.0027
—
[0.7290]
0.0032
—
[0.6979]
0.0116
—

SimBoost KronRLS

[0.8871]
5.97E-4
1.35E-7
—
—
—
—
—
—
—
—
—
[0.7944]
6.92E-4
5.05E-7
—
—
—
—
—
—
—
—
—
[0.8405]
1.35E-4
3.60E-5
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—

[0.8758]
0.0015
2.97E-11
[0.6924]
0.0117
0.0042
[0.6800]
0.0421
0.0254
[0.8075]
0.0027
8.45E-9
[0.7485]
6.98E-4
3.71E-11
[0.7092]
0.0021
5.63E-10
[0.6818]
0.0037
1.02E-6
[0.6470]
0.0048
7.82E-10
[0.7831]
3.26E-4
2.59E-7
[0.6890]
0.0014
9.49E-11
[0.6654]
0.0038
5.43E-9
[0.7122]
0.0045
1.82E-6
—
—
—
—
—
—
—
—
—
—
—
—

Compound-Only
ECFP
—
—
—
[0.7027]
0.0162
0.1361
[0.6994]
0.0265
0.0547
—
—
—
—
—
—
[0.7110]
0.0033
2.60E-7
[0.6878]
0.0018
2.82E-5
—
—
—
—
—
—
[0.7405]
0.0027
3.05E-7
[0.7074]
0.0034
1.37E-7
—
—
—
—
—
—
[0.6692]
0.0041
1.25E-8
[0.6440]
0.0045
3.27E-9
—
—
—

Compound-Only
GraphConv
—
—
—
[0.6668]
0.0187
0.0026
[0.6828]
0.0215
0.0047
—
—
—
—
—
—
[0.7197]
0.0037
6.11E-6
[0.6966]
0.0026
9.20E-5
—
—
—
—
—
—
[0.7356]
0.0023
2.66E-8
[0.7068]
0.0023
9.90E-9
—
—
—
—
—
—
[0.6414]
0.0068
2.18E-7
[0.6192]
0.0061
1.20E-8
—
—
—

14/29

Table 4. The regression performance across the datasets measured in R2 (larger is
better), averaged across independent repetitions of CV. Similar to Table 2, one-sided
two-sample t-tests are conducted against the best PADME models. The blue values are
insignificantly smaller (worse) (p > 0.05) than the boldfaced values, while the orange ones
are insignificantly larger (better) (p < 0.95). The uncolored ones are significantly worse
than the boldfaced values.

R2

Dataset

Davis

Metz

KIBA

Warm

Warm

Cold Drug

Cold Drug

Cold Target

Cold Target

SimBoost KronRLS

Cold Drug
Cluster

Cold Drug
Cluster

Cross Validation
Splitting type

Compound-Only
GraphConv
—
—
—
[0.0612]
0.0606
0.0178
[0.0991]
0.0321
1.57E-4
—
—
—
—
—
—
[0.3430]
0.0114
8.93E-6
[0.2241]
0.0099
7.22E-6
—
—
—
—
—
—
[0.4312]
0.0054
1.93E-7
[0.2887]
0.0042
0.4354
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
a We did not report R2 for ToxCast because of its imbalanced/skewed nature.

Compound-Only
ECFP
—
—
—
[0.1358]
0.0446
0.3762
[0.1174]
0.1172
0.0545
—
—
—
—
—
—
[0.3471]
0.0241
3.41E-4
[0.2457]
0.0158
1.46E-5
—
—
—
—
—
—
[0.4599]
0.0039
9.92E-6
[0.3267]
0.0140
0.7878
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—

PADME-
GraphConv
[0.7614]
0.0031
0.0402
[0.0851]
0.0499
0.0325
[0.1631]
0.0520
0.0304
[0.5741]
0.0127
0.0195
[0.5961]
0.0153
2.1E-4
[0.3813]
0.0145
2E-4
[0.2779]
0.0187
4E-4
[0.2562]
0.0207
8E-4
[0.7560]
0.0032
—
[0.4352]
0.0270
0.0051
[0.2793]
0.0463
0.3620
[0.4734]
0.0100
—
—
—
—
—
—
—
—
—
—
—
—
—

PADME-
ECFP
[0.7652]
0.0029
—
[0.1439]
0.0335
—
[0.2253]
0.0340
—
[0.5915]
0.0087
—
[0.6654]
0.0027
—
[0.4477]
0.0024
—
[0.3390]
0.0183
—
[0.3185]
0.0214
—
[0.7449]
0.0084
0.0356
[0.5093]
0.0057
—
[0.2948]
0.0687
—
[0.4715]
0.0051
0.3752
—
—
—
—
—
—
—
—
—
—
—
—

[0.7031]
0.0023
2.79E-10
—
—
—
—
—
—
—
—
—
[0.6323]
0.0020
2.56E-8
—
—
—
—
—
—
—
—
—
[0.7007]
0.0013
2.92E-6
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—

[0.5801]
0.0077
2.42E-10
[0.0478]
0.0592
0.0047
[0.0540]
0.0931
0.0024
[0.4393]
0.0064
2.02E-9
[0.3355]
6.15E-4
1.18E-10
[0.3285]
0.0084
2.59E-8
[0.2416]
0.0080
3.93E-5
[0.1130]
0.0195
6.30E-8
[0.4128]
3.34E-4
8.54E-8
[0.3266]
0.0048
3.57E-9
[0.2215]
0.0070
0.0611
[0.3631]
0.0136
1.14E-5
—
—
—
—
—
—
—
—
—
—
—
—

Value
Type
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val
mean
std
p-val

Cold Drug
Cluster

Cold Drug
Cluster

Cold Target

Cold Target

Cold Drug

Cold Drug

Warm

Warm

ToxCast

15/29

ToxCast datasets stay almost the same. This could be due to the different distributions of
compounds in different datasets. We suggest that future researches also employ cold-drug
clusters splits in their experiments, so that a more stringent evaluation could be performed.
The fact that PADME outperforms both SimBoost and KronRLS demonstrates the
power of DNN to learn complicated nonlinear relationships between drug-target pairs
and interaction strength. We were also able to show the superiority of PADME over the
Compound-Only DNN models, both in the applicability of cold-target scenario and overall
performance, which might suggest the improvement introduced by protein-specific descrip-
tors (PSC in this paper). Furthermore, the results presented might be an understatement
of the real performance of PADME in cold-drug and cold-target scenarios, as the training
and validation sets for hyperparameter searching are randomly split, resulting in a set of
hyperparameters that suit well for randomly split CV folds, but perhaps not for cold-drug
and cold-target folds. This deliberately unfair comparison shows the robustness of the
PADME models.

Applicability Domain Applicability Domain (AD) is an important issue in considering
the usage of QSAR models, because every QSAR model has limitations and cannot be
applied to all possible inputs. Conceptually, AD defines the region of “normal” objects in
the chemical space, for which the QSAR model can be applied and get reliable predictions
[24, 37]. [37] categorizes different types of approaches for estimating AD, including ranges
in the descriptor space, geometrical methods, distance-based methods, probability density
distribution, and range of the response variable.

Some approaches for AD estimation are widely used but do not apply to PADME,
like the standard deviation of ensemble predictions, or the bounding box method, or in
general descriptor space analysis [37, 44]. In particular, descriptor space analysis is not
applicable because the input of PADME does not involve chemical descriptors obtained
through feature engineering, like Dragon descriptors. Furthermore, unlike typical QSAR
models which only take compounds as input, PADME also has protein information as part
of the input, adding a lot more complexity into AD estimation, like measuring the distance
in the distance-based methods. Unlike conventional QSAR methods which only need to
calculate the distance between compounds, for PADME, we have to consider the distance
between compound-protein (drug-target) pairs, but to our knowledge, there is no widely
accepted way of measuring the distance between those pairs.

Thus, we decided to simply use the range of the response variable to define AD, which
is a viable approach for regression models [24, 37]. A natural method is to use mean and
standard deviation, but since DTI datasets are often highly skewed (like the Davis dataset),
using mean and standard deviation does not give reasonable AD range estimations. For
example, in the 5-fold CV of Davis dataset, for one iteration, the training folds have a
range [5.0, 10.796], but the mean is 5.398, and the standard deviation is 0.8506. If we deem
a test DT pair with response values lying within (mean − k ∗ std, mean + k ∗ std) as being
inside AD, where k is some constant, k must be very large to encompass the right end of
the training set value range, but that would make the left end of the AD range too small
to be realistic.

Instead, we propose the following simple method. Let min denote the minimum value of
response variable y in the training set, and max denote the maximum value in the training
set, and the rangesize = max−min. We define the AD to be (min−0.15∗rangesize, max+

16/29

0.15 ∗ rangesize). If a test drug-target pair has response value (experimental measurement)
outside this range, we deem it as outside of AD. If the experimental measurement is
unknown, we use the predicted response value as an approximation to the true response.
Fig. 3 demonstrates that, in an iteration of CV of Davis dataset, the predicted response
values in the validation fold lie within the aforementioned range calculated from the
training folds. So we know the elements in the validation fold belong to AD, even if the
true response values of the validation fold are unknown. Specifically, the range of response
values of the training folds is [5.0, 10.796], so the range size is 5.796. The AD is thus
[5 − 0.15 ∗ 5.796, 10.796 + 0.15 ∗ 5.796], which equals [4.131, 11.665]. The range of predicted
values in the validation fold is [4.558, 10.123], so all the predicted values lie in the AD
range, thus all validation fold elements are in the AD.

Figure 3. The histograms of response variable values of the training folds and the
predicted response variable values of the validation fold of a CV iteration of the Davis
dataset. The range of response values of training folds is [5.0, 10.796], while the range of
predicted values in the validation fold is [4.558, 10.123], demonstrating the validation fold
elements are within the AD even if we don’t know the true values of the response variable.

3.5.2 Qualitative Results

We used plots to visualize the prediction performance, so we can assess the results qualita-
tively.

Fig. 4 presents the predicted values (by PADME-ECFP) VS true values for each dataset.
For each panel(row) in the figure, there is a scatter plot, and a contour plot (the darkness
of the color represents the density of data points) with univariate density curves on the
margins. Both plots in each row are plotted from the exact same data. Figure 4(d) is an
exception, it includes a hexagon plot instead of a contour plot, because the contour plot
fails to show anything (but the density curves on the margins are plotted), possibly because
the data points of ToxCast are too concentrated to be shown correctly on the contour plot,
as can be observed from the hexagon plot. To help visualize ToxCast better, we added a
Figure 5 which is a scatter plot of the same data as Figure 4(d), with histograms on the
margins.

Clearly, all datasets except Metz data are very concentrated at some values.
Because the concentration of Davis and ToxCast datasets pose problems in visualizing
the prediction performances on them, we decided to plot the scatter plots, contour plots

17/29

Figure 4. Scatter plot and contour plot of predicted VS true values across all datasets.
The panels a, b, c and d correspond to Davis, Metz, Kiba and ToxCast datasets,
respectively. The axes in the two plots of the same panel are the same, and both plots are
generated from the same data. The diagonal lines in the scatter plots are the reference
lines where predicted = true value.

and hexagon plots of the true active and true inactive data points separately for those
datasets (Figs. 6 and 7). From Fig. 6 we can see the Davis dataset was predicted pretty well
on both true active and inactive values, Fig. 6 (a) shows a nice pattern of correspondence
between predicted VS true values on the active data points, while Fig. 6 (b) presents true
inactive values, in which the hexagon plot shows a high concentration of predicted values
close to the true values. As reflected in Fig. 7 (a), the model fitted on the ToxCast dataset
was strongly influenced by inactive values, and the prediction performance for the true
active data points was not very good, but the true inactive data points were predicted to

18/29

Figure 5. ToxCast data scatter plot with marginal histograms, generated from the same
data as Figure 4(d)

concentrate around the true values (shown in the hexagon plot in Fig. 7 (b)), which might
explain why its quantitative analysis results were decent.

To tackle the imbalanced dataset problem in ToxCast, we tried to train a model on
the oversampled dataset and measured its performances. Please refer to the Supporting
Information.

Figure 6. Plots for Davis dataset predicted value VS true value. Panel (a) corresponds
to the true active values, while panel (b) corresponds to true inactive values. Similar to
figure 4, all plots in the same panel are plotted from the same data.

So why does PADME perform well on Davis, Metz and KIBA datasets, but not so
satisfactorily on the ToxCast dataset? We think it might be related to the nature of the
ToxCast dataset itself. The ToxCast dataset not only contains a much larger variety of
proteins (unlike the other 3 datasets which only contain kinase inhibitors), but it also
has a much larger number of assays (measurement endpoints), which are often quite

19/29

Figure 7. Similar to Figure 6, plots for the ToxCast dataset. Panel (b) uses a different
hexagon plot from (a), because that form of hexagon plot on panel (b) does not display
properly.

different from each other. Though we only selected the assays with single intended targets,
many of those assays are cell-based (for example, OT AR ARSRC1 0480), which could
introduce some more complexities in addition to the drug-target interaction, due to the
intricacies of biochemical processes in cells. These challenges might be some reasons why,
to our knowledge, previous non-docking researches on drug-target interaction prediction
containing protein information as input did not use this dataset [13, 32, 33, 51], though
other kinds of researches did [5, 21, 23].

The challenges with the ToxCast dataset, including its large number of measurement
endpoints and the imbalanced dataset problem, should be investigated in future work,
as it is an important objective to build a more general-purpose DTI prediction model
that handles a larger variety of input proteins, compounds and measurement endpoints.
Imbalanced datasets also frequently appear in virtual screening. We believe that, based
on our work, future researches will either make improvements on the ToxCast dataset or
deem it as a great challenge for DTI prediction models involving protein information, and
our results presented here, though not ideal, is of reference value to the community.

3.5.3 Case Study

In addition to the quantitative and qualitative evaluations shown above, we performed a
case study to further validate the predictions of PADME by investigating the compounds
predicted to interact strongly with selected target proteins.

We focused on the androgen receptor (AR), for which alterations of functions are

associated with prostate [55] and breast cancers [28].

We used all compounds in the datasets used in this paper, together with all the

20/29

compounds in US National Cancer Institute human tumour cell line anticancer drug screen
data (NCI60), totaling more than 100000, and AR as the only target protein. NCI60
dataset records the in vitro drug response of cancer cell lines [41]. For prediction, we
used PADME-ECFP and PADME-GraphConv trained on the whole ToxCast dataset,
then took the average of their predictions, we call this averaged model PADME-Ensemble.
The reason we chose the ToxCast dataset is that its endpoints are the most suitable for
calculating AR binding affinity or antagonistic effects, and the ToxCast dataset has the
most diverse set of compounds and proteins.

There are many different assays in ToxCast, some are cell-based, while some are cell-free.
Cell-based assays are much more complicated than their cell-free counterparts, since the
results of cell-based assays might involve some intricate biochemical reactions in the cells.
Thus, we used the assay NVS NR hAR, a cell-free assay measuring the binding affinity
between Human Androgen Receptor (AR) and ligands (please refer to the Supporting
Information for details), to examine the efficacy of PADME’s predictions.

From the predictions of PADME-Ensemble, we selected the prediction results corre-
sponding to NVS NR hAR, and then sorted the predicted values in descending order. Due
to the transformations we performed in subsection 3.3, the larger the number (higher in the
sorted list), the stronger the binding affinity. Next, we filtered out those compounds in the
predicted list that have appeared in the ToxCast dataset, or had a large Tanimoto similarity
with some compounds in ToxCast, calculated using rdkit fingerprint. We then did a search
on PubChem database [16] for the top 30 compounds predicted to bind strongly with AR.
The top 30 (and beyond) compounds all shared a highly similar structure with androgen,
so most of them may be able to bind strongly with AR. After a stringent search on
PubChem, we confirmed that 4 of them are active, which is reflected in patents, bioassay
results, or research papers. The other compounds in top 30 are also possibly active, but
since there is no direct evidence from PubChem, we take the conservative approach and do
not consider them here.

The 4 compounds’ PubChem CID are 88050176, 247304, 9921701, 220507. Fig. 8 shows

their 2D images.

Obviously, the compounds are all very similar to androgen, since NVS NR hAR is a
very simple assay, the model learns from the dataset that analogs of androgen tend to bind
strongly with AR. This shows that the prediction results of PADME are effective in drug
discovery.

Based on this, we tried to take one step further to do a more interesting task: calculating
the AR antagonist effect of compounds based on the predictions produced by PADME.
Because there are 61 outputs in PADME models trained on ToxCast data, we had to
propose a set of coefficients to calculate a composite AR antagonist score (details in the
Supporting Information) from the averaged prediction results, for which we expect the
compounds with higher scores would show stronger anticancer activity in AR-related cell
lines in NCI60 dataset. We then ordered the AR antagonist scores in descending order.

Compared to those predicted to bind strongly with AR, the predicted list of AR
antagonistic compounds have much more diverse structures. However, their results (see
Supporting Information) are not well aligned with our expectations. This is not caused by
PADME, which captures the patterns in the training data faithfully and shows it in the
test set. It is the results on the true dataset that are different from our assumptions.

There are two major challenges in this process: the formula we used for calculating

21/29

Figure 8. The 4 compounds from top 30 predictions that are confirmed to bind strongly
with AR. The numbers are their corresponding PubChem CIDs. On the right side is the
2d representation of testosterone, the major androgen. The images are downloaded from
the PubChem website.

AR antagonist score is based on assumptions, the existence of cell-based assays is also
a possible source of problem; our expectation that AR antagonistic compounds should
perform selectively on some known AR-related cancer cell lines might deviate from the
truth, or AR influences many types of cancers in different ways from what we knew, like
suggested in [30]. Tackling these two challenges is a task that could require years or decades
of work by the community.

Although the AR antagonist score is problematic, we still decided to check whether
it is useful. We chose 38 compounds from the top predictions for AR antagonists that
were available from the vendors including the National Cancer Institute [31], and 3 of
them were confirmed to be active, which have ZINC IDs ZINC8665890, ZINC3861637, and
ZINC4947964. Please see Supporting Information for details.

In all, the obtained results indicate that PADME is capable of identifying compounds

that have the desired simple interactions with the target protein.

4 Discussion

PADME is compatible with a large variety of different protein and compound featurization
methods. By combining different protein descriptors like PSSM and other molecule
featurization schemes like Weave [15], many more variants of PADME can be constructed,
whose performances can be compared against each other. In fact, we used the Weave
implementation in DeepChem as a molecular featurization method and ran hyperparameter
searching on it, but the result was worse than ECFP and GraphConv, and it consumed
much more time and memory than the other two, so we did not pursue it any further.

Mapping a protein into a feature vector is a task in proteochemometrics. However, most

22/29

existing methods in proteochemometrics require expert knowledge and often involve 3D
structural information [34, 49], which is often not available. We only considered sequence
information for both drugs and targets in this work to make our model more generally
applicable.

It is also possible to use CNN or RNN to learn a latent feature vector to represent
the proteins, based on its amino acid sequence information, instead of using fixed-rule
protein descriptors like PSC as the input, so that the whole model can be trained in a
completely end-to-end fashion without standalone components of the network like PSC in
our implementation, making the network structure more “symmetrical”. Actually, it was
already attempted by ¨Ozt¨urk et al. [32], who showed a performance similar to PADME,
but they did not use cross-validation to get average performances, they only ran different
models on the same test set, which was just 1/6 the size of the whole dataset, so we think
there is still much room for improvement in that direction. Nonetheless, it is a very good
step towards it.

We only used simple feedforward neural networks in our implementations of PADME
from the Combined Input Vector to the output, but other types of Neural Networks might
be able to generate better results, like Highway Networks [43], which allows the units in
the network to take shortcuts, circumventing the large amounts of layers in some networks.
Pretraining also has the potential to improve our model, but we did not include it,
because it might be difficult for the community to compare the real performance of PADME
with other models.

Compared to previous models like SimBoost and KronRLS, PADME is not only out-
performing them in terms of prediction accuracy, but is more scalable in terms of number
of drugs and targets and number of prediction endpoints2, because both SimBoost and
KronRLS rely on similarity matrices and they are only single-task models. In the age of
Big Data, this scalability will be a big advantage in virtual high-throughput screening.

5 Conclusion

To tackle DTI regression problem more effectively, we devised the PADME framework
that utilizes deep neural networks for this task. PADME incorporates both compound
and target protein sequence information, so it can handle the cold-start problem, which
most current deep learning-based models for DTI prediction cannot do. Using sequence
information as the input makes the model simple and generally applicable. Predicting
real-valued endpoints also makes it desirable for problems requiring finer granularity than
binary classification.

PADME is the first method to incorporate MGC with protein descriptors into the DTI
prediction task, and has been shown be consistently outperform state-of-the-art methods
as well as Compound-Only DNN models. Surprisingly enough, PADME based on MGC
(GraphConv in our case) does not outperform PADME based on ECFP, which could be
due to the difficulty of finding the best set of hyperparameters for MGC. More work is
needed to construct a better MGC model or find a better set of hyperparameters for the
existing model. PADME is also more scalable than the state-of-the art models for DTI
regression task, namely SimBoost and KronRLS, and this advantage might be significant

2As elaborated in subsection 2.4, the scalability in the number of drugs and targets might not always be

the case, but the scalability in the number of prediction endpoints is.

23/29

in datasets with lots of compounds/targets and multiple measurement endpoints. Another
contribution is the use of the ToxCast dataset in DTI prediction problems with protein
information input, which we believe future research should investigate further in addition
to the other benchmarking datasets. Our results on the ToxCast dataset suggests it is a
greater challenge than we expected.

As a case study, we predicted the binding affinity between compounds and the androgen
receptor (AR), a high proportion of the compounds predicted to bind strongly with AR
are confirmed through database/literature search. This suggests that PADME has the
potential to be applied in drug development, and will likely benefit domains like toxicity
prediction, computer-aided drug discovery, precision medicine, etc.

With the compatibility of PADME to different drug molecule and target protein
featurization methods, as well as its scalability compared to methods which rely on
similarity matrices and have single outputs, we believe that future work could propose
more PADME variants that advance the frontier of DTI prediction research.

Supporting Information

The following files are available free of charge.

• PADME supplementary dataset files.zip: will be available in journal version of this

paper. Exceeds size limit of arXiv.

• supplementary text.pdf: explains data preprocessing steps and presents some addi-

tional experiments.

The source code and some processed datasets are deposited at https://github.com/simonfqy/PADME.

Some bigger processed datasets could be obtained upon request.

Acknowledgments

The authors thank Dr. Fuqiang Ban and Dr. Michael Hsing for giving us some useful
information that we incorporated into this paper. Helene Morin and Eric LeBlanc were
in charge of wet-lab experiments for validating our results. We also thank the help and
suggestions received from other fellow lab members, including but not limited to Zaccary
Alperstein, Oliver Snow, Michael Lllamosa, Hossein Sharifi, Beidou Wang, Jiaxi Tang
and Sahand Khakabimamaghani. We also express our gratitude towards our family and
friends, especially Jun Li, Wen Xie, Qiao He, Yue Long, Aster Li, Lan Lin, Xuyan Qian,
Zhilin Zhang, Qing Rong, Si Chen, Fengjie Lun and Stephen Tseng, the list goes on; most
important of all, the late Mr. Xiefu Zang.

We also thank George Lucas (and his prequel trilogy) and Smith et al. [42] for the

inspiration for naming.

References

1. M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
G. Irving, and M. Isard. Tensorflow: A system for large-scale machine learning.

24/29

In 12th {USENIX} Symposium on Operating Systems Design and Implementation
({OSDI} 16), pages 265–283, 2016.

2. H. Altae-Tran, B. Ramsundar, A. S. Pappu, and V. Pande. Low data drug discovery

with one-shot learning. ACS Cent. Sci., 3(4):283–293, 2017.

3. D. Baumann and K. Baumann. Reliable estimation of prediction errors for qsar
models under model uncertainty using double cross-validation. J. Cheminf., 6(1):47,
2014.

4. D.-S. Cao, Q.-S. Xu, and Y.-Z. Liang. Propy: A tool to generate various modes of

chou’s pseaac. Bioinformatics, 29(7):960–962, 2013.

5. Y. Chushak, H. Shows, J. Gearhart, and H. Pangburn. In silico identification of
protein targets for chemical neurotoxins using toxcast in vitro data and read-across
within the qsar toolbox. Toxicol. Res. (Cambridge, U. K.), 7(3):423–431, 2018.

6. I. Cort´es-Ciriano, Q. U. Ain, V. Subramanian, E. B. Lenselink, O. M´endez-Lucio,
A. P. IJzerman, G. Wohlfahrt, P. Prusis, T. E. Malliavin, G. J. van Westen, and
A. Bender. Polypharmacology modelling using proteochemometrics (pcm): recent
methodological developments, applications to target families, and future prospects.
MedChemComm, 6(1):24–50, 2015.

7. G. E. Dahl, N. Jaitly, and R. Salakhutdinov. Multi-task neural networks for qsar

predictions. arXiv preprint arXiv:1406.1231, 2014.

8. M. I. Davis, J. P. Hunt, S. Herrgard, P. Ciceri, L. M. Wodicka, G. Pallares, M. Hocker,
D. K. Treiber, and P. P. Zarrinkar. Comprehensive analysis of kinase inhibitor
selectivity. Nat. Biotechnol., 29(11):1046–1051, 2011.

9. H. Ding, I. Takigawa, H. Mamitsuka, and S. Zhu. Similarity-based machine learning
methods for predicting drug–target interactions: A brief review. Briefings Bioinf.,
15(5):734–747, 2014.

10. D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-
Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing systems, pages 2224–2232,
2015.

11. J. Gomes, B. Ramsundar, E. N. Feinberg, and V. S. Pande. Atomic convolutional net-
works for predicting protein-ligand binding affinity. arXiv preprint arXiv:1703.10603,
2017.

12. I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.

http://www.deeplearningbook.org.

13. T. He, M. Heidemeyer, F. Ban, A. Cherkasov, and M. Ester. Simboost: A read-
across approach for predicting drug–target binding affinities using gradient boosting
machines. J. Cheminf., 9(1):24, 2017.

14. J. Jimenez. pygpgo: Bayesian optimization for python.

25/29

15. S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. Molecular graph
convolutions: Moving beyond fingerprints. J. Comput.-Aided Mol. Des., 30(8):595–
608, 2016.

16. S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li, B. A. Shoemaker, P. A.
Thiessen, B. Yu, L. Zaslavsky, J. Zhang, and E. E. Bolton. Pubchem 2019 update:
improved access to chemical data. Nucleic Acids Res., 47(D1):D1102–D1109, 2018.

17. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980, 2014.

18. G. Landrum. Rdkit: Open-source cheminformatics.

19. Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature, 521(7553):436–444,

2015.

20. E. B. Lenselink, N. Ten Dijke, B. Bongers, G. Papadatos, H. W. Van Vlijmen,
W. Kowalczyk, A. P. IJzerman, and G. J. Van Westen. Beyond the hype: deep neural
networks outperform established methods using a chembl bioactivity benchmark set.
J. Cheminf., 9(1):45, 2017.

21. J. Liu, K. Mansouri, R. S. Judson, M. T. Martin, H. Hong, M. Chen, X. Xu, R. S.
Thomas, and I. Shah. Predicting hepatotoxicity using toxcast in vitro bioactivity
and chemical structure. Chem. Res. Toxicol., 28(4):738–751, 2015.

22. J. Ma, R. P. Sheridan, A. Liaw, G. E. Dahl, and V. Svetnik. Deep neural nets as
a method for quantitative structure–activity relationships. J. Chem. Inf. Model.,
55(2):263–274, 2015.

23. K. Mansouri, A. Abdelaziz, A. Rybacka, A. Roncaglioni, A. Tropsha, A. Varnek,
A. Zakharov, A. Worth, A. M. Richard, and C. M. Grulke. Cerapp: Collaborative
estrogen receptor activity prediction project. Environ. Health Perspect., 124(7):1023,
2016.

24. M. Mathea, W. Klingspohn, and K. Baumann. Chemoinformatic classification

methods and their applicability domain. Mol. Inf., 35(5):160–180, 2016.

25. A. Mayr, G. Klambauer, T. Unterthiner, and S. Hochreiter. Deeptox: Toxicity
prediction using deep learning. Frontiers in Environmental Science, 3:80, 2016.

26. A. Mayr, G. Klambauer, T. Unterthiner, M. Steijaert, J. K. Wegner, H. Ceulemans,
D.-A. Clevert, and S. Hochreiter. Large-scale comparison of machine learning
methods for drug target prediction on chembl. Chem. Sci., 2018.

27. J. T. Metz, E. F. Johnson, N. B. Soni, P. J. Merta, L. Kifle, and P. J. Hajduk.

Navigating the kinome. Nat. Chem. Biol., 7(4):200, 2011.

28. A. Mina, R. Yoder, and P. Sharma. Targeting the androgen receptor in triple-negative

breast cancer: Current perspectives. OncoTargets Ther., 10:4675, 2017.

26/29

29. Z. Mousavian, S. Khakabimamaghani, K. Kavousi, and A. Masoudi-Nejad. Drug–
target Interaction Prediction From PSSM Based Evolutionary Information. J.
Pharmacol. Toxicol. Methods, 78:42–51, 2016.

30. J. Munoz, J. J. Wheler, and R. Kurzrock. Androgen receptors beyond prostate

cancer: an old marker as a new target. Oncotarget, 6(2):592, 2015.

31. National Cancer Institute. Developmental therapeutics program, division of cancer
treatment and diagnosis. https://dtp.cancer.gov/. Accessed: 2019-01-17.

32. H. ¨Ozt¨urk, E. Ozkirimli, and A. ¨Ozg¨ur. DeepDTA: Deep drug-target binding affinity

prediction. arXiv preprint arXiv:1801.10193, 2018.

33. T. Pahikkala, A. Airola, S. Pietil¨a, S. Shakyawar, A. Szwajda, J. Tang, and T. Ait-
tokallio. Toward more realistic drug–target interaction predictions. Briefings Bioinf.,
16(2):325–337, 2014.

34. T. Qiu, J. Qiu, J. Feng, D. Wu, Y. Yang, K. Tang, Z. Cao, and R. Zhu. The recent
progress in proteochemometric modelling: Focusing on target descriptors, cross-term
descriptors and application scope. Briefings Bioinf., page bbw004, 2016.

35. B. Ramsundar, P. Eastman, K. Leswing, P. Walters, and V. Pande. Deep Learn-
https://www.amazon.com/

ing for the Life Sciences. O’Reilly Media, 2019.
Deep-Learning-Life-Sciences-Microscopy/dp/1492039837.

36. D. Rogers and M. Hahn. Extended-connectivity fingerprints. J. Chem. Inf. Model.,

50(5):742–754, 2010.

37. K. Roy, S. Kar, and R. N. Das. Chapter 7 - validation of QSAR models. In K. Roy,
S. Kar, and R. N. Das, editors, Understanding the Basics of QSAR for Applications
in Pharmaceutical Sciences and Risk Assessment, pages 231 – 289. Academic Press,
Boston, 2015.

38. J. Schmidhuber. Deep learning in neural networks: An overview. Neural networks,

61:85–117, 2015.

39. B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. De Freitas. Taking the
human out of the loop: A review of bayesian optimization. Proc. IEEE, 104(1):148–
175, 2016.

40. A. Sharma, J. Lyons, A. Dehzangi, and K. K. Paliwal. A feature extraction
technique using bi-gram probabilities of position specific scoring matrix for protein
fold recognition. J. Theor. Biol., 320:41–46, 2013.

41. R. H. Shoemaker. The nci60 human tumour cell line anticancer drug screen. Nat.

Rev. Cancer, 6(10):813–823, 2006.

42. J. S. Smith, O. Isayev, and A. E. Roitberg. Ani-1: An extensible neural network
potential with dft accuracy at force field computational cost. Chem. Sci., 8(4):3192–
3203, 2017.

27/29

43. R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv preprint

arXiv:1505.00387, 2015.

44. I. Sushko, S. Novotarskyi, R. Korner, A. K. Pandey, A. Cherkasov, J. Li, P. Gramatica,
K. Hansen, T. Schroeter, and K.-R. Muller. Applicability domains for classification
problems: benchmarking of distance to models for Ames mutagenicity set. J. Chem.
Inf. Model., 50(12):2094–2111, 2010.

45. J. Tang, A. Szwajda, S. Shakyawar, T. Xu, P. Hintsanen, K. Wennerberg, and
T. Aittokallio. Making sense of large-scale kinase inhibitor bioactivity data sets: A
comparative and integrative analysis. J. Chem. Inf. Model., 54(3):735–743, 2014.

46. T. Unterthiner, A. Mayr, G. Klambauer, M. Steijaert, J. K. Wegner, H. Ceulemans,
and S. Hochreiter. Multi-task deep networks for drug target prediction. In Neural
Information Processing System, pages 1–4, 2014.

47. US, EPA.

Summary files

https://www.epa.gov/
chemical-research/toxicity-forecaster-toxcasttm-data, 10 2015. Accessed:
2018-03-22.

from invitrodb v2.

48. US, EPA. Toxicity forecaster (toxcast) fact sheet. https://www.epa.gov/sites/
production/files/2016-12/documents/tox_cast_fact_sheet_dec2016.pdf,
2016. Accessed: 2018-03-22.

49. G. J. van Westen, J. K. Wegner, A. P. IJzerman, H. W. van Vlijmen, and A. Ben-
der. Proteochemometric modeling as a tool to design selective compounds and for
extrapolating to novel targets. MedChemComm, 2(1):16–30, 2011.

50. I. Wallach, M. Dzamba, and A. Heifets. Atomnet: A deep convolutional neural
network for bioactivity prediction in structure-based drug discovery. arXiv preprint
arXiv:1510.02855, 2015.

51. M. Wen, Z. Zhang, S. Niu, H. Sha, R. Yang, Y. Yun, and H. Lu. Deep-learning-based
drug–target interaction prediction. J. Proteome Res., 16(4):1401–1409, 2017.

52. J. Wikberg, M. Lapinsh, and P. Prusis. Proteochemometrics: a tool for modeling
the molecular interaction space. Chemogenomics in drug discovery, pages 289–309,
2004.

53. Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu,
K. Leswing, and V. Pande. Moleculenet: A benchmark for molecular machine
learning. Chem. Sci., 9(2):513–530, 2018.

54. Y. Yamanishi, M. Araki, A. Gutteridge, W. Honda, and M. Kanehisa. Prediction
of drug-target interaction networks from the integration of chemical and genomic
spaces. Bioinformatics, 24(13):232–240, 2008.

55. T. A. Yap, A. D. Smith, R. Ferraldeschi, B. Al-Lazikani, P. Workman, and J. S.
de Bono. Drug discovery in advanced prostate cancer: Translating biology into
therapy. Nat. Rev. Drug Discovery, 15(10):699, 2016.

28/29

56. Y. 徐优俊 Xu and J. 裴剑锋 Pei. 深度学习在化学信息学中的应用(the application
of deep learning in cheminformatics). 大数据(Big Data), 3(2):2017019, 2017.

29/29

