ARES: Adaptive Receding-Horizon
Synthesis of Optimal Plans

Anna Lukina1, Lukas Esterle1, Christian Hirsch1, Ezio Bartocci1,
Junxing Yang2, Ashish Tiwari3, Scott A. Smolka2, and Radu Grosu1,2

1 Cyber-Physical Systems Group, Technische Universit¨at Wien, Austria
2 Department of Computer Science, Stony Brook University, USA
3 SRI International, USA

Abstract. We introduce ARES, an eﬃcient approximation algorithm
for generating optimal plans (action sequences) that take an initial state
of a Markov Decision Process (MDP) to a state whose cost is below a
speciﬁed (convergence) threshold. ARES uses Particle Swarm Optimiza-
tion, with adaptive sizing for both the receding horizon and the particle
swarm. Inspired by Importance Splitting, the length of the horizon and
the number of particles are chosen such that at least one particle reaches
a next-level state, that is, a state where the cost decreases by a required
delta from the previous-level state. The level relation on states and the
plans constructed by ARES implicitly deﬁne a Lyapunov function and an
optimal policy, respectively, both of which could be explicitly generated
by applying ARES to all states of the MDP, up to some topological equiv-
alence relation. We also assess the eﬀectiveness of ARES by statistically
evaluating its rate of success in generating optimal plans. The ARES
algorithm resulted from our desire to clarify if ﬂying in V-formation is
a ﬂocking policy that optimizes energy conservation, clear view, and ve-
locity alignment. That is, we were interested to see if one could ﬁnd
optimal plans that bring a ﬂock from an arbitrary initial state to a state
exhibiting a single connected V-formation. For ﬂocks with 7 birds, ARES
is able to generate a plan that leads to a V-formation in 95% of the 8,000
random initial conﬁgurations within 63 seconds, on average. ARES can
also be easily customized into a model-predictive controller (MPC) with
an adaptive receding horizon and statistical guarantees of convergence.
To the best of our knowledge, our adaptive-sizing approach is the ﬁrst
to provide convergence guarantees in receding-horizon techniques.

1

Introduction

Flocking or swarming in groups of social animals (birds, ﬁsh, ants, bees, etc.)
that results in a particular global formation is an emergent collective behavior
that continues to fascinate researchers [1, 8]. One would like to know if such a
formation serves a higher purpose, and, if so, what that purpose is.

One well-studied ﬂight-formation behavior is V-formation. Most of the work
in this area has concentrated on devising simple dynamical rules that, when fol-
lowed by each bird, eventually stabilize the ﬂock to the desired V-formation [12,

6
1
0
2
c
e
D
1
2

]
I

A
.
s
c
[

1
v
9
5
0
7
0
.
2
1
6
1
:
v
i
X
r
a

 
 
 
 
 
 
2

Lukina, Esterle, Hirsch, Bartocci, Yang, Tiwari, Smolka, Grosu

13, 26]. This approach, however, does not shed very much light on the overall
purpose of this emergent behavior.

In previous work [35,36], we hypothesized that ﬂying in V-formation is noth-
ing but an optimal policy for a ﬂocking-based Markov Decision Process (MDP)
M. States of M, at discrete time t, are of the form (xi(t), vi(t)), 1 (cid:54) i (cid:54) N ,
where xi(t) and vi(t) are N -vectors (for an N -bird ﬂock) of 2-dimensional posi-
tions and velocities, respectively. M’s transition relation, shown here for bird i
is simply and generically given by

xi(t + 1) = xi(t) + vi(t + 1),
vi(t + 1) = vi(t) + ai(t),

where ai(t) is an action, a 2-dimensional acceleration in this case, that bird i
can take at time t. M’s cost function reﬂects the energy-conservation, velocity-
alignment and clear-view beneﬁts enjoyed by a state of M (see Section 2).

In this paper, we not only conﬁrm this hypothesis, but we also devise a very
general adaptive, receding-horizon synthesis algorithm (ARES) that, given an
MDP and one of its initial states, generates an optimal plan (action sequence)
taking that state to a state whose cost is below a desired threshold. In fact,
ARES implicitly deﬁnes an optimal, online-policy, synthesis algorithm that could
be used in practice if plan generation can be performed in real-time.

ARES makes repeated use of Particle Swarm Optimization (PSO) [22] to ef-
fectively generate a plan. This was in principle unnecessary, as one could generate
an optimal plan by calling PSO only once, with a maximum plan-length horizon.
Such an approach, however, is in most cases impractical, as every unfolding of
the MDP adds a number of new dimensions to the search space. Consequently,
to obtain an adequate coverage of this space, one needs a very large number of
particles, a number that is either going to exhaust available memory or require
a prohibitive amount of time to ﬁnd an optimal plan.

A simple solution to this problem would be to use a short horizon, typically of
size two or three. This is indeed the current practice in Model Predictive Control
(MPC) [14]. This approach, however, has at least three major drawbacks. First,
and most importantly, it does not guarantee convergence and optimality, as one
may oscillate or become stuck in a local optimum. Second, in some of the steps,
the window size is unnecessarily large thereby negatively impacting performance.
Third, in other steps, the window size may be not large enough to guide the
optimizer out of a local minimum (see Fig. 1 (left)). One would therefore like to
ﬁnd the proper window size adaptively, but the question is how one can do it.

Inspired by Importance Splitting (IS), a sequential Monte-Carlo technique for
estimating the probability of rare events, we introduce the notion of a level-based
horizon (see Fig. 1 (right)). Level (cid:96)0 is the cost of the initial state, and level (cid:96)m is
the desired threshold. By using a state function, asymptotically converging to the
desired threshold, we can determine a sequence of levels, ensuring convergence
of ARES towards the desired optimal state(s) having a cost below (cid:96)m = ϕ.

The levels serve two purposes. First, they implicitly deﬁne a Lyapunov func-
tion, which guarantees convergence. If desired, this function can be explicitly

ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans

3

Cost

(cid:96)0

(cid:96)1

(cid:96)i

(cid:96)i+1

s0 s1

. . .

si

. . .

si+3

Level

(cid:96)0

(cid:96)1
.
.
.

(cid:96)i

(cid:96)i+1
.
.
.

(cid:96)m

State

s0s1 sisi+3

. . .

ϕ

s∗

State

Fig. 1. Left: If state s0 has cost (cid:96)0, and its successor-state s1 has cost less than (cid:96)1, then
a horizon of length 1 is appropriate. However, if si has a local-minimum cost (cid:96)i, one
has to pass over the cost ridge in order to reach level (cid:96)i+1, and therefore ARES has to
adaptively increase the horizon to 3. Right: The cost of the initial state deﬁnes (cid:96)0 and
the given threshold ϕ deﬁnes (cid:96)m. By choosing m equal segments on an asympthotically
converging (Lyapunov) function (where the number m is empirically determined), one
obtains on the vertical cost-axis the levels required for ARES to converge.

generated for all states, up to some topological equivalence. Second, the levels
help PSO overcome local minima (see Fig. 1 (left)). If reaching a next level
requires PSO to temporarily pass over a state-cost ridge, ARES incrementally
increases the size of the horizon, up to a maximum length.

Another idea imported from IS is to maintain n clones of the initial state at
a time, and run PSO on each of them (see Fig. 3). This allows us to call PSO
for each clone and desired horizon, with a very small number of particles per
clone. Clones that do not reach the next level are discarded, and the successful
ones are resampled. The number of particles is increased if no clone reaches a
next level, for all horizons chosen. Once this happens, we reset the horizon to
one, and repeat the process. In this way, we adaptively focus our resources on
escaping from local minima. At the last level, we choose the optimal particle (a
V-formation in case of ﬂocking) and traverse its predecessors to ﬁnd a plan.

We asses the rate of success in generating optimal plans in form of an (ε, δ)-
approximation scheme, for a desired error margin ε, and conﬁdence ratio 1−δ.
Moreover, we can use the state-action pairs generated during the assessment (and
possibly some additional new plans) to construct an explicit (tabled) optimal
policy, modulo some topological equivalence. Given enough memory, one can
use this policy in real time, as it only requires a table look-up.

To experimentally validate our approach, we have applied ARES to the prob-
lem of V-formation in bird ﬂocking (with a deterministic MDP). The cost func-
tion to be optimized is deﬁned as a weighted sum of the (ﬂock-wide) clear-view,
velocity-alignment, and upwash-beneﬁt metrics. Clear view and velocity align-
ment are more or less obvious goals. Upwash optimizes energy savings. By ﬂap-
ping its wings, a bird generates a trailing upwash region oﬀ its wing tips; by
using this upwash, a bird ﬂying in this region (left or right) can save energy.

4

Lukina, Esterle, Hirsch, Bartocci, Yang, Tiwari, Smolka, Grosu

Note that by requiring that at most one bird does not feel its eﬀect, upwash can
be used to deﬁne an analog version of a connected graph.

We ran ARES on 8,000 initial states chosen uniformly and at random, such
that they are packed closely enough to feel upwash, but not too close to collide.
We succeeded to generate a V-formation 95% of the time, with an error margin
of 0.05 and a conﬁdence ratio of 0.99. These error margin and conﬁdence ratio
dramatically improve if we consider all generated states and the fact that each
state within a plan is independent from the states in all other plans.

The rest of this paper is organized as follows. Section 2 reviews our work on
bird ﬂocking and V-formation, and deﬁnes the manner in which we measure the
cost of a ﬂock (formation). Section 3 revisits the swarm optimization algorithm
used in this paper, and Section 4 examines the main characteristics of importance
splitting. Section 5 states the deﬁnition of the problem we are trying to solve.
Section 6 introduces ARES, our adaptive receding-horizon synthesis algorithm
for optimal plans, and discusses how we can extend this algorithm to explicitly
generate policies. Section 7 measures the eﬃciency of ARES in terms of an (ε, δ)-
approximation scheme. Section 8 compares our algorithm to related work, and
Section 9 draws our conclusions and discusses future work.

2 V-Formation MDP

We represent a ﬂock of birds as a dynamically evolving system. Every bird in
our model [17] moves in 2-dimensional space performing acceleration actions
determined by a global controller. Let xi(t), vi(t) and ai(t) be 2-dimensional
vectors of positions, velocities, and accelerations, respectively, of bird i at time
t, where i ∈ {1, . . . , b}, for a ﬁxed b. The discrete-time behavior of bird i is then

xi(t + 1) = xi(t) + vi(t + 1),
vi(t + 1) = vi(t) + ai(t).

(1)

The controller detects the positions and velocities of all birds through sensors,
and uses this information to compute an optimal acceleration for the entire ﬂock.
A bird uses its own component of the solution to update its velocity and position.
We extend this discrete-time dynamical model to a (deterministic) MDP by
adding a cost (ﬁtness) function4 based on the following metrics inspired by [35]:

– Clear View (CV ). A bird’s visual ﬁeld is a cone with angle θ that can be
blocked by the wings of other birds. We deﬁne the clear-view metric by
accumulating the percentage of a bird’s visual ﬁeld that is blocked by other
birds. Fig. 2 (left) illustrates the calculation of the clear-view metric. The
optimal value in a V-formation is CV ∗= 0, as all birds have a clear view.
– Velocity Matching (VM ). The accumulated diﬀerences between the velocity
of each bird and all other birds, summed up over all birds in the ﬂock deﬁnes
VM . Fig. 2 (middle) depicts the values of VM in a velocity-unmatched ﬂock.

4 A classic MDP [28] is obtained by adding sensor/actuator or wind-gust noise.

ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans

5

Fig. 2.
Illustration of the clear view (CV ), velocity matching (VM ), and upwash
beneﬁt (UB ) metrics. Left: Bird i’s view is partially blocked by birds j and k. Hence,
its clear view is CV = (α + β)/θ. Middle: A ﬂock and its unaligned bird velocities results
in a velocity-matching metric VM = 6.2805. In contrast, VM = 0 when the velocities
of all birds are aligned. Right: Illustration of the (right-wing) upwash beneﬁt bird i
receives from bird j depending on how it is positioned behind bird j. Note that bird
j’s downwash region is directly behind it.

The optimal value in a V-formation is VM ∗= 0, as all birds will have the
same velocity (thus maintaining the V-formation).

– Upwash Beneﬁt (UB ). The trailing upwash is generated near the wingtips of
a bird, while downwash is generated near the center of a bird. We accumulate
all birds’ upwash beneﬁts using a Gaussian-like model of the upwash and
downwash region, as shown in Fig. 2 (right) for the right wing. The maximum
upwash a bird can obtain has an upper bound of 1. For bird i with UB i, we
use 1 −UB i as its upwash-beneﬁt metric, because the optimization algorithm
performs minimization of the ﬁtness metrics. The optimal value in a V-
formation is UB ∗= 1, as the leader does not receive any upwash.

Finding smooth and continuous formulations of the ﬁtness metrics is a key ele-
ment of solving optimization problems. The PSO algorithm has a very low prob-
ability of ﬁnding an optimal solution if the ﬁtness metric is not well-designed.

Let c(t) = {ci(t)}b

i=1 be a ﬂock conﬁguration at time-step
t. Given the above metrics, the overall ﬁtness (cost) metric J is of a sum-of-
squares combination of VM , CV , and UB deﬁned as follows:

i=1 = {xi(t), vi(t)}b

J(c(t), ah(t), h) = (CV (ch

a(t)) − CV ∗)2 + (VM (ch
+ (UB (ch

a(t)) − VM ∗)2
a(t)) − UB ∗)2,

(2)

where h is the receding prediction horizon (RPH), ah(t) is a sequence of accel-
a(t) is the conﬁguration reached after applying ah(t)
erations of length h, and ch
to c(t). Formally, we have

a(t) = {xh
ch

a(t), vh

a(t)} = {x(t) +

h(t)
(cid:88)

τ =1

v(t + τ ), v(t) +

h(t)
(cid:88)

τ =1

aτ (t)},

(3)

where aτ (t) is the τ th acceleration of ah(t). A novelty of this paper is that, as
described in Section 6, we allow RPH h(t) to be adaptive in nature.

6

Lukina, Esterle, Hirsch, Bartocci, Yang, Tiwari, Smolka, Grosu

The ﬁtness function J has an optimal value of 0 in a perfect V-formation.
The main goal of ARES is to compute the sequence of acceleration actions
that lead the ﬂock from a random initial conﬁguration towards a controlled V-
formation characterized by optimal ﬁtness in order to conserve energy during
ﬂight including optimal combination of a clear visual ﬁeld along with visibility
of lateral neighbors. Similar to the centralized version of the approach given
in [35], ARES performs a single ﬂock-wide minimization of J at each time-step
t to obtain an optimal plan of length h of acceleration actions:

opt-ah(t) = {opt-ah

i (t)}b

i=1 = arg min

ah(t)

J(c(t), ah(t), h).

(4)

The optimization is subject to the following constraints on the maximum
i (t)|| (cid:54) ρ||vi(t)|| ∀ i ∈ {1, . . . , b},
velocities and accelerations: ||vi(t)|| (cid:54) vmax, ||ah
where vmax is a constant and ρ ∈ (0, 1). The initial positions and velocities of
each bird are selected at random within certain ranges, and limited such that
the distance between any two birds is greater than a (collision) constant dmin,
and small enough for all birds, except for at most one, to feel the UB . In the
following sections, we demonstrate how to generate optimal plans taking the
initial state to a stable state with optimal ﬁtness.

3 Particle Swarm Optimization

Particle Swarm Optimization (PSO) is a randomized approximation algorithm
for computing the value of a parameter minimizing a possibly nonlinear cost
(ﬁtness) function. Interestingly, PSO itself is inspired by bird ﬂocking [22]. Hence,
PSO assumes that it works with a ﬂock of birds.

Note, however, that in our running example, these birds are “acceleration
birds” (or particles), and not the actual birds in the ﬂock. Each bird has the
same goal, ﬁnding food (reward), but none of them knows the location of the
food. However, every bird knows the distance (horizon) to the food location.
PSO works by moving each bird preferentially toward the bird closest to food.
ARES uses Matlab-Toolbox particleswarm, which performs the classical
version of PSO. This PSO creates a swarm of particles, of size say p, uniformly
at random within a given bound on their positions and velocities. Note that in
our example, each particle represents itself a ﬂock of bird-acceleration sequences
{ah
i=1, where h is the current length of the receding horizon. PSO further
chooses a neighborhood of a random size for each particle j, j = {1, . . . , p}, and
computes the ﬁtness of each particle. Based on the ﬁtness values, PSO stores two
vectors for j: its so-far personal-best position xj
P (t), and its ﬁttest neighbor’s
position xj
G(t). The positions and velocities of each particle j in the particle
swarm 1 (cid:54) j (cid:54) p are updated according to the following rule:

i }b

vj(t + 1) = ω · vj(t) + y1 · u1(t + 1) ⊗ (xj
+ y2 · u2(t + 1) ⊗ (xj

P (t) − xj(t))
G(t) − xj(t)),

(5)

ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans

7

where ω is inertia weight, which determines the trade-oﬀ between global and
local exploration of the swarm (the value of ω is proportional to the explo-
ration range); y1 and y2 are self adjustment and social adjustment, respectively;
u1, u2 ∈ Uniform(0, 1) are randomization factors; and ⊗ is the vector dot prod-
uct, that is, ∀ random vector z: (z1, . . . , zb) ⊗ (xj

1, . . . , zbxj

b) = (z1xj

If the ﬁtness value for xj(t + 1) = xj(t) + vj(t + 1) is lower than the one
for xj
P (t), then xj(t + 1) is assigned to xj
P (t + 1). The particle with the best
ﬁtness over the whole swarm becomes a global best for the next iteration. The
procedure is repeated until the number of iterations reaches its maximum, the
time elapses, or the minimum criteria is satisﬁed. For our bird-ﬂock example we
obtain in this way the best acceleration.

1, . . . , xj

b).

4

Importance Splitting

Importance Splitting (IS) is a sequential Monte-Carlo approximation technique
for estimating the probability of rare events in a Markov process [7]. The algo-
rithm uses a sequence S0, S1, S2, . . . , Sm of sets of states (of increasing “impor-
tance”) such that S0 is the set of initial states and Sm is the set of states deﬁning
the rare event. The probability p, computed as P(Sm | S0) of reaching Sm from
the initial set of states S0, is assumed to be extremely low (thus, a rare event),
and one desires to estimate this probability [16]. Random sampling approaches,
such as the additive-error approximation algorithm described in Section 7, are
bound to fail (are intractable) in this case, as they would require an enormous
number of samples to estimate p with low-variance.

Importance splitting is a way of decomposing the estimation of p. In IS, the
sequence S0, S1, . . . of sets of states is deﬁned so that the conditional proba-
bilities pi = P(Si | Si−1) of going from one level, Si−1, to the next one, Si, are
considerably larger than p, and essentially equal to one another. The resulting
probability of the rare event is then calculated as the product p = (cid:81)k
i=1 pi of the
intermediate probabilities. The levels can be deﬁned adaptively [23].

To estimate pi, IS uses a swarm of particles of size N , with a given initial
distribution over the states of the stochastic process. During stage i of the algo-
rithm, each particle starts at level Si−1 and traverses the states of the stochastic
process, checking if it reaches Si. If, at the end of the stage, the particle fails to
reach Si, the particle is discarded. Suppose that Ki particles survive. In this case,
pi = Ki/N . Before starting the next stage, the surviving particles are resampled,
such that IS once again has N particles. Whereas IS is used for estimating prob-
ability of a rare event in a Markov process, we use it here for synthesizing a plan
for a controllable Markov process, by combining it with ideas from controller
synthesis (receding-horizon control) and nonlinear optimization (PSO).

5 Problem Deﬁnition

Deﬁnition 1. A Markov decision process (MDP) M is a sequential deci-
sion problem that consists of a set of states S (with an initial state s0), a set of

8

Lukina, Esterle, Hirsch, Bartocci, Yang, Tiwari, Smolka, Grosu

actions A, a transition model T , and a cost function J. An MDP is determin-
istic if for each state and action, T : S × A → S speciﬁes a unique state.

Deﬁnition 2. The optimal plan synthesis problem for an MDP M, an
arbitrary initial state s0 of M, and a threshold ϕ is to synthesize a sequence of
actions ai of length 1 (cid:54) i (cid:54) m taking s0 to a state s∗ such that cost J(s∗) (cid:54) ϕ.

Section 6 presents our adaptive receding-horizon synthesis algorithm (ARES)
for the optimal plan synthesis problem. In our ﬂocking example (Section 2),
ARES is used to synthesize a sequence of acceleration-actions bringing an arbi-
trary bird ﬂock s0 to an optimal state of V-formation s∗. We assume that we
can easily extend such an optimal plan to maintain the cost of successor states
below ϕ ad inﬁnitum (optimal stability).

6 The ARES Algorithm for Plan Synthesis

As mentioned in Section 1, one could in principle solve the optimization problem
deﬁned in Section 5 by calling the PSO only once, with a horizon h in M equaling
the maximum length m allowed for a plan. This approach, however, tends to
explode the search space, and is therefore in most cases intractable. Indeed,
preliminary experiments with this technique applied to our running example
could not generate any convergent plan.

A more tractable approach is to make repeated calls to PSO with a small
horizon length h. The question is how small h can be. The current practice
in model-predictive control (MPC) is to use a ﬁxed h, 1 (cid:54) h (cid:54) 3 (see the outer
loop of Fig. 3, where resampling and conditional branches are disregarded).
Unfortunately, this forces the selection of locally-optimal plans (of size less than
three) in each call, and there is no guarantee of convergence when joining them
together. In fact, in our running example, we were able to ﬁnd plans leading to
a V-formation in only 45% of the time for 10, 000 random initial ﬂocks.

Inspired by IS (see Fig. 1 (right) and Fig. 3), we introduce the notion of a
level-based horizon, where level (cid:96)0 equals the cost of the initial state, and level
(cid:96)m equals the threshold ϕ. Intuitively, by using an asymptotic cost-convergence
function ranging from (cid:96)0 to (cid:96)m, and dividing its graph in m equal segments, we
can determine on the vertical axis a sequence of levels ensuring convergence.

The asymptotic function ARES implements is essentially (cid:96)i = (cid:96)0 (m − i)/ m,
but speciﬁcally tuned for each particle. Formally, if particle k has previously
reached level equaling Jk(si−1), then its next target level is within the distance
∆k = Jk(si−1)/(m − i + 1). In Fig. 3, after passing the thresholds assigned to
them, values of the cost function in the current state si are sorted in ascending
order { (cid:98)Jk}n
k=1. The lowest cost (cid:98)J1 should be apart from the previous level (cid:96)i−1
at least on its ∆1 for the algorithm to proceed to the next level (cid:96)i := (cid:98)J1.

The levels serve two purposes. First, they implicitly deﬁne a Lyapunov func-
tion, which guarantees convergence. If desired, this function can be explicitly
generated for all states, up to some topological equivalence. Second, the levels
(cid:96)i help PSO overcome local minima (see Fig. 1 (left)). If reaching a next level

ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans

9

next states after
applying {ah

k }n

k=1

n clones
s0

s0

si

ah
1

ah
2

ah
3

PSO

PSO

PSO

...

...

...

ah
n

PSO

J1

J2

J3

...

Jn

{ (cid:98)Jk}n

k=1

Sort

(cid:96)i−1 − (cid:98)J1 > ∆1

Yes

No

Yes

h ++

h < hmax

Resampling

I

...

...

i ++

Yes

i < m

No

Timeout

h := 1;

Yes

p += pinc;

No

p < pmax

No

Particle exhaustion

Yes

(cid:96)i > ϕ

No

Stable state

(cid:96)i := (cid:98)J1

Fig. 3. Graphical representation of ARES.

requires PSO to temporarily pass over a state-cost ridge, then ARES incremen-
tally increases the size of the horizon h, up to a maximum size hmax. For particle
k, passing the thresholds ∆k means that it reaches a new level, and the deﬁnition
of ∆k ensures a smooth degradation of its threshold.

Another idea imported from IS and shown in Fig. 3, is to maintain n clones
k=1 of the MDP M (and its initial state) at any time t, and run PSO, for
k of them. This results in an action sequence
k of length h (see Algo. 1). This approach allows us to call PSO for each clone

{Mk}n
a horizon h, on each h-unfolding Mh
ah
and desired horizon, with a very small number of particles p per clone.

k=1)

Algorithm 1: Simulate (M, h, i, {∆k, Jk(si−1)}n
1 foreach Mk ∈ M do
k, Mh
2

[ah
best next action for the MDP Mk with RPH h
Jk(si) ← Cost(Mh
k, ah
sequence of optimal actions of length h
if Jk(si−1) − Jk(si) > ∆k then

∆k ← Jk(si)/(m − i); // new level-threshold

3

4

5

k] ← particleswarm(Mk, p, h); // use PSO in order to determine

k, h); // calculate cost function if applying the

end

6
7 end

10

Lukina, Esterle, Hirsch, Bartocci, Yang, Tiwari, Smolka, Grosu

Algorithm 2: Resample ({Mh

k, Jk(si)}n

k=1)

1 I ← Sort ascending Mh

k by their current costs; // ﬁnd indexes of MDPs whose

costs are below the median among all the clones

2 for k = 1 to n do
if k /∈ I then
3

4

5

6

Sample r uniformly at random from I; Mk ← Mh
r ;

else

Mk ← Mh

k; // Keep more successful MDPs unchanged

end

7
8 end

To check which particles have overcome their associated thresholds, we sort
the particles according to their current cost, and split them in two sets: the
successful set, having the indexes I and whose costs are lower than the median
among all clones; and the unsuccessful set with indexes in {1, . . ., n} \I, which are
discarded. The unsuccessful ones are further replenished, by sampling uniformly
at random from the successful set I (see Algo. 2).

The number of particles is increased p = p + pinc if no clone reaches a next
level, for all horizons chosen. Once this happens, we reset the horizon to one, and
repeat the process. In this way, we adaptively focus our resources on escaping
from local minima. From the last level, we choose the state s∗ with the minimal
cost, and traverse all of its predecessor states to ﬁnd an optimal plan comprised
of actions {ai}1(cid:54)i(cid:54)m that led MDP M to the optimal state s∗. In our running
example, we select a ﬂock in V-formation, and traverse all its predecessor ﬂocks.
The overall procedure of ARES is shown in Algo. 3.

Proposition 1 (Optimality and Minimality). (1) Let M be an MDP. For
any initial state s0 of M, ARES is able to solve the optimal-plan synthesis
problem for M and s0. (2) An optimal choice of m in function ∆k, for some
particle k, ensures that ARES also generates the shortest optimal plan.

Proof (Sketch). (1) The dynamic-threshold function ∆k ensures that the initial
cost in s0 is continuously decreased until it falls below ϕ. Moreover, for an appro-
priate number of clones, by adaptively determining the horizon and the number
of particles needed to overcome ∆k, ARES always converges, with probability
1, to an optimal state, given enough time and memory. (2) This follows from
convergence property (1), and from the fact that ARES always gives preference
to the shortest horizon while trying to overcome ∆k.

The optimality referred to in the title of the paper is in the sense of (1).
One, however, can do even better than (1), in the sense of (2), by empirically
determining parameter m in the dynamic-threshold function ∆k. Also note that
ARES is an approximation algorithm. As a consequence, it might return non-
minimal plans. Even in these circumstances, however, the plans will still lead to
an optimal state. This is a V-formation in our ﬂocking example.

ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans

11

Algorithm 3: ARES

Input : M, ϕ, pstart, pinc, pmax, hmax, m, n
Output: {ai}1(cid:54)i(cid:54) m // synthesized optimal plans

k=1 ← inf; p ← pstart; i ← 1; h ← 1; ∆k ← 0;

1 Initialize (cid:96)0 ← inf; {Jk(s0)}n
2 while ((cid:96)i > ϕ) ∨ (i < m) do
3

k, Jk(si), Mh

// ﬁnd and apply best actions with RPH h
[{ah
(cid:98)J1 ← sort(J1(si), . . . , Jn(si)); // ﬁnd minimum cost among all the clones
if (cid:96)i−1 − (cid:98)J1 > ∆1 then

k=1] ←Simulate(M, h, i, {∆k, Jk(si−1)}n

k=1);

k}n

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

(cid:96)i ← (cid:98)J1; // new level has been reached
i ← i + 1; h ← 1; p ← pstart; // reset adaptive parameters
{Mk}n

k=1 ← Resample({Mh

k, Jk(si)}n

k=1);

else

if h < hmax then

h ← h + 1; // improve time exploration

else

if p < pmax then

h ← 1; p ← p + pinc; // improve space exploration

else

break;

end

end

end

19
20 end
21 Take a clone in the state with minimum cost (cid:96)i = J(s∗
22 foreach i do
{s∗
23
24 end

i−1, ai} ← P re(s∗

i ); // ﬁnd predecessor and corresponding action

i ) (cid:54) ϕ at the last level i;

7 Experimental Results

To assess the performance of our approach, we developed a simple simulation
environment in Matlab. All experiments were run on an Intel Core i7-5820K
CPU with 3.30 GHz and with 32GB RAM available.

We performed numerous experiments with a varying number of birds. Unless
stated otherwise, results refer to 8,000 experiments with 7 birds with the follow-
ing parameters: pstart = 10, pinc = 5, pmax = 40, (cid:96)max = 20, hmax = 5, ϕ = 10−3,
and n = 20. The initial conﬁgurations were generated independently uniformly
at random subject to the following constraints:

1. Position constraints: ∀ i ∈ {1, . . ., 7}. xi(0) ∈ [0, 3] × [0, 3].
2. Velocity constraints: ∀ i ∈ {1, . . ., 7}. vi(0) ∈ [0.25, 0.75] × [0.25, 0.75].

Table 1 gives an overview of the results with respect to the 8,000 experiments
we performed with 7 birds for a maximum of 20 levels. The average ﬁtness
across all experiments is at 0.0282 with a standard deviation of 0.1654. We

12

Lukina, Esterle, Hirsch, Bartocci, Yang, Tiwari, Smolka, Grosu

Fig. 4. Left: Example of an arbitrary initial conﬁguration of 7 birds. Right: The V-
formation obtained by applying the plan generated by ARES. In the ﬁgures, we show
the wings of the birds, bird orientations, bird speeds (as scaled arrows), upwash regions
in yellow, and downwash regions in dark blue.

Table 1. Overview of the results for 8,000 experiments with 7 birds

No. Experiments

Cost, J
Time, t
Plan Length, i
RPH, h

Successful

7573

Total

8000

Min

Max

Avg

Std

Min

Max

Avg

Std

2.88·10−7 9·10−4 4·10−4 3·10−4 2.88·10−7 1.4840 0.0282 0.1607
661.46s 64.85s 28.05s
13.13
1.27

310.83s 63.55s 22.81s
12.80
1.40

23.14s
7
1

23.14s
7
1

2.71
0.17

2.39
0.15

20
5

20
5

achieved a success rate of 94.66% with ﬁtness threshold ϕ = 10−3. The average
ﬁtness is higher than the threshold due to comparably high ﬁtness of unsuccessful
experiments. When increasing the bound for the maximal plan length m to 30 we
achieved a 98.4% success rate in 1,000 experiments at the expense of a slightly
longer average execution time.

The left plot in Fig. 5 depicts the resulting distribution of execution times for
8,000 runs of our algorithm, where it is clear that, excluding only a few outliers
from the histogram, an arbitrary conﬁguration of birds (Fig. 4 (left)) reaches
V-formation (Fig. 4 (right)) in around 1 minute. The execution time rises with
the number of birds as shown in Table 2.

In Fig. 5, we illustrate for how many experiments the algorithm had to in-
crease RPH h (Fig. 5 (middle)) and the number of particles used by PSO p
(Fig. 5 (right)) to improve time and space exploration, respectively.

After achieving such a high success rate of ARES for an arbitrary initial
conﬁguration, we would like to demonstrate that the number of experiments

Table 2. Average duration for 100 experiments with various number of birds

No. of birds

3

5

7

9

Avg. duration 4.58s 18.92s 64.85s 269.33s

ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans

13

Fig. 5. Left: Distribution of execution times for 8,000 runs. Middle: Statistics of in-
creasing RPH h. Right: Particles of PSO p for 8,000 experiments

performed is suﬃcient for high conﬁdence in our results. This requires us to
determine the appropriate number N of random variables Z1, ...ZN necessary
for the Monte-Carlo approximation scheme we apply to assess eﬃciency of our
approach. For this purpose, we use the additive approximation algorithm as
discussed in [17]. If the sample mean µZ = (Z1 + . . . + ZN )/N is expected to be
large, then one can exploit the Bernstein’s inequality and ﬁx N to Υ ∝ ln(1/δ)/ε2.
This results in an additive or absolute-error (ε, δ)-approximation scheme:

P[µZ − ε ≤ (cid:101)µZ ≤ µZ + ε)] ≥ 1 − δ,

where (cid:101)µZ approximates µZ with absolute error ε and probability 1 − δ.

In particular, we are interested in Z being a Bernoulli random variable:

Z =

(cid:26) 1, if J(c(t), a(t), h(t)) (cid:54) ϕ,

0, otherwise.

Therefore, we can use the Chernoﬀ-Hoeﬀding instantiation of the Bernstein’s
inequality, and further ﬁx the proportionality constant to Υ = 4 ln(2/δ)/ε2, as
in [20]. Hence, for our performed 8,000 experiments, we achieve a success rate of
95% with absolute error of ε = 0.05 and conﬁdence ratio 0.99.

Moreover, considering that the average length of a plan is 13, and that each
state in a plan is independent from all other plans, we can roughly consider
that our above estimation generated 80,000 independent states. For the same
conﬁdence ratio of 0.99 we then obtain an approximation error ε = 0.016, and
for a conﬁdence ratio of 0.999, we obtain an approximation error ε = 0.019.

8 Related Work

Organized ﬂight in ﬂocks of birds can be categorized in cluster ﬂocking and line
formation [19]. In cluster ﬂocking the individual birds in a large ﬂock seem to
be uncoordinated in general. However, the ﬂock moves, turns, and wheels as
if it were one organism. In 1987 Reynolds [27] deﬁned his three famous rules
describing separation, alignment, and cohesion for individual birds in order to
have them ﬂock together. This work has been great inspiration for research in
the area of collective behavior and self-organization.

14

Lukina, Esterle, Hirsch, Bartocci, Yang, Tiwari, Smolka, Grosu

In contrast, line formation ﬂight requires the individual birds to ﬂy in a very
speciﬁc formation. Line formation has two main beneﬁts for the long-distance
migrating birds. First, exploiting the generated uplift by birds ﬂying in front,
trailing birds are able to conserve energy [10, 24, 34]. Second, in a staggered
formation, all birds have a clear view in front as well as a view on their neigh-
bors [1]. While there has been quite some eﬀort to keep a certain formation for
multiple entities when traveling together [11, 15, 30], only little work deals with
a task of achieving this extremely important formation from a random starting
conﬁguration [6]. The convergence of bird ﬂocking into V-formation has been
also analyzed with the use of combinatorial techniques [8].

Compared to previous work, in [5] this question is addressed without using
any behavioral rules but as problem of optimal control. In [35] a cost func-
tion was proposed that reﬂects all major features of V-formation, namely, Clear
View (CV), Velocity Matching (VM), and Upwash Beneﬁt (UB). The technique
of MPC is used to achieve V-formation starting from an arbitrary initial con-
ﬁguration of n birds. MPC solves the task by minimizing a functional deﬁned
as squared distance from the optimal values of CV, VM, and UB, subject to
constraints on input and output. The approach is to choose an optimal velocity
adjustment, as a control input, at each time-step applied to the velocity of each
bird by predicting model behavior several time-steps ahead.

The controller synthesis problem has been widely studied [33]. The most pop-
ular and natural technique is Dynamic Programming (DP) [4] that improves the
approximation of the functional at each iteration, eventually converging to the
optimal one given a ﬁxed asymptotic error. Compared to DP, which considers all
the possible states of the system and might suﬀer from state-space explosion in
case of environmental uncertainties, approximate algorithms [2, 3, 18, 25, 31, 32]
take into account only the paths leading to desired target. One of the most ef-
ﬁcient ones is Particle Swarm Optimization (PSO) [22] that has been adopted
for ﬁnding the next best step of MPC in [35]. Although it is a very powerful
optimization technique, it has not yet been possible to achieve a high success
rate in solving the considered ﬂocking problem. Sequential Monte-Carlo methods
proved to be eﬃcient in tackling the question of control for linear stochastic sys-
tems [9], in particular, Importance Splitting (IS) [23]. The approach we propose
is, however, the ﬁrst attempt to combine adaptive IS, PSO, and receding-horizon
technique for synthesis of optimal plans for controllable systems. We use MPC
to synthesize a plan, but use IS to determine the intermediate ﬁtness-based way-
points. We use PSO to solve the multi-step optimization problem generated by
MPC, but choose the planning horizon and the number of particles adaptively.
These choices are governed by the diﬃculty to reach the next level.

9 Conclusion and Future Work

In this paper, we have presented ARES, a very general adaptive, receding-horizon
synthesis algorithm for MDP-based optimal plans. Additionally, ARES can be
readily converted into a model-predictive controller with an adaptive receding

ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans

15

horizon and statistical guarantees of convergence. We have also conducted a very
thorough performance analysis of ARES based on the problem of V-formation
in a ﬂock of birds. For ﬂocks of 7 birds, ARES is able to generate an optimal
plan leading to a V-formation in 95% of the 8,000 random initial conﬁgurations
we considered, with an average execution time of only 63 seconds per plan.

The execution time of the ARES algorithm can be even further improved in
a number of ways. First, we currently do not parallelize our implementation of
the PSO algorithm. Recent work [21, 29, 37] has shown how Graphic Process-
ing Units (GPUs) are very eﬃcient at accelerating PSO computation. Modern
GPUs, by providing thousands of cores, are well-suited for implementing PSO
as they enable execution of a very large number of particles in parallel, which
can improve accuracy of the optimization procedure. Likewise, the calculation
of the ﬁtness function can also be run in parallel. The parallelization of these
steps should signiﬁcantly speed up our simulations.

Second, we are currently using a static approach to decide how to increase
our prediction horizon and the number of particles used in PSO. Speciﬁcally, we
ﬁrst increase the prediction horizon from 1 to 5, while keeping the number of
particles unchanged at 10; if this fails to ﬁnd a solution with ﬁtness (cid:99)J1 satisfying
(cid:96)i−1 − (cid:99)J1 > ∆1, we then increase the number of particles by 5. Based on our
results, we speculate that in the initial stages, increasing the prediction horizon is
more beneﬁcial (leading rapidly to the appearance of cost-eﬀective formations),
whereas in the later stages, increasing the number of particles is more helpful. As
future work, we will use machine-learning approaches to decide on the prediction
horizon and the number of particles deployed at runtime given the current level
and state of the MDP.

Third, in our approach, we always calculate the number of clones for resam-
pling based on the current state. An alternative approach would rely on statistics
built up over multiple levels in combination with the rank in the sorted list to
determine whether a conﬁguration should be used for resampling or not.

Finally, we are currently using our approach to generate plans for a ﬂock
to go from an initial conﬁguration to a ﬁnal V-formation. Our eventual goal
is to achieve formation ﬂight for a robotic swarm of (bird-like) drones. A real-
world example is parcel-delivering drones that follow the same route to their
destinations. Letting them ﬂy together for a while could save energy and increase
ﬂight time. To achieve this goal, we ﬁrst need to investigate the wind dynamics
of multi-rotor drones. Then, the ﬁtness function needs to be adopted to the new
wind dynamics. Lastly, a decentralized approach of this method needs to be
implemented and tested on the drone ﬁrmware.

Acknowledgments. The ﬁrst author and the last author would like to thank
Jan K˘ret´ınsk´y for very valuable feedback. This work was partially supported
by the Doctoral Program Logical Methods in Computer Science funded by the
Austrian Science Fund (FWF) project W1255-N23, and the Austrian National
Research Network (nr. S 11405-N23 and S 11412-N23) SHiNE funded by FWF.

16

Lukina, Esterle, Hirsch, Bartocci, Yang, Tiwari, Smolka, Grosu

References

1. Bajec, I.L., Heppner, F.H.: Organized ﬂight in birds. Animal Behaviour 78(4),

777–789 (2009)

2. Bartocci, E., Bortolussi, L., Br´azdil, T., Milios, D., Sanguinetti, G.: Policy learning
for time-bounded reachability in continuous-time markov decision processes via
doubly-stochastic gradient ascent. In: Proc. of QEST 2016: the 13th International
Conference on Quantitative Evaluation of Systems. vol. 9826, pp. 244–259 (2016)
3. Baxter, J., Bartlett, P.L., Weaver, L.: Experiments with inﬁnite-horizon, policy-

gradient estimation. J. Artif. Int. Res. 15(1), 351–381 (2011)

4. Bellman, R.: Dynamic Programming. Princeton University Press (1957)
5. Camacho, E.F., Alba, C.B.: Model Predictive Control. Advanced Textbooks in

Control and Signal Processing, Springer (2007)

6. Cattivelli, F.S., Sayed, A.H.: Modeling bird ﬂight formations using diﬀusion adap-

tation. IEEE Transactions on Signal Processing 59(5), 2038–2051 (2011)

7. C´erou, F., Guyader, A.: Adaptive multilevel splitting for rare event analysis.

Stochastic Analysis and Applications 25, 417–443 (2007)

8. Chazelle, B.: The Convergence of Bird Flocking. Journal of the ACM 61(4), 21:1–

21:35 (2014)

9. Chen, Y., Wu, B., Lai, T.L.: Fast Particle Filters and Their Applications to Adap-
tive Control in Change-Point ARX Models and Robotics. INTECH Open Access
Publisher (2009)

10. Cutts, C., Speakman, J.: Energy savings in formation ﬂight of pink-footed geese.

Journal of Experimental Biology 189(1), 251–261 (1994)

11. Dang, A.D., Horn, J.: Formation control of autonomous robots following desired
formation during tracking a moving target. In: Proceedings of the International
Conference on Cybernetics. pp. 160–165. IEEE (2015)

12. Dimock, G., Selig, M.: The Aerodynamic Beneﬁts of Self-Organization in Bird

Flocks. Urbana 51, 1–9 (2003)

13. Flake, G.W.: The Computational Beauty of Nature: Computer Explorations of

Fractals, Chaos, Complex Systems, and Adaptation. MIT Press (1998)

14. Garc´ıa, C.E., Prett, D.M., Morari, M.: Model predictive control: Theory and prac-

tice – a survey. Automatica 25(3), 335–348 (1989)

15. Gennaro, M.C.D., Iannelli, L., Vasca, F.: Formation Control and Collision Avoid-
ance in Mobile Agent Systems. In: Proceedings of the International Symposium on
Control and Automation Intelligent Control. pp. 796–801. IEEE (2005)

16. Glasserman, P., Heidelberger, P., Shahabuddin, P., Zajic, T.: Multilevel Split-
ting for Estimating Rare Event Probabilities. Operations Research 47(4), 585–600
(1999)

17. Grosu, R., Peled, D., Ramakrishnan, C.R., Smolka, S.A., Stoller, S.D., Yang, J.:
Using statistical model checking for measuring systems. In: Proceedings of the
International Symposium Leveraging Applications of Formal Methods, Veriﬁcation
and Validation. LNCS, vol. 8803, pp. 223–238. Springer (2014)

18. Henriques, D., Martins, J.G., Zuliani, P., Platzer, A., Clarke, E.M.: Statisti-
cal model checking for markov decision processes. In: Proc. of QEST 2012: the
Ninth International Conference on Quantitative Evaluation of Systems. pp. 84–93.
QEST’12, IEEE Computer Society (2012)

19. Heppner, F.H.: Avian ﬂight formations. Bird-Banding 45(2), 160–169 (1974)
20. H´erault, T., Lassaigne, R., Magniette, F., Peyronnet, S.: Approximate probabilistic
model checking. In: Proceedings of the International Conference on Veriﬁcation,
Model Checking, and Abstract Interpretation (2004)

ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans

17

21. Hung, Y., Wang, W.: Accelerating parallel particle swarm optimization via gpu.

Optimization Methods and Software 27(1), 33–51 (2012)

22. James, K., Russell, E.: Particle swarm optimization. In: Proceedings of 1995 IEEE

International Conference on Neural Networks. pp. 1942–1948 (1995)

23. Kalajdzic, K., J´egourel, C., Lukina, A., Bartocci, E., Legay, A., Smolka, S.A.,
Grosu, R.: Feedback Control for Statistical Model Checking of Cyber-Physical
Systems. In: Proceedings of the International Symposium Leveraging Applications
of Formal Methods, Veriﬁcation and Validation: Foundational Techniques. pp. 46–
61. LNCS, Springer (2016)

24. Lissaman, P., Shollenberger, C.A.: Formation ﬂight of birds. Science 168(3934),

1003–1005 (1970)

25. Mannor, S., Rubinstein, R.Y., Gat, Y.: The cross entropy method for fast policy

search. In: ICML. pp. 512–519 (2003)

26. Nathan, A., Barbosa, V.C.: V-like Formations in Flocks of Artiﬁcial Birds. Artiﬁcial

Life 14(2), 179–188 (2008)

27. Reynolds, C.W.: Flocks, herds and schools: A distributed behavioral model. SIG-

GRAPH Computer Graphics 21(4), 25–34 (1987)

28. Russell, S., Norvig, P.: Artiﬁcial Intelligence: A Modern Approach. Prentice-Hall,

3rd edn. (2010)

29. Rymut, B., Kwolek, B., Krzeszowski, T.: GPU-Accelerated Human Motion Track-
ing Using Particle Filter Combined with PSO. In: Proceedings. of the International
Conference on Advanced Concepts for Intelligent Vision Systems. LNCS, vol. 8192,
pp. 426–437. Springer (2013)

30. Seiler, P., Pant, A., Hedrick, K.: Analysis of bird formations. In: Proceedings of
the Conference on Decision and Control. vol. 1, pp. 118–123 vol.1. IEEE (2002)
31. Stulp, F., Sigaud, O.: Path integral policy improvement with covariance matrix
adaptation. arXiv preprint arXiv:1206.4621 (2012), http://arxiv.org/abs/1206.
4621

32. Stulp, F., Sigaud, O.: Policy improvement methods: Between black-box op-
timization and episodic reinforcement learning (2012), http://hal.upmc.fr/
hal-00738463/

33. Verfaillie, G., Pralet, C., Teichteil, F., Infantes, G., Lesire, C.: Synthesis of plans
or policies for controlling dynamic systems. AerospaceLab (4), p. 1–12 (2012)
34. Weimerskirch, H., Martin, J., Clerquin, Y., Alexandre, P., Jiraskova, S.: Energy

Saving in Flight Formation. Nature 413(6857), 697–698 (2001)

35. Yang, J., Grosu, R., Smolka, S.A., Tiwari, A.: Love Thy Neighbor: V-Formation
as a Problem of Model Predictive Control. In: LIPIcs-Leibniz International Pro-
ceedings in Informatics. vol. 59. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik
(2016)

36. Yang, J., Grosu, R., Smolka, S.A., Tiwari, A.: V-Formation as Optimal Control.

In: Proceedings of the Biological Distributed Algorithms Workshop 2016 (2016)

37. Zhou, Y., Tan, Y.: GPU-based Parallel Particle Swarm Optimization. In: Proceed-
ings of the Congress on Evolutionary Computation. pp. 1493–1500. IEEE (2009)

