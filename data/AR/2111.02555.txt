1 

Implementing augmented reality technology to measure 
structural changes across time 

Jiaqi Xu1, Elijah Wyckoff2, Marlon Aguero1, John-Wesley Hanson1, Fernando Moreu1*, 
Derek Doyle3 

1 Department of Civil, Construction & Environmental Engineering, University of New Mexico, 
Albuquerque, NM, USA 

2 Department of Mechanical Engineering, University of New Mexico, Albuquerque, NM, USA 

3 Air Force Research Laboratory, Space Vehicles Directorate, Kirtland Air Force Base, Albuquerque, NM, 
USA 

In recent years, augmented reality (AR) technology has been increasingly employed in structural 
health  monitoring  (SHM).  In  the  case  of  conditions  following  a  seismic  event,  inspections  are 
conducted to evaluate the progression of the damage pattern quantitatively and efficiently respond 
if  the  displacement  pattern  is  determined  to  be  unsafe.  Additionally,  quantification  of  nearby 
structural  changes  over  short-term  and  long-term  periods  can  provide  building  inspectors  with 
information  to  improve  safety.  This  paper  proposes  the  Time  Machine  Measure  (TMM) 
application on  an Augmented Reality  (AR) Head-Mounted-Device  (HMD) platform. The  main 
function of the TMM application is to restore the saved meshes of a past environment and overlay 
them onto the real environment so that inspectors can intuitively measure structural deformation 
and other movement across  time. The proposed TMM application  was verified by  experiments 
meant to simulate a real-world inspection. 

Keywords: augmented reality; virtual images; deformation measurement; structural health 
monitoring; inspection. 

 INTRODUCTION 

Built environment is defined as human-made surroundings designed for human activities (Handy 
et al. 2002). Various emergencies and hazards can happen in a built environment, e.g., earthquakes. 
Ensuring the safety of inspectors is a priority, and hazards exist that threaten their safety. Seismic 
hazards include mainshock and secondary  hazards, e.g.,  aftershocks.  Aftershocks  can  suddenly 
happen  after  the  mainshock  in  days,  weeks,  months,  or  even  years.  For  example,  the  Mw7.8 
Gorkha earthquake struck central Nepal on April 25th, 2015, followed by an Mw6.7 aftershock on 
April 26th (1 day after the mainshock) and an Mw7.3 aftershock on May 12th (17 days after the 
mainshock)  (Arora  et  al.  2017).  Although  the  magnitude  of  aftershocks  is  smaller  than  the 
mainshock,  the  risk  of  structural  deformation  during  aftershocks  is  higher  due  to  the  dynamic 
characteristic change of the damaged structures. 

Earthquake rescuers agree that the first 72 hours are essential for life-saving, usually known as the 
‚Äògolden relief time‚Äô (Ochoa et al. 2007; Dourandish, Zumel, and Manno 2007; Fiedrich, Gehbauer, 
and Rickers 2000). However, the first 72-hour entry for earthquake inspectors has the maximum 
probability of structural movement caused structural collapse (Felzer, Abercrombie, and Ekstr√∂m 

 
 
 
 
 
2 

Xu et al. 

2003). Therefore, reliable and easy-to-operate observation methods for monitoring environmental 
activities over time are vital (Gallagher, Reasenberg, and Poland 1999). ATC-20 makes a general 
recommendation  about  the  maximum  waiting  time  for  emergency  access  by  considering 
earthquake mainshock magnitude and days after the mainshock. The limitation of ATC-20 is that 
the  officers  make  the  recommendations  based  on  statistical  analyses  and  experiences  (Applied 
Technology Council (ATC) 1989; ATC 2005). Hence ATC-20 cannot provide recommendations 
based on real-time movement that varies with each disaster and building/structure.  

There are three major implementations of AR technology in emergency management: i) simulating 
disaster  environments;  ii)  studying  disaster  factors;  and  iii)  training  (Zhu  and  Li  2021).  For 
simulating disaster environments, researchers have implemented AR technology to draw AR maps 
of the building so first responders can detect rooms that may be invisible due to debris blockage 
or  smoke  during  rescue  operations  (Yang  et  al.  2018;  Sebillo  et  al.  2016;  Sharma,  Stigal,  and 
Bodempudi 2020). An AR map can also help inspectors make responsible decisions with planning 
routes  to  avoid  tie-ups  or  unsafe  regions.  AR  has  also  been  implemented  to  simulate  building 
collapse procedures in an earthquake (Leebmann 2004; Tadokoro et al. 2000; Kitano et al. 1999). 
Researchers have been focused on AR-aided rapid post-disaster building assessment to provide 
more valuable information support for inspectors. With AR technology, inspectors can conduct a 
fast building assessment before entering the damaged building. Kamat and El-Tawil (2007; 2005) 
developed  an  HMD  to  automatically  compare  the  CAD  images  (i.e.,  original  design)  with  the 
current state of the damaged building. The structural damage can be detected by measuring the 
critical differences between the baseline image and the facility‚Äôs realistic view. 

The concept of overlapping previously saved virtual images onto real environments is referred to 
as  ‚Äòtime  machine‚Äô  in  this  paper.  Zoellner  et  al.  (2008)  developed  a  time  machine  application, 
Reality  Filtering,  to  visualize  paintings  of  buildings  and  frescos  seamlessly  superimposed  onto 
cultural heritage  and architecture area.  Holmgren, Johansson,  and Andersson (2014) developed 
another  time  machine  software  application  implementing  an  AR  view  to  access  multimedia 
information of the past and the present when standing in a specific position. These applications of 
time machine can only restore pre-input images, i.e., they are not reference-free. Also, current time 
machine applications cannot measure movement of structures and objects. 

This paper summarizes the design, development, and validation of Time Machine Measure, a novel 
tool to measure and share reference-free changes during inspection. TMM can capture and measure 
changes  in  environmental  movements  and  position  over  time  and  can  assist  inspectors  by 
informing them of these changes over short and extended time periods. Additionally, the TMM 
application  is  contact-free  and  solely  controlled  by  gestures,  freeing  both  hands  for  other 
inspection tasks. Voice input can be used with the application as well, allowing the user perform 
actions in the AR application with their gaze and voice. No prerequisite for implementing the pre-
input of the CAD model is needed in the TMM application. In the following sections, the authors 
first  design  theory  and  summarize  the  software  framework  design,  presenting  both  the  human-
device interface and operational instructions. 

 MEASUREMENT DURING INSPECTIONS 

Measurement across time can be of value during post-seismic event inspections. Damage patterns 
have varying levels of seismic vulnerability, and the various damage patterns observed in built 

 
 
 
 
 
3 

environments  are  classified  by  Okada  and  Takai  (1999).  For  example,  structural  deformation 
consisting  of  inter-story  drift  and  rigid  body  rotation  (X.  Lu  et  al.  2017;  Ghobarah  2004)  can 
indicate  progressive decay on  the structure‚Äôs  stability.  It would be valuable to notice structural 
deformation  across  time  and  other  local  movements,  for  example  inspection  of  a  residential 
building  post-earthquake.  This  information  can  inform  inspectors  of  real-time  and  short-term 
changes in their surroundings and can also help them classify damage and monitor changes over 
an extended time period. 

Today‚Äôs  solutions  to  measure  structural  deformation  during  the  inspection  process  include 
comparing  the  current  building  visually  with  the  embedded  blueprint  and  recognize  the 
discrepancies (Kamat and El Tawil 2007; Kamat and El-Tawil 2005). Okada and Takai (1999) cite 
the  importance  of  hazard  reduction  by  identifying  damage  to  individual  structures  in  post-
earthquake  inspections.  Under  the  premise  of  enough  time  and  access  to  the  design  blueprint, 
today‚Äôs solutions work well for inspectors to quickly conduct a building safety assessment if they 
plan to enter a structure. However, time and design blueprints are not as useful for multiple short-
term assessments. 

Additionally, today‚Äôs solutions do not enable direct observation of the structural changes across 
time. The included initial construction error overestimates the risk level of damaged buildings and 
shortens the precious rescue time. Figure 1 illustrates a straightforward example of the value of 
 represents the deformation 
measuring structural deformation after a seismic event. In Figure 1, 
before  the  disaster,  including  the  initial  construction  error  and  rigid  body  rotation,  which  are 
ùëëùëë1
  represents the changes 
informative but do not capture changes as time progresses. Conversely, 
 while a measuring method 
that can occur after the disaster. Current devices can only measure 
ùëëùëë2
of 

 is still not available today. 

ùëëùëë2
The authors design two primary functions to fulfill the core requirement of direct measurement of 
: (i) time machine and (ii) measure. The time machine function enables inspectors to recreate 
structures recorded as a 3D mesh and overlay the past mesh in the present to track deformation 
ùëëùëë2
across time. The measure function allows inspectors to measure distance between past and present 
surroundings, i.e., measurement across time.  

ùëëùëë1

(a) Measure structural deformation

(b) Real-time deformation 

Figure 1 Schematic diagram of the structural deformation and inspection 
ùëëùëë2

4 

Xu et al. 

 AR DEVICE SELECTION 

The factors to be considered in selecting any AR platform consist of five fundamental categories: 
general properties, sensors, computational capabilities, connectivity, and display capabilities. Xu, 
Doyle, and Moreu (2020) summarized the parameters in each category. General properties include 
release date, country, weight, price, product durability (waterproof, working temperature, relative 
humidity, and drop safe distance). Sensors consist of camera resolution, head-tracking sensor, eye-
tracking  sensor,  depth  sensor,  gyroscope,  accelerometer,  magnetometer,  and  location  sensor 
(GPS/GLONASS/GALILEO).  Computational  capabilities  include  the  processing  unit,  Random 
Access  Memory  (RAM),  and  storage.  Connectivity  capabilities  include  WiFi,  Bluetooth,  USB, 
and  battery  capacity.  Display  capabilities  include  the  field  of  view  (FoV),  Optics  method, 
resolution, and refresh rate. An evaluation system can simplify the selection procedure of the AR 
platform. The composite score of an AR device, 

, can be described as 

ùëÜùëÜ
ùëöùëö

ùëñùëñ

 represents the 

 parameter considered in the selection. 

ùëÜùëÜ = ùë£ùë£ùëñùëñ ÔøΩ ùë§ùë§ùëñùëñùëÉùëÉùëñùëñ
(1)
 can be a full set or subset of 
where 
ùëñùëñ=1
all  the  parameters  illustrated  by  Xu,  Doyle,  and  Moreu  (2020).  For  a  specific  implementation, 
researchers can always add extra parameters as demanded. 
, 
 is  the  correction  coefficient  with  values  of 
measuring  the  importance  of  the 
ùëÉùëÉùëñùëñ
 for the devices with 

 is a necessary parameter for a specific application, then 

 is the weighing coefficient of 

 parameter. 

. If 

ùë§ùë§ùëñùëñ

ùëÉùëÉùëñùëñ

ùëÉùëÉùëñùëñ

th

th

ùëñùëñ

.  

ùë£ùë£ùëñùëñ = 1

ùë£ùë£ùëñùëñ = 0

this character, otherwise 
ùëÉùëÉùëñùëñ
{0 1}
The authors select the AR platform for inspection from the sixteen AR devices published in the 
last three years, with subset parameters provided by Xu, Doyle, and Moreu (2020). According to 
this  research,  the  weight  and  price  of  AR  devices  will  decrease  while  increasing  their  outdoor 
durability,  further  augmenting  its  likelihood  for  field  implementations.  The  authors  use  a 
correction coefficient for the depth sensor to meet the time machine function requirement and the 
 of  the  device  with  the  best  behavior  in  this 
rescue  requirements.  The  weighting  coefficient 
parameter is 1, and 
. For example, 
 of the other devices are multiplied with a fraction of 
ùë§ùë§ùëñùëñ
the  Toshiba  dynaEdge  AR100  has  the  maximum  working  temperature  of  60‚ÑÉ,  therefore, 
. The maximum working temperature of Google Glass Enterprise Edition 2 

ùëÉùëÉùëñùëñ/ùëÉùëÉbest

ùë§ùë§ùëñùëñ

ùë£ùë£ùëñùëñ

.  

ùë§ùë§temp(Google) = 45 60‚ÅÑ = 0.75

is 45‚ÑÉ, therefore, 
ùë§ùë§temp(Toshiba) = 1
Table  1  summarizes  the  parameters  and  the  corresponding  weighting  coefficients  to  the 
parameters.  The  last  column  of  Table  1  lists  the  total  evaluation  score  of  each  AR  device  in 
descending order. The authors selected the Microsoft HoloLens 2 platform to design the TMM 
application. Besides the highest score, the Microsoft HoloLens 2 is contact-free and controlled by 
gestures as an HMD, freeing both hands for other inspection tasks. 

 SOFTWARE DESIGN 

The aforementioned primary functions of TMM are time machine and measure. The time machine 
function  enables  inspectors  to  observe  and  compare  the  current  environment  to  virtual 
environments from the past. The measure function enables the measurement of deformations over 
time. The following sections describe in detail the development and use of these two functions. 

 
Table 1 Evaluation score of AR devices for the emergency rescue 

5 

4.1 Time machine function

The time machine function enables a 3D representation of the past to be saved and restored to be 
displayed  simultaneously  with  the  immediate  environment.  The  AR  device  can  track 
users‚Äô  positions  in  space  using  the  spatial  mapping  function  to  place  stable  and  accurate 
holograms  despite  the  users‚Äô  movement.  Therefore,  even  if  the  perspective  changes  the  AR 
device  can  recognize proper positioning. The following procedure is to fulfill the time machine 
function. The spatial mapping of the real environment is defined at time t as 

 represents the 

where 
mesh  elements  in  this  environment; 
ùëõùëõ
represents the timestamp of the spatial mapping; and 
coordinates.  

{ùêàùêàùëõùëõ}ùë°ùë° = (ùë•ùë•, ùë¶ùë¶, ùëßùëß, ùë°ùë°)

ùêàùêàùëõùëõ

ùëõùëõ

th

ùëáùëá

nth mesh element of the real environment; 

, ùëõùëõ ‚àà [1, ùëÅùëÅ]
 is  the  spatial  mapping  consisting  of 

 is the total number of the 
(2)
 mesh  elements; 

ùëÅùëÅ

 are the three-dimensional position 
ùë°ùë°

ùëõùëõ

(ùë•ùë•, ùë¶ùë¶, ùëßùëß)

Figure 2 explains the time machine function with the movement of a lamp. The lamp labeled with 
a red frame is a real object (i.e., a physical lamp in the room), and the one with a blue frame is 
virtual  (i.e.,  simulated  meshes  of  a  lamp).  In  Figure  2(a),  the  environment  is  mapped,  and  the 
. In Figure 2(b), the lamp has been moved 
meshes of the lamp in the position 
. In summary, with 
 to 
from position 
 is restored, i.e., the virtual lamp at position 
ùëÉùëÉ1
the time machine function, the users can observe the saved images from the past and the real object 
in the environment at the same time.   

 are saved as 

{ùêàùêàùëõùëõ}ùë°ùë°1

{ùêàùêàùëõùëõ}ùë°ùë°1

, and 

ùëÉùëÉ2

ùëÉùëÉ1

ùëÉùëÉ1

 
 
6 

Xu et al. 

(a) Real image 

(b) Real image 

 and virtual image 

Figure 2 Illustration of the time machine function 

{ùêàùêàùíèùíè}ùíïùíïùüèùüè

{ùêàùêàùíèùíè}ùíïùíïùüêùüê

{ùêàùêàùíèùíè}ùíïùíïùüèùüè

The  time  machine  function  utilizes  Microsoft‚Äôs  Mixed  Reality  Toolkit  (MRTK)  (2020)  mesh 
observer to add a collection of meshes. The meshes represent the geometry of the nearby real-
world environment in the Unity scene. The MRTK mesh observer gets real-time environment data 
from  the  HoloLens  Holographic  Processing  Unit  (HPU).  Figure  3  summarizes  the  saving  and 
loading procedure in the TMM. Figure 4 shows a more detailed description of the time machine 
design frame. 

Figure 3 Time machine function components 

The mesh vertices, triangles, and time are saved to the non-volatile memory as an XML file for 
later access. This process is performed by a custom class to make a deep copy of the vertices and 
triangles of the meshes from the MRTK mesh observer. A deep copy is required because the mesh 
data is read-only, and Unity components cannot be directly serialized. The custom class then adds 
a timestamp to the deep copied meshes, copies the mesh positions relative to the observer, and 
serializes the System.Xml.Serialization component (Joshi 2017) from the .NET Runtime Library 
(Barnett and Schulte 2003).  

To  load  a  saved  mesh,  a  Unity  GameObject  is  created  with  four  components  attached,  i.e., 
MeshFilter,  MeshRenderer,  MeshCollider, 
and  TimeMachineData.  The  MeshFilter, 
MeshRenderer, and MeshCollider are components provided by the Unity game engine to enable 
the rendering and collision of meshes. The TimeMachineData is a custom-defined component. The 
saved XML file is  then  deserialized into an int  array that defines the mesh triangles, a  3-point 
vector array that defines the mesh vertices, and a timestamp. The triangles and vertices are then 
copied  to  the  MeshFilter;  the  timestamp  is  assigned  to  the  TimeMachineData,  and  the  new 
GameObject‚Äôs position is set to the saved position. 

4.2 Measure function 

The  3D  transformation  during  from  time 
translation 
motion. 

.  Combining 

 and 

‚àÜùêàùêàTra

‚àÜùêàùêàRot

ùë°ùë°1
‚àÜùêàùêàTra

ùë°ùë°2

 consists  of  a  3D  rotation 

 to 
 in  a  transformation  matrix 

,  then

 and  a  3D 
 is  a  rigid 

‚àÜùêàùêàRot
 ‚àÜùêàùêà

‚àÜùêàùêà

7 

(3)

‚àÜùêàùêà = ÔøΩ

‚àÜùêàùêàRot ‚àÜùêàùêàTra

ÔøΩ

ùüèùüè

ùüéùüé
{ùêàùêàùëõùëõ}ùë°ùë°2 = ‚àÜùêàùêàùë°ùë°1‚Üíùë°ùë°2{ùêàùêàùëõùëõ}ùë°ùë°1, ùëõùëõ ‚àà [1, ùëÅùëÅ]
(4)
The  measure  function  is  to  measure 
,  as  shown  in  Equation  (3).  The 
,  especially 
programming  of  the  measure  function  is  based  on  the  spatial  mapping  that  recognizes  the 
surrounding  environment.  The  authors  use  a  script  consisting  of  the  Measure  Manager,  Input 
Manager, and Spatial Mapping components to conduct the measurement, as shown in Figure 5. 
The Measure Manager manages all the measurements in the Unity platform, and the Input Manager 
handles the input according to the users‚Äô sight and gesture. Specifically, the AR user performs an 
air  tap  gesture  to  place  pins  in  the  desired  locations  of  measurement.  Figure  6  shows  a  more 
detailed description of the measure function design frame. 

‚àÜùêàùêàTra

‚àÜùêàùêà

The  Spatial  Mapping  component  consists  of  three  scripts,  Spatial  Mapping  Observer,  Spatial 
Mapping  Manager,  and  Object  Surface  Observer.  The  Spatial  Mapping  Observer  script  is  to 
manage the Surface Observer component to conduct an environmental scan. The Object Surface 
Observer component scans the surrounding environment and loads the meshed objects. The Spatial 
Mapping Manager script has four functions: (i) restore the Surface Observer component, (ii) store 
the mesh data obtained by the Spatial Mapping component, (iii) shut down the Surface Observer 
component, and (iv) update the Spatial Mapping component in real-time while TMM is loading 
the mesh data. 

4.3 Measurement over time 

The authors combined the two primary functions of the TMM by leveraging Unity‚Äôs layer system 
and a custom class to store time information on loaded GameObjects. Full meshes are assigned to 
the  same  layer  as  the  meshes  from  the  Spatial  Observer  component  so  that  ray  casts  from  the 
InputHandler (Figure 6) collide with the MeshCollider component. The InputHandler component 
checks the TimeMachineData component on any hit objects, allowing measurement over time by 
comparing the saved time of the hit meshes. If the InputHandler does not find a TimeMachineData 
component,  it uses  the current time  to compare  with the other meshes.  Figure 7  shows a more 
detailed description of the measurement over time function design frame. 

Figure 4 Time machine function flowchart 

 
 
 
 
 
 
 
 
8 

Xu et al. 

Figure 5 Measure function components 

Figure 6 Measure function flowchart 

Figure 7 Measurement over time flowchart  

 HUMAN-DEVICE INTERFACE 

This section presents the human-device interface and illustrates the functions of different modules. 
Figure 8 shows the full view of the TMM human-device interface. The two modules in the TMM 
interface are the time machine menu and the measure menu. These correspond to their respective 

 
 
 
 
 
functions. The interface is developed with human factors considered. Each button is clearly labeled 
and  the  functions  work  simple  enough  for  an  inexperienced  AR  user.  Figure  9  is  a  schematic 
diagram illustrating the interface during the implementation. The following subsections show a 
detailed explanation of the functions and instructions of each module. 

9 

Figure 8 The TMM human-device interface 

(a) Spatial mapping of the real environment 

(b) TMM interface in an implementation 

Figure 9 Measuring Movements Across Time 

5.1 Interface menus 

The measure menu contains the following keys: Distance, Quick Measure, Manipulate, Font Size, 
Line  Width,  Clear  Measurement,  and  Units.  The  time  machine  menu  includes  the  following 
functions: Toggle Real-time Mesh, Reset Room Position, Load Rooms, and Save Room Mesh. 
Table 2 summarizes the functions of each key in detail. The authors name the keys in the interface 
according to their operations and show directly in the menu to decrease cognition load. 

5.2 Control gestures 

The  user  interfaces  with  the  menus  and  keys  of  the  TMM  using  hand  gestures  and  their  gaze 
following this sequence: (i) user gazes at the hologram meant to be clicked; (ii) user points the 
index finger straight up toward the ceiling; and (iii) user air taps the key intended to be accessed 
by first lowering the finger and then quickly raising it. Figure 10 summarizes the control gesture 
of the air tap.   

 
 
 
  
 
 
 
 
 
 
10 

Xu et al. 

Table 2 The function of each key in the interface 

# 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 

Menu 

Key 

Function 

Time Machine 

Measure 

Toggle Real-time Mesh  Scan the surroundings 
Reset Room Position 
Load Rooms 
Save Room Mesh 
Distance 
Quick Measure 
Manipulate 
Font Size 
Line Width 
Clear Measurement 
Units 

Reset the environment and re-scan the surroundings 
load a pre-scanned environment 
Save the mapping of the current environment 
Measure distance between two points 
Measure with default settings 
Downsize the virtual environments 
Modify the measurement font size 
Modify the measurement line width 
Clear all measurements 
Show the measurement units 

To conduct a measurement, a user first points to a node as the starting point with the index finger 
of their right hand, performing an air tap to place a pin. Next, they point to another node with a 
different timestamp endpoint (i.e., from a saved and loaded image) and place a second pin. The 
interface  then  shows  the  distance  between  the  start  and  endpoints  automatically.  Multiple 
measurements  can  be  conducted  in  the  same  space  where  the  measure  function  displays  the 
distance  between  each  successive  pin  placement.  Measurements  can  also  be  cleared  for  new 
measurements and the pins can be manually moved after they have been placed if the user desires 
a different or more exact location. 

(a) Front image 

(b) Profile image 

Figure 10 Air tap gestures to control the TMM by user  

5.3 Color 

TMM can simultaneously save a maximum of six environment meshes with different timestamps. 
Therefore, there could be a maximum of six virtual environments displayed overlapping onto the 
reality at a given time. The saved meshes with different timestamps are shown in different colors 
to decrease cognitive load for the users. An inspection site can have low visibility, and with this 
consideration  the  authors select opposite  colors  with  high  saturation  for  meshes  in a  sequence. 
Two contiguous meshes are restored with opposite colors according to the color wheel to generate 
high contrast and improve the visualizations of the variations in time (Cohen-Or et al. 2006). With 
the contrasting colors selected, the measurement procedure between different meshes will be less 
time-consuming. The colors implemented in the TMM interface are cyan, orange, lime, red, blue, 
and magenta. Table 3 shows the names, RBG values, and a palette of the selected colors. 

 
 
 
 
 
 
 
 
11 

Table 3 Colors for the saved meshes in the TMM interface 

Color Palette 

#  Color Name 
1 
2 
3 
4 
5 
6  Magenta 

Cyan 
Lime 
Blue 
Orange 
Red 

RBG 
(0, 255, 255) 
(0, 255, 0) 
(0, 0, 255) 
(255, 128, 0) 
(255, 0, 0) 
(255, 0, 255) 

5.4 Special functionalities 

Three  functionalities  are  designed  in  the  interface  to  help  the  users  implement  the  TMM 
application.  These  are  timestamp,  downsizing,  and  automatic  font  scaling.  The  following 
subsections provide detailed explanations of these three functionalities. 

5.4.1 Timestamp 

Displaying  timestamps  helps  distinguish  how  the  environment  has  been  changing  over  time. 
Without the timestamps, it would be difficult for the users to tell the measurement duration and it 
is valuable to document the time and date that prior captures were taken. Timestamps of the saved 
meshes are automatically loaded and displayed on the TMM interface, where each key has a date 
and timestamp with a color that matches the corresponding mesh. The demonstration of the TMM 
section shows examples of the timestamp feature. 

5.4.2 Downsizing 

The downsizing feature of the application allows the user to manipulate the size and angle of one 
capture or overlaid captures. This feature is necessary for easy viewing of the saved environment, 
enabling the users to inspect the surroundings quickly. It also allows the users to manipulate the 
view to take measurements and draw conclusions more quickly. Users can deploy the downsizing 
feature by air tapping the ‚ÄòManipulate‚Äô key (right upper corner in Figure 8) and then pinching with 
both hands. Users can move both hands closer to downsize the figure and change either hand‚Äôs 
position  to  alter  the  angle.  The  demonstration  of  the  TMM  section  shows  an  example  of  the 
downsizing feature.  

5.4.3 Automatic font scaling 

Both  the  font  size  and  line  width  of  the  measurement  results  can  be  adjusted  automatically  or 
manually. Users can alter the font size and line width by the sliders on the ‚ÄòFont Size‚Äô and ‚ÄòLine 
Width‚Äô  keys,  as  shown  in  Figure  8.  The  measurement  text  is  rendered  in  3D  space  and  scaled 
relative  to  the  user‚Äôs  distance  to  maintain  font  readability  with  different  lengths.  As  shown  in 
Figure  11,  the  font  size  for  the  measurements  automatically  keeps  the  same  despite  measuring 
distance.   

 
 
 
 
 
 
 
 
 
 
 
 
12 

Xu et al. 

(a) Font at 0.5m 

(b) Font at 2.0m 

Figure 11 Automatic font scaling function  

 TMM EXPERIMENT 

All  the  TMM  application  functions  should  be  pre-verified  before  its  implementation  in  a  real 
inspection site. This experiment is to verify the accuracy of the measurement over time function 
and show its convenience and efficiency over the traditional measuring method.  

6.1 Experiment description 

Researchers conducted one experiment to verify the reliability of the TMM in comparison with 
traditional measurement methods. To this end, researchers moved one piece of furniture, a cube 
storage container, and measured the movement by both a traditional method with a tape measurer 
and TMM. Researchers marked the floor with the original and final position to compare the two 
methods (Figure 12(a)). 

6.2 Measurements   

The  researchers  validated  the  TMM  by  conducting  two  measurements  simultaneously  and 
comparing them. The two different measurement approaches are described herein, for comparison 
purposes: 

(a)  Figure 12(b) and (c) show the traditional tape measurement sequence: first, researchers marked 
the starting position of the object that would serve as a reference about its past position; then, after 
the movement, a mark was added that would serve as the present position of interest; finally, the 
researchers used a tape measure to measure the distance between the marks representing past and 
present positions, which was the movement across time.  

(b)  Figure 12(d) shows the measurement using the TMM: the user can collect the change on position 
without marks on the ground. The TMM overlapped the virtual objects from both the past and the 
present time of the movement simultaneously, and the user measured the movement of the object 
across time. Additionally, the user can measure more than one movement across time, enabling 
multiple comparisons across time with small effort. 

 
 
 
 
 
 
 
 
 
 
13 

6.3 Experimental results 

The result by the traditional method is 0.91m, and by TMM is 0.93m. The measuring difference is 
about 2% between the two methods. The measuring difference is induced by (i) the spatial meshing 
precision; and (ii) the accuracy of the user‚Äôs placement. It can be inferred that human error may 
cause some error, where the user may not have placed the AR pins perfectly at the centered of the 
marks  on  the  floor.  The  difference  of  2%  is  accurate  enough  for  a  real-time  immediate 
environmental observation.  

The  traditional  method  takes  about  5  min  to  set  the  markers  and  conduct  the  measurement.  In 
comparison, the TMM uses less than 1 min, according to the time stamp in Figure 12(d). Setting 
markers is inconvenient, time-consuming, sometimes even not applicable during an inspection.  

 TMM DEMONSTRATION EXPERIMENT 

After verifying the reliability of the TMM, the authors design a demonstration to show a complete 
process of TMM implementation.  

7.1 Experiment objective  

This  demonstration  simulates  an  environment  in  which  inspectors  can  observe  and  measure 
changes across time. This demonstration shows the features of the TMM application by executing 
the same steps as an inspection task, capturing, and overlaying separate environments to measure 
the changes between  captures over time.  The authors  conduct  a demonstration experiment in a 
residential house to test the TMM in a realistic scenario where results can have the highest impact 
for inspections in the future. 

7.2 Experiment procedure 

There are seven steps in the demonstration:  

(a)  Nominate  and  save  the  initial  environment  when  the  inspectors  enter  the  site  to  verify  the 

timestamp function.  

(b)  Move the container to simulate the environmental change.  
(c)  Restore the saved initial environment to verify the time machine function.  
(d)  Measure over time using the measure function and save the current environment, verifying the 

measurement over time function. 

(e)  Move the container again to simulate a continuous environmental change. 
(f)  Restore the saved images and measure over time to verify the robustness of the measurement over 

time function. 

(g)  Manipulate the virtual images to verify the downsizing function. 

 
 
 
 
 
 
  
14 

Xu et al. 

0.91m 

(a) Experimental 
objective 

(b) Traditional 
method-starting 
point 

(d) TMM 
(c) Traditional 
method-end point 
measurement 
Figure 12 TMM verification experiment 

7.3 Experiment results 

The following are the demonstration results:  

(a)  The authors nominate the time of the initial environment as 

. The environment at time 

 is saved 

with a timestamp and appointed as 

 (cyan color), as shown in Figure 13(a).  

ùë°ùë°1
(b)  The cube container is moved from the initial location to a new location during the time 

ùë°ùë°1

 to 

. 

ùë°ùë°2

, the saved 

(c)  At time 

The environment at time 

 is saved with a timestamp and nominated as 
ùë°ùë°2
 are 
is restored as a virtual image. Both the real 
shown to the user at the same time, as shown in Figure 13(b). The users can maneuver the color-
coded environments to observe the container movement and verify the capture capabilities of the 
TMM.  
(d)  At time 

 by air tapping the container in each capture. The interface will then 
show the distance between points pinned at two desired locations with the elapsed time between 
the captures. The moving distance in step (b) is 0.87m, as shown in Figure 13(c).  

 (lime color).  
ùë°ùë°1

ùë°ùë°2
{ùêàùêàùëõùëõ}ùë°ùë°1

 and virtual 

, measure 

‚àÜùêàùêàùë°ùë°1‚Üíùë°ùë°2

{ùêàùêàùëõùëõ}ùë°ùë°2

{ùêàùêàùëõùëõ}ùë°ùë°2

{ùêàùêàùëõùëõ}ùë°ùë°1

ùë°ùë°2

{ùêàùêàùëõùëõ}ùë°ùë°1

(e)  Save  the  environment  at  time 

 it  as 

 (lime  color).  Furthermore,  move  the  cube  storage 

(f)  At  time 

again, as shown in Figure 13(d).  
ùë°ùë°2
 and 
,  restore  both 

{ùêàùêàùëõùëõ}ùë°ùë°2
distance is 1.8m, as shown in Figure 13(e).  

ùë°ùë°3

{ùêàùêàùëõùëõ}ùë°ùë°1

{ùêàùêàùëõùëõ}ùë°ùë°2

(h)  The authors verify the downsizing feature by shrinking the three collaborative environments. With 
the  downsizing  function,  users  can  conveniently  obtain  a  whole  picture  of  the  surroundings  in 
which they are standing. Users can observe the relative position of the object in the environment. 
Additionally,  the  downsizing  function  can  clearly  show  the  observed  object‚Äôs  movement  trail, 
shown as the red line in Figure 13(f). 

‚àÜùêàùêàùë°ùë°1‚Üíùë°ùë°3

 as  virtual  images.  Measure 

.  The  total  moving 

 
 
 
 
 
 
 
 
15 

Real 

{ùêàùêàùíèùíè}ùíïùíïùüèùüè

(a) Initial environment 

save 

{ùêàùêàùíèùíè}ùíïùíïùüèùüè

Real 

{ùêàùêàùíèùíè}ùíïùíïùüêùüê

Virtual 

{ùêàùêàùíèùíè}ùíïùíïùüèùüè

(b) Time machine 

function, restore 

Real 

{ùêàùêàùíèùíè}ùíïùíïùüèùüè

{ùêàùêàùíèùíè}ùíïùíïùüëùüë

Virtual 

{ùêàùêàùíèùíè}ùíïùíïùüêùüê

Virtual 

(c) Measurement over time 
function, measure 

{ùêàùêàùíèùíè}ùíïùíïùüèùüè

(d) Move the object, 

save 

‚àÜùêàùêàùíïùíïùüèùüè‚Üíùíïùíïùüêùüê

t3 

{ùêàùêàùíèùíè}ùíïùíïùüëùüë

t1 

t2 

(e) Measurement over time 
function, measure 

(f) Downsizing function 
and movement trail 

Figure 13 Demonstration of TMM implementation 

‚àÜùêàùêàùíïùíïùüèùüè‚Üíùíïùíïùüëùüë

 CONCLUSIONS 

SHM inspectors can benefit from a convenient method to observe and measure structural changes 
over extended time periods and in real-time to ensure safety following seismic events. Inspired by 
input  from  first  responders  for  inspection  priorities  in  buildings  after  disasters,  the  authors 
developed an AR application to measure the immediate environment movements over time. TMM 
has two primary functions, time machine and measure. With the time machine function, users can 
save  and  restore  the  nearby  environment  as  virtual  meshes.  The  virtual  meshes  can  then  be 
displayed overlapping onto the real environment. With the measure function, the users can measure 
the  distance  between  the  virtual  and  real  meshes.  Consequently,  TMM  enables  inspectors  to 
measure structural movement over time. 

Besides the two primary functions, the researchers also design some practical functionalities for 
the TMM. First, the saved environments are stamped with different saturate and opposite colors to 
be easily distinguished. Then, the measurement fonts and line width can be automatically adjusted 
to minimize eye strain. Next, the virtual environment can be downsized for alternative viewing 
and inspection.  Users can control  the final  version of  TMM by  gaze, hand  gestures, and voice 
commands freeing both hands for inspection tasks. Finally, the researchers designed the human-
device interface to be instinctively controlled to minimize cognitive load. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
16 

Xu et al. 

ACKNOWLEDGMENTS 

The financial support of this research is provided in part by the Air Force Research Laboratory 
(AFRL,  Grant  number  FA9453-18-2-0022),  and  the  New  Mexico  Consortium  (NMC,  Grant 
number  2RNA6.)  The  authors  appreciate  the  discussion  and  feedback  from  the  Department  of 
Emergency Management in the City of Albuquerque. The conclusions of this research represent 
solely those of the authors. 

REFERENCES 

Applied  Technology  Council  (ATC).  1989.  ATC-20:  Procedures  for  Post-Earthquake  Safety 
Evaluation of Buildings. 

Arora,  B.R.,  B.K.  Bansal,  Sanjay  K.  Prajapati,  Anup  K.  Sutar,  and  Shailesh  Nayak.  2017. 
‚ÄúSeismotectonics and Seismogenesis of Mw7.8 Gorkha Earthquake and Its Aftershocks.‚Äù Journal 
of Asian Earth Sciences 133 (January): 2‚Äì11. https://doi.org/10.1016/j.jseaes.2016.07.018. 

ATC. 2005. ‚ÄúATC-20-1 Field Manual: Postearthquake Safety Evaluation of Buildings.‚Äù 

Barnett, Mike, and Wolfram Schulte. 2003. ‚ÄúRuntime Verification of. Net Contracts.‚Äù Journal of 
Systems and Software 65 (3): 199‚Äì208. 

Carbotte,  Kevin.  2018.  ‚ÄúKopin‚Äôs  Golden-i  Infinity  is  a  Wearable  Display  For  Smartphones, 
Tablets.‚Äù 
https://www.tomshardware.com/news/kopin-golden-i-infinity-smart-
display,37138.html. 

2018. 

Cohen-Or, Daniel, Olga Sorkine, Ran Gal, Tommer Leyvand, and Ying Qing Xu. 2006. ‚ÄúColor 
Harmonization.‚Äù 
624‚Äì30. 
https://doi.org/10.1145/1179352.1141933. 

SIGGRAPH 

Papers, 

ACM 

2006 

In 

‚ÄúDAQRI Smart Glasses.‚Äù 2017. 2017. https://daqri.com/products/smart-glasses/. 

Dourandish, Robert, Nina Zumel, and Michael Manno. 2007. ‚ÄúCommand and Control during the 
First 72 Hours of a Joint Military-Civilian Disaster Response.‚Äù In 2007 Command and Control 
Research  &  Technology  Symposium  Adapting  C2  to  the  21st  Century-Newport,  I‚Äì114.  Rhode 
Island, USA. 

Epson.  2017.  ‚ÄúDurable  Smart  Headset  Built  for  Industrial,  Hands-Free  Applications.‚Äù  2017. 
https://epson.com/For-Work/Wearables/Smart-Glasses/Moverio-Pro-BT-2200-Smart-
Headset/p/V11H853020#. 

‚ÄúEverysight Raptor.‚Äù 2017. 2017. https://everysight.com/. 

Felzer, Karen R., Rachel E. Abercrombie, and G√∂ran Ekstr√∂m. 2003. ‚ÄúSecondary Aftershocks and 
Their Importance for Aftershock Forecasting.‚Äù Bulletin of the Seismological Society of America 
93 (4): 1433‚Äì48. https://doi.org/10.1785/0120020229. 

 
 
 
 
 
 
 
 
 
 
 
 
 
17 

Fiedrich,  Frank,  Fritz  Gehbauer,  and  Uwe  Rickers.  2000.  ‚ÄúOptimized  Resource  Allocation  for 
Emergency Response after Earthquake Disasters.‚Äù Safety Science 35 (1‚Äì3): 41‚Äì57. 

Gallagher, Ronald P., Paul A. Reasenberg, and Chris D. Poland. 1999. ‚ÄúEarthquake Aftershocks-
Entering Damaged Buildings.‚Äù In Applied Technology Council, 1‚Äì12. 

Ghobarah,  Ahmed.  2004.  ‚ÄúOn  Drift  Limits  Associated  with  Different  Damage  Levels.‚Äù 
International Workshop on Performance-Based Seismic Design. Vol. 28. 

‚ÄúGlass Enterprise Edition 2 Tech Specs.‚Äù 2019. 2019. https://www.google.com/glass/tech-specs/. 

‚ÄúGlassup F4.‚Äù 2017. 2017. https://www.glassup.com/en/f4/. 

Handy, Susan L., Marlon G. Boarnet, Reid Ewing, and Richard E. Killingsworth. 2002. ‚ÄúHow the 
Built Environment Affects Physical Activity: Views from Urban Planning.‚Äù American Journal of 
Preventive Medicine 23 (2 SUPPL. 1): 64‚Äì73. https://doi.org/10.1016/S0749-3797(02)00475-0. 

Holmgren, Mikael, Dan Johansson, and Karl Andersson. 2014. ‚ÄúA Web-Based Time Machine with 
Augmented Reality.‚Äù In IEEE Conference on Local Computer Networks. 

Joshi, Bipin. 2017. ‚ÄúXML Serialization.‚Äù In Beginning XML with C# 7, 211‚Äì37. Berkeley, CA: 
Apress. https://doi.org/10.1007/978-1-4842-3105-0_8. 

Kamat,  Vineet  R.,  and  Sherif  El-Tawil.  2005.  ‚ÄúRapid  Post-Disaster  Evaluation  of  Building 
Damage Using Augmented Situational Visualization.‚Äù In Construction Research Congress 2005: 
Broadening Perspectives, 1‚Äì10. 

Kamat,  Vineet  R.,  and  Sherif  El  Tawil.  2007.  ‚ÄúEvaluation  of  Augmented  Reality  for  Rapid 
Assessment  of  Earthquake-Induced  Building  Damage.‚Äù  Journal  of  Computing  in  Civil 
Engineering 21 (5): 303‚Äì10. https://doi.org/10.1061/(ASCE)0887-3801(2007)21:5(303). 

Kitano, Hiroaki, Satoshi Tadokoro, Itsuki Noda, Hitoshi Matsubara, Tomoichi Takahishi, Atsuhi 
Shinjou,  and  Susumu  Shimada.  1999.  ‚ÄúRobocup  Rescue:  Search  and  Rescue  in  Large-Scale 
Disasters  as  a  Domain  for  Autonomous  Agents  Research.‚Äù  In  IEEE  SMC‚Äô99  Conference 
Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No. 
99CH37028) Vol 6, 739‚Äì43. 

Leebmann, Johannes. 2004. ‚ÄúAn Augmented Reality System for Earthquake Disaster Response.‚Äù 
International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences 
34 (XXX). 

Lu,  Xilin,  Jiaqi  Xu,  Hongmei  Zhang,  and  Peng  Wei.  2017.  ‚ÄúTopology  Optimization  of  the 
Photovoltaic Panel Connector in High-Rise Buildings.‚Äù Structural Engineering and Mechanics 62 
(4): 465‚Äì75. https://doi.org/10.12989/sem.2017.62.4.465. 

‚ÄúMagic Leap 1.‚Äù 2018. 2018. https://www.magicleap.com/en-us/magic-leap-1. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
18 

Xu et al. 

‚ÄúMicrosoft HoloLens 2.‚Äù 2019. https://www.microsoft.com/en-us/hololens. 

Ochoa, Sergio F., Andr√©s Neyem, Jos√© A. Pino, and Marcos R.S. Borges. 2007. ‚ÄúSupporting Group 
Decision Making and Coordination in Urban Disasters Relief.‚Äù Journal of Decision Systems 16 
(2): 143‚Äì72. https://doi.org/10.3166/jds.16.143-172. 

Okada, Shigeyuki & Takai, Nobuo. (1999). Classifications of structural types and damage patterns 
of buildings for earthquake field investigation. Journal of Structural and Construction Engineering, 
AIJ. 524. 10.3130/aijs.64.65_5. 

Realwear. 2017. ‚ÄúHMT-1: The World‚Äôs Leading Hands-Free Remote Collaboration Tool.‚Äù 2017. 
https://www.realwear.com/products/hmt-1/. 2018.  

‚ÄúHMT-1Z1: The World‚Äôs Only Intrinsically Safe Hands-Free Remote Collaboration Tool.‚Äù 2018. 
https://www.realwear.com/products/hmt-1z1/. 

Sebillo, Monica, Giuliana Vitiello, Luca Paolino, and Athula Ginige. 2016. ‚ÄúTraining Emergency 
Responders through Augmented Reality Mobile Interfaces.‚Äù Multimedia Tools and Applications 
75 (16): 9609‚Äì22. https://doi.org/10.1007/s11042-015-2955-0. 

Sharma,  Sharad,  James  Stigal,  and  Sri  Teja  Bodempudi.  2020.  ‚ÄúSituational  Awareness-Based 
Augmented Reality Instructional (ARI) Module for Building Evacuation.‚Äù In Proceedings - 2020 
IEEE  Conference  on  Virtual  Reality  and  3D  User  Interfaces,  VRW  2020,  70‚Äì78.  Institute  of 
Electrical and Electronics Engineers Inc. https://doi.org/10.1109/VRW50115.2020.00020. 

‚ÄúStay Focused & Fully Present, Connected.‚Äù 2018. https://solos-wearables.com/. 

Tadokoro, Satoshi, Hiroaki Kitano, Tomoichi Takahashi, Itsuki Noda, Hitoshi Matsubara, Atsushi 
Shinjoh,  Tetsuo  Koto,  et  al.  2000.  ‚ÄúThe  Robocup-Rescue  Project:  A  Robotic  Approach  to  the 
Disaster  Mitigation  Problem.‚Äù  In  Proceedings  of  the  2000  IEEE  International  Conference  on 
Robotics 
US. 
https://doi.org/10.1109/robot.2000.845369. 

Automation, 

4:4089‚Äì94. 

Francisco, 

CA, 

San 

& 

Tajima,  Fumiko,  and  Hiroo  Kanamori.  1985.  ‚ÄúGlobal  Survey  of  Aftershock  Area  Expansion 
Patterns.‚Äù  Physics 
77‚Äì134. 
of 
https://doi.org/10.1016/0031-9201(85)90066-4. 

and  Planetary 

the  Earth 

Interiors 

(2): 

40 

‚ÄúThe Mixed Reality Toolkit (MRTK).‚Äù 2020. 2020. https://rb.gy/93r4nf. 

Thirdeye. 2020. ‚ÄúMarcus Hook Fire Department Webinar Recap.‚Äù 2020. https://rb.gy/giyucy. 

‚ÄúThirdEye Gen X2 Mixed Reality Smart Glasses.‚Äù 2019. 2019. https://thirdeyegen.com/x2-smart-
glasses. 

‚ÄúToshiba 

DynaEdge 

AR100 

Head 

Mounted 

Display.‚Äù 

2018. 

2018. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
https://us.dynabook.com/smartglasses/products/index.html. 

‚ÄúVuzix 
https://www.vuzix.com/products/m400-smart-glasses. 

Augmented 

Reality 

M400 

(AR) 

19 

Smart 

Glasses.‚Äù 

2020. 

Worrel, 
https://www.fudzilla.com/news/42622-odg-unveils-r8-and-r9-smartglasses-at-ces. 

Jon.  2017.  ‚ÄúODG  Unveils  R8  and  R9  Smartglasses  at  CES.‚Äù  2017. 

Xu Jiaqi, Derek Doyle, and Fernando Moreu. 2020. ‚ÄúState of the Art of Augmented Reality (AR) 
Capabilities for Civil Infrastructure Applications (in Submission).‚Äù Automation in Construction. 

Yang, Li, Yu Liang, Dalei Wu, and Jim Gault. 2018. ‚ÄúTrain and Equip Firefighters with Cognitive 
Virtual and Augmented Reality.‚Äù In 2018 IEEE 4th International Conference on Collaboration 
and  Internet  Computing  (CIC),  453‚Äì59.  Philadelphia,  PA,  USA:  Institute  of  Electrical  and 
Electronics Engineers Inc. https://doi.org/10.1109/CIC.2018.00068. 

Zhu, Yiqing, and Nan Li. 2021.  ‚ÄúVirtual  and Augmented  Reality Technologies for Emergency 
Management in the Built Environments: A State-of-the-Art Review.‚Äù Journal of Safety Science 
and Resilience 2 (1): 1‚Äì10. https://doi.org/10.1016/j.jnlssr.2020.11.004. 

Zoellner,  Michael,  Alain  Pagani,  Yulian  Pastarmov,  Harald  Wuest,  and  Didier  Stricker.  2008. 
‚ÄúReality  Filtering:  A  Visual  Time  Machine  in  Augmented  Reality.‚Äù  In  The  9th  International 
71‚Äì77. 
Symposium 
https://doi.org/10.2312/VAST/VAST08/071-077. 

on  Virtual  Reality,  Archaeology 

and  Cultural  Heritage, 

 
 
 
 
 
 
 
 
 
