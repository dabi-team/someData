1
2
0
2

n
a
J

5
1

]
E
S
.
s
c
[

2
v
6
3
6
2
0
.
1
0
1
2
:
v
i
X
r
a

Deep Reinforcement Learning for Black-Box Testing of
Android Apps

ANDREA ROMDHANA, DIBRIS - Università degli Studi di Genova, FBK-ICT, Security & Trust Unit
ALESSIO MERLO, DIBRIS - Università degli Studi di Genova
MARIANO CECCATO, Università di Verona
PAOLO TONELLA, Università della Svizzera italiana
The state space of Android apps is huge and its thorough exploration during testing remains a major challenge.
In fact, the best exploration strategy is highly dependent on the features of the app under test. Reinforcement
Learning (RL) is a machine learning technique that learns the optimal strategy to solve a task by trial and
error, guided by positive or negative reward, rather than by explicit supervision. Deep RL is a recent extension
of RL that takes advantage of the learning capabilities of neural networks. Such capabilities make Deep RL
suitable for complex exploration spaces such as the one of Android apps. However, state of the art, publicly
available tools only support basic, tabular RL. We have developed ARES, a Deep RL approach for black-box
testing of Android apps. Experimental results show that it achieves higher coverage and fault revelation than
the baselines, which include state of the art tools, such as TimeMachine and Q-Testing. We also investigated
qualitatively the reasons behind such performance and we have identified the key features of Android apps
that make Deep RL particularly effective on them to be the presence of chained and blocking activities.

CCS Concepts: • Software verification and validation → Empirical software validation.

Additional Key Words and Phrases: Deep reinforcement learning, Android testing

ACM Reference Format:
Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella. 2021. Deep Reinforcement Learning
for Black-Box Testing of Android Apps. ACM Trans. Softw. Eng. Methodol. 01, 1, Article 000 (January 2021),
26 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
The complexity of mobile applications (hereafter, apps) keeps growing, as apps provide always more
advanced services to the users. Nonetheless, it is of utmost importance that they work properly
once they are published on app markets as most of their success/failure depend on the user’s
evaluation. Therefore, an effective testing phase is fundamental to minimize the likelihood of app
failures during execution. However, automated testing of mobile apps is still an open problem and
the complexity of current apps makes their exploration trickier than in the past, as they can contain
states that are difficult to reach and events that are hard to trigger.

There exist several approaches to automated testing of mobile apps that aim at maximizing code
coverage and bug detection during testing. Random testing strategies [20, 30] stimulate the App

Authors’ addresses: Andrea Romdhana, andrea.romdhana@dibris.unige.it, DIBRIS - Università degli Studi di Genova,
FBK-ICT, Security & Trust Unit; Alessio Merlo, alessio.merlo@unige.it, DIBRIS - Università degli Studi di Genova; Mariano
Ceccato, mariano.ceccato@univr.it, Università di Verona; Paolo Tonella, paolo.tonella@usi.ch, Università della Svizzera
italiana.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2021 Association for Computing Machinery.
1049-331X/2021/1-ART000 $15.00
https://doi.org/10.1145/1122445.1122456

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

 
 
 
 
 
 
000:2

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

Under Test (AUT) by producing pseudo-random events. However, random exploration with no
guidance may get stuck when dealing with complex transitions. Model-Based strategies [4, 21, 39]
extract test cases from navigation models built by means of static or dynamic analysis. If the
model accurately reflects the AUT, a deep exploration can be achieved. Nonetheless, automatically
constructed models tend to be incomplete and inaccurate. Structural strategies [6, 16, 31] generate
coverage-oriented inputs using symbolic execution or evolutionary algorithms. These strategies
are more powerful, since they are guided by a specific coverage target. However, they do not take
advantage of past exploration successes to learn the most effective exploration strategy dynamically.
Reinforcement Learning (RL) is a machine learning approach that does not require a labeled
training set as input, since the learning process is guided by the positive or negative reward
experienced during the tentative execution of the task. Hence, it represents a way to dynamically
build an optimal exploration strategy by taking advantage of the past successful or unsuccessful
moves.

RL has been extensively applied to the problem of GUI and Android testing [34, 36]. However,
only the most basic form of RL (i.e., tabular RL) has been applied to testing problems so far. In tabular
RL the value of the state-action associations is stored in a fixed table. The advent of Deep Neural
Networks (DNN) replaced tabular approaches with deep learning ones, in which the action-value
function is learned from the past positive and negative experiences made by one or more neural
networks. When the state space to explore is extremely large (e.g. when an app has a big amount
of widgets), Deep RL has proved to be substantially superior to tabular RL [8] [37] [27]. In this
respect, we argue that the state space of Android apps is definitely a good candidate for a successful
adoption of Deep RL instead of tabular RL for testing purposes.

This paper presents the first Deep RL approach, ARES, for automated black-box testing of Android
apps. ARES uses a DNN to learn the best exploration strategy from previous attempts. Thanks to
such DNN, it achieves high scalability, general applicability and the capability to handle complex
app behaviors.

ARES implements multiple Deep RL algorithms that come with a set of configurable, often
critical, hyperparameters. To speed up both the selection of the most appropriate algorithm for the
AUT and the fine tuning of its hyperparameters, we have developed another tool, FATE, which
integrates with ARES.

FATE is a simulation environment that supports fast assessment of Android testing algorithms
by running synthetic Android apps (i.e., abstract navigational models of real Android apps). The
execution of a testing session on a FATE synthetic app is on average 10 to 100 times faster than the
execution of the same session on the corresponding real Android app.

We applied ARES to two benchmarks made by 41 and 68 Android apps, respectively. The first
benchmark compares the performance of the ARES algorithms, while the latter one evaluates ARES
w.r.t. the state-of-the-art testing tools for Android.

Experimental results confirmed the hypothesis that Deep RL outperforms tabular RL in the
exploration of the state space of Android apps, as ARES exposed the highest number of faults and
obtained the highest code coverage. Furthermore, we carried out a qualitative analysis showing
that the features of Android apps that make Deep RL particularly effective include, among others,
the presence of concatenated activities and blocking activities, protected by authentication.

To sum up, this paper provides the following advancements to the state of the art:

• ARES, the first publicly available testing approach based on Deep Reinforcement Learning,

released as open source;

• FATE, a simulation environment for fast experimentation of Android testing algorithms, also

available as open source;

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:3

• A thorough empirical evaluation of the proposed approach, whose replication package is

publicly available to the research community.

2 BACKGROUND
After a general overview on RL, this section presents in more detail Tabular RL and Deep RL.

2.1 Overview on Reinforcement Learning
The objective of Reinforcement Learning [41] is to train an agent that interacts with some environ-
ment to achieve a given goal. The agent is assumed to be capable of sensing the current state of the
environment, and to receive a feedback signal, named reward, each time the agent takes an action.
At each time step t, the agent receives an observation 𝑥𝑡 , takes an action 𝑎𝑡 that causes the
transition of the environment from state 𝑠𝑡 to state 𝑠𝑡 +1. The agent also receives a scalar reward
𝑅(𝑥𝑡, 𝑎𝑡, 𝑥𝑡 +1), that quantifies the goodness of the last transition.

For simplicity, let us assume 𝑥𝑡 = 𝑠𝑡 (in the general case, 𝑥𝑡 might be a subset of 𝑠𝑡 ). The behavior
of an agent is represented by a policy 𝜋, i.e., a rule for making the decision on what action to take,
based on the perceived state 𝑠𝑡 . A policy can be:

• Deterministic: 𝑎𝑡 = 𝜋 (𝑠𝑡 ), i.e. a direct mapping between states and actions;
• Stochastic: 𝜋 (𝑎𝑡 |𝑠𝑡 ), a probability distribution over actions, given their state.

DDPG [29] and TD3 [15] are examples of RL algorithms that learn a deterministic policy, while
SAC [22] is a RL algorithm that learns a stochastic policy.

The standard mathematical formalism used to describe the agent environment is a Markov

Decision Process (MDP). An MDP is a 5-tuple, ⟨𝑆, 𝐴, 𝑅, 𝑃, 𝜌0⟩, where :

• 𝑆 is the set of all valid states,
• 𝐴 is the set of all valid actions,
• 𝑅 : 𝑆 × 𝐴 → IR is the reward function, with 𝑟𝑡 = 𝑅(𝑠𝑡, 𝑎𝑡, 𝑠𝑡 +1),
• 𝑃 : 𝑆 ×𝐴 → 𝑃 (𝑠) is the transition probability function, with 𝑃 (𝑠𝑡 +1|𝑠𝑡, 𝑎𝑡 ) being the probability

of transitioning into state 𝑠𝑡 +1 starting from state 𝑠𝑡 and taking action 𝑎𝑡 ,

• 𝜌0(𝑠) is the starting state distribution.

Markov Decision Processes obey the Markov property: a transition only depends on the most recent
state and action (and not on states/actions that precede the most recent ones).

The goal in RL is to learn a policy 𝜋 which maximizes the so-called expected return, which can

be computed as:

𝑅(𝜏) =

∞
∑︁

𝛾𝑡𝑟𝑡

𝑡 =0
A discount factor 𝛾 ∈ (0, 1) is needed for convergence. It determines how much the agent cares
about rewards in the distant future relative to those in the immediate future. 𝜏 is a sequence of
states and actions in the environment 𝜏 = (𝑎0, 𝑠0, 𝑎1, 𝑠1...), named trajectory or episode. Testing an
Android app can be seen as a task divided into finite-length episodes.

To estimate the expected return in practice, it is convenient to express it using a value function
and an action-value function. The value function 𝑉 𝜋 (𝑠) is defined as the expected return starting in
a state 𝑠 and then acting according to a given policy 𝜋:

𝑉 𝜋 (𝑠) = 𝐸 [𝑅𝑡 |𝑠0 = 𝑠]
The action-value function 𝑄 𝜋 (𝑠, 𝑎) can be used to describe the expected return after taking an

action 𝑎 in state 𝑠 and thereafter following the policy 𝜋:

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:4

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

𝑄 𝜋 (𝑠, 𝑎) = 𝐸 [𝑅𝑡 |𝑠, 𝑎]
Correspondingly, we can define the optimal value function, 𝑉 ∗(𝑠), as the 𝑉 𝜋 (𝑠) that gives the
highest expected return when starting in state 𝑠 and acting according to the optimal policy in the
environment. The optimal action-value function, 𝑄 ∗(𝑠, 𝑎), gives the highest achievable expected
return under the constraints that the process starts at state 𝑠, takes an action 𝑎 and then acts
according to the optimal policy in the environment.

A policy that chooses greedy actions only with respect to 𝑄 ∗ is optimal, i.e., knowledge of 𝑄 ∗
alone is sufficient for finding the optimal policy. As a result, if we have 𝑄 ∗, we can directly obtain
the optimal action, 𝑎∗ (𝑠), via 𝑎∗ (𝑠) = argmax𝑎 𝑄 ∗ (𝑠, 𝑎). The optimal value function 𝑉 ∗ (𝑠) and
action-value function 𝑄 ∗ (𝑠, 𝑎) can be computed by means of a recursive relationship known as the
optimal Bellman equations:

𝑉 ∗(𝑠𝑡 ) = max

𝐸 [𝑟 (𝑠𝑡, 𝑎𝑡 ) + 𝛾𝑉 ∗ (𝑠𝑡 +1)]

𝑎
𝑄 ∗(𝑠𝑡, 𝑎𝑡 ) = 𝐸 [𝑟 (𝑠𝑡, 𝑎𝑡 ) + 𝛾 max
𝑎𝑡 +1

[𝑄 ∗ (𝑠𝑡 +1, 𝑎𝑡 +1)]]

2.2 Tabular RL
Tabular techniques refer to RL algorithms where the state and action spaces are approximated
using value functions defined by means of tables. In particular, Q-Learning [43] is one of the most
adopted algorithms of this family. Q-Learning aims at learning a so-called Q-Table, i.e., a table
that contains the value of each state-action pair. A Q-Table represents the current estimate of
the action-value function 𝑄 (𝑠, 𝑎). Every time an action 𝑎𝑡 is taken and a state 𝑠𝑡 is reached, the
associated state-action value in the Q-Table is updated as follows:

𝑄 (𝑠𝑡, 𝑎𝑡 ) := 𝑄 (𝑠𝑡, 𝑎𝑡 ) + 𝛼 (𝑟𝑡 + 𝛾 max

𝑎

𝑄 (𝑠𝑡 +1, 𝑎) − 𝑄 (𝑠𝑡, 𝑎𝑡 ))

where 𝛼 is the learning rate (between 0 and 1); 𝛾 is the discount factor, applied to the future reward.
Typically, 𝛾 ranges from 0.8 to 0.99 [35] [29] [15], while max𝑎 𝑄 (𝑠𝑡 +1, 𝑎) gives the maximum value
for future rewards across all actions. It is used to update the reward for the current state-action
pair.

RL algorithms based on the tabular approach do not scale to high-dimensional problems, because
in such cases it is difficult to manually define a good initial Q-Table. In case a good initial Q-Table
is not available, convergence to the optimal table by means of the update rule described above is
too slow [29].

2.3 Deep Reinforcement Learning
In large or unbounded discrete spaces, where representing all states and actions in a Q-Table is
impractical, tabular methods become highly unstable and incapable to learn a successful policy[35].
The rise of deep learning, relying on the powerful function approximation properties of deep neural
networks, has provided new tools to overcome these limitations. One of the first deep reinforcement
learning algorithms is DQN (Deep Q-Networks) [35].

DQN uses convolutional neural networks to approximate the computation of the action-value
function 𝑄 𝜋 . Training of such neural networks is achieved by means of memory replay: the last 𝑁
experience tuples are sampled uniformly and replayed when updating the weights of the networks.
While DQN can indeed solve problems with high-dimensional observation spaces, it can only
handle discrete and low-dimensional action spaces. The recent advancements over DQN described
in the following paragraphs (namely, DDPG, TD3 and SAC) overcome such limitation and allow
dealing with high-dimensional action spaces.

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:5

Deep Deterministic Policy Gradient (DDPG). DDPG [29] is an Actor-Critic algorithm, i.e., it
includes two roles: the Critic, which estimates the value function, and the Actor, which updates the
policy 𝜋 in the direction suggested by the Critic. It is based on a deterministic policy gradient [38]
that can operate over continuous action spaces.

More specifically, the Critic has the objective of learning an approximation of the function 𝑄 ∗ (𝑠).
Suppose that the approximator is a neural network 𝑄𝜙 (𝑠, 𝑎), with parameters 𝜙, and that we have
a set D of transitions (𝑠𝑡, 𝑎𝑡, 𝑟𝑡, 𝑠𝑡 +1, 𝑑), where d=1 if 𝑠𝑡 +1 is a final state; 0 otherwise. Such set D of
previous experiences is called replay buffer. To avoid overfitting, the replay buffer should be large
enough. The loss function of the neural approximator is the so-called Mean-Squared Bellman Error
(MSBE), which measures how close 𝑄𝜃 (𝑠, 𝑎) is to satisfying the Bellman equation:
𝐿(𝜙, 𝐷) = 𝐸 [(𝑄𝜙 (𝑠𝑡, 𝑎𝑡 ) − (𝑟𝑡 + 𝛾 (1 − 𝑑) max
𝑎𝑡 +1

𝑄𝜙 (𝑠𝑡 +1, 𝑎𝑡 +1)))2]

The term subtracted from 𝑄𝜙 (𝑠𝑡, 𝑎𝑡 ) is named the target, because minimization of MSBE makes
the Q-function as close as possible to this value. Since the target depends recursively on the same
parameter 𝜙 to train, MSBE minimization can become unstable. The solution is to use a second
neural network, called target network, whose parameters are updated to be close to 𝜙, but with
some time delay that gives stability to the process.

The Actor’s goal is to learn a deterministic policy 𝜋𝜃 (𝑠) which maximizes 𝑄𝜙 (𝑠, 𝑎). Because the
action space is continuous and we assume the Q-function is differentiable with respect to the action,
we can use gradient ascent with respect to the policy parameter.

Twin Delayed DDPG (TD3). Although DDPG can often achieve good performance, it tends to
be susceptible to critical tuning of its hyperparameters. This happens when the hyperparameters
of the Q-function make it overestimate the Q-values, which in turn leads to policy failure, as the
Actor is actually exploiting such errors as guidance.

TD3 [15] is an algorithm which addresses this issue by introducing three major changes, mostly
on the Critic side: (1) Clipped Double Q-Learning: TD3 learns two Q-functions instead of one, and
uses the smaller of the two Q-values as the target in the MSBE function. (2) Delayed Policy Update:
TD3 updates the policy and the target networks less frequently than the Q-function. (3) Target
Policy Smoothing: TD3 adds noise to the target action to make it harder for the policy to exploit
Q-function errors, by smoothing out Q across changes of the action.

Soft Actor Critic (SAC). The central feature of SAC [22] is entropy regularization. Using the
entropy-regularized method, an agent gets a bonus reward at each time step which is proportional
to the entropy of the policy at that time step. In fact, differently from TD3, the policy of SAC is non
deterministic and inclusion of entropy in the reward aims at promoting policies with a wider spread
of alternatives to choose stochastically from. The RL problem becomes the problem of finding the
optimal policy 𝜋 ∗ according to the following equation:

𝜋 ∗ = argmax

𝐸 [

𝛾𝑡 (𝑅(𝑠𝑡, 𝑎𝑡, 𝑠𝑡 +1) + 𝛼𝐻 (𝜋 (·|𝑠𝑡 )))]

∞
∑︁

𝜋
where 𝐻 is the entropy, and 𝛼 > 0 is the trade-off coefficient.

𝑡 =0

3 APPROACH
In this section, we describe ARES (Application of REinforcement learning to android Software
testing), our approach to black-box Android GUI testing based on Deep RL. The overview of the
approach is shown in Figure 1. The RL environment is represented by the Android application
under test (AUT), which is subject to several interaction steps. At each time step, assuming the
GUI state is 𝑠𝑡 , ARES first takes an action 𝑎𝑡 . Then, it observes the new GUI state 𝑠𝑡 +1 of the AUT

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:6

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

Fig. 1. The ARES testing workflow

and a reward 𝑟𝑡 is computed. Intuitively, if the new state 𝑠𝑡 +1 is similar to the prior state 𝑠𝑡 , the
reward is negative. Otherwise, the reward is a large positive value. In this way, ARES promotes the
exploration of new states in the AUT, under the assumption that this is useful to test the application
more thoroughly.

The reward is used to update the neural network, which learns how to guide the Deep RL
algorithm to explore the AUT in depth. The actual update strategy depends on the deep RL
algorithm being used (either DDPG, TD3 or SAC).

3.1 Problem Formulation
To apply RL, we have to map the problem of Android black-box testing to the standard mathematical
formalization of RL: an MDP, defined by the 5-tuple, ⟨𝑆, 𝐴, 𝑅, 𝑃, 𝜌0⟩. Moreover, we have to map the
testing problem onto an RL task divided into several finite-length episodes.

State Representation. Our approach is black-box, because it does not access the source code of
the AUT. It only relies on the GUI of the AUT. The state 𝑠𝑡 ∈ 𝑆 is defined as a combined state
(𝑎0, ...𝑎𝑛, 𝑤0, ...𝑤𝑚). The first part of the state 𝑎0, ...𝑎𝑛 is a one hot encoding of the current activity,
i.e., 𝑎𝑖 is equal to 1 only if the currently displayed activity is the 𝑖-th activity, it is equal to 0 for
all the other activities. In the second part of the state vector, 𝑤 𝑗 is equal to 1 if the 𝑗-th widget is
available in the current activity; it is equal to 0 otherwise.

Action Representation. User interaction events in the AUT are mapped to the action set 𝐴 of
the MDP. ARES infers executable events in the current state by analyzing the dumped widgets and
their attributes (i.e., clickable, long-clickable, and scrollable). In addition to the widget-level actions
we also model two system-level actions, namely toggle internet connection and rotate screen. These
system-level actions are the only system actions that can be easily tested. In fact, since Android
version 7.0, testing other system-level actions (like those implemented in Stoat [39]) would depend
on the Android version used [40], [19], and would require a rooted device [17]. In fact, working
with a rooted device compromise the testing of protected apps [42], such as apps in the finance
category.

Each action 𝑎 is 3-dimensional: the first dimension represents the widget ARES is going to
interact with or the identifier of a system action. The second dimension specifies a string to be
used as text input, if needed. Actually, an index pointing to an entry in a dictionary of predefined
strings is used for this dimension. The third dimension depends on the context: when the selected
widget is both clickable and long-clickable, the third action determines which of the two actions to

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:7

take. When ARES interacts with a scrollable object, the third dimension determines the scrolling
direction.

Transition Probability Function. The transition function 𝑃 determines which state the applica-
tion can transit to after ARES has taken an action. In our case, this is decided solely by the execution
of the AUT: ARES observes the process passively, collecting the new state after the transition has
taken place.

Reward Function. The RL algorithm used by ARES receives a reward 𝑟𝑡 ∈ 𝑅 every time it executes
an action 𝑎𝑡 . We define the following reward function:

Γ1
−Γ2
−Γ3

if 𝑎𝑐𝑡 (𝑠𝑡 +1) ∉ 𝑎𝑐𝑡 (𝐸𝑡 ) or crash
if 𝑝𝑎𝑐𝑘 (𝑎𝑐𝑡 (𝑠𝑡 +1)) ≠ 𝑝𝑎𝑐𝑘 (𝐴𝑈𝑇 )
otherwise.

(1)

𝑟𝑡 =





with Γ1 ≫ Γ2 ≫ Γ3 (in our implementation Γ1 = 1000, Γ2 = 100, Γ3 = 1).

The exploration of ARES is divided into episodes. At time 𝑡, the reward 𝑟𝑡 is high (Γ1) if ARES
was able to transition to an activity never observed during the current episode 𝐸𝑡 (i.e., the next
activity 𝑎𝑐𝑡 (𝑠𝑡 +1) does not belong to the set of activities encountered so far in 𝐸𝑡 ): if a new episode
is started at 𝑡 + 1, its set of activities is reset: 𝑎𝑐𝑡 (𝐸𝑡 +1) = ∅.

Resetting the set of encountered activities at the beginning of each new episode is a technique
that encourages ARES to visit and explore the highest number of activities in each episode, so
as to continuously reinforce its explorative behaviors. In contrast, giving a positive reward for
each new activity only once, or, on the contrary, each time we see an activity change, would result
respectively in too sparse, singular reinforcement feedback or in rewarding of cycling behaviors
[12].

The reward is high (Γ1) also when a faulty behavior (crash) occurs. It is very low (−Γ2) when the dis-
played activity does not belong to the AUT (i.e., the package of the current activity, 𝑝𝑎𝑐𝑘 (𝑎𝑐𝑡 (𝑠𝑡 +1)),
is not the package of the AUT), as we aim to explore the current AUT only. In all other cases,
the reward is moderately negative (−Γ3), as the exploration still remains inside the AUT, even if
nothing new has been discovered.

4 IMPLEMENTATION
ARES features a custom environment based on the OpenAI Gym [9] interface, which is a de-facto
standard in the RL field. OpenAI Gym is a toolkit for designing and comparing RL algorithms
and includes a number of built-in environments. It also includes guidelines for the definition of
custom environments. Our custom environment interacts with the Android emulator [18] using
the Appium Test Automator Framework [3].

4.1 Tool Overview
As soon as it is launched, ARES leverages Appium to dump the widget hierarchy of the GUI in the
starting activity of the AUT. The widget hierarchy is analyzed searching for clickable, long-clickable
and scrollable widgets. These widgets are afterwards stored in a dictionary containing several
associated attributes (e.g., resource-id, clickable, long-clickable, scrollable, etc.) and compose the
action vector, i.e., the vector of executable actions in the current state. At each time step, ARES
takes an action according to the behavior of the exploration algorithm. Once the action has been
fully processed, ARES extracts the new widget hierarchy from the current GUI and calculates its
MD5 hash value. If it has the same MD5 value of the previous state, ARES leaves the action vector
unchanged. If the MD5 value does not match, ARES updates the action vector. ARES performs the

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:8

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

observation of the AUT state and returns the combined vector of activities and widgets. ARES
organizes the testing of each app as a task divided into finite-length episodes. The goal of ARES is
to maximize the total reward received during each episode. Every episode lasts at least 250 time
steps. Its duration is shorter only if the app crashes. Once an episode comes to an end, the app is
restarted and ARES uses the acquired knowledge to explore the app more thoroughly in the next
episode.

4.2 Application Environment
The application environment is responsible for handling the actions to interact with the AUT. Since
the environment follows the guidelines of the Gym interface, it is structured as a class with two
key functions. The first function init(desired_capabilities) is the initialization of the class.
The additional parameter desired_capabilities consists of a dictionary containing the emulator
setup along with the application to be tested. The second function is the step(a) function, that
takes an action a as command and returns a list of objects, including observation (current AUT
state) and reward.

4.3 Algorithm Implementation
ARES is a modular framework that adopts a plugin architecture for the integration of the RL
algorithm to use. Hence, extension with a new exploration algorithm can be easily achieved. In
the current implementation, ARES provides five different exploration strategies: (1) random, (2)
Q-Learning, (3) DDPG, (4) SAC, (5) TD3. The random algorithm interacts with the AUT by randomly
selecting an action from those in the action vector. Compared to Monkey [20], our random approach
performs better, since it selects only actions from the action vector. In fact, Monkey generates
random, low level events on the whole GUI, which could target no actual widget and then be
discarded.

Our Q-Learning strategy implements the algorithm by Watkins and Dayan [43]. The deep RL
algorithms available in ARES are DDPG, SAC and TD3. Their implementation comes from the
Python library Stable Baselines [23], and allow ARES to save the status of the neural network
as a policy file, at the end of the exploration. In this way, the policy can be loaded and reused
on a new version of the AUT at a later stage, rather than restarting ARES from scratch each
time a new AUT version is released. ARES is publicly available as open source software at https:
//github.com/H2SO4T/ARES.

4.4 Compatibility
ARES has been successfully tested on Windows 10, MacOS 11.1 (and older), Ubuntu 20 (and older)
and Scientific Linux 7.5. ARES is fully compliant with parallel execution and enables parallel
experiments to be performed on emulators or real devices, handling each instance in a completely
separate manner. ARES is also compatible with several Android versions (i.e., it has been successfully
tested on Android 6.0, 7.0, 7.1, 8.0, 8.1, 9.0, and 10.0). Moreover, since ARES is based on the standard
OpenAIGym, new algorithms and exploration strategies can be easily added to the tool.

5 FAST ANDROID TEST ENVIRONMENT (FATE)
Deep RL algorithms require fine tuning, which is expensive on real apps. Therefore, we developed
FATE, a simulation environment for fast Android testing. FATE models only the navigation con-
straints of Android apps, so it can be used to efficiently compare alternative testing algorithms and
to quickly tune their corresponding hyperparameters. After this algorithm selection and tuning
phase through FATE is completed, the selected algorithms and their configurations are ported to
ARES to test real apps.

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:9

5.1 Model-based Prototyping
In FATE, developers model an Android app by means of a deterministic Finite State Machine (FSM)
F = (Σ, 𝑆, 𝑠0, 𝛿, 𝐹 ), where Σ is a set of events, 𝑆 a set of states with 𝑠0 the initial state and 𝐹 the set of
final states, and 𝛿 the state transition function 𝛿 : 𝑆 × Σ −→ 2𝑆 . The states 𝑆 of the FSM correspond
to the activities of the app, while the events Σ trigger the transitions between activities, which
in turn are modeled as a transition table 𝛿. Events represent the clickable widgets (e.g., buttons)
available in each activity. Transitions have access to a set of global variables and possess, among
others, the following attributes: ID, type, active (boolean attribute), guard (boolean expression that
prevents the transition from being taken if it evaluates to false), set (new values to be assigned to
global variables), destination (target activity, i.e., value of 𝛿).

Fig. 2. FATE model of Social Network: global variables are shown on the top-left; inputs on the bottom-left;
red edges indicate non deterministic transitions.

Figure 2 shows the FATE model of the prototypical app Social Network. To build such model,
developers can use Ptolemy [26] to graphically draw a FSM that mimics the behavior of the applica-
tion. While creating a FSM with Ptolemy is not mandatory in FATE, it simplifies the job of designing
a logically correct model, thanks to the checks it performs. Then, FATE translates automatically
the Ptolemy model (saved in XML format) into a Json file that replicates the structure and behavior
of the Ptolemy model. The Json model translation has two main fields: global_vars and nodes.
The first contains a list of global variables organized by name and value. The latter contains a list
of all the activities. Each activity is characterized by a node_id and a list of corresponding node
transitions, each including all the respective transition attributes.

The model of Social Network in Figure 2 contains a transition from login to main_act which is
subjected to the guard user_pass == real_pass, i.e., the entered password must be correct in
order for the transition to be taken. In the Json model, such transition is coded as:

1 " transition ": {
2

" transition_id ": 0,
" type ": " button " ,
" active ": true ,
" guard ": " user_pass == real_pass " ,
" set ": null ,
" destination ": " main_act "

3

4

5

6

7

8 }

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:10

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

Another example of guarded transition is the one between messages and chat_x. The guard
count_messages >= 1 checks whether there exists at least one message from which a chat thread
can be started. In the Json model this is coded as:

1 " transition ": {
2

" transition_id ": 0,
" type ": " button " ,
" active ": true ,
" guard ": " count_messages >= 1" ,
" set ": null ,
" destination ": " main_act "

3

4

5

6

7

8 }

In FATE, a Python environment compliant with the OpenAI Gym standard takes as input the
Json app model and tests it automatically using the selected algorithm. The available algorithms
are: (1) Random, (2) Q-Learning, (3) DDPG, (4) SAC, (5) TD3. FATE was built with modularity in
mind and new exploration algorithms can be easily added to the tool.

Compared to testing an Android app through Espresso or Appium, FATE makes test case execution
extremely faster, because there is no need to interact with the app via its GUI. Moreover, the
application navigation logic is simulated by the transition function 𝛿, which makes it usually
much faster to execute. As a consequence, developers can run a large number of experiments,
evaluate multiple algorithms, check various algorithm or application configurations, and find
the optimal set of hyperparameters, all of which would be prohibitively expensive to execute
on a standard Android testing platform. FATE is publicly available as open source software at
https://github.com/H2SO4T/ARES.

5.2 Representative Family of Models
For fast evaluation of the Deep RL algorithms implemented in ARES, we modeled four Android
apps using FATE. Each model is configurable with a variable degree of complexity. To obtain a set
of app models that are representative of the most common apps used in everyday life, we inspected
AppBrain (a website that aggregates Google Play Store statistics and rankings) [2] and selected
four different and representative categories from the top-ten: Music & Audio, Lifestyle, Business,
and Shopping. From each category we then selected and modeled in FATE one prototypical app:
Player, Social Network, Bank and Market Place.

The simplest scenario is Player. It features a wide number of activities arranged in a tree-like
structure. It reflects the generalization of a variety of applications, including apps or app components
to manage the settings and to stream/add/remove media contents. Social Network (see Figure 2)
starts by prompting a login activity with fields for username and password. Following the login
activity we have several activities that replicate the behavior of a standard social network, including
a basic chat activity. The Bank model is characterized by the presence of inner password-protected
operations. Market Place models a typical app for e-commerce: the user can search for goods, login,
purchase products, and monitor the orders. The four representative app models used in this work
are publicly available inside the FATE tool.

6 EVALUATION
We seek to address the following research questions, split between the following two studies:

Study 1 (FATE):

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:11

• RQ1 Which deep RL algorithm and which algorithm configuration performs better on the

synthetic apps?

• RQ2 How does activity coverage vary as the model of the AUT becomes increasingly difficult to

explore?

• RQ3 What are the features of the synthetic apps that allow deep RL to perform better than

Q-Learning?

• RQ4 Are the results of synthetic apps comparable to those of the their translated counterparts?

In Study 1, we take advantage of the fast execution granted by FATE to compare alternative
RL algorithms (RQ1) and determine their optimal configuration (see Appendix 1). Since in this
study we use synthetic apps generated from models (i.e., Player, Social Network, Bank and Market
Place), coverage is measured at the granularity of Android activities. In fact, there is no source
code implementing the business logic. Text inputs are chosen from a pool of 20 strings, which
include credentials necessary to pass through the login activities. To account for non determinism,
we executed each algorithm 60 times for each hyperparameter configuration of the algorithms,
and applied the Wilcoxon non-parametric statistical test to draw conclusions on the difference
between algorithms and configurations, adopting the conventional 𝑝-value threshold at alpha. Since
multiple pairwise comparisons are performed with overlapping data, the chance to reject true null
hypotheses may increase (type I error). To control this problem, we adopt the Holm-Bonferroni
correction [24], that consists in using more strict significance levels when the number of pairwise
tests increases.

Each run has a length of 4000 time steps, which is close to an hour of testing in a real Android

test setting. With FATE, 4000 times steps are executed approximately in 200 seconds.

To answer RQ2 we consider the best performing configuration of the Deep RL algorithms, as
selected from RQ1, and gradually increase the exploration complexity of the apps. Specifically,
20_strings, 40_strings, 80_strings indicate an increasing size of the string pool (from 20, to 40 and
80), while augmented_5 and augmented_10 indicate an increasing size of the self navigation links
(with 5 or 10 “dummy” buttons) within the login activities. For the assessment, we adopt the widely
used metric AUC (Area Under the Curve), measuring the area below the activity coverage plot over
time. To account for the non determinism of the algorithms, we repeated each experiment 30 times
and applied the Wilcoxon non-parametric statistical test.

In RQ3 we investigate qualitatively the cases where Deep RL is superior to tabular Q-Learning.
With RQ4, we want to understand if results obtained on synthetic app models run by FATE correlate
with those obtained when executing the same apps, once they are translated into real Android
apps in Java code that can be executed by ARES. In particular, we translated 3 synthetic apps to
Java/Android: Social Network, Bank and Market Place. We then repeated the scenario 40_strings on
them using ARES. We compare the rankings of the algorithms being evaluated that are produced
respectively by FATE and by ARES.

Study 2 (ARES):

• RQ5 How do code coverage and time-dependent code coverage compare between Random,

Q-Learning, DDPG, and SAC?

• RQ6 What are the fault exposure capabilities of the alternative approaches?
• RQ7 What features of the real apps make deep RL perform better than Q-Learning?
• RQ8 How does ARES compare with state-of-the-art tools in terms of coverage and bug detection?

In Study 2, we use real apps and compare the alternative Deep RL algorithms between each other
and with Random and tabular Q-Learning. At last, we compare ARES to state of the art testing
tools.

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:12

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

To address RQ5-RQ6 and RQ7, we randomly selected 41 apps among the 500 most starred F-Droid
apps available on GitHub. We consider coverage at the source code level and compare both the final
coverage as well as the coverage increase over time (RQ5). To obtain coverage data we instrumented
each app by means of JaCoCo [33]. As in Study 1, we measured AUC and compared AUC values
using the Wilcoxon statistical test with significance level set to 0.05 (with correction). We exclude
TD3 from the comparison, since it performed consistently worse than the other RL algorithms on
synthetic apps.

In addition to code coverage, we also report the number of failures (unique app crashes) triggered
by each approach (RQ6). To measure the number of unique crashes observed, we parsed the output
of Logcat and (1) removed all crashes that do not contain the package name of the app; (2) extracted
the stack trace; (3) computed the hash code of the sanitized stack trace, to uniquely identify it. With
RQ7 we analyze qualitatively the different performance of Deep RL vs Q-Learning on real apps.

To address RQ8, we evaluate and compare ARES and state of the art tools in terms of code
coverage and number of crashes, using two different sets of apps under test, RQ8-a and RQ8-b,
that accommodate the different requirements and constraints of the tools being compared. As
state of the art tools, we selected Monkey [20], Sapienz [32], TimeMachine [14] and Q-Testing
[36]. In RQ8-a we compare ARES, Monkey, Sapienz and TimeMachine on a set of 68 apps coming
from AndroTest [10]. These apps are instrumented using Emma [1], the same coverage tool that is
used in Sapienz and TimeMachine. In RQ8-b, we compare ARES to Q-Testing on a set of ten apps
instrumented using JaCoCo, the coverage tool supported by Q-Testing.

All experimental data was generated and processed automatically. Each experiment was con-
ducted with a 1 hour timeout and was repeated 10 times, for a total of 4560 hours (≈ 190 days). The
emulators involved in the study are equipped with 2 GB of RAM and Android 10.0 (API Level 29)
or Android 4.4 only for the tool comparison.

6.1 Experimental Results: Study 1

App
Player

Social

Bank

Market

Config
20_str
20_str
40_str
80_str
aug_5
aug_10
20_str
40_str
80_str
aug_5
aug_10
20_str
40_str
80_str
aug_5
aug_10

Rand
88719
15840
9291
4535
13960
5291
22894
9746
3998
12815
4121
19236
15943
15944
18917
4121

Q-Learn
89022
20387
7737
5640
15400
3998
21622
7750
4843
8634
6289
18471
16496
15945
16377
6289

TD3
89903
22809
14451
5730
13094
13737
29159
9305
4776
8702
13289
20980
15949
15935
16500
13289

SAC
89943
25463
10361
7254
17402
11559
28016
16535
5621
14914
14195
23403
15936
15937
21208
14195

DDPG
90337
30008
7363
4774
13385
8870
36977
6458
4798
11472
15361
25923
16318
15932
16027
15361

Effect Size
-
L(Rand), M(Q)
S(DDPG)
-
-
M(Q, Rand)
M(Q, Rand)
S(Q, DDPG)
-
-
M(Q, Rand)
-
-
-
-
-

Table 1. AUC for synthetic apps: effect size between winner (shaded cell) and other algorithms is reported
only when 𝑝-value is statistically significant (S = Small; M = Medium; L = Large)

√

Figure 3 shows the coverage growth for the synthetic app Social. Each curve shows the mean
over 60 runs. The shaded area around the mean represents the Standard Error of the Mean (SEM =
𝑛, where 𝜎 is the standard deviation and 𝑛 = 60 the number of points). The highest activity
𝜎/
coverage is obtained consistently by deep RL algorithms, which correspondingly have also higher
AUC values. Table 1 reports the AUC obtained on the synthetic apps in all tested configurations.

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:13

Fig. 3. Activity coverage of the Social synthetic app in FATE

Table 1 also shows the Vargha-Delaney effect size in the case of a 𝑝-value < 𝛼 (with correction),
between the winner algorithm (highest AUC) and the remainders.

Results show that deep RL algorithms achieve higher coverage in most experiments. DDPG
performs better in the simplest configuration, 20_strings, while SAC performs better in almost all
other configurations, including the most complex ones. Q-Learning prevails in only two scenarios
belonging to Market Place, but the difference from the other algorithms is not statistically significant
(𝑝-value > 𝛼).

RQ1: Results lead us to select DDPG and SAC as best performing deep RL algorithms, to be
involved in Study 2 experiments.

DDPG is selected due to its high performance in fairly simple scenarios; SAC because of its

ability to adapt and maintain good performance in the majority of scenarios.

RQ2: While in simple situations (e.g., the Player app), all algorithms achieve a high level
of coverage, when things get more complex (e.g., when string pool increases), Deep RL
algorithms retain higher coverage than all other algorithms.

We have manually inspected the step by step exploration performed by Q-Learning and by
the Deep RL algorithms. We found that login activities complicate substantially the exploration
performed by Q-Learning. In fact, it is more difficult to reproduce the right username-password
combination for a tabular Q-Learning algorithm, which has limited adaptation capabilities, whereas
Deep RL algorithms memorize the right combination in the DNN used to guide the exploration. In
addition, large action spaces make it challenging for Q-Learning to learn an effective exploration
strategy, while the DNNs used by deep RL algorithms can easily cope with large spaces of alternatives

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:14

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

to choose from. This is confirmed by the performance degradation of Q-Learning as the string
pool becomes larger or as new interactive elements (“dummy” buttons) are added, which confuse
Q-Learning during its exploration.

RQ3: The performance of Q-Learning declines in the presence of blocking activities that
require specific input combinations that must be learned from past interactions or when
the input/action space becomes excessively large, while Deep RL can learn how to cope
with such obstacles thanks to the DNN employed to learn the exploration strategy.

Table 2 shows the ranking of the algorithms produced by ARES vs FATE on the 3 apps that were
translated from the synthetic FATE models to Java/Android. Below the ranking, Table 2 shows the
AUC values obtained by the respective algorithms. The behaviors of the considered algorithms
on synthetic (FATE) vs translated (ARES) apps are very similar. The AUC values are quite close
and the Spearman’s correlation between AUC values across algorithms is 0.99 for Social, 0.89 for
Bank and 0.99 for Market; it is 0.95 overall. All correlations are statistically significant at level 0.05.
ARES required 450 hours to complete the experiments. FATE required around 10 hours, reducing
the computation time by a factor 45, while producing similar results as ARES.

App
Social

Tool
ARES

FATE

Bank

ARES

FATE

Market

ARES

FATE

Q-Learn
4: 7788
Q-Learn
4:7737
Q-Learn
4: 8614
Q-Learn
4: 7750
Q-Learn
1: 16866
Q-Learn
1: 16496

Ranking / AUC
Rand
DDPG
3: 9547
5: 6802
Rand
DDPG
3:9291
5:7363
Rand
DDPG
3: 9344
5: 7976
Rand
DDPG
2: 9746
5: 6458
Rand
DDPG
4: 15936
2: 16788
Rand
DDPG
4: 15943
2: 16318

SAC
2: 9594
SAC
2:10361
SAC
1: 12138
SAC
1: 16535
SAC
5: 15930
SAC
5: 15936

TD3
1: 15101
TD3
1:14451
TD3
2: 10932
TD3
3: 9305
TD3
3: 15944
TD3
3: 15949

Table 2. Ranking of algorithms produced by ARES vs FATE; AUC values below ranked algorithms

RQ4: The results obtained on synthetic apps are comparable to those obtained on their
translated counterparts.

6.2 Experimental Results: Study 2
Table 3 shows coverage and crashes produced by each algorithm deployed in ARES. The highest
average coverage and average number of crashes over 10 runs are shaded in gray for each app. We
grouped the apps into three different size categories (Low-Medium, Medium, and High), depending
on their ELOC (Executable Lines Of Code). Inspecting the three categories, results show that the
Deep RL algorithms arise more often as winners when the ELOC increase. Usually, larger size apps
are more sophisticated and offer a richer set of user interactions, making their exploration more
challenging for automated tools. We already know from Study 1 that when the action space or the
observation space of the apps increase, Deep RL can infer the complex actions needed to explore
such apps more easily than other algorithms. Study 2 confirms the same trend.

Overall, SAC achieves the best performance, with 42.61% instruction coverage and 0.3 faults
detected on average. DDPG comes next, with 40.09% instruction coverage and 0.12 faults detected

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:15

Applications

Silent-ping-sms
Drawablenotepad
SmsMatrix
Busybox
WiFiKeyShare
Talalarmo
AquaDroid
Lexica
Loyalty-card-locker
Dns66
Gpstest
Memento
Editor
AndOTP
BookyMcBookface
Tuner
WifiAnalyzer
AdAway
Gpslogger
Connectbot
Neurolab
Anuto
PassAndroid
Markor
Vanilla
Average
Afwall
OpenTracks
Opentasks
UserLAnd
Simple-Solitaire
Authorizer
YalpStore
CameraRoll
AntennaPod
Phonograph
Average
MicroMathematics
LightningBrowser
Firefox-focus
RedReader
Wikipedia
Slide
Average

Total Average
Unique crashes

ELOC Rand Q

SAC DDPG Rand Q

SAC DDPG

%Coverage(mean)

#Crashes(mean)

263
452
466
540
627
1094
1157
1215
1228
1264
1311
1336
1547
1560
1595
2207
2511
3064
3201
3904
3954
4325
4569
4607
4747

5130
5260
5772
5901
5907
5923
6734
6836
7975
8758

41
20
23
75
37
69
55
72
41
58
47
77
50
20
26
80
78
38
36
26
29
46
1
51
29
45

12
45
43
60
10
5
35
32
46
16
30.4

41
21
20
73
36
71
55
72
37
58
46
76
46
25
25
74
75
37
31
25
28
46
1
43
34
43.84

12
42
50
60
30
5
34
31
40
16
30.5

10506
11961
12482
12958
23543
30483

35
36
34
42
43
17

35
35
33
42
42
19
34.33 34.50

41
26
24
74
37
71
55
74
50
58
47
74
51
27
25
79
80
45
32
28
29
47
1
53
41
46.76

16
44
53
60
31
5
38
31
48
16
34.2

47
43
41
44
44
18
39.5

41
25
22
76
37
71
55
75
41
58
46
77
50
20
24
75
79
40
28
18
28
47
1
41
33
44.32

13
45
44
60
31
5
33
32
38
16
31.7

41
37
35
46
41
19
36.5

39.62 39.58

42.61

40.09

0
0.7
1.2
0
0
0
1.0
0.3
0.5
0.1
0
0
0
0.5
0
0
0
0
0
0
0
0
0
0.3
0
0.15

0
0
0
0.1
0
0
0
0.8
0.5
0
0.14

0
0
0.5
0
0
0.8
0.21

0.17
73

0
0.6
0
0
0
0.5
0.4
0.1
0.4
0
0
0
0
0.5
0
0
0
0
0
0
0.4
0
0
0
0
0.12

0
0
0
0.2
0.4
0
0
0.1
0.1
0
0.08

0
0
0.3
0
0
0.3
0.1

0.1
43

0
0.7
1.2
0
0
0.5
0.8
1.5
0.8
0
0
0
0
0.7
0
0
0
0.1
0
0
0.3
0
0
0.4
0
0.28

0
0
0.2
0.4
0.4
0
0
1.6
0.8
0
0.34

0
0.4
0.8
0.1
0
1.2
0.38

0.3
102

0
0.1
0.9
0
0
0
0.3
1.2
0.1
0.2
0
0
0
0.2
0
0
0
0.1
0.1
0
0.6
0
0
0
0
0.15

0
0
0
0.2
0.2
0
0
0.1
0.4
0
0.09

0
0.1
0.1
0
0
0.3
0.1

0.12
52

Table 3. Average coverage and number of crashes observed on 41 real open-source apps in 10 runs of ARES

on average. To further investigate these results, we computed the AUCs reached by each algorithm
and we applied the Wilcoxon test to each pair of algorithms. Table 4 shows the AUCs achieved by
the 4 algorithms and the Vargha-Delaney effect size between the winner and the other algorithms
when the 𝑝-value is less than 𝛼. SAC results as the winner on 56% of the considered real apps,
followed by Random (34%). Moreover, Table 4 confirms the trend observed in Table 3: as ELOC
increase, a higher proportion of Deep RL algorithms produces the highest AUC. Figure 4 shows
an example of code coverage over time for the app Loyalty-card-locker, averaged on 10 runs. SAC
increases its coverage almost until the end of the exploration, while the other algorithms reach a
plateau after around 35 minutes of exploration.

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:16

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

RQ5: SAC reached the highest coverage in 24/41 apps, followed by DDPG (11 apps), Random
(10 apps), and Q-Learning(1 app). SAC has also the highest AUC in 24/41 apps, followed by
Random (13 apps), Q-Learning (11 apps) and DDPG (5 apps).

Fig. 4. Instruction coverage over time for the app Loyalty-card-locker

Table 3 shows that SAC exposed the highest number of unique crashes (102), followed by Random
(73), DDPG (52) and Q-Learning (43). The most common types of error exposed by SAC during
testing are: RuntimeException (34 occurrences), NullPointerException (14), IllegalArgumentException
(13). As shown in Figure 5, there is around thirty percent overlap between the crashes found by
SAC and the other algorithms. The overlap with Random is the highest. SAC discovers about 40%
of unique crashes found by Random; however, SAC found many new crashes that Random did not
find.

RQ6: The SAC algorithm implemented in ARES generates the highest number of crashes,
102, in line with the results on coverage (RQ5), where SAC was also the best performing
algorithm.

We have manually inspected the coverage progress of the different algorithms on some of the
real apps considered in Study 2. We observed that deep RL algorithms achieve higher coverage than
the other algorithms when it is necessary to replicate complex behaviors in order to: (1) overcome
blocking activities, e.g., to create an item in order to be able to later access its properties, or to
bypass authentication; (2) to pass through concatenated activities without being distracted by
already seen activities or ineffective buttons (high dimensional action/observation space); (3) reach
an activity located deeply in the app. Such behaviors are possible thanks to the learning capabilities

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:17

App
Silent-ping-sms
Drawable-notepad
SmsMatrix
Busybox
WiFiKeyShare
Talalarmo
AquaDroid
Lexica
Loyalty-card-locker
Dns66
Gpstest
Memento
Editor
AndOTP
BookyMcBookface
Tuner
WifiAnalyzer
AdAway
Gpslogger
Connectbot
Neurolab
Anuto
PassAndroid
Markor
Vanilla

Afwall
OpenTracks
Opentasks
UserLAnd
Simple-Solitaire
Authorizer
YalpStore
Camera-Roll
AntennaPod
Phonograph

MicroMathematics
Lightning-Browser
Firefox-focus
RedReader
Wikipedia
Slide

Rand Q
0.36
0.36
0.14
0.13
0.13
0.13
0.56
0.54
0.29
0.29
0.60
0.62
0.526
0.529
0.61
0.63
0.23
0.23
0.51
0.51
0.40
0.40
0.65
0.64
0.37
0.42
0.18
0.16
0.20
0.22
0.66
0.68
0.56
0.56
0.25
0.25
0.28
0.28
0.19
0.19
0.23
0.23
0.40
0.35
0.017
0.018
0.40
0.40
0.23
0.17

0.09
0.37
0.18
0.49
0.06
0.05
0.28
0.26
0.33
0.085

0.17
0.28
0.27
0.31
0.30
0.13

0.09
0.35
0.46
0.49
0.21
0.046
0.28
0.37
0.33
0.077

0.17
0.28
0.28
0.31
0.30
0.13

SAC
0.38
0.20
0.11
0.68
0.33
0.64
0.531
0.66
0.34
0.47
0.39
0.64
0.37
0.23
0.20
0.60
0.67
0.27
0.23
0.22
0.22
0.43
0.017
0.43
0.26

0.13
0.35
0.46
0.49
0.19
0.047
0.31
0.25
0.35
0.075

0.30
0.36
0.43
0.31
0.33
0.12

DDPG Effect Size
0.37
0.18
0.11
0.68
0.30
0.64
0.522
0.65
0.21
0.45
0.36
0.65
0.37
0.15
0.20
0.60
0.58
0.25
0.20
0.09
0.22
0.33
0.017
0.25
0.23

S(DDPG),M(Rand),L(Q)
L(Rand,Q)
-
L(Q,Rand)
L(DDPG,Q,Rand)
L(Q)
L(Rand, Q, DDPG)
L(Q,Rand)
L(DDPG,Q,Rand)
L(DDPG,SAC)
L(DDPG)
-
L(DDPG,Q,SAC)
M(Rand), L(DDPG)
M(DDPG), L(SAC)
L(DDPG,SAC)
L(DDPG,Q,Rand)
-
L(DDPG,SAC)
L(DDPG,Q,Rand)
-
L(DDPG,Q,Rand)
-
L(DDPG,Q,Rand)
L(Rand)

0.10
0.35
0.29
0.47
0.18
0.049
0.26
0.25
0.22
0.076

0.18
0.29
0.35
0.33
0.28
0.14

L(DDPG,Q,Rand)
-
L(DDPG,Rand)
-
L(Rand)
S(SAC), M(Q)
L(DDPG,Q,Rand)
L(Rand,DDPG,SAC)
L(DDPG)
S(Q,DDPG,SAC)

L(DDPG,Q,Rand)
L(DDPG,Q,Rand)
L(Rand, Q)
-
-
-

Table 4. AUCs achieved on real apps; effect size between winner and others when 𝑝-value < 𝛼

of the DNNs used by deep RL algorithms, while they are hardly achieved by the other existing
approaches, including tabular Q-Learning.

RQ7: In presence of blocking activities or complex concatenated activities (activities with a
high number of widgets or located in depth) that require the capability to reuse knowledge
acquired in previous explorations, the learning capabilities of deep RL algorithms make
them the most effective and efficient exploration strategies.

6.2.1 Comparison between ARES and state-of-the-art tools. RQ8-a. Table 5 shows the coverage
reached and the faults exposed by each testing tool on 68 Android apps from AndroTest. Coverage
data are summarized by means of boxplots in Figure 6. The highest average coverage and average
number of crashes over 10 runs are highlighted with a gray background. ARES achieved 54.2%
coverage and detected 0.48 crashes on average. TimeMachine achieved 50.4% code coverage and
0.42 faults on average. Sapienz reached a mean code coverage of 48.8%, and discovered 0.22 faults

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:18

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

Fig. 5. Comparison of total number of unique crashes on the 41 apps involved in RQ5-6-: dark gray areas
indicate the proportion of crashes found by both techniques

on average. Monkey achieved 43.9% code coverage and discovered 0.11 faults. ARES achieved the
highest code coverage on average, followed by TimeMachine, Sapienz and Monkey. TimeMachine
detected most unique crashes (179), followed by ARES (171), Sapienz (103) and Monkey (51). Actually,
ARES discovered less crashes than TimeMachine mostly because TimeMachine uses a system-level
event generator, taken from Stoat [39], which ARES does not support. However, system events
present two major drawbacks: a) they vastly change depending on the Android version [40] [19]
(despite TimeMachine is compatible with Android 7.1, it uses only system-level actions compatible
with Android 4.4 [13]); and b) in order to execute them, root privileges are required. ARES does not
require root privileges in order to work properly on any app (i.e., we recall that protected apps
do not execute on rooted devices [42]). Analyzing the execution traces of each tool, we searched
and identified the faults immediately after the generation of system events not related to the AUT.
More than a third (63) of the crashes generated by TimeMachine comes from system-level actions.
Figure 7 shows a pairwise comparison of detected crashes among evaluated techniques. Only 20%
of unique crashes found by ARES are also found by TimeMachine. This shows that ARES can be
used together with other state-of-the-art Android testing techniques (in particular, TimeMachine)
to jointly cover more code and discover more crashes.

The good results of ARES in code coverage and exposed faults are due to the reinforcement
mechanisms of the RL algorithms and to the reward function that drives the testing tool through
states of the app leading to hard-to-reach states. Search-based techniques such as Sapienz typically
observe the program behavior over an event sequence that may be very long. Hence, the associated
coverage feedback, used to drive search-based exploration, does not have enough fine-grained
details to support good local choices of the actions to be taken. In fact, the fitness function used by
these algorithms evaluates an entire action sequence and does not consider the individual steps in
the sequence. TimeMachine, improved this weakness, by relying on the coverage feedback obtained
at an individual state to determine which portion of the app is not still explored. The drawback of
this kind of approach is a higher computational cost, that requires a higher testing time. ARES offers
a better trade-off between the granularity of the feedback and the computational cost required to
obtain it.

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

0306090120SACRandom316042420306090120SACDDPG217131310306090120SACQ16752727020406080DDPGRandom55341818015304560DDPGQ26351717020406080RandomQ24541919Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:19

RQ8-a: ARES achieved the highest code coverage in 41/68 apps, followed by TimeMachine
(12/68), Sapienz (6/68) and Monkey (2/68). ARES triggered crashes more often than other
tools in 23/68 apps, followed by TimeMachine (15/68), Sapienz (5/68) and Monkey (0/68).
However, TimeMachine generated the highest number of unique crashes (179), but 63 of
them come from system-level events. ARES generates 171 faults, Sapienz 103, and Monkey
51.

Fig. 6. Code coverage achieved by ARES, TimeMachine, Sapienz, and Monkey

Fig. 7. Comparison of total number of unique crashes involved in RQ8-a: dark gray areas indicate the
proportion of crashes found by both testing tools

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

020406080MonkeySapienzTimeMachineARES++++04590135180ARESTimeMachine14413635350306090120SapienzMonkey3181222204590135180TimeMachineMonkey22148313104590135180TimeMachineSapienz80156232304590135180ARESMonkey20138333304590135180ARESSapienz771452626000:20

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

App

Coverage

Faults

a2dp
aagtl
aarddict
acal
addi
adsdroid
aGrep
aka
alarmclock
aLogCat
Amazed
AnyCut
anymemo
autoanswer
baterrydog
battery
bites
blokish
bomber
Book-Catalogue
CountdownTimer
dalvik-explorer
dialer2
DivideAndConquer
fileexplorer
frozenbubble
gestures
hndroid
hotdeath
importcontacts
jamendo
k9mail
LNM
lockpatterngenerator
LolcatBuilder
manpages
mileage
Mirrored
mnv
multismssender
MunchLife
MyExpenses
myLock
Nectroid
netcounter
PasswordMaker
passwordmanager
Photostream
QuickSettings
RandomMusicPlayer
Ringdroid
sanity
soundboard
SpriteMethodTest
SpriteText
swiftp
SyncMyPix
tippy
tomdroid
Translate
Triangle
weight-chart
whohasmystuff
wikipedia
Wordpress
worldclock
yahtzee
zooborns
Average
Sum

Monkey
38
16
13
18
19
30
45
65
64
67
36
63
31
12
63
73
34
55
76
27
74
66
39
84
41
80
37
7
75
40
53
6
47
75
26
40
38
57
41
34
67
41
25
34
43
53
7
30
50
53
22
26
42
58
60
12
21
75
47
49
-
63
61
31
4
83
51
30
43.9

Sapienz
44
18
15
27
20
36
-
84
41
71
66
65
50
16
67
78
41
52
73
29
62
72
42
83
49
76
52
15
75
39
41
7
-
79
25
73
45
59
60
59
72
60
31
66
70
58
8
34
45
58
29
19
32
80
60
14
21
83
46
48
-
67
68
32
5
88
57
16
48.8

TM ARES Monkey
42
17
17
27
17
36
59
77
60
75
63
63
43
21
62
77
45
68
77
27
77
70
42
82
55
75
51
18
72
40
54
8
-
74
29
70
48
62
43
61
71
50
50
58
58
55
17
35
46
58
48
31
59
73
57
13
23
74
51
48
-
66
66
33
7
86
56
35
50.4

42
19
17
28
20
35
56
84
71
87
89
73
53
15
69
92
43
45
84
25
84
72
44
80
64
70
55
18
74
42
63
8
75
78
26
74
45
59
56
73
88
63
30
57
69
59
18
29
52
63
30
22
61
88
60
17
25
85
69
50
-
71
81
35
8
90
69
37
54.2

0
0.3
0
0.5
0.4
0.1
0.1
0.1
0.5
0
0.1
0
0.2
0
0
0
0.1
0
0
0.1
0
0.3
0
0
0
0
0
0.1
0.1
0.1
0
0.4
0
0
0
0
0.3
0.4
0.5
0
0
0
0
0
0
0.3
0
0.1
0
0
0
0.2
0
0
0
0
0
0
0
0
-
0
0.1
0
0
0
2
0
0.11
51

Sapienz
0.4
1.0
0
0.8
0.3
0.4
-
0.7
0.5
0
0.3
0
0.8
0
0.1
0.4
0.2
0.2
0
0.2
0
0.2
0
0.2
0
0
0
0.4
0.2
0
0.4
0
-
0
0
0.4
1.0
0
0.3
0.2
0
0
0
1.0
0
0.8
0
0
0.2
0
0.1
0.3
0.6
0
0.4
0.4
0
0.4
0.3
0
-
0
0
0
0.5
0
0.2
0
0.22
103

TM ARES
0.1
1.7
0.2
1.0
0.4
0.5
0.3
0.1
0.6
0
0.2
0
0.3
0.5
0.4
0.5
0.9
0
0
0.8
0
0.7
0.3
1.0
0
0
0
1.1
0.8
0.6
1.4
1.8
-
0
0.1
0.3
2.3
0.8
1.0
0.3
0
0.2
0.5
0.3
0.4
0.6
0
0.4
0
0.6
0.3
0.5
0
0
0
-
0
0.3
0
0
-
0
0.9
0
1.5
0.6
0.5
0.1
0.42
179

0.3
1.5
0.2
1.0
0.6
0.7
0.2
0.4
0.8
0
0.2
0
1.0
0.5
0.5
0.4
0.5
0.3
0
0.5
0
0.8
0.4
0.9
0
0
0
1.3
0.9
0.8
1.6
1.2
0.2
0
0
0.4
1.8
0.7
1.1
0.4
0
0.2
0.2
0.9
0.5
0.9
0
0.8
0.4
0.8
0.2
0.4
0.4
0
0.5
0.6
0
0.4
0.3
0
-
0
1.0
0
1.0
0.4
0.5
0.5
0.48
171

Table 5. Results on 68 open-source apps coming from AndroTest

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:21

Coverage
ARES Q-Testing

Faults
ARES Q-Testing

App

Alogcat
Antennapod
AnyCut
batterydog
Jamendo
Multismssender
Myexpanses
talalarmo
Tomdroid
vanilla
Average
Sum

84
48
72
65
64
71
63
74
61
41
64.3

76
42
67
49
46
45
36
74
50
40
52.5

0
0.8
0
0.5
0.4
0.4
0.4
0.8
0.3
0.5
0.41
17

0
0.4
0
0.3
0.7
0
0.2
0.5
0.2
0.4
0.27
16

Table 6. Comparison between Q-Testing and ARES

RQ8-b. Table 6 shows the coverage reached and the faults exposed by ARES and Q-Testing on
10 Android apps instrumented with JaCoCo. ARES achieved 64.3% coverage and exposed 0.41
faults on average; it detected 17 unique crashes. Q-Testing achieved 52.5% code coverage and 0.27
crashes on average, and it detected 16 unique crashes. ARES achieved the highest code coverage on
almost all apps, and on average ARES covered 12% more code than Q-Testing. Q-testing generated
6 faults in common with ARES, while other 4 faults are generated using the system-level events
of Stoat. In this comparison, the main advantage of ARES seems to be a better reward function,
that encourages the tool to visit the greatest number of activities within the same episode. Instead,
Q-Testing determines the reward of the Q-Learning algorithm by computing the similarity between
Android app states, which does not guarantee an efficient way to overcome blocking activities.

Fig. 8. Code coverage achieved by ARES and Q-Testing.

RQ8-b: ARES reached the highest code coverage on 9/10 apps, the highest average of crashes
in 7/10 apps, and the highest number of unique crashes. Q-Testing, the state-of-the art RL
Android testing tool, reached the same level of exploration of ARES only in 1/10 app, and
the highest average number of crashes on only 1/10 apps.

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

l4050607080Q−TestingARES++000:22

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

7 RELATED WORK

7.1 Random Testing
Testing tools in this category generate random events on the AUT GUI. Monkey [20] is one of
the most popular black-box Android testing tools. It triggers events by interacting randomly with
screen coordinates. This simple random approach worked relatively well on some benchmark
applications [11]. Nonetheless, Monkey tests involve a significant number of ineffective or repeated
events, as there is no guidance to make the exploration efficient. ARES implements a smarter
version of Monkey, which we used as a baseline (Random). Such improved version selects only
actions that are actually possible in a given GUI state, thus making the exploration strategy a bit
more efficient.

7.2 Model-based Testing
Model-based tools [4] [5] first build navigation models of the Android app by means of static or
dynamic analysis, used to explore efficiently the application states, and then they extract test cases
from such models, to eventually expose bugs. Stoat [39] uses a stochastic FSM to model the app
behavior. The app model is built using dynamic analysis, enhanced with a weighted UI exploration
strategy and with the help of static analysis. Compared to Stoat, ARES can be viewed as computing
a navigation model implicitly: the MDP model used by the deep RL algorithms. One key advantage
of using an implicit model is that we do not have to deal with the combinatorial explosion of its
size, which is controlled by Stoat by means of model compaction heuristics.

7.3 Structural Testing
Structural strategies [6, 14, 16, 31, 32] generate coverage oriented inputs using symbolic execution
or evolutionary algorithms. Sapienz [32] maximizes code coverage and bug revelation using a
Pareto-optimal multi-objective search based approach, which applies genetic operators such as
mutation and crossover to produce new test cases. It can generate specific input for text fields by
reverse engineering the APK. This process occasionally results in invalid sequences, which are
discarded by the fitness functions that reward test cases with high coverage. TimeMachine [14]
improves Sapienz by identifying interesting states in the past and restarting the search process
from them when the search stagnates. While coverage oriented approaches require the possibility
to instrument the app to measure coverage and to possibly execute it symbolically, our approach is
black-box and does not suffer from the scalability issues affecting, e.g., symbolic execution.

7.4 Machine Learning Based Testing
Some ML based testing approaches [7] [25] [28] use an explicit, supervised training process to learn
from previous test executions. They can reuse previous knowledge, acquired on different apps or
on past versions of the app under test. Approaches as QBE [25] make the transfer of knowledge to
new apps possible by abstracting the app state in a form that is supposed to hold across different
domains and implementations. However, the effectiveness of such transfer learning process depends
on the similarity between new and old apps.

One of the first works proposing RL for GUI testing is AutoBlackTest [34]. This approach is
based on the simplest form of RL, tabular Q-Learning, whose effectiveness is strongly dependent
on the initial values in the Q-Table. On the contrary, ARES learns the action-value function from
scratch during the exploration of the AUT. One of the most recent approaches to Android testing
based on deep learning is Q-Testing [36]. However, it also uses tabular Q-Learning as backbone,
while learning is limited to the computation of the similarity between Android app states, which

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:23

determines the reward of the Q-Learning algorithm. ARES instead learns both the state similarity
and the action-value function during its interactions with the AUT.

8 CONCLUSION AND FUTURE WORK
We have proposed an approach based on Deep RL for the automated exploration of Android
apps. The best exploration strategy is learned automatically as the test progresses. The approach
is implemented in the open source tool ARES, which is complemented by FATE, a model based
Android testing tool that we developed to support fast execution and configuration of the alternative
deep RL algorithms of ARES. The resulting configuration of ARES, in particular when running the
SAC deep RL algorithm, outperformed all the considered baselines in terms of coverage achieved
over time and exposed bugs.

In our future work, we plan to investigate specific fault categories that are particularly relevant
for Android apps, such as security vulnerabilities. In fact, we think that the adaptation and reward
mechanisms used by deep RL algorithms to learn the optimal exploration strategy could be particu-
larly effective when the fault to be exposed is a security fault. We also plan to port ARES and FATE
to iOS.

REFERENCES
[1] 2006. Emma. Retrieved December 30, 2020 from http://emma.sourceforge.net
[2] 2020. Appbrain. Retrieved November 20, 2020 from https://www.appbrain.com
[3] 2020. Appium. Retrieved September 25, 2020 from http://appium.io
[4] Domenico Amalfitano, Anna Rita Fasolino, Porfirio Tramontana, Salvatore De Carmine, and Atif M Memon. 2012.
Using GUI ripping for automated testing of Android applications. In 2012 Proceedings of the 27th IEEE/ACM International
Conference on Automated Software Engineering. IEEE, 258–261.

[5] Domenico Amalfitano, Anna Rita Fasolino, Porfirio Tramontana, Bryan Dzung Ta, and Atif M Memon. 2014. MobiGUI-

TAR: Automated model-based testing of mobile apps. IEEE software 32, 5 (2014), 53–59.

[6] Saswat Anand, Mayur Naik, Mary Jean Harrold, and Hongseok Yang. 2012. Automated concolic testing of smartphone
apps. In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering.
1–11.

[7] Nataniel P Borges, Maria Gómez, and Andreas Zeller. 2018. Guiding app testing with mined interaction models. In
2018 IEEE/ACM 5th International Conference on Mobile Software Engineering and Systems (MOBILESoft). IEEE, 133–143.
[8] Justin A Boyan and Andrew W Moore. 1995. Generalization in reinforcement learning: Safely approximating the value

function. In Advances in neural information processing systems. 369–376.

[9] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.

2016. Openai gym. arXiv preprint arXiv:1606.01540 (2016).

[10] Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Automated test input generation for android:
Are we there yet?(e). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE,
429–440.

[11] Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Automated test input generation for android:
Are we there yet?(e). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE,
429–440.

[12] Jack Clark and Dario Amodei. 2016. Faulty Reward Functions in the Wild.

Retrieved January 4, 2021 from

https://openai.com/blog/faulty-reward-functions/

[13] Zhen Dong, Marcel Bohme, Lucia Cojocaru, and Abhik Roychoudhury. 2020. Github Repository: TimeMachine.
Retrieved November 20, 2020 from https://github.com/DroidTest/TimeMachine/blob/master/fuzzingandroid/sys_
event_generator/sys_event.py

[14] Zhen Dong, Marcel Bohme, Lucia Cojocaru, and Abhik Roychoudhury. 2020. Time-travel Testing of Android Apps. In

Proceedings of the 42nd International Conference on Software Engineering (ICSE). 481–492.

[15] Scott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing function approximation error in actor-critic

methods. arXiv preprint arXiv:1802.09477 (2018).

[16] Xiang Gao, Shin Hwei Tan, Zhen Dong, and Abhik Roychoudhury. 2018. Android testing via synthetic symbolic
execution. In 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 419–429.
Retrieved October 20, 2020 from https://developer.android.com/guide/components/

[17] Google. 2019. Broadcasts.

broadcasts

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:24

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

[18] Google. 2020. Android Emulator. Retrieved October 20, 2020 from https://developer.android.com/studio/run/emulator/
Retrieved December 28, 2020 from https://cs.android.com/android/
[19] Google. 2020. System-Level events API 25.

platform/superproject/+/android-7.1.2_r36:frameworks/base/core/res/AndroidManifest.xml

[20] Google. 2020. UI/Application Exerciser Monkey. Retrieved November 10, 2020 from https://developer.android.com/

studio/test/monkey

[21] Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, and Zhendong
Su. 2019. Practical GUI testing of Android applications via model abstraction and refinement. In 2019 IEEE/ACM 41st
International Conference on Software Engineering (ICSE). IEEE, 269–280.

[22] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Off-policy maximum entropy

deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290 (2018).

[23] Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal,
Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and
Yuhuai Wu. 2018. Stable Baselines. https://github.com/hill-a/stable-baselines.

[24] Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics (1979),

65–70.

[25] Yavuz Koroglu, Alper Sen, Ozlem Muslu, Yunus Mete, Ceyda Ulker, Tolga Tanriverdi, and Yunus Donmez. 2018. QBE:
QLearning-based exploration of android applications. In 2018 IEEE 11th International Conference on Software Testing,
Verification and Validation (ICST). IEEE, 105–115.

[26] Edward A. Lee. 2009. Finite State Machines and Modal Models in Ptolemy II. Technical Report UCB/EECS-2009-151.
EECS Department, University of California, Berkeley. http://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-
2009-151.html

[27] Yuxi Li. 2017. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274 (2017).
[28] Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2019. Humanoid: a deep learning-based approach to automated
black-box Android app testing. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE, 1070–1073.

[29] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan
Wierstra. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).
[30] Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An input generation system for android apps.

In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering. 224–234.

[31] Riyadh Mahmood, Nariman Mirzaei, and Sam Malek. 2014. Evodroid: Segmented evolutionary testing of android apps.

In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. 599–609.
[32] Ke Mao, Mark Harman, and Yue Jia. 2016. Sapienz: Multi-objective automated testing for Android applications. In

Proceedings of the 25th International Symposium on Software Testing and Analysis. 94–105.

[33] Evgeny Mandrikov Marc R. Hoffmann, Brock Janiczak. 2020. Jacoco Code Coverage. Retrieved November 15, 2020

from https://www.eclemma.org/jacoco/

[34] Leonardo Mariani, Mauro Pezze, Oliviero Riganelli, and Mauro Santoro. 2012. Autoblacktest: Automatic black-box
testing of interactive applications. In 2012 IEEE Fifth International Conference on Software Testing, Verification and
Validation. IEEE, 81–90.

[35] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin

Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).

[36] Minxue Pan, An Huang, Guoxin Wang, Tian Zhang, and Xuandong Li. 2020. Reinforcement learning based curiosity-
driven testing of Android applications. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software
Testing and Analysis. 153–164.

[37] Martin Riedmiller. 2005. Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning

method. In European Conference on Machine Learning. Springer, 317–328.
[38] et al. Silver, David. 2014. Deterministic Policy Gradient Algorithms. (2014).
[39] Ting Su, Guozhu Meng, Yuting Chen, Ke Wu, Weiming Yang, Yao Yao, Geguang Pu, Yang Liu, and Zhendong Su.
[n.d.]. Guided, stochastic model-based GUI testing of Android apps. In Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering, ESEC/FSE 2017, Paderborn, Germany, September 4-8, 2017. 245–256.

[40] Ting Su, Guozhu Meng, Yuting Chen, Ke Wu, Weiming Yang, Yao Yao, Geguang Pu, Yang Liu, and Zhendong Su. 2017.
System-Level events API 19. Retrieved December 28, 2020 from https://sites.google.com/site/stoat2017/evaluation/stoat-
s-system-level-events

[41] Sutton. 2014. Reinforcement Learning: An Introduction.
[42] Vincent F Taylor and Ivan Martinovic. 2017. Short paper: A longitudinal study of financial apps in the Google Play

store. In International Conference on Financial Cryptography and Data Security. Springer, 302–309.
[43] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine learning 8, 3-4 (1992), 279–292.

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

Deep Reinforcement Learning for Black-Box Testing of Android Apps

000:25

9 APPENDIX

9.1 Study 1- Deep RL
Certain parameters of the algorithms are omitted for simplicity, and can be found in the documen-
tation of Stable Baselines [23].

Control Policy:
Deep RL algorithms rely on a policy based on a MLP composed of 2 layers and 64 neurons each.

Learning rates:

• DDPG: 0.0001
• SAC: 0.0003
• TD3: 0.0003

DDPG
Configuration
random_exploration 0.5
nb_train_steps

1

5

2
0.5
25

3
0.6
5

4
0.6
25

5
0.7
5

6
0.7
25

7
0.8
5

8
0.8
25

TD3
Configuration
random_exploration 0.5
train_frequency
25

1

2
0.5
100

3
0.6
25

4
0.6
100

5
0.7
25

6
0.7
100

7
0.8
25

8
0.8
100

SAC
Configuration
target_update_interval
train_frequency

9.2 Study 1- Tabular RL

1 2 3 4 5 6 7
10
2
1
1
5
1

5
5

2
1

1
5

5
1

8
10
5

Q-Learning
Configuration 1
epsilon
gamma
alpha

0.5
0.99
0.628

2
0.6
0.99
0.628

3
0.7
0.99
0.628

4
0.8
0.99
0.628

5
0.5
0.9
0.628

6
0.6
0.9
0.628

7
0.7
0.9
0.628

8
0.8
0.9
0.628

9.3 Study 2
Some parameters used in Study 2 have been selected from the results of Study 1 (see Section 6).

DDPG:
• Control Policy: MLP, 2 layers, 64 neurons
• Learning rate: 0.0001
• nb_train_steps: 10
• random_exploration: 0.7
SAC:

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

000:26

Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella

• Control Policy: MLP, 2 layers, 64 neurons
• Learning rate: 0.0003
• train_freq: 5
• target_update_interval: 10
TD3:
• Control Policy: MLP, 2 layers, 64 neurons
• Learning rate: 0.0003
• train_freq: 10
• random_exploration: 0.8
Q-Learning:
• epsilon: 0.8
• gamma: 0.9
• alpha: 0.628

ACM Trans. Softw. Eng. Methodol., Vol. 01, No. 1, Article 000. Publication date: January 2021.

