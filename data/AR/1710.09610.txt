0
2
0
2

v
o
N
8
1

]
T
S
.
h
t
a
m

[

4
v
0
1
6
9
0
.
0
1
7
1
:
v
i
X
r
a

Controlled Mean-Reverting Estimation for The AR(1) Model
with Stationary Gaussian Noise

Chunhao Cai
School of Mathematics, Shanghai University of Finance and Economics, Shanghai, China.
Email: caichunhao@mail.shufe.edu.cn

November 19, 2020

Abstract

This paper deals with the maximum likelihood estimator for the mean-reverting parameter
of a ﬁrst order autoregressive models with exogenous variables, which are stationary Gaussian
noises (Colored noise). Using the method of the Laplace transform, both the asymptotic prop-
erties and the asymptotic design problem of the maximum likelihood estimator are investigated.
The numerical simulation results conﬁrm the theoretical analysis and show that the proposed
maximum likelihood estimator performs well in ﬁnite sample.

Key words: Laplace Transform; fractional Gaussian noise; Optimal input

1

Introduction

Temporal dependence in volatility has been one of the most studied problems in ﬁnancial econo-
metrics. For example, Gatheral et a.l. [13] has presented that the volatility is rough with some real
data and statistical tools. Comte and Renault [12] also takes the following expression:

dyt = σ∗eht/2dWt
dht = γhtdt + v∗dBH
t

where yt is the log price of an asset at time t, Wt is a standard Brownian motion and BH
t
fractional Brownian motion whose covariance function is

is a

E(BH

t BH

s ) =

1
2

(t2H + s2H

t

s

2H ), s, t
|

−

∈

− |

[0, T ].

If we apply the Euler approximation with discrete time t = ∆, 2∆,
ht, then the volatility ht has the discrete-time model

· · ·

, N ∆(= T ) to the volatility

hi∆ = βh(i−1)∆ + vηH
i∆.

BH
where β = 1+γ∆, ηH
γ is a negative constant such that

i∆ = BH

i∆−

(i−1)∆ is a fractional Gaussian noise with distance ∆. If v = ∆ = 1,
< 1 then it is a AR(1) process with fractional Gaussian noise.

β

|

|

1

 
 
 
 
 
 
Regarding that the fractional Gaussian noise is stationary Gaussian noise, we can extend this model
and rewrite it as

and ξ = (ξn, n

∈

Xn = ϑXn−1 + ξn,
Z) is the centred regular stationary Gaussian noise with

< 1

ϑ
|

|

π

−π |
Z

log fξ(λ)
|

dλ <

,

∞

(1)

(2)

where fξ(λ) is the spectral density of ξ. Brouste et a.l. found the MLE for the unknown parameter
ϑ and proved its consistency.

In the sense of continuous observation, Brouste and Cai [10] considered the controlled drift

estimation problem in the fractional O-U process:

dXt =

ϑXtdt + u(t)dt + dBH

t , ϑ > 0, t

[0, T ]

(3)

∈
is a fractional Brownian motion with Hurst parameter H

where BH
(0, 1) and u(t) is a determin-
t
istic function in a control space. They have found a function in the control space which maximize
the Fisher Information and also obtained the consistency of the MLE of ϑ.

−

∈

Still applying the Euler approximation in the model (3) and extend it to the general stationary

Gaussian noise, in this paper we will consider a similar model

Xn = ϑXn−1 + u(n) + ξn, 0 <

< 1, X0 = 0 ,

ϑ
|

|

(4)

where u(n) denotes a deterministic function of n and ξn is the same noise deﬁned in (1). When
considering the problem of estimating the unknown parameter ϑ with the observation data X (N ) =
, N ), we denote L(ϑ, X (N )) the likelihood function for ϑ, then the Fisher information
(Xn, n = 1,
can be written as

· · ·

IN (ϑ, u) =

−

Eϑ

∂2
∂ϑ2 ln L(ϑ, X (N )).

Let

UN be some control space deﬁned in (16), therefor we want to ﬁnd the function such that

JN (ϑ) = sup

u∈UN IN (ϑ, u) .

and then ﬁnd an adapted estimator ¯ϑN of the parameter ϑ, which is asymptotically eﬃcient in the
sense that, for any compact K
0 < ϑ < 1

,

ϑ
|

∈
sup

(0, 1) =
ϑ∈K JN (ϑ)Eϑ( ¯ϑN −

{

}

ϑ)2 = 1 + o(1),

(5)

.
→ ∞

as N
Remark 1. This model is diﬀerent from the ARX model of Ljung ([7], Page 73). We present here
a direct generalization of [1] and [10]. We can see that the control space
UN of (16) is so nearby of
that in [10].
Remark 2. In this paper, we suppose that the covariance structure of the noise ξ is known. In fact,
if this covariance depends only on one parameter, for example, the Hurst parameter H of fractional
Gaussian noise presented in Section 4 we can estimate this parameter with the log-peridogram
method [9] or with generalized quadratic variation [4] and study the plug-in estimator. This will
be our future study.

2

Remark 3. In this paper, we will not estimate the function u(n) but just ﬁnd one function which
maximize the Fisher Information.

The organization of this paper is as follows.

In Section 2, we give some basic result of the
Regular stationary noise ξn, ﬁnd the likelihood function and the formula of the Fisher Information.
Section 3 provides the main results of this paper and Section 4 shows some simulation examples to
show the performance of the proposed MLE. All the proofs are collected in the Appendix.

2 Preliminaries and Notations

2.1 Stationary Gaussian sequences

Let us suppose that the covariance of the real valued centered stationary Gaussian sequence ξ =
(ξn)n≥1

Eξmξn = c(m, n) = ρ(
(6)
|
is positive deﬁned, then there exist an associated innovation sequence (σnεn)n≥1 where εn ∼
N

1 are independent, deﬁned by the following relations:

), ρ(0) = 1 ,

(0, 1), n

m

−

≥

n

|

E(ξn|
It follows from the theorem of Normal Correlation (Theorem 13.1, [6]) that there exists a deter-
ministic kernel k = (k(n, m), n

σnεn = ξ1, σnεn = ξn −

n) such that k(n, n) = 1 and

ξ1, ξ2, . . . , ξn−1), n

1, m

2.

≥

≥

≤

n

σnεn =

k(n, m)ξm.

m=1
X

For n

≥

1, we will denote by βn−1 the partial correlation coeﬃcient

βn−1 =

k(n, 1).

−

(7)

(8)

The following relationship between k(
·
partial correlation coeﬃcients (β)n≥1 and the variances of innovation (σ2
algorithm [14])

), the covariance function ρ(
·

,

·

) deﬁned in (6), the sequence of
n)n≥1 (see Levinson-Durbin

n−1

σ2
n =

(1

m=1
Y
n

β2
m), n

−

≥

2, σ1 = 1,

k(n, m)ρ(m) = βnσ2
n,

m=1
X
k(n + 1, n + 1

m) = k(n, n

m)

βnk(n, m)

(9)

(10)

(11)

−
,
Since we assume the positive deﬁniteness of the covariance c(
·
ministic kernel K = (K(n, m), n

n) such that

1, m

−

−

≥

≤

n

ξn =

K(n, m)σmεm.

m=1
X

The relationship of the kernel k and K can be found in [1].

3

), there also esists an inverse deter-

·

(12)

Remark 4. It is worth mentioning that the condition (2) implies that

β2
n <

.
∞

Xn≥1

This condition ies theoretically veriﬁed for classical ARMA noises. There has been no explicit form
of the partial autocorrelation coeﬃcients for the fractional Gaussian noise but we can numerically
verify this condition. For the very similar fractional ARIMA processes, it has been proved that
βn = O(1) in [3].

2.2 Model Transformation

Let us deﬁne the process Z = (Zn, n

1) such that

≥

n

where k(n, m) is the kernel deﬁned in (7). Similar to (12), we have

Zn =

k(n, m)Xm, n

m=1
X

1 ,

≥

(13)

n

Xn =

K(n, m)Zm .

m=1
X

It is worth mentioning that the process Z has the same ﬁltration of X. In the following parts, let
, ZN ). Actually, it was shown in [1] the process Z can be considered

the observation be (Z1, Z2,
as the ﬁrst component of a 2-dimensional AR(1) process ζ = ζn, n

· · ·

1, which is deﬁned by:

≥

Zn

ζn =



n−1

.

βrZr 

It is not hard to obtain that ζn is a 2-dimensional Markov process which satisﬁes the following

r=1
P





equation:

with

ζn = An−1ζn−1 + bv(n) + bσnǫn, n

An =

(cid:18)

ϑ
βn

ϑβn
1

, b =

(cid:19)

1, ζ0 = 02×1 ,

1
0

(cid:19)

≥

(cid:18)

(14)

(15)

(0, 1) are independent. Following from the idea of [10] we will deﬁne the control space

and ǫn ∼ N
VN of the function v(n):

From the control space of

VN =

v

(

N

1
N

n=1 (cid:13)
X
(cid:13)
(cid:13)
VN we can deﬁne that for the function u(n):
(cid:13)

(cid:12)
(cid:12)

2

v(n)
σn+1 (cid:13)
(cid:13)
(cid:13)
(cid:13)

1

.

)

≤

1
N

N

n=1
X

u(n)

(cid:12)
(cid:12)

UN = 



n

m=1
P

4

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k(n, m)u(m)

σn+1

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

,

1



(16)

2.3 Fisher Information

As we have interpreted, the observation will be the ﬁrst component of the process ζ = (ζn, n
1).
Now from the equation (14), it is easy to write the Likelihood function L(ϑ, X (N )), which depends
on the function v(n):

≥

L(ϑ, X (N )) =

N

n=1
Y

1
2πσ2
n

exp

1
2

N

b∗

n−1ζn−1

ζn −
(cid:0)

Aϑ
σn



−

n=1  
X
IN (ϑ, v) can be written as



p

2

.





!

(cid:1)

Consequently, the Fisher Information

IN (ϑ, v) =

−

Eϑ

∂2
∂ϑ2 ln L(ϑ, X (N )) = Eϑ

N −1

n=1 (cid:18)
X

2

,

a∗
nζn
σn+1 (cid:19)

where an =

1
.
βn (cid:19)

(cid:18)

3 Main Results

(17)

(18)

In this part, we will present our main Results in this paper. First of all, from the presentation of
the Fisher Information (18) we have

UT is uopt(n) =

n

K(n, m)σm+1

m=1
P

K(n, m)σm+1 for

n

m=1
P

Theorem 3.1. The asymptotical optimal input in the class of control

for 0 < ϑ < 1 and uopt(n) = (

−

1 < ϑ < 0. Moreover,

−

n

1)n

m=1
P

K(n, m)σm+1 or uopt(n) = (

−

1)n+1

lim
N→∞

JN (ϑ)
N

=

(ϑ) ,

I
1−ϑ2 + 1

1+ϑ

2.

(ϑ) = 1

where

(ϑ) = 1

1−ϑ2 + 1

(1−ϑ)2 for 0 < ϑ < 1 and

I

I
Remark 5. The theorem 3.1 can be generalized to the AR(p) case with the norm of the matrix of
Fisher Information, but this purpose is not so clear as well as as AR(1) that is: when the Fisher
Information is larger, the error will be smaller. For this reason, we will illustrate only the result of
ﬁrst order but not order p.

From Theorem 3.1, since the optimal input does not depend on the unknown parameter ϑ, we
can consider ¯ϑ as the MLE ˆϑN . The following theorem states that ˆϑN will reach the eﬃciency of
(5).

Theorem 3.2. With the optimal input uopt(n) deﬁned in Theorem 3.1, for 0 <
ˆϑN has the following properties:

ϑ
|

|

< 1, the MLE

• ˆϑN is strong consistency, that is ˆϑN
• ˆϑN is uniformly consistent on compact K

a.s.
→

ϑ as N

.
→ ∞

R, i.e. for any ν > 0

⊂

lim
N→∞

sup
ϑ∈K

PN
ϑ

ˆϑN −
n(cid:12)
(cid:12)
(cid:12)

5

ϑ

> ν

= 0

o

(cid:12)
(cid:12)
(cid:12)

• ˆϑN is uniformly on compacts asymptotically normal, i.e., as N

lim
N→∞

sup
ϑ∈K

Eϑf

√N

ϑ

ˆϑN −
(cid:16)

−

(cid:17)

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:12)
(cid:12)
(cid:12)

Ef (ξ)

= 0,

where ξ is a zero mean Gaussian random variable with variance
3.1. Moreover, we have the uniform on ϑ

,
→ ∞
f
∀

∈ Cb ,

lim
N→∞

sup
ϑ∈K

4 Simulation study

Eϑ

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

−1(ϑ) deﬁned in Theorem

∈
√N

I
K convergence of the moments: for any q > 0,
ˆϑN −
(cid:16)

−

E

ϑ

ξ

|

|

.

q

q

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

In this section, Monte-Carlo simulations are done for the asymptotical normality of the MLE ˆϑN
with diﬀerent Gaussian noise such as AR(1), MA(1) and fractional Gaussian noise(fGn). However,
βn deﬁned in (8) for the ARMA case is explicit, we can easily obtain the result of MLE, so we just
simulate the fGn with H = 0.65 and diﬀerent ϑ positive and negative.

The covariance function of fGn is

n

ρ(
|

m

|

−

) =

1
2

for a known Hurst exponent H
∈
and the asymptotical normality of the estimation error ˆϑN −

2H

|

2

n

−

m

m

n + 1

(
|
|
(0, 1). For this simulation we use Wood and Chan method in [15]
ϑ follows

m

−

−

−

−

n

1

|

|

|

2H +

2H )

Fig.1. Histogram of the statistic Φ(N, ϑ, X) with ϑ =

0.7.

−

Fig.2. Histogram of the statistic Φ(N, ϑ, X) with ϑ = 0.7.

Remark 6. In this simulation, the function uopt(n) is just a deterministic function. Its contribution
here will be the value of Fisher Information but not others.

6

5 Appendix

The appendix provides the proofs of Theorem 3.1 and Theorem 3.2. Without special note we only
consider of 0 < ϑ < 1, for

1 < ϑ < 0 the proofs will be the same.

−

5.1 Proof of Theorem 3.1

To prove the theorem 3.1, we separate the Fisher Information of (18) into two parts:

IN (ϑ, v) = Eϑ

= Eϑ

= Eϑ

N −1

n=1 (cid:18)
X
N −1

n=1 (cid:18)
X
N −1

a∗
nζn −

nEϑζn + a∗
a∗

nEϑζn

2

σn+1

Eϑζn)

a∗
n(ζn −
σn+1

2

N −1

+

(cid:19)
a∗
nEϑζn
σn+1 (cid:19)

2

(cid:19)

N −1

n=1 (cid:18)
X

n=1 (cid:18)
X
a∗
nEϑζn
σn+1 (cid:19)

2

2

ϑ
n

+

a∗
nP
σn+1 (cid:19)
I2,N (ϑ, v).

n=1 (cid:18)
X
I1,N (ϑ) +
ϑ
n satisﬁes the following equation:

=

where

P

n = Aϑ
ϑ

n−1P

P

ϑ
n−1 + bσnǫn,

ϑ
0 = 02×1.

P

Obviously,

I1(ϑ) does not depends on v(n). Thus, and as presented in [1], we have
(a∗
n )2
ϑ
nP
σ2
n+1 !

lim
N→∞

Eϑ exp

1
2N

= exp

 −

1
2 I1(ϑ)
(cid:19)

N −1

−

(cid:18)

n=1
X

and

I1(ϑ) = 1
A standard calculation yields

1−ϑ2 .

lim
N→∞

I1,N (ϑ)
N

=

I1(ϑ) =

1

1

−

.

ϑ2

(19)

To compute

I2,N (ϑ), let s(n) =

Eϑζn
σn+1

. Then, we can see that s(n) satisﬁes the following equation:

s(n) = An−1s(n

1)

−

σn
σn+1

+ bf (n) ,

(20)

where f (n) = v(n)
σn+1
Note that βn →

and it is bounded.
0 and σn

ε for
1, we assume that for n = 1, 2,
the suﬃciently small positive constant ε and (1 + ε)ϑ < 1. Consequently, we can state the following
result.

(1 + ε) and βn ≤

σn+1 →

σn+1 ≤

· · ·

, σn

Lemma 5.1. Let Y = (Yn, n
equation:

≥

1) be the 2-dimension equation, which satisﬁes the following

Yn =

(cid:18)

ϑ 0
1
0

(cid:19)

Yn−1 + bf (n), Y (0) = y0 .

7

Then, we have

lim
N→∞

1
N

(a∗

ns(n))2 = lim
N→∞

1
N

N

n=1
X

N

(b∗Yn)2 .

n=1
X

Proof. For the sake of notational simplicity, we introduce a 2-dimensional equation Y = (Y ′
which satisﬁes the following equation:

n, n

1),

≥

Y ′
n =

(cid:18)

ϑ 0
1
0

(cid:19)

Yn−1

σn
σn+1

+ bf (n), Y ′

0 = y′
0 .

In this situation, we have three comparison. First, we compare b∗s(n)

calculation implies

b∗Y ′

n. A standard

−

s(n)

−

Y ′
n =

(cid:18)

0
βn−1

ϑβn−1
0

(cid:19)

(s(n

1)

−

−

Y ′
n−1)

σn
σn+1

,

which implies b∗(s(n)

Y ′
0.
n)
→
b∗Y ′
Now, we compare b∗Yn −
n. A simple calculation shows that
b∗(Y ′

Yn) = ϑ(b∗(Y ′

Yn)) + ϑ

−

1

n −

n −

b∗Y ′

n−1 ,

σn
σn+1 −

(cid:18)

(cid:19)
1

0.

σn+1 −

→

which implies b∗(Y ′
Finally, as βn →

n −

0, which achieves the proof.

Yn)

→

0 since f (n) is bounded and σn

0 and the component of s(n) is bounded, we can easily obtain a∗

ns(n)

b∗s(n)

−

→

Now, we deﬁne α(n) = b∗Yn. Then α(n) = ϑα(n

N

1
N

f 2(n)

f (n)

FN =
n=1
(cid:26)
α(0) = 0 without loss of generality.
(cid:12)
P
(cid:12)
v∈VN I2,N (ϑ, v). Then, it is clear that
J2,N = sup

Let

≤

(cid:27)

1

. Since the initial value α(0) will not change our result, we assume

1) + f (n), where f (n) is in the space of

−

lim
N→∞

J2,N (ϑ)
N
1
−

= lim
N→∞

1
N

sup
f ∈FN

α2(n).

N

n=1
X

Now to prove theorem 3.1, we only need the following lemma.

Lemma 5.2. As

J2,N (ϑ) presented in (21), we have
J2,N (ϑ)
N
1
−

lim
N→∞

=

I2(ϑ) ,

where

I2(ϑ) = 1

(1−ϑ)2 .

Proof. First of all, taking f (n) = 1, it is easy to get the lower bound

lim
N→∞

1
N

N

n=1
X

α2(n)

≥

(1

1

−

.

ϑ)2

8

(21)

(22)

Moreover, a simple calculation shows that

α(n) = ϕ(n)

n

i=1
X

ϕ−1(i)f (i), n

1, α(0) = 0 ,

≥

where

Obviously, we can rewrite 1
N

N
n=1 α2(n) as

ϕ(n) = ϑϕ(n

1), ϕ(0) = 1 .

−

1
N

N

n=1
X

α2(n) =

P
N

n

ϕ(n)

ϕ−1(i)

i=1
X

n=1  
X

f (i)
√N !  

n

ϕ(n)

ϕ−1(i)

i=1
X

f (i)
√N !

.

or

where

Let φn = ϕ−1(n)

N

ℓ=n
P

and

1
N

N

n=1
X

α2(n) =

N

N

i=1
X

j=1
X

FN (i, j)

f (i)
√N

f (j)
√N

,

N

FN (i, j) =

ϕ(ℓ)ϕ−1(i)

ϕ(ℓ)ϕ−1(j)

.

Xℓ=i W j

(cid:0)

(cid:1) (cid:0)

(cid:1)

ϕℓǫℓ with ǫℓ ∼ N

(0, 1) are independent. Then, we have

FN (i, j) = E(φiφj)

φn−1 = ϑφn + ǫn−1, φN = 0.

Let us mentioned that FN (i, j) is a compact symmetric operator for ﬁxed N . We should estimate
the spectral gap (the ﬁrst eigenvalue ν1(N )) of the operator. The estimation of the spectral gap is

based on the Laplace transform of

N

i=1
P

φ2
i , which is written as

LN (a) = Eϑ exp

a
2

 −

N

i=1
X

φ2
i

,

!

ν1(N ) , φ is a centred Gaussian
for suﬃciently small negative a < 0. On one hand, when a >
process with covariance operator FN . Using Mercer’s theorem and Parseval’s identity, LT (a) can
be represented by

−

2

LN (a) =

(1 + aνi(N ))−1/2 ,

(23)

Yi≥1

where νi(N ) is the sequence of positive eigenvalues of the covariance operator. A straightforward
algebraic calculation shows

LN (a) =

ϑN −1Ψ1
N

−1/2

,

(24)

(cid:0)

(cid:1)

9

where

For

ΨN =

1

0

1
ϑ
a
ϑ

1
ϑ
a
ϑ + ϑ

N −1

(cid:19)

(cid:18)

1
0

.

(cid:19)

(cid:0)

∆ =

(cid:18)

(cid:1)
1 + a
ϑ
(cid:18)
1, λ1 ≤

2

+ ϑ

(cid:19)

4

−

≥

0 ,

1 of the matrix

1
ϑ
a
ϑ

1
ϑ
a
ϑ + ϑ

.

(cid:19)

(cid:18)

there exists two real eigenvalues λ2 ≥

Then, we can see that

ΨN =

λN −1

−
ϑ2

λN −1

λN −2
2

−

λN −2
1

−
ϑ

ϑ
λ2 −

!

0 .

λ1 ≥

That is to say for ϑ > 0 and for any 0 > a

(1

−

≥ −

ϑ)2, LN (a)

0. Thus,

≥

lim
N→∞

ν1(N )

≤

1
(1−ϑ)2

0 means 1+a

ϑ + ϑ

≤ −

2 and 0 > a >

−

(1 + ϑ)2. As a consequence,

and we complete the proof.

Remark 7. For
we have ν1(N )

−
≤

1 < ϑ < 0, ∆
(1 + ϑ)2.

≥

5.2 Proof of theorem 3.2

Let vopt(n) = σn+1 and ζo = (ζo

n, n

1) be the process ζ with the function vopt(n). Then, we have

≥
ζo
n = An−1ζo

n−1 + bvopt(n) + bσnǫn, ζ0 = 02×1

To estimate the parameter ϑ from the observations ζ1, ζ2,

, ζN , we can write the MLE of ϑ

· · ·

uing (17)

ˆϑN =

A standard calculation yields

N

n=1 (cid:18)
X

2

a∗
nζn
σn+1 (cid:19)

!

−1

N

n=1
X

nζnb∗ζn+1
a∗

σ2

n+1 !

.

(25)

ˆϑN −

ϑ =

MN
M
iN
h

,

where

N

MN =

a∗
nζn
σn+1

ǫn+1,

M
h

iN =

N

2

a∗
nζn
σn+1 (cid:19)

.

n+1 (cid:18)
X

n=1
X

The second and third conclusion about the asymptotically normality of theorem 3.2 are crucially

based on the asymptotical study of the Laplace transform

for N

.
→ ∞

ϑ
N
L

µ
N

(cid:16)

(cid:17)

= Eϑ exp

µ
2N h

M

iN

,

(cid:17)

−

(cid:16)

10

 
 
 
First, we can rewrite

ϑ
N
L

µ
N

by the following formula:

(cid:0)

(cid:1)

ϑ
N
L

(cid:16)

µ
N

(cid:17)

= Eϑ exp

1
2

 −

N

n=1
X

ζ∗
nMnζn

,

!

.

where

Mn = µ

N σ2

n+1

anan∗

As presented in [1] and using the Cameron-Martin formula [5], we have the following result.

Lemma 5.3. For any N , the following equality holds:

ϑ
N
L

µ
N

N

=

(cid:16)
where (γ(n, m), 1

(cid:17)

n=1
Y

[det(Id + γ(n, n)

Mn)]−1/2 exp

1
2

 −

N

n=1
X

z∗
nMn(Id + γ(n, n)

Mn)−1zn

,

!

m

≤

≤

n) is the unique solution of the equation

γ(n, m) =

n

"

r=m+1
Y

Ar−1(Id + γ(r, r)

Mr)−1

γ(m, m),

#

and the function (γ(n, n, n

≥

1)) is the solution of the Ricatti equation:

γ(n, n) = An−1(Id + γ(n

−
It is worth to emphasize that (zn, 1

1, n

n

≤

−

≤

Mn−1)−1γ(n
1)
N ) is the unique solution of the equation

n−1 + σ2

nbb∗.

1)A∗

1, n

−

−

(26)

(27)

zn = mn −

n−1

r=1
X

γ(n, r)[Id + γ(r, r)

Mr]−1

Mrzr, z0 = m0.

where mn = Eζo
n.

With the explicit formula of the Laplace transform presented in Lemma 5.3, we have its asymp-

totical property.

Lemma 5.4. Under the condition (2), for any µ

R, we have

∈

ϑ
lim
N
N→∞ L

µ
N

(cid:16)

(cid:17)

= exp

µ
2 I

(ϑ)

(cid:17)

−

(cid:16)

where

I

(ϑ) = 1

1−ϑ2 + 1

(1−ϑ)2 .

Proof. In [1] we have stated that

N

lim
N→∞

n=1
Y

[det(Id + γ(n, n)

Mn)]−1/2 =

Since the component of γ(n, n) is bounded, we have

lim
N→∞

Id + γ(n, n)

Mn = Id.

11

µ

1

−

.

ϑ2

(28)

(29)

On the other hand,

N

a∗
nEζ0
n
σn+1 (cid:19)
which has been presented in last part. At last, notice

nMnmn =

n=1 (cid:18)
X

n=1
X

µ
N

m∗

N

2

−→

(1

µ

−

, N

.
→ ∞

ϑ)2

(30)

n−1

γ(n, r)[Id + γ(r, r)

Mr]−1

r=1
X
we have that

Mrzr =

n−1

n

r=1 "
X

τ =r+1
Y

n−1

Aτ −1(Id + γ(τ, τ )

Mτ )−1

[Id + γ(r, r)

Mr]

Mrzr

#

lim
N→∞

γ(n, r)[Id + γ(r, r)

Mr]−1

Mrzr = 0.

(31)

Combining (29) and (30) and (31), the Lemma 5.4 achieves.

r=1
X

From this conclusion, it is immediate that

Pϑ lim
N→∞

1
N h

M

iN =

I

(ϑ) .

Moreover, using the central limit theorem for martingale, we have

1
√N

MN =

⇒ N

(ϑ)).

(0,

I

Consequently, the asymptotical part of the theorem 3.2 is obtained.
The strong consistent is immediate when we change µ

N with a positive proper constant µ in
Lemma 5.4 because the determinant part tends to 0 as presented in section 5.2 of [1]and the extra
part is bounded.

Acknowledgements The authors would like to thank the editor and reviewers for their valuable
comments which improve the manuscript.

Availability of data and materials Data sharing not applicable to this article as no data sets
were generated or analyzed during the current study.

Funding Not applicable.

Competing interests The authors declare that they have no competing interests.

References

[1] Brouste, A., Cai, C., Kleptsyna, M. (2014). Asymptotic properties of the MLE for the au-
toregressive process coeﬃcients under stationary Gaussian noise. Mathematical Methods of
Statistics, 23(2), 103-115.

12

[2] Brouste, A., Kleptsyna, M. (2012). Kalman type ﬁlter under stationary noises. Systems Control

Letters, 61(12), 1229-1234.

[3] Hosking, J. (1981) Fractional Diﬀerencing, Biometrika, 68(1), 165-176.

[4] Istas, J. and Lang. G (1997) Quadratic variation and estimation of local Holder index of a

Gaussian process, Annales de l’ I.H.P. section B, 33(4), 407-436.

[5] Kleptsyna, M. L., Le Breton, A., Viot, M. (2002). New formulas concerning Laplace transforms
of quadratic forms for general Gaussian sequences. International Journal of Stochastic Analysis,
15(4), 309-325.

[6] Liptser, R. S., Shiryaev, A. N. (2001). Statistics of Random Processes II: Applications (Vol.

2). Springer Science and Business Media.

[7] Ljung, L. (1987) System Identiﬁcation-Theory for the user, Prentice Hall, 1987.

[8] Ng, T. S., Qureshi, Z. H., Cheah, Y. C. (1984). Optimal input design for an AR model with

output constraints. Automatica, 20(3), 359-363.

[9] Robinson, P. (1995) Log-periodogram regression of time series with long-range dependence,

Annals of Statistics, 23(3), 1048-1072.

[10] Brouste, A. and Cai, C. (2013) Controlled drift estimation in fractional diﬀusion process,

Stochastic and Dynamics, 13(3).

[11] Brouste, A., Kleptsyna, M. and Popier, A. (2012) Design for estimation of drift parameter in
fractional diﬀusion system, Statistical Inference for Stochastic Processes, 15(2), 133-149.

[12] Comte, F. and Renault, E. (1998) Long memory in continuous-time stochastic volatility models,

Mathematical Finance, 8(4), 291-323.

[13] Gatheral, J. and Jaisson, T. and Rosenbaum, M. (2018) Volatility is rough, Quantitative

Finance, 18(6), 933-949.

[14] Durbin, J. (1960) The ﬁtting of time series models, Rev. Inst. Int, Stat, 28, 233-243.

[15] Yajima, Y. (1985) On estimation of Long-Memory Time Series Models, Australian J. Statist.

27(3), 302-320.

13

