2
2
0
2

n
u
J

7

]

V
C
.
s
c
[

2
v
7
5
2
2
0
.
6
0
2
2
:
v
i
X
r
a

Eﬃcient Annotation and Learning for 3D Hand Pose
Estimation: A Survey

Takehiko Ohkawa1*, Ryosuke Furuta1 and Yoichi Sato1

1Institute of Industrial Science, The University of Tokyo, 4-6-1 Komaba, Meguro-ku,
Tokyo, 153-8505, Japan.

*Corresponding author(s). E-mail(s): ohkawa-t@iis.u-tokyo.ac.jp;
Contributing authors: {furuta, ysato}@iis.u-tokyo.ac.jp;

Abstract

In this survey, we present comprehensive analysis of 3D hand pose estimation from the perspec-
tive of eﬃcient annotation and learning. In particular, we study recent approaches for 3D hand
pose annotation and learning methods with limited annotated data. In 3D hand pose estimation,
collecting 3D hand pose annotation is a key step in developing hand pose estimators and their appli-
cations, such as video understanding, AR/VR, and robotics. However, acquiring annotated 3D hand
poses is cumbersome, e.g., due to the diﬃculty of accessing 3D information and occlusion. Motivated
by elucidating how recent works address the annotation issue, we investigated annotation meth-
ods classiﬁed as manual, synthetic-model-based, hand-sensor-based, and computational approaches.
Since these annotation methods are not always available on a large scale, we examined methods of
learning 3D hand poses when we do not have enough annotated data, namely self-supervised pre-
training, semi-supervised learning, and domain adaptation. Based on the analysis of these eﬃcient
annotation and learning, we further discuss limitations and possible future directions of this ﬁeld.

Keywords: Hand Pose Estimation, Annotation, Learning with Limited Labels

1 Introduction

What kind of methods for annotating and learn-
ing 3D hand poses have been developed in the past,
are currently in vogue, and can be expected in
the future? This question will be taken into con-
sideration in the development of 3D hand pose
estimators because it is not straightforward to
acquire 3D hand pose annotation (i.e., 3D key-
points of the hand joints) and train the estimators
with limited resources in deployment. The two key
issues concerning annotation and learning have
inhibited us from constructing accurate 3D hand
pose estimators and utilizing them in downstream

applications, such as hand-object interaction anal-
ysis [2, 48, 57], egocentric vision [12, 18, 41],
augmented and virtual reality [31, 59], and robot
learning from human demonstration [21, 34, 44].
When collecting the annotation for training
3D hand pose estimators, we often face three
obstacles, namely, 3D measurement, occlusion,
and dataset bias depending on the annotation
method. As for the ﬁrst obstacle, particularly dur-
ing manual annotation of a single RGB image,
3D information on hands is not always accessi-
ble. Other possible annotation methods use hand
markers, depth sensors, or multi-view camera stu-
dios to provide the 3D positional labels on a single

1

 
 
 
 
 
 
synthetic-model-based [8, 23, 37, 39, 65], hand-
marker-based [15, 55, 62], and computational
approaches [19, 27, 28, 36, 49, 66]. While man-
ual annotation requires querying of human anno-
tators, hand markers automate the annotation
process by tracking sensors attached to hand
joints. Synthetic methods utilize computer graph-
ics engines to render plausible hand images
with precise keypoint coordinates. Computational
methods assign labels based on a network’s pre-
dictions and/or external knowledge of a 3D hand
template model.

Since the scenarios in which these annotation
methods can be applied are limited, learning with
a small amount of labels is another key issue. For
learning from limited annotated data, leveraging
a large pool of unlabeled hand images as well as
the labeled images is a primary objective in self-
supervised pretraining, semi-supervised learning,
and domain adaptation. Self-supervised pretrain-
ing encourages the hand pose estimator to learn
from unlabeled hand images, so it enables to
build a strong encoder network before optimizing
a supervised loss. While semi-supervised learning
trains the estimator with labeled and unlabeled
hand images collected from the same environment,
domain adaptation further solves the so-called
problem of domain gap between the two image
sets, e.g., the diﬀerence between synthetic data
and real data.

Overall, this survey provides unique insight
into the estimation of 3D hand poses because
previously published surveys mainly focus on
methodical modeling [7, 13, 29, 33], joint mod-
eling with object poses [30], real and virtual
object interaction [1], and sensor-based estima-
tion [10]. The rest of this survey is organized
as follows. In Section 2, we introduce the for-
mulation and modeling of 3D hand pose estima-
tion. In Section 3, we present open challenges in
the construction of hand pose datasets, involv-
ing depth measurement, occlusion, and dataset
bias. In Section 4, we cover existing methods
of 3D hand pose annotation, namely manual,
synthetic-model-based, hand-marker-based, and
computational approaches. In Section 5, we pro-
vide learning methods from a limited amount
of annotated data, namely self-supervised pre-
training, semi-supervised learning, and domain
adaptation. In Section 6, we ﬁnally show possible
future directions of 3D hand pose estimation.

Fig. 1 Issues concerning 3D hand pose estimation. In
this survey, we tackle (i) how to obtain 3D hand pose
annotation and (ii) learning even with a limited amount
of annotated data. These two perspectives are crucial for
building 3D hand pose estimators eﬃciently.

image. However, systems based on such meth-
ods are expensive and/or cumbersome, so they
cannot be installed in every situation. As for
the second obstacle, occlusions by hand parts or
hand-held objects are problematic because they
hinder human annotators from localizing the posi-
tions of hand joints. As for the third obstacle,
annotated data are biased to a speciﬁc condi-
tion constrained by the annotation method. For
instance, the annotation methods based on hand
markers and multi-view setups prefer to capture
the data in a laboratory setting because it is easy
to install the sensors and supply power to them.
Existing datasets based on these annotation meth-
ods thus contain an inevitable bias to a limited
variety of backgrounds and interacting objects.
Given these obstacles in the annotation process,
learning methods with limited access to the anno-
tated data have attracted attention, but have not
yet been established.

The aforementioned obstacles have motivated
us to review recent studies on (i) 3D hand pose
annotation and (ii) learning from a limited amount
of annotated data, as shown in Fig. 11. Our survey
aims to study how to give 3D hand pose annota-
tion inexpensively, when such annotation methods
are available, and how to train 3D hand pose esti-
mators eﬃciently. We also discuss possible future
directions of this ﬁeld beyond the current state of
the art.

The annotation methods focused on in this
[6, 39, 54],

study are categorized as manual

1adapted from [65].

2

2 Overview of 3D Hand Pose

Estimation

Input. 3D hand poses are usually inferred from
a monocular RGB/depth image or multi-view
RGB images. Among the methods based on such
images, 3D hand pose estimation from a single
RGB image has been popularly studied because
RGB images are more aﬀordable and convenient
to capture. In contrast, depth images and multi-
view RGB images can provide richer information
about 3D structure of hands. However, depth sen-
sors and multi-camera setups are expensive and
energy-consuming, so they are not always applied
to every environment (e.g., outdoors).

Output. The output of 3D hand pose estima-
tors is parameterized by the hand joint positions
with 14, 16, or 21 keypoints, which are deﬁned
in [58], [56], and [43], respectively. The dense rep-
resentation of 21 hand joints has been popularly
used since it contains more precise information
about hand structure. For a single RGB image
in which depth and scale are ambiguous, the 3D
coordinates of the hand joint relative to the hand
root are estimated from a scale-normalized hand
image [3, 16, 65]. To understand the structure of
hands in more detail, recent works regress 3D hand
pose and shape parameters together [2, 16, 38].

Methods. Single image-based methods learn
the correspondence between the input image and
its label of the 3D hand pose. Standard methods
based on a single RGB image [3, 16, 65] con-
sist of two steps: estimation of 2D hand poses
by heatmap optimization and depth regression for
each 2D keypoint. The 2D keypoints are learned
by optimizing heatmaps centered on each hand
joint position. An additional regression network
predicts the depth of each detected 2D hand
keypoint. In contrast, depth-based hand pose
estimation also utilizes this heatmap optimiza-
tion [26, 45, 60]. Other single image-based meth-
ods directly regress keypoint coordinates [47, 53].
Multi-image-based methods use multiple 2D pose
predictions and triangulation to reconstruct 3D
hand poses [36, 49].

To generate feasible hand poses, regularization
is a key trick in correcting the predicted 3D hand
poses. Based on the anatomical study of hands,
bio-mechanical constraints are imposed to limit

Fig. 2 Diﬃculty of hand pose annotation in a single
RGB image [49]. Occlusion of hand joints is caused by (a)
articulation, (b) viewpoint bias, and (c) grasping objects.

predicted bone lengths and joint angles [11, 32,
51].

3 Challenges in Dataset

Construction

Task formulation and algorithms for estimating
3D hand poses are outlined in Section 2. To train
an estimator, a large amount of training data with
diverse hand poses, viewpoints, and backgrounds
is necessary. However, obtaining such massive
hand datasets with an accurate annotation of 3D
hand poses is challenging for the following reasons.
3D information.
of
Although numerous works seek to estimate 3D
hand poses from a single RGB image [3, 16, 65],
annotating the 3D position of hand joints from a
single RGB image is inherently impossible due to
an ill-posed condition.

Inaccessibility

the

To assign accurate hand pose labels, hand-
marker-based annotation using magnetic sen-
sors [15, 62], motion capture systems [55], or hand
gloves [17] has been studied. They provide 6-
DoF information (i.e., location and orientation)
about hand joints. However, these methods are
unsuitable for visual learning based on the RGB
modality because they change hand appearance
drastically.
(e.g.,
On
stu-
RealSense)
dios [6, 19, 36, 49, 66] make it possible to obtain
depth information near hand regions. Given
2D keypoints for an image, these setups enable
annotation of 3D hand poses by measuring the
depth distance at each 2D keypoint. However,
these annotation methods do not always produce
an accurate annotation e.g., due to an occlu-
sion problem. The depth images are signiﬁcantly
aﬀected by the sensor noise and become inaccu-
rate especially when the hands are far from the

depth
contrary,
or multi-view

sensors
camera

3

multi-camera setups [6, 19, 36, 49, 66] enable the
annotation easier because they can reduce the
occlusion eﬀect and make it simple to extract the
hand regions. However, since the setups are not
easily portable, their variations in environments
(e.g., backgrounds and interacting objects) are
limited.

4 Annotation Methods

Given the above challenges concerning the con-
struction of hand pose datasets, we review how
existing methods for annotating 3D hand poses
address the challenges, and then discuss the pros
and cons of each method. As shown in Table 1,
we categorize the annotation methods as manual,
synthetic-model-based, hand-marker-based, and
computational approaches. We also summarize the
characteristics of each approach in Table 2.

4.1 Manual annotation

Dexter+Object [54] and EgoDexter [39] simply
annotated hand keypoints for the ﬁve ﬁnger-
tips manually on the depth images. This method
enables to assign accurate 3D coordinates (i.e.,
2D position and depth) when hand joints are fully
visible. However, it not only lacks the scalability
and density of the hand keypoints due to annota-
tion cost, but is inapplicable when the occlusion
occurs.

Recently, instead of annotating the ﬁngertips
only, DexYCB [6] utilized a multi-camera setup to
solve the occlusion problem and label hand joints
more densely. They assumed that a multi-view
systems with eight cameras can avoid self- and
object occlusions and complement invisible key-
points from the images captured in diﬀerent views.
They also used a crowd-source service (Amazon
Mechanical Turk) for scaling up the annotation
and labeled 21 hand joints over 582K frames. Since
the subjects interact only with 20 YCB objects [4]
in a simpliﬁed background, the multi-view dataset
contains the dataset bias explained in Section 3.

4.2 Synthetic-model-based

annotation

To acquire tremendous hand images and labels,
synthetic methods based on computer graph-
ics engines (e.g., Unity and Blender) have been

Fig. 3 Example of major data collection setups. The syn-
thetic image on the left (ObMan [23]) can be generated
inexpensively, but they exhibit unrealistic hand texture.
The hand markers on the middle (FPHA [15]) enable auto-
matic 6-DoF tracking of hand joints although the markers
distort the appearance of hands. The in-lab setup on
the right (DexYCB [6]) makes the data acquisition easier
although its variation in environments is limited.

sensor and appear to be tiny. Depth measure-
ment by triangulation in a multi-camera setting
is unreliable when the cameras are not densely
arranged.

Occlusion. Hand images often contain com-
plex occlusions that distract human annotators
from localizing hand keypoints. Examples of pos-
sible occlusions are shown in Fig. 2. In ﬁgure
(a), articulation causes a self-occlusion that makes
some hand joints (e.g., ﬁngertips) invisible due to
the overlap with the other parts of the hand. In
ﬁgure (b), the self-occlusion depends on a speciﬁc
camera viewpoint. In ﬁgure (c), hand-held objects
induce occlusion that hides the hand joints by the
objects during interaction. To address this issue,
multi-view camera studios [6, 19, 36, 49, 66] or
hand-marker-based tracking [15, 55, 62] has been
studied, but these approaches have not completely
removed the eﬀect of occlusion.

Dataset bias. To improve the generalization
ability of hand pose estimators, hand images must
be annotated under diverse imaging conditions
(e.g., lighting, viewpoints, hand poses, and back-
grounds). However, it is diﬃcult to create such
large and diverse datasets nowadays due to the
aforementioned issues concerning 3D accessibility
and occlusion. Rather, existing hand pose datasets
exhibit the bias to a particular imaging condition
constrained by the annotation method.

As shown in Fig. 3, generating data using
is cost-
synthetic models [8, 23, 37, 39, 65]
eﬀective, but
it crates unrealistic hand tex-
ture [41]. Although the hand-marker-based anno-
tation [15, 55, 62] can automatically track the
6-DoF of sensor-attached hand joints, the sen-
sors distort the hand appearance and hinder the
natural hand movement. In-lab data acquired by

4

Table 1 Taxonomy of methods for annotating 3D hand poses. We categorize the annotation methods as manual,
synthetic-model-based, hand-marker-based, and computational annotation.

Annotation

Dataset

Year Modality

Resolution

#Frame

Manual

(multi-view)

Synthetic

Hand marker

Computational
Triangulation

Model ﬁtting

Dexter+Object [54]
EgoDexter [39]
DexYCB [6]

SynthHands [39]
RHD [65]
GANerated [37]
ObMan [23]
MVHM [8]

BigHand2.2M [62]
FPHA [15]
GRAB [55]

2016
2017
2021

2017
2017
2018
2019
2021

RGB-D
RGB-D
RGB-D

RGB-D
RGB
RGB
RGB-D
RGB-D

2017 Depth
RGB-D
2018
2020 MoCap

640×480
640×480
640×480

640×480
320×320
256×256
256×256
256×256

640×480
640×480
-

Panoptic Studio [49]
InterHand2.6M [36]
FreiHAND [64, 66]
YouTube3DHands [27]
HO-3D [19]
H2O [28]

2017
2020
2019
2020
2020
2021

RGB
RGB
RGB
RGB
RGB-D
RGB-D

1920×1080
512×334
224×224
640×480
640×480
1280×720

3K
3K
582K

220K
43K
331K
154K
320K

2,200K
105K
1,624K

15K
2,590K
37K
47K
103K
571K

#Subj./
#Obj.

2 / 2
4 / -
10 / 20

- / 7
20 / -
- / -
20 / 3K
- / -

10 / -
6 / 4
10 / 51

- / -
27 / -
32 / 27
(109) / -
10 / 10
4 / 10

#View

1 (3rd)
1 (1st)
8 (3rd)

Motion

(cid:88)
(cid:88)
(cid:88)

Obj.
pose
(cid:88)
×
(cid:88)

×
5 (1st)
×
1 (3rd)
×
1 (3rd)
×
1 (3rd)
8 (3rd)
×
1 (1st + 3rd) (cid:88)
(cid:88)
1 (1st)
(cid:88)
-

(cid:88)
31 (3rd)
(cid:88)
80-140 (3rd)
×
8 (3rd)
(cid:88)
1 (3rd)
(cid:88)
1-5 (3rd)
5 (1st & 3rd) (cid:88)

×
×
×
(cid:88)
×

-
(cid:88)
(cid:88)

×
-
×
×
(cid:88)
(cid:88)

Sensor/Engine

Creative Senz3D
RealSense SR300
RealSense D415

Unity
Unity
Unity
Blender/GraspIt [35]
Blender

NDI trakSTAR
NDI trakSTAR
VICON Vantage 16

HD camera
HD camera
Basler ace
-

Azure Kinect

Table 2 Pros and cons of each annotation approach.

Annotation

Pros

Cons

Manual

High accuracy

Synthetic

Large scale
High diveristy
Low cost

Hand marker

6-DoF tracking
Low annotation cost

Computational

Natural motion
Low annotation cost

Labor intensive
Vulnerable to occlusion

Sim-to-real gap
Diﬃculty of interaction simulation

Requires special devices
Changes visual modality
Prevents natural motion

Lacks diversity
Depends on model performance
Needs multi-camera setups

investigated. SynthHands [39] and RHD [65] ren-
dered synthetic hand images with randomized
real backgrounds from either a ﬁrst- or third-
person view. MVHM [8] rendered multi-view syn-
thetic hand data rendered from eight viewpoints.
These datasets have succeeded in providing accu-
rate hand keypoint labels on a larger scale than
those built by manual annotation. Although they
can generate various background patterns inex-
pensively, the lighting and texture of hands are
not well simulated, and the simulation of hand-
object interaction is not considered in the data
generation process.

To handle these issues, GANerated [37] uti-
lized GAN-based image translation to stylize
synthetic hands more realistically. Furthermore,
ObMan [23] modeled the hand-object relation-
ship in data generation using a hand interac-
tion simulator (Graspit [35]) with 3D object
models (ShapeNet [5]). Ohkawa et al . proposed
foreground-aware image stylization to convert the
simulation texture in the ObMan data to a more

Fig. 4 Illustration of a hand marker setup [62].

realistic one while separating the hand regions and
backgrounds [41]. However, the ObMan data pro-
vide static hand images with hand-held objects
but without hand motion. Overall, these latest
works have not been able to simulate hands with
appearance and interaction comparable to real
hands.

4.3 Hand-marker-based annotation

As shown in Fig. 4, hand-marker-based annotation
automatically tracks hand joints with attached
hand sensors. BigHand2.2M [62] and FPHA [15]
used several magnetic sensors attached to the
hands and inverse kinematics to infer all 21 hand
joints. Thus, the 6-DoF of all hand joints was
not accurately detected. The sensors also obstruct
natural hand movement and distort the appear-
ance of the hand. Due to the changes in hand
appearance, these datasets are typically used for

5

Fig. 5 Illustration of a multi-camera setup [66].

the benchmark of depth-based 3D hand pose esti-
mation, not RGB-based tasks. On the contrary,
GRAB [55] was built with a motion capture sys-
tem for human hands and body, but it does not
possess visual modality.

Fig. 6 Self-supervised pretraining of 3D hand pose esti-
mation [64]. The pretraining phase aims to construct an
improved encoder network by using many unlabeled data
before the supervised learning (step 2).

These computational methods can generate
labels with little human eﬀort, but the quality of
the labels is not ensured. In fact, annotation qual-
ity depends on the number of cameras and their
arrangement, the accuracy of hand detection and
estimation of 2D hand poses, and the performance
of triangulation and ﬁtting algorithms.

4.4 Computational annotation

5 Learning with Limited

Computational annotation typically needs a
multi-camera studio in which 3D keypoint posi-
tion are calculated by using triangulation or hand
model ﬁtting (see Fig. 5). Unlike hand-marker-
based annotation, this methods can capture nat-
ural hand motion without using hand markers.

Panoptic Studio [49] and InterHand2.6M [36]
triangulated a 3D hand pose from multiple 2D
hand keypoints provided by an open source
library, OpenPose [25], or human annotators. The
reconstructed 3D hand pose is re-projected onto
the image planes of other cameras to annotate
hand images with novel viewpoints.

Although the variation of backgrounds and
grasping objects is limited in these datasets, Frei-
HAND [64, 66] increased the image variation by
randomly synthesizing the background with real
human hands captured in the studio. It utilized a
3D hand template (MANO [46]) to ﬁt multi-view
hand images with sparse 2D keypoint annotation.
YouTube3DHands [27] used the MANO model to
ﬁt estimated 2D hand poses in YouTube videos.
HO-3D [19] and H2O [28] attempted to jointly
annotate 3D hand and object poses. Using esti-
mated 2D keypoints, they ﬁtted the MANO model
and 3D object models to the hand images with
object interaction.

6

Labels

As explained in Section 4, existing annotation
methods have certain pros and cons. Since perfect
annotation in terms of amount and quality can-
not be assumed, training 3D hand pose estimators
with limited annotated data is another important
study. Accordingly, we introduce learning methods
using unlabeled data in this section, namely self-
supervised pretraining, semi-supervised learning,
and domain adaptation.

5.1 Self-supervised pretraining

Self-supervised pretraining aims to utilize mas-
sive unlabeled hand images and build an improved
encoder network before supervised learning with
labeled images. As shown in Fig. 6,
recent
works [50, 64] ﬁrst pretrained the encoder network
by using contrastive learning (e.g., MoCo [24] and
SimCLR [9]) and then ﬁne-tuned the whole net-
work in a supervised manner. The core idea of
contrastive learning is to push a pair of similar
instances closer together in an embedding space,
while unrelated instances are pushed apart. This
approach focuses on how to deﬁne the similar-
ity of hand images and how to design embedding

Fig. 7 Semi-supervised learning of 3D hand pose estima-
tion [32]. The model is trained jointly on annotated data
and unlabeled data with pseudo-labels.

techniques. Spurr et al . proposed to geometri-
cally align two features generated from diﬀerently
augmented instances [50]. Zimmermann et al .
found that multi-view images with the same hand
pose can be used as a positive pair with high
similarity [64].

Fig. 8 Simulation-to-real domain gap [42]. The mod-
els learned on synthetic data exhibit limited capacity of
inferring hand (object) poses in real images.

5.2 Semi-supervised learning

5.3 Domain adaptation

As shown in Fig. 7, semi-supervised learning is
used to learn small labeled data and large unla-
beled data simultaneously. Liu et al . proposed
a pseudo-labeling method that learns unlabeled
instances with pseudo-ground-truth given from
the model’s prediction [32]. This pseudo-label
training is applied only when its prediction satis-
ﬁes spatial and temporal constraints. The spatial
constraints check the correspondence of 2D hand
pose (bounding box) prediction and its 3D predic-
tion reprojected onto an image plane. In addition,
they include a constraint based on bio-mechanical
feasibility, such as bone lengths and joint angles.
The temporal constraints indicate the smoothness
of hand pose and mesh predictions over time.

Yang et al . proposed the combination of
pseudo-labeling and consistency training [61]. In
pseudo-labeling, the generated pseudo-labels are
corrected by ﬁtting the hand template model.
In addition, consistency losses between the pre-
dictions of diﬀerently augmented instances and
between the modalities of 2D hand poses and hand
masks are enforced.

Spurr et al . applied adversarial training to
a sequence of predicted hand poses [52]. The
encoder network is expected to be improved by
fooling a discriminator that distinguishes between
plausible and invalid hand poses.

A few works have tackled to solve a sim-to-real
domain adaptation problem for 3D hand pose esti-
mation. This problem involves accessing labeled
synthetic images and unlabeled real images dur-
ing training and aims to improve the performance
on the real images (see Fig. 8). Qi et al . proposed
an image translation method to alter synthetic
texture and appearance to real ones and learn real-
like synthetic data [42]. Zhang et al . developed a
feature matching method between the two image
sets by using adversarial training based on the
Wasserstein distance [63].

6 Future Directions

6.1 Extensive and ﬂexible camera

systems

We believe that hand image capture will require
more extensive and ﬂexible camera systems. To
reduce the occlusion eﬀect without the need for
hand markers, recently published hand datasets
have been acquired by multi-camera setups, e.g.,
DexYCB [6], InterHand2.6M [36], FreiHAND [66],
and H2O [28]. In particular, InterHand2.6M has
numerous camera views (80-140), and H2O cap-
tures images from ego-exo perspective that syn-
chronizes cameras with ﬁrst and third-person
views [20]. These attempts beneﬁt for capturing
less occluded hand motion and more dynamic
behavior because the ﬁrst-person users with body-
worn cameras are mobile. Despite such numerous

7

cameras and ﬂexible users’ behavior, these cam-
era systems are not large enough to allow multiple
persons to work jointly. Thus, expanding their size
is another direction, the completion of which will
advance the hand analysis in social interaction and
many-person cooperation.

6.2 Increase the variation of users’

performing tasks

A major limitation of existing hand datasets is
the narrow variation of users’ performing tasks
and grasping objects. To avoid object occlusion,
some works did not capture hand-object interac-
tion [36, 62, 65]. Other works [6, 19, 23] used
pre-registered 3D object models (e.g., YCB [4]
and ShapeNet [5]) to make the formulation and
annotation of hand-object poses easier. Thus, the
types of objects used in the current hand datasets
are biased and the existing networks cannot infer
hand poses grasping unknown objects well. From
an aﬀordance perspective [22], diversifying the
object types and performing tasks are crucial to
enrich hand pose variation in training data and
improve the generalization of the estimators to
other diﬀerent situations.

6.3 Annotation and learning with

minimal human intervention

Sections 4 and 5 provide eﬃcient hand pose anno-
tation and learning methods with limited labels
independently. To minimize the eﬀort of human
intervention, utilizing ﬁndings from both anno-
tation and learning perspectives is one of the
promising approaches. Feng et al . exploited active
learning to optimize which unlabeled instance
should be annotated and semi-supervised learn-
ing to jointly learn labeled data and large unla-
beled data [14]. Although their use cases are
still constrained to the multi-camera setups and
triangulation-based 3D pose estimation, such a
collaborative approach is eﬃcient and eﬀective in
terms of annotation resources and performance
gain.

6.4 Cross-dataset generalization and

adaptation

Increasing the generalization ability across dif-
ferent datasets or adapting models to a speciﬁc

8

domain is a remaining issue. The bias of existing
training datasets towards their imaging conditions
hinders the estimators from inferring test images
captured under very diﬀerent imaging conditions.
In fact, as reported in [20, 66], the generalization
ability of existing models is poor. Since pop-
ularly studied multi-camera settings cannot be
applied to unconstrained settings (e.g., outdoors),
it is crucial to transfer hand pose estimators
from indoor hand datasets to in-the-wild outdoor
videos [40]. Thus, aggregating multiple annotated
yet biased datasets for generalization and robustly
adapting the estimators to diﬀerent environments
are important future tasks.

7 Summary

In this survey, we studied 3D hand pose estima-
tion from the perspective of eﬃcient annotation
and learning. We provided a clear picture of the
current state of 3D hand pose modeling and
open challenges related to dataset construction.
To reveal the pros and cons of existing meth-
ods for annotating 3D hand poses, we introduced
the annotation methods categorized as manual,
synthetic-model-based, hand-marker-based, and
computational approaches. In addition, we pre-
sented learning methods that can be applied even
when the annotation is inaccessible, namely self-
supervised pretraining, semi-supervised learning,
and domain adaptation. Lastly, we discussed pos-
sible future advances of 3D hand pose estimation
in terms of the conﬁguration of the camera sys-
tems, object variations, selecting instances to be
labeled, and generalization and adaptation.

References

[1] A. Ahmad, C. Migniot, and A. Dipanda.
Hand pose estimation and tracking in real
and virtual interaction:a review. Image and
Vision Computing, 89:35–49, 2019.

[2] A. Boukhayma, R. de Bem, and P. H. S.
Torr. 3D hand shape and pose from images
in the wild. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern
Recognition (CVPR), pages 10843–10852,
2019.

[3] Y. Cai, L. Ge, J. Cai, and J. Yuan. Weakly-
supervised 3D hand pose estimation from
monocular RGB images. In Proceedings of the

European Conference on Computer Vision
(ECCV), pages 678–694, 2018.

[4] B. C¸ alli, A. Walsman, A. Singh, S. S. Srini-
vasa, P. Abbeel, and A. M. Dollar. Bench-
marking in manipulation research: Using
the yale-cmu-berkeley object and model
set.
IEEE Robotics Automation Magazine,
22(3):36–52, 2015.

[5] A. X. Chang, T. A. Funkhouser, L. J. Guibas,
P. Hanrahan, Q.-X. Huang, Z. Li, S. Savarese,
M. Savva, S. Song, H. Su, J. Xiao, L. Yi,
and F. Yu. Shapenet: An information-rich
3d model repository. CoRR, abs/1512.03012,
2015.

[6] Y.-W. Chao, W. Yang, Y. Xiang, P.
Molchanov, A. Handa, J. Tremblay, Y. S.
Narang, K. Van Wyk, U. Iqbal, S. Birchﬁeld,
J. Kautz, and D. Fox. DexYCB: A bench-
mark for capturing hand grasping of objects.
In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition
(CVPR), pages 9044–9053, 2021.

[7] T. Chatzis, A. Stergioulas, D. Konstantinidis,
K. Dimitropoulos, and P. Daras. A compre-
hensive study on deep learning-based 3d hand
pose estimation methods. Applied Sciences,
10:6850, 09 2020.

[8] L. Chen, S.-Y. Lin, Y. Xie, Y.-Y. Lin, and X.
Xie. MVHM: A large-scale multi-view hand
mesh benchmark for accurate 3d hand pose
estimation. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Com-
puter Vision (WACV), pages 836–845, 2021.
[9] T. Chen, S. Kornblith, M. Norouzi, and
G. E. Hinton. A simple framework for con-
trastive learning of visual representations. In
Proceedings of the International Conference
on Machine Learning (ICML), volume 119,
pages 1597–1607, 2020.

[10] W. Chen, C. Yu, C. Tu, Z. Lyu, J. Tang,
S. Ou, Y. Fu, and Z. Xue. A survey on
hand pose estimation with wearable sensors
and computer-vision-based methods.
Sen-
sors, 20(4), 2020.

[11] Y. Chen, Z. Tu, D. Kang, L. Bao, Y. Zhang,
X. Zhe, R. Chen, and J. Yuan. Model-based
3d hand reconstruction via self-supervised
learning.
In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern
Recognition (CVPR), pages 10451–10460,
2021.

[12] D. Damen, H. Doughty, G. M. Farinella, A.
Furnari, J. Ma, E. Kazakos, D. Moltisanti, J.
Munro, T. Perrett, W. Price, and M. Wray.
Rescaling egocentric vision.
International
Journal of Computer Vision (IJCV), early
access, 2021.

[13] B. Doosti. Hand pose estimation: A survey.

CoRR, abs/1903.01013, 2019.

[14] Q. Feng, K. He, H. Wen, C. Keskin, and
Y. Ye. Active learning with pseudo-labels
for multi-view 3d pose estimation. CoRR,
abs/2112.13709, 2021.

[15] G. Garcia-Hernando, S. Yuan, S. Baek, and
T.-K. Kim. First-person hand action bench-
mark with RGB-D videos and 3D hand
pose annotations.
the
IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 409–
419, 2018.

In Proceedings of

[16] L. Ge, Z. Ren, Y. Li, Z. Xue, Y. Wang,
J. Cai, and J. Yuan. 3D hand shape and
pose estimation from a single RGB image.
In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition
(CVPR), pages 10833–10842, 2019.

[17] O. Glauser, S. Wu, D. Panozzo, O. Hilliges,
and O. Sorkine-Hornung.
Interactive hand
pose estimation using a stretch-sensing soft
glove.
ACM Transactions on Graphics
(ToG), 38(4):41:1–41:15, 2019.

[18] K. Grauman, A. Westbury, E. Byrne, Z.
Chavis, A. Furnari, R. Girdhar, J. Ham-
burger, H. Jiang, M. Liu, X. Liu, M. Martin,
T. Nagarajan, I. Radosavovic, S. K. Ramakr-
ishnan, F. Ryan, J. Sharma, M. Wray, M.g
Xu, E. Zhongcong Xu, C. Zhao, S. Bansal,
D. Batra, V. Cartillier, S. Crane, T. Do, M.
Doulaty, A. Erapalli, C. Feichtenhofer, A.
Fragomeni, Q. Fu, C. Fuegen, A. Gebrese-
lasie, C. Gonzalez, J. Hillis, X. Huang, Y.
Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur,
A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K.
Mangalam, R. Modhugu, J. Munro, T. Mur-
rell, T. Nishiyasu, W. Price, P. R. Puentes,
M. Ramazanova, L. Sari, K. Somasundaram,
A. Southerland, Y. Sugano, R. Tao, M. Vo,
Y. Wang, X. Wu, T. Yagi, Y. Zhu, P. Arbe-
laez, D. Crandall, D. Damen, G. M. Farinella,
B. Ghanem, V. K. Ithapu, C. V. Jawahar, H.
Joo, K. Kitani, H. Li, R. Newcombe, A. Oliva,
H. Soo Park, J. M. Rehg, Y. Sato, J. Shi,

9

M. Z. Shou, A. Torralba, Lo Torresani, M.i
Yan, and J. Malik. Ego4D: Around the world
in 3, 000 hours of egocentric video. CoRR,
abs/2110.07058, 2021.

[19] S. Hampali, M. Rad, M. Oberweger, and
V. Lepetit. Honnotate: A method for 3D
annotation of hand and object poses.
In
Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition
(CVPR), pages 3196–3206, 2020.

[20] S. Han, B. Liu, R. Cabezas, C. D. Twigg,
P. Zhang, J. Petkau, T.-H. Yu, C.-J. Tai,
M. Akbay, Z. Wang, A. Nitzan, G. Dong,
Y. Ye, L. Tao, C. Wan, and R. Wang.
MEgATrack: monochrome egocentric articu-
lated hand-tracking for virtual reality. ACM
Transactions on Graphics (ToG), 39(4):87,
2020.

[21] A. Handa, K. Van . Wyk, W. Yang, J.
Liang, Y.-W. Chao, Q. Wan, S. Birchﬁeld, N.
Ratliﬀ, and D. Fox. DexPilot: Vision-based
teleoperation of dexterous robotic hand-arm
system. In Proceedings of the IEEE Interna-
tional Conference on Robotics and Automa-
tion (ICRA), pages 9164–9170, 2020.

[22] M. Hassanin, S. Khan, and M. Tahtali. Visual
aﬀordance and function understanding: A
survey. ACM Computing Survey, 54(3):47:1–
47:35, 2021.

[23] Y. Hasson, G. Varol, D. Tzionas, I. Kale-
I. Laptev, and C.
vatykh, M. J. Black,
Learning joint reconstruction of
Schmid.
hands and manipulated objects.
In Pro-
ceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition
(CVPR), pages 11807–11816, 2019.

[24] K. He, H. Fan, Y. Wu, S. Xie, and R. B.
Girshick. Momentum contrast for unsu-
pervised visual representation learning.
In
Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition
(CVPR), pages 9726–9735, 2020.

Joo,

Simon,

S.-
[25] G. Hidalgo, Z. Cao, T.
and Y.
E. Wei, Y. Raaj, H.
Sheikh. OpenPose.
https://github.com/
CMU-Perceptual-Computing-Lab/openpose.
[26] W. Huang, P. Ren, J. Wang, Q. Qi, and H.
Sun. AWR: Adaptive weighting regression
for 3D hand pose estimation. In Proceedings
of the AAAI Conference on Artiﬁcial Intelli-
gence (AAAI), pages 11061–11068, 2020.

10

[27] D. Kulon, R. A. G¨uler, I. Kokkinos, M. M.
Weakly-
Bronstein, and S. Zafeiriou.
supervised mesh-convolutional hand recon-
struction in the wild.
In Proceedings of
the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR),
pages 4989–4999, 2020.

[28] T. Kwon, B. Tekin, J. St¨uhmer, F. Bogo, and
M. Pollefeys. H2O: two hands manipulating
objects for ﬁrst person interaction recogni-
tion. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision
(ICCV), pages 10118–10128, 2021.

[29] V.-H. Le and H.-C. Nguyen. A survey on 3d
hand skeleton and pose estimation by convo-
lutional neural network. Advances in Science,
Technology and Engineering Systems Journal
(ASTES), 5(4):144–159, 2020.

[30] V. Lepetit.

in 3d
object and hand pose estimation. CoRR,
abs/2006.05927, 2020.

Recent advances

[31] H. Liang, J.G. Yuan, D. Thalmann, and N.
Magnenat-Thalmann. AR in hand: Egocen-
tric palm pose tracking and gesture recogni-
tion for augmented reality applications.
In
Proceedings of the ACM International Con-
ference on Multimedia (MM), pages 743–744,
2015.

[32] S. Liu, H. Jiang, J. Xu, S. Liu, and X.
Wang. Semi-supervised 3D hand-object poses
estimation with interactions in time. In Pro-
ceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition
(CVPR), pages 14687–14697, 2021.

[33] Y. Liu, J. Jiang, and J. Sun. Hand pose
estimation from rgb images based on deep
learning: A survey.
In IEEE International
Conference on Virtual Reality (ICVR), pages
82–89, 2021.

[34] P. Mandikal and K. Grauman. DexVIP:
Learning dexterous grasping with human
hand pose priors from video. In Proceedings
of the Conference on Robot Learning (CoRL),
pages 651–661, 2021.

[35] A. Miller and P. Allen. Graspit!: A versatile
simulator for robotic grasping. IEEE Robotics
and Automation Magazine (RAM), 11:110–
122, 2005.

[36] G. Moon, S.-I. Yu, H. Wen, T. Shiratori, and
InterHand2.6M: A dataset and

K. M. Lee.

baseline for 3D interacting hand pose estima-
tion from a single RGB image. In Proceedings
of the European Conference on Computer
Vision (ECCV), pages 548–564, 2020.
[37] F. Mueller, F. Bernard, O. Sotnychenko,
D. Mehta, S. Sridhar, D. Casas, and C.
Theobalt. GANerated Hands for real-time
3D hand tracking from monocular RGB. In
Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition
(CVPR), pages 49–59, 2018.

[38] F. Mueller, M. Davis, F. Bernard, O. Sot-
nychenko, M. Verschoor, M. A. Otaduy, D.
Casas, and C. Theobalt. Real-time pose and
shape reconstruction of two interacting hands
with a single depth camera. ACM Transac-
tions on Graphics (ToG), 38(4):49:1–49:13,
2019.

[39] F. Mueller, D. Mehta, O. Sotnychenko, S.
Sridhar, D. Casas, and C. Theobalt. Real-
time hand tracking under occlusion from an
egocentric RGB-D sensor. In Proceedings of
the IEEE/CVF International Conference on
Computer Vision (ICCV), pages 1163–1172,
2017.

[40] T. Ohkawa, Y.-J. Li, Q. Fu, R. Furuta, K. M.
Kitani, and Y. Sato. Domain adaptive hand
keypoint and pixel localization in the wild.
CoRR, abs/2203.08344, 2022.

[41] T. Ohkawa, T. Yagi, A. Hashimoto, Y.
Ushiku, and Y. Sato.
Foreground-aware
stylization and consensus pseudo-labeling for
domain adaptation of ﬁrst-person hand seg-
mentation.
IEEE Access, 9:94644–94655,
2021.

[42] M. Qi, E. Remelli, M. Salzmann, and
P. Fua. Unsupervised domain adaptation
with temporal-consistent self-training for 3d
hand-object joint reconstruction.
CoRR,
abs/2012.11260, 2020.

[43] C. Qian, X. Sun, Y. Wei, X. Tang, and J.
Sun. Realtime and robust hand tracking
from depth. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1106–1113, 2014.
[44] Y. Qin, Y.-H. Wu, S. Liu, H. Jiang, R.
Yang, Y. Fu, and X. Wang. DexMV: Imita-
tion learning for dexterous manipulation from
human videos. CoRR, abs/2108.05877, 2021.
[45] P. Ren, H. Sun, Q. Qi, J. Wang, and W.
SRN: stacked regression network

Huang.

for real-time 3D hand pose estimation.
In
Proceedings of the British Machine Vision
Conference (BMVC), 2019.

[46] J. Romero, D. Tzionas, and M. J. Black.
Embodied hands: Modeling and capturing
hands and bodies together. ACM Transac-
tions on Graphics (ToG), 36(6):245:1–245:17,
2017.

[47] N. Santavas,

I. Kansizoglou, L. Bampis,
E. G. Karakasis, and A. Gasteratos. Atten-
tion! A lightweight 2d hand pose estimation
approach. CoRR, abs/2001.08047, 2020.
[48] F. Sener, D. Chatterjee, D. Shelepov, K.
He, D. Singhania, R. Wang, and A. Yao.
Assembly101: A large-scale multi-view video
dataset for understanding procedural activi-
ties. CoRR, abs/2203.14712, 2022.

[49] T. Simon, H. Joo, I. Matthews, and Y.
Sheikh. Hand keypoint detection in single
images using multiview bootstrapping.
In
Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition
(CVPR), pages 4645–4653, 2017.

[50] A. Spurr, A. Dahiya, X. Wang, X. Zhang,
and O. Hilliges.
Self-supervised 3d hand
pose estimation from monocular RGB via
contrastive learning.
In Proceedings of
the IEEE/CVF International Conference on
Computer Vision (ICCV), pages 11210–
11219, 2021.

[51] A. Spurr, U. Iqbal, P. Molchanov, O. Hilliges,
and J. Kautz. Weakly supervised 3D
hand pose estimation via biomechanical con-
straints. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages
211–228, 2020.

[52] A. Spurr, P. Molchanov, U. Iqbal, J. Kautz,
and O. Hilliges. Adversarial motion mod-
elling helps semi-supervised hand pose esti-
mation. CoRR, abs/2106.05954, 2021.
[53] A. Spurr, J. Song, S. Park, and O. Hilliges.
Cross-modal deep variational hand pose esti-
mation.
In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern
Recognition (CVPR), pages 89–98, 2018.
[54] S. Sridhar, F. Mueller, M. Zollhoefer, D.
Casas, A. Oulasvirta, and C. Theobalt. Real-
time joint tracking of a hand manipulating
an object from RGB-D input. In Proceedings
of the European Conference on Computer
Vision (ECCV), pages 294–310, 2016.

11

estimation. In Proceedings of the ACM Inter-
national Conference on Multimedia (MM),
pages 2076–2084, 2020.

[64] C. Zimmermann, M. Argus, and T. Brox.
Contrastive representation learning for hand
shape estimation. CoRR, abs/2106.04324,
2021.

[65] C. Zimmermann and T. Brox.

Learn-
ing to estimate 3D hand pose from sin-
gle RGB images.
In Proceedings of
the IEEE/CVF International Conference on
Computer Vision (ICCV), pages 4913–4921,
2017.

[66] C. Zimmermann, D. Ceylan, J. Yang, B. Rus-
sell, M. J. Argus, and T. Brox. FreiHAND:
A dataset for markerless capture of hand
pose and shape from single RGB images. In
Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV),
pages 813–822, 2019.

[55] O. Taheri, N. Ghorbani, M. J. Black, and D.
Tzionas. GRAB: A dataset of whole-body
human grasping of objects.
In Proceedings
of the European Conference on Computer
Vision (ECCV), pages 581–600, 2020.
[56] D. Tang, H. J. Chang, A. Tejani, and T.-
K. Kim. Latent regression forest: Structured
estimation of 3d articulated hand posture.
In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition
(CVPR), pages 3786–3793, 2014.

[57] B. Tekin, F. Bogo, and M. Pollefeys. H+O:
uniﬁed egocentric recognition of 3D hand-
object poses and interactions. In Proceedings
of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR),
pages 4511–4520, 2019.

[58] J. Tompson, M. Stein, Y. LeCun, and K.
Perlin. Real-time continuous pose recov-
ery of human hands using convolutional
networks. ACM Transactions on Graphics
(ToG), 33(5):169:1–169:10, 2014.

[59] M.-Y. Wu, P.-W. Ting, Y.-H. Tang, E. T.
Chou, and L.-C. Fu. Hand pose estimation in
object-interaction based on deep learning for
virtual reality applications. Journal of Visual
Communication and Image Representation,
70:102802, 04 2020.

[60] F. Xiong, B. Zhang, Y. Xiao, Z. Cao, T. Yu,
J. T. Zhou, and J. Yuan. A2J: anchor-to-
joint regression network for 3D articulated
pose estimation from a single depth image. In
Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV),
pages 793–802, 2019.

[61] L. Yang, S. Chen, and A. Yao.

Semi-
hand: Semi-supervised hand pose estima-
tion with consistency.
In Proceedings of
the IEEE/CVF International Conference on
Computer Vision (ICCV), pages 11364–
11373, 2021.

[62] S. Yuan, Q. Ye, B. Stenger, S. Jain, and T.-
K. Kim. BigHand2.2M benchmark: Hand
pose dataset and state of the art analysis.
In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition
(CVPR), pages 2605–2613, 2017.

[63] Y. Zhang, L. Chen, Y. Liu, W. Zheng, and
J. Yong. Adaptive wasserstein hourglass
for weakly supervised RGB 3d hand pose

12

