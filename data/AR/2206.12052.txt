IEEE TRANSACTIONS

1

Learning the policy for mixed electric platoon
control of automated and human-driven vehicles at
signalized intersection: a random search approach

Xia Jiang, Jian Zhang, Member, IEEE, Xiaoyu Shi, and Jian Cheng

2
2
0
2

n
u
J

4
2

]

Y
S
.
s
s
e
e
[

1
v
2
5
0
2
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—The upgrading and updating of vehicles have ac-
celerated in the past decades. Out of the need for environ-
mental friendliness and intelligence, electric vehicles (EVs) and
connected and automated vehicles (CAVs) have become new
components of transportation systems. This paper develops a
reinforcement learning framework to implement adaptive control
for an electric platoon composed of CAVs and human-driven
vehicles (HDVs) at a signalized intersection. Firstly, a Markov
is proposed to describe the
Decision Process (MDP) model
decision process of the mixed platoon. Novel state representation
and reward function are designed for the model to consider
the behavior of the whole platoon. Secondly, in order to deal
with the delayed reward, an Augmented Random Search (ARS)
algorithm is proposed. The control policy learned by the agent
can guide the longitudinal motion of the CAV, which serves as
the leader of the platoon. Finally, a series of simulations are
carried out in simulation suite SUMO. Compared with several
state-of-the-art (SOTA) reinforcement learning approaches, the
proposed method can obtain a higher reward. Meanwhile, the
simulation results demonstrate the effectiveness of the delay
reward, which is designed to outperform distributed reward
mechanism. Compared with normal car-following behavior, the
sensitivity analysis reveals that the energy can be saved to
different extends (39.27%-82.51%) by adjusting the relative
importance of the optimization goal. On the premise that travel
delay is not sacriﬁced, the proposed control method can save up
to 53.64% electric energy.

Index Terms—Connected and automated vehicles, reinforce-
ment learning, platoon control, signalized intersection, random
search.

I. INTRODUCTION

T HE advancements in artiﬁcial intelligence (AI), com-

munication technologies, and vehicular technology have
promoted the automation and electriﬁcation of vehicles in
recent years. Automation nurtures the creation of connected
and automated vehicles (CAVs), which is widely accepted
as an effective way to improve trafﬁc conditions [1]–[5].
One problem associated with the application of CAVs to real
world is that the design of the control strategy is uncertain,
whereas the efﬁcient functioning of the CAVs is based on

Manuscript received December 28, 2021. (Corresponding author is Jian

Zhang.)
X.

Jiang and X Shi was with the School of Transportation,
sum-

Jiangsu 210096, China (e-mail:

Southeast University, Nanjing,
mer142857.jiang@gmail.com;2373497459@qq.com).

J. Zhang is with the School of Transportation, Southeast University, Nan-
jing, Jiangsu 210096, China and the School of Engineering, Tibet University,
Lhasa, Tibet 850000, China (e-mail: jianzhang@seu.edu.cn)

J. Cheng is with Nanjing Les Information Technology Co.Ltd, Nanjing,

Jiangsu 210096, China(e-mail: cheng j@les.cn)

their decision and control modules. The task is especially
challenged in urban intersection scenarios, which are viewed
as the bottlenecks of urban trafﬁc, as they are the places
where trafﬁc ﬂows with different directions converge. Since
the operation of vehicles can be interrupted by trafﬁc signals,
the control law of CAVs at signalized intersections is crucial,
when it can determine the trafﬁc performance in such urban
scenarios. Practical control approaches of CAVs have shown
that the travel efﬁciency, energy consumption, and safety can
be improved at intersections [6]–[8], so the signiﬁcance of
CAV-related research with regard to intersections is revealed.
In terms of research topic, a majority of studies focus
on a trafﬁc environment with 100% CAV penetration rate,
in which the conventional trafﬁc signals can be eliminated,
because the information of vehicular trafﬁc can be completely
obtained in real time [9] and the vehicles can be controlled
in a centralized manner [10]–[12]. Despite the fact that a
pure CAV environment can create an unprecedented intelligent
transportation system (ITS),
there is a general consensus
among researchers about the inevitability of the coexistence
of CAVs and human-driven vehicles (HDVs) [13]–[18]. Given
this, controlling individual CAV at intersections becomes a
promising way to exploit the potential advantages that CAVs
can bring to urban transportation system. Having CAVs under
control by embedded controller, relative indicators, such as
travel time, energy consumption, and trafﬁc safety, can be
optimized for individual vehicle. [19]–[21]

On the other side, the communication ability of CAVs makes
it possible to implement cooperative control of several indi-
vidual CAVs in a mixed trafﬁc environment, which is usually
achieved by platooning to extend the beneﬁcial effect from
vehicle level to platoon level [14], [22]–[26]. The cooperative
control approaches are capable of generating smoother trajec-
tories and energy-saving speed proﬁles for CAVs. However,
the application of automated driving system and vehicle-to-
infrastructure (V2I) communication should not only enable the
intelligent vehicles to make better decisions and enhance its
own functionality [27], but also improve the overall trafﬁc
performance,
instead of sacriﬁcing the mobility or energy
consumption of other HDVs. Whereas the operation of CAVs
may has a direct impact on other HDVs, and sometimes this
inﬂuence would interfere with the normal running of those
controlled by human drivers [28], leveraging CAVs in mixed
trafﬁc condition to avert negative impact and promote the
performance of HDVs is crucial, and this topic with mixed
trafﬁc is rarely discussed in urban intersection scenario. Zhao

 
 
 
 
 
 
IEEE TRANSACTIONS

2

et al. [29] proposed a framework that considers a mixed
platoon of CAVs and HDVs at a signalized intersection to
reduce the holistic energy consumption. Chen et al. [30]
explicitly made a deﬁnition of mixed platoon and formulated
a control framework.

In addition to research topic, the formulation of control laws
for CAV-related control problems is also important, usually
obtaining by Model Predictive Control (MPC) [29], [31],
[32] or Dynamic Programming (DP) [33], [34], which are
challenged with computation complexity. Similarly, the afore-
mentioned mixed platoon control framework are all based on a
perspective of optimal control theory by expressly embodying
cost functions, constraints, and solving algorithms. It is also
pointed out that these model-based methods need to simplify
the dynamics of the environment or decompose the control
problem into several sub-processes [35]. Accordingly, the lack
of accuracy and generalization ability of the methods can
impose an adverse impact on their practical application. To
achieve cost-efﬁcient in terms of computation time, some rule-
based approaches are studied [36]–[38], but the optimality
can not be ensured. With the intent to implement adaptive
control with real-time ability, more competent approaches are
supposed to be developed.

The Deep Reinforcement Learning (DRL) algorithms re-
cently brought about new solutions for the vehicular control
problem [39]. Beneﬁting from the strong ﬁtting ability of deep
neural networks (DNNs), the DRL technique has the potential
to approximate the optimal control process. In the DRL theory,
an agent can choose actions according to the observed states
so as to maximize its expected accumulated reward. For the
general trafﬁc control problems, the reward can be energy
trafﬁc delay, or the combination of relevant
consumption,
indicators. Based on the DRL algorithms, a few frameworks
have been proposed in recent years to control CAVs at the
proximity of signalized intersections. Shi et al. [40] applied
Q-learning to improve the fuel consumption efﬁciency of a
connected vehicle at a signalized intersection. An improved
version of Q-learning based control framework, integrating
with a deep Q network (DQN), was developed by Mousa et al.
[20]. However, as one of the value-based DQL algorithms, the
DQN approach cannot deal with the problems with continuous
action space. Therefore, they directly took discrete velocity
change rate as the action space, which can result in a local
optimum solution. With the application of policy-based algo-
rithms, the aforementioned problems can be tackled. Guo et
al. [35] utilized a deep deterministic policy gradient (DDPG)
algorithm to implement continuous longitudinal control of a
CAV. Similarly, Zhou et al. [6] also trained DDPG agents to
develop an efﬁcient and energy-saving car following strategy.
Furthermore, based on DDPG algorithm, they demonstrated
that the method could improve travel efﬁciency by reducing
the negative impact of trafﬁc oscillations [41]. Wegner et al.
[42] and Zhang et al. [43] had explored the energy-saving
potential of electric CAV at urban signalized intersections by
employing a twin-delayed deep deterministic policy gradient
(TD3) agent, which is trained to control itself adaptively.

Nevertheless, there are some drawbacks that do exist among
the aforementioned policy-based DRL approaches. Firstly,

they all used stepwise reward signals to facilitate the learning
process, and the policy learned by the agent in this situation
cannot be equivalent to global optimum. For example, the
framework put forward by Guo et al. used stepwise travel
distance to surrogate the total travel time of a CAV in one
episode [41], while the value of travel
time can only be
acquired after the CAV crosses the intersection. Although the
agent can obtain reward signal in distributed form for each
simulation step, the combination of the travel distance of all
the steps is not tantamount to the total travel time. Intuitively,
the agent may encounter a red light if it chooses the action in
such a greedy way (i.e., aiming to maximize its stepwise travel
time). Secondly, the previous DRL-based studies focus on the
performance of a single CAV and ignore the integrated control
of several vehicles. The CAVs can produce selﬁsh policies
in an ”ego-efﬁcient” way, which cannot guarantee improved
performance of mixed platoons. Finally,
is known that
algorithms like DDPG are highly sensitive to hyperparameter
choices [44]. The traditional DRL approaches can also suffer
from the sample efﬁciency problem, especially for the delayed
reward situation. Therefore, a more effective method should
be built to promote the application of reinforcement learning
in this domain.

it

To address the above issues, this article develops a novel
reinforcement learning control framework for CAVs at signal-
ized intersections. A delayed reward Markov Decision Process
(MDP) is formulated to describe the mathematical model of
the control task in terms of the longitudinal motion of the
platoon. The state of the MDP considers the leading CAV
and its following HDVs in a mixed platoon. With regard
to the reward signal, this paper deﬁne that it can only be
obtained when the platoon crosses the junction, and simulation
studies would manifest the beneﬁts of the setting. With the
intent to deal with the delayed reward, an augmented random
search (ARS) algorithm is proposed for the agent learning the
control policy. The learning and evaluation of the framework
are carried out in SUMO platform [45], which can demonstrate
the effectiveness of the proposed method through microscopic
trafﬁc simulations.

Moreover, this paper takes the electric mixed platoons as
research objects and make effort to optimize its electricity
consumption. The starting point of electric vehicles is based
on following reasons: (1) The electriﬁcation of vehicles shows
great promise to achieve sustainable trafﬁc development [46],
as the carbon emissions and air pollution caused by the
transportation system is still rising [47]. (2) Due to the
regenerative braking energy of electric vehicles (EVs), the
control of electric CAVs is more challenging than traditional
the EVs show a higher
gasoline cars. At
potential of energy conversion efﬁciency at low load range
[48]. In this case,
the research of electric mixed platoon
would have realistic meaning for a electric and intelligent road
transportation system in the near future.

the same time,

The remainder of this paper is structured as follows. Section
II introduces the preliminaries of DRL and the car-following
model of HDVs. Section III provides the MDP formulation of
the platoon-based control strategy. Section IV proposes the
ARS algorithm to implement the self-learning mechanism.

IEEE TRANSACTIONS

3

Section V reports a series of simulations carried out in the
SUMO software and makes a comparison study with several
state-of-the-art (SOTA) methods. Finally, some concluding
remarks are presented in Section VI.

II. PRELIMINARY

A. Background of DRL

Reinforcement learning is an important branch of machine
learning. The object to be controlled in reinforcement learning
is seen as an agent, and the learning process can be promoted
by a series of agent-environment interactions. One complete
play of the agent interacting with the environment is called
as an episode. Generally, in step t of an episode, an agent
can observe a state st, which is usually a feedback by
the environment. Then, the agent can conduct an action at
according to its policy π(at|st). As a result, the agent can
obtain a reward signal rt, which is usually the representation
of its optimization goal. Note that rt can be sparse when the
reward can only be acquired in the terminal stage (i.e. with
delayed rewards).

The process can be basically given by the MDP, which is
deﬁned as a ﬁve-tuple (S, A, R, P, γ). S, A, and R denote
the state space, the action space, and the reward space of
the agent, respectively. For each timestep t, we have st ∈ S,
at ∈ A, and rt ∈ R. Meanwhile, P speciﬁes the state transition
probability function: S × S × A → [0, ∞), which can emit the
probability density of the next state st+1 ∈ S given the current
state st ∈ S and action at ∈ A. Moreover, γ is a discount
factor that measures the relative importance of the current
reward and future reward. By interacting with the environment
continuously, the agent aims to ﬁnd an optimum policy that
can maximize the expected sum of discounted future rewards
t = rt+1 + γrt+2 + γ2rt+3 + ... = (cid:80)∞
rγ
k=0 γkrt+k. For
any policy π, the state-action value function is Qπ(s, a) =
E[rγ
t |st = s, at = a, π], where at+k ∼ π(·|st+k) for all
at+k and st+k for k ∈ [t + 1, ∞). Meanwhile, the state value
function is vπ(s) = E[rγ
t |st = s, π]. According to the Bellman
equation, we have vπ(s) = (cid:80)
a∈A π(a|s)Qπ(s, a). Finally, let
Π represent the set of all possible policies, and the optimal
policy π∗ can be deﬁned as:

π∗ ∈ arg max
π∈Π

E[rγ

t |π]

(1)

As a result, the agent can always select the optimal action
following the optimal policy π∗.

The DRL technique makes use of deep learning to promote
the traditional reinforcement learning approaches. Suppose the
set of parameters of the utilized neural network is θ, we can
parameterize the state-action value function by Q(s, a|θ) ≈
Q∗(s, a) for the value-based DRL algorithms, in order to
approximate the optimal state-action value function Q∗(s, a).
As for the policy-based DRL algorithms, the policy is directly
parameterized as π(s, a|θ). Hereafter, the learning process will
adjust the set of parameters θ according to the ”trial-and-error”
mechanism to search for a suitable policy.

B. Car Following Model of HDVs

In this paper, we adopt the Intelligent Driver Model (IDM)
to simulate the driving behavior of human drivers [49],
whereas the model
is widely used in microscopic trafﬁc
simulations [50]–[52]. The acceleration of the nth vehicle at
time t is related to its current velocity, time headway, and the
velocity of the front vehicle. The mathematical form of IDM
is deﬁned by Equation 2 and Equation 3.

an(t) =

dvn(t)
dt

= a0(1 − (

vn(t)
v0

) − (

s∗
n(t)
sn(t)

)2)

s∗
n(t) = s0 + T vn(t) +

vn(t)∆v(t)
2a0b

√

(2)

(3)

where, a0 and v0 are the maximal acceleration and the
expected velocity of the vehicle in free ﬂow; vn(t) denotes
the velocity of vehicle n at time t; s∗
n(t) and sn(t) are the
expected headway and the real headway between the vehicle
and its front vehicle, respectively; s0 represents the minimal
headway; T denotes the safe time headway; ∆v(t) denotes the
velocity difference between the vehicle and its leading vehicle.
Finally, b denotes an acceptable comfort-related deceleration.

III. MARKOV DECISION PROCESS FOR THE PROBLEM

A. Problem Description

Fig. 1: The illustration of the studied scenario.

As shown in Figure 1, this study mainly focuses on a ”1+n”
form of the mixed platoon, consisting of one leading CAV and
n following HDVs. We call the electric CAV of the platoon
”ego CAV”, while the platoon led by the ego CAV is called
”ego platoon”. Besides the ego platoon, there are some other
HDVs travel on the road, and this can make the simulation
get close to the real trafﬁc situation. In order to simplify
the problem without losing any generality, we make some
assumptions as below:

1) With the support of V2I communication, the ego CAV
can obtain the Signal Phase and Timing (SPaT) information
of the ﬁxed-timing trafﬁc signal.

2) The ego CAV can get the position, velocity, and accel-

eration of itself by vehicular operation system.

(cid:40)

δ =

T rue,
F alse,

if xL(t) − x(t) ≤ χx;
otherwise.

4

(9)

The calculation of ∆x, ∆v, and ∆a according to the value

δ are expressed as:

(cid:40)

∆x =

xL(t) − x(t),
χx,

if δ;
otherwise.

∆v =

(cid:40)

vL(t) − v(t),
χv,

if δ;
otherwise.

(cid:40)

∆a =

aL(t) − a(t),
χa,

if δ;
otherwise.

(10)

(11)

(12)

where, χv and χa are the predeﬁned default value of the two
variables. In this paper, χx is set to 500m, which means that
the vehicle 500 meters away from the ego CAV will not affect
its driving. Moreover, χv and χa are set to 13.88m/s and
7.5m/s2.

As for the signal-related state, RT (t) in Equation 8 de-
notes the remaining time of the current phase for the ﬁrst
downstream trafﬁc signal, and this value can be retrieved in a
communication environment. Furthermore, Es(t) denotes the
one-hot encoding of the current phase of the trafﬁc signal. The
encoding process is illustrated in Figure 2. The phase diagram
shows the signal phase used in this study, and yellow light is
added between two adjacent phases. If one phase is activated
by the trafﬁc signal (i.e., the phase with red box in Figure 2),
the corresponding element in the encoding vector will be set
to 1, while other elements are all set to 0.

IEEE TRANSACTIONS

3) The positions and velocities of the HDVs belonging to the
ego platoon can be obtained by the ego CAV. At the same time,
the ego CAV can also get these data of its leading vehicle if
the position of the leading vehicle is in a predeﬁned range. The
assumption can usually be achieved by the vehicle-to-vehicle
(V2V) communication, roadside units, or the perception ability
of the CAV [29], [30].

Since the operation of the mixed platoon can be interrupted
by other HDVs or trafﬁc signals, the goal of the platoon is
to reduce the overall delay and electric energy consumption.
We basically study the longitudinal motion of the vehicles,
because the unexpected lane changing may interfere with
normal operations of other HDVs, especially in the vicinity
of signalized intersections. Although the scenario presented in
Figure 1 is a single-lane environment, the proposed framework
can be conducted for CAVs in a decentralized fashion for
multi-lane scenarios. Accordingly, an effective control law will
generate a speed proﬁle for the leading CAV and consider the
motion of the subsequent HDVs. In this case, unnecessary
stops and oscillations can be avoided to achieve the energy-
saving goal.

B. Speciﬁcation of the MDP

The elements in the MDP model,

including S, A, and
R should be speciﬁed to apply the DRL framework. For a
”1+n” mixed platoon, the formation of the three factors can
be deﬁned as follows.

1) State: State is the description of the agent in current
situation. All of the vehicles within the mixed platoon should
be taken into account as part of the state. Meanwhile, the
potential leading vehicle can be taken into account, as the
ego CAV should keep a safe gap and estimate the trafﬁc
ahead. With the intent
to reduce unnecessary stop-and-go
operation, the agent also needs the SPaT information of the
ﬁrst downstream trafﬁc signal. Therefore, let sC
t , and
sS
t be the CAV-related part, the HDVs-related part, the leading-
vehicle-related part, and the signal-related part of the state, the
state can be expressed as:

t , sH

t , sL

st = (sC

t , sH

t , sL

t , sS

t )T

(4)

In this case, the details of each part of the state are shown

as follows:

sC
t = (d(t), v(t))

sH
t = (x1(t), v1(t), x2(t), v2(t), . . . , xn(t), vn(t))

sH
t = (∆x(t), ∆v(t), ∆a(t))

sH
t = (RT (t), Es(t))

(5)

(6)

(7)

(8)

where, d(t) is the distance between the ego CAV and the stop
line of the ﬁrst downstream intersection at time t; v(t) is the
velocity of the ego CAV at time t; xi(t) and vi(t) are the lane
position and speed of HDV i for i from 1 to n. For the third
item in Equation 4, we set a predeﬁned threshold χx to judge
if there is a leading vehicle in front of the platoon. Let L be
the index of the potential leading vehicle, we set a boolean
variable δ to identify the existence of the potential preceding
vehicle:

Fig. 2: The phase diagram and its one-hot encoding.

2) Action: Due to the maneuverability of the system, the
action is to change the acceleration of the ego CAV. Hence,
the action space is constrained by the dynamics of the vehicle:
at ∈ [amin, amax], where amin and amax are the maximal
deceleration and acceleration of the vehicle. However, it is
problematic to take the acceleration as the action directly. On
the one hand, irrational accelerations will lead to unsafe op-
erations like rear-end accidents, and this kind of phenomenon
can occur very often during the training process; on the other
hand, the speed of the vehicle may exceed the road speed limit

IEEE TRANSACTIONS

5

with the effect of the action. Consequently, the modiﬁed action
is stipulated as:

at = min( ˜at, aIDM (t))

(13)

where, ˜at denotes the original acceleration value output by
the DRL algorithm; aIDM (t) is the acceleration calculated by
IDM. Equation 13 makes the acceleration of the ego CAV be
kept in a safe range.

The velocity change of the ego CAV is deﬁned as below to

meet the speed limit Vmax:

v(t) = max(min(Vmax, v(t − 1) + at), 0)

(14)

where, v(t − 1) is the speed of the ego CAV in last timestep.
3) Reward: The optimization goal, including total energy
consumption and travel delay, can only be calculated when the
vehicles have crossed the signalized intersection. Distributing
the delayed reward to each step in an episode is known as
the temporal Credit Assignment Problem (CAP) [53], which
is hard to deal with. The previous studies took stepwise energy
consumption and travel distance to serve as a distributed proxy
of the two parts of the delayed reward [35], [43]. Nevertheless,
the cumulative travel distance cannot indicate the delay of the
vehicles accurately. A more intuitive way is using the delayed
reward, which can directly reﬂect the optimization goal. In
this case, the reward is non-Markovian. In this study, we will
show that our algorithm can commendably solve the CAP and
train the agent. The reward function is deﬁned as:

(cid:40)(cid:80)n

rt =

0,

i=0 −ω1ei − ω2di,

if t = tf inal
otherwise.

(15)

where, ei denotes the total energy consumption of vehicle
i; di denotes the delay of vehicle i. Note that the vehicle
with i = 0 here represents the ego CAV. Meanwhile, ω1
and ω2 are weighting parameters that measure the relative
importance of mobility indicator and energy indicator. Finally,
tf inal speciﬁes the ﬁnale of an episode. It is the time when
the last HDV in the ego platoon crosses the intersection.

In Equation 15, ei is calculated by a series of records in
the whole episode. This study utilizes an energy model with
energy brake-recovery mechanism embedded in SUMO to cal-
culate the instantaneous electric consumption [54]. Note that
any other energy model can be used owing to the generality
of the proposed framework, even if a simple indicator that
derived from the difference of the battery. The instantaneous
energy consumption is calculated for each vehicle within the
platoon in each step. Finally, ei is calculated when vehicle i
enter the intersection. Similarly, di is expressed as:

di = ti

f − t0 −

L
Vmax

(16)

where, tf is the time when vehicle i crosses the stop line of
the junction;t0 is the initial time; L denotes the length of the
entrance lane where the platoon locates.

IV. AUGMENTED RANDOM SEARCH

The purpose of the algorithm is to directly search a policy
in continuous action space, while the obtained policy can

approximate the optimal policy π∗ in Equation 1. As the
transition dynamics is unknown in most cases, model-free
reinforcement learning algorithms are usually deployed. It is
pointed out that many model-free DRL methods need too
much data to search a proper optimization direction, and they
can be very complicated without robustness [44]. Considering
the practicability, we develop a ARS algorithm in this paper
to search the policy in a black-box way. Being compared with
the gradient-based DRL methods, the black-box optimization
approach can achieve sample efﬁciency and have an advantage
in cases with long action sequences and delayed reward [55].
In the context of DRL, the policy is usually parameterized
by a set of parameters θ, which is supposed to be trained
in training process. The ARS utilized a linear policy with
parameter set θ instead of DNNs like most DRL algorithms.
Note that throughout the rest of the paper we use πθ to denote
the ARS-based policy with parameter set θ. Let the dimension
of the state in Equation 4 be p. The parameter set θ is a p × n
matrix, while the dimension of action is represented as n.

The update increment ∆θ of θ follows:

∆θ =

r(πθ+υµ, ξ1) − r(πθ−υµ, ξ2)
υ

(17)

where, ξ1 and ξ2 are random variables that encode the ran-
domness of the environment; υ is a positive real number that
denotes the standard deviation of the exploration noise; µ
denotes a vector with zero mean Gaussian distribution.

The basic idea of ARS is to randomly adds some tiny
variables to the parameter θ along with the negative value of
the corresponding value. After the perturbation, the variables
with a higher reward have a bigger inﬂuence on the adjustment
of θ. This process is shown in Figure 3. The directions with
red crosses represent the variables with relatively low rewards,
so they are eliminated when calculating the ﬁnal updating
direction. In particular, the red dashes represent the update
direction weighted by the rest of the variables.

Fig. 3: The sketch for the idea of ARS.

More speciﬁcally, The pseudocode of the proposed ARS is
shown in Algorithm 1. Three tricks are adopted in the ARS
algorithm to enhance its performance [44]:

1) Scaling by the standard deviation: During the training
process across the iterations, there will exist a large variation
in the collected rewards record. In particular, the circumstance
brings about difﬁculties for choosing a proper step size α. In
each iteration, 2K rewards are recorded. A standard deviation
(cid:15)R will be calculated and is used to scale the update step (see
line 5 in Algorithm 1).

IEEE TRANSACTIONS

6

vehicles and pedestrians. The value of simulation in SUMO
can be retrieved and changed through the ”TraCI” interface by
other program languages. In this study, a signalized intersec-
tion is built in SUMO environment, and the scenario is similar
to that shown in Figure 1.

A. Simulation Settings

The signal phases are shown in Figure 2. As a ﬁxed-
timing trafﬁc signal, the last time for each phase is 30s,
while a 3s yellow phase is inserted for every phase changing.
Under the premise of comprehensive consideration of reality
and generality, the other related parameters for simulation
conﬁguration are presented in Table I. Before the learning
process of each episode, a pre-loading procedure is carried
out. More precisely, the trafﬁc volume is loaded for time
tp (sampled from a uniform distribution) before the ego
platoon enters the road, aiming at generating more dynamic
trafﬁc scenarios. Meanwhile, after a series of simulations, the
hyperparameters of ARS are tuned manually. The standard
deviation of parameter noise υ is set to 0.2; the number of
directions sampled per iteration is set to 32. Note that the
weighting parameters are set as: ω1 = 6, ω2 = 1 if no special
explanation is provided. The sensitivity analysis of the two
parameters are presented in the subsequent subsection.

TABLE I: The parameter setting of simulations

Item

Maximum acceleration of vehicles amax
Minimum acceleration of vehicles amin
Road speed limit Vmax
The length of the lane L
Safe time headway T in IDM
Acceptable comfort-related deceleration b in IDM
Hourly trafﬁc volume
Pre-loading time tp

Value

3.0
-4.5
13.88
500
1
-2.8
400
U(180, 220)

Unit

m/s2
m/s2
m/s
m
s
m/s2
veh
s

B. Training Performance

Algorithm 1 ARS for Mixed Platoon Control
Hyperparameters: step-size α, number of directions sampled
per iteration K, noise υ, number of top-performing directions
to use b(b < K)
Initialize: θ0 = 0 ∈ Rp×n, σ0 = 0 ∈ R, Σ0 = In ∈ R,
j = 0

1: while end condition not satisﬁed do
2:

Sample µ1, µ2, . . . , µK in Rp×n with i.i.d standard

normal entries.

3:

Collect 2K episodes of horizon H and their corre-

sponding rewards using the 2k policies in SUMO:

(cid:40)

πj,k,+(x) = (θj + υµk)diag(Σj)− 1
πj,k,−(x) = (θj − υµk)diag(Σj)− 1

2 (x − σj)
2 (x − σj)

4:

5:

6:

for k ∈ 1, 2, . . . , K
the
Sort

directions

to
max r(πj,k,+, πj,k,−). Let µ(k) be the k−th largest
direction, and by πj,(k),+, πj,(k),− the corresponding
policies.

according

µk

Update θ by step ((cid:15)R denotes the standard deviation

of the 2b rewards):
(cid:80)b
θj+1 = θj + α
b(cid:15)R

k=1[r(πj,k,+ − πj,k,−]µ(k)

Set σj+1, Σj+1 to be the mean and covariance value
of the 2KH(j + 1) states encountered from the start
of training.

j ← j + 1

7:
8: end while

2) States normalization: The purpose of normalization is to
eliminate the inﬂuence of dimensional inconsistency of differ-
ent elements in state vectors. For the parametric linear policy, it
can promote non-isotropic explorations in the parameter space.
For a perturbation direction µ, there is:
(θ+υµ)diag(Σ)− 1
where, ´θ = θdiag(Σ)− 1

2 (x−σ) = (´θ+υµdiag(Σ)− 1

2 )(x−σ) (18)

2

3) Using top-performing directions: The perturbation direc-
tion µ is weighted by the difference of two opposed rewards
r(πj, k, +) and r(πj, k, −) (see line 3 in Algorithm 1).
Without this trick, the update steps push θ in the direction
of µk. However, using top-performing directions can order de-
creasingly the directions µk by max{r(πj, k, +), r(πj, k, −)}.
Finally, only the top b directions are utilized to update the
policy parameters (see line 5 in Algorithm 1).

During the iterations of training, only the total reward of
an episode is used to evaluate the performance of a series of
actions, so ARS can deal with maximally sparse and delayed
rewards and avoid the difﬁculties produced by CAP. The
feature makes it suitable to solve the platoon control problems
with delayed reward conﬁgurations. Without the training of
DNN, ARS can save much inference time, and it is promising
to deploy such a computation-efﬁciency framework in real
world.

V. SIMULATION ANALYSIS

As one of the most popular open-source trafﬁc simulator,
SUMO allows the modelling of the microscopic behavior of

Fig. 4: The smoothed episode rewards from seven rounds of
training.

IEEE TRANSACTIONS

7

Firstly, with the intent to show the robustness of the ARS
approach, we conduct 7 rounds of independent training with
different random seeds. Figure 4 illustrates the training results.
Owing to the noise rewards for different episode, a moving
average function is applied to smooth the tendency: Rk ←
0.8Rk−1 +0.2Rk, where k denotes the k-th episode. Although
the ﬂuctuation range of each round can be different, they can
all converge to the same result with about -1250 reward.

Secondly, we make a comparison study with other SOTA
methods,
including Proximal Policy Optimization (PPO),
DDPG, and DQN. For each algorithm, the hyperparameters are
tuned manually through several simulations, and the training
results from seven independent are aggregated to obtain the
ﬁnal result to reduce the effect of randomness. Note that the
action space of the DQN is set to a 16-length vector, which
varies from amin to amax with the step of 0.5m/s2. Taking
the scenario with ”1+3” mixed platoon as an example, the
training processes are shown in Figure 5. It can be seen that it
is hard to train the agent for the other three SOTA algorithms
with the delayed reward cases. However, the reward of the
ARS agent can converge to a higher value compared with the
other approaches.

Fig. 5: The comparison for different algorithms of the training
process.

C. Exploring the Impact of Reward Conﬁguration

To investigate the inﬂuence of reward settings, we compare
the cases with episodic reward (ER) and distributed reward
(DR) settings. In DR setting, the reward is calculated step by
step according to the stepwise sum of energy consumption
and travel distance of the platoon. Accordingly, we train the
ARS agent ﬁve times, and collect ﬁve episodes of reward for
each trained agent. More precisely, 25 groups of simulations
are carried out to record the data for each reward setting. The
results are presented in Figure 6. Whether in terms of travel
delay or electric consumption, the ER setting can outperform
the DR setting. The agents with DR show high variance with
respect of energy consumption indicator, and this illuminates
the instability of this kind of conﬁgurations.

Fig. 6: The comparison for different reward settings. (a) Total
delay. (b) Total energy consumption.

TABLE II: Numerical results for different algorithm and
reward settings

Method

Delay per Vehicle (s) Total Electricity (Wh)

PPO with DR
PPO with ER
ARS with DR
ARS with ER
IDM

143.03
241.58
239.78
165.22
57.98

222.77
246.55
156.85
104.81
612.42

Similar studies can be conducted for PPO algorithm. Ta-
ble II shows the mean value of the indicators, deriving from
25 episodes of simulations. IDM is introduced to serve as a
baseline. In this case, the ego CAV is controlled by the IDM,
which can represent the general car following scenario.

Table II demonstrates that the ER-based ARS can reduce
energy consumption to the maximum extent. The DR-based
PPO has a similar performance with the ER-based ARS in
terms of total delay. However, ARS can reduce the electric
energy consumption by 52.95% compared with the DR-based
PPO for a ”1+3” platoon on average. Inevitably, the optimiza-
tion on energy will lead to the sacriﬁce of mobility [35]. With
the setting of ω1 = 6 and ω2 = 1, 82.89% energy is saved by
the adaptive control implemented by ARS algorithm compared
with IDM. The agent in this case behaves toward an extreme
energy efﬁciency direction. Nevertheless, we will conduct a
sensitivity analysis in the following subsection. The analysis
can reveal that the agent can reduce energy consumption with
almost no sacriﬁce of mobility.

D. Performance for Different Platoon Size

Figure 7 shows the smoothed training curve for the scenarios
with different platoon sizes. It can be concluded that
the
cases with different platoon sizes can be optimized, and the
results can converge to different values. The more HDVs are
considered by the agent, the higher optimization rate can be
observed. As a result, the framework has the potential to
normally extend to multi-vehicle systems.

More speciﬁcally, we use the trained ARS agents with
different platoon size conﬁgurations to run the simulations
for evaluation. We make comparison study with several other

IEEE TRANSACTIONS

8

ARS algorithm still achieves the optimal performance in
terms of energy-related indicator, while the change of delay
indicator is not signiﬁcant. In addition, the consumed energy
and time decline slightly when rule-based GLOSA system is
employed, but this change is limited by its ”ego-efﬁcient”
feature, which cannot takes the following HDVs into account.
Although the trafﬁc delay increases compared to IDM and
GLOSA approach, it is just the result of extremely energy-
saving setting due to the large ratio of weighting parameters
ω1 and ω2, and we will show that the sacriﬁced travel delay can
be reduced to approximately zero by regulating the parameters.
For each platoon size conﬁguration, the trajectories of the
vehicles in the ego platoon are collected. We randomly sample
several trajectories and draw the ﬁgures. The results are shown
in Figure 9. The color depth reﬂects the speed of the vehicles,
while the horizontal line represents the phase of the trafﬁc
signal. Meanwhile, we implement an IDM-based study to
make a comparison, and the sampled trajectories are also
provided in Figure 9. According to the ﬁgure, the ego platoon
can cross the signalized intersection without any stops when
the ego CAV is controlled by ARS. Thus, the unnecessary stop
and rapid acceleration/deceleration can be avoided to promote
energy conservation. In addition, the ego CAV can consider the
crossing of more HDVs as the size of the platoon increases.
When the number of HDVs exceeds 4, the platoon controlled
purely by IDM can be divided so that some of the vehicles
in the platoon cannot cross the intersection with the leading
vehicles during the same phase. The ARS agents can adjust
its velocity to relatively low value to ﬁt the phase change and
guarantee the effective operation of subsequent HDVs, while
the CAVs controlled by IDM can only speed up if there is
no interruption. This also illuminated that only based on the
appropriate control methods can the comprehensive beneﬁts
of the CAVs be brought into trafﬁc.

E. The Impact of weighting parameters

The weighting parameters determine the optimization direc-
tion of the algorithm. Exploring the impact of the weighting
parameters is valuable for understanding the effect of the ER-
based reward signal. In particular, the impact of ω1 and ω2
mainly originates from the ratio (i.e., ω1
) of the two values.
ω2
Therefore, we ﬁx ω2 to 1 and change ω1 from 1 to 6, and
then ﬁx ω1 to 1 with ω2 changing from 1 to 6. A ”1+3”
mixed platoon scenario is still taken as an example to observe
the impact of the ratio, and the results are shown in Figure 10.
According to the ﬁgure, we can see that the delay of vehicles
reduces rapidly with the increase of ω2 when ω1 < ω2. The
policies learned by the agent can reduce both delay and energy
consumption in these cases. When we set ω1 > ω2, the energy
consumption can be reduced signiﬁcantly. The policies in these
cases can serve as economic driving strategies to maximize
energy efﬁciency.

Simulations for other DRL algorithms with different weight-
ing parameter settings are also carried out to make more com-
prehensive comparison studies, and the results are collected in
Table III. It can be found that the performance of the proposed
ARS-based control varies regularly with the change of weight-
ing parameters, while the same outcome cannot be achieved by

Fig. 7: The training curve of the cases with different platoon
size.

Fig. 8: Single-vehicle-based Performance for Different Platoon
Size. (a)Energy consumption. (b)Trafﬁc delay.

approaches and offer the single-vehicle-based statistical results
for different platoon size conﬁgurations. Since optimization-
based methods always suffer from unacceptable computation
complexity, we only compare ARS with learning-based al-
gorithms and rule-based algorithms: (1) IDM is regarded as
paradigm of normal driving behavior, which resembles human
drivers; (2) DDPG is deployed in several studies and achieves
SOTA performance [6], [35]; (3) PPO with DR setting serves
as a baseline to observe the performance of DR settings;
(4) Rule-based model [56], which is known as Green Light
Optimal Speed Advisory (GLOSA) system, provides CAVs
with speed guidance in an ”ego-efﬁcient” way.

The results are also collected from 10 independent simula-
tions, which is illustrated in Figure 8. It can be seen that the
energy consumption and trafﬁc delay increase sharply with the
rise of platoon size for normal car-following approach (IDM),
but the two indicators can maintain a stable level under ARS
control. As far as DDPG and PPO are concerned, the proposed

IEEE TRANSACTIONS

9

(a)

(b)

Fig. 9: The trajectories of the mixed platoon with different platoon size. (a) Controlled by ARS (b) Controlled by IDM

the other two DRL algorithms. This ﬁnding further enhances
the ﬂexibility and applicability of the framework with delay
reward when considering regulating the relative importance
between mobility and energy efﬁciency. Moreover, the ARS
algorithm can achieve the optimal performance in terms of
both travel delay and energy consumption. The signiﬁcant
decline of consumed electricity demonstrates that our method
possess tremendous potential for the mixed platoon control
task.

More precisely, compared with the basic IDM car-following
behavior, the electricity consumption is reduced by 39.27% to
82.51% with different weighting parameter settings. If we set
ω1 = 1 with ω2 = 6, the energy can be saved by 53.64% with
approximately the same performance in terms of delay. This
result achieves SOTA performance when it is difﬁcult to have
both energy consumption and travel delay decline [35], [42].

VI. CONCLUSION

Fig. 10: The variation curve with different settings of ω1 and
ω2. (a) Delay per vehicle. (b) Energy consumption per vehicle.

In this paper, we propose a reinforcement learning frame-
work to control the mixed platoon composed of CAVs and
HDVs at a signalized intersection. By designing a novel
state representation and reward function, the approach can be
extended to the platoons with different platoon size. ARS is
implemented to overcome the challenge caused by episodic
reward, which is proved can outperform the distributed reward

conﬁguration for the utilized algorithm. Analysis and simula-
tion results validate that ARS is capable of controlling the
ego CAV and make the platoon cross the intersection without
any stops. Meanwhile, great energy-efﬁcient performance can
be achieved, so we recommend the method as an economic
driving strategy in practice. Being compared with several
SOTA DRL algorithms, the proposed method gives a much

IEEE TRANSACTIONS

10

TABLE III: Performance of the methods with different weighting parameter settings. Improvements for ARS compared with
the IDM model are shown as Imp. The best performance is notated by bold style

Ratio ( ω1
ω2

)

1/1
1/2
1/3
1/4
1/5
1/6
2/1
3/1
4/1
5/1
6/1

Delay per vehicle (s)

PPO with DR DDPG
61.78
162.51
81.91
81.98
162.24
161.71
151.91
79.64
165.38
159.24
162.04

116.24
107.44
78.71
96.77
124.84
98.64
72.78
67.78
90.91
98.91
154.18

ARS
133.84
103.24
96.38
77.04
61.04
55.11
164.64
167.71
163.78
163.41
165.78

Imp. ↑
-130.84%
-78.07%
-66.22%
-32.88%
-5.28%
4.95%
-183.97%
-189.26%
-182.47%
-181.83%
-185.92%

Energy consumption per vehicle (Wh)
PPO with DR DDPG
89.78
78.27
106.77
111.74
77.70
77.94
77.81
111.56
77.79
78.39
69.68

99.47
126.26
106.04
124.78
106.10
88.01
113.87
95.69
112.64
102.88
48.40

ARS
59.30
92.99
79.54
83.48
81.58
70.98
29.03
31.55
29.84
29.73
26.79

Imp. ↑
61.27%
39.27%
48.05%
45.47%
46.72%
53.64%
81.04%
79.40%
80.51%
80.58%
82.51%

higher reward with a simple architecture.

It should be noted that the strategy put forward in this paper
is still feasible with multi-intersection scenario by taking the
SPaT information of the ﬁrst downstream trafﬁc signal as part
of the state in succession. However, we only study the control
of a single agent, while multi-agent cooperation may bring
about more return. A collaboration way can be introduced
with the support of vehicle-to-vehicle communication in this
context.

As for the future research, ﬁrstly, the longitudinal motion
of vehicles can be controlled by setting the acceleration in
continuous action space. More comprehensive studies can start
from the combination of longitudinal and lateral control in
order to further tap the advantages of CAVs. By designing
proper strategy to incorporate car-following and lane-changing
motion, the cooperative operation of CAVs from multi-lane
trafﬁc environment may has a profound inﬂuence on the
overall mixed trafﬁc performance. Secondly, the inﬂuence of
the trafﬁc signal timing scheme is not explored in this paper,
and it can be discussed speciﬁcally. Thirdly, the difference
between traditional gasoline vehicles and electric vehicles can
be discussed for the DRL-based adaptive control. Finally, it is
valuable to study the impact range of the ego CAV, which is
determined by its sensing ability or communication ability, so
as to make the model more practical.. With the development
of ITS, more reliable control methods will be implemented to
create a sustainable and efﬁcient urban trafﬁc environment.

REFERENCES

[1] D. J. Fagnant and K. Kockelman, “Preparing a nation for autonomous
vehicles: opportunities, barriers and policy recommendations,” Trans-
portation Research Part A: Policy and Practice, vol. 77, pp. 167–181,
2015.

[2] J. Rios-Torres and A. A. Malikopoulos, “A survey on the coordination
of connected and automated vehicles at intersections and merging at
highway on-ramps,” IEEE Transactions on Intelligent Transportation
Systems, vol. 18, no. 5, pp. 1066–1077, 2017.

[3] D. Bevly, X. Cao, M. Gordon, G. Ozbilgin, D. Kari, B. Nelson,
J. Woodruff, M. Barth, C. Murray, A. Kurt, K. Redmill, and U. Ozguner,
“Lane change and merge maneuvers for connected and automated
vehicles: A survey,” IEEE Transactions on Intelligent Vehicles, vol. 1,
no. 1, pp. 105–120, 2016.

[4] S. Ilgin Guler, M. Menendez, and L. Meier, “Using connected vehicle
technology to improve the efﬁciency of intersections,” Transportation
Research Part C: Emerging Technologies, vol. 46, pp. 121–131, 2014.
[5] C. Dong, H. Wang, Y. Li, W. Wang, and Z. Zhang, “Route control strate-
gies for autonomous vehicles exiting to off-ramps,” IEEE Transactions
on Intelligent Transportation Systems, vol. 21, no. 7, pp. 3104–3116,
2020.

[6] M. Zhou, Y. Yu, and X. Qu, “Development of an efﬁcient driving
strategy for connected and automated vehicles at signalized intersections:
A reinforcement learning approach,” IEEE Transactions on Intelligent
Transportation Systems, vol. 21, no. 1, pp. 433–443, 2020.

[7] J. Guanetti, Y. Kim, and F. Borrelli, “Control of connected and auto-
mated vehicles: State of the art and future challenges,” Annual Reviews
in Control, vol. 45, pp. 18–40, 2018.

[8] B. Peng, M. F. Keskin, B. Kulcs´ar, and H. Wymeersch, “Connected
autonomous vehicles for improving mixed trafﬁc efﬁciency in unsignal-
ized intersections with deep reinforcement learning,” Communications
in Transportation Research, vol. 1, p. 100017, 2021.

[9] T. Olovsson, T. Svensson, and J. Wu, “Future connected vehicles:
Communications demands, privacy and cyber-security,” Communications
in Transportation Research, vol. 2, p. 100056, 2022.

[10] F. Perronnet, J. Buisson, A. Lombard, A. Abbas-Turki, M. Ahmane, and
A. El Moudni, “Deadlock prevention of self-driving vehicles in a net-
work of intersections,” IEEE Transactions on Intelligent Transportation
Systems, vol. 20, no. 11, pp. 4219–4233, 2019.

[11] C. Yu, W. Sun, H. X. Liu, and X. Yang, “Managing connected
and automated vehicles at isolated intersections: From reservation- to
optimization-based methods,” Transportation Research Part B: Method-
ological, vol. 122, pp. 416–435, 2019.

[12] B. Chalaki and A. A. Malikopoulos, “Time-optimal coordination for
intersections,” IEEE

connected and automated vehicles at adjacent
Transactions on Intelligent Transportation Systems, pp. 1–16, 2021.
[13] M. A. S. Kamal, J.-i. Imura, T. Hayakawa, A. Ohata, and K. Aihara,
“A vehicle-intersection coordination scheme for smooth ﬂows of trafﬁc
without using trafﬁc lights,” IEEE Transactions on Intelligent Trans-
portation Systems, vol. 16, no. 3, pp. 1136–1147, 2015.

[14] Y. Hu, C. Chen, J. He, and B. Yang, “Eco-platooning for cooperative
automated vehicles under mixed trafﬁc ﬂow,” IEEE Transactions on
Intelligent Transportation Systems, vol. 22, no. 4, pp. 2023–2034, 2021.
[15] L. Zhu, Y. Tang, and D. Yang, “Cellular automata-based modeling and
simulation of the mixed trafﬁc ﬂow of vehicle platoon and normal
vehicles,” Physica A: Statistical Mechanics and its Applications, vol.
584, p. 126368, 2021.

[16] O. Orki and S. Arogeti, “Control of mixed platoons consist of auto-
mated and manual vehicles,” in 2019 IEEE International Conference on
Connected Vehicles and Expo (ICCVE), 2019, pp. 1–6.

[17] Y. Du, W. ShangGuan, and L. Chai, “A coupled vehicle-signal control
method at signalized intersections in mixed trafﬁc environment,” IEEE
Transactions on Vehicular Technology, vol. 70, no. 3, pp. 2089–2100,
2021.

[18] A. Sharma, Z. Zheng, J. Kim, A. Bhaskar, and M. Mazharul Haque,
“Assessing trafﬁc disturbance, efﬁciency, and safety of the mixed trafﬁc
ﬂow of connected vehicles and traditional vehicles by considering human

IEEE TRANSACTIONS

11

factors,” Transportation Research Part C: Emerging Technologies, vol.
124, p. 102934, 2021.

Conference on Intelligent Transportation Systems (ITSC), 2011, pp. 341–
346.

[39] H. Shi, Y. Zhou, K. Wu, X. Wang, Y. Lin, and B. Ran, “Connected au-
tomated vehicle cooperative control with a deep reinforcement learning
approach in a mixed trafﬁc environment,” Transportation Research Part
C: Emerging Technologies, vol. 133, p. 103421, 2021.

[40] J. Shi, F. Qiao, Q. Li, L. Yu, and Y. Hu, “Application and evaluation of
the reinforcement learning approach to eco-driving at intersections un-
der infrastructure-to-vehicle communications,” Transportation Research
Record, vol. 2672, no. 25, pp. 89–98, 2018.

[41] X. Qu, Y. Yu, M. Zhou, C.-T. Lin, and X. Wang, “Jointly dampening
trafﬁc oscillations and improving energy consumption with electric,
connected and automated vehicles: A reinforcement
learning based
approach,” Applied Energy, vol. 257, p. 114030, 2020.

[42] M. Wegener, L. Koch, M. Eisenbarth, and J. Andert, “Automated eco-
driving in urban scenarios using deep reinforcement learning,” Trans-
portation Research Part C: Emerging Technologies, vol. 126, p. 102967,
2021.

[43] J. Zhang, X. Jiang, S. Cui, C. Yang, and B. Ran, “Navigating
electric vehicles along a signalized corridor via reinforcement learning:
Toward adaptive eco-driving control,” Transportation Research Record,
vol. 0, no. 0, p. 03611981221084683, 2022.
[Online]. Available:
https://doi.org/10.1177/03611981221084683

[44] H. Mania, A. Guy, and B. Recht, “Simple random search provides a

competitive approach to reinforcement learning,” 2018.

[45] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Fl¨otter¨od,
R. Hilbrich, L. L¨ucken, J. Rummel, P. Wagner, and E. Wiessner,
“Microscopic trafﬁc simulation using sumo,” in 2018 21st International
Conference on Intelligent Transportation Systems (ITSC), 2018, pp.
2575–2582.

[46] E. Ferrero, S. Alessandrini, and A. Balanzino, “Impact of the electric
vehicles on the air pollution from a highway,” Applied Energy, vol. 169,
pp. 450–459, 2016.

[47] J. Zhang, T.-Q. Tang, Y. Yan, and X. Qu, “Eco-driving control for
connected and automated electric vehicles at signalized intersections
with wireless charging,” Applied Energy, vol. 282, p. 116215, 2021.

[48] H. Kato, R. Ando, Y. Kondo, T. Suzuki, K. Matsuhashi, and
S. Kobayashi, “The eco-driving effect of electric vehicles compared to
conventional gasoline vehicles,” AIMS Energy, vol. 4, no. 6, pp. 804–
816, 2016.

[49] M. Treiber, A. Hennecke, and D. Helbing, “Congested trafﬁc states
in empirical observations and microscopic simulations,” Phys. Rev. E,
vol. 62, pp. 1805–1824, Aug 2000.

[50] P. Wang, C.-Y. Chan, and A. de La Fortelle, “A reinforcement learning
based approach for automated lane change maneuvers,” in 2018 IEEE
Intelligent Vehicles Symposium (IV), 2018, pp. 1379–1384.

[51] A. Sharma, Z. Zheng, A. Bhaskar, and M. M. Haque, “Modelling
car-following behaviour of connected vehicles with a focus on driver
compliance,” Transportation Research Part B: Methodological, vol. 126,
pp. 256–279, 2019.

[52] S. Calvert, W. Schakel, and J. Van Lint, “Will automated vehicles
negatively impact trafﬁc ﬂow?” Journal of Advanced Transportation,
vol. 2017, 2017.

[53] M. Minsky, “Steps toward artiﬁcial intelligence,” Proceedings of the

IRE, vol. 49, no. 1, pp. 8–30, 1961.

[54] T. Kurczveil, P.

´A. L´opez, and E. Schnieder, “Implementation of an
energy model and a charging infrastructure in sumo,” in Simulation
of Urban Mobility, M. Behrisch, D. Krajzewicz, and M. Weber, Eds.
Berlin, Heidelberg: Springer Berlin Heidelberg, 2014, pp. 33–43.
[55] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, “Evolution
strategies as a scalable alternative to reinforcement learning,” arXiv
preprint arXiv:1703.03864, 2017.

[56] A. Stevanovic, J. Stevanovic, and C. Kergaye, “Green light optimized
speed advisory systems: Impact of signal phasing information accuracy,”
Transportation Research Record, vol. 2390, no. 1, pp. 53–59, 2013.

[19] Y. Bichiou and H. A. Rakha, “Developing an optimal

intersection
control system for automated connected vehicles,” IEEE Transactions
on Intelligent Transportation Systems, vol. 20, no. 5, pp. 1908–1916,
2019.

[20] S. R. Mousa, S. Ishak, R. M. Mousa, J. Codjoe, and M. Elhenawy, “Deep
reinforcement learning agent with varying actions strategy for solving
the eco-approach and departure problem at signalized intersections,”
Transportation Research Record, vol. 2674, no. 8, pp. 119–131, 2020.
[21] Z. Bai, P. Hao, W. Shangguan, B. Cai, and M. J. Barth, “Hybrid
reinforcement learning-based eco-driving strategy for connected and
automated vehicles at signalized intersections,” IEEE Transactions on
Intelligent Transportation Systems, pp. 1–14, 2022.

[22] Z. Wang, G. Wu, and M. J. Barth, “Cooperative eco-driving at signalized
intersections in a partially connected and automated vehicle environ-
ment,” IEEE Transactions on Intelligent Transportation Systems, vol. 21,
no. 5, pp. 2029–2038, 2020.

[23] F. Ma, Y. Yang, J. Wang, X. Li, G. Wu, Y. Zhao, L. Wu, B. Aksun-
Guvenc, and L. Guvenc, “Eco-driving-based cooperative adaptive cruise
control of connected vehicles platoon at signalized intersections,” Trans-
portation Research Part D: Transport and Environment, vol. 92, p.
102746, 2021.

[24] R. W. Timmerman and M. A. A. Boon, “Platoon forming algorithms for
intelligent street intersections,” Transportmetrica A: Transport Science,
vol. 17, no. 3, pp. 278–307, 2021.

[25] S. Yao and B. Friedrich, “Managing connected and automated vehicles in
mixed trafﬁc by human-leading platooning strategy: a simulation study,”
in 2019 IEEE Intelligent Transportation Systems Conference (ITSC),
2019, pp. 3224–3229.

[26] J. Zhang, S. Dong, Z. Li, B. Ran, R. Li, and H. Wang, “An eco-
driving signal control model for divisible electric platoons in cooperative
vehicle-infrastructure systems,” IEEE Access, vol. 7, pp. 83 277–83 285,
2019.

[27] J. Zhang, X. Jiang, Z. Liu, L. Zheng, and B. Ran, “A study on au-
tonomous intersection management: Planning-based strategy improved
by convolutional neural network,” KSCE Journal of Civil Engineering,
vol. 25, no. 10, pp. 3995–4004, 2021.

[28] L. Yue, M. Abdel-Aty, and Z. Wang, “Effects of connected and au-
tonomous vehicle merging behavior on mainline human-driven vehicle,”
Journal of Intelligent and Connected Vehicles, vol. 5, no. 1, pp. 36–45,
2022.

[29] W. Zhao, D. Ngoduy, S. Shepherd, R. Liu, and M. Papageorgiou,
“A platoon based cooperative eco-driving model for mixed automated
and human-driven vehicles at a signalised intersection,” Transportation
Research Part C: Emerging Technologies, vol. 95, pp. 802–821, 2018.
[30] C. Chen, J. Wang, Q. Xu, J. Wang, and K. Li, “Mixed platoon control
of automated and human-driven vehicles at a signalized intersection:
Dynamical analysis and optimal control,” Transportation Research Part
C: Emerging Technologies, vol. 127, p. 103138, 2021.

[31] B. Asadi and A. Vahidi, “Predictive cruise control: Utilizing upcoming
trafﬁc signal information for improving fuel economy and reducing trip
time,” IEEE Transactions on Control Systems Technology, vol. 19, no. 3,
pp. 707–714, 2011.

[32] S. Gong and L. Du, “Cooperative platoon control for a mixed trafﬁc
ﬂow including human drive vehicles and connected and autonomous
vehicles,” Transportation Research Part B: Methodological, vol. 116,
pp. 25–61, 2018.

[33] M. Kuriyama, S. Yamamoto, and M. Miyatake, “Theoretical study on
eco-driving technique for an electric vehicle with dynamic program-
ming,” in 2010 International Conference on Electrical Machines and
Systems, 2010, pp. 2026–2030.

[34] A. Dabiri and A. Hegyi, “Personalised optimal speed advice to cyclists
approaching an intersection with uncertain green time,” in 2018 Euro-
pean Control Conference (ECC), 2018, pp. 1666–1671.

[35] Q. Guo, O. Angah, Z. Liu, and X. J. Ban, “Hybrid deep reinforce-
ment learning based eco-driving for low-level connected and automated
vehicles along signalized corridors,” Transportation Research Part C:
Emerging Technologies, vol. 124, p. 102980, 2021.

[36] Y. Ci, L. Wu, J. Zhao, Y. Sun, and G. Zhang, “V2i-based car-following
modeling and simulation of signalized intersection,” Physica A: Statis-
tical Mechanics and its Applications, vol. 525, pp. 672–679, 2019.
[37] X. Zhao, X. Wu, Q. Xin, K. Sun, and S. Yu, “Dynamic eco-driving on
signalized arterial corridors during the green phase for the connected
vehicles,” Journal of Advanced Transportation, vol. 2020, 2020.
[38] H. Rakha and R. K. Kamalanathsharma, “Eco-driving at signalized
intersections using v2i communication,” in 2011 14th International IEEE

