Alpha-Reﬁne: Boosting Tracking Performance by
Precise Bounding Box Estimation

1
2
0
2

l
u
J

1
1

]

V
C
.
s
c
[

3
v
5
1
8
6
0
.
2
1
0
2
:
v
i
X
r
a

Bin Yan1*, Xinyu Zhang1*, Dong Wang1†, Huchuan Lu1,2 and Xiaoyun Yang3
1School of Information and Communication Engineering, Dalian University of Technology, China
2Peng Cheng Laboratory 3Remark AI
{yan bin,zhangxy71102}@mail.dlut.edu.cn, {wdice, lhchuan}@dlut.edu.cn, xyang@remarkholdings.com

Abstract

Visual object tracking aims to precisely estimate the
bounding box for the given target, which is a challeng-
ing problem due to factors such as deformation and occlu-
sion. Many recent trackers adopt the multiple-stage strategy
to improve bounding box estimation. These methods ﬁrst
coarsely locate the target and then reﬁne the initial predic-
tion in the following stages. However, existing approaches
still suffer from limited precision, and the coupling of dif-
ferent stages severely restricts the method’s transferability.
This work proposes a novel, ﬂexible, and accurate reﬁne-
ment module called Alpha-Reﬁne (AR), which can signiﬁ-
cantly improve the base trackers’ box estimation quality. By
exploring a series of design options, we conclude that the
key to successful reﬁnement is extracting and maintaining
detailed spatial information as much as possible. Following
this principle, Alpha-Reﬁne adopts a pixel-wise correlation,
a corner prediction head, and an auxiliary mask head as the
core components. Comprehensive experiments on Track-
ingNet, LaSOT, GOT-10K, and VOT2020 benchmarks with
multiple base trackers show that our approach signiﬁcantly
improves the base tracker’s performance with little extra
latency. The proposed Alpha-Reﬁne method leads to a se-
ries of strengthened trackers, among which the ARSiamRPN
(AR strengthened SiamRPNpp) and the ARDiMP50 (AR
strengthened DiMP50) achieve good efﬁciency-precision
trade-off, while the ARDiMPsuper (AR strengthened DiMP-
super) achieves very competitive performance at a real-
time speed. Code and pretrained models are available at
https://github.com/MasterBin-IIAU/AlphaReﬁne.

1. Introduction

Precise box estimation is indispensable for a success-
ful tracker. Early trackers usually solve this problem by

*Equal contribution.
†Corresponding Author: Dr. Dong Wang, wdice@dlut.edu.cn

Table 1. Oracle experiment on LaSOT. The center of the search
region is always set at the center of the ground truth, reﬂecting the
box estimation capacity of these methods. The best three results
are marked in red, green and blue bold fonts respectively.

Oracle
SiamRPNpp[23]
ATOM[7]
DiMPsuper[2]
ECO[6]
AlphaReﬁne

AUC PNorm
0.829
0.682
0.686
0.580
0.799
0.693
0.666
0.496
0.902
0.762

P
0.745
0.604
0.734
0.533
0.919

multi-scale search [1, 6, 35, 3] or sampling-then-regression
strategy [37, 29], which are inaccurate and greatly limit the
performance of the trackers. For obtaining more robust
and precise tracking results, many state-of-the-art track-
ers [40, 12, 7, 2] adopt a multiple-stage tracking strategy,
which introduces additional tracking stages for more pre-
cise box estimation. These trackers ﬁrst coarsely locate the
target and then reﬁne the initial result in the additional track-
ing stages to get more precise box prediction. However,
this box estimation can still be improved, even for state-of-
the-art trackers. An oracle experiment veriﬁes this opinion.
We set the tracker’s search region always centering at the
ground truth so that the performance will be mainly deter-
mined by the capacity of box estimation. Table 1 shows
that with the aforementioned oracle setting, state-of-the-art
trackers’ AUC scores are still far from perfection, indicating
unsatisfying box estimation, although some methods (e.g.
ATOM [7], DiMP [2]) have built-in box estimation mod-
ules. In contrast, given the perfectly centered search region,
the proposed Alpha-Reﬁne achieves signiﬁcantly better per-
formance, demonstrating that Alpha-Reﬁne’s superiority in
box estimation.

Additionally, most of reﬁnement methods in existing
trackers [40, 12, 7, 2] are weak in transferability, because
their training is coupled with other components. Extra re-
training is required if above reﬁnement modules are applied
to new base trackers. As opposed to these methods, Alpha-

 
 
 
 
 
 
2. Related Works

Early Box Estimation. Early box estimation methods are
mainly scale estimation, which can be summarized into
two categories: multiple-scale search and sampling-then-
regression strategies. Most correlation-ﬁlter-based track-
ers [13, 6, 35] and SiamFC [1] adopt the former strategy.
Speciﬁcally, these trackers construct search regions with
different sizes, then compute correlation with the template,
and ﬁnally determine the size of the target as the size-level
where the highest response locates. Multiple-scale search
is coarse and time-consuming due to its ﬁxed-aspect-ratio
prediction and heavy image pyramid operation. Another
type of method ﬁrst generates several bounding box sam-
ples, then uses some methods to choose the best one, and ﬁ-
nally apply regression on it to obtain more accurate results.
SINT [37], MDNet [29] and RT-MDNet [16] are three rep-
resentative trackers that exploit this approach.
Modern Box Estimation. As deep learning techniques
become mature, several high-performance scale estimation
approaches are developed and can be categorized into the
following classes: RPN-based [24, 51, 23], Mask-based[42,
26], IoU-based [7, 2], and Anchor-free-based [45, 4]. RPN-
based methods learn a region proposal network [31], which
determines whether the current anchor contains the tar-
get and makes reﬁnement to the target simultaneously.
SiameseRPN-series trackers [24, 51, 23] utilize the RPN-
based mechanism as the core component and achieve great
success in recent years. Mask representation is more accu-
rate, and the ability to predict mask is quite beneﬁcial to pre-
cise box estimation. SiamMask [42] and D3S [26] belong
to this class, which obtain higher precision than the Siamese
tracker that can only predict boxes. IoU-based approaches
learn a network to predict the overlap between candidate
boxes and groundtruth. During the inference phase, this
strategy optimizes candidate boxes by gradient-ascent, and
therefore obtains more precise results. ATOM [7] and
DiMP [2] fully exploit this method and surpass traditional
In the past
correlation-ﬁlter trackers by a large margin.
years, anchor-free philosophy has become quite popular in
the object detection ﬁeld [22, 49, 18, 38]. SiamFC++ [45]
introduces this structure into object tracking ﬁeld and there-
fore achieves state-of-the-art performance. The CGACD
method [9] designs a corner-based box estimation for ob-
ject tracking which wisely adopts soft-argmax to decode
the corner heat map into box coordinates. The corner-based
version of Alpha-Reﬁne adopts a similar box representa-
tion, and experiments demonstrate that this design retains
more precise spatial information for a reﬁnement module.
Reﬁnement Modules. Many state-of-the-art trackers [40,
12, 7, 2, 21] apply a multiple-stage tracking strategy to
obtain accurate and robust results. This approach ﬁrst lo-
cates the target coarsely and then utilizes a reﬁnement mod-
ule to reﬁne results from the previous stage. SPM [40]

Figure 1. Performance improvement of our Alpha-Reﬁne module
on LaSOT. ‘Base’: the base tracker; ‘Base+AR’: the base tracker
with Alpha-Reﬁne (AR). This ﬁgure shows that all base trackers
are signiﬁcantly improved by the proposed AR module.

Reﬁne is trained independently and can be directly applied
to any existing trackers in a plug-and-play style, requiring
no extra training or modiﬁcation of the base tracker.

In this work, a series of design options are investigated
and compared. Speciﬁcally, we assess multiple feature fu-
sion modules and prediction heads. We also explore to use
an auxiliary mask head, which introduces pixel-level super-
vision into the training. We ﬁnd that extracting and main-
taining precise spatial information is the key to precise box
estimation. To this end, we ﬁnally adopt a pixel-wise corre-
lation as well as a key-point style prediction head for better
maintaining and utilizing the detailed spatial information.
Additionally, an auxiliary mask head is used, which en-
courages the network to extract more detailed spatial infor-
mation, leading to more precise box estimation. Moreover,
if we reserve the mask head at the inference stage, Alpha-
Reﬁne will enable the base trackers to predict the mask of
the object, satisfying scenarios where the mask is required.
The design options will be discussed in Section 3 and veri-
ﬁed in Section 4.

To verify the effectiveness of our Alpha-Reﬁne mod-
ule, we choose six famous base trackers: ECO [6], RT-
MDNet [16], SiamRPNpp [23], ATOM [7], DiMP [2],
and DiMPsuper [2], on multiple tracking benchmarks,
namely, LaSOT [11], GOT-10K [14], TrackingNet [28]
and VOT2020 [19]. Take Fig. 1 as an example, exper-
imental results show that our proposed reﬁnement mod-
ule improves the base trackers’ performance signiﬁcantly.
Compared with its competitors (i.e.
IoU-Net [7, 2] and
SiamMask [42]), Alpha-Reﬁne’s performance also sur-
passes them by a large margin.

The proposed Alpha-Reﬁne method leads to a se-
ries of strengthened trackers, among which the AR-
SiamRPN (Alpha-Reﬁne strengthened SiamRPNpp) and
ARDiMP50 (Alpha-Reﬁne strengthened DiMP50) achieve
good efﬁciency-precision trade-off, while ARDiMPsuper
(Alpha-Reﬁne strengthened DiMPsuper) achieves state-of-
the-art performance at a real-time speed on multiple bench-
marks.

ECORT-MDNetSiamRPNppATOMDiMP50DiMPsuper0.20.30.40.50.60.7AUC0.3690.3080.4760.4950.5590.6370.4610.4990.5590.5700.6020.653LaSOTBaseBase+ARand Siamese Cascaded RPN [12] adopt a light-weight re-
lation network [36] and stacked RPNs [31], respectively,
as the reﬁnement module to further increase trackers’ dis-
criminative power and precision. However, the two reﬁne-
ment modules have to be trained together with their pre-
vious Siamese tracker in an end-to-end manner; this pro-
cedure limits their ﬂexibility of combining with other base
trackers. ATOM [7] and DiMP [2] ﬁrst use an online clas-
siﬁcation module to locate the target and then draw some
random samples around it. Finally, they deploy a modi-
ﬁed IoU-Net [15] to maximize the overlap between these
samples and groundtruth to obtain more precise bounding
boxes. This modiﬁed IoU-Net can be trained separately
from the base tracker. Thus, the IoU-Net has good transfer-
ability but its precision can still be greatly improved. No-
tably, the winners of VOT2019 [21] utilize SiamMask [42]
as a reﬁnement module [21]. Similar to IoU-Net [15] men-
tioned before, SiamMask [42] can be combined with any
base tracker. However, SiamMask is designed as an in-
dependent tracker rather than a reﬁnement module, which
is not suitable and not economical to reﬁne other trackers.
Considering previous reﬁnement modules’ weak transfer-
ability and limited accuracy, we propose a novel, ﬂexible,
and accurate reﬁnement module named Alpha-Reﬁne.

3. Alpha-Reﬁne

Alpha-Reﬁne is a reﬁnement module which is able to
efﬁciently reﬁne the base tracker’s outputs and signiﬁcantly
improve the tracking performance. We detail the network
architecture (Fig. 2), design options, and training process as
follows.

3.1. Network Architecture

Fig. 2 shows the overall architecture of the proposed
Alpha-Reﬁne module. This module adopts the Siamese ar-
chitecture with two input branches, namely, the reference
branch and the test branch. A parameter-shared backbone
is applied to both branches for feature extraction. Features
extracted from two branches are aggregated by a fusion
module, which is typically a correlation module (e.g. naive-
correlation, depth-wise correlation, pixel-wise correlation).
The fused feature is further processed by some convolu-
tional layers, producing the features for the prediction head.
An auxiliary mask head can be added parallel to the box
head to introduce pixel-level supervision into training. The
output of the mask head can be used for scenarios that also
require a mask result.

To function as a reﬁnement module, the reference branch
is initialized by the ﬁrst frame with the ground truth. In the
current frame, the test branch extends the base tracker’s pre-
diction into a concentric search region of two times the size,
from which Alpha-Reﬁne predicts a ﬁner result. Alpha-

Reﬁne can be combined with arbitrary trackers in a plug-
and-play style and improve their performance.

Notably, compared with independent trackers, the size
of Alpha-Reﬁne’s search region is roughly two times the
size of the object, which is smaller than normal trackers
(four times in most cases). The smaller search region can
depress the cluttered background and enable the model to
focus on more detailed spatial information, which is beneﬁ-
cial to precise localization. Small search region also lowers
the computation cost, so that Alpha-Reﬁne can improve the
base tracker with little latency increase. Alpha-Reﬁne is not
capable of tracking by itself because of the small search re-
gion light-weight design. A complete base tracker is always
needed.

3.2. Feature Fusion

Most methods with Siamese architecture aggregate fea-
tures of the template and the search region using the
coarse naive correlation [1, 24, 51] or depth-wise correla-
tion [23, 42]. As shown in Fig. 3, both naive correlation
or depth-wise correlation take the whole template feature as
the kernel to correlate with the search region feature, mak-
ing adjacent sliding window on the feature map producing
similar response and blur the spatial information. As a re-
ﬁnement module, Alpha-Reﬁne requires the feature fusion
module to maintain as much spatial information as possi-
ble. Thus, the popular naive nor depth-wise correlation is
not suitable for Alpha-Reﬁne.

In this work, we adopt pixel-wise correlation [43] for
high-quality feature representation. We denote K ∈
RC×H0×W0 and S ∈ RC×H×W as features of the template
and the search region. Pixel-wise correlation ﬁrst decom-
poses K into H0W0 small kernels Kj ∈ RC×1×1 and then
uses them to compute correlation separately to obtain cor-
relation maps C ∈ RH0W0×H×W . The process can be de-
scribed as

C = {Cj|Cj = Kj ∗ S}j∈{1,...,H0×W0},

(1)

where ∗ denotes naive correlation.

In contrast to naive or depth-wise correlation, pixel-wise
correlation takes each part of the target features as a ker-
nel. Pixel-wise correlation ensures that each correlation
map encodes information of a local region on the target
while avoiding an extremely large correlation window from
blurring the feature.

Fig. 5 shows the computation process of three correlation
methods, and Fig. 4 shows some fusion outputs. Fig. 4(c)
indicates that naive convolution can only roughly repre-
sent the center location of the object while losing most of
the shape and scale information. Fig. 4(d) illustrates that
depth-wise correlation has to encode the blurred location
into channels, which is less explainable and inefﬁcient. By
contrast, Fig. 4(e) shows that pixel-wise correlation is better

Figure 2. Overall architecture of the proposed Alpha-Reﬁne. Better viewed in color with zoom-in.

ing receptive ﬁelds, making them good at representing var-
ious parts of the object. The RPN style method ignores the
relationship between feature points at different locations,
not fully utilizing the information contained in the spatial
distribution of the feature map. Additionally, this strategy
has inconsistency because most precise box predictions may
have a low score.
In the experiment, we implement this
strategy by stacking four Conv-BN-ReLU layers followed
by a prediction layer. For simplicity, we directly regress
four distances from the feature point location to four edges
of the bounding box. Another four Conv-BN-ReLU layers
are used to predict the conﬁdence score.

RCNN Style Box Head.
Similar to the second stage of
Faster-RCNN [31], this RCNN Style method reduces the
feature map into a vector and estimates the bounding box of
the object with some fully connected layers. Fig. 6(b) shows
the diagram. Compared with the RPN style method, this
method utilize the whole feature map rather than individual
feature points. Apparently, this method crashes spatial in-
formation when reducing the feature map, indicating that it
is not suitable for a reﬁnement module. For this strategy,
we use a bounding box head containing four stacked Conv-
BN-ReLU layers in our experiment, followed by a global
average pooling layer and a fully-connected layer, which
predicts four coordinates of the bounding box.

Corner Head. Recently, keypoints detection techniques
have become popular in the object detection ﬁeld, produc-
ing several state-of-the-art methods [22, 49, 10, 50].
In
our experiment, we implement a corner head with four
stacked Conv-BN-ReLU layers, followed by a Conv layer
predicting two heatmaps, which represent top-left corner
and bottom-right corner respectively. Different from meth-
ods like CornerNet [22], we do not upsample the feature
map for the computation issue, resulting in a coarse-grained
heatmaps. We apply soft-argmax [27] to the heatmaps to
make the discrete heatmaps precisely describe the position

Figure 3. Illustration of the blur effect. Depth-wise correlation or
naive correlation may blur the spatial information.

at retaining the target’s boundary and other detailed spatial
information.

3.3. Prediction Heads

We explore two ways of predicting the bounding box:
directly regressing the box coordinates and predicting two
corner points1 from two heatmaps. For regressing the box
coordinates, we evaluate RPN style and RCNN style de-
signs. For predicting the corners, we evaluate the key-point
style design.
RPN Style Box Head. One way of regressing the bound-
ing box is the RPN Style Box Head. Fig. 6(a) shows the
diagram. Similar to the FCN structure of one-stage object
detectors, a 4D box coordinates together with a conﬁdence
score is predicted at each location. The box with the highest
score is regarded as the tracking result.

However, we notice some drawbacks of using this
method in the reﬁnement module. In this method, each box
prediction is generated by an individual feature point, which
requires this feature point to summarize the information in
its receptive ﬁeld and encode spatial information into the
channels, so that a single feature point can make a predic-
tion by itself. However, different feature points have vary-

1The top-left corner and the bottom-right corner

Base TrackerRefinement ModuleFeature FusionResponse FeatureBBoxPredMaskPredLow-level Backbone FeaturesReference Frame(the first frame)Test Frame(the current frame)3x256x2563x256x256Backbone (stride=16)Cx8x8Cx16x16CorrTest featureReference featureResponseFigure 4. Comparison among different correlation responses. (a) and (b) denote the reference branch and the test branch, respectively. (c),
(d), and (e) are correlation result of naive, depth-wise (the ﬁrst 32 channels of 256), and pixel-wise correlations, respectively.

shape information would be helpful. To this end, we add
an auxiliary mask head parallel to the box head, which in-
troduces pixel-level supervision into training. When the
box head is trained with the auxiliary mask head, the net-
work is encouraged to extract more detailed spatial infor-
mation which is required by the mask head and facilitates
precise box estimation. In addition, supervision from mask
annotation also teach the model to better discriminate fore-
ground and background, which is required by the segmen-
tation task and also beneﬁcial to tracking. Some previous
works [42, 26] also demonstrate that mask prediction is
quite beneﬁcial for improving tracking performance, espe-
cially on benchmarks (e.g., VOT [20, 21]) that adopt rotated
bounding box labels. In this work, the mask head is im-
plemented as a U-Net [32] style decoder, which gradually
upsample the feature map while fusing them with low-level
features from the backbone until the resolution is the same
as the input image, and a mask is predicted from the last
layer. At the inference stage, the mask head is by default
disabled to speed up Alpha-Reﬁne. For scenarios requir-
ing pixel-level prediction, the mask head can be activated,
producing mask prediction as the output.

3.4. Training

Training Set Construction. We use the training splits of
LaSOT [11] and GOT-10K [14], ImageNet VID [33], Ima-
geNet DET [33], COCO [25], Youtube-VOS [44], and some
segmentation datasets [46, 41, 34] to train the Alpha-Reﬁne.
Given a video sequence, two stochastic frames Fref and
Ftest with an interval of less than 50 frames are ﬁrst se-
lected. The input of the reference branch is obtained by
cropping Fref at the center of the ground truth with two
times the size of the ground truth box. The input of the
test branch is obtained by cropping Ftest randomly centered
around the ground truth, with a jittered size. Speciﬁcally,
we randomly translate and scale the ground truth of Ftest to

Figure 5. Comparison among different correlation methods. From
left to right, naive, depth-wise, and pixel-wise correlations are
demonstrated. The black-edged cubes or squares represent sliding
kernels. The red edged ones represent corresponding correlation
maps.

Figure 6. Options for predicting box results. (a) RPN style box
prediction. (b) RCNN style box prediction. (c) Corner prediction.

of the corner point. The soft-argmax operation enable our
model to predict continuous values from discrete heatmaps.
It encodes the box estimation into the distribution of conﬁ-
dence (heat) scores, avoiding the inconsistency problem in
the RPN style head. The key-point style method retrains the
natural spatial structure of the feature map, avoiding encod-
ing spatial information into channels, which is desirable for
Alpha-Reﬁne.
Auxiliary Mask Head. As Alpha-Reﬁne is a module for
precisely estimating the bounding box, additional detailed

(a) Reference image(b) Test image(c) Naive correlationresponses(d) Depth-wise correlation responses (the first 32 channels)(e) Pixel-wise correlation responses(of all 64 pixelkernels)NaïveCorrelationDepth-wiseCorrelationPixel-wiseCorrelationFCNFlattenMLP(x1, y1, x2, y2)FCN1FCN2(a) RPN based(b) RCNN based(c) Key-points basedMethod

Table 2. Comparison results on the LaSOT test set. ‘Base’: the
base tracker; and ‘Base+AR’: the base tracker with Alpha-Reﬁne.
The best three results are marked in red, green and blue bold
fonts, respectively. Numbers are shown in percentage (%).
Base+AR
Base
AUC PNorm
43.5
36.9
36.0
30.8
54.7
47.6
56.0
49.5
63.3
55.9
72.5
63.7

AUC PNorm
50.8
46.1
63.1
49.9
62.2
55.9
63.0
57.0
66.8
60.2
73.2
65.3

ECO
RT-MDNet
SiamRPNpp
ATOM
DiMP50
DiMPsuper

P
36.4
30.1
47.2
49.1
55.3
65.6

P
46.0
50.7
57.4
58.1
61.7
68.0

4.1. Representative Visual Results

Figure 7 provides some representative visual results re-
garding different reﬁnement module. We can see that our
Alpha-Reﬁne module facilitates the tracker obtaining more
precise bounding boxes than IoU-Net and SiamMask.

obtain the region to be cropped. With the following equa-
tions:

[h, w] = [2hGT , 2wGT ] × eNf test

s

Omax =

√

hw × f test

c

[cx, cy] = [cGT

x , cGT

y

] + (U − 0.5) × Omax

(2)

(3)

(4)

s

y

, f test
c

and f test
c

we obtain the region centering at [cx, cy] with size [h, w].
, hGT , wGT ] is the ground truth bounding box.
[cGT
x , cGT
f test
are two scalar factors corresponding to scale
s
and center, respectively. We use [f test
] = [0.25, 0.25]
in our experiments. N and U represent the 2D standard
normal distributed random variable and 2D uniform ran-
dom variable respectively. The cropped images are resize
into 256 × 256 as the inputs of Alpha-Reﬁne.
Training Approach. For the box output (i.e. output of
RPN style, RCNN syle, Key-Point style Heads), the mean
squared error Lbox is used. All predictions are converted
into coordinate vectors of the format [left-most, top-most,
right-most, bottom-most] and compared with ground truth
to obtain the mean squared error. For the mask output, a
binary cross-entropy loss Lmask is used. The total loss L is
the weighted sum of two losses.

L = Lbox + λLmask,

(5)

where λ = 1000 is used in the experiments. We train Alpha-
Reﬁne for 40 epochs, each of which consists of 500 itera-
tions on eight Nvidia 2080Ti GPU with a batch size of 32
per GPU (32 × 8 samples per iteration in total). Consider-
ing the abundance of the training data, we do not freeze any
parameters of the backbone. The Adam optimizer [17] is
applied and the learning rate halves every 8 epochs.

4. Experiments

We implement our algorithm with the Pytorch [30] deep
learning library. In this section, we verify the effectiveness
of Alpha-Reﬁne by performing comprehensive experiments
on many popular tracking benchmarks: LaSOT [11], Track-
ingNet [28], GOT-10K [14], and VOT2020 [19] together
with six representative and state-of-the-art base trackers (in-
cluding ECO [6], RT-MDNet [16], ATOM [7], SiamRP-
Npp [23], DiMP50 [2], and DiMPsuper [2]) to demonstrate
our Alpha-Reﬁne’s capacity of boosting the trackers’ per-
formance. Besides, we evaluate our design options with
SiamRPNpp [23] as the base tracker and determine the ef-
fects of different settings of our Alpha-Reﬁne. ResNet-34 is
used as backbone by default if not otherwise speciﬁed. All
experiments of our trackers run ﬁve times, and the results
are obtained by average.

Figure 7. Visual Comparison of Alpha-Reﬁne and other reﬁne-
ment modules. From left to right, we present the origin prediction
of the SiamRPNpp base tracker, and reﬁned results obtained by
SiamMask [42], IoU-Net [7, 2], our Alpha-Reﬁne.

4.2. Evaluation on LaSOT

LaSOT [11] is a recent large-scale tracking benchmark,
which consists of 1400 challenging videos (1120 for train-
ing and 280 for testing). In this work, we follow the one-
pass evaluation, using Success (AUC), Normalized Preci-
sion (PNorm), and Precision (P), to compare different track-
ers without and with Alpha-Reﬁne. Table 2 shows that our
Alpha-Reﬁne (AR) module consistently and signiﬁcantly
improves the base trackers in all evaluation metrics. Es-
pecially for RT-MDNet, the improvement of the AUC score
is up to 19%. As shown in Table 3 the previous best tracker
is Siam R-CNN [39], which obtains a 64.8% AUC score
but merely runs around 5 fps.
in contrast, ARDiMPsuper
achieves the best record (AUC: 65.3%), while maintaining
a real-time speed.
Latency and Speed. Table 4 reports the latency and speed
performance of different trackers without and with Alpha-
Reﬁne, showing that our Alpha-Reﬁne module introduces
few computation loads (merely about 5-6ms every frame),
while signiﬁcantly improving the tracking accuracies (see
Table 2).

Base  Base+SiamMask  Base+IoUNet Base+AR GT Table 3. Comparison state-of-the-art results on the LaSOT test set. The best three results are marked in red, green and blue bold fonts,
respectively. Numbers are shown in percentage (%). More results are available at https://github.com/MasterBin-IIAU/AlphaReﬁne.

Method

AUC(%)
Speed(fps)

ARDiMPsuper
(ours)
65.3
33

SiamRCNN
[39]
64.8
5

ARDiMP50
(ours)
60.2
46

PrDiMP
[8]
59.8
30

LTMU
[5]
57.2
13

DiMP50
[2]
56.8
59

Ocean
[48]
56.0
25

ARSiamRPN
(ours)
56.0
50

SiamAttn
[47]
56.0
45

SiamFC++
[45]
54.4
90

Table 4. Latency and speed of different methods. The tracking
speed is measured using frame per second (fps).

Method

ECO
RTMDNet
ATOM
SiamRPNpp
DiMP50
DiMPsuper

Base

Base+AR

latency
13.3ms
14.3ms
16.8ms
14.9ms
16.7ms
25.2ms

fps
75.2
69.9
59.5
67.1
59.9
39.7

latency
18.9ms
20.1ms
22.1ms
20.0ms
21.9ms
30.4ms

fps
52.9
49.8
45.2
50.0
45.7
32.9

∆t

+5.6ms
+5.7ms
+5.3ms
+5.1ms
+5.2ms
+5.2ms

4.3. Ablation Studies

In this subsection, we conduct ablation studies of our
Alpha-Reﬁne (AR) module using SiamRPNpp [23] as the
base tracker, evaluated on the LaSOT [11] test set.

Table 5. Analysis of different head options. The best three results
are marked in red, green and blue bold fonts, respectively. Num-
bers are shown in percentage (%).

Method
SiamRPNpp
+ARrpn
+ARrcnn
+ARc
+ARrpn+m
+ARrcnn+m
+ARc+m

AUC(%) PNorm(%) P(%)
47.2
51.2
46.9
55.3
54.7
52.3
57.4

54.7
55.5
54.2
60.3
60.3
58.1
62.2

47.6
50.2
48.9
54.6
53.7
51.6
55.9

Head Options. The head option is a very important com-
ponent in this work, since it is directly related with the ﬁnal
output. Table 5 reports the performance of the SiamRP-
Npp+AR tracker with different head options. The symbols
ARrpn, ARrcnn and ARc denote the Alpha-Reﬁne module
with RPN style box head, RCNN style box head and Key-
Point style corner head, respectively. ‘+m’ stands for the
auxiliary mask head used during training. From Table 5,
we have the following two conclusions: (1) all adopted box
estimation heads improve the original SiamRPNpp method,
and the corner head performs much better than the other
two; and (2) the auxiliary mask head further makes addi-
tional improvements, and the combination of the corner and
mask heads obtains the best performance. Thus, we chose
ARc+m as our ﬁnal module in this work.
Feature Fusion Options. Table 6 compares the SiamRP-
Npp+AR variants using different feature fusion options
(naive, depthwise, or pixelwise in Sec 3.2), where the pre-
diction head is determined based on aforementioned discus-

Table 6. Analysis of different feature fusion types. Naive indicates
the typical feature correlation between reference and test branches.
Numbers are shown in percentage (%).

Method
Pixelwise
Depthwise
Naive

AUC(%) PNorm(%) P(%)
57.4
55.8
53.9

62.2
60.8
59.4

55.9
54.8
53.1

sions. The results show that the adopted pixelwise correla-
tion performs the best, indicating that the pixelwise correla-
tion is better at extracting and maintaining spatial informa-
tion than the depthwise correlation or naive correlation.

Table 7. Comparison of different reﬁnement modules. The best
result is marked in red bold fonts.

Method
SiamRPNpp
+IoU-Net
+SiamMask
+AR

AUC(%) PNorm(%) P(%)
47.2
47.8
48.7
57.4

54.7
55.6
54.7
62.2

47.6
48.8
50.3
55.9

Comparison with Different Reﬁnement Modules. We
compare our Alpha-Reﬁne (AR) with two recent reﬁnement
modules (IoU-Net presented in [7, 2] and SiamMask pro-
posed in [42]), and report the results in Table 7. Our Alpha-
Reﬁne module surpasses IoU-Net and SiamMask by a large
margin.
Different Backbones. We investigate the Alpha-Reﬁne
module with different backbones and reports the compari-
son results in Table 8. When the ResNet-18 backbone is
used, the latency of our AR model is very low but the corre-
sponding performance is also 7.4% higher than the original
SiamPRNpp. As the backbone goes deeper, the AUC score
is better but the speed is slower. In this work, we choose
ResNet-34 as the default backbone to balance accuracy and
speed.

Table 8. Accuracy and Speed Comparison of SiamRPNpp+AR
with different backbones.

Method
SiamRPNpp
+ AR(ResNet-50)
+ AR(ResNet-34)
+ AR(ResNet-18)

AUC(%)
47.6
56.2
55.9
55.0

fps
67.1
46.5
50.0
52.4

latency
14.9ms
21.5ms
20.0ms
19.1ms

∆t

6.6ms
5.1ms
4.2ms

4.4. Evaluation on Other Benchmarks

TrackingNet. TrackingNet [28] is a popular large-scale

Table 9. Comparison results on the TrackingNet test set. ‘Base’:
the base tracker; and ‘Base+AR’: the base tracker with Alpha-
Reﬁne. The best three results are marked in red, green and blue
bold fonts, respectively. Numbers are shown in percentage (%).

Method

ECO
RT-MDNet
ATOM
SiamRPNpp
DiMP50
DiMPsuper

Base
AUC PNorm
71.0
61.2
69.4
58.4
77.1
70.3
80.0
73.3
80.1
74.0
82.5
77.6

Base+AR
AUC PNorm
80.0
75.1
81.0
76.0
82.5
77.7
83.7
78.8
84.1
79.5
85.6
80.5

P
71.4
72.3
74.5
76.4
76.5
78.3

P
55.9
53.3
64.8
69.4
68.7
72.6

short-term tracking benchmark. We evaluate various meth-
ods on its test set, which contains 511 sequences. For the
test set, only groundtruth of the ﬁrst frame is given and par-
ticipants need to submit their results to the evaluation server.
Table 9 shows that our Alpha-Reﬁne module improves dif-
ferent base trackers by a large margin. ARDiMPsuper ob-
tains 80.5% in the main AUC metric, which is slightly
worse than the previous best tracker (Siam R-CNN [39]:
81.2% in AUC). However, ARDiMPsuper runs approxi-
mately 32.9 fps, being six times faster than Siam R-CNN.

GOT-10K. GOT-10K [14] is a recent large-scale dataset,
which contains 10K sequences for training and 180 for test-
ing. We submit the tracking outputs to the ofﬁcial evalua-
tion server and obtain the comparison results (i.e., AO and
SRT) in Table 10. On one hand, our Alpha-Reﬁne module
consistently and signiﬁcantly improves the base trackers in
all evaluation metrics. On the other hand, ARDiMPsuper
achieves 70.1% in the main AO metric, which performs
much better and runs much faster than the previous best
tracker (Siam R-CNN [39]: 64.9% in AO, 72.8% in SRT,
and 59.7% in SR0.75).

Table 10. Comparison results on the GOT-10K test set.
‘Base’:
the base tracker; and ‘Base+AR’: the base tracker with Alpha-
Reﬁne. The best three results are marked in red, green and blue
bold fonts, respectively. Numbers are shown in percentage (%).

Method

ECO
RT-MDNet
ATOM
SiamRPNpp
DiMP50
DiMPsuper

Base
AO SR0.5
43.8
41.3
35.8
35.0
62.2
53.5
61.7
51.8
71.8
60.3
78.8
67.2

Base+AR

SR0.75 AO SR0.5
64.8
56.7
13.4
63.7
56.1
9.2
71.1
63.1
37.8
69.6
61.5
32.4
74.3
65.4
46.0
80.0
70.1
59.3

SR0.75
46.1
46.9
55.8
46.9
58.5
64.2

VOT2020. VOT2020 [19] includes 60 challenging videos
with high-quality mask-based ground truth. This bench-
mark takes expected average overlap (EAO) as the main
ranking metric, which simultaneously considers the track-
ers’ accuracy and robustness. The evaluation on VOT2020
has two settings: Baseline and real-time. The real-time re-
quires the trackers to predict bounding boxes no slower than

Table 11. Comparison results on the VOT2020 benchmark. ‘Base’:
the base tracker; and ‘Base+AR’: the base tracker with Alpha-
Reﬁne. The best three results are marked in red, green and blue
bold fonts, respectively. The main EAO metric is reported.
Base+AR
Base

Method

Baseline Real Time Baseline Real Time

RT-MDNet
SiamRPNpp
ECO
ATOM
DiMP50
DiMPsuper

0.248
0.254
0.280
0.275
0.286
0.314

0.247
0.254
0.276
0.279
0.278
0.311

0.371
0.395
0.426
0.416
0.444
0.471

0.356
0.395
0.426
0.414
0.438
0.478

the video frame rate (20 fps in the ofﬁcial toolkit). The re-
sults of different trackers are shown in Table 11. We can see
that the proposed Alpha-Reﬁne module improves the base
trackers in terms of EAO signiﬁcantly. Besides, Figure 8
demonstrates that our AR strengthened method ARDiMP-
super obtains the best performance in the Real-Time set-
ting.

Figure 8. State-of-the-art evaluation on VOT2020. Our ARDiMP-
super obtains the best result in the real-time setting.

5. Conclusion.

In this work, we propose a novel Alpha-Reﬁne method
for visual tracking, which is an accurate and general re-
ﬁnement module to effectively improve the tracking perfor-
mance of different types of trackers in a plug-and-play style.
By exploring multiple design options, we ﬁnd that extract-
ing and maintaining precise spatial information is the key
to the precise box estimation. Alpha-Reﬁne ﬁnally adopts a
precise pixel-wise correlation layer, a Key-Point style pre-
diction head, and an auxiliary mask head. Finally, we ap-
ply the Alpha-Reﬁne model to six well-known and top-
performed trackers and conduct numerous evaluations on
four popular benchmarks. The experimental results demon-
strate that our Alpha-Reﬁne could consistently improve the
tracking performance with few computational loads.

Acknowledgement. This work was supported in part by the
National Natural Science Foundation of China under Grant
nos. 62022021, 61806037, 61872056, and 61725202, and
in part by the Science and Technology Innovation Founda-
tion of Dalian under Grant no. 2020JJ26GX036.

160.30.350.40.450.5Real TimeReferences

[1] Luca Bertinetto, Jack Valmadre, Jo˜ao F. Henriques, Andrea
Vedaldi, and Philip H. S. Torr. Fully-convolutional siamese
networks for object tracking. In ECCVW, 2016. 1, 2, 3
[2] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu
Timofte. Learning discriminative model prediction for track-
ing. In ICCV, 2019. 1, 2, 3, 6, 7

[3] Goutam Bhat, Joakim Johnander, Martin Danelljan, Fahad
Shahbaz Khan, and Michael Felsberg. Unveiling the power
of deep tracking. In ECCV, 2018. 1

[4] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang,

and Huchuan Lu. Transformer tracking. In CVPR, 2021. 2

[5] Kenan Dai, Yunhua Zhang, Dong Wang, Jianhua Li,
Huchuan Lu, and Xiaoyun Yang. High-performance long-
term tracking with meta-updater. In CVPR, 2020. 7

[6] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Michael Felsberg. ECO: Efﬁcient convolution operators for
tracking. In CVPR, 2017. 1, 2, 6

[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Michael Felsberg. ATOM: Accurate tracking by overlap
maximization. In CVPR, 2019. 1, 2, 3, 6, 7

[8] Martin Danelljan, Luc Van Gool, and Radu Timofte. Proba-
bilistic regression for visual tracking. In CVPR, 2020. 7
[9] Fei Du, Peng Liu, Wei Zhao, and Xianglong Tang.
Correlation-guided attention for corner detection based vi-
sual tracking. In CVPR, 2020. 2

[10] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qing-
ming Huang, and Qi Tian. CenterNet: Keypoint triplets for
object detection. In ICCV, 2019. 4

[11] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia
Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.
LaSOT: A high-quality benchmark for large-scale single ob-
ject tracking. In CVPR, 2019. 2, 5, 6, 7

[12] Heng Fan and Haibin Ling. Siamese cascaded region pro-
posal networks for real-time visual tracking. In CVPR, 2019.
1, 2, 3

[13] Jo˜ao F. Henriques, Rui Caseiro, Pedro Martins, and Jorge
Batista. High-speed tracking with kernelized correlation ﬁl-
ters. In ICVS, 2008. 2

[14] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A
large high-diversity benchmark for generic object tracking in
the wild. TPAMI, 2019. 2, 5, 6, 8

[15] Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, and Yun-
ing Jiang. Acquisition of localization conﬁdence for accurate
object detection. In ECCV, 2018. 3

[16] Ilchae Jung, Jeany Son, Mooyeol Baek, and Bohyung Han.

Real-time MDNet. In ECCV, 2018. 2, 6

[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015. 6

[18] Tao Kong, Fuchun Sun, Huaping Liu, Yuning Jiang, Lei Li,
and Jianbo Shi. FoveaBox: Beyound anchor-based object
detection. TIP, 2020. 2

[19] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-
berg, Roman Pﬂugfelder, Joni-Kristian Kamarainen, Luka
ˇCehovin Zajc, Martin Danelljan, Alan Lukezic, Ondrej Dr-
bohlav, Linbo He, Yushan Zhang, Song Yan, Jinyu Yang,

Gustavo Fernandez, and et al. The eighth visual object track-
ing vot2020 challenge results. In ECCVW, 2020. 2, 6, 8
[20] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-
berg, Roman Pﬂugfelder, Luka ˇCehovin Zajc, Tomas Vojir,
Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey, et al.
The sixth visual object tracking vot2018 challenge results. In
ECCVW, 2018. 5

[21] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Fels-
berg, Roman Pﬂugfelder, Joni-Kristian Kamarainen, Luka
ˇCehovin Zajc, Ondrej Drbohlav, Alan Lukezic, Amanda
Berg, et al. The seventh visual object tracking vot2019 chal-
lenge results. In ICCVW, 2019. 2, 3, 5

[22] Hei Law and Jia Deng. CornerNet: detecting objects as

paired keypoints. In ECCV, 2018. 2, 4

[23] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing,
and Junjie Yan. SiamRPN++: Evolution of siamese visual
tracking with very deep networks. In CVPR, 2019. 1, 2, 3,
6, 7

[24] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.
High performance visual tracking with siamese region pro-
posal network. In CVPR, 2018. 2, 3

[25] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.
Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva
Ramanan, Piotr Doll´ar, and C. Lawrence Zitnick. Microsoft
COCO: Common objects in context. In ECCV, 2014. 5
[26] Alan Lukezic, Jiri Matas, and Matej Kristan. D3S - A dis-
criminative single shot segmentation tracker. In CVPR, 2020.
2, 5

[27] Diogo C. Luvizon, Hedi Tabia, and David Picard. Human
pose regression by combining indirect part detection and
contextual information. Computers & Graphics, 2019. 4
[28] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-
subaihi, and Bernard Ghanem. TrackingNet: A large-scale
dataset and benchmark for object tracking in the wild.
In
ECCV, 2018. 2, 6, 7

[29] Hyeonseob Nam and Bohyung Han. Learning multi–domain
convolutional neural networks for visual tracking. In CVPR,
2016. 1, 2

[30] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017. 6

[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R–CNN: towards real-time object detection with re-
gion proposal networks. In NIPS, 2015. 2, 3, 4

[32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
Net: convolutional networks for biomedical image segmen-
tation. In MICCAI, 2015. 5

[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, and Michael Bernstein.
ImageNet Large
scale visual recognition challenge. IJCV, 2015. 5

[34] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical
image saliency detection on extended cssd. TPAMI, 2015. 5
[35] Chong Sun, Dong Wang, Huchuan Lu, and Ming-Hsuan
Yang. Correlation tracking via joint discrimination and re-
liability learning. In CVPR, 2018. 1, 2

[36] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS
Torr, and Timothy M Hospedales. Learning to compare: Re-
lation network for few-shot learning. In CVPR, 2018. 3
[37] Ran Tao, Efstratios Gavves, and Arnold W. M. Smeulders.

Siamese instance search for tracking. In CVPR, 2016. 1, 2

[38] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:
In ICCV,

Fully convolutional one-stage object detection.
2019. 2

[39] Paul Voigtlaender, Jonathon Luiten, Philip H. S. Torr, and
Bastian Leibe. Siam R-CNN: visual tracking by re-detection.
In CVPR, 2020. 6, 7, 8

[40] Guangting Wang, Chong Luo, Zhiwei Xiong, and Wenjun
Zeng. SPM-tracker: Series-parallel matching for real-time
visual object tracking. In CVPR, 2019. 1, 2

[41] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,
Dong Wang, Baocai Yin, and Xiang Ruan. Learning to de-
tect salient objects with image-level supervision. In CVPR,
2017. 5

[42] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and
Philip H. S. Torr. Fast online object tracking and segmen-
In CVPR, 2019. 2, 3, 5, 6,
tation: A unifying approach.
7

[43] Ziqin Wang, Jun Xu, Li Liu, Fan Zhu, and Ling Shao.
RANet: Ranking attention network for fast video object seg-
mentation. In ICCV, 2019. 3

[44] Ning Xu, Linjie Yang, Yuchen Fan,

Jianchao Yang,
Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen,
and Thomas Huang. YouTube-VOS: Sequence-to-sequence
video object segmentation. In ECCV, 2018. 5

[45] Yinda Xu, Zeyu Wang, Zuoxin Li, Yuan Ye, and Gang Yu.
SiamFC++: Towards robust and accurate visual tracking
with target estimation guidelines. In AAAI, 2020. 2, 7
[46] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and
Ming-Hsuan Yang. Saliency detection via graph-based man-
ifold ranking. In CVPR, 2013. 5

[47] Yuechen Yu, Yilei Xiong, Weilin Huang, and Matthew R.
Scott. Deformable siamese attention networks for visual ob-
ject tracking. In CVPR, 2020. 7

[48] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and
Weiming Hu. Ocean: Object-aware anchor-free tracking. In
ECCV, 2020. 7

[49] Xingyi Zhou, Dequan Wang, and Philipp Kr¨ahenb¨uhl. Ob-
jects as points. arXiv preprint arXiv:1904.07850, 2019. 2,
4

[50] Xingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl.
Bottom-up object detection by grouping extreme and center
points. In CVPR, 2019. 4

[51] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and
Weiming Hu. Distractor-aware siamese networks for visual
object tracking. In ECCV, 2018. 2, 3

