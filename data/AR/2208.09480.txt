NeuralLightFieldEstimationforStreetSceneswithDiﬀerentiableVirtualObjectInsertionZianWang1,2,3,WenzhengChen1,2,3,DavidAcuna1,2,3,JanKautz1,andSanjaFidler1,2,31NVIDIA2UniversityofToronto3VectorInstitute{zianw,wenzchen,dacunamarrer,jkautz,sfidler}@nvidia.comAbstract.Weconsiderthechallengingproblemofoutdoorlightingestimationforthegoalofphotorealisticvirtualobjectinsertionintophotographs.Existingworksonoutdoorlightingestimationtypicallysimplifythescenelightingintoanenvironmentmapwhichcannotcapturethespatially-varyinglightingeﬀectsinoutdoorscenes.Inthiswork,weproposeaneuralapproachthatestimatesthe5DHDRlightﬁeldfromasingleimage,andadiﬀerentiableobjectinsertionformulationthatenablesend-to-endtrainingwithimage-basedlossesthatencouragerealism.Speciﬁcally,wedesignahybridlightingrepresentationtailoredtooutdoorscenes,whichcontainsanHDRskydomethathandlestheextremeintensityofthesun,andavolumetriclightingrepresentationthatmodelsthespatially-varyingappearanceofthesurroundingscene.Withtheestimatedlighting,ourshadow-awareobjectinsertionisfullydiﬀerentiable,whichenablesadversarialtrainingoverthecompositedimagetoprovideadditionalsupervisorysignaltothelightingprediction.Weexperimentallydemonstratethatourhybridlightingrepresentationismoreperformantthanexistingoutdoorlightingestimationmethods.WefurthershowthebeneﬁtsofourARobjectinsertioninanautonomousdrivingapplication,whereweobtainperformancegainsfora3Dobjectdetectorwhentrainedonouraugmenteddata.Keywords:LightingEstimation,ImageEditing,AugmentedReality1IntroductionInthiswork,weaddressthetaskofoutdoorlightingestimationfrommonocularimagery,speciﬁcallyfocusingonstreetscenes,asshowninFig.1.Thisisanimportanttask,asitenablesvirtualobjectinsertionthatcancatertomanydownstreamdomains[8,28,18,35,10],suchasvirtuallyinsertingnewlyplannedbuildingsforarchitecturalvisualization,realisticallyrenderinggamecharactersintosurroundings,orasawaytoaugmentrealdatasetswithobjectsthatareotherwisehardtorecordintherealworld,suchasroaddebrisandexoticanimals,forthepurposeoftrainingmorerobustandperformantcomputervisionmodels.LightingestimationforARapplicationsneedstoaccountforcomplex5Dlighttransport[1],i.e.afunctionofspatiallocationandviewingdirection.WitharXiv:2208.09480v1  [cs.CV]  19 Aug 20222Z.Wangetal.Fig.1.Weestimatelightingandperformvirtualobjectsinsertioninrealstreetscenery.Weinsertcarsandheavyvehicles(top),andcompositerarebutsafety-criticalscenarioswithgarbage,constructionsite,adog,anddebris(bottom).3DassetsprovidedcourtesyofTurboSquidandtheirartistsHum3D,befast,rabser,FirelightCGStudio,amaranthusand3DTree_LLC.usuallyalimitedﬁeld-of-viewobservedfrominput,thetaskofestimatingthelightﬁeldischallengingandill-posed.Anadditionalchallengeencounteredforoutdoorscenes,incontrasttoindoorscenes,istheextremehighdynamicrange(HDR)ofthesun,whichiscriticaltoestimatecorrectlyinordertorendercastshadows.Existingliteratureonlightingestimationusuallytacklesasimpliﬁedproblemsetting.[37,34]focusonspatially-varyingeﬀectsbutdonothandleHDRintensities.Incontrast,methodsthatfocusonHDRandpredictparametricsky[17,39]orutilizelearnedskymodels[16]typicallyignorethespatially-varyingeﬀectsandlackhigh-frequencydetails.Theselimitationsnotonlyresultininaccuratelightingestimationbutalsohampervirtualobjectinsertioneﬀects.Inthispaper,weproposeauniﬁedapproachthatovercomesthepreviouslymentionedlimitations,estimatingtheHDRscenelightﬁeldfromasingleimage.Tailoredtooutdoorscenes,weestimateahybridlightingrepresentationthatcomprisesoftwocomponents:anHDRskydomeandavolumetriclightingrepresentationforthesurroundingscene.Weemployalearnedlatentvectortorepresenttheskydomeinspiredby[16],whichcanbedecodedintoanHDRenvironmentmapthatisdesignedtomodelthestrongintensityofthesun.WeadoptthevolumetricsphericalGaussianrepresentation[37]torepresentthenon-inﬁnitysurroundingssuchasroadandbuildings.Thetwocomponentsnaturallycombinewithvolumerenderinganddemonstratesuperiorityoverpriorworks[16,37].Wefurtherdesignaphysics-basedobjectinsertionformulationthatrenderstheinsertedobjectsandtheirshadowscastonthescene.Weutilizeray-tracingtocapturethesecond-orderlightingeﬀects,whichisfullydiﬀerentiablewithrespecttothelightingparameters.Wetrainourmethodwithsupervisedandself-supervisedlosses,andshowthatadversarialtrainingoverthecompositedARimagesprovidescomplementarysupervisorysignaltoimprovelightingestimation.Ourmethodoutperformspriorworkinthetasksoflightingestimationandphotorealisticobjectinsertion,whichweshowthroughnumericalresultsandauserstudy.WefurthershowcaseourvirtualobjectinsertionthroughtheNeuralLightFieldEstimationforStreetScenes3applicationof3Dobjectdetectioninautonomousdriving.Ourapproach,whichcanrendersynthetic3Dobjectsintorealimageryinarealisticway,providesusefuldataaugmentationthatleadstonotableperformancegainsoverthevanilladataset,andanaiveinsertionmethodthatdoesnotaccountforlighting.2RelatedWorkLightingestimationaimstopredictanHDRlightﬁeldfromimageobser-vations.Duetotheill-posednatureoftheproblem,priorworksoftentackleasimpliﬁedtaskandignorespatially-varyingeﬀects,usinglightingrepresentationssuchassphericallobes[3,24],lightprobes[21],skyparameters[16,17,39],anden-vironmentmaps[12,31,38,32].Recentworksexploredvariousrepresentationsforcapturingspatially-varyinglighting,includingper-pixelsphericallobes[13,23,41],lightsourceparameters[11],per-locationenvironmentmap[33,43]and3Dvolu-metriclighting[34,37].TheseworksusuallytrainwithsyntheticdataduetothescarcityofavailablegroundtruthHDRlightﬁeldonreal-worldcaptures.OutdoorscenesrequirespecialattentionfortheextremeHighDynamicRange(HDR)lightingintensity,e.g.thesun’sintensity,whichisseveralordershigherinmagnitudethantypicallightsourcesfoundinindoorscenes.Priorworksemployedskyparameters[17,39],orlearnedskymodels[16]withanencoder-decoderarchitectureformodelingHDRsky.However,theseworkstypicallyonlyfocusonmodelingthesky,andignorethehigh-frequencyandspatially-varyingeﬀects,whichareequallyimportanttogetrightforARapplications.Inthiswork,weproposeauniﬁedrepresentationthatcanhandlebothHDRskyintensityaswellasspatiallyvaryingeﬀectsinoutdoorscenestoachievebetterperformance.Self-supervisedlightingestimationmethodsapplydiﬀerentiableren-deringtoprovidegradientforlightingestimation[6,7,40,22,27].Diﬀerentiablerasterization-basedrenderers[6,40,7]aretypicallylimitedtoimagesofsingleob-jectsandignoresthespatially-varyingeﬀects.Physically-basedrendering(PBR)methods[22,27]requireintensivememoryandrunningtime,andarethuslimitedtooptimizationtasks.Notethatexistingdiﬀerentiablerenderers[6,7,27]typicallydonotprovidedirectfunctionalityforobjectinsertion,whichisanimageeditingtask.Weproposeanoveldiﬀerentiableobjectinsertionformulation,providingvaluablesupervisionsignalforlightingestimation.Imagemanipulation.Relatedtooursisalsoworkthataimstoinsertsyn-theticobjectsintoimagesusingalternativetechniques,suchasadversarialmeth-ods[25,20],orbyperturbingrealscenesusingrecentadvancesinneuralrender-ing[28].Alhaijaetal.[2]assumesknownlightingandproposetouseARasadatagenerationtechniquebyinsertingsyntheticassetsintorealworldscenes.Naivecopy-pasteobjectinsertionhasalsobeenshowntoboostdownstreamobjectrecognitionaccuracy[35,10].NeuralSceneGraph[28]optimizeneuralimplicitfunctionsforeachobjectinthescene.Despiterealisticeditingresults,lightingeﬀectsarebakedintotherepresentationandthusswappingassetsfromonescenetoanotherisnoteasilypossible.GeoSim[8]reconstructsassetssuchascarsfromreal-worlddrivingsequences,andinsertsthemintoagivenimage4Z.Wangetal.Hybrid LightingVirtual ObjectDifferentiable Object InsertionEditing ResultReal World ImageDiscriminator“Fake”“Real”(b) Differentiable Object InsertionLightingFeature3D CNNResNetLightingVolumeHDRSkyDomeSky VectorSkyDecoderImageDepth(a) Hybrid Lighting Joint EstimationFig.2.Modeloverview.Ourmonocularlightingestimationmodel(a)predictsahybridlightingrepresentationcontaininganHDRskydome(top)representingskyandsunatinﬁnity,andalightingvolume(bottom)representingthesurroundingscene.Thedepth(a,left)forlightingvolumepredictioncomesfromoﬀ-the-shelfmonoculardepthestimator[14].Withthepredictedlighting,ourobjectinsertionmodule(b)rendersa3Dassetintoagivenimageandisfullydiﬀerentiablew.r.t.lightingparameters,thusenablingend-to-endtrainingwithadversarialobjectiveforphotorealism.usingaclassicalrendererfollowedbyashallowneuralrendererthatﬁxeserrorssuchasunrealisticcompositingeﬀects.Whileachievingimpressiveresultsoncarinsertion,itremainsdiﬃculttoapplyonlessfrequentobjects.Incontrast,ourmethodsupportsinserting3Dassetsofvariousclasses(Fig.1).3MethodWeaimtoestimatescenelightingsoastorendersynthetic3Dassetsintoimagesrealistically.Inwhatfollows,weintroducethehybridrepresentation,whichcomprisesanHDRskydomeandavolumetricscenelighting(Sec.3.1),thehybridlightingpredictionmodel(Sec.3.2,Fig.2a),thediﬀerentiableobjectinsertionmodulethatrendersavirtualobjectintoanimage(Sec.3.3,Fig.2b),andthetrainingschema(Sec.3.4).3.1HybridLightingRepresentationOurgoalistomodelthe5Dlightﬁeld,whichmapsaspatiallocationx∈R3andlightdirectionl∈S2intoanHDRradiancevaluer∈R3+.Incontrasttoindoor,outdoorscenesrequiresimultaneouslymodelingtheextremeHDRskyaswellasthesurroundingenvironment.Thepeakmagnitudeoftheformer(sun)canbeseveralordershigherthanthelatter.Toaddressthis,weproposetouseahybridlightingrepresentationthatseparatelymodelstheskyatinﬁnityandthesurroundingscene.Thisdecompositionallowsustocaptureboth,theextremeintensityoftheskywhilepreservingthespatially-varyingeﬀectsofthescene.HDRskyrepresentation.Theskydometypicallycontainsarelativelysimplestructure,i.e.sun,sky,andpossiblyclouds,whichaﬀordsamuchlowerdimensionalrepresentationthanthatofatypicalenvironmentmap.Thus,insteadofdirectlypredictingahighresolution2Denvironmentmap,welearnafeaturespaceofthesky,andrepresenttheskydomewithaskyfeaturevectorf∈Rd,NeuralLightFieldEstimationforStreetScenes5HDRSkyDomeLDRPanoramaSkyEncoderSkyDecoderSky LatentPeak DirectionPeak IntensityFig.3.SkymodelingnetworktakesasinputanLDRpanoramaandproducesanHDRskywithanencoder-decoderstructure,wherethenetworkalsolearnstocompressskyinformationintotheintermediatevectorrepresentationf∈Rd.Theskyvectorconsistsofexplicitfeatureofpeakintensityanddirection,andalatentfeaturevector.whichcanfurtherbepassedtoapretrainedCNNdecodertodecodeintoanHDRenvironmentmap,asshowninFig.2a(top).TheskyfeaturespacelearningisdescribedinSec.3.2,skymodelingpart.Spatially-varyingsurroundingscenerepresentation.Theoutdoorscenesgenerallyconsistofcomplexgeometricstructuresresultinginlocation-dependentlightingeﬀectslikeshadowsandreﬂections,whichcannotbesimplymodeledasanenvironmentmap.Toaddressthis,weuseavolumetricsphericalGaussian(VSG)[37]torepresentthenearbysurroundingscene.VSGisa8-channelvolumetrictensorLVSG∈R8×X×Y×Z,augmentingtheRGBαvolumewithview-dependentsphericalGaussianlobes.EachvoxelofVSGcontainsasphericalGaussianlobeG(l)=ce−(1−l·µ)/σ2,wherelisviewingdirectionandξ={c,µ,σ}are7-dimensionalparameterstomodeltheexitingradianceofthecorresponding3Dlocation.Eachvoxelalsohasanalphachannelα∈[0,1]torepresentoccupancy.The5Dlightﬁeldcanbequeriedwithvolumerenderingwhichwedetailbelow.Radiancequeryfunction.Withourhybridlightingrepresentationutilizingbothskydomeandvolumetriclighting,thelightingintensityatany3Dpointalonganyraydirectioncanbequeried.Tocomputetheradianceofaraythatstartsinsidethevolume,weﬁrstshoottheraythroughthelightingvolumeLVSGandﬁnallyhittheHDRskydomeLenv.Weadoptalphacompositingtocombinethetwolightingeﬀects.Speciﬁcally,tocomputethelightingintensityfortherayemittedfromthelocationx∈R3inthedirectionl∈S2,weselectKequi-spacedlocationsalongtherayandusenearestneighborinterpolationtogetthevoxelvalues{αk,ξk}Kk=1fromLVSG.WethenquerytheintensityofLenvinthedirectionl,referredtoasLenv(l),viabilinearinterpolation.TheﬁnalHDRlightintensityL(x,l)∈R3+canbecomputedusingvolumerendering:L(x,l)=(cid:16)KXk=1τk−1αkG(−l;ξk)(cid:17)+τKLenv(l)(1)whereτk=Qki=1(1−αi)isthetransmittance.ThisfunctionwillbeusedforrenderingobjectinsertioninEq.2andshadowsinEq.4.3.2NetworkArchitectureInthissection,weintroducethenetworkarchitectureforthepre-trainedskymodelandlightingprediction.6Z.Wangetal.Skymodeling.Tolearntheskyfeaturespace,wedesignaskymodelingnetworkasshowninFig.3.Speciﬁcally,theencodercompressestheinputLDRpanoramaintoafeaturevector,thenthedecoderdecodeittotheoriginalHDRskydome.OurskyfeaturevectorfcontainsexplicitHDRpeakintensityfintensity∈R3,peakdirectionfdir∈R3,andalatentvectorflatent∈Rd−6encodingthecontentoftheskydome.Thisdesignallowsustocontrolthelightingofthesunbycontrollingfdirandfintensity,whichisconvenientforapplicationsthatallowmanualediting.Wepre-traintheskyencoder-decodernetworkonasetofoutdoorHDRpanoramasandkeepitfreezed.Oncetrained,weintegratetheﬁxeddecoderintotheskypredictionbranch(Fig.2atop).Thepre-trainedskymodelingnetwork,whichmapsanLDRpanoramaintoHDR,willalsobeusedtogenerateHDRpseudolabelsforsupervision(Sec.3.4)duetothelackofHDRdata.Hybridlightingprediction.AsshowninFig.2,weuseatwo-branchnetworktopredicttheskydomeandthelightingvolume.Wedetaileachbranchbelow.HDRskypredictionbranch.Givenaninputimage,theskybranchdirectlypredictstheskyfeaturevectorffromtheResNet[15]backbone.Thepre-trainedﬁxedskydecoderthenmapsftoanHDRskyenvironmentmap.Lightingvolumepredictionbranch.Weadaptfromasubnetworkof[37]forVSGprediction.Speciﬁcally,weﬁrstuseanMLP[26]tomapthelightingfeatureextractedbyResNetbackboneintoafeaturevolume,andthenunprojecttheinputimageintoaRGBαvolume.Weadopta3DUNettofusethetwovolumesandpredicttheVSGlightingrepresentation.Sinceunprojectionrequiresdepthinformation,weadoptanoﬀ-the-shelfmonoculardepthestimatorPackNet[14]topredictadensedepthmap.ArchitecturedetailsareincludedintheAppendix.3.3DiﬀerentiableObjectInsertionWenowstudythetaskofrealisticobjectinsertion,whichiskeyinARandourmainﬁnalobjective.GivenanestimatedlightingrepresentationL,ourgoalistocompositeavirtualobjectwithknowngeometryMandmaterialΘintoarealimageIwithknowndepthDandcameraintrinsics.Toachieverealisticlightingeﬀectsinthisprocess,notonlytheinsertedobjectshouldbeinﬂuencedbythescenelighting,butitshouldalsoaﬀectthescenetocreatecastshadows.Inourwork,weaimtomakethisinsertionmodulediﬀerentiable,includingshadowrendering,suchthatitcanaﬀordgradientbackpropagationfromthelossesdeﬁnedonthecompositedimage,backtoourlightingparameters.Sincegroundtruthlightﬁeldisnoteasilyavailableforoutdoorscenes,wearguethataplausiblecomplementarysupervisionsignalistousethequalityofobjectinsertionasanobjective.Thus,carefullydesignedadversarialtrainingonthecompositeimagescanbeapowerfulapproachtosuperviselightingestimation.Wealsoarguethatevenifgroundtruthlightinginformationwouldbeavailable,optimizingforthequalityofobjectinsertionend-to-endwilllikelyleadtoimprovedresults.Foregroundobjectappearancerendering.WerenderthevirtualobjectwithourpredictedhybridlightingrepresentationLusingaphysically-basedrenderer.WeadopttheDisneyBRDF[4,19,7]forenhancedrealism.NeuralLightFieldEstimationforStreetScenes7Speciﬁcally,weﬁrstshootraysfromthecameraorigintothescene,whereweapplyray-meshintersectiondetectionfortheraysandtheinsertedobjectM.Foreachintersectedray,wecreateaG-buﬀerforthelocationoftheintersectionx,thesurfacenormalnandthematerialpropertiesθ.WebouncemultipleraysatxandrenderwithMonte-Carlonumericalintegration:Ix=1NNXk=1f(lk,v;θ)L(x,lk)(n·lk)+p(lk)(2)whereL(·,·)istheradiancequeryfunctionasdeﬁnedinEq.1,Nandlkisthenumberanddirectionofsampledlighting,vistheviewingdirectionofthecameraray,andfistheDisneyBRDF.Backgroundshadowmaprendering.Theinsertedobjectchangesthelighttransportinthesceneandaﬀectstheappearanceofthebackgroundscenepixels,whichtypicallycausesshadows.Weadoptraytracingtogeneratefaithfulratioshadowmapsfortheinsertedobject,inspiredbyclassicratioimagingtechniques[29].Speciﬁcally,foreachscenepixelp,wecomputeits3DlocationxfromthedepthmapD.Weﬁrstcomputethelightingdistributionbeforeobjectinsertion{L(xs,lk)}Nsk=1,where{lk}Nsk=1areuniformlyselectedlightdirectionsontheupperhemisphere.Afterobjectinsertion,theraysmaypotentiallygetoccludedbytheinsertedobjects,resultinginapost-insertionlightingdistribution{L0(xs,lk)}Nsk=1.Tocompute{L0(xs,lk)}Nsk=1,weperformray-meshqueryforallrays.Ifray(xs,lk)isoccludedbytheinsertedobject,wesetthelightingintensityvaluetoanambientvalueIawhichweempiricallysetto0.1,whilethelightingintensitiesofunoccludedraysremainthesameastheoriginalradiance.Wethendeﬁnetheshadoweﬀectsastheratioofthepixelintensityvaluesbefore(I)andafter(I0)objectinsertion,Sp=I0pIp=PNsk=1fscene(xs,lk,v;θ)L0(xs,lk)(n·lk)+PNsk=1fscene(xs,lk,v;θ)L(xs,lk)(n·lk)+(3)wheretheBRDFofthescenepixelfsceneandnormaldirectionnareunknown.Asourobjectinsertionandshadowsoccuronﬂatsurfacesintypicalstreetscenesweconsider,wesimplifyitbyassumingthenormaldirectionispointingupward,andassumethescenesurfaceisLambertianwithconstantdiﬀusealbedofscene(xs,lk,v)=fd.Asaresult,wecanmovetheBRDFtermoutsidethesuminEq.3andcancelitouttoobtainasimplerterm:Sp=fdNsPk=1L0(xs,lk)(n·lk)+fdNsPk=1L(xs,lk)(n·lk)+=NsPk=1L0(xs,lk)(n·lk)+NsPk=1L(xs,lk)(n·lk)+(4)whichcanbecomputedwiththeestimatedlightingL.ScenepixelsafterinsertioncanthenbecomputedbymultiplyingtheratioshadowmapI0=S(cid:12)I.8Z.Wangetal.Gradientpropagation.Wedesigntheforwardrenderingprocesstobediﬀer-entiableforbothforegroundobjectandbackgroundshadows,whichallowsustobackpropagategradientsfromimagepixelstothelightingparameters.Foreachforegroundpixel,therenderedappearanceoftheinsertedobjectisdiﬀerentiableviaEq.2.GradientsfrombackgroundpixelsI0withrespecttothelightingL,i.e.∂I0∂L,canbecomputedvia∂I0∂L=∂S∂LI,wheretheshadowratioSinEq.4isalsodiﬀerentiablewrt.lightingL.Intuitively,ifwewanttheshadowsaroundtheobjecttobeperceptuallydarker,thiswillencouragetheoccludedlightdirectionstohavestrongerintensity.3.4TrainingWeﬁrstpre-traintheskymodelingnetworkonacollectionofoutdoorHDRpanoramas,andthenkeepitﬁxedinthefollowingtrainingprocessforourhybridlightingprediction.Thesupervisionforourhybridlightingjointestimationmodulecomesfromtwoparts:(1)thedirectionsupervisionthatlearnslightinginformationfromthetrainingdata,and(2)theadversarialsupervisionthatappliesontheﬁnaleditingresultsandoptimizesforrealism.SkyModelingSupervision.Wetraintheskymodelingencoder-decodernetwork(Fig.3)onacollectionofoutdoorHDRpanoramas.ForeachHDRpanoramaIHDR,wecomputeitsground-truthpeakintensityfintensityanddirec-tionfdir.Wetraintheencoder-decoderwiththeLDR-HDRpair(ILDR,IHDR),wheretheinputLDRpanoramaILDRisconvertedfromtheHDRpanoramaviagammacorrectionandintensityclipping.Wesupervisethenetworkwithacombinationofthreelosses,includingpeakdirectionlossLdirwithL1angularerror,andpeakintensitylossLintensityandHDRreconstructionlossLhdrusinglog-encodedL2errordeﬁnedasLogEncodedL2(ˆx,x)=||log(1+ˆx)−log(1+x)||22.SupervisionforHybridLightingPrediction.Tosuperviselightingpre-diction,weusetwocomplementarydatasets:theself-drivingdatasetnuScenes[5],andthepanoramicstreetviewdatasetHoliCity[42].AsbothdatasetsareLDR,wepredictHDRpseudolabelsfromthepre-trainedmodelingnetwork(Fig.3)byliftingHoliCityLDRpanoramasintoHDR.Inwhatfollows,wedescibethesupervisionfortheskypredictionbranch,thelightingvolumepredictionbranch,andthelosssignaltocombinethetworepresentation.HDRskybranchlosses.WetrainourskybranchonHoliCity[42],whichcontainsLDRpanoramasIpanowithskymasksMskyandsunlocationfdir.TocomputeHDRpseudolabels,wefeedtheLDRpanoramaIpanointothepre-trainedskynetwork,andgettheestimatedskypeakintensity˜fintensityandlatentcode˜flatentaspseudogroundtruthtosuperviseourskypredictionbranch.Inatrainingstep,wecropaperspectiveimageIcropfromthepanoramaasinputtoourlightingestimationnetwork,andpredicttheskyvectoroutput(ˆfintensity,ˆfdir,ˆflatent)andthereconstructedHDRskyimageˆIpano.Weuseacombinationofthelog-encodedL2lossforpeakintensity(ˆfintensity,˜fintensity),L1lossforlatentcode(ˆflatent,˜flatent),L1angularlossforpeakdirection(ˆfdir,fdir),andL1reconstructionlossbetween(ˆIpano(cid:12)Msky,Ipano(cid:12)Msky)withintheLDRskyregionindicatedbyMsky.NeuralLightFieldEstimationforStreetScenes9Lightingvolumebranchloss.Recallthatimagesinthedatasetsareinherentlygroundtruthofasubsetofthelightﬁeldcapturedbycamerasensorrays,e.g.thevideoscapturedbyself-drivingcarsinnuScenes[5]andthepanoramasinHoliCity[42].Meanwhile,thepredictedhybridlightingrepresentationsupportsradiancequeryalongarbitraryraysasshowninEq.1.Thus,wecanenforcetheconsistencyofradiancebetweenourpredictedlightﬁeldandcapturedimagegroundtruth,givenknowncameraposeandintrinsics.Speciﬁcally,wesampleimagesfromnuScenes,andcropperspectiveimagesfromHoliCitypanoramasasinputimages.Wethenpredictthecorrespondinglightingvolumetogetherwiththeskydome,querytheradianceofthecamerarays,andenforceconsistencywithgroundtruthcapturedimagesusingL2loss.Following[37],wealsorenderthealphachannelintoadepthmapandenforceconsistencywithgroundtruthdepth.Skyseparationloss.Intuitively,therealworldcameraraysthatdirectlyreachtheskyshouldalsotransmitthroughthelightingvolumeandhittheskyenvironmentmap.Withthelossesmentionedabove,themodelmaystillfallintothedegeneratecasewherethelightingvolumecompletelyoccludesthesky.Toaddressthis,weusetheskymaskMskyinformationtosupervisetheskytransmittanceτKinEq.1withbinarycrossentropyloss.TrainingLightingviaObjectInsertion.Ourﬁnalgoalistorealisticallyinsertvirtualobjectsintoimages.Weformulatetheobjectinsertionprocessinanend-to-endfashionanduseadiscriminatortosupervisetheperceptuallightingeﬀectsontheimageeditingresults.Speciﬁcally,wecollectasetofhighquality3DcarmodelsfromTurbosquid1.Givenaninputimage,weestimatescenelighting,randomlyselecta3Dasset,andinsertitintothesceneusingourobjectinsertionmoduletogetˆIedit.WeusethemapinformationavailableinnuScenestoplacethecaronadriveablesurface.Wealsoperformcollisionandocclusioncheckingwiththedepthmaptoavoidunrealisticobjectinsertionduetoerroneousplacement.AsshowninFig.2b,weuseadiscriminatortojudgethequalityofˆIeditcomparedtorealcars,andemployadversarialsupervisiontooptimizeforrealismofinsertion:Ladv=−D(ˆIedit).Intuitively,adiscriminatorcouldeasilydetecterroneousshadowdirectionandintensity,anderrorinspecularhighlights.Throughtheadversarialsupervision,theestimatedlightingisencouragedtoproduceobjectinsertionresultssimilartorealworldimagesamples.WerefertofurtheranalysisintheAppendix.4ExperimentsWeextensivelyevaluateourmethodbothqualitativelyandquantitatively.Weﬁrstprovideexperimentdetails(Sec.4.1).Wethencomparelightingestimation,evaluatethequalityofobjectinsertion(Sec.4.2)andperformablationstudy(Sec.4.3).Finally,weshowthatourARdatahelpsdownstreamself-drivingperceptiontasks(Sec.4.4).1www.turbosquid.com10Z.Wangetal.MethodMedianangularerror↓Hold-Geoﬀroyetal.[16]24.88◦Wangetal.[37]53.86◦Ours22.43◦Ours(w/oskymodeling)31.45◦Ours(w/oadv.supervision)24.16◦Table1.Quantitativeresultsofpeakdi-rectiononHoliCity[42].Weoutperformpastwork,andeachcomponent(skymod-eling,adversarialsupervision)helps.MethodPSNR↑si-PSNR↑Hold-Geoﬀroyetal.[16]9.3310.73Hold-Geoﬀroyetal.[16]*10.8114.20Wangetal.[37]14.0615.28Ours(w/oadv.supervision)14.2315.31Ours14.4915.35Table2.QuantitativeresultsofLDRappearanceonthenuScenesdataset[5].*indicatesconstrainingtheevaluationontheupperhemisphere.4.1ExperimentalDetailsLightingestimation.OurlightingrepresentationcombinesaskyfeaturevectorandaVSGlightingvolume.Wesetthedimensionoftheskyvectortobe64anddecodeittoa64x256HDRskydome.Diﬀerentfrom[37],wetailorthesizeofVSGlightingvolumetobe256x256x64(xyz)toaccommodate300x300x80(meters3)outdoorscenes.Asoutdoorscenesarelargerinscalewhilevisiblescenesurfacesarerelativelydenseinclose-to-cameraregions,weemploylogprojectiontomapthevolumerepresentationtoa3Dscenelocation.Inferencetimeofthelightingestimationnetworkis180msperimage,clockedonaTITANVGPU.Objectinsertion.Ourobjectinsertionmodulereliesonadiﬀerentiablerender-ingprocessofbothforegroundobjectandbackgroundshadows.Duringtraining,wesample5000raysforforegroundobjects.Forbackgroundshadows,werendera160x90resolutionshadowmapandsample450raysperpixeltosavememoryandcomputation.Aftertraining,wedoimportancesamplingforeachpixelinforegroundandcanaﬀordahighresolutionshadowmapforbackgroundtogeneratemorerealisticeﬀects.Duringinferencetime,wealsohavetheoptiontousecommercialrenderersuchasBlender[9],whichwedetailintheAppendix.Datasets.Wecollected724outdoorHDRpanoramasfromonlineHDRIdatabasestotraintheskyencoder-decoder.WetrainthefullmodelwithnuScenes[5]andHoliCity[42].FornuScenes,weusetheoﬃcialsplitcontaining700scenesfortrainingand150scenesforevaluation.ForHoliCitydataset,weapply90%v.s.10%datasplitfortrainingandevaluation.Multi-viewextension.Whilewefocusonmonocularestimation,ourmodelisextendabletomulti-viewinput.Itcanconsumemulti-viewimagestopredictmoreaccuratelighting,asshowninFig.6.FortheHDRskypredictionbranch,weapplymaxpoolingforfintensity,flatent,andaveragepoolingtofdirafterrotatingfdirindiﬀerentviewstothecanonicalview.Asforthevolumetriclighting,sinceitisdeﬁnedinthe“world”coordinatespace,weunprojectandfusemulti-viewimagesintoacommonlightingvolumerepresentation,akinto[30].4.2EvaluationofLightingEstimationBaselines.Wecomparewithcurrentstate-of-the-artlightingestimationmethods[16,37].Hold-Geoﬀroyetal.[16]estimatestheHDRskyenvironmentmapfromNeuralLightFieldEstimationforStreetScenes11Approach%Ours(w/oadv.sup.)ispreferred↓Hold-Geoﬀroyetal.[16]68.1±5.4%Wangetal.[37]94.2±2.0%Ours40.6±10.2%Table3.Quantitativeresultsofuserstudy.Userscomparebaselinemethodstoanablatedversionofourmethod(Oursw/oadv.supervision)inapair-wisecomparison.EachrowreportsthepercentageofimagesthatOursw/oadv.supervisionispreferred.Ourmethodoutperformsbaselines,andadv.supervisionimprovesperformance.1212InputimageHold-Geoﬀroyetal.[16]Wangetal.[37]OursFig.4.Qualitativecomparisonoflightingestimation.Weinsertapurelyspecularsphereintotheimagetovisualizethelightingprediction,anddisplaytheenvironmentmapsonthebottom.Notethesunandenvironmentmapchangesbetweenlocations.asingleimage.Wangetal.[37]predictsVolumetricSphericalGaussian.Were-trainorﬁnetunethesemethodsonthesamedatasourcesweusedforourmethodtoensureafaircomparison.HDRevaluationofpeakdirection.WeevaluatepeakdirectionpredictiononHoliCitydataset[42].WereportthemedianangularerrorbetweenthepredicteddirectionandGTinTbl.1.Wangetal.[37]predictsHDRcomponentinaself-supervisedmannerandcannotlearnstrongpeaks.WealsooutperformHold-Geoﬀroyetal.[16],whichseparatelypredictsaskydomeanditsazimuth.LDRevaluationofnovelviewreconstruction.Recallthatanylightingrep-resentation,suchasenvironmentmapandourhybridlighting,aimstorepresentthecompleteorasubsetofthelightﬁeld,whichcanberenderedintoimagesbyqueryingtheradiancefunctionwithspeciﬁedcamerarays.Priorwork[34,37]pro-posedtousenovelviewradiancereconstructionPSNRasquantitativeevaluationofthequalityoflightingestimation,whichwereportonnuScenesdataset[5].AsthenuScenes-capturedimagesmayhavediﬀerentexposurevalues,wereportbothPSNRandscaleinvariantPSNR(si-PSNR)inTbl.2.Forthelatter,wemultiplythepredictednovelviewwithascalingfactorthatminimizesL2error.SinceHold-Geoﬀroyetal.[16]onlypredictslightingontheupperhemisphere,we12Z.Wangetal.InputimageHold-Geoﬀroyetal.[16]Wangetal.[37]OursFig.5.Qualitativecomparisonofvirtualobjectinsertion.Ourmethodproducesrealisticcastshadowsandhigh-frequency“clearcoat”eﬀects.InputimageInsideshadowShadowboundaryOutsideshadowFig.6.Qualitativeresultsofspatially-varyingshadoweﬀects.Ourmethodcanhandlethespatialchangesofshadowintensityaroundshadowboundary,notpossiblepreviously.(Resultstakesixsurroundingperspectiveviewsasinput.)alsoconstraintheevaluationontheupperhemispheretomakeafaircomparison.Ourmethodoutperformsbothbaselineswithalargemargin,as[16]ignoresspatially-varyingeﬀectsandusuallypredictsaskydomewithlittlehigh-frequencydetails.OurmethodalsooutperformsWangetal.[37]whichcannothandlehighHDRintensityofanoutdoorscene.Humanstudy.Toquantitativelyevaluatethequalityofobjectinsertion,weperformahumanstudywithAmazonMechanicalTurk,whereweshowtwoaugmentedimages,randomlypermuted,producedbyourmethodandbythebaseline.Wethenaskuserstocomparetherealismoftheinsertedobject,e.g.thecastshadowsandthereﬂections,andselectthemorerealisticimage.Foreachcomparison,weinvite15userstojudge23examples.Weadoptmajorityvoteforthepreferenceofeachexample,andrunthreetimestoreportmeanandstandarddeviationinTbl.3.TheablatedversionofOurs(w/oadv.supervision)outperformsbaselines,indicatingthehybridlightingrepresentationimprovesuponpriorworks.ComparingOursandOurs(w/oadv.supervision),theresultsindicateincludingadversarialsupervisionleadstomorevisuallyrealisticediting.Qualitativecomparison.WeﬁrstvisualizetheenvironmentmapsatdiﬀerentscenelocationsandinsertionofapurelyspecularsphereinFig.4.Hold-Geoﬀroyetal.[16]onlypredictsoneenvironmentlightingandignoredspatially-varyingeﬀects.Forinsertedspheresaroundshadowedregion,itstillproducesstrongcastshadows.Also,thehigh-frequencydetailsarenotwellpreservedintheskyprediction.Wangetal.[37]cangeneratehigh-frequencydetailsbutfailstohandletheextremeHDRintensityoftheoutdoorscene,andthuscannotgenerateNeuralLightFieldEstimationforStreetScenes13InitialeditingAfteroptimizationInitialeditingAfteroptimizationFig.7.Discriminatortest-timeoptimization.Notethediscriminatorcorrectstheshadowdirection(left),andremoveserroneousspecularhighlight(right).Thepixelatedeﬀectisduetolow-resolutionrenderingduringend-to-endtraining.realisticcastshadows.OurmethodistheonlyonethathandlesextremeHDR,angulardetailsandspatially-varyinglighting.WeshowvirtualobjectinsertionresultsinFig.5.Ourlightingpredictionpreserveshigh-frequencydetailswithHDRintensity,producingrealistichighlightsandclearcoatingeﬀects,whilepriormethodcannotgeneratesucheﬀects.Spatially-varyingshadows.BeneﬁtingfromtheaccurateHDRskyandsurroundingsceneestimation,ourmethodcanproducespatially-varyingshadoweﬀects.AsshowninFig.6,thecarcastsanintenseshadowwhenoutsidetheshadowofthebuilding,whiletheshadowcausedbythecarismuchweakerwhenthecarisinsidetheshadowregion.Especially,italsoshowsreasonablydiﬀerentshadowintensityaroundtheshadowboundary.Thischallengingeﬀectrequiresaccuratepredictionofdirection,intensityandgeometryofHDRlighting,andwillnotoccurwithanincapablelightingrepresentation.4.3AblationStudyWeverifytheeﬀectivenessoftheskydecodermoduleandtheadversarialsuper-vision.InTbl.1,comparedtotheablatedversionthatdirectlypredictskydomeasanenvironmentmap(denotedas“Oursw/oskymodeling”),thefullmodelreducestheerrorbyaround50%,whichdemonstratestheskymodelingnetworkisimportantforachievingaccuratesunpositionpredictioninoutdoorscenes.AdversarialsupervisionimprovestheperformanceonquantitativeevaluationinTbl.1,2,especiallyforpeakdirectionestimation,whichindicatesthatdiscrim-inatingontheﬁnalimageeditingresultiscomplementarytoexistingsupervisionandbeneﬁtslightingprediction.Intheuserstudy(Tbl.3),adversarialsupervi-sionimprovesperceptualrealismandreceivesahigheruserpreference.WealsoqualitativelyvisualizethebehaviourofthediscriminatorinFig.7.Tounderstandthe“photorealism”implicitlyperceivedbythediscriminatorduringthetrainingprocess,weperformtest-timeoptimizationontheobjectinsertionresultstominimizetheadversarialloss,andshowtheoptimizedresultsinFig.7.Intheﬁrstexample,theinitialeditingresultsfailtopredictthecorrectsunlocationandproducewrongshadows.Aftertest-timeoptimization,theshadowdirectionpointstothebottom-leftoftheimageandtheshadowintensityalsomatchesthevisualcuesfromtherestofthescene.Inthesecondexample,theinitialeditingresultscontainanobviouslyerroneoushighlight,andthediscriminatordetectstheartifactandremovesit.Thisagreeswiththeintuitionthataneural14Z.Wangetal.MethodmAPcarbustrailerconst.vehiclebicycleRealData0.1900.3560.1240.0110.0160.116+AugNoLight.0.2010.3630.1630.0290.0210.120+AugLight.0.2110.3690.1820.0360.0200.146Table4.PerformanceofaSOTA3Dobjectdetector[36]onnuScenesbenchmark.mAPrepresentsthemeanforthe10objectcategories.Wereportindividualcategoriesthatsawasigniﬁcantboost(fulltableintheAppendix).discriminatorhavethecapacitytocatchlightingeﬀectssuchascastshadowsandincorrectspecularhighlights.FurtherdetailsareincludedintheAppendix.4.4DownstreamPerceptionTaskWeinvestigatethebeneﬁtsofourobjectinsertionasdataaugmentationforadownstream3DobjectdetectiontaskonnuScenes.Thegoalofthistaskistoplacea3Dboundingboxfor10diﬀerentobjectcategories.Weﬁrsttrainastate-of-the-artmonocular3Ddetector[36]ona10%subsetofrealdatafromthenuScenestrainingset.ThissubsetwaschosenrandomlyacrossallscenesbutinawaythatthenumberofobjectspercategoryresemblestheoriginalnuScenestrainingset.Wethenaugmentthefront-cameraimagesofthissubsetwithourmethod.Speciﬁcally,wecollectasetof3Dmodelswithcategoriesofcarandconstructionvehicles,andrandomlyinsertoneobjectperimage.Ouraugmenteddatasethasapproximately15Kaugmented(new)images.Weusethesametrainingstrategyandmodelhyperparametersas[36]butdonotsuperviseattributesorvelocityasthesearenotpresentfortheaugmenteddata.Quantitatively,inTbl.4wecanobservethattheperformanceofthedetectorimprovesby2%whencomparingtorealdata.Moreover,wecanalsoseethatwhilenaivelyaddingobjectsleadstoa1%improvement,another1%isaresultofhavingbetterlightestimation.Interestingly,wecanalsonoticethattheperformanceoftheobjectdetectoralsoimprovesindiﬀerentcategorieseventhoughwedonotdirectlyaugmentthose.5DiscussionInthispaper,weproposedahybridrepresentationoflightingandanoveldiﬀerentiableobjectinsertionmodulethatallowsend-to-endoptimizationofARobjectives.Inavarietyofcomparisons,wedemonstratetheeﬀectivenessofourapproachinlightingestimation.Furthermore,weshowcaseperformancegainsona3Dobjectdetectiontaskwhentrainingthedetectoronouraugmenteddataset.Whileourmethodpresentsaneﬀectivewayofrendering3Dassetsintoimages,somelimitationsremainforfuturework.Currently,theinsertedvirtualobjectpixelsdonotpassthesamecapturingprocessasthebackgroundscene,andtheshadowrenderingassumesLambertiansurface.ModelingofcameraISP,weather,andnon-Lambertianscenematerialscanbeinterestingdirectionsforfuturework.NeuralLightFieldEstimationforStreetScenes15References1.Adelson,E.H.,Bergen,J.R.:Theplenopticfunctionandtheelementsofearlyvision.In:ComputationalModelsofVisualProcessing.pp.3–20.MITPress(1991)12.Alhaija,H.A.,Mustikovela,S.K.,Mescheder,L.,Geiger,A.,Rother,C.:Augmentedrealitymeetscomputervision:Eﬃcientdatagenerationforurbandrivingscenes.InternationalJournalofComputerVision126(9),961–972(2018)33.Boss,M.,Jampani,V.,Kim,K.,Lensch,H.,Kautz,J.:Two-shotspatially-varyingbrdfandshapeestimation.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.3982–3991(2020)34.Burley,B.,Studios,W.D.A.:Physically-basedshadingatdisney.In:ACMSIG-GRAPH.vol.2012,pp.1–7.vol.2012(2012)65.Caesar,H.,Bankiti,V.,Lang,A.H.,Vora,S.,Liong,V.E.,Xu,Q.,Krishnan,A.,Pan,Y.,Baldan,G.,Beijbom,O.:nuscenes:Amultimodaldatasetforautonomousdriving.arXivpreprintarXiv:1903.11027(2019)8,9,10,116.Chen,W.,Ling,H.,Gao,J.,Smith,E.,Lehtinen,J.,Jacobson,A.,Fidler,S.:Learningtopredict3dobjectswithaninterpolation-baseddiﬀerentiablerenderer.In:NeurIPS(2019)37.Chen,W.,Litalien,J.,Gao,J.,Wang,Z.,Tsang,C.F.,Khalis,S.,Litany,O.,Fidler,S.:DIB-R++:Learningtopredictlightingandmaterialwithahybriddiﬀerentiablerenderer.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS)(2021)3,68.Chen,Y.,Rong,F.,Duggal,S.,Wang,S.,Yan,X.,Manivasagam,S.,Xue,S.,Yumer,E.,Urtasun,R.:Geosim:Realisticvideosimulationviageometry-awarecompositionforself-driving.In:CVPR(2021)1,39.Community,B.O.:Blender-a3Dmodellingandrenderingpackage.BlenderFounda-tion,StichtingBlenderFoundation,Amsterdam(2018),http://www.blender.org1010.Dwibedi,D.,Misra,I.,Hebert,M.:Cut,pasteandlearn:Surprisinglyeasysynthesisforinstancedetection.In:TheIEEEInternationalConferenceonComputerVision(ICCV)(Oct2017)1,311.Gardner,M.A.,Hold-Geoﬀroy,Y.,Sunkavalli,K.,Gagné,C.,Lalonde,J.F.:Deepparametricindoorlightingestimation.In:ProceedingsoftheIEEEInternationalConferenceonComputerVision.pp.7175–7183(2019)312.Gardner,M.A.,Sunkavalli,K.,Yumer,E.,Shen,X.,Gambaretto,E.,Gagné,C.,Lalonde,J.F.:Learningtopredictindoorilluminationfromasingleimage.arXivpreprintarXiv:1704.00090(2017)313.Garon,M.,Sunkavalli,K.,Hadap,S.,Carr,N.,Lalonde,J.F.:Fastspatially-varyingindoorlightingestimation.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.pp.6908–6917(2019)314.Guizilini,V.,Ambrus,R.,Pillai,S.,Raventos,A.,Gaidon,A.:3dpackingforself-supervisedmonoculardepthestimation.In:IEEEConferenceonComputerVisionandPatternRecognition(CVPR)(2020)4,615.He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.CoRRabs/1512.03385(2015),http://arxiv.org/abs/1512.03385616.Hold-Geoﬀroy,Y.,Athawale,A.,Lalonde,J.F.:Deepskymodelingforsingleimageoutdoorlightingestimation.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.6927–6935(2019)2,3,10,11,1217.Hold-Geoﬀroy,Y.,Sunkavalli,K.,Hadap,S.,Gambaretto,E.,Lalonde,J.F.:Deepoutdoorilluminationestimation.In:ProceedingsoftheIEEEConferenceonCom-puterVisionandPatternRecognition.pp.7312–7321(2017)2,316Z.Wangetal.18.Hong,S.,Yan,X.,Huang,T.E.,Lee,H.:Learninghierarchicalsemanticimagema-nipulationthroughstructuredrepresentations.In:AdvancesinNeuralInformationProcessingSystems.pp.2713–2723(2018)119.Karis,B.,Games,E.:Realshadinginunrealengine4.Proc.PhysicallyBasedShadingTheoryPractice4(3)(2013)620.Kim,S.W.,Philion,J.,Torralba,A.,Fidler,S.:DriveGAN:TowardsaControllableHigh-QualityNeuralSimulation.In:IEEEConferenceonComputerVisionandPatternRecognition(CVPR)(2021)321.LeGendre,C.,Ma,W.C.,Fyﬀe,G.,Flynn,J.,Charbonnel,L.,Busch,J.,Debevec,P.:Deeplight:Learningilluminationforunconstrainedmobilemixedreality.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.pp.5918–5928(2019)322.Li,T.M.,Aittala,M.,Durand,F.,Lehtinen,J.:Diﬀerentiablemontecarloraytracingthroughedgesampling.ACMTrans.Graph.(Proc.SIGGRAPHAsia)37(6),222:1–222:11(2018)323.Li,Z.,Shaﬁei,M.,Ramamoorthi,R.,Sunkavalli,K.,Chandraker,M.:Inverserenderingforcomplexindoorscenes:Shape,spatially-varyinglightingandsvbrdffromasingleimage.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.2475–2484(2020)324.Li,Z.,Xu,Z.,Ramamoorthi,R.,Sunkavalli,K.,Chandraker,M.:Learningtoreconstructshapeandspatially-varyingreﬂectancefromasingleimage.ACMTransactionsonGraphics(TOG)37(6),1–11(2018)325.Ling,H.,Acuna,D.,Kreis,K.,Kim,S.W.,Fidler,S.:Variationalamodalobjectcompletion.AdvancesinNeuralInformationProcessingSystems(2020)326.Mescheder,L.,Oechsle,M.,Niemeyer,M.,Nowozin,S.,Geiger,A.:Occupancynetworks:Learning3dreconstructioninfunctionspace.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.pp.4460–4470(2019)627.Nimier-David,M.,Vicini,D.,Zeltner,T.,Jakob,W.:Mitsuba2:Aretargetableforwardandinverserenderer.TransactionsonGraphics(ProceedingsofSIGGRAPHAsia)38(6)(Dec2019).https://doi.org/10.1145/3355089.3356498328.Ost,J.,Mannan,F.,Thuerey,N.,Knodt,J.,Heide,F.:Neuralscenegraphsfordynamicscenes.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).pp.2856–2865(June2021)1,329.Peers,P.,Tamura,N.,Matusik,W.,Debevec,P.:Post-productionfacialperformancerelightingusingreﬂectancetransfer.ACMTransactionsonGraphics(TOG)26(3),52–es(2007)730.Philion,J.,Fidler,S.:Lift,splat,shoot:Encodingimagesfromarbitrarycamerarigsbyimplicitlyunprojectingto3d.arXivpreprintarXiv:2008.05711(2020)1031.Sengupta,S.,Gu,J.,Kim,K.,Liu,G.,Jacobs,D.W.,Kautz,J.:Neuralinverserenderingofanindoorscenefromasingleimage.In:InternationalConferenceonComputerVision(ICCV)(2019)332.Somanath,G.,Kurz,D.:Hdrenvironmentmapestimationforreal-timeaugmentedreality.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(2021)333.Song,S.,Funkhouser,T.:Neuralillumination:Lightingpredictionforindoorenvironments.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.pp.6918–6926(2019)334.Srinivasan,P.P.,Mildenhall,B.,Tancik,M.,Barron,J.T.,Tucker,R.,Snavely,N.:Lighthouse:Predictinglightingvolumesforspatially-coherentillumination.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.8080–8089(2020)2,3,11NeuralLightFieldEstimationforStreetScenes1735.Su,H.,Qi,C.R.,Li,Y.,Guibas,L.J.:Renderforcnn:Viewpointestimationinimagesusingcnnstrainedwithrendered3dmodelviews.In:TheIEEEInternationalConferenceonComputerVision(ICCV)(December2015)1,336.Wang,T.,Zhu,X.,Pang,J.,Lin,D.:Fcos3d:Fullyconvolutionalone-stagemonoc-ular3dobjectdetection.arXivpreprintarXiv:2104.10956(2021)1437.Wang,Z.,Philion,J.,Fidler,S.,Kautz,J.:Learningindoorinverserenderingwith3dspatially-varyinglighting.In:ProceedingsofInternationalConferenceonComputerVision(ICCV)(2021)2,3,5,6,9,10,11,1238.Wei,X.,Chen,G.,Dong,Y.,Lin,S.,Tong,X.:Object-basedilluminationestimationwithrendering-awareneuralnetworks.arXivpreprintarXiv:2008.02514(2020)339.Zhang,J.,Sunkavalli,K.,Hold-Geoﬀroy,Y.,Hadap,S.,Eisenman,J.,Lalonde,J.F.:All-weatherdeepoutdoorlightingestimation.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.pp.10158–10166(2019)2,340.Zhang,Y.,Chen,W.,Ling,H.,Gao,J.,Zhang,Y.,Torralba,A.,Fidler,S.:Imagegansmeetdiﬀerentiablerenderingforinversegraphicsandinterpretable3dneuralrendering.In:InternationalConferenceonLearningRepresentations(2021)341.Zhao,Y.,Guo,T.:Pointar:Eﬃcientlightingestimationformobileaugmentedreality.arXivpreprintarXiv:2004.00006(2020)342.Zhou,Y.,Huang,J.,Dai,X.,Luo,L.,Chen,Z.,Ma,Y.:HoliCity:Acity-scaledataplatformforlearningholistic3Dstructures(2020),arXiv:2008.03286[cs.CV]8,9,10,1143.Zhu,Y.,Zhang,Y.,Li,S.,Shi,B.:Spatially-varyingoutdoorlightingestimationfromintrinsics.In:CVPR(2021)3SupplementaryMaterial:NeuralLightFieldEstimationforStreetSceneswithDiﬀerentiableVirtualObjectInsertionZianWang1,2,3,WenzhengChen1,2,3,DavidAcuna1,2,3,JanKautz1,andSanjaFidler1,2,31NVIDIA2UniversityofToronto3VectorInstitute{zianw,wenzchen,dacunamarrer,jkautz,sfidler}@nvidia.comInthesupplementarymaterial,weincludeadditionaldetailsandresultsofourapproach.WeprovidetechnicaldetailsinSection1.WeshowadditionalauxiliaryresultswhichfurtherablateandexplainourmethodinSection2.WediscusslimitationsandbroaderimpactinSection3.1ImplementationDetails1.1SkyModelingArchitectureTheprimarygoaloftheskymodelingneuralnetworkistolearnalow-dimensionalfeaturespacefortheskydome.TheresultingnetworkcanalsopredictHDRinformationgiveninputLDRpanorama.ThemodelarchitectureisillustratedinFigureA.A2DCNNencodertakesasinputanLDRpanoramawithapositionalencoding,andpredictstheskyvectorˆfinthefeaturespace.Aseachpixelinthepanoramacorrespondstoadirectionthroughequi-rectangularprojection,thepositionalencoding(R3×H×W)encodesthedirectionalinformation,whereeachpixelcontainsaunitvectorindicatingthedirectionofthatpixellocation.Thedecoderisa2DUNet[15]decodingtheskyvectorintoanHDRskypanorama.WecarefullydesignitsarchitecturetofacilitatethereconstructionoftheHDRsunpeak.Theinputtothe2DUNetisa7-channelpanorama,includinga4-channelpeakencodinganda3-channelpositionalencoding,asshowninFigureA.Speciﬁcally,weembedthepeakdirectionˆfdirinformationintoa1-channelpeakdirectionencodingR1×H×WwithasphericalGaussianlobe.Foreachpixellocationcorrespondingtothedirectionu,wecomputethepeakdirectionencodingas:PeakDirEncoding(u)=e100(u·ˆfdir−1).(1)Weencodethepeakintensityintoa3-channelpanoramabyassigningthepeakpixelstothepredictedpeakintensityˆfintensity,PeakIntensityEncoding(u)=(ˆfintensity,ifPeakDirEncoding(u)≥0.980,Otherwise(2)Thisresultsin7-channelinputtothe2DUNetbyconcatenatingthe1-channelpeakdirectionencoding,3-channelpeakintensityencoding,andthe3-channelarXiv:2208.09480v1  [cs.CV]  19 Aug 20222Z.Wangetal.HDRSkyDomeLDRPanoramaSky LatentPeak DirectionPeak IntensityPos. Enc.Pos. & Peak Enc.GT PeakDirectionGT Peak IntensityAlternateSkyEncoderSkyDecoderFig.A:Architectureofourskymodelingnetwork.Theencodertakesasinputanadditionalpositionalencoding,concatenatedwiththeinputLDRpanorama.Wecomputeapanoramaimageencodingpeakandpositionalinformation,andfeeditintoa2DUNet[15]decoder.Theskylatentcodeisfusedinthelatentspaceofthedecoder.Duringtraining,wealternatebetweenend-to-endtrainingandteacherforcing.LightingFeatureResNetLightingVolumeHDRSkyDomeSky VectorSkyDecoderImageDepth3D UnprojectionVisibleFoVVolumeDown3DDown3D3D UNet…Voxel PositionalEncodingConcat.Down3DUp3DUp3DUp3DMLPFig.B:Architectureofourhybridlightingpredictionnetwork.ForHDRskydomeprediction(top),wedirectlypredicttheskyfeaturevectorfromtheResNetbackbone.Forlightingvolumeprediction(bottom),inspiredby[20],weunprojectthe2Dinputimageintoa3Dvolumeandprocessitwitha3DUNet.ThegloballightingfeatureisconvertedtoafeaturevolumewithanMLPandlaterfusedintothe3DUNet.positionalencodingusedintheskyencoder.Theskylatentcodeˆflatentiscon-catenatedwiththelatentvectoroutputbythe2DUNettojointlydecodetheHDRskydome.Theskyencodercontainstwoseparate2DCNNswithonepredictingthepeakinformationandtheotherpredictingthelatentcode,whereeachCNNcontains5downsamplingconv-blockswithintermediateoutputchanneldimensionsof:(64,128,256,256,256).Fortheskydecoder,the2DUNetcontains5downsamplingconv-blocksand5upsamplingconv-blocks,connectedwithresiduallinks[8].Theintermediateoutputchanneldimensionsare(64,128,256,256,256,256,128,64,32,16).Eachconv-blockcontainstwo2Dconvolutionlayersfollowedbybatchnormaliza-tionandReLUactivation.SupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes31.2HybridLightingPredictionArchitectureThe2DCNNbackbonefortheHybridLightingJointPredictionmodule(depictedinFigureBandmainpaperFigure2(a))isResNet50[8],withtwoseparatebranchespredictingtheskyfeaturevectorandthescenelightingfeature.Fortheskypredictionbranch,theskyfeaturevector(R64)isthenpassedintothepre-trainedskydecoder,withfrozenweights,todecodetheHDRskydome(R3×64×256).Weadaptthearchitectureusedin[20]forthelightingvolumepredictionbranch.Toencodethevisibleﬁeld-of-view(FoV)information,weunprojecttheinputimageintotheinitialvisiblesurfacevolume(R4×64×256×256)[20].Thescenelightingfeature(R128)extractedwiththeResNetbackboneispassedintoacoordinateMLP[16],anddecodedintoaglobalscenefeaturevolume(R32×16×64×64).Here,thecoordinatenetwork[16]containsthreeresidualMLPblocksandthehiddensizeis64.Weuse3DUNet[15]toprocessthevisiblesurfacevolume(R4×64×256×256),whichcontainsﬁvedownsamplingandupsamplingconv-blockswithresidualconnections[8],andeachconv-blockcontainstwo3Dconvolutionlayers.Theglobalscenefeaturevolume(R32×16×64×64)isfusedintothe3DUNetafter2downsamplingconv-blocks.Theintermediateoutputchannelsofeachconv-blockhavedimensionsof(12,16,32,128,256,256,128,64,32,16).Theﬁnalconv-layerproducesthelightingvolumeprediction(R8×64×256×256).Pre-traineddepthestimation.Inthiswork,werelyontheexistingstate-of-the-artoﬀ-the-shelfmonoculardepthestimatorPackNet[7]toobtainthe2.5Dgeometryofthescene.Weempiricallyﬁndthatthedepthperceptionisrobustwithreasonableperformance,andprovideanalysisontheinﬂuenceofdepthpredictionerrorbelow.Thepredicteddepthisﬁrstusedinlightingvolumeprediction,wherethescenepixelvaluesareunprojectedinto3Dwiththedepthprediction.Moreaccuratedepthpredictioninformsmoreprecisegeometricinformationofthescene,andwillimprovetheperformanceoflightingestimation,e.g.thequantitativemetricinmainpaperTable2.Then,duringdiﬀerentiableobjectinsertion,weusedepthtodecideonthe3Dplacementoftheobjectwewanttoinsert.Sincethelocationoftheinsertedobjectiscomputedfromthedepthmap,thescaleerrorofthedepthpredictionleadstoerrorinthesizeoftheinsertedobjects.Toaddressthis,weusetheprojectedsparseLiDARpointsasground-truthdepthtorescaletheestimateddepthbyminimizingtheL2errorbetweenthem.Whenrenderingshadowsoftheinsertedobject,werelyonthepre-computeddepthtocomputethe3Dlocationofeachscenepixel,whichisneededtoperformtheray-meshqueries.Thus,errorsintheestimateddepthmayresultinincorrectshadows.Weempiricallyﬁndthatthiserrorisusuallynegligibleasweinsertmostlyonﬂatsurfaces.Priorworksthatcontainsubmoduleswithdepthprediction[14,20]typicallydonotfocusonimprovingdepthperception,butsimplyusesyntheticdatawithpairedground-truthtotrainthedepthestimationbranch,whichmayeasilysuﬀer4Z.Wangetal.(a)Inputimage(d)Objectmask(e)Sceneshadowmap(b)Compositedresult(c)ForegroundobjectFig.C:Visualizationofobjectinsertioncomposition.Givenaninputimage(a),weestimateourhybridlightingrepresentation.Werendertheforegroundinsertedobjectwithstandarddeferredrendering[3]andobtaintherenderingresult(c)andobjectmask(d).Werendertheshadowcastbytheinsertedobjectintoaratiomap(e)asdescribedinmainpaperSection3.3.Theﬁnaleditingresult(b)isproducedbycompositing(c-e).fromthedomaingap.Withafocusonlightingestimationandrealisticobjectinsertion,webelievethatusingtheexistingmaturedepthpredictionmodelsasabuildingblockisaplausibledesignchoiceanddoesnotdecreaseourtechnicalcontribution.1.3DiﬀerentiableObjectInsertionWevisualizetheobjectinsertioncompositionprocessinFigureC.WecompositetheinputimageI,foregroundobjectIobject,alphamaskM,sceneshadowmapIshadowintotheﬁnaleditingresultIeditbyIedit=M(cid:12)Iobject+(1−M)(cid:12)I(cid:12)Ishadow.(3)Weincludetheimplementationdetailsbelow.Renderingdetails.Forrenderingtheforegroundobject,weusetheBRDFusedbyUnrealEngine4,whichisasimpliﬁedversionofDisneyBRDF[1,12].Speciﬁcally,weusethebasecolorcbase∈R3,metallicm∈[0,1],roughnessr∈[0,1]andspeculars∈[0,1]todescribematerialpropertiesofobjectsurfaces.TheBRDFisdeﬁnedasf(l,v)=cdiﬀuseπ+DFG4(n·l)(n·v),(4)SupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes5wherecdiﬀuse=(1−m)cbase(5)cspecular=(1−m)0.08s+mcbase(6)D=α2π((n·h)2(α2−1)+1)2(7)G=(n·l)(n·v)((n·l)(1−k)+k)((n·v)(1−k)+k)(8)F=cspecular+(1−cspecular)2(−5.55473(v·h)−6.98316)(v·h)(9)α=r2,k=(r+1)28,h=l+v||l+v||.(10)TherenderedrawpixelvaluesisHDRinlinearRGBspace.ToconverttoLDRsRGBimages,weapplygammacorrection(γ=2.2),anddosoftclippingfollowing[20]ϕ(x)=(xifx≤τ1−(1−τ)e−x−τ1−τifx>τ(11)wherewesetτ=0.95.Duringtraining,tosaveonthecomputationalcostandGPUmemory,weuniformlysample5000raysforobjectcenteralone,andrendertheforegroundobjectona320x180imagecanvas.Forbackgroundshadows,werendera160x90resolutionshadowmap.Wesample50raysforeachscenepixel,andaggregatethe8-neighborsofeachpixeltoobtain450raysintotal.Theaveragerenderingtimeforforegroundobjectsandbackgroundshadowsare0.2sand2srespectively.Thecurrenteﬃciencybottleneckintherendereristheray-meshqueryfunction,whichisaCPUimplementationusingtrimesh1withthepotentialtofurtherspeedups.Itconsumes12GGPUmemoryduringtraining,includingboththeforwardandthebackwardpass.Duringinference,wesampleraysforeachobjectpixelusingper-pixelimportancesamplingfollowing[3]toachievehighqualityrendering,wherewesample1024raysforthediﬀusecomponentand256raysforthespecularcomponent.Raysamplingscheme.Alargernumberofraysamplesmayleadtohigherqualityrendering,butusuallylimitedbytheaﬀordablememoryandcomputation,especiallyforadiﬀerentiablerenderingmoduleinend-to-endlearningtasks.Toimprovetheeﬃciencyofraysamplingespeciallyfortheprocessofshadowrendering,weselectequi-spacedraysforeachpixelontheupper-hemispherewithFibonaccilattice[6]torenderthespatially-varyingshadows.AsshowninFigureD,theFibonaccilatticerayselectionstrategybetterutilizestherayscomparedtonaiveuniformsampling.1trimsh.org6Z.Wangetal.(a)Uniformsampling(b)Fibonaccilatticerayselection(Ours)Fig.D:Visualizationofraydirections.Insteadofusingnaiveuniformraysampling(a),weselectequi-spacedrayswithFibonaccilattice(b).Thisschemebetterutilizestheraysandcanensurethesunlightisproperlysampled.Objectinsertiondetails.Duringtraining,weusethetaskofobjectinsertiontoprovideadditionalsupervision.Speciﬁcally,wediﬀerentiablyinsertavirtualobjectintothephotographandencouragethephotorealismoftheﬁnalimageeditingresults.Weadoptarelativelyconservativeschemetoavoidunrealisticeditingresultsduetoassetqualityandobjectplacement.Forassets,wecollectasetof283highquality3DcarmodelsfromTurbosquid2.Toplacethecarsintoplausiblelocations,weusethedensedepthmapprediction[7],lidarsemanticsegmentationand3DboundingboxannotationinnuScenes[2].Thelocationcandidatesforinsertionshouldsatisfythefollowingconditions:(1)Semantics:belongtothesemanticclass“driveablesurface”,(2)Distance:withintherangeof10to40meters,(3)Collision:candidatelocationis1meterawayfromthestaticbackgroundclasses(suchas“sidewalk”)and3metersawayfromdynamicclasses(suchas“vehicles”),and(4)Occlusion:the3Dboundingboxcornersoftheinsertedobjectdonotgetoccludedbyotherobjects.Thecarorientationisrandomlysettothesameoroppositedirectionastheegocamera,withaGaussianrandomperturbationintheyawangle(sigmavaluesetto3◦).Objectinsertionfordataaugmentation.Fordownstreamtasks,weuseobjectinsertionasadataaugmentationapproach,toenhancethedatawithadditionaldiversity.Weusethe283carCADmodelsandrandomizethecarpaintwith60diversecolors.Fordiversity,wealsoinclude30highqualityconstructionvehicleCADmodelsthatarerarelyobservedonthestreet.Weremovetheocclusioncheckmentionedabove,andnaivelyhandletheocclusionbycomparingthedepthmapofthesceneandinsertedobject.Fororientation,weincreasetheGaussianperturbationwithsigmavaluesetto15◦.Renderingobjectinsertionwithcommercialrenderer.Whileweadoptourproposedcustomdiﬀerentiableobjectinsertionmoduleduringtrainingtoprovidevaluablesupervisionsignal,ourrenderingisfullyphysics-basedwithstandardPBRmaterialdeﬁnition.Thus,ourestimatedlightingcanalsobemadeuseableincommercialrenderingenginessuchastheCyclesrendererin2www.turbosquid.comSupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes7Hybrid LightingRadianceQuery(a)Inputimage(b)Compositedresult(c)Render Location-Specific Env. Map(d)Virtual Plane(e)Render object-only(f)Render plane-only(g)Plane with object occlusionFig.E:RenderingobjectinsertionwithBlender.Ourlightingrepresentationcanbeconvertedtothestandardenvironmentmaptobecompatiblewithcommercialrenderers(thusgainingthebeneﬁtsofrichermaterialsupportandspeedofcommercialrenderers).Giventheinputimage(a)andthe3Dlocationshowningreentoinsertobject,weﬁrstconvertourhybridlightingintoalocalenvironmentmapwithvolumerendering(c).Weplaceavirtualplaneundertheinsertedobject(d),andrenderthreeimages–Object-only(e),plane-only(f)andplanewithobjectocclusion(g).Theshadowmapcanbecomputedbytheratioof(f)and(g).Theﬁnaleditingresultisshownin(b).Blender[5].Thisallowsustoutilizefasterrenderingaswellasrichermaterialsupportavailableinthecommercialrenderers.Commercialrenderersusuallyonlysupportstandardenvironmentmaplight-inganddonotsupportspatially-varyinglightingsuchasourhybridlightingrepresentation.Inaddition,theradiance-levelAPIisnotavailableforshadowrendering.WemodifyourobjectinsertionpipelinetobecompatiblewithBlenderCyclesrendererinFigureE.Giventhe3Dlocationtoinsertthevirtualobject,weﬁrstrenderourlightingrepresentationintoastandardHDRenvironmentmapbydoingradiancequeryatthespeciﬁed3Dlocation(mainpaperEq.1),andloadintoBlenderaslighting.Torendercastshadows,weinsertavirtualplanebeneaththeinsertedobject,andrenderthreeimages:(1)renderonlyinsertedobjectIobject,(2)renderonlyvirtualplaneIplane,(3)renderthevirtualplanebuttakeintoaccountoftheinsertedobjectwhendoingray-tracingIshadowed.TheshadowmapcanbecomputedasIshadow=IshadowedIplane,followedbytheimagecompositioninFigureC.Notethatthismaximallypreservesscene-levelspatially-varyingeﬀects,butstillignoresthelocalgeometryofobjectandvirtualplane,andthuscannotrenderlocalizedshadoweﬀectsasshowninmainpaperFigure6.WeshowqualitativeresultsfromBlenderinFigureN.Inmostcases,withtheCyclesrendereratinferencetime,wecanbeneﬁtfrommorebouncesofray-tracingandcomplexmaterialssuchastransparencyandsub-surfacescattering.8Z.Wangetal.1.4TrainingDetailsSkymodelingnetwork.Weuseamulti-tasklosstotraintheskyencoder-decodernetwork:Lskym=λdirLskymdir+λintensityLskymintensity+λhdrLskymhdr(12)wheretheweightsareallsetto1.Duringtraining,weintroduce“teacherforcing”onHDRreconstructionlossLhdrbyalternatingtheinputtoskydecoderbetween(ˆfdir,ˆfintensity,ˆflatent)and(fdir,fintensity,ˆflatent).Thisisbecausethepredictionofthepeakdirectionˆfdirandpeakintensityˆfintensityareusuallyinaccurateintheearlystagesoftraining.Thisalsohelpstoeﬃcientlydisentanglethepeakandthebackground(FigureI),encouragingthenetworktoaccuratelyreconstructHDRpeakwhengivengroundtruthpeakinformationfdir,fintensity.WeusetheAdamoptimizer[13]andtrainfor4000epochs,withthelearningratesetto1e-3anddecayingby0.3every1000epochs.Lightingpredictionnetwork.WeuseaweightedsumoflosstermsintroducedinthemainpaperinSection3.4,includingtheskyregressionlossLsky,radianceanddepthreconstructionlossLrecon,skyseparationlossLtransmit,andadversarialsupervisionLadv.Toaddresstheambiguityofdepthrendering,wealsofollow[20]andusearegularizationlossLregtoencouragethealphachannelofthevolumetobeeither0or1.TheﬁnallossfunctionLhybrid=λskyLsky+λreconLrecon+λregLreg+λtransmitLtransmit+λadvLadv(13)wherewesetλsky,λrecon,λtransmitto1,λregto1e-4,andλadvto3e-3.FortheskyregressionlossLsky,welinearlyfadeouttheL1lossforlatentcodeintheﬁrst50epochs.FortheadversarialsupervisionLadv,thediscriminatorDisa5-layerPatchGANwithspectralnorm[11,18].WeusehingelossforthediscriminatorLD=max(0,1−D(Ireal))+max(0,1+D(ˆIedit)).(14)whereweusethetrainingsetofrealworldimages{Ireal}fromnuScenes[2]andperspectiveimagecropsfromHoliCitystreetviews[22]aspositiveexamples.Wesetthebatchsizeto1.Thefullmodeltakes20GGPUmemoryduringtraining(8Gfornetworkinferenceand12Gforrendering).Weﬁrstpre-trainwithoutLadvfor50epochsandthenjointlytrainanother50epochs.WetrainwiththeAdamoptimizer[13]withlearningrateof3e-4,decayingby0.3every30epochs.1.5ExperimentalSettingsDataprocessing.WetrainourmodelonnuScenes[2],HoliCity[22],andasetof724outdoorHDRpanoramascollectedfromthreedatasources:HDRIHaven3,3polyhaven.com/hdris(License:CC0)SupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes9Fig.F:UserStudyInterface(AMT):Weinserttwocars,whicharerenderedwithdiﬀerentlightingapproaches,intothesamebackgroundimage,andaskahumanparticipanttoselectthemorerealisticone.Weconduct3comparisons,whereweﬁrstcompareourfullmodelwiththebaselinelightingmethods[20,9],thenablateagainstourmethodwithoutadversarialsupervision.WerandomizetheirorderineachHIT.DoschDesign4andHDRMaps5.ForHDRIdata,weuse90%and10%fortrainingandevaluation.Duringtraining,weapplyrandomﬂippingandrandomazimuthshiftingasdataaugmentation.FornuScenes,weusetheoﬃcialsplitcontaining700scenesfortrainingand150scenesforevaluation.Foreachkeyframe,wetakethefrontcameraastheinputimageandusethecapturedviewsatanovelviewpoint(1.5secondsaftertheinputframe)tosuperviselighting.ForHoliCitydataset,wefollow[10]anddetectthesunlocationasthecentriodoflargestconnectedregionafterthresholdingwiththe98-thpercentile.Forevaluation,wemanuallyannotatedthesunlocationforthetestset.Consideringthatthepeakdirectionisambiguousforanalmostuniformskydome,weonlyuseasubsetoftheHoliCitydatawithastrongpeaktotrainandevaluatethepeakdirectionprediction.Speciﬁcally,wepasstheLDRpanoramastothepre-trainedskyencoderandgettheintensityprediction˜fintensity.Lossisonlyusedwhenthepeakintensityisgreaterthan10.Thisresultsin1897panoramasfortrainingand183panoramasforevaluation.Humanperceptionstudydetails.Inthissection,weprovideadditionaldetailsofouruserstudy.WeperformtheuserstudyonAmazonMechanicalTurk(AMT)andvisualizetheinterfaceinFigureF.IneachHIT,weprovidetwoimagesandasktheusertoselectthemorerealisticone.Amongthetwoimages,oneisrelightedbyourapproachandtheotheriseitheroneofthebaselines[20,9]4doschdesign.com(License:doschdesign.com/information.php?p=2)5hdrmaps.com(License:Royalty-Free)10Z.Wangetal.ortheablationsetting(Oursw/oadversarialsupervision).Toavoidbiasinorder,wealwaysrandomizewhichofthemethodsisshownontheleftvsright.Weprovideinstructionsasfollows:Anartiﬁcialintelligenceagentistryingtoinsertanewcarintoanimageinanaturalway.Itwantstomakethecarlooklikeitwaspartofthesceneafterinsertion.Itmadetwotrials,presentedundertrial1andtrial2.Pleasezoomintoinspectthetwoimages,e.g.lookattheshadowscastbytheinsertedcararepointingtothecorrectdirection,andthatthesunreﬂectsonthecarintherightway,andpickthemorerealistictrialimage.Wesynthesize23insertionexamples.Foreachexample,weask15userstojudgetherealismoftheinsertedobjectandadoptamajorityvotetocomputetheﬁnalpreference.Insummary,itresultsin3comparisons×23examples×15selections×3repeatexperiments=3105HITs.WeprovideresultsinTable3inthemainpaper,demonstratingthatuserspreferourapproachoverthebaselines[20,9],andaddingadversarialsupervisionfurtherimprovestherealismofobjectinsertion.2AdditionalResults2.1SkymodelingTheskyencoder-decodertakesasinputanLDRskypanorama,andproducesalow-dimensionalvectorrepresentationoftheskyˆfandareconstructedHDRskydome.ThequalitativeresultsareshowninFigureI.OurmodelcanaccuratelyreconstructHDRsky,especiallythepeakswithextremeintensityvalues.Astheskyvectorˆf=(ˆfintensity,ˆfdir,ˆflatent)containsexplicitpeakinformation,itenablespeakeditingbyfeedingintoeditedskyvector(ˆfeditintensity,ˆfeditdir,ˆflatent).Astheresultsshown,ourskydecodercaneﬃcientlydisentangletheHDRpeakfromthebackground,andenablesprecisepeakediting.Thecontrollablepropertyallowsforpotentialpost-editingofinaccuratepredictions.WequantitativelyablateourarchitecturedesignandshowtheresultsinTableA.WeevaluatebycomparingtheMSEofreconstructedHDRskydomeandthegroundtruth,andthemedianangularerrorbetweenthereconstructedpeakandtheground-truthpeakdirection.Fortheablatedversion,“oursw/oencoding”removesthepositionalencodingandpeakencodingconcatenatedtotheSkyEncoderandSkyDecoder,and“oursw/opeakinformation”removesthepeakdirectionandpeakintensityandonlyreliesonthelatentcodetoreconstructHDRsky.Thequantitativeresultsvalidatedtheeﬀectivenessofourdesignchoicestoachievethebestperformance.Wealsocompareunderthesameexperimentsettingwiththeskyrepresentationusedin[9],wheretheHDRskyisrepresentedwithalatentvectorandexplicitazimuthofthesun.Ourmethodoutperforms[9],whichshowsthebeneﬁtsfromtheexplicitrepresentationandsupervisionofpeakdirectionandintensity.SupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes11MethodHDRreconstructionMSE(×10−2)↓Peakdirectionmedianangularerror↓Latentcodew/explicitazimuth[9]10.565.67◦Ours7.493.38◦Ours(w/oencoding)8.314.09◦Ours(w/opeakinformation)11.017.93◦TableA:Quantitativeresultsofskymodeling.WecomparetheHDRreconstruc-tionMSEandthemedianangularerrorofthereconstructedpeakdirection.2.2AuxiliaryQuantitativeEvalutationofLightingEstimationPriorworks[21]thatignorespatially-varyingeﬀectsmayalsoenforcethataknownrenderedvirtualsceneOscene(e.g.asphere)appearsconsistentwhenusingthepredictedenvenvironmentmapandgroundtruthenvenvironmentmap,i.e.,||Render(Lpred,Osphere)−Render(Lgt,Osphere)||.Comparedtothedirectenvenvironmentmapregressionloss||Lpred−Lgt||,thisisstillregressionbutsmartlyweighted.Thiscanbeusedasbothtraininglossandevaluationmetric,whenpriorworkssimplifythelightingtoonlyoneenvironmentmap.However,forspatially-varyinglightingestimationwhichisthefocusofourwork,thereisnogroundtruthlightingprovidedforthespeciﬁc3Dlocationoftheinsertedobject,andthuswecannotdirectlyutilizethisasatrainingloss.Despitenotaprecisemetric,weshowthequantitativeevaluationofthismetricasanauxiliaryresult.Speciﬁcally,wetakeacroppedperspectiveimageofgroundtruthHDRpanoramaasinput,andinsertadiﬀuseorspecularspherewiththeestimatedlighting.WereportMSEbetweeninsertionresultsgeneratedwithpredictedlightingandgroundtruthHDRpanorama,asshowninTable.B.Hold-Geoﬀroyetal.[9]ignoresspatially-varyingeﬀectsandusuallycannotrecoverthehigh-frequencydetails,andthusleadtoinferiorperformanceonspecularsphereinsertion.Wangetal.[20]cannotreconstructtheHDRcomponentwellandespeciallysuﬀerswheninsertingadiﬀusesphere.Ourmethodoutperformsbaselineswithbetterquantitativeperformance.RenderingMSE(×10−3)↓DiﬀuseSphereSpecularSphereHold-Geoﬀroyetal.[9]2.304.44Wangetal.[20]3.363.13Ours1.792.41TableB:Renderingerrorofinsertedobjects.Notethatthisevaluationignoresspatially-varyingeﬀects.2.3QuantitativeAblationofMulti-viewInputWhilewefocusonmonocularestimation,ourmodelisextendabletomulti-viewinput,suchasthesixsurroundingperspectivecamerasinnuScenessensorrig[17].Forextremelychallengingcasessuchaspredictingpreciseshadowboundaries,12Z.Wangetal.multi-viewinputimagescanprovidemoreﬁeld-of-viewinformationandpredictmoreaccuratelighting,asshowninmainpaperFigure6.WeadditionallyprovidequantitativeresultinTableC,followingtheexperimentsettingsinmainpaperTable1,2.Althoughthesixsurroundingviewsstillonlycoverasubsetofthepanorama,itcansigniﬁcantlyimproveuponmonocularlightingprediction.MethodHoliCity[22]sunlocationMedianangularerror↓nuScenes[2]PSNR↑nuScenes[2]si-PSNR↑Ours22.43◦14.4915.35Ours(6views)19.91◦17.9618.45TableC:Quantitativeablationstudyofmulti-viewinput.2.4AnalysisoftheAdversarialSupervisionTheadversarialsupervision(mainpaperSection3.4,TrainingLightingviaObjectInsertion)usesadiscriminatortoencouragethephotorealismofthelighting-awareimageeditingresults,wherethissignalisbackpropagatedtothepredictedlightingthroughtheDiﬀerentiableObjectInsertionmodule(mainpaperSection3.3).Tounderstandthe“photorealism”implicitlyperceivedbythediscriminatorduringthetrainingprocess,weperformtest-timeoptimizationontheobjectinsertionresultsandvisualizetheoptimizationprocess(mainpaperFigure7).Speciﬁcally,wetakethenetworkweightsofthelightingpredictionnetworkΘandthediscriminatorDinthemiddleofthetrainingprocess(10-thepoch),andoptimizethelightingpredictionnetworkweightsΘtominimizethediscriminatorlossLadv=−D(ˆIedit).Notethatthediscriminatorandthepre-trainedskydecoderarekeptfrozen.WeuseAdamoptimizer[13]withlearningrate1e-4,andshowtheoptimizedresultsafter5and10iterationsinFigureO.Werefertotheaccompaniedvideoforanimatedresults.Inhigh-level,thesceneappearance(encodedbyimagepixelvalues)isaninteractionbetweenscenegeometry,materialandlighting,wherethisprocesscanbeapproximatedbytherenderingequation.Withknowngroundtruthimagedistributionimplicitlylearnedwithadiscriminator,weutilizegroundtruthmate-rialandgeometrytosuperviselightingprediction,byinsertingartist-designedvirtualobjectsintothesceneimages.Thisobjectivematchestheendgoalofourpredictedlighting–thephotorealismofimageeditingresults.Bothquantita-tiveresultsandqualitativevisualizationindicateitsvalueasacomplementarysupervisionsignaltoexistingdatasets.2.5ImprovingDownstreamPerceptionwithDataAugmentationQualitativeresultsofdataaugmentation.WeshowtheaugmenteddatausingourARpipelineinFigureJ,K,L,M.The3DassetsareprovidedcourtesySupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes13ofTurboSquidandtheirartistsHum3D,befast,rabser,FirelightCGStudio,amaranthus,3DTree_LLC,3dferomonandPipon3D.Notethatthe3Dboundingboxandorientationoftheinsertedobjectsareknownandbecomefreelabelstotraina3Dobjectdetector.AsshowninFigureJ,ourdataaugmentationinsertsadiversesetofcarassetsintocapturedimagesfromthenuScenesdataset[2]basedontheestimatedlighting.Thedatagenerationprocessisfullyautomatic.Oureditingresultscanproducerealisticlightingeﬀects,suchascastshadows,whichrequiresaccurateHDRprediction,and“clearcoat”reﬂectiononthecarbody,whichrequireshigh-frequencylightingpredictionandHDRhighlights.Whilemanyexistingimagemanipulationmethods[4,17]haveunderlyingassumptionsaboutobjectcategories,ourmethodpredictsphysics-basedscenelightinginformation,andthusisagnostictothecategoryofthevirtualobjecttoinsert.Thisenableseditingwithobjectclassesrarelyobservedinrealworld–acriticalusecasefordataaugmentation.FigureKshowsinsertionresultsofconstructionvehicles,whereourmethoddemonstratesconsistentperformance.InFigureL,weshowresultsofocclusionhandling.Weadoptasimplestrategybycomparingthedepthoftheinsertedobject(cachedinG-buﬀer)withthescenedepthmap,andtakethecloserpixeltodisplay.Whileweprimarilyfocusondaytimeoutdoorscenes,ourmethodcanalsopredictreasonableresultsfornight-timescenes,asshowninFigureM.NotethatourmodelhasnoHDRdatasupervisionforthisdomain,duetothescarcityofgroundtruthlightingfornighttimestreetscenes,andonlyreliesonlimitedFoVLDRdataandtheadversarialsupervisiontolearn.Quantitativeresults.Wecompareagainstthebaselinethatusesreal-worlddataonly,andastrongbaselinethataugmentsvirtualobjectswithaﬁxeddomelighting.InTableD,wecanobservethattheperformanceofthedetectorimprovesby2%comparingtothesamedetectorwhentrainedonrealdataalone.Moreover,wecanalsoseethatwhilenaivelyaddingobjectsleadstoa1%improvement,another1%isaresultofhavingbetterlightestimation.Webelievethatfurtherimprovementsarepossiblewithmoresophisticatedplacementstrategiesaswellasusingmoreassets(bothforeachclass,andmoreclasses).Interestingly,noticethattheperformanceoftheobjectdetectoralsoimprovesinotherobjectcategorieseventhoughwedonotdirectlyaugmentthose.Weattributethistovariousfactors,includingprovidingthedetectorwithmorechallengingnegativesforotherclasses,improvingthedetector’sconﬁdenceinclassesthatmaygetconfusedwithconstructionvehicles(e.g.busvsconstructionvehiclesandtrailervsconstructionvehicles),aswellasthewaycertainobjectsareannotated.Forexample,wenoticethatcranesandextremitiesofconstructionvehiclesareonlyincludedinnuScenesannotationsiftheyinterferewithtraﬃc(seeFigureH),thereforewhatconstitutesa3Dboundingboxforthatobjectcategoryisnotuniquelydeﬁned.WeadditionallynoticethatinnuScenestrucksusedtohaulingrocksorbuildingmaterialsareconsideredastruckratherthanconstructionvehicles.Thesefactorsmightalsoexplainwhywedonotseea14Z.Wangetal.bigimprovementintheclassofconstructionvehicleseventhoughwedirectlyaugmentit.MethodmAPcartruckbustrailerconst.veh.pedestrianmotorcyclebicycletraﬃcconebarrierRealData0.1900.3560.1120.1240.0110.0160.3270.1270.1160.3890.317+AugNoLight0.2010.3630.1490.1630.0290.0210.3110.1460.1200.3940.309+AugLight0.2110.3690.1460.1820.0360.0200.3170.1610.1460.4000.332TableD:PerformanceofaSoTA3DobjectdetectoronthenuScenes3Dobjectdetectiontask.mAPrepresentsthemeanforthe10objectcategories.Weadditionallyreporttheaverageprecision(AP)forallcategories.RealDatacorrespondstoasubsetofthenuScenestrainingset(10%).Performanceofthedetectorimprovesby2%whencomparingtoRealData,and1%isduetobetterlightingestimation.(a)ModelTrainedonRealData(b)ModelTrainedonRealData+LightingAugFig.G:Downstream3DDetection.(a)Resultsofa3DobjectdetectortrainedonRealData.(b)ResultsofthesamedetectortrainedonRealData+AugLighting.Weobtainimproveddetectionsaftertrainingondatageneratedwithourmethod.3DiscussionFailurecasesandfutureworks.WeshowfailurecasesofourmethodinFigurePanddescribebelow.Inourcurrentobjectinsertionpipeline,theinsertedobjectpixelswillnotpassexactlythesameimagecapturingprocessasthebackgroundscenepixels,andthuscannotsimulatethesunhaloeﬀectsandblurryrain-dropeﬀects.Inaddition,abettermodelingofthecameraimagesignalprocessor(ISP)pipeline,suchastone-mapping,couldpotentiallyleadtomorerealisticobjectinsertion.Webelieveitisaninterestingfutureworktolearncamerasensorparameters,andhandlethediverseweathersuchasrainy,snowyandfoggyeﬀects.Withunknownscenematerialproperties,ourcurrentshadowmaprendering(mainpaperEq.4)assumesaLambertianscenesurface.Thisassumptionenablesmeasuringtheresidualeﬀectscausedbytheinsertedobjectwitharatioimage.AlthoughtheLambertianassumptionisusuallyagoodapproximationandwidelyadoptedinpriorworks[19,20],theshadowmapqualitywilldecreasewhenthisSupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes15Fig.H:ExampleofnuScenesannotationsofconstructionvehicles.ExtremitiesofconstructionvehiclesareonlyincludedinnuScenesannotationsiftheyinterferewithtraﬃc.Therefore,3Dboundingboxesforthatobjectcategoryarenotuniquelydeﬁned.assumptionnolongerholds.Forexample,thewetroadsurfacesinrainydaysbecomequitespecularandshouldreﬂecttheappearanceoftheinsertedobject,asshowninthesecondcolumnofFigureP.Itisaninterestingdirectiontojointlyestimatescenematerialpropertiestoaddresssuchcomplexeﬀects.InthethirdcolumnofFigureP,oureditingresultsexhibitocclusionartifactswhenthescenedepthmapisinaccurate.Whilepredictingmoreaccuratedepthisoutofthescopeofourpaper,webelieveanadditionalgeometryreﬁnementadoptedinpriorworks[4]couldbebeneﬁcialtoaddressthisissue.Broaderimpact.OurpaperfocusesonaneuralAugmentedReality(AR)pipelineforoutdoorscenes,whichﬁrstestimatesthelightinginformationandinsertvirtualobjectsintotheinputimage.Weshowthatourcarefullydesignedhybridlightingrepresentationhandlesboththespatially-varyingeﬀectsandtheextremeHDRintensityofoutdoorscenes.Thediﬀerentiableobjectinsertionformulationwithanadversarialdiscriminatorservesasavaluablesupervisionsignal,whichfortheﬁrsttimeenableslightingsupervisionbyjointlyleveraginggroundtruthmaterial,geometryandrealworldimages.WeshowthebeneﬁtsofourARapproachonadownstream3Dobjectdetector,indicatingitspotentialasavaluabledataaugmentationtechniqueforsafety-criticalapplicationssuchasautonomousdriving.Ourmethodfallsintothecategoryofworksthatenableimageediting.Whilewebelievethattherearemanypositiveimplications,however–justlikewiththedeepfaketechnology,wecanalsoforeseenefarioususecases,suchasrenderingoﬀensivecontentintophotographs.Technologytargetingdetectingoﬀensivecontentcouldhelpalleviatesuchusecases.16Z.Wangetal.Example1Example2InputLDR&GTHDRReconstructedHDRskyPeakdirectioneditingInputLDR&GTHDRReconstructedHDRskyPeakdirectioneditingInputLDR&GTHDRReconstructedHDRskyPeakdirectioneditingFig.I:Qualitativeresultsoftheskymodelingnetwork.Weshowtwoexampleseachrow,andvisualizewithtwoexposurevaluestoshowLDRandHDR.GivenanLDRskypanoramaasinput,ourskymodelingnetworkcanreconstructHDRpeakswithextremeintensityvalues.Wealsoprovidepeakeditingresultsbychangingthepeakdirectionoftheskyfeaturevector,wherewechangefdirbutﬁxfintensity,flatent.Asaresult,wesuccessfullygenerateHDRskymapwiththesamecontentbutdiﬀerentsundirections.Thisallowsforpotentialpost-editing.SupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes17Fig.J:QualitativeresultsofautomaticdataaugmentationonnuScenesdataset[2].Eachimagecontainsoneinsertedvirtualcarrenderedwithourneu-ralARapproach.18Z.Wangetal.Fig.K:Constructionvehicleinsertionresultsofautomaticdataaugmenta-tiononnuScenesdataset[2].OurneuralARapproachisagnostictothecategoryof3Dassetsandcanrealisticallyinsertvirtualobjectsbelongingtorareclasses.Fig.L:OcclusionresultsofautomaticdataaugmentationonnuScenes[2].WenaivelyhandleocclusionbycomparingthescenedepthmapandobjectZbuﬀer.Scenedepthmapispredictedwithapre-trainedstate-of-the-artmonoculardepthestimationmodelPackNet[7].Fig.M:NighttimeresultsofautomaticdataaugmentationonthenuScenesdataset[2].OurmethodcanproducereasonableresultswithoutanyHDRdatasupervisionforthenighttimeoutdoorscenes.SupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes19Fig.N:QualitativeresultsofobjectinsertionrenderedbyBlender.Ourlight-ingrepresentationcanberenderedintolocation-speciﬁcenvironmentmap,whichiscompatiblewithcommercialrenderers.Integratedwithpowerfulrenderingengines,ourmethodcanleveragecomplexmaterialssuchastransparencyandmulti-bounceray-tracing.Initialeditingresults5iterations10iterationsFig.O:Qualitativevisualizationofthetest-timeoptimization.Tounderstandthebehaviourofthediscriminator,weperformtest-timeoptimizationforthenetworkparameters,wheretheonlyobjectiveistominimizethediscriminatorlossLadv.Ontheﬁrstcolumn,wedisplaythe320x180resolutionimageeditingresults,whichareconsumedbythediscriminator.Thesecondandthethirdcolumnshowstheimageeditingresultsafter5and10iterations.Notehowthediscriminatorcorrectstheshadowdirectionintheﬁrstrow,andremovesobviouserroneoushighlightinthesecondrow.(Bestviewedzoomingin.Werefertoadditionalanimatedresultintheaccompaniedvideo.)20Z.Wangetal.(a)Sensoreﬀects(b)Non-Lambertianshadows(c)OcclusionFig.P:Qualitativeresults–failurecases.(a)Thevirtuallyinsertedobjectmaynotpassexactlythesameenvironmentandsignalprocessingpipelineasthereal-worldcapturedscenepixels,e.g.thevirtualobjectcannotreconstructthehaloeﬀectsofthesunandblurryregionscausedbyraindropsonthecamera.(b)WeassumeaLambertianscenesurfacewhenrenderingtheshadows,andthusthequalityofeditingresultsmaydecreaseforwetspecularroadsurfacesonrainydays.(c)Ourocclusionhandlingreliesondepthordering,andmaygenerateartifactswhenthescenedepthmappredictionisinaccurate.SupplementaryMaterial:NeuralLightFieldEstimationforStreetScenes21References1.Burley,B.,Studios,W.D.A.:Physically-basedshadingatdisney.In:ACMSIG-GRAPH.vol.2012,pp.1–7.vol.2012(2012)42.Caesar,H.,Bankiti,V.,Lang,A.H.,Vora,S.,Liong,V.E.,Xu,Q.,Krishnan,A.,Pan,Y.,Baldan,G.,Beijbom,O.:nuscenes:Amultimodaldatasetforautonomousdriving.arXivpreprintarXiv:1903.11027(2019)6,8,12,13,17,183.Chen,W.,Litalien,J.,Gao,J.,Wang,Z.,Tsang,C.F.,Khalis,S.,Litany,O.,Fidler,S.:DIB-R++:Learningtopredictlightingandmaterialwithahybriddiﬀerentiablerenderer.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS)(2021)4,54.Chen,Y.,Rong,F.,Duggal,S.,Wang,S.,Yan,X.,Manivasagam,S.,Xue,S.,Yumer,E.,Urtasun,R.:Geosim:Realisticvideosimulationviageometry-awarecompositionforself-driving.In:CVPR(2021)13,155.Community,B.O.:Blender-a3Dmodellingandrenderingpackage.BlenderFounda-tion,StichtingBlenderFoundation,Amsterdam(2018),http://www.blender.org76.González,Á.:Measurementofareasonasphereusingﬁbonacciandlatitude–longitudelattices.MathematicalGeosciences42(1),49–64(2010)57.Guizilini,V.,Ambrus,R.,Pillai,S.,Raventos,A.,Gaidon,A.:3dpackingforself-supervisedmonoculardepthestimation.In:IEEEConferenceonComputerVisionandPatternRecognition(CVPR)(2020)3,6,188.He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.CoRRabs/1512.03385(2015),http://arxiv.org/abs/1512.033852,39.Hold-Geoﬀroy,Y.,Athawale,A.,Lalonde,J.F.:Deepskymodelingforsingleimageoutdoorlightingestimation.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.6927–6935(2019)9,10,1110.Hold-Geoﬀroy,Y.,Sunkavalli,K.,Hadap,S.,Gambaretto,E.,Lalonde,J.F.:Deepoutdoorilluminationestimation.In:ProceedingsoftheIEEEConferenceonCom-puterVisionandPatternRecognition.pp.7312–7321(2017)911.Isola,P.,Zhu,J.Y.,Zhou,T.,Efros,A.A.:Image-to-imagetranslationwithcondi-tionaladversarialnetworks.CVPR(2017)812.Karis,B.,Games,E.:Realshadinginunrealengine4.Proc.PhysicallyBasedShadingTheoryPractice4(3)(2013)413.Kingma,D.P.,Ba,J.:Adam:Amethodforstochasticoptimization(2017)8,1214.Li,Z.,Shaﬁei,M.,Ramamoorthi,R.,Sunkavalli,K.,Chandraker,M.:Inverserenderingforcomplexindoorscenes:Shape,spatially-varyinglightingandsvbrdffromasingleimage.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.2475–2484(2020)315.Long,J.,Shelhamer,E.,Darrell,T.:Fullyconvolutionalnetworksforsemanticsegmentation.In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.pp.3431–3440(2015)1,2,316.Mescheder,L.,Oechsle,M.,Niemeyer,M.,Nowozin,S.,Geiger,A.:Occupancynetworks:Learning3dreconstructioninfunctionspace.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.pp.4460–4470(2019)317.Ost,J.,Mannan,F.,Thuerey,N.,Knodt,J.,Heide,F.:Neuralscenegraphsfordynamicscenes.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).pp.2856–2865(June2021)11,1318.Park,T.,Liu,M.Y.,Wang,T.C.,Zhu,J.Y.:Semanticimagesynthesiswithspatially-adaptivenormalization.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition(2019)822Z.Wangetal.19.Sengupta,S.,Gu,J.,Kim,K.,Liu,G.,Jacobs,D.W.,Kautz,J.:Neuralinverserenderingofanindoorscenefromasingleimage.In:InternationalConferenceonComputerVision(ICCV)(2019)1420.Wang,Z.,Philion,J.,Fidler,S.,Kautz,J.:Learningindoorinverserenderingwith3dspatially-varyinglighting.In:ProceedingsofInternationalConferenceonComputerVision(ICCV)(2021)2,3,5,8,9,10,11,1421.Zhang,J.,Sunkavalli,K.,Hold-Geoﬀroy,Y.,Hadap,S.,Eisenman,J.,Lalonde,J.F.:All-weatherdeepoutdoorlightingestimation.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.pp.10158–10166(2019)1122.Zhou,Y.,Huang,J.,Dai,X.,Luo,L.,Chen,Z.,Ma,Y.:HoliCity:Acity-scaledataplatformforlearningholistic3Dstructures(2020),arXiv:2008.03286[cs.CV]8,12