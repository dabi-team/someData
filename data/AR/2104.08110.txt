LU-BZU at SemEval-2021 Task 2:
Word2Vec and Lemma2Vec performance in Arabic Word-in-Context
disambiguation

Moustafa Al-Hajj
Lebanese University
Lebanon
moustafa.alhajj@ul.edu.lb

Mustafa Jarrar
Birzeit University
Palestine
mjarrar@birzeit.edu

Abstract

This paper presents a set of experiments
to evaluate and compare between the per-
formance of using CBOW Word2Vec and
for Arabic Word-in-
Lemma2Vec models
Context (WiC) disambiguation without using
sense inventories or sense embeddings. As
part of the SemEval-2021 Shared Task 2 on
WiC disambiguation, we used the dev.ar-ar
dataset (2k sentence pairs) to decide whether
two words in a given sentence pair carry the
same meaning. We used two Word2Vec mod-
els: Wiki-CBOW, a pre-trained model on Ara-
bic Wikipedia, and another model we trained
on large Arabic corpora of about 3 billion to-
kens. Two Lemma2Vec models was also con-
structed based on the two Word2Vec models.
Each of the four models was then used in the
WiC disambiguation task, and then evaluated
on the SemEval-2021 test.ar-ar dataset.
At the end, we reported the performance of
different models and compared between using
lemma-based and word-based models.

1

Introduction

As a word may denote multiple meanings (i.e.,
senses) in different contexts, disambiguating them
is important for many NLP applications, such as
information retrieval, machine translation, summa-
rization, among others. For example, the word â€œta-
bleâ€ in sentences like â€œI am cleaning the tableâ€, â€œI
am serving the tableâ€, â€œI am emailing the tableâ€, re-
fer to â€œfurnitureâ€, â€œpeopleâ€, and â€œdataâ€ respectively.
Disambiguating the sense that a word denotes in a
given sentence is important for understanding the
semantics of this sentence.

To automatically disambiguate word senses in
a given context, many approaches have been pro-
posed based on supervised, semi-supervised, or
unsupervised learning models. Supervised and
semi-supervised methods rely on full, or partial,
labeling of the word senses in the training corpus

to construct a model (Lee and Ng, 2002; Klein
et al., 2002). On the other hand, unsupervised
approaches induce senses from unannotated raw
corpora and do not use lexical resources. The prob-
lem in such approaches, is that unsupervised learn-
ing of word embeddings produces a single vector
for each word in all contexts, and thus ignoring
its polysemy. Such approaches are called static
Word Embeddings. To overcome the problem, two
types of approaches are suggested (Pilehvar and
Camacho-Collados, 2018): multi-prototype embed-
dings, and contextualized word embeddings. The
latter suggests to model context embeddings as a
dynamic contextualized word representation in or-
der to represent complex characteristics of word
use. Proposed architectures such as ELMo (Peters
et al., 2018), ULMFiT (Howard and Ruder, 2018),
GPT (Radford et al., 2018), T5 (Raffel et al., 2019),
and BERT (Devlin et al., 2018), achieved break-
through performance on a wide range of natural
language processing tasks. In multi-prototype em-
beddings, a set of embedding vectors are computed
for each word, representing its senses. In (Pelevina
et al., 2017), multi-prototype embeddings are pro-
duced based on the embeddings of a word. As such,
a graph of similar words is constructed, then simi-
lar words are grouped into multiple clusters, each
cluster representing a sense. As for Mancini et al.
(2016), multi-prototype embeddings are produced
by learning word and sense embeddings jointly
from both, a corpus and a semantic network. In
this paper we aim at using static word embeddings
for WiC disambiguation.

Works on Arabic Word Sense Disambiguation
(WSD) are limited, and the proposed approaches
are lacking a decent or common evaluation frame-
work. Additionally, there are some speciï¬cities of
the Arabic language that might not be known in
other languages. Although polysemy and disam-
biguating are challenging issues in all languages,

1
2
0
2

r
p
A
6
1

]
L
C
.
s
c
[

1
v
0
1
1
8
0
.
4
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
they might be more challenging in the case of Ara-
bic (Jarrar et al., 2018; Jarrar, 2021) and this for
many reasons. For example, the word Ë‡sÂ¯ahd ((cid:89)(cid:235)(cid:65) (cid:17)(cid:131))
could be Ë‡sÂ¯ahid ((cid:89)(cid:235)(cid:11) (cid:65) (cid:17)(cid:131)) which means a witness, or
(cid:11)(cid:89)(cid:11)(cid:235)(cid:65) (cid:17)(cid:131)) which means watch. As such, dis-
Ë‡sÂ¯ahada (
ambiguating words senses in Arabic, is similar to
disambiguating senses of English words written
without vowels. Second, Arabic is a highly in-
ï¬‚ected and derivational language. As such, thou-
sands of different word forms could be inï¬‚ected
and derived from the same stem. Therefore, words
in word embeddings models will be considered as
different, which may affect the accuracy and the
utility of their representation vectors, as the same
meaning could be incarnated in distributed word
forms in corpora, which has led some researchers
to think that using lemma-based models might be
better than using word-based embeddings in Arabic
(Salama et al., 2018; Shapiro and Duh, 2018). This
idea will be discussed later in sections 5 and 6.

Alkhatlan et al. (2018) suggested an Arabic
WSD approach based on Stem2Vec and Sense2Vec.
The Stem2Vec is produced by training word em-
beddings after stemming a corpus, whereas the
Sense2Vec is produced based on the Arabic Word-
Net sense inventory, such that each synset is rep-
resented by a vector. To determine the sense of a
given word in a sentence, the sentence vector is
compared with every sense vector, then the sense
with maximum similarity is selected.

Laatar et al. (2017) did not use either stemming
or lemmatization. Instead, they proposed to deter-
mine the sense of a word in context by comparing
the context vector with a set of sense vectors, then
the vector with the maximum similarity is selected.
The context vector is computed as the sum of vec-
tors of all words in a given context, which are learnt
from a corpus of historical Arabic. On the other
hand, sense vectors are produced based on dictio-
nary glosses. Each sense vector is computed as the
sum of vectors (learnt from the historical Arabic
corpus) of all words in the gloss of a word.

Other approaches to Arabic WSD (Elayeb, 2019)
employ other techniques in machine learning and
knowledge-based methods (Bouhriz et al., 2016;
Bousmaha et al., 2013; Soudani et al., 2014; Mer-
hbene et al., 2014; Al-Maghasbeh and Bin Hamzah,
2015; Bounhas et al., 2015).

In this paper, we present a set of experiments
to evaluate the performance of using Lemma2Vec
vs CBOW Word2Vec in Arabic WiC disambigua-

tion. The paper is structured as follows: Sec-
tion 2 presetns the background of this work. Sec-
tion 3 overviews the WiC disambiguation system.
Section 4 and Section 5, respectively, present the
Word2Vec and Lemma2Vec models. In Section 6
we present the experiments and the results; and in
section 7 we summarize our conclusions and future
work.

2 Background

Experiments presented in this paper are part of the
SemEval shared task for Word-in-Context disam-
biguation (Martelli et al., 2021).

The task aims at capturing the polysemous na-
ture of words without relying on a ï¬xed sense in-
ventory. A common evaluation dataset is provided
to participants in ï¬ve languages, including Arabic,
our target language in this paper. The dataset was
carefully designed to include all parts of speeches
and to cover many domains and genres. The Ara-
bic dataset (called multilingual ar-ar) consists of
two sets: a train set of 1000 sentence pairs for
which tags (TRUE or FALSE) are revealed, and a
test set of 1000 sentence pairs for which tags were
kept hidden during the competition. Figure 1 gives
two examples of sentence pairs in the dev.ar-ar
dataset. Each sentence pair has a word in com-
mon for which start and end positions in sentences
are provided. Participants in the shared task were
asked to infer whether the target word carries the
same meaning (TRUE) or not (FALSE) in the two
sentences.

Figure 1: Two examples of sentence pairs.

{"id": "dev.ar-ar.0","lemma": "ÙƒÙ„Ø§ÙÙ…","pos": "NOUN","sentence1": "Ø§Ø±Ø¸Ù†ÙˆØ©ÙŠÙ…Ù‡Ù„Ø£Ù‡Ø°Ù‡Ù„Ø¦Ø§Ø³Ù„Ù…Ø§ÙŠØ±Ø³Ù„Ù„Ù…Ø¹Ø©Ù…ÙƒØ­Ù„Ù…Ø§Ù„Ø§Ø¨Ù‚ØªØ³Ù…ØŒÙ…Ø²Ù„ÙŠÙŠØ±ÙÙˆØªÙƒÙ„Ø§Ù…ÙØ§ÙƒÙ†Ù…ÙŠÙ†ÙØ¸ÙˆÙ„Ù…Ø§Ø°Ù†Ù…Ø¡Ø¯Ø¨Ø§Ù‡ØªØ§ÙŠÙ„Ù…Ø¹.","sentence2": "Ù„Ø§ÙˆØ¯Ø¬ÙˆØªØ©Ø³Ø§Ø±Ø­Ù…Ø§Ù…Ø£Ø¹ÙŠÙ…Ø¬ØªØ§Ø«Ø¹Ø¨Ù„Ø§Ø©ÙŠØ³Ø§Ù…ÙˆÙ„Ø¨Ø¯Ù„Ø§Ø¨Ø¨Ø³Ø¨ØµÙ‚Ù†Ù„Ø§Ù†Ù…Ø²Ù„Ù…Ø§ÙÙŠÙƒÙ„Ø§Ù…Ø¯Ø§Ø±ÙØ£Ø©Ø·Ø´Ø±Ù„Ø§ÙÙŠØ±Ù‚Ù…Ø§Ù‡ØªØ¯Ø§ÙŠÙ‚ÙÙŠÙ…ØµØ§Ø¹Ù„Ø§Ø©.â€}{"id": "dev.ar-ar.1","lemma": "ÙƒÙ„Ø§ÙÙ…","pos": "NOUN","sentence1": "Ø§Ø±Ø¸Ù†ÙˆØ©ÙŠÙ…Ù‡Ù„Ø£Ù‡Ø°Ù‡Ù„Ø¦Ø§Ø³Ù„Ù…Ø§ÙŠØ±Ø³Ù„Ù„Ù…Ø¹Ø©Ù…ÙƒØ­Ù„Ù…Ø§Ù„Ø§Ø¨Ù‚ØªØ³Ù…ØŒÙ…Ø²Ù„ÙŠÙŠØ±ÙÙˆØªÙƒÙ„Ø§Ù…ÙØ§ÙƒÙ†Ù…ÙŠÙ†ÙØ¸ÙˆÙ„Ù…Ø§Ø°Ù†Ù…Ø¡Ø¯Ø¨Ø§Ù‡ØªØ§ÙŠÙ„Ù…Ø¹.","sentence2": "ØªØ¨Ø±Ø¹Ø£ÙˆÙ†Ø¹Ø§Ù‡ØªØ¨ØºØ±ÙÙŠÙ„ÙˆØµØ­Ù„Ø§Ù„Ù‰Ø¹ØªØ§Ù…ÙˆÙ„Ø¹Ù…Ù†Ø£Ø´Ø¨Ø¯Ø±Ø§ÙˆÙ…ÙŠÙ†ØªØ§Ù‡ÙŠÙ†ØªØ±Ø§Ø²ÙˆÙ„Ø§Ù…Ø§Ù‡ÙƒÙ„Ø§Ù…ÙˆÙ†Ø£Ø´Ø¨ÙˆÙ†Ø§Ø¬Ù„Ø©Ø¯Ø§Ø¹Ø¥Ù„ÙŠÙ‡Ø£ØªÙ„Ø§Ø©Ø¯Ø§Ø¹Ø¥ÙˆÙŠÙ†Ø·ÙˆØªÙ„Ø§Ø©Ø¯Ø¹Ø§Ø³Ù„Ù…Ø§ÙˆØ¯Ø±Ø§ÙˆÙ„Ø§Ø§Ù‡Ø±ÙƒØ°ÙÙŠØ¶Ø±Ø¹Ù„Ø§ÙŠÙˆÙØ´Ù„Ø§.â€}3 System Overview

This section describes our method to Arabic WiC
disambiguation based on two types of embeddings:
CBOW Word2Vec and Lemma2Vec.

Given two sentences, s1 and s2 , and two words,
vi from s1 and wj from s2 , the objective is to
check whether vi and wj have the same meaning.
To this end, our system extracts contexts of vi and
wj from the sentence pair, represents them in two
vectors and ï¬nally compares the two resulting vec-
tors using the cosine similarity. The context of
a word w of size n (denoted by context(w, n))
is composed of the words that surround the word
w, with n words on the left and n words on the
right (n varying between 1 and 10 in conducted
experiments). To represent context(w, n) in a vec-
tor space, two methods are proposed: ï¬rst one is
based on CBOW Word2Vec embeddings vectors
(Mikolov et al., 2013) of the words appearing in
the context, whereas the second is based on the
Lemma2Vec of lemmas of words appearing in the
context. To select the best way to represent the
context(w, n) by a vector, classiï¬cation experi-
ments were conducted using (i) different pooling
operations, min, max, mean, and std to combine
words/lemmas vectors of the context, (ii) different
threshold values (between 0.55 and 0.85) and (iii)
the removal of functional words (also called stop
words). The later are used to express grammatical
relationships among other words, they are charac-
terized by they high frequency in the corpus which
might affect the WiC disambiguation accuracy. The
cosine similarity is then used to compare vectors of
context(vi, n) and context(wj, n). Figure 2 illus-
trates how the cosine similarity is calculated from
context(vi, 3) and context(wj, 3).

2:

Calculation

Figure
and
context(wj, 3) vectors and the cosine similarity
between.

context(vi, 3)

of

Classiï¬cation experiments on SemEval-2021
ar-ar datasets were conducted using the following

two CBOW Word2Vec models and two correspond-
ing Lemma2Vec models: (i) Wiki-CBOW, a pre-
trained Word2Vec model from the set of AraVec
models (Soliman et al., 2017) , (ii) our CBOW
Word2Vec model that we trained ourselves, (iii)
Lemma2Vec model that we constructed based on
the Wiki-CBOW model, and (iv) Lemma2Vec that
we constructed based on our CBOW Word2Vec
model. Based on these four models, four exper-
iments were conducted to tune the following pa-
rameters: context size (context size), threshold,
pooling operation (pooling) and removing of func-
tional words (stop words).

4 Corpus and trained Word Embeddings

Two CBOW Word2Vec models were used in our ex-
periments. The Wiki-CBOW (Soliman et al., 2017),
which consists of 234,173 vocabulary size, and an-
other model we trained our self which consists of
334,161 vocabulary size. The Wiki-CBOW model
was learnt from a corpus of Arabic Wikipedia arti-
cles of about 78 million words, the principal hyper-
parameters are: 5 for minimum word count and 5
for window size.

Our CBOW Word2Vec model was trained on
Modern Standard Arabic corpora, such as (El-
Khair, 2017; Abbas and Smaili, 2005; Abdelali
et al., 2014) of about 3 billion words; it was ï¬t
using 300-dimensional word vectors, 100 the min-
imum count of words, training epochs of 5 and
window size of 5.

Before training the Word2Vec model, several
normalization and preprocessing steps were per-
formed. First, all diacritics, punctuations, Madda
character, digits (Hindi and Arabic), Latin char-
acters (including accented letters) were removed.
Second, some special Arabic letters are uniï¬ed.
Third, sequences of repeated characters with length
larger than 2 were reduced to one character; re-
peated spaces were also replaced by one space.
(cid:13)
(cid:14)
(cid:64)) are replaced
(cid:64)
Fourth, different forms of Alifs (
with ((cid:64)). Spaces followed by a period character and
new lines were considered to be end of sentence
marks. The split method in Python is used in to-
kenization. The vocabulary size of the resulted
model is 334,161.

(cid:64)(cid:13)

5 Constructing the Lemma2Vec models

Two Lemma2Vec models were produced, based
on both: the Wiki-CBOW Word2Vec model, and
our CBOW Word2Vec model. Each vocabulary in

ğ‘£1â€¦ğ‘£ğ‘–âˆ’3ğ‘£ğ‘–âˆ’2ğ‘£ğ‘–âˆ’1ğ‘£ğ‘–ğ‘£ğ‘–+1ğ‘£ğ‘–+2ğ‘£ğ‘–+3â€¦ğ‘£ğ‘›ğ‘¤1â€¦ğ‘¤ğ‘—âˆ’3ğ‘¤ğ‘—âˆ’2ğ‘¤ğ‘—âˆ’1ğ‘¤ğ‘—ğ‘¤ğ‘—+1ğ‘¤ğ‘—+2ğ‘¤ğ‘—+3â€¦ğ‘¤ğ‘šğ’”ğŸğ’”ğŸContext(ğ‘£ğ‘–,3) Context(ğ‘¤ğ‘—,3)ğ‘£ğ‘–âˆ’31ğ‘£ğ‘–âˆ’32â€¦ğ‘£ğ‘–âˆ’3300ğ‘£ğ‘–âˆ’21ğ‘£ğ‘–âˆ’22â€¦ğ‘£ğ‘–âˆ’2300ğ‘£ğ‘–âˆ’11ğ‘£ğ‘–âˆ’12â€¦ğ‘£ğ‘–âˆ’1300ğ‘£ğ‘–1ğ‘£ğ‘–2â€¦ğ‘£ğ‘–300ğ‘£ğ‘–+11ğ‘£ğ‘–+12â€¦ğ‘£ğ‘–+1300ğ‘£ğ‘–+31ğ‘£ğ‘–+32â€¦ğ‘£ğ‘–+3300ğ‘£ğ‘–+21ğ‘£ğ‘–+22â€¦ğ‘£ğ‘–+2300ğ‘¤ğ‘–âˆ’31ğ‘¤ğ‘–âˆ’32â€¦ğ‘¤ğ‘–âˆ’3300ğ‘¤ğ‘–âˆ’21ğ‘¤ğ‘–âˆ’22â€¦ğ‘¤ğ‘–âˆ’2300ğ‘¤ğ‘–âˆ’11ğ‘¤ğ‘–âˆ’12â€¦ğ‘¤ğ‘–âˆ’1300ğ‘¤ğ‘–1ğ‘¤ğ‘–2â€¦ğ‘¤ğ‘–300ğ‘¤ğ‘–+11ğ‘¤ğ‘–+12â€¦ğ‘¤ğ‘–+1300ğ‘¤ğ‘–+31ğ‘¤ğ‘–+32â€¦ğ‘¤ğ‘–+3300ğ‘¤ğ‘–+21ğ‘¤ğ‘–+22â€¦ğ‘¤ğ‘–+2300Lemma2Vec or Word2Vec of wordsğ‘ğ‘£ğ‘–1ğ‘ğ‘£ğ‘–2â€¦ğ‘ğ‘£ğ‘–300ğ‘ğ‘¤ğ‘—1ğ‘ğ‘¤ğ‘—2â€¦ğ‘ğ‘¤ğ‘—300Word2Vec or  Lemma2Vec of the context ofğ‘¤ğ‘—mean, max, min, stdmean, max, min, stdmean, max, min, stdmean, max, min, stdmean, max, min, stdmean, max, min, stdCosineâ€¦â€¦Lemma2Vec or Word2Vec of the context of ğ‘£ğ‘–each of the Word2Vec models was lemmatized ï¬rst.
Then a vector for each lemma ( i.e., Lemma2Vec) is
calculated as following: ï¬rst all word forms belong-
ing to this lemma are fetched, then their Word2Vec
vectors are combined through a mean pooling op-
eration. The lemmatization process was performed
using in-house tools and lexicographic databases 1
belonging to Birzeit University (Jarrar, 2021; Jarrar
and Amayreh, 2019; Jarrar et al., 2019). In case of
a word cannot be lemmatized due to misspelling,
incorrect tokenization or in case of foreign word
(not included in our database), then the correspond-
ing Lemma2Vec is considered to be its Word2Vec
vector.

Table 1 summarizes the lemmatization results
that we performed on both, the Wiki-CBOW model
and our CBOW Word2Vec model. The lemma-
tized words of SemEval-2021 all ar-ar dataset, as
well as the Word2Vec and Lemma2Vec of ar-ar
datasets wordsâ€™s vectors used in this paper are avail-
able on-line 2.

Wiki-CBOW Our Word2Vec

78M words

3B words

min count 5

min count 100

Unique word forms

Unique lemmas

Words not lemmatized

234,173
100,040
22,054

334,161

54,788

28,098

Table 1: Lemmatization results for both models.

6 Experiments Results and Discussion

Given our Arabic WiC disambigation method de-
scribed in Section 3, and given the SemEval mul-
tilingual dev.ar-ar dataset provided by SemEval-
2021 (Martelli et al., 2021), four classiï¬cation ex-
periments were conducted using the cosine sim-
ilarity and based on the two Word2Vec models
and the two Lemma2Vec models. The objective is
to tune the following parameters for each model:
context size (ranging from 1 to 10), threshold
(we determined empirically the range from 0.55
to 0.85 with 0.1 step size), pooling (min, max,
mean and std), and stop words (yes, no). Then
the values of parameters corresponding to the high
F1-scores for TRUE (T) and FALSE (F) classes
are selected in order to classify sentence pairs in
the test.ar-ar dataset. For each model we did

1https://ontology.birzeit.edu
2https://ontology.birzeit.edu/

semeval2021_data.zip

Exp1

Exp2

Exp3

Exp4

Word2Vec

Lemma2Vec

Word2Vec

Wiki-

CBOW
4
min
0.66
yes

Wiki-

CBOW
1
min
0.56
yes

dev.ar-ar

our

model
4
min
0.83
no

Lemma2Vec

our model

1
mean
0.58
yes

T F
52
54
53

52
51
52

T F
58
57
53
61
56
59

T F
56
55
55

56
57
56

T F
56
56
58
55
57
56

test.ar-ar

57

59

59

60

Model

context size

pooling

threshold

stop words

Dataset

Tag

Precision

Recall

F1-score

Dataset

Accuracy

Table 2: Best F1-score, precision and recall values of
the four experiments on dev.ar-ar dataset with the
values of tuned parameters. Below are accuracies on
test.ar-ar dataset.

the following to ï¬nd the high F1-scores for T and
F: For each context size (between 1 and 10) and
for each value of the stop words (yes or no) we
plotted 8 line plots (4 for T and 4 for F) for each of
the four pooling operations (mean, max, min and
std) and for threshold ranging from 0.55 to 0.85
(i.e., 20 plots for each model, resulting 80 plots).

Figures 3a, 3b, 3c and 3d show the best 4 F1-
scores line plots for each of the four models, and
Table 2 shows the effective F1-scores values for T
and F classes as well as precision and recall values
(best results marked in bold). The values of pa-
rameters corresponding to the best result were then
used in classifying the test.ar-ar dataset. The
accuracies are reported in Table 2 as well.

As shown in Figure 3, the Lemma2Vec models
have the tendency to perform better with shorter
context sizes compared with the Word2Vec mod-
els. A possible reason may be that, in case of
Lemma2Vec, the narrow meaning of words is af-
fected due to the increase number of words in-
volved in Lemma2Vec vector calculation. The
impact of Lemma2Vec on the narrow meaning of
words is discussed in the next subsection.

The results with yes for stop words are slightly
better but not signiï¬cant. Additionally, the min
pooling was generally the best operation to com-
bine the context vectors, and the results of both
min and max pooling were close to each other.

(a) Wiki-CBOW Word2Vec model.
context size = 4 - pooling = min
threshold = 0.66 - stop words = yes

(b) our Word2Vec model.
context size = 4 - pooling = min
threshold = 0.83 - stop words = no

(c) Wiki-CBOW Lemma2Vec model.
context size = 1 - pooling = min
threshold = 0.56 - stop words = yes
Figure 3: The best four F1-scores markers plots for each of the four models.The values of parameters are under
each plot.

(d) our Lemma2Vec model.
context size = 1 - pooling = mean
threshold = 0.58 - stop words = yes

6.1 Lemma2Vec-Word2Vec Error Analyses

This subsection discusses the performance of us-
ing lemma-based vs. word-based models in the
WiC disambiguation task, which we summarize in
Table 3 and Table 4.

TRUE FALSE Total
370
145
225
Correct L2V - Correct W2V
216
98
Correct L2V - Wrong W2V 118
182
116
66
Wrong L2V - Correct W2V
232
141
91
1000
500
500

Wrong L2V - Wrong W2V

Total

Table 3: Wiki-CBOW Lemma2Vec vs. Word2Vec

Table 3 presents the results of experiments
1 and 2 (using Word2Vec and Lemma2Vec of
Wiki-CBOW) whereas Table 4 presents the re-
sults of experiments 3 and 4 (using Word2Vec
and Lemma2Vec of our CBOW model). In each
table, we compare between cases that were cor-
rectly or wrongly classiï¬ed by both models. For
example, the second row in Table 3 shows that
216 sentence pairs (118 TRUE class + 98 FALSE

TRUE FALSE Total
365
241
124
Correct L2V - Correct W2V
233
45
Correct L2V - Wrong W2V 188
236
178
58
Wrong L2V - Correct W2V
166
36
130
1000
500
500

Wrong L2V - Wrong W2V

Total

Table 4: Our Lemma2Vec vs. our Word2Vec

class) were correctly classiï¬ed using the Wiki-
CBOWâ€™s Lemma2Vec model but wrongly classi-
ï¬ed using the Word2Vec. Similarly, 182 sentence
pairs in the third row were correctly classiï¬ed us-
ing the Word2Vec but wrongly classiï¬ed using the
Lemma2Vec.

As shown in both tablesâ€™ second and third rows,
the Lemma2Vec did not signiï¬cantly improve
the overall results; but notably, the Lemma2Vec
shows a signiï¬cant improvement over Word2Vec
for TRUE class whereas Word2Vec is better for
FALSE class.

This conclusion is valid for all models, what-
ever are the corpora content, size and min count

Lemma2Vec outperforms Word2Vec for TRUE sen-
tence pairs, but underperforms it for FALSE sen-
tence pairs.

We plan to extend our work to use our
Lemma2Vec model to build a multi-prototype em-
beddings using the large lexicographic database
available at Birzeit University. We plan also to ï¬ne
tune the recently released Arabic BERT models,
such as (Safaya et al., 2020; Antoun et al., 2020;
Abdelali et al., 2021; Inoue et al., 2021), using the
same database.

Acknowledgments

We would like to thank the shared task organiz-
ers and the reviewers for their valuable comments
and efforts towards improving our manuscript. We
would like to also thank Taymaa Hammouda for
her technical support.

References

Mourad Abbas and Kamel Smaili. 2005. Compari-
son of topic identiï¬cation methods for arabic lan-
guage. In Proceedings of International Conference
on Recent Advances in Natural Language Process-
ing, RANLP, pages 14â€“17.

Ahmed Abdelali, Francisco Guzman, Hassan Sajjad,
and Stephan Vogel. 2014. The amara corpus: Build-
ing parallel language resources for the educational
domain. In LREC, volume 14, pages 1044â€“1054.

Ahmed Abdelali, Sabit Hassan, Hamdy Mubarak, Ka-
reem Darwish, and Younes Samih. 2021.
Pre-
training bert on arabic tweets: Practical considera-
tions.

Mohammad

Khaled

A
MP Bin Hamzah. 2015.
meaning of prepositions at arabic texts:
ploratory study.
30(3):116â€“120.

and
the semantic
an ex-
Int J Comput Trends Technol,

Al-Maghasbeh
Extract

Ali Alkhatlan, Jugal Kalita, and Ahmed Alhaddad.
2018. Word sense disambiguation for arabic exploit-
ing arabic wordnet and word embedding. Procedia
computer science, 142:50â€“60.

Wissam Antoun, Fady Baly, and Hazem Hajj. 2020.
Arabert: Transformer-based model for arabic lan-
In LREC 2020 Workshop
guage understanding.
Language Resources and Evaluation Conference,
page 9.

Nadia Bouhriz, Faouzia Benabbou, and EH Ben Lah-
mar. 2016. Word sense disambiguation approach for
arabic text. International Journal of Advanced Com-
puter Science and Applications, 7(4):381â€“385.

Figure 4: Example of errors.

hyperparameter.

To understand the gain and loss by the lemma-
based models, we manually analyzed most cases.
Figure 4 illustrates such cases. The ï¬rst sen-
tence pair in Figure 4 was correctly classiï¬ed by
the Lemma2Vec (in Exp4) and wrongly by the
Word2Vec (in Exp3). This illustrates that the
lemma vector as a generalized model for its in-
ï¬‚ections (i.e., a mean of word formsâ€™ vectors) was
better in deciding that both contexts are similar and
that the two word forms have the same meaning.
However, the second example in Figure 4 illustrates
the other way. The Lemma2Vec was too general,
and the Word2Vec was speciï¬c enough, to decide
that the two word forms, in the two contexts, are
different. The word from al-Ë‡gins ((cid:129)(cid:9)(cid:28)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64)) could mean
both genus and sex; however the other word form
(cid:13)
al-(cid:45)aË‡gnÂ¯as ((cid:128)(cid:65)(cid:9)(cid:74)(cid:107)(cid:46)
(cid:66)(cid:64)), is semantically distinctive by its
own morphology - as it can only be plural of genus,
and cannot be plural of sex.

To conclude, although Lemma2Vec outperforms
Word2Vec in some cases (mostly in the TRUE sen-
tence pairs class), it underperforms Word2Vec in
others cases (mostly in the FALSE sentence pairs
class). Since the distribution of TRUE and FALSE
is equal in the datasets, the overall performance of
both models is close to each other. Nevertheless, in
case of an application scenario where a large pro-
portion of sentence pairs is expected to be TRUE,
we recommend the use of Lemma2Vec, otherwise
the Word2Vec.

7 Conclusions and Further Work

We presented a set of experiments to evaluate the
performance of using Word2Vec and Lemma2Vec
models in Arabic WiC disambiguation, without
using external resources or any context/sense em-
bedding model. Different models were constructed
based on two different corpora, and different types
of parameters were tuned. The ï¬nal results demon-
strated that Lemma2Vec models are slightly bet-
ter than Word2Vec models for Arabic WiC dis-
ambiguation. More speciï¬cally, we found that

test.ar-ar.342              (Correct with Lemma2Vec  â€“Wrong with Word2Vec)Sentence1: â€¦ÙˆØ§ï»Ÿï»¨ï»ˆïºÙ… Ø§ï»Ÿïº¤ïº23 ï»Ÿï» ïºªÙˆØ±Ø§Øª 8ïº´ï»¤ïº¢ <ï»¤ïº®Ø§ï»‹ïºØ© ï»£ïº¨ïº˜ï» ï»’ ï»£ï»®Ø§ï»—ï»’ Ø§ï»Ÿï»®ï»“ï»®Ø¯ØŒ ÙˆIïº˜ï»¤ïº˜ï»Š !ï»¤ïº®Ùˆï»§ïº”ï»£ï»Œï»´ï»¨ïº”â€¦sentence2: â€¦ï»‹ï»¦ Ø§ï»Ÿï»¤QRSØ§ï»§Uïº” ÙˆVï»‹ïº” ïº‘ïº®Ø§ï»£ïº Ø§ï»Ÿï»¤ï»¨ï»ˆï»¤ïº” Ùˆï»£ïº®Ùˆï»§ïº˜ï»¬ïºï»£ï»¦ ï»£ïº¤ïºªÙˆØ¯Yïº” Ø§ï»Ÿï»¤ï»®Ø§Ø±Ø¯ Ø§ï»Ÿï»¤ïº˜ïºïº£ïº” [S3 Ø§ï»Ÿï»¤QRSØ§ï»§Uïº” Ø§ï»Ÿï»ŒïºØ¯Yïº”.Class: TRUEtest.ar-ar.994             (Wrong with Lemma2Vec  â€“Correct with Word2Vec)Sentence1: ï»³ïº˜ï»¤QRS <ï»˜ï» ïº” Ø£Ùˆ ï»‹ïºªÙ… ÙˆïºŸï»®Ø¯ ïº—ïº¨Uï»¼Øª ïºŸjïº´Uïº” ÙˆØ§ï»Ÿïº®ï»lïº” [S3 ï»£ï»¤ïºØ±ïº³ïº” Ø§ï»Ÿïº .ïº²ï»Ÿï»”QoØ© ï»£ï»¦ Ø§ï»Ÿïº°ï»£ï»¦.sentence2:..Ø§ï»Ÿïº¬ÙŠ Yï»¤QRS Ø§ï»Ÿïº®ÙˆØ§Yïº” ï»‹ï»¦ <ïº[o3 Ø§ï»·ïºŸï»¨ïºØ³Ø§ï»·Ø¯ïº‘Uïº” Ø§ï»Ÿï»¨QtIïº” Ø§ï»·ïº§ïº®Ù‰ØŒ Ùˆwï»§ï»¤ïº ïº—ï»®ïºŸïºª ï»£ï»˜ï»®ï»£ïºØª ï»“ï»¨Uïº” Ø£ïº§ïº®Ù‰ â€¦Class: FALSEIbrahim Bounhas, Raja Ayed, Bilel Elayeb, Fabrice
Evrard, and Narjes Bellamine Ben Saoud. 2015. Ex-
perimenting a discriminative possibilistic classiï¬er
with reweighting model for arabic morphological
disambiguation. Computer Speech & Language,
33(1):67â€“87.

KZ Bousmaha, S Charef Abdoun, L Hadrich Bel-
guith, and MK Rahmouni. 2013. Une approche
de dÂ´esambiguÂ¨Ä±sation morpho lexicale Â´evaluÂ´ee sur
lâ€™analyseur morphologique alkhalil. Revue RISTâ€”
Vol, 20(2):33.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.

Ibrahim Abu El-Khair. 2017. Abu el-khair corpus: A
modern standard arabic corpus. International Jour-
nal of Recent Trends in Engineering & Research
(IJRTER), 03(1):95â€“100.

Bilel Elayeb. 2019. Arabic word sense disambiguation:
a review. Artiï¬cial Intelligence Review, 52(4):2475â€“
2532.

Jeremy Howard and Sebastian Ruder. 2018.

Fine-
tuned language models for text classiï¬cation. CoRR,
abs/1801.06146.

Go Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda
Bouamor, and Nizar Habash. 2021. The interplay
of variant, size, and task type in Arabic pre-trained
language models. In Proceedings of the Sixth Ara-
bic Natural Language Processing Workshop, Kyiv,
Ukraine (Online). Association for Computational
Linguistics.

Mustafa Jarrar. 2021. The arabic ontology - an arabic
wordnet with ontologically clean content. Applied
Ontology Journal, 16(1):1â€“26.

Mustafa Jarrar and Hamzeh Amayreh. 2019. An arabic-
multilingual database with a lexicographic search en-
In International Conference on Applications
gine.
of Natural Language to Information Systems, vol-
ume 11608 of LNCS, pages 234â€“246. Springer.

Mustafa Jarrar, Hamzeh Amayreh, and John P McCrae.
2019. Representing arabic lexicons in lemon - a
preliminary study. In The 2nd Conference on Lan-
guage, Data and Knowledge (LDK 2019), volume
2402, pages 29â€“33. CEUR.

Mustafa Jarrar, Fadi Zaraket, Rami Asia, and Hamzeh
Amayreh. 2018. Diacritic-based matching of ara-
bic words. ACM Transactions on Asian and Low-
Resource Language Information Processing (TAL-
LIP), 18(2):10:1â€“10:21.

Dan Klein, Kristina Toutanova, H Tolga Ilhan, Sepan-
dar D Kamvar, and Christopher D Manning. 2002.
Combining heterogeneous classiï¬ers for word sense
In Proceedings of the ACL-02
disambiguation.
workshop on Word sense disambiguation: recent
successes and future directions, pages 74â€“80.

Rim Laatar, Chaï¬k Aloulou, and Lamia Hadrich Bel-
guith. 2017. Word sense disambiguation of arabic
language with word embeddings as part of the cre-
ation of a historical dictionary. In LPKM.

Yoong Keok Lee and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithms for word sense disambiguation.
In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2002), pages 41â€“48.

Massimiliano Mancini, Jose Camacho-Collados, Igna-
cio Iacobacci, and Roberto Navigli. 2016. Em-
together via joint
bedding words and senses
arXiv preprint
knowledge-enhanced training.
arXiv:1612.02703.

Federico Martelli, Najla Kalach, Gabriele Tola, and
Roberto Navigli. 2021. SemEval-2021 Task 2: Mul-
tilingual and Cross-lingual Word-in-Context Disam-
In Proceedings of the Fif-
biguation (MCL-WiC).
teenth Workshop on Semantic Evaluation (SemEval-
2021).

Larousi Merhbene, Anis Zouaghi, and Mounir Zrigui.
2014. Approche basÂ´ee sur les arbres sÂ´emantiques
pour la dÂ´esambiguÂ¨Ä±sation lexicale de la langue arabe
en utilisant une procÂ´edure de vote. In Proceedings of
the 21st conference on natural language processing
(TALN 2014), Marseille, France, pages 281â€“290.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efï¬cient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781.

Maria Pelevina, Nikolay Arefyev, Chris Biemann, and
Alexander Panchenko. 2017. Making sense of word
embeddings. arXiv preprint arXiv:1708.03390.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. arXiv preprint arXiv:1802.05365.

Mohammad Taher Pilehvar and Jose Camacho-
Collados. 2018. Wic: the word-in-context dataset
for evaluating context-sensitive meaning representa-
tions. arXiv preprint arXiv:1808.09121.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Improving language under-

Ilya Sutskever. 2018.
standing by generative pre-training.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2019. Exploring the limits
of transfer learning with a uniï¬ed text-to-text trans-
former. CoRR, abs/1910.10683.

Ali Safaya, Moutasem Abdullatif, and Deniz Yuret.
2020. KUISAIL at SemEval-2020 task 12: BERT-
CNN for offensive speech identiï¬cation in social
In Proceedings of the Fourteenth Work-
media.
shop on Semantic Evaluation, pages 2054â€“2059,

Barcelona (online). International Committee for
Computational Linguistics.

Rana Aref Salama, Abdou Youssef, and Aly Fahmy.
2018. Morphological word embedding for arabic.
Procedia computer science, 142:83â€“93.

Pamela Shapiro and Kevin Duh. 2018. Morphological
word embeddings for arabic neural machine trans-
In Proceedings of
lation in low-resource settings.
the Second Workshop on Subword/Character LEvel
Models, pages 1â€“11.

Abu Bakr Soliman, Kareem Eissa, and Samhaa R El-
Beltagy. 2017. Aravec: A set of arabic word embed-
ding models for use in arabic nlp. Procedia Com-
puter Science, 117:256â€“265.

Nadia Soudani, Ibrahim Bounhas, Bilel ElAyeb, and
Yahya Slimani. 2014.
Generic normalization
approach of arabic dictionaries for arabic word
sense disambiguation. Proceedings of Cinqui`eme
JournÂ´ees Francophones sur les Ontologies (JFO),
Hammamet, Tunisia, pages 309â€“315.

