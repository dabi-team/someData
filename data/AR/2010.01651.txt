0
2
0
2

t
c
O
4

]

C
H
.
s
c
[

1
v
1
5
6
1
0
.
0
1
0
2
:
v
i
X
r
a

Interface Design for HCI Classroom: From
Learners’ Perspective

Huyen N. Nguyen[0000−0001−6554−2327], Vinh T. Nguyen[0000−0002−1300−3943],
and Tommy Dang[0000−0001−8322−0014]

Texas Tech University, Lubbock TX 79409, USA
{huyen.nguyen,vinh.nguyen,tommy.dang}@ttu.edu

Abstract. Having a good Human-Computer Interaction (HCI) design
is challenging. Previous works have contributed signiﬁcantly to fostering
HCI, including design principle with report study from the instructor
view. The questions of how and to what extent students perceive the
design principles are still left open. To answer this question, this paper
conducts a study of HCI adoption in the classroom. The studio-based
learning method was adapted to teach 83 graduate and undergraduate
students in 16 weeks long with four activities. A standalone presenta-
tion tool for instant online peer feedback during the presentation session
was developed to help students justify and critique other’s work. Our tool
provides a sandbox, which supports multiple application types, including
Web-applications, Object Detection, Web-based Virtual Reality (VR),
and Augmented Reality (AR). After presenting one assignment and two
projects, our results showed that students acquired a better understand-
ing of the Golden Rules principle over time, which was demonstrated by
the development of visual interface design. The Wordcloud reveals the
primary focus was on the user interface and shed some light on students’
interest in user experience. The inter-rater score indicates the agree-
ment among students that they have the same level of understanding
of the principles. The results show a high level of guideline compliance
with HCI principles, in which we witnessed variations in visual cognitive
styles. Regardless of diversity in visual preference, the students presented
high consistency and a similar perspective on adopting HCI design prin-
ciples. The results also elicited suggestions into the development of the
HCI curriculum in the future.

Keywords: Human-computer interaction · instant online peer feedback
· interface design · learners’ perspective · user study design · inter-rater
measurement

1

Introduction

HCI has a long, rich history, and its origin can be dated back to the 1980s with
the advent of personal computing. As such, computers were no longer consid-
ered an expensive tool, and room-sized dedicated to experts in a given domain.
Consequently, the need to have an easy and eﬃcient interaction for general and

 
 
 
 
 
 
2

H. N. Nguyen et al.

untrained users became increasingly vital for technology adoption. In recent
years, along with the presence of new devices (e.g., smartphones, tablets), HCI
has expanded its perceptual concept from interaction with a computer to that
of any target device, and it has been incorporated into multiple disciplines, such
as computer science, cognitive science, and human-factors engineering.

Having a good HCI design is a challenging task since it not only has to cope
with “more than just a computer now... but considerable awareness that touch,
speech, and gesture-based interfaces” [4] but also requires a substantial cognitive
eﬀort to think and make the product that encompassed the aspects of useful,
usable, and ultimately used by the public [13], such as user interfaces assisting
monitoring and system operational tasks [7,15]. Literature work has contributed
signiﬁcantly to fostering HCI from imposing design principles, processes, guide-
lines to teaching. For example, the ACM SIGCHI Executive Committee [12]
developed a set of curriculum recommendations for HCI in education, or Ben
Shneiderman [22] suggested eight golden rules of interface design. Adopting the
existing guidelines, teachers/instructors have attributed an abundance of eﬀorts
to support students in understanding the concept, developing, and creating a
good interaction design in classrooms [4,10]. In line with instructing learners,
curriculum, teaching/learning methods, project outputs, tools/techniques are
reported to share with the HCI community. In this regard, ﬁndings are often
observed from the instructors’ perspectives, the questions of how, and to what
extent students perceive the design principles are unexplored.

Having the answers to these questions would play an essential indicator for
both educators and learners as it allows them to reshape their perceptual think-
ing on how HCI is being taught and learned. For instructors, looking at HCI
from students’ perspectives enables them to reorganize teaching materials and
methods so that learning performance can be best achieved. For learners, the
opportunity of having their point of view to justify or being justiﬁed by instruc-
tors/peers would allow them to develop the critical thinking skills prepared for
their future careers.

To the best of our knowledge, no work in the literature exploits these ques-
tions in the HCI domain, making this research a unique contribution. In this
study, we seek to answer the aforementioned questions by decomposing them
into sub-questions as 1) Given a set of design principles/guidelines, to what ex-
tents students follow them, 2) Which part of the HCI design the learners focus
on, and 3) Do they have the same perspectives on adopting design principles and
are these views consistent? By addressing these research questions qualitatively
and quantitatively, the contributions of our paper can be laid out as:

– it reports the instructional methods and instruments used for teaching and

learning HCI in the classroom.

– it provides an analysis of the qualitative method for student peer-review

project assessment.

– it extracts insights of HCI design principles adoption in the classroom.

The rest of this paper is organized as follows: Section 2 summarizes existing
research that is close to our paper. Section 3 presents study design methods for

Interface Design for HCI Classroom: From Learners’ Perspective

3

collecting students’ information. Section 4 analyzes data and provide insights in
detail. Section 5 concludes our paper with future work direction.

2 Related work

Numerous researches on teaching HCI guidelines have been studied: from the
general design of the course, major topics should be covered to the incorpora-
tion of user-centered design. One of the fundamental literature in designing the
HCI curriculum is presented in 1992 by Hewett et al. [12], “ACM SIGCHI Cur-
ricula for Human-Computer Interaction”. The report provided “a blueprint for
early HCI courses” [4], which concentrated on the concept of “HCI-oriented,”
not “HCI-centered,” programs. Therefore, it is beneﬁcial to frame the problem of
HCI broadly enough to aid learners and practitioners to avoid the classic pitfall
of design separated from the context of the problem [12]. Regarding the in-class
setting, PeerPresents [21] introduced a peer feedback tool using online Google
docs on student presentations. With this approach, students receive qualitative
feedback by the end of class. To encourage immediacy in the discussion, our
system provides feedback on-the-ﬂy: the presenters receive feedback and ques-
tions as they are submitted to the system, the presenters can start the discussion
right after their presentation session. Instant Online Feedback [8] demonstrates
the use of instant online feedback on face-to-face presentations. Besides the sim-
ilar feedback form containing quantitative and qualitative aspects, we include
the demo of the ﬁnal interface within the form so that the audience can di-
rectly perform testing. Online feedback demonstrates its advantages in terms of
engagement, anonymity, and diversity. Online Feedback System [11] facilitates
students’ motivation and engagement in the feedback process. Compared to the
face-to-face setting, the establishment of anonymity encourages more students to
participate and contributes to balanced participation in the feedback process [8].
Along with a higher quantity, diversity in the feedback elicits novel perspectives
and reveals unique insights [16]. For a classroom setting, time is often limited
within a class session. With such time restriction, the time frame dedicated to
giving feedback should be held rather low [8]. Peer feedback should be delivered
in a timely manner [14]: for online classes, the feedback only demonstrates its
eﬃciency if provided within 24 hours. Another study shows that instant feed-
back helps enhance accuracy estimate than that received at the end of a study
because of its immediacy [17].

3 Materials and Methods

3.1 Teaching method and activities

The main goal of our study is to investigate the adoption of HCI design prin-
ciples in the classroom setting from the learners’ perspective. It is a daunting
task for instructors to justify the adoption level due to variations in visual cog-
nitive styles. For example, teachers may like bright and high contrast design,

4

H. N. Nguyen et al.

whereas students prefer a colorful dashboard. Thus, a teaching method and as-
sessment should be carefully designed to help motivate and engage students and
adequately reﬂect their performance level. Literature work has proposed several
teaching methods [3], such as inquiry-based learning, situated-based learning,
project-based learning, and studio-based learning. As such, we chose the studio-
based learning approach due to its characteristics, encompassed our critical fac-
tors. These characteristics can be laid out as follows:

– Students should be engaged in project-based assignments (C1).
– Student learning outcomes should be iteratively assessed in formal and in-

formal fashion through design critiques (C2).

– Students are required to engage in critiquing the work of their peers (C3).
– Design critiques should revolve around the artifacts typically created by the

domains (C4).

Based on the above criteria, we divided the entire course work into four
main activities spanning over 16-week long, including 1) Introduction to HCI, 2)
Homework assignment, 3) Projects engagement: Throughout the course, students
were actively involved in two projects (characteristic C1), and 4) Evaluation and
design critiques: In each assignment/projects both instructors and students were
engaged for evaluations (characteristics C2, C4), this process not only helps to
avoid bias in the results but also allows learners to reﬂect their learning for
justifying their peers (characteristic C3). The assignment’s purpose is to give
students some practice in the design of everyday things. In the ﬁrst project,
we incorporated the problem-based learning method on the problem extended
from the assignment. The second project had a diﬀerent approach by adapting
the computational thinking-based method, where students can explore a real-life
problem and then solve it on their own.

3.2 Participants

The present study was conducted with university students enrolled in the Human-
Computer Interaction course in computer science. There was a total of 83 stu-
dents, of which 62 students are undergraduate, 13 masters, and eight doctoral
students. There were 64 males and 19 females.

3.3 Assessment tool

Aalberg and Lors [1] indicated that ‘the lack of technology support for peer
assessment is one likely cause for the lack of systematic use in education.’ To
address these issues, many assessment tools have been introduced [8,9,21]. How-
ever, no such a comprehensive tool enables instructors to carry out a new task
that has not been presented. Particularly, in our study, the evaluation requires
features such as instant feedback, timing control, list preview, or even 3D ob-
jects embedded applications. We developed a standalone tool to facilitate peer
evaluation and data collection, supporting multiple application types, including
webpage, web-based VR, and AR. The tool is expected to serve these goals:

Interface Design for HCI Classroom: From Learners’ Perspective

5

Fig. 1. User interface of the web presentation application in various contexts. A) List
of presentations from the instructor’s view, B) Presenter’s view, C) Audience’s view.

– G1 From the presenter’s perspective: Each presenter (group or individual)
demonstrates their interface design within their session. The presenter can
see the feedback and evaluation visualization within their session.

– G2 From the audience’s perspective: The audience can give comments and
evaluations for the current design presented [9]. Authentication for the in-
class audience is necessary for input validity.

– G3 The system presents online, instant evaluation from the audience (anony-

mously) to the presenter without interrupting the presentation.

User Interface Figure 1 describes our web presentation system according to
three perspectives. Panel A shows the presenter list (student names and group
members’ images have been customized for demonstration purposes). This view
allows the instructors to manage students’ turn to present, each group has two
thumbnail images for sketch and ﬁnal design, indicating the development pro-
cess. Panel B is the presenter’s view with live comments on the right-hand side,
updated on-the-ﬂy. The average scores and evaluation are updated live in the
presenter’s view – visualized in an interactive, dynamic chart. Panel C is the
audience’s view for providing scores and feedback on the presenting project.

6

H. N. Nguyen et al.

Each presentation is given a bounded time window for demonstration and
discussion. The system automatically switches to the next presentation when
the current session is over, renewing both Panel B and C interfaces. During
the presentation, the clock timer in the two panels are synchronized, and the
assessment and comments submitted from Panel C are updated live on Panel
B. Questions for peer assessment consist of seven 10-point Likert scale ques-
tions (which ranged from “strongly disagree(1)” to “strongly agree(10)”) and
one open-ended question. The criteria for scoring are as follows.

– Q1, Does the interface follow the Golden rules and principle in design [22]?
– Q2, Usability: The ease of use of the interface [24].
– Q3, Visual appealing: Is the design visually engaging for the users [23]?
– Q4, Interactivity: To what extent does the interface provide user interac-

tions [23]?

– Q5, Soundness: The quality and proper use of the audio from the applica-

tion [23].

– Q6, Eﬀorts: Does the group provide enough eﬀort for the work?
– Q7, Teamwork: Is the work equally distributed to team members?

3.4 Assignment and Project outputs

There were a total of 126 visual interaction designs as assignment and project
outcomes, including 82 sketches from the assignment, 21 problem-based designs,
and 22 computational thinking based solutions. Figure 2 presents some selected
work corresponding to the assignment, project 1, project 2, respectively.

Previous work showed that there was some ambiguity in the results when
students gave scores to their peers. Part of the issue is that they wanted to
be ‘nice’ or aimed to get the job done. To alleviate these issues, unusual score
patterns will be excluded, and evaluators (students) will be notiﬁed anonymously
through their alias names. Examples of good grading and inadequate grading are
presented in Figure 3. Good grading is illustrated via the diversity in assessment
outcome among diﬀerent criteria, opposite to inadequate examples.

4 Results

4.1 R1: Given a set of design principles/guidelines, to what extents

students follow them?

In order to tackle the ﬁrst research question, we are looking for an indication
among the students that reﬂects the overall level of guideline compliance. To
measure the extent that the learners follow the guidelines, we use the typical
statistic measures mean and standard deviation, where each record is a quanti-
tative assessment from a user to a presenter, spreading on the provided criteria.
Table 1 presents the mean and standard deviation on the peer assessment
score for project 1 and project 2. In both projects, Eﬀorts criterion always
has the highest mean value, indicating that the students highly appreciate the

Interface Design for HCI Classroom: From Learners’ Perspective

7

Fig. 2. Selected work on interface design for the individual assignment (A), project 1
(B), project 2 (C). For each project 1 and 2, sketches are on the left, and ﬁnal designs
are on the right. Our study covers a variety of applications, including Object Detection
(as a Computer Vision application), Web-based 3D application, VR, and AR. The
interdisciplinary nature is presented in the ﬁnal result.

eﬀorts their peers put in. Regarding project 1, Visual Design has the lowest
mean (8.20) and also the highest standard deviation (1.48), showing variation
visual cognitive styles and diverse individual opinions on what should be con-
sidered “visually engaging”. In project 2, Golden Rules criterion has the mean
increased and became the criterion with the lowest standard deviation (1.12),
meaning that there is less diﬀerence between the perception of Golden Rules
among the students, demonstrating that the students present better compliance
to the guidelines. As an experience learned from project 1, Sounds was intro-
duced in project 2 as a channel for feedback interactions. Sounds having the
lowest mean (7.33) and highest standard deviation (2.30), setting it apart from
other standard deviations (around 1.1 and 1.2), indicating that the students had
diﬃculty incorporating this feature on their projects. Overall, the other criteria
have their mean values increased from project 1 to project 2, showing a better
presentation and understanding of the guidelines.

4.2 R2: Which part of the HCI design the learners focus on?

We answer this question using a qualitative method for analysis; we take inputs
from students’ comments for each project. We expected that the majority of the
captured keywords would be centered around UI design, as indicated in previous
literature that most of the HCI class focus on UI design [10,12].

8

H. N. Nguyen et al.

Fig. 3. Examples of good (panel A) and inadequate (panel B) peer grading for 18
students. Each dot represents grade (from 1 to 10), corresponds to ﬁve criteria above.

Table 1. Mean and standard deviation on peer assessment score for project 1 and 2

Criteria

Mean Standard deviation

Project 1

Project 2

8.39
Golden Rules
8.90
Eﬀorts
8.56
Interactivity
Usability
8.45
Visual Design 8.20

8.52
Golden Rules
8.78
Eﬀorts
8.60
Interactivity
Usability
8.54
Visual Design 8.30
7.33
Sounds

1.30
1.23
1.27
1.31
1.48

1.12
1.24
1.23
1.17
1.36
2.30

We constructed a wordcloud of the most frequent words in regards to course
content, with stop words removed. Figure 4 presents the most frequent words
used in the project 1 – interface design for a smart mirror. Bigger font size in
the wordcloud indicates more frequent occurrences, hence more common use of
the words. Essentially, wordcloud gives an engaging visualization, which can be
extended with a time dimension to maximize its use in characterizing subject de-
velopment [6], being able to provide insights within an interactive, comprehensive
dashboard [19]. Hereafter, the number in parentheses following a word indicates
the word’s frequency. In terms of dominant keywords, besides the design topic
such as mirror (72), design (40), interface (33), the students were interested in
the service that the interface provides: widget (34) and feature (23), then color
(24), text (20) button (19) – the expectation on fundamental visual components
aligned, with cluttered (16) and consistent (16) – highlighting the most common
pitfall and standard that the design should pay close attention. Functionality in
design are taken into account: touch (4), draggable (8), facial recognition (6),
voice command (3). As such, when looking at an application, students focused
primarily on UI design, hence our study conﬁrmed existing research [10,12].

Interface Design for HCI Classroom: From Learners’ Perspective

9

Furthermore, user experience (UX) concerns are demonstrated through the
students’ perspective. By exploring more uncommon terms, user’s experience
is indicated by: understandable (4), usability (2), helpful (2). Users’ emotions
and attitudes towards the product are expressed: easy (8), love (5) and enjoy
(2) (positive), in contrast to hard (15), distracting (5), diﬃcult (3), confusing
(2) (negative). Indeed, the comments reﬂect the views of students: “The mirror
is well done. The entire thing looks very consistent and I enjoy the speaking
commands that lets you know what you are doing.” (compliment), “Accessing the
bottom menu to access dark mode could be a little confusing for some users as no
icons are listed on the screen.” (suggestion). The ﬁndings on UX can complement
existing work in ways that emphasize the need to integrate UX subjects in the
HCI curriculum. HCI principles can be considered a crucial instrument for UX
development; these results validated that HCI is the forerunner to UX design [13].
Overall, we can classify the keywords into three groups: UI, UX and Imple-
mentation. Besides UI and UX discussed above, the Implementation aspect is
viewed through data (10), implementation (8), api (6), function (5), and coding
(2). Compared to the expectation, there is a minimal amount of terminologies
to the principle used, such as golden rule (2) – with only two occurrences, as op-
posed to the large number of visual elements mentioned in practical development.
Some students even suggested wheelchair (3), as they consider the design for a
variety of users. This pattern demonstrates that the focus shifts from theoretical
design to direct visual aesthetics, as the students perceived and adopted the HCI
principles eﬀectively to apply them in the empirical application. To sum up, the
keywords retrieved from the wordcloud cover primarily UI and UX interests from
the students’ perspective in the process of adopting HCI into interface design,
demonstrating the diversity and broad coverage that peer assessment outcomes
can provide. The method can be scaled to other disciplines that regard crowd
wisdom in the development process.

4.3 R3: Do they have the same perspectives on adopting the design

principles and are these views consistent?

To answer the third question, we are looking for an agreement among students
when they evaluated their peers. We hypothesize that the HCI principles are
adopted when the scores provided by learners are agreed, and this agreement
may imply that students have the same level of understanding of the principles.
To measure the level of agreement among learners, we use the intraclass
correlation coeﬃcient (ICC) [18], where each student is considered a rater, the
project is the subject of being measured. ICC is widely used in the literature
because it is easy to understand, can be used to assess both relative and absolute
agreement, and the ability to accommodate a broad array of research scenarios
compared to other measurements such as Cohen’s Kappa or Fleiss Kappa [20].
Cicchetti [5] provides a guideline for inter-rater agreement measures, which can
be brieﬂy described as poor (less than 0.40), fair (between 0.40 and 0.59), good
(between 0.60 and 0.74), excellent (between 0.75 and 1.00).

10

H. N. Nguyen et al.

Fig. 4. Wordcloud constructed from students’ comments about the design for project
1. This is the top 150 most common words used, distributed in three categories: UI,
UX, and Implementation.

Table 2. Inter-rater agreement measures for project 1 and project 2.

Subjects Raters Type

Agreement Consistency ICC scale [5]

Project 1

Project 2

19
19
19
19
19

21
21
21
21
21
21

77 Golden Rules
77 Eﬀorts
77
77 Usability
77 Visual Design

Interactivity

Interactivity

75 Golden Rules
75 Eﬀorts
75
75 Usability
75 Visual Design
Sounds
75

0.963
0.900
0.955
0.965
0.971

0.922
0.932
0.896
0.879
0.964
0.970

0.972
0.970
0.962
0.972
0.977

0.947
0.952
0.923
0.886
0.971
0.977

excellent
excellent
excellent
excellent
excellent

excellent
excellent
excellent
excellent
excellent
excellent

Table 2 provides the results on the level of agreement and consistency when
students evaluate their peers. There is a diﬀerence in the number of raters (83
students in total, 77 in project 1, and 75 in project 2); this is due to the exclusion
of ambiguous responses, as noted in Section 3.4 and Figure 3. It can be seen from
Table 2 that students tend to give the same score (agreement score = 0.963)
given the principle guideline (or golden rules) in project 1, so do as in project 2
(agreement score = 0.922). These scores are considerably high (excellent) when
mapped to the inter-rater agreement measure scales suggested by Cicchetti [5].
In addition, we also ﬁnd a high consistency among students when evaluating the

UIUXImplementationmirrordesignwidgetinterfacecolorfeaturetextbuttonoveralliconconsistentclutteredhardusermenuweathervisuallycleansimpledatanewssizefunctionalityrecognitioneasydraggablepresentationcalendarmodeappealingimplementationhealthbackgroundimplementedidealightapilayoutcamerafacefacial recognitionﬁtthemefontspotifyinformationitemschemefunctiondarkloveuniqueawesomeamazingdistractingorganizedapplicationvoicestylingmusicdisplaysketchunderstandabletimetouchUIinteractivitysimplicitycontrolspaceimpressivefeelpersonaleventsshakingyoutubetwittercentercompletewheelchairabilitydifﬁcultdragdropscrollvoice commandgraphcomprehensivecohesivetweetshoverdisplayingtransparentaccessusabilityoutsideinsidefrontmessagetechnicaleffectivecomponentsplainp5jsconfusingfacialenjoyautomaticallyinterestingcornerhelpfulcollisionsresizeﬁltercodingcomplainopinionharmonyuniﬁedinstructionssmartoverviewtopgolden rulevideovoice recognitionmovingpanelblocksrelevantbadportionfeedbacklocationunevenwellprepareddaynightoverlappingzindexannoyedintuitivelypointlessreactusefulsocial mediaframesinsightfulrestartoverpowerblurryInterface Design for HCI Classroom: From Learners’ Perspective

11

other aspects of the projects such as eﬀorts, interactivity, usability, and visual
design since consistency scores are all above 0.75.

5 Conclusion and Future work

In this paper, we have presented the learners’ perspective on the perception and
adoption of HCI principles in a classroom setting. A standalone web presentation
platform is utilized to gather instant online peer feedback from students through-
out the course. The qualitative analysis of peer feedback demonstrated that the
students primarily emphasize the design feature and then visual components
and interactivity. This approach can be extended to present subject evolution
and student development over time, providing a bigger context. The outcome
also pointed out that the interests spread in both UI and UX, suggesting further
incorporation of UX in the HCI curriculum, which is currently having a shortage
in the existing literature. On the quantitative analysis, we have found that the
students followed the guidelines by a large margin. The results indicated a high
level of agreement among the students, determined by the inter-rater agreement
measures. We also have found high consistency within the class regarding other
aspects evaluated, such as eﬀorts, interactivity, usability, and visually appealing.
This research, however, is subject to several limitations. The course duration
in this study is short, with only 16 weeks; a more prolonged period can help
reduce random errors. Another limitation is the sample size, with 83 students;
we believe that a larger sample size could increase the generalizability of the
result (nevertheless, our sample size meets the minimum requirement suggested
by [2]). We will expand the study in the following semesters for future work, and
we expect that researchers can conduct related studies to conﬁrm our ﬁndings.

References

1. Aalberg, T., Lor˚as, M.: Active learning and student peer assessment in a web
development course. In: Norsk IKT-konferanse for forskning og utdanning (2018)
2. Albano, A.: Introduction to educational and psychological measurement using R

(2017)

3. Carter, A.S., Hundhausen, C.D.: A review of studio-based learning in computer

science. Journal of Computing Sciences in Colleges 27(1), 105–111 (2011)

4. Churchill, E.F., Bowser, A., Preece, J.: Teaching and learning human-computer

interaction: past, present, and future. interactions 20(2), 44–53 (2013)

5. Cicchetti, D.V.: Guidelines, criteria, and rules of thumb for evaluating normed and
standardized assessment instruments in psychology. Psychological assessment 6(4),
284 (1994)

6. Dang, T., Nguyen, H.N., Pham, V.: WordStream:

Interactive Visualiza-
tion for Topic Evolution.
In: Johansson, J., Sadlo, F., Marai, G.E.
(eds.) EuroVis 2019 - Short Papers. The Eurographics Association (2019).
https://doi.org/10.2312/evs.20191178

7. Dang, T., Pham, V., Nguyen, H.N., Nguyen, N.V.: AgasedViz: visualizing ground-
water availability of Ogallala Aquifer, USA. Environmental Earth Sciences 79(5),
1–12 (2020)

12

H. N. Nguyen et al.

8. Figl, K., Bauer, C., Kriglstein, S.: Students’ view on instant online feedback for

presentations. AMCIS 2009 Proceedings p. 775 (2009)

9. Gehringer, E.F.: Electronic peer review and peer grading in computer-science

courses. ACM SIGCSE Bulletin 33(1), 139–143 (2001)

10. Greenberg, S.: Teaching human computer interaction to programmers. interactions

3(4), 62–76 (1996)

11. Hatziapostolou, T., Paraskakis, I.: Enhancing the impact of formative feedback
on student learning through an online feedback system. Electronic Journal of E-
learning 8(2), 111–122 (2010)

12. Hewett, T.T., Baecker, R., Card, S., Carey, T., Gasen, J., Mantei, M., Perlman, G.,
Strong, G., Verplank, W.: ACM SIGCHI curricula for human-computer interaction.
ACM (1992)

13. Interaction Design

tion
human-computer-interaction (2019), last accessed on July 1, 2020

Foundation: What

Interac-
https://www.interaction-design.org/literature/topics/

is Human-Computer

(HCI)?

14. Kulkarni, C.E., Bernstein, M.S., Klemmer, S.R.: Peerstudio: rapid peer feedback
emphasizes revision and improves performance. In: Proceedings of the second
(2015) ACM conference on learning@ scale. pp. 75–84 (2015)

15. Le, D.D., Pham, V., Nguyen, H.N., Dang, T.: Visualization and explainable ma-
chine learning for eﬃcient manufacturing and system operations. Smart and Sus-
tainable Manufacturing Systems 3(2), 127–147 (2019)

16. Ma, X., Yu, L., Forlizzi, J.L., Dow, S.P.: Exiting the design studio: Leveraging
online participants for early-stage design feedback. In: Proceedings of the 18th
ACM Conference on Computer Supported Cooperative Work & Social Computing.
pp. 676–685 (2015)

17. Magrabi, F., Coiera, E.W., Westbrook, J.I., Gosling, A.S., Vickland, V.: General
practitioners’ use of online evidence during consultations. International journal of
medical informatics 74(1), 1–12 (2005)

18. McGraw, K.O., Wong, S.P.: Forming inferences about some intraclass correlation

coeﬃcients. Psychological methods 1(1), 30 (1996)

19. Nguyen, H.N., Dang,
from social media.

T.:
In:

ics
alytics
https://doi.org/10.1109/VAST47406.2019.8986947

and Technology

(VAST).

Science

EQSA:
2019

Earthquake
IEEE Conference
pp.

situational

analyt-
on Visual An-
2019).

(Oct

142–143

20. Nichols, T.R., Wisner, P.M., Cripe, G., Gulabchand, L.: Putting the kappa statistic

to use. The Quality Assurance Journal 13(3-4), 57–61 (2010)

21. Shannon, A., Hammer, J., Thurston, H., Diehl, N., Dow, S.: Peerpresents:
A web-based system for in-class peer feedback during student presentations.
In: Proceedings of the 2016 ACM Conference on Designing Interactive Sys-
tems. p. 447–458. DIS ’16, Association for Computing Machinery, New York,
NY, USA (2016). https://doi.org/10.1145/2901790.2901816, https://doi.org/
10.1145/2901790.2901816

22. Shneiderman, B., Plaisant, C., Cohen, M., Jacobs, S., Elmqvist, N., Diakopoulos,
N.: Designing the user interface: strategies for eﬀective human-computer interac-
tion. Pearson (2016)

23. Sims, R.: Interactivity: A forgotten art? Computers in human behavior 13(2),

157–180 (1997)

24. T. Nguyen, V., Hite, R., Dang, T.: Web-based virtual reality development in
classroom: From learner’s perspectives. In: 2018 IEEE International Conference
on Artiﬁcial Intelligence and Virtual Reality (AIVR). pp. 11–18 (Dec 2018).
https://doi.org/10.1109/AIVR.2018.00010

