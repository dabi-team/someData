4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface

Yang Li1 Hikari Takehara2 Takafumi Taketomi2 Bo Zheng2 Matthias Nießner3

1The University of Tokyo 2Tokyo Research Center, Huawei

3Technical University Munich

1
2
0
2

y
a
M
5

]

V
C
.
s
c
[

1
v
5
0
9
1
0
.
5
0
1
2
:
v
i
X
r
a

Figure 1: Given an input partial scan and inter-frame scene ﬂow from a non-rigidly deforming scene (left), our method jointly
recovers missing geometry (middle) and a volumetric motion ﬁeld (right). The colors show the motion directions on the unit
sphere (bottom right corner) where the vector’s length corresponds to the magnitude of the motion.

Abstract

1. Introduction

Tracking non-rigidly deforming scenes using range sen-
sors has numerous applications including computer vision,
AR/VR, and robotics. However, due to occlusions and phys-
ical limitations of range sensors, existing methods only han-
dle the visible surface, thus causing discontinuities and in-
completeness in the motion ﬁeld. To this end, we intro-
duce 4DComplete, a novel data-driven approach that es-
timates the non-rigid motion for the unobserved geometry.
4DComplete takes as input a partial shape and motion ob-
servation, extracts 4D time-space embedding, and jointly
infers the missing geometry and motion ﬁeld using a sparse
fully-convolutional network. For network training, we con-
structed a large-scale synthetic dataset called Deform-
ingThings4D, which consists of 1,972 animation sequences
spanning 31 different animals or humanoid categories with
dense 4D annotation. Experiments show that 4DComplete
1) reconstructs high-resolution volumetric shape and mo-
tion ﬁeld from a partial observation, 2) learns an entan-
gled 4D feature representation that beneﬁts both shape and
motion estimation, 3) yields more accurate and natural de-
formation than classic non-rigid priors such as As-Rigid-
As-Possible (ARAP) deformation, and 4) generalizes well
to unseen objects in real-world sequences.

Understanding the motion of non-rigidly deforming
scenes using a single range sensor lies at the core of many
computer vision, AR/VR, and robotics applications. In this
context, one fundamental limitation is that a single-view
range sensor cannot capture data in occluded regions, lead-
ing to incomplete observations of a 3D environment. As a
result, existing non-rigid motion tracking methods are re-
stricted to the observable part of the scene. However, the
ability to infer complete motion from a partial observation
is indispensable for many high-level tasks. For instance, as
a nursing robot, to safely care for an elderly person (e.g.,
predict the person’s action and react accordingly), it needs
to understand both the complete body shape and how the
whole body moves even if the person is always partially oc-
cluded.

In order to address these challenges, we pose the ques-
tion how can we infer the motion of the unobserved geome-
try in a non-rigidly deforming scene? Existing works such
as DynamicFusion [38] and VolumeDeform [26] propose to
propagate deformations from the visible surface to the invis-
ible space through a latent deformation graph. Hidden de-
formations are then determined by optimizing hand-crafted
deformation priors such as As-Rigid-As-Possible [47] or
Embedded Deformation [49], which enforces that graph
vertices locally move in an approximately rigid manner.

 
 
 
 
 
 
Such deformation priors have several limitations: 1) they re-
quire heavy parameter tuning; 2) they do not always reﬂect
natural deformations; and 3) they often assume a continuous
surface. As a result, these priors are mostly used as regular-
izers for local deformations, but struggle with larger hidden
regions. One promising avenue towards solving this prob-
lem is to leverage data-driven priors that learn to infer the
missing geometry. Very recently, deep learning approaches
for 3D shape or scene completion and other generative tasks
involving a single depth image or room-scale scans have
shown promising results [12, 46, 11, 7, 10]. However, these
works primarily focus on static environments.

In this paper, we make the ﬁrst effort to combine geom-
etry completion with non-rigid motion tracking. We argue
that the shape and motion of non-rigidly deforming objects
are highly entangled data modalities: on one hand, the abil-
ity to infer the geometry of unobserved object parts pro-
vides valuable information for motion estimation. On the
other hand, motion is considered as the shape’s evolution in
the time axis, as similarity in motion patterns are a strong
indicator for structural connectivity. To leverage these syn-
ergies, we propose 4DComplete, which jointly recovers the
missing geometry and predicts motion for both seen and
unseen regions. We build 4DComplete on a sparse, fully-
convolutional neural network, which facilitates the joint es-
timation of shape and motion at high resolutions. In addi-
tion, we introduce DeformingThings4D, a new large-scale
synthetic dataset which captures a variety of non-rigidly
deforming objects including humanoids and animals. Our
dataset provides holistic 4D ground truth with color, opti-
cal/scene ﬂow, depth, signed distance representations, and
volumetric motion ﬁelds.

In summary, we propose the following contributions:

• We introduce 4DComplete, the ﬁrst method that jointly
recovers the shape and motion ﬁeld from partial obser-
vations.

• We demonstrate that these two tasks help each other,
resulting in strong 4D feature representations outper-
forming existing baselines by a signiﬁcant margin.

• We provide a large-scale non-rigid 4D dataset for
The dataset consists
training and benchmarking.
of 1,972 animation sequences, and 122,365 frames.
Dataset is available at: https://github.com/
rabbityl/DeformingThings4D.

2. Related Work

2.1. Non-Rigid Tracking Using Depth Sensors

Many methods for non-rigid tracking use variations
of the Non-rigid Iterative Closest Point (N-ICP) algo-
rithm [1, 41, 29, 59], where the point-to-point or point-
to-plane distance of correspondences points are iteratively

minimized. To prevent uncontrolled deformations and re-
solve motion ambiguities, the N-ICP optimization usually
employs deformation regularizers such as As-Rigid-As-
Possible (ARAP) [47] or embedded deformation [49]. One
of the ﬁrst real-time methods to jointly track and reconstruct
non-rigid surfaces was DynamicFusion [38]. VolumeDe-
form [26] extends the ideas of DynamicFusion by adding
sparse SIFT feature matches to improve tracking robust-
ness. Using deep learning, DeepDeform [5] replaces the
classical feature matching by CNN-based correspondence
matching. Li et al. [30] goes one step further and differen-
tiates through the N-ICP algorithm thus obtaining a dense
feature matching term. A similar direction is taken by Neu-
ral Non-Rigid Tracking [4]; however, their focus lies in an
end-to-end robust correspondence estimation. To handle
topology changes, KillingFusion [45] directly estimates the
motion ﬁeld given a pair of signed distance ﬁelds (SDF).
Optical/scene ﬂow [15, 51, 50, 53, 32, 55, 33, 22, 34] is a
closely related technique. They have been used to generate
initial guess for non-rigid tracking in [17, 18, 4, 16, 54].
Among these works, FlowNet3D [32] is one of the ﬁrst
methods that directly estimates scene ﬂow from two sets of
point clouds. While existing methods mainly focus on the
visible surface of a scene, we take one step further to model
the deformation of the hidden surface.

2.2. Shape and Scene completion

Completing partial 3D scans is an active research area
in geometry processing. Traditional methods, such as Pois-
son Surface Reconstruction [28], locally optimize for a sur-
face to ﬁt observed points and work well for small missing
regions. Zheng et al. [57] predict the unobserved voxels
by reasoning physics and Halimi et al. [24] complete par-
tial human scans by deforming human templates. More
recently, we have seen 3D CNNs with promising results
for geometry completion for depth scans [46, 12, 11, 10].
These works operate either on a single depth image of
a scene as with SSCNet [46], or scene completion on
room- and building ﬂoor-scale scans, as shown by Scan-
Complete [11] and SGNN [10]. An alternative line of re-
search for shape completion uses implicit scene representa-
tions [37, 39, 42, 40, 31, 7, 27]; however, while these ap-
proaches achieve stunning results for ﬁtting and interpola-
tion of objects/scenes, they still struggle to generalize across
object categories with high geometric variety. While these
existing works mainly focus on static scenes, we investigate
how to leverage the synergies of shape completion in the
dynamic 4D domain.

2.3. Non-Rigid 4D Datasets

Collecting large scale 4D datasets for deforming ob-
jects is a non-trivial task, in particular when the goal is
to obtain a sufﬁciently large number of objects. Non-rigid

Figure 2: The network architecture of 4DComplete (Pink capsule: training loss; ⊕: concatenation; ⊗: ﬁlter by geometry,
number in the brackets: (nin, nout) feature dimension). The input partial TSDF and VMF are concatenated together and fed
into the 4D encoder. The two decoders predict the complete TSDF and VMF in parallel. There are 4 hierarchical levels. The
shape decoder predicts the geometry in each hierarchical level and passes the predicted geometry to the corresponding layer
in the motion decoder and the following layer in the shape decoder. Our method is trained on cropped volumes of spatial
dimension 96 × 96 × 128, which covers about 70 percent of an object. The fully-convolutional nature of our approach enables
testing on whole objects of arbitrary sizes.

datasets [14, 3, 2, 52, 23, 56, 26, 45, 5, 58, 35, 60] have
been widely used, but they are either relatively small, lim-
ited to speciﬁc scene types, or suffer from occlusion and
sensor noise; hence, they are not directly suited for our
4D completion task. Notably, obtaining dense motion ﬁeld
ground truth from real-world 3D scans is quite challenging
as it requires costly per-point correspondence annotations.
This is one of the reasons why we have seen many syn-
thetic datasets in the context of dense optical ﬂow methods
[36, 6, 34, 44]; Among them, Sintel [6] and Monka [36]
are composed of rendered animations of deforming objects.
However, these sequences are relatively short and do not
provide complete 3D shapes and motion ﬁelds. In order to
facilitate learning data-driven deformation priors, we intro-
duce a much larger synthetic dataset with over 1,972 anima-
tion sequences, spanning a large diversity of objects ranging
from humanoids to various animal species (cf. Sec. 4).

3. Method: 4DComplete

Given a single-view depth map observation of a 3D
scene, and the scene ﬂow that is computed between the cur-
rent frame and its next frame, the goal of 4DComplete is to
recover the hidden geometry and its motion ﬁeld.

Input. We use a 3D volumetric grid to represent both shape
and motion. The input shape is represented as a truncated
signed distance ﬁeld (TSDF), as a sparse set of voxel lo-
cations within truncation and their corresponding distance
values. The TSDF is computed from a single depth map us-
ing volumetric fusion [9]; i.e., every voxel is projected into

the current depth map and their distance values are updated
accordingly. To represent the input motion of the visible
surface, we pre-compute the 3D motion vector (in R3) for
each occupied voxel, resulting in a volumetric motion ﬁeld
(VMF) representation. We concatenate the TSDF and VMF
and feed it as input to a neural network.

Scene Flow Field ⇔ Volumetric Motion Field. We use
FlowNet3D [32] to predict the motion for the visible sur-
face, which estimates the scene ﬂow ﬁeld (SFF) between
two sets of point clouds. Because a 3D point does not
necessarily lie on a regular 3D grid position, we convert
between the SFF and the VMF as follows: given a point
cloud {pi|i = 1, ..., N }, where pi ∈ R3 are XYZ coordi-
nates of individual point, the SFF is deﬁned as {SFF i|i =
1, ..., N }, where SFF i ∈ R3 are the 3D translational
motion vectors of the points. Similarly, given a set 3D
voxel positions {vj|j = 1, ..., M }, the VMF is deﬁned as
{VMF i|i = 1, ..., M }, where VMF i ∈ R3 are the 3D
translational motion vectors of the voxels. To convert from
SFF to VMF, we use the inverse-distance weighted interpo-
lation as deﬁned in [43]:

VMF j =

(cid:88)

pi∈knn(vj )

SFF i · dist(pi, vj)−1
pi∈knn(vj ) dist(pi, vj)−1

(cid:80)

(1)

where knn() is the function to ﬁnd K-Nearest-Neighbors.
We set the number of neighbors to K = 3, and dist(, )
computes the euclidean distance between two positions. To

convert from VMF to SFF, we do tri-linear interpolation:

SFF j =

(cid:88)

vj ∈knn(pi)

VMF j · w(pi, vj)

(2)

where w(, ) computes the linear-interpolation weights, and
K = 8 represents the neighboring 8 corners of the cube that
a point lies in.

Network Architecture. To allow for high-resolution out-
puts of the shape and motion ﬁeld, we leverage sparse con-
volutions [21, 20, 8] for our neural network architecture,
which makes our architecture computationally efﬁcient in
processing 3D volumetric data by operating only on the sur-
face geometry. Hence, our method only processes the sur-
face region and ignores the truncated region. Fig. 2 shows
an overview of our network architecture. The network con-
sists of a shared 4D encoder and two decoders to estimate
shape and motion in parallel. The input sparse tensor is
ﬁrst fed into the 4D Encoder, which encodes the data using
a series of sparse convolutions where each set reduces the
spatial dimensions by a factor of two. The two decoders are
designed in a coarse-to-ﬁne architecture with 4 hierarchical
levels. We use skip connections between the 4D encoder
and the 2 decoders to connect feature maps of the same spa-
tial resolution. Since the shape decoder usually generates
a larger set of sparse locations than the input, we use zero
feature vector for the locations that do not exist in the input
volume.

Message passing between two branches. At a hierar-
chical level k, the shape decoder predicts the voxels’ oc-
cupancy Ok and TSDF value Sk. We ﬁlter voxels with
sigmoid(Ok(v)) > 0.5 as the input geometry for the next
hierarchical level. Within each hierarchical level, the shape
decoder feeds the predicted geometry to the parallel motion
decoder to inform where the motion should be estimated. In
return, the motion feature is ﬁltered by the sparse geometry
and shared to the shape decoder.

Shape Loss. The shape decoder’s ﬁnal output is a sparse
TSDF from which a mesh can be extracted by Marching
Cubes. Following [10], we apply an l1 loss on the log-
transformed TSDF values. Using the log-transformation on
the TSDF values helps to shift the losses attention more to-
wards the surface points as larger values far away from the
surface get smaller, thus encouraging more accurate pre-
diction near the surface geometry. We additionally employ
proxy losses at each hierarchy level for outputs Ok and Sk,
using binary cross-entropy with target occupancies and l1
with target TSDF values, respectively.

Motion Loss. The output of our sparse neural network is
facilitated by the motion decoder, which estimates the com-
pleted volumetric motion ﬁeld {VMF i|i = 1, ..., M }. The

ground truth motion ﬁeld at the predicted sparse locations
is given by {VMF i,gt|i = 1, ..., M }. We formulate the
loss for the motion ﬁeld on the ﬁnal predicted sparse loca-
tions using the l2 loss: (cid:80)M
2. In
addition, we apply the cosine similarity loss: (cid:80)M
i=1(1 −
||VMF i||·||VMF i,gt|| ) on the normalized motion vectors to
encourage the directions of the motion to be consistent with
the ground truth.

i=1 ||VMF i − VMF i,gt||2

VMF i·VMF i,gt

Progressive Growing. We train our network in a progres-
sively growing fashion following the ideas of [10]. There
are four hierarchy levels, we progressively introduce higher
resolution geometry decoder after every 2000 training itera-
tions. To facilitate motion decoder learning, instead of using
the predicted geometry of shape decoder, we fed ground-
truth geometry to motion decoder during the beginning 10K
iterations.

Training. We use our newly-constructed DeformingTh-
ings4D dataset (c.f. Sec. 4) to train our network. At train-
ing time, we consider cropped views of scans for efﬁciency
(see Fig. 2); we use random crops of size [96 × 96 × 128]
voxels for the ﬁnest level. We crop the volumes at 1 meter
intervals out of each train object and discard empty volume.
The resolution drops by a factor of 2, resulting resolution
of [48 × 48 × 64], [24 × 24 × 32], and [12 × 12 × 16] for
each hierarchical level. The fully-convolutional design of
our approach enables testing on whole objects of arbitrary
sizes at testing time. To learn viewpoint-invariant motion
representation, we apply random rigid rotation transforma-
tions on the 3D motion vectors as data augmentation during
training. The randomness is drawn from the Haar distribu-
tion [48], which yields uniform distribution on SO3. We
train our network using the Adam optimizer with a learning
rate of 0.001 and a batch size of 8.

4. DeformingThings4D Dataset

Training our network requires a sufﬁcient amount of
non-rigidly deforming target sequences with ground truth
4D correspondences at the voxel level (i.e., motion and
shape). In order to provide such data, we construct a syn-
thetic non-rigid dataset, DeformingThings4D, which con-
sists of a large number of animated characters including hu-
manoids and animals with skin mesh, texture, and skeleton.
We obtained the characters from Adobe Mixamo1 where
humanoid motion data was collected using a motion cap-
ture system. Animals’ skin and motion are designed by CG
experts. Generally, these objects are animated by using “rig-
ging” and “skinning” to blend the skeletal movement to the
surface skin mesh. Fig. 3 shows examples of characters in
the dataset and the statistics of our dataset.

1https://mixamo.com

Figure 3: DeformingThings4D dataset. Left: examples of animated characters. Right: dataset statistics. In total, we collected
147 different characters spanning 31 categories, with a total of 1,972 animations and 122,365 frames.

4.1. Data Generation

Given a animated 3D mesh, we generate per-frame
RGB-D maps, inter-frame scene ﬂow, signed distance ﬁeld
and volumetric motion ﬁeld; see Fig. 4. We perform data
generation with Blender2 scripts.

Figure 4: Data Generation Process: given an animated 3D
mesh (a), virtual cameras are sampled on a sphere. One of
the cameras is selected as the input view, for which depth
maps (b) are rendered. Depth frames are used to compute
the projective TSDF (e) and inter-frame scene ﬂow (f). The
ground-truth complete TSDF (c) is computed by integrating
the depth images from all virtual cameras. The complete 3D
motion ﬁeld (d) is obtained by blending the mesh vertices’
motion to nearby occupied voxels.

RGB-D Map. To render depth maps, we uniformly sample
42 camera viewpoints on a sphere that is centered by the tar-
get character’s mesh. The mesh-to-camera distance ranges
within 0.5 − 2.5m. We render all the depth maps using the
intrinsic parameters of the Azure Kinect camera. We store
per-pixel depth in millimeters and render the color chan-
nel using Blender’s built-in Eevee engine with a principled
BSDF shader.

2https://www.blender.org/

Inter-frame Scene Flow Field (SFF). The mesh anima-
tions run at 25 frames per second. We track the mesh ver-
tices’ 3D displacements between a pair of temporally adja-
cent frames and project the 3D displacements to the cam-
era’s pixel coordinates as scene ﬂow. The ﬂow vector for a
pixel is computed by interpolating the 3 vertices’ motion on
a triangle face where the pixel’s casted ray is ﬁrst received.
We generate scene ﬂow ground truth for all observable pix-
els in the source frame even if the pixels are occluded in
the target frame. To simulate the different magnitudes of
deformation we sub-sample the sequences using the frame
jumps: {1, 3, 7, 12}.

Signed Distance Field (SDF). In order to generate the
ground truth SDF, we volumetrically fuse the depth maps
from all virtual cameras into a dense regular grid [9], where
each voxel stores a truncated signed distance value. We re-
peat this process independently for four hierarchy levels,
with voxel sizes of 1.0cm3, 2.0cm3, 4.0cm3, and 8.0cm3.
From the input depth map, we compute the projective SDF
with voxel sizes of 1.0cm3 as network input while setting
the truncation to 3× the voxel size. TSDF values are stored
in voxel-distance metrics, which facilitates testing on vol-
umes with arbitrarily sampled voxel size.

Volumetric Motion Field (VMF). We compute the mo-
tion ground-truth for all voxels near the mesh surface, i.e.,
within 3x voxel truncation. For each valid voxel, we ﬁrst
ﬁnd its K-nearest-neighbor vertices on the mesh surface
and then use Dual Quaternion Blending (DQB) to bind the
motion of the KNN vertices to the voxel position. Empiri-
cally, we set K = 3. We follow the same procedure for the
SDF volume and we repeat this process independently for
all four resolutions, i.e., with voxel size of 1.0cm3, 2.0cm3,
4.0cm3, and 8.0cm3.

Method

FlowNet3D

DeepDeform Dataset [5]
EPE↓ ACC(5)↑ ACC (10)↑ EPE↓ ACC(5)↑ ACC(10)↑ EPE↓ ACC(5)↑ ACC(10)↑
7.36
DeformingThings4D (Ours) 3.74

80.04% 21.07 24.62 % 45.09% 16.88 38.49% 67.17%
36.89% 71.67%

69.43%
82.02% 91.63% 13.08 27.78 % 61.26% 17.01

Training dataset
FlyingThings3D [36]

DeformingThings4D

KITTI SFLow [19]

Table 1: Scene ﬂow estimation results on the DeformingThings4D, DeepDeform [5], and KITTI [19] datasets. Metrics are
end-point-error (EPE) in centimeters, and Accuracy ( <5cm or 5%, 10cm or 10%) for motion.

Figure 5: Testing on a pair of real-world RGB-D images.

5. Results

5.1. Evaluation Metrics

Motion Estimation Evaluation Metric. Following [32],
we use 3D end-point-error (EPE) and motion accuracy
(ACC) as our motion evaluation metrics. The 3D EPE mea-
sures the average euclidean distance between the estimated
motion vector to the ground truth motion vector. The ACC
score measures the portion of estimated motion vectors that
are below a speciﬁed end-point-error among all the points.
We report two ACC metrics with two different thresholds.
Note that throughout the experiments we convert all VMF
to SFF (using Eqn. 2) before doing motion evaluation.

Shape Completion Evaluation Metric. We use the fol-
lowing metrics to evaluate the reconstructed geometry, Vol-
umetric IoU (IoU), Chamfer Distance (CD) in centimeters,
Surface Normal Consistency (SNC), Point to Plane distance
(P2P), and (cid:96)1 error of SDF value.

5.2. Benchmarking Scene Flow

We compare our DeformingThings4D dataset with the
FlyingThings3D [36], which is a large-scale dynamic mo-
tion dataset consists of ﬂying rigid objects. We train
FlowNet3D [32] with the two datasets and evaluate it on
the test sets of DeformingThings4D, the DeepDeform [5],
and the KITTI [19] scene ﬂow benchmark. The results are
shown in Tab. 1. DeepDeform [5] is a very challenging real-
world benchmark for non-rigid motion. The FlowNet3D
model trained on our dataset signiﬁcantly reduces the scene
ﬂow error on the real-world DeepDeform benchmark (from
21.07 to 13.08). KITTI dataset captures street scenes with
mainly rigid cars moving around which is more close to the
ﬂying things scenario. Our dataset still shows comparable
results to FlyingThings3D on KITTI.

5.3. Motion Prediction for the Hidden Surface

This section evaluates the motion estimation of the hid-
den surface. We conduct the following experiment:
the
complete mesh shape, a subset of mesh vertices that is vis-
ible from a given camera viewpoint, and the ground truth
scene ﬂow for the visible vertices are given, and the goal is
to estimate the motion of the hidden vertices of the mesh.
We evaluate the following methods:

•Rigid Fitting. This method assumes that the shape under-
goes rigid motion. It ﬁnds a single rigid transform in SE(3)
for the entire shape that best explains the surface motion.

•As-Rigid-As-Possible (ARAP) Deformation. ARAP [47]
is widely used as a deformation prior in non-rigid recon-
struction [38, 26, 59].
It assumes that locally, a point is
transformed with a rigid transformation. Such rigid con-
straints are imposed upon nearby vertices that are connected
by edges. ARAP deformation ﬁnds for each mesh vertex a
local fan-rotation R ∈ SO(3) and a global translation vec-
tor t ∈ R3 that best explains the scene ﬂow motion with the
local rigidity constraints.

•Motion Complete (Ours). Given the complete shape and
the partial motion on the visible surface, this method pre-
dicts the VMF for the complete shape and converts it to SFF
to get the motion on mesh vertices’ positions. This method
is trained only on humanoid motions and evaluates on an
animal motion subset (we aim to conﬁrm how the model
generalizes across domains).

•Motion Complete + Post Processing (PP) (Ours). We
found that the motion prediction of our Motion Complete
model is sometimes noisy. We employ optimization-based
post-processing to alleviate the noise: the predicted motion
ﬁled on the mesh surface is jointly optimized with ARAP
prior that enforce that nearby vertices have similar motions.

Tab. 2 reports the motion estimation results for the oc-
cluded surface. The testing sequence includes one hu-
manoid sequence and 6 animal sequences with different an-
imations. Note that our method is trained only on the hu-
manoids dataset. Among the baselines, rigid ﬁtting yields
signiﬁcantly larger errors on most sequences, which indi-
cate that the sequences undergo large non-rigid motion. Our
Motion Complete overall achieves lower end-point-error
than the ARAP on most sequences. Motion Complete + PP
further improves the numbers. Fig. 6 shows the qualitative
results of surface deformation for the “Deer” and “Dairy

Figure 6: Surface deformation for the “Deer” and “Dairy Cow” sequence. The complete shape and the motion of the visible
surface (blue) are given, and the goal is to estimate the deformation of the hidden surface (red). The gray mesh shows the
ground truth deformation (gray), which is not available/used for registration. ARAP leads to severe distortion on the neck
and head of the deer; The dairy cow’s stomach is undergoing a contraction movement. ARAP can not evenly distribute such
deformation, leading to unnatural surface folding at the stomach. Our method yields natural deformations for both sequences.
Note that our method is trained only on humanoid motions.

Methods

Rigid Fitting
ARAP Deformation [47]
Motion Complete (Ours)
Motion Complete + PP (Ours)

Humanoids
(Samba Dance)
15.30
3.24
2.32
1.81

Dairy Cow
(Attack)
17.67
2.73
2.90
2.24

Moose Bull
(Walk)
2.98
1.27
1.34
1.21

Fox
(Jump)
18.78
5.71
4.88
4.20

Dear Stag
(Attack)
16.96
4.99
4.08
2.05

Panthera Onca
(Run)
22.61
13.78
8.56
7.18

Avg.

94.23
31.72
24.08
18.69

Table 2: Quantitative evaluation for the motion estimation results of the unobserved surface. The Metric is 3D End-Point-
Error (EPE) in centimeter. Note that our method is trained only on humanoid motions.

Cow” sequence. The deformed surfaces are achieved by us-
ing the estimated motion to warp the source model. Our
method yields more plausible deformation than ARAP de-
formation for the occluded surface. We conclude that the
3D sparse ConvNets with large receptive ﬁelds learn to cap-
ture the global deformation.

5.4. Real-World Results

Fig. 5 shows that our method, trained only on our syn-
thetic data, generalizes well to a real-world RGB-D input
captured with an Azure Kinect camera.

5.5. Ablation of Shape and Motion Estimation

This experiment examines how the two tasks, geome-
try completion and motion estimation, inﬂuence each other.
To get the scene ﬂow of the visible surface, we re-train
FlowNet3D [32] using our scene ﬂow dataset. FlowNet3D
predicts the SFF given a pair of point clouds with a subsam-
pled size of 2048. We convert the sparse SFF to VMF using
Eqn. 1 as network input. The voxel position in the VMF is
consistent with the input projective TSDF.

As deﬁned in Fig. 2, we alternatively remove the shape
completion head or the motion estimation head to exam-
ine the synergy of the two tasks. Tab. 3 reports the mo-
tion prediction results for the visible surface. Though only
evaluating on the visible surface, the model trained with the
added supervision of the geometry completion task show
improvement over the model trained only on motion pre-
diction. This demonstrates that complete the missing shape
is beneﬁcial for non-rigid motion estimation. Tab. 4 reports
the geometry completion results in our synthetic Deform-
ingThings4D dataset. The whole model shows improve-
ment over the model that is trained for geometry completion
only. This result validates the idea that in a dynamic scene
it is beneﬁcial to understand the motion in order to achieve
better geometric completion.

5.6. Shape Completion Results

We show qualitative shape completion results of our ap-
IF-Nets [7] is a state-of-the-art method that per-
proach.
forms single depth image reconstruction from point clouds.
At the core of IF-Nets is an implicit function that maps a 3D

Method
Ours (w/o shape completion)
Ours (w/ shape completion)

EPE↓
3.82
3.56

ACC(5)↑
79.02%
85.02%

ACC (10)↑
90.55%
91.59%

EPE↓
13.75
13.15

ACC(5)↑
26.89%
28.57%

ACC(10)↑
63.42%
63.66%

DeformingThings4D

DeepDeform Dataset [5]

Table 3: Scene Flow estimation results on our DeformingThings4D dataset and DeepDeform [5] dataset. All scores are
reported only for the visible surface points. Metrics are end-point-error (EPE) in centimeter, and accuracy ( <5cm or 5%,
10cm or 10%).

Method
Ours (w/o motion)
Ours (w/ motion)

CD↓
2.66
2.57

SNC ↑
IoU ↑
74.98% 0.779
75.72% 0.812

L1 ↓
0.531
0.503

Table 4: Surface prediction error on the test set of De-
formingThings4D. The metrics are Volumetric IoU (IoU),
Chamfer Distance (CD) in centimeters, Surface Normal
Consistency (SNC), and (cid:96)1 score of SDF.

Figure 8: Shape completion on a large scene. The image
is from MPI Sintel [6] dataset. The maximum depth is set
to 10 meters in this sintel scene. With ﬁxed volume size
(2563), IF-Net [7] loses the ability to model details for large
scenes. Our fully-convolutional approach better captures all
levels of detail.

Methods
P2P (cm)↓
SNC ↑
time (s) ↓
memory (MB) ↓

IF-Nets Ours (w/o motion)
2.231
0.757
14.26
19,437

1.983
0.899
3.19
1,103

Ours
1.876
0.908
3.45
1,379

Table 5: Quantitative results on VolumeDeform examples.
The surface ground-truth is provided by VolumeDeform.
The metrics are the point-to-plane (P2P) distance, and sur-
face normal consistency (SNC). We also report the aver-
age time and memory required for the inference on a Tesla
V100-SXM2-32GB GPU.

Figure 7: Shape completion results on real-world RGB-
D images. The top 3 rows are images from VolumeDe-
form [26], and the last row is from Li et al. [30].

coordinate to an occupancy score using a multi-layer per-
ceptron. We train both methods on the humanoids dataset
and evaluate the completion performance on unseen se-
quences. Fig. 7 shows shape completion from real-world
RGB-D images. Our fully-convolutional approach shows
more complete, sharper results than the implicit IF-Net.
Tab. 5 shows quantitative evaluation on VolumeDeform [26]
sequences. In particular, for large scenes, our approach ef-
fectively captures both global and local structures, as shown
in Fig. 8.

Limitations Our approach maintains several limitations:
1) estimating the uncertainty of hidden motion is necessary
but not handled by our approach. A probabilistic approach
(e.g., Huang et al. [25]) would be promising for modeling
motion uncertainty. 2) our method does not predict surface
colors. The differentiable volumetric rendering approach
of [13] is a potential solution to learn colored deformable
objects. 3) DeformingThings4D largely contains articulated
objects such as humans and animals. We are planning to
expand the dataset with examples of loose clothing or plants
that are deformed by external forces.

6. Conclusion

In this work, we present the ﬁrst method that jointly
estimates the invisible shape and deformation from partial
depth frame observation. We show that shape completion
and motion estimation are mutually complementary tasks,
with joint learning beneﬁting each. Our newly proposed an-
imation dataset allows for cross-domain generalization for
both motion and shape. We believe that our method and new
dataset open a new research avenue on generic non-rigid 4D
reconstruction.

7. Acknowledgements

This work was conducted during Yang Li’s internship
at Tokyo Research Center, Huawei. Matthias Nießner is
supported by a TUM-IAS Rudolf M¨oßbauer Fellowship and
the ERC Starting Grant Scan2CAD (804724). We thank
Angela Dai for the voice-over of the video and also thank
for setting up the DeformingThings4D dataset.

References

[1] Brian Amberg, Sami Romdhani, and Thomas Vetter. Opti-
mal step nonrigid icp algorithms for surface registration. In
Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV), pages 1–8. IEEE, 2007. 2

[2] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-
bastian Thrun, Jim Rodgers, and James Davis. Scape: shape
completion and animation of people. In ACM SIGGRAPH
2005 Papers, pages 408–416. 2005. 3

[3] Federica Bogo,

Javier Romero, Matthew Loper, and
Michael J Black. Faust: Dataset and evaluation for 3d mesh
registration. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 3794–
3801, 2014. 3

[4] Aljaˇz Boˇziˇc, Pablo Palafox, Michael Zollh¨ofer, Angela Dai,
Justus Thies, and Matthias Nießner. Neural non-rigid track-
ing. arXiv preprint arXiv:2006.13240, 2020. 2

[5] Aljaˇz Boˇziˇc, Michael Zollh¨ofer, Christian Theobalt, and
Matthias Nießner. Deepdeform: Learning non-rigid rgb-d
In Proceedings
reconstruction with semi-supervised data.
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 7002–7012, 2020. 2, 3, 6, 8
[6] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and
Michael J Black. A naturalistic open source movie for optical
ﬂow evaluation. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 611–625, 2012. 3, 8
[7] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll.
Implicit functions in feature space for 3d shape reconstruc-
tion and completion. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
6970–6981, 2020. 2, 7, 8

[8] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d
spatio-temporal convnets: Minkowski convolutional neural
networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 3075–
3084, 2019. 4

[9] Brian Curless and Marc Levoy. A volumetric method for
building complex models from range images. In Proceedings
of the 23rd annual conference on Computer graphics and
interactive techniques, pages 303–312, 1996. 3, 5

[10] Angela Dai, Christian Diller, and Matthias Nießner. Sg-
nn: Sparse generative neural networks for self-supervised
In Proceedings of the
scene completion of rgb-d scans.
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 849–858, 2020. 2, 4

[11] Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed,
J¨urgen Sturm, and Matthias Nießner. Scancomplete: Large-
scale scene completion and semantic segmentation for 3d
scans. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 4578–4587,
2018. 2

[12] Angela Dai, Charles Ruizhongtai Qi, and Matthias Nießner.
Shape completion using 3d-encoder-predictor cnns and
In Proceedings of the IEEE Conference
shape synthesis.
on Computer Vision and Pattern Recognition (CVPR), pages
5868–5877, 2017. 2

[13] Angela Dai, Yawar Siddiqui, Justus Thies, Julien Valentin,
and Matthias Nießner. Spsg: Self-supervised photomet-
arXiv preprint
ric scene generation from rgb-d scans.
arXiv:2006.14660, 2020. 8

[14] Edilson De Aguiar, Carsten Stoll, Christian Theobalt,
Naveed Ahmed, Hans-Peter Seidel, and Sebastian Thrun.
Performance capture from sparse multi-view video. In ACM
SIGGRAPH 2008 papers, pages 1–10, 2008. 3

[15] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical ﬂow with convolutional networks. In Pro-
ceedings of the IEEE International Conference on Computer
Vision (ICCV), pages 2758–2766, 2015. 2

[16] Mingsong Dou, Philip Davidson, Sean Ryan Fanello, Sameh
Khamis, Adarsh Kowdle, Christoph Rhemann, Vladimir
Tankovich, and Shahram Izadi. Motion2fusion: Real-time
ACM Transactions on
volumetric performance capture.
Graphics (TOG), 36(6):1–16, 2017. 2

[17] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip
Davidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts
Escolano, Christoph Rhemann, David Kim, Jonathan Tay-
lor, et al. Fusion4d: Real-time performance capture of chal-
lenging scenes. ACM Transactions on Graphics (TOG),
35(4):114, 2016. 2

[18] Wei Gao and Russ Tedrake. Surfelwarp: Efﬁcient non-
arXiv

volumetric single view dynamic reconstruction.
preprint arXiv:1904.13073, 2019. 2

[19] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 3354–3361,
2012. 6

[20] Benjamin Graham, Martin Engelcke, and Laurens Van
Der Maaten. 3d semantic segmentation with submanifold
sparse convolutional networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 9224–9232, 2018. 4

[21] Benjamin Graham and Laurens van der Maaten.

manifold sparse convolutional networks.
arXiv:1706.01307, 2017. 4

Sub-
arXiv preprint

[22] Xiuye Gu, Yijie Wang, Chongruo Wu, Yong Jae Lee, and
Panqu Wang. Hplﬂownet: Hierarchical permutohedral lattice
ﬂownet for scene ﬂow estimation on large-scale point clouds.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 3254–3263, 2019. 2
[23] Kaiwen Guo, Feng Xu, Yangang Wang, Yebin Liu, and
Qionghai Dai. Robust non-rigid motion tracking and surface
reconstruction using l0 regularization. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV),
pages 3083–3091, 2015. 3

[24] Oshri Halimi, Ido Imanuel, Or Litany, Giovanni Trappolini,
Emanuele Rodol`a, Leonidas Guibas, and Ron Kimmel. The
whole is greater than the sum of its nonrigid parts. arXiv
preprint arXiv:2001.09650, 2020. 2

[25] Jiahui Huang, Shi-Sheng Huang, Haoxuan Song, and Shi-
Min Hu. Di-fusion: Online implicit 3d reconstruction with
deep priors. arXiv preprint arXiv:2012.05551, 2020. 8
[26] Matthias Innmann, Michael Zollh¨ofer, Matthias Nießner,
Christian Theobalt, and Marc Stamminger. Volumedeform:
In Pro-
Real-time volumetric non-rigid reconstruction.
ceedings of the European Conference on Computer Vision
(ECCV), pages 362–379, 2016. 1, 2, 3, 6, 8

[27] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei
Huang, Matthias Nießner, and Thomas Funkhouser. Local
implicit grid representations for 3d scenes. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 6001–6010, 2020. 2

[28] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.
the
Poisson surface reconstruction.
fourth Eurographics symposium on Geometry processing,
volume 7, 2006. 2

In Proceedings of

[29] Hao Li, Robert W Sumner, and Mark Pauly. Global cor-
respondence optimization for non-rigid registration of depth
scans. In Computer graphics forum, volume 27, pages 1421–
1430. Wiley Online Library, 2008. 2

[30] Yang Li, Aljaz Bozic, Tianwei Zhang, Yanli Ji, Tatsuya
Harada, and Matthias Nießner. Learning to optimize non-
In Proceedings of the IEEE Conference on
rigid tracking.
Computer Vision and Pattern Recognition (CVPR), pages
4910–4918, 2020. 2, 8

[31] Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, and Yebin Liu.
In Proceedings of the
Robust 3d self-portraits in seconds.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1344–1353, 2020. 2

[32] Xingyu Liu, Charles R Qi, and Leonidas J Guibas.
Flownet3d: Learning scene ﬂow in 3d point clouds. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 529–537, 2019. 2, 3, 6,
7

[33] Xingyu Liu, Mengyuan Yan, and Jeannette Bohg. Meteor-
net: Deep learning on dynamic 3d point cloud sequences. In
Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV), pages 9246–9255, 2019. 2

[34] Zhaoyang Lv, Kihwan Kim, Alejandro Troccoli, Deqing
Sun, James M Rehg, and Jan Kautz. Learning rigidity in

dynamic scenes with a moving camera for 3d motion ﬁeld
estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 468–484, 2018. 2, 3
[35] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-
ard Pons-Moll, and Michael J. Black. Amass: Archive of
motion capture as surface shapes. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), Oct
2019. 3

[36] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical ﬂow, and scene ﬂow estimation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 4040–4048, 2016. 3, 6

[37] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
In Proceed-
Learning 3d reconstruction in function space.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 4460–4470, 2019. 2

[38] Richard A Newcombe, Dieter Fox, and Steven M Seitz.
Dynamicfusion: Reconstruction and tracking of non-rigid
scenes in real-time. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
343–352, 2015. 1, 2, 6

[39] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and
Andreas Geiger. Occupancy ﬂow: 4d reconstruction by
learning particle dynamics. In Proceedings of the IEEE In-
ternational Conference on Computer Vision (ICCV), pages
5379–5389, 2019. 2

[40] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 165–174, 2019. 2

[41] Mark Pauly, Niloy J Mitra, Joachim Giesen, Markus H
Gross, and Leonidas J Guibas. Example-based 3d scan com-
pletion. In Symposium on Geometry Processing, pages 23–
32, 2005. 2

[42] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc
Pollefeys, and Andreas Geiger. Convolutional occupancy
networks. arXiv preprint arXiv:2003.04618, 2020. 2
[43] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In Advances in neural informa-
tion processing systems, pages 5099–5108, 2017. 3

[44] Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun.
In Proceedings of the IEEE In-
Playing for benchmarks.
ternational Conference on Computer Vision (ICCV), pages
2213–2222, 2017. 3

[45] Miroslava Slavcheva, Maximilian Baust, Daniel Cremers,
and Slobodan Ilic. Killingfusion: Non-rigid 3d reconstruc-
tion without correspondences. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 1386–1395, 2017. 2, 3

[46] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene com-
In Proceedings of the
pletion from a single depth image.

[60] Silvia Zufﬁ, Angjoo Kanazawa, David W Jacobs, and
Michael J Black. 3d menagerie: Modeling the 3d shape and
In Proceedings of the IEEE Conference
pose of animals.
on Computer Vision and Pattern Recognition (CVPR), pages
6365–6373, 2017. 3

IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1746–1754, 2017. 2

[47] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface
modeling. In Symposium on Geometry Processing, volume 4,
pages 109–116, 2007. 1, 2, 6, 7

[48] Gilbert W Stewart. The efﬁcient generation of random or-
thogonal matrices with an application to condition estima-
tors. SIAM Journal on Numerical Analysis, 17(3):403–409,
1980. 4

[49] Robert W Sumner, Johannes Schmid, and Mark Pauly. Em-
bedded deformation for shape manipulation. ACM Transac-
tions on Graphics (TOG), 26(3):80, 2007. 1, 2

[50] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
Pwc-net: Cnns for optical ﬂow using pyramid, warping, and
In Proceedings of the IEEE Conference on
cost volume.
Computer Vision and Pattern Recognition (CVPR), pages
8934–8943, 2018. 2

[51] Zachary Teed and Jia Deng.

pairs ﬁeld transforms for optical ﬂow.
arXiv:2003.12039, 2020. 2

Raft: Recurrent all-
arXiv preprint

[52] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan
Popovi´c. Articulated mesh animation from multi-view sil-
In ACM SIGGRAPH 2008 papers, pages 1–9,
houettes.
2008. 3

[53] Shenlong Wang, Sean Ryan Fanello, Christoph Rhemann,
Shahram Izadi, and Pushmeet Kohli. The global patch col-
In Proceedings of the IEEE Conference on Com-
lider.
puter Vision and Pattern Recognition (CVPR), pages 127–
135, 2016. 2

[54] Zirui Wang, Shuda Li, Henry Howard-Jenkins, Victor
Prisacariu, and Min Chen. Flownet3d++: Geometric losses
In The IEEE Winter Con-
for deep scene ﬂow estimation.
ference on Applications of Computer Vision, pages 91–98,
2020. 2

[55] Wenxuan Wu, Zhiyuan Wang, Zhuwen Li, Wei Liu, and Li
Fuxin. Pointpwc-net: A coarse-to-ﬁne network for super-
vised and self-supervised scene ﬂow estimation on 3d point
clouds. arXiv preprint arXiv:1911.12408, 2019. 2

[56] Genzhi Ye, Yebin Liu, Nils Hasler, Xiangyang Ji, Qiong-
hai Dai, and Christian Theobalt. Performance capture of in-
teracting characters with handheld kinects. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 828–841, 2012. 3

[57] Bo Zheng, Yibiao Zhao, Joey C Yu, Katsushi Ikeuchi, and
Song-Chun Zhu. Beyond point clouds: Scene understanding
by reasoning geometry and physics. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 3127–3134, 2013. 2

[58] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and
Yebin Liu. Deephuman: 3d human reconstruction from a
single image. In Proceedings of the IEEE International Con-
ference on Computer Vision (ICCV), October 2019. 3
[59] Michael Zollh¨ofer, Matthias Nießner, Shahram Izadi,
Christoph Rehmann, Christopher Zach, Matthew Fisher,
Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian
Theobalt, et al. Real-time non-rigid reconstruction using
an rgb-d camera. ACM Transactions on Graphics (ToG),
33(4):156, 2014. 2, 6

