1
2
0
2

t
c
O
0
3

]
E
M

.
t
a
t
s
[

3
v
1
2
3
2
1
.
1
1
9
1
:
v
i
X
r
a

LSAR: Eﬃcient Leverage Score Sampling Algorithm for
the Analysis of Big Time Series Data

Ali Eshragh∗

Fred Roosta†

Asef Nazari‡ Michael W. Mahoney§

November 2, 2021

Abstract

We apply methods from randomized numerical linear algebra (RandNLA) to de-
velop improved algorithms for the analysis of large-scale time series data. We ﬁrst
develop a new fast algorithm to estimate the leverage scores of an autoregressive (AR)
model in big data regimes. We show that the accuracy of approximations lies within
(1 + O (ε)) of the true leverage scores with high probability. These theoretical results
are subsequently exploited to develop an eﬃcient algorithm, called LSAR, for ﬁtting an
appropriate AR model to big time series data. Our proposed algorithm is guaranteed,
with high probability, to ﬁnd the maximum likelihood estimates of the parameters of
the underlying true AR model and has a worst case running time that signiﬁcantly im-
proves those of the state-of-the-art alternatives in big data regimes. Empirical results
on large-scale synthetic as well as real data highly support the theoretical results and
reveal the eﬃcacy of this new approach.

1

Introduction

A time series is a collection of random variables indexed according to the order in which they
are observed in time. The main objective of time series analysis is to develop a statistical
model to forecast the future behavior of the system. At a high level, the main approaches
for this include the ones based on considering the data in its original time domain and
those arising from analyzing the data in the corresponding frequency domain [31, Chapter
1]). More speciﬁcally, the former approach focuses on modeling some future value of a time
series as a parametric function of the current and past values by studying the correlation

∗School of Information and Physical Sciences, University of Newcastle, Australia, and International Com-

puter Science Institute, Berkeley, CA, USA. Email: ali.eshragh@newcastle.edu.au

†School of Mathematics and Physics, University of Queensland, Australia, and International Computer

Science Institute, Berkeley, CA, USA. Email: fred.roosta@uq.edu.au

‡School of Information Technology, Deakin University, Australia. Email: asef.nazari@deakin.edu.au
§Department of Statistics, University of California at Berkeley, USA, and International Computer Science

Institute, Berkeley, CA, USA. Email: mmahoney@stat.berkeley.edu

1

 
 
 
 
 
 
between adjacent points in time. The latter framework, however, assumes the primary
characteristics of interest in time series analysis relate to periodic or systematic sinusoidal
variations. Although the two approaches may produce similar outcomes for many cases, the
comparative performance is better done in the “time domain” [31, Chapter 1] which is the
main focus of this paper.

Box and Jenkins [6] introduced their celebrated autoregressive moving average (ARMA)
model for analyzing stationary time series. Although it has been more than 40 years since
this model was developed, due to its simplicity and vast practicality, it continues to be
widely used in theory and practice. A special case of an ARMA model is an autoregressive
(AR) model, which merely includes the autoregressive component. Despite their simplicity,
AR models have a wide range of applications spanning from genetics and medical sciences to
ﬁnance and engineering (e.g., [1, 2, 7, 11, 15, 23, 29]).

The main hyper-parameter of an AR model is its order, which directly relates to the
dimension of the underlying predictor variable. In other words, the order of an AR model
amounts to the number of lagged values that are included in the model. In problems involving
big time series data, selecting an appropriate order for an AR model amounts to computing
the solutions of many potentially large scale ordinary least squares (OLS) problems, which
can be the main bottleneck of computations (cf. Section 2.1). Here is where randomized
sub-sampling algorithms can be used to greatly speed-up such model selection procedures.
For computations involving large matrices in general, and large-scale OLS problems in
particular, randomized numerical linear algebra (RandNLA) has successfully employed var-
ious random sub-sampling and sketching strategies. There, the underlying data matrix is
randomly, yet appropriately, “compressed” into a smaller one, while approximately retain-
ing many of its original properties. As a result, much of the expensive computations can
be performed on the smaller matrix; Mahoney [19] and Woodruﬀ [34] provided an extensive
overview of RandNLA subroutines and their many applications. Moreover, implementa-
tions of algorithms based on those ideas have been shown to beat state-of-the-art numerical
routines (e.g., [4, 22, 36]).

Despite their simplicity and eﬃcient constructions, matrix approximations using uniform
sampling strategies are highly ineﬀective in the presence of non-uniformity in the data (e.g.,
outliers). In such situations, non-uniform (but still i.i.d.) sampling schemes in general, and
leverage score sampling in particular [9], are instrumental not only in obtaining the strongest
worst case theoretical guarantees, but also in devising high-quality numerical implementa-
tions. In times series data, one might expect that sampling methods based on leverage scores
can be highly eﬀective (cf. Figure 8). However, the main challenge lies in computing the
leverage scores, which na¨ıvely can be as costly as the solution of the original OLS problems.
In this light, exploiting the structure of the time series model for estimating the leverage
scores can be the determining factor in obtaining eﬃcient algorithms for time series analysis.
We carry out that here in the context of AR models. In particular, our contributions can be
summarized as follows:
(i) We introduce RandNLA techniques to the analysis of big time series data.
(ii) By exploiting the available structure, we propose an algorithm for approximating the

2

leverage scores of the underlying data matrix that is shown to be faster than the state-of-
the-art alternatives.
(iii) We theoretically obtain a high-probability relative error bound on the leverage score
approximations.
(iv) Using these approximations, we then develop a highly-eﬃcient algorithm, called LSAR,
for ﬁtting AR models with provable guarantees.
(v) We empirically demonstrate the eﬀectiveness of the LSAR algorithm on several large-scale
synthetic as well as real big time series data.

The structure of this paper is as follows: Section 2 introduces AR models and RandNLA
techniques in approximately solving large-scale OLS problems. Section 3 deals with the
theoretical results on developing an eﬃcient leverage score sampling algorithm to ﬁt and
estimate the parameters of an AR model. All proofs are presented in Appendix A. Section 4
illustrates the eﬃcacy of the new approach by implementing it on several large-scale synthetic
as well as real big time series data. Section 5 concludes the paper and addresses future work.

Notation

Throughout the paper, vectors and matrices are denoted by bold lower-case and bold upper-
case letters, respectively (e.g., v and V ). All vectors are assume to be column vectors. We
use regular lower-case to denote scalar constants (e.g., d). Random variables are denoted
(cid:124)
by regular upper-case letters (e.g., Y ). For a real vector, v, its transpose is denoted by v
.
w. For a vector v and a
For two vectors v, w, their inner-product is denoted as (cid:104)v, w(cid:105) = v
matrix V , (cid:107)v(cid:107) and (cid:107)V (cid:107) denote vector (cid:96)2 norm and matrix spectral norm, respectively. The
condition number of a matrix A, which is the ratio of its largest and smallest singular values,
is denoted by κ(A). Range of a matrix A ∈ Rn×d, denoted by Range(A), is a sub-space of
Rn, consisting all the vectors (cid:8)Ax | x ∈ Rd(cid:9). Adopting Matlab notation, we use A(i, :) to
refer to the ith row of the matrix A and consider it as a column vector. Finally, ei denotes
a vector whose ith component is one, and zero elsewhere.

(cid:124)

2 Background

In this section, we present a brief overview of the two main ingredients of the results of
this paper, namely autoregressive models (Section 2.1) and leverage score sampling for OLS
problems (Section 2.2).

2.1 Autoregressive Models
A time series {Yt; t = 0, ±1, ±2, . . .} is called (weakly) stationary, if the mean E[Yt] is
independent of time t, and the auto-covariance Cov(Yt, Yt+h) depends only on the lag h
for any integer values t and h. A stationary time series {Yt; t = 0, ±1, ±2, . . .}1 with the

1Throughout this paper, we assume that Yt’s are continuous random variables.

3

constant mean E[Yt] = 0 is an AR model with the order p, denoted by AR(p), if we have

Yt = φ1Yt−1 + · · · + φpYt−p + Wt,

(1)

where φp (cid:54)= 0 and the time series {Wt; t = 0, ±1, ±2, . . .} is a Gaussian white noise with
the mean E[Wt] = 0 and variance V ar(Wt) = σ2
W . Recall that a Gaussian white noise is a
stationary time series in which each individual random variable Wt has a normal distribution
and any pair of random variables Wt1 and Wt2 for distinct values of t1, t2 ∈ Z are uncorrelated.

Remark 1. For the sake of simplicity, we assume that E[Yt] = 0. Otherwise, if E[Yt] =
µ (cid:54)= 0, then one can replace Yt with Yt − µ to obtain

Yt − µ = φ1(Yt−1 − µ) + · · · + φp(Yt−p − µ) + Wt,

which is simpliﬁed to

Yt = µ(1 − φ1 · · · − φp) + φ1Yt−1 + · · · + φpYt−p + Wt.

It is readily seen that each AR(p) model has p + 2 unknown parameters consisting of the
W . Here, we brieﬂy explain the

order p, the coeﬃcients φi and the variance of white noises σ2
common methods in the literature to estimate each of these unknown parameters.

Estimating the order p. A common method to estimate the order of an AR(p) model is to
use the partial autocorrelation function (PACF) [31, Chapter 3]. The PACF of a stationary
time series {Yt; t = 0, ±1, ±2, . . .} at lag h is deﬁned by

PACFh :=






ρ(Yt, Yt+1)

for h = 1,

ρ(Yt+h − (cid:98)Yt+h,−h, Yt − (cid:98)Yt,h)

for h ≥ 2,

(2)

where ρ denotes the correlation function, and where (cid:98)Yt,h and (cid:98)Yt+h,−h denote the linear regres-
sion, in the population sense, of Yt and Yt+h on {Yt+1, . . . , Yt+h−1}, respectively. It can be
shown that for a causal AR(p) model, while the theoretical PACF (2) at lags h = 1, . . . , p − 1
may be non-zero and at lag h = p may be strictly non-zero, at lag h = p + 1 it drops to zero
and then remains at zero henceforth [31, Chapter 3]. Recall that an AR(p) model is said to
be causal if the time series {Yt; t = 0, ±1, ±2, . . .} can be written as Yt = Wt + (cid:80)∞
i=1 ψiWt−i
with constant coeﬃcients ψi such that (cid:80)∞
i=1 |ψi| < ∞. Furthermore, if a sample of size n is
obtained from a causal AR(p) model, then under some mild conditions, an estimated sample
n, has a standard normal distribution, in limit as n tends
PACF at lags h > p, scaled by
to inﬁnity [5, Chapter 8].

√

Thus, in practice, the sample PACF versus lag h along with a 95% zero-conﬁdence bound-
n, are plotted. Then, the largest lag h in which
ary, that is two horizontal lines at ±1.96/
the sample PACF lies out of the zero-conﬁdence boundary for PACF is used as an estimation

√

4

of the order p. For instance, Figures 4a, 4d and 4g display the sample PACF plots for the
synthetic time series data generated from models AR(20), AR(100), and AR(200), respectively.
Each ﬁgure illustrates that the largest PACF lying out of the red dashed 95% zero-conﬁdence
boundary, locates at a lag which is equal to the order of the AR model.

Maximum likelihood estimation of the coeﬃcients φi and variance σ2
W . Let
y1, . . . , yn be a time series realization of an AR(p) model where p is known and n (cid:29) p.
Unlike a linear regression model, the log-likelihood function

log(fY1,...,Yn(y1, . . . , yn; φ1, . . . , φp, σ2

W )),

where f is the joint probability distribution function of the random variables Y1, . . . , Yn, is
a complicated non-linear function of the unknown parameters. Hence, ﬁnding an analytical
form of the maximum likelihood estimates (MLEs) is intractable. Consequently, one typically
uses some numerical optimization methods to ﬁnd an MLE of the parameters of an AR(p)
model approximately. However, it can be shown that the conditional log-likelihood function
is analogous to the log-likelihood function of a linear regression model given below [16,
Chapter 5]:

log(fYp+1,...,Yn|Y1,...,Yp(yp+1, . . . , yn|y1, . . . , yp; φ1, . . . , φp, σ2
n − p
2

n − p
2

log(2π) −

log(σ2

W ) −

n
(cid:88)

= −

W ))

(yt − φ1yt−1 − · · · − φpyt−p)2
2σ2
W

.

t=p+1

Thus, the conditional MLE (CMLE) of the coeﬃcients φi as well as the variance σ2
be estimated from an OLS regression of yt on p of its own lagged values. More precisely,

W can

where φn,p is the CMLE of the coeﬃcient vector [φ1, . . . , φp]

(cid:124)

, the data matrix

φn,p := (X

(cid:124)
n,pXn,p)−1X

(cid:124)
n,pyn,p,

Xn,p :=

and yn,p := (cid:2)yp+1 yp+2

. . . yn

(cid:3)(cid:124)

.








yp
yp−1
yp+1
yp
...
...
yn−1 yn−2








,

· · ·
y1
· · ·
y2
...
. . .
· · · yn−p

(3)

(4)

Remark 2. The data matrix Xn,p in (4) possesses Toeplitz structure that we take
advantage of for our derivations in this paper, in particular developing the recursion for
the leverage scores given in Theorem 1. Also, it is highlighted that as the estimated
parameter vector (3) is operating under “conditional” MLE, the data matrix Xn,p is a
ﬁxed design matrix.

5

Moreover, the CMLE of σ2

W , the so-called MSE, is given by

(cid:98)σ2
W =

(cid:107)rn,p(cid:107)2
n − p

,

where

and

rn,p := yn,p − Xn,pφn,p = (cid:2)rn,p(1)

. . . rn,p(n − p)(cid:3)(cid:124)

(5)

(6)

rn,p(i) = yp+i − X

(cid:124)
n,p(i, :)φn,p for i = 1, . . . , n − p.

Recall that Xn,p(i, :) is the ith row of matrix Xn,p, that is,

Xn,p(i, :) := (cid:2)yi+p−1 yi+p−2

. . . yi

(cid:3)(cid:124)

.

One may criticize the CMLE as it requires one to exclude the ﬁrst p observations to
construct the conditional log-likelihood function. Although this is a valid statement, due to
the assumption n (cid:29) p, dropping the ﬁrst p observation from the whole time series realization
could be negligible.

Remark 3. It can be shown [31, Chapter 3] that if

(cid:98)Yt+h,−h = α1Yt+h−1 + · · · + αh−1Yt+1,

then

(cid:98)Yt,h = α1Yt+1 + · · · + αh−1Yt+h−1.

This implies that ﬁnding PACF at each lag requires the solution to only one corre-
sponding OLS problem. Furthermore, one can see that an empirical estimation of the
coeﬃcients αi is the same as ﬁnding a CMLE of the coeﬃcients of an AR(h − 1) model
ﬁtted to the data. Thus, empirically estimating the order p using a given time series
data involves repeated solutions of OLS problems, which can be computationally pro-
hibitive in large-scale settings. Indeed, for n realizations y1, . . . , yn, PACF at lag h can
be calculated in O (nh) using Toeplitz properties of the underlying matrix, and as a
result selecting an appropriate order parameter p amounts to O ((cid:80)p
h=1 nh) = O (np2)
time complexity.

Remark 4. It should be noted that there is another method to estimate the parame-
ters of an AR(p) model by solving the Yule-Walker equations with the Durbin-Levinson
algorithm [5, Chapter 8]. Although, those estimates have asymptotic properties similar

6

to CMLEs, solving the corresponding OLS problem is computationally faster than the
Durbin-Levinson algorithm and also the CMLEs are statistically more eﬃcient.

2.2 Leverage Scores and RandNLA

Linear algebra, which is the mathematics of linear mappings between vector spaces, has
long had a large footprint in statistical data analysis. For example, canonical linear algebra
problems such as principal component analysis and OLS are arguably among the ﬁrst and
most widely used techniques by statisticians. In the presence of large amounts data, however,
such linear algebra routines, despite their simplicity of formulation, can pose signiﬁcant
computational challenges. For example, consider an over-determined OLS problem

(cid:107)Ax − b(cid:107)2 ,

min
x

(7)

involving m × d matrix A, where m > d. Note that, instead of n − p and p for the dimensions
of the matrix (4), we adopt the notation m and d for the number of rows and columns, respec-
tively. This is due to the fact that our discussion in this section involves arbitrary matrices
and not those speciﬁcally derived from AR models. Solving (7) amounts to O (md2 + d3/3)
ﬂops by forming the normal equations, O (md2 − d3) ﬂops via QR factorization with House-
holder reﬂections, and O (md2 + d3) ﬂops using singular value decomposition (SVD) [14].
Iterative solvers such as LSQR [25], LSMR [13], and LSLQ [12], involve matrix-vector prod-
ucts at each iterations, which amount to O (mdc) ﬂops after c iterations. In other words, in
“big-data” regimes where md2 (cid:29) 1, na¨ıvely performing these algorithms can be costly.

RandNLA subroutines involve the construction of an appropriate sampling/sketching
matrix, S ∈ Rs×m for d ≤ s (cid:28) m, and compressing the data matrix into a smaller version
SA ∈ Rs×d. In the context of (7), using the smaller matrix, the above-mentioned classical
OLS algorithms can be readily applied to the smaller scale problem

(cid:107)SAx − Sb(cid:107)2 ,

min
x

(8)

at much lower costs.
In these algorithms, sampling/sketching is used to obtain a data-
oblivious or data-aware subspace embedding, which ensures that for any 0 < ε, δ < 1 and
for large enough s, we get

Pr (cid:0)(cid:107)Ax(cid:63) − b(cid:107)2 ≤ (cid:107)Ax(cid:63)

s − b(cid:107)2 ≤ (1 + O (ε)) (cid:107)Ax(cid:63) − b(cid:107)2(cid:1) ≥ 1 − δ,

(9)

where x(cid:63) and x(cid:63)
s are the solutions to (7) and (8), respectively. In other words, the solution
to the reduced problem (8) is a 1 + O (ε) approximation of the solution to the original
problem (7).

Arguably, the simplest data-oblivious way to construct the matrix S is using uniform
sampling, where each row of S is chosen uniformly at random (with or without replacement)
from the rows of the m × m identity matrix. Despite the fact that the construction and
application of such a matrix can be done in constant O (1) time, in the presence of non-
uniformity among the rows of A, such uniform sampling strategies perform very poorly. In

7

such cases, it can be shown that one indeed requires s ∈ O (m) samples to obtain the above
sub-space embedding property.

To alleviate this signiﬁcant shortcoming, data-oblivious sketching schemes involve ran-
domly transforming the data so as to smooth out the non-uniformities, which in turn allows
for subsequent uniform sampling in the randomly rotated space [10]. Here, the random pro-
jection acts as a preconditioner (for the class of random sampling algorithms), which makes
the preconditioned data better behaved (in the sense that simple uniform sampling methods
can be used successfully) (e.g., [19, 20]). With such sketching schemes, depending on the
random projection matrix, diﬀerent sample sizes are required, for instance, O (d log(1/δ)/ε2)
samples for Gaussian projection, O (d log(d/δ)/ε2) samples for fast Hadamard-based trans-
forms, and O (d2poly(log(d/δ))/ε2) samples using sparse embedding matrices. Woodruﬀ [34]
provided a comprehensive overview of such methods and their extensions.

Alternative to data-oblivious random embedding methods are data-aware sampling tech-
niques, which by taking into account the information contained in the data, sample the
rows of the matrix proportional to non-uniform distributions. Among many such strategies,
those schemes based on statistical leverage scores [9] have not only shown to improve worst
case theoretical guarantees of matrix algorithms, but also they are amenable to high-quality
numerical implementations [19]. Roughly speaking, the “best” random sampling algorithms
base their importance sampling distribution on these scores and the “best” random projec-
tion algorithms transform the data to be represented in a rotated basis where these scores
are approximately uniform.

The concept of statistical leverage score has long been used in statistical regression di-
agnostics to identify outliers [28]. Given a data matrix A ∈ Rm×d with m ≥ d, consider any
orthogonal matrix Q such that Range(Q) = Range(A). The ith leverage score corresponding
to ith row of A is deﬁned as

(cid:96)(i) := (cid:107)Q(i, :)(cid:107)2 .

It can be easily shown that this is well-deﬁned in that the leverage score does not depend
on the particular choice of the basis matrix Q. Furthermore, the ith leverage score boils
down to the ith diagonal entry of the hat matrix, that is,

where

It is also easy to see that

Thus,

(cid:96)(i) = e

(cid:124)
i Hei

for i = 1, . . . , m,

H := A (cid:0)A

(cid:124)

A(cid:1)−1 A

(cid:124)

.

(cid:96)(i) ≥ 0 ∀ i,

and

m
(cid:88)

i=1

(cid:96)(i) = d.

(10a)

(10b)

deﬁnes a non-uniform probability distribution over the rows of A.

π(i) :=

(cid:96)(i)
d

,

for i = 1, . . . , m,

(10c)

8

Leverage score sampling matrix S. Sampling according to the leverage scores amounts
to randomly picking and re-scaling rows of A proportional to their leverage scores and
appropriately re-scaling the sampled rows so as to maintain an unbiased estimator of A
A,
that is,

(cid:124)

E[(cid:107)SAx(cid:107)2] = (cid:107)Ax(cid:107)2 , ∀x.

More precisely, each row of the s×m sampling matrix S is chosen randomly from the rows of
the m × m identity matrix according to the probability distribution (10c), with replacement.
Furthermore, if the ith row is selected, it is re-scaled with the multiplicative factor

1
√
sπi

,

(11)

implying that 1/

√

(cid:124)
i is appended to S.
sπie

Clearly, obtaining any orthogonal matrix Q as above by using SVD or QR factorization
is almost as costly as solving the original OLS problem (i.e., O (md2) ﬂops), which defeats
the purpose of sampling altogether. In this light, Drineas et al. [9] proposed randomized
approximation algorithms, which eﬃciently estimate the leverage scores in O (md log m + d3)
ﬂops. For sparse matrices, this was further improved by Clarkson and Woodruﬀ [8], Meng
and Mahoney [21], and Nelson and Nguyen [24] to O (nnz(A) log m + d3). In particular, it
has been shown that with the leverage score estimates ˆ(cid:96)(i) such that

ˆ(cid:96)(i) ≥ β(cid:96)(i),

for i = 1, 2, . . . m,

for some misestimation factor 0 < β ≤ 1, one can obtain (9) with

s ∈ O (cid:0)d log(d/δ)/(βε2)(cid:1) ,

(12)

(13)

samples [34]. As it can be seen from (13), the required sample size s is adversely aﬀected by
the leverage score misestimation factor β.

Recently, randomized sublinear time algorithms for estimating the parameters of an AR
model for a given order d have been developed by Shi and Woodruﬀ [30]. There, by using the
notion of generalized leverage scroes, the authors propose a method for approximating CMLE
of the parameters in O(m log2 m+(d2 log2 m)/ε2 +(d3 log m)/ε2) time, with high probability.
The analysis in [30] makes use of Toeplitz structure of data matrices arising from AR models.
Also related to our settings here are [32] and [35], which developed, respectively, an exact and
a (numerically stable) randomized approximation algorithm to solve Toeplitz least square
problems, both with the time complexity of O (cid:0)(m + d) log2(m + d)(cid:1). An alternative sub-
sampling algorithm to algorithmic leveraging for OLS problems has been considered by Wang
et al. [33]. There, the sub-sampling is approached from the perspective of optimal design
using D-optimality criterion, aiming to maximize the determinant of the Fisher information
in the sub-sample. We also note that algorithms various statistical aspects of leverage scores
have been extensively studied by Raskutti and Mahoney [26] and Ma et al. [18]. Finally, a
more general notion of leverage scores in the context of recovery of continuous time signals
from discrete measurements has recently been introduced by Avron et al. [3].

9

2.3 Theoretical Contributions

Here, by taking the advantage of the structure of AR models, we derive an algorithm, called
LSAR, which given the (approximate) leverage scores of the data matrix for an AR(p − 1)
model (cf. (4)), eﬃciently provides an estimate for the leverage scores related to an AR(p)
model. In the process, we derive explicit bounds on the misestimation factor β in (12). An
informal statement of our main results (Theorems 3 to 5 and 6) are as follows.

√

Claim (Informal). For any ε > 0 small enough, we prove (with a constant probability of
success):
• Theorem 3: If only some suitable approximations of the leverage scores of an AR(p − 1)
model are known, we can estimate those of an AR(p) model with a misestimation factor
ε) in O(n + p3 log p) time complexity. This should be compared with na¨ıve
β ∈ 1 − O(p
QR-based methods with O (np2) and the universal approximation schemes developed by
Drineas et al. [9] with O (np log n + p3).
• Theorems 4 and 5: Furthermore, an appropriate AR(p) model can be ﬁtted, with high-
probability, in overall time complexity of O(np + (p4 log p)/ε2) as compared with O(np2)
using exact methods (cf. Remark 3), O((n+p)p log2(n+p)) by leveraging structured matrices
as in [32], and O(np log2 n + (p3 log2 n)/ε2 + (p4 log n)/ε2) from sublinear time algorithms
developed by Shi and Woodruﬀ [30].

Remark 5. In big data regimes where typically n (cid:29) p the above result implies an
improvement over the existing methods for ﬁtting an appropriate AR model. However,
ε) on p is
we believe that the dependence of the misestimation factor β ∈ 1 − O(p
superﬂuously a by-product of our analysis, as in our numerical experiments, we show
that a sensible factor may be in the order of β ∈ 1 − O(log p

ε).

√

√

3 Theoretical Results

In this section, we use the speciﬁc structure of the data matrix induced by an AR model to
develop a fast algorithm to approximate the leverage scores corresponding to the rows of
the data matrix (4). Furthermore, we theoretically show that our approximations possess
relative error (cf. (21)) bounds with high probability. Motivated from the leverage score based
sampling strategy in Section 2.2, we then construct a highly eﬃcient algorithm, namely LSAR,
to ﬁt an appropriate AR(p) model on big time series data. It should be noted that all proofs
of this section are presented in Appendix A.

3.1 Leverage Score Approximation for AR Models

We ﬁrst introduce Deﬁnition 1 which relates and uniﬁes notation of Sections 2.1 and 2.2
together.

10

Deﬁnition 1. In what follows, we deﬁne (cid:96)n,p, Hn,p, and πn,p as

(cid:124)
(cid:96)n,p(i) := e
i Hn,pei,
(cid:0)X
(cid:124)
n,pXn,p

Hn,p := Xn,p

for i = 1, . . . , n − p,
(cid:124)
n,p,

(cid:1)−1 X

πn,p(i) :=

(cid:96)n,p(i)
p

,

for i = 1, . . . , n − p.

That is, they refer, respectively, to (10a),(10b), and (10c), using A = Xn,p as deﬁned
in (4).

We show that the leverage scores associated with an AR(p) model can be recursively
described using those arising from an AR(p − 1) model. This recursive pattern is a direct
result of the special structure of the data matrix (4), which amounts to a rectangular Hankel
matrix [14].

Theorem 1 (Exact Leverage Score Computations). The leverage scores of an AR(1)
model are given by

(cid:96)n,1(i) =

y2
i
n−1
(cid:88)

y2
t

t=1

,

for i = 1, . . . , n − 1.

(14a)

For an AR(p) model with p ≥ 2, the leverage scores are obtained by the following recursion

(cid:96)n,p(i) = (cid:96)n−1,p−1(i) +

(rn−1,p−1(i))2
(cid:107)rn−1,p−1(cid:107)2 ,

for i = 1, . . . , n − p,

(14b)

where the residual vector rn−1,p−1 is deﬁned in (6).

Theorem 1 shows that the leverage scores of (4) can be exactly calculated through the re-
cursive (14b) on the parameter p with the initial condition (14a). This recursion incorporates
the leverage cores of the data matrix Xn−1,p−1 along with the residual terms of ﬁtting an
AR(p − 1) model to the time series data y1, . . . , yn−1. Note that both matrices Xn−1,p−1 and
Xn,p have equal number of rows, and accordingly equal number of leverage scores. Moreover,
since we are dealing with big time series data (i.e., n (cid:29) p), excluding one observation in
practice is indeed negligible.

Theorem 1, though enticing at ﬁrst glance, suﬀers from two major drawbacks in that
not only does it require exact leverage scores associated with AR(p − 1) models, but it also
involves exact residuals from the corresponding OLS estimations.
In the presence of big
data, computing either of these factors exactly defeats the whole purpose of data sampling

11

altogether. To alleviate these two issues, we ﬁrst focus on approximations in computing the
latter, and then incorporate the estimations of the former. In doing so, we obtain leverage
score approximations, which enjoy desirable a priori relative error bounds.

A natural way to approximate the residuals in the preceding AR(p − 1) model (i.e.,
rn−1,p−1), is by means of sampling the data matrix Xn−1,p−1 and solving the correspond-
ing reduced OLS problem. More speciﬁcally, we consider the sampled data matrix

˜Xn,p := SXn,p,

(15a)

where S ∈ Rs×(n−p) is the sampling matrix whose s rows are chosen at random with replace-
ment from the rows of the (n − p) × (n − p) identity matrix according to the distribution
i=1 (cf. Deﬁnition 1) and rescaled by the appropriate factor (11). Using ˜Xn,p, the
{πn,p(i)}n−p
estimated parameter vector ˜φn,p is calculated as

˜φn,p := ( ˜X

(cid:124)
n,p

˜Xn,p)−1 ˜X

(cid:124)
n,p ˜yn,p,

where ˜yn,p := Syn,p. Finally, the residuals of ˜φn,p, analogous to (6), are given by

˜rn,p := yn,p − Xn,p

˜φn,p.

(15b)

(15c)

Remark 6. We note that the residual vector ˜rn,p is computed using the sampled data
matrix ˜Xn,p, which is itself formed according to the leverage scores. In other words, the
availability ˜rn,p is equivalent to that of {πn,p(i)}n−p
i=1 .

The following theorem, derived from the structural result [10], gives estimates on the

approximations (15b) and (15c).

Theorem 2 ( [10, Theorem 1]). Consider an AR(p) model and let 0 < ε, δ < 1. For
sampling with (approximate) leverage scores using a sample size s as in (13) with d = p,
we have with probability at least 1 − δ,

(cid:107)˜rn,p(cid:107) ≤ (1 + ε) (cid:107)rn,p(cid:107) ,

(cid:13)
(cid:13)φn,p − ˜φn,p
(cid:13)

(cid:13)
(cid:13)
(cid:13) ≤

√

εηn,p (cid:107)φn,p(cid:107) ,

(16a)

(16b)

where φn,p, rn,p, ˜φn,p and ˜rn,p are deﬁned,, respectively, in (3), (6), (15b) and (15c),

ηn,p = κ(Xn,p)

(cid:112)

ξ−2 − 1,

(17)

κ(Xn,p) is the condition number of matrix Xn,p, and ξ ∈ (0, 1] is the fraction of yn,p
that lies in Range(Xn,p), that is, ξ := (cid:107)Hn,pyn,p(cid:107) / (cid:107)yn,p(cid:107) with Hn,p as in Deﬁnition 1.

12

Using a combination of exact leverage scores and the estimates (15c) on the OLS residuals
associated with the AR(p−1) model, we deﬁne quasi-approximate leverage scores for the AR(p)
model.

Deﬁnition 2 (Quasi-approximate Leverage Scores). For an AR(p) model with p ≥ 2,
the quasi-approximate leverage scores are deﬁned by the following equation

˜(cid:96)n,p(i) := (cid:96)n−1,p−1(i) +

(˜rn−1,p−1(i))2
(cid:107)˜rn−1,p−1(cid:107)2

for i = 1, . . . , n − p,

(18)

where (cid:96)n,p(i) and ˜rn,p are as in Deﬁnition 1 and (15c).

Clearly, the practical advantage of ˜(cid:96)n,p is entirely contingent upon the availability of the
exact leverage scores for p − 1, that is, (cid:96)n−1,p−1 (cf. Deﬁnition 2). For p = 2, this is indeed
possible. More speciﬁcally, from (14a), the exact leverage scores of an AR(1) model can be
trivially calculated, which in turn give the quasi-approximate leverage scores {˜(cid:96)n−2,2(i)}n−2
i=1
using (18). However, for p = 3 (and subsequent values), the relation (18) does not apply
as not only are {(cid:96)n−1,p−1(i)}n−p
i=1 no longer readily available, but also for the same token
without having {πn−1,p−1(i)}n−p
i=1 , the residual vector ˜rn−1,p−1 may not be computed directly
(cf. Remark 6). Nonetheless, replacing the exact leverage scores with quasi-approximate ones
in (18) for p = 2 allows for a new approximation for p = 3. Such new leverage score estimates
can be in turn incorporated in approximation of subsequent leverage scores for p ≥ 4. This
idea leads to our ﬁnal and practical deﬁnition of fully-approximate leverage scores.

Deﬁnition 3 (Fully-approximate Leverage Scores). For an AR(p) model with p ≥ 1, the
fully-approximate leverage scores are deﬁned by the following equation

ˆ(cid:96)n,p(i) :=





(cid:96)n,1(i),
˜(cid:96)n,2(i),
ˆ(cid:96)n−1,p−1(i) +

(ˆrn−1,p−1(i))2
(cid:107)ˆrn−1,p−1(cid:107)2 ,

for p = 1

for p = 2

,

for p ≥ 3

where

ˆrn−1,p−1 := yn−1,p−1 − Xn−1,p−1
(cid:124)
ˆφn−1,p−1 := ( ˆX
n−1,p−1

ˆXn−1,p−1)−1 ˆX

ˆφn−1,p−1,

(cid:124)
n−1,p−1 ˆyn−1,p−1

(19a)

(19b)

(19c)

and ˆXn−1,p−1 and ˆyn−1,p−1 are the reduced data matrix and response vector, sampled

13

respectively, according to the distribution

ˆπn−1,p−1(i) =

ˆ(cid:96)n−1,p−1(i)
p − 1

for i = 1, . . . , n − p.

(19d)

Remark 7. It should be noted that (18) estimates the leverage scores of an AR(p) model,
given the corresponding exact values of an AR(p − 1) model. This is in sharp contrast
to (19a), which recursively provides similar estimates without requiring any information
on the exact values.

Unlike the quasi-approximate leverage scores, the fully-approximate ones in Deﬁnition 3
can be easily calculated for any given the parameter value p ≥ 1. Finally, Theorem 3 provides
a priori relative-error estimate on individual fully-approximate leverage scores.

Theorem 3 (Relative Errors for Fully-approximate Leverage Scores). For the fully-
approximate leverage scores, we have with probability at least 1 − δ,

|(cid:96)n,p(i) − ˆ(cid:96)n,p(i)|
(cid:96)n,p(i)

≤ (cid:0)1 + 3ηn−1,p−1κ2(Xn,p)(cid:1) (p − 1)

√

ε,

for i = 1, . . . , n − p,

recalling that δ, ηn,p, κ(Xn,p), and ε are as in Theorem 2.

Although qualitatively descriptive, the bound in Theorem 3 is admittedly pessimistic and
involves an overestimation factor that scales quadratically with the condition number of the
data matrix, κ, and linearly with the order of the AR model, p. We conjecture that the linear
dependence on p can be instead replaced with log(p), which is supported by the experiment
depicted in Figure 2. We leave the investigation of ways to improve the upper-bound of
Theorem 3 to future work.

Theorem 3 prescribes the misestimation factor β (cf. (12)) for the fully-approximate

leverage sores of an AR(p) model, stated in Corollary 1.

Corollary 1. The misestimation factor β for the fully-approximate leverage scores of
an AR(p) model is 1 − O(p

ε).

√

3.2 LSAR Algorithm for Fitting AR Models

Based on these theoretical results, we introduce the LSAR algorithm, depicted in Algorithm 1,
which is the ﬁrst leverage score sampling algorithm to approximately ﬁt an appropriate AR
model to a given big time series data. The theoretical properties of the LSAR algorithm are
given in Theorems 4 and 5.

14

Algorithm 1 LSAR: Leverage Score Sampling Algorithm for Approximate AR Fitting

Input:
- Time series data {y1, . . . , yn} ;
- A relatively large value ¯p (cid:28) n ;
- Constant parameters 0 < ε < 1 and 0 < δ0 < 1;
Step 0. Set p = 0 and m = n − ¯p ;
while p < ¯p do

Step 1. p ← p + 1 and m ← m + 1 ;
Step 2. Estimate PACF at lag p, i.e., ˆτp ;
Step 3. Compute the approximate leverage scores ˆ(cid:96)m,p(i) for i = 1, . . . , m − p as in
(19a) ;
Step 4. Compute the sampling distribution ˆπm,p(i) for i = 1, . . . , m − p as in (19d) ;
Step 5. Set s as in (13) by replacing d with p, δ = δ0/p, and β with the bound given in
Corollary 1 ;
Step 6. Form the s × m sampling matrix S by randomly choosing s rows of the corre-
sponding identity matrix according to the probability distribution found in Step 4, with
replacement, and rescaling them with the factor (11) ;
Step 7. Construct the sampled data matrix ˆXm,p = SXm,p and response vector ˆym,p =
Sym,p ;
Step 8. Solve the associated reduced OLS problem to estimate the parameters ˆφm,p and
residuals ˆrm,p as in (19b) and (19c), respectively ;

end while
Step 9. Estimate p∗ as the largest p such that |ˆτp| ≥ 1.96/
Output: Estimated order p∗ and parameters ˆφn−¯p+p∗,p∗.

√

s ;

Remark 8. For the overall failure probability, recall that in order to get an accumulative
success probability of 1 − δ0 for ¯p iterations, the per-iteration failure probability is set
√
as δ = 1 − ¯p
1 − δ0 ∈ Ω(δ0/¯p). However, since this dependence manifest itself only
logarithmically, it is of negligible consequence in overall complexity.

The quality of the ﬁtted model by the LSAR algorithm depends on two crucial ingredients,
the order of the underlying AR model as well the accuracy of the estimated parameters. The
latter is guaranteed by Theorem 2. For the former, Theorem 4 shows that for small enough
ε, the LSAR algorithm can estimate the same model order as that using the full data matrix.
Let τp and ˆτp be the PACF values estimated using the CMLE of parameter vectors based

on the full and sampled data matrices, φn,p−1 and ˆφn,p−1, respectively.

15

Theorem 4 (LSAR Model-order Estimation). Consider a causal AR(p∗) model and let
0 < ε, δ < 1. For sampling with fully-approximate leverage scores using a sample size s
as in (13) with d = p∗ and β as in Corollary 1 with p = p∗, we have with probability at
least 1 − δ,

|ˆτp| ≥ |τp| − c1

√

ε,

|ˆτp| ≤ |τp| + c2

(cid:112)(p − 1)ε,

for p = p∗,
for p > p∗,

(20a)

(20b)

where c1 and c2 are bounded positive constants depending on a given realization of the
model.

Theorem 4 implies that, when |τp∗| ≥ 1.96/
probability, we are guaranteed to have |ˆτp∗| ≥ 1.96/
n+O(
for p > p∗, respectively. In practice, we can consider a larger bandwidth of size 2 × 1.96/
see the experiments of Section 4.

n for p > p∗, with high
√
(cid:15))
s;

(cid:15)) and |ˆτp| ≤ 1.96/

n and |τp| ≤ 1.96/

n−O(

√
√

√

√

√

√

Theorem 5 gives the overall running time of the LSAR algorithm.

Theorem 5 (LSAR Computational Complexity). The worst case time complexity of
the LSAR algorithm for an input AR(p∗) time series data is O
, with
probability at least 1−δ0 (0 < δ0 < 1) and the pth iteration of the algorithm has δ = δ0/p,
which appears in the log for each sample size.

np∗ + p∗4 log p∗/ε2(cid:17)

(cid:16)

Remark 9. We believe that the restriction on ε given by Theorem 5 is highly pessimistic
and merely a by-product of our proof techniques here. As evidenced by numerical
experiments, e.g., Figure 2, we conjecture that a more sensible bound is 0 < ε ≤
(log p∗)−2; see also the discussion in the last paragraph of Section 2 and Remark 5. In
fact, even the tight bounds on the sample size for RandNLA routines rarely manifest
themselves in practice (e.g., [19,20,27]). Guided by these observations, in our numerical
experiments of Section 4, we set our sample sizes at factions of the total data, e.g.,
s = 0.001n, even for small values of p∗.

4 Empirical Results

In this section, we present the performance of the LSAR algorithm on several synthetic as
well as real big time series data. The numerical experiments are run in MATLAB R2018b on
a 64-bit windows server with dual processor each at 2.20GHz with 128 GB installed RAM.
The numerical results reveal the eﬃciency of the LSAR algorithm, as compared with the
classical alternative using the entire data. More precisely, it is illustrated that by sam-
pling only 0.1% of the data, not only are the approximation errors kept signiﬁcantly small,

16

but also the underlying computational times are considerably less than the corresponding
exact algorithms.

We present our numerical analysis in three subsequent sections. In Section 4.1, we report
the computational times as well as the quality of leverage score approximations (19a) on
three synthetically generated data by running Steps 0-8 of the LSAR algorithm. Analogously,
Section 4.2 shows similar results for estimating PACF (i.e., the output of Step 2 in the LSAR
algorithm). Finally, Section 4.3 displays the performance of the LSAR algorithm on a real
big time series data. It should be noted that all computational times reported in this section
are in “seconds”.

4.1 Synthetic Data: Veriﬁcation of Theory

We generate synthetic large-scale time series data with two million realizations from the
models AR(20), AR(100), and AR(200). For each dataset, the leverage scores over a range
of lag values (i.e., the variable h in the LSAR algorithm) are calculated once by using the
exact formula as given in Deﬁnition 1, and another time by estimating the fully-approximate
leverage scores as deﬁned in (19a). The latter is computed by running Steps 0-8 of the LSAR
algorithm with s = 0.001n = 2000.

Figure 1 displays and compares the quality and run time between the fast sampled
randomized Hadamard transform (SRHT) approximation technique developed by Drineas
et al. [9] and (19). At each lag p, the maximum pointwise relative error (MPRE, for short) is
deﬁned by

(cid:40)

|ˆ(cid:96)n,p(i) − (cid:96)n,p(i)|
(cid:96)n,p(i)

(cid:41)

.

max
1≤i≤n−p

(21)

As displayed in Figures 1a to 1c, while the MPRE curves have sharp increase at the be-
ginning and then quickly converge to an upper limit around 0.1670 for fully-approximate
leverage scores, the output of SRHT seems to converge around 3. This demonstrates the
high-quality of the fully-approximate leverage scores using only 0.1% of the rows of the data
matrix. More interestingly, Figures 1d to 1f demonstrate the computational eﬃciency of
the fully-approximate leverage scores. In light of the inferior performance of SRHT, both in
terms of the quality of approximation and also run time, in the subsequent experiments, we
will no longer consider SRHT approximation alternative.

Figures 1a to 1c suggest that the upper bound provided in Theorem 3 might be improved
by replacing p − 1 with an appropriate scaled function of log(p). This observation is nu-
merically investigated in Figure 2. In this ﬁgure (which in logarithmic scale), the MPRE (21)
(in blue) is compared with the right hand side (RHS) of Theorem 3 (in red) as well as the
RHS of Theorem 3 with p − 1 replaced with a scaled log(p) (in green). These results are in
strong agreement with Remarks 5 and 9. Indeed, improving the dependence of the RHS of
Theorem 3 on p is an interesting problem, which we intend to address in future works.

Figure 3 exhibits the impact of the data size n and the sample size s on MPRE for the
AR(100) synthetic data. More precisely, this ﬁgure demonstrates MPRE for values of n ∈

17

(a) AR(20)

(b) AR(100)

(c) AR(200)

(d) AR(20)

(e) AR(100)

(f) AR(200)

Figure 1: Figures (a), (b) and (c) correspond to AR(20), AR(100), and AR(200) using synthetic
data, respectively, and display the MPRE (21) versus the lag values h for fully-approximate
and the SRHT method. Similarly, Figures (d), (e), and (f) represent the computational time
spent, in seconds, to compute the fully-approximate leverage scores (in blue), the SRHT
approximation (in magenta), and the exact leverage scores (in red) on AR(20), AR(100), and
AR(200) using synthetic data, respectively.

(a) AR(20)

(b) AR(100)

(c) AR(200)

Figure 2: Figures (a), (b) and (c) correspond to AR(20), AR(100), and AR(200) with synthetic
data, respectively. Here, we display the MPRE (21) (in blue), the RHS of Theorem 3 (in red)
and RHS of Theorem 3 with p − 1 replaced with a scaled log(p) (in green).

{500K, 1M, 2M } (where, K and M stand for “thousand” and “million”, respectively) and
s ∈ {0.001n, 0.01n, 0.1n}. Clearly, for each ﬁxed value of n, by increasing s, MPRE decreases.
Furthermore, for each ﬁxed ratio of s/n, by increasing n, s increases and accordingly MPRE
decreases. It is clear that more data amounts to smaller approximation errors.

18

(a) n = 500K, s = 0.001n

(b) n = 500K, s = 0.01n

(c) n = 500K, s = 0.1n

(d) n = 1M, s = 0.001n

(e) n = 1M, s = 0.01n

(f) n = 1M, s = 0.1n

(g) n = 2M, s = 0.001n

(h) n = 2M, s = 0.01n

(i) n = 2M, s = 0.1n

Figure 3: The impact of the data size n ∈ {500K, 1M, 2M } and the sample size s ∈
{0.001n, 0.01n, 0.1n} on MPRE for the AR(100) synthetic data.

4.2 PACF: Computational Time and Estimation Accuracy

In this section, using the same synthetic data as in Section 4.1, we estimate PACF and ﬁt an
AR model. More precisely, for each dataset, PACF is estimated for a range of lag values h,
once by solving the corresponding OLS problem with the full-data matrix (called, “exact”),
and another time by running the LSAR algorithm (Algorithm 1).

The numerical results of these experiments for the three synthetic datasets are displayed
in Figure 4. As explained in Section 2.1, the most important application of a PACF plot
is estimating the order p by choosing the largest lag h such that its corresponding PACF
bar lies out of the 95% zero-conﬁdence boundary.
It is readily seen that Figures 4b, 4e
and 4h not only provide the correct estimate of the order p for the generated synthetic
data, but also are very close to the exact PACF plots in Figures 4a, 4d and 4g. This is
achieved all the while by merely sampling only 0.1% of the rows of the data matrix (i.e.,

19

s = 0.001n = 2000). Subsequently, from Figures 4c, 4f and 4i, one can observe a signiﬁcant
diﬀerence in the time required for computing PACF exactly as compared with obtaining a
high-quality approximation using the LSAR algorithm.

(a) AR(20)

(b) AR(20)

(c) AR(20)

(d) AR(100)

(e) AR(100)

(f) AR(100)

(g) AR(200)

(h) AR(200)

(i) AR(200)

Figure 4: Figures (a), (b) and (c) corresponding to the AR(20) synthetic data, display the
exact PACF plot, the PACF plot generated by the LSAR algorithm, and the comparison
between the computational time of (a) (in red) and (b) (in blue), respectively. Figures (d),
(e) and (f) are similar for the AR(100) synthetic data; and Figures (g), (h) and (i) are similar
for the AR(200) synthetic data.

Remark 10. Following Remark 3, ﬁnding PACF at each lag requires the solution to the
corresponding OLS problem. Hence, to avoid duplication, the computational times of

20

Steps 4-8 of the LSAR algorithm are excluded in Figure 1. Indeed, those computational
times are considered in Figure 4.

To show the accuracy of maximum likelihood estimates generated by the LSAR algorithm,
the estimates derived by the two scenarios of “full-data matrix” and “reduced data matrix”
are relatively compared. For this purpose, following notation deﬁned in Sections 2 and 3,
let φn,p and ˆφs
n,p denote the maximum likelihood estimates of parameters based on the full-
data matrix (cf. (3)) and reduced sampled data matrix with the sample size of s (cf. (19c)),
respectively. Accordingly, we deﬁne the relative error of parameter estimates by

|| ˆφs

n,p − φn,p||
||φn,p||

.

(22a)

Analogously, let rn,p and ˆrs
n,p be the residuals of estimates based on the full-data matrix (cf.
(6)) and reduced sampled data matrix with the sample size of s (cf. (19b)), respectively. The
ratio of two residual norms is given by

(cid:13)
(cid:13)
(cid:13)ˆrs
(cid:13)
n,p
(cid:107)rn,p(cid:107)

.

(22b)

The two ratios (22a) and (22b) are calculated for a range of values of s ∈ {200, 300, . . . ,
1000} by computing the maximum likelihood estimates of the AR(100) synthetic data once
with the full-data matrix and another time by running the LSAR algorithm. Also, the es-
timates are smoothed out by replicating the LSAR algorithm 1, 000 times and taking the
average of all estimates. The outcome is displayed in Figure 5. Figure 5a displays the rel-
ative errors of parameter estimates (22a) versus the sample size s and Figure 5b shows the
ratio of residual norms (22b) versus the sample size s.

(a) (22a) for the AR(100) data

(b) (22b) for the AR(100) data

Figure 5: Figures (a) and (b) display the relative error of parameter estimates (22a) and the
ratio of residual norms (22b) for the AR(100) synthetic data, respectively, both as a function
of sample size s.

21

4.3 Real-world Big Time Series: Gas Sensors Data

Huerta et al. [17] studied the accuracy of electronic nose measurements. They constructed a
nose consisting of eight diﬀerent metal-oxide sensors in addition to humidity and temperature
sensors with a wireless communication channel to collect data. The nose monitored airﬂow
for two years in a designated location, and data continuously collected with a rate of two
observations per second. In this conﬁguration, a standard energy band model for an n−type
metal-oxide sensor was used to estimate the changes in air temperature and humidity. Based
on their observations, humidity changes and correlated changes of humidity and temperature
were the most signiﬁcant statistical factors in variations of sensor conductivity. The model
successfully used for gas discrimination with an R-squared close to 1.

The data is available in the UCI machine learning repository2. In our experiment, we use
the output of sensor number 8 (column labeled R8 in the dataset) as real time series data
with n = 919, 438 observations. The original time series data is heteroscedastic. However,
by taking the logarithm of the data and diﬀerencing in one lag, it becomes stationary and
an AR(16) model seems to be a good ﬁt to the transformed data. We run the LSAR algorithm
with the initial input parameter ¯p = 100. Recalling that ¯p (p < ¯p (cid:28) n) is initially set large
enough to estimate the order p of the underlying AR model. Also, in all iterations of the
LSAR algorithm, we set the sample size s = 0.001n. For sake of fairness and completeness,
all ﬁgures generated for synthetic data in Sections 4.1 and 4.2, are regenerated for the gas
sensors data.

Figure 6a shows (in logarithmic scale) the maximum pointwise relative error (21) (in
blue) along with the RHS of Theorem 3 (in red) as well as the RHS of Theorem 3 with
p − 1 replaced with a scaled log(p) (in green). The behavior of these three graphs are very
similar to those ones on synthetic data discussed in Section 4.1. Furthermore, Figure 6b
reveals analogous computational eﬃciency in ﬁnding the fully-approximate leverage scores
comparing with the exact values for the gas sensors data.

In order to show the eﬃcacy
Leverage score sampling versus uniform sampling.
of the LSAR algorithm, we compare the performance of leverage score sampling with na¨ıve
uniform sampling in estimating the order as well as parameters of an AR model for the gas
sensor data. For the uniform sampling, we modify the LSAR algorithm slightly by removing
Step 3 and replacing the uniform distribution ˆπm,p(i) = 1/(m − p) for i = 1, . . . , m − p in
Step 4.

Figures 7a to 7d demonstrate the PACF plot calculated exactly, the PACF plot approxi-
mated with the LSAR algorithm, the PACF plot approximated based on a uniform sampling,
and a comparison between the computational times of these three PACF plots, respectively.
Similar to Section 4.2, Figure 7d reveals that while Figure 7b can be generated much faster
than Figure 7a, they both suggest the same AR model for the gas sensors data. In addition,
Figures 7c and 7d divulge that although the uniform sampling is slightly faster than the
LSAR algorithm, the PACF plot generated by the former is very poor and far away from the

2https://archive.ics.uci.edu/ml/datasets/Gas+sensors+for+home+activity+monitoring, Ac-

cessed on 30 December 2019.

22

(a) MPRE

(b) Computational time

Figure 6: Figure (a) displays the MPRE (21) (in blue), the RHS of Theorem 3 (in red) and
the RHS of Theorem 3 with p − 1 replaced with a scaled log(p) (in green) for the real gas
sensors data, Figure (b) shows the computational time spent, in seconds, to compute the
fully-approximate (in blue) and exact (in red) leverage scores for the real gas sensors data.

exact plot given in Figure 7a. While both Figures 7a and 7b estimate an AR(16) model for
the data, Figure 7c fails to make an appropriate estimate of the order.

Finally, we compare the performance of leverage score sampling with na¨ıve uniform sam-
pling in estimating the parameters of an AR(16) model for the gas sensor data. Figure 8
compares the performance of these two sampling strategies for sample sizes chosen from
s ∈ {200, 300, . . . , 1000}. For each sampling scheme and a ﬁxed sample size, the maximum
likelihood estimates are smoothed out by replicating the LSAR algorithm 1, 000 times and
taking the average of all estimates. Note that in all three Figures 8a to 8c, the blue and red
plots correspond with the leverage score and uniform sampling scheme, respectively.

Figure 8a displays the relative errors of parameter estimates (22a) and Figure 8b shows
the ratio of residual norms (22b), under the two sampling schemes. Both ﬁgures strongly
suggest that the leverage score sampling scheme outperforms the uniform sampling scheme.
Furthermore, while the output of the former shows stability and almost monotonic con-
vergence, the latter exhibits oscillations and does not show any indication of convergence
for such small sample sizes. This observation is consistent with the literature discussed in
Section 2.2. Despite the fact uniform sampling can be performed almost for free, Figure 8c
shows no signiﬁcant diﬀerence between the computational time of both sampling scheme.

Finally, in our numerical examples, depending on the order of the AR model, the time
diﬀerence between the exact method and the LSAR algorithm for model ﬁtting vary between
75 to 1600 seconds. In many practical situations, one might need to ﬁt hundreds of such
models and make time-sensitive decisions based on the generated forecasts, before new data
is provided. One such example is predicting several stock prices in a ﬁnancial market for
portfolio optimization, while the prices may be updated every few seconds. Another practical
example is predicting the meteorology indices for several diﬀerent purposes, with updates
becoming available every few minutes. In these situations, saving a few seconds/minutes in
forecasting can be crucial.

23

(a) Exact

(b) LSAR

(c) Uniform

(d) Computational time

Figure 7: Figures (a), (b), (c) and (d) display the exact PACF plot, the PACF plot generated
by the LSAR algorithm, the PACF plot approximated based on a uniform sampling, and the
comparison between the computational time of (a) (in red), (b) (in blue), and (c) (in pink),
respectively.

(a) Relative error (22a)

(b) Ratio of residuals (22b)

(c) Computational time

Figure 8: Figures (a), (b) and (c) display the relative error of parameter estimates (22a),
the ratio of residual norms (22b), and the computational time of two sampling schemes
based on the leverage scores (blue) and uniform distribution (pink) for the gas sensors data,
respectively.

24

5 Conclusion

In this paper, we have developed a new approach to ﬁt an AR model to big time series data.
Motivated from the literature of RandNLA in dealing with large matrices, we construct a
fast and eﬃcient algorithm, called LSAR, to approximate the leverage scores corresponding
to the data matrix of an AR model, to estimate the appropriate underlying order, and to ﬁnd
the conditional maximum likelihood estimates of its parameters. Analytical error bounds are
developed for such approximations and the worst case running time of the LSAR algorithm is
derived. Empirical results on large-scale synthetic as well as big real time series data highly
support the theoretical results and reveal the eﬃcacy of this new approach.

For future work, we are mainly interested in developing this approach for a more general
ARMA model. However, unlike AR, the (conditional) log-likelihood function for ARMA is a
complicated non-linear function such that (C)MLEs cannot be derived analytically. Thus,
it may require to exploit not only RandNLA techniques, but also modern optimization
algorithms in big data regime to develop an eﬃcient leverage score sampling scheme for
ARMA models.

References

[1] M. Abolghasemi, J. Hurley, A. Eshragh, and B. Fahimnia. Demand forecasting in
the presence of systematic events: Cases in capturing sales promotions. International
Journal of Production Economics, 230:107892, 2020.

[2] C.W. Anderson, E.A. Stolz, and S. Shamsunder. Multivariate autoregressive models for
classiﬁcation of spontaneous electroencephalographic signals during mental tasks. IEEE
Transactions on Biomedical Engineering, 45(3):277–286, 1998.

[3] H. Avron, M. Kapralov, C. Musco, C. Musco, A. Velingker, and A. Zandieh. A uni-
versal sampling method for reconstructing signals with simple Fourier transforms. In
Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing,
pages 1051–1063. ACM, 2019.

[4] H. Avron, P. Maymounkov, and S. Toledo. Blendenpik: Supercharging LAPACK’s
least-squares solver. SIAM Journal on Scientiﬁc Computing, 32(3):1217–1236, 2010.

[5] P.J. Blackwell and R.A. Davis. Time Series: Theory and Methods. Springer Series in

Statistics. Springer, 2009.

[6] G.E.P. Box and G.M. Jenkins. Time Series Analysis, Forecasting and Control. Holden-

Day, San Francisco, 1976.

[7] N. Chakravarthy, A. Spanias, L.D. Iasemidis, and K. Tsakalis. Autoregressive model-
ing and feature analysis of dna sequences. EURASIP Journal on Advances in Signal
Processing, 2004:13–28, 2004.

25

[8] K.L Clarkson and D.P. Woodruﬀ. Low-rank approximation and regression in input

sparsity time. Journal of the ACM (JACM), 63(6):54, 2017.

[9] P. Drineas, M. Magdon-Ismail, M.W. Mahoney, and D.P. Woodruﬀ. Fast approximation
of matrix coherence and statistical leverage. Journal of Machine Learning Research,
13(Dec):3475–3506, 2012.

[10] P. Drineas, M.W. Mahoney, S. Muthukrishnan, and T. Sarl´os. Faster least squares

approximation. Numerische Mathematik, 117(2):219–249, 2011.

[11] A. Eshragh, B. Ganim, and T. Perkins. The importance of environmental factors in

forecasting australian power demand. arXiv preprint arXiv:1911.00817, 2019.

[12] R. Estrin, D. Orban, and M.A. Saunders. LSLQ: An iterative method for linear least-
squares with an error minimization property. SIAM Journal on Matrix Analysis and
Applications, 40(1):254–275, 2019.

[13] D.C-L. Fong and M. Saunders. LSMR: An iterative algorithm for sparse least-squares

problems. SIAM Journal on Scientiﬁc Computing, 33(5):2950–2971, 2011.

[14] G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins paperback.

Johns Hopkins University Press, 1983.

[15] J.D. Hamilton. A new approach to the economic analysis of nonstationary time series

and the business cycle. Econometrica, 57(2):357–384, 1989.

[16] J.D. Hamilton. Time Series Analysis. Princeton University Press, New Jersey, 1994.

[17] R.A. Huerta, T.S. Mosqueiro, J. Fonollosa, N.F. Rulkov, and I. Rodr´ıguez-Luj´an. Online
humidity and temperature decorrelation of chemical sensors for continuous monitoring.
Chemometrics and Intelligent Laboratory Systems, 157(15):169–176, 2016.

[18] P. Ma, M.W. Mahoney, and B. Yu. A statistical perspective on algorithmic leveraging.

Journal of Machine Learning Research, 16(1):861–911, 2015.

[19] M.W. Mahoney. Randomized algorithms for matrices and data. Foundations and

Trends® in Machine Learning, 2011.

[20] M.W. Mahoney.

Lecture notes on randomized linear algebra.

arXiv preprint

arXiv:1608.04481, 2016.

[21] X. Meng and M.W. Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Proceedings of the 45th Annual
ACM Symposium on Theory of Computing, pages 91–100, 2013.

[22] X. Meng, M.A. Saunders, and M.W. Mahoney. LSRN: A parallel iterative solver for
strongly over-or underdetermined systems. SIAM Journal on Scientiﬁc Computing,
36(2):C95–C118, 2014.

26

[23] J.W. Messner and P. Pinson. Online adaptive lasso estimation in vector autoregressive
models for high dimensional wind power forecasting. International Journal of Forecast-
ing, 35(4):1485–1498, 2019.

[24] J. Nelson and H.L. Nguyen. OSNAP: Faster numerical linear algebra algorithms via
sparser subspace embeddings. In Proceedings of the 54th Annual IEEE Symposium on
Foundations of Computer Science, pages 117–126, 2013.

[25] C.C. Paige and M.A. Saunders. LSQR: An algorithm for sparse linear equations and
sparse least squares. ACM Transactions on Mathematical Software (TOMS), 8(1):43–71,
1982.

[26] G. Raskutti and M.W. Mahoney. A statistical perspective on randomized sketching for
ordinary least-squares. Journal of Machine Learning Research, 17(1):7508–7538, 2016.

[27] F. Roosta-Khorasani, G.J. Sz´ekely, and U.M. Ascher. Assessing stochastic algorithms
for large scale nonlinear least squares problems using extremal probabilities of linear
combinations of gamma random variables. SIAM/ASA Journal on Uncertainty Quan-
tiﬁcation, 3(1):61–90, 2015.

[28] P.J. Rousseeuw and M. Hubert. Robust statistics for outlier detection. Wiley Interdis-

ciplinary Reviews: Data Mining and Knowledge Discovery, 1(1):73–79, 2011.

[29] X. Shen and Q. Lu. Joint analysis of genetic and epigenetic data using a conditional

autoregressive model. BMC Genetics, 16(Suppl 1):51–54, 2018.

[30] X. Shi and D.P. Woodruﬀ. Sublinear time numerical linear algebra for structured ma-

trices. In Proceedings of the AAAI, 2019.

[31] R.H. Shumway and D.S. Stoﬀer. Time Series Analysis and Its Applications. Springer,

London, 2017.

[32] Marc Van Barel, Georg Heinig, and Peter Kravanja. A superfast method for solving
Toeplitz linear least squares problems. Linear algebra and its applications, 366:441–457,
2003.

[33] H.Y. Wang, M. Yang, and J. Stufken.

Information-based optimal subdata selec-
tion for big data linear regression. Journal of the American Statistical Association,
114(525):393–405, 2018.

[34] D.P. Woodruﬀ. Sketching as a tool for numerical linear algebra. Foundations and

Trends® in Theoretical Computer Science, 2014.

[35] Yuanzhe Xi, Jianlin Xia, Stephen Cauley, and Venkataramanan Balakrishnan. Superfast
and stable structured solvers for Toeplitz least squares via randomized sampling. SIAM
Journal on Matrix Analysis and Applications, 35(1):44–72, 2014.

27

[36] J. Yang, X. Meng, and M.W. Mahoney. Implementing randomized matrix algorithms in
parallel and distributed environments. In Proceedings of the IEEE, pages 58–92, 2016.

A Technical Lemmas and Proofs

A.1 Proof of Theorem 1

We ﬁrst present Lemma 1 which is used in the proof of Theorem 1.

Lemma 1 (Matrix Inversion Lemma [14]). Consider the 2 × 2 block matrix





(cid:124)



c b

 ,

b A

M =

where A, b, and c are an m × m matrix, an m × 1 vector and a scalar, respectively. If
A is invariable, the inverse of matrix M exists an can be calculated as follows

M −1 =

1
k



1

(cid:124)
−b

A−1





−A−1b kA−1 + A−1bb

(cid:124)

A−1

 ,

where k = c − b

(cid:124)

A−1b.

Proof of Theorem 1

Proof. For p = 1, computing the leverage score trivially boils down to normalizing the data
vector. For p ≥ 2, the data matrix is given by

Xn,p = (cid:0)yn−1,p−1 Xn−1,p−1

(cid:1) .

So, we have

X

(cid:124)
n,pXn,p =





(cid:124)
n−1,p−1yn−1,p−1
y
(cid:124)
n−1,p−1yn−1,p−1 X

(cid:124)
n−1,p−1Xn−1,p−1
y
(cid:124)
n−1,p−1Xn−1,p−1

X



 .

For sake of simplicity, let us deﬁne

Wn,p := X

(cid:124)
n,pXn,p.

Following Lemma 1, the inverse of matrix Wn,p is given by


1

(cid:124)
−φ
n−1,p−1

W −1

n,p =

1
un,p



−φn−1,p−1 un,pW −1

(cid:124)
n−1,p−1 + φn−1,p−1φ
n−1,p−1

28



 ,

where

(cid:124)
(cid:124)
n−1,p−1Xn−1,p−1W −1
un,p := y
n−1,p−1yn−1,p−1 − y
(cid:124)
(cid:124)
n−1,p−1Xn−1,p−1φn−1,p−1.
n−1,p−1yn−1,p−1 − y
= y

n−1,p−1X

(cid:124)
n−1,p−1yn−1,p−1

It is readily seen that

(cid:124)
n−1,p−1Xn−1,p−1W −1

(cid:124)
(cid:124)
un,p := y
n−1,p−1Xn−1,p−1φn−1,p−1 + y
n−1,p−1yn−1,p−1 − 2y
(cid:124)
(cid:124)
n−1,p−1Xn−1,p−1φn−1,p−1
n−1,p−1yn−1,p−1 − 2y
= y
+ y
n−1,p−1Wn−1,p−1φn−1,p−1
(cid:124)
(cid:124)
(cid:124)
n−1,p−1Xn−1,p−1φn−1,p−1 + φ
n−1,p−1yn−1,p−1 − 2y
n−1,p−1Wn−1,p−1φn−1,p−1
(cid:124)
(cid:124)
n−1,p−1yn−1,p−1 − 2y
n−1,p−1Xn−1,p−1φn−1,p−1
(cid:124)
n−1,p−1X
+ φ

(cid:124)
n−1,p−1Xn−1,p−1φn−1,p−1

(cid:124)
n−1,p−1Xn−1,p−1φn−1,p−1

= y

= y

= (cid:107)yn−1,p−1 − Xn−1,p−1φn−1,p−1(cid:107)2
= (cid:107)rn−1,p−1(cid:107)2 .

The ith leverage score is given by

(cid:96)n,p(i) = X

(cid:124)
n,p(i, :)W −1

= (cid:2)yi+p−1 X

n,p Xn,p(i, :)
n−1,p−1(i, :)(cid:3) W −1

n,p

(cid:124)

(cid:20)

(cid:21)

yi+p−1
Xn−1,p−1(i, :)

(cid:124)
n−1,p−1(i, :)φn−1,p−1

n−1,p−1(i, :)((cid:107)rn−1,p−1(cid:107)2 W −1

(cid:124)
n−1,p−1 + φn−1,p−1φ

n−1,p−1)(cid:3)

(cid:2)yi+p−1 − X

1
(cid:107)rn−1,p−1(cid:107)2
(cid:124)
n−1,p−1 + X
−yi+p−1φ

(cid:124)

(cid:20)

×

(cid:21)

yi+p−1
Xn−1,p−1(i, :)

=

=

(cid:124)
(cid:124)
n−1,p−1Xn−1,p−1(i, :)
n−1,p−1(i, :)φn−1,p−1yi+p−1 − yi+p−1φ

n−1,p−1)Xn−1,p−1(i, :)(cid:1)

(cid:124)

n−1,p−1(i, :)φn−1,p−1)2(cid:1)

(cid:0)y2

i+p−1 − X

n−1,p−1(i, :)((cid:107)rn−1,p−1(cid:107)2 W −1

1
(cid:107)rn−1,p−1(cid:107)2
(cid:124)
+X
(cid:124)
n−1,p−1(i, :)W −1
n−1,p−1Xn−1,p−1(i, :)
(cid:0)y2
(cid:124)
n−1,p−1(i, :)φn−1,p−1 + (X

(cid:124)
n−1,p−1 + φn−1,p−1φ

i+p−1 − 2yi+p−1X

+

1
(cid:107)rn−1,p−1(cid:107)2

= X

= (cid:96)n−1,p−1(i) +

= (cid:96)n−1,p−1(i) +

(cid:13)
(cid:13)yi+p−1 − X

1
(cid:107)rn−1,p−1(cid:107)2
r2
n−1,p−1(i)
(cid:107)rn−1,p−1(cid:107)2 .

(cid:124)
n−1,p−1(i, :)φn−1,p−1

(cid:13)
2
(cid:13)

29

A.2 Theorem 6 and Its Proof

Theorem 6 (Relative Errors for Quasi-approximate Leverage Scores). For the quasi-
approximate leverage scores, we have with probability at least 1 − δ,

|(cid:96)n,p(i) − ˜(cid:96)n,p(i)|
(cid:96)n,p(i)

≤ (cid:0)1 + 3ηn−1,p−1κ2(Xn,p)(cid:1) √

ε,

for i = 1, . . . , n − p,

recalling that ηn,p, κ(Xn,p), and ε are as in Theorem 2.

In order to prove Theorem 6, we ﬁrst introduce the following lemmas and corollary.

Lemma 2. The leverage scores of an AR(p) model for p ≥ 1, are given by

(cid:96)n,p(i) = min
z∈Rn−p

(cid:8)(cid:107)z(cid:107)2 | X

(cid:124)

n,pz = Xn,p(i, :)(cid:9) ,

for i = 1, . . . , n − p,

where Xn,p is the data matrix of the AR(p) model deﬁned in (4).

Proof. We prove this lemma by using Lagrangian multipliers. Deﬁne the function

h(z, λ) :=

1
2

(cid:124)

z

z − λ

(cid:124)

(X

(cid:124)
n,pz − Xn,p(i, :)).

By taking the ﬁrst derivative with respect to the vector z and setting equal to zero, we have,

∂h(z, λ)
∂z

= z − Xn,pλ = 0 ⇒ z(cid:63) = Xn,pλ(cid:63).

Now, by multiplying both sides by X

(cid:124)
n,p, we obtain,

simpliﬁed to

This implies that,

Thus,

X

(cid:124)
n,pz(cid:63) = X

(cid:124)
n,pXn,pλ(cid:63),

Xn,p(i, :) = X

(cid:124)
n,pXn,pλ(cid:63).

λ(cid:63) = (X

(cid:124)
n,pXn,p)−1Xn,p(i, :).

z(cid:63) = Xn,p(X

(cid:124)
n,pXn,p)−1Xn,p(i, :).

30

The square of the norm of z(cid:63) is equal to

(cid:107)z(cid:63)(cid:107)2 = (cid:0)X
= X

(cid:124)
(cid:124)
n,pXn,p)−1X
n,p(i, :)(X
(cid:124)
(cid:124)
n,pXn,p)−1Xn,p(i, :)
n,p(i, :)(X

(cid:124)
n,p

(cid:1) (cid:0)Xn,p(X

(cid:124)

n,pXn,p)−1Xn,p(i, :)(cid:1)

= (cid:96)n,p(i).

Lemma 3. For an AR(p) model with p ≥ 1, we have

(cid:107)Xn,p(i, :)(cid:107) ≤ (cid:107)Xn,p(cid:107)

(cid:113)

(cid:96)n,p(i),

for i = 1, . . . , n − p,

where Xn,p and (cid:96)n,p(i) are deﬁned, respectively, in (4) and Deﬁnition 1.

Proof. From Lemma 2 we have,

(cid:107)Xn,p(i, :)(cid:107) = (cid:13)
≤ (cid:13)

(cid:13)X
(cid:13)X

(cid:124)

n,pz(cid:63)(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:107)z(cid:63)(cid:107)
(cid:113)

(cid:124)
n,p

= (cid:107)Xn,p(cid:107)

(cid:96)n,p(i).

Lemma 4. For an AR(p) model with p ≥ 1, we have

|rn,p(i) − ˜rn,p(i)| ≤

√

εηn,p (cid:107)φn,p(cid:107) (cid:107)Xn,p(cid:107)

(cid:113)

(cid:96)n,p(i),

for i = 1, . . . , n − p,

where rn,p, ˜rn,p, ηn,p, φn,p, Xn,p, and (cid:96)n,p(i) are deﬁned respectively in (6), (15c), (17),
(3), (4), and Deﬁnition 1 and ε is the error in (16a).

Proof. From (16b) and the deﬁnition of l2 norm, we have
(cid:28) Xn,p(i, :)
(cid:107)Xn,p(i, :)(cid:107)

, (φn,p − ˜φn,p)

≤

(cid:29)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)φn,p − ˜φn,p
(cid:13)
√
εηn,p (cid:107)φn,p(cid:107) .

≤

So, we have

X

(cid:124)
n,p(i, :)φn,p − X

(cid:124)

n,p(i, :) ˜φn,p ≤

√

εηn,p (cid:107)φn,p(cid:107) (cid:107)Xn,p(i, :)(cid:107) .

31

Now by adding and subtracting yi+p on the left hand side, we yield

(cid:16)

yi+p − X

(cid:124)

n,p(i, :) ˜φn,p

(cid:17)

− (cid:0)yi+p − X

(cid:124)
n,p(i, :)φn,p

(cid:1) ≤

√

εηn,p (cid:107)φn,p(cid:107) (cid:107)Xn,p(i, :)(cid:107) ,

implying that,

˜rn,p(i) − rn,p(i) ≤

√

εηn,p (cid:107)φn,p(cid:107) (cid:107)Xn,p(i, :)(cid:107) .

As analogously we can construct a similar inequality for rn,p(i) − ˜rn,p(i), we have that
√

|rn,p(i) − ˜rn,p(i)| ≤

εηn,p (cid:107)φn,p(cid:107) (cid:107)Xn,p(i, :)(cid:107) .

Now, by using Lemma 3, we obtain

|rn,p(i) − ˜rn,p(i)| ≤

√

εηn,p (cid:107)φn,p(cid:107) (cid:107)Xn,p(cid:107)

(cid:113)

(cid:96)n,p(i).

Lemma 5. Let {y1, . . . , yn} be a time series data. For i = 1, . . . , n − p, we have

|rn−1,p−1(i)| ≤

(cid:113)

(cid:107)φn−1,p−1(cid:107)2 + 1 (cid:107)Xn,p(cid:107)

|˜rn−1,p−1(i)| ≤

(cid:114)(cid:13)
˜φn−1,p−1
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+ 1 (cid:107)Xn,p(cid:107)

(cid:113)

(cid:96)n,p(i),

(cid:113)

(cid:96)n,p(i),

(23a)

(23b)

where rn,p, ˜rn,p, φn,p, ˜φn,p, Xn,p, and (cid:96)n,p(i) are deﬁned respectively in (6), (15c), (3),
(15b), (4), and Deﬁnition 1.

Proof. The left hand side of (23a) can be written as below:

|rn−1,p−1(i)| = |yi+p−1 − X
= | (cid:2)yi+p−1 X
= |X

(cid:124)

n,p(i, :) (cid:2)1 −φ

(cid:124)
n−1,p−1(i, :)φn−1,p−1|
n−1,p−1(i, :)(cid:3) (cid:2)1 −φ
(cid:124)

(cid:124)
n−1,p−1

(cid:3)(cid:124)

|

(cid:3)(cid:124)

|

(cid:124)
n−1,p−1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X

(cid:124)
n,p(i, :)

(cid:124)
n−1,p−1

(cid:3)(cid:124)

(cid:2)1 −φ
(cid:113)

(cid:107)φn−1,p−1(cid:107)2 + 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:113)

(cid:107)φn−1,p−1(cid:107)2 + 1

=

≤

(cid:113)

(cid:107)φn−1,p−1(cid:107)2 + 1 (cid:107)Xn,p(i, :)(cid:107) .

Now, by using Lemma 3, we obtain,

|rn−1,p−1(i)| ≤

(cid:113)

(cid:107)φn−1,p−1(cid:107)2 + 1 (cid:107)Xn,p(cid:107)

(cid:113)

(cid:96)n,p(i).

(24)

Inequality (23b) can be proved analogously.

32

Lemma 6. Let {y1, . . . , yn} be a time series data. We have,

(rn−1,p−1(i))2
(cid:107)rn−1,p−1(cid:107)2 ≤ (cid:96)n,p(i),

for i = 1, . . . , n − p,

where rn,p and (cid:96)n,p(i) are deﬁned respectively in (6) and Deﬁnition 1.

Proof. Since the leverage score is a non-negative valued function, the proof is directly
achieved from Theorem 1.

Lemma 7. Let {y1, . . . , yn} be a time series data. We have

(cid:113)

(cid:115)

(cid:107)rn−1,p−1(cid:107) ≥

(cid:107)˜rn−1,p−1(cid:107) ≥

λmin(X (cid:124)

n,pXn,p) (cid:0)(cid:107)φn−1,p−1(cid:107)2 + 1(cid:1),
(cid:19)

(cid:18)(cid:13)
˜φn−1,p−1
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+ 1

,

λmin(X (cid:124)

n,pXn,p)

(25a)

(25b)

where rn,p, ˜rn,p, φn,p, ˜φn,p, and Xn,p are deﬁned respectively in (6), (15c), (3), (15b), and
(4) and λmin(.) denotes the minimum eigenvalue.

Proof. By deﬁnition, we have

(cid:107)rn−1,p−1(cid:107) = (cid:107)yn−1,p−1 − Xn−1,p−1φn−1,p−1(cid:107)
(cid:1) (cid:2)1 −φ
(cid:124)
n−1,p−1
(cid:3)(cid:124)(cid:13)
(cid:13)
(cid:13)

=

=

(cid:3)(cid:124)(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:0)yn−1,p−1 Xn−1,p−1
(cid:13)
(cid:13)
(cid:13)
(cid:2)1 −φ
(cid:13)
(cid:13)Xn,p
(cid:113)(cid:2)1 −φ
(cid:113)
λmin(X (cid:124)

(cid:124)
n−1,p−1

(cid:113)

λmin(X (cid:124)

(cid:124)
n−1,p−1

(cid:3) X (cid:124)
(cid:2)1 −φ
n,pXn,p
n,pXn,p) (cid:13)
(cid:3)(cid:13)
(cid:2)1 −φ
(cid:124)
(cid:13)
(cid:13)
n−1,p−1
n,pXn,p) (cid:0)(cid:107)φn−1,p−1(cid:107)2 + 1(cid:1).

=

≥

=

(cid:124)
n−1,p−1

(cid:3)(cid:124)

Inequality (25b) is proved analogously.

Lemma 8. For any positive integer numbers 1 < p < n, we have

κ(Xn−1,p−1) ≤ κ(Xn,p),

33

where Xn,p is deﬁned in (4) an κ(.) denotes the condition number.

Proof. It is readily seen that the matrix Xn,p can be written in the form of

Xn,p = (cid:0)yn,p Xn−1,p−1

(cid:1) .

On the other hand, by deﬁnition, we know that

λmax(X

(cid:124)
n,pXn,p) = sup
(cid:107)ν(cid:107)≤1

(cid:124)

X

(cid:124)
n,pXn,pν.

ν

Let u be a unit vector corresponding to the maximum eigenvalue λmax(X
and construct the vector

(cid:124)
n−1,p−1Xn−1,p−1)

¯u := (cid:2)0 u

(cid:124)(cid:3)(cid:124)

.

We have

λmax(X

(cid:124)

(cid:124)
n,pXn,p) ≥ ¯u
(cid:124)
= u

X

X

(cid:124)
n,pXn,p ¯u
(cid:124)
n−1,p−1Xn−1,p−1u

= λmax(X

(cid:124)
n−1,p−1Xn−1,p−1).

Analogously, one can show that λmin(X

(cid:124)
n,pXn,p) ≤ λmin(X

(cid:124)
n−1,p−1Xn−1,p−1). Thus, we have

(cid:115)

(cid:115)

κ(Xn,p) =

≥

λmax(X (cid:124)
λmin(X (cid:124)

λmax(X
λmin(X

n,pXn,p)
n,pXn,p)
(cid:124)
n−1,p−1Xn−1,p−1)
(cid:124)
n−1,p−1Xn−1,p−1)

= κ(Xn−1,p−1).

Corollary 2. For any positive integer numbers 1 < p < n, we have

where Xn,p is deﬁned in (4).

(cid:107)Xn−1,p−1(cid:107) ≤ (cid:107)Xn,p(cid:107) ,

Proof. Since λmax(X
Lemma 8.

(cid:124)

n,pXn,p) = (cid:107)Xn,p(cid:107)2, this inequality is directly derived from the proof of

34

Proof of Theorem 6

Proof. By using Theorems 1 and 2, we have

|(cid:96)n,p(i) − ˜(cid:96)n,p(i)| =

=

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:96)n−1,p−1(i) +

(rn−1,p−1(i))2
(cid:107)rn−1,p−1(cid:107)2 − (cid:96)n−1,p−1(i) −

(˜rn−1,p−1(i))2
(cid:107)˜rn−1,p−1(cid:107)2

(rn−1,p−1(i))2
(cid:107)rn−1,p−1(cid:107)2 −
(rn−1,p−1(i))2
(cid:107)rn−1,p−1(cid:107)2 −

(rn−1,p−1(i))2
(cid:107)˜rn−1,p−1(cid:107)2 +
(cid:12)
(rn−1,p−1(i))2
(cid:12)
(cid:12)
(cid:12)
(cid:107)˜rn−1,p−1(cid:107)2
(cid:12)

+

(rn−1,p−1(i))2
(cid:107)˜rn−1,p−1(cid:107)2 −
(cid:12)
(rn−1,p−1(i))2
(cid:12)
(cid:12)
(cid:107)˜rn−1,p−1(cid:107)2 −
(cid:12)
(cid:12)

≤ (rn−1,p−1(i))2

+

1
(cid:107)˜rn−1,p−1(cid:107)2

(cid:12)
(cid:12)
1
(cid:12)
(cid:107)rn−1,p−1(cid:107)2 −
(cid:12)
(cid:12)
(cid:12)(rn−1,p−1(i))2 − (˜rn−1,p−1(i))2(cid:12)
(cid:12)
(cid:12)

1
(cid:107)˜rn−1,p−1(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(˜rn−1,p−1(i))2
(cid:12)
(cid:12)
(cid:12)
(cid:107)˜rn−1,p−1(cid:107)2
(cid:12)
(˜rn−1,p−1(i))2
(cid:107)˜rn−1,p−1(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ (rn−1,p−1(i))2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)rn−1,p−1(cid:107)2 −

1
(1 + ε)2 (cid:107)rn−1,p−1(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

+

(cid:107)rn−1,p−1(cid:107)2 |(rn−1,p−1(i) − (˜rn−1,p−1) (i)) (rn−1,p−1(i) + (˜rn−1,p−1) (i))|
ε2 + 2ε
(1 + ε)2
1

(rn−1,p−1(i))2
(cid:107)rn−1,p−1(cid:107)2

≤

+

(cid:107)rn−1,p−1(cid:107)2 |rn−1,p−1(i) − (˜rn−1,p−1) (i)|
× (|rn−1,p−1(i)| + | (˜rn−1,p−1) (i)|) .

Now, from Lemmas 4 to 6, we have

|(cid:96)n,p(i) − ˜(cid:96)n,p(i)| ≤

ε2 + 2ε
(1 + ε)2 (cid:96)n,p(i)
1
(cid:107)rn−1,p−1(cid:107)2
(cid:18)(cid:113)

+

(cid:18)√

εηn−1,p−1 (cid:107)φn−1,p−1(cid:107) (cid:107)Xn−1,p−1(cid:107)

(cid:113)

(cid:96)n−1,p−1(i)

(cid:19)

×

(cid:107)φn−1,p−1(cid:107)2 + 1 (cid:107)Xn,p(cid:107)

(cid:113)

(cid:96)n,p(i)

(cid:114)(cid:13)
˜φn−1,p−1
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+

+ 1 (cid:107)Xn,p(cid:107)

(cid:113)

(cid:96)n,p(i)

(cid:33)

(cid:32)√

ε(2 + ε)
(1 + ε)2 +

≤

ηn−1,p−1 (cid:107)φn−1,p−1(cid:107) (cid:107)Xn−1,p−1(cid:107) (cid:107)Xn,p(cid:107)
(cid:107)rn−1,p−1(cid:107)2

35

×

(cid:32)

×







×



(cid:32)(cid:113)

(cid:107)φn−1,p−1(cid:107)2 + 1 +

(cid:114)(cid:13)
˜φn−1,p−1
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+ 1

(cid:33)(cid:33)

√

ε(cid:96)n,p(i)

≤

1 +

ηn−1,p−1 (cid:107)φn−1,p−1(cid:107) (cid:107)Xn−1,p−1(cid:107) (cid:107)Xn,p(cid:107)
(cid:107)rn−1,p−1(cid:107)2
(cid:114)(cid:13)
˜φn−1,p−1
(cid:13)
(cid:13)

(cid:107)φn−1,p−1(cid:107)2 + 1 +

(cid:13)
2
(cid:13)
(cid:13)

+ 1

(cid:32)(cid:113)

(cid:33)(cid:33)

√

ε(cid:96)n,p(i).

Motivated from Lemma 7 along with using Corollary 2, we obtain

|(cid:96)n,p(i) − ˜(cid:96)n,p(i)| ≤

(cid:18)

1 +

ηn−1,p−1 (cid:107)φn−1,p−1(cid:107) (cid:107)Xn−1,p−1(cid:107) (cid:107)Xn,p(cid:107)
(cid:107)rn−1,p−1(cid:107)
(cid:114)(cid:13)
˜φn−1,p−1
(cid:13)
(cid:13)

+ 1

(cid:113)

(cid:107)φn−1,p−1(cid:107)2 + 1
(cid:107)rn−1,p−1(cid:107)

+

(cid:13)
2
(cid:13)
(cid:13)
(cid:107)rn−1,p−1(cid:107)













√

ε(cid:96)n,p(i)

≤

1 +

ηn−1,p−1

(cid:113)

(cid:107)φn−1,p−1(cid:107)2 + 1 (cid:107)Xn,p(cid:107)2

(cid:107)rn−1,p−1(cid:107)







×

(cid:113)

(cid:107)φn−1,p−1(cid:107)2 + 1
(cid:107)rn−1,p−1(cid:107)

+

(1 + ε)

(cid:114)(cid:13)
˜φn−1,p−1
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+ 1

(cid:107)˜rn−1,p−1(cid:107)













√

ε(cid:96)n,p(i).

Now, by using Lemma 7, we obtain



|(cid:96)n,p(i) − ˜(cid:96)n,p(i)| ≤

1 +

ηn−1,p−1λmax(X

(cid:124)
n,pXn,p)

(cid:113)

λmin(X (cid:124)

n,pXn,p)





×

(cid:18)

(cid:113)

1
λmin(X (cid:124)

n,pXn,p)

+

(cid:113)

1 +

3ηn−1,p−1λmax(X
λmin(X (cid:124)

≤
n,pXn,p)
= (cid:0)1 + 3ηn−1,p−1κ2(Xn,p)(cid:1) √

(cid:124)
n,pXn,p)

ε(cid:96)n,p(i).

1 + ε
λmin(X (cid:124)
(cid:19) √

n,pXn,p)

ε(cid:96)n,p(i)









√

ε(cid:96)n,p(i)

A.3 Proof of Theorem 3

Proof. We prove by induction. For p = 2, it is derived directly from Theorem 6. Let us
assume that the statement of theorem is correct for all values of p < ¯p, and prove that it is

36

also correct for p = ¯p.

|(cid:96)n,¯p(i) − ˆ(cid:96)n,¯p(i)| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:96)n−1,¯p−1(i) +

(rn−1,p−1(i))2
(cid:107)rn−1,p−1(cid:107)2 − ˆ(cid:96)n−1,¯p−1(i) −
(rn−1,p−1(i))2
(cid:107)rn−1,p−1(cid:107)2 −
√

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(ˆrn−1,p−1(i))2
(cid:12)
(cid:12)
(cid:12)
(cid:107)ˆrn−1,p−1(cid:107)2
(cid:12)
(ˆrn−1,p−1(i))2
(cid:107)ˆrn−1,p−1(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ε(cid:96)n−1,¯p−1(i)

≤

(cid:12)
(cid:12)(cid:96)n−1,¯p−1(i) − ˆ(cid:96)n−1,¯p−1(i)
(cid:12)

≤ (cid:0)1 + 3ηn−2,¯p−2κ2(Xn−1,¯p−1)(cid:1) (¯p − 2)
+ (cid:0)1 + 3ηn−1,¯p−1κ2(Xn,¯p)(cid:1) √
ε(cid:96)n,¯p(i)
√
≤ (cid:0)1 + 3ηn−1,¯p−1κ2(Xn,¯p)(cid:1) (¯p − 2)
+ (cid:0)1 + 3ηn−1,¯p−1κ2(Xn,¯p)(cid:1) √
= (cid:0)1 + 3ηn−1,¯p−1κ2(Xn,¯p)(cid:1) (¯p − 1)

ε(cid:96)n,¯p(i)
√

ε(cid:96)n,¯p(i)

ε(cid:96)n,¯p(i).

The second last inequality comes from the induction hypothesis as well as Theorem 6 and
the last inequality is from Lemma 8.

A.4 Proof of Theorem 4

Proof. From Theorem 2, we have (16b), which in turn implies

(cid:12)
(cid:12)φn,p∗(k) − ˆφn,p∗(k)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤

√

εηn,p∗ (cid:107)φn,p∗(cid:107) ,

for 1 ≤ k ≤ p∗.

One can estimate the PACF value at lag p∗ using the p∗th component of the CMLE of the
parameter vector based on the full data matrix, i.e., φn,p∗(p∗), [31, Chapter 3]. Hence, (20a)
now readily follows by an application of reverse triangular inequality.
To show (20b), we recall that [31, Chapter 3]

PACFp =

Cov(p) − (cid:80)p−1
k=1 φkCov(p − k)
σ2
W

,

where Cov(p) is the autocovariance function at lag p and σ2
W is the variance of white noise
series in an AR(p − 1) model. It follows that τp is given by plugin the CMLE of Cov(p), φk
for k = 1, . . . , p − 1 and σ2

W , that is,

τp =

γ(p) − (cid:80)p−1

k=1 φn,p−1(k)γ(p − k)
(cid:107)rn,p−1(cid:107)2 /n

.

Hence, for p > p∗, we have

(cid:12)
(cid:12)γ(p) − (cid:80)p−1
(cid:12)

(cid:12)
ˆφn,p−1(k)γ(p − k)
(cid:12)
(cid:12)

k=1
(cid:107)ˆrn,p−1(cid:107)2 /n

|ˆτp| =

37

(cid:17)

γ(p − k)

(cid:105)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)γ(p) − (cid:80)p−1
(cid:12)

k=1

=

(cid:12)
(cid:12)
γ(p − k)
(cid:12)

(cid:104)(cid:16) ˆφn,p−1(k) + φn,p−1(k) − φn,p−1(k)
(cid:107)ˆrn,p−1(cid:107)2 /n
(cid:17)
φn,p−1(k) − ˆφn,p−1(k)
(cid:107)rn,p−1(cid:107)2 /n
(cid:12)
(cid:12)φn,p−1(k) − ˆφn,p−1(k)
(cid:12)
(cid:107)rn,p−1(cid:107)2 /n
(cid:13)
(cid:13)φn,p−1 − ˆφn,p−1
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

(cid:80)p−1
k=1

(cid:12)
(cid:12)
(cid:12)

γ(0) (cid:80)p−1

k=1

√

γ(0)

p − 1

(cid:107)rn,p−1(cid:107)2 /n

≤ |τp| +

≤ |τp| +

≤ |τp| +

≤ |τp| +

ηn,p (cid:107)φn,p(cid:107) γ(0)
(cid:107)rn,p−1(cid:107)2 /n

(cid:112)(p − 1)ε.

Now, the result follows by noting that (cid:107)rn,p−1(cid:107)2 /n is an MLE estimate of σ2
W , and from
convergence in probability of this estimate, we have that, for large enough n, it is bounded
with probability at least 1 − δ.

A.5 Proof of Theorem 5

Proof. Consider an input AR(p∗) time series data of size n. From Deﬁnition 3, Theorem 3,
and Remark 10, given the fully-approximate leverage scores for the data matrix corre-
sponding to the AR(p − 1) models for p varying from 2 to p∗, we can estimate those of
AR(p) models in O (n) time. Here, we assume that κ(Xn,p) does not scale with the di-
mension p (at least unfavorably so), and treat it as a constant. Theorem 3 implies that
we must choose 0 < ε ≤ p−2. Now, solving the compressed OLS problem (e.g., applying
QR factorization with Householder reﬂections) requires O (sp2) = O (p3 log p/ε2). As a re-
sult, the overall complexity of performing the LSAR for an input AR(p∗) time series data is
O

np∗ + p∗4 log p∗/ε2(cid:17)

(cid:16)(cid:80)p∗

p=1(n + p3 log p/ε2)

= O

(cid:16)

(cid:17)

.

38

