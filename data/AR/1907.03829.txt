Empirical Bayesian Learning in AR Graphical Models

aDipartimento di Ingegneria dell’Informazione, Universit`a degli studi di Padova, via Gradenigo 6/B, 35131 Padova, Italy

Mattia Zorzi a

9
1
0
2

l
u
J

8

]

C
O
.
h
t
a
m

[

1
v
9
2
8
3
0
.
7
0
9
1
:
v
i
X
r
a

Abstract

We address the problem of learning graphical models which correspond to high dimensional autoregressive stationary stochastic
processes. A graphical model describes the conditional dependence relations among the components of a stochastic process
and represents an important tool in many ﬁelds. We propose an empirical Bayes estimator of sparse autoregressive graphical
models and latent-variable autoregressive graphical models. Numerical experiments show the beneﬁt to take this Bayesian
perspective for learning these types of graphical models.

Key words: Sparsity and low rank inducing priors, empirical Bayesian learning, convex relaxation, convex optimization.

1 Introduction

In modern applications many variables are accessible to
observation. In some cases the latter can be modeled
with a high dimensional Gaussian random vector. To
gain some insight about the relation among those vari-
ables we can attach to it a graphical model (Lauritzen,
1996; Willsky, 2002). The latter is an undirected graph
wherein nodes correspond to the components (i.e. vari-
ables) of the random vector and there is the lack of an
edge between two nodes if and only if the corresponding
variables are conditionally independent given the oth-
ers. It turns out that sparse graphical models, i.e. graphs
with few edges, have the inverse covariance matrix of
the random vector which is sparse. Then, the problem of
estimating a sparse graphical model from the observed
data can be formulated as a regularization problem: ﬁnd
such an inverse matrix which minimizes the negative log-
likelihood and a regularization term inducing sparsity
(Banerjee et al., 2008).

An important aspect is that variables are typically
measured over time and can thus be modeled as a high
dimensional autoregressive (AR) Gaussian stationary
stochastic process. Then, we can attach a graphical
model describing the conditional dependence relations
among the variables. It is possible to prove that sparse
graphical models have the inverse power spectral den-
sity (PSD) of the process which is sparse. Songsiri &
Vandenberghe (2010) proposed a regularized estimator
for estimating sparse AR graphical models in the same

Email address: zorzimat@dei.unipd.it (Mattia Zorzi).

spirit of (Banerjee et al., 2008). Avventi et al. (2013)
showed that the aforementioned estimator is a relaxed
version of a maximum entropy estimator. The latter
solves a covariance extension problem whose dual prob-
lem does coincide with the one proposed in Songsiri
et al. (2010). As a consequence, the estimator proposed
by Songsiri & Vandenberghe (2010) is strictly connected
with the generalized moment problems in the sense
of Byrnes-Georgiou-Lindquist which have been exten-
sively studied by many researchers, e.g. Byrnes et al.
(2000); Ferrante et al. (2012); Karlsson & Georgiou
(2013); Zorzi (2015, 2014a, 2015, 2014b). Since then,
many other extensions has been proposed: Maanan
et al. (2017) proposed a two stage approach to esti-
mate sparse AR graphical models; Alpago et al. (2018)
proposed a regularized estimator for sparse graphical
models of reciprocal processes; Chandrasekaran et al.
(2010), Zorzi & Sepulchre (2016), Maanan et al. (2018),
Li´egeois et al. (2015) and Ciccone et al. (2018) proposed
regularized estimators for the so called latent-variable
graphical models.

The regularizers for inducing sparsity in such graphi-
cal models are (cid:96)1-like norms. The (cid:96)1 norm, however, pe-
nalizes diﬀerently the nonnull coeﬃcients: larger coef-
ﬁcients are penalized more heavily than smaller coeﬃ-
cients. This imbalance produces an estimator with a re-
markable mean squared error. Cand`es et al. (2008) pro-
posed to reduce this imbalance by considering a weighted
(cid:96)1 norm wherein the weights are computed in an iter-
ative fashion. The resulting procedure is similar to the
adaptive lasso and is typically called iterative reweighted
algorithm (Wipf & Nagarajan, 2010).

Preprint submitted to Automatica

10 July 2019

 
 
 
 
 
 
The present paper proposes a regularized estimator in
the spirit of Songsiri & Vandenberghe (2010) for sparse
AR graphical models where the (cid:96)1-like norm is substi-
tuted by a weighted (cid:96)1-like norm leading to an iterative
reweighted procedure. Interestingly, drawing inspiration
by Asadi et al. (2009); Scheinberg et al. (2010), the pro-
posed method can be understood as an empirical Bayes
approach which provides a suitable updating rule for the
weights. Such idea is then extended to the regularized
estimator for latent-variable AR graphical models pro-
posed in Zorzi & Sepulchre (2016).

The outline of the paper is as follows. In Section 2 we in-
troduce the problem of estimating sparse AR graphical
models. In Section 3 we propose an iterative reweighted
for solving such a problem, while in Section 4 we derive
the estimator using a Bayesian perspective. Section 5 re-
gards the identiﬁcation of latent-variable AR graphical
models. Section 6 contains some numerical experiments
to test the performance of the proposed estimators. Fi-
nally, the conclusions are drawn in Section 7.

Notation

The vector space Rm is endowed with the inner product
(cid:104)x, y(cid:105) = xT y. x ≥ 0 (x > 0) with x ∈ Rm means that all
the entries of the vector are nonnegative (positive). The
vector space Rm×m is endowed with the inner product
(cid:104)X, L(cid:105) = tr(XLT ). Qm denotes the vector space of sym-
metric matrices of dimension m × m, if X ∈ Qm is posi-
tive deﬁnite (semi-deﬁnite) we write X (cid:31) 0 (X (cid:23) 0). |X|
denotes the determinant of matrix X ∈ Qm. diag(X) is
the diagonal matrix whose main diagonal coincides with
the one of X. (X)jh denotes the entry in position (j, h)
of matrix X. A matrix A ∈ Rl×m(n+1) with l ≤ m will
be partitioned as A = [ A0 A1 . . . An ] with Aj ∈ Rl×m.
Mm,n is the vector space of matrices Y := [ Y0 Y1 . . . Yn ]
with Y0 ∈ Qm and Y1 . . . Yn ∈ Rm×m. The correspond-
ing inner product is (cid:104)Y, Z(cid:105) = tr(Y Z T ). The linear op-
erator T : Mm,n → Qm(n+1) constructs a symmet-
ric Toeplitz matrix T(Y ) where its ﬁrst block row is
[ Y0 Y1 . . . Yn ]. The adjoint operator of T is denoted
by D : Qm(n+1) → Mm,n and deﬁned as follows. If
X ∈ Qm(n+1) is partitioned as a n + 1 × n + 1 block
matrix with Xhj, j, h = 0 . . . n, the block in position
(h, j) then D(X) = [ D0(X) . . . Dn(X) ] where D0(X) =
(cid:80)n
h=0 Xh h+k, k = 1 . . . n. We
deﬁne the index set E ⊆ V × V with V := {1, 2, . . . m}.
Functions on the unit circle {eiϑ s.t. ϑ ∈ [−π, π]} will
be denoted by capital Greek letters, e.g. Φ(eiϑ) with
ϑ ∈ [−π, π], and the dependence upon ϑ will be dropped
if not needed, e.g. Φ instead of Φ(eiϑ). Lm×m
denotes
the space of Cm×m-valued functions deﬁned on the unit
circle which are square integrable. Given Φ ∈ Lm×m
,
the shorthand notation (cid:82) Φ denotes the integration of
Φ taking place on the unit circle with respect to the
normalized Lebesgue measure. Then, the inner prod-
is (cid:104)Φ, Σ(cid:105) = tr (cid:82) ΦΣ∗. Given an analytic
uct in Lm×m

h=0 Xhh, Dk(X) = 2 (cid:80)n−k

2

2

2

2

function Λ ∈ Lm×m
, its (normal) rank is denoted by
rank(Λ). If Φ(eiϑ) is positive deﬁnite (semi-deﬁnite) for
each ϑ ∈ [−π, π], we will write Φ (cid:31) 0 (Φ (cid:23) 0). We deﬁne
the following family of matrix pseudo-polynomials

Qm,n = {∆X∆∗ s.t. X ∈ Qm(n+1)}

(1)

where ∆(eiϑ) := [ Im eiϑIm . . . einϑIm ] is the shift op-
erator. Given a m-dimensional stochastic process y =
{ y(t), t ∈ Z }, yj denotes the j-th component (i.e. vari-
able) of y. With some abuse of notation, y(t) will both
denote a random vector and its sample value. Given a
function f (u, v), ∇uf (¯u, ¯v) and ∇vf (¯u, ¯v) denote the
gradient of f with respect to u and v, respectively, com-
puted at (¯u, ¯v).

2

Identiﬁcation of Sparse AR Graphical Models

Assume to collect the data yN := {y(1), y(2) . . . y(N )}
generated by the AR Gaussian discrete-time zero mean
full rank stationary stochastic process y = { y(t), t ∈ Z}
deﬁned as

y(t) = −

n
(cid:88)

k=1

Aky(t − k) + e(t).

(2)

y(t) takes values in Rm, Ak ∈ Rm×m and e(t) is white
Gaussian noise with covariance matrix R (cid:31) 0. Both Ak
and R are unknown. The order of the AR process, i.e. n,
is assumed to be known. We want to estimate Ak and R
using yN . It is well known that an equivalent description
of y is given by its PSD

Φ(eiϑ) =

(cid:88)

k∈Z

e−ikϑRk, ϑ ∈ [−π, π]

(3)

where Rk = E[y(t + k)y(t)T ], with k ∈ Z, is the co-
variance lags sequence. Accordingly, the aforementioned
problem is equivalent to estimate Φ from yN . There are
situations (e.g. m is large) in which one is interested
to estimate a PSD reﬂecting only the most important
conditional dependence relations among the variables of
y. Let I ⊂ V be an arbitrary index set. We denote as
χI = span{ yj(t) s.t. j ∈ I , t ∈ Z } the closure of the
vector space of all ﬁnite linear combinations (with real
coeﬃcients) of yj(t) with j ∈ I and t ∈ Z. Let j (cid:54)= h,
we say that yj and yh are conditionally independent
given the other variables if χ{j} ⊥ χ{h} | χV\{j,h}. These
conditional dependence relations deﬁne an interaction
graph G(V, E) where V and E denote the set of nodes and
edges, respectively. More precisely, the nodes represent
the variables y1, y2 . . . ym and the lack of an edge means
conditional independence (Brillinger, 1996):

(j, h) /∈ E ⇐⇒ χ{j} ⊥ χ{h} | χV\{j,h}, j (cid:54)= h.

(4)

2

1. Left. Example

Fig.
for
of
y = [ y1 y2 y3 y4 ]T . Right. Example of a latent-variable
graphical model: y1 . . . y6 are the manifest variables x1, x2
are the latent variables.

graphical model

a

An example of graphical model is provided in Figure 1
(left). Dahlhaus (2000) proved that yj and yh are con-
ditionally independent if and only if (Φ−1(eiϑ))jh = 0,
∀ ϑ ∈ [−π, π]. This characterization allows to infer con-
ditional independence relations by promoting sparsity in
the estimation of the inverse PSD for model (2). Since y
is an AR process of order n, we can parametrize its PSD
as Φ = Σ−1 where Σ = S0 + 1
k eikϑ ∈
2
Qm,n and we deﬁne S = [ S0 S1 . . . Sn ] ∈ Mm,n. Then,
a regularized maximum likelihood (ML) estimator of Σ,
and thus of Φ, is given by solving (Songsiri & Vanden-
berghe, 2010):

k=1 Ske−ikϑ +ST

(cid:80)n

(cid:96)(yN ; Σ) + γ¯h∞(Σ)

ˆΣ =argmin
Σ∈Qm,n
s.t. Σ (cid:31) 0.

(5)

The term (cid:96)(yN ; Σ) is an approximation of the negative
log-likelihood of y(n + 1) . . . y(N ) given y(1) . . . y(n) un-
der model (2):

(cid:96)(yN ; Σ) = −

(cid:90)

N − n
2

log |Σ| +

N − n
2

(cid:10)ΦyN , Σ(cid:11) + c
(6)

where

ΦyN = ˆR0 +

1
2

n
(cid:88)

k=1

ˆRke−ikϑ + ˆRT

k eikϑ

ˆR = [ ˆR0 . . . ˆRn ], ˆRk =

1
N − n

N −k
(cid:88)

t=1

y(t + k)y(t)T (7)

and c is a term not depending on yN and Σ. It is worth
noting that ˆRk represents an estimate of Rk from data
yN , indeed ΦyN is the truncated periodogram of Φ com-
puted from yN . Under the assumption that y is a full
rank process, then for N suﬃciently large we have that
T( ˆR) (cid:31) 0 with high probability. Accordingly, through-
out the paper we make the assumtion that T( ˆR) (cid:31) 0.

The penalty term ¯h∞ : Qm,n −→ R

¯h∞(Σ) =

(cid:88)

j>h

qjh(Σ)

(8)

with

qjh(Σ) := max{|(S0)jh|, max
k=1...n

|(Sk)jh|, max
k=1...n

|(Sk)hj|}

encourages a common sparsity pattern (i.e. group spar-
sity) on the coeﬃcients Sk of Σ. Finally, γ > 0 is the
regularization parameter. Since Σ ∈ Qm,n and in view
of (1), Problem (5) can be rewritten in terms of a matrix
X ∈ Qm(n+1) (Songsiri & Vandenberghe, 2010):

ˆX = argmin

(cid:96)(yN ; X) + γh∞(D(X))

X∈Qm(n+1)
s.t. X00 (cid:31) 0, X (cid:23) 0

(9)

2

2

log |X00|+ N −n

tr(T( ˆR)X)+c;
where (cid:96)(yN ; X) = − N −n
function h∞ : Mm,n −→ R is deﬁned as (8) but now the
domain is replaced by Mm,n. Finally, the optimal solu-
tion ˆΦ = ˆΣ−1 is such that ˆΣ = ∆ ˆX∆∗. It is clear that
ˆΦ−1 depends on γ. Songsiri & Vandenberghe (2010) pro-
posed to select γ by computing j values of γ according to
the so called “trade-oﬀ” curve. Then the corresponding
candidate models are ranked by using a BIC criterium.
It is worth noting that the latter applies a thresholding
on the partial coherence of the estimated PSD in order
to measure the complexity (in terms of number of edges)
of the candidate model.

3 Iterative Reweighted Method

In this section we investigate the possibility to mod-
ify the penalty term ¯h∞ in (5) in such a way to im-
prove the ability to estimate the support of Φ−1. No-
tice that h∞ can be understood as the (cid:96)1 norm of the
vector [ q11(Σ) q12(Σ) . . . qnn(Σ) ]T which represents the
convex surrogate of the corresponding (cid:96)0 norm. As high-
lighted in Cand`es et al. (2008): “A key diﬀerence be-
tween the (cid:96)0 and the (cid:96)1 norms is the dependence on mag-
nitude: larger coeﬃcients are penalized more heavily in
the (cid:96)1 norm than smaller coeﬃcients, unlike the more
democratic penalization of the (cid:96)0 norm”. We address
this imbalance by considering a weighted penalty func-
tion ¯hW (Σ) = (cid:80)
j≥h γjhqjh(Σ) where Σ ∈ Qm,n and
γjh > 0. Note that, we penalize also the entries in the
main diagonal of Σ. The latter regularization is not im-
posed in order to obtain sparsity in the main diagonal of
Σ, otherwise constraint Σ (cid:31) 0 is no longer satisﬁed, but
rather it is imposed in order to reduce the variance of
the estimator for the variables in the main diagonal of Σ.
Accordingly, we will expect to ﬁnd γjj > 0 but smaller

3

y1y2y3y41y1y2y3y4y5y6x1x21than γjh with j (cid:54)= h. Thus, we modify Problem (5) as

(cid:96)(yN ; Σ) + ¯hW (Σ)

ˆΣ =argmin
Σ∈Qm,n
s.t. Σ (cid:31) 0.

(10)

Following the same reasoning in Songsiri & Vanden-
berghe (2010) we can reformulate Problem (10) as fol-
lows:

ˆX = argmin

(cid:96)(yN ; X) + ¯hW (D(X))

X∈Qm(n+1)
s.t. X00 (cid:31) 0, X (cid:23) 0

(11)

where hW : Mm,n → R is deﬁned as ¯hW but now the
domain is Mm,n. It is worth noting that (10) and (11)
are equivalent provided that the optimal solution ˆX is
such that ∆ ˆX∆∗ (cid:31) 0.

Theorem 1 The dual problem of (11) is

log |W | + m

max
Z∈Mm,n
W ∈Qm
s.t. W (cid:31) 0

T( ˆR) + T(Z) (cid:23)

(cid:34)

(cid:35)

W 0

0 0

n
(cid:88)

k=0
n
(cid:88)

k=0

|(Zk)jh| + |(Zk)hj| ≤

2γjh
N − n

, j > h

|(Zk)jj| ≤

2γjj
N − n

, j = h.

(12)

The latter does admit a unique solution ˆX such that
∆ ˆX∆∗ (cid:31) 0. Accordingly (10) and (11) are equivalent.

Theorem 1 is important not only for establishing the
existence of the solution of (10) and (11), but it provides
also a tractable formulation for the computation of the
solution, see Section 6.1.

It remains to select a suitable set of weights γjh. The
latter should counteract the magnitude imbalance which
characterizes the (cid:96)1-like norm; more precisely, γjh should
be inversely proportional to qjh(Σ). For instance, we
could take γjh = (qjh(Σ)+εS)−1 with εS > 0 suﬃciently
small. However, Σ is unknown. Accordingly, we propose
an iterative reweighted algorithm (Wipf & Nagarajan,
2010) which constructs iteratively a set of weights by us-
ing the information from the current estimate of Σ (i.e.
X), see Algorithm 1 where we recall that Sk = Dk(X).
Parameter εS > 0 in Step 6 ensures that a zero-valued
entry in positions (j, h) and (h, j) does not strictly pro-
hibit a nonnull estimate at the next step. Although Al-
gorithm 1 has been introduced by an heuristic reasoning,

Algorithm 1 Iterative Reweighted Algorithm
1: l = 0
2: Initialize ˆγ(0)
3: repeat
4:

jh with j, h = 1 . . . m

Solve Problem (11) with γjh = ˆγ(l)
the corresponding solution
qjh(D( ˆX (l))) ← max{|(D0( ˆX (l)))jh|,

5:

jh ; let ˆX (l) denote

maxk>0 |(Dk( ˆX (l)))jh|, maxk>0 |(Dk( ˆX (l)))hj|}
jh ← (qjh(D( ˆX (l))) + εS)−1

Update the weights ˆγ(l+1)
l ← l + 1

6:
7:
8: until (cid:107) ˆX (l) − ˆX (l−1)(cid:107) ≥ ε or l ≤ lMAX
9: ˆΦ = (∆ ˆX (l)∆∗)−1

it can be interpreted as the Majorization-Minimization
(MM) algorithm for the problem

argmin
X∈Qm(n+1)

(cid:88)

j≥h

log(qjh(D(X)) + εS) + (cid:96)(yN ; X)

s.t. X00 (cid:31) 0, X (cid:23) 0.

(13)

The latter provides a regularized ML estimator of Σ
through the relation Sk = Dk(X) and Sk are the coeﬃ-
cients of Σ ∈ Qm,n. The log-sum penalty induces spar-
sity on Σ. It is well known that such a penalty outper-
forms the corresponding (cid:96)1-like norm for estimating the
correct sparsity pattern, see Cand`es et al. (2008). On
the other hand, the log-sum penalty is concave making
Problem (13) nonconvex. In order to see the aforemen-
tioned connection, we can rewrite (13) as

log(ujh + εS) + (cid:96)(yN ; X)

(cid:88)

j≥h

argmin
X∈Qm(n+1)
ujh∈R, j≥h
s.t. X00 (cid:31) 0, X (cid:23) 0
ujh ≥ qjh(D(X)).

(14)

Indeed, if ˆX is the optimal solution of (13), then the
optimal solution of (14) is ˆX and ˆujh = qjh(D( ˆX)) with
j ≥ h. Let u ∈ Rm(m+1)/2 be the vector obtained by
stacking ujh with j ≥ h. We deﬁne

f (u, X) =

(cid:88)

j≥h

log(ujh + εS) + (cid:96)(yN ; X).

The latter is majorized by:

g(u, X; ˆu(l), ˆX (l))

:= f (ˆu(l), ˆX (l)) +

= f (ˆu(l), ˆX (l)) +

+ (cid:96)(yN ; X)

∇uf (ˆu(l)), u − ˆu(l)(cid:69)
(cid:68)
ujh − ˆu(l)
jh
ˆu(l)
jh + εS

(cid:88)

j≥h

+ (cid:96)(yN ; X).

More precisely, the ﬁrst term in f , which is a concave
function in u, is majorized by the tangent at ˆu(l). Then,

4

Q+
m,n := {Σ s.t. Σ (cid:31) 0}. This means that the parame-
ters characterizing Σ are random variables with a suit-
able PDF or simply prior. Let p(Σ) be the prior of Σ.
We recall that (Σ)jh is the entry of Σ in position (j, h):

(15)

(Σ(eiϑ))jh = (S0)jh +

1
2

n
(cid:88)

(Sk)jhe−ikϑ + (Sk)hjeikϑ.

k=1

We assume that (Σ)jhs are independent each other, ac-
cordingly

the MM algorithm is

(ˆu(l+1), ˆX (l+1)) =argmin

g(u, X; ˆu(l), ˆX (l))

u,X
s.t. X00 (cid:31) 0, X (cid:23) 0
ujh ≥ qjh(D(X)).

Removing the terms in g not depending on u and X, we
obtain

(u(l+1), X (l+1)) =argmin

u,X

(cid:88)

ujh
ˆu(l)
jh + εS
j≥h
s.t. X00 (cid:31) 0, X (cid:23) 0
ujh ≥ qjh(D(X)).

+ (cid:96)(yN ; X)

Since we have ujh = qjh(X) for the optimal solution of
(16), then the latter problem is equivalent to

(16)

and

pγγγ(Σ) =

(cid:89)

j≥h

pγjh((Σ)jh),

pγjh((Σ)jh) =

e−γjhqjh(Σ)
cjh

ˆX (l+1) =argmin

(cid:88)

qjh(X)
qjh( ˆX (l)) + εS

+ (cid:96)(yN ; X)

X

j≥h
s.t. X00 (cid:31) 0, X (cid:23) 0.

Deﬁning

ˆγ(l+1)
jh

:=

1
qjh(D( ˆX (l))) + εS

(17)

(18)

and substituting it in (17) we obtain (11) where γjh has
been replaced by ˆγ(l+1)
. In other words, (17) is equivalent
jh
to: compute ˆγ(l+1)
γjh = ˆγ(l+1)
iterative reweighted scheme in Algorithm 1.

. The latter procedure coincides with the

as in (18) and then solve (11) with

jh

jh

Proposition 2 Consider the sequence ˆX (l) generated by
Algorithm 1. Then, ˆX (l) converges to the set of stationary
points of Problem (13).

Simulation evidence showed that the proposed iterative
reweighted algorithm does not perform well, that is the
estimated PSD is not close to the actual one, and neither
the sparsity pattern of the inverse. Even if the updating
rule for γjhs seems reasonable, it is not the best choice
that we can apply. In the next section we will see how
to ﬁnd a better update.

4 A Bayesian Perspective

Till now the problem of estimating Σ sparse has been
considered according to the Fisherian perspective that
is Σ is an unknown but ﬁxed function in Qm,n, i.e.
the parameters Sk, k = 0 . . . n, characterizing Σ are
unknown but ﬁxed quantities. In this section we pro-
pose a method which is based on a Bayesian perspec-
tive that is Σ is a stochastic process taking values in

5

where cjh is the normalizing constant and γjh > 0, j ≥ h,
are referred to as hyperparameters. The latter are the
parameters characterizing the prior. Therefore, we have

pγγγ(Σ) =

e−¯hW (Σ)
(cid:81)
j≥h cjh

(19)

where γγγ ∈ Rm(m+1)/2 denotes the hyperparameters
vector containing γjh with j ≥ h. The negative log-
likelihood of yN and Σ takes the form:

(cid:96)(yN , Σ; γγγ) = − log pγγγ(yN , Σ)

= − log p(yN |Σ) − log pγγγ(Σ).

(20)

It is clear that the negative log-conditional PDF
− log p(yN |Σ) does coincide with (6), thus

(cid:96)(yN , Σ; γγγ) = (cid:96)(yN ; Σ) + ¯hW (Σ) +

(cid:88)

j≥h

log cjh

(21)

where the last term does not depend on Σ. We conclude
that the MAP estimator of Σ is given by (10). This re-
sult is not surprising, indeed it is well known that MAP
estimators in a Bayesian perspective can be interpreted
as regularized estimators in the Fisherian perspective.
The substantial diﬀerence between the two perspectives
is that the former provides the way to estimate the hy-
perparameters vector γγγ. We deﬁne as the negative log-
marginal likelihood:

(cid:96)(yN ; γγγ) := − log

(cid:90)

Q+

m,n

pγγγ(yN , Σ)dΣ.

(22)

An estimate of γγγ is given by the empirical Bayes ap-
proach (Friedman et al., 2001):

ˆγγγ =argmin

γγγ>0

(cid:96)(yN ; γγγ).

(23)

Then, the MAP estimator of Σ is given by (10) with
γγγ = ˆγγγ. However, it is not possible to ﬁnd an analytical
expression for (cid:96)(yN ; γγγ) making challenging the optimiza-
tion of γγγ.

Proposition 3 Consider the prior of Σ deﬁned in (19).
Then, we have

(cid:40)

cjh ≤

υjhγ−(n+1)
,
jh
υjhγ−(2n+1)
jh

if j = h;

,

if j > h

(24)

where the terms υjh in the relation above do not depend
on γγγ.

Accordingly,

(cid:96)(yN , Σ; γγγ) ≤ ˜(cid:96)(yN , Σ; γγγ) := (cid:96)(yN ; Σ) + ¯hW (Σ)

(cid:88)

−

(2n + 1) log γjh −

n
(cid:88)

(n + 1) log γjj + c

(25)

j>h

j=1

where c is a term not depending on yN , Σ and γγγ. An al-
ternative simpliﬁed approach to estimate γγγ is the gener-
alized maximum likelihood (GML) method, Zhou et al.
(1997): instead of maximizing (cid:96)(yN ; γγγ) with respect to
γγγ, the latter is computed with Σ as the pair (ˆγγγ, ˆΣ) that
jointly minimizes ˜(cid:96)(yN , Σ; γγγ). Then, the optimization
can be performed in a two-step algorithm:

ˆΣ(l) = argmin

˜(cid:96)(yN , Σ; ˆγγγ(l))

Σ

ˆγγγ(l+1) = argmin

γγγ

˜(cid:96)(yN , ˆΣ(l); γγγ).

(26)

(27)

Step (26) is the MAP estimator of Σ given the current
choice of γγγ which is equivalent to (10). Step (27) is the
estimator of γγγ using the current MAP estimate of Σ as
a direct observation. Notice that (27) is equivalent to

ˆγ(l+1)
jh =

argmin
γjh>0

argmin
γjh>0






γjhqjh( ˆS(l)) − (n + 1) log γjh,

if j = h;
γjhqjh( ˆS(l)) − (2n + 1) log γjh,

if j > h

where we recall that ˆS(l) = D( ˆX (l)).

Proposition 4 Under the assumption that qjh( ˆS(l)) >
0, it holds that

ˆγ(l+1)
jh =






n+1
qjh( ˆS(l))
2n+1
qjh( ˆS(l))

,

,

if j = h

if j > h.

(28)

6

To deal also with the case that qjh( ˆS(l)) = 0, we consider
the modiﬁed updating

ˆγ(l+1)
jh =






n+1
qjh( ˆS(l))+εS
2n+1
qjh( ˆS(l))+εS

,

,

if j = h

if j > h

(29)

with εS > 0. It is not diﬃcult to see that the latter modi-
ﬁcation is equivalent to assume that γjhs are modeled as
independent random variables with exponential hyper-
prior: p(γjh) = εSe−εS γjh. We conclude that the GML
approach is equivalent to Algorithm 1 wherein Step 6 is
now replaced by (29). Finally, it is not diﬃcult to see
that the sequence ˆX (l) generated by the GML method
converges to the set of stationary points of the Problem
(13) where the log-sum penalty now is

f (X) := (2n + 1)

(cid:88)

j>h

log(qjh(D(X)) + εS)

+ (n + 1)

m
(cid:88)

j=1

log(qjj(D(X)) + εS).

5 Identiﬁcation of Latent-variable Graphical

Models

Consider a Gaussian discrete-time zero mean full rank
stationary stochastic process z = { z(t), t ∈ Z }. z is
composed by m manifest variables and r latent vari-
ables, so that z = [ (y)T (x)T ]T with y = [ y1 . . . ym ]T
and x = [ x1 . . . xr ]T . We assume that y is an AR pro-
cess of order n (known) and dimension m (known). r as
well as the PSD of z are unknown. We assume to collect
the data yN := { y(1), y(2) . . . y(N ) } from the mani-
fest process y. We want to estimate the PSD of y, say
Φ, in such a way that it corresponds to a latent-variable
graphical model. The latter is an interaction graph with
two layers: latent nodes x1 . . . xr are in the upper level,
manifest nodes y1 . . . ym are in the lower level, r (cid:28) m
and there are few edges among the manifest nodes. An
example of a latent-variable graphical model is provided
in Figure 1 (right). The powerfulness of latent-variable
graphical models is that the introduction of few latent
variables (in respect to the manifest ones) may reduces
drastically the conditional dependences among the man-
ifest variables. Accordingly, in such graphs we expect
that the interdependence relations among the manifest
variables are mainly explained by few and common la-
tent variables. In Zorzi & Sepulchre (2016) it has been
shown that Φ corresponds to a latent-variable graphical
model if it admits the following decomposition

Φ−1 = Σ − Λ

(30)

where

Σ = S0 +

Λ = L0 +

1
2

1
2

n
(cid:88)

k=1
n
(cid:88)

k=1

Ske−iθk + ST

k eiθk

Lke−iθk + LT

k eiθk.

(31)

Σ (cid:31) 0 is sparse and its support reﬂects the conditional
dependence relations among the manifest variables. Λ (cid:23)
0 is low rank and its rank is equal to r, i.e the num-
ber of latent variables. The approximate negative log-
likelihood of y(n + 1) . . . y(N ) given y(1) . . . y(n) under
the aforementioned model is:

(cid:96)(yN ; Σ, Λ) := (cid:96)(yN ; Σ − Λ)

(32)

where (cid:96)(yN ; ·) has been deﬁned as in (6). Accordingly,
the regularized ML estimator of Σ and Λ is (Zorzi &
Sepulchre, 2016):

( ˆΣ, ˆΛ) = argmin
Σ,Λ∈Qm,n
s.t. Σ (cid:31) 0, Λ (cid:23) 0.

(cid:96)(yN ; Σ, Λ) + γS¯h∞(Σ) + γL¯r(cid:63)(Λ)

(33)

Here, ¯r(cid:63)(Λ) = tr (cid:82) Λ is the nuclear norm of Λ ∈ Qm,n
with Λ (cid:23) 0. γS, γL > 0 are the regularization parameters
for Σ and Λ, respectively. The term ¯h∞(Σ), deﬁned in
(8), induces sparsity on Σ. The term ¯r(cid:63)(Λ) induces low
rank on Λ, indeed it has been shown that such a function
is the convex envelop of rank(Λ), see Proposition 3.1 in
Zorzi & Sepulchre (2016). It is worth noting that the
estimate of r is given by the numerical rank of ˆΛ. Since
Σ − Λ and Λ belong to Qm,n and in view of (1), we can
deﬁne X, H ∈ Qm(n+1) such that

Σ − Λ = ∆X∆∗, Λ = ∆H∆∗

(34)

and X, H (cid:23) 0. It is possible to prove that Problem (33)
can be rewritten in terms of X and H as follows:

( ˆX, ˆH) = argmin

X,H∈Qm(n+1)

(cid:96)(yN ; X) + γSh∞(D(X + H))

s.t. X00 (cid:31) 0, X (cid:23) 0, H (cid:23) 0

(35)

+ γL tr(H)

where (cid:96)(yN ; X) does coincide with the ﬁrst term in the
objective function of (9). Also in this case the optimal
solution ( ˆΣ, ˆΛ) depends on the regularization parame-
ters γS, γL. The values (γS, γL) can be selected by con-
sidering a 2-dimensional grid. Then, the candidate mod-
els can be ranked by using a BIC criterium similar to
the one in Songsiri & Vandenberghe (2010). The latter
measures the complexity by thresholding the partial co-
herence of the sparse component and the singular val-
ues of the low rank component. Alternatively, Zorzi &

7

Sepulchre (2016) proposes a score function based on the
Kullback-Leibler divergence to rank the candidate mod-
els.

Similarly to the case without latent variables, the weak-
nesses of the regularization Problem (33) are that: (i)
the nonnull entries in Σ are penalized in a diﬀerent way;
(ii) the nonnull eigenvalues of Λ, understood as func-
tions over the unit circle, are penalized in a diﬀerent way.
The sparse regularizer can be made “democratic” in the
same way of before. Regarding the low rank part, we can
replace ¯r(cid:63)(Λ) with the “weighted” penalty:

(cid:18)

(cid:90)

(cid:19)

¯rW (Λ) = tr

Q

Λ

where the weight matrix Q ∈ Qm is such that Q (cid:31) 0.
Thus, we consider the problem:

( ˆΣ, ˆΛ) = argmin
Σ,Λ∈Qm,n
s.t. Σ (cid:31) 0, Λ (cid:23) 0.

(cid:96)(yN ; Σ, Λ) + ¯hW (Σ) + ¯rW (Λ)

(36)

Using the parametrization in (34), we have that

(cid:18)

(cid:90)

rW (Λ) = tr

Q

∆H∆∗

(cid:19)

(cid:18)(cid:90)

= tr

(cid:19)

∆∗Q∆H

= tr((In+1 ⊗ Q)H)

(37)

where we exploited the well known identity (cid:82) eikθ = δk
and δk denotes the Kronecker delta function. Thus, we
consider the problem:

( ˆX, ˆH) = argmin

X,H∈Qm(n+1)

(cid:96)(yN ; X) + hW (D(X + H))

s.t. X00 (cid:31) 0, X (cid:23) 0, H (cid:23) 0.

+ tr((I ⊗ Q)H)
(38)

Note that, (36) and (38) are equivalent provided that
∆ ˆX∆∗ (cid:31) 0.

Theorem 5 The dual problem of (38) is

log |W | + m

max
Z∈Mm,n
W ∈Qm
s.t. W (cid:31) 0

T( ˆR) + T(Z) (cid:23)

(cid:34)

(cid:35)

W 0

0 0

n
(cid:88)

k=0
n
(cid:88)

|(Zk)jh| + |(Zk)hj| ≤

2γjh
N − n

, j > h

|(Zk)jj| ≤

2γjj
N − n

k=0
I ⊗ Q + T(Z) (cid:23) 0.

, j = h

(39)

The latter does admit solution ( ˆX, ˆH) such that ˆX is
unique and ∆ ˆX∆∗ (cid:31) 0. Accordingly (36) and (38) are
equivalent.

Theorem 5 is important not only for establishing the
existence of the solution of (36) and (38), but it pro-
vides also a tractable formulation for the computation
of the solution, see Section 6.1. It is worth noting that
Problem (36) may have more than one solution. On the
other hand, if we compute an optimal solution of (39),
then from the latter we can recover the solution of (38)
by solving a system of linear equations in H (a similar
idea has been used in Section III.C in Zorzi & Sepulchre
(2016)). The uniqueness of the solution of this system of
linear equations is guaranteed provided that: (i) Q has
a suﬃcient number of eigenvalues which are suﬃciently
large; (ii) there is a suﬃcient number of γjhs with j (cid:54)= h
taking suﬃciently large values. The latter implies the
uniqueness of the solution to (38) and thus the unique-
ness of the one to (36).

m,n and in the closure of Q+

To select a suitable set of γjh and a suitable Q, we de-
sign a reweighted algorithm which exploits a Bayesian
perspective. We model Σ and Λ as stochastic processes
taking values in Q+
m,n, re-
spectively. Let p(Σ) and p(Λ) denote the prior of Σ and
Λ, respectively. We assume that Σ and Λ are indepen-
dent, that is the joint PDF of Σ and Λ is such that
p(Σ, Λ) = p(Σ)p(Λ). We set the prior of Σ as in (19).
Regarding Λ, Lks are modeled as independent random
matrices. More precisely, we model L0 as a Wishart ran-
dom matrix with m + 1 degrees of freedom and variance
mQ−1:

pQ(L0) =

(cid:112)

e− tr(QL0)
2(m+1)m|Q|−(m+1)Gm( m+1
2 )

(40)

where Q ∈ Qm such that Q (cid:31) 0 and Gm is the mul-
tivariate gamma function. Finally, we attach an unin-
formative prior on Lk with k ≥ 1. Then, the negative
log-likelihood of yN , Σ and Λ is deﬁned as

(cid:96)(yN , Σ, Λ; γγγ, Q) = − log pγγγ,Q(yN , Σ, Λ)

= − log p(yN |Σ, Λ) − log pγγγ(Σ) − log pQ(Λ) + c

where c is a constant term not depending on yN , Σ, Λ,
γγγ and Q. Note that, − log p(yN |Σ, Λ) does coincide with
(cid:96)(yN ; Σ, Λ) deﬁned in (32). Also in this case, it is not
possible to ﬁnd an analytical expression for the negative
log-marginal likelihood. Therefore, we have the following
upper bound for (cid:96)(yN , Σ, Λ; γγγ, Q):

˜(cid:96)(yN , Σ, Λ; γγγ, Q) = (cid:96)(yN ; Σ, Λ) + ¯hW (Σ) +

+ ¯rW (Λ) −

m + 1
2

log |Q| + c.

(cid:88)

j≥h

log cjh

(41)

8

Then, according to the GML approach we consider the
following two-step algorithm:

( ˆΣ(l), ˆΛ(l)) = argmin

˜(cid:96)(yN , Σ, Λ; ˆγγγ(l), ˆQ(l))

(42)

Σ,Λ

(ˆγγγ(l+1), ˆQ(l+1)) = argmin

˜(cid:96)(yN , ˆΣ(l), ˆΛ(l); γγγ, Q).

(43)

γγγ,Q

Clearly, (42) is the MAP estimator of Σ and Λ given the
current choice of γγγ and Q, while (43) is the estimator of
γγγ and Q using the current MAP estimate of Σ and Λ as
a direct observation. The objective function in (43) can
be split in two terms depending on γγγ and Q, respectively.
Accordingly, ˆγγγ(l+1) takes a form similar to the one in
(28). Regarding Q, we have:

ˆQ(l+1) = argmin

Q(cid:31)0

tr(Q ˆL(l)

0 ) −

m + 1
2

log |Q|.

(44)

Proposition 6 Under the assumption that ˆL(l)
have that

0 (cid:31) 0, we

ˆQ(l+1) =

m + 1
2

( ˆL(l)

0 )−1.

(45)

To deal also with the case ˆL(l)
0
sider the modiﬁed updating rule:

singular matrix we con-

ˆQ(l+1) =

m + 1
2

( ˆL(l)

0 + εLI)−1

(46)

(cid:113)

where εL > 0. It is not diﬃcult to see that the lat-
ter modiﬁcation is equivalent to assume that Q is
modeled as a Wishart random matrix with m + 1
degrees of freedom and variance mε−1
L I, i.e. p(Q) =
e−εL tr(Q)/(
Gm((m + 1)/2)), which
is assumed to be independent of γγγ. Algorithm 2 de-
scribes the corresponding procedure which is clearly
an iterative reweighted algorithm. Here, we recall that
ˆS(l) = D( ˆX (l) + ˆH (l)) and ˆL(l)

2(m+1)mε−m(m+1)
L

0 = D0( ˆH (l)).

It is not diﬃcult to see that Algorithm 2 can be inter-
preted as the MM algorithm for solving the regularized
ML problem:

f (X, H)

argmin
X,H∈Qm(n+1)
s.t. X00 (cid:31) 0, X (cid:23) 0, H (cid:23) 0

(47)

Algorithm 2 Iterative Reweighted Algorithm
1: l = 0
2: Initialize ˆγ(0)
3: repeat
4:

jh with j, h = 1 . . . m and ˆQ(0)

Solve Problem (38) with γγγ = ˆγγγ(l) and Q = ˆQ(l); let
( ˆX (l), ˆH (l)) denote the corresponding solution
Update the weights

5:

ˆγ(l+1)
jh =






n+1
qjh(D( ˆX(l)+ ˆH(l)))+εS
2n+1
qjh(D( ˆX(l)+ ˆH(l)))+εS

,

,

if j = h

if j > h

ˆQ(l+1) =

m + 1
2

(D0( ˆH (l)) + εLI)−1

l ← l + 1

6:
7: until (cid:107) ˆX (l) − ˆX (l−1)(cid:107)+(cid:107) ˆH (l) − ˆH (l−1)(cid:107) ≥ ε or l ≤ lMAX
8: ˆΣ = ∆( ˆX (l) + ˆH (l))∆∗, ˆΛ = ∆ ˆH (l)∆∗
9: ˆΦ = ( ˆΣ − ˆΛ)−1

where

f (X, H) := (2n + 1)

(cid:88)

j>h

log(qjh(D(X + H)) + εS)

+ (n + 1)

m
(cid:88)

j=1

log(qjj(D(X + H)) + εS)

+

m + 1
2

log |D0(H) + εLI| + (cid:96)(yN ; X).

Beside the sparse-inducing part (already analyzed in
Section 4), it is known that the log-det penalty outper-
forms the nuclear norm for estimating the correct rank,
see Fazel et al. (2003).

Corollary 7 Consider the sequence ( ˆX (l), ˆH (l)) gener-
ated by Algorithm 2 and assume that the solution of
(38), with weights ˆγγγ(l) and ˆQ(l), is always unique. Then,
( ˆX (l), ˆH (l)) converges to the set of stationary points of
Problem (47).

6 Simulation Results

In this section we test the performance of the proposed
empirical Bayes estimators for learning AR graphical
models.

6.1 Implementation details

The empirical Bayes estimate of a sparse AR graphi-
cal model is obtained by running Algorithm 1 wherein
Step 6 is replaced by the updating rule (29). The most
challenging part is the computation of ˆX (l) (Step 4)
which is obtained through the dual problem (12). The
optimal solution of the latter is computed by imple-
menting a ﬁrst-order projection algorithm. The empir-
ical Bayes estimate of a latent-variable AR graphical

Fig. 2. First experiment with sparse AR models of order
n = 1.

Fig. 3. Second experiment with sparse AR models of order
n = 2.

Fig. 4. First experiment with latent-variable AR models of
order n = 2 with s = 0.1 and r = 2.

model is obtained by running Algorithm 2. Here, the
most challenging part is the computation of ( ˆX (l), ˆH (l)),
in Step 4, which is obtained through the dual problem
(39). The optimal solution of the latter is computed
by using the alternating direction method of multipli-
ers (ADMM). More precisely, we introduce an auxiliary
variable K = I ⊗Q+T(Z). In the ﬁrst step, we minimize
the augmented Lagrangian with respect to W and Z un-
der the ﬁrst four constraints of (39). This minimization is

9

TD9TD17RW0.020.040.060.080.1N=500eTD9TD17RW0.010.020.030.040.05N=1000e+1+1+1TD9TD17RW0.050.10.150.20.250.3N=500eSPTD9TD17RW00.050.10.150.2N=1000eSP+10.050.10.150.2TD9TD17RWeN=5000.040.060.080.1TD9TD17RWeN=10000.10.20.30.4TD9TD17RWeSPN=50000.050.10.150.20.250.3TD9TD17RWeSPN=1000+10.050.10.150.2CV−KLCV−BICRWe0.050.10.150.20.25CV−KLCV−BICRWeSL• we compute the fraction of null and nonnull mis-
placed entries in the inverse of the estimated PSD
with respect to the inverse of the true PSD as
eSP = # ˆEm/(m(m − 1)/2) where # ˆEm denotes the
total number of null and nonnull misplaced entries in
the inverse of the estimated PSD.

In the ﬁrst experiment, the AR models are of order n = 1
and we have considered two diﬀerent lengths of the data,
that is N = 500 and N = 1000. The boxplots of e and
eSP are depicted in Figure 2. As we can see, RW outper-
forms TD9 and TD17 both in terms of estimation error
and of misplaces entries. It is worth noting that TD9 and
TD17 perform in the same way: this conﬁrms the ob-
servation that for tracing accurately the trade-oﬀ curve
it is required just a small number of points (Songsiri &
Vandenberghe, 2010). We have noticed that TD9 and
TD17 estimate the inverse PSD too sparse in respect to
the true one. For this reason, we also have considered
TD9 and TD17 with the threshold for the partial coher-
ence equal to 0.05: the resulting performances are worse
than the ones with 0.1. In the second experiment, the
AR models are of order n = 2 and we have considered
the two diﬀerent lengths of the data as before. The box-
plots of e and eSP are depicted in Figure 3. The results
are similar to the ones of the previous experiment: RW
outperforms the other two estimators; TD9 and TD17
perform in the same way.

6.3 Identiﬁcation of latent-variable AR graphical mod-

els

We consider two Monte Carlo experiments which are
structured as follows. We generate 200 AR models of
dimension m = 30 of order n = 2 whose inverse PSD has
a sparse plus low rank decomposition, i.e. Φ−1 = Σ − Λ
with Σ sparse and Λ low rank. The fraction of nonnull
entries in the sparse part is denoted by s; the position of
the nonnull entries is chosen randomly for each model.
The rank of Λ is denoted by r; for each model, the image
of Λ is chosen randomly in such a way that Σ − Λ is
positive deﬁnite over the unit circle. For each model we
generate a ﬁnite data sequence of length N = 1000 and
we consider the following estimators:

• TD-KL is the estimator proposed in Zorzi & Sepul-
chre (2016) where the model is chosen using the score
function based on the Kullback-Leibler divergence;
the threshold for the partial coherence (sparse com-
ponent) is set equal to 0.1; the threshold for the nor-
malized singular values (low rank component) is set
equal to 0.1; the regularization grid (16 points) is
chosen in such a way that the candidate model, say
ˆΦ = ( ˆS − ˆL)−1, ranges from ˆS diagonal - rank( ˆL) large
to ˆS moderately sparse and rank( ˆL) = 0.

• TD-BIC is the estimator proposed in Zorzi & Sepul-
chre (2016) where the model is chosen using the BIC

Fig. 5. Second experiment with latent-variable AR models
of order n = 2 with s = 0.1 and r = 5.

performed by using a ﬁrst-order projection algorithm. In
the second step, we minimize the augmented Lagrangian
with respect to K under the last constraint of (39). This
optimal solution can be computed in closed form. Fi-
nally, the corresponding Matlab functions are available
at https://github.com/MattiaZ85/EBL-AR-GM.

6.2 Identiﬁcation of sparse graphical models

We consider two Monte Carlo experiments which are
structured as follows. We generate 200 AR models of di-
mension m = 30, of order n and whose fraction of non-
null entries in the inverse PSD is equal to 0.1. The posi-
tion of such nonnull entries is chosen randomly for each
model. For each model we generate a ﬁnite data sequence
of length N and we consider the following estimators:

• TD9 is the estimator proposed in Songsiri & Vanden-
berghe (2010) where the number of points for tracing
the trade-oﬀ curve is set equal to 9 and the threshold
for the partial coherence is set equal to 0.1;

• TD17 is the estimator proposed in Songsiri & Vanden-
berghe (2010) where the number of points for tracing
the trade-oﬀ curve is set equal to 17 and the threshold
for the partial coherence is set equal to 0.1;

• RW is the empirical Bayes estimator that we have
proposed in Section 4 where εS = 10−3. The hy-
perparameters vector γγγ is initialized with (29) where
ˆS(l) = D(XB), XB ∈ Qm(n+1) is such that ˆΦB =
(∆XB∆∗)−1 and ˆΦB is the Burg estimator computed
from the data 1 . The threshold for the stopping condi-
tion is set equal to ε = 10−4 and the maximum num-
ber of iterations is set equal to lMAX = 50.

For each estimator:

• we compute the relative error in the estimation of
the inverse PSD coeﬃcients as e = (cid:107) ˆS − S(cid:107)2/(cid:107)S(cid:107)2
where ˆS = [ ˆS0 ˆS1 . . . ˆSn] ∈ Mm,n are the coeﬃ-
cients of the inverse of ˆΦ (estimated PSD), while S =
[ S0 S1 . . . Sn] ∈ Mm,n are the ones of the inverse of
Φ (true PSD);

1 It is worth recalling that the Burg method provides an
estimate of the PSD which corresponds to an AR model of
a certain order, ﬁxed by the user. In our case, the latter is
ﬁxed equal to n; in this way ˆΦB = (∆XB∆∗)−1.

10

0.050.10.150.20.25CV−KLCV−BICRWe0.050.10.150.20.250.30.35CV−KLCV−BICRWeSL+3+31st exp.

¯C

2nd exp.

¯C

TRUE

0.1890

TRUE

0.2684

TD-KL

0.0902

TD-KL

0.1029

TD-BIC 0.1012

TD-BIC 0.1158

RW

0.2100

RW

0.2810

Fig. 6. Average complexity of the estimated models in the
ﬁrst Monte Carlo experiment where s = 0.1, r = 2 (left table)
and in the second Monte Carlo experiment where s = 0.1,
r = 5 (right table).

score function; the threshold parameters and the reg-
ularization grid are chosen as in the previous estima-
tor.

• RW is the empirical Bayes estimator that we have
proposed in Section 5 where εS = 10−3 and εL =
10−3. The hyperparameters are initialized as follows.
Let ˆΦB = (∆XB∆∗)−1 be the Burg estimator of the
PSD computed from the data. Then, the initial value
for γγγ is given by (29) where ˆS(l) = (1 + α)D(XB),
the initial value for Q is given by (46) where ˆL(l) =
αD0(XB) and α = 0.1. The threshold for the stopping
condition is set equal to ε = 10−4 and the maximum
number of iterations is set equal to lMAX = 50.

For each estimator:

• we compute the relative error in the estimation of the
inverse PSD coeﬃcients as e = (cid:107) ˆS− ˆL−(S−L)(cid:107)2/(cid:107)S−
L(cid:107)2 where ˆS = [ ˆS0 ˆS1 . . . ˆSn] ∈ Mm,n are the coeﬃ-
cients of ˆΣ, while ˆL = [ ˆL0 ˆL1 . . . ˆLn] ∈ Mm,n are the
ones of ˆΛ. ˆΦ = ( ˆΣ − ˆΛ)−1 denotes the estimated PSD.
In a similar way, S and L contain the coeﬃcients of Σ
and Λ such that Φ = (Σ − Λ)−1 is the true PSD.
• we compute the relative error in the estimation of the
sparse and low rank coeﬃcients in the inverse PSD as
eSL = ((cid:107) ˆS − S(cid:107)2 + (cid:107) ˆL − L(cid:107)2)/(cid:107)S − L(cid:107)2;

• we compute the complexity of the estimated model as
C = #p/(m(m+1)/2+m2n) where #p is the number
of parameters needed to characterize the model ˆΦ =
( ˆΣ − ˆΛ)−1. For instance if ˆΣ has s nonnull entries and
rank(Λ) = r, then #p = (s + m)/2 + sn + rm(n + 1).
The denominator in C is the number of coeﬃcients
needed in an unstructured model, i.e. the correspond-
ing interaction graph is complete.

In the ﬁrst experiment, the AR models are with s = 0.1
and r = 2. The relative errors e and eSL are depicted in
Figure 4. As we can see, RW provides the best perfor-
mance while TD-KL the worst performance. The aver-
age complexity of the estimated models for each estima-
tor (and denoted by ¯C) is shown in the table on the left
of Figure 6. As we can see, the average complexity of the
models estimated by RW is closer to the true one than

the one of TD-KL and TD-BIC. More precisely, TD-KL
and TD-BIC estimate models which are extremely sim-
ple, in respect to the true complexity, and the price to
pay is the inferiority in respect to e and eSL. In the sec-
ond experiment, the AR models are with s = 0.1 and
r = 5. The relative errors e and eSL are depicted in
Figure 5, while the average complexity of the estimated
models for each estimator is shown in the table on the
right of Figure 6. As we can see, the conclusions of the
previous case still hold. Finally, we have tested RW for
diﬀerent values of α such that 0.08 ≤ α ≤ 0.15. We have
noticed that RW provides similar performances of be-
fore.

7 Conclusion

We have analyzed the problem of estimating sparse and
latent-variable AR graphical models. These two prob-
lems are traditionally solved by using (cid:96)1-like and nuclear
norm-like regularizes. The latter introduce a magnitude
imbalance in the optimization problem which produces
an estimator with a remarkable mean squared error. We
have proposed two empirical Bayes estimators which
counteract such an imbalance. The hyperparameters of
these estimators are computed by the generalized max-
imum likelihood approach which leads to an iterative
reweighted method. Simulation results showed the ben-
eﬁt to introduce a prior for the model that we have to
estimate.

Appendix

Proof of Theorem 1

First, we can rewrite (11) by adding a new variable Y ∈
Mm,n:

(cid:96)(yN ; X) +

2
N − n

argmin
X∈Qm(n+1)
Y ∈Mm,n
s.t. X00 (cid:31) 0, X (cid:23) 0
Y = D(X).

2
N − n

hW (Y )

(48)

The Lagrangian is

L(X, Y, U, Z) = − log |X00| +

(cid:68)
T( ˆR), X

(cid:69)

+

2
N − n

¯hW (Y ) − (cid:104)U, X(cid:105) + (cid:104)Z, D(X) − Y (cid:105)

where U ∈ Qm(n+1) is such that U (cid:23) 0 and Z ∈ Mm,n.
We proceed to perform the unconstrained minimization
of L with respect to the primal variables. It is not diﬃcult

11

to see that:

2
N − n
2γjj
N − n

min
Y

L(X, Y, U, Z) ≥
(cid:32)

m
(cid:88)

qjj(Y )

+ min
Y

+

(cid:88)

j>h

j=1

(cid:32)

qjh(Y )

2γjh
N − n

−

n
(cid:88)

k=0

(cid:96)(yN ; X) + (cid:104)T(Z) − U, X(cid:105)

(cid:33)

|(Zk)jj|

−

n
(cid:88)

k=0

(cid:33)

|(Zk)jh| + |(Zk)hj|

.

Accordingly, the latter admits minimum if and only if

n
(cid:88)

k=0
n
(cid:88)

k=0

|(Zk)jh| + |(Zk)hj| ≤

2γjh
N − n

, j > h

|(Zk)jj| ≤

2γjj
N − n

, j = h;

(49)

moreover, in such a situation it takes value equal to zero.
Thus,

L(X, Y, U, Z) = − log |X00| + (cid:104)T( ˆR) + T(Z) − U, X(cid:105)

min
Y

if (49) holds, otherwise it takes −∞. Regarding the min-
imization of L with respect to X: the minimum with re-
spect to X00 is X00 = (T( ˆR) + T(Z) − U )−1
00 ; regarding
the other blocks Xjh, it is not diﬃcult to see that L is
bounded below if and only if

(T( ˆR) + T(Z) − U )jh = 0, (j, h) (cid:54)= (0, 0).

(50)

We conclude that:

L(X, Y, U, Z) = log |(T( ˆR) + T(Z) − U )00| + m

min
X,Y

if (49)-(50) hold, otherwise it takes −∞. Substituting U
with the new variable W := (T( ˆR)+T(Z)−U )00, we ob-
tain (12). Regarding the existence of the solution, notice
that Problem (12) consists in maximizing log |W | sub-
(cid:35)

(cid:34)

ject to the constraints W (cid:31) 0, T( ˆR) + T(Z) (cid:23)

W 0

0 0

,

and Z ∈ C where C := { Z ∈ Mm,n s.t. (49) holds }
which is a closed convex subset of {Z ∈ Mm,n s.t. tr(Z0)
bounded}. Then, the existence of the solution is guar-
anteed by applying a modiﬁed version of Lemma A.1 in
Zorzi & Sepulchre (2016). More precisely, it is not dif-
ﬁcult to see that the conclusion of such Lemma does
not change by replacing the condition tr(Z0) = 0 with
tr(Z0) bounded. The remaining part of the proof follows
the same lines of the one of Proposition 3.3 in Zorzi &
Sepulchre (2016). (cid:50)

Proof of Proposition 2 and Corollary 7

We need the following result.

12

Lemma 8 (Razaviyayn et al., 2013, Corollary 1) Con-
sider the problem

u =argmin

f (u)

u∈C

(51)

which is assumed to be feasible, i.e. there exists ¯u ∈ C
such that f (¯u) takes ﬁnite value, and C is a convex set. Let
M denote the set of stationary points for (51). Consider
the corresponding MM algorithm

u(l+1) =argmin

g(u; u(l))

u∈C

where g(u; v) is such that g(u; u(l)) ≥ f (u(l)), g(u(l); u(l)) =
f (u(l)), ∇ug(u; u) = ∇uf (u) and g(u; v) is continuous
in (u, v). If the level set S := {u ∈ C s.t. f (u) ≤ f (¯u)}
is compact, then the sequence u(l) converges to the set
M, i.e. liml→∞ d(x(l), M) = 0 where d(x(l), M) :=
inf s∈M (cid:107)x − s(cid:107).

We proceed to prove Proposition 2 by applying Lemma
8 to Problem (14) and its MM algorithm (15). It is not
diﬃcult to see all the hypotheses on the approximating
function g are satisﬁed. It remains to prove the feasibil-
ity and compactness of the level set. Notice that (14)
and (13) are equivalent, thus we can consider Problem
(13). In our case, C = { X s.t. X00 (cid:31) 0, X (cid:23) 0 } which
is a convex set, and f (X) = (cid:80)
j≥h log(qjh(D(X)) +
εS) + (cid:96)(yN ; X). Problem (13) is feasible indeed it is suf-
ﬁcient to pick ¯X = I. Consider the level set S := { X ∈
f (X) ≤ f ( ¯X) }. We show that S is closed and
C s.t.
bounded, since it is a subset of a ﬁnite dimensional vec-
tor space, then S is compact. Consider a convergent se-
quence X (l) ∈ C, l ∈ N, such that X (l)
00 converges to a
singular matrix, then − log |X (l)
00 | tends to inﬁnity. Then,
f (X (l)) ≥ m(m + 1)/2 log εS − log |X (l)
00 | → ∞. Accord-
ingly, such a sequence cannot belong to S, and thus S
is a closed set. We proceed to show that unbounded
sequences cannot belong to S, i.e. S is bounded. Con-
sider a convergent sequence X (l) ∈ C, l ∈ N such that
(cid:107)X (l)(cid:107) → ∞. This means that X (l) has at least one
eigenvalues tending to inﬁnity as l → ∞. In f (X (l))
the term tr(T( ˆR)X (l)) → ∞ because T( ˆR) (cid:31) 0. More-
over, it dominates the other two terms. We conclude that
f (X (l)) → ∞, and thus the sequence cannot belong to
S. We conclude that S is bounded, and thus compact.
Thus, we can apply Corollary 1 and hence the claim is
proved. Finally, Corollary 7 can be proved in a similar
way. (cid:50)

Proof of Proposition 3

Before to prove the statement we need the following
Lemma.

Lemma 9 Let γ > 0. Consider the integral:

Hence, by Lemma 9 we have that

I =

(cid:90) ∞

(cid:90) ∞

. . .

0

0

e−γ max{|x1|,...,|xn|}dx1 . . . dxn.

Then, γ−n ≤ I ≤ nnγ−n.

Proof. Notice that:

1
n

n
(cid:88)

k=1

|xk| ≤ max{|x1|, . . . , |xn|} ≤

n
(cid:88)

k=1

|xk|,

accordingly
e−γ (cid:80)n

k=1

|xk| ≤ e−γ max{|x1|,...,|xn|} ≤ e−γ 1

n

(cid:80)n

k=1

|xk|.

Taking the integral with respect to x1, . . . , xk over
[0, ∞) × . . . × [0, ∞) in the above inequalities, we obtain
a lower and an upper bound for I, respectively:

IL =

=

IU =

=

(cid:90) ∞

(cid:90) ∞

. . .

(cid:90) ∞

0
n
(cid:89)

e−γ (cid:80)n

k=1

|xk|dx1 . . . dxn

0

e−γ|xk|dxk =

n
(cid:89)

γ−1 = γ−n

0

k=1
(cid:90) ∞

(cid:90) ∞

. . .

(cid:90) ∞

0
n
(cid:89)

k=1

0

(cid:80)n

k=1

k=1
|xk|dx1 . . . dxn

e−γ 1

n

0

e−γ |xk |

n dxk =

n
(cid:89)

k=1

nγ−1 = nnγ−n

˜cjh ≤ 22n+1(2n + 1)2n+1γ−(2n+1).

(53)

In the case that j = h, we have that (Sk)jh = (Sk)hj for
k ≥ 0, accordingly

(cid:90) ∞

(cid:90) ∞

˜cjh =

. . .

˜qjh(S)d(S0)jh . . . d(Sn)jh

−∞

−∞

and the integration is taken with respect to n + 1 vari-
ables. Along the same reasoning of before we have that

˜cjh ≤ 2n+1(n + 1)n+1γ−(n+1).

(54)

In view of (53) and (54), we get the claim where vjh =
22n+1(2n + 1)2n+1 or vjh = 2n+1(n + 1)n+1. (cid:50)

Proof of Proposition 4

Consider the case j = h, we have to minimize the objec-
tive function fjh(γjh) := γjhqjh( ˆS(l)) − (n + 1) log γjh
under the constraint that γjh > 0. Notice that fjh(γjh)
is strictly convex. Thus, the minimum point is given by
setting equal to zero the ﬁrst derivate: qjh( ˆS(l)) − (n +
jh = 0 which is satisﬁed with γjh = (n+1)/qjh( ˆS(l))
1)γ−1
and the constraint γjh > 0 is satisﬁed. The proof for the
case j > h is similar. (cid:50)

Proof of Theorem 5

which concludes the proof. (cid:50)

The proof is similar to the one of Theorem 1. (cid:50)

We proceed to prove Proposition 3. Since cjh is the nor-
malizing constant of pγjh ((Σ)jh), then we have that

Proof of Proposition 6

cjh =

(cid:90)

Q+

m,n

e−¯hW (Σ)dΣ ≤ ˜cjh

(52)

where

(cid:90)

˜cjh =

e−¯hW (Σ)dΣ

Qm,n

(cid:90) ∞

(cid:90) ∞

=

. . .

˜qjh(S)d(S0)jh . . . d(Sn)jhd(Sn)hj

−∞

−∞

and

˜qjh(S) = e−γjh max{|(S0)jh|,maxk>0 |(Sk)jh|,maxk>0 |(Sk)hj |}.

In the case that j (cid:54)= h, ˜cjh is an integral taken with
respect to 2n+1 variables. Since ˜qjh(S) = ˜qjh(−S), then

˜cjh = 22n+1

(cid:90) ∞

(cid:90) ∞

. . .

˜qjh(S)d(S0)jh . . . d(Sn)jhd(Sn)hj.

0

0

The objective function in (44) is strictly convex in Q.
Accordingly, the point of minimum is given by setting
equal to zero the ﬁrst variation of the objective function
for any δQ ∈ Qm: tr( ˆL(l)
0 δQ − m+1
2 Q−1δQ) = 0, which
implies that ˆL(l)
0 − m+1
2 Q−1 = 0. Therefore, under the
assumption that ˆL(l)

0 (cid:31) 0 we have (45). (cid:50)

References

Alpago, D., Zorzi, M., & Ferrante, A. (2018).

Identi-
ﬁcation of sparse reciprocal graphical models. IEEE
Control Systems Letters, 2 , 659–664.

Asadi, N., Rish, I., Scheinberg, K., Kanevsky, D., &
Ramabhadran, B. (2009). MAP approach to learning
In International
sparse gaussian markov networks.
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP) (pp. 1721–1724).

Avventi, E., Lindquist, A., & Wahlberg, B. (2013).
IEEE

ARMA identiﬁcation of graphical models.
Trans. Autom. Control , 58 , 1167–1178.

13

markov net learning with priors on regularization pa-
rameters. In ISAIM .

Songsiri, J., Dahl, J., & Vandenberghe, L. (2010).
Graphical models of autoregressive processes.
In
D. Palomar, & Y. Eldar (Eds.), Convex Optimization
in Signal Processing and Communications (pp. 1–29).
Cambridge: Cambridge Univ. Press.

Songsiri, J., & Vandenberghe, L. (2010). Topology selec-
tion in graphical models of autoregressive processes.
J. Mach. Learning Res., 11 , 2671–2705.

Willsky, A. (2002). Multiresolution markov models for
signal and image processing. In Proc. IEEE (pp. 1396–
1458). volume 90.

Wipf, D., & Nagarajan, S. (2010). Iterative reweighted
(cid:96)1 and (cid:96)2 methods for ﬁnding sparse solutions. J. Sel.
Topics Signal Processing, 4 , 317–329.

Zhou, Z., Leahy, R., & Qi, J. (1997). Approximate max-
imum likelihood hyperparameter estimation for gibbs
IEEE transactions on image processing, 6 ,
priors.
844–861.

Zorzi, M. (2014a). A new family of high-resolution mul-
IEEE Trans. Autom.

tivariate spectral estimators.
Control , 59 , 892–904.

Zorzi, M. (2014b). Rational approximations of spectral
densities based on the Alpha divergence. Math. Con-
trol Signals Syst., 26 , 259–278.

Zorzi, M. (2015). An interpretation of the dual problem
of the THREE-like approaches. Automatica, 62 , 87 –
92.

Zorzi, M. (2015). Multivariate spectral estimation based
on the concept of optimal prediction. IEEE Trans.
Autom. Control , 60 , 1647–1652.

Zorzi, M., & Sepulchre, R. (2016). AR identiﬁcation
of latent-variable graphical models. IEEE Trans. on
Automatic Control , 61 , 2327–2340.

Banerjee, O., El Ghaoui, L., & d’Aspremont, A. (2008).
Model selection through sparse maximum likelihood
estimation for multivariate gaussian or binary data.
Journal of Machine learning research, 9 , 485–516.
Brillinger, D. (1996). Remarks concerning graphical
models for times series and point processes. Revista
de Econometrica, 16 , 1–23.

Byrnes, C., Georgiou, T., & Lindquist, A. (2000). A
new approach to spectral estimation: A tunable high-
IEEE Trans. Signal
resolution spectral estimator.
Processing, 48 , 3189–3205.

Cand`es, E., Wakin, M., & Boyd, S. (2008). Enhancing
sparsity by reweighted (cid:96)1 minimization. Journal of
Fourier analysis and applications, 14 , 877–905.

Chandrasekaran, V., Parrilo, P., & Willsky, A. (2010).
Latent variable graphical model selection via convex
optimization. Annals of Statistics, 40 , 1935–2013.
Ciccone, V., Ferrante, A., & Zorzi, M. (2018). Ro-
bust identiﬁcation of “sparse plus low-rank” graphi-
cal models: An optimization approach. In 57th IEEE
Conference on Decision and Control (pp. 2241–2246).
Miami, Florida.

Dahlhaus, R. (2000). Graphical interaction models for

multivariate time series1. Metrika, 51 , 157–172.

Fazel, M., Hindi, H., & Boyd, S. P. (2003). Log-det
heuristic for matrix rank minimization with applica-
tions to hankel and euclidean distance matrices. In
American Control Conference, 2003. Proceedings of
the 2003 (pp. 2156–2162). volume 3.

Ferrante, A., Masiero, C., & Pavon, M. (2012). Time and
spectral domain relative entropy: A new approach to
multivariate spectral estimation. IEEE Trans. Autom.
Control , 57 , 2561–2575.

Friedman, J., Hastie, T., & Tibshirani, R. (2001). The
elements of statistical learning volume 1. Springer
series in statistics New York.

Karlsson, J., & Georgiou, T. (2013). Uncertainty bounds
for spectral estimation. IEEE Transactions on Auto-
matic Control , 58 , 1659–1673.

Lauritzen, S. (1996). Graphical Models. Oxford: Oxford

University Press.

Li´egeois, R., Mishra, B., Zorzi, M., & Sepulchre, R.
(2015). Sparse plus low-rank autoregressive identiﬁca-
tion in neuroimaging time series. In 54th IEEE Con-
ference on Decision and Control (CDC) (pp. 3965–
3970). Osaka, Japan.

Maanan, S., Dumitrescu, B., & Giurc¨aneanu, C. (2017).
Conditional independence graphs for multivariate au-
toregressive models by convex optimization: Eﬃcient
algorithms. Signal Processing, 133 , 122–134.

Maanan, S., Dumitrescu, B., & Giurc¨aneanu, C. (2018).
Maximum entropy expectation-maximization algo-
rithm for ﬁtting latent-variable graphical models to
multivariate time series. Entropy, 20 .

Razaviyayn, M., Hong, M., & Luo, Z.-Q. (2013). A
uniﬁed convergence analysis of block successive mini-
mization methods for nonsmooth optimization. SIAM
Journal on Optimization, 23 , 1126–1153.

Scheinberg, K., Rish, I., & Asadi, N. (2010). Sparse

14

