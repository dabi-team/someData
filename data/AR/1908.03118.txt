Efﬁcient 3D Reconstruction and Streaming for Group-Scale Multi-Client
Live Telepresence

Patrick Stotko*

Stefan Krumpen†

Michael Weinmann‡

Reinhard Klein§

University of Bonn

0
2
0
2

n
a
J

3
1

]

C
H
.
s
c
[

2
v
8
1
1
3
0
.
8
0
9
1
:
v
i
X
r
a

Figure 1: Illustration of our novel highly scalable multi-client live telepresence system. While previous approaches are limited to a
low number of up to 4 remote exploration clients, our system is capable of providing an immersive telepresence experience within a
live-captured high-quality scene reconstruction to more than 24 clients simultaneously without introducing further latency.

ABSTRACT
Sharing live telepresence experiences for teleconferencing or remote
collaboration receives increasing interest with the recent progress in
capturing and AR/VR technology. Whereas impressive telepresence
systems have been proposed on top of on-the-ﬂy scene capture, data
transmission and visualization, these systems are restricted to the im-
mersion of single or up to a low number of users into the respective
scenarios. In this paper, we direct our attention on immersing signif-
icantly larger groups of people into live-captured scenes as required
in education, entertainment or collaboration scenarios. For this pur-
pose, rather than abandoning previous approaches, we present a
range of optimizations of the involved reconstruction and streaming
components that allow the immersion of a group of more than 24
users within the same scene – which is about a factor of 6 higher than
in previous work – without introducing further latency or changing
the involved consumer hardware setup. We demonstrate that our
optimized system is capable of generating high-quality scene recon-
structions as well as providing an immersive viewing experience to
a large group of people within these live-captured scenes.

Index Terms:
Human-centered computing—Human com-
puter interaction (HCI)—Interaction paradigms—Virtual real-
ity; Human-centered computing—Human computer interaction
(HCI)—Interaction paradigms—Collaborative interaction; Comput-
ing methodologies—Computer graphics—Graphics systems and
interfaces—Virtual reality; Computing methodologies—Computer
vision—Computer vision problems—Reconstruction

1 INTRODUCTION
The rapidly increasing potential of AR/VR technology has led to
several highly advanced telepresence applications such as telecon-

*e-mail: stotko@cs.uni-bonn.de
†e-mail: krumpen@cs.uni-bonn.de
‡e-mail: mw@cs.uni-bonn.de
§e-mail: rk@cs.uni-bonn.de

ferencing in room-scale environments [4, 24] or the exploration of
places – that may vary from the users’ local physical environment
– for live-captured scenes beyond room-scale [27] and for remote
collaboration purposes. To meet the critical success factors of an im-
mersive telepresence experience for on-the-ﬂy captured 3D data as
required by these scenarios, these systems impose strong demands re-
garding the reconstruction and streaming speed as well as the visual
quality of the acquired scene. Furthermore, the interactive explo-
ration within the scene requires rendering at high framerates and low
latency to avoid motion sickness. This means that all of the involved
processing steps including 3D scene capture, data transmission and
visualization have to be achieved in real-time, while taking the typi-
cally available network bandwidth and client-side compute hardware
into account. Previous teleconferencing systems [4, 24, 28] were
designed to capture a ﬁxed region of interest based on expensive
well-calibrated acquisition setups involving statically mounted cam-
eras. In contrast, the live telepresence system by Stotko et al. [27] is
tailored to the acquisition of scenes beyond such a ﬁxed size deﬁned
by the setup and involves portable, consumer-grade capture hard-
ware. As a result, efﬁciently representing and transmitting the scene
to visualization devices is signiﬁcantly harder. While further work
has been spent on parallelized capturing [6], the goal of immersing
a large number of people into the same live-captured environment
while allowing them to interact with each other for e.g. remote col-
laboration and exploration scenarios, to the best of our knowledge,
has not received a lot of attention so far. In particular, the major
challenge is given by the accurate reconstruction and transmission
of 3D models while keeping the computational burden as well as
the memory and streaming requirements as low as possible, thus,
minimizing the amount of unnecessary or unreliable model data
resulting from noise and outliers in the captured input data.

In this paper, we address the scalability of live telepresence sys-
tems to the immersion of whole groups of (more than 24) people
without introducing further latency as required for education, enter-
tainment and collaboration scenarios (see Fig. 1). For this purpose,
rather than developing new techniques for 3D capture and data trans-
mission, we demonstrate how existing well-established systems for
3D reconstruction and streaming can be optimized to signiﬁcantly
increase the scalability of live telepresence systems under strong

© 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising or promotional purposes,creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. The ﬁnal version of record is available at
http://dx.doi.org/10.1109/ISMAR.2019.00018.

SLAMCast4ECsOurs24ECs 
 
 
 
 
 
Figure 2: Overview of the major components of state-of-the-art live multi-client live telepresence systems. RGB-D image data acquired by a single
camera device are streamed to a cloud server (red arrows) where a global 3D scene model is reconstructed in a dedicated 3D reconstruction
process. This scene model is then passed to the central server process (blue arrows) which also runs on the cloud server and manages a
bandwidth-optimized version of the model as well as the client states. Large groups of people (more than 24 in our system), each running an
exploration client on their local hardware, can independently request parts of the reconstructed global scene model (green arrows) and render the
locally generated mesh on their display devices (red arrows).

constraints regarding latency and bandwidth. We demonstrate that
our extensions result in compact high-quality 3D reconstructions
and ﬁnally allow the immersion of more than 24 people within
the same live-captured scene (beyond room-scale), thereby signiﬁ-
cantly exceeding the number of immersed persons in previous ap-
proaches [27] without adding or exchanging hardware components.
In summary, the key contributions of this work are (1) an efﬁcient
novel set of ﬁlters designed to optimize the performance and scala-
bility of current state-of-the-art telepresence systems at the example
of the SLAMCast system [27], (2) an adaption of the telepresence-
speciﬁc ﬁlters to standalone volumetric 3D reconstruction and (3)
a comprehensive evaluation of the beneﬁcial effect of the proposed
set of ﬁlters regarding scalability, latency and visual quality.

2 RELATED WORK

Telepresence applications for sharing live experiences rely on real-
time 3D scene capture. For this purpose, the underlying scene repre-
sentation, where the scene is reconstructed based on the fusion of the
incoming sensor data, is of particular importance. Well-established
representations include surface modeling in the form of implicit
truncated signed distance ﬁelds (TSDFs). Early real-time volumetric
reconstruction approaches [9, 21] are based on storing the scene
model in a uniform grid. This results in high memory requirements
as the data structure is not adapted according to the local presence
of a surface. To improve the scalability to large-scale scenes, further
work exploited the sparsity in the TSDF representation, e.g. based on
moving volume techniques [26, 31], representing scenes in terms of
blocks of volumes that follow dominant planes [8] or storing TSDF
values only near the actual surface areas [1, 12, 23]. The individual
blocks can be managed using tree structures or hash maps as pro-
posed by Nießner et al. [23] and respective optimizations [12,13,25].
Furthermore, the replacement of the TSDF representation by a high-
resolution binary voxel grid has also been considered by Reichl
et al. [25] to improve the scalability and reduce the memory re-
quirements. Recent extensions include the detection of loop clo-
sures [2, 11, 16] to reduce drift artifacts in camera localization as
well as multi-client collaborative acquisition and reconstruction of
static scenes [6].

This progress in real-time capturing enabled the development
of various telepresence applications. Early telepresence sys-
tems [5, 9, 10, 17–19] were designed for room-scale environments
and faced the problems of a limited reconstruction quality due to
high sensor noise and a reduced resolution. Relying on an expensive
capturing setup with several cameras, GPUs and desktop computers,
the Holoportation system [24] was designed for high-quality real-
time reconstruction of a dynamic room-scale environment based on
the Fusion4D system [3] as well as real-time data transmission. This
has been complemented with AR/VR systems to allow immersive
end-to-end teleconferencing. In contrast, interactive telepresence for
individual remote users within live-captured static scenes has been
addressed by Mossel and Kr¨oter [20] based on voxel block hash-

ing [12,23]. The limitations of this system regarding high bandwidth
requirements, the immersion of only a single remote user into the
captured scenarios as well as network interruptions leading to loss
of scene parts that are reconstructed in the meantime have been over-
come in the recent SLAMCast system [27]. However, the scalability
to immersing large groups of people into on-the-ﬂy captured scenes
has not been achieved so far. In this paper, we directly address this
problem by several modiﬁcations to the major components involved
in telepresence systems.

3 SYSTEM OUTLINE
Akin to previous work, we build our scalable multi-client telepres-
ence system on top of a volumetric scene representation in terms
of voxel blocks, i.e. blocks of 83 = 512 voxels. This approach
has been well-established by previous investigations in the context
of real-time reconstruction [1, 2, 9, 11–13, 21–23, 30, 31] and telep-
resence [20, 24, 27]. As shown in Fig. 2, current state-of-the-art
telepresence systems involving live-captured scenarios rely on the
core components of (1) a real-time 3D reconstruction process, (2)
a central server process as well as (3) exploration clients. RGB-D
images captured by a single camera are streamed to the reconstruc-
tion process that runs on a cloud server and allows on-the-ﬂy camera
localization and scene capture via volumetric fusion. The recon-
structed scene data is then passed to the central server process that
manages a bandwidth-optimized version of the global model as well
as the streaming of these data according to requests by connected
exploration clients. Each exploration client integrates the transmitted
scene parts into a locally generated mesh that can be interactively
explored with VR devices on their local computers. In the follow-
ing, we focus on the extension of such live telepresence systems to
the immersion of larger groups of remote users into a live-captured
scene at the example of the SLAMCast system [27]. This requires
the optimization of the reconstruction (see Sect. 4) and the central
server processes (see Sect. 5). In contrast, the exploration client
receives the compressed and optimized scene representation and is
already capable of providing an immersive viewing experience at
the remote user’s site.

4 OPTIMIZATION OF THE 3D RECONSTRUCTION PROCESS
Since our optimizations are not particularly restricted to the recon-
struction process used in the SLAMCast system, we show their
application to volumetric 3D reconstruction approaches in general
and provide an overview of the respective pipeline (see Fig. 3). Here,
the surface is represented in terms of implicit truncated signed dis-
tance ﬁelds (TSDFs) and stored as a sparse unordered set of voxel
blocks using spatial hashing [2, 11, 12, 23, 25]. Input to the recon-
struction pipeline is an incremental stream of RGB-D images which
is processed in an online fashion. First, the current RGB-D frame is
preprocessed where camera-speciﬁc distortion effects are removed
and a normal map is computed from the depth data. Afterwards,
the current camera pose is estimated either using frame-to-model

3DReconstructionProcessReconstructs3DmodelfromRGB-Ddataoflive-capturedscene.CentralServerProcessManagesglobal3Dmodelandexplorationclientstreamstates.ExplorationClientGenerateslocalmeshandprovidesimmersivevisualization.......Figure 3: General volumetric 3D reconstruction pipeline. Our set of efﬁcient ﬁlters designed to improve the performance and scalability of the
state-of-the-art live telepresence systems can also be applied to the components of standalone 3D reconstruction (highlighted).

tracking [9, 12, 21, 23, 25, 29, 31] (as also used in the SLAMCast
system) or using bundle adjustment for globally-consistent recon-
struction [2, 11]. Using this pose, non-visible voxel block data are
streamed out to CPU memory whereas visible blocks in CPU mem-
ory are streamed back into GPU memory [12, 16, 23, 25]. In the next
step, new voxel blocks are allocated in the volume and the RGB-D
data are fused into the volumetric model. Finally, a novel view
depicting the current state of the reconstruction is generated using
raycasting to provide a live feedback to the user during capturing.

4.1 Image Preprocessing

We improve the robustness of the acquired RGB-D data by ﬁltering
potentially unreliable data from the depth map. A further beneﬁt
of this operation is the resulting more compact scene model repre-
sentation. Inspired by previous work [31], we discard samples d
located on stark depth discontinuities by considering the deviations
to the depth values di in a 7
7 neighborhood N (d). Due to the
limited resolution and the overall noise characteristics of the sensor,
such samples are likely to be outliers and might largely deviate from
the true depth values. We extend this ﬁlter by further discarding
samples d with a signiﬁcant amount of missing data in their local
neighborhood. In such regions, which may not only contain depth
discontinuities, the depth measurements are also susceptible of being
unreliable. Thus, we consider the set

×

Do =

d
{

i
| ∃

∈

N (d):

d
|

di

|

−

> cd

No(d)
|

∨ |

> ch

· |

N (d)

(1)

|}

as outliers where cd and ch are user-deﬁned thresholds, N (d) de-
notes the neighborhood of the depth sample d and No(d) the set of
neighboring pixels with no valid depth data. These outliers affect
the overall reconstruction quality as well as the model compactness.

4.2 Data Fusion

Although potentially unreliable data around stark depth discontinu-
ities have been ﬁltered out during the preprocessing step, there are
still samples, e.g. around small discontinuities, that do not contribute
to the reconstruction and negatively affect the model compactness
and streaming performance. In the voxel block allocation step, these
unreliable data unnecessarily enlarge the global truncation region
around the unknown surface since all voxel blocks located within
the local truncation region around the respective depth samples are
considered during allocation. Traditional approaches tried to remove
these blocks afterwards using a garbage collection [23] which re-
quires a costly analysis of the voxel data. In contrast, we propose a
novel implicit ﬁlter which reduces the amount of unnecessary block
allocations. By considering only every ca-th pixel per column and
row, where ca is a user-deﬁned control parameter, the depth image
is virtually downsampled and the likelihood for an over-sized global
truncation region is signiﬁcantly reduced. Furthermore, this reduces
the number of processed voxels during data fusion which greatly
speed-ups the reconstruction and reduces the amount of blocks that
are later queued for streaming to the server. Note that this down-
sampling is only performed during allocation whereas the whole
depth image is still used for data fusion to employ TSDF-based
regularization. In the context of globally-consistent 3D reconstruc-
tion using bundle-adjusted submaps [6, 11], our ﬁlter improves the
compactness of the respective submap into which the RGB-D data

allocateBlocks(P T SDF )
copyVoxelData(V T SDF )

Algorithm 1 Our optimized server voxel block data integration
Input: Received TSDF voxel block positions P T SDF and voxel data V T SDF
Output: Voxel block position list P MC for updating the stream sets
1: M T SDF
←
2: M T SDF
←
3: P MC
createBlockUpdateSet(P T SDF )
4: V MC, F MC
5: M MC
6: M MC
7: M MC
8: P MC
9: P MC

computeVoxelData(P MC,M T SDF )
allocateNonEmptyBlocks(P MC, F MC)
copyNonEmptyVoxelData(V MC, F MC)
pruneEmptyBlocks(P MC, F MC)
pruneBlockUpdateSet(P MC, P MC
updateNonEmptyBlockSet(P MC

A , F MC)

←

←

A , P MC)

←
←
←
←
A ←

are fused whereas the fusion of the more compact submaps into a
single global model would be performed as in previous work.

4.3 Model Visualization
In order to provide a decent live preview of the current model state,
the generation of such model views should preserve all the relevant
scene information while suppressing noise as much as possible. Fur-
thermore, if frame-to-model tracking is used to estimate the current
camera pose, this is also crucial to allow a robust alignment. We
propose a Marching Cubes (MC) voxel block pruning approach
which will be described in more detail in Sect. 5, as it has been
carefully designed for the central server process. Here, we show an
adaption of this contribution to standalone volumetric 3D reconstruc-
tion where the model is stored implicitly using TSDF voxels. Each
TSDF voxel stores a TSDF value D
1; 1] and fusion weight
W
[0; 255] (both compressed using 16-bit linear encoding [12])
[0; 255]3. Inspired by the garbage col-
as well as a 24-bit color C
lection approach of point-based reconstruction techniques [14], we
ignore TSDF voxels for raycasting and triangle generation which
are currently considered unstable. These voxels contain only very
few, possibly unreliable observations from the input data, so their
fusion weight falls below a user-deﬁned threshold cw:

[
−

∈

∈

∈

V T SDF
o

=

(D,W,C)
{

|

W < cw

}

(2)

However, in contrast to previous garbage collection approaches [14,
23], we do not remove these voxel blocks but only ignore them.
This avoids accidental removal of blocks that might become stable
at a future time when this scene part is also partially stored in a
different submap or revisited by the user or another client in multi-
client acquisition setups [6, 11]. Furthermore, by ignoring unstable
data, the raycasted view will also be consistent with the exploration
client’s version of the 3D model.

5 OPTIMIZATION OF THE CENTRAL SERVER PROCESS
Beyond optimizations in the reconstruction process, the scalability
of a live telepresence system also relies on the optimization of its
central server process that takes care of managing the reconstructed
global scene model as well as the stream states and requests by
connected exploration clients. In this regard, we show respective
optimizations at the example of the recently published SLAMCast
system [27]. In comparison to the standard voxel block data inte-
gration at the server side, we propose a further ﬁltering step which

Volumetric3DReconstructionImagePreprocessingCameraPoseEstimationCPU–GPUStreamingDataFusionModelVisualizationdiscards empty or unstable voxel blocks that contain only very few
or none observations from the input RGB-D image data. This sig-
niﬁcantly improves the streaming performance and scalability and
allows the immersion of groups of people. The individual steps of
our optimized integration approach are shown in Algorithm 1.

∈

Similar to the original SLAMCast system, we ﬁrst integrate the
TSDF voxel block positions P T SDF and voxel data V T SDF into
the global TSDF voxel block model M T SDF of the central server
process. Afterwards, we update the global MC voxel block model
M MC which is optimized for streaming and stores a Marching
[0; 255]3 in each
Cubes index I
[0; 255] as well as a 24-bit color C
MC voxel. For this purpose, we create the set P MC of MC voxel
block positions requiring an update as well as a set of ﬂags F MC
and the respective MC voxel data V MC by performing the Marching
Cubes algorithm on the corresponding TSDF voxels [15]. The ﬂags
F MC indicate whether a block will generate reliable triangles and
are constructed by analyzing the Marching Cubes indices I of the
MC voxels as well as the fusion weight W of the corresponding
TSDF voxels. Therefore, the following set V MC
of voxels either
does not contain surface information in terms of triangles or would
generate unstable triangle data:

∈

o

V MC

o =

(I,C)

{

|

I = 0

∨

I = 255

∨

W < cw

}

(3)

We only allocate those blocks in the MC voxel block model M MC
that are ﬂagged and prune blocks that are currently not ﬂagged. This
minimizes the amount of scene data that are streamed to the explo-
ration clients. Finally, we integrate the generated MC voxel data
V MC of the ﬂagged blocks. We do not prune the TSDF voxel block
model M T SDF which would otherwise lead to potential artifacts, i.e.
missing geometry at block boundaries, since currently empty blocks
might be needed for future updates.

In contrast to the MC voxel block model, pruning the list of
updated MC voxel block positions P MC in the same way would in-
troduce artifacts at the exploration client side since they may already
have received a previous version of blocks that have been pruned in
the meantime. To properly handle updates, we manage the update
set P MC
containing all voxel block positions that were considered
for streaming in the past. We generate the list of updated MC voxel
blocks by only considering the ones which either generated trian-
gles in the past or with the current update. Finally, after the MC
voxel blocks have been integrated into the volume and the list of
updated block positions has been generated, we update the set P MC
by inserting all currently integrated voxel block positions.

A

A

6 EVALUATION
We tested our highly scalable telepresence system on a variety of
different datasets and analyzed several aspects such as system scala-
bility, streaming latency and visual quality. For a quantitative com-
parison of the proposed contributions, we considered the following
variants of our system:

• Base (B): Our 3D reconstruction and streaming system with de-
activated ﬁltering contributions, yielding equivalent performance
to SLAMCast [27].

• Base + Depth Discontinuity Filter (B+DDF): The base ap-
proach with an additional depth map ﬁltering at discontinuities
with ch = 0.25, cd = 0.2m (see Sect. 4.1).

• Base + Voxel Block Allocation Downsampling (B+VBAD):
The base approach with an additional virtual downsampling at the
voxel block allocation stage with ca = 4 (see Sect. 4.2).

• Base + MC Voxel Block Pruning (B+MCVBP): The base ap-
proach with an additional pruning of empty MC voxel blocks at
the server side with cw = 2.0 (see Sect. 4.3 and Sect. 5).

• Ours: Our approach incorporating all ﬁltering contributions.

Table 1: Maximum number of exploration clients (ECs) that the server
can handle without any delay compared to a single client. Instead of
using different package sizes with a ﬁxed request rate of 100Hz, we
use a ﬁxed size of 512 and vary the rate accordingly to demonstrate
the highest possible scalability. If empty MC voxel block pruning is
used (B+MCVBP and Ours), the sizes of the TSDF and MC voxel
block models differ and we list both (M MC/P MC

A (M T SDF )).

Approach

Dataset

Max.
ECs

Request
Rate [Hz]

103
Model Size [#
MC Voxel Blocks]

×

B

B+DDF

B+VBAD

B+MCVBP

Ours

lounge
copyroom
heating room
pool
lr kt2

lounge
copyroom
heating room
pool
lr kt2

lounge
copyroom
heating room
pool
lr kt2

lounge
copyroom
heating room
pool
lr kt2

lounge
copyroom
heating room
pool
lr kt2

5
9
3
5
1

9
10
9
10
10

8
5
4
8
1

21
9
8
13
7

25
18
27
28
26

100
50
100
100
200

50
50
50
50
50

50
100
100
50
200

12
50
25
25
25

12
25
12
12
12

314
228
850
590
834

270
230
443
379
227

264
226
622
446
550

47 / 51 (314)
65 / 75 (298)
120 / 127 (850)
104 / 108 (590)
64 / 64 (834)

44 / 47 (240)
57 / 67 (202)
90 / 94 (352)
95 / 99 (317)
53 / 55 (201)

The ﬁlter sizes and thresholds as described above were determined
empirically using several datasets. For validation, we used differ-
ent real-world datasets recorded with an ASUS Xtion Pro (lounge,
copyroom) [32] and a Kinect v2 (heating room, pool) [27] as well
as synthetic data (lr kt2 with simulated noise) [7]. Throughout the
experiments, we used three computers where each of them takes the
role of one part of the telepresence system, i.e. 3D reconstruction
process (RC), central server process (S) and exploration client (EC).
All computers were equipped with an Intel Core i7-4930K CPU and
32GB RAM and a NVIDIA GTX 1080 GPU with 8GB VRAM and
connected via a local network. We replaced the exploration client
by a benchmark client which starts requesting voxel blocks with a
ﬁxed frame rate of 100Hz when the reconstruction process starts.
Furthermore, the reconstruction process uses a ﬁxed reconstruction
speed of 30Hz matching the framerate of the used datasets. We set
the voxel size to 5mm as well as the truncation region to 60mm and
used hash map/set sizes of 220 and 222 buckets as well as GPU and
CPU voxel block pool sizes of 219 and 220 blocks, thereby following
previous work [27].

6.1 System Scalability

In this section, we will evaluate the scalability of our system in
comparison to the baseline SLAMCast approach (see Table 1). In
contrast to the following evaluations, the benchmark client discards
the received data which allows for running all benchmark clients on
a single computer without an overhead. Furthermore, rather than
lowering the package size, we used a ﬁxed package size of 512 voxel
blocks and lower the request rate accordingly. This signiﬁcantly
reduced the constant overheads of kernel calls and memory copies
and introduces only a minimal delay in the range of milliseconds
which made it the preferred setting for handling a large number of
clients. For an appropriate choice of the streaming rate, we deter-

Figure 4: Streaming progress and latency between server (S) and exploration client (EC) over time for the heating room dataset using our full
system. Left: Absolute model sizes for the highest and lowest chosen package size. Right: Relative size differences between S and EC (w.r.t.
model size S MC and update set size S MC US).

Figure 5: Streaming progress and latency between server (S) and exploration client (EC) over time for the heating room dataset for each system
variant. Left: Absolute model sizes. Right: Relative size differences between S and EC.

mined the lowest package size which still allows the benchmark
client to retrieve the whole model with an acceptable delay of at
most one second (see supplemental material for a detailed analysis).
Then, we measured the maximum number of benchmark clients that
the server could handle without introducing a further delay. While
the original SLAMCast system was only able to handle around 3-5
clients in general, both ﬁlters at the reconstruction side (B+DDF and
B+VBAD) raised this limit to up to 10 clients. Since there is a track-
ing loss at the end of the copyroom sequence resulting in a slightly
higher delay, a higher request rate was chosen and the scalability
decreased accordingly. Although the number of MC voxel blocks
is signiﬁcantly lower after pruning (B+MCVBP), we observed that
the general performance is similar to the depth discontinuity ﬁlter
approach. Here, the TSDF voxel block model has the same size as in
the base approach and is, hence, considerably larger than in the other
approaches. In contrast, our full system reduces the request rate
requirements to 12Hz for most scenes making it the preferred choice
for this parameter. This signiﬁcantly improves its scalability to more
than 24 clients in all scenes which is sufﬁcient for applications in
education, entertainment or collaboration scenarios.

6.2 Latency and Streaming Progress Analysis

In addition to the scalability analysis, we also measured the stream-
ing latency over time (see Fig. 4). Similar to the original SLAMCast
approach, our system has a small delay between the reconstruction
process and the server process due to the shared streaming strategy.
However, our optimized server model prunes unreliable or irrelevant
blocks which results in a very low latency between the server and the
exploration client. We also compared the latency between the largest

A

and smallest chosen package size, i.e. 1024 and 64 blocks/request.
Here, the model size of the exploration client is close to the size of
the server’s update set P MC
indicating a very fast and low-latent
streaming while the gap to the minimal size of the server model
M MC increases over time. Note that these two sizes are the bounds
for the exploration client’s model size and clients which have recon-
nected, e.g. due to network outages, will receive a slightly more
compact model closer to the lower bound. Reducing the package
size from 1024 to 64 blocks signiﬁcantly reduces the bandwidth
requirements (see supplemental material for a detailed analysis) and
leads to a slightly worse latency when the reconstruction process
queues the currently visible voxel blocks for streaming.

A

In Fig. 5, we also compared the different system variants regard-
ing streaming progress and latency. For a fair comparison between
the approaches, the package size is chosen such that the mean band-
widths are similar, i.e. around 15Mbit/s. Here, we also considered
the size of the update set P MC
in addition to size of the server model
M MC when empty MC voxel block pruning is enabled (B+MCVBP
and Ours). In these scenarios, the number of voxel blocks transmitted
to the exploration client bound by these two sizes is typically close
to the upper bound P MC
A . In comparison to the baseline, both ﬁlter-
ing approaches at the reconstruction side (B+DDF and B+VBAD)
reduce the latency signiﬁcantly. Similar results can be seen when
empty MC voxel blocks are pruned (B+MCVBP). Whereas all of
these approaches still introduce a noticeable delay at the time steps
40s and 90-100s, our full system is capable of streaming the recon-
structed model with almost no delay across the whole sequence.
Additional results regarding bandwidth and streaming latency over
time are provided in the supplemental material.

0204060801001200100200300Time[s]#VoxelBlocks×103RCTSDF512RCSSSTSDFSMCSMCUSECMC1024ECMC64020406080100120-4048Time[s]0204060801001200100200300400500600700800Time[s]#VoxelBlocks×103B–SB–EC512B+DDF–SB+DDF–EC512B+VBAD–SB+VBAD–EC512B+MCVBP–SB+MCVBP–SUSB+MCVBP–EC128Ours–SOurs–SUSOurs–EC128020406080100120050100150200Time[s](a) heating room: B,
16.5ms (8.6ms), 3482MB

(b) heating room: B+DDF,
10.3ms (4.4ms), 1815MB

(c) heating room: B+VBAD,
13.2ms (6.6ms), 2548MB

(d) heating room: B+MCVBP,
16.1ms (8.7ms), 3482MB

(e) heating room: Ours,
9.7ms (3.3ms), 1442MB

(f) lounge: B,
10.9ms (5.0ms), 1286MB

(g) lounge: B+DDF,
10.0ms (4.4ms), 1106MB

(h) lounge: B+VBAD,
10.0ms (5.4ms), 1081MB

(i) lounge: B+MCVBP,
10.9ms (5.2ms), 1286MB

(j) lounge: Ours,
9.7ms (4.3ms), 983MB

Figure 6: Comparison of visual quality, mean runtime (and standard deviation) as well as memory requirements for each system variant. All
individual contributions reduced the amount of reconstruction artifacts while improving the overall reconstruction performance.

6.3 Visual Quality

more thoroughly acquired by the user.

In order to demonstrate the beneﬁt for standalone volumetric 3D
reconstruction, we also provide a qualitative comparison regarding
the visual quality of the reconstructed 3D models as well as the re-
spective runtime and memory requirements for the individual system
variants (see Fig. 6). In general, all approaches generated detailed
and accurate 3D models from the noisy RGB-D input data. However,
without ﬁltering, there might be some artifacts around depth discon-
tinuities as well as in regions which have not been fully observed
by the camera. These artifacts affect the overall visual experience
and lead to high runtime and memory requirements. Using virtual
downsampling at the voxel block allocation stage (B+VBAD), we
obtain almost identical 3D models but the computational burden
is signiﬁcantly lower since the number of empty blocks within the
model is reduced. In contrast, ﬁltering depth samples at depth dis-
continuities (B+DDF) or unreliable triangle data during Marching
Cubes (B+MCVBP) reduces the amount of artifacts in the afore-
mentioned regions. Note that in standalone 3D reconstruction, voxel
block pruning (B+MCVBP) mainly affects the triangulation step
at the end of the capturing session which leads to results similar
to the base approach regarding runtime and memory. Our full sys-
tem enhances the visual quality even further and almost completely
removes artifacts without sacriﬁcing the overall model complete-
ness. Here, we observe improvements of 10-40% and 25-60% for
the runtime and memory footprint respectively depending on the
scene. The objects in the lounge scene have been captured at a much
smaller distance and from more angles than in the heating room
scene which leads to less unreliable input data and, hence, a lower
impact of our outlier ﬁltering approach. Additional performance
measurements and results are provided in the supplemental material.
In the context of live remote collaboration, a slightly less complete
model can be beneﬁcial and helps to identify regions that still need
to be captured and reliably reconstructed. This, in turn, might even
increase the model completeness and accuracy since the scene is

6.4 Limitations

Despite the signiﬁcant improvements in terms of scalability, latency
and visual quality, our system still has some limitations. Since our
work is based on the SLAMCast system, misalignments within the
reconstruction might occur due to fast camera movement. While this
problem has been addressed by loop-closure techniques [2, 11], their
integration into live telepresence systems is still highly challenging.
Furthermore, too aggressive virtual downsampling during voxel
block allocation might lead to holes in the ﬁnal model when some
blocks covering distant objects are always skipped and, hence, never
allocated. However, this is only problematic for long-range devices
whereas typical RGB-D cameras have a smaller range of up to 5
meter which is still sufﬁcient for most scenarios.

7 CONCLUSION

We presented a highly scalable multi-client live telepresence sys-
tem which allows immersing a large number of people into a live-
captured environment. For this purpose, we used well-established
systems and proposed several optimizations regarding scalability,
latency, and visual quality. While our contributions are designed
with the telepresence system in mind, we also show their application
to standalone volumetric 3D reconstruction approaches. As demon-
strated in a comprehensive evaluation, our novel system allows the
immersion of more than 24 people within the same scene using
consumer hardware.

ACKNOWLEDGMENTS

This work was supported by the DFG projects KL 1142/11-1 (DFG
Research Unit FOR 2535 Anticipating Human Behavior) and KL
1142/9-2 (DFG Research Unit FOR 1505 Mapping on Demand).

32(6):169:1–169:11, 2013.

[24] S. Orts-Escolano et al. Holoportation: Virtual 3D Teleportation in
Real-time. In Proc. of the Annual Symp. on User Interface Software
and Technology, pp. 741–754, 2016.

[25] F. Reichl, J. Weiss, and R. Westermann. Memory-Efﬁcient Interac-
tive Online Reconstruction From Depth Image Streams. Computer
Graphics Forum, 35(8):108–119, 2016.

[26] H. Roth and M. Vona. Moving volume kinectfusion. In Proc. of the

British Machine Vision Conference, pp. 112.1–112.11, 2012.

[27] P. Stotko, S. Krumpen, M. B. Hullin, M. Weinmann, and R. Klein.
SLAMCast: Large-Scale, Real-Time 3D Reconstruction and Stream-
ing for Immersive Multi-Client Live Telepresence. IEEE Trans. on
Visualization and Computer Graphics, 25(5):2102–2112, 2019.
[28] R. Vasudevan, G. Kurillo, E. Lobaton, T. Bernardin, O. Kreylos, R. Ba-
jcsy, and K. Nahrstedt. High-Quality Visualization for Geographically
Distributed 3-D Teleimmersive Applications. IEEE Trans. on Multime-
dia, 13(3):573–584, 2011.

[29] T. Whelan, H. Johannsson, M. Kaess, J. J. Leonard, and J. McDonald.
Robust Real-Time Visual Odometry for Dense RGB-D Mapping. In
IEEE Int. Conf. on Robotics and Automation, pp. 5724–5731, 2013.

[30] T. Whelan, M. Kaess, M. Fallon, H. Johannsson, J. Leonard, and
J. McDonald. Kintinuous: Spatially Extended KinectFusion. In RSS
Workshop on RGB-D: Advanced Reasoning with Depth Cameras, 2012.
[31] T. Whelan, M. Kaess, H. Johannsson, M. Fallon, J. J. Leonard, and
J. McDonald. Real-time large-scale dense RGB-D SLAM with volu-
metric fusion. The Int. Journal of Robotics Research, 34(4-5):598–626,
2015.

[32] Q.-Y. Zhou and V. Koltun. Dense Scene Reconstruction with Points of

Interest. ACM Trans. Graph., 32(4):112, 2013.

REFERENCES

[1] J. Chen, D. Bautembach, and S. Izadi. Scalable Real-time Volumetric
Surface Reconstruction. ACM Trans. Graph., 32:113:1–113:16, 2013.
[2] A. Dai, M. Nießner, M. Zollh¨ofer, S. Izadi, and C. Theobalt. Bundle-
Fusion: Real-time Globally Consistent 3D Reconstruction using On-
the-ﬂy Surface Reintegration. ACM Trans. Graph., 36(3):24, 2017.
[3] M. Dou et al. Fusion4D: Real-time Performance Capture of Challeng-

ing Scenes. ACM Trans. Graph., 35(4):114:1–114:13, 2016.

[4] A. J. Fairchild, S. P. Campion, A. S. Garc´ıa, R. Wolff, T. Fernando, and
D. J. Roberts. A Mixed Reality Telepresence System for Collaborative
IEEE Trans. on Circuits and Systems for Video
Space Operation.
Technology, 27(4):814–827, 2016.

[5] H. Fuchs, A. State, and J. Bazin. Immersive 3D Telepresence. Com-

puter, 47(7):46–52, 2014.

[6] S. Golodetz, T. Cavallari, N. A. Lord, V. A. Prisacariu, D. W. Murray,
and P. H. S. Torr. Collaborative Large-Scale Dense 3D Reconstruction
with Online Inter-Agent Pose Optimisation. IEEE Trans. on Visualiza-
tion and Computer Graphics, 24(11):2895–2905, Nov 2018.

[7] A. Handa, T. Whelan, J. McDonald, and A. J. Davison. A Benchmark
for RGB-D Visual Odometry, 3D Reconstruction and SLAM. In Proc.
of the Int. Conf. on Robotics and Automation, pp. 1524–1531, 2014.
[8] P. Henry, D. Fox, A. Bhowmik, and R. Mongia. Patch Volumes:
Segmentation-Based Consistent Mapping with RGB-D Cameras. In
Int. Conf. on 3D Vision, 2013.

[9] S. Izadi et al. KinectFusion: Real-time 3D Reconstruction and Inter-
action Using a Moving Depth Camera. In Proc. of the ACM Symp. on
User Interface Software and Technology, pp. 559–568, 2011.

[10] B. Jones et al. RoomAlive: Magical Experiences Enabled by Scalable,
Adaptive Projector-camera Units. In Proc. of the Annual Symp. on
User Interface Software and Technology, pp. 637–644, 2014.

[11] O. K¨ahler, V. A. Prisacariu, and D. W. Murray. Real-Time Large-Scale
Dense 3D Reconstruction with Loop Closure. In European Conference
on Computer Vision, pp. 500–516, 2016.

[12] O. K¨ahler, V. A. Prisacariu, C. Y. Ren, X. Sun, P. Torr, and D. Murray.
Very High Frame Rate Volumetric Integration of Depth Images on
Mobile Devices. IEEE Trans. on Visualization and Computer Graphics,
21(11):1241–1250, 2015.

[13] O. K¨ahler, V. A. Prisacariu, J. P. C. Valentin, and D. W. Murray. Hierar-
chical Voxel Block Hashing for Efﬁcient Integration of Depth Images.
In IEEE Robotics and Automation Letters, pp. 1(1):192–197, 2016.

[14] M. Keller, D. Leﬂoch, M. Lambers, S. Izadi, T. Weyrich, and A. Kolb.
Real-Time 3D Reconstruction in Dynamic Scenes Using Point-Based
Fusion. In Proc. of Joint 3DIM/3DPVT Conference, p. 8, 2013.
[15] W. E. Lorensen and H. E. Cline. Marching Cubes: A High Resolution
3D Surface Construction Algorithm. In Proc. of the 14th Annual Conf.
on Computer Graphics and Interactive Techniques, pp. 163–169, 1987.
[16] R. Maier, R. Schaller, and D. Cremers. Efﬁcient Online Surface Correc-
tion for Real-time Large-Scale 3D Reconstruction. In British Machine
Vision Conference (BMVC), 2017.

[17] A. Maimone, J. Bidwell, K. Peng, and H. Fuchs. Enhanced personal
autostereoscopic telepresence system using commodity depth cameras.
Computers & Graphics, 36(7):791 – 807, 2012.

[18] A. Maimone and H. Fuchs. Real-time volumetric 3D capture of room-
sized scenes for telepresence. In Proc. of the 3DTV-Conference, 2012.
[19] D. Molyneaux, S. Izadi, D. Kim, O. Hilliges, S. Hodges, X. Cao,
A. Butler, and H. Gellersen. Interactive Environment-Aware Handheld
Projectors for Pervasive Computing Spaces. In Proc. of the Int. Conf.
on Pervasive Computing, pp. 197–215, 2012.

[20] A. Mossel and M. Kr¨oter. Streaming and exploration of dynamically
changing dense 3d reconstructions in immersive virtual reality. In Proc.
of IEEE Int. Symp. on Mixed and Augmented Reality, pp. 43–48, 2016.
[21] R. A. Newcombe et al. KinectFusion: Real-Time Dense Surface
Mapping and Tracking. In Proc. of IEEE Int. Symp. on Mixed and
Augmented Reality. IEEE, 2011.

[22] R. A. Newcombe, D. Fox, and S. M. Seitz. DynamicFusion: Recon-
struction and tracking of non-rigid scenes in real-time. In IEEE Conf.
on Computer Vision and Pattern Recognition, pp. 343–352, 2015.
[23] M. Nießner, M. Zollh¨ofer, S. Izadi, and M. Stamminger. Real-time 3D
Reconstruction at Scale Using Voxel Hashing. ACM Trans. Graph.,

