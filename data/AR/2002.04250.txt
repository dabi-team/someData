Non-Autoregressive Neural Dialogue Generation

Qinghong Han1‚àó, Yuxian Meng1‚àó, Fei Wu2 and Jiwei Li1
1 ShannonAI
2 Department of Computer Science and Technology, Zhejiang University
{qinghong han, jiwei li}@shannonai.com, wufei@zju.edu.cn

0
2
0
2

b
e
F
3
1

]
L
C
.
s
c
[

2
v
0
5
2
4
0
.
2
0
0
2
:
v
i
X
r
a

Abstract

information

Maximum Mutual
(MMI),
which models the bidirectional dependency
between responses (y) and contexts (x),
the forward probability log p(y|x) and
i.e.,
the backward probability log p(x|y), has
been widely used as the objective in the
SEQ2SEQ model to address the dull-response
issue in open-domain dialog generation.
Unfortunately, under the framework of the
SEQ2SEQ model,
direct decoding from
log p(y|x) + log p(x|y) is infeasible since
the second part (i.e., p(x|y)) requires the
completion of target generation before it can
be computed, and the search space for y is
enormous. Empirically, an N-best list is Ô¨Årst
generated given p(y|x), and p(x|y) is then
used to rerank the N-best list, which inevitably
results in non-globally-optimal solutions.

to

this

propose

(non-AR)

paper, we

Since target

In
use
generation
non-autoregressive
model to address this non-global optimality
issue.
tokens are generated
independently in non-AR generation, p(x|y)
for each target word can be computed as soon
as it‚Äôs generated, and does not have to wait for
the completion of the whole sequence. This
naturally resolves the non-global optimal issue
in decoding. Experimental results demonstrate
that the proposed non-AR strategy produces
more diverse,
and appropriate
coherent,
responses, yielding substantive gains in BLEU
scores and in human evaluations.1

1

Introduction

Open-domain neural dialogue generation (Vinyals
and Le, 2015; Sordoni et al., 2015; Li et al.,
2016a; Mou et al., 2016; Serban et al., 2016a;
Asghar et al., 2016; Mei et al., 2016; Serban et al.,
2016e,b,d; Baheti et al., 2018; Wang et al., 2018;
Ghazvininejad et al., 2018; Zhang et al., 2018;

1Qinghong and Yuxian contribute equally to this work.

Gao et al., 2019) treats dialog contexts (x) as
sources,and responses (y) as targets and uses the
encoder-decoder model (Sutskever et al., 2014;
Vaswani et al., 2017b) as the backbone to generate
responses. SEQ2SEQ models offer the promise
of
language-independence,
along with the capacity to capture contextual
dependencies semantic and syntactic relations
between sources and targets.

scalability

and

One of key issues with the SEQ2SEQ structure
is that it exhibits a strong tendency to generate
trivial or non-committal responses (e.g.,
dull,
I don‚Äôt know or I‚Äôm OK) regardless of the input,
which has been observed by many recent works
(Li et al., 2016a; Sordoni et al., 2015; Serban
et al., 2016c; Niu and Bansal, 2020). Various
strategies (Li et al., 2016a; Vijayakumar et al.,
2016; Baheti et al., 2018; Niu and Bansal, 2020)
have been proposed to address this issue, , one
of the most widely used of which is to replace
the MLE objective in the SEQ2SEQ training with
the maximum mutual information objective (MMI
for short) (Li et al., 2016a). MMI models the
bidirectional dependency between responses (y)
and contexts (x). It takes the form of the linear
combination of the forward probability log p(y|x)
and the backward probability log p(x|y). The
intuition behind MMI is straightforward:
it is
easy to predict a dull response given any context,
the context given a dull
but hard to predict
response since the context that corresponds to a
dull response could be anything.
the

of
the SEQ2SEQ model, direct decoding from
log p(y|x) + log p(x|y) is infeasible since the
second part (i.e., p(x|y)) requires the completion
target generation before p(x|y) can be
of
computed, and the search space for y is huge.
Empirically, an N-best list is Ô¨Årst generated given
p(y|x), and p(x|y) is then used to rerank the

Unfortunately,

framework

under

 
 
 
 
 
 
N-best list. Due to the fact that beam search
candidates
lacks for diversity in the beam:
often differ only by punctuation or minor
morphological variations, with most of the words
this reranking strategy inevitably
overlapping,
results in non-globally-optimal solutions. Some
strategies have been proposed to alleviate this
non-global-optimality issue, such as generating
a more diverse N-best list (Li et al., 2016c; Gu
et al., 2017; Vijayakumar et al., 2016), or using
reinforcement learning to estimate the future score
of p(x|y) (Li et al., 2017a), which help alleviate
the non-globally-optimal issue, but cannot fully
address it.

target

Under

tokens yt

Non-autoregressive (non-AR) generation (Gu
et al., 2018; Ma et al., 2019; Lee et al., 2018)
provides resolution to the non-global-optimality
the formalization of non-AR
issue.
are generated
generation,
independently, which enables p(x|yt)
to be
computed as soon as yt
This
is generated.
naturally resolves the non-global optimal issue in
decoding. We conduct experiments on the widely
and experimental
used Opensubtitle dataset
results demonstrate that
the proposed strategy
produces more diverse, coherent, and appropriate
responses, yielding substantive gains in BLEU
scores and in human evaluations.

The rest of this paper is organized as follows:
Section 2 and section 3 present related work and
background knowledge respectively. The propose
model is described in Section 4. Experimental
results and ablation studies are detailed in Section
5 and 6, followed by a brief conclusion in Section
7.

2 Related Work

2.1 Neural Dialogue Generation

approaches

End-to-end neural
for dialogue
generation use SEQ2SEQ architectures (Sutskever
et al., 2014; Vaswani et al., 2017b) as the
backbone to generate syntactically Ô¨Çuent and
meaningful responses, providing the Ô¨Çexibility
to capture contextual semantics between source
contexts and target responses. Recent studies
have endowed these models with the ability to
model contexts (Sordoni et al., 2015; Serban et al.,
2016e,b; Tian et al., 2017; Lewis et al., 2017),
generating coherent and personalized responses
(Li et al., 2016b; Zhao et al., 2017; Shao et al.,
2017; Xing et al., 2017; Zhang et al., 2018;

Bosselut et al., 2018), generating uttterances with
different attributes or topics (Wang et al., 2017;
Niu and Bansal, 2018) and interacting Ô¨Çuently
with humans (Ghazvininejad et al., 2018; Zhang
et al., 2019; Adiwardana et al., 2020).

2.2 Diverse Decoding

One major issue with SEQ2SEQ systems is their
propensity to select dull, non-committal responses
regardless of the input, for which many diverse
decoding algorithms have been proposed to tackle
this problem (Li et al., 2016a; Li and Jurafsky,
2016; Vijayakumar et al., 2016; Cho, 2016;
Kulikov et al., 2018; Kriz et al., 2019; Ippolito
et al., 2019).
Li et al. (2016a) proposed to
use Maximum Mutual Information (MMI) as
the objective function in neural dialog models.
MMI models use both the forward probability
p(y|x) and the backward probability p(x|y) to
better capture the contextual relations between the
source and target sequences. Li and Jurafsky
(2016) introduced a Beam Search diversiÔ¨Åcation
heuristic to discourage sequences from sharing
implicitly resulting in diverse
common roots,
sequences. Vijayakumar et al. (2016) improved
upon Li and Jurafsky (2016) and presented
Diverse Beam Search, which formalizes beam
search as an optimization problem and augments
the objective with a diversity term. Cho (2016)
introduced Noisy Parallel Approximate Decoding,
a method encouraging diversity by adding small
amounts of noise to the hidden state of the
decoder at each step, instead of manipulating the
probabilities outputted from the model. Kulikov
et al. (2018) attempted to explore larger beam
search space by running beam search many times,
where the states explored by subsequent beam
searches are restricted based on the intermediate
These
states explored by previous iterations.
works have pushed dialogue models to generate
more interesting and diverse responses that are
both high-quality and meaningful.

2.3 Non-Autoregressive Sequence Generation

Besides diverse responses,
another problem
for these dialogue generation models is their
autoregressive generation strategy that decodes
words one-by-one, making it extremely slow
to execute on long sentences, especially on
conditions where multi-turn dialogue often
appears (Adiwardana et al., 2020). One solution
is to use non-autoregressive sequence generation

methods, which has recently aroused general
in the community of neural machine
interest
translation (NMT) (Gu et al., 2018; Lee et al.,
2018; Ma et al., 2019; Sun et al., 2019; Shu
et al., 2019; Bao et al., 2019). Gu et al. (2018)
proposed to alleviate latency by using fertility
during inference in autoregressive Seq2Seq NMT
systems, which led to a ‚àº15 times speedup to
traditional autoregressive methods, whereas the
performance degrades rapidly. Lee et al. (2018);
Ma et al. (2019); Shu et al. (2019) proposed
to use latent variables to model
intermediate
word alignments between source and target
sequence pairs and mitigate the trade-off between
decoding speed and performance. Bao et al.
(2019) pointed out position information is
crucial for non-autoregressive models and thus
proposed to explicitly model position as latent
variables. Sun et al. (2019) incorporated CRF
to enhance
into non-autoregressive models
local dependencies during decoding.
This
work is greatly inspired by these advances in
non-autoregressive sequence generation.

3 Background

3.1 Autoregressive SEQ2SEQ Models

An encoder-decoder model (Sutskever et al., 2014;
Vaswani et al., 2017b; Bahdanau et al., 2014)
deÔ¨Ånes the probability of a target sequence Y =
{y1, y2, ..., yLy }, which is a response in the
context of dialogue generation, given a source
sequence X = {x1, x2, ..., xLx}, where where
Lx and Ly are the length of the source and target
sentence respectively.
An autoregressive

encoder-decoder model
decomposes
target
sequence y = {y1, ¬∑ ¬∑ ¬∑ , yLy } into a chain of
conditional probabilities:

the distribution over

a

(Sutskever et al., 2014), CNNs (Gehring et al.,
2017) or transformers (Vaswani et al., 2017b). eyt
denotes the representation for yt.

During decoding,

the algorithm terminates
when the < EOS > token is predicted. At
each time step, either a greedy approach or
beam search can be adopted for word prediction.
Greedy search selects the token with the largest
conditional probability, the embedding of which
is then combined with preceding output to predict
the token at the next step.

3.2 Non-Autoregressive SEQ2SEQ Models

3.2.1 Overview
The autoregressive generation model has two
major drawbacks: it prohibits generating multiple
tokens simultaneously, which leads to inefÔ¨Åciency
in GPU usage; and erroneously generated tokens
leads to error accumulation and the performance
of beam search deteriorates when exposed to a
larger search space (Koehn and Knowles, 2017).
Non-autoregressive methods address these two
issues by removing the sequential dependencies
within the target sentence and generating all target
tokens simultaneously, with the probability giving
as follows:

pNon-AR(y|x; œÜ) =

Ly
(cid:89)

t=1

p(yt|x; œÜ)

(2)

Now that each target token yt only depends on
the source sentence x, the full target sentence
can be decoded in parallel, where argmax
is applied to each token. A vital challenge
that non-autoregressive face is the inconsistency
problem Gu et al. (2018), which indicates the
decoded sequence contains duplicated or missing
tokens.
Improving decoding consistency on the
target side is thus crucial to Non-AR models.

pAR(y|x; œÜ) =

Ly+1
(cid:89)

log p(yt|y0:t‚àí1, x1:Lx; Œ∏)

4.1 Overview

4 Model

t=1
m
(cid:89)

t=1

=

exp(f (ht‚àí1, eyt))
y(cid:48) exp(f (ht‚àí1, ey(cid:48)))

(cid:80)

(1)
with y0 being the special < BOS > token and
yLy+1 being the special < EOS > token. The
probability of generating a token yt depends on all
tokens in the source X, and all its previous tokens
y0:t‚àí1 in Y . The concatenation of X and y0:t‚àí1
is mapped to a representation ht‚àí1 using LSTMs

The maximum mutual information (MMI) model,
proposed in (Li et al., 2016a), tries to Ô¨Ånd the
response that has the largest value of mutual
information with respect to the context. The form
of MMI is given as follows:2

ÀÜy = arg max

(cid:8)(1 ‚àí Œª) log p(y|x) + Œª log p(x|y)(cid:9)

y

(3)

2We refer readers to (Li et al., 2016a) for how Eq.3 is

obtained.

This weighted MMI objective function can be
viewed as representing a tradeoff between sources
given targets (i.e., p(x|y)) and targets given
sources (i.e., p(y|x)). Direct decoding from
log(1 ‚àí Œª)p(y|x) + Œª log p(x|y) is infeasible
since the second part (i.e., p(x|y)) requires the
completion of target generation before p(x|y) can
be computed. Empirically, an N-best list is Ô¨Årst
generated given p(y|x), and p(x|y) is then used to
rerank the N-best list, which inevitably results in
non-globally-optimal solutions.

Here to propose to use Non-AR generation
models to handle to non-globally-optimality issue.
The generation of each target word yt
is
independent under the non-AR formalization, and
the forward probability p(y|x) is given as follows:

Figure 1: Overview of the non-auto MMI generation
model.

forward prob =

t=Ly
(cid:89)

t=1

p(yt|x)

(4)

follows:

For the backward probability p(x|y), which
denotes the probability of generating a source
sequence given a target sequence, we propose
to replace it with the geometric mean of the
probability of generating the source sequence
given each target token, denoted as follows:

backward prob = [

t=Ly
(cid:89)

p(x|yt)]1/Ly

(5)

t=1

We also use the non-AR framework to model the
backward probability. Based on the independence
assumption of non-AR, in which the generations
of xt are independent, Eq.
5 can be further
factorized as follows:

backward prob = [

t=Ly
(cid:89)

t(cid:48)=Lx(cid:89)

p(xt(cid:48)|yt)]1/Ly

(6)

t=1

t(cid:48)=1

it actually
A close look at Equ.6 shows that
mimics the IBM model (Brown et al., 1993):
p(xt(cid:48)|yt) handles the pairwise word alignment
between sources and targets.
Since position
representations are incorporated at both the
encoding and decoding stage, Eq.6 actually
mimics IBM model2, where relative positions
between source and target words are modeled.

Combining the forward probability in Eq. 4.2
and the backward probability in Eq.6, the full form
of mutual information of Eq.3 can be rewritten as

L =(1 ‚àí Œª)

t=Ly
(cid:88)

t=1

log p(yt|x) +

t=Ly
(cid:88)

=

[(1 ‚àí Œª) log p(yt|x) +

t=1

t=Ly
(cid:88)

t(cid:48)=Lx(cid:88)

t=1

t(cid:48)=1

log p(xt(cid:48)|yt)

t(cid:48)=Lx(cid:88)

log p(xt(cid:48)|yt)]

Œª
Ly

Œª
Ly

t(cid:48)=1

(7)
as can be seen, we are able to factorize the full
form of the MMI objective with respect to yt
under the framework of non-AR generation. This
means that the mutual information between source
x and different target words yt are independent
and can be computed in parallel. Also, for each
token yt, its mutual information with respect to the
source x can be readily computed as soon as yt is
generated, and we do not have to wait until the
completion of the entire sequence. This naturally
resolves the non-globally-optimality issue in the
Figure 1 gives an
AR generation model.
illustration for the proposed model.

4.2 Forward Probability p(y|x)

We use the non-autoregressive SEQ2SEQ model
as the backbone to compute (cid:81)
t p(yt|x), which
consists of two major components:
the encoder
and the decoder.

4.2.1 Encoder

We use transformers (Vaswani et al., 2017a) as
a backbone and use a stack of N = 6 identical
transformer blocks as the encoder. Given the
source sequence x = {x1, ¬∑ ¬∑ ¬∑ , xn}, the encoder

ForwardEncoderForwardDecoderI like cats, how about you?IlikecatstooBackwardEncoderBackwardDecoder<P>   like   <P>   <P>I like cats, how about you?Forward Probability ùíëùíöùíïùíôBackward Probability ùíë(ùíô|ùíöùíï)produces its contextual representations H =
{h1, ¬∑ ¬∑ ¬∑ , hn} from the last layer of the encoder.

4.2.2 Decoder

Target Length We Ô¨Årst need to obtain the
length of the target sequence for decoding. We
follow previous works (Gu et al., 2018; Ma
et al., 2019; Bao et al., 2019) to predict
the
length difference ‚àÜm between source and target
sequences using a classiÔ¨Åer with a range of [-20,
20]. This is accomplished by max-pooling the
source embeddings into a single vector, running
this through a linear layer followed by a softmax
operation, as follows:

p(‚àÜm|x) = softmax(Wp(maxpool(H)) + bp)

(8)

Decoder Structure The decoder also consists
of N = 6 identical transformer blocks. The
i-th position of the input di
to the decoder
is the round(n ‚àó (i/m))-th input‚Äôs contextual
representation hround(n‚àó(i/m)) copied from the
encoder, which is equivalent
to scanning the
source inputs from left to right and leads to a
deterministic decoding process given the predicted
target length. Both absolute and relative positional
embeddings are incorporated. For relative position
information, we follow Shaw et al. (2018) which
produces a different learned embedding according
to the offset between the ‚Äúkey‚Äù and ‚Äúquery‚Äù
in the self-attention mechanism with a clipping
distance k (we set k = 4) for relative positions.
For absolute positional embeddings, we follow
Radford et al.
(2019) and used a learnable
positional embedding pt for position t.

over

Attention
Vocabulary Layer-wise
attention over vocabulary is incorporated into each
decoding layer to make the model aware of which
token is to be generated regarding each position.
More concretely, we use Z(i)(1 ‚â§ i ‚â§ 6) to
denote the contextual representations for the i-th
decoder layer , and Z(0) = {d1, ¬∑ ¬∑ ¬∑ , dm} to
denote the input to the decoder. The intermediate
token attention representation a(i)
of position
j
j(1 ‚â§ j ‚â§ m) in the i-th decoder layer is thus
given by:

j = softmax(z(i)
a(i)

j

¬∑ W T) ¬∑ W

(9)

where W is the representation matrix of the
token vocabulary. By doing so, each position

is able to know which token is about
to be
decoded at the current position. The input to
the next layer Z(i+1) is the concatenation of the
contextual representations and the intermediate
token representations [Z(i); A(i)] .

each position t,
softmax For
is
computed by outputting the representation for that
position to a softmax function.

p(yt|x)

4.3 Backward Probability p(x|y)

We use the non-AR model to obtain p(x|yt).

4.3.1 Encoder

The encoder for p(x|yt) is again a stack of N = 6
identical transformer blocks. The input to the
encoder is a text sequence with length being Ly,
which is identical to the length of the target. The
t-th position of the input sequence is the word
yt, with the rest being the place-holding dummy
token. For each posiition,
the embedding for
the absolute position and the embedding for the
relative position are appended.

4.3.2 Decoder

The decoder for the backward probability is the
same as that of the forward probability, with the
only difference being changing target y to source
x.

4.4 Decoding from Mutual Information

The most commonly used decoding strategy for
non-AR generation is the noisy parallel decoding
strategy (NPD for short) proposed in Gu et al.
(2018): a number of sequence candidates are Ô¨Årst
generated by the non-AR generation, then an AR
SEQ2SEQ model is used to select the candidate
that has the largest value of probability output
from the AR model. Since this NPD strategy is
used for the MLE objective which only concerns
about the forward probability, we need to tailor
it to the MMI objective. SpeciÔ¨Åcally, we Ô¨Årst
generate N-best sequences based on the score
of non-AR MMI function, computed from Eq.7.
The Ô¨Ånal selected response is the sequence with
highest AR MMI score, which is computed based
on two AR SEQ2SEQ models, one to model the
forward probability and the other to model the
backward probability.

5 Experiments

5.1 Datasets

We use the OpenSubtitles dataset for evaluation.
It‚Äôs a widely used open-domain dataset, which
contains roughly 60M-70M scripted lines spoken
by movie characters. It has been used in a broad
range of recent work on data-driven conversation
This dataset does not specify which character
speaks each subtitle line, which prevents us from
inferring speaker turns. Following (Vinyals and
Le, 2015; Li et al., 2016a), we make an assumption
that each line of subtitle constitutes a full speaker
turn. Although this assumption is often violated,
prior work has successfully trained and evaluated
neural conversation models using this corpus. In
our experiments we used a preprocessed version
of this dataset distributed by Li et al. (2016a).3

The noisy nature of the OpenSubtitle dataset
renders it unreliable for evaluation purposes. We
thus follow Li et al. (2016a) to use data from
the Internet Movie Script Database (IMSDB)4
for evaluation. The IMSDB dataset explicitly
identiÔ¨Åes which character speaks each line of the
script. We followed protocols in (Li et al., 2016a)
and randomly selected two subsets as development
and test datasets, each containing 2,000 pairs, with
source and target length restricted to the range of
[6,18].

5.2 Baselines

Our baselines include the AR generation models
(using or not using MMI) based on transformers
(Vaswani et al., 2017b), with the number of
encoder and decoder blocks set to 6. For the
standard AR model, the value of beam size is set to
10 for decoding, and the sequence with the largest
value of p(y|x) is selected. For AR+MMI, we
followed Li et al. (2016a), and Ô¨Årst use p(y|x) to
generate an N-best list with beam-size 10. Then
p(x|y) is used to rerank the N-best list. Œª is treated
as the hyper-parameter to be tuned on the dev set.
We also implement two variant of the AR+MMI
model: (1) AR+MMI+diverse (Li et al., 2016c),
which uses a diverse decoding model to generate
the N-best list and uses the backward probability
to rerank the diverse N-best list. The diverse
decoding model adds an additional
term to
penalize siblings in beam searchexpansions of

3http://nlp.stanford.edu/data/

OpenSubData.tar

4 http://www.imsdb.com/

the same parent node in the search thus favoring
choosing hypotheses from diverse parents; and
(2) AR+MMI+RL (Li et al., 2017a), which
incorporates the critic that estimates further
backward probability into decoding.

5.3 Training Details

All experiments were run using 64 Nvidia V100
GPUs with mini-batches of approximately 100K
tokens. We use the same hyper-parameters for
all experiments, i.e., word representations of size
1024, feed-forward layers with inner dimension
4096. Dropout rate is set to 0.2 and the number of
attention heads is set to 16. Models are optimized
with Adam (Kingma and Ba, 2014) using Œ≤1 =
0.9, Œ≤2 = 0.98, (cid:15) = 1e8. Differentiable scheduled
sampling Goyal et al. (2017) is used to mitigate
the exposure bias issue. We train models with
16-bit Ô¨Çoating point operations. The backward
model and the forward model are jointly trained
with word embeddings shared.

5.4 Automatic Evaluation

For automatic evaluation, we report the results of
the following metrics:

‚Ä¢ the BLEU score following previous work. It
should be noted that BLEU is not generally
accepted (Liu et al., 2016) to match human
evaluation in generation tasks since there are
distinct ways to reply to an input.

‚Ä¢ distinct-1 and distinct-2 (Li et al., 2016a):
calculating the number of distinct unigrams
and bigrams in generated responses scaled
by total number of generated unigrams and
bigrams.

‚Ä¢ Avg.length:

the average length of

the

generated response.

‚Ä¢ Stopword%:

the percentage of stop-words5

of the responses generated by each model.

‚Ä¢ Adversarial

the

Success:

adversarial
evaluation strategy proposed by Kannan
(2017b).
and Vinyals (2017); Li et al.
Adversarial evaluation trains a discriminator
(or evaluator) function to labels dialogues
as
or
Positive
human-generated

machine-generated

(positive).

(negative)

5Thecombinationofstopwordsinhttps:
//www.ranks.nl/stopwordsandpunctuations.

Model
Human
AR
AR+MMI
AR+MMI+diverse
AR+MMI+RL
NonAR
NonAR+MMI

BLEU distinct-1

-
1.64
2.10
2.16
2.34
1.54
2.68

16.8%
3.7%
10.6%
16.0%
13.7%
8.9%
15.9%

distinct-2 Avg.length Stopword
14.2
6.4
7.2
7.5
7.3
7.1
7.4

69.8%
82.3%
76.4%
72.1%
73.0%
77.9%
71.9%

58.1%
9.5%
20.5%
27.3%
25.2%
14.6%
27.0%

adv succ

2.7%
6.3%
6.4%
8.0%
2.4%
9.2%

Table 1: Automatic Metrics Evaluation for Different Models.

examples are taken from training dialogues,
while negative examples are decoded using
generative models from a model. Adversarial
success is the percentage of the generated
the evaluator to
responses that can fool
believe that it is human-generated. We refer
readers to Li et al. (2017b) for more details
about the adversarial evaluation.

Results are shown in Table 1. When comparing
AR with AR+MMI, AR+MMI
signiÔ¨Åcantly
outperforms AR across all metrics, which is in
line with previous Ô¨Åndings (Li et al., 2016a).
For the variants of AR+MMI, AR+MMI+diverse
generates a more diverse N-best list for reranking,
and thus outperforms AR+MMI; AR+MMI+RL
uses
future
lookahead strategy to estimate
and thus outperforms
backward probability,
AR+MMI as well.
It‚Äôs hard to tell which
model performs better, AR or non-AR: AR
performs better than non-AR for BLEU and
adversarial success, but worse for
the other
metrics. This means comparing with AR model,
non-AR model tends to generate more diverse
responses, but might be less coherent. Because
of the ability to handle the non-local-optimality
issue, Non-AR+MMI consistently outperforms
AR+MMI by a large margin across all evaluation
metrics.
When comparing non-AR with
AR+MMI+diverse, non-AR has relatively lower
distinct score, but signiÔ¨Åcantly higher scores
BLEU and adversarial success. This is because
the diverse decoding strategy in AR sacriÔ¨Åces
language model probability for diversity, and thus
harms the BLEU score but promotes the diversity
score. NonAR+MMI outperforms AR+MMI+RL
across all metrics.

5.5 Examples

5.6 Qualitative Evaluation

We employed crowdsourced judges to provide
evaluations for a random sample of 1000 items
from the test set. Following protocols in Baheti
et al. (2018), we assigned each output to a human
judge, who were asked to score every model
response on a 5-point scale (Strongly Agree,
Agree, Unsure, Disagree, Strongly Disagree) on
2 categories: 1) Coherence - is the response
coherent to the given source?
and 2) Content
Richness - does the response add new information
to the conversation? Ratings were later collapsed
to 3 categories (Agree, Unsure, Disagree).

is better

For dialogue coherence,

The results for plausibility and content richness
of different models are presented in Table
the trend is
3.
that NonAR+MMI
than AR+MMI,
followed by AR and Non-AR. AR is slightly
better than Non-AR. For Content Richness, the
is signiÔ¨Åcantly better
proposed NonAR+MMI
than AR+MMI, and the gap is greater
than
dialogue coherence. This is because the N-best list
generated by the AR model tends to be dull and
generic, and the reranking model in AR+MMI can
help alleviate but cannot fully address this issue.
The output from the AR+MMI model is thus by
far less diverse than nonAR+MMI, which obtains
the MMI score for each generated token.

To verify the statistical signiÔ¨Åcance of the
reported results, we performed a pairwise
bootstrap test (Johnson, 2001; Berg-Kirkpatrick
et al., 2012) to compare the difference between
percentage of responses that were labeled as
yes. We computed p-values for non-AR+MMI
vs AR+MMI and non-AR vs AR. Regarding
non-AR vs AR, we did not Ô¨Ånd a signiÔ¨Åcant
difference (p-value = 0.18) for coherence, but a
signiÔ¨Åcant difference for content richness (p-value
< 0.01). For non-AR+MMI vs AR+MMI and

Input
it feels like i must have been asleep for
weeks.
where does she work?
who is in charge?
I am off all week next week.
why can ‚Äôt you just believe us?

can ‚Äôt you see how they re exploiting
you?
I mean, wed have to talk to him.
i ‚Äôm sorry to detain you for so long.

AR+MMI
i m sorry that i can ‚Äôt help you.

NonAR+MMI
Yeah, it has been long.

i don ‚Äôt know where she works.
I don ‚Äôt know sir.
what time you going?
I mean , it s not like we re gonna be
together forever .
i m not a human being .

she has work to do at the library.
i ‚Äôm the captain in charge!
then who will be in charge ?
because you are not a good person.

it ‚Äôs not that simple.

do you have any idea what caused the
explosion ?

i have no idea.

I mean, he ‚Äôs a good guy.
i ‚Äôm sorry i can ‚Äôt.

let ‚Äôs do it.
that ‚Äôs Ô¨Åne since i ‚Äôm not leaving until
tomorrow.
the explosion was caused by the failure
of the motor.

Table 2: Response generation: Sample responses using the diversity-promoting beam search and vanilla beam
search.

Model

Human
AR
AR+MMI
AR+MMI+diverse
AR+MMI+RL
nonAR
nonAR+MMI

Human
AR
AR+MMI
AR+MMI+diverse
AR+MMI+RL
NonAR
NonAR+MMI

disagr (%)
Coherence
17.4
28.6
25.3
24.8
24.1
29.9
23.1
Content Richness
14.0
38.2
30.6
23.9
26.4
31.4
24.2

un(%)

agr(%)

20.8
29.5
27.9
27.8
26.5
28.7
24.0

16.6
30.4
26.2
21.3
24.9
25.0
20.5

61.8
41.9
46.8
47.4
49.4
41.4
52.9

69.4
31.4
43.2
54.8
48.7
44.6
55.3

Table 3: Human judgments for Coherence and Content
Richeness of the different models.

AR+MMI+RL, we Ô¨Ånd a signiÔ¨Åcant difference
for both coherence (p-value < 0.01) and content
richness (p-value < 0.01). For non-AR+MMI
vs AR+MMI+RL, the difference for coherence is
signiÔ¨Åcant (p-value < 0.01), but content richness
is insigniÔ¨Åcant (p-value=0.25).

5.7 Sample Responses

Sample responses are presented in Table 2. As
can be seen, the nonAR+MMI tends to generate
more diverse and content-rich responses.
It is
also interesting to see that responses from the
AR+MMI model mostly start with the word ‚ÄúI
‚Äù. This is because of the fact that the N-best
list from the AR model lacks for diversity. The
preÔ¨Åxes of the responses are mostly the same and
the reranking process can only affect sufÔ¨Åxes. On
the contrary, for nonAR+MMI, MMI reranking is
performed once a token is generated, and does

not wait for the completion of the whole target
sequence, leading to more diverse and appropriate
responses.

5.8 Results on Machine Translation

Mutual information has been found to improve
machine translation, both in the context of NMT
models (Li and Jurafsky, 2016) and phrase-based
MT models (Och and Ney, 2002; Shen et al.,
2010). It would be interesting to see whether the
proposed model can also help non-AR NMT as
well. We evaluate the proposed method on the
three widely used machine translation benchmark
tasks (three datasets): WMT2014 De‚ÜíEn (4.5M
sentence pairs), WMT2014 En‚ÜíDe, WMT2016
Ro‚ÜíEn (610K sentence pairs) and IWSLT2014
De‚ÜíEn (150K sentence pairs).
We use
the Transformer (Vaswani et al., 2017a) as a
backbone. Knowledge Distillation is applied for
all models. Since building SOTA non-AR MT
models is out of the scope of this paper, we used
the commonly used NonAR structure described in
Section 4.2 as the backbone. Results are shown in
Table 4. As can be seen, the incorporation of MMI
model signiÔ¨Åcantly improves MT performances.
This shows that the proposed model has potentials
to beneÔ¨Åt a wide range of generation tasks.

6 Conclusion

to

we

this

propose

(non-AR)

generation

use
paper,
In
non-autoregressive
to
address the non-global optimality issue for MMI
in neural dialog generation. Target tokens are
generated independently in non-AR generation.
p(x|y) for each target word can thus be computed
as soon as it s generated, and does not have to wait

NAT (Gu et al., 2018)
iNAT (Lee et al., 2018)
FlowSeq-large (raw data) (Ma et al., 2019)
NAT (our implementation)
NAT +MMI

WMT14 En‚ÜíDe WMT14 De‚ÜíEn WMT16 Ro‚ÜíEn
20.62
25.43
25.40
24.83
26.05
(+1.22)

17.69
21.54
20.85
22.32
23.80
(+1.48)

29.79
29.32
29.86
29.93
30.50
(+0.57)

Table 4: The performances of NonAR+MMI methods on WMT14 En‚ÜîDe and WMT16 Ro‚ÜíEn. Results from
Gu et al. (2018); Lee et al. (2018); Ma et al. (2019) are copied from original papers for reference purposes.

for the completion of the whole sequence. This
naturally resolves the non-global optimal issue
in decoding. Experimental results demonstrate
that the proposed strategy produces more diverse,
coherent, and appropriate responses, yielding
substantive gains in BLEU scores and in human
evaluations.

References

Daniel Adiwardana, Minh-Thang Luong, David R.
So, Jamie Hall, Noah Fiedel, Romal Thoppilan,
Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade,
Yifeng Lu, and Quoc V. Le. 2020. Towards a
human-like open-domain chatbot.

Nabiha Asghar, Pasca Poupart, Jiang Xin, and Hang Li.
2016. Online sequence-to-sequence reinforcement
learning for open-domain conversational agents.
arXiv preprint arXiv:1612.03929.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Ashutosh Baheti, Alan Ritter, Jiwei Li, and Bill
Dolan. 2018. Generating more interesting responses
in neural conversation models with distributional
constraints. arXiv preprint arXiv:1809.01215.

Yu Bao, Hao Zhou,

Jiangtao Feng, Mingxuan
Wang, Shujian Huang, Jiajun Chen, and Lei Li.
2019. Non-autoregressive transformer by position
learning. arXiv preprint arXiv:1911.10677.

Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statistical
In Proceedings of the 2012
signiÔ¨Åcance in nlp.
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 995‚Äì1005. Association
for Computational Linguistics.

Antoine Bosselut, Asli Celikyilmaz, Xiaodong He,
Jianfeng Gao, Po-Sen Huang, and Yejin Choi. 2018.
Discourse-aware neural rewards for coherent text
generation. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pages
173‚Äì184, New Orleans, Louisiana. Association for
Computational Linguistics.

Peter F Brown, Vincent J Della Pietra, Stephen
A Della Pietra, and Robert L Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263‚Äì311.

Kyunghyun Cho. 2016. Noisy parallel approximate
decoding for conditional recurrent language model.
arXiv preprint arXiv:1605.03835.

Jianfeng Gao, Michel Galley, Lihong Li, et al.
2019.
Neural approaches to conversational ai.
Foundations and Trends R(cid:13) in Information Retrieval,
13(2-3):127‚Äì298.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
In Proceedings
sequence to sequence learning.
of the 34th International Conference on Machine
Learning-Volume 70, pages 1243‚Äì1252. JMLR. org.

Marjan Ghazvininejad, Chris Brockett, Ming-Wei
Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih,
and Michel Galley. 2018. A knowledge-grounded
neural conversation model. In Thirty-Second AAAI
Conference on ArtiÔ¨Åcial Intelligence.

Kartik

Chris

Goyal,

Taylor
Berg-Kirkpatrick. 2017. Differentiable scheduled
arXiv preprint
sampling for credit assignment.
arXiv:1704.06970.

Dyer,

and

Jiatao Gu,

James Bradbury, Caiming Xiong,
Victor O. K. Li, and Richard Socher. 2018.
Non-autoregressive neural machine
translation.
In 6th International Conference on Learning
Representations,
ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track
Proceedings.

Jiatao Gu, Kyunghyun Cho, and Victor OK Li.
2017. Trainable greedy decoding for neural machine
translation. arXiv preprint arXiv:1702.02429.

and Chris Callison-Burch.

Daphne Ippolito, Reno Kriz, Joao Sedoc, Maria
Kustikova,
2019.
Comparison of diverse decoding methods from
In Proceedings of
conditional language models.
the 57th Annual Meeting of the Association for
Computational Linguistics,
pages 3752‚Äì3762,
Florence,
Italy. Association for Computational
Linguistics.

Roger W Johnson. 2001. An introduction to the

bootstrap. Teaching Statistics, 23(2):49‚Äì54.

Anjuli Kannan and Oriol Vinyals. 2017. Adversarial
arXiv preprint

evaluation of dialogue models.
arXiv:1701.08198.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2017a.
arXiv

Learning to decode for future success.
preprint arXiv:1701.06549.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Six
Philipp Koehn and Rebecca Knowles. 2017.
In
challenges for neural machine translation.
the First Workshop on Neural
Proceedings of
Machine Translation, pages 28‚Äì39, Vancouver.
Association for Computational Linguistics.

Reno Kriz, JoÀúao Sedoc, Marianna Apidianaki, Carolina
Zheng, Gaurav Kumar, Eleni Miltsakaki, and Chris
Callison-Burch. 2019. Complexity-weighted loss
and diverse reranking for sentence simpliÔ¨Åcation.
In Proceedings of
the
the Association for
North American Chapter of
Computational Linguistics:
Human Language
Technologies, Volume 1 (Long and Short Papers),
pages
3137‚Äì3147, Minneapolis, Minnesota.
Association for Computational Linguistics.

the 2019 Conference of

Ilia Kulikov, Alexander H. Miller, Kyunghyun Cho,
and Jason Weston. 2018. Importance of search and
evaluation strategies in neural dialogue modeling.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
Deterministic non-autoregressive neural
2018.
sequence modeling by iterative reÔ¨Ånement.
In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
1173‚Äì1182, Brussels, Belgium. Association for
Computational Linguistics.

Mike Lewis, Denis Yarats, Yann Dauphin, Devi
Parikh, and Dhruv Batra. 2017. Deal or no deal?
end-to-end learning of negotiation dialogues.
In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
2443‚Äì2453, Copenhagen, Denmark. Association for
Computational Linguistics.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng
Gao, and Bill Dolan. 2016a. A diversity-promoting
objective function for neural conversation models.
In Proc. of NAACL-HLT.

Jiwei Li, Michel Galley, Chris Brockett, Georgios
Spithourakis, Jianfeng Gao, and Bill Dolan. 2016b.
In
A persona-based neural conversation model.
Proceedings of
the
the 54th Annual Meeting of
Association for Computational Linguistics (Volume
1: Long Papers), pages 994‚Äì1003, Berlin, Germany.

Jiwei Li and Dan Jurafsky. 2016. Mutual information
and diverse decoding improve neural machine
translation.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016c. A
simple, fast diverse decoding algorithm for neural
generation. arXiv preprint arXiv:1611.08562.

Jiwei Li, Will Monroe, Tianlin Shi, S¬¥ebastien Jean,
Alan Ritter, and Dan Jurafsky. 2017b. Adversarial
arXiv
learning for neural dialogue generation.
preprint arXiv:1701.06547.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation
arXiv
metrics for dialogue response generation.
preprint arXiv:1603.08023.

Xuezhe Ma, Chunting Zhou, Xian Li, Graham
Neubig, and Eduard Hovy. 2019.
Flowseq:
Non-autoregressive conditional sequence generation
with generative Ô¨Çow.

Hongyuan Mei, Mohit Bansal, and Matthew R Walter.
2016.
Coherent dialogue with attention-based
language models. arXiv preprint arXiv:1611.06997.

Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang,
and Zhi Jin. 2016.
Sequence to backward and
forward sequences: A content-introducing approach
to generative short-text conversation. arXiv preprint
arXiv:1607.00970.

Tong Niu and Mohit Bansal. 2018. Polite dialogue
generation without parallel data. Transactions of the
Association for Computational Linguistics, 6(0).

Tong Niu and Mohit Bansal. 2020. Avgout: A
simple output-probability measure to eliminate dull
responses. arXiv preprint arXiv:2001.05467.

Franz

for

Josef Och

and Hermann Ney.

2002.
Discriminative training and maximum entropy
translation.
statistical machine
models
the 40th annual meeting
In Proceedings of
on association for
linguistics,
computational
pages 295‚Äì302. Association for Computational
Linguistics.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
Blog, 1(8).

Iulian V Serban,

II Ororbia, G Alexander, Joelle
Pineau, and Aaron Courville. 2016a. Multi-modal
arXiv preprint
variational encoder-decoders.
arXiv:1612.00377.

Iulian V Serban, Alessandro Sordoni, Yoshua
Bengio, Aaron Courville,
and Joelle Pineau.
2016b. Building end-to-end dialogue systems using
generative hierarchical neural network models.
In Proceedings of the 30th AAAI Conference on
ArtiÔ¨Åcial Intelligence (AAAI-16).

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016c. Building
end-to-end dialogue
systems using generative
hierarchical neural network models. In Proceedings
of AAAI.

Iulian Vlad Serban, Ryan Lowe, Laurent Charlin,
and Joelle Pineau. 2016d. Generative deep neural
networks for dialogue: A short review.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016e. A hierarchical latent
variable encoder-decoder model
for generating
dialogues. arXiv preprint arXiv:1605.06069.

Yuanlong Shao, Stephan Gouws, Denny Britz,
Anna Goldie, Brian Strope, and Ray Kurzweil.
2017. Generating high-quality and informative
conversation responses with sequence-to-sequence
In Proceedings of the 2017 Conference
models.
in Natural Language
on Empirical Methods
Processing,
Copenhagen,
Denmark.
Computational
Linguistics.

pages
Association

2210‚Äì2219,

for

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
Self-attention with relative position

2018.
representations. arXiv preprint arXiv:1803.02155.

Libin Shen,
2010.
translation.
36(4):649‚Äì671.

Jinxi Xu,

and Ralph Weischedel.
String-to-dependency statistical machine
Linguistics,

Computational

Raphael Shu,

Jason Lee, Hideki Nakayama,
and Kyunghyun Cho. 2019.
Latent-variable
non-autoregressive neural machine translation with
deterministic inference using a delta posterior.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Meg Mitchell, Jian-Yun
Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural
network approach to context-sensitive generation
In Proceedings of
of conversational responses.
NAACL-HLT.

Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He,
Zi Lin, and Zhihong Deng. 2019. Fast structured
In Advances
decoding for sequence models.
in Neural Information Processing Systems, pages
3011‚Äì3020.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014. Sequence to sequence learning with neural
information
In Advances in neural
networks.
processing systems, pages 3104‚Äì3112.

Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song,
Yansong Feng, and Dongyan Zhao. 2017. How
to make context more useful? an empirical study
on context-aware neural conversational models. In
the
Proceedings of
Association for Computational Linguistics (Volume
2:
Short Papers), pages 231‚Äì236, Vancouver,
Canada. Association for Computational Linguistics.

the 55th Annual Meeting of

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz
Kaiser, and Illia Polosukhin. 2017a. Attention is all
you need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 30, pages 5998‚Äì6008. Curran
Associates, Inc.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017b. Attention is
In Advances in neural information
all you need.
processing systems, pages 5998‚Äì6008.

Ashwin K Vijayakumar, Michael Cogswell,
Ramprasath R Selvaraju, Qing Sun, Stefan
Lee, David Crandall, and Dhruv Batra. 2016.
Diverse beam search: Decoding diverse solutions
arXiv preprint
from neural sequence models.
arXiv:1610.02424.

Oriol Vinyals and Quoc Le. 2015.

A neural
In Proceedings of ICML

conversational model.
Deep Learning Workshop.

Di Wang, Nebojsa Jojic, Chris Brockett,
Steering output
response generation.

and
style
Eric Nyberg. 2017.
and topic in neural
In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
2140‚Äì2150, Copenhagen, Denmark. Association for
Computational Linguistics.

William Yang Wang, Jiwei Li, and Xiaodong He.
In
2018. Deep reinforcement learning for nlp.
Proceedings of
the
the 56th Annual Meeting of
Association for Computational Linguistics: Tutorial
Abstracts, pages 19‚Äì21.

Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,
Topic
Ming Zhou, and Wei-Ying Ma. 2017.
In Proceedings
aware neural response generation.
of the Thirty-First AAAI Conference on ArtiÔ¨Åcial
Intelligence, AAAI17, page 33513357. AAAI Press.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018.
Personalizing dialogue agents: I have a dog, do you
have pets too? arXiv preprint arXiv:1801.07243.

Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun
Jianfeng
and Bill Dolan. 2019.
Large-scale generative pre-training

Chen, Chris Brockett, Xiang Gao,
Gao,
Dialogpt:
for conversational response generation.

Jingjing Liu,

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
Learning discourse-level diversity for
2017.
neural dialog models using conditional variational
In Proceedings of the 55th Annual
autoencoders.
the Association for Computational
Meeting of
Long Papers), pages
(Volume 1:
Linguistics
654‚Äì664, Vancouver, Canada. Association for
Computational Linguistics.

