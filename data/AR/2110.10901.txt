A Fast Location Algorithm for Very Sparse Point
Clouds Based on Object Detection

1st Shiyu Fan
dept. name of organization (of Aff.)
name of organization (of Aff.)
City, Country
email address or ORCID

1
2
0
2

t
c
O
1
2

]

V
C
.
s
c
[

1
v
1
0
9
0
1
.
0
1
1
2
:
v
i
X
r
a

Abstract—Limited by the performance factor, it is arduous
to recognize target object and locate it in Augmented Reality
(AR) scenes on low-end mobile devices, especially which using
monocular cameras. In this paper, we proposed an algorithm
which can quickly locate the target object through image object
detection in the circumstances of having very sparse feature
points. We introduce YOLOv3-Tiny to our algorithm as the
object detection module to ﬁlter the possible points and using
Principal Component Analysis (PCA) to determine the location.
We conduct the experiment in a manually designed scene by
holding a smartphone and the results represent high positioning
speed and accuracy of our method.

Keywords—augment reality, object detection, YOLOv3-Tiny,

PCA

I. INTRODUCTION

Augmented Reality (AR) is a research hot spot on mobile
smart devices gradually. More developers utilize AR to de-
velop diverse mobile games and video effects. AR integrates
computer three-dimensional reconstruction with computer vi-
sion and establishes the mapping relationship between the real
world and the screen, so that the 3D models we want to draw
can be displayed on the screen as if they are attached to
real objects. In many circumstances, developer need detect
one speciﬁc target’s position in real world and transform it to
virtual rendering space. Owing to the rapid development of
Simultaneous Localization and Mapping (SLAM) and com-
puter vision, researchers apply a variety of different method
to locate the target.

In most of SLAM system, the plane can be easily identiﬁed
and tracked. In this case, developer can put object on the
plane or hang something on the wall. Meanwhile, researchers
develop other method to detect object without regular shape.
AR marker is a However, no matter marker system or scanning
system, most of current target location solutions rely on extra
markup or preprocessing of the scene. These solutions cannot
handle generalized or diverse objects moreover.

To solving the problem that mentioned before, we propose a
new method which can be applied easily in mobile device. The
process of this method combines object detection algorithm
based on neural network with Principal Component Analysis
(PCA) and it concludes three main parts. Firstly, the object
recognition network outputs the bounding box of target from
the camera stream. Then, the sparse feature points extracted
by SLAM are projected to the screen space and the bounding

box ﬁlter out the effective points. The last and most important
step is applying PCA to calculate the approximate location
and direction.

This method is test on the real scene experiment captured
by mobile device and the result veriﬁes the feasibility of this
method. It indicates that our system can successfully recognize
the target we set and locate the transform matrix of the target.

II. RELATED WORK

For target detection and positioning, the current AR system
proposes different solutions according to its own characteris-
tics. For the detection and positioning of ﬂat objects, most of
the algorithms in the industry are template matching for the
feature points of ﬂat objects. These methods extract the SIFT []
feature points from target and calculate the transformation ma-
trix through the deformation of target to realize the positioning
function []. Another way of target recognition and positioning
is through learning. The point cloud information generated by
slam can be used by neural networks. In recent years, a large
number of networks for target detection and location based on
the point cloud generated by lidar have been proposed, e.g.
PointNet [], MV3D [] and Point RCNN []. However, all these
methods rely on the LiDAR.

In this paper, We used an image-based approach for target
positioning without any depth information. Object detection
algorithms has two types: one is traditional machine learning
type and the other is deep neural network. For those using
neural network, they can be divided into one-stage and two-
stage. Two-stage refers to the detection algorithm that needs
to be completed in two steps. First, the proposal areas need
to be cropped from the original image and then the network
classify all these areas to select the best match. Algorithms
grounded on this principle include R-CNN [], Faster R-CNN
[] and so on. However, due to the huge cost for classifying all
regions, the hardware resources requirement for this type of
algorithm are relatively high, even Faster R-CNN operate at
speed, which means it cannot be applied for the real-time
operation. One-stage detectors cut down the step of generating
proposal region and extract image features only once, SSD []
and YOLO [] are the most typical examples.YOLO divide one
image into several grids, each grid is responsible for predicting
the target whose center is inside this grid. This means that each
grid predicts several bounding box’s position and conﬁdence

 
 
 
 
 
 
at the same time. The ﬁrst generation of YOLO has the defect
of poor detection of small
targets. In subsequent version,
YOLO replace the backbone from VGG-16 to DarkNet-19
and improve the performance of speed and accuracy []. In
YOLOv3 [], the ideal of residual network is introduced and
three feature maps of different scales are used for object
detection.

III. METHODOLOGY

The main process of our method is shown as in Fig.1. We
use ARkit in our pipeline to point clouds generation. Note that
other monocular SLAM like ARCore can also be used in this
stage.

Fig. 1. Example of a ﬁgure caption.

The object detection module-YOLOv3-Tiny is modiﬁed by
YOLOv3, which is a quiet large network and not suitable for
the mobile platform. In this case YOLOv3-Tiny has implement
the following changes. First,the backbone of YOLOv3-Tiny
is simpliﬁed. The original backbone network implemented
in YOLOv3 is DarkNet-53, which has 53 convolution lay-
ers. In YOLOv3-Tiny,
the structure of backbone is a 7-
layers of convolution and max-pooling. Secondly, comparing
to YOLOv3, YOLOv3-Tiny only retains two independence
prediction branches with 16x16 and 32x32 resolutions. Fur-
thermore, YOLOv3-Tiny apply quantization strategy to reduce
the BFLOPS. In this network, the weights are stored in low
precision as INT8 type. Comparing to the YOLOv3-SPP3,
YOLOv3-Tiny successfully decrease parameter’s amount from
63.9M to 8.7M. When with the same input size as 412, the
BFLOPS of YOLOv3-Tiny is 7.69% of YOLOv3 has.

TABLE I
TABLE TYPE STYLES

Model
Name
YOLOv3
YOLOv3-Tiny INT8

Input
Size
416x416
416x416

Parameter
Size
65.3M
8.7MB

Model
Size
263MB
8.9MB

FPS

5.32
222.59

Trained YoloV3-Tiny network return the target box in image
coordinate system. The coordinate of this box can be easily
transformed from image coordinate system to a normalized
device coordinate(NDC) system. The

Meanwhile, the world coordinates of the sparse point cloud
can be converted into the screen coordinate system. With the
following steps, we can ﬁlter the possible point cloud data
related to the target object. The transformation of an object
from world coordinates to NDC space needs to undergo cam-
era view matrix Mview transformation, orthogonal projection
matrix MOtho,norm transformation and perspective projection
Mproj matrix transformation. Assuming P is one point of the
cloud which has the world coordinate Pworld(xw, yw, zw) Mp.
Generally, point P ’s position in NDC space can be obtained
by the following formula:

Pndc = MprojMotho,normM −1

viewPw

(1)

After calculating all the raw points position of NDC coor-
dinate, the points inside the target box will be analyzed in the
next step.

Following the previous steps, we get the point cloud set
W {P1, P2, P3, ..., Pn} located in the target box. The center
C of target object can be obtained by the mean of all points‘
coordinate in W set.

(xc, yc, zc) = ( P

n
1 xn
n

n
1 yn
, P
n

, P

n
1 zn
n

)

(2)

With the center of the point cloud data, we can use PCA to
calculate the principal component vector of the point cloud
data, and to roughly calculate the direction vector of the point
cloud. To ensure that the ﬁrst principal component is the
direction of maximum variance, data matrix Xw ∈ R3×n.

Xw =

x1 − xc x2 − xc
y2 − yc
y1 − yc
z2 − zc
z1 − zc





... xn − xc
yn − yc
...
zn − zc
...





(3)

By calculating the covariance matrix, and then according to
the three eigenvectors of the covariance matrix, the deﬂection
angle of the point cloud data can be determined.

Σw ∈ R3×3

=

1
n − 1
|Σw − λI| = 0

XwX T
w

(4)

(5)

The three λ1, λ2 and λ3 of matrix Σw obtained by the solution
correspond to the three eigenvectors λ1, λ2 and λ3.

The above operations can also be replaced by singular value

decomposition(SVD):

Xw = U ΣV T

(6)

where U , Σ are , and V is the combination of λ1, λ2 and λ3.
In the process of using the above algorithm, the following
issues need to be pay attention. First, due to the RGB cameras
used by most mobile devices, point clouds require dozens
of frames to be created. This means that
there must be
enough target feature points to be generated before starting

the process. In this paper, we set a threshold N of feature
points’ number for starting detection process. Secondly, since
the eigenvectorsr can take two different values, we need to set
the value range of the vector in advance.

IV. IMPLEMENTATION

With our system integrated on IOS Scenekit platform,

V. EXPERIMENT RESULT

VI. LIMITATIONS

Due to the limitations of the algorithm, our method only
works well on regular objects with certain directivity. Other-
wise, the scale of YOLOv3-Tiny is too large for some mobile
devices that it cannot perform real-time object detection.

VII. CONCLUSION

On most mobile platforms, the point cloud generated by the
AR system is extremely sparse and it is completely impossible
to obtain effective target information from these point cloud
data. The method we propose in this paper can successfully
recognize the target on mobile devices and acquire the position
of the target in the rendering world space coordinate system
and the possible rotation angle.

Continue...

[1] empty

REFERENCES

REFERENCES

This figure "fig1.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/2110.10901v1

