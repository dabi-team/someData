2
2
0
2

p
e
S
2
2

]
E
M

.
t
a
t
s
[

2
v
3
8
7
6
0
.
9
0
2
2
:
v
i
X
r
a

Sources of residual autocorrelation in multiband task fMRI and
strategies for eﬀective mitigation

Fatma Parlak1, Damon D. Pham1, Daniel A. Spencer1, Robert C. Welsh2, and Amanda
F. Mejia ∗1

1Department of Statistics, Indiana University, Bloomington, IN, USA
2Department of Psychiatry and Biobehavioral Sciences, Los Angeles, CA, USA

Abstract

Analysis of task fMRI studies is typically based on using ordinary least squares within a voxel-
or vertex-wise linear regression framework known as the general linear model. This use produces
estimates and standard errors of the regression coeﬃcients representing amplitudes of task-induced
activations. To produce valid statistical inferences, several key statistical assumptions must be met,
including that of independent residuals. Since task fMRI residuals often exhibit temporal autocorre-
lation, it is common practice to perform “prewhitening” to mitigate that dependence. Prewhitening
involves estimating the residual correlation structure and then applying a ﬁlter to induce residual
temporal independence. While theoretically straightforward, a major challenge in prewhitening for
fMRI data is accurately estimating the residual autocorrelation at each voxel or vertex of the brain.
Assuming a global model for autocorrelation, which is the default in several standard fMRI soft-
ware tools, may under- or over-whiten in certain areas and produce diﬀerential false positive control
across the brain. The increasing popularity of multiband acquisitions with faster temporal resolution
increases the challenge of eﬀective prewhitening because more complex models are required to accu-
rately capture the strength and structure of autocorrelation. These issues are becoming more critical
now because of a trend towards subject-level analysis and inference.
In group-average or group-
diﬀerence analyses, the within-subject residual correlation structure is accounted for implicitly, so
inadequate prewhitening is of little real consequence. For individual subject inference, however,
accurate prewhitening is crucial to avoid inﬂated or spatially variable false positive rates. In this
paper, we ﬁrst thoroughly examine the patterns, sources and strength of residual autocorrelation in
multiband task fMRI data. We ﬁnd that residual autocorrelation exhibits marked spatial variance
across the cortex and is inﬂuenced by many factors including the task being performed, the speciﬁc
acquisition protocol, mis-modeling of the hemodynamic response function, unmodeled noise due to
subject head motion, and systematic individual diﬀerences. Second, we evaluate the ability of diﬀer-
ent autoregressive (AR) model-based prewhitening strategies to eﬀectively mitigate autocorrelation
and control false positives. We consider two main factors: the choice of AR model order and the level
of spatial regularization of AR model coeﬃcients, ranging from local smoothing to global averaging.
We also consider determining the AR model order optimally at every vertex, but we do not observe
an additional beneﬁt of this over the use of higher-order AR models (e.g. (AR(6)). We ﬁnd that lo-
cal regularization is much more eﬀective than global averaging at mitigating autocorrelation. While
increasing the AR model order is also helpful, it has a lesser eﬀect than allowing AR coeﬃcients to
vary spatially. We ﬁnd that prewhitening with an AR(6) model with local regularization is eﬀective
at reducing or even eliminating autocorrelation and controlling false positives. To overcome the com-
putational challenge associated with spatially variable prewhitening, we developed a computationally
eﬃcient R implementation using parallelization and fast C++ backend code. This implementation
is included in the open source R package BayesfMRI.

Keywords:
acquisition, surface-based analysis

temporal autocorrelation, prewhitening, false positive control, task fMRI analysis, multi-band

∗Corresponding author: Amanda F. Mejia, mandy.mejia@gmail.com

1

 
 
 
 
 
 
1

Introduction

The general linear model (GLM) has long been a popular framework for the analysis of task functional magnetic
resonance imaging (fMRI) data.
In the GLM, a linear regression model is used to relate the observed blood
oxygenation level dependent (BOLD) signal to the expected BOLD response due to each task or stimulus in
the experiment, along with nuisance regressors, yielding an estimate of the activation across the brain due to
each task (Friston et al. 1994). Hypothesis testing with multiplicity correction is used to determine areas of
the brain that are signiﬁcantly activated in response to each task or contrast. One well-known issue with the
GLM approach is that BOLD data generally violates the ordinary least squares (OLS) assumption of residual
independence (Lindquist 2008, Monti 2011). When this happens, standard errors associated with the model
coeﬃcients are biased, invalidating inference and generally giving rise to inﬂated false positive rates for areas of
activation.

Violations of residual independence are most consequential for subject-level inference (the “ﬁrst-level GLM”),
since OLS-based group inference (the “second-level GLM”) has been found to be relatively robust to dependent
errors in the ﬁrst level (Mumford and Nichols 2009). While group-level analysis has historically been the norm
in fMRI studies, more recently subject-level analysis is gaining in relevance. This in part due to the rise of
“highly sampled” datasets collecting lots of data on individual subjects (Laumann et al. 2015, Choe et al. 2015,
Braga and Buckner 2017, Gordon et al. 2017), as well as growing interest in using fMRI data for biomarker
discovery, clinical translation, and other contexts where robust and reliable subject-level measures are required.
Unfortunately, fMRI data presents several challenges for proper statistical analysis (Monti 2011), and subject-
level task fMRI measures have been shown to be unreliable (Elliott et al. 2020). One important factor for reliable
subject-level task fMRI analysis is dealing appropriately with temporal dependence to avoid inﬂated rates of false
positives.

Generalized least squares (GLS) is a regression framework that accounts for dependent and/or heteroskedastic
errors. Brieﬂy, in a regression model y = Xβ + (cid:15), (cid:15) ∼ N (0, V), assume that the residual covariance matrix V is
known. Then the GLS solution is ˆβGLS = (X(cid:48)WX)−1X(cid:48)Wy, where W = V−1, and V ar( ˆβGLS) = (X(cid:48)WX)−1.
This is mathematically equivalent to pre-multiplying both sides of the regression equation by V−1/2, which
induces independent and homoskedastic residuals and gives rise to an OLS solution of the same form as ˆβGLS. In
the context of task fMRI analysis, such “prewhitening” is a common remedy to eliminate temporal dependence
as it produces the best linear unbiased estimate (BLUE) of model coeﬃcients (Bullmore et al. 1996). A key
challenge in GLS analysis is determining the form of V, which is not actually known in practice. In a conventional
statistical analysis, an iterative approach to estimating V is commonly used through iteratively reweighted least
squares (IRLS). However, in fMRI analysis this is typically considered computationally prohibitive and prone
to overﬁtting, so V is often estimated in a single pass based on the OLS residuals (Woolrich et al. 2001). To
constrain the estimation of V, a parametric form is typically assumed based on the temporal structure of the data,
such as an autoregressive (AR) or AR moving average (ARMA) model. It is also common practice to regularize
the AR or ARMA model parameters by smoothing or averaging across the brain or within tissue boundaries.

Prewhitening methods are implemented in the major software packages AFNI (Cox 1996), FSL (Jenkinson et al.
2012) and SPM (Penny et al. 2011). Yet, many of these standard prewhitening techniques have received criticism
for failing to eﬀectively remove residual autocorrelation (Worsley et al. 2002, Eklund et al. 2012). These criticisms
have pointed to two main sources of mismodelled residual autocorrelation: (1) use of overly parsimonious auto-
correlation models that fail to fully capture the autocorrelation in the data—an issue of ever-increasing relevance
with the rise of faster multi-band acquisitions—and/or (2) assuming the same degree of residual autocorrelation
across the brain. Olszowy et al. (2019) performed a systematic comparison of prewhitening techniques imple-
mented in SPM, AFNI and FSL using several task and rest datasets of varying repetition time (TR) between
0.645s and 3s. They found that, while some techniques clearly performed better than others, all failed to control
false positives at the nominal level, especially for low TR data. The best performance was seen using AFNI, which
assumes a ﬁrst-order autoregressive moving average (ARMA) model with unsmoothed, spatially varying coeﬃ-
cients, and the FAST option in SPM, which employs a ﬂexible but global model using a dictionary of covariance
components (Corbin et al. 2018). Interestingly, these two methods represent opposite approaches: AFNI allows
for spatially varying autocorrelation but uses a relatively restrictive ARMA(1,1) model, while SPM FAST uses
a quite ﬂexible temporal correlation model but imposes a restrictive global assumption. Neither AFNI, FSL or
SPM currently oﬀers a prewhitening technique that provides for a ﬂexible and spatially varying autocorrelation
model. Therefore, the ability to fully account for residual autocorrelation remains a limitation of many ﬁrst-level
task fMRI analyses.

Several recent studies have considered the ability of higher-order autoregressive models to adequately capture
residual autocorrelation in fast TR fMRI data (Bollmann et al. 2018, Chen et al. 2019, Luo et al. 2020). Bollmann

2

et al. (2018) found that optimal AR model order and AR coeﬃcient magnitude varied markedly across the brain
in fast TR task fMRI data, and that physiological noise modeling reduced but by no means eliminated the spatial
variability in residual autocorrelation or the need for a high AR model order. Luo et al. (2020) used resting-state
fMRI data of varying sub-second TRs to examine false positives rates with assumed task paradigms. They found
that the optimal AR model order varied spatially and depended on TR, with faster TR requiring a higher AR
model order. They found that a too-low or too-high AR model order resulted in inﬂated false positive rates, and
that a global model order (even with spatially varying coeﬃcients) performed worse than when model order was
allowed to vary spatially. Their approach of allowing the AR model order to vary spatially also outperformed
both SPM FAST and the ARMA(1,1) model used by AFNI, the two methods found to have the best performance
by Olszowy et al. (2019).

These recent studies also highlight several challenges associated with the use of volumetric fMRI in prewhitening.
Both Bollmann et al. (2018) and Luo et al. (2020) observed sharp diﬀerences in the strength of residual auto-
correlation across tissue classes, with cerebral spinal ﬂuid (CSF) exhibiting much stronger autocorrelation than
gray matter, and white matter exhibiting relatively low autocorrelation. Because of this, they point out that the
standard practice of spatial smoothing (of the data, of the AR model order, or of the AR coeﬃcient estimates)
may be problematic at tissue class boundaries: gray matter bordering CSF may have higher autocorrelation
due to mixing with CSF signals, while gray matter bordering white matter may have decreased autocorrelation
due to mixing with white matter signals.
Indeed, Luo et al. (2020) found smoothing of the sample autocor-
relations at 6mm FWHM to result in inﬂated false positive rates. Yet some regularization of autocorrelation
model parameters is believed to be necessary to avoid very noisy estimates (Worsley et al. 2002, Bollmann et al.
2018, Chen et al. 2019), and data smoothing is nearly universal practice in the massive univariate framework,
given its ability to enhance signal-to-noise ratio (SNR) and increase power to detect activations. This presents a
dilemma: smoothing across tissue classes can be detrimental for autocorrelation modeling, but regularization of
autocorrelation coeﬃcients is needed to avoid overly noisy estimates.

The use of cortical surface fMRI (cs-fMRI) could mitigate this dilemma in two ways. First, geodesic smoothing
along the surface can increase SNR without blurring across tissue classes or neighboring sulcal folds. Second, by
eliminating white matter and CSF, the spatial variability in residual autocorrelation is simpliﬁed, since the most
dramatic spatial diﬀerences have been observed between tissue classes. An additional potential beneﬁt of the use
of cs-fMRI is the utility of spatial Bayesian models, which cs-fMRI is uniquely suited for (Mejia et al. 2020), to
spatially regularize autocorrelation coeﬃcients in a statistically principled way. Therefore, in this work we adopt
cortical surface format fMRI.

In this work, we advance prewhitening methods for modern fMRI acquisitions in three ways. First, we thoroughly
examine the spatial variability and inﬂuence of various factors on residual autocorrelation, including the task
protocol, the acquisition technique, and systematic individual diﬀerences. We also examine the inﬂuence of
potential model mis-speciﬁcation for the GLM. Un-modelled neural activity is temporally correlated and may
be absorbed into the model residuals, thus increasing residual autocorrelation (Lindquist et al. 2009, Bollmann
et al. 2018). Task-related neural activity is assumed to be captured through task regressors, which are typically
constructed by convolving a hemodynamic response function (HRF) with a stick function representing the task
paradigm. However, the shape and duration of the HRF are known to vary across the brain (Lindquist and Wager
2007) as well as within and across individuals (Aguirre et al. 1998). Therefore, assuming a ﬁxed “canonical”
HRF may fail to adequately capture task-induced activity (Lindquist et al. 2009). More ﬂexible models can
capture diﬀerences in HRF height, width, and time to peak, including the model with temporal and dispersion
derivatives (Friston et al. 1998b), the ﬁnite impulse response model (Glover 1999), and the inverse logit model
(Lindquist and Wager 2007). Here, we consider the eﬀect of including the temporal and/or dispersion derivatives
of the HRF on autocorrelation. Another potential source of model mis-speciﬁcation is failure to account for noise
resulting from head motion, scanner drift, and other sources. If such noise is not modeled, it will be reﬂected in
the model residuals. Because such sources of noise tend to be temporally correlated, this will tend have the eﬀect
of increasing residual autocorrelation. Here, we consider the eﬀect of including more or fewer head motion-based
regressors on residual autocorrelation.

Second, we evaluate the eﬀectiveness of diﬀerent autoregressive (AR) model-based prewhitening strategies at
reducing autocorrelation and controlling false positive rates. We consider AR model order varying from 1 to
6, as well as determining the AR model order optimally at each vertex. We also consider local regularization
of AR model coeﬃcients versus global averaging. We ﬁnd that local surface-based regularization of AR model
coeﬃcients is much more eﬀective than a global prewhitening strategy at eliminating autocorrelation across the
cortex.

Third, we overcome the major computational challenges associated with spatially-varying prewhitening. We have
developed a computationally eﬃcient implementation of the AR-based prewhitening techniques considered here.

3

Using parallelization and backend code written in C++, we are able to perform spatially varying prewhitening
very eﬃciently for surface-based analysis and “grayordinates” analysis more generally. This implementation is
available in the open-source BayesfMRI R package (Mejia et al. 2022).

The remainder of this paper is organized as follows.
In Section 2 we describe the data, the GLM approach,
and the methods for autocorrelation estimation, and prewhitening. We also describe a mixed eﬀect modeling
framework we use to assess the inﬂuence of several key factors on the strength and spatial variability of residual
autocorrelation, including acquisition method, task protocol, modeling choices, and individual variability.
In
Section 3 we present results based on an analysis of several task and resting state fMRI studies from the Human
Connectome Project, utilizing 40 subjects with test-retest data. In Section 4, we conclude with a discussion of
these results and what they suggest for future research in prewhitening.

2 Materials and methods

2.1 Data Collection and Processing

The data used in this paper are from the Human Connectome Project (HCP) 1200-subject release
(http://humanconnectome.org). The HCP includes task and resting-state fMRI data collected on a customized
Siemens 3T Skyra scanner with a multiband factor of 8 to provide high spatial (2mm isotropic voxels) and
temporal (TR = 0.72s) resolution (Van Essen et al. 2013). The fMRI data were processed according to the
HCP minimal preprocessing pipelines including projection to the cortical surface, as described in Glasser et al.
(2013). The resulting surface mesh for each hemisphere consists of approximately 32,000 vertices. For all fMRI
scans, we perform surface-based spatial smoothing using a 2-dimensional Gaussian kernel with 6mm full-width-at-
half-maximum (FWHM). To reduce the computational burden of estimating the mixed-eﬀects models described
below, prior to smoothing, we resample to approximately 6,000 vertices per hemisphere. Note that this level of
resampling results in a much milder degree of interpolation than smoothing at 6mm FWHM, and therefore results
in a negligible loss of information when performed in combination with smoothing (see, e.g., Mejia et al. (2020)
Appendix Figure C4). For both resampling and smoothing, we employ the Connectome Workbench (Marcus
et al. 2011) via the ciftiTools R package (Pham et al. 2022).

Each subject underwent several task and resting-state fMRI protocols across two sessions. Each task and rest
session was performed twice, using opposite phase encoding directions (LR and RL). For a subset of 45 partici-
pants, the entire imaging protocol was repeated. We analyze data from the 40 participants having a complete set
of test and retest data for the protocols we analyze in this study. We analyze four task experiments, namely the
emotion, gambling, motor, and relational tasks (Table 1). In the emotion processing task, developed by Hariri
et al. (2002), participants are shown sets of faces or geometric shapes, and are asked to determine which of two
faces/shapes match a reference face/shape. Each face has an angry or fearful expression. A 3s cue (”shape” or
”face”) precedes a block of 6 trials, lasting 18s in total. Each run includes three blocks of each condition (shape
or face).

In the gambling task, adopted from Delgado et al. (2000), participants play a game in which they are asked to
guess the value of a mystery card to win or lose money. They indicate their guess for the value, which can range
from 1 to 9, as being more or less than 5. Their response is evaluated by a program which predetermines whether
the trial is a win, loss, or neutral event. In each run, there are 2 mostly win blocks (6 win trials and 2 non-win
trials), 2 mostly loss blocks (6 loss trials and 2 non-loss trials), alternating with 4 15s ﬁxation blocks. While
this protocol can be considered a block design since the task protocol is comprised of short events rather than
continuous blocks of stimulus, we analyze it as an event-related design.

In the motor task, developed by Buckner et al. (2011), participants are given a 3s visual cue which instructs them
to perform one of ﬁve motor tasks: tap left or right ﬁngers, wiggle left or right toes, or move tongue. Each task
block lasts 12 seconds, and each run includes two blocks of each task as well as three 15s ﬁxation blocks.

In the relational task, developed by Smith Rachelle and Kalina (2007), subjects undergo two conditions: relational
processing and control matching. In the relational condition, one pair of objects is shown at the top of the screen
and another pair is shown at the bottom of the screen. In this condition, participants are asked to determine the
dimension (shape or texture) across which the pair displayed at the top diﬀers. Next, they determine whether
the bottom pair diﬀers along the same dimension. During the matching condition, two objects are displayed at
the top of the screen and one is shown at the bottom, and the word ”shape” or ”texture” appears in the middle
of the screen. In this condition, participants are asked to determine whether the bottom object matches either

4

of the top objects, based on the dimension displayed in the middle. Each condition is administered as blocks of
trials of the same condition, with each block lasting 18s total, with three blocks of each condition (relational,
matching and ﬁxation block) in each run.

To quantify false positive rates, we also analyze resting-state fMRI data acquired for the same subjects and
sessions, which we analyze under a false task protocol. To emulate the duration of the task fMRI runs, we
truncate the resting-state runs to have 284 volumes, the same number of volumes as the motor task, after
dropping the ﬁrst 15 rest volumes. The boxcar design consists of a single event with three “boxcars”: three
periods of stimulus lasting ten seconds each, with ten seconds in between each consecutive stimulus. The ﬁrst
stimulus begins at the 20th second, or approximately the 28th volume. We use only three boxcars instead of
extending the boxcars to the duration of the scan, in order to more closely resemble the number of stimuli in the
HCP task scans. We used the same GLM model as with the task analysis, except we did not include any HRF
derivatives since there is no true task-evoked signal to potentially mis-model.

Table 1: Type and duration of each task protocol analyzed. Adopted from Barch et al. (2013).

Task
Emotion
Gambling
Motor
Relational

Block or Event Frames per run Run duration (mm:ss)

Block
Event
Block
Block

176
253
284
232

2:16
3:12
3:34
2:56

2.2 Statistical Analysis

Our analysis consists of three primary steps. First, for each subject, session, task, acquisition protocol, HRF
modeling strategy, and motion regression strategy, we ﬁt a vertex-wise general linear model (GLM) to estimate
the amplitude of task-evoked activation assuming residual independence. Based on the ﬁtted residuals, we
estimate the degree of autocorrelation at every location in the brain. Second, we ﬁt a series of mixed eﬀects
models to identify the eﬀects of acquisition and modeling factors on residual autocorrelation across the brain, as
well as systematic individual variability. Finally, we prewhiten the data using a range of strategies, varying the
parametric model order and spatial regularization level. We evaluate the ability of each prewhitening strategy
to eﬀectively mitigate autocorrelation and control false positives.

2.2.1 GLM Estimation

We ﬁrst ﬁt a series of GLMs to each task fMRI dataset assuming residual independence in order to quantify
residual autocorrelation and examine its patterns and sources. Let yv be the BOLD response at vertex v, and let
X be a design matrix containing an intercept, task-related regressors, and nuisance regressors. For each vertex
v, the GLM proposed by Friston et al. (1994) can be written as:

yv = Xβv + (cid:15)v,

(cid:15)v ∼ M V N (0, Σv).

(1)

Σv (T × T ) encodes the residual autocorrelation and variance, which may diﬀer across the brain. If Σv (cid:54)= σ2
vI,
the OLS assumption of residual independence is violated, and a generalized least squares (GLS) approach is
appropriate in place of OLS to improve estimation eﬃciency and to avoid invalid statistical inference. In some
cases, such as in spatial Bayesian variants of the GLM where a single Bayesian linear model is ﬁt to all vertices,
spatially homogeneous variance may also be assumed, i.e. σ2
v ≡ σ2. GLS can be used to satisfy this assumption
by inducing unit variance across the brain. In GLS, OLS is ﬁrst used to obtain an initial set of ﬁtted residuals
ˆ(cid:15)v, which are utilized to estimate Σv (Kariya and Kurata 2004). The GLS coeﬃcient estimates are given by
ˆβGLS
v X)−1. Equivalently, prewhitening
v
involves pre-multiplying both sides of the regression equation (1) by Wv = Σ−1/2
to induce residual independence.
Traditionally this process is repeated until convergence, but due to computational considerations a single iteration
is often assumed to be suﬃcient for task fMRI analysis (Woolrich et al. 2001).

v y, and their covariance is V ar( ˆβGLS

v X)−1X(cid:48)Σ−1

) = (X(cid:48)Σ−1

= (X(cid:48)Σ−1

v

v

To avoid overly noisy estimates of Σv, restrictive parametric models (e.g., low-order autoregressive models) and/or
aggressive regularization (e.g., averaging across all gray matter) are often used to estimate Σv. Recent work has
suggested that these approaches generally fail to fully account for autocorrelation or control false positives as a
nominal rate (Luo et al. 2020, Olszowy et al. 2019, Chen et al. 2019, Corbin et al. 2018, Bollmann et al. 2018).

5

The challenge is how to produce a suﬃciently eﬃcient estimate of Σv while accurately representing the diﬀerential
levels of autocorrelation across the brain. In Section 2.2.3, we consider various strategies for estimation of Wv =
Σ−1/2
. However, our ﬁrst step is to examine the sources and patterns of residual autocorrelation, and therefore we
v
do not impose any parametric model or regularization in estimating Σv. Instead, we use empirical autocorrelation
function (ACF) at each vertex. Consider the timeseries of ﬁtted OLS residuals ev = yv −X(X(cid:48)X)−1X(cid:48)y at vertex
v for a particular fMRI dataset. The ACF of ev at lag u is deﬁned as

ρv,u =

Cov(ev,t, ev,t+u)
(cid:112)V ar(ev,t)V ar(ev,t+u)

,

for u = 0, . . . , T − 1, with lag-0 ACF ρv,0 = 1 (Venables and Ripley 2013). We summarize the ACF ρv,u into a
single metric of autocorrelation, the autocorrelation index (ACI) (Afyouni et al. 2019), which is given by

τ (v) =

T −1
(cid:88)

u=0

ρ2
u(v).

We consider the eﬀect of two potential sources of model misspeciﬁcation on temporal autocorrelation: unmodeled
neuronal activity via the task regressors in X, and unmodeled head motion-induced noise via the nuisance
regressors in X. Since both neuronal activity and motion-induced noise exhibit temporal dependence, failing to
adequately account for either may contribute to residual autocorrelation. The task regressors in X are constructed
by convolving a stimulus function representing the timing of the tasks or stimuli with a canonical HRF, which is
typically modeled as a gamma function or a diﬀerence of two gamma functions (Worsley et al. 2002). However,
HRF onset and duration is known to vary across the brain and across individuals (Aguirre et al. 1998), so using
a ﬁxed HRF may fail to accurately capture the task-evoked BOLD signal (Loh et al. 2008, Lindquist et al.
2009). Therefore, we consider three models for the HRF: one assuming a ﬁxed canonical HRF; one including
the temporal derivative of the HRF to allow for diﬀerences in HRF onset timing; and one additionally including
its dispersion derivative to allow for diﬀerences in HRF duration (Friston et al. 1998a, Lindquist et al. 2009).
Regarding nuisance regressors, the inclusion of measures of head motion is a common practice to account for
head motion-induced noise in the data. We therefore consider two sets of motion regressors: the 6 rigid body
realignment parameters and their one-back diﬀerences (RP12) or those terms plus their squares (RP24). In all
models, we include discrete cosine transform (DCT) bases to achieve high-pass ﬁltering at 0.01 Hz, which is
important to satisfy the stationarity assumption of AR-based prewhitening.

In sum, we estimate a vertex-wise GLM via OLS for each subject i = 1, . . . , 40, session j = 1, 2, task k =
1, 2, 3, 4, and phase encoding direction ∈ {LR,RL}. Each GLM is ﬁt using the canonical HRF only, with its
temporal derivative, and with its temporal and dispersion derivatives. Each GLM is also ﬁt with 12 or 24 motion
realignment parameters. In total, we ﬁt 3,840 GLMs (40 × 2 × 4 × 2 × 3 × 2) before prewhitening. All models are
ﬁt using the BayesfMRI R package Mejia et al. (2022). In the next section, we describe the mixed eﬀects modeling
framework we use to disentangle the inﬂuence of each factor (e.g. subject eﬀects versus acquisition eﬀects versus
modeling eﬀects) on residual autocorrelation.

2.2.2 Examining Sources of Residual Autocorrelation through Mixed Eﬀects Modeling

Let τ hr(cid:96)
ijk (v) be the ACI at vertex v for subject i, session j, task k, phase encoding direction (cid:96), HRF modeling
strategy h and motion regression strategy r. To determine the inﬂuences of population variability, spatial vari-
ability and other factors on ACI, we ﬁt a mixed eﬀect model at each vertex. We include ﬁxed eﬀects for each
task, for the interaction between task and HRF modeling strategy, and for the motion regression strategy. For
each of these ﬁxed eﬀects, we also include a random eﬀect to represent population heterogeneity. Finally, we
include a ﬁxed eﬀect for phase encoding direction. The mixed eﬀect model at vertex v is given by:

baseline eﬀects
(cid:125)(cid:124)

(cid:122)
{αk(v) + ak,i(v)} +

(cid:123)

HRF modeling eﬀects
(cid:122)
(cid:123)
(cid:125)(cid:124)
{βk(v) + bk,i(v)} h +

τ hr(cid:96)
ijk (v) =
ijk (v) iid∼ N (0, σ2
(cid:15)hr(cid:96)

ijk (v)
v), bi(v) = {a1,i(v), . . . , a4,i(v), b1,i(v), . . . , b4,i(v), gi(v)}(cid:48) iid∼ N (0, G(v)).

motion
regression eﬀects
(cid:125)(cid:124)

(cid:122)
{γ(v) + gi(v)} r +

(cid:123)

acquisition
eﬀects
(cid:122) (cid:125)(cid:124) (cid:123)
θ(v)(cid:96) + (cid:15)hr(cid:96)

(2)

The covariates h, r and (cid:96) are constructed as dummy variables equalling zero for the “baseline” conditions
(canonical HRF only, RP12 motion regression, and LR phase encoding direction acquisition) and equalling one
for the alternative conditions (canonical HRF plus derivative(s), RP24 motion regression, and RL phase encoding

6

direction acquisition). The model in equation (2) is estimated separately for h = 1 representing the inclusion of
HRF temporal derivatives or the HRF temporal and dispersion derivatives.

We perform model ﬁtting using the lmer function from the lme4 R package (version 1.1.-30) (Bates et al. 2015)
to estimate each ﬁxed eﬀect (the αk(v), βk(v), γ(v) and θ(v)), the error variance, and the covariance of all
the random eﬀects (the ak,i(v), bk,i(v) and gi(v)). The ﬁxed eﬀect for task, αk(v), represents the baseline
autocorrelation for task protocol k for the model including the canonical HRF only (h = 0). The corresponding
random subject eﬀect, ak,i(v), represents the diﬀerence in autocorrelation for subject i, versus the average over
subjects for task k. The ﬁxed eﬀect for HRF modeling, βk(v), represents the change in autocorrelation when the
temporal derivative of the HRF is included (h = 1) for task k, while bk,i(v) represents the random variation in
that change over subjects. We generally expect negative values for βk(v), representing a reduction in residual
autocorrelation when the HRF derivative is included, since discrepancies between the true HRF and the canonical
HRF tend to exhibit temporal dependence. The ﬁxed eﬀect for motion regression strategy, γ(v), represents the
change in autocorrelation associated with the use of RP24 (r = 1) versus RP12 (r = 0) motion regression. The
corresponding random eﬀect, gi(v), represents random variation in that eﬀect over subjects. θ(v) represents the
diﬀerence in autocorrelation when using phase encoding direction RL ((cid:96) = 1), compared with phase encoding
direction LR ((cid:96) = 0).

Since the model includes multiple sessions from each subject, the random eﬀects ak,i(v), bk,i(v) and gi(v) represent
systematic eﬀects that are consistently observed across sessions for subject i. G(v) is the covariance matrix of
the random eﬀects vector bi(v). It encodes population variance for each random eﬀect, as well as the correlation
between diﬀerent random eﬀects. For example, Cor {a1,i(v), a4,i(v)} represents the correspondence between the
direction and strength of subject i’s deviation from the population mean autocorrelation for tasks 1 (emotion)
and 4 (relational), using the canonical HRF only. A strong positive correlation here would suggest that the
same subjects tend to exhibit stronger or weaker autocorrelation, possibly due to their having a longer or shorter
HRF than the canonical HRF, regardless of task. Cor {a1,i(v), b1,i(v)} represents the correspondence between
the direction and strength of subject i’s deviation from the population mean on task 1 and the eﬀect of including
the HRF derivative for the same task. A strong anti-correlation here would suggest that including the HRF
derivative has a bigger eﬀect on subjects who exhibit stronger autocorrelation when using the canonical HRF –
in short, that inclusion of the HRF derivative achieves the goal of accounting for some population variability in
HRF timing.

2.2.3 Prewhitening Strategies

To evaluate the eﬀectiveness of diﬀerent prewhitening strategies on mitigating residual autocorrelation, we use an
AR model with varying model order and varying degrees of regularization to estimate the prewhitening matrix
Wv. Speciﬁcally, we vary AR model order from p = 1 to p = 6. We also consider automatic selection of the
optimal AR model order at each vertex using Akaike information criterion (AIC) (Sakamoto et al. 1986) as
proposed by Luo et al. (2020), with a maximum model order of 10. The AR model coeﬃcients and residual
variance are estimated using the Yule-Walker equations (Brockwell and Davis 2009). We consider both local
and global regularization of the AR coeﬃcients and white noise variance. Local regularization refers to surface
smoothing with a 5mm FWHM Gaussian kernel; global regularization refers to smoothing with an inﬁnitely-wide
Gaussian kernel, nearly equivalent to averaging the AR coeﬃcients across the cortex. In the case of optimal AR
model order selection, we impute a value of 0 for any AR coeﬃcients above the selected model order prior to
regularization.

Our R/C++ implementation of prewhitening is available in the open-source BayesfMRI R package (Mejia et al.
2022), which is compatible with cortical surface and “grayordinates” neuroimaging ﬁle formats via the ciftiTools
R package (Pham et al. 2022). After estimating the prewhitening matrix Wv as described in Appendix B, the
response and design matrix are premultiplied at each vertex by Wv, changing the GLM in (1) to

˜yv = ˜Xvβv + ˜(cid:15)v,

˜(cid:15)v ∼ MVN(0, σ2I),

(3)

where ˜yv = Wvyv, ˜Xv = WvXv and ˜(cid:15)v = Wv(cid:15)v. Note that the prewhitened design matrix ˜Xv may vary
across vertices when using a local approach to prewhitening. This increases the computational burden associated
with GLM coeﬃcient estimation: with a common design matrix ˜X, the model coeﬃcients for all vertices can
be estimated with a single matrix multiplication step as ( ˜X(cid:48) ˜X)−1 ˜X(cid:48) ˜Y, where ˜Y = (˜y1, . . . , ˜yV ); when the
design matrix varies spatially, however, we must perform V matrix multiplications ( ˜X(cid:48)
v ˜yv to obtain
v
the coeﬃcient estimate at each vertex v. Even more computationally burdensome is estimating Wv at each
location, which involves performing V diﬀerent eigendecompositions. These obstacles are perhaps one reason
that global prewhitening approaches are often preferred in a practical sense. To overcome these challenges, we

˜Xv)−1 ˜X(cid:48)

7

have developed a highly computationally eﬃcient implementation using parallelization and C++ backend code.
This implementation typically completes in approximately 1 minute per scan for the task fMRI data we analyze
here.

2.2.4 Evaluation Metrics

To evaluate the performance of each prewhitening strategy, we take two approaches. First, we directly assess the
degree of residual autocorrelation still present in each task fMRI dataset after prewhitening. Using on the ﬁtted
GLS residuals at each vertex, we compute the autocorrelation index (ACI) as in Section 2.2.2. We also perform a
Ljung-Box (LB) test at every vertex (Ljung and Box 1978) to identify vertices exhibiting statistically signiﬁcant
levels of residual autocorrelation after prewhitening. We use the Box.test function in the stats R package,
version 4.2.0. As in (Corbin et al. 2018), we use the ﬁrst 100 volumes of each session and consider up to 20 lags.
We consider two approaches to determine the degrees of freedom (DOF) for the test: accounting for the intercept
only, or accounting for the intercept and the AR model coeﬃcients. We consider the intercept-only approach
for maximum comparability with Corbin et al. (2018).1 When accounting for the AR(p) model coeﬃcients as
well, the DOF for the LB test is 20 − [p ∗ 100/T ] − 1, where T is the original number of volumes in the task
fMRI session (given in Table 1). We scale the number of AR coeﬃcients p by 100/T to account for the fact
that the AR model parameters were estimated across the whole duration of the scan, not just the 100 volumes
used for the LB test. We determine vertices whose residuals exhibit signiﬁcant autocorrelation based on those
with p < 0.05 after false discovery rate correction (Benjamini and Hochberg 1995). We compute the proportion
of vertices exhibiting signiﬁcant autocorrelation before and after prewhitening with each technique to determine
the ability of each prewhitening method to eﬀectively eliminate autocorrelation.

Second, we quantify false positives using resting-state fMRI data, assuming a false boxcar task paradigm. For
each resting-state fMRI dataset, we perform GLS using the estimated prewhitening matrix for each prewhitening
strategy. We then perform a t-test at every vertex. We correct for multiple comparisons across all vertices
with Bonferroni correction to control the family-wise error rate (FWER) at 0.05. While Bonferroni correction
is typically considered overly conservative for whole-brain voxel-wise analysis involving potentially hundreds of
thousands of tests, here we are performing fewer than 6,000 tests per hemisphere. In previous work we have found
Bonferroni correction to have similar power as permutation testing for the cortical surface resampled to a similar
resolution (Spencer et al. 2022). We visualize the spatial distribution of false positive vertices and quantify the
false positive rate and FWER before and after prewhitening. We obtain 95% conﬁdence intervals for the FWER
using Agresti-Coull intervals for proportions (Agresti and Coull (1998)).

3 Results

3.1 Overview

We ﬁrst examine the spatial patterns and factors inﬂuencing autocorrelation in task fMRI prior to any prewhiten-
ing, using a random eﬀects analysis of task fMRI data from the HCP retest dataset. This allows us to understand
to what degree residual autocorrelation varies across the cortex in task fMRI studies, which in turn helps inform
an eﬀective approach to prewhitening. We ﬁnd that residual autocorrelation varies markedly across the cortex,
and the spatial topology is inﬂuenced by the task being performed, the phase encoding direction, and system-
atic inter-subject diﬀerences. Eﬀective modeling choices (e.g. HRF ﬂexibility, nuisance regression) can mitigate
autocorrelation, but their eﬀects are relatively modest and they do not eliminate spatial variability. We then
assess the ability of diﬀerent AR-based prewhitening strategies to eﬀectively mitigate residual autocorrelation
and control false positive rates. We consider low- and high-order AR models, as well as optimal determination
of the AR model order at each vertex. We also consider two opposing approaches to spatial regularization of AR
model parameters: local spatial smoothing and global averaging across the cortex. We ﬁnd that higher-order AR
models that allow for spatial variability in AR model parameters are able to eﬀectively mitigate autocorrelation,
while global averaging and very low-order AR models retain substantial levels of autocorrelation.

1Though Corbin et al. (2018) did not state explicitly how DOF was determined in their analysis, their implementation used
the Matlab Ljung-Box test function, where ignoring the model DOF is the default. Further, the higher-order SPM FAST
models considered in Corbin et al. (2018) contain more than 20 DOF, which would result in negative DOF for the LB test
if taken into account.

8

Figure 1: Spatial patterns of autocorrelation and the eﬀect of HRF modeling. (A) The average
autocorrelation index (ACI) across all subjects, sessions and tasks, for three diﬀerent cases: assuming a canonical
HRF, including the HRF temporal derivative (TD) to allow for diﬀerences in HRF onset; and including the HRF
TD and dispersion derivative (DD) to allow for diﬀerences in HRF onset and duration. (B) The reduction in ACI
when HRF derivatives are included to allow for diﬀerences in HRF shape. Including the HRF TD has a sizeable
eﬀect in reducing autocorrelation; additionally including the HRF DD has a more subtle eﬀect.

3.2 Spatial patterns of residual autocorrelation

Figure 1 shows the autocorrelation index (ACI) across the cortex, averaged over all subjects, sessions, runs
and tasks in the HCP retest study. The spatial topology of autocorrelation across the cortex is striking: higher
autocorrelation is seen in frontal, parietal and occipital areas, particularly the inferior parietal cortex and the
occipital pole. Much lower autocorrelation is seen in the insula, the fusiform gyrus and the temporal pole, for
example. The regions with highest autocorrelation tend to be near the edge of the brain, possibly reﬂecting in
part eﬀects of motion. Three diﬀerent modeling strategies for the hemodynamic response function (HRF) are
considered, ranging from rigid (canonical HRF) to more ﬂexible (canonical HRF plus its temporal derivative (TD)
and dispersion derivative (DD)). The degree of autocorrelation is reduced by the inclusion of HRF derivatives,
illustrating that HRF mis-modeling is one source of autocorrelation that can be mitigated with more ﬂexible HRF
models. Panel B shows that inclusion of the TD serves to reduce autocorrelation most in the same areas where
autocorrelation tends to be the highest, suggesting that the spatial patterns of autocorrelation may be due in part
to heterogeneity in HRF onset and duration that varies across the cortex. Even after including HRF derivatives,
however, there are still marked diﬀerences in the degree of residual autocorrelation across the cortex. Including
the dispersion derivative has a more subtle eﬀect, compared with just including the temporal derivative. For the
remaining analyses, we therefore focus on inclusion of the HRF temporal derivative only.

3.3 Random eﬀects analysis of residual autocorrelation

Here, we examine the sources of residual autocorrelation in task fMRI studies through a random eﬀects analysis
of repeated task fMRI scans from the HCP retest participants. We ﬁt a series of general linear models (GLMs) to
each task fMRI scan, varying the HRF modeling strategy and the number of motion regressors across GLMs. For
each GLM, we quantify the autocorrelation index (ACI) of the model residuals at each surface vertex. We then
ﬁt the random eﬀects model in (2) at each vertex to quantify the contribution of task-speciﬁc diﬀerences, HRF
modeling strategy, phase encoding direction, and number of nuisance regressors on the residual autocorrelation

9

AB12.4Canonical HRFHRF + TDHRF + TD + DD+ TD+ DD-0.20index. The inclusion of random eﬀects accounts for systematic between-subject variability and allows us to
understand population heterogeneity in these eﬀects.

Figure 2 displays the ﬁxed baseline eﬀects associated with each task (α(k)
v , k = 1, . . . , 4 in model (2)), along
with the eﬀect of including the HRF derivative (β(k)
v , k = 1, . . . , 4 in model (2)). The ﬁrst column displays the
mean eﬀect over tasks k = 1, . . . , 4; the other columns display the diﬀerence between the task-speciﬁc eﬀects
and that mean eﬀect. The baseline eﬀects shown on the ﬁrst row (αk(v) in model 2) represent the average ACI
when assuming a canonical HRF, including 12 motion regressors, and using the LR phase encoding direction.
The average pattern is very similar to the mean ACI in the dataset shown in Figure 1. The task-speciﬁc
deviations show that ACI tends to be markedly higher or lower in certain regions depending on the task. For
example, the motor task tends to have higher residual ACI in many areas, whereas the emotion task tends to
have lower residual ACI. These results show that there are systematic task-related eﬀects on autocorrelation that
vary across the cortex. The HRF derivative eﬀects shown on the second row represent the change in average
ACI when including the HRF derivative in each GLM (βk(v) in model 2). The mean eﬀect shows that including
HRF derivatives tends to decrease ACI, particularly in areas where ACI tends to be the highest, as observed in
Figure 1. The task-speciﬁc deviations show that more ﬂexible HRF modeling has the strongest eﬀect for the
motor task, mimicing the more severe autocorrelation seen in the motor task. The areas most aﬀected by ﬂexible
HRF modeling for each task tend to somewhat mimic the spatial patterns unique to each task, but do not fully
account for them. The sum of both eﬀects shown in the third row represent the average ACI when including
HRF derivatives (αk(v) + βk(v) in model 2). The average image shows reduced autocorrelation on average,
consistent with 1. The task-speciﬁc deviations show that task-related diﬀerences in residual autocorrelation are
substantially reduced but not eliminated when using a more ﬂexible HRF model.

Figure 3 displays the random eﬀects associated with each ﬁxed eﬀect shown in Figure 2. These random eﬀects
represent reliable between-subject diﬀerences observed across repeated sessions from each subject. The scale
of the values is standard deviation, so they share the same units as the ﬁxed eﬀects. The ﬁgure organization
is the same as that of Figure 2, with the ﬁrst column representing the mean over tasks and the remaining
columns representing diﬀerences between each task and that mean. The mean eﬀects show that there is sub-
stantial population heterogeneity in the degree of residual autocorrelation, as well as in the reduction in residual
autocorrelation achieved by more ﬂexible HRF modeling. In general, the spatial patterns mimic that of the ﬁxed
eﬀects: areas that tend to have higher autocorrelation on average also tend to exhibit greater systematic vari-
ability across subjects, and areas that beneﬁt more from ﬂexible HRF modeling on average also tend to exhibit
the most population variability in the degree of reduction.

Figure 4 shows ﬁxed and random eﬀects of including additional motion regressors (24 versus 12) in the GLM
In the GLM with 12 motion regressors, the six
on the degree of residual autocorrelation across the cortex.
realignment parameters (RPs) plus their one-back diﬀerences are included as covariates;
in the model with
24 motion regressors, their square of each term is also included. Panel (A) shows that on average, including
additional motion regressors decreases residual autocorrelation. This illustrates that without adequate nuisance
signal modeling, temporally correlated noise such as that arising from head motion will be at least partly absorbed
into the residuals, which will consequently exhibit greater autocorrelation. The spatial patterns mimic the spatial
topology of baseline autocorrelation seen in Figures 1 and 2, suggesting that thorough noise modeling helps to
alleviate, without eliminating, the spatial heterogeneity in autocorrelation across the cortex. Panel (B) shows
that there is population heterogeneity in the beneﬁt of including additional motion regressors on autocorrelation,
particularly in those areas with the most beneﬁt on average.

Figure 5 takes a deeper look at population heterogeneity by examining the correlation between diﬀerent random
eﬀects in the model. The lower triangle of the matrix is divided into several blocks, representing the diﬀerent
random components of the model. Blocks (A) and (B) show that there is moderate correlation between the
diﬀerent tasks in terms of the baseline eﬀect and the eﬀect of including HRF derivatives. This shows that for
a particular subject, the spatial topology of autocorrelation and HRF shape are similar but not identical across
diﬀerent tasks. In block (C), the diagonal elements show a strong negative correlation between the baseline level
of autocorrelation and the reduction in autocorrelation due to ﬂexible HRF modeling. This shows that subjects
having higher (or lower) baseline autocorrelation tend to beneﬁt more (or less) from ﬂexible HRF modeling.
The strong but not perfect correlations suggest that accounting for between-subject diﬀerences in HRF shape
reduces, but does not eliminate, systematic population variability in residual autocorrelation. Turning to the
eﬀect of motion, the strongly negative correlations in block (E) show that including additional motion regressors
(24 instead of 12) reduces autocorrelation most for subjects who tend to exhibit stronger baseline autocorrelation.
The moderate positive correlations in block (D) show that subjects who beneﬁt more from more ﬂexible HRF
modeling also tend to beneﬁt more from inclusion of additional motion regressors, suggesting that these two
approaches may be complementary in reducing residual autocorrelation.

10

Figure 2: Baseline autocorrelation index (ACI) and eﬀect of including HRF derivatives by task,
based on the ﬁxed eﬀects (FEs) from the mixed eﬀects model shown in equation (2). The ﬁrst column shows the
average of FE estimates across tasks, indicating general spatial patterns of autocorrelation and the eﬀect of HRF
derivatives across the cortex. The other columns show the diﬀerence between each task and the average, to show
areas of stronger or weaker eﬀects during speciﬁc tasks. The ﬁrst row shows the average ACI when assuming a
canonical HRF (αk(v)); the second row shows the eﬀect of including HRF derivatives to allow for heterogeneity
in the shape of the HRF (βk(v)); the third row shows the sum of both eﬀects, which represents the average ACI
when including HRF derivatives in the model (αk(v) + βk(v)).

11

Figure 3: Population variability in the eﬀects shown in Figure 2, based on the random eﬀect (RE)
standard deviations (SD) from model (2). The ﬁrst column shows the average across tasks, indicating general
spatial patterns of population variability. The other columns show the diﬀerence between each task and the
average, indicating areas of greater (warm colors) or lesser (cool colors) variability during speciﬁc tasks. The
ﬁrst row shows variability in autocorrelation when assuming a canonical HRF (ak,i(v)); the second row shows
variability in the eﬀect of using HRF derivatives to allow for diﬀerences in HRF shape (bk,i(v)).

(A) Fixed eﬀects of RP24

(B) Random eﬀects of RP24

Figure 4: Eﬀect of including additional motion regressors on autocorrelation index (ACI), based
on the ﬁxed eﬀects in model (2). Values represent the decrease in ACI associated with including 24 rather than 12
motion regressors. Panel (A) shows the ﬁxed eﬀects at every vertex; panel (B) shows the random eﬀect standard
deviation at every vertex.

12

Figure 5: Correlation among sources of population variability in autocorrelation, based on the
random eﬀect (RE) correlations in model (2). Values represent the mean across all vertices. Positive values (warm
colors) between two eﬀects indicate that subjects who exhibit higher values in one eﬀect also tend to exhibit higher
values in the other eﬀect. Negative values (cool colors) indicate that subjects who exhibit higher values in one
eﬀect tend to exhibit lower values in the other eﬀect. The lower triangle of the correlation matrix is divided into
several parts, representing the correlation between: a) diﬀerent tasks in the baseline level of autocorrelation; b)
diﬀerent tasks in the eﬀect of HRF derivatives on autocorrelation; c) the baseline level of autocorrelation and the
eﬀect of including HRF derivatives on autocorrelation; d) the eﬀect of including additional motion regressors and
the eﬀect of including HRF derivatives; e) the eﬀect of including additional motion regressors and the baseline
level of autocorrelation.

13

EGMRcanHRF (baseline)canHRF(baseline)dHRFEffect of using 24RPinstead of 12RPdHRFa. canHRF-canHRFc. canHRF-dHRFe.d.RMGE24 RPRMGE10.380.370.3910.440.3510.3810.540.610.760.59110.570.560.61−0.72−0.46−0.48−0.45−0.710.660.57−0.43−0.78−0.55−0.4−0.7810.54−0.41−0.49−0.88−0.39−0.851−0.5−0.46−0.48−0.78−0.76b. dHRF-dHRFEGMR−1.00.01.0TaskE: emotionG: gamblingM: motorR: relationalPearson Cor.Before Correction

After Correction

(A) Fixed eﬀect of RL acquisition

(B) LR (blue) and RL (red) distortions

Figure 6: Eﬀect of phase encoding direction on autocorrelation. (A) Fixed eﬀect of RL phase encoding
direction in model (2). Values represent the diﬀerences in average autocorrelation index (ACI) at each vertex when
the RL (versus LR) phase encoding direction is used during image acquisition. Cool colors on the right lateral
cortex, for example, indicate that RL acquisitions tend to have reduced autocorrelation in those areas compared
with LR acquisitions. The eﬀect of phase encoding direction is clearly lateralized, with the RL acquisition
resulting in relatively lower autocorrelation on the right side of each hemisphere and higher autocorrelation on
the left side of each hemisphere. This is likely due to distortions induced by the RL and LR phase encoding
directions, even after distortion correction.
(B) Mean rest fMRI image for a single subject (103818) for LR
(blue) and RL (red) runs during the same session, before and after distortion correction, shown in neurological
convention. Lateralized distortions persist after distortion correction, based on the imperfect overlap between
the LR and RL runs.

Table 2: Settings for the simulation study shown in Figure 7.

Tissue Class
White Matter
Gray Matter
CSF
Background

Number of Voxels
11 voxels
2 voxels
3 voxels
11 voxels

AR(3) Model
(0.1, 0.0, 0.0)
(0.425, 0.25, 0.1)
(0.5, 0.3, 0.1)
white noise only

ACI
1.1
2.3
4.5
1

Figure 6A displays the ﬁxed eﬀects associated with the utilizing an RL phase encoding direction acquisition,
compared with an LR phase encoding direction. The eﬀect of phase encoding direction on residual autocorrelation
is clearly lateralized, with the RL phase encoding direction generally producing less autocorrelation on the right
side of each hemisphere (the lateral cortex of the right hemisphere, and the medial cortex of the left hemisphere).
This is likely due to lateralized distortions induced by the RL and LR phase encoding directions even after
distortion correction, as shown in Figure 6B. The spatial distribution of residual autocorrelation is therefore
sensitive to speciﬁc acquisition. This may result in an increased risk of false positives in certain areas, depending
on the acquisition method. For example, using an LR phase encoding direction, residual autocorrelation is
generally higher within the right lateral cortex. This will result in higher rates of false positives compared with
the left lateral cortex if not accounted for with prewhitening techniques that account for such spatial discrepancies
in residual autocorrelation.

We examine this further through a small simulation study from HCP subject 103818, shown in Figure 7. We
consider a strip of nine voxels overlapping with the edge of the brain of the subject, shown in Figure 7A. These
include, sequentially, three voxels in CSF (red), two cortical gray matter voxels (yellow), and four white matter
voxels. In addition, we include 26 background voxels on the left and 14 additional WM voxels on the right in
order to absorb any edge eﬀects. We generate autocorrelated timeseries for each voxel using an AR(3) model with
white noise variance equal to 1. The AR coeﬃcients are chosen to induce low ACI in white matter, moderate
ACI in gray matter, high ACI in CSF, and unit ACI (the minimum) in background voxels. Table 2 gives the
AR coeﬃcients and resulting ACI in each region.

14

True ACI

After distortion

After correction

Bias

(A) Voxels used in the simulation

(B) ACI and bias in the simulation study

Figure 7: Eﬀect of distortions on the spatial topology of autocorrelation. (A) A nine-voxel sequence
contains three voxels from cerebral spinal ﬂuid (CSF, red), two voxels from gray matter (GM, yellow), and
four voxels from white matter (WM, blue). These were padded by 26 background voxels on the left and 15
additional WM voxels on the right to avoid edge eﬀects. (B) An AR(3) was used to generate autocorrelated
timeseries within each tissue class, resulting in the true autocorrelation indices (ACI) shown on the top row. The
ACI of the timeseries after after forward-direction distortion are shown on the second row and after distortion
correction on the third row. While distortion correction clearly helps to resolve changes in ACI induced by the
distortions, the fourth row shows that there is still bias (after/true) present after correction. Namely, the GM
voxel neighboring CSF has increased ACI, and the GM voxel neighboring WM has decreased ACI. There is also
a lesser amount of bias in the CSF and WM voxels neighboring GM.

To examine the eﬀect of the distortions introduced through lateralized phase encoding on the ACI, we estimate
the distortion map from the original temporal mean undistorted brain images of the subject. We distort the
simulated timeseries then apply distortion correction using the Anima image processing toolbox2. Figure 7B
shows the true ACI in each voxel, the ACI after distortion, and the ACI after distortion correction based on the
mean over 7000 randomly generated timeseries. The last row shows the bias between the ACI of the distortion-
corrected data, as a proportion of the true ACI. We see inﬂated ACI in the gray matter voxel bordering CSF
and diminished ACI in the gray matter voxel bordering white matter. This agrees with our ﬁndings in Figure 6
and supports the hypothesis that the LR and RL acquisitions result in changes in autocorrelation in gray matter
due to distortion-induced mixing of signals with white matter and CSF.

In sum, we observe marked spatial discrepancies in autocorrelation within cortical gray matter due to acquisition
factors, modeling choices, task-related factors and individual diﬀerences. The following section evaluates the eﬀect
of diﬀerent prewhitening strategies on mitigating autocorrelation, reducing spatial variability in autocorrelation,
and controlling false positives.

3.4 The eﬀect of prewhitening strategy on autocorrelation and false positives

Here, we apply several prewhitening strategies based on autoregressive (AR) modeling and evaluate their eﬀect
on both residual autocorrelation and false positive rates. Speciﬁcally, we consider AR model order ranging from
1 to 6, as well as a spatially varying “optimal” model order based on Akaike information criterion (AIC). For
each AR model order, we also consider two spatial regularization levels of the AR model coeﬃcient estimates:
“local” regularization is achieved by surface-based spatial smoothing the coeﬃcient estimates using a 5mm full
width at half maximum (FWHM) Gaussian kernel (Pham et al. 2022); “global” regularization is achieved by
averaging the estimates across all cortical vertices. For optimal model order selection, all remaining coeﬃcients
after the AIC-based model order p∗ (up to the maximum of 10) are set to zero prior to regularization. All of the
prewhitening methods considered are implemented in the open-source BayesfMRI R package, version 2.0 (Mejia
et al. 2022, R Core Team 2022).

In this section, we include just 12 motion parameters (6 rigid body realignment parameters and their one-back
diﬀerences) in each GLM by default. This is because when combined with eﬀective prewhitening, there does not

2https://github.com/Inria-Visages/Anima-Public

15

CSFGMWMtissuewhen12345123450.971.001.03appear to be a beneﬁt of including the squares of the motion parameters and their one-back diﬀerences. In fact,
it appears to be slightly detrimental, as shown in Figure A.1. This suggests that including these additional
motion parameters no longer serves to reduce autocorrelation when eﬀective prewhitening is performed, and the
loss of degrees of freedom associated with the inclusion of superﬂuous covariates in the GLM actually worsens
the performance of prewhitening.

Figure 8 displays the distribution of AIC-based AR model order, p∗, across the cortex. Panel (A) shows the
value of p∗ for a single run from a single subject across all four tasks. The spatial patterns mimic the general
patterns of autocorrelation strength seen in Figure 1. Areas of higher autocorrelation generally require a higher
AR model order (e.g. the inferior parietal cortex and the occipital pole), while areas of lower autocorrelation (e.g.
the insula and the temporal pole) generally require a lower AR model order or no prewhitening at all (p∗ = 0).
Diﬀerences across tasks can also been seen: for example, the motor task generally requires a higher AR model
order, reﬂecting the stronger residual autocorrelation associated with the motor task as seen in Figure 2. The
AIC-based model order is somewhat noisy, suggesting that some degree of regularization is needed. Panel (B)
shows a histogram of AIC-based model order across all vertices. The proportions are averaged across all subjects,
sessions and tasks. We see that on average, while the optimal AR model order is 2 or less for most vertices, over
30% of vertices have optimal AR model order of 3 or higher, while over 10% require an AR model order of 7
or higher. This again underscores the important spatial diﬀerences in residual autocorrelation across the cortex
and the need for prewhitening methods that account for those diﬀerences to avoid under- or over-whitening in a
given area.

Figure 9 shows the eﬀect of each prewhitening strategy on the degree of residual autocorrelation. Panel (A)
displays the autocorrelation index (ACI) at each vertex averaged over all subjects, sessions and tasks, prior to any
prewhitening. Panel (B) displays the average ACI at each vertex after prewhitening with each strategy (varying
AR model order, local versus global regularization of the AR model coeﬃcients). Panel (C) displays the mean and
95th quantile of ACI across the cortex by task, averaged over all subjects and sessions, after prewhitening with
each strategy. Panels (B) and (C) show that there is a dramatic diﬀerence between local and global regularization
in terms of reducing autocorrelation:
local regularization reduces ACI more and mostly eliminates the spatial
variability in ACI. The combination of higher AR model order (e.g. AR(6)) with local regularization is the most
eﬀective at reducing ACI. Notably, the use of higher model orders in combination with global regularization is
not very eﬀective at reducing autocorrelation in many areas of the cortex: even an AR(1) model with coeﬃcients
that are allowed to spatially vary appears to be more eﬀective than an AR(6) model with global regularization.
Interestingly, the use of AIC to select the optimal model order at each vertex does not appear to be advantageous
over ﬁtting an AR(6) model at every vertex. It is worth noting that an AR(6) model encompasses lower-order
AR models, since the higher coeﬃcients can equal zero. Local regularization of the AR model coeﬃcients may
have the eﬀect of shrinking those higher coeﬃcients closer to zero when that is appropriate. Therefore, ﬁtting
an AR(6) model at each vertex, combined with local regularization, may allow for less aggressive prewhitening
in those areas that exhibit less autocorrelation.

Figure 10 displays the eﬀect of prewhitening on the rate of vertices with statistically signiﬁcant autocorrelation,
based on performing a Ljung-Box (LB) test at every vertex (Ljung and Box 1978). We correct for multiple
comparisons by controlling the false discovery rate (FDR) at 0.05 using the Benjamini-Hochberg procedure
In panels (A) and (B), the value at each vertex represents the proportion
(Benjamini and Hochberg 1995).
of sessions that show signiﬁcant autocorrelation across all subjects, sessions and tasks. Panel (C) shows the
proportion of signiﬁcantly autocorrelated vertices in each session by task, averaged over all subjects and sessions.
For now we will focus on the solid lines, which represent the results of the LB test when we assume a single
degree of freedom lost in all models. The patterns in panels (A) and (B) mimic those seen in Figure 9: local
regularization of AR prewhitening parameters is much more eﬀective at reducing autocorrelation than global
regularization, and even a parsimonious (e.g. AR(1)) AR model with local coeﬃcient regularization is more
eﬀective than a high-order AR model with global regularization. Panel (C) shows that AR-based prewhitening
with local regularization (the black lines) essentially eliminates statistically signiﬁcant autocorrelation in all
vertices, particularly when using an AR model order of 3 or higher. A globally regularized, high-order AR model
approach is less eﬀective, reducing the proportion of signiﬁcantly autocorrelated vertices to 10-15%. Note that
this is similar to the performance of the optimal 12-component SPM FAST model for data with TR=0.7 (Corbin
et al. 2018). The much greater reduction in autocorrelation with local regularization illustrates the need to
consider spatial diﬀerences in autocorrelation for eﬀective prewhitening.

Note that in panel (C) of Figure 10, we also consider the eﬀect on the test result of accounting for the degrees
of freedom (DOF) lost through the AR model ﬁt (1 + p) or just the intercept (1).3 Though accounting for the AR

3Note that for the optimal AR model order approach (AR(*)), we do not consider accounting for the DOF lost through
model ﬁtting when summarizing across vertices, since the DOF varies across vertices.

16

Emotion

Gambling

Motor

Relational

(A) Optimal AR model order for one subject for each task.

(B) Average distribution of optimal AR model order across the cortex.

Figure 8: Optimal AR model order across the brain based on the Akaike information criterion
(AIC). (A) The optimal AR model order at every vertex for a single subject for each task. The optimal order
clearly varies across the cortex and with the task being performed. (B) The distribution of optimal AR model
order across all vertices, averaged over all subjects, sessions and tasks. The optimal AR model order is 2 or less
for most vertices, but over 20% of vertices have optimal AR model order of 3 to 6, while over 10% have optimal
order of 7 or higher.

17

0.000.050.100.150.200246810AIC orderMean proportion on surfaceFigure 9: The eﬀect of prewhitening on autocorrelation index (ACI). (A) Mean ACI over all subjects,
sessions and tasks before prewhitening. (B) Mean ACI after prewhitening. Eight diﬀerent prewhitening strategies
are shown, based on four diﬀerent AR model orders (1, 3, 6 and optimal at each vertex) and two diﬀerent
regularization strategies for AR model coeﬃcients (local smoothing versus global averaging). Higher AR model
order and allowing AR model coeﬃcients to vary spatially results in substantially greater reduction in ACI. (C)
Mean ACI over subjects and sessions, averaged across all vertices, by task and prewhitening method. Notably,
allowing AR model coeﬃcients to spatially vary reduces ACI much more than increasing AR model order.

18

Figure 10: The eﬀect of prewhitening on the number of vertices with statistically signiﬁcant auto-
correlation. (A) Proportion of sessions exhibiting signiﬁcant autocorrelation at each vertex before prewhitening.
(B) Proportion of sessions exhibiting statistically signiﬁcant autocorrelation after prewhitening. Eight diﬀerent
prewhitening strategies are shown, based on four diﬀerent AR model orders (1, 3, 6 and optimal at each vertex)
and two diﬀerent regularization strategies for AR model coeﬃcients (local smoothing versus global averaging).
Higher AR model order and allowing AR model coeﬃcients to vary spatially results in substantially greater
reduction in the number of vertices with statistically signiﬁcant autocorrelation. Notably, allowing AR model
coeﬃcients to spatially vary has a greater eﬀect than increasing AR model order. (C) Percentage of vertices with
statistically signiﬁcant autocorrelation, averaged across all subjects, sessions and tasks. Dotted lines correspond
to accounting for the degrees of freedom (DOF) lost when estimating AR coeﬃcients. Adopting an AR model
order of 3 or higher and allowing AR coeﬃcients to vary spatially results in virtually no vertices with statistically
signiﬁcant autocorrelation.

19

ﬁt in the total DOF is recommended, we account for the intercept only in (A), (B) and the solid lines in (C). This
is done in order to replicate the analysis of Corbin et al. (2018), which was based on the Matlab implementation
of the Ljung-Box test, where ignoring the model DOF is the default. For more complex models involving many
parameters, accounting for the model DOF generally results in apparently higher rates of autocorrelation, as seen
in the U-shaped gray dashed lines in panel (c). This somewhat counterintuitive eﬀect is simply a consequence
of the loss in total degrees of freedom going from a more parsimonious model (e.g. AR(3)) to a more complex
one involving more parameter estimates (e.g., AR(6)). Accounting properly for the DOF lost helps to avoid
overestimating the performance of more highly parameterized models, which run the risk of overﬁtting to the
data.

In Figure 11, we examine the eﬀect of prewhitening on false positives in null (resting-state) data. Assuming
a false on-oﬀ 10s boxcar design, we ﬁt a GLM and perform a t-test on the task coeﬃcient at every vertex. We
perform Bonferroni correction of the p-values to control the FWER at 0.05. Note that while Bonferroni correction
is often considered overly conservative for volumetric fMRI analyses involving hundreds of thousands of tests,
here we have resampled the data to 6, 000 vertices per hemisphere, so the number of tests being performed is
an order of magnitude less. We have previously observed that Bonferroni correction is not more conservative
than permutation testing in this data. Panel (A) displays the proportion of sessions showing a false detection
at each vertex when no prewhitening is performed or when an AR(6) model is used for prewhitening with local
or global coeﬃcient regularization. Panel (B) displays the false positive rate (FPR), the proportion of vertices
labeled as active in each session, averaged across all subjects, sessions and tasks. Panel (C) displays the FWER,
the proportion of sessions exhibiting a single false positive vertex. In (B) and (C) we also consider the eﬀect of
including additional motion parameters (24) versus the 12 included by default in our analyses. We observe that
for both local and global regularization, the inclusion of additional motion parameters actually worsense the FPR
and FWER. This is in line with the slight increase in ACI we observe when these parameters are included in
combination with prewhitening (see Figure A.1). Taken together, these results suggest that the loss in degrees
of freedom associated with including superﬂuous covariates in the GLM worsen the performance of prewhitening.
This illustrates that overparameterized GLMs may actually result in inﬂated false positive rates, in addition to
their well-known eﬀect of reducing power to detect true eﬀects. Comparing the FPR and FWER before and after
prewhitening, we see that prewhitening drastically reduces the FPR within each session, and achieves FWER
fairly close to the nominal rate of 0.05.

Interestingly, using global regularization achieves slightly lower FWER while achieving slightly worse but still very
low FPR. This surprisingly strong performance of global regularization stands in contrast to its poor performance
in our task fMRI-based analyses, displayed in Figures 9 and 10. This may indicate some limitations of using
resting-state fMRI as “null” data for evaluating false positive control in task fMRI. There are many features of
task fMRI data that may not be reﬂected in resting-state fMRI data. For example, mismodeling of the task-
induced HRF can induce residual autocorrelation, as shown in Figure 1.
Inclusion of HRF derivatives only
partly accounts for the task-related diﬀerences in autocorrelation, as shown in Figure 2.

4 Discussion

In this paper, we have made three primary advances in prewhitening in fMRI. First, we performed a comprehensive
analysis to examine the spatial topology of autocorrelation across the cortex and identify the diﬀerent factors
driving autocorrelation. Second, we evaluated the eﬃcacy of a range of AR-based prewhitening methods at
eliminating autocorrelation and controlling false positives. We found that “local” prewhitening methods that
account for spatial variability strongly outperform “global” methods where the same ﬁlter is applied to each
voxel or vertex of the brain. Third, we developed a fast implementation of local prewhitening, available through
the open-source BayesfMRI R package, that overcomes the computational challenges associated with performing
prewhitening at thousands of locations.

4.1 Variable autocorrelation across the cortex results in spatially diﬀerential

false positive control

Using a mixed eﬀects modeling approach and test-retest data from the Human Connectome Project, we showed
that autocorrelation varies markedly across the cortex and is inﬂuenced by task-related diﬀerences, modeling
choices, acquisition factors, and population variability. As a result, the spatial topology of autocorrelation in each
fMRI scan is unique. Given the spatial variability in autocorrelation, global prewhitening will result in diﬀerential

20

No
Prewhitening

AR(6), Local
Regularization

AR(6), Global
Regularization

(A) Proportion of sessions showing false positives at each vertex

(B) False positive rate (FPR)

(C) Family-wise error rate (FWER)

Figure 11: False positives in resting-state data before and after prewhitening. (A) Values at each
vertex represent the proportion of sessions where a false positive is detected.
(B) Boxplots representing the
distribution across all sessions and subjects of the FPR, deﬁned as the proportion of vertices ﬂagged as false
positives for a given scan. (C) FWER with 95% Agresti-Coull conﬁdence intervals for proportions. Prewhitening
dramatically reduces false positive rates and brings FWER close to the nominal rate of 0.05 (dashed line).
Local regularization of prewhitening parameters achieves near-zero FPR across nearly all sessions. Interestingly,
including additional motion covariates (24 versus 12) seems to worsen both FPR and FWER when used alongside
prewhitening.

21

0.00.51.01.5NoPrewhiteningAR(6),LocalRegularizationAR(6),GlobalRegularizationFalse Positive Rate (%)Motion realignment parameters12240.000.250.500.751.00NoPrewhiteningAR(6),LocalRegularizationAR(6),GlobalRegularizationFamily Wise Error RateMotion realignment parameters1224levels of false positive control across the brain or cortex. And because the spatial topology of autocorrelation is
unique to each fMRI scan, the topology of false positive control as well as power will likewise vary across fMRI
scans, even within the same study. For example, one subject may be less likely to see a signiﬁcant eﬀect in a
certain region compared with another subject in the same study, simply due to diﬀerences in autocorrelation in
that region. These results illustrate the importance of prewhitening techniques that capture the spatial variability
in autocorrelation, in order to avoid diﬀerential false positive rates across the cortex or across the brain.

Current prewhitening methods implemented in major fMRI software tools often use a global prewhitening ap-
proach. One likely reason for this is the computational eﬃciency of global prewhitening, since it requires a single
T ×T matrix inversion, unlike local prewhitening which requires V such inversions. Likewise, the GLM coeﬃcients
can be estimated in a single matrix multiplication step with global prewhitening, whereas local prewhitening re-
quires V multiplications. Another seeming advantage of global regularization of the prewhitening parameters
is the low sampling variability in the estimates of those parameters, though this comes as the cost of large
biases for speciﬁc locations. Local prewhitening can lead to noisier estimates of the prewhitening parameters,
though smoothing can help combat this. While previous work based on volumetric fMRI found smoothing to be
detrimental because of mixing signals across tissue classes (Luo et al. 2020), our use of surface-based smoothing
largely avoids this limitation. Using cortical surface fMRI data also has the advantage of reduced dimensionality
and the option to further reduce dimensionality through resampling without signiﬁcant loss of spatial resolution.
This lower dimensionality, combined with an implementation optimized for speed, makes our approach to local
prewhitening quite feasible (approximately 1 minute per run for the task fMRI we analyze here).

4.2 Implications for volumetric fMRI analyses

Our analysis focused on cortical surface-based analysis, but our ﬁndings have major implications for volumetric
fMRI analysis as well. The issue of spatially varying autocorrelation is actually more salient in volumetric fMRI,
because autocorrelation is known to diﬀer markedly across tissue classes, with CSF generally exhibiting higher
autocorrelation and white matter exhibiting lower autocorrelation (Bollmann et al. 2018, Luo et al. 2020). A
global prewhitening approach in volumetric fMRI may have more severe consequences than in surface-based
analyses because of the more dramatic diﬀerences in autocorrelation across tissue classes. For example, to
eliminate autocorrelation within CSF and thereby control false positive rates there, we may over-whiten within
gray matter. Even if we target the gray matter, standard volumetric smoothing exacerbates diﬀerences in
autocorrelation, increasing autocorrelation in voxels near CSF and decreasing autocorrelation in voxels near
white matter. While these issues point to the importance of local prewhitening in volumetric fMRI analysis, the
higher dimensionality of that data introduces new computational challenges. For example, our implementation
of local prewhitening would take approximately 10 minutes per run for a volumetric analysis involving 100,000
voxels or more, compared with 1 minute per run for 12,000 surface vertices. Additionally, our local regularization
approach of smoothing AR coeﬃcients may not translate seamlessly to volumetric fMRI analysis, given the risks
associated with smoothing across tissue classes. For volumetric fMRI analysis, it may be preferable to avoid AR
coeﬃcient smoothing or employ smoothing techniques that respect tissue class boundaries.

4.3 Low-order AR models perform surprisingly well when allowed to vary

spatially

We were somewhat surprised by the fairly strong performance of AR(1) models when the AR coeﬃcients were
allowed to vary spatially through local regularization. This echoes the relatively strong performance of AFNI (Cox
1996), which assumes a spatially varying ARMA(1,1) model with no smoothing, observed by Olszowy et al. (2019).
In our analysis, locally regularized AR(1) prewhitening consistently outperformed globally regularized AR(6)
prewhitening at reducing autocorrelation. We generally saw the best performance for local AR(6) prewhitening,
but its improvement over local AR(1) was small compared to the diﬀerence between local and global regularization.
This suggests that for volumetric fMRI analysis where the computational burden associated with voxel-speciﬁc
prewhitening may be substantial, lower-order AR models may be worth consideration since they would speed up
estimation of the prewhitening coeﬃcients and matrix inversion.

22

4.4 FWER is not the whole picture

Because many multiplicity correction methods focus on controlling the family-wise error rate (FWER) or the
probability of observing a single false positive voxel, vertex or cluster, FWER control is often used as an evaluation
metric for prewhitening methods. While this can be a useful metric in determining whether important modeling
assumptions (e.g., independent residuals) have been satisﬁed, it is not the only important one. We advocate for
two additional considerations: equal false positive control across the brain or cortex, and avoiding unnecessary
loss of power. Global prewhitening that does not consider the spatial variance in autocorrelation may control the
FWER, but will generally fail to achieve spatially homogeneous levels of false positive control. As a result, we
may be much more likely to observe false positives in certain regions, which may also diﬀer across subjects. In
addition, global prewhitening will tend to over-whiten regions with low autocorrelation, which can lead to overly
conservative inference, manifesting as a lack of power to detect eﬀects. Just as we will be more likely to see false
positives in certain regions, other regions will suﬀer disproportionately from a loss of power to detect real eﬀects.
Indeed, achieving nominal FWER without spatially homogeneous false positive control will almost surely come
at the cost of unnecessary loss of power in many parts of the brain.

4.5 Acquisition-induced distortions change the spatial topology of autocor-

relation and false positive control

One striking ﬁnding in our analysis was the eﬀect of phase encoding direction on the spatial topology of residual
autocorrelation. Comparing the RL and LR phase encoding directions, LR generally produced higher autocorre-
lation in the right lateral cortex and left medial cortex, while RL generally produced higher autocorrelation in the
left lateral cortex and the right medial cortex. In other words, the choice of phase encoding direction generally
had opposing eﬀects on the lateral and medial cortices within each hemisphere, as well as across hemispheres.
Why might this be? The LR and RL phase encoding directions are known to introduce lateralized distortions,
even after distortion correction. These distortions are due to areas of varying magnetic susceptibility giving rise
to signal stretching and signal pile-up based on the direction of the phase-encode direction for acquisitions such
as echo-planar imaging (Jezzard and Clare 1999). Those distortions cause a slight misalignment of the fMRI data
on the structure of the brain. This has the result of mixing CSF signals with higher autocorrelation into some
cortical areas, and mixing white matter signals with lower autocorrelation into others. For example, the higher
autocorrelation on the right lateral cortex with the LR acquisition may result from the introduction of CSF
signals produced by a slight shift of gray matter voxels to the left; a similar shift to the left of the right medial
cortex will introduce white matter signals, resulting in lower autocorrelation. This is somewhat analogous to the
eﬀect of volumetric smoothing on gray matter voxels bordering CSF and white matter observed by (Luo et al.
2020) (though note that these eﬀects of volumetric smoothing would be in addition to, and perhaps exacerbate,
the eﬀects of distortions). As a result, in the HCP there may be sizeable discrepancies in false positive control
and power across and within each hemispheres before prewhitening or when using global prewhitening. While
such lateralized eﬀects are somewhat unique to HCP-style acquisitions that employ left-to-right or right-to-left
phase encoding, other acquisitions can introduce diﬀerent types of distortions that may also change the spatial
topology of autocorrelation. Diﬀerent acquisitions may therefore produce very diﬀerent spatial distributions of
false positive control, if not addressed through an eﬀective local prewhitening strategy.

4.6 Limitations and future directions

Our study has several limitations. First, our analysis was based on a single dataset, the Human Connectome
Project, which includes data from healthy young adults collected using a certain multi-band acquisition protocol.
Previous studies have found the baseline level of autocorrelation, as well as the eﬃcacy of prewhitening, to
diﬀer across datasets of varying TR (Corbin et al. 2018, Olszowy et al. 2019). Future work should examine the
generalizability of our ﬁndings, particularly the eﬃcacy of our local prewhitening approach, to data collected with
diﬀerent acquisition protocols and in more diverse populations. Similarly, a valuable area of future work would
be to assess the eﬃcacy of local prewhitening strategies in volumetric task fMRI data, which presents unique
challenges as described in Section 4.2.

Our study, as well as most prior studies on the eﬃcacy of prewhitening in task fMRI analyses, focused on the
ability of prewhitening techniques to eﬀectively mitigate autocorrelation and control false positives. Here we
discussed, but did not explicitly analyze, the possibility of a loss of power due to over-whitening. We argue that
this is more likely with global prewhitening strategies that aim to achieve false positive control within a particular

23

region or tissue class, at the risk of over-whitening in other areas. A valuable area of future work would be to
examine the eﬀect of diﬀerent prewhitening techniques on power across the brain.

Here, we considered the eﬀect of HRF modeling strategy on residual autocorrelation, and observed that the
inclusion of temporal and dispersion derivatives of the HRF helped to alleviate it. However, we did not consider
alternative, potentially advantageous HRF modeling strategies such as the inverse logit or ﬁnite impulse response
models (Lindquist et al. 2009). When longer scan durations are available, these more ﬂexible models may
help account for additional spatial, within-subject and between-subject heterogeneity in HRF shape, onset and
duration, particularly in more diverse populations. Thus, these more ﬂexible HRF modeling strategies could
further reduce autocorrelation and its spatial variance.

Finally, one limitation of our implementation of AR-based prewhitening implementation is that we did not
account for potential bias in the prewhitening matrix due to using the ﬁtted residuals as a proxy for the true
residuals. Since the ﬁtted residuals have a diﬀerent dependence structure induced by the GLM, their covariance
matrix is not equal to that of the true residuals. This bias will generally be worse in overparameterized GLMs,
which may help explain why we observed a slightly detrimental eﬀect of including all 24 motion regressors when
prewhitening was also performed (see Supplementary Figure A.1). A valuable topic of future work would be to
develop prewhitening methods that formally model and adjust for this source of bias.

5 Conclusion

We performed a comprehensive investigation of the sources and patterns of residual autocorrelation across the
cortical surface in multi-band task fMRI data. Our analysis revealed dramatic spatial diﬀerences in autocor-
relation across the cortex. This spatial topology is unique to each session, being inﬂuenced by the task being
performed, the acquisition technique, various modeling choices, and individual diﬀerences. If not accounted for,
these diﬀerences will result in diﬀerential false positive control and power across the cortex and across subjects.
We evaluated the eﬃcacy of diﬀerent prewhitening methods to mitigate autocorrelation and control false positives.
Our ﬁndings demonstrate that allowing the prewhitening ﬁlter to vary spatially is crucial to eﬀectively reducing
autocorrelation and its spatial variability across the cortex. Our computationally eﬃcient implementation of
“local” prewhitening is available in the open-source BayesfMRI R package.

Funding

This work was supported by the National Institute of Biomedical Imaging and Bioengineering at the National
Institutes of Health (NIH) under grant R01EB027119, by the National Institute of Neurobiological Disorders and
Stroke at the NIH under the grant R25NS117281, and by the National Science Foundation (NSF) under grant
IIS-2023985 from the Division of Information and Intelligent Systems.

References

Afyouni, S., Smith, S. M., and Nichols, T. E. (2019). Eﬀective degrees of freedom of the Pearson’s correlation

coeﬃcient under autocorrelation. NeuroImage, 199:609–625.

Agresti, A. and Coull, B. A. (1998). Approximate is better than “exact” for interval estimation of binomial

proportions. The American Statistician, 52(2):119–126.

Aguirre, G. K., Zarahn, E., and D’Esposito, M. (1998). The variability of human, BOLD hemodynamic responses.

Neuroimage, 8(4):360–369.

Barch, D. M., Burgess, G. C., Harms, M. P., Petersen, S. E., Schlaggar, B. L., Corbetta, M., Glasser, M. F.,
Curtiss, S., Dixit, S., Feldt, C., Nolan, D., Bryant, E., Hartley, T., Footer, O., Bjork, J. M., Poldrack, R.,
Smith, S., Johansen-Berg, H., Snyder, A. Z., and Van Essen, D. C. (2013). Function in the human connectome:
Task-fMRI and individual diﬀerences in behavior. NeuroImage, 80:169–189.

Bates, D., M¨achler, M., Bolker, B., and Walker, S. (2015). Fitting Linear Mixed-Eﬀects Models Using lme4.

Journal of Statistical Software, 67(1):1–48.

24

Benjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach

to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1):289–300.

Bollmann, S., Puckett, A. M., Cunnington, R., and Barth, M. (2018). Serial correlations in single-subject fMRI

with sub-second TR. NeuroImage, 166:152–166.

Braga, R. M. and Buckner, R. L. (2017). Parallel interdigitated distributed networks within the individual

estimated by intrinsic functional connectivity. Neuron, 95(2):457–471.

Brockwell, P. J. and Davis, R. A. (2009). Time series: theory and methods. Springer science & business media.

Buckner, R. L., Krienen, F. M., Castellanos, A., Diaz, J. C., and Yeo, B. T. T. (2011). The organization of the
human cerebellum estimated by intrinsic functional connectivity. Journal of Neurophysiology, 106(5):2322–
2345.

Bullmore, E., Brammer, M., Williams, S. C. R., Rabe-Hesketh, S., Janot, N., David, A., Mellers, J., Howard,
R., and Sham, P. (1996). Statistical methods of estimation and inference for functional MR image analysis.
Magnetic Resonance in Medicine, 35(2):261–277.

Chen, J. E., Polimeni, J. R., Bollmann, S., and Glover, G. H. (2019). On the analysis of rapidly sampled fMRI

data. Neuroimage, 188:807–820.

Choe, A. S., Jones, C. K., Joel, S. E., Muschelli, J., Belegu, V., Caﬀo, B. S., Lindquist, M. A., Van Zijl, P. C.,
and Pekar, J. J. (2015). Reproducibility and temporal structure in weekly resting-state fMRI over a period of
3.5 years. PloS one, 10(10):e0140134.

Corbin, N., Todd, N., Friston, K. J., and Callaghan, M. F. (2018). Accurate modeling of temporal correlations

in rapidly sampled fMRI time series. Human brain mapping, 39(10):3884–3897.

Cox, R. W. (1996). AFNI: software for analysis and visualization of functional magnetic resonance neuroimages.

Computers and Biomedical research, 29(3):162–173.

Delgado, M., Nystrom, L., Fissell, C., Noll, D., and Fiez, J. A. (2000). Tracking the hemodynamic responses to

reward and punishment in the striatum. Journal of neurophysiology, 84 6:3072–7.

Eklund, A., Andersson, M., Josephson, C., Johannesson, M., and Knutsson, H. (2012). Does parametric fMRI

analysis with SPM yield valid results? NeuroImage, 61(3):565–578.

Elliott, M. L., Knodt, A. R., Ireland, D., Morris, M. L., Poulton, R., Ramrakha, S., Sison, M. L., Moﬃtt,
T. E., Caspi, A., and Hariri, A. R. (2020). What is the test-retest reliability of common task-functional MRI
measures? new empirical evidence and a meta-analysis. Psychological Science, 31(7):792–806.

Friston, K. J., Fletcher, P., Josephs, O., Holmes, A., Rugg, M., and Turner, R. (1998a). Event-related fMRI:

characterizing diﬀerential responses. Neuroimage, 7(1):30–40.

Friston, K. J., Holmes, A. P., Worsley, K. J., Poline, J.-P., Frith, C. D., and Frackowiak, R. S. (1994). Statistical

parametric maps in functional imaging: a general linear approach. Human brain mapping, 2(4):189–210.

Friston, K. J., Josephs, O., Rees, G., and Turner, R. (1998b). Nonlinear event-related responses in fMRI. Magnetic

resonance in medicine, 39(1):41–52.

Glasser, M. F., Sotiropoulos, S. N., Wilson, J. A., Coalson, T. S., Fischl, B., Andersson, J. L., Xu, J., Jbabdi, S.,
Webster, M., Polimeni, J. R., et al. (2013). The minimal preprocessing pipelines for the Human Connectome
Project. Neuroimage, 80:105–124.

Glover, G. H. (1999). Deconvolution of impulse response in event-related bold fMRI1. Neuroimage, 9(4):416–429.

Gordon, E. M., Laumann, T. O., Gilmore, A. W., Newbold, D. J., Greene, D. J., Berg, J. J., Ortega, M., Hoyt-
Drazen, C., Gratton, C., Sun, H., et al. (2017). Precision functional mapping of individual human brains.
Neuron, 95(4):791–807.

Hariri, A. R., Tessitore, A., Mattay, V. S., Fera, F., and Weinberger, D. R. (2002). The Amygdala Response to

Emotional Stimuli: A Comparison of Faces and Scenes. NeuroImage, 17:317–323.

Jenkinson, M., Beckmann, C. F., Behrens, T. E., Woolrich, M. W., and Smith, S. M. (2012). Fsl. Neuroimage,

62(2):782–790.

25

Jezzard, P. and Clare, S. (1999). Sources of distortion in functional mri data. Human brain mapping, 8(2-3):80–85.

Kariya, T. and Kurata, H. (2004). Generalized least squares. John Wiley & Sons.

Laumann, T. O., Gordon, E. M., Adeyemo, B., Snyder, A. Z., Joo, S. J., Chen, M.-Y., Gilmore, A. W., McDer-
mott, K. B., Nelson, S. M., Dosenbach, N. U., et al. (2015). Functional system and areal organization of a
highly sampled individual human brain. Neuron, 87(3):657–670.

Lindquist, M. A. (2008). The Statistical Analysis of fMRI Data. Statistical Science, 23(4):439–464.

Lindquist, M. A., Loh, J. M., Atlas, L. Y., and Wager, T. D. (2009). Modeling the hemodynamic response

function in fMRI: eﬃciency, bias and mis-modeling. Neuroimage, 45(1):S187–S198.

Lindquist, M. A. and Wager, T. D. (2007). Validity and power in hemodynamic response modeling: A comparison

study and a new approach. Human Brain Mapping, 28(8):764–784.

Ljung, G. M. and Box, G. E. (1978). On a measure of lack of ﬁt in time series models. Biometrika, 65(2):297–303.

Loh, J. M., Lindquist, M. A., and Wager, T. D. (2008). Residual analysis for detecting mis-modeling in fMRI.

Statistica Sinica, 18:1421–1448.

Luo, Q., Misaki, M., Mulyana, B., Wong, C.-K., and Bodurka, J. (2020). Improved autoregressive model for

correction of noise serial correlation in fast fMRI. Magnetic Resonance in Medicine.

Marcus, D., Harwell, J., Olsen, T., Hodge, M., Glasser, M., Prior, F., Jenkinson, M., Laumann, T., Curtiss,
S., and Van Essen, D. (2011). Informatics and data mining tools and strategies for the human connectome
project. Frontiers in neuroinformatics, 5:4.

Mejia, A. F., Spencer, D., Pham, D., Bolin, D., Ryan, S., and Yue, Y. R. (2022). BayesfMRI: Bayesian Methods

for Functional MRI. R package version 0.2.0.

Mejia, A. F., Yue, Y., Bolin, D., Lindgren, F., and Lindquist, M. A. (2020). A Bayesian general linear modeling
approach to cortical surface fMRI data analysis. Journal of the American Statistical Association, 115(530):501–
520.

Monti, M. M. (2011). Statistical analysis of fMRI time-series: a critical review of the GLM approach. Frontiers

in human neuroscience, 5:28.

Mumford, J. A. and Nichols, T. (2009). Simple group fMRI modeling and inference. Neuroimage, 47(4):1469–1475.

Olszowy, W., Aston, J., Rua, C., and Williams, G. B. (2019). Accurate autocorrelation modeling substantially

improves fMRI reliability. Nature communications, 10(1):1–11.

Penny, W. D., Friston, K. J., Ashburner, J. T., Kiebel, S. J., and Nichols, T. E. (2011). Statistical parametric

mapping: the analysis of functional brain images. Elsevier.

Pham, D., Muschelli, J., and Mejia, A. (2022). ciftitools: A package for reading, writing, visualizing, and

manipulating CIFTI ﬁles in R. NeuroImage, page 118877.

R Core Team (2022). R: A Language and Environment for Statistical Computing. R Foundation for Statistical

Computing, Vienna, Austria.

Sakamoto, Y., Ishiguro, M., and Kitagawa, G. (1986). Akaike information criterion statistics. Dordrecht, The

Netherlands: D. Reidel, 81(10.5555):26853.

Smith Rachelle, K. K. and Kalina, C. (2007). Localizing the rostrolateral prefrontal cortex at the individual level.

NeuroImage, 36(4):1387–1396.

Spencer, D., Yue, Y. R., Bolin, D., Ryan, S., and Mejia, A. F. (2022). Spatial Bayesian GLM on the cortical

surface produces reliable task activations in individuals and groups. NeuroImage, 249:118908.

Van Essen, D. C., Smith, S. M., Barch, D. M., Behrens, T. E., Yacoub, E., Ugurbil, K., Consortium, W.-M. H.,

et al. (2013). The WU-Minn human connectome project: an overview. Neuroimage, 80:62–79.

Venables, W. N. and Ripley, B. D. (2013). Modern applied statistics with S-PLUS. Springer Science & Business

Media.

26

Woolrich, M. W., Ripley, B. D., Brady, M., and Smith, S. M. (2001). Temporal autocorrelation in univariate

linear modeling of fMRI data. Neuroimage, 14(6):1370–1386.

Worsley, K. J., Liao, C. H., Aston, J., Petre, V., Duncan, G., Morales, F., and Evans, A. C. (2002). A general

statistical analysis for fMRI data. Neuroimage, 15(1):1–15.

27

A Supplementary Figures

Average ACI,
12 RPs

Average ACI,
24 RPs

Diﬀerence
(24 RPs - 12 RPs)

Figure A.1: Eﬀect of including additional motion covariates on autocorrelation, when eﬀective
prewhitening is performed. For prewhitening, we use an AR(6) model with local regularization of AR model
coeﬃcients, which we observe to be highly eﬀective at reducing autocorrelation. The ﬁrst two columns show
the average autocorrelation index (ACI) across all subjects, sessions and tasks when 12 realignment parameters
(RPs) or 24 RPs are included in each GLM. The last column shows the diﬀerence (24 RPs - 12 RPs). The mostly
warm colors indicate that using additional RPs results in very slightly worse autocorrelation when eﬀective
prewhitening is performed.

B Prewhitening Algorithm

Result: Prewhitened data values for the response and design across all vertices v = 1, . . . , V
for v ← 1 to V do

Find estimates ˆβv by ﬁtting the GLM at vertex v;
Find AR coeﬃcients φv,1, . . . , φv,p and white noise variance sv after global or local regularization for
the time series yv − X ˆβv using the Levinson-Durbin recursion (Brockwell and Davis 2009);
Create the symmetric temporal precision matrix Q−1
the diagonal and −φv,q for the qth location oﬀ the diagonal;
Set S−1
Find the eigenvectors and eigenvalues of S−1
diagonal matrix of corresponding eigenvalues;
Compute the prewhitening matrix Wv = UvDvU(cid:48)
v;
Set oﬀ-diagonal values Wij = 0 at row i and column j within Wv if |i − j| > p;

v , where Uv is the matrix of eigenvectors and Dv is the

as a sparse band matrix taking the value 1 on

v = ((1/sv)IT )Q−1
v ;

v

end
Create W as a block-diagonal matrix for all prewhitening matrices Wv;
Output prewhitened response ˜Y = WY and block-diagonal prewhitened design matrix ˜X with elements
WvX;

Algorithm 1: Prewhitening time series data in order to reduce temporal dependence in residuals.
Steps in italics are performed using C++ for computational eﬃciency.

1

