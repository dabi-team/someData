Knowledge Transfer and Distillation from Autoregressive to
Non-Autoregressive Speech Recognition

Xun Gong, Zhikai Zhou, Yanmin Qian†

MoE Key Lab of Artiﬁcial Intelligence, AI Institute
X-LANCE Lab, Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, China
{gongxun,zhikai.zhou,yanminqian}@sjtu.edu.cn

2
2
0
2

l
u
J

5
1

]

D
S
.
s
c
[

1
v
0
0
6
0
1
.
7
0
2
2
:
v
i
X
r
a

Abstract

Modern non-autoregressive (NAR) speech recognition systems
aim to accelerate the inference speed; however, they suffer from
performance degradation compared with autoregressive (AR)
models as well as the huge model size issue. We propose a
novel knowledge transfer and distillation architecture that lever-
ages knowledge from AR models to improve the NAR perfor-
mance while reducing the model’s size. Frame- and sequence-
level objectives are well-designed for transfer learning. To fur-
ther boost the performance of NAR, a beam search method
on Mask-CTC is developed to enlarge the search space during
the inference stage. Experiments show that the proposed NAR
beam search relatively reduces CER by over 5% on AISHELL-
1 benchmark with a tolerable real-time-factor (RTF) increment.
By knowledge transfer, the NAR student who has the same size
as the AR teacher obtains relative CER reductions of 8/16% on
AISHELL-1 dev/test sets, and over 25% relative WER reduc-
tions on LibriSpeech test-clean/other sets. Moreover, the ∼9x
smaller NAR models achieve ∼25% relative CER/WER reduc-
tions on both AISHELL-1 and LibriSpeech benchmarks with
the proposed knowledge transfer and distillation.
Index Terms: knowledge transfer, knowledge distillation, non-
autoregressive, end-to-end, speech recognition

1. Introduction

In recent years, the performance of automatic speech recog-
nition (ASR) has been greatly improved by sequence-to-
sequence modeling, such as connectionist temporal classiﬁca-
tion (CTC) [1], recurrent neural network transducer (RNN-
T) [2], and attention-based encoder-decoder (AED) [3]. Many
of the earlier researches have focused on autoregressive (AR)
modeling, which generates the token sequence using a left-to-
right probabilistic chain rule. Despite their great performance,
such AR models require L-step incremental model calculations
to generate L tokens, resulting in high inference latency and
considerable computational costs.

From another aspect, non-autoregressive (NAR) modeling
generates the token sequence in constant steps, and removes the
chain-rule assumption. CTC [1] plays an essential role in recent
NAR researches. Modern NAR approaches outperform CTC
by leveraging alignments (align-based) and the output token
sequence (token-based). Based on the joint CTC/attention ar-
chitecture [3], Mask-CTC [4] utilizes the (conditional) masked
language model ((C)MLM) decoder to reﬁne the CTC token se-
quence.
[5] proposes two auxiliary tasks to solve the length
prediction problem that occurred in Mask-CTC. From another

†corresponding author

point of view, CTC alignment shows its advantage for build-
ing NAR models in Align-Reﬁne [6], CASS-NAT [7] and AL-
NAT [8]. Moreover, the self-supervised model wav2vec2.0 [9]
has achieved promising results by CTC.

However, there are two major challenges remaining for
NAR modeling: First, NAR models converge slowly and per-
form poorly compared to state-of-the-art (SOTA) AR mod-
els [4, 10]. Second, although NAR models are often favored
in resource-constrained situations for fast inference speed and
high accuracy [4], the large model scale and computational
cost limit the application of NAR modeling. Knowledge dis-
tillation (Transfer learning) is typically used to solve such a
problem by teaching a smaller student model [11]. To be
speciﬁc, the student aims to mimic the soft targets provided
by the well-trained teacher using the Kullback-Leibler diver-
gence (KLD) [12, 13, 14]. Nevertheless, when applying knowl-
edge distillation on non-autoregressive ASR, the poor NAR
teacher limits the improvement.

In this paper, we propose a novel architecture by transfer-
ring and distilling the knowledge from an autoregressive (AR)
teacher to a non-autoregressive (NAR) student together with a
beam-search decoding method to boost the performance of non-
autoregressive modeling. Firstly, we introduce a beam-search
decoding method to enlarge the search space for the (condi-
tional) masked language model ((C)MLM) decoder [15]. Then,
we extend the knowledge distillation technique by transferring
the AR teacher’s knowledge to NAR in two distillation levels,
therefore improving NAR students’ performance. The encoder
distillation is conducted following our previous setup [12]. For
the decoder distillation, we develop the frame- and sequence-
level distillation from the attention-based autoregressive model
into Mask-CTC. The distillation loss is customized for token-
based NAR models, so that the NAR decoder can beneﬁt from
the AR decoder.

The structure of the paper is organized as follows:
In
Section 2, the attention-based autoregressive model and non-
autoregressive Mask-CTC are brieﬂy introduced. In Section 3,
we present the proposed beam search method for Mask-CTC
and the knowledge distillation from AR to NAR ASR. In Sec-
tion 4, experimental results and analysis are given on the Man-
darin AISHELL-1 and English LibriSpeech datasets. Finally,
the conclusion is drawn in Section 5.

2. Autoregressive and Non-autoregressive
ASR

Basically, end-to-end ASR models map speech features X =
[x1, x2, · · · , xT ](cid:124), xt ∈ RF to a token sequence y =
[y1, y2, · · · , yL](cid:124), yl ∈ U, where F is the feature dimension

 
 
 
 
 
 
and U denotes the vocabulary set.

3.1. Beam Search for NAR ASR

Traditional attention-based autoregressive (AR) ASR mod-
els [16, 17] ﬁrstly encode speech features X into a hidden rep-
resentation H: H = Encoder(X), and then compose it with
previous tokens y<l to estimate the posterior p(yl|y<l, X):

par(yl|y<l, H) = Decoder(y<l, H),

(1)

and the whole sequence probability is

par(y|H) = par(y1|H)

L
(cid:89)

l=2

par(yl|y<l, H).

(2)

During inference, the AR model generates hypothesis ˆy token-
by-token.

Connectionist temporal classiﬁcation (CTC) [1] is one of
the earliest non-autoregressive (NAR) method, which intro-
duces a many-to-one function η from the frame-level alignment
Z = [z1, z2, · · · , zT ](cid:124), zt ∈ U ∪ {blank} to the token se-
quence y, by merging same labels and removing the blank in
Z. The sequence probability is represented as:

pctc(y|H) =

(cid:88)

(cid:89)

Z∈η−1(y)

t

pctc(zt|H),

(3)

where η is a many-to-one function from Z to y. During infer-
ence, greedy CTC predicts the alignment by selecting the tokens
with the highest probability for each step.

Mask-CTC [4], which is a popular instantiation of NAR
ASR, is actually a reﬁnement of CTC results via the condi-
tional masked language model (MLM) [15]. During training,
the groundtruth y are randomly replaced by the special to-
ken <MASK> , and the MLM decoder predicts masked tokens
ymask, conditioned on the observed tokens yobs = y \ ymask:

pmlm(ymask|yobs, H) =

(cid:89)

y∈ymask

pmlm(y|yobs, H).

(4)

During inference,
the output is initialized by CTC greedy
decoding, and low-conﬁdence tokens are substituted by
<MASK> based on pre-deﬁned threshold pthr. After that, masks
are ﬁlled using easy-ﬁrst algorithm: ﬁll all masks in (cid:100)N/K(cid:101) it-
erations, where N denotes the total number of <MASK> and
each iteration predicts top-k tokens with the highest conﬁdence
guided by MLM:

ˆC = arg max
C⊂ymask,
|C|=k

(cid:89)

c∈C

pmlm(c|yobs, H),

(5)

(cid:40)

where k =

N mod K,
K,
date set of <MASK> tokens and (cid:98)y(cid:48)
result after mask ﬁlling.

the last iteration
otherwise

, C is the candi-

obs = yobs + ˆC is the updated

The joint CTC/attention architecture [3, 4] is widely used in
modern AR and NAR ASR models, with a multi-task learning
based loss function:

Ljca = αLctc + (1 − α)Latt,

(6)

where α ∈ [0, 1] is a hyperparameter, Latt = Lar for AR ASR,
and Latt = Lmlm for NAR ASR, respectively.

3. Proposed Methods

In this section, we introduce: (1) the proposed beam search
method for NAR, (2) the distillation architecture transferring
knowledge from AR to NAR ASR.

Inspired by joint and rescoring decoding [3], we design a beam-
search decoding method to enlarge the search space for the
MLM decoder. The procedure is shown in Algorithm 1. Ω1
is a sorted queue to be updated at the beginning of one iteration,
Ω0 stores the ﬁnal Ω1 after one iteration. During each iteration,
a B-size beam is preserved, and the number of updated tokens
is ﬁxed and computed by k (i.e. |(cid:98)yi+1
obs| + k). Top-B
candidates are selected, according to the log domain posterior
probability and Equation 5.

obs | = |(cid:98)yi

Algorithm 1: Beam Search on NAR MLM Decoding

obs by pthr ;

1 ˆy ← greedy CTC ;
2 ˆymask and ˆy0
3 Ω0 ← { ˆy0
4 for each iteration i = 1, 2, · · · , (cid:100)N/K(cid:101) do
5

Ω1 ← ∅: Ω1 is a sorted queue with B length ;

obs}: a list to store accepted hypotheses ;

(cid:40)

k =

N mod K,
K,
for yobs ∈ Ω0 do

the last iteration,
otherwise.

;

(cid:48)1
Get top-B candidate sets (cid:98)y
obs, · · · , (cid:98)y
posterior in Equation 5 ;
(cid:48)1
obs, · · · , (cid:98)y
Push (cid:98)y
Ω0 = Ω1 ;

(cid:48)B
obs into Ω1 ;

(cid:48)B
obs by the

6

7

8

9

10

11 return arg max

ˆy ;

ˆy∈Ω0

3.2. Knowledge Transfer and Distillation from Autoregres-
sive to Non-Autoregressive ASR

As previously stated, knowledge distillation performance on
NAR is constrained owing to the poor performance of the NAR
teacher. Here, we propose knowledge transfer and distillation
from autoregressive (AR) to non-autoregressive (NAR) ASR,
pushing the limits of NAR.

Firstly, we introduce two types of knowledge distilla-
tion techniques based on Kullback-Leibler divergence (KLD):
KLD(P, Q) = (cid:80)
i Pi log(Pi/Qi), where P, Q are the teacher
and student output distributions, respectively.

Frame-level knowledge distillation as a basic distillation

criterion is formulated as below:

LF-KD = −

T
(cid:88)

(cid:88)

t=1

c∈U

Pt(c) log Qt(c),

(7)

where Pt(c) and Qt(c) are the posterior probabilities of token
c at timestamp t of teacher P and student Q. H, yobs, and y<t
are the conditions of the above probabilities, but omitted for
simplicity. Pt(c) log Pt(c) is omitted in computing KLD loss
due to the frozen teacher during training.

Sequence-level knowledge distillation is another distilla-

tion criterion:

LS-KD = −

(cid:88)
(cid:98)yi∈τ

P ( (cid:98)yi) log Q( (cid:98)yi)

(8)

where (cid:98)yi is the hypotheses from teacher model, τ is the set of
all possible sequences and similar omission as in Equation 7.
Using such sequence-level knowledge distillation is unafford-
able, as we are approximating an exponentially-sized sequence

And the ﬁnal loss is then:

L = Lstudent

jca + γencLEncoder

KD + γdecLDecoder

KD

,

(13)

where γenc, γdec are weight coefﬁcients for encoder and decoder
knowledge distillation.

4. Experiments

4.1. Datasets

Our experiments are conducted on the Mandarin AISHELL-
1 [19] and the English LibriSpeech corpora [20]. AISHELL-1
contains a 150h training set, with a development (dev) and test
set for evaluation, while LibriSpeech has a 960h training set,
with test-clean/other (test c/o) used for tests. We report the char-
acter error rate (CER) on AISHELL-1, word error rate (WER)
on LibriSpeech, respectively.

4.2. Model description

For acoustic feature extraction, 80-dimensional mel ﬁlter-
bank (Fbank) features are extracted with global level cepstral
mean and variance normalization (CMVN). When it comes
to data augmentation, speed perturbation is applied only for
AISHELL-1 and SpecAugment [21] for both datasets, respec-
tively. For text modeling, 5000 English Byte Pair encod-
ing (BPE) [22] subword units are adopted for English, and 4233
characters for Mandarin. The baseline follows the recipe of
ESPnet v2 [23], a 12-layer conformer encoder with four times
down-sampling and a 6-layer transformer decoder. The weight
α for CTC module is ﬁxed to 0.3.

For knowledge transfer and distillation, we ﬁrstly train a
new NAR student model from scratch with LF-KD for 80 epochs.
The hyper-parameters are set to βF = 1.0, βS = 0, γenc =
0.5, γdec = 0.3. Then we ﬁne-tune the distillation proce-
dure by adding LS-KD, and the tuning parameters are set as
βF = 1.0, βS = 1.0, γenc = 0.5, γdec = 0.5 with 20 epochs. In
Equation 8, 9, we use beam-size |Ω| = 10, which is consistent
with the decoding hyper-parameters in AR model.

Different NAR student model sizes are explored in Ta-
ble 2, identiﬁed as large (L), medium (M), small (S), and ex-
tremely small (XS). The AR teacher model keeps L size for
LibriSpeech, and M-size for AISHELL-1.

In the inference stage, no language model is used dur-
ing the following experiments. Model parameters are aver-
aged over the last 5 checkpoints. For autoregressive models,
joint CTC/attention one-pass decoding [3] is used with beam
size 10, and the score interpolation of CTC is 0.3. For non-
autoregressive Mask-CTC decoding, we follow the beam de-
coding method in Section 3.1, with beam B = 10, the thresh-
old pthr = 0.99 and K = 2, for AISHELL-1 and LibriSpeech
corpus.

4.3. Results with the NAR Beam Decoding

As proposed in Section 3.1, we ﬁrstly evaluate the beam search
performance with real time factor (RTF) in Table 3. RTF is
computed using Intel-Xeon E5-2690 CPU with a single core at
test set. To be consistent with literature [4], the NAR (M) model
speeds up AR (M) [3] model by more than 10x, as the RTF is
0.58 for AR (M) and 0.31 for AR (S). Without too much degra-
dation of inference speed (1.5x as slow as ‘Beam1’), the beam
decoding method achieves better performance compared with
the greedy (Beam1) one by 5%∼9% relative WER reduction on

Figure 1: Overview of proposed knowledge distillation from au-
toregressive to non-autoregressive ASR.

distribution τ . Similar to MWER [18] training, N-best candi-
dates set Ω is accessed by beam search, and then P ( (cid:98)yi) can be
approximated by:

P ( (cid:98)yi) (cid:39)

(cid:80)

P ( (cid:98)yi)
(cid:98)yj ∈Ω P ( (cid:98)yj)

.

(9)

And then we can achieve the knowledge distillation loss by:

LKD = βF LF-KD + βSLS-KD,

(10)

where βF , βS are hyper-parameters for frame-level knowledge
distillation loss LF-KD and sequence-level LS-KD, respectively.

As shown in Figure 1, the proposed knowledge distillation
methods are divided into two parts: the ﬁrst part is the distil-
lation after the encoder, and the second part is the distillation
after the decoder. The encoder distillation LEncoder
is done af-
ter the linear layer of the encoder, which has LF-KD and LS-KD
similar to literature [12]. The decoder distillation is setup as
follows. For frame-level distillation, only ymask positions are
selected, so the objective function is normalized by the number
of <MASK> tokens:

KD

LDecoder

F-KD = −

(cid:88)

(cid:88)

t∈ymask

c∈U

Pt(c|y<t, H)
|ymask|

log Qt(c|yobs, H).

(11)

For sequence-level distillation, approximate probability P (cid:48)

from N-best Ω is used:

LDecoder

S-KD = −

(cid:88)

ˆy∈Ω

P (cid:48)( ˆy)
|ymask|

log Q(ymask|yobs, H)).

(12)

NAR EncoderLinearAR EncoderAR DecoderLinearNAR DecodermaskingAR Teacher NAR Student Table 1: Knowledge transfer and distillation performance on AISHELL1 corpus (CER) (%) and Librispeech test corpus (WER) (%),
‘I+D’ is the sum of insertion and deletion errors, and ‘A’ is the total CER/WER. ‘#Param’ has brackets with ‘XS’, ‘S’, ‘M’, ‘L’, which
are mentioned in Table 2. ‘Same size’ means NAR has the same model scale as AR, ‘Smaller’ means NAR is ∼9x smaller than AR.

Method

Criterion

W2V2-CTC [10, 9]
CASS-NAT v2 [7]
AL-NAT [8]
AR
NAR (Same size)

NAR (Smaller)

-
-
-
Ljca
Ljca
+LF-KD
++LS-KD
Ljca
+LF-KD
++LS-KD

#Param

95M
-
71.3M
46.8M (M)
46.8M (M)
-
-
6.5M (XS)
-
-

AISHELL1
Dev

Test

I+D
-
-
-
0.2
0.4
0.2
0.2
0.4
0.2
0.2

A
4.8
4.9
4.9
4.5
5.4
5.1
5.0
7.2
6.1
6.0

I+D
-
-
-
0.2
0.5
0.3
0.2
2.1
0.5
0.3

A
5.3
5.4
5.3
4.9
6.4
5.6
5.4
8.5
6.9
6.5

#Param

95M
-
∼115M
115.0M (L)
115.0M (L)
-
-
12.4M (S)
-
-

Librispeech Test
clean

other

I+D
-
-
-
0.2
0.6
0.5
0.4
0.8
0.4
0.3

A
2.7
3.1
3.2
2.8
4.1
3.5
3.3
5.1
3.8
3.7

I+D
-
-
-
1.3
2.0
1.6
1.4
2.4
1.8
1.4

A
8.0
7.2
7.4
6.6
10.2
8.4
7.8
12.4
10.3
9.2

Table 2: Model hyper-parameters for different AR and NAR
Conformer scales at L, M, X and XS.

Model
#Params (M)
Attention Dim
Attention Heads
Inner-dim

L
115.0
512
8
2048

M
46.8
256
4
2048

S
12.4
128
4
1024

XS
6.5
128
4
256

the test set. As the beam size B grows, the rate of improvement
decreases.

Table 3: Non-autoregressive Mask-CTC Performance (CER) on
AISHELL-1 corpus. Real time factor (RTF) are reported for the
test set.

Decoding
Non-Autoregressive (M)

Dev (%)

Test (%)

RTF

Beam1
Beam5
Beam10

5.4
5.4
5.4

Non-Autoregressive (XS)

Beam1
Beam5
Beam10

7.6
7.3
7.2

6.7
6.4
6.4

9.2
8.6
8.5

0.051
0.064
0.072

0.017
0.023
0.027

4.4. Results on Knowledge Transfer and Distillation

Table 1 compares the knowledge transfer distillation and other
modern AR and NAR models on AISHELL-1 and LibriSpeech
datasets to validate the performance.

AISHELL-1: As shown in Table 1, the teacher AR model
obtains more than 24% relative reduction on CER compared
with NAR (M), and 40% with NAR (XS). After knowledge dis-
tillation, the NAR (M) with ‘+LF-KD’ achieves 8% and 16% rel-
ative CER reduction on dev and test sets respectively, and the
one based on ‘+LF-KD’ with ‘++LS-KD’ shows a further CER re-
duction on test set by 15%. The student results achieve compet-
itive performance (5.0%/5.4% CER) to the state-of-the-art NAR
models like CASS-NAT [7] or AL-NAT [8]. Similar results are
obtained for distilled NAR (XS) as 18%/25% CER reduction on
two evaluation sets.

LibriSpeech: Table 1 shows the performance compari-
son on the large LibriSpeech corpus. The AR (L) is adopted

as teacher model while NAR (L,S) as student model. The ob-
servations are consistent with that in Table 1, LF-KD and LS-KD
further boost the performance of NAR Mask-CTC model at
L (3.3/7.8% WER) and S (3.7/9.2% WER) scale by ∼25% rel-
ative WER reduction. However, due to the limits of the AR
teacher, the insertion and deletion error rate is high on AR (L).
Results show that such our method narrows the gap between
AR and NAR, with the improvement being signiﬁcantly greater
in the more difﬁcult evaluation set (i.e. test set in AISHELL-1,
test-other in LibriSpeech). After knowledge transfer and distil-
lation, the length error problem is greatly alleviated compared
with the original NAR model owing to the high prediction accu-
racy of AR teacher. Moreover, both LF-KD and LS-KD attribute
to reducing insertion and deletion errors (‘I+D’), pushing the
length error problem [4] to its limits at 0.2% CER for ‘I+D’
in AISHELL-1 and 1.4% for LibriSpeech test-other. Mean-
while, the NAR student model performs comparable results
with other state-of-the-art NAR methods, including wav2vec2-
CTC [9, 10], Improved CASS-NAT [7] and AL-NAT [8].

5. Conclusions

In this paper we propose a novel knowledge transfer and dis-
tillation architecture that leverages knowledge from AR mod-
els to improve NAR performance while reducing the model
size. To further boost the performance of NAR, we propose a
beam search method on Mask-CTC, which enlarges the search
space during inference stage. Experiments demonstrate that
NAR beam search obtains relative 5% reduction in AISHELL-1
dataset with a tolerable RTF increment. For knowledge distil-
lation, most results achieve over 15% relative CER/WER re-
duction on large and smaller NAR modeling. Future works
are going to explore the generalization of knowledge distilla-
tion from AR to NAR. Different non-autoregressive like CASS-
NAT [7] and AL-NAT [8] might be explored with external
non-autoregressive language models. Hidden feature distilla-
tion [11] is also a valuable extension of this paper.

6. Acknowledgment

This work is supported in part by China NSFC projects un-
der Grants 62122050 and 62071288, and in part by Shanghai
Municipal Science and Technology Major Project under Grant
2021SHZDZX0102. The authors would like to thank Tian tan
and Wangyou Zhang for discussion.

[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems, vol. 30, Dec.
2017.

[17] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu,
W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, “Conformer:
Convolution-augmented transformer for speech recognition,” in
Proc. Interspeech 2020, May 2020, pp. 5036–5040.

[18] R. Prabhavalkar, T. N. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.-
C. Chiu, and A. Kannan, “Minimum word error rate training for
attention-based sequence-to-sequence models,” in 2018 IEEE In-
ternational Conference on Acoustics, Speech and Signal Process-
ing (ICASSP).

IEEE, 2018, pp. 4839–4843.

[19] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “AISHELL-1: An
open-source Mandarin speech corpus and a speech recognition
baseline,” in 2017 20th Conference of the Oriental Chapter of
the International Coordinating Committee on Speech Databases
and Speech I/O Systems and Assessment (O-COCOSDA).
IEEE,
2017, pp. 1–5.

[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
rispeech: An ASR corpus based on public domain audio books,”
in ICASSP 2015 - 2015 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP).
IEEE, 2015, pp.
5206–5210.

[21] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,
and Q. V. Le, “Specaugment: A simple data augmentation method
for automatic speech recognition,” Interspeech 2019, pp. 2613–
2617, Sep. 2019.

[22] T. Kudo and J. Richardson, “Sentencepiece: A simple and lan-
guage independent subword tokenizer and detokenizer for neural
text processing,” in Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing: System Demon-
strations, 2018, pp. 66–71.

[23] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno,
N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen,
A. Renduchintala, and T. Ochiai, “ESPnet: End-to-end speech
processing toolkit,” in Proc. Interspeech 2018, 2018, pp. 2207–
2211.

7. References
[1] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Con-
nectionist
labelling unsegmented se-
quence data with recurrent neural networks,” in Proc. ICML,
2006, pp. 369–376.

temporal classiﬁcation:

[2] E. Battenberg, J. Chen, R. Child, A. Coates, Y. G. Y. Li, H. Liu,
S. Satheesh, A. Sriram, and Z. Zhu, “Exploring neural trans-
ducers for end-to-end speech recognition,” in 2017 IEEE Auto-
matic Speech Recognition and Understanding Workshop (ASRU).
IEEE, 2017, pp. 206–213.

[3] T. Hori, S. Watanabe, and J. R. Hershey, “Joint CTC/attention
decoding for end-to-end speech recognition,” in Proceedings of
the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 2017, pp. 518–529.

[4] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,
“Mask CTC: Non-autoregressive end-to-end ASR with CTC and
mask predict,” in Interspeech 2020.
ISCA, Oct. 2020, pp. 3655–
3659.

[5] Y. Higuchi, H.

Inaguma, S. Watanabe, T. Ogawa,

and
T. Kobayashi, “Improved mask-ctc for non-autoregressive end-to-
end asr,” in ICASSP 2021-2021 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP).
IEEE,
2021, pp. 8363–8367.

[6] E. A. Chi, J. Salazar, and K. Kirchhoff, “Align-reﬁne: Non-
autoregressive speech recognition via iterative realignment,” in
Proceedings of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics: Human Lan-
guage Technologies. Online: Association for Computational Lin-
guistics, Jun. 2021, pp. 1920–1927.

[7] R. Fan, W. Chu, P. Chang, J. Xiao, and A. Alwan, “An improved
single step non-autoregressive transformer for automatic speech
recognition,” in Proc. Interspeech 2021, Jun. 2021.

[8] Y. Wang, R. Liu, H. Zhang, F. Bao, and G. Gao, “Alignment-
learning based single-step decoding for accurate and fast non-
autoregressive speech recognition,” in ICASSP 2022 - 2022 IEEE
International Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), Feb. 2022.

[9] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “Wav2vec 2.0:
A framework for self-supervised learning of speech representa-
tions,” in Advances in Neural Information Processing Systems,
vol. 33, Oct. 2020, pp. 12 449–12 460.

[10] K. Deng, Z. Yang, S. Watanabe, Y. Higuchi, G. Cheng, and
P. Zhang, “Improving non-autoregressive end-to-end speech
recognition with pre-trained acoustic and language models,”
arXiv:2201.10103 [cs, eess], Jan. 2022.

[11] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distilla-
tion: A survey,” International Journal of Computer Vision, vol.
129, no. 6, pp. 1789–1819, 2021.

[12] M. Huang, Y. You, Z. Chen, Y. Qian, and K. Yu, “Knowledge dis-
tillation for sequence model,” Proc. Interspeech 2018, p. 5, 2018.

[13] R. Takashima, L. Sheng, and H. Kawai, “Investigation of
sequence-level knowledge distillation methods for CTC acoustic
models,” in ICASSP 2019 - 2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), May 2019,
pp. 6156–6160.

[14] R. M. Munim, N. Inoue, and K. Shinoda, “Sequence-level
knowledge distillation for model compression of attention-based
sequence-to-sequence speech recognition,” in ICASSP 2019 -
2019 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), May 2019, pp. 6151–6155.

[15] M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer, “Mask-
predict: Parallel decoding of conditional masked language mod-
els,” in Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-
IJCNLP). Hong Kong, China: Association for Computational
Linguistics, Nov. 2019, pp. 6112–6121.

