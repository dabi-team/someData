Open-Domain Dialogue Generation Based on Pre-trained Language
Models

Yan Zeng
DIRO, Universit´e de Montr´eal
yan.zeng@umontreal.ca

Jian-Yun Nie
DIRO, Universit´e de Montr´eal
nie@iro.umontreal.ca

0
2
0
2

t
c
O
4
2

]
L
C
.
s
c
[

1
v
0
8
7
2
1
.
0
1
0
2
:
v
i
X
r
a

Abstract

Pre-trained language models have been suc-
cessfully used in response generation for open-
Four main frameworks
domain dialogue.
have been proposed: (1) Transformer-ED us-
ing Transformer encoder and decoder sepa-
rately for source and target sentences;
(2)
Transformer-Dec using Transformer decoder
for both source and target sentences;
(3)
Transformer-MLM using Transformer decoder
that applies bi-directional attention on the
source side and left-to-right attention on the
target side with masked language model objec-
tive; and (4) Transformer-AR that uses auto-
regressive objective instead. In this study, we
compare these frameworks on 3 datasets, and
our comparison reveals that the best frame-
work uses bidirectional attention on the source
side and does not separate encoder and de-
coder. We also examine model discrepancy,
and our experiments conﬁrm that the perfor-
mance of a model is directly impacted by the
underlying discrepancies. We then propose
two correction methods to reduce the discrep-
ancies, and both improve the model perfor-
mance. These results show that discrepancies
is an important factor to consider when we use
a pre-trained model, and a reduction in discrep-
ancies can lead to improved performance.

1

Introduction

General purpose (non goal-oriented) dialogue has
been investigated using data-driven sequence-to-
sequence (SEQ2SEQ) recurrent neural networks
(RNN) (Sordoni et al., 2015; Shang et al., 2015;
Wen et al., 2015; Vinyals and Le, 2015). Re-
cently, ﬁne-tuning pre-trained language models has
demonstrated superior performance in the ConvAI2
(Dinan et al., 2019) competition. Several other
studies also successfully exploited pre-trained lan-
guage models for dialogue generation. It is intuitive
that the rich language knowledge encoded in the

model pre-trained on a large amount of raw texts
can help dialogue systems to generate more rea-
sonable responses. While one can easily agree on
this principle, it is much more difﬁcult to deter-
mine how a language model should be adapted
for dialogue generation. Different frameworks
have been proposed in the literature: Transformer-
ED (explicit encoder and decoder architecture)
(Zheng et al., 2019), Transformer-Dec (decoder
only) (Wolf et al., 2019; Lin et al., 2019; Zhang
et al., 2019b), Transformer decoder that uses bi-
directional attention on the source side and left-
to-right attention on the target side with Masked
Language Model (MLM) objective (Transformer-
MLM) (Dong et al., 2019) or with Auto-Regressive
(AR) objective (Transformer-AR) (Bao et al., 2019;
Shuster et al., 2019). The ﬁrst two frameworks
utilize Generative Pre-Training (GPT) (Radford
et al., 2018), a left-to-right architecture pre-trained
with AR. The left-to-right generation fashion cor-
responds well to that of dialogue response gen-
eration. Thus, some researchers believe that this
framework naturally works well for dialogue re-
sponse generation (Lin et al., 2020). The two lat-
ter frameworks use BERT (Devlin et al., 2018), a
bi-directional architecture pre-trained with MLM.
BERT has been used widely as the encoder for
classiﬁcation tasks (Zhang et al., 2019a; Zeng and
Nie, 2020b), while some studies (Dong et al., 2019;
Zeng and Nie, 2020a) show that ﬁne-tuning BERT
can also achieve state-of-the-art performance for
response generation in dialogue. To our knowledge,
no study has investigated into the ﬁne-tuning meth-
ods to understand why and how a pre-trained lan-
guage model can be ﬁne-tuned for dialogue gener-
ation. The choice of a pre-trained language model
and the way to adapt it are still an art.

In this study, we aim at providing an analysis
about the utilization of pre-trained language mod-
els for dialogue generation. To this end, we re-

 
 
 
 
 
 
implement the existing approaches proposed in
the literature and run extensive experiments on
3 datasets to compare them. Our results show
that Transformer-ED that separates encoder and de-
coder does not produce competitive results against
others that combine them, and models that use bi-
directional attention to encode dialogue history out-
performs the one using unidirectional (left-to-right)
attention. However, an advantage of using unidi-
rectional attention is generating diverse responses.
Additionally, this comparison reveals some impor-
tant aspects that were neglected in the utilization
of pre-trained models, namely, the discrepancies
that may occur between pre-training and the ﬁne-
tuning processes and between ﬁne-tuning and the
generation (inference) process.

The concept of model discrepancy has been
brieﬂy mentioned in Yang et al. (2019) to mean
that the model has been trained in a way, but used
in a different way for the task. However, the prob-
lem has not been investigated in depth. We be-
lieve that model discrepancy is a very important
aspect that can bring a signiﬁcant impact on the
ﬁnal result. Going further in this direction, we
deﬁne two discrepancies: pretrain-ﬁnetune dis-
crepancy which means the differences in archi-
tecture and loss function between pre-training and
ﬁne-tuning, and ﬁnetune-generation discrepancy
which means that the way it is used in genera-
tion (inference/test) is different from the way it
has been trained. For the four frameworks we
mentioned for dialogue generation based on pre-
trained models, except Transformer-Dec, they all
have some pretrain-ﬁnetune discrepancies, while
only Transformer-MLM has ﬁnetune-generation
discrepancy because of MLM objective: during
training, the model input has random masks, while
in generation process, the input does not contain
masks (see Figure 4). We summarize the discrep-
ancies of different models in (Table 1).

Discrepancies may affect the model performance
since models with such discrepancies cannot best
exploit the pre-trained language model or the ﬁne-
tuned model. The impact of pretrain-ﬁnetune dis-
crepancy could be reduced by using a large dataset
since a large amount of training data can correct the
discrepancies to some extent. This also explains
why discrepancy has not attracted much attention
– most of the work on dialogue generation tries to
use as much training data as possible. However,
if the amount of training data is limited, the dis-

crepancy problem may surface. Therefore, we will
analyze the performance of the models with both
large and small amount of training data in order to
make it easier to observe the impact of discrepan-
cies. In particular, our experiments will show that
Transformer-ED and Transformer-AR, which have
the largest pretrain-ﬁnetune discrepancy, are more
impacted than Transformer-MLM and Transformer-
Dec by a small amount of data.

To further conﬁrm that discrepancies are truly
the hidden reason of model performance, we
propose correction measures to reduce pretrain-
ﬁnetune discrepancy and ﬁnetune-generation dis-
crepancy of Transformer-MLM, in order to see if
these measures can avoid the negative impact of
discrepancies. These results show that the perfor-
mance of the model is increased when the discrep-
ancy issues are corrected, conﬁrming that discrep-
ancies are indeed an important factor that inﬂuence
the effectiveness of a pre-trained model for dia-
logue generation. This study is the ﬁrst investiga-
tion to show explicitly the phenomenon of model
discrepancy and its impact on performance. It can
be seen as an appeal to more investigations on this
important problem when we adapt a pre-trained
model.

The contributions in this work are summarized

as:

• We re-implement and compare four major
frameworks that utilize pre-trained language
models for dialog generation on three public
dialogue datasets and we consider two data
scales 1. Our experimental results will support
our analysis on model’s architectural appro-
priateness.

• We formally introduce the concept of pretrain-
ﬁnetune discrepancy and ﬁnetune-generation
discrepancy when exploit a pre-trained model.
We examine the discrepancies of each frame-
work and conﬁrm the impact of discrepancies
in experiments.

• We propose two correction methods to
decrease
discrepancy
pretrain-ﬁnetune
and ﬁnetune-generation discrepancy of
Transformer-MLM. Both corrections lead to
increased performance of Transformer-MLM,
conﬁrming that the correction of discrepan-
cies is an important aspect to consider in
utilizing a pre-trained model.

1We will release our codes later.

2 Pre-training Based Transformer

Frameworks

We start with a brief description of the existing
approaches to dialogue generation based on pre-
trained language models. Figure 1 and Table 1
provide an overview of them.

2.1 Transformer-ED

Transformer-ED is an encoder-decoder architecture
as in Vaswani et al. (2017), which utilizes Gener-
ative Pre-Training (GPT) (Radford et al., 2018)
to initialize the encoder and the decoder. GPT is
based on left-to-right attention, where every posi-
tion can only attend to previous positions in the
masked multi-head attention layer of each Trans-
former Block. Transformer-ED ﬁrst encodes dia-
logue history, and then the ﬁnal outputs of encoder
are fed into the decoder to generate a response.
The encoder uses bi-directional attention, while the
decoder uses left-to-right attention.

In this framework, the decoder is stacked upon
the encoder outputs. This makes the ﬁne-tuning
process less effective in updating the encoder’s
parameters. The potential problem with separate
encoder and decoder has been noticed in a previ-
ous work on abstractive summarization (Liu et al.,
2018), which noticed that an explicit encoder in the
Transformer-ED architecture might be redundant,
i.e. the encoding step could be directly incorpo-
rated in the decoder, allowing for a more direct
update of the parameters. Our experiments will
conﬁrm this problem.

2.2 Transformer-Dec

Transformer-Dec is a decoder-only architecture uti-
lizing GPT. It encodes dialog history using only
left-to-right attention, in the same way as in GPT
pre-training. However, the fact that only left-to-
right attention is used may limit the scope of con-
text when generating responses. Many previous
studies have shown that the context after the token
is useful for the encoding of the token. It might
be better to use bi-directional attention whenever
possible.

2.3 Transformer-MLM and AR

Transformer-MLM and Transformer-AR have an
identical decoder-only architecture 2 that employs

2Since the architecture is not explicit encoder-decoder, we
categorize it into decoder-only though BERT is usually viewed
as a pre-trained encoder.

different type embeddings and self-attention masks
for the source/encoder and target/decoder sides:
they use bi-directional attention on the source side
and left-to-right attention on the target side.

The pre-trained BERT (Devlin et al., 2018) is
a bi-directional architecture using MLM as the
pre-training objective. The same attention mech-
anism is used in the encoder part of Transformer-
MLM/AR, but the decoder part uses left-to-right
attention. On the loss function, Transformer-MLM
ﬁne-tunes model parameters using MLM as BERT,
which masks a certain percentage of tokens at
the target side and tries to predict them; while
Transformer-AR uses a different auto-regressive
objective, which tries to predict the next tokens
successively.

2.4 Applications of the frameworks

The frameworks we described have been recently
applied to dialogue generation. For personal-
ized response generation, Wolf et al. (2019) uses
Transformer-Dec and Zheng et al. (2019) uti-
lizes Transformer-ED. Lin et al. (2019) uses
Transformer-Dec for empathetic response gener-
ation. The training data for ﬁne-tuning could
be limited in practice. Several studies propose
to further pre-train models using large-scale di-
alogue datasets before ﬁne-tuning: Zhang et al.
(2019b) trains Transformer-Dec on 147M Reddit
data, Dong et al. (2019) trains Transformer-MLM
on natural language understanding and natural lan-
guage generation datasets, Shuster et al. (2019)
trains Transformer-AR on large-scale Reddit data
and then jointly trains on 12 dialog sub-tasks, and
Bao et al. (2019) trains a variant of Transformer-
AR on large-scale Reddit and Twitter data. Another
trend emerging in the literature (Adiwardana et al.,
2020; Roller et al., 2020; Bao et al., 2020b) is to
train Transformer variants with billions of parame-
ters on extremely large-scale dialogue dataset, e.g.
elaborately pre-processed Reddit.

The purpose of our study is different: we do not
intend to augment training data to push the perfor-
mance. Instead, we want to examine the intrinsic
characteristics of different frameworks for dialogue
generation by comparing them on the same datasets.
In addition, it may often be the case that the train-
ing data for ﬁne-tuning is limited in some domains.
So, we also need to investigate the behaviors of the
frameworks in such situations. Therefore, in the
next section, we will compare the 4 frameworks on

Figure 1: Architectures of four pre-training based Transformers for dialogue generation.

Pre-trained LM
Architecture
Source Side Attn.
Target Side Attn.
Objective

Transformer-ED Transformer-Dec
GPT
encoder-decoder
bi-directional
left-to-right
auto-regressive

GPT
decoder-only
left-to-right
left-to-right
auto-regressive

Transformer-MLM Transformer-AR
BERT
decoder-only
bi-directional
left-to-right
Masked-LM

BERT
decoder-only
bi-directional
left-to-right
auto-regressive

Table 1: Key characteristics of the four pre-training based Transformers. Characteristics in red are inconsistent
between pre-training and ﬁne-tuning.

3 public datasets, and examine them with two data
scales – millions and 100K.

3 Experimental Comparison

3.1 Datasets

We use three public datasets that are widely used
for open-domain dialog generation. Some impor-
tant characteristics of the datasets are summarized
in Table 5. For each dataset, we also evaluate model
performance using only 100K training data with
the same validation set and test set, in order to sim-
ulate the situation of limited data. Many dialogue
datasets that have human annotations such as Per-
sonaChat (Zhang et al., 2018) and EmpatheticDial
(Rashkin et al., 2019) are in similar small scale.

Twitter Dialog Corpus 3 is collected from Twit-
ter consisting of 2.6M (message, response) pairs.
We ﬁltered out a sample if its history length is
longer than 72 words or shorter than 6 words. Sam-
ples whose response is longer than 36 words or
shorter than 6 words are also removed. As results,
we keep 2M samples.

Reddit Conversational Corpus 4(Dziri et al.,
2019) is a high-quality 3-turns conversational
dataset collected from 95 selected subreddits. The
dataset contains 9.2M samples. We applied the
same length constraints and then randomly sam-
pled 3M samples.

Ubuntu Dialogue Corpus V2.0 5 (Lowe et al.,

3https://github.com/Marsan-Ma-zz/chat corpus
4https://github.com/nouhadziri/THRED
5https://github.com/rkadlec/ubuntu-ranking-dataset

2017) is two-person conversations extracted from
the Ubuntu chat logs of technical support for vari-
ous Ubuntu-related problems. We split a conversa-
tion that has more than 3 turns into several 3-turn
dialogues. Besides, we ﬁltered out samples to have
similar dialogue history and response length as the
other two datasets.

3.2

Implementation Details

We equip all transformer variants with an identical
decoding algorithm6 to avoid extra factor affect-
ing the generation quality, which uses beam search
with beam size of 4, prevents duplicated uni-grams,
and sets minimum response length that encourages
diverse generation (Roller et al., 2020). The min-
imum response length is set to make the average
length of generated responses match with the aver-
age target length of the dataset. Generation results
are evaluated after applying an identical word to-
kenization method. With two P100 GPU devices,
we ﬁne-tune all models for 4 epochs on Twitter and
Reddit dataset, and for 5 epochs on Ubuntu dataset.
Maximum input length is set to 128. Our methods
(PF-free and FG-free, which will be described in
Section 4.2) do not add parameters or increase run-
time in comparison to Transformer-MLM. More
implementation details are given in Appendix C.

3.3 Evaluation

Metrics We compare the similarity between gen-
erated responses and ground-truth responses using7:

6From UniLM, https://github.com/microsoft/unilm/
7We

open-source

evaluation

use

an

tool:

-creator

https://github.com/Maluuba/nlg-eval

Model
SEQ2SEQ-MMI
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
PF-free
FG-free
PF&FG-free

BLEU-1
10.872 (**)
15.319 (**)
14.363 (**)
13.749 (**)
15.694
14.813 (**)
13.805 (**)
15.487(/)
15.213 (**)
15.880
16.395
15.714

BLEU-2
4.555 (**)
4.877 (**)
4.861 (**)
4.253 (**)
5.221
4.249 (**)
4.407 (/)
4.766(/)
4.700 (/)
4.970
5.218
4.916

BLEU-3
2.259 (/)
2.037 (**)
2.120 (*)
1.715 (**)
2.272
1.330 (**)
1.787 (/)
1.814(/)
1.767 (/)
1.868
2.043
1.780

CIDEr
0.119 (/)
0.097 (**)
0.101 (**)
0.061 (**)
0.119
0.066(**)
0.092(/)
0.092 (/)
0.090(*)
0.093
0.101
0.093

Dist-1
0.008 (**)
0.014 (**)
0.031 (**)
0.018 (**)
0.029
0.001 (**)
0.033 (**)
0.016(**)
0.019(**)
0.022
0.026
0.020

Dist-2
0.028 (**)
0.063 (**)
0.178 (/)
0.106 (**)
0.164
0.004 (**)
0.195 (**)
0.080(**)
0.091(**)
0.114
0.129
0.111

avgLen
10.6
19.0
19.9
29.3
18.9
18.4
20.2
19.7
18.8
15.7
16.2
18.4

Table 2: Evaluation results on Twitter dataset. The results of the second part is of the 100k data setting. PF-free
denotes the method with reduced pretrain-ﬁnetune discrepancy of Transformer-MLM. FG-free denotes the method
that removes ﬁnetune-generation discrepancy of Transformer-MLM. Two-sided t-test compares each method with
the one without () sign, which is usually the best performer.

Model
SEQ2SEQ-MMI
HRED-MMI
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
PF-free
FG-free
PF&FG-free

BLEU-1
12.056(**)
13.518(**)
19.295(/)
18.974(*)
17.574(**)
20.103
14.195(**)
17.944(**)
18.338(**)
19.005 (/)
19.116
18.884
19.024

BLEU-2
5.512(**)
4.564(**)
6.712(**)
6.911(/)
5.884(**)
7.270
4.533(**)
6.360(/)
6.018(**)
6.431 (/)
6.356
6.530
6.448

BLEU-3
2.841(**)
1.947(**)
2.986(*)
3.022(*)
2.552(**)
3.339
1.756(**)
2.727(/)
2.480(**)
2.733 (/)
2.684
2.869
2.740

CIDEr
0.142(**)
0.060(**)
0.125(**)
0.130(**)
0.096(**)
0.143
0.074(**)
0.121(*)
0.108(**)
0.114(*)
0.118
0.125
0.118

Dist-1
0.005(**)
0.001(**)
0.010(**)
0.018(**)
0.012(**)
0.017
0.003(**)
0.018(**)
0.011(**)
0.012(/)
0.012
0.014
0.012

Dist-2
0.024(**)
0.003(**)
0.069(**)
0.134(**)
0.097(**)
0.127
0.012(**)
0.143(**)
0.066(**)
0.078(**)
0.086
0.095
0.087

avgLen
9.8
13.6
16.8
18.0
25.5
16.8
16.3
18.3
17.0
17.4
16.7
17.3
17.1

Table 3: Evaluation results on Ubuntu dataset. The results of the second part is of the 100K data setting.

BLEU (Papineni et al., 2002) evaluating how many
n-grams (n=1,2,3) overlapped; CIDEr (Vedantam
et al., 2015) utilizing TF-IDF weighting for each
n-gram. Besides, we evaluate response diversity
using Distinct (denoted Dist) (Li et al., 2016) that
indicates the proportion of unique n-grams (n=1,2)
in the entire set of generated responses. Notice
that only when similarity scores are comparable
that higher Distinct scores might represent better
responses. Otherwise, grammatically wrong re-
sponses that are not ﬂuent could also have high
Distinct scores. We also report average length of
the generated responses (denoted avgLen). We
run statistical signiﬁcance tests using two-sided t-
tests. Scores are denoted with * (p < 0.05) or **
(p < 0.01) for statistically signiﬁcant differences.

Human Evaluation Furthermore, we ask human
evaluators to rate a response in {0, 1, 2}. 2 repre-
sents a coherent and informative response. De-
tails are given in Appendix D. We also do a pair-
wise evaluation to compare two models and indi-
cate which one is better. To reduce time cost, we

only evaluate model performances for the Twitter
and Reddit datasets that are closer to daily dia-
logue. However, during evaluation, we observe
that ∼ 65% Reddit data are professional discus-
sions that are difﬁcult to understand given only
2-turn dialog history. The percentage is ∼ 30%
for Twitter data. We skip these test samples, and
at the end the test set for each dataset consists of
200 random samples. The inter-rater annotation
agreement is measured using the Cohen’s kappa
(Cohen, 1960). The κ is 0.44 and 0.42 on average
for Twitter and Reddit, which indicates moderate
agreement.

3.4 Architecture Analysis

Transformer-ED performs the worst. We observe
that it is the only framework that tends to generate
safe responses such as ”i am not sure what you
are talking about” (except on million-scale Reddit
dataset). This phenomenon is also revealed by its
substantially lower Distinct scores. When train-
ing with 100K data, the generated responses are
almost bland, which can be observed in generation

Model
SEQ2SEQ-MMI
HRED-MMI
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
PF-free
FG-free
PF&FG-free

BLEU-1
15.550(**)
13.278(**)
17.946(/)
17.581(**)
18.672(**)
18.849
17.337(**)
17.460(**)
19.193 (/)
18.749(/)
18.466
18.610
19.302

BLEU-2
6.814(**)
3.845(**)
6.626(**)
6.790(*)
7.115(**)
7.245
5.366(**)
6.586(**)
6.877 (/)
6.746(/)
6.688
6.937
6.923

BLEU-3
3.321(**)
1.398(**)
3.213(**)
3.372(*)
3.484(/)
3.662
1.967(**)
3.161(**)
3.175(/)
3.119(*)
3.075
3.302
3.073

CIDEr
0.168(**)
0.047(**)
0.165(**)
0.180(**)
0.177(**)
0.192
0.073(**)
0.172(**)
0.152(**)
0.153(**)
0.169
0.175
0.159

Dist-1
0.011(**)
0.001(**)
0.039(**)
0.043(/)
0.041(**)
0.044
0.001(**)
0.045(**)
0.029(**)
0.031(**)
0.038
0.040
0.034

Dist-2
0.036(**)
0.003(**)
0.203(**)
0.248(**)
0.215(**)
0.235
0.003(**)
0.254(**)
0.128(**)
0.141(**)
0.180
0.191
0.164

avgLen
11.2
13.8
18.8
18.2
16.8
16.8
17.1
17.7
15.0
16.2
14.1
14.1
15.3

Table 4: Evaluation results on Reddit dataset. The results of the second part is of the 100k data setting.

Dialog Turns
Avg. Src Len
Avg. Tgt Len
Train Set
Valid Set
Test Set

Twitter Ubuntu
2
18
16
2M
60K
20K

3
34
16
1.5M
30K
20K

Reddit
3
28
15
3M
80K
20K

Table 5: Key characteristics of the three public datasets.
For each dataset, we also evaluate model performance
using 100K training data and the same test set.

Model
SEQ2SEQ-MMI
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
PF&FG-free

SEQ2SEQ-MMI
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
PF&FG-free

Score (M)
0.39 (**)
0.53 (**)
1.26 (/)
1.28
1.20 (/)
1.22 (/)
Trans-MLM (M)
(7%, 63%)
(10%, 60%)
(26%, 37%)
/
(27%, 40%)
(26%, 34%)

Score (K)
-
0.11 (**)
1.03 (/)
1.03 (/)
0.84 (*)
1.16
PF&FG-free (K)
-
(3%, 65%)
(28%, 41%)
(27%, 37%)
(24%, 43%)
/

Table 6: Human evaluation including pair-wise eval-
uation for generated response quality for million-scale
(M) Twitter dataset and its 100K training subset (K).

samples (Appendix E). As mentioned earlier (sec-
tion 2.1), the Transformer-ED architecture might be
redundant (Liu et al., 2018) – the separate encoder
may not be necessary and be difﬁcult to optimize.
Transformer-AR obtains the highest BLEU and
CIDEr scores on all three million-scale datasets.
Since the only difference from Transformer-Dec
is that Transformer-AR applies bi-directional at-
tention on source side, we can explain the dif-
ference in performance by the attention mecha-
nism. This suggests that bi-directional attention
on the source side helps the model to better en-

Model
SEQ2SEQ-MMI
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
PF&FG-free

SEQ2SEQ-MMI
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
PF&FG-free

Score (M)
0.12 (**)
0.33 (**)
0.72 (/)
0.72 (/)
0.77
0.76 (/)
Trans-AR (M)
(6%, 46%)
(8%, 38%)
(17%, 21%)
(25%, 26%)
/
(26%, 27%)

Score (K)
-
0.03 (**)
0.71 (/)
0.32 (**)
0.28 (**)
0.59
PF&FG-free (K)
-
(0%, 41%)
(32%, 27%)
(13%, 34%)
(12%, 32%)
/

Table 7: Human evaluation including pair-wise eval-
uation for generated response quality for million-scale
(M) Reddit dataset and its 100K training subset (K).

code the dialogue history and thus produce better
responses. Human evaluation results also show that
Transformer-AR performs well when ﬁne-tuning
with million-scale data. However, with small-scale
datasets, Transformer-AR loses its advantages, and
becomes less effective than Transformer-Dec and
Transformer-MLM. We believe that this is due to
the discrepancies in Transformer-AR, which will
be analyzed later.

Transformer-Dec is able to generate the most
diverse responses. We attribute this to the left-
to-right attention on the source side, which can
introduce more ﬂexibility for generation since
it does not have constraints from the right side.
Furthermore, when ﬁne-tuning data are limited,
Transformer-Dec has the best performance on both
Twitter and Reddit datasets according to human
evaluation.

Transformer-MLM outperforms other frame-
works in terms of BLEU metrics when ﬁne-tuning
data are limited, which might have beneﬁted from
the bi-directional attention on the source side. Al-

though with million-scale dataset its automatic
evaluation scores are not the best, human eval-
uation shows that its performance is compara-
ble to Transformer-Dec and Transformer-AR. Fur-
thermore, we observe that Transformer-Dec and
sometimes Transformer-AR will simply completely
copy the input sentence in the output (see examples
in Appendix E, Table 11 and 12), but this seldom
happens with Transformer-MLM. We conjecture
that the MLM objective contributed in alleviating
the issue.

Overall, our experiments suggest model architec-
ture is strongly related to the success of ﬁne-tuning
a pre-trained model for dialogue generation. The
best conﬁguration uses bi-directional attention on
the source side and combines encoder and decoder
in the same multi-layer transformer blocks.

3.5 Discrepancy Impact

In Table 1, we emphasized the pretrain-ﬁnetune
discrepancy in red. All the frameworks, except
Transformer-Dec, have some discrepancies, while
only Transformer-MLM has ﬁnetune-generation
discrepancy: during training, the model input has
random masks as shown in Figure 1, while in gener-
ation process, the input consisting does not contain
masks. Discrepancies may affect the model per-
formance since models with such discrepancies
cannot best exploit the pre-trained language model
or best employ the ﬁne-tuned model. In this subsec-
tion, we try to understand how model discrepancies
affect the model.

When a large training dataset is available for ﬁne-
tuning, we observe the impact of pretrain-ﬁnetune
discrepancy is less severe – the model can gradu-
ally be adapted to the given task. However, if the
training data are limited, the discrepancy problems
may surface.

Speciﬁcally, Transformer-ED is the framework
that has the largest pretrain-ﬁnetune discrepancy.
We observe that its performance decreases a lot
from large training data to small training data.
On the other hand, Transformer-Dec has the least
pretrain-ﬁnetune discrepancy, and we can observe
much smaller decrease in performance (especially
in human evaluation). We can also compare
Transformer-MLM and Transforme-AR, which use
an identical architecture. Transformer-AR has ad-
ditional pretrain-ﬁnetune discrepancy due to its
auto-regressive objective in ﬁne-tuning. We can
see that the performance of Transformer-AR is

more reduced when ﬁne-tuned on a small dataset:
Transformer-AR performs better than Transformer-
MLM with large training datasets, but loses its
advantages with small training datasets according
to automatic evaluation. The human evaluation can
show this larger decrease more clearly.

The above analysis suggests that with a small
dataset one should make efforts to reduce pretrain-
ﬁnetune discrepancy to best exploit pre-trained
models. In terms of ﬁnetune-generation discrep-
ancy, since this is a problem for Transformer-MLM
only, it is difﬁcult to show its impact because no
comparison with other frameworks is possible.

As we have revealed the two discrepancies as
potential factors that negatively affect the model
performance, in the next section, we propose ap-
proaches to reduce pretrain-ﬁnetune discrepancy
and ﬁnetune-generation discrepancy, and we will
test the performance of the modiﬁed models.

4 Discrepancy-Free Transformer-MLM

As we have shown that
the architecture of
Transformer-MLM and Transformed-AR is the best
in Section 3.4, our question now is how the model
performance would be if we reduce its discrepan-
cies.

4.1 Multi-Layer Transformer

Let us ﬁrst brieﬂy introduce the general transformer
mechanism, and then describe our methods to re-
duce discrepancies.

A dialog history is denoted by x, and a corre-
sponding response is denoted by y. The input to
the multi-layer transformer is the concatenation
of dialog history and the response. When using
MLM objective, the response is randomly masked.
The input representation H0 ∈ Rn×dh, where n
is the input length and dh = 768 is the hidden
dimension, is the sum of token embedding, posi-
tion embedding, and type embedding at each posi-
tion. The type embeddings introduce a separation
between encoder/source side and decoder/target
side in order to warrant different treatments in
the model. Then, H0 is encoded into hidden rep-
resentations of i-th layer Hi = [hi
n] by:
Hi = Transi(Hi−1),
i ∈ [1, L], where Transi
denotes the i-th Transformer Block as shown in Fig-
ure 2(Left). The core component of a transformer
block is the masked multi-head attention, whose
outputs are Ci = [ci
n] that are computed via

1, ..., hi

1, ..., ci

generation. Our experiments show that with such
high updating frequency the generated responses
usually lack ﬂuency (e.g. BLEU-2 is 1.519 on Twit-
ter). Furthermore, to keep training consistent with
inference, only one token can be masked in each
training sample; otherwise, there will be conﬂict
for the self-attention mask (Appendix A, Figure
5), i.e. different self-attention mask matrices are
required for different masked tokens, while only
one mask matrix can be provided per training sam-
ple. This would lead to much lower training efﬁ-
ciency: the loss on validation set only decreases
slightly to 5.39 from 6.27 after four epochs, while
Transformer-MLM masking 40% of the target to-
kens can reduce it to 4.35.

To summarize, in generation, we cannot always
update previous hidden states using bi-directional
attention, and the training process should be con-
sistent to the generation in order not to add new
ﬁnetune-generation discrepancy. Therefore, we
propose a simple method by setting a time-step in-
terval for bi-directional attention on the target side
to decrease updating frequency – within the inter-
val we apply left-to-right attention and at the end of
an interval we apply bi-directional attention. The
corresponding training method allows us to mask
multiple target tokens at the same time to guarantee
training efﬁciency.

Figure 3 illustrates the generation process of our
method with interval of 3. Before time step 3,
left-to-right attention is used (e.g. t=2). At time
step 3, bidirectional attention is allowed. Then
left-to-right attention is used (e.g. t=5) before the
end of next interval cycle (t=6). Accordingly, the
training process is: given a target response, we ﬁrst
randomly select among all (3 in the ﬁgure) possible
attention patterns (e.g. the case of t=3 or t=5 in
Figure 3, where we apply bi-directional attention
on y0,1,2); then in the part of left-to-right attention,
we randomly mask several tokens. We can mask
multiple tokens because this part applies left-to-
right attention and the masks at other positions will
not inﬂuence the prediction on a given mask. We
call our modiﬁed approach PF-free, which means
that the pretrain-ﬁnetune discrepancy is reduced.

4.3 Finetune-Generation Discrepancy

A model having ﬁnetune-generation discrepancy
means the way that it is used in generation (infer-
ence/test) is different from the way it has been
trained. Only Transformer-MLM has ﬁnetune-

Figure 2: i-th Transformer Block and two M settings
represented in two ways. Shaded areas are blocked.

Ci = Concat(head1, ..., headh), with

headj = softmax(

QjKT
j√
dk

+ M)Vj

(1)

j , WV

j , WK

where Qj, Kj, Vj ∈ Rn×dk are obtained by trans-
forming Hi−1 ∈ Rn×dh using WQ
j ∈
Rdh×dk respectively. M ∈ Rn×n is the self-
attention mask matrix that determines whether a
position can attend to other positions. Mij ∈
{0, −∞}. In particular, Mij = 0 allows the i-th
position to attend to j-th position and Mij = −∞
prevents from it. Figure 2 (Right) shows two M
settings that are applied by Transformer-MLM/AR
and Transformer-Dec respectively.

4.2 Pretrain-Finetune Discrepancy

If we reduce the pretrain-ﬁnetune discrepancies of
Transformer-AR, we get Transformer-MLM by re-
placing the AR objective to MLM objective. The
discrepancy of Transformer-MLM comes from the
left-to-right attention on the target side that has not
been pre-trained in BERT. Therefore, this discrep-
ancy cannot be eliminated during ﬁne-tuning for a
generation task. However, we might alleviate the
discrepancy by using bi-directional attention also
on the target side. Speciﬁcally, at inference time, to
generate a new token denoted as gt, [MASK] is fed
into t-th position, denoted as gt-M. Previously gen-
erated tokens g<t could be viewed as a special type
of dialogue history, and we can apply bi-directional
attention on it.

However, previously left-to-right attention is
used on g≤t, and thus only hidden states at (t − 1)-
th and t-th positions need to be updated. If we
directly apply bi-directional attention on g≤t, all
hidden states at ≤ t positions need to be updated.
In other words, this generation strategy requires
to update all hidden states at each time step of

ods to reduce discrepancies improve Transformer-
MLM performance. Speciﬁcally, PF-free improves
BLEU on Twitter and Ubuntu datasets, and it al-
ways outperforms Transformer-MLM in terms of
CIDEr and Distinct scores on all three datasets. Be-
sides, as have been discussed, Transformer-MLM
that has smaller pretrain-ﬁnetune discrepancies out-
performs Transformer-AR when training data are
small. These results together suggest that the ef-
fort to reduce pretrain-ﬁnetune discrepancy when
training data are small is beneﬁcial.

In comparison, PG-free results in even better
performances, which always brings statistically sig-
niﬁcant improvement over Transformer-MLM in
all automatic metrics on all three datasets. There-
fore, we can see that it is necessary to apply
our method on Transformer-MLM to eliminate
ﬁnetune-generation discrepancy. However, when
we combine the two correction methods to reduce
both discrepancies, we do not observe an cumula-
tive effect. The relation between the two discrepan-
cies remains unclear. This question is worth being
investigated further in the future.

Conclusion

In this paper, we compared experimentally 4 main
frameworks using pre-trained language models for
dialogue generation. The comparison revealed the
best strategy to use pre-trained model: using bi-
directional attention, and integrate encoder and de-
coder in the same transformer blocks.

In addition to the architectural appropriateness,
we also examined the discrepancies of each frame-
work. We identiﬁed two discrepancies: pretrain-
ﬁnetune discrepancy between the pre-training and
ﬁne-tuning processes, and ﬁnetune-generation dis-
crepancy that exists between the ﬁne-tuning and
generation processes. Our experiments showed
that discrepancies affect the model performance
since models with such discrepancies cannot best
exploit the pre-trained language model or best em-
ploy the ﬁne-tuned model. To reduce the negative
impact of discrepancies, we proposed two meth-
ods to reduce the pretrain-ﬁnetune discrepancy and
ﬁnetune-generation discrepancy respectively, and
both improved the model performance. This con-
ﬁrms the necessity to design approaches to dialogue
generation based on pre-trained models, containing
as little discrepancy as possible.

The appropriate utilization of pre-trained models
is important, and is worth being paid more attention

Figure 3: Alleviate pretrain-ﬁnetune discrepancy of
Transformer-MLM by using bi-directional attention on
the target side. We show the generation process at 4
different steps and annotate the range of positions to
update. Bi-attention interval is 3 in the graph.

Figure 4: Eliminating ﬁnetune-generation discrepancy
of Transformer-MLM. We only plot the attention con-
nection at y2 − M .

generation discrepancy because of its MLM ob-
jective as shown in Figure 4: during training, there
is a masked token, y1-M, before y2-M, while in
generation process, there is not a masked token
before when generating the token g2-M.

Therefore, we propose that at training time,
rather than replacing the tokens with [MASK] as in
vanilla MLM, we keep all original input tokens un-
changed and prepend [MASK] tokens in the input
sequence as illustrated. The prepended [MASK]
token uses the same position embedding of the cor-
responding token. Then, every position after y1-M
attends to y1 instead of the [MASK] token, and
thus the ﬁnetune-generation discrepancy of MLM
is eliminated. A similar method has also been ex-
plored in Bao et al. (2020a), where they introduced
an extra pseudo mask in additional to [MASK]
and prepend it before the original token in order
to handle factorization steps of their partially auto-
regressive language model.

We call the modiﬁed model FG-free, in which

the ﬁnetune-generation discrepancy is reduced.

4.4 Experimental Results

The results with PF-free, FG-free and PF&FG-free
models are reported in previous tables together
with other models. We can see that both meth-

to. This study is a ﬁrst investigation on model
discrepancy and more studies are needed.

Jiwei Li and Dan Jurafsky. 2016. Mutual information
and diverse decoding improve neural machine trans-
lation. arXiv preprint arXiv:1601.00372.

References

Daniel Adiwardana, Minh-Thang Luong, David R So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al. 2020. Towards a human-like open-domain
chatbot. arXiv preprint arXiv:2001.09977.

Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang,
Nan Yang, Xiaodong Liu, Yu Wang, Songhao
Piao, Jianfeng Gao, Ming Zhou, et al. 2020a.
Unilmv2: Pseudo-masked language models for uni-
ﬁed language model pre-training. arXiv preprint
arXiv:2002.12804.

Siqi Bao, Huang He, Fan Wang, and Hua Wu.
Pre-trained dialogue generation
2019.
model with discrete latent variable. arXiv preprint
arXiv:1910.07931.

Plato:

Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng
Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and
Xinchao Xu. 2020b. Plato-2: Towards building an
open-domain chatbot via curriculum learning. arXiv
preprint arXiv:2006.16779.

Jacob Cohen. 1960. A coefﬁcient of agreement for
nominal scales. Educational and psychological mea-
surement, 20(1):37–46.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Emily Dinan, Varvara Logacheva, Valentin Malykh,
Alexander Miller, Kurt Shuster,
Jack Urbanek,
Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
The second conversational
Lowe, et al. 2019.
arXiv preprint
intelligence challenge (convai2).
arXiv:1902.00098.

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. 2019. Uniﬁed language
model pre-training for natural language understand-
ing and generation. In Advances in Neural Informa-
tion Processing Systems, pages 13042–13054.

Zhaojiang Lin, Andrea Madotto, and Pascale Fung.
Exploring versatile generative language
learning.

2020.
model via parameter-efﬁcient
arXiv preprint arXiv:2004.03829.

transfer

Zhaojiang Lin, Peng Xu, Genta Indra Winata, Zi-
han Liu, and Pascale Fung. 2019. Caire: An
arXiv preprint
end-to-end empathetic chatbot.
arXiv:1907.12108.

Peter J Liu, Mohammad Saleh, Etienne Pot, Ben
and
Goodrich, Ryan Sepassi, Lukasz Kaiser,
Noam Shazeer. 2018. Generating wikipedia by
arXiv preprint
summarizing long sequences.
arXiv:1801.10198.

Ryan Thomas Lowe, Nissan Pow, Iulian Vlad Serban,
Laurent Charlin, Chia-Wei Liu, and Joelle Pineau.
2017. Training end-to-end dialogue systems with
the ubuntu dialogue corpus. Dialogue & Discourse,
8(1):31–65.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Alec Radford, Karthik Narasimhan, Tim Salimans,
Improving language
and Ilya Sutskever. 2018.
understanding by generative pre-training. URL
https://s3-us-west-2.
com/openai-
assets/researchcovers/languageunsupervised/language
understanding paper. pdf.

amazonaws.

Hannah Rashkin, Eric Michael Smith, Margaret Li, and
Y-Lan Boureau. 2019. Towards empathetic open-
domain conversation models: A new benchmark and
In Proceedings of the 57th Annual Meet-
dataset.
ing of the Association for Computational Linguistics,
pages 5370–5381.

Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Kurt Shuster, Eric M Smith, et al. 2020. Recipes
for building an open-domain chatbot. arXiv preprint
arXiv:2004.13637.

Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and
Osmar R Zaiane. 2019. Augmenting neural re-
sponse generation with context-aware topical atten-
tion. In Proceedings of the First Workshop on NLP
for Conversational AI, pages 18–31.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Thirtieth AAAI
Conference on Artiﬁcial Intelligence.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting objec-
tive function for neural conversation models. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
110–119.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-
ral responding machine for short-text conversation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1577–1586.

Kurt Shuster, Da Ju, Stephen Roller, Emily Dinan,
The
Y-Lan Boureau, and Jason Weston. 2019.
dialogue dodecathlon: Open-domain knowledge
and image grounded conversational agents. arXiv
preprint arXiv:1911.03768.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceedings
of the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 196–205.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
In Advances in neural information pro-
you need.
cessing systems, pages 5998–6008.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
In Proceedings of the IEEE
scription evaluation.
conference on computer vision and pattern recogni-
tion, pages 4566–4575.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkˇsi´c, Pei-
Hao Su, David Vandyke, and Steve Young. 2015. Se-
mantically conditioned lstm-based natural language
generation for spoken dialogue systems. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 1711–1721.

Thomas Wolf, Victor Sanh, Julien Chaumond, and
Transfertransfo: A
learning approach for neural network
arXiv preprint

Clement Delangue. 2019.
transfer
based conversational agents.
arXiv:1901.08149.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for
language understanding. In Advances in neural in-
formation processing systems, pages 5753–5763.

Yan Zeng and Jian-Yun Nie. 2020a. Generalized condi-
tioned dialogue generation based on pre-trained lan-
guage model.

Yan Zeng and Jian-Yun Nie. 2020b. Multi-domain dia-

logue state tracking based on state graph.

Jian-Guo Zhang, Kazuma Hashimoto, Chien-Sheng
Wu, Yao Wan, Philip S Yu, Richard Socher, and
Caiming Xiong. 2019a. Find or classify? dual strat-
egy for slot-value predictions on multi-domain dia-
log state tracking. arXiv preprint arXiv:1910.03544.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you

In Proceedings of the 56th An-
have pets too?
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 2204–
2213.

Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,
Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and Bill Dolan. 2019b. Dialogpt: Large-scale
generative pre-training for conversational response
generation. arXiv preprint arXiv:1911.00536.

Yinhe Zheng, Rongsheng Zhang, Xiaoxi Mao, and
Minlie Huang. 2019. A pre-training based personal-
ized dialogue generation model with persona-sparse
data. arXiv preprint arXiv:1911.04700.

A Attention Conﬂict Illustration

In generation, it seems appropriate if applying bi-
directional attention at each generation step (how-
ever, we experimentally show the efﬁciency and
effectiveness issues.). The corresponding training
method is extremely inefﬁcient – only one token at
the target side could be masked for each training
sample; otherwise there will be attention conﬂicts.
In Figure 5, we assume y1 and y3 are masked and
need to be predicted at the same time.

Figure 5: Self-attention mask, M, conﬂicts – if predict-
ing y1, M is as the left ﬁgure, where y2 and y3-M are
”future” and forbidden to access by y1-M; if predict-
ing y3, M is as the right ﬁgure, in which case y1-M
accesses to y2 and y3-M. Masking two positions thus
causes conﬂicts.

B Baselines

We include two general RNN-based frameworks in
this comparison to show how pre-trained models
perform against them – SEQ2SEQ-MMI (Li et al.,
2016), a seq2seq model using bi-directional GRU
encoder and applying Maximum Mutual Informa-
tion (MMI) as the objective function to generate
more diverse responses, and HRED-MMI 8, a hier-
archical recurrent encoder-decoder neural network
(Serban et al., 2016) applying diverse decoding
strategy based on MMI (Li and Jurafsky, 2016).

8https://github.com/hsgodhia/hred

C Implementation Details

response.

E Generation Samples

We implemented SEQ2SEQ-MMI based on Open-
NMT 9, HRED-MMI based on an open-source
implementation10, Transformer-ED based on the
codes of Zheng et al. (2019), Transformer-Dec
upon Wolf et al. (2019)11, Transformer-MLM and
Transformer-AR upon Dong et al. (2019) 12. Hyper-
parameters are set following the original papers.
We set the bi-directional attention interval of PF-
free to 5. Since the average length of ground-truth
responses in the datasets is ∼ 15, This setting is
generally appropriate.

In Table 8, the average runtime is tested using
a 1080Ti GPU device, and the batch size is set
to take all of the GPU memories. Notice that the
runtime will be inﬂuenced by code implementation
in addition to model structure.

50
25

Params Runtime(min/M)
Model
66M
SEQ2SEQ-MMI
58M
HRED-MMI
117M 180
Transformer-ED
124M 290
Transformer-Dec
Transformer-MLM 110M 140
110M 140
Transformer-AR
110M 140
PF&FG-free

Table 8: The number of parameters of each tested ap-
proach and the average runtime (minutes) for every mil-
lion training samples.

D Human Evaluation Details

Some existing studies considered response ﬂuency,
coherence, and informativeness. We make the man-
ual evaluation simpler and ask the human evalua-
tors to rate a response in {0, 1, 2}. A score of 0
represents an unacceptable response, which might
have ﬂaw in ﬂuency and logic or be incoherent.
A special case is a bland response such as “i do
not know what you are talking about , but it ’s a
good point .”. A score of 1 represents an acceptable
response that might be simple or generic. 2 repre-
sents a coherent and informative response. Each
generated response is rated by three annotators. An-
notators are unaware of which model generates a

9http://opennmt.net/
10https://github.com/hsgodhia/hred
11https://github.com/huggingface/pytorch-openai-

transformer-lm

12https://github.com/microsoft/unilm/tree/master/unilm-

v1

Dialog History
Gold Response
SEQ2SEQ-MMI
Trans-ED
Trans-Dec
Trans-MLM

Trans-AR
PF&FG-free
Trans-ED (K)
Trans-Dec (K)
Trans-MLM (K)
Trans-AR (K)
PF&FG-free (K)

one week before election day , early voting is nearly twice as high as <num>
i hope it ’s <person> out in full force .
i am not voting for a third party candidate . (2)
i am not sure what you are talking about , but it ’s a good thing that <person> is going to win . (2)
that ’s not true . early voting in nyc is times higher than the national average (2)
it ’s not even close to <num> % of the vote . i am sure you are right , but there is a huge difference
between early voting and <person> voter suppression in ca (2)
it ’s not that high , but i am sure there will be a lot of voter fraud . (2)
it ’s not even sure how many people voted for <person> . <num> % of the vote went to <person> (2)
i am not sure what you are talking about . it ’s the only one who is going to do with this (0)
it ’s not that early voting is a bad thing , but it shouldnot be the case . (0)
it ’s a little too late . i am not sure if that is the case , but we will see what happens next (2)
it ’s not even close to <num> % of voter turnout . i am sure you are right about that (0)
it ’s not even <num> % of the vote . i am going to be a voter (2)

Table 9: Responses generated by baselines and our methods trained on the Twitter dataset(million-scale and
100K). Human evaluation scores are given at the end of each generated reply.

Dialog History
Gold Response
SEQ2SEQ-MMI
Trans-ED
Trans-Dec
Trans-MLM

Trans-AR
PF&FG-free
Trans-ED (K)
Trans-Dec (K)
Trans-MLM (K)
Trans-AR (K)
PF&FG-free (K)

i think about this man every day
it is so hypnotic . this man was found dead post-election
i do not know what you are talking about . (0)
i think he ’s the only one who has a lot of work to do with it . (1)
i think about this man every day too . he ’s my favorite person in the world (2)
he ’s such a good guy . i love him so much, but it doesnot make sense to me that this is the only way
we can be together in our lives (2)
i think about this man every day too . he ’s so good at it ! (2)
he ’s the best friend of mine and i am so happy for him . it makes me want to cry (2)
i am not sure what you are talking about , but it ’s a good idea . (0)
i think about this man every day . he ’s my hero and i love him (2)
i think he ’s a great actor , but no one cares about him . lol (2)
i think he ’s a good guy , but not sure what you are talking about . (0)
i think he is little silly , but you are a fool . (2)

Table 10: Responses generated by baselines and our methods trained on the Twitter dataset(million-scale and
100K). Human evaluation scores are given at the end of each generated reply.

Dialog History

Gold Response

SEQ2SEQ-MMI
Trans-ED
Trans-Dec

Trans-MLM

Trans-AR
PF&FG-free

Trans-ED (K)
Trans-Dec (K)
Trans-MLM (K)
Trans-AR (K)
PF&FG-free (K)

so i asked the person who rang me up if y’all had a comment box . she pointed out the
number on the receipt & nicely asked me what was up .
i asked her why you folks sell those costumes since they are seen as really offensive .
i thought for sure she ’d say , ” sorry , i know ”
i do not know what you are talking about . (0)
i am so sorry for your loss . you are the one who did not have a problem with me (0)
so i asked the person who rang me up if y’all had a comment box . she pointed out that
it was not on my receipt . (0)
and i asked her if y ’ all had a comment box . she told me that
it was not my phone number , but the receipt said they were going to call you at <num> (0)
so i asked the person who rang me up if y’all had a comment box . (0)
i was like , ” what ’s wrong with a comment box . and she did not answer the phone
until <num> minutes after we got home from work (2)
i am not sure what you are talking about , but it ’s a good idea . (0)
i did not have a comment box , so i asked the person who rang me up if y’all had . (0)
that ’s why i do not even know what you are talking about . lol ! (0)
i think it ’s a good idea to call you up , but that would be great . (0)
i am not sure if she had a comment box , but it ’s probably the same . (2)

Table 11: Responses generated by baselines and our methods trained on the Twitter dataset(million-scale and
100K). This example shows that Transformer-Dec and Transformer-AR sometimes completely copy the input as
the output.

Dialog History

Gold Response
SEQ2SEQ-MMI
Trans-ED
Trans-Dec
Trans-MLM

Trans-AR
PF&FG-free
Trans-ED (K)
Trans-Dec (K)
Trans-MLM (K)
Trans-AR (K)
PF&FG-free (K)

how old are you guys ? you did not have mechanical pencils ? [SEP] ¡url¿ young are you ?
i was in school all the way up to a few years ago and most of us still used non - mechanical ones .
i started using mechanical pencil in elementary school about 15 years ago .
i do not know what you are talking about . (0)
how old are you ? i was in school all the way up to a few years ago . (1)
i was in school all the way up to a few years ago and most of us still used non - mechanical ones . (0)
i was in school for a few years , and most of us still use mechanical pencils . it ’s not the same age
as you (2)
i was in high school when we used to use mechanical pencils . (2)
i was in high school and my teacher told me how to use mechanical pencils . (2)
i do not know what you are talking about , but it ’s just a joke . (0)
i was in school all the way up to a few years ago and most of us still used non - mechanical ones . (0)
i have been in school for a few years , but not really . (0)
how old are you ? i have never been in school for a few years . (0)
how old are you ? i have never had a lot of time in school (2)

Table 12: Responses generated by baselines and our methods trained on the Reddit dataset(million-scale and
100K). Human evaluation scores are given at the end of each generated reply.

Dialog History

Gold Response
SEQ2SEQ-MMI
Trans-ED
Trans-Dec
Trans-MLM
Trans-AR
PF&FG-free
Trans-ED (K)
Trans-Dec (K)
Trans-MLM (K)
Trans-AR (K)
PF&FG-free (K)

i have read all of the books , which character do you not like ? [SEP] it ’s gon na be the dornish
and greyjoys . and probably brienne . you will see .
nah , they have only just started a game of thrones so it must be one of the starks , dan , or tyrion .
i do not know why you are being downvoted . (0)
i have read all of the books , and it ’s gon na be dornish . (0)
i have not read the books yet , but i will deﬁnitely check them out . (2)
i have not read the dornish and greyjoys , but you will see them all as well . (0)
brienne is not a ﬁctional character , she ’s an actual person . (2)
i have read all of the dornish , but that ’s just me . (1)
i am not sure what you are talking about , but it ’s just a joke . (0)
i do n’t know , i have read all of the dornish books . (0)
i do not think it ’s a good idea to say that . (0)
i do not think it ’s gon na be the best . (0)
i do not know , but it ’s a lot of people ? (1)

Table 13: Responses generated by baselines and our methods trained on the Reddit dataset(million-scale and
100K). Human evaluation scores are given at the end of each generated reply.

Dialog History

Gold Response
SEQ2SEQ-MMI
Trans-ED
Trans-Dec

Trans-MLM

Trans-AR
PF&FG-free
Trans-ED (K)
Trans-Dec (K)
Trans-MLM (K)
Trans-AR (K)
PF&FG-free (K)

religion , unfortunately . most of the world is still religious . science is opening people ’s minds
but there is still a long way to go . [SEP] religious people can be scientists as well , i do not know
why reddit thinks they are mutually exclusive .
nah , they have only just started a game of thrones so it must be one of the starks , dan , or tyrion .
i do not know what you are talking about , but it ’s a religion . (0)
i do not know why you are being downvoted , but there is a long way to go . (1)
i do n’t know why you think reddit thinks they are mutually exclusive . science is opening people ’s
minds , but there still a long way to go (0)
because they are not mutually exclusive . it ’s a lot of people , but reddit is the only place where science
and religion exist ? (0)
religious people can be scientists as well , i do not know why reddit thinks they are mutually exclusive . (0)
they are not mutually exclusive . i am just saying that most of the world is still open to science (2)
i am not sure what you are talking about , but it ’s just a joke . (0)
i do n’t know why reddit thinks they are mutually exclusive , but i am sure there is a way . (0)
i do not know why reddit thinks it ’s a science . (0)
they can be scientists , but not all of the world is religious . (1)
i do not know why reddit thinks they are mutually exclusive . science is the only thing that matters to
you , right ? (2)

Table 14: Responses generated by baselines and our methods trained on the Reddit dataset(million-scale and
100K). Human evaluation scores are given at the end of each generated reply.

Dialog History
Gold Response
SEQ2SEQ-MMI
HRED-MMI
Trans-ED
Trans-Dec
Trans-MLM

Trans-AR
PF&FG-free
Trans-ED (K)
Trans-Dec (K)
Trans-MLM (K)
Trans-AR (K)
PF&FG-free (K)

is that easy to do ? [SEP] usually the ubuntu installer will set it up perfectly for you
and then i would select at start-up ?
how do i install it ?
i am not sure what you mean by that
is there a way to do it without installing the ubuntu installer ? i am not sure .
ok , i will try that . thanks for the help ! :-) it ’s working now
is there a way to do it with the live cd ? i am not sure if that ’s what you are looking for , but how
does one install ubuntu on
ok , i will give it a try . thank you very much for your help !
how do i set it up for me ? :) . is there a way to install ubuntu on my laptop with the live cd
i am not sure what you mean by that , but it ’s just a little bit more speciﬁc
ok , i will try that . thanks for your help ! :-) <url>
i am not sure what you mean by that , but it ’s a good idea .
i am not sure what you are talking about , but it ’s possible that the installer will be able to boot from cd
i am not sure what you mean by setting it up perfectly . is there a way to do that ?

Table 15: Responses generated by baselines and our methods trained on the Ubuntu dataset(million-scale and
100K).

usually if on the same machine , <url> [SEP] : i am getting error . phpmyadmin not in this server
you installed it from the software center , or with aptitude or apt-get , or some other way ?

Dialog History
Gold Response
SEQ2SEQ-MMI what is the error you are getting ?
HRED-MMI
Trans-ED
Trans-Dec
Trans-MLM

i have no idea what you are talking about
i am not sure what you are trying to do , but it ’s a good idea .
did you install phpmyadmin from the repos ? if so , it should be in <path>
i am not sure what you are trying to do , but it should be in <path> . try sudo dpkg -
reconﬁgure phpmyadmin
i am not sure what you are trying to do , but it ’s possible .
what is the exact error messages , you need to be honest i havenot used it .
i am not sure what you mean by ” phpmyadmin <path>
i am not sure what phpmyadmin is , but it ’s probably in <path>
i am not sure what you mean by ” phpmyadmin - > <path> . <url>
i do not know what phpmyadmin is , but it ’s in the repos
i am not sure if it ’s on the same machine , you can use phpmyadmin

Trans-AR
PF&FG-free
Trans-ED (K)
Trans-Dec (K)
Trans-MLM (K)
Trans-AR (K)
PF&FG-free (K)

Table 16: Responses generated by baselines and our methods trained on the Ubuntu dataset(million-scale and
100K).

