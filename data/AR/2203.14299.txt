Adversarial Representation Sharing: A Quantitative and Secure
Collaborative Learning Framework

Jikun Chen
cjk7989@sjtu.edu.cn
Shanghai Jiao Tong University

Feng Qiang
f.qiang@gmail.com

Na Ruan∗
naruan@cs.sjtu.edu.cn
Shanghai Jiao Tong University

2
2
0
2

r
a

M
7
2

]

R
C
.
s
c
[

1
v
9
9
2
4
1
.
3
0
2
2
:
v
i
X
r
a

ABSTRACT
The performance of deep learning models highly depends on the
amount of training data. It is common practice for today’s data hold-
ers to merge their datasets and train models collaboratively, which
yet poses a threat to data privacy. Different from existing methods
such as secure multi-party computation (MPC) and federated learn-
ing (FL), we find representation learning has unique advantages in
collaborative learning due to the lower communication overhead
and task-independency. However, data representations face the
threat of model inversion attacks. In this article, we formally define
the collaborative learning scenario, and quantify data utility and
privacy. Then we present ARS, a collaborative learning framework
wherein users share representations of data to train models, and
add imperceptible adversarial noise to data representations against
reconstruction or attribute extraction attacks. By evaluating ARS in
different contexts, we demonstrate that our mechanism is effective
against model inversion attacks, and achieves a balance between
privacy and utility. The ARS framework has wide applicability. First,
ARS is valid for various data types, not limited to images. Second,
data representations shared by users can be utilized in different
tasks. Third, the framework can be easily extended to the vertical
data partitioning scenario.

KEYWORDS
privacy, collaborative learning, adversarial examples, quantification,
trade-off

1 INTRODUCTION
Deep learning has made great progress in a variety of fields, such as
computer vision, natural language processing and recommendation
systems. This impressive success largely attributes to the increas-
ing amount of available computation and datasets [9]. Companies
and institutions require large-scale data to build stronger machine
learning systems, whereas data is generally held by distributed
parties. Therefore, it is a common practice for multiple parties to
share data and train deep learning models collaboratively [23]. In
most of collaborative training scenarios, users provide their local
data to cloud computing services or share data with others, which
brings privacy concerns.

Due to security considerations and privacy protection regula-
tions, it is inappropriate to exchange data among different organi-
zations. Obviously, sharing raw data directly may cause a leakage
of private and sensitive information contained in datasets. For in-
stance, if some hospitals hope to integrate their patients’ informa-
tion to establish models for disease detection, they must carefully
protect the identity of patients from being obtained and abused by

∗Corresponding author.

any partner or possible eavesdropper. The problem of "data islands"
requires a privacy-preserving collaborative framework.

Generally speaking, privacy-preserving collaborative learning
can be grouped under two approaches. Earlier works focus on
MPC, which ensures that multiple users can jointly calculate a cer-
tain function while keeping their inputs secret without the trusted
third party. In a joint learning framework, data in local devices
are encrypted by cryptographic tools before being shared [1, 21].
Cryptographic methods contain garbled circuits (GC) [32], secret
sharing (SS) [24], homomorphic encryption (HE) [7], etc. However,
current cryptographic approaches can just perform several types
of operations, and only propose friendly alternatives to some of
non-linear functions [21]. Moreover, encryption often causes high
computation and communication cost.

Shokri et al. [29] proposed a distributed stochastic gradient de-
scent algorithm to replace the sharing training data framework. The
method is now well known as federated learning (FL) [11] and has
a wide application in practice. In FL, a cloud server builds a global
deep learning model. For each training iteration, the server ran-
domly sends the model to a part of client devices. The clients then
optimize the model locally, and send the updates back to the server
to aggregate them. Only parameters of the model are communi-
cated, while the training data is retained by the local device, which
ensures the privacy. Some recent works combine federated learning
with other information security mechanisms (e.g., differential pri-
vacy) to further improve privacy [8]. However, the communication
cost between each local device and the central server is high. After
each iteration of training process, each user needs to keep their
local deep learning model synchronized.

Different from the above approaches, we consider representa-
tion learning [2] to solve this problem. The idea is inspired by deep
neural networks, which embed inputs into real feature vectors (rep-
resentations). Containing high-level features of the original data,
latent representations is efficient to various downstream machine
learning tasks [9]. The motivation is that both MPC and FL conduct
collaborative learning with limited task-applicability. Once the ma-
chine learning task changes, the entire training process needs to
be executed again, which incurs high communication cost. Com-
paratively speaking, data representations are task-independent and
thus has unique potential in joint learning. Some recent works have
studied privacy-preserving data representations [13, 30], but few of
them gave further discussion on the collaborative learning scenario.
The primary problem in privacy representations learning field is to
defend against model inversion attacks[12, 20], which aims to train
inverse models to reconstruct original inputs or extract private
attributes from shared data representations.

In this article, we propose ARS (for Adversarial Representa-
tion Sharing), a decentralized collaborative learning framework

 
 
 
 
 
 
based on data representations. Our work contains two levels: (i)
a collaborative learning framework wherein users share data rep-
resentations instead of raw data for further training; and (ii) an
imperceptible adversarial noise added to shared data representa-
tions to defend against model inversion attacks. ARS helps joint
learning participants "encode" their data locally, then add adver-
sarial noise to representations before sharing them. The published
adversarial latent representation can defend against reconstruction
attacks, thereby ensuring privacy.

Owning to the good qualities of latent representations, ARS has
wide applicability. First, ARS is valid for various data types as train-
ing samples, not limited to images. Second, ARS is task-independent.
Shared data representations are reusable for various tasks. Third,
prior joint learning frameworks are commonly designed under sce-
narios of horizontal data partitioning (in which datasets of users
share the same feature space but differ in sample ID space), whereas
ARS can easily extended its framework to the vertical data parti-
tioning scenario (in which datasets of different users share the same
sample Ids but differ in feature columns) [31].

Based on the collaborative learning framework, we apply adver-
sarial example noise [10] to protect shared representations from
model inversion attacks. The intuition is that adding special-designed
small perturbations on shared data representations can confuse the
adversaries so that they cannot reconstruct the original data or par-
ticular private attributes from the obfuscated latent representations.
By simulating the behavior of attackers, we generate adversarial
noise for potential inverse models. The noise is added to data repre-
sentations before sharing them, in order to make it hard to recover
the original inputs. In the meantime, the scale of these perturba-
tions are too small to influence data utility. We propose defense
strategies against reconstruction attacks and attribute extraction
attacks respectively.

The main contributions of our work are summarized as follows:

• We propose ARS, a new paradigm for collaborative frame-
work which is based on representation learning. Different
from MPC and FL, ARS is decentralized and has wide appli-
cability.

• We introduce adversarial noise to defend against model in-
version attacks. To the best of our knowledge, we are the
first to apply adversarial examples to ensure privacy in col-
laborative learning.

• We evaluate our mechanism on multiple datasets and aspects.
The results demonstrate that ARS achieves a balance between
privacy and utility. We further discuss the limitations and
challenges of our work.

The remainder of the paper is organized as follows. We first
review related work in Section 2. Then we introduce how ARS
achieves collaborative learning from an overall perspective in Sec-
tion 3, and detail how adversarial noise is applied to defend against
model inversion attacks and ensure privacy in Section 4. Experi-
mental results are shown in Section 5. In Section 6, we discuss the
details and challenges of the work. Finally, we conclude the work
in Section 7.

2

2 RELATED WORK
2.1 Privacy-Preserving Representation

Learning

To avoid privacy leakage in collaborative learning, some previous
works focus on learning privacy-preserving representations [6, 30].
Latent representations retain the abstract features of data, which
can be used for further analysis like classification or regression.
Generally, the distribution of data representations can be learned
by unsupervised latent variable model, such as autoencoders [22].
Meanwhile, the original information of data can not be directly
inferred from representations. For example, in natural language
processing, words are transformed into vectors by embedding net-
works, which is called word2vec [3]. Without embedding networks,
it is difficult to recover original words from embedding vectors.

However, data representations are still vulnerable to model inver-
sion attacks [12]. Adversaries can build reconstruction networks to
recover original data or reveal some attributes of data from shared
representations, even though they have no knowledge of the struc-
ture or parameters of the feature extraction models [12, 20]. For
example, they can recover face samples, or infer gender, age and
other personal information from shared representations of face
images, which were only supposed to be used for training face
recognition models.

In order to defend against inversion attacks, recent works fo-
cus on adversarial training [30] or generative adversarial networks
(GANs) [13]. Attackers’ behaviors are simulated by another neu-
ral network while learning privacy representations of data, and
the two networks compete against each other to improve the ro-
bustness of representations. However, since it’s hard to achieve a
balance between the attacker models and defender models during
training [26], these methods may cost much time in the pretreat-
ing phase. Ferdowsi et al. [6] generate privacy representations by
producing sparse codemaps. The above defense methods could be
applied to collaborative learning, but the authors didn’t give a fur-
ther discussion on this scenario. In addition, all these methods are
task-oriented. When the task of shared data changes, they need to
generate new task-oriented data representation again.

2.2 Adversarial Examples
Adversarial examples are perturbed inputs designed to fool machine
learning models [10]. Formally, we denote by 𝑓 : X → {1, . . . , 𝑛} a
classifier. For an input 𝑥 ∈ X and a label 𝑙 = 𝑓 (𝑥), we call a vector
𝑟 an adversarial noise if it satisfies:

∥𝑟 ∥ ≤ 𝜖, 𝑓 (𝑥 + 𝑟 ) ≠ 𝑙,

where 𝜖 is a small hyper-parameter to adjust the scale of noise.

Adversarial examples have strong transferability. Some works
[18] have shown that adversarial examples generated for a model
can often confuse another model. This property is used to execute
transferability based attacks [25]. Even if an attacker has no knowl-
edge about the details of a target model, it can still craft adversarial
examples successfully by attacking a substitute model. Therefore,
adversarial examples have become a significant threat to machine
learning models [10, 27].

Except for treating adversarial examples as threats, some works
utilize the properties of adversarial examples to protect user’s pri-
vacy [28]. In this work, we also use adversarial noise to defend
against machine learning based inferring attacks. To the best of our
knowledge, we are the first to apply adversarial examples to data
sharing mechanisms for collaborative learning.

3 OVERVIEW OF ARS FRAMEWORK
ARS achieves collaborative learning by helping users share data
representations instead of raw data to train models. In this section,
we first present a joint learning scenario, and propose standards for
evaluating the effectiveness of a framework. Then we introduce how
ARS works in the whole collaborative learning process. Specifically,
how participants encoder their data into latent representations,
how to share data representations with others, and how shared
data is used for various machine learning tasks.

3.1 Collaborative Learning Scenario
Consider the scenario where 𝐾 parties share local data for collab-
orative training. Note that data belonging to distributed parties
can be partitioned horizontally (which means datasets shares same
feature space but differ in sample ID space) or vertically (which
means datasets shares different same feature space while sample
ids are same). For simplicity, we assume data is partitioned hori-
zontally (extension to vertical data partitioning in Section 4.4). The
dataset of the 𝑖-th party is represented as {(𝑋𝑖, 𝑌𝑖 )} = {(𝑥 1
𝑖 , 𝑦1
𝑖 ),
(𝑥 2
𝑖 ) is a pair of training sample
and corresponding label, and 𝑁𝑖 is the number of samples. The goal
of each participant is to encode their data 𝑋𝑖 into representations
𝑍𝑖 = {𝑧1
𝑖 }, and finally train deep learning models on
𝑍1, 𝑍2, . . . , 𝑍𝐾 which is published by all the data owners.

)}, where (𝑥 𝑗

𝑖 ), . . . , (𝑥 𝑁𝑖

𝑖 , . . . , 𝑧𝑁𝑖

𝑖 , 𝑦 𝑗

, 𝑦𝑁𝑖
𝑖

𝑖 , 𝑦2

𝑖 , 𝑧2

𝑖

To privacy concerns, participants have to defend against model
inversion attacks that reconstruct original inputs from latent repre-
sentations. Furthermore, there might be some sensitive features or
attributes in training samples. Any label of a training sample can
become the machine learning object or a private attribute, depend-
ing on the particular user and tasks. For example, when building a
disease detection model, the illness of each patient can be a public
label, while the identity information such as gender, age should be
private. We denote by 𝑀 the number of private attributes that the
𝑖-th party has, and denote the set of labels of 𝑘-th private attributes
by 𝐴𝑖𝑘 = {𝑎1
𝑖𝑘 }, where 𝑘 ∈ {1, 2, . . . , 𝑀 }. Attackers can
𝑖𝑘
also conduct attribute extraction attacks to recover these private
attributes.

, . . . , 𝑎𝑁𝑖

, 𝑎2
𝑖𝑘

We highlight the following notations which are important to our

discussion.

• (𝑋𝑖, 𝑌𝑖 ): original training set of user 𝑖, and label set depending

on deep learning tasks.

𝑖 ): the 𝑗-th sample and corresponding label of user 𝑖.

𝑖 , 𝑦 𝑗
: value (label) of the 𝑘-th private attribute of 𝑥 𝑗
𝑖 .

• (𝑥 𝑗
• 𝑎 𝑗
𝑖𝑘
• Enc(·): feature extractor which encode inputs into latent

representations.

• 𝜃Enc: parameters of the model Enc.
• 𝑧 𝑗

𝑖 : latent representation of the 𝑗-th sample and label of user
𝑖 = Enc(𝑥 𝑗
𝑖, which satisfies 𝑧 𝑗
𝑖 ).

3

• 𝐶 (·): downstream deep learning model which takes {(𝑍𝑖, 𝑌𝑖 )}𝐾
𝑖=1

as the training set.

• Dec(·): (theoretical) inverse model of Enc, which maps rep-

resentations to reconstruction of inputs.

• SDec(·): decoder trained by adversaries to conduct model
inversion attacks, aiming to reconstruct inputs from shared
data representations.

3.2 Quantitative Criteria of ARS Mechanism
Formally, given a sample set 𝑋 and a label set 𝑌 , the main goal of
data sharing in ARS is to design a mapping Enc : X −→ Z that has
"nice" properties. These properties consists of utility and privacy
of shared data representations as follows.

3.2.1 Utility. After obtaining the shared data, each user can use
dataset {(𝑍1, 𝑌1), (𝑍2, 𝑌2), . . . , (𝑍𝐾 , 𝑌𝐾 )} to train downstream deep
learning model 𝐶 : Z −→ Y, such as classifiers to predict the label
𝑦 from 𝑧. Data utility requires the accuracy to approach the results
of models trained directly on {(𝑋𝑖, 𝑌𝑖 )}𝐾
𝑖=1. Therefore, Enc and 𝐶
should be optimized by minimizing the following expectation:

E𝑥,𝑦∼𝑝 (𝑥 𝑦)

(cid:16)(cid:12)
(cid:12)
(cid:12)

E𝑧∼𝑝Enc (𝑧 |𝑥) [𝑝𝐶 (𝑦|𝑧)] − 𝑝 (𝑦|𝑥)

(cid:17)

(cid:12)
(cid:12)
(cid:12)

,

(1)

which ensures that encoding data into representations will not lose
much data utility.

3.2.2 Privacy. Privacy characterizes the difficulty of finding a model
Dec : Z −→ X to recover raw data, such as visualization informa-
tion of inputs of picture type. Without loss of generality, we focus
on reconstruction attacks (attribute extraction attacks is discussed
in Section 4.4). Therefore, ARS minimizes privacy leakage L, which
can be defined as reconstruction loss:

L = L𝑅 = −

1
𝑁

𝑁
∑︁

𝑗=1

(cid:16)
𝑥 𝑗 , Dec(Enc(𝑥 𝑗 ))

(cid:17)

,

D

(2)

where distance function D (·, ·) is used to describe the similarity
of original and reconstructed samples. In practice, the distance
function is often defined as 𝑙𝑝 norm. In computer vision, distance
between two images can also be measured as PSNR or SSIM [14].
If we consider attribute extraction attacks, we similarly use fea-
ture loss to indicate how possible an attacker can predict private
features successfully. Note that we don’t adopt the error between
the true value of private attributes and prediction results of at-
tackers, in case adversaries break the defense by just flipping their
results. Instead, we calculate the distance between the prediction
result and a fixed vector. The feature loss corresponding to the 𝑘-th
privacy attribute is defined as:

L𝐴𝑘 =

1
𝑁

𝑁
∑︁

𝑗=1

(cid:16)
𝑟𝑘, 𝐹𝑘 (Enc(𝑥 𝑗 ))

(cid:17)

,

D

(3)

where 𝑟𝑘 is a fixed vector generated randomly, whose size is the
same as 𝑎𝑖𝑘 . 𝐹𝑘 is a corresponding attribute extraction network
trained by adversaries. Low L𝐴𝑘
implies meaningless prediction
results. In this condition, the encoding system minimizes the overall
generalization loss:

L = 𝜆0L𝑅 +

𝑀
∑︁

𝑘=1

𝜆𝑘 L𝐴𝑘

,

(4)

Figure 1: An overview of our basic encoding-based privacy-preserving data sharing mechanism.

where (cid:205)𝑀
privacy requirements.

𝑘=0 𝜆𝑘 = 1. The value of 𝜆𝑘 is assigned depending on users’

For example, model 𝐶 : Z −→ Y can be trained by minimize
the empirical risk:

3.3 The ARS Collaborative Learning

Framework

We apply autoencoder [22], an unsupervised neural network to
learn data representations. Autoencoder can be divided into an
encoder part and a decoder part. The encoder transforms inputs
into latent representations, while the decoder map representations
to reconstruction of inputs with size smaller than inputs. The op-
timization target of the autoencoder is to minimize the difference
between original inputs and reconstructed ones. Since the encoder
compress the feature space, latent representations are wished to
remain high-level features of input data [5].

As shown in Fig. 1, the ARS collaborative learning framework

consists of three phases:

• Encoder publishing phase. In this phase, a common encoder
Enc is trained and published to all participants. The user
having permission to train Enc is called initiator, who can
be selected randomly, or the party owning the most amount
of data. The mechanism in which users shares the same
encoder is to ensure that each column of representation
vectors shared by any user expresses a specific meaning.
Suppose that user 𝑡 is chosen, it first trains an autoencoder
on its local dataset and then publish the encoder part (Enc),
as:

𝑁𝑡
∑︁

1
𝜃Enc, 𝜃Dec = arg min
𝑁𝑡
𝜃Enc,𝜃Dec
𝑡 = Dec(Enc(𝑥 𝑗
¯𝑥 𝑗

where

𝑗=1
𝑡 )).

(cid:16)
𝑥 𝑗
𝑡 , ¯𝑥 𝑗

𝑡

(cid:17)

,

D

(5)

• Data sharing phase. After the common encoder Enc is pub-
lished, user 𝑖 encode its data into representations as: 𝑧 𝑗
𝑖 =
Enc(𝑥 𝑗
𝑖 ). Then it shares the pair {(𝑍𝑖, 𝑌𝑖 )} with the other
parties. (In fact, users generate noise 𝛿 for each data rep-
resentation, then publish ˆ𝑧 = 𝑧 + 𝛿 instead of 𝑧 to defend
against model inversion attacks. See Section 4).

• Collaborative learning phase. Finally, each participant can use
𝑖=1 to train deep learning models locally.

the pairs {(𝑍𝑖, 𝑌𝑖 )}𝐾

4

ˆ𝑅(𝐶) =

1
𝑁

𝑁
∑︁

𝑗=1

L (𝑦 𝑗 , 𝐶 (𝑧 𝑗 )),

(6)

where 𝑁 is the amount of (𝑧, 𝑦) pairs shared by all users, and
L is the loss function of model 𝐶.

The ARS framework provides data utility from two aspects. First,
accuracy of the deep learning task (i.e. prediction of label 𝑦) is en-
sured. Equation (1) can be explained as: for any 𝑥 sampled from the
input distribution and its ground truth label 𝑦, we aim to maximize
E𝑧∼𝑝Enc (𝑧 |𝑥) [𝑝𝐶 (𝑦|𝑧)] to make it close to 𝑝 (𝑦|𝑥) (which should be
slightly less than 1). The goal is equivalent to minimize the gener-
alization error of 𝐶:

𝑅(𝐶) = E𝑥,𝑦∼𝑝 (𝑥 𝑦)

(cid:2)E𝑧∼𝑝Enc (𝑧 |𝑥) [L (𝑦, 𝐶 (𝑧))](cid:3) .

(7)

Empirically, models with less empirical risk (training error) nor-
mally has less generalization error as well. ARS reduces 𝑅(𝐶) by
decreasing ˆ𝑅(𝐶) in the collaborative learning phase. If dataset
{𝑋𝑖, 𝑌𝑖 }𝐾
𝑖=1 is representative and Enc is well trained, the joint learn-
ing framework will not lose much accuracy.

Second, shared data representations are adaptive to various tasks.
Since Enc is trained without any supervision or knowledge about
collaborative learning tasks, we have 𝑝 (𝑧|𝑥) = 𝑝 (𝑧|𝑥𝑦) indicating
that latent representations are task-independent. Meanwhile, data
representations are valid for downstream tasks, which can be ex-
plained by a basic idea that learning about the input distribution
helps with learning about the mapping from inputs to outputs [9].
A well trained autoencoder remains high level features of inputs,
which are also useful for supervised learning tasks.

4 ADVERSARIAL NOISE MECHANISM
AGAINST INVERSION ATTACKS

This section discusses the privacy of ARS. We first define a threat
model caused by model inversion attacks. Then we describe how
adversarial noise protects data representations from the inversion
attacks. Finally, we extend ARS to the scenarios of vertical data
partitioning and attribute extraction attacks.

Figure 2: Model inversion attack to reconstruct samples.

Figure 3: Adding adversarial noise on latent representations.

4.1 Threat Model
The main threat of ARS collaborative learning is model inversion
attacks. Although private information of raw data is hidden into
representations, it may still be recovered by reconstruction models,
like Dec trained by initiator in the encoder publishing phase. For
simplicity, we firstly focus on reconstruction attacks, and extend
our method to overcome attribute extraction attacks in Section 4.4.
The threat model defines adversaries who act like curious partic-
ipants and try to recover the original input 𝑥 from data represen-
tation 𝑧 = Enc(𝑥) for private information. Like other participants,
adversaries can obtain data representations shared by users and
have query access to the published encoder Enc, but have no knowl-
edge about the architecture and parameters of Enc. Nonetheless,
adversaries can still exploit query feedback of Enc to carry out
black-box attacks and acquire substitute decoders of Dec. For in-
stance, if user 𝑖 with data {(𝑋𝑖, 𝑌𝑖 )} is an attacker, it can generate
latent representations by querying 𝑧 𝑗
𝑖 ) for 𝑁𝑖 times. Then
it can build a substitute decoder SDec : Z −→ X as Figure 2 shows.
SDec can be optimized by minimizing the distance between original
and recovered samples:

𝑖 = Enc(𝑥 𝑗

𝜃SDec = arg min

𝜃SDec

1
𝑁𝑖

𝑁𝑖
∑︁

𝑗=1

(cid:16)
𝑖 , SDec(Enc(𝑥 𝑗
𝑥 𝑗

𝑖 ))

(cid:17)

.

D

(8)

This kind of attack can be regarded as chosen-plaintext attack
(CPA) from the cryptographic point of view. The reconstruction
ability of model inversion attacks is strong when training samples
of victims and attackers has similar distribution. In the following,
we propose the adversarial noise mechanism to defend against the
inversion attacks.

4.2 Adding Adversarial Noise
The strategy to defense against inversion attacks comes from a
simple idea: adding an intentionally designed small noise on latent
representations before sharing them. On the one hand, the scale of the
noise vector is so small that it would not reduce the utility of shared
data representations. On the other hand, we hope adding noise on
data representations can make data reconstructed by SDec different
enough from the original inputs. Inspired by adversarial examples,
we let users add adversarial noise to latent representations in set 𝑍 ,
and share the set of adversarial examples ˆ𝑍 instead, as shown in
Figure 3.

According to some researches [25], adversarial examples have
strong transferability. Empirically, if ˆ𝑍 successfully fools the de-
coder from which it is generated, then it is likely to cause another
decoder with similar decision boundary to recover samples that are
very different from the original inputs. The properties of adversar-
ial noise determine its ability to protect the privacy of shared data
representations even if the scale of noise is small.

The method to generate effective adversarial noise consists of
two steps. A participant first trains a substitute decoder SDec locally
by simulating reconstruction attacks; then it generates adversarial
noise for 𝑍 to make SDec invalid. Adversarial noise is generated
through iterative fast gradient sign method (I-FGSM) [10], which
sets the direction of adversarial noise to the gradient of objective
function L𝑅 with respect to 𝑧. The adversarial latent representation
ˆ𝑧 is calculated as:

ˆ𝑧 (𝑡 +1) = ˆ𝑧 (𝑡 ) + 𝛼 · sign (cid:16)
ˆ𝑧 (1) = 𝑧,

𝑡 = 1, . . . , 𝑛,

(cid:16)
𝑥, SDec( ˆ𝑧 (𝑡 ) )

∇𝑧 D
ˆ𝑧 = ˆ𝑧 (𝑛+1),

(cid:17)(cid:17)

,

(9)

where 𝛼 is a hyper-parameter regulating the scale of noise in each
iteration and 𝑛 is the iteration time. The adversarial noise on 𝑧 can
be denoted as:

𝛿𝑅 = ˆ𝑧 − 𝑧.
(10)
In consideration of data utility, the difference between 𝑧 and ˆ𝑧
should not be so great, otherwise ˆ𝑧 would lose most of the features
of 𝑥. Therefore, given an encoded vector 𝑧, we must ensure that
|𝛿𝑅 | = | ˆ𝑧 − 𝑧| ≤ 𝜖, where 𝜖 is a hyper-parameter to be chosen. Next
we prove that adversarial noise in Equation (9) satisfies the privacy
budget.

Proposition 1. Given 𝜖 ∈ R∗, 𝑛 ∈ N∗. Suppose 𝛼 = 𝜖

𝑛 , then
adversarial data representation ˆ𝑧 defined by Equation (9) satisfies:
| ˆ𝑧 − 𝑧| ≤ 𝜖.

∇𝑧 D

Proof. For any iteration step 𝑡 ∈ {1, . . . , 𝑛}, we have:
ˆ𝑧 (𝑡 +1) − ˆ𝑧 (𝑡 ) (cid:12)
(cid:12)
(cid:17)(cid:17)(cid:12)
(cid:12)
𝛼 · sign (cid:16)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sign (cid:16)
= 𝛼
(cid:12)
(cid:12)
= 𝛼 .
Therefore, |𝛿𝑅 | = | ˆ𝑧 − 𝑧| = | ˆ𝑧 (𝑛+1) − ˆ𝑧 (1) | ≤ (cid:205)𝑛
𝑛 · 𝛼 = 𝜖.

(cid:16)
𝑥, SDec( ˆ𝑧 (𝑡 ) )
(cid:17)(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:16)
𝑥, SDec( ˆ𝑧 (𝑡 ) )

∇𝑧 D

𝑡 =1 | ˆ𝑧 (𝑡 +1) − ˆ𝑧 (𝑡 ) | =
□

Proposition 1 shows that if we set 𝛼 to 𝜖

𝑛 in Equation (9), then
the scale of adversarial noise will be limited to 𝜖. Here 𝜖 is called

5

defense intensity, which determines the utility and privacy of repre-
sentations.

4.3 Adding Masked Adversarial Noise
ˆ𝑍 can mislead most of decoders trained from pairs {(𝑍, 𝑋 )} due to
the transferability of adversarial examples. However, model inver-
sion can still occur if attackers apply adversarial training [10] (also
called data enhancement) to build SDec′ : ˆZ −→ X. Suppose an
attacker (user 𝑖) aims to execute a reconstruction attack. In the data
sharing phase, the attacker first trains SDec on its local data 𝑋𝑖 by
optimizing Equation (8). Then it transforms 𝑋𝑖 into representations
𝑍𝑖 and adding adversarial noise to them as Equation (9) shows.
After that, the attacker can train SDec′ on {( ˆ𝑍𝑖, 𝑋𝑖 )} as:

𝜃SDec′ = arg min

𝜃SDec′

1
𝑁𝑖

𝑁𝑖
∑︁

𝑗=1

(cid:16)
𝑖 , SDec′( ˆ𝑧 𝑗
𝑥 𝑗
𝑖 )

(cid:17)

.

D

(11)

To defend against model inversion attack with data enhancement,
we propose noise masking, a simple and effective method to make
participants perturb data representations in unique ways. Each
user possess a mask vector (denoted by m) with the same size as
data representations. All dimensions of m are randomly initialized
to either 0 or 1, in order to mask some dimensions of calculated
gradients and thus allow users to perturb other dimensions of
representations. The process of generating masked adversarial noise
on representation 𝑧 is expressed as:

ˆ𝑧 (𝑡 +1) = ˆ𝑧 (𝑡 ) +

𝑡 = 1, . . . , 𝑛,

𝜖
· sign (cid:16)
𝑛
ˆ𝑧 (1) = 𝑧,

(cid:16)
𝑥, SDec( ˆ𝑧 (𝑡 ) )

m · ∇𝑧 D
ˆ𝑧 = ˆ𝑧 (𝑛+1) .

(cid:17)(cid:17)

,

(12)

The value of a vector m is held by its owner secretly, which leads
attackers to train substitute models on data representations that
are perturbed in a significant different way from representations of
target users with high probability. We discuss whether and when
the masks are effective in Section 6.1, and prove that mask vectors
with sufficiently large dimension are difficult to crack through brute
force. Therefore, ARS with noise masking can be considered safe
enough. Algorithm 1 shows the overall process of generating data
representations in ARS.

4.4 Extension
4.4.1 Attribute Exaction Attacks. In addition to reconstruction at-
tacks, user 𝑖 acting as an adversary can also train a classifier 𝐹𝑘 :
Z −→ A𝑘 on its local pairs {(𝑍𝑖, 𝐴𝑖,𝑘 )}, as:

𝜃𝐹𝑘 = arg min

𝜃𝐹𝑘

1
𝑁𝑖

𝑁𝑖
∑︁

𝑗=1

D

(cid:16)
𝑎 𝑗
𝑖𝑘

, 𝐹𝑘 (𝑧 𝑗
𝑖 )

(cid:17)

,

(13)

where 𝑘 ∈ {1, 2, . . . , 𝑀 } is the index of the target private attribute.
After that, the attacker can conduct attribute extraction attacks by
predicting the 𝑘-th private attribute from shared data representa-
tions.

The strategy to overcome the feature leakage is similar to the
above method. To preserve the 𝑘-th private attribute, users should
first train a classifier 𝐹𝑘 locally, and then craft adversarial noise on
. Here we simply apply FGSM
𝑍 to maximize the feature loss L𝐴𝑘

Algorithm 1: ARS Data Sharing Method for User 𝑖
Input: training samples 𝑋𝑖 = {𝑥 1
Output: representations ˆ𝑍𝑖 = { ˆ𝑧1

𝑖 , . . . , 𝑥 𝑁𝑖
𝑖 }
𝑖 , . . . , ˆ𝑧𝑁𝑖
𝑖 }

𝑖 , 𝑥 2
𝑖 , ˆ𝑧2

1 initialize Enc, 𝜖, 𝑛, m𝑖 ;
2 𝑍𝑖 = {𝑧1
𝑖 , 𝑧2
3 update SDec via:

𝑖 , . . . , 𝑧𝑁𝑖

𝑖 }, where 𝑧 𝑗

𝑖 = Enc(𝑥 𝑗
𝑖 );

𝜃SDec = arg min𝜃SDec

1
𝑁𝑖

(cid:205)𝑁𝑖

𝑗=1 D

(cid:16)
𝑖 , SDec(𝑧 𝑗
𝑥 𝑗
𝑖 )

(cid:17);

5

4 for 𝑗 = 1 to 𝑁𝑖 do
ˆ𝑧 𝑗
𝑖 = 𝑧 𝑗
𝑖 ;
for 𝑘 = 1 to 𝑛 do
𝑖 = ˆ𝑧 𝑗
ˆ𝑧 𝑗

𝑖 + 𝜖

7

6

𝑛 · sign (cid:16)

m𝑖 · ∇𝑧 D

(cid:16)
𝑥, SDec( ˆ𝑧 𝑗
𝑖 )

(cid:17)(cid:17);

end

8
9 end
10 return ˆ𝑍𝑖 = { ˆ𝑧1

𝑖 , ˆ𝑧2

𝑖 , . . . , ˆ𝑧𝑁𝑖

𝑖 };

method as:

𝛿𝐴𝑘 = −𝜖 · sign (cid:16)

∇𝑧 D

(cid:16)
𝑟𝑘, 𝐹𝑘 (Enc(𝑥 𝑗 ))

(cid:17)(cid:17)

,

(14)

where 𝛿𝑘 is the adversarial noise to preserve the 𝑘-th private at-
tribute. The purpose is to keep prediction results close to a certain
vector 𝑟𝑘 , regardless of inputs, thus lead the prediction to meaning-
less results.

Consequently, the overall adversarial noise of an arbitrary par-

ticipant 𝑖 can be calculated as:

𝛿𝑖 = 𝜆0𝛿𝑅 +

𝑀
∑︁

𝜆𝑘𝛿𝐴𝑘

,

(15)

𝑘=1
where (cid:205)𝑀
𝑘=0 𝜆𝑘 = 1, so that |𝛿𝑖 | ≤ 𝜖. Experimental results in Section
5.3 show that letting 𝜆0 = 1
2𝑀 (𝑘 ≥ 1) may be a good choice
to ensure the defense against data leakage and feature leakage at
the same time.

2 , 𝜆𝑘 = 1

4.4.2 Vertical Data Partitioning. In the second extension, we gen-
eralize ARS to the vertical data partitioning scenario. For simplicity,
we suppose that datasets are aligned on IDs, and each participant
owns 𝑁 samples. In the training phase, user 𝑖 trains Enc𝑖 on its lo-
𝑖 }, then calculate 𝑧 𝑗
cal dataset 𝑋𝑖 = {𝑥 1
𝑖 ) and
share ˆ𝑧 𝑗
𝑖 = 𝑧 𝑗
𝑖 . One of the users share labels 𝑌 = {𝑦1, 𝑦2, . . . , 𝑦𝑁 }.
Different from horizontal data partition, a common encoder is un-
used, so Enc𝑖 does not need to be shared.

𝑖 = Enc𝑖 (𝑥 𝑗

𝑖 , . . . , 𝑥 𝑁

𝑖 +𝛿 𝑗

𝑖 , 𝑥 2

To train downstream models collaboratively, data representa-
tions shared by each users are concatenated as 𝑤 𝑗 = concat( ˆ𝑧 𝑗
1,
2, . . . , ˆ𝑧 𝑗
ˆ𝑧 𝑗
𝐾 ). Afterwards, any user 𝑖 who has sufficient computational
power is able to train model 𝐶𝑖 on {(𝑊 , 𝑌 )} locally.

5 EXPERIMENTS
In this section, we evaluate ARS by simulating a multi-party collab-
orative learning scenario. We present the performance of ARS in
privacy preserving, as well as compare it with other joint learning
frameworks, and then study the effectiveness of our mechanism in
protecting private attributes.

6

5.1 Experiment Settings
5.1.1 Datasets. The experiments are conducted on three datasets:
MNIST [17] and CelebA [19] for horizontal data partitioning, and
Adult [4] from UCI Machine Learning Repository for vertical data
partitioning. MNIST consists of 70000 handwritten digits, the size
of each image is 28 × 28. CelebA is a face dataset with more than
200K images, each with 40 binary attributes. Each image is resized
to 96 × 96 × 3. Adult dataset is census information, and the given
task is to determine whether a person makes over $50,000 a year
basing on 13 attributes including "age", "workclass", "education". In
our experiments, the inputs are mapped into real vectors with 133
feature columns.

Scenario. We simulate horizontal data partitioning scenarios
5.1.2
where the number of participants 𝐾 = 5 in MNIST, and 𝐾 = 3
in CelebA. Each participant is randomly assigned 10000 examples
as a local dataset. When conducting experiments on MNIST, the
common encoder Enc and each user’s substitute decoder SDec are
implemented by three-layer ReLU-based fully connected neural
networks. On CelebA, Enc and SDec are implemented by four-layer
convolutional neural networks. In the data sharing phase, the itera-
tion time 𝑛 is set to 10. We suppose that attackers execute adversar-
ial training attacks (data enhancement) for two types of objectives:
to recover original samples (see Section 5.2), or to extract private
attributes (see Section 5.3). In the collaborative learning phase, the
tasks are set as training classifiers on shared data representations
{ ˆ𝑍𝑖 }𝐾
𝑖=1. The labels are 10-dimensional one-hot codes in MNIST,
and 2-dimensional vectors corresponding to binary attributes in
CelebA.

We also present a vertical data partitioning scenario on Adult
dataset in Section 5.2.4. Each user holds 20000 aligned samples,
while the number of column vectors owned by different users is
kept as close as possible. The encoders and substitute decoders
are implemented by four-layer ReLU-based fully connected neural
networks. An adversary owning 5000 partial inputs would like to
attack the user who possess the same feature columns of samples.
All above tasks can be regarded as case studies of collaborative
learning in the real world. For example, companies can share pri-
vacy representations of photos to train face recognition models;
enterprises may cooperate with each other to draw more compre-
hensive customer personas.

5.1.3 Privacy Metrics. We consider three metrics to measure D (𝑥, ¯𝑥),
which reflects the privacy leakage caused by reconstruction attacks.
According to a common practice that represents D (𝑥, ¯𝑥) as 𝑙𝑝 norm,
we define D (𝑥, ¯𝑥) as ∥𝑥 − ¯𝑥 ∥2. This metric is also well known as
MSE (Mean Square Error). PSNR (Peak Signal-to-Noise Ratio) and
SSIM (Structural Similarity Index) [14] reflects pixel level difference
between original and reconstructed images, and are highly consis-
tent with human perceptual capability, so they are also applicable
for evaluating privacy leakage. The metrics are widely adopted in
related researches, which allows us to compare their results with
ours directly. To measure the privacy of private attributes, we sim-
ulate attacks on these attributes and record the proportion that the
predictions are equal to the fixed vector.

7

(a) MNIST, Accuray

(b) MNIST, MSE

(c) CelebA, Accuray

(d) CelebA, MSE

Figure 4: Classification accuracy and reconstruction loss ver-
sus 𝜖.

(a)

(b)

(c)

Figure 5: Digit images and corresponding reconstructed im-
ages. (a) Original input images. (b) Reconstructed images cor-
responding to data representations without noise. (c) Recon-
structed images corresponding to representations with ad-
versarial noise (𝜖 = 50).

5.2 Protecting Privacy Against Reconstruction

Attacks

5.2.1 Defense Intensity. We firstly evaluate the utility and privacy
of ARS with different defense intensity 𝜖. Figure 4 reports the ac-
curacy of the classifier trained on shared data representations to
evaluate the utility, and MSE of reconstructed images to evaluate
privacy. Experiments are conducted on both MNIST and CelebA
datasets. On CelebA, the task is to predict the attribute "Male". We
set up another method as a baseline, which generates random noise
with uniform distribution instead of adversarial noise. Observe
that with the increase of 𝜖, the reconstruction loss becomes higher,
which indicates that adversarial noise with a larger scale makes it
more difficult to filch private information from shared representa-
tions. When measuring the utility of shard data, we find that when
𝜖 becomes larger, the accuracy decreases slightly from 97.2% to
89.3% in MNIST, and from 93.7% to 84.3% in CelebA. The variety of

(a) Original Images

(b) 𝜖 = 0

(c) 𝜖 = 25

(d) 𝜖 = 50, no mask

(e) 𝜖 = 50

(f) 𝜖 = 100

Figure 6: Face images and corresponding reconstructed images. Images in column (a) are raw data. Column (b), (c), (e), (f)
corresponds to adversarial latent representations with different 𝜖. Column (d) corresponds to adversarial representations (𝜖 =
50) without noise masking.

MSE and accuracy with the scale of noise illustrates the trade-off
between utility and privacy of shared data.

5.2.2 Visualization of Reconstructed Images. We then explore the
effectiveness of adversarial noise defense by displaying the im-
ages under reconstruction attacks. Figure 5 compares digit images
recovered from adversarial latent representations with the unde-
fended version, and illustrates that 𝜖 = 50 can well ensure data
privacy. This preliminarily proves the privacy of the adversarial
noise mechanism.

We further study the influence of different 𝜖 in CelebA. Figure 6
shows the reconstructed images corresponding to data representa-
tions adding several kinds of noise. If adversarial noise is not used,
the reconstructed image restores almost all private information of
faces. When 𝜖 is set to 50, the recovered faces lose most of the fea-
tures used to determine identity. When 𝜖 = 100, the reconstructed
images become completely unrecognizable. For further discussion,
we present the result when 𝜖 = 50, while the adversarial noise is
not masked. As shown in the column (d) of Figure 6, the faces do get
blurry, but some features with private information are still retained.
The experiments present satisfactory performance of adversarial
noise on latent representations. If defense intensity is set to a suffi-
ciently small value, the shared data can maintain high utility and
privacy. In a real application, data utility is expected to be as higher
as possible while privacy is well preserved. We choose 𝜖 = 50 as
a suitable defense intensity in both datasets in the following ex-
periments, because of its high privacy and acceptable classification
accuracy, which is 92.8% in MNIST, and 88.3% in CelebA.

5.2.3 Comparison with Existing Mechanisms. We first evaluate data
utility by comparing ARS with mainstream joint learning frame-
works on MNIST. Table 1 summarizes the performance of these

8

methods in terms of important metrics such as classification ac-
curacy and communication cost. SecureML [21] is a two-party
computation (2PC) collaborative learning protocol based on secret
sharing and garbled circuits. Differential private federated learning
[8] (which we call DPFL) approximates the aggregation results with
a randomized mechanism to protect datasets against differential
attacks. As shown in the table, the classification accuracy of ARS
reaches a level similar to that of MPC and FL based methods, when
we regard 𝜖 = 50 as a compromise between utility and privacy.
Note that ARS is designed for general scenarios rather than specific
datasets or tasks, and better results can be achieved by using more
complex models or fine tuning hyper-parameters. Moreover, ARS
has a low communication cost. We denote by 𝑁 the average number
of training samples owned by each party, by 𝑑 the dimension of
the original data, by 𝐵 the batch size, by 𝑡 the number of epochs to
train models, by 𝑊 the number of parameters in a model, and by ℎ
the dimension of latent representations. Since the iteration times to
train a deep learning model using stochastic gradient descent (SGD)
method depends on the number of training samples, otherwise the
model will be underfitted, the complexity of 𝐸 should not be lower
than 𝑁 , it is easy to prove that ARS has the lowest communication
complexity among the three methods.

We next evaluate the privacy of ARS by simulating reconstruc-
tion attacks. Since PSNR and SSIM are widely adopted by the lat-
est researches in this field, we also calculate these two metrics as
privacy leakage, and compare ARS with two state-of-the-art data
sharing mechanisms: generative adversarial training based sharing
mechanism [30] and SCA based sharing mechanism [6]. Similar to
ARS, both of them learn representations of data. The experiment
is conducted on CelebA. Table 2 reports the privacy leakage of the
mechanisms. As we can see, ARS performs better than the other two

Table 1: Comparison of ARS and mainstream methods on important metrics.

Framework

Basic Method

Reported Accuracy on MNIST Communication Cost per Server

SecureML

MPC

DPFL

ARS

Federated Learning

Representation Learning

93.4%

≤ 92% (𝐾 ≤ 1000)
92.8% (𝜖 = 50)

⪰ 𝑂 (𝑁 · 𝐾 · 𝑑 + (𝐵 + 𝑑) · 𝑡)
𝑂 (𝐾 · 𝑊 · 𝑡)
𝑶 (𝑵 · 𝑲 · 𝒉)

frameworks in PSNR, even when 𝜖 = 50. When 𝜖 increases to 100,
SSIM of ARS also reaches the best result of the three mechanisms.

Table 2: Privacy of different representation based mecha-
nisms.

ARS

ARS

Baseline

(𝜖 = 50)

(𝜖 = 100)

(𝜖 = 50)

[30]

[6]

PSNR

SSIM

9.932
0.531

5.748

0.101

15.527

15.445

12.31

0.728

0.300

0.25

5.2.4 Vertical Data Partitioning. We next evaluate the performance
of ARS on Adult dataset under the vertical data partitioning scenario.
In Table 3, we divide the totally 133 feature columns of samples as
evenly as possible among three users, then present the prediction
accuracy (Acc), 𝐹1-score, and robustness to reconstruction attacks
of the collaborative learning system with different numbers of
participants. To confirm the effectiveness of the inversion attack,
we show the accuracy of adversarial training based reconstruction
from data representations without adversarial noise (Adv. Tr.). Here
we suppose that encoders used to generate data representations
can be obtained by adversaries. To estimate the privacy of ARS, we
present the reconstruction accuracy on concatenated adversarial
latent representations (Rec. Acc). We observe that large number
of collaborating users leads to higher prediction accuracy and 𝐹1-
score of the downstream classification model, and when the noise
scales up, the reconstruction accuracy drops to less than 50%. So we
demonstrate the great utility and privacy of ARS under the vertical
data partitioning scenario.

5.3 Preserving Privacy of Attributes
In this section, we evaluate ARS on a stronger assumption that
users have some private attributes to protect. We assess the ef-
fectiveness of defense against attribute extraction attacks by how
close the extracted features are to a fixed vector given by users.
The experiments are conducted on CelebA. For all participants, we
set predicting attribute "High Cheekbones" as the collaborative
learning task, while selecting "Male" and "Smiling" as private at-
tributes. Then we let each user train attribute extraction network
𝐹𝑘 corresponding to the 𝑘-th private attribute, which is similar to
the adv-training decoder attack. We choose some typical values of
𝜆 to generate adversarial noise, and set the fixed vector 𝑟 = (1, 0)
since the outputs of classifiers are two-dimensional vectors. Then
we record the proportion that the predicted private attribute 𝐹𝑘 ( ˆ𝑧)

(a) 𝜆𝐷 = 1, 𝜆1 = 𝜆2 = 0

(b) 𝜆𝐷 = 0.5, 𝜆1 = 𝜆2 = 0.25

(c) 𝜆𝐷 = 𝜆1 = 𝜆2 = 0.33

(d) 𝜆𝐷 = 0, 𝜆1 = 𝜆2 = 0.5

(e) 𝜆𝐷 = 0, 𝜆1 = 1, 𝜆2 = 0

(f) 𝜆𝐷 = 𝜆1 = 0.5, 𝜆2 = 0

Figure 7: Proportion that the predictions of private at-
tributes are equal to a given fixed vector.

is equal to 𝑟 to estimate the ability of ARS to mislead attribute
extraction models.

We analyze the effect of different compositions of adversarial
noise by changing 𝜆 as illustrated in Figure 7. (a-d) show that with
the increase of 𝜆𝑘 , the probability that predictions of the 𝑘-th at-
tribute are equal to 𝑟 becomes higher. Note that sometimes the
equating rate gets lower when 𝜖 increases to 100, this may be
caused by influence of the other components of adversarial noise.
(e-f) demonstrate that the weight of a noise 𝛿𝑘 has a great effect
on the privacy of the 𝑘-th attribute. Our further experiment shows
that the accuracy of attack classifiers can be close to 50% when
𝜖 = 50.

We further show that the accuracy of classifiers for preserved
attributes is close to 50%. Figure 8 presents the accuracy of classifiers

9

Table 3: Results with different numbers of data owners on the vertical data partitioning scenario.

𝜖

0

10

25

50

Acc

78.9%

78.9%

78.5%

78.5%

K=1

K=2

K=3

𝐹1-score Adv. Tr. Rec. Acc
97.4%
87.2%

97.3%

87.1%

86.8%

86.9%

97.2%

96.7%

92.8%

77.5%

45.4%

32.6%

Acc

82.2%

81.6%

81.2%

81.5%

𝐹1-score Adv. Tr. Rec. Acc
97.6%
88.7%

97.3%

88.1%

87.7%

88.2%

97.4%

93.4%

91.7%

82.8%

61.3%

46.2%

Acc

84.6%

83.8%

83.6%

83.5%

𝐹1-score Adv. Tr. Rec. Acc
97.7%
89.9%

97.5%

89.3%

89.5%

89.6%

97.2%

96.7%

93.6%

84.7%

55.9%

36.1%

for three attributes, two of which are private. With the increase of
𝜆𝑘 , the accuracy corresponding to the 𝑘-th attribute approaches
50% if 𝜆𝑘 > 0. In addition, when 𝜖 is set to 100, the classification
accuracy of the second private attribute "Smiling" has a significant
drop even if 𝜆2 = 0, this again proves the trade-off between utility
and privacy of shared data.In summary, ARS is shown to be effective
against attribute extraction attacks with acceptable privacy budget
𝜖.

Table 4: PSNR of different composition of noise and 𝜖.

(𝜆𝐷, 𝜆1, 𝜆2)

𝜖 = 0

𝜖 = 25

𝜖 = 50

𝜖 = 75

𝜖 = 100

(1.0, 0.0, 0.0)
(0.5, 0.25, 0.25)
(0.33, 0.33, 0.33)
(0.0, 0.5, 0.5)

22.617

15.274

22.492

18.363

22.497

19.408

8.812

12.033
14.473

7.456

10.261

6.720

9.522

11.976

10.671

22.482

20.827

19.109

15.004

13.179

(a) 𝜆𝐷 = 1, 𝜆1 = 𝜆2 = 0

(b) 𝜆𝐷 = 0.5, 𝜆1 = 𝜆2 = 0.25

(c) 𝜆𝐷 = 𝜆1 = 𝜆2 = 0.33

(d) 𝜆𝐷 = 0, 𝜆1 = 𝜆2 = 0.5

(e) 𝜆𝐷 = 0, 𝜆1 = 1, 𝜆2 = 0

(f) 𝜆𝐷 = 0.5, 𝜆1 = 0.5, 𝜆2 = 0

Figure 8: Classification accuracy on three attributes, with
variable value of 𝜖 and 𝜆. "Male" and "Smiling" are private
attributes.

We next evaluate the reconstruction error under the same sce-
nario. As we can see in Table 4, larger 𝜖 and 𝜆𝐷 lead to greater de-
fense against reconstruction attacks. If we consider 𝑃𝑆𝑁 𝑅 = 12.033

10

an acceptable privacy leakage since it is smaller than the results
of similar representation sharing works [30] and [6] we compared
in Section 5.2.3, then 𝜆𝐷 = 1
2 , 𝜆𝑘 = 1
2𝑀 is a good choice to defend
against reconstruction and attribute extraction attacks at the same
time.

5.4 Task-Independence Study
Another advantage of ARS mechanism is that the encoder publish-
ing phase is independent of the collaborative learning phase. Most
of the up-to-date joint learning frameworks are task-oriented. For
example, in the FederatedAveraging algorithm, a server builds a
global model according to a specific task, and communicate the
parameters with clients. For prior data sharing frameworks, the net-
works to extract latent representations of data are usually trained
with the task-oriented models (such as classifiers). The accuracy
of models trained from these representations is regarded as a part
of optimization objective functions of feature extracting networks.
Task-dependence causes low data utilization. If parties in an existing
collaborative learning framework have a new deep learning task,
they have to build another framework and train feature extracting
networks once again, which results in a heavy cost to transform
data and train deep learning models.

In the ARS mechanism, apart from private attributes that are
protected, the representation generating process is independent of
deep learning tasks. We set up a series of experiments on CelebA
to demonstrate this property. We select five of the forty attributes
in CelebA and set a binary classification for each attribute, none
of them are private to users. In the collaborative learning phase,
participants train five classification networks on the latent repre-
sentations. Each classifier corresponding to an attribute. Since the
MSE loss is only related to the encoder, but not to deep learning
tasks, we only focus on classification accuracy. Table 5 shows the
results with various scale of noise.

Table 5: Classification accuracy in different tasks. For each
of the five attributes from CelebA dataset, we train classi-
fiers to predict whether the label is positive or negative from
ˆ𝑧 generated by the same public encoder.

For any 𝑛-dimensional mask vectors m1 and m2, we denote the
Hamming distance between them as 𝐻 (m1, m2), and define the
𝑛−𝐻 (m1,m2)
overlapping rate between m1 and m2 as 𝑜 (m1, m2) =
.
𝑛
Then we have

Attribute

𝜖 = 0

𝜖 = 25

𝜖 = 50

𝜖 = 75

𝜖 = 100

Heavy Makeup

86.4% 85.7%

High Cheekbones

80.6% 84.2%

Male

Smiling

91.7% 91.7%

88.0% 84.7%

Wearing Lipstick

90.2% 80.6%

85.0%

80.1%

90.0%

87.9%

86.7%

87.5%

82.8%

89.3%

87.5%

85.0%

85.9%

82.1%

88.3%

84.3%

82.8%

As shown in the results, classification accuracy is higher than
73%, which indicates acceptable correctness. Only the accuracy
of predicting the attribute "Mouth Slightly Open" is lower than
80% with the increase of 𝜖. The results demonstrate that ARS is
task-independent. If there are demands for data sharing, the ini-
tiator can just train the public autoencoder, without considering
how data representations would be used. For participants, the only
remarkable thing is to determine their private attributes, and gen-
erate adversarial noise to preserve these attributes. This property
prevents the utility of latent representations from being limited
to specific tasks, and ensures the robustness of shared data. Rep-
resentations generated by users independently can achieve good
performance in various tasks if the value of hyper-parameter 𝜖
is chosen well. Consequently, ARS can preserve privacy during
the data sharing process, while maintaining the utility of data in
collaborative learning.

6 DISCUSSION
6.1 Discussions on Noise Masking
We focus on the security of noise masking mechanism by studying
whether it can defend against brute-force searching attacks. An at-
tacker can randomly enumerate several mask vectors, train inverse
models on representations with these mask vectors respectively
and take the vector that performs best in reconstructing others’
data as a good approximation of the victim’s mask. We explore ex-
perimentally the relationship between the reconstruction loss L𝑅
and the overlapping rate of masks held by attackers and defenders,
which equals to the Hamming distance of the mask vectors divided
by their dimension. The experiment is conducted on MNIST, with
settings stated in Section 5. As Table 6 illustrates, a higher over-
lapping rate leads to a higher risk of privacy leakage. So we’ll next
study the overlapping of masks.

Table 6: Reconstruction loss with various overlapping rate
of masks held by attackers and defenders.

Overlapping Rate

0%

25%

50%

75%

100%

𝜖 = 50
𝜖 = 100

0.119

0.101

0.082

0.048

0.021

0.179

0.156

0.128

0.09

0.025

11

P [𝑛 − 𝐻 (m1, m2) = 𝑖] =

1
2𝑛

(cid:19)

,

(cid:18)𝑛
𝑖

(16)

which means that 𝑋 = 𝑛 − 𝐻 (m1, m2) ∼ 𝐵(𝑛, 0.5).

Suppose 𝑡 is a real number such that 1

2 < 𝑡 ≤ 1, then from the
De Moivre-Laplace theorem, the probability that m1 and m2 have
𝑡 · 𝑛 bits different is:

lim
𝑛→∞
= lim
𝑛→∞

P [𝑜 (m1, m2) ≥ 𝑡]
P [𝑡𝑛 ≤ 𝑛 − 𝐻 (m1, m2) ≤ 𝑛]

(cid:34)

(2𝑡 − 1)

√

𝑛 ≤

= lim
𝑛→∞

P

𝑋 − 1
2𝑛
√
1
𝑛
2

(cid:35)

√
𝑛

≤

(17)

√

𝑛

∫

e− 𝑥 2

2 d𝑥

√

√

(2𝑡 −1)
𝑛
𝑛) − Φ((2𝑡 − 1)

√

𝑛)

= lim
𝑛→∞

1
2𝜋

= lim
𝑛→∞

Φ(

= 0.

Therefore, if the dimension 𝑛 is large enough, the probability
that the overlapping rate of two random 𝑛-dimensional vectors is
larger than 𝑡 approaches to 0 for ∀ 1
2 < 𝑡 ≤ 1. Moreover, we consider
𝜖 = 50 as an acceptable privacy budget for preserving information
of data. That is to say, an attack is considered successful if the
overlapping rate of masks held by the attacker and user should
be greater than a real number 𝑡, where 1
2 < 𝑡 ≤ 1. When the
dimension of latent representations is large enough, the privacy
of users’ data can be guaranteed. For example, the dimension of
latent representations is 256. If we accept 75% as overlapping rate,
then we have P [𝑜 (m1, m2) ≥ 0.75] ≤ 2.449 × 10−16, which means
that the privacy of data can be considered well preserved by mask
mechanism.

6.2 Future Work
Although ARS shows good performance in our given scenarios, this
mechanism still has some limitations. In the horizontal data parti-
tioning scenario, all users generate data representations through
the same feature extraction network, which is called the common
encoder. This requires the selected initiator to have a sufficient
amount of representative data. In practice, however, participants in
joint learning may lack enough training samples, or the data of each
user may not be independent and identically distributed (non-IID)
[15]. This leads the common encoder to be overfitted, so that it will
no longer be valid for all users. To cope with this problem, we tried
to let the parties train their own encoders on the local datasets,
while ensuring that the latent representations have the same dis-
tribution. For example, each user applies variational autoencoder
(VAE) [16] to constrain data representations to the standard nor-
mal distribution. Nevertheless, it is difficult to guarantee that the
same dimension of representations generated by different encoders
expresses the same semantic. Therefore, aggregation of the shared
data representations will no longer make sense.

In this study, we relax the hypothesis of the data owners, re-
quiring at least one party has sufficient training samples, and the
samples of each participant have identical distribution. This assump-
tion in accordance with the realistic B2C (business to customer)
settings, where the initiator can be an enterprise with a certain
accumulation of data. It can initiate data sharing with individual
users and provide pre-trained feature extraction models to them.
Future work will be dedicated to collaborative learning on non-IID
data, and we believe domain adaptation of data representations is a
viable solution to this problem.

We also introduce task-independence of the shared data, and how
this property can help to reduce communication cost. In this work,
data representations are extracted by unsupervised autoencoders
to avoid task-orientation, yet some prior knowledge such as the
available labels of data is underutilized. A possible area of future
work is multi-task learning [33], where the given tasks or training
labels can be made full use of and contribute to each client’s different
local problems. Related studies may bring higher utility of data to
task-independent collaborative learning.

7 CONCLUSION
In this work, we propose ARS, a privacy-preserving collaborative
learning framework. Users share representations of data to train
downstream models. Adversarial noise is used to protect shared
data from model inversion attacks. We evaluate our mechanism
and demonstrate that adding masked adversarial noise on latent
representations has a great effect in defending against reconstruc-
tion and attribute extraction attacks, while maintaining almost the
same utility as MPC and FL based training. Compared with some
prior data sharing mechanisms, ARS outperforms them in privacy
preservation. Besides, ARS is task-independent, and requires no
centralized control. Our work can be applied to collaborative learn-
ing scenarios, and provides a new idea on the research of data
sharing and joint learning frameworks.

REFERENCES
[1] Nitin Agrawal, Ali Shahin Shamsabadi, Matt J Kusner, and Adrià Gascón. 2019.
QUOTIENT: two-party secure neural network training and prediction. In Pro-
ceedings of the 2019 ACM SIGSAC Conference on Computer and Communications
Security. 1231–1247.

[2] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation
learning: A review and new perspectives. IEEE transactions on pattern analysis
and machine intelligence 35, 8 (2013), 1798–1828.

[3] Kenneth Ward Church. 2017. Word2Vec. Natural Language Engineering 23, 1

(2017), 155–162.

[4] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http:

//archive.ics.uci.edu/ml

[5] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. 2010. Why
does unsupervised pre-training help deep learning?. In Proceedings of the thir-
teenth international conference on artificial intelligence and statistics. JMLR Work-
shop and Conference Proceedings, 201–208.

[6] Sohrab Ferdowsi, Behrooz Razeghi, Taras Holotyak, Flavio P. Calmon, and Slava
Voloshynovskiy. 2020. Privacy-Preserving Image Sharing via Sparsifying Layers
on Convolutional Groups. ICASSP (2020).

[7] Craig Gentry. 2009. Fully homomorphic encryption using ideal lattices. In Proceed-
ings of the forty-first annual ACM symposium on Theory of computing. 169–178.
[8] Robin C. Geyer, Tassilo J. Klein, and Moin Nabi. 2017. Differentially Private
Federated Learning: A Client Level Perspective. arXiv preprint arXiv:1712.07557
(2017).

[9] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT

press.

[10] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and

harnessing adversarial examples. ICLR (2015).

12

[11] Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise
Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ram-
age. 2018. Federated learning for mobile keyboard prediction. arXiv preprint
arXiv:1811.03604 (2018).

[12] Zecheng He, Tianwei Zhang, and Ruby B Lee. 2019. Model inversion attacks
against collaborative inference. In Proceedings of the 35th Annual Computer Secu-
rity Applications Conference. 148–162.

[13] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. 2017. Deep Models
Under the GAN: Information Leakage from Collaborative Deep Learning. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
Security. 603–618.

[14] Alain Hore and Djemel Ziou. 2010. Image quality metrics: PSNR vs. SSIM. In
2010 20th international conference on pattern recognition. IEEE, 2366–2369.
[15] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al. 2021. Advances and open problems in federated learning.
Foundations and Trends® in Machine Learning 14, 1–2 (2021), 1–210.

[16] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.

arXiv preprint arXiv:1312.6114 (2013).

[17] Yann LeCun. 1998. The mnist database of handwritten digits. http://yann.lecun.

com/exdb/mnist/.

[18] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017. Delving into Trans-
ferable Adversarial Examples and Black-box Attacks. In ICLR 2017 : International
Conference on Learning Representations 2017.

[19] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2014. Deep Learning
Face Attributes in the Wild. 2015 IEEE International Conference on Computer
Vision (ICCV) (2014), 3730–3738.

[20] Aravindh Mahendran and Andrea Vedaldi. 2015. Understanding deep image rep-
resentations by inverting them. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 5188–5196.

[21] Payman Mohassel and Yupeng Zhang. 2017. Secureml: A system for scalable
privacy-preserving machine learning. In 2017 IEEE Symposium on Security and
Privacy (SP). IEEE, 19–38.

[22] Andrew Ng et al. 2011. Sparse autoencoder. CS294A Lecture notes 72, 2011 (2011),

1–19.

[23] Olga Ohrimenko, Felix Schuster, Cédric Fournet, Aastha Mehta, Sebastian
Nowozin, Kapil Vaswani, and Manuel Costa. 2016. Oblivious multi-party machine
learning on trusted processors. In 25th {USENIX} Security Symposium ({USENIX}
Security 16). 619–636.

[24] Pascal Paillier. 1999. Public-key cryptosystems based on composite degree resid-
uosity classes. In International Conference on the Theory and Applications of
Cryptographic Techniques. Springer, 223–238.

[25] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia conference on computer and
communications security. ACM, 506–519.

[26] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
and Xi Chen. 2016. Improved techniques for training gans. In Advances in neural
information processing systems. 2234–2242.

[27] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. 2018. Defense-GAN:
Protecting Classifiers Against Adversarial Attacks Using Generative Models. In
ICLR 2018 : International Conference on Learning Representations 2018.

[28] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. 2016.
Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recog-
nition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. 1528–1540.

[29] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving deep learning. In
Proceedings of the 22nd ACM SIGSAC conference on computer and communications
security. ACM, 1310–1321.

[30] Taihong Xiao, Yi-Hsuan Tsai, Kihyuk Sohn, Manmohan Chandraker, and Ming-
Hsuan Yang. 2020. Adversarial Learning of Privacy-Preserving and Task-Oriented
Representations. AAAI (2020).

[31] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine
learning: Concept and applications. ACM Transactions on Intelligent Systems and
Technology (TIST) 10, 2 (2019), 1–19.

[32] Andrew Chi-Chih Yao. 1986. How to generate and exchange secrets. In 27th
Annual Symposium on Foundations of Computer Science (sfcs 1986). IEEE, 162–167.
[33] Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE Transac-

tions on Knowledge and Data Engineering (2021).

