Attention Mesh: High-ﬁdelity Face Mesh Prediction in Real-time

Ivan Grishchenko Artsiom Ablavatski Yury Kartynnik Karthik Raveendran Matthias Grundmann
Google Research
1600 Amphitheatre Pkwy, Mountain View, CA 94043, USA
{igrishchenko, artsiom, kartynnik, krav, grundman}@google.com

0
2
0
2

n
u
J

9
1

]

V
C
.
s
c
[

1
v
2
6
9
0
1
.
6
0
0
2
:
v
i
X
r
a

Abstract

We present Attention Mesh, a lightweight architecture
for 3D face mesh prediction that uses attention to seman-
tically meaningful regions. Our neural network is designed
for real-time on-device inference and runs at over 50 FPS
on a Pixel 2 phone. Our solution enables applications like
AR makeup, eye tracking and AR puppeteering that rely on
highly accurate landmarks for eye and lips regions. Our
main contribution is a uniﬁed network architecture that
achieves the same accuracy on facial landmarks as a multi-
stage cascaded approach, while being 30 percent faster.

1. Introduction

In this work, we address the problem of registering a de-
tailed 3D mesh template to a human face on an image. This
registered mesh can be used for the virtual try-on of lipstick
or puppeteering of virtual avatars where the accuracy of lip
and eye contours is critical to realism.

In contrast to methods that use a parametric model of
the human face [1], we directly predict the positions of face
mesh vertices in 3D. We base our architecture on earlier
efforts in this ﬁeld [5] that use a two stage architecture in-
volving a face detector followed by a landmark regression
network. However, using a single regression network for
the entire face leads to degraded quality in regions that are
perceptually more signiﬁcant (e.g. lips, eyes).

One possible way to alleviate this issue is a cascaded
approach: use the initial mesh prediction to produce tight
crops around these regions and pass them to specialized net-
works to produce higher quality landmarks. While this di-
rectly addresses the problem of accuracy, it introduces per-
formance issues, e.g. relatively large separate models that
use the original image as input, and additional synchroniza-
tion steps between the GPU and CPU that are very costly on
mobile phones. In this paper, we show that it is possible for
a single model to achieve the same quality as the cascaded
approach by employing region-speciﬁc heads that transform
the feature maps with spatial transformers [4], while being

Figure 1. Salient contours predicted by Attention Mesh
submodels

up to 30 percent faster during inference. We term this ar-
chitecture as attention mesh. An added beneﬁt is that it is
easier to train and distribute since it is internally consistent
compared to multiple disparate networks that are chained
together.

We use an architecture similar to one described in [7],
where the authors build a network that is robust to the ini-
tialization provided by different face detectors. Despite the
differing goals of the two papers, it is interesting to note that
both suggest that a combination of using spatial transform-
ers with heads corresponding to salient face regions pro-
duces marked improvements over a single large network.
We provide the details of our implementation for producing
landmarks corresponding to eyes, irises, and lips, as well as
quality and inference performance benchmarks.

2. Attention mesh

Model architecture The model accepts a 256 × 256 im-
age as input. This image is provided by either the face de-
tector or via tracking from a previous frame. After extract-

1

 
 
 
 
 
 
Figure 2. The inference pipeline and the model architecture overview

ing a 64 × 64 feature map, the model splits into several
sub-models (Figure 2). One submodel predicts all 478 face
mesh landmarks in 3D and deﬁnes crop bounds for each
region of interest. The remaining submodels predict region
landmarks from the corresponding 24×24 feature maps that
are obtained via the attention mechanism.

We concentrate on three facial regions with key contours:
the lips and two eyes (Figure 1). Each eye submodel pre-
dicts the iris as a separate output after reaching the spa-
tial resolution of 6 × 6. This allows the reuse of eye fea-
tures while keeping dynamic iris independent from the more
static eye landmarks.

Individual submodels allow us to control the network ca-
pacity dedicated to each region and boost quality where nec-
essary. To further improve the accuracy of the predictions,
we apply a set of normalizations to ensure that the eyes and
lips are aligned with the horizontal and are of uniform size.
We train the attention mesh network in two phases.
First, we employ ideal crops from the ground truth with
slight augmentations and train all submodels independently.
Then, we obtain crop locations from the model itself and
train again to adapt the region submodels to them.

Attention mechanism Several attention mechanisms
(soft and hard) have been developed for visual feature ex-
traction [2, 4]. These attention mechanisms sample a grid
of 2D points in feature space and extract the features un-
der the sampled points in a differentiable manner (e.g. using
2D Gaussian kernels or afﬁne transformations and differen-
tiable interpolations). This allows to train architectures end-
to-end and enrich the features that are used by the attention
mechanism. Speciﬁcally, we use a spatial transformer mod-

ule [4] to extract 24 × 24 region features from the 64 × 64
feature map. The spatial transformer is controlled by an
afﬁne transformation matrix θ (Equation 1) and allows us to
zoom, rotate, translate, and skew the sampled grid of points.

θ =

(cid:20) xx
shy

shx
sy

(cid:21)

tx
ty

(1)

This afﬁne transformation can be constructed either via
supervised prediction of matrix parameters, or by comput-
ing them from the output of the face mesh submodel.

Figure 3. Spatial transformer as the attention mechanism

Dataset Our dataset contains 30K in-the-wild mobile
camera photos taken with numerous camera sensors and in
varied conditions. We used manual annotation with special
emphasis on consistency for salient contours to obtain the
ground truth mesh vertex coordinates in 2D. The Z coordi-
nate was approximated using a synthetic model.

3. Results

To evaluate our uniﬁed approach, we compare it against
the cascaded model which consists of independently trained
region-speciﬁc models for the base mesh, eyes and lips that
are run in succession.

Performance Table 1 demonstrates that
the attention
mesh runs 25%+ faster than the cascade of separate face
and region models on a typical modern mobile device. The
performance has been measured using the TFLite GPU in-
ference engine [6]. An additional 5% speed-up is achieved
due to the reduction of costly CPU-GPU synchronizations,
since the whole attention mesh inference is performed in
one pass on the GPU.

Model
Mesh
Lips
Eye & iris
Cascade (sum of above)
Attention Mesh

Inference Time (ms)
8.82
4.18
4.70
22.4
16.6

Table 1. Performance on Pixel 2XL (ms)

Mesh quality A quantitative comparison of both models
is presented in Table 2. As the representative metric, we em-
ploy the mean distance between the predicted and ground
truth locations of a speciﬁc subset of the points, normalized
by 3D interocular distance (or the distance between the cor-
ners in the case of lips and eyes) for scale invariance. The
attention mesh model outperforms the cascade of models on
the eye regions and demonstrates comparable performance
on the lips region.

Model
Mesh
Cascade
Attention mesh

All
2.99
2.99
3.11

Lips Eyes
6.6
3.28
6.28
2.70
6.04
2.89

Table 2. Mean normalized error in 2D.

4. Applications

The performance of our model enables several real-time
AR applications like virtual try-on of makeup and pup-
peteering.

AR Makeup Accurate registration of the face mesh is
critical to applications like AR makeup where even small
errors in alignment can drive the rendered effect into the
”uncanny valley” [8]. We built a lipstick rendering solu-
tion (Figure 4) on top of our attention mesh model by using
the contours provided by the lip submodel. A/B testing on
10 images and 80 people showed that 46% of AR samples
were actually classiﬁed as real and 38% of real samples —
as AR.

Figure 4. Virtual makeup comparison: base mesh without
reﬁnements (left) vs. attention mesh with submodels (right)

Puppeteering Our model can also be used for virtual pup-
peteering and facial triggers. We built a small fully con-
nected model that predicts 10 blend shape coefﬁcients for
the mouth and 8 blend shape coefﬁcients for each eye. We
feed the output of the attention mesh submodels to this
blend shape network.
In order to handle differences be-
tween various human faces, we apply Laplacian mesh edit-
ing to morph a canonical mesh into the predicted mesh [3].
This lets us use the blend shape coefﬁcients for different hu-
man faces without additional ﬁne-tuning. We demonstrate
some results in Figure 5.

Figure 5. Puppeteering

5. Conclusion

We present a uniﬁed model that enables accurate face
mesh prediction in real-time. By using a differentiable
attention mechanism, we are able to devote computa-
tional resources to salient face regions without incurring
the performance penalty of running independent region-
speciﬁc models. Our model and demos will soon be avail-

able in MediaPipe (https://github.com/google/
mediapipe).

References

[1] Volker Blanz and Thomas Vetter. A morphable model for the
In Proceedings of 36th Internaional
synthesis of 3D faces.
Conference and Exhibition on Computer Graphics and Inter-
active Techniques, pages 187–194, 1999. 1

[2] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez
Rezende, and Daan Wierstra. Draw: A recurrent neural net-
work for image generation. arXiv preprint arXiv:1502.04623,
2015. 2

[3] Jianwei Hu, Ligang Liu, and Guozhao Wang. Dual laplacian
morphing for triangular meshes. Computer Animation and
Virtual Worlds, 18(45):271–277, 2007. 3

[4] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Advances in neural informa-
tion processing systems, pages 2017–2025, 2015. 1, 2

[5] Yury Kartynnik, Artsiom Ablavatski, Ivan Grishchenko, and
Matthias Grundmann. Real-time Facial Surface Geometry
arXiv preprint
from Monocular Video on Mobile GPUs.
arXiv:1502.04623, July 2019. 1

[6] Juhyun Lee, Nikolay Chirkov, Ekaterina Ignasheva, Yury Pis-
archyk, Mogan Shieh, Fabio Riccardi, Raman Sarokin, Andrei
Kulik, and Matthias Grundmann. On-device neural net in-
ference with mobile gpus. arXiv preprint arXiv:1907.01989,
2019. 3

[7] J. Lv, X. Shao, J. Xing, C. Cheng, and X. Zhou. A deep re-
gression architecture with two-stage re-initialization for high
performance facial landmark detection. In 2017 IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR),
pages 3691–3700, 2017. 1

[8] Junichiro Seyama and Ruth S. Nagayama. The uncanny val-
ley: Effect of realism on the impression of artiﬁcial human
faces. Presence: Teleoper. Virtual Environ., 16(4):337351,
Aug. 2007. 3

