Synaptic balance due to homeostatically self-organized

quasi-critical dynamics

Mauricio Girardi-Schappo∗,1 Ludmila Brochini,2 Ariadne A.

Costa,3 Tawan T. A. Carvalho,4 and Osame Kinouchi1, ∗

1Universidade de S˜ao Paulo, FFCLRP,

Departamento de F´ısica, Ribeir˜ao Preto, SP, 14040-901, Brazil

2Universidade de S˜ao Paulo, Instituto de Matem´atica

e Estat´ıstica, S˜ao Paulo, SP, 05508-090, Brazil

3Universidade Federal de Goi´as - Regional Jata´ı,

Unidade Acadˆemica Especial de Ciˆencias Exatas, Jata´ı, GO, 75801-615, Brazil

4Universidade Federal de Pernambuco,

Departamento de F´ısica, Recife, PE, 50670-901, Brazil

(Dated: February 24, 2020)

0
2
0
2

b
e
F
1
2

]

O
A
.
n
i
l
n
[

1
v
7
1
1
9
0
.
2
0
0
2
:
v
i
X
r
a

1

 
 
 
 
 
 
Abstract

Recent experiments suggested that homeostatic regulation of synaptic balance leads the

visual system to recover and maintain a regime of power-law avalanches. Here we study an

excitatory/inhibitory (E/I) mean-ﬁeld neuronal network that has a critical point with power-law

avalanches and synaptic balance. When short term depression in inhibitory synapses and ﬁring

threshold adaptation are added, the system hovers around the critical point. This homeostatically

self-organized quasi-critical (SOqC) dynamics generates E/I synaptic current cancellation in fast

time scales, causing ﬂuctuation-driven asynchronous-irregular (AI) ﬁring. We present the full

phase diagram of the model without adaptation varying external input versus synaptic coupling.

This system has a rich dynamical repertoire of spiking patterns:

synchronous regular (SR),

asynchronous regular (AR), synchronous irregular (SI), slow oscillations (SO) and AI. It also

presents dynamic balance of synaptic currents, since inhibitory currents try and compensate

excitatory currents over time, resulting in both of them scaling linearly with external input.

Our model thus uniﬁes two diﬀerent perspectives on cortical spontaneous activity: both critical

avalanches and ﬂuctuation-driven AI ﬁring arise from SOqC homeostatic adaptation, and are

indeed two sides of the same coin.

DOI: 10.1103/PhysRevResearch.2.012042

2

Experimental and theoretical evidence suggests that spontaneous cortical activity hap-

pens in the form of asynchronous irregular ﬁring patterns (AI). This could be generated

by the balance of excitatory/inhibitory (E/I) synaptic currents entering individual neurons

(see [1, 2]): inhibition has to nearly compensate excitation, such that cells remain near their

ﬁring threshold and ﬁre sporadically, generating a ﬂuctuation-driven regime [2]. These ﬁr-

ings may be organized in avalanches of action potentials that spread throughout the cortex.

Critical avalanches are known to enable the propagation of ﬂuctuations through local inter-

actions due to long-range spatiotemporal correlations [3], generating optimized processing

and functional features [4–8].

Two important issues remain: (i) how to self-organize a neuronal network close to a criti-

cal point, and (ii) could a network display an AI ﬁring pattern through this self-organization?

Concerning the ﬁrst point, it has been shown that simple local homeostatic mechanisms,

such as dynamical synapses [9–13] and dynamical neuronal gains [14–16], are suﬃcient to

drive networks towards the so-called Self-Organized quasi-Critical state (SOqC as deﬁned by

Bonachela & Mu˜noz [10, 17]). Particularly, our model requires two independent homeostatic

mechanisms to generate the SOqC dynamics: plasticity in the inhibitory synapses [18] and

adaptive ﬁring thresholds [19].

As for the second point, we will show that our homeostatic mechanisms for SOqC gen-

erate a near cancellation of excitatory/inhibitory (E/I) synaptic currents that produces a

ﬂuctuation-driven AI regime. Therefore, AI is a direct consequence of the hovering around

a critical point where the system displays quasi-critical power-law avalanches. Indeed, re-

cent experiments show homeostatic regulation of network activity close to a critical state

happening most probably through the adaptation of inhibitory synapses [20].

There have been attempts to model E/I networks in the context of criticality [21–24].

However, none of these models have shown that neuronal avalanches with the correct expo-

nents arise when E/I synaptic currents cancel each other. Also, none of these models show

that synaptic currents balance each other in the vicinity of a critical point. Not only the

SOqC dynamics proposed here does that, but it also generates activity where avalanches

and AI spiking coexist.

Without SOqC, we have a static system presenting the typical synchronicity states of

E/I networks exempliﬁed by Brunel’s model [25]: synchronous regular (SR), asynchronous

regular (AR), synchronous irregular (SI) and asynchronous irregular (AI). This system has

3

a directed percolation (DP) critical point with power-law avalanches, and dynamic balance

of E/I currents, since inhibitory inputs follow excitatory ones over time. Even though the

E/I neuron ratio is 80%:20% from cortical data [26], our model predicts that the ratio of

coupling strengths of inhibitory to excitatory synapses does not need to be 4:1 to achieve

the critical balanced state.

We ﬁrst deﬁne both the static and the adaptive versions of the model. Then, we make a

mean-ﬁeld calculation obtaining the critical exponents and phase diagrams, and discuss the

dynamic states of the static network. Finally, we add SOqC homeostatic adaptation and

observe the hovering around the critical balanced point that displays near cancellation of

E/I currents and ﬂuctuation-driven AI activity.

THE MODEL

We use discrete-time stochastic integrate-and-ﬁre neurons [14, 27, 28]. A Boolean variable

denotes if a neuron ﬁres (X[t] = 1) or not (X[t] = 0) at time t. The membrane potentials

of neurons in E and I populations evolve as:

(cid:34)

V E
i

[t + 1] =

µV E
i

[t] + I (E)

i

[t] +

1
N

NE(cid:88)

j=1

W EE

ij X E

j [t] −

1
N

(cid:34)

V I
i [t + 1] =

µV I

i [t] + I (I)

i

[t] +

1
N

NE(cid:88)

j=1

W IE

ij X E

j [t] −

1
N

NI(cid:88)

W EI

ij X I

j [t]

(cid:35)(cid:18)

(cid:19)

1 − X E

i [t]

j=1

NI(cid:88)

j=1

W II

ij X I

j [t]

(cid:35)(cid:18)

1 − X I

i [t]

,

,

(1)

(2)

(cid:19)

where N = NE + NI is the total number of neurons, µ is a leakage parameter and I (E)/(I)
are external inputs over E and I populations, respectively. The second index in W ab

i
ij , with
a, b ∈ {E, I}, refers always to the presynaptic neuron. All the W ’s are positive (inhibition is

[t]

explicitly given by the minus). The term (1 − Xi[t]) resets the voltage to zero after a spike,

resulting in one time step of refractoriness. Our network is fully connected with K = N − 1

neighbors.

The individual neurons ﬁre following a piece-wise linear probability function (see Fig. 1a):

P (X = 1|V ) ≡ Φ(V ) = (V − θ) Γ Θ(V − θ) Θ(VS − V ) + Θ(V − VS),

(3)

where Γ is the neuronal ﬁring gain, θ is a ﬁring threshold, VS = θ + 1/Γ is the saturation

potential and Θ(x) is the Heaviside function. The ﬁring probability Φ(V ) captures the

4

eﬀects of membrane noises, inducing stochastic spiking. The limit Γ → ∞ reduces to the

leaky integrate-and-ﬁre (LIF) neuron with hard threshold VS = θ.

ORDER AND CONTROL PARAMETERS

We assume that the synaptic weights have ﬁnite variance (are self-averaging), approximat-
(cid:11) (for all the a, b ∈ {E, I}). We also deﬁne the ﬁr-

ing them by their mean values W ab = (cid:10)W ab

ij

ing densities (the fraction of active sites) ρE[t] = 1/NE

j [t].
The fractions of excitatory and inhibitory neurons are p = NE/N and q = 1 − p = NI/N ,

j [t] and ρI[t] = 1/NI

(cid:80)

j X E

(cid:80)

j X I

respectively. Finally, we consider only the case with a stationary average external input

I = (cid:104)Ii[t](cid:105) with ﬁnite variance over both populations.

We introduce the synaptic balance parameter g by letting the synaptic weights obey

W EE = W IE = J, and W II = W EI = gJ (Brunel’s model A [25]). This is not a necessary

assumption, but it reduces Eqs. (1) and (2) to a single iterative map that is equal for both

E/I populations:

(cid:20)

(cid:21)(cid:18)

(cid:19)

Vi[t + 1] =

µVi[t] + I + pJρE[t] − qgJρI[t]

1 − Xi[t]

,

(4)

where we may omit the E/I superscripts. Letting the excitatory synaptic current be1

I E[t] = pJρE[t] and the inhibitory be2 I I[t] = −qgJρI[t], we deﬁne the average net synaptic

current,

∆I E/I = I E + I I = pJρE − qgJρI = W ρ ,

(5)

where we used ρE = ρI = ρ (from the constraints imposed on the synaptic weights) and

deﬁned W = (p − qg)J as our ﬁrst control parameter. This holds because, after a neuron

spikes, the voltage reset erases initial conditions and the voltages for both E/I populations

evolve following Eq. (4). The ﬁring density ρ is our order parameter, equivalent to the

network ﬁring frequency ν0 of Brunel’s model [25].

Consider the stationary state (1 − µ)V ∗ = I for ρ = 0 in Eq. (4). When V ∗ = θ, Φ(V ∗) =

0, so we have ρ > 0 for I > (1 − µ)θ. Thus, we deﬁne the external ﬁeld h = I − θ(1 − µ) as

the average suprathreshold external current. The h variable is our second control parameter.

The parameters (W, h) are usual for Statistical Physics. By introducing the external current

1 Not to be confused with external input over the excitatory population I (E)
2 Not to be confused with external input over the inhibitory population I (I)

i

[t] in Eq. (1).

[t] in Eq. (2).

i

5

FIG. 1. Firing rate function and phase transitions. a, Solid: soft ﬁring threshold (Γ = 1),

dashed: hard threshold (Γ → ∞). b, Order parameter vs. g (inset: vs. Y ), highlighting the

activity states high (H), low (L), intermediary (I, unstable, from a fold bifurcation) and quiescent

(Q ≡ ρ0 = 0). c, Order parameter ρ∗ vs. W for h = 0 (inset: vs. g for Y = 1); notice as the critical

point shifts away from Wc = 0 (gc = 4) as Γ decreases (Wc = 1/Γ). b and c, Dot-dashed lines

are marginally stable cycle-2 attractors (SR state). Dotted lines in the ρ+(L) branch are cyclic

attractors of the network (quasi-cycle-2 SI states). Density ρ is given by Eqs. (11) and (14). d,

Phase diagram in the balanced notation (g, Y ) plane for the hard threshold neurons. The critical

point lies at (Wc = 0, hc = 0) or (gc = 4, Yc = 1) [bullet, Eq. (13)]. The Q phase loses stability at

the horizontal dashed line Yc = 1 − µ (or hc = 0); µ = 0 (black and thin dashed line) and µ = 0.2

(blue dashed line). This diagram should be compared to Fig. 1A of Ref. [25].

6

0246800.20.40.60.81*02400.10.20.3-3-2-101200.20.40.60.81*1234500.510123010246800.511.52a)b)c)d)ratio, Y = I/θ, we may switch from (W, h) to describe the system in the balanced notation

(g, Y ) by using g = p/q − W/(qJ) and Y = (h/θ) + 1 − µ.

HOMEOSTATIC MECHANISMS

To obtain a quasi-critical balanced state without ﬁne tuning, we introduce two inde-

pendent homeostatic biological mechanisms: inhibition depression [20] and ﬁring threshold

adaptation [29]. We use the Levina-Hermann-Geisel short-term plasticity for the synaptic

weights [9]:

W II/EI
ij

[t + 1] = W II/EI

ij

[t] +

1
τW

(cid:16)
A − W II/EI

ij

[t]

(cid:17)

− uW W II/EI

ij

[t]X I

j [t] ,

(6)

where τW is a (large) recovery time, A is the synaptic baseline and uW is the fraction of

the synaptic strength depressed when a presynaptic neuron ﬁres. This dynamic generates
(cid:68)

(cid:69)

homeostatic tuning because g is then g[t] =

/J in Eq. (4). The (cid:104).(cid:105) bracket is

W EI/II
ij

[t]

an average over neurons i and j.

To self-organize towards zero-ﬁeld hc = I − (1 − µ) θ = 0 or Yc = I/θ = 1 − µ, we add

threshold adaptation:

1
τθ
where the parameter uθ is the fractional increase in the neuron threshold after it ﬁres, and

θi[t] + uθθi[t]Xi[t] ,

θi[t + 1] = θi[t] −

(7)

τθ is a recovery time scale. This dynamic is inspired by the biological mechanism of ﬁring

rate adaptation [29]. It enters the model through Eq. (3), changing θ to θ[t] = (cid:104)θi[t](cid:105).

MEAN-FIELD CALCULATIONS

We consider only the µ = 0, since µ > 0 does not present any new phenomenol-

ogy (although it admits numerical solutions and analytic approximations close the critical

point [14, 15]). For this case, the stationary voltage distribution has only two delta peaks,

Pt(V ) = ρ[t]δ (V ) + (1 − ρ[t]) δ(V − V [t]), and the number of active sites is the average of

Φ(V ) over V [14, 16],

(cid:90)

ρ[t + 1] =

Φ(V ) Pt(V ) dV ,

with V [t] given by Eq. (4), resulting in:

ρ[t + 1] = (1 − ρ[t]) Γ (W ρ[t] + h) Θ(W ρ[t] + h) .

7

(8)

(9)

This map has, in principle, three ﬁxed points. For h ≤ 0, there is a quiescent solution ρ0 = 0

(also called the Q state) since the Heaviside Θ(x) function is zero in the right hand side in

Eq. (9).

The active states are the two other ﬁxed points of the ﬁring density Eq. (9), given by:

ΓW ρ2 + (1 + Γh − ΓW ) ρ − Γh = 0 ,

with solutions:

ρ± =

ΓW − Γh − 1
2ΓW

±

(cid:112)(ΓW − Γh − 1)2 + 4Γ2W h
2ΓW

.

(10)

(11)

For h > 0 (Y > 1), there is a single solution ρ+ (corresponding to high activity H and low

activity L) because ρ− < 0. For h < 0 (Y < 1), we have a positive but unstable branch ρ−

(the intermediary solution I) that separates the stable branch ρ+ (H) from the absorbing

state ρ0 (Q), see Fig. 1b. When h = 0 (Y = 1), the unstable branch vanishes into a critical

point with W = Wc = 1/Γ [g = gc, Eq. (13)].

CRITICAL EXPONENTS

For zero-ﬁeld, Eq. (10) yields ρ0 = 0 (the absorbing quiescent phase, Q), stable for

W < Wc ≡ 1/Γ and an active state:

ρ∗ =

ΓW − 1
ΓW

=

W − Wc
W

∼ (W − Wc)β ,

(12)

with β = 1, stable for W > Wc = 1/Γ. The ﬁeld exponent is obtained by isolating h from

Eq. (10) and expanding for small ρ (due to small external h) with W = Wc, resulting in
ρ∗ ∼ (h/Wc)1/δh with δh = 2. The exponent of the susceptibility, χ = ∂ρ/∂h ∼ |W − Wc|−γ,

using Γ = 1/Wc, is γ = 1.

These exponents pertain to the mean-ﬁeld directed percolation (DP) universality class [30–

32], the framework that has been proposed to describe neuronal avalanches [33, 34]. The
variance of the network activity is Var(ρ) ∼ |W − Wc|−γ(cid:48) with γ(cid:48) = 0 [30]. This explains the

jump in the coeﬃcient of variation of ρ observed by Brunel [25].

In the balanced notation, h = hc = 0 is the same as Yc = (hc/θ) + 1 − µ = 1 (recalling

that µ = 0 and θ = 1). The equivalent of Wc = 1/Γ is given by

gc =

p
q

−

Wc
qJ

= 4 −

5
ΓJ

,

8

(13)

where the usual cortical estimates p = 80% and q = 20% were used [26]. This generalizes the

usual condition gc ≈ 4: if neurons have a soft threshold (ﬁnite Γ) or the synapses are weak

(ﬁnite J), the critical balance point shifts towards lower values of g (Fig. 1c). The phase

diagram for large ΓJ (i.e. hard threshold LIF neurons) is shown in Fig. 1d, and matches

exactly the one obtained by Brunel [25].

SYNAPTIC CURRENTS OF THE STATIC MODEL

We can write Eq. (11) in the balanced notation by letting h = (Y −1)θ and W = (p−qg)J

[see Fig. 1b]:

ρ± =

1
2

+

ρ1
2Γθ(Y − 1)

+

ρ1
2

±

(cid:115)(cid:18) 1
2

+

ρ1
2Γθ(Y − 1)

+

ρ1
2

(cid:19)2

− ρ1 ,

(14)

where ρ1 = (Y − 1)θ/[Jq(g − p/q)] is the ﬁrst order expansion of Eq. (14).

The synaptic currents are balanced if the net synaptic current from Eq. (4) is zero,

∆I E/I = W ρ = 0, such that either W = (p − qg)J = 0 (i.e., g = gbal = p/q for Y > 1), or

ρ = 0 (i.e., the quiescent solution of the subcritical and critical states, g ≥ gc and Y ≤ 1).

For g (cid:54)= p/q, the synaptic currents scale linearly with the external input. We can see that

by expanding Eq. (14) for small Y , giving ρ ≈ ρ1:

I E = pJρ1 =

p/q
g − p/q

(Y − 1)

I I = −qgJρ1 = −

g
g − p/q

(Y − 1) .

(15)

(16)

The variable ρ ≈ ρ1 = I E/(pJ) is shown in the inset of Fig. 1b. These currents saturate for

large enough ΓJ. This linear scaling highlights the dynamic balance of synaptic inputs, as

inhibition tracks excitation over time [25, 35].

PHASE DIAGRAM

The soft threshold neurons’ phase diagram is shown in Fig. 2a. The curves are bifurcations

of the stable ﬁxed point ρ+ in Eq. (14): (i) a fold bifurcation – i.e., a ﬁrst order phase

transition for Y < 1 that ends in the critical point (gc, Yc). (ii) a bifurcation to cycle-2 that

separates SR from AR when ρ+ = 1/2, because the refractory period does not allow a stable

9

FIG. 2. Avalanches and ﬁring patterns. a, Phase diagram for µ = 0, and ΓJ = 10; a critical

line starts at gc = 3.5, see Eq. (13), for Y = 1. The critical point, the subcritical region with

g > gFold; Y ≤ 1, and the supercritical region g = 4; Y > 1 have balanced synaptic currents,

such that the net current is ∆I E/I = I E + I I ≈ 0. At Y = 1.2, from left to right: SR/cycle-2

(g = 3), AR/High (g = 3.5), AI/Low (g = 4.3), and SI/fast oscillations (g = 4.7). SR and AR are

separated by a bifurcation tho cycle-2 due to the refractory period; SI and AI are separated by a

ﬂip bifurcation. b, Distribution of avalanche sizes (main plot, τ = 1.5) and duration (bottom inset,

τt = 2) at the critical point; Top inset: size and duration scaling law (cid:104)s(cid:105) ∼ T a has a crossover with

a = 2.5 for small avalanches (a ﬁnite-size eﬀect) and a = 2 for the rest of the data. c, Network

simulation results (N = 106 neurons), ρ[t], for the points in panel a. From the top left to the

bottom right panel: critical point absorbing-state avalanches (peaks); SR, AR, SO (slow waves for

Y (cid:38) Yc = 1 − µ, µ = 0.9, Y = 0.101), SI, and AI. The background shows the raster plot of 1,000

randomly selected neurons.

10

234560.811.21.62AR50052056058000.5SR00.5AI50052056058000.5SI00.5SO00.05020040060080010001200140000.0102468-10-5 0  10010210-610-3100100102100105a)b)c)ﬁxed point with ρ+ > 1/2, generating bursts of synchronized activity with period 2 ms. (iii)

a ﬂip bifurcation at gFlip = p/q + 1/(qΓJ) that separates the uniform AI from the oscillatory

SI in the low activity regime. (iv) the line Yc = 1 is a continuous transcritical bifurcation

for g > gc and g < gFlip; and a synchronization phase transition for g > gFlip (Fig. 1c, inset).

The critical balanced point at (gc, Yc) displays power-law distributed avalanches with

exponents τ = 1.5 and τt = 2 for size and duration, respectively, see Fig. 2b. The avalanches

also respect the scaling law 1/(σνz) = (τt − 1)/(τ − 1) (inset in Fig. 2b), as expected for the

DP universality class [30, 36, 37], and observed in experiments [38].

The simulated network activity in all the six dynamical regimes is shown in Fig. 2c. The

critical point (gc = 3.5, Y = 1) displays avalanches sparked by a vanishing external stimulus.

The self-sustained activity regime (g < gc), when summed up to an external current Y > 1,

generates the regular microscopic behaviors, SR or AR. The SR state is a marginally stable

cycle-2 of the ﬁring density and the AR is a state of high and homogeneous activity [the

ρ+ in Eq. (11)]. The addition of an external current to the inhibition dominated quiescent

regime results in the low irregular activity AI (and SI if g > gFlip). Slow oscillations (SO)
are observed when Y (cid:38) Yc = 1 − µ and g ≥ gc for µ ≥ 0.

HOMEOSTATIC SOQC DYNAMICS

The dynamics in the inhibitory weights tunes the system along the g axis of the phase

diagram. Threshold adaptation regulates the system along the Y axis. Both mechanisms

contribute to self-organize the network towards the critical point. For the parameters consid-

ered in Fig. 2a, the critical point is gc = 3.5 and Yc = 1, and the two independent dynamics
yield ¯g = (cid:104)gij[t](cid:105) = 3.59(7) and ¯Y = (cid:104)Yi[t](cid:105) = 1.02(2) (Fig. 3a).

This homeostatic tuning, however, is not perfect, since stochastic oscillations make the

system hover around the critical point – a distinctive feature of Self-Organized quasi-

Criticality or SOqC [10, 17] – see Fig. 3b inset. This oscillation is triggered by ﬁnite-size

(demographic) noise and its amplitude decreases with increasing N (inset in the bottom

panel of Fig. 3a and bottom inset of Fig. 4). Thus, the larger the network, the closer the

system gets to the critical point [16].

The spiking pattern of the SOqC dynamics is very similar to standard AI activity (com-

pare Fig. 3b with Fig2c). This happens because the E/I synaptic currents (deﬁned in Eq. (5))

11

FIG. 3. Self-organization towards the balanced critical point. Parameters: τW = τθ = 100,

A = 73.5, uW = uθ = 0.1, Γ = 1 and J = 10. a, Time series for ρ[t], g[t] = W II/EI [t]/J,
E/I

= 0.08(7). I E and I I have been displaced by

Y [t] = I/θ[t], and the synaptic currents with ∆I

their means. The amplitude of I E and I I are one order of magnitude larger than ∆I E/I for all N

(bottom inset). b, Detail of the SOqC ρ[t] dynamics with a raster plot of 1,000 randomly selected

neurons displaying AI-like activity. b (inset), Self-organization trajectories in the g vs. Y plane.

The system hovers around the critical balanced point of the static model, gc = 3.5 (¯g = 3.59(7))
and Yc = 1 ( ¯Y = 1.02(2)), which displays power-law avalanches.

12

1.00011.0051.011.0150.20.4   0.81  33.540.911.150005020504050605080-1015000520054005600580011.1500050505100515011.13.5450005050510051503.43.601500050505100515000.10.210410610-1100Amplitudeb)a)cancel each other in fast time scales, generating a net current ∆I E/I that is always one order

of magnitude smaller than either I E or I I (bottom panel in Fig. 3).

Contrary to the static version of the model, increasing the external input I on the home-

ostatic system slightly decreases the average net current, but increases the ﬂuctuations of

∆I E/I (see Fig. 4): the network gets more balanced and more noisy at the same time. On

the other hand, independently of I, the ﬂuctuations of the net synaptic current decreases

with N due to ﬁnite-size eﬀects.

The nearly total cancellation of E/I currents generates sporadic ﬂuctuations of activity

the spread throughout the network (the avalanches) in an AI fashion. These avalanches

should converge to nearly perfect power-law distributions for large enough τW = τθ [15].

Such stochastic oscillations should have low amplitude [16], but rare large events (dragon-

kings) also occur [15]. Although the demographic noise vanishes in the thermodynamic limit,

other sources of biological noise (not included in the model and that does not vanishes for

large N ) will continue to trigger the stochastic oscillations and the AI behavior.

DISCUSSION

In contrast to our model, Brunel [25] used a random network, deterministic LIF neurons,

noisy inputs and a distribution of delays in the synapses. In our model, noise is captured

by the intrinsic stochasticity of the neurons. Our model does not have a distribution of

synaptic delays, but its discrete time step implies that spikes are transmitted with a ﬁxed

delay of 1 ms. Also, since Φ(0) = 0, the reset of voltages after spiking implements a

refractory period of 1 ms. The other ingredients do not seem to be crucial to obtain either

the synchronicity/activity states or the critical balanced point.

Our mean-ﬁeld calculation is valid for fully connected networks where the number of

neighbors is K = N − 1. When there is no threshold θ nor external current I, the condition

W = (p − qg)J allows our model to be directly mapped on the Kinouchi et al. [16] model.

In turn, the authors showed that the latter model presents exactly the same dynamics as

the sparse random network of probabilistic cellular automata where K = O(1), both in

the static version [5, 39] and in the homeostatic version [12]. All these models share the

mean-ﬁeld DP results obtained here [14]. Calculations for the case K = O(

N ), as studied

√

in [35, 40], should be done to check the performance of the homeostatic mechanisms.

13

FIG. 4. Synaptic balance in SOqC. Net synaptic current ∆I

E/I

as function of external input

I. The mean net synaptic current decreases with increasing I, making the network more balanced.

Insets: Amplitude of the ﬂuctuations of ∆I E/I for diﬀerent intensities of the external input (top)

and for increasing network size (bottom). The decrease of the amplitude with increasing N shows

that the ﬂuctuations in ∆I E/I are due to ﬁnite-size eﬀects.

Heavy-tailed synaptic distributions are also expected to generate a critical point for

threshold neurons [41]. Our mean-ﬁeld calculations do not apply directly in this context,

but our homeostatic mechanisms could still be employed to synaptic weights and thresholds

to check whether the critical point would also become an attractor of that model.

While inhibition frequently increases together with excitation after the stimulation of a

neuron, the reverse does not seem to happen; that is, excitation does not compensate for

inhibition when the neuron is suppressed [1, 18, 20]. This suggests a self-organizing home-

ostatic mechanism regulating the inhibitory synapses, which was suggested to be necessary

to re-establish power-law neuronal avalanches in rats [20].

This fact motivated the addition of adaptation to our model. We showed that two

homeostatic mechanisms are suﬃcient to take the network towards any critical balance point.

Adding homeostasis, we avoided ﬁne tuning of the g and Y parameters towards gc and Yc.

However, that comes at the cost of introducing ﬁve new parameters (A, τW , uW , τθ, uθ) that

perhaps should also be ﬁne tuned. This is not the case: the dependence on these parameters

is weak, representing a kind of gross tuning [12, 15]. Also, if necessary, metaplasticity in

longer time scales can be employed to tune these homeostatic parameters [11].

14

123458.388.4 8.428.448.468.4812345123Amplitude (10-1)105106123Amplitude (10-1)CONCLUDING REMARKS

Homeostatic adaptation in synapses and ﬁring thresholds are suﬃcient mechanisms to

self-organize a neuronal network towards its DP critical (and synaptically balanced) point.

The hovering around this attractor is due to small ﬂuctuations in the net synaptic current,

such that there is always some residual excitation driving the network activity (a sort of

ﬂuctuation-driven AI regime due to SOqC). The underlying critical point shows power-law

avalanches with exponents compatible with in vitro experiments [38]. Our model thus uniﬁes

two diﬀerent perspectives on the spontaneous activity of the brain: power-law neuronal

avalanches and ﬂuctuation driven asynchronous-irregular ﬁring patterns are indeed two sides

of the same coin.

We thank A. C. Roque, M. Copelli and J. Stolﬁ for discussions. This article was pro-

duced as part of the FAPESP Research, Innovation and Dissemination Center for Neuro-

mathematics (grant #2013/07699-0, S. Paulo Research Foundation). L.B. thanks FAPESP

(grant #2016/24676-1), A.A.C. thanks FAPESP (grants #2016/00430-3 and #2016/20945-

8) and M.G.-S. thanks FAPESP (grant #2018/09150-9). O.K. thanks the Center for Natu-

ral and Artiﬁcial Information Processing Systems (CNAIPS)-USP and FAPESP BPE grant

#2019/12746-3. The present work was also realized with the support of CNPq, Conselho

Nacional de Desenvolvimento Cient´ıﬁco e Tecnol´ogico, Brazil.

∗ osame@ﬀclrp.usp.br; girardi.s@gmail.com; These authors contributed equally to this work.

[1] Sophie Den`eve and Christian K Machens. Eﬃcient codes and balanced networks. Nat. Neu-

rosci., 19:375–382, 2016.

[2] Yashar Ahmadian and Kenneth D. Miller. What is the dynamical regime of cerebral cortex?

arXiv:1908.10101v2 [q-bio.NC], 2019.

[3] G´eza ´Odor. Universality classes in nonequilibrium lattice systems. Rev. Mod. Phys., 76(3):663–

724, 2004.

[4] Clayton Haldeman and John M Beggs. Critical branching captures activity in living neural

networks and maximizes the number of metastable states. Phys. Rev. Lett., 94(5):058101,

2005.

15

[5] Osame Kinouchi and Mauro Copelli. Optimal dynamical range of excitable networks at criti-

cality. Nat. Phys., 2(5):348–351, 2006.

[6] Woodrow L Shew, Hongdian Yang, Thomas Petermann, Rajarshi Roy, and Dietmar Plenz.

Neuronal avalanches imply maximum dynamic range in cortical networks at criticality. J.

Neurosci., 29(49):15595–15600, 2009.

[7] Mauricio Girardi-Schappo, Germano S Bortolotto, Jheniﬀer J Gonsalves, Leonel T Pinto,

and Marcelo H R Tragtenberg. Griﬃths phase and long-range correlations in a biologically

motivated visual cortex model. Sci. Rep., 6:29561, 2016.

[8] Thiago S Mosqueiro and Leonardo P Maia. Optimal channel eﬃciency in a sensory network.

Phys. Rev. E, 88(1):012712, 2013.

[9] Anna Levina, J Michael Herrmann, and Theo Geisel. Dynamical synapses causing self-

organized criticality in neural networks. Nat. Phys., 3(12):857–860, 2007.

[10] Juan A Bonachela, Sebastiano de Franciscis, Joaqu´ın J Torres, and Miguel A Mu˜noz. Self-

organization without conservation: are neuronal avalanches generically critical?

J. Stat.

Mech., 2010(02):P02015, 2010.

[11] Jiayi Peng and John M Beggs. Attaining and maintaining criticality in a neuronal network

model. Physica A: Statistical Mechanics and its Applications, 392(7):1611–1620, 2013.

[12] Ariadne A Costa, Mauro Copelli, and Osame Kinouchi. Can dynamical synapses produce true

self-organized criticality? J. Stat. Mech., 2015(6):P06004, 2015.

[13] Jo˜ao G F Campos, Ariadne A Costa, M Copelli, and O Kinouchi. Correlations induced by

depressing synapses in critically self-organized networks with quenched dynamics. Phys. Rev.

E, 95:042303, Apr 2017.

[14] Ludmila Brochini, Ariadne A Costa, Miguel Abadi, Antˆonio C Roque, Jorge Stolﬁ, and Osame

Kinouchi. Phase transitions and self-organized criticality in networks of stochastic spiking

neurons. Sci. Rep., 6:35831, 2016.

[15] Ariadne A Costa, Ludmila Brochini, and Osame Kinouchi. Self-organized supercriticality and

oscillations in networks of stochastic spiking neurons. Entropy, 19(8):399, 2017.

[16] Osame Kinouchi, Ludmila Brochini, Ariadne A Costa, Jo˜ao G F Campos, and Mauro Copelli.

Stochastic oscillations and dragon king avalanches in self-organized quasi-critical systems. Sci.

Rep., 9:3874, 2019.

16

[17] Juan A Bonachela and Miguel A Mu˜noz. Self-organization without conservation: true or just

apparent scale-invariance? J. Stat. Mech., 2009(09):P09009, 2009.

[18] N. A. Lambert and W. A. Wilson. Temporally distinct mechanisms of use-dependent depres-

sion at inhibitory synapses in the rat hippocampus in vitro. J Neurophysiol., 72(1):121–130,

1994.

[19] Colin W. G. Cliﬀord, Michael A. Webster, Garrett B. Stanley, Alan A. Stocker, Adam Kohn,

Tatyana O. Sharpee, and Odelia Schwartz. Visual adaptation: Neural, psychological and

computational aspects. Vision Research, 47(25):3125–3131, 2007.

[20] Zhengyu Ma, Gina G. Turrigiano, Ralf Wessel, and Keith B. Hengen. Cortical circuit dynamics

are homeostatically tuned to criticality in vivo. Neuron, 104(4):655–664.E4, 2019.

[21] Simon-Shlomo Poil, Richard Hardstone, Huibert D Mansvelder, and Klaus Linkenkaer-Hansen.

Critical-state dynamics of avalanches and oscillations jointly emerge from balanced excita-

tion/inhibition in neuronal networks. J. Neurosci., 32(29):9817–9823, 2012.

[22] F. Lombardi, H. J. Herrmann, C. Perrone-Capano, D. Plenz, and L. de Arcangelis. Balance

between excitation and inhibition controls the temporal organization of neuronal avalanches.

Phys. Rev. Lett., 108:228703, 2012.

[23] F. Lombardi, H. J. Herrmann, and L. de Arcangelis. Balance of excitation and inhibition

determines 1/f power spectrum in neuronal networks. Chaos, 27:047402, 2017.

[24] Leonardo Dalla Porta and Mauro Copelli. Modeling neuronal avalanches and long-range tem-

poral correlations at the emergence of collective oscillations: Continuously varying exponents

mimic M/EEG results. PLoS Comput. Biol., 15:e1006924, 2019.

[25] Nicolas Brunel. Dynamics of sparsely connected networks of excitatory and inhibitory spiking

neurons. J. Comput. Neurosci., 8(3):183–208, 2000.

[26] Peter Somogyi, G´abor Tam´as, Rafael Lujan, and Eberhard H. Buhl. Salient features of

synaptic organisation in the cerebral cortex. Brain Res. Rev., 26:113–135, 1998.

[27] Wulfram Gerstner and J Leo van Hemmen. Associative memory in a network of spiking

neurons. Network: Comput. Neural Syst., 3(2):139–164, 1992.

[28] Antonio Galves and Eva L¨ocherbach. Inﬁnite systems of interacting chains with memory of

variable length a stochastic model for biological neural nets. J. Stat. Phys., 151(5):896–921,

2013.

17

[29] Jan Benda and Andreas V. M. Herz. A universal model for spike-frequency adaptation. Neural

Comput., 15:2523–2564, 2003.

[30] Miguel A. Mu˜noz, Ronald Dickman, Alessandro Vespignani, and Stefano Zapperi. Avalanche

and spreading exponents in systems with absorbing states. Phys. Rev. E, 59(5):6175, 1999.

[31] Sven L¨ubeck. Universal scaling behavior of non-equilibrium phase transitions. International

Journal of Modern Physics B, 18(31n32):3977–4118, 2004.

[32] Mauricio Girardi-Schappo and M. H. R. Tragtenberg. Comment on “Convergence towards

asymptotic state in 1-D mappings: A scaling investigation”. Phys. Lett. A, 383(36):126031,

2019.

[33] Dante R Chialvo. Emergent complex neural dynamics. Nat. Phys., 6(10):744–750, 2010.

[34] Miguel A Mu˜noz. Colloquium: Criticality and dynamical scaling in living systems. Rev. Mod.

Phys., 90(3):031001, 2018.

[35] Carl van Vreeswijk and Haim Sompolinsky. Chaos in neuronal networks with balanced exci-

tatory and inhibitory activity. Science, 274(5293):1724–1726, 1996.

[36] Germano S. Bortolotto, Mauricio Girardi-Schappo, Jheniﬀer J. Gonsalves, Leonel T. Pinto,

and Marcelo H. R. Tragtenberg. Information processing occurs via critical avalanches in a

model of the primary visual cortex. J. Phys. Conf. Ser., 686(1):012008, 2016.

[37] Mauricio Girardi-Schappo and Marcelo H. R. Tragtenberg. Measuring neuronal avalanches in

disordered systems with absorbing states. Phys. Rev. E, 97:042415, 2018.

[38] John M Beggs and Dietmar Plenz. Neuronal avalanches in neocortical circuits. J. Neurosci.,

23(35):11167–11177, 2003.

[39] Chong-Yang Wang, Zhi-Xi Wu, and Michael Z Q Chen. Approximate-master-equation ap-

proach for the Kinouchi-Copelli neural model on networks. Phys. Rev. E, 95(1):012310, 2017.

[40] Alfonso Renart, Jaime de la Rocha, Peter Bartho, Liad Hollender, N´estor Parga, Alex Reyes,

, and Kenneth D. Harris. The asynchronous state in cortical circuits. Science, 327:587–590,

2010.

[41]

(cid:32)Lukasz Ku´smierz, Shun Ogawa, and Taro Toyoizumi. Edge of chaos and scale-free avalanches

in neural networks with heavy-tailed synaptic disorder. arXiv:1910.05780 [physics.bio-ph],

2019.

18

