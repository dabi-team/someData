Massive MIMO Channel Prediction:
Kalman Filtering vs. Machine Learning

Hwanjin Kim, Sucheol Kim, Hyeongtaek Lee, Chulhee Jang, Yongyun Choi, and Junil Choi

1

0
2
0
2

p
e
S
1
2

]
T
I
.
s
c
[

1
v
7
6
9
9
0
.
9
0
0
2
:
v
i
X
r
a

Abstract—This paper focuses on channel prediction techniques
for massive multiple-input multiple-output (MIMO) systems.
Previous channel predictors are based on theoretical channel
models, which would be deviated from realistic channels. In this
paper, we develop and compare a vector Kalman ﬁlter (VKF)-
based channel predictor and a machine learning (ML)-based
channel predictor using the realistic channels from the spatial
channel model (SCM), which has been adopted in the 3GPP
standard for years. First, we propose a low-complexity mobility
estimator based on the spatial average using a large number of
antennas in massive MIMO. The mobility estimate can be used
to determine the complexity order of developed predictors. The
VKF-based channel predictor developed in this paper exploits
the autoregressive (AR) parameters estimated from the SCM
channels based on the Yule-Walker equations. Then, the ML-
based channel predictor using the linear minimum mean square
error (LMMSE)-based noise pre-processed data is developed.
Numerical results reveal
that both channel predictors have
substantial gain over the outdated channel in terms of the channel
prediction accuracy and data rate. The ML-based predictor
has larger overall computational complexity than the VKF-
based predictor, but once trained, the operational complexity
of ML-based predictor becomes smaller than that of VKF-based
predictor.

Index Terms—Massive MIMO, mobility estimation, channel
prediction, autoregressive model, vector Kalman ﬁlter, machine
learning.

I. INTRODUCTION

A CCURATE channel state information (CSI) at base sta-

tions (BSs) is essential to fully exploit massive multiple-
input multiple-output (MIMO) systems, which are one of the
key techniques for 5G and beyond wireless communication
systems [1]. Wireless channels vary in time due to the mobility
of user equipment (UE) in practice [2], and the CSI at the
BS could be outdated, resulting in signiﬁcant performance
degradation in massive MIMO [3]. The best way to resolve
the outdated CSI problem without additional channel training
overhead is to predict future channels based on past CSI [4],
[5].

This work was partly supported by the Network Business, Samsung Elec-
tronics Co., LTD, by the MSIT(Ministry of Science and ICT), Korea, under the
ITRC(Information Technology Research Center) support program(IITP-2020-
0-01787) supervised by the IITP(Institute of Information & Communications
Technology Planning & Evaluation), and by IITP grant funded by the Ko-
rea government(MSIT) (No. 2016-0-00123, Development of Integer-Forcing
MIMO Transceivers for 5G & Beyond Mobile Communication Systems).

H. Kim, S. Kim, H. Lee, and J. Choi are with the School of Electrical
Engineering, Korea Advanced Institute of Science and Technology, Daejeon
34141, South Korea (e-mail: jin0903@kaist.ac.kr;
loehcusmik@kaist.ac.kr;
htlee8459@kaist.ac.kr; junil@kaist.ac.kr).

C.

Jang and Y. Choi are with the Network Business, Samsung
chulhee.jang@samsumg.com;

(e-mail:

LTD
Electronics
yongyun.choi@samsung.com).

Co.,

The conventional way of channel estimation and prediction
in massive MIMO usually relies on the Wiener ﬁltering or
Kalman ﬁltering assuming model-based analytical channels
[3], [6]–[16]. A simple ﬁrst-order Gauss-Markov process
channel was considered in [12] while more complex autore-
gressive (AR) or autoregressive moving average (ARMA)
models, which are linear stochastic models describing cor-
related random processes [13], were taken into account in
[14]–[16]. Although effective, these approaches are based on
simple analytical models for long-term channel statistics, e.g.,
rectangular power spectrum to represent the temporal variation
of channels in [15], which may not hold in practice.

Recently, data-driven machine learning (ML)-based channel
estimation and prediction methods were proposed for massive
MIMO systems in [17]–[22]. The ML-based approaches can
discover inherent linear or nonlinear channel characteristics
from sufﬁcient amount of channel data without assuming
any prior knowledge of channel model. A deep convolu-
tional neural network (CNN)-based massive MIMO channel
estimator using spectral correlation was proposed in [20],
and image super resolution and image restoration networks
were exploited in [21] to estimate communication channels
considering the channel as two-dimensional images. For the
channel prediction, a CNN-AR based channel predictor by
leveraging an auto-correlation function pattern was developed
in [22]. Most of previous ML-based techniques, however,
assumed perfect, noiseless channel data during the training
phase, which is impractical.

Both model-based and ML-based channel prediction tech-
niques may suffer from high complexity. It is possible for
the BS to predict the channel with minimal complexity once
the BS has the knowledge of UE mobility, which determines
how fast the channel varies in time. For the UE mobility
estimation, maximum likelihood, power spectrum density, and
channel covariance were respectively exploited in [23], [24],
and [25]. Similar to the channel estimation and prediction,
these mobility estimators are based on theoretical channel
models, e.g., the Rician fading, which may not be able to
accurately represent realistic channels. Moreover, the previous
mobility estimators require a large number of time samples to
obtain mobility estimates, which hinders their practicality.

Different from previous approaches, we consider the spatial
channel model (SCM) [26], which has been adopted in the
3GPP standard for years, to consider realistic wireless channel
environments in this paper. We ﬁrst propose a mobility esti-
mator, dubbed as the spatial average of temporal correlation
(SATC)-based mobility estimator, using only a few numbers of
time samples. The estimated mobility can be used to balance

 
 
 
 
 
 
2

the complexity and accuracy of channel predictors. Then, we
develop and compare the vector Kalman ﬁlter (VKF)-based
predictor and ML-based predictor for time-varying massive
MIMO channels. In the VKF-based prediction, we estimate
the vector AR parameters by the Yule-Walker equations [13]
using the sampled auto-correlation matrix and predict
the
time-varying channels with the VKF. In the ML-based pre-
diction, we use the multi-layer perceptron (MLP) after linear
minimum mean square error (LMMSE)-based pre-processing
noisy received signals. The numerical results show that the
prediction accuracy of VKF-based and ML-based predictors
is comparable with respect to the time slot and number of
samples. We also compare the complexity of VKF-based and
ML-based predictors. It turns out that the total complexity of
ML-based predictor is much higher than that of VKF-based
predictor. After trained, however, it becomes the opposite,
i.e., the ML-based predictor becomes far less complex than
the VKF-based predictor. It is also possible for the BS to
have more advanced processors, e.g., neural processing units
(NPUs), which could even facilitate the effectiveness of ML-
based predictors.1 We believe our ﬁndings in this paper can
guide system operators to select a proper channel predictor
depending on their operational environment.

The rest of paper is organized as follows. In Section II,
we explain a system model including the SCM and a general
framework of the channel prediction. In Section III, we imple-
ment the low-complexity mobility estimator. Then, we develop
the VKF-based predictor with the AR parameter estimation
in Section IV and explain the ML-based predictor using pre-
processed received signals in Section V. After analyzing the
complexity and verifying the numerical results in Section VI,
we conclude the paper in Section VII.

·

Notation: Lower case and upper case bold letters represent
column vectors and matrices. AT, AH, A†, and a denote the
transpose, conjugate transpose, pseudo inverse, and column-
wise vectorization of matrix A. E[
] represents the expectation,
), Im(
) denote the real part and imaginary part of
and Re(
·
·
) denotes the column-wise vectorization. 0m×n
variable. vec(
·
n all zero matrix, 0m is used for the m
represents the m
m
×
all zero matrix, and Im denotes the m
m identity matrix.
×
Cm×n represents the set of all m
n complex matrices.
represents the
(m, σ2) denotes the complex normal
1 represents the
denotes the Big-O notation.

|·|
k·k
ℓ2-norm of vector.
distribution with mean m and variance σ2. yn
vector sequences

denotes the amplitude of scalar, and

CN

×

×

.

, yn

y1,

{

· · ·

}

O

II. SYSTEM MODEL AND GENERAL FRAMEWORK

A. System model

We consider an uplink narrow-band single-cell massive
MIMO system as in Fig. 1 with a BS with Mr antennas,
K UEs with Nk antennas each, making the total number of
K
k=1 Nk. The received
transmit antennas at the UEs Nt =

1Note that the BS does not need to perform any ﬁrmware update to train

P

the ML-based predictor for new environments.

UEs

BS with

antennas

(cid:1837)

(cid:1709) (cid:1840)(cid:2869)

(cid:1709) (cid:1840)(cid:2870)

(cid:1709)

(cid:1709) (cid:1840)(cid:3012)

(cid:1839)(cid:3045)

(cid:1709)

Mobility 
estimation

Adaptive 
order 
selection

Channel 
prediction

Fig. 1: Massive MIMO systems with a BS with Mr antennas
and K UEs with Nk antennas each. The BS predicts the
channel using proper complexity order based on the mobility
estimate.

signal at the n-th time slot is given by

yn = √ρ ¯Hnxn + wn

= √ρHn,kxn,k + √ρ

Hn,jxn,j + wn,

(1)

j6=k
X
signal-to-noise

· · ·

the

Hn,K

is
where
ρ
¯Hn =
Hn,1
channel matrix, Hn,k =
the
is
CNt×1
xn =
symbol vector from all UEs, and wn
the complex Gaussian noise.

between
(cid:2)
T

(cid:2)
channel

hn,k,1
(cid:3)

xT

xT

the

· · ·

n,K

· · ·

n,1

∈

∈

(cid:2)

(cid:3)

CMr ×Nt

hn,k,Nk

(SNR),
ratio
is the overall
CMr ×Nk
∈
k-th UE and BS,
(cid:3)
transmitted
the
(0Mr×1, IMr ) is

is

∼ CN

To reduce the interference, we use the zero-forcing (ZF)

combiner

˜FT

n =

ˆ¯Hn

−1 ˆ¯HH
n ,

(2)

ˆ¯HH
n
(cid:16)

(cid:17)

· · ·

Fn,K

fn,k,Nk

where ˆ¯Hn is the predicted channel matrix. We set the re-
based on ˜Fn
ceive combiner ¯Fn =
Fn,1
to satisfy the unit-norm constraint. Note that Fn,k =
fn,k,1
represents the receive combiner for the
· · ·
k-th UE, satisfying
Nk and
(cid:2)
K.
1

2 = 1 for all 1

≤
After applying the receive combiner ¯Fn, which is based
on the predicted channels, the achievable rate of the k-th UE
assuming the Gaussian channel inputs is given as

fn,k,m
(cid:3)

m

≤

≤

≤

k

k

k

(cid:2)

(cid:3)

Rk = E

Nk

"

m=1
X

log2

1 +

(cid:18)

Sn,k,m
IUIn,k,m + ISIn,k,m + Nn,k,m (cid:19)#
(3)

,

where

Sn,k,m = ρ

IUIn,k,m = ρ

ISIn,k,m = ρ

(cid:12)
(cid:12)
j6=k
X

f T
n,k,mhn,k,m
Nk

2

,

f T
n,k,mhn,j,m

2

,

(cid:12)
(cid:12)

m=1
X
(cid:12)
(cid:12)
f T
n,k,mhn,k,m′

(cid:12)
2
(cid:12)
,

Nn,k,m =

m′6=m
X
f T
n,k,mwn

(cid:12)
(cid:12)

2

.

(cid:12)
(cid:12)

The achievable sum-rate is then deﬁned as

(cid:12)
(cid:12)

(cid:12)
(cid:12)

K

R =

Rk.

k=1
X

(4)

(5)

Since channel prediction techniques can be applied to each
UE separately, we focus on a single UE case and drop the UE
index k throughout the paper. The BS predicts the channel
after receiving length τ pilot sequences from the UE as

Yn = √ρHnΨT

n + Wn,

(6)

3

Cτ ×N is
CMr ×τ is the received signal, Ψn
where Yn
∈
the pilot matrix assuming that the pilot sequences are column-
CMr ×τ is
wise orthogonal, i.e., ΨT
the Gaussian noise. For the sake of simplicity, the received
signal is vectorized as

n = τ IN , and Wn

n Ψ∗

∈

∈

y

n = ¯Ψnhn + wn,

(7)

where ¯Ψn = (√ρΨn
∈
CMr N ×1, and wn = vec(Wn)

IMr )

⊗

CMr τ ×MrN , hn = vec(Hn)

CMrτ ×1.

∈

∈
To predict Hn, most previous works assumed certain analyt-
ical models to represent Hn. The SCM channels considered in
this paper can also be represented with stochastic parameters.
As in [27], the SCM channel from the u-th UE antenna to the
m-th BS antenna through the t-th path at the n-th time slot
can be written as

hSCM
u,m,t,n =

Pt
Ls

r

Ls

l=1 n
X
·

exp (jkdm sin(θt,l,AoA)) exp(jφt,l)

exp(jkdu sin(θt,l,AoA))

exp (jk

v

|

|

·

cos(θt,l,AoD

−

θv)n)

,

o

(8)

−

1, k = 2π

where Pt is the power of the t-th path, Ls is the number of
subpaths per-path, j = √
λ is the wave number
with λ denoting the carrier wavelength, dm is the distance of
BS antenna element m from the reference antenna m = 1, du
is the distance of UE antenna element u from the reference
antenna u = 1, θt,l,AoA is the angle-of-arrival (AoA) for the
l-th subpath of the t-th path at the BS, θt,l,AoD is the angle-
of-departure (AoD) for the l-th subpath of the t-th path at the
is
UE, φt,l is the phase of the l-th subpath of the t-th path,
the magnitude of UE mobility, and θv is the angle of moving
N channel matrix Hn is
direction of UE. Then, the Mr
given as

×

v

|

|

Hn = 

t hSCM
1,1,t,n
...
t hSCM

1,Mr ,t,n

P

· · ·
. . .

· · ·




P

N,1,t,n

t hSCM
...
t hSCM

N,Mr,t,n

P




.



(9)

P

Note that the SCM incorporates the important parameters
in the wireless environment channels as in (8). Thus, we
believe it is critical to evaluate developed predictors with the
SCM to guarantee their usefulness in practice. It is difﬁcult,
however, to predict all the SCM parameters directly due to
its complex structure. Note that the parameters in (8) are
temporally correlated random processes. Thus, it would be
possible to predict the channel itself, i.e., Hn, not individual
parameters, using the temporal correlation inherent in the
channels.

Fig. 2: CDF of η with different UE geometries and mobilities.

B. General framework of channel prediction

As in Fig. 1, the BS ﬁrst estimates the mobility of UE, which
can be done with very low overhead in time by using a large
number of antennas in massive MIMO. Using the mobility
estimate, the BS then determines proper complexity order for
the channel prediction to balance the prediction complexity
and accuracy.2 With the proper complexity order, the BS
predicts the channel hn+1 with the previous measurements
y
1 where no is the complexity

for no

, y

,

n−no+1

· · ·

n

≥

order. Formally, we can set an optimization problem
(cid:8)

(cid:9)

2

,

hn+1 −
minimize
(cid:13)
subject to ˆhn+1 = f
(cid:13)
(cid:13)
no = g(ˆv),
(cid:0)

ˆhn+1
y

(cid:13)
(cid:13)
n−no+1
(cid:13)

,

· · ·

, y

n

,

(cid:1)

(10)

(11)

(12)

where ˆhn+1 is the predicted channel, f (
) is an arbitrary
·
predictor, and g(
) is the relation between the complexity order
·
of predictor no and estimated mobility of UE ˆv. Due to the
complex structure of realistic SCM channel, the optimization
problem is highly nonlinear. Therefore, we develop the two
channel predictors with the tractable complexity based on the
Kalman ﬁltering and machine learning.

III. MOBILITY ESTIMATION

There is in general trade-off relation between channel pre-
diction performance and complexity. If the BS has the mobility
information of UE, which is the main factor that determines
how “fast” the channel between the UE and BS varies, it would
be possible to balance between the prediction performance and
complexity. Most of previous mobility estimation methods are
based on simple channel models and require a large number of
channel snapshots in time, making them impractical [23]–[25].
In this section, we propose the spatial average of temporal
correlation (SATC)-based mobility estimator, which works

2The channel prediction with adaptive complexity according to the mobility

for single-input single-output (SISO) SCM channels was proposed in [28].

4

well in time-varying channels. The proposed mobility estima-
tor requires just two channel snapshots in time by spatially av-
eraging the temporal correlation in massive MIMO. Precisely,
the BS ﬁrst obtains

,

k

(13)

η = Re

hnk !

hH
n−1hn
hn−1kk
and compare η with given thresholds to estimate the mobility
of UE. We only take the real part in (13) since η will be
close to one when the UE moves slowly, and the channel does
not vary much. For the sake of simplicity, we assume the
perfect channel in the SATC-based estimator. In general, the
minimum mean square error (MMSE) estimate or least square
(LS) estimates can be used to obtain η.

}

X < η

In Fig. 2, we plot

the cumulative distribution function
(CDF) of η, i.e., FX (η) = Pr
with different UEs
{
experiencing various mobilities. We set different geometries
for the UEs with the same mobility. Fig. 2 shows that the
BS can estimate the UE mobilities, even when the UEs
experience different geometries, by only using two channel
snapshots. Using the estimated mobility, we discuss how to
set the complexity order of developed channel predictors in
Section VI. It requires further efforts to accurately estimate
the Doppler frequency, which is out of scope of this paper.

IV. KALMAN FILTER-BASED PREDICTION

In this section, we ﬁrst elaborate how to estimate the AR
parameters from the SCM channels.3 Based on the established
AR model and estimated parameters, we develop the VKF-
based predictor to predict the SCM channel variation in time.

with

Algorithm 1 Kalman Filter-Based Channel Predictor

1: Initialization:

ˆ˜h0|0 = 0MrN p×1,
M0|0 = E

˜h

H
0

˜h0
h
ˆ˜hn+1|n = E

i

2: Prediction:

˜hn+1
h
= ¯Φˆ˜hn|n
3: Minimum prediction MSE matrix:

= ¯R

yn
1

i

(cid:12)
(cid:12)

Mn+1|n = E

˜hn+1 −
(cid:20)(cid:16)
= ¯ΦMn|n ¯ΦH

ˆ˜hn+1|n
(cid:17) (cid:16)
+ ¯ΘΣ ¯ΘH

˜hn+1 −

H

ˆ˜hn+1|n

(cid:17)

(cid:12)
(cid:12)

yn
1

(cid:21)

4: Kalman gain matrix:

Kn+1 = Mn+1|nSH

SMn+1|nSH + IMr τ

−1

5: Correction:

ˆ˜hn+1|n+1 =
6: Minimum MSE matrix:

(cid:0)
ˆ˜hn+1|n + Kn+1

(cid:1)
Sˆ˜hn+1|n

(cid:17)

y

(cid:16)

n+1 −

Mn+1|n+1 = E

(cid:20) (cid:16)

˜hn+1 −
˜hn+1 −

×

ˆ˜hn+1|n+1

(cid:17)
ˆ˜hn+1|n+1

H

(cid:16)
= (IMr N p

Kn+1S) Mn+1|n

(cid:17)

−

yn+1
1

(cid:21)

(cid:12)
(cid:12)

A. AR parameter estimation

The AR models are widely used in stochastic processes [13].

The vector AR(p) model is given by

p

hn =

Φihn−i + un,

(14)

i=1
X
CMrN ×Mr N is the i-th AR pa-
where p is the AR-order, Φi
rameter matrix, and un
(0MrN ×1, Σ) is the innovation
process. We estimate the two sets of AR model parameters,
which are the AR parameter matrix Φi and covariance matrix
Σ of innovation process un via the Yule-Walker equations,
in order to uniquely deﬁne the vector AR(p) model [13]. The
Yule-Walker equations can be represented as a matrix form

∈
∼ CN

R(1) R(2)

· · ·

R(p)

=

Φ1 Φ2

(cid:2)

(cid:3)

(cid:2)

· · ·

Φp

¯R,
(15)

(cid:3)

3The AR model has a computational advantage over the ARMA model.
When estimating the parameters, the AR model requires only linear equations
whereas the ARMA model needs to solve nonlinear equations. Besides, based
on an analytical channel model, it was shown in [15] that the Kalman ﬁltering
using the AR and ARMA models have almost the same channel prediction
performance. Therefore, we use the AR model instead of the ARMA model
to represent the SCM channels.

1)
2)

· · ·
· · ·
. . .

R(p
R(p

R(0)
RH(1)
...
RH(p

R(1)
R(0)
...
1) RH(p



¯R = 



−

where R(i) = E
is the auto-correlation matrix of
hn with the lag i. Thus, we can obtain the AR parameters by
solving (15) as

hnhH
h






(16)

· · ·

n−i

2)

−

i

,

−
−
...
R(0)

Φ1 Φ2

· · ·

Φp

=

R(1) R(2)

R(p)

· · ·

(cid:3)
(cid:2)
where the covariance matrix Σ is given by

(cid:2)

(cid:3)

¯R−1,
(17)

Σ = R(0)

p

−

i=1
X

ΦiRH(i).

(18)

To avoid the large matrix inversion in the Yule-Walker equa-
tions, the Levinson-Durbin recursion can be used [29].

Remark 1: Note that the aggregated auto-correlation ma-
trix ¯R is required in the Yule-Walker equations. Assuming
the channel statistics do not change much in several co-
herence time intervals, the BS can obtain a sampled auto-
correlation matrix ˆ¯R from the measurement vectors ˜y
n =

 
Input-layer

MLP
Hidden-layers

Output-layer

5

e
p
a
h
s
e
R

Fig. 3: MLP structure with LMMSE pre-processing.

(cid:1838)

Hidden-layers

E
S
M
M
L

yT

n−p+1

T

as

yT
n

h

· · ·

ˆ¯R = ˜Ψ

†
n

with

i

1
Ns

Ns

n=1
X

˜y

n

˜yH

n −

IMr τ p

!

˜Ψ

H
n

†

,

(cid:16)

(cid:17)

(19)

¯Ψn
0Mr τ ×MrN
...

0Mr τ ×MrN
¯Ψn−1
...

0Mr τ ×MrN 0Mr τ ×MrN

˜Ψn = 





· · ·
· · ·
. . .

· · ·

0Mr τ ×MrN
0Mr τ ×MrN
...
¯Ψn−p+1



,




(20)


where Ns is the number of measurement vectors. Note that
the SNR ρ in ¯Ψn can be estimated by measuring the received
signal power [30].

Remark 2: To have an accurate solution in (15) by using
ˆ¯R instead of ¯R, the auto-correlation matrix ˆ¯R needs to be
non-singular. If the order p is high, however, ˆ¯R could be ill-
conditioned. We can resolve this issue using

ˆ¯Rǫ = ˆ¯R + ǫIMr N p,

(21)

instead of ˆ¯R where ǫ is a very small number [14]. Note that
we perform all the simulations in Section VI with ˆ¯Rǫ.

B. Kalman ﬁlter-based prediction

In the Kalman ﬁltering, we need to deﬁne the state equation
and measurement equation to predict the channel sequentially.
For convenience, we can express the state equation by rewrit-
ing the vector AR(p) in (14) as an equivalent vector AR model
of order 1 as

˜hn = ¯Φ˜hn−1 + ¯Θun,

(22)

hT
n

where ˜hn =
CMr N p×1 is the state
the n-th time slot with the order p, un is the
vector at
innovation process in (14), and the state transition matrices

n−p+1

hT

· · ·

∈

(cid:2)

(cid:3)

T

¯Φ and ¯Θ are
¯Φ =

Φ2
Φ1
IMr N 0MrN
0Mr N IMr N

...

...










· · ·
· · ·
· · ·
. . .

· · ·

0Mr N 0MrN

IMr N 0Mr N

Φp
Φp−1
0Mr N 0Mr N
0Mr N 0Mr N

...

∈










CMr N p×MrN .

∈

...








IMr N
0Mr N
...
0Mr N

¯Θ = 





CMr N p×MrN p,

(23)

(24)

The measurement equation can be reformulated from (7) as
n = S˜hn + wn,

y

(25)

(cid:2)

¯Ψn 0Mr τ ×MrN

0Mr τ ×MrN

where S =
∈
CMr τ ×MrN p is the measurement matrix, and wn is the Gaus-
sian noise in (7). Based on these parameters and equations, the
VKF-based prediction method is summarized in Algorithm 1.
The predicted channel ˆhn+1|n is the ﬁrst MrN elements of
ˆ˜hn+1|n deﬁned in Step 2 of Algorithm 1.

· · ·

(cid:3)

V. MACHINE LEARNING-BASED PREDICTION

In this section, we develop the ML-based predictor for
the SCM channel. First, we explain the MLP, a popular
neural network (NN) structure adopted in this paper. Then,
we propose the LMMSE-based received signal pre-processing
technique. Finally, we present the training method for the MLP.

A. MLP structure

While any NN structures can be used for channel prediction,
we adopt the MLP structure since it is simple to implement and
has good prediction performance as shown in Section VI. In
Fig. 3, the MLP structure consists of input-layer, output-layer,
and hidden-layers where the hidden-layer is composed with
L fully-connected layers. For the MLP input, we perform the
LMMSE pre-processing, as explained in the next subsection,
to make the predictor more robust to the noise wn.

 
6

B. LMMSE-based noise pre-processing

We ﬁrst deﬁne

Chnyn

= E

hnyH
n

= E

Cyn
Chn = E

h

h

y

yH
n
n
hnhH
n

i

i

,

,

.

(26)

(27)

(28)

Then, g

n

, the LMMSE estimate of y

h

i
, is given as

n

g

n = Chnyn
¯ΨH
n

= Chn

n

y

C−1
yn
¯ΨnChn
(cid:16)

¯ΨH

n + IMr τ

−1

y

n.

(29)

the BS can obtain the sampled auto-

(cid:17)

As in Section IV,
covariance ˆCyn

,

ˆCyn

=

1
Ns

Ns

y

yH
i ,

i

(30)

i=1
X

where Ns is the number of samples. To obtain the sampled
auto-covariance ˆChn, we exploit the relation

Cyn

= E

y

yH
n

n

h

= E

i
¯Ψnhn + wn
¯ΨH

(cid:1) (cid:0)
n + IMr τ .

h(cid:0)
= ¯ΨnChn

¯Ψnhn + wn

H

i

(cid:1)

(31)

¯ΨH
n

†

.

, the BS can

(cid:17) (cid:16)

(cid:17)

n

From (31), we have ˆChn = ¯Ψ†
Using ˆCyn
acquire g
.

ˆCyn −
, ˆChn , and the received signal y
(cid:16)

n

IMr τ

n

By denoting the predicted channel ˆhn+1 as the output, the

input-output relationship of MLP is given as
ˆhn+1 = fΠ

, g

g

,

n−I+1

· · ·

n

,

(32)

(cid:0)

(cid:1)

where Π is the parameter set of MLP, and I is the input-
order, which can balance between the MLP complexity and
prediction performance. Note that denoising the input data for
channel estimation has been proposed in recent works, [31],
[32]; however, these works relied on the deep CNN-based
architectures with considerable complexity. On the contrary,
the proposed LMMSE-based pre-processing is simple yet
practical.

C. MLP training

,

n

g

(cid:9)

(cid:8)

, g

· · ·

n−I+1

In the MLP training phase, the inputs to the MLP are the
noise pre-processed channel vectors
, and
the output is the predicted channel vector at the (n + 1)-
th time slot ˆhn+1. For the real-valued MLP architecture,
we reshape the inputs to a 2IMrN -dimension input-layer,
which is the real and imaginary parts of input vectors,
. In
i.e.,
the hidden-layer, we use L fully-connected layers with fl
n
(cid:0)
(cid:0)
nodes for 1
L. The output-layer is designed to
have 2MrN -dimension, which corresponds to the real and
imaginary parts of channel vector at the (n + 1)-th time slot
. The last reshape layer combines the

n−I+1

n−I+1

, Im

, Im

, Re

(cid:1)o

Re

· · ·

(cid:1)
l

, Im

Re

≤

≤

g

g

g

g

(cid:0)

(cid:0)

(cid:1)

(cid:1)

n

n

,

ˆhn+1

ˆhn+1

n

(cid:0)

(cid:1)

(cid:0)

(cid:1)o

real and imaginary parts to reconstruct the complex-valued
predicted channel vector ˆhn+1.

We use the adaptive moment estimation (ADAM) as the
optimizer, and the loss function for the NN training by the
mean square error (MSE) between the predicted channel and
the noise pre-processed channel g
, not the true channel
hn+1,

n+1

2

Ntrain

1
Ntrain

,

C

g

n+1

(33)

loss =

ˆhn+1 −
n=I (cid:13)
X
(cid:13)
where Ntrain is the number of training samples. Although
(cid:13)
previous works on channel estimation and prediction using
NN, e.g., [20]–[22], used the true channel for the loss function,
this is not possible in practice. Therefore, we have used the
pre-processed data g
for the NN training and also for the
prediction.

(cid:13)
(cid:13)
(cid:13)

n

VI. COMPLEXITY ANALYSIS AND NUMERICAL RESULTS

In this section, we ﬁrst analyze the computational complex-
ity of developed predictors. Then, through numerical studies,
we determine proper parameter values for the VKF-based
and MLP-based predictors. Using these parameter values,
we thoroughly compare the two predictors in terms of the
prediction accuracy and achievable sum-rate.

A. Complexity analysis

In this subsection, we analyze the computational complexity
of VKF-based predictor with the AR parameter estimation
and that of MLP-based predictor based on the LMMSE pre-
processing. We use the number of ﬂoating-point operations
(FLOPs) as the performance metric with the Big-O notation
[33].

In the VKF-based predictor, we ﬁrst consider the AR pa-
rameter estimation complexity. To estimate the AR parameters
in (15) for the Kalman ﬁltering, the AR parameter estimation
(pMrN )3
has the complexity of
because of the matrix
inversion in (17). Also, the Kalman ﬁltering to predict the
channel has the complexity of
due to the matrix
O
inversion of Kalman gain matrix. Thus, the total complexity
of VKF-based predictor is

(cid:1)
(MrN )3

O

(cid:1)

(cid:0)

(cid:0)

CVKF =

O

p3 + 1

(MrN )3

.

(34)

(cid:1)

(cid:0)(cid:0)

The MLP-based predictor has two types of complexity, i.e.,
the complexity of training phase and prediction phase, which is
usually called as the test phase in machine learning literature.
With the number of epochs Nepoch, the number of training
samples Ntrain, and the number of hidden-layer L with fl
nodes, the complexity of training phase becomes [34]

(cid:1)

Ctrain

NepochNtrain

IMrN f1 +

flfl+1 + fLMrN

L−1

l=1
X
αI(MrN )2 + (L

1)α2(MrN )2

−

!!

O

NepochNtrain
+ α(MrN )2
(cid:0)
(cid:0)
NepochNtrainα(I + (L

(cid:1)(cid:1)

1)α + 1)(MrN )2

,

(35)

−

(cid:1)

O  

=

(a)
=

=

O

(cid:0)

 
TABLE I
Complexity of VKF-based predictor and MLP-based predictor

7

Channel predictor Method

VKF

MLP

AR estimation

Complexity
O (cid:0)(pMrN )3
O (cid:0)(MrN )3
Kalman ﬁltering
LMMSE estimation O (cid:0)(MrN )3
MLP train

(cid:1)

(cid:1)

MLP test

O (cid:0)NepochNtrainα(I + (L − 1)α + 1)(Mr N )2
O (cid:0)α(I + (L − 1)α + 1)(Mr N )2

(cid:1)

(cid:1)

Total complexity

O (cid:0)(cid:0)p3 + 1(cid:1) (MrN )3

(cid:1)

where (a) comes from fl = αMrN for 1
L. The
constant α is to scale the hidden-layer nodes according to the
number of antennas at the BS and UE. In the prediction phase,
the complexity is given by

≤

≤

l

Ctest =

α(I + (L

1)α + 1)(MrN )2

.

(36)

(cid:0)

O

−
In addition, the complexity of LMMSE estimation in (29)
. Thus, the total complexity of MLP-based
is
predictor is
(cid:0)
CMLP =

(MrN )3

O

(cid:1)

(cid:1)

O

(NepochNtrain + 1)α(I + (L
+ (MrN )3
.
(cid:0)

−

1)α + 1)(MrN )2
(37)

(cid:1)

O (cid:0)(NepochNtrain + 1)α(I + (L − 1)α + 1)(Mr N )2 + (MrN )3

(cid:1)

TABLE II
System parameters

Parameter

Environment

Mobility of UE

Value

UMi

3 km/h

Carrier frequency

2.3 GHz

Time slot duration

BS antenna structure

40 ms
64, 8×8 UPA

The complexity of VKF-based predictor and MLP-based pre-
dictor is summarized in Table I.

(cid:1)

O

and

(MrN )3

NepochNtrain(MrN )2

To compare the total complexity of VKF-based predictor
and MLP-based predictor, we approximate the complexity of
.
both predictors as
The MLP-based predictor has much higher complexity than
(cid:1)
(cid:1)
(cid:0)
the VKF-based predictor since NepochNtrain
MrN . How-
ever, once trained, the prediction phase complexity of MLP-
, which is much lower
based predictor becomes
than that of VKF-based predictor
. Thus, we can
mitigate the high complexity problem of MLP-based predictor
by conducting ofﬂine training.

(MrN )3
(cid:1)

(MrN )2

≫

O

O

O

(cid:0)

(cid:0)

(cid:1)

(cid:0)

B. Numerical results

In the simulation,

the MLP is implemented with Deep
Learning toolbox of MATLAB. For the MLP, the training
rate is set to 0.001 with the batch size 128 and number of
epochs Nepoch = 1000. We consider the urban micro (UMi)
cell in the SCM [26], where the mobility of UE is 3 km/h,
the carrier frequency is 2.3 GHz, and the time slot duration is
8 uniform planar array
40 ms. The BS is equipped with the 8
(UPA) at the height of 15 m. Also, each UE is equipped with
N = 2 transmit antennas as the uniform linear array (ULA)
and transmits the length τ = 2 pilot sequences. We adopt the
pilot matrix Ψn using the discrete Fourier transform (DFT)
matrix, which satisﬁes the assumption in Section II-A. Note
that the SCM takes the pathloss into account, resulting in very
small channel gains. To fairly compare the channel predictors,
we normalize the average gain of channel vectors to MrN ,
i.e., E
= MrN , for all simulations. We deﬁne the
normalized mean square error (NMSE) as the performance
metric

hnk

k
h

×

i

2

NMSE = E

ˆhn+1 −
(cid:20)(cid:13)
(cid:13)
(cid:13)

hn+1

2

/

(cid:13)
(cid:13)
(cid:13)

2

hn+1k
k

,

(cid:21)

(38)

Fig. 4: NMSE of MLP-based predictor with different numbers
of hidden-layer L according to SNR with I = 3, Ntrain = 2048,
and fl = 512.

where ˆhn+1 is the predicted channel, and hn+1 is the true
channel. The system parameters are summarized in Table II
while we explicitly state parameter values if they are different
from the table.

In our simulation, we compare the following methods:
• Outdated: outdated channel ˆh

where ¯Ψn
is the known pilot matrix. This serves as a baseline of
channel predictors.

n+1 = ¯Ψ†y

outdated

n

• Extrapolation: extrapolation-based prediction. We use
the ﬁrst-order polynomial-based extrapolation to predict
ext
the channel ˆh
n+1 = fext(n + 1) = (n + 1)a + b where
the coefﬁcient vectors a and b are determined by solving
n = na + b and y
y
= (n

1)a + b.

n−1

−

8

Fig. 5: NMSE of VKF-based predictor with different AR-order
p according to SNR with Ns = 2048 and v = 3, 10 km/h.

Fig. 7: AR-order according to mobility of UE with Ns = 2048.

Fig. 6: NMSE of MLP-based predictor with different input-
order I according to SNR with Ntrain = 2048 and v = 3, 10
km/h.

Fig. 8: NMSE of VKF-based predictor and MLP-based predic-
tor according to time slot with Ns = 512, 2048, p = I = 3,
and SNR = 20 dB.

• MLP without pre-processing: MLP trained with the
without any pre-

y

, y

,

measurements
processing.

n−I+1

· · ·

(cid:8)

n

(cid:9)

• MLP with pre-processing (or simply MLP): MLP

developed in Section V.

• VKF: vector Kalman ﬁlter-based prediction developed in

Section IV.

To have fair comparison, we set the number of measurement
vectors Ns in (19) and number of training samples Ntrain in
(33) to be the same, i.e., Ns = Ntrain.

In Fig. 4, we compare the NMSE of MLP with different
numbers of hidden-layer L according to the SNR with the
input-order I = 3, Ntrain = 2048, and fl = 512 for 1
L.
It is obvious from the ﬁgure that L = 2 is sufﬁcient for channel
prediction. Note that we do not use the activation layer (e.g.

≤

≤

l

sigmoid or rectiﬁed linear unit (ReLU)) in the hidden-layer
since it worsens the result. We set L = 2 and fl = 512 for the
following simulations to reduce the MLP training complexity.
Figs. 5 and 6 depict the NMSE of VKF-based predictor with
different AR-order p and that of MLP-based predictor with
different input-order I with Ns = Ntrain = 2048 and v = 3, 10
km/h. It is clear from the ﬁgures that higher order is needed for
higher mobility to achieve the same prediction accuracy. Thus,
it is crucial to ﬁnd the proper order to balance the accuracy and
complexity according to the mobility of UE. Since the AR-
order and input-order balance the complexity and prediction
accuracy, we use the same order for both predictors in the
remaining simulations.

In Fig. 7, we numerically determine the effective AR-
order according to the mobility of UE with Ns = 2048. We
deﬁne the effective order as the minimum AR-order satisfying

9

11: Achievable

Fig.
channel,
sum-rate
extrapolation-based predictor, VKF-based predictor with
adaptive order and ﬁxed order p = 3, and MLP-based
predictor with adaptive order and ﬁxed order I = 3 according
to SNR with Ns = 2048 and K = 8.

outdated

of

based predictors with different SNR values according to the
number of samples Ns with p = I = 3. Fig. 9 shows that the
VKF-based predictor requires less number of samples than the
MLP-based predictor to achieve the same prediction accuracy
for all SNR values.

We compare in Fig. 10 the NMSE of outdated channel,
extrapolation-based predictor, VKF-based predictor, and MLP-
based predictor with and without pre-processing with respect
to the SNR. We set p = I = 3 and Ns = 2048. The
extrapolation-based predictor would not be able to track the
SCM channel even in high SNR regime. The developed predic-
tors outperform the outdated channel and extrapolation-based
predictor. The MLP-based predictor with the pre-processing
has almost the same performance as the VKF-based predictor.
Also, the proposed LMMSE pre-processing gives about 5 dB
NMSE gain at 0 dB SNR.

In Fig. 11, we compare the achievable sum-rates, deﬁned in
(5), of outdated channel, extrapolation-based predictor, VKF-
based predictor with adaptive order and ﬁxed order p = 3,
and MLP-based predictor with adaptive order and ﬁxed order
I = 3 according to the SNR with Ns = 2048 and number
of UEs K = 8. The ﬁrst four UEs experience the mobility
v = 3 km/h while the other UEs experience the mobility
v = 10 km/h with different geometries. We adaptively select
the order with respect to the mobility of UE as in Fig. 7. The
achievable sum-rates of outdated channel and extrapolation-
based predictor increase as the SNR increases, but saturate
in high SNR regime. The VKF-based predictor and MLP-
based predictor have substantial gain over the extrapolation-
based predictor, and the gain becomes larger as the SNR
increases. Both predictors have almost the same sum-rates
since we set the same complexity order for both predictors.
Also, the adaptive order selection provides additional gain for
both predictors.

Fig. 9: NMSE of VKF-based predictor and MLP-based predic-
tor with different SNR according to number of samples with
p = I = 3.

Fig. 10: NMSE of outdated channel, extrapolation-based pre-
dictor, VKF-based predictor, and MLP-based predictor with
and without pre-processing with respect to SNR with p =
I = 3 and Ns = 2048.

−

NMSE <
20 dB with SNR = 20 dB. Approximately,
the relation no = g(ˆv) in (12) turns out to be linear, i.e.,
AR-order
Mobility [km/h]. Thus, we can determine
the effective complexity order by using the estimated mobility
in Section III.

0.3

≈

·

Fig. 8 shows the NMSE of VKF-based and MLP-based
predictors according to the time slot with Ns = 512, 2048,
p = I = 3, and SNR = 20 dB. As the time slot increases, the
VKF-based predictor outperforms the MLP-based predictor
after only two successive predictions for both cases of Ns.
Note that all the numerical results except those in Fig. 8 are
averaged over 100 time slots.

In Fig. 9, we verify the NMSE of VKF-based and MLP-

10

VII. CONCLUSION

In this paper, we developed the channel predictors for the
time-varying massive MIMO systems. First, we implemented
the low-complexity mobility estimator to set proper prediction
complexity order. Then, we developed the VKF-based predic-
tor with the AR parameter estimation from the SCM data. We
also developed the MLP-based channel predictor based on the
LMMSE noise pre-processing. The numerical results showed
that both channel predictors have substantial gain over the
outdated channel in terms of the prediction accuracy and sum-
rate. Regarding the complexity, the MLP-based predictor has
much higher total complexity than the VKF-based predictor.
Once trained, however, the MLP-based predictor has much
lower complexity, which shows ofﬂine learning is crucial to
adopt the MLP-based channel predictor in practice. It might be
also possible to exploit more advanced ML techniques, e.g.,
meta learning or few-shot learning, to mitigate the training
overhead, which is an interesting future research topic. We
believe the complexity analysis and numerical results in this
paper can give good guidelines for system operators to adopt
a better channel prediction technique depending on their
situation, e.g., whether it is possible to have a large number
of training samples or to perform the ofﬂine learning.

REFERENCES

[1] T. L. Marzetta, “Noncooperative cellular wireless with unlimited num-
bers of base station antennas,” IEEE Transactions on Wireless Commu-
nications, vol. 9, no. 11, pp. 3590–3600, Nov. 2010.

[2] A. K. Papazafeiropoulos, “Impact of general channel aging conditions
on the downlink performance of massive MIMO,” IEEE Transactions
on Vehicular Technology, vol. 66, no. 2, pp. 1428–1442, Feb. 2017.
[3] K. T. Truong and R. W. Heath, “Effects of channel aging in massive
MIMO systems,” Journal of Communications and Networks, vol. 15,
no. 4, pp. 338–351, Aug. 2013.

[4] N. Palleit and T. Weber, “Time prediction of non ﬂat fading channels,”
in 2011 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), May 2011, pp. 2752–2755.

[5] C. Kong, C. Zhong, A. K. Papazafeiropoulos, M. Matthaiou, and
Z. Zhang, “Sum-rate and power scaling of massive MIMO systems with
channel aging,” IEEE Transactions on Communications, vol. 63, no. 12,
pp. 4879–4893, Dec. 2015.

[6] V. Jungnickel, K. Manolakis, W. Zirwas, B. Panzner, V. Braun, M. Los-
sow, M. Sternad, R. Apelfrjd, and T. Svensson, “The role of small cells,
coordinated multipoint, and massive MIMO in 5G,” IEEE Communica-
tions Magazine, vol. 52, no. 5, pp. 44–51, May 2014.

[7] V. Arya and K. Appaiah, “Kalman ﬁlter based tracking for channel aging
in massive MIMO systems,” in 2018 International Conference on Signal
Processing and Communications (SPCOM), Jul. 2018, pp. 362–366.
[8] S. Noh, M. D. Zoltowski, Y. Sung, and D. J. Love, “Pilot beam pattern
design for channel estimation in massive MIMO systems,” IEEE Journal
of Selected Topics in Signal Processing, vol. 8, no. 5, pp. 787–801, Oct.
2014.

[9] J. Choi, D. J. Love, and P. Bidigare, “Downlink training techniques for
FDD massive MIMO systems: Open-loop and closed-loop training with
memory,” IEEE Journal of Selected Topics in Signal Processing, vol. 8,
no. 5, pp. 802–814, Oct. 2014.

[10] S. Bazzi and W. Xu, “Downlink training sequence design for FDD
multiuser massive MIMO systems,” IEEE Transactions on Signal Pro-
cessing, vol. 65, no. 18, pp. 4732–4744, Sep. 2017.

[11] H. Kim and J. Choi, “Channel estimation for spatially/temporally cor-
related massive MIMO systems with one-bit ADCs,” EURASIP Journal
on Wireless Communications and Networking, vol. 2019, no. 1, p. 267,
Dec. 2019.

[12] C. Li, J. Zhang, S. Song, and K. B. Letaief, “Selective uplink training
for massive MIMO systems,” in 2016 IEEE International Conference
on Communications (ICC), May 2016, pp. 1–6.

[13] S. Haykin, Adaptive Filter Theory (3rd Ed.). Upper Saddle River, NJ,

USA: Prentice-Hall, Inc., 1996.

[14] K. E. Baddour and N. C. Beaulieu, “Autoregressive modeling for fading
channel simulation,” IEEE Transactions on Wireless Communications,
vol. 4, no. 4, pp. 1650–1662, Jul. 2005.

[15] S. Kashyap, C. Molln, E. Bjrnson, and E. G. Larsson, “Performance
analysis of TDD massive MIMO with Kalman channel prediction,” in
2017 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), Mar. 2017, pp. 3554–3558.

[16] B. Y. Shikur and T. Weber, “Channel prediction using an adaptive
Kalman ﬁlter,” in WSA 2015; 19th International ITG Workshop on Smart
Antennas, Mar. 2015, pp. 1–7.

[17] T. Wang, C. Wen, H. Wang, F. Gao, T. Jiang, and S. Jin, “Deep
layer: Opportunities and challenges,”

learning for wireless physical
China Communications, vol. 14, no. 11, pp. 92–111, Nov. 2017.
[18] C. Wen, W. Shih, and S. Jin, “Deep learning for massive MIMO CSI
feedback,” IEEE Wireless Communications Letters, vol. 7, no. 5, pp.
748–751, Oct. 2018.

[19] T. Wang, C. Wen, S. Jin, and G. Y. Li, “Deep learning-based CSI
feedback approach for time-varying massive MIMO channels,” IEEE
Wireless Communications Letters, vol. 8, no. 2, pp. 416–419, Apr. 2019.
[20] P. Dong, H. Zhang, G. Y. Li, N. NaderiAlizadeh, and I. S. Gaspar,
“Deep CNN for wideband mmwave massive MIMO channel estimation
using frequency correlation,” in 2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), May 2019, pp.
4529–4533.

[21] M. Soltani, V. Pourahmadi, A. Mirzaei, and H. Sheikhzadeh, “Deep
learning-based channel estimation,” IEEE Communications Letters,
vol. 23, no. 4, pp. 652–655, Apr. 2019.

[22] J. Yuan, H. Q. Ngo, and M. Matthaiou, “Machine learning-based channel
estimation in massive MIMO with channel aging,” in 2019 IEEE 20th
International Workshop on Signal Processing Advances in Wireless
Communications (SPAWC), Jul. 2019, pp. 1–5.

[23] L. Krasny, H. Arslan, D. Koilpillai, and S. Chennakeshu, “Doppler
spread estimation in mobile radio systems,” IEEE Communications
Letters, vol. 5, no. 5, pp. 197–199, May 2001.

[24] K. E. Baddour and N. C. Beaulieu, “Robust Doppler spread estima-
tion in nonisotropic fading channels,” IEEE Transactions on Wireless
Communications, vol. 4, no. 6, pp. 2677–2682, Nov. 2005.

[25] Y. R. Zheng and C. Xiao, “Mobile speed estimation for broadband wire-
less communications over Rician fading channels,” IEEE Transactions
on Wireless Communications, vol. 8, no. 1, pp. 1–5, Jan. 2009.
[26] Study on 3D channel model for LTE, 3GPP TR 36.873 V12.7.0 Std.,

Jan. 2018.

[27] S. Pan, S. Durrani, and M. Bialkowski, “MIMO capacity for spatial
channel model scenarios,” in IEEE Proceedings Australian Communi-
cation Theory Workshop Adelade Australia, Feb. 2007, pp. 25–29.
[28] Y. Liu and L. Li, “Adaptive multi-step channel prediction in spatial
channel model using Kalman ﬁlter,” in ICT 2013, May 2013, pp. 1–5.
[29] J. Durbin, “The ﬁtting of time-series models,” Revue de l’Institut
International de Statistique / Review of the International Statistical
Institute, vol. 28, no. 3, pp. 233–244, 1960.

[30] K. Hung and D. W. Lin, “Pilot-based LMMSE channel estimation
for OFDM systems with powerdelay proﬁle approximation,” IEEE
Transactions on Vehicular Technology, vol. 59, no. 1, pp. 150–159, Jan.
2010.

[31] H. He, C. Wen, S. Jin, and G. Y. Li, “Deep learning-based channel
estimation for beamspace mmwave massive MIMO systems,” IEEE
Wireless Communications Letters, vol. 7, no. 5, pp. 852–855, Oct. 2018.
[32] S. Khan, K. S. Khan, and S. Y. Shin, “Symbol denoising in high order M-
QAM using residual learning of deep CNN,” in 2019 16th IEEE Annual
Consumer Communications Networking Conference (CCNC), Jan. 2019,
pp. 1–6.

[33] R. Hunger, Floating point operations in matrix-vector calculus. Munich

University of Technology, Inst. for Circuit Theory and Signal, 2005.

[34] E. Mizutani and S. E. Dreyfus, “On complexity analysis of super-
vised MLP-learning for algorithmic comparisons,” in IJCNN’01. In-
ternational Joint Conference on Neural Networks. Proceedings (Cat.
No.01CH37222), vol. 1, Jul. 2001, pp. 347–352.

