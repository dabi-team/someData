7
1
0
2

r
p
A
1
1

]
T
S
.
h
t
a
m

[

1
v
2
6
2
3
0
.
4
0
7
1
:
v
i
X
r
a

Gaussian autoregressive process with dependent innovations.
Some asymptotic results.

Fabio Gobbi

Sabrina Mulinacci

University of Bologna - Department of Statistics

November 9, 2018

Abstract

In this paper we introduce a modiﬁed version of a gaussian standard ﬁrst-order autore-
gressive process where we allow for a dependence structure between the state variable Yt−1
and the next innovation ξt. We call this model dependent innovations gaussian AR(1) pro-
cess (DIG-AR(1)). We analyze the moment and temporal dependence properties of the new
model. After proving that the OLS estimator does not consistently estimate the autoregressive
parameter, we introduce an infeasible estimator and we provide its √T -asymptotic normality.

Mathematics Subject Classiﬁcation (2010): 62M10, 62F10

Keywords: autoregressive processes, dependent innovations, √T -asymptotic normality.

1

Introduction

−

−

In this paper we consider a modiﬁed version of the standard ﬁrst-order autoregressive process,
1 + ξt, in which, unlike the classical case, we assume that the state variable at the time
Yt = φYt
−
1, Yt
1, and the next innovation ξt are no longer independent.
t
The model here considered is a particular case of the class of the C-convolution based processes
studied in Cherubini et al. (2011) and Cherubini et al. (2012). Recent literature has mainly focused
on stationary copula-based Markov process (see, among the others, Chen and Fan, 2006, and Beare,
2010), while the mentioned C-convolution based processes have a stationary dependence structure
between each level and the next innovation generating a process which no longer stationary.

In particular, this paper focuses on the case where the pairs (Yt

1, ξt)t have a joint distribution
function given by a gaussian copula with a time-invariant parameter ρ and the innovations (ξt)t
are identically distributed with a gaussian distribution: in addition, we assume that the resulting
stochastic process is a Markov process. This model ﬁrst appeared in Cherubini et al. (2016) where
some moment properties have been analyzed without, however, considering temporal dependencies.
The unit root case (φ = 1) is studied in Gobbi and Mulinacci (2017) while here we will concentrate
< 1. We call the model “dependent innovations gaussian AR(1) process (DIG-
on the case
AR(1))”.

φ
|

−

|

The analysis of the model starts from the study of the implied temporal dependence properties
both in the sequence (Yt)t and in the sequence of innovations (ξt)t which, as expected, are no
more independent. As a consequence, statistical inference on the model changes signiﬁcantly.
In particular, estimating the autoregressive parameter can no longer be made by ordinary least
squares (OLS). As we shall prove, the OLS estimator is not consistent for φ and, as expected,
the asymptotic bias depends on ρ. To overcome this drawback, in this paper we propose a new
infeasible estimator of φ which allows us to achieve a √T -consistency and asymptotic normality.
The new estimator takes into account a correction due to the presence of ρ whose importance

1

 
 
 
 
 
 
will be emphasized both as regards the estimation procedure and as regards the dynamics of the
process.

The paper is organized as follows. Section 2 introduces the DIG-AR(1) process. Section 3 shows
the autocorrelation functions of (Yt)t and (ξt)t. In Section 4 we prove that the OLS estimator is
not consistent for the autoregressive parameter and we introduce a new infeasible estimator which
is proved to be √T -asymptotically normal. Section 5 concludes.

2 The model

In this paper we consider a generalized version of the standard stationary gaussian ﬁrst-order
autoregressive process, AR(1), deﬁned as

Yt = φYt

−

1 + ξt,

< 1 (condition for the weak stationarity) and the sequence of innovations
1 is i.i.d. and normally distributed with zero mean and standard deviation σξ (gaussian white
1 for all t. It is just the case to recall

where Y0 = 0 a.s.,
(ξ)t
noise process): as a consequence, ξt is independent of Yt
that in this framework we have

φ
|

−

≥

|

E[Yt] = 0, t
1
≥
σ2
E[Y 2
ξ
φ2 , t
t ] =
1
≥
ηk = corr(Yt, Yt+k) = φk, k = 1, 2, ...

−

1






For a detailed discussion of AR(1) processes see, among others, Hamilton (1994) and Brockwell
and Davis (1991). The modiﬁed version of the AR(1) process considered in this paper is inspired
by the Markov processes modeling with dependent increments considered in Cherubini et al. (2011,
2012, 2016): the idea is to relax the assumption of independence between ξt and Yt
1, allowing for
some time-invariant gaussian dependence.

−

More precisely

Deﬁnition 2.1. A dependent innovations gaussian AR(1) process (DIG-AR(1)) is a discrete time
stochastic process deﬁned as

where Y0 = 0 a.s.,
associated to the vector (ξt, Yt
t.

φ
|

|

−

Yt = φYt

1 + ξt, t

N

∈

−
< 1, (ξt)t are identically distributed with ξt

1) is gaussian with time-invariant parameter ρ with

(1)

N (0, σ2

∼

ξ ) and the copula
< 1 for all

ρ

|

|

Therefore, in this version of the model the sequence of innovations is no more a gaussian white
noise but it has a temporal dependence structure that we will analyze in the following. Moreover, in
Section 4.3.1 of Cherubini et al. (2016), the authors determine the variance of Yt and its asymptotic
convergence, that is

t = σ2
V 2
ξ

φ2(t

−

1) +

1

t
−

i=1
X

φ2(i

−

1)

!

+ 2ρσξ

1

t
−

i=1
X

φ2i

−

1Vt

−

i,

and

Vt

σξ

ρφ +

ρ2φ2 + 1

−→t
+
∞
↑

(cid:16)

1
p

φ2

−

The limit, that we denote with ¯V (ρ, φ), can be written as

¯V (ρ, φ) = S

(cid:16)

ρφ +

ρ2φ2 + 1

p

1

φ2

−

p

2

φ2

−

.

(cid:17)

φ2

−

,

(cid:17)

 
where S = σξ
φ2 is the standard deviation of the AR(1) process. It is just the case to observe
√1
−
that, when ρ = 0, ¯V (0, φ) = S. As shown in Figure 1, we can notice that ¯V (ρ, φ) > S if ρφ > 0
whereas ¯V (ρ, φ) < S if ρφ < 0. In plain words, Yt becomes more and more volatile if the correlation
between ξt and Yt
1 has the same sign of the autoregressive coeﬃcient. On the other hand, the
volatility tends to decrease if they have opposite sign.

−

φ=0.95
φ=0.7
φ=−0.7
φ=−0.95

22

20

18

16

14

12

10

8

6

4

2

0

−1

−0.8

−0.6

−0.4

−0.2

0.2

0.4

0.6

0.8

1

0
ρ

Figure 1: Graph of ¯V (ρ, φ) as a function of ρ for diﬀerent values of φ. Here we ﬁx σξ = 1.

In addition to the above assumptions, we assume that the stochastic process (Yt)t is a Markov
process. As proved in Darsow et al. (1992) a necessary condition for a process (Yt)t to be Markovian
is that, if Cs,t is the copula associated to (Ys, Yt), then

Cs,t(u, v) = Cs,r

Cr,t(u, v) =

∗

1

∂
∂w

0
Z

Cs,r(u, w)

∂
∂w

Cr,t(w, v) dw,

s < r < t.

∀

Morover, as shown in section 3.2.3 in Cherubini et al. (2012), if copulas Cs,r and Cr,t are gaussian
with parameters τs,r and τr,t, respectively, then Cs,t is again gaussian with parameter

τs,t = τs,rτr,t.

(2)

3 Autocorrelation functions and mixing-properties

+

In this subsection we study the behavior of the autocorrelation functions of the process (Yt)t
when t
. It is just the case to recall that in the standard AR(1) process the k-th order
autocorrelation function of (Yt)t depends only on k and it is equal to φk. In our more general
setting, this is no longer true.
In section 4.3.1 of Cherubini et al. (2016) it is shown that the
copula between Yt and Yt+1 is gaussian with time-dependent parameter given by

−→

∞

τt,t+1 =

φVt + ρσξ
Vt+1

,

and

τt,t+1 −→t

+

↑

∞

φ +

ρσξ
¯V (ρ, φ)

= ¯τ (ρ, φ).

3

(3)

 
 
The limit of the k-th order autocorrelation function of the Markov process (Yt)t is a function

of k, σξ and ρ as the following proposition shows.

Proposition 3.1. The k-th order autocorrelation function of the DIG-AR(1) Markov process (Yt)t

tends to

φ + ρσξ
¯V (ρ,φ)

for any k

1 as t

+

.
∞

−→

≥

k

(cid:17)

(cid:16)

Proof. Thanks to (2), we have that the copula between Yt and Yt+k is gaussian with parameter

τt,t+k =

φVt+s + ρσξ
Vt+s+1

.

k

1

−

s=0
Y

Therefore, since for any s

1 as t

−→

≥
Vt+s + ρσξ

+

∞

Vt+s+1 −→

we easily get the result.

¯V (ρ, φ) + ρσξ
¯V (ρ, φ)

= φ +

ρσξ
¯V (ρ, φ)

,

(4)

It is interesting to consider the role of ρ in the dynamics of the limit of τt,t+k with respect to
the standard case when τt,t+k = φk. If φ > 0 the limit autocorrelation monotonically decreases
to zero as k increases, more slowly if ρ > 0 and more rapidly if ρ < 0. On the contrary, if φ < 0
the limit autocorrelation ﬂuctuates more widely if ρ < 0 whereas the ﬂuctuations are more blunt
if ρ > 0.

On the other hand, the innovations (ξt)t are of course no longer serially independent as in
the standard case and the k-th order autocorrelation function approaches to a limit which again
depends on ρ, σξ and k.

Proposition 3.2. Given the DIG-AR(1) Markov process of Deﬁnition 2.1, the k-th order auto-
correlation function of (ξt)t, δt,t+k, for any k

1, tends to

≥
1(ρ, φ)

(¯τ (ρ, φ)

φ)(1

−

−

φ¯τ (ρ, φ))

δt,t+k

−→t
+
∞
↑

¯V (ρ, φ)¯τ k
−
σ2
ξ

where ¯τ is deﬁned in (3).

Proof. We have

E[ξtξt+k] = E[(Yt

φYt

−

= E[YtYt+k]
−
= τt,t+kVtVt+k

1)(Yt+k
−
−
φE[YtYt+k
φτt,t+k

−

φYt+k
1]
−
−
1VtVt+k

1)] =
−
φE[Yt
−
1 −
1(ρ, φ) and Vt

−

−

Since for any ﬁxed k

1, τt,t+k

≥

¯τ k

−

−→

1Yt+k] + φ2E[Yt
−
1,t+kVt
φτt

1] =

1Yt+k
1Vt+k + φ2τt
−

−

−

−

1,t+k

1Vt

−

1Vt+k

1.

−

−

−→

¯τ k(ρ, φ)

¯V 2(ρ, φ)
= ¯V 2(ρ, φ)¯τ k

(cid:0)

−

1(ρ, φ)(¯τ (ρ, φ)

−

φ¯τ k+1(ρ, φ)

φ¯τ k

−

−
φ)(1

−

φ¯τ (ρ, φ)).

−

(cid:1)

−→

¯V (ρ, φ) as t

+

we get

−→
1(ρ, φ) + φ2 ¯τ k(ρ, φ)

∞

=

E[ξtξt+k]

It follows

δt,t+k

−→

¯V 2(ρ, φ)¯τ k
σ2
ξ

1(ρ, φ)

−

(¯τ (ρ, φ)

φ)(1

−

−

φ¯τ (ρ, φ)).

4

To conclude this section we brieﬂy discuss the mixing properties of the sequence (Yt)t. Given a
Z, deﬁned on a given probability

(not necessarily stationary) sequence of random variables (Xt)t
∈
l) with
space (Ω,
−∞ ≤

l
t be the σ-ﬁeld

l
t = σ(Xt, t

t

l

+

∞

≤

≤

and let

, P), let

≤

F

F

F

t

≤
J

I

˜β(

F

t

−∞

,

F

+
∞t+k ) = sup
{
{

Ai

}

,

Bj

1
2

}

P(Ai

|

∩

Bj)

P(Ai)P(Bj )
|

,

−

(5)

i=1
X

j=1
X

where the second supremum is taken over all ﬁnite partitions
that Ai

A1, ...AI
∞t+k for each j. Deﬁne the following dependence coeﬃcient

for each i and Bj

B1, ...BJ

of Ω such

and

{

}

{

}

t

∈ F

−∞

∈ F

βk = sup
Z

t
∈

˜β(

F

t

−∞

+
∞t+k ).

,

F

We say that the sequence (Xt)t
∈
Volkonskii and Rozanov, 1959 and 1961, for more details).

Z is β

−

mixing (or absolutely regular) if βk

0 as k

+

∞

→

(see

→

Proposition 3.3. The DIG-AR(1) Markov process (Yt)t is β-mixing.

Proof. The result is an immediate consequence of Theorem 2.1 in Gobbi and Mulinacci (2017) and
< 1 for all t and its
the proof is very close to that of Corollary 3.1 therein. In fact, since
limit, φ + ρσǫ
< 1. Of course, exactly

φ + ρσǫ

τt,t+1|

¯V (ρ,φ)

< 1, we have that ˆη = sup
t |

¯V (ρ,φ) , satisﬁes

|
τt,t+1|

as shown in the proof of Corollary 3.1 of Gobbi and Mulinacci (2017) the density of the copula
associated to the vector (Yt, Yt+1) is uniformly bounded in L2([0, 1]) and Theorem 2.1 of Gobbi
and Mulinacci (2017) applies, allowing to conclude that the process is β-mixing.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

4 An infeasible estimator of the autoregressive parameter

This section is devoted to the analysis of the consistency of the ordinary least squares (OLS) esti-
mator, ˆφT , of the autoregressive coeﬃcient φ in our DIG-AR(1) markovian model. Unfortunately,
unlike the standard case, the OLS estimator is no more consistent. Therefore, we propose an infea-
sible estimator which is by construction consistent and that it is proved to be √T -asymptotically
normal.

We recall that given a random sample (Y1, ...., YT ) generated by (Yt)t, the OLS estimator is
. In the standard AR(1) model, since (Yt)t is a stationary and ergodic process we

t=2 YtYt−1
t=2 Y 2
t−1

ˆφT = PT
PT
have that ˆφT
is no more the case in our more general context.

a.s.
−→

φ as T

−→

∞

+

(see, among others, Anderson, 1971 and Hamilton, 1994). This

Proposition 4.1. Let (Yt)t be a markovian DIG-AR(1) process as in Deﬁnition 2.1. Then, the
OLS estimator ˆφT of φ is not consistent; in particular

where ¯τ (ρ, φ) is deﬁned in (3).

ˆφT

a.s.
−→

¯τ (ρ, φ)

Let’s observe that, coherently with the standard AR(1) process, the OLS estimator converges
to the limit of the ﬁrst-order autocorrelation. Unfortunately, in our DIG-AR(1) model such a
ﬁrst-order autocorrelation does not coincide with the autoregressive parameter.

Proof. First we observe that the assumption of markovianity of (Yt)t implies that, if (
ﬁltration generated by (Yt)t, then P(ξt
x
the assumptions of the model, the conditional distribution of ξt given Yt

1) for all x
1 is

1) = P(ξt

x
|

t
−

|F

Yt

≤

≤

∈

t)t is the
F
R and, thanks to

−

−

(ξt

1)

Yt

−

|

∼

N

(cid:18)

ρσξ
Vt

1

−

5

1, σ2

ξ (1

Yt

−

−

ρ2)

.

(cid:19)

(6)

Now, consider the expression of ˆφT ,

ˆφT =

1

= φ +

T
t=2 YtYt
−
T
t=2 Y 2
t
−

1

P

P

T
t=2 ξtYt
−
T
t=2 Y 2
t
−

1

1

.

P

P

Diﬀerently from the standard case (see Andrews, 1988 and Hamilton, 1994) in our setting the
ratio PT
1)t is
PT
not a martingale diﬀerence sequence and its mean is not constant over time, E[ξtYt
1.
−
However, the process

. In particular, the process (ξtYt

does not converge to zero as T

t=2 ξtYt−1
t=2 Y 2
t−1

1] = ρσξVt

−→

∞

+

−

−

Zt = ξtYt

−

1 −

Yt

−

1E[ξt

Yt

−

1]

|

t)t and, since thanks to equation

(7)

is a martingale diﬀerence sequence with respect to the ﬁltration (
Y 2
t−1
(6), Yt
Vt−1 , we have

1] = ρσξ

1E[ξt

Yt

F

−

|

−

ˆφT

φ =

−

+ ρσξ

T
t=2 Zt
T
t=2 Y 2
P
t
−

1

P

Y 2
T
t−1
t=2
Vt−1
T
t=2 Y 2
P
t
−

1

.

P

Since, thanks to (6),

E

Z 2
t

= E

E

Z 2
t |

Yt

−

1

= E

ξ Y 2
σ2
t
−

1(1

−

ρ2)

= σ2

ξ V 2
t
−

1(1

−

ρ2)

(8)

(9)

and Vt is a convergent deterministic sequence and hence bounded by a positive constant ∆, we
have that

(cid:2)

(cid:3)

(cid:2)

(cid:2)

(cid:3)(cid:3)

(cid:2)

(cid:3)

E

sup
t

Z 2
t

= σ2

ξ (1

ρ2) sup

t

V 2
t
−

1 ≤

σ2
ξ (1

−

−

ρ2)∆2 < +

.
∞

Since the sequence of the second moments of Zt is bounded, we can apply Theorem 3.77 in White
(1984) and conclude that

(cid:2)

(cid:3)

1
T

T

t=2
X

Zt

a.s.
−→

0,

as T

+

.
∞

−→

Notice that, since (Yt)t is β-mixing (see Proposition 3.3), also (Y 2

t )t is β-mixing.

Moreover, (see Bradley, 2007) for a Markov process (Xt)t, (5) can be rewritten as

˜β(

F

t

−∞

,

F

+
∞t,t+k) =

1
2 k

Ft,t+k(x, y)

−

Ft(x)Ft+k(y)

T V

k

k · k

T V is the total variation norm in the Vitali sense and Ft,t+k, Ft and Ft+k are the
where
cumulative distribution functions of (Xt, Xt+k), Xt and Xt+k. If (at)t is a sequence of positive
constants, it can be easily shown that, as a consequence of the Vitali’s total variation norm deﬁni-
tion, the coeﬃcient ˜β(
+
∞t,t+k) calculated for the Markov process (atXt)t is exactly the same

,

t

F
as that calculated for (Xt)t. This allows to conclude that

−∞

F

Since supt

E

Y 4
t
−

1

= 3 supt V 4
t
−

1 and supt

E

Theorem 3.57 in White (1984), we have that
(cid:3)

(cid:2)

Y 4
t−1
Vt−1

h

i

Y 2
t−1
Vt−1
(cid:16)
= 3 supt V 2
t
−

(cid:17)

t

is β-mixing.

1 are both ﬁnite, thanks to

and

1
T

1
T

T

t=2
X

T

t=2
X

Y 2
t
−

1 −

1
T

Y 2
t
−
Vt

−

1
1 −

1
T

T

t=2
X

T

t=2
X

V 2
t
−

1

a.s.
−→

0

Vt

−

1

a.s.
−→

0.

6

But, thanks to Ces`aro averages theorem(see appendix A30 in Billingsley, 1968), we have that
1
T

¯V 2(ρ, φ) and 1
T

¯V (ρ, φ) and

T
t=2 V 2
t
−

1 −→

T
t=2 Vt
−

1 −→

P

P
a.s.
−→

Y 2
t
−

1

1
T

T

t=2
X

¯V 2(ρ, φ) and

1
T

T

t=2
X

Y 2
t
−
Vt

−

1

1

a.s.
−→

¯V (ρ, φ).

Substituting in (8) we get the conclusion.

(10)

It is interesting to analyze the impact of the correlation coeﬃcient ρ on the asymptotic bias of
the OLS estimator for diﬀerent and ﬁxed values of the autoregressive parameter φ. In particular,
¯V (ρ,φ0) : we observe that if ρ and
Figure 2 shows the graph of the asymptotic bias ¯τ (ρ, φ0)
φ0 have the same sign the bias is low and decreases if the value of the autoregressive parameter
raises in absolute value; on the other hand, if ρ and φ0 have opposite sign the bias is high and
tends to increase with the absolute value of the correlation.

φ0 = ρσξ

−

2

1

0

−1

−2

2

1

0

−1

−2

φ
=0.95
0
φ
=−0.95
0

−1

−0.8

−0.6

−0.4

−0.2

φ
=0.5
0
φ
=−0.5
0

−1

−0.8

−0.6

−0.4

−0.2

0
ρ

0
ρ

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Figure 2: Graph of the asymptotic bias ¯τ (ρ, φ0)
values of the autoregressive parameter φ0 Here we ﬁx σξ = 1.

−

φ0 = ρσξ

¯V (ρ,φ0) as a function of ρ for diﬀerent

Thanks to Proposition 4.1 we can introduce a new infeasible estimator ˜φ of φ, which is consistent

by construction, deﬁned as

where ˆφ is the OLS estimator of φ.

˜φT = ˆφT

ρσξ

−

Y 2
T
t−1
t=2
Vt−1
T
t=2 Y 2
P
t
−

1

P

(11)

The following proposition provides a √T -asymptotic normality for ˜φT .

Theorem 4.1. Let (Yt)t be a markovian DIG-AR(1) process as in Deﬁnition 2.1 and ˜φT be the
infeasible estimator given in (11). Then

where ¯η =

ρ2

σξ√1

−
¯V (ρ,φ)

√T ( ˜φT

φ)

L
−→

X

∼

−

N (0, ¯η),

and

L
−→

denotes the convergence in distribution.

7

 
 
 
 
(12)

Proof. By (8) we have

√T ( ˜φT

φ) =

−

1
√T
1
T

T
t=2 Zt
T
t=2 Y 2
P
t
−

1

.

¯V 2(ρ, φ),
where (Zt)t is the martingale diﬀerence deﬁned in (7). Since, see (10), 1
T
in order to prove the statement, we apply the central limit theorem of Proposition 7.8 in Hamilton
(1994) to the numerator in (12). Let us show that the assumptions of that Proposition are satisﬁed.
t > 0 for all t and, using

: thanks to (9) and the assumptions of the model, σ2

T
t=2 Y 2
t
−

P

P

a.s.
−→

1

Let σ2

t = E
Ces`aro averages theorem,

Z 2
t

(cid:2)

(cid:3)

1
T

T

t=2
X

σ2
t = σ2

ξ (1

ρ2)

1
T

−

T

t=2
X
φ
|

|

V 2
t
−

1 −→

σ2
ξ (1

−

ρ2) ¯V 2(ρ, φ) = ¯σ2,

ρ

E

which is strictly positive for all
|
|
Z 4
< +
to show that supt
t
∞
T
remains to prove that 1
t=2 Z 2
(cid:3)
t
T
ξ Y 2
= σ2
1(1
t
P
−

Since E

Z 2
t |

Yt

−

(cid:2)

1

¯σ2.

P

−→
−

ρ2), the process (Wt)t deﬁned as

< 1. Moreover, it is a straightforward computation
< 1 and all
and, in order to apply the mentioned central limit theorem, it

(cid:2)

(cid:3)

Wt = Z 2

t −

ξ Y 2
σ2
t
−

1(1

−

ρ2)

is a martingale diﬀerence sequence with respect to the ﬁltration (
F
< +
(Yt)t. Moreover, since it can be easily shown that supt
∞
in White (1984) to the martingale diﬀerence (Wt)t and conclude that

W 2
t

E

t)t generated by the process
, we can apply Theorem 3.77

(cid:2)

(cid:3)

1
T

T

t=2
X

Wt

a.s.
−→

0.

Wt + σ2

ξ (1

ρ2)

1
T

−

Therefore

1
T

Z 2

t =

1
T

T

t=2
X

T

t=2
X

T

t=2
X

Y 2
t
−

1

a.s.
−→

¯σ2,

1
as required. Hence, by Proposition 7.8 in Hamilton (1994), we have that
√T
distribution to a random variable distributed as N (0, ¯σ) and, in conclusion,

T
t=2 Zt converges in

P

√T ( ˜φT

φ)

L
−→

X

∼

−

N

0,

which is the statement of the theorem.

5 Concluding remarks

σξ

1

ρ2

−
¯V (ρ, φ) !
p

,

The paper proposes a modiﬁed version of the standard AR(1) process allowing for a time-invariant
1 and the next innovation ξt. This induces temporal
dependence between the state variable Yt
dependence in the sequence of innovations and the standard methods of derivation of the asymptotic
properties of the estimators of parameters are no longer applicable.
In fact, we show that the
standard OLS estimator, generally used for the estimation of the autoregressive parameter, is no
more consistent and its asymptotic bias is computed. As a consequence we propose a new infeasible
estimator and we establish its √T -asymptotic normality.

−

References

[1] Anderson T.W. (1971): The Statistical Analysis of Time Series, New York, Wiley.

8

 
[2] Beare B. (2010): ”Copulas and temporal dependence”, Econometrica, 78(1), 395-410.

[3] Billingsley P. (1968): Convergence of Probability Measures. John Wiley & Sons, New York.

[4] Bradley R.C. (2007): Introduction to strong mixing conditions, vols. 1-3. Kendrick Press. Herber

City.

[5] Brockwell P. J. - Davis R. A. (1991) Time Series. Theory and Methods, Springer Series in

Statistics. Springer-Verlag.

[6] Chen X., Fan Y. (2006): ”Estimation of Copula-Based Semiparametric Time Series Models”,

Journal of Econometrics, 130, 307335.

[7] Cherubini U., Gobbi F., Mulinacci S., Romagnoli S. (2012): Dynamic Copula Methods in

Finance, John Wiley & Sons.

[8] Cherubini U., Gobbi F., Mulinacci S., (2016): Convolution Copula Econometrics, Springer-

Briefs in Statistics.

[9] Cherubini, U., Mulinacci S., Romagnoli S. (2011): ”A Copula-based Model of Speculative Price

Dynamics in Discrete Time”, Journal of Multivariate Analysis, 102, 1047-1063.

[10] Darsow W.F. - Nguyen B. - Olsen E.T.(1992): ”Copulas and Markov Processes”, Illinois

Journal of Mathematics, 36, 600-642.

[11] Durante F., Sempi C. (2015): Principles of copula theory, Boca Raton: Chapman and

Hall/CRC

[12] Gobbi F., Mulinacci S. (2017): ”β-mixing and moments properties of a non-stationary copula-

based Markov process”, http://arxiv.org/abs/1704.01458

[13] Hamilton J. D. (1994): Time Series Analysis, Princeton University Press.

[14] Joe H. (1997): Multivariate Models and Dependence Concepts, Chapman & Hall, London

[15] Nelsen R.(2006): An Introduction to Copulas, Springer

[16] White H. (1984): Asymptotic theory for econometricians, Academic Press, New York.

[17] Volkonskii V.A., Rozanov Yu.A. (1959): ”Some limit theorems for random functions I”, Theor.

Probab. Appl., 4, 178-197.

[18] Volkonskii V.A., Rozanov Yu.A. (1961): ”Some limit theorems for random functions II”,

Theor. Probab. Appl., 6, 186-198.

9

