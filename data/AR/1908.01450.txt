0
2
0
2

r
p
A
5
1

]

V
C
.
s
c
[

3
v
0
5
4
1
0
.
8
0
9
1
:
v
i
X
r
a

TopoTag: A Robust and Scalable
Topological Fiducial Marker System

Guoxing Yu, Yongtao Hu, Jingwen Dai, Member, IEEE

1

Abstract—Fiducial markers have been playing an important role in augmented reality (AR), robot navigation, and general applications
where the relative pose between a camera and an object is required. Here we introduce TopoTag, a robust and scalable topological
ﬁducial marker system, which supports reliable and accurate pose estimation from a single image. TopoTag uses topological and
geometrical information in marker detection to achieve higher robustness. Topological information is extensively used for 2D marker de-
tection, and further corresponding geometrical information for ID decoding. Robust 3D pose estimation is achieved by taking advantage
of all TopoTag vertices. Without sacriﬁcing bits for higher recall and precision like previous systems, TopoTag can use full bits for ID
encoding. TopoTag supports tens of thousands unique IDs and easily extends to millions of unique tags resulting in massive scalability.
We collected a large test dataset including in total 169,713 images for evaluation, involving in-plane and out-of-plane rotation, image
blur, different distances and various backgrounds, etc. Experiments on the dataset and real indoor and outdoor scene tests with a rolling
shutter camera both show that TopoTag signiﬁcantly outperforms previous ﬁducial marker systems in terms of various metrics, including
detection accuracy, vertex jitter, pose jitter and accuracy, etc. In addition, TopoTag supports occlusion as long as the main tag topological
structure is maintained and allows for ﬂexible shape design where users can customize internal and external marker shapes. Code for our
marker design/generation, marker detection, and dataset are available at http://herohuyongtao.github.io/research/publications/topo-tag/.

Index Terms—Fiducial Marker, Monocular Pose Estimation, Topological Information, Marker Design, ID Decoding.

(cid:70)

1 INTRODUCTION

I N this paper, we introduce TopoTag, a new ﬁducial

marker and detection algorithm that is more robust and
accurate than current ﬁducial marker systems. Fiducial
markers are artiﬁcial objects (typically paired with a detec-
tion algorithm) designed to be easily detected in an image
from a variety of perspectives. They are widely used for
augmented reality and robotics applications because they
enable localization and landmark detection in featureless
environments [1]. Previous work on ﬁducial markers mainly
focus on one or more of the following areas: (1) improving
detection accuracy via specialized tag design [2], [3], [4],
[5]; (2) reducing pose estimation error via precise vertex
estimation [6] or introducing more feature points [7]; (3)
increasing unique identities [8], [9], [10], [11]; (4) improving
robustness under occlusion [7], [12] and other use cases [13],
[14], [15], [16] and (5) speed-up [1], [6], [17], [18].

TopoTag utilizes topological information in tag design
to improve robustness, which achieves perfect detection
accuracy on the large dataset we collected and on datasets
from others. We show that all tag bits can be used to en-
code identities without sacriﬁcing detection accuracy, thus
achieving rich identiﬁcation and massive scalability. In addi-
tion, TopoTag offers more feature point correspondences for
better pose estimation. Results show that TopoTag achieves
the best performance in vertex jitter, pose error and pose
jitter. TopoTag also supports occlusion and noise, to some
extent, if the main tag topological structure is maintained
and supports ﬂexible shape design where users can cus-

• G. Yu, Y. Hu and J. Dai are with Guangdong Virtual Reality Technology

Co., Ltd. (aka. Ximmerse)
E-mail: {calvin.yu, ythu, dai}@ximmerse.com

Manuscript received [Month] [Date], [Year]; revised [Month] [Date], [Year].

Fig. 1. Three TopoTag markers. TopoTag supports both customized
internal and external shapes. Here shows three TopoTags with various
internal shapes like squares, circles, hexagons and different external
shapes like square and butterﬂy.

tomize internal and external marker shapes. Fig. 1 shows
three TopoTag markers.

We collected a large dataset including 169,713 images
with TopoTag and several state-of-the-art tag systems. A
robot arm is used to make sure each tag has the same
trajectory for consistent comparison. The rich modalities
of the dataset include in-plane and out-of-plane rotations,
image blur, different distances and various backgrounds,
etc. which offer a challenging benchmark evaluation.

In summary, the contributions of this paper are: (1)
we present TopoTag, a topological-based ﬁducial marker
system and detection algorithm; (2) we demonstrate that
TopoTag achieves the best performance in various metrics
including detection accuracy, localization jitter and accuracy,
etc. while at the same time supports occlusion, noise and
ﬂexible shapes; (3) we show that it’s possible in tag design
to use full bits for ID encoding without sacriﬁcing detection
accuracy, thus achieving scalability; and (4) we collect a
large dataset of various tags, involving in-plane and out-of-
plane rotation, image blur, different distances and various

 
 
 
 
 
 
backgrounds, etc.

The remainder of the paper is organized as follows:
In Section 2, we discuss related work in different marker
patterns. We introduce the TopoTag design and detection
algorithm in Section 3 and Section 4 respectively. Dataset
and experimentation are discussed in Section 5. Section 6 is
devoted to the conclusions.

2 RELATED WORK

Fig. 2 shows many different ﬁducial marker systems dis-
cussed in this section.

Circular patterns. Among the earliest work, Gatrell et
al. [19] propose to use concentric contrasting circle (CCC)
for ﬁducial marker design. It’s further enhanced in [8] by
adding colors and multiple scales. In [20], [21], dedicated
data rings are added to the marker design for rich identiﬁ-
cation. Sattar et al. [22] and Xu et al. [23] propose FourierTag
with a frequency image as the signature. In RuneTag [7], [24]
and Pi-Tag [26], they propose using rings of dots to improve
robustness to occlusion and provide more points for pose
estimation. CCTag [14], [25] and followed work by Prasad et
al. [13] use multiple rings to increase robustness to blur and
ring width for encoding. Circular patterns, e.g. RuneTag,
provide the state-of-the-art for most identities. However,
the tracking distance is usually limited due to their require-
ment of ﬁnding enough conﬁdent ellipses. In comparison,
TopoTag can provide even more identities while at the same
time offering much larger tracking range.

Square patterns.

To be easily localized, most ﬁducial
systems are designed to contain a thick square border.
Matrix [27], CyberCode [29] and VisualCode [30] are the
ﬁrst and simplest proposals. ARToolkit [28] is well known
and widely used in many augmented reality applications. It
includes a pattern in their internal region for identiﬁcation
via image correlation. ARTag [2] and ARToolkitPlus [31]
improve the recognition technique with a binary coded pat-
tern. In addition, they are designed with an error correction
mechanism to increase robustness. BinARyID [9] proposes a
method to generate markers that attempt to avoid rotation
ambiguities. Schweiger et al. [33] propose using SIFT and
SURF ﬁlters that are speciﬁcally designed for SIFT and
SURF detectors. Tateno et al. [32] propose using nested
markers to improve performance under different distances.
Several works investigate using multiple ﬁducial markers
in a checkerboard to improve camera calibration [4] and
reduce the perspective ambiguity by further adding color
[10]. AprilTag [5], [6] is a faster and more robust reimple-
mentation of ARTag. Garrido-Jurado et al. [11], [12], [18]
propose ArUco using mixed integer programming to gen-
erate markers. ChromaTag [1] uses color over AprilTag to
improve marker detection speed. Square patterns are most
popular among practical applications due to this technique’s
detection robustness and large tracking range. However,
some encoding bits must be reserved to handle rotation
ambiguities and incorporate Hamming distance strategy.
In contrast, TopoTag can provide much richer identities by
encoding full bits while at the same time achieving the state-
of-the-art robustness and tracking range. Moreover, unlike
square markers using four corner points for pose estimation

2

(which is the minimum number for unambiguous pose esti-
mation [40]), TopoTag offers better pose estimation utilizing
all vertices of tag bits. It’s worth noting that [10] shows the
possibility of reducing rotation ambiguities, increases rich
identities by adding color information and achieves better
pose accuracy by using more inner corners. However, it still
needs to reserve some bits for error detection and correction.
In comparison, TopoTag offers even richer identities without
using color due to the unique baseline node design and
can utilize more feature correspondences for better pose
estimation.

Topological patterns. D-touch [34], [35] is the earliest
work to use topological patterns in tag design. Marker
detection is based on the region adjacency tree information.
D-touch employs a single topology for all markers in the
set and does not provide a speciﬁc method for computing
location and orientation. ReacTIVision [36], [37], [38] im-
proves over D-touch and provides unique identities purely
with the topological structure by building a left heavy
depth sequence of the region adjacency graph. BullsEye [39],
which is specially optimized for GPU, consists of a central
white dot surrounded by a solid black ring and one or more
data rings again surrounded by a solid white ring inside
a black ring with three white studs. Topological patterns
demonstrate the ability to improve robustness using topo-
logical information. However, they (including ReacTIVision
and BullsEye) can only recover 2D location and orientation
due to the lack of sufﬁcient matched feature points. In
comparison, TopoTag offers accurate 3D pose estimation
and state-of-the-art robustness at the same time.

Machine learning. Claus et. al [41], [42] use trained
classiﬁers to improve detection in cases of insufﬁcient il-
lumination and blurring caused by fast camera movement.
Randomized forests are also used to learn and detect pla-
nar objects [43], [44]. Machine learning methods show the
potential to detect natural objects. However, in practice,
these algorithms do not achieve detection accuracies on par
with detection algorithms speciﬁcally designed for marker
detection [1]. In contrast, TopoTag achieves the state-of-
the-art detection accuracy over machine learning and other
previous types of patterns.

3 TOPOTAG DESIGN

TopoTag utilizes topological structure information in tag de-
sign. This method has been validated with proven increases
in robustness across illumination variation and a reduction
in false detection [34]. Existing ﬁducial marker systems,
especially with square patterns, sacriﬁce tag encoding bits
to handle rotation ambiguities during decoding [10]. Addi-
tional bits will also be reserved for incorporating Hamming
distance strategy in order to improve false positive rejection.
Strong robustness with topological design helps by saving
tag bits for encoding identities. To avoid rotation ambi-
guities, TopoTag introduces baseline node in its topological
structure. The baseline node is specially designed to be
different from other nodes in the tag. TopoTag uses a black
node with two white children nodes inside as the baseline
node and other black nodes, with at most one white child
node, as normal nodes. Note that, baseline node can be
deﬁned with other forms. For example, it can be deﬁned

3

CCC [19]

Cho et al. [8]

Knyaz et al. [20]

InterSense [21]

FourierTag [22] [23] RuneTag [7] [24]

CCTag [14] [25]

Pi-Tag [26]

Prasad et al. [13]

Matrix [27]

ARToolKit [28]

CyberCode [29]

VisualCode [30]

ARToolKitPlus [31]

binARyID [9]

Tateno et al. [32]

SIFTTag [33]

ARTag [2]

AprilTag [5] [6]

ArUco [11] [12] [18]

ChromaTag [1]

D-touch [34] [35]

reacTIVision [36]
[37] [38]

BullsEye [39]

Fig. 2. Existing ﬁducial marker systems.

Fig. 1 shows three different design samples of TopoTag. For
easy searching and model simplicity, in current TopoTag
design, we place all internal nodes uniformly spaced and
compacted into a n × n squared shape.

4 TOPOTAG DETECTION

Fig. 4 outlines main steps of TopoTag detection. Topological
information is extensively used for 2D marker detection,
and further corresponding geometrical information for ID
decoding. 3D pose estimation is achieved by taking advan-
tage of all TopoTag vertices.

4.1 2D Marker Detection

Threshold map estimation.
Similar to the idea of adap-
tive thresholding, we estimate the threshold for each pixel
by analyzing its neighboring pixels. The analysis can be
conducted on the original image, however, in order to deal
with the image noise and blur in real applications, analyzing
a downsampled image (scalar s1) is more accurate, which
also brings speed beneﬁts. Any pixel will be set to α if
its value is less than α to remove pixels that are too dark.
Average values are computed on a local region (window
size w) on the downsampled image. To further handle
the image noise, the downsampled average map can be
further downsampled (scalar s2). The ﬁnal threshold map
is achieved by upsampling the downsampled average map
by s1 × s2 using bilinear interpolation, see Fig. 4b.

Binarization.

Binarization is achieved by comparing
the input image with the threshold map. A minimum bright-
ness (β) is set to ﬁlter regions that are too small (i.e. set to
black if pixel value is less than β). See Fig. 4c for an example
of binarization result.

Topological ﬁltering. After the binarization, we build
the topological tree of the connected binary regions. To ﬁnd

Fig. 3. Topological tree of two TopoTags. Each node in the topological
tree denotes one TopoTag connected component (starting from the
inner biggest white connected component). Except the two white nodes
inside the baseline node, all leaf nodes are used for identify encoding.
The identity encodings for these two markers are 0000000 = 0 and
1111111 = 127 respectively.

with three or more white children nodes for different needs.
Baseline node deﬁnes the search starting position of the
whole tag, thus avoiding checking rotation ambiguities. All
normal nodes are used for identity encoding with 0 denot-
ing no child node and 1 otherwise. The identity encoding
for the two markers shown in Fig. 3 is 0000000 = 0 and
1111111 = 127 respectively.

For pose estimation, instead of using only four border
points in previous square systems [1], [2], [5], [6], [11], [12],
[31] which is the minimum number required, TopoTag offers
more point correspondences resulting in more accurate pose
estimation. Baseline node (more speciﬁcally its two children
nodes) and all normal nodes are all employed as feature
points, thus achieving a better pose estimation.

Note that, as TopoTag design is based on topological
information, there is no restriction for the shapes used in the
tag. Both internal and external shapes can be customized
as long as the desired topological structure is preserved.

4

(a) Input image.

(b) Threshold map.

(c) Binarization.

(d) Topological ﬁltering.

(e) Error correction.

(f) Decoding.

(g) Vertex estimation.

(h) Pose estimation.

Fig. 4. Main steps of TopoTag detection. (Best viewed in color)

the baseline node (including p1 and p2) and determine its
search direction based on whether there are nodes along
the direction with angel tolerance θ2, i.e. p1→p2. Along
the direction, we ﬁnd the node with the largest distance,
i.e. p3. For the remaining nodes, we ﬁrst ﬁnd the node
with largest angle against the baseline direction p1→p2
and then the largest distance along the direction, i.e. p4.
p5 is determined along direction p1→p3, p6 and p7 along
p1→p4. The remaining nodes are determined in order and
in a similar way. After ﬁnding each node, we can simply
map each node to 1 or 0 depending on whether it contains
a white child node or not and then decode the tag based on
the binary code string. For the example shown here and in
Fig. 4, the binary code string is 10000011000110, which is
decoded with ID = 8390. It’s worth noting that ID decoding
is processed on the images after removing the perspective
distortion in which lines will still be lines in images with no
distortion to improve the robustness of direction searching.

4.3 3D Pose Estimation

}.

For each node, we estimate the vertex by computing the
centroid on the original image of its supporting region. The
supporting region can be the binary mask or its dilated
version (with dilate size δ). The centroid can be determined
via image moments, i.e. {¯u, ¯v} = { M10
M00

, M01
M00

For pose estimation, the exact correspondence between
the 2D image features and the features of the associated
model is needed (feature correspondence). At least four
points are needed to recover unambiguous pose estimation
for planar tags [40]. Unlike most of previous work using
only four corner points, all TopoTag vertices of tag bits
are used for a better pose estimation. As reported in [45],
a larger number of feature correspondences consistently
leads to lower error and better pose estimation to noise
for various PnP methods. We refer the reader to [46] for a
detailed analysis on the stability of homography estimation
by 1st-order perturbation theory. For 16-bit tag, 16 vertex

Fig. 5. Vertex decoding order. (Best viewed in color)

candidate tags, we search the tree based on two conditions:
(1) the number of children nodes should be within [ζmin −
τ, ζmax + τ ], where ζmin is the number of nodes for tag ID =
0 with all black leaves except the baseline node and ζmax for
the tag with maximum ID with no black leaves, and τ is the
tolerance level allowed; (2) max depth of the tree should be
exactly 3. See Fig. 3 for examples of the topological trees for
both ζmin and ζmax cases of 9-bit TopoTags. Fig. 4d shows
the result after the topological ﬁltering.

Error correction.

There are possible error nodes within
the tag region due to noise or occlusion. Fig. 4d shows an
example of one error node close to the baseline node because
of one ant sitting on the tag. To correct these error nodes, we
ﬁrst compute the area of the baseline node and then ﬁlter out
smaller nodes if their areas are less than θ1% of the baseline
node area. Fig. 4e shows the result after error correction.

4.2 ID Decoding

To decode ID, we need to determine the node sequence and
map it to a binary code string. Take a 16-bit TopoTag as
an example, see Fig. 5 and Fig. 4f of the sequence where
we ﬁnd for each node of the tag. To start, we ﬁrst ﬁnd

correspondences are used, including two baseline white
nodes and 14 normal black nodes. 6-DoF pose estimation
is achieved by solving the PnP problem and Levenberg-
Marquardt algorithms [47], [48] based on these feature cor-
respondences.

5 RESULTS AND DISCUSSION
Algorithm setup.
Throughout the experiment, we use
s1 = 4, s2 = 8, w = 5, α = 45, β = 50 for segmentation, τ =
0, θ1 = 30, θ2 = 0.1 rad for decoding, δ = max{2, (cid:4) l
(cid:5)} for
vertex estimation, where l is the short length of the binary
mask region.

10

All of the experiments have been performed on a typical
laptop PC equipped with an Intel Core i7-7700HQ processor
(8 cores @2.8Ghz) and 8GB of RAM.

5.1 Dataset

The previous work, like [5], [7], mainly focused on eval-
uating performance on synthetic images. Although some
of the work evaluated parts of the performance on more
realistic scenes, e.g. ARToolKitPlus [31] evaluates the speed
on several handheld devices and AprilTag [5], [6] evaluates
false positive on LabelMe [49] dataset which is designed for
general object detection and recognition research, there is
still no uniform dataset for ﬁducial marker evaluation. This
makes it difﬁcult to reproduce the result and compare with
others. More recently, in ChromaTag [1] work, they collected
a dataset to compare their work with AprilTag [5], CCTag
[14], and RuneTag [7]. However, different tags are placed
side-by-side during their dataset collection, thus it is not
ideal for comparison, especially when tags viewed from a
large angle as different markers will have different distances
and facing angles towards the camera.

In this work, we try to ﬁll this gap by collecting a large
dataset, including a total of 169,713 images, which include
in-plane and out-of-plane rotations, image blur, various
distances and cluttered backgrounds, etc. Please refer to the
supplementary material for details of our dataset variations.
We use an industrial camera with a global shutter that has
1280×960 resolution streaming at 38.8 fps and 98◦ diagonal
ﬁeld of view. The exposure time is ﬁxed at 10 ms. Using
relatively long exposure guarantees sufﬁcient brightness of
the captured images, which at the same time introduces the
image blur phenomenon for more challenging use cases (see
the ﬁrst image in Fig. 7 for an example). The camera is
ﬁxed to a robot arm1 to ensure the same trajectories for
different tags. Fig. 6 shows the dataset collection setup.
Three sequences will be collected for each tag, and the
trajectory for each sequence is shown in Fig. 8. In all the
three sequences, the camera keeps facing the front as shown
in the ﬁrst image of Fig. 8. In Seq #1, the camera moves along
several lines at a constant speed, with different out-of-plane
rotations for each line including 0◦ (i.e. camera faces the
tag right ahead), 30◦ and 60◦. In Seq #2, the camera moves
back and at the same time rotates in-plane within 0-180◦ at
a constant speed back and forth. Note that, as we can only
rotate around the end joint of the robot arm and there is an

1. We use a robot arm from DENSO (VS-6556). Link: https://www.

denso-wave.com/en/robot/product/ﬁve-six/vs.html

5

Fig. 6. Dataset collection setup. We collect dataset by putting tags (label
#2) in a rich textured background of an indoor environment with ﬁxed
lighting (label #1). The camera (label #3) is ﬁxed to a robot arm (label
#4) to ensure the same trajectories for different tags.

Fig. 7. Sample images from the dataset. Images are from Seq #1 (with
ARToolKit), Seq #2 (with AprilTag 25h9) and Seq #3 (with TopoTag) from
left to right respectively.

offset between the camera and arm, the camera’s trajectory
will not be an ideal half circle. In Seq #3, the camera is placed
at 10 ﬁxed positions (P1→P10). Besides 0◦, 30◦and 60◦ out-
of-plane rotations as in Seq #1, we further collect data with
75◦ (P1 and P10). In all three sequences, the background is
ﬁlled with rich textured images to simulate more complex
use scenarios.

We collect the dataset for TopoTag and previous tags
including ARToolKit [28], ARToolKitPlus [31], ArUco [12],
RuneTag [7], ChromaTag [1] and AprilTag [6]. A 16-bit
TopoTag is used throughout the experiment as it provides
the most unique identities, see Tab. 1 for details. And,
without loss of generality, the tag comes with square internal
and external shapes (see the ﬁrst image in Fig. 1). For
systems with multiple tag families, we collect data for each
tag family, including 16h3, 25h7, 36h12 for ArUco and 16h5,
25h7, 25h9, 36h9, 36h11 for AprilTag. For each tag family
(including TopoTag), we randomly select one ID for evalu-
ation. In our experiment, we randomly selected ID = 1 for
ARToolKit, 262 for ARToolKitPlus, [104, 90, 136] for ArUco’s
[16h3, 25h7, 36h12], 107 for RuneTag, 0 for ChromaTag, [0,
204, 25, 1314, 343] for AprilTag’s [16h5, 25h7, 25h9, 36h9,
36h11] and 278 for TopoTag. Note that, for AprilTag, there
are three shared tag families, i.e. 16h5, 25h9 and 36h11, for
AprilTag-1 [5] and AprilTag-2 [6], and 25h7, 36h9 only exist
in AprilTag-1. In following sections, we will report the best
result of AprilTag-1 and AprilTag-2 for these shared tag
families if not otherwise speciﬁed. For evaluation fairness,
outer border sizes of all tags are kept at the same of 5 cm.
For each tag, there are ≈100,000 images collected, including
≈1,000 for Seq #1, ≈1,200 for Seq #2 and ≈7,800 for Seq #3.
Please see Fig. 7 for sample images for each sequence.
It’s worth noting that segmentation is crucial
for
marker detection and pose estimation for all marker
for fair comparison, we ﬁne tune the
systems. Thus,

6

Fig. 8. Robot arm trajectory/points in different sequences (1st image for Seq #1, 2nd image for Seq #3). Camera trajectory is shown for Seq #2 for
better visualization (3rd and 4th images). Tag position is shown in blue. (Best viewed in color)

segmentation parameters for each marker algorithm un-
less it already uses advanced approaches like adaptive
thresholding,
line detection, etc. Speciﬁcally, we use a
threshold of 60 instead of default 100 for ARToolKit,
15 and 2 for AdaptiveThresholdWindowSize and
AdaptiveThresWindowSize_range instead of default -1
and 0 for ArUco. Please refer to the supplementary mate-
rial for the performance comparison between their default
setups and our ﬁnely tuned versions.

5.2 Dictionary Size vs. Tracking Distance

Tab. 1 shows the comparison of dictionary size vs. tracking
distance (both min and max) of different tag systems. Gen-
erally speaking, more tag bits offer more spaces to encode
identities, but sacriﬁce maximum tracking distance as region
for each bit becomes smaller. On the other hand, minimum
tracking distance is affected by the marker occlusion be-
cause of the camera FoV limitation and blur issue at the
small range of a ﬁxed-focus camera. Fig. 9 shows the images
of TopoTag at minimum and maximum tracking distance
respectively. TopoTag achieves a state-of-the-art minimum
tracking distance, which further demonstrates the robust-
ness of TopoTag under partial occlusion and out-of-focus
image blur. TopoTag also achieves comparable maximum
tracking range when the dictionary size is small (9-bit),
while offering a signiﬁcantly larger tracking range when
the dictionary size extends to tens of thousands (16-bit
vs. RuneTag with the state-of-the-art most identities). In
addition, TopoTag offers the scalability of extending the
dictionary size to millions with a still acceptable tracking
distance (25-bit). Interestingly, AprilTag-2 achieves better
minimum tracking range but worse maximum tracking
range over AprilTag-1 by a large margin. We suspect that
this is due to the tag detection strategy change from gradient
computing based in AprilTag-1 to adaptive thresholding
based in AprilTag-2 for speedup. It’s worth noting that, we
also tested markers with different sizes, including 2.5 cm
and 10 cm. The conclusions for both min and max tracking
ranges still hold.

2. 10 tags are provided in ARToolKit package. Theoretically, any
pattern can be used for tag design, but the author didn’t provide the
approach.

TABLE 1
Dictionary size vs. tracking distance. For shared tag families of
AprilTag-1 and AprilTag-2, results of both versions are reported with
format “AprilTag-1→AprilTag-2”.

Tag

ARToolKit
ARToolKitPlus
ArUco (16h3)
ArUco (25h7)
ArUco (36h12)
RuneTag
ChromaTag
AprilTag (16h5)
AprilTag (25h7)
AprilTag (25h9)
AprilTag (36h9)
AprilTag (36h11)
TopoTag (3x3)
TopoTag (4x4)
TopoTag (5x5)

Dictionary
Size
102
512
250
100
250
17,000
30
30
242
35
5,329
587
128
16,384
8,388,608

Min
Distance (m)
0.047
0.087
0.117
0.117
0.120
0.103
0.547
0.161 → 0.043
0.160
0.156 → 0.040
0.163
0.163 → 0.042
0.029
0.029
0.029

Max
Distance (m)
1.199
1.154
1.309
1.187
1.199
0.221
0.560
1.220 → 0.757
1.171
1.226 → 0.968
1.223
1.168 → 0.906
1.204
1.055
0.670

Fig. 9. Images of TopoTag at minimum and maximum tracking distance
respectively. Note that, there are partial marker occlusion because of the
camera FoV limitation and blur issues especially at the min distance.

5.3 Detection Accuracy

Tab. 2 summarizes the detection results for TopoTag com-
pared to previous marker systems. Fig. 10 highlights the
recall and precision of different captured points on Seq #3.
We follow the metrics used in [1]. True positives (TP) are
deﬁned as when the tag is correctly detected, including
locating the tag and correctly identifying the ID. Correct
identiﬁcation of the tag is determined by having at least
50% intersection over union between the detection and the
ground truth. False positives (FP) are deﬁned as detections

TABLE 2
Detection accuracy (with run time). For shared tag families of
AprilTag-1 and AprilTag-2, run time of both versions are reported with
format “AprilTag-1→AprilTag-2”.

Tag
ARToolKit
ARToolKitPlus
ArUco (16h3)
ArUco (25h7)
ArUco (36h12)
RuneTag
ChromaTag
AprilTag (16h5)
AprilTag (25h7)
AprilTag (25h9)
AprilTag (36h9)
AprilTag (36h11)
TopoTag

Recall (%)
99.990
98.297
100.000
99.009
99.470
0.281
9.088
77.285
75.711
80.405
78.704
100.000
100.000

Precision (%)
99.880
100.000
99.910
100.000
100.000
100.000
9.190
99.883
100.000
100.000
100.000
99.990
100.000

Time (ms)
5.864
9.314
54.319
53.930
56.001
455.832
9.103
246.762 → 15.114
244.433
251.275 → 13.603
240.694
241.314 → 13.431
33.638

7

Fig. 10. Recall and precision by different points on Seq #3. (Best viewed
in color)

returned by the detection algorithms that do not identify the
location and ID correctly. False negatives (FN) are deﬁned
as any marker that is not identiﬁed correctly. Precision is
T P +F P and recall is

T P
T P +F N .

T P

Tab. 2 shows that TopoTag performs perfectly on all three
sequences, achieving 100% across both recall and precision.
All tested marker systems, except ChromaTag, work great
and achieve > 99.5% on precision due to their unique false
positive rejection techniques. However, most systems except
ARToolKit, ARToolKitPlus and ArUco fail to achieve a high
recall, i.e. < 81%. Fig. 10 shows that all previous systems
degrade on recall or precision or both when markers are
viewed from wide angles. This result is probably from
large distortion, decreased lightness and blur issues, which
distract marker detection. TopoTag, on the other hand, has
no obvious degradation on these issues.

ChromaTag performs worse on both recall and preci-
sion possibly due to the cluttered colored background and
relative low brightness of collected images distracting its
detection based on color information. For ablation study,
we tried to replace all ChromaTag’s background with pure
white pixels and keep only the marker region. With such
a setup, ChromaTag achieves the same recall (i.e. same
number of false negatives) and the number of false posi-
tives decreased from 8962 to 257, which further validates
that ChromaTag is sensitive to cluttered background (i.e.
detecting false positives).

RuneTag performs the worst with lowest recall < 0.3%
and fails to detect any frame on Seq #3 where it fails to ﬁnd
enough conﬁdent ellipses on the images. As also found in
ChromaTag work [1], RuneTag requires larger tag sizes for
detection, which is the major cause of its lesser performance
on our dataset with small marker size in long distance and
challenging blur. In our experiment, we found that RuneTag
cannot be detected when the marker is smaller than 180 ×
180 pixels.

False Positive Rejection.

Since all of the images in
our dataset contain valid tags, FP mainly focuses on the
background excluding the tag regions. To better evaluate
FP, as in [6], we further run the experiment on LabelMe [49]
dataset, which consists of 207,8833 images of natural scenes

3. This is the latest LabelMe dataset size, which is slightly different

the size of 180,829 from that was used in [5], [6].

Fig. 11. Pose position (left) and rotation (right) error comparison. We
have trimmed the ﬁgures for better visualization. Please refer to the
supplementary material for full ﬁgures. (Best viewed in color)

from a wide variety of indoor and outdoor environments,
none of which contain any valid ﬁducial markers. We run
this test for ARToolKit, ARToolKitPlus, ArUco, AprilTag and
TopoTag as they achieve top detection accuracy results on
our dataset as shown in Tab. 2. In addition, we further run
this test for reacTIVision [36], [37], [38] which only recovers
2D location and orientation by default. There are 49321 false
positives returned by AprilTag (16h5), 9756 by ARToolKit,
348 by reacTIVision and 146 by ArUco (16h3). In contrast,
TopoTag and ARToolKitPlus both have no false positives.

5.4 Localization Jitter and Accuracy

We evaluate localization jitter (including 6-DoF pose jitter
and 2D vertex jitter) and accuracy (i.e. 6-DoF pose accuracy)
on Seq #3.

5.4.1 Pose Error

We evaluate the accuracy between each point and its
adjacent point. The robot’s measurements serve as the
groundtruth. Since there are in total 10 points in Seq #3, nine
accuracy values will be computed. See Fig. 11 for the results
of both position and rotation accuracies. Average and max-
imum pose errors for each tag are listed in Tab. 3. TopoTag
outperforms all previous systems in position error by a
large margin (about 28% error reduction by average and
14% by max compared to the 2nd best) and is comparable
with the state-of-the-art on rotation error (<0.1 degree for
both average and max). A further two-sample Kolmogorov-
Smirnov test shows that TopoTag signiﬁcantly outperforms
the 2nd best (i.e. AprilTag) in position error with p = 0.000.

12345678910Index0102030405060708090100RecallRecall by different pointsARToolKitARToolKitPlusArUco (16h3)ArUco (25h7)ArUco (36h12)ChromaTagAprilTag (16h5)AprilTag (25h7)AprilTag (25h9)AprilTag (36h9)AprilTag (36h11)TopoTag12345678910Index0102030405060708090100PrecisionPrecision by different pointsARToolKitARToolKitPlusArUco (16h3)ArUco (25h7)ArUco (36h12)ChromaTagAprilTag (16h5)AprilTag (25h7)AprilTag (25h9)AprilTag (36h9)AprilTag (36h11)TopoTagP1P2P2P3P3P4P4P5P5P6P6P7P7P8P8P9P9P10Index051015202530ErrorPose position error (unit: mm)P1P2P2P3P3P4P4P5P5P6P6P7P7P8P8P9P9P10Index00.050.10.150.20.250.30.350.40.450.5ErrorPose rotatoin error (unit: deg)ARToolKitARToolKitPlusArUco (16h3)ArUco (25h7)ArUco (36h12)ChromaTagAprilTag (16h5)AprilTag (25h7)AprilTag (25h9)AprilTag (36h9)AprilTag (36h11)TopoTagTABLE 3
Average and maximum pose errors of each tag. Best results are shown
in bold and underlined.

Tag

ARToolKit
ARToolKitPlus
ArUco (16h3)
ArUco (25h7)
ArUco (36h12)
ChromaTag
AprilTag (16h5)
AprilTag (25h7)
AprilTag (25h9)
AprilTag (36h9)
AprilTag (36h11)
TopoTag

position (mm)
max
avg
16.499
8.639
20.101
8.923
21.876
8.191
27.212
10.049
22.663
8.768
45.643
29.586
7.287
2.894
6.641
2.704
7.320
3.178
7.394
3.228
3.824
1.402
3.289
1.011

rotation (deg)
avg
max
0.058
0.022
0.089
0.040
0.908
0.248
0.765
0.225
0.195
0.078
0.158
0.131
0.055
0.031
0.049
0.026
0.041
0.024
0.047
0.024
0.018
0.010
0.068
0.019

5.4.2 Pose Jitter

Both position and rotation jitters are evaluated at each
point using the standard deviation (STD) metric. See Fig. 12
for the result. Average and maximum jitter for each tag
can be seen in Tab. 4. TopoTag outperforms all previous
systems in rotation jitter by a signiﬁcant margin (about 56%
average jitter reduction and 49% by max compared to the
2nd best). This result is comparable with the state-of-the-
art on position jitter (<0.1 mm for average and <0.2 mm
for max). A further two-sample Kolmogorov-Smirnov test
shows that TopoTag signiﬁcantly outperform the 2nd best
(i.e. AprilTag) in rotation jitter with p = 0.000.

5.4.3 Vertex Jitter

Vertex jitter measures the noise of the 2D feature point esti-
mation, whose errors will propagate to the estimation of the
6-DoF pose. To evaluate vertex jitter, we compare two of the
best previous methods, AprilTag and ArUco. Both AprilTag
and ArUco are square markers, which use intersections of
quad lines to achieve sub-pixel vertex precision. RUNE-Tag
and ChromaTag are not evaluated as they fail to reliably
detect all positions in Seq #3, i.e. the number of detected
frames for a point is less than 504. Square markers, like
ARToolKitPlus and ChromaTag, theoretically will have sim-
ilar performance as AprilTag and ArUco. ARToolKit is not
evaluated as it uses correlation against a database to detect
instead of ﬁnding ﬁxed corners. All candidate methods are
evaluated on markers with 16 bits (i.e. AprilTag’s 16h5 and
ArUco’s 16h3). Similar to pose jitter evaluation, STD metric
is used.

Results can be seen in Fig. 13. It is evident that TopoTag
performs consistently the best or comparable to the state-of-
the-art across all points, especially when the marker angles
become greater (e.g. ≥ 60◦) and with more image blur (see
P1, P2, P9 and P10). AprilTag performs better than ArUco
where marker angles are relatively small (≤ 30◦, see P3-P8)
thanks to its edge reﬁnement, but become worse where the
marker has larger angle w.r.t. the camera.

4. ChromaTag fails to reliably detect P2, P4, P5, P6 and P7; and all 10

positions are failed for RUNE-Tag.

8

Fig. 12. Pose position (left) and rotation (right) jitter comparison. We trim
the ﬁgures for better visualization. Please refer to the supplementary
material for full ﬁgures. (Best viewed in color)

TABLE 4
Average and maximum pose jitters of each tag. Best results are shown
in bold and underlined.

Tag

ARToolKit
ARToolKitPlus
ArUco (16h3)
ArUco (25h7)
ArUco (36h12)
ChromaTag
AprilTag (16h5)
AprilTag (25h7)
AprilTag (25h9)
AprilTag (36h9)
AprilTag (36h11)
TopoTag

position (mm)
avg
max
0.481
0.112
3.584
1.134
1.636
0.363
1.155
0.364
2.553
0.573
130.958
49.880
0.163
0.079
0.231
0.104
0.154
0.087
0.222
0.102
0.352
0.074
0.055
0.173

rotation (deg)
max
avg
0.754
0.160
1.496
0.421
0.491
0.230
0.710
0.322
2.832
0.526
14.616
8.479
2.512
0.654
3.160
0.879
2.333
0.673
2.299
0.753
0.416
0.133
0.211
0.058

5.5 Speed

5.5.1 Dictionary Computation

Dictionary computation is usually a time-consuming opera-
tion due to the specially designed lexicode generation algo-
rithm and Hamming distance strategy required to achieve
high detection robustness. Although there is no need to do
dictionary computation online normally, it’s still meaningful
to make this step efﬁcient enough. ArUco takes approxi-
mately 8, 20 and 90 minutes respectively for dictionaries
of sizes 10, 100 and 1000 [12], while it can take several
days to generate 36-bit tags for AprilTag [5]. As TopoTag
supports full tag bits for identity encoding, it is extremely
fast for dictionary computation as an ID can be directly
mapped to the binary code string. In our experiment, it takes
only 4.1 seconds to generate dictionary of size 8,388,608 (i.e.
TopoTag-5×5).

5.5.2 Tag Detection

The last column of Tab. 2 shows the running time compari-
son. TopoTag takes less time than ArUco (38% ⇓), AprilTag-
1 (86% ⇓) and RuneTag (93% ⇓). Though ARToolKit,
ChromaTag, AprilTag-2 and ARToolKitPlus run faster than
TopoTag, they offer signiﬁcantly less unique identities. See
Tab. 1 for details. For TopoTag, most time is spent on
segmentation (68.8%), followed by decoding and vertex
estimation (29.7%). Pose estimation takes the least time
(1.5%).

It’s worth noting that no parallelization is utilized in
current TopoTag implementation, which will normally bring

12345678910Index00.10.20.30.40.50.60.70.80.91JitterPose position jitter (unit: mm)12345678910Index00.511.522.53JitterPose rotatoin jitter (unit: deg)ARToolKitARToolKitPlusArUco (16h3)ArUco (25h7)ArUco (36h12)ChromaTagAprilTag (16h5)AprilTag (25h7)AprilTag (25h9)AprilTag (36h9)AprilTag (36h11)TopoTag9

Fig. 15. Occlusion test by blocking 40% marker area starting from
away the baseline node. Makers from left
to right are ARToolKit,
TopoTag, RuneTag, ArUco, ARToolKitPlus, ChromaTag, AprilTag-1&2
and AprilTag-3 respectively. (Best viewed in color)

Fig. 13. Average and maximum vertex jitter comparison by different
points on Seq #3. (Best viewed in color)

Fig. 14. Detection and pose estimation of two customized TopoTags.
(Best viewed in color)

further speed-up. To demonstrate the possible applications
on mobile, we have implemented the 2D marker detection
process in a single pipeline on a Lattice FPGA (LFE5UM-45
with 44k LUTs, 1.9 Mb RAM and without using external
DDR), which is decreased to < 100 us achieving 230×
speedup.

5.6 Flexible Shape Support

TopoTag supports both customized external and internal
shapes as long as the topological structure is maintained.
Fig. 1 shows three TopoTags with various internal shapes
like square, circle, hexagon and different external shapes
including square and butterﬂy. Fig. 14 shows our algorithm
running upon these customized TopoTags.

Experiments show that tags with different shapes have
compatible results. Please see Tab. 5 for detailed compari-
son. It’s worth noting that all these four different TopoTags
have 100% result on both detection recall and precision
which further validates the robustness of the TopoTag sys-
tem.

5.7 Occlusion Support

TopoTag can handle occlusion as long as topological struc-
ture is preserved. The left image of Fig. 14 is an example
working under occlusion. Similar to [50], we conduct an
occlusion test by blocking different percentages (10%→100%
with 10% step size) of the marker area. As TopoTag is used
with a unique baseline node, for fairness, we conduct the
occlusion test twice, i.e. one starting from the baseline node
side and the other away from it. Fig. 15 shows an example of
an occlusion test setup with result in Tab. 6. We can see that
all markers except TopoTag and RuneTag fail all occlusion
tests. RuneTag achieves the best occlusion performance with

Fig. 16. 360◦-freedom tracking via using 18 TopoTags on a
rhombicuboctahedron-shaped object. (Best viewed in color)

max 30% occlusion and TopoTag can work well with up
to 10% occlusion. Note that, as shown in above results,
RuneTag has limitations of low detection rate and narrow
tracking range due to its requirement of ﬁnding enough
conﬁdent ellipses.

To handle more severe occlusions, similar to [4], [12],
we can use multiple tags in a grid to increase the prob-
ability of detecting complete markers and other forms
can be also considered. Fig. 16 shows an example of
achieving 360◦-freedom tracking using 18 TopoTags on a
rhombicuboctahedron-shaped object.

5.8 Noise Handing

TopoTag can handle certain noise due to our specially de-
signed threshold map estimation, topological ﬁltering and
error correction. In Fig. 17, we show an example of TopoTag
working under severe noise (adding Gaussian noise σ =
0.45 to the original image) by introducing image smoothing
(i.e. Gaussian blur with kernel size = 5, σx = 5.5, σy = 5.5)
as the pre-processing step.

5.9 Real Scene Test with a Rolling Shutter Camera

Besides the above laboratory testing with a global shutter
camera, we further conduct real indoor and outdoor scene
tests with a rolling shutter camera which is widely used
in mobile phones and other smart devices. Speciﬁcally, we
use a Logitech C930E webcam with 1280×720 resolution at
30 fps and 90 deg diagonal ﬁeld of view. The experiment
is conducted in four different scenarios, including dark,
bright outdoor, shadow and motion blur. Fig. 18 shows the
test setup with TopoTag detection overlay. For dark, bright
outdoor and shadow scenarios, we also evaluate pose jitter
and compare it with existing markers including the latest
AprilTag-3 [51]. For fairness, for markers with multiple tag
families, we randomly select one tag from the tag family

12345678910Point index00.20.40.60.811.21.41.6Vertex jitterAverage vertex jitter (unit: pixel)TopoTagAprilTagArUco12345678910Point index00.20.40.60.811.21.41.61.82Vertex jitterMaximum vertex jitter (unit: pixel)TopoTagAprilTagArUcoTABLE 5
Pose estimation of different bits and shapes.

Different Bits & Shapes

3x3, circle
3x3, square
4x4, circle
4x4, square

Pose Accuracy

Pose Jitter

position (mm)
max
avg
3.299
1.073
2.780
0.837
2.867
0.995
3.289
1.011

rotation (deg)
max
avg
0.048
0.016
0.065
0.022
0.057
0.017
0.068
0.019

position (mm)
max
avg
0.265
0.069
0.383
0.085
0.192
0.058
0.173
0.055

rotation (deg)
max
avg
0.205
0.080
0.276
0.081
0.272
0.073
0.211
0.058

10

TABLE 6
Occlusion test result. “top→bottom occlusion” and “bottom→top
occlusion” means occlusion starting from and away from the baseline
node side respectively.

Tag

ARToolKit
ARToolKitPlus
ArUco
RuneTag
ChromaTag
AprilTag-1&2
AprilTag-3
TopoTag

(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

top→bottom occlusion

bottom→top occlusion

10% 20% 30% ≥40% 10% 20% 30% ≥40%

(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(a) Original image.

(b) Image after adding noise.

Fig. 17. Example of TopoTag working under severe noise. Original image
is from Seq #1. Noise image is obtained by adding Gaussian noise σ =
0.45 to the original image.

with closest and smaller dictionary size compared with
used TopoTag-4x4. Results are evaluated for a ﬁxed length
sequence of 100 frames and results shown in Tab. 7. We
can see that TopoTag, together with ARToolKitPlus and
AprilTag-3, performs well in all test scenarios, while all
other markers fail in at least one scenarios. TopoTag also
achieves the best (9 out of 12) or 2nd best (3 out of 12)
performance in terms of position and rotation jitter for all
scenarios.

5.10 Failure Cases

TopoTag can handle lighting change and motion blur better
due to our unique threshold map estimation and topological
ﬁltering modules, see examples in Fig. 18. However, it will
fail to detect the markers where dramatic lighting change
or severe motion blur happens over the marker region.
Fig. 19 shows two typical failure cases as a result of dramatic
lighting change and severe motion blur. Their binarization
results show that markers’ topological structure is dramati-
cally changed. This change is the root cause of the detection
failure.

6 CONCLUSIONS

We present TopoTag, a new topological-based ﬁducial
marker and detection algorithm that utilizes topological
information to achieve high robustness, and near-perfect de-
tection accuracy. We show that all tag bits can be used to en-
code identities without sacriﬁcing detection accuracy, thus
achieving rich identiﬁcation and scalability. TopoTag offers
more feature correspondences for better pose estimation. We
demonstrate that TopoTag achieves the best performance
in various metrics including detection accuracy, localization
jitter and accuracy, and at the same time supports occlusion
and ﬂexible shapes. We also collected a large dataset of
TopoTag and other previous state-of-the-art tags for better
evaluation, involving in-plane and out-of-plane rotations,
image blur, various distances and cluttered background, etc.
For future research, we will explore novel ID encod-
ing/decoding strategy. We believe that this is key for a better
marker system with a goal of strong occlusion resistance
and scalability in addition to high detection rate and long
distance tracking range.

REFERENCES

[1]

J. DeGol, T. Bretl, and D. Hoiem, “Chromatag: a colored marker
and fast detection algorithm,” in Proceedings of the IEEE Interna-
tional Conference on Computer Vision, 2017, pp. 1472–1481.

[2] M. Fiala, “Artag, a ﬁducial marker system using digital tech-
niques,” in 2005 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR’05), vol. 2.
IEEE, 2005, pp.
590–596.

[3] ——, “Designing highly reliable ﬁducial markers,” IEEE Transac-
tions on Pattern analysis and machine intelligence, vol. 32, no. 7, pp.
1317–1324, 2010.

[4] B. Atcheson, F. Heide, and W. Heidrich, “CALTag: High Precision
Fiducial Markers for Camera Calibration,” Int. Workshop on Vision,
Modeling and Visualization (VMV), 2010.

[5] E. Olson, “Apriltag: A robust and ﬂexible visual ﬁducial system,”
in 2011 IEEE International Conference on Robotics and Automation.
IEEE, 2011, pp. 3400–3407.
J. Wang and E. Olson, “Apriltag 2: Efﬁcient and robust ﬁducial
detection,” in 2016 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS).

IEEE, 2016, pp. 4193–4198.

[6]

[7] F. Bergamasco, A. Albarelli, E. Rodol`a, and A. Torsello, “RUNE-
Tag: A high accuracy ﬁducial marker with strong occlusion re-
silience,” Proceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pp. 113–120, 2011.

[8] Y. Cho, J. Lee, and U. Neumann, “A multi-ring color ﬁducial
system and an intensity-invariant detection method for scalable
ﬁducial-tracking augmented reality,” in IWAR, 1998.

[9] D. Flohr and J. Fischer, “A lightweight id-based extension for
marker tracking systems,” in Eurographics Symposium on Virtual
Environments (EGVE) Short Paper Proceedings, 2007, pp. 59–64.
[10] V. F. da Camara Neto, D. B. de Mesquita, R. F. Garcia, and M. F. M.
Campos, “On the design and evaluation of a precise scalable
ﬁducial marker framework,” in 2010 23rd SIBGRAPI Conference on
Graphics, Patterns and Images.

IEEE, 2010, pp. 216–223.

11

(a) Dark.

(b) Bright outdoor.

(c) Shadow.

(d) Motion blur.

Fig. 18. TopoTag detection in different real scene tests with a rolling shutter camera. (Best viewed in color)

TABLE 7
Average and maximum pose jitters of each tag under different real test scenarios. “—” means detection failure. Best results are shown in bold and
underlined.

Dark

Bright

Shadow

Tag

ARToolKit
ARToolKitPlus
ArUco
RuneTag
ChromaTag
AprilTag-1&2
AprilTag-3
TopoTag

position (mm)
max
avg
—
—
1.301
0.103
—
—
—
—
—
—
0.224
0.065
0.334
0.112
0.105
0.038

rotation (deg)
max
avg
—
—
0.200
0.068
—
—
—
—
—
—
0.139
0.040
0.228
0.060
0.113
0.030

position (mm)
max
avg
—
—
0.020
0.113
0.183
0.056
—
—
—
—
0.118
0.033
0.462
0.044
0.066
0.021

rotation (deg)
max
avg
—
—
0.110
0.030
0.077
0.023
—
—
—
—
0.066
0.018
0.099
0.020
0.025
0.009

position (mm)
max
avg
0.879
0.504
3.711
1.399
7.793
1.768
—
—
—
—
—
—
0.523
0.103
0.220
0.067

rotation (deg)
max
avg
0.210
0.093
0.960
0.469
4.913
1.199
—
—
—
—
—
—
0.099
0.027
0.116
0.039

[15] T. Birdal, I. Dobryden, and S. Ilic, “X-tag: A ﬁducial tag for ﬂexible
and accurate bundle adjustment,” in 2016 Fourth International
Conference on 3D Vision (3DV).

IEEE, 2016, pp. 556–564.

[16] H. Cruz-Hern´andez and L. G. de la Fraga, “A ﬁducial tag invariant
to rotation, translation, and perspective transformations,” Pattern
Recognition, vol. 81, pp. 213–223, 2018.

[17] J. Molineros and R. Sharma, “Real-time tracking of multiple ob-
jects using ﬁducials for augmented reality,” Real-Time Imaging,
vol. 7, no. 6, pp. 495–506, 2001.

[18] F. J. Romero-Ramirez, R. Mu ˜noz-Salinas, and R. Medina-Carnicer,
“Speeded up detection of squared ﬁducial markers,” Image and
Vision Computing, vol. 76, pp. 38–47, 2018.

[19] L. B. Gatrell, W. A. Hoff, and C. W. Sklair, “Robust image features:
Concentric contrasting circles and their image extraction,” in Co-
operative Intelligent Robotics in Space II, vol. 1612.
International
Society for Optics and Photonics, 1992, pp. 235–245.

[20] V. A. Knyaz, “The development of new coded targets for auto-
mated point identiﬁcation and non-contact 3d surface measure-
ments,” IAPRS, vol. 5, pp. 80–85, 1998.

[21] L. Naimark and E. Foxlin, “Circular data matrix ﬁducial system
and robust image processing for a wearable vision-inertial self-
tracker,” in Proceedings of the 1st International Symposium on Mixed
and Augmented Reality.

IEEE Computer Society, 2002, p. 27.

[22] J. Sattar, E. Bourque, P. Giguere, and G. Dudek, “Fourier tags:
Smoothly degradable ﬁducial markers for use in human-robot
interaction,” in Fourth Canadian Conference on Computer and Robot
Vision (CRV’07).

IEEE, 2007, pp. 165–174.
[23] A. Xu and G. Dudek, “Fourier tag: A smoothly degradable ﬁdu-
cial marker system with conﬁgurable payload capacity,” in 2011
Canadian Conference on Computer and Robot Vision.
IEEE, 2011, pp.
40–47.

[24] F. Bergamasco, A. Albarelli, L. Cosmo, E. Rodola, and A. Torsello,
“An accurate and robust artiﬁcial marker based on cyclic codes,”
IEEE transactions on pattern analysis and machine intelligence, vol. 38,
no. 12, pp. 2359–2373, 2016.

[25] L. Calvet, P. Gurdjos, and V. Charvillat, “Camera tracking using
concentric circle markers: Paradigms and algorithms,” in 2012 19th
IEEE International Conference on Image Processing.
IEEE, 2012, pp.
1361–1364.

[26] F. Bergamasco, A. Albarelli, and A. Torsello, “Pi-tag: a fast image-
space marker design based on projective invariants,” Machine
vision and applications, vol. 24, no. 6, pp. 1295–1310, 2013.

[27] J. Rekimoto, “Matrix: A realtime object identiﬁcation and registra-
tion method for augmented reality,” in Proceedings. 3rd Asia Paciﬁc
Computer Human Interaction (Cat. No. 98EX110).
IEEE, 1998, pp.
63–68.

Fig. 19. Failure cases. Top row shows one of the failure cases of dra-
matic lighting change, and bottom row shows one of the severe motion
blur. On right, binarization results are shown for each case respectively.

[11] S. Garrido-Jurado, R. Munoz-Salinas, F. J. Madrid-Cuevas, and
R. Medina-Carnicer, “Generation of ﬁducial marker dictionaries
using mixed integer linear programming,” Pattern Recognition,
vol. 51, pp. 481–491, 2016.

[12] S. Garrido-Jurado, R. Mu ˜noz-Salinas, F. J. Madrid-Cuevas, and
M. J. Mar´ın-Jim´enez, “Automatic generation and detection of
highly reliable ﬁducial markers under occlusion,” Pattern Recog-
nition, vol. 47, no. 6, pp. 2280–2292, 2014.

[13] M. G. Prasad, S. Chandran, and M. S. Brown, “A motion blur
resilient ﬁducial for quadcopter imaging,” in 2015 IEEE Winter
Conference on Applications of Computer Vision.
IEEE, 2015, pp. 254–
261.

[14] L. Calvet, P. Gurdjos, C. Griwodz, and S. Gasparini, “Detec-
tion and accurate localization of circular ﬁducials under highly
challenging conditions,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2016, pp. 562–570.

12

active Collaborative Robotics, A. Ronzhin, G. Rigoll, and R. Meshch-
eryakov, Eds. Cham: Springer International Publishing, 2018, pp.
249–258.

[51] M. Krogius, A. Haggenmiller, and E. Olson, “Flexible layouts for
ﬁducial tags,” in Proceedings of the IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), 2019.

Guoxing Yu received his B.Eng degree in
electronic information engineering from Wuhan
University of Science and Technology, Wuhan,
China,
in 2013, and the M.E. degree in in-
formation and communication engineering from
Huazhong University of Science and Technol-
ogy, Wuhan, China, in 2016.

He is currently with Guangdong Virtual Re-
ality Co., Ltd. (aka. Ximmerse) as an algorithm
engineer. Prior to joining Ximmerse, he was an
algorithm engineer with Wuhan Guide Infrared
Co., Ltd. Wuhan from Jul. 2016 to Aug. 2017. His research interests
include computer vision, augmented reality and virtual reality.

Yongtao Hu received his B.Eng degree in com-
puter science from Shandong University, Jinan,
China, in 2010, and the Ph.D. degree in com-
puter science from The University of Hong Kong,
Hong Kong, in 2014.

He is currently with Guangdong Virtual Reality
Co., Ltd. (aka. Ximmerse) as a research scien-
tist. Prior to joining Ximmerse, he was a staff re-
searcher with Image and Visual Computing Lab
(IVCL), Lenovo Research, Hong Kong from Jan.
2015 to Oct. 2015, was a researcher assistant
with IVCL from Jul. 2014 to Nov. 2014, and was a research intern
at Internet Graphics Group in Microsoft Research Asia (MSRA) from
Mar. 2010 to Jun. 2010. His research interests include computer vision,
multimedia, machine learning, augmented reality and virtual reality.

Jingwen Dai (S’09 - M’12) received the B.E.
degree in automation from Southeast University,
Nanjing, China, in 2005, the M.E. degree in au-
tomation from Shanghai Jiao Tong University,
Shanghai, China, in 2009, and the Ph.D. de-
gree in mechanical and automation engineering
from the Chinese University of Hong Kong, Hong
Kong, in 2012.

He is currently with Guangdong Virtual Re-
ality Co., Ltd. (aka. Ximmerse) as co-founder
and chief technology ofﬁcer. Prior to joining Xim-
merse, he was a manager and advisory researcher with Image and
Visual Computing Lab (IVCL), Lenovo Research, Hong Kong from Jan.
2014 to July 2015, and was a Post-Doctoral Research Associate with
the Department of Computer Science, University of North Carolina at
Chapel Hill, Chapel Hill, NC, USA from Oct. 2012 to Dec. 2013. His
current research interests include computer vision and its applications
in human-computer interaction, augmented reality and virtual reality.

[28] H. Kato and M. Billinghurst, “Marker tracking and hmd calibra-
tion for a video-based augmented reality conferencing system,” in
Proceedings 2nd IEEE and ACM International Workshop on Augmented
Reality (IWAR’99).

IEEE, 1999, pp. 85–94.
[29] J. Rekimoto and Y. Ayatsuka, “Cybercode: designing augmented
reality environments with visual tags,” in Proceedings of DARE
2000 on Designing augmented reality environments. ACM, 2000,
pp. 1–10.

[30] M. Rohs and B. Gfeller, “Using camera-equipped mobile phones
for interacting with real-world objects,” Advances in pervasive com-
puting, vol. 176, pp. 265–271, 2004.

[31] D. WAGNER, “Artoolkitplus for pose tracking on mobile devices,”
in Proceedings of 12th Computer Vision Winter Workshop (CVWW’07),
February, 2007.

[32] K. Tateno, I. Kitahara, and Y. Ohta, “A nested marker for aug-
IEEE,

mented reality,” in 2007 IEEE Virtual Reality Conference.
2007, pp. 259–262.

[33] F. Schweiger, B. Zeisl, P. Georgel, G. Schroth, E. Steinbach, and
N. Navab, “Maximum detector response markers for sift and
surf,” in Vision, Modeling and Visualization Workshop (VMV), 2009.
[34] E. Costanza and J. Robinson, “A Region Adjacency Tree Approach
to the Detection and Design of Fiducials,” in Video Vision and
Graphics, 2003, pp. 63–69.

[35] E. Costanza, “D-touch: A consumer-grade tangible interface mod-
ule and musical applications,” in Proceedings of Conference on
HumanComputer Interaction, 2003.

[36] R. Bencina, M. Kaltenbrunner, and S. Jorda, “Improved topological
ﬁducial tracking in the reactivision system,” in 2005 IEEE Com-
puter Society Conference on Computer Vision and Pattern Recognition
(CVPR’05)-Workshops.

IEEE, 2005, pp. 99–99.

[37] R. Bencina and M. Kaltenbrunner, “The design and evolution of
ﬁducials for the reactivision system,” in Proceedings of the Third
International Conference on Generative Systems in the Electronic Arts,
2005.

[38] M. Kaltenbrunner and R. Bencina, “reactivision: a computer-vision
framework for table-based tangible interaction,” in Proceedings of
the 1st international conference on Tangible and embedded interaction.
ACM, 2007, pp. 69–74.

[39] C. N. Klokmose, J. B. Kristensen, R. Bagge, and K. Halskov,
“Bullseye: high-precision ﬁducial tracking for table-based tangible
interaction,” in Proceedings of the Ninth ACM International Confer-
ence on Interactive Tabletops and Surfaces. ACM, 2014, pp. 269–278.
[40] C. B. Owen, F. Xiao, and P. Middlin, “What is the best ﬁducial?”
in The First IEEE International Workshop Agumented Reality Toolkit,.
IEEE, 2002, pp. 8–pp.

[41] D. Claus and A. W. Fitzgibbon, “Reliable ﬁducial detection in nat-
ural scenes,” in European Conference on Computer Vision. Springer,
2004, pp. 469–480.

[42] ——, “Reliable automatic calibration of a marker-based position
tracking system,” in 2005 Seventh IEEE Workshops on Applications
of Computer Vision (WACV/MOTION’05)-Volume 1, vol. 1.
IEEE,
2005, pp. 300–305.

[43] V. Lepetit and P. Fua, “Keypoint recognition using randomized
trees,” IEEE transactions on pattern analysis and machine intelligence,
vol. 28, no. 9, pp. 1465–1479, 2006.

[44] M. Ozuysal, M. Calonder, V. Lepetit, and P. Fua, “Fast keypoint
recognition using random ferns,” IEEE transactions on pattern
analysis and machine intelligence, vol. 32, no. 3, pp. 448–461, 2010.

[45] T. Collins and A. Bartoli, “Inﬁnitesimal plane-based pose estima-
tion,” International Journal of Computer Vision, vol. 109, no. 3, pp.
252–286, Sep. 2014.

[46] P. Chen and D. Suter, “Error analysis in homography estimation
by ﬁrst order approximation tools: A general technique,” Journal
of Mathematical Imaging and Vision, vol. 33, no. 3, pp. 281–295, Mar.
2009.

[47] D. W. Marquardt, “An algorithm for least-squares estimation
of nonlinear parameters,” Journal of the society for Industrial and
Applied Mathematics, vol. 11, no. 2, pp. 431–441, 1963.

[48] R. Hartley and A. Zisserman, Multiple view geometry in computer

vision. Cambridge university press, 2003.

[49] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman,
“Labelme: a database and web-based tool for image annotation,”
International journal of computer vision, vol. 77, no. 1-3, pp. 157–173,
2008.

[50] K. Shabalina, A. Sagitov, M. Svinin, and E. Magid, “Comparing
ﬁducial markers performance for a task of a humanoid robot self-
calibration of manipulators: A pilot experimental study,” in Inter-

