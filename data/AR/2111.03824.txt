Neural Implicit Event Generator for Motion Tracking

Mana Masuda1âˆ—, Yusuke Sekikawa2âˆ—, Ryo Fujii1, Hideo Saito1

1
2
0
2

v
o
N
6

]

V
C
.
s
c
[

1
v
4
2
8
3
0
.
1
1
1
2
:
v
i
X
r
a

Fig. 1.
(a) The conventional explicit event generation based tracking methods. (b) Our proposed implicit event generator (IEG) based tracking method.
The existing methods require to generate events explicitly with dense computation for computing error to update estimation of the position T and velocity
Ë™T . Ours can estimate the T and Ë™T with sparse computation by using IEG which implicitly model event generation process. By utsing our framework using
IEG, error is computed directly with sparse computation without explicitly computing event frame from T and Ë™T using dense computation

Abstractâ€” We present a novel framework of motion tracking
from event data using implicit expression. Our framework
use pre-trained event generation MLP named implicit event
generator (IEG) and does motion tracking by updating its
state (position and velocity) based on the difference between
the observed event and generated event from the current state
estimate. The difference is computed implicitly by the IEG.
Unlike the conventional explicit approach, which requires dense
computation to evaluate the difference, our implicit approach
realizes efï¬cient state update directly from sparse event data.
Our sparse algorithm is especially suitable for mobile robotics
applications where computational resources and battery life are
limited. To verify the effectiveness of our method on real-world
data, we applied it to the AR marker tracking application. We
have conï¬rmed that our framework works well in real-world
environments in the presence of noise and background clutter.

* Equal contribution
1Mana Masuda, Ryo Fujii and Hideo Saito are with the School of Science

for Open and Environmental Systems, Keio University, Tokyo, Japan
2Yusuke Sekikawa is with Denso IT Laboratory, Tokyo, Japan

I. INTRODUCTION

Motion tracking from spatiotemporal data such as video
frames is one of the fundamental functionality for robotics
applications. It either tracks a speciï¬c target position from
the camera or tracks a camera position from worlds coordi-
nates. The application of tracking algorithms ranges from
robotic arm manipulation, and mobile robot
localization.
Before the tracking, the targetâ€™s initial position is identiï¬ed
by globally matching known patterns with camera observa-
tions. And then, a tracking algorithm sequentially updates the
position using. This update is usually done by using gradient-
based algorithms. However, in existing systems using frame
cameras [1], [2], the loss function deï¬ned by the difference
between the known pattern and the observation is often
trapped by the local minimum when the motion of the camera
or the target is fast; results in the failure of tracking.

Event cameras are bio-inspired novel vision sensors

Known PatternIntensityGradientOptical FlowEvent StreamPredicted EventIntegrated Eventğ‘‡: Positionğ‘‡Ì‡: VelocityErrorEvent StreamÎ£IntegrationIEGğ‘‡: Positionğ‘‡Ì‡: VelocityError(a) Conventional Explicit Event Generation based Method(b) Our Implicit Event Generator based Method : Forward Operation: Backpropagation Operation 
 
 
 
 
 
Fig. 2. An overview of our framework of object tracking pipeline which inverts an optimized implicit event generator (IEG). Given an initially estimated
position T and velocity Ë™T and the place and time where intensity change is detected, IEG estimate the corresponding intensity change and gives its error.
Since the whole pipeline is differentiable, we can reï¬ne our estimated position T and velocity Ë™T by minimizing the error. All operations are done sparsely
only where the events are detected.

that mimic biological retinas and report per-pixel intensity
changes in the form of asynchronous event stream instead
of intensity frames. Due to the unique operating princi-
ple, event-based sensing offers signiï¬cant advantages over
conventional cameras; high dynamic range (HDR), high
temporal resolution, and blurless sensing. As a result, it has
the potential to achieve robust tracking in intense motion and
severe lighting environments.

Many methods have been proposed which use these fea-
tures to achieve tracking in high-speed or high-dynamic-
range environments [3], [4], [5], [6], [7], [8], [9], [10].
In particular, Bryner et al. [7] and Gehrig et al. [6], [10]
proposed method as an extension of Lucas-Kanade tracker
(KLT) [11], [12] developed for frame-based video data. They
extend KLT to sparse intensity difference data from the
event-based camera. In their method, position and veloc-
ity are updated by minimizing the difference between the
observation and the expected observation, as shown in Fig.
1(a). The expected observation is computed by warping the
given photometric 2D/3D map using the current estimate of
position and velocity. Unfortunately, these methods could not
fully take advantage of the sparseness of event data. Still,
they require dense computation to compute the difference,
and this optimization needs to be repeated many times until
convergence.

To realize efï¬cient tracking, we focus on leveraging the
sparsity of event data. To this end, we propose a novel
framework for object tracking using implicit expression. As
shown in Fig. 1(b), our method achieves the goal by updating
the position and velocity based on the difference between
the events generated by the implicit event generator (IEG)
and the observed events. In our method, the difference is
directly computed by the feed-forward operation of sparse
event data without explicitly predict and generate events from
the current position and velocity estimates. This implicit
approach is totally different from the conventional explicit
approach, which requires dense computation to evaluate the
difference. The gradient of position and velocity w.r.t the IEG
is computed by simply backpropagating the error through
IEG realized by MLP. The entire operation, namely feed-
forward operation for difference evaluation and derivative
computation w.r.t the difference, are executed sparsely; com-
putation happens only where the events occur.

In this paper, we ï¬rst introduce a speciï¬c tracking algo-
rithm based on this new framework. For the sake of proof
of concept, we conducted experiments using an AR marker
captured by an actual event-based camera to show that the
method based on this new framework can be applied to real
problems.

Our main contributions are summarized as follows:
â€¢ We propose a novel object

tracking framework by
using an implicit event generator (IEG). In our method,
computation is performed only at the eventâ€™s location;
therefore, it could be signiï¬cantly efï¬cient than the con-
ventional explicit method requiring dense computation.
â€¢ We conduct experiments using data captured from AR
markers using a real event-based camera; demonstrate
the proposed method can be applied to practical prob-
lems.

II. RELATED WORKS

Most of the existing tracking algorithms are developed
based on RGB cameras and track the target object frame
by frame. Recently, the Siamese network-based approach
achieve a better result and many methods have been proposed
based on it [13], [14], [15]. RGB images, however, cannot
provide sufï¬cient tracking performance under fast motion
and low-illumination conditions.

By taking advantage of the HDR and no-motion blur
properties of event cameras, a method for tracking fast
objects and in harsh light environments has been proposed.
Rebecq et al. [3] and Zihao et al. [4] convert IMU data
into lower rate pieces of information at desired times where
events are collected. Chamorro et al. [9] and Alzugaray
et al. [8] explore the high-speed feature tracking with the
dynamic vision sensor (DVS) [16], and Gallego et al. [5]
presented a ï¬lter-based approach that requires to maintain
a history of past camera positions which are continuously
reï¬ned. Bryner et al. [7] and Gehrig et al. [6], [10] use
the extended Lucas-Kanade method [11], [12] and update
the position and velocity by warping the given photometric
3D map or frame and taking the difference between the
observation and the expected observation.

Considering the processing in the neural network, sparse
event data is inherently difï¬cult to handle using a convention
deep neural network. Therefore, many neural network-based

ğ’²,ğ±ğ‘€3ğ‘€ğ‘€ErrorEvent Streamğ‘‡Ì‡ğ‘‡,IEGğ’¢!Backpropagationğ‘€ğ‘‡Ì‡ğ‘‡PoseVelocity{ğ‘’}Fig. 3.
Procedure for creating artiï¬cial intensity change data for training IEG. The detailed process is as follows: 1) compute the gradient of tracking
target intensity image; 2) randomly determine the time t and velocity v and the rotation velocity Ï‰, and calculate the intensity change value Î´t when
moving at that velocity and time; 3) based on the threshold value Â¯Î´, determine whether the intensity change value Î´t exceeds the change detection threshold
of an event-based camera to generate binary intensity changes where Î´(cid:48)
t = (+1, âˆ’1, Nan); 4) we apply Gaussian blur G(x, y) to the calculated intensity
change Î´(cid:48)
t to obtain the artiï¬cial intensity change data Î´a to train IEG.

methods for event data ï¬rst convert sparse events to dense
frames and then process it by a dense neural network such
as CNN [17], [18], [19], [20], [21]. Comparing with these
methods,
the computational complexity of our approach
could be signiï¬cantly smaller.

Since NeRF [22], implicit volumetric representations have
attracted in computer vision community [23], [24], [25]. In
this paradigm, the RGB-D scene representation is learned in
a parameterized manner by a multi-layer perceptron (MLP).
Some methods have been proposed to estimate the position
of an object by back-propagating the scene representation
MLP, such as iNeRF [26], [27]. These methods, however,
are proposed for dense RGB images and do not apply to
sparse event data with time-series information. We used IEG
to represent events that occur in response to the position,
time, and velocity of a pixel, making it possible to implicitly
represent events under a variety of conditions.

III. METHOD
We present a framework for motion tracking using a neural
implicit event generator (IEG) to realize efï¬cient motion
update by sparse operations. In our framework, position
T and velocity Ë™T is updated by simply feed-forwarding
sparse event observation into IEG and then backpropagate
through the IEG. The IEG G is realized by
the output
differential MLP (parameterized by Î˜), and it is trained
before inference to output intensity changes of tracking-
target for given position T and velocity Ë™T . To track the
motion, i.e., estimate (T, Ë™T ), at inference, (T, Ë™T ) is otpmized
to minimized the difference L betweeen obserbation and
estimation for observation of event stream {e} such that:

Ë†T , Ë†Ë™T = argmin

L(T, Ë™T |{e}, Î˜)

(1)

T, Ë™T

In this formulation, explicit generation of events using by
current estimate (T, Ë™T ) does not required as existing method
(Fig.1, left). Instead, ours implicitly generates events and
computes the difference directly from the sparse event (Fig.1,
right).

This paper applied the proposed framework for 3DoF

camera motion in a 2D planer scene.

A. Formulation of IEG

and time t and 3DoF Ë™T = (vx, vy, Ï‰), and whose output is an
detected intensity change Î´ âˆˆ [+/ âˆ’ /NaN]. We approximate
the intensity change appearance with continuous 3DoF with
the IEG GÎ˜ : (x, y, t, vx, vy, Ï‰) â†’ Î´ and optimize its weights
Î˜ to map from each input 6D vector to its corresponding
intensity change Î´.

B. Artiï¬cial Data of Intensity Changement

To learn the intensity change with velocity using IEG, we
create artiï¬cial intensity change data based on the following
equation;

(cid:12)
(cid:12)
(cid:12)
(cid:12)

âˆ‚I
âˆ‚x

(cid:12)
(cid:12)
Â· v(x)âˆ†t
(cid:12)
(cid:12)

> C,

(2)

where v(x) is the speed of coordinate x. C is the threshold
of event ï¬ring.

To create intensity change data at various velocities, we
use a randomly determined velocity Ë™T = (vx, vy, Ï‰), and
from the edge coordinates of a known object (xe, ye) and
âˆ‚x =
the intensity derivative data at
(âˆ‚ix, âˆ‚iy). From these values, we can calculate the theo-
retical intensity change value Î´t at the coordinate (xt, yt),
which corresponding to the edge coordinate (xe, ye) when
the object moves by time t as follows;

those coordinates âˆ‚I

Î´t = U

(cid:18) âˆ‚I
âˆ‚x

(cid:19)

, v, Ï‰, t

=

(cid:18)cos(Ï‰t) âˆ’ sin(Ï‰t)
cos(Ï‰t)

sin(Ï‰t)

(cid:19) (cid:18)âˆ‚ix
âˆ‚iy

(cid:19)

(cid:19)
(cid:18)vx + cos(Ï‰)
vy + sin(Ï‰)

t

Â·

(3)

ï£±
ï£²

Î´(cid:48)
t =

We set the value corresponding to the threshold of event
ï¬ring as Â¯Î´, and deï¬ne Î´(cid:48)
t as follows;
(Î´t > Â¯Î´)
+1
Nan (âˆ’Â¯Î´ <= Î´t <= Â¯Î´)
(Î´t < âˆ’Â¯Î´)
âˆ’1
To perform the optimization operation described in III-D,
we applied a Gaussian blur to the intensity change Î´(cid:48)
t to make
it differentiable. Depending on the distance (dx, dy) to the
intensity change Î´(cid:48)
t, we use an extended Gaussian function
G(dx, dy) with mean 0 and variance Ïƒ;

(4)

ï£³

.

G(dx, dy) = exp

âˆ’

(cid:18)

dx2 + dy2
2Ïƒ2

(cid:19)
.

(5)

We represent intensity change appearance as a 6D vector-
valued function whose input is a 2D location x = (x, y, r)

To avoid interference with events generated by other edges,
the blur width is set to w, and the Gaussian blurred intensity

ğœ•ğ¼ğœ•ğ±	,ğ‘¥!,ğ‘¦!(ğ‘¥",ğ‘¦",ğ›¿")(ğ‘¥!â€²,ğ‘¦!â€²,ğ›¿")(ğ‘¥",ğ‘¦",ğ›¿"â€²)Threshold : ğ›¿Ì…Gaussian BlurTracking TargetSparse GradientIntensity ChangeBinary Intensity ChangeGaussian blurredBinary Intensity Changeâˆ‡ğ’°ğœ•ğ¼ğœ•ğ±,ğ‘£,ğœ”,ğ‘¡Algorithm 1 Tracking using gradient of IEG
Ensure: Ë†Tk = (xk, yk) is the inferenced position at k-
Ë†Ë™Tk = (vxk, vy k, Ï‰k) is the inferenced

th iteration and
velocity at k-th iteration.

1: (cid:15) â† inf , k â† 0
2: Ë†T â† T0
Ë†Ë™T â† (0, 0, 0)
3:
4: while (cid:15) â‰¥ Â¯(cid:15) do
Lk = (cid:80)
5:
Update Ë†Tk, Ë†Ë™Tk
Îµ â† Lkâˆ’1 âˆ’ Lk, k â† k + 1

6:
7:
8: end while

iâˆˆM (1 âˆ’ |GÎ˜(W(xi, Ë†Tk), Ë†Ë™T )|)

change Î´a at (x(cid:48)
follows:

t, y(cid:48)

t) = (xt + dx, yt + dy) is calculated as

Î´a =

(cid:26) G(dx, dy) Ã— Î´(cid:48)
t

0

((cid:112)dx2 + dy2 â‰¤ w)
((cid:112)dx2 + dy2 > w)

.

(6)

C. Training IEG

We trained the IEG with the artiï¬cial data as described
in III-B. When we input the 6D vector of artiï¬cial data
(x, y, t, vx, vy, Ï‰) and the IEG outputs the estimated intensity
change Î´p. The IEG is updated by taking the difference
between the generated intensity change Î´g and the artiï¬cially
calculated intensity change Î´a;

Ltrain = ||Î´g âˆ’ Î´a||2.

(7)

Fig. 4. How to capture the data by using event camera [28]. (a) on a white
background, and (b) on the wood grain pattern. Since our proposed tracking
algorithm is limited to 2 dimension, we ï¬xed the camera on the dolly so
that the distance between the camera and the AR marker is constant.

E. Object Tracking

During tracking, the events of an existing small group
(window) consisting of M events are processed as in III-
D to obtain the position Tj and velocity Ë™Tj at j-th iteration.
Then, at the j + 1-th iteration, slide the window with sliding
width K, and update the position Tj+1 and velocity Ë™Tj+1 by
optimizing again as in III-D. The position and velocity do
not change much between the j-th and j + 1-th iterations,
so to update the (Tj, Ë™Tj) at j + 1-th iteration could help to
reduce the latency.

Our pipeline does not take acceleration Â¨T into account,
but this is not a problem because if the window size M
is sufï¬ciently small, the motion within the window can be
approximated as constant velocity motion.

D. Tracking using gradient of IEG

IV. EXPERIMENTAL RESULTS

Let Î˜ be the parameters of the trained and ï¬xed IEG
G, xi = (xi, yi, ti) the place (xi, yi) and time ti of i-
th detected intensity change, Î´(xi) is the intensity change
sensed at xi, ( Ë†Tk, Ë†Ë™Tk) the estimated position and velocity at
current optimization step k. Before inputting into the IEG,
xi is transformed by the transformation function W based
on Tk in the 2D plane.

W(xi, T ) =

(cid:18)cos(âˆ’Tr) âˆ’ sin(âˆ’Tr)
cos(âˆ’Tr)

sin(âˆ’Tr)

(cid:19) (cid:18)xi âˆ’ Ttx
yi âˆ’ Tty

(cid:19)

, (8)

where Tt is the translation component and Tr is the rotation
component of T . During tracking, optimization is performed
so that the absolute value output by IEG is close to 1;

Lupdate =

(cid:88)

iâˆˆM

(1 âˆ’ |GÎ˜(W(xi, Ë†Tk), Ë†Ë™T )|),

(9)

The signiï¬cance of this method is that it can estimate the
motion of an object only from the event data. To demonstrate
the effectiveness of this method, we experimented using an
image pattern with AR markers as a target scene to verify
whether the motion of the AR marker can be accurately
estimated or not. Since the events only occur from the
edge of the AR marker, we can effectively show that the
camera motion can be estimated using only sparse events.
Another reason for using AR markers is that they are easy to
annotate (detect and map feature points) and can be used as
a criterion for accuracy evaluation. To capture the event data,
we used Propheseeâ€™s event camera Gen3 [28] with 120dB at
10000fps. As the problem setup for object tracking, we can
assume that the initial position of the object is known by
the object detection algorithm, so we gave its approximate
position as the initial value for tracking. The evaluation was
done quantitatively and qualitatively.

where M is the number of detected events. We employ
gradient-based optimization to solve for ( Ë†T , Ë†Ë™T ) as deï¬ned
in Eq. 1. The optimization is iterated until the change in
loss (cid:15) is less than the optimization threshold Â¯(cid:15).

In other words, this optimization problem can be deï¬ned

as ï¬nding (T, Ë™T ) by solving the function 1 âˆ’ |G| = 0.

A. Experimental Setup

We evaluated the performance of our algorithm by tracking
AR Marker in two environments; one is on the white
background and the other is on the wood grain pattern. Since
our proposed tracking algorithm is limited to 2 dimension,
we ï¬xed the event camera on the dolly as shown in Fig.

(A) White background(B) Wood Grain BackgroundTABLE I
ERROR MEASUREMENTS OF ON WHITE BACKGROUND AND ON WOOD GRAIN BACKGROUND

On White Background

On Wood Grain Background

position

velocity

position

velocity

K

w [pixel]

x [pixel]

y [pixel]

r/10âˆ’2 [rad]

vx [pixel/s]

vy [pixel/s]

Ï‰ [rad/s]

K

w [pixel]

x [pixel]

y [pixel]

r/10âˆ’2 [rad]

vx [pixel/s]

vy [pixel/s]

Ï‰ [rad/s]

300

500

2
4
8
16

Ave.

2
4
8
16

Ave.

1.065
1.238
0.623
1.424

1.088

1.090
1.292
0.646
1.438

1.117

2.121
2.641
8.191
3.113

4.016

2.878
2.919
8.919
3.121

4.459

0.013
0.009
0.014
0.124

0.040

0.013
0.009
0.013
0.125

0.040

3.040
2.074
2.680
1.403

2.299

2.859
1.905
2.512
1.383

2.165

3.179
1.541
4.174
1.807

2.675

3.173
1.472
4.056
1.796

2.624

0.021
0.069
0.081
0.091

0.065

0.018
0.071
0.079
0.092

0.065

3000

5000

2
4
8
16

Ave.

2
4
8
16

Ave.

0.773
1.753
0.246
2.635

1.352

0.759
1.728
0.250
2.629

1.342

1.794
0.967
0.281
0.201

0.811

1.783
0.962
0.280
0.201

0.807

0.034
0.023
0.006
0.011

0.019

0.033
0.023
0.006
0.011

0.018

0.692
1.635
0.253
0.208

0.697

0.673
1.525
0.253
0.203

0.665

0.416
4.334
0.363
0.919

1.508

0.381
3.522
0.354
0.880

1.234

0.430
0.130
0.032
0.206

0.199

0.374
0.132
0.028
0.205

0.185

TABLE II

AVERAGE NUMBER OF ITERATION

White Background

Wood Grain Background

K = 300 K = 500 K = 3000 K = 5000

8.65
8.72
11.89
11.80
10.27

13.66
19.45
20.64
20.90
18.66

16.57
20.37
18.16
18.82
18.48

22.66
27.75
27.64
29.83
26.97

w
2
4
8
16
Ave.

Fig. 5. Tracking result visualization of A: on the white background and
B: on the wood grain background when w = 16. A-(1) is when K = 300,
A-(2) is when K = 500, B-(1) is when K = 3000 and B-(2) is when
K = 5000. The white box indicates the predicted position of the AR
marker, the white arrows indicate the velocity and rotation speed, The green
box indicates the ground truth position of the AR marker and the green
arrows indicate the ground truth velocity and rotation speed. Positive events
are shown in red and negative events are shown in blue.

4 so that
the distance between the camera and the AR
marker is constant. We use optimizer Adam [29] with the
recommended hyperparameter settings of: Î²1 = 0.9, Î²2 =
0.999, and (cid:15) = 10âˆ’8 for training IEG and stochastic gradient
descent (SGD) for gradient-based optimization. The learning
rate was 0.0001 for both training and inference and kept
constant. We set the threshold of synthetic event ï¬ring as
Â¯Î´ = 0.001 and the optimization threshold as Â¯(cid:15) = 1.0 Ã— 10âˆ’6.

a) On the White Background: For on the white back-
ground data, we conduct the experiment with M = 2 Ã— 104
and K = 300/500 and evaluated for every 1.5 Ã— 104 events,
i.e. for every ï¬ve iterations until the 5000-th iteration for
K = 300 and for every three iterations until the 3000-th
iteration for K = 500.

b) On the Wood Grain Background: For on the wood
grain background data, we conduct
the experiment with
M = 2 Ã— 105 and K = 3000/5000 and evaluated for every
1.5 Ã— 105 events, i.e. for every ï¬ve iterations until the 500-th
iteration for K = 3000 and for every three iterations until
the 300-th iteration for K = 5000.

B. Tracking Errors

The position estimation error and velocity estimation error
are given by the mean square error for each value and we
conduct experiments for w = 2, 4, 8, 16. The result of the
data with the white background the wood grain pattern is
shown in Table I.

In the case of on white background and on wood grain
background, the wood grain background is supposed to be
a more difï¬cult problem setting for object tracking, but by
comparing the result of on white background and on wood
grain background. In this experiment, stable tracking was
achieved regardless of the width of the Gaussian blur w and
we can see that the tracking performance does not deteriorate
even in the case of on wood grain pattern. This shows
the possibility that our gradient-based algorithm can work
effectively in various situations.

C. Number of Iterations

The average numbers of iterations for 3000 times of opti-
mization for white background and 300 times of optimization
for wood grain pattern after the second optimization are
shown in Table II. The ï¬rst optimization required a large
number of iterations compared to the second optimization
to localize the object, 1301.25 for the white background and
1684.25 for the wood grain background. As can be seen from
Table II, the number of optimizations increases with larger
K, which indicates that the larger the motion of the object,
it takes time to convergence.

(1) ğ¾=300(2) ğ¾=500(2) ğ¾=5000(1) ğ¾=3000B: Wood Grain BackgroundA: White BackgroundFig. 6. Results of position estimation (solid line) and ground truth (dashed line). (a) shows the result of on a white background, (b) shows that of on
a wood grain background and from left to right, these are the results for x, y, and r. These results show that not only is the difference from the ground
truth small but also that the misalignment does not increase over time. Also, the tracking performance does not depend on the white background or the
wood grain background.

D. Qualitative Evaluation

a) Visualization of tracking results: Fig. 5-A shows
the event of AR marker on white background with tracking
result in white box (position) and white arrow (velocity)
and ground truth in green box (position) and green arrow
(velocity). Fig. 5-B shows the event of AR marker on wood
grain background with tracking result and ground truth as in
Fig. 5-A. This shows our algorithm can track the movements
with small errors for real data regardless of the size of sliding
width K. In addition to that, as shown in Fig. 5-B, our
algorithm gracefully tracks the object precisely despite the
considerable amount of events generated by the background.
b) Robustness over time: The transition of estimated
position T = (x, y, r) (solid line) and ground truth position
(dashed line) is shown in Fig. 6. (a) shows the result of on
the white background and (b) shows that of on the wood
grain background. These results show that not only is the
difference from the ground truth small but also that the
misalignment does not increase over time. Also, the tracking
performance did not depend on whether the background was
wood grain or white. This indicates that our method is not
affected by a large number of events from the background,
and can perform gradient optimization and ï¬nd the position
properly and accurately in every iteration.

V. CONCLUSION

We proposed a novel concept of motion tracking using
implicit expression. Our method estimates an objectâ€™s posi-
tion and velocity by gradient-based optimization using our
proposed implicit event generator (IEG). Unlike conven-
tional approaches that explicitly generate events, this method
generates events implicitly. It makes it possible to handle
events sparsely, thereby realizing highly efï¬cient processing,
especially suitable for mobile robotics applications. We show
that our framework could work in the practical scenario

through evaluation using event data capture using a real
event-based camera.

A. Limitation and Future Work

The purpose of this study is to verify the principle of
our novel concept of estimating position and velocity by
gradient-based optimization using IEG as shown in Fig.
2, and for comparison with existing methods, we need to
consider the extension to 6-DoF, speed-up of inference time,
and improvement of Optimizer.

a) Extention to 6-DoF: For performance veriï¬cation,
in this paper, we have applied it to a simple 3-DoF problem
and conï¬rmed its operation. In the future, we would like to
extend the idea to 6-DoF, as shown in [7], [6], [10]. The
problem with extending this method to 6-DoF is that the
number of variables that need to be updated increases. It
is necessary to consider a new scheme for updating each
variable appropriately using backpropagation, rather than
simply updating each variable as in this proposed method.

b) Speed up using Look-up table: In this paper, we
have only veriï¬ed our concept and have not studied the
reduction of inference time, which is necessary for practical
applications such as AR/VR. To conduct the fast inference,
we think it is an effective algorithm that reduces inference
time by combining look-up tables and MLP, as proposed by
Sekikawa et al. [30].

c) Optimizer:

In this paper, we used SGD as the
optimizer for inference. However, it is necessary to consider
optimizers for speeding up and stabilizing inference, such
as Momentum [31], Nesterovâ€™s accelerated gradient [32]. In
addition, we would like to consider the application of the
Gauss-Newton method or the Levenberg-Marquardt method,
which use the (approximate) second-order derivatives.

GroundTruth*=16*=8*=4*=203000iteration10002000300200![pixel]GroundTruth*=16*=8*=4*=20900iteration300600300380![pixel]0900iteration300600150270GroundTruth*=16*=8*=4*=2![pixel]GroundTruth*=16*=8*=4*=203000iteration10002000400200![pixel](a) White Background(b) Wood Grain BackgroundGroundTruth*=16*=8*=4*=20900iteration300600-2.4![rad]03000100020001.351.70![rad]GroundTruth*=16*=8*=4*=2REFERENCES

[1] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, â€œOrb-slam: a
versatile and accurate monocular slam system,â€ T-RO, vol. 31, no. 5,
pp. 1147â€“1163, 2015.

[2] C. Forster, Z. Zhang, M. Gassner, M. Werlberger, and D. Scaramuzza,
â€œSvo: Semidirect visual odometry for monocular and multicamera
systems,â€ T-RO, vol. 33, no. 2, pp. 249â€“265, 2016.

[3] H. Rebecq, T. Horstschaefer, and D. Scaramuzza, â€œReal-time visual-
inertial odometry for event cameras using keyframe-based nonlinear
optimization,â€ in BMVC, 2017.

[4] A. Zihao Zhu, N. Atanasov, and K. Daniilidis, â€œEvent-based visual

inertial odometry,â€ in CVPR, 2017, pp. 5391â€“5399.

[5] G. Gallego, J. E. Lund, E. Mueggler, H. Rebecq, T. Delbruck, and
D. Scaramuzza, â€œEvent-based, 6-dof camera tracking from photometric
depth maps,â€ TPAMI, vol. 40, no. 10, pp. 2402â€“2412, 2017.

[6] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, â€œAsyn-
chronous, photometric feature tracking using events and frames,â€ in
ECCV, 2018, pp. 750â€“765.

[7] S. Bryner, G. Gallego, H. Rebecq, and D. Scaramuzza, â€œEvent-based,
direct camera tracking from a photometric 3d map using nonlinear
optimization,â€ in ICRA, 2019, pp. 325â€“331.

[8] I. Alzugaray Lopez and M. Chli, â€œHaste: multi-hypothesis asyn-
chronous speeded-up tracking of events,â€ in BMVC, 2020, p. 744.
[9] W. O. Chamorro Hernandez, J. Andrade-Cetto, and J. Sol`a Ortega,
â€œHigh-speed event camera tracking,â€ in BMVC, 2020, pp. 1â€“12.
[10] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, â€œEklt:
Asynchronous photometric feature tracking using events and frames,â€
International Journal of Computer Vision, vol. 128, no. 3, pp. 601â€“
618, 2020.

[11] B. D. Lucas and T. Kanade, â€œAn iterative image registration technique
with an application to stereo vision,â€ in IJCAI, vol. 2, 1981, p.
674â€“679.

[12] S. Baker and I. Matthews, â€œLucas-kanade 20 years on: A unifying

framework,â€ IJCV, vol. 56, pp. 221â€“255, 2004.

[13] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr,
â€œFully-convolutional siamese networks for object tracking,â€ in ECCV,
2016, pp. 850â€“865.

[14] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, â€œHigh performance visual
tracking with siamese region proposal network,â€ in CVPR, 2018, pp.
8971â€“8980.

[15] P. Voigtlaender, J. Luiten, P. H. Torr, and B. Leibe, â€œSiam r-cnn: Visual

tracking by re-detection,â€ in CVPR, 2020, pp. 6578â€“6588.

[16] P. Lichtsteiner, C. Posch, and T. Delbruck, â€œA 128x128 120 db 15 us
latency asynchronous temporal contrast vision sensor,â€ JSSC, vol. 43,
no. 2, pp. 566â€“576, 2008.

[17] A. I. Maqueda, A. Loquercio, G. Gallego, N. GarcÂ´Ä±a, and D. Scara-
muzza, â€œEvent-based vision meets deep learning on steering prediction
for self-driving cars,â€ in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018, pp. 5419â€“5427.
[18] R. Baldwin, R. Liu, M. Almatraï¬, V. Asari, and K. Hirakawa, â€œTime-
ordered recent event (tore) volumes for event cameras,â€ arXiv preprint
arXiv:2103.06108, 2021.

[19] S. Tulyakov, F. Fleuret, M. Kiefel, P. Gehler, and M. Hirsch, â€œLearning
an event sequence embedding for dense event-based deep stereo,â€ in
Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2019, pp. 1527â€“1537.

[20] D. Gehrig, A. Loquercio, K. G. Derpanis, and D. Scaramuzza, â€œEnd-
to-end learning of representations for asynchronous event-based data,â€
in Proceedings of the IEEE/CVF International Conference on Com-
puter Vision, 2019, pp. 5633â€“5643.

[21] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, â€œMatrix-
lstm: a differentiable recurrent surface for asynchronous event-based
data,â€ arXiv preprint arXiv:2001.03455, vol. 1, no. 2, p. 3, 2020.
[22] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoor-
thi, and R. Ng, â€œNerf: Representing scenes as neural radiance ï¬elds
for view synthesis,â€ in ECCV, 2020, pp. 405â€“421.

[23] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger,
â€œOccupancy networks: Learning 3d reconstruction in function space,â€
in CVPR, 2019, pp. 4460â€“4470.

[24] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M.
Seitz, and R. Martin-Brualla, â€œDeformable neural radiance ï¬elds,â€
arXiv preprint arXiv:2011.12948, 2020.

[25] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Doso-
vitskiy, and D. Duckworth, â€œNerf in the wild: Neural radiance ï¬elds
for unconstrained photo collections,â€ in CVPR, 2021, pp. 7210â€“7219.
[26] L. Yen-Chen, P. Florence, J. T. Barron, A. Rodriguez, P. Isola, and T.-
Y. Lin, â€œiNeRF: Inverting neural radiance ï¬elds for pose estimation,â€
in IROS, 2021.

[27] S.-Y. Su, F. Yu, M. Zollhoefer, and H. Rhodin, â€œA-nerf: Surface-
free human 3d pose reï¬nement via neural rendering,â€ arXiv preprint
arXiv:2102.06199, 2021.

[28] â€œProphesee metavision for machines,â€ https://www.prophesee.ai/.
[29] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimiza-

tion,â€ arXiv preprint arXiv:1412.6980, 2014.

[30] Y. Sekikawa and T. Suzuki, â€œIrregularly tabulated mlp for fast point

feature embedding,â€ arXiv preprint arXiv:2011.09852, 2020.
[31] N. Qian, â€œOn the momentum term in gradient descent

learning

algorithms,â€ Neural networks, vol. 12, no. 1, pp. 145â€“151, 1999.
[32] Y. Nesterov, â€œA method for unconstrained convex minimization prob-
lem with the rate of convergence o (1/kË† 2),â€ in Doklady an ussr, vol.
269, 1983, pp. 543â€“547.

