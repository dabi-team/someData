Neural Implicit Event Generator for Motion Tracking

Mana Masuda1∗, Yusuke Sekikawa2∗, Ryo Fujii1, Hideo Saito1

1
2
0
2

v
o
N
6

]

V
C
.
s
c
[

1
v
4
2
8
3
0
.
1
1
1
2
:
v
i
X
r
a

Fig. 1.
(a) The conventional explicit event generation based tracking methods. (b) Our proposed implicit event generator (IEG) based tracking method.
The existing methods require to generate events explicitly with dense computation for computing error to update estimation of the position T and velocity
˙T . Ours can estimate the T and ˙T with sparse computation by using IEG which implicitly model event generation process. By utsing our framework using
IEG, error is computed directly with sparse computation without explicitly computing event frame from T and ˙T using dense computation

Abstract— We present a novel framework of motion tracking
from event data using implicit expression. Our framework
use pre-trained event generation MLP named implicit event
generator (IEG) and does motion tracking by updating its
state (position and velocity) based on the difference between
the observed event and generated event from the current state
estimate. The difference is computed implicitly by the IEG.
Unlike the conventional explicit approach, which requires dense
computation to evaluate the difference, our implicit approach
realizes efﬁcient state update directly from sparse event data.
Our sparse algorithm is especially suitable for mobile robotics
applications where computational resources and battery life are
limited. To verify the effectiveness of our method on real-world
data, we applied it to the AR marker tracking application. We
have conﬁrmed that our framework works well in real-world
environments in the presence of noise and background clutter.

* Equal contribution
1Mana Masuda, Ryo Fujii and Hideo Saito are with the School of Science

for Open and Environmental Systems, Keio University, Tokyo, Japan
2Yusuke Sekikawa is with Denso IT Laboratory, Tokyo, Japan

I. INTRODUCTION

Motion tracking from spatiotemporal data such as video
frames is one of the fundamental functionality for robotics
applications. It either tracks a speciﬁc target position from
the camera or tracks a camera position from worlds coordi-
nates. The application of tracking algorithms ranges from
robotic arm manipulation, and mobile robot
localization.
Before the tracking, the target’s initial position is identiﬁed
by globally matching known patterns with camera observa-
tions. And then, a tracking algorithm sequentially updates the
position using. This update is usually done by using gradient-
based algorithms. However, in existing systems using frame
cameras [1], [2], the loss function deﬁned by the difference
between the known pattern and the observation is often
trapped by the local minimum when the motion of the camera
or the target is fast; results in the failure of tracking.

Event cameras are bio-inspired novel vision sensors

Known PatternIntensityGradientOptical FlowEvent StreamPredicted EventIntegrated Event𝑇: Position𝑇̇: VelocityErrorEvent StreamΣIntegrationIEG𝑇: Position𝑇̇: VelocityError(a) Conventional Explicit Event Generation based Method(b) Our Implicit Event Generator based Method : Forward Operation: Backpropagation Operation 
 
 
 
 
 
Fig. 2. An overview of our framework of object tracking pipeline which inverts an optimized implicit event generator (IEG). Given an initially estimated
position T and velocity ˙T and the place and time where intensity change is detected, IEG estimate the corresponding intensity change and gives its error.
Since the whole pipeline is differentiable, we can reﬁne our estimated position T and velocity ˙T by minimizing the error. All operations are done sparsely
only where the events are detected.

that mimic biological retinas and report per-pixel intensity
changes in the form of asynchronous event stream instead
of intensity frames. Due to the unique operating princi-
ple, event-based sensing offers signiﬁcant advantages over
conventional cameras; high dynamic range (HDR), high
temporal resolution, and blurless sensing. As a result, it has
the potential to achieve robust tracking in intense motion and
severe lighting environments.

Many methods have been proposed which use these fea-
tures to achieve tracking in high-speed or high-dynamic-
range environments [3], [4], [5], [6], [7], [8], [9], [10].
In particular, Bryner et al. [7] and Gehrig et al. [6], [10]
proposed method as an extension of Lucas-Kanade tracker
(KLT) [11], [12] developed for frame-based video data. They
extend KLT to sparse intensity difference data from the
event-based camera. In their method, position and veloc-
ity are updated by minimizing the difference between the
observation and the expected observation, as shown in Fig.
1(a). The expected observation is computed by warping the
given photometric 2D/3D map using the current estimate of
position and velocity. Unfortunately, these methods could not
fully take advantage of the sparseness of event data. Still,
they require dense computation to compute the difference,
and this optimization needs to be repeated many times until
convergence.

To realize efﬁcient tracking, we focus on leveraging the
sparsity of event data. To this end, we propose a novel
framework for object tracking using implicit expression. As
shown in Fig. 1(b), our method achieves the goal by updating
the position and velocity based on the difference between
the events generated by the implicit event generator (IEG)
and the observed events. In our method, the difference is
directly computed by the feed-forward operation of sparse
event data without explicitly predict and generate events from
the current position and velocity estimates. This implicit
approach is totally different from the conventional explicit
approach, which requires dense computation to evaluate the
difference. The gradient of position and velocity w.r.t the IEG
is computed by simply backpropagating the error through
IEG realized by MLP. The entire operation, namely feed-
forward operation for difference evaluation and derivative
computation w.r.t the difference, are executed sparsely; com-
putation happens only where the events occur.

In this paper, we ﬁrst introduce a speciﬁc tracking algo-
rithm based on this new framework. For the sake of proof
of concept, we conducted experiments using an AR marker
captured by an actual event-based camera to show that the
method based on this new framework can be applied to real
problems.

Our main contributions are summarized as follows:
• We propose a novel object

tracking framework by
using an implicit event generator (IEG). In our method,
computation is performed only at the event’s location;
therefore, it could be signiﬁcantly efﬁcient than the con-
ventional explicit method requiring dense computation.
• We conduct experiments using data captured from AR
markers using a real event-based camera; demonstrate
the proposed method can be applied to practical prob-
lems.

II. RELATED WORKS

Most of the existing tracking algorithms are developed
based on RGB cameras and track the target object frame
by frame. Recently, the Siamese network-based approach
achieve a better result and many methods have been proposed
based on it [13], [14], [15]. RGB images, however, cannot
provide sufﬁcient tracking performance under fast motion
and low-illumination conditions.

By taking advantage of the HDR and no-motion blur
properties of event cameras, a method for tracking fast
objects and in harsh light environments has been proposed.
Rebecq et al. [3] and Zihao et al. [4] convert IMU data
into lower rate pieces of information at desired times where
events are collected. Chamorro et al. [9] and Alzugaray
et al. [8] explore the high-speed feature tracking with the
dynamic vision sensor (DVS) [16], and Gallego et al. [5]
presented a ﬁlter-based approach that requires to maintain
a history of past camera positions which are continuously
reﬁned. Bryner et al. [7] and Gehrig et al. [6], [10] use
the extended Lucas-Kanade method [11], [12] and update
the position and velocity by warping the given photometric
3D map or frame and taking the difference between the
observation and the expected observation.

Considering the processing in the neural network, sparse
event data is inherently difﬁcult to handle using a convention
deep neural network. Therefore, many neural network-based

𝒲,𝐱𝑀3𝑀𝑀ErrorEvent Stream𝑇̇𝑇,IEG𝒢!Backpropagation𝑀𝑇̇𝑇PoseVelocity{𝑒}Fig. 3.
Procedure for creating artiﬁcial intensity change data for training IEG. The detailed process is as follows: 1) compute the gradient of tracking
target intensity image; 2) randomly determine the time t and velocity v and the rotation velocity ω, and calculate the intensity change value δt when
moving at that velocity and time; 3) based on the threshold value ¯δ, determine whether the intensity change value δt exceeds the change detection threshold
of an event-based camera to generate binary intensity changes where δ(cid:48)
t = (+1, −1, Nan); 4) we apply Gaussian blur G(x, y) to the calculated intensity
change δ(cid:48)
t to obtain the artiﬁcial intensity change data δa to train IEG.

methods for event data ﬁrst convert sparse events to dense
frames and then process it by a dense neural network such
as CNN [17], [18], [19], [20], [21]. Comparing with these
methods,
the computational complexity of our approach
could be signiﬁcantly smaller.

Since NeRF [22], implicit volumetric representations have
attracted in computer vision community [23], [24], [25]. In
this paradigm, the RGB-D scene representation is learned in
a parameterized manner by a multi-layer perceptron (MLP).
Some methods have been proposed to estimate the position
of an object by back-propagating the scene representation
MLP, such as iNeRF [26], [27]. These methods, however,
are proposed for dense RGB images and do not apply to
sparse event data with time-series information. We used IEG
to represent events that occur in response to the position,
time, and velocity of a pixel, making it possible to implicitly
represent events under a variety of conditions.

III. METHOD
We present a framework for motion tracking using a neural
implicit event generator (IEG) to realize efﬁcient motion
update by sparse operations. In our framework, position
T and velocity ˙T is updated by simply feed-forwarding
sparse event observation into IEG and then backpropagate
through the IEG. The IEG G is realized by
the output
differential MLP (parameterized by Θ), and it is trained
before inference to output intensity changes of tracking-
target for given position T and velocity ˙T . To track the
motion, i.e., estimate (T, ˙T ), at inference, (T, ˙T ) is otpmized
to minimized the difference L betweeen obserbation and
estimation for observation of event stream {e} such that:

ˆT , ˆ˙T = argmin

L(T, ˙T |{e}, Θ)

(1)

T, ˙T

In this formulation, explicit generation of events using by
current estimate (T, ˙T ) does not required as existing method
(Fig.1, left). Instead, ours implicitly generates events and
computes the difference directly from the sparse event (Fig.1,
right).

This paper applied the proposed framework for 3DoF

camera motion in a 2D planer scene.

A. Formulation of IEG

and time t and 3DoF ˙T = (vx, vy, ω), and whose output is an
detected intensity change δ ∈ [+/ − /NaN]. We approximate
the intensity change appearance with continuous 3DoF with
the IEG GΘ : (x, y, t, vx, vy, ω) → δ and optimize its weights
Θ to map from each input 6D vector to its corresponding
intensity change δ.

B. Artiﬁcial Data of Intensity Changement

To learn the intensity change with velocity using IEG, we
create artiﬁcial intensity change data based on the following
equation;

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂I
∂x

(cid:12)
(cid:12)
· v(x)∆t
(cid:12)
(cid:12)

> C,

(2)

where v(x) is the speed of coordinate x. C is the threshold
of event ﬁring.

To create intensity change data at various velocities, we
use a randomly determined velocity ˙T = (vx, vy, ω), and
from the edge coordinates of a known object (xe, ye) and
∂x =
the intensity derivative data at
(∂ix, ∂iy). From these values, we can calculate the theo-
retical intensity change value δt at the coordinate (xt, yt),
which corresponding to the edge coordinate (xe, ye) when
the object moves by time t as follows;

those coordinates ∂I

δt = U

(cid:18) ∂I
∂x

(cid:19)

, v, ω, t

=

(cid:18)cos(ωt) − sin(ωt)
cos(ωt)

sin(ωt)

(cid:19) (cid:18)∂ix
∂iy

(cid:19)

(cid:19)
(cid:18)vx + cos(ω)
vy + sin(ω)

t

·

(3)




δ(cid:48)
t =

We set the value corresponding to the threshold of event
ﬁring as ¯δ, and deﬁne δ(cid:48)
t as follows;
(δt > ¯δ)
+1
Nan (−¯δ <= δt <= ¯δ)
(δt < −¯δ)
−1
To perform the optimization operation described in III-D,
we applied a Gaussian blur to the intensity change δ(cid:48)
t to make
it differentiable. Depending on the distance (dx, dy) to the
intensity change δ(cid:48)
t, we use an extended Gaussian function
G(dx, dy) with mean 0 and variance σ;

(4)



.

G(dx, dy) = exp

−

(cid:18)

dx2 + dy2
2σ2

(cid:19)
.

(5)

We represent intensity change appearance as a 6D vector-
valued function whose input is a 2D location x = (x, y, r)

To avoid interference with events generated by other edges,
the blur width is set to w, and the Gaussian blurred intensity

𝜕𝐼𝜕𝐱	,𝑥!,𝑦!(𝑥",𝑦",𝛿")(𝑥!′,𝑦!′,𝛿")(𝑥",𝑦",𝛿"′)Threshold : 𝛿̅Gaussian BlurTracking TargetSparse GradientIntensity ChangeBinary Intensity ChangeGaussian blurredBinary Intensity Change∇𝒰𝜕𝐼𝜕𝐱,𝑣,𝜔,𝑡Algorithm 1 Tracking using gradient of IEG
Ensure: ˆTk = (xk, yk) is the inferenced position at k-
ˆ˙Tk = (vxk, vy k, ωk) is the inferenced

th iteration and
velocity at k-th iteration.

1: (cid:15) ← inf , k ← 0
2: ˆT ← T0
ˆ˙T ← (0, 0, 0)
3:
4: while (cid:15) ≥ ¯(cid:15) do
Lk = (cid:80)
5:
Update ˆTk, ˆ˙Tk
ε ← Lk−1 − Lk, k ← k + 1

6:
7:
8: end while

i∈M (1 − |GΘ(W(xi, ˆTk), ˆ˙T )|)

change δa at (x(cid:48)
follows:

t, y(cid:48)

t) = (xt + dx, yt + dy) is calculated as

δa =

(cid:26) G(dx, dy) × δ(cid:48)
t

0

((cid:112)dx2 + dy2 ≤ w)
((cid:112)dx2 + dy2 > w)

.

(6)

C. Training IEG

We trained the IEG with the artiﬁcial data as described
in III-B. When we input the 6D vector of artiﬁcial data
(x, y, t, vx, vy, ω) and the IEG outputs the estimated intensity
change δp. The IEG is updated by taking the difference
between the generated intensity change δg and the artiﬁcially
calculated intensity change δa;

Ltrain = ||δg − δa||2.

(7)

Fig. 4. How to capture the data by using event camera [28]. (a) on a white
background, and (b) on the wood grain pattern. Since our proposed tracking
algorithm is limited to 2 dimension, we ﬁxed the camera on the dolly so
that the distance between the camera and the AR marker is constant.

E. Object Tracking

During tracking, the events of an existing small group
(window) consisting of M events are processed as in III-
D to obtain the position Tj and velocity ˙Tj at j-th iteration.
Then, at the j + 1-th iteration, slide the window with sliding
width K, and update the position Tj+1 and velocity ˙Tj+1 by
optimizing again as in III-D. The position and velocity do
not change much between the j-th and j + 1-th iterations,
so to update the (Tj, ˙Tj) at j + 1-th iteration could help to
reduce the latency.

Our pipeline does not take acceleration ¨T into account,
but this is not a problem because if the window size M
is sufﬁciently small, the motion within the window can be
approximated as constant velocity motion.

D. Tracking using gradient of IEG

IV. EXPERIMENTAL RESULTS

Let Θ be the parameters of the trained and ﬁxed IEG
G, xi = (xi, yi, ti) the place (xi, yi) and time ti of i-
th detected intensity change, δ(xi) is the intensity change
sensed at xi, ( ˆTk, ˆ˙Tk) the estimated position and velocity at
current optimization step k. Before inputting into the IEG,
xi is transformed by the transformation function W based
on Tk in the 2D plane.

W(xi, T ) =

(cid:18)cos(−Tr) − sin(−Tr)
cos(−Tr)

sin(−Tr)

(cid:19) (cid:18)xi − Ttx
yi − Tty

(cid:19)

, (8)

where Tt is the translation component and Tr is the rotation
component of T . During tracking, optimization is performed
so that the absolute value output by IEG is close to 1;

Lupdate =

(cid:88)

i∈M

(1 − |GΘ(W(xi, ˆTk), ˆ˙T )|),

(9)

The signiﬁcance of this method is that it can estimate the
motion of an object only from the event data. To demonstrate
the effectiveness of this method, we experimented using an
image pattern with AR markers as a target scene to verify
whether the motion of the AR marker can be accurately
estimated or not. Since the events only occur from the
edge of the AR marker, we can effectively show that the
camera motion can be estimated using only sparse events.
Another reason for using AR markers is that they are easy to
annotate (detect and map feature points) and can be used as
a criterion for accuracy evaluation. To capture the event data,
we used Prophesee’s event camera Gen3 [28] with 120dB at
10000fps. As the problem setup for object tracking, we can
assume that the initial position of the object is known by
the object detection algorithm, so we gave its approximate
position as the initial value for tracking. The evaluation was
done quantitatively and qualitatively.

where M is the number of detected events. We employ
gradient-based optimization to solve for ( ˆT , ˆ˙T ) as deﬁned
in Eq. 1. The optimization is iterated until the change in
loss (cid:15) is less than the optimization threshold ¯(cid:15).

In other words, this optimization problem can be deﬁned

as ﬁnding (T, ˙T ) by solving the function 1 − |G| = 0.

A. Experimental Setup

We evaluated the performance of our algorithm by tracking
AR Marker in two environments; one is on the white
background and the other is on the wood grain pattern. Since
our proposed tracking algorithm is limited to 2 dimension,
we ﬁxed the event camera on the dolly as shown in Fig.

(A) White background(B) Wood Grain BackgroundTABLE I
ERROR MEASUREMENTS OF ON WHITE BACKGROUND AND ON WOOD GRAIN BACKGROUND

On White Background

On Wood Grain Background

position

velocity

position

velocity

K

w [pixel]

x [pixel]

y [pixel]

r/10−2 [rad]

vx [pixel/s]

vy [pixel/s]

ω [rad/s]

K

w [pixel]

x [pixel]

y [pixel]

r/10−2 [rad]

vx [pixel/s]

vy [pixel/s]

ω [rad/s]

300

500

2
4
8
16

Ave.

2
4
8
16

Ave.

1.065
1.238
0.623
1.424

1.088

1.090
1.292
0.646
1.438

1.117

2.121
2.641
8.191
3.113

4.016

2.878
2.919
8.919
3.121

4.459

0.013
0.009
0.014
0.124

0.040

0.013
0.009
0.013
0.125

0.040

3.040
2.074
2.680
1.403

2.299

2.859
1.905
2.512
1.383

2.165

3.179
1.541
4.174
1.807

2.675

3.173
1.472
4.056
1.796

2.624

0.021
0.069
0.081
0.091

0.065

0.018
0.071
0.079
0.092

0.065

3000

5000

2
4
8
16

Ave.

2
4
8
16

Ave.

0.773
1.753
0.246
2.635

1.352

0.759
1.728
0.250
2.629

1.342

1.794
0.967
0.281
0.201

0.811

1.783
0.962
0.280
0.201

0.807

0.034
0.023
0.006
0.011

0.019

0.033
0.023
0.006
0.011

0.018

0.692
1.635
0.253
0.208

0.697

0.673
1.525
0.253
0.203

0.665

0.416
4.334
0.363
0.919

1.508

0.381
3.522
0.354
0.880

1.234

0.430
0.130
0.032
0.206

0.199

0.374
0.132
0.028
0.205

0.185

TABLE II

AVERAGE NUMBER OF ITERATION

White Background

Wood Grain Background

K = 300 K = 500 K = 3000 K = 5000

8.65
8.72
11.89
11.80
10.27

13.66
19.45
20.64
20.90
18.66

16.57
20.37
18.16
18.82
18.48

22.66
27.75
27.64
29.83
26.97

w
2
4
8
16
Ave.

Fig. 5. Tracking result visualization of A: on the white background and
B: on the wood grain background when w = 16. A-(1) is when K = 300,
A-(2) is when K = 500, B-(1) is when K = 3000 and B-(2) is when
K = 5000. The white box indicates the predicted position of the AR
marker, the white arrows indicate the velocity and rotation speed, The green
box indicates the ground truth position of the AR marker and the green
arrows indicate the ground truth velocity and rotation speed. Positive events
are shown in red and negative events are shown in blue.

4 so that
the distance between the camera and the AR
marker is constant. We use optimizer Adam [29] with the
recommended hyperparameter settings of: β1 = 0.9, β2 =
0.999, and (cid:15) = 10−8 for training IEG and stochastic gradient
descent (SGD) for gradient-based optimization. The learning
rate was 0.0001 for both training and inference and kept
constant. We set the threshold of synthetic event ﬁring as
¯δ = 0.001 and the optimization threshold as ¯(cid:15) = 1.0 × 10−6.

a) On the White Background: For on the white back-
ground data, we conduct the experiment with M = 2 × 104
and K = 300/500 and evaluated for every 1.5 × 104 events,
i.e. for every ﬁve iterations until the 5000-th iteration for
K = 300 and for every three iterations until the 3000-th
iteration for K = 500.

b) On the Wood Grain Background: For on the wood
grain background data, we conduct
the experiment with
M = 2 × 105 and K = 3000/5000 and evaluated for every
1.5 × 105 events, i.e. for every ﬁve iterations until the 500-th
iteration for K = 3000 and for every three iterations until
the 300-th iteration for K = 5000.

B. Tracking Errors

The position estimation error and velocity estimation error
are given by the mean square error for each value and we
conduct experiments for w = 2, 4, 8, 16. The result of the
data with the white background the wood grain pattern is
shown in Table I.

In the case of on white background and on wood grain
background, the wood grain background is supposed to be
a more difﬁcult problem setting for object tracking, but by
comparing the result of on white background and on wood
grain background. In this experiment, stable tracking was
achieved regardless of the width of the Gaussian blur w and
we can see that the tracking performance does not deteriorate
even in the case of on wood grain pattern. This shows
the possibility that our gradient-based algorithm can work
effectively in various situations.

C. Number of Iterations

The average numbers of iterations for 3000 times of opti-
mization for white background and 300 times of optimization
for wood grain pattern after the second optimization are
shown in Table II. The ﬁrst optimization required a large
number of iterations compared to the second optimization
to localize the object, 1301.25 for the white background and
1684.25 for the wood grain background. As can be seen from
Table II, the number of optimizations increases with larger
K, which indicates that the larger the motion of the object,
it takes time to convergence.

(1) 𝐾=300(2) 𝐾=500(2) 𝐾=5000(1) 𝐾=3000B: Wood Grain BackgroundA: White BackgroundFig. 6. Results of position estimation (solid line) and ground truth (dashed line). (a) shows the result of on a white background, (b) shows that of on
a wood grain background and from left to right, these are the results for x, y, and r. These results show that not only is the difference from the ground
truth small but also that the misalignment does not increase over time. Also, the tracking performance does not depend on the white background or the
wood grain background.

D. Qualitative Evaluation

a) Visualization of tracking results: Fig. 5-A shows
the event of AR marker on white background with tracking
result in white box (position) and white arrow (velocity)
and ground truth in green box (position) and green arrow
(velocity). Fig. 5-B shows the event of AR marker on wood
grain background with tracking result and ground truth as in
Fig. 5-A. This shows our algorithm can track the movements
with small errors for real data regardless of the size of sliding
width K. In addition to that, as shown in Fig. 5-B, our
algorithm gracefully tracks the object precisely despite the
considerable amount of events generated by the background.
b) Robustness over time: The transition of estimated
position T = (x, y, r) (solid line) and ground truth position
(dashed line) is shown in Fig. 6. (a) shows the result of on
the white background and (b) shows that of on the wood
grain background. These results show that not only is the
difference from the ground truth small but also that the
misalignment does not increase over time. Also, the tracking
performance did not depend on whether the background was
wood grain or white. This indicates that our method is not
affected by a large number of events from the background,
and can perform gradient optimization and ﬁnd the position
properly and accurately in every iteration.

V. CONCLUSION

We proposed a novel concept of motion tracking using
implicit expression. Our method estimates an object’s posi-
tion and velocity by gradient-based optimization using our
proposed implicit event generator (IEG). Unlike conven-
tional approaches that explicitly generate events, this method
generates events implicitly. It makes it possible to handle
events sparsely, thereby realizing highly efﬁcient processing,
especially suitable for mobile robotics applications. We show
that our framework could work in the practical scenario

through evaluation using event data capture using a real
event-based camera.

A. Limitation and Future Work

The purpose of this study is to verify the principle of
our novel concept of estimating position and velocity by
gradient-based optimization using IEG as shown in Fig.
2, and for comparison with existing methods, we need to
consider the extension to 6-DoF, speed-up of inference time,
and improvement of Optimizer.

a) Extention to 6-DoF: For performance veriﬁcation,
in this paper, we have applied it to a simple 3-DoF problem
and conﬁrmed its operation. In the future, we would like to
extend the idea to 6-DoF, as shown in [7], [6], [10]. The
problem with extending this method to 6-DoF is that the
number of variables that need to be updated increases. It
is necessary to consider a new scheme for updating each
variable appropriately using backpropagation, rather than
simply updating each variable as in this proposed method.

b) Speed up using Look-up table: In this paper, we
have only veriﬁed our concept and have not studied the
reduction of inference time, which is necessary for practical
applications such as AR/VR. To conduct the fast inference,
we think it is an effective algorithm that reduces inference
time by combining look-up tables and MLP, as proposed by
Sekikawa et al. [30].

c) Optimizer:

In this paper, we used SGD as the
optimizer for inference. However, it is necessary to consider
optimizers for speeding up and stabilizing inference, such
as Momentum [31], Nesterov’s accelerated gradient [32]. In
addition, we would like to consider the application of the
Gauss-Newton method or the Levenberg-Marquardt method,
which use the (approximate) second-order derivatives.

GroundTruth*=16*=8*=4*=203000iteration10002000300200![pixel]GroundTruth*=16*=8*=4*=20900iteration300600300380![pixel]0900iteration300600150270GroundTruth*=16*=8*=4*=2![pixel]GroundTruth*=16*=8*=4*=203000iteration10002000400200![pixel](a) White Background(b) Wood Grain BackgroundGroundTruth*=16*=8*=4*=20900iteration300600-2.4![rad]03000100020001.351.70![rad]GroundTruth*=16*=8*=4*=2REFERENCES

[1] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a
versatile and accurate monocular slam system,” T-RO, vol. 31, no. 5,
pp. 1147–1163, 2015.

[2] C. Forster, Z. Zhang, M. Gassner, M. Werlberger, and D. Scaramuzza,
“Svo: Semidirect visual odometry for monocular and multicamera
systems,” T-RO, vol. 33, no. 2, pp. 249–265, 2016.

[3] H. Rebecq, T. Horstschaefer, and D. Scaramuzza, “Real-time visual-
inertial odometry for event cameras using keyframe-based nonlinear
optimization,” in BMVC, 2017.

[4] A. Zihao Zhu, N. Atanasov, and K. Daniilidis, “Event-based visual

inertial odometry,” in CVPR, 2017, pp. 5391–5399.

[5] G. Gallego, J. E. Lund, E. Mueggler, H. Rebecq, T. Delbruck, and
D. Scaramuzza, “Event-based, 6-dof camera tracking from photometric
depth maps,” TPAMI, vol. 40, no. 10, pp. 2402–2412, 2017.

[6] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, “Asyn-
chronous, photometric feature tracking using events and frames,” in
ECCV, 2018, pp. 750–765.

[7] S. Bryner, G. Gallego, H. Rebecq, and D. Scaramuzza, “Event-based,
direct camera tracking from a photometric 3d map using nonlinear
optimization,” in ICRA, 2019, pp. 325–331.

[8] I. Alzugaray Lopez and M. Chli, “Haste: multi-hypothesis asyn-
chronous speeded-up tracking of events,” in BMVC, 2020, p. 744.
[9] W. O. Chamorro Hernandez, J. Andrade-Cetto, and J. Sol`a Ortega,
“High-speed event camera tracking,” in BMVC, 2020, pp. 1–12.
[10] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, “Eklt:
Asynchronous photometric feature tracking using events and frames,”
International Journal of Computer Vision, vol. 128, no. 3, pp. 601–
618, 2020.

[11] B. D. Lucas and T. Kanade, “An iterative image registration technique
with an application to stereo vision,” in IJCAI, vol. 2, 1981, p.
674–679.

[12] S. Baker and I. Matthews, “Lucas-kanade 20 years on: A unifying

framework,” IJCV, vol. 56, pp. 221–255, 2004.

[13] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr,
“Fully-convolutional siamese networks for object tracking,” in ECCV,
2016, pp. 850–865.

[14] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual
tracking with siamese region proposal network,” in CVPR, 2018, pp.
8971–8980.

[15] P. Voigtlaender, J. Luiten, P. H. Torr, and B. Leibe, “Siam r-cnn: Visual

tracking by re-detection,” in CVPR, 2020, pp. 6578–6588.

[16] P. Lichtsteiner, C. Posch, and T. Delbruck, “A 128x128 120 db 15 us
latency asynchronous temporal contrast vision sensor,” JSSC, vol. 43,
no. 2, pp. 566–576, 2008.

[17] A. I. Maqueda, A. Loquercio, G. Gallego, N. Garc´ıa, and D. Scara-
muzza, “Event-based vision meets deep learning on steering prediction
for self-driving cars,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018, pp. 5419–5427.
[18] R. Baldwin, R. Liu, M. Almatraﬁ, V. Asari, and K. Hirakawa, “Time-
ordered recent event (tore) volumes for event cameras,” arXiv preprint
arXiv:2103.06108, 2021.

[19] S. Tulyakov, F. Fleuret, M. Kiefel, P. Gehler, and M. Hirsch, “Learning
an event sequence embedding for dense event-based deep stereo,” in
Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2019, pp. 1527–1537.

[20] D. Gehrig, A. Loquercio, K. G. Derpanis, and D. Scaramuzza, “End-
to-end learning of representations for asynchronous event-based data,”
in Proceedings of the IEEE/CVF International Conference on Com-
puter Vision, 2019, pp. 5633–5643.

[21] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, “Matrix-
lstm: a differentiable recurrent surface for asynchronous event-based
data,” arXiv preprint arXiv:2001.03455, vol. 1, no. 2, p. 3, 2020.
[22] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoor-
thi, and R. Ng, “Nerf: Representing scenes as neural radiance ﬁelds
for view synthesis,” in ECCV, 2020, pp. 405–421.

[23] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger,
“Occupancy networks: Learning 3d reconstruction in function space,”
in CVPR, 2019, pp. 4460–4470.

[24] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M.
Seitz, and R. Martin-Brualla, “Deformable neural radiance ﬁelds,”
arXiv preprint arXiv:2011.12948, 2020.

[25] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Doso-
vitskiy, and D. Duckworth, “Nerf in the wild: Neural radiance ﬁelds
for unconstrained photo collections,” in CVPR, 2021, pp. 7210–7219.
[26] L. Yen-Chen, P. Florence, J. T. Barron, A. Rodriguez, P. Isola, and T.-
Y. Lin, “iNeRF: Inverting neural radiance ﬁelds for pose estimation,”
in IROS, 2021.

[27] S.-Y. Su, F. Yu, M. Zollhoefer, and H. Rhodin, “A-nerf: Surface-
free human 3d pose reﬁnement via neural rendering,” arXiv preprint
arXiv:2102.06199, 2021.

[28] “Prophesee metavision for machines,” https://www.prophesee.ai/.
[29] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-

tion,” arXiv preprint arXiv:1412.6980, 2014.

[30] Y. Sekikawa and T. Suzuki, “Irregularly tabulated mlp for fast point

feature embedding,” arXiv preprint arXiv:2011.09852, 2020.
[31] N. Qian, “On the momentum term in gradient descent

learning

algorithms,” Neural networks, vol. 12, no. 1, pp. 145–151, 1999.
[32] Y. Nesterov, “A method for unconstrained convex minimization prob-
lem with the rate of convergence o (1/kˆ 2),” in Doklady an ussr, vol.
269, 1983, pp. 543–547.

