Attention routing between capsules

Jaewoong Choi

Hyun Seo

Suii Im Myungjoo Kang

Seoul National University
{chjw1475, hseo0618, a5828167, mkang}@snu.ac.kr

9
1
0
2

v
o
N
3
1

]

V
C
.
s
c
[

4
v
0
5
7
1
0
.
7
0
9
1
:
v
i
X
r
a

Abstract

In this paper, we propose a new capsule network archi-
tecture called Attention Routing CapsuleNet (AR CapsNet).
We replace the dynamic routing and squash activation func-
tion of the capsule network with dynamic routing (Capsu-
leNet) with the attention routing and capsule activation.
The attention routing is a routing between capsules through
an attention module. The attention routing is a fast forward-
pass while keeping spatial information. On the other hand,
the intuitive interpretation of the dynamic routing is ï¬nding
a centroid of the prediction capsules. Thus, the squash ac-
tivation function and its variant focus on preserving a vec-
tor orientation. However, the capsule activation focuses on
performing a capsule-scale activation function.

We evaluate our proposed model on the MNIST, affNIST,
and CIFAR-10 classiï¬cation tasks. The proposed model
achieves higher accuracy with fewer parameters (Ã—0.65 in
the MNIST, Ã—0.82 in the CIFAR-10) and less training time
than CapsuleNet (Ã—0.19 in the MNIST, Ã—0.35 in the CIFAR-
10). These results validate that designing a capsule-scale
operation is a key factor to implement the capsule concept.
Also, our experiment shows that our proposed model is
transformation equivariant as CapsuleNet. As we perturb
each element of the output capsule, the decoder attached to
the output capsules shows global variations. Further ex-
periments show that the difference in the capsule features
caused by applying afï¬ne transformations on an input im-
age is signiï¬cantly aligned in one direction.

1. Introduction

Convolutional neural networks(CNNs) have had much
success in computer vision tasks [8] [13] [3] [6]. The convo-
lutional layer is an effective method to extract local features
due to its local connectivity and parameter sharing with spa-
tial location. However, the convolutional layer has a lim-
ited ability to encode a transformation. For example, if the
convolutional layer is combined with a max-pooling layer,
the extracted feature is local translation invariant. As CNN
models become deeper [3] [15], the receptive ï¬eld of each

feature is getting larger. Then, the information loss from the
translation invariance also increases.

To overcome the transformation invariance of CNNs, the
transforming autoencoder [4] uses the concept of â€capsuleâ€.
A capsule is a vector representation of a feature. Each cap-
sule not only represents a speciï¬c type of entity but also de-
scribes how the entity is instantiated, such as precise pose
and deformation. In other words, the capsules are transfor-
mation equivariant.

The CapsuleNet [12] is a novel method that implements
the idea of the capsules. By introducing the dynamic rout-
ing algorithm and squash activation function 1, CapsuleNet
uses vector-output capsules as a basic unit instead of scalar-
output features.

squash(sj) =

||sj||2
1 + ||sj||2

sj
||sj||

(1)

where sj is a pre-activation capsule. However, CapsuleNet
has a room for development. The number of parameters of
CapsuleNet is much larger than that of comparable perfor-
mance CNN-based models. Also, the dynamic routing is an
iterative process. The reported accuracy of CapsuleNet on
the benchmark datasets like CIFAR-10 is inferior to state-
of-the-art CNN models. [18]

In this paper, we propose a convolutional capsule net-
work architecture comprised of building blocks of CNNs.
We substitute the dynamic routing and squash capsule-
activation function of CapsuleNet[12] with attention rout-
ing and capsule activation. In the attention routing, the log
probabilities of agreement coefï¬cients between the lth layer
and the (l + 1)th layer are learned by a scalar-product be-
tween the capsules of the lth layer and the kernel of con-
volution. The kernel of convolution serves as an approxi-
mation of the reference vector to perform routing. By re-
placing an iterative process of the dynamic routing with
forward-pass convolution, the attention routing is fast while
maintaining spatial information. Two important properties
of squash activation function 1 is that the squash activation
function preserves a vector orientation and is a capsule-wise
activation function, not an element-wise activation function
such as ReLU or tanh. The dynamic routing is an unsuper-

 
 
 
 
 
 
Figure 1. Overview of AR CapsNet. AR CapsNet is composed of primary caps layer, conv caps layer, and fully conv caps layer. BN denotes
the batch normalization. Conv Transform and Caps Activation denotes the convolutional transform and capsule activation respectively.

vised algorithm to ï¬nd a centroid-like output capsule of the
prediction capsules. Therefore, the squash activation func-
tion and its variant 2 [18] focus on preserving a capsules
orientation.

squash variant(sj) =

1 âˆ’

(cid:18)

1
exp (||sj||)

(cid:19) sj
||sj||

(2)

However, we focus on the capsule-wise operation rather
than preserving orientation. The capsule activation per-
forms an afï¬ne transform on the capsules and then applies
an element-wise activation function. The capsules on the
same capsule channel share parameters used in the afï¬ne
transformation. Thus, the capsules on the same capsule
channel are mapped to the same feature space, and the op-
eration is parameter efï¬cient. Therefore, the capsule activa-
tion is a capsule-wise function that does not preserve a vec-
tor orientation. Since the capsule activation applies a non-
linear transformation to a linear combination of the predic-
tion capsules, parametrizing the routing process through the
attention routing is compatible. We refer to our proposed
model as Attention Routing CapsuleNet (AR CapsNet).

We evaluate the AR CapsNet on three datasets (MNIST,
affNIST, and CIFAR-10). The AR CapsNet signiï¬cantly
outperforms CapsuleNet in the affNIST and CIFAR-10
classiï¬cation task and shows a comparable performance in
the MNIST dataset while being faster and using less than
half parameters than CapsuleNet. Moreover, the AR Cap-
sNet preserves the transformation equivariant property of
CapsuleNet. As we perturb each element of the output
capsule, the decoder attached to the output capsules shows
global variations as in [12]. Further experiment showed that
the afï¬ne transformations on an input image cause the fea-
ture capsules to change in the signiï¬cantly aligned direc-
tion. From these experiments, we prove that the AR Cap-
sNet encodes an afï¬ne transformation on the input image in
some basis of capsule space. In addition, our proposed ar-
chitecture is constructed in a convolutional manner so that
it can be easily extended to a deeper network structure.

1.1. Contribution

â€¢ We propose a new architecture called AR CapsNet by
introducing two modiï¬cations to the CapsuleNet [12].
These modiï¬cations are the attention routing and cap-
sule activation.

â€¢ The capsule activation expands the concept of the ex-
isting capsule-wise activation functions such as the
squash activation. The capsule activation performs
an orientation-nonpreserving transform on the pre-
activation capsules. The performance of the AR Cap-
sNet demonstrates that the transformation equivariant
features can be extracted even if the routing process is
not restricted to the clustering approach and the cap-
sule activation is not limited to the normalization.

â€¢ The AR CapsNet shows better results on the affNIST,
and CIFAR-10 classiï¬cation tasks and comparable re-
sults on the MNIST classiï¬cation task while using
much smaller parameters than CapsuleNet. Also, the
AR CapsNet preserves the transformation equivariant
property of the CapsuleNet. As we perturb each ele-
ment of the output capsule, the decoder attached to the
output capsule shows global variation as in [12].

â€¢ To investigate the transformation equivariance further,
we suggest a new experiment. We observe the differ-
ence in the output capsule caused by applying transfor-
mations on an input image. In the AR CapsNet, these
difference vectors are signiï¬cantly aligned compared
to a set of random vectors. These results demonstrate
that transformation on an input image is encoded in
some basis vector.

2. Related Works

The CNN models that consist of convolutional layers and
max-pooling layers have a local translation invariance. To
overcome transformation invariance, CapsuleNet [12] uses
vector-output capsules and the dynamic routing in place of
scalar-output features and max-pooling. By demonstrating

ConvTransformPrimary Caps Layer...Inputdwh...d'w'h'nConv Caps LayerFully Conv Caps LayerConv3x3,Â BNConvolution layern'LengthOperationCapsActivationConvTransformCapsActivationAttentionRoutingConvTransformCapsActivationAttentionRoutingthat the dimension perturbation of digit capsules leads to a
global transformation of the reconstruction image, Capsu-
leNet claims to have transformation equivariance.

A number of methods to improve the performance of
CapsuleNet have been proposed in [5] [17] [1] [9] [10].
In [17], they interpreted the routing-by-agreement process
as an optimization problem of minimizing clustering loss.
They proposed another routing process from the point of
view of clustering. Their approach achieved better results
on an unsupervised perceptual grouping task compared to
[12]. The matrix capsules with EM routing [5] proposed
another routing method called EM routing. The EM routing
measures compatibility between matrix capsules by clus-
tering matrix capsules through Gaussian distributions. The
matrix capsules with EM routing achieved the state-of-the-
art performance on a shape recognition task using the small-
NORM dataset. The spectral capsule networks [1] is a vari-
ation of [5]. Spectral capsule networks use a singular value
to compute the activation of each capsule instead of the lo-
gistic function in [5]. Spectral capsule networks achieved
better performance on a diagnosis dataset compared to [5]
and deep GRU networks while showing faster convergence
compared to [5].

The SegCaps [9] applied a capsule network to the object
segmentation task. The SegCaps introduced two modiï¬ca-
tions to the CapsuleNet and devised the concept of deconvo-
lutional capsules from these modiï¬cations. The two modi-
ï¬cations are the locally connected dynamic routing and the
sharing of transformation matrices within the same capsules
channel. The sharing of transformation matrices is equiva-
lent to the convolutional transform of our conv caps layer
except for the addition of biases. The EncapNet [10] per-
forms a one-time pass approximation of the routing process
by introducing two branches. The master branch extracts
a feature from the locally connected capsules as in [9] and
the aide branch combines information from all the remain-
ing capsules. Also, they introduced a Sinkhorn divergence
loss which works as a regularizer. The EncapNet achieved
competitive results on CIFAR-10/100, SVHN, and a subset
of ImageNet.

Our proposed model uses attention architecture as a rout-
ing algorithm. The attention architecture learns a compat-
ibility function between low-level features and high-level
features.
In [2], the output of attention architecture is a
weighted sum of input features, and the weights are the
compatibilities based on the input features and the RNN
hidden state. The compatibility function is a feedforward
neural network with a softmax activation function. In [11],
they experimented on various kinds of attention architec-
tures from global attention to local attention and three com-
patibility functions. One of the three compatibility func-
tions was a softmax output of the scalar-products between
a target hidden state vector and source hidden state vector.

Figure 2. Detailed operation process of conv caps layer. Conv
Transform denotes the convolutional transform. The convolutional
transform performs a locally connected afï¬ne transform on each
capsule channel. The attention routing learns the agreement be-
tween the convolutional transformed capsules for each spatial lo-
cation. The capsule activation applies a capsule-wise activation
function on each capsule channel.

The transformer network [16] uses a similar attention archi-
tecture as in [11]. Transformer performs a scaled scalar-
product between the keys and values and then applies a
softmax activation function. Our proposed attention routing
computes the scalar product between capsules and a kernel.

3. Proposed Method

Our proposed architecture consists of primary caps
layer, conv caps layer, and fully conv caps layer. We de-
note the lth capsule layer as ul
w,h,d,n, where w, h, d, and
n index the spatial width axis, spatial height axis, capsule
dimension axis, and capsule channel axis, respectively. We
refer to the capsules with the same capsule channel index as
a capsule channel ul

1.

(:,:,:,n)

3.1. Primary Caps Layer

We denote the primary capsule layer as the 0th cap-
sule layer. Before entering the primary caps layer, we ex-
tract local features Ëœx from the input image x by performing
the convolution blocks composed of convolution layer and
batch normalization.[7] We consider the local features Ëœx as
In our primary caps layer with N
a single capsule layer.
channels of D dimensional output capsules, 3x3 convolu-
tion with kernels of ï¬lter size D and stride 2 is performed
on the input capsules Ëœx N times independently. Each output
of a convolution layer is a capsule channel.

s0
(:,:,:,n0) = ReLU (Conv3Ã—3 (Ëœx))

(3)

Note that this is equivalent to performing a 3x3 convo-
lution of N Ã— D kernels and then reshaping the features
to (B, W, H, D, N ) where B denotes the batch size and
(W, H) denotes the spatial size of the capsule layer. Then,
the capsule activation is applied to each capsule channel in-
stead of the squash activation function in [12].

1ul

(:,:,:,n0) := {ul

w,h,d,n|n = n0}

ConvTransformAttention Routingthrough capsule channelInput capsule layersCapsuleActivationx N output capsule channelOutputput capsule layers3.2. Capsule Activation

The capsule activation takes an afï¬ne transformation on
each capsule channel and then applies tanh activation func-
tion. The capsules on the same capsule channel share pa-
rameters of the afï¬ne transformation. Thus, the capsule ac-
tivation is equivalent to taking 1x1 convolution with a kernel
of ï¬lter size D and tanh activation function on each capsule
channel.

u(:,:,:,n0) = tanh (cid:0)Conv1Ã—1

(cid:0)s(:,:,:,n0)

(cid:1)(cid:1)

(4)

Each element of the output capsules of the capsule activa-
tion depends on the corresponding input capsule. Therefore,
the capsule activation is a capsule-wise activation function.
The tanh activation function normalizes each element of
capsules, thus stabilizes the lengths of the capsules.

3.3. Conv Caps Layer

We denote the input to the lth conv caps layer as ulâˆ’1
w,h,d,n
which is the output of the (l âˆ’ 1)th conv caps layer. We ï¬rst
perform a convolutional transform on each capsule chan-
nel. The convolutional transform is a locally-connected
afï¬ne transformation sharing parameters within the same
capsule channel. In particular, the convolutional transform
is a 3x3 convolution of Dl kernels without activation func-
tion, where Dl denotes the capsule dimension of the lth
conv caps layer.

Ëœsl,n
(:,:,:,m) = Conv3Ã—3

(cid:16)

ulâˆ’1

(:,:,:,m)

(cid:17)

(5)

Each output of the convolutional transform is fed to the at-
tention routing. The output of attention routing is a linear
combination of the convolutional transformed capsules with
the same spatial location.

sl
(w,h,:,n) =

(cid:88)

cl,n
(w,h,m) Â· Ëœsl,n

(w,h,:,m)

(6)

m=1,Â·Â·Â· ,N lâˆ’1

(w,h,m)

(w,h,:,m) âˆˆ RDl

(w,h,:,n), Ëœsl
where the capsules sl
. The
weights cl,n
âˆˆ R are computed by the at-
The log probabilities bl,n
tention routing.
(w,h,m) are
the scalar-product between a concatenation of capsules
[Ëœul
w,h,:,1, Ëœul
w,h,:,N lâˆ’1] and a parameter vector
n âˆˆ RDlÃ—N lâˆ’1
wl
. This operation can be implemented
efï¬ciently by 3D convolution on the convolutional trans-
n âˆˆ R1Ã—1Ã—DlÃ—N lâˆ’1
formed capsule layers with kernels wl
,
stride=(1,1,1), and valid padding.

w,h,:,2, Â· Â· Â· , Ëœul

bl,n
(:,:,:) = Conv3D1Ã—1Ã—Dl

(cid:16)
Ëœsl
(:,:,:,:)

(cid:17)

(7)

The weights cl,n
bilities bl,n

(w,h,m) are softmax outputs of the log proba-

(w,h,m) along the capsule channel axis.

cl,n
w,h,m =

exp(bl,n
(w,h,m))
1â‰¤mâ‰¤N lâˆ’1 exp(bl,n

(cid:80)

(w,h,m))

(8)

Note that the attention routing adjusts the weight cl,n
w,h,m for
each spatial location (w, h) corresponding to the convolu-
tional transformed capsules {Ëœul
(w,h,:,m)}m with the same
spatial location.

Finally, the capsule activation is performed on each cap-
sule channel sl
(:,:,:,n). A set of convolutional transform, at-
tention routing, and capsule activation is performed inde-
pendently N l times. (i.e., each output of the convolutional
transform, attention routing, and capsule activation is a cap-
sule channel ul

(:,:,:,n) )

ul

(:,:,:,n) = tanh

(cid:16)

Conv1Ã—1

(cid:16)

sl
(:,:,:,n)

(cid:17)(cid:17)

(9)

Intuitively, the dynamic routing uses a centroid of the
transformed capsules as the reference vector to measure
agreement by scalar-product. As the dynamic routing pro-
cess iterates, the capsule with the higher agreement has
a larger weight, and the reference vector evolves in that
capsule direction. On the other hand, since the capsule
activation in the conv caps layer do not preserve vector
the output capsule ul
(:,:,:,n) cannot approxi-
orientation,
mate the centroid of transformed capsules {Ëœul
(w,h,:,n)}. In-
stead of measuring agreement between the transformed cap-
sules and the output capsule ul
(:,:,:,n), the attention routing
parametrizes the routing process. The parameter vector wl
n
which is the kernel of convolution serves as an approxima-
tion of the reference vector to perform routing.

We propose replacing the dynamic routing of [12] with
the convolutional transform and attention routing. Com-
pared to dynamic routing, our proposed operation is faster
and more parameter efï¬cient. Since dynamic routing is con-
structed in a fully connected manner, the transform weight
matrices are assigned for each pair of the input capsule and
output capsule. We share the weight matrices across the
spatial location and keep the translation equivariance by
performing 3x3 convolution on the lth layer in the convo-
lutional transform.(Section 4.4) Besides, the dynamic rout-
ing has an iterative routing process to compute the weight
cl
w,h,n. On the other hand, by introducing a trainable param-
eter vector, our proposed operation is a fast forward-pass.

3.4. Fully Conv Caps Layer

The fully conv caps layer is almost the same as the
conv caps layer and serves as the output layer of AR Cap-
sNet. The convolutional transform combines capsule fea-
tures from the all spatial location by applying a kernel of

Algorithm 1: The process of Attention Routing
1 Input: u(cid:96)=0 âˆˆ R(w,h,D(cid:96)=0,N (cid:96)=0)
2 for (cid:96) = 1, Â· Â· Â· , L do
3

for n = 1, Â· Â· Â· , N (cid:96) do

/* Convolutional transformation
*/
for each capsule channel

for m = 1, Â· Â· Â· , N (cid:96)âˆ’1 do

Ëœs(cid:96),n
(:,:,:,m) â† Conv2D3Ã—3(u(cid:96)âˆ’1

(:,:,:,m))

end

/* Attention through capsule

channel

bl,n â† Conv3D1Ã—1Ã—D(cid:96)(Ëœs(cid:96))
for w, h = 1, Â· Â· Â· , W (cid:96), H (cid:96) do

*/

cn,l
w,h,:N (cid:96)âˆ’1 â† softmax(b(cid:96),n
s(cid:96)
w,h,:,n â†
(cid:80)

m=1,Â·Â·Â· ,N lâˆ’1 cn,l

w,h,m Â· Ëœs(cid:96),n

w,h,:N (cid:96)âˆ’1)

(w,h,:,m)

end

/* Capsule activation for each

capsule channel
(:,:,:,n) â† tanh(Conv2D1Ã—1(s(cid:96)
u(cid:96)

(:,:,:,n)))

*/

end

4

5

6

7

8

9

10

11

12

13

14

15

16
17 end

the same spatial size as the input with valid padding. There-
fore, the output of the fully conv caps Layer has a shape of
(1, 1, DL, N L).

3.5. Margin Loss and Reconstruction Regularizer

We adopt the margin loss and reconstruction regularizer
in [12]. Since the output capsules of capsule activation have
DL where DL denotes the capsule di-
a length of up to
mension, we use the normalized length to predict the prob-
ability of the corresponding class of the dataset.

âˆš

||uL

n ||nor =

||uL
n ||
âˆš
DL

(10)

where ||uL
n || denotes the output capsules of the fully conv
caps layer and n indexes the capsule channel axis. We ap-
plied the Margin loss, Ln, for each class n on the ||uL
n ||nor.

Ln = Tn max(0, m+ âˆ’ ||uL

n ||nor)2
+ Î»(1 âˆ’ Tn) max(0, ||uL

n ||nor âˆ’ mâˆ’)2

(11)

where Tn = 1 iff the corresponding class of output capsule
is present and m+ = 0.9 and mâˆ’ = 0.1.

The output capsules {uL

n }n=1,Â·Â·Â· ,N are fed to the recon-
struction decoder. We used a decoder consisting of 3 fully

connected layers as in [12] except that our decoder has (512,
512, the number of input image pixel) nodes. We refer to
the mean of L2 loss between an input image and the decoder
output as a reconstruction loss. We add the reconstruction
loss that is scaled down by 0.3 to the margin loss as a regu-
larization method.2

4. Experiments

We evaluate our model on the MNIST, affNIST, and
CIFAR-10 datasets. For each dataset, we split the training
images into a training set (90%) and a validation set (10%).
We choose the model with the lowest validation error and
evaluate the model on the test set. Then, we compare the
results with CapsuleNet [12]. We use a Keras implementa-
tion3 for CapsuleNet.

Before training the model on the image dataset, we di-
vide each pixel value by 255 so that it is scaled in the range
of 0 to 1. Then, we extract the local features Ëœx from an input
image through two convolutional layers with batch normal-
ization(BN) [7] and ReLU activation function. These two
convolutional layers use 3x3 kernels with a stride 1. Then,
the features go through the AR CapsNet to obtain vector
outputs. For each conv caps layer and fully conv caps layer,
the dropout layer [14] of keep probability 0.5 is applied to
the input capsules before the convolutional transform.

We use the RMSprop optimizer with rho of 0.9 and decay
of 1e-4 to minimize the loss deï¬ned in Section 3.5. We set
the learning rate as 0.001 and batch size as 100

4.1. Classiï¬cation Results on MNIST and affNIST

Dataset The MNIST dataset is composed of 28Ã—28 hand-
written digit images. We adopted 0.1 translation as a data
augmentation for the MNIST dataset. The affNIST dataset
consists of 40 Ã— 40 images, which are obtained by apply-
ing various afï¬ne transformations such as rotation and ex-
pansion to the images from MNIST. For the affNIST clas-
siï¬cation task, we trained our model with randomly trans-
lated MNIST images in horizontal or vertical directions up
to shift fraction 0.2 as in [12]. Any other afï¬ne transfor-
mations like rotations were not used in the training process.
The affNIST dataset has a separate validation set, thus we
chose the model with the lowest validation error based on
the affNIST validation set. Then, we tested our model with
the affNIST test set.

Implementation
For the MNIST and affNIST datasets,
we used the AR CapsNet which consists of a primary caps
layer, one conv caps layer and fully conv caps layer. Be-
fore entering the AR CapsNet, an input image goes through

2CapsuleNet [12] scaled the reconstruction loss by 0.392. Since we use
the mean of L2 loss and CapsuleNet use the sum of L2 loss, 0.392 = 0.0005
Ã— 784.

3https://github.com/XifengGuo/CapsNet-Keras

Method
CapsuleNet[12]
CapsuleNet+ensemble(7)
Ours
Ours+ensemble(7)

MNIST
99.45âˆ—
-
99.46
-

MNIST+
99.75 (99.52âˆ—)
-
99.46
-

affNIST
79.0
-
91.6
-

C10
63.1âˆ—
-
87.19
88.94

C10+
69.6âˆ—
89.4
88.61
90.11

Table 1. Test accuracy (%) on the MNIST, affNIST, and CIFAR-10 classiï¬cation tasks. C10 represents the CIFAR-10 dataset. + denotes
training with data augmentation. We adopted translation for MNIST+ and translation, rotation, and horizontal ï¬‚ip for C10+. âˆ— indicates
the results from our experiment.

two convolutional layers of 64 channels (3x3 Conv - BN
- ReLU). The primary caps layer has eight channels of 16-
dimensional capsules, the conv caps has eight channels, and
the fully conv caps layer has ten channels. Each capsule
channels in the conv caps layer and fully conv caps layer
has 32 dimensions in the MNIST and 16 dimensions in the
affNIST. We decreased the spatial size of the capsule fea-
tures by applying a 3x3 convolution of stride 2 in the con-
volutional transform of the conv caps layer. We trained our
model for 20 epochs.

Accuracy Our model shows a comparable accuracy with
the substantial decrease in the number of parameters and
training time. Our model with 5.31M parameters achieved
99.45% accuracy on the MNIST dataset without any data
augmentation and 99.46% accuracy with data augmenta-
tion.
(Table 1) The CapsuleNet with 8.21M parameters
achieved 99.45% accuracy without any data augmentation
and 99.52% with data augmentation. The reported accu-
racy of CapsuleNet on the MNIST dataset with translation
augmentation is 99.75% [12]. Also, the training took 37.2
seconds per epoch for our proposed model and 199.5 sec-
onds per epoch for CapsuleNet when we experimented on
GTX 1080 GPUs.

In the affNIST experiments, there are two options to gen-
erate training images from the MNIST dataset. The ï¬rst
option is to create a larger dataset by generating a set of
all the possible augmented data before training. The sec-
ond option is to apply translation over the original dataset
for each epoch. The reported accuracy of CapsuleNet is
79% and that of the baseline CNN model is 66% in [12].
The experiment is performed on the former option.4 Our
proposed model achieved 91.6% accuracy for the latter op-
tion. Under the comparable experiment, our model outper-
formed the CapsuleNet and the baseline CNN model. Since
our model is transformation equivariant (Section 4.4), our
model is robust to afï¬ne transformations.

4.2. Classiï¬cation Results on CIFAR-10

Dataset The CIFAR-10 dataset is a 32Ã—32 colored natural
images in 10 classes. We adopted 0.1 translation, rotation
up to 20 degrees, and horizontal ï¬‚ip as a data augmentation

4https://github.com/Sarasra/models/tree/master/research/capsules

for CIFAR-10.

Implementation
For the CIFAR-10 classiï¬cation task,
we added four conv caps layer between a primary caps layer
and fully conv caps layer. We decreased the spatial size of
the capsule features in the ï¬rst conv caps layer as in Sec-
tion 4.1. Each conv caps layer has eight channels of 32-
dimensional capsules and is connected to the next conv caps
layer with a residual connection [3]. Note that the residual
connection in [3] connects the lth layer and (l + 2)th layer,
but our residual connection connects the lth conv caps layer
and (l + 1) conv caps layer. We trained our model for 200
epochs.

Accuracy
The results in Table 1 show that our model
outperforms CapsuleNet with and without data augmenta-
tion. CapsuleNet with 11.74M parameters shows 63.1%
accuracy in C10 and 69.6% accuracy in C10+. However,
our proposed model with 9.6M parameters shows 87.19 %
accuracy in C10 and 88.61% accuracy in C10+.
In [12],
an ensemble of 7 models achieves 89.4% accuracy when
the models are trained with 24 Ã— 24 patches of images and
the introduction of a none-of-the-above category. However,
an ensemble of 7 AR CapsNet models trained with C10+
achieved 90.11% test accuracy. Note that C10+ only uses
rotation, shift, and horizontal ï¬‚ip as data augmentation and
not the cropping or the none-of-the-above category.

4.3. Robustness to hyperparameters

Implementation The AR CapsNet requires a set of hy-
perparameters, such as the number of conv caps layer and
the capsule dimension of each capsule layer. To test the ro-
bustness to hyperparameters, we evaluate the AR CapsNet
in the CIFAR-10 classiï¬cation tasks according to the vari-
ous setting of hyperparameters. The evaluated AR CapsNet
architecture is the same as the models mentioned in Sec-
tion 4.1 and 4.2. The primary caps layer has eight chan-
nels of 16-dimensional capsules, and the conv caps layer
has eight capsule channels. In the setting of hyperparame-
ters, the Conv caps layer denotes the number of conv caps
layer between the primary caps layer and fully conv caps
layer. In every model with at least one conv caps layer, the
ï¬rst conv caps layer decreases the spatial size of the capsule
layer by adopting a 3x3 convolution of stride 2 in the con-

Conv caps layer Caps dim Params

0

1

2

3

4

16
32
16
32
16
32
16
32
16
32

C10
7.3M 77.51
12.6M 77.44
3.5M 81.96
7.7M 82.39
3.7M 84.48
8.4M 85.46
3.8M 85.56
8.9M 86.56
4.0M 86.37
9.6M 87.19

C10+
81.89
81.97
82.83
83.92
84.77
87.01
86.93
87.91
87.21
88.61

Table 2. Test accuracy (%) on the MNIST and CIFAR-10 for
various hyperparameters. In each experiment, we trained a model
for 200 epochs and chose the model with the lowest validation
error. For each hyperparameter setting, AR CapsNet shows stable
performance without showing severe degradation.

Figure 3. Decoder outputs according to dimension perturbations.
We observed the variations of decoder output as we perturbed
DL from
one dimension of the output capsules by steps of 0.05
DL. The perturbation leads to the combi-
âˆ’0.25
nation of variations in the decoder output images. (e.g., rotation,
thickness, etc.).

DL to +0.25

âˆš

âˆš

âˆš

volutional transform. The Capsule dim denotes the capsule
dimension of the conv caps layer and fully conv caps layer.

Robustness All the AR CapsNet models trained with
CIFAR-10 dataset show decent performance in Table 2. In-
creasing capsule dimension and the number of conv caps
layer lead to an improvement in the test accuracy. The AR
CapsNet model with four conv caps layer shows 86.37% ac-
curacy with 16-dimensional capsules and 87.19% accuracy
with 32-dimensional capsules. The AR CapsNet with four
conv caps layer and 32-dimensional capsules shows the best
results of 87.19% in C10 and 88.61% in C10+. Also, the AR
CapsNet model with no conv caps layer has more parame-
ters than the model with four conv cap layer and shows the
worst performance. The features in the primary caps layer
has a large spatial size. Thus, the fully conv caps layer con-
nected to the primary caps layer assigns excessive parame-
ters, and this causes overï¬tting.

4.4. Transformation Equivariance

Dimension perturbation
To prove that our proposed
model is transformation equivariant, we executed experi-
ments on the MNIST model as in [12]. We observed the

Digit
8
5

avg

Rot+
0.89
0.89
0.88
(0.89)

x+
0.89
0.78
0.86
(0.88)

y+
0.91
0.73
0.84
(0.80)

Rot-
0.86
0.84
0.83
(0.81)

x-
0.87
0.88
0.83
(0.84)

y-
0.86
0.86
0.84
(0.84)

Table 3. The average of relative ratio {ri} for each combination
of digit and transformation. The avg represents the average of all
10,000 test samples for each afï¬ne transformation. We report the
results of models trained on the MNIST+ dataset in the (). A high
relative ratio implies the difference vectors are strongly aligned.
For random vectors, the average of relative ratio {ri} is 0.311 and
standard deviation is 0.262.

âˆš

variations of decoder output as we perturbed one scalar el-
ement of the output capsules (Figure 3). The experiments
in the [12] perturbed one scalar element from -0.25 to 0.25.
Since the output capsules of the AR CapsNet have lengths
DL compared to 1 in [12], we perturbed one
of up to
âˆš
DL where DL
DL to 0.25
scalar element from âˆ’0.25
denotes the capsule dimension of output capsules. Figure
3 shows that some dimensions of the output capsules rep-
resent variations in the way the digit of the corresponding
class is instantiated. Some dimensions of the output cap-
sules represent the localized skew in digit 0, the rotation
and the size of the higher circle in digit 8, and the rotation,
thickness, and skew in digit 9.

âˆš

Alignment ratio Each scalar element of the output cap-
sules represents a combination of variations such as rota-
tion, thickness, and skew. (Digit 9 in Figure 3) Since the
length of the output capsules is basis-invariant, the trans-
formation on an input image could be represented in co-
ordinates of any basis. To further test the transformation
equivariance of the AR CapsNet, we tested whether the dif-
ference in the output capsules caused by applying a trans-
formation on an input image is aligned in one direction.

Let {Ti}i=1,Â·Â·Â· ,N be a set of afï¬ne transformations on an
input image x. We denote the difference between the output
capsules uL
n (Ti(x)) and uL
n (x) as vi(x) where n denotes
the corresponding class of x.

vi(x) = uL

n (Ti(x)) âˆ’ uL

n (x)

(12)

We denote the concatenation of vi(x) along the row axis
as V. In order to obtain a representative unit vector Ëœv of
{vi(x)}, we apply a Singular-Value Decomposition(SVD)
on matrix V.

c, Ëœv = arg min
ci,Ëœv

= arg min
ci,Ëœv

(cid:88)

||vi(x) âˆ’ ci Â· Ëœv||2
2

i
||V âˆ’ c Â· ËœvT ||2
F

(13)

(14)

where F denotes the Frobenius norm, c = (c1, Â· Â· Â· , cN )T ,
and ci âˆˆ R. The exact solution of this low rank approxima-

Figure 4. Distribution of cosine similarity between unit align vec-
tors Ëœv of positive and negative afï¬ne transformations.

tion problem is the ï¬rst right-singular vector Ëœv of V. This
experiment is similar to the Principal Component Analy-
sis(PCA) except that we do not subtract the mean for each
columns of V. The align vector Ëœv corresponds to the princi-
pal vector of PCA. We observed the relative ratio ri of prin-
cipal component of vi(x) to the vector norms ||vi(x)||2.

ri =

|vi(x) Â· Ëœv|
||vi(x)||2

(15)

We randomly chose 10,000 images from the test set. For
each test image, we generated ï¬ve images by applying an
afï¬ne transformation and observed the relative ratio ri. In
Table 3, Rot(Â±) represents Â±{5, 10, 15, 20, 25} degrees ro-
tations and x(Â±) represents a horizontal translation up to
Â±5 pixels. y(Â±) represents a vertical translation up to Â±5
pixels as well. We observed the average of relative ratio ri
for each combination of digit and transformation. Table 3
shows the average of relative ratio ri for two digits (highest :
digit 8, lowest : digit 5) and the average for 10,000 test sam-
ples for each transformation. As a reference, we generated
ï¬ve random vectors from the standard multivariate normal
distribution. We conducted the same experiment for random
vectors for 1,000 times as well. We obtained an average of
0.311 and a standard deviation of 0.262 for random vectors.
Even for the worst-case digit 5, every transformation shows
a signiï¬cantly higher relative ratio ri of 0.73 in y+ than the
random vectors. This result implies that the difference vec-
tors are strongly aligned in one direction. Therefore, AR
CapsNet encodes afï¬ne transformations on an input image
by some vector components. Also, we report the results of
models trained on the MNIST+ dataset in the (). The mod-
els trained on the MNIST+ show comparable relative ratio
ri to those trained on the MNIST. This result shows that
AR CapsNet encodes afï¬ne transformations even without
observing transformations during training.

An interesting observation is that AR CapsNet is trans-
formation equivariant but do not distinguish the positive and
negative transformations. Figure 4 shows the histogram

Figure 5. Output capsules uL
n when the cosine similarity between
align vectors of positive and negative transformations is -1 or 1.
T+ and Tâˆ’ represent the positive and negative transformation.
Left: cosine similarity -1. Right: cosine similarity 1.

of the cosine similarity between the align vectors of posi-
tive and negative transformation.5 We observed two peaks
around -1 and 1. The cosine similarity of -1 and 1 imply that
positive and negative transformations are encoded in one di-
mension. However, the cosine similarity of 1 suggests that
the difference vectors of positive and negative transforma-
tions have the same direction.(Figure 5) We leave the expla-
nation of this observation to future work.

5. Conclusion

In this work, we suggested a new capsule network ar-
chitecture called Attention Routing CapsuleNet (AR Cap-
sNet). By introducing the attention routing and capsule ac-
tivation, AR CapsNet obtained a higher accuracy compared
to CapsuleNet while using fewer parameters and less train-
ing time. The attention routing is an effective way to route
between capsules because it only compares capsules of the
same spatial location. In addition, the attention routing does
not require an iterative routing process as the dynamic rout-
ing does because it directly learns the weights between cap-
sules. The capsule activation is based on the assumption
that the capsule-scale activation can extract transformation
equivariant features even if it is not orientation-preserving.
This assumption distinguish the capsule activation from the
squash activation function and its variant.

While using the building blocks of CNNs, AR Cap-
sNet is transformation equivariant. We showed that cap-
sules have transformation information by manipulating the
output capsules and then observing the decoder output im-
ages. Also, we observed the difference vectors between the
output capsules of an original image and an afï¬ne trans-
formed image. By showing that the difference vectors are
strongly aligned in one direction, we proved that AR Cap-
sNet encodes transformation information in some dimen-
sions. There are natural variations of AR CapsNet such as
introducing a feature compression by 1x1 convolution to the
capsule activation and a transformer network [16] to the at-
tention routing. We plan to study these variations in the
future.

5The direction of the ï¬rst right-singular vector Ëœv is given by vi(x)Â·Ëœv >

0 in for each positive and negative transformation.

1.000.750.500.250.000.250.500.751.00Cosine similarity01000200030004000500060007000The number of ğ‘¢"#(ğ‘‡&(ğ‘¥))ğ‘¢"#(ğ‘‡)(ğ‘¥))ğ‘¢"#(ğ‘¥)ğ‘¢"#(ğ‘‡&(ğ‘¥))ğ‘¢"#(ğ‘‡)(ğ‘¥))ğ‘¢"#(ğ‘¥)References

[1] M. T. Bahadori. Spectral capsule networks. 2018. 3
[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473, 2014. 3

[3] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770â€“778, 2016. 1, 6

[4] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transform-
ing auto-encoders. In International Conference on Artiï¬cial
Neural Networks, pages 44â€“51. Springer, 2011. 1

[5] G. E. Hinton, S. Sabour, and N. Frosst. Matrix capsules with

em routing. 2018. 3

[6] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In CVPR, vol-
ume 1, page 3, 2017. 1

[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015. 3, 5
[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiï¬cation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097â€“1105, 2012. 1

[9] R. LaLonde and U. Bagci. Capsules for object segmentation.

arXiv preprint arXiv:1804.04241, 2018. 3

[10] H. Li, X. Guo, B. DaiWanli Ouyang, and X. Wang. Neural
network encapsulation. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 252â€“267, 2018.
3

[11] M.-T. Luong, H. Pham, and C. D. Manning. Effective
approaches to attention-based neural machine translation.
arXiv preprint arXiv:1508.04025, 2015. 3

[12] S. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing
between capsules. In Advances in Neural Information Pro-
cessing Systems, pages 3856â€“3866, 2017. 1, 2, 3, 4, 5, 6,
7

[13] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 1

[14] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: a simple way to prevent neural
networks from overï¬tting. The Journal of Machine Learning
Research, 15(1):1929â€“1958, 2014. 5

[15] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training
very deep networks. In Advances in neural information pro-
cessing systems, pages 2377â€“2385, 2015. 1

[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, Å. Kaiser, and I. Polosukhin. Attention is all
In Advances in Neural Information Processing
you need.
Systems, pages 5998â€“6008, 2017. 3, 8

[17] D. Wang and Q. Liu. An optimization view on dynamic rout-

ing between capsules. 2018. 3

[18] E. Xi, S. Bing, and Y. Jin. Capsule network performance on
complex data. arXiv preprint arXiv:1712.03480, 2017. 1, 2

